Numerical Mathematics
Alfio Quarteroni
Riccardo Sacco
Fausto Saleri
Springer

Texts in Applied Mathematicsm37
Springer
New York
Berlin
Heidelberg
Barcelona
Hong Kong
London
Milan
Paris
Singapore
Tokyo
Editors
J.E. Marsden
L. Sirovich
M. Golubitsky
W. J√§ger
Advisors
G. Iooss
P. Holmes
D. Barkley
M. Dellnitz
P. Newton


Alfio QuarteroniMMRiccardo Sacco
Fausto Saleri
123
Numerical Mathematics
With 134 Illustrations

Alfio Quarteroni
Department of Mathematics
Ecole Polytechnique
MFe¬¥de¬¥rale de Lausanne 
CH-1015 Lausanne
Switzerland
alfio.quarteroni@epfl.ch
Riccardo Sacco
Dipartimento di Matematica
Politecnico di Milano
Piazza Leonardo da Vinci 32
20133 Milan
Italy
ricsac@mate.polimi.it
Fausto Saleri
Dipartimento di Matematica,
M‚ÄúF. Enriques‚Äù
Universit√† degli Studi di
MMilano
Via Saldini 50
20133 Milan
Italy
fausto.saleri@unimi.it
Series Editors
J.E. Marsden 
Control and Dynamical Systems, 107‚Äì81
California Institute of Technology
Pasadena, CA 91125
USA
M. Golubitsky
Department of Mathematics
University of Houston
Houston, TX 77204-3476
USA
L. Sirovich
Division of Applied Mathematics
Brown University 
Providence, RI 02912
USA
W. J¬®ager
Department of Applied Mathematics
Universit ¬®at Heidelberg
Im Neuenheimer Feld 294
69120 Heidelberg 
Germany
Library of Congress Cataloging-in-Publication Data
Quarteroni, Alfio.
Numerical mathematics/Alfio Quarteroni, Riccardo Sacco, Fausto Saleri.
p.Mcm. ‚Äî (Texts in applied mathematics; 37)
Includes bibliographical references and index.
ISBN 0-387-98959-5 (alk. paper)
1. Numerical analysis.MI. Sacco, Riccardo.MII. Saleri, Fausto.MIII. Title.MIV. Series.
I. Title.MMII. Series.
QA297.Q83M2000
519.4‚Äîdc21
99-059414
¬© 2000 Springer-Verlag New York, Inc.
All rights reserved. This work may not be translated or copied in whole or in part without
the written permission of the publisher (Springer-Verlag New York, Inc., 175 Fifth Avenue,
New York, NY 10010, USA), except for brief excerpts in connection with reviews or scholarly
analysis. Use in connection with any form of information storage and retrieval, electronic
adaptation, computer software, or by similar or dissimilar methodology now known or heraf-
ter developed is forbidden.
The use of general descriptive names, trade names, trademarks, etc., in this publication, even
if the former are not especially identified, is not to be taken as a sign that such names, as
understood by the Trade Marks and Merchandise Marks Act, may accordingly be used freely
by anyone.
ISBN 0-387-98959-5nSpringer-VerlagnNew YorknBerlinnHeidelbergMSPIN 10747955
Mathematics Subject Classification (1991): 15-01, 34-01, 35-01, 65-01


Preface
Numerical mathematics is the branch of mathematics that proposes, de-
velops, analyzes and applies methods from scientiÔ¨Åc computing to several
Ô¨Åelds including analysis, linear algebra, geometry, approximation theory,
functional equations, optimization and diÔ¨Äerential equations. Other disci-
plines such as physics, the natural and biological sciences, engineering, and
economics and the Ô¨Ånancial sciences frequently give rise to problems that
need scientiÔ¨Åc computing for their solutions.
As such, numerical mathematics is the crossroad of several disciplines of
great relevance in modern applied sciences, and can become a crucial tool
for their qualitative and quantitative analysis. This role is also emphasized
by the continual development of computers and algorithms, which make it
possible nowadays, using scientiÔ¨Åc computing, to tackle problems of such
a large size that real-life phenomena can be simulated providing accurate
responses at aÔ¨Äordable computational cost.
The corresponding spread of numerical software represents an enrichment
for the scientiÔ¨Åc community. However, the user has to make the correct
choice of the method (or the algorithm) which best suits the problem at
hand. As a matter of fact, no black-box methods or algorithms exist that
can eÔ¨Äectively and accurately solve all kinds of problems.
One of the purposes of this book is to provide the mathematical foun-
dations of numerical methods, to analyze their basic theoretical proper-
ties (stability, accuracy, computational complexity), and demonstrate their
performances on examples and counterexamples which outline their pros

viii
Preface
and cons. This is done using the MATLAB¬Æ 1 software environment. This
choice satisÔ¨Åes the two fundamental needs of user-friendliness and wide-
spread diÔ¨Äusion, making it available on virtually every computer.
Every chapter is supplied with examples, exercises and applications of
the discussed theory to the solution of real-life problems. The reader is
thus in the ideal condition for acquiring the theoretical knowledge that is
required to make the right choice among the numerical methodologies and
make use of the related computer programs.
This book is primarily addressed to undergraduate students, with partic-
ular focus on the degree courses in Engineering, Mathematics, Physics and
Computer Science. The attention which is paid to the applications and the
related development of software makes it valuable also for graduate stu-
dents, researchers and users of scientiÔ¨Åc computing in the most widespread
professional Ô¨Åelds.
The content of the volume is organized into four parts and 13 chapters.
Part I comprises two chapters in which we review basic linear algebra and
introduce the general concepts of consistency, stability and convergence of
a numerical method as well as the basic elements of computer arithmetic.
Part II is on numerical linear algebra, and is devoted to the solution of
linear systems (Chapters 3 and 4) and eigenvalues and eigenvectors com-
putation (Chapter 5).
We continue with Part III where we face several issues about functions
and their approximation. SpeciÔ¨Åcally, we are interested in the solution of
nonlinear equations (Chapter 6), solution of nonlinear systems and opti-
mization problems (Chapter 7), polynomial approximation (Chapter 8) and
numerical integration (Chapter 9).
Part IV, which is the more demanding as a mathematical background, is
concerned with approximation, integration and transforms based on orthog-
onal polynomials (Chapter 10), solution of initial value problems (Chap-
ter 11), boundary value problems (Chapter 12) and initial-boundary value
problems for parabolic and hyperbolic equations (Chapter 13).
Part I provides the indispensable background. Each of the remaining
Parts has a size and a content that make it well suited for a semester
course.
A guideline index to the use of the numerous MATLAB Programs de-
veloped in the book is reported at the end of the volume. These programs
are also available at the web site address:
http://www1.mate.polimi.it/Àúcalnum/programs.html
For the reader‚Äôs ease, any code is accompanied by a brief description of
its input/output parameters.
We express our thanks to the staÔ¨Äat Springer-Verlag New York for their
expert guidance and assistance with editorial aspects, as well as to Dr.
1MATLAB is a registered trademark of The MathWorks, Inc.

Preface
ix
Martin Peters from Springer-Verlag Heidelberg and Dr. Francesca Bonadei
from Springer-Italia for their advice and friendly collaboration all along
this project.
We gratefully thank Professors L. Gastaldi and A. Valli for their useful
comments on Chapters 12 and 13.
We also wish to express our gratitude to our families for their forbearance
and understanding, and dedicate this book to them.
Lausanne, Switzerland
AlÔ¨Åo Quarteroni
Milan, Italy
Riccardo Sacco
Milan, Italy
Fausto Saleri
January 2000

Contents
Series Preface
v
Preface
vii
PART I: Getting Started
1.
Foundations of Matrix Analysis
1
1.1
Vector Spaces . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.2
Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
1.3
Operations with Matrices . . . . . . . . . . . . . . . . . . .
5
1.3.1
Inverse of a Matrix . . . . . . . . . . . . . . . . . .
6
1.3.2
Matrices and Linear Mappings
. . . . . . . . . . .
7
1.3.3
Operations with Block-Partitioned Matrices . . . .
7
1.4
Trace and Determinant of a Matrix . . . . . . . . . . . . .
8
1.5
Rank and Kernel of a Matrix
. . . . . . . . . . . . . . . .
9
1.6
Special Matrices . . . . . . . . . . . . . . . . . . . . . . . .
10
1.6.1
Block Diagonal Matrices . . . . . . . . . . . . . . .
10
1.6.2
Trapezoidal and Triangular Matrices . . . . . . . .
11
1.6.3
Banded Matrices . . . . . . . . . . . . . . . . . . .
11
1.7
Eigenvalues and Eigenvectors
. . . . . . . . . . . . . . . .
12
1.8
Similarity Transformations . . . . . . . . . . . . . . . . . .
14
1.9
The Singular Value Decomposition (SVD)
. . . . . . . . .
16
1.10
Scalar Product and Norms in Vector Spaces . . . . . . . .
17
1.11
Matrix Norms . . . . . . . . . . . . . . . . . . . . . . . . .
21

xii
Contents
1.11.1
Relation Between Norms and the
Spectral Radius of a Matrix . . . . . . . . . . . . .
25
1.11.2
Sequences and Series of Matrices . . . . . . . . . .
26
1.12
Positive DeÔ¨Ånite, Diagonally Dominant and M-Matrices
.
27
1.13
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . .
30
2.
Principles of Numerical Mathematics
33
2.1
Well-Posedness and Condition Number of a Problem
. . .
33
2.2
Stability of Numerical Methods . . . . . . . . . . . . . . .
37
2.2.1
Relations Between Stability and Convergence . . .
40
2.3
A priori and a posteriori Analysis . . . . . . . . . . . . . .
41
2.4
Sources of Error in Computational Models . . . . . . . . .
43
2.5
Machine Representation of Numbers
. . . . . . . . . . . .
45
2.5.1
The Positional System . . . . . . . . . . . . . . . .
45
2.5.2
The Floating-Point Number System
. . . . . . . .
46
2.5.3
Distribution of Floating-Point Numbers . . . . . .
49
2.5.4
IEC/IEEE Arithmetic . . . . . . . . . . . . . . . .
49
2.5.5
Rounding of a Real Number in Its
Machine Representation . . . . . . . . . . . . . . .
50
2.5.6
Machine Floating-Point Operations . . . . . . . . .
52
2.6
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . .
54
PART II: Numerical Linear Algebra
3.
Direct Methods for the Solution of Linear Systems
57
3.1
Stability Analysis of Linear Systems
. . . . . . . . . . . .
58
3.1.1
The Condition Number of a Matrix
. . . . . . . .
58
3.1.2
Forward a priori Analysis . . . . . . . . . . . . . .
60
3.1.3
Backward a priori Analysis . . . . . . . . . . . . .
63
3.1.4
A posteriori Analysis . . . . . . . . . . . . . . . . .
64
3.2
Solution of Triangular Systems
. . . . . . . . . . . . . . .
65
3.2.1
Implementation of Substitution Methods
. . . . .
65
3.2.2
Rounding Error Analysis
. . . . . . . . . . . . . .
67
3.2.3
Inverse of a Triangular Matrix
. . . . . . . . . . .
67
3.3
The Gaussian Elimination Method (GEM) and
LU Factorization
. . . . . . . . . . . . . . . . . . . . . . .
68
3.3.1
GEM as a Factorization Method . . . . . . . . . .
72
3.3.2
The EÔ¨Äect of Rounding Errors
. . . . . . . . . . .
76
3.3.3
Implementation of LU Factorization . . . . . . . .
77
3.3.4
Compact Forms of Factorization
. . . . . . . . . .
78
3.4
Other Types of Factorization . . . . . . . . . . . . . . . . .
79
3.4.1
LDMT Factorization . . . . . . . . . . . . . . . . .
79
3.4.2
Symmetric and Positive DeÔ¨Ånite Matrices:
The Cholesky Factorization . . . . . . . . . . . . .
80
3.4.3
Rectangular Matrices: The QR Factorization
. . .
82

Contents
xiii
3.5
Pivoting . . . . . . . . . . . . . . . . . . . . . . . . . . . .
85
3.6
Computing the Inverse of a Matrix
. . . . . . . . . . . . .
89
3.7
Banded Systems . . . . . . . . . . . . . . . . . . . . . . . .
90
3.7.1
Tridiagonal Matrices . . . . . . . . . . . . . . . . .
91
3.7.2
Implementation Issues . . . . . . . . . . . . . . . .
92
3.8
Block Systems . . . . . . . . . . . . . . . . . . . . . . . . .
93
3.8.1
Block LU Factorization
. . . . . . . . . . . . . . .
94
3.8.2
Inverse of a Block-Partitioned Matrix
. . . . . . .
95
3.8.3
Block Tridiagonal Systems . . . . . . . . . . . . . .
95
3.9
Sparse Matrices . . . . . . . . . . . . . . . . . . . . . . . .
97
3.9.1
The Cuthill-McKee Algorithm
. . . . . . . . . . .
98
3.9.2
Decomposition into Substructures
. . . . . . . . .
100
3.9.3
Nested Dissection . . . . . . . . . . . . . . . . . . .
103
3.10
Accuracy of the Solution Achieved Using GEM
. . . . . .
103
3.11
An Approximate Computation of K(A) . . . . . . . . . . .
106
3.12
Improving the Accuracy of GEM
. . . . . . . . . . . . . .
109
3.12.1
Scaling
. . . . . . . . . . . . . . . . . . . . . . . .
110
3.12.2
Iterative ReÔ¨Ånement . . . . . . . . . . . . . . . . .
111
3.13
Undetermined Systems . . . . . . . . . . . . . . . . . . . .
112
3.14
Applications . . . . . . . . . . . . . . . . . . . . . . . . . .
115
3.14.1
Nodal Analysis of a Structured Frame . . . . . . .
115
3.14.2
Regularization of a Triangular Grid
. . . . . . . .
118
3.15
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . .
121
4.
Iterative Methods for Solving Linear Systems
123
4.1
On the Convergence of Iterative Methods . . . . . . . . . .
123
4.2
Linear Iterative Methods . . . . . . . . . . . . . . . . . . .
126
4.2.1
Jacobi, Gauss-Seidel and Relaxation Methods . . .
127
4.2.2
Convergence Results for Jacobi and
Gauss-Seidel Methods . . . . . . . . . . . . . . . .
129
4.2.3
Convergence Results for the Relaxation Method
.
131
4.2.4
A priori Forward Analysis . . . . . . . . . . . . . .
132
4.2.5
Block Matrices . . . . . . . . . . . . . . . . . . . .
133
4.2.6
Symmetric Form of the Gauss-Seidel and
SOR Methods . . . . . . . . . . . . . . . . . . . . .
133
4.2.7
Implementation Issues . . . . . . . . . . . . . . . .
135
4.3
Stationary and Nonstationary Iterative Methods . . . . . .
136
4.3.1
Convergence Analysis of the Richardson Method .
137
4.3.2
Preconditioning Matrices
. . . . . . . . . . . . . .
139
4.3.3
The Gradient Method . . . . . . . . . . . . . . . .
146
4.3.4
The Conjugate Gradient Method . . . . . . . . . .
150
4.3.5
The Preconditioned Conjugate Gradient Method .
156
4.3.6
The Alternating-Direction Method . . . . . . . . .
158
4.4
Methods Based on Krylov Subspace Iterations . . . . . . .
159
4.4.1
The Arnoldi Method for Linear Systems . . . . . .
162

xiv
Contents
4.4.2
The GMRES Method
. . . . . . . . . . . . . . . .
165
4.4.3
The Lanczos Method for Symmetric Systems . . .
167
4.5
The Lanczos Method for Unsymmetric Systems . . . . . .
168
4.6
Stopping Criteria . . . . . . . . . . . . . . . . . . . . . . .
171
4.6.1
A Stopping Test Based on the Increment
. . . . .
172
4.6.2
A Stopping Test Based on the Residual
. . . . . .
174
4.7
Applications . . . . . . . . . . . . . . . . . . . . . . . . . .
174
4.7.1
Analysis of an Electric Network . . . . . . . . . . .
174
4.7.2
Finite DiÔ¨Äerence Analysis of Beam Bending . . . .
177
4.8
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . .
179
5.
Approximation of Eigenvalues and Eigenvectors
183
5.1
Geometrical Location of the Eigenvalues . . . . . . . . . .
183
5.2
Stability and Conditioning Analysis . . . . . . . . . . . . .
186
5.2.1
A priori Estimates . . . . . . . . . . . . . . . . . .
186
5.2.2
A posteriori Estimates . . . . . . . . . . . . . . . .
190
5.3
The Power Method . . . . . . . . . . . . . . . . . . . . . .
192
5.3.1
Approximation of the Eigenvalue of
Largest Module . . . . . . . . . . . . . . . . . . . .
192
5.3.2
Inverse Iteration
. . . . . . . . . . . . . . . . . . .
195
5.3.3
Implementation Issues . . . . . . . . . . . . . . . .
196
5.4
The QR Iteration . . . . . . . . . . . . . . . . . . . . . . .
200
5.5
The Basic QR Iteration . . . . . . . . . . . . . . . . . . . .
201
5.6
The QR Method for Matrices in Hessenberg Form . . . . .
203
5.6.1
Householder and Givens Transformation Matrices
204
5.6.2
Reducing a Matrix in Hessenberg Form
. . . . . .
207
5.6.3
QR Factorization of a Matrix in Hessenberg Form
209
5.6.4
The Basic QR Iteration Starting from
Upper Hessenberg Form . . . . . . . . . . . . . . .
210
5.6.5
Implementation of Transformation Matrices . . . .
212
5.7
The QR Iteration with Shifting Techniques . . . . . . . . .
215
5.7.1
The QR Method with Single Shift
. . . . . . . . .
215
5.7.2
The QR Method with Double Shift . . . . . . . . .
218
5.8
Computing the Eigenvectors and the SVD of a Matrix
. .
221
5.8.1
The Hessenberg Inverse Iteration . . . . . . . . . .
221
5.8.2
Computing the Eigenvectors from the
Schur Form of a Matrix . . . . . . . . . . . . . . .
221
5.8.3
Approximate Computation of the SVD of a Matrix 222
5.9
The Generalized Eigenvalue Problem . . . . . . . . . . . .
224
5.9.1
Computing the Generalized Real Schur Form . . .
225
5.9.2
Generalized Real Schur Form of
Symmetric-DeÔ¨Ånite Pencils
. . . . . . . . . . . . .
226
5.10
Methods for Eigenvalues of Symmetric Matrices . . . . . .
227
5.10.1
The Jacobi Method
. . . . . . . . . . . . . . . . .
227
5.10.2
The Method of Sturm Sequences . . . . . . . . . .
230

Contents
xv
5.11
The Lanczos Method . . . . . . . . . . . . . . . . . . . . .
233
5.12
Applications . . . . . . . . . . . . . . . . . . . . . . . . . .
235
5.12.1
Analysis of the Buckling of a Beam . . . . . . . . .
236
5.12.2
Free Dynamic Vibration of a Bridge . . . . . . . .
238
5.13
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . .
240
PART III: Around Functions and Functionals
6.
RootÔ¨Ånding for Nonlinear Equations
245
6.1
Conditioning of a Nonlinear Equation . . . . . . . . . . . .
246
6.2
A Geometric Approach to RootÔ¨Ånding
. . . . . . . . . . .
248
6.2.1
The Bisection Method . . . . . . . . . . . . . . . .
248
6.2.2
The Methods of Chord, Secant and Regula Falsi
and Newton‚Äôs Method . . . . . . . . . . . . . . . .
251
6.2.3
The Dekker-Brent Method
. . . . . . . . . . . . .
256
6.3
Fixed-Point Iterations for Nonlinear Equations . . . . . . .
257
6.3.1
Convergence Results for
Some Fixed-Point Methods . . . . . . . . . . . . .
260
6.4
Zeros of Algebraic Equations . . . . . . . . . . . . . . . . .
261
6.4.1
The Horner Method and DeÔ¨Çation . . . . . . . . .
262
6.4.2
The Newton-Horner Method
. . . . . . . . . . . .
263
6.4.3
The Muller Method
. . . . . . . . . . . . . . . . .
267
6.5
Stopping Criteria . . . . . . . . . . . . . . . . . . . . . . .
269
6.6
Post-Processing Techniques for Iterative Methods . . . . .
272
6.6.1
Aitken‚Äôs Acceleration
. . . . . . . . . . . . . . . .
272
6.6.2
Techniques for Multiple Roots
. . . . . . . . . . .
275
6.7
Applications . . . . . . . . . . . . . . . . . . . . . . . . . .
276
6.7.1
Analysis of the State Equation for a Real Gas
. .
276
6.7.2
Analysis of a Nonlinear Electrical Circuit
. . . . .
277
6.8
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . .
279
7.
Nonlinear Systems and Numerical Optimization
281
7.1
Solution of Systems of Nonlinear Equations
. . . . . . . .
282
7.1.1
Newton‚Äôs Method and Its Variants . . . . . . . . .
283
7.1.2
ModiÔ¨Åed Newton‚Äôs Methods . . . . . . . . . . . . .
284
7.1.3
Quasi-Newton Methods . . . . . . . . . . . . . . .
288
7.1.4
Secant-Like Methods . . . . . . . . . . . . . . . . .
288
7.1.5
Fixed-Point Methods . . . . . . . . . . . . . . . . .
290
7.2
Unconstrained Optimization . . . . . . . . . . . . . . . . .
294
7.2.1
Direct Search Methods . . . . . . . . . . . . . . . .
295
7.2.2
Descent Methods . . . . . . . . . . . . . . . . . . .
300
7.2.3
Line Search Techniques
. . . . . . . . . . . . . . .
302
7.2.4
Descent Methods for Quadratic Functions . . . . .
304
7.2.5
Newton-Like Methods for Function Minimization .
307
7.2.6
Quasi-Newton Methods . . . . . . . . . . . . . . .
308

xvi
Contents
7.2.7
Secant-Like Methods . . . . . . . . . . . . . . . . .
309
7.3
Constrained Optimization
. . . . . . . . . . . . . . . . . .
311
7.3.1
Kuhn-Tucker Necessary Conditions for
Nonlinear Programming . . . . . . . . . . . . . . .
313
7.3.2
The Penalty Method . . . . . . . . . . . . . . . . .
315
7.3.3
The Method of Lagrange Multipliers . . . . . . . .
317
7.4
Applications . . . . . . . . . . . . . . . . . . . . . . . . . .
319
7.4.1
Solution of a Nonlinear System Arising from
Semiconductor Device Simulation . . . . . . . . . .
320
7.4.2
Nonlinear Regularization of a Discretization Grid .
323
7.5
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . .
325
8.
Polynomial Interpolation
327
8.1
Polynomial Interpolation . . . . . . . . . . . . . . . . . . .
328
8.1.1
The Interpolation Error . . . . . . . . . . . . . . .
329
8.1.2
Drawbacks of Polynomial Interpolation on Equally
Spaced Nodes and Runge‚Äôs Counterexample . . . .
330
8.1.3
Stability of Polynomial Interpolation . . . . . . . .
332
8.2
Newton Form of the Interpolating Polynomial . . . . . . .
333
8.2.1
Some Properties of Newton Divided DiÔ¨Äerences . .
335
8.2.2
The Interpolation Error Using Divided DiÔ¨Äerences
337
8.3
Piecewise Lagrange Interpolation
. . . . . . . . . . . . . .
338
8.4
Hermite-BirkoÔ¨ÄInterpolation
. . . . . . . . . . . . . . . .
341
8.5
Extension to the Two-Dimensional Case
. . . . . . . . . .
343
8.5.1
Polynomial Interpolation
. . . . . . . . . . . . . .
343
8.5.2
Piecewise Polynomial Interpolation . . . . . . . . .
344
8.6
Approximation by Splines
. . . . . . . . . . . . . . . . . .
348
8.6.1
Interpolatory Cubic Splines . . . . . . . . . . . . .
349
8.6.2
B-Splines . . . . . . . . . . . . . . . . . . . . . . .
353
8.7
Splines in Parametric Form
. . . . . . . . . . . . . . . . .
357
8.7.1
B¬¥ezier Curves and Parametric B-Splines . . . . . .
359
8.8
Applications . . . . . . . . . . . . . . . . . . . . . . . . . .
362
8.8.1
Finite Element Analysis of a Clamped Beam
. . .
363
8.8.2
Geometric Reconstruction Based on
Computer Tomographies . . . . . . . . . . . . . . .
366
8.9
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . .
368
9.
Numerical Integration
371
9.1
Quadrature Formulae . . . . . . . . . . . . . . . . . . . . .
371
9.2
Interpolatory Quadratures . . . . . . . . . . . . . . . . . .
373
9.2.1
The Midpoint or Rectangle Formula . . . . . . . .
373
9.2.2
The Trapezoidal Formula . . . . . . . . . . . . . .
375
9.2.3
The Cavalieri-Simpson Formula . . . . . . . . . . .
377
9.3
Newton-Cotes Formulae
. . . . . . . . . . . . . . . . . . .
378
9.4
Composite Newton-Cotes Formulae . . . . . . . . . . . . .
383

Contents
xvii
9.5
Hermite Quadrature Formulae . . . . . . . . . . . . . . . .
386
9.6
Richardson Extrapolation
. . . . . . . . . . . . . . . . . .
387
9.6.1
Romberg Integration . . . . . . . . . . . . . . . . .
389
9.7
Automatic Integration
. . . . . . . . . . . . . . . . . . . .
391
9.7.1
Non Adaptive Integration Algorithms
. . . . . . .
392
9.7.2
Adaptive Integration Algorithms . . . . . . . . . .
394
9.8
Singular Integrals . . . . . . . . . . . . . . . . . . . . . . .
398
9.8.1
Integrals of Functions with Finite
Jump Discontinuities . . . . . . . . . . . . . . . . .
398
9.8.2
Integrals of InÔ¨Ånite Functions . . . . . . . . . . . .
398
9.8.3
Integrals over Unbounded Intervals . . . . . . . . .
401
9.9
Multidimensional Numerical Integration
. . . . . . . . . .
402
9.9.1
The Method of Reduction Formula . . . . . . . . .
403
9.9.2
Two-Dimensional Composite Quadratures . . . . .
404
9.9.3
Monte Carlo Methods for
Numerical Integration . . . . . . . . . . . . . . . .
407
9.10
Applications . . . . . . . . . . . . . . . . . . . . . . . . . .
408
9.10.1
Computation of an Ellipsoid Surface . . . . . . . .
408
9.10.2
Computation of the Wind Action on a
Sailboat Mast . . . . . . . . . . . . . . . . . . . . .
410
9.11
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . .
412
PART IV: Transforms, DiÔ¨Äerentiation
and Problem Discretization
10. Orthogonal Polynomials in Approximation Theory
415
10.1
Approximation of Functions by Generalized Fourier Series
415
10.1.1
The Chebyshev Polynomials . . . . . . . . . . . . .
417
10.1.2
The Legendre Polynomials
. . . . . . . . . . . . .
419
10.2
Gaussian Integration and Interpolation . . . . . . . . . . .
419
10.3
Chebyshev Integration and Interpolation . . . . . . . . . .
424
10.4
Legendre Integration and Interpolation . . . . . . . . . . .
426
10.5
Gaussian Integration over Unbounded Intervals
. . . . . .
428
10.6
Programs for the Implementation of Gaussian Quadratures 429
10.7
Approximation of a Function in the Least-Squares Sense .
431
10.7.1
Discrete Least-Squares Approximation . . . . . . .
431
10.8
The Polynomial of Best Approximation . . . . . . . . . . .
433
10.9
Fourier Trigonometric Polynomials
. . . . . . . . . . . . .
435
10.9.1
The Gibbs Phenomenon . . . . . . . . . . . . . . .
439
10.9.2
The Fast Fourier Transform . . . . . . . . . . . . .
440
10.10 Approximation of Function Derivatives . . . . . . . . . . .
442
10.10.1 Classical Finite DiÔ¨Äerence Methods . . . . . . . . .
442
10.10.2 Compact Finite DiÔ¨Äerences . . . . . . . . . . . . .
444
10.10.3 Pseudo-Spectral Derivative
. . . . . . . . . . . . .
448
10.11 Transforms and Their Applications . . . . . . . . . . . . .
450

xviii
Contents
10.11.1 The Fourier Transform . . . . . . . . . . . . . . . .
450
10.11.2 (Physical) Linear Systems and Fourier Transform .
453
10.11.3 The Laplace Transform
. . . . . . . . . . . . . . .
455
10.11.4 The Z-Transform . . . . . . . . . . . . . . . . . . .
457
10.12 The Wavelet Transform . . . . . . . . . . . . . . . . . . . .
458
10.12.1 The Continuous Wavelet Transform
. . . . . . . .
458
10.12.2 Discrete and Orthonormal Wavelets
. . . . . . . .
461
10.13 Applications . . . . . . . . . . . . . . . . . . . . . . . . . .
463
10.13.1 Numerical Computation of Blackbody Radiation .
463
10.13.2 Numerical Solution of Schr¬®odinger Equation . . . .
464
10.14 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . .
467
11. Numerical Solution of Ordinary DiÔ¨Äerential Equations
469
11.1
The Cauchy Problem . . . . . . . . . . . . . . . . . . . . .
469
11.2
One-Step Numerical Methods
. . . . . . . . . . . . . . . .
472
11.3
Analysis of One-Step Methods . . . . . . . . . . . . . . . .
473
11.3.1
The Zero-Stability . . . . . . . . . . . . . . . . . .
475
11.3.2
Convergence Analysis
. . . . . . . . . . . . . . . .
477
11.3.3
The Absolute Stability . . . . . . . . . . . . . . . .
479
11.4
DiÔ¨Äerence Equations
. . . . . . . . . . . . . . . . . . . . .
482
11.5
Multistep Methods
. . . . . . . . . . . . . . . . . . . . . .
487
11.5.1
Adams Methods
. . . . . . . . . . . . . . . . . . .
490
11.5.2
BDF Methods
. . . . . . . . . . . . . . . . . . . .
492
11.6
Analysis of Multistep Methods . . . . . . . . . . . . . . . .
492
11.6.1
Consistency . . . . . . . . . . . . . . . . . . . . . .
493
11.6.2
The Root Conditions . . . . . . . . . . . . . . . . .
494
11.6.3
Stability and Convergence Analysis for
Multistep Methods . . . . . . . . . . . . . . . . . .
495
11.6.4
Absolute Stability of Multistep Methods . . . . . .
499
11.7
Predictor-Corrector Methods . . . . . . . . . . . . . . . . .
502
11.8
Runge-Kutta Methods
. . . . . . . . . . . . . . . . . . . .
508
11.8.1
Derivation of an Explicit RK Method
. . . . . . .
511
11.8.2
Stepsize Adaptivity for RK Methods . . . . . . . .
512
11.8.3
Implicit RK Methods
. . . . . . . . . . . . . . . .
514
11.8.4
Regions of Absolute Stability for RK Methods
. .
516
11.9
Systems of ODEs . . . . . . . . . . . . . . . . . . . . . . .
517
11.10 StiÔ¨ÄProblems . . . . . . . . . . . . . . . . . . . . . . . . .
519
11.11 Applications . . . . . . . . . . . . . . . . . . . . . . . . . .
521
11.11.1 Analysis of the Motion of a Frictionless Pendulum
522
11.11.2 Compliance of Arterial Walls . . . . . . . . . . . .
523
11.12 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . .
527
12. Two-Point Boundary Value Problems
531
12.1
A Model Problem . . . . . . . . . . . . . . . . . . . . . . .
531
12.2
Finite DiÔ¨Äerence Approximation . . . . . . . . . . . . . . .
533

Contents
xix
12.2.1
Stability Analysis by the Energy Method
. . . . .
534
12.2.2
Convergence Analysis
. . . . . . . . . . . . . . . .
538
12.2.3
Finite DiÔ¨Äerences for Two-Point Boundary
Value Problems with Variable CoeÔ¨Écients . . . . .
540
12.3
The Spectral Collocation Method . . . . . . . . . . . . . .
542
12.4
The Galerkin Method . . . . . . . . . . . . . . . . . . . . .
544
12.4.1
Integral Formulation of Boundary-Value Problems
544
12.4.2
A Quick Introduction to Distributions . . . . . . .
546
12.4.3
Formulation and Properties of the
Galerkin Method . . . . . . . . . . . . . . . . . . .
547
12.4.4
Analysis of the Galerkin Method . . . . . . . . . .
548
12.4.5
The Finite Element Method . . . . . . . . . . . . .
550
12.4.6
Implementation Issues . . . . . . . . . . . . . . . .
556
12.4.7
Spectral Methods . . . . . . . . . . . . . . . . . . .
559
12.5
Advection-DiÔ¨Äusion Equations . . . . . . . . . . . . . . . .
560
12.5.1
Galerkin Finite Element Approximation . . . . . .
561
12.5.2
The Relationship Between Finite Elements and
Finite DiÔ¨Äerences; the Numerical Viscosity
. . . .
563
12.5.3
Stabilized Finite Element Methods . . . . . . . . .
567
12.6
A Quick Glance to the Two-Dimensional Case . . . . . . .
572
12.7
Applications . . . . . . . . . . . . . . . . . . . . . . . . . .
575
12.7.1
Lubrication of a Slider . . . . . . . . . . . . . . . .
575
12.7.2
Vertical Distribution of Spore
Concentration over Wide Regions . . . . . . . . . .
576
12.8
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . .
578
13. Parabolic and Hyperbolic Initial Boundary
Value Problems
581
13.1
The Heat Equation . . . . . . . . . . . . . . . . . . . . . .
581
13.2
Finite DiÔ¨Äerence Approximation of the Heat Equation
. .
584
13.3
Finite Element Approximation of the Heat Equation
. . .
586
13.3.1
Stability Analysis of the Œ∏-Method . . . . . . . . .
588
13.4
Space-Time Finite Element Methods for the
Heat Equation . . . . . . . . . . . . . . . . . . . . . . . . .
593
13.5
Hyperbolic Equations: A Scalar Transport Problem . . . .
597
13.6
Systems of Linear Hyperbolic Equations
. . . . . . . . . .
599
13.6.1
The Wave Equation
. . . . . . . . . . . . . . . . .
601
13.7
The Finite DiÔ¨Äerence Method for Hyperbolic Equations . .
602
13.7.1
Discretization of the Scalar Equation . . . . . . . .
602
13.8
Analysis of Finite DiÔ¨Äerence Methods . . . . . . . . . . . .
605
13.8.1
Consistency . . . . . . . . . . . . . . . . . . . . . .
605
13.8.2
Stability . . . . . . . . . . . . . . . . . . . . . . . .
605
13.8.3
The CFL Condition
. . . . . . . . . . . . . . . . .
606
13.8.4
Von Neumann Stability Analysis . . . . . . . . . .
608
13.9
Dissipation and Dispersion . . . . . . . . . . . . . . . . . .
611

xx
Contents
13.9.1
Equivalent Equations
. . . . . . . . . . . . . . . .
614
13.10 Finite Element Approximation of Hyperbolic Equations . .
618
13.10.1 Space Discretization with Continuous and
Discontinuous Finite Elements
. . . . . . . . . . .
618
13.10.2 Time Discretization
. . . . . . . . . . . . . . . . .
620
13.11 Applications . . . . . . . . . . . . . . . . . . . . . . . . . .
623
13.11.1 Heat Conduction in a Bar . . . . . . . . . . . . . .
623
13.11.2 A Hyperbolic Model for Blood Flow
Interaction with Arterial Walls . . . . . . . . . . .
623
13.12 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . .
625
References
627
Index of MATLAB Programs
643
Index
647

1
Foundations of Matrix Analysis
In this chapter we recall the basic elements of linear algebra which will be
employed in the remainder of the text. For most of the proofs as well as
for the details, the reader is referred to [Bra75], [Nob69], [Hal58]. Further
results on eigenvalues can be found in [Hou75] and [Wil65].
1.1
Vector Spaces
DeÔ¨Ånition 1.1 A vector space over the numeric Ô¨Åeld K (K = R or K = C)
is a nonempty set V , whose elements are called vectors and in which two
operations are deÔ¨Åned, called addition and scalar multiplication, that enjoy
the following properties:
1. addition is commutative and associative;
2. there exists an element 0 ‚ààV (the zero vector or null vector) such
that v + 0 = v for each v ‚ààV ;
3. 0 ¬∑ v = 0, 1 ¬∑ v = v, where 0 and 1 are respectively the zero and the
unity of K;
4. for each element v ‚ààV there exists its opposite, ‚àív, in V such that
v + (‚àív) = 0;

2
1. Foundations of Matrix Analysis
5. the following distributive properties hold
‚àÄŒ± ‚ààK, ‚àÄv, w ‚ààV, Œ±(v + w) = Œ±v + Œ±w,
‚àÄŒ±, Œ≤ ‚ààK, ‚àÄv ‚ààV, (Œ± + Œ≤)v = Œ±v + Œ≤v;
6. the following associative property holds
‚àÄŒ±, Œ≤ ‚ààK, ‚àÄv ‚ààV, (Œ±Œ≤)v = Œ±(Œ≤v).
‚ñ†
Example 1.1 Remarkable instances of vector spaces are:
- V = Rn (respectively V = Cn): the set of the n-tuples of real (respectively
complex) numbers, n ‚â•1;
- V = Pn: the set of polynomials pn(x) = n
k=0 akxk with real (or complex)
coeÔ¨Écients ak having degree less than or equal to n, n ‚â•0;
- V = Cp([a, b]): the set of real (or complex)-valued functions which are con-
tinuous on [a, b] up to their p-th derivative, 0 ‚â§p < ‚àû.
‚Ä¢
DeÔ¨Ånition 1.2 We say that a nonempty part W of V is a vector subspace
of V iÔ¨ÄW is a vector space over K.
‚ñ†
Example 1.2 The vector space Pn is a vector subspace of C‚àû(R), which is the
space of inÔ¨Ånite continuously diÔ¨Äerentiable functions on the real line. A trivial
subspace of any vector space is the one containing only the zero vector.
‚Ä¢
In particular, the set W of the linear combinations of a system of p vectors
of V , {v1, . . . , vp}, is a vector subspace of V , called the generated subspace
or span of the vector system, and is denoted by
W
= span {v1, . . . , vp}
= {v = Œ±1v1 + . . . + Œ±pvp
with Œ±i ‚ààK, i = 1, . . . , p} .
The system {v1, . . . , vp} is called a system of generators for W.
If W1, . . . , Wm are vector subspaces of V , then the set
S = {w : w = v1 + . . . + vm with vi ‚ààWi, i = 1, . . . , m}
is also a vector subspace of V . We say that S is the direct sum of the
subspaces Wi if any element s ‚ààS admits a unique representation of the
form s = v1 + . . . + vm with vi ‚ààWi and i = 1, . . . , m. In such a case, we
shall write S = W1 ‚äï. . . ‚äïWm.

1.2 Matrices
3
DeÔ¨Ånition 1.3 A system of vectors {v1, . . . , vm} of a vector space V is
called linearly independent if the relation
Œ±1v1 + Œ±2v2 + . . . + Œ±mvm = 0
with Œ±1, Œ±2, . . . , Œ±m ‚ààK implies that Œ±1 = Œ±2 = . . . = Œ±m = 0. Otherwise,
the system will be called linearly dependent.
‚ñ†
We call a basis of V any system of linearly independent generators of V .
If {u1, . . . , un} is a basis of V , the expression v = v1u1 + . . . + vnun is
called the decomposition of v with respect to the basis and the scalars
v1, . . . , vn ‚ààK are the components of v with respect to the given basis.
Moreover, the following property holds.
Property 1.1 Let V be a vector space which admits a basis of n vectors.
Then every system of linearly independent vectors of V has at most n el-
ements and any other basis of V has n elements. The number n is called
the dimension of V and we write dim(V ) = n.
If, instead, for any n there always exist n linearly independent vectors of
V , the vector space is called inÔ¨Ånite dimensional.
Example 1.3 For any integer p the space Cp([a, b]) is inÔ¨Ånite dimensional. The
spaces Rn and Cn have dimension equal to n. The usual basis for Rn is the set of
unit vectors {e1, . . . , en} where (ei)j = Œ¥ij for i, j = 1, . . . n, where Œ¥ij denotes
the Kronecker symbol equal to 0 if i Ã∏= j and 1 if i = j. This choice is of course
not the only one that is possible (see Exercise 2).
‚Ä¢
1.2
Matrices
Let m and n be two positive integers. We call a matrix having m rows and
n columns, or a matrix m √ó n, or a matrix (m, n), with elements in K, a
set of mn scalars aij ‚ààK, with i = 1, . . . , m and j = 1, . . . n, represented
in the following rectangular array
A =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
a11
a12
. . .
a1n
a21
a22
. . .
a2n
...
...
...
am1
am2
. . .
amn
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª.
(1.1)
When K = R or K = C we shall respectively write A ‚ààRm√ón or A ‚àà
Cm√ón, to explicitly outline the numerical Ô¨Åelds which the elements of A
belong to. Capital letters will be used to denote the matrices, while the
lower case letters corresponding to those upper case letters will denote the
matrix entries.

4
1. Foundations of Matrix Analysis
We shall abbreviate (1.1) as A = (aij) with i = 1, . . . , m and j = 1, . . . n.
The index i is called row index, while j is the column index. The set
(ai1, ai2, . . . , ain) is called the i-th row of A; likewise, (a1j, a2j, . . . , amj)
is the j-th column of A.
If n = m the matrix is called squared or having order n and the set of
the entries (a11, a22, . . . , ann) is called its main diagonal.
A matrix having one row or one column is called a row vector or column
vector respectively. Unless otherwise speciÔ¨Åed, we shall always assume that
a vector is a column vector. In the case n = m = 1, the matrix will simply
denote a scalar of K.
Sometimes it turns out to be useful to distinguish within a matrix the set
made up by speciÔ¨Åed rows and columns. This prompts us to introduce the
following deÔ¨Ånition.
DeÔ¨Ånition 1.4 Let A be a matrix m √ó n. Let 1 ‚â§i1 < i2 < . . . < ik ‚â§m
and 1 ‚â§j1 < j2 < . . . < jl ‚â§n two sets of contiguous indexes. The matrix
S(k √ó l) of entries spq = aipjq with p = 1, . . . , k, q = 1, . . . , l is called a
submatrix of A. If k = l and ir = jr for r = 1, . . . , k, S is called a principal
submatrix of A.
‚ñ†
DeÔ¨Ånition 1.5 A matrix A(m √ó n) is called block partitioned or said to
be partitioned into submatrices if
A =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
A11
A12
. . .
A1l
A21
A22
. . .
A2l
...
...
...
...
Ak1
Ak2
. . .
Akl
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª,
where Aij are submatrices of A.
‚ñ†
Among the possible partitions of A, we recall in particular the partition by
columns
A = (a1, a2, . . . , an),
ai being the i-th column vector of A. In a similar way the partition by rows
of A can be deÔ¨Åned. To Ô¨Åx the notations, if A is a matrix m √ó n, we shall
denote by
A(i1 : i2, j1 : j2) = (aij)
i1 ‚â§i ‚â§i2, j1 ‚â§j ‚â§j2
the submatrix of A of size (i2 ‚àíi1 + 1) √ó (j2 ‚àíj1 + 1) that lies between the
rows i1 and i2 and the columns j1 and j2. Likewise, if v is a vector of size
n, we shall denote by v(i1 : i2) the vector of size i2 ‚àíi1 + 1 made up by
the i1-th to the i2-th components of v.
These notations are convenient in view of programming the algorithms
that will be presented throughout the volume in the MATLAB language.

1.3 Operations with Matrices
5
1.3
Operations with Matrices
Let A = (aij) and B = (bij) be two matrices m √ó n over K. We say that
A is equal to B, if aij = bij for i = 1, . . . , m, j = 1, . . . , n. Moreover, we
deÔ¨Åne the following operations:
- matrix sum: the matrix sum is the matrix A+B = (aij +bij). The neutral
element in a matrix sum is the null matrix, still denoted by 0 and
made up only by null entries;
- matrix multiplication by a scalar: the multiplication of A by Œª ‚ààK, is a
matrix ŒªA = (Œªaij);
- matrix product: the product of two matrices A and B of sizes (m, p)
and (p, n) respectively, is a matrix C(m, n) whose entries are cij =
p

k=1
aikbkj, for i = 1, . . . , m, j = 1, . . . , n.
The matrix product is associative and distributive with respect to the ma-
trix sum, but it is not in general commutative. The square matrices for
which the property AB = BA holds, will be called commutative.
In the case of square matrices, the neutral element in the matrix product
is a square matrix of order n called the unit matrix of order n or, more
frequently, the identity matrix given by In = (Œ¥ij). The identity matrix
is, by deÔ¨Ånition, the only matrix n √ó n such that AIn = InA = A for all
square matrices A. In the following we shall omit the subscript n unless it
is strictly necessary. The identity matrix is a special instance of a diagonal
matrix of order n, that is, a square matrix of the type D = (diiŒ¥ij). We will
use in the following the notation D = diag(d11, d22, . . . , dnn).
Finally, if A is a square matrix of order n and p is an integer, we deÔ¨Åne Ap
as the product of A with itself iterated p times. We let A0 = I.
Let us now address the so-called elementary row operations that can be
performed on a matrix. They consist of:
- multiplying the i-th row of a matrix by a scalar Œ±; this operation is
equivalent to pre-multiplying A by the matrix D = diag(1, . . . , 1, Œ±,
1, . . . , 1), where Œ± occupies the i-th position;
- exchanging the i-th and j-th rows of a matrix; this can be done by pre-
multiplying A by the matrix P(i,j) of elements
p(i,j)
rs
=
Ô£±
Ô£¥
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£¥
Ô£≥
1
if r = s = 1, . . . , i ‚àí1, i + 1, . . . , j ‚àí1, j + 1, . . . n,
1
if r = j, s = i or r = i, s = j,
0
otherwise,
(1.2)

6
1. Foundations of Matrix Analysis
where Ir denotes the identity matrix of order r = j ‚àíi ‚àí1 if j >
i (henceforth, matrices with size equal to zero will correspond to
the empty set). Matrices like (1.2) are called elementary permutation
matrices. The product of elementary permutation matrices is called
a permutation matrix, and it performs the row exchanges associated
with each elementary permutation matrix. In practice, a permutation
matrix is a reordering by rows of the identity matrix;
- adding Œ± times the j-th row of a matrix to its i-th row. This operation
can also be performed by pre-multiplying A by the matrix I + N(i,j)
Œ±
,
where N(i,j)
Œ±
is a matrix having null entries except the one in position
i, j whose value is Œ±.
1.3.1
Inverse of a Matrix
DeÔ¨Ånition 1.6 A square matrix A of order n is called invertible (or regular
or nonsingular) if there exists a square matrix B of order n such that
A B = B A = I. B is called the inverse matrix of A and is denoted by A‚àí1.
A matrix which is not invertible is called singular.
‚ñ†
If A is invertible its inverse is also invertible, with (A‚àí1)‚àí1 = A. Moreover,
if A and B are two invertible matrices of order n, their product AB is also
invertible, with (A B)‚àí1 = B‚àí1A‚àí1. The following property holds.
Property 1.2 A square matrix is invertible iÔ¨Äits column vectors are lin-
early independent.
DeÔ¨Ånition 1.7 We call the transpose of a matrix A‚ààRm√ón the matrix
n √ó m, denoted by AT , that is obtained by exchanging the rows of A with
the columns of A.
‚ñ†
Clearly, (AT )T = A, (A + B)T = AT + BT , (AB)T = BT AT and (Œ±A)T =
Œ±AT ‚àÄŒ± ‚ààR. If A is invertible, then also (AT )‚àí1 = (A‚àí1)T = A‚àíT .
DeÔ¨Ånition 1.8 Let A ‚ààCm√ón; the matrix B = AH ‚ààCn√óm is called the
conjugate transpose (or adjoint) of A if bij = ¬Øaji, where ¬Øaji is the complex
conjugate of aji.
‚ñ†
In analogy with the case of the real matrices, it turns out that (A+B)H =
AH + BH, (AB)H = BHAH and (Œ±A)H = ¬ØŒ±AH ‚àÄŒ± ‚ààC.
DeÔ¨Ånition 1.9 A matrix A ‚ààRn√ón is called symmetric if A = AT , while
it is antisymmetric if A = ‚àíAT . Finally, it is called orthogonal if AT A =
AAT = I, that is A‚àí1 = AT .
‚ñ†
Permutation matrices are orthogonal and the same is true for their prod-
ucts.

1.3 Operations with Matrices
7
DeÔ¨Ånition 1.10 A matrix A ‚ààCn√ón is called hermitian or self-adjoint if
AT = ¬ØA, that is, if AH = A, while it is called unitary if AHA = AAH = I.
Finally, if AAH = AHA, A is called normal.
‚ñ†
As a consequence, a unitary matrix is one such that A‚àí1 = AH.
Of course, a unitary matrix is also normal, but it is not in general her-
mitian. For instance, the matrix of the Example 1.4 is unitary, although
not symmetric (if s Ã∏= 0). We Ô¨Ånally notice that the diagonal entries of an
hermitian matrix must necessarily be real (see also Exercise 5).
1.3.2
Matrices and Linear Mappings
DeÔ¨Ånition 1.11 A linear map from Cn into Cm is a function f : Cn ‚àí‚Üí
Cm such that f(Œ±x + Œ≤y) = Œ±f(x) + Œ≤f(y), ‚àÄŒ±, Œ≤ ‚ààK and ‚àÄx, y ‚ààCn. ‚ñ†
The following result links matrices and linear maps.
Property 1.3 Let f : Cn ‚àí‚ÜíCm be a linear map. Then, there exists a
unique matrix Af ‚ààCm√ón such that
f(x) = Afx
‚àÄx ‚ààCn.
(1.3)
Conversely, if Af ‚ààCm√ón then the function deÔ¨Åned in (1.3) is a linear
map from Cn into Cm.
Example 1.4 An important example of a linear map is the counterclockwise
rotation by an angle œë in the plane (x1, x2). The matrix associated with such a
map is given by
G(œë) =

c
s
‚àís
c

,
c = cos(œë), s = sin(œë)
and it is called a rotation matrix.
‚Ä¢
1.3.3
Operations with Block-Partitioned Matrices
All the operations that have been previously introduced can be extended
to the case of a block-partitioned matrix A, provided that the size of each
single block is such that any single matrix operation is well-deÔ¨Åned.
Indeed, the following result can be shown (see, e.g., [Ste73]).
Property 1.4 Let A and B be the block matrices
A =
Ô£Æ
Ô£ØÔ£∞
A11
. . .
A1l
...
...
...
Ak1
. . .
Akl
Ô£π
Ô£∫Ô£ª,
B =
Ô£Æ
Ô£ØÔ£∞
B11
. . .
B1n
...
...
...
Bm1
. . .
Bmn
Ô£π
Ô£∫Ô£ª
where Aij and Bij are matrices (ki √ó lj) and (mi √ó nj). Then we have

8
1. Foundations of Matrix Analysis
1.
ŒªA =
Ô£Æ
Ô£ØÔ£∞
ŒªA11
. . .
ŒªA1l
...
...
...
ŒªAk1
. . .
ŒªAkl
Ô£π
Ô£∫Ô£ª,
Œª ‚ààC;
AT =
Ô£Æ
Ô£ØÔ£∞
AT
11
. . .
AT
k1
...
...
...
AT
1l
. . .
AT
kl
Ô£π
Ô£∫Ô£ª;
2. if k = m, l = n, mi = ki and nj = lj, then
A + B =
Ô£Æ
Ô£ØÔ£∞
A11 + B11
. . .
A1l + B1l
...
...
...
Ak1 + Bk1
. . .
Akl + Bkl
Ô£π
Ô£∫Ô£ª;
3. if l = m, li = mi and ki = ni, then, letting Cij =
m

s=1
AisBsj,
AB =
Ô£Æ
Ô£ØÔ£∞
C11
. . .
C1l
...
...
...
Ck1
. . .
Ckl
Ô£π
Ô£∫Ô£ª.
1.4
Trace and Determinant of a Matrix
Let us consider a square matrix A of order n. The trace of a matrix is the
sum of the diagonal entries of A, that is tr(A) =
n

i=1
aii.
We call the determinant of A the scalar deÔ¨Åned through the following for-
mula
det(A) =

œÄ‚ààP
sign(œÄ)a1œÄ1a2œÄ2 . . . anœÄn,
where P =

œÄ = (œÄ1, . . . , œÄn)T 
is the set of the n! vectors that are ob-
tained by permuting the index vector i = (1, . . . , n)T and sign(œÄ) equal to
1 (respectively, ‚àí1) if an even (respectively, odd) number of exchanges is
needed to obtain œÄ from i.
The following properties hold
det(A) = det(AT ),
det(AB) = det(A)det(B),
det(A‚àí1) = 1/det(A),
det(AH) = det(A),
det(Œ±A) = Œ±ndet(A), ‚àÄŒ± ‚ààK.
Moreover, if two rows or columns of a matrix coincide, the determinant
vanishes, while exchanging two rows (or two columns) produces a change

1.5 Rank and Kernel of a Matrix
9
of sign in the determinant. Of course, the determinant of a diagonal matrix
is the product of the diagonal entries.
Denoting by Aij the matrix of order n ‚àí1 obtained from A by elimi-
nating the i-th row and the j-th column, we call the complementary minor
associated with the entry aij the determinant of the matrix Aij. We call
the k-th principal (dominating) minor of A, dk, the determinant of the
principal submatrix of order k, Ak = A(1 : k, 1 : k). If we denote by
‚àÜij = (‚àí1)i+jdet(Aij) the cofactor of the entry aij, the actual computa-
tion of the determinant of A can be performed using the following recursive
relation
det(A) =
Ô£±
Ô£¥
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£¥
Ô£≥
a11
if n = 1,
n

j=1
‚àÜijaij,
for n > 1,
(1.4)
which is known as the Laplace rule. If A is a square invertible matrix of
order n, then
A‚àí1 =
1
det(A)C
where C is the matrix having entries ‚àÜji, i, j = 1, . . . , n.
As a consequence, a square matrix is invertible iÔ¨Äits determinant is non-
vanishing. In the case of nonsingular diagonal matrices the inverse is still
a diagonal matrix having entries given by the reciprocals of the diagonal
entries of the matrix.
Every orthogonal matrix is invertible, its inverse is given by AT , moreover
det(A) = ¬±1.
1.5
Rank and Kernel of a Matrix
Let A be a rectangular matrix m √ó n. We call the determinant of order
q (with q ‚â•1) extracted from matrix A, the determinant of any square
matrix of order q obtained from A by eliminating m ‚àíq rows and n ‚àíq
columns.
DeÔ¨Ånition 1.12 The rank of A (denoted by rank(A)) is the maximum
order of the nonvanishing determinants extracted from A. A matrix has
complete or full rank if rank(A) = min(m,n).
‚ñ†
Notice that the rank of A represents the maximum number of linearly
independent column vectors of A that is, the dimension of the range of A,
deÔ¨Åned as
range(A) = {y ‚ààRm : y = Ax for x ‚ààRn} .
(1.5)

10
1. Foundations of Matrix Analysis
Rigorously speaking, one should distinguish between the column rank of A
and the row rank of A, the latter being the maximum number of linearly
independent row vectors of A. Nevertheless, it can be shown that the row
rank and column rank do actually coincide.
The kernel of A is deÔ¨Åned as the subspace
ker(A) = {x ‚ààRn : Ax = 0} .
The following relations hold
1.
rank(A) = rank(AT )
(if A ‚ààCm√ón, rank(A) = rank(AH))
2.
rank(A) + dim(ker(A)) = n.
In general, dim(ker(A)) Ã∏= dim(ker(AT )). If A is a nonsingular square ma-
trix, then rank(A) = n and dim(ker(A)) = 0.
Example 1.5 Let
A =
 1
1
0
1
‚àí1
1

.
Then, rank(A) = 2, dim(ker(A)) = 1 and dim(ker(AT )) = 0.
‚Ä¢
We Ô¨Ånally notice that for a matrix A ‚ààCn√ón the following properties are
equivalent:
1. A is nonsingular;
2. det(A) Ã∏= 0;
3. ker(A) = {0};
4. rank(A) = n;
5. A has linearly independent rows and columns.
1.6
Special Matrices
1.6.1
Block Diagonal Matrices
These are matrices of the form D = diag(D1, . . . , Dn), where Di are square
matrices with i = 1, . . . , n. Clearly, each single diagonal block can be of
diÔ¨Äerent size. We shall say that a block diagonal matrix has size n if n
is the number of its diagonal blocks. The determinant of a block diagonal
matrix is given by the product of the determinants of the single diagonal
blocks.

1.6 Special Matrices
11
1.6.2
Trapezoidal and Triangular Matrices
A matrix A(m √ó n) is called upper trapezoidal if aij = 0 for i > j, while it
is lower trapezoidal if aij = 0 for i < j. The name is due to the fact that,
in the case of upper trapezoidal matrices, with m < n, the nonzero entries
of the matrix form a trapezoid.
A triangular matrix is a square trapezoidal matrix of order n of the form
L =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
l11
0
. . .
0
l21
l22
. . .
0
...
...
...
ln1
ln2
. . .
lnn
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª
or
U =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
u11
u12
. . .
u1n
0
u22
. . .
u2n
...
...
...
0
0
. . .
unn
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª.
The matrix L is called lower triangular while U is upper triangular.
Let us recall some algebraic properties of triangular matrices that are easy
to check.
- The determinant of a triangular matrix is the product of the diagonal
entries;
- the inverse of a lower (respectively, upper) triangular matrix is still lower
(respectively, upper) triangular;
- the product of two lower triangular (respectively, upper trapezoidal) ma-
trices is still lower triangular (respectively, upper trapezodial);
- if we call unit triangular matrix a triangular matrix that has diagonal
entries equal to 1, then, the product of lower (respectively, upper) unit
triangular matrices is still lower (respectively, upper) unit triangular.
1.6.3
Banded Matrices
The matrices introduced in the previous section are a special instance of
banded matrices. Indeed, we say that a matrix A ‚ààRm√ón (or in Cm√ón)
has lower band p if aij = 0 when i > j + p and upper band q if aij = 0
when j > i+q. Diagonal matrices are banded matrices for which p = q = 0,
while trapezoidal matrices have p = m‚àí1, q = 0 (lower trapezoidal), p = 0,
q = n ‚àí1 (upper trapezoidal).
Other banded matrices of relevant interest are the tridiagonal matrices
for which p = q = 1 and the upper bidiagonal (p = 0, q = 1) or lower bidiag-
onal (p = 1, q = 0). In the following, tridiagn(b, d, c) will denote the triadi-
agonal matrix of size n having respectively on the lower and upper principal
diagonals the vectors b = (b1, . . . , bn‚àí1)T and c = (c1, . . . , cn‚àí1)T , and on
the principal diagonal the vector d = (d1, . . . , dn)T . If bi = Œ≤, di = Œ¥ and
ci = Œ≥, Œ≤, Œ¥ and Œ≥ being given constants, the matrix will be denoted by
tridiagn(Œ≤, Œ¥, Œ≥).

12
1. Foundations of Matrix Analysis
We also mention the so-called lower Hessenberg matrices (p = m ‚àí1,
q = 1) and upper Hessenberg matrices (p = 1, q = n ‚àí1) that have the
following structure
H =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£∞
h11
h12
0
h21
h22
...
...
...
hm‚àí1n
hm1
. . .
. . .
hmn
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
or H =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£∞
h11
h12
. . .
h1n
h21
h22
h2n
...
...
...
0
hmn‚àí1
hmn
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
Matrices of similar shape can obviously be set up in the block-like format.
1.7
Eigenvalues and Eigenvectors
Let A be a square matrix of order n with real or complex entries; the number
Œª ‚ààC is called an eigenvalue of A if there exists a nonnull vector x ‚ààCn
such that Ax = Œªx. The vector x is the eigenvector associated with the
eigenvalue Œª and the set of the eigenvalues of A is called the spectrum of A,
denoted by œÉ(A). We say that x and y are respectively a right eigenvector
and a left eigenvector of A, associated with the eigenvalue Œª, if
Ax = Œªx,
yHA = ŒªyH.
The eigenvalue Œª corresponding to the eigenvector x can be determined by
computing the Rayleigh quotient Œª = xHAx/(xHx). The number Œª is the
solution of the characteristic equation
pA(Œª) = det(A ‚àíŒªI) = 0,
where pA(Œª) is the characteristic polynomial. Since this latter is a polyno-
mial of degree n with respect to Œª, there certainly exist n eigenvalues of A
not necessarily distinct. The following properties can be proved
det(A) =
n

i=1
Œªi,
tr(A) =
n

i=1
Œªi,
(1.6)
and since det(AT ‚àíŒªI) = det((A ‚àíŒªI)T ) = det(A ‚àíŒªI) one concludes that
œÉ(A) = œÉ(AT ) and, in an analogous way, that œÉ(AH) = œÉ(¬ØA).
From the Ô¨Årst relation in (1.6) it can be concluded that a matrix is
singular iÔ¨Äit has at least one null eigenvalue, since pA(0) = det(A) =
Œ†n
i=1Œªi.
Secondly, if A has real entries, pA(Œª) turns out to be a real-coeÔ¨Écient
polynomial so that complex eigenvalues of A shall necessarily occur in com-
plex conjugate pairs.

1.7 Eigenvalues and Eigenvectors
13
Finally, due to the Cayley-Hamilton Theorem if pA(Œª) is the charac-
teristic polynomial of A, then pA(A) = 0, where pA(A) denotes a matrix
polynomial (for the proof see, e.g., [Axe94], p. 51).
The maximum module of the eigenvalues of A is called the spectral radius
of A and is denoted by
œÅ(A) = max
Œª‚ààœÉ(A)|Œª|.
(1.7)
Characterizing the eigenvalues of a matrix as the roots of a polynomial
implies in particular that Œª is an eigenvalue of A ‚ààCn√ón iÔ¨Ä¬ØŒª is an eigen-
value of AH. An immediate consequence is that œÅ(A) = œÅ(AH). Moreover,
‚àÄA ‚ààCn√ón, ‚àÄŒ± ‚ààC, œÅ(Œ±A) = |Œ±|œÅ(A), and œÅ(Ak) = [œÅ(A)]k ‚àÄk ‚ààN.
Finally, assume that A is a block triangular matrix
A =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
A11
A12
. . .
A1k
0
A22
. . .
A2k
...
...
...
0
. . .
0
Akk
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª.
As pA(Œª) = pA11(Œª)pA22(Œª) ¬∑ ¬∑ ¬∑ pAkk (Œª), the spectrum of A is given by the
union of the spectra of each single diagonal block. As a consequence, if A
is triangular, the eigenvalues of A are its diagonal entries.
For each eigenvalue Œª of a matrix A the set of the eigenvectors associated
with Œª, together with the null vector, identiÔ¨Åes a subspace of Cn which is
called the eigenspace associated with Œª and corresponds by deÔ¨Ånition to
ker(A-ŒªI). The dimension of the eigenspace is
dim [ker(A ‚àíŒªI)] = n ‚àírank(A ‚àíŒªI),
and is called geometric multiplicity of the eigenvalue Œª. It can never be
greater than the algebraic multiplicity of Œª, which is the multiplicity of
Œª as a root of the characteristic polynomial. Eigenvalues having geometric
multiplicity strictly less than the algebraic one are called defective. A matrix
having at least one defective eigenvalue is called defective.
The eigenspace associated with an eigenvalue of a matrix A is invariant
with respect to A in the sense of the following deÔ¨Ånition.
DeÔ¨Ånition 1.13 A subspace S in Cn is called invariant with respect to a
square matrix A if AS ‚äÇS, where AS is the transformed of S through A.
‚ñ†

14
1. Foundations of Matrix Analysis
1.8
Similarity Transformations
DeÔ¨Ånition 1.14 Let C be a square nonsingular matrix having the same
order as the matrix A. We say that the matrices A and C‚àí1AC are similar,
and the transformation from A to C‚àí1AC is called a similarity transfor-
mation. Moreover, we say that the two matrices are unitarily similar if C
is unitary.
‚ñ†
Two similar matrices share the same spectrum and the same characteris-
tic polynomial. Indeed, it is easy to check that if (Œª, x) is an eigenvalue-
eigenvector pair of A, (Œª, C‚àí1x) is the same for the matrix C‚àí1AC since
(C‚àí1AC)C‚àí1x = C‚àí1Ax = ŒªC‚àí1x.
We notice in particular that the product matrices AB and BA, with A ‚àà
Cn√óm and B ‚ààCm√ón, are not similar but satisfy the following property
(see [Hac94], p.18, Theorem 2.4.6)
œÉ(AB)\ {0} = œÉ(BA)\ {0}
that is, AB and BA share the same spectrum apart from null eigenvalues
so that œÅ(AB) = œÅ(BA).
The use of similarity transformations aims at reducing the complexity
of the problem of evaluating the eigenvalues of a matrix. Indeed, if a given
matrix could be transformed into a similar matrix in diagonal or triangular
form, the computation of the eigenvalues would be immediate. The main
result in this direction is the following theorem (for the proof, see [Dem97],
Theorem 4.2).
Property 1.5 (Schur decomposition) Given A‚ààCn√ón, there exists U
unitary such that
U‚àí1AU = UHAU =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
Œª1
b12
. . .
b1n
0
Œª2
b2n
...
...
...
0
. . .
0
Œªn
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª= T,
where Œªi are the eigenvalues of A.
It thus turns out that every matrix A is unitarily similar to an upper
triangular matrix. The matrices T and U are not necessarily unique [Hac94].
The Schur decomposition theorem gives rise to several important results;
among them, we recall:
1. every hermitian matrix is unitarily similar to a diagonal real ma-
trix, that is, when A is hermitian every Schur decomposition of A is
diagonal. In such an event, since
U‚àí1AU = Œõ = diag(Œª1, . . . , Œªn),

1.8 Similarity Transformations
15
it turns out that AU = UŒõ, that is, Aui = Œªiui for i = 1, . . . , n so
that the column vectors of U are the eigenvectors of A. Moreover,
since the eigenvectors are orthogonal two by two, it turns out that
an hermitian matrix has a system of orthonormal eigenvectors that
generates the whole space Cn. Finally, it can be shown that a matrix
A of order n is similar to a diagonal matrix D iÔ¨Äthe eigenvectors of
A form a basis for Cn [Axe94];
2. a matrix A ‚ààCn√ón is normal iÔ¨Äit is unitarily similar to a diagonal
matrix. As a consequence, a normal matrix A ‚ààCn√ón admits the
following spectral decomposition: A = UŒõUH = n
i=1 ŒªiuiuH
i
being
U unitary and Œõ diagonal [SS90];
3. let A and B be two normal and commutative matrices; then, the
generic eigenvalue ¬µi of A+B is given by the sum Œªi + Œæi, where
Œªi and Œæi are the eigenvalues of A and B associated with the same
eigenvector.
There are, of course, nonsymmetric matrices that are similar to diagonal
matrices, but these are not unitarily similar (see, e.g., Exercise 7).
The Schur decomposition can be improved as follows (for the proof see,
e.g., [Str80], [God66]).
Property 1.6 (Canonical Jordan Form) Let A be any square matrix.
Then, there exists a nonsingular matrix X which transforms A into a block
diagonal matrix J such that
X‚àí1AX = J = diag (Jk1(Œª1), Jk2(Œª2), . . . , Jkl(Œªl)) ,
which is called canonical Jordan form, Œªj being the eigenvalues of A and
Jk(Œª) ‚ààCk√ók a Jordan block of the form J1(Œª) = Œª if k = 1 and
Jk(Œª) =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
Œª
1
0
. . .
0
0
Œª
1
¬∑ ¬∑ ¬∑
...
...
...
...
1
0
...
...
Œª
1
0
. . .
. . .
0
Œª
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
,
for k > 1.
If an eigenvalue is defective, the size of the corresponding Jordan block
is greater than one. Therefore, the canonical Jordan form tells us that a
matrix can be diagonalized by a similarity transformation iÔ¨Äit is nonde-
fective. For this reason, the nondefective matrices are called diagonalizable.
In particular, normal matrices are diagonalizable.

16
1. Foundations of Matrix Analysis
Partitioning X by columns, X = (x1, . . . , xn), it can be seen that the
ki vectors associated with the Jordan block Jki(Œªi) satisfy the following
recursive relation
Axl = Œªixl,
l =
i‚àí1

j=1
mj + 1,
Axj = Œªixj + xj‚àí1,
j = l + 1, . . . , l ‚àí1 + ki, if ki Ã∏= 1.
(1.8)
The vectors xi are called principal vectors or generalized eigenvectors of A.
Example 1.6 Let us consider the following matrix
A =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
7/4
3/4
‚àí1/4
‚àí1/4
‚àí1/4
1/4
0
2
0
0
0
0
‚àí1/2
‚àí1/2
5/2
1/2
‚àí1/2
1/2
‚àí1/2
‚àí1/2
‚àí1/2
5/2
1/2
1/2
‚àí1/4
‚àí1/4
‚àí1/4
‚àí1/4
11/4
1/4
‚àí3/2
‚àí1/2
‚àí1/2
1/2
1/2
7/2
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
The Jordan canonical form of A and its associated matrix X are given by
J =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
2
1
0
0
0
0
0
2
0
0
0
0
0
0
3
1
0
0
0
0
0
3
1
0
0
0
0
0
3
0
0
0
0
0
0
2
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
,
X =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
1
0
0
0
0
1
0
1
0
0
0
1
0
0
1
0
0
1
0
0
0
1
0
1
0
0
0
0
1
1
1
1
1
1
1
1
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
Notice that two diÔ¨Äerent Jordan blocks are related to the same eigenvalue (Œª =
2). It is easy to check property (1.8). Consider, for example, the Jordan block
associated with the eigenvalue Œª2 = 3; we have
Ax3 = [0 0 3 0 0 3]T = 3 [0 0 1 0 0 1]T = Œª2x3,
Ax4 = [0 0 1 3 0 4]T = 3 [0 0 0 1 0 1]T + [0 0 1 0 0 1]T = Œª2x4 + x3,
Ax5 = [0 0 0 1 3 4]T = 3 [0 0 0 0 1 1]T + [0 0 0 1 0 1]T = Œª2x5 + x4.
‚Ä¢
1.9
The Singular Value Decomposition (SVD)
Any matrix can be reduced in diagonal form by a suitable pre and post-
multiplication by unitary matrices. Precisely, the following result holds.
Property 1.7 Let A‚ààCm√ón. There exist two unitary matrices U‚ààCm√óm
and V‚ààCn√ón such that
UHAV = Œ£ = diag(œÉ1, . . . , œÉp) ‚ààCm√ón
with p = min(m, n)
(1.9)
and œÉ1 ‚â•. . . ‚â•œÉp ‚â•0. Formula (1.9) is called Singular Value Decompo-
sition or (SVD) of A and the numbers œÉi (or œÉi(A)) are called singular
values of A.

1.10 Scalar Product and Norms in Vector Spaces
17
If A is a real-valued matrix, U and V will also be real-valued and in (1.9)
UT must be written instead of UH. The following characterization of the
singular values holds
œÉi(A) =

Œªi(AHA),
i = 1, . . . , n.
(1.10)
Indeed, from (1.9) it follows that A = UŒ£VH, AH = VŒ£UH so that, U and
V being unitary, AHA = VŒ£2VH, that is, Œªi(AHA) = Œªi(Œ£2) = (œÉi(A))2.
Since AAH and AHA are hermitian matrices, the columns of U, called the
left singular vectors of A, turn out to be the eigenvectors of AAH (see
Section 1.8) and, therefore, they are not uniquely deÔ¨Åned. The same holds
for the columns of V, which are the right singular vectors of A.
Relation (1.10) implies that if A ‚ààCn√ón is hermitian with eigenvalues given
by Œª1, Œª2, . . . , Œªn, then the singular values of A coincide with the modules
of the eigenvalues of A. Indeed because AAH = A2, œÉi =

Œª2
i = |Œªi| for
i = 1, . . . , n. As far as the rank is concerned, if
œÉ1 ‚â•. . . ‚â•œÉr > œÉr+1 = . . . = œÉp = 0,
then the rank of A is r, the kernel of A is the span of the column vectors
of V, {vr+1, . . . , vn}, and the range of A is the span of the column vectors
of U, {u1, . . . , ur}.
DeÔ¨Ånition 1.15 Suppose that A‚ààCm√ón has rank equal to r and that it
admits a SVD of the type UHAV = Œ£. The matrix A‚Ä† = VŒ£‚Ä†UH is called
the Moore-Penrose pseudo-inverse matrix, being
Œ£‚Ä† = diag
 1
œÉ1
, . . . , 1
œÉr
, 0, . . . , 0

.
(1.11)
‚ñ†
The matrix A‚Ä† is also called the generalized inverse of A (see Exercise 13).
Indeed, if rank(A) = n < m, then A‚Ä† = (AT A)‚àí1AT , while if n = m =
rank(A), A‚Ä† = A‚àí1. For further properties of A‚Ä†, see also Exercise 12.
1.10
Scalar Product and Norms in Vector Spaces
Very often, to quantify errors or measure distances one needs to compute
the magnitude of a vector or a matrix. For that purpose we introduce in
this section the concept of a vector norm and, in the following one, of a
matrix norm. We refer the reader to [Ste73], [SS90] and [Axe94] for the
proofs of the properties that are reported hereafter.

18
1. Foundations of Matrix Analysis
DeÔ¨Ånition 1.16 A scalar product on a vector space V deÔ¨Åned over K
is any map (¬∑, ¬∑) acting from V √ó V into K which enjoys the following
properties:
1. it is linear with respect to the vectors of V, that is
(Œ≥x + Œªz, y) = Œ≥(x, y) + Œª(z, y), ‚àÄx, z ‚ààV, ‚àÄŒ≥, Œª ‚ààK;
2. it is hermitian, that is, (y, x) = (x, y), ‚àÄx, y ‚ààV ;
3. it is positive deÔ¨Ånite, that is, (x, x) > 0, ‚àÄx Ã∏= 0 (in other words,
(x, x) ‚â•0, and (x, x) = 0 if and only if x = 0).
‚ñ†
In the case V = Cn (or Rn), an example is provided by the classical Eu-
clidean scalar product given by
(x, y) = yHx =
n

i=1
xi¬Øyi,
where ¬Øz denotes the complex conjugate of z.
Moreover, for any given square matrix A of order n and for any x, y‚ààCn
the following relation holds
(Ax, y) = (x, AHy).
(1.12)
In particular, since for any matrix Q ‚ààCn√ón, (Qx, Qy) = (x, QHQy), one
gets
Property 1.8 Unitary matrices preserve the Euclidean scalar product, that
is, (Qx, Qy) = (x, y) for any unitary matrix Q and for any pair of vectors
x and y.
DeÔ¨Ånition 1.17 Let V be a vector space over K. We say that the map
‚à•¬∑ ‚à•from V into R is a norm on V if the following axioms are satisÔ¨Åed:
1. (i) ‚à•v‚à•‚â•0 ‚àÄv ‚ààV and (ii) ‚à•v‚à•= 0 if and only if v = 0;
2. ‚à•Œ±v‚à•= |Œ±|‚à•v‚à•‚àÄŒ± ‚ààK, ‚àÄv ‚ààV (homogeneity property);
3. ‚à•v + w‚à•‚â§‚à•v‚à•+ ‚à•w‚à•‚àÄv, w ‚ààV (triangular inequality),
where |Œ±| denotes the absolute value of Œ± if K = R, the module of Œ± if
K = C.
‚ñ†

1.10 Scalar Product and Norms in Vector Spaces
19
The pair (V, ‚à•¬∑ ‚à•) is called a normed space. We shall distinguish among
norms by a suitable subscript at the margin of the double bar symbol. In
the case the map | ¬∑ | from V into R enjoys only the properties 1(i), 2 and
3 we shall call such a map a seminorm. Finally, we shall call a unit vector
any vector of V having unit norm.
An example of a normed space is Rn, equipped for instance by the p-norm
(or H¬®older norm); this latter is deÔ¨Åned for a vector x of components {xi}
as
‚à•x‚à•p =
 n

i=1
|xi|p
1/p
,
for 1 ‚â§p < ‚àû.
(1.13)
Notice that the limit as p goes to inÔ¨Ånity of ‚à•x‚à•p exists, is Ô¨Ånite, and equals
the maximum module of the components of x. Such a limit deÔ¨Ånes in turn
a norm, called the inÔ¨Ånity norm (or maximum norm), given by
‚à•x‚à•‚àû= max
1‚â§i‚â§n|xi|.
When p = 2, from (1.13) the standard deÔ¨Ånition of Euclidean norm is
recovered
‚à•x‚à•2 = (x, x)1/2 =
 n

i=1
|xi|2
1/2
=

xT x
1/2 ,
for which the following property holds.
Property 1.9 (Cauchy-Schwarz inequality) For any pair x, y ‚ààRn,
|(x, y)| = |xT y| ‚â§‚à•x‚à•2 ‚à•y‚à•2,
(1.14)
where strict equality holds iÔ¨Äy = Œ±x for some Œ± ‚ààR.
We recall that the scalar product in Rn can be related to the p-norms
introduced over Rn in (1.13) by the H¬®older inequality
|(x, y)| ‚â§‚à•x‚à•p‚à•y‚à•q,
with 1
p + 1
q = 1.
In the case where V is a Ô¨Ånite-dimensional space the following property
holds (for a sketch of the proof, see Exercise 14).
Property 1.10 Any vector norm ‚à•¬∑‚à•deÔ¨Åned on V is a continuous function
of its argument, namely, ‚àÄŒµ > 0, ‚àÉC > 0 such that if ‚à•x ‚àíx‚à•‚â§Œµ then
| ‚à•x‚à•‚àí‚à•x‚à•| ‚â§CŒµ, for any x, x ‚ààV .
New norms can be easily built using the following result.

20
1. Foundations of Matrix Analysis
Property 1.11 Let ‚à•¬∑ ‚à•be a norm of Rn and A ‚ààRn√ón be a matrix with
n linearly independent columns. Then, the function ‚à•¬∑ ‚à•A2 acting from Rn
into R deÔ¨Åned as
‚à•x‚à•A2 = ‚à•Ax‚à•
‚àÄx ‚ààRn,
is a norm of Rn.
Two vectors x, y in V are said to be orthogonal if (x, y) = 0. This statement
has an immediate geometric interpretation when V = R2 since in such a
case
(x, y) = ‚à•x‚à•2‚à•y‚à•2 cos(œë),
where œë is the angle between the vectors x and y. As a consequence, if
(x, y) = 0 then œë is a right angle and the two vectors are orthogonal in the
geometric sense.
DeÔ¨Ånition 1.18 Two norms ‚à•¬∑ ‚à•p and ‚à•¬∑ ‚à•q on V are equivalent if there
exist two positive constants cpq and Cpq such that
cpq‚à•x‚à•q ‚â§‚à•x‚à•p ‚â§Cpq‚à•x‚à•q
‚àÄx ‚ààV.
‚ñ†
In a Ô¨Ånite-dimensional normed space all norms are equivalent. In particular,
if V = Rn it can be shown that for the p-norms, with p = 1, 2, and ‚àû, the
constants cpq and Cpq take the value reported in Table 1.1.
cpq
q = 1
q = 2
q = ‚àû
p = 1
1
1
1
p = 2
n‚àí1/2
1
1
p = ‚àû
n‚àí1
n‚àí1/2
1
Cpq
q = 1
q = 2
q = ‚àû
p = 1
1
n1/2
n
p = 2
1
1
n1/2
p = ‚àû
1
1
1
TABLE 1.1. Equivalence constants for the main norms of Rn
In this book we shall often deal with sequences of vectors and with their
convergence. For this purpose, we recall that a sequence of vectors

x(k)
in a vector space V having Ô¨Ånite dimension n, converges to a vector x, and
we write lim
k‚Üí‚àûx(k) = x if
lim
k‚Üí‚àûx(k)
i
= xi,
i = 1, . . . , n
(1.15)
where x(k)
i
and xi are the components of the corresponding vectors with
respect to a basis of V . If V = Rn, due to the uniqueness of the limit of a

1.11 Matrix Norms
21
sequence of real numbers, (1.15) implies also the uniqueness of the limit, if
existing, of a sequence of vectors.
We further notice that in a Ô¨Ånite-dimensional space all the norms are topo-
logically equivalent in the sense of convergence, namely, given a sequence
of vectors x(k),
|||x(k)||| ‚Üí0 ‚áî‚à•x(k)‚à•‚Üí0 if k ‚Üí‚àû,
where ||| ¬∑ ||| and ‚à•¬∑ ‚à•are any two vector norms. As a consequence, we can
establish the following link between norms and limits.
Property 1.12 Let ‚à•¬∑ ‚à•be a norm in a space Ô¨Ånite dimensional space V .
Then
lim
k‚Üí‚àûx(k) = x
‚áî
lim
k‚Üí‚àû‚à•x ‚àíx(k)‚à•= 0,
where x ‚ààV and

x(k)
is a sequence of elements of V .
1.11
Matrix Norms
DeÔ¨Ånition 1.19 A matrix norm is a mapping ‚à•¬∑‚à•: Rm√ón ‚ÜíR such that:
1. ‚à•A‚à•‚â•0 ‚àÄA ‚ààRm√ón and ‚à•A‚à•= 0 if and only if A = 0;
2. ‚à•Œ±A‚à•= |Œ±|‚à•A‚à•‚àÄŒ± ‚ààR, ‚àÄA ‚ààRm√ón (homogeneity);
3. ‚à•A + B‚à•‚â§‚à•A‚à•+ ‚à•B‚à•‚àÄA, B ‚ààRm√ón (triangular inequality).
‚ñ†
Unless otherwise speciÔ¨Åed we shall employ the same symbol ‚à•¬∑‚à•, to denote
matrix norms and vector norms.
We can better characterize the matrix norms by introducing the concepts
of compatible norm and norm induced by a vector norm.
DeÔ¨Ånition 1.20 We say that a matrix norm ‚à•¬∑‚à•is compatible or consistent
with a vector norm ‚à•¬∑ ‚à•if
‚à•Ax‚à•‚â§‚à•A‚à•‚à•x‚à•,
‚àÄx ‚ààRn.
(1.16)
More generally, given three norms, all denoted by ‚à•¬∑ ‚à•, albeit deÔ¨Åned on
Rm, Rn and Rm√ón, respectively, we say that they are consistent if ‚àÄx ‚ààRn,
Ax = y ‚ààRm, A ‚ààRm√ón, we have that ‚à•y‚à•‚â§‚à•A‚à•‚à•x‚à•.
‚ñ†
In order to single out matrix norms of practical interest, the following
property is in general required

22
1. Foundations of Matrix Analysis
DeÔ¨Ånition 1.21 We say that a matrix norm ‚à•¬∑ ‚à•is sub-multiplicative if
‚àÄA ‚ààRn√óm, ‚àÄB ‚ààRm√óq
‚à•AB‚à•‚â§‚à•A‚à•‚à•B‚à•.
(1.17)
‚ñ†
This property is not satisÔ¨Åed by any matrix norm. For example (taken from
[GL89]), the norm ‚à•A‚à•‚àÜ= max |aij| for i = 1, . . . , n, j = 1, . . . , m does
not satisfy (1.17) if applied to the matrices
A = B =
 1
1
1
1

,
since 2 = ‚à•AB‚à•‚àÜ> ‚à•A‚à•‚àÜ‚à•B‚à•‚àÜ= 1.
Notice that, given a certain sub-multiplicative matrix norm ‚à•¬∑ ‚à•Œ±, there
always exists a consistent vector norm. For instance, given any Ô¨Åxed vector
y Ã∏= 0 in Cn, it suÔ¨Éces to deÔ¨Åne the consistent vector norm as
‚à•x‚à•= ‚à•xyH‚à•Œ±
x ‚ààCn.
As a consequence, in the case of sub-multiplicative matrix norms it is no
longer necessary to explicitly specify the vector norm with respect to the
matrix norm is consistent.
Example 1.7 The norm
‚à•A‚à•F =




n

i,j=1
|aij|2 = tr(AAH)
(1.18)
is a matrix norm called the Frobenius norm (or Euclidean norm in Cn2) and is
compatible with the Euclidean vector norm ‚à•¬∑ ‚à•2. Indeed,
‚à•Ax‚à•2
2 =
n

i=1

n

j=1
aijxj

2
‚â§
n

i=1
 n

j=1
|aij|2
n

j=1
|xj|2

= ‚à•A‚à•2
F ‚à•x‚à•2
2.
Notice that for such a norm ‚à•In‚à•F = ‚àön.
‚Ä¢
In view of the deÔ¨Ånition of a natural norm, we recall the following theorem.
Theorem 1.1 Let ‚à•¬∑‚à•be a vector norm. The function
‚à•A‚à•= sup
xÃ∏=0
‚à•Ax‚à•
‚à•x‚à•
(1.19)
is a matrix norm called induced matrix norm or natural matrix norm.

1.11 Matrix Norms
23
Proof. We start by noticing that (1.19) is equivalent to
‚à•A‚à•= sup
‚à•x‚à•=1
‚à•Ax‚à•.
(1.20)
Indeed, one can deÔ¨Åne for any x Ã∏= 0 the unit vector u = x/‚à•x‚à•, so that (1.19)
becomes
‚à•A‚à•= sup
‚à•u‚à•=1
‚à•Au‚à•= ‚à•Aw‚à•
with ‚à•w‚à•= 1.
This being taken as given, let us check that (1.19) (or, equivalently, (1.20)) is
actually a norm, making direct use of DeÔ¨Ånition 1.19.
1. If ‚à•Ax‚à•‚â•0, then it follows that ‚à•A‚à•= sup
‚à•x‚à•=1
‚à•Ax‚à•‚â•0. Moreover
‚à•A‚à•= sup
xÃ∏=0
‚à•Ax‚à•
‚à•x‚à•
= 0 ‚áî‚à•Ax‚à•= 0 ‚àÄx Ã∏= 0
and Ax = 0 ‚àÄx Ã∏= 0 if and only if A=0; therefore ‚à•A‚à•= 0 ‚áîA = 0.
2. Given a scalar Œ±,
‚à•Œ±A‚à•= sup
‚à•x‚à•=1
‚à•Œ±Ax‚à•= |Œ±| sup
‚à•x‚à•=1
‚à•Ax‚à•= |Œ±| ‚à•A‚à•.
3. Finally, triangular inequality holds. Indeed, by deÔ¨Ånition of supremum, if
x Ã∏= 0 then
‚à•Ax‚à•
‚à•x‚à•
‚â§‚à•A‚à•
‚áí
‚à•Ax‚à•‚â§‚à•A‚à•‚à•x‚à•,
so that, taking x with unit norm, one gets
‚à•(A + B)x‚à•‚â§‚à•Ax‚à•+ ‚à•Bx‚à•‚â§‚à•A‚à•+ ‚à•B‚à•,
from which it follows that ‚à•A + B‚à•= sup
‚à•x‚à•=1
‚à•(A + B)x‚à•‚â§‚à•A‚à•+ ‚à•B‚à•.
3
Relevant instances of induced matrix norms are the so-called p-norms de-
Ô¨Åned as
‚à•A‚à•p = sup
xÃ∏=0
‚à•Ax‚à•p
‚à•x‚à•p
The 1-norm and the inÔ¨Ånity norm are easily computable since
‚à•A‚à•1 =
max
j=1,... ,n
m

i=1
|aij|,
‚à•A‚à•‚àû=
max
i=1,... ,m
n

j=1
|aij|
and they are called the column sum norm and the row sum norm, respec-
tively.
Moreover, we have ‚à•A‚à•1 = ‚à•AT ‚à•‚àûand, if A is self-adjoint or real sym-
metric, ‚à•A‚à•1 = ‚à•A‚à•‚àû.
A special discussion is deserved by the 2-norm or spectral norm for which
the following theorem holds.

24
1. Foundations of Matrix Analysis
Theorem 1.2 Let œÉ1(A) be the largest singular value of A. Then
‚à•A‚à•2 =

œÅ(AHA) =

œÅ(AAH) = œÉ1(A).
(1.21)
In particular, if A is hermitian (or real and symmetric), then
‚à•A‚à•2 = œÅ(A),
(1.22)
while, if A is unitary, ‚à•A‚à•2 = 1.
Proof. Since AHA is hermitian, there exists a unitary matrix U such that
UHAHAU = diag(¬µ1, . . . , ¬µn),
where ¬µi are the (positive) eigenvalues of AHA. Let y = UHx, then
‚à•A‚à•2
=
sup
xÃ∏=0

(AHAx, x)
(x, x)
= sup
yÃ∏=0

(UHAHAUy, y)
(y, y)
=
sup
yÃ∏=0




n

i=1
¬µi|yi|2/
n

i=1
|yi|2 =

max
i=1,... ,n|¬µi|,
from which (1.21) follows, thanks to (1.10).
If A is hermitian, the same considerations as above apply directly to A.
Finally, if A is unitary
‚à•Ax‚à•2
2 = (Ax, Ax) = (x, AHAx) = ‚à•x‚à•2
2
so that ‚à•A‚à•2 = 1.
3
As a consequence, the computation of ‚à•A‚à•2 is much more expensive than
that of ‚à•A‚à•‚àûor ‚à•A‚à•1. However, if only an estimate of ‚à•A‚à•2 is required,
the following relations can be proÔ¨Åtably employed in the case of square
matrices
max
i,j |aij| ‚â§‚à•A‚à•2 ‚â§n max
i,j |aij|,
1
‚àön‚à•A‚à•‚àû‚â§‚à•A‚à•2 ‚â§‚àön‚à•A‚à•‚àû,
1
‚àön‚à•A‚à•1 ‚â§‚à•A‚à•2 ‚â§‚àön‚à•A‚à•1,
‚à•A‚à•2 ‚â§

‚à•A‚à•1 ‚à•A‚à•‚àû.
For other estimates of similar type we refer to Exercise 17. Moreover, if A
is normal then ‚à•A‚à•2 ‚â§‚à•A‚à•p for any n and all p ‚â•2.
Theorem 1.3 Let ||| ¬∑ ||| be a matrix norm induced by a vector norm ‚à•¬∑ ‚à•.
Then
1. ‚à•Ax‚à•‚â§|||A||| ‚à•x‚à•, that is, ||| ¬∑ ||| is a norm compatible with ‚à•¬∑ ‚à•;

1.11 Matrix Norms
25
2. |||I||| = 1;
3. |||AB||| ‚â§|||A||| |||B|||, that is, ||| ¬∑ ||| is sub-multiplicative.
Proof. Part 1 of the theorem is already contained in the proof of Theorem 1.1,
while part 2 follows from the fact that |||I||| = sup
xÃ∏=0
‚à•Ix‚à•/‚à•x‚à•= 1. Part 3 is simple
to check.
3
Notice that the p-norms are sub-multiplicative. Moreover, we remark that
the sub-multiplicativity property by itself would only allow us to conclude
that |||I||| ‚â•1. Indeed, |||I||| = |||I ¬∑ I||| ‚â§|||I|||2.
1.11.1
Relation between Norms and the Spectral Radius of a
Matrix
We next recall some results that relate the spectral radius of a matrix to
matrix norms and that will be widely employed in Chapter 4.
Theorem 1.4 Let ‚à•¬∑ ‚à•be a consistent matrix norm; then
œÅ(A) ‚â§‚à•A‚à•
‚àÄA ‚ààCn√ón.
Proof. Let Œª be an eigenvalue of A and v Ã∏= 0 an associated eigenvector. As a
consequence, since ‚à•¬∑ ‚à•is consistent, we have
|Œª| ‚à•v‚à•= ‚à•Œªv‚à•= ‚à•Av‚à•‚â§‚à•A‚à•‚à•v‚à•
so that |Œª| ‚â§‚à•A‚à•.
3
More precisely, the following property holds (see for the proof [IK66], p.
12, Theorem 3).
Property 1.13 Let A ‚ààCn√ón and Œµ > 0. Then, there exists a consistent
matrix norm ‚à•¬∑ ‚à•A,Œµ (depending on Œµ) such that
‚à•A‚à•A,Œµ ‚â§œÅ(A) + Œµ.
As a result, having Ô¨Åxed an arbitrarily small tolerance, there always exists
a matrix norm which is arbitrarily close to the spectral radius of A, namely
œÅ(A) = inf
‚à•¬∑‚à•‚à•A‚à•,
(1.23)
the inÔ¨Åmum being taken on the set of all the consistent norms.
For the sake of clarity, we notice that the spectral radius is a sub-
multiplicative seminorm, since it is not true that œÅ(A) = 0 iÔ¨ÄA = 0.
As an example, any triangular matrix with null diagonal entries clearly has
spectral radius equal to zero. Moreover, we have the following result.

26
1. Foundations of Matrix Analysis
Property 1.14 Let A be a square matrix and let ‚à•¬∑‚à•be a consistent norm.
Then
lim
m‚Üí‚àû‚à•Am‚à•1/m = œÅ(A).
1.11.2
Sequences and Series of Matrices
A sequence of matrices

A(k)
‚ààRn√ón is said to converge to a matrix
A ‚ààRn√ón if
lim
k‚Üí‚àû‚à•A(k) ‚àíA‚à•= 0.
The choice of the norm does not inÔ¨Çuence the result since in Rn√ón all norms
are equivalent. In particular, when studying the convergence of iterative
methods for solving linear systems (see Chapter 4), one is interested in the
so-called convergent matrices for which
lim
k‚Üí‚àûAk = 0,
0 being the null matrix. The following theorem holds.
Theorem 1.5 Let A be a square matrix; then
lim
k‚Üí‚àûAk = 0 ‚áîœÅ(A) < 1.
(1.24)
Moreover, the geometric series
‚àû

k=0
Ak is convergent iÔ¨ÄœÅ(A) < 1. In such a
case
‚àû

k=0
Ak = (I ‚àíA)‚àí1.
(1.25)
As a result, if œÅ(A) < 1 the matrix I ‚àíA is invertible and the following
inequalities hold
1
1 + ‚à•A‚à•‚â§‚à•(I ‚àíA)‚àí1‚à•‚â§
1
1 ‚àí‚à•A‚à•
(1.26)
where ‚à•¬∑ ‚à•is an induced matrix norm such that ‚à•A‚à•< 1.
Proof. Let us prove (1.24). Let œÅ(A) < 1, then ‚àÉŒµ > 0 such that œÅ(A) < 1 ‚àíŒµ
and thus, thanks to Property 1.13, there exists a consistent matrix norm ‚à•¬∑‚à•such
that ‚à•A‚à•‚â§œÅ(A) + Œµ < 1. From the fact that ‚à•Ak‚à•‚â§‚à•A‚à•k < 1 and from the
deÔ¨Ånition of convergence it turns out that as k ‚Üí‚àûthe sequence

Ak
tends
to zero. Conversely, assume that lim
k‚Üí‚àûAk = 0 and let Œª denote an eigenvalue of
A. Then, Akx = Œªkx, being x(Ã∏=0) an eigenvector associated with Œª, so that

1.12 Positive DeÔ¨Ånite, Diagonally Dominant and M-matrices
27
lim
k‚Üí‚àûŒªk = 0. As a consequence, |Œª| < 1 and because this is true for a generic
eigenvalue one gets œÅ(A) < 1 as desired. Relation (1.25) can be obtained noting
Ô¨Årst that the eigenvalues of I‚àíA are given by 1 ‚àíŒª(A), Œª(A) being the generic
eigenvalue of A. On the other hand, since œÅ(A) < 1, we deduce that I‚àíA is
nonsingular. Then, from the identity
(I ‚àíA)(I + A + . . . + An) = (I ‚àíAn+1)
and taking the limit for n tending to inÔ¨Ånity the thesis follows since
(I ‚àíA)
‚àû

k=0
Ak = I.
Finally, thanks to Theorem 1.3, the equality ‚à•I‚à•= 1 holds, so that
1 = ‚à•I‚à•‚â§‚à•I ‚àíA‚à•‚à•(I ‚àíA)‚àí1‚à•‚â§(1 + ‚à•A‚à•) ‚à•(I ‚àíA)‚àí1‚à•,
giving the Ô¨Årst inequality in (1.26). As for the second part, noting that I =
I‚àíA+A and multiplying both sides on the right by (I‚àíA)‚àí1, one gets (I‚àíA)‚àí1 =
I + A(I ‚àíA)‚àí1. Passing to the norms, we obtain
‚à•(I ‚àíA)‚àí1‚à•‚â§1 + ‚à•A‚à•‚à•(I ‚àíA)‚àí1‚à•,
and thus the second inequality, since ‚à•A‚à•< 1.
3
Remark 1.1 The assumption that there exists an induced matrix norm
such that ‚à•A‚à•< 1 is justiÔ¨Åed by Property 1.13, recalling that A is conver-
gent and, therefore, œÅ(A) < 1.
‚ñ†
Notice that (1.25) suggests an algorithm to approximate the inverse of a
matrix by a truncated series expansion.
1.12
Positive DeÔ¨Ånite, Diagonally Dominant and
M-matrices
DeÔ¨Ånition 1.22 A matrix A ‚ààCn√ón is positive deÔ¨Ånite in Cn if the num-
ber (Ax, x) is real and positive ‚àÄx ‚ààCn, x Ã∏= 0. A matrix A ‚ààRn√ón is
positive deÔ¨Ånite in Rn if (Ax, x) > 0 ‚àÄx ‚ààRn, x Ã∏= 0. If the strict in-
equality is substituted by the weak one (‚â•) the matrix is called positive
semideÔ¨Ånite.
‚ñ†
Example 1.8 Matrices that are positive deÔ¨Ånite in Rn are not necessarily sym-
metric. An instance is provided by matrices of the form
A =

2
Œ±
‚àí2 ‚àíŒ±
2

(1.27)

28
1. Foundations of Matrix Analysis
for Œ± Ã∏= ‚àí1. Indeed, for any non null vector x = (x1, x2)T in R2
(Ax, x) = 2(x2
1 + x2
2 ‚àíx1x2) > 0.
Notice that A is not positive deÔ¨Ånite in C2. Indeed, if we take a complex vector
x we Ô¨Ånd out that the number (Ax, x) is not real-valued in general.
‚Ä¢
DeÔ¨Ånition 1.23 Let A ‚ààRn√ón. The matrices
AS = 1
2(A + AT ),
ASS = 1
2(A ‚àíAT )
are respectively called the symmetric part and the skew-symmetric part
of A. Obviously, A = AS + ASS. If A ‚ààCn√ón, the deÔ¨Ånitions modify as
follows: AS = 1
2(A + AH) and ASS = 1
2(A ‚àíAH).
‚ñ†
The following property holds
Property 1.15 A real matrix A of order n is positive deÔ¨Ånite iÔ¨Äits sym-
metric part AS is positive deÔ¨Ånite.
Indeed, it suÔ¨Éces to notice that, due to (1.12) and the deÔ¨Ånition of ASS,
xT ASSx = 0 ‚àÄx ‚ààRn. For instance, the matrix in (1.27) has a positive
deÔ¨Ånite symmetric part, since
AS = 1
2(A + AT ) =

2
‚àí1
‚àí1
2

.
This holds more generally (for the proof see [Axe94]).
Property 1.16 Let A ‚ààCn√ón (respectively, A ‚ààRn√ón); if (Ax, x) is real-
valued ‚àÄx ‚ààCn, then A is hermitian (respectively, symmetric).
An immediate consequence of the above results is that matrices that are
positive deÔ¨Ånite in Cn do satisfy the following characterizing property.
Property 1.17 A square matrix A of order n is positive deÔ¨Ånite in Cn
iÔ¨Äit is hermitian and has positive eigenvalues. Thus, a positive deÔ¨Ånite
matrix is nonsingular.
In the case of positive deÔ¨Ånite real matrices in Rn, results more speciÔ¨Åc
than those presented so far hold only if the matrix is also symmetric (this is
the reason why many textbooks deal only with symmetric positive deÔ¨Ånite
matrices). In particular
Property 1.18 Let A ‚ààRn√ón be symmetric. Then, A is positive deÔ¨Ånite
iÔ¨Äone of the following properties is satisÔ¨Åed:
1. (Ax, x) > 0 ‚àÄx Ã∏= 0 with x‚ààRn;

1.12 Positive DeÔ¨Ånite, Diagonally Dominant and M-matrices
29
2. the eigenvalues of the principal submatrices of A are all positive;
3. the dominant principal minors of A are all positive (Sylvester crite-
rion);
4. there exists a nonsingular matrix H such that A = HT H.
All the diagonal entries of a positive deÔ¨Ånite matrix are positive. Indeed,
if ei is the i-th vector of the canonical basis of Rn, then eT
i Aei = aii > 0.
Moreover, it can be shown that if A is symmetric positive deÔ¨Ånite, the
entry with the largest module must be a diagonal entry (these last two
properties are therefore necessary conditions for a matrix to be positive
deÔ¨Ånite).
We Ô¨Ånally notice that if A is symmetric positive deÔ¨Ånite and A1/2 is
the only positive deÔ¨Ånite matrix that is a solution of the matrix equation
X2 = A, the norm
‚à•x‚à•A = ‚à•A1/2x‚à•2 = (Ax, x)1/2
(1.28)
deÔ¨Ånes a vector norm, called the energy norm of the vector x. Related to
the energy norm is the energy scalar product given by (x, y)A = (Ax, y).
DeÔ¨Ånition 1.24 A matrix A‚ààRn√ón is called diagonally dominant by rows
if
|aii| ‚â•
n

j=1,jÃ∏=i
|aij|, with i = 1, . . . , n,
while it is called diagonally dominant by columns if
|aii| ‚â•
n

j=1,jÃ∏=i
|aji|, with i = 1, . . . , n.
If the inequalities above hold in a strict sense, A is called strictly diagonally
dominant (by rows or by columns, respectively).
‚ñ†
A strictly diagonally dominant matrix that is symmetric with positive di-
agonal entries is also positive deÔ¨Ånite.
DeÔ¨Ånition 1.25 A nonsingular matrix A ‚ààRn√ón is an M-matrix if aij ‚â§0
for i Ã∏= j and if all the entries of its inverse are nonnegative.
‚ñ†
M-matrices enjoy the so-called discrete maximum principle, that is, if A is
an M-matrix and Ax ‚â§0, then x ‚â§0 (where the inequalities are meant
componentwise). In this connection, the following result can be useful.
Property 1.19 (M-criterion) Let a matrix A satisfy aij ‚â§0 for i Ã∏= j.
Then A is an M-matrix if and only if there exists a vector w > 0 such that
Aw > 0.

30
1. Foundations of Matrix Analysis
Finally, M-matrices are related to strictly diagonally dominant matrices
by the following property.
Property 1.20 A matrix A ‚ààRn√ón that is strictly diagonally dominant
by rows and whose entries satisfy the relations aij ‚â§0 for i Ã∏= j and aii > 0,
is an M-matrix.
For further results about M-matrices, see for instance [Axe94] and [Var62].
1.13
Exercises
1. Let W1 and W2 be two subspaces of Rn. Prove that if V = W1 ‚äïW2, then
dim(V ) = dim(W1) + dim(W2), while in general
dim(W1 + W2) = dim(W1) + dim(W2) ‚àídim(W1 ‚à©W2).
[Hint : Consider a basis for W1 ‚à©W2 and Ô¨Årst extend it to W1, then to
W2, verifying that the basis formed by the set of the obtained vectors is a
basis for the sum space.]
2. Check that the following set of vectors
vi =

xi‚àí1
1
, xi‚àí1
2
, . . . , xi‚àí1
n

,
i = 1, 2, . . . , n,
forms a basis for Rn, x1, . . . , xn being a set of n distinct points of R.
3. Exhibit an example showing that the product of two symmetric matrices
may be nonsymmetric.
4. Let B be a skew-symmetric matrix, namely, BT = ‚àíB. Let A = (I+B)(I‚àí
B)‚àí1 and show that A‚àí1 = AT .
5. A matrix A ‚ààCn√ón is called skew-hermitian if AH = ‚àíA. Show that the
diagonal entries of A must be purely imaginary numbers.
6. Let A, B and A+B be invertible matrices of order n. Show that also A‚àí1 +
B‚àí1 is nonsingular and that

A‚àí1 + B‚àí1‚àí1 = A (A + B)‚àí1 B = B (A + B)‚àí1 A.
[Solution :

A‚àí1 + B‚àí1‚àí1 = A

I + B‚àí1A
‚àí1 = A (B + A)‚àí1 B. The sec-
ond equality is proved similarly by factoring out B and A, respectively from
left and right.]
7. Given the non symmetric real matrix
A =
Ô£Æ
Ô£∞
0
1
1
1
0
‚àí1
‚àí1
‚àí1
0
Ô£π
Ô£ª,
check that it is similar to the diagonal matrix D = diag(1, 0, ‚àí1) and Ô¨Ånd
its eigenvectors. Is this matrix normal?
[Solution : the matrix is not normal.]

1.13 Exercises
31
8. Let A be a square matrix of order n. Check that if P(A) =
n

k=0
ckAk and
Œª(A) are the eigenvalues of A, then the eigenvalues of P(A) are given by
Œª(P(A)) = P(Œª(A)). In particular, prove that œÅ(A2) = [œÅ(A)]2.
9. Prove that a matrix of order n having n distinct eigenvalues cannot be
defective. Moreover, prove that a normal matrix cannot be defective.
10. Commutativity of matrix product. Show that if A and B are square matri-
ces that share the same set of eigenvectors, then AB = BA. Prove, by a
counterexample, that the converse is false.
11. Let A be a normal matrix whose eigenvalues are Œª1, . . . , Œªn. Show that the
singular values of A are |Œª1|, . . . , |Œªn|.
12. Let A ‚ààCm√ón with rank(A) = n. Show that A‚Ä† = (AT A)‚àí1AT enjoys the
following properties
(1) A‚Ä†A = In;
(2) A‚Ä†AA‚Ä† = A‚Ä†, AA‚Ä†A = A;
(3) if m = n, A‚Ä† = A‚àí1.
13. Show that the Moore-Penrose pseudo-inverse matrix A‚Ä† is the only matrix
that minimizes the functional
min
X‚ààCn√óm‚à•AX ‚àíIm‚à•F,
where ‚à•¬∑ ‚à•F is the Frobenius norm.
14. Prove Property 1.10.
[Solution : For any x, x ‚ààV show that | ‚à•x‚à•‚àí‚à•x‚à•| ‚â§‚à•x ‚àíx‚à•. Assuming
that dim(V ) = n and expanding the vector w = x ‚àíx on a basis of V,
show that ‚à•w‚à•‚â§C‚à•w‚à•‚àû, from which the thesis follows by imposing in
the Ô¨Årst obtained inequality that ‚à•w‚à•‚àû‚â§Œµ.]
15. Prove Property 1.11 in the case A ‚ààRn√óm with m linearly independent
columns.
[Hint : First show that ‚à•¬∑ ‚à•A fulÔ¨Ålls all the properties characterizing a
norm: positiveness (A has linearly independent columns, thus if x Ã∏= 0, then
Ax Ã∏= 0, which proves the thesis), homogeneity and triangular inequality.]
16. Show that for a rectangular matrix A ‚ààRm√ón
‚à•A‚à•2
F = œÉ2
1 + . . . + œÉ2
p,
where p is the minimum between m and n, œÉi are the singular values of A
and ‚à•¬∑ ‚à•F is the Frobenius norm.
17. Assuming p, q = 1, 2, ‚àû, F, recover the following table of equivalence con-
stants cpq such that ‚àÄA ‚ààRn√ón, ‚à•A‚à•p ‚â§cpq‚à•A‚à•q.
cpq
q = 1
q = 2
q = ‚àû
q = F
p = 1
1
‚àön
n
‚àön
p = 2
‚àön
1
‚àön
1
p = ‚àû
n
‚àön
1
‚àön
p = F
‚àön
‚àön
‚àön
1

32
1. Foundations of Matrix Analysis
18. A matrix norm for which ‚à•A‚à•= ‚à•|A| ‚à•is called absolute norm, having
denoted by |A| the matrix of the absolute values of the entries of A. Prove
that ‚à•¬∑ ‚à•1, ‚à•¬∑ ‚à•‚àûand ‚à•¬∑ ‚à•F are absolute norms, while ‚à•¬∑ ‚à•2 is not. Show
that for this latter
1
‚àön‚à•A‚à•2 ‚â§‚à•|A| ‚à•2 ‚â§‚àön‚à•A‚à•2.

2
Principles of Numerical Mathematics
The basic concepts of consistency, stability and convergence of a numerical
method will be introduced in a very general context in the Ô¨Årst part of
the chapter: they provide the common framework for the analysis of any
method considered henceforth. The second part of the chapter deals with
the computer Ô¨Ånite representation of real numbers and the analysis of error
propagation in machine operations.
2.1
Well-posedness and Condition Number of a
Problem
Consider the following problem: Ô¨Ånd x such that
F(x, d) = 0
(2.1)
where d is the set of data which the solution depends on and F is the
functional relation between x and d. According to the kind of problem
that is represented in (2.1), the variables x and d may be real numbers,
vectors or functions. Typically, (2.1) is called a direct problem if F and d
are given and x is the unknown, inverse problem if F and x are known
and d is the unknown, identiÔ¨Åcation problem when x and d are given while
the functional relation F is the unknown (these latter problems will not be
covered in this volume).
Problem (2.1) is well posed if it admits a unique solution x which depends
with continuity on the data. We shall use the terms well posed and stable in

34
2. Principles of Numerical Mathematics
an interchanging manner and we shall deal henceforth only with well-posed
problems.
A problem which does not enjoy the property above is called ill posed or
unstable and before undertaking its numerical solution it has to be regular-
ized, that is, it must be suitably transformed into a well-posed problem (see,
for instance [Mor84]). Indeed, it is not appropriate to pretend the numerical
method can cure the pathologies of an intrinsically ill-posed problem.
Example 2.1 A simple instance of an ill-posed problem is Ô¨Ånding the number
of real roots of a polynomial. For example, the polynomial p(x) = x4 ‚àíx2(2a ‚àí
1) + a(a ‚àí1) exhibits a discontinuous variation of the number of real roots as a
continuously varies in the real Ô¨Åeld. We have, indeed, 4 real roots if a ‚â•1, 2 if
a ‚àà[0, 1) while no real roots exist if a < 0.
‚Ä¢
Continuous dependence on the data means that small perturbations on
the data d yield ‚Äúsmall‚Äù changes in the solution x. Precisely, denoting by Œ¥d
an admissible perturbation on the data and by Œ¥x the consequent change
in the solution, in such a way that
F(x + Œ¥x, d + Œ¥d) = 0,
(2.2)
then
‚àÄŒ∑ > 0, ‚àÉK(Œ∑, d) : ‚à•Œ¥d‚à•< Œ∑ ‚áí‚à•Œ¥x‚à•‚â§K(Œ∑, d)‚à•Œ¥d‚à•.
(2.3)
The norms used for the data and for the solution may not coincide, when-
ever d and x represent variables of diÔ¨Äerent kinds.
With the aim of making this analysis more quantitative, we introduce the
following deÔ¨Ånition.
DeÔ¨Ånition 2.1 For problem (2.1) we deÔ¨Åne the relative condition number
to be
K(d) = sup
Œ¥d‚ààD
‚à•Œ¥x‚à•/‚à•x‚à•
‚à•Œ¥d‚à•/‚à•d‚à•,
(2.4)
where D is a neighborhood of the origin and denotes the set of admissible
perturbations on the data for which the perturbed problem (2.2) still makes
sense. Whenever d = 0 or x = 0, it is necessary to introduce the absolute
condition number, given by
Kabs(d) = sup
Œ¥d‚ààD
‚à•Œ¥x‚à•
‚à•Œ¥d‚à•.
(2.5)
‚ñ†
Problem (2.1) is called ill-conditioned if K(d) is ‚Äúbig‚Äù for any admissible
datum d (the precise meaning of ‚Äúsmall‚Äù and ‚Äúbig‚Äù is going to change
depending on the considered problem).

2.1 Well-posedness and Condition Number of a Problem
35
The property of a problem of being well-conditioned is independent of
the numerical method that is being used to solve it. In fact, it is possible
to generate stable as well as unstable numerical schemes for solving well-
conditioned problems. The concept of stability for an algorithm or for a
numerical method is analogous to that used for problem (2.1) and will be
made precise in the next section.
Remark 2.1 (Ill-posed problems) Even in the case in which the condi-
tion number does not exist (formally, it is inÔ¨Ånite), it is not necessarily true
that the problem is ill-posed. In fact there exist well posed problems (for
instance, the search of multiple roots of algebraic equations, see Example
2.2) for which the condition number is inÔ¨Ånite, but such that they can be
reformulated in equivalent problems (that is, having the same solutions)
with a Ô¨Ånite condition number.
‚ñ†
If problem (2.1) admits a unique solution, then there necessarily exists a
mapping G, that we call resolvent, between the sets of the data and of the
solutions, such that
x = G(d),
that is
F(G(d), d) = 0.
(2.6)
According to this deÔ¨Ånition, (2.2) yields x + Œ¥x = G(d + Œ¥d). Assuming
that G is diÔ¨Äerentiable in d and denoting formally by G‚Ä≤(d) its derivative
with respect to d (if G : Rn ‚ÜíRm, G‚Ä≤(d) will be the Jacobian matrix of
G evaluated at the vector d), a Taylor‚Äôs expansion of G truncated at Ô¨Årst
order ensures that
G(d + Œ¥d) ‚àíG(d) = G‚Ä≤(d)Œ¥d + o(‚à•Œ¥d‚à•)
for Œ¥d ‚Üí0,
where ‚à•¬∑ ‚à•is a suitable norm for Œ¥d and o(¬∑) is the classical inÔ¨Ånitesimal
symbol denoting an inÔ¨Ånitesimal term of higher order with respect to its
argument. Neglecting the inÔ¨Ånitesimal of higher order with respect to ‚à•Œ¥d‚à•,
from (2.4) and (2.5) we respectively deduce that
K(d) ‚âÉ‚à•G‚Ä≤(d)‚à•
‚à•d‚à•
‚à•G(d)‚à•,
Kabs(d) ‚âÉ‚à•G‚Ä≤(d)‚à•,
(2.7)
the symbol ‚à•¬∑‚à•denoting the matrix norm associated with the vector norm
(deÔ¨Åned in (1.19)). The estimates in (2.7) are of great practical usefulness
in the analysis of problems in the form (2.6), as shown in the forthcoming
examples.
Example 2.2 (Algebraic equations of second degree) The solutions to the
algebraic equation x2 ‚àí2px + 1 = 0, with p ‚â•1, are x¬± = p ¬±

p2 ‚àí1. In this
case, F(x, p) = x2 ‚àí2px+1, the datum d is the coeÔ¨Écient p, while x is the vector
of components {x+, x‚àí}. As for the condition number, we notice that (2.6) holds

36
2. Principles of Numerical Mathematics
by taking G : R ‚ÜíR2, G(p) = {x+, x‚àí}. Letting G¬±(p) = x¬±, it follows that
G‚Ä≤
¬±(p) = 1 ¬± p/

p2 ‚àí1. Using (2.7) with ‚à•¬∑ ‚à•= ‚à•¬∑ ‚à•2 we get
K(p) ‚âÉ
|p|

p2 ‚àí1
,
p > 1.
(2.8)
From (2.8) it turns out that in the case of separated roots (say, if p ‚â•
‚àö
2)
problem F(x, p) = 0 is well conditioned. The behavior dramatically changes in
the case of multiple roots, that is when p = 1. First of all, one notices that the
function G¬±(p) = p ¬±

p2 ‚àí1 is no longer diÔ¨Äerentiable for p = 1, which makes
(2.8) meaningless. On the other hand, equation (2.8) shows that, for p close to
1, the problem at hand is ill conditioned. However, the problem is not ill posed.
Indeed, following Remark 2.1, it is possible to reformulate it in an equivalent
manner as F(x, t) = x2 ‚àí((1 + t2)/t)x + 1 = 0, with t = p +

p2 ‚àí1, whose
roots x‚àí= t and x+ = 1/t coincide for t = 1. The change of parameter thus
removes the singularity that is present in the former representation of the roots
as functions of p. The two roots x‚àí= x‚àí(t) and x+ = x+(t) are now indeed
regular functions of t in the neighborhood of t = 1 and evaluating the condition
number by (2.7) yields K(t) ‚âÉ1 for any value of t. The transformed problem is
thus well conditioned.
‚Ä¢
Example 2.3 (Systems of linear equations) Consider the linear system Ax
= b, where x and b are two vectors in Rn, while A is the matrix (n √ó n) of the
real coeÔ¨Écients of the system. Suppose that A is nonsingular; in such a case x
is the unknown solution x, while the data d are the right-hand side b and the
matrix A, that is, d = {bi, aij, 1 ‚â§i, j ‚â§n}.
Suppose now that we perturb only the right-hand side b. We have d = b,
x = G(b) = A‚àí1b so that, G‚Ä≤(b) = A‚àí1, and (2.7) yields
K(d) ‚âÉ‚à•A‚àí1‚à•‚à•b‚à•
‚à•A‚àí1b‚à•
= ‚à•Ax‚à•
‚à•x‚à•‚à•A‚àí1‚à•‚â§‚à•A‚à•‚à•A‚àí1‚à•= K(A),
(2.9)
where K(A) is the condition number of matrix A (see Section 3.1.1) and the use
of a consistent matrix norm is understood. Therefore, if A is well conditioned,
solving the linear system Ax=b is a stable problem with respect to perturbations
of the right-hand side b. Stability with respect to perturbations on the entries of
A will be analyzed in Section 3.10.
‚Ä¢
Example 2.4 (Nonlinear equations) Let f : R ‚ÜíR be a function of class
C1 and consider the nonlinear equation
F(x, d) = f(x) = œï(x) ‚àíd = 0,
where œï : R ‚ÜíR is a suitable function and d ‚ààR a datum (possibly equal
to zero). The problem is well deÔ¨Åned only if œï is invertible in a neighborhood
of d: in such a case, indeed, x = œï‚àí1(d) and the resolvent is G = œï‚àí1. Since
(œï‚àí1)‚Ä≤(d) = [œï‚Ä≤(x)]‚àí1, the Ô¨Årst relation in (2.7) yields, for d Ã∏= 0,
K(d) ‚âÉ|d|
|x||[œï‚Ä≤(x)]‚àí1|,
(2.10)

2.2 Stability of Numerical Methods
37
while if d = 0 or x = 0 we have
Kabs(d) ‚âÉ|[œï‚Ä≤(x)]‚àí1|.
(2.11)
The problem is thus ill posed if x is a multiple root of œï(x)‚àíd; it is ill conditioned
when œï‚Ä≤(x) is ‚Äúsmall‚Äù, well conditioned when œï‚Ä≤(x) is ‚Äúlarge‚Äù. We shall further
address this subject in Section 6.1.
‚Ä¢
In view of (2.8), the quantity ‚à•G‚Ä≤(d)‚à•is an approximation of Kabs(d) and
is sometimes called Ô¨Årst order absolute condition number. This latter rep-
resents the limit of the Lipschitz constant of G (see Section 11.1) as the
perturbation on the data tends to zero.
Such a number does not always provide a sound estimate of the condition
number Kabs(d). This happens, for instance, when G‚Ä≤ vanishes at a point
whilst G is non null in a neighborhood of the same point. For example,
take x = G(d) = cos(d) ‚àí1 for d ‚àà(‚àíœÄ/2, œÄ/2); we have G‚Ä≤(0) = 0, while
Kabs(0) = 2/œÄ.
2.2
Stability of Numerical Methods
We shall henceforth suppose the problem (2.1) to be well posed. A numer-
ical method for the approximate solution of (2.1) will consist, in general,
of a sequence of approximate problems
Fn(xn, dn) = 0
n ‚â•1
(2.12)
depending on a certain parameter n (to be deÔ¨Åned case by case). The
understood expectation is that xn ‚Üíx as n ‚Üí‚àû, i.e. that the numerical
solution converges to the exact solution. For that, it is necessary that dn ‚Üí
d and that Fn ‚Äúapproximates‚Äù F, as n ‚Üí‚àû. Precisely, if the datum d of
problem (2.1) is admissible for Fn, we say that (2.12) is consistent if
Fn(x, d) = Fn(x, d) ‚àíF(x, d) ‚Üí0 for n ‚Üí‚àû
(2.13)
where x is the solution to problem (2.1) corresponding to the datum d.
The meaning of this deÔ¨Ånition will be made precise in the next chapters
for any single class of considered problems.
A method is said to be strongly consistent if Fn(x, d) = 0 for any value
of n and not only for n ‚Üí‚àû.
In some cases (e.g., when iterative methods are used) problem (2.12)
could take the following form
Fn(xn, xn‚àí1, . . . , xn‚àíq, dn) = 0
n ‚â•q
(2.14)
where x0, x1, . . . , xq‚àí1 are given. In such a case, the property of strong
consistency becomes Fn(x, x, . . . , x, d) = 0 for all n ‚â•q.

38
2. Principles of Numerical Mathematics
Example 2.5 Let us consider the following iterative method (known as New-
ton‚Äôs method and discussed in Section 6.2.2) for approximating a simple root Œ±
of a function f : R ‚ÜíR,
given x0,
xn = xn‚àí1 ‚àíf(xn‚àí1)
f ‚Ä≤(xn‚àí1),
n ‚â•1.
(2.15)
The method (2.15) can be written in the form (2.14) by setting Fn(xn, xn‚àí1, f) =
xn ‚àíxn‚àí1 + f(xn‚àí1)/f ‚Ä≤(xn‚àí1) and is strongly consistent since Fn(Œ±, Œ±, f) = 0
for all n ‚â•1.
Consider now the following numerical method (known as the composite mid-
point rule discussed in Section 9.2) for approximating x =
 b
a f(t) dt,
xn = H
n

k=1
f
 tk + tk+1
2

,
n ‚â•1
where H = (b ‚àía)/n and tk = a + (k ‚àí1)H, k = 1, . . . , (n + 1). This method
is consistent; it is also strongly consistent provided thet f is a piecewise linear
polynomial.
More generally, all numerical methods obtained from the mathematical prob-
lem by truncation of limit operations (such as integrals, derivatives, series, . . . )
are not strongly consistent.
‚Ä¢
Recalling what has been previously stated about problem (2.1), in order
for the numerical method to be well posed (or stable) we require that for any
Ô¨Åxed n, there exists a unique solution xn corresponding to the datum dn,
that the computation of xn as a function of dn is unique and, furthermore,
that xn depends continuously on the data, i.e.
‚àÄŒ∑ > 0, ‚àÉKn(Œ∑, dn) : ‚à•Œ¥dn‚à•< Œ∑ ‚áí‚à•Œ¥xn‚à•‚â§Kn(Œ∑, dn)‚à•Œ¥dn‚à•.
(2.16)
As done in (2.4), we introduce for each problem in the sequence (2.12) the
quantities
Kn(dn) =
sup
Œ¥dn‚ààDn
‚à•Œ¥xn‚à•/‚à•xn‚à•
‚à•Œ¥dn‚à•/‚à•dn‚à•,
Kabs,n(dn) =
sup
Œ¥dn‚ààDn
‚à•Œ¥xn‚à•
‚à•Œ¥dn‚à•,
(2.17)
and then deÔ¨Åne
Knum(dn) = lim
k‚Üí‚àûsup
n‚â•k
Kn(dn),
Knum
abs (dn) = lim
k‚Üí‚àûsup
n‚â•k
Kabs,n(dn).
We call Knum(dn) the relative asymptotic condition number of the numer-
ical method (2.12) and Knum
abs (dn) absolute asymptotic condition number,
corresponding to the datum dn.
The numerical method is said to be well conditioned if Knum is ‚Äúsmall‚Äù
for any admissible datum dn, ill conditioned otherwise. As in (2.6), let us

2.2 Stability of Numerical Methods
39
consider the case where, for each n, the functional relation (2.1) deÔ¨Ånes a
mapping Gn between the sets of the numerical data and the solutions
xn = Gn(dn),
that is Fn(Gn(dn), dn) = 0.
(2.18)
Assuming that Gn is diÔ¨Äerentiable, we can obtain from (2.17)
Kn(dn) ‚âÉ‚à•G‚Ä≤
n(dn)‚à•
‚à•dn‚à•
‚à•Gn(dn)‚à•,
Kabs,n(dn) ‚âÉ‚à•G‚Ä≤
n(dn)‚à•.
(2.19)
Example 2.6 (Sum and subtraction) The function f : R2 ‚ÜíR, f(a, b) =
a + b, is a linear mapping whose gradient is the vector f ‚Ä≤(a, b) = (1, 1)T . Using
the vector norm ‚à•¬∑ ‚à•1 deÔ¨Åned in (1.13) yields K(a, b) ‚âÉ(|a| + |b|)/(|a + b|), from
which it follows that summing two numbers of the same sign is a well conditioned
operation, being K(a, b) ‚âÉ1. On the other hand, subtracting two numbers almost
equal is ill conditioned, since |a + b| ‚â™|a| + |b|. This fact, already pointed out in
Example 2.2, leads to the cancellation of signiÔ¨Åcant digits whenever numbers can
be represented using only a Ô¨Ånite number of digits (as in Ô¨Çoating-point arithmetic,
see Section 2.5).
‚Ä¢
Example 2.7 Consider again the problem of computing the roots of a polyno-
mial of second degree analyzed in Example 2.2. When p > 1 (separated roots),
such a problem is well conditioned. However, we generate an unstable algorithm
if we evaluate the root x‚àíby the formula x‚àí= p ‚àí

p2 ‚àí1. This formula is
indeed subject to errors due to numerical cancellation of signiÔ¨Åcant digits (see
Section 2.4) that are introduced by the Ô¨Ånite arithmetic of the computer. A pos-
sible remedy to this trouble consists of computing x+ = p +

p2 ‚àí1 at Ô¨Årst,
then x‚àí= 1/x+. Alternatively, one can solve F(x, p) = x2 ‚àí2px + 1 = 0 using
Newton‚Äôs method (proposed in Example 2.5)
xn = xn‚àí1 ‚àí(x2
n‚àí1 ‚àí2pxn‚àí1 + 1)/(2xn‚àí1 ‚àí2p) = fn(p),
n ‚â•1,
x0 given.
Applying (2.19) for p > 1 yields Kn(p) ‚âÉ|p|/|xn ‚àíp|. To compute Knum(p)
we notice that, in the case when the algorithm converges, the solution xn would
converge to one of the roots x+ or x‚àí; therefore, |xn ‚àíp| ‚Üí

p2 ‚àí1 and thus
Kn(p) ‚ÜíKnum(p) ‚âÉ|p|/

p2 ‚àí1, in perfect agreement with the value (2.8) of
the condition number of the exact problem.
We can conclude that Newton‚Äôs method for the search of simple roots of a
second order algebraic equation is ill conditioned if |p| is very close to 1, while it
is well conditioned in the other cases.
‚Ä¢
The Ô¨Ånal goal of numerical approximation is, of course, to build, through
numerical problems of the type (2.12), solutions xn that ‚Äúget closer‚Äù to the
solution of problem (2.1) as much as n gets larger. This concept is made
precise in the next deÔ¨Ånition.
DeÔ¨Ånition 2.2 The numerical method (2.12) is convergent iÔ¨Ä
‚àÄŒµ > 0 ‚àÉn0(Œµ), ‚àÉŒ¥(n0, Œµ) > 0 :
‚àÄn > n0(Œµ), ‚àÄ‚à•Œ¥dn‚à•< Œ¥(n0, Œµ)
‚áí‚à•x(d) ‚àíxn(d + Œ¥dn)‚à•‚â§Œµ,
(2.20)

40
2. Principles of Numerical Mathematics
where d is an admissible datum for the problem (2.1), x(d) is the corre-
sponding solution and xn(d+Œ¥dn) is the solution of the numerical problem
(2.12) with datum d + Œ¥dn.
‚ñ†
To verify the implication (2.20) it suÔ¨Éces to check that under the same
assumptions
‚à•x(d + Œ¥dn) ‚àíxn(d + Œ¥dn)‚à•‚â§Œµ
2.
(2.21)
Indeed, thanks to (2.3) we have
‚à•x(d) ‚àíxn(d + Œ¥dn)‚à•‚â§‚à•x(d) ‚àíx(d + Œ¥dn)‚à•
+‚à•x(d + Œ¥dn) ‚àíxn(d + Œ¥dn)‚à•‚â§K(Œ¥(n0, Œµ), d)‚à•Œ¥dn‚à•+ Œµ
2.
Choosing Œ¥dn such that K(Œ¥(n0, Œµ), d)‚à•Œ¥dn‚à•< Œµ
2, one obtains (2.20).
Measures of the convergence of xn to x are given by the absolute error
or the relative error, respectively deÔ¨Åned as
E(xn) = |x ‚àíxn|,
Erel(xn) = |x ‚àíxn|
|x|
,
(if x Ã∏= 0).
(2.22)
In the cases where x and xn are matrix or vector quantities, in addition
to the deÔ¨Ånitions in (2.22) (where the absolute values are substituted by
suitable norms) it is sometimes useful to introduce the error by component
deÔ¨Åned as
Ec
rel(xn) = max
i,j
|(x ‚àíxn)ij|
|xij|
.
(2.23)
2.2.1
Relations between Stability and Convergence
The concepts of stability and convergence are strongly connected.
First of all, if problem (2.1) is well posed, a necessary condition in order
for the numerical problem (2.12) to be convergent is that it is stable.
Let us thus assume that the method is convergent, and prove that it is
stable by Ô¨Ånding a bound for ‚à•Œ¥xn‚à•. We have
‚à•Œ¥xn‚à•
=
‚à•xn(d + Œ¥dn) ‚àíxn(d)‚à•‚â§‚à•xn(d) ‚àíx(d)‚à•
+
‚à•x(d) ‚àíx(d + Œ¥dn)‚à•+ ‚à•x(d + Œ¥dn) ‚àíxn(d + Œ¥dn)‚à•
‚â§
K(Œ¥(n0, Œµ), d)‚à•Œ¥dn‚à•+ Œµ,
(2.24)
having used (2.3) and (2.21) twice. From (2.24) we can conclude that, for n
suÔ¨Éciently large, ‚à•Œ¥xn‚à•/‚à•Œ¥dn‚à•can be bounded by a constant of the order

2.3 A priori and a posteriori Analysis
41
of K(Œ¥(n0, Œµ), d), so that the method is stable. Thus, we are interested in
stable numerical methods since only these can be convergent.
The stability of a numerical method becomes a suÔ¨Écient condition for
the numerical problem (2.12) to converge if this latter is also consistent
with problem (2.1). Indeed, under these assumptions we have
‚à•x(d + Œ¥dn) ‚àíxn(d + Œ¥dn)‚à•
‚â§
‚à•x(d + Œ¥dn) ‚àíx(d)‚à•
+
‚à•x(d) ‚àíxn(d)‚à•+ ‚à•xn(d) ‚àíxn(d + Œ¥dn)‚à•.
Thanks to (2.3), the Ô¨Årst term at right-hand side can be bounded by ‚à•Œ¥dn‚à•
(up to a multiplicative constant independent of Œ¥dn). A similar bound holds
for the third term, due to the stability property (2.16). Finally, concerning
the remaining term, if Fn is diÔ¨Äerentiable with respect to the variable x,
an expansion in a Taylor series gives
Fn(x(d), d) ‚àíFn(xn(d), d) = ‚àÇFn
‚àÇx |(x,d)(x(d) ‚àíxn(d)),
for a suitable x ‚Äúbetween‚Äù x(d) and xn(d). Assuming also that ‚àÇFn/‚àÇx is
invertible, we get
x(d) ‚àíxn(d) =
‚àÇFn
‚àÇx
‚àí1
|(x,d)
[Fn(x(d), d) ‚àíFn(xn(d), d)].
(2.25)
On the other hand, replacing Fn(xn(d), d) with F(x(d), d) (since both terms
are equal to zero) and passing to the norms, we Ô¨Ånd
‚à•x(d) ‚àíxn(d)‚à•‚â§
!!!!!
‚àÇFn
‚àÇx
‚àí1
|(x,d)
!!!!! ‚à•Fn(x(d), d) ‚àíF(x(d), d)‚à•.
Thanks to (2.13) we can thus conclude that ‚à•x(d)‚àíxn(d)‚à•‚Üí0 for n ‚Üí‚àû.
The result that has just been proved, although stated in qualitative terms,
is a milestone in numerical analysis, known as equivalence theorem (or
Lax-Richtmyer theorem): ‚Äúfor a consistent numerical method, stability is
equivalent to convergence‚Äù. A rigorous proof of this theorem is available in
[Dah56] for the case of linear Cauchy problems, or in [Lax65] and in [RM67]
for linear well-posed initial value problems.
2.3
A priori and a posteriori Analysis
The stability analysis of a numerical method can be carried out following
diÔ¨Äerent strategies:
1. forward analysis, which provides a bound to the variations ‚à•Œ¥xn‚à•on
the solution due to both perturbations in the data and to errors that
are intrinsic to the numerical method;

42
2. Principles of Numerical Mathematics
2. backward analysis, which aims at estimating the perturbations that
should be ‚Äúimpressed‚Äù to the data of a given problem in order to
obtain the results actually computed under the assumption of working
in exact arithmetic. Equivalently, given a certain computed solution
xn, backward analysis looks for the perturbations Œ¥dn on the data
such that Fn(xn, dn + Œ¥dn) = 0. Notice that, when performing such
an estimate, no account at all is taken into the way xn has been
obtained (that is, which method has been employed to generate it).
Forward and backward analyses are two diÔ¨Äerent instances of the so
called a priori analysis. This latter can be applied to investigate not only
the stability of a numerical method, but also its convergence. In this case
it is referred to as a priori error analysis, which can again be performed
using either a forward or a backward technique.
A priori error analysis is distincted from the so called a posteriori error
analysis, which aims at producing an estimate of the error on the grounds
of quantities that are actually computed by a speciÔ¨Åc numerical method.
Typically, denoting by xn the computed numerical solution, approximation
to the solution x of problem (2.1), the a posteriori error analysis aims at
evaluating the error x ‚àíxn as a function of the residual rn = F(xn, d) by
means of constants that are called stability factors (see [EEHJ96]).
Example 2.8 For the sake of illustration, consider the problem of Ô¨Ånding the
zeros Œ±1, . . . , Œ±n of a polynomial pn(x) = n
k=0 akxk of degree n.
Denoting by Àúpn(x) = n
k=0 Àúakxk a perturbed polynomial whose zeros are ÀúŒ±i,
forward analysis aims at estimating the error between two corresponding zeros
Œ±i and ÀúŒ±i, in terms of the variations on the coeÔ¨Écients ak ‚àíÀúak, k = 0, 1, . . . , n.
On the other hand, let {ÀÜŒ±i} be the approximate zeros of pn (computed some-
how). Backward analysis provides an estimate of the perturbations Œ¥ak which
should be impressed to the coeÔ¨Écients so that n
k=0(ak +Œ¥ak)ÀÜŒ±k
i = 0, for a Ô¨Åxed
ÀÜŒ±i. The goal of a posteriori error analysis would rather be to provide an estimate
of the error Œ±i ‚àíÀÜŒ±i as a function of the residual value pn(ÀÜŒ±i).
This analysis will be carried out in Section 6.1.
‚Ä¢
Example 2.9 Consider the linear system Ax=b, where A‚ààRn√ón is a nonsin-
gular matrix.
For the perturbed system ÀúAÀúx = Àúb, forward analysis provides an estimate of
the error x ‚àíÀúx in terms of A ‚àíÀúA and b ‚àíÀúb, while backward analysis estimates
the perturbations Œ¥A = (Œ¥aij) and Œ¥b = (Œ¥bi) which should be impressed to the
entries of A and b in order to get (A + Œ¥A)xn = b + Œ¥b, xn being the solution of
the linear system (computed somehow). Finally, a posteriori error analysis looks
for an estimate of the error x ‚àíxn as a function of the residual rn = b ‚àíAxn.
We will develop this analysis in Section 3.1.
‚Ä¢
It is important to point out the role played by the a posteriori analysis in
devising strategies for adaptive error control. These strategies, by suitably
changing the discretization parameters (for instance, the spacing between

2.4 Sources of Error in Computational Models
43
nodes in the numerical integration of a function or a diÔ¨Äerential equation),
employ the a posteriori analysis in order to ensure that the error does not
exceed a Ô¨Åxed tolerance.
A numerical method that makes use of an adaptive error control is called
adaptive numerical method. In practice, a method of this kind applies in the
computational process the idea of feedback, by activating on the grounds of
a computed solution a convergence test which ensures the control of error
within a Ô¨Åxed tolerance. In case the convergence test fails, a suitable strat-
egy for modifying the discretization parameters is automatically adopted
in order to enhance the accuracy of the solution to be newly computed,
and the overall procedure is iterated until the convergence check is passed.
2.4
Sources of Error in Computational Models
Whenever the numerical problem (2.12) is an approximation to the math-
ematical problem (2.1) and this latter is in turn a model of a physical
problem (which will be shortly denoted by PP), we shall say that (2.12) is
a computational model for PP.
In this process the global error, denoted by e, is expressed by the dif-
ference between the actually computed solution, xn, and the physical so-
lution, xph, of which x provides a model. The global error e can thus be
interpreted as being the sum of the error em of the mathematical model,
given by x‚àíxph, and the error ec of the computational model, xn ‚àíx, that
is e = em + ec (see Figure 2.1).
PP : xph
F(x, d) = 0
Fn(xn, dn) = 0
em
xn
e
ea
ec
en
FIGURE 2.1. Errors in computational models
The error em will in turn take into account the error of the mathematical
model in strict sense (that is, the extent at which the functional equation
(2.1) does realistically describe the problem PP) and the error on the data
(that is, how much accurately does d provide a measure of the real physical

44
2. Principles of Numerical Mathematics
data). In the same way, ec turns out to be the combination of the numerical
discretization error en = xn ‚àíx, the error ea introduced by the numerical
algorithm and the roundoÔ¨Äerror introduced by the computer during the
actual solution of problem (2.12) (see Section 2.5).
In general, we can thus outline the following sources of error:
1. errors due to the model, that can be controlled by a proper choice of
the mathematical model;
2. errors in the data, that can be reduced by enhancing the accuracy in
the measurement of the data themselves;
3. truncation errors, arising from having replaced in the numerical model
limits by operations that involve a Ô¨Ånite number of steps;
4. rounding errors.
The errors at the items 3. and 4. give rise to the computational error. A
numerical method will thus be convergent if this error can be made arbi-
trarily small by increasing the computational eÔ¨Äort. Of course, convergence
is the primary, albeit not unique, goal of a numerical method, the others
being accuracy, reliability and eÔ¨Éciency.
Accuracy means that the errors are small with respect to a Ô¨Åxed tol-
erance. It is usually quantiÔ¨Åed by the order of inÔ¨Ånitesimal of the error
en with respect to the discretization characteristic parameter (for instance
the largest grid spacing between the discretization nodes). By the way, we
notice that machine precision does not limit, on theoretical grounds, the
accuracy.
Reliability means it is likely that the global error can be guaranteed to be
below a certain tolerance. Of course, a numerical model can be considered
to be reliable only if suitably tested, that is, successfully applied to several
test cases.
EÔ¨Éciency means that the computational complexity that is needed to
control the error (that is, the amount of operations and the size of the
memory required) is as small as possible.
Having encountered the term algorithm several times in this section, we
cannot refrain from providing an intuitive description of it. By algorithm
we mean a directive that indicates, through elementary operations, all the
passages that are needed to solve a speciÔ¨Åc problem. An algorithm can in
turn contain sub-algorithms and must have the feature of terminating after
a Ô¨Ånite number of elementary operations. As a consequence, the executor
of the algorithm (machine or human being) must Ô¨Ånd within the algorithm
itself all the instructions to completely solve the problem at hand (provided
that the necessary resources for its execution are available).
For instance, the statement that a polynomial of second degree surely
admits two roots in the complex plane does not characterize an algorithm,

2.5 Machine Representation of Numbers
45
whereas the formula yielding the roots is an algorithm (provided that the
sub-algorithms needed to correctly execute all the operations have been
deÔ¨Åned in turn).
Finally, the complexity of an algorithm is a measure of its executing
time. Calculating the complexity of an algorithm is therefore a part of the
analysis of the eÔ¨Éciency of a numerical method. Since several algorithms,
with diÔ¨Äerent complexities, can be employed to solve the same problem P,
it is useful to introduce the concept of complexity of a problem, this latter
meaning the complexity of the algorithm that has minimum complexity
among those solving P. The complexity of a problem is typically measured
by a parameter directly associated with P. For instance, in the case of
the product of two square matrices, the computational complexity can be
expressed as a function of a power of the matrix size n (see, [Str69]).
2.5
Machine Representation of Numbers
Any machine operation is aÔ¨Äected by rounding errors or roundoÔ¨Ä. They are
due to the fact that on a computer only a Ô¨Ånite subset of the set of real
numbers can be represented. In this section, after recalling the positional
notation of real numbers, we introduce their machine representation.
2.5.1
The Positional System
Let a base Œ≤ ‚ààN be Ô¨Åxed with Œ≤ ‚â•2, and let x be a real number with
a Ô¨Ånite number of digits xk with 0 ‚â§xk < Œ≤ for k = ‚àím, . . . , n. The
notation (conventionally adopted)
xŒ≤ = (‚àí1)s [xnxn‚àí1 . . . x1x0.x‚àí1x‚àí2 . . . x‚àím] ,
xn Ã∏= 0
(2.26)
is called the positional representation of x with respect to the base Œ≤. The
point between x0 and x‚àí1 is called decimal point if the base is 10, binary
point if the base is 2, while s depends on the sign of x (s = 0 if x is positive,
1 if negative). Relation (2.26) actually means
xŒ≤ = (‚àí1)s

n

k=‚àím
xkŒ≤k

.
Example 2.10 The conventional writing x10 = 425.33 denotes the number x =
4 ¬∑ 102 + 2 ¬∑ 10 + 5 + 3 ¬∑ 10‚àí1 + 3 ¬∑ 10‚àí2, while x6 = 425.33 would denote the
real number x = 4 ¬∑ 62 + 2 ¬∑ 6 + 5 + 3 ¬∑ 6‚àí1 + 3 ¬∑ 6‚àí2. A rational number can of
course have a Ô¨Ånite number of digits in a base and an inÔ¨Ånite number of digits in
another base. For example, the fraction 1/3 has inÔ¨Ånite digits in base 10, being
x10 = 0.¬Ø3, while it has only one digit in base 3, being x3 = 0.1.
‚Ä¢

46
2. Principles of Numerical Mathematics
Any real number can be approximated by numbers having a Ô¨Ånite repre-
sentation. Indeed, having Ô¨Åxed the base Œ≤, the following property holds
‚àÄŒµ > 0, ‚àÄxŒ≤ ‚ààR, ‚àÉyŒ≤ ‚ààR such that |yŒ≤ ‚àíxŒ≤| < Œµ,
where yŒ≤ has Ô¨Ånite positional representation.
In fact, given the positive number xŒ≤ = xnxn‚àí1 . . . x0.x‚àí1 . . . x‚àím . . . with
a number of digits, Ô¨Ånite or inÔ¨Ånite, for any r ‚â•1 one can build two
numbers
x(l)
Œ≤ =
r‚àí1

k=0
xn‚àíkŒ≤n‚àík,
x(u)
Œ≤
= x(l)
Œ≤ + Œ≤n‚àír+1,
having r digits, such that x(l)
Œ≤
< xŒ≤ < x(u)
Œ≤
and x(u)
Œ≤
‚àíx(l)
Œ≤
= Œ≤n‚àír+1. If
r is chosen in such a way that Œ≤n‚àír+1 < œµ, then taking yŒ≤ equal to x(l)
Œ≤
or x(u)
Œ≤
yields the desired inequality. This result legitimates the computer
representation of real numbers (and thus by a Ô¨Ånite number of digits).
Although theoretically speaking all the bases are equivalent, in the com-
putational practice three are the bases generally employed: base 2 or binary,
base 10 or decimal (the most natural) and base 16 or hexadecimal. Almost
all modern computers use base 2, apart from a few which traditionally
employ base 16. In what follows, we will assume that Œ≤ is an even integer.
In the binary representation, digits reduce to the two symbols 0 and 1,
called bits (binary digits), while in the hexadecimal case the symbols used
for the representation of the digits are 0,1,...,9,A,B,C,D,E,F. Clearly,
the smaller the adopted base, the longer the string of characters needed to
represent the same number.
To simplify notations, we shall write x instead of xŒ≤, leaving the base Œ≤
understood.
2.5.2
The Floating-point Number System
Assume a given computer has N memory positions in which to store any
number. The most natural way to make use of these positions in the rep-
resentation of a real number x diÔ¨Äerent from zero is to Ô¨Åx one of them for
its sign, N ‚àík ‚àí1 for the integer digits and k for the digits beyond the
point, in such a way that
x = (‚àí1)s ¬∑ [aN‚àí2aN‚àí3 . . . ak . ak‚àí1 . . . a0]
(2.27)
s being equal to 1 or 0. Notice that one memory position is equivalent to
one bit storage only when Œ≤ = 2. The set of numbers of this kind is called
Ô¨Åxed-point system. Equation (2.27) stands for
x = (‚àí1)s ¬∑ Œ≤‚àík
N‚àí2

j=0
ajŒ≤j
(2.28)

2.5 Machine Representation of Numbers
47
and therefore this representation amounts to Ô¨Åxing a scaling factor for all
the representable numbers.
The use of Ô¨Åxed point strongly limits the value of the minimum and maxi-
mum numbers that can be represented on the computer, unless a very large
number N of memory positions is employed. This drawback can be easily
overcome if the scaling in (2.28) is allowed to be varying. In such a case,
given a non vanishing real number x, its Ô¨Çoating-point representation is
given by
x = (‚àí1)s ¬∑ (0.a1a2 . . . at) ¬∑ Œ≤e = (‚àí1)s ¬∑ m ¬∑ Œ≤e‚àít
(2.29)
where t ‚ààN is the number of allowed signiÔ¨Åcant digits ai (with 0 ‚â§ai ‚â§
Œ≤ ‚àí1), m = a1a2 . . . at an integer number called mantissa such that 0 ‚â§
m ‚â§Œ≤t ‚àí1 and e an integer number called exponent. Clearly, the exponent
can vary within a Ô¨Ånite interval of admissible values: we let L ‚â§e ‚â§U
(typically L < 0 and U > 0). The N memory positions are now distributed
among the sign (one position), the signiÔ¨Åcant digits (t positions) and the
digits for the exponent (the remaining N ‚àít ‚àí1 positions). The number
zero has a separate representation.
Typically, on the computer there are two formats available for the Ô¨Çoating-
point number representation: single and double precision. In the case of bi-
nary representation, these formats correspond in the standard version to
the representation with N = 32 bits (single precision)
1
s
8 bits
e
23 bits
m
and with N = 64 bits (double precision)
1
s
11 bits
e
52 bits
m
Let us denote by
F(Œ≤, t, L, U) = {0} ‚à™
"
x ‚ààR : x = (‚àí1)sŒ≤e
t

i=1
aiŒ≤‚àíi
#
the set of Ô¨Çoating-point numbers with t signiÔ¨Åcant digits, base Œ≤ ‚â•2, 0 ‚â§
ai ‚â§Œ≤ ‚àí1, and range (L,U) with L ‚â§e ‚â§U.
In order to enforce uniqueness in a number representation, it is typi-
cally assumed that a1 Ã∏= 0 and m ‚â•Œ≤t‚àí1. In such an event a1 is called
the principal signiÔ¨Åcant digit, while at is the last signiÔ¨Åcant digit and the
representation of x is called normalized. The mantissa m is now varying
between Œ≤t‚àí1 and Œ≤t ‚àí1.
For instance, in the case Œ≤ = 10, t = 4, L = ‚àí1 and U = 4, without
the assumption that a1 Ã∏= 0, the number 1 would admit the following
representations
0.1000 ¬∑ 101,
0.0100 ¬∑ 102,
0.0010 ¬∑ 103,
0.0001 ¬∑ 104.

48
2. Principles of Numerical Mathematics
To always have uniqueness in the representation, it is assumed that also
the number zero has its own sign (typically s = 0 is assumed).
It can be immediately noticed that if x ‚ààF(Œ≤, t, L, U) then also ‚àíx ‚àà
F(Œ≤, t, L, U). Moreover, the following lower and upper bounds hold for the
absolute value of x
xmin = Œ≤L‚àí1 ‚â§|x| ‚â§Œ≤U(1 ‚àíŒ≤‚àít) = xmax.
(2.30)
The cardinality of F(Œ≤, t, L, U) (henceforth shortly denoted by F) is
card F = 2(Œ≤ ‚àí1)Œ≤t‚àí1(U ‚àíL + 1) + 1.
From (2.30) it turns out that it is not possible to represent any number
(apart from zero) whose absolute value is less than xmin. This latter limi-
tation can be overcome by completing F by the set FD of the Ô¨Çoating-point
de-normalized numbers obtained by removing the assumption that a1 is
non null, only for the numbers that are referred to the minimum exponent
L. In such a way the uniqueness in the representation is not lost and it is
possible to generate numbers that have mantissa between 1 and Œ≤t‚àí1 ‚àí1
and belong to the interval (‚àíŒ≤L‚àí1, Œ≤L‚àí1). The smallest number in this set
has absolute value equal to Œ≤L‚àít.
Example 2.11 The positive numbers in the set F(2, 3, ‚àí1, 2) are
(0.111) ¬∑ 22 = 7
2,
(0.110) ¬∑ 22 = 3,
(0.101) ¬∑ 22 = 5
2,
(0.100) ¬∑ 22 = 2,
(0.111) ¬∑ 2 = 7
4,
(0.110) ¬∑ 2 = 3
2,
(0.101) ¬∑ 2 = 5
4,
(0.100) ¬∑ 2 = 1,
(0.111) = 7
8,
(0.110) = 3
4,
(0.101) = 5
8,
(0.100) = 1
2,
(0.111) ¬∑ 2‚àí1 = 7
16,
(0.110) ¬∑ 2‚àí1 = 3
8,
(0.101) ¬∑ 2‚àí1 = 5
16,
(0.100) ¬∑ 2‚àí1 = 1
4.
They are included between xmin = Œ≤L‚àí1 = 2‚àí2 = 1/4 and xmax = Œ≤U(1‚àíŒ≤‚àít) =
22(1‚àí2‚àí3) = 7/2. As a whole, we have (Œ≤ ‚àí1)Œ≤t‚àí1(U ‚àíL+1) = (2‚àí1)23‚àí1(2+
1 + 1) = 16 strictly positive numbers. Their opposites must be added to them,
as well as the number zero. We notice that when Œ≤ = 2, the Ô¨Årst signiÔ¨Åcant digit
in the normalized representation is necessarily equal to 1 and thus it may not be
stored in the computer (in such an event, we call it hidden bit).
When considering also the positive de-normalized numbers, we should complete
the above set by adding the following numbers
(.011)2 ¬∑ 2‚àí1 = 3
16,
(.010)2 ¬∑ 2‚àí1 = 1
8,
(.001)2 ¬∑ 2‚àí1 = 1
16.
According to what previously stated, the smallest de-normalized number is Œ≤L‚àít =
2‚àí1‚àí3 = 1/16.
‚Ä¢

2.5 Machine Representation of Numbers
49
2.5.3
Distribution of Floating-point Numbers
The Ô¨Çoating-point numbers are not equally spaced along the real line, but
they get dense close to the smallest representable number. It can be checked
that the spacing between a number x ‚ààF and its next nearest y ‚ààF, where
both x and y are assumed to be non null, is at least Œ≤‚àí1œµM|x| and at most
œµM|x|, being œµM = Œ≤1‚àít the machine epsilon. This latter represents the
distance between the number 1 and the nearest Ô¨Çoating-point number, and
therefore it is the smallest number of F such that 1 + œµM > 1.
Having instead Ô¨Åxed an interval of the form [Œ≤e, Œ≤e+1], the numbers of F
that belong to such an interval are equally spaced and have distance equal
to Œ≤e‚àít. Decreasing (or increasing) by one the exponent gives rise to a
decrement (or increment) of a factor Œ≤ of the distance between consecutive
numbers.
Unlike the absolute distance, the relative distance between two consecu-
tive numbers has a periodic behavior which depends only on the mantissa
m. Indeed, denoting by (‚àí1)sm(x)Œ≤e‚àít one of the two numbers, the dis-
tance ‚àÜx from the successive one is equal to (‚àí1)sŒ≤e‚àít, which implies that
the relative distance is
‚àÜx
x
=
(‚àí1)sŒ≤e‚àít
(‚àí1)sm(x)Œ≤e‚àít =
1
m(x).
(2.31)
Within the interval [Œ≤e, Œ≤e+1], the ratio in (2.31) is decreasing as x increases
since in the normalized representation the mantissa varies from Œ≤t‚àí1 to Œ≤t‚àí
1 (not included). However, as soon as x = Œ≤e+1, the relative distance gets
back to the value Œ≤‚àít+1 and starts decreasing on the successive intervals,
as shown in Figure 2.2. This oscillatory phenomenon is called wobbling
precision and the greater the base Œ≤, the more pronounced the eÔ¨Äect. This
is another reason why small bases are preferably employed in computers.
2.5.4
IEC/IEEE Arithmetic
The possibility of building sets of Ô¨Çoating-point numbers that diÔ¨Äer in base,
number of signiÔ¨Åcant digits and range of the exponent has prompted in the
past the development, for almost any computer, of a particular system F. In
order to avoid this proliferation of numerical systems, a standard has been
Ô¨Åxed that is nowadays almost universally accepted. This standard was de-
veloped in 1985 by the Institute of Electrical and Electronics Engineers
(shortly, IEEE) and was approved in 1989 by the International Electroni-
cal Commission (IEC) as the international standard IEC559 and it is now
known by this name (IEC is an organization analogue to the International
Standardization Organization (ISO) in the Ô¨Åeld of electronics). The stan-
dard IEC559 endorses two formats for the Ô¨Çoating-point numbers: a basic
format, made by the system F(2, 24, ‚àí125, 128) for the single precision,
and by F(2, 53, ‚àí1021, 1024) for the double precision, both including the

50
2. Principles of Numerical Mathematics
2
-126
2
-125
2
-124
2
-123
2
2
-24
-23
FIGURE
2.2.
Variation
of
relative
distance
for
the
set
of
numbers
F(2, 24, ‚àí125, 128) IEC/IEEE in single precision
de-normalized numbers, and an extended format, for which only the main
limitations are Ô¨Åxed (see Table 2.1).
single
double
single
double
N
‚â•43 bits
‚â•79 bits
t
‚â•32
‚â•64
L
‚â§‚àí1021
‚â§16381
U
‚â•1024
‚â•16384
TABLE 2.1. Lower or upper limits in the standard IEC559 for the extended
format of Ô¨Çoating-point numbers
Almost all the computers nowadays satisfy the requirements above. We
summarize in Table 2.2 the special codings that are used in IEC559 to
deal with the values ¬±0, ¬±‚àûand with the so-called non numbers (shortly,
NaN, that is not a number), which correspond for instance to 0/0 or to
other exceptional operations.
value
exponent
mantissa
¬±0
L ‚àí1
0
¬±‚àû
U + 1
0
NaN
U + 1
Ã∏= 0
TABLE 2.2. IEC559 codings of some exceptional values
2.5.5
Rounding of a Real Number in its Machine
Representation
The fact that on any computer only a subset F(Œ≤, t, L, U) of R is actually
available poses several practical problems, Ô¨Årst of all the representation in F

2.5 Machine Representation of Numbers
51
of any given real number. To this concern, notice that, even if x and y were
two numbers in F, the result of an operation on them does not necessarily
belong to F. Therefore, we must deÔ¨Åne an arithmetic also on F.
The simplest approach to solve the Ô¨Årst problem consists of rounding
x ‚ààR in such a way that the rounded number belongs to F. Among all
the possible rounding operations, let us consider the following one. Given
x ‚ààR in the normalized positional notation (2.29) let us substitute x by
its representant fl(x) in F, deÔ¨Åned as
fl(x) = (‚àí1)s(0. a1a2 . . . $at) ¬∑ Œ≤e,
Àúat =
%
at
if at+1 < Œ≤/2
at + 1
if at+1 ‚â•Œ≤/2. (2.32)
The mapping fl : R ‚ÜíF is the most commonly used and is called rounding
(in the chopping one would take more trivially $at = at). Clearly, fl(x) = x
if x ‚ààF and moreover fl(x) ‚â§fl(y) if x ‚â§y ‚àÄx, y ‚ààR (monotonicity
property).
Remark 2.2 (OverÔ¨Çow and underÔ¨Çow) Everything written so far holds
only for the numbers that in (2.29) have exponent e within the range of F.
If, indeed, x ‚àà(‚àí‚àû, ‚àíxmax) ‚à™(xmax, ‚àû) the value fl(x) is not deÔ¨Åned,
while if x ‚àà(‚àíxmin, xmin) the operation of rounding is deÔ¨Åned anyway
(even in absence of de-normalized numbers). In the Ô¨Årst case, if x is the
result of an operation on numbers of F, we speak about overÔ¨Çow, in the
second case about underÔ¨Çow (or graceful underÔ¨Çow if de-normalized num-
bers are accounted for). The overÔ¨Çow is handled by the system through an
interrupt of the executing program.
‚ñ†
Apart from exceptional situations, we can easily quantify the error, ab-
solute and relative, that is made by substituting fl(x) for x. The following
result can be shown (see for instance [Hig96], Theorem 2.2).
Property 2.1 If x ‚ààR is such that xmin ‚â§|x| ‚â§xmax, then
fl(x) = x(1 + Œ¥) with |Œ¥| ‚â§u
(2.33)
where
u = 1
2Œ≤1‚àít = 1
2œµM
(2.34)
is the so-called roundoÔ¨Äunit (or machine precision).
As a consequence of (2.33), the following bound holds for the relative error
Erel(x) = |x ‚àífl(x)|
|x|
‚â§u,
(2.35)
while, for the absolute error, one gets
E(x) = |x ‚àífl(x)| ‚â§Œ≤e‚àít|(a1 . . . at.at+1 . . . ) ‚àí(a1 . . . Àúat)|.

52
2. Principles of Numerical Mathematics
From (2.32), it follows that
|(a1 . . . at.at+1 . . . ) ‚àí(a1 . . . Àúat)| ‚â§Œ≤‚àí1 Œ≤
2 ,
from which
E(x) ‚â§1
2Œ≤‚àít+e.
Remark 2.3 In the MATLAB environment it is possible to know imme-
diately the value of œµM, which is given by the system variable eps.
‚ñ†
2.5.6
Machine Floating-point Operations
As previously stated, it is necessary to deÔ¨Åne on the set of machine numbers
an arithmetic which is analogous, as far as possible, to the arithmetic in R.
Thus, given any arithmetic operation ‚ó¶: R √ó R ‚ÜíR on two operands in
R (the symbol ‚ó¶may denote sum, subtraction, multiplication or division),
we shall denote by ‚ó¶the corresponding machine operation
‚ó¶: R √ó R ‚ÜíF,
x ‚ó¶y = fl(fl(x) ‚ó¶fl(y)).
From the properties of Ô¨Çoating-point numbers one could expect that for the
operations on two operands, whenever well deÔ¨Åned, the following property
holds: ‚àÄx, y ‚ààF, ‚àÉŒ¥ ‚ààR such that
x ‚ó¶y = (x ‚ó¶y)(1 + Œ¥)
with |Œ¥| ‚â§u.
(2.36)
In order for (2.36) to be satisÔ¨Åed when ‚ó¶is the operator of subtraction, it
will require an additional assumption on the structure of the numbers in F,
that is the presence of the so-called round digit (which is addressed at the
end of this section). In particular, when ‚ó¶is the sum operator, it follows
that for all x, y ‚ààF (see Exercise 11)
|x + y ‚àí(x + y)|
|x + y|
‚â§u(1 + u)|x| + |y|
|x + y| + u,
(2.37)
so that the relative error associated with every machine operation will be
small, unless x + y is not small by itself. An aside comment is deserved by
the case of the sum of two numbers close in module, but opposite in sign.
In fact, in such a case x+y can be quite small, this generating the so-called
cancellation errors (as evidenced in Example 2.6).
It is important to notice that, together with properties of standard arith-
metic that are preserved when passing to Ô¨Çoating-point arithmetic (like, for
instance, the commutativity of the sum of two addends, or the product of

2.5 Machine Representation of Numbers
53
two factors), other properties are lost. An example is given by the associa-
tivity of sum: it can indeed be shown (see Exercise 12) that in general
x + (y + z) Ã∏= (x + y) + z.
We shall denote by Ô¨Çop the single elementary Ô¨Çoating-point operation (sum,
subtraction, multiplication or division) (the reader is warned that in some
texts Ô¨Çop identiÔ¨Åes an operation of the form a + b ¬∑ c). According to the
previous convention, a scalar product between two vectors of length n will
require 2n ‚àí1 Ô¨Çops, a product matrix-vector 2(m ‚àí1)n Ô¨Çops if the matrix
is n √ó m and Ô¨Ånally, a product matrix-matrix 2(r ‚àí1)mn Ô¨Çops if the two
matrices are m √ó r and r √ó n respectively.
Remark 2.4 (IEC559 arithmetic) The IEC559 standard also deÔ¨Ånes a
closed arithmetic on F, this meaning that any operation on it produces
a result that can be represented within the system itself, although not
necessarily being expected from a pure mathematical standpoint. As an
example, in Table 2.3 we report the results that are obtained in exceptional
situations.
exception
examples
result
non valid operation
0/0, 0 ¬∑ ‚àû
NaN
overflow
¬±‚àû
division by zero
1/0
¬±‚àû
underflow
subnormal numbers
TABLE 2.3. Results for some exceptional operations
The presence of a NaN (Not a Number) in a sequence of operations au-
tomatically implies that the result is a NaN. General acceptance of this
standard is still ongoing.
‚ñ†
We mention that not all the Ô¨Çoating-point systems satisfy (2.36). One of
the main reasons is the absence of the round digit in subtraction, that is,
an extra-bit that gets into action on the mantissa level when the subtrac-
tion between two Ô¨Çoating-point numbers is performed. To demonstrate the
importance of the round digit, let us consider the following example with
a system F having Œ≤ = 10 and t = 2. Let us subtract 1 and 0.99. We have
101 ¬∑ 0.1
101 ¬∑ 0.10
100 ¬∑ 0.99
‚áí
101 ¬∑ 0.09
101 ¬∑ 0.01
‚àí‚Üí
100 ¬∑ 0.10
that is, the result diÔ¨Äers from the exact one by a factor 10. If we now
execute the same subtraction using the round digit, we obtain the exact

54
2. Principles of Numerical Mathematics
result. Indeed
101 ¬∑ 0.1
101 ¬∑ 0.10
100 ¬∑ 0.99
‚áí
101 ¬∑ 0.09 9
101 ¬∑ 0.00 1
‚àí‚Üí
100 ¬∑ 0.01
In fact, it can be shown that addition and subtraction, if executed without
round digit, do not satisfy the property
fl(x ¬± y) = (x ¬± y)(1 + Œ¥) with |Œ¥| ‚â§u,
but the following one
fl(x ¬± y) = x(1 + Œ±) ¬± y(1 + Œ≤) with |Œ±| + |Œ≤| ‚â§u.
An arithmetic for which this latter event happens is called aberrant. In some
computers the round digit does not exist, most of the care being spent on
velocity in the computation. Nowadays, however, the trend is to use even
two round digits (see [HP94] for technical details about the subject).
2.6
Exercises
1. Use (2.7) to compute the condition number K(d) of the following expres-
sions
(1)
x ‚àíad = 0, a > 0
(2)
d ‚àíx + 1 = 0,
d being the datum, a a parameter and x the ‚Äúunknown‚Äù.
[Solution : (1) K(d) ‚âÉ|d|| log a|, (2) K(d) = |d|/|d + 1|.]
2. Study the well posedness and the conditioning in the inÔ¨Ånity norm of the
following problem as a function of the datum d: Ô¨Ånd x and y such that
% x + dy = 1,
dx + y = 0.
[Solution : the given problem is a linear system whose matrix is A =
 1
d
d
1

. It is well-posed if A is nonsingular, i.e., if d Ã∏= ¬±1. In such a
case, K‚àû(A) = |(|d| + 1)/(|d| ‚àí1)|.]
3. Study the conditioning of the solving formula x¬± = ‚àíp ¬±

p2 + q for
the second degree equation x2 + 2px ‚àíq with respect to changes in the
parameters p and q separately.
[Solution : K(p) = |p|/

p2 + q, K(q) = |q|/(2|x¬±|

p2 + q).]
4. Consider the following Cauchy problem
% x‚Ä≤(t) = x0eat (a cos(t) ‚àísin(t)) ,
t > 0
x(0) = x0
(2.38)

2.6 Exercises
55
whose solution is x(t) = x0eat cos(t) (a is a given real number). Study the
conditioning of (2.38) with respect to the choice of the initial datum and
check that on unbounded intervals it is well conditioned if a < 0, while it
is ill conditioned if a > 0.
[Hint : consider the deÔ¨Ånition of Kabs(a).]
5. Let x Ã∏= 0 be an approximation of a non null quantity x. Find the relation
between the relative error œµ = |x ‚àíx|/|x| and ÀúE = |x ‚àíx|/|x|.
6. Find a stable formula for evaluating the square root of a complex number.
7. Determine all the elements of the set F = (10, 6, ‚àí9, 9), in both normalized
and de-normalized cases.
8. Consider the set of the de-normalized numbers FD and study the behavior
of the absolute distance and of the relative distance between two of these
numbers. Does the wobbling precision eÔ¨Äect arise again?
[Hint : for these numbers, uniformity in the relative density is lost. As a
consequence, the absolute distance remains constant (equal to Œ≤L‚àít), while
the relative one rapidly grows as x tends to zero.]
9. What is the value of 00 in IEEE arithmetic?
[Solution : ideally, the outcome should be NaN. In practice, IEEE systems
recover the value 1. A motivation of this result can be found in [Gol91].]
10. Show that, due to cancellation errors, the following sequence
I0 = log 6
5,
Ik + 5Ik‚àí1 = 1
k ,
k = 1, 2, . . . , n,
(2.39)
is not well suited to Ô¨Ånite arithmetic computations of the integral In =
 1
0
xn
x + 5dx when n is suÔ¨Éciently large, although it works in inÔ¨Ånite arith-
metic.
[Hint : consider the initial perturbed datum ÀúI0 = I0 + ¬µ0 and study the
propagation of the error ¬µ0 within (2.39).]
11. Prove (2.37).
[Solution : notice that
|x + y ‚àí(x + y)|
|x + y|
‚â§
|x + y ‚àí(fl(x) + fl(y))|
|x + y|
+ |fl(x) ‚àíx + fl(y) ‚àíy|
|x + y|
.
Then, use (2.36) and (2.35).]
12. Given x, y, z ‚ààF with x + y, y + z, x + y + z that fall into the range of F,
show that
|(x + y) + z ‚àí(x + y + z)| ‚â§C1 ‚âÉ(2|x + y| + |z|)u
|x + (y + z) ‚àí(x + y + z)| ‚â§C2 ‚âÉ(|x| + 2|y + z|)u.
13. Which among the following approximations of œÄ,
œÄ = 4

1 ‚àí1
3 + 1
5 ‚àí1
7 + 1
9 ‚àí. . .

,
œÄ = 6

0.5 + (0.5)3
2 ¬∑ 3 + 3(0.5)5
2 ¬∑ 4 ¬∑ 5 + 3 ¬∑ 5(0.5)7
2 ¬∑ 4 ¬∑ 6 ¬∑ 7 + . . .

(2.40)

56
2. Principles of Numerical Mathematics
better limits the propagation of rounding errors? Compare using MATLAB
the obtained results as a function of the number of the terms in each sum
in (2.40).
14. Analyze the stability, with respect to propagation of rounding errors, of the
following two MATLAB codes to evaluate f(x) = (ex ‚àí1)/x for |x| ‚â™1
% Algorithm 1
if x == 0
f = 1;
else
f = (exp(x) - 1) / x;
end
% Algorithm 2
y = exp (x);
if y == 1
f = 1;
else
f = (y - 1) / log (y);
end
[Solution : the Ô¨Årst algorithm is inaccurate due to cancellation errors, while
the second one (in presence of round digit) is stable and accurate.]
15. In binary arithmetic one can show [Dek71] that the rounding error in the
sum of two numbers a and b, with a ‚â•b, can be computed as
((a + b) ‚àía) ‚àíb).
Based on this property, a method has been proposed, called Kahan com-
pensated sum, to compute the sum of n addends ai in such a way that the
rounding errors are compensated. In practice, letting the initial rounding
error e1 = 0 and s1 = a1, at the i-th step, with i ‚â•2, the algorithm
evaluates yi = xi ‚àíei‚àí1, the sum is updated setting si = si‚àí1 + yi and
the new rounding error is computed as ei = (si ‚àísi‚àí1) ‚àíyi. Implement
this algorithm in MATLAB and check its accuracy by evaluating again the
second expression in (2.40).
16. The area A(T) of a triangle T with sides a, b and c, can be computed using
the following formula
A(T) =

p(p ‚àía)(p ‚àíb)(p ‚àíc),
where p is half the perimeter of T. Show that in the case of strongly de-
formed triangles (a ‚âÉb + c), this formula lacks accuracy and check this
experimentally.

3
Direct Methods for the Solution of
Linear Systems
A system of m linear equations in n unknowns consists of a set of algebraic
relations of the form
n

j=1
aijxj = bi,
i = 1, . . . , m
(3.1)
where xj are the unknowns, aij are the coeÔ¨Écients of the system and bi
are the components of the right hand side. System (3.1) can be more con-
veniently written in matrix form as
Ax = b,
(3.2)
where we have denoted by A = (aij) ‚ààCm√ón the coeÔ¨Écient matrix, by
b=(bi) ‚ààCm the right side vector and by x=(xi) ‚ààCn the unknown
vector, respectively. We call a solution of (3.2) any n-tuple of values xi
which satisÔ¨Åes (3.1).
In this chapter we shall be mainly dealing with real-valued square systems
of order n, that is, systems of the form (3.2) with A ‚ààRn√ón and b ‚ààRn.
In such cases existence and uniqueness of the solution of (3.2) are ensured
if one of the following (equivalent) hypotheses holds:
1. A is invertible;
2. rank(A)=n;
3. the homogeneous system Ax=0 admits only the null solution.

58
3. Direct Methods for the Solution of Linear Systems
The solution of system (3.2) is formally provided by Cramer‚Äôs rule
xj =
‚àÜj
det(A),
j = 1, . . . , n,
(3.3)
where ‚àÜj is the determinant of the matrix obtained by substituting the
j-th column of A with the right hand side b. This formula is, however,
of little practical use. Indeed, if the determinants are evaluated by the
recursive relation (1.4), the computational eÔ¨Äort of Cramer‚Äôs rule is of the
order of (n + 1)! Ô¨Çops and therefore turns out to be unacceptable even for
small dimensions of A (for instance, a computer able to perform 109 Ô¨Çops
per second would take 9.6 ¬∑ 1047 years to solve a linear system of only 50
equations).
For this reason, numerical methods that are alternatives to Cramer‚Äôs rule
have been developed. They are called direct methods if they yield the so-
lution of the system in a Ô¨Ånite number of steps, iterative if they require
(theoretically) an inÔ¨Ånite number of steps. Iterative methods will be ad-
dressed in the next chapter. We notice from now on that the choice between
a direct and an iterative method does not depend only on the theoretical ef-
Ô¨Åciency of the scheme, but also on the particular type of matrix, on memory
storage requirements and, Ô¨Ånally, on the architecture of the computer.
3.1
Stability Analysis of Linear Systems
Solving a linear system by a numerical method invariably leads to the
introduction of rounding errors. Only using stable numerical methods can
keep away the propagation of such errors from polluting the accuracy of the
solution. In this section two aspects of stability analysis will be addressed.
Firstly, we will analyze the sensitivity of the solution of (3.2) to changes
in the data A and b (forward a priori analysis). Secondly, assuming that
an approximate solution x of (3.2) is available, we shall quantify the per-
turbations on the data A and b in order for x to be the exact solution
of a perturbed system (backward a priori analysis). The size of these per-
turbations will in turn allow us to measure the accuracy of the computed
solution x by the use of a posteriori analysis.
3.1.1
The Condition Number of a Matrix
The condition number of a matrix A ‚ààCn√ón is deÔ¨Åned as
K(A) = ‚à•A‚à•‚à•A‚àí1‚à•,
(3.4)
where ‚à•¬∑ ‚à•is an induced matrix norm. In general K(A) depends on the
choice of the norm; this will be made clear by introducing a subscript

3.1 Stability Analysis of Linear Systems
59
into the notation, for instance, K‚àû(A) = ‚à•A‚à•‚àû‚à•A‚àí1‚à•‚àû. More generally,
Kp(A) will denote the condition number of A in the p-norm. Remarkable
instances are p = 1, p = 2 and p = ‚àû(we refer to Exercise 1 for the
relations among K1(A), K2(A) and K‚àû(A)).
As already noticed in Example 2.3, an increase in the condition number
produces a higher sensitivity of the solution of the linear system to changes
in the data. Let us start by noticing that K(A) ‚â•1 since
1 = ‚à•AA‚àí1‚à•‚â§‚à•A‚à•‚à•A‚àí1‚à•= K(A).
Moreover, K(A‚àí1) = K(A) and ‚àÄŒ± ‚ààC with Œ± Ã∏= 0, K(Œ±A) = K(A).
Finally, if A is orthogonal, K2(A) = 1 since ‚à•A‚à•2 =

œÅ(AT A) =

œÅ(I) = 1
and A‚àí1 = AT . The condition number of a singular matrix is set equal to
inÔ¨Ånity.
For p = 2, K2(A) can be characterized as follows. Starting from (1.21),
it can be proved that
K2(A) = ‚à•A‚à•2 ‚à•A‚àí1‚à•2 = œÉ1(A)
œÉn(A)
where œÉ1(A) and œÉn(A) are the maximum and minimum singular values of
A (see Property 1.7). As a consequence, in the case of symmetric positive
deÔ¨Ånite matrices we have
K2(A) = Œªmax
Œªmin
= œÅ(A)œÅ(A‚àí1)
(3.5)
where Œªmax and Œªmin are the maximum and minimum eigenvalues of A.
To check (3.5), notice that
‚à•A‚à•2 =

œÅ(AT A) =

œÅ(A2) =

Œª2max = Œªmax.
Moreover, since Œª(A‚àí1) = 1/Œª(A), one gets ‚à•A‚àí1‚à•2 = 1/Œªmin from which
(3.5) follows. For that reason, K2(A) is called spectral condition number.
Remark 3.1 DeÔ¨Åne the relative distance of A ‚ààCn√ón from the set of
singular matrices with respect to the p-norm by
distp(A) = min
%‚à•Œ¥A‚à•p
‚à•A‚à•p
: A + Œ¥A is singular
&
.
It can then be shown that ([Kah66], [Gas83])
distp(A) =
1
Kp(A).
(3.6)
Equation (3.6) suggests that a matrix A with a high condition number
can behave like a singular matrix of the form A+Œ¥A. In other words, null

60
3. Direct Methods for the Solution of Linear Systems
perturbations in the right hand side do not necessarily yield non vanishing
changes in the solution since, if A+Œ¥A is singular, the homogeneous system
(A + Œ¥A)z = 0 does no longer admit only the null solution. From (3.6) it
also follows that if A+Œ¥A is nonsingular then
‚à•Œ¥A‚à•p‚à•A‚à•p < 1.
(3.7)
‚ñ†
Relation (3.6) seems to suggest that a natural candidate for measuring
the ill-conditioning of a matrix is its determinant, since from (3.3) one is
prompted to conclude that small determinants mean nearly-singular matri-
ces. However this conclusion is wrong, as there exist examples of matrices
with small (respectively, high) determinants and small (respectively, high)
condition numbers (see Exercise 2).
3.1.2
Forward a priori Analysis
In this section we introduce a measure of the sensitivity of the system to
changes in the data. These changes will be interpreted in Section 3.10 as
being the eÔ¨Äects of rounding errors induced by the numerical method used
to solve the system. For a more comprehensive analysis of the subject we
refer to [Dat95], [GL89], [Ste73] and [Var62].
Due to rounding errors, a numerical method for solving (3.2) does not
provide the exact solution but only an approximate one, which satisÔ¨Åes a
perturbed system. In other words, a numerical method yields an (exact)
solution x + Œ¥x of the perturbed system
(A + Œ¥A)(x + Œ¥x) = b + Œ¥b.
(3.8)
The next result provides an estimate of Œ¥x in terms of Œ¥A and Œ¥b.
Theorem 3.1 Let A ‚ààRn√ón be a nonsingular matrix and Œ¥A ‚ààRn√ón be
such that (3.7) is satisÔ¨Åed for a matrix norm ‚à•¬∑ ‚à•. Then, if x‚ààRn is the
solution of Ax=b with b ‚ààRn (b Ã∏= 0) and Œ¥x ‚ààRn satisÔ¨Åes (3.8) for
Œ¥b ‚ààRn,
‚à•Œ¥x‚à•
‚à•x‚à•‚â§
K(A)
1 ‚àíK(A)‚à•Œ¥A‚à•/‚à•A‚à•
‚à•Œ¥b‚à•
‚à•b‚à•+ ‚à•Œ¥A‚à•
‚à•A‚à•

.
(3.9)
Proof. From (3.7) it follows that the matrix A‚àí1Œ¥A has norm less than 1. Then,
due to Theorem 1.5, I + A‚àí1Œ¥A is invertible and from (1.26) it follows that
‚à•(I + A‚àí1Œ¥A)‚àí1‚à•‚â§
1
1 ‚àí‚à•A‚àí1Œ¥A‚à•‚â§
1
1 ‚àí‚à•A‚àí1‚à•‚à•Œ¥A‚à•.
(3.10)
On the other hand, solving for Œ¥x in (3.8) and recalling that Ax = b, one gets
Œ¥x = (I + A‚àí1Œ¥A)‚àí1A‚àí1(Œ¥b ‚àíŒ¥Ax),

3.1 Stability Analysis of Linear Systems
61
from which, passing to the norms and using (3.10), it follows that
‚à•Œ¥x‚à•‚â§
‚à•A‚àí1‚à•
1 ‚àí‚à•A‚àí1‚à•‚à•Œ¥A‚à•(‚à•Œ¥b‚à•+ ‚à•Œ¥A‚à•‚à•x‚à•) .
Finally, dividing both sides by ‚à•x‚à•(which is nonzero since b Ã∏= 0 and A is
nonsingular) and noticing that ‚à•x‚à•‚â•‚à•b‚à•/‚à•A‚à•, the result follows.
3
Well-conditioning alone is not enough to yield an accurate solution of the
linear system. It is indeed crucial, as pointed out in Chapter 2, to resort to
stable algorithms. Conversely, ill-conditioning does not necessarily exclude
that for particular choices of the right side b the overall conditioning of the
system is good (see Exercise 4).
A particular case of Theorem 3.1 is the following.
Theorem 3.2 Assume that the conditions of Theorem 3.1 hold and let
Œ¥A = 0. Then
1
K(A)
‚à•Œ¥b‚à•
‚à•b‚à•‚â§‚à•Œ¥x‚à•
‚à•x‚à•‚â§K(A)‚à•Œ¥b‚à•
‚à•b‚à•.
(3.11)
Proof. We will prove only the Ô¨Årst inequality since the second one directly
follows from (3.9). Relation Œ¥x = A‚àí1Œ¥b yields ‚à•Œ¥b‚à•‚â§‚à•A‚à•‚à•Œ¥x‚à•. Multiplying
both sides by ‚à•x‚à•and recalling that ‚à•x‚à•‚â§‚à•A‚àí1‚à•‚à•b‚à•it follows that ‚à•x‚à•‚à•Œ¥b‚à•‚â§
K(A)‚à•b‚à•‚à•Œ¥x‚à•, which is the desired inequality.
3
In order to employ the inequalities (3.10) and (3.11) in the analysis of
propagation of rounding errors in the case of direct methods, ‚à•Œ¥A‚à•and
‚à•Œ¥b‚à•should be bounded in terms of the dimension of the system and of
the characteristics of the Ô¨Çoating-point arithmetic that is being used.
It is indeed reasonable to expect that the perturbations induced by a
method for solving a linear system are such that ‚à•Œ¥A‚à•‚â§Œ≥‚à•A‚à•and ‚à•Œ¥b‚à•‚â§
Œ≥‚à•b‚à•, Œ≥ being a positive number that depends on the roundoÔ¨Äunit u (for
example, we shall assume henceforth that Œ≥ = Œ≤1‚àít, where Œ≤ is the base
and t is the number of digits of the mantissa of the Ô¨Çoating-point system
F). In such a case (3.9) can be completed by the following theorem.
Theorem 3.3 Assume that ‚à•Œ¥A‚à•‚â§Œ≥‚à•A‚à•, ‚à•Œ¥b‚à•‚â§Œ≥‚à•b‚à•with Œ≥ ‚ààR+ and
Œ¥A ‚ààRn√ón, Œ¥b ‚ààRn. Then, if Œ≥K(A) < 1 the following inequalities hold
‚à•x + Œ¥x‚à•
‚à•x‚à•
‚â§1 + Œ≥K(A)
1 ‚àíŒ≥K(A),
(3.12)
‚à•Œ¥x‚à•
‚à•x‚à•‚â§
2Œ≥
1 ‚àíŒ≥K(A)K(A).
(3.13)

62
3. Direct Methods for the Solution of Linear Systems
Proof. From (3.8) it follows that (I + A‚àí1Œ¥A)(x + Œ¥x) = x + A‚àí1Œ¥b. Moreover,
since Œ≥K(A) < 1 and ‚à•Œ¥A‚à•‚â§Œ≥‚à•A‚à•it turns out that I + A‚àí1Œ¥A is nonsingular.
Taking the inverse of such a matrix and passing to the norms we get ‚à•x + Œ¥x‚à•‚â§
‚à•(I + A‚àí1Œ¥A)‚àí1‚à•

‚à•x‚à•+ Œ≥‚à•A‚àí1‚à•‚à•b‚à•

. From Theorem 1.5 it then follows that
‚à•x + Œ¥x‚à•‚â§
1
1 ‚àí‚à•A‚àí1Œ¥A‚à•

‚à•x‚à•+ Œ≥‚à•A‚àí1‚à•‚à•b‚à•

,
which implies (3.12), since ‚à•A‚àí1Œ¥A‚à•‚â§Œ≥K(A) and ‚à•b‚à•‚â§‚à•A‚à•‚à•x‚à•.
Let us prove (3.13). Subtracting (3.2) from (3.8) it follows that
AŒ¥x = ‚àíŒ¥A(x + Œ¥x) + Œ¥b.
Inverting A and passing to the norms, the following inequality is obtained
‚à•Œ¥x‚à•
‚â§
‚à•A‚àí1Œ¥A‚à•‚à•x + Œ¥x‚à•+ ‚à•A‚àí1‚à•‚à•Œ¥b‚à•
‚â§
Œ≥K(A)‚à•x + Œ¥x‚à•+ Œ≥‚à•A‚àí1‚à•‚à•b‚à•.
(3.14)
Dividing both sides by ‚à•x‚à•and using the triangular inequality ‚à•x+Œ¥x‚à•‚â§‚à•Œ¥x‚à•+
‚à•x‚à•, we Ô¨Ånally get (3.13).
3
Remarkable instances of perturbations Œ¥A and Œ¥b are those for which
|Œ¥A| ‚â§Œ≥|A| and |Œ¥b| ‚â§Œ≥|b| with Œ≥ ‚â•0. Hereafter, the absolute value
notation B = |A| denotes the matrix n √ó n having entries bij = |aij| with
i, j = 1, . . . , n and the inequality C ‚â§D, with C, D ‚ààRm√ón has the
following meaning
cij ‚â§dij for i = 1, . . . , m, j = 1, . . . , n.
If ‚à•¬∑ ‚à•‚àûis considered, from (3.14) it follows that
‚à•Œ¥x‚à•‚àû
‚à•x‚à•‚àû
‚â§Œ≥ ‚à•|A‚àí1| |A| |x| + |A‚àí1| |b| ‚à•‚àû
(1 ‚àíŒ≥‚à•|A‚àí1| |A| ‚à•‚àû)‚à•x‚à•‚àû
‚â§
2Œ≥
1 ‚àíŒ≥‚à•|A‚àí1| |A| ‚à•‚àû
‚à•|A‚àí1| |A| ‚à•‚àû.
(3.15)
Estimate (3.15) is generally too pessimistic; however, the following compo-
nentwise error estimates of Œ¥x can be derived from (3.15)
|Œ¥xi| ‚â§Œ≥|rT
(i)| |A| |x + Œ¥x|,
i = 1, . . . , n
if Œ¥b = 0,
|Œ¥xi|
|xi| ‚â§Œ≥
|rT
(i)| |b|
|rT
(i)b| ,
i = 1, . . . , n
if Œ¥A = 0,
(3.16)
being rT
(i) the row vector eT
i A‚àí1. Estimates (3.16) are more stringent than
(3.15), as can be seen in Example 3.1. The Ô¨Årst inequality in (3.16) can be
used when the perturbed solution x+Œ¥x is known, being henceforth x+Œ¥x
the solution computed by a numerical method.

3.1 Stability Analysis of Linear Systems
63
In the case where |A‚àí1| |b| = |x|, the parameter Œ≥ in (3.15) is equal
to 1. For such systems the components of the solution are insensitive to
perturbations to the right side. A slightly worse situation occurs when A
is a triangular M-matrix and b has positive entries. In such a case Œ≥ is
bounded by 2n ‚àí1, since
|rT
(i)| |A| |x| ‚â§(2n ‚àí1)|xi|.
For further details on the subject we refer to [Ske79], [CI95] and [Hig89].
Results linking componentwise estimates to normwise estimates through
the so-called hypernorms can be found in [ADR92].
Example 3.1 Consider the linear system Ax=b with
A =
Ô£Æ
Ô£∞
Œ±
1
Œ±
0
1
Œ±
Ô£π
Ô£ª,
b =
Ô£Æ
Ô£∞
Œ±2 + 1
Œ±
1
Œ±
Ô£π
Ô£ª
which has solution xT = (Œ±, 1), where 0 < Œ± < 1. Let us compare the results
obtained using (3.15) and (3.16). From
|A‚àí1| |A| |x| = |A‚àí1| |b| =

Œ± + 2
Œ±2 , 1
T
(3.17)
it follows that the supremum of (3.17) is unbounded as Œ± ‚Üí0, exactly as it
happens in the case of ‚à•A‚à•‚àû. On the other hand, the ampliÔ¨Åcation factor of
the error in (3.16) is bounded. Indeed, the component of the maximum absolute
value, x2, of the solution, satisÔ¨Åes |rT
(2)| |A| |x|/|x2| = 1.
‚Ä¢
3.1.3
Backward a priori Analysis
The numerical methods that we have considered thus far do not require the
explicit computation of the inverse of A to solve Ax=b. However, we can
always assume that they yield an approximate solution of the form x = Cb,
where the matrix C, due to rounding errors, is an approximation of A‚àí1.
In practice, C is very seldom constructed; in case this should happen, the
following result yields an estimate of the error that is made substituting C
for A‚àí1 (see [IK66], Chapter 2, Theorem 7).
Property 3.1 Let R = AC ‚àíI; if ‚à•R‚à•< 1, then A and C are nonsingular
and
‚à•A‚àí1‚à•‚â§
‚à•C‚à•
1 ‚àí‚à•R‚à•,
‚à•R‚à•
‚à•A‚à•‚â§‚à•C ‚àíA‚àí1‚à•‚â§‚à•C‚à•‚à•R‚à•
1 ‚àí‚à•R‚à•.
(3.18)
In the frame of backward a priori analysis we can interpret C as being the
inverse of A + Œ¥A (for a suitable unknown Œ¥A). We are thus assuming that
C(A + Œ¥A) = I. This yields
Œ¥A = C‚àí1 ‚àíA = ‚àí(AC ‚àíI)C‚àí1 = ‚àíRC‚àí1

64
3. Direct Methods for the Solution of Linear Systems
and, as a consequence, if ‚à•R‚à•< 1 it turns out that
‚à•Œ¥A‚à•‚â§‚à•R‚à•‚à•A‚à•
1 ‚àí‚à•R‚à•,
(3.19)
having used the Ô¨Årst inequality in (3.18), where A is assumed to be an
approximation of the inverse of C (notice that the roles of C and A can be
interchanged).
3.1.4
A posteriori Analysis
Having approximated the inverse of A by a matrix C turns into having an
approximation of the solution of the linear system (3.2). Let us denote by
y a known approximate solution. The aim of the a posteriori analysis is to
relate the (unknown) error e = y ‚àíx to quantities that can be computed
using y and C.
The starting point of the analysis relies on the fact that the residual
vector r = b ‚àíAy is in general nonzero, since y is just an approximation
to the unknown exact solution. The residual can be related to the error
through Property 3.1 as follows. We have e = A‚àí1(Ay ‚àíb) = ‚àíA‚àí1r and
thus, if ‚à•R‚à•< 1 then
‚à•e‚à•‚â§‚à•r‚à•‚à•C‚à•
1 ‚àí‚à•R‚à•.
(3.20)
Notice that the estimate does not necessarily require y to coincide with
the solution x = Cb of the backward a priori analysis. One could therefore
think of computing C only for the purpose of using the estimate (3.20) (for
instance, in the case where (3.2) is solved through the Gauss elimination
method, one can compute C a posteriori using the LU factorization of A,
see Sections 3.3 and 3.3.1).
We conclude by noticing that if Œ¥b is interpreted in (3.11) as being the
residual of the computed solution y = x + Œ¥x, it also follows that
‚à•e‚à•
‚à•x‚à•‚â§K(A) ‚à•r‚à•
‚à•b‚à•.
(3.21)
The estimate (3.21) is not used in practice since the computed residual
is aÔ¨Äected by rounding errors. A more signiÔ¨Åcant estimate (in the ‚à•¬∑ ‚à•‚àû
norm) is obtained letting r = fl(b ‚àíAy) and assuming that r = r + Œ¥r
with |Œ¥r| ‚â§Œ≥n+1(|A| |y| + |b|), where Œ≥n+1 = (n + 1)u/(1 ‚àí(n + 1)u) > 0,
from which we have
‚à•e‚à•‚àû
‚à•y‚à•‚àû
‚â§‚à•|A‚àí1|(|r| + Œ≥n+1(|A||y| + |b|))‚à•‚àû
‚à•y‚à•‚àû
.
Formulae like this last one are implemented in the library for linear algebra
LAPACK (see [ABB+92]).

3.2 Solution of Triangular Systems
65
3.2
Solution of Triangular Systems
Consider the nonsingular 3√ó3 lower triangular system
Ô£Æ
Ô£∞
l11
0
0
l21
l22
0
l31
l32
l33
Ô£π
Ô£ª
Ô£Æ
Ô£∞
x1
x2
x3
Ô£π
Ô£ª=
Ô£Æ
Ô£∞
b1
b2
b3
Ô£π
Ô£ª.
Since the matrix is nonsingular, its diagonal entries lii, i = 1, 2, 3, are
non vanishing, hence we can solve sequentially for the unknown values
xi, i = 1, 2, 3 as follows
x1 = b1/l11,
x2 = (b2 ‚àíl21x1)/l22,
x3 = (b3 ‚àíl31x1 ‚àíl32x2)/l33.
This algorithm can be extended to systems n √ó n and is called forward
substitution. In the case of a system Lx=b, with L being a nonsingular
lower triangular matrix of order n (n ‚â•2), the method takes the form
x1 = b1
l11
,
xi = 1
lii
Ô£´
Ô£≠bi ‚àí
i‚àí1

j=1
lijxj
Ô£∂
Ô£∏,
i = 2, . . . , n.
(3.22)
The number of multiplications and divisions to execute the algorithm is
equal to n(n+1)/2, while the number of sums and subtractions is n(n‚àí1)/2.
The global operation count for (3.22) is thus n2 Ô¨Çops.
Similar conclusions can be drawn for a linear system Ux=b, where U
is a nonsingular upper triangular matrix of order n (n ‚â•2). In this case
the algorithm is called backward substitution and in the general case can
be written as
xn = bn
unn
,
xi = 1
uii
Ô£´
Ô£≠bi ‚àí
n

j=i+1
uijxj
Ô£∂
Ô£∏,
i = n ‚àí1, . . . , 1.
(3.23)
Its computational cost is still n2 Ô¨Çops.
3.2.1
Implementation of Substitution Methods
Each i-th step of algorithm (3.22) requires performing the scalar product
between the row vector L(i, 1 : i ‚àí1) (this notation denoting the vector
extracted from matrix L taking the elements of the i-th row from the Ô¨Årst

66
3. Direct Methods for the Solution of Linear Systems
to the (i-1)-th column) and the column vector x(1 : i ‚àí1). The access to
matrix L is thus by row; for that reason, the forward substitution algorithm,
when implemented in the form above, is called row-oriented.
Its coding is reported in Program 1 (the Program mat square that is
called by forward row merely checks that L is a square matrix).
Program 1 - forward row : Forward substitution: row-oriented version
function [x]=forward row(L,b)
[n]=mat square(L);
x(1) = b(1)/L(1,1);
for i = 2:n, x (i) = (b(i)-L(i,1:i-1)*(x(1:i-1))‚Äô)/L(i,i); end
x=x‚Äô;
To obtain a column-oriented version of the same algorithm, we take ad-
vantage of the fact that i-th component of the vector x, once computed,
can be conveniently eliminated from the system.
An implementation of such a procedure, where the solution x is over-
written on the right vector b, is reported in Program 2.
Program 2 - forward col : Forward substitution: column-oriented version
function [b]=forward col(L,b)
[n]=mat square(L);
for j=1:n-1,
b(j)= b(j)/L(j,j); b(j+1:n)=b(j+1:n)-b(j)*L(j+1:n,j);
end; b(n) = b(n)/L(n,n);
Implementing the same algorithm by a row-oriented rather than a column-
oriented approach, might dramatically change its performance (but of course,
not the solution). The choice of the form of implementation must therefore
be subordinated to the speciÔ¨Åc hardware that is used.
Similar considerations hold for the backward substitution method, pre-
sented in (3.23) in its row-oriented version.
In Program 3 only the column-oriented version of the algorithm is coded.
As usual, the vector x is overwritten on b.
Program 3 - backward col : Backward substitution: column-oriented ver-
sion
function [b]=backward col(U,b)
[n]=mat square(U);
for j = n:-1:2,
b(j)=b(j)/U(j,j); b(1:j-1)=b(1:j-1)-b(j)*U(1:j-1,j);
end; b(1) = b(1)/U(1,1);
When large triangular systems must be solved, only the triangular portion
of the matrix should be stored leading to considerable saving of memory
resources.

3.2 Solution of Triangular Systems
67
3.2.2
Rounding Error Analysis
The analysis developed so far has not accounted for the presence of round-
ing errors. When including these, the forward and backward substitution
algorithms no longer yield the exact solutions to the systems Lx=b and
Uy=b, but rather provide approximate solutions x that can be regarded
as being exact solutions to the perturbed systems
(L + Œ¥L)x = b,
(U + Œ¥U)x = b,
where Œ¥L = (Œ¥lij) and Œ¥U = (Œ¥uij) are perturbation matrices. In order
to apply the estimates (3.9) carried out in Section 3.1.2, we must provide
estimates of the perturbation matrices, Œ¥L and Œ¥U, as a function of the
entries of L and U, of their size and of the characteristics of the Ô¨Çoating-
point arithmetic. For this purpose, it can be shown that
|Œ¥T| ‚â§
nu
1 ‚àínu|T|,
(3.24)
where T is equal to L or U, u = 1
2Œ≤1‚àít is the roundoÔ¨Äunit deÔ¨Åned in (2.34).
Clearly, if nu < 1 from (3.24) it turns out that, using a Taylor expansion,
|Œ¥T| ‚â§nu|T| + O(u2). Moreover, from (3.24) and (3.9) it follows that, if
nuK(T) < 1, then
‚à•x ‚àíx‚à•
‚à•x‚à•
‚â§
nuK(T)
1 ‚àínuK(T) = nuK(T) + O(u2)
(3.25)
for the norms ‚à•¬∑ ‚à•1, ‚à•¬∑ ‚à•‚àûand the Frobenius norm. If u is suÔ¨Éciently
small (as typically happens), the perturbations introduced by the rounding
errors in the solution of a triangular system can thus be neglected. As
a consequence, the accuracy of the solution computed by the forward or
backward substitution algorithm is generally very high.
These results can be improved by introducing some additional assump-
tions on the entries of L or U. In particular, if the entries of U are such
that |uii| ‚â•|uij| for any j > i, then
|xi ‚àíxi| ‚â§2n‚àíi+1
nu
1 ‚àínu max
j‚â•i |xj|,
1 ‚â§i ‚â§n.
The same result holds if T=L, provided that |lii| ‚â•|lij| for any j < i, or if
L and U are diagonally dominant. The previous estimates will be employed
in Sections 3.3.1 and 3.4.2.
For the proofs of the results reported so far, see [FM67], [Hig89] and
[Hig88].
3.2.3
Inverse of a Triangular Matrix
The algorithm (3.23) can be employed to explicitly compute the inverse
of an upper triangular matrix. Indeed, given an upper triangular matrix

68
3. Direct Methods for the Solution of Linear Systems
U, the column vectors vi of the inverse V=(v1, . . . , vn) of U satisfy the
following linear systems
Uvi = ei,
i = 1, . . . , n
(3.26)
where {ei} is the canonical basis of Rn (deÔ¨Åned in Example 1.3). Solving
for vi thus requires the application of algorithm (3.23) n times to (3.26).
This procedure is quite ineÔ¨Écient since at least half the entries of the
inverse of U are null. Let us take advantage of this as follows. Denote by
v‚Ä≤
k = (v‚Ä≤
1k, . . . , v‚Ä≤
kk)T the vector of size k such that
U(k)v‚Ä≤
k = lk
k = 1, . . . , n
(3.27)
where U(k) is the principal submatrix of U of order k and lk the vector of
Rk having null entries, except the Ô¨Årst one which is equal to 1. Systems
(3.27) are upper triangular, but have order k and can be again solved using
the method (3.23). We end up with the following inversion algorithm for
upper triangular matrices: for k = n, n ‚àí1, . . . , 1 compute
v‚Ä≤
kk = u‚àí1
kk ,
v‚Ä≤
ik = ‚àíu‚àí1
ii
k

j=i+1
uijv‚Ä≤
jk,
for i = k ‚àí1, k ‚àí2, . . . , 1.
(3.28)
At the end of this procedure the vectors v‚Ä≤
k furnish the non vanishing entries
of the columns of U‚àí1. The algorithm requires about n3/3 + (3/4)n2 Ô¨Çops.
Once again, due to rounding errors, the algorithm (3.28) no longer yields
the exact solution, but an approximation of it. The error that is introduced
can be estimated using the backward a priori analysis carried out in Section
3.1.3.
A similar procedure can be constructed from (3.22) to compute the in-
verse of a lower triangular system.
3.3
The Gaussian Elimination Method (GEM) and
LU Factorization
The Gaussian elimination method aims at reducing the system Ax=b to an
equivalent system (that is, having the same solution) of the form Ux=b,
where U is an upper triangular matrix and b is an updated right side
vector. This latter system can then be solved by the backward substitution
method. Let us denote the original system by A(1)x = b(1). During the
reduction procedure we basically employ the property which states that
replacing one of the equations by the diÔ¨Äerence between this equation and
another one multiplied by a non null constant yields an equivalent system
(i.e., one with the same solution).

3.3 The Gaussian Elimination Method (GEM) and LU Factorization
69
Thus, consider a nonsingular matrix A ‚ààRn√ón, and suppose that the
diagonal entry a11 is non vanishing. Introducing the multipliers
mi1 = a(1)
i1
a(1)
11
,
i = 2, 3, . . . , n,
where a(1)
ij
denote the elements of A(1), it is possible to eliminate the un-
known x1 from the rows other than the Ô¨Årst one by simply subtracting
from row i, with i = 2, . . . , n, the Ô¨Årst row multiplied by mi1 and doing
the same on the right side. If we now deÔ¨Åne
a(2)
ij = a(1)
ij ‚àími1a(1)
1j ,
i, j = 2, . . . , n,
b(2)
i
= b(1)
i
‚àími1b(1)
1 ,
i = 2, . . . , n,
where b(1)
i
denote the components of b(1), we get a new system of the form
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£∞
a(1)
11
a(1)
12
. . .
a(1)
1n
0
a(2)
22
. . .
a(2)
2n
...
...
...
0
a(2)
n2
. . .
a(2)
nn
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
x1
x2
...
xn
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª
=
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£∞
b(1)
1
b(2)
2...
b(2)
n
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
,
which we denote by A(2)x = b(2), that is equivalent to the starting one.
Similarly, we can transform the system in such a way that the unknown
x2 is eliminated from rows 3, . . . , n. In general, we end up with the Ô¨Ånite
sequence of systems
A(k)x = b(k),
1 ‚â§k ‚â§n,
(3.29)
where, for k ‚â•2, matrix A(k) takes the following form
A(k) =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
a(1)
11
a(1)
12
. . .
. . .
. . .
a(1)
1n
0
a(2)
22
a(2)
2n
...
...
...
0
. . .
0
a(k)
kk
. . .
a(k)
kn
...
...
...
...
0
. . .
0
a(k)
nk
. . .
a(k)
nn
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
,

70
3. Direct Methods for the Solution of Linear Systems
having assumed that a(i)
ii Ã∏= 0 for i = 1, . . . , k ‚àí1. It is clear that for k = n
we obtain the upper triangular system A(n)x = b(n)
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
a(1)
11
a(1)
12
. . .
. . .
a(1)
1n
0
a(2)
22
a(2)
2n
...
...
...
0
...
...
0
a(n)
nn
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
x1
x2
...
...
xn
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
=
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
b(1)
1
b(2)
2...
...
b(n)
n
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
Consistently with the notations that have been previously introduced, we
denote by U the upper triangular matrix A(n). The entries a(k)
kk are called
pivots and must obviously be non null for k = 1, . . . , n ‚àí1.
In order to highlight the formulae which transform the k-th system into
the k + 1-th one, for k = 1, . . . , n ‚àí1 we assume that a(k)
kk Ã∏= 0 and deÔ¨Åne
the multiplier
mik = a(k)
ik
a(k)
kk
,
i = k + 1, . . . , n.
(3.30)
Then we let
a(k+1)
ij
= a(k)
ij ‚àímika(k)
kj ,
i, j = k + 1, . . . , n
b(k+1)
i
= b(k)
i
‚àímikb(k)
k ,
i = k + 1, . . . , n.
(3.31)
Example 3.2 Let us use GEM to solve the following system
(A(1)x = b(1))
Ô£±
Ô£¥
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£¥
Ô£≥
x1
+
1
2x2
+
1
3x3
=
11
6
1
2x1
+
1
3x2
+
1
4x3
=
13
12
1
3x1
+
1
4x2
+
1
5x3
=
47
60
,
which admits the solution x=(1, 1, 1)T . At the Ô¨Årst step we compute the mul-
tipliers m21 = 1/2 and m31 = 1/3, and subtract from the second and third
equation of the system the Ô¨Årst row multiplied by m21 and m31, respectively. We
obtain the equivalent system
(A(2)x = b(2))
Ô£±
Ô£¥
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£¥
Ô£≥
x1
+
1
2x2
+
1
3x3
=
11
6
0
+
1
12x2
+
1
12x3
=
1
6
0
+
1
12x2
+
4
45x3
=
31
180
.
If we now subtract the second row multiplied by m32 = 1 from the third one, we
end up with the upper triangular system
(A(3)x = b(3))
Ô£±
Ô£¥
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£¥
Ô£≥
x1
+
1
2x2
+
1
3x3
=
11
6
0
+
1
12x2
+
1
12x3
=
1
6
0
+
0
+
1
180x3
=
1
180
,

3.3 The Gaussian Elimination Method (GEM) and LU Factorization
71
from which we immediately compute x3 = 1 and then, by back substitution, the
remaining unknowns x1 = x2 = 1.
‚Ä¢
Remark 3.2 The matrix in Example 3.2 is called the Hilbert matrix of
order 3. In the general n √ó n case, its entries are
hij = 1/(i + j ‚àí1),
i, j = 1, . . . , n.
(3.32)
As we shall see later on, this matrix provides the paradigm of an ill-
conditioned matrix.
‚ñ†
To complete Gaussian elimination 2(n ‚àí1)n(n + 1)/3 + n(n ‚àí1) Ô¨Çops are
required, plus n2 Ô¨Çops to backsolve the triangular system U x = b(n).
Therefore, about (2n3/3 + 2n2) Ô¨Çops are needed to solve the linear sys-
tem using GEM. Neglecting the lower order terms, we can state that the
Gaussian elimination process has a cost of 2n3/3 Ô¨Çops.
As previously noticed, GEM terminates safely iÔ¨Äthe pivotal elements a(k)
kk ,
for k = 1, . . . , n ‚àí1, are non vanishing. Unfortunately, having non null
diagonal entries in A is not enough to prevent zero pivots to arise during
the elimination process. For example, matrix A in (3.33) is nonsingular and
has nonzero diagonal entries
A =
Ô£Æ
Ô£∞
1
2
3
2
4
5
7
8
9
Ô£π
Ô£ª,
A(2) =
Ô£Æ
Ô£∞
1
2
3
0
0
‚àí1
0
‚àí6
‚àí12
Ô£π
Ô£ª.
(3.33)
Nevertheless, when GEM is applied, it is interrupted at the second step
since a(2)
22 = 0.
More restrictive conditions on A are thus needed to ensure the appli-
cability of the method. We shall see in Section 3.3.1 that if the leading
dominating minors di of A are nonzero for i = 1, . . . , n ‚àí1, then the corre-
sponding pivotal entries a(i)
ii must necessarily be non vanishing. We recall
that di is the determinant of Ai, the i-th principal submatrix made by the
Ô¨Årst i rows and columns of A. The matrix in the previous example does
not satisfy this condition, having d1 = 1 and d2 = 0.
Classes of matrices exist such that GEM can be always safely employed in
its basic form (3.31). Among them, we recall the following ones:
1. matrices diagonally dominant by rows;
2. matrices diagonally dominant by columns. In such a case one can even
show that the multipliers are in module less than or equal to 1 (see
Property 3.2);
3. matrices symmetric and positive deÔ¨Ånite (see Theorem 3.6).
For a rigorous derivation of these results, we refer to the forthcoming sec-
tions.

72
3. Direct Methods for the Solution of Linear Systems
3.3.1
GEM as a Factorization Method
In this section we show how GEM is equivalent to performing a factorization
of the matrix A into the product of two matrices, A=LU, with U=A(n).
Since L and U depend only on A and not on the right hand side, the same
factorization can be reused when solving several linear systems having the
same matrix A but diÔ¨Äerent right hand side b, with a considerable reduction
of the operation count (indeed, the main computational eÔ¨Äort, about 2n3/3
Ô¨Çops, is spent in the elimination procedure).
Let us go back to Example 3.2 concerning the Hilbert matrix H3. In
practice, to pass from A(1)=H3 to the matrix A(2) at the second step, we
have multiplied the system by the matrix
M1 =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
1
0
0
‚àí1
2
1
0
‚àí1
3
0
1
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª=
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
1
0
0
‚àím21
1
0
‚àím31
0
1
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª.
Indeed,
M1A = M1A(1) =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
1
1
2
1
3
0
1
12
1
12
0
1
12
4
45
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª= A(2).
Similarly, to perform the second (and last) step of GEM, we must multiply
A(2) by the matrix
M2 =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
1
0
0
0
1
0
0
‚àí1
1
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª=
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
1
0
0
0
1
0
0
‚àím32
1
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª,
where A(3) = M2A(2). Therefore
M2M1A = A(3) = U.
(3.34)
On the other hand, matrices M1 and M2 are lower triangular, their product
is still lower triangular, as is their inverse; thus, from (3.34) one gets
A = (M2M1)‚àí1U = LU,
which is the desired factorization of A.
This identity can be generalized as follows. Setting
mk = (0, . . . , 0, mk+1,k, . . . , mn,k)T ‚ààRn

3.3 The Gaussian Elimination Method (GEM) and LU Factorization
73
and deÔ¨Åning
Mk =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
1
. . .
0
0
. . .
0
...
...
...
...
...
0
1
0
0
0
‚àímk+1,k
1
0
...
...
...
...
...
...
0
. . .
‚àímn,k
0
. . .
1
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
= In ‚àímkeT
k
as the k-th Gaussian transformation matrix, one Ô¨Ånds out that
(Mk)ip = Œ¥ip ‚àí(mkeT
k )ip = Œ¥ip ‚àímikŒ¥kp,
i, p = 1, . . . , n.
On the other hand, from (3.31) we have that
a(k+1)
ij
= a(k)
ij ‚àímikŒ¥kka(k)
kj =
n

p=1
(Œ¥ip ‚àímikŒ¥kp)a(k)
pj ,
i, j = k + 1, . . . , n,
or, equivalently,
A(k+1) = MkA(k).
(3.35)
As a consequence, at the end of the elimination process the matrices Mk,
with k = 1, . . . , n ‚àí1, and the matrix U have been generated such that
Mn‚àí1Mn‚àí2 . . . M1A = U.
The matrices Mk are unit lower triangular with inverse given by
M‚àí1
k
= 2In ‚àíMk = In + mkeT
k ,
(3.36)
where (mieT
i )(mjeT
j ) are equal to the null matrix if i Ã∏= j. As a consequence
A
=
M‚àí1
1 M‚àí1
2
. . . M‚àí1
n‚àí1U
=
(In + m1eT
1 )(In + m2eT
2 ) . . . (In + mn‚àí1eT
n‚àí1)U
=

In +
n‚àí1

i=1
mieT
i

U
=
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
1
0
. . .
. . .
0
m21
1
...
...
m32
...
...
...
...
...
0
mn1
mn2
. . .
mn,n‚àí1
1
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
U.
(3.37)

74
3. Direct Methods for the Solution of Linear Systems
DeÔ¨Åning L = (Mn‚àí1Mn‚àí2 . . . M1)‚àí1 = M‚àí1
1
. . . M‚àí1
n‚àí1, it follows that
A = LU.
We notice that, due to (3.37), the subdiagonal entries of L are the multi-
pliers mik produced by GEM, while the diagonal entries are equal to one.
Once the matrices L and U have been computed, solving the linear system
consists only of solving successively the two triangular systems
Ly = b
Ux = y.
The computational cost of the factorization process is obviously the same
as that required by GEM.
The following result establishes a link between the leading dominant
minors of a matrix and its LU factorization induced by GEM.
Theorem 3.4 Let A ‚ààRn√ón. The LU factorization of A with lii = 1 for
i = 1, . . . , n exists and is unique iÔ¨Äthe principal submatrices Ai of A of
order i = 1, . . . , n ‚àí1 are nonsingular.
Proof. The existence of the LU factorization can be proved following the steps
of the GEM. Here we prefer to pursue an alternative approach, which allows for
proving at the same time both existence and uniqueness and that will be used
again in later sections.
Let us assume that the leading minors Ai of A are nonsingular for i = 1, . . . , n‚àí
1 and prove, by induction on i, that under this hypothesis the LU factorization
of A(= An) with lii = 1 for i = 1, . . . , n, exists and is unique.
The property is obviously true if i = 1. Assume therefore that there exists an
unique LU factorization of Ai‚àí1 of the form Ai‚àí1 = L(i‚àí1)U(i‚àí1) with l(i‚àí1)
kk
= 1
for k = 1, . . . , i ‚àí1, and show that there exists an unique factorization also for
Ai. We partition Ai by block matrices as
Ai =
Ô£Æ
Ô£∞
Ai‚àí1
c
dT
aii
Ô£π
Ô£ª
and look for a factorization of Ai of the form
Ai = L(i)U(i) =
Ô£Æ
Ô£∞
L(i‚àí1)
0
lT
1
Ô£π
Ô£ª
Ô£Æ
Ô£∞
U(i‚àí1)
u
0T
uii
Ô£π
Ô£ª,
(3.38)
having also partitioned by blocks the factors L(i) and U(i). Computing the prod-
uct of these two factors and equating by blocks the elements of Ai, it turns out
that the vectors l and u are the solutions to the linear systems L(i‚àí1)u = c,
lT U(i‚àí1) = dT .

3.3 The Gaussian Elimination Method (GEM) and LU Factorization
75
On the other hand, since 0 Ã∏= det(Ai‚àí1) = det(L(i‚àí1))det(U(i‚àí1)), the matrices
L(i‚àí1) and U(i‚àí1) are nonsingular and, as a result, u and l exist and are unique.
Thus, there exists a unique factorization of Ai, where uii is the unique solution
of the equation uii = aii ‚àílT u. This completes the induction step of the proof.
It now remains to prove that, if the factorization at hand exists and is unique,
then the Ô¨Årst n‚àí1 leading minors of A must be nonsingular. We shall distinguish
the case where A is singular and when it is nonsingular.
Let us start from the second one and assume that the LU factorization of A
with lii = 1 for i = 1, . . . , n, exists and is unique. Then, due to (3.38), we have
Ai = L(i)U(i) for i = 1, . . . , n. Thus
det(Ai) = det(L(i))det(U(i)) = det(U(i)) = u11u22 . . . uii,
(3.39)
from which, taking i = n and A nonsingular, we obtain u11u22 . . . unn Ã∏= 0, and
thus, necessarily, det(Ai) = u11u22 . . . uii Ã∏= 0 for i = 1, . . . , n ‚àí1.
Now let A be a singular matrix and assume that (at least) one diagonal entry
of U is equal to zero. Denote by ukk the null entry of U with minimum index k.
Thanks to (3.38), the factorization can be computed without troubles until the
k + 1-th step. From that step on, since the matrix U(k) is singular, existence and
uniqueness of the vector lT are certainly lost, and, thus, the same holds for the
uniqueness of the factorization. In order for this not to occur before the process
has factorized the whole matrix A, the ukk entries must all be nonzero up to the
index k = n‚àí1 included, and thus, due to (3.39), all the leading minors Ak must
be nonsingular for k = 1, . . . , n ‚àí1.
3
From the above theorem we conclude that, if an Ai, with i = 1, . . . , n ‚àí1,
is singular, then the factorization may either not exist or not be unique.
Example 3.3 Consider the matrices
B =
 1
2
1
2

,
C =
 0
1
1
0

,
D =
 0
1
0
2

.
According to Theorem 3.4, the singular matrix B, having nonsingular leading
minor B1 = 1, admits a unique LU factorization. The remaining two examples
outline that, if the assumptions of the theorem are not fulÔ¨Ålled, the factorization
may fail to exist or be unique.
Actually, the nonsingular matrix C, with C1 singular, does not admit any
factorization, while the (singular) matrix D, with D1 singular, admits an inÔ¨Ånite
number of factorizations of the form D = LŒ≤UŒ≤, with
LŒ≤ =
 1
0
Œ≤
1

,
UŒ≤ =
 0
1
0
2 ‚àíŒ≤

,
‚àÄŒ≤ ‚ààR.
‚Ä¢
In the case where the LU factorization is unique, we point out that, because
det(A) = det(LU) = det(L) det(U) = det(U), the determinant of A is given

76
3. Direct Methods for the Solution of Linear Systems
by
det(A) = u11 ¬∑ ¬∑ ¬∑ unn.
Let us now recall the following property (referring for its proof to [GL89]
or [Hig96]).
Property 3.2 If A is a matrix diagonally dominant by rows or by columns,
then the LU factorization of A exists. In particular, if A is diagonally dom-
inant by columns, then |lij| ‚â§1 ‚àÄi, j.
In the proof of Theorem 3.4 we exploited the fact the the diagonal entries
of L are equal to 1. In a similar manner, we could have Ô¨Åxed to 1 the
diagonal entries of the upper triangular matrix U, obtaining a variant of
GEM that will be considered in Section 3.3.4.
The freedom in setting up either the diagonal entries of L or those of U,
implies that several LU factorizations exist which can be obtained one from
the other by multiplication with a suitable diagonal matrix (see Section
3.4.1).
3.3.2
The EÔ¨Äect of Rounding Errors
If rounding errors are taken into account, the factorization process induced
by GEM yields two matrices, L and U, such that LU = A+Œ¥A, Œ¥A being a
perturbation matrix. The size of such a perturbation can be estimated by
|Œ¥A| ‚â§
nu
1 ‚àínu|L| |U|,
(3.40)
where u is the roundoÔ¨Äunit (for the proof of this result we refer to [Hig89]).
From (3.40) it is seen that the presence of small pivotal entries can make
the right side of the inequality virtually unbounded, with a consequent loss
of control on the size of the perturbation matrix Œ¥A. The interest is thus
in Ô¨Ånding out estimates like (3.40) of the form
|Œ¥A| ‚â§g(u)|A|,
where g(u) is a suitable function of u. For instance, assuming that L and
U have nonnegative entries, then since |L| |U| = |LU| one gets
|L| |U| = |LU| = |A + Œ¥A| ‚â§|A| + |Œ¥A| ‚â§|A| +
nu
1 ‚àínu|L| |U|,
(3.41)
from which the desired bound is achieved by taking g(u) = nu/(1 ‚àí2nu).
The technique of pivoting, examined in Section 3.5, keeps the size of the
pivotal entries under control and makes it possible to obtain estimates like
(3.41) for any matrix.

3.3 The Gaussian Elimination Method (GEM) and LU Factorization
77
3.3.3
Implementation of LU Factorization
Since L is a lower triangular matrix with diagonal entries equal to 1 and U
is upper triangular, it is possible (and convenient) to store the LU factor-
ization directly in the same memory area that is occupied by the matrix A.
More precisely, U is stored in the upper triangular part of A (including the
diagonal), whilst L occupies the lower triangular portion of A (the diagonal
entries of L are not stored since they are implicitly assumed to be 1).
A coding of the algorithm is reported in Program 4. The output matrix
A contains the overwritten LU factorization.
Program 4 - lu kji : LU factorization of matrix A. kji version
function [A] = lu kji (A)
[n,n]=size(A);
for k=1:n-1
A(k+1:n,k)=A(k+1:n,k)/A(k,k);
for j=k+1:n, for i=k+1:n
A(i,j)=A(i,j)-A(i,k)*A(k,j);
end,
end
end
This implementation of the factorization algorithm is commonly referred
to as the kji version, due to the order in which the cycles are executed.
In a more appropriate notation, it is called the SAXPY ‚àíkji version,
due to the fact that the basic operation of the algorithm, which consists of
multiplying a scalar A by a vector X, summing another vector Y and then
storing the result, is usually called SAXPY (i.e. Scalar A X Plus Y ).
The factorization can of course be executed by following a diÔ¨Äerent order.
In general, the forms in which the cycle on index i precedes the cycle on
j are called row-oriented, whilst the others are called column-oriented. As
usual, this terminology refers to the fact that the matrix is accessed by
rows or by columns.
An example of LU factorization, jki version and column-oriented, is given
in Program 5. This version is commonly called GAXPY ‚àíjki, since the
basic operation (a product matrix-vector), is called GAXPY which stands
for Generalized sAXPY (see for further details [DGK84]). In the GAXPY
operation the scalar A of the SAXPY operation is replaced by a matrix.
Program 5 - lu jki : LU factorization of matrix A. jki version
function [A] = lu jki (A)
[n,n]=size(A);
for j=1:n
for k=1:j-1,
for i=k+1:n
A(i,j)=A(i,j)-A(i,k)*A(k,j);
end,
end
for i=j+1:n,
A(i,j)=A(i,j)/A(j,j); end
end

78
3. Direct Methods for the Solution of Linear Systems
3.3.4
Compact Forms of Factorization
Remarkable variants of LU factorization are the Crout factorization and
Doolittle factorization, and are known also as compact forms of the Gauss
elimination method. This name is due to the fact that these approaches
require less intermediate results than the standard GEM to generate the
factorization of A.
Computing the LU factorization of A is formally equivalent to solving
the following nonlinear system of n2 equations
aij =
min(i,j)

r=1
lirurj,
(3.42)
the unknowns being the n2 +n coeÔ¨Écients of the triangular matrices L and
U. If we arbitrarily set n coeÔ¨Écients to 1, for example the diagonal entries
of L or U, we end up with the Doolittle and Crout methods, respectively,
which provide an eÔ¨Écient way to solve system (3.42).
In fact, supposing that the Ô¨Årst k ‚àí1 columns of L and U are available
and setting lkk = 1 (Doolittle method), the following equations are obtained
from (3.42)
akj =
k‚àí1

r=1
lkrurj + ukj ,
j = k, . . . , n
aik =
k‚àí1

r=1
lirurk + lik ukk,
i = k + 1, . . . , n.
Note that these equations can be solved in a sequential way with respect
to the boxed variables ukj and lik. From the Doolittle compact method
we thus obtain Ô¨Årst the k-th row of U and then the k-th column of L, as
follows: for k = 1, . . . , n
ukj = akj ‚àí
k‚àí1

r=1
lkrurj
j = k, . . . , n
lik =
1
ukk

aik ‚àí
k‚àí1

r=1
lirurk

i = k + 1, . . . , n.
(3.43)
The Crout factorization is generated similarly, computing Ô¨Årst the k-th
column of L and then the k-th row of U: for k = 1, . . . , n
lik = aik ‚àí
k‚àí1

r=1
lirurk
i = k, . . . , n
ukj = 1
lkk

akj ‚àí
k‚àí1

r=1
lkrurj

j = k + 1, . . . , n,

3.4 Other Types of Factorization
79
where we set ukk = 1. Recalling the notations introduced above, the Doolit-
tle factorization is nothing but the ijk version of GEM.
We provide in Program 6 the implementation of the Doolittle scheme.
Notice that now the main computation is a dot product, so this scheme is
also known as the DOT ‚àíijk version of GEM.
Program 6 - lu ijk : LU factorization of the matrix A: ijk version
function [A] = lu ijk (A)
[n,n]=size(A);
for i=1:n
for j=2:i
A(i,j-1)=A(i,j-1)/A(j-1,j-1);
for k=1:j-1,
A(i,j)=A(i,j)-A(i,k)*A(k,j); end
end
for j=i+1:n
for k=1:i-1,
A(i,j)=A(i,j)-A(i,k)*A(k,j); end
end
end
3.4
Other Types of Factorization
We now address factorizations suitable for symmetric and rectangular ma-
trices.
3.4.1
LDMT Factorization
It is possible to devise other types of factorizations of A removing the
hypothesis that the elements of L are equal to one. SpeciÔ¨Åcally, we will
address some variants where the factorization of A is of the form
A = LDMT .
where L, MT and D are lower triangular, upper triangular and diagonal
matrices, respectively.
After the construction of this factorization, the resolution of the system
can be carried out solving Ô¨Årst the lower triangular system Ly=b, then the
diagonal one Dz=y, and Ô¨Ånally the upper triangular system MT x=z, with
a cost of n2 + n Ô¨Çops. In the symmetric case, we obtain M = L and the
LDLT factorization can be computed with half the cost (see Section 3.4.2).
The LDLT factorization enjoys a property analogous to the one in The-
orem 3.4 for the LU factorization. In particular, the following result holds.
Theorem 3.5 If all the principal minors of a matrix A‚ààRn√ón are nonzero
then there exist a unique diagonal matrix D, a unique unit lower triangu-
lar matrix L and a unique unit upper triangular matrix MT , such that
A = LDMT .

80
3. Direct Methods for the Solution of Linear Systems
Proof. By Theorem 3.4 we already know that there exists a unique LU factor-
ization of A with lii = 1 for i = 1, . . . , n. If we set the diagonal entries of D
equal to uii (nonzero because U is nonsingular), then A = LU = LD(D‚àí1U).
Upon deÔ¨Åning MT = D‚àí1U, the existence of the LDMT factorization follows,
where D‚àí1U is a unit upper triangular matrix. The uniqueness of the LDMT
factorization is a consequence of the uniqueness of the LU factorization.
3
The above proof shows that, since the diagonal entries of D coincide
with those of U, we could compute L, MT and D starting from the LU
factorization of A. It suÔ¨Éces to compute MT as D‚àí1U. Nevertheless, this
algorithm has the same cost as the standard LU factorization. Likewise,
it is also possible to compute the three matrices of the factorization by
enforcing the identity A=LDMT entry by entry.
3.4.2
Symmetric and Positive DeÔ¨Ånite Matrices: The
Cholesky Factorization
As already pointed out, the factorization LDMT simpliÔ¨Åes considerably
when A is symmetric because in such a case M=L, yielding the so-called
LDMT factorization. The computational cost halves, with respect to the
LU factorization, to about (n3/3) Ô¨Çops.
As an example, the Hilbert matrix of order 3 admits the following LDLT
factorization
H3 =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
1
1
2
1
3
1
2
1
3
1
4
1
3
1
4
1
5
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª=
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
1
0
0
1
2
1
0
1
3
1
1
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
1
0
0
0
1
12
0
0
0
1
180
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
1
1
2
1
3
0
1
1
0
0
1
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª.
In the case that A is also positive deÔ¨Ånite, the diagonal entries of D in the
LDLT factorization are positive. Moreover, we have the following result.
Theorem 3.6 Let A ‚ààRn√ón be a symmetric and positive deÔ¨Ånite matrix.
Then, there exists a unique upper triangular matrix H with positive diagonal
entries such that
A = HTH.
(3.44)
This factorization is called Cholesky factorization and the entries hij of HT
can be computed as follows: h11 = ‚àöa11 and, for i = 2, . . . , n,
hij =

aij ‚àí
j‚àí1

k=1
hikhjk

/hjj,
j = 1, . . . , i ‚àí1,
hii =

aii ‚àí
i‚àí1

k=1
h2
ik
1/2
.
(3.45)

3.4 Other Types of Factorization
81
Proof. Let us prove the theorem proceeding by induction on the size i of the
matrix (as done in Theorem 3.4), recalling that if Ai ‚ààRi√ói is symmetric positive
deÔ¨Ånite, then all its principal submatrices enjoy the same property.
For i = 1 the result is obviously true. Thus, suppose that it holds for i ‚àí1 and
prove that it also holds for i. There exists an upper triangular matrix Hi‚àí1 such
that Ai‚àí1 = HT
i‚àí1Hi‚àí1. Let us partition Ai as
Ai =
 Ai‚àí1
v
vT
Œ±

,
with Œ± ‚ààR+, vT ‚ààRi‚àí1 and look for a factorization of Ai of the form
Ai = HT
i Hi =
 HT
i‚àí1
0
hT
Œ≤
  Hi‚àí1
h
0T
Œ≤

.
Enforcing the equality with the entries of Ai yields the equations HT
i‚àí1h = v
and hT h + Œ≤2 = Œ±. The vector h is thus uniquely determined, since HT
i‚àí1 is
nonsingular. As for Œ≤, due to the properties of determinants
0 < det(Ai) = det(HT
i ) det(Hi) = Œ≤2(det(Hi‚àí1))2,
we can conclude that it must be a real number. As a result, Œ≤ =
‚àö
Œ± ‚àíhT h is
the desired diagonal entry and this concludes the inductive argument.
Let us now prove formulae (3.45). The fact that h11 = ‚àöa11 is an immediate
consequence of the induction argument for i = 1. In the case of a generic i,
relations (3.45)1 are the forward substitution formulae for the solution of the
linear system HT
i‚àí1h = v = (a1i, a2i, . . . , ai‚àí1,i)T , while formulae (3.45)2 state
that Œ≤ =
‚àö
Œ± ‚àíhT h, where Œ± = aii.
3
The algorithm which implements (3.45) requires about (n3/3) Ô¨Çops and it
turns out to be stable with respect to the propagation of rounding errors.
It can indeed be shown that the upper triangular matrix ÀúH is such that
ÀúHT ÀúH = A + Œ¥A, where Œ¥A is a pertubation matrix such that ‚à•Œ¥A‚à•2 ‚â§
8n(n + 1)u‚à•A‚à•2, when the rounding errors are considered and assuming
that 2n(n + 1)u ‚â§1 ‚àí(n + 1)u (see [Wil68]).
Also, for the Cholesky factorization it is possible to overwrite the matrix
HT in the lower triangular portion of A, without any further memory stor-
age. By doing so, both A and the factorization are preserved, noting that
A is stored in the upper triangular section since it is symmetric and that
its diagonal entries can be computed as a11 = h2
11, aii = h2
ii + i‚àí1
k=1 h2
ik,
i = 2, . . . , n.
An example of implementation of the Cholesky factorization is coded in
Program 7.
Program 7 - chol2 : Cholesky factorization
function [A] = chol2 (A)
[n,n]=size(A);

82
3. Direct Methods for the Solution of Linear Systems
for k=1:n-1
A(k,k)=sqrt(A(k,k));
A(k+1:n,k)=A(k+1:n,k)/A(k,k);
for j=k+1:n,
A(j:n,j)=A(j:n,j)-A(j:n,k)*A(j,k);
end
end
A(n,n)=sqrt(A(n,n));
3.4.3
Rectangular Matrices: The QR Factorization
DeÔ¨Ånition 3.1 A matrix A ‚ààRm√ón, with m ‚â•n, admits a QR fac-
torization if there exist an orthogonal matrix Q ‚ààRm√óm and an upper
trapezoidal matrix R ‚ààRm√ón with null rows from the n + 1-th one on,
such that
A = QR.
(3.46)
‚ñ†
This factorization can be constructed either using suitable transformation
matrices (Givens or Householder matrices, see Section 5.6.1) or using the
Gram-Schmidt orthogonalization algorithm discussed below.
It is also possible to generate a reduced version of the QR factorization
(3.46), as stated in the following result.
Property 3.3 Let A ‚ààRm√ón be a matrix of rank n for which a QR fac-
torization is known. Then there exists a unique factorization of A of the
form
A = $Q$R
(3.47)
where $Q and $R are submatrices of Q and R given respectively by
$Q = Q(1 : m, 1 : n),
$R = R(1 : n, 1 : n).
(3.48)
Moreover, $Q has orthonormal vector columns and $R is upper triangular
and coincides with the Cholesky factor H of the symmetric positive deÔ¨Ånite
matrix AT A, that is, AT A = $RT $R.
If A has rank n (i.e., full rank), then the column vectors of ÀúQ form an
orthonormal basis for the vector space range(A) (deÔ¨Åned in (1.5)). As a
consequence, constructing the QR factorization can also be interpreted as
a procedure for generating an orthonormal basis for a given set of vectors.
If A has rank r < n, the QR factorization does not necessarily yield an
orthonormal basis for range(A). However, one can obtain a factorization of
the form
QT AP =

R11
R12
0
0

,

3.4 Other Types of Factorization
83
n
A
m
=
m ‚àín
ÀúQ
ÀúR
0
n
n
n
m ‚àín
FIGURE 3.1. The reduced factorization. The matrices of the QR factorization
are drawn in dashed lines
where Q is orthogonal, P is a permutation matrix and R11 is a nonsingular
upper triangular matrix of order r.
In general, when using the QR factorization, we shall always refer to its
reduced form (3.47) as it Ô¨Ånds a remarkable application in the solution of
overdetermined systems (see Section 3.13).
The matrix factors ÀúQ and ÀúR in (3.47) can be computed using the Gram-
Schmidt orthogonalization. Starting from a set of linearly independent vec-
tors, x1, . . . , xn, this algorithm generates a new set of mutually orthogonal
vectors, q1, . . . , qn, given by
q1 = x1,
qk+1 = xk+1 ‚àí
k

i=1
(qi, xk+1)
(qi, qi) qi,
k = 1, . . . , n ‚àí1.
(3.49)
Denoting by a1, . . . , an the column vectors of A, we set Àúq1 = a1/‚à•a1‚à•2
and, for k = 1, . . . , n ‚àí1, compute the column vectors of ÀúQ as
Àúqk+1 = qk+1/‚à•qk+1‚à•2,
where
qk+1 = ak+1 ‚àí
k

j=1
(Àúqj, ak+1)Àúqj.
Next, imposing that A=ÀúQÀúR and exploiting the fact that ÀúQ is orthogonal
(that is, ÀúQ‚àí1 = ÀúQT ), the entries of ÀúR can easily be computed. The overall
computational cost of the algorithm is of the order of mn2 Ô¨Çops.
It is also worth noting that if A has full rank, the matrix AT A is sym-
metric and positive deÔ¨Ånite (see Section 1.9) and thus it admits a unique
Cholesky factorization of the form HT H. On the other hand, since the or-
thogonality of ÀúQ implies
HT H = AT A = ÀúRT ÀúQT ÀúQÀúR = ÀúRT ÀúR,

84
3. Direct Methods for the Solution of Linear Systems
we conclude that ÀúR is actually the Cholesky factor H of AT A. Thus, the
diagonal entries of ÀúR are all nonzero only if A has full rank.
The Gram-Schmidt method is of little practical use since the generated
vectors lose their linear independence due to rounding errors. Indeed, in
Ô¨Çoating-point arithmetic the algorithm produces very small values of ‚à•qk+1‚à•2
and Àúrkk with a consequent numerical instability and loss of orthogonality
for matrix ÀúQ (see Example 3.4).
These drawbacks suggest employing a more stable version, known as
modiÔ¨Åed Gram-Schmidt method. At the beginning of the k + 1-th step, the
projections of the vector ak+1 along the vectors Àúq1, . . . , Àúqk are progressively
subtracted from ak+1. On the resulting vector, the orthogonalization step
is then carried out. In practice, after computing (Àúq1, ak+1)Àúq1 at the k+1-th
step, this vector is immediately subtracted from ak+1. As an example, one
lets
a(1)
k+1 = ak+1 ‚àí(Àúq1, ak+1)Àúq1.
This new vector a(1)
k+1 is projected along the direction of Àúq2 and the obtained
projection is subtracted from a(1)
k+1, yielding
a(2)
k+1 = a(1)
k+1 ‚àí(Àúq2, a(1)
k+1)Àúq2
and so on, until a(k)
k+1 is computed.
It can be checked that a(k)
k+1 coincides with the corresponding vector qk+1
in the standard Gram-Schmidt process, since, due to the orthogonality of
vectors Àúq1, Àúq2, . . . , Àúqk,
a(k)
k+1
=
ak+1 ‚àí(Àúq1, ak+1)Àúq1 ‚àí(Àúq2, ak+1 ‚àí(Àúq1, ak+1)Àúq1) Àúq2 + . . .
=
ak+1 ‚àí
k

j=1
(Àúqj, ak+1)Àúqj.
Program 8 implements the modiÔ¨Åed Gram-Schmidt method. Notice that
it is not possible to overwrite the computed QR factorization on the ma-
trix A. In general, the matrix $R is overwritten on A, whilst $Q is stored
separately. The computational cost of the modiÔ¨Åed Gram-Schmidt method
has the order of 2mn2 Ô¨Çops.
Program 8 - mod grams : ModiÔ¨Åed Gram-Schmidt method
function [Q,R] = mod grams(A)
[m,n]=size(A);
Q=zeros(m,n);
Q(1:m,1) = A(1:m,1);
R=zeros(n);
R(1,1)=1;
for k = 1:n
R(k,k) = norm (A(1:m,k));
Q(1:m,k) = A(1:m,k)/R(k,k);

3.5 Pivoting
85
for j=k+1:n
R (k,j) = Q (1:m,k)‚Äô * A(1:m,j);
A (1:m,j) = A (1:m,j) - Q(1:m,k)*R(k,j);
end
end
Example 3.4 Let us consider the Hilbert matrix H4 of order 4 (see (3.32)). The
matrix ÀúQ, generated by the standard Gram-Schmidt algorithm, is orthogonal up
to the order of 10‚àí10, being
I ‚àíÀúQT ÀúQ = 10‚àí10
Ô£Æ
Ô£ØÔ£ØÔ£∞
0.0000
‚àí0.0000
0.0001
‚àí0.0041
‚àí0.0000
0
0.0004
‚àí0.0099
0.0001
0.0004
0
‚àí0.4785
‚àí0.0041
‚àí0.0099
‚àí0.4785
0
Ô£π
Ô£∫Ô£∫Ô£ª
and ‚à•I ‚àíÀúQT ÀúQ‚à•‚àû= 4.9247 ¬∑ 10‚àí11. Using the modiÔ¨Åed Gram-Schmidt method,
we would obtain
I ‚àíÀúQT ÀúQ = 10‚àí12
Ô£Æ
Ô£ØÔ£ØÔ£∞
0.0001
‚àí0.0005
0.0069
‚àí0.2853
‚àí0.0005
0
‚àí0.0023
0.0213
0.0069
‚àí0.0023
0.0002
‚àí0.0103
‚àí0.2853
0.0213
‚àí0.0103
0
Ô£π
Ô£∫Ô£∫Ô£ª
and this time ‚à•I ‚àíÀúQT ÀúQ‚à•‚àû= 3.1686 ¬∑ 10‚àí13.
An improved result can be obtained using, instead of Program 8, the intrinsic
function QR of MATLAB. This function can be properly employed to generate
both the factorization (3.46) as well as its reduced version (3.47).
‚Ä¢
3.5
Pivoting
As previously pointed out, the GEM process breaks down as soon as a zero
pivotal entry is computed. In such an event, one needs to resort to the so-
called pivoting technique, which amounts to exchanging rows (or columns)
of the system in such a way that non vanishing pivots are obtained.
Example 3.5 Let us go back to matrix (3.33) for which GEM furnishes at the
second step a zero pivotal element. By simply exchanging the second row with
the third one, we can execute one step further of the elimination method, Ô¨Ånding
a nonzero pivot. The generated system is equivalent to the original one and it
can be noticed that it is already in upper triangular form. Indeed
A(2) =
Ô£Æ
Ô£∞
1
2
3
0
‚àí6
‚àí12
0
0
‚àí1
Ô£π
Ô£ª= U,
while the transformation matrices are given by
M(1) =
Ô£Æ
Ô£∞
1
0
0
‚àí2
1
0
‚àí7
0
1
Ô£π
Ô£ª,
M(2) =
Ô£Æ
Ô£∞
1
0
0
0
1
0
0
0
1
Ô£π
Ô£ª.

86
3. Direct Methods for the Solution of Linear Systems
From an algebraic standpoint, a permutation of the rows of A has been performed.
In fact, it now no longer holds that A=M‚àí1
1 M‚àí1
2 U, but rather A=M‚àí1
1
P M‚àí1
2 U,
P being the permutation matrix
P =
Ô£Æ
Ô£∞
1
0
0
0
0
1
0
1
0
Ô£π
Ô£ª.
(3.50)
‚Ä¢
The pivoting strategy adopted in Example 3.5 can be generalized by look-
ing, at each step k of the elimination procedure, for a nonzero pivotal entry
by searching within the entries of the subcolumn A(k)(k : n, k). For that
reason, it is called partial pivoting (by rows).
From (3.30) it can be seen that a large value of mik (generated for ex-
ample by a small value of the pivot a(k)
kk ) might amplify the rounding errors
aÔ¨Äecting the entries a(k)
kj . Therefore, in order to ensure a better stability,
the pivotal element is chosen as the largest entry (in module) of the column
A(k)(k : n, k) and partial pivoting is generally performed at every step of
the elimination procedure, even if not strictly necessary (that is, even if
nonzero pivotal entries are found).
Alternatively, the searching process could have been extended to the
whole submatrix A(k)(k : n, k : n), ending up with a complete pivoting
(see Figure 3.2). Notice, however, that while partial pivoting requires an
additional cost of about n2 searches, complete pivoting needs about 2n3/3,
with a considerable increase of the computational cost of GEM.
                                                                                                                                        







                                                                                                                                







0
r
k
r
k
q
k
k
0
FIGURE 3.2. Partial pivoting by row (left) or complete pivoting (right). Shaded
areas of the matrix are those involved in the searching for the pivotal entry
Example 3.6 Let us consider the linear system Ax = b with
A =
 10‚àí13
1
1
1


3.5 Pivoting
87
and where b is chosen in such a way that x = (1, 1)T is the exact solution.
Suppose we use base 2 and 16 signiÔ¨Åcant digits. GEM without pivoting would
give xMEG = (0.99920072216264, 1)T , while GEM plus partial pivoting furnishes
the exact solution up to the 16th digit.
‚Ä¢
Let us analyze how partial pivoting aÔ¨Äects the LU factorization induced
by GEM. At the Ô¨Årst step of GEM with partial pivoting, after Ô¨Ånding
out the entry ar1 of maximum module in the Ô¨Årst column, the elementary
permutation matrix P1 which exchanges the Ô¨Årst row with the r-th row is
constructed (if r = 1, P1 is the identity matrix). Next, the Ô¨Årst Gaussian
transformation matrix M1 is generated and we set A(2) = M1P1A(1). A
similar approach is now taken on A(2), searching for a new permutation
matrix P2 and a new matrix M2 such that
A(3) = M2P2A(2) = M2P2M1P1A(1).
Executing all the elimination steps, the resulting upper triangular matrix
U is now given by
U = A(n) = Mn‚àí1Pn‚àí1 . . . M1P1A(1).
(3.51)
Letting M = Mn‚àí1Pn‚àí1 . . . M1P1 and P = Pn‚àí1 . . . P1, we obtain that
U=MA and, thus, U = (MP‚àí1)PA. It can easily be checked that the matrix
L = PM‚àí1 is unit lower triangular, so that the LU factorization reads
PA = LU.
(3.52)
One should not be worried by the presence of the inverse of M, since M‚àí1 =
P‚àí1
1 M‚àí1
1
. . . P‚àí1
n‚àí1M‚àí1
n‚àí1 and P‚àí1
i
= PT
i while M‚àí1
i
= 2In ‚àíMi.
Once L, U and P are available, solving the initial linear system amounts
to solving the triangular systems Ly = Pb and Ux = y. Notice that the
entries of the matrix L coincide with the multipliers computed by LU fac-
torization, without pivoting, when applied to the matrix PA.
If complete pivoting is performed, at the Ô¨Årst step of the process, once the
element aqr of largest module in submatrix A(1 : n, 1 : n) has been found,
we must exchange the Ô¨Årst row and column with the q-th row and the
r-th column. This generates the matrix P1A(1)Q1, where P1 and Q1 are
permutation matrices by rows and by columns, respectively.
As a consequence, the action of matrix M1 is now such that A(2) =
M1P1A(1)Q1. Repeating the process, at the last step, instead of (3.51) we
obtain
U = A(n) = Mn‚àí1Pn‚àí1 . . . M1P1A(1)Q1 . . . Qn‚àí1.
In the case of complete pivoting the LU factorization becomes
PAQ = LU,

88
3. Direct Methods for the Solution of Linear Systems
where Q = Q1 . . . Qn‚àí1 is a permutation matrix accounting for all permu-
tations that have been operated. By construction, matrix L is still lower
triangular, with module entries less than or equal to 1. As happens in
partial pivoting, the entries of L are the multipliers produced by the LU
factorization process without pivoting, when applied to the matrix PAQ.
Program 9 is an implementation of the LU factorization with complete
pivoting. For an eÔ¨Écient computer implementation of the LU factorization
with partial pivoting, we refer to the MATLAB intrinsic function lu.
Program 9 - LUpivtot : LU factorization with complete pivoting
function [L,U,P,Q] = LUpivtot(A,n)
P=eye(n); Q=P; Minv=P;
for k=1:n-1
[Pk,Qk]=pivot(A,k,n);
A=Pk*A*Qk;
[Mk,Mkinv]=MGauss(A,k,n);
A=Mk*A;
P=Pk*P;
Q=Q*Qk;
Minv=Minv*Pk*Mkinv;
end
U=triu(A);
L=P*Minv;
function [Mk,Mkinv]=MGauss(A,k,n)
Mk=eye(n);
for i=k+1:n,
Mk(i,k)=-A(i,k)/A(k,k);
end
Mkinv=2*eye(n)-Mk;
function [Pk,Qk]=pivot(A,k,n)
[y,i]=max(abs(A(k:n,k:n))); [piv,jpiv]=max(y);
ipiv=i(jpiv);
jpiv=jpiv+k-1;
ipiv=ipiv+k-1;
Pk=eye(n); Pk(ipiv,ipiv)=0; Pk(k,k)=0; Pk(k,ipiv)=1; Pk(ipiv,k)=1;
Qk=eye(n); Qk(jpiv,jpiv)=0; Qk(k,k)=0; Qk(k,jpiv)=1; Qk(jpiv,k)=1;
Remark 3.3 The presence of large pivotal entries is not in itself suÔ¨Écient
to guarantee accurate solutions, as demonstrated by the following example
(taken from [JM92]). For the linear system Ax = b
Ô£Æ
Ô£∞
‚àí4000
2000
2000
2000
0.78125
0
2000
0
0
Ô£π
Ô£ª
Ô£Æ
Ô£∞
x1
x2
x3
Ô£π
Ô£ª=
Ô£Æ
Ô£∞
400
1.3816
1.9273
Ô£π
Ô£ª
at the Ô¨Årst step the pivotal entry coincides with the diagonal entry ‚àí4000
itself. However, executing GEM on such a matrix yields the solution
x = [0.00096365, ‚àí0.698496, 0.90042329]T
whose Ô¨Årst component drastically diÔ¨Äers from that of the exact solution
x = [1.9273, ‚àí0.698496, 0.9004233]T . The cause of this behaviour should

3.6 Computing the Inverse of a Matrix
89
be ascribed to the wide variations among the system coeÔ¨Écients. Such cases
can be remedied by a suitable scaling of the matrix (see Section 3.12.1). ‚ñ†
Remark 3.4 (Pivoting for symmetric matrices) As already noticed,
pivoting is not strictly necessary if A is symmetric and positive deÔ¨Ånite.
A separate comment is deserved when A is symmetric but not positive
deÔ¨Ånite, since pivoting could destroy the symmetry of the matrix. This
can be avoided by employing a complete pivoting of the form PAPT , even
though this pivoting can only turn out into a reordering of the diagonal
entries of A. As a consequence, the presence on the diagonal of A of small
entries might inhibit the advantages of the pivoting. To deal with matrices
of this kind, special algorithms are needed (like the Parlett-Reid method
[PR70] or the Aasen method [Aas71]) for whose description we refer to
[GL89], and to [JM92] for the case of sparse matrices.
‚ñ†
3.6
Computing the Inverse of a Matrix
The explicit computation of the inverse of a matrix can be carried out using
the LU factorization as follows. Denoting by X the inverse of a nonsingular
matrix A‚ààRn√ón, the column vectors of X are the solutions to the linear
systems Axi = ei, for i = 1, . . . , n.
Supposing that PA=LU, where P is the partial pivoting permutation
matrix, we must solve 2n triangular systems of the form
Lyi = Pei,
Uxi = yi
i = 1, . . . , n,
i.e., a succession of linear systems having the same coeÔ¨Écient matrix but
diÔ¨Äerent right hand sides. The computation of the inverse of a matrix is a
costly procedure which can sometimes be even less stable than MEG (see
[Hig88]).
An alternative approach for computing the inverse of A is provided by
the Faddev or Leverrier formula, which, letting B0=I, recursively computes
Œ±k = 1
k tr(ABk‚àí1),
Bk = ‚àíABk‚àí1 + Œ±kI,
k = 1, 2, . . . , n.
Since Bn = 0, if Œ±n Ã∏= 0 we get
A‚àí1 = 1
Œ±n
Bn‚àí1,
and the computational cost of the method for a full matrix is equal to
(n ‚àí1)n3 Ô¨Çops (for further details see [FF63], [Bar89]).

90
3. Direct Methods for the Solution of Linear Systems
3.7
Banded Systems
Discretization methods for boundary value problems often lead to solving
linear systems with matrices having banded, block or sparse forms. Ex-
ploiting the structure of the matrix allows for a dramatic reduction in the
computational costs of the factorization and of the substitution algorithms.
In the present and forthcoming sections, we shall address special variants
of MEG or LU factorization that are properly devised for dealing with ma-
trices of this kind. For the proofs and a more comprehensive treatment, we
refer to [GL89] and [Hig88] for banded or block matrices, while we refer to
[JM92], [GL81] and [Saa96] for sparse matrices and the techniques for their
storage.
The main result for banded matrices is the following.
Property 3.4 Let A‚ààRn√ón. Suppose that there exists a LU factorization
of A. If A has upper bandwidth q and lower bandwidth p, then L has lower
bandwidth p and U has upper bandwidth q.
In particular, notice that the same memory area used for A is enough to
also store its LU factorization. Consider, indeed, that a matrix A having
upper bandwidth q and lower bandwidth p is usually stored in a matrix B
(p + q + 1) √ó n, assuming that
bi‚àíj+q+1,j = aij
for all the indices i, j that fall into the band of the matrix. For instance, in
the case of the tridiagonal matrix A=tridiag5(‚àí1, 2, ‚àí1) (where q = p = 1),
the compact storage reads
B =
Ô£Æ
Ô£∞
0
‚àí1
‚àí1
‚àí1
‚àí1
2
2
2
2
2
‚àí1
‚àí1
‚àí1
‚àí1
0
Ô£π
Ô£ª.
The same format can be used for storing the factorization LU of A. It is
clear that this storage format can be quite inconvenient in the case where
only a few bands of the matrix are large. In the limit, if only one column
and one row were full, we would have p = q = n and thus B would be a
full matrix with a lot of zero entries.
Finally, we notice that the inverse of a banded matrix is generally full
(as happens for the matrix A considered above).

3.7 Banded Systems
91
3.7.1
Tridiagonal Matrices
Consider the particular case of a linear system with nonsingular tridiagonal
matrix A given by
A =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
a1
c1
0
b2
a2
...
...
cn‚àí1
0
bn
an
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
In such an event, the matrices L and U of the LU factorization of A are
bidiagonal matrices of the form
L =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£∞
1
0
Œ≤2
1
...
...
0
Œ≤n
1
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
U =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
Œ±1
c1
0
Œ±2
...
...
cn‚àí1
0
Œ±n
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
The coeÔ¨Écients Œ±i and Œ≤i can easily be computed by the following relations
Œ±1 = a1,
Œ≤i =
bi
Œ±i‚àí1
,
Œ±i = ai ‚àíŒ≤ici‚àí1, i = 2, . . . , n.
(3.53)
This is known as the Thomas algorithm and can be regarded as a particular
instance of the Doolittle factorization, without pivoting. When one is not
interested in storing the coeÔ¨Écients of the original matrix, the entries Œ±i
and Œ≤i can be overwritten on A.
The Thomas algorithm can also be extended to solve the whole tridi-
agonal system Ax = f. This amounts to solving two bidiagonal systems
Ly = f and Ux = y, for which the following formulae hold
(Ly = f)
y1 = f1,
yi = fi ‚àíŒ≤iyi‚àí1,
i = 2, . . . , n,
(3.54)
(Ux = y) xn = yn
Œ±n
,
xi = (yi ‚àícixi+1) /Œ±i,
i = n ‚àí1, . . . , 1.
(3.55)
The algorithm requires only 8n ‚àí7 Ô¨Çops: precisely, 3(n ‚àí1) Ô¨Çops for the
factorization (3.53) and 5n ‚àí4 Ô¨Çops for the substitution procedure (3.54)-
(3.55).
As for the stability of the method, if A is a nonsingular tridiagonal matrix
and L and U are the factors actually computed, then
|Œ¥A| ‚â§(4u + 3u2 + u3)|L| |U|,

92
3. Direct Methods for the Solution of Linear Systems
where Œ¥A is implicitly deÔ¨Åned by the relation A + Œ¥A = LU while u is the
roundoÔ¨Äunit. In particular, if A is also symmetric and positive deÔ¨Ånite or
it is an M-matrix, we have
|Œ¥A| ‚â§4u + 3u2 + u3
1 ‚àíu
|A|,
which implies the stability of the factorization procedure in such cases. A
similar result holds even if A is diagonally dominant.
3.7.2
Implementation Issues
An implementation of the LU factorization for banded matrices is shown
in Program 10.
Program 10 - lu band : LU factorization for a banded matrix
function [A] = lu band (A,p,q)
[n,n]=size(A);
for k = 1:n-1
for i = k+1:min(k+p,n), A(i,k)=A(i,k)/A(k,k); end
for j = k+1:min(k+q,n)
for i = k+1:min(k+p,n), A(i,j)=A(i,j)-A(i,k)*A(k,j); end
end
end
In the case where n ‚â´p and n ‚â´q, this algorithm approximately takes
2npq Ô¨Çops, with a considerable saving with respect to the case in which A
is a full matrix.
Similarly, ad hoc versions of the substitution methods can be devised
(see Programs 11 and 12). Their costs are, respectively, of the order of 2np
Ô¨Çops and 2nq Ô¨Çops, always assuming that n ‚â´p and n ‚â´q.
Program 11 - forw band : Forward substitution for a banded matrix L
function [b] = forw band (L, p, b)
[n,n]=size(L);
for j = 1:n
for i=j+1:min(j+p,n); b(i) = b(i) - L(i,j)*b(j); end
end
Program 12 - back band : Backward substitution for a banded matrix U
function [b] = back band (U, q, b)
[n,n]=size(U);
for j=n:-1:1
b (j) = b (j) / U (j,j);
for i = max(1,j-q):j-1, b(i)=b(i)-U(i,j)*b(j); end
end

3.8 Block Systems
93
The programs assume that the whole matrix is stored (including also the
zero entries).
Concerning the tridiagonal case, the Thomas algorithm can be imple-
mented in several ways. In particular, when implementing it on computers
where divisions are more costly than multiplications, it is possible (and
convenient) to devise a version of the algorithm without divisions in (3.54)
and (3.55), by resorting to the following form of the factorization
A = LDMT =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£∞
Œ≥‚àí1
1
0
0
b2
Œ≥‚àí1
2
...
...
...
0
0
bn
Œ≥‚àí1
n
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£∞
Œ≥1
0
Œ≥2
...
0
Œ≥n
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£∞
Œ≥‚àí1
1
c1
0
0
Œ≥‚àí1
2
...
...
...
cn‚àí1
0
0
Œ≥‚àí1
n
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
The coeÔ¨Écients Œ≥i can be recursively computed by the formulae
Œ≥i = (ai ‚àíbiŒ≥i‚àí1ci‚àí1)‚àí1,
for i = 1, . . . , n
where Œ≥0 = 0, b1 = 0 and cn = 0 have been assumed. The forward and
backward substitution algorithms respectively read
(Ly = f)
y1 = Œ≥1f1,
yi = Œ≥i(fi ‚àíbiyi‚àí1),
i = 2, . . . , n
(Ux = y)
xn = yn
xi = yi ‚àíŒ≥icixi+1,
i = n ‚àí1, . . . , 1.
(3.56)
In Program 13 we show an implementation of the Thomas algorithm in
the form (3.56), without divisions. The input vectors a, b and c contain
the coeÔ¨Écients of the tridiagonal matrix {ai}, {bi} and {ci}, respectively,
while the vector f contains the components fi of the right-hand side f.
Program 13 - mod thomas : Thomas algorithm, modiÔ¨Åed version
function [x] = mod thomas (a,b,c,f)
n = size(a); b = [0; b]; c = [c; 0];
gamma (1) = 1/a (1);
for i =2:n, gamma(i)=1/(a(i)-b(i)*gamma(i-1)*c(i-1)); end
y (1) = gamma (1) * f (1);
for i = 2:n, y(i)=gamma(i)*(f(i)-b(i)*y(i-1)); end
x (n) = y (n);
for i = n-1:-1:1, x(i)=y(i)-gamma(i)*c(i)*x(i+1); end
3.8
Block Systems
In this section we deal with the LU factorization of block-partitioned matri-
ces, where each block can possibly be of a diÔ¨Äerent size. Our aim is twofold:
optimizing the storage occupation by suitably exploiting the structure of
the matrix and reducing the computational cost of the solution of the sys-
tem.

94
3. Direct Methods for the Solution of Linear Systems
3.8.1
Block LU Factorization
Let A‚ààRn√ón be the following block partitioned matrix
A =
 A11
A12
A21
A22

,
where A11 ‚ààRr√ór is a nonsingular square matrix whose factorization
L11D1R11 is known, while A22 ‚ààR(n‚àír)√ó(n‚àír). In such a case it is possible
to factorize A using only the LU factorization of the block A11. Indeed, it
is true that

A11
A12
A21
A22

=
 L11
0
L21
In‚àír
  D1
0
0
‚àÜ2
  R11
R12
0
In‚àír

,
where
L21 = A21R‚àí1
11 D‚àí1
1 , R12 = D‚àí1
1 L‚àí1
11 A12,
‚àÜ2 = A22 ‚àíL21D1R12.
If necessary, the reduction procedure can be repeated on the matrix ‚àÜ2,
thus obtaining a block-version of the LU factorization.
If A11 were a scalar, the above approach would reduce by one the size of
the factorization of a given matrix. Applying iteratively this method yields
an alternative way of performing the Gauss elimination.
We also notice that the proof of Theorem 3.4 can be extended to the
case of block matrices, obtaining the following result.
Theorem 3.7 Let A ‚ààRn√ón be partitioned in m √ó m blocks Aij with
i, j = 1, . . . , m. A admits a unique LU block factorization (with L having
unit diagonal entries) iÔ¨Äthe m ‚àí1 dominant principal block minors of A
are nonzero.
Since the block factorization is an equivalent formulation of the standard
LU factorization of A, the stability analysis carried out for the latter holds
for its block-version as well. Improved results concerning the eÔ¨Écient use
in block algorithms of fast forms of matrix-matrix product are dealt with
in [Hig88]. In the forthcoming section we focus solely on block-tridiagonal
matrices.
3.8.2
Inverse of a Block-partitioned Matrix
The inverse of a block matrix can be constructed using the LU factorization
introduced in the previous section. A remarkable application is when A is
a block matrix of the form
A = C + UBV,

3.8 Block Systems
95
where C is a block matrix that is ‚Äúeasy‚Äù to invert (for instance, when C
is given by the diagonal blocks of A), while U, B and V take into account
the connections between the diagonal blocks. In such an event A can be
inverted by using the Sherman-Morrison or Woodbury formula
A‚àí1 = (C + UBV)‚àí1 = C‚àí1 ‚àíC‚àí1U

I + BVC‚àí1U
‚àí1 BVC‚àí1,
(3.57)
having assumed that C and I + BVC‚àí1U are two nonsingular matrices.
This formula has several practical and theoretical applications, and is par-
ticularly eÔ¨Äective if connections between blocks are of modest relevance.
3.8.3
Block Tridiagonal Systems
Consider block tridiagonal systems of the form
Anx =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
A11
A12
0
A21
A22
...
...
...
An‚àí1,n
0
An,n‚àí1
Ann
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£∞
x1
...
...
xn
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
=
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£∞
b1
...
...
bn
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
,
(3.58)
where Aij are matrices of order ni √ónj and xi and bi are column vectors of
size ni, for i, j = 1, . . . , n. We assume that the diagonal blocks are squared,
although not necessarily of the same size. For k = 1, . . . , n, set
Ak =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£∞
In1
0
L1
In2
...
...
0
Lk‚àí1
Ink
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
U1
A12
0
U2
...
...
Ak‚àí1,k
0
Uk
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
Equating for k = n the matrix above with the corresponding blocks of An,
it turns out that U1 = A11, while the remaining blocks can be obtained
solving sequentially, for i = 2, . . . , n, the systems Li‚àí1Ui‚àí1 = Ai,i‚àí1 for
the columns of L and computing Ui = Aii ‚àíLi‚àí1Ai‚àí1,i.
This procedure is well deÔ¨Åned only if all the matrices Ui are nonsingular,
which is the case if, for instance, the matrices A1, . . . , An are nonsingular.
As an alternative, one could resort to factorization methods for banded
matrices, even if this requires the storage of a large number of zero entries
(unless a suitable reordering of the rows of the matrix is performed).
A remarkable instance is when the matrix is block tridiagonal and sym-
metric, with symmetric and positive deÔ¨Ånite blocks. In such a case (3.58)

96
3. Direct Methods for the Solution of Linear Systems
takes the form
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
A11
AT
21
0
A21
A22
...
...
...
AT
n,n‚àí1
0
An,n‚àí1
Ann
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£∞
x1
...
...
xn
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
=
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£∞
b1
...
...
bn
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
Here we consider an extension to the block case of the Thomas algorithm,
which aims at transforming A into a block bidiagonal matrix. To this pur-
pose, we Ô¨Årst have to eliminate the block corresponding to matrix A21.
Assume that the Cholesky factorization of A11 is available and denote by
H11 the Cholesky factor. If we multiply the Ô¨Årst row of the block system
by H‚àíT
11 , we Ô¨Ånd
H11x1 + H‚àíT
11 AT
21x2 = H‚àíT
11 b1.
Letting H21 = H‚àíT
11 AT
21 and c1 = H11b1, it follows that A21 = HT
21H11 and
thus the Ô¨Årst two rows of the system are
H11x1 + H21x2 = c1,
HT
21H11x1 + A22x2 + AT
32x3 = b2.
As a consequence, multiplying the Ô¨Årst row by HT
21 and subtracting it from
the second one, the unknown x1 is eliminated and the following equivalent
equation is obtained
A(1)
22 x2 + AT
32x3 = b2 ‚àíH21c1,
with A(1)
22 = A22 ‚àíHT
21H21. At this point, the factorization of A(1)
22 is carried
out and the unknown x3 is eliminated from the third row of the system,
and the same is repeated for the remaining rows of the system. At the end
of the procedure, which requires solving (n ‚àí1) n‚àí1
j=1 nj linear systems to
compute the matrices Hi+1,i, i = 1, . . . , n‚àí1, we end up with the following
block bidiagonal system
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
H11
H21
0
H22
...
...
Hn,n‚àí1
0
Hnn
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£∞
x1
...
...
xn
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
=
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£∞
c1
...
...
cn
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
which can be solved with a (block) back substitution method. If all blocks
have the same size p, then the number of multiplications required by the
algorithm is about (7/6)(n‚àí1)p3 Ô¨Çops (assuming both p and n very large).

3.9 Sparse Matrices
97
3.9
Sparse Matrices
In this section we brieÔ¨Çy address the numerical solution of linear sparse
systems, that is, systems where the matrix A‚ààRn√ón has a number of
nonzero entries of the order of n (and not n2). We call a pattern of a sparse
matrix the set of its nonzero coeÔ¨Écients.
Banded matrices with suÔ¨Éciently small bands are sparse matrices. Ob-
viously, for a sparse matrix the matrix structure itself is redundant and it
can be more conveniently substituted by a vector-like structure by means
of matrix compacting techniques, like the banded matrix format discussed
in Section 3.7.
x
x
x x x
x
x x x
x x x x
x x
x
x
x
x x
x x x
x
x x x
x
x
x
x
x
x
x
x
x
x
x
x x x
x
x
x
x x
x
x
x
x
x
x
x
x
1
2
3
4
5
6
7
8
9
10
11
12
FIGURE 3.3. Pattern of a symmetric sparse matrix (left) and of its associated
graph (right). For the sake of clarity, the loops have not been drawn; moreover,
since the matrix is symmetric, only one of the two sides associated with each
aij Ã∏= 0 has been reported
For sake of convenience, we associate with a sparse matrix A an oriented
graph G(A). A graph is a pair (V, X) where V is a set of p points and X
is a set of q ordered pairs of elements of V that are linked by a line. The
elements of V are called the vertices of the graph, while the connection lines
are called the paths of the graph.
The graph G(A) associated with a matrix A‚ààRm√ón can be constructed
by identifying the vertices with the set of the indices from 1 to the maximum
between m and n and supposing that a path exists which connects two
vertices i and j if aij Ã∏= 0 and is directed from i to j, for i = 1, . . . , m and
j = 1, . . . , n. For a diagonal entry aii Ã∏= 0, the path joining the vertex i
with itself is called a loop. Since an orientation is associated with each side,
the graph is called oriented (or Ô¨Ånite directed). As an example, Figure 3.3
displays the pattern of a symmetric and sparse 12 √ó 12 matrix, together
with its associated graph.
As previously noticed, during the factorization procedure, nonzero entries
can be generated in memory positions that correspond to zero entries in

98
3. Direct Methods for the Solution of Linear Systems
the starting matrix. This action is referred to as Ô¨Åll-in. Figure 3.4 shows the
eÔ¨Äect of Ô¨Åll-in on the sparse matrix whose pattern is shown in Figure 3.3.
Since use of pivoting in the factorization process makes things even more
complicated, we shall only consider the case of symmetric positive deÔ¨Ånite
matrices for which pivoting is not necessary.
A Ô¨Årst remarkable result concerns the amount of Ô¨Åll-in. Let mi(A) =
i ‚àímin {j < i : aij Ã∏= 0} and denote by E(A) the convex hull of A, given
by
E(A) = {(i, j) : 0 < i ‚àíj ‚â§mi(A)} .
(3.59)
For a symmetric positive deÔ¨Ånite matrix,
E(A) = E(H + HT )
(3.60)
where H is the Cholesky factor, so that Ô¨Åll-in is conÔ¨Åned within the convex
hull of A (see Figure 3.4). Moreover, if we denote by lk(A) the number of
active rows at the k-th step of the factorization (i.e., the number of rows
of A with i > k and aik Ã∏= 0), the computational cost of the factorization
process is
1
2
n

k=1
lk(A) (lk(A) + 3)
Ô¨Çops,
(3.61)
having accounted for all the nonzero entries of the convex hull. ConÔ¨Ånement
of Ô¨Åll-in within E(A) ensures that the LU factorization of A can be stored
without extra memory areas simply by storing all the entries of E(A) (in-
cluding the null elements). However, such a procedure might still be highly
ineÔ¨Écient due to the large number of zero entries in the hull (see Exercise
11).
On the other hand, from (3.60) one gets that the reduction in the convex
hull reÔ¨Çects a reduction of Ô¨Åll-in, and in turn, due to (3.61), of the number
of operations needed to perform the factorization. For this reason several
strategies for reordering the graph of the matrix have been devised. Among
them, we recall the Cuthill-McKee method, which will be addressed in the
next section.
An alternative consists of decomposing the matrix into sparse subma-
trices, with the aim of reducing the original problem to the solution of
subproblems of reduced size, where matrices can be stored in full format.
This approach leads to submatrix decomposition methods which will be
addressed in Section 3.9.2.
3.9.1
The Cuthill-McKee Algorithm
The Cuthill-McKee algorithm is a simple and eÔ¨Äective method for reorder-
ing the system variables.

3.9 Sparse Matrices
99
                



         


      

       
 
x
x
x x
x x
x x
x
x
x
x
x
x
x
x x
x
x
x
x
x
x
x
x
x
x x
x
x
x
x
x
x
x
x x x
x
x x x
x x x x
x x
x
x
x
x
x x
x x x
x
x x x
x
x
x
x
x
x
x
x
x
x
x x x
x
x
x
x x
x
x
x
x
x
x
x
x
FIGURE 3.4. The shaded regions in the left Ô¨Ågure show the areas of the matrix
that can be aÔ¨Äected by Ô¨Åll-in, for the matrix considered in Figure 3.3. Solid
lines denote the boundary of E(A). The right Ô¨Ågure displays the factors that
have been actually computed. Black dots denote the elements of A that were
originarily equal to zero
The Ô¨Årst step of the algorithm consists of associating with each vertex of
the graph the number of its connections with neighboring vertices, called
the degree of the vertex. Next, the following steps are taken:
1. a vertex with a low number of connections is chosen as the Ô¨Årst vertex
of the graph;
2. the vertices connected to it are progressively re-labeled starting from
those having lower degrees;
3. the procedure is repeated starting from the vertices connected to the
second vertex in the updated list. The nodes already re-labeled are
ignored. Then, a third new vertex is considered, and so on, until all
the vertices have been explored.
The usual way to improve the eÔ¨Éciency of the algorithm is based on the
so-called reverse form of the Cuthill-McKee method. This consists of ex-
ecuting the Cuthill-McKee algorithm described above where, at the end,
the i-th vertex is moved into the n ‚àíi + 1-th position of the list, n being
the number of nodes in the graph. Figure 3.5 reports, for comparison, the
graphs obtained using the direct and reverse Cuthill-McKee reordering in
the case of the matrix pattern represented in Figure 3.3, while in Figure
3.6 the factors L and U are compared. Notice the absence of Ô¨Åll-in when
the reverse Cuthill-McKee method is used.
Remark 3.5 For an eÔ¨Écient solution of linear systems with sparse ma-
trices, we mention the public domain libraries SPARSKIT [Saa90], UMF-
PACK [DD95] and the MATLAB sparfun package.
‚ñ†

100
3. Direct Methods for the Solution of Linear Systems
1 (3)
2 (4)
3 (2)
4 (5)
5 (6)
6 (1)
7 (12)
8 (8)
9 (10)
10 (9)
11 (11)
12 (7)
1 (10)
2 (9)
3 (11)
4 (8)
5 (12)
6 (7)
7 (6)
8 (5)
9 (3)
10 (4)
11 (2)
12 (1)
FIGURE 3.5. Reordered graphs using the direct (left) and reverse (right)
Cuthill-McKee algorithm. The label of each vertex, before reordering is per-
formed, is reported in braces
x
x
x x x
x
x x
x x
x x
x
x
x x
x
x x
x
x x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x x x
x x
x x
x x
x
x x
x x
x x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x x
x
x
x
x
x x
x
x
x
x x
x x
x
x
x
x
FIGURE 3.6. Factors L and U after the direct (left) and reverse (right)
Cuthill-McKee reordering. In the second case, Ô¨Åll-in is absent
3.9.2
Decomposition into Substructures
These methods have been developed in the framework of numerical ap-
proximation of partial diÔ¨Äerential equations. Their basic strategy consists
of splitting the solution of the original linear system into subsystems of
smaller size which are almost independent from each other and can be
easily interpreted as a reordering technique.
We describe the methods on a special example, referring for a more
comprehensive presentation to [BSG96]. Consider the linear system Ax=b,
where A is a symmetric positive deÔ¨Ånite matrix whose pattern is shown in
Figure 3.3. To help develop an intuitive understanding of the method, we
draw the graph of A in the form as in Figure 3.7.

3.9 Sparse Matrices
101
We then partition the graph of A into the two subgraphs (or substruc-
tures) identiÔ¨Åed in the Ô¨Ågure and denote by xk, k = 1, 2, the vectors of the
unknowns relative to the nodes that belong to the interior of the k-th sub-
structure. We also denote by x3 the vector of the unknowns that lie along
the interface between the two substructures. Referring to the decomposi-
tion in Figure 3.7, we have x1 = (2, 3, 4, 6)T , x2 = (8, 9, 10, 11, 12)T
and x3 = (1, 5, 7)T .
As a result of the decomposition of the unknowns, matrix A will be
partitioned in blocks, so that the linear system can be written in the form
substructure II
6
7
3
2
4
5
1
12
8
9
11
10
substructure I
FIGURE 3.7. Decomposition into two substructures
Ô£Æ
Ô£∞
A11
0
A13
0
A22
A23
AT
13
AT
23
A33
Ô£π
Ô£ª
Ô£Æ
Ô£∞
x1
x2
x3
Ô£π
Ô£ª=
Ô£Æ
Ô£∞
b1
b2
b3
Ô£π
Ô£ª,
having reordered the unknowns and partitioned accordingly the right hand
side of the system. Suppose that A33 is decomposed into two parts, A‚Ä≤
33
and A‚Ä≤‚Ä≤
33, which represent the contributions to A33 of each substructure.
Similarly, let the right hand side b3 be decomposed as b‚Ä≤
3+b‚Ä≤‚Ä≤
3. The original
linear system is now equivalent to the following pair
 A11
A13
AT
13
A‚Ä≤
33
  x1
x3

=
 b1
b‚Ä≤
3 + Œ≥3

,
 A22
A23
AT
23
A‚Ä≤‚Ä≤
33
  x2
x3

=
 b2
b‚Ä≤‚Ä≤
3 ‚àíŒ≥3

having denoted by Œ≥3 a vector that takes into account the coupling between
the substructures. A typical way of proceeding in decomposition techniques
consists of eliminating Œ≥3 to end up with independent systems, one for each

102
3. Direct Methods for the Solution of Linear Systems
substructure. Let us apply this strategy to the example at hand. The linear
system for the Ô¨Årst substructure is
 A11
A13
AT
13
A‚Ä≤
33
  x1
x3

=
 b1
b‚Ä≤
3 + Œ≥3

.
(3.62)
Let us now factorize A11 as HT
11H11 and proceed with the reduction method
already described in Section 3.8.3 for block tridiagonal matrices. We obtain
the system
 H11
H21
0
A‚Ä≤
33 ‚àíH21HT
21
  x1
x3

=
 c1
b‚Ä≤
3 + Œ≥3 ‚àíH21c1

where H21 = H‚àíT
11 A13 and c1 = H‚àíT
11 b1. The second equation of this system
yields Œ≥3 explicitly as
Œ≥3 =

A‚Ä≤
33 ‚àíHT
21H21

x3 ‚àíb‚Ä≤
3 + HT
21c1.
Substituting this equation into the system for the second substructure, one
ends up with a system only in the unknowns x2 and x3
 A22
A23
AT
23
A‚Ä≤‚Ä≤‚Ä≤
33
  x2
x3

=
 b2
b‚Ä≤‚Ä≤‚Ä≤
3

,
(3.63)
where A‚Ä≤‚Ä≤‚Ä≤
33 = A33 ‚àíHT
21H21 and b‚Ä≤‚Ä≤‚Ä≤
3 = b3 ‚àíHT
21c1. Once (3.63) has been
solved, it will be possible, by backsubstitution into (3.62), to compute also
x1.
The technique described above can be easily extended to the case of
several substructures and its eÔ¨Éciency will increase the more the substruc-
tures are mutually independent. It reproduces in nuce the so-called frontal
method (introduced by Irons [Iro70]), which is quite popular in the solution
of Ô¨Ånite element systems (for an implementation, we refer to the UMF-
PACK library [DD95]).
Remark 3.6 (The Schur complement) An approach that is dual to
the above method consists of reducing the starting system to a system
acting only on the interface unknowns x3, passing through the assembling
of the Schur complement of matrix A, deÔ¨Åned in the 3√ó3 case at hand as
S = A33 ‚àíAT
13A‚àí1
11 A13 ‚àíAT
23A‚àí1
22 A23.
The original problem is thus equivalent to the system
Sx3 = b3 ‚àíAT
13A‚àí1
11 b1 ‚àíAT
23A‚àí1
22 b2.
This system is full (even if the matrices Aij were sparse) and can be solved
using either a direct or an iterative method, provided that a suitable pre-
conditioner is available. Once x3 has been computed, one can get x1 and

3.10 Accuracy of the Solution Achieved Using GEM
103
x2 by solving two systems of reduced size, whose matrices are A11 and A22,
respectively.
We also notice that if the block matrix A is symmetric and positive
deÔ¨Ånite, then the linear system on the Schur complement S is no more
ill-conditioned than the original system on A, since
K2(S) ‚â§K2(A)
(for a proof, see Lemma 3.12, [Axe94]. See also [CM94] and [QV99]).
‚ñ†
3.9.3
Nested Dissection
This is a renumbering technique quite similar to substructuring. In practice,
it consists of repeating the decomposition process several times at each
substructure level, until the size of each single block is made suÔ¨Éciently
small. In Figure 3.8 a possible nested dissection is shown in the case of the
matrix considered in the previous section. Once the subdivision procedure
has been completed, the vertices are renumbered starting with the nodes
belonging to the latest substructuring level and moving progressively up to
the Ô¨Årst level. In the example at hand, the new node ordering is 11, 9, 7,
6, 12, 8, 4, 2, 1, 5, 3.
This procedure is particularly eÔ¨Äective if the problem has a large size and
the substructures have few connections between them or exhibit a repetitive
pattern [Geo73].
3.10
Accuracy of the Solution Achieved Using
GEM
Let us analyze the eÔ¨Äects of rounding errors on the accuracy of the solution
yielded by GEM. Suppose that A and b are a matrix and a vector of
Ô¨Çoating-point numbers. Denoting by L and U, respectively, the matrices
of the LU factorization induced by GEM and computed in Ô¨Çoating-point
arithmetic, the solution x yielded by GEM can be regarded as being the
solution (in exact arithmetic) of the perturbed system (A + Œ¥A)x = b,
where Œ¥A is a perturbation matrix such that
|Œ¥A| ‚â§nu
+
3|A| + 5|L||U|
,
+ O(u2),
(3.64)
where u is the roundoÔ¨Äunit and the matrix absolute value notation has
been used (see [GL89], Section 3.4.6). As a consequence, the entries of Œ¥A
will be small in size if the entries of L and U are small. Using partial
pivoting allows for bounding below 1 the module of the entries of L in such
a way that, passing to the inÔ¨Ånity norm and noting that ‚à•L‚à•‚àû‚â§n, the

104
3. Direct Methods for the Solution of Linear Systems
1
A
2
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                






























                                                 






1
                         




A
A
2
1
2
      


    

B
A
6
5
4
3
C
         


         


         


    

                                                  









                                                                                           












A
4
A
                                                                                                                                                                                                                                                                                                                                                                                                                                                                  





























C
3
4
C
B
6B
5
6
5
3
FIGURE 3.8. Two steps of nested dissection. Graph partitioning (left) and matrix
reordering (right)
estimate (3.64) becomes
‚à•Œ¥A‚à•‚àû‚â§nu
+
3‚à•A‚à•‚àû+ 5n‚à•U‚à•‚àû
,
+ O(u2).
(3.65)
The bound for ‚à•Œ¥A‚à•‚àûin (3.65) is of practical use only if it is possible to
provide an estimate for ‚à•U‚à•‚àû. With this aim, backward analysis can be
carried out introducing the so-called growth factor
œÅn =
max
i,j,k |a(k)
ij |
max
i,j |aij| .
(3.66)
Taking advantage of the fact that |uij| ‚â§œÅnmax
i,j |aij|, the following result
due to Wilkinson can be drawn from (3.65),
‚à•Œ¥A‚à•‚àû‚â§8un3œÅn‚à•A‚à•‚àû+ O(u2).
(3.67)
The growth factor can be bounded by 2n‚àí1 and, although in most of the
cases it is of the order of 10, there exist matrices for which the inequality in
(3.67) becomes an equality (see, for instance, Exercise 5). For some special
classes of matrices, a sharp bound for œÅn can be found:

3.10 Accuracy of the Solution Achieved Using GEM
105
1. for banded matrices with upper and lower bands equal to p, œÅn ‚â§
22p‚àí1 ‚àí(p ‚àí1)2p‚àí2. As a consequence, in the tridiagonal case one
gets œÅn ‚â§2;
2. for Hessenberg matrices, œÅn ‚â§n;
3. for symmetric positive deÔ¨Ånite matrices, œÅn = 1;
4. for matrices strictly diagonally dominant by columns, œÅn ‚â§2.
To achieve better stability when using GEM for arbitrary matrices, re-
sorting to complete pivoting would seem to be mandatory, since it ensures
that œÅn ‚â§n1/2 
2 ¬∑ 31/2 ¬∑ . . . ¬∑ n1/(n‚àí1)1/2. Indeed, this growth is slower
than 2n‚àí1 as n increases.
However, apart from very special instances, GEM with only partial piv-
oting exhibits acceptable growth factors. This make it the most commonly
employed method in the computational practice.
Example 3.7 Consider the linear system (3.2) with
A =
 Œµ
1
1
0

,
b =
 1 + Œµ
1

,
(3.68)
which admits the exact solution x=1 for any value of Œµ. The matrix is well-
conditioned, having K‚àû(A) = (1 + Œµ)2. Attempting to solve the system for Œµ =
10‚àí15 by the LU factorization with 16 signiÔ¨Åcant digits, and using the Programs
5, 2 and 3, yields the solution x = [0.8881784197001253, 1.000000000000000]T ,
with an error greater than 11% on the Ô¨Årst component. Some insight into the
causes of the inaccuracy of the computed solution can be drawn from (3.64).
Indeed this latter does not provide a uniformly small bound for all the entries of
matrix Œ¥A, rather
|Œ¥A| ‚â§
 3.55 ¬∑ 10‚àí30
1.33 ¬∑ 10‚àí15
1.33 ¬∑ 10‚àí15
2.22

.
Notice that the entries of the corresponding matrices L and U are quite large in
module. Conversely, resorting to GEM with partial or complete pivoting yields
the exact solution of the system (see Exercise 6).
‚Ä¢
Let us now address the role of the condition number in the error analysis
for GEM. GEM yields a solution x that is typically characterized by having
a small residual r = b ‚àíAx (see [GL89]). This feature, however, does not
ensure that the error x ‚àíx is small when K(A) ‚â´1 (see Example 3.8). In
fact, if Œ¥b in (3.11) is regarded as being the residual, then
‚à•x ‚àíx‚à•
‚à•x‚à•
‚â§K(A)‚à•r‚à•
1
‚à•A‚à•‚à•x‚à•‚â§K(A) ‚à•r‚à•
‚à•b‚à•.
This result will be applied to devise methods, based on the a posteriori
analysis, for improving the accuracy of the solution of GEM (see Section
3.12).

106
3. Direct Methods for the Solution of Linear Systems
Example 3.8 Consider the linear system Ax = b with
A =

1
1.0001
1.0001
1

,
b =
 1
1

,
which admits the solution x = (0.499975 . . . , 0.499975 . . . )T . Assuming as an ap-
proximate solution the vector x = (‚àí4.499775, 5.5002249)T , one Ô¨Ånds the residual
r ‚âÉ(‚àí0.001, 0)T , which is small although x is quite diÔ¨Äerent from the exact so-
lution. The reason for this is due to the ill-conditioning of matrix A. Indeed in
this case K‚àû(A) = 20001.
‚Ä¢
An estimate of the number of exact signiÔ¨Åcant digits of a numerical
solution of a linear system can be given as follows. From (3.13), letting
Œ≥ = u and assuming that uK‚àû(A) ‚â§1/2 we get
‚à•Œ¥x‚à•‚àû
‚à•x‚à•‚àû
‚â§
2uK‚àû(A)
1 ‚àíuK‚àû(A) ‚â§4uK‚àû(A).
As a consequence
‚à•x ‚àíx‚à•‚àû
‚à•x‚à•‚àû
‚âÉuK‚àû(A).
(3.69)
Assuming that u ‚âÉŒ≤‚àít and K‚àû(A) ‚âÉŒ≤m, one gets that the solution x
computed by GEM will have at least t‚àím exact digits, t being the number
of digits available for the mantissa. In other words, the ill-conditioning of a
system depends both on the capability of the Ô¨Çoating-point arithmetic that
is being used and on the accuracy that is required in the solution.
3.11
An Approximate Computation of K(A)
Suppose that the linear system (3.2) has been solved by a factorization
method. To determine the accuracy of the computed solution, the analy-
sis carried out in Section 3.10 can be used if an estimate of the condition
number K(A) of A, which we denote by K(A), is available. Indeed, al-
though evaluating ‚à•A‚à•can be an easy task if a suitable norm is chosen
(for instance, ‚à•¬∑ ‚à•1 or ‚à•¬∑ ‚à•‚àû), it is by no means reasonable (or compu-
tationally convenient) to compute A‚àí1 if the only purpose is to evaluate
‚à•A‚àí1‚à•. For this reason, we describe in this section a procedure (proposed
in [CMSW79]) that approximates ‚à•A‚àí1‚à•with a computational cost of the
order of n2 Ô¨Çops.
The basic idea of the algorithm is as follows: ‚àÄd ‚ààRn with d Ã∏= 0, thanks
to the deÔ¨Ånition of matrix norm, ‚à•A‚àí1‚à•‚â•‚à•y‚à•/‚à•d‚à•= Œ≥(d) with Ay = d.
Thus, we look for d in such a way that Œ≥(d) is as large as possible and
assume the obtained value as an estimate of ‚à•A‚àí1‚à•.
For the method to be eÔ¨Äective, the selection of d is crucial. To explain
how to do this, we start by assuming that the QR factorization of A has

3.11 An Approximate Computation of K(A)
107
been computed and that K2(A) is to be approximated. In such an event,
since K2(A) = K2(R) due to Property 1.8, it suÔ¨Éces to estimate ‚à•R‚àí1‚à•2
instead of ‚à•A‚àí1‚à•2. Considerations related to the SVD of R induce approx-
imating ‚à•R‚àí1‚à•2 by the following algorithm:
compute the vectors x and y, solutions to the systems
RT x = d,
Ry = x,
(3.70)
then estimate ‚à•R‚àí1‚à•2 by the ratio Œ≥2 = ‚à•y‚à•2/‚à•x‚à•2. The vector d appearing
in (3.70) should be determined in such a way that Œ≥2 is as close as possible
to the value actually attained by ‚à•R‚àí1‚à•2. It can be shown that, except in
very special cases, Œ≥2 provides for any choice of d a reasonable (although
not very accurate) estimate of ‚à•R‚àí1‚à•2 (see Exercise 15). As a consequence,
a proper selection of d can encourage this natural trend.
Before going on, it is worth noting that computing K2(R) is not an easy
matter even if an estimate of ‚à•R‚àí1‚à•2 is available. Indeed, it would remain
to compute ‚à•R‚à•2 =

œÅ(RT R). To overcome this diÔ¨Éculty, we consider
henceforth K1(R) instead of K2(R) since ‚à•R‚à•1 is easily computable. Then,
heuristics allows us to assume that the ratio Œ≥1 = ‚à•y‚à•1/‚à•x‚à•1 is an estimate
of ‚à•R‚àí1‚à•1, exactly as Œ≥2 is an estimate of ‚à•R‚àí1‚à•2.
Let us now deal with the choice of d. Since RT x = d, the generic compo-
nent xk of x can be formally related to x1, . . . , xk‚àí1 through the formulae
of forward substitution as
r11x0 = d1,
rkkxk = dk ‚àí(r1kx1 + . . . + rk‚àí1,kxk‚àí1),
k ‚â•1.
(3.71)
Assume that the components of d are of the form dk = ¬±Œ∏k, where Œ∏k
are random numbers and set arbitrarily d1 = Œ∏1. Then, x1 = Œ∏1/r11 is
completely determined, while x2 = (d2 ‚àír12x1)/r22 depends on the sign of
d2. We set the sign of d2 as the opposite of r12x1 in such a way to make
‚à•x(1 : 2)‚à•1 = |x1| + |x2|, for a Ô¨Åxed x1, the largest possible. Once x2 is
known, we compute x3 following the same criterion, and so on, until xn.
This approach sets the sign of each component of d and yields a vector
x with a presumably large ‚à•¬∑ ‚à•1. However, it can fail since it is based on
the idea (which is in general not true) that maximizing ‚à•x‚à•1 can be done
by selecting at each step k in (3.71) the component xk which guarantees
the maximum increase of ‚à•x(1 : k ‚àí1)‚à•1 (without accounting for the fact
that all the components are related).
Therefore, we need to modify the method by including a sort of ‚Äúlook-
ahead‚Äù strategy, which accounts for the way of choosing dk aÔ¨Äects all later
values xi, with i > k, still to be computed. Concerning this point, we notice
that for a generic row i of the system it is always possible to compute at

108
3. Direct Methods for the Solution of Linear Systems
step k the vector p(k‚àí1) with components
p(k‚àí1)
i
= 0
i = 1, . . . , k ‚àí1,
p(k‚àí1)
i
= r1ix1 + . . . + rk‚àí1,ixk‚àí1
i = k, . . . , n.
Thus xk = (¬±Œ∏k ‚àíp(k‚àí1)
k
)/rkk. We denote the two possible values of xk by
x+
k and x‚àí
k . The choice between them is now taken not only accounting for
which of the two most increases ‚à•x(1 : k)‚à•1, but also evaluating the increase
of ‚à•p(k)‚à•1. This second contribution accounts for the eÔ¨Äect of the choice of
dk on the components that are still to be computed. We can include both
criteria in a unique test. Denoting by
p(k)+
i
= 0,
p(k)‚àí
i
= 0,
i = 1, . . . , k,
p(k)+
i
= p(k‚àí1)
i
+ rkix+
k ,
p(k)‚àí
i
= p(k‚àí1)
i
+ rkix‚àí
k ,
i = k + 1, . . . , n,
the components of the vectors p(k)+ and p(k)‚àírespectively, we set each
k-th step dk = +Œ∏k or dk = ‚àíŒ∏k according to whether |rkkx+
k | + ‚à•p(k)+‚à•1
is greater or less than |rkkx‚àí
k | + ‚à•p(k)‚àí‚à•1.
Under this choice d is completely determined and the same holds for x.
Now, solving the system Ry = x, we are warranted that ‚à•y‚à•1/‚à•x‚à•1 is a reli-
able approximation to ‚à•R‚àí1‚à•1, so that we can set K1(A) = ‚à•R‚à•1‚à•y‚à•1/‚à•x‚à•1.
In practice the PA=LU factorization introduced in Section 3.5 is usually
available. Based on the previous considerations and on some heuristics, an
analogous procedure to that shown above can be conveniently employed
to approximate ‚à•A‚àí1‚à•1. Precisely, instead of systems (3.70), we must now
solve
(LU)T x = d,
LUy = x.
We set ‚à•y‚à•1/‚à•x‚à•1 as the approximation of ‚à•A‚àí1‚à•1 and, consequently, we
deÔ¨Åne K1(A). The strategy for selecting d can be the same as before;
indeed, solving (LU)T x = d amounts to solving
UT z = d,
LT x = z,
(3.72)
and thus, since UT is lower triangular, we can proceed as in the previous
case. A remarkable diÔ¨Äerence concerns the computation of x. Indeed, while
the matrix RT in the second system of (3.70) has the same condition number
as R, the second system in (3.72) has a matrix LT which could be even more
ill-conditioned than UT . If this were the case, solving for x could lead to
an inaccurate outcome, thus making the whole process useless.
Fortunately, resorting to partial pivoting prevents this circumstance from
occurring, ensuring that any ill-condition in A is reÔ¨Çected in a correspond-
ing ill-condition in U. Moreover, picking Œ∏k randomly between 1/2 and 1

3.12 Improving the Accuracy of GEM
109
guarantees accurate results even in the special cases where L turns out to
be ill-conditioned.
The algorithm presented below is implemented in the LINPACK library
[BDMS79] and in the MATLAB function rcond. This function, in order
to avoid rounding errors, returns as output parameter the reciprocal of
K1(A). A more accurate estimator, described in [Hig88], is implemented in
the MATLAB function condest.
Program 14 implements the approximate evaluation of K1 for a matrix
A of generic form. The input parameters are the size n of the matrix A, the
matrix A, the factors L, U of its PA=LU factorization and the vector theta
containing the random numbers Œ∏k, for k = 1, . . . , n.
Program 14 - cond est : Algorithm for the approximation of K1(A)
function [k1] = cond est(n,A,L,U,theta)
for i=1:n, p(i)=0; end
for k=1:n
zplus=(theta(k)-p(k))/U(k,k);
zminu=(-theta(k)-p(k))/U(k,k);
splus=abs(theta(k)-p(k));
sminu=abs(-theta(k)-p(k));
for i=(k+1):n
splus=splus+abs(p(i)+U(k,i)*zplus);
sminu=sminu+abs(p(i)+U(k,i)*zminu);
end
if splus >= sminu, z(k)=zplus;
else, z(k)=zminu; end
for i=(k+1):n, p(i)=p(i)+U(k,i)*z(k); end
end
z = z‚Äô; x = backward col(L‚Äô,z);
w = forward col(L,x);
y = backward col(U,w);
k1=norm(A,1)*norm(y,1)/norm(x,1);
Example 3.9 Let us consider the Hilbert matrix H4. Its condition number
K1(H4), computed using the MATLAB function invhilb which returns the exact
inverse of H4, is 2.8375 ¬∑ 104. Running Program 14 with theta=(1, 1, 1, 1)T gives
the reasonable estimate K1(H4) = 2.1523 ¬∑ 104 (which is the same as the output
of rcond), while the function condest returns the exact result.
‚Ä¢
3.12
Improving the Accuracy of GEM
As previously noted if the matrix of the system is ill-conditioned, the so-
lution generated by GEM could be inaccurate even though its residual is
small. In this section, we mention two techniques for improving the accu-
racy of the solution computed by GEM.

110
3. Direct Methods for the Solution of Linear Systems
3.12.1
Scaling
If the entries of A vary greatly in size, it is likely that during the elimination
process large entries are summed to small entries, with a consequent onset
of rounding errors. A remedy consists of performing a scaling of the matrix
A before the elimination is carried out.
Example 3.10 Consider again the matrix A of Remark 3.3. Multiplying it on
the right and on the left with matrix D=diag(0.0005, 1, 1), we obtain the scaled
matrix
ÀúA = DAD =
Ô£Æ
Ô£∞
‚àí0.0001
1
1
1
0.78125
0
1
0
0
Ô£π
Ô£ª.
Applying GEM to the scaled system ÀúAÀúx = Db = (0.2, 1.3816, 1.9273)T , we get
the correct solution x = DÀúx.
‚Ä¢
Row scaling of A amounts to Ô¨Ånding a diagonal nonsingular matrix D1
such that the diagonal entries of D1A are of the same size. The linear
system Ax = b transforms into
D1Ax = D1b.
When both rows and columns of A are to be scaled, the scaled version of
(3.2) becomes
(D1AD2)y = D1b
with y = D‚àí1
2 x,
having also assumed that D2 is invertible. Matrix D1 scales the equations
while D2 scales the unknowns. Notice that, to prevent rounding errors, the
scaling matrices are chosen in the form
D1 = diag(Œ≤r1, . . . , Œ≤rn), D2 = diag(Œ≤c1, . . . , Œ≤cn),
where Œ≤ is the base of the used Ô¨Çoating-point arithmetic and the exponents
r1, . . . , rn, c1, . . . , cn must be determined. It can be shown that
‚à•D‚àí1
2 (x ‚àíx)‚à•‚àû
‚à•D‚àí1
2 x‚à•‚àû
‚âÉuK‚àû(D1AD2).
Therefore, scaling will be eÔ¨Äective if K‚àû(D1AD2) is much less than K‚àû(A).
Finding convenient matrices D1 and D2 is not in general an easy matter.
A strategy consists, for instance, of picking up D1 and D2 in such a way
that ‚à•D1AD2‚à•‚àûand ‚à•D1AD2‚à•1 belong to the interval [1/Œ≤, 1], where Œ≤ is
the base of the used Ô¨Çoating-point arithmetic (see [McK62] for a detailed
analysis in the case of the Crout factorization).

3.12 Improving the Accuracy of GEM
111
Remark 3.7 (The Skeel condition number) The Skeel condition num-
ber, deÔ¨Åned as cond(A) = ‚à•|A‚àí1| |A| ‚à•‚àû, is the supremum over the set
x‚ààRn, with x Ã∏= 0, of the numbers
cond(A, x) = ‚à•|A‚àí1| |A| |x| ‚à•‚àû
‚à•x‚à•‚àû
.
Unlike what happens for K(A), cond(A,x) is invariant with respect to a
scaling by rows of A, that is, to transformations of A of the form DA, where
D is a nonsingular diagonal matrix. As a consequence, cond(A) provides a
sound indication of the ill-conditioning of a matrix, irrespectively of any
possible row diagonal scaling.
‚ñ†
3.12.2
Iterative ReÔ¨Ånement
Iterative reÔ¨Ånement is a technique for improving the accuracy of a solution
yielded by a direct method. Suppose that the linear system (3.2) has been
solved by means of LU factorization (with partial or complete pivoting),
and denote by x(0) the computed solution. Having Ô¨Åxed an error tolerance,
toll, the iterative reÔ¨Ånement performs as follows: for i = 0, 1, . . . , until
convergence:
1. compute the residual r(i) = b ‚àíAx(i);
2. solve the linear system Az = r(i) using the LU factorization of A;
3. update the solution setting x(i+1) = x(i) + z;
4. if ‚à•z‚à•/‚à•x(i+1)‚à•< toll, then terminate the process returning the solu-
tion x(i+1). Otherwise, the algorithm restarts at step 1.
In absence of rounding errors, the process would stop at the Ô¨Årst step,
yielding the exact solution. The convergence properties of the method
can be improved by computing the residual r(i) in double precision, while
computing the other quantities in single precision. We call this procedure
mixed-precision iterative reÔ¨Ånement (shortly, MPR), as compared to Ô¨Åxed-
precision iterative reÔ¨Ånement (FPR).
It can be shown that, if ‚à•|A‚àí1| |L| |U| ‚à•‚àûis suÔ¨Éciently small, then at
each step i of the algorithm, the relative error ‚à•x‚àíx(i)‚à•‚àû/‚à•x‚à•‚àûis reduced
by a factor œÅ, which is given by
œÅ ‚âÉ2 n cond(A, x)u
(FPR),
œÅ ‚âÉu
(MPR),
where œÅ is independent of the condition number of A in the case of MPR.
Slow convergence of FPR is a clear indication of the ill-conditioning of the

112
3. Direct Methods for the Solution of Linear Systems
matrix, as it can be shown that, if p is the number of iterations for the
method to converge, then K‚àû(A) ‚âÉŒ≤t(1‚àí1/p).
Even if performed in Ô¨Åxed precision, iterative reÔ¨Ånement is worth using
since it improves the overall stability of any direct method for solving the
system. We refer to [Ric81], [Ske80], [JW77] [Ste73], [Wil63] and [CMSW79]
for an overview of this subject.
3.13
Undetermined Systems
We have seen that the solution of the linear system Ax=b exists and is
unique if n = m and A is nonsingular. In this section we give a meaning
to the solution of a linear system both in the overdetermined case, where
m > n, and in the underdetermined case, corresponding to m < n. We
notice that an underdetermined system generally has no solution unless
the right side b is an element of range(A).
For a detailed presentation, we refer to [LH74], [GL89] and [Bj¬®o88].
Given A‚ààRm√ón with m ‚â•n, b‚ààRm, we say that x‚àó‚ààRn is a solution
of the linear system Ax=b in the least-squares sense if
Œ¶(x‚àó) = ‚à•Ax‚àó‚àíb‚à•2
2 ‚â§min
x‚ààRn‚à•Ax ‚àíb‚à•2
2 = min
x‚ààRnŒ¶(x).
(3.73)
The problem thus consists of minimizing the Euclidean norm of the resid-
ual. The solution of (3.73) can be found by imposing the condition that the
gradient of the function Œ¶ in (3.73) must be equal to zero at x‚àó. From
Œ¶(x) = (Ax ‚àíb)T (Ax ‚àíb) = xT AT Ax ‚àí2xT AT b + bT b,
we Ô¨Ånd that
‚àáŒ¶(x‚àó) = 2AT Ax‚àó‚àí2AT b = 0,
from which it follows that x‚àómust be the solution of the square system
AT Ax‚àó= AT b
(3.74)
known as the system of normal equations. The system is nonsingular if
A has full rank and in such a case the least-squares solution exists and
is unique. We notice that B = AT A is a symmetric and positive deÔ¨Ånite
matrix. Thus, in order to solve the normal equations, one could Ô¨Årst com-
pute the Cholesky factorization B = HT H and then solve the two systems
HT y = AT b and Hx‚àó= y. However, due to roundoÔ¨Äerrors, the com-
putation of AT A may be aÔ¨Äected by a loss of signiÔ¨Åcant digits, with a
consequent loss of positive deÔ¨Åniteness or nonsingularity of the matrix, as
happens in the following example (implemented in MATLAB) where for a

3.13 Undetermined Systems
113
matrix A with full rank, the corresponding matrix fl(AT A) turns out to
be singular
A =
Ô£Æ
Ô£∞
1
1
2‚àí27
0
0
2‚àí27
Ô£π
Ô£ª,
fl(AT A) =

1
1
1
1

.
Therefore, in the case of ill-conditioned matrices it is more convenient to
utilize the QR factorization introduced in Section 3.4.3. Indeed, the follow-
ing result holds.
Theorem 3.8 Let A ‚ààRm√ón, with m ‚â•n, be a full rank matrix. Then
the unique solution of (3.73) is given by
x‚àó= ÀúR‚àí1 ÀúQT b
(3.75)
where ÀúR ‚ààRn√ón and ÀúQ ‚ààRm√ón are the matrices deÔ¨Åned in (3.48) starting
from the QR factorization of A. Moreover, the minimum of Œ¶ is given by
Œ¶(x‚àó) =
m

i=n+1
[(QT b)i]2.
Proof. The QR factorization of A exists and is unique since A has full rank.
Thus, there exist two matrices, Q‚ààRm√óm and R‚ààRm√ón such that A=QR, where
Q is orthogonal. Since orthogonal matrices preserve the Euclidean scalar product
(see Property 1.8), it follows that
‚à•Ax ‚àíb‚à•2
2 = ‚à•Rx ‚àíQT b‚à•2
2.
Recalling that R is upper trapezoidal, we have
‚à•Rx ‚àíQT b‚à•2
2 = ‚à•ÀúRx ‚àíÀúQT b‚à•2
2 +
m

i=n+1
[(QT b)i]2,
so that the minimum is achieved when x = x‚àó.
3
For more details about the analysis of the computational cost the algo-
rithm (which depends on the actual implementation of the QR factoriza-
tion), as well as for results about its stability, we refer the reader to the
texts quoted at the beginning of the section.
If A does not have full rank, the solution techniques above fail, since in
this case if x‚àóis a solution to (3.73), the vector x‚àó+ z, with z ‚ààker(A), is
a solution too. We must therefore introduce a further constraint to enforce
the uniqueness of the solution. Typically, one requires that x‚àóhas minimal
Euclidean norm, so that the least-squares problem can be formulated as
Ô¨Ånd x‚àó‚ààRn with minimal Euclidean norm such that
‚à•Ax‚àó‚àíb‚à•2
2 ‚â§min
x‚ààRn‚à•Ax ‚àíb‚à•2
2.
(3.76)

114
3. Direct Methods for the Solution of Linear Systems
This problem is consistent with (3.73) if A has full rank, since in this case
(3.73) has a unique solution which necessarily must have minimal Euclidean
norm.
The tool for solving (3.76) is the singular value decomposition (or SVD,
see Section 1.9), for which the following theorem holds.
Theorem 3.9 Let A ‚ààRm√ón with SVD given by A = UŒ£VT . Then the
unique solution to (3.76) is
x‚àó= A‚Ä†b
(3.77)
where A‚Ä† is the pseudo-inverse of A introduced in DeÔ¨Ånition 1.15.
Proof. Using the SVD of A, problem (3.76) is equivalent to Ô¨Ånding w = VT x
such that w has minimal Euclidean norm and
‚à•Œ£w ‚àíUT b‚à•2
2 ‚â§‚à•Œ£y ‚àíUT b‚à•2
2,
‚àÄy ‚ààRn.
If r is the number of nonzero singular values œÉi of A, then
‚à•Œ£w ‚àíUT b‚à•2
2 =
r

i=1
+
œÉiwi ‚àí(UT b)i
,2
+
m

i=r+1
+
(UT b)i
,2
,
which is minimum if wi = (UT b)i/œÉi for i = 1, . . . , r. Moreover, it is clear that
among the vectors w of Rn having the Ô¨Årst r components Ô¨Åxed, the one with
minimal Euclidean norm has the remaining n ‚àír components equal to zero.
Thus the solution vector is w‚àó= Œ£‚Ä†UT b, that is, x‚àó= VŒ£‚Ä†UT b = A‚Ä†b, where
Œ£‚Ä† is the diagonal matrix deÔ¨Åned in (1.11).
3
As for the stability of problem (3.76), we point out that if the matrix
A does not have full rank, the solution x‚àóis not necessarily a continuous
function of the data, so that small changes on these latter might produce
large variations in x‚àó. An example of this is shown below.
Example 3.11 Consider the system Ax = b with
A =
Ô£Æ
Ô£∞
1
0
0
0
0
0
Ô£π
Ô£ª,
b =
Ô£Æ
Ô£∞
1
2
3
Ô£π
Ô£ª,
rank(A) = 1.
Using the MATLAB function svd we can compute the SVD of A. Then computing
the pseudo-inverse, one Ô¨Ånds the solution vector x‚àó= (1, 0)T . If we perturb the
null entry a22, with the value 10‚àí12, the perturbed matrix has (full) rank 2
and the solution (which is unique in the sense of (3.73)) is now given by x‚àó=

1, 2 ¬∑ 1012T .
‚Ä¢
We refer the reader to Section 5.8.3 for the approximate computation of
the SVD of a matrix.

3.14 Applications
115
In the case of underdetermined systems, for which m < n, if A has full
rank the QR factorization can still be used. In particular, when applied
to the transpose matrix AT , the method yields the solution of minimal
euclidean norm. If, instead, the matrix has not full rank, one must resort
to SVD.
Remark 3.8 If m = n (square system), both SVD and QR factorization
can be used to solve the linear system Ax=b, as alternatives to GEM.
Even though these algorithms require a number of Ô¨Çops far superior to
GEM (SVD, for instance, requires 12n3 Ô¨Çops), they turn out to be more
accurate when the system is ill-conditioned and nearly singular.
‚ñ†
Example 3.12 Compute the solution to the linear system H15x=b, where H15
is the Hilbert matrix of order 15 (see (3.32)) and the right side is chosen in
such a way that the exact solution is the unit vector x = 1. Using GEM with
partial pivoting yields a solution aÔ¨Äected by a relative error larger than 100%. A
solution of much better quality is obtained by passing through the computation
of the pseudo-inverse, where the entries in Œ£ that are less than 10‚àí13 are set
equal to zero.
‚Ä¢
3.14
Applications
In this section we present two problems, suggested by structural mechan-
ics and grid generation in Ô¨Ånite element analysis, whose solutions require
solving large linear systems.
3.14.1
Nodal Analysis of a Structured Frame
Let us consider a structured frame which is made by rectilinear beams con-
nected among them through hinges (referred to as the nodes) and suitably
constrained to the ground. External loads are assumed to be applied at
the nodes of the frame and for any beam in the frame the internal actions
amount to a unique force of constant strength and directed as the beam
itself. If the normal stress acting on the beam is a traction we assume
that it has positive sign, otherwise the action has negative sign. Structured
frames are frequently employed as covering structures for large size public
buildings like exhibition stands, railway stations or airport halls.
To determine the internal actions in the frame, that are the unknowns
of the mathematical problem, a nodal analysis is used (see [Zie77]): the
equilibrium with respect to translation is imposed at every node of the
frame yielding a sparse and large-size linear system. The resulting matrix
has a sparsity pattern which depends on the numbering of the unknowns
and that can strongly aÔ¨Äect the computational eÔ¨Äort of the LU factorization

116
3. Direct Methods for the Solution of Linear Systems
due to Ô¨Åll-in. We will show that the Ô¨Åll-in can be dramatically reduced by
a suitable reordering of the unknowns.
The structure shown in Figure 3.9 is arc-shaped and is symmetric with
respect to the origin. The radii r and R of the inner and outer circles are
equal to 1 and 2, respectively. An external vertical load of unit size directed
downwards is applied at (0, 1) while the frame is constrained to ground
through a hinge at (‚àí(r + R), 0) and a bogie at (r + R, 0). To generate
the structure we have partitioned the half unit circle in nŒ∏ uniform slices,
resulting in a total number of n = 2(nŒ∏ + 1) nodes and a matrix size of
m = 2n. The structure in Figure 3.9 has nŒ∏ = 7 and the unknowns are
numbered following a counterclockwise labeling of the beams starting from
the node at (1, 0).
We have represented the structure along with the internal actions com-
puted by solving the nodal equilibrium equations where the width of the
beams is proportional to the strength of the computed action. Black is
used to identify tractions whereas gray is associated with compressions. As
expected the maximum traction stress is attained at the node where the
external load is applied.
‚àí2
‚àí1.5
‚àí1
‚àí0.5
0
0.5
1
1.5
2
‚àí0.5
0
0.5
1
1.5
2
2.5
FIGURE 3.9. A structured frame loaded at the point (0, 1)
We show in Figure 3.10 the sparsity pattern of matrix A (left) and that
of the L-factor of its LU factorization with partial pivoting (right) in the
case nŒ∏ = 40 which corresponds to a size of 164 √ó 164. Notice the large
Ô¨Åll-in eÔ¨Äect arising in the lower part of L which results in an increase of
the nonzero entries from 645 (before the factorization) to 1946 (after the
factorization).

3.14 Applications
117
0
20
40
60
80
100
120
140
160
0
20
40
60
80
100
120
140
160
nz = 645
0
20
40
60
80
100
120
140
160
0
20
40
60
80
100
120
140
160
nz = 1946
FIGURE 3.10. Sparsity pattern of matrix A (left) and of the L-factor of the LU
factorization with partial pivoting (right) in the case nŒ∏ = 40
In view of the solution of the linear system by a direct method, the
increase of the nonzero entries demands for a suitable reordering of the
unknowns. For this purpose we use the MATLAB function symrcm which
implements the symmetric reverse Cuthill-McKee algorithm described in
Section 3.9.1. The sparsity pattern, after reordering, is shown in Figure 3.11
(left) while the L-factor of the LU factorization of the reordered matrix
is shown in Figure 3.11 (right). The results indicate that the reordering
procedure has ‚Äúscattered‚Äù the sparsity pattern throughout the matrix with
a relatively modest increase of the nonzero entries from 645 to 1040.
0
20
40
60
80
100
120
140
160
0
20
40
60
80
100
120
140
160
nz = 645
0
20
40
60
80
100
120
140
160
0
20
40
60
80
100
120
140
160
nz = 1040
FIGURE 3.11. Sparsity pattern of matrix A (left) after a reordering with the sym-
metric reverse Cuthill-McKee algorithm and the L-factor of the LU factorization
of the reordered matrix with partial pivoting (right) in the case nŒ∏ = 40
The eÔ¨Äectiveness of the symmetric reverse Cuthill-McKee reordering pro-
cedure is demonstrated in Figure 3.12 which shows the number of nonzero
entries nz in the L-factor of A as a function of the size m of the matrix
(represented on the x-axis). In the reordered case (solid line) a linear in-

118
3. Direct Methods for the Solution of Linear Systems
crease of nz with m can be clearly appreciated at the expense of a dramatic
Ô¨Åll-in growing with m if no reordering is performed (dashed line).
0
100
200
300
400
500
600
700
800
900
1000
0
1
2
3
4
5
6 x 10
4
FIGURE 3.12. Number of nonzero entries in the L-factor of A as a function of
the size m of the matrix, with (solid line) and without (dashed line) reordering
3.14.2
Regularization of a Triangular Grid
The numerical solution of a problem in a two-dimensional domain D of
polygonal form, for instance by Ô¨Ånite element or Ô¨Ånite diÔ¨Äerence methods,
very often requires that D be decomposed in smaller subdomains, usually
of triangular form (see for instance Section 9.9.2).
Suppose that D =
-
T ‚ààTh
T, where Th is the considered triangulation (also
called computational grid) and h is a positive parameter which characterizes
the triangulation. Typically, h denotes the maximum length of the triangle
edges. We shall also assume that two triangles of the grid, T1 and T2, have
either null intersection or share a vertex or a side.
The geometrical properties of the computational grid can heavily aÔ¨Äect the
quality of the approximate numerical solution. It is therefore convenient to
devise a suÔ¨Éciently regular triangulation, such that, for any T ‚ààTh, the
ratio between the maximum length of the sides of T (the diameter of T)
and the diameter of the circle inscribed within T (the sphericity of T) is
bounded by a constant independent of T. This latter requirement can be
satisÔ¨Åed employing a regularization procedure, applied to an existing grid.
We refer to [Ver96] for further details on this subject.
Let us assume that Th contains NT triangles and N vertices, of which Nb,
lying on the boundary ‚àÇD of D, are kept Ô¨Åxed and having coordinates
x(‚àÇD)
i
= (x(‚àÇD)
i
, y(‚àÇD)
i
). We denote by Nh the set of grid nodes, excluding
the boundary nodes, and for each node xi = (xi, yi)T ‚ààNh, let Pi and Zi
respectively be the set of triangles T ‚ààTh sharing xi (called the patch of

3.14 Applications
119
T
k
xi
xj
x
FIGURE 3.13. An example of a decomposition into triangles of a polygonal do-
main D (left), and the eÔ¨Äect of the barycentric regularization on a patch of
triangles (right). The newly generated grid is plotted in dashed line
xi) and the set of nodes of Pi except node xi itself (see Figure 3.13, right).
We let ni = dim(Zi).
The regularization procedure consists of moving the generic node xi to
a new position which is determined by the center of gravity of the polygon
generated by joining the nodes of Zi, and for that reason it is called a
barycentric regularization. The eÔ¨Äect of such a procedure is to force all the
triangles that belong to the interior of the domain to assume a shape that
is as regular as possible (in the limit, each triangle should be equilateral).
In practice, we let
xi =
Ô£´
Ô£≠
xj‚ààZi
xj
Ô£∂
Ô£∏/ni,
‚àÄxi ‚ààNh,
xi = x(‚àÇD)
i
if xi ‚àà‚àÇD.
Two systems must then be solved, one for the x-components {xi} and the
other for the y-components {yi}. Denoting by zi the generic unknown, the
i-th row of the system, in the case of internal nodes, reads
nizi ‚àí

zj‚ààZi
zj = 0,
‚àÄi ‚ààNh,
(3.78)
while for the boundary nodes the identities zi = z(‚àÇD)
i
hold. Equations
(3.78) yield a system of the form Az = b, where A is a symmetric and pos-
itive deÔ¨Ånite matrix of order N ‚àíNb which can be shown to be an M-matrix
(see Section 1.12). This property ensures that the new grid coordinates sat-
isfy minimum and maximum discrete principles, that is, they take a value
which is between the minimum and the maximum values attained on the
boundary.
Let us apply the regularization technique to the triangulation of the unit
square in Figure 3.14, which is aÔ¨Äected by a severe non uniformity of the
triangle size. The grid consists of NT = 112 triangles and N = 73 vertices,

120
3. Direct Methods for the Solution of Linear Systems
0
0.2
0.4
0.6
0.8
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.2
0.4
0.6
0.8
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
FIGURE 3.14. Triangulation before (left) and after (right) the regularization
of which Nb = 32 are on the boundary. The size of each of the two linear
systems (3.78) is thus equal to 41 and their solution is carried out by the
LU factorization of matrix A in its original form (1) and using its sparse
format (2), obtained using the Cuthill-McKee inverse reordering algorithm
described in Section 3.9.1.
In Figure 3.15 the sparsity patterns of A are displayed, without and with
reordering; the integer nz = 237 denotes the number of nonzero entries
in the matrix. Notice that in the second case there is a decrease in the
bandwidth of the matrix, to which corresponds a large reduction in the
operation count from 61623 to 5552. The Ô¨Ånal conÔ¨Åguration of the grid is
displayed in Figure 3.14 (right), which clearly shows the eÔ¨Äectiveness of the
regularization procedure.
0
5
10
15
20
25
30
35
40
0
5
10
15
20
25
30
35
40
nz = 237
0
5
10
15
20
25
30
35
40
0
5
10
15
20
25
30
35
40
nz = 237
FIGURE 3.15. Sparsity patterns of matrix A without and with reordering (left
and right, respectively)

3.15 Exercises
121
3.15
Exercises
1. For any square matrix A‚ààRn√ón, prove the following relations
1
nK2(A) ‚â§K1(A) ‚â§nK2(A),
1
nK‚àû(A) ‚â§K2(A) ‚â§nK‚àû(A),
1
n2 K1(A) ‚â§K‚àû(A) ‚â§n2K1(A).
They allow us to conclude that if a matrix is ill-conditioned in a certain
norm it remains so even in another norm, up to a factor depending on n.
2. Check that the matrix B ‚ààRn√ón: bii = 1, bij = ‚àí1 if i < j, bij = 0 if
i > j, has determinant equal to 1, yet K‚àû(B) is large (equal to n2n‚àí1).
3. Prove that K(AB) ‚â§K(A)K(B), for any two square nonsingular matrices
A,B‚ààRn√ón.
4. Given the matrix A ‚ààR2√ó2, a11 = a22 = 1, a12 = Œ≥, a21 = 0, check that
for Œ≥ ‚â•0, K‚àû(A) = K1(A) = (1 + Œ≥)2. Next, consider the linear system
Ax = b where b is such that x = (1 ‚àíŒ≥, 1)T is the solution. Find a bound
for ‚à•Œ¥x‚à•‚àû/‚à•x‚à•‚àûin terms of ‚à•Œ¥b‚à•‚àû/‚à•b‚à•‚àûwhen Œ¥b = (Œ¥1, Œ¥2)T . Is the
problem well- or ill-conditioned?
5. Consider the matrix A ‚ààRn√ón, with entries aij = 1 if i = j or j = n,
aij = ‚àí1 if i > j, zero otherwise. Show that A admits an LU factorization,
with |lij| ‚â§1 and unn = 2n‚àí1.
6. Consider matrix (3.68) in Example 3.7. Prove that the matrices L and U
have entries very large in module. Check that using GEM with complete
pivoting yields the exact solution.
7. Devise a variant of GEM that transforms a nonsingular matrix A ‚ààRn√ón
directly into a diagonal matrix D. This process is commonly known as the
Gauss-Jordan method. Find the Gauss-Jordan transformation matrices Gi,
i = 1, . . . , n, such that Gn . . . G1A = D.
8. Let A be a sparse matrix of order n. Prove that the computational cost of
the LU factorization of A is given by (3.61). Prove also that it is always
less than
1
2
n

k=1
mk(A) (mk(A) + 3) .
9. Prove that, if A is a symmetric and positive deÔ¨Ånite matrix, solving the
linear system Ax = b amounts to computing x= n
i=1(ci/Œªi)vi, where Œªi
are the eigenvalues of A and vi are the corresponding eigenvectors.
10. (From [JM92]). Consider the following linear system
 1001
1000
1000
1001
  x1
x2

=
 b1
b2

.
Using Exercise 9, explain why, when b = (2001, 2001)T , a small change
Œ¥b = (1, 0)T produces large variations in the solution, while, conversely,

122
3. Direct Methods for the Solution of Linear Systems
when b = (1, ‚àí1)T , a small variation Œ¥x = (0.001, 0)T in the solution
induces a large change in b.
[Hint : expand the right hand side on the basis of the eigenvectors of the
matrix.]
11. Characterize the Ô¨Åll-in for a matrix A ‚ààRn√ón having nonzero entries only
on the main diagonal and on the Ô¨Årst column and last row. Propose a
permutation that minimizes the Ô¨Åll-in.
[Hint : it suÔ¨Éces to exchange the Ô¨Årst row and the Ô¨Årst column with the
last row and the last column, respectively.]
12. Consider the linear system Hnx = b, where Hn is the Hilbert matrix of
order n. Estimate, as a function of n, the maximum number of signiÔ¨Åcant
digits that are expected when solving the system by GEM.
13. Given the vectors
v1 = [1, 1, 1, ‚àí1]T ,
v2 = [2, ‚àí1, ‚àí1, 1]T
v3 = [0, 3, 3, ‚àí3]T ,
v4 = [‚àí1, 2, 2, 1]T
generate an orthonormal system using the Gram-Schmidt algorithm, in
either its standard and modiÔ¨Åed versions, and compare the obtained results.
What is the dimension of the space generated by the given vectors?
14. Prove that if A=QR then
1
nK1(A) ‚â§K1(R) ‚â§nK1(A),
while K2(A) = K2(R).
15. Let A ‚ààRn√ón be a nonsingular matrix. Determine the conditions under
which the ratio ‚à•y‚à•2/‚à•x‚à•2, with x and y as in (3.70), approximates ‚à•A‚àí1‚à•2.
[Solution : let UŒ£VT be the singular value decomposition of A. Denote
by ui, vi the column vectors of U and V, respectively, and expand the
vector d in (3.70) on the basis spanned by {vi}. Then d = n
i=1 Àúdivi and,
from (3.70), x = n
i=1( Àúdi/œÉi)ui, y = n
i=1( Àúdi/œÉ2
i )vi, having denoted the
singular values of A by œÉ1, . . . , œÉn.
The ratio
‚à•y‚à•2/‚à•x‚à•2 =
. n

i=1
( Àúdi/œÉ2
i )2/
n

i=1
( Àúdi/œÉi)2
/1/2
is about equal to œÉ‚àí1
n
= ‚à•A‚àí1‚à•2 if: (i) y has a relevant component in the
direction of vn (i.e., if Àúdn is not excessively small), and (ii) the ratio Àúdn/œÉn
is not negligible with respect to the ratios Àúdi/œÉi for i = 1, . . . , n ‚àí1. This
last circumstance certainly occurs if A is ill-conditioned in the ‚à•¬∑ ‚à•2-norm
since œÉn ‚â™œÉ1.]

4
Iterative Methods for Solving Linear
Systems
Iterative methods formally yield the solution x of a linear system after an
inÔ¨Ånite number of steps. At each step they require the computation of the
residual of the system. In the case of a full matrix, their computational
cost is therefore of the order of n2 operations for each iteration, to be
compared with an overall cost of the order of 2
3n3 operations needed by
direct methods. Iterative methods can therefore become competitive with
direct methods provided the number of iterations that are required to con-
verge (within a prescribed tolerance) is either independent of n or scales
sublinearly with respect to n.
In the case of large sparse matrices, as discussed in Section 3.9, direct
methods may be unconvenient due to the dramatic Ô¨Åll-in, although ex-
tremely eÔ¨Écient direct solvers can be devised on sparse matrices featuring
special structures like, for example, those encountered in the approximation
of partial diÔ¨Äerential equations (see Chapters 12 and 13).
Finally, we notice that, when A is ill-conditioned, a combined use of direct
and iterative methods is made possible by preconditioning techniques that
will be addressed in Section 4.3.2.
4.1
On the Convergence of Iterative Methods
The basic idea of iterative methods is to construct a sequence of vectors
x(k) that enjoy the property of convergence
x = lim
k‚Üí‚àûx(k),
(4.1)

124
4. Iterative Methods for Solving Linear Systems
where x is the solution to (3.2). In practice, the iterative process is stopped
at the minimum value of n such that ‚à•x(n) ‚àíx‚à•< Œµ, where Œµ is a Ô¨Åxed
tolerance and ‚à•¬∑ ‚à•is any convenient vector norm. However, since the exact
solution is obviously not available, it is necessary to introduce suitable
stopping criteria to monitor the convergence of the iteration (see Section
4.6).
To start with, we consider iterative methods of the form
x(0) given,
x(k+1) = Bx(k) + f,
k ‚â•0,
(4.2)
having denoted by B an n √ó n square matrix called the iteration matrix
and by f a vector that is obtained from the right hand side b.
DeÔ¨Ånition 4.1 An iterative method of the form (4.2) is said to be consis-
tent with (3.2) if f and B are such that x = Bx + f. Equivalently,
f = (I ‚àíB)A‚àí1b.
‚ñ†
Having denoted by
e(k) = x(k) ‚àíx
(4.3)
the error at the k-th step of the iteration, the condition for convergence
(4.1) amounts to requiring that lim
k‚Üí‚àûe(k) = 0 for any choice of the initial
datum x(0) (often called the initial guess).
Consistency alone does not suÔ¨Éce to ensure the convergence of the iter-
ative method (4.2), as shown in the following example.
Example 4.1 To solve the linear system 2Ix = b, consider the iterative method
x(k+1) = ‚àíx(k) + b,
which is obviously consistent. This scheme is not convergent for any choice of
the initial guess. If, for instance, x(0) = 0, the method generates the sequence
x(2k) = 0, x(2k+1) = b, k = 0, 1, . . . .
On the other hand, if x(0) = 1
2b the method is convergent.
‚Ä¢
Theorem 4.1 Let (4.2) be a consistent method. Then, the sequence of vec-
tors

x(k)
converges to the solution of (3.2) for any choice of x(0) iÔ¨Ä
œÅ(B) < 1.
Proof. From (4.3) and the consistency assumption, the recursive relation e(k+1) =
Be(k) is obtained. Therefore,
e(k) = Bke(0),
‚àÄk = 0, 1, . . .
(4.4)

4.1 On the Convergence of Iterative Methods
125
Thus, thanks to Theorem 1.5, it follows that lim
k‚Üí‚àûBke(0) = 0 for any e(0) iÔ¨Ä
œÅ(B) < 1.
Conversely, suppose that œÅ(B) > 1, then there exists at least one eigenvalue
Œª(B) with module greater than 1. Let e(0) be an eigenvector associated with Œª;
then Be(0) = Œªe(0) and, therefore, e(k) = Œªke(0). As a consequence, e(k) cannot
tend to 0 as k ‚Üí‚àû, since |Œª| > 1.
3
From (1.23) and Theorem 1.5 it follows that a suÔ¨Écient condition for con-
vergence to hold is that ‚à•B‚à•< 1, for any matrix norm. It is reasonable
to expect that the convergence is faster when œÅ(B) is smaller so that an
estimate of œÅ(B) might provide a sound indication of the convergence of
the algorithm. Other remarkable quantities in convergence analysis are con-
tained in the following deÔ¨Ånition.
DeÔ¨Ånition 4.2 Let B be the iteration matrix. We call:
1. ‚à•Bm‚à•the convergence factor after m steps of the iteration;
2. ‚à•Bm‚à•1/m the average convergence factor after m steps;
3. Rm(B) = ‚àí1
m log ‚à•Bm‚à•the average convergence rate after m steps.
‚ñ†
These quantities are too expensive to compute since they require evaluating
Bm. Therefore, it is usually preferred to estimate the asymptotic conver-
gence rate, which is deÔ¨Åned as
R(B) = lim
k‚Üí‚àûRk(B) = ‚àílog œÅ(B)
(4.5)
where Property 1.13 has been accounted for. In particular, if B were sym-
metric, we would have
Rm(B) = ‚àí1
m log ‚à•Bm‚à•2 = ‚àílog œÅ(B).
In the case of nonsymmetric matrices, œÅ(B) sometimes provides an overop-
timistic estimate of ‚à•Bm‚à•1/m (see [Axe94], Section 5.1). Indeed, although
œÅ(B) < 1, the convergence to zero of the sequence ‚à•Bm‚à•might be non-
monotone (see Exercise 1). We Ô¨Ånally notice that, due to (4.5), œÅ(B) is
the asymptotic convergence factor. Criteria for estimating the quantities
deÔ¨Åned so far will be addressed in Section 4.6.
Remark 4.1 The iterations introduced in (4.2) are a special instance of
iterative methods of the form
x(0) = f0(A, b),
x(n+1) = fn+1(x(n), x(n‚àí1), . . . , x(n‚àím), A, b), for n ‚â•m,

126
4. Iterative Methods for Solving Linear Systems
where fi and x(m), . . . , x(1) are given functions and vectors, respectively.
The number of steps which the current iteration depends on is called the
order of the method. If the functions fi are independent of the step index i,
the method is called stationary, otherwise it is nonstationary. Finally, if fi
depends linearly on x(0), . . . , x(m), the method is called linear, otherwise
it is nonlinear.
In the light of these deÔ¨Ånitions, the methods considered so far are there-
fore stationary linear iterative methods of Ô¨Årst order. In Section 4.3, exam-
ples of nonstationary linear methods will be provided.
‚ñ†
4.2
Linear Iterative Methods
A general technique to devise consistent linear iterative methods is based
on an additive splitting of the matrix A of the form A=P‚àíN, where P
and N are two suitable matrices and P is nonsingular. For reasons that
will be clear in the later sections, P is called preconditioning matrix or
preconditioner.
Precisely, given x(0), one can compute x(k) for k ‚â•1, solving the systems
Px(k+1) = Nx(k) + b,
k ‚â•0.
(4.6)
The iteration matrix of method (4.6) is B = P‚àí1N, while f = P‚àí1b. Alter-
natively, (4.6) can be written in the form
x(k+1) = x(k) + P‚àí1r(k),
(4.7)
where
r(k) = b ‚àíAx(k)
(4.8)
denotes the residual vector at step k. Relation (4.7) outlines the fact that
a linear system, with coeÔ¨Écient matrix P, must be solved to update the
solution at step k+1. Thus P, besides being nonsingular, ought to be easily
invertible, in order to keep the overall computational cost low. (Notice that,
if P were equal to A and N=0, method (4.7) would converge in one iteration,
but at the same cost of a direct method).
Let us mention two results that ensure convergence of the iteration (4.7),
provided suitable conditions on the splitting of A are fulÔ¨Ålled (for their
proof, we refer to [Hac94]).
Property 4.1 Let A = P ‚àíN, with A and P symmetric and positive def-
inite. If the matrix 2P ‚àíA is positive deÔ¨Ånite, then the iterative method
deÔ¨Åned in (4.7) is convergent for any choice of the initial datum x(0) and
œÅ(B) = ‚à•B‚à•A = ‚à•B‚à•P < 1.

4.2 Linear Iterative Methods
127
Moreover, the convergence of the iteration is monotone with respect to the
norms ‚à•¬∑ ‚à•P and ‚à•¬∑ ‚à•A (i.e., ‚à•e(k+1)‚à•P < ‚à•e(k)‚à•P and ‚à•e(k+1)‚à•A < ‚à•e(k)‚à•A
k = 0, 1, . . . ).
Property 4.2 Let A = P ‚àíN with A symmetric and positive deÔ¨Ånite. If
the matrix P+PT ‚àíA is positive deÔ¨Ånite, then P is invertible, the iterative
method deÔ¨Åned in (4.7) is monotonically convergent with respect to norm
‚à•¬∑ ‚à•A and œÅ(B) ‚â§‚à•B‚à•A < 1.
4.2.1
Jacobi, Gauss-Seidel and Relaxation Methods
In this section we consider some classical linear iterative methods.
If the diagonal entries of A are nonzero, we can single out in each equation
the corresponding unknown, obtaining the equivalent linear system
xi = 1
aii
Ô£Æ
Ô£ØÔ£∞bi ‚àí
n

j=1
jÃ∏=i
aijxj
Ô£π
Ô£∫Ô£ª,
i = 1, . . . , n.
(4.9)
In the Jacobi method, once an arbitrarily initial guess x0 has been chosen,
x(k+1) is computed by the formulae
x(k+1)
i
= 1
aii
Ô£Æ
Ô£ØÔ£∞bi ‚àí
n

j=1
jÃ∏=i
aijx(k)
j
Ô£π
Ô£∫Ô£ª,
i = 1, . . . , n.
(4.10)
This amounts to performing the following splitting for A
P = D,
N = D ‚àíA = E + F,
where D is the diagonal matrix of the diagonal entries of A, E is the lower
triangular matrix of entries eij = ‚àíaij if i > j, eij = 0 if i ‚â§j, and F is
the upper triangular matrix of entries fij = ‚àíaij if j > i, fij = 0 if j ‚â§i.
As a consequence, A=D-(E+F).
The iteration matrix of the Jacobi method is thus given by
BJ = D‚àí1(E + F) = I ‚àíD‚àí1A.
(4.11)
A generalization of the Jacobi method is the over-relaxation method
(or JOR), in which, having introduced a relaxation parameter œâ, (4.10) is
replaced by
x(k+1)
i
= œâ
aii
Ô£Æ
Ô£ØÔ£∞bi ‚àí
n

j=1
jÃ∏=i
aijx(k)
j
Ô£π
Ô£∫Ô£ª+ (1 ‚àíœâ)x(k)
i
,
i = 1, . . . , n.

128
4. Iterative Methods for Solving Linear Systems
The corresponding iteration matrix is
BJœâ = œâBJ + (1 ‚àíœâ)I.
(4.12)
In the form (4.7), the JOR method corresponds to
x(k+1) = x(k) + œâD‚àí1r(k).
This method is consistent for any œâ Ã∏= 0 and for œâ = 1 it coincides with
the Jacobi method.
The Gauss-Seidel method diÔ¨Äers from the Jacobi method in the fact that
at the k + 1-th step the available values of x(k+1)
i
are being used to update
the solution, so that, instead of (4.10), one has
x(k+1)
i
= 1
aii
Ô£Æ
Ô£∞bi ‚àí
i‚àí1

j=1
aijx(k+1)
j
‚àí
n

j=i+1
aijx(k)
j
Ô£π
Ô£ª,
i = 1, . . . , n.
(4.13)
This method amounts to performing the following splitting for A
P = D ‚àíE,
N = F,
and the associated iteration matrix is
BGS = (D ‚àíE)‚àí1F.
(4.14)
Starting from Gauss-Seidel method, in analogy to what was done for
Jacobi iterations, we introduce the successive over-relaxation method (or
SOR method)
x(k+1)
i
= œâ
aii
Ô£Æ
Ô£∞bi ‚àí
i‚àí1

j=1
aijx(k+1)
j
‚àí
n

j=i+1
aijx(k)
j
Ô£π
Ô£ª+ (1 ‚àíœâ)x(k)
i
,
(4.15)
for i = 1, . . . , n. The method (4.15) can be written in vector form as
(I ‚àíœâD‚àí1E)x(k+1) = [(1 ‚àíœâ)I + œâD‚àí1F]x(k) + œâD‚àí1b
(4.16)
from which the iteration matrix is
B(œâ) = (I ‚àíœâD‚àí1E)‚àí1[(1 ‚àíœâ)I + œâD‚àí1F].
(4.17)
Multiplying by D both sides of (4.16) and recalling that A = D ‚àí(E + F)
yields the following form (4.7) of the SOR method
x(k+1) = x(k) +
 1
œâ D ‚àíE
‚àí1
r(k).
It is consistent for any œâ Ã∏= 0 and for œâ = 1 it coincides with Gauss-Seidel
method. In particular, if œâ ‚àà(0, 1) the method is called under-relaxation,
while if œâ > 1 it is called over-relaxation.

4.2 Linear Iterative Methods
129
4.2.2
Convergence Results for Jacobi and Gauss-Seidel
Methods
There exist special classes of matrices for which it is possible to state a
priori some convergence results for the methods examined in the previous
section. The Ô¨Årst result in this direction is the following.
Theorem 4.2 If A is a strictly diagonally dominant matrix by rows, the
Jacobi and Gauss-Seidel methods are convergent.
Proof. Let us prove the part of the theorem concerning the Jacobi method, while
for the Gauss-Seidel method we refer to [Axe94]. Since A is strictly diagonally
dominant by rows, |aii| > n
j=1 |aij| for j Ã∏= i and i = 1, . . . , n. As a consequence,
‚à•BJ‚à•‚àû=
max
i=1,... ,n
n

j=1,jÃ∏=i
|aij|/|aii| < 1, so that the Jacobi method is convergent.
3
Theorem 4.3 If A and 2D‚àíA are symmetric and positive deÔ¨Ånite matri-
ces, then the Jacobi method is convergent and œÅ(BJ) = ‚à•BJ‚à•A = ‚à•BJ‚à•D.
Proof. The theorem follows from Property 4.1 taking P=D.
3
In the case of the JOR method, the assumption on 2D‚àíA can be removed,
yielding the following result.
Theorem 4.4 If A if symmetric positive deÔ¨Ånite, then the JOR method is
convergent if 0 < œâ < 2/œÅ(D‚àí1A).
Proof. The result immediately follows from (4.12) and noting that A has real
positive eigenvalues.
3
Concerning the Gauss-Seidel method, the following result holds.
Theorem 4.5 If A is symmetric positive deÔ¨Ånite, the Gauss-Seidel method
is monotonically convergent with respect to the norm ‚à•¬∑ ‚à•A.
Proof. We can apply Property 4.2 to the matrix P=D‚àíE, upon checking that
P + PT ‚àíA is positive deÔ¨Ånite. Indeed
P + PT ‚àíA = 2D ‚àíE ‚àíF ‚àíA = D,
having observed that (D ‚àíE)T = D ‚àíF. We conclude by noticing that D is
positive deÔ¨Ånite, since it is the diagonal of A.
3
Finally, if A is positive deÔ¨Ånite and tridiagonal, it can be shown that also
the Jacobi method is convergent and
œÅ(BGS) = œÅ2(BJ).
(4.18)

130
4. Iterative Methods for Solving Linear Systems
In this case, the Gauss-Seidel method is more rapidly convergent than the
Jacobi method. Relation (4.18) holds even if A enjoys the following A-
property.
DeÔ¨Ånition 4.3 A consistently ordered matrix M ‚ààRn√ón (that is, a matrix
such that Œ±D‚àí1E+Œ±‚àí1D‚àí1F, for Œ± Ã∏= 0, has eigenvalues that do not depend
on Œ±, where M=D-E-F, D = diag(m11, . . . , mnn), E and F are strictly lower
and upper triangular matrices, respectively) enjoys the A-property if it can
be partitioned in the 2 √ó 2 block form
M =
 ÀúD1
M12
M21
ÀúD2

,
where ÀúD1 and ÀúD2 are diagonal matrices.
‚ñ†
When dealing with general matrices, no a priori conclusions on the conver-
gence properties of the Jacobi and Gauss-Seidel methods can be drawn, as
shown in Example 4.2.
Example 4.2 Consider the 3 √ó 3 linear systems of the form Aix = bi, where bi
is always taken in such a way that the solution of the system is the unit vector,
and the matrices Ai are
A1 =
Ô£Æ
Ô£∞
3
0
4
7
4
2
‚àí1
1
2
Ô£π
Ô£ª,
A2 =
Ô£Æ
Ô£∞
‚àí3
3
‚àí6
‚àí4
7
‚àí8
5
7
‚àí9
Ô£π
Ô£ª,
A3 =
Ô£Æ
Ô£∞
4
1
1
2
‚àí9
0
0
‚àí8
‚àí6
Ô£π
Ô£ª,
A4 =
Ô£Æ
Ô£∞
7
6
9
4
5
‚àí4
‚àí7
‚àí3
8
Ô£π
Ô£ª.
It can be checked that the Jacobi method does fail to converge for A1 (œÅ(BJ) =
1.33), while the Gauss-Seidel scheme is convergent. Conversely, in the case of
A2, the Jacobi method is convergent, while the Gauss-Seidel method fails to
converge (œÅ(BGS) = 1.¬Ø1). In the remaining two cases, the Jacobi method is more
slowly convergent than the Gauss-Seidel method for matrix A3 (œÅ(BJ) = 0.44
against œÅ(BGS) = 0.018), and the converse is true for A4 (œÅ(BJ) = 0.64 while
œÅ(BGS) = 0.77).
‚Ä¢
We conclude the section with the following result.
Theorem 4.6 If the Jacobi method is convergent, then the JOR method
converges if 0 < œâ ‚â§1.
Proof. From (4.12) we obtain that the eigenvalues of BJœâ are
¬µk = œâŒªk + 1 ‚àíœâ,
k = 1, . . . , n,

4.2 Linear Iterative Methods
131
where Œªk are the eigenvalues of BJ. Then, recalling the Euler formula for the
representation of a complex number, we let Œªk = rkeiŒ∏k and get
|¬µk|2 = œâ2r2
k + 2œârk cos(Œ∏k)(1 ‚àíœâ) + (1 ‚àíœâ)2 ‚â§(œârk + 1 ‚àíœâ)2,
which is less than 1 if 0 < œâ ‚â§1.
3
4.2.3
Convergence Results for the Relaxation Method
The following result provides a necessary condition on œâ in order the SOR
method to be convergent.
Theorem 4.7 For any œâ ‚ààR we have œÅ(B(œâ)) ‚â•|œâ ‚àí1|; therefore, the
SOR method fails to converge if œâ ‚â§0 or œâ ‚â•2.
Proof. If {Œªi} denote the eigenvalues of the SOR iteration matrix, then

n

i=1
Œªi
 =
det
0
(1 ‚àíœâ)I + œâD‚àí1F
1 = |1 ‚àíœâ|n.
Therefore, at least one eigenvalue Œªi must exist such that |Œªi| ‚â•|1‚àíœâ| and thus,
in order for convergence to hold, we must have |1 ‚àíœâ| < 1, that is 0 < œâ < 2. 3
Assuming that A is symmetric and positive deÔ¨Ånite, the condition 0 < œâ <
2, besides being necessary, becomes also suÔ¨Écient for convergence. Indeed
the following result holds (for the proof, see [Hac94]).
Property 4.3 (Ostrowski) If A is symmetric and positive deÔ¨Ånite, then
the SOR method is convergent iÔ¨Ä0 < œâ < 2. Moreover, its convergence is
monotone with respect to ‚à•¬∑ ‚à•A.
Finally, if A is strictly diagonally dominant by rows, the SOR method
converges if 0 < œâ ‚â§1.
The results above show that the SOR method is more or less rapidly
convergent, depending on the choice of the relaxation parameter œâ. The
question of how to determine the value œâopt for which the convergence rate
is the highest possible can be given a satisfactory answer only in special
cases (see, for instance, [Axe94], [You71], [Var62] or [Wac66]). Here we limit
ourselves to quoting the following result (whose proof is in [Axe94]).
Property 4.4 If the matrix A enjoys the A-property and if BJ has real
eigenvalues, then the SOR method converges for any choice of x(0) iÔ¨Ä
œÅ(BJ) < 1 and 0 < œâ < 2. Moreover,
œâopt =
2
1 +

1 ‚àíœÅ(BJ)2
(4.19)

132
4. Iterative Methods for Solving Linear Systems
and the corresponding asymptotic convergence factor is
œÅ(B(œâopt)) = 1 ‚àí

1 ‚àíœÅ(BJ)2
1 +

1 ‚àíœÅ(BJ)2 .
4.2.4
A priori Forward Analysis
In the previous analysis we have neglected the rounding errors. However, as
shown in the following example (taken from [HW76]), they can dramatically
aÔ¨Äect the convergence rate of the iterative method.
Example 4.3 Let A be a lower bidiagonal matrix of order 100 with entries
aii = 1.5 and ai,i‚àí1 = 1, and let b ‚ààR100 be the right-side with bi = 2.5. The
exact solution of the system Ax = b has components xi = 1 ‚àí(‚àí2/3)i. The
SOR method with œâ = 1.5 should be convergent, working in exact arithmetic,
since œÅ(B(1.5)) = 0.5 (far below one). However, running Program 16 with x(0) =
Ô¨Ç(x)+œµM, which is extremely close to the exact value, the sequence x(k) diverges
and after 100 iterations the algorithm yields a solution with ‚à•x(100)‚à•‚àû= 1013.
The Ô¨Çaw is due to rounding error propagation and must not be ascribed to a
possible ill-conditioning of the matrix since K‚àû(A) ‚âÉ5.
‚Ä¢
To account for rounding errors, let us denote by x(k) the solution (in Ô¨Ånite
arithmetic) generated by an iterative method of the form (4.6) after k steps.
Due to rounding errors, x(k) can be regarded as the exact solution to the
problem
Px(k+1) = Nx(k) + b ‚àíŒ∂k,
(4.20)
with
Œ∂k = Œ¥Pk+1x(k+1) ‚àígk.
The matrix Œ¥Pk+1 accounts for the rounding errors in the solution of (4.6),
while the vector gk includes the errors made in the evaluation of Nx(k) +b.
From (4.20), we obtain
x(k+1) = Bk+1x(0) +
k

j=0
BjP‚àí1(b ‚àíŒ∂k‚àíj)
and for the absolute error e(k+1) = x ‚àíx(k+1)
e(k+1) = Bk+1e(0) +
k

j=0
BjP‚àí1Œ∂k‚àíj.
The Ô¨Årst term represents the error that is made by the iterative method
in exact arithmetic; if the method is convergent, this error is negligible for
suÔ¨Éciently large values of k. The second term refers instead to rounding
error propagation; its analysis is quite technical and is carried out, for
instance, in [Hig88] in the case of Jacobi, Gauss-Seidel and SOR methods.

4.2 Linear Iterative Methods
133
4.2.5
Block Matrices
The methods of the previous sections are also referred to as point (or line)
iterative methods, since they act on single entries of matrix A. It is possible
to devise block versions of the algorithms, provided that D denotes the block
diagonal matrix whose entries are the m √ó m diagonal blocks of matrix A
(see Section 1.6).
The block Jacobi method is obtained taking again P=D and N=D-A. The
method is well-deÔ¨Åned only if the diagonal blocks of D are nonsingular. If
A is decomposed in p √ó p square blocks, the block Jacobi method is
Aiix(k+1)
i
= bi ‚àí
p

j=1
jÃ∏=i
Aijx(k)
j ,
i = 1, . . . , p,
having also decomposed the solution vector and the right side in blocks of
size p, denoted by xi and bi, respectively. As a result, at each step, the block
Jacobi method requires solving p linear systems of matrices Aii. Theorem
4.3 is still valid, provided that D is substituted by the corresponding block
diagonal matrix.
In a similar manner, the block Gauss-Seidel and block SOR methods can
be introduced.
4.2.6
Symmetric Form of the Gauss-Seidel and SOR Methods
Even if A is a symmetric matrix, the Gauss-Seidel and SOR methods gen-
erate iteration matrices that are not necessarily symmetric. For that, we
introduce in this section a technique that allows for symmetrizing these
schemes. The Ô¨Ånal aim is to provide an approach for generating symmetric
preconditioners (see Section 4.3.2).
Firstly, let us remark that an analogue of the Gauss-Seidel method can
be constructed, by simply exchanging E with F. The following iteration
can thus be deÔ¨Åned, called the backward Gauss-Seidel method
(D ‚àíF)x(k+1) = Ex(k) + b
with iteration matrix given by BGSb = (D ‚àíF)‚àí1E.
The symmetric Gauss-Seidel method is obtained by combining an itera-
tion of Gauss-Seidel method with an iteration of backward Gauss-Seidel
method. Precisely, the k-th iteration of the symmetric Gauss-Seidel method
is
(D ‚àíE)x(k+1/2) = Fx(k) + b,
(D ‚àíF)x(k+1) = Ex(k+1/2) + b.

134
4. Iterative Methods for Solving Linear Systems
Eliminating x(k+1/2), the following scheme is obtained
x(k+1) = BSGSx(k) + bSGS,
BSGS = (D ‚àíF)‚àí1E(D ‚àíE)‚àí1F,
bSGS = (D ‚àíF)‚àí1[E(D ‚àíE)‚àí1 + I]b.
(4.21)
The preconditioning matrix associated with (4.21) is
PSGS = (D ‚àíE)D‚àí1(D ‚àíF).
The following result can be proved (see [Hac94]).
Property 4.5 If A is a symmetric positive deÔ¨Ånite matrix, the symmet-
ric Gauss-Seidel method is convergent, and, moreover, BSGS is symmetric
positive deÔ¨Ånite.
In a similar manner, deÔ¨Åning the backward SOR method
(D ‚àíœâF)x(k+1) = [œâE + (1 ‚àíœâ)D] x(k) + œâb,
and combining it with a step of SOR method, the following symmetric SOR
method or SSOR, is obtained
x(k+1) = Bs(œâ)x(k) + bœâ
where
Bs(œâ) = (D ‚àíœâF)‚àí1(œâE + (1 ‚àíœâ)D)(D ‚àíœâE)‚àí1(œâF + (1 ‚àíœâ)D),
bœâ = œâ(2 ‚àíœâ)(D ‚àíœâF)‚àí1D(D ‚àíœâE)‚àí1b.
The preconditioning matrix of this scheme is
PSSOR(œâ) =
 1
œâ D ‚àíE

œâ
2 ‚àíœâ D‚àí1
 1
œâ D ‚àíF

.
(4.22)
If A is symmetric and positive deÔ¨Ånite, the SSOR method is convergent if
0 < œâ < 2 (see [Hac94] for the proof). Typically, the SSOR method with an
optimal choice of the relaxation parameter converges more slowly than the
corresponding SOR method. However, the value of œÅ(Bs(œâ)) is less sensitive
to a choice of œâ around the optimal value (in this respect, see the behavior
of the spectral radii of the two iteration matrices in Figure 4.1). For this
reason, the optimal value of œâ that is chosen in the case of SSOR method
is usually the same used for the SOR method (for further details, we refer
to [You71]).

4.2 Linear Iterative Methods
135
0
0.5
1
1.5
2
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
œâ
œÅ
SSOR
SOR
FIGURE 4.1. Spectral radius of the iteration matrix of SOR and SSOR methods,
as a function of the relaxation parameter œâ for the matrix tridiag10(‚àí1, 2, ‚àí1)
4.2.7
Implementation Issues
We provide the programs implementing the Jacobi and Gauss-Seidel meth-
ods in their point form and with relaxation.
In Program 15 the JOR method is implemented (the Jacobi method is
obtained as a special case setting omega = 1). The stopping test monitors
the Euclidean norm of the residual at each iteration, normalized to the
value of the initial residual.
Notice that each component x(i) of the solution vector can be computed
independently; this method can thus be easily parallelized.
Program 15 - JOR : JOR method
function [x, iter]= jor ( a, b, x0, nmax, toll, omega)
[n,n]=size(a);
iter = 0; r = b - a * x0; r0 = norm(r); err = norm (r); x = x0;
while err > toll & iter < nmax
iter = iter + 1;
for i=1:n
s = 0;
for j = 1:i-1,
s = s + a (i,j) * x (j);
end
for j = i+1:n,
s = s + a (i,j) * x (j);
end
x (i) = omega * ( b(i) - s) / a(i,i) + (1 - omega) * x(i);
end
r = b - a * x;
err = norm (r) / r0;
end
Program 16 implements the SOR method. Taking omega=1 yields the
Gauss-Seidel method.

136
4. Iterative Methods for Solving Linear Systems
Unlike the Jacobi method, this scheme is fully sequential. However, it can
be eÔ¨Éciently implemented without storing the solution of the previous step,
with a saving of memory storage.
Program 16 - SOR : SOR method
function [x, iter]= sor ( a, b, x0, nmax, toll, omega)
[n,n]=size(a);
iter = 0; r = b - a * x0; r0 = norm (r); err = norm (r); xold = x0;
while err > toll & iter < nmax
iter = iter + 1;
for i=1:n
s = 0;
for j = 1:i-1,
s = s + a (i,j) * x (j);
end
for j = i+1:n
s = s + a (i,j) * xold (j);
end
x (i) = omega * ( b(i) - s) / a(i,i) + (1 - omega) * xold (i);
end
x = x‚Äô; xold = x;
r = b - a * x;
err = norm (r) / r0;
end
4.3
Stationary and Nonstationary Iterative
Methods
Denote by
RP = I ‚àíP‚àí1A
the iteration matrix associated with (4.7). Proceeding as in the case of
relaxation methods, (4.7) can be generalized introducing a relaxation (or
acceleration) parameter Œ±. This leads to the following stationary Richard-
son method
x(k+1) = x(k) + Œ±P‚àí1r(k),
k ‚â•0.
(4.23)
More generally, allowing Œ± to depend on the iteration index, the nonsta-
tionary Richardson method or semi-iterative method given by
x(k+1) = x(k) + Œ±kP‚àí1r(k),
k ‚â•0.
(4.24)
The iteration matrix at the k-th step for these methods (depending on k)
is
R(Œ±k) = I ‚àíŒ±kP‚àí1A,

4.3 Stationary and Nonstationary Iterative Methods
137
with Œ±k = Œ± in the stationary case. If P=I, the methods will be called
nonpreconditioned. The Jacobi and Gauss-Seidel methods can be regarded
as stationary Richardson methods with Œ± = 1, P = D and P = D ‚àíE,
respectively.
We can rewrite (4.24) (and, thus, also (4.23)) in a form of greater inter-
est for computation. Letting z(k) = P‚àí1r(k) (the so-called preconditioned
residual), we get x(k+1) = x(k) + Œ±kz(k) and r(k+1) = b ‚àíAx(k+1) =
r(k)‚àíŒ±kAz(k). To summarize, a nonstationary Richardson method requires
at each k + 1-th step the following operations:
solve the linear system Pz(k) = r(k);
compute the acceleration parameter Œ±k;
update the solution x(k+1) = x(k) + Œ±kz(k);
update the residual r(k+1) = r(k) ‚àíŒ±kAz(k).
(4.25)
4.3.1
Convergence Analysis of the Richardson Method
Let us Ô¨Årst consider the stationary Richardson methods for which Œ±k = Œ±
for k ‚â•0. The following convergence result holds.
Theorem 4.8 For any nonsingular matrix P, the stationary Richardson
method (4.23) is convergent iÔ¨Ä
2ReŒªi
Œ±|Œªi|2 > 1
‚àÄi = 1, . . . , n,
(4.26)
where Œªi ‚ààC are the eigenvalues of P‚àí1A.
Proof. Let us apply Theorem 4.1 to the iteration matrix RŒ± = I ‚àíŒ±P‚àí1A. The
condition |1 ‚àíŒ±Œªi| < 1 for i = 1, . . . , n yields the inequality
(1 ‚àíŒ±ReŒªi)2 + Œ±2(ImŒªi)2 < 1
from which (4.26) immediately follows.
3
Let us notice that, if the sign of the real parts of the eigenvalues of P‚àí1A
is not constant, the stationary Richardson method cannot converge.
More speciÔ¨Åc results can be obtained provided that suitable assumptions
are made on the spectrum of P‚àí1A.
Theorem 4.9 Assume that P is a nonsingular matrix and that P‚àí1A has
positive real eigenvalues, ordered in such a way that Œª1 ‚â•Œª2 ‚â•. . . ‚â•
Œªn > 0. Then, the stationary Richardson method (4.23) is convergent iÔ¨Ä
0 < Œ± < 2/Œª1. Moreover, letting
Œ±opt =
2
Œª1 + Œªn
(4.27)

138
4. Iterative Methods for Solving Linear Systems
the spectral radius of the iteration matrix RŒ± is minimum if Œ± = Œ±opt, with
œÅopt = min
Œ± [œÅ(RŒ±)] = Œª1 ‚àíŒªn
Œª1 + Œªn
.
(4.28)
Proof. The eigenvalues of RŒ± are given by Œªi(RŒ±) = 1 ‚àíŒ±Œªi, so that (4.23) is
convergent iÔ¨Ä|Œªi(RŒ±)| < 1 for i = 1, . . . , n, that is, if 0 < Œ± < 2/Œª1. It follows
(see Figure 4.2) that œÅ(RŒ±) is minimum when 1 ‚àíŒ±Œªn = Œ±Œª1 ‚àí1, that is, for
Œ± = 2/(Œª1 + Œªn), which furnishes the desired value for Œ±opt. By substitution, the
desired value of œÅopt is obtained.
3
1
Œªn
1
Œª1
Œ±opt
2
Œª1
œÅ = 1
|1 ‚àíŒ±Œª1|
|1 ‚àíŒ±Œªn|
œÅopt
|1 ‚àíŒ±Œªk|
Œ±
FIGURE 4.2. Spectral radius of RŒ± as a function of the eigenvalues of P‚àí1A
If P‚àí1A is symmetric positive deÔ¨Ånite, it can be shown that the convergence
of the Richardson method is monotone with respect to either ‚à•¬∑‚à•2 and ‚à•¬∑‚à•A.
In such a case, using (4.28), we can also relate œÅopt to K2(P‚àí1A) as follows
œÅopt = K2(P‚àí1A) ‚àí1
K2(P‚àí1A) + 1,
Œ±opt =
2‚à•A‚àí1P‚à•2
K2(P‚àí1A) + 1.
(4.29)
The choice of a suitable preconditioner P is, therefore, of paramount im-
portance for improving the convergence of a Richardson method. Of course,
such a choice should also account for the need of keeping the computational
eÔ¨Äort as low as possible. In Section 4.3.2, some preconditioners of common
use in practice will be described.
Corollary 4.1 Let A be a symmetric positive deÔ¨Ånite matrix. Then, the
non preconditioned stationary Richardson method is convergent and
‚à•e(k+1)‚à•A ‚â§œÅ(RŒ±)‚à•e(k)‚à•A,
k ‚â•0.
(4.30)

4.3 Stationary and Nonstationary Iterative Methods
139
The same result holds for the preconditioned Richardson method, provided
that the matrices P, A and P‚àí1A are symmetric positive deÔ¨Ånite.
Proof. The convergence is a consequence of Theorem 4.8. Moreover, we notice
that
‚à•e(k+1)‚à•A = ‚à•RŒ±e(k)‚à•A = ‚à•A1/2RŒ±e(k)‚à•2 ‚â§‚à•A1/2RŒ±A‚àí1/2‚à•2‚à•A1/2e(k)‚à•2.
The matrix RŒ± is symmetric positive deÔ¨Ånite and is similar to A1/2RŒ±A‚àí1/2.
Therefore,
‚à•A1/2RŒ±A‚àí1/2‚à•2 = œÅ(RŒ±).
The result (4.30) follows by noting that ‚à•A1/2e(k)‚à•2 = ‚à•e(k)‚à•A. A similar proof
can be carried out in the preconditioned case, provided we replace A with P‚àí1A.
3
Finally, the inequality (4.30) holds even if only P and A are symmetric
positive deÔ¨Ånite (for the proof, see [QV94], Chapter 2).
4.3.2
Preconditioning Matrices
All the methods introduced in the previous sections can be cast in the form
(4.2), so that they can be regarded as being methods for solving the system
(I ‚àíB)x = f = P‚àí1b.
On the other hand, since B=P‚àí1N, system (3.2) can be equivalently refor-
mulated as
P‚àí1Ax = P‚àí1b.
(4.31)
The latter is the preconditioned system, being P the preconditioning matrix
or left preconditioner. Right and centered preconditioners can be introduced
as well, if system (3.2) is transformed, respectively, as
AP‚àí1y = b,
y = Px,
or
P‚àí1
L AP‚àí1
R y = P‚àí1
L b,
y = PRx.
There are point preconditioners or block preconditioners, depending on
whether they are applied to the single entries of A or to the blocks of
a partition of A. The iterative methods considered so far correspond to
Ô¨Åxed-point iterations on a left-preconditioned system. As stressed by (4.25),
computing the inverse of P is not mandatory; actually, the role of P is to
‚Äúpreconditioning‚Äù the residual r(k) through the solution of the additional
system Pz(k) = r(k).

140
4. Iterative Methods for Solving Linear Systems
Since the preconditioner acts on the spectral radius of the iteration ma-
trix, it would be useful to pick up, for a given linear system, an optimal
preconditioner, i.e., a preconditioner which is able to make the number of
iterations required for convergence independent of the size of the system.
Notice that the choice P=A is optimal but, trivially, ‚ÄúineÔ¨Écient‚Äù; some
alternatives of greater computational interest will be examined below.
There is a lack of general theoretical results that allow to devise optimal
preconditioners. However, an established ‚Äúrule of thumb‚Äù is that P is a
good preconditioner for A if P‚àí1A is near to being a normal matrix and if
its eigenvalues are clustered within a suÔ¨Éciently small region of the com-
plex Ô¨Åeld. The choice of a preconditioner must also be guided by practical
considerations, noticeably, its computational cost and its memory require-
ments.
Preconditioners can be divided into two main categories: algebraic and
functional preconditioners, the diÔ¨Äerence being that the algebraic precon-
ditioners are independent of the problem that originated the system to
be solved, and are actually constructed via algebraic procedure, while the
functional preconditioners take advantage of the knowledge of the problem
and are constructed as a function of it. In addition to the preconditioners
already introduced in Section 4.2.6, we give a description of other algebraic
preconditioners of common use.
1. Diagonal preconditioners: choosing P as the diagonal of A is generally
eÔ¨Äective if A is symmetric positive deÔ¨Ånite. A usual choice in the non
symmetric case is to set
pii =
Ô£´
Ô£≠
n

j=1
a2
ij
Ô£∂
Ô£∏
1/2
.
Block diagonal preconditioners can be constructed in a similar man-
ner. We remark that devising an optimal diagonal preconditioner is
far from being trivial, as previously noticed in Section 3.12.1 when
dealing with the scaling of a matrix.
2. Incomplete LU factorization (shortly ILU) and Incomplete Cholesky
factorization (shortly IC).
An incomplete factorization of A is a process that computes P =
LinUin, where Lin is a lower triangular matrix and Uin is an upper
triangular matrix. These matrices are approximations of the exact
matrices L, U of the LU factorization of A and are chosen in such a
way that the residual matrix R = A‚àíLinUin satisÔ¨Åes some prescribed
requirements, such as having zero entries in speciÔ¨Åed locations.
For a given matrix M, the L-part (U-part) of M will mean henceforth
the lower (upper) triangular part of M. Moreover, we assume that the
factorization process can be carried out without resorting to pivoting.

4.3 Stationary and Nonstationary Iterative Methods
141
The basic approach to incomplete factorization, consists of requiring
the approximate factors Lin and Uin to have the same sparsity pat-
tern as the L-part and U-part of A, respectively. A general algorithm
for constructing an incomplete factorization is to perform Gauss elim-
ination as follows: at each step k, compute mik = a(k)
ik /a(k)
kk only if
aik Ã∏= 0 for i = k + 1, . . . , n. Then, compute for j = k + 1, . . . , n
a(k+1)
ij
only if aij Ã∏= 0. This algorithm is implemented in Program 17
where the matrices Lin and Uin are progressively overwritten onto
the L-part and U-part of A.
Program 17 - basicILU : Incomplete LU factorization
function [a] = basicILU(a)
[n,n]=size(a);
for k=1:n-1, for i=k+1:n,
if a(i,k) Àú= 0
a(i,k) = a(i,k) / a(k,k);
for j=k+1:n
if a(i,j) Àú= 0
a(i,j) = a(i,j) -a(i,k)*a(k,j);
end
end
end
end, end
We notice that having Lin and Uin with the same patterns as the
L and U-parts of A, respectively, does not necessarily imply that R
has the same sparsity pattern as A, but guarantees that rij = 0 if
aij Ã∏= 0, as is shown in Figure 4.3.
The resulting incomplete factorization is known as ILU(0), where ‚Äú0‚Äù
means that no Ô¨Åll-in has been introduced in the factorization process.
An alternative strategy might be to Ô¨Åx the structure of Lin and Uin
irrespectively of that of A, in such a way that some computational
criteria are satisÔ¨Åed (for example, that the incomplete factors have
the simplest possible structure).
The accuracy of the ILU(0) factorization can obviously be improved
by allowing some Ô¨Åll-in to arise, and thus, by accepting nonzero entries
in the factorization whereas A has elements equal to zero. To this
purpose, it is convenient to introduce a function, which we call Ô¨Åll-
in level, that is associated with each entry of A and that is being
modiÔ¨Åed during the factorization process. If the Ô¨Åll-in level of an

142
4. Iterative Methods for Solving Linear Systems
0
1
2
3
4
5
6
7
8
9
10
11
0
1
2
3
4
5
6
7
8
9
10
11
FIGURE 4.3. The sparsity pattern of the original matrix A is represented by the
squares, while the pattern of R = A‚àíLinUin, computed by Program 17, is drawn
by the bullets
element is greater than an admissible value p ‚ààN, the corresponding
entry in Uin or Lin is set equal to zero.
Let us explain how this procedure works, assuming that the matri-
ces Lin and Uin are progressively overwritten to A (as happens in
Program 4). The Ô¨Åll-in level of an entry a(k)
ij
is denoted by levij,
where the dependence on k is understood, and it should provide a
reasonable estimate of the size of the entry during the factorization
process. Actually, we are assuming that if levij = q then |aij| ‚âÉŒ¥q
with Œ¥ ‚àà(0, 1), so that q is greater when |a(k)
ij | is smaller.
At the starting step of the procedure, the level of the nonzero entries
of A and of the diagonal entries is set equal to 0, while the level of
the null entries is set equal to inÔ¨Ånity. For any row i = 2, . . . , n, the
following operations are performed: if levik ‚â§p, k = 1, . . . , i ‚àí1, the
entry mik of Lin and the entries a(k+1)
ij
of Uin, j = i + 1, . . . , n, are
updated. Moreover, if a(k+1)
ij
Ã∏= 0 the value levij is updated as being
the minimum between the available value of levij and levik+levkj +1.
The reason of this choice is that |a(k+1)
ij
| = |a(k)
ij ‚àímika(k)
kj | ‚âÉ|Œ¥levij ‚àí
Œ¥levik+levkj+1|, so that one can assume that the size of |a(k+1)
ij
| is the
maximum between Œ¥levij and Œ¥levik+levkj+1.
The above factorization process is called ILU(p) and turns out to be
extremely eÔ¨Écient (with p small) provided that it is coupled with a
suitable matrix reordering (see Section 3.9).
Program 18 implements the ILU(p) factorization; it returns in out-
put the approximate matrices Lin and Uin (overwritten to the input
matrix a), with the diagonal entries of Lin equal to 1, and the ma-

4.3 Stationary and Nonstationary Iterative Methods
143
trix lev containing the Ô¨Åll-in level of each entry at the end of the
factorization.
Program 18 - ilup : ILU(p) factorization
function [a,lev] = ilup (a,p)
[n,n]=size(a);
for i=1:n, for j=1:n
if (a(i,j) Àú= 0) | (i==j)
lev(i,j)=0;
else
lev(i,j)=Inf;
end
end, end
for i=2:n,
for k=1:i-1
if lev(i,k) <= p
a(i,k)=a(i,k)/a(k,k);
for j=k+1:n
a(i,j)=a(i,j)-a(i,k)*a(k,j);
if a(i,j) Àú= 0
lev(i,j)=min(lev(i,j),lev(i,k)+lev(k,j)+1);
end
end
end
end
for j=1:n, if lev(i,j) > p, a(i,j) = 0; end, end
end
Example 4.4 Consider the matrix A ‚ààR46√ó46 associated with the Ô¨Ånite
diÔ¨Äerence approximation of the Laplace operator ‚àÜ¬∑ =
‚àÇ2¬∑
‚àÇx2 +
‚àÇ2¬∑
‚àÇy2 (see
Section 12.6). This matrix can be generated with the following MATLAB
commands: G=numgrid(‚ÄôB‚Äô,10); A=delsq(G) and corresponds to the dis-
cretization of the diÔ¨Äerential operator on a domain having the shape of the
exterior of a butterÔ¨Çy and included in the square [‚àí1, 1]2 (see Section 12.6).
The number of nonzero entries of A is 174. Figure 4.4 shows the pattern of
matrix A (drawn by the bullets) and the entries in the pattern added by
the ILU(1) and ILU(2) factorizations due to Ô¨Åll-in (denoted by the squares
and the triangles, respectively). Notice that these entries are all contained
within the envelope of A since no pivoting has been performed.
‚Ä¢
The ILU(p) process can be carried out without knowing the actual
values of the entries of A, but only working on their Ô¨Åll-in levels.
Therefore, we can distinguish between a symbolic factorization (the
generation of the levels) and an actual factorization (the computation
of the entries of ILU(p) starting from the informations contained in

144
4. Iterative Methods for Solving Linear Systems
0
5
10
15
20
25
30
35
40
45
0
5
10
15
20
25
30
35
40
45
FIGURE 4.4. Pattern of the matrix A in Example 4.4 (bullets); entries added by
the ILU(1) and ILU(2) factorizations (squares and triangles, respectively)
the level function). The scheme is thus particularly eÔ¨Äective when
several linear systems must be solved, with matrices having the same
structure but diÔ¨Äerent entries.
On the other hand, for certain classes of matrices, the Ô¨Åll-in level
does not always provide a sound indication of the actual size attained
by the entries. In such cases, it is better to monitor the size of the
entries of R by neglecting each time the entries that are too small.
For instance, one can drop out the entries a(k+1)
ij
such that
|a(k+1)
ij
| ‚â§c|a(k+1)
ii
a(k+1)
jj
|1/2,
i, j = 1, . . . , n,
with 0 < c < 1 (see [Axe94]).
In the strategies considered so far, the entries of the matrix that are
dropped out can no longer be recovered in the incomplete factoriza-
tion process. Some remedies exist for this drawback: for instance, at
the end of each k-th step of the factorization, one can sum, row by
row, the discarded entries to the diagonal entries of Uin. By doing
so, an incomplete factorization known as MILU (ModiÔ¨Åed ILU) is
obtained, which enjoys the property of being exact with respect to
the constant vectors, i.e., such that R1T = 0T (see [Axe94] for other
formulations). In the practice, this simple trick provides, for a wide
class of matrices, a better preconditioner than obtained with the ILU
method. In the case of symmetric positive deÔ¨Ånite matrices one can
resort to the ModiÔ¨Åed Incomplete Cholesky Factorization (MICh).
We conclude by mentioning the ILUT factorization, which collects the
features of ILU(p) and MILU. This factorization can also include par-
tial pivoting by columns with a slight increase of the computational

4.3 Stationary and Nonstationary Iterative Methods
145
cost. For an eÔ¨Écient implementation of incomplete factorizations, we
refer to the MATLAB function luinc in the toolbox sparfun.
The existence of the ILU factorization is not guaranteed for all non-
singular matrices (see for an example [Elm86]) and the process stops
if zero pivotal entries arise. Existence theorems can be proved if A is
an M-matrix [MdV77] or diagonally dominant [Man80]. It is worth
noting that sometimes the ILU factorization turns out to be more
stable than the complete LU factorization [GM83].
3. Polynomial preconditioners: the preconditioning matrix is deÔ¨Åned as
P‚àí1 = p(A),
where p is a polynomial in A, usually of low degree.
A remarkable example is given by Neumann polynomial precondi-
tioners. Letting A = D ‚àíC, we have A = (I ‚àíCD‚àí1)D, from which
A‚àí1 = D‚àí1(I ‚àíCD‚àí1)‚àí1 = D‚àí1(I + CD‚àí1 + (CD‚àí1)2 + . . . ).
A preconditioner can then be obtained by truncating the series above
at a certain power p. This method is actually eÔ¨Äective only if œÅ(CD‚àí1)
< 1, which is the necessary condition in order the series to be con-
vergent.
4. Least-squares preconditioners: A‚àí1 is approximated by a least-squares
polynomial ps(A) (see Section 3.13). Since the aim is to make ma-
trix I ‚àíP‚àí1A as close as possible to the null matrix, the least-
squares approximant ps(A) is chosen in such a way that the function
œï(x) = 1‚àíps(x)x is minimized. This preconditioning technique works
eÔ¨Äectively only if A is symmetric and positive deÔ¨Ånite.
For further results on preconditioners, see [dV89] and [Axe94].
Example 4.5 Consider the matrix A‚ààR324√ó324 associated with the Ô¨Ånite diÔ¨Äer-
ence approximation of the Laplace operator on the square [‚àí1, 1]2. This matrix
can be generated with the following MATLAB commands: G=numgrid(‚ÄôN‚Äô,20);
A=delsq(G). The condition number of the matrix is K2(A) = 211.3. In Table
4.1 we show the values of K2(P‚àí1A) computed using the ILU(p) and Neumann
preconditioners, with p = 0, 1, 2, 3. In the last case D is the diagonal part of A. ‚Ä¢
Remark 4.2 Let A and P be real symmetric matrices of order n, with P
positive deÔ¨Ånite. The eigenvalues of the preconditioned matrix P‚àí1A are
solutions of the algebraic equation
Ax = ŒªPx,
(4.32)

146
4. Iterative Methods for Solving Linear Systems
p
ILU(p)
Neumann
0
22.3
211.3
1
12
36.91
2
8.6
48.55
3
5.6
18.7
TABLE 4.1. Spectral condition numbers of the preconditioned matrix A of Ex-
ample 4.5 as a function of p
where x is an eigenvector associated with the eigenvalue Œª. Equation (4.32)
is an example of generalized eigenvalue problem (see Section 5.9 for a thor-
ough discussion) and the eigenvalue Œª can be computed through the fol-
lowing generalized Rayleigh quotient
Œª = (Ax, x)
(Px, x) .
Applying the Courant-Fisher Theorem (see Section 5.11) yields
Œªmin(A)
Œªmax(P) ‚â§Œª ‚â§Œªmax(A)
Œªmin(P) .
(4.33)
Relation (4.33) provides a lower and upper bound for the eigenvalues of the
preconditioned matrix as a function of the extremal eigenvalues of A and
P, and therefore it can be proÔ¨Åtably used to estimate the condition number
of P‚àí1A.
‚ñ†
4.3.3
The Gradient Method
The expression of the optimal parameter that has been provided in Theo-
rem 4.9 is of limited usefulness in practical computations, since it requires
the knowledge of the extremal eigenvalues of the matrix P‚àí1A. In the spe-
cial case of symmetric and positive deÔ¨Ånite matrices, however, the optimal
acceleration parameter can be dynamically computed at each step k as
follows.
We Ô¨Årst notice that, for such matrices, solving system (3.2) is equivalent
to Ô¨Ånding the minimizer x ‚ààRn of the quadratic form
Œ¶(y) = 1
2yT Ay ‚àíyT b,
which is called the energy of system (3.2). Indeed, the gradient of Œ¶ is given
by
‚àáŒ¶(y) = 1
2(AT + A)y ‚àíb = Ay ‚àíb.
(4.34)
As a consequence, if ‚àáŒ¶(x) = 0 then x is a solution of the original system.
Conversely, if x is a solution, then
Œ¶(y) = Œ¶(x + (y ‚àíx)) = Œ¶(x) + 1
2(y ‚àíx)T A(y ‚àíx),
‚àÄy ‚ààRn

4.3 Stationary and Nonstationary Iterative Methods
147
and thus, Œ¶(y) > Œ¶(x) if y Ã∏= x, i.e. x is a minimizer of the functional Œ¶.
Notice that the previous relation is equivalent to
1
2‚à•y ‚àíx‚à•2
A = Œ¶(y) ‚àíŒ¶(x)
(4.35)
where ‚à•¬∑ ‚à•A is the A-norm or energy norm, deÔ¨Åned in (1.28).
The problem is thus to determine the minimizer x of Œ¶ starting from a
point x(0) ‚ààRn and, consequently, to select suitable directions along which
moving to get as close as possible to the solution x. The optimal direction,
that joins the starting point x(0) to the solution point x, is obviously un-
known a priori. Therefore, we must take a step from x(0) along another
direction d(0), and then Ô¨Åx along this latter a new point x(1) from which
to iterate the process until convergence.
Thus, at the generic step k, x(k+1) is computed as
x(k+1) = x(k) + Œ±kd(k),
(4.36)
where Œ±k is the value which Ô¨Åxes the length of the step along d(k). The most
natural idea is to take the descent direction of maximum slope ‚àáŒ¶(x(k)),
which yields the gradient method or steepest descent method.
On the other hand, due to (4.34), ‚àáŒ¶(x(k)) = Ax(k) ‚àíb = ‚àír(k), so that
the direction of the gradient of Œ¶ coincides with that of residual and can
be immediately computed using the current iterate. This shows that the
gradient method, as well as the Richardson method, moves at each step k
along the direction d(k) = r(k).
To compute the parameter Œ±k let us write explicitly Œ¶(x(k+1)) as a func-
tion of a parameter Œ±
Œ¶(x(k+1)) = 1
2(x(k) + Œ±r(k))T A(x(k) + Œ±r(k)) ‚àí(x(k) + Œ±r(k))T b.
DiÔ¨Äerentiating with respect to Œ± and setting it equal to zero, yields the
desired value of Œ±k
Œ±k = r(k)T r(k)
r(k)T Ar(k)
(4.37)
which depends only on the residual at the k-th step. For this reason, the
nonstationary Richardson method employing (4.37) to evaluate the acceler-
ation parameter, is also called the gradient method with dynamic parameter
(shortly, gradient method), to distinguish it from the stationary Richardson
method (4.23) or gradient method with constant parameter, where Œ±k = Œ±
is a constant for any k ‚â•0.
Summarizing, the gradient method can be described as follows:

148
4. Iterative Methods for Solving Linear Systems
given x(0) ‚ààRn, for k = 0, 1, . . . until convergence, compute
r(k) = b ‚àíAx(k)
Œ±k = r(k)T r(k)
r(k)T Ar(k)
x(k+1) = x(k) + Œ±kr(k).
Theorem 4.10 Let A be a symmetric and positive deÔ¨Ånite matrix; then
the gradient method is convergent for any choice of the initial datum x(0)
and
‚à•e(k+1)‚à•A ‚â§K2(A) ‚àí1
K2(A) + 1‚à•e(k)‚à•A,
k = 0, 1, . . . ,
(4.38)
where ‚à•¬∑ ‚à•A is the energy norm deÔ¨Åned in (1.28).
Proof. Let x(k) be the solution generated by the gradient method at the k-th
step. Then, let x(k+1)
R
be the vector generated by taking one step of the non
preconditioned Richardson method with optimal parameter starting from x(k),
i.e., x(k+1)
R
= x(k) + Œ±optr(k).
Due to Corollary 4.1 and (4.28), we have
‚à•e(k+1)
R
‚à•A ‚â§K2(A) ‚àí1
K2(A) + 1‚à•e(k)‚à•A,
where e(k+1)
R
= x(k+1)
R
‚àíx. Moreover, from (4.35) we have that the vector x(k+1),
generated by the gradient method, is the one that minimizes the A-norm of
the error among all vectors of the form x(k) + Œ∏r(k), with Œ∏ ‚ààR. Therefore,
‚à•e(k+1)‚à•A ‚â§‚à•e(k+1)
R
‚à•A which is the desired result.
3
We notice that the line through x(k) and x(k+1) is tangent at the point
x(k+1) to the ellipsoidal level surface

x ‚ààRn : Œ¶(x) = Œ¶(x(k+1))

(see
also Figure 4.5).
Relation (4.38) shows that convergence of the gradient method can be
quite slow if K2(A) = Œª1/Œªn is large. A simple geometric interpretation of
this result can be given in the case n = 2. Suppose that A=diag(Œª1, Œª2),
with 0 < Œª2 ‚â§Œª1 and b = (b1, b2)T .
In such a case, the curves corresponding to Œ¶(x1, x2) = c, as c varies
in R+, form a sequence of concentric ellipses whose semi-axes have length
inversely proportional to the values Œª1 and Œª2. If Œª1 = Œª2, the ellipses
degenerate into circles and the direction of the gradient crosses the center
directly, in such a way that the gradient method converges in one iteration.
Conversely, if Œª1 ‚â´Œª2, the ellipses become strongly eccentric and the
method converges quite slowly, as shown in Figure 4.5, moving along a
‚Äúzig-zag‚Äù trajectory.

4.3 Stationary and Nonstationary Iterative Methods
149
‚àí2
0
2
‚àí2
‚àí1
0
1
2
x
(0)
x
(1)
‚àí1
‚àí0.5
0
0.5
1
‚àí0.5
0
0.5
1
x
(2)
x
(3)
FIGURE 4.5. The Ô¨Årst iterates of the gradient method on the level curves of Œ¶
Program 19 provides an implementation of the gradient method with dy-
namic parameter. Here and in the programs reported in the remainder of
the section, the input parameters A, x, b, M, maxit and tol respectively
represent the coeÔ¨Écient matrix of the linear system, the initial datum x(0),
the right side, a possible preconditioner, the maximum number of admis-
sible iterations and a tolerance for the stopping test. This stopping test
checks if the ratio ‚à•r(k)‚à•2/‚à•b‚à•2 is less than tol. The output parameters of
the code are the the number of iterations niter required to fulÔ¨Åll the stop-
ping test, the vector x with the solution computed after niter iterations
and the normalized residual error = ‚à•r(niter)‚à•2/‚à•b‚à•2. A null value of the
parameter flag warns the user that the algorithm has actually satisÔ¨Åed
the stopping test and it has not terminated due to reaching the maximum
admissible number of iterations.
Program 19 - gradient : Gradient method with dynamic parameter
function [x, error, niter, Ô¨Çag] = gradient(A, x, b, M, maxit, tol)
Ô¨Çag = 0;
niter = 0;
bnrm2 = norm( b );
if ( bnrm2 == 0.0 ), bnrm2 = 1.0; end
r = b - A*x; error = norm( r ) / bnrm2;
if ( error < tol ) return, end
for niter = 1:maxit
z = M \ r; rho = (r‚Äô*z);
q = A*z;
alpha = rho / (z‚Äô*q );
x = x + alpha * z;
r = r - alpha*q;
error = norm( r ) / bnrm2;
if ( error <= tol ), break, end
end
if ( error > tol ) Ô¨Çag = 1; end

150
4. Iterative Methods for Solving Linear Systems
Example 4.6 Let us solve with the gradient method the linear system with ma-
trix Am ‚ààRm√óm generated with the MATLAB commands G=numgrid(‚ÄôS‚Äô,n);
A=delsq(G) where m = (n ‚àí2)2. This matrix is associated with the discretiza-
tion of the diÔ¨Äerential Laplace operator on the domain [‚àí1, 1]2. The right-hand
side bm is selected in such a way that the exact solution is the vector 1T ‚ààRm.
The matrix Am is symmetric and positive deÔ¨Ånite for any m and becomes ill-
conditioned for large values of m. We run Program 19 in the cases m = 16 and
m = 400, with x(0) = 0T , tol=10‚àí10 and maxit=200. If m = 400, the method
fails to satisfy the stopping test within the admissible maximum number of it-
erations and exhibits an extremely slow reduction of the residual (see Figure
4.6). Actually, K2(A400) ‚âÉ258. If, however, we precondition the system with the
matrix P = RT
inRin, where Rin is the lower triangular matrix in the Cholesky
incomplete factorization of A, the algorithm fulÔ¨Ålls the convergence within the
maximum admissible number of iterations (indeed, now K2(P‚àí1A400) ‚âÉ38).
‚Ä¢
0
50
100
150
200
250
10
‚àí14
10
‚àí12
10
‚àí10
10
‚àí8
10
‚àí6
10
‚àí4
10
‚àí2
10
0
(a)
(b)
(c)
(d)
FIGURE 4.6. The residual normalized to the starting one, as a function of the
number of iterations, for the gradient method applied to the systems in Example
4.6. The curves labelled (a) and (b) refer to the case m = 16 with the non precon-
ditioned and preconditioned method, respectively, while the curves labelled (c)
and (d) refer to the case m = 400 with the non preconditioned and preconditioned
method, respectively
4.3.4
The Conjugate Gradient Method
The gradient method consists essentially of two phases: choosing a descent
direction (the one of the residual) and picking up a point of local minimum
for Œ¶ along that direction. The second phase is independent of the Ô¨Årst one
since, for a given direction p(k), we can determine Œ±k as being the value
of the parameter Œ± such that Œ¶(x(k) + Œ±p(k)) is minimized. DiÔ¨Äerentiating
with respect to Œ± and setting to zero the derivative at the minimizer, yields
Œ±k =
p(k)T r(k)
p(k)T Ap(k) ,
(4.39)

4.3 Stationary and Nonstationary Iterative Methods
151
instead of (4.37). The question is how to determine p(k). A diÔ¨Äerent ap-
proach than the one which led to identify p(k) with r(k) is suggested by the
following deÔ¨Ånition.
DeÔ¨Ånition 4.4 A direction x(k) is said to be optimal with respect to a
direction p Ã∏= 0 if
Œ¶(x(k)) ‚â§Œ¶(x(k) + Œªp),
‚àÄŒª ‚ààR.
(4.40)
If x(k) is optimal with respect to any direction in a vector space V, we say
that x(k) is optimal with respect to V.
‚ñ†
From the deÔ¨Ånition of optimality, it turns out that p must be orthogonal
to the residual r(k). Indeed, from (4.40) we conclude that Œ¶ admits a local
minimum along p for Œª = 0, and thus the partial derivative of Œ¶ with
respect to Œª must vanish at Œª = 0. Since
‚àÇŒ¶
‚àÇŒª (x(k) + Œªp) = pT (Ax(k) ‚àíb) + ŒªpT Ap,
we therefore have
‚àÇŒ¶
‚àÇŒª (x(k))|Œª=0 = 0
iÔ¨Ä
pT (r(k)) = 0,
that is, p ‚ä•r(k). Notice that the iterate x(k+1) of the gradient method
is optimal with respect to r(k) since, due to the choice of Œ±k, we have
r(k+1) ‚ä•r(k), but this property no longer holds for the successive iterate
x(k+2) (see Exercise 12). It is then natural to ask whether there exist descent
directions that maintain the optimality of iterates. Let
x(k+1) = x(k) + q,
and assume that x(k) is optimal with respect to a direction p (thus, r(k) ‚ä•
p). Let us impose that x(k+1) is still optimal with respect to p, that is,
r(k+1) ‚ä•p. We obtain
0 = pT r(k+1) = pT (r(k) ‚àíAq) = ‚àípT Aq.
The conclusion is that, in order to preserve optimality between succes-
sive iterates, the descent directions must be mutually A-orthogonal or A-
conjugate, i.e.
pT Aq = 0.
A method employing A-conjugate descent directions is called conjugate.
The next step is how to generate automatically a sequence of conjugate

152
4. Iterative Methods for Solving Linear Systems
directions. This can be done as follows. Let p(0) = r(0) and search for the
directions of the form
p(k+1) = r(k+1) ‚àíŒ≤kp(k),
k = 0, 1, . . .
(4.41)
where Œ≤k ‚ààR must be determined in such a way that
(Ap(j))T p(k+1) = 0,
j = 0, 1, . . . , k.
(4.42)
Requiring that (4.42) is satisÔ¨Åed for j = k, we get from (4.41)
Œ≤k = (Ap(k))T r(k+1)
(Ap(k))T p(k) ,
k = 0, 1, . . .
We must now verify that (4.42) holds also for j = 0, 1, . . . , k‚àí1. To do this,
let us proceed by induction on k. Due to the choice of Œ≤0, relation (4.42)
holds for k = 0; let us thus assume that the directions p(0), . . . , p(k‚àí1) are
mutually A-orthogonal and, without losing generality, that
(p(j))T r(k) = 0,
j = 0, 1, . . . , k ‚àí1,
k ‚â•1.
(4.43)
Then, from (4.41) it follows that
(Ap(j))T p(k+1) = (Ap(j))T r(k+1),
j = 0, 1, . . . , k ‚àí1.
Moreover, due to (4.43) and by the assumption of of A-orthogonality we
get
(p(j))T r(k+1) = (p(j))T r(k) ‚àíŒ±k(p(j))T Ap(k) = 0,
j = 0, . . . , k ‚àí1(4.44)
i.e., we conclude that r(k+1) is orthogonal to every vector of the space Vk =
span(p(0), . . . , p(k‚àí1)). Since p(0) = r(0), from (4.41) it follows that Vk is
also equal to span(r(0), . . . , r(k‚àí1)). Then, (4.41) implies that Ap(j) ‚ààVj+1
and thus, due to (4.44)
(Ap(j))T r(k+1) = 0,
j = 0, 1, . . . , k ‚àí1.
As a consequence, (4.42) holds for j = 0, . . . , k.
The conjugate gradient method (CG) is the method obtained by choosing
the descent directions p(k) given by (4.41) and the acceleration parameter
Œ±k as in (4.39). As a consequence, setting r(0) = b ‚àíAx(0) and p(0) = r(0),
the k-th iteration of the conjugate gradient method takes the following

4.3 Stationary and Nonstationary Iterative Methods
153
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
‚àí0.2
0
0.2
0.4
0.6
0.8
1
1.2
1.4
CG
G
FIGURE 4.7. Descent directions for the conjugate gradient method (denoted by
CG, dashed line) and the gradient method (denoted by G, solid line). Notice that
the CG method reaches the solution after two iterations
form
Œ±k =
p(k)T r(k)
p(k)T Ap(k)
x(k+1) = x(k) + Œ±kp(k)
r(k+1) = r(k) ‚àíŒ±kAp(k)
Œ≤k = (Ap(k))T r(k+1)
(Ap(k))T p(k)
p(k+1) = r(k+1) ‚àíŒ≤kp(k).
It can also be shown (see Exercise 13) that the two parameters Œ±k and Œ≤k
may be alternatively expressed as
Œ±k =
‚à•r(k)‚à•2
2
p(k)T Ap(k) ,
Œ≤k = ‚à•r(k+1)‚à•2
2
‚à•r(k)‚à•2
2
.
(4.45)
We Ô¨Ånally notice that, eliminating the descent directions from r(k+1) =
r(k) ‚àíŒ±kAp(k), the following recursive three-terms relation is obtained for
the residuals (see Exercise 14)
Ar(k) = ‚àí1
Œ±k
r(k+1) +
 1
Œ±k
‚àíŒ≤k‚àí1
Œ±k‚àí1

r(k) +
Œ≤k
Œ±k‚àí1
r(k‚àí1).
(4.46)
As for the convergence of the CG method, we have the following results.
Theorem 4.11 Let A be a symmetric and positive deÔ¨Ånite matrix. Any
method which employs conjugate directions to solve (3.2) terminates after
at most n steps, yielding the exact solution.

154
4. Iterative Methods for Solving Linear Systems
Proof. The directions p(0), p(1), . . . , p(n‚àí1) form an A-orthogonal basis in Rn.
Moreover, since x(k) is optimal with respect to all the directions p(j), j =
0, . . . , k‚àí1, it follows that r(k) is orthogonal to the space Sk‚àí1 = span(p(0), p(1),
. . . , p(k‚àí1)). As a consequence, r(n) ‚ä•Sn‚àí1 = Rn and thus r(n) = 0 which
implies x(n) = x.
3
Theorem 4.12 Let A be a symmetric and positive deÔ¨Ånite matrix and
let Œª1, Œªn be its maximum and minimum eigenvalues, respectively. The
conjugate gradient method for solving (3.2) converges after at most n steps.
Moreover, the error e(k) at the k-th iteration (with k < n) is orthogonal to
p(j), for j = 0, . . . , k ‚àí1 and
‚à•e(k)‚à•A ‚â§
2ck
1 + c2k ‚à•e(0)‚à•A,
with c =

K2(A) ‚àí1

K2(A) + 1
.
(4.47)
Proof. The convergence of the CG method in n steps is a consequence of The-
orem 4.11.
Let us prove the error estimate, assuming for simplicity that x(0) = 0. Notice
Ô¨Årst that, for Ô¨Åxed k
x(k+1) =
k

j=0
Œ≥jAjb,
for suitable Œ≥j ‚ààR. Moreover, by construction, x(k+1) is the vector which min-
imizes the A-norm of the error at step k + 1, among all vectors of the form
z = k
j=0 Œ¥jAjb = pk(A)b, where pk(Œæ) = k
j=0 Œ¥jŒæj is a polynomial of degree
k and pk(A) denotes the corresponding matrix polynomial. As a consequence
‚à•e(k+1)‚à•2
A ‚â§(x ‚àíz)T A(x ‚àíz) = xT qk+1(A)Aqk+1(A)x,
(4.48)
where qk+1(Œæ) = 1 ‚àípk(Œæ)Œæ ‚ààP0,1
k+1, being P0,1
k+1 = {q ‚ààPk+1 : q(0) = 1} and
qk+1(A) the associated matrix polynomial. From (4.48) we get
‚à•e(k+1)‚à•2
A =
min
qk+1‚ààP0,1
k+1
xT qk+1(A)Aqk+1(A)x.
(4.49)
Since A is symmetric positive deÔ¨Ånite, there exists an orthogonal matrix Q
such that A = QŒõQT with Œõ = diag(Œª1, . . . , Œªn). Noticing that qk+1(A) =
Qqk+1(Œõ)QT , we get from (4.49)
‚à•e(k+1)‚à•2
A
=
min
qk+1‚ààP0,1
k+1
xT Qqk+1(Œõ)QT QŒõQT Qqk+1(Œõ)QT x
=
min
qk+1‚ààP0,1
k+1
xT Qqk+1(Œõ)Œõqk+1(Œõ)QT x
=
min
qk+1‚ààP0,1
k+1
yT diag(qk+1(Œªi)Œªiqk+1(Œªi))y
=
min
qk+1‚ààP0,1
k+1
n

i=1
y2
i Œªi(qk+1(Œªi))2

4.3 Stationary and Nonstationary Iterative Methods
155
having set y = Qx. Thus, we can conclude that
‚à•e(k+1)‚à•2
A ‚â§
.
min
qk+1‚ààP0,1
k+1
max
Œªi‚ààœÉ(A)(qk+1(Œªi))2
/
n

i=1
y2
i Œªi.
Recalling that
n

i=1
y2
i Œªi = ‚à•e(0)‚à•2
A, we have
‚à•e(k+1)‚à•A
‚à•e(0)‚à•A
‚â§
min
qk+1‚ààP0,1
k+1
max
Œªi‚ààœÉ(A)|qk+1(Œªi)|.
Let us now recall the following property
Property 4.6 The problem of minimizing
max
Œªn‚â§z‚â§Œª1|q(z)| over the space
P0,1
k+1([Œªn, Œª1]) admits a unique solution, given by the polynomial
pk+1(Œæ) = Tk+1
 Œª1 + Œªn ‚àí2Œæ
Œª1 ‚àíŒªn

/Ck+1,
Œæ ‚àà[Œªn, Œª1],
where Ck+1 = Tk+1( Œª1+Œªn
Œª1‚àíŒªn ) and Tk+1 is the Chebyshev polynomial of degree k+1
(see Section 10.10). The value of the minimum is 1/Ck+1.
Using this property we get
‚à•e(k+1)‚à•A
‚à•e(0)‚à•A
‚â§
1
Tk+1
 Œª1 + Œªn
Œª1 ‚àíŒªn

from which the thesis follows since in the case of a symmetric positive deÔ¨Ånite
matrix
1
Ck+1 =
2ck+1
1 + c2(k+1) .
3
The generic k-th iteration of the conjugate gradient method is well deÔ¨Åned
only if the descent direction p(k) is non null. Besides, if p(k) = 0, then the
iterate x(k) must necessarily coincide with the solution x of the system.
Moreover, irrespectively of the choice of the parameters Œ≤k, one can show
(see [Axe94], p. 463) that the sequence x(k) generated by the CG method
is such that either x(k) Ã∏= x, p(k) Ã∏= 0, Œ±k Ã∏= 0 for any k, or there must exist
an integer m such that x(m) = x, where x(k) Ã∏= x, p(k) Ã∏= 0 and Œ±k Ã∏= 0 for
k = 0, 1, . . . , m ‚àí1.
The particular choice made for Œ≤k in (4.45) ensures that m ‚â§n. In ab-
sence of rounding errors, the CG method can thus be regarded as being a
direct method, since it terminates after a Ô¨Ånite number of steps. However,
for matrices of large size, it is usually employed as an iterative scheme,

156
4. Iterative Methods for Solving Linear Systems
where the iterations are stopped when the error gets below a Ô¨Åxed toler-
ance. In this respect, the dependence of the error reduction factor on the
condition number of the matrix is more favorable than for the gradient
method. We also notice that estimate (4.47) is often overly pessimistic and
does not account for the fact that in this method, unlike what happens for
the gradient method, the convergence is inÔ¨Çuenced by the whole spectrum
of A, and not only by its extremal eigenvalues.
Remark 4.3 (EÔ¨Äect of rounding errors) The termination property of
the CG method is rigorously valid only in exact arithmetic. The cumulating
rounding errors prevent the descent directions from being A-conjugate and
can even generate null denominators in the computation of coeÔ¨Écients Œ±k
and Œ≤k. This latter phenomenon, known as breakdown, can be avoided by
introducing suitable stabilization procedures; in such an event, we speak
about stabilized gradient methods.
Despite the use of these strategies, it may happen that the CG method
fails to converge (in Ô¨Ånite arithmetic) after n iterations. In such a case,
the only reasonable possibility is to restart the iterative process, taking
as residual the last computed one. By so doing, the cyclic CG method or
CG method with restart is obtained, for which, however, the convergence
properties of the original CG method are no longer valid.
‚ñ†
4.3.5
The Preconditioned Conjugate Gradient Method
If P is a symmetric and positive deÔ¨Ånite preconditioning matrix, the pre-
conditioned conjugate gradient method (PCG) consists of applying the CG
method to the preconditioned system
P‚àí1/2AP‚àí1/2y = P‚àí1/2b,
with y = P1/2x.
In practice, the method is implemented without explicitly requiring the
computation of P1/2 or P‚àí1/2. After some algebra, the following scheme is
obtained:
given x(0) and setting r(0) = b ‚àíAx(0), z(0) = P‚àí1r(0) e p(0) = z(0), the
k-th iteration reads

4.3 Stationary and Nonstationary Iterative Methods
157
Œ±k =
p(k)T r(k)
p(k)T Ap(k)
x(k+1) = x(k) + Œ±kp(k)
r(k+1) = r(k) ‚àíŒ±kAp(k)
Pz(k+1) = r(k+1)
Œ≤k = (Ap(k))T z(k+1)
(Ap(k))T p(k)
p(k+1) = z(k+1) ‚àíŒ≤kp(k).
The computational cost is increased with respect to the CG method, as
one needs to solve at each step the linear system Pz(k+1) = r(k+1). For this
system the symmetric preconditioners examined in Section 4.3.2 can be
used. The error estimate is the same as for the nonpreconditioned method,
provided to replace the matrix A by P‚àí1A.
In Program 20 an implementation of the PCG method is reported. For
a description of the input/output parameters, see Program 19.
Program 20 - conjgrad : Preconditioned conjugate gradient method
function [x, error, niter, Ô¨Çag] = conjgrad(A, x, b, P, maxit, tol)
Ô¨Çag = 0; niter = 0; bnrm2 = norm( b );
if ( bnrm2 == 0.0 ), bnrm2 = 1.0; end
r = b - A*x; error = norm( r ) / bnrm2;
if ( error < tol ) return, end
for niter = 1:maxit
z = P \ r; rho = (r‚Äô*z);
if niter > 1
beta = rho / rho1;
p = z + beta*p;
else
p = z;
end
q = A*p;
alpha = rho / (p‚Äô*q );
x = x + alpha * p;
r = r - alpha*q;
error = norm( r ) / bnrm2;
if ( error <= tol ), break, end
rho1 = rho;
end
if ( error > tol ) Ô¨Çag = 1; end

158
4. Iterative Methods for Solving Linear Systems
Example 4.7 Let us consider again the linear system of Example 4.6. The CG
method has been run with the same input data as in the previous example. It
converges in 3 iterations for m = 16 and in 45 iterations for m = 400. Using the
same preconditioner as in Example 4.6, the number of iterations decreases from
45 to 26, in the case m = 400.
‚Ä¢
0
5
10
15
20
25
30
35
40
45
10
‚àí14
10
‚àí12
10
‚àí10
10
‚àí8
10
‚àí6
10
‚àí4
10
‚àí2
10
0
FIGURE 4.8. Behavior of the residual, normalized to the right-hand side, as a
function of the number of iterations for the conjugate gradient method applied
to the systems of Example 4.6 in the case m = 400. The curve in dashed line
refers to the non preconditioned method, while the curve in solid line refers to
the preconditioned one
4.3.6
The Alternating-Direction Method
Assume that A = A1+A2, with A1 and A2 symmetric and positive deÔ¨Ånite.
The alternating direction method (ADI), as introduced by Peaceman and
Rachford [PJ55], is an iterative scheme for (3.2) which consists of solving
the following systems ‚àÄk ‚â•0
(I + Œ±1A1)x(k+1/2) = (I ‚àíŒ±1A2)x(k) + Œ±1b,
(I + Œ±2A2)x(k+1) = (I ‚àíŒ±2A1)x(k+1/2) + Œ±2b
(4.50)
where Œ±1 and Œ±2 are two real parameters. The ADI method can be cast in
the form (4.2) setting
B = (I + Œ±2A2)‚àí1(I ‚àíŒ±2A1)(I + Œ±1A1)‚àí1(I ‚àíŒ±1A2),
f =
0
Œ±1(I ‚àíŒ±2A1)(I + Œ±1A1)‚àí1 + Œ±2I
1
b.
Both B and f depend on Œ±1 and Œ±2. The following estimate holds
œÅ(B) ‚â§
max
i=1,... ,n

1 ‚àíŒ±2Œª(1)
i
1 + Œ±1Œª(1)
i
 max
i=1,... ,n

1 ‚àíŒ±1Œª(2)
i
1 + Œ±2Œª(2)
i
 ,

4.4 Methods Based on Krylov Subspace Iterations
159
where Œª(i)
1
and Œª(i)
2 , for i = 1, . . . , n, are the eigenvalues of A1 and A2,
respectively. The method converges if œÅ(B) < 1, which is always veriÔ¨Åed if
Œ±1 = Œ±2 = Œ± > 0. Moreover (see [Axe94]) if Œ≥ ‚â§Œª(j)
i
‚â§Œ¥ ‚àÄi = 1, . . . , n,
‚àÄj = 1, 2, for suitable Œ≥ and Œ¥ then the ADI method converges with the
choice Œ±1 = Œ±2 = 1/‚àöŒ¥Œ≥, provided that Œ≥/Œ¥ tends to 0 as the size of A
grows. In such an event the corresponding spectral radius satisÔ¨Åes
œÅ(B) ‚â§

1 ‚àí

Œ≥/Œ¥
1 +

Œ≥/Œ¥
2
.
4.4
Methods Based on Krylov Subspace Iterations
In this section we introduce iterative methods based on Krylov subspace
iterations. For the proofs and further analysis, we refer to [Saa96], [Axe94]
and [Hac94].
Consider the Richardson method (4.24) with P=I; the residual at the
k-th step can be related to the initial residual as
r(k) =
k‚àí1

j=0
(I ‚àíŒ±jA)r(0)
(4.51)
so that r(k) = pk(A)r(0), where pk(A) is a polynomial in A of degree k. If
we introduce the space
Km(A; v) = span

v, Av, . . . , Am‚àí1v

,
(4.52)
it immediately appears from (4.51) that r(k) ‚ààKk+1(A; r(0)). The space
deÔ¨Åned in (4.52) is called the Krylov subspace of order m. It is a subspace
of the set spanned by all the vectors u ‚ààRn that can be written as u =
pm‚àí1(A)v, where pm‚àí1 is a polynomial in A of degree ‚â§m ‚àí1.
In an analogous manner as for (4.51), it is seen that the iterate x(k) of
the Richardson method is given by
x(k) = x(0) +
k‚àí1

j=0
Œ±jr(j)
so that x(k) belongs to the following space
Wk =
2
v = x(0) + y, y ‚ààKk(A; r(0))
3
.
(4.53)
Notice also that k‚àí1
j=0 Œ±jr(j) is a polynomial in A of degree less than k ‚àí1.
In the non preconditioned Richardson method we are thus looking for an

160
4. Iterative Methods for Solving Linear Systems
approximate solution to x in the space Wk. More generally, we can think
of devising methods that search for approximate solutions of the form
x(k) = x(0) + qk‚àí1(A)r(0),
(4.54)
where qk‚àí1 is a polynomial selected in such a way that x(k) be, in a sense
that must be made precise, the best approximation of x in Wk. A method
that looks for a solution of the form (4.54) with Wk deÔ¨Åned as in (4.53) is
called a Krylov method.
A Ô¨Årst question concerning Krylov subspace iterations is whether the
dimension of Km(A; v) increases as the order m grows. A partial answer is
provided by the following result.
Property 4.7 Let A ‚ààRn√ón and v ‚ààRn. The Krylov subspace Km(A; v)
has dimension equal to m iÔ¨Äthe degree of v with respect to A, denoted by
degA(v), is not less than m, where the degree of v is deÔ¨Åned as the minimum
degree of a monic non null polynomial p in A, for which p(A)v = 0.
The dimension of Km(A; v) is thus equal to the minimum between m and
the degree of v with respect to A and, as a consequence, the dimension
of the Krylov subspaces is certainly a nondecreasing function of m. Notice
that the degree of v cannot be greater than n due to the Cayley-Hamilton
Theorem (see Section 1.7).
Example 4.8 Consider the matrix A = tridiag4(‚àí1, 2, ‚àí1). The vector v =
(1, 1, 1, 1)T has degree 2 with respect to A since p2(A)v = 0 with p2(A) = I4 ‚àí
3A+A2, while there is no monic polynomial p1 of degree 1 for which p1(A)v = 0.
As a consequence, all Krylov subspaces from K2(A; v) on, have dimension equal
to 2. The vector w = (1, 1, ‚àí1, 1)T has, instead, degree 4 with respect to A.
‚Ä¢
For a Ô¨Åxed m, it is possible to compute an orthonormal basis for Km(A; v)
using the so-called Arnoldi algorithm.
Setting v1 = v/‚à•v‚à•2, this method generates an orthonormal basis {vi}
for Km(A; v1) using the Gram-Schmidt procedure (see Section 3.4.3). For
k = 1, . . . , m, the Arnoldi algorithm computes
hik = vT
i Avk,
i = 1, 2, . . . , k,
wk = Avk ‚àí
k

i=1
hikvi,
hk+1,k = ‚à•wk‚à•2.
(4.55)
If wk = 0 the process terminates and in such a case we say that a breakdown
of the algorithm has occurred; otherwise, we set vk+1 = wk/‚à•wk‚à•2 and the
algorithm restarts, incrementing k by 1.

4.4 Methods Based on Krylov Subspace Iterations
161
It can be shown that if the method terminates at the step m then the
vectors v1, . . . , vm form a basis for Km(A; v). In such a case, if we denote
by Vm ‚ààRn√óm the matrix whose columns are the vectors vi, we have
VT
mAVm = Hm,
VT
m+1AVm = Hm,
(4.56)
where Hm ‚ààR(m+1)√óm is the upper Hessenberg matrix whose entries hij
are given by (4.55) and Hm ‚ààRm√óm is the restriction of Hm to the Ô¨Årst m
rows and m columns.
The algorithm terminates at an intermediate step k < m iÔ¨ÄdegA(v1) =
k. As for the stability of the procedure, all the considerations valid for the
Gram-Schmidt method hold. For more eÔ¨Écient and stable computational
variants of (4.55), we refer to [Saa96].
The functions arnoldi alg and GSarnoldi, invoked by Program 21, pro-
vide an implementation of the Arnoldi algorithm. In output, the columns
of V contain the vectors of the generated basis, while the matrix H stores
the coeÔ¨Écients hik computed by the algorithm. If m steps are carried out,
V = Vm and H(1 : m, 1 : m) = Hm.
Program 21 - arnoldi alg : The Arnoldi algorithm
function [V,H]=arnoldi alg(A,v,m)
v=v/norm(v,2); V=[v1];
H=[];
k=0;
while k <= m-1
[k,V,H] = GSarnoldi(A,m,k,V,H);
end
function [k,V,H]=GSarnoldi(A,m,k,V,H)
k=k+1;
H=[H,V(:,1:k)‚Äô*A*V(:,k)];
s=0; for i=1:k,
s=s+H(i,k)*V(:,i);
end
w=A*V(:,k)-s;
H(k+1,k)=norm(w,2);
if ( H(k+1,k) <= eps ) & ( k < m )
V=[V,w/H(k+1,k)];
else
k=m+1;
end
Having introduced an algorithm for generating the basis for a Krylov sub-
space of any order, we can now solve the linear system (3.2) by a Krylov
method. As already noticed, for all of these methods the iterate x(k) is
always of the form (4.54) and, for a given r(0), the vector x(k) is selected as
being the unique element in Wk which satisÔ¨Åes a criterion of minimal dis-
tance from x. Thus, the feature distinguishing two diÔ¨Äerent Krylov methods
is the criterion for selecting x(k).
The most natural idea consists of searching for x(k) ‚ààWk as the vector
which minimizes the Euclidean norm of the error. This approach, how-

162
4. Iterative Methods for Solving Linear Systems
ever, does not work in practice since x(k) would depend on the (unknown)
solution x.
Two alternative strategies can be pursued:
1. compute x(k) ‚ààWk enforcing that the residual r(k) is orthogonal to
any vector in Kk(A; r(0)), i.e., we look for x(k) ‚ààWk such that
vT (b ‚àíAx(k)) = 0
‚àÄv ‚ààKk(A; r(0));
(4.57)
2. compute x(k) ‚ààWk minimizing the Euclidean norm of the residual
‚à•r(k)‚à•2, i.e.
‚à•b ‚àíAx(k)‚à•2 = min
v‚ààWk‚à•b ‚àíAv‚à•2.
(4.58)
Satisfying (4.57) leads to the Arnoldi method for linear systems (more
commonly known as FOM, full orthogonalization method), while satisfying
(4.58) yields the GMRES (generalized minimum residual) method.
In the two forthcoming sections we shall assume that k steps of the
Arnoldi algorithm have been carried out, in such a way that an orthonormal
basis for Kk(A; r(0)) has been generated and stored into the column vectors
of the matrix Vk with v1 = r(0)/‚à•r(0)‚à•2. In such a case the new iterate x(k)
can always be written as
x(k) = x(0) + Vkz(k),
(4.59)
where z(k) must be selected according to a Ô¨Åxed criterion.
4.4.1
The Arnoldi Method for Linear Systems
Let us enforce that r(k) be orthogonal to Kk(A; r(0)) by requiring that
(4.57) holds for all the basis vectors vi, i.e.
VT
k r(k) = 0.
(4.60)
Since r(k) = b‚àíAx(k) with x(k) of the form (4.59), relation (4.60) becomes
VT
k (b ‚àíAx(0)) ‚àíVT
k AVkz(k) = VT
k r(0) ‚àíVT
k AVkz(k) = 0.
(4.61)
Due to the orthonormality of the basis and the choice of v1, VT
k r(0) =
‚à•r(0)‚à•2e1, e1 being the Ô¨Årst unit vector of Rm. Recalling (4.56), from (4.61)
it turns out that z(k) is the solution to the linear system
Hkz(k) = ‚à•r(0)‚à•2e1.
(4.62)
Once z(k) is known, we can compute x(k) from (4.59). Since Hk is an upper
Hessenberg matrix, the linear system in (4.62) can be easily solved, for
instance, resorting to the LU factorization of Hk.

4.4 Methods Based on Krylov Subspace Iterations
163
We notice that the method, if working in exact arithmetic, cannot execute
more than n steps and that it terminates after m < n steps only if a
breakdown in the Arnoldi algorithm occurs. As for the convergence of the
method, the following result holds.
Theorem 4.13 In exact arithmetic the Arnoldi method yields the solution
of (3.2) after at most n iterations.
Proof. If the method terminates at the n-th iteration, then it must necessarily
be x(n) = x since Kn(A; r(0)) = Rn. Conversely, if a breakdown occurs after m
iterations, for a suitable m < n, then x(m) = x. Indeed, inverting the Ô¨Årst relation
in (4.56), we get
x(m) = x(0) + Vmz(m) = x(0) + VmH‚àí1
m VT
mr(0) = A‚àí1b.
3
In its naive form, FOM does not require an explicit computation of the
solution or the residual, unless a breakdown occurs. Therefore, monitoring
its convergence (by computing, for instance, the residual at each step) might
be computationally expensive. The residual, however, is available without
explicitly requiring to compute the solution since at the k-th step we have
‚à•b ‚àíAx(k)‚à•2 = hk+1,k|eT
k zk|
and, as a consequence, one can decide to stop the method if
hk+1,k|eT
k zk|/‚à•r(0)‚à•2 ‚â§Œµ
(4.63)
Œµ > 0 being a Ô¨Åxed tolerance.
The most relevant consequence of Theorem 4.13 is that FOM can be
regarded as a direct method, since it yields the exact solution after a Ô¨Ånite
number of steps. However, this fails to hold when working in Ô¨Çoating point
arithmetic due to the cumulating rounding errors. Moreover, if we also
account for the high computational eÔ¨Äort, which, for a number of m steps
and a sparse matrix of order n with nz nonzero entries, is of the order of
2(nz + mn) Ô¨Çops, and the large memory occupation needed to store the
matrix Vm, we conclude that the Arnoldi method cannot be used in the
practice, except for small values of m.
Several remedies to this drawback are available, one of which consisting
of preconditioning the system (using, for instance, one of the precondition-
ers proposed in Section 4.3.2). Alternatively, we can also introduce some
modiÔ¨Åed versions of the Arnoldi method following two approaches:
1. no more than m consecutive steps of FOM are taken, m being a small
Ô¨Åxed number (usually, m ‚âÉ10). If the method fails to converge, we set

164
4. Iterative Methods for Solving Linear Systems
x(0) = x(m) and FOM is repeated for other m steps. This procedure
is carried out until convergence is achieved. This method, known as
FOM(m) or FOM with restart, reduces the memory occupation, only
requiring to store matrices with m columns at most;
2. a limitation is set on the number of directions involved in the orthog-
onalization procedure in the Arnoldi algorithm, yielding the incom-
plete orthogonalization method or IOM. In the practice, the k-th step
of the Arnoldi algorithm generates a vector vk+1 which is orthonor-
mal, at most, to the q preceding vectors, where q is Ô¨Åxed according
to the amount of available memory.
It is worth noticing that Theorem 4.13 does no longer hold for the methods
stemming from the two strategies above.
Program 22 provides an implementation of the FOM algorithm with a
stopping criterion based on the residual (4.63). The input parameter m is
the maximum admissible size of the Krylov subspace that is being gener-
ated and represents, as a consequence, the maximum admissible number of
iterations.
Program 22 - arnoldi met : The Arnoldi method for linear systems
function [x,k]=arnoldi met(A,b,m,x0,toll)
r0=b-A*x0; nr0=norm(r0,2);
if nr0 Àú= 0
v1=r0/nr0; V=[v1]; H=[]; k=0; istop=0;
while (k <= m-1) & (istop == 0)
[k,V,H] = GSarnoldi(A,m,k,V,H);
[nr,nc]=size(H); e1=eye(nc);
y=(e1(:,1)‚Äô*nr0)/H(1:nc,:);
residual = H(nr,nc)*abs(y*e1(:,nc));
if residual <= toll
istop = 1; y=y‚Äô;
end
end
if istop==0
[nr,nc]=size(H); e1=eye(nc);
y=(e1(:,1)‚Äô*nr0)/H(1:nc,:); y=y‚Äô;
end
x=x0+V(:,1:nc)*y;
else
x=x0;
end
Example 4.9 Let us solve the linear system Ax = b with A = tridiag100(‚àí1, 2,
‚àí1) and b such that the solution is x = 1T . The initial vector is x(0) = 0T
and toll=10‚àí10. The method converges in 50 iterations and Figure 4.9 reports
its convergence history. Notice the sudden, dramatic, reduction of the residual,

4.4 Methods Based on Krylov Subspace Iterations
165
which is a typical warning that the last generated subspace Wk is suÔ¨Éciently rich
to contain the exact solution of the system.
‚Ä¢
0
10
20
30
40
50
60
10
‚àí16
10
‚àí14
10
‚àí12
10
‚àí10
10
‚àí8
10
‚àí6
10
‚àí4
10
‚àí2
10
0
10
2
FIGURE 4.9. The behavior of the residual as a function of the number of itera-
tions for the Arnoldi method applied to the linear system in Example 4.9
4.4.2
The GMRES Method
This method is characterized by selecting x(k) in such a way to minimize
the Euclidean norm of the residual at each k-th step. Recalling (4.59) we
have
r(k) = r(0) ‚àíAVkz(k),
(4.64)
but, since r(0) = v1‚à•r(0)‚à•2 and (4.56) holds, relation (4.64) becomes
r(k) = Vk+1(‚à•r(0)‚à•2e1 ‚àíHkz(k)),
(4.65)
where e1 is the Ô¨Årst unit vector of Rk+1. Therefore, in the GMRES method
the solution at step k can be computed through (4.59) as
z(k) chosen in such a way to minimize ‚à•‚à•r(0)‚à•2e1 ‚àíHkz(k)‚à•2
(4.66)
(the matrix Vk+1 appearing in (4.65) does not change the value of ‚à•¬∑ ‚à•2
since it is orthogonal). Having to solve at each step a least-squares problem
of size k, the GMRES method will be the more eÔ¨Äective the smaller is
the number of iterations. Exactly as for the Arnoldi method, the GMRES
method terminates at most after n iterations, yielding the exact solution.
Premature stops are due to a breakdown in the orthonormalization Arnoldi
algorithm. More precisely, we have the following result.
Property 4.8 A breakdown occurs for the GMRES method at a step m
(with m < n) iÔ¨Äthe computed solution x(m) coincides with the exact solu-
tion to the system.

166
4. Iterative Methods for Solving Linear Systems
A basic implementation of the GMRES method is provided in Program 23.
This latter requires in input the maximum admissible size m for the Krylov
subspace and the tolerance toll on the Euclidean norm of the residual
normalized to the initial residual. This implementation of the method com-
putes the solution x(k) at each step in order to evaluate the residual, with
a consequent increase of the computational eÔ¨Äort.
Program 23 - GMRES : The GMRES method for linear systems
function [x,k]=gmres(A,b,m,toll,x0)
r0=b-A*x0; nr0=norm(r0,2);
if nr0 Àú= 0
v1=r0/nr0; V=[v1]; H=[]; k=0; residual=1;
while k <= m-1 & residual > toll,
[k,V,H] = GSarnoldi(A,m,k,V,H);
[nr,nc]=size(H);
y=(H‚Äô*H) \ (H‚Äô*nr0*[1;zeros(nr-1,1)]);
x=x0+V(:,1:nc)*y;
residual = norm(b-A*x,2)/nr0;
end
else
x=x0;
end
To improve the eÔ¨Éciency of the GMRES algorithm it is necessary to devise
a stopping criterion which does not require the explicit evaluation of the
residual at each step. This is possible, provided that the linear system with
upper Hessenberg matrix Hk is appropriately solved.
In practice, Hk is transformed into an upper triangular matrix Rk ‚àà
R(k+1)√ók with rk+1,k = 0 such that QT
k Rk = Hk, where Qk is a matrix
obtained as the product of k Givens rotations (see Section 5.6.3). Then,
since Qk is orthogonal, it can be seen that minimizing ‚à•‚à•r(0)‚à•2e1‚àíHkz(k)‚à•2
is equivalent to minimize ‚à•fk ‚àíRkz(k)‚à•2, with fk = Qk‚à•r(0)‚à•2e1. It can
also be shown that the k + 1-th component of fk is, in absolute value, the
Euclidean norm of the residual at the k-th step.
As FOM, the GMRES method entails a high computational eÔ¨Äort and
a large amount of memory, unless convergence occurs after few iterations.
For this reason, two variants of the algorithm are available, one named
GMRES(m) and based on the restart after m steps, the other named Quasi-
GMRES or QGMRES and based on stopping the Arnoldi orthogonalization
process. It is worth noting that these two methods do not enjoy Property
4.8.
Remark 4.4 (Projection methods) Denoting by Yk and Lk two generic
m-dimensional subspaces of Rn, we call projection method a process which
generates an approximate solution x(k) at step k, enforcing that x(k) ‚ààYk

4.4 Methods Based on Krylov Subspace Iterations
167
and that the residual r(k) = b‚àíAx(k) be orthogonal to Lk. If Yk = Lk, the
projection process is said to be orthogonal, oblique otherwise (see [Saa96]).
The Krylov subspace iterations can be regarded as being projection
methods. For instance, the Arnoldi method is an orthogonal projection
method where Lk = Yk = Kk(A; r(0)), while the GMRES method is an
oblique projection method with Yk = Kk(A; r(0)) and Lk = AYk. It is
worth noticing that some classical methods introduced in previous sections
fall into this category. For example, the Gauss-Seidel method is an orthogo-
nal projection method where at the k-th step Kk(A; r(0)) = span(ek), with
k = 1, . . . , n. The projection steps are carried out cyclically from 1 to n
until convergence.
‚ñ†
4.4.3
The Lanczos Method for Symmetric Systems
The Arnoldi algorithm simpliÔ¨Åes considerably if A is symmetric since the
matrix Hm is tridiagonal and symmetric (indeed, from (4.56) it turns out
that Hm must be symmetric, so that, being upper Hessenberg by construc-
tion, it must necessarily be tridiagonal). In such an event the method is
more commonly known as the Lanczos algorithm. For ease of notation, we
henceforth let Œ±i = hii and Œ≤i = hi‚àí1,i.
An implementation of the Lanczos algorithm is provided in Program 24.
Vectors alpha and beta contain the coeÔ¨Écients Œ±i and Œ≤i computed by the
scheme.
Program 24 - Lanczos : The Lanczos method for linear systems
function [V,alpha,beta]=lanczos(A,m)
n=size(A); V=[0*[1:n]‚Äô,[1,0*[1:n-1]]‚Äô];
beta(1)=0; normb=1; k=1;
while k <= m & normb >= eps
vk = V(:,k+1);
w = A*vk-beta(k)*V(:,k);
alpha(k)= w‚Äô*vk;
w = w - alpha(k)*vk
normb = norm(w,2);
if normb Àú= 0
beta(k+1)=normb;
V=[V,w/normb];
k=k+1;
end
end
[n,m]=size(V); V=V(:,2:m-1);
alpha=alpha(1:n); beta=beta(2:n);
The algorithm, which is far superior to Arnoldi‚Äôs one as far as memory
saving is concerned, is not numerically stable since only the Ô¨Årst generated
vectors are actually orthogonal. For this reason, several stable variants have
been devised.
As in previous cases, also the Lanczos algorithm can be employed as a
solver for linear systems, yielding a symmetric form of the FOM method. It

168
4. Iterative Methods for Solving Linear Systems
can be shown that r(k) = Œ≥kvk+1, for a suitable Œ≥k (analogously to (4.63))
so that the residuals are all mutually orthogonal.
Remark 4.5 (The conjugate gradient method) If A is symmetric and
positive deÔ¨Ånite, starting from the Lanczos method for linear systems it is
possible to derive the conjugate gradient method already introduced in Sec-
tion 4.3.4 (see [Saa96]). The conjugate gradient method is a variant of the
Lanczos method where the orthonormalization process remains incomplete.
As a matter of fact, the A-conjugate directions of the CG method can
be characterized as follows. If we carry out at the generic k-th step the
LU factorization Hk = LkUk, with Lk (Uk) lower (upper) bidiagonal, the
iterate x(k) of the Lanczos method for systems reads
x(k) = x(0) + PkL‚àí1
k ‚à•r(0)‚à•2e1,
with Pk = VkU‚àí1
k . The column vectors of Pk are mutually A-conjugate.
Indeed, PT
k APk is symmetric and bidiagonal since
PT
k APk = U‚àíT
k
HkU‚àí1
k
= U‚àíT
k
Lk,
so that it must necessarily be diagonal. As a result, pT
j Api = 0 if i Ã∏= j,
having denoted by pi the i-th column vector of matrix Pk.
‚ñ†
As happens for the FOM method, also the GMRES method simpliÔ¨Åes
if A is symmetric. The resulting scheme is called conjugate residuals or
CR method since it enjoys the property that the residuals are mutually
A-conjugate. Variants of this method are the generalized conjugate resid-
uals method (GCR) and the method commonly known as ORTHOMIN
(obtained by truncation of the orthonormalization process as done for the
IOM method).
4.5
The Lanczos Method for Unsymmetric Systems
The Lanczos orthogonalization process can be extended to deal with un-
symmetric matrices through a bi-orthogonalization procedure as follows.
Two bases, {vi}m
i=1 and {zi}m
i=1, are generated for the subspaces Km(A; v1)
and Km(AT ; z1), respectively, with zT
1 v1 = 1, such that
zT
i vj = Œ¥ij,
i, j = 1, . . . , m.
(4.67)
Two sets of vectors satisfying (4.67) are said to be bi-orthogonal and can
be obtained through the following algorithm: setting Œ≤1 = Œ≥1 = 0 and z0 =
v0 = 0T , at the generic k-th step, with k = 1, . . . , m, we set Œ±k = zT
k Avk,
then we compute
Àúvk+1 = Avk ‚àíŒ±kvk ‚àíŒ≤kvk‚àí1,
Àúzk+1 = AT zk ‚àíŒ±kzk ‚àíŒ≥kzk‚àí1.

4.5 The Lanczos Method for Unsymmetric Systems
169
If Œ≥k+1 =

|ÀúzT
k+1Àúvk+1| = 0 the algorithm is stopped, otherwise we set
Œ≤k+1 = ÀúzT
k+1Àúvk+1/Œ≥k+1 and generate two new vectors in the basis as
vk+1 = Àúvk+1/Œ≥k+1,
zk+1 = Àúzk+1/Œ≤k+1.
If the process terminates after m steps, denoting by Vm and Zm the ma-
trices whose columns are the vectors of the basis that has been generated,
we have
ZT
mAVm = Tm,
Tm being the following tridiagonal matrix
Tm =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
Œ±1
Œ≤2
0
Œ≥2
Œ±2
...
...
...
Œ≤m
0
Œ≥m
Œ±m
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
As in the symmetric case, the bi-orthogonalization Lanczos algorithm can
be utilized to solve the linear system (3.2). For this purpose, for m Ô¨Åxed,
once the bases {vi}m
i=1 and {zi}m
i=1 have been constructed, it suÔ¨Éces to set
x(m) = x(0) + Vmy(m),
where y(m) is the solution to the linear system Tmy(m) = ‚à•r(0)‚à•2e1. It
is also possible to introduce a stopping criterion based on the residual,
without computing it explicitly, since
‚à•r(m)‚à•2 = |Œ≥m+1eT
mym| ‚à•vm+1‚à•2.
An implementation of the Lanczos method for unsymmetric systems is
given in Program 25. If a breakdown of the algorithm occurs, i.e., if Œ≥k+1 =
0, the method stops returning in output a negative value of the variable
niter which denotes the number of iterations necessary to reduce the initial
residual by a factor toll.

170
4. Iterative Methods for Solving Linear Systems
Program 25 - Lanczosnosym
: The Lanczos method for unsymmetric
systems
function [xk,nres,niter]=lanczosnosym(A,b,x0,m,toll)
r0=b-A*x0; nres0=norm(r0,2);
if nres0 Àú= 0
V=r0/nres0; Z=V; gamma(1)=0; beta(1)=0; k=1; nres=1;
while k <= m & nres > toll
vk=V(:,k); zk=Z(:,k);
if
k==1, vk1=0*vk;
zk1=0*zk;
else, vk1=V(:,k-1); zk1=Z(:,k-1); end
alpha(k)=zk‚Äô*A*vk;
tildev=A*vk-alpha(k)*vk-beta(k)*vk1;
tildez=A‚Äô*zk-alpha(k)*zk-gamma(k)*zk1;
gamma(k+1)=sqrt(abs(tildez‚Äô*tildev));
if gamma(k+1) == 0,
k=m+2;
else
beta(k+1)=tildez‚Äô*tildev/gamma(k+1);
Z=[Z,tildez/beta(k+1)];
V=[V,tildev/gamma(k+1)];
end
if kÀú=m+2
if k==1
Tk = alpha;
else
Tk=diag(alpha)+diag(beta(2:k),1)+diag(gamma(2:k),-1);
end
yk=Tk \ (nres0*[1,0*[1:k-1]]‚Äô);
xk=x0+V(:,1:k)*yk;
nres=abs(gamma(k+1)*[0*[1:k-1],1]*yk)*norm(V(:,k+1),2)/nres0;
k=k+1;
end
end
else
x=x0;
end
if k==m+2, niter=-k; else, niter=k-1; end
Example 4.10 Let us solve the linear system with matrix A = tridiag100(‚àí0.5, 2,
‚àí1) and right-side b selected in such a way that the exact solution is x = 1T .
Using Program 25 with toll= 10‚àí13 and a randomly generated x0, the algorithm
converges in 59 iterations. Figure 4.10 shows the convergence history reporting
the graph of ‚à•r(k)‚à•2/‚à•r(0)‚à•2 as a function of the number of iterations.
‚Ä¢
We conclude recalling that some variants of the unsymmetric Lanczos
method have been devised, that are characterized by a reduced compu-
tational cost. We refer the interested reader to the bibliography below for a

4.6 Stopping Criteria
171
0
10
20
30
40
50
60
10
‚àí14
10
‚àí12
10
‚àí10
10
‚àí8
10
‚àí6
10
‚àí4
10
‚àí2
10
0
FIGURE 4.10. Graph of the residual normalized to the initial residual as a func-
tion of the number of iterations for the Lanczos method applied to the system in
Example 4.10
complete description of the algorithms and to the programs included in the
MATLAB version of the public domain library templates for their eÔ¨Écient
implementation [BBC+94].
1. The bi-conjugate gradient method (BiCG): it can be derived by the
unsymmetric Lanczos method in the same way as the conjugate gra-
dient method is obtained from the FOM method [Fle75];
2. the Quasi-Minimal Residual method (QMR): it is analogous to the
GMRES method, the only diÔ¨Äerence being the fact that the Arnoldi
orthonormalization process is replaced by the Lanczos bi-orthogona-
lization;
3. the conjugate gradient squared method (CGS): the matrix-vector prod-
ucts involving the transposed matrix AT are removed. A variant of
this method, known as BiCGStab, is characterized by a more reg-
ular convergence than provided by the CGS method (see [Son89],
[vdV92]).
4.6
Stopping Criteria
In this section we address the problem of how to estimate the error intro-
duced by an iterative method and the number kmin of iterations needed to
reduce the initial error by a factor Œµ.
In practice, kmin can be obtained by estimating the convergence rate of
(4.2), i.e. the rate at which ‚à•e(k)‚à•‚Üí0 as k tends to inÔ¨Ånity. From (4.4),
we get
‚à•e(k)‚à•
‚à•e(0)‚à•‚â§‚à•Bk‚à•,

172
4. Iterative Methods for Solving Linear Systems
so that ‚à•Bk‚à•is an estimate of the reducing factor of the norm of the error
after k steps. Typically, the iterative process is continued until ‚à•e(k)‚à•has
reduced with respect to ‚à•e(0)‚à•by a certain factor Œµ < 1, that is
‚à•e(k)‚à•‚â§Œµ‚à•e(0)‚à•.
(4.68)
If we assume that œÅ(B) < 1, then Property 1.13 implies that there exists
a suitable matrix norm ‚à•¬∑ ‚à•such that ‚à•B‚à•< 1. As a consequence, ‚à•Bk‚à•
tends to zero as k tends to inÔ¨Ånity, so that (4.68) can be satisÔ¨Åed for a
suÔ¨Éciently large k such that ‚à•Bk‚à•‚â§Œµ holds. However, since ‚à•Bk‚à•< 1, the
previous inequality amounts to requiring that
k ‚â•log(Œµ)/
1
k log ‚à•Bk‚à•

= ‚àílog(Œµ)/Rk(B),
(4.69)
where Rk(B) is the average convergence rate introduced in DeÔ¨Ånition 4.2.
From a practical standpoint, (4.69) is useless, being nonlinear in k; if, how-
ever, the asymptotic convergence rate is adopted, instead of the average
one, the following estimate for kmin is obtained
kmin ‚âÉ‚àílog(Œµ)/R(B).
(4.70)
This latter estimate is usually rather optimistic, as conÔ¨Årmed by Example
4.11.
Example 4.11 For the matrix A3 of Example 4.2, in the case of Jacobi method,
letting Œµ = 10‚àí5, condition (4.69) is satisÔ¨Åed with kmin = 16, while (4.70) yields
kmin = 15, with a good agreement between the two estimates. Instead, on the
matrix A4 of Example 4.2, we Ô¨Ånd that (4.69) is satisÔ¨Åed with kmin = 30, while
(4.70) yields kmin = 26.
‚Ä¢
4.6.1
A Stopping Test Based on the Increment
From the recursive error relation e(k+1) = Be(k), we get
‚à•e(k+1)‚à•‚â§‚à•B‚à•‚à•e(k)‚à•.
(4.71)
Using the triangular inequality we get
‚à•e(k+1)‚à•‚â§‚à•B‚à•(‚à•e(k+1)‚à•+ ‚à•x(k+1) ‚àíx(k)‚à•),
from which it follows that
‚à•x ‚àíx(k+1)‚à•‚â§
‚à•B‚à•
1 ‚àí‚à•B‚à•‚à•x(k+1) ‚àíx(k)‚à•.
(4.72)
In particular, taking k = 0 in (4.72) and applying recursively (4.71) we also
get
‚à•x ‚àíx(k+1)‚à•‚â§‚à•B‚à•k+1
1 ‚àí‚à•B‚à•‚à•x(1) ‚àíx(0)‚à•,

4.6 Stopping Criteria
173
which can be used to estimate the number of iterations necessary to fulÔ¨Åll
the condition ‚à•e(k+1)‚à•‚â§Œµ, for a given tolerance Œµ.
In the practice, ‚à•B‚à•can be estimated as follows: since
x(k+1) ‚àíx(k) = ‚àí(x ‚àíx(k+1)) + (x ‚àíx(k)) = B(x(k) ‚àíx(k‚àí1)),
a lower bound of ‚à•B‚à•is provided by c = Œ¥k+1/Œ¥k, where Œ¥j+1 = ‚à•x(j+1) ‚àí
x(j)‚à•, with j = k ‚àí1, k. Replacing ‚à•B‚à•by c, the right-hand side of (4.72)
suggests using the following indicator for ‚à•e(k+1)‚à•
œµ(k+1) =
Œ¥2
k+1
Œ¥k ‚àíŒ¥k+1
.
(4.73)
Due to the kind of approximation of ‚à•B‚à•that has been used, the reader is
warned that œµ(k+1) should not be regarded as an upper bound for ‚à•e(k+1)‚à•.
However, often œµ(k+1) provides a reasonable indication about the true error
behavior, as we can see in the following example.
Example 4.12 Consider the linear system Ax=b with
A =
Ô£Æ
Ô£∞
4
1
1
2
‚àí9
0
0
‚àí8
‚àí6
Ô£π
Ô£ª,
b =
Ô£Æ
Ô£∞
6
‚àí7
‚àí14
Ô£π
Ô£ª,
which admits the unit vector as exact solution. Let us apply the Jacobi method
and estimate the error at each step by using (4.73). Figure 4.11 shows an ac-
ceptable agreement between the behavior of the error ‚à•e(k+1)‚à•‚àûand that of its
estimate œµ(k+1).
‚Ä¢
0
5
10
15
20
25
10
‚àí8
10
‚àí7
10
‚àí6
10
‚àí5
10
‚àí4
10
‚àí3
10
‚àí2
10
‚àí1
10
0
10
1
FIGURE 4.11. Absolute error (in solid line) versus the error estimated by (4.73)
(dashed line). The number of iterations is indicated on the x-axis

174
4. Iterative Methods for Solving Linear Systems
4.6.2
A Stopping Test Based on the Residual
A diÔ¨Äerent stopping criterion consists of continuing the iteration until
‚à•r(k)‚à•‚â§Œµ, Œµ being a Ô¨Åxed tolerance. Note that
‚à•x ‚àíx(k)‚à•= ‚à•A‚àí1b ‚àíx(k)‚à•= ‚à•A‚àí1r(k)‚à•‚â§‚à•A‚àí1‚à•Œµ.
Considering instead a normalized residual, i.e. stopping the iteration as
soon as ‚à•r(k)‚à•/‚à•b‚à•‚â§Œµ, we obtain the following control on the relative
error
‚à•x ‚àíx(k)‚à•
‚à•x‚à•
‚â§‚à•A‚àí1‚à•‚à•r(k)‚à•
‚à•x‚à•
‚â§K(A)‚à•‚à•r(k)‚à•
‚à•b‚à•
‚â§ŒµK(A).
In the case of preconditioned methods, the residual is replaced by the pre-
conditioned residual, so that the previous criterion becomes
‚à•P‚àí1r(k)‚à•
‚à•P‚àí1r(0)‚à•‚â§Œµ,
where P is the preconditioning matrix.
4.7
Applications
In this section we consider two examples arising in electrical network anal-
ysis and structural mechanics which lead to the solution of large sparse
linear systems.
4.7.1
Analysis of an Electric Network
We consider a purely resistive electric network (shown in Figure 4.12, left)
which consists of a connection of n stages S (Figure 4.12, right) through
the series resistances R. The circuit is completed by the driving current
generator I0 and the load resistance RL. As an example, a purely resistive
network is a model of a signal attenuator for low-frequency applications
where capacitive and inductive eÔ¨Äects can be neglected. The connecting
points between the electrical components will be referred to henceforth as
nodes and are progressively labeled as drawn in the Ô¨Ågure. For n ‚â•1, the
total number of nodes is 4n. Each node is associated with a value of the
electric potential Vi, i = 0, . . . , 4n, which are the unknowns of the problem.
The nodal analysis method is employed to solve the problem. Precisely,
the KirchhoÔ¨Äcurrent law is written at any node of the network leading
to the linear system ÀúY ÀúV = ÀúI, where ÀúV ‚ààRN+1 is the vector of nodal
potentials, ÀúI ‚ààRN+1 is the load vector and the entries of the matrix ÀúY ‚àà

4.7 Applications
175
R
I
S
S
R
RL
2
4
1
3
5
n
n-1
6
R
R
R
R
R
R
1
2
3
4
5
FIGURE 4.12. Resistive electric network (left) and resistive stage S (right)
R(N+1)√ó(N+1), for i, j = 0, . . . , 4n, are given by
ÀúYij =
Ô£±
Ô£¥
Ô£≤
Ô£¥
Ô£≥

j‚ààk(i)
Gij,
for i = j,
‚àíGij,
for i Ã∏= j,
where k(i) is the index set of the neighboring nodes of node i and Gij =
1/Rij is the admittance between node i and node j, provided Rij denotes
the resistance between the two nodes i and j. Since the potential is deÔ¨Åned
up to an additive constant, we arbitrarily set V0 = 0 (ground potential). As
a consequence, the number of independent nodes for potential diÔ¨Äerence
computations is N = 4n ‚àí1 and the linear system to be solved becomes
YV = I, where Y ‚ààRN√óN, V ‚ààRN and I ‚ààRN are obtained eliminating
the Ô¨Årst row and column in ÀúY and the Ô¨Årst entry in ÀúV and ÀúI, respectively.
The matrix Y is symmetric, diagonally dominant and positive deÔ¨Ånite. This
last property follows by noting that
ÀúVT ÀúY ÀúV =
N

i=1
GiiV 2
i +
N

i,j=1
Gij(Vi ‚àíVj)2,
which is always a positive quantity, being equal to zero only if ÀúV = 0. The
sparsity pattern of Y in the case n = 3 is shown in Figure 4.13 (left) while
the spectral condition number of Y as a function of the number of blocks n
is reported in Figure 4.13 (right). Our numerical computations have been
carried out setting the resistance values equal to 1 ‚Ñ¶while I0 = 1 A.
In Figure 4.14 we report the convergence history of several non precondi-
tioned iterative methods in the case n = 5 corresponding to a matrix size of
19 √ó 19. The plots show the Euclidean norms of the residual normalized to
the initial residual. The dashed curve refers to the Gauss-Seidel method, the
dash-dotted line refers to the gradient method, while the solid and circled
lines refer respectively to the conjugate gradient (CG) and SOR method
(with an optimal value of the relaxation parameter œâ ‚âÉ1.76 computed
according to (4.19) since Y is block tridiagonal symmetric positive deÔ¨Å-
nite). The SOR method converges in 109 iterations, while the CG method
converges in 10 iterations.
We have also considered the solution of the system at hand by the conju-
gate gradient (CG) method using the Cholesky version of the ILU(0) and

176
4. Iterative Methods for Solving Linear Systems
0
2
4
6
8
10
12
0
2
4
6
8
10
12
0
5
10
15
20
25
30
35
40
45
50
100
101
102
103
104
105
FIGURE 4.13. Sparsity pattern of Y for n = 3 (left) and spectral condition
number of Y as a function of n (right)
0
50
100
150
200
250
10
‚àí16
10
‚àí14
10
‚àí12
10
‚àí10
10
‚àí8
10
‚àí6
10
‚àí4
10
‚àí2
10
0
10
2
FIGURE 4.14. Convergence history of several non preconditioned iterative meth-
ods
MILU(0) preconditioners, where drop tolerances equal to Œµ = 10‚àí2, 10‚àí3
have been chosen for the MILU(0) preconditioner (see Section 4.3.2). Cal-
culations with both preconditioners have been done using the MATLAB
functions cholinc and michol. Table 4.2 shows the convergence iterations
of the method for n = 5, 10, 20, 40, 80, 160 and for the considered values of
Œµ. We report in the second column the number of nonzero entries in the
Cholesky factor of matrix Y, in the third column the number of iterations
for the CG method without preconditioning to converge, while the columns
ICh(0) and MICh(0) with Œµ = 10‚àí2 and Œµ = 10‚àí3 show the same infor-
mation for the CG method using the incomplete Cholesky and modiÔ¨Åed
incomplete Cholesky preconditioners, respectively.
The entries in the table are the number of iterations to converge and the
number in the brackets are the nonzero entries of the L-factor of the cor-
responding preconditioners. Notice the decrease of the iterations as Œµ de-
creases, as expected. Notice also the increase of the number of iterations
with respect to the increase of the size of the problem.

4.7 Applications
177
n
nz
CG
ICh(0)
MICh(0) Œµ = 10‚àí2
MICh(0) Œµ = 10‚àí3
5
114
10
9 (54)
6 (78)
4 (98)
10
429
20
15 (114)
7 (173)
5 (233)
20
1659
40
23 (234)
10 (363)
6 (503)
40
6519
80
36 (474)
14 (743)
7 (1043)
80
25839
160
62 (954)
21 (1503)
10 (2123)
160
102879
320
110 (1914)
34 (3023)
14 (4283)
TABLE 4.2. Convergence iterations for the preconditioned CG method
4.7.2
Finite DiÔ¨Äerence Analysis of Beam Bending
Consider the beam clamped at the endpoints that is drawn in Figure 4.15
(left). The structure, of length L, is subject to a distributed load P, varying
along the free coordinate x and expressed in [Kgm‚àí1]. We assume hence-
forth that the beam has uniform rectangular section, of width r and depth
s, momentum of inertia J = rs3/12 and Young‚Äôs module E, expressed in
[m4] and [Kg m‚àí2], respectively.
P(x)
                                                















                                                















u(x)
x
0
20
40
60
80
100
120
10
‚àí20
10
‚àí15
10
‚àí10
10
‚àí5
10
0
10
5
n=10
n=60
n=110
FIGURE 4.15. Clamped beam (left); convergence histories for the preconditioned
conjugate gradient method in the solution of system (4.76) (right)
The transverse bending of the beam, under the assumption of small dis-
placements, is governed by the following fourth-order diÔ¨Äerential equation
(EJu‚Ä≤‚Ä≤)‚Ä≤‚Ä≤(x) = P(x),
0 < x < L,
(4.74)
where u = u(x) denotes the vertical displacement. The following boundary
conditions (at the endpoints x = 0 and x = L)
u(0) = u(L) = 0,
u‚Ä≤(0) = u‚Ä≤(L) = 0,
(4.75)
model the eÔ¨Äect of the two clampings (vanishing displacements and rota-
tions). To solve numerically the boundary-value problem (4.74)-(4.75), we
use the Ô¨Ånite diÔ¨Äerence method (see Section 10.10.1 and Exercise 11 of
Chapter 12).

178
4. Iterative Methods for Solving Linear Systems
With this aim, let us introduce the discretization nodes xj = jh, with
h = L/Nh and j = 0, . . . , Nh, and substitute at each node xj the fourth-
order derivative with an approximation through centered Ô¨Ånite diÔ¨Äerences.
Letting f(x) = P(x)/(EJ), fj = f(xj) and denoting by Œ∑j the (approx-
imate) nodal displacement of the beam at node xj, the Ô¨Ånite diÔ¨Äerence
discretization of (4.74)-(4.75) is
"
Œ∑j‚àí2 ‚àí4Œ∑j‚àí1 + 6Œ∑j ‚àí4Œ∑j+1 + Œ∑j+2 = h4fj, ‚àÄj = 2, . . . , Nh ‚àí2,
Œ∑0 = Œ∑1 = Œ∑Nh‚àí1 = Œ∑Nh = 0.
(4.76)
The null displacement boundary conditions in (4.76) that have been im-
posed at the Ô¨Årst and the last two nodes of the grid, require that Nh ‚â•4.
Notice that a fourth-order scheme has been used to approximate the fourth-
order derivative, while, for sake of simplicity, a Ô¨Årst-order approximation
has been employed to deal with the boundary conditions (see Section
10.10.1).
The Nh ‚àí3 discrete equations (4.76) yield a linear system of the form
Ax = b where the unknown vector x ‚ààRNh‚àí3 and the load vector
b ‚ààRNh‚àí3 are given respectively by x = (Œ∑2, Œ∑3, . . . , Œ∑Nh‚àí2)T and b =
(f2, f3, . . . , fNh‚àí2)T , while the coeÔ¨Écient matrix A ‚ààR(Nh‚àí3)√ó(Nh‚àí3) is
pentadiagonal and symmetric, given by A = pentadiagNh‚àí3(1, ‚àí4, 6, ‚àí4, 1).
The matrix A is symmetric and positive deÔ¨Ånite. Therefore, to solve
system Ax = b, the SSOR preconditioned conjugated gradient method (see
Section 4.21) and the Cholesky factorization method have been employed.
In the remainder of the section, the two methods are identiÔ¨Åed by the
symbols (CG) and (CH).
The convergence histories of CG are reported in Figure 4.15 (right),
where the sequences ‚à•r(k)‚à•2/‚à•b(k)‚à•2, for the values n = 10, 60, 110, are
plotted, r(k) = b ‚àíAx(k) being the residual at the k-th step. The results
have been obtained using Program 20, with toll=10‚àí15 and œâ = 1.8 in
(4.22). The initial vector x(0) has been set equal to the null vector.
As a comment to the graphs, it is worth noting that CG has required 7, 33
and 64 iterations to converge, respectively, with a maximum absolute error
of 5¬∑10‚àí15 with respect to the solution produced by CH. This latter has an
overall computational cost of 136, 1286 and 2436 Ô¨Çops respectively, to be
compared with the corresponding 3117, 149424 and 541647 Ô¨Çops of method
CG. As for the performances of the SSOR preconditioner, we remark that
the spectral condition number of matrix A is equal to 192, 3.8 ¬∑ 105 and
4.5 ¬∑ 106, respectively, while the corresponding values in the preconditioned
case are 65, 1.2 ¬∑ 104 and 1.3 ¬∑ 105.

4.8 Exercises
179
4.8
Exercises
1. The spectral radius of the matrix
B =
 a
4
0
a

is œÅ(B) = a. Check that if 0 < a < 1, then œÅ(B) < 1, while ‚à•Bm‚à•1/m
2
can
be greater than 1.
2. Let A ‚ààRn√ón be a strictly diagonally dominant matrix by rows. Show
that the Gauss-Seidel method for the solution of the linear system (3.2) is
convergent.
3. Check that the matrix A = tridiag(‚àí1, Œ±, ‚àí1), with Œ± ‚ààR, has eigenvalues
given by
Œªj = Œ± ‚àí2 cos(jŒ∏),
j = 1, . . . , n
where Œ∏ = œÄ/(n + 1) and the corresponding eigenvectors are
qj = [sin(jŒ∏), sin(2jŒ∏), . . . , sin(njŒ∏)]T .
Under which conditions on Œ± is the matrix positive deÔ¨Ånite?
[Solution : Œ± ‚â•2.]
4. Consider the pentadiagonal matrix A = pentadiagn(‚àí1, ‚àí1, 10, ‚àí1, ‚àí1).
Assume n = 10 and A = M + N + D, with D = diag(8, . . . , 8) ‚ààR10√ó10,
M = pentadiag10(‚àí1, ‚àí1, 1, 0, 0) and N = MT . To solve Ax = b, analyze
the convergence of the following iterative methods
(a)
(M + D)x(k+1) = ‚àíNx(k) + b,
(b)
Dx(k+1) = ‚àí(M + N)x(k) + b,
(c)
(M + N)x(k+1) = ‚àíDx(k) + b.
[Solution : denoting respectively by œÅa, œÅb and œÅc the spectral radii of the
iteration matrices of the three methods, we have œÅa = 0.1450, œÅb = 0.5
and œÅc = 12.2870 which implies convergence for methods (a) and (b) and
divergence for method (c).]
5. For the solution of the linear system Ax = b with
A =
 1
2
2
3

,
b =
 3
5

,
consider the following iterative method
x(k+1) = B(Œ∏)x(k) + g(Œ∏),
k ‚â•0,
with x(0) given,
where Œ∏ is a real parameter and
B(Œ∏) = 1
4

2Œ∏2 + 2Œ∏ + 1
‚àí2Œ∏2 + 2Œ∏ + 1
‚àí2Œ∏2 + 2Œ∏ + 1
2Œ∏2 + 2Œ∏ + 1

,
g(Œ∏) =

1
2 ‚àíŒ∏
1
2 ‚àíŒ∏

.

180
4. Iterative Methods for Solving Linear Systems
Check that the method is consistent ‚àÄŒ∏ ‚ààR. Then, determine the values
of Œ∏ for which the method is convergent and compute the optimal value
of Œ∏ (i.e., the value of the parameter for which the convergence rate is
maximum).
[Solution : the method is convergent iÔ¨Ä‚àí1 < Œ∏ < 1/2 and the convergence
rate is maximum if Œ∏ = (1 ‚àí
‚àö
3)/2.]
6. To solve the following block linear system
 A1
B
B
A2
  x
y

=
 b1
b2

,
consider the two methods
(1)
A1x(k+1) + By(k) = b1,
Bx(k) + A2y(k+1) = b2;
(2)
A1x(k+1) + By(k) = b1,
Bx(k+1) + A2y(k+1) = b2.
Find suÔ¨Écient conditions in order for the two schemes to be convergent for
any choice of the initial data x(0), y(0).
[Solution : method (1) is a decoupled system in the unknowns x(k+1) and
y(k+1). Assuming that A1 and A2 are invertible, method (1) converges if
œÅ(A‚àí1
1 B) < 1 and œÅ(A‚àí1
2 B) < 1. In the case of method (2) we have a coupled
system to solve at each step in the unknowns x(k+1) and y(k+1). Solving
formally the Ô¨Årst equation with respect to x(k+1) (which requires A1 to be
invertible) and substituting into the second one we see that method (2) is
convergent if œÅ(A‚àí1
2 BA‚àí1
1 B) < 1 (again A2 must be invertible).]
7. Consider the linear system Ax = b with
A =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£∞
62
24
1
8
15
23
50
7
14
16
4
6
58
20
22
10
12
19
66
3
11
18
25
2
54
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
,
b =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£∞
110
110
110
110
110
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
(1) Check if the Jacobi and Gauss-Seidel methods can be applied to solve
the system. (2) Check if the stationary Richardson method with optimal
parameter can be applied with P = I and P = D, where D is the diagonal
part of A, and compute the corresponding values of Œ±opt and œÅopt.
[Solution : (1): matrix A is neither diagonally dominant nor symmetric
positive deÔ¨Ånite, so that we must compute the spectral radii of the itera-
tion matrices of the Jacobi and Gauss-Seidel methods to verify if they are
convergent. It turns out that œÅJ = 0.9280 and œÅGS = 0.3066 which implies
convergence for both methods. (2): in the case P = I all the eigenvalues
of A are positive so that the Richardson method can be applied yielding
Œ±opt = 0.015 and œÅopt = 0.6452. If P = D the method is still applicable
and Œ±opt = 0.8510, œÅopt = 0.6407.]
8. Consider the linear system Ax = b with
A =
Ô£Æ
Ô£ØÔ£ØÔ£∞
5
7
6
5
7
10
8
7
6
8
10
9
5
7
9
10
Ô£π
Ô£∫Ô£∫Ô£ª,
b =
Ô£Æ
Ô£ØÔ£ØÔ£∞
23
32
33
31
Ô£π
Ô£∫Ô£∫Ô£ª.

4.8 Exercises
181
Analyze the convergence properties of the Jacobi and Gauss-Seidel methods
applied to the system above in their point and block forms (for a 2√ó2 block
partition of A).
[Solution : both methods are convergent, the block form being the faster
one. Moreover, œÅ2(BJ) = œÅ(BGS).]
9. To solve the linear system Ax = b, consider the iterative method (4.6),
with P = D + œâF and N = ‚àíŒ≤F ‚àíE, œâ and Œ≤ being real numbers. Check
that the method is consistent only if Œ≤ = 1 ‚àíœâ. In such a case, express
the eigenvalues of the iteration matrix as a function of œâ and determine for
which values of œâ the method is convergent, as well as the value of œâopt,
assuming that A = tridiag10(‚àí1, 2, ‚àí1).
[Hint : Take advantage of the result in Exercise 3.]
10. Let A ‚ààRn√ón be such that A = (1+œâ)P‚àí(N+œâP), with P‚àí1N nonsingular
and with real eigenvalues 1 > Œª1 ‚â•Œª2 ‚â•. . . ‚â•Œªn. Find the values of œâ ‚ààR
for which the following iterative method
(1 + œâ)Px(k+1) = (N + œâP)x(k) + b,
k ‚â•0,
converges ‚àÄx(0) to the solution of the linear system (3.2). Determine also
the value of œâ for which the convergence rate is maximum.
[Solution : œâ > ‚àí(1 + Œªn)/2; œâopt = ‚àí(Œª1 + Œªn)/2.]
11. Consider the linear system
Ax = b
with A =
 3
2
2
6

,
b =

2
‚àí8

.
Write the associated functional Œ¶(x) and give a graphical interpretation of
the solution of the linear system. Perform some iterations of the gradient
method, after proving convergence for it.
12. Check that in the gradient method x(k+2) is not an optimal direction with
respect to r(k).
13. Show that the coeÔ¨Écients Œ±k and Œ≤k in the conjugate gradient method can
be written in the alternative form (4.45).
[Solution: notice Ap(k) = (r(k) ‚àír(k+1))/Œ±k and thus (Ap(k))T r(k+1) =
‚àí‚à•r(k+1)‚à•2
2/Œ±k. Moreover, Œ±k(Ap(k))T p(k) = ‚àí‚à•r(k)‚à•2
2.]
14. Prove the three-terms recursive relation (4.46) for the residual in the con-
jugate gradient method.
[Solution: subtract from both sides of Ap(k) = (r(k) ‚àír(k+1))/Œ±k the quan-
tity Œ≤k‚àí1/Œ±kr(k) and recall that Ap(k) = Ar(k) ‚àíŒ≤k‚àí1Ap(k‚àí1). Then, ex-
pressing the residual r(k) as a function of r(k‚àí1) one immediately gets the
desired relation.]

5
Approximation of Eigenvalues and
Eigenvectors
In this chapter we deal with approximations of the eigenvalues and eigen-
vectors of a matrix A ‚ààCn√ón. Two main classes of numerical methods
exist to this purpose, partial methods, which compute the extremal eigen-
values of A (that is, those having maximum and minimum module), or
global methods, which approximate the whole spectrum of A.
It is worth noting that methods which are introduced to solve the matrix
eigenvalue problem are not necessarily suitable for calculating the matrix
eigenvectors. For example, the power method (a partial method, see Section
5.3) provides an approximation to a particular eigenvalue/eigenvector pair.
The QR method (a global method, see Section 5.5) instead computes the
real Schur form of A, a canonical form that displays all the eigenvalues of
A but not its eigenvectors. These eigenvectors can be computed, starting
from the real Schur form of A, with an extra amount of work, as described
in Section 5.8.2.
Finally, some ad hoc methods for dealing eÔ¨Äectively with the special case
where A is a symmetric (n √ó n) matrix are considered in Section 5.10.
5.1
Geometrical Location of the Eigenvalues
Since the eigenvalues of A are the roots of the characteristic polynomial
pA(Œª) (see Section 1.7), iterative methods must be used for their approxi-
mation when n ‚â•5. Knowledge of eigenvalue location in the complex plane
can thus be helpful in accelerating the convergence of the process.

184
5. Approximation of Eigenvalues and Eigenvectors
A Ô¨Årst estimate is provided by Theorem 1.4,
|Œª| ‚â§‚à•A‚à•,
‚àÄŒª ‚ààœÉ(A),
(5.1)
for any consistent matrix norm ‚à•¬∑ ‚à•. Inequality (5.1), which is often quite
rough, states that all the eigenvalues of A are contained in a circle of radius
R‚à•A‚à•= ‚à•A‚à•centered at the origin of the Gauss plane.
Another result is obtained by extending the Decomposition Property 1.23
to complex-valued matrices.
Theorem 5.1 If A ‚ààCn√ón, let
H =

A + AH
/2
and
iS =

A ‚àíAH
/2
be the hermitian and skew-hermitian parts of A, respectively, i being the
imaginary unit. For any Œª ‚ààœÉ(A)
Œªmin(H) ‚â§Re(Œª) ‚â§Œªmax(H),
Œªmin(S) ‚â§Im(Œª) ‚â§Œªmax(S).
(5.2)
Proof. From the deÔ¨Ånition of H and S it follows that A = H + iS. Let u ‚ààCn,
‚à•u‚à•2 = 1, be the eigenvector associated with the eigenvalue Œª; the Rayleigh
quotient (introduced in Section 1.7) reads
Œª = uHAu = uHHu + iuHSu.
(5.3)
Notice that both H and S are hermitian matrices, whilst iS is skew-hermitian.
Matrices H and S are thus unitarily similar to a real diagonal matrix (see Section
1.7), and therefore their eigenvalues are real. In such a case, (5.3) yields
Re(Œª) = uHHu,
Im(Œª) = uHSu,
from which (5.2) follows.
3
An a priori bound for the eigenvalues of A is given by the following result.
Theorem 5.2 (of the Gershgorin circles) Let A ‚ààCn√ón. Then
œÉ(A) ‚äÜSR =
n
4
i=1
Ri,
Ri = {z ‚ààC : |z ‚àíaii| ‚â§
n

j=1
jÃ∏=i
|aij|}.
(5.4)
The sets Ri are called Gershgorin circles.
Proof. Let us decompose A as A = D + E, where D is the diagonal part of
A, whilst eii = 0 for i = 1, . . . , n. For Œª ‚ààœÉ(A) (with Œª Ã∏= aii, i = 1, . . . , n),
let us introduce the matrix BŒª = A ‚àíŒªI = (D ‚àíŒªI) + E. Since BŒª is singular,
there exists a non-null vector x ‚ààCn such that BŒªx = 0. This means that
((D ‚àíŒªI) + E) x = 0, that is, passing to the ‚à•¬∑ ‚à•‚àûnorm,
x = ‚àí(D ‚àíŒªI)‚àí1Ex,
‚à•x‚à•‚àû‚â§‚à•(D ‚àíŒªI)‚àí1E‚à•‚àû‚à•x‚à•‚àû,

5.1 Geometrical Location of the Eigenvalues
185
and thus
1 ‚â§‚à•(D ‚àíŒªI)‚àí1E‚à•‚àû=
n

j=1
|ekj|
|akk ‚àíŒª| =
n

j=1
jÃ∏=k
|akj|
|akk ‚àíŒª|
(5.5)
for a certain k, 1 ‚â§k ‚â§n. Inequality (5.5) implies Œª ‚ààRk and thus (5.4).
3
The bounds (5.4) ensure that any eigenvalue of A lies within the union
of the circles Ri. Moreover, since A and AT share the same spectrum,
Theorem 5.2 also holds in the form
œÉ(A) ‚äÜSC =
n
4
j=1
Cj,
Cj = {z ‚ààC : |z ‚àíajj| ‚â§
n

i=1
iÃ∏=j
|aij|}.
(5.6)
The circles Ri in the complex plane are called row circles, and Cj column
circles. The immediate consequence of (5.4) and (5.6) is the following.
Property 5.1 (First Gershgorin theorem) For a given matrix A ‚àà
Cn√ón,
‚àÄŒª ‚ààœÉ(A),
Œª ‚ààSR
5
SC.
(5.7)
The following two location theorems can also be proved (see [Atk89], pp.
588-590 and [Hou75], pp. 66-67).
Property 5.2 (Second Gershgorin theorem) Let
S1 =
m
4
i=1
Ri,
S2 =
n
4
i=m+1
Ri.
If S1 ‚à©S2 = ‚àÖ, then S1 contains exactly m eigenvalues of A, each one being
accounted for with its algebraic multiplicity, while the remaining eigenvalues
are contained in S2.
Remark 5.1 Properties 5.1 and 5.2 do not exclude the possibility that
there exist circles containing no eigenvalues, as happens for the matrix in
Exercise 1.
‚ñ†
DeÔ¨Ånition 5.1 A matrix A ‚ààCn√ón is called reducible if there exists a
permutation matrix P such that
PAPT =
.
B11
B12
0
B22
/
,
where B11 and B22 are square matrices; A is irreducible if it is not reducible.
‚ñ†

186
5. Approximation of Eigenvalues and Eigenvectors
To check if a matrix is reducible, the oriented graph of the matrix can be
conveniently employed. Recall from Section 3.9 that the oriented graph of a
real matrix A is obtained by joining n points (called vertices of the graph)
P1, . . . , Pn through a line oriented from Pi to Pj if the corresponding
matrix entry aij Ã∏= 0. An oriented graph is strongly connected if for any
pair of distinct vertices Pi and Pj there exists an oriented path from Pi to
Pj. The following result holds (see [Var62] for the proof).
Property 5.3 A matrix A ‚ààRn√ón is irreducible iÔ¨Äits oriented graph is
strongly connected.
Property 5.4 (Third Gershgorin theorem) Let A ‚ààCn√ón be an irre-
ducible matrix. An eigenvalue Œª ‚ààœÉ(A) cannot lie on the boundary of SR
unless it belongs to the boundary of every circle Ri, for i = 1, . . . , n.
Example 5.1 Let us consider the matrix
A =
Ô£Æ
Ô£ØÔ£ØÔ£∞
10
2
3
‚àí1
2
‚àí1
0
1
3
Ô£π
Ô£∫Ô£∫Ô£ª
whose spectrum is (to four signiÔ¨Åcant Ô¨Ågures) œÉ(A) = {9.687, 2.656¬±i0.693}. The
following values of the norm of A: ‚à•A‚à•1 = 11, ‚à•A‚à•2 = 10.72, ‚à•A‚à•‚àû= 15 and
‚à•A‚à•F = 11.36 can be used in the estimate (5.1). Estimate (5.2) provides instead
1.96 ‚â§Re(Œª(A)) ‚â§10.34, ‚àí2.34 ‚â§Im(Œª(A)) ‚â§2.34, while the row and column
circles are given respectively by R1 = {|z| : |z‚àí10| ‚â§5}, R2 = {|z| : |z‚àí2| ‚â§2},
R3 = {|z| : |z ‚àí3| ‚â§1} and C1 = {|z| : |z ‚àí10| ‚â§1}, C2 = {|z| : |z ‚àí2| ‚â§3},
C3 = {|z| : |z ‚àí3| ‚â§4}.
In Figure 5.1, for i = 1, 2, 3 the Ri and Ci circles and the intersection SR ‚à©SC
(shaded areas) are drawn. In agreement with Property 5.2, we notice that an
eigenvalue is contained in C1, which is disjoint from C2 and C3, while the remaining
eigenvalues, thanks to Property 5.1, lie within the set R2 ‚à™{C3 ‚à©R1}.
‚Ä¢
5.2
Stability and Conditioning Analysis
In this section we introduce some a priori and a posteriori estimates that
are relevant in the stability analysis of the matrix eigenvalue and eigenvec-
tor problem. The presentation follows the guidelines that have been traced
in Chapter 2.
5.2.1
A priori Estimates
Assume that A ‚ààCn√ón is a diagonalizable matrix and denote by X =
(x1, . . . , xn) ‚ààCn√ón the matrix of its right eigenvectors, where xk ‚ààCn

5.2 Stability and Conditioning Analysis
187
2
Re(z)
3
10
C3
C2
C1
R3
R2
R1
Im(z)
FIGURE 5.1. Row and column circles for matrix A in Example 5.1
for k = 1, . . . , n, such that D = X‚àí1AX = diag(Œª1, . . . , Œªn), Œªi being the
eigenvalues of A, i = 1, . . . , n. Moreover, let E ‚ààCn√ón be a perturbation
of A. The following theorem holds.
Theorem 5.3 (Bauer-Fike) Let ¬µ be an eigenvalue of the matrix A+E ‚àà
Cn√ón; then
min
Œª‚ààœÉ(A) |Œª ‚àí¬µ| ‚â§Kp(X)‚à•E‚à•p
(5.8)
where ‚à•¬∑ ‚à•p is any matrix p-norm and Kp(X) = ‚à•X‚à•p‚à•X‚àí1‚à•p is called the
condition number of the eigenvalue problem for matrix A.
Proof. We Ô¨Årst notice that if ¬µ ‚ààœÉ(A) then (5.8) is trivially veriÔ¨Åed, since
‚à•X‚à•p‚à•X‚àí1‚à•p‚à•E‚à•p ‚â•0. Let us thus assume henceforth that ¬µ Ã∏‚ààœÉ(A). From the
deÔ¨Ånition of eigenvalue it follows that matrix (A+E‚àí¬µI) is singular, which means
that, since X is invertible, the matrix X‚àí1(A + E ‚àí¬µI)X = D + X‚àí1EX ‚àí¬µI is
singular. Therefore, there exists a non-null vector x ‚ààCn such that

(D ‚àí¬µI) + X‚àí1EX

x = 0.
Since ¬µ Ã∏‚ààœÉ(A), the diagonal matrix (D ‚àí¬µI) is invertible and the previous
equation can be written in the form

I + (D ‚àí¬µI)‚àí1(X‚àí1EX)

x = 0.
Passing to the ‚à•¬∑ ‚à•p norm and proceeding as in the proof of Theorem 5.2, we get
1 ‚â§‚à•(D ‚àí¬µI)‚àí1‚à•pKp(X)‚à•E‚à•p,
from which the estimate (5.8) follows, since
‚à•(D ‚àí¬µI)‚àí1‚à•p = ( min
Œª‚ààœÉ(A) |Œª ‚àí¬µ|)‚àí1.
3

188
5. Approximation of Eigenvalues and Eigenvectors
If A is a normal matrix, from the Schur decomposition theorem (see Section
1.8) it follows that the similarity transformation matrix X is unitary so that
Kp(X) = 1. This implies that
‚àÄ¬µ ‚ààœÉ(A + E),
min
Œª‚ààœÉ(A) |Œª ‚àí¬µ| ‚â§‚à•E‚à•p,
(5.9)
hence the eigenvalue problem is well-conditioned with respect to the abso-
lute error. This, however, does not prevent the matrix eigenvalue problem
from being aÔ¨Äected by signiÔ¨Åcant relative errors, especially when A has a
widely spread spectrum.
Example 5.2 Let us consider, for 1 ‚â§n ‚â§10, the calculation of the eigenvalues
of the Hilbert matrix Hn ‚ààRn√ón (see Example 3.2, Chapter 3). It is symmetric
(thus, in particular, normal) and exhibits, for n ‚â•4, a very large condition
number. Let En ‚ààRn√ón be a matrix having constant entries equal to Œ∑ = 10‚àí3.
We show in Table 5.1 the results of the computation of the minimum in (5.9),
taking p = 2 (that is, ‚à•En‚à•2 = nŒ∑). Notice how the absolute error is decreasing,
since the eigenvalue of minimum module tends to zero, whilst the relative error
is increasing as the size n of the matrix increases, due to the higher sensitivity of
‚Äúsmall‚Äù eigenvalues with respect to rounding errors.
‚Ä¢
n
Abs. Err.
Rel. Err.
‚à•En‚à•2
K2(Hn)
K2(Hn + En)
1
1 ¬∑ 10‚àí3
1 ¬∑ 10‚àí3
1 ¬∑ 10‚àí3
1 ¬∑ 10‚àí3
1
2
1.677 ¬∑ 10‚àí4
1.446 ¬∑ 10‚àí3
2 ¬∑ 10‚àí3
19.28
19.26
4
5.080 ¬∑ 10‚àí7
2.207 ¬∑ 10‚àí3
4 ¬∑ 10‚àí3
1.551 ¬∑ 104
1.547 ¬∑ 104
8
1.156 ¬∑ 10‚àí12
3.496 ¬∑ 10‚àí3
8 ¬∑ 10‚àí3
1.526 ¬∑ 1010
1.515 ¬∑ 1010
10
1.355 ¬∑ 10‚àí15
4.078 ¬∑ 10‚àí3
1 ¬∑ 10‚àí2
1.603 ¬∑ 1013
1.589 ¬∑ 1013
TABLE 5.1. Relative and absolute errors in the calculation of the eigenvalues of
the Hilbert matrix (using the MATLAB intrinsic function eig). ‚ÄúAbs. Err.‚Äù and
‚ÄúRel. Err.‚Äù denote respectively the absolute and relative errors (with respect to
Œª)
The Bauer-Fike theorem states that the matrix eigenvalue problem is well-
conditioned if A is a normal matrix. Failure to fulÔ¨Ål this property, however,
does not necessarily imply that A must exhibit a ‚Äústrong‚Äù numerical sen-
sitivity to the computation of every one of its eigenvalues. In this respect,
the following result holds, which can be regarded as an a priori estimate of
the conditioning of the calculation of a particular eigenvalue of a matrix.
Theorem 5.4 Let A ‚ààCn√ón be a diagonalizable matrix; let Œª, x and y
be a simple eigenvalue of A and its associated right and left eigenvectors,
respectively, with ‚à•x‚à•2 = ‚à•y‚à•2 = 1. Moreover, for Œµ > 0, let A(Œµ) =
A + ŒµE, with E ‚ààCn√ón such that ‚à•E‚à•2 = 1. Denoting by Œª(Œµ) and x(Œµ) the

5.2 Stability and Conditioning Analysis
189
eigenvalue and the corresponding eigenvector of A(Œµ), such that Œª(0) = Œª
and x(0) = x,

‚àÇŒª
‚àÇŒµ (0)
 ‚â§
1
|yHx|.
(5.10)
Proof. Let us Ô¨Årst prove that yHx Ã∏= 0. Setting Y = (y1, . . . , yn) = (XH)‚àí1,
with yk ‚ààCn for k = 1, . . . , n, it follows that yH
k A = ŒªkyH
k , i.e., the rows
of X‚àí1 = YH are left eigenvectors of A. Then, since YHX = I, yH
i xj = Œ¥ij
for i, j = 1, . . . , n, Œ¥ij being the Kronecker symbol. This result is equivalent to
saying that the eigenvectors {x} of A and the eigenvectors {y} of AH form a
bi-orthogonal set (see (4.67)).
Let us now prove (5.10). Since the roots of the characteristic equation are
continuous functions of the coeÔ¨Écients of the characteristic polynomial associated
with A(Œµ), it follows that the eigenvalues of A(Œµ) are continuous functions of Œµ
(see, for instance, [Hen74], p. 281). Therefore, in a neighborhood of Œµ = 0,
(A + ŒµE)x(Œµ) = Œª(Œµ)x(Œµ).
DiÔ¨Äerentiating the previous equation with respect to Œµ and setting Œµ = 0 yields
A‚àÇx
‚àÇŒµ (0) + Ex = ‚àÇŒª
‚àÇŒµ (0)x + Œª‚àÇx
‚àÇŒµ (0),
from which, left-multiplying both sides by yH and recalling that yH is a left
eigenvector of A,
‚àÇŒª
‚àÇŒµ (0) = yHEx
yHx .
Using the Cauchy-Schwarz inequality gives the desired estimate (5.10).
3
Notice that |yHx| = | cos(Œ∏Œª)|, where Œ∏Œª is the angle between the eigenvec-
tors yH and x (both having unit Euclidean norm). Therefore, if these two
vectors are almost orthogonal the computation of the eigenvalue Œª turns
out to be ill-conditioned. The quantity
Œ∫(Œª) =
1
|yHx| =
1
| cos(Œ∏Œª)|
(5.11)
can thus be taken as the condition number of the eigenvalue Œª. Obviously,
Œ∫(Œª) ‚â•1; when A is a normal matrix, since it is unitarily similar to a
diagonal matrix, the left and right eigenvectors y and x coincide, yielding
Œ∫(Œª) = 1/‚à•x‚à•2
2 = 1.
Inequality (5.10) can be roughly interpreted as stating that perturbations
of the order of Œ¥Œµ in the entries of matrix A induce changes of the order of
Œ¥Œª = Œ¥Œµ/| cos(Œ∏Œª)| in the eigenvalue Œª. If normal matrices are considered,
the calculation of Œª is a well-conditioned problem; the case of a generic non-
symmetric matrix A can be conveniently dealt with using methods based
on similarity transformations, as will be seen in later sections.

190
5. Approximation of Eigenvalues and Eigenvectors
It is interesting to check that the conditioning of the matrix eigenvalue
problem remains unchanged if the transformation matrices are unitary. To
this end, let U ‚ààCn√ón be a unitary matrix and let $A = UHAU. Also let
Œªj be an eigenvalue of A and denote by Œ∫j the condition number (5.11).
Moreover, let $Œ∫j be the condition number of Œªj when it is regarded as an
eigenvalue of $A. Finally, let {xk}, {yk} be the right and left eigenvectors of
A respectively. Clearly, {UHxk}, {UHyk} are the right and left eigenvectors
of $A. Thus, for any j = 1, . . . , n,
$Œ∫j =
yH
j UUHxj
‚àí1 = Œ∫j,
from which it follows that the stability of the computation of Œªj is not
aÔ¨Äected by performing similarity transformations using unitary matrices.
It can also be checked that unitary transformation matrices do not change
the Euclidean length and the angles between vectors in Cn. Moreover, the
following a priori estimate holds (see [GL89], p. 317)
fl

X‚àí1AX

= X‚àí1AX + E,
with ‚à•E‚à•2 ‚âÉuK2(X)‚à•A‚à•2
(5.12)
where fl(M) is the machine representation of matrix M and u is the roundoÔ¨Ä
unit (see Section 2.5). From (5.12) it follows that using nonunitary trans-
formation matrices in the eigenvalue computation can lead to an unstable
process with respect to rounding errors.
We conclude this section with a stability result for the approximation of
the eigenvector associated with a simple eigenvalue. Under the same as-
sumptions of Theorem 5.4, the following result holds (see for the proof,
[Atk89], Problem 6, pp. 649-650).
Property 5.5 The eigenvectors xk and xk(Œµ) of the matrices A and A(Œµ) =
A + ŒµE, with ‚à•xk(Œµ)‚à•2 = ‚à•xk‚à•2 = 1 for k = 1, . . . , n, satisfy
‚à•xk(Œµ) ‚àíxk‚à•2 ‚â§
Œµ
minjÃ∏=k |Œªk ‚àíŒªj|‚à•E‚à•2 + O(Œµ2),
‚àÄk = 1, . . . , n.
Analogous to (5.11), the quantity
Œ∫(xk) =
1
minjÃ∏=k |Œªk ‚àíŒªj|
can be regarded as being the condition number of the eigenvector xk. Com-
puting xk might be an ill-conditioned operation if some eigenvalues Œªj are
‚Äúvery close‚Äù to the eigenvalue Œªk associated with xk.
5.2.2
A posteriori Estimates
The a priori estimates examined in the previous section characterize the
stability properties of the matrix eigenvalue and eigenvector problem. From

5.2 Stability and Conditioning Analysis
191
the implementation standpoint, it is also important to dispose of a pos-
teriori estimates that allow for a run-time control of the quality of the
approximation that is being constructed. Since the methods that will be
considered later are iterative processes, the results of this section can be
usefully employed to devise reliable stopping criteria for these latter.
Theorem 5.5 Let A ‚ààCn√ón be an hermitian matrix and let (Œª, x) be
the computed approximations of an eigenvalue/eigenvector pair (Œª, x) of
A. DeÔ¨Åning the residual as
r = Ax ‚àíŒªx,
x Ã∏= 0,
it then follows that
min
Œªi‚ààœÉ(A) |Œª ‚àíŒªi| ‚â§‚à•r‚à•2
‚à•x‚à•2
.
(5.13)
Proof. Since A is hermitian, it admits a system of orthonormal eigenvectors {uk}
which can be taken as a basis of Cn. In particular, x =
n

i=1
Œ±iui with Œ±i = uH
i x,
and thus r =
n

i=1
Œ±i(Œªi ‚àíŒª)ui. As a consequence
 ‚à•r‚à•2
‚à•x‚à•2
2
=
n

i=1
Œ≤i(Œªi ‚àíŒª)2,
with Œ≤i = |Œ±k|2/(
n

j=1
|Œ±j|2).
(5.14)
Since
n

i=1
Œ≤i = 1, the inequality (5.13) immediately follows from (5.14).
3
The estimate (5.13) ensures that a small absolute error corresponds to a
small relative residual in the computation of the eigenvalue of the matrix
A which is closest to Œª.
Let us now consider the following a posteriori estimate for the eigenvector
x (for the proof, see [IK66], pp. 142-143).
Property 5.6 Under the same assumptions of Theorem 5.5, suppose that
|Œªi ‚àíŒª| ‚â§‚à•r‚à•2 for i = 1, . . . , m and that |Œªi ‚àíŒª| ‚â•Œ¥ > 0 for i =
m + 1, . . . , n. Then
d(x, Um) ‚â§‚à•r‚à•2
Œ¥
(5.15)
where d(x, Um) is the Euclidean distance between x and the space Um gen-
erated by the eigenvectors ui, i = 1, . . . , m associated with the eigenvalues
Œªi of A.

192
5. Approximation of Eigenvalues and Eigenvectors
Notice that the a posteriori estimate (5.15) ensures that a small absolute
error corresponds to a small residual in the approximation of the eigenvec-
tor associated with the eigenvalue of A that is closest to Œª, provided that
the eigenvalues of A are well-separated (that is, if Œ¥ is suÔ¨Éciently large).
In the general case of a nonhermitian matrix A, an a posteriori estimate
can be given for the eigenvalue Œª only when the matrix of the eigenvectors
of A is available. We have the following result (for the proof, we refer to
[IK66], p. 146).
Property 5.7 Let A ‚ààCn√ón be a diagonalizable matrix, with matrix of
eigenvectors X = [x1, . . . , xn]. If, for some Œµ > 0,
‚à•r‚à•2 ‚â§Œµ‚à•x‚à•2,
then
min
Œªi‚ààœÉ(A) |Œª ‚àíŒªi| ‚â§Œµ‚à•X‚àí1‚à•2‚à•X‚à•2.
This estimate is of little practical use, since it requires the knowledge of all
the eigenvectors of A. Examples of a posteriori estimates that can actually
be implemented in a numerical algorithm will be provided in Sections 5.3.1
and 5.3.2.
5.3
The Power Method
The power method is very good at approximating the extremal eigenvalues
of the matrix, that is, the eigenvalues having largest and smallest module,
denoted by Œª1 and Œªn respectively, as well as their associated eigenvectors.
Solving such a problem is of great interest in several real-life applications
(geosysmic, machine and structural vibrations, electric network analysis,
quantum mechanics, . . . ) where the computation of Œªn (and its associated
eigenvector xn) arises in the determination of the proper frequency (and
the corresponding fundamental mode) of a given physical system. We shall
come back to this point in Section 5.12.
Having approximations of Œª1 and Œªn can also be useful in the analysis of
numerical methods. For instance, if A is symmetric and positive deÔ¨Ånite,
one can compute the optimal value of the acceleration parameter of the
Richardson method and estimate its error reducing factor (see Chapter
4), as well as perform the stability analysis of discretization methods for
systems of ordinary diÔ¨Äerential equations (see Chapter 11).
5.3.1
Approximation of the Eigenvalue of Largest Module
Let A ‚ààCn√ón be a diagonalizable matrix and let X ‚ààCn√ón be the matrix of
its eigenvectors xi, for i = 1, . . . , n. Let us also suppose that the eigenvalues

5.3 The Power Method
193
of A are ordered as
|Œª1| > |Œª2| ‚â•|Œª3| . . . ‚â•|Œªn|,
(5.16)
where Œª1 has algebraic multiplicity equal to 1. Under these assumptions,
Œª1 is called the dominant eigenvalue of matrix A.
Given an arbitrary initial vector q(0) ‚ààCn of unit Euclidean norm, consider
for k = 1, 2, . . . the following iteration based on the computation of powers
of matrices, commonly known as the power method:
z(k) = Aq(k‚àí1)
q(k) = z(k)/‚à•z(k)‚à•2
ŒΩ(k) = (q(k))HAq(k).
(5.17)
Let us analyze the convergence properties of method (5.17). By induction
on k one can check that
q(k) =
Akq(0)
‚à•Akq(0)‚à•2
,
k ‚â•1.
(5.18)
This relation explains the role played by the powers of A in the method.
Because A is diagonalizable, its eigenvectors xi form a basis of Cn; it is
thus possible to represent q(0) as
q(0) =
n

i=1
Œ±ixi,
Œ±i ‚ààC,
i = 1, . . . , n.
(5.19)
Moreover, since Axi = Œªixi, we have
Akq(0) = Œ±1Œªk
1

x1 +
n

i=2
Œ±i
Œ±1
 Œªi
Œª1
k
xi

, k = 1, 2, . . .
(5.20)
Since |Œªi/Œª1| < 1 for i = 2, . . . , n, as k increases the vector Akq(0) (and
thus also q(k), due to (5.18)), tends to assume an increasingly signiÔ¨Åcant
component in the direction of the eigenvector x1, while its components in
the other directions xj decrease. Using (5.18) and (5.20), we get
q(k) =
Œ±1Œªk
1(x1 + y(k))
‚à•Œ±1Œªk
1(x1 + y(k))‚à•2
= ¬µk
x1 + y(k)
‚à•x1 + y(k)‚à•2
,
where ¬µk is the sign of Œ±1Œªk
1 and y(k) denotes a vector that vanishes as
k ‚Üí‚àû.
As k ‚Üí‚àû, the vector q(k) thus aligns itself along the direction of eigen-
vector x1, and the following error estimate holds at each step k.

194
5. Approximation of Eigenvalues and Eigenvectors
Theorem 5.6 Let A ‚ààCn√ón be a diagonalizable matrix whose eigenvalues
satisfy (5.16). Assuming that Œ±1 Ã∏= 0, there exists a constant C > 0 such
that
‚à•Àúq(k) ‚àíx1‚à•2 ‚â§C

Œª2
Œª1

k
,
k ‚â•1,
(5.21)
where
Àúq(k) = q(k)‚à•Akq(0)‚à•2
Œ±1Œªk
1
= x1 +
n

i=2
Œ±i
Œ±1
 Œªi
Œª1
k
xi,
k = 1, 2, . . .
(5.22)
Proof. Since A is diagonalizable, without losing generality, we can pick up the
nonsingular matrix X in such a way that its columns have unit Euclidean length,
that is ‚à•xi‚à•2 = 1 for i = 1, . . . , n. From (5.20) it thus follows that
‚à•x1 +
n

i=2
.
Œ±i
Œ±1
 Œªi
Œª1
k
xi
/
‚àíx1‚à•2 = ‚à•
n

i=2
Œ±i
Œ±1
 Œªi
Œª1
k
xi‚à•2
‚â§
 n

i=2
 Œ±i
Œ±1
2  Œªi
Œª1
2k1/2
‚â§

Œª2
Œª1

k  n

i=2
 Œ±i
Œ±1
21/2
,
that is (5.21) with C =
 n

i=2
(Œ±i/Œ±1)2
1/2
.
3
Estimate (5.21) expresses the convergence of the sequence Àúq(k) towards x1.
Therefore the sequence of Rayleigh quotients
((Àúq(k))HAÀúq(k))/‚à•Àúq(k)‚à•2
2 =
+
q(k),H
Aq(k) = ŒΩ(k)
will converge to Œª1. As a consequence, limk‚Üí‚àûŒΩ(k) = Œª1, and the conver-
gence will be faster when the ratio |Œª2/Œª1| is smaller.
If the matrix A is real and symmetric it can be proved, always assuming
that Œ±1 Ã∏= 0, that (see [GL89], pp. 406-407)
|Œª1 ‚àíŒΩ(k)| ‚â§|Œª1 ‚àíŒªn| tan2(Œ∏0)

Œª2
Œª1

2k
,
(5.23)
where cos(Œ∏0) = |xT
1 q(0)| Ã∏= 0. Inequality (5.23) outlines that the conver-
gence of the sequence ŒΩ(k) to Œª1 is quadratic with respect to the ratio |Œª2/Œª1|
(we refer to Section 5.3.3 for numerical results).
We conclude the section by providing a stopping criterion for the iteration
(5.17). For this purpose, let us introduce the residual at step k
r(k) = Aq(k) ‚àíŒΩ(k)q(k),
k ‚â•1,
and, for Œµ > 0, the matrix ŒµE(k) = ‚àír(k) 0
q(k)1H ‚ààCn√ón with ‚à•E(k)‚à•2 = 1.
Since
ŒµE(k)q(k) = ‚àír(k),
k ‚â•1,
(5.24)

5.3 The Power Method
195
we obtain

A + ŒµE(k)
q(k) = ŒΩ(k)q(k). As a result, at each step of the
power method ŒΩ(k) is an eigenvalue of the perturbed matrix A + ŒµE(k).
From (5.24) and from deÔ¨Ånition (1.20) it also follows that Œµ = ‚à•r(k)‚à•2 for
k = 1, 2, . . . . Plugging this identity back into (5.10) and approximating the
partial derivative in (5.10) by the incremental ratio |Œª1 ‚àíŒΩ(k)|/Œµ, we get
|Œª1 ‚àíŒΩ(k)| ‚âÉ‚à•r(k)‚à•2
| cos(Œ∏Œª)|,
k ‚â•1,
(5.25)
where Œ∏Œª is the angle between the right and the left eigenvectors, x1 and y1,
associated with Œª1. Notice that, if A is an hermitian matrix, then cos(Œ∏Œª) =
1, so that (5.25) yields an estimate which is analogue to (5.13).
In practice, in order to employ the estimate (5.25) it is necessary at each
step k to replace | cos(Œ∏Œª)| with the module of the scalar product between
two approximations q(k) and w(k) of x1 and y1, computed by the power
method. The following a posteriori estimate is thus obtained
|Œª1 ‚àíŒΩ(k)| ‚âÉ
‚à•r(k)‚à•2
|(w(k))Hq(k)|,
k ‚â•1.
(5.26)
Examples of applications of (5.26) will be provided in Section 5.3.3.
5.3.2
Inverse Iteration
In this section we look for an approximation of the eigenvalue of a matrix
A ‚ààCn√ón which is closest to a given number ¬µ ‚ààC, where ¬µ Ã∏‚ààœÉ(A).
For this, the power iteration (5.17) can be applied to the matrix (M¬µ)‚àí1 =
(A ‚àí¬µI)‚àí1, yielding the so-called inverse iteration or inverse power method.
The number ¬µ is called a shift.
The eigenvalues of M‚àí1
¬µ
are Œæi = (Œªi ‚àí¬µ)‚àí1; let us assume that there
exists an integer m such that
|Œªm ‚àí¬µ| < |Œªi ‚àí¬µ|,
‚àÄi = 1, . . . , n
and i Ã∏= m.
(5.27)
This amounts to requiring that the eigenvalue Œªm which is closest to ¬µ has
multiplicity equal to 1. Moreover, (5.27) shows that Œæm is the eigenvalue of
M‚àí1
¬µ
with largest module; in particular, if ¬µ = 0, Œªm turns out to be the
eigenvalue of A with smallest module.
Given an arbitrary initial vector q(0) ‚ààCn of unit Euclidean norm, for
k = 1, 2, . . . the following sequence is constructed:
(A ‚àí¬µI) z(k) = q(k‚àí1)
q(k) = z(k)/‚à•z(k)‚à•2
œÉ(k) = (q(k))HAq(k).
(5.28)

196
5. Approximation of Eigenvalues and Eigenvectors
Notice that the eigenvectors of M¬µ are the same as those of A since M¬µ =
X (Œõ ‚àí¬µIn) X‚àí1, where Œõ = diag(Œª1, . . . , Œªn). For this reason, the Rayleigh
quotient in (5.28) is computed directly on the matrix A (and not on M‚àí1
¬µ ).
The main diÔ¨Äerence with respect to (5.17) is that at each step k a linear
system with coeÔ¨Écient matrix M¬µ = A ‚àí¬µI must be solved. For numerical
convenience, the LU factorization of M¬µ is computed once for all at k = 1,
so that at each step only two triangular systems are to be solved, with a
cost of the order of n2 Ô¨Çops.
Although being more computationally expensive than the power method
(5.17), the inverse iteration has the advantage that it can converge to any
desired eigenvalue of A (namely, the one closest to the shift ¬µ). Inverse iter-
ation is thus ideally suited for reÔ¨Åning an initial estimate ¬µ of an eigenvalue
of A, which can be obtained, for instance, by applying the localization tech-
niques introduced in Section 5.1. Inverse iteration can be also eÔ¨Äectively
employed to compute the eigenvector associated with a given (approximate)
eigenvalue, as described in Section 5.8.1.
In view of the convergence analysis of the iteration (5.28) we assume that
A is diagonalizable, so that q(0) can be represented in the form (5.19).
Proceeding in the same way as in the power method, we let
Àúq(k) = xm +
n

i=1,iÃ∏=m
Œ±i
Œ±m
 Œæi
Œæm
k
xi,
where xi are the eigenvectors of M‚àí1
¬µ
(and thus also of A), while Œ±i are as
in (5.19). As a consequence, recalling the deÔ¨Ånition of Œæi and using (5.27),
we get
lim
k‚Üí‚àûÀúq(k) = xm,
lim
k‚Üí‚àûœÉ(k) = Œªm.
Convergence will be faster when ¬µ is closer to Œªm. Under the same assump-
tions made for proving (5.26), the following a posteriori estimate can be
obtained for the approximation error on Œªm
|Œªm ‚àíœÉ(k)| ‚âÉ
‚à•r(k)‚à•2
|(w(k))Hq(k)|,
k ‚â•1,
(5.29)
where r(k) = Aq(k) ‚àíœÉ(k)q(k) and w(k) is the k-th iterate of the inverse
power method to approximate the left eigenvector associated with Œªm.
5.3.3
Implementation Issues
The convergence analysis of Section 5.3.1 shows that the eÔ¨Äectiveness of
the power method strongly depends on the dominant eigenvalues being
well-separated (that is, |Œª2|/|Œª1| ‚â™1). Let us now analyze the behavior of
iteration (5.17) when two dominant eigenvalues of equal module exist (that
is, |Œª2| = |Œª1|). Three cases must be distinguished:

5.3 The Power Method
197
1. Œª2 = Œª1: the two dominant eigenvalues are coincident. The method
is still convergent, since for k suÔ¨Éciently large (5.20) yields
Akq(0) ‚âÉŒªk
1 (Œ±1x1 + Œ±2x2)
which is an eigenvector of A. For k ‚Üí‚àû, the sequence Àúq(k) (after
a suitable redeÔ¨Ånition) converges to a vector lying in the subspace
spanned by the eigenvectors x1 and x2, while the sequence ŒΩ(k) still
converges to Œª1.
2. Œª2 = ‚àíŒª1: the two dominant eigenvalues are opposite. In this case
the eigenvalue of largest module can be approximated by applying the
power method to the matrix A2. Indeed, for i = 1, . . . , n, Œªi(A2) =
[Œªi(A)]2, so that Œª2
1 = Œª2
2 and the analysis falls into the previous case,
where the matrix is now A2.
3. Œª2 = Œª1: the two dominant eigenvalues are complex conjugate. Here,
undamped oscillations arise in the sequence of vectors q(k) and the
power method is not convergent (see [Wil65], Chapter 9, Section 12).
As for the computer implementation of (5.17), it is worth noting that nor-
malizing the vector q(k) to 1 keeps away from overÔ¨Çow (when |Œª1| > 1) or
underÔ¨Çow (when |Œª1| < 1) in (5.20). We also point out that the requirement
Œ±1 Ã∏= 0 (which is a priori impossible to fulÔ¨Ål when no information about
the eigenvector x1 is available) is not essential for the actual convergence
of the algorithm.
Indeed, although it can be proved that, working in exact arithmetic, the
sequence (5.17) converges to the pair (Œª2, x2) if Œ±1 = 0 (see Exercise 10),
the arising of (unavoidable) rounding errors ensures that in practice the
vector q(k) contains a non-null component also in the direction of x1. This
allows for the eigenvalue Œª1 to ‚Äúshow-up‚Äù and the power method to quickly
converge to it.
An implementation of the power method is given in Program 26. Here
and in the following algorithm, the convergence check is based on the a
posteriori estimate (5.26).
Here and in the remainder of the chapter, the input data z0, toll and
nmax are the initial vector, the tolerance for the stopping test and the
maximum admissible number of iterations, respectively. In output, the vec-
tors nu1 and err contain the sequences {ŒΩ(k)} and {‚à•r(k)‚à•2/| cos(Œ∏Œª)|} (see
(5.26)), whilst x1 and niter are the approximation of the eigenvector x1
and the number of iterations taken by the algorithm to converge, respec-
tively.
Program 26 - powerm : Power method
function [nu1,x1,niter,err]=powerm(A,z0,toll,nmax)

198
5. Approximation of Eigenvalues and Eigenvectors
q=z0/norm(z0); q2=q; err=[]; nu1=[]; res=toll+1; niter=0; z=A*q;
while (res >= toll & niter <= nmax)
q=z/norm(z); z=A*q; lam=q‚Äô*z; x1=q;
z2=q2‚Äô*A; q2=z2/norm(z2); q2=q2‚Äô;
y1=q2; costheta=abs(y1‚Äô*x1);
if (costheta >= 5e-2),
niter=niter+1; res=norm(z-lam*q)/costheta;
err=[err; res]; nu1=[nu1; lam];
else
disp(‚Äô Multiple eigenvalue ‚Äô); break;
end
end
A coding of the inverse power method is provided in Program 27. The
input parameter mu is the initial approximation of the eigenvalue. In output,
the vectors sigma and err contain the sequences {œÉ(k)} and

‚à•r(k)‚à•2/|(w(k))H
q(k)|

(see (5.29)). The LU factorization (with partial pivoting) of the ma-
trix M¬µ is carried out using the MATLAB intrinsic function lu.
Program 27 - invpower : Inverse power method
function [sigma,x,niter,err]=invpower(A,z0,mu,toll,nmax)
n=max(size(A)); M=A-mu*eye(n); [L,U,P]=lu(M);
q=z0/norm(z0); q2=q‚Äô; err=[]; sigma=[]; res=toll+1; niter=0;
while (res >= toll & niter <= nmax)
niter=niter+1; b=P*q; y=L\b; z=U\y;
q=z/norm(z); z=A*q; lam=q‚Äô*z;
b=q2‚Äô; y=U‚Äô\b; w=L‚Äô\y;
q2=(P‚Äô*w)‚Äô; q2=q2/norm(q2); costheta=abs(q2*q);
if (costheta >= 5e-2),
res=norm(z-lam*q)/costheta; err=[err; res]; sigma=[sigma; lam];
else,
disp(‚Äô Multiple eigenvalue ‚Äô); break;
end
x=q;
end
Example 5.3 The matrix A in (5.30)
A =
Ô£Æ
Ô£ØÔ£ØÔ£∞
15
‚àí2
2
1
10
‚àí3
‚àí2
1
0
Ô£π
Ô£∫Ô£∫Ô£ª,
V =
Ô£Æ
Ô£ØÔ£ØÔ£∞
‚àí0.944
0.393
‚àí0.088
‚àí0.312
0.919
0.309
0.112
0.013
0.947
Ô£π
Ô£∫Ô£∫Ô£ª
(5.30)
has the following eigenvalues (to Ô¨Åve signiÔ¨Åcant Ô¨Ågures): Œª1 = 14.103, Œª2 = 10.385
and Œª3 = 0.512, while the corresponding eigenvectors are the vector columns of
matrix V.

5.3 The Power Method
199
0
10
20
30
40
50
60
70
80
10
‚àí12
10
‚àí10
10
‚àí8
10
‚àí6
10
‚àí4
10
‚àí2
10
0
0
6
12
18
10
‚àí14
10
‚àí12
10
‚àí10
10
‚àí8
10
‚àí6
10
‚àí4
10
‚àí2
10
0
(S)
(NS)
FIGURE 5.2. Comparison between the a posteriori error estimate and the actual
absolute error for matrix A in (5.30) (left); convergence curves for the power
method applied to matrix A in (5.31) in its symmetric (S) and nonsymmetric
(NS) forms (right)
To approximate the pair (Œª1, x1), we have run the Program 26 with initial datum
z(0) = (1, 1, 1)T . After 71 iterations of the power method the absolute errors are
|Œª1 ‚àíŒΩ(71)| = 7.91 ¬∑ 10‚àí11 and ‚à•x1 ‚àíx(71)
1
‚à•‚àû= 1.42 ¬∑ 10‚àí11.
In a second run, we have used z(0) = x2 + x3 (notice that with this choice
Œ±1 = 0). After 215 iterations the absolute errors are |Œª1 ‚àíŒΩ(215)| = 4.26 ¬∑ 10‚àí14
and ‚à•x1 ‚àíx(215)
1
‚à•‚àû= 1.38 ¬∑ 10‚àí14.
Figure 5.2 (left) shows the reliability of the a posteriori estimate (5.26). The
sequences |Œª1 ‚àíŒΩ(k)| (solid line) and the corresponding a posteriori estimates
(5.26) (dashed line) are plotted as a function of the number of iterations (in
abscissae). Notice the excellent agreement between the two curves.
The symmetric matrix A in (5.31)
A =
Ô£Æ
Ô£ØÔ£ØÔ£∞
1
3
4
3
1
2
4
2
1
Ô£π
Ô£∫Ô£∫Ô£ª,
T =
Ô£Æ
Ô£ØÔ£ØÔ£∞
8
1
6
3
5
7
4
9
2
Ô£π
Ô£∫Ô£∫Ô£ª
(5.31)
has the following spectrum: Œª1 = 7.047, Œª2 = ‚àí3.1879 and Œª3 = ‚àí0.8868 (to Ô¨Åve
signiÔ¨Åcant Ô¨Ågures).
It is interesting to compare the behaviour of the power method when computing
Œª1 for the symmetric matrix A and for its similar matrix M = T‚àí1AT, where T
is the nonsingular (and nonorthogonal) matrix in (5.31).
Running Program 26 with z(0) = (1, 1, 1)T , the power method converges to the
eigenvalue Œª1 in 18 and 30 iterations, for matrices A and M, respectively. The
sequence of absolute errors |Œª1 ‚àíŒΩ(k)| is plotted in Figure 5.2 (right) where (S)
and (NS) refer to the computations on A and M, respectively. Notice the rapid
error reduction in the symmetric case, according to the quadratic convergence
properties of the power method (see Section 5.3.1).
We Ô¨Ånally employ the inverse power method (5.28) to compute the eigenvalue
of smallest module Œª3 = 0.512 of matrix A in (5.30). Running Program 27 with
q(0) = (1, 1, 1)T /
‚àö
3, the method converges in 9 iterations, with absolute errors
|Œª3 ‚àíœÉ(9)| = 1.194 ¬∑ 10‚àí12 and ‚à•x3 ‚àíx(9)
3 ‚à•‚àû= 4.59 ¬∑ 10‚àí13.
‚Ä¢

200
5. Approximation of Eigenvalues and Eigenvectors
5.4
The QR Iteration
In this section we present some iterative techniques for simultaneously ap-
proximating all the eigenvalues of a given matrix A. The basic idea consists
of reducing A, by means of suitable similarity transformations, into a form
for which the calculation of the eigenvalues is easier than on the starting
matrix.
The problem would be satisfactorily solved if the unitary matrix U of the
Schur decomposition theorem 1.5, such that T = UHAU, T being upper
triangular and with tii = Œªi(A) for i = 1, . . . , n, could be determined in a
direct way, that is, with a Ô¨Ånite number of operations. Unfortunately, it is
a consequence of Abel‚Äôs theorem that, for n ‚â•5, the matrix U cannot be
computed in an elementary way (see Exercise 8). Thus, our problem can
be solved only resorting to iterative techniques.
The reference algorithm in this context is the QR iteration method, that is
here examined only in the case of real matrices. (For some remarks on the
extension of the algorithms to the complex case, see [GL89], Section 5.2.10
and [Dem97], Section 4.2.1).
Let A ‚ààRn√ón; given an orthogonal matrix Q(0) ‚ààRn√ón and letting
T(0) = (Q(0))T AQ(0), for k = 1, 2, . . . , until convergence, the QR iteration
consists of:
determine Q(k), R(k) such that
Q(k)R(k) = T(k‚àí1)
(QR factorization);
then, let
T(k) = R(k)Q(k).
(5.32)
At each step k ‚â•1, the Ô¨Årst phase of the iteration is the factorization of
the matrix T(k‚àí1) into the product of an orthogonal matrix Q(k) with an
upper triangular matrix R(k) (see Section 5.6.3). The second phase is a
simple matrix product. Notice that
T(k)
= R(k)Q(k) = (Q(k))T (Q(k)R(k))Q(k) = (Q(k))T T(k‚àí1)Q(k)
= (Q(0)Q(1) . . . Q(k))T A(Q(0)Q(1) . . . Q(k)),
k ‚â•0,
(5.33)
i.e., every matrix T(k) is orthogonally similar to A. This is particularly
relevant for the stability of the method, since, as shown in Section 5.2, the
conditioning of the matrix eigenvalue problem for T(k) is not worse than it
is for A (see also [GL89], p. 360).
A basic implementation of the QR iteration (5.32), assuming Q(0) = In,
is examined in Section 5.5, while a more computationally eÔ¨Écient version,
starting from T(0) in upper Hessenberg form, is described in detail in Sec-
tion 5.6.

5.5 The Basic QR Iteration
201
If A has real eigenvalues, distinct in module, it will be seen in Section 5.5
that the limit of T(k) is an upper triangular matrix (with the eigenvalues of
A on the main diagonal). However, if A has complex eigenvalues the limit
of T(k) cannot be an upper triangular matrix T. Indeed if it were T would
necessarily have real eigenvalues, although it is similar to A.
Failure to converge to a triangular matrix may also happen in more
general situations, as addressed in Example 5.9.
For this, it is necessary to introduce variants of the QR iteration (5.32),
based on deÔ¨Çation and shift techniques (see Section 5.7 and, for a more
detailed discussion of the subject, [GL89], Chapter 7, [Dat95], Chapter 8
and [Dem97], Chapter 4).
These techniques allow for T(k) to converge to an upper quasi-triangular
matrix, known as the real Schur decomposition of A, for which the following
result holds (for the proof we refer to [GL89], pp. 341-342).
Property 5.8 Given a matrix A ‚ààRn√ón, there exists an orthogonal ma-
trix Q ‚ààRn√ón such that
QT AQ =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
R11
R12
. . .
R1m
0
R22
. . .
R2m
...
...
...
...
0
0
. . .
Rmm
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
,
(5.34)
where each block Rii is either a real number or a matrix of order 2 having
complex conjugate eigenvalues, and
Q = lim
k‚Üí‚àû
6
Q(0)Q(1) ¬∑ ¬∑ ¬∑ Q(k)7
(5.35)
Q(k) being the orthogonal matrix generated by the k-th factorization step of
the QR iteration (5.32).
The QR iteration can be also employed to compute all the eigenvectors
of a given matrix. For this purpose, we describe in Section 5.8 two possi-
ble approaches, one based on the coupling between (5.32) and the inverse
iteration (5.28), the other working on the real Schur form (5.34).
5.5
The Basic QR Iteration
In the basic version of the QR method, one sets Q(0) = In in such a way
that T(0) = A. At each step k ‚â•1 the QR factorization of the matrix T(k‚àí1)

202
5. Approximation of Eigenvalues and Eigenvectors
can be carried out using the modiÔ¨Åed Gram-Schmidt procedure introduced
in Section 3.4.3, with a cost of the order of 2n3 Ô¨Çops (for a full matrix A).
The following convergence result holds (for the proof, see [GL89], Theorem
7.3.1, or [Wil65], pp. 517-519).
Property 5.9 (Convergence of QR method) Let A ‚ààRn√ón be a ma-
trix such that
|Œª1| > |Œª2| > . . . > |Œªn|.
Then
lim
k‚Üí+‚àûT(k) =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
Œª1
t12
. . .
t1n
0
Œª2
t23
. . .
...
...
...
...
0
0
. . .
Œªn
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
(5.36)
As for the convergence rate, we have
|t(k)
i,i‚àí1| = O

Œªi
Œªi‚àí1

k
,
i = 2, . . . , n,
for k ‚Üí+‚àû.
(5.37)
Under the additional assumption that A is symmetric, the sequence {T(k)}
tends to a diagonal matrix.
If the eigenvalues of A, although being distinct, are not well-separated, it
follows from (5.37) that the convergence of T(k) towards a triangular matrix
can be quite slow. With the aim of accelerating it, one can resort to the
so-called shift technique, which will be addressed in Section 5.7.
Remark 5.2 It is always possible to reduce the matrix A into a triangular
form by means of an iterative algorithm employing nonorthogonal similarity
transformations. In such a case, the so-called LR iteration (known also as
Rutishauser method, [Rut58]) can be used, from which the QR method has
actually been derived (see also [Fra61], [Wil65]). The LR iteration is based
on the factorization of the matrix A into the product of two matrices L
and R, respectively unit lower triangular and upper triangular, and on the
(nonorthogonal) similarity transformation
L‚àí1AL = L‚àí1(LR)L = RL.
The rare use of the LR method in practical computations is due to the loss
of accuracy that can arise in the LR factorization because of the increase
in module of the upper diagonal entries of R. This aspect, together with
the details of the implementation of the algorithm and some comparisons
with the QR method, is examined in [Wil65], Chapter 8.
‚ñ†

5.6 The QR Method for Matrices in Hessenberg Form
203
Example 5.4 We apply the QR method to the symmetric matrix A‚ààR4√ó4 such
that aii = 4, for i = 1, . . . , 4, and aij = 4 + i ‚àíj for i < j ‚â§4, whose eigenvalues
are (to three signiÔ¨Åcant Ô¨Ågures) Œª1 = 11.09, Œª2 = 3.41, Œª3 = 0.90 and Œª4 = 0.59.
After 20 iterations, we get
T(20) =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
11.09
6.44 ¬∑ 10‚àí10
‚àí3.62 ¬∑ 10‚àí15
9.49 ¬∑ 10‚àí15
6.47 ¬∑ 10‚àí10
3.41
1.43 ¬∑ 10‚àí11
4.60 ¬∑ 10‚àí16
1.74 ¬∑ 10‚àí21
1.43 ¬∑ 10‚àí11
0.90
1.16 ¬∑ 10‚àí4
2.32 ¬∑ 10‚àí25
2.68 ¬∑ 10‚àí15
1.16 ¬∑ 10‚àí4
0.58
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
Notice the ‚Äúalmost-diagonal‚Äù structure of the matrix T(20) and, at the same
time, the eÔ¨Äect of rounding errors which slightly alter its expected symmetry.
Good agreement can also be found between the under-diagonal entries and the
estimate (5.37).
‚Ä¢
A computer implementation of the basic QR iteration is given in Program
28. The QR factorization is executed using the modiÔ¨Åed Gram-Schmidt
method (Program 8). The input parameter niter denotes the maximum
admissible number of iterations, while the output parameters T, Q and R
are the matrices T, Q and R in (5.32) after niter iterations of the QR
procedure.
Program 28 - basicqr : Basic QR iteration
function [T,Q,R]=basicqr(A,niter)
T=A;
for i=1:niter,
[Q,R]=mod grams(T);
T=R*Q;
end
5.6
The QR Method for Matrices in Hessenberg
Form
The naive implementation of the QR method discussed in the previous
section requires (for a full matrix) a computational eÔ¨Äort of the order of
n3 Ô¨Çops per iteration. In this section we illustrate a variant for the QR
iteration, known as Hessenberg-QR iteration, with a greatly reduced com-
putational cost. The idea consists of starting the iteration from a matrix
T(0) in upper Hessenberg form, that is, t(0)
ij = 0 for i > j + 1. Indeed, it can
be checked that with this choice the computation of T(k) in (5.32) requires
only an order of n2 Ô¨Çops per iteration.

204
5. Approximation of Eigenvalues and Eigenvectors
To achieve maximum eÔ¨Éciency and stability of the algorithm, suitable
transformation matrices are employed. Precisely, the preliminary reduc-
tion of matrix A into upper Hessenberg form is realized with Householder
matrices, whilst the QR factorization of T(k) is carried out using Givens
matrices, instead of the modiÔ¨Åed Gram-Schmidt procedure introduced in
Section 3.4.3.
We brieÔ¨Çy describe Householder and Givens matrices in the next section,
referring to Section 5.6.5 for their implementation. The algorithm and ex-
amples of computations of the real Schur form of A starting from its upper
Hessenberg form are then discussed in Section 5.6.4.
5.6.1
Householder and Givens Transformation Matrices
For any vector v ‚ààRn, let us introduce the orthogonal and symmetric
matrix
P = I ‚àí2vvT /‚à•v‚à•2
2.
(5.38)
Given a vector x ‚ààRn, the vector y = Px is the reÔ¨Çection of x with respect
to the hyperplane œÄ = span{v}‚ä•formed by the set of the vectors that are
orthogonal to v (see Figure 5.3, left). Matrix P and the vector v are called
the Householder reÔ¨Çection matrix and the Householder vector, respectively.
v
x
y
œÄ
y
xi
x
Œ∏
xk
FIGURE 5.3. ReÔ¨Çection across the hyperplane orthogonal to v (left); rotation by
an angle Œ∏ in the plane (xi, xk) (right)
Householder matrices can be used to set to zero a block of components of
a given vector x ‚ààRn. If, in particular, one would like to set to zero all the
components of x, except the m-th one, the Householder vector ought to be
chosen as
v = x ¬± ‚à•x‚à•2em,
(5.39)

5.6 The QR Method for Matrices in Hessenberg Form
205
em being the m-th unit vector of Rn. The matrix P computed by (5.38)
depends on the vector x itself, and it can be checked that
Px =
Ô£Æ
Ô£∞0, 0, . . . , ¬±‚à•x‚à•2
8 9: ;
m
, 0, . . . , 0
Ô£π
Ô£ª
T
.
(5.40)
Example 5.5 Let x = [1, 1, 1, 1]T and m = 3; then
v =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
1
1
3
1
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
,
P = 1
6
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
5
‚àí1
‚àí3
‚àí1
‚àí1
5
‚àí3
‚àí1
‚àí3
‚àí3
‚àí3
‚àí3
‚àí1
‚àí1
‚àí3
5
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
,
Px =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
0
0
‚àí2
0
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
‚Ä¢
If, for some k ‚â•1, the Ô¨Årst k components of x must remain unaltered,
while the components from k + 2 on are to be set to zero, the Householder
matrix P = P(k) takes the following form
P(k) =
Ô£Æ
Ô£∞
Ik
0
0
Rn‚àík
Ô£π
Ô£ª,
Rn‚àík = In‚àík ‚àí2w(k)(w(k))T
‚à•w(k)‚à•2
2
.
(5.41)
As usual, Ik is the identity matrix of order k, while Rn‚àík is the elementary
Householder matrix of order n ‚àík associated with the reÔ¨Çection across the
hyperplane orthogonal to the vector w(k) ‚ààRn‚àík. According to (5.39), the
Householder vector is given by
w(k) = x(n‚àík) ¬± ‚à•x(n‚àík)‚à•2e(n‚àík)
1
,
(5.42)
where x(n‚àík) ‚ààRn‚àík is the vector formed by the last n ‚àík components
of x and e(n‚àík)
1
is the Ô¨Årst unit vector of the canonical basis of Rn‚àík. We
notice that P(k) is a function of x through w(k). The criterion for Ô¨Åxing
the sign in the deÔ¨Ånition of w(k) will be discussed in Section 5.6.5.
The components of the transformed vector y = P(k) x read
Ô£±
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£≥
yj = xj
j = 1, ¬∑ ¬∑ ¬∑ , k,
yj = 0
j = k + 2, ¬∑ ¬∑ ¬∑ , n,
yk+1 = ¬±‚à•x(n‚àík)‚à•2.
The Householder matrices will be employed in Section 5.6.2 to carry out the
reduction of a given matrix A to a matrix H(0) in upper Hessenberg form.
This is the Ô¨Årst step for an eÔ¨Écient implementation of the QR iteration
(5.32) with T(0) = H(0) (see Section 5.6).

206
5. Approximation of Eigenvalues and Eigenvectors
Example 5.6 Let x=[1, 2, 3, 4, 5]T and k = 1 (this means that we want to set to
zero the components xj, with j = 3, 4, 5). The matrix P(1) and the transformed
vector y=P(1) x are given by
P(1) =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
1
0
0
0
0
0
0.2722
0.4082
0.5443
0.6804
0
0.4082
0.7710
‚àí0.3053
‚àí0.3816
0
0.5443
‚àí0.3053
0.5929
‚àí0.5089
0
0.6804
‚àí0.3816
‚àí0.5089
0.3639
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
,
y =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
1
7.3485
0
0
0
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
‚Ä¢
The Givens elementary matrices are orthogonal rotation matrices that al-
low for setting to zero in a selective way the entries of a vector or matrix.
For a given pair of indices i and k, and a given angle Œ∏, these matrices are
deÔ¨Åned as
G(i, k, Œ∏) = In ‚àíY
(5.43)
where Y‚ààRn√ón is a null matrix except for the following entries: yii =
ykk = 1 ‚àícos(Œ∏), yik = ‚àísin(Œ∏) = ‚àíyki. A Givens matrix is of the form
i
k
G(i, k, Œ∏)
=
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
1
0
1
...
cos(Œ∏)
sin(Œ∏)
...
‚àísin(Œ∏)
cos(Œ∏)
...
1
0
1
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
i
k
For a given vector x ‚ààRn, the product y = (G(i, k, Œ∏))T x is equivalent to
rotating x counterclockwise by an angle Œ∏ in the coordinate plane (xi, xk)
(see Figure 5.3, right). After letting c = cos Œ∏, s = sin Œ∏, it follows that
yj =
Ô£±
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£≥
xj
j Ã∏= i, k,
cxi ‚àísxk
j = i,
sxi + cxk
j = k.
(5.44)

5.6 The QR Method for Matrices in Hessenberg Form
207
Let Œ±ik =

x2
i + x2
k and notice that if c and s satisfy c = xi/Œ±ik, s =
‚àíxk/Œ±ik (in such a case, Œ∏ = arctan(‚àíxk/xi)), we get yk = 0, yi = Œ±ik
and yj = xj for j Ã∏= i, k. Similarly, if c = xk/Œ±ik, s = xi/Œ±ik (that is,
Œ∏ = arctan(xi/xk)), then yi = 0, yk = Œ±ik and yj = xj for j Ã∏= i, k.
The Givens rotation matrices will be employed in Section 5.6.3 to carry
out the QR factorization step in the algorithm (5.32) and in Section 5.10.1
where the Jacobi method for symmetric matrices is considered.
Remark 5.3 (Householder deÔ¨Çation for power iterations) The ele-
mentary Householder tranformations can be conveniently employed to com-
pute the Ô¨Årst (largest or smallest) eigenvalues of a given matrix A ‚ààRn√ón.
Assume that the eigenvalues of A are ordered as in (5.16) and suppose
that the eigenvalue/eigenvector pair (Œª1, x1) has been computed using the
power method. Then the matrix A can be transformed into the following
block form (see for the proof [Dat95], Theorem 8.5.4, p. 418)
A1 = HAH =

Œª1
bT
0
A2

where b ‚ààRn‚àí1, H is the Householder matrix such that Hx1 = Œ±x1 for
some Œ± ‚ààR, the matrix A2 ‚ààR(n‚àí1)√ó(n‚àí1) and the eigenvalues of A2 are
the same as those of A except for Œª1. The matrix H can be computed using
(5.38) with v = x1 ¬± ‚à•x1‚à•2e1.
The deÔ¨Çation procedure consists of computing the second dominant (sub-
dominant) eigenvalue of A by applying the power method to A2 provided
that |Œª2| Ã∏= |Œª3|. Once Œª2 is available, the corresponding eigenvector x2
can be computed by applying the inverse power iteration to the matrix A
taking ¬µ = Œª2 (see Section 5.3.2) and proceeding in the same manner with
the remaining eigenvalue/eigenvector pairs. An example of deÔ¨Çation will
be presented in Section 5.12.2.
‚ñ†
5.6.2
Reducing a Matrix in Hessenberg Form
A given matrix A‚ààRn√ón can be transformed by similarity transforma-
tions into upper Hessenberg form with a cost of the order of n3 Ô¨Çops. The
algorithm takes n ‚àí2 steps and the similarity transformation Q can be
computed as the product of Householder matrices P(1) ¬∑ ¬∑ ¬∑ P(n‚àí2). For this,
the reduction procedure is commonly known as the Householder method.
Precisely, the k-th step consists of a similarity transformation of A through
the Householder matrix P(k) which aims at setting to zero the elements in
positions k + 2, . . . , n of the k-th column of A, for k = 1, . . . , (n ‚àí2) (see

208
5. Approximation of Eigenvalues and Eigenvectors
Section 5.6.1). For example, in the case n = 4 the reduction process yields
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
‚Ä¢
‚Ä¢
‚Ä¢
‚Ä¢
‚Ä¢
‚Ä¢
‚Ä¢
‚Ä¢
‚Ä¢
‚Ä¢
‚Ä¢
‚Ä¢
‚Ä¢
‚Ä¢
‚Ä¢
‚Ä¢
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
‚àí‚Üí
P(1)
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
‚Ä¢
‚Ä¢
‚Ä¢
‚Ä¢
‚Ä¢
‚Ä¢
‚Ä¢
‚Ä¢
0
‚Ä¢
‚Ä¢
‚Ä¢
0
‚Ä¢
‚Ä¢
‚Ä¢
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
‚àí‚Üí
P(2)
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
‚Ä¢
‚Ä¢
‚Ä¢
‚Ä¢
‚Ä¢
‚Ä¢
‚Ä¢
‚Ä¢
0
‚Ä¢
‚Ä¢
‚Ä¢
0
0
‚Ä¢
‚Ä¢
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
,
having denoted by ‚Ä¢ the entries of the matrices that are a priori non zero.
Given A(0) = A, the method generates a sequence of matrices A(k) that
are orthogonally similar to A
A(k) = PT
(k)A(k‚àí1)P(k) = (P(k) ¬∑ ¬∑ ¬∑ P(1))T A(P(k) ¬∑ ¬∑ ¬∑ P(1))
= QT
(k)AQ(k),
k ‚â•1.
(5.45)
For any k ‚â•1 the matrix P(k) is given by (5.41), where x is substituted
by the k-th column vector in matrix A(k‚àí1). From the deÔ¨Ånition (5.41) it
is easy to check that the operation PT
(k) A(k‚àí1) leaves the Ô¨Årst k rows of
A(k‚àí1) unchanged, whilst PT
(k) A(k‚àí1) P(k) = A(k) does the same on the
Ô¨Årst k columns. After n ‚àí2 steps of the Householder reduction, we obtain
a matrix H = A(n‚àí2) in upper Hessenberg form.
Remark 5.4 (The symmetric case) If A is symmetric, the transforma-
tion (5.45) maintains such a property. Indeed
(A(k))T = (QT
(k)AQ(k))T = A(k),
‚àÄk ‚â•1,
so that H must be tridiagonal. Its eigenvalues can be eÔ¨Éciently computed
using the method of Sturm sequences with a cost of the order of n Ô¨Çops, as
will be addressed in Section 5.10.2.
‚ñ†
A coding of the Householder reduction method is provided in Program
29. To compute the Householder vector, Program 32 is employed. In output,
the two matrices H and Q, respectively in Hessenberg form and orthogonal,
are such that H = QT AQ.
Program 29 - houshess : Hessenberg-Householder method
function [H,Q]=houshess(A)
n=max(size(A)); Q=eye(n); H=A;
for k=1:(n-2),
[v,beta]=vhouse(H(k+1:n,k)); I=eye(k); N=zeros(k,n-k);
m=length(v); R=eye(m)-beta*v*v‚Äô; H(k+1:n,k:n)=R*H(k+1:n,k:n);
H(1:n,k+1:n)=H(1:n,k+1:n)*R; P=[I, N; N‚Äô, R]; Q=Q*P;
end

5.6 The QR Method for Matrices in Hessenberg Form
209
The algorithm coded in Program 29 requires a cost of 10n3/3 Ô¨Çops and
is well-conditioned with respect to rounding errors. Indeed, the following
estimate holds (see [Wil65], p. 351)
H = QT (A + E) Q,
‚à•E‚à•F ‚â§cn2u‚à•A‚à•F
(5.46)
where H is the Hessenberg matrix computed by Program 29, Q is an or-
thogonal matrix, c is a constant, u is the roundoÔ¨Äunit and ‚à•¬∑ ‚à•F is the
Frobenius norm (see (1.18)).
Example 5.7 Consider the reduction in upper Hessenberg form of the Hilbert
matrix H4 ‚ààR4√ó4. Since H4 is symmetric, its Hessenberg form should be a
triadigonal symmetric matrix. Program 29 yields the following results
Q =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
1.00
0
0
0
0
0.77
‚àí0.61
0.20
0
0.51
0.40
‚àí0.76
0
0.38
0.69
0.61
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
,
H =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
1.00
0.65
0
0
0.65
0.65
0.06
0
0
0.06
0.02
0.001
0
0
0.001
0.0003
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
The accuracy of the transformation procedure (5.45) can be measured by com-
puting the ‚à•¬∑ ‚à•F norm of the diÔ¨Äerence between H and QT H4Q. This yields
‚à•H ‚àíQT H4Q‚à•F = 3.38 ¬∑ 10‚àí17, which conÔ¨Årms the stability estimate (5.46).
‚Ä¢
5.6.3
QR Factorization of a Matrix in Hessenberg Form
In this section we explain how to eÔ¨Éciently implement the generic step of
the QR iteration, starting from a matrix T(0) = H(0) in upper Hessenberg
form.
For any k ‚â•1, the Ô¨Årst phase consists of computing the QR factorization
of H(k‚àí1) by means of n ‚àí1 Givens rotations
+
Q(k),T
H(k‚àí1) =
+
G(k)
n‚àí1
,T
. . .
+
G(k)
1
,T
H(k‚àí1) = R(k),
(5.47)
where, for any j = 1, . . . , n ‚àí1, G(k)
j
= G(j, j + 1, Œ∏j)(k) is, for any k ‚â•1,
the j-th Givens rotation matrix (5.43) in which Œ∏j is chosen according
to (5.44) in such a way that the entry of indices (j + 1, j) of the matrix
+
G(k)
j
,T
¬∑ ¬∑ ¬∑
+
G(k)
1
,T
H(k‚àí1) is set equal to zero. The product (5.47) requires
a computational cost of the order of 3n2 Ô¨Çops.
The next step consists of completing the orthogonal similarity transfor-
mation
H(k) = R(k)Q(k) = R(k) +
G(k)
1
. . . G(k)
n‚àí1
,
.
(5.48)

210
5. Approximation of Eigenvalues and Eigenvectors
The orthogonal matrix Q(k) =
+
G(k)
1
. . . G(k)
n‚àí1
,
is in upper Hessenberg
form. Indeed, taking for instance n = 3, and recalling Section 5.6.1, we get
Q(k) = G(k)
1 G(k)
2
=
Ô£Æ
Ô£ØÔ£ØÔ£∞
‚Ä¢
‚Ä¢
0
‚Ä¢
‚Ä¢
0
0
0
1
Ô£π
Ô£∫Ô£∫Ô£ª
Ô£Æ
Ô£ØÔ£ØÔ£∞
1
0
0
0
‚Ä¢
‚Ä¢
0
‚Ä¢
‚Ä¢
Ô£π
Ô£∫Ô£∫Ô£ª=
Ô£Æ
Ô£ØÔ£ØÔ£∞
‚Ä¢
‚Ä¢
‚Ä¢
‚Ä¢
‚Ä¢
‚Ä¢
0
‚Ä¢
‚Ä¢
Ô£π
Ô£∫Ô£∫Ô£ª.
Also (5.48) requires a cost of the order of 3n2 operations, for an overall eÔ¨Äort
of the order of 6n2 Ô¨Çops. In conclusion, performing the QR factorization
with elementary Givens rotations on a starting matrix in upper Hessenberg
form yields a reduction of the operation count of one order of magnitude
with respect to the corresponding factorization with the modiÔ¨Åed Gram-
Schmidt procedure of Section 5.5.
5.6.4
The Basic QR Iteration starting from Upper Hessenberg
Form
A basic implementation of the QR iteration to generate the real Schur
decomposition of a matrix A is given in Program 30.
This program uses Program 29 to reduce A in upper Hessenberg form;
then each QR factorization step in (5.32) is carried out with Program 31
which utilizes Givens rotations. The overall eÔ¨Éciency of the algorithm is
ensured by pre- and post-multiplying with Givens matrices as explained in
Section 5.6.5, and by constructing the matrix Q(k) = G(k)
1
. . . G(k)
n‚àí1 in the
function prodgiv, with a cost of n2 ‚àí2 Ô¨Çops and without explicitly forming
the Givens matrices G(k)
j , for j = 1, . . . , n ‚àí1.
As for the stability of the QR iteration with respect to rounding er-
ror propagation, it can be shown that the computed real Schur form T is
orthogonally similar to a matrix ‚Äúclose‚Äù to A, i.e.
T = QT (A + E)Q
where Q is orthogonal and ‚à•E‚à•2 ‚âÉu‚à•A‚à•2, u being the machine roundoÔ¨Ä
unit.
Program 30 returns in output, after niter iterations of the QR proce-
dure, the matrices T, Q and R in (5.32).
Program 30 - hessqr : Hessenberg-QR method
function [T,Q,R]=hessqr(A,niter)
n=max(size(A));
[T,Qhess]=houshess(A);
for j=1:niter
[Q,R,c,s]= qrgivens(T);

5.6 The QR Method for Matrices in Hessenberg Form
211
T=R;
for k=1:n-1,
T=gacol(T,c(k),s(k),1,k+1,k,k+1);
end
end
Program 31 - givensqr : QR factorization with Givens rotations
function [Q,R,c,s]= qrgivens(H)
[m,n]=size(H);
for k=1:n-1
[c(k),s(k)]=givcos(H(k,k),H(k+1,k));
H=garow(H,c(k),s(k),k,k+1,k,n);
end
R=H; Q=prodgiv(c,s,n);
function Q=prodgiv(c,s,n)
n1=n-1; n2=n-2;
Q=eye(n); Q(n1,n1)=c(n1); Q(n,n)=c(n1);
Q(n1,n)=s(n1); Q(n,n1)=-s(n1);
for k=n2:-1:1,
k1=k+1; Q(k,k)=c(k); Q(k1,k)=-s(k);
q=Q(k1,k1:n); Q(k,k1:n)=s(k)*q;
Q(k1,k1:n)=c(k)*q;
end
Example 5.8 Consider the matrix A (already in Hessenberg form)
A =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
3
17
‚àí37
18
‚àí40
1
0
0
0
0
0
1
0
0
0
0
0
1
0
0
0
0
0
1
0
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
To compute its eigenvalues, given by ‚àí4, ¬±i, 2 and 5, we apply the QR method
and we compute the matrix T(40) after 40 iterations of Program 30. Notice that
the algorithm converges to the real Schur decomposition of A (5.34), with three
blocks Rii of order 1 (i = 1, 2, 3) and with the block R44 = T(40)(4 : 5, 4 : 5)
having eigenvalues equal to ¬±i
T(40) =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
4.9997
18.9739
‚àí34.2570
32.8760
‚àí28.4604
0
‚àí3.9997
6.7693
‚àí6.4968
5.6216
0
0
2
‚àí1.4557
1.1562
0
0
0
0.3129
‚àí0.8709
0
0
0
1.2607
‚àí0.3129
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.

212
5. Approximation of Eigenvalues and Eigenvectors
‚Ä¢
Example 5.9 Let us now employ the QR method to generate the Schur real
decomposition of the matrix A below, after reducing it to upper Hessenberg form
A =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
17
24
1
8
15
23
5
7
14
16
4
6
13
20
22
10
12
19
21
3
11
18
25
2
9
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
The eigenvalues of A are real and given (to four signiÔ¨Åcant Ô¨Ågures) by Œª1 =
65, Œª2,3 = ¬±21.28 and Œª4,5 = ¬±13.13. After 40 iterations of Program 30, the
computed matrix reads
T(40) =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
65
0
0
0
0
0
14.6701
14.2435
4.4848
‚àí3.4375
0
16.6735
‚àí14.6701
‚àí1.2159
2.0416
0
0
0
‚àí13.0293
‚àí0.7643
0
0
0
‚àí3.3173
13.0293
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
It is not upper triangular, but block upper triangular, with a diagonal block
R11 = 65 and the two blocks
R22 =
. 14.6701
14.2435
16.6735
‚àí14.6701
/
,
R33 =
. ‚àí13.0293
‚àí0.7643
‚àí3.3173
13.0293
/
,
having spectrums given by œÉ(R22) = Œª2,3 and œÉ(R33) = Œª4,5 respectively.
It is important to recognize that matrix T(40) is not the real Schur decomposi-
tion of A, but only a ‚Äúcheating‚Äù version of it. In fact, in order for the QR method
to converge to the real Schur decomposition of A, it is mandatory to resort to
the shift techniques introduced in Section 5.7.
‚Ä¢
5.6.5
Implementation of Transformation Matrices
In the deÔ¨Ånition (5.42) it is convenient to choose the minus sign, obtaining
w(k) = x(n‚àík)‚àí‚à•x(n‚àík)‚à•2e(n‚àík)
1
, in such a way that the vector Rn‚àíkx(n‚àík)
is a positive multiple of e(n‚àík)
1
. If xk+1 is positive, in order to avoid nu-
merical cancellations, the computation can be rationalized as follows
w(k)
1
= x2
k+1 ‚àí‚à•x(n‚àík)‚à•2
2
xk+1 + ‚à•x(n‚àík)‚à•2
=
‚àí
n

j=k+2
x2
j
xk+1 + ‚à•x(n‚àík)‚à•2
.

5.6 The QR Method for Matrices in Hessenberg Form
213
The construction of the Householder vector is performed by Program 32,
which takes as input a vector p ‚ààRn‚àík (formerly, the vector x(n‚àík)) and
returns a vector q ‚ààRn‚àík (the Householder vector w(k)), with a cost of
the order of n Ô¨Çops.
If M ‚ààRm√óm is the generic matrix to which the Householder matrix P
(5.38) is applied (where I is the identity matrix of order m and v‚ààRm),
letting w = MT v, then
PM = M ‚àíŒ≤vwT ,
Œ≤ = 2/‚à•v‚à•2
2.
(5.49)
Therefore, performing the product PM amounts to a matrix-vector product
(w = MT v) plus an external product vector-vector (vwT ). The overall
computational cost of the product PM is thus equal to 2(m2 + m) Ô¨Çops.
Similar considerations hold in the case where the product MP is to be
computed; deÔ¨Åning w = Mv, we get
MP = M ‚àíŒ≤wvT .
(5.50)
Notice that (5.49) and (5.50) do not require the explicit construction of
the matrix P. This reduces the computational cost to an order of m2 Ô¨Çops,
whilst executing the product PM without taking advantage of the special
structure of P would increase the operation count to an order of m3 Ô¨Çops.
Program 32 - vhouse : Construction of the Householder vector
function [v,beta]=vhouse(x)
n=length(x); x=x/norm(x); s=x(2:n)‚Äô*x(2:n); v=[1; x(2:n)];
if (s==0), beta=0;
else
mu=sqrt(x(1)ÀÜ2+s);
if (x(1) <= 0), v(1)=x(1)-mu;
else,
v(1)=-s/(x(1)+mu); end
beta=2*v(1)ÀÜ2/(s+v(1)ÀÜ2); v=v/v(1);
end
Concerning the Givens rotation matrices, the computation of c and s is
carried out as follows. Let i and k be two Ô¨Åxed indices and assume that
the k-th component of a given vector x ‚ààRn must be set to zero. Letting
r =

x2
i + x2
k, relation (5.44) yields
.
c
‚àís
s
c
/ .
xi
xk
/
=
.
r
0
/
(5.51)
hence there is no need of explicitly computing Œ∏, nor evaluating any trigono-
metric function.
Executing Program 33 to solve system (5.51), requires 5 Ô¨Çops, plus the
evaluation of a square root. As already noticed in the case of Householder

214
5. Approximation of Eigenvalues and Eigenvectors
matrices, even for Givens rotations we don‚Äôt have to explicitly compute the
matrix G(i, k, Œ∏) to perform its product with a given matrix M‚ààRm√óm.
For that purpose Programs 34 and 35 are used, both at the cost of 6m
Ô¨Çops. Looking at the structure (5.43) of matrix G(i, k, Œ∏), it is clear that
the Ô¨Årst algorithm only modiÔ¨Åes rows i and k of M, whilst the second one
only changes columns i and k of M.
We conclude by noticing that the computation of the Householder vector
v and of the Givens sine and cosine (c, s), are well-conditioned operations
with respect to rounding errors (see [GL89], pp. 212-217 and the references
therein).
The solution of system (5.51) is implemented in Program 33. The input
parameters are the vector components xi and xk, whilst the output data
are the Givens cosine and sine c and s.
Program 33 - givcos : Computation of Givens cosine and sine
function [c,s]=givcos(xi, xk)
if (xk==0), c=1; s=0; else,
if abs(xk) > abs(xi)
t=-xi/xk; s=1/sqrt(1+tÀÜ2); c=s*t;
else
t=-xk/xi; c=1/sqrt(1+tÀÜ2); s=c*t;
end
end
Programs 34 and 35 compute G(i, k, Œ∏)T M and MG(i, k, Œ∏) respectively.
The input parameters c and s are the Givens cosine and sine. In Program
34, the indices i and k identify the rows of the matrix M that are being
aÔ¨Äected by the update M ‚ÜêG(i, k, Œ∏)T M, while j1 and j2 are the indices
of the columns involved in the computation. Similarly, in Program 35 i
and k identify the columns eÔ¨Äected by the update M ‚ÜêMG(i, k, Œ∏), while
j1 and j2 are the indices of the rows involved in the computation.
Program 34 - garow : Product G(i, k, Œ∏)T M
function [M]=garow(M,c,s,i,k,j1,j2)
for j=j1:j2
t1=M(i,j);
t2=M(k,j);
M(i,j)=c*t1-s*t2;
M(k,j)=s*t1+c*t2;
end
Program 35 - gacol : Product MG(i, k, Œ∏)
function [M]=gacol(M,c,s,j1,j2,i,k)
for j=j1:j2
t1=M(j,i);

5.7 The QR Iteration with Shifting Techniques
215
t2=M(j,k);
M(j,i)=c*t1-s*t2;
M(j,k)=s*t1+c*t2;
end
5.7
The QR Iteration with Shifting Techniques
Example 5.9 reveals that the QR iteration does not always converge to the
real Schur form of a given matrix A. To make this happen, an eÔ¨Äective
approach consists of incorporating in the QR iteration (5.32) a shifting
technique similar to that introduced for inverse iteration in Section 5.3.2.
This leads to the QR method with single shift described in Section 5.7.1,
which is used to accelerate the convergence of the QR iteration when A has
eigenvalues with moduli very close to each other.
In Section 5.7.2, a more sophisticated shifting technique is considered,
which guarantees the convergence of the QR iteration to the (approximate)
Schur form of matrix A (see Property 5.8). The resulting method (known as
QR iteration with double shift) is the most popular version of the QR iter-
ation (5.32) for solving the matrix eigenvalue problem, and is implemented
in the MATLAB intrinsic function eig.
5.7.1
The QR Method with Single Shift
Given ¬µ ‚ààR, the shifted QR iteration is deÔ¨Åned as follows. For k = 1, 2, . . . ,
until convergence:
determine Q(k), R(k) such that
Q(k)R(k) = T(k‚àí1) ‚àí¬µI
(QR factorization);
then, let
T(k) = R(k)Q(k) + ¬µI.
(5.52)
where T(0) =

Q(0)T AQ(0) is in upper Hessenberg form. Since the QR
factorization in (5.52) is performed on the shifted matrix T(k‚àí1) ‚àí¬µI, the
scalar ¬µ is called shift. The sequence of matrices T(k) generated by (5.52)
is still similar to the initial matrix A, since for any k ‚â•1
R(k)Q(k) + ¬µI
=

Q(k)T 
Q(k)R(k)Q(k) + ¬µQ(k)
=

Q(k)T 
Q(k)R(k) + ¬µI

Q(k) =

Q(k)T T(k‚àí1)Q(k)
=
(Q(0)Q(1) . . . Q(k))T A(Q(0)Q(1) . . . Q(k)),
k ‚â•0.

216
5. Approximation of Eigenvalues and Eigenvectors
Assume ¬µ is Ô¨Åxed and that the eigenvalues of A are ordered in such a way
that
|Œª1 ‚àí¬µ| ‚â•|Œª2 ‚àí¬µ| ‚â•. . . ‚â•|Œªn ‚àí¬µ|.
Then it can be shown that, for 1 < j ‚â§n, the subdiagonal entry t(k)
j,j‚àí1
tends to zero with a rate that is proportional to the ratio
|(Œªj ‚àí¬µ)/(Œªj‚àí1 ‚àí¬µ)|k.
This extends the convergence result (5.37) to the shifted QR method (see
[GL89], Sections 7.5.2 and 7.3).
The result above suggests that if ¬µ is chosen in such a way that
|Œªn ‚àí¬µ| < |Œªi ‚àí¬µ|,
i = 1, . . . , n ‚àí1,
then the matrix entry t(k)
n,n‚àí1 in the iteration (5.52) tends rapidly to zero
as k increases. (In the limit, if ¬µ were equal to an eigenvalue of T(k), that
is of A, then t(k)
n,n‚àí1 = 0 and t(k)
n,n = ¬µ). In practice one takes
¬µ = t(k)
n,n,
(5.53)
yielding the so called QR iteration with single shift. Correspondingly, the
convergence to zero of the sequence
2
t(k)
n,n‚àí1
3
is quadratic in the sense that
if |t(k)
n,n‚àí1|/‚à•T(0)‚à•2 = Œ∑k < 1, for some k ‚â•0, then |t(k+1)
n,n‚àí1|/‚à•T(0)‚à•2 = O(Œ∑2
k)
(see [Dem97], pp. 161-163 and [GL89], pp. 354-355).
This can be proÔ¨Åtably taken into account when programming the QR
iteration with single shift by monitoring the size of the subdiagonal entry
|t(k)
n,n‚àí1|. In practice, t(k)
n,n‚àí1 is set equal to zero if
|t(k)
n,n‚àí1| ‚â§Œµ(|t(k)
n‚àí1,n‚àí1| + |t(k)
n,n|),
k ‚â•0,
(5.54)
for a prescribed Œµ, in general of the order of the roundoÔ¨Äunit. (This con-
vergence test is adopted in the library EISPACK). If A is an Hessenberg
matrix, when for a certain k a(k)
n,n‚àí1 is set to zero, t(k)
n,n provides the desired
approximation of Œªn. Then the QR iteration with shift can continue on the
matrix T(k)(1 : n ‚àí1, 1 : n ‚àí1), and so on. This is a deÔ¨Çation algorithm
(for another example see Remark 5.3).
Example 5.10 We consider again the matrix A as in Example 5.9. Program 36,
with toll equal to the roundoÔ¨Äunit, converges in 14 iterations to the following
approximate real Schur form of A, which displays the correct eigenvalues of matrix
A on its diagonal (to six signiÔ¨Åcant Ô¨Ågures)
T(40) =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
65
0
0
0
0
0
‚àí21.2768
2.5888
‚àí0.0445
‚àí4.2959
0
0
‚àí13.1263
‚àí4.0294
‚àí13.079
0
0
0
21.2768
‚àí2.6197
0
0
0
0
13.1263
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.

5.7 The QR Iteration with Shifting Techniques
217
We also report in Table 5.2 the convergence rate p(k) of the sequence
2
t(k)
n,n‚àí1
3
(n = 5) computed as
p(k) = 1 +
1
log(Œ∑k) log |t(k)
n,n‚àí1|
|t(k‚àí1)
n,n‚àí1|
,
k ‚â•1.
The results show good agreement with the expected quadratic rate.
k
|t(k)
n,n‚àí1|/‚à•T(0)‚à•2
p(k)
0
0.13865
1
1.5401 ¬∑ 10‚àí2
2.1122
2
1.2213 ¬∑ 10‚àí4
2.1591
3
1.8268 ¬∑ 10‚àí8
1.9775
4
8.9036 ¬∑ 10‚àí16
1.9449
TABLE 5.2. Convergence rate of the sequence
2
t(k)
n,n‚àí1
3
in the QR iteration with
single shift
‚Ä¢
The coding of the QR iteration with single shift (5.52) is given in Pro-
gram 36. The code utilizes Program 29 to reduce the matrix A in upper
Hessenberg form and Program 31 to perform the QR factorization step.
The input parameters toll and itmax are the tolerance Œµ in (5.54) and
the maximum admissible number of iterations, respectively. In output, the
program returns the (approximate) real Schur form of A and the number
of iterations needed for its computation.
Program 36 - qrshift : QR iteration with single shift
function [T,iter]=qrshift(A,toll,itmax)
n=max(size(A)); iter=0; [T,Q]=houshess(A);
for k=n:-1:2
I=eye(k);
while abs(T(k,k-1)) > toll*(abs(T(k,k))+abs(T(k-1,k-1)))
iter=iter+1;
if (iter > itmax),
return
end
mu=T(k,k); [Q,R,c,s]=qrgivens(T(1:k,1:k)-mu*I);
T(1:k,1:k)=R*Q+mu*I;
end
T(k,k-1)=0;
end

218
5. Approximation of Eigenvalues and Eigenvectors
5.7.2
The QR Method with Double Shift
The single-shift QR iteration (5.52) with the choice (5.53) for ¬µ is eÔ¨Äective
if the eigenvalues of A are real, but not necessarily when complex conjugate
eigenvalues are present, as happens in the following example.
Example 5.11 The matrix A ‚ààR4√ó4 (reported below to Ô¨Åve signiÔ¨Åcant Ô¨Ågures)
A =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
1.5726
‚àí0.6392
3.7696
‚àí1.3143
0.2166
‚àí0.0420
0.4006
‚àí1.2054
0.0226
0.3592
0.2045
‚àí0.1411
‚àí0.1814
1.1146
‚àí3.2330
1.2648
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
has eigenvalues {¬±i, 1, 2}, i being the imaginary unit. Running Program 36 with
toll equal to the roundoÔ¨Äunit yields after 100 iterations
T(101) =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
2
1.1999
0.5148
4.9004
0
‚àí0.0001
‚àí0.8575
0.7182
0
1.1662
0.0001
‚àí0.8186
0
0
0
1
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
The obtained matrix is the real Schur form of A, where the 2√ó2 block T(101)(2:3,
2:3) has complex conjugate eigenvalues ¬±i. These eigenvalues cannot be computed
by the algorithm (5.52)-(5.53) since ¬µ is real.
‚Ä¢
The problem with this example is that working with real matrices neces-
sarily yields a real shift, whereas a complex one would be needed. The QR
iteration with double shift is set up to account for complex eigenvalues and
allows for removing the 2√ó2 diagonal blocks of the real Schur form of A.
Precisely, suppose that the QR iteration with single shift (5.52) detects
at some step k a 2√ó2 diagonal block R(k)
kk that cannot be reduced into
upper triangular form. Since the iteration is converging to the real Schur
form of the matrix A the two eigenvalues of R(k)
kk are complex conjugate
and will be denoted by Œª(k) and ¬ØŒª(k). The double shift strategy consists of

5.7 The QR Iteration with Shifting Techniques
219
the following steps:
determine Q(k), R(k) such that
Q(k)R(k) = T(k‚àí1) ‚àíŒª(k)I
(Ô¨Årst QR factorization);
then, let
T(k) = R(k)Q(k) + Œª(k)I;
determine Q(k+1), R(k+1) such that
Q(k+1)R(k+1) = T(k) ‚àí¬ØŒª(k)I
(second QR factorization);
then, let
T(k+1) = R(k+1)Q(k+1) + ¬ØŒª(k)I.
(5.55)
Once the double shift has been carried out the QR iteration with single shift
is continued until a situation analogous to the one above is encountered.
The QR iteration incorporating the double shift strategy is the most eÔ¨Äec-
tive algorithm for computing eigenvalues and yields the approximate Schur
form of a given matrix A. Its actual implementation is far more sophisti-
cated than the outline above and is called QR iteration with Francis shift
(see [Fra61], and, also, [GL89], Section 7.5 and [Dem97], Section 4.4.5). As
for the case of the QR iteration with single shift, quadratic convergence can
also be proven for the QR method with Francis shift. However, special ma-
trices have recently been found for which the method fails to converge (see
for an example Exercise 14 and Remark 5.13). We refer for some analysis
and remedies to [Bat90], [Day96], although the Ô¨Ånding of a shift strategy
that guarantees convergence of the QR iteration for all matrices is still an
open problem.
Example 5.12 Let us apply the QR iteration with double shift to the matrix
A in Example 5.11. After 97 iterations of Program 37, with toll equal to the
roundoÔ¨Äunit, we get the following (approximate) Schur form of A, which displays
on its diagonal the four eigenvalues of A
T(97) =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
2
1 + 2i
‚àí2.33 + 0.86i
4.90
0
5.02 ¬∑ 10‚àí14 + i
‚àí2.02 + 6.91 ¬∑ 10‚àí14i
0.72
t(97)
31
0
‚àí1.78 ¬∑ 10‚àí14 ‚àíi
‚àí0.82
t(97)
41
t(97)
42
0
1
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
where t(97)
31
= 2.06 ¬∑ 10‚àí17 + 7.15 ¬∑ 10‚àí49i, t(97)
41
= ‚àí5.59 ¬∑ 10‚àí17 and t(97)
42
=
‚àí4.26 ¬∑ 10‚àí18, respectively.
‚Ä¢
Example 5.13 Consider the pseudo-spectral diÔ¨Äerentiation matrix (10.73) of
order 5. This matrix is singular, with a unique eigenvalue Œª = 0 of algebraic

220
5. Approximation of Eigenvalues and Eigenvectors
multiplicity equal to 5 (see [CHQZ88], p. 44). In this case the QR method with
double shift provides an inaccurate approximation of the spectrum of the ma-
trix. Indeed, using Program 37, with toll=eps, the method converges after 59
iterations to an upper triangular matrix with diagonal entries given by 0.0020,
0.0006 ¬± 0.0019i and ‚àí0.0017 ¬± 0.0012i, respectively. Using the MATLAB intrin-
sic function eig yields instead the eigenvalues ‚àí0.0024, ‚àí0.0007 ¬± 0.0023i and
0.0019 ¬± 0.0014i.
‚Ä¢
A basic implementation of the QR iteration with double shift is provided
in Program 37. The input/output parameters are the same as those of
Program 36. The output matrix T is the approximate Schur form of matrix
A.
Program 37 - qr2shift : QR iteration with double shift
function [T,iter]=qr2shift(A,toll,itmax)
n=max(size(A)); iter=0; [T,Q]=houshess(A);
for k=n:-1:2
I=eye(k);
while abs(T(k,k-1)) > toll*(abs(T(k,k))+abs(T(k-1,k-1)))
iter=iter+1; if (iter > itmax), return, end
mu=T(k,k); [Q,R,c,s]=qrgivens(T(1:k,1:k)-mu*I);
T(1:k,1:k)=R*Q+mu*I;
if (k > 2),
Tdiag2=abs(T(k-1,k-1))+abs(T(k-2,k-2));
if abs(T(k-1,k-2)) ¬°= toll*Tdiag2;
[lambda]=eig(T(k-1:k,k-1:k));
[Q,R,c,s]=qrgivens(T(1:k,1:k)-lambda(1)*I);
T(1:k,1:k)=R*Q+lambda(1)*I;
[Q,R,c,s]=qrgivens(T(1:k,1:k)-lambda(2)*I);
T(1:k,1:k)=R*Q+lambda(2)*I;
end
end
end, T(k,k-1)=0;
end
I=eye(2);
while (abs(T(2,1)) > toll*(abs(T(2,2))+abs(T(1,1)))) & (iter <= itmax)
iter=iter+1; mu=T(2,2);
[Q,R,c,s]=qrgivens(T(1:2,1:2)-mu*I); T(1:2,1:2)=R*Q+mu*I;
end

5.8 Computing the Eigenvectors and the SVD of a Matrix
221
5.8
Computing the Eigenvectors and the SVD of a
Matrix
The power and inverse iterations described in Section 5.3.2 can be used
to compute a selected number of eigenvalue/eigenvector pairs. If all the
eigenvalues and eigenvectors of a matrix are needed, the QR iteration can be
proÔ¨Åtably employed to compute the eigenvectors as shown in Sections 5.8.1
and 5.8.2. In Section 5.8.3 we deal with the computation of the singular
value decomposition (SVD) of a given matrix.
5.8.1
The Hessenberg Inverse Iteration
For any approximate eigenvalue Œª computed by the QR iteration as de-
scribed in Section 5.7.2, the inverse iteration (5.28) can be applied to the
matrix H = QT AQ in Hessenberg form, yielding an approximate eigenvec-
tor q. Then, the eigenvector x associated with Œª is computed as x = Qq.
Clearly, one can take advantage of the structure of the Hessenberg matrix
for an eÔ¨Écient solution of the linear system at each step of (5.28). Typi-
cally, only one iteration is required to produce an adequate approximation
of the desired eigenvector x (see [GL89], Section 7.6.1 and [PW79] for more
details).
5.8.2
Computing the Eigenvectors from the Schur Form of a
Matrix
Suppose that the (approximate) Schur form QHAQ=T of a given matrix
A‚ààRn√ón has been computed by the QR iteration with double shift, Q
being a unitary matrix and T being upper triangular.
Then, if Ax=Œªx, we have QHAQQHx= QH Œªx, i.e., letting y=QHx, T
y=Œªy holds. Therefore y is an eigenvector of T, so that to compute the
eigenvectors of A we can work directly on the Schur form T.
Assume for simplicity that Œª = tkk ‚ààC is a simple eigenvalue of A. Then
the upper triangular matrix T can be decomposed as
T =
Ô£Æ
Ô£ØÔ£ØÔ£∞
T11
v
T13
0
Œª
wT
0
0
T33
Ô£π
Ô£∫Ô£∫Ô£ª,
where T11 ‚ààC(k‚àí1)√ó(k‚àí1) and T33 ‚ààC(n‚àík)√ó(n‚àík) are upper triangular
matrices, v‚ààCk‚àí1, w‚ààCn‚àík and Œª Ã∏‚ààœÉ(T11) ‚à™œÉ(T33).

222
5. Approximation of Eigenvalues and Eigenvectors
Thus, letting y =

yT
k‚àí1, y, yT
n‚àík

, with yk‚àí1 ‚ààCk‚àí1, y ‚ààC and yn‚àík ‚àà
Cn‚àík, the matrix eigenvector problem (T - ŒªI) y=0 can be written as
Ô£±
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£≥
(T11 ‚àíŒªIk‚àí1)yk‚àí1+
vy+
T13yn‚àík
=
0
wT yn‚àík
=
0
(T33 ‚àíŒªIn‚àík)yn‚àík
=
0.
(5.56)
Since Œª is simple, both matrices T11 ‚àíŒªIk‚àí1 and T33 ‚àíŒªIn‚àík are nonsin-
gular, so that the third equation in (5.56) yields yn‚àík = 0 and the Ô¨Årst
equation becomes
(T11 ‚àíŒªIk‚àí1)yk‚àí1 = ‚àívy.
Setting arbitrarily y = 1 and solving the triangular system above for yk‚àí1
yields (formally)
y =
Ô£´
Ô£¨
Ô£¨
Ô£≠
‚àí(T11 ‚àíŒªIk‚àí1)‚àí1v
1
0
Ô£∂
Ô£∑
Ô£∑
Ô£∏.
The desired eigenvector x can then be computed as x=Qy.
An eÔ¨Écient implementation of the above procedure is carried out in the
intrinsic MATLAB function eig. Invoking this function with the format [V,
D]= eig(A) yields the matrix V whose columns are the right eigenvectors
of A and the diagonal matrix D contains its eigenvalues. Further details can
be found in the strvec subroutine in the LAPACK library, while for the
computation of eigenvectors in the case where A is symmetric, we refer to
[GL89], Chapter 8 and [Dem97], Section 5.3.
5.8.3
Approximate Computation of the SVD of a Matrix
In this section we describe the Golub-Kahan-Reinsch algorithm for the
computation of the SVD of a matrix A ‚ààRm√ón with m ‚â•n (see [GL89],
Section 5.4). The method consists of two phases, a direct one and an iter-
ative one.
In the Ô¨Årst phase A is transformed into an upper trapezoidal matrix of
the form
UT AV =

B
0

,
(5.57)
where U and V are two orthogonal matrices and B ‚ààRn√ón is upper bidi-
agonal. The matrices U and V are generated using n + m ‚àí3 Householder
matrices U1, . . . , Um‚àí1, V1, . . . , Vn‚àí2 as follows.

5.8 Computing the Eigenvectors and the SVD of a Matrix
223
The algorithm initially generates U1 in such a way that the matrix A(1) =
U1A has a(1)
i1 = 0 if i > 1. Then, V1 is determined so that A(2) = A(1)V1
has a(2)
1j = 0 for j > 2, preserving at the same time the null entries of the
previous step. The procedure is repeated starting from A(2), and taking U2
such that A(3) = U2A(2) has a(3)
i2 = 0 for i > 2 and V2 in such a way that
A(4) = A(3)V2 has a(4)
2j = 0 for j > 3, yet preserving the null entries already
generated. For example, in the case m = 5, n = 4 the Ô¨Årst two steps of the
reduction process yield
A(1) = U1A =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
‚Ä¢
‚Ä¢
‚Ä¢
‚Ä¢
0
‚Ä¢
‚Ä¢
‚Ä¢
0
‚Ä¢
‚Ä¢
‚Ä¢
0
‚Ä¢
‚Ä¢
‚Ä¢
0
‚Ä¢
‚Ä¢
‚Ä¢
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
‚àí‚ÜíA(2) = A(1)V1 =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
‚Ä¢
‚Ä¢
0
0
0
‚Ä¢
‚Ä¢
‚Ä¢
0
‚Ä¢
‚Ä¢
‚Ä¢
0
‚Ä¢
‚Ä¢
‚Ä¢
0
‚Ä¢
‚Ä¢
‚Ä¢
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
,
having denoted by ‚Ä¢ the entries of the matrices that in principle are diÔ¨Äerent
than zero. After at most m ‚àí1 steps, we Ô¨Ånd (5.57) with
U = U1U2 . . . Um‚àí1,
V = V1V2 . . . Vn‚àí2.
In the second phase, the obtained matrix B is reduced into a diagonal
matrix Œ£ using the QR iteration. Precisely, a sequence of upper bidiagonal
matrices B(k) are constructed such that, as k ‚Üí‚àû, their oÔ¨Ä-diagonal
entries tend to zero quadratically and the diagonal entries tend to the
singular values œÉi of A. In the limit, the process generates two orthogonal
matrices W and Z such that
WT BZ = Œ£ = diag(œÉ1, . . . , œÉn).
The SVD of A is then given by
UT AV =

Œ£
0

,
with U = Udiag(W, Im‚àín) and V = VZ.
The computational cost of this procedure is 2m2n + 4mn2 + 9
2n3 Ô¨Çops,
which reduces to 2mn2‚àí2
3n3 Ô¨Çops if only the singular values are computed.
In this case, recalling what was stated in Section 3.13 about AT A, the
method described in the present section is preferable to computing directly
the eigenvalues of AT A and then taking their square roots.
As for the stability of this procedure, it can be shown that the computed
œÉi turn out to be the singular values of the matrix A + Œ¥A with
‚à•Œ¥A‚à•2 ‚â§Cmnu‚à•A‚à•2,

224
5. Approximation of Eigenvalues and Eigenvectors
Cmn being a constant dependent on n, m and the roundoÔ¨Äunit u. For other
approaches to the computation of the SVD of a matrix, see [Dat95] and
[GL89].
5.9
The Generalized Eigenvalue Problem
Let A, B ‚ààCn√ón be two given matrices; for any z ‚ààC, we call A ‚àízB a
matrix pencil and denote it by (A,B). The set œÉ(A,B) of the eigenvalues of
(A,B) is deÔ¨Åned as
œÉ(A, B) = {¬µ ‚ààC : det(A ‚àí¬µB) = 0} .
The generalized matrix eigenvalue problem can be formulated as: Ô¨Ånd Œª ‚àà
œÉ(A,B) and a nonnull vector x ‚ààCn such that
Ax = ŒªBx.
(5.58)
The pair (Œª, x) satisfying (5.58) is an eigenvalue/eigenvector pair of the
pencil (A,B). Note that by setting B=In in (5.58) we recover the standard
matrix eigenvalue problem considered thus far.
Problems like (5.58) arise frequently in engineering applications, e.g.,
in the study of vibrations of structures (buildings, aircrafts and bridges)
or in the mode analysis for waveguides (see [Inm94] and [Bos93]). Another
example is the computation of the extremal eigenvalues of a preconditioned
matrix P‚àí1A (in which case B = P in (5.58)) when solving a linear system
with an iterative method (see Remark 4.2).
Let us introduce some deÔ¨Ånitions. We say that the pencil (A,B) is regular
if det(A-zB) is not identically zero, otherwise the pencil is singular. When
(A,B) is regular, p(z) = det(A ‚àízB) is the characteristic polynomial of the
pencil; denoting by k the degree of p, the eigenvalues of (A,B) are deÔ¨Åned
as:
1. the roots of p(z) = 0, if k = n;
2. ‚àûif k < n (with multiplicity equal to n ‚àík).
Example 5.14 (Taken from [Par80], [Saa92] and [GL89])
A =
 ‚àí1
0
0
1

,
B =
 0
1
1
0

p(z) = z2 + 1
=‚áí
œÉ(A, B) = ¬±i
A =
 ‚àí1
0
0
0

,
B =
 0
0
0
1

p(z) = z
=‚áí
œÉ(A, B) = {0, ‚àû}
A =
 1
2
0
0

,
B =
 1
0
0
0

p(z) = 0
=‚áí
œÉ(A, B) = C.
The Ô¨Årst pair of matrices shows that symmetric pencils, unlike symmetric ma-
trices, may exhibit complex conjugate eigenvalues. The second pair is a regular
pencil displaying an eigenvalue equal to inÔ¨Ånity, while the third pair is an example
of singular pencil.
‚Ä¢

5.9 The Generalized Eigenvalue Problem
225
5.9.1
Computing the Generalized Real Schur Form
The deÔ¨Ånitions and examples above imply that the pencil (A,B) has n Ô¨Ånite
eigenvalues iÔ¨ÄB is nonsingular.
In such a case, a possible approach to the solution of problem (5.58) is
to transform it into the equivalent eigenvalue problem Cx = Œªx, where the
matrix C is the solution of the system BC = A, then apply the QR iteration
to C. For actually computing the matrix C, one can use Gauss elimination
with pivoting or the techniques shown in Section 3.6. This procedure can
yield inaccurate results if B is ill-conditioned, since computing C is aÔ¨Äected
by rounding errors of the order of u ‚à•A‚à•2‚à•B‚àí1‚à•2 (see [GL89], p. 376).
A more attractive approach is based on the following result, which gen-
eralizes the Schur decomposition theorem 1.5 to the case of regular pencils
to (for a proof, see [Dat95], p. 497).
Property 5.10 (Generalized Schur decomposition) Let (A,B) be a
regular pencil. Then, there exist two unitary matrices U and Z such that
UHAZ = T, UHBZ = S, where T and S are upper triangular. For i =
1, . . . , n the eigenvalues of (A,B) are given by
Œªi = tii/sii,
if sii Ã∏= 0,
Œªi = ‚àû,
if tii Ã∏= 0, sii = 0.
Exactly as in the matrix eigenvalue problem, the generalized Schur form
cannot be explicitly computed, so the counterpart of the real Schur form
(5.34) has to be computed. Assuming that the matrices A and B are real, it
can be shown that there exist two orthogonal matrices ÀúU and ÀúZ such that
ÀúT = ÀúUT AÀúZ is upper quasi-triangular and ÀúS = ÀúUT BÀúZ is upper triangular.
This decomposition is known as the generalized real Schur decomposition
of a pair (A,B) and can be computed by a suitably modiÔ¨Åed version of the
QR algorithm, known as QZ iteration, which consists of the following steps
(for a more detailed description, see [GL89], Section 7.7, [Dat95], Section
9.3):
1. reduce A and B into upper Hessenberg form and upper triangular
form, respectively, i.e., Ô¨Ånd two orthogonal matrices Q and Z such
that A = QT AZ is upper Hessenberg and B = QT BZ is upper trian-
gular;
2. the QR iteration is applied to the matrix AB‚àí1 to reduce it to real
Schur form.
To save computational resources, the QZ algorithm overwrites the matrices
A and B on their upper Hessenberg and triangular forms and requires 30n3
Ô¨Çops; an additional cost of 36n3 operations is required if Q and Z are

226
5. Approximation of Eigenvalues and Eigenvectors
also needed. The method is implemented in the LAPACK library in the
subroutine sgges and can be invoked in the MATLAB environment with
the command eig(A,B).
5.9.2
Generalized Real Schur Form of Symmetric-DeÔ¨Ånite
Pencils
A remarkable situation occurs when both A and B are symmetric, and one
of them, say B, is also positive deÔ¨Ånite. In such a case, the pair (A,B) forms
a symmetric-deÔ¨Ånite pencil for which the following result holds.
Theorem 5.7 The symmetric-deÔ¨Ånite pencil (A,B) has real eigenvalues
and linearly independent eigenvectors. Moreover, the matrices A and B can
be simultaneously diagonalized. Precisely, there exists a nonsingular matrix
X ‚ààRn√ón such that
XT AX = Œõ = diag(Œª1, Œª2, . . . , Œªn),
XT BX = In,
where for i = 1, . . . , n, Œªi are the eigenvalues of the pencil (A, B).
Proof. Since B is symmetric positive deÔ¨Ånite, it admits a unique Cholesky fac-
torization B = HT H, where H is upper triangular (see Section 3.4.2). From (5.58)
we deduce that Cz = Œªz with C = H‚àíT AH‚àí1, z = Hx, where (Œª, x) is an
eigenvalue/eigenvector pair of (A,B).
The matrix C is symmetric; therefore, its eigenvalues are real and a set of
orthonormal eigenvectors (y1, . . . , yn) = Y exists. As a consequence, letting X =
H‚àí1Y allows for simultaneously diagonalizing both A and B since
XT AX = YT H‚àíT AH‚àí1Y = YT CY = Œõ = diag(Œª1, . . . , Œªn),
XT BX = YT H‚àíT BH‚àí1Y = YT Y = In.
3
The following QR-Cholesky algorithm computes the eigenvalues Œªi and
the corresponding eigenvectors xi of a symmetric-deÔ¨Ånite pencil (A,B), for
i = 1, . . . , n (see for more details [GL89], Section 8.7, [Dat95], Section 9.5):
1. compute the Cholesky factorization B = HT H;
2. compute C = H‚àíT AH‚àí1;
3. for i = 1, . . . , n, compute the eigenvalues Œªi and eigenvectors zi of the
symmetric matrix C using the QR iteration. Then construct from the
set {zi} an orthonormal set of eigenvectors {yi} (using, for instance,
the modiÔ¨Åed Gram-Schmidt procedure of Section 3.4.3);
4. for i = 1, . . . , n, compute the eigenvectors xi of the pencil (A,B) by
solving the systems Hxi = yi.

5.10 Methods for Eigenvalues of Symmetric matrices
227
This algorithm requires an order of 14n3 Ô¨Çops and it can be shown (see
[GL89], p. 464) that, if ÀÜŒª is a computed eigenvalue, then
ÀÜŒª ‚ààœÉ(H‚àíT AH‚àí1 + E),
with ‚à•E‚à•2 ‚âÉu‚à•A‚à•2‚à•B‚àí1‚à•2.
Thus, the generalized eigenvalue problem in the symmetric-deÔ¨Ånite case
may become unstable with respect to rounding errors propagation if B is
ill-conditioned. For a stabilized version of the QR-Cholesky method, see
[GL89], p. 464 and the references cited therein.
5.10
Methods for Eigenvalues of Symmetric
matrices
In this section we deal with the computation of the eigenvalues of a sym-
metric matrix A ‚ààRn√ón. Besides the QR method previously examined,
speciÔ¨Åc algorithms which take advantage of the symmetry of A are avail-
able.
Among these, we Ô¨Årst consider the Jacobi method, which generates a
sequence of matrices orthogonally similar to A and converging to the diag-
onal Schur form of A. Then, the Sturm sequence and Lanczos procedures
are presented, for handling the case of tridiagonal matrices and large sparse
matrices respectively.
5.10.1
The Jacobi Method
The Jacobi method generates a sequence of matrices A(k) that are orthog-
onally similar to matrix A and converge to a diagonal matrix whose entries
are the eigenvalues of A. This is done using the Givens similarity transfor-
mations (5.43) as follows.
Given A(0) = A, for any k = 1, 2, . . . , a pair of indices p and q is Ô¨Åxed,
with 1 ‚â§p < q ‚â§n. Next, letting Gpq = G(p, q, Œ∏), the matrix A(k) =
(Gpq)T A(k‚àí1)Gpq, orthogonally similar to A, is constructed in such a way
that
a(k)
ij = 0
if
(i, j) = (p, q).
(5.59)
Letting c = cos Œ∏ and s = sin Œ∏, the procedure for computing the entries of
A(k) that are changed with respect to those of A(k‚àí1), can be written as
Ô£Æ
Ô£∞a(k)
pp
a(k)
pq
a(k)
pq
a(k)
qq
Ô£π
Ô£ª=
.
c
s
‚àís
c
/T Ô£Æ
Ô£∞a(k‚àí1)
pp
a(k‚àí1)
pq
a(k‚àí1)
pq
a(k‚àí1)
qq
Ô£π
Ô£ª
.
c
s
‚àís
c
/
.
(5.60)
If a(k‚àí1)
pq
= 0, we can satisfy (5.59) by taking c = 1 and s = 0. If a(k‚àí1)
pq
Ã∏=
0, letting t = s/c, (5.60) requires the solution of the following algebraic

228
5. Approximation of Eigenvalues and Eigenvectors
equation
t2 + 2Œ∑t ‚àí1 = 0,
Œ∑ = a(k‚àí1)
qq
‚àía(k‚àí1)
pp
2a(k‚àí1)
pq
.
(5.61)
The root t = 1/(Œ∑ +

1 + Œ∑2) is chosen in (5.61) if Œ∑ ‚â•0, otherwise we
take t = ‚àí1/(‚àíŒ∑ +

1 + Œ∑2); next, we let
c =
1
‚àö
1 + t2 ,
s = ct.
(5.62)
To examine the rate at which the oÔ¨Ä-diagonal entries of A(k) tend to zero,
it is convenient to introduce, for any matrix M ‚ààRn√ón, the nonnegative
quantity
Œ®(M) =
Ô£´
Ô£¨
Ô£≠
n

i,j=1
iÃ∏=j
m2
ij
Ô£∂
Ô£∑
Ô£∏
1/2
=

‚à•M‚à•2
F ‚àí
n

i=1
m2
ii
1/2
.
(5.63)
The Jacobi method ensures that Œ®(A(k)) ‚â§Œ®(A(k‚àí1)) for any k ‚â•1.
Indeed, the computation of (5.63) for matrix A(k) yields
(Œ®(A(k)))2 = (Œ®(A(k‚àí1)))2 ‚àí2
+
a(k‚àí1)
pq
,2
‚â§(Œ®(A(k‚àí1)))2.
(5.64)
The estimate (5.64) suggests that, at each step k, the optimal choice of the
indices p and q is that corresponding to the entry in A(k‚àí1) such that
|a(k‚àí1)
pq
| = max
iÃ∏=j |a(k‚àí1)
ij
|.
The computational cost of this strategy is of the order of n2 Ô¨Çops for the
search of the maximum module entry, while the updating step A(k) =
(Gpq)T A(k‚àí1)Gpq requires only a cost of the order of n Ô¨Çops, as already
noticed in Section 5.6.5. It is thus convenient to resort to the so called row
cyclic Jacobi method, in which the choice of the indices p and q is done by
a row-sweeping of the matrix A(k) according to the following algorithm: for
any k = 1, 2, . . . and for any i-th row of A(k) (i = 1, . . . , n ‚àí1), we set
p = i and q = (i+1), . . . , n. Each complete sweep requires N = n(n‚àí1)/2
Jacobi transformations. Assuming that |Œªi ‚àíŒªj| ‚â•Œ¥ for i Ã∏= j, it can be
shown that the cyclic Jacobi method converges quadratically, that is (see
[Wil65], [Wil62])
Œ®(A(k+N)) ‚â§
1
Œ¥
‚àö
2(Œ®(A(k)))2,
k = 1, 2, . . .
For further details of the algorithm, we refer to [GL89], Section 8.4.

5.10 Methods for Eigenvalues of Symmetric matrices
229
Example 5.15 Let us apply the cyclic Jacobi method to the Hilbert matrix H4,
whose eigenvalues read (to Ô¨Åve signiÔ¨Åcant Ô¨Ågures) Œª1 = 1.5002, Œª2 = 1.6914¬∑10‚àí1,
Œª3 = 6.7383 ¬∑ 10‚àí3 and Œª4 = 9.6702 ¬∑ 10‚àí5. Running Program 40 with toll =
10‚àí15, the method converges in 3 sweeps to a matrix whose diagonal entries
coincide with the eigenvalues of H4 unless 4.4409 ¬∑ 10‚àí16. As for the oÔ¨Ä-diagonal
entries, the values attained by Œ®(H(k)
4 ) are reported in Table 5.3.
‚Ä¢
Sweep
Œ®(H(k)
4 )
Sweep
Œ®(H(k)
4 )
Sweep
Œ®(H(k)
4 )
1
5.262 ¬∑ 10‚àí2
2
3.824 ¬∑ 10‚àí5
3
5.313 ¬∑ 10‚àí16
TABLE 5.3. Convergence of the cyclic Jacobi algorithm
Formulae (5.63) and (5.62) are implemented in Programs 38 and 39.
Program 38 - psinorm : Evaluation of Œ®(A)
function [psi]=psinorm(A)
n=max(size(A)); psi=0;
for i=1:(n-1), for j=(i+1):n, psi=psi+A(i,j)ÀÜ2+A(j,i)ÀÜ2; end; end; psi=sqrt(psi);
Program 39 - symschur : Evaluation of c and s
function [c,s]=symschur(A,p,q)
if (A(p,q)==0), c=1; s=0; else,
eta=(A(q,q)-A(p,p))/(2*A(p,q));
if (eta >= 0), t=1/(eta+sqrt(1+etaÀÜ2));
else, t=-1/(-eta+sqrt(1+etaÀÜ2)); end; c=1/sqrt(1+tÀÜ2); s=c*t;
end
A coding of the cyclic Jacobi method is implemented in Program 40.
This program gets as input parameters the symmetric matrix A ‚ààRn√ón
and a tolerance toll. The program returns a matrix D = GT AG, G be-
ing orthogonal, such that Œ®(D) ‚â§toll‚à•A‚à•F , the value of Œ®(D) and the
number of sweeps to achieve convergence.
Program 40 - cycjacobi : Cyclic Jacobi method for symmetric matrices
function [D,sweep,psi]=cycjacobi(A,toll)
n=max(size(A)); D=A; psiD=norm(A,‚Äôfro‚Äô);
epsi=toll*psiD; psiD=psinorm(D); [psi]=psiD; sweep=0;
while (psiD > epsi), sweep=sweep+1;
for p=1:(n-1), for q=(p+1):n
[c,s]=symschur(D,p,q); [D]=gacol(D,c,s,1,n,p,q); [D]=garow(D,c,s,p,q,1,n);
end; end; psiD=psinorm(D); psi=[psi; psiD];
end

230
5. Approximation of Eigenvalues and Eigenvectors
5.10.2
The Method of Sturm Sequences
In this section we deal with the calculation of the eigenvalues of a real,
tridiagonal and symmetric matrix T. Typical instances of such a problem
arise when applying the Householder transformation to a given symmetric
matrix A (see Section 5.6.2) or when solving boundary value problems in
one spatial dimension (see for an example Section 5.12.1).
We analyze the method of Sturm sequences, or Givens method, introduced
in [Giv54]. For i = 1, . . . , n, we denote by di the diagonal entries of T and by
bi, i = 1, . . . , n‚àí1, the elements of the upper and lower subdiagonals of T.
We shall assume that bi Ã∏= 0 for any i. Otherwise, indeed, the computation
reduces to problems of less complexity.
Letting Ti be the principal minor of order i of matrix T and p0(x) = 1,
we deÔ¨Åne for i = 1, . . . , n the following sequence of polynomials pi(x) =
det(Ti ‚àíxIi)
p1(x) = d1 ‚àíx
pi(x) = (di ‚àíx)pi‚àí1(x) ‚àíb2
i‚àí1pi‚àí2(x),
i = 2, . . . , n.
(5.65)
It can be checked that pn is the characteristic polynomial of T; the com-
putational cost of its evaluation at point x is of the order of 2n Ô¨Çops. The
sequence (5.65) is called the Sturm sequence owing to the following result,
for whose proof we refer to [Wil65], Chapter 2, Section 47 and Chapter 5,
Section 37.
Property 5.11 (of Sturm sequence) For i = 2, . . . , n the eigenvalues
of Ti‚àí1 strictly separate those of Ti, that is
Œªi(Ti) < Œªi‚àí1(Ti‚àí1) < Œªi‚àí1(Ti) < . . . < Œª2(Ti) < Œª1(Ti‚àí1) < Œª1(Ti).
Moreover, letting for any real number ¬µ
S¬µ = {p0(¬µ), p1(¬µ), . . . , pn(¬µ)},
the number s(¬µ) of sign changes in S¬µ yields the number of eigenvalues of T
that are strictly less than ¬µ, with the convention that pi(¬µ) has opposite sign
to pi‚àí1(¬µ) if pi(¬µ) = 0 (two consecutive elements in the sequence cannot
vanish at the same value of ¬µ).
Example 5.16 Let T be the tridiagonal part of the Hilbert matrix H4 ‚ààR4√ó4,
having entries hij = 1/(i + j ‚àí1). The eigenvalues of T are (to Ô¨Åve signiÔ¨Åcant
Ô¨Ågures) Œª1 = 1.2813, Œª2 = 0.4205, Œª3 = ‚àí0.1417 and Œª4 = 0.1161. Taking ¬µ = 0,
Program 41 computes the following Sturm sequence
S0 = {p0(0), p1(0), p2(0), p3(0), p4(0)} = {1, 1, 0.0833, ‚àí0.0458, ‚àí0.0089}
from which, applying Property 5.11, one concludes that matrix T has one eigen-
value less than 0. In the case of matrix T = tridiag4(‚àí1, 2, ‚àí1), with eigenvalues

5.10 Methods for Eigenvalues of Symmetric matrices
231
{0.38, 1.38, 2.62, 3.62} (to three signiÔ¨Åcant Ô¨Ågures), we get, taking ¬µ = 3
{p0(3), p1(3), p2(3), p3(3), p4(3)} = {1, ‚àí1, 0, 1, ‚àí1}
which shows that matrix T has three eigenvalues less than 3, since three sign
changes occur.
‚Ä¢
The Givens method for the calculation of the eigenvalues of T proceeds as
follows. Letting b0 = bn = 0, Theorem 5.2 yields the interval J = [Œ±, Œ≤]
which contains the spectrum of T, where
Œ± = min
1‚â§i‚â§n [di ‚àí(|bi‚àí1| + |bi|)] ,
Œ≤ = max
1‚â§i‚â§n [di + (|bi‚àí1| + |bi|)] .
The set J is used as an initial guess in the search for generic eigenvalues
Œªi of matrix T, for i = 1, . . . , n, using the bisection method (see Chapter
6).
Precisely, given a(0) = Œ± and b(0) = Œ≤, we let c(0) = (Œ± + Œ≤)/2 and
compute s(c(0)); then, recalling Property 5.11, we let b(1) = c(0) if s(c(0)) >
(n ‚àíi), otherwise we set a(1) = c(0). After r iterations, the value c(r) =
(a(r) + b(r))/2 provides an approximation of Œªi within (|Œ±| + |Œ≤|) ¬∑ 2‚àí(r+1),
as is shown in (6.9).
A systematic procedure can be set up to store any information about
the position within the interval J of the eigenvalues of T that are being
computed by the Givens method. The resulting algorithm generates a se-
quence of neighboring subintervals a(r)
j , b(r)
j , for j = 1, . . . , n, each one of
arbitrarily small length and containing one eigenvalue Œªj of T (for further
details, see [BMW67]).
Example 5.17 Let us employ the Givens method to compute the eigenvalue
Œª2 ‚âÉ2.62 of matrix T considered in Example 5.16. Letting toll=10‚àí4 in Program
42 we obtain the results reported in Table 5.4, which demonstrate the convergence
of the sequence c(k) to the desired eigenvalue in 13 iterations. We have denoted
for brevity, s(k) = s(c(k)). Similar results are obtained by running Program 42 to
compute the remaining eigenvalues of T.
‚Ä¢
k
a(k)
b(k)
c(k)
s(k)
k
a(k)
b(k)
c(k)
s(k)
0
0
4.000
2.0000
2
7
2.5938
2.625
2.6094
2
1
2.0000
4.000
3.0000
3
8
2.6094
2.625
2.6172
2
2
2.0000
3.000
2.5000
2
9
2.6094
2.625
2.6172
2
3
2.5000
3.000
2.7500
3
10
2.6172
2.625
2.6211
3
4
2.5000
2.750
2.6250
3
11
2.6172
2.621
2.6191
3
5
2.5000
2.625
2.5625
2
12
2.6172
2.619
2.6182
3
6
2.5625
2.625
2.5938
2
13
2.6172
2.618
2.6177
2
TABLE 5.4. Convergence of the Givens method for the calculation of the eigen-
value Œª2 of the matrix T in Example 5.16

232
5. Approximation of Eigenvalues and Eigenvectors
An implementation of the polynomial evaluation (5.65) is given in Pro-
gram 41. This program receives in input the vectors dd and bb containing
the main and the upper diagonals of T. The output values pi(x) are stored,
for i = 0, . . . , n, in the vector p.
Program 41 - sturm : Sturm sequence evaluation
function [p]=sturm(dd,bb,x)
n=length(dd); p(1)=1; p(2)=d(1)-x;
for i=2:n, p(i+1)=(dd(i)-x)*p(i)-bb(i-1)ÀÜ2*p(i-1); end
A basic implementation of the Givens method is provided in Program
42. In input, ind is the pointer to the searched eigenvalue, while the other
parameters are similar to those in Program 41. In output the values of
the elements of sequences a(k), b(k) and c(k) are returned, together with
the required number of iterations niter and the sequence of sign changes
s(c(k)).
Program 42 - givsturm : Givens method using the Sturm sequence
function [ak,bk,ck,nch,niter]=givsturm(dd,bb,ind,toll)
[a, b]=bound(dd,bb); dist=abs(b-a); s=abs(b)+abs(a);
n=length(d); niter=0; nch=[];
while (dist > (toll*s)),
niter=niter+1; c=(b+a)/2;
ak(niter)=a; bk(niter)=b; ck(niter)=c;
nch(niter)=chcksign(dd,bb,c);
if (nch(niter) > (n-ind)), b=c;
else, a=c; end; dist=abs(b-a); s=abs(b)+abs(a);
end
Program 43 - chcksign : Sign changes in the Sturm sequence
function nch=chcksign(dd,bb,x)
[p]=sturm(dd,bb,x); n=length(dd); nch=0; s=0;
for i=2:(n+1),
if ((p(i)*p(i-1)) <= 0), nch=nch+1; end
if (p(i)==0), s=s+1; end
end
nch=nch-s;
Program 44 - bound : Calculation of the interval J = [Œ±, Œ≤]
function [alfa,beta]=bound(dd,bb)
n=length(dd); alfa=dd(1)-abs(bb(1)); temp=dd(n)-abs(bb(n-1));
if (temp < alfa), alfa=temp; end;
for i=2:(n-1),
temp=dd(i)-abs(bb(i-1))-abs(bb(i));

5.11 The Lanczos Method
233
if (temp < alfa), alfa=temp; end;
end
beta=dd(1)+abs(bb(1)); temp=dd(n)+abs(bb(n-1));
if (temp > beta), beta=temp; end;
for i=2:(n-1),
temp=dd(i)+abs(bb(i-1))+abs(bb(i));
if (temp > beta), beta=temp; end;
end
5.11
The Lanczos Method
Let A ‚ààRn√ón be a symmetric sparse matrix, whose (real) eigenvalues are
ordered as
Œª1 ‚â•Œª2 ‚â•. . . ‚â•Œªn‚àí1 ‚â•Œªn.
(5.66)
When n is very large, the Lanczos method [Lan50] described in Section
4.4.3 can be applied to approximate the extremal eigenvalues Œªn and Œª1. It
generates a sequence of tridiagonal matrices Hm whose extremal eigenvalues
rapidly converge to the extremal eigenvalues of A.
To estimate the convergence of the tridiagonalization process, we intro-
duce the Rayleigh quotient r(x) = (xT Ax)/(xT x) associated with a nonnull
vector x ‚ààRn. The following result, known as Courant-Fisher Theorem,
holds (for the proof see [GL89], p. 394)
Œª1(A) = max
x‚ààRn
xÃ∏=0
r(x),
Œªn(A) = min
x‚ààRn
xÃ∏=0
r(x).
Its application to the matrix Hm = VT
mAVm, yields
Œª1(Hm) = maxx‚ààRn
xÃ∏=0
(Vmx)T A(Vmx)
xT x
= max
‚à•x‚à•2=1 r(Hmx) ‚â§Œª1(A)
Œªm(Hm) = minx‚ààRn
xÃ∏=0
(Vmx)T A(Vmx)
xT x
= min
‚à•x‚à•2=1 r(Hmx) ‚â•Œªn(A).
(5.67)
At each step of the Lanczos method, the estimates (5.67) provide a lower
and upper bound for the extremal eigenvalues of A. The convergence of the
sequences {Œª1(Hm)} and {Œªm(Hm)} to Œª1 and Œªn, respectively, is governed
by the following property, for whose proof we refer to [GL89], pp. 475-477.
Property 5.12 Let A ‚ààRn√ón be a symmetric matrix with eigenvalues
ordered as in (5.66) and let u1, . . . , un be the corresponding orthonormal
eigenvectors. If Œ∑1, . . . , Œ∑m denote the eigenvalues of Hm, with Œ∑1 ‚â•Œ∑2 ‚â•
. . . ‚â•Œ∑m, then
Œª1 ‚â•Œ∑1 ‚â•Œª1 ‚àí(Œª1 ‚àíŒªn)(tan œÜ1)2
(Tm‚àí1(1 + 2œÅ1))2 ,

234
5. Approximation of Eigenvalues and Eigenvectors
where cos œÜ1 = |(q(1))T u1|, œÅ1 = (Œª1 ‚àíŒª2)/(Œª2 ‚àíŒªn) and Tm‚àí1(x) is the
Chebyshev polynomial of degree m ‚àí1 (see Section 10.1.1).
A similar result holds of course for the convergence estimate of the eigen-
values Œ∑m to Œªn
Œªn ‚â§Œ∑m ‚â§Œªn + (Œª1 ‚àíŒªn)(tan œÜn)2
(Tm‚àí1(1 + 2œÅn))2 ,
where œÅn = (Œªn‚àí1 ‚àíŒªn)/(Œª1 ‚àíŒªn‚àí1) and cos œÜn = |(q(n))T un|.
A naive implementation of the Lanczos algorithm can be aÔ¨Äected by nu-
merical instability due to propagation of rounding errors. In particular, the
Lanczos vectors will not verify the mutual orthogonality relation, making
the extremal properties (5.67) false. This requires careful programming of
the Lanczos iteration by incorporating suitable reorthogonalization proce-
dures as described in [GL89], Sections 9.2.3-9.2.4.
Despite this limitation, the Lanczos method has two relevant features:
it preserves the sparsity pattern of the matrix (unlike Householder tridiag-
onalization), and such a property makes it quite attractive when dealing
with large size matrices; furthermore, it converges to the extremal eigen-
values of A much more rapidly than the power method does (see [Kan66],
[GL89], p. 477).
The Lanczos method can be generalized to compute the extremal eigen-
values of an unsymmetric matrix along the same lines as in Section 4.5
in the case of the solution of a linear system. Details on the practical im-
plementation of the algorithm and a theoretical convergence analysis can
be found in [LS96] and [Jia95], while some documentation of the latest
software can be found in NETLIB/scalapack/readme.arpack (see also the
MATLAB command eigs).
An implementation of the Lanczos algorithm is provided in Program 45.
The input parameter m is the size of the Krylov subspace in the tridiago-
nalization procedure, while toll is a tolerance monitoring the size of the
increment of the computed eigenvalues between two successive iterations.
The output vectors lmin, lmax and deltaeig contain the sequences of the
approximate extremal eigenvalues and of their increments between succes-
sive iterations. Program 42 is invoked for computing the eigenvalues of the
tridiagonal matrix Hm.
Program 45 - eiglancz : Extremal eigenvalues of a symmetric matrix
function [lmin,lmax,deltaeig,k]=eiglancz(A,m,toll)
n=size(A); V=[0*[1:n]‚Äô,[1,0*[1:n-1]]‚Äô];
beta(1)=0; normb=1; k=1; deltaeig(1)=1;
while k <= m & normb >= eps & deltaeig(k) < toll

5.12 Applications
235
vk = V(:,k+1);
w = A*vk-beta(k)*V(:,k);
alpha(k)= w‚Äô*vk; w = w - alpha(k)*vk;
normb = norm(w,2); beta(k+1)=normb;
if normb Àú= 0
V=[V,w/normb];
if k==1
lmin(1)=alpha; lmax(1)=alpha;
k=k+1; deltaeig(k)=1;
else
d=alpha; b=beta(2:length(beta)-1);
[ak,bk,ck,nch,niter]=givsturm(d,b,1,toll);
lmax(k)=(ak(niter)+bk(niter))/2;
[ak,bk,ck,nch,niter]=givsturm(d,b,k,toll);
lmin(k)=(ak(niter)+bk(niter))/2;
deltaeig(k+1)=max(abs(lmin(k)-lmin(k-1)),abs(lmax(k)-lmax(k-1)));
k=k+1;
end
else
disp(‚ÄôBreakdown‚Äô);
d=alpha; b=beta(2:length(beta)-1);
[ak,bk,ck,nch,niter]=givsturm(d,b,1,toll);
lmax(k)=(ak(niter)+bk(niter))/2;
[ak,bk,ck,nch,niter]=givsturm(d,b,k,toll);
lmin(k)=(ak(niter)+bk(niter))/2;
deltaeig(k+1)=max(abs(lmin(k)-lmin(k-1)),abs(lmax(k)-lmax(k-1)));
k=k+1;
end
end
k=k-1;
return
Example 5.18 Consider the eigenvalue problem for the matrix A‚ààRn√ón with
n = 100, having diagonal entries equal to 2 and oÔ¨Ä-diagonal entries equal to -1
on the upper and lower tenth diagonal. Program 45, with m=100 and toll=eps,
takes 10 iterations to approximate the extremal eigenvalues of A with an absolute
error of the order of the machine precision.
‚Ä¢
5.12
Applications
A classical problem in engineering is to determine the proper or natural
frequencies of a system (mechanical, structural or electric). Typically, this
leads to solving a matrix eigenvalue problem. Two examples coming from
structural applications are presented in the forthcoming sections where the
buckling problem of a beam and the study of the free vibrations of a bridge
are considered.

236
5. Approximation of Eigenvalues and Eigenvectors
5.12.1
Analysis of the Buckling of a Beam
Consider the homogeneous and thin beam of length L shown in Figure
5.4. The beam is simply supported at the end and is subject to a normal
compression load P at x = L. Denote by y(x) the vertical displacement
of the beam; the structure constraints demand that y(0) = y(L) = 0. Let
y
L
P
x
FIGURE 5.4. A simply supported beam subject to a normal compression load
us consider the problem of the buckling of the beam. This amounts to
determining the critical load Pcr, i.e. the smallest value of P such that an
equilibrium conÔ¨Åguration of the beam exists which is diÔ¨Äerent from being
rectilinear. Reaching the condition of critical load is a warning of structure
instability, so that it is quite important to determine its value accurately.
The explicit computation of the critical load can be worked out under
the assumption of small displacements, writing the equilibrium equation
for the structure in its deformed conÔ¨Åguration (drawn in dashed line in
Figure 5.4)
"
‚àíE (J(x)y‚Ä≤(x))‚Ä≤ = Me(x),
0 < x < L
y(0) = y(L) = 0,
(5.68)
where E is the constant Young‚Äôs modulus of the beam and Me(x) = Py(x)
is the momentum of the load P with respect to a generic point of the beam
of abscissa x. In (5.68) we are assuming that the momentum of inertia
J can be varying along the beam, which indeed happens if the beam has
nonuniform cross-section.
Equation (5.68) expresses the equilibrium between the external momen-
tum Me and the internal momentum Mi = ‚àíE(Jy‚Ä≤)‚Ä≤ which tends to restore
the rectilinear equilibrium conÔ¨Åguration of the beam. If the stabilizing re-
action Mi prevails on the unstabilizing action Me, the equilibrium of the
initial rectilinear conÔ¨Åguration is stable. The critical situation (buckling of
the beam) clearly arises when Mi = Me.
Assume that J is constant and let Œ±2 = P/(EJ); solving the boundary value
problem (5.68), we get the equation C sin Œ±L = 0, which admits nontrivial

5.12 Applications
237
solutions Œ± = (kœÄ)/L, k = 1, 2, . . . . Taking k = 1 yields the value of the
critical load Pcr = œÄ2EJ
L2 .
To solve numerically the boundary value problem (5.68) it is conve-
nient to introduce for n ‚â•1, the discretization nodes xj = jh, with
h = L/(n + 1) and j = 1, . . . , n, thus deÔ¨Åning the vector of nodal ap-
proximate displacements uj at the internal nodes xj (where u0 = y(0) = 0,
un+1 = y(L) = 0). Then, using the Ô¨Ånite diÔ¨Äerence method (see Section
12.2), the calculation of the critical load amounts to determining the small-
est eigenvalue of the tridiagonal symmetric and positive deÔ¨Ånite matrix
A = tridiagn(‚àí1, 2, ‚àí1) ‚ààRn√ón.
It can indeed be checked that the Ô¨Ånite diÔ¨Äerence discretization of prob-
lem (5.68) by centered diÔ¨Äerences leads to the following matrix eigenvalue
problem
Au = Œ±2h2u,
where u ‚ààRn is the vector of nodal displacements uj. The discrete coun-
terpart of condition C sin(Œ±) = 0 requires that Ph2/(EJ) coincides with
the eigenvalues of A as P varies.
Denoting by Œªmin and P h
cr, the smallest eigenvalue of A and the (approx-
imate) value of the critical load, respectively, then P h
cr = (ŒªminEJ)/h2.
Letting Œ∏ = œÄ/(n + 1), it can be checked (see Exercise 3, Chapter 4) that
the eigenvalues of matrix A are
Œªj = 2(1 ‚àícos(jŒ∏)),
j = 1, . . . , n.
(5.69)
The numerical calculation of Œªmin has been carried out using the Givens
algorithm described in Section 5.10.2 and assuming n = 10. Running the
Program 42 with an absolute tolerance equal to the roundoÔ¨Äunit, the
solution Œªmin ‚âÉ0.081 has been obtained after 57 iterations.
It is also interesting to analyze the case where the beam has nonuniform
cross-section, since the value of the critical load, unlike the previous situa-
tion, is not exactly known a priori. We assume that, for each x ‚àà[0, L], the
section of the beam is rectangular, with depth a Ô¨Åxed and height œÉ that
varies according to the rule
œÉ(x) = s

1 +
S
s ‚àí1
 + x
L ‚àí1
,2
,
0 ‚â§x ‚â§L,
where S and s are the values at the ends, with S ‚â•s > 0. The momentum
of inertia, as a function of x, is given by J(x) = (1/12)aœÉ3(x); proceeding
similarly as before, we end up with a system of linear algebraic equations
of the form
ÀúAu = (P/E)h2u,
where this time ÀúA = tridiagn(b, d, b) is a tridiagonal, symmetric and posi-
tive deÔ¨Ånite matrix having diagonal entries di = J(xi‚àí1/2) + J(xi+1/2), for
i = 1, . . . , n, and oÔ¨Ä-diagonal entries bi = ‚àíJ(xi+1/2), for i = 1, . . . , n ‚àí1.

238
5. Approximation of Eigenvalues and Eigenvectors
Assume the following values of the parameters: a = 0.4 [m], s = a, S =
0.5 [m] and L = 10 [m]. To ensure a correct dimensional comparison, we
have multiplied by ¬ØJ = a4/12 the smallest eigenvalue of the matrix A in the
uniform case (corresponding to S = s = a), obtaining Œªmin = 1.7283¬∑10‚àí4.
Running Program 42, with n = 10, yields in the nonuniform case the value
Œªmin = 2.243 ¬∑ 10‚àí4. This result conÔ¨Årms that the critical load increases
for a beam having a wider section at x = 0, that is, the structure enters
the instability regime for higher values of the load than in the uniform
cross-section case.
5.12.2
Free Dynamic Vibration of a Bridge
We are concerned with the analysis of the free response of a bridge whose
schematic structure is shown in Figure 5.5. The number of the nodes of
the structure is equal to 2n while the number of the beams is 5n. Each
horizontal and vertical beam has a mass equal to m while the diagonal
beams have mass equal to m
‚àö
2. The stiÔ¨Äness of each beam is represented
by the spring constant Œ∫. The nodes labeled by ‚Äú0‚Äù and ‚Äú2n + 1‚Äù are
constrained to ground.
2n + 1
n + 2
2n ‚àí2
2n ‚àí1
n + 3
n + 1
2n
0
1
2
3
n ‚àí2
n ‚àí1
n
FIGURE 5.5. Schematic structure of a bridge
Denoting by x and y the vectors of the 2n nodal horizontal and vertical
displacements the free response of the bridge can be studied by solving the
generalized eigenvalue problems
Mx = ŒªKx,
My = ŒªKy,
(5.70)
where M = mdiag2n(Œ±, b, Œ±, Œ≥, b, Œ≥), where Œ± = 3 +
‚àö
2, b = (Œ≤, . . . , Œ≤)T ‚àà
Rn‚àí2 with Œ≤ = 3/2 +
‚àö
2 and Œ≥ = 1 +
‚àö
2,
K = Œ∫

K11
K12
K12
K11

for a positive constant Œ∫ and where K12 = tridiagn(‚àí1, ‚àí1, ‚àí1), K11 =
tridiagn(‚àí1, d, ‚àí1) with d = (4, 5, . . . , 5, 4)T ‚ààRn. The diagonal matrix
M is the mass matrix while the symmetric and positive deÔ¨Ånite matrix K
is the stiÔ¨Äness matrix.

5.12 Applications
239
0
10
20
30
40
50
60
70
80
90
100
0
10
20
30
40
50
60
70
80
90
100
FIGURE 5.6. Iterations number of the Lanczos method and of the inverse power
method versus the size 2n of matrix C. The solid and the dash-dotted curves
refer to the inverse power method (for ÀúŒª2n and ÀúŒª2n‚àí1 respectively), while the
dashed and the dotted curves refer to the Lanczos method (still for ÀúŒª2n and
ÀúŒª2n‚àí1, respectively)
For k = 1, . . . , 2n we denote by (Œªk, zk) any eigenvalue/eigenvector pair
of (5.70) and call œâk = ‚àöŒªk the natural frequencies and zk the modes of
vibration of the bridge. The study of the free vibrations is of primary im-
portance in the design of a structure like a bridge or a multi-story building.
Indeed, if the excitation frequency of an external force (vehicles, wind or,
even worse, an earthquake) coincides with one of the natural frequencies of
the structure then a condition of resonance occurs and, as a result, large
oscillations may dangerously arise.
Let us now deal with the numerical solution of the matrix eigenvalue
problem (5.70). For this purpose we introduce the change of variable z =
M1/2x (or z = M1/2y) so that each generalized eigenvalue problem in (5.70)
can be conveniently reformulated as
Cz = ÀúŒªz
where ÀúŒª = 1/Œª and the matrix C = M‚àí1/2KM‚àí1/2 is symmetric positive
deÔ¨Ånite. This property allows us to use the Lanczos method described in
Section 5.11 and also ensures quadratic convergence of the power iterations
(see Section 5.11).
We approximate the Ô¨Årst two subdominant eigenvalues ÀúŒª2n and ÀúŒª2n‚àí1
of the matrix C (i.e., its smallest and second smallest eigenvalues) in the
case m = Œ∫ = 1 using the deÔ¨Çation procedure considered in Remark 5.3.
The inverse power iteration and the Lanczos method are compared in the
computation of ÀúŒª2n and ÀúŒª2n‚àí1 in Figure 5.6.
The results show the superiority of the Lanczos method over the inverse
iterations only when the matrix C is of small size. This is to be ascribed
to the fact that, as n grows, the progressive inÔ¨Çuence of the rounding er-

240
5. Approximation of Eigenvalues and Eigenvectors
rors causes a loss of mutual orthogonality of the Lanczos vectors and, in
turn, an increase in the number of iterations for the method to converge.
Suitable reorthogonalization procedures are thus needed to improve the
performances of the Lanczos iteration as pointed out in Section 5.11.
We conclude the free response analysis of the bridge showing in Figure
5.7 (in the case n = 5, m = 10 and Œ∫ = 1) the modes of vibration z8
and z10 corresponding to the natural frequencies œâ8 = 990.42 and œâ10 =
2904.59. The MATLAB built-in function eig has been employed to solve
the generalized eigenvalue problems (5.70) as explained in Section 5.9.1.
‚àí1
0
1
2
3
4
5
6
7
‚àí0.5
0
0.5
1
1.5
‚àí1
0
1
2
3
4
5
6
7
‚àí0.5
0
0.5
1
1.5
FIGURE 5.7. Modes of vibration corresponding to the natural frequencies œâ8
(left) and œâ10 (right). The undeformed conÔ¨Åguration of the bridge is drawn in
dotted line
5.13
Exercises
1. Using the Gershgorin theorems, localize the eigenvalues of the matrix A
which is obtained setting A = (P‚àí1DP)T and then a13 = 0, a23 = 0, where
D=diag3(1, 50, 100) and
P =
Ô£Æ
Ô£ØÔ£ØÔ£∞
1
1
1
10
20
30
100
50
60
Ô£π
Ô£∫Ô£∫Ô£ª.
[Solution : œÉ(A) = {‚àí151.84, 80.34, 222.5}.]
2. Localize the spectrum of the matrix
A =
Ô£Æ
Ô£ØÔ£ØÔ£∞
1
2
‚àí1
2
7
0
‚àí1
0
5
Ô£π
Ô£∫Ô£∫Ô£ª.
[Solution : œÉ(A) ‚äÇ[‚àí2, 9].]

5.13 Exercises
241
3. Draw the oriented graph of the matrix
A =
Ô£Æ
Ô£ØÔ£ØÔ£∞
1
3
0
0
2
‚àí1
‚àí1
0
2
Ô£π
Ô£∫Ô£∫Ô£ª.
4. Check if the following matrices are reducible.
A1 =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
1
0
‚àí1
0
2
3
‚àí2
1
‚àí1
0
‚àí2
0
1
‚àí1
1
4
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
,
A2 =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
0
0
1
0
0
0
0
1
0
1
0
0
1
0
0
0
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
[Solution : A1, reducible; A2, irreducible.]
5. Provide an estimate of the number of complex eigenvalues of the matrix
A =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
‚àí4
0
0
0.5
0
2
2
4
‚àí3
1
0.5
0
‚àí1
0
0
0.5
0
0.2
3
0
2
0.5
‚àí1
3
4
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
[Hint : Check that A can be reduced to the form
A =
. M1
M2
0
M3
/
where M1 ‚ààR2√ó2 and M2 ‚ààR3√ó3. Then, study the eigenvalues of blocks M1
and M2 using the Gershgorin theorems and check that A has no complex
eigenvalues.]
6. Let A ‚ààCn√ón be a diagonal matrix and let $A = A + E be a perturbation
of A with eii = 0 for i = 1, . . . , n. Show that
|Œªi($A) ‚àíŒªi(A)| ‚â§
n

j=1
|eij|,
i = 1, . . . , n.
(5.71)
7. Apply estimate (5.71) to the case in which A and E are, for Œµ ‚â•0, the
matrices
A =
. 1
0
0
2
/
,
E =
. 0
Œµ
Œµ
0
/
.
[Solution : œÉ(A) = {1, 2} and œÉ($A) = {(3 ‚àì
‚àö
1 + 4Œµ2)/2}.]

242
5. Approximation of Eigenvalues and Eigenvectors
8. Check that Ô¨Ånding the zeros of a polynomial of degree ‚â§n with real coef-
Ô¨Åcients
pn(x) =
n

k=0
akxk = a0 + a1x + ... + anxn,
an Ã∏= 0,
ak ‚ààR, k = 0, . . . n
is equivalent to determining the spectrum of the Frobenius matrix C ‚àà
Rn√ón associated with pn (known as the companion matrix)
C =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
‚àí(an‚àí1/an)
‚àí(an‚àí2/an)
. . .
‚àí(a1/an)
‚àí(a0/an)
1
0
. . .
0
0
0
1
. . .
0
0
...
...
...
...
...
0
0
. . .
1
0
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.(5.72)
An important consequence of the result above is that, due to Abel‚Äôs theo-
rem, there exist in general no direct methods for computing the eigenvalues
of a given matrix, for n ‚â•5.
9. Show that if matrix A ‚ààCn√ón admits eigenvalue/eigenvector pairs (Œª, x),
then the matrix UHAU, with U unitary, admits eigenvalue/eigenvector
pairs

Œª, UHx

. (Similarity transformation using an orthogonal matrix).
10. Suppose that all the assumptions needed to apply the power method are
satisÔ¨Åed except for the requirement Œ±1 Ã∏= 0 (see Section 5.3.1). Show that
in such a case the sequence (5.17) converges to the eigenvalue/eigenvector
pair (Œª2, x2). Then, study experimentally the behaviour of the method,
computing the pair (Œª1, x1) for the matrix
A =
Ô£Æ
Ô£ØÔ£ØÔ£∞
1
‚àí1
2
‚àí2
0
5
6
‚àí3
6
Ô£π
Ô£∫Ô£∫Ô£ª.
For this, use Program 26, taking q(0) = 1T /
‚àö
3 and q(0) = w(0)/‚à•w(0)‚à•2,
respectively, where w(0) = (1/3)x2 ‚àí(2/3)x3.
[Solution : Œª1 = 5, Œª2 = 3, Œª3 = ‚àí1 and x1 = [5, 16, 18]T , x2 = [1, 6, 4]T ,
x3 = [5, 16, 18]T .]
11. Show that the companion matrix associated with the polynomial pn(x) =
xn + anxn‚àí1 + . . . + a1, can be written in the alternative form (5.72)
A =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
0
a1
0
‚àí1
0
a2
...
...
...
‚àí1
0
an‚àí1
0
‚àí1
an
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
12. (From [FF63]) Suppose that a real matrix A ‚ààRn√ón has two maximum
module complex eigenvalues given by Œª1 = œÅeiŒ∏ and Œª2 = œÅe‚àíiŒ∏, with

5.13 Exercises
243
Œ∏ Ã∏= 0. Assume, moreover, that the remaining eigenvalues have modules
less than œÅ. The power method can then be modiÔ¨Åed as follows:
let q(0) be a real vector and q(k) be the vector provided by the power
method without normalization. Then, set xk = q(k)
n0 for some n0, with
1 ‚â§n0 ‚â§n. Prove that
œÅ2 = xkxk+2 ‚àíx2
k+1
xk‚àí1xk+1 ‚àíx2
k
+ O

‚à•Œª3
œÅ ‚à•k

,
cos(Œ∏) = œÅxk‚àí1 + r‚àí1xk+1
2xk
+ O

‚à•Œª3
œÅ ‚à•k

.
[Hint : Ô¨Årst, show that
xk = C(œÅk cos(kŒ∏ + Œ±)) + O

‚à•Œª3
œÅ ‚à•k

,
where Œ± depends on the components of the initial vector along the direc-
tions of the eigenvectors associated with Œª1 and Œª2.]
13. Apply the modiÔ¨Åed power method of Exercise 12 to the matrix
A =
Ô£Æ
Ô£∞
1
‚àí1
4
1
4
1
0
0
0
1
0
Ô£π
Ô£ª,
and compare the obtained results with those yielded by the standard power
method.
14. (Taken from [Dem97]). Apply the QR iteration with double shift to com-
pute the eigenvalues of the matrix
A =
Ô£Æ
Ô£∞
0
0
1
1
0
0
0
1
0
Ô£π
Ô£ª.
Run Program 37 setting toll=eps, itmax=100 and comment about the
form of the obtained matrix T(iter) after iter iterations of the algorithm.
[Solution : the eigenvalues of A are the solution of Œª3 ‚àí1 = 0, i.e.,
œÉ(A) =

1, ‚àí1/2 ¬±
‚àö
3/2i

. After iter=100 iterations, Program 37 yields
the matrix
T(100) =
Ô£Æ
Ô£∞
0
0
‚àí1
1
0
0
0
‚àí1
0
Ô£π
Ô£ª,
which means that the QR iteration leaves A unchanged (except for sign
changes that are non relevant for eigenvalues computation). This is a simple
but glaring example of matrix for which the QR method with double shift
fails to converge.]

6
RootÔ¨Ånding for Nonlinear Equations
This chapter deals with the numerical approximation of the zeros of a real-
valued function of one variable, that is
given f : I = (a, b) ‚äÜR ‚ÜíR, Ô¨Ånd Œ± ‚ààC such that f(Œ±) = 0.
(6.1)
The analysis of problem (6.1) in the case of systems of nonlinear equations
will be addressed in Chapter 7.
Methods for the numerical approximation of a zero of f are usually iter-
ative. The aim is to generate a sequence of values x(k) such that
lim
k‚Üí‚àûx(k) = Œ±.
The convergence of the iteration is characterized by the following deÔ¨Ånition.
DeÔ¨Ånition 6.1 A sequence

x(k)
generated by a numerical method is
said to converge to Œ± with order p ‚â•1 if
‚àÉC > 0 : |x(k+1) ‚àíŒ±|
|x(k) ‚àíŒ±|p ‚â§C, ‚àÄk ‚â•k0,
(6.2)
where k0 ‚â•0 is a suitable integer. In such a case, the method is said to be
of order p. Notice that if p is equal to 1, in order for x(k) to converge to
Œ± it is necessary that C < 1 in (6.2). In such an event, the constant C is
called the convergence factor of the method.
‚ñ†
Unlike the case of linear systems, convergence of iterative methods for
rootÔ¨Ånding of nonlinear equations depends in general on the choice of the

246
6. RootÔ¨Ånding for Nonlinear Equations
initial datum x(0). This allows for establishing only local convergence re-
sults, that is, holding for any x(0) which belongs to a suitable neighborhood
of the root Œ±. Methods for which convergence to Œ± holds for any choice of
x(0) in the interval I, are said to be globally convergent to Œ±.
6.1
Conditioning of a Nonlinear Equation
Consider the nonlinear equation f(x) = œï(x) ‚àíd = 0 and assume that f
is a continuously diÔ¨Äerentiable function. Let us analyze the sensitivity of
Ô¨Ånding the roots of f with respect to changes in the datum d.
The problem is well posed only if the function œï is invertible. In such a
case, indeed, one gets Œ± = œï‚àí1(d) from which, using the notation of Chapter
2, the resolvent G is œï‚àí1. On the other hand, (œï‚àí1)‚Ä≤(d) = 1/œï‚Ä≤(Œ±), so that
formula (2.7) for the approximate condition number (relative and absolute)
yields
K(d) ‚âÉ
|d|
|Œ±||f ‚Ä≤(Œ±)|,
Kabs(d) ‚âÉ
1
|f ‚Ä≤(Œ±)|.
(6.3)
The problem is thus ill-conditioned when f ‚Ä≤(Œ±) is ‚Äúsmall‚Äù and well-condi-
tioned if f ‚Ä≤(Œ±) is ‚Äúlarge‚Äù.
The analysis which leads to (6.3) can be generalized to the case in which
Œ± is a root of f with multiplicity m > 1 as follows. Expanding œï in a Taylor
series around Œ± up to the m-th order term, we get
d + Œ¥d = œï(Œ± + Œ¥Œ±) = œï(Œ±) +
m

k=1
œï(k)(Œ±)
k!
(Œ¥Œ±)k + o((Œ¥Œ±)m).
Since œï(k)(Œ±) = 0 for k = 1, . . . , m ‚àí1, we obtain
Œ¥d = f (m)(Œ±)(Œ¥Œ±)m/m!
so that an approximation to the absolute condition number is
Kabs(d) ‚âÉ

m!Œ¥d
f (m)(Œ±)

1/m
1
|Œ¥d|.
(6.4)
Notice that (6.3) is the special case of (6.4) where m = 1. From this it also
follows that, even if Œ¥d is suÔ¨Éciently small to make |m!Œ¥d/f (m)(Œ±)| < 1,
Kabs(d) could nevertheless be a large number. We therefore conclude that
the problem of rootÔ¨Ånding of a nonlinear equation is well-conditioned if Œ±
is a simple root and |f ‚Ä≤(Œ±)| is deÔ¨Ånitely diÔ¨Äerent from zero, ill-conditioned
otherwise.
Let us now consider the following problem, which is closely connected
with the previous analysis. Assume d = 0 and let Œ± be a simple root of f;

6.1 Conditioning of a Nonlinear Equation
247
moreover, for ÀÜŒ± Ã∏= Œ±, let f(ÀÜŒ±) = ÀÜr Ã∏= 0. We seek a bound for the diÔ¨Äerence
ÀÜŒ± ‚àíŒ± as a function of the residual ÀÜr. Applying (6.3) yields
Kabs(0) ‚âÉ
1
|f ‚Ä≤(Œ±)|.
Therefore, letting Œ¥x = ÀÜŒ± ‚àíŒ± and Œ¥d = ÀÜr in the deÔ¨Ånition of Kabs (see
(2.5)), we get
|ÀÜŒ± ‚àíŒ±|
|Œ±|
‚â≤
|ÀÜr|
|f ‚Ä≤(Œ±)||Œ±|,
(6.5)
where the following convention has been adopted: if a ‚â§b and a ‚âÉc, then
we write a ‚â≤c. If Œ± has multiplicity m > 1, using (6.4) instead of (6.3) and
proceeding as above, we get
|ÀÜŒ± ‚àíŒ±|
|Œ±|
‚â≤

m!
|f (m)(Œ±)||Œ±|m
1/m
|ÀÜr|1/m.
(6.6)
These estimates will be useful in the analysis of stopping criteria for itera-
tive methods (see Section 6.5).
A remarkable example of a nonlinear problem is when f is a polynomial
pn of degree n, in which case it admits exactly n roots Œ±i, real or complex,
each one counted with its multiplicity. We want to investigate the sensitivity
of the roots of pn with respect to the changes of its coeÔ¨Écients.
To this end, let ÀÜpn = pn + qn, where qn is a perturbation polynomial of
degree n, and let ÀÜŒ±i be the corresponding roots of ÀÜpn. A direct use of (6.6)
yields for any root Œ±i the following estimate
Ei
rel = |ÀÜŒ±i ‚àíŒ±i|
|Œ±i|
‚â≤

m!
|p(m)
n
(Œ±i)||Œ±i|m
1/m
|qn(ÀÜŒ±i)|1/m = Si,
(6.7)
where m is the multiplicity of the root at hand and qn(ÀÜŒ±i) = ‚àípn(ÀÜŒ±i) is
the ‚Äúresidual‚Äù of the polynomial pn evaluated at the perturbed root.
Remark 6.1 A formal analogy exists between the a priori estimates so
far obtained for the nonlinear problem œï(Œ±) = d and those developed in
Section 3.1.2 for linear systems, provided that A corresponds to œï and b
to d. More precisely, (6.5) is the analogue of (3.9) if Œ¥A=0, and the same
holds for (6.7) (for m = 1) if Œ¥b = 0.
‚ñ†
Example 6.1 Let p4(x) = (x‚àí1)4, and let ÀÜp4(x) = (x‚àí1)4 ‚àíŒµ, with 0 < Œµ ‚â™1.
The roots of the perturbed polynomial are simple and equal to ÀÜŒ±i = Œ±i +
4‚àöŒµ,
where Œ±i = 1 are the (coincident) zeros of p4. They lie with intervals of œÄ/2 on
the circle of radius
4‚àöŒµ and center z = (1, 0) in the complex plane.

248
6. RootÔ¨Ånding for Nonlinear Equations
The problem is stable (that is limŒµ‚Üí0 ÀÜŒ±i = 1), but is ill-conditioned since
|ÀÜŒ±i ‚àíŒ±i|
|Œ±i|
=
4‚àöŒµ,
i = 1, . . . 4,
For example, if Œµ = 10‚àí4 the relative change is 10‚àí1. Notice that the right-side
of (6.7) is just
4‚àöŒµ, so that, in this case, (6.7) becomes an equality.
‚Ä¢
Example 6.2 (Wilkinson). Consider the following polynomial
p10(x) = Œ†10
k=1(x + k) = x10 + 55x9 + . . . + 10!.
Let ÀÜp10 = p10 + Œµx9, with Œµ = 2‚àí23 ‚âÉ1.2 ¬∑ 10‚àí7. Let us study the conditioning of
Ô¨Ånding the roots of p10. Using (6.7) with m = 1, we report for i = 1, . . . , 10 in
Table 6.1 the relative errors Ei
rel and the corresponding estimates Si.
These results show that the problem is ill-conditioned, since the maximum
relative error for the root Œ±8 = ‚àí8 is three orders of magnitude larger than
the corresponding absolute perturbation. Moreover, excellent agreement can be
observed between the a priori estimate and the actual relative error.
‚Ä¢
i
Ei
rel
Si
i
Ei
rel
Si
1
3.039 ¬∑ 10‚àí13
3.285 ¬∑ 10‚àí13
6
6.956 ¬∑ 10‚àí5
6.956 ¬∑ 10‚àí5
2
7.562 ¬∑ 10‚àí10
7.568 ¬∑ 10‚àí10
7
1.589 ¬∑ 10‚àí4
1.588 ¬∑ 10‚àí4
3
7.758 ¬∑ 10‚àí8
7.759 ¬∑ 10‚àí8
8
1.984 ¬∑ 10‚àí4
1.987 ¬∑ 10‚àí4
4
1.808 ¬∑ 10‚àí6
1.808 ¬∑ 10‚àí6
9
1.273 ¬∑ 10‚àí4
1.271 ¬∑ 10‚àí4
5
1.616 ¬∑ 10‚àí5
1.616 ¬∑ 10‚àí5
10
3.283 ¬∑ 10‚àí5
3.286 ¬∑ 10‚àí5
TABLE 6.1. Relative error and estimated error using (6.7) for the Wilkinson
polynomial of degree 10
6.2
A Geometric Approach to RootÔ¨Ånding
In this section we introduce the following methods for Ô¨Ånding roots: the
bisection method, the chord method, the secant method, the false position
(or Regula Falsi) method and Newton‚Äôs method. The order of the presen-
tation reÔ¨Çects the growing complexity of the algorithms. In the case of the
bisection method, indeed, the only information that is being used is the sign
of the function f at the end points of any bisection (sub)interval, whilst
the remaining algorithms also take into account the values of the function
and/or its derivative.
6.2.1
The Bisection Method
The bisection method is based on the following property.

6.2 A Geometric Approach to RootÔ¨Ånding
249
Property 6.1 (theorem of zeros for continuous functions) Given a
continuous function f : [a, b] ‚ÜíR, such that f(a)f(b) < 0, then ‚àÉŒ± ‚àà(a, b)
such that f(Œ±) = 0.
Starting from I0 = [a, b], the bisection method generates a sequence of
subintervals Ik = [a(k), b(k)], k ‚â•0, with Ik ‚äÇIk‚àí1, k ‚â•1, and enjoys the
property that f(a(k))f(b(k)) < 0. Precisely, we set a(0) = a, b(0) = b and
x(0) = (a(0) + b(0))/2; then, for k ‚â•0:
set a(k+1) = a(k), b(k+1) = x(k)
if f(x(k))f(a(k)) < 0;
set a(k+1) = x(k), b(k+1) = b(k)
if f(x(k))f(b(k)) < 0;
Ô¨Ånally, set x(k+1) = (a(k+1) + b(k+1))/2.
x(1)
x
y
f(x)
a
b
Œ±
I1
I0
x(0)
0
5
10
15
20
25
30
10
‚àí12
10
‚àí10
10
‚àí8
10
‚àí6
10
‚àí4
10
‚àí2
10
0
FIGURE 6.1. The bisection method. The Ô¨Årst two steps (left); convergence history
for the Example 6.3 (right). The number of iterations and the absolute error as
a function of k are reported on the x- and y-axis, respectively
The bisection iteration terminates at the m-th step for which |x(m)‚àíŒ±| ‚â§
|Im| ‚â§Œµ, where Œµ is a Ô¨Åxed tolerance and |Im| is the length of Im. As for
the speed of convergence of the bisection method, notice that |I0| = b ‚àía,
while
|Ik| = |I0|/2k = (b ‚àía)/2k,
k ‚â•0.
(6.8)
Denoting by e(k) = x(k) ‚àíŒ± the absolute error at step k, from (6.8) it
follows that |e(k)| ‚â§(b ‚àía)/2k, k ‚â•0, which implies limk‚Üí‚àû|e(k)| = 0.
The bisection method is therefore globally convergent. Moreover, to get
|x(m) ‚àíŒ±| ‚â§Œµ we must take
m ‚â•log2(b ‚àía) ‚àílog2(Œµ) = log((b ‚àía)/Œµ)
log(2)
‚âÉlog((b ‚àía)/Œµ)
0.6931
.
(6.9)
In particular, to gain a signiÔ¨Åcant Ô¨Ågure in the accuracy of the approxi-
mation of the root (that is, to have |x(k) ‚àíŒ±| = |x(j) ‚àíŒ±|/10), one needs

250
6. RootÔ¨Ånding for Nonlinear Equations
k‚àíj = log2(10) ‚âÉ3.32 bisections. This singles out the bisection method as
an algorithm of certain, but slow, convergence. We must also point out that
the bisection method does not generally guarantee a monotone reduction
of the absolute error between two successive iterations, that is, we cannot
ensure a priori that
|e(k+1)| ‚â§Mk|e(k)|,
for any k ‚â•0,
(6.10)
with Mk < 1. For this purpose, consider the situation depicted in Figure
6.1 (left), where clearly |e(1)| > |e(0)|. Failure to satisfy (6.10) does not
allow for qualifying the bisection method as a method of order 1, in the
sense of DeÔ¨Ånition 6.1.
Example 6.3 Let us check the convergence properties of the bisection method
in the approximation of the root Œ± ‚âÉ0.9062 of the Legendre polynomial of degree
5
L5(x) = x
8 (63x4 ‚àí70x2 + 15),
whose roots lie within the interval (‚àí1, 1) (see Section 10.1.2). Program 46 has
been run taking a = 0.6, b = 1 (whence, L5(a) ¬∑ L5(b) < 0), nmax = 100,
toll = 10‚àí10 and has reached convergence in 32 iterations, this agrees with
the theoretical estimate (6.9) (indeed, m ‚â•31.8974). The convergence history is
reported in Figure 6.1 (right) and shows an (average) reduction of the error by a
factor of two, with an oscillating behavior of the sequence {x(k)}.
‚Ä¢
The slow reduction of the error suggests employing the bisection method
as an ‚Äúapproaching‚Äù technique to the root. Indeed, taking few bisection
steps, a reasonable approximation to Œ± is obtained, starting from which a
higher order method can be successfully used for a rapid convergence to
the solution within the Ô¨Åxed tolerance. An example of such a procedure
will be addressed in Section 6.7.1.
The bisection algorithm is implemented in Program 46. The input pa-
rameters, here and in the remainder of this chapter, have the following
meaning: a and b denote the end points of the search interval, fun is the
variable containing the expression of the function f, toll is a Ô¨Åxed toler-
ance and nmax is the maximum admissible number of steps for the iterative
process.
In the output vectors xvect, xdif and fx the sequences {x(k)}, {|x(k+1)‚àí
x(k)|} and {f(x(k))}, for k ‚â•0, are respectively stored, while nit denotes
the number of iterations needed to satisfy the stopping criteria. In the case
of the bisection method, the code returns as soon as the half-length of the
search interval is less than toll.
Program 46 - bisect : Bisection method
function [xvect,xdif,fx,nit]=bisect(a,b,toll,nmax,fun)
err=toll+1; nit=0; xvect=[]; fx=[]; xdif=[];
while (nit < nmax & err > toll)

6.2 A Geometric Approach to RootÔ¨Ånding
251
nit=nit+1; c=(a+b)/2; x=c; fc=eval(fun); xvect=[xvect;x];
fx=[fx;fc]; x=a; if (fc*eval(fun) > 0), a=c; else, b=c; end;
err=abs(b-a); xdif=[xdif;err];
end;
6.2.2
The Methods of Chord, Secant and Regula Falsi and
Newton‚Äôs Method
In order to devise algorithms with better convergence properties than the
bisection method, it is necessary to include information from the values
attained by f and, possibly, also by its derivative f ‚Ä≤ (if f is diÔ¨Äerentiable)
or by a suitable approximation.
For this purpose, let us expand f in a Taylor series around Œ± and truncate
the expansion at the Ô¨Årst order. The following linearized version of problem
(6.1) is obtained
f(Œ±) = 0 = f(x) + (Œ± ‚àíx)f ‚Ä≤(Œæ),
(6.11)
for a suitable Œæ between Œ± and x. Equation (6.11) prompts the following
iterative method: for any k ‚â•0, given x(k), determine x(k+1) by solving
equation f(x(k)) + (x(k+1) ‚àíx(k))qk = 0, where qk is a suitable approxima-
tion of f ‚Ä≤(x(k)).
The method described here amounts to Ô¨Ånding the intersection between
the x-axis and the straight line of slope qk passing through the point
(x(k), f(x(k))), and thus can be more conveniently set up in the form
x(k+1) = x(k) ‚àíq‚àí1
k f(x(k)),
‚àÄk ‚â•0.
We consider below four particular choices of qk.
f(x)
x(1)
y
b
a
f(x)
x(0)
x
y
b
a
x(1)
x
x(2)
Œ±
Œ±
x(3)
FIGURE 6.2. The Ô¨Årst step of the chord method (left) and the Ô¨Årst three steps
of the secant method (right). For this method we set x(‚àí1) = b and x(0) = a

252
6. RootÔ¨Ånding for Nonlinear Equations
The chord method. We let
qk = q = f(b) ‚àíf(a)
b ‚àía
,
‚àÄk ‚â•0
from which, given an initial value x(0), the following recursive relation is
obtained
x(k+1) = x(k) ‚àí
b ‚àía
f(b) ‚àíf(a)f(x(k)),
k ‚â•0.
(6.12)
In Section 6.3.1, we shall see that the sequence {x(k)} generated by (6.12)
converges to the root Œ± with order of convergence p = 1.
The secant method. We let
qk = f(x(k)) ‚àíf(x(k‚àí1))
x(k) ‚àíx(k‚àí1)
,
‚àÄk ‚â•0
(6.13)
from which, giving two initial values x(‚àí1) and x(0), we obtain the following
relation
x(k+1) = x(k) ‚àí
x(k) ‚àíx(k‚àí1)
f(x(k)) ‚àíf(x(k‚àí1))f(x(k)),
k ‚â•0.
(6.14)
If compared with the chord method, the iterative process (6.14) requires
an extra initial point x(‚àí1) and the corresponding function value f(x(‚àí1)),
as well as, for any k, computing the incremental ratio (6.13). The beneÔ¨Åt
due to the increase in the computational cost is the higher speed of con-
vergence of the secant method, as stated in the following property which
can be regarded as a Ô¨Årst example of the local convergence theorem (for the
proof see [IK66], pp. 99-101).
Property 6.2 Let f ‚ààC2(J ), J being a suitable neighborhood of the root
Œ± and assume that f ‚Ä≤‚Ä≤(Œ±) Ã∏= 0. Then, if the initial data x(‚àí1) and x(0) are
chosen in J suÔ¨Éciently close to Œ±, the sequence (6.14) converges to Œ± with
order p = (1 +
‚àö
5)/2 ‚âÉ1.63.
The Regula Falsi (or false position) method. This is a variant of
the secant method in which, instead of selecting the secant line through
the values (x(k), f(x(k)) and (x(k‚àí1), f(x(k‚àí1)), we take the one through
(x(k), f(x(k)) and (x(k‚Ä≤), f(x(k‚Ä≤)), k‚Ä≤ being the maximum index less than k
such that f(x(k‚Ä≤)) ¬∑ f(x(k)) < 0. Precisely, once two values x(‚àí1) and x(0)
have been found such that f(x(‚àí1)) ¬∑ f(x(0)) < 0, we let
x(k+1) = x(k) ‚àí
x(k) ‚àíx(k‚Ä≤)
f(x(k)) ‚àíf(x(k‚Ä≤))f(x(k)),
k ‚â•0.
(6.15)

6.2 A Geometric Approach to RootÔ¨Ånding
253
Having Ô¨Åxed an absolute tolerance Œµ, the iteration (6.15) terminates at the
m-th step such that |f(x(m))| < Œµ. Notice that the sequence of indices k‚Ä≤
is nondecreasing; therefore, in order to Ô¨Ånd at step k the new value of k‚Ä≤,
it is not necessary to sweep all the sequence back, but it suÔ¨Éces to stop at
the value of k‚Ä≤ that has been determined at the previous step. We show in
Figure 6.3 (left) the Ô¨Årst two steps of (6.15) in the special case in which
x(k‚Ä≤) coincides with x(‚àí1) for any k ‚â•0.
The Regula Falsi method, though of the same complexity as the secant
method, has linear convergence order (see, for example, [RR78], pp. 339-
340). However, unlike the secant method, the iterates generated by (6.15)
are all contained within the starting interval [x(‚àí1), x(0)].
In Figure 6.3 (right), the Ô¨Årst two iterations of both the secant and Regula
Falsi methods are shown, starting from the same initial data x(‚àí1) and x(0).
Notice that the iterate x(1) computed by the secant method coincides with
that computed by the Regula Falsi method, while the value x(2) computed
by the former method (and denoted in the Ô¨Ågure by x(2)
Sec) falls outside the
searching interval [x(‚àí1), x(0)].
In this respect, the Regula Falsi method, as well as the bisection method,
can be regarded as a globally convergent method.
f(x)
y
f(x)
x(‚àí1)
x(0)
x
x(1)
x(2)
y
x(0)
x
x(‚àí1)
x(1)
x(2)
x(2)
Sec
FIGURE 6.3. The Ô¨Årst two steps of the Regula Falsi method for two diÔ¨Äerent
functions
Newton‚Äôs method.
Assuming that f ‚ààC1(I) and that f ‚Ä≤(Œ±) Ã∏= 0 (i.e., Œ± is a simple root of f),
if we let
qk = f ‚Ä≤(x(k)),
‚àÄk ‚â•0
and assign the initial value x(0), we obtain the so called Newton‚Äôs method
x(k+1) = x(k) ‚àíf(x(k))
f ‚Ä≤(x(k)),
k ‚â•0.
(6.16)

254
6. RootÔ¨Ånding for Nonlinear Equations
x(2)
a
b
x
f(x)
y
x(1)
x(0)
0
5
10
15
20
25
30
35
10
‚àí15
10
‚àí10
10
‚àí5
10
0
(1)
(2)
(3)
(4)
FIGURE 6.4. The Ô¨Årst two steps of Newton‚Äôs method (left); convergence histories
in Example 6.4 for the chord method (1), bisection method (2), secant method
(3) and Newton‚Äôs method (4) (right). The number of iterations and the absolute
error as a function of k are shown on the x-axis and y-axis, respectively
At the k-th iteration, Newton‚Äôs method requires the two functional evalu-
ations f(x(k)) and f ‚Ä≤(x(k)). The increasing computational cost with respect
to the methods previously considered is more than compensated for by a
higher order of convergence, Newton‚Äôs method being of order 2 (see Section
6.3.1).
Example 6.4 Let us compare the methods introduced so far for the approxima-
tion of the root Œ± ‚âÉ0.5149 of the function f(x) = cos2(2x) ‚àíx2 in the interval
(0, 1.5). The tolerance Œµ on the absolute error has been taken equal to 10‚àí10 and
the convergence histories are drawn in Figure 6.4 (right). For all methods, the
initial guess x(0) has been set equal to 0.75. For the secant method we chose
x(‚àí1) = 0.
The analysis of the results singles out the slow convergence of the chord
method. The error curve for the Regula Falsi method is similar to that of se-
cant method, thus it was not reported in Figure 6.4.
It is interesting to compare the performances of Newton‚Äôs and secant methods
(both having order p > 1), in terms of their computational eÔ¨Äort. It can indeed
be proven that it is more convenient to employ the secant method whenever the
number of Ô¨Çoating point operations to evaluate f ‚Ä≤ are about twice those needed
for evaluating f (see [Atk89], pp. 71-73). In the example at hand, Newton‚Äôs
method converges to Œ± in 6 iterations, instead of 7, but the secant method takes
94 Ô¨Çops instead of 177 Ô¨Çops required by Newton‚Äôs method.
‚Ä¢
The chord, secant, Regula Falsi and Newton‚Äôs methods are implemented
in Programs 47, 48, 49 and 50, respectively. Here and in the rest of the
chapter, x0 and xm1 denote the initial data x(0) and x(‚àí1). In the case of
the Regula Falsi method the stopping test checks is |f(x(k))| < toll, while
for the other methods the test is |x(k+1) ‚àíx(k)| < toll. The string dfun
contains the expression of f ‚Ä≤ to be used in the Newton method.

6.2 A Geometric Approach to RootÔ¨Ånding
255
Program 47 - chord : The chord method
function [xvect,xdif,fx,nit]=chord(a,b,x0,nmax,toll,fun)
x=a; fa=eval(fun); x=b; fb=eval(fun); r=(fb-fa)/(b-a);
err=toll+1; nit=0; xvect=x0; x=x0; fx=eval(fun); xdif=[];
while (nit < nmax & err > toll),
nit=nit+1; x=xvect(nit); xn=x-fx(nit)/r; err=abs(xn-x);
xdif=[xdif; err]; x=xn; xvect=[xvect;x]; fx=[fx;eval(fun)];
end;
Program 48 - secant : The secant method
function [xvect,xdif,fx,nit]=secant(xm1,x0,nmax,toll,fun)
x=xm1; fxm1=eval(fun); xvect=[x]; fx=[fxm1]; x=x0; fx0=eval(fun);
xvect=[xvect;x]; fx=[fx;fx0]; err=toll+1; nit=0; xdif=[];
while (nit < nmax & err > toll),
nit=nit+1; x=x0-fx0*(x0-xm1)/(fx0-fxm1); xvect=[xvect;x];
fnew=eval(fun); fx=[fx;fnew]; err=abs(x0-x); xdif=[xdif;err];
xm1=x0; fxm1=fx0; x0=x; fx0=fnew;
end;
Program 49 - regfalsi : The Regula Falsi method
function [xvect,xdif,fx,nit]=regfalsi(xm1,x0,toll,nmax,fun)
nit=0; x=xm1; f=eval(fun); fx=[f]; x=x0; f=eval(fun); fx=[fx, f];
xvect=[xm1,x0]; xdif=[]; f=toll+1; kprime=1;
while (nit < nmax & (abs(f) > toll),
nit=nit+1; dim=length(xvect);
x=xvect(dim); fxk=eval(fun); xk=x; i=dim;
while (i >= kprime), i=i-1; x=xvect(i); fxkpr=eval(fun);
if ((fxkpr*fxk) < 0), xkpr=x; kprime=i; break; end;
end;
x=xk-fxk*(xk-xkpr)/(fxk-fxkpr); xvect=[xvect, x]; f=eval(fun);
fx=[fx, f]; err=abs(x-xkpr); xdif=[xdif, err];
end;
Program 50 - newton : Newton‚Äôs method
function [xvect,xdif,fx,nit]=newton(x0,nmax,toll,fun,dfun)
err=toll+1; nit=0; xvect=x0; x=x0; fx=eval(fun); xdif=[];
while (nit < nmax & err > toll),
nit=nit+1; x=xvect(nit); dfx=eval(dfun);
if (dfx == 0), err=toll*1.e-10;
disp(‚Äô Stop for vanishing dfun ‚Äô);
else,
xn=x-fx(nit)/dfx; err=abs(xn-x); xdif=[xdif; err];
x=xn; xvect=[xvect;x]; fx=[fx;eval(fun)];

256
6. RootÔ¨Ånding for Nonlinear Equations
end;
end;
6.2.3
The Dekker-Brent Method
The Dekker-Brent method combines the bisection and secant methods, pro-
viding a synthesis of the advantages of both. This algorithm carries out an
iteration in which three abscissas a, b and c are present at each stage. Nor-
mally, b is the latest iterate and closest approximation to the zero, a is
the previous iterate and c is the previous or an older iterate so that f(b)
and f(c) have opposite signs. At all times b and c bracket the zero and
|f(b)| ‚â§|f(c)|.
Once an interval [a, b] containing at least one root Œ± of the function
y = f(x) is found with f(a)f(b) < 0, the algorithm generates a sequence of
values a, b and c such that Œ± always lies between b and c and, at convergence,
the half-length |c ‚àíb|/2 is less than a Ô¨Åxed tolerance. If the function f is
suÔ¨Éciently smooth around the desired root, then the order of convergence
of the algorithm is more than linear (see [Dek69], [Bre73] Chapter 4 and
[Atk89], pp. 91-93).
In the following we describe the main lines of the algorithm as imple-
mented in the MATLAB function fzero. Throughout the parameter d will
be a correction to the point b since it is best to arrange formulae so that
they express the desired quantity as a small correction to a good approx-
imation. For example, if the new value of b were computed as (b + c)/2
(bisection step) a numerical cancellation might occur, while computing b
as b + (c ‚àíb)/2 gives a more stable formula.
Denote by Œµ a suitable tolerance (usually the machine precision) and let
c = b; then, the Dekker-Brent method proceeds as follows:
First, check if f(b) = 0. Should this be the case, the algorithm terminates
and returns b as the approximate zero of f. Otherwise, the following steps
are executed:
1. if f(b)f(c) > 0, set c = a, d = b ‚àía and e = d.
2. If |f(c)| < |f(b)|, perform the exchanges a = b, b = c and c = a.
3. Set Œ¥ = 2Œµ max {|b|, 1} and m = (c ‚àíb)/2. If |m| ‚â§Œ¥ or f(b) = 0 then
the algorithm terminates and returns b as the approximate zero of f.
4. Choose bisection or interpolation.
(a) If |e| < Œ¥ or |f(a)| ‚â§|f(b)| then a bisection step is taken, i.e., set
d = m and e = m; otherwise, the interpolation step is executed.
(b) if a = c execute linear interpolation, i.e., compute the zero of the
straight line passing through the points (b, f(b)) and (c, f(c)) as

6.3 Fixed-point Iterations for Nonlinear Equations
257
a correction Œ¥b to the point b. This amounts to taking a step of
the secant method on the interval having b and c as end points.
If a Ã∏= c execute inverse quadratic interpolation, i.e., construct
the second-degree polynomial with respect to y, that interpo-
lates at the points (f(a), a), (f(b), b) and (f(c), c) and its value
at y = 0 is computed as a correction Œ¥b to the point b. Notice
that at this stage the values f(a), f(b) and f(c) are diÔ¨Äerent one
from the others, being |f(a)| > |f(b)|, f(b)f(c) < 0 and a Ã∏= c.
Then the algorithm checks whether the point b + Œ¥b can be ac-
cepted. This is a rather technical issue but essentially it amounts
to ascertaining if the point is inside the current interval and not
too close to the end points. This guarantees that the length of
the interval decreases by a large factor when the function is well
behaved. If the point is accepted then e = d and d = Œ¥b, i.e.,
the interpolation is actually carried out, else a bisection step is
executed by setting d = m and e = m.
5. The algorithm now updates the current iterate. Set a = b and if
|d| > Œ¥ then b = b + d else b = b + Œ¥sign(m) and go back to step 1.
Example 6.5 Let us consider the Ô¨Ånding of roots of the function f considered in
Example 6.4, taking Œµ equal to the roundoÔ¨Äunit. The MATLAB function fzero
has been employed. It automatically determines the values a and b, starting from
a given initial guess Œæ provided by the user. Starting from Œæ = 1.5, the algorithm
Ô¨Ånds the values a = 0.3 and b = 2.1; convergence is achieved in 5 iterations and
the sequences of the values a, b, c and f(b) are reported in Table 6.2.
Notice that the tabulated values refer to the state of the algorithm before step
3., and thus, in particular, after possible exchanges between a and b.
‚Ä¢
k
a
b
c
f(b)
0
2.1
0.3
2.1
0.5912
1
0.3
0.5235
0.3
‚àí2.39 ¬∑ 10‚àí2
2
0.5235
0.5148
0.5235
3.11 ¬∑ 10‚àí4
3
0.5148
0.5149
0.5148
‚àí8.8 ¬∑ 10‚àí7
4
0.5149
0.5149
0.5148
‚àí3.07 ¬∑ 10‚àí11
TABLE 6.2. Solution of the equation cos2(2x) ‚àíx2 = 0 using the Dekker-Brent
algorithm. The integer k denotes the current iteration
6.3
Fixed-point Iterations for Nonlinear Equations
In this section a completely general framework for Ô¨Ånding the roots of a
nonlinear function is provided. The method is based on the fact that, for a
given f : [a, b] ‚ÜíR, it is always possible to transform the problem f(x) = 0

258
6. RootÔ¨Ånding for Nonlinear Equations
into an equivalent problem x ‚àíœÜ(x) = 0, where the auxiliary function
œÜ : [a, b] ‚ÜíR has to be chosen in such a way that œÜ(Œ±) = Œ± whenever
f(Œ±) = 0. Approximating the zeros of a function has thus become the
problem of Ô¨Ånding the Ô¨Åxed points of the mapping œÜ, which is done by the
following iterative algorithm:
given x(0), let
x(k+1) = œÜ(x(k)),
k ‚â•0.
(6.17)
We say that (6.17) is a Ô¨Åxed-point iteration and œÜ is its associated iteration
function. Sometimes, (6.17) is also referred to as Picard iteration or func-
tional iteration for the solution of f(x) = 0. Notice that by construction
the methods of the form (6.17) are strongly consistent in the sense of the
deÔ¨Ånition given in Section 2.2.
The choice of œÜ is not unique. For instance, any function of the form
œÜ(x) = x + F(f(x)), where F is a continuous function such that F(0) = 0,
is an admissible iteration function.
The next two results provide suÔ¨Écient conditions in order for the Ô¨Åxed-
point method (6.17) to converge to the root Œ± of problem (6.1). These
conditions are stated precisely in the following theorem.
Theorem 6.1 (convergence of Ô¨Åxed-point iterations) Consider the se-
quence x(k+1) = œÜ(x(k)), for k ‚â•0, being x(0) given. Assume that:
1. œÜ : [a, b] ‚Üí[a, b];
2. œÜ ‚ààC1([a, b]);
3. ‚àÉK < 1 : |œÜ‚Ä≤(x)| ‚â§K ‚àÄx ‚àà[a, b].
Then, œÜ has a unique Ô¨Åxed point Œ± in [a, b] and the sequence {x(k)} con-
verges to Œ± for any choice of x(0) ‚àà[a, b]. Moreover, we have
lim
k‚Üí‚àû
x(k+1) ‚àíŒ±
x(k) ‚àíŒ±
= œÜ‚Ä≤(Œ±).
(6.18)
Proof. The assumption 1. and the continuity of œÜ ensure that the iteration
function œÜ has at least one Ô¨Åxed point in [a, b]. Assumption 3. states that œÜ is
a contraction mapping and ensures the uniqueness of the Ô¨Åxed point. Indeed,
suppose that there exist two distinct values Œ±1, Œ±2 ‚àà[a, b] such that œÜ(Œ±1) = Œ±1
and œÜ(Œ±2) = Œ±2. Expanding œÜ in a Taylor series around Œ±1 and truncating it at
Ô¨Årst order, it follows that
|Œ±2 ‚àíŒ±1| = |œÜ(Œ±2) ‚àíœÜ(Œ±1)| = |œÜ‚Ä≤(Œ∑)(Œ±2 ‚àíŒ±1)| ‚â§K|Œ±2 ‚àíŒ±1| < |Œ±2 ‚àíŒ±1|,
for Œ∑ ‚àà(Œ±1, Œ±2), from which it must necessarily be that Œ±2 = Œ±1.
The convergence analysis for the sequence {x(k)} is again based on a Taylor
series expansion. Indeed, for any k ‚â•0 there exists a value Œ∑(k) between Œ± and
x(k) such that
x(k+1) ‚àíŒ± = œÜ(x(k)) ‚àíœÜ(Œ±) = œÜ‚Ä≤(Œ∑(k))(x(k) ‚àíŒ±)
(6.19)

6.3 Fixed-point Iterations for Nonlinear Equations
259
from which |x(k+1) ‚àíŒ±| ‚â§K|x(k) ‚àíŒ±| ‚â§Kk+1|x(0) ‚àíŒ±| ‚Üí0 for k ‚Üí‚àû. Thus,
x(k) converges to Œ± and (6.19) implies that
lim
k‚Üí‚àû
x(k+1) ‚àíŒ±
x(k) ‚àíŒ±
= lim
k‚Üí‚àûœÜ‚Ä≤(Œ∑(k)) = œÜ‚Ä≤(Œ±),
that is (6.18).
3
The quantity |œÜ‚Ä≤(Œ±)| is called the asymptotic convergence factor and, in
analogy with the case of iterative methods for linear systems, the asymp-
totic convergence rate can be deÔ¨Åned as
R = ‚àílog
1
|œÜ‚Ä≤(Œ±)|.
(6.20)
Theorem 6.1 ensures convergence of the sequence {x(k)} to the root Œ± for
any choice of the initial value x(0) ‚àà[a, b]. As such, it represents an example
of a global convergence result.
In practice, however, it is often quite diÔ¨Écult to determine a priori the
width of the interval [a, b]; in such a case the following convergence result
can be useful (see for the proof, [OR70]).
Property 6.3 (Ostrowski theorem) Let Œ± be a Ô¨Åxed point of a func-
tion œÜ, which is continuous and diÔ¨Äerentiable in a neighborhood J of Œ±. If
|œÜ‚Ä≤(Œ±)| < 1 then there exists Œ¥ > 0 such that the sequence {x(k)} converges
to Œ±, for any x(0) such that |x(0) ‚àíŒ±| < Œ¥.
Remark 6.2 If |œÜ‚Ä≤(Œ±)| > 1 it follows from (6.19) that if x(n) is suÔ¨Éciently
close to Œ±, so that |œÜ‚Ä≤(x(n))| > 1, then |Œ± ‚àíx(n+1)| > |Œ± ‚àíx(n)|, thus
no convergence is possible. In the case |œÜ‚Ä≤(Œ±)| = 1 no general conclusion
can be stated since both convergence and nonconvergence may be possible,
depending on the problem at hand.
‚ñ†
Example 6.6 Let œÜ(x) = x ‚àíx3, which admits Œ± = 0 as Ô¨Åxed point. Although
œÜ‚Ä≤(Œ±) = 1, if x(0) ‚àà[‚àí1, 1] then x(k) ‚àà(‚àí1, 1) for k ‚â•1 and it converges (very
slowly) to Œ± (if x(0) = ¬±1, we even have x(k) = Œ± for any k ‚â•1). Starting from
x(0) = 1/2 the absolute error after 2000 iterations is 0.0158. Let now œÜ(x) = x+x3
having also Œ± = 0 as Ô¨Åxed point. Again, œÜ‚Ä≤(Œ±) = 1 but in this case the sequence
x(k) diverges for any choice x(0) Ã∏= 0.
‚Ä¢
We say that a Ô¨Åxed-point method has order p (p non necessarily being an
integer) if the sequence that is generated by the method converges to the
Ô¨Åxed point Œ± with order p according to DeÔ¨Ånition 6.1.

260
6. RootÔ¨Ånding for Nonlinear Equations
Property 6.4 If œÜ ‚ààCp+1(J ) for a suitable neighborhood J of Œ± and an
integer p ‚â•0, and if œÜ(i)(Œ±) = 0 for 0 ‚â§i ‚â§p and œÜ(p+1)(Œ±) Ã∏= 0, then the
Ô¨Åxed-point method with iteration function œÜ has order p + 1 and
lim
k‚Üí‚àû
x(k+1) ‚àíŒ±
(x(k) ‚àíŒ±)p+1 = œÜ(p+1)(Œ±)
(p + 1)! ,
p ‚â•0.
(6.21)
Proof. Let us expand œÜ in a Taylor series around x = Œ± obtaining
x(k+1) ‚àíŒ± =
p

i=0
œÜ(i)(Œ±)
i!
(x(k) ‚àíŒ±)i + œÜ(p+1)(Œ∑)
(p + 1)! (x(k) ‚àíŒ±)p+1,
for a certain Œ∑ between x(k) and Œ±. Thus, we have
lim
k‚Üí‚àû
x(k+1) ‚àíŒ±
(x(k) ‚àíŒ±)p+1 = lim
k‚Üí‚àû
œÜ(p+1)(Œ∑)
(p + 1)! = œÜ(p+1)(Œ±)
(p + 1)! .
3
The convergence of the sequence to the root Œ± will be faster, for a Ô¨Åxed
order p, when the quantity at right-side in (6.21) is smaller.
The Ô¨Åxed-point method (6.17) is implemented in Program 51. The variable
phi contains the expression of the iteration function œÜ.
Program 51 - Ô¨Åxpoint : Fixed-point method
function [xvect,xdif,fx,nit]=Ô¨Åxpoint(x0,nmax,toll,fun,phi)
err=toll+1; nit=0; xvect=x0; x=x0; fx=eval(fun); xdif=[];
while (nit < nmax & err > toll),
nit=nit+1; x=xvect(nit); xn=eval(phi); err=abs(xn-x);
xdif=[xdif; err]; x=xn; xvect=[xvect;x]; fx=[fx;eval(fun)];
end;
6.3.1
Convergence Results for Some Fixed-point Methods
Theorem 6.1 provides a theoretical tool for analyzing some of the iterative
methods introduced in Section 6.2.2.
The chord method. Equation (6.12) is a special instance of (6.17), in
which we let œÜ(x) = œÜchord(x) = x‚àíq‚àí1f(x) = x‚àí(b‚àía)/(f(b)‚àíf(a))f(x).
If f ‚Ä≤(Œ±) = 0, œÜ‚Ä≤
chord(Œ±) = 1 and the method is not guaranteed to converge.
Otherwise, the condition |œÜ‚Ä≤
chord(Œ±)| < 1 is equivalent to requiring that
0 < q‚àí1f ‚Ä≤(Œ±) < 2.
Therefore, the slope q of the chord must have the same sign as f ‚Ä≤(Œ±),
and the search interval [a, b] has to satisfy the constraint
(b ‚àía) < 2f(b) ‚àíf(a)
f ‚Ä≤(Œ±)
.

6.4 Zeros of Algebraic Equations
261
The chord method converges in one iteration if f is a straight line, otherwise
it converges linearly, apart the (lucky) case when f ‚Ä≤(Œ±) = (f(b)‚àíf(a))/(b‚àí
a), for which œÜ‚Ä≤
chord(Œ±) = 0.
Newton‚Äôs method. Equation (6.16) can be cast in the general framework
(6.17) letting
œÜNewt(x) = x ‚àíf(x)
f ‚Ä≤(x).
Assuming f ‚Ä≤(Œ±) Ã∏= 0 (that is, Œ± is a simple root)
œÜ‚Ä≤
Newt(Œ±) = 0,
œÜ‚Ä≤‚Ä≤
Newt(Œ±) = f ‚Ä≤‚Ä≤(Œ±)
f ‚Ä≤(Œ±) .
If the root Œ± has multiplicity m > 1, then the method (6.16) is no longer
second-order convergent. Indeed we have (see Exercise 2)
œÜ‚Ä≤
Newt(Œ±) = 1 ‚àí1
m.
(6.22)
If the value of m is known a priori, then the quadratic convergence of
Newton‚Äôs method can be recovered by resorting to the so-called modiÔ¨Åed
Newton‚Äôs method
x(k+1) = x(k) ‚àím f(x(k))
f ‚Ä≤(x(k)),
k ‚â•0.
(6.23)
To check the convergence order of the iteration (6.23), see Exercise 2.
6.4
Zeros of Algebraic Equations
In this section we address the special case in which f is a polynomial of
degree n ‚â•0, i.e., a function of the form
pn(x) =
n

k=0
akxk,
(6.24)
where ak ‚ààR are given coeÔ¨Écients.
The above representation of pn is not the only one possible. Actually,
one can also write
pn(x) = an(x ‚àíŒ±1)m1...(x ‚àíŒ±k)mk,
k

l=1
ml = n
where Œ±i and mi denote the i-th root of pn and its multiplicity, respectively.
Other representations are available as well, see Section 6.4.1.

262
6. RootÔ¨Ånding for Nonlinear Equations
Notice that, since the coeÔ¨Écients ak are real, if Œ± is a zero of pn, then
its complex conjugate ¬ØŒ± is a zero of pn too.
Abel‚Äôs theorem states that for n ‚â•5 there does not exist an explicit
formula for the zeros of pn (see, for instance, [MM71], Theorem 10.1). This,
in turn, motivates numerical solutions of the nonlinear equation pn(x) = 0.
Since the methods introduced so far must be provided by a suitable search
interval [a, b] or an initial guess x(0), we recall two results that can be useful
to localize the zeros of a polynomial.
Property 6.5 (Descartes‚Äô rule of signs) Let pn ‚ààPn. Denote by ŒΩ the
number of sign changes in the set of coeÔ¨Écients {aj} and by k the number
of real positive roots of pn (each counted with its multiplicity). Then, k ‚â§ŒΩ
and ŒΩ ‚àík is an even number.
Property 6.6 (Cauchy‚Äôs Theorem) All zeros of pn are contained in the
circle Œì in the complex plane
Œì = {z ‚ààC : |z| ‚â§1 + Œ∑k} ,
where Œ∑k =
max
0‚â§k‚â§n‚àí1|ak/an|.
This second property is of little use if Œ∑k ‚â´1. In such an event, it is con-
venient to perform a translation through a suitable change of coordinates.
6.4.1
The Horner Method and DeÔ¨Çation
In this section we describe the Horner method for eÔ¨Éciently evaluating a
polynomial (and its derivative) at a given point z. The algorithm allows for
generating automatically a procedure, called deÔ¨Çation, for the sequential
approximation of all the roots of a polynomial.
Horner‚Äôs method is based on the observation that any polynomial pn ‚àà
Pn can be written as
pn(x) = a0 + x(a1 + x(a2 + . . . + x(an‚àí1 + anx) . . . )).
(6.25)
Formulae (6.24) and (6.25) are completely equivalent from an algebraic
standpoint; nevertheless, (6.24) requires n sums and 2n ‚àí1 multiplications
to evaluate pn(x), while (6.25) requires n sums and n multiplications. The
second expression, known as nested multiplications algorithm, is the basic
ingredient of Horner‚Äôs method. This method eÔ¨Éciently evaluates the poly-
nomial pn at a point z through the following synthetic division algorithm
bn = an,
bk = ak + bk+1z,
k = n ‚àí1, n ‚àí2, ..., 0,
(6.26)
which is implemented in Program 52. The coeÔ¨Écients aj of the polynomial
are stored in vector a ordered from an back to a0.

6.4 Zeros of Algebraic Equations
263
Program 52 - horner : Synthetic division algorithm
function [pnz,b] = horner(a,n,z)
b(1)=a(1); for j=2:n+1, b(j)=a(j)+b(j-1)*z; end; pnz=b(n+1);
All the coeÔ¨Écients bk in (6.26) depend on z and b0 = pn(z). The polynomial
qn‚àí1(x; z) = b1 + b2x + ... + bnxn‚àí1 =
n

k=1
bkxk‚àí1
(6.27)
has degree n‚àí1 in the variable x and depends on the parameter z through
the coeÔ¨Écients bk; it is called the associated polynomial of pn.
Let us now recall the following property of polynomial division:
given two polynomials hn ‚ààPn and gm ‚ààPm with m ‚â§n, there exist
an unique polynomial Œ¥ ‚ààPn‚àím and an unique polynomial œÅ ‚ààPm‚àí1 such
that
hn(x) = gm(x)Œ¥(x) + œÅ(x).
(6.28)
Then, dividing pn by x ‚àíz, from (6.28) it follows that
pn(x) = b0 + (x ‚àíz)qn‚àí1(x; z),
having denoted by qn‚àí1 the quotient and by b0 the remainder of the di-
vision. If z is a zero of pn, then b0 = pn(z) = 0 and thus pn(x) =
(x ‚àíz)qn‚àí1(x; z). In such a case, the algebraic equation qn‚àí1(x; z) = 0
yields the n ‚àí1 remaining roots of pn(x). This observation suggests adopt-
ing the following deÔ¨Çation procedure for Ô¨Ånding the roots of pn. For m =
n, n ‚àí1, . . . , 1:
1. Ô¨Ånd a root r of pm using a suitable approximation method;
2. evaluate qm‚àí1(x; r) by (6.26);
3. let pm‚àí1 = qm‚àí1.
In the two forthcoming sections some deÔ¨Çation methods will be ad-
dressed, making a precise choice for the scheme at point 1.
6.4.2
The Newton-Horner Method
A Ô¨Årst example of deÔ¨Çation employs Newton‚Äôs method for computing the
root r at step 1. of the procedure in the previous section. Implement-
ing Newton‚Äôs method fully beneÔ¨Åts from Horner‚Äôs algorithm (6.26). In-
deed, if qn‚àí1 is the associated polynomial of pn deÔ¨Åned in (6.27), since

264
6. RootÔ¨Ånding for Nonlinear Equations
p‚Ä≤
n(x) = qn‚àí1(x; z) + (x ‚àíz)q‚Ä≤
n‚àí1(x; z) then p‚Ä≤
n(z) = qn‚àí1(z; z). Thanks to
this identity, the Newton-Horner method for the approximation of a root
(real or complex) rj of pn (j = 1, . . . , n) takes the following form:
given an initial estimate r(0)
j
of the root, solve for any k ‚â•0
r(k+1)
j
= r(k)
j
‚àí
pn(r(k)
j
)
p‚Ä≤n(r(k)
j
)
= r(k)
j
‚àí
pn(r(k)
j
)
qn‚àí1(r(k)
j
; r(k)
j
)
.
(6.29)
Once convergence has been achieved for the iteration (6.29), polynomial
deÔ¨Çation is performed, this deÔ¨Çation being helped by the fact that pn(x) =
(x ‚àírj)pn‚àí1(x). Then, the approximation of a root of pn‚àí1(x) is carried
out until all the roots of pn have been computed.
Denoting by nk = n ‚àík the degree of the polynomial that is obtained at
each step of the deÔ¨Çation process, for k = 0, . . . , n ‚àí1, the computational
cost of each Newton-Horner iteration (6.29) is equal to 4nk. If rj ‚ààC, it
is necessary to work in complex arithmetic and take r(0)
j
‚ààC; otherwise,
indeed, the Newton-Horner method (6.29) would yield a sequence {r(k)
j
} of
real numbers.
The deÔ¨Çation procedure might be aÔ¨Äected by rounding error propagation
and, as a consequence, can lead to inaccurate results. For the sake of stabil-
ity, it is therefore convenient to approximate Ô¨Årst the root r1 of minimum
module, which is the most sensitive to ill-conditioning of the problem (see
Example 2.7, Chapter 2) and then to continue with the successive roots
r2, . . . , until the root of maximum module is computed. To localize r1, the
techniques described in Section 5.1 or the method of Sturm sequences can
be used (see [IK66], p. 126).
A further increase in accuracy can be obtained, once an approximation $rj
of the root rj is available, by going back to the original polynomial pn and
generating through the Newton-Horner method (6.29) a new approximation
to rj, taking as initial guess r(0)
j
= $rj. This combination of deÔ¨Çation and
successive correction of the root is called the Newton-Horner method with
reÔ¨Ånement.
Example 6.7 Let us examine the performance of the Newton-Horner method in
two cases: in the Ô¨Årst one, the polynomial admits real roots, while in the second
one there are two pairs of complex conjugate roots. To single out the importance
of reÔ¨Ånement, we have implemented (6.29) both switching it on and oÔ¨Ä(methods
NwtRef and Nwt, respectively). The approximate roots obtained using method Nwt
are denoted by rj, while sj are those computed by method NwtRef. As for the
numerical experiments, the computations have been done in complex arithmetic,
with x(0) = 0+i 0, i being the imaginary unit, nmax = 100 and toll = 10‚àí5. The
tolerance for the stopping test in the reÔ¨Ånement cycle has been set to 10‚àí3toll.
1) p5(x) = x5 + x4 ‚àí9x3 ‚àíx2 + 20x ‚àí12 = (x ‚àí1)2(x ‚àí2)(x + 2)(x + 3).

6.4 Zeros of Algebraic Equations
265
We report in Tables 6.3(a) and 6.3(b) the approximate roots rj (j = 1, . . . , 5)
and the number of Newton iterations (Nit) needed to get each of them; in the
case of method NwtRef we also show the number of extra Newton iterations for
the reÔ¨Ånement (Extra).
rj
Nit
0.99999348047830
17
1 ‚àíi3.56 ¬∑ 10‚àí25
6
2 ‚àíi2.24 ¬∑ 10‚àí13
9
‚àí2 ‚àíi1.70 ¬∑ 10‚àí10
7
‚àí3 + i5.62 ¬∑ 10‚àí6
1
(a)
sj
Nit
Extra
0.9999999899210124
17
10
1 ‚àíi2.40 ¬∑ 10‚àí28
6
10
2 + i1.12 ¬∑ 10‚àí22
9
1
‚àí2 + i8.18 ¬∑ 10‚àí22
7
1
‚àí3 ‚àíi7.06 ¬∑ 10‚àí21
1
2
(b)
TABLE 6.3. Roots of the polynomial p5. Roots computed by the Newton-Horner
method without reÔ¨Ånement (left), and with reÔ¨Ånement (right)
Notice a neat increase in the accuracy of rootÔ¨Ånding due to reÔ¨Ånement, even with
few extra iterations.
2) p6(x) = x6 ‚àí2x5 + 5x4 ‚àí6x3 + 2x2 + 8x ‚àí8.
The zeros of p6 are the complex numbers {1, ‚àí1, 1 ¬± i, ¬±2i}. We report below,
denoting them by rj, (j = 1, . . . , 6), the approximations to the roots of p6 ob-
tained using method Nwt, with a number of iterations equal to 2, 1, 1, 7, 7 and 1,
respectively. Beside, we also show the corresponding approximations sj computed
by method NwtRef and obtained with a maximum number of 2 extra iterations.
‚Ä¢
rj
Nwt
sj
NwtRef
r1
1
s1
1
r2
‚àí0.99 ‚àíi9.54 ¬∑ 10‚àí17
s2
‚àí1 + i1.23 ¬∑ 10‚àí32
r3
1+i
s3
1+i
r4
1-i
s4
1-i
r5
-1.31 ¬∑ 10‚àí8 + i2
s5
‚àí5.66 ¬∑ 10‚àí17 + i2
r6
-i2
s6
-i2
TABLE 6.4. Roots of the polynomial p6 obtained using the Newton-Horner
method without (left) and with (right) reÔ¨Ånement
A coding of the Newton-Horner algorithm is provided in Program 53. The
input parameters are A (a vector containing the polynomial coeÔ¨Écients), n
(the degree of the polynomial), toll (tolerance on the maximum variation
between successive iterates in Newton‚Äôs method), x0 (initial value, with
x(0) ‚ààR), nmax (maximum number of admissible iterations for Newton‚Äôs

266
6. RootÔ¨Ånding for Nonlinear Equations
method) and iref (if iref = 1, then the reÔ¨Ånement procedure is activated).
For dealing with the general case of complex roots, the initial datum is
automatically converted into the complex number z = x(0) + ix(0), where
i = ‚àö‚àí1.
The program returns as output the variables xn (a vector containing the
sequence of iterates for each zero of pn(x)), iter (a vector containing the
number of iterations needed to approximate each root), itrefin (a vector
containing the Newton iterations required to reÔ¨Åne each estimate of the
computed root) and root (vector containing the computed roots).
Program 53 - newthorn : Newton-Horner method with reÔ¨Ånement
function [xn,iter,root,itreÔ¨Ån]=newthorn(A,n,toll,x0,nmax,iref)
apoly=A;
for i=1:n, it=1; xn(it,i)=x0+sqrt(-1)*x0; err=toll+1; Ndeg=n-i+1;
if (Ndeg == 1), it=it+1; xn(it,i)=-A(2)/A(1);
else
while (it < nmax & err > toll),
[px,B]=horner(A,Ndeg,xn(it,i));
[pdx,C]=horner(B,Ndeg-1,xn(it,i));
it=it+1; if (pdx Àú=0), xn(it,i)=xn(it-1,i)-px/pdx;
err=max(abs(xn(it,i)-xn(it-1,i)),abs(px));
else,
disp(‚Äô Stop due to a vanishing p‚Äô‚Äô ‚Äô);
err=0; xn(it,i)=xn(it-1,i);
end
end
end
A=B;
if (iref==1), alfa=xn(it,i); itr=1; err=toll+1;
while ((err > toll*1e-3) & (itr < nmax))
[px,B]=horner(apoly,n,alfa);
[pdx,C]=horner(B,n-1,alfa); itr=itr+1;
if (pdxÀú=0)
alfa2=alfa-px/pdx;
err=max(abs(alfa2-alfa),abs(px)); alfa=alfa2;
else,
disp(‚Äô Stop due to a vanishing p‚Äô‚Äô ‚Äô); err=0;
end
end; itreÔ¨Ån(i)=itr-1; xn(it,i)=alfa;
end
iter(i)=it-1; root(i)=xn(it,i); x0=root(i);
end

6.4 Zeros of Algebraic Equations
267
6.4.3
The Muller Method
A second example of deÔ¨Çation employs Muller‚Äôs method for Ô¨Ånding an
approximation to the root r at step 1. of the procedure described in Section
6.4.1 (see [Mul56]). Unlike Newton‚Äôs or secant methods, Muller‚Äôs method is
able to compute complex zeros of a given function f, even starting from a
real initial datum; moreover, its order of convergence is almost quadratic.
The action of Muller‚Äôs method is drawn in Figure 6.5. The scheme ex-
tends the secant method, substituting the linear polynomial introduced in
(6.13) with a second-degree polynomial as follows. Given three distinct
values x(0), x(1) and x(2), the new point x(3) is determined by setting
p2(x(3)) = 0, where p2 ‚ààP2 is the unique polynomial that interpolates
f at the points x(i), i = 0, 1, 2, that is, p2(x(i)) = f(x(i)) for i = 0, 1, 2.
Therefore,
x(3)
f
p2
x(0)x(1) x(2)
FIGURE 6.5. The Ô¨Årst step of Muller‚Äôs method
p2(x) = f(x(2)) + (x ‚àíx(2))f[x(2), x(1)] + (x ‚àíx(2))(x ‚àíx(1))f[x(2), x(1), x(0)]
where
f[Œæ, Œ∑] = f(Œ∑) ‚àíf(Œæ)
Œ∑ ‚àíŒæ
,
f[Œæ, Œ∑, œÑ] = f[Œ∑, œÑ] ‚àíf[Œæ, Œ∑]
œÑ ‚àíŒæ
are the divided diÔ¨Äerences of order 1 and 2 associated with the points Œæ, Œ∑
and œÑ (see Section 8.2.1). Noticing that x‚àíx(1) = (x‚àíx(2))+(x(2) ‚àíx(1)),
we get
p2(x) = f(x(2)) + w(x ‚àíx(2)) + f[x(2), x(1), x(0)](x ‚àíx(2))2
having deÔ¨Åned
w
=
f[x(2), x(1)] + (x(2) ‚àíx(1))f[x(2), x(1), x(0)]
=
f[x(2), x(1)] + f[x(2), x(0)] ‚àíf[x(0), x(1)].

268
6. RootÔ¨Ånding for Nonlinear Equations
Requiring that p2(x(3)) = 0 it follows that
x(3) = x(2) + ‚àíw ¬±

w2 ‚àí4f(x(2))f[x(2), x(1), x(0)]
1/2
2f[x(2), x(1), x(0)]
.
Similar computations must be done for getting x(4) starting from x(1), x(2)
and x(3) and, more generally, to Ô¨Ånd x(k+1) starting from x(k‚àí2), x(k‚àí1)
and x(k), with k ‚â•2, according with the following formula (notice that the
numerator has been rationalized)
x(k+1) = x(k) ‚àí
2f(x(k))
w ‚àì

w2 ‚àí4f(x(k))f[x(k), x(k‚àí1), x(k‚àí2)]
1/2 .
(6.30)
The sign in (6.30) is chosen in such a way that the module of the denomina-
tor is maximized. Assuming that f ‚ààC3(J ) in a suitable neighborhood J
of the root Œ±, with f ‚Ä≤(Œ±) Ã∏= 0, the order of convergence is almost quadratic.
Precisely, the error e(k) = Œ± ‚àíx(k) obeys the following relation (see for the
proof [Hil87])
lim
k‚Üí‚àû
|e(k+1)|
|e(k)|p = 1
6

f ‚Ä≤‚Ä≤‚Ä≤(Œ±)
f ‚Ä≤(Œ±)
 ,
p ‚âÉ1.84.
Example 6.8 Let us employ Muller‚Äôs method to approximate the roots of the
polynomial p6 examined in Example 6.7. The tolerance on the stopping test
is toll = 10‚àí6, while x(0) = ‚àí5, x(1) = 0 and x(2) = 5 are the inputs to
(6.30). We report in Table 6.5 the approximate roots of p6, denoted by sj and
rj (j = 1, . . . , 5), where, as in Example 6.7, sj and rj have been obtained by
switching the reÔ¨Ånement procedure on and oÔ¨Ä, respectively. To compute the roots
rj, 12, 11, 9, 9, 2 and 1 iterations are needed, respectively, while only one extra
iteration is taken to reÔ¨Åne all the roots.
rj
sj
r1
1 + i2.2 ¬∑ 10‚àí15
s1
1 + i9.9 ¬∑ 10‚àí18
r2
‚àí1 ‚àíi8.4 ¬∑ 10‚àí16
s2
-1
r3
0.99 + i
s3
1 + i
r4
0.99 ‚àíi
s4
1 ‚àíi
r5
‚àí1.1 ¬∑ 10‚àí15 + i1.99
s5
i2
r6
‚àí1.0 ¬∑ 10‚àí15 ‚àíi2
s6
-i2
TABLE 6.5. Roots of polynomial p6 with Muller‚Äôs method without (rj) and with
(sj) reÔ¨Ånement
Even in this example, one can notice the eÔ¨Äectiveness of the reÔ¨Ånement procedure,
based on Newton‚Äôs method, on the accuracy of the solution yielded by (6.30). ‚Ä¢

6.5 Stopping Criteria
269
The Muller method is implemented in Program 54, in the special case
where f is a polynomial of degree n. The deÔ¨Çation process also includes a
reÔ¨Ånement phase; the evaluation of f(x(k‚àí2)), f(x(k‚àí1)) and f(x(k)), with
k ‚â•2, is carried out using Program 52. The input/output parameters are
analogous to those described in Program 53.
Program 54 - mulldeÔ¨Ç: Muller‚Äôs method with reÔ¨Ånement
function [xn,iter,root,itreÔ¨Ån]=mulldeÔ¨Ç(A,n,toll,x0,x1,x2,nmax,iref)
apoly=A;
for i=1:n
xn(1,i)=x0; xn(2,i)=x1; xn(3,i)=x2; it=0; err=toll+1; k=2; Ndeg=n-i+1;
if (Ndeg == 1), it=it+1; k=0; xn(it,i)=-A(2)/A(1);
else
while ((err > toll) & (it < nmax)),
k=k+1; it=it+1; [f0,B]=horner(A,Ndeg,xn(k-2,i));
[f1,B]=horner(A,Ndeg,xn(k-1,i)); [f2,B]=horner(A,Ndeg,xn(k,i));
f01=(f1-f0)/(xn(k-1,i)-xn(k-2,i)); f12=(f2-f1)/(xn(k,i)-xn(k-1,i));
f012=(f12-f01)/(xn(k,i)-xn(k-2,i)); w=f12+(xn(k,i)-xn(k-1,i))*f012;
arg=wÀÜ2-4*f2*f012; d1=w-sqrt(arg); d2=w+sqrt(arg); den=max(d1,d2);
if (denÀú=0); xn(k+1,i)=xn(k,i)-(2*f2)/den;
err=abs(xn(k+1,i)-xn(k,i));
else
disp(‚Äô Vanishing denominator ‚Äô); return; end;
end; end; radix=xn(k+1,i);
if (iref==1),
alfa=radix; itr=1; err=toll+1;
while ((err > toll*1e-3) & (itr < nmax)),
[px,B]=horner(apoly,n,alfa); [pdx,C]=horner(B,n-1,alfa);
if (pdx == 0), disp(‚Äô Vanishing derivative ‚Äô); err=0; end;
itr=itr+1; if (pdxÀú=0), alfa2=alfa-px/pdx;
err=abs(alfa2-alfa); alfa=alfa2; end;
end; itreÔ¨Ån(i)=itr-1; xn(k+1,i)=alfa; radix=alfa;
end
iter(i)=it; root(i)=radix; [px,B]=horner(A,Ndeg-1,xn(k+1,i)); A=B;
end
6.5
Stopping Criteria
Suppose that {x(k)} is a sequence converging to a zero Œ± of the function
f. In this section we provide some stopping criteria for terminating the
iterative process that approximates Œ±. Analogous to Section 4.6, where
the case of iterative methods for linear systems has been examined, there
are two possible criteria: a stopping test based on the residual and on the
increment. Below, Œµ is a Ô¨Åxed tolerance on the approximate calculation of

270
6. RootÔ¨Ånding for Nonlinear Equations
Œ± and e(k) = Œ±‚àíx(k) denotes the absolute error. We shall moreover assume
that f is continuously diÔ¨Äerentiable in a suitable neighborhood of the root.
1. Control of the residual: the iterative process terminates at the Ô¨Årst
step k such that |f(x(k))| < Œµ.
Situations can arise where the test turns out to be either too restrictive or
excessively optimistic (see Figure 6.6). Applying the estimate (6.6) to the
case at hand yields
|e(k)|
|Œ±|
‚â≤

m!
|f (m)(Œ±)||Œ±|m
1/m
|f(x(k))|1/m.
In particular, in the case of simple roots, the error is bound to the residual
by the factor 1/|f ‚Ä≤(Œ±)| so that the following conclusions can be drawn:
1. if |f ‚Ä≤(Œ±)| ‚âÉ1, then |e(k)| ‚âÉŒµ; therefore, the test provides a satisfac-
tory indication of the error;
2. if |f ‚Ä≤(Œ±)| ‚â™1, the test is not reliable since |e(k)| could be quite large
with respect to Œµ;
3. if, Ô¨Ånally, |f ‚Ä≤(Œ±)| ‚â´1, we get |e(k)| ‚â™Œµ and the test is too restrictive.
We refer to Figure 6.6 for an illustration of the last two cases.
f(x)
x(k)
Œ±
Œ±
f(x)
x(k)
FIGURE 6.6. Two situations where the stopping test based on the residual
is either too restrictive (when |e(k)| ‚â™|f(x(k))|, left) or too optimistic (when
|e(k)| ‚â´|f(x(k))|, right)
The conclusions that we have drawn agree with those in Example 2.4.
Indeed, when f ‚Ä≤(Œ±) ‚âÉ0, the condition number of the problem f(x) = 0 is
very high and, as a consequence, the residual does not provide a signiÔ¨Åcant
indication of the error.
2. Control of the increment: the iterative process terminates as soon as
|x(k+1) ‚àíx(k)| < Œµ.

6.5 Stopping Criteria
271
Let

x(k)
be generated by the Ô¨Åxed-point method x(k+1) = œÜ(x(k)). Using
the mean value theorem, we get
e(k+1) = œÜ(Œ±) ‚àíœÜ(x(k)) = œÜ‚Ä≤(Œæ(k))e(k),
where Œæ(k) lies between x(k) and Œ±. Then,
x(k+1) ‚àíx(k) = e(k) ‚àíe(k+1) =
+
1 ‚àíœÜ‚Ä≤(Œæ(k))
,
e(k)
so that, assuming that we can replace œÜ‚Ä≤(Œæ(k)) with œÜ‚Ä≤(Œ±), it follows that
e(k) ‚âÉ
1
1 ‚àíœÜ‚Ä≤(Œ±)(x(k+1) ‚àíx(k)).
(6.31)
-1
1 œÜ‚Ä≤(Œ±)
0
1
1
2
Œ≥
FIGURE 6.7. Behavior of Œ≥ = 1/(1 ‚àíœÜ‚Ä≤(Œ±)) as a function of œÜ‚Ä≤(Œ±)
As shown in Figure 6.7, we can conclude that the test:
- is unsatisfactory if œÜ‚Ä≤(Œ±) is close to 1;
- provides an optimal balancing between increment and error in the case
of methods of order 2 for which œÜ‚Ä≤(Œ±) = 0 as is the case for Newton‚Äôs
method;
- is still satisfactory if ‚àí1 < œÜ‚Ä≤(Œ±) < 0.
Example 6.9 The zero of the function f(x) = e‚àíx ‚àíŒ∑ is given by Œ± = ‚àílog(Œ∑).
For Œ∑ = 10‚àí9, Œ± ‚âÉ20.723 and f ‚Ä≤(Œ±) = ‚àíe‚àíŒ± ‚âÉ‚àí10‚àí9. We are thus in the case
where |f ‚Ä≤(Œ±)| ‚â™1 and we wish to examine the behaviour of Newton‚Äôs method in
the approximation of Œ± when the two stopping criteria above are adopted in the
computations.
We show in Tables 6.6 and 6.7 the results obtained using the test based on the
control of the residual (1) and of the increment (2), respectively. We have taken
x(0) = 0 and used two diÔ¨Äerent values of the tolerance. The number of iterations
required by the method is denoted by nit.
According to (6.31), since œÜ‚Ä≤(Œ±) = 0, the stopping test based on the increment
reveals to be reliable for both the values (which are quite diÔ¨Äering) of the stop
tolerance Œµ. The test based on the residual, instead, yields an acceptable estimate
of the root only for very small tolerances, while it is completely wrong for large
values of Œµ.
‚Ä¢

272
6. RootÔ¨Ånding for Nonlinear Equations
Œµ
nit
|f(x(nit))|
|Œ± ‚àíx(nit)|
|Œ± ‚àíx(nit)|/Œ±
10‚àí10
22
5.9 ¬∑ 10‚àí11
5.7 ¬∑ 10‚àí2
0.27
10‚àí3
7
9.1 ¬∑ 10‚àí4
13.7
66.2
TABLE
6.6.
Newton‚Äôs
method
for
the
approximation
of
the
root
of
f(x) = e‚àíx ‚àíŒ∑ = 0. The stopping test is based on the control of the residual
Œµ
nit
|x(nit) ‚àíx(nit‚àí1)|
|Œ± ‚àíx(nit)|
|Œ± ‚àíx(nit)|/Œ±
10‚àí10
26
8.4 ¬∑ 10‚àí13
‚âÉ0
‚âÉ0
10‚àí3
25
1.3 ¬∑ 10‚àí6
8.4 ¬∑ 10‚àí13
4 ¬∑ 10‚àí12
TABLE
6.7.
Newton‚Äôs
method
for
the
approximation
of
the
root
of
f(x) = e‚àíx ‚àíŒ∑ = 0. The stopping test is based on the control of the incre-
ment
6.6
Post-processing Techniques for Iterative
Methods
We conclude this chapter by introducing two algorithms that aim at ac-
celerating the convergence of iterative methods for Ô¨Ånding the roots of a
function.
6.6.1
Aitken‚Äôs Acceleration
We describe this technique in the case of linearly convergent Ô¨Åxed-point
methods, referring to [IK66], pp. 104‚Äì108, for the case of methods of higher
order.
Consider a Ô¨Åxed-point iteration that is linearly converging to a zero Œ± of
a given function f. Denoting by Œª an approximation of œÜ‚Ä≤(Œ±) to be suitably
determined and recalling (6.18) we have, for k ‚â•1
Œ±
‚âÉx(k) ‚àíŒªx(k‚àí1)
1 ‚àíŒª
= x(k) ‚àíŒªx(k) + Œªx(k) ‚àíŒªx(k‚àí1)
1 ‚àíŒª
= x(k) +
Œª
1 ‚àíŒª(x(k) ‚àíx(k‚àí1)).
(6.32)
Aitken‚Äôs method provides a simple way of computing Œª that is able to
accelerate the convergence of the sequence {x(k)} to the root Œ±. With this
aim, let us consider for k ‚â•2 the following ratio
Œª(k) =
x(k) ‚àíx(k‚àí1)
x(k‚àí1) ‚àíx(k‚àí2) ,
(6.33)
and check that
lim
k‚Üí‚àûŒª(k) = œÜ‚Ä≤(Œ±).
(6.34)

6.6 Post-processing Techniques for Iterative Methods
273
Indeed, for k suÔ¨Éciently large
x(k+2) ‚àíŒ± ‚âÉœÜ‚Ä≤(Œ±)(x(k+1) ‚àíŒ±)
and thus, elaborating (6.33), we get
lim
k‚Üí‚àûŒª(k) = lim
k‚Üí‚àû
x(k) ‚àíx(k‚àí1)
x(k‚àí1) ‚àíx(k‚àí2) = lim
k‚Üí‚àû
(x(k) ‚àíŒ±) ‚àí(x(k‚àí1) ‚àíŒ±)
(x(k‚àí1) ‚àíŒ±) ‚àí(x(k‚àí2) ‚àíŒ±)
= lim
k‚Üí‚àû
x(k) ‚àíŒ±
x(k‚àí1) ‚àíŒ± ‚àí1
1 ‚àíx(k‚àí2) ‚àíŒ±
x(k‚àí1) ‚àíŒ±
= œÜ‚Ä≤(Œ±) ‚àí1
1 ‚àí
1
œÜ‚Ä≤(Œ±)
= œÜ‚Ä≤(Œ±)
which is (6.34). Substituting in (6.32) Œª with its approximation Œª(k) given
by (6.33), yields the updated estimate of Œ±
Œ± ‚âÉx(k) +
Œª(k)
1 ‚àíŒª(k) (x(k) ‚àíx(k‚àí1))
(6.35)
which, rigorously speaking, is signiÔ¨Åcant only for a suÔ¨Éciently large k.
However, assuming that (6.35) holds for any k ‚â•2, we denote by x(k) the
new approximation of Œ± that is obtained by plugging (6.33) back into (6.35)
x(k) = x(k) ‚àí
(x(k) ‚àíx(k‚àí1))2
(x(k) ‚àíx(k‚àí1)) ‚àí(x(k‚àí1) ‚àíx(k‚àí2)),
k ‚â•2.
(6.36)
This relation is known as Aitken‚Äôs extrapolation formula.
Letting, for k ‚â•2,
‚ñ≥x(k) = x(k) ‚àíx(k‚àí1),
‚ñ≥2x(k) = ‚ñ≥(‚ñ≥x(k)) = ‚ñ≥x(k+1) ‚àí‚ñ≥x(k),
formula (6.36) can be written as
x(k) = x(k) ‚àí(‚ñ≥x(k))2
‚ñ≥2x(k‚àí1) ,
k ‚â•2.
(6.37)
Form (6.37) explains the reason why method (6.36) is more commonly
known as Aitken‚Äôs ‚ñ≥2 method.
For the convergence analysis of Aitken‚Äôs method, it is useful to write (6.36)
as a Ô¨Åxed-point method in the form (6.17), by introducing the iteration
function
œÜ‚ñ≥(x) =
xœÜ(œÜ(x)) ‚àíœÜ2(x)
œÜ(œÜ(x)) ‚àí2œÜ(x) + x.
(6.38)
This function is indeterminate at x = Œ± since œÜ(Œ±) = Œ±; however, by
applying L‚ÄôHospital‚Äôs rule one can easily check that limx‚ÜíŒ± œÜ‚ñ≥(x) = Œ±

274
6. RootÔ¨Ånding for Nonlinear Equations
under the assumption that œÜ is diÔ¨Äerentiable at Œ± and œÜ‚Ä≤(Œ±) Ã∏= 1. Thus, œÜ‚ñ≥
is consistent and has a continuos extension at Œ±, the same being also true
if Œ± is a multiple root of f. Moreover, it can be shown that the Ô¨Åxed points
of (6.38) coincide with those of œÜ even in the case where Œ± is a multiple
root of f (see [IK66], pp. 104-106).
From (6.38) we conclude that Aitken‚Äôs method can be applied to a Ô¨Åxed-
point method x = œÜ(x) of arbitrary order. Actually, the following conver-
gence result holds.
Property 6.7 (convergence of Aitken‚Äôs method) Let x(k+1) = œÜ(x(k))
be a Ô¨Åxed-point iteration of order p ‚â•1 for the approximation of a simple
zero Œ± of a function f. If p = 1, Aitken‚Äôs method converges to Œ± with order
2, while if p ‚â•2 the convergence order is 2p ‚àí1. In particular, if p = 1,
Aitken‚Äôs method is convergent even if the Ô¨Åxed-point method is not. If Œ±
has multiplicity m ‚â•2 and the method x(k+1) = œÜ(x(k)) is Ô¨Årst-order con-
vergent, then Aitken‚Äôs method converges linearly, with convergence factor
C = 1 ‚àí1/m.
Example 6.10 Consider the computation of the simple zero Œ± = 1 for the func-
tion f(x) = (x ‚àí1)ex. For this, we use three Ô¨Åxed-point methods whose iteration
functions are, respectively, œÜ0(x) = log(xex), œÜ1(x) = (ex + x)/(ex + 1) and
œÜ2(x) = (x2 ‚àíx + 1)/x (for x Ã∏= 0). Notice that, since |œÜ‚Ä≤
0(1)| = 2, the corre-
sponding Ô¨Åxed-point method is not convergent, while in the other two cases the
methods have order 1 and 2, respectively.
Let us check the performance of Aitken‚Äôs method, running Program 55 with
x(0) = 2, toll = 10‚àí10 and working in complex arithmetic. Notice that in the case
of œÜ0 this produces complex numbers if x(k) happens to be negative. According
to Property 6.7, Aitken‚Äôs method applied to the iteration function œÜ0 converges
in 8 steps to the value x(8) = 1.000002 + i 0.000002. In the other two cases, the
method of order 1 converges to Œ± in 18 iterations, to be compared with the 4
iterations required by Aitken‚Äôs method, while in the case of the iteration function
œÜ2 convergence holds in 7 iterations against 5 iterations required by Aitken‚Äôs
method.
‚Ä¢
Aitken‚Äôs method is implemented in Program 55. The input/output pa-
rameters are the same as those of previous programs in this chapter.
Program 55 - aitken : Aitken‚Äôs extrapolation
function [xvect,xdif,fx,nit]=aitken(x0,nmax,toll,phi,fun)
nit=0; xvect=[x0]; x=x0; fxn=eval(fun);
fx=[fxn]; xdif=[]; err=toll+1;
while err >= toll & nit <= nmax
nit=nit+1; xv=xvect(nit); x=xv; phix=eval(phi);
x=phix; phixx=eval(phi); den=phixx-2*phix+xv;
if den == 0, err=toll*1.e-01;
else, xn=(xv*phixx-phixÀÜ2)/den; xvect=[xvect; xn];
xdif=[xdif; abs(xn-xv)]; x=xn; fxn=abs(eval(fun));

6.6 Post-processing Techniques for Iterative Methods
275
fx=[fx; fxn]; err=fxn;
end
end
6.6.2
Techniques for Multiple Roots
As previously noticed in deriving Aitken‚Äôs acceleration, taking the incre-
mental ratios of successive iterates Œª(k) in (6.33) provides a way to estimate
the asymptotic convergence factor œÜ‚Ä≤(Œ±).
This information can be employed also to estimate the multiplicity of the
root of a nonlinear equation and, as a consequence, it provides a tool for
modifying Newton‚Äôs method in order to recover its quadratic convergence
(see (6.23)). Indeed, deÔ¨Åne the sequence m(k) through the relation Œª(k) =
1 ‚àí1/m(k), and recalling (6.22), it follows that m(k) tends to m as k ‚Üí‚àû.
If the multiplicity m is known a priori, it is clearly convenient to use the
modiÔ¨Åed Newton method (6.23). In other cases, the following adaptive New-
ton algorithm can be used
x(k+1) = x(k) ‚àím(k) f(x(k))
f ‚Ä≤(x(k)),
k ‚â•2,
(6.39)
where we have set
m(k) =
1
1 ‚àíŒª(k) =
x(k‚àí1) ‚àíx(k‚àí2)
2x(k‚àí1) ‚àíx(k) ‚àíx(k‚àí2) .
(6.40)
Example 6.11 Let us check the performances of Newton‚Äôs method in its three
versions proposed so far (standard (6.16), modiÔ¨Åed (6.23) and adaptive (6.39)),
to approximate the multiple zero Œ± = 1 of the function f(x) = (x2 ‚àí1)p log x
(for p ‚â•1 and x > 0). The desired root has multiplicity m = p + 1. The values
p = 2, 4, 6 have been considered and x(0) = 0.8, toll=10‚àí10 have always been
taken in numerical computations.
The obtained results are summarized in Table 6.8, where for each method
the number of iterations nit required to converge are reported. In the case of
the adaptive method, beside the value of nit we have also shown in braces the
estimate m(nit) of the multiplicity m that is yielded by Program 56.
‚Ä¢
m
standard
adaptive
modiÔ¨Åed
3
51
13 (2.9860)
4
5
90
16 (4.9143)
5
7
127
18 (6.7792)
5
TABLE 6.8. Solution of problem (x2 ‚àí1)p log x = 0 in the interval [0.5, 1.5], with
p = 2, 4, 6

276
6. RootÔ¨Ånding for Nonlinear Equations
In Example 6.11, the adaptive Newton method converges more rapidly
than the standard method, but less rapidly than the modiÔ¨Åed Newton
method. It must be noticed, however, that the adaptive method yields as
a useful by-product a good estimate of the multiplicity of the root, which
can be proÔ¨Åtably employed in a deÔ¨Çation procedure for the approximation
of the roots of a polynomial.
The algorithm 6.39, with the adaptive estimate (6.40) of the multiplicity
of the root, is implemented in Program 56. To avoid the onset of numerical
instabilities, the updating of m(k) is performed only when the variation be-
tween two consecutive iterates is suÔ¨Éciently diminished. The input/output
parameters are the same as those of previous programs in this chapter.
Program 56 - adptnewt : Adaptive Newton‚Äôs method
function [xvect,xdif,fx,nit,m] = adptnewt(x0,nmax,toll,fun,dfun)
xvect=x0; nit=0; r=[1]; err=toll+1; m=[1]; xdif=[];
while (nit < nmax) & (err > toll)
nit=nit+1; x=xvect(nit); fx(nit)=eval(fun); f1x=eval(dfun);
if (f1x == 0), disp(‚Äô Stop due to vanishing derivative ‚Äô); return; end;
x=x-m(nit)*fx(nit)/f1x; xvect=[xvect;x]; fx=[fx;eval(fun)];
rd=err; err=abs(xvect(nit+1)-xvect(nit)); xdif=[xdif;err];
ra=err/rd; r=[r;ra]; diÔ¨Ä=abs(r(nit+1)-r(nit));
if (diÔ¨Ä< 1.e-3) & (r(nit+1) > 1.e-2),
m(nit+1)=max(m(nit),1/abs(1-r(nit+1)));
else, m(nit+1)=m(nit); end
end
6.7
Applications
We apply iterative methods for nonlinear equations considered so far in the
solution of two problems arising in the study of the thermal properties of
gases and electronics, respectively.
6.7.1
Analysis of the State Equation for a Real Gas
For a mole of a perfect gas, the state equation Pv = RT establishes a re-
lation between the pressure P of the gas (in Pascals [Pa]), the speciÔ¨Åc vol-
ume v (in cubic meters per kilogram [m3Kg‚àí1]) and its temperature T (in
Kelvin [K]), R being the universal gas constant, expressed in [JKg‚àí1K‚àí1]
(joules per kilogram per Kelvin).
For a real gas, the deviation from the state equation of perfect gases is
due to van der Waals and takes into account the intermolecular interaction
and the space occupied by molecules of Ô¨Ånite size (see [Sla63]).

6.7 Applications
277
Denoting by Œ± and Œ≤ the gas constants according to the van der Waals
model, in order to determine the speciÔ¨Åc volume v of the gas, once P and
T are known, we must solve the nonlinear equation
f(v) = (P + Œ±/v2)(v ‚àíŒ≤) ‚àíRT = 0.
(6.41)
With this aim, let us consider Newton‚Äôs method (6.16) in the case of carbon
dioxide (CO2), at the pressure of P = 10[atm] (equal to 1013250[Pa]) and
at the temperature of T = 300[K]. In such a case, Œ± = 188.33[Pam6Kg‚àí2]
and Œ≤ = 9.77 ¬∑ 10‚àí4[m3Kg‚àí1]; as a comparison, the solution computed by
assuming that the gas is perfect is Àúv ‚âÉ0.056[m3Kg‚àí1].
We report in Table 6.9 the results obtained by running Program 50 for
diÔ¨Äerent choices of the initial guess v(0). We have denoted by Nit the num-
ber of iterations needed by Newton‚Äôs method to converge to the root v‚àóof
f(v) = 0 using an absolute tolerance equal to the roundoÔ¨Äunit.
v(0)
Nit
v(0)
Nit
v(0)
Nit
v(0)
Nit
10‚àí4
47
10‚àí2
7
10‚àí3
21
10‚àí1
5
TABLE 6.9. Convergence of Newton‚Äôs method to the root of equation (6.41)
The computed approximation of v‚àóis vNit ‚âÉ0.0535. To analyze the causes
of the strong dependence of Nit on the value of v(0), let us examine the
derivative f ‚Ä≤(v) = P ‚àíŒ±v‚àí2 + 2Œ±Œ≤v‚àí3. For v > 0, f ‚Ä≤(v) = 0 at vM ‚âÉ
1.99¬∑10‚àí3[m3Kg‚àí1] (relative maximum) and at vm ‚âÉ1.25¬∑10‚àí2[m3Kg‚àí1]
(relative minimum), as can be seen in the graph of Figure 6.8 (left).
A choice of v(0) in the interval (0, vm) (with v(0) Ã∏= vM) thus necessarily
leads to a slow convergence of Newton‚Äôs method, as demonstrated in Figure
6.8 (right), where, in solid circled line, the sequence {|v(k+1) ‚àív(k)|} is
shown, for k ‚â•0.
A possible remedy consists of resorting to a polyalgorithmic approach,
based on the sequential use of the bisection method and Newton‚Äôs method
(see Section 6.2.1). Running the bisection-Newton‚Äôs method with the end-
points of the search interval equal to a = 10‚àí4[m3Kg‚àí1] and b = 0.1[m3Kg‚àí1]
and an absolute tolerance of 10‚àí3[m3Kg‚àí1], yields an overall convergence
of the algorithm to the root v‚àóin 11 iterations, with an accuracy of the
order of the roundoÔ¨Äunit. The plot of the sequence {|v(k+1) ‚àív(k)|}, for
k ‚â•0, is shown in solid and starred lines in Figure 6.8 (right).
6.7.2
Analysis of a Nonlinear Electrical Circuit
Let us consider the electrical circuit in Figure 6.9 (left), where v and j
denote respectively the voltage drop across the device D (called a tunneling
diode) and the current Ô¨Çowing through D, while R and E are a resistor and
a voltage generator of given values.

278
6. RootÔ¨Ånding for Nonlinear Equations
0
0.02
0.04
0.06
0.08
0.1
‚àí6
‚àí4
‚àí2
0
2
4
6 x 10
4
0
10
20
30
40
50
10
‚àí18
10
‚àí16
10
‚àí14
10
‚àí12
10
‚àí10
10
‚àí8
10
‚àí6
10
‚àí4
10
‚àí2
10
0
FIGURE 6.8. Graph of the function f in (6.41) (left); increments |v(k+1) ‚àív(k)|
computed by the Newton‚Äôs method (circled curve) and bisection-Newton‚Äôs
method (starred curve)
The circuit is commonly employed as a biasing circuit for electronic de-
vices working at high frequency (see [Col66]). In such applications the pa-
rameters R and E are designed in such a way that v attains a value internal
to the interval for which g‚Ä≤(v) < 0, where g is the function which describes
the bound between current and voltage for D and is drawn in Figure 6.9
(right). Explicitly, g = Œ±(ev/Œ≤ ‚àí1) ‚àí¬µv(v ‚àíŒ≥), for suitable constants Œ±, Œ≤,
Œ≥ and ¬µ.
_+
j
E
R
D
v
0
0.1
0.2
0.3
0.4
0.5
‚àí4
‚àí2
0
2
4
6
8
10
12 x 10
‚àí5
g(v)
FIGURE 6.9. Tunneling diode circuit (left) and working point computation
(right)
Our aim is to determine the working point of the circuit at hand, that
is, the values attained by v and j for given parameters R and E. For that,
we write KirchhoÔ¨Ä‚Äôs law for the voltages across the loop, obtaining the
following nonlinear equation
f(v) = v
 1
R + ¬µŒ≥

‚àí¬µv2 + Œ±(ev/Œ≤ ‚àí1) ‚àíE
R = 0.
(6.42)
From a graphical standpoint, Ô¨Ånding out the working point of the circuit
amounts to determining the intersection between the function g and the

6.8 Exercises
279
straight line of equation j = (E ‚àív)/R, as shown in Figure 6.9 (right).
Assume the following (real-life) values for the parameters of the problem:
E/R = 1.2¬∑10‚àí4 [A], Œ± = 10‚àí12 [A], Œ≤‚àí1 = 40 [V ‚àí1], ¬µ = 10‚àí3 [AV ‚àí2] and
Œ≥ = 0.4 [V ]. The solution of (6.42), which is also unique for the considered
values of the parameters, is v‚àó‚âÉ0.3 [V ].
To approximate v‚àó, we compare the main iterative methods introduced
in this chapter. We have taken v(0) = 0 [V ] for Newton‚Äôs method, Œæ = 0
for the Dekker-Brent algorithm (for the meaning of Œæ, see Example 6.5),
while for all the other schemes the search interval has been taken equal to
[0, 0.5]. The stopping tolerance toll has been set to 10‚àí10. The obtained
results are reported in Table 6.10 where nit and f (nit) denote respectively
the number of iterations needed by the method to converge and the value
of f at the computed solution.
Notice the extremely slow convergence of the Regula Falsi method, due
to the fact that the value v(k‚Ä≤) always coincides with the right end-point
v = 0.5 and the function f around v‚àóhas derivative very close to zero. An
analogous interpretation holds for the chord method.
Method
nit
f (nit)
Method
nit
f (nit)
bisection
33
‚àí1.12 ¬∑ 10‚àí15
Dekker-Brent
11
1.09 ¬∑ 10‚àí14
Regula Falsi
225
‚àí9.77 ¬∑ 10‚àí11
secant
11
2.7 ¬∑ 10‚àí20
chord
186
‚àí9.80 ¬∑ 10‚àí14
Newton‚Äôs
8
‚àí1.35 ¬∑ 10‚àí20
TABLE 6.10. Convergence of the methods for the approximation of the root of
equation (6.42)
6.8
Exercises
1. Derive geometrically the sequence of the Ô¨Årst iterates computed by bisec-
tion, Regula Falsi, secant and Newton‚Äôs methods in the approximation of
the zero of the function f(x) = x2 ‚àí2 in the interval [1, 3].
2. Let f be a continuous function that is m-times diÔ¨Äerentiable (m ‚â•1), such
that f(Œ±) = . . . = f (m‚àí1)(Œ±) = 0 and f (m)(Œ±) Ã∏= 0. Prove (6.22) and check
that the modiÔ¨Åed Newton method (6.23) has order of convergence equal to
2.
[Hint: let f(x) = (x ‚àíŒ±)mh(x), h being a function such that h(Œ±) Ã∏= 0].
3. Let f(x) = cos2(2x) ‚àíx2 be the function in the interval 0 ‚â§x ‚â§1.5
examined in Example 6.4. Having Ô¨Åxed a tolerance Œµ = 10‚àí10 on the abso-
lute error, determine experimentally the subintervals for which Newton‚Äôs
method is convergent to the zero Œ± ‚âÉ0.5149.
[Solution: for 0 < x(0) ‚â§0.02, 0.94 ‚â§x(0) ‚â§1.13 and 1.476 ‚â§x(0) ‚â§1.5,
the method converges to the solution ‚àíŒ±. For any other value of x(0) in
[0, 1.5], the method converges to Œ±].

280
6. RootÔ¨Ånding for Nonlinear Equations
4. Check the following properties:
(a) 0 < œÜ‚Ä≤(Œ±) < 1: monotone convergence, that is, the error x(k) ‚àíŒ±
maintains a constant sign as k varies;
(b) ‚àí1 < œÜ‚Ä≤(Œ±) < 0: oscillatory convergence that is, x(k) ‚àíŒ± changes sign
as k varies;
(c) |œÜ‚Ä≤(Œ±)| > 1: divergence. More precisely, if œÜ‚Ä≤(Œ±) > 1, the sequence
is monotonically diverging, while for œÜ‚Ä≤(Œ±) < ‚àí1 it diverges with
oscillatory sign.
5. Consider for k ‚â•0 the Ô¨Åxed-point method, known as SteÔ¨Äensen‚Äôs method
x(k+1) = x(k) ‚àíf(x(k))
œï(x(k)),
œï(x(k)) = f(x(k) + f(x(k))) ‚àíf(x(k))
f(x(k))
,
and prove that it is a second-order method. Implement the SteÔ¨Äensen
method in a MATLAB code and employ it to approximate the root of
the nonlinear equation e‚àíx ‚àísin(x) = 0.
6. Analyze the convergence of the Ô¨Åxed-point method x(k+1) = œÜj(x(k)) for
computing the zeros Œ±1 = ‚àí1 and Œ±2 = 2 of the function f(x) = x2 ‚àíx‚àí2,
when the following iteration functions are used: œÜ1(x) = x2 ‚àí2, œÜ2(x) =
‚àö2 + x œÜ3(x) = ‚àí‚àö2 + x and œÜ4(x) = 1 + 2/x, x Ã∏= 0.
[Solution: the method is non convergent with œÜ1, it converges only to Œ±2,
with œÜ2 and œÜ4, while it converges only to Œ±1 with œÜ3].
7. For the approximation of the zeros of the function f(x) = (2x2 ‚àí3x ‚àí
2)/(x ‚àí1), consider the following Ô¨Åxed-point methods:
(1) x(k+1) = g(x(k)), where g(x) = (3x2 ‚àí4x ‚àí2)/(x ‚àí1);
(2) x(k+1) = h(x(k)), where h(x) = x ‚àí2 + x/(x ‚àí1).
Analyze the convergence properties of the two methods and determine
in particular their order. Check the behavior of the two schemes using
Program 51 and provide, for the second method, an experimental estimate
of the interval such that if x(0) is chosen in the interval then the method
converges to Œ± = 2.
[Solution: zeros: Œ±1 = ‚àí1/2 and Œ±2 = 2. Method (1) is not convergent,
while (2) can approximate only Œ±2 and is second-order. Convergence holds
for any x(0) > 1].
8. Propose at least two Ô¨Åxed-point methods for approximating the root Œ± ‚âÉ
0.5885 of equation e‚àíx ‚àísin(x) = 0 and analyze their convergence.
9. Using Descartes‚Äôs rule of signs, determine the number of real roots of the
polynomials p6(x) = x6 ‚àíx ‚àí1 and p4(x) = x4 ‚àíx3 ‚àíx2 + x ‚àí1.
[Solution: both p6 and p4 have one negative and one positive real root].
10. Let g : R ‚ÜíR be deÔ¨Åned as g(x) =
‚àö
1 + x2. Show that the iterates of
Newton‚Äôs method for the equation g‚Ä≤(x) = 0 satisfy the following proper-
ties:
(a)
|x(0)| < 1 ‚áíg(x(k+1)) < g(x(k)), k ‚â•0, lim
k‚Üí‚àûx(k) = 0,
(b)
|x(0)| > 1 ‚áíg(x(k+1)) > g(x(k)), k ‚â•0, lim
k‚Üí‚àû|x(k)| = +‚àû.

7
Nonlinear Systems and Numerical
Optimization
In this chapter we address the numerical solution of systems of nonlinear
equations and the minimization of a function of several variables.
The Ô¨Årst problem generalizes to the n-dimensional case the search for
the zeros of a function, which was considered in Chapter 6, and can be
formulated as follows: given F : Rn ‚ÜíRn,
Ô¨Ånd x‚àó‚ààRn such that F(x‚àó) = 0.
(7.1)
Problem (7.1) will be solved by extending to several dimensions some of
the schemes that have been proposed in Chapter 6.
The basic formulation of the second problem reads: given f : Rn ‚ÜíR,
called an objective function,
minimize f(x) in Rn,
(7.2)
and is called an unconstrained optimization problem.
A typical example consists of determining the optimal allocation of n
resources, x1, x2, . . . , xn, in competition with each other and ruled by a
speciÔ¨Åc law. Generally, such resources are not unlimited; this circumstance,
from a mathematical standpoint, amounts to requiring that the minimizer
of the objective function lies within a subset ‚Ñ¶‚äÇRn, and, possibly, that
some equality or inequality constraints must be satisÔ¨Åed.
When these constraints exist the optimization problem is called con-
strained and can be formulated as follows: given the objective function f,
minimize f(x) in ‚Ñ¶‚äÇRn.
(7.3)

282
7. Nonlinear Systems and Numerical Optimization
Remarkable instances of (7.3) are those in which ‚Ñ¶is characterized by con-
ditions like h(x) = 0 (equality constraints) or h(x) ‚â§0 (inequality con-
straints), where h : Rn ‚ÜíRm, with m ‚â§n, is a given function, called cost
function, and the condition h(x) ‚â§0 means hi(x) ‚â§0, for i = 1, . . . , m.
If the function h is continuous and ‚Ñ¶is connected, problem (7.3) is
usually referred to as a nonlinear programming problem. Notable examples
in this area are:
convex programming if f is a convex function and h has convex compo-
nents (see (7.21));
linear programming if f and h are linear;
quadratic programming if f is quadratic and h is linear.
Problems (7.1) and (7.2) are strictly related to one another. Indeed, if we
denote by Fi the components of F, then a point x‚àó, a solution of (7.1),
is a minimizer of the function f(x) = n
i=1 F 2
i (x). Conversely, assuming
that f is diÔ¨Äerentiable and setting the partial derivatives of f equal to
zero at a point x‚àóat which f is minimum leads to a system of nonlinear
equations. Thus, any system of nonlinear equations can be associated with
a suitable minimization problem, and vice versa. We shall take advantage
of this observation when devising eÔ¨Écient numerical methods.
7.1
Solution of Systems of Nonlinear Equations
Before considering problem (7.1), let us set some notation which will be
used throughout the chapter.
For k ‚â•0, we denote by Ck(D) the set of k-continuously diÔ¨Äerentiable
functions from D to Rn, where D ‚äÜRn is a set that will be made precise
from time to time. We shall always assume that F ‚ààC1(D), i.e., F : Rn ‚Üí
Rn is a continuously diÔ¨Äerentiable function on D.
We denote also by JF(x) the Jacobian matrix associated with F and
evaluated at the point x = (x1, . . . , xn)T of Rn, deÔ¨Åned as
(JF(x))ij =
‚àÇFi
‚àÇxj

(x),
i, j = 1, . . . , n.
Given any vector norm ‚à•¬∑‚à•, we shall henceforth denote the sphere of radius
R with center x‚àóby
B(x‚àó; R) = {y ‚ààRn : ‚à•y ‚àíx‚àó‚à•< R} .

7.1 Solution of Systems of Nonlinear Equations
283
7.1.1
Newton‚Äôs Method and Its Variants
An immediate extension to the vector case of Newton‚Äôs method (6.16) for
scalar equations can be formulated as follows:
given x(0) ‚ààRn, for k = 0, 1, . . . , until convergence:
solve
JF(x(k))Œ¥x(k) = ‚àíF(x(k));
set
x(k+1) = x(k) + Œ¥x(k).
(7.4)
Thus, at each step k the solution of a linear system with matrix JF(x(k))
is required.
Example 7.1 Consider the nonlinear system
Ô£±
Ô£≤
Ô£≥
ex2
1+x2
2 ‚àí1
= 0,
ex2
1‚àíx2
2 ‚àí1
= 0,
which admits the unique solution x‚àó= 0. In this case, F(x) = (ex2
1+x2
2 ‚àí
1, ex2
1‚àíx2
2 ‚àí1). Running Program 57, leads to convergence in 15 iterations to the
pair (0.61 ¬∑ 10‚àí5, 0.61 ¬∑ 10‚àí5)T , starting from the initial datum x(0) = (0.1, 0.1)T ,
thus demonstrating a fairly rapid convergence rate. The results, however, dra-
matically change as the choice of the initial guess is varied. For instance, picking
up x(0) = (10, 10)T , 220 iterations are needed to obtain a solution comparable to
the previous one, while, starting from x(0) = (20, 20)T , Newton‚Äôs method fails to
converge.
‚Ä¢
The previous example points out the high sensitivity of Newton‚Äôs method
on the choice of the initial datum x(0), as conÔ¨Årmed by the following local
convergence result.
Theorem 7.1 Let F : Rn ‚ÜíRn be a C1 function in a convex open set
D of Rn that contains x‚àó. Suppose that J‚àí1
F (x‚àó) exists and that there exist
positive constants R, C and L, such that ‚à•J‚àí1
F (x‚àó)‚à•‚â§C and
‚à•JF(x) ‚àíJF(y)‚à•‚â§L‚à•x ‚àíy‚à•
‚àÄx, y ‚ààB(x‚àó; R),
having denoted by the same symbol ‚à•¬∑ ‚à•two consistent vector and matrix
norms. Then, there exists r > 0 such that, for any x(0) ‚ààB(x‚àó; r), the
sequence (7.4) is uniquely deÔ¨Åned and converges to x‚àówith
‚à•x(k+1) ‚àíx‚àó‚à•‚â§CL‚à•x(k) ‚àíx‚àó‚à•2.
(7.5)
Proof. Proceeding by induction on k, let us check (7.5) and, moreover, that
x(k+1) ‚ààB(x‚àó; r), where r = min(R, 1/(2CL)). First, we prove that for any
x(0) ‚ààB(x‚àó; r), the inverse matrix J‚àí1
F (x(0)) exists. Indeed
‚à•J‚àí1
F (x‚àó)[JF(x(0)) ‚àíJF(x‚àó)]‚à•‚â§‚à•J‚àí1
F (x‚àó)‚à•‚à•JF(x(0)) ‚àíJF(x‚àó)‚à•‚â§CLr ‚â§1
2,

284
7. Nonlinear Systems and Numerical Optimization
and thus, thanks to Theorem 1.5, we can conclude that J‚àí1
F (x(0)) exists, since
‚à•J‚àí1
F (x(0))‚à•‚â§
‚à•J‚àí1
F (x‚àó)‚à•
1 ‚àí‚à•J‚àí1
F (x‚àó)[JF(x(0)) ‚àíJF(x‚àó)]‚à•‚â§2‚à•J‚àí1
F (x‚àó)‚à•‚â§2C.
As a consequence, x(1) is well deÔ¨Åned and
x(1) ‚àíx‚àó= x(0) ‚àíx‚àó‚àíJ‚àí1
F (x(0))[F(x(0)) ‚àíF(x‚àó)].
Factoring out J‚àí1
F (x(0)) on the right hand side and passing to the norms, we get
‚à•x(1) ‚àíx‚àó‚à•
‚â§‚à•J‚àí1
F (x(0))‚à•‚à•F(x‚àó) ‚àíF(x(0)) ‚àíJF(x(0))[x‚àó‚àíx(0)]‚à•
‚â§2C L
2 ‚à•x‚àó‚àíx(0)‚à•2
where the remainder of Taylor‚Äôs series of F has been used. The previous relation
proves (7.5) in the case k = 0; moreover, since x(0) ‚ààB(x‚àó; r), we have ‚à•x‚àó‚àí
x(0)‚à•‚â§1/(2CL), from which ‚à•x(1) ‚àíx‚àó‚à•‚â§1
2‚à•x‚àó‚àíx(0)‚à•.
This ensures that x(1) ‚ààB(x‚àó; r).
By a similar proof, one can check that, should (7.5) be true for a certain k,
then the same inequality would follow also for k + 1 in place of k. This proves
the theorem.
3
Theorem 7.1 thus conÔ¨Årms that Newton‚Äôs method is quadratically conver-
gent only if x(0) is suÔ¨Éciently close to the solution x‚àóand if the Jacobian
matrix is nonsingular. Moreover, it is worth noting that the computational
eÔ¨Äort needed to solve the linear system (7.4) can be excessively high as n
gets large. Also, JF(x(k)) could be ill-conditioned, which makes it quite diÔ¨É-
cult to obtain an accurate solution. For these reasons, several modiÔ¨Åcations
to Newton‚Äôs method have been proposed, which will be brieÔ¨Çy considered
in the later sections, referring to the specialized literature for further details
(see [OR70], [DS83], [Erh97], [BS90] and the references therein).
7.1.2
ModiÔ¨Åed Newton‚Äôs Methods
Several modiÔ¨Åcations of Newton‚Äôs method have been proposed in order
to reduce its cost when the computed solution is suÔ¨Éciently close to x‚àó.
Further variants, that are globally convergent, will be introduced for the
solution of the minimization problem (7.2).
1. Cyclic updating of the Jacobian matrix
An eÔ¨Écient alternative to method (7.4) consists of keeping the Jacobian
matrix (more precisely, its factorization) unchanged for a certain number,
say p ‚â•2, of steps. Generally, a deterioration of convergence rate is accom-
panied by a gain in computational eÔ¨Éciency.

7.1 Solution of Systems of Nonlinear Equations
285
Program 57 implements Newton‚Äôs method in the case in which the LU
factorization of the Jacobian matrix is updated once every p steps. The pro-
grams used to solve the triangular systems have been described in Chapter
3.
Here and in later codings in this chapter, we denote by x0 the initial
vector, by F and J the variables containing the functional expressions of F
and of its Jacobian matrix JF, respectively. The parameters toll and nmax
represent the stopping tolerance in the convergence of the iterative process
and the maximum admissible number of iterations, respectively. In output,
the vector x contains the approximation to the searched zero of F, while
nit denotes the number of iterations necessary to converge.
Program 57 - newtonxsys : Newton‚Äôs method for nonlinear systems
function [x, nit] = newtonsys(F, J, x0, toll, nmax, p)
[n,m]=size(F); nit=0; Fxn=zeros(n,1); x=x0; err=toll+1;
for i=1:n, for j=1:n, Jxn(i,j)=eval(J((i-1)*n+j,:)); end; end
[L,U,P]=lu(Jxn); step=0;
while err > toll
if step == p
step = 0;
for i=1:n;
Fxn(i)=eval(F(i,:));
for j=1:n; Jxn(i,j)=eval(J((i-1)*n+j,:)); end
end
[L,U,P]=lu(Jxn);
else
for i=1:n, Fxn(i)=eval(F(i,:)); end
end
nit=nit+1; step=step+1; Fxn=-P*Fxn; y=forward col(L,Fxn);
deltax=backward col(U,y); x = x + deltax; err=norm(deltax);
if nit > nmax
disp(‚Äô Fails to converge within maximum number of iterations ‚Äô);
break
end
end
2. Inexact solution of the linear systems
Another possibility consists of solving the linear system (7.4) by an iter-
ative method where the maximum number of admissible iterations is Ô¨Åxed
a priori. The resulting schemes are identiÔ¨Åed as Newton-Jacobi, Newton-
SOR or Newton-Krylov methods, according to the iterative process that is
used for the linear system (see [BS90], [Kel99]). Here, we limit ourselves to
describing the Newton-SOR method.

286
7. Nonlinear Systems and Numerical Optimization
In analogy with what was done in Section 4.2.1, let us decompose the
Jacobian matrix at step k as
JF(x(k)) = Dk ‚àíEk ‚àíFk
(7.6)
where Dk = D(x(k)), ‚àíEk = ‚àíE(x(k)) and ‚àíFk = ‚àíF(x(k)), the diagonal
part and the lower and upper triangular portions of the matrix JF(x(k)),
respectively. We suppose also that Dk is nonsingular. The SOR method for
solving the linear system in (7.4) is organized as follows: setting Œ¥x(k)
0
= 0,
solve
Œ¥x(k)
r
= MkŒ¥x(k)
r‚àí1 ‚àíœâk(Dk ‚àíœâkEk)‚àí1F(x(k)),
r = 1, 2, . . . ,
(7.7)
where Mk is the iteration matrix of SOR method
Mk = [Dk ‚àíœâkEk]‚àí1 [(1 ‚àíœâk)Dk + œâkFk] ,
and œâk is a positive relaxation parameter whose optimal value can rarely
be determined a priori. Assume that only r = m steps of the method are
carried out. Recalling that Œ¥x(k)
r
= x(k)
r
‚àíx(k) and still denoting by x(k+1)
the approximate solution computed after m steps, we Ô¨Ånd that this latter
can be written as (see Exercise 1)
x(k+1) = x(k) ‚àíœâk

Mm‚àí1
k
+ ¬∑ ¬∑ ¬∑ + I

(Dk ‚àíœâkEk)‚àí1 F(x(k)).
(7.8)
This method is thus a composite iteration, in which at each step k, starting
from x(k), m steps of the SOR method are carried out to solve approxi-
mately system (7.4).
The integer m, as well as œâk, can depend on the iteration index k; the
simplest choice amounts to performing, at each Newton‚Äôs step, only one
iteration of the SOR method, thus obtaining for r = 1 from (7.7) the one-
step Newton-SOR method
x(k+1) = x(k) ‚àíœâk (Dk ‚àíœâkEk)‚àí1 F(x(k)).
In a similar way, the preconditioned Newton-Richardson method with ma-
trix Pk, if truncated at the m-th iteration, is
x(k+1) = x(k) ‚àí
0
I + Mk + . . . + Mm‚àí1
k
1
P‚àí1
k F(x(k)),
where Pk is the preconditioner of JF and
Mk = P‚àí1
k Nk,
Nk = Pk ‚àíJF(x(k)).
For an eÔ¨Écient implementation of these techniques we refer to the MAT-
LAB software package developed in [Kel99].

7.1 Solution of Systems of Nonlinear Equations
287
3. DiÔ¨Äerence approximations of the Jacobian matrix
Another possibility consists of replacing JF(x(k)) (whose explicit compu-
tation is often very expensive) with an approximation through n-dimensional
diÔ¨Äerences of the form
(J(k)
h )j =
F(x(k) + h(k)
j ej) ‚àíF(x(k))
h(k)
j
,
‚àÄk ‚â•0,
(7.9)
where ej is the j-th vector of the canonical basis of Rn and h(k)
j
> 0 are
increments to be suitably chosen at each step k of the iteration (7.4). The
following result can be shown.
Property 7.1 Let F and x‚àóbe such that the hypotheses of Theorem 7.1
are fulÔ¨Ålled, where ‚à•¬∑‚à•denotes the ‚à•¬∑‚à•1 vector norm and the corresponding
induced matrix norm. If there exist two positive constants Œµ and h such
that x(0) ‚ààB(x‚àó, Œµ) and 0 < |h(k)
j | ‚â§h for j = 1, . . . , n then the sequence
deÔ¨Åned by
x(k+1) = x(k) ‚àí
6
J(k)
h
7‚àí1
F(x(k)),
(7.10)
is well deÔ¨Åned and converges linearly to x‚àó. Moreover, if there exists a
positive constant C such that max
j |h(k)
j | ‚â§C‚à•x(k) ‚àíx‚àó‚à•or, equivalently,
there exists a positive constant c such that max
j |h(k)
j | ‚â§c‚à•F(x(k))‚à•, then
the sequence (7.10) is convergent quadratically.
This result does not provide any constructive indication as to how to com-
pute the increments h(k)
j . In this regard, the following remarks can be made.
The Ô¨Årst-order truncation error with respect to h(k)
j , which arises from the
divided diÔ¨Äerence (7.10), can be reduced by reducing the sizes of h(k)
j . On
the other hand, a too small value for h(k)
j
can lead to large rounding er-
rors. A trade-oÔ¨Ämust therefore be made between the need of limiting the
truncation errors and ensuring a certain accuracy in the computations.
A possible choice is to take
h(k)
j
= ‚àöœµM max
2
|x(k)
j |, Mj
3
sign(xj),
where Mj is a parameter that characterizes the typical size of the com-
ponent xj of the solution. Further improvements can be achieved using
higher-order divided diÔ¨Äerences to approximate derivatives, like
(J(k)
h )j =
F(x(k) + h(k)
j ej) ‚àíF(x(k) ‚àíh(k)
j ej)
2h(k)
j
,
‚àÄk ‚â•0.
For further details on this subject, see, for instance, [BS90].

288
7. Nonlinear Systems and Numerical Optimization
7.1.3
Quasi-Newton Methods
By this term, we denote all those schemes in which globally convergent
methods are coupled with Newton-like methods that are only locally con-
vergent, but with an order greater than one.
In a quasi-Newton method, given a continuously diÔ¨Äerentiable function
F : Rn ‚ÜíRn, and an initial value x(0) ‚ààRn, at each step k one has to
accomplish the following operations:
1. compute F(x(k));
2. choose ÀúJF(x(k)) as being either the exact JF(x(k)) or an approxima-
tion of it;
3. solve the linear system ÀúJF(x(k))Œ¥x(k) = ‚àíF(x(k));
4. set x(k+1) = x(k) + Œ±kŒ¥x(k), where Œ±k are suitable damping parame-
ters.
Step 4. is thus the characterizing element of this family of methods. It will
be addressed in Section 7.2.6, where a criterion for selecting the ‚Äúdirection‚Äù
Œ¥x(k) will be provided.
7.1.4
Secant-like Methods
These methods are constructed starting from the secant method introduced
in Section 6.2 for scalar functions. Precisely, given two vectors x(0) and x(1),
at the generic step k ‚â•1 we solve the linear system
QkŒ¥x(k+1) = ‚àíF(x(k))
(7.11)
and we set x(k+1) = x(k) + Œ¥x(k+1). Qk is an n √ó n matrix such that
QkŒ¥x(k) = F(x(k)) ‚àíF(x(k‚àí1)) = b(k),
k ‚â•1,
and is obtained by a formal generalization of (6.13). However, the algebraic
relation above does not suÔ¨Éce to uniquely determine Qk. For this purpose
we require Qk for k ‚â•n to be a solution to the following set of n systems
Qk
+
x(k) ‚àíx(k‚àíj),
= F(x(k)) ‚àíF(x(k‚àíj)),
j = 1, . . . , n.
(7.12)
If the vectors x(k‚àíj), . . . , x(k) are linearly independent, system (7.12) allows
for calculating all the unknown coeÔ¨Écients {(Qk)lm, l, m = 1, . . . , n} of
Qk. Unfortunately, in practice the above vectors tend to become linearly
dependent and the resulting scheme is unstable, not to mention the need
for storing all the previous n iterates.
For these reasons, an alternative approach is pursued which aims at pre-
serving the information already provided by the method at step k. Precisely,

7.1 Solution of Systems of Nonlinear Equations
289
Qk is looked for in such a way that the diÔ¨Äerence between the following
linear approximants to F(x(k‚àí1)) and F(x(k)), respectively
F(x(k)) + Qk(x ‚àíx(k)),
F(x(k‚àí1)) + Qk‚àí1(x ‚àíx(k‚àí1)),
is minimized jointly with the constraint that Qk satisÔ¨Åes system (7.12).
Using (7.12) with j = 1, the diÔ¨Äerence between the two approximants is
found to be
dk = (Qk ‚àíQk‚àí1)
+
x ‚àíx(k‚àí1),
.
(7.13)
Let us decompose the vector x ‚àíx(k‚àí1) as
x ‚àíx(k‚àí1) = Œ±Œ¥x(k) + s,
where Œ± ‚ààR and sT Œ¥x(k) = 0. Therefore, (7.13) becomes
dk = Œ± (Qk ‚àíQk‚àí1) Œ¥x(k) + (Qk ‚àíQk‚àí1) s.
Only the second term in the relation above can be minimized since the Ô¨Årst
one is independent of Qk, being
(Qk ‚àíQk‚àí1)Œ¥x(k) = b(k) ‚àíQk‚àí1Œ¥x(k).
The problem has thus become: Ô¨Ånd the matrix Qk such that (Qk ‚àíQk‚àí1) s
is minimized ‚àÄs orthogonal to Œ¥x(k) with the constraint that (7.12) holds.
It can be shown that such a matrix exists and can be recursively computed
as follows
Qk = Qk‚àí1 + (b(k) ‚àíQk‚àí1Œ¥x(k))Œ¥x(k)T
Œ¥x(k)T Œ¥x(k)
.
(7.14)
The method (7.11), with the choice (7.14) of matrix Qk is known as the
Broyden method. To initialize (7.14), we set Q0 equal to the matrix JF(x(0))
or to any approximation of it, for instance, the one yielded by (7.9). As for
the convergence of Broyden‚Äôs method, the following result holds.
Property 7.2 If the assumptions of Theorem 7.1 are satisÔ¨Åed and there
exist two positive constants Œµ and Œ≥ such that
‚à•x(0) ‚àíx‚àó‚à•‚â§Œµ,
‚à•Q0 ‚àíJF(x‚àó)‚à•‚â§Œ≥,
then the sequence of vectors x(k) generated by Broyden‚Äôs method is well
deÔ¨Åned and converges superlinearly to x‚àó, that is
‚à•x(k) ‚àíx‚àó‚à•‚â§ck‚à•x(k‚àí1) ‚àíx‚àó‚à•
(7.15)
where the constants ck are such that lim
k‚Üí‚àûck = 0.

290
7. Nonlinear Systems and Numerical Optimization
Under further assumptions, it is also possible to prove that the sequence
Qk converges to JF(x‚àó), a property that does not necessarily hold for the
above method as demonstrated in Example 7.3.
There exist several variants to Broyden‚Äôs method which aim at reducing
its computational cost, but are usually less stable (see [DS83], Chapter 8).
Program 58 implements Broyden‚Äôs method (7.11)-(7.14). We have denoted
by Q the initial approximation Q0 in (7.14).
Program 58 - broyden : Broyden‚Äôs method for nonlinear systems
function [x,it]=broyden(x,Q,nmax,toll,f)
[n,m]=size(f); it=0;
err=1;
fk=zeros(n,1); fk1=fk;
for i=1:n,
fk(i)=eval(f(i,:)); end
while it < nmax & err > toll
s=-Q \ fk; x=s+x; err=norm(s,inf);
if err > toll
for i=1:n, fk1(i)=eval(f(i,:)); end
Q=Q+1/(s‚Äô*s)*fk1*s‚Äô
end
it=it+1; fk=fk1;
end
Example 7.2 Let us solve using Broyden‚Äôs method the nonlinear system of Ex-
ample 7.1. The method converges in 35 iterations to the value (0.7 ¬∑ 10‚àí8, 0.7 ¬∑
10‚àí8)T compared with the 26 iterations required by Newton‚Äôs method starting
from the same initial guess (x(0) = (0.1, 0.1)T ). The matrix Q0 has been set equal
to the Jacobian matrix evaluated at x(0). Figure 7.1 shows the behavior of the
Euclidean norm of the error for both methods.
‚Ä¢
Example 7.3 Suppose we wish to solve using the Broyden method the nonlinear
system F(x) = (x1+x2‚àí3; x2
1+x2
2‚àí9)T = 0. This system admits the two solutions
(0, 3)T and (3, 0)T . Broyden‚Äôs method converges in 8 iterations to the solution
(0, 3)T starting from x(0) = (2, 4)T . However, the sequence of Qk, stored in the
variable Q of Program 58, does not converge to the Jacobian matrix, since
lim
k‚Üí‚àûQ(k) =
 1
1
1.5
1.75

Ã∏= JF[(0, 3)T ] =
 1
1
0
6

.
‚Ä¢
7.1.5
Fixed-point Methods
We conclude the analysis of methods for solving systems of nonlinear equa-
tions by extending to n-dimensions the Ô¨Åxed-point techniques introduced
in the scalar case. For this, we reformulate problem (7.1) as
given G : Rn ‚ÜíRn, Ô¨Ånd x‚àó‚ààRn such that G(x‚àó) = x‚àó
(7.16)

7.1 Solution of Systems of Nonlinear Equations
291
0
5
10
15
20
25
30
35
40
10
‚àí9
10
‚àí8
10
‚àí7
10
‚àí6
10
‚àí5
10
‚àí4
10
‚àí3
10
‚àí2
10
‚àí1
FIGURE 7.1. Euclidean norm of the error for the Newton method (solid line)
and the Broyden method (dashed line) in the case of the nonlinear system of
Example 7.1
where G is related to F through the following property: if x‚àóis a Ô¨Åxed
point of G, then F(x‚àó) = 0.
Analogously to what was done in Section 6.3, we introduce iterative meth-
ods for the solution of (7.16) of the form:
given x(0) ‚ààRn, for k = 0, 1, . . . until convergence, Ô¨Ånd
x(k+1) = G(x(k)).
(7.17)
In order to analyze the convergence of the Ô¨Åxed-point iteration (7.17) the
following deÔ¨Ånition will be useful.
DeÔ¨Ånition 7.1 A mapping G : D ‚äÇRn ‚ÜíRn is contractive on a set
D0 ‚äÇD if there exists a constant Œ± < 1 such that ‚à•G(x) ‚àíG(y)‚à•‚â§
Œ±‚à•x ‚àíy‚à•for all x, y in D0 where ‚à•¬∑ ‚à•is a suitable vector norm.
‚ñ†
The existence and uniqueness of a Ô¨Åxed point for G is ensured by the
following theorem.
Theorem 7.2 (contraction-mapping theorem) Suppose that G : D ‚äÇ
Rn ‚ÜíRn is contractive on a closed set D0 ‚äÇD and that G(x) ‚äÇD0 for
all x ‚ààD0. Then G has a unique Ô¨Åxed point in D0.
Proof. Let us Ô¨Årst prove the uniqueness of the Ô¨Åxed point. For this, assume that
there exist two distinct Ô¨Åxed points, x‚àó, y‚àó. Then
‚à•x‚àó‚àíy‚àó‚à•= ‚à•G(x‚àó) ‚àíG(y‚àó)‚à•‚â§Œ±‚à•x‚àó‚àíy‚àó‚à•
from which (1 ‚àíŒ±)‚à•x‚àó‚àíy‚àó‚à•‚â§0. Since (1 ‚àíŒ±) > 0, it must necessarily be that
‚à•x‚àó‚àíy‚àó‚à•= 0, i.e., x‚àó= y‚àó.

292
7. Nonlinear Systems and Numerical Optimization
To prove the existence we show that x(k) given by (7.17) is a Cauchy sequence.
This in turn implies that x(k) is convergent to a point x(‚àó) ‚ààD0. Take x(0)
arbitrarily in D0. Then, since the image of G is included in D0, the sequence x(k)
is well deÔ¨Åned and
‚à•x(k+1) ‚àíx(k)‚à•= ‚à•G(x(k)) ‚àíG(x(k‚àí1))‚à•‚â§Œ±‚à•x(k) ‚àíx(k‚àí1)‚à•.
After p steps, p ‚â•1, we obtain
‚à•x(k+p) ‚àíx(k)‚à•
‚â§
p

i=1
‚à•x(k+i) ‚àíx(k+i‚àí1)‚à•‚â§

Œ±p‚àí1 + . . . + 1

‚à•x(k+1) ‚àíx(k)‚à•
‚â§
Œ±k
1 ‚àíŒ±‚à•x(1) ‚àíx(0)‚à•.
Owing to the continuity of G it follows that lim
k‚Üí‚àûG(x(k)) = G(x(‚àó)) which proves
that x(‚àó) is a Ô¨Åxed point for G.
3
The following result provides a suÔ¨Écient condition for the iteration (7.17) to
converge (for the proof see [OR70], pp. 299-301), and extends the analogous
Theorem 6.3 in the scalar case.
Property 7.3 Suppose that G : D ‚äÇRn ‚ÜíRn has a Ô¨Åxed point x‚àóin the
interior of D and that G is continuously diÔ¨Äerentiable in a neighborhood of
x‚àó. Denote by JG the Jacobian matrix of G and assume that œÅ(JG(x(‚àó))) <
1. Then there exists a neighborhood S of x‚àósuch that S ‚äÇD and, for any
x(0) ‚ààS, the iterates deÔ¨Åned by (7.17) all lie in D and converge to x‚àó.
As usual, since the spectral radius is the inÔ¨Åmum of the induced matrix
norms, in order for convergence to hold it suÔ¨Éces to check that ‚à•JG(x)‚à•< 1
for some matrix norm.
Example 7.4 Consider the nonlinear system
F(x) =

x2
1 + x2
2 ‚àí1, 2x1 + x2 ‚àí1
T = 0,
whose solutions are x‚àó
1 = (0, 1)T and x‚àó
2 = (4/5, ‚àí3/5)T . To solve it, let us use
two Ô¨Åxed-point schemes, respectively deÔ¨Åned by the following iteration functions
G1(x) =
Ô£Æ
Ô£∞
1 ‚àíx2
2

1 ‚àíx2
1
Ô£π
Ô£ª,
G2(x) =
Ô£Æ
Ô£∞
1 ‚àíx2
2
‚àí

1 ‚àíx2
1
Ô£π
Ô£ª.
(7.18)
It can be checked that Gi(x‚àó
i ) = x‚àó
i for i = 1, 2 and that the Jacobian matrices
of G1 and G2, evaluated at x‚àó
1 and x‚àó
2 respectively, are
JG1(x‚àó
1) =
Ô£Æ
Ô£∞
0
‚àí1
2
0
0
Ô£π
Ô£ª,
JG2(x‚àó
2) =
Ô£Æ
Ô£∞
0
‚àí1
2
4
3
0
Ô£π
Ô£ª.

7.1 Solution of Systems of Nonlinear Equations
293
The spectral radii are œÅ(JG1(x‚àó
1)) = 0 and œÅ(JG2(x‚àó
2)) =

2/3 ‚âÉ0.817 < 1 so
that both methods are convergent in a suitable neighborhood of their respective
Ô¨Åxed points.
Running Program 59, with a tolerance of 10‚àí10 on the maximum absolute
diÔ¨Äerence between two successive iterates, the Ô¨Årst scheme converges to x‚àó
1 in 9
iterations, starting from x(0) = (‚àí0.9, 0.9)T , while the second one converges to
x‚àó
2 in 115 iterations, starting from x(0) = (0.9, 0.9)T . The dramatic change in
the convergence behavior of the two methods can be explained in view of the
diÔ¨Äerence between the spectral radii of the corresponding iteration matrices.
‚Ä¢
Remark 7.1 Newton‚Äôs method can be regarded as a Ô¨Åxed-point method
with iteration function
GN(x) = x ‚àíJ‚àí1
F (x)F(x).
(7.19)
If we denote by r(k) = F(x(k)) the residual at step k, from (7.19) it turns
out that Newton‚Äôs method can be alternatively formulated as
+
I ‚àíJGN (x(k))
, +
x(k+1) ‚àíx(k),
= ‚àír(k).
This equation allows us to interpret Newton‚Äôs method as a preconditioned
stationary Richardson method. This prompts introducing a parameter Œ±k
in order to accelerate the convergence of the iteration
+
I ‚àíJGN (x(k))
, +
x(k+1) ‚àíx(k),
= ‚àíŒ±kr(k).
The problem of how to select Œ±k will be addressed in Section 7.2.6.
‚ñ†
An implementation of the Ô¨Åxed-point method (7.17) is provided in Pro-
gram 59. We have denoted by dim the size of the nonlinear system and
by Phi the variables containing the functional expressions of the iteration
function G. In output, the vector alpha contains the approximation of the
sought zero of F and the vector res contains the sequence of the maximum
norms of the residuals of F(x(k)).
Program 59 - Ô¨Åxposys : Fixed-point method for nonlinear systems
function [alpha, res, nit]=Ô¨Åxposys(dim, x0, nmax, toll, Phi, F)
x = x0; alpha=[x‚Äô]; res = 0;
for k=1:dim,
r=abs(eval(F(k,:))); if (r > res), res = r; end
end;
nit = 0; residual(1)=res;
while ((nit <= nmax) & (res >= toll)),
nit = nit + 1;
for k = 1:dim, xnew(k) = eval(Phi(k,:)); end
x = xnew; res = 0; alpha=[alpha;x]; x=x‚Äô;
for k = 1:dim,

294
7. Nonlinear Systems and Numerical Optimization
r = abs(eval(F(k,:)));
if (r > res), res=r; end,
end
residual(nit+1)=res;
end
res=residual‚Äô;
7.2
Unconstrained Optimization
We turn now to minimization problems. The point x‚àó, the solution of (7.2),
is called a global minimizer of f, while x‚àóis a local minimizer of f if ‚àÉR > 0
such that
f(x‚àó) ‚â§f(x),
‚àÄx ‚ààB(x‚àó; R).
Throughout this section we shall always assume that f ‚ààC1(Rn), and
we refer to [Lem89] for the case in which f is non diÔ¨Äerentiable. We shall
denote by
‚àáf(x) =
 ‚àÇf
‚àÇx1
(x), . . . , ‚àÇf
‚àÇxn
(x)
T
,
the gradient of f at a point x. If d is a non null vector in Rn, then the
directional derivative of f with respect to d is
‚àÇf
‚àÇd(x) = lim
Œ±‚Üí0
f(x + Œ±d) ‚àíf(x)
Œ±
and satisÔ¨Åes ‚àÇf(x)/‚àÇd = [‚àáf(x)]T d. Moreover, denoting by (x, x + Œ±d)
the segment in Rn joining the points x and x + Œ±d, with Œ± ‚ààR, Taylor‚Äôs
expansion ensures that ‚àÉŒæ ‚àà(x, x + Œ±d) such that
f(x + Œ±d) ‚àíf(x) = Œ±‚àáf(Œæ)T d.
(7.20)
If f ‚ààC2(Rn), we shall denote by H(x) (or ‚àá2f(x)) the Hessian matrix of
f evaluated at a point x, whose entries are
hij(x) = ‚àÇ2f(x)
‚àÇxi‚àÇxj
,
i, j = 1, . . . , n.
In such a case it can be shown that, if d Ã∏= 0, the second-order directional
derivative exists and we have
‚àÇ2f
‚àÇd2 (x) = dT H(x)d.

7.2 Unconstrained Optimization
295
For a suitable Œæ ‚àà(x, x + d) we also have
f(x + d) ‚àíf(x) = ‚àáf(x)T d + 1
2dT H(Œæ)d.
Existence and uniqueness of solutions for (7.2) are not guaranteed in Rn.
Nevertheless, the following optimality conditions can be proved.
Property 7.4 Let x‚àó‚ààRn be a local minimizer of f and assume that
f ‚ààC1(B(x‚àó; R)) for a suitable R > 0. Then ‚àáf(x‚àó) = 0. Moreover,
if f ‚ààC2(B(x‚àó; R)) then H(x‚àó) is positive semideÔ¨Ånite. Conversely, if
x‚àó‚ààB(x‚àó; R) and H(x‚àó) is positive deÔ¨Ånite, then x‚àóis a local minimizer
of f in B(x‚àó; R).
A point x‚àósuch that ‚àáf(x‚àó) = 0, is said to be a critical point for f. This
condition is necessary for optimality to hold. However, this condition also
becomes suÔ¨Écient if f is a convex function on Rn, i.e., such that ‚àÄx, y ‚ààRn
and for any Œ± ‚àà[0, 1]
f[Œ±x + (1 ‚àíŒ±)y] ‚â§Œ±f(x) + (1 ‚àíŒ±)f(y).
(7.21)
For further and more general existence results, see [Ber82].
7.2.1
Direct Search Methods
In this section we deal with direct methods for solving problem (7.2), which
only require f to be continuous. In later sections, we shall introduce the
so-called descent methods, which also involve values of the derivatives of f
and have, in general, better convergence properties.
Direct methods are employed when f is not diÔ¨Äerentiable or if the com-
putation of its derivatives is a nontrivial task. They can also be used to
provide an approximate solution to employ as an initial guess for a descent
method. For further details, we refer to [Wal75] and [Wol78].
The Hooke and Jeeves Method
Assume we are searching for the minimizer of f starting from a given initial
point x(0) and requiring that the error on the residual is less than a certain
Ô¨Åxed tolerance œµ. The Hooke and Jeeves method computes a new point x(1)
using the values of f at suitable points along the orthogonal coordinate
directions around x(0). The method consists of two steps: an exploration
step and an advancing step.
The exploration step starts by evaluating f(x(0) + h1e1), where e1 is the
Ô¨Årst vector of the canonical basis of Rn and h1 is a positive real number to
be suitably chosen.
If f(x(0) + h1e1) < f(x(0)), then a success is recorded and the starting
point is moved in x(0) +h1e1, from which an analogous check is carried out
at point x(0) + h1e1 + h2e2 with h2 ‚ààR+.

296
7. Nonlinear Systems and Numerical Optimization
If, instead, f(x(0) + h1e1) ‚â•f(x(0)), then a failure is recorded and a
similar check is performed at x(0) ‚àíh1e1. If a success is registered, the
method explores, as previously, the behavior of f in the direction e2 starting
from this new point, while, in case of a failure, the method passes directly
to examining direction e2, keeping x(0) as starting point for the exploration
step.
To achieve a certain accuracy, the step lengths hi must be selected in
such a way that the quantities
|f(x(0) ¬± hjej) ‚àíf(x(0)|,
j = 1, . . . , n
(7.22)
have comparable sizes.
The exploration step terminates as soon as all the n Cartesian directions
have been examined. Therefore, the method generates a new point, y(0),
after at most 2n+1 functional evaluations. Only two possibilities may arise:
1. y(0) = x(0). In such a case, if
max
i=1,... ,nhi ‚â§œµ the method terminates
and yields the approximate solution x(0). Otherwise, the step lengths
hi are halved and another exploration step is performed starting from
x(0);
2. y(0) Ã∏= x(0). If
max
i=1,... ,n|hi| < œµ, then the method terminates yielding
y(0) as an approximate solution, otherwise the advancing step starts.
The advancing step consists of moving further from y(0) along the
direction y(0) ‚àíx(0) (which is the direction that recorded the maxi-
mum decrease of f during the exploration step), rather then simply
setting y(0) as a new starting point x(1).
This new starting point is instead set equal to 2y(0) ‚àíx(0). From this
point a new series of exploration moves is started. If this exploration
leads to a point y(1) such that f(y(1)) < f(y(0) ‚àíx(0)), then a new
starting point for the next exploration step has been found, otherwise
the initial guess for further explorations is set equal to y(1) = y(0) ‚àí
x(0).
The method is now ready to restart from the point x(1) just com-
puted.
Program 60 provides an implementation of the Hooke and Jeeves method.
The input parameters are the size n of the problem, the vector h of the
initial steps along the Cartesian directions, the variable f containing the
functional expression of f in terms of the components x(1), . . . , x(n), the
initial point x0 and the stopping tolerance toll equal to œµ. In output, the
code returns the approximate minimizer of f, x, the value minf attained by
f at x and the number of iterations needed to compute x up to the desired
accuracy. The exploration step is performed by Program 61.

7.2 Unconstrained Optimization
297
Program 60 - hookejeeves : The method of Hooke and Jeeves (HJ)
function [x,minf,nit]=hookejeeves(n,h,f,x0,toll)
x = x0; minf = eval(f); nit = 0;
while h > toll
[y] = explore(h,n,f,x);
if y == x, h = h/2; else
x = 2*y-x;
[z] = explore(h,n,f,x);
if z == x,
x = y; else,
x = z; end
end
nit = nit +1;
end
minf = eval(f);
Program 61 - explore : Exploration step in the HJ method
function [x]=explore(h,n,f,x0)
x = x0; f0 = eval(f);
for i=1:n
x(i) = x(i) + h(i); Ô¨Ä= eval(f);
if Ô¨Ä< f0,
f0 = Ô¨Ä;
else
x(i) = x0(i) - h(i);
Ô¨Ä= eval(f);
if Ô¨Ä< f0,
f0 = Ô¨Ä;
else,
x(i) = x0 (i);
end
end
end
The Method of Nelder and Mead
This method, proposed in [NM65], employs local linear approximants of f
to generate a sequence of points x(k), approximations of x‚àó, starting from
simple geometrical considerations. To explain the details of the algorithm,
we begin by noticing that a plane in Rn is uniquely determined by Ô¨Åxing
n + 1 points that must not be lying on a hyperplane.
Denote such points by x(k), for k = 0, . . . , n. They could be generated as
x(k) = x(0) + hkek,
k = 1, . . . , n
(7.23)
having selected the steplengths hk ‚ààR+ in such a way that the variations
(7.22) are of comparable size.
Let us now denote by x(M), x(m) and x(¬µ) those points of the set

x(k)
at which f respectively attains its maximum and minimum value and the
value immediately preceding the maximum. Moreover, denote by x(k)
c
the
centroid of point x(k) deÔ¨Åned as
x(k)
c
= 1
n
n

j=0,jÃ∏=k
x(j).

298
7. Nonlinear Systems and Numerical Optimization
The method generates a sequence of approximations of x‚àó, starting from
x(k), by employing only three possible transformations: reÔ¨Çections with
respect to centroids, dilations and contractions. Let us examine the details
of the algorithm assuming that n + 1 initial points are available.
1. Determine the points x(M), x(m) and x(¬µ).
2. Compute as an approximation of x‚àóthe point
¬Øx =
1
n + 1
n

i=0
x(i)
and check if ¬Øx is suÔ¨Éciently close (in a sense to be made precise) to
x‚àó. Typically, one requires that the standard deviation of the values
f(x(0)), . . . , f(x(n)) from
¬Øf =
1
n + 1
n

i=0
f(x(i))
are less than a Ô¨Åxed tolerance Œµ, that is
1
n
n

i=0
+
f(x(i)) ‚àí¬Øf
,2
< Œµ.
Otherwise, x(M) is reÔ¨Çected with respect to x(M)
c
, that is, the follow-
ing new point xr is computed
xr = (1 + Œ±)x(M)
c
‚àíŒ±x(M),
where Œ± ‚â•0 is a suitable reÔ¨Çection factor. Notice that the method
has moved along the ‚Äúopposite‚Äù direction to x(M). This statement has
a geometrical interpretation in the case n = 2, since the points x(k)
coincide with x(M), x(m) and x(¬µ). They thus deÔ¨Åne a plane whose
slope points from x(M) towards x(m) and the method provides a step
along this direction.
3. If f(x(m)) ‚â§f(x(r)) ‚â§f(x(¬µ)), the point x(M) is replaced by x(r)
and the algorithm returns to step 2.
4. If f(x(r)) < f(x(m)) then the reÔ¨Çection step has produced a new
minimizer. This means that the minimizer could lie outside the set
deÔ¨Åned by the convex hull of the considered points. Therefore, this
set must be expanded by computing the new vertex
x(e) = Œ≤x(r) + (1 ‚àíŒ≤)x(M)
c
,
where Œ≤ > 1 is an expansion factor. Then, before coming back to step
2., two possibilities arise:

7.2 Unconstrained Optimization
299
4a. if f(x(e)) < f(x(m)) then x(M) is replaced by x(e);
4b. f(x(e)) ‚â•f(x(m)) then x(M) is replaced by x(r) since f(x(r)) <
f(x(m)).
5. If f(x(r)) > f(x(¬µ)) then the minimizer probably lies within a subset
of the convex hull of points

x(k)
and, therefore, two diÔ¨Äerent ap-
proaches can be pursued to contract this set. If f(x(r)) < f(x(M)),
the contraction generates a new point of the form
x(co) = Œ≥x(r) + (1 ‚àíŒ≥)x(M)
c
,
Œ≥ ‚àà(0, 1),
otherwise,
x(co) = Œ≥x(M) + (1 ‚àíŒ≥)x(M)
c
,
Œ≥ ‚àà(0, 1),
Finally, before returning to step 2., if f(x(co)) < f(x(M)) and f(x(co)) <
f(x(r)), the point x(M) is replaced by x(co), while if f(x(co)) ‚â•f(x(M))
or if f(x(co)) > f(x(r)), then n new points x(k) are generated, with
k = 1, . . . , n, by halving the distances between the original points
and x(0).
As far as the choice of the parameters Œ±, Œ≤ and Œ≥ is concerned, the following
values are empirically suggested in [NM65]: Œ± = 1, Œ≤ = 2 and Œ≥ = 1/2. The
resulting scheme is known as the Simplex method (that must not be con-
fused with a method sharing the same name used in linear programming),
since the set of the points x(k), together with their convex combinations,
form a simplex in Rn.
The convergence rate of the method is strongly aÔ¨Äected by the orientation
of the starting simplex. To address this concern, in absence of information
about the behavior of f, the initial choice (7.23) turns out to be satisfactory
in most cases.
We Ô¨Ånally mention that the Simplex method is the basic ingredient of
the MATLAB function fmins for function minimization in n dimensions.
Example 7.5 Let us compare the performances of the Simplex method with the
Hooke and Jeeves method, in the minimization of the Rosembrock function
f(x) = 100(x2 ‚àíx2
1)2 + (1 ‚àíx1)2.
(7.24)
This function has a minimizer at (1, 1)T and represents a severe benchmark for
testing numerical methods in minimization problems. The starting point for both
methods is set equal to x(0) = (‚àí1.2, 1)T , while the step sizes are taken equal
to h1 = 0.6 and h2 = 0.5, in such a way that (7.23) is satisÔ¨Åed. The stopping
tolerance on the residual is set equal to 10‚àí4. For the implementation of Simplex
method, we have used the MATLAB function fmins.
Figure 7.2 shows the iterates computed by the Hooke and Jeeves method (of
which one in every ten iterates have been reported, for the sake of clarity) and by

300
7. Nonlinear Systems and Numerical Optimization
‚àí1.5
‚àí1
‚àí0.5
0
0.5
1
1.5
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
FIGURE
7.2.
Convergence
histories
of
the
Hooke
and
Jeeves
method
(crossed-line) and the Simplex method (circled-line). The level curves of the min-
imized function (7.24) are reported in dashed line
the Simplex method, superposed to the level curves of the Rosembrock function.
The graph demonstrates the diÔ¨Éculty of this benchmark: actually, the function
is like a curved, narrow valley, which attains its minimum along the parabola of
equation x2
1 ‚àíx2 = 0.
The Simplex method converges in only 165 iterations, while 935 are needed for
the Hooke and Jeeves method to converge. The former scheme yields a solution
equal to (0.999987, 0.999978)T , while the latter gives the vector (0.9655, 0.9322)T .
‚Ä¢
7.2.2
Descent Methods
In this section we introduce iterative methods that are more sophisticated
than those examined in Section 7.2.1. They can be formulated as follows:
given an initial vector x(0) ‚ààRn, compute for k ‚â•0 until convergence
x(k+1) = x(k) + Œ±kd(k),
(7.25)
where d(k) is a suitably chosen direction and Œ±k is a positive parameter
(called stepsize) that measures the step along the direction d(k). This di-
rection d(k) is a descent direction if
d(k)T ‚àáf(x(k)) < 0
if ‚àáf(x(k)) Ã∏= 0,
d(k) = 0
if ‚àáf(x(k)) = 0.
(7.26)
A descent method is a method like (7.25), in which the vectors d(k) are
descent directions.
Property (7.20) ensures that there exists Œ±k > 0, suÔ¨Éciently small, such
that
f(x(k) + Œ±kd(k)) < f(x(k)),
(7.27)

7.2 Unconstrained Optimization
301
provided that f is continuously diÔ¨Äerentiable. Actually, taking in (7.20)
Œæ = x(k) + œëŒ±kd(k) with œë ‚àà(0, 1), and employing the continuity of ‚àáf,
we get
f(x(k) + Œ±kd(k)) ‚àíf(x(k)) = Œ±k‚àáf(x(k))T d(k) + Œµ,
(7.28)
where Œµ tends to zero as Œ±k tends to zero. As a consequence, if Œ±k > 0 is
suÔ¨Éciently small, the sign of the left-side of (7.28) coincides with the sign
of ‚àáf(x(k))T d(k), so that (7.27) is satisÔ¨Åed if d(k) is a descent direction.
DiÔ¨Äerent choices of d(k) correspond to diÔ¨Äerent methods. In particular,
we recall the following ones:
- Newton‚Äôs method, in which
d(k) = ‚àíH‚àí1(x(k))‚àáf(x(k)),
provided that H is positive deÔ¨Ånite within a suÔ¨Éciently large neigh-
borhood of point x‚àó;
- inexact Newton‚Äôs methods, in which
d(k) = ‚àíB‚àí1
k ‚àáf(x(k)),
where Bk is a suitable approximation of H(x(k));
- the gradient method or steepest descent method, corresponding to setting
d(k) = ‚àí‚àáf(x(k)). This method is thus an inexact Newton‚Äôs method,
in which Bk = I. It can also be regarded as a gradient-like method,
since d(k)T ‚àáf(x(k)) = ‚àí‚à•‚àáf(x(k))‚à•2
2;
- the conjugate gradient method, for which
d(k) = ‚àí‚àáf(x(k)) + Œ≤kd(k‚àí1),
where Œ≤k is a scalar to be suitably selected in such a way that the
directions

d(k)
turn out to be mutually orthogonal with respect to
a suitable scalar product.
Selecting d(k) is not enough to completely identify a descent method,
since it remains an open problem how to determine Œ±k in such a way that
(7.27) is fulÔ¨Ålled without resorting to excessively small stepsizes Œ±k (and,
thus, to methods with a slow convergence).
A method for computing Œ±k consists of solving the following minimiza-
tion problem in one dimension:
Ô¨Ånd Œ± such that œÜ(Œ±) = f(x(k) + Œ±d(k)) is minimized.
(7.29)
In such a case we have the following result.

302
7. Nonlinear Systems and Numerical Optimization
Theorem 7.3 Consider the descent method (7.25). If at the generic step
k, the parameter Œ±k is set equal to the exact solution of (7.29), then the
following orthogonality property holds
‚àáf(x(k+1))T d(k) = 0.
Proof. Let Œ±k be a solution to (7.29). Then, the Ô¨Årst derivative of œÜ, given by
œÜ‚Ä≤(Œ±) =
n

i=1
‚àÇf
‚àÇxi (x(k) + Œ±kd(k)) ‚àÇ
‚àÇŒ±(x(k)
i
+ Œ±d(k)
i
) = ‚àáf(x(k) + Œ±kd(k))T d(k),
vanishes at Œ± = Œ±k. The thesis then follows, recalling the deÔ¨Ånition of x(k+1). 3
Unfortunately, except for in special cases (which are nevetherless quite
relevant, see Section 7.2.4), providing an exact solution of (7.29) is not fea-
sible, since this is a nonlinear problem. One possible strategy consists of
approximating f along the straight line x(k) + Œ±d(k) through an interpo-
lating polynomial and then minimizing this polynomial (see the quadratic
interpolation Powell methods and cubic interpolation Davidon methods in
[Wal75]).
Generally speaking, a process that leads to an approximate solution to
(7.29) is said to be a line search technique and is addressed in the next
section.
7.2.3
Line Search Techniques
The methods that we are going to deal with in this section, are iterative
techniques that terminate as soon as some accuracy stopping criterion on
Œ±k is satisÔ¨Åed. We shall assume that (7.26) holds.
Practical experience reveals that it is not necessary to solve accurately
for (7.29) in order to devise eÔ¨Écient methods, rather, it is crucial to enforce
some limitation on the step lengths (and, thus, on the admissible values for
Œ±k). Actually, without introducing any limitation, a reasonable request on
Œ±k would seem be that the new iterate x(k+1) satisÔ¨Åes the inequality
f(x(k+1)) < f(x(k)),
(7.30)
where x(k) and d(k) have been Ô¨Åxed. For this purpose, the procedure based
on starting from a (suÔ¨Éciently large) value of the step length Œ±k and halve
this value until (7.30) is fulÔ¨Ålled, can yield completely wrong results (see,
[DS83]).
More stringent criteria than (7.30) should be adopted in the choice of
possible values for Œ±k. To this end, we notice that two kinds of diÔ¨Éculties
arise with the above examples: a slow descent rate of the sequence and the
use of small stepsizes.

7.2 Unconstrained Optimization
303
The Ô¨Årst diÔ¨Éculty can be overcome by requiring that
0 ‚â•vM(x(k+1))
=
1
Œ±k
6
f(x(k)) ‚àíf(x(k) + Œ±kd(k))
7
‚â•
‚àíœÉ‚àáf(x(k))T d(k),
(7.31)
with œÉ ‚àà(0, 1/2). This amounts to requiring that the average descent
rate vM of f along d(k), evaluated at x(k+1), be at least equal to a given
fraction of the initial descent rate at x(k). To avoid the generation of too
small stepsizes, we require that the descent rate in the direction d(k) at
x(k+1) is not less than a given fraction of the descent rate at x(k)
|‚àáf(x(k) + Œ±kd(k))T d(k)| ‚â§Œ≤|‚àáf(x(k))T d(k)|,
(7.32)
with Œ≤ ‚àà(œÉ, 1) in such a way as to also satisfy (7.31). In computational
practice, œÉ ‚àà[10‚àí5, 10‚àí1] and Œ≤ ‚àà[10‚àí1, 1
2] are usual choices. Sometimes,
(7.32) is replaced by the milder condition
‚àáf(x(k) + Œ±kd(k))T d(k) ‚â•Œ≤‚àáf(x(k))T d(k)
(7.33)
(recall that ‚àáf(x(k))T d(k) is negative, since d(k) is a descent direction).
The following property ensures that, under suitable assumptions, it is pos-
sible to Ô¨Ånd out values of Œ±k which satisfy (7.31)-(7.32) or (7.31)-(7.33).
Property 7.5 Assume that f(x) ‚â•M for any x ‚ààRn. Then there exists
an interval I = [c, C] for the descent method, with 0 < c < C, such that
‚àÄŒ±k ‚ààI, (7.31), (7.32) (or (7.31)-(7.33)) are satisÔ¨Åed, with œÉ ‚àà(0, 1/2)
and Œ≤ ‚àà(œÉ, 1).
Under the constraint of fulÔ¨Ålling conditions (7.31) and (7.32), several
choices for Œ±k are available. Among the most up-to-date strategies, we re-
call here the backtracking techniques: having Ô¨Åxed œÉ ‚àà(0, 1/2), then start
with Œ±k = 1 and then keep on reducing its value by a suitable scale factor
œÅ ‚àà(0, 1) (backtrack step) until (7.31) is satisÔ¨Åed. This procedure is im-
plemented in Program 62, which requires as input parameters the vector x
containing x(k), the macros f and J of the functional expressions of f and
its Jacobian, the vector d of the direction d(k), and a value for œÉ (usually
of the order of 10‚àí4) and the scale factor œÅ. In output, the code returns
the vector x(k+1), computed using a suitable value of Œ±k.
Program 62 - backtrackr : Backtraking for line search
function [xnew]= backtrackr(sigma,rho,x,f,J,d)
alphak = 1; fk = eval(f); Jfk = eval (J);
xx = x; x = x + alphak * d; fk1 = eval (f);
while fk1 > fk + sigma * alphak * Jfk‚Äô*d

304
7. Nonlinear Systems and Numerical Optimization
alphak = alphak*rho;
x = xx + alphak*d;
fk1 = eval(f);
end
Other commonly used strategies are those developed by Armijo and Gold-
stein (see [Arm66], [GP67]). Both use œÉ ‚àà(0, 1/2). In the Armijo formula,
one takes Œ±k = Œ≤mk ¬ØŒ±, where Œ≤ ‚àà(0, 1), ¬ØŒ± > 0 and mk is the Ô¨Årst non-
negative integer such that (7.31) is satisÔ¨Åed. In the Goldstein formula, the
parameter Œ±k is determined in such a way that
œÉ ‚â§f(x(k) + Œ±kd(k)) ‚àíf(x(k))
Œ±k‚àáf(x(k))T d(k)
‚â§1 ‚àíœÉ.
(7.34)
A procedure for computing Œ±k that satisÔ¨Åes (7.34) is provided in [Ber82],
Chapter 1. Of course, one can even choose Œ±k = ¬ØŒ± for any k, which is
clearly convenient when evaluating f is a costly task.
In any case, a good choice of the value ¬ØŒ± is mandatory. In this respect,
one can proceed as follows. For a given value ¬ØŒ±, the second degree poly-
nomial Œ†2 along the direction d(k) is constructed, subject to the following
interpolation constraints
Œ†2(x(k)) = f(x(k)),
Œ†2(x(k) + ¬ØŒ±d(k)) = f(x(k) + ¬ØŒ±d(k)),
Œ†‚Ä≤
2(x(k)) = ‚àáf(x(k))T d(k).
Next, the value ÀúŒ± is computed such that Œ†2 is minimized, then, we let
¬ØŒ± = ÀúŒ±.
7.2.4
Descent Methods for Quadratic Functions
A case of remarkable interest, where the parameter Œ±k can be exactly com-
puted, is the problem of minimizing the quadratic function
f(x) = 1
2xT Ax ‚àíbT x,
(7.35)
where A‚ààRn√ón is a symmetric and positive deÔ¨Ånite matrix and b ‚ààRn.
In such a case, as already seen in Section 4.3.3, a necessary condition for
x‚àóto be a minimizer for f is that x‚àóis the solution of the linear system
(3.2). Actually, it can be checked that if f is a quadratic function
‚àáf(x) = Ax ‚àíb = ‚àír,
H(x) = A.
As a consequence, all gradient-like iterative methods developed in Section
4.3.3 for linear systems, can be extended tout-court to solve minimization
problems.

7.2 Unconstrained Optimization
305
In particular, having Ô¨Åxed a descent direction d(k), we can determine
the optimal value of the acceleration parameter Œ±k that appears in (7.25),
in such a way as to Ô¨Ånd the point where the function f, restricted to the
direction d(k), is minimized. Setting to zero the directional derivative, we
get
d
dŒ±k
f(x(k) + Œ±kd(k)) = ‚àíd(k)T r(k) + Œ±kd(k)T Ad(k) = 0
from which the following expression for Œ±k is obtained
Œ±k =
d(k)T r(k)
d(k)T Ad(k) .
(7.36)
The error introduced by the iterative process (7.25) at the k-th step is
‚à•x(k+1) ‚àíx‚àó‚à•2
A =

x(k+1) ‚àíx‚àóT A

x(k+1) ‚àíx‚àó
= ‚à•x(k) ‚àíx‚àó‚à•2
A + 2Œ±kd(k)T A

x(k) ‚àíx‚àó
+ Œ±2
kd(k)T Ad(k).
(7.37)
On the other hand ‚à•x(k) ‚àíx‚àó‚à•2
A = r(k)T A‚àí1r(k), so that from (7.37) it
follows that
‚à•x(k+1) ‚àíx‚àó‚à•2
A = œÅk‚à•x(k) ‚àíx‚àó‚à•2
A
(7.38)
having denoted by œÅk = 1 ‚àíœÉk, with
œÉk = (d(k)T r(k))2/
+
d(k),T
Ad(k) +
r(k),T
A‚àí1r(k)

.
Since A is symmetric and positive deÔ¨Ånite, œÉk is always positive. Moreover,
it can be directly checked that œÅk is strictly less than 1, except when d(k)
is orthogonal to r(k), in which case œÅk = 1.
The choice d(k) = r(k), which leads to the steepest descent method, pre-
vents this last circumstance from arising. In such a case, from (7.38) we
get
‚à•x(k+1) ‚àíx‚àó‚à•A ‚â§Œªmax ‚àíŒªmin
Œªmax + Œªmin
‚à•x(k) ‚àíx‚àó‚à•A
(7.39)
having employed the following result.
Lemma 7.1 (Kantorovich inequality) Let A ‚ààRn√ón be a symmetric
positive deÔ¨Ånite matrix whose eigenvalues with largest and smallest module
are given by Œªmax and Œªmin, respectively. Then, ‚àÄy ‚ààRn, y Ã∏= 0,
(yT y)2
(yT Ay)(yT A‚àí1y) ‚â•
4ŒªmaxŒªmin
(Œªmax + Œªmin)2 .

306
7. Nonlinear Systems and Numerical Optimization
It follows from (7.39) that, if A is ill-conditioned, the error reducing factor
for the steepest descent method is close to 1, yielding a slow convergence to
the minimizer x‚àó. As done in Chapter 4, this drawback can be overcome
by introducing directions d(k) that are mutually A-conjugate, i.e.
d(k)T Ad(m) = 0
if k Ã∏= m.
The corresponding methods enjoy the following Ô¨Ånite termination property.
Property 7.6 A method for computing the minimizer x‚àóof the quadratic
function (7.35) which employs A-conjugate directions terminates after at
most n steps if the acceleration parameter Œ±k is selected as in (7.36). More-
over, for any k, x(k+1) is the minimizer of f over the subspace generated
by the vectors x(0), d(0), . . . , d(k) and
r(k+1)T d(m) = 0
‚àÄm ‚â§k.
The A-conjugate directions can be determined by following the proce-
dure described in Section 4.3.4. Letting d(0) = r(0), the conjugate gradient
method for function minimization is
d(k+1) = r(k) + Œ≤kd(k),
Œ≤k = ‚àír(k+1)T Ad(k)
d(k)T Ad(k)
= r(k+1)T r(k+1)
r(k)T r(k)
,
x(k+1) = x(k) + Œ±kd(k).
It satisÔ¨Åes the following error estimate
‚à•x(k) ‚àíx‚àó‚à•A ‚â§2

K2(A) ‚àí1

K2(A) + 1
k
‚à•x(0) ‚àíx‚àó‚à•A,
which can be improved by lowering the condition number of A, i.e., resort-
ing to the preconditioning techniques that have been dealt with in Section
4.3.2.
Remark 7.2 (The nonquadratic case) The conjugate gradient method
can be extended to the case in which f is a non quadratic function. However,
in such an event, the acceleration parameter Œ±k cannot be exactly deter-
mined a priori, but requires the solution of a local minimization problem.
Moreover, the parameters Œ≤k can no longer be uniquely found. Among the
most reliable formulae, we recall the one due to Fletcher-Reeves,
Œ≤1 = 0,
Œ≤k =
‚à•‚àáf(x(k))‚à•2
2
‚à•‚àáf(x(k‚àí1))‚à•2
2
,
for k > 1

7.2 Unconstrained Optimization
307
and the one due to Polak-Ribi¬¥ere
Œ≤1 = 0,
Œ≤k = ‚àáf(x(k))
T (‚àáf(x(k)) ‚àí‚àáf(x(k‚àí1)))
‚à•‚àáf(x(k‚àí1))‚à•2
2
,
for k > 1.
‚ñ†
7.2.5
Newton-like Methods for Function Minimization
An alternative is provided by Newton‚Äôs method, which diÔ¨Äers from its ver-
sion for nonlinear systems in that now it is no longer applied to f, but to
its gradient.
Using the notation of Section 7.2.2, Newton‚Äôs method for function mini-
mization amounts to computing, for k = 0, 1, . . . , until convergence
d(k) = ‚àíH‚àí1
k ‚àáf(x(k)),
x(k+1) = x(k) + d(k),
(7.40)
where x(0) ‚ààRn is a given initial vector and having set Hk = H(x(k)). The
method can be derived by truncating Taylor‚Äôs expansion of f(x(k)) at the
second-order
f(x(k) + p) ‚âÉf(x(k)) + ‚àáf(x(k))T p + 1
2pT Hkp.
(7.41)
Selecting p in (7.41) in such a way that the new vector x(k+1) = x(k) + p
satisÔ¨Åes ‚àáf(xk+1) = 0, we end up with method (7.40), which thus con-
verges in one step if f is quadratic.
In the general case, a result analogous to Theorem 7.1 also holds for func-
tion minimization. Method (7.40) is therefore locally quadratically conver-
gent to the minimizer x‚àó. However, it is not convenient to use Newton‚Äôs
method from the beginning of the computation, unless x(0) is suÔ¨Éciently
close to x‚àó. Otherwise, indeed, Hk could not be invertible and the direc-
tions d(k) could fail to be descent directions. Moreover, if Hk is not positive
deÔ¨Ånite, nothing prevents the scheme (7.40) from converging to a saddle
point or a maximizer, which are points where ‚àáf is equal to zero. All these
drawbacks, together with the high computational cost (recall that a linear
system with matrix Hk must be solved at each iteration), prompt suit-
ably modifying method (7.40), which leads to the so-called quasi-Newton
methods.
A Ô¨Årst modiÔ¨Åcation, which applies to the case where Hk is not posi-
tive deÔ¨Ånite, yields the so-called Newton‚Äôs method with shift. The idea is
to prevent Newton‚Äôs method from converging to non-minimizers of f, by
applying the scheme to a new Hessian matrix ÀúHk = Hk + ¬µkIn, where, as

308
7. Nonlinear Systems and Numerical Optimization
usual, In denotes the identity matrix of order n and ¬µk is selected in such
a way that ÀúHk is positive deÔ¨Ånite. The problem is to determine the shift
¬µk with a reduced eÔ¨Äort. This can be done, for instance, by applying the
Gershgorin theorem to the matrix ÀúHk (see Section 5.1). For further details
on the subject, see [DS83] and [GMW81].
7.2.6
Quasi-Newton Methods
At the generic k-th iteration, a quasi-Newton method for function mini-
mization performs the following steps:
1. compute the Hessian matrix Hk, or a suitable approximation Bk;
2. Ô¨Ånd a descent direction d(k) (not necessarily coinciding with the di-
rection provided by Newton‚Äôs method), using Hk or Bk;
3. compute the acceleration parameter Œ±k;
4. update the solution, setting x(k+1) = x(k) + Œ±kd(k), according to a
global convergence criterion.
In the particular case where d(k) = ‚àíH‚àí1
k ‚àáf(x(k)), the resulting scheme is
called the damped Newton‚Äôs method. To compute Hk or Bk, one can resort
to either Newton‚Äôs method or secant-like methods, which will be considered
in Section 7.2.7.
The criteria for selecting the parameter Œ±k, that have been discussed in
Section 7.2.3, can now be usefully employed to devise globally convergent
methods. Property 7.5 ensures that there exist values of Œ±k satisfying (7.31),
(7.33) or (7.31), (7.32).
Let us then assume that a sequence of iterates x(k), generated by a
descent method for a given x(0), converge to a vector x‚àó. This vector will
not be, in general, a critical point for f. The following result gives some
conditions on the directions d(k) which ensure that the limit x‚àóof the
sequence is also a critical point of f.
Property 7.7 (Convergence) Let f : Rn ‚ÜíR be a continuously diÔ¨Äer-
entiable function, and assume that there exists L > 0 such that
‚à•‚àáf(x) ‚àí‚àáf(y)‚à•2 ‚â§L‚à•x ‚àíy‚à•2.
Then, if

x(k)
is a sequence generated by a gradient-like method which
fulÔ¨Ålls (7.31) and (7.33), then, one (and only one) of the following events
can occur:
1. ‚àáf(x(k)) = 0 for some k;
2.
lim
k‚Üí‚àûf(x(k)) = ‚àí‚àû;

7.2 Unconstrained Optimization
309
3.
lim
k‚Üí‚àû
‚àáf(x(k))T d(k)
‚à•d(k)‚à•2
= 0.
Thus, unless the pathological cases where the directions d(k) become too
large or too small with respect to ‚àáf(x(k)) or, even, are orthogonal to
‚àáf(x(k)), any limit of the sequence

x(k)
is a critical point of f.
The convergence result for the sequence x(k) can also be extended to the
sequence f(x(k)). Indeed, the following result holds.
Property 7.8 Let

x(k)
be a convergent sequence generated by a gradient-
like method, i.e., such that any limit of the sequence is also a critical point
of f. If the sequence

x(k)
is bounded, then ‚àáf(x(k)) tends to zero as
k ‚Üí‚àû.
For the proofs of the above results, see [Wol69] and [Wol71].
7.2.7
Secant-like methods
In quasi-Newton methods the Hessian matrix H is replaced by a suitable
approximation. Precisely, the generic iterate is
x(k+1) = x(k) ‚àíB‚àí1
k ‚àáf(x(k)) = x(k) + s(k).
Assume that f : Rn ‚ÜíR is of class C2 on an open convex set D ‚äÇRn.
In such a case, H is symmetric and, as a consequence, approximants Bk
of H ought to be symmetric. Moreover, if Bk were symmetric at a point
x(k), we would also like the next approximant Bk+1 to be symmetric at
x(k+1) = x(k) + s(k).
To generate Bk+1 starting from Bk, consider the Taylor expansion
‚àáf(x(k)) = ‚àáf(x(k+1)) + Bk+1(x(k) ‚àíx(k+1)),
from which we get
Bk+1s(k) = y(k),
with y(k) = ‚àáf(x(k+1)) ‚àí‚àáf(x(k)).
Using again a series expansion of B, we end up with the following Ô¨Årst-order
approximation of H
Bk+1 = Bk + (y(k) ‚àíBks(k))cT
cT s(k)
,
(7.42)
where c ‚ààRn and having assumed that cT s(k) Ã∏= 0. We notice that taking
c = s(k) yields Broyden‚Äôs method, already discussed in Section 7.1.4 in the
case of systems of nonlinear equations.
Since (7.42) does not guarantee that Bk+1 is symmetric, it must be
suitably modiÔ¨Åed. A way for constructing a symmetric approximant Bk+1

310
7. Nonlinear Systems and Numerical Optimization
consists of choosing c = y(k) ‚àíBks(k) in (7.42), assuming that (y(k) ‚àí
Bks(k))T s(k) Ã∏= 0. By so doing, the following symmetric Ô¨Årst-order approx-
imation is obtained
Bk+1 = Bk + (y(k) ‚àíBks(k))(y(k) ‚àíBks(k))T
(y(k) ‚àíBks(k))T s(k)
.
(7.43)
From a computational standpoint, disposing of an approximation for H
is not completely satisfactory, since the inverse of the approximation of
H appears in the iterative methods that we are dealing with. Using the
Sherman-Morrison formula (3.57), with Ck = B‚àí1
k , yields the following
recursive formula for the computation of the inverse
Ck+1 = Ck + (s(k) ‚àíCky(k))(s(k) ‚àíCky(k))T
(s(k) ‚àíCky(k))T y(k)
, k = 0, 1, . . .
(7.44)
having assumed that y(k) = Bs(k), where B is a symmetric nonsingular
matrix, and that (s(k) ‚àíCky(k))T y(k) Ã∏= 0.
An algorithm that employs the approximations (7.43) or (7.44), is po-
tentially unstable when (s(k) ‚àíCky(k))T y(k) ‚âÉ0, due to rounding errors.
For this reason, it is convenient to set up the previous scheme in a more
stable form. To this end, instead of (7.42), we introduce the approximation
B(1)
k+1 = Bk + (y(k) ‚àíBks(k))cT
cT s(k)
,
then, we deÔ¨Åne B(2)
k+1 as being the symmetric part
B(2)
k+1 = B(1)
k+1 + (B(1)
k+1)T
2
.
The procedure can be iterated as follows
B(2j+1)
k+1
= B(2j)
k+1 + (y(k) ‚àíB(2j)
k+1s(k))cT
cT s
,
B(2j+2)
k+1
= B(2j+1)
k+1
+ (B(2j+1)
k+1
)T
2
(7.45)
with k = 0, 1, . . . and having set B(0)
k+1 = Bk. It can be shown that the limit
as j tends to inÔ¨Ånity of (7.45) is
lim
j‚Üí‚àûB(j)
= Bk+1 = Bk + (y(k) ‚àíBks(k))cT + c(y(k) ‚àíBks(k))T
cT s(k)
‚àí(y(k) ‚àíBks(k))T s(k)
(cT s(k))2
ccT ,
(7.46)

7.3 Constrained Optimization
311
having assumed that cT s(k) Ã∏= 0. If c = s(k), the method employing (7.46)
is known as the symmetric Powell-Broyden method. Denoting by BSP B
the corresponding matrix Bk+1, it can be shown that BSP B is the unique
solution to the problem:
Ô¨Ånd ¬ØB such that ‚à•¬ØB ‚àíB‚à•F is minimized,
where ¬ØBs(k) = y(k) and ‚à•¬∑ ‚à•F is the Frobenius norm.
As for the error made approximating H(x(k+1)) with BSP B, it can be proved
that
‚à•BSP B ‚àíH(x(k+1))‚à•F ‚â§‚à•Bk ‚àíH(x(k))‚à•F + 3L‚à•s(k)‚à•,
where it is assumed that H is Lipschitz continuous, with Lipschitz constant
L, and that the iterates x(k+1) and x(k) belong to D.
To deal with the particular case in which the Hessian matrix is not only
symmetric but also positive deÔ¨Ånite, we refer to [DS83], Section 9.2.
7.3
Constrained Optimization
The simplest case of constrained optimization can be formulated as follows.
Given f : Rn ‚ÜíR,
minimize f(x), with x ‚àà‚Ñ¶‚äÇRn.
(7.47)
More precisely, the point x‚àóis said to be a global minimizer in ‚Ñ¶if it
satisÔ¨Åes (7.47), while it is a local minimizer if ‚àÉR > 0 such that
f(x‚àó) ‚â§f(x),
‚àÄx ‚ààB(x‚àó; R) ‚äÇ‚Ñ¶.
Existence of solutions to problem (7.47) is, for instance, ensured by the
Weierstrass theorem, in the case in which f is continuous and ‚Ñ¶is a closed
and bounded set. Under the assumption that ‚Ñ¶is a convex set, the following
optimality conditions hold.
Property 7.9 Let ‚Ñ¶‚äÇRn be a convex set, x‚àó‚àà‚Ñ¶and f ‚ààC1(B(x‚àó; R)),
for a suitable R > 0. Then:
1. if x‚àóis a local minimizer of f then
‚àáf(x‚àó)T (x ‚àíx‚àó) ‚â•0,
‚àÄx ‚àà‚Ñ¶;
(7.48)
2. moreover, if f is convex on ‚Ñ¶(see (7.21)) and (7.48) is satisÔ¨Åed, then
x‚àóis a global minimizer of f.

312
7. Nonlinear Systems and Numerical Optimization
We recall that f : ‚Ñ¶‚ÜíR is a strongly convex function if ‚àÉœÅ > 0 such
that
f[Œ±x + (1 ‚àíŒ±)y] ‚â§Œ±f(x) + (1 ‚àíŒ±)f(y) ‚àíŒ±(1 ‚àíŒ±)œÅ‚à•x ‚àíy‚à•2
2,
(7.49)
‚àÄx, y ‚àà‚Ñ¶and ‚àÄŒ± ‚àà[0, 1]. The following result holds.
Property 7.10 Let ‚Ñ¶‚äÇRn be a closed and convex set and f be a strongly
convex function in ‚Ñ¶. Then there exists a unique local minimizer x‚àó‚àà‚Ñ¶.
Throughout this section, we refer to [Avr76], [Ber82], [CCP70], [Lue73] and
[Man69], for the proofs of the quoted results and further details.
A remarkable instance of (7.47) is the following problem: given f : Rn ‚ÜíR,
minimize f(x), under the constraint that h(x) = 0,
(7.50)
where h : Rn ‚ÜíRm, with m ‚â§n, is a given function of components
h1, . . . , hm. The analogues of critical points in problem (7.50) are called
the regular points.
DeÔ¨Ånition 7.2 A point x‚àó‚ààRn, such that h(x‚àó) = 0, is said to be
regular if the column vectors of the Jacobian matrix Jh(x‚àó) are linearly
independent, having assumed that hi ‚ààC1(B(x‚àó; R)), for a suitable R > 0
and i = 1, . . . , m.
‚ñ†
Our aim now is to convert problem (7.50) into an unconstrained minimiza-
tion problem of the form (7.2), to which the methods introduced in Section
7.2 can be applied.
For this purpose, we introduce the Lagrangian function L : Rn+m ‚ÜíR
L(x, Œª) = f(x) + ŒªT h(x),
where the vector Œª is called the Lagrange multiplier. Moreover, let us de-
note by JL the Jacobian matrix associated with L, but where the partial
derivatives are only taken with respect to the variables x1, . . . , xn. The link
between (7.2) and (7.50) is then expressed by the following result.
Property 7.11 Let x‚àóbe a local minimizer for (7.50) and suppose that,
for a suitable R > 0, f, hi ‚ààC1(B(x‚àó; R)), for i = 1, . . . , m. Then there
exists a unique vector Œª‚àó‚ààRm such that JL(x‚àó, Œª‚àó) = 0.
Conversely, assume that x‚àó‚ààRn satisÔ¨Åes h(x‚àó) = 0 and that, for a
suitable R > 0 and i = 1, . . . , m, f, hi ‚ààC2(B(x‚àó; R)). Let HL be the
matrix of entries ‚àÇ2L/‚àÇxi‚àÇxj for i, j = 1, . . . , n. If there exists a vector
Œª‚àó‚ààRm such that JL(x‚àó, Œª‚àó) = 0 and
zT HL(x‚àó, Œª‚àó)z > 0
‚àÄz Ã∏= 0,
with
‚àáh(x‚àó)T z = 0,
then x‚àóis a strict local minimizer of (7.50).

7.3 Constrained Optimization
313
The last class of problems that we are going to deal with includes the case
where inequality constraints are also present, i.e.: given f : Rn ‚ÜíR,
minimize f(x), under the constraint that h(x) = 0 and g(x) ‚â§0,(7.51)
where h : Rn ‚ÜíRm, with m ‚â§n, and g : Rn ‚ÜíRr are two given functions.
It is understood that g(x) ‚â§0 means gi(x) ‚â§0 for i = 1, . . . , r. Inequality
constraints give rise to some extra formal complication with respect to the
case previously examined, but do not prevent converting the solution of
(7.51) into the minimization of a suitable Lagrangian function.
In particular, DeÔ¨Ånition 7.2 becomes
DeÔ¨Ånition 7.3 Assume that hi, gj ‚ààC1(B(x‚àó; R)) for a suitable R >
0 with i = 1, . . . , m and j = 1, . . . , r, and denote by J (x‚àó) the set of
indices j such that gj(x‚àó) = 0. A point x‚àó‚ààRn such that h(x‚àó) = 0 and
g(x‚àó) ‚â§0 is said to be regular if the column vectors of the Jacobian matrix
Jh(x‚àó) together with the vectors ‚àágj(x‚àó), j ‚ààJ (x‚àó) form a set of linearly
independent vectors.
‚ñ†
Finally, an analogue of Property 7.11 holds, provided that the following
Lagrangian function is used
M(x, Œª, ¬µ) = f(x) + ŒªT h(x) + ¬µT g(x)
instead of L and that further assumptions on the constraints are made.
For the sake of simplicity, we report in this case only the following nec-
essary condition for optimality of problem (7.51) to hold.
Property 7.12 Let x‚àóbe a regular local minimizer for (7.51) and suppose
that, for a suitable R > 0, f, hi, gj ‚ààC1(B(x‚àó; R)) with i = 1, . . . , m,
j = 1, . . . , r. Then, there exist only two vectors Œª‚àó‚ààRm and ¬µ‚àó‚ààRr,
such that JM(x‚àó, Œª‚àó, ¬µ‚àó) = 0 with ¬µ‚àó
j ‚â•0 and ¬µ‚àó
jgj(x‚àó) = 0 ‚àÄj = 1, . . . , r.
7.3.1
Kuhn-Tucker Necessary Conditions for Nonlinear
Programming
In this section we recall some results, known as Kuhn-Tucker conditions
[KT51], that ensure in general the existence of a local solution for the non-
linear programming problem. Under suitable assumptions they also guar-
antee the existence of a global solution. Throughout this section we suppose
that a minimization problem can always be reformulated as a maximization
one.

314
7. Nonlinear Systems and Numerical Optimization
Let us consider the general nonlinear programming problem:
given f : Rn ‚ÜíR,
maximize f(x), subject to
gi(x) ‚â§bi
i = 1, . . . , l,
gi(x) ‚â•bi
i = l + 1, . . . , k,
gi(x) = bi
i = k + 1, . . . , m,
x ‚â•0.
(7.52)
A vector x that satisÔ¨Åes the constraints above is called a feasible solution of
(7.52) and the set of the feasible solutions is called the feasible region. We
assume henceforth that f, gi ‚ààC1(Rn), i = 1, . . . , m, and deÔ¨Åne the sets
I= = {i : gi(x‚àó) = bi}, IÃ∏= = {i : gi(x‚àó) Ã∏= bi}, J= = {i : x‚àó
i = 0}, J> =
{i : x‚àó
i > 0}, having denoted by x‚àóa local maximizer of f. We associate
with (7.52) the following Lagrangian
L(x, Œª) = f(x) +
m

i=1
Œªi [bi ‚àígi(x)] ‚àí
m+n

i=m+1
Œªixi‚àím.
The following result can be proved.
Property 7.13 (Kuhn-Tucker conditions I and II) If f has a con-
strained local maximum at the point x = x‚àó, it is necessary that a vector
Œª‚àó‚ààRm+n exists such that (Ô¨Årst Kuhn-Tucker condition)
‚àáxL(x‚àó, Œª‚àó) ‚â§0,
where strict equality holds for every component i ‚ààJ>. Moreover (second
Kuhn-Tucker condition)
(‚àáxL(x‚àó, Œª‚àó))T x‚àó= 0.
The other two necessary Kuhn-Tucker conditions are as follows.
Property 7.14 Under the same hypothesis as in Property 7.13, the third
Kuhn-Tucker condition requires that:
‚àáŒªL(x‚àó, Œª‚àó) ‚â•0
i = 1, . . . , l,
‚àáŒªL(x‚àó, Œª‚àó) ‚â§0
i = l + 1, . . . , k,
‚àáŒªL(x‚àó, Œª‚àó) = 0
i = k + 1, . . . , m.
Moreover (fourth Kuhn-Tucker condition)
(‚àáŒªL(x‚àó, Œª‚àó))T x‚àó= 0.

7.3 Constrained Optimization
315
It is worth noticing that the Kuhn-Tucker conditions hold provided that
the vector Œª‚àóexists. To ensure this, it is necessary to introduce a further
geometric condition that is known as constraint qualiÔ¨Åcation (see [Wal75],
p. 48).
We conclude this section by the following fundamental theorem which
establishes when the Kuhn-Tucker conditions become also suÔ¨Écient for the
existence of a global maximizer for f.
Property 7.15 Assume that the function f in (7.52) is a concave func-
tion (i.e., ‚àíf is convex) in the feasible region. Suppose also that the point
(x‚àó, Œª‚àó) satisÔ¨Åes all the Kuhn-Tucker necessary conditions and that the
functions gi for which Œª‚àó
i > 0 are convex while those for which Œª‚àó
i < 0 are
concave. Then f(x‚àó) is the constrained global maximizer of f for problem
(7.52).
7.3.2
The Penalty Method
The basic idea of this method is to eliminate, partly or completely, the
constraints in order to transform the constrained problem into an uncon-
strained one. This new problem is characterized by the presence of a pa-
rameter that yields a measure of the accuracy at which the constraint is
actually imposed.
Let us consider the constrained problem (7.50), assuming we are search-
ing for the solution x‚àóonly in ‚Ñ¶‚äÇRn. Suppose that such a problem admits
at least one solution in ‚Ñ¶and write it in the following penalized form
minimize LŒ±(x)
for x ‚àà‚Ñ¶,
(7.53)
where
LŒ±(x) = f(x) + 1
2Œ±‚à•h(x)‚à•2
2.
The function LŒ± : Rn ‚ÜíR is called the penalized Lagrangian, and Œ± is
called the penalty parameter. It is clear that if the constraint was exactly
satisÔ¨Åed then minimizing f would be equivalent to minimizing LŒ±.
The penalty method is an iterative technique for solving (7.53).
For k = 0, 1, . . . , until convergence, one must solve the sequence of prob-
lems
minimize LŒ±k(x)
with x ‚àà‚Ñ¶,
(7.54)
where {Œ±k} is an increasing monotonically sequence of positive penalty
parameters, such that Œ±k ‚Üí‚àûas k ‚Üí‚àû. As a consequence, after choosing
Œ±k, at each step of the penalty process we have to solve a minimization
problem with respect to the variable x, leading to a sequence of values x‚àó
k,
solutions to (7.54). By doing so, the objective function LŒ±k(x) tends to
inÔ¨Ånity, unless h(x) is equal to zero.

316
7. Nonlinear Systems and Numerical Optimization
The minimization problems can then be solved by one of the methods
introduced in Section 7.2. The following property ensures the convergence
of the penalty method in the form (7.53).
Property 7.16 Assume that f : Rn ‚ÜíR and h : Rn ‚ÜíRm, with m ‚â§n,
are continuous functions on a closed set ‚Ñ¶‚äÇRn and suppose that the
sequence of penalty parameters Œ±k > 0 is monotonically divergent. Finally,
let x‚àó
k be the global minimizer of problem (7.54) at step k. Then, taking
the limit as k ‚Üí‚àû, the sequence x‚àó
k converges to x‚àó, which is a global
minimizer of f in ‚Ñ¶and satisÔ¨Åes the constraint h(x‚àó) = 0.
Regarding the selection of the parameters Œ±k, it can be shown that large
values of Œ±k make the minimization problem in (7.54) ill-conditioned, thus
making its solution quite prohibitive unless the initial guess is particularly
close to x‚àó. On the other hand, the sequence Œ±k must not grow too slowly,
since this would negatively aÔ¨Äect the overall convergence of the method.
A choice that is commonly made in practice is to pick up a not too large
value of Œ±0 and then set Œ±k = Œ≤Œ±k‚àí1 for k > 0, where Œ≤ is an integer
number between 4 and 10 (see [Ber82]). Finally, the starting point for the
numerical method used to solve the minimization problem (7.54) can be
set equal to the last computed iterate.
The penalty method is implemented in Program 63. This requires as
input parameters the functions f, h, an initial value alpha0 for the penalty
parameter and the number beta.
Program 63 - lagrpen : Penalty method
function [x,vinc,nit]=lagrpen(x0,alpha0,beta,f,h,toll)
x = x0; [r,c]=size(h); vinc = 0;
for i=1:r,
vinc = max(vinc,eval(h(i,1:c)));
end
norm2h=[‚Äô(‚Äô,h(1,1:c),‚Äô)ÀÜ2‚Äô];
for i=2:r,
norm2h=[norm2h,‚Äô+(‚Äô,h(i,1:c),‚Äô)ÀÜ2‚Äô];
end
alpha = alpha0; options(1)=0; options(2)=toll*0.1; nit = 0;
while vinc > toll
g=[f,‚Äô+0.5*‚Äô,num2str(alpha,16),‚Äô*‚Äô,norm2h];
[x]=fmins(g,x,options);
vinc=0; nit = nit + 1;
for i=1:r, vinc = max(vinc,eval(h(i,1:c))); end
alpha=alpha*beta;
end
Example 7.6 Let us employ the penalty method to compute the minimizer of
f(x) = 100(x2 ‚àíx2
1)2 + (1 ‚àíx1)2 under the constraint h(x) = (x1 + 0.5)2 +
(x2 + 0.5)2 ‚àí0.25 = 0. The crosses in Figure 7.3 denote the sequence of iterates
computed by Program 63 starting from x(0) = (1, 1)T and choosing Œ±0 = 0.1, Œ≤ =
6. The method converges in 12 iterations to the value x = (‚àí0.2463, ‚àí0.0691)T ,
satisfying the constraint up to a tolerance of 10‚àí4.
‚Ä¢

7.3 Constrained Optimization
317
‚àí2
‚àí1.5
‚àí1
‚àí0.5
0
0.5
1
1.5
2
‚àí2
‚àí1.5
‚àí1
‚àí0.5
0
0.5
1
1.5
2
FIGURE 7.3. Convergence history of the penalty method in Example 7.6
7.3.3
The Method of Lagrange Multipliers
A variant of the penalty method makes use of (instead of LŒ±(x) in (7.53))
the augmented Lagrangian function GŒ± : Rm √ó Rn ‚ÜíR given by
GŒ±(x, Œª) = f(x) + ŒªT h(x) + 1
2Œ±‚à•h(x)‚à•2
2,
(7.55)
where Œª ‚ààRm is a Lagrange multiplier. Clearly, if x‚àóis a solution to prob-
lem (7.50), then it will also be a solution to (7.55), but with the advantage,
with respect to (7.53), of disposing of the further degree of freedom Œª. The
penalty method applied to (7.55) reads: for k = 0, 1, . . . , until convergence,
solve the sequence of problems
minimize GŒ±k(x, Œªk)
for x ‚àà‚Ñ¶,
(7.56)
where {Œªk} is a bounded sequence of unknown vectors in Rn, and the
parameters Œ±k are deÔ¨Åned as above (notice that if Œªk were zero, then we
would recover method (7.54)).
Property 7.16 also holds for method (7.56), provided that the multipliers
are assumed to be bounded. Notice that the existence of the minimizer
of (7.56) is not guaranteed, even in the case where f has a unique global
minimizer (see Example 7.7). This circumstance can be overcome by adding
further non quadratic terms to the augmented Lagrangian function (e.g.,
of the form ‚à•h‚à•p
2, with p large).

318
7. Nonlinear Systems and Numerical Optimization
Example 7.7 Let us Ô¨Ånd the minimizer of f(x) = ‚àíx4 under the constraint
x = 0. Such problem clearly admits the solution x‚àó= 0. If, instead, one considers
the augmented Lagrangian function
LŒ±k(x, Œªk) = ‚àíx4 + Œªkx + 1
2Œ±kx2,
one Ô¨Ånds that it no longer admits a minimum at x = 0, though vanishing there,
for any Œ±k diÔ¨Äerent from zero.
‚Ä¢
As far as the choice of the multipliers is concerned, the sequence of vectors
Œªk is typically assigned by the following formula
Œªk+1 = Œªk + Œ±kh(x(k)),
where Œª0 is a given value while the sequence of Œ±k can be set a priori or
modiÔ¨Åed during run-time.
As for the convergence properties of the method of Lagrange multipliers,
the following local result holds.
Property 7.17 Assume that x‚àóis a regular strict local minimizer of (7.50)
and that:
1. f, hi ‚ààC2(B(x‚àó; R)) with i = 1, . . . , m and for a suitable R > 0;
2. the pair (x‚àó, Œª‚àó) satisÔ¨Åes zT HG0(x‚àó, Œª‚àó)z > 0, ‚àÄz Ã∏= 0 such that
Jh(x‚àó)T z = 0;
3. ‚àÉ¬ØŒ± > 0 such that HG ¬Ø
Œ±(x‚àó, Œª‚àó) > 0.
Then, there exist three positive scalars Œ¥, Œ≥ and M such that, for any pair
(Œª, Œ±) ‚ààV =

(Œª, Œ±) ‚ààRm+1 : ‚à•Œª ‚àíŒª‚àó‚à•2 < Œ¥Œ±, Œ± ‚â•¬ØŒ±

, the problem
minimize GŒ±(x, Œª),
with x ‚ààB(x‚àó; Œ≥),
admits a unique solution x(Œª, Œ±), diÔ¨Äerentiable with respect to its argu-
ments. Moreover, ‚àÄ(Œª, Œ±) ‚ààV
‚à•x(Œª, Œ±) ‚àíx‚àó‚à•2 ‚â§M‚à•Œª ‚àíŒª‚àó‚à•2.
Under further assumptions (see [Ber82], Proposition 2.7), it can be proved
that the Lagrange multipliers method converges. Moreover, if Œ±k ‚Üí‚àû, as
k ‚Üí‚àû, then
lim
k‚Üí‚àû
‚à•Œªk+1 ‚àíŒª‚àó‚à•2
‚à•Œªk ‚àíŒª‚àó‚à•2
= 0.
and the convergence of the method is more than linear. In the case where
the sequence Œ±k has an upper bound, the method converges linearly.

7.4 Applications
319
Finally, we notice that, unlike the penalty method, it is no longer nec-
essary that the sequence of Œ±k tends to inÔ¨Ånity. This, in turn, limits the
ill-conditioning of problem (7.56) as Œ±k is growing. Another advantage con-
cerns the convergence rate of the method, which turns out to be indepen-
dent of the growth rate of the penalty parameter, in the case of the Lagrange
multipliers technique. This of course implies a considerable reduction of the
computational cost.
The method of Lagrange multipliers is implemented in Program 64. Com-
pared with Program 63, this further requires in input the initial value
lambda0 of the multiplier.
Program 64 - lagrmult : Method of Lagrange multipliers
function [x,vinc,nit]=lagrmult(x0,lambda0,alpha0,beta,f,h,toll)
x = x0; [r,c]=size(h); vinc = 0; lambda = lambda0;
for i=1:r,
vinc = max(vinc,eval(h(i,1:c)));
end
norm2h=[‚Äô(‚Äô,h(1,1:c),‚Äô)ÀÜ2‚Äô];
for i=2:r,
norm2h=[norm2h,‚Äô+(‚Äô,h(i,1:c),‚Äô)ÀÜ2‚Äô];
end
alpha = alpha0; options(1)=0; options(2)=toll*0.1; nit = 0;
while vinc > toll
lh=[‚Äô(‚Äô,h(1,1:c),‚Äô)*‚Äô,num2str(lambda(1))];
for i=2:r, lh=[lh,‚Äô+(‚Äô,h(i,1:c),‚Äô)*‚Äô,num2str(lambda(i))];
end
g=[f,‚Äô+0.5*‚Äô,num2str(alpha,16),‚Äô*‚Äô,norm2h,‚Äô+‚Äô,lh];
[x]=fmins(g,x,options);
vinc=0; nit = nit + 1;
for i=1:r, vinc = max(vinc,eval(h(i,1:c))); end
alpha=alpha*beta;
for i=1:r, lambda(i)=lambda(i)+alpha*eval(h(i,1:c)); end
end
Example 7.8 We use the method of Lagrange multipliers to solve the prob-
lem presented in Example 7.6. Set Œª = 10 and leave the remaining parameters
unchanged. The method converges in 6 iterations and the crosses in Figure 7.4
show the iterates computed by Program 64. The constraint is here satisÔ¨Åed up
to machine precision.
‚Ä¢
7.4
Applications
The two applications of this section are concerned with nonlinear systems
arising in the simulation of the electric potential in a semiconductor device
and in the triangulation of a two-dimensional polygon.

320
7. Nonlinear Systems and Numerical Optimization
‚àí1
‚àí0.5
0
0.5
1
‚àí1
‚àí0.5
0
0.5
1
FIGURE 7.4. Convergence history for the method of Lagrange multipliers in
Example 7.8
7.4.1
Solution of a Nonlinear System Arising from
Semiconductor Device Simulation
Let us consider the nonlinear system in the unknown u ‚ààRn
F(u) = Au + œÜ(u) ‚àíb = 0,
(7.57)
where A = (Œª/h)2tridiagn(‚àí1, 2‚àí1), for h = 1/(n+1), œÜi(u) = 2K sinh(ui)
for i = 1, . . . , n, where Œª and K are two positive constants and b ‚ààRn is
a given vector. Problem (7.57) arises in the numerical simulation of semi-
conductor devices in microelectronics, where u and b represent electric
potential and doping proÔ¨Åle, respectively.
In Figure 7.5 (left) we show schematically the particular device consid-
ered in the numerical example, a p ‚àín junction diode of unit normalized
length, subject to an external bias ‚ñ≥V = Vb ‚àíVa, together with the doping
proÔ¨Åle of the device, normalized to 1 (right). Notice that bi = b(xi), for
i = 1, . . . , n, where xi = ih. The mathematical model of the problem at
hand comprises a nonlinear Poisson equation for the electric potential and
two continuity equations of advection-diÔ¨Äusion type, as those addressed in
Chapter 12, for the current densities. For the complete derivation of the
model and its analysis see, for instance, [Mar86] and [Jer96].
Solving system (7.57) corresponds to Ô¨Ånding the minimizer in Rn of the
function f : Rn ‚ÜíR deÔ¨Åned as
f(u) = 1
2uT Au + 2
n

i=1
cosh(ui)) ‚àíbT u.
(7.58)

7.4 Applications
321
+
p
n
‚àÜV
‚àí
0
L
x
b(x)
‚àí1
1
FIGURE 7.5. Scheme of a semiconductor device (left); doping proÔ¨Åle (right)
It can be checked (see Exercise 5) that for any u, v ‚ààRn with u Ã∏= v and
for any Œª ‚àà(0, 1)
Œªf(u) + (1 ‚àíŒª)f(v) ‚àíf(Œªu + (1 ‚àíŒª)v) > (1/2)Œª(1 ‚àíŒª)‚à•u ‚àív‚à•2
A,
where ‚à•¬∑ ‚à•A denotes the energy norm introduced in (1.28). This implies
that f(u) is an uniformly convex function in Rn, that is, it strictly satisÔ¨Åes
(7.49) with œÅ = 1/2.
Property 7.10 ensures, in turn, that the function in (7.58) admits a unique
minimizer u‚àó‚ààRn and it can be shown (see Theorem 14.4.3, p. 503 [OR70])
that there exists a sequence {Œ±k} such that the iterates of the damped
Newton method introduced in Section 7.2.6 converge to u‚àó‚ààRn (at least)
superlinearly.
Thus, using the damped Newton method for solving system (7.57) leads to
the following sequence of linearized problems:
given u(0) ‚ààRn, ‚àÄk ‚â•0 solve
6
A + 2K diagn(cosh(u(k)
i
))
7
Œ¥u(k) = b ‚àí
+
Au(k) + œÜ(u(k))
,
,
(7.59)
then set u(k+1) = u(k) + Œ±kŒ¥u(k).
Let us now address two possible choices of the acceleration parameters
Œ±k. The Ô¨Årst one has been proposed in [BR81] and is
Œ±k =
1
1 + œÅk ‚à•F(u(k))‚à•,
k = 0, 1, . . . ,
(7.60)
where ‚à•¬∑ ‚à•denotes a vector norm, for instance ‚à•¬∑ ‚à•= ‚à•¬∑ ‚à•‚àû, and the
coeÔ¨Écients œÅk ‚â•0 are suitable acceleration parameters picked in such a
way that the descent condition ‚à•F(u(k) + Œ±kŒ¥u(k))‚à•‚àû< ‚à•F(u(k))‚à•‚àûis
satisÔ¨Åed (see [BR81] for the implementation details of the algorithm).

322
7. Nonlinear Systems and Numerical Optimization
We notice that, as ‚à•F(u(k))‚à•‚àû‚Üí0, (7.60) yields Œ±k ‚Üí1, thus recov-
ering the full (quadratic) convergence of Newton‚Äôs method. Otherwise, as
typically happens in the Ô¨Årst iterations, ‚à•F(u(k))‚à•‚àû‚â´1 and Œ±k is quite
close to zero, with a strong reduction of the Newton variation (damping).
As an alternative to (7.60), the sequence {Œ±k} can be generated using the
simpler formula, suggested in [Sel84], Chapter 7
Œ±k = 2‚àíi(i‚àí1)/2,
k = 0, 1, . . . ,
(7.61)
where i is the Ô¨Årst integer in the interval [1, Itmax] such that the descent
condition above is satisÔ¨Åed, Itmax being the maximum admissible number
of damping cycles for any Newton‚Äôs iteration (Ô¨Åxed equal to 10 in the
numerical experiments).
As a comparison, both damped and standard Newton‚Äôs methods have been
implemented, the former one with both choices (7.60) and (7.61) for the
coeÔ¨Écients Œ±k. In the case of Newton‚Äôs method, we have set in (7.59) Œ±k = 1
for any k ‚â•0.
The numerical examples have been performed with n = 49, bi = ‚àí1 for
i ‚â§n/2 and the remaining values bi equal to 1. Moreover, we have taken
Œª2 = 1.67 ¬∑ 10‚àí4, K = 6.77 ¬∑ 10‚àí6 and Ô¨Åxed the Ô¨Årst n/2 components of the
initial vector u(0) equal to Va and the remaining ones equal to Vb, where
Va = 0 and Vb = 10.
The tolerance on the maximum change between two successive iterates,
which monitors the convergence of damped Newton‚Äôs method (7.59), has
been set equal to 10‚àí4.
10
0
10
1
10
2
10
‚àí6
10
‚àí4
10
‚àí2
10
0
10
2
10
4
(1)
(2)
(3)
0
2
4
6
8
10
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
FIGURE 7.6. Absolute error (left) and damping parameters Œ±k (right). The error
curve for standard Newton‚Äôs method is denoted by (1), while (2) and (3) refer to
damped Newton‚Äôs method with the choices (7.61) and (7.60) for the coeÔ¨Écients
Œ±k, respectively
Figure 7.6 (left) shows the log-scale absolute error for the three algorithms
as functions of the iteration number. Notice the rapid convergence of the

7.4 Applications
323
damped Newton‚Äôs method (8 and 10 iterations in the case of (7.60) and
(7.61), respectively), compared with the extremely slow convergence of the
standard Newton‚Äôs method (192 iterations). Moreover, it is interesting to
analyze in Figure 7.6 (right) the plot of the sequences of parameters Œ±k as
functions of the iteration number.
The starred and the circled curves refer to the choices (7.60) and (7.61)
for the coeÔ¨Écients Œ±k, respectively. As previously observed, the Œ±k‚Äôs start
from very small values, to converge quickly to 1 as the damped Newton
method (7.59) enters the attraction region of the minimizer x‚àó.
7.4.2
Nonlinear Regularization of a Discretization Grid
In this section we go back to the problem of regularizing a discretization
grid that has been introduced in Section 3.14.2. There, we considered the
technique of barycentric regularization, which leads to solving a linear sys-
tem, typically of large size and featuring a sparse coeÔ¨Écient matrix.
In this section we address two alternative techniques, denoted as reg-
ularization by edges and by areas. The main diÔ¨Äerence with respect to
the method described in Section 3.14.2 lies in the fact that these new ap-
proaches lead to systems of nonlinear equations.
Using the notation of Section 3.14.2, for each pair of nodes xj, xk ‚ààZi,
denote by ljk the edge on the boundary ‚àÇPi of Pi which connects them
and by xjk the midpoint of ljk, while for each triangle T ‚ààPi we denote
by xb,T the centroid of T. Moreover, let ni = dim(Zi) and denote for any
geometric entity (side or triangle) by | ¬∑ | its measure in R1 or R2.
In the case of regularization by edges, we let
xi =
Ô£´
Ô£≠
ljk‚àà‚àÇPi
xjk|ljk|
Ô£∂
Ô£∏/|‚àÇPi|,
‚àÄxi ‚ààNh,
(7.62)
while in the case of regularization by areas, we let
xi =
 
T ‚ààPi
xb,T |T|

/|Pi|,
‚àÄxi ‚ààNh.
(7.63)
In both the regularization procedures we assume that xi = x(‚àÇD)
i
if xi ‚àà
‚àÇD, that is, the nodes lying on the boundary of the domain D are Ô¨Åxed. Let-
ting n = N‚àíNb be the number of internal nodes, relation (7.62) amounts to
solving the following two systems of nonlinear equations for the coordinates

324
7. Nonlinear Systems and Numerical Optimization
{xi} and {yi} of the internal nodes, with i = 1, . . . , n
xi ‚àí1
2
Ô£´
Ô£≠
ljk‚àà‚àÇPi
(xj + xk)|ljk|
Ô£∂
Ô£∏/

ljk‚àà‚àÇPi
|ljk| = 0,
yi ‚àí1
2
Ô£´
Ô£≠
ljk‚àà‚àÇPi
(yj + yk)|ljk|
Ô£∂
Ô£∏/

ljk‚àà‚àÇPi
|ljk| = 0.
(7.64)
Similarly, (7.63) leads to the following nonlinear systems, for i = 1, . . . , n
xi ‚àí1
3
 
T ‚ààPi
(x1,T + x2,T + x3,T )|T|

/

T ‚ààPi
|T| = 0,
yi ‚àí1
3
 
T ‚ààPi
(y1,T + y2,T + y3,T )|T|

/

T ‚ààPi
|T| = 0,
(7.65)
where xs,T = (xs,T , ys,T ), for s = 1, 2, 3, are the coordinates of the vertices
of each triangle T ‚ààPi. Notice that the nonlinearity of systems (7.64) and
(7.65) is due to the presence of terms |ljk| and |T|.
Both systems (7.64) and (7.65) can be cast in the form (7.1), denoting,
as usual, by fi the i-th nonlinear equation of the system, for i = 1, . . . , n.
The complex functional dependence of fi on the unknowns makes it pro-
hibitive to use Newton‚Äôs method (7.4), which would require the explicit
computation of the Jacobian matrix JF.
A convenient alternative is provided by the nonlinear Gauss-Seidel method
(see [OR70], Chapter 7), which generalizes the corresponding method pro-
posed in Chapter 4 for linear systems and can be formulated as follows.
Denote by zi, for i = 1, . . . , n, either of the unknown xi or yi. Given the
initial vector z(0) = (z(0)
1 , . . . , z(0)
n )T , for k = 0, 1, . . . until convergence,
solve
fi(z(k+1)
1
, . . . , z(k+1)
i‚àí1
, Œæ, z(k)
i+1, . . . , z(k)
n ) = 0,
i = 1, . . . , n,
(7.66)
then, set z(k+1)
i
= Œæ. Thus, the nonlinear Gauss-Seidel method converts
problem (7.1) into the successive solution of n scalar nonlinear equations.
In the case of system (7.64), each of these equations is linear in the unknown
z(k+1)
i
(since Œæ does not explicitly appear in the bracketed term at the right
side of (7.64)). This allows for its exact solution in one step.
In the case of system (7.65), the equation (7.66) is genuinely nonlinear
with respect to Œæ, and is solved taking one step of a Ô¨Åxed-point iteration.
The nonlinear Gauss-Seidel (7.66) has been implemented in MATLAB
to solve systems (7.64) and (7.65) in the case of the initial triangulation
shown in Figure 7.7 (left). Such a triangulation covers the external region
of a two dimensional wing section of type NACA 2316. The grid contains
NT = 534 triangles and n = 198 internal nodes.

7.5 Exercises
325
The algorithm reached convergence in 42 iterations for both kinds of reg-
ularization, having used as stopping criterion the test ‚à•z(k+1) ‚àíz(k)‚à•‚àû‚â§
10‚àí4. In Figure 7.7 (right) the discretization grid obtained after the reg-
ularization by areas is shown (a similar result has been provided by the
regularization by edges). Notice the higher uniformity of the triangles with
respect to those of the starting grid.
FIGURE 7.7. Triangulation before (left) and after (right) the regularization
7.5
Exercises
1. Prove (7.8) for the m-step Newton-SOR method.
[Hint: use the SOR method for solving a linear system Ax=b with A=D-
E-F and express the k-th iterate as a function of the initial datum x(0),
obtaining
x(k+1) = x(0) + (Mk+1 ‚àíI)x(0) + (Mk + . . . + I)B‚àí1b,
where B= œâ‚àí1(D ‚àíœâE) and M = B‚àí1œâ‚àí1 [(1 ‚àíœâ)D + œâF]. Since B‚àí1A =
I ‚àíM and
(I + . . . + Mk)(I ‚àíM) = I ‚àíMk+1
then (7.8) follows by suitably identifying the matrix and the right-side of
the system.]
2. Prove that using the gradient method for minimizing f(x) = x2 with the
directions p(k) = ‚àí1 and the parameters Œ±k = 2‚àík+1, does not yield the
minimizer of f.
3. Show that for the steepest descent method applied to minimizing a quadratic
functional f of the form (7.35) the following inequality holds
f(x(k+1)) ‚â§
 Œªmax ‚àíŒªmin
Œªmax + Œªmin
2
f(x(k)),

326
7. Nonlinear Systems and Numerical Optimization
where Œªmax, Œªmin are the eigenvalues of maximum and minimum module,
respectively, of the matrix A that appears in (7.35).
[Hint: proceed as done for (7.38).]
4. Check that the parameters Œ±k of Exercise 2 do not fulÔ¨Åll the conditions
(7.31) and (7.32).
5. Consider the function f : Rn ‚ÜíR introduced in (7.58) and check that it is
uniformly convex on Rn, that is
Œªf(u) + (1 ‚àíŒª)f(v) ‚àíf(Œªu + (1 ‚àíŒª)v) > (1/2)Œª(1 ‚àíŒª)‚à•u ‚àív‚à•2
A
for any u, v ‚ààRn with u Ã∏= v and 0 < Œª < 1.
[Hint: notice that cosh(¬∑) is a convex function.]
6. To solve the nonlinear system
Ô£±
Ô£¥
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£¥
Ô£≥
‚àí1
81 cos x1 + 1
9x2
2 + 1
3 sin x3 = x1
1
3 sin x1 + 1
3 cos x3 = x2
‚àí1
9 cos x1 + 1
3x2 + 1
6 sin x3 = x3,
use the Ô¨Åxed-point iteration x(n+1) = Œ®(x(n)), where x = (x1, x2, x3)T and
Œ®(x) is the left-hand side of the system. Analyze the convergence of the
iteration to compute the Ô¨Åxed point Œ± = (0, 1/3, 0)T .
[Solution: the Ô¨Åxed-point method is convergent since ‚à•Œ®(Œ±)‚à•‚àû= 1/2.]
7. Using Program 50 implementing Newton‚Äôs method, determine the global
maximizer of the function
f(x) = e‚àíx2
2 ‚àí1
4 cos(2x)
and analyze the performance of the method (input data: xv=1; toll=1e-6;
nmax=500). Solve the same problem using the following Ô¨Åxed-point iteration
x(k+1) = g(xk)
with
g(x) = sin(2x)
Ô£Æ
Ô£∞e
x2
2 (x sin(2x) + 2 cos(2x)) ‚àí2
2 (x sin(2x) + 2 cos(2x))
Ô£π
Ô£ª.
Analyze the performance of this second scheme, both theoretically and
experimentally, and compare the results obtained using the two methods.
[Solution: the function f has a global maximum at x = 0. This point is
a double zero for f ‚Ä≤. Thus, Newton‚Äôs method is only linearly convergent.
Conversely, the proposed Ô¨Åxed-point method is third-order convergent.]

8
Polynomial Interpolation
This chapter is addressed to the approximation of a function which is known
through its nodal values.
Precisely, given m+1 pairs (xi, yi), the problem consists of Ô¨Ånding a func-
tion Œ¶ = Œ¶(x) such that Œ¶(xi) = yi for i = 0, . . . , m, yi being some given
values, and say that Œ¶ interpolates {yi} at the nodes {xi}. We speak about
polynomial interpolation if Œ¶ is an algebraic polynomial, trigonometric ap-
proximation if Œ¶ is a trigonometric polynomial or piecewise polynomial
interpolation (or spline interpolation) if Œ¶ is only locally a polynomial.
The numbers yi may represent the values attained at the nodes xi by a
function f that is known in closed form, as well as experimental data. In the
former case, the approximation process aims at replacing f with a simpler
function to deal with, in particular in view of its numerical integration
or derivation. In the latter case, the primary goal of approximation is to
provide a compact representation of the available data, whose number is
often quite large.
Polynomial interpolation is addressed in Sections 8.1 and 8.2, while piece-
wise polynomial interpolation is introduced in Sections 8.3, 8.4 and 8.5. Fi-
nally, univariate and parametric splines are addressed in Sections 8.6 and
8.7. Interpolation processes based on trigonometric or algebraic orthogonal
polynomials will be considered in Chapter 10.

328
8. Polynomial Interpolation
8.1
Polynomial Interpolation
Let us consider n + 1 pairs (xi, yi). The problem is to Ô¨Ånd a polynomial
Œ†m ‚ààPm, called an interpolating polynomial, such that
Œ†m(xi) = amxm
i + . . . + a1xi + a0 = yi
i = 0, . . . , n.
(8.1)
The points xi are called interpolation nodes. If n Ã∏= m the problem is over
or under-determined and will be addressed in Section 10.7.1. If n = m, the
following result holds.
Theorem 8.1 Given n+1 distinct points x0, . . . , xn and n+1 correspond-
ing values y0, . . . , yn, there exists a unique polynomial Œ†n ‚ààPn such that
Œ†n(xi) = yi for i = 0, . . . , n.
Proof. To prove existence, let us use a constructive approach, providing an
expression for Œ†n. Denoting by {li}n
i=0 a basis for Pn, then Œ†n admits a repre-
sentation on such a basis of the form Œ†n(x) = n
i=0 bili(x) with the property
that
Œ†n(xi) =
n

j=0
bjlj(xi) = yi,
i = 0, . . . , n.
(8.2)
If we deÔ¨Åne
li ‚ààPn :
li(x) =
n

j=0
jÃ∏=i
x ‚àíxj
xi ‚àíxj
i = 0, . . . , n,
(8.3)
then li(xj) = Œ¥ij and we immediately get from (8.2) that bi = yi.
The polynomials {li, i = 0, . . . , n} form a basis for Pn (see Exercise 1). As a con-
sequence, the interpolating polynomial exists and has the following form (called
Lagrange form)
Œ†n(x) =
n

i=0
yili(x).
(8.4)
To prove uniqueness, suppose that another interpolating polynomial Œ®m of de-
gree m ‚â§n exists, such that Œ®m(xi) = yi for i = 0, ..., n. Then, the diÔ¨Äerence
polynomial Œ†n ‚àíŒ®m vanishes at n + 1 distinct points xi and thus coincides with
the null polynomial. Therefore, Œ®m = Œ†n.
An alternative approach to prove existence and uniqueness of Œ†n is provided
in Exercise 2.
3
It can be checked that (see Exercise 3)
Œ†n(x) =
n

i=0
œân+1(x)
(x ‚àíxi)œâ‚Ä≤
n+1(xi)yi
(8.5)

8.1 Polynomial Interpolation
329
where œân+1 is the nodal polynomial of degree n + 1 deÔ¨Åned as
œân+1(x) =
n

i=0
(x ‚àíxi).
(8.6)
Formula (8.4) is called the Lagrange form of the interpolating polynomial,
while the polynomials li(x) are the characteristic polynomials. In Figure
8.1 we show the characteristic polynomials l2(x), l3(x) and l4(x), in the
case of degree n = 6, on the interval [-1,1] where equally spaced nodes are
taken, including the end points.
‚àí1
‚àí0.5
0
0.5
1
‚àí1.5
‚àí1
‚àí0.5
0
0.5
1
1.5
l
l
l
2
3
4
FIGURE 8.1. Lagrange characteristic polynomials
Notice that |li(x)| can be greater than 1 within the interpolation interval.
If yi = f(xi) for i = 0, . . . , n, f being a given function, the interpolating
polynomial Œ†n(x) will be denoted by Œ†nf(x).
8.1.1
The Interpolation Error
In this section we estimate the interpolation error that is made when re-
placing a given function f with its interpolating polynomial Œ†nf at the
nodes x0, x1, . . . , xn (for further results, we refer the reader to [Wen66],
[Dav63]).
Theorem 8.2 Let x0, x1, . . . , xn be n+1 distinct nodes and let x be a point
belonging to the domain of a given function f. Assume that f ‚ààCn+1(Ix),
where Ix is the smallest interval containing the nodes x0, x1, . . . , xn and x.
Then the interpolation error at the point x is given by
En(x) = f(x) ‚àíŒ†nf(x) = f (n+1)(Œæ)
(n + 1)! œân+1(x),
(8.7)
where Œæ ‚ààIx and œân+1 is the nodal polynomial of degree n + 1.

330
8. Polynomial Interpolation
Proof. The result is obviously true if x coincides with any of the interpola-
tion nodes. Otherwise, deÔ¨Åne, for any t ‚ààIx, the function G(t) = En(t) ‚àí
œân+1(t)En(x)/œân+1(x). Since f ‚ààC(n+1)(Ix) and œân+1 is a polynomial, then
G ‚ààC(n+1)(Ix) and it has n + 2 distinct zeros in Ix, since
G(xi) = En(xi) ‚àíœân+1(xi)En(x)/œân+1(x) = 0,
i = 0, . . . , n
G(x) = En(x) ‚àíœân+1(x)En(x)/œân+1(x) = 0.
Then, thanks to the mean value theorem, G‚Ä≤ has n + 1 distinct zeros and, by
recursion, G(j) admits n + 2 ‚àíj distinct zeros. As a consequence, G(n+1) has a
unique zero, which we denote by Œæ. On the other hand, since E(n+1)
n
(t) = f (n+1)(t)
and œâ(n+1)
n+1 (x) = (n + 1)! we get
G(n+1)(t) = f (n+1)(t) ‚àí(n + 1)!
œân+1(x)En(x),
which, evaluated at t = Œæ, gives the desired expression for En(x).
3
8.1.2
Drawbacks of Polynomial Interpolation on Equally
Spaced Nodes and Runge‚Äôs Counterexample
In this section we analyze the behavior of the interpolation error (8.7) as
n tends to inÔ¨Ånity. For this purpose, for any function f ‚ààC0([a, b]), deÔ¨Åne
its maximum norm
‚à•f‚à•‚àû= max
x‚àà[a,b]|f(x)|.
(8.8)
Then, let us introduce a lower triangular matrix X of inÔ¨Ånite size, called the
interpolation matrix on [a, b], whose entries xij, for i, j = 0, 1, . . . , represent
points of [a, b], with the assumption that on each row the entries are all
distinct.
Thus, for any n ‚â•0, the n + 1-th row of X contains n + 1 distinct
values that we can identify as nodes, so that, for a given function f, we
can uniquely deÔ¨Åne an interpolating polynomial Œ†nf of degree n at those
nodes (any polynomial Œ†nf depends on X, as well as on f).
Having Ô¨Åxed f and an interpolation matrix X, let us deÔ¨Åne the interpo-
lation error
En,‚àû(X) = ‚à•f ‚àíŒ†nf‚à•‚àû,
n = 0, 1, . . .
(8.9)
Next, denote by p‚àó
n ‚ààPn the best approximation polynomial, for which
E‚àó
n = ‚à•f ‚àíp‚àó
n‚à•‚àû‚â§‚à•f ‚àíqn‚à•‚àû
‚àÄqn ‚ààPn.
The following comparison result holds (for the proof, see [Riv74]).

8.1 Polynomial Interpolation
331
Property 8.1 Let f ‚ààC0([a, b]) and X be an interpolation matrix on [a, b].
Then
En,‚àû(X) ‚â§E‚àó
n (1 + Œõn(X)) ,
n = 0, 1, . . .
(8.10)
where Œõn(X) denotes the Lebesgue constant of X, deÔ¨Åned as
Œõn(X) =
!!!!!!
n

j=0
|l(n)
j
|
!!!!!!
‚àû
,
(8.11)
and where l(n)
j
‚ààPn is the j-th characteristic polynomial associated with
the n + 1-th row of X, that is, satisfying l(n)
j
(xnk) = Œ¥jk, j, k = 0, 1, . . .
Since E‚àó
n does not depend on X, all the information concerning the eÔ¨Äects
of X on En,‚àû(X) must be looked for in Œõn(X). Although there exists an
interpolation matrix X‚àósuch that Œõn(X) is minimized, it is not in general a
simple task to determine its entries explicitly. We shall see in Section 10.3,
that the zeros of the Chebyshev polynomials provide on the interval [‚àí1, 1]
an interpolation matrix with a very small value of the Lebesgue constant.
On the other hand, for any possible choice of X, there exists a constant
C > 0 such that (see [Erd61])
Œõn(X) > 2
œÄ log(n + 1) ‚àíC,
n = 0, 1, . . .
This property shows that Œõn(X) ‚Üí‚àûas n ‚Üí‚àû. This fact has important
consequences: in particular, it can be proved (see [Fab14]) that, given an
interpolation matrix X on an interval [a, b], there always exists a continuous
function f in [a, b], such that Œ†nf does not converge uniformly (that is, in
the maximum norm) to f. Thus, polynomial interpolation does not allow for
approximating any continuous function, as demonstrated by the following
example.
Example 8.1 (Runge‚Äôs counterexample) Suppose we approximate the fol-
lowing function
f(x) =
1
1 + x2 ,
‚àí5 ‚â§x ‚â§5
(8.12)
using Lagrange interpolation on equally spaced nodes. It can be checked that
some points x exist within the interpolation interval such that
lim
n‚Üí‚àû|f(x) ‚àíŒ†nf(x)| Ã∏= 0.
In particular, Lagrange interpolation diverges for |x| > 3.63 . . . . This phenomenon
is particularly evident in the neighborhood of the end points of the interpolation
interval, as shown in Figure 8.2, and is due to the choice of equally spaced nodes.
We shall see in Chapter 10 that resorting to suitably chosen nodes will allow for
uniform convergence of the interpolating polynomial to the function f to hold. ‚Ä¢

332
8. Polynomial Interpolation
‚àí5
‚àí4
‚àí3
‚àí2
‚àí1
0
1
2
3
4
5
‚àí0.5
0
0.5
1
1.5
2
FIGURE 8.2. Lagrange interpolation on equally spaced nodes for the function
f(x) = 1/(1 + x2): the interpolating polynomials Œ†5f and Œ†10f are shown in
dotted and dashed line, respectively
8.1.3
Stability of Polynomial Interpolation
Let us consider a set of function values
2
$f(xi)
3
which is a perturbation
of the data f(xi) relative to the nodes xi, with i = 0, . . . , n, in an interval
[a, b]. The perturbation may be due, for instance, to the eÔ¨Äect of rounding
errors, or may be caused by an error in the experimental measure of the
data.
Denoting by Œ†n $f the interpolating polynomial on the set of values $f(xi),
we have
‚à•Œ†nf ‚àíŒ†n $f‚à•‚àû
= max
a‚â§x‚â§b

n

j=0
(f(xj) ‚àí$f(xj))lj(x)

‚â§Œõn(X) max
i=0,...,n|f(xi) ‚àí$f(xi)|.
As a consequence, small changes on the data give rise to small changes
on the interpolating polynomial only if the Lebesgue constant is small.
This constant plays the role of the condition number for the interpolation
problem.
As previously noticed, Œõn grows as n ‚Üí‚àûand in particular, in the case
of Lagrange interpolation on equally spaced nodes, it can be proved that
(see [Nat65])
Œõn(X) ‚âÉ
2n+1
en log n
where e ‚âÉ2.7183 is the naeperian number. This shows that, for n large,
this form of interpolation can become unstable. Notice also that so far we
have completely neglected the errors generated by the interpolation process
in constructing Œ†nf. However, it can be shown that the eÔ¨Äect of such errors
is generally negligible (see [Atk89]).

8.2 Newton Form of the Interpolating Polynomial
333
‚àí1
‚àí0.5
0
0.5
1
‚àí1.5
‚àí1
‚àí0.5
0
0.5
1
1.5
2
2.5
FIGURE 8.3. Instability of Lagrange interpolation. In solid line Œ†21f, on unper-
turbed data, in dashed line Œ†21 $f, on perturbed data, for Example 8.2
Example 8.2 On the interval [‚àí1, 1] let us interpolate the function f(x) =
sin(2œÄx) at 22 equally spaced nodes xi. Next, we generate a perturbed set of val-
ues $f(xi) of the function evaluations f(xi) = sin(2œÄxi) with maxi=0,...,21 |f(xi)‚àí
$f(xi)| ‚âÉ9.5 ¬∑ 10‚àí4. In Figure 8.3 we compare the polynomials Œ†21f and Œ†21 $f:
notice how the diÔ¨Äerence between the two interpolating polynomials, around the
end points of the interpolation interval, is quite larger than the impressed per-
turbation (actually, ‚à•Œ†21f ‚àíŒ†21 $f‚à•‚àû‚âÉ2.1635 and Œõ21 ‚âÉ24000).
‚Ä¢
8.2
Newton Form of the Interpolating Polynomial
The Lagrange form (8.4) of the interpolating polynomial is not the most
convenient from a practical standpoint. In this section we introduce an
alternative form characterized by a cheaper computational cost. Our goal
is the following:
given n + 1 pairs {xi, yi}, i = 0, . . . , n, we want to represent Œ†n (with
Œ†n(xi) = yi for i = 0, . . . , n) as the sum of Œ†n‚àí1 (with Œ†n‚àí1(xi) = yi for
i = 0, . . . , n‚àí1) and a polynomial of degree n which depends on the nodes
xi and on only one unknown coeÔ¨Écient. We thus set
Œ†n(x) = Œ†n‚àí1(x) + qn(x),
(8.13)
where qn ‚ààPn. Since qn(xi) = Œ†n(xi) ‚àíŒ†n‚àí1(xi) = 0 for i = 0, . . . , n ‚àí1,
it must necessarily be that
qn(x) = an(x ‚àíx0) . . . (x ‚àíxn‚àí1) = anœân(x).

334
8. Polynomial Interpolation
To determine the unknown coeÔ¨Écient an, suppose that yi = f(xi), i =
0, . . . , n, where f is a suitable function, not necessarily known in explicit
form. Since Œ†nf(xn) = f(xn), from (8.13) it follows that
an = f(xn) ‚àíŒ†n‚àí1f(xn)
œân(xn)
.
(8.14)
The coeÔ¨Écient an is called n-th the Newton divided diÔ¨Äerence and is gen-
erally denoted by
an = f[x0, x1, . . . , xn]
(8.15)
for n ‚â•1. As a consequence, (8.13) becomes
Œ†nf(x) = Œ†n‚àí1f(x) + œân(x)f[x0, x1, . . . , xn].
(8.16)
If we let y0 = f(x0) = f[x0] and œâ0 = 1, by recursion on n we can obtain
from (8.16) the following formula
Œ†nf(x) =
n

k=0
œâk(x)f[x0, . . . , xk].
(8.17)
Uniqueness of the interpolating polynomial ensures that the above expres-
sion yields the same interpolating polynomial generated by the Lagrange
form. Form (8.17) is commonly known as the Newton divided diÔ¨Äerence
formula for the interpolating polynomial.
Program 65 provides an implementation of Newton‚Äôs formula. The input
vectors x and y contain the interpolation nodes and the corresponding func-
tional evaluations of f, respectively, while vector z contains the abscissae
where the polynomial Œ†nf is to be evaluated. This polynomial is stored in
the output vector f.
Program 65 - interpol : Lagrange polynomial using Newton‚Äôs formula
function [f] = interpol (x,y,z)
[m n] = size(y);
for j = 1:m
a (:,1) = y (j,:)‚Äô;
for i = 2:n
a (i:n,i) = ( a(i:n,i-1)-a(i-1,i-1) )./(x(i:n)-x(i-1))‚Äô;
end
f(j,:) = a(n,n).*(z-x(n-1)) + a(n-1,n-1);
for i = 2:n-1
f(j,:) = f(j,:).*(z-x(n-i))+a(n-i,n-i);
end
end

8.2 Newton Form of the Interpolating Polynomial
335
8.2.1
Some Properties of Newton Divided DiÔ¨Äerences
The n-th divided diÔ¨Äerence f[x0, . . . , xn] = an can be further characterized
by noticing that it is the coeÔ¨Écient of xn in Œ†nf. Isolating such a coeÔ¨Écient
from (8.5) and equating it with the corresponding coeÔ¨Écient in the Newton
formula (8.17), we end up with the following explicit representation
f[x0, . . . , xn] =
n

i=0
f(xi)
œâ‚Ä≤
n+1(xi).
(8.18)
This formula has remarkable consequences:
1. the value attained by the divided diÔ¨Äerence is invariant with respect
to permutations of the indexes of the nodes. This instance can be
proÔ¨Åtably employed when stability problems suggest exchanging the
indexes (for example, if x is the point where the polynomial must be
computed, it is convenient to introduce a permutation of the indexes
such that |x ‚àíxk| ‚â§|x ‚àíxk‚àí1| with k = 1, . . . , n);
2. if f = Œ±g + Œ≤h for some Œ±, Œ≤ ‚ààR, then
f[x0, . . . , xn] = Œ±g[x0, . . . , xn] + Œ≤h[x0, . . . , xn];
3. if f = gh, the following formula (called the Leibniz formula) holds
(see [Die93])
f[x0, . . . , xn] =
n

j=0
g[x0, . . . , xj]h[xj, . . . , xn];
4. an algebraic manipulation of (8.18) (see Exercise 7) yields the follow-
ing recursive formula for computing divided diÔ¨Äerences
f[x0, . . . , xn] = f[x1, . . . , xn] ‚àíf[x0, . . . , xn‚àí1]
xn ‚àíx0
,
n ‚â•1.
(8.19)
Program 66 implements the recursive formula (8.19). The evaluations of f
at the interpolation nodes x are stored in vector y, while the output matrix
d (lower triangular) contains the divided diÔ¨Äerences, which are stored in
the following form
x0
f[x0]
x1
f[x1]
f[x0, x1]
x2
f[x2]
f[x1, x2]
f[x0, x1, x2]
...
...
...
...
xn
f[xn]
f[xn‚àí1, xn]
f[xn‚àí2, xn‚àí1, xn]
. . .
f[x0, . . . , xn]

336
8. Polynomial Interpolation
The coeÔ¨Écients involved in the Newton formula are the diagonal entries of
the matrix.
Program 66 - dividif : Newton divided diÔ¨Äerences
function [d]=dividif(x,y)
[n,m]=size(y);
if n == 1, n = m; end
n = n-1;
d = zeros (n+1,n+1);
d (:,1) = y‚Äô;
for j = 2:n+1
for i = j:n+1
d (i,j) = ( d (i-1,j-1)-d (i,j-1))/(x (i-j+1)-x (i));
end
end
Using (8.19), n(n + 1) sums and n(n + 1)/2 divisions are needed to gen-
erate the whole matrix. If a new evaluation of f were available at a new
node xn+1, only the calculation of a new row of the matrix would be re-
quired (f[xn, xn+1], . . . , f[x0, x1, . . . , xn+1]). Thus, in order to construct
Œ†n+1f from Œ†nf, it suÔ¨Éces to add to Œ†nf the term an+1œân+1(x), with a
computational cost of (n + 1) divisions and 2(n + 1) sums. For the sake of
notational simplicity, we write below Drfi = f[xi, xi+1, . . . , xr].
Example 8.3 In Table 8.1 we show the divided diÔ¨Äerences on the interval (0,2)
for the function f(x) = 1+sin(3x). The values of f and the corresponding divided
diÔ¨Äerences have been computed using 16 signiÔ¨Åcant Ô¨Ågures, although only the Ô¨Årst
5 Ô¨Ågures are reported. If the value of f were available at node x = 0.2, updating
the divided diÔ¨Äerence table would require only to computing the entries denoted
by italics in Table 8.1.
‚Ä¢
xi
f(xi)
f [xi, xi‚àí1]
D2fi
D3fi
D4fi
D5fi
D6fi
0
1.0000
0.2
1.5646
2.82
0.4
1.9320
1.83
-2.46
0.8
1.6755
-0.64
-4.13
-2.08
1.2
0.5575
-2.79
-2.69
1.43
2.93
1.6
0.0038
-1.38
1.76
3.71
1.62
-0.81
2.0
0.7206
1.79
3.97
1.83
-1.17
-1.55
-0.36
TABLE 8.1. Divided diÔ¨Äerences for the function f(x) = 1 + sin(3x) in the case
in which the evaluation of f at x = 0.2 is also available. The newly computed
values are denoted by italics

8.2 Newton Form of the Interpolating Polynomial
337
Notice that f[x0, . . . , xn] = 0 for any f ‚ààPn‚àí1. This property, how-
ever, is not always veriÔ¨Åed numerically, since the computation of divided
diÔ¨Äerences might be highly aÔ¨Äected by rounding errors.
Example 8.4 Consider again the divided diÔ¨Äerences for the function f(x) =
1 + sin(3x) on the interval (0, 0.0002). The function behaves like 1 + 3x in a
suÔ¨Éciently small neighbourhood of 0, so that we expect to Ô¨Ånd smaller numbers as
the order of divided diÔ¨Äerences increases. However, the results obtained running
Program 66, and shown in Table 8.2 in exponential notation up to the Ô¨Årst 4
signiÔ¨Åcant Ô¨Ågures (although 16 digits have been employed in the calculations),
exhibit a substantially diÔ¨Äerent pattern. The small rounding errors introduced in
the computation of divided diÔ¨Äerences of low order have dramatically propagated
on the higher order divided diÔ¨Äerences.
‚Ä¢
xi
f(xi)
f[xi, xi‚àí1]
D2fi
D3fi
D4fi
D5fi
0
1.0000
4.0e-5
1.0001
3.000
8.0e-5
1.0002
3.000
-5.39e-4
1.2e-4
1.0004
3.000
-1.08e-3
-4.50
1.6e-4
1.0005
3.000
-1.62e-3
-4.49
1.80e+1
2.0e-4
1.0006
3.000
-2.15e-3
-4.49
-7.23
‚àí1.2e + 5
TABLE 8.2. Divided diÔ¨Äerences for the function f(x) = 1+sin(3x) on the interval
(0,0.0002). Notice the completely wrong value in the last column (it should be
approximately equal to 0), due to the propagation of rounding errors throughout
the algorithm
8.2.2
The Interpolation Error Using Divided DiÔ¨Äerences
Consider the nodes x0, . . . , xn and let Œ†nf be the interpolating polynomial
of f on such nodes. Now let x be a node distinct from the previous ones;
letting xn+1 = x, we denote by Œ†n+1f the interpolating polynomial of f
at the nodes xk, k = 0, . . . , n + 1. Using the Newton divided diÔ¨Äerences
formula, we get
Œ†n+1f(t) = Œ†nf(t) + (t ‚àíx0) . . . (t ‚àíxn)f[x0, . . . , xn, t].
Since Œ†n+1f(x) = f(x), we obtain the following formula for the interpola-
tion error at t = x
En(x)
=
f(x) ‚àíŒ†nf(x) = Œ†n+1f(x) ‚àíŒ†nf(x)
=
(x ‚àíx0) . . . (x ‚àíxn)f[x0, . . . , xn, x]
=
œân+1(x)f[x0, . . . , xn, x].
(8.20)

338
8. Polynomial Interpolation
Assuming f ‚ààC(n+1)(Ix) and comparing (8.20) with (8.7), yields
f[x0, . . . , xn, x] = f (n+1)(Œæ)
(n + 1)!
(8.21)
for a suitable Œæ ‚ààIx. Since (8.21) resembles the remainder of the Tay-
lor series expansion of f, the Newton formula (8.17) for the interpolating
polynomial is often regarded as being a truncated expansion around x0
provided that |xn ‚àíx0| is not too big.
8.3
Piecewise Lagrange Interpolation
In Section 8.1.1 we have outlined the fact that, for equally spaced inter-
polating nodes, uniform convergence of Œ†nf to f is not guaranteed as
n ‚Üí‚àû. On the other hand, using equally spaced nodes is clearly computa-
tionally convenient and, moreover, Lagrange interpolation of low degree is
suÔ¨Éciently accurate, provided suÔ¨Éciently small interpolation intervals are
considered.
Therefore, it is natural to introduce a partition Th of [a, b] into K subin-
tervals Ij = [xj, xj+1] of length hj, with h = max0‚â§j‚â§K‚àí1 hj, such that
[a, b] = ‚à™K‚àí1
j=0
Ij and then to employ Lagrange interpolation on each Ij
using n + 1 equally spaced nodes
2
x(i)
j , 0 ‚â§i ‚â§n
3
with a small n.
For k ‚â•1, we introduce on Th the piecewise polynomial space
Xk
h =

v ‚ààC0([a, b]) : v|Ij ‚ààPk(Ij) ‚àÄIj ‚ààTh

(8.22)
which is the space of the continuous functions over [a, b] whose restric-
tions on each Ij are polynomials of degree ‚â§k. Then, for any continuous
function f in [a, b], the piecewise interpolation polynomial Œ†k
hf coincides
on each Ij with the interpolating polynomial of f|Ij at the n + 1 nodes
2
x(i)
j , 0 ‚â§i ‚â§n
3
. As a consequence, if f ‚ààCk+1([a, b]), using (8.7) within
each interval we obtain the following error estimate
‚à•f ‚àíŒ†k
hf‚à•‚àû‚â§Chk+1 ‚à•f (k+1)‚à•‚àû.
(8.23)
Note that a small interpolation error can be obtained even for low k pro-
vided that h is suÔ¨Éciently ‚Äúsmall‚Äù.
Example 8.5 Let us go back to the function of Runge‚Äôs counterexample. Now,
piecewise polynomials of degree k = 1 and k = 2 are employed. We check ex-
perimentally for the behavior of the error as h decreases. In Table 8.3 we show
the absolute errors measured in the maximum norm over the interval [‚àí5, 5] and
the corresponding estimates of the convergence order p with respect to h. Except
when using an excessively small number of subintervals, the results conÔ¨Årm the
theoretical estimate (8.23), that is p = k + 1.
‚Ä¢

8.3 Piecewise Lagrange Interpolation
339
h
‚à•f ‚àíŒ†h
1‚à•‚àû
p
‚à•f ‚àíŒ†h
2‚à•‚àû
p
5
0.4153
0.0835
2.5
0.1787
1.216
0.0971
-0.217
1.25
0.0631
1.501
0.0477
1.024
0.625
0.0535
0.237
0.0082
2.537
0.3125
0.0206
1.374
0.0010
3.038
0.15625
0.0058
1.819
1.3828e-04
2.856
0.078125
0.0015
1.954
1.7715e-05
2.964
TABLE 8.3. Interpolation error for Lagrange piecewise interpolation of degree
k = 1 and k = 2, in the case of Runge‚Äôs function (8.12); p denotes the trend of
the exponent of h. Notice that, as h ‚Üí0, p ‚Üík + 1, as predicted by (8.23)
Besides estimate (8.23), convergence results in integral norms exist (see
[QV94], [EEHJ96]). For this purpose, we introduce the following space
L2(a, b) =
Ô£±
Ô£≤
Ô£≥f : (a, b) ‚ÜíR,
b
>
a
|f(x)|2dx < +‚àû
Ô£º
Ô£Ω
Ô£æ,
(8.24)
with
‚à•f‚à•L2(a,b) =
Ô£´
Ô£≠
b
>
a
|f(x)|2dx
Ô£∂
Ô£∏
1/2
.
(8.25)
Formula (8.25) deÔ¨Ånes a norm for L2(a, b). (We recall that norms and semi-
norms of functions can be deÔ¨Åned in a manner similar to what was done in
DeÔ¨Ånition 1.17 in the case of vectors). We warn the reader that the integral
of the function |f|2 in (8.24) has to be intended in the Lebesgue sense (see,
e.g., [Rud83]). In particular, f needs not be continuous everywhere.
Theorem 8.3 Let 0 ‚â§m ‚â§k + 1, with k ‚â•1 and assume that f (m) ‚àà
L2(a, b) for 0 ‚â§m ‚â§k + 1; then there exists a positive constant C, inde-
pendent of h, such that
‚à•(f ‚àíŒ†k
hf)(m)‚à•L2(a,b) ‚â§Chk+1‚àím‚à•f (k+1)‚à•L2(a,b).
(8.26)
In particular, for k = 1, and m = 0 or m = 1, we obtain
‚à•f ‚àíŒ†1
hf‚à•L2(a,b) ‚â§C1h2‚à•f ‚Ä≤‚Ä≤‚à•L2(a,b),
‚à•(f ‚àíŒ†1
hf)‚Ä≤‚à•L2(a,b) ‚â§C2h‚à•f ‚Ä≤‚Ä≤‚à•L2(a,b),
(8.27)
for two suitable positive constants C1 and C2.
Proof. We only prove (8.27) and refer to [QV94], Chapter 3 for the proof of
(8.26) in the general case.

340
8. Polynomial Interpolation
DeÔ¨Åne e = f ‚àíŒ†1
hf. Since e(xj) = 0 for all j = 0, . . . , K, Rolle‚Äôs theorem
infers the existence of Œæj ‚àà(xj, xj+1), for j = 0, . . . , K ‚àí1 such that e‚Ä≤(Œæj) = 0.
Since Œ†1
hf is a linear function on each Ij, for x ‚ààIj we obtain
e‚Ä≤(x) =
> x
Œæj
e‚Ä≤‚Ä≤(s)ds =
> x
Œæj
f ‚Ä≤‚Ä≤(s)ds,
whence
|e‚Ä≤(x)| ‚â§
> xj+1
xj
|f ‚Ä≤‚Ä≤(s)|ds,
for x ‚àà[xj, xj+1].
(8.28)
We recall the Cauchy-Schwarz inequality

> Œ≤
Œ±
u(x)v(x)dx
 ‚â§
> Œ≤
Œ±
u2(x)dx
1/2 > Œ≤
Œ±
v2(x)dx
1/2
(8.29)
which holds if u, v ‚ààL2(Œ±, Œ≤). If we apply this inequality to (8.28) we obtain
|e‚Ä≤(x)|
‚â§
Ô£´
Ô£¨
Ô£≠
xj+1
>
xj
12dx
Ô£∂
Ô£∑
Ô£∏
1/2 Ô£´
Ô£¨
Ô£≠
xj+1
>
xj
|f ‚Ä≤‚Ä≤(s)|2ds
Ô£∂
Ô£∑
Ô£∏
1/2
‚â§h1/2
Ô£´
Ô£¨
Ô£≠
xj+1
>
xj
|f ‚Ä≤‚Ä≤(s)|2ds
Ô£∂
Ô£∑
Ô£∏
1/2
.
(8.30)
To Ô¨Ånd a bound for |e(x)|, we notice that
e(x) =
> x
xj
e‚Ä≤(s)ds,
so that, applying (8.30), we get
|e(x)| ‚â§
> xj+1
xj
|e‚Ä≤(s)|ds ‚â§h3/2
> xj+1
xj
|f ‚Ä≤‚Ä≤(s)|2ds
1/2
.
(8.31)
Then
xj+1
>
xj
|e‚Ä≤(x)|2dx ‚â§h2
xj+1
>
xj
|f ‚Ä≤‚Ä≤(s)|2ds
and
xj+1
>
xj
|e(x)|2dx ‚â§h4
xj+1
>
xj
|f ‚Ä≤‚Ä≤(s)|2ds,
from which, summing over the index j from 0 to K ‚àí1 and taking the square
root of both sides, we obtain
> b
a
|e‚Ä≤(x)|2dx
1/2
‚â§h
> b
a
|f ‚Ä≤‚Ä≤(x)|2dx
1/2
,
and
> b
a
|e(x)|2dx
1/2
‚â§h2
> b
a
|f ‚Ä≤‚Ä≤(x)|2dx
1/2
,
which is the desired estimate (8.27), with C1 = C2 = 1.
3

8.4 Hermite-BirkoÔ¨ÄInterpolation
341
8.4
Hermite-BirkoÔ¨ÄInterpolation
Lagrange polynomial interpolation can be generalized to the case in which
also the values of the derivatives of a function f are available at some (or
all) of the nodes xi.
Let us then suppose that (xi, f (k)(xi)) are given data, with i = 0, . . . , n,
k = 0, . . . , mi and mi ‚ààN. Letting N = n
i=0(mi + 1), it can be proved
(see [Dav63]) that, if the nodes {xi} are distinct, there exists a unique
polynomial HN‚àí1 ‚ààPN‚àí1, called the Hermite interpolation polynomial,
such that
H(k)
N‚àí1(xi) = y(k)
i
,
i = 0, . . . , n
k = 0, . . . , mi,
of the form
HN‚àí1(x) =
n

i=0
mi

k=0
y(k)
i
Lik(x)
(8.32)
where y(k)
i
= f (k)(xi), i = 0, . . . , n, k = 0, . . . , mi.
The functions Lik ‚ààPN‚àí1 are called the Hermite characteristic polynomials
and are deÔ¨Åned through the relations
dp
dxp (Lik)(xj) =
"
1
if i = j and k = p,
0
otherwise.
DeÔ¨Åning the polynomials
lij(x) = (x ‚àíxi)j
j!
n

k=0
kÃ∏=i
 x ‚àíxk
xi ‚àíxk
mk+1
, i = 0, . . . , n, j = 0, . . . , mi,
and letting Limi(x) = limi(x) for i = 0, . . . , n, we have the following recur-
sive formula for the polynomials Lij
Lij(x) = lij(x) ‚àí
mi

k=j+1
l(k)
ij (xi)Lik(x)
j = mi ‚àí1, mi ‚àí2, . . . , 0.
As for the interpolation error, the following estimate holds
f(x) ‚àíHN‚àí1(x) = f (N)(Œæ)
N!
‚Ñ¶N(x)
‚àÄx ‚ààR
where Œæ ‚ààI(x; x0, . . . , xn) and ‚Ñ¶N is the polynomial of degree N deÔ¨Åned
by
‚Ñ¶N(x) = (x ‚àíx0)m0+1(x ‚àíx1)m1+1 . . . (x ‚àíxn)mn+1.
(8.33)

342
8. Polynomial Interpolation
Example 8.6 (osculatory interpolation) Let us set mi = 1 for i = 0, . . . , n.
In this case N = 2n + 2 and the interpolating Hermite polynomial is called the
osculating polynomial, and it is given by
HN‚àí1(x) =
n

i=0
+
yiAi(x) + y(1)
i
Bi(x)
,
where Ai(x) = (1 ‚àí2(x ‚àíxi)l‚Ä≤
i(xi))li(x)2 and Bi(x) = (x ‚àíxi)li(x)2, for i =
0, . . . , n, with
l‚Ä≤
i(xi) =
n

k=0,kÃ∏=i
1
xi ‚àíxk ,
i = 0, . . . , n.
As a comparison, we use Programs 65 and 67 to compute the Lagrange and
Hermite interpolating polynomials of the function f(x) = sin(4œÄx) on the interval
[0, 1] taking four equally spaced nodes (n = 3). Figure 8.4 shows the superposed
graphs of the function f (dashed line) and of the two polynomials Œ†nf (dotted
line) and HN‚àí1 (solid line).
‚Ä¢
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
‚àí1.5
‚àí1
‚àí0.5
0
0.5
1
1.5
FIGURE
8.4.
Lagrange
and
Hermite
interpolation
for
the
function
f(x) = sin(4œÄx) on the interval [0, 1]
Program 67 computes the values of the osculating polynomial at the ab-
scissae contained in the vector z. The input vectors x, y and dy contain the
interpolation nodes and the corresponding function evaluations of f and
f ‚Ä≤, respectively.
Program 67 - hermpol : Osculating polynomial
function [herm] = hermite(x,y,dy,z)
n = max(size(x)); m = max(size(z)); herm = [];
for j = 1:m
xx = z(j); hxv = 0;
for i = 1:n,
den = 1; num = 1; xn = x(i); derLi = 0;
for k = 1:n,

8.5 Extension to the Two-Dimensional Case
343
if k Àú= i, num = num*(xx-x(k)); arg = xn-x(k);
den = den*arg; derLi = derLi+1/arg;
end
end
Lix2 = (num/den)ÀÜ2; p = (1-2*(xx-xn)*derLi)*Lix2;
q = (xx-xn)*Lix2; hxv = hxv+(y(i)*p+dy(i)*q);
end
herm = [herm, hxv];
end
8.5
Extension to the Two-Dimensional Case
In this section we brieÔ¨Çy address the extension of the previous concepts to
the two-dimensional case, referring to [SL89], [CHQZ88], [QV94] for more
details. We denote by ‚Ñ¶a bounded domain in R2 and by x = (x, y) the
coordinate vector of a point in ‚Ñ¶.
8.5.1
Polynomial Interpolation
A particularly simple situation occurs when ‚Ñ¶= [a, b] √ó [c, d], i.e., the
interpolation domain ‚Ñ¶is the tensor product of two intervals. In such a
case, introducing the nodes a = x0 < x1 < . . . < xn = b and c = y0 <
y1 < . . . < ym = d, the interpolating polynomial Œ†n,mf can be written as
Œ†n,mf(x, y) = n
i=0
m
j=0 Œ±ijli(x)lj(y), where li ‚ààPn, i = 0, . . . , n, and
lj ‚ààPm, j = 0, . . . , m, are the characteristic one-dimensional Lagrange
polynomials with respect to the x and y variables respectively, and where
Œ±ij = f(xi, yj).
The drawbacks of one-dimensional Lagrange interpolation are inherited
by the two-dimensional case, as conÔ¨Årmed by the example in Figure 8.5.
Remark 8.1 (The general case) If ‚Ñ¶is not a rectangular domain or if
the interpolation nodes are not uniformly distributed over a Cartesian grid,
the interpolation problem is diÔ¨Écult to solve, and, generally speaking, it is
preferable to resort to a least-squares solution (see Section 10.7). We also
point out that in d dimensions (with d ‚â•2) the problem of Ô¨Ånding an
interpolating polynomial of degree n with respect to each space variable on
n + 1 distinct nodes might be ill-posed.
Consider, for example, a polynomial of degree 1 with respect to x and y
of the form p(x, y) = a3xy+a2x+a1y+a0 to interpolate a function f at the
nodes (‚àí1, 0), (0, ‚àí1), (1, 0) and (0, 1). Although the nodes are distinct, the
problem (which is nonlinear) does not in general admit a unique solution;
actually, imposing the interpolation constraints, we end up with a system
that is satisÔ¨Åed by any value of the coeÔ¨Écient a3.
‚ñ†

344
8. Polynomial Interpolation
‚àí5
0
5
‚àí5
0
5
‚àí0.1
0
0.1
0.2
0.3
0.4
0.5
‚àí5
0
5
‚àí5
0
5
‚àí2
0
2
4
6
8
FIGURE 8.5. Runge‚Äôs counterexample extended to the two-dimensional case:
interpolating polynomial on a 6 √ó 6 nodes grid (left) and on a 11 √ó 11 nodes grid
(right). Notice the change in the vertical scale between the two plots
8.5.2
Piecewise Polynomial Interpolation
In the multidimensional case, the higher Ô¨Çexibility of piecewise interpola-
tion allows for easy handling of domains of complex shape. Let us suppose
that ‚Ñ¶is a polygon in R2. Then, ‚Ñ¶can be partitioned into K nonover-
lapping triangles (or elements) T, which deÔ¨Åne the so called triangulation
of the domain which will be denoted by Th. Clearly, ‚Ñ¶=
-
T ‚ààTh
T. Suppose
that the maximum length of the edges of the triangles is less than a positive
number h. As shown in Figure 8.6 (left), not any arbitrary triangulation is
allowed. Precisely, the admissible ones are those for which any pair of non
disjoint triangles may have a vertex or an edge in common.
T
T2
1
2
T
T
2
1
T1
T2
T1
T
1
0
1
FT
T
x
y
y
x
aT
1
T
aT
3
aT
2
FIGURE 8.6. The left side picture shows admissible (above) and non admissible
(below) triangulations while the right side picture shows the aÔ¨Éne map from the
reference triangle ÀÜT to the generic element T ‚ààTh
Any element T ‚ààTh, of area equal to |T|, is the image through the aÔ¨Éne
map x = FT (ÀÜx) = BT ÀÜx + bT of the reference triangle T, of vertices (0,0),

8.5 Extension to the Two-Dimensional Case
345
(1,0) and (0,1) in the ÀÜx = (ÀÜx, ÀÜy) plane (see Figure 8.6, right), where the
invertible matrix BT and the right-hand side bT are given respectively by
BT =
.
x2 ‚àíx1
x3 ‚àíx1
y2 ‚àíy1
y3 ‚àíy1
/
,
bT = (x1, y1)T ,
(8.34)
while the coordinates of the vertices of T are denoted by a(l)
T
= (xl, yl)T
for l = 1, 2, 3.
(x)
l (x,y)
i
iz
z
1
i
li
1
z
iz
i
1
li(x,y)
1
li(x)
FIGURE 8.7. Characteristic piecewise Lagrange polynomial, in one and two space
dimensions. Left, k = 0; right, k = 1
The aÔ¨Éne map (8.34) is of remarkable importance in practical computa-
tions, since, once a basis has been generated for representing the piecewise
polynomial interpolant on ÀÜT, it is possible, applying the change of coor-
dinates x = FT (ÀÜx), to reconstruct the polynomial on each element T of
Th. We are thus interested in devising local basis functions, which can be
fully described over each triangle without needing any information from
adjacent triangles.
For this purpose, let us introduce on Th the set Z of the piecewise interpo-
lation nodes zi = (xi, yi)T , for i = 1, . . . , N, and denote by Pk(‚Ñ¶), k ‚â•0,
the space of algebraic polynomials of degree ‚â§k in the space variables x, y
Pk(‚Ñ¶) =
Ô£±
Ô£¥
Ô£≤
Ô£¥
Ô£≥
p(x, y) =
k

i,j=0
i+j‚â§k
aijxiyj, x, y ‚àà‚Ñ¶
Ô£º
Ô£¥
Ô£Ω
Ô£¥
Ô£æ
.
(8.35)
Finally, for k ‚â•0, let Pc
k(‚Ñ¶) be the space of piecewise polynomials of degree
‚â§k, such that, for any p ‚ààPc
k(‚Ñ¶), p|T ‚ààPk(T) for any T ‚ààTh. An ele-
mentary basis for Pc
k(‚Ñ¶) consists of the Lagrange characteristic polynomials
li = li(x, y), such that li ‚ààPc
k(‚Ñ¶) and
li(zj) = Œ¥ij,
i, j = 1, . . . , N,
(8.36)

346
8. Polynomial Interpolation
where Œ¥ij is the Kronecker symbol. We show in Figure 8.7 the functions li for
k = 0, 1, together with their corresponding one-dimensional counterparts.
In the case k = 0, the interpolation nodes are collocated at the centers of
gravity of the triangles, while in the case k = 1 the nodes coincide with
the vertices of the triangles. This choice, that we are going to maintain
henceforth, is not the only one possible. The midpoints of the edges of the
triangles could be used as well, giving rise to a discontinuous piecewise
polynomial over ‚Ñ¶.
For k ‚â•0, the Lagrange piecewise interpolating polynomial of f, Œ†k
hf ‚àà
Pc
k(‚Ñ¶), is deÔ¨Åned as
Œ†k
hf(x, y) =
N

i=1
f(zi)li(x, y).
(8.37)
Notice that Œ†0
hf is a piecewise constant function, while Œ†1
hf is a linear
function over each triangle, continuous at the vertices, and thus globally
continuous.
For any T ‚ààTh, we shall denote by Œ†k
T f the restriction of the piecewise
interpolating polynomial of f over the element T. By deÔ¨Ånition, Œ†k
T f ‚àà
Pk(T); noticing that dk = dimPk(T) = (k + 1)(k + 2)/2, we can therefore
write
Œ†k
T f(x, y) =
dk‚àí1

m=0
f(Àúz(m)
T
)lm,T (x, y),
‚àÄT ‚ààTh.
(8.38)
In (8.38), we have denoted by Àúz(m)
T
, for m = 0, . . . , dk ‚àí1, the piecewise
interpolation nodes on T and by lm,T (x, y) the restriction to T of the La-
grange characteristic polynomial having index i in (8.37) which corresponds
in the list of the ‚Äúglobal‚Äù nodes zi to that of the ‚Äúlocal‚Äù node Àúz(m)
T
.
Keeping on with this notation, we have lj,T (x) = ÀÜlj ‚ó¶F ‚àí1
T (x), where
ÀÜlj = ÀÜlj(ÀÜx) is, for j = 0, . . . , dk ‚àí1, the j-th Lagrange basis function for
Pk( ÀÜT) generated on the reference element ÀÜT. We notice that if k = 0 then
d0 = 1, that is, only one local interpolation node exists (coinciding with
the center of gravity of the triangle T), while if k = 1 then d1 = 3, that is,
three local interpolation nodes exist, coinciding with the vertices of T. In
Figure 8.8 we draw the local interpolation nodes on ÀÜT for k = 0, 1 and 2.
As for the interpolation error estimate, denoting for any T ‚ààTh by hT the
maximum length of the edges of T, the following result holds (see for the
proof, [CL91], Theorem 16.1, pp. 125-126 and [QV94], Remark 3.4.2, pp.
89-90)
‚à•f ‚àíŒ†k
T f‚à•‚àû,T ‚â§Chk+1
T
‚à•f (k+1)‚à•‚àû,T ,
k ‚â•0,
(8.39)
where for every g ‚ààC0(T), ‚à•g‚à•‚àû,T = maxx‚ààT |g(x)|. In (8.39), C is a
positive constant independent of hT and f.

8.5 Extension to the Two-Dimensional Case
347
FIGURE 8.8. Local interpolation nodes on ÀÜT; left, k = 0, center k = 1, right,
k = 2
Let us assume that the triangulation Th is regular, i.e., there exists a
positive constant œÉ such that
max
T ‚ààTh
hT
œÅT
‚â§œÉ,
where ‚àÄT ‚ààTh, œÅT is the diameter of the inscribed circle to T, Then, it
is possible to derive from (8.39) the following interpolation error estimate
over the whole domain ‚Ñ¶
‚à•f ‚àíŒ†k
hf‚à•‚àû,‚Ñ¶‚â§Chk+1‚à•f (k+1)‚à•‚àû,‚Ñ¶,
k ‚â•0,
‚àÄf ‚ààCk+1(‚Ñ¶).
(8.40)
The theory of piecewise interpolation is a basic tool of the Ô¨Ånite element
method, a computational technique that is widely used in the numerical
approximation of partial diÔ¨Äerential equations (see Chapter 12 for the one-
dimensional case and [QV94] for a complete presentation of the method).
Example 8.7 We compare the convergence of the piecewise polynomial interpo-
lation of degree 0, 1 and 2, on the function f(x, y) = e‚àí(x2+y2) on ‚Ñ¶= (‚àí1, 1)2.
We show in Table 8.4 the error Ek = ‚à•f ‚àíŒ†k
hf‚à•‚àû,‚Ñ¶, for k = 0, 1, 2, and the order
of convergence pk as a function of the mesh size h = 2/N for N = 2, . . . , 32.
Clearly, linear convergence is observed for interpolation of degree 0 while the
order of convergence is quadratic with respect to h for interpolation of degree 1
and cubic for interpolation of degree 2.
‚Ä¢
h
E0
p0
E1
p1
E2
p2
1
0.4384
0.2387
0.016
1
2
0.2931
0.5809
0.1037
1.2028
1.6678 ¬∑ 10‚àí3
3.2639
1
4
0.1579
0.8924
0.0298
1.7990
2.8151 ¬∑ 10‚àí4
2.5667
1
8
0.0795
0.9900
0.0077
1.9524
3.5165 ¬∑ 10‚àí5
3.001
1
16
0.0399
0.9946
0.0019
2.0189
4.555 ¬∑ 10‚àí6
2.9486
TABLE 8.4. Convergence rates and orders for piecewise interpolations of degree
0, 1 and 2

348
8. Polynomial Interpolation
8.6
Approximation by Splines
In this section we address the matter of approximating a given function us-
ing splines, which allow for a piecewise interpolation with a global smooth-
ness.
DeÔ¨Ånition 8.1 Let x0, . . . , xn, be n + 1 distinct nodes of [a, b], with a =
x0 < x1 < . . . < xn = b. The function sk(x) on the interval [a,b] is a spline
of degree k relative to the nodes xj if
sk|[xj,xj+1] ‚ààPk,
j = 0, 1, . . . , n ‚àí1
(8.41)
sk ‚ààCk‚àí1[a, b].
(8.42)
‚ñ†
Denoting by Sk the space of splines sk on [a, b] relative to n + 1 distinct
nodes, then dim Sk = n+k. Obviously, any polynomial of degree k on [a, b]
is a spline; however, in the practice a spline is represented by a diÔ¨Äerent
polynomial on each subinterval and for this reason there could be a discon-
tinuity in its k-th derivative at the internal nodes x1, . . . , xn‚àí1. The nodes
for which this actually happens are called active nodes.
It is simple to check that conditions (8.41) and (8.42) do not suÔ¨Éce to
characterize a spline of degree k. Indeed, the restriction sk,j = sk|[xj,xj+1]
can be represented as
sk,j(x) =
k

i=0
sij(x ‚àíxj)i,
if x ‚àà[xj, xj+1]
(8.43)
so that (k + 1)n coeÔ¨Écients sij must be determined. On the other hand,
from (8.42) it follows that
s(m)
k,j‚àí1(xj) = s(m)
k,j (xj),
j = 1, . . . , n ‚àí1,
m = 0, ..., k ‚àí1
which amounts to setting k(n ‚àí1) conditions. As a consequence, the re-
maining degrees of freedom are (k + 1)n ‚àík(n ‚àí1) = k + n.
Even if the spline were interpolatory, that is, such that sk(xj) = fj for
j = 0, . . . , n, where f0, . . . , fn are given values, there would still be k ‚àí1
unsaturated degrees of freedom. For this reason further constraints are
usually imposed, which lead to:
1. periodic splines, if
s(m)
k
(a) = s(m)
k
(b),
m = 0, 1, . . . , k ‚àí1;
(8.44)

8.6 Approximation by Splines
349
2. natural splines, if for k = 2l ‚àí1, with l ‚â•2
s(l+j)
k
(a) = s(l+j)
k
(b) = 0,
j = 0, 1, . . . , l ‚àí2.
(8.45)
From (8.43) it turns out that a spline can be conveniently represented using
k +n spline basis functions, such that (8.42) is automatically satisÔ¨Åed. The
simplest choice, which consists of employing a suitably enriched monomial
basis (see Exercise 10), is not satisfactory from the numerical standpoint,
since it is ill-conditioned. In Sections 8.6.1 and 8.6.2 possible examples of
spline basis functions will be provided: cardinal splines for the speciÔ¨Åc case
k = 3 and B-splines for a generic k.
8.6.1
Interpolatory Cubic Splines
Interpolatory cubic splines are particularly signiÔ¨Åcant since: i. they are
the splines of minimum degree that yield C2 approximations; ii. they are
suÔ¨Éciently smooth in the presence of small curvatures.
Let us thus consider, in [a, b], n + 1 ordered nodes a = x0 < x1 < . . . <
xn = b and the corresponding evaluations fi, i = 0, . . . , n. Our aim is to
provide an eÔ¨Écient procedure for constructing the cubic spline interpolating
those values. Since the spline is of degree 3, its second-order derivative must
be continuous. Let us introduce the following notation
fi = s3(xi),
mi = s‚Ä≤
3(xi),
Mi = s‚Ä≤‚Ä≤
3(xi),
i = 0, . . . , n.
Since s3,i‚àí1 ‚ààP3, s‚Ä≤‚Ä≤
3,i‚àí1 is linear and
s‚Ä≤‚Ä≤
3,i‚àí1(x) = Mi‚àí1
xi ‚àíx
hi
+ Mi
x ‚àíxi‚àí1
hi
for x ‚àà[xi‚àí1, xi]
(8.46)
where hi = xi ‚àíxi‚àí1. Integrating (8.46) twice we get
s3,i‚àí1(x) = Mi‚àí1
(xi ‚àíx)3
6hi
+ Mi
(x ‚àíxi‚àí1)3
6hi
+ Ci‚àí1(x ‚àíxi‚àí1) + $Ci‚àí1,
and the constants Ci‚àí1 and $Ci‚àí1 are determined by imposing the end point
values s3(xi‚àí1) = fi‚àí1 and s3(xi) = fi. This yields, for i = 1, . . . , n ‚àí1
$Ci‚àí1 = fi‚àí1 ‚àíMi‚àí1
h2
i
6 ,
Ci‚àí1 = fi ‚àífi‚àí1
hi
‚àíhi
6 (Mi ‚àíMi‚àí1).
Let us now enforce the continuity of the Ô¨Årst derivatives at xi; we get
s‚Ä≤
3(x‚àí
i )
= hi
6 Mi‚àí1 + hi
3 Mi + fi ‚àífi‚àí1
hi
= ‚àíhi+1
3
Mi ‚àíhi+1
6
Mi+1 + fi+1 ‚àífi
hi+1
= s‚Ä≤
3(x+
i ),

350
8. Polynomial Interpolation
where s‚Ä≤
3(x¬±
i ) = lim
t‚Üí0s‚Ä≤
3(xi ¬± t). This leads to the following linear system
(called M-continuity system)
¬µiMi‚àí1 + 2Mi + ŒªiMi+1 = di
i = 1, . . . , n ‚àí1
(8.47)
where we have set
¬µi =
hi
hi + hi+1
,
Œªi =
hi+1
hi + hi+1
,
di =
6
hi + hi+1
fi+1 ‚àífi
hi+1
‚àífi ‚àífi‚àí1
hi

,
i = 1, . . . , n ‚àí1.
System (8.47) has n + 1 unknowns and n ‚àí1 equations; thus, 2(= k ‚àí1)
conditions are still lacking. In general, these conditions can be of the form
2M0 + Œª0M1 = d0,
¬µnMn‚àí1 + 2Mn = dn,
with 0 ‚â§Œª0, ¬µn ‚â§1 and d0, dn given values. For instance, in order to
obtain the natural splines (satisfying s‚Ä≤‚Ä≤
3(a) = s‚Ä≤‚Ä≤
3(b) = 0), we must set the
above coeÔ¨Écients equal to zero. A popular choice sets Œª0 = ¬µn = 1 and
d0 = d1, dn = dn‚àí1, which corresponds to prolongating the spline outside
the end points of the interval [a, b] and treating a and b as internal points.
This strategy produces a spline with a ‚Äúsmooth‚Äù behavior. In general, the
resulting linear system is tridiagonal of the form
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
2
Œª0
0
. . .
0
¬µ1
2
Œª1
...
0
...
...
...
0
...
¬µn‚àí1
2
Œªn‚àí1
0
. . .
0
¬µn
2
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
M0
M1
...
Mn‚àí1
Mn
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
=
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
d0
d1
...
dn‚àí1
dn
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
(8.48)
and it can be eÔ¨Éciently solved using the Thomas algorithm (3.53).
A closure condition for system (8.48), which can be useful when the
derivatives f ‚Ä≤(a) and f ‚Ä≤(b) are not available, consists of enforcing the con-
tinuity of s‚Ä≤‚Ä≤‚Ä≤
3 (x) at x1 and xn‚àí1. Since the nodes x1 and xn‚àí1 do not
actually contribute in constructing the cubic spline, it is called a not-a-
knot spline, with ‚Äúactive‚Äù knots {x0, x2, . . . , xn‚àí2, xn} and interpolating f
at all the nodes {x0, x1, x2, . . . , xn‚àí2, xn‚àí1, xn}.
Remark 8.2 (SpeciÔ¨Åc software) Several packages exist for dealing with
interpolating splines. In the case of cubic splines, we mention the command
spline, which uses the not-a-knot condition introduced above, or, in gen-
eral, the spline toolbox of MATLAB [dB90] and the library FITPACK
[Die87a], [Die87b].
‚ñ†

8.6 Approximation by Splines
351
A completely diÔ¨Äerent approach for generating s3 consists of providing
a basis {œïi} for the space S3 of cubic splines, whose dimension is equal to
n + 3. We consider here the case in which the n + 3 basis functions œïi have
global support in the interval [a, b], referring to Section 8.6.2 for the case
of a basis with local support.
Functions œïi, for i, j = 0, . . . , n, are deÔ¨Åned through the following inter-
polation constraints
œïi(xj) = Œ¥ij,
œï‚Ä≤
i(x0) = œï‚Ä≤
i(xn) = 0,
and two suitable splines must be added, œïn+1 and œïn+2. For instance, if
the spline must satisfy some assigned conditions on the derivative at the
end points, we ask that
œïn+1(xj) = 0,
j = 0, ..., n
œï‚Ä≤
n+1(x0) = 1,
œï‚Ä≤
n+1(xn) = 0,
œïn+2(xj) = 0,
j = 0, ..., n
œï‚Ä≤
n+2(x0) = 0,
œï‚Ä≤
n+2(xn) = 1.
By doing so, the spline takes the form
s3(x) =
n

i=0
fiœïi(x) + f ‚Ä≤
0œïn+1(x) + f ‚Ä≤
nœïn+2(x),
where f ‚Ä≤
0 and f ‚Ä≤
n are two given values. The resulting basis {œïi, i = 0, ..., n + 2}
is called a cardinal spline basis and is frequently employed in the numerical
solution of diÔ¨Äerential or integral equations. Figure 8.9 shows a generic car-
dinal spline, which is computed over a virtually unbounded interval where
the interpolation nodes xj are the integers. The spline changes sign in any
adjacent intervals [xj‚àí1, xj] and [xj, xj+1] and rapidly decays to zero.
Restricting ourselves to the positive axis, it can be shown (see [SL89])
that the extremant of the function on the interval [xj, xj+1] is equal to
the extremant on the interval [xj+1, xj+2] multiplied by a decaying factor
Œª ‚àà(0, 1). In such a way, possible errors arising over an interval are rapidly
damped on the next one, thus ensuring the stability of the algorithm.
Let us summarize the main properties of interpolating cubic splines, re-
ferring to [Sch81] and [dB83] for the proofs and more general results.
Property 8.2 Let f ‚ààC2([a, b]), and let s3 be the natural cubic spline
interpolating f. Then
b
>
a
[s‚Ä≤‚Ä≤
3(x)]2dx ‚â§
b
>
a
[f ‚Ä≤‚Ä≤(x)]2dx,
(8.49)
where equality holds if and only if f = s3.
The above result is known as the minimum norm property and has the
meaning of the minimum energy principle in mechanics. Property (8.49)

352
8. Polynomial Interpolation
‚àí4
‚àí3
‚àí2
‚àí1
0
1
2
3
4
5
‚àí0.2
0
0.2
0.4
0.6
0.8
FIGURE 8.9. Cardinal spline
still holds if conditions on the Ô¨Årst derivative of the spline at the end points
are assigned instead of natural conditions (in such a case, the spline is called
constrained, see Exercise 11).
The cubic interpolating spline sf of a function f ‚ààC2([a, b]), with
s‚Ä≤
f(a) = f ‚Ä≤(a) and s‚Ä≤
f(b) = f ‚Ä≤(b), also satisÔ¨Åes the following property
b
>
a
[f ‚Ä≤‚Ä≤(x) ‚àís‚Ä≤‚Ä≤
f(x)]2dx ‚â§
b
>
a
[f ‚Ä≤‚Ä≤(x) ‚àís‚Ä≤‚Ä≤(x)]2dx, ‚àÄs ‚ààS3.
As far as the error estimate is concerned, the following result holds.
Property 8.3 Let f ‚ààC4([a, b]) and Ô¨Åx a partition of [a, b] into subinter-
vals of width hi such that h = maxi hi and Œ≤ = h/ mini hi. Let s3 be the
cubic spline interpolating f. Then
‚à•f (r) ‚àís(r)
3 ‚à•‚àû‚â§Crh4‚àír‚à•f (4)‚à•‚àû,
r = 0, 1, 2, 3,
(8.50)
with C0 = 5/384, C1 = 1/24, C2 = 3/8 and C3 = (Œ≤ + Œ≤‚àí1)/2.
As a consequence, spline s3 and its Ô¨Årst and second order derivatives
uniformly converge to f and to its derivatives, as h tends to zero. The third
order derivative converges as well, provided that Œ≤ is uniformly bounded.
Example 8.8 Figure 8.10 shows the cubic spline approximating the function in
the Runge‚Äôs example, and its Ô¨Årst, second and third order derivatives, on a grid
of 11 equally spaced nodes, while in Table 8.5 the error ‚à•s3 ‚àíf‚à•‚àûis reported as
a function of h together with the computed order of convergence p. The results
clearly demonstrate that p tends to 4 (the theoretical order) as h tends to zero.
‚Ä¢

8.6 Approximation by Splines
353
h
1
0.5
0.25
0.125
0.0625
‚à•s3 ‚àíf‚à•‚àû
0.022
0.0032
2.7741e-4
1.5983e-5
9.6343e-7
p
‚Äì
2.7881
3.5197
4.1175
4.0522
TABLE 8.5. Experimental interpolation error for Runge‚Äôs function using cubic
splines
‚àí5
‚àí4
‚àí3
‚àí2
‚àí1
0
1
2
3
4
5
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
(a)
‚àí5
‚àí4
‚àí3
‚àí2
‚àí1
0
1
2
3
4
5
‚àí0.8
‚àí0.6
‚àí0.4
‚àí0.2
0
0.2
0.4
0.6
0.8
(b)
‚àí5
‚àí4
‚àí3
‚àí2
‚àí1
0
1
2
3
4
5
‚àí2
‚àí1.5
‚àí1
‚àí0.5
0
0.5
1
(c)
‚àí5
‚àí4
‚àí3
‚àí2
‚àí1
0
1
2
3
4
5
‚àí5
‚àí4
‚àí3
‚àí2
‚àí1
0
1
2
3
4
5
(d)
FIGURE 8.10. Interpolating spline (a) and its Ô¨Årst (b), second (c) and third (d)
order derivatives (in solid line) for the function of Runge‚Äôs example (in dashed
line)
8.6.2
B-splines
Let us go back to splines of a generic degree k, and consider the B-spline
(or bell-spline) basis, referring to divided diÔ¨Äerences introduced in Section
8.2.1.
DeÔ¨Ånition 8.2 The normalized B-spline Bi,k+1 of degree k relative to the
distinct nodes xi, . . . , xi+k+1 is deÔ¨Åned as
Bi,k+1(x) = (xi+k+1 ‚àíxi)g[xi, . . . , xi+k+1],
(8.51)
where
g(t) = (t ‚àíx)k
+ =
" (t ‚àíx)k
if x ‚â§t,
0
otherwise.
(8.52)

354
8. Polynomial Interpolation
‚ñ†
Substituting (8.18) into (8.51) yields the following explicit representation
Bi,k+1(x) = (xi+k+1 ‚àíxi)
k+1

j=0
(xj+i ‚àíx)k
+
k+1

l=0
lÃ∏=j
(xi+j ‚àíxi+l)
.
(8.53)
From (8.53) it turns out that the active nodes of Bi,k+1(x) are xi, . . . , xi+k+1
and that Bi,k+1(x) is non null only within the interval [xi, xi+k+1].
Actually, it can be proved that it is the unique non null spline of min-
imum support relative to nodes xi, . . . , xi+k+1 [Sch67]. It can also be
shown that Bi,k+1(x) ‚â•0 [dB83] and |B(l)
i,k+1(xi)| = |B(l)
i,k+1(xi+k+1)| for
l = 0, . . . , k‚àí1 [Sch81]. B-splines admit the following recursive formulation
([dB72], [Cox72])
Bi,1(x) =
" 1
if x ‚àà[xi, xi+1],
0
otherwise,
Bi,k+1(x) =
x ‚àíxi
xi+k ‚àíxi
Bi,k(x) +
xi+k+1 ‚àíx
xi+k+1 ‚àíxi+1
Bi+1,k(x), k ‚â•1,
(8.54)
which is usually preferred to (8.53) when evaluating a B-spline at a given
point.
Remark 8.3 It is possible to deÔ¨Åne B-splines even in the case of partially
coincident nodes, by suitably extending the deÔ¨Ånition of divided diÔ¨Äerences.
This leads to a new recursive form of Newton divided diÔ¨Äerences given by
(see for further details [Die93])
f[x0, . . . , xn] =
Ô£±
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£≥
f[x1, . . . , xn] ‚àíf[x0, . . . , xn‚àí1]
xn ‚àíx0
if x0 < x1 < . . . < xn
f (n+1)(x0)
(n + 1)!
if x0 = x1 = . . . = xn.
Assuming that m (with 1 < m < k +2) of the k +2 nodes xi, . . . , xi+k+1
are coincident and equal to Œª, then (8.46) will contain a linear combination
of the functions (Œª ‚àíx)k+1‚àíj
+
, for j = 1, . . . , m. As a consequence, the
B-spline can have continuous derivatives at Œª only up to order k ‚àím and,
therefore, it is discontinuous if m = k + 1. It can be checked [Die93] that,
if xi‚àí1 < xi = . . . = xi+k < xi+k+1, then
Bi,k+1(x) =
Ô£±
Ô£¥
Ô£≤
Ô£¥
Ô£≥
 xi+k+1 ‚àíx
xi+k+1 ‚àíxi
k
if x ‚àà[xi, xi+k+1],
0
otherwise,

8.6 Approximation by Splines
355
while for xi < xi+1 = . . . = xi+k+1 < xi+k+2
Bi,k+1(x) =
Ô£±
Ô£¥
Ô£≤
Ô£¥
Ô£≥

x ‚àíxi
xi+k+1 ‚àíxi
k
if x ‚àà[xi, xi+k+1],
0
otherwise.
Combining these formulae with the recursive relation (8.54) allows for con-
structing B-splines with coincident nodes.
‚ñ†
Example 8.9 Let us examine the special case of cubic B-splines on equally
spaced nodes xi+1 = xi + h for i = 0, ..., n ‚àí1. Equation (8.53) becomes
6h3Bi,4(x) =
Ô£±
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≥
(x ‚àíxi)3,
if x ‚àà[xi, xi+1],
h3 + 3h2(x ‚àíxi+1) + 3h(x ‚àíxi+1)2 ‚àí3(x ‚àíxi+1)3, if x ‚àà[xi+1, xi+2],
h3 + 3h2(xi+3 ‚àíx) + 3h(xi+3 ‚àíx)2 ‚àí3(xi+3 ‚àíx)3, if x ‚àà[xi+2, xi+3],
(xi+4 ‚àíx)3,
if x ‚àà[xi+3, xi+4],
0
otherwise.
In Figure 8.11 the graph of Bi,4 is shown in the case of distinct nodes and of
partially coincident nodes.
‚Ä¢
‚àí2
‚àí1
0
1
2
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
FIGURE 8.11. B-spline with distinct nodes (in solid line) and with three coin-
cident nodes at the origin (in dashed line). Notice the discontinuity of the Ô¨Årst
derivative
Given n + 1 distinct nodes xj, j = 0, . . . , n, n ‚àík linearly independent
B-splines of degree k can be constructed, though 2k degrees of freedom are

356
8. Polynomial Interpolation
still available to generate a basis for Sk. One way of proceeding consists of
introducing 2k Ô¨Åctitious nodes
x‚àík ‚â§x‚àík+1 ‚â§. . . ‚â§x‚àí1 ‚â§x0 = a,
b = xn ‚â§xn+1 ‚â§. . . ‚â§xn+k
(8.55)
which the B-splines Bi,k+1, with i = ‚àík, . . . , ‚àí1 and i = n ‚àík, . . . , n ‚àí1,
are associated with. By doing so, any spline sk ‚ààSk can be uniquely written
as
sk(x) =
n‚àí1

i=‚àík
ciBi,k+1(x).
(8.56)
The real numbers ci are the B-spline coeÔ¨Écients of sk. Nodes (8.55) are
usually chosen as coincident or periodic.
1. Coincident: this choice is suitable for enforcing the values attained
by a spline at the end points of its deÔ¨Ånition interval. In such a case,
indeed, thanks to Remark 8.3 about B-splines with coincident nodes,
we get
sk(a) = c‚àík,
sk(b) = cn‚àí1.
(8.57)
2. Periodic, that is
x‚àíi = xn‚àíi ‚àíb + a,
xi+n = xi + b ‚àía,
i = 1, . . . , k.
This choice is useful if the periodicity conditions (8.44) have to be
imposed.
Remark 8.4 (Inserting nodes) Using B-splines instead of cardinal spli-
nes is advantageous when handling, with a reduced computational eÔ¨Äort, a
given conÔ¨Åguration of nodes for which a spline sk is known. In particular,
assume that the coeÔ¨Écients ci of sk (in form (8.56)) are available over the
nodes x‚àík, x‚àík+1, . . . , xn+k, and that we wish to add to these a new node
$x.
The spline $sk ‚ààSk, deÔ¨Åned over the new set of nodes, admits the follow-
ing representation with respect to a new B-spline basis
2
ÀúBi,k+1
3
$sk(x) =
n‚àí1

i=‚àík
di $Bi,k+1(x).
The new coeÔ¨Écients di can be computed starting from the known coeÔ¨É-
cients ci using the following algorithm [Boe80]:

8.7 Splines in Parametric Form
357
let $x ‚àà[xj, xj+1); then, construct a new set of nodes {yi} such that
yi = xi for i = ‚àík, . . . , j,
yj+1 = $x,
yi = xi‚àí1 for i = j + 2, . . . , n + k + 1;
deÔ¨Åne
œâi =
Ô£±
Ô£¥
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£¥
Ô£≥
1
for i = ‚àík, . . . , j ‚àík,
yj+1 ‚àíyi
yi+k+1 ‚àíyi
for i = j ‚àík + 1, . . . , j,
0
for i = j + 1, . . . , n;
compute
di = œâici + (1 ‚àíœâi)ci
i = ‚àík, ..., n ‚àí1.
This algorithm has good stability properties and can be generalized to the
case where more than one node is inserted at the same time (see [Die93]).
‚ñ†
8.7
Splines in Parametric Form
Using interpolating splines presents the following two drawbacks:
1. the resulting approximation is of good quality only if the function
f does not exhibit large derivatives (in particular, we require that
|f ‚Ä≤(x)| < 1 for every x). Otherwise, oscillating behaviors may arise
in the spline, as demonstrated by the example considered in Figure
8.12 which shows, in solid line, the cubic interpolating spline over the
following set of data (from [SL89])
xi
8.125
8.4
9
9.845
9.6
9.959
10.166
10.2
fi
0.0774
0.099
0.28
0.6
0.708
1.3
1.8
2.177
2. sk depends on the choice of the coordinate system. In fact, performing
a clockwise rotation of 36 degrees of the coordinate system in the
above example, would lead to the spline without spurious oscillations
reported in the boxed frame in Figure 8.12.
All the interpolation procedures considered so far depend on the cho-
sen Cartesian reference system, which is a negative feature if the
spline is used for a graphical representation of a given Ô¨Ågure (for in-
stance, an ellipse). Indeed, we would like such a representation to
be independent of the reference system, that is, to have a geometric
invariance property.

358
8. Polynomial Interpolation
8
8.5
9
9.5
10
10.5
0
0.5
1
1.5
2
2.5
6
7
8
9
10
‚àí5.2
‚àí5
‚àí4.8
‚àí4.6
‚àí4.4
‚àí4.2
FIGURE 8.12. Geometric noninvariance for an interpolating cubic spline s3: the
set of data for s3 in the boxed frame is the same as in the main Ô¨Ågure, rotated
by 36 degrees. The rotation diminishes the slope of the interpolated curve and
eliminates any oscillation from s3. Notice that resorting to a parametric spline
(dashed line) removes the oscillations in s3 without any rotation of the reference
system
A solution is provided by parametric splines, in which any component of the
curve, written in parametric form, is approximated by a spline function.
Consider a plane curve in parametric form P(t) = (x(t), y(t)), with t ‚àà
[0, T], then take the set of the points in the plane of coordinates Pi =
(xi, yi), for i = 0, . . . , n, and introduce a partition onto [0, T]: 0 = t0 <
t1 < . . . < tn = T.
Using the two sets of values {ti, xi} and {ti, yi} as interpolation data,
we obtain the two splines sk,x and sk,y, with respect to the independent
variable t, that interpolate x(t) and y(t), respectively. The parametric curve
Sk(t) = (sk,x(t), sk,y(t)) is called the parametric spline. Obviously, diÔ¨Äerent
parameterizations of the interval [0, T] yield diÔ¨Äerent splines (see Figure
8.13).
A reasonable choice of the parameterization makes use of the length of
each segment Pi‚àí1Pi,
li =

(xi ‚àíxi‚àí1)2 + (yi ‚àíyi‚àí1)2,
i = 1, . . . , n.
Setting t0 = 0 and ti = i
k=1 lk for i = 1, . . . , n, every ti represents the
cumulative length of the piecewise line that joins the points from P0 to
Pi. This function is called the cumulative length spline and approximates
satisfactorily even those curves with large curvature. Moreover, it can also
be proved (see [SL89]) that it is geometrically invariant.
Program 68 implements the construction of cumulative parametric cu-
bic splines in two dimensions (it can be easily generalized to the three-

8.7 Splines in Parametric Form
359
‚àí2
0
2
4
6
‚àí4
‚àí2
0
2
4
FIGURE 8.13. Parametric splines for a spiral-like node distribution. The spline
of cumulative length is drawn in solid line
dimensional case). Composite parametric splines can be generated as well
by enforcing suitable continuity conditions (see [SL89]).
Program 68 - par spline : Parametric splines
function [xi,yi] = par spline (x, y)
t (1) = 0;
for i = 1:length (x)-1
t (i+1) = t (i) + sqrt ( (x(i+1)-x(i))ÀÜ2 + (y(i+1)-y(i))ÀÜ2 );
end
z = [t(1):(t(length(t))-t(1))/100:t(length(t))];
xi = spline (t,x,z);
yi = spline (t,y,z);
8.7.1
B¬¥ezier Curves and Parametric B-splines
The B¬¥ezier curves and parametric B-splines are widely employed in graph-
ical applications, where the nodes‚Äô locations might be aÔ¨Äected by some
uncertainty.
Let P0, P1, . . . , Pn be n + 1 points ordered in the plane. The oriented
polygon formed by them is called the characteristic polygon or B¬¥ezier poly-
gon. Let us introduce the Bernstein polynomials over the interval [0, 1]
deÔ¨Åned as
bn,k(t) =

n
k

tk(1 ‚àít)n‚àík =
n!
k!(n ‚àík)!tk(1 ‚àít)n‚àík,

360
8. Polynomial Interpolation
for n = 0, 1, . . . and k = 0, . . . , n. They can be obtained by the following
recursive formula
"
bn,0(t) = (1 ‚àít)n
bn,k(t) = (1 ‚àít)bn‚àí1,k(t) + tbn‚àí1,k‚àí1(t),
k = 1, . . . , n, t ‚àà[0, 1].
It is easily seen that bn,k ‚ààPn, for k = 0, . . . , n. Also, {bn,k, k = 0, . . . , n}
provides a basis for Pn. The B¬¥ezier curve is deÔ¨Åned as follows
Bn(P0, P1, . . . , Pn, t) =
n

k=0
Pkbn,k(t),
0 ‚â§t ‚â§1.
(8.58)
This expression can be regarded as a weighted average of the points Pk,
with weights bn,k(t).
The B¬¥ezier curves can also be obtained by a pure geometric approach
starting from the characteristic polygon. Indeed, for any Ô¨Åxed t ‚àà[0, 1],
we deÔ¨Åne Pi,1(t) = (1 ‚àít)Pi + tPi+1 for i = 0, . . . , n ‚àí1 and, for t
Ô¨Åxed, the piecewise line that joins the new nodes Pi,1(t) forms a polygon
of n ‚àí1 edges. We can now repeat the procedure by generating the new
vertices Pi,2(t) (i = 0, . . . , n ‚àí2), and terminating as soon as the polygon
comprises only the vertices P0,n‚àí1(t) and P1,n‚àí1(t). It can be shown that
P0,n(t) = (1 ‚àít)P0,n‚àí1(t) + tP1,n‚àí1(t) = Bn(P0, P1, . . . , Pn, t),
that is, P0,n(t) is equal to the value of the B¬¥ezier curve Bn at the points
corresponding to the Ô¨Åxed value of t. Repeating the process for several val-
ues of the parameter t yields the construction of the curve in the considered
region of the plane.
2
4
6
8
10
12
14
16
‚àí4
‚àí2
0
2
4
6
8
FIGURE 8.14. Computation of the value of B3 relative to the points (0,0), (4,7),
(14,7), (17,0) for t = 0.5, using the graphical method described in the text
Notice that, for a given node conÔ¨Åguration, several curves can be con-
structed according to the ordering of points Pi. Moreover, the B¬¥ezier curve

8.7 Splines in Parametric Form
361
Bn(P0, P1, . . . , Pn, t) coincides with Bn(Pn, Pn‚àí1, . . . , P0, t), apart from
the orientation.
Program 69 computes bn,k at the point x for x ‚àà[0, 1].
Program 69 - bernstein : Bernstein polynomials
function [bnk]=bernstein (n,k,x)
if k == 0,
C = 1;
else,
C = prod ([1:n])/( prod([1:k])*prod([1:n-k]));
end
bnk = C * xÀÜk * (1-x)ÀÜ(n-k);
Program 70 plots the B¬¥ezier curve relative to the set of points (x, y).
Program 70 - bezier : B¬¥ezier curves
function [bezx,bezy] = bezier (x, y, n)
i = 0; k = 0;
for t = 0:0.01:1,
i = i + 1; bnk = bernstein (n,k,t); ber(i) = bnk;
end
bezx = ber * x (1); bezy = ber * y (1);
for k = 1:n
i = 0;
for t = 0:0.01:1
i = i + 1; bnk = bernstein (n,k,t); ber(i) = bnk;
end
bezx = bezx + ber * x (k+1); bezy = bezy + ber * y (k+1);
end
plot(bezx,bezy)
In practice, the B¬¥ezier curves are rarely used since they do not provide a
suÔ¨Éciently accurate approximation to the characteristic polygon. For this
reason, in the 70‚Äôs the parametric B-splines were introduced, and they are
used in (8.58) instead of the Bernstein polynomials. Parametric B-splines
are widely employed in packages for computer graphics since they enjoy
the following properties:
1. perturbing a single vertex of the characteristic polygon yields a local
perturbation of the curve only around the vertex itself;
2. the parametric B-spline better approximates the control polygon than
the corresponding B¬¥ezier curve does, and it is always contained within
the convex hull of the polygon.

362
8. Polynomial Interpolation
In Figure 8.15 a comparison is made between B¬¥ezier curves and para-
metric B-splines for the approximation of a given characteristic polygon.
FIGURE 8.15. Comparison of a B¬¥ezier curve (left) and a parametric B-spline
(right). The vertices of the characteristic polygon are denoted by √ó
We conclude this section by noticing that parametric cubic B-splines
allow for obtaining locally straight lines by aligning four consecutive ver-
tices (see Figure 8.16) and that a parametric B-spline can be constrained
at a speciÔ¨Åc point of the characteristic polygon by simply making three
consecutive points of the polygon coincide with the desired point.
FIGURE 8.16. Some parametric B-splines as functions of the number and posi-
tions of the vertices of the characteristic polygon. Notice in the last Ô¨Ågure (right)
the localization eÔ¨Äects due to moving a single vertex
8.8
Applications
In this section we consider two problems arising from the solution of fourth-
order diÔ¨Äerential equations and from the reconstruction of images in axial
tomographies.

8.8 Applications
363
8.8.1
Finite Element Analysis of a Clamped Beam
Let us employ piecewise Hermite polynomials (see Section 8.4) for the nu-
merical approximation of the transversal bending of a clamped beam. This
problem was already considered in Section 4.7.2 where centered Ô¨Ånite dif-
ferences were used.
The mathematical model is the fourth-order boundary value problem
(4.74), here presented in the following general formulation
"
(Œ±(x)u‚Ä≤‚Ä≤(x))‚Ä≤‚Ä≤ = f(x),
0 < x < L
u(0) = u(L) = 0,
u‚Ä≤(0) = u‚Ä≤(L) = 0.
(8.59)
In the particular case of (4.74) we have Œ± = EJ and f = P; we assume
henceforth that Œ± is a positive and bounded function over (0, L) and that
f ‚ààL2(0, L).
We multiply (8.59) by a suÔ¨Éciently smooth arbitrary function v, then,
we integrate by parts twice, to obtain
L
>
0
Œ±u‚Ä≤‚Ä≤v‚Ä≤‚Ä≤dx ‚àí[Œ±u‚Ä≤‚Ä≤‚Ä≤v]L
0 + [Œ±u‚Ä≤‚Ä≤v‚Ä≤]L
0 =
L
>
0
fvdx.
Problem (8.59) is then replaced by the following problem in integral form
Ô¨Ånd u ‚ààV such that
L
>
0
Œ±u‚Ä≤‚Ä≤v‚Ä≤‚Ä≤dx =
L
>
0
fvdx,
‚àÄv ‚ààV,
(8.60)
where
V =
2
v : v(k) ‚ààL2(0, L), k = 0, 1, 2, v(k)(0) = v(k)(L) = 0, k = 0, 1
3
.
Problem (8.60) admits a unique solution, which represents the deformed
conÔ¨Åguration that minimizes the total potential energy of the beam over
the space V (see, for instance, [Red86], p. 156)
J(u) =
L
>
0
1
2Œ±(u‚Ä≤‚Ä≤)2 ‚àífu

dx.
In view of the numerical solution of problem (8.60), we introduce a partition
Th of [0, L] into K subintervals Tk = [xk‚àí1, xk], (k = 1, . . . , K) of uniform
length h = L/K, with xk = kh, and the Ô¨Ånite dimensional space
Vh =

vh ‚ààC1([0, L]), vh|T ‚ààP3(T)
‚àÄT ‚ààTh, v(k)
h (0) = v(k)
h (L) = 0, k = 0, 1
3
.
(8.61)

364
8. Polynomial Interpolation
Let us equip Vh with a basis. For this purpose, we associate with each
internal node xi (i = 1, . . . , K ‚àí1) a support œÉi = Ti ‚à™Ti+1 and two
functions œïi, œài deÔ¨Åned as follows: for any k, œïi|Tk ‚ààP3(Tk), œài|Tk ‚ààP3(Tk)
and for any j = 0, . . . , K,
Ô£±
Ô£≤
Ô£≥
œïi(xj) = Œ¥ij,
œï‚Ä≤
i(xj) = 0,
œài(xj) = 0,
œà‚Ä≤
i(xj) = Œ¥ij.
(8.62)
Notice that the above functions belong to Vh and deÔ¨Åne a basis
Bh = {œïi, œài, i = 1, . . . , K ‚àí1}.
(8.63)
These basis functions can be brought back to the reference interval ÀÜT =
[0, 1] for 0 ‚â§ÀÜx ‚â§1, by the aÔ¨Éne maps x = hÀÜx + xk‚àí1 between ÀÜT and Tk,
for k = 1, . . . , K.
Therefore, let us introduce on the interval ÀÜT the basis functions ÀÜœï(0)
0
and ÀÜœï(1)
0 , associated with the node ÀÜx = 0, and ÀÜœï(0)
1
and ÀÜœï(1)
1 , associated
with node ÀÜx = 1. Each of these is of the form ÀÜœï = a0 + a1ÀÜx + a2ÀÜx2 +
a3ÀÜx3; in particular, the functions with superscript ‚Äú0‚Äù must satisfy the
Ô¨Årst two conditions in (8.62), while those with superscript ‚Äú1‚Äù must fulÔ¨Åll
the remaining two conditions. Solving the (4√ó4) associated system, we get
ÀÜœï(0)
0 (ÀÜx) = 1 ‚àí3ÀÜx2 + 2ÀÜx3,
ÀÜœï(1)
0 (ÀÜx) = ÀÜx ‚àí2ÀÜx2 + ÀÜx3,
ÀÜœï(0)
1 (ÀÜx) = 3ÀÜx2 ‚àí2ÀÜx3,
ÀÜœï(1)
1 (ÀÜx) = ‚àíÀÜx2 + ÀÜx3.
(8.64)
The graphs of the functions (8.64) are drawn in Figure 8.17 (left), where
(0), (1), (2) and (3) denote ÀÜœï(0)
0 , ÀÜœï(0)
1 , ÀÜœï(1)
0
and ÀÜœï(1)
1 , respectively.
The function uh ‚ààVh can be written as
uh(x) =
K‚àí1

i=1
uiœïi(x) +
K‚àí1

i=1
u(1)
i œài(x).
(8.65)
The coeÔ¨Écients and the degrees of freedom of uh have the following mean-
ing: ui = uh(xi), u(1)
i (xi) = u‚Ä≤
h(xi) for i = 1, . . . , K ‚àí1. Notice that (8.65)
is a special instance of (8.32), having set mi = 1.
The discretization of problem (8.60) reads
Ô¨Ånd uh ‚ààVh such that
L
>
0
Œ±u‚Ä≤‚Ä≤
hv‚Ä≤‚Ä≤
hdx =
L
>
0
fvhdx,
‚àÄvh ‚ààBh.
(8.66)
This is called the Galerkin Ô¨Ånite element approximation of the diÔ¨Äerential
problem (8.59). We refer to Chapter 12, Sections 12.4 and 12.4.5, for a
more comprehensive discussion and analysis of the method.

8.8 Applications
365
0
0.2
0.4
0.6
0.8
1
‚àí0.2
0
0.2
0.4
0.6
0.8
1
(0)
(1)
(2)
(3)
0
20
40
60
80
100
120
140
160
10
‚àí20
10
‚àí15
10
‚àí10
10
‚àí5
10
0
10
5
No Prec.
Prec.
FIGURE 8.17. Canonical Hermite basis on the reference interval 0 ‚â§ÀÜx ‚â§1 (left);
convergence histories for the conjugate gradient method in the solution of system
(8.69) (right). On the x-axis the number of iterations k is shown, while the y-axis
represents the quantity ‚à•r(k)‚à•2/‚à•b1‚à•2, where r is the residual of system (8.69)
Using the representation (8.65) we end up with the following system in
the 2K ‚àí2 unknowns u1, u2, . . . , uK‚àí1, u(1)
1 , u(1)
2 , . . . u(1)
K‚àí1
Ô£±
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≥
K‚àí1

j=1
Ô£±
Ô£≤
Ô£≥uj
L
>
0
Œ±œï‚Ä≤‚Ä≤
j œï‚Ä≤‚Ä≤
i dx + u(1)
j
L
>
0
Œ±œà‚Ä≤‚Ä≤
j œï‚Ä≤‚Ä≤
i dx
Ô£º
Ô£Ω
Ô£æ=
L
>
0
fœïidx,
K‚àí1

j=1
Ô£±
Ô£≤
Ô£≥uj
L
>
0
Œ±œï‚Ä≤‚Ä≤
j œà‚Ä≤‚Ä≤
i dx + u(1)
j
L
>
0
Œ±œà‚Ä≤‚Ä≤
j œà‚Ä≤‚Ä≤
i dx
Ô£º
Ô£Ω
Ô£æ=
L
>
0
fœàidx,
(8.67)
for i = 1, . . . , K‚àí1. Assuming, for the sake of simplicity, that the beam has
unit length L, that Œ± and f are two constants and computing the integrals
in (8.67), the Ô¨Ånal system reads in matrix form
"
Au + Bp =
b1
BT u + Cp =
0,
(8.68)
where the vectors u, p ‚ààRK‚àí1 contain the nodal unknowns ui and u(1)
i ,
b1 ‚ààRK‚àí1 is the vector of components equal to h4f/Œ±, while
A = tridiagK‚àí1(‚àí12, 24, ‚àí12),
B = tridiagK‚àí1(‚àí6, 0, 6),
C = tridiagK‚àí1(2, 8, 2).
System (8.68) has size equal to 2(K ‚àí1); eliminating the unknown p from
the second equation, we get the reduced system (of size K ‚àí1)

A ‚àíBC‚àí1BT 
u = b1.
(8.69)

366
8. Polynomial Interpolation
Since B is skew-symmetric and A is symmetric and positive deÔ¨Ånite (s.p.d.),
the matrix M = A‚àíBC‚àí1BT is s.p.d. too. Using Cholesky factorization for
solving system (8.69) is impractical as C‚àí1 is full. An alternative is thus the
conjugate gradient method (CG) supplied with a suitable preconditioner
as the spectral condition number of M is of the order of h‚àí4 = K4.
We notice that computing the residual at each step k ‚â•0 requires solv-
ing a linear system whose right side is the vector BT u(k), u(k) being the
current iterate of CG method, and whose coeÔ¨Écient matrix is matrix C.
This system can be solved using the Thomas algorithm (3.53) with a cost
of the order of K Ô¨Çops.
The CG algorithm terminates in correspondence to the lowest value of k
for which ‚à•r(k)‚à•2 ‚â§u‚à•b1‚à•2, where r(k) is the residual of system (8.69) and
u is the roundoÔ¨Äunit.
The results obtained running the CG method in the case of a uniform
partition of [0, 1] with K = 50 elements and setting Œ± = f = 1 are sum-
marized in Figure 8.17 (right), which shows the convergence histories of
the method in both nonpreconditioned form (denoted by ‚ÄúNon Prec.‚Äù) and
with SSOR preconditioner (denoted by ‚ÄúPrec.‚Äù), having set the relaxation
parameter œâ = 1.95.
We notice that the CG method does not converge within K ‚àí1 steps
due to the eÔ¨Äect of the rounding errors. Notice also the eÔ¨Äectiveness of the
SSOR preconditioner in terms of the reduction of the number of iterations.
However, the high computational cost of this preconditioner prompts us to
devise another choice. Looking at the structure of the matrix M a natural
preconditioner is M = A‚àíB$C‚àí1BT , where $C is the diagonal matrix whose
entries are $cii = K‚àí1
j=1 |cij|. The matrix M is banded so that its inversion
requires a strongly reduced cost than for the SSOR preconditioner. More-
over, as shown in Table 8.6, using M provides a dramatic decrease of the
number of iterations to converge.
K
Without Precond.
SSOR
M
25
51
27
12
50
178
61
25
100
685
118
33
200
2849
237
34
TABLE 8.6. Number of iterations as a function of K
8.8.2
Geometric Reconstruction Based on Computer
Tomographies
A typical application of the algorithms presented in Section 8.7 deals with
the reconstruction of the three-dimensional structure of internal organs of
human body based on computer tomographies (CT).

8.8 Applications
367
FIGURE 8.18. Cross-section of a blood vessel (left) and an associated character-
istic polygon using 16 points Pi (right)
The CT usually provides a sequence of images which represent the sections
of an organ at several horizontal planes; as a convention, we say that the
CT produces sections of the x, y plane in correspondance of several values of
z. The result is analogous to what we would get by sectioning the organ at
diÔ¨Äerent values of z and taking the picture of the corresponding sections.
Obviously, the great advantage in using the CT is that the organ under
investigation can be visualized without being hidden by the neighboring
ones, as happens in other kinds of medical images, e.g., angiographies.
The image that is obtained for each section is coded into a matrix of
pixels (abbreviation of pictures elements) in the x, y plane; a certain value
is associated with each pixel expressing the level of grey of the image at
that point. This level is determined by the density of X rays which are
collected by a detector after passing through the human body. In practice,
the information contained in a CT at a given value of z is expressed by a
set of points (xi, yi) which identify the boundary of the organ at z.
To improve the diagnostics it is often useful to reconstruct the three-
dimensional structure of the organ under examination starting from the
sections provided by the CT. With this aim, it is necessary to convert the
information coded by pixels into a parametric representation which can be
expressed by suitable functions interpolating the image at some signiÔ¨Åcant
points on its boundary. This reconstruction can be carried out by using the
methods described in Section 8.7 as shown in Figure 8.19.
A set of curves like those shown in Figure 8.19 can be suitably stacked to
provide an overall three-dimensional view of the organ under examination.

368
8. Polynomial Interpolation
(a)
(b)
(c)
FIGURE 8.19. Reconstruction of the internal vessel of Figure 8.18 using diÔ¨Äerent
interpolating splines with the same characteristic polygon: (a) B¬¥ezier curves, (b)
parametric splines and (c) parametric B-splines
8.9
Exercises
1. Prove that the characteristic polynomials li ‚ààPn deÔ¨Åned in (8.3) form a
basis for Pn.
2. An alternative approach to the method in Theorem 8.1, for constructing
the interpolating polynomial, consists of directly enforcing the n + 1 in-
terpolation constraints on Œ†n and then computing the coeÔ¨Écients ai. By
doing so, we end up with a linear system Xa= y, with a = (a0, . . . , an)T ,
y = (y0, . . . , yn)T and X = [xj
i]. X is called Vandermonde matrix. Prove
that X is nonsingular if the nodes xi are distinct.
[Hint: show that det(X)=

0‚â§j<i‚â§n
(xi ‚àíxj) by recursion on n.]
3. Prove that œâ‚Ä≤
n+1(xi) =
n

j=0
jÃ∏=i
(xi ‚àíxj) where œân+1 is the nodal polynomial
(8.6). Then, check (8.5).
4. Provide an estimate of ‚à•œân+1‚à•‚àû, in the cases n = 1 and n = 2, for a
distribution of equally spaced nodes.
5. Prove that
(n ‚àí1)!hn‚àí1|(x ‚àíxn‚àí1)(x ‚àíxn)| ‚â§|œân+1(x)| ‚â§n!hn‚àí1|(x ‚àíxn‚àí1)(x ‚àíxn)|,
where n is even, ‚àí1 = x0 < x1 < . . . < xn‚àí1 < xn = 1, x ‚àà(xn‚àí1, xn) and
h = 2/n.

8.9 Exercises
369
[Hint : let N = n/2 and show Ô¨Årst that
œân+1(x) = (x + Nh)(x + (N ‚àí1)h) . . . (x + h)x
(x ‚àíh) . . . (x ‚àí(N ‚àí1)h)(x ‚àíNh).
(8.70)
Then, take x = rh with N ‚àí1 < r < N.]
6. Under the assumptions of Exercise 5, show that |œân+1| is maximum if
x ‚àà(xn‚àí1, xn) (notice that |œân+1| is an even function).
[Hint : use (8.70) to prove that |œân+1(x + h)/œân+1(x)| > 1 for any x ‚àà
(0, xn‚àí1) with x not coinciding with any interpolation node.]
7. Prove the recursive relation (8.19) for Newton divided diÔ¨Äerences.
8. Determine an interpolating polynomial Hf ‚ààPn such that
(Hf)(k)(x0) = f (k)(x0),
k = 0, . . . , n,
and check that
Hf(x) =
n

j=0
f (j)(x0)
j!
(x ‚àíx0)j,
that is, the Hermite interpolating polynomial on one node coincides with
the Taylor polynomial.
9. Given the following set of data

f0 = f(‚àí1) = 1, f1 = f ‚Ä≤(‚àí1) = 1, f2 = f ‚Ä≤(1) = 2, f3 = f(2) = 1

,
prove that the Hermite-BirkoÔ¨Äinterpolating polynomial H3 does not exist
for them.
[Solution : letting H3(x) = a3x3 + a2x2 + a1x + a0, one must check that
the matrix of the linear system H3(xi) = fi for i = 0, . . . , 3 is singular.]
10. Check that any sk ‚ààSk[a, b] admits a representation of the form
sk(x) =
k

i=0
bixi +
g

i=1
ci(x ‚àíxi)k
+,
that is, 1, x, x2, . . . , xk, (x ‚àíx1)k
+, . . . , (x ‚àíxg)k
+ form a basis for Sk[a, b].
11. Prove Property 8.2 and check its validity even in the case where the spline
s satisÔ¨Åes conditions of the form s‚Ä≤(a) = f ‚Ä≤(a), s‚Ä≤(b) = f ‚Ä≤(b).
[Hint: start from
b
>
a
0
f ‚Ä≤‚Ä≤(x) ‚àís‚Ä≤‚Ä≤(x)
1
s‚Ä≤‚Ä≤(x)dx =
n

i=1
xi
>
xi‚àí1
0
f ‚Ä≤‚Ä≤(x) ‚àís‚Ä≤‚Ä≤(x)
1
s‚Ä≤‚Ä≤dx
and integrate by parts twice.]

370
8. Polynomial Interpolation
12. Let f(x) = cos(x) = 1 ‚àíx2
2! + x4
4! ‚àíx6
6! + . . . ; then, consider the following
rational approximation
r(x) = a0 + a2x2 + a4x4
1 + b2x2
,
(8.71)
called the Pad¬¥e approximation. Determine the coeÔ¨Écients of r in such a
way that
f(x) ‚àír(x) = Œ≥8x8 + Œ≥10x10 + . . .
[Solution: a0 = 1, a2 = ‚àí7/15, a4 = 1/40, b2 = 1/30.]
13. Assume that the function f of the previous exercise is known at a set of n
equally spaced points xi ‚àà(‚àíœÄ/2, œÄ/2) with i = 0, . . . , n. Repeat Exercise
12, determining, by using MATLAB, the coeÔ¨Écients of r in such a way
that the quantity n
i=0 |f(xi) ‚àír(xi)|2 is minimized. Consider the cases
n = 5 and n = 10.

9
Numerical Integration
In this chapter we present the most commonly used methods for numer-
ical integration. We will mainly consider one-dimensional integrals over
bounded intervals, although in Sections 9.8 and 9.9 an extension of the tech-
niques to integration over unbounded intervals (or integration of functions
with singularities) and to the multidimensional case will be considered.
9.1
Quadrature Formulae
Let f be a real integrable function over the interval [a, b]. Computing ex-
plicitly the deÔ¨Ånite integral I(f) =
 b
a f(x)dx may be diÔ¨Écult or even
impossible. Any explicit formula that is suitable for providing an approxi-
mation of I(f) is said to be a quadrature formula or numerical integration
formula.
An example can be obtained by replacing f with an approximation fn,
depending on the integer n ‚â•0, then computing I(fn) instead of I(f).
Letting In(f) = I(fn), we have
In(f) =
b
>
a
fn(x)dx,
n ‚â•0.
(9.1)
The dependence on the end points a, b is always understood, so we write
In(f) instead of In(f; a, b).

372
9. Numerical Integration
If f ‚ààC0([a, b]), the quadrature error En(f) = I(f) ‚àíIn(f) satisÔ¨Åes
|En(f)| ‚â§
b
>
a
|f(x) ‚àífn(x)|dx ‚â§(b ‚àía)‚à•f ‚àífn‚à•‚àû.
Therefore, if for some n, ‚à•f ‚àífn‚à•‚àû< Œµ, then |En(f)| ‚â§Œµ(b ‚àía).
The approximant fn must be easily integrable, which is the case if, for
example, fn ‚ààPn. In this respect, a natural approach consists of using
fn = Œ†nf, the interpolating Lagrange polynomial of f over a set of n + 1
distinct nodes {xi}, with i = 0, . . . , n. By doing so, from (9.1) it follows
that
In(f) =
n

i=0
f(xi)
b
>
a
li(x)dx,
(9.2)
where li is the characteristic Lagrange polynomial of degree n associated
with node xi (see Section 8.1). We notice that (9.2) is a special instance of
the following quadrature formula
In(f) =
n

i=0
Œ±if(xi),
(9.3)
where the coeÔ¨Écients Œ±i of the linear combination are given by
 b
a li(x)dx.
Formula (9.3) is a weighted sum of the values of f at the points xi, for
i = 0, . . . , n. These points are said to be the nodes of the quadrature
formula, while the numbers Œ±i ‚ààR are its coeÔ¨Écients or weights. Both
weights and nodes depend in general on n; again, for notational simplicity,
this dependence is always understood.
Formula (9.2), called the Lagrange quadrature formula, can be generalized
to the case where also the values of the derivative of f are available. This
leads to the Hermite quadrature formula (see Section 9.5)
In(f) =
1

k=0
n

i=0
Œ±ikf (k)(xi)
(9.4)
where the weights are now denoted by Œ±ik.
Both (9.2) and (9.4) are interpolatory quadrature formulae, since the
function f has been replaced by its interpolating polynomial (Lagrange
and Hermite polynomials, respectively). We deÔ¨Åne the degree of exactness
of a quadrature formula as the maximum integer r ‚â•0 for which
In(f) = I(f),
‚àÄf ‚ààPr.
Any interpolatory quadrature formula that makes use of n + 1 distinct
nodes has degree of exactness equal to at least n. Indeed, if f ‚ààPn, then

9.2 Interpolatory Quadratures
373
Œ†nf = f and thus In(Œ†nf) = I(Œ†nf). The converse statement is also true,
that is, a quadrature formula using n + 1 distinct nodes and having degree
of exactness equal at least to n is necessarily of interpolatory type (for the
proof see [IK66], p. 316).
As we will see in Section 10.2, the degree of exactness of a Lagrange
quadrature formula can be as large as 2n + 1 in the case of the so-called
Gaussian quadrature formulae.
9.2
Interpolatory Quadratures
We consider three remarkable instances of formula (9.2), corresponding to
n = 0, 1 and 2.
9.2.1
The Midpoint or Rectangle Formula
This formula is obtained by replacing f over [a, b] with the constant function
equal to the value attained by f at the midpoint of [a, b] (see Figure 9.1,
left). This yields
I0(f) = (b ‚àía)f
a + b
2

(9.5)
with weight Œ±0 = b ‚àía and node x0 = (a + b)/2. If f ‚ààC2([a, b]), the
quadrature error is
E0(f) = h3
3 f ‚Ä≤‚Ä≤(Œæ),
h = b ‚àía
2
,
(9.6)
where Œæ lies within the interval (a, b).
b
a
f(x)
x0
x
f(x)
xm‚àí1
xk
x
x0
FIGURE 9.1. The midpoint formula (left); the composite midpoint formula
(right)
Indeed, expanding f in a Taylor‚Äôs series around c = (a + b)/2 and trun-
cating at the second-order, we get
f(x) = f(c) + f ‚Ä≤(c)(x ‚àíc) + f ‚Ä≤‚Ä≤(Œ∑(x))(x ‚àíc)2/2,

374
9. Numerical Integration
from which, integrating on (a, b) and using the mean-value theorem, (9.6)
follows. From this, it turns out that (9.5) is exact for constant and aÔ¨Éne
functions (since in both cases f ‚Ä≤‚Ä≤(Œæ) = 0 for any Œæ ‚àà(a, b)), so that the
midpoint rule has degree of exactness equal to 1.
It is worth noting that if the width of the integration interval [a, b] is
not suÔ¨Éciently small, the quadrature error (9.6) can be quite large. This
drawback is common to all the numerical integration formulae that will
be described in the three forthcoming sections and can be overcome by
resorting to their composite counterparts as discussed in Section 9.4.
Suppose now that we approximate the integral I(f) by replacing f over
[a, b] with its composite interpolating polynomial of degree zero, constructed
on m subintervals of width H = (b ‚àía)/m, for m ‚â•1 (see Figure 9.1,
right). Introducing the quadrature nodes xk = a + (2k + 1)H/2, for k =
0, . . . , m ‚àí1, we get the composite midpoint formula
I0,m(f) = H
m‚àí1

k=0
f(xk),
m ‚â•1.
(9.7)
The quadrature error E0,m(f) = I(f) ‚àíI0,m(f) is given by
E0,m(f) = b ‚àía
24 H2f ‚Ä≤‚Ä≤(Œæ),
H = b ‚àía
m
(9.8)
provided that f ‚ààC2([a, b]) and where Œæ ‚àà(a, b). From (9.8) we conclude
that (9.7) has degree of exactness equal to 1; (9.8) can be proved by recalling
(9.6) and using the additivity of integrals. Indeed, for k = 0, . . . , m‚àí1 and
Œæk ‚àà(a + kH, a + (k + 1)H),
E0,m(f) =
m‚àí1

k=0
f ‚Ä≤‚Ä≤(Œæk)(H/2)3/3 =
m‚àí1

k=0
f ‚Ä≤‚Ä≤(Œæk)H2
24
b ‚àía
m
= b ‚àía
24 H2f ‚Ä≤‚Ä≤(Œæ).
The last equality is a consequence of the following theorem, that is applied
letting u = f ‚Ä≤‚Ä≤ and Œ¥j = 1 for j = 0, . . . , m ‚àí1.
Theorem 9.1 (discrete mean-value theorem) Let u ‚ààC0([a, b]) and
let xj be s + 1 points in [a, b] and Œ¥j be s + 1 constants, all having the same
sign. Then there exists Œ∑ ‚àà[a, b] such that
s

j=0
Œ¥ju(xj) = u(Œ∑)
s

j=0
Œ¥j.
(9.9)
Proof. Let um = minx‚àà[a,b] u(x) = u(¬Øx) and uM = maxx‚àà[a,b] u(x) = u(¬Ø¬Øx),
where ¬Øx and ¬Ø¬Øx are two points in (a, b). Then
um
s

j=0
Œ¥j ‚â§
s

j=0
Œ¥ju(xj) ‚â§uM
s

j=0
Œ¥j.
(9.10)

9.2 Interpolatory Quadratures
375
Let œÉs = s
j=0 Œ¥ju(xj) and consider the continuous function U(x) = u(x) s
j=0 Œ¥j.
Thanks to (9.10), U(¬Øx) ‚â§œÉs ‚â§U(¬Ø¬Øx). Applying the mean-value theorem, there
exists a point Œ∑ between a and b such that U(Œ∑) = œÉs, which is (9.9). A similar
proof can be carried out if the coeÔ¨Écients Œ¥j are negative.
3
The composite midpoint formula is implemented in Program 71. Through-
out this chapter, we shall denote by a and b the end points of the integration
interval and by m the number of quadrature subintervals. The variable fun
contains the expression of the function f, while the output variable int
contains the value of the approximate integral.
Program 71 - midpntc : Midpoint composite formula
function int = midpntc(a,b,m,fun)
h=(b-a)/m; x=[a+h/2:h:b]; dim = max(size(x)); y=eval(fun);
if size(y)==1, y=diag(ones(dim))*y; end; int=h*sum(y);
9.2.2
The Trapezoidal Formula
This formula is obtained by replacing f with Œ†1f, its Lagrange interpolat-
ing polynomial of degree 1, relative to the nodes x0 = a and x1 = b (see
Figure 9.2, left). The resulting quadrature, having nodes x0 = a, x1 = b
and weights Œ±0 = Œ±1 = (b ‚àía)/2, is
I1(f) = b ‚àía
2
[f(a) + f(b)] .
(9.11)
If f ‚ààC2([a, b]), the quadrature error is given by
E1(f) = ‚àíh3
12f ‚Ä≤‚Ä≤(Œæ),
h = b ‚àía
(9.12)
where Œæ is a point within the integration interval.
f(x)
a = x0
b = x1
x
b = x2
a+b
2
= x1
x
f(x)
a = x0
FIGURE 9.2. Trapezoidal formula (left) and Cavalieri-Simpson formula (right)

376
9. Numerical Integration
Indeed, from the expression of the interpolation error (8.7) one gets
E1(f) =
b
>
a
(f(x) ‚àíŒ†1f(x))dx = ‚àí1
2
b
>
a
f ‚Ä≤‚Ä≤(Œæ(x))(x ‚àía)(b ‚àíx)dx.
Since œâ2(x) = (x ‚àía)(x ‚àíb) < 0 in (a, b), the mean-value theorem yields
E1(f) = (1/2)f ‚Ä≤‚Ä≤(Œæ)
b
>
a
œâ2(x)dx = ‚àíf ‚Ä≤‚Ä≤(Œæ)(b ‚àía)3/12,
for some Œæ ‚àà(a, b), which is (9.12). The trapezoidal quadrature therefore
has degree of exactness equal to 1, as is the case with the midpoint rule.
To obtain the composite trapezoidal formula, we proceed as in the case
where n = 0, by replacing f over [a, b] with its composite Lagrange polyno-
mial of degree 1 on m subintervals, with m ‚â•1. Introduce the quadrature
nodes xk = a + kH, for k = 0, . . . , m and H = (b ‚àía)/m, getting
I1,m(f) = H
2
m‚àí1

k=0
(f(xk) + f(xk+1)) ,
m ‚â•1.
(9.13)
Each term in (9.13) is counted twice, except the Ô¨Årst and the last one, so
that the formula can be written as
I1,m(f) = H
1
2f(x0) + f(x1) + . . . + f(xm‚àí1) + 1
2f(xm)

.
(9.14)
As was done for (9.8), it can be shown that the quadrature error associated
with (9.14) is
E1,m(f) = ‚àíb ‚àía
12 H2f ‚Ä≤‚Ä≤(Œæ),
provided that f ‚ààC2([a, b]), where Œæ ‚àà(a, b). The degree of exactness is
again equal to 1.
The composite trapezoidal rule is implemented in Program 72.
Program 72 - trapezc : Composite trapezoidal formula
function int = trapezc(a,b,m,fun)
h=(b-a)/m; x=[a:h:b]; dim = max(size(x)); y=eval(fun);
if size(y)==1, y=diag(ones(dim))*y; end;
int=h*(0.5*y(1)+sum(y(2:m))+0.5*y(m+1));

9.2 Interpolatory Quadratures
377
9.2.3
The Cavalieri-Simpson Formula
The Cavalieri-Simpson formula can be obtained by replacing f over [a, b]
with its interpolating polynomial of degree 2 at the nodes x0 = a, x1 =
(a + b)/2 and x2 = b (see Figure 9.2, right). The weights are given by
Œ±0 = Œ±2 = (b ‚àía)/6 and Œ±1 = 4(b ‚àía)/6, and the resulting formula reads
I2(f) = b ‚àía
6

f(a) + 4f
a + b
2

+ f(b)

.
(9.15)
It can be shown that the quadrature error is
E2(f) = ‚àíh5
90f (4)(Œæ),
h = b ‚àía
2
(9.16)
provided that f ‚ààC4([a, b]), and where Œæ lies within (a, b). From (9.16) it
turns out that (9.15) has degree of exactness equal to 3.
Replacing f with its composite polynomial of degree 2 over [a, b] yields
the composite formula corresponding to (9.15). Introducing the quadrature
nodes xk = a + kH/2, for k = 0, . . . , 2m and letting H = (b ‚àía)/m, with
m ‚â•1 gives
I2,m = H
6
.
f(x0) + 2
m‚àí1

r=1
f(x2r) + 4
m‚àí1

s=0
f(x2s+1) + f(x2m)
/
.
(9.17)
The quadrature error associated with (9.17) is
E2,m(f) = ‚àíb ‚àía
180 (H/2)4f (4)(Œæ),
provided that f ‚ààC4([a, b]) and where Œæ ‚àà(a, b); the degree of exactness
of the formula is 3.
The composite Cavalieri-Simpson quadrature is implemented in Program
73.
Program 73 - simpsonc : Composite Cavalieri-Simpson formula
function int = simpsonc(a,b,m,fun)
h=(b-a)/m; x=[a:h/2:b]; dim = max(size(x)); y=eval(fun);
if size(y)==1, y=diag(ones(dim))*y; end;
int=(h/6)*(y(1)+2*sum(y(3:2:2*m-1))+4*sum(y(2:2:2*m))+y(2*m+1));
Example 9.1 Let us employ the midpoint, trapezoidal and Cavalieri-Simpson
composite formulae to compute the integral
2œÄ
>
0
xe‚àíx cos(2x)dx =
0
3(e‚àí2œÄ ‚àí1) ‚àí10œÄe‚àí2œÄ1
25
‚âÉ‚àí0.122122.
(9.18)

378
9. Numerical Integration
Table 9.1 shows in even columns the behavior of the absolute value of the er-
ror when halving H (thus, doubling m), while in odd columns the ratio Rm =
|Em|/|E2m| between two consecutive errors is given. As predicted by the previous
theoretical analysis, Rm tends to 4 for the midpoint and trapezoidal rules and
to 16 for the Cavalieri-Simpson formula.
‚Ä¢
m
|E0,m|
Rm
|E1,m|
Rm
|E2,m|
Rm
1
0.9751
1.589e-01
7.030e-01
2
1.037
0.9406
0.5670
0.2804
0.5021
1.400
4
0.1221
8.489
0.2348
2.415
3.139 ¬∑ 10‚àí3
159.96
8
2.980 ¬∑ 10‚àí2
4.097
5.635 ¬∑ 10‚àí2
4.167
1.085 ¬∑ 10‚àí3
2.892
16
6.748 ¬∑ 10‚àí3
4.417
1.327 ¬∑ 10‚àí2
4.245
7.381 ¬∑ 10‚àí5
14.704
32
1.639 ¬∑ 10‚àí3
4.118
3.263 ¬∑ 10‚àí3
4.068
4.682 ¬∑ 10‚àí6
15.765
64
4.066 ¬∑ 10‚àí4
4.030
8.123 ¬∑ 10‚àí4
4.017
2.936 ¬∑ 10‚àí7
15.946
128
1.014 ¬∑ 10‚àí4
4.008
2.028 ¬∑ 10‚àí4
4.004
1.836 ¬∑ 10‚àí8
15.987
256
2.535 ¬∑ 10‚àí5
4.002
5.070 ¬∑ 10‚àí5
4.001
1.148 ¬∑ 10‚àí9
15.997
TABLE 9.1. Absolute error for midpoint, trapezoidal and Cavalieri-Simpson com-
posite formulae in the approximate evaluation of integral (9.18)
9.3
Newton-Cotes Formulae
These formulae are based on Lagrange interpolation with equally spaced
nodes in [a, b]. For a Ô¨Åxed n ‚â•0, let us denote the quadrature nodes
by xk = x0 + kh, k = 0, . . . , n. The midpoint, trapezoidal and Simpson
formulae are special instances of the Newton-Cotes formulae, taking n = 0,
n = 1 and n = 2 respectively. In the general case, we deÔ¨Åne:
- closed formulae, those where x0 = a, xn = b and h = b ‚àía
n
(n ‚â•1);
- open formulae, those where x0 = a+h, xn = b‚àíh and h = b ‚àía
n + 2 (n ‚â•0).
A signiÔ¨Åcant property of the Newton-Cotes formulae is that the quadra-
ture weights Œ±i depend explicitly only on n and h, but not on the integration
interval [a, b]. To check this property in the case of closed formulae, let us
introduce the change of variable x = Œ®(t) = x0 +th. Noting that Œ®(0) = a,
Œ®(n) = b and xk = a + kh, we get
x ‚àíxk
xi ‚àíxk
= a + th ‚àí(a + kh)
a + ih ‚àí(a + kh) = t ‚àík
i ‚àík .
Therefore, if n ‚â•1
li(x) =
n

k=0,kÃ∏=i
t ‚àík
i ‚àík = œïi(t),
0 ‚â§i ‚â§n.

9.3 Newton-Cotes Formulae
379
The following expression for the quadrature weights is obtained
Œ±i =
b
>
a
li(x)dx =
n
>
0
œïi(t)hdt = h
n
>
0
œïi(t)dt,
from which we get the formula
In(f) = h
n

i=0
wif(xi),
wi =
n
>
0
œïi(t)dt.
Open formulae can be interpreted in a similar manner. Actually, using again
the mapping x = Œ®(t), we get x0 = a+h, xn = b‚àíh and xk = a+h(k +1)
for k = 1, . . . , n ‚àí1. Letting, for sake of coherence, x‚àí1 = a, xn+1 = b and
proceeding as in the case of closed formulae, we get Œ±i = h
 n+1
‚àí1
œïi(t)dt,
and thus
In(f) = h
n

i=0
wif(xi),
wi =
n+1
>
‚àí1
œïi(t)dt.
In the special case where n = 0, since l0(x) = œï0(t) = 1, we get w0 = 2.
The coeÔ¨Écients wi do not depend on a, b, h and f, but only depend on n,
and can therefore be tabulated a priori. In the case of closed formulae, the
polynomials œïi and œïn‚àíi, for i = 0, . . . , n ‚àí1, have by symmetry the same
integral, so that also the corresponding weights wi and wn‚àíi are equal for
i = 0, . . . , n‚àí1. In the case of open formulae, the weights wi and wn‚àíi are
equal for i = 0, . . . , n. For this reason, we show in Table 9.2 only the Ô¨Årst
half of the weights.
Notice the presence of negative weights in open formulae for n ‚â•2. This can
be a source of numerical instability, in particular due to rounding errors.
n
1
2
3
4
5
6
w0
1
2
1
3
3
8
14
45
95
288
41
140
w1
0
4
3
9
8
64
45
375
288
216
140
w2
0
0
0
24
45
250
288
27
140
w3
0
0
0
0
0
272
140
n
0
1
2
3
4
5
w0
2
3
2
8
3
55
24
66
20
4277
1440
w1
0
0
‚àí4
3
5
24
‚àí84
20
‚àí3171
1440
w2
0
0
0
0
156
20
3934
1440
TABLE 9.2. Weights of closed (left) and open Newton-Cotes formulae (right)
Besides its degree of exactness, a quadrature formula can also be qualiÔ¨Åed
by its order of inÔ¨Ånitesimal with respect to the integration stepsize h, which
is deÔ¨Åned as the maximum integer p such that |I(f) ‚àíIn(f)| = O(hp).
Regarding this, the following result holds

380
9. Numerical Integration
Theorem 9.2 For any Newton-Cotes formula corresponding to an even
value of n, the following error characterization holds
En(f) =
Mn
(n + 2)!hn+3f (n+2)(Œæ),
(9.19)
provided f ‚ààCn+2([a, b]), where Œæ ‚àà(a, b) and
Mn =
Ô£±
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≥
n
>
0
t œÄn+1(t)dt < 0
for closed formulae,
n+1
>
‚àí1
t œÄn+1(t)dt > 0
for open formulae,
having deÔ¨Åned œÄn+1(t) = Bn
i=0(t ‚àíi). From (9.19), it turns out that the
degree of exactness is equal to n + 1 and the order of inÔ¨Ånitesimal is n + 3.
Similarly, for odd values of n, the following error characterization holds
En(f) =
Kn
(n + 1)!hn+2f (n+1)(Œ∑),
(9.20)
provided f ‚ààCn+1([a, b]), where Œ∑ ‚àà(a, b) and
Kn =
Ô£±
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≥
n
>
0
œÄn+1(t)dt < 0
for closed formulae,
n+1
>
‚àí1
œÄn+1(t)dt > 0
for open formulae.
The degree of exactness is thus equal to n and the order of inÔ¨Ånitesimal is
n + 2.
Proof. We give a proof in the particular case of closed formulae with n even,
referring to [IK66], pp. 308-314, for a complete demonstration of the theorem.
Thanks to (8.20), we have
En(f) = I(f) ‚àíIn(f) =
b
>
a
f[x0, . . . , xn, x]œân+1(x)dx.
(9.21)
Set W(x) =
 x
a œân+1(t)dt. Clearly, W(a) = 0; moreover, œân+1(t) is an odd func-
tion with respect to the midpoint (a + b)/2 so that W(b) = 0. Integrating by

9.3 Newton-Cotes Formulae
381
parts (9.21) we get
En(f)
=
b
>
a
f[x0, . . . , xn, x]W ‚Ä≤(x)dx = ‚àí
b
>
a
d
dxf[x0, . . . , xn, x]W(x)dx
=
‚àí
b
>
a
f (n+2)(Œæ(x))
(n + 2)!
W(x)dx.
In deriving the formula above we have used the following identity (see Exercise
4)
d
dxf[x0, . . . , xn, x] = f[x0, . . . , xn, x, x].
(9.22)
Since W(x) > 0 for a < x < b (see [IK66], p. 309), using the mean-value theorem
we obtain
En(f) = ‚àíf (n+2)(Œæ)
(n + 2)!
b
>
a
W(x)dx = ‚àíf (n+2)(Œæ)
(n + 2)!
b
>
a
x
>
a
œân+1(t) dt dx
(9.23)
where Œæ lies within (a, b). Exchanging the order of integration, letting s = x0+œÑh,
for 0 ‚â§œÑ ‚â§n, and recalling that a = x0, b = xn, yields
b
>
a
W(x)dx
=
b
>
a
b
>
s
(s ‚àíx0) . . . (s ‚àíxn)dxds
=
xn
>
x0
(s ‚àíx0) . . . (s ‚àíxn‚àí1)(s ‚àíxn)(xn ‚àís)ds
=
‚àíhn+3
n
>
0
œÑ(œÑ ‚àí1) . . . (œÑ ‚àín + 1)(œÑ ‚àín)2dœÑ.
Finally, letting t = n ‚àíœÑ and combining this result with (9.23), we get (9.19). 3
Relations (9.19) and (9.20) are a priori estimates for the quadrature error
(see Chapter 2, Section 2.3). Their use in generating a posteriori estimates
of the error in the frame of adaptive algorithms will be examined in Section
9.7.
In the case of closed Newton-Cotes formulae, we show in Table 9.3, for
1 ‚â§n ‚â§6, the degree of exactness (that we denote henceforth by rn) and
the absolute value of the constant Mn = Mn/(n + 2)! (if n is even) or
Kn = Kn/(n + 1)! (if n is odd).
Example 9.2 The purpose of this example is to assess the importance of the
regularity assumption on f for the error estimates (9.19) and (9.20). Consider
the closed Newton-Cotes formulae, for 1 ‚â§n ‚â§6, to approximate the integral
 1
0 x5/2dx = 2/7 ‚âÉ0.2857. Since f is only C2([0, 1]), we do not expect a substan-
tial increase of the accuracy as n gets larger. Actually, this is conÔ¨Årmed by Table
9.4, where the results obtained by running Program 74 are reported.

382
9. Numerical Integration
n
rn
Mn
Kn
n
rn
Mn
Kn
n
rn
Mn
Kn
1
1
1
12
3
3
3
80
5
5
275
12096
2
3
1
90
4
5
8
945
6
7
9
1400
TABLE 9.3. Degree of exactness and error constants for closed Newton-Cotes
formulae
For n = 1, . . . , 6, we have denoted by Ec
n(f) the module of the absolute error,
by qc
n the computed order of inÔ¨Ånitesimal and by qs
n the corresponding theoretical
value predicted by (9.19) and (9.20) under optimal regularity assumptions for f.
As is clearly seen, qc
n is deÔ¨Ånitely less than the potential theoretical value qs
n. ‚Ä¢
n
Ec
n(f)
qc
n
qs
n
n
Ec
n(f)
qc
n
qs
n
1
0.2143
3
3
4
5.009 ¬∑ 10‚àí5
4.7
7
2
1.196 ¬∑ 10‚àí3
3.2
5
5
3.189 ¬∑ 10‚àí5
2.6
7
3
5.753 ¬∑ 10‚àí4
3.8
5
6
7.857 ¬∑ 10‚àí6
3.7
9
TABLE 9.4. Error in the approximation of
 1
0 x5/2dx
Example 9.3 From a brief analysis of error estimates (9.19) and (9.20), we could
be led to believe that only non-smooth functions can be a source of trouble when
dealing with Newton-Cotes formulae. Thus, it is a little surprising to see results
like those in Table 9.5, concerning the approximation of the integral
I(f) =
5
>
‚àí5
1
1 + x2 dx = 2 arctan 5 ‚âÉ2.747,
(9.24)
where f(x) = 1/(1+x2) is Runge‚Äôs function (see Section 8.1.2), which belongs to
C‚àû(R). The results clearly demonstrate that the error remains almost unchanged
as n grows. This is due to the fact that singularities on the imaginary axis may
also aÔ¨Äect the convergence properties of a quadrature formula. This is indeed the
case with the function at hand, which exhibits two singularities at ¬±‚àö‚àí1 (see
[DR75], pp. 64-66).
‚Ä¢
n
En(f)
n
En(f)
n
En(f)
1
0.8601
3
0.2422
5
0.1599
2
-1.474
4
0.1357
6
-0.4091
TABLE 9.5. Relative error En(f) = [I(f) ‚àíIn(f)]/In(f) in the approximate
evaluation of (9.24) using closed Newton-Cotes formulae
To increase the accuracy of an interpolatory quadrature rule, it is by
no means convenient to increase the value of n. By doing so, the same

9.4 Composite Newton-Cotes Formulae
383
drawbacks of Lagrange interpolation on equally spaced nodes would arise.
For example, the weights of the closed Newton-Cotes formula with n = 8
do not have the same sign (see Table 9.6 and recall that wi = wn‚àíi for
i = 0, . . . , n ‚àí1).
n
w0
w1
w2
w3
w4
rn
Mn
8
3956
14175
23552
14175
‚àí3712
14175
41984
14175
‚àí18160
14175
9
2368
467775
TABLE 9.6. Weights of the closed Newton-Cotes formula with 9 nodes
This can give rise to numerical instabilities, due to rounding errors (see
Chapter 2), and makes this formula useless in the practice, as happens for
all the Newton-Cotes formulae using more than 8 nodes. As an alternative,
one can resort to composite formulae, whose error analysis is addressed in
Section 9.4, or to Gaussian formulae, which will be dealt with in Chapter
10 and which yield maximum degree of exactness with a non equally spaced
nodes distribution.
The closed Newton-Cotes formulae, for 1 ‚â§n ‚â§6, are implemented in
Program 74.
Program 74 - newtcot : Closed Newton-Cotes formulae
function int = newtcot(a,b,n,fun)
h=(b-a)/n; n2=Ô¨Åx(n/2);
if n > 6, disp(‚Äômaximum value of n equal to 6 ‚Äô); return; end
a03=1/3; a08=1/8; a45=1/45; a288=1/288; a140=1/140;
alpha=[0.5
0
0
0; ...
a03
4*a03
0
0; ...
3*a08
9*a08
0
0; ...
14*a45 64*a45
24*a45
0; ...
95*a288 375*a288 250*a288 0; ...
41*a140 216*a140 27*a140 272*a140];
x=a; y(1)=eval(fun);
for j=2:n+1,
x=x+h; y(j)=eval(f); end;
int=0;
for j=1:n2+1,
int=int+y(j)*alpha(n,j);
end;
for j=n2+2:n+1, int=int+y(j)*alpha(n,n-j+2); end; int=int*h;
9.4
Composite Newton-Cotes Formulae
The examples of Section 9.2 have already pointed out that composite
Newton-Cotes formulae can be constructed by replacing f with its com-
posite Lagrange interpolating polynomial, introduced in Section 8.1.

384
9. Numerical Integration
The general procedure consists of partitioning the integration interval
[a, b] into m subintervals Tj = [yj, yj+1] such that yj = a + jH, where H =
(b ‚àía)/m for j = 0, . . . , m. Then, over each subinterval, an interpolatory
formula with nodes {x(j)
k , 0 ‚â§k ‚â§n} and weights {Œ±(j)
k , 0 ‚â§k ‚â§n} is
used. Since
I(f) =
b
>
a
f(x)dx =
m‚àí1

j=0
>
Tj
f(x)dx,
a composite interpolatory quadrature formula is obtained by replacing I(f)
with
In,m(f) =
m‚àí1

j=0
n

k=0
Œ±(j)
k f(x(j)
k ).
(9.25)
The quadrature error is deÔ¨Åned as En,m(f) = I(f)‚àíIn,m(f). In particular,
over each subinterval Tj one can resort to a Newton-Cotes formula with
n + 1 equally spaced nodes: in such a case, the weights Œ±(j)
k
= hwk are still
independent of Tj.
Using the same notation as in Theorem 9.2, the following convergence
result holds for composite formulae.
Theorem 9.3 Let a composite Newton-Cotes formula, with n even, be
used. If f ‚ààCn+2([a, b]), then
En,m(f) =
b ‚àía
(n + 2)!
Mn
(n + 2)n+3 Hn+2f (n+2)(Œæ)
(9.26)
where Œæ ‚àà(a, b). Therefore, the quadrature error is an inÔ¨Ånitesimal in H
of order n + 2 and the formula has degree of exactness equal to n + 1.
For a composite Newton-Cotes formula, with n odd, if f ‚ààCn+1([a, b])
En,m(f) =
b ‚àía
(n + 1)!
Kn
nn+2 Hn+1f (n+1)(Œ∑)
(9.27)
where Œ∑ ‚àà(a, b). Thus, the quadrature error is an inÔ¨Ånitesimal in H of
order n + 1 and the formula has degree of exactness equal to n.
Proof. We only consider the case where n is even. Using (9.19), and noticing
that Mn does not depend on the integration interval, we get
En,m(f) =
m‚àí1

j=0
0
I(f)|Tj ‚àíIn(f)|Tj
1
=
Mn
(n + 2)!
m‚àí1

j=0
hn+3
j
f (n+2)(Œæj),
where, for j = 0, . . . , (m ‚àí1), hj = |Tj|/(n + 2) = (b ‚àía)/(m(n + 2)); this time,
Œæj is a suitable point of Tj. Since (b ‚àía)/m = H, we obtain
En,m(f) =
Mn
(n + 2)!
b ‚àía
m(n + 2)n+3 Hn+2
m‚àí1

j=0
f (n+2)(Œæj),

9.4 Composite Newton-Cotes Formulae
385
from which, applying Theorem 9.1 with u(x) = f (n+2)(x) and Œ¥j = 1 for j =
0, . . . , m ‚àí1, (9.26) immediately follows. A similar procedure can be followed to
prove (9.27).
3
We notice that, for n Ô¨Åxed, En,m(f) ‚Üí0 as m ‚Üí‚àû(i.e., as H ‚Üí0).
This ensures the convergence of the numerical integral to the exact value
I(f). We notice also that the degree of exactness of composite formulae
coincides with that of simple formulae, whereas its order of inÔ¨Ånitesimal
(with respect to H) is reduced by 1 with respect to the order of inÔ¨Ånitesimal
(in h) of simple formulae.
In practical computations, it is convenient to resort to a local interpolation
of low degree (typically n ‚â§2, as done in Section 9.2), this leads to com-
posite quadrature rules with positive weights, with a minimization of the
rounding errors.
Example 9.4 For the same integral (9.24) considered in Example 9.3, we show
in Table 9.7 the behavior of the absolute error as a function of the number of
subintervals m, in the case of the composite midpoint, trapezoidal and Cavalieri-
Simpson formulae. Convergence of In,m(f) to I(f) as m increases can be clearly
observed. Moreover, we notice that E0,m(f) ‚âÉE1,m(f)/2 for m ‚â•32 (see Exer-
cise 1).
m
|E0,m|
|E1,m|
|E2,m|
1
7.253
2.362
4.04
2
1.367
2.445
9.65 ¬∑ 10‚àí2
8
3.90 ¬∑ 10‚àí2
3.77 ¬∑ 10‚àí2
1.35 ¬∑ 10‚àí2
32
1.20 ¬∑ 10‚àí4
2.40 ¬∑ 10‚àí4
4.55 ¬∑ 10‚àí8
128
7.52 ¬∑ 10‚àí6
1.50 ¬∑ 10‚àí5
1.63 ¬∑ 10‚àí10
512
4.70 ¬∑ 10‚àí7
9.40 ¬∑ 10‚àí7
6.36 ¬∑ 10‚àí13
TABLE 9.7. Absolute error for composite quadratures in the computation of
(9.24)
‚Ä¢
Convergence of In,m(f) to I(f) can be established under less stringent
regularity assumptions on f than those required by Theorem 9.3. In this
regard, the following result holds (see for the proof [IK66], pp. 341-343).
Property 9.1 Let f ‚ààC0([a, b]) and assume that the weights Œ±(j)
k
in (9.25)
are nonnegative. Then
lim
m‚Üí‚àûIn,m(f) =
> b
a
f(x)dx,
‚àÄn ‚â•0.
Moreover

> b
a
f(x)dx ‚àíIn,m(f)
 ‚â§2(b ‚àía)‚Ñ¶(f; H),

386
9. Numerical Integration
where
‚Ñ¶(f; H) = sup{|f(x) ‚àíf(y)|, x, y ‚àà[a, b], x Ã∏= y, |x ‚àíy| ‚â§H}
is the module of continuity of function f.
9.5
Hermite Quadrature Formulae
Thus far we have considered quadrature formulae based on Lagrange inter-
polation (simple or composite). More accurate formulae can be devised by
resorting to Hermite interpolation (see Section 8.4).
Suppose that 2(n + 1) values f(xk), f ‚Ä≤(xk) are available at n + 1 distinct
points x0, . . . , xn, then the Hermite interpolating polynomial of f is given
by
H2n+1f(x) =
n

i=0
f(xi)Li(x) +
n

i=0
f ‚Ä≤(xi)Mi(x),
(9.28)
where the polynomials Lk, Mk ‚ààP2n+1 are deÔ¨Åned, for k = 0, . . . , n, as
Lk(x) =

1 ‚àíœâ‚Ä≤‚Ä≤
n+1(xk)
œâ‚Ä≤
n+1(xk)(x ‚àíxk)

l2
k(x),
Mk(x) = (x ‚àíxk)l2
k(x).
Integrating (9.28) over [a, b], we get the quadrature formula of type (9.4)
In(f) =
n

k=0
Œ±kf(xk) +
n

k=0
Œ≤kf ‚Ä≤(xk)
(9.29)
where
Œ±k = I(Lk),
Œ≤k = I(Mk),
k = 0, . . . , n.
Formula (9.29) has degree of exactness equal to 2n + 1. Taking n = 1, the
so-called corrected trapezoidal formula is obtained
Icorr
1
(f) = b ‚àía
2
[f(a) + f(b)] + (b ‚àía)2
12
[f ‚Ä≤(a) ‚àíf ‚Ä≤(b)]
(9.30)
with weights Œ±0 = Œ±1 = (b‚àía)/2, Œ≤0 = (b‚àía)2/12 and Œ≤1 = ‚àíŒ≤0. Assuming
f ‚ààC4([a, b]), the quadrature error associated with (9.30) is
Ecorr
1
(f) = h5
720f (4)(Œæ),
h = b ‚àía
(9.31)
with Œæ ‚àà(a, b). Notice the increase of accuracy from O(h3) to O(h5) with
respect to the corresponding expression (9.12) (of the same order as the

9.6 Richardson Extrapolation
387
Cavalieri-Simpson formula (9.15)). The composite formula can be generated
in a similar manner
Icorr
1,m (f) = b ‚àía
m
%1
2 [f(x0) + f(xm)]
+f(x1) + . . . + f(xm‚àí1)} + (b ‚àía)2
12
[f ‚Ä≤(a) ‚àíf ‚Ä≤(b)] ,
(9.32)
where the assumption that f ‚ààC1([a, b]) gives rise to the cancellation of
the Ô¨Årst derivatives at the nodes xk, with k = 1, . . . , m ‚àí1.
Example 9.5 Let us check experimentally the error estimate (9.31) in the simple
(m = 1) and composite (m > 1) cases, running Program 75 for the approximate
computation of integral (9.18). Table 9.8 reports the behavior of the module of
the absolute error as H is halved (that is, m is doubled) and the ratio Rm between
two consecutive errors. This ratio, as happens in the case of Cavalieri-Simpson
formula, tends to 16, demonstrating that formula (9.32) has order of inÔ¨Ånitesimal
equal to 4. Comparing Table 9.8 with the corresponding Table 9.1, we can also
notice that |Ecorr
1,m (f)| ‚âÉ4|E2,m(f)| (see Exercise 9).
‚Ä¢
m
Ecorr
1,m (f)
Rm
m
Ecorr
1,m (f)
Rm
m
Ecorr
1,m (f)
Rm
1
3.4813
8
4.4 ¬∑ 10‚àí3
6.1
64
1.1 ¬∑ 10‚àí6
15.957
2
1.398
2.4
16
2.9 ¬∑ 10‚àí4
14.9
128
7.3 ¬∑ 10‚àí8
15.990
4
2.72 ¬∑ 10‚àí2
51.4
32
1.8 ¬∑ 10‚àí5
15.8
256
4.5 ¬∑ 10‚àí9
15.997
TABLE 9.8. Absolute error for the corrected trapezoidal formula in the compu-
tation of I(f) =
 2œÄ
0
xe‚àíx cos(2x)dx
The corrected composite trapezoidal quadrature is implemented in Pro-
gram 75, where dfun contains the expression of the derivative of f.
Program 75 - trapmodc : Composite corrected trapezoidal formula
function int = trapmodc(a,b,m,fun,dfun)
h=(b-a)/m; x=[a:h:b]; y=eval(fun);
f1a=feval(dfun,a); f1b=feval(dfun,b);
int=h*(0.5*y(1)+sum(y(2:m))+0.5*y(m+1))+(hÀÜ2/12)*(f1a-f1b);
9.6
Richardson Extrapolation
The Richardson extrapolation method is a procedure which combines several
approximations of a certain quantity Œ±0 in a smart way to yield a more
accurate approximation of Œ±0. More precisely, assume that a method is
available to approximate Œ±0 by a quantity A(h) that is computable for any

388
9. Numerical Integration
value of the parameter h Ã∏= 0. Moreover, assume that, for a suitable k ‚â•0,
A(h) can be expanded as follows
A(h) = Œ±0 + Œ±1h + . . . + Œ±khk + Rk+1(h),
(9.33)
where |Rk+1(h)| ‚â§Ck+1hk+1. The constants Ck+1 and the coeÔ¨Écients Œ±i,
for i = 0, . . . , k, are independent of h. Henceforth, Œ±0 = limh‚Üí0 A(h).
Writing (9.33) with Œ¥h instead of h, for 0 < Œ¥ < 1 (typically, Œ¥ = 1/2),
we get
A(Œ¥h) = Œ±0 + Œ±1(Œ¥h) + . . . + Œ±k(Œ¥h)k + Rk+1(Œ¥h).
Subtracting (9.33) multiplied by Œ¥ from this expression then yields
B(h) = A(Œ¥h) ‚àíŒ¥A(h)
1 ‚àíŒ¥
= Œ±0 + $Œ±2h2 + . . . + $Œ±khk + $Rk+1(h),
having deÔ¨Åned, for k ‚â•2, $Œ±i = Œ±i(Œ¥i ‚àíŒ¥)/(1 ‚àíŒ¥), for i = 2, . . . , k and
$Rk+1(h) = [Rk+1(Œ¥h) ‚àíŒ¥Rk+1(h)] /(1 ‚àíŒ¥).
Notice that $Œ±i Ã∏= 0 iÔ¨ÄŒ±i Ã∏= 0. In particular, if Œ±1 Ã∏= 0, then A(h) is a Ô¨Årst-
order approximation of Œ±0, while B(h) is at least second-order accurate.
More generally, if A(h) is an approximation of Œ±0 of order p, then the
quantity B(h) = [A(Œ¥h) ‚àíŒ¥pA(h)] /(1 ‚àíŒ¥p) approximates Œ±0 up to order
p + 1 (at least).
Proceeding by induction, the following Richardson extrapolation algorithm
is generated: setting n ‚â•0, h > 0 and Œ¥ ‚àà(0, 1), we construct the sequences
Am,0 = A(Œ¥mh),
m = 0, . . . , n,
Am,q+1 = Am,q ‚àíŒ¥q+1Am‚àí1,q
1 ‚àíŒ¥q+1
,
q = 0, . . . , n ‚àí1,
m = q + 1, . . . , n,
(9.34)
which can be represented by the diagram below
A0,0
‚Üò
A1,0
‚Üí
A1,1
‚Üò
‚Üò
A2,0
‚Üí
A2,1
‚Üí
A2,2
‚Üò
‚Üò
‚Üò
A3,0
‚Üí
A3,1
‚Üí
A3,2
‚Üí
A3,3
‚Üò
‚Üò
‚Üò
‚Üò
...
...
...
...
...
‚Üò
‚Üò
‚Üò
‚Üò
An,0
‚Üí
An,1
‚Üí
An,2
‚Üí
An,3
. . .
‚Üí
An,n
where the arrows indicate the way the terms which have been already
computed contribute to the construction of the ‚Äúnew‚Äù ones.
The following result can be proved (see [Com95], Proposition 4.1).

9.6 Richardson Extrapolation
389
Property 9.2 For n ‚â•0 and Œ¥ ‚àà(0, 1)
Am,n = Œ±0 + O((Œ¥mh)n+1),
m = 0, . . . , n.
(9.35)
In particular, for the terms in the Ô¨Årst column (n = 0) the convergence
rate to Œ±0 is O((Œ¥mh)), while for those of the last one it is O((Œ¥mh)n+1),
i.e., n times higher.
Example 9.6 Richardson extrapolation has been employed to approximate at
x = 0 the derivative of the function f(x) = xe‚àíx cos(2x), introduced in Ex-
ample 9.1. For this purpose, algorithm (9.34) has been executed with A(h) =
[f(x + h) ‚àíf(x)] /h, Œ¥ = 0.5, n = 5 and h = 0.1. Table 9.9 reports the sequence
of absolute errors Em,k = |Œ±0 ‚àíAm,k|. The results demonstrate that the error
decays as predicted by (9.35).
‚Ä¢
Em,0
Em,1
Em,2
Em,3
Em,4
Em,5
0.113
‚Äì
‚Äì
‚Äì
‚Äì
‚Äì
5.3 ¬∑ 10‚àí2
6.1 ¬∑ 10‚àí3
‚Äì
‚Äì
‚Äì
‚Äì
2.6 ¬∑ 10‚àí2
1.7 ¬∑ 10‚àí3
2.2 ¬∑ 10‚àí4
‚Äì
‚Äì
‚Äì
1.3 ¬∑ 10‚àí2
4.5 ¬∑ 10‚àí4
2.8 ¬∑ 10‚àí5
5.5 ¬∑ 10‚àí7
‚Äì
‚Äì
6.3 ¬∑ 10‚àí3
1.1 ¬∑ 10‚àí4
3.5 ¬∑ 10‚àí6
3.1 ¬∑ 10‚àí8
3.0 ¬∑ 10‚àí9
‚Äì
3.1 ¬∑ 10‚àí3
2.9 ¬∑ 10‚àí5
4.5 ¬∑ 10‚àí7
1.9 ¬∑ 10‚àí9
9.9 ¬∑ 10‚àí11
4.9 ¬∑ 10‚àí12
TABLE 9.9. Errors in the Richardson extrapolation for the approximate evalua-
tion of f ‚Ä≤(0) where f(x) = xe‚àíx cos(2x)
9.6.1
Romberg Integration
The Romberg integration method is an application of Richardson extrap-
olation to the composite trapezoidal rule. The following result, known as
the Euler-MacLaurin formula, will be useful (for its proof see, e.g., [Ral65],
pp. 131-133, and [DR75], pp. 106-111).
Property 9.3 Let f ‚ààC2k+2([a, b]), for k ‚â•0, and let us approximate
Œ±0 =
 b
a f(x)dx by the composite trapezoidal rule (9.14). Letting hm =
(b ‚àía)/m for m ‚â•1,
I1,m(f) = Œ±0
+
k

i=1
B2i
(2i)!h2i
m
+
f (2i‚àí1)(b) ‚àíf (2i‚àí1)(a)
,
+ B2k+2
(2k + 2)!h2k+2
m
(b ‚àía)f (2k+2)(Œ∑),
(9.36)
where Œ∑ ‚àà(a, b) and B2j = (‚àí1)j‚àí1
.+‚àû

n=1
2/(2nœÄ)2j
/
(2j)!, for j ‚â•1, are
the Bernoulli numbers.

390
9. Numerical Integration
Equation (9.36) is a special case of (9.33) where h = h2
m and A(h) =
I1,m(f); notice that only even powers of the parameter h appear in the
expansion.
The Richardson extrapolation algorithm (9.34) applied to (9.36) gives
Am,0 = A(Œ¥mh),
m = 0, . . . , n,
Am,q+1 = Am,q ‚àíŒ¥2(q+1)Am‚àí1,q
1 ‚àíŒ¥2(q+1)
,
q = 0, . . . , n ‚àí1,
m = q + 1, . . . , n.
(9.37)
Setting h = b ‚àía and Œ¥ = 1/2 into (9.37) and denoting by T(hs) = I1,s(f)
the composite trapezoidal formula (9.14) over s = 2m subintervals of width
hs = (b ‚àía)/2m, for m ‚â•0, the algorithm (9.37) becomes
Am,0 = T((b ‚àía)/2m),
m = 0, . . . , n,
Am,q+1 = 4q+1Am,q ‚àíAm‚àí1,q
4q+1 ‚àí1
,
q = 0, . . . , n ‚àí1,
m = q + 1, . . . , n.
This is the Romberg numerical integration algorithm. Recalling (9.35), the
following convergence result holds for Romberg integration
Am,n =
b
>
a
f(x)dx + O(h2(n+1)
s
),
n ‚â•0.
Example 9.7 Table 9.10 shows the results obtained by running Program 76 to
compute the quantity Œ±0 in the two cases Œ±(1)
0
=
 œÄ
0 ex cos(x)dx = ‚àí(eœÄ + 1)/2
and Œ±(2)
0
=
 1
0
‚àöxdx = 2/3.
The maximum size n has been set equal to 9. In the second and third columns
we show the modules of the absolute errors E(r)
k
= |Œ±(r)
0
‚àíA(r)
k+1,k+1|, for r = 1, 2
and k = 0, . . . , 6.
The convergence to zero is much faster for E(1)
k
than for E(2)
k . Indeed, the Ô¨Årst
integrand function is inÔ¨Ånitely diÔ¨Äerentiable whereas the second is only continu-
ous.
‚Ä¢
k
E(1)
k
E(2)
k
k
E(1)
k
E(2)
k
0
22.71
0.1670
4
8.923 ¬∑ 10‚àí7
1.074 ¬∑ 10‚àí3
1
0.4775
2.860 ¬∑ 10‚àí2
5
6.850 ¬∑ 10‚àí11
3.790 ¬∑ 10‚àí4
2
5.926 ¬∑ 10‚àí2
8.910 ¬∑ 10‚àí3
6
5.330 ¬∑ 10‚àí14
1.340 ¬∑ 10‚àí4
3
7.410 ¬∑ 10‚àí5
3.060 ¬∑ 10‚àí3
7
0
4.734 ¬∑ 10‚àí5
TABLE
9.10.
Romberg
integration
for
the
approximate
evaluation
of
 œÄ
0 ex cos(x)dx (error E(1)
k ) and
 1
0
‚àöxdx (error E(2)
k )
The Romberg algorithm is implemented in Program 76.

9.7 Automatic Integration
391
Program 76 - romberg : Romberg integration
function [A]=romberg(a,b,n,fun);
for i=1:(n+1), A(i,1)=trapezc(a,b,2ÀÜ(i-1),fun); end;
for j=2:(n+1), for i=j:(n+1),
A(i,j)=(4ÀÜ(j-1)*A(i,j-1)-A(i-1,j-1))/(4ÀÜ(j-1)-1); end; end;
9.7
Automatic Integration
An automatic numerical integration program, or automatic integrator, is
a set of algorithms which yield an approximation of the integral I(f) =
 b
a f(x)dx, within a given tolerance, Œµa, or relative tolerance, Œµr, prescribed
by the user.
With this aim, the program generates a sequence {Ik, Ek}, for k =
1, . . . , N, where Ik is the approximation of I(f) at the k-th step of the
computational process, Ek is an estimate of the error I(f) ‚àíIk, and is N
a suitable Ô¨Åxed integer.
The sequence terminates at the s-th level, with s ‚â§N, such that the
automatic integrator fulÔ¨Ålls the following requirement on the accuracy
max
2
Œµa, Œµr|$I(f)|
3
‚â•|Es|(‚âÉ|I(f) ‚àíIs|),
(9.38)
where $I(f) is a reasonable guess of the integral I(f) provided as an input
datum by the user. Otherwise, the integrator returns the last computed
approximation IN, together with a suitable error message that warns the
user of the algorithm‚Äôs failure to converge.
Ideally, an automatic integrator should:
(a) provide a reliable criterion for determining |Es| that allows for moni-
toring the convergence check (9.38);
(b) ensure an eÔ¨Écient implementation, which minimizes the number of
functional evaluations for yielding the desired approximation Is.
In computational practice, for each k ‚â•1, moving from level k to level
k + 1 of the automatic integration process can be done according to two
diÔ¨Äerent strategies, which we deÔ¨Åne as non adaptive or adaptive.
In the non adaptive case, the law of distribution of the quadrature nodes
is Ô¨Åxed a priori and the quality of the estimate Ik is reÔ¨Åned by increas-
ing the number of nodes corresponding to each level of the computational
process. An example of an automatic integrator that is based on such a
procedure is provided by the composite Newton-Cotes formulae on m and

392
9. Numerical Integration
2m subintervals, respectively, at levels k and k + 1, as described in Section
9.7.1.
In the adaptive case, the positions of the nodes is not set a priori, but at
each level k of the process they depend on the information that has been
stored during the previous k ‚àí1 levels. An adaptive automatic integration
algorithm is performed by partitioning the interval [a, b] into successive
subdivisions which are characterized by a nonuniform density of the nodes,
this density being typically higher in a neighborhood of strong gradients
or singularities of f. An example of an adaptive integrator based on the
Cavalieri-Simpson formula is described in Section 9.7.2.
9.7.1
Non Adaptive Integration Algorithms
In this section, we employ the composite Newton-Cotes formulae. Our aim
is to devise a criterion for estimating the absolute error |I(f) ‚àíIk| by
using Richardson extrapolation. From (9.26) and (9.27) it turns out that,
for m ‚â•1 and n ‚â•0, In,m(f) has order of inÔ¨Ånitesimal equal to Hn+p,
with p = 2 for n even and p = 1 for n odd, where m, n and H = (b ‚àía)/m
are the number of partitions of [a, b], the number of quadrature nodes over
each subinterval and the constant length of each subinterval, respectively.
By doubling the value of m (i.e., halving the stepsize H) and proceeding
by extrapolation, we get
I(f) ‚àíIn,2m(f) ‚âÉ
1
2n+p [I(f) ‚àíIn,m(f)] .
(9.39)
The use of the symbol ‚âÉinstead of = is due to the fact that the point Œæ
or Œ∑, where the derivative in (9.26) and (9.27) must be evaluated, changes
when passing from m to 2m subintervals. Solving (9.39) with respect to
I(f) yields the following absolute error estimate for In,2m(f)
I(f) ‚àíIn,2m(f) ‚âÉIn,2m(f) ‚àíIn,m(f)
2n+p ‚àí1
.
(9.40)
If the composite Simpson rule is considered (i.e., n = 2), (9.40) predicts a
reduction of the absolute error by a factor of 15 when passing from m to 2m
subintervals. Notice also that only 2m‚àí1 extra functional evaluations are
needed to compute the new approximation I1,2m(f) starting from I1,m(f).
Relation (9.40) is an instance of an a posteriori error estimate (see Chapter
2, Section 2.3). It is based on the combined use of an a priori estimate (in
this case, (9.26) or (9.27)) and of two evaluations of the quantity to be ap-
proximated (the integral I(f)) for two diÔ¨Äerent values of the discretization
parameter (that is, H = (b ‚àía)/m).

9.7 Automatic Integration
393
Example 9.8 Let us employ the a posteriori estimate (9.40) in the case of the
composite Simpson formula (n = p = 2), for the approximation of the integral
œÄ
>
0
(ex/2 + cos 4x)dx = 2(eœÄ ‚àí1) ‚âÉ7.621,
where we require the absolute error to be less than 10‚àí4. For k = 0, 1, . . . , set
hk = (b‚àía)/2k and denote by I2,m(k)(f) the integral of f which is computed using
the composite Simpson formula on a grid of size hk with m(k) = 2k intervals. We
can thus assume as a conservative estimate of the quadrature error the following
quantity
|Ek| = |I(f) ‚àíI2,m(k)(f)| ‚âÉ1
10|I2,2m(k)(f) ‚àíI2,m(k)(f)| = |Ek|,
k ‚â•1.
(9.41)
Table 9.11 shows the sequence of the estimated errors |Ek| and of the correspond-
ing absolute errors |Ek| that have been actually made by the numerical integration
process. Notice that, when convergence has been achieved, the error estimated
by (9.41) is deÔ¨Ånitely higher than the actual error, due to the conservative choice
above.
‚Ä¢
k
|Ek|
|Ek|
k
|Ek|
|Ek|
0
3.156
2
0.10
4.52 ¬∑ 10‚àí5
1
0.42
1.047
3
5.8 ¬∑ 10‚àí6
2 ¬∑ 10‚àí9
TABLE 9.11. Non adaptive automatic Simpson rule for the approximation of
 œÄ
0 (ex/2 + cos 4x)dx
An alternative approach for fulÔ¨Ålling the constraints (a) and (b) con-
sists of employing a nested sequence of special Gaussian quadratures Ik(f)
(see Chapter 10), having increasing degree of exactness for k = 1, . . . , N.
These formulae are constructed in such a way that, denoting by Snk =
{x1, . . . , xnk} the set of quadrature nodes relative to quadrature Ik(f),
Snk ‚äÇSnk+1 for any k = 1, . . . , N ‚àí1. As a result, for k ‚â•1, the formula
at the k + 1-th level employs all the nodes of the formula at level k and
this makes nested formulae quite eÔ¨Äective for computer implementation.
As an example, we recall the Gauss-Kronrod formulae with 10, 21, 43
and 87 points, that are available in [PdK¬®UK83] (in this case, N = 4). The
Gauss-Kronrod formulae have degree of exactness rnk (optimal) equal to
2nk‚àí1, where nk is the number of nodes for each formula, with n1 = 10 and
nk+1 = 2nk+1 for k = 1, 2, 3. The criterion for devising an error estimate is
based on comparing the results given by two successive formulae Ink(f) and
Ink+1(f) with k = 1, 2, 3, and then terminating the computational process
at the level k such that (see also [DR75], p. 321)
|Ik+1 ‚àíIk| ‚â§max {Œµa, Œµr|Ik+1|} .

394
9. Numerical Integration
9.7.2
Adaptive Integration Algorithms
The goal of an adaptive integrator is to yield an approximation of I(f)
within a Ô¨Åxed tolerance Œµ by a non uniform distribution of the integration
stepsize along the interval [a, b]. An optimal algorithm is able to adapt
automatically the choice of the steplength according to the behavior of the
integrand function, by increasing the density of the quadrature nodes where
the function exhibits stronger variations.
In view of describing the method, it is convenient to restrict our attention
to a generic subinterval [Œ±, Œ≤] ‚äÜ[a, b]. Recalling the error estimates for the
Newton-Cotes formulae, it turns out that the evaluation of the derivatives
of f, up to a certain order, is needed to set a stepsize h such that a Ô¨Åxed
accuracy is ensured, say Œµ(Œ≤‚àíŒ±)/(b‚àía). This procedure, which is unfeasible
in practical computations, is carried out by an automatic integrator as
follows. We consider throughout this section the Cavalieri-Simpson formula
(9.15), although the method can be extended to other quadrature rules.
Set If(Œ±, Œ≤) =
 Œ≤
Œ± f(x)dx, h = h0 = (Œ≤ ‚àíŒ±)/2 and
Sf(Œ±, Œ≤) = (h0/3) [f(Œ±) + 4f(Œ± + h0) + f(Œ≤)] .
From (9.16) we get
If(Œ±, Œ≤) ‚àíSf(Œ±, Œ≤) = ‚àíh5
0
90f (4)(Œæ),
(9.42)
where Œæ is a point in (Œ±, Œ≤). To estimate the error If(Œ±, Œ≤) ‚àíSf(Œ±, Œ≤)
without using explicitly the function f (4) we employ again the Cavalieri-
Simpson formula over the union of the two subintervals [Œ±, (Œ± + Œ≤)/2] and
[(Œ± + Œ≤)/2, Œ≤], obtaining, for h = h0/2 = (Œ≤ ‚àíŒ±)/4
If(Œ±, Œ≤) ‚àíSf,2(Œ±, Œ≤) = ‚àí(h0/2)5
90
+
f (4)(Œæ) + f (4)(Œ∑)
,
,
where Œæ ‚àà(Œ±, (Œ± + Œ≤)/2), Œ∑ ‚àà((Œ± + Œ≤)/2, Œ≤) and Sf,2(Œ±, Œ≤) = Sf(Œ±, (Œ± +
Œ≤)/2) + Sf((Œ± + Œ≤)/2, Œ≤).
Let us now make the assumption that f (4)(Œæ) ‚âÉf (4)(Œ∑) (which is true,
in general, only if the function f (4) does not vary ‚Äútoo much‚Äù on [Œ±, Œ≤]).
Then,
If(Œ±, Œ≤) ‚àíSf,2(Œ±, Œ≤) ‚âÉ‚àí1
16
h5
0
90f (4)(Œæ),
(9.43)
with a reduction of the error by a factor 16 with respect to (9.42), corre-
sponding to the choice of a steplength of doubled size. Comparing (9.42)
and (9.43), we get the estimate
h5
0
90f (4)(Œæ) ‚âÉ16
15Ef(Œ±, Œ≤),

9.7 Automatic Integration
395
where Ef(Œ±, Œ≤) = Sf(Œ±, Œ≤) ‚àíSf,2(Œ±, Œ≤). Then, from (9.43), we have
|If(Œ±, Œ≤) ‚àíSf,2(Œ±, Œ≤)| ‚âÉ|Ef(Œ±, Œ≤)|
15
.
(9.44)
We have thus obtained a formula that allows for easily computing the error
made by using composite Cavalieri-Simpson numerical integration on the
generic interval [Œ±, Œ≤]. Relation (9.44), as well as (9.40), is another instance
of an a posteriori error estimate. It combines the use of an a priori es-
timate (in this case, (9.16)) and of two evaluations of the quantity to be
approximated (the integral I(f)) for two diÔ¨Äerent values of the discretiza-
tion parameter h.
In the practice, it might be convenient to assume a more conservative
error estimate, precisely
|If(Œ±, Œ≤) ‚àíSf,2(Œ±, Œ≤)| ‚âÉ|Ef(Œ±, Œ≤)|/10.
Moreover, to ensure a global accuracy on [a, b] equal to the Ô¨Åxed tolerance
Œµ, it will suÔ¨Éce to enforce that the error Ef(Œ±, Œ≤) satisÔ¨Åes on each single
subinterval [Œ±, Œ≤] ‚äÜ[a, b] the following constraint
|Ef(Œ±, Œ≤)|
10
‚â§ŒµŒ≤ ‚àíŒ±
b ‚àía .
(9.45)
The adaptive automatic integration algorithm can be described as follows.
Denote by:
1. A: the active integration interval, i.e., the interval where the integral
is being computed;
2. S: the integration interval already examined, for which the error test
(9.45) has been successfully passed;
3. N: the integration interval yet to be examined.
At the beginning of the integration process we have N = [a, b], A = N
and S = ‚àÖ, while the situation at the generic step of the algorithm is
depicted in Figure 9.3. Set JS(f) ‚âÉ
 Œ±
a f(x)dx, with JS(f) = 0 at the
beginning of the process; if the algorithm successfully terminates, JS(f)
yields the desired approximation of I(f). We also denote by J(Œ±,Œ≤)(f) the
approximate integral of f over the ‚Äúactive‚Äù interval [Œ±, Œ≤]. This interval
is drawn in bold in Figure 9.3. At each step of the adaptive integration
method the following decisions are taken:
1. if the local error test (9.45) is passed, then:
(i) JS(f) is increased by J(Œ±,Œ≤)(f), that is, JS(f) ‚ÜêJS(f)+J(Œ±,Œ≤)(f);
(ii) we let S ‚ÜêS ‚à™A, A = N (corresponding to the path (I) in
Figure 9.3), Œ≤ = b.

396
9. Numerical Integration
a
Œ±‚Ä≤
b
b
b
Œ±
Œ≤
S
A
N
(I)
S
a
a
Œ±
A
Œ±
S
A
N
(II)
FIGURE 9.3. Distribution of the integration intervals at the generic step of the
adaptive algorithm and updating of the integration grid
2. If the local error test (9.45) fails, then:
(j) A is halved, and the new active interval is set to A = [Œ±, Œ±‚Ä≤] with
Œ±‚Ä≤ = (Œ± + Œ≤)/2 (corresponding to the path (II) in Figure 9.3);
(jj) we let N ‚ÜêN ‚à™[Œ±‚Ä≤, Œ≤], Œ≤ ‚ÜêŒ±‚Ä≤;
(jjj) a new error estimate is provided.
In order to prevent the algorithm from generating too small stepsizes, it
is convenient to monitor the width of A and warn the user, in case of
an excessive reduction of the steplength, about the presence of a possible
singularity in the integrand function (see Section 9.8).
Example 9.9 Let us employ Cavalieri-Simpson adaptive integration for com-
puting the integral
I(f)
=
4
>
‚àí3
tan‚àí1(10x)dx
= 4tan‚àí1(40) + 3tan‚àí1(‚àí30) ‚àí(1/20) log(16/9) ‚âÉ1.54201193.
Running Program 77 with tol = 10‚àí4 and hmin = 10‚àí3 yields an approximation
of the integral with an absolute error of 2.104 ¬∑ 10‚àí5. The algorithm performs
77 functional evaluations, corresponding to partitioning the interval [a, b] into
38 nonuniform subintervals. We notice that the corresponding composite formula
with uniform stepsize would have required 128 subintervals with an absolute error
of 2.413 ¬∑ 10‚àí5.
In Figure 9.4 (left) we show, together with the plot of the integrand function,
the distribution of the quadrature nodes as a function of x, while on the right
the integration step density (piecewise constant) ‚àÜh(x) is shown, deÔ¨Åned as the
inverse of the step size h over each active interval A. Notice the high value attained
by ‚àÜh at x = 0, where the derivative of the integrand function is maximum.
‚Ä¢
The adaptive algorithm described above is implemented in Program 77.
Among the input parameters, hmin is the minimum admissible value of
the integration steplength. In output the program returns the approximate

9.7 Automatic Integration
397
‚àí3
‚àí2
‚àí1
0
1
2
3
4
‚àí2
‚àí1.5
‚àí1
‚àí0.5
0
0.5
1
1.5
2
‚àí3
‚àí2
‚àí1
0
1
2
3
4
0
10
20
30
40
50
60
70
80
FIGURE 9.4. Distribution of quadrature nodes (left); density of the integration
stepsize in the approximation of the integral of Example 9.9 (right)
value of the integral integ, the total number of functional evaluations nfv
and the set of integration points xfv.
Program 77 - simpadpt : Adaptive Cavalieri-Simpson formula
function [integ,xfv,nfv]=simpadpt(a,b,tol,fun,hmin);
integ=0; level=0; i=1; alfa(i)=a; beta(i)=b;
step=(beta(i)-alfa(i))/4; nfv=0;
for k=1:5, x=a+(k-1)*step; f(i,k)=eval(fun); nfv=nfv+1; end
while (i > 0),
S=0; S2=0; h=(beta(i)-alfa(i))/2; S=(h/3)*(f(i,1)+4*f(i,3)+f(i,5));
h=h/2; S2=(h/3)*(f(i,1)+4*f(i,2)+f(i,3));
S2=S2+(h/3)*(f(i,3)+4*f(i,4)+f(i,5));
tolrv=tol*(beta(i)-alfa(i))/(b-a); errrv=abs(S-S2)/10;
if (errrv > tolrv)
i=i+1; alfa(i)=alfa(i-1); beta(i)=(alfa(i-1)+beta(i-1))/2;
f(i,1)=f(i-1,1);f(i,3)=f(i-1,2);f(i,5)=f(i-1,3);len=abs(beta(i)-alfa(i));
if (len >= hmin),
if (len <= 11*hmin)
disp(‚Äô Steplength close to hmin ‚Äô),
str=sprintf(‚ÄôThe approximate integral is %12.7e‚Äô,integ);disp(str),end;
step=len/4; x=alfa(i)+step; f(i,2)=eval(fun);
nfv=nfv+1; x=beta(i)-step; f(i,4)=eval(fun); nfv=nfv+1;
else, xfv=xfv‚Äô; disp(‚Äô Too small steplength
‚Äô)
str=sprintf(‚ÄôThe approximate integral is %12.7e‚Äô,integ);
disp(str), return
end, else
integ=integ+S2; level=level+1; if (level==1),
for k=1:5, xfv(k)=alfa(i)+(k-1)*h; end; ist=5;
else, for k=1:4, xfv(ist+k)=alfa(i)+k*h; end; ist=ist+4; end;
if (beta(i)==b), xfv=xfv‚Äô;
str=sprintf(‚ÄôThe approximate integral is %12.7e‚Äô,integ);
disp(str), return, end; i=i-1; alfa(i)=beta(i+1);
f(i,1)=f(i+1,5); f(i,3)=f(i,4); step=abs(beta(i)-alfa(i))/4;
x=alfa(i)+step; f(i,2)=eval(fun); nfv=nfv+1; x=beta(i)-step;
f(i,4)=eval(fun); nfv=nfv+1;

398
9. Numerical Integration
end
end
9.8
Singular Integrals
In this section we extend our analysis to deal with singular integrals, arising
when f has Ô¨Ånite jumps or is even inÔ¨Ånite at some point. Besides, we
will consider the case of integrals of bounded functions over unbounded
intervals. We brieÔ¨Çy address the most relevant numerical techniques for
properly handling these integrals.
9.8.1
Integrals of Functions with Finite Jump Discontinuities
Let c be a known point within [a, b] and assume that f is a continuous and
bounded function in [a, c) and (c, b], with Ô¨Ånite jump f(c+) ‚àíf(c‚àí). Since
I(f) =
> b
a
f(x)dx =
> c
a
f(x)dx +
> b
c
f(x)dx,
(9.46)
any integration formula of the previous sections can be used on [a, c‚àí] and
[c+, b] to furnish an approximation of I(f). We proceed similarly if f admits
a Ô¨Ånite number of jump discontinuities within [a, b].
When the position of the discontinuity points of f is not known a priori,
a preliminary analysis of the graph of the function should be carried out.
Alternatively, one can resort to an adaptive integrator that is able to detect
the presence of discontinuities when the integration steplength falls below
a given tolerance (see Section 9.7.2).
9.8.2
Integrals of InÔ¨Ånite Functions
Let us deal with the case in which limx‚Üía+ f(x) = ‚àû; similar consider-
ations hold when f is inÔ¨Ånite as x ‚Üíb‚àí, while the case of a point of
singularity c internal to the interval [a, b] can be recast to one of the pre-
vious two cases owing to (9.46). Assume that the integrand function is of
the form
f(x) =
œÜ(x)
(x ‚àía)¬µ ,
0 ‚â§¬µ < 1,
where œÜ is a function whose absolute value is bounded by M. Then
|I(f)| ‚â§M lim
t‚Üía+
b
>
t
1
(x ‚àía)¬µ dx = M (b ‚àía)1‚àí¬µ
1 ‚àí¬µ
.

9.8 Singular Integrals
399
Suppose we wish to approximate I(f) up to a Ô¨Åxed tolerance Œ¥. For this, let
us describe the following two methods (for further details, see also [IK66],
Section 7.6, and [DR75], Section 2.12 and Appendix 1).
Method 1. For any Œµ such that 0 < Œµ < (b ‚àía), we write the singular
integral as I(f) = I1 + I2, where
I1 =
a+Œµ
>
a
œÜ(x)
(x ‚àía)¬µ dx,
I2 =
b
>
a+Œµ
œÜ(x)
(x ‚àía)¬µ dx.
The computation of I2 is not troublesome. After replacing œÜ by its p-th
order Taylor‚Äôs expansion around x = a, we obtain
œÜ(x) = Œ¶p(x) + (x ‚àía)p+1
(p + 1)! œÜ(p+1)(Œæ(x)),
p ‚â•0
(9.47)
where Œ¶p(x) = p
k=0 œÜ(k)(a)(x ‚àía)k/k!. Then
I1 = Œµ1‚àí¬µ
p

k=0
ŒµkœÜ(k)(a)
k!(k + 1 ‚àí¬µ) +
1
(p + 1)!
a+Œµ
>
a
(x ‚àía)p+1‚àí¬µœÜ(p+1)(Œæ(x))dx.
Replacing I1 by the Ô¨Ånite sum, the corresponding error E1 can be bounded
as
|E1| ‚â§
Œµp+2‚àí¬µ
(p + 1)!(p + 2 ‚àí¬µ)
max
a‚â§x‚â§a+Œµ|œÜ(p+1)(x)|,
p ‚â•0.
(9.48)
For Ô¨Åxed p, the right side of (9.48) is an increasing function of Œµ. On the
other hand, taking Œµ < 1 and assuming that the successive derivatives of œÜ
do not grow too much as p increases, the same function is decreasing as p
grows.
Let us next approximate I2 using a composite Newton-Cotes formula
with m subintervals and n quadrature nodes for each subinterval, n being
an even integer. Recalling (9.26) and aiming at equidistributing the error
Œ¥ between I1 and I2, it turns out that
|E2| ‚â§M(n+2)(Œµ)b ‚àía ‚àíŒµ
(n + 2)!
|Mn|
nn+3
b ‚àía ‚àíŒµ
m
n+2
= Œ¥/2,
(9.49)
where
M(n+2)(Œµ) =
max
a+Œµ‚â§x‚â§b

dn+2
dxn+2

œÜ(x)
(x ‚àía)¬µ
 .
The value of the constant M(n+2)(Œµ) grows rapidly as Œµ tends to zero; as
a consequence, (9.49) might require such a large number of subintervals
mŒµ = m(Œµ) to make the method at hand of little practical use.

400
9. Numerical Integration
Example 9.10 Consider the singular integral (known as the Fresnel integral)
I(f) =
œÄ/2
>
0
cos(x)
‚àöx dx.
(9.50)
Expanding the integrand function in a Taylor‚Äôs series around the origin and
applying the theorem of integration by series, we get
I(f) =
‚àû

k=0
(‚àí1)k
(2k)!
1
(2k + 1/2)(œÄ/2)2k+1/2.
Truncating the series at the Ô¨Årst 10 terms, we obtain an approximate value of
the integral equal to 1.9549.
Using the composite Cavalieri-Simpson formula, the a priori estimate (9.49)
yields, as Œµ tends to zero and letting n = 2, |M2| = 4/15,
mŒµ ‚âÉ
0.018
Œ¥
+œÄ
2 ‚àíŒµ
,5
Œµ‚àí9/2
1/4
.
For Œ¥ = 10‚àí4, taking Œµ = 10‚àí2, it turns out that 1140 (uniform) subintervals are
needed, while for Œµ = 10‚àí4 and Œµ = 10‚àí6 the number of subintervals is 2 ¬∑ 105
and 3.6 ¬∑ 107, respectively.
As a comparison, running Program 77 (adaptive integration with Cavalieri-
Simpson formula) with a = Œµ = 10‚àí10, hmin = 10‚àí12 and tol = 10‚àí4, we
get the approximate value 1.955 for the integral at the price of 1057 functional
evaluations, which correspond to 528 nonuniform subdivisions of the interval
[0, œÄ/2].
‚Ä¢
Method 2. Using the Taylor expansion (9.47) we obtain
I(f) =
b
>
a
œÜ(x) ‚àíŒ¶p(x)
(x ‚àía)¬µ
dx +
b
>
a
Œ¶p(x)
(x ‚àía)¬µ dx = I1 + I2.
Exact computation of I2 yields
I2 = (b ‚àía)1‚àí¬µ
p

k=0
(b ‚àía)kœÜ(k)(a)
k!(k + 1 ‚àí¬µ) .
(9.51)
The integral I1 is, for p ‚â•0
I1 =
b
>
a
(x ‚àía)p+1‚àí¬µ œÜ(p+1)(Œæ(x))
(p + 1)!
dx =
b
>
a
g(x)dx.
(9.52)
Unlike the case of method 1, the integrand function g does not blow up
at x = a, since its Ô¨Årst p derivatives are Ô¨Ånite at x = a. As a consequence,
assuming we approximate I1 using a composite Newton-Cotes formula, it is
possible to give an estimate of the quadrature error, provided that p ‚â•n+2,
for n ‚â•0 even, or p ‚â•n + 1, for n odd.

9.8 Singular Integrals
401
Example 9.11 Consider again the singular Fresnel integral (9.50), and assume
we use the composite Cavalieri-Simpson formula for approximating I1. We will
take p = 4 in (9.51) and (9.52). Computing I2 yields the value (œÄ/2)1/2(2 ‚àí
(1/5)(œÄ/2)2 + (1/108)(œÄ/2)4) ‚âÉ1.9588. Applying the error estimate (9.26) with
n = 2 shows that only 2 subdivisions of [0, œÄ/2] suÔ¨Éce for approximating I1 up
to an error Œ¥ = 10‚àí4, obtaining the value I1 ‚âÉ‚àí0.0173. As a whole, method 2
returns for (9.50) the approximate value 1.9415.
‚Ä¢
9.8.3
Integrals over Unbounded Intervals
Let f ‚ààC0([a, +‚àû)); should it exist and be Ô¨Ånite, the following limit
lim
t‚Üí+‚àû
t
>
a
f(x)dx
is taken as being the value of the singular integral
I(f) =
> ‚àû
a
f(x)dx =
lim
t‚Üí+‚àû
t
>
a
f(x)dx.
(9.53)
An analogous deÔ¨Ånition holds if f is continuous over (‚àí‚àû, b], while for a
function f : R ‚ÜíR, integrable over any bounded interval, we let
> ‚àû
‚àí‚àû
f(x)dx =
> c
‚àí‚àû
f(x)dx +
> +‚àû
c
f(x)dx
(9.54)
if c is any real number and the two singular integrals on the right hand side
of (9.54) are convergent. This deÔ¨Ånition is correct since the value of I(f)
does not depend on the choice of c.
A suÔ¨Écient condition for f to be integrable over [a, +‚àû) is that
‚àÉœÅ > 0, such that
lim
x‚Üí+‚àûx1+œÅf(x) = 0,
that is, we require f to be inÔ¨Ånitesimal of order > 1 with respect to 1/x
as x ‚Üí‚àû. For the numerical approximation of (9.53) up to a tolerance Œ¥,
we consider the following methods, referring for further details to [DR75],
Chapter 3.
Method 1. To compute (9.53), we can split I(f) as I(f) = I1 + I2, where
I1 =
 c
a f(x)dx and I2 =
 ‚àû
c
f(x)dx.
The end-point c, which can be taken arbitrarily, is chosen in such a way
that the contribution of I2 is negligible. Precisely, taking advantage of the
asymptotic behavior of f, c is selected to guarantee that I2 equals a fraction
of the Ô¨Åxed tolerance, say, I2 = Œ¥/2.
Then, I1 will be computed up to an absolute error equal to Œ¥/2. This
ensures that the global error in the computation of I1 + I2 is below the
tolerance Œ¥.

402
9. Numerical Integration
Example 9.12 Compute up to an error Œ¥ = 10‚àí3 the integral
I(f) =
‚àû
>
0
cos2(x)e‚àíxdx = 3/5.
For any given c > 0, we have I2 =
‚àû
>
c
cos2(x)e‚àíxdx ‚â§
> ‚àû
c
e‚àíxdx = e‚àíc; re-
quiring that e‚àíc = Œ¥/2, one gets c ‚âÉ7.6. Then, assuming we use the compos-
ite trapezoidal formula for approximating I1, thanks to (9.27) with n = 1 and
M = max0‚â§x‚â§c |f ‚Ä≤‚Ä≤(x)| ‚âÉ1.04, we obtain m ‚â•

Mc3/(6Œ¥)
1/2 = 277.
Program 72 returns the value I1 ‚âÉ0.599905, instead of the exact value I1 =
3/5 ‚àíe‚àíc(cos2(c) ‚àí(sin(2c) + 2 cos(2c))/5) ‚âÉ0.599842, with an absolute error of
about 6.27 ¬∑ 10‚àí5. The global numerical outcome is thus I1 + I2 ‚âÉ0.600405, with
an absolute error with respect to I(f) equal to 4.05 ¬∑ 10‚àí4.
‚Ä¢
Method 2. For any real number c, we let I(f) = I1 + I2, as for method 1,
then we introduce the change of variable x = 1/t in order to transform I2
into an integral over the bounded interval [0, 1/c]
I2 =
1/c
>
0
f(t)t‚àí2dt =
1/c
>
0
g(t)dt.
(9.55)
If g(t) is not singular at t = 0, (9.55) can be treated by any quadrature
formula introduced in this chapter. Otherwise, one can resort to the inte-
gration methods considered in Section 9.8.2.
Method 3. Gaussian interpolatory formulae are used, where the integra-
tion nodes are the zeros of Laguerre and Hermite orthogonal polynomials
(see Section 10.5).
9.9
Multidimensional Numerical Integration
Let ‚Ñ¶be a bounded domain in R2 with a suÔ¨Éciently smooth boundary. We
consider the problem of approximating the integral I(f) =
 
‚Ñ¶f(x, y)dxdy,
where f is a continuous function in ‚Ñ¶. For this purpose, in Sections 9.9.1
and 9.9.2 we address two methods.
The Ô¨Årst method applies when ‚Ñ¶is a normal domain with respect to a
coordinate axis. It is based on the reduction formula for double integrals
and consists of using one-dimensional quadratures along both coordinate
direction. The second method, which applies when ‚Ñ¶is a polygon, consists
of employing composite quadratures of low degree on a triangular decom-
position of the domain ‚Ñ¶. Section 9.9.3 brieÔ¨Çy addresses the Monte Carlo

9.9 Multidimensional Numerical Integration
403
method, which is particularly well-suited to integration in several dimen-
sions.
9.9.1
The Method of Reduction Formula
Let ‚Ñ¶be a normal domain with respect to the x axis, as drawn in Figure
9.5, and assume for the sake of simplicity that œÜ2(x) > œÜ1(x), ‚àÄx ‚àà[a, b].
FIGURE 9.5. Normal domain with respect to x axis
The reduction formula for double integrals gives (with obvious choice of
notation)
I(f) =
b
>
a
œÜ2(x)
>
œÜ1(x)
f(x, y)dydx =
b
>
a
Ff(x)dx.
(9.56)
The integral I(Ff) =
 b
a Ff(x)dx can be approximated by a composite
quadrature rule using Mx subintervals {Jk, k = 1, . . . , Mx}, of width H =
(b ‚àía)/Mx, and in each subinterval n(k)
x
+ 1 nodes {xk
i , i = 0, . . . , n(k)
x }.
Thus, in the x direction we can write
Ic
nx(f) =
Mx

k=1
n(k)
x

i=0
Œ±k
i Ff(xk
i ),
where the coeÔ¨Écients Œ±k
i are the quadrature weights on each subinterval Jk.
For each node xk
i , the approximate evaluation of the integral Ff(xk
i ) is then
carried out by a composite quadrature using My subintervals {Jm, m =
1, . . . , My}, of width hk
i = (œÜ2(xk
i ) ‚àíœÜ1(xk
i ))/My and in each subinterval
n(m)
y
+ 1 nodes {yi,k
j,m, j = 0, . . . , n(m)
y
}.
In the particular case Mx = My = M, n(k)
x
= n(m)
y
= 0, for k, m =
1, . . . , M, the resulting quadrature formula is the midpoint reduction for-
mula
Ic
0,0(f) = H
M

k=1
hk
0
M

m=1
f(xk
0, y0,k
0,m),

404
9. Numerical Integration
where H = (b ‚àía)/M, xk
0 = a + (k ‚àí1/2)H for k = 1, . . . , M and
y0,k
0,m = œÜ1(xk
0) + (m ‚àí1/2)hk
0 for m = 1, . . . , M. With a similar procedure
the trapezoidal reduction formula can be constructed along the coordinate
directions (in that case, n(k)
x
= n(m)
y
= 1, for k, m = 1, . . . , M).
The eÔ¨Éciency of the approach can obviously be increased by employing
the adaptive method described in Section 9.7.2 to suitably allocate the
quadrature nodes xk
i and yi,k
j,m according to the variations of f over the
domain ‚Ñ¶. The use of the reduction formulae above becomes less and less
convenient as the dimension d of the domain ‚Ñ¶‚äÇRd gets larger, due to
the large increase in the computational eÔ¨Äort. Indeed, if any simple integral
requires N functional evaluations, the overall cost would be equal to N d.
The midpoint and trapezoidal reduction formulae for approximating the
integral (9.56) are implemented in Programs 78 and 79. For the sake of
simplicity, we set Mx = My = M. The variables phi1 and phi2 contain
the expressions of the functions œÜ1 and œÜ2 which delimitate the integration
domain.
Program 78 - redmidpt : Midpoint reduction formula
function inte=redmidpt(a,b,phi1,phi2,m,fun)
H=(b-a)/m; xx=[a+H/2:H:b]; dim=max(size(xx));
for i=1:dim, x=xx(i); d=eval(phi2); c=eval(phi1); h=(d-c)/m;
y=[c+h/2:h:d]; w=eval(fun); psi(i)=h*sum(w(1:m)); end;
inte=H*sum(psi(1:m));
Program 79 - redtrap : Trapezoidal reduction formula
function inte=redtrap(a,b,phi1,phi2,m,fun)
H=(b-a)/m; xx=[a:H:b]; dim=max(size(xx));
for i=1:dim, x=xx(i); d=eval(phi2); c=eval(phi1); h=(d-c)/m;
y=[c:h:d]; w=eval(fun); psi(i)=h*(0.5*w(1)+sum(w(2:m))+0.5*w(m+1));
end; inte=H*(0.5*psi(1)+sum(psi(2:m))+0.5*psi(m+1));
9.9.2
Two-Dimensional Composite Quadratures
In this section we extend to the two-dimensional case the composite inter-
polatory quadratures that have been considered in Section 9.4. We assume
that ‚Ñ¶is a convex polygon on which we introduce a triangulation Th of NT
triangles or elements, such that ‚Ñ¶=
-
T ‚ààTh
T, where the parameter h > 0 is
the maximum edge-length in Th (see Section 8.5.2).
Exactly as happens in the one-dimensional case, interpolatory composite
quadrature rules on triangles can be devised by replacing
 
‚Ñ¶f(x, y)dxdy
with
 
‚Ñ¶Œ†k
hf(x, y)dxdy, where, for k ‚â•0, Œ†k
hf is the composite interpolat-
ing polynomial of f on the triangulation Th introduced in Section 8.5.2.

9.9 Multidimensional Numerical Integration
405
For an eÔ¨Écient evaluation of this last integral, we employ the property of
additivity which, combined with (8.38), leads to the following interpolatory
composite rule
Ic
k(f) =
>
‚Ñ¶
Œ†k
hf(x, y)dxdy =

T ‚ààTh
>
T
Œ†k
T f(x, y)dxdy =

T ‚ààTh
IT
k (f)
=

T ‚ààTh
dk‚àí1

j=0
f(ÀúzT
j )
>
T
lT
j (x, y)dxdy =

T ‚ààTh
dk‚àí1

j=0
Œ±T
j f(ÀúzT
j ).
(9.57)
The coeÔ¨Écients Œ±(j)
T
and the points Àúz(j)
T
are called the local weights and
nodes of the quadrature formula (9.57), respectively.
The weights Œ±(j)
T
can be computed on the reference triangle ÀÜT of vertices
(0, 0), (1, 0) and (0, 1), as follows
Œ±(j)
T
=
>
T
lj,T (x, y)dxdy = 2|T|
>
ÀÜT
ÀÜlj(ÀÜx, ÀÜy)dÀÜxdÀÜy,
j = 0, . . . , dk ‚àí1,
where |T| is the area of T. If k = 0, we get Œ±(0)
T
= |T|, while if k = 1 we
have Œ±(j)
T
= |T|/3, for j = 0, 1, 2.
Denoting respectively by a(j)
T
and aT = 3
j=1(a(j)
T )/3, for j = 1, 2, 3, the
vertices and the center of gravity of the triangle T ‚ààTh, the following
formulae are obtained.
Composite midpoint formula
Ic
0(f) =

T ‚ààTh
|T|f(aT ).
(9.58)
Composite trapezoidal formula
Ic
1(f) = 1
3

T ‚ààTh
|T|
3

j=1
f(a(j)
T ).
(9.59)
In view of the analysis of the quadrature error Ec
k(f) = I(f) ‚àíIc
k(f), we
introduce the following deÔ¨Ånition.
DeÔ¨Ånition 9.1 The quadrature formula (9.57) has degree of exactness
equal to n, with n ‚â•0, if I T
k (p) =
 
T pdxdy for any p ‚ààPn( T), where
Pn( T) is deÔ¨Åned in (8.35).
‚ñ†
The following result can be proved (see [IK66], pp. 361‚Äì362).

406
9. Numerical Integration
Property 9.4 Assume that the quadrature rule (9.57) has degree of exact-
ness on ‚Ñ¶equal to n, with n ‚â•0, and that its weights are all nonnegative.
Then, there exists a positive constant Kn, independent of h, such that
|Ec
k(f)| ‚â§Knhn+1|‚Ñ¶|Mn+1,
for any function f ‚ààCn+1(‚Ñ¶), where Mn+1 is the maximum value of the
modules of the derivatives of order n + 1 of f and |‚Ñ¶| is the area of ‚Ñ¶.
The composite formulae (9.58) and (9.59) both have degrees of exactness
equal to 1; then, due to Property 9.4, their order of inÔ¨Ånitesimal with
respect to h is equal to 2.
An alternative family of quadrature rules on triangles is provided by the so-
called symmetric formulae. These are Gaussian formulae with n nodes and
high degree of exactness, and exhibit the feature that the quadrature nodes
occupy symmetric positions with respect to all corners of the reference
triangle T or, as happens for Gauss-Radau formulae, with respect to the
straight line y = x.
Considering the generic triangle T ‚ààTh and denoting by aT
(j), j = 1, 2, 3,
the midpoints of the edges of T, two examples of symmetric formulae,
having degree of exactness equal to 2 and 3, respectively, are the following
I3(f) = |T|
3
3

j=1
f(aT
(j)),
n = 3,
I7(f) = |T|
60
Ô£´
Ô£≠3
3

i=1
f(a(i)
T ) + 8
3

j=1
f(aT
(j)) + 27f(aT )
Ô£∂
Ô£∏,
n = 7.
For a description and analysis of symmetric formulae for triangles, see
[Dun85], while we refer to [Kea86] and [Dun86] for their extension to tetra-
hedra and cubes.
The composite quadrature rules (9.58) and (9.59) are implemented in
Programs 80 and 81 for the approximate evaluation of the integral of f(x, y)
over a single triangle T ‚ààTh. To compute the integral over ‚Ñ¶it suÔ¨Éces
to sum the result provided by the program over each triangle of Th. The
coordinates of the vertices of the triangle T are stored in the arrays xv and
yv.
Program 80 - midptr2d : Midpoint rule on a triangle
function inte=midptr2d(xv,yv,fun)
y12=yv(1)-yv(2); y23=yv(2)-yv(3); y31=yv(3)-yv(1);
areat=0.5*abs(xv(1)*y23+xv(2)*y31+xv(3)*y12);
x=sum(xv)/3; y=sum(yv)/3; inte=areat*eval(fun);
Program 81 - traptr2d : Trapezoidal rule on a triangle

9.9 Multidimensional Numerical Integration
407
function inte=traptr2d(xv,yv,fun)
y12=yv(1)-yv(2); y23=yv(2)-yv(3); y31=yv(3)-yv(1);
areat=0.5*abs(xv(1)*y23+xv(2)*y31+xv(3)*y12); inte=0;
for i=1:3, x=xv(i); y=yv(i); inte=inte+eval(fun); end;
inte=inte*areat/3;
9.9.3
Monte Carlo Methods for Numerical Integration
Numerical integration methods based on Monte Carlo techniques are a valid
tool for approximating multidimensional integrals when the space dimen-
sion of Rn gets large. These methods diÔ¨Äer from the approaches considered
thus far, since the choice of quadrature nodes is done statistically accord-
ing to the values attained by random variables having a known probability
distribution.
The basic idea of the method is to interpret the integral as a statistic
mean value
>
‚Ñ¶
f(x)dx = |‚Ñ¶|
>
Rn
|‚Ñ¶|‚àí1œá‚Ñ¶(x)f(x)dx = |‚Ñ¶|¬µ(f),
where x = (x1, x2, . . . , xn)T and |‚Ñ¶| denotes the n-dimensional volume of
‚Ñ¶, œá‚Ñ¶(x) is the characteristic function of the set ‚Ñ¶, equal to 1 for x ‚àà‚Ñ¶and
to 0 elsewhere, while ¬µ(f) is the mean value of the function f(X), where
X is a random variable with uniform probability density |‚Ñ¶|‚àí1œá‚Ñ¶over Rn.
We recall that the random variable X ‚ààRn (or, more properly, random
vector) is an n-tuple of real numbers X1(Œ∂), . . . , Xn(Œ∂) assigned to every
outcome Œ∂ of a random experiment (see [Pap87], Chapter 4).
Having Ô¨Åxed a vector x ‚ààRn, the probability P{X ‚â§x} of the random
event {X1 ‚â§x1, . . . , Xn ‚â§xn} is given by
P{X ‚â§x} =
> x1
‚àí‚àû
. . .
> xn
‚àí‚àû
f(X1, . . . , Xn)dX1 . . . dXn
where f(X) = f(X1, . . . , Xn) is the probability density of the random vari-
able X ‚ààRn, such that
f(X1, . . . , Xn) ‚â•0,
>
Rn f(X1, . . . , Xn)dX = 1.
The numerical computation of the mean value ¬µ(f) is carried out by taking
N independent samples x1, . . . , xN ‚ààRn with probability density |‚Ñ¶|‚àí1œá‚Ñ¶
and evaluating their average
f N = 1
N
N

i=1
f(xi) = IN(f).
(9.60)

408
9. Numerical Integration
From a statistical standpoint, the samples x1, . . . , xN can be regarded as
the realizations of a sequence of N random variables {X1, . . . , XN}, mu-
tually independent and each with probability density |‚Ñ¶|‚àí1œá‚Ñ¶.
For such a sequence the strong law of large numbers ensures with prob-
ability 1 the convergence of the average IN(f) =
+N
i=1 f(Xi)
,
/N to
the mean value ¬µ(f) as N ‚Üí‚àû. In computational practice the sequence
of samples x1, . . . , xN is deterministically produced by a random-number
generator, giving rise to the so-called pseudo-random integration formulae.
The quadrature error EN(f) = ¬µ(f) ‚àíIN(f) as a function of N can be
characterized through the variance
œÉ(IN(f)) =

¬µ (IN(f) ‚àí¬µ(f))2.
Interpreting again f as a function of the random variable X, distributed
with uniform probability density |‚Ñ¶|‚àí1 in ‚Ñ¶‚äÜRn and variance œÉ(f), we
have
œÉ(IN(f)) = œÉ(f)
‚àö
N
,
(9.61)
from which, as N ‚Üí‚àû, a convergence rate of O(N ‚àí1/2) follows for the
statistical estimate of the error œÉ(IN(f)). Such convergence rate does not
depend on the dimension n of the integration domain, and this is a most
relevant feature of the Monte Carlo method. However, it is worth noting
that the convergence rate is independent of the regularity of f; thus, un-
like interpolatory quadratures, Monte Carlo methods do not yield more
accurate results when dealing with smooth integrands.
The estimate (9.61) is extremely weak and in practice one does often
obtain poorly accurate results. A more eÔ¨Écient implementation of Monte
Carlo methods is based on composite approach or semi-analytical methods;
an example of these techniques is provided in [ NAG95], where a composite
Monte Carlo method is employed for the computation of integrals over
hypercubes in Rn.
9.10
Applications
We consider in the next sections the computation of two integrals suggested
by applications in geometry and the mechanics of rigid bodies.
9.10.1
Computation of an Ellipsoid Surface
Let E be the ellipsoid obtained by rotating the ellipse in Figure 9.6 around
the x axis, where the radius œÅ is described as a function of the axial coor-

9.10 Applications
409
x )
œÅ
1/Œ≤
-
1/Œ≤
(
x
E
FIGURE 9.6. Section of the ellipsoid
dinate by the equation
œÅ2(x) = Œ±2(1 ‚àíŒ≤2x2),
‚àí1
Œ≤ ‚â§x ‚â§1
Œ≤ ,
Œ± and Œ≤ being given constants, assigned in such a way that Œ±2Œ≤2 < 1.
We set the following values for the parameters: Œ±2 = (3 ‚àí2
‚àö
2)/100 and
Œ≤2 = 100. Letting K2 = Œ≤2
1 ‚àíŒ±2Œ≤2, f(x) =
‚àö
1 ‚àíK2x2 and Œ∏ =
cos‚àí1(K/Œ≤), the computation of the surface of E requires evaluating the
integral
I(f) = 4œÄŒ±
1/Œ≤
>
0
f(x)dx = 2œÄŒ±
K [(œÄ/2 ‚àíŒ∏) + sin(2Œ∏)/2] .
(9.62)
Notice that f ‚Ä≤(1/Œ≤) = ‚àí100; this prompts us to use a numerical adaptive
formula able to provide a nonuniform distribution of quadrature nodes,
with a possible reÔ¨Ånement of these nodes around x = 1/Œ≤.
Table 9.12 summarizes the results obtained using the composite midpoint,
trapezoidal and Cavalieri-Simpson rules (respectively denoted by (MP),
(TR) and (CS)), which are compared with Romberg integration (RO) and
with the adaptive Cavalieri-Simpson quadrature introduced in Section 9.7.2
and denoted by (AD).
In the table, m is the number of subintervals, while Err and flops denote
the absolute quadrature error and the number of Ô¨Çoating-point operations
required by each algorithm, respectively. In the case of the AD method,
we have run Program 77 taking hmin=10‚àí5 and tol=10‚àí8, while for the
Romberg method we have used Program 76 with n=9.
The results demonstrate the advantage of using the composite adaptive
Cavalieri-Simpson formula, both in terms of computational eÔ¨Éciency and
accuracy, as can be seen in the plots in Figure 9.7 which allow to check
the successful working of the adaptivity procedure. In Figure 9.7 (left),
we show, together with the graph of f, the nonuniform distribution of
the quadrature nodes on the x axis, while in Figure 9.7 (right) we plot
the logarithmic graph of the integration step density (piecewise constant)
‚àÜh(x), deÔ¨Åned as the inverse of the value of the stepsize h on each active
interval A (see Section 9.7.2).

410
9. Numerical Integration
Notice the high value of ‚àÜh at x = 1/Œ≤, where the derivative of the
integrand function is maximum.
(PM)
(TR)
(CS)
(RO)
(AD)
m
4000
5600
250
50
Err
3.24e ‚àí10
3.30e ‚àí10
2.98e ‚àí10
3.58e ‚àí11
3.18e ‚àí10
flops
20007
29013
2519
5772
3540
TABLE 9.12. Methods for approximating I(f) = 4œÄŒ±
 1/Œ≤
0
‚àö
1 ‚àíK2x2dx, with
Œ±2 = (3 ‚àí2
‚àö
2)/100, Œ≤ = 10 and K2 =

Œ≤2(1 ‚àíŒ±2Œ≤2)
0
0.02
0.04
0.06
0.08
0.1
‚àí0.2
0
0.2
0.4
0.6
0.8
1
0
0.02
0.04
0.06
0.08
0.1
10
2
10
3
10
4
10
5
FIGURE 9.7. Distribution of quadrature nodes (left); integration stepsize density
in the approximation of integral (9.62) (right)
9.10.2
Computation of the Wind Action on a Sailboat Mast
Let us consider the sailboat schematically drawn in Figure 9.8 (left) and
subject to the action of the wind force. The mast, of length L, is denoted by
the straight line AB, while one of the two shrouds (strings for the side stiÔ¨Ä-
ening of the mast) is represented by the straight line BO. Any inÔ¨Ånitesimal
element of the sail transmits to the corresponding element of length dx of
the mast a force of magnitude equal to f(x)dx. The change of f along with
the height x, measured from the point A (basis of the mast), is expressed
by the following law
f(x) =
Œ±x
x + Œ≤ e‚àíŒ≥x,
where Œ±, Œ≤ and Œ≥ are given constants.

9.10 Applications
411
The resultant R of the force f is deÔ¨Åned as
R =
L
>
0
f(x)dx ‚â°I(f),
(9.63)
and is applied at a point at distance equal to b (to be determined) from
the basis of the mast.
                              


dx
wind
direction
mast
shroud
T
A
O
dx
B
f
L
A
O
V
B
T
M
H
b
R
FIGURE 9.8. Schematic representation of a sailboat (left); forces acting on the
mast (right)
Computing R and the distance b, given by b = I(xf)/I(f), is crucial for the
structural design of the mast and shroud sections. Indeed, once the values
of R and b are known, it is possible to analyze the hyperstatic structure
mast-shroud (using for instance the method of forces), thus allowing for the
computation of the reactions V , H and M at the basis of the mast and the
traction T that is transmitted by the shroud, and are drawn in Figure 9.8
(right). Then, the internal actions in the structure can be found, as well as
the maximum stresses arising in the mast AB and in the shroud BO, from
which, assuming that the safety veriÔ¨Åcations are satisÔ¨Åed, one can Ô¨Ånally
design the geometrical parameters of the sections of AB and BO.
For the approximate computation of R we have considered the compos-
ite midpoint, trapezoidal and Cavalieri-Simpson rules, denoted henceforth
by (MP), (TR) and (CS), and, for a comparison, the adaptive Cavalieri-
Simpson quadrature formula introduced in Section 9.7.2 and denoted by
(AD). Since a closed-form expression for the integral (9.63) is not available,
the composite rules have been applied taking mk = 2k uniform partitions
of [0, L], with k = 0, . . . , 15.
We have assumed in the numerical experiments Œ± = 50, Œ≤ = 5/3 and
Œ≥ = 1/4 and we have run Program 77 taking tol=10‚àí4 and hmin=10‚àí3.
The sequence of integrals computed using the composite formulae has been
stopped at k = 12 (corresponding to mk = 212 = 4096) since the remaining

412
9. Numerical Integration
0
20
40
60
80
100
120
10
‚àí9
10
‚àí8
10
‚àí7
10
‚àí6
10
‚àí5
10
‚àí4
10
‚àí3
10
‚àí2
10
‚àí1
10
0
(TR)
(PM)
(CS)
(AD)
FIGURE 9.9. Relative errors in the approximate computation of the integral
 L
0 (Œ±xe‚àíŒ≥x)/(x + Œ≤)dx
elements, in the case of formula CS, diÔ¨Äer among them only up to the last
signiÔ¨Åcant Ô¨Ågure. Therefore, we have assumed as the exact value of I(f)
the outcome I(CS)
12
= 100.0613683179612 provided by formula CS.
We report in Figure 9.9 the logarithmic plots of the relative error |I(CS)
12
‚àí
Ik|/I12, for k = 0, . . . , 7, Ik being the generic element of the sequence for
the three considered formulae. As a comparison, we also display the graph
of the relative error in the case of formula AD, applied on a number of
(nonuniform) partitions equivalent to that of the composite rules.
Notice how, for the same number of partitions, formula AD is more accu-
rate, with a relative error of 2.06 ¬∑ 10‚àí7 obtained using 37 (nonuniform)
partitions of [0, L]. Methods PM and TR achieve a comparable accuracy
employing 2048 and 4096 uniform subintervals, respectively, while formula
CS requires about 64 partitions. The eÔ¨Äectiveness of the adaptivity pro-
cedure is demonstrated by the plots in Figure 9.10, which show, together
with the graph of f, the distribution of the quadrature nodes (left) and the
function ‚àÜh(x) (right) that expresses the (piecewise constant) density of
the integration stepsize h, deÔ¨Åned as the inverse of the value of h over each
active interval A (see Section 9.7.2).
Notice also the high value of ‚àÜh at x = 0, where the derivatives of f are
maximum.
9.11
Exercises
1. Let E0(f) and E1(f) be the quadrature errors in (9.6) and (9.12). Prove
that |E1(f)| ‚âÉ2|E0(f)|.
2. Check that the error estimates for the midpoint, trapezoidal and Cavalieri-
Simpson formulae, given respectively by (9.6), (9.12) and (9.16), are special
instances of (9.19) or (9.20). In particular, show that M0 = 2/3, K1 =

9.11 Exercises
413
0
2
4
6
8
10
‚àí5
0
5
10
15
20
0
2
4
6
8
10
0
5
10
15
20
25
30
FIGURE 9.10. Distribution of quadrature nodes (left); integration step density
in the approximation of the integral
 L
0 (Œ±xe‚àíŒ≥x)/(x + Œ≤)dx (right)
‚àí1/6 and M2 = ‚àí4/15 and determine, using the deÔ¨Ånition, the degree of
exactness r of each formula.
[Hint: Ô¨Ånd r such that In(xk) =
 b
a xkdx, for k = 0, . . . , r, and In(xj) Ã∏=
 b
a xjdx, for j > r.]
3. Let In(f) = n
k=0 Œ±kf(xk) be a Lagrange quadrature formula on n + 1
nodes. Compute the degree of exactness r of the formulae:
(a) I2(f) = (2/3)[2f(‚àí1/2) ‚àíf(0) + 2f(1/2)],
(b) I4(f) = (1/4)[f(‚àí1) + 3f(‚àí1/3) + 3f(1/3) + f(1)].
Which is the order of inÔ¨Ånitesimal p for (a) and (b)?
[Solution: r = 3 and p = 5 for both I2(f) and I4(f).]
4. Compute df[x0, . . . , xn, x]/dx by checking (9.22).
[Hint: proceed by computing directly the derivative at x as an incremental
ratio, in the case where only one node x0 exists, then upgrade progressively
the order of the divided diÔ¨Äerence.]
5. Let Iw(f) =
 1
0 w(x)f(x)dx with w(x) = ‚àöx, and consider the quadrature
formula Q(f) = af(x1). Find a and x1 in such a way that Q has maximum
degree of exactness r.
[Solution: a = 2/3, x1 = 3/5 and r = 1.]
6. Let us consider the quadrature formula Q(f) = Œ±1f(0) + Œ±2f(1) + Œ±3f ‚Ä≤(0)
for the approximation of I(f) =
 1
0 f(x)dx, where f ‚ààC1([0, 1]). Determine
the coeÔ¨Écients Œ±j, for j = 1, 2, 3 in such a way that Q has degree of
exactness r = 2.
[Solution: Œ±1 = 2/3, Œ±2 = 1/3 and Œ±3 = 1/6.]
7. Apply the midpoint, trapezoidal and Cavalieri-Simpson composite rules to
approximate the integral
> 1
‚àí1
|x|exdx,
and discuss their convergence as a function of the size H of the subintervals.

414
9. Numerical Integration
8. Consider the integral I(f) =
 1
0 exdx and estimate the minimum number m
of subintervals that is needed for computing I(f) up to an absolute error
‚â§5 ¬∑ 10‚àí4 using the composite trapezoidal (TR) and Cavalieri-Simpson
(CS) rules. Evaluate in both cases the absolute error Err that is actually
made.
[Solution: for TR, we have m = 17 and Err = 4.95 ¬∑ 10‚àí4, while for CS,
m = 2 and Err = 3.70 ¬∑ 10‚àí5.]
9. Consider the corrected trapezoidal formula (9.30) and check that |Ecorr
1
(f)| ‚âÉ
4|E2(f)|, where Ecorr
1
(f) and E2(f) are deÔ¨Åned in (9.31) and (9.16), re-
spectively.
10. Compute, with an error less than 10‚àí4, the following integrals:
(a)
 ‚àû
0
sin(x)/(1 + x4)dx;
(b)
 ‚àû
0
e‚àíx(1 + x)‚àí5dx;
(c)
 ‚àû
‚àí‚àûcos(x)e‚àíx2dx.
11. Use the reduction midpoint and trapezoidal formulae for computing the
double integral I(f) =
 
‚Ñ¶
y
(1 + xy)dxdy over the domain ‚Ñ¶= (0, 1)2. Run
Programs 78 and 79 with M = 2i, for i = 0, . . . , 10 and plot in log-scale
the absolute error in the two cases as a function of M. Which method is
the most accurate? How many functional evaluations are needed to get an
(absolute) accuracy of the order of 10‚àí6?
[Solution: the exact integral is I(f) = log(4) ‚àí1, and almost 2002 = 40000
functional evaluations are needed.]

10
Orthogonal Polynomials in
Approximation Theory
Trigonometric polynomials, as well as other orthogonal polynomials like
Legendre‚Äôs and Chebyshev‚Äôs, are widely employed in approximation theory.
This chapter addresses the most relevant properties of orthogonal poly-
nomials, and introduces the transforms associated with them, in particular
the discrete Fourier transform and the FFT, but also the Zeta and Wavelet
transforms.
Application to interpolation, least-squares approximation, numerical dif-
ferentiation and Gaussian integration are addressed.
10.1
Approximation of Functions by Generalized
Fourier Series
Let w = w(x) be a weight function on the interval (‚àí1, 1), i.e., a nonneg-
ative integrable function in (‚àí1, 1). Let us denote by {pk, k = 0, 1, . . . } a
system of algebraic polynomials, with pk of degree equal to k for each k,
mutually orthogonal on the interval (‚àí1, 1) with respect to w. This means
that
1
>
‚àí1
pk(x)pm(x)w(x)dx = 0
if k Ã∏= m.

416
10. Orthogonal Polynomials in Approximation Theory
Set (f, g)w =
 1
‚àí1 f(x)g(x)w(x)dx and ‚à•f‚à•w = (f, f)1/2
w ; (¬∑, ¬∑)w and ‚à•¬∑ ‚à•w
are respectively the scalar product and the norm for the function space
L2
w = L2
w(‚àí1, 1) =
%
f : (‚àí1, 1) ‚ÜíR,
> 1
‚àí1
f 2(x)w(x)dx < ‚àû
&
.
(10.1)
For any function f ‚ààL2
w the series
Sf =
+‚àû

k=0
fkpk,
with fk = (f, pk)w
‚à•pk‚à•2w
,
is called the generalized Fourier series of f, and fk is the k-th Fourier
coeÔ¨Écient. As is well-known, Sf converges in average (or in the sense of
L2
w) to f. This means that, letting for any integer n
fn(x) =
n

k=0
fkpk(x)
(10.2)
(fn ‚ààPn is the truncation of order n of the generalized Fourier series of
f), the following convergence result holds
lim
n‚Üí+‚àû‚à•f ‚àífn‚à•w = 0.
Thanks to Parseval‚Äôs equality, we have
‚à•f‚à•2
w =
+‚àû

k=0
f 2
k‚à•pk‚à•2
w
and, for any n, ‚à•f‚àífn‚à•2
w = +‚àû
k=n+1 f 2
k‚à•pk‚à•2
w is the square of the remainder
of the generalized Fourier series.
The polynomial fn ‚ààPn satisÔ¨Åes the following minimization property
‚à•f ‚àífn‚à•w = min
q‚ààPn‚à•f ‚àíq‚à•w.
(10.3)
Indeed, since f ‚àífn = +‚àû
k=n+1 fkpk, the property of orthogonality of
polynomials {pk} implies (f ‚àífn, q)w = 0 ‚àÄq ‚ààPn. Then, the Cauchy-
Schwarz inequality (8.29) yields
‚à•f ‚àífn‚à•2
w
=
(f ‚àífn, f ‚àífn)w = (f ‚àífn, f ‚àíq)w + (f ‚àífn, q ‚àífn)w
=
(f ‚àífn, f ‚àíq)w ‚â§‚à•f ‚àífn‚à•w‚à•f ‚àíq‚à•w,
‚àÄq ‚ààPn,
and (10.3) follows since q is arbitrary in Pn. In such a case, we say that fn
is the orthogonal projection of f over Pn in the sense of L2
w. It is therefore
interesting to compute the coeÔ¨Écients fk of fn. As will be seen in later

10.1 Approximation of Functions by Generalized Fourier Series
417
sections, this is usually done by suitably approximating the integrals that
appear in the deÔ¨Ånition of fk. By doing so, one gets the so-called discrete
coeÔ¨Écients Àúfk of f, and, as a consequence, the new polynomial
f ‚àó
n(x) =
n

k=0
Àúfkpk(x)
(10.4)
which is called the discrete truncation of order n of the Fourier series of f.
Typically,
Àúfk = (f, pk)n
‚à•pk‚à•2n
,
(10.5)
where, for any pair of continuous functions f and g, (f, g)n is the approxi-
mation of the scalar product (f, g)w and ‚à•g‚à•n =

(g, g)n is the seminorm
associated with (¬∑, ¬∑)w. In a manner analogous to what was done for fn, it
can be checked that
‚à•f ‚àíf ‚àó
n‚à•n = min
q‚ààPn‚à•f ‚àíq‚à•n
(10.6)
and we say that f ‚àó
n is the approximation to f in Pn in the least-squares
sense (the reason for using this name will be made clear later on).
We conclude this section by recalling that, for any family of monic orthog-
onal polynomials {pk}, the following recursive three-term formula holds (for
the proof, see for instance [Gau96])
"
pk+1(x) = (x ‚àíŒ±k)pk(x) ‚àíŒ≤kpk‚àí1(x)
k ‚â•0,
p‚àí1(x) = 0,
p0(x) = 1,
(10.7)
where
Œ±k = (xpk, pk)w
(pk, pk)w
,
Œ≤k+1 = (pk+1, pk+1)w
(pk, pk)w
,
k ‚â•0.
(10.8)
Since p‚àí1 = 0, the coeÔ¨Écient Œ≤0 is arbitrary and is chosen according to
the particular family of orthogonal polynomials at hand. The recursive
three-term relation is generally quite stable and can thus be conveniently
employed in the numerical computation of orthogonal polynomials, as will
be seen in Section 10.6.
In the forthcoming sections we introduce two relevant families of orthogonal
polynomials.
10.1.1
The Chebyshev Polynomials
Consider the Chebyshev weight function w(x) = (1‚àíx2)‚àí1/2 on the interval
(‚àí1, 1), and, according to (10.1), introduce the space of square-integrable

418
10. Orthogonal Polynomials in Approximation Theory
functions with respect to the weight w
L2
w(‚àí1, 1) =
%
f : (‚àí1, 1) ‚ÜíR :
> 1
‚àí1
f 2(x)(1 ‚àíx2)‚àí1/2dx < ‚àû
&
.
A scalar product and a norm for this space are deÔ¨Åned as
(f, g)w =
1
>
‚àí1
f(x)g(x)(1 ‚àíx2)‚àí1/2dx,
‚à•f‚à•w =
Ô£±
Ô£≤
Ô£≥
1
>
‚àí1
f 2(x)(1 ‚àíx2)‚àí1/2dx
Ô£º
Ô£Ω
Ô£æ
1/2
.
(10.9)
The Chebyshev polynomials are deÔ¨Åned as follows
Tk(x) = cos kŒ∏,
Œ∏ = arccos x,
k = 0, 1, 2, . . .
(10.10)
They can be recursively generated by the following formula (a consequence
of (10.7), see [DR75], pp. 25-26)
Ô£±
Ô£≤
Ô£≥
Tk+1(x) = 2xTk(x) ‚àíTk‚àí1(x)
k = 1, 2, . . .
T0(x) = 1,
T1(x) = x.
(10.11)
In particular, for any k ‚â•0, we notice that Tk ‚ààPk, i.e., Tk(x) is an alge-
braic polynomial of degree k with respect to x. Using well-known trigono-
metric relations, we have
(Tk, Tn)w = 0 if k Ã∏= n,
(Tn, Tn)w =
" c0 = œÄ
if n = 0,
cn = œÄ/2
if n Ã∏= 0,
which expresses the orthogonality of the Chebyshev polynomials with re-
spect to the scalar product (¬∑, ¬∑)w. Therefore, the Chebyshev series of a
function f ‚ààL2
w takes the form
Cf =
‚àû

k=0
fkTk,
with
fk = 1
ck
1
>
‚àí1
f(x)Tk(x)(1 ‚àíx2)‚àí1/2dx.
Notice that ‚à•Tn‚à•‚àû= 1 for every n and the following minimax property
holds
‚à•21‚àínTn‚à•‚àû‚â§min
p‚ààP1
n
‚à•p‚à•‚àû,
where P1
n = {p(x) = n
k=0 akxk, an = 1} denotes the subset of polynomials
of degree n with leading coeÔ¨Écient equal to 1.

10.2 Gaussian Integration and Interpolation
419
10.1.2
The Legendre Polynomials
The Legendre polynomials are orthogonal polynomials over the interval
(‚àí1, 1) with respect to the weight function w(x) = 1. For these polynomials,
L2
w is the usual L2(‚àí1, 1) space introduced in (8.25), while (¬∑, ¬∑)w and ‚à•¬∑‚à•w
coincide with the scalar product and norm in L2(‚àí1, 1), respectively given
by
(f, g) =
1
>
‚àí1
f(x)g(x) dx,
‚à•f‚à•L2(‚àí1,1) =
Ô£´
Ô£≠
1
>
‚àí1
f 2(x) dx
Ô£∂
Ô£∏
1
2
.
The Legendre polynomials are deÔ¨Åned as
Lk(x) = 1
2k
[k/2]

l=0
(‚àí1)l
 k
l
  2k ‚àí2l
k

xk‚àí2l
k = 0, 1, . . . (10.12)
where [k/2] is the integer part of k/2, or, recursively, through the three-
term relation
Ô£±
Ô£¥
Ô£≤
Ô£¥
Ô£≥
Lk+1(x) = 2k + 1
k + 1 xLk(x) ‚àí
k
k + 1Lk‚àí1(x)
k = 1, 2 . . .
L0(x) = 1,
L1(x) = x.
For every k = 0, 1 . . . , Lk ‚ààPk and (Lk, Lm) = Œ¥km(k + 1/2)‚àí1 for k, m =
0, 1, 2, . . . . For any function f ‚ààL2(‚àí1, 1), its Legendre series takes the
following form
Lf =
‚àû

k=0
fkLk,
with
fk =

k + 1
2
‚àí1
1
>
‚àí1
f(x)Lk(x)dx.
Remark 10.1 (The Jacobi polynomials) The polynomials previously
introduced belong to the wider family of Jacobi polynomials {JŒ±Œ≤
k , k =
0, . . . , n}, that are orthogonal with respect to the weight w(x) = (1 ‚àí
x)Œ±(1 + x)Œ≤, for Œ±, Œ≤ > ‚àí1. Indeed, setting Œ± = Œ≤ = 0 we recover the
Legendre polynomials, while choosing Œ± = Œ≤ = ‚àí1/2 gives the Chebyshev
polynomials.
‚ñ†
10.2
Gaussian Integration and Interpolation
Orthogonal polynomials play a crucial role in devising quadrature formulae
with maximal degrees of exactness. Let x0, . . . , xn be n + 1 given distinct
points in the interval [‚àí1, 1]. For the approximation of the weighted integral

420
10. Orthogonal Polynomials in Approximation Theory
Iw(f) =
 1
‚àí1 f(x)w(x)dx, being f ‚ààC0([‚àí1, 1]), we consider quadrature
rules of the type
In,w(f) =
n

i=0
Œ±if(xi)
(10.13)
where Œ±i are coeÔ¨Écients to be suitably determined. Obviously, both nodes
and weights depend on n, however this dependence will be understood.
Denoting by
En,w(f) = Iw(f) ‚àíIn,w(f)
the error between the exact integral and its approximation (10.13), if
En,w(p) = 0 for any p ‚ààPr (for a suitable r ‚â•0) we shall say that for-
mula (10.13) has degree of exactness r with respect to the weight w. This
deÔ¨Ånition generalizes the one given for ordinary integration with weight
w = 1.
Clearly, we can get a degree of exactness equal to (at least) n taking
In,w(f) =
1
>
‚àí1
Œ†nf(x)w(x)dx
where Œ†nf ‚ààPn is the Lagrange interpolating polynomial of the function
f at the nodes {xi, i = 0, . . . , n}, given by (8.4). Therefore, (10.13) has
degree of exactness at least equal to n taking
Œ±i =
1
>
‚àí1
li(x)w(x)dx,
i = 0, . . . , n,
(10.14)
where li ‚ààPn is the i-th characteristic Lagrange polynomial such that
li(xj) = Œ¥ij, for i, j = 0, . . . , n.
The question that arises is whether suitable choices of the nodes exist
such that the degree of exactness is greater than n, say, equal to r = n+m
for some m > 0. The answer to this question is furnished by the following
theorem, due to Jacobi [Jac26].
Theorem 10.1 For a given m > 0, the quadrature formula (10.13) has
degree of exactness n + m iÔ¨Äit is of interpolatory type and the nodal poly-
nomial œân+1 (8.6) associated with the nodes {xi} is such that
1
>
‚àí1
œân+1(x)p(x)w(x)dx = 0,
‚àÄp ‚ààPm‚àí1.
(10.15)
Proof. Let us prove that these conditions are suÔ¨Écient. If f ‚ààPn+m then
there exist a quotient œÄm‚àí1 ‚ààPm‚àí1 and a remainder qn ‚ààPn, such that f =

10.2 Gaussian Integration and Interpolation
421
œân+1œÄm‚àí1 + qn. Since the degree of exactness of an interpolatory formula with
n + 1 nodes is equal to n (at least), we get
n

i=0
Œ±iqn(xi) =
1
>
‚àí1
qn(x)w(x)dx =
1
>
‚àí1
f(x)w(x)dx ‚àí
1
>
‚àí1
œân+1(x)œÄm‚àí1(x)w(x)dx.
As a consequence of (10.15), the last integral is null, thus
1
>
‚àí1
f(x)w(x)dx =
n

i=0
Œ±iqn(xi) =
n

i=0
Œ±if(xi).
Since f is arbitrary, we conclude that En,w(f) = 0 for any f ‚ààPn+m. Proving
that the conditions are also necessary is an exercise left to the reader.
3
Corollary 10.1 The maximum degree of exactness of the quadrature for-
mula (10.13) is 2n + 1.
Proof. If this would not be true, one could take m ‚â•n + 2 in the previous
theorem. This, in turn, would allow us to choose p = œân+1 in (10.15) and come
to the conclusion that œân+1 is identically zero, which is absurd.
3
Setting m = n + 1 (the maximum admissible value), from (10.15) we get
that the nodal polynomial œân+1 satisÔ¨Åes the relation
1
>
‚àí1
œân+1(x)p(x)w(x)dx = 0,
‚àÄp ‚ààPn.
Since œân+1 is a polynomial of degree n + 1 orthogonal to all the polyno-
mials of lower degree, we conclude that œân+1 is the only monic polynomial
multiple of pn+1 (recall that {pk} is the system of orthogonal polynomials
introduced in Section 10.1). In particular, its roots {xj} coincide with those
of pn+1, that is
pn+1(xj) = 0,
j = 0, . . . , n.
(10.16)
The abscissae {xj} are the Gauss nodes associated with the weight func-
tion w(x). We can thus conclude that the quadrature formula (10.13) with
coeÔ¨Écients and nodes given by (10.14) and (10.16), respectively, has degree
of exactness 2n + 1, the maximum value that can be achieved using inter-
polatory quadrature formulae with n + 1 nodes, and is called the Gauss
quadrature formula.
Its weights are all positive and the nodes are internal to the interval
(‚àí1, 1) (see, for instance, [CHQZ88], p. 56). However, it is often useful to
also include the end points of the interval among the quadrature nodes. By

422
10. Orthogonal Polynomials in Approximation Theory
doing so, the Gauss formula with the highest degree of exactness is the one
that employs as nodes the n + 1 roots of the polynomial
œân+1(x) = pn+1(x) + apn(x) + bpn‚àí1(x),
(10.17)
where the constants a and b are selected in such a way that œân+1(‚àí1) =
œân+1(1) = 0.
Denoting these roots by x0 = ‚àí1, x1, . . . , xn = 1, the coeÔ¨Écients {Œ±i, i =
0, . . . , n} can then be obtained from the usual formulae (10.14), that is
Œ±i =
1
>
‚àí1
li(x)w(x)dx,
i = 0, . . . , n,
where li ‚ààPn is the i-th characteristic Lagrange polynomial such that
li(xj) = Œ¥ij, for i, j = 0, . . . , n. The quadrature formula
IGL
n,w(f) =
n

i=0
Œ±if(xi)
(10.18)
is called the Gauss-Lobatto formula with n + 1 nodes, and has degree of
exactness 2n ‚àí1. Indeed, for any f ‚ààP2n‚àí1, there exist a polynomial
œÄn‚àí2 ‚ààPn‚àí2 and a remainder qn ‚ààPn such that f = œân+1œÄn‚àí2 + qn.
The quadrature formula (10.18) has degree of exactness at least equal to
n (being interpolatory with n + 1 distinct nodes), thus we get
n

j=0
Œ±jqn(xj) =
1
>
‚àí1
qn(x)w(x)dx =
1
>
‚àí1
f(x)w(x)dx ‚àí
1
>
‚àí1
œân+1(x)œÄn‚àí2(x)w(x)dx.
From (10.17) we conclude that ¬Øœân+1 is orthogonal to all the polynomials
of degree ‚â§n ‚àí2, so that the last integral is null. Moreover, since f(xj) =
qn(xj) for j = 0, . . . , n, we conclude that
1
>
‚àí1
f(x)w(x)dx =
n

i=0
Œ±if(xi),
‚àÄf ‚ààP2n‚àí1.
Denoting by Œ†GL
n,wf the polynomial of degree n that interpolates f at the
nodes {xj, j = 0, . . . , n}, we get
Œ†GL
n,wf(x) =
n

i=0
f(xi)li(x)
(10.19)
and thus IGL
n,w(f) =
 1
‚àí1 Œ†GL
n,wf(x)w(x)dx.

10.2 Gaussian Integration and Interpolation
423
Remark 10.2 In the special case where the Gauss-Lobatto quadrature is
considered with respect to the Jacobi weight w(x) = (1 ‚àíx)Œ±(1 ‚àíx)Œ≤,
with Œ±, Œ≤ > ‚àí1, the internal nodes x1, . . . , xn‚àí1 can be identiÔ¨Åed as the
roots of the polynomial (J(Œ±,Œ≤)
n
)‚Ä≤, that is, the extremants of the n-th Jacobi
polynomial J(Œ±,Œ≤)
n
(see [CHQZ88], pp. 57-58).
‚ñ†
The following convergence result holds for Gaussian integration (see [Atk89],
Chapter 5)
lim
n‚Üí+‚àû

1
>
‚àí1
f(x)w(x)dx ‚àí
n

j=0
Œ±jf(xj)

= 0,
‚àÄf ‚ààC0([‚àí1, 1]).
A similar result also holds for Gauss-Lobatto integration. If the integrand
function is not only continuous, but also diÔ¨Äerentiable up to the order
p ‚â•1, we shall see that Gaussian integration converges with an order of
inÔ¨Ånitesimal with respect to 1/n that is larger when p is greater. In the
forthcoming sections, the previous results will be speciÔ¨Åed in the cases of
the Chebyshev and Legendre polynomials.
Remark 10.3 (Integration over an arbitrary interval) A quadrature
formula with nodes Œæj and coeÔ¨Écients Œ≤j, j = 0, . . . , n over the interval
[‚àí1, 1] can be mapped on any interval [a, b]. Indeed, let œï : [‚àí1, 1] ‚Üí[a, b]
be the aÔ¨Éne map x = œï(Œæ) = a+b
2 Œæ + b‚àía
2 . Then
b
>
a
f(x)dx = a + b
2
1
>
‚àí1
(f ‚ó¶œï)(Œæ)dŒæ.
Therefore, we can employ on the interval [a, b] the quadrature formula with
nodes xj = œï(Œæj) and weights Œ±j = a+b
2 Œ≤j. Notice that this formula main-
tains on the interval [a, b] the same degree of exactness of the generating
formula over [‚àí1, 1]. Indeed, assuming that
1
>
‚àí1
p(Œæ)dŒæ =
n

j=0
p(Œæj)Œ≤j
for any polynomial p of degree r over [‚àí1, 1] (for a suitable integer r), for
any polynomial q of the same degree on [a, b] we get
n

j=0
q(xj)Œ±j = a + b
2
n

j=0
(q ‚ó¶œï)(Œæj)Œ≤j = a + b
2
1
>
‚àí1
(q ‚ó¶œï)(Œæ)dŒæ =
b
>
a
q(x)dx,
having recalled that (q ‚ó¶œï)(Œæ) is a polynomial of degree r on [‚àí1, 1].
‚ñ†

424
10. Orthogonal Polynomials in Approximation Theory
10.3
Chebyshev Integration and Interpolation
If Gaussian quadratures are considered with respect to the Chebyshev
weight w(x) = (1 ‚àíx2)‚àí1/2, Gauss nodes and coeÔ¨Écients are given by
xj = ‚àícos (2j + 1)œÄ
2(n + 1) ,
Œ±j =
œÄ
n + 1,
0 ‚â§j ‚â§n,
(10.20)
while Gauss-Lobatto nodes and weights are
xj = ‚àícos œÄj
n ,
Œ±j =
œÄ
djn,
0 ‚â§j ‚â§n,
n ‚â•1,
(10.21)
where d0 = dn = 2 and dj = 1 for j = 1, . . . , n ‚àí1. Notice that the Gauss
nodes (10.20) are, for a Ô¨Åxed n ‚â•0, the zeros of the Chebyshev polynomial
Tn+1 ‚ààPn+1, while, for n ‚â•1, the internal nodes {¬Øxj, j = 1, . . . , n ‚àí1}
are the zeros of T ‚Ä≤
n, as anticipated in Remark 10.2.
Denoting by Œ†GL
n,wf the polynomial of degree n + 1 that interpolates f
at the nodes (10.21), it can be shown that the interpolation error can be
bounded as
‚à•f ‚àíŒ†GL
n,wf‚à•w ‚â§Cn‚àís‚à•f‚à•s,w,
for s ‚â•1,
(10.22)
where ‚à•¬∑ ‚à•w is the norm in L2
w deÔ¨Åned in (10.9), provided that for some
s ‚â•1 the function f has derivatives f (k) of order k = 0, . . . , s in L2
w. In
such a case
‚à•f‚à•s,w =
 s

k=0
‚à•f (k)‚à•2
w
 1
2
.
(10.23)
Here and in the following, C is a constant independent of n that can assume
diÔ¨Äerent values at diÔ¨Äerent places. In particular, for any continuous function
f the following pointwise error estimate can be derived (see Exercise 3)
‚à•f(x) ‚àíŒ†GL
n,wf(x)‚à•‚àû‚â§Cn1/2‚àís‚à•f‚à•s,w.
(10.24)
Thus, Œ†GL
n,wf converges pointwise to f as n ‚Üí‚àû, for any f ‚ààC1([‚àí1, 1]).
The same kind of results (10.22) and (10.24) hold if Œ†GL
n,wf is replaced with
the polynomial Œ†G
n f of degree n that interpolates f at the n+1 Gauss nodes
xj in (10.20). (For the proof of these results see, for instance, [CHQZ88],
p. 298, or [QV94], p. 112). We have also the following result (see [Riv74],
p.13)
‚à•f ‚àíŒ†G
n f‚à•‚àû‚â§(1 + Œõn)E‚àó
n(f),
with Œõn ‚â§2
œÄ log(n + 1) + 1, (10.25)
where ‚àÄn, E‚àó
n(f) = inf
p‚ààPn‚à•f ‚àíp‚à•‚àûis the best approximation error for f
in Pn and Œõn is the Lebesgue constant associated with the Chebyshev

10.3 Chebyshev Integration and Interpolation
425
nodes (10.20). As far as the numerical integration error is concerned, let
us consider, for instance, the Gauss-Lobatto quadrature rule (10.18) with
nodes and weights given in (10.21). First of all, notice that
1
>
‚àí1
f(x)(1 ‚àíx2)‚àí1/2dx = lim
n‚Üí‚àûIGL
n,w(f)
for any function f whose left integral is Ô¨Ånite (see [Sze67], p. 342). If,
moreover, ‚à•f‚à•s,w is Ô¨Ånite for some s ‚â•1, we have

1
>
‚àí1
f(x)(1 ‚àíx2)‚àí1/2dx ‚àíIGL
n,w(f)

‚â§Cn‚àís‚à•f‚à•s,w.
(10.26)
This result follows from the more general one
|(f, vn)w ‚àí(f, vn)n| ‚â§Cn‚àís‚à•f‚à•s,w‚à•vn‚à•w,
‚àÄvn ‚ààPn,
(10.27)
where the so-called discrete scalar product has been introduced
(f, g)n =
n

j=0
Œ±jf(xj)g(xj) = IGL
n,w(fg).
(10.28)
Actually, (10.26) follows from (10.27) setting vn ‚â°1 and noticing that
‚à•vn‚à•w =
+ 1
‚àí1(1 ‚àíx2)‚àí1/2dx
,1/2
= ‚àöœÄ. Thanks to (10.26) we can thus
conclude that the (Chebyshev) Gauss-Lobatto formula has degree of ex-
actness 2n ‚àí1 and order of accuracy (with respect to n‚àí1) equal to s,
provided that ‚à•f‚à•s,w < ‚àû. Therefore, the order of accuracy is only limited
by the regularity threshold s of the integrand function. Completely similar
considerations can be drawn for (Chebyshev) Gauss formulae with n + 1
nodes.
Let us Ô¨Ånally determine the coeÔ¨Écients Àúfk, k = 0, . . . , n, of the interpolat-
ing polynomial Œ†GL
n,wf at the n + 1 Gauss-Lobatto nodes in the expansion
with respect to the Chebyshev polynomials (10.10)
Œ†GL
n,wf(x) =
n

k=0
ÀúfkTk(x).
(10.29)
Notice that Œ†GL
n,wf coincides with the discrete truncation of the Chebyshev
series f ‚àó
n deÔ¨Åned in (10.4). Enforcing the equality Œ†GL
n,wf(xj) = f(xj), j =
0, . . . , n, we Ô¨Ånd
f(xj) =
n

k=0
cos
kjœÄ
n

Àúfk,
j = 0, . . . , n.
(10.30)

426
10. Orthogonal Polynomials in Approximation Theory
Recalling the exactness of the Gauss-Lobatto quadrature, it can be checked
that (see Exercise 2)
Àúfk =
2
ndk
n

j=0
1
dj
cos
kjœÄ
n

f(xj),
k = 0, . . . , n.
(10.31)
Relation (10.31) yields the discrete coeÔ¨Écients { Àúfk, k = 0, . . . , n} in terms
of the nodal values {f(xj), j = 0, . . . , n}. For this reason it is called
the Chebyshev discrete transform (CDT) and, thanks to its trigonomet-
ric structure, it can be eÔ¨Éciently computed using the FFT algorithm (Fast
Fourier transform) with a number of Ô¨Çoating-point operations of the order
of n log2 n (see Section 10.9.2). Of course, (10.30) is the inverse of the CDT,
and can be computed using the FFT.
10.4
Legendre Integration and Interpolation
As previously noticed, the Legendre weight is w(x) ‚â°1. For n ‚â•0, the
Gauss nodes and the related coeÔ¨Écients are given by
xj zeros of Ln+1(x), Œ±j =
2
(1 ‚àíx2
j)[L‚Ä≤
n+1(xj)]2 , j = 0, . . . , n, (10.32)
while the Gauss-Lobatto ones are, for n ‚â•1
x0 = ‚àí1, xn = 1,
xj zeros of L‚Ä≤
n(x),
j = 1, . . . , n ‚àí1
(10.33)
Œ±j =
2
n(n + 1)
1
[Ln(xj)]2 ,
j = 0, . . . , n
(10.34)
where Ln is the n-th Legendre polynomial deÔ¨Åned in (10.12). It can be
checked that, for a suitable constant C independent of n,
2
n(n + 1) ‚â§Œ±j ‚â§C
n ,
‚àÄj = 0, . . . , n
(see [BM92], p. 76). Then, letting Œ†GL
n f be the polynomial of degree n that
interpolates f at the n+1 nodes xj given by (10.33), it can be proved that
it fulÔ¨Ålls the same error estimates as those reported in (10.22) and (10.24)
in the case of the corresponding Chebyshev polynomial.
Of course, the norm ‚à•¬∑‚à•w must here be replaced by the norm ‚à•¬∑‚à•L2(‚àí1,1),
while ‚à•f‚à•s,w becomes
‚à•f‚à•s =
 s

k=0
‚à•f (k)‚à•2
L2(‚àí1,1)
 1
2
.
(10.35)

10.4 Legendre Integration and Interpolation
427
The same kinds of results are ensured if Œ†GL
n f is replaced by the polynomial
of degree n that interpolates f at the n + 1 nodes xj given by (10.32).
Referring to the discrete scalar product deÔ¨Åned in (10.28), but taking
now the nodes and coeÔ¨Écients given by (10.33) and (10.34), we see that
(¬∑, ¬∑)n is an approximation of the usual scalar product (¬∑, ¬∑) of L2(‚àí1, 1).
Actually, the equivalent relation to (10.27) now reads
|(f, vn) ‚àí(f, vn)n| ‚â§Cn‚àís‚à•f‚à•s‚à•vn‚à•L2(‚àí1,1),
‚àÄvn ‚ààPn
(10.36)
and holds for any s ‚â•1 such that ‚à•f‚à•s < ‚àû. In particular, setting vn ‚â°1,
we get ‚à•vn‚à•=
‚àö
2, and from (10.36) it follows that

1
>
‚àí1
f(x)dx ‚àíIGL
n
(f)

‚â§Cn‚àís‚à•f‚à•s
(10.37)
which demonstrates a convergence of the Gauss-Legendre-Lobatto quadra-
ture formula to the exact integral of f with order of accuracy s with respect
to n‚àí1 provided that ‚à•f‚à•s < ‚àû. A similar result holds for the Gauss-
Legendre quadrature formulae.
Example 10.1 Consider the approximate evaluation of the integral of f(x) =
|x|Œ±+ 3
5 over [‚àí1, 1] for Œ± = 0, 1, 2. Notice that f has ‚Äúpiecewise‚Äù derivatives up
to order s = s(Œ±) = Œ± + 1 in L2(‚àí1, 1). Figure 10.1 shows the behavior of the
error as a function of n for the Gauss-Legendre quadrature formula. According
to (10.37), the convergence rate of the formula increases by one when Œ± increases
by one.
‚Ä¢
10
0
10
1
10
2
10
3
10
‚àí10
10
‚àí8
10
‚àí6
10
‚àí4
10
‚àí2
10
0
10
2
FIGURE 10.1. The quadrature error in logarithmic scale as a function of n in the
case of a function with the Ô¨Årst s derivatives in L2(‚àí1, 1) for s = 1 (solid line),
s = 2 (dashed line), s = 3 (dotted line)

428
10. Orthogonal Polynomials in Approximation Theory
The interpolating polynomial at the nodes (10.33) is given by
Œ†GL
n f(x) =
n

k=0
ÀúfkLk(x).
(10.38)
Notice that also in this case Œ†GL
n f coincides with the discrete truncation
of the Legendre series f ‚àó
n deÔ¨Åned in (10.4). Proceeding as in the previous
section, we get
f(xj) =
n

k=0
ÀúfkLk(xj),
j = 0, . . . , n,
(10.39)
and also
Àúfk =
Ô£±
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≥
2k + 1
n(n + 1)
n

j=0
Lk(xj)
1
L2n(xj)f(xj),
k = 0, . . . , n ‚àí1,
1
n + 1
n

j=0
1
Ln(xj)f(xj),
k = n
(10.40)
(see Exercise 6). Formulae (10.40) and (10.39) provide, respectively, the
discrete Legendre transform (DLT) and its inverse.
10.5
Gaussian Integration over Unbounded
Intervals
We consider integration on both half and on the whole of real axis. In both
cases we use interpolatory Gaussian formulae whose nodes are the zeros of
Laguerre and Hermite orthogonal polynomials, respectively.
The Laguerre polynomials. These are algebraic polynomials, orthogonal
on the interval [0, +‚àû) with respect to the weight function w(x) = e‚àíx.
They are deÔ¨Åned by
Ln(x) = ex dn
dxn (e‚àíxxn),
n ‚â•0,
and satisfy the following three-term recursive relation
"
Ln+1(x) = (2n + 1 ‚àíx)Ln(x) ‚àín2Ln‚àí1(x)
n ‚â•0,
L‚àí1 = 0,
L0 = 1.
For any function f, deÔ¨Åne œï(x) = f(x)ex. Then, I(f) =
 ‚àû
0
f(x)dx =
 ‚àû
0
e‚àíxœï(x)dx, so that it suÔ¨Éces to apply to this last integral the Gauss-
Laguerre quadratures, to get, for n ‚â•1 and f ‚ààC2n([0, +‚àû))
I(f) =
n

k=1
Œ±kœï(xk) + (n!)2
(2n)!œï(2n)(Œæ),
0 < Œæ < +‚àû,
(10.41)

10.6 Programs for the Implementation of Gaussian Quadratures
429
where the nodes xk, for k = 1, . . . , n, are the zeros of Ln and the weights
are Œ±k = (n!)2xk/[Ln+1(xk)]2. From (10.41), one concludes that Gauss-
Laguerre formulae are exact for functions f of the type œïe‚àíx, where œï ‚àà
P2n‚àí1. In a generalized sense, we can then state that they have optimal
degrees of exactness equal to 2n ‚àí1.
Example 10.2 Using a Gauss-Laguerre quadrature formula with n = 12 to com-
pute the integral in Example 9.12 we obtain the value 0.5997 with an absolute
error with respect to exact integration equal to 2.96 ¬∑ 10‚àí4. For the sake of com-
parison, the composite trapezoidal formula would require 277 nodes to obtain the
same accuracy.
‚Ä¢
The Hermite polynomials. These are orthogonal polynomials on the
real line with respect to the weight function w(x) = e‚àíx2. They are deÔ¨Åned
by
Hn(x) = (‚àí1)nex2 dn
dxn (e‚àíx2),
n ‚â•0.
Hermite polynomials can be recursively generated as
"
Hn+1(x) = 2xHn(x) ‚àí2nHn‚àí1(x)
n ‚â•0,
H‚àí1 = 0,
H0 = 1.
As in the previous case, letting œï(x) = f(x)ex2, we have I(f) =
 ‚àû
‚àí‚àûf(x)dx =
 ‚àû
‚àí‚àûe‚àíx2œï(x)dx. Applying to this last integral the Gauss-Hermite quadra-
tures we obtain, for n ‚â•1 and f ‚ààC2n(R)
I(f) =
‚àû
>
‚àí‚àû
e‚àíx2œï(x)dx =
n

k=1
Œ±kœï(xk) + (n!)‚àöœÄ
2n(2n)!œï(2n)(Œæ),
Œæ ‚ààR,
(10.42)
where the nodes xk, for k = 1, . . . , n, are the zeros of Hn and the weights
are Œ±k = 2n+1n!‚àöœÄ/[Hn+1(xk)]2. As for Gauss-Laguerre quadratures, the
Gauss-Hermite rules also are exact for functions f of the form œïe‚àíx2, where
œï ‚ààP2n‚àí1; therefore, they have optimal degrees of exactness equal to 2n‚àí1.
More details on the subject can be found in [DR75], pp. 173-174.
10.6
Programs for the Implementation of Gaussian
Quadratures
Programs 82, 83 and 84 compute the coeÔ¨Écients {Œ±k} and {Œ≤k}, introduced
in (10.8), in the cases of the Legendre, Laguerre and Hermite polynomials.
These programs are then called by Program 85 for the computation of nodes

430
10. Orthogonal Polynomials in Approximation Theory
and weights (10.32), in the case of the Gauss-Legendre formulae, and by
Programs 86, 87 for computing nodes and weights in the Gauss-Laguerre
and Gauss-Hermite quadrature rules (10.41) and (10.42). All the codings
reported in this section are excerpts from the library ORTHPOL [Gau94].
Program 82 - coeÔ¨Çege : CoeÔ¨Écients of Legendre polynomials
function [a, b] = coeÔ¨Çege(n)
if (n <= 1), disp(‚Äô n must be > 1 ‚Äô); return; end
for k=1:n, a(k)=0; b(k)=0; end; b(1)=2;
for k=2:n, b(k)=1/(4-1/(k-1)ÀÜ2); end
Program 83 - coeÔ¨Çagu : CoeÔ¨Écients of Laguerre polynomials
function [a, b] = coeÔ¨Çagu(n)
if (n <= 1), disp(‚Äô n must be > 1 ‚Äô); return; end
a=zeros(n,1); b=zeros(n,1); a(1)=1; b(1)=1;
for k=2:n, a(k)=2*(k-1)+1; b(k)=(k-1)ÀÜ2; end
Program 84 - coefherm : CoeÔ¨Écients of Hermite polynomials
function [a, b] = coefherm(n)
if (n <= 1), disp(‚Äô n must be > 1 ‚Äô); return; end
a=zeros(n,1); b=zeros(n,1); b(1)=sqrt(4.*atan(1.));
for k=2:n, b(k)=0.5*(k-1); end
Program 85 - zplege : CoeÔ¨Écients of Gauss-Legendre formulae
function [x,w]=zplege(n)
if (n <= 1), disp(‚Äô n must be > 1 ‚Äô); return; end
[a,b]=coeÔ¨Çege(n);
JacM=diag(a)+diag(sqrt(b(2:n)),1)+diag(sqrt(b(2:n)),-1);
[w,x]=eig(JacM); x=diag(x); scal=2; w=w(1,:)‚Äô.ÀÜ2*scal;
[x,ind]=sort(x); w=w(ind);
Program 86 - zplagu : CoeÔ¨Écients of Gauss-Laguerre formulae
function [x,w]=zplagu(n)
if (n <= 1), disp(‚Äô n must be > 1 ‚Äô); return; end
[a,b]=coeÔ¨Çagu(n);
JacM=diag(a)+diag(sqrt(b(2:n)),1)+diag(sqrt(b(2:n)),-1);
[w,x]=eig(JacM); x=diag(x); w=w(1,:)‚Äô.ÀÜ2;
Program 87 - zpherm : CoeÔ¨Écients of Gauss-Hermite formulae
function [x,w]=zpherm(n)
if (n <= 1), disp(‚Äô n must be > 1 ‚Äô); return; end

10.7 Approximation of a Function in the Least-Squares Sense
431
[a,b]=coefherm(n);
JacM=diag(a)+diag(sqrt(b(2:n)),1)+diag(sqrt(b(2:n)),-1);
[w,x]=eig(JacM); x=diag(x); scal=sqrt(pi); w=w(1,:)‚Äô.ÀÜ2*scal;
[x,ind]=sort(x); w=w(ind);
10.7
Approximation of a Function in the
Least-Squares Sense
Given a function f ‚ààL2(a, b), we look for a polynomial rn of degree ‚â§n
that satisÔ¨Åes
‚à•f ‚àírn‚à•w = min
pn‚ààPn‚à•f ‚àípn‚à•w,
where w is a Ô¨Åxed weight function in (a, b). Should it exist, rn is called a
least-squares polynomial. The name derives from the fact that, if w ‚â°1, rn
is the polynomial that minimizes the mean-square error E = ‚à•f ‚àírn‚à•L2(a,b)
(see Exercise 8).
As seen in Section 10.1, rn coincides with the truncation fn of order
n of the Fourier series (see (10.2) and (10.3)). Depending on the choice
of the weight w(x), diÔ¨Äerent least-squares polynomials arise with diÔ¨Äerent
convergence properties.
Analogous to Section 10.1, we can introduce the discrete truncation f ‚àó
n
(10.4) of the Chebyshev series (setting pk = Tk) or the Legendre series
(setting pk = Lk). If the discrete scalar product induced by the Gauss-
Lobatto quadrature rule (10.28) is used in (10.5) then the Àúfk‚Äôs coincide with
the coeÔ¨Écients of the expansion of the interpolating polynomial Œ†GL
n,wf (see
(10.29) in the Chebyshev case, or (10.38) in the Legendre case).
Consequently, f ‚àó
n = Œ†GL
n,wf, i.e., the discrete truncation of the (Cheby-
shev or Legendre) series of f turns out to coincide with the interpolating
polynomial at the n + 1 Gauss-Lobatto nodes. In particular, in such a case
(10.6) is trivially satisÔ¨Åed, since ‚à•f ‚àíf ‚àó
n‚à•n = 0.
10.7.1
Discrete Least-Squares Approximation
Several applications require representing in a synthetic way, using elemen-
tary functions, a large set of data that are available at a discrete level, for
instance, the results of experimental measurements. This approximation
process, often referred to as data Ô¨Åtting, can be satisfactorily solved using
the discrete least-squares technique that can be formulated as follows.
Assume we are given m + 1 pairs of data
{(xi, yi), i = 0, . . . , m}
(10.43)

432
10. Orthogonal Polynomials in Approximation Theory
where yi may represent, for instance, the value of a physical quantity mea-
sured at the position xi. We assume that all the abscissae are distinct.
We look for a polynomial pn(x) =
n

i=0
aiœïi(x) such that
m

j=0
wj|pn(xj) ‚àíyj|2 ‚â§
m

j=0
wj|qn(xj) ‚àíyj|2
‚àÄqn ‚ààPn,
(10.44)
for suitable coeÔ¨Écients wj > 0. If n = m the polynomial pn clearly co-
incides with the interpolating polynomial of degree n at the nodes {xi}.
Problem (10.44) is called a discrete least-squares problem since a discrete
scalar product is involved, and is the discrete counterpart of the contin-
uous least-squares problem. The solution pn is therefore referred to as a
least-squares polynomial. Notice that
|||q||| =
Ô£±
Ô£≤
Ô£≥
m

j=0
wj[q(xj)]2
Ô£º
Ô£Ω
Ô£æ
1/2
(10.45)
is an essentially strict seminorm on Pn (see, Exercise 7). By deÔ¨Ånition
a discrete norm (or seminorm) ‚à•¬∑ ‚à•‚àóis essentially strict if ‚à•f + g‚à•‚àó=
‚à•f‚à•‚àó+ ‚à•g‚à•‚àóimplies there exist nonnull Œ±, Œ≤ such that Œ±f(xi) + Œ≤g(xi) = 0
for i = 0, . . . , m. Since ||| ¬∑ ||| is an essentially strict seminorm, problem
(10.44) admits a unique solution (see, [IK66], Section 3.5). Proceeding as
in Section 3.13, we Ô¨Ånd
n

k=0
ak
m

j=0
wjœïk(xj)œïi(xj) =
m

j=0
wjyjœïi(xj),
‚àÄi = 0, . . . , n,
which is called a system of normal equations, and can be conveniently
written in the form
BT Ba = BT y,
(10.46)
where B is the rectangular matrix (m+1)√ó(n+1) of entries bij = œïj(xi), i =
0, . . . , m, j = 0, . . . , n, a ‚ààRn+1 is the vector of the unknown coeÔ¨Écients
and y ‚ààRm+1 is the vector of data.
Notice that the system of normal equations obtained in (10.46) is of
the same nature as that introduced in Section 3.13 in the case of over-
determined systems. Actually, if wj = 1 for j = 0, . . . , m, the above system
can be regarded as the solution in the least-squares sense of the system
n

k=0
akœïk(xi) = yi,
i = 0, 1, . . . , m,

10.8 The Polynomial of Best Approximation
433
which would not admit a solution in the classical sense, since the number of
rows is greater than the number of columns. In the case n = 1, the solution
to (10.44) is a linear function, called linear regression for the data Ô¨Åtting
of (10.43). The associated system of normal equations is
1

k=0
m

j=0
wjœïi(xj)œïk(xj)ak =
m

j=0
wjœïi(xj)yj,
i = 0, 1.
Setting (f, g)m =
m

j=0
f(xj)g(xj) the previous system becomes
" (œï0, œï0)ma0 + (œï1, œï0)ma1 = (y, œï0)m,
(œï0, œï1)ma0 + (œï1, œï1)ma1 = (y, œï1)m,
where y(x) is a function that takes the value yi at the nodes xi, i =
0, . . . , m. After some algebra, we get this explicit form for the coeÔ¨Écients
a0 = (y, œï0)m(œï1, œï1)m ‚àí(y, œï1)m(œï1, œï0)m
(œï1, œï1)m(œï0, œï0)m ‚àí(œï0, œï1)2m
,
a1 = (y, œï1)m(œï0, œï0)m ‚àí(y, œï0)m(œï1, œï0)m
(œï1, œï1)m(œï0, œï0)m ‚àí(œï0, œï1)2m
.
Example 10.3 As already seen in Example 8.2, small changes in the data can
give rise to large variations on the interpolating polynomial of a given function
f. This doesn‚Äôt happen for the least-squares polynomial where m is much larger
than n. As an example, consider the function f(x) = sin(2œÄx) in [‚àí1, 1] and
evaluate it at the 22 equally spaced nodes xi = 2i/21, i = 0, . . . , 21, setting
fi = f(xi). Then, suppose to add to the data fi a random perturbation of the
order of 10‚àí3 and denote by p5 and Àúp5 the least-squares polynomials of degree
5 approximating the data fi and Àúfi, respectively. The maximum norm of the
diÔ¨Äerence p5 ‚àíÀúp5 over [‚àí1, 1] is of the order of 10‚àí3, i.e., it is of the same order
as the perturbation on the data. For comparison, the same diÔ¨Äerence in the case
of Lagrange interpolation is about equal to 2 as can be seen in Figure 10.2.
‚Ä¢
10.8
The Polynomial of Best Approximation
Consider a function f ‚ààC0([a, b]). A polynomial p‚àó
n ‚ààPn is said to be the
polynomial of best approximation of f if it satisÔ¨Åes
‚à•f ‚àíp‚àó
n‚à•‚àû= min
pn‚ààPn‚à•f ‚àípn‚à•‚àû,
‚àÄpn ‚ààPn
(10.47)
where ‚à•g‚à•‚àû= maxa‚â§x‚â§b |g(x)|. This problem is referred to as a minimax
approximation, as we are looking for the minimum error measured in the
maximum norm.

434
10. Orthogonal Polynomials in Approximation Theory
‚àí1
‚àí0.5
0
0.5
1
‚àí2
‚àí1.5
‚àí1
‚àí0.5
0
0.5
1
1.5
2
FIGURE 10.2. The perturbed data (circles), the associated least-squares polyno-
mial of degree 5 (solid line) and the Lagrange interpolating polynomial (dashed
line)
Property 10.1 (Chebyshev equioscillation theorem) For any n ‚â•0,
the polynomial of best approximation p‚àó
n of f exists and is unique. More-
over, in [a, b] there exist n + 2 points x0 < x1 < . . . < xn+1 such that
f(xj) ‚àíp‚àó
n(xj) = œÉ(‚àí1)jE‚àó
n(f),
j = 0, . . . , n + 1
with œÉ = 1 or œÉ = ‚àí1 depending on f and n, and E‚àó
n(f) = ‚à•f ‚àíp‚àó
n‚à•‚àû.
(For the proof, see [Dav63], Chapter 7). As a consequence, there exist n+1
points Àúx0 < Àúx1 < . . . < Àúxn, with xk < Àúxk < xk+1 for k = 0, . . . , n, to be
determined in [a, b] such that
p‚àó
n(Àúxj) = f(Àúxj),
j = 0, 1, . . . , n,
so that the best approximation polynomial is a polynomial of degree n that
interpolates f at n + 1 unknown nodes.
The following result yields an estimate of E‚àó
n(f) without explicitly com-
puting p‚àó
n (we refer for the proof to [Atk89], Chapter 4).
Property 10.2 (de la Vall¬¥ee-Poussin theorem) Let n ‚â•0 and let x0 <
x1 < . . . < xn+1 be n + 2 points in [a, b]. If there exists a polynomial qn of
degree ‚â§n such that
f(xj) ‚àíqn(xj) = (‚àí1)jej
j = 0, 1, . . . , n + 1
where all ej have the same sign and are non null, then
min
0‚â§j‚â§n+1|ej| ‚â§E‚àó
n(f).
We can now relate E‚àó
n(f) with the interpolation error. Indeed,
‚à•f ‚àíŒ†nf‚à•‚àû‚â§‚à•f ‚àíp‚àó
n‚à•‚àû+ ‚à•p‚àó
n ‚àíŒ†nf‚à•‚àû.

10.9 Fourier Trigonometric Polynomials
435
On the other hand, using the Lagrange representation of p‚àó
n we get
‚à•p‚àó
n ‚àíŒ†nf‚à•‚àû= ‚à•
n

i=0
(p‚àó
n(xi) ‚àíf(xi))li‚à•‚àû‚â§‚à•p‚àó
n ‚àíf‚à•‚àû‚à•
n

i=0
|li|‚à•‚àû,
from which it follows
‚à•f ‚àíŒ†nf‚à•‚àû‚â§(1 + Œõn)E‚àó
n(f),
where Œõn is the Lebesgue constant (8.11) associated with the nodes {xi}.
Thanks to (10.25) we can conclude that the Lagrange interpolating poly-
nomial on the Chebyshev nodes is a good approximation of p‚àó
n. The above
results yield a characterization of the best approximation polynomial, but
do not provide a constructive way for generating it. However, starting from
the Chebyshev equioscillation theorem, it is possible to devise an algorithm,
called the Remes algorithm, that is able to construct an arbitrarily good
approximation of the polynomial p‚àó
n (see [Atk89], Section 4.7).
10.9
Fourier Trigonometric Polynomials
Let us apply the theory developed in the previous sections to a particular
family of orthogonal polynomials which are no longer algebraic polynomials
but rather trigonometric. The Fourier polynomials on (0, 2œÄ) are deÔ¨Åned
as
œïk(x) = eikx,
k = 0, ¬±1, ¬±2, . . .
where i is the imaginary unit. These are complex-valued periodic functions
with period equal to 2œÄ. We shall use the notation L2(0, 2œÄ) to denote the
complex-valued functions that are square integrable over (0, 2œÄ). Therefore
L2(0, 2œÄ) =
%
f : (0, 2œÄ) ‚ÜíC such that
> 2œÄ
0
|f(x)|2dx < ‚àû
&
with scalar product and norm deÔ¨Åned respectively by
(f, g) =
 2œÄ
0
f(x)g(x)dx,
‚à•f‚à•L2(0,2œÄ) =

(f, f).
If f ‚ààL2(0, 2œÄ), its Fourier series is
Ff =
‚àû

k=‚àí‚àû
fkœïk,
with fk = 1
2œÄ
2œÄ
>
0
f(x)e‚àíikxdx = 1
2œÄ (f, œïk). (10.48)
If f is complex-valued we set f(x) = Œ±(x) + iŒ≤(x) for x ‚àà[0, 2œÄ], where
Œ±(x) is the real part of f and Œ≤(x) is the imaginary one. Recalling that

436
10. Orthogonal Polynomials in Approximation Theory
e‚àíikx = cos(kx) ‚àíi sin(kx) and letting
ak = 1
2œÄ
2œÄ
>
0
[Œ±(x) cos(kx) + Œ≤(x) sin(kx)] dx
bk = 1
2œÄ
2œÄ
>
0
[‚àíŒ±(x) sin(kx) + Œ≤(x) cos(kx)] dx,
the Fourier coeÔ¨Écients of the function f can be written as
fk = ak + ibk
‚àÄk = 0, ¬±1, ¬±2, . . . .
(10.49)
We shall assume henceforth that f is a real-valued function; in such a case
f‚àík = fk for any k.
Let N be an even positive integer. Analogously to what was done in
Section 10.1, we call the truncation of order N of the Fourier series the
function
f ‚àó
N(x) =
N
2 ‚àí1

k=‚àíN
2
fkeikx.
The use of capital N instead of small n is to conform with the notation usu-
ally adopted in the analysis of discrete Fourier series (see [Bri74], [Wal91]).
To simplify the notations we also introduce an index shift so that
f ‚àó
N(x) =
N‚àí1

k=0
fkei(k‚àíN
2 )x,
where now
fk = 1
2œÄ
2œÄ
>
0
f(x)e‚àíi(k‚àíN/2)xdx = 1
2œÄ (f, $œïk), k = 0, . . . , N ‚àí1
(10.50)
and $œïk = e‚àíi(k‚àíN/2)x. Denoting by
SN = span{$œïk, 0 ‚â§k ‚â§N ‚àí1},
if f ‚ààL2(0, 2œÄ) its truncation of order N satisÔ¨Åes the following optimal
approximation property in the least-squares sense
‚à•f ‚àíf ‚àó
N‚à•L2(0,2œÄ) = min
g‚ààSN‚à•f ‚àíg‚à•L2(0,2œÄ).
Set h = 2œÄ/N and xj = jh, for j = 0, . . . , N ‚àí1, and introduce the
following discrete scalar product
(f, g)N = h
N‚àí1

j=0
f(xj)g(xj).
(10.51)

10.9 Fourier Trigonometric Polynomials
437
Replacing (f, $œïk) in (10.50) with (f, $œïk)N, we get the discrete Fourier
coeÔ¨Écients of the function f
$fk = 1
N
N‚àí1

j=0
f(xj)e‚àíikjheijœÄ = 1
N
N‚àí1

j=0
f(xj)W
(k‚àíN
2 )j
N
(10.52)
for k = 0, . . . , N ‚àí1, where
WN = exp

‚àíi2œÄ
N

is the principal root of order N of unity. According to (10.4), the trigono-
metric polynomial
Œ†F
Nf(x) =
N‚àí1

k=0
$fkei(k‚àíN
2 )x
(10.53)
is called the discrete Fourier series of order N of f.
Lemma 10.1 The following property holds
(œïl, œïj)N = h
N‚àí1

k=0
e‚àíik(l‚àíj)h = 2œÄŒ¥jl,
0 ‚â§l, j ‚â§N ‚àí1,
(10.54)
where Œ¥jl is the Kronecker symbol.
Proof. For l = j the result is immediate. Thus, assume l Ã∏= j; we have that
N‚àí1

k=0
e‚àíik(l‚àíj)h =
1 ‚àí
+
e‚àíi(l‚àíj)h,N
1 ‚àíe‚àíi(l‚àíj)h
= 0.
Indeed, the numerator is 1 ‚àí(cos(2œÄ(l ‚àíj)) ‚àíi sin(2œÄ(l ‚àíj))) = 1 ‚àí1 = 0,
while the denominator cannot vanish. Actually, it vanishes iÔ¨Ä(j ‚àíl)h = 2œÄ, i.e.,
j ‚àíl = N, which is impossible.
3
Thanks to Lemma 10.1, the trigonometric polynomial Œ†F
Nf is the Fourier
interpolate of f at the nodes xj, that is
Œ†F
Nf(xj) = f(xj),
j = 0, 1, . . . , N ‚àí1.
Indeed, using (10.52) and (10.54) in (10.53) it follows that
Œ†F
Nf(xj) =
N‚àí1

k=0
$fkeikjhe‚àíijh N
2 =
N‚àí1

l=0
f(xl)
.
1
N
N‚àí1

k=0
e‚àíik(l‚àíj)h
/
= f(xj).

438
10. Orthogonal Polynomials in Approximation Theory
Therefore, looking at the Ô¨Årst and last equality, we get
f(xj) =
N‚àí1

k=0
$fkeik(j‚àíN
2 )h =
N‚àí1

k=0
$fkW
‚àí(j‚àíN
2 )k
N
, j = 0, . . . , N ‚àí1. (10.55)
The mapping {f(xj)} ‚Üí{ $fk} described by (10.52) is called the Discrete
Fourier Transform (DFT), while the mapping (10.55) from { $fk} to {f(xj)}
is called the inverse transform (IDFT). Both DFT and IDFT can be written
in matrix form as { $fk} = T{f(xj)} and {f(xj)} = C{ $fk} where T ‚àà
CN√óN, C denotes the inverse of T and
Tkj = 1
N W
(k‚àíN
2 )j
N
,
k, j = 0, . . . , N ‚àí1,
Cjk = W
‚àí(j‚àíN
2 )k
N
,
j, k = 0, . . . , N ‚àí1.
A naive implementation of the matrix-vector computation in the DFT and
IDFT would require N 2 operations. Using the FFT (Fast Fourier Trans-
form) algorithm only O(N log2 N) Ô¨Çops are needed, provided that N is a
power of 2, as will be explained in Section 10.9.2.
The function Œ†F
Nf ‚ààSN introduced in (10.53) is the solution of the
minimization problem ‚à•f ‚àíŒ†F
Nf‚à•N ‚â§‚à•f ‚àíg‚à•N for any g ‚ààSN, where
‚à•¬∑ ‚à•N = (¬∑, ¬∑)1/2
N
is a discrete norm for SN. In the case where f is periodic
with all its derivatives up to order s (s ‚â•1), an error estimate analogous
to that for Chebyshev and Legendre interpolation holds
‚à•f ‚àíŒ†F
Nf‚à•L2(0,2œÄ) ‚â§CN ‚àís‚à•f‚à•s
and also
max
0‚â§x‚â§2œÄ|f(x) ‚àíŒ†F
Nf(x)| ‚â§CN 1/2‚àís‚à•f‚à•s.
In a similar manner, we also have
|(f, vN) ‚àí(f, vN)N| ‚â§CN ‚àís‚à•f‚à•s‚à•vN‚à•
for any vN ‚ààSN, and in particular, setting vN = 1 we have the following
error for the quadrature formula (10.51)

2œÄ
>
0
f(x)dx ‚àíh
N‚àí1

j=0
f(xj)

‚â§CN ‚àís‚à•f‚à•s
(see for the proof [CHQZ88], Chapter 2).
Notice that h
N‚àí1

j=0
f(xj) is nothing else than the composite trapezoidal
rule for approximating the integral
 2œÄ
0
f(x)dx. Therefore, such a formula

10.9 Fourier Trigonometric Polynomials
439
turns out to be extremely accurate when dealing with periodic and smooth
integrands.
Programs 88 and 89 provide an implementation of the DFT and IDFT. The
input parameter f is a string containing the function f to be transformed
while fc is a vector of size N containing the values $fk.
Program 88 - dft : Discrete Fourier transform
function fc = dft(N,f)
h = 2*pi/N; x=[0:h:2*pi*(1-1/N)]; fx = eval(f); wn = exp(-i*h);
for k=0:N-1,
s = 0;
for j=0:N-1
s = s + fx(j+1)*wnÀÜ((k-N/2)*j);
end
fc (k+1) = s/N;
end
Program 89 - idft : Inverse discrete Fourier transform
function fv = idft(N,fc)
h = 2*pi/N; wn = exp(-i*h);
for k=0:N-1
s = 0;
for j=0:N-1
s = s + fc(j+1)*wnÀÜ(-k*(j-N/2));
end
fv (k+1) = s;
end
10.9.1
The Gibbs Phenomenon
Consider the discontinuous function f(x) = x/œÄ for x ‚àà[0, œÄ] and equal to
x/œÄ ‚àí2 for x ‚àà(œÄ, 2œÄ], and compute its DFT using Program 88. The inter-
polate Œ†F
Nf is shown in Figure 10.3 (above) for N = 8, 16, 32. Notice the
spurious oscillations around the point of discontinuity of f whose maximum
amplitude, however, tends to a Ô¨Ånite limit. The arising of these oscillations
is known as Gibbs phenomenon and is typical of functions with isolated
jump discontinuities; it aÔ¨Äects the behavior of the truncated Fourier series
not only in the neighborhood of the discontinuity but also over the entire
interval, as can be clearly seen in the Ô¨Ågure. The convergence rate of the
truncated series for functions with jump discontinuities is linear in N ‚àí1 at
every given non-singular point of the interval of deÔ¨Ånition of the function
(see [CHQZ88], Section 2.1.4).

440
10. Orthogonal Polynomials in Approximation Theory
0
1
2
3
4
5
6
‚àí1
‚àí0.5
0
0.5
1
0
1
2
3
4
5
6
‚àí1
‚àí0.5
0
0.5
1
FIGURE 10.3. Above: Fourier interpolate of the sawtooth function (thick solid
line) for N = 8 (dash-dotted line), 16 (dashed line) and 32 (thin solid line).
Below: the same informations are plotted in the case of the Lanczos smoothing
Since the Gibbs phenomenon is related to the slow decay of the Fourier
coeÔ¨Écients of a discontinuous function, smoothing procedures can be prof-
itably employed to attenuate the higher-order Fourier coeÔ¨Écients. This can
be done by multiplying each coeÔ¨Écient $fk by a factor œÉk such that œÉk is a
decreasing function of k. An example is provided by the Lanczos smoothing
œÉk = sin(2(k ‚àíN/2)(œÄ/N))
2(k ‚àíN/2)(œÄ/N)
,
k = 0, . . . , N ‚àí1.
(10.56)
The eÔ¨Äect of applying the Lanczos smoothing to the computation of the
DFT of the above function f is represented in Figure 10.3 (below), which
shows that the oscillations have almost completely disappeared.
For a deeper analysis of this subject we refer to [CHQZ88], Chapter 2.
10.9.2
The Fast Fourier Transform
As pointed out in the previous section, computing the discrete Fourier
transform (DFT) or its inverse (IDFT) as a matrix-vector product, would
require N 2 operations. In this section we illustrate the basic steps of the
Cooley-Tukey algorithm [CT65], commonly known as Fast Fourier Trans-
form (FFT). The computation of a DFT of order N is split into DFTs of
order p0, . . . , pm, where {pi} are the prime factors of N. If N is a power of
2, the computational cost has the order of N log2 N Ô¨Çops.
A recursive algorithm to compute the DFT when N is a power of 2
is described in the following. Let f = (fi)T , i = 0, . . . , N ‚àí1 and set
p(x) =
1
N
N‚àí1

j=0
fjxj. Then, computing the DFT of the vector f amounts to

10.9 Fourier Trigonometric Polynomials
441
evaluating p(W
k‚àíN
2
N
) for k = 0, . . . , N ‚àí1. Let us introduce the polynomials
pe(x) = 1
N
6
f0 + f2x + . . . + fN‚àí2x
N
2 ‚àí17
,
po(x) = 1
N
6
f1 + f3x + . . . + fN‚àí1x
N
2 ‚àí17
.
Notice that
p(x) = pe(x2) + xpo(x2)
from which it follows that the computation of the DFT of f can be carried
out by evaluating the polynomials pe and po at the points W
2(k‚àíN
2 )
N
, k =
0, . . . , N ‚àí1. Since
W
2(k‚àíN
2 )
N
= W 2k‚àíN
N
= exp

‚àíi 2œÄk
N/2

exp(i2œÄ) = W k
N/2,
it turns out that we must evaluate pe and po at the principal roots of unity
of order N/2. In this manner the DFT of order N is rewritten in terms
of two DFTs of order N/2; of course, we can recursively apply again this
procedure to po and pe. The process is terminated when the degree of the
last generated polynomials is equal to one.
In Program 90 we propose a simple implementation of the FFT recursive
algorithm. The input parameters are the vector f containing the NN values
fk, where NN is a power of 2.
Program 90 - Ô¨Ätrec : FFT algorithm in the recursive version
function [Ô¨Ätv]=Ô¨Ätrec(f,NN)
N = length(f);
w = exp(-2*pi*sqrt(-1)/N);
if N == 2
Ô¨Ätv = f(1)+w.ÀÜ[-NN/2:NN-1-NN/2]*f(2);
else
a1 = f(1:2:N);
b1 = f(2:2:N);
a2 = Ô¨Ätrec(a1,NN); b2 = Ô¨Ätrec(b1,NN);
for k=-NN/2:NN-1-NN/2
Ô¨Ätv(k+1+NN/2) = a2(k+1+NN/2) + b2(k+1+NN/2)*wÀÜk;
end
end
Remark 10.4 A FFT procedure can also be set up when N is not a power
of 2. The simplest approach consists of adding some zero samples to the
original sequence {fi} in such a way to obtain a total number of ÀúN = 2p
values. This technique, however, does not necessarily yield the correct re-
sult. Therefore, an eÔ¨Äective alternative is based on partitioning the Fourier
matrix C into subblocks of smaller size. Practical FFT implementations
can handle both strategies (see, for instance, the fft package available in
MATLAB).
‚ñ†

442
10. Orthogonal Polynomials in Approximation Theory
10.10
Approximation of Function Derivatives
A problem which is often encountered in numerical analysis is the ap-
proximation of the derivative of a function f(x) on a given interval [a, b].
A natural approach to it consists of introducing in [a, b] n + 1 nodes
{xk, k = 0, . . . , n}, with x0 = a, xn = b and xk+1 = xk+h, k = 0, . . . , n‚àí1
where h = (b ‚àía)/n. Then, we approximate f ‚Ä≤(xi) using the nodal values
f(xk) as
h
m

k=‚àím
Œ±kui‚àík =
m‚Ä≤

k=‚àím‚Ä≤
Œ≤kf(xi‚àík),
(10.57)
where {Œ±k}, {Œ≤k} ‚ààR are m + m‚Ä≤ + 1 coeÔ¨Écients to be determined and uk
is the desired approximation to f ‚Ä≤(xk).
A non negligible issue in the choice of scheme (10.57) is the computa-
tional eÔ¨Éciency. Regarding this concern, it is worth noting that, if m Ã∏= 0,
determining the values {ui} requires the solution of a linear system.
The set of nodes which are involved in constructing the derivative of f at
a certain node, is called a stencil. The band of the matrix associated with
system (10.57) increases as the stencil gets larger.
10.10.1
Classical Finite DiÔ¨Äerence Methods
The simplest way to generate a formula like (10.57) consists of resorting to
the deÔ¨Ånition of the derivative. If f ‚Ä≤(xi) exists, then
f ‚Ä≤(xi) = lim
h‚Üí0+
f(xi + h) ‚àíf(xi)
h
.
(10.58)
Replacing the limit with the incremental ratio, with h Ô¨Ånite, yields the
approximation
uF D
i
= f(xi+1) ‚àíf(xi)
h
,
0 ‚â§i ‚â§n ‚àí1.
(10.59)
Relation (10.59) is a special instance of (10.57) setting m = 0, Œ±0 = 1,
m‚Ä≤ = 1, Œ≤‚àí1 = 1, Œ≤0 = ‚àí1, Œ≤1 = 0.
The right side of (10.59) is called the forward Ô¨Ånite diÔ¨Äerence and the
approximation that is being used corresponds to replacing f ‚Ä≤(xi) with
the slope of the straight line passing through the points (xi, f(xi)) and
(xi+1, f(xi+1)), as shown in Figure 10.4.
To estimate the error that is made, it suÔ¨Éces to expand f in Taylor‚Äôs
series, obtaining
f(xi+1) = f(xi) + hf ‚Ä≤(xi) + h2
2 f ‚Ä≤‚Ä≤(Œæi)
with Œæi ‚àà(xi, xi+1).

10.10 Approximation of Function Derivatives
443
We assume henceforth that f has the required regularity, so that
f ‚Ä≤(xi) ‚àíuF D
i
= ‚àíh
2 f ‚Ä≤‚Ä≤(Œæi).
(10.60)
f(xi)
f(xi‚àí1)
f(xi+1)
xi‚àí1
xi
xi+1
FIGURE 10.4. Finite diÔ¨Äerence approximation of f ‚Ä≤(xi): backward (solid line),
forward (pointed line) and centred (dashed line)
Obviously, instead of (10.58) we could employ a centred incremental
ratio, obtaining the following approximation
uCD
i
= f(xi+1) ‚àíf(xi‚àí1)
2h
,
1 ‚â§i ‚â§n ‚àí1.
(10.61)
Scheme (10.61) is a special instance of (10.57) setting m = 0, Œ±0 = 1,
m‚Ä≤ = 1, Œ≤‚àí1 = 1/2, Œ≤0 = 0, Œ≤1 = ‚àí1/2.
The right side of (10.61) is called the centred Ô¨Ånite diÔ¨Äerence and geometri-
cally amounts to replacing f ‚Ä≤(xi) with the slope of the straight line passing
through the points (xi‚àí1, f(xi‚àí1)) and (xi+1, f(xi+1)) (see Figure 10.4).
Resorting again to Taylor‚Äôs series, we get
f ‚Ä≤(xi) ‚àíuCD
i
= ‚àíh2
6 f ‚Ä≤‚Ä≤‚Ä≤(Œæi).
(10.62)
Formula (10.61) thus provides a second-order approximation to f ‚Ä≤(xi) with
respect to h.
Finally, with a similar procedure, we can derive a backward Ô¨Ånite diÔ¨Äer-
ence scheme, where
uBD
i
= f(xi) ‚àíf(xi‚àí1)
h
,
1 ‚â§i ‚â§n,
(10.63)

444
10. Orthogonal Polynomials in Approximation Theory
which is aÔ¨Äected by the following error
f ‚Ä≤(xi) ‚àíuBD
i
= h
2 f ‚Ä≤‚Ä≤(Œæi).
(10.64)
The values of the parameters in (10.57) are m = 0, Œ±0 = 1, m‚Ä≤ = 1 and
Œ≤‚àí1 = 0, Œ≤0 = 1, Œ≤1 = ‚àí1.
Higher-order schemes, as well as Ô¨Ånite diÔ¨Äerence approximations of higher-
order derivatives of f, can be constructed using Taylor‚Äôs expansions of
higher order. A remarkable example is the approximation of f ‚Ä≤‚Ä≤; if f ‚àà
C4([a, b]) we easily get
f ‚Ä≤‚Ä≤(xi)
= f(xi+1) ‚àí2f(xi) + f(xi‚àí1)
h2
‚àíh2
24
+
f (4)(xi + Œ∏ih) + f (4)(xi ‚àíœâih)
,
,
0 < Œ∏i, œâi < 1.
The following centred Ô¨Ånite diÔ¨Äerence scheme can thus be derived
u‚Ä≤‚Ä≤
i = f(xi+1) ‚àí2f(xi) + f(xi‚àí1)
h2
,
1 ‚â§i ‚â§n ‚àí1
(10.65)
which is aÔ¨Äected by the error
f ‚Ä≤‚Ä≤(xi) ‚àíu‚Ä≤‚Ä≤
i = ‚àíh2
24
+
f (4)(xi + Œ∏ih) + f (4)(xi ‚àíœâih)
,
.
(10.66)
Formula (10.65) provides a second-order approximation to f ‚Ä≤‚Ä≤(xi) with re-
spect to h.
10.10.2
Compact Finite DiÔ¨Äerences
More accurate approximations are provided by using the following formula
(which we call compact diÔ¨Äerences)
Œ±ui‚àí1 + ui + Œ±ui+1 = Œ≤
2h(fi+1 ‚àífi‚àí1) + Œ≥
4h(fi+2 ‚àífi‚àí2)
(10.67)
for i = 2, . . . , n ‚àí1. We have set, for brevity, fi = f(xi).
The coeÔ¨Écients Œ±, Œ≤ and Œ≥ are to be determined in such a way that the
relations (10.67) yield values ui that approximate f ‚Ä≤(xi) up to the highest
order with respect to h. For this purpose, the coeÔ¨Écients are selected in
such a way as to minimize the consistency error (see Section 2.2)
œÉi(h)
= Œ±f (1)
i‚àí1 + f (1)
i
‚àíŒ±f (1)
i+1
‚àí
 Œ≤
2h(fi+1 ‚àífi‚àí1) + Œ≥
4h(fi+2 ‚àífi‚àí2)

(10.68)

10.10 Approximation of Function Derivatives
445
which comes from ‚Äúforcing‚Äù f to satisfy the numerical scheme (10.67). For
brevity, we set f (k)
i
= f (k)(xi), k = 1, 2, . . . .
Precisely, assuming that f ‚ààC5([a, b]) and expanding it in a Taylor‚Äôs
series around xi, we Ô¨Ånd
fi¬±1 = fi ¬± hf (1)
i
+ h2
2 f (2)
i
¬± h3
6 f (3)
i
+ h4
24 f (4)
i
¬± h5
120f (5)
i
+ O(h6),
f (1)
i¬±1 = f (1)
i
¬± hf (2)
i
+ h2
2 f (3)
i
¬± h3
6 f (4)
i
+ h4
24 f (5)
i
+ O(h5).
Substituting into (10.68) we get
œÉi(h) = (2Œ± + 1)f (1)
i
+ Œ±h2
2 f (3)
i
+ Œ±h4
12f (5)
i
‚àí(Œ≤ + Œ≥)f (1)
i
‚àíh2
2
Œ≤
6 + 2Œ≥
3

f (3)
i
‚àíh4
60
Œ≤
2 + 8Œ≥

f (5)
i
+ O(h6).
Second-order methods are obtained by equating to zero the coeÔ¨Écient of
f (1)
i
, i.e., if 2Œ± + 1 = Œ≤ + Œ≥, while we obtain schemes of order 4 by equating
to zero also the coeÔ¨Écient of f (3)
i
, yielding 6Œ± = Œ≤+4Œ≥ and Ô¨Ånally, methods
of order 6 are obtained by setting to zero also the coeÔ¨Écient of f (5)
i
, i.e.,
10Œ± = Œ≤ + 16Œ≥.
The linear system formed by these last three equations has a nonsingular
matrix. Thus, there exists a unique scheme of order 6 that corresponds to
the following choice of the parameters
Œ± = 1/3,
Œ≤ = 14/9,
Œ≥ = 1/9,
(10.69)
while there exist inÔ¨Ånitely many methods of second and fourth order.
Among these inÔ¨Ånite methods, a popular scheme has coeÔ¨Écients Œ± = 1/4,
Œ≤ = 3/2 and Œ≥ = 0. Schemes of higher order can be generated at the
expense of furtherly expanding the computational stencil.
Traditional Ô¨Ånite diÔ¨Äerence schemes correspond to setting Œ± = 0 and
allow for computing explicitly the approximant of the Ô¨Årst derivative of f
at a node, in contrast with compact schemes which are required in any case
to solve a linear system of the form Au = Bf (where the notation has the
obvious meaning).
To make the system solvable, it is necessary to provide values to the vari-
ables ui with i < 0 and i > n. A particularly favorable instance is that
where f is a periodic function of period b ‚àía, in which case ui+n = ui
for any i ‚ààZ. In the nonperiodic case, system (10.67) must be supplied
by suitable relations at the nodes near the boundary of the approximation
interval. For example, the Ô¨Årst derivative at x0 can be computed using the
relation
u0 + Œ±u1 = 1
h(Af1 + Bf2 + Cf3 + Df4),

446
10. Orthogonal Polynomials in Approximation Theory
and requiring that
A = ‚àí3 + Œ± + 2D
2
,
B = 2 + 3D,
C = ‚àí1 ‚àíŒ± + 6D
2
,
in order for the scheme to be at least second-order accurate (see [Lel92]
for the relations to enforce in the case of higher-order methods). Finally,
we notice that, for any given order of accuracy, compact schemes have a
stencil smaller than the one of standard Ô¨Ånite diÔ¨Äerences.
Program 91 provides an implementation of the compact Ô¨Ånite diÔ¨Äerence
schemes (10.67) for the approximation of the derivative of a given function f
which is assumed to be periodic on the interval [a, b). The input parameters
alpha, beta and gamma contain the coeÔ¨Écients of the scheme, a and b are
the endpoints of the interval, f is a string containing the expression of f
and n denotes the number of subintervals in which [a, b] is partitioned. The
output vectors u and x contain the computed approximate values ui and
the node coordinates. Notice that setting alpha=gamma=0 and beta=1 we
recover the centered Ô¨Ånite diÔ¨Äerence approximation (10.61).
Program 91 - compdiÔ¨Ä: Compact diÔ¨Äerence schemes
function [u, x] = compdiÔ¨Ä(alpha,beta,gamma,a,b,n,f)
h=(b-a)/(n+1); x=[a:h:b]; fx = eval(f);
A
= eye(n+2)+alpha*diag(ones(n+1,1),1)+alpha*diag(ones(n+1,1),-1);
rhs = 0.5*beta/h*(fx(4:n+1)-fx(2:n-1))+0.25*gamma/h*(fx(5:n+2)-fx(1:n-2));
if gamma == 0
rhs=[0.5*beta/h*(fx(3)-fx(1)), rhs, 0.5*beta/h*(fx(n+2)-fx(n))];
A(1,1:n+2) = zeros(1,n+2);
A(1,1) = 1; A(1,2)=alpha; A(1,n+1)=alpha;
rhs=[0.5*beta/h*(fx(2)-fx(n+1)), rhs];
A(n+2,1:n+2) = zeros(1,n+2);
A(n+2,n+2) = 1; A(n+2,n+1)=alpha; A(n+2,2)=alpha;
rhs=[rhs, 0.5*beta/h*(fx(2)-fx(n+1))];
else
rhs=[0.5*beta/h*(fx(3)-fx(1))+0.25*gamma/h*(fx(4)-fx(n+1)), rhs];
A(1,1:n+2) = zeros(1,n+2);
A(1,1) = 1; A(1,2)=alpha; A(1,n+1)=alpha;
rhs=[0.5*beta/h*(fx(2)-fx(n+1))+0.25*gamma/h*(fx(3)-fx(n)), rhs];
rhs=[rhs,0.5*beta/h*(fx(n+2)-fx(n))+0.25*gamma/h*(fx(2)-fx(n-1))];
A(n+2,1:n+2) = zeros(1,n+2);
A(n+2,n+2) = 1; A(n+2,n+1)=alpha; A(n+2,2)=alpha;
rhs=[rhs,0.5*beta/h*(fx(2)-fx(n+1))+0.25*gamma/h*(fx(3)-fx(n))];
end
u = A \ rhs‚Äô;
return
Example 10.4 Let us consider the approximate evaluation of the derivative of
the function f(x) = sin(x) on the interval [0, 2œÄ]. Figure 10.5 shows the loga-

10.10 Approximation of Function Derivatives
447
rithm of the maximum nodal errors for the second-order centered Ô¨Ånite diÔ¨Äer-
ence scheme (10.61) and of the fourth and sixth-order compact diÔ¨Äerence schemes
introduced above, as a function of p = log(n).
‚Ä¢
4
8
16
32
64
10
‚àí10
10
‚àí8
10
‚àí6
10
‚àí4
10
‚àí2
10
0
FIGURE 10.5. Maximum nodal errors for the second-order centred Ô¨Ånite diÔ¨Äer-
ence scheme (solid line) and for the fourth (dashed line) and sixth-order (dotted
line) compact diÔ¨Äerence schemes as functions of p = log(n)
Another nice feature of compact schemes is that they maximize the range
of well-resolved waves as we are going to explain. Assume that f is a real
and periodic function on [0, 2œÄ], that is, f(0) = f(2œÄ). Using the same
notation as in Section 10.9, we let N be an even positive integer and set
h = 2œÄ/N. Then replace f by its truncated Fourier series
f ‚àó
N(x) =
N/2‚àí1

k=‚àíN/2
fkeikx.
Since the function f is real-valued, fk = ¬Øf ‚àík for k = 1, . . . , N/2 and
f0 = ¬Øf 0. For sake of convenience, introduce the normalized wave number
wk = kh = 2œÄk/N and perform a scaling of the coordinates setting s = x/h.
As a consequence, we get
f ‚àó
N(x(s)) =
N/2‚àí1

k=‚àíN/2
fkeiksh =
N/2‚àí1

k=‚àíN/2
fkeiwks.
(10.70)
Taking the Ô¨Årst derivative of (10.70) with respect to s yields a function
whose Fourier coeÔ¨Écients are f ‚Ä≤
k = iwk fk. We can thus estimate the ap-
proximation error on (f ‚àó
N)‚Ä≤ by comparing the exact coeÔ¨Écients f ‚Ä≤
k with the
corresponding ones obtained by an approximate derivative, in particular,
by comparing the exact wave number wk with the approximate one, say
wk,app.

448
10. Orthogonal Polynomials in Approximation Theory
Let us neglect the subscript k and perform the comparison over the whole
interval [0, œÄ) where wk is varying. It is clear that methods based on the
Fourier expansion have wapp = w if w Ã∏= œÄ (wapp = 0 if w = œÄ). The family
of schemes (10.67) is instead characterized by the wave number
wapp(z) = a sin(z) + (b/2) sin(2z) + (c/3) sin(3z)
1 + 2Œ± cos(z) + 2Œ≤ cos(2z)
,
z ‚àà[0, œÄ)
(see [Lel92]). Figure 10.6 displays a comparison among wave numbers of
several schemes, of compact and non compact type.
The range of values for which the wave number computed by the numer-
ical scheme adequately approximates the exact wave number, is the set of
well-resolved waves. As a consequence, if wmin is the smallest well-resolved
wave, the diÔ¨Äerence 1 ‚àíwmin/œÄ represents the fraction of waves that are
unresolved by the numerical scheme. As can be seen in Figure 10.6, the
standard Ô¨Ånite diÔ¨Äerence schemes approximate correctly the exact wave
number only for small wave numbers.
0.5
1
1.5
2
2.5
3
0
0.5
1
1.5
2
2.5
3
(a)
(b)
(c)
(d)
FIGURE 10.6. Computed wave numbers for centred Ô¨Ånite diÔ¨Äerences (10.61) (a)
and for compact schemes of fourth (b), sixth (c) and tenth (d) order, compared
with the exact wave number (the straight line). On the x axis the normalized
coordinate s is represented
10.10.3
Pseudo-Spectral Derivative
An alternative way for numerical diÔ¨Äerentiation consists of approximating
the Ô¨Årst derivative of a function f with the exact Ô¨Årst derivative of the
polynomial Œ†nf interpolating f at the nodes {x0, . . . , xn}.
Exactly as happens for Lagrange interpolation, using equally spaced
nodes does not yield stable approximations to the Ô¨Årst derivative of f for
n large. For this reason, we limit ourselves to considering the case where
the nodes are nonuniformly distributed according to the Gauss-Lobatto-
Chebyshev formula.

10.10 Approximation of Function Derivatives
449
For simplicity, assume that I = [a, b] = [‚àí1, 1] and for n ‚â•1, take in
I the Gauss-Lobatto-Chebyshev nodes as in (10.21). Then, consider the
Lagrange interpolating polynomial Œ†GL
n,wf, introduced in Section 10.3. We
deÔ¨Åne the pseudo-spectral derivative of f ‚ààC0(I) to be the derivative of
the polynomial Œ†GL
n,wf
Dnf = (Œ†GL
n,wf)‚Ä≤ ‚ààPn‚àí1(I).
The error made in replacing f ‚Ä≤ with Dnf is of exponential type, that is, it
only depends on the smoothness of the function f. More precisely, there
exists a constant C > 0 independent of n such that
‚à•f ‚Ä≤ ‚àíDnf‚à•w ‚â§Cn1‚àím‚à•f‚à•m,w,
(10.71)
for any m ‚â•2 such that the norm ‚à•f‚à•m,w, introduced in (10.23), is Ô¨Ånite.
Recalling (10.19) and using (10.27) yields
(Dnf)(¬Øxi) =
n

j=0
f(¬Øxj)¬Øl‚Ä≤
j(¬Øxi),
i = 0, . . . , n,
(10.72)
so that the pseudo-spectral derivative at the interpolation nodes can be
computed knowing only the nodal values of f and of ¬Øl‚Ä≤
j. These values can
be computed once for all and stored in a matrix D ‚ààR(n+1)√ó(n+1): Dij =
¬Øl‚Ä≤
j(¬Øxi) for i, j = 0, ..., n, called a pseudo-spectral diÔ¨Äerentiation matrix.
Relation (10.72) can thus be cast in matrix form as f ‚Ä≤ = Df, letting
f = [f(¬Øxi)] and f ‚Ä≤ = [(Dnf)(¬Øxi)] for i = 0, ..., n.
The entries of D have the following explicit form (see [CHQZ88], p. 69)
Dlj =
Ô£±
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≥
dl
dj
(‚àí1)l+j
¬Øxl ‚àí¬Øxj
,
l Ã∏= j,
‚àí¬Øxj
2(1 ‚àí¬Øx2
j),
1 ‚â§l = j ‚â§n ‚àí1,
‚àí2n2 + 1
6
,
l = j = 0,
2n2 + 1
6
,
l = j = n,
(10.73)
where the coeÔ¨Écients dl have been deÔ¨Åned in Section 10.3 (see also Example
5.13 concerning the approximation of the multiple eigenvalue Œª = 0 of D).
To compute the pseudo-spectral derivative of a function f over the generic
interval [a, b], we only have to resort to the change of variables considered
in Remark 10.3.
The second-order pseudo-spectral derivative can be computed as the
product of the matrix D and the vector f ‚Ä≤, that is, f ‚Ä≤‚Ä≤ = Df ‚Ä≤, or by di-
rectly applying matrix D2 to the vector f.

450
10. Orthogonal Polynomials in Approximation Theory
10.11
Transforms and Their Applications
In this section we provide a short introduction to the most relevant integral
transforms and discuss their basic analytical and numerical properties.
10.11.1
The Fourier Transform
DeÔ¨Ånition 10.1 Let L1(R) denote the space of real or complex functions
deÔ¨Åned on the real line such that
‚àû
>
‚àí‚àû
|f(t)| dt < +‚àû.
For any f ‚ààL1(R) its Fourier transform is a complex-valued function
F = F[f] deÔ¨Åned as
F(ŒΩ) =
‚àû
>
‚àí‚àû
f(t)e‚àíi2œÄŒΩt dt.
‚ñ†
Should the independent variable t denote time, then ŒΩ would have the
meaning of frequency. Thus, the Fourier transform is a mapping that to a
function of time (typically, real-valued) associates a complex-valued func-
tion of frequency.
The following result provides the conditions under which an inversion for-
mula exists that allows us to recover the function f from its Fourier trans-
form F (for the proof see [Rud83], p. 199).
Property 10.3 (inversion theorem) Let f be a given function in L1(R),
F ‚ààL1(R) be its Fourier transform and g be the function deÔ¨Åned by
g(t) =
‚àû
>
‚àí‚àû
F(ŒΩ)ei2œÄŒΩt dŒΩ,
t ‚ààR.
(10.74)
Then g ‚ààC0(R), with lim|x|‚Üí‚àûg(x) = 0, and f(t) = g(t) almost every-
where in R (i.e., for any t unless possibly a set of zero measure).
The integral at right hand side of (10.74) is to be meant in the Cauchy
principal value sense, i.e., we let
‚àû
>
‚àí‚àû
F(ŒΩ)ei2œÄŒΩt dŒΩ = lim
a‚Üí‚àû
a
>
‚àía
F(ŒΩ)ei2œÄŒΩt dŒΩ

10.11 Transforms and Their Applications
451
and we call it the inverse Fourier transform or inversion formula of the
Fourier transform. This mapping that associates to the complex function
F the generating function f will be denoted by F‚àí1[F], i.e., F = F[f] iÔ¨Ä
f = F‚àí1[F].
Let us brieÔ¨Çy summarize the main properties of the Fourier transform and
its inverse.
1. F and F‚àí1 are linear operators, i.e.
F[Œ±f + Œ≤g] = Œ±F[f] + Œ≤F[g],
‚àÄŒ±, Œ≤ ‚ààC,
F‚àí1[Œ±F + Œ≤G] = Œ±F‚àí1[F] + Œ≤F‚àí1[G],
‚àÄŒ±, Œ≤ ‚ààC;
(10.75)
2. scaling: if Œ± is any nonzero real number and fŒ± is the function fŒ±(t) =
f(Œ±t), then
F[fŒ±] = 1
|Œ±|F 1
Œ±
where F 1
Œ± (ŒΩ) = F(ŒΩ/Œ±);
3. duality: let f(t) be a given function and F(ŒΩ) be its Fourier trans-
form. Then the function g(t) = F(‚àít) has a Fourier transform given
by f(ŒΩ). Thus, once an associated function-transform pair is found,
another dual pair is automatically generated. An application of this
property is provided by the pair r(t)-F[r] in Example 10.5;
4. parity: if f(t) is a real even function then F(ŒΩ) is real and even, while
if f(t) is a real and odd function then F(ŒΩ) is imaginary and odd.
This property allows one to work only with nonnegative frequencies;
5. convolution and product: for any given functions f, g ‚ààL1(R), we
have
F[f ‚àóg] = F[f]F[g],
F[fg] = F ‚àóG,
(10.76)
where the convolution integral of two functions œÜ and œà is given by
(œÜ ‚àóœà)(t) =
‚àû
>
‚àí‚àû
œÜ(œÑ)œà(t ‚àíœÑ) dœÑ.
(10.77)
Example 10.5 We provide two examples of the computation of the Fourier
transforms of functions that are typically encountered in signal processing.
Let us Ô¨Årst consider the square wave (or rectangular) function r(t) deÔ¨Åned as
r(t) =
" A
if ‚àíT
2 ‚â§t ‚â§T
2 ,
0
otherwise,

452
10. Orthogonal Polynomials in Approximation Theory
where T and A are two given positive numbers. Its Fourier transform F[r] is the
function
F(ŒΩ) =
T/2
>
‚àíT/2
Ae‚àíi2œÄŒΩt dt = AT sin(œÄŒΩT)
œÄŒΩT
,
ŒΩ ‚ààR
where AT is the area of the rectangular function.
Let us consider the sawtooth function
s(t) =
Ô£±
Ô£≤
Ô£≥
2At
T
if ‚àíT
2 ‚â§t ‚â§T
2 ,
0
otherwise,
whose DFT is shown in Figure 10.3 and whose Fourier transform F[s] is the
function
F(ŒΩ) = i AT
œÄŒΩT

cos(œÄŒΩT) ‚àísin(œÄŒΩT)
œÄŒΩT

,
ŒΩ ‚ààR
and is purely imaginary since s is an odd real function. Notice also that the
functions r and s have a Ô¨Ånite support whereas their transforms have an inÔ¨Ånite
support (see Figure 10.7). In signal theory this corresponds to saying that the
transform has an inÔ¨Ånite bandwidth.
‚Ä¢
‚àí10
‚àí8
‚àí6
‚àí4
‚àí2
0
2
4
6
8
10
‚àí0.4
‚àí0.2
0
0.2
0.4
0.6
0.8
1
,
‚àí10
‚àí8
‚àí6
‚àí4
‚àí2
0
2
4
6
8
10
‚àí0.5
‚àí0.4
‚àí0.3
‚àí0.2
‚àí0.1
0
0.1
0.2
0.3
0.4
0.5
FIGURE 10.7. Fourier transforms of the rectangular (left) and the sawtooth
(right) functions
Example 10.6 The Fourier transform of a sinusoidal function is of paramount
interest in signal and communication systems. To start with, consider the constant
function f(t) = A for a given A ‚ààR. Since it has an inÔ¨Ånite time duration its
Fourier transform F[A] is the function
F(ŒΩ) = lim
a‚Üí‚àû
a
>
‚àía
Ae‚àíi2œÄŒΩt dt = A lim
a‚Üí‚àû
sin(2œÄŒΩa)
œÄŒΩ
,
where the integral above is again the Cauchy principal value of the corresponding
integral over (‚àí‚àû, ‚àû). It can be proved that the limit exists and is unique in the
sense of distributions (see Section 12.4) yielding
F(ŒΩ) = AŒ¥(ŒΩ),
(10.78)

10.11 Transforms and Their Applications
453
where Œ¥ is the so-called Dirac mass, i.e., a distribution that satisÔ¨Åes
> ‚àû
‚àí‚àû
Œ¥(Œæ)œÜ(Œæ) dŒæ = œÜ(0)
(10.79)
for any function œÜ continuous at the origin. From (10.78) we see that the trans-
form of a function with inÔ¨Ånite time duration has a Ô¨Ånite bandwidth.
Let us now consider the computation of the Fourier transform of the function
f(t) = A cos(2œÄŒΩ0t) where ŒΩ0 is a Ô¨Åxed frequency. Recalling Euler‚Äôs formula
cos(Œ∏) = eiŒ∏ + e‚àíiŒ∏
2
,
Œ∏ ‚ààR,
and applying (10.78) twice we get
F[A cos(2œÄŒΩ0t)] = A
2 Œ¥(ŒΩ ‚àíŒΩ0) + A
2 Œ¥(ŒΩ + ŒΩ0),
which shows that the spectrum of a sinusoidal function with frequency ŒΩ0 is
centred around ¬±ŒΩ0 (notice that the transform is even and real since the same
holds for the function f(t)).
‚Ä¢
It is worth noting that in real-life there do not exist functions (i.e. signals)
with inÔ¨Ånite duration or bandwidth. Actually, if f(t) is a function whose
value may be considered as ‚Äúnegligible‚Äù outside of some interval (ta, tb),
then we can assume that the eÔ¨Äective duration of f is the length ‚àÜt =
tb ‚àíta. In a similar manner, if F(ŒΩ) is the Fourier transform of f and
it happens that F(ŒΩ) may be considered as ‚Äúnegligible‚Äù outside of some
interval (ŒΩa, ŒΩb), then the eÔ¨Äective bandwidth of f is ‚àÜŒΩ = ŒΩb‚àíŒΩa. Referring
to Figure 10.7, we clearly see that the eÔ¨Äective bandwidth of the rectangular
function can be taken as (‚àí10, 10).
10.11.2
(Physical) Linear Systems and Fourier Transform
Mathematically speaking, a physical linear system (LS) can be regarded
as a linear operator S that enjoys the linearity property (10.75). Denoting
by i(t) and u(t) an admissible input function for S and its corresponding
output function respectively, the LS can be represented as u(t) = S(i(t))
or S : i ‚Üíu. A special category of LS are the so-called shift invariant (or
time-invariant) linear systems (ILS) which satisfy the property
S(i(t ‚àít0)) = u(t ‚àít0),
‚àÄt0 ‚ààR
and for any admissible input function i.
Let S be an ILS system and let f and g be two admissible input functions
for S with w = S(g). An immediate consequence of the linearity and shift-
invariance is that
S((f ‚àóg)(t)) = (f ‚àóS(g))(t) = (f ‚àów)(t)
(10.80)

454
10. Orthogonal Polynomials in Approximation Theory
where ‚àóis the convolution operator deÔ¨Åned in (10.77).
Assume we take as input function the impulse function Œ¥(t) introduced in
the previous section and denote by h(t) = S(Œ¥(t)) the corresponding output
through S (usually referred to as the system impulse response function).
Property (10.79) implies that for any function œÜ, (œÜ ‚àóŒ¥)(t) = œÜ(t), so that,
recalling (10.80) and taking œÜ(t) = i(t) we have
u(t) = S(i(t)) = S(i ‚àóŒ¥)(t) = (i ‚àóS(Œ¥))(t) = (i ‚àóh)(t).
Thus, S can be completely described through its impulse response function.
Equivalently, we can pass to the frequency domain by means of the Ô¨Årst
relation in (10.76) obtaining
U(ŒΩ) = I(ŒΩ)H(ŒΩ),
(10.81)
where I, U and H are the Fourier transforms of i(t), u(t) and h(t), respec-
tively; H is the so-called system transfer function.
Relation (10.81) plays a central role in the analysis of linear time-invariant
systems as it is simpler to deal with the system transfer function than with
the corresponding impulse response function, as demonstrated in the fol-
lowing example.
Example 10.7 (ideal low-pass Ô¨Ålter) An ideal low-pass Ô¨Ålter is an ILS char-
acterized by the transfer function
H(ŒΩ) =
% 1,
if |ŒΩ| ‚â§ŒΩ0/2,
0,
otherwise.
Using the duality property, the impulse response function F‚àí1[H] is
h(t) = ŒΩ0 sin(œÄŒΩ0t)
œÄŒΩ0t
.
Given an input signal i(t) with Fourier transform I(ŒΩ), the corresponding output
u(t) has a spectrum given by (10.81)
I(ŒΩ)H(ŒΩ) =
% I(ŒΩ),
if |ŒΩ| ‚â§ŒΩ0/2,
0
otherwise.
The eÔ¨Äect of the Ô¨Ålter is to cut oÔ¨Äthe input frequencies that lie outside the
window |ŒΩ| ‚â§ŒΩ0/2.
‚Ä¢
The input/output functions i(t) and u(t) usually denote signals and
the linear system described by H(ŒΩ) is typically a communication system.
Therefore, as pointed out at the end of Section 10.11.1, we are legitimated
in assuming that both i(t) and u(t) have a Ô¨Ånite eÔ¨Äective duration. In

10.11 Transforms and Their Applications
455
particular, referring to i(t) we suppose i(t) = 0 if t Ã∏‚àà[0, T0). Then, the
computation of the Fourier transform of i(t) yields
I(ŒΩ) =
T0
>
0
i(t)e‚àíi2œÄŒΩt dt.
Letting ‚àÜt = T0/n for n ‚â•1 and approximating the integral above by the
composite trapezoidal formula (9.14), we get
ÀúI(ŒΩ) = ‚àÜt
n‚àí1

k=0
i(k‚àÜt)e‚àíi2œÄŒΩk‚àÜt.
It can be proved (see, e.g., [Pap62]) that ÀúI(ŒΩ)/‚àÜt is the Fourier transform
of the so-called sampled signal
is(t) =
‚àû

k=‚àí‚àû
i(k‚àÜt)Œ¥(t ‚àík‚àÜt),
where Œ¥(t ‚àík‚àÜt) is the Dirac mass at k‚àÜt. Then, using the convolution
and the duality properties of the Fourier transform, we get
ÀúI(ŒΩ) =
‚àû

j=‚àí‚àû
I

ŒΩ ‚àíj
‚àÜt

,
(10.82)
which amounts to replacing I(ŒΩ) by its periodic repetition with period
1/‚àÜt. Let J‚àÜt = [‚àí
1
2‚àÜt,
1
2‚àÜt]; then, it suÔ¨Éces to compute (10.82) for ŒΩ ‚àà
J‚àÜt. This can be done numerically by introducing a uniform discretization
of J‚àÜt with frequency step ŒΩ0 = 1/(m‚àÜt) for m ‚â•1. By doing so, the
computation of ÀúI(ŒΩ) requires evaluating the following m+1 discrete Fourier
transforms (DFT)
ÀúI(jŒΩ0) = ‚àÜt
n‚àí1

k=0
i(k‚àÜt)e‚àíi2œÄjŒΩ0k‚àÜt,
j = ‚àím
2 , . . . , m
2 .
For an eÔ¨Écient computation of each DFT in the formula above it is crucial
to use the FFT algorithm described in Section 10.9.2.
10.11.3
The Laplace Transform
The Laplace transform can be employed to solve ordinary diÔ¨Äerential equa-
tions with constant coeÔ¨Écients as well as partial diÔ¨Äerential equations.
DeÔ¨Ånition 10.2 Let f ‚ààL1
loc([0, ‚àû)) i.e., f ‚ààL1([0, T]) for any T > 0.
Let s = œÉ + iœâ be a complex variable. The Laplace integral of f is deÔ¨Åned

456
10. Orthogonal Polynomials in Approximation Theory
as
‚àû
>
0
f(t)e‚àíst dt = lim
T ‚Üí‚àû
T
>
0
f(t)e‚àíst dt.
If this integral exists for some s, it turns out to be a function of s; then,
the Laplace transform L[f] of f is the function
L(s) =
‚àû
>
0
f(t)e‚àíst dt.
‚ñ†
The following relation between Laplace and Fourier transforms holds
L(s) = F(e‚àíœÉt Àúf(t)),
where Àúf(t) = f(t) if t ‚â•0 while Àúf(t) = 0 if t < 0.
Example 10.8 The Laplace transform of the unit step function f(t) = 1 if t > 0,
f(t) = 0 otherwise, is given by
L(s) =
‚àû
>
0
e‚àíst dt = 1
s .
We notice that the Laplace integral exists if œÉ > 0.
‚Ä¢
In Example 10.8 the convergence region of the Laplace integral is the half-
plane {Re(s) > 0} of the complex Ô¨Åeld. This property is quite general, as
stated by the following result.
Property 10.4 If the Laplace transform exists for s = ¬Øs then it exists
for all s with Re(s) > Re(¬Øs). Moreover, let E be the set of the real parts
of s such that the Laplace integral exists and denote by Œª the inÔ¨Åmum of
E. If Œª happens to be Ô¨Ånite, the Laplace integral exists in the half-plane
Re(s) > Œª. If Œª = ‚àí‚àûthen it exists for all s ‚ààC; Œª is called the abscissa
of convergence.
We recall that the Laplace transform enjoys properties completely analo-
gous to those of the Fourier transform. The inverse Laplace transform is
denoted formally as L‚àí1 and is such that
f(t) = L‚àí1[L(s)].

10.11 Transforms and Their Applications
457
Example 10.9 Let us consider the ordinary diÔ¨Äerential equation y‚Ä≤(t)+ay(t) =
g(t) with y(0) = y0. Multiplying by est, integrating between 0 and ‚àûand passing
to the Laplace transform, yields
sY (s) ‚àíy0 + aY (s) = G(s).
(10.83)
Should G(s) be easily computable, (10.83) would furnish Y (s) and then, by ap-
plying the inverse Laplace transform, the generating function y(t). For instance,
if g(t) is the unit step function, we obtain
y(t) = L‚àí1
% 1
a
1
s ‚àí
1
s + a

+
y0
s + a
&
= 1
a(1 ‚àíe‚àíat) + y0e‚àíat.
‚Ä¢
For an extensive presentation and analysis of the Laplace transform see,
e.g., [Tit37]. In the next section we describe a discrete version of the Laplace
transform, known as the Z-transform.
10.11.4
The Z-Transform
DeÔ¨Ånition 10.3 Let f be a given function, deÔ¨Åned for any t ‚â•0, and
‚àÜt > 0 be a given time step. The function
Z(z) =
‚àû

n=0
f(n‚àÜt)z‚àín,
z ‚ààC
(10.84)
is called the Z-transform of the sequence {f(n‚àÜt)} and is denoted by
Z[f(n‚àÜt)].
‚ñ†
The parameter ‚àÜt is the sampling time step of the sequence of samples
f(n‚àÜt). The inÔ¨Ånite sum (10.84) converges if
|z| > R = lim sup
n‚Üí‚àû
n
|f(n‚àÜt)|.
It is possible to deduce the Z-transform from the Laplace transform as
follows. Denoting by f0(t) the piecewise constant function such that f0(t) =
f(n‚àÜt) for t ‚àà(n‚àÜt, (n + 1)‚àÜt), the Laplace transform L[f0] of f0 is the
function
L(s)
=
‚àû
>
0
f0(t)e‚àíst dt =
‚àû

n=0
(n+1)‚àÜt
>
n‚àÜt
e‚àístf(n‚àÜt) dt
=
‚àû

n=0
f(n‚àÜt)e‚àíns‚àÜt ‚àíe‚àí(n+1)s‚àÜt
s
=
1 ‚àíe‚àís‚àÜt
s
 ‚àû

n=0
f(n‚àÜt)e‚àíns‚àÜt.

458
10. Orthogonal Polynomials in Approximation Theory
The discrete Laplace transform Zd[f0] of f0 is the function
Zd(s) =
‚àû

n=0
f(n‚àÜt)e‚àíns‚àÜt.
Then, the Z-transform of the sequence {f(n‚àÜt), n = 0, . . . , ‚àû} coincides
with the discrete Laplace transform of f0 up to the change of variable
z = e‚àís‚àÜt. The Z-transform enjoys similar properties (linearity, scaling,
convolution and product) to those already seen in the continuous case.
The inverse Z-transform is denoted by Z‚àí1 and is deÔ¨Åned as
f(n‚àÜt) = Z‚àí1[Z(z)].
The practical computation of Z‚àí1 can be carried out by resorting to classi-
cal techniques of complex analysis (for example, using the Laurent formula
or the Cauchy theorem for residual integral evaluation) coupled with an
extensive use of tables (see, e.g., [Pou96]).
10.12
The Wavelet Transform
This technique, originally developed in the area of signal processing, has
successively been extended to many diÔ¨Äerent branches of approximation
theory, including the solution of diÔ¨Äerential equations. It is based on the
so-called wavelets, which are functions generated by an elementary wavelet
through traslations and dilations. We shall limit ourselves to a brief intro-
duction of univariate wavelets and their transform in both the continuous
and discrete cases referring to [DL92], [Dau88] and to the references cited
therein for a detailed presentation and analysis.
10.12.1
The Continuous Wavelet Transform
Any function
hs,œÑ(t) = 1
‚àösh
t ‚àíœÑ
s

,
t ‚ààR
(10.85)
that is obtained from a reference function h ‚ààL2(R) by means of traslations
by a traslation factor œÑ and dilations by a positive scaling factor s is called
a wavelet. The function h is called an elementary wavelet.
Its Fourier transform, written in terms of œâ = 2œÄŒΩ, is
Hs,œÑ(œâ) = ‚àösH(sœâ)e‚àíiœâœÑ,
(10.86)
where i denotes the imaginary unit and H(œâ) is the Fourier transform of
the elementary wavelet. A dilation t/s (s > 1) in the real domain produces

10.12 The Wavelet Transform
459
therefore a contraction sœâ in the frequency domain. Therefore, the factor
1/s plays the role of the frequency ŒΩ in the Fourier transform (see Section
10.11.1). In wavelets theory s is usually referred to as the scale. Formula
(10.86) is known as the Ô¨Ålter of the wavelet transform.
DeÔ¨Ånition 10.4 Given a function f ‚ààL2(R), its continuous wavelet trans-
form Wf = W[f] is a decomposition of f(t) onto a wavelet basis {hs,œÑ(t)},
that is
Wf(s, œÑ) =
‚àû
>
‚àí‚àû
f(t)¬Øhs,œÑ(t) dt,
(10.87)
where the overline bar denotes complex conjugate.
‚ñ†
When t denotes the time-variable, the wavelet transform of f(t) is a func-
tion of the two variables s (scale) and œÑ (time shift); as such, it is a repre-
sentation of f in the time-scale space and is usually referred to as time-scale
joint representation of f. The time-scale representation is the analogue of
the time-frequency representation introduced in the Fourier analysis. This
latter representation has an intrinsic limitation: the product of the res-
olution in time ‚àÜt and the resolution in frequency ‚àÜœâ must satisfy the
following constraint (Heisenberg inequality)
‚àÜt‚àÜœâ ‚â•1
2
(10.88)
which is the counterpart of the Heisenberg uncertainty principle in quantum
mechanics. This inequality states that a signal cannot be represented as
a point in the time-frequency space. We can only determine its position
within a rectangle of area ‚àÜt‚àÜœâ in the time-frequency space.
The wavelet transform (10.87) can be rewritten in terms of the Fourier
transform F(œâ) of f as
Wf(s, œÑ) =
‚àös
2œÄ
‚àû
>
‚àí‚àû
F(œâ) ¬ØH(sœâ)eiœâœÑ dœâ,
which shows that the wavelets transform is a bank of wavelet Ô¨Ålters char-
acterized by diÔ¨Äerent scales. More precisely, if the scale is small the wavelet
is concentrated in time and the wavelet transform provides a detailed de-
scription of f(t) (which is the signal). Conversely, if the scale is large, the
wavelet transform is able to resolve only the large-scale details of f. Thus,
the wavelet transform can be regarded as a bank of multiresolution Ô¨Ålters.
The theoretical properties of this transform do not depend on the partic-
ular elementary wavelet that is considered. Hence, speciÔ¨Åc bases of wavelets
can be derived for speciÔ¨Åc applications. Some examples of elementary wave-
lets are reported below.

460
10. Orthogonal Polynomials in Approximation Theory
Example 10.10 (Haar wavelets) These functions can be obtained by choos-
ing as the elementary wavelet the Haar function deÔ¨Åned as
h(x) =
Ô£±
Ô£≤
Ô£≥
1
if x ‚àà(0, 1
2),
‚àí1
if x ‚àà( 1
2, 1),
0
otherwise.
Its Fourier transform is the complex-valued function
H(œâ) = 4ie‚àíiœâ/2 +
1 ‚àícos(œâ
2 )
,
/œâ,
which has symmetric module with respect to the origin (see Figure 10.8). The
bases that are obtained from this wavelet are not used in practice due to their
ineÔ¨Äective localization properties in the frequency domain.
‚Ä¢
‚àí0.5
0
0.5
1
1.5
‚àí1.5
‚àí1
‚àí0.5
0
0.5
1
1.5
‚àí80
‚àí60
‚àí40
‚àí20
0
20
40
60
80
0
0.5
1
1.5
FIGURE 10.8. The Haar wavelet (left) and the module of its Fourier transform
(right)
Example 10.11 (Morlet wavelets) The Morlet wavelet is deÔ¨Åned as follows
(see [MMG87])
h(x) = eiœâ0xe‚àíx2/2.
Thus, it is a complex-valued function whose real part has a real positive Fourier
transform, symmetric with respect to the origin, given by
H(œâ) = ‚àöœÄ
6
e‚àí(œâ‚àíœâ0)2/2 + e‚àí(œâ+œâ0)2/27
.
‚Ä¢
We point out that the presence of the dilation factor allows for the wavelets
to easily handle possible discontinuities or singularities in f. Indeed, using
the multi-resolution analysis, the signal, properly divided into frequency
bandwidths, can be processed at each frequency by suitably tuning up the
scale factor of the wavelets.

10.12 The Wavelet Transform
461
‚àí10
‚àí8
‚àí6
‚àí4
‚àí2
0
2
4
6
8
10
‚àí1
‚àí0.8
‚àí0.6
‚àí0.4
‚àí0.2
0
0.2
0.4
0.6
0.8
1
‚àí10
‚àí8
‚àí6
‚àí4
‚àí2
0
2
4
6
8
10
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
FIGURE 10.9. The real part of the Morlet wavelet (left) and the real part of the
corresponding Fourier transforms (right) for œâ0 = 1 (solid line), œâ0 = 2.5 (dashed
line) and œâ0 = 5 (dotted line)
Recalling what was already pointed out in Section 10.11.1, the time lo-
calization of the wavelet gives rise to a Ô¨Ålter with inÔ¨Ånite bandwidth. In
particular, deÔ¨Åning the bandwidth ‚àÜœâ of the wavelet Ô¨Ålter as
‚àÜœâ =
Ô£´
Ô£≠
‚àû
>
‚àí‚àû
œâ2|H(œâ)|2 dœâ/
‚àû
>
‚àí‚àû
|H(œâ)|2 dœâ
Ô£∂
Ô£∏
2
,
then the bandwidth of the wavelet Ô¨Ålter with scale equal to s is
‚àÜœâs =
Ô£´
Ô£≠
‚àû
>
‚àí‚àû
œâ2|H(sœâ)|2 dœâ/
‚àû
>
‚àí‚àû
|H(sœâ)|2 dœâ
Ô£∂
Ô£∏
2
= 1
s‚àÜœâ.
Consequently, the quality factor Q of the wavelet Ô¨Ålter, deÔ¨Åned as the in-
verse of the bandwidth of the Ô¨Ålter, is independent of s since
Q = 1/s
‚àÜœâs
= ‚àÜœâ
provided that (10.88) holds. At low frequencies, corresponding to large
values of s, the wavelet Ô¨Ålter has a small bandwidth and a large temporal
width (called window) with a low resolution. Conversely, at high frequencies
the Ô¨Ålter has a large bandwidth and a small temporal window with a high
resolution. Thus, the resolution furnished by the wavelet analysis increases
with the frequency of the signal. This property of adaptivity makes the
wavelets a crucial tool in the analysis of unsteady signals or signals with
fast transients for which the standard Fourier analysis turns out to be
ineÔ¨Äective.
10.12.2
Discrete and Orthonormal Wavelets
The continuous wavelet transform maps a function of one variable into a bi-
dimensional representation in the time-scale domain. In many applications

462
10. Orthogonal Polynomials in Approximation Theory
this description is excessively rich. Resorting to the discrete wavelets is
an attempt to represent a function using a Ô¨Ånite (and small) number of
parameters.
A discrete wavelet is a continuous wavelet that is generated by using
discrete scale and translation factors. For s0 > 1, denote by s = sj
0 the
scale factors; the dilation factors usually depend on the scale factors by
setting œÑ = kœÑ0sj
0, œÑ0 ‚ààR. The corresponding discrete wavelet is
hj,k(t) = s‚àíj/2
0
h(s‚àíj
0 (t ‚àíkœÑ0sj
0)) = s‚àíj/2
0
h(s‚àíj
0 t ‚àíkœÑ0).
The scale factor sj
0 corresponds to the magniÔ¨Åcation or the resolution of
the observation, while the translation factor œÑ0 is the location where the
observations are made. If one looks at very small details, the magniÔ¨Åcation
must be large, which corresponds to large negative index j. In this case the
step of translation is small and the wavelet is very concentrated around the
observation point. For large and positive j, the wavelet is spread out and
large translation steps are used.
The behavior of the discrete wavelets depends on the steps s0 and œÑ0.
When s0 is close to 1 and œÑ0 is small, the discrete wavelets are close to the
continuous ones. For a Ô¨Åxed scale s0 the localization points of the discrete
wavelets along the scale axis are logarithmic as log s = j log s0. The choice
s0 = 2 corresponds to the dyadic sampling in frequency. The discrete time-
step is œÑ0sj
0 and, typically, œÑ0 = 1. Hence, the time-sampling step is a
function of the scale and along the time axis the localization points of the
wavelet depend on the scale.
For a given function f ‚ààL1(R), the corresponding discrete wavelet trans-
form is
Wf(j, k) =
‚àû
>
‚àí‚àû
f(t)¬Øhj,k(t) dt.
It is possible to introduce an orthonormal wavelet basis using discrete di-
lation and traslation factors, i.e.
‚àû
>
‚àí‚àû
hi,j¬Øhk,l(t) dt = Œ¥ikŒ¥jl,
‚àÄi, j, k, l ‚ààZ.
With an orthogonal wavelet basis, an arbitrary function f can be recon-
structed by the expansion
f(t) = A

j,k‚ààZ
Wf(j, k)hj,k(t),
where A is a constant that does not depend on f.
As of the computational standpoint, the wavelet discrete transform can
be implemented at even a cheaper cost than the FFT algorithm for com-
puting the Fourier transform.

10.13 Applications
463
10.13
Applications
In this section we apply the theory of orthogonal polynomials to solve two
problems arising in quantum physics. In the Ô¨Årst example we deal with
Gauss-Laguerre quadratures, while in the second case the Fourier analysis
and the FFT are considered.
10.13.1
Numerical Computation of Blackbody Radiation
The monochromatic energy density E(ŒΩ) of blackbody radiation as a func-
tion of frequency ŒΩ is expressed by the following law
E(ŒΩ) = 8œÄh
c3
ŒΩ3
ehŒΩ/KBT ‚àí1,
where h is the Planck constant, c is the speed of light, KB is the Boltz-
mann constant and T is the absolute temperature of the blackbody (see,
for instance, [AF83]).
To compute the total density of monochromatic energy that is emitted
by the blackbody (that is, the emitted energy per unit volume) we must
evaluate the integral
E =
‚àû
>
0
E(ŒΩ)dŒΩ = Œ±T 4
‚àû
>
0
x3
ex ‚àí1dx,
where x = hŒΩ/KBT and Œ± = (8œÄK4
B)/(ch)3 ‚âÉ1.16 ¬∑ 10‚àí16 [J][K‚àí4][m‚àí3].
We also let f(x) = x3/(ex ‚àí1) and I(f) =
 ‚àû
0
f(x)dx.
To approximate I(f) up to a previously Ô¨Åxed absolute error ‚â§Œ¥, we com-
pare method 1. introduced in Section 9.8.3 with Gauss-Laguerre quadra-
tures.
In the case of method 1. we proceed as follows. For any a > 0 we let
I(f) =
 a
0 f(x)dx +
 ‚àû
a f(x)dx and try to Ô¨Ånd a function œÜ such that
‚àû
>
a
f(x)dx ‚â§
‚àû
>
a
œÜ(x)dx ‚â§Œ¥
2,
(10.89)
the integral
 ‚àû
a œÜ(x)dx being ‚Äúeasy‚Äù to compute. Once the value of a
has been found such that (10.89) is fulÔ¨Ålled, we compute the integral
I1(f) =
 a
0 f(x)dx using for instance the adaptive Cavalieri-Simpson for-
mula introduced in Section 9.7.2 and denoted in the following by AD.
A natural choice of a bounding function for f is œÜ(x) = Kx3e‚àíx, for a
suitable constant K > 1. Thus, we have K ‚â•ex/(ex ‚àí1), for any x > 0,
that is, letting x = a, K = ea/(ea‚àí1). Substituting back into (10.89) yields
‚àû
>
a
f(x)dx ‚â§a3 + 3a2 + 6a + 6
ea ‚àí1
= Œ∑(a) ‚â§Œ¥
2.

464
10. Orthogonal Polynomials in Approximation Theory
0
2
4
6
8
10
12
14
16
‚àí0.2
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
FIGURE 10.10. Distribution of quadrature nodes and graph of the integrand
function
Letting Œ¥ = 10‚àí3, we see that (10.89) is satisÔ¨Åed by taking a ‚âÉ16. Pro-
gram 77 for computing I1(f) with the AD method, setting hmin=10‚àí3 and
tol=5 ¬∑ 10‚àí4, yields the approximate value I1 ‚âÉ6.4934 with a number of
(nonuniform) partitions equal to 25.
The distribution of the quadrature nodes produced by the adaptive algo-
rithm is plotted in Figure 10.10. Globally, using method 1. yields an approx-
imation of I(f) equal to 6.4984. Table 10.1 shows, for sake of comparison,
some approximate values of I(f) obtained using the Gauss-Laguerre formu-
lae with the number of nodes varying between 2 to 20. Notice that, taking
n = 4 nodes, the accuracy of the two computational procedures is roughly
equivalent.
n
In(f)
2
6.413727469517582
3
6.481130171540022
4
6.494535639802632
5
6.494313365790864
10
6.493939967652101
15
6.493939402671590
20
6.493939402219742
TABLE 10.1. Approximate evaluation of I(f)
=
 ‚àû
0
x3/(ex ‚àí1)dx with
Gauss-Laguerre quadratures
10.13.2
Numerical Solution of Schr¬®odinger Equation
Let us consider the following diÔ¨Äerential equation arising in quantum me-
chanics known as the Schr¬®odinger equation
i‚àÇœà
‚àÇt = ‚àí‚Ñè
2m
‚àÇ2œà
‚àÇx2 ,
x ‚ààR
t > 0.
(10.90)
The symbols i and ‚Ñèdenote the imaginary unit and the reduced Planck
constant, respectively. The complex-valued function œà = œà(x, t), the solu-

10.13 Applications
465
tion of (10.90), is called a wave function and the quantity |œà(x, t)|2 deÔ¨Ånes
the probability density in the space x of a free electron of mass m at time
t (see [FRL55]).
The corresponding Cauchy problem may represent a physical model for
describing the motion of an electron in a cell of an inÔ¨Ånite lattice (for more
details see, e.g., [AF83]).
Consider the initial condition œà(x, 0) = w(x), where w is the step func-
tion that takes the value 1/
‚àö
2b for |x| ‚â§b and is zero for |x| > b, with
b = a/5, and where 2a represents the inter-ionic distance in the lattice.
Therefore, we are searching for periodic solutions, with period equal to 2a.
Solving problem (10.90) can be carried out using Fourier analysis as
follows. We Ô¨Årst write the Fourier series of w and œà (for any t > 0)
w(x) =
N/2‚àí1

k=‚àíN/2
wkeiœÄkx/a,
wk = 1
2a
a
>
‚àía
w(x)e‚àíiœÄkx/adx,
œà(x, t) =
N/2‚àí1

k=‚àíN/2
œàk(t)eiœÄkx/a,
œàk(t) = 1
2a
a
>
‚àía
œà(x, t)e‚àíiœÄkx/adx.
(10.91)
Then, we substitute back (10.91) into (10.90), obtaining the following
Cauchy problem for the Fourier coeÔ¨Écients œàk, for k = ‚àíN/2, . . . , N/2‚àí1
Ô£±
Ô£¥
Ô£≤
Ô£¥
Ô£≥
œà‚Ä≤
k(t) = ‚àíi ‚Ñè
2m
kœÄ
a
2
œàk(t),
œàk(0) = $wk.
(10.92)
The coeÔ¨Écients {$wk} have been computed by regularizing the coeÔ¨Écients
{ wk} of the step function w using the Lanczos smoothing (10.56) in order
to avoid the Gibbs phenomenon arising around the discontinuities of w (see
Section 10.9.1).
After solving (10.92), we Ô¨Ånally get, recalling (10.91), the following expres-
sion for the wave function
œàN(x, t) =
N/2‚àí1

k=‚àíN/2
$wke‚àíiEkt/‚ÑèeiœÄkx/a,
(10.93)
where the coeÔ¨Écients Ek = (k2œÄ2‚Ñè2)/(2ma2) represent, from the physical
standpoint, the energy levels that the electron may assume in its motion
within the potential well.
To compute the coeÔ¨Écients wk (and, as a consequence, $wk), we have used
the MATLAB intrinsic function fft (see Section 10.9.2), employing N =
26 = 64 points and letting a = 10
‚ó¶
A= 10‚àí9[m]. Time analysis has been
carried out up to T = 10 [s], with time steps of 1 [s]; in all the reported

466
10. Orthogonal Polynomials in Approximation Theory
‚àí10
‚àí5
0
5
10
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
‚àí10
‚àí5
0
5
10
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
FIGURE 10.11. Probability density |œà(x, t)|2 at t = 0, 2, 5 [s], corresponding to
a step function as initial datum: solution without Ô¨Åltering (left), with Lanczos
Ô¨Åltering (right)
graphs, the x-axis is measured in [
‚ó¶
A], while the y-axes are respectively in
units of 105 [m‚àí1/2] and 1010 [m‚àí1].
In Figure 10.11 we draw the probability density |œà(x, t)|2 at t = 0, 2
and 5 [s]. The result obtained without the regularizing procedure above is
shown on the left, while the same calculation with the ‚ÄúÔ¨Åltering‚Äù of the
Fourier coeÔ¨Écients is reported on the right. The second plot demonstrates
the smoothing eÔ¨Äect on the solution by the regularization, at the price of
a slight enlargement of the step-like initial probability distribution.
Finally, it is interesting to apply Fourier analysis to solve problem (10.90)
starting from a smooth initial datum. For this, we choose an initial probabil-
ity density w of Gaussian form such that ‚à•w‚à•2 = 1. The solution |œà(x, t)|2,
this time computed without regularization, is shown in Figure 10.12, at
t = 0, 2, 5, 7, 9[s]. Notice the absence of spurious oscillations with respect
to the previous case.
‚àí10
‚àí5
0
5
10
0
0.02
0.04
0.06
0.08
0.1
0.12
0.14
0.16
0.18
0.2
FIGURE 10.12. Probability density |œà(x, t)|2 at t = 0, 2, 5, 7, 9[s], corresponding
to an initial datum with Gaussian form

10.14 Exercises
467
10.14
Exercises
1. Prove the three-term relation (10.11).
[Hint: set x = cos(Œ∏), for 0 ‚â§Œ∏ ‚â§œÄ.]
2. Prove (10.31).
[Hint: Ô¨Årst prove that ‚à•vn‚à•n = (vn, vn)1/2, ‚à•Tk‚à•n = ‚à•Tk‚à•w for k < n and
‚à•Tn‚à•2
n = 2‚à•Tn‚à•2
w (see [QV94], formula (4.3.16)). Then, the thesis follows
from (10.29) multiplying by Tl (l Ã∏= k) and taking (¬∑, ¬∑)n.]
3. Prove (10.24) after showing that ‚à•(f ‚àíŒ†GL
n f)‚Ä≤‚à•œâ ‚â§Cn1‚àís‚à•f‚à•s,œâ.
[Hint: use the Gagliardo-Nirenberg inequality
max
‚àí1‚â§x‚â§1|f(x)| ‚â§‚à•f‚à•1/2‚à•f ‚Ä≤‚à•1/2
valid for any f ‚ààL2 with f ‚Ä≤ ‚ààL2. Next, use the relation that has been just
shown to prove (10.24).]
4. Prove that the discrete seminorm ‚à•f‚à•n = (f, f)1/2
n
is a norm for Pn.
5. Compute weights and nodes of the following quadrature formulae
b
>
a
w(x)f(x)dx =
n

i=0
œâif(xi),
in such a way that the order is maximum, setting
œâ(x) = ‚àöx,
a = 0,
b = 1,
n = 1;
œâ(x) = 2x2 + 1,
a = ‚àí1,
b = 1,
n = 0;
œâ(x) =
% 2
if 0 < x ‚â§1,
1
if
‚àí1 ‚â§x ‚â§0
a = ‚àí1,
b = 1,
n = 1.
[Solution: for œâ(x) = ‚àöx, the nodes x1 = 5
9 + 2
9

10/7, x2 = 5
9 ‚àí2
9

10/7
are obtained, from which the weights can be computed (order 3); for œâ(x) =
2x2 + 1, we get x1 = 3/5 and œâ1 = 5/3 (order 1); for œâ(x) = 2x2 + 1, we
have x1 =
1
22 +
1
22
‚àö
155, x2 =
1
22 ‚àí
1
22
‚àö
155 (order 3).]
6. Prove (10.40).
[Hint: notice that (Œ†GL
n f, Lj)n = 
k f ‚àó
k(Lk, Lj)n = . . . , distinguishing the
case j < n from the case j = n.]
7. Show that ||| ¬∑ |||, deÔ¨Åned in (10.45), is an essentially strict seminorm.
[Solution : use the Cauchy-Schwarz inequality (1.14) to check that the
triangular inequality is satisÔ¨Åed. This proves that |||¬∑||| is a seminorm. The
second part of the exercise follows after a direct computation.]
8. Consider in an interval [a, b] the nodes
xj = a +

j ‚àí1
2
  b ‚àía
m

j = 1, 2, . . . , m

468
10. Orthogonal Polynomials in Approximation Theory
for m ‚â•1. They are the midpoints of m equally spaced intervals in [a, b].
Let f be a given function; prove that the least-squares polynomial rn with
respect to the weight w(x) = 1 minimizes the error average, deÔ¨Åned as
E = lim
m‚Üí‚àû
"
1
m
m

j=1
[f(xj) ‚àírn(xj)]2
#1/2
.
9. Consider the function
F(a0, a1, . . . , an) =
1
>
0
.
f(x) ‚àí
n

j=0
ajxj
/2
dx
and determine the coeÔ¨Écients a0, a1, . . . , an in such a way that F is mini-
mized. Which kind of linear system is obtained?
[Hint: enforce the conditions ‚àÇF/‚àÇai = 0 with i = 0, 1, . . . , n. The matrix
of the Ô¨Ånal linear system is the Hilbert matrix (see Example 3.2, Chapter
3) which is strongly ill-conditioned.]

11
Numerical Solution of Ordinary
DiÔ¨Äerential Equations
In this chapter we deal with the numerical solutions of the Cauchy problem
for ordinary diÔ¨Äerential equations (henceforth abbreviated by ODEs). After
a brief review of basic notions about ODEs, we introduce the most widely
used techniques for the numerical approximation of scalar equations. The
concepts of consistency, convergence, zero-stability and absolute stability
will be addressed. Then, we extend our analysis to systems of ODEs, with
emphasis on stiÔ¨Äproblems.
11.1
The Cauchy Problem
The Cauchy problem (also known as the initial-value problem) consists of
Ô¨Ånding the solution of an ODE, in the scalar or vector case, given suitable
initial conditions. In particular, in the scalar case, denoting by I an interval
of R containing the point t0, the Cauchy problem associated with a Ô¨Årst
order ODE reads:
Ô¨Ånd a real-valued function y ‚ààC1(I), such that
" y‚Ä≤(t) = f(t, y(t)),
t ‚ààI,
y(t0) = y0,
(11.1)
where f(t, y) is a given real-valued function in the strip S = I √ó(‚àí‚àû, +‚àû),
which is continuous with respect to both variables. Should f depend on t
only through y, the diÔ¨Äerential equation is called autonomous.

470
11. Numerical Solution of Ordinary DiÔ¨Äerential Equations
Most of our analysis will be concerned with one single diÔ¨Äerential equa-
tion (scalar case). The extension to the case of systems of Ô¨Årst-order ODEs
will be addressed in Section 11.9.
If f is continuous with respect to t, then the solution to (11.1) satisÔ¨Åes
y(t) ‚àíy0 =
t
>
t0
f(œÑ, y(œÑ))dœÑ.
(11.2)
Conversely, if y is deÔ¨Åned by (11.2), then it is continuous in I and y(t0) =
y0. Moreover, since y is a primitive of the continuous function f(¬∑, y(¬∑)),
y ‚ààC1(I) and satisÔ¨Åes the diÔ¨Äerential equation y‚Ä≤(t) = f(t, y(t)).
Thus, if f is continuous the Cauchy problem (11.1) is equivalent to the
integral equation (11.2). We shall see later on how to take advantage of
this equivalence in the numerical methods.
Let us now recall two existence and uniqueness results for (11.1).
1. Local existence and uniqueness.
Suppose that f(t, y) is locally Lipschitz continuous at (t0, y0) with
respect to y, that is, there exist two neighborhoods, J ‚äÜI of t0 of
width rJ, and Œ£ of y0 of width rŒ£, and a constant L > 0, such that
|f(t, y1) ‚àíf(t, y2)| ‚â§L|y1 ‚àíy2|
‚àÄt ‚ààJ, ‚àÄy1, y2 ‚ààŒ£.
(11.3)
Then, the Cauchy problem (11.1) admits a unique solution in a neigh-
borhood of t0 with radius r0 with 0 < r0 < min(rJ, rŒ£/M, 1/L),
where M is the maximum of |f(t, y)| on J √óŒ£. This solution is called
the local solution.
Notice that condition (11.3) is automatically satisÔ¨Åed if f has con-
tinuous derivative with respect to y: indeed, in such a case it suÔ¨Éces
to choose L as the maximum of |‚àÇf(t, y)/‚àÇy| in J √ó Œ£.
2. Global existence and uniqueness. The problem admits a unique
global solution if one can take J = I and Œ£ = R in (11.3), that is, if
f is uniformly Lipschitz continuous with respect to y.
In view of the stability analysis of the Cauchy problem, we consider the
following problem
" z‚Ä≤(t) = f(t, z(t)) + Œ¥(t),
t ‚ààI,
z(t0) = y0 + Œ¥0,
(11.4)
where Œ¥0 ‚ààR and Œ¥ is a continuous function on I. Problem (11.4) is derived
from (11.1) by perturbing both the initial datum y0 and the source func-
tion f. Let us now characterize the sensitivity of the solution z to those
perturbations.

11.1 The Cauchy Problem
471
DeÔ¨Ånition 11.1 ([Hah67], [Ste71] or [PS91]). Let I be a bounded set. The
Cauchy problem (11.1) is stable in the sense of Liapunov (or stable) on I
if, for any perturbation (Œ¥0, Œ¥(t)) satisfying
|Œ¥0| < Œµ,
|Œ¥(t)| < Œµ
‚àÄt ‚ààI,
with Œµ > 0 suÔ¨Éciently small to guarantee that the solution to the perturbed
problem (11.4) does exist, then
‚àÉC > 0 independent of Œµ such that
|y(t) ‚àíz(t)| < CŒµ,
‚àÄt ‚ààI.
(11.5)
If I has no upperly bound we say that (11.1) is asymptotically stable if, as
well as being Liapunov stable in any bounded interval I, the following limit
also holds
|y(t) ‚àíz(t)| ‚Üí0,
for t ‚Üí+‚àû.
(11.6)
‚ñ†
The requirement that the Cauchy problem is stable is equivalent to requir-
ing that it is well-posed in the sense stated in Chapter 2.
The uniform Lipschitz-continuity of f with respect to y suÔ¨Éces to ensure
the stability of the Cauchy problem. Indeed, letting w(t) = z(t) ‚àíy(t), we
have
w‚Ä≤(t) = f(t, z(t)) ‚àíf(t, y(t)) + Œ¥(t).
Therefore,
w(t) = Œ¥0 +
t
>
t0
[f(s, z(s)) ‚àíf(s, y(s))] ds +
t
>
t0
Œ¥(s)ds,
‚àÄt ‚ààI.
Thanks to previous assumptions, it follows that
|w(t)| ‚â§(1 + |t ‚àít0|) Œµ + L
t
>
t0
|w(s)|ds.
Applying the Gronwall lemma (which we include below for the reader‚Äôs
ease) yields
|w(t)| ‚â§(1 + |t ‚àít0|) ŒµeL|t‚àít0|,
‚àÄt ‚ààI
and, thus, (11.5) with C = (1 + KI)eLKI where KI = maxt‚ààI |t ‚àít0|.
Lemma 11.1 (Gronwall) Let p be an integrable function nonnegative on
the interval (t0, t0 + T), and let g and œï be two continuous functions on
[t0, t0 + T], g being nondecreasing. If œï satisÔ¨Åes the inequality
œï(t) ‚â§g(t) +
t
>
t0
p(œÑ)œï(œÑ)dœÑ,
‚àÄt ‚àà[t0, t0 + T],

472
11. Numerical Solution of Ordinary DiÔ¨Äerential Equations
then
œï(t) ‚â§g(t) exp
Ô£´
Ô£≠
t
>
t0
p(œÑ)dœÑ
Ô£∂
Ô£∏,
‚àÄt ‚àà[t0, t0 + T].
For the proof, see, for instance, [QV94], Lemma 1.4.1.
The constant C that appears in (11.5) could be very large and, in general,
depends on the upper extreme of the interval I, as in the proof above.
For that reason, the property of asymptotic stability is more suitable for
describing the behavior of the dynamical system (11.1) as t ‚Üí+‚àû(see
[Arn73]).
As is well-known, only a restricted number of nonlinear ODEs can be
solved in closed form (see, for instance, [Arn73]). Moreover, even when
this is possible, it is not always a straightforward task to Ô¨Ånd an explicit
expression of the solution; for example, consider the (very simple) equation
y‚Ä≤ = (y‚àít)/(y+t), whose solution is only implicitly deÔ¨Åned by the relation
(1/2) log(t2 + y2) + tan‚àí1(y/t) = C, where C is a constant depending on
the initial condition.
For this reason we are interested in numerical methods, since these can
be applied to any ODE under the sole condition that it admits a unique
solution.
11.2
One-Step Numerical Methods
Let us address the numerical approximation of the Cauchy problem (11.1).
Fix 0 < T < +‚àûand let I = (t0, t0 + T) be the integration interval and,
correspondingly, for h > 0, let tn = t0 + nh, with n = 0, 1, 2, . . . , Nh, be
the sequence of discretization nodes of I into subintervals In = [tn, tn+1].
The width h of such subintervals is called the discretization stepsize. Notice
that Nh is the maximum integer such that tNh ‚â§t0 + T. Let uj be the
approximation at node tj of the exact solution y(tj); this solution will be
henceforth shortly denoted by yj. Similarly, fj denotes the value f(tj, uj).
We obviously set u0 = y0.
DeÔ¨Ånition 11.2 A numerical method for the approximation of problem
(11.1) is called a one-step method if ‚àÄn ‚â•0, un+1 depends only on un.
Otherwise, the scheme is called a multistep method.
‚ñ†
For now, we focus our attention on one-step methods. Here are some of
them:

11.3 Analysis of One-Step Methods
473
1. forward Euler method
un+1 = un + hfn;
(11.7)
2. backward Euler method
un+1 = un + hfn+1.
(11.8)
In both cases, y‚Ä≤ is approximated through a Ô¨Ånite diÔ¨Äerence: forward and
backward diÔ¨Äerences are used in (11.7) and (11.8), respectively. Both Ô¨Ånite
diÔ¨Äerences are Ô¨Årst-order approximations of the Ô¨Årst derivative of y with
respect to h (see Section 10.10.1).
3. trapezoidal (or Crank-Nicolson) method
un+1 = un + h
2 [fn + fn+1] .
(11.9)
This method stems from approximating the integral on the right side of
(11.2) by the trapezoidal quadrature rule (9.11).
4. Heun method
un+1 = un + h
2 [fn + f(tn+1, un + hfn)].
(11.10)
This method can be derived from the trapezoidal method substituting
f(tn+1, un + hf(tn, un)) for f(tn+1, un+1) in (11.9) (i.e., using the forward
Euler method to compute un+1).
In this last case, we notice that the aim is to transform an implicit method
into an explicit one. Addressing this concern, we recall the following.
DeÔ¨Ånition 11.3 (explicit and implicit methods) A method is called
explicit if un+1 can be computed directly in terms of (some of) the previous
values uk, k ‚â§n. A method is said to be implicit if un+1 depends implicitly
on itself through f.
‚ñ†
Methods (11.7) and (11.10) are explicit, while (11.8) and (11.9) are implicit.
These latter require at each time step to solving a nonlinear problem if f
depends nonlinearly on the second argument.
A remarkable example of one-step methods are the Runge-Kutta meth-
ods, which will be analyzed in Section 11.8.
11.3
Analysis of One-Step Methods
Any one-step explicit method for the approximation of (11.1) can be cast
in the concise form
un+1 = un + hŒ¶(tn, un, fn; h),
0 ‚â§n ‚â§Nh ‚àí1,
u0 = y0,
(11.11)

474
11. Numerical Solution of Ordinary DiÔ¨Äerential Equations
where Œ¶(¬∑, ¬∑, ¬∑; ¬∑) is called an increment function. Letting as usual yn = y(tn),
analogously to (11.11) we can write
yn+1 = yn + hŒ¶(tn, yn, f(tn, yn); h) + Œµn+1,
0 ‚â§n ‚â§Nh ‚àí1, (11.12)
where Œµn+1 is the residual arising at the point tn+1 when we pretend that
the exact solution ‚ÄúsatisÔ¨Åes‚Äù the numerical scheme. Let us write the residual
as
Œµn+1 = hœÑn+1(h).
The quantity œÑn+1(h) is called the local truncation error (LTE) at the node
tn+1. We thus deÔ¨Åne the global truncation error to be the quantity
œÑ(h) =
max
0‚â§n‚â§Nh‚àí1|œÑn+1(h)|
Notice that œÑ(h) depends on the solution y of the Cauchy problem (11.1).
The forward Euler‚Äôs method is a special instance of (11.11), where
Œ¶(tn, un, fn; h) = fn,
while to recover Heun‚Äôs method we must set
Œ¶(tn, un, fn; h) = 1
2 [fn + f(tn + h, un + hfn)] .
A one-step explicit scheme is fully characterized by its increment function
Œ¶. This function, in all the cases considered thus far, is such that
lim
h‚Üí0Œ¶(tn, yn, f(tn, yn); h) = f(tn, yn),
‚àÄtn ‚â•t0
(11.13)
Property (11.13), together with the obvious relation yn+1 ‚àíyn = hy‚Ä≤(tn) +
O(h2), ‚àÄn ‚â•0, allows one to obtain from (11.12) that lim
h‚Üí0œÑn(h) = 0,
0 ‚â§n ‚â§Nh ‚àí1. In turn, this condition ensures that
lim
h‚Üí0œÑ(h) = 0
which expresses the consistency of the numerical method (11.11) with the
Cauchy problem (11.1). In general, a method is said to be consistent if its
LTE is inÔ¨Ånitesimal with respect to h. Moreover, a scheme has order p if,
‚àÄt ‚ààI, the solution y(t) of the Cauchy problem (11.1) fulÔ¨Ålls the condition
œÑ(h) = O(hp)
for h ‚Üí0.
(11.14)
Using Taylor expansions, as was done in Section 11.2, it can be proved that
the forward Euler method has order 1, while the Heun method has order 2
(see Exercises 1 and 2).

11.3 Analysis of One-Step Methods
475
11.3.1
The Zero-Stability
Let us formulate a requirement analogous to the one for Liapunov stability
(11.5), speciÔ¨Åcally for the numerical scheme. If (11.5) is satisÔ¨Åed with a
constant C independent of h, we shall say that the numerical problem is
zero-stable. Precisely:
DeÔ¨Ånition 11.4 (zero-stability of one-step methods) The numerical
method (11.11) for the approximation of problem (11.1) is zero-stable if
‚àÉh0 > 0, ‚àÉC > 0 : ‚àÄh ‚àà(0, h0], |z(h)
n
‚àíu(h)
n | ‚â§CŒµ, 0 ‚â§n ‚â§Nh, (11.15)
where z(h)
n , u(h)
n
are respectively the solutions of the problems
Ô£±
Ô£≤
Ô£≥
z(h)
n+1 = z(h)
n
+ h
6
Œ¶(tn, z(h)
n , f(tn, z(h)
n ); h) + Œ¥n+1
7
,
z0 = y0 + Œ¥0,
(11.16)
Ô£±
Ô£≤
Ô£≥
u(h)
n+1 = u(h)
n
+ hŒ¶(tn, u(h)
n , f(tn, u(h)
n ); h),
u0 = y0,
(11.17)
for 0 ‚â§n ‚â§Nh ‚àí1, under the assumption that |Œ¥k| ‚â§Œµ, 0 ‚â§k ‚â§Nh.
‚ñ†
Zero-stability thus requires that, in a bounded interval, (11.15) holds for
any value h ‚â§h0. This property deals, in particular, with the behavior
of the numerical method in the limit case h ‚Üí0 and this justiÔ¨Åes the
name of zero-stability. This latter is therefore a distinguishing property of
the numerical method itself, not of the Cauchy problem (which, indeed,
is stable due to the uniform Lipschitz continuity of f). Property (11.15)
ensures that the numerical method has a weak sensitivity with respect to
small changes in the data and is thus stable in the sense of the general
deÔ¨Ånition given in Chapter 2.
Remark 11.1 The constant C in (11.15) is independent of h (and thus
of Nh), but it can depend on the width T of the integration interval I.
Actually, (11.15) does not exclude a priori the constant C from being an
unbounded function of T.
‚ñ†
The request that a numerical method be stable arises, before anything else,
from the need of keeping under control the (unavoidable) errors introduced
by the Ô¨Ånite arithmetic of the computer. Indeed, if the numerical method
were not zero-stable, the rounding errors made on y0 as well as in the pro-
cess of computing f(tn, un) would make the computed solution completely
useless.

476
11. Numerical Solution of Ordinary DiÔ¨Äerential Equations
Theorem 11.1 (Zero-stability) Consider the explicit one-step method
(11.11) for the numerical solution of the Cauchy problem (11.1). Assume
that the increment function Œ¶ is Lipschitz continuous with respect to the
second argument, with constant Œõ independent of h and of the nodes tj ‚àà
[t0, t0 + T], that is
‚àÉh0 > 0, ‚àÉŒõ > 0 : ‚àÄh ‚àà(0, h0]
|Œ¶(tn, u(h)
n , f(tn, u(h)
n ); h) ‚àíŒ¶(tn, z(h)
n , f(tn, z(h)
n ); h)|
‚â§Œõ|u(h)
n
‚àíz(h)
n |, 0 ‚â§n ‚â§Nh.
(11.18)
Then, method (11.11) is zero-stable.
Proof. Setting w(h)
j
= z(h)
j
‚àíu(h)
j
, by subtracting (11.17) from (11.16) we obtain,
for j = 0, . . . , Nh ‚àí1,
w(h)
j+1 = w(h)
j
+ h
6
Œ¶(tj, z(h)
j
, f(tj, z(h)
j
); h) ‚àíŒ¶(tj, u(h)
j
, f(tj, u(h)
j
); h)
7
+ hŒ¥j+1.
Summing over j gives, for n = 1, . . . , Nh,
w(h)
n
= w(h)
0
+h
n‚àí1

j=0
Œ¥j+1 + h
n‚àí1

j=0
+
Œ¶(tj, z(h)
j
, f(tj, z(h)
j
); h) ‚àíŒ¶(tj, u(h)
j
, f(tj, u(h)
j
); h)
,
,
so that, by (11.18)
|w(h)
n | ‚â§|w0| + h
n‚àí1

j=0
|Œ¥j+1| + hŒõ
n‚àí1

j=0
|w(h)
j
|,
1 ‚â§n ‚â§Nh.
(11.19)
Applying the discrete Gronwall lemma, given below, we obtain
|w(h)
n | ‚â§(1 + hn) ŒµenhŒõ,
1 ‚â§n ‚â§Nh.
Then (11.15) follows from noticing that hn ‚â§T and setting C = (1 + T) eŒõT . 3
Notice that zero-stability implies the boundedness of the solution when f
is linear with respect to the second argument.
Lemma 11.2 (discrete Gronwall) Let kn be a nonnegative sequence and
œïn a sequence such that
Ô£±
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£≥
œï0 ‚â§g0
œïn ‚â§g0 +
n‚àí1

s=0
ps +
n‚àí1

s=0
ksœÜs,
n ‚â•1.

11.3 Analysis of One-Step Methods
477
If g0 ‚â•0 and pn ‚â•0 for any n ‚â•0, then
œïn ‚â§

g0 +
n‚àí1

s=0
ps

exp
n‚àí1

s=0
ks

,
n ‚â•1.
For the proof, see, for instance, [QV94], Lemma 1.4.2. In the speciÔ¨Åc case
of the Euler method, checking the property of zero-stability can be done
directly using the Lipschitz continuity of f (we refer the reader to the end
of Section 11.3.2). In the case of multistep methods, the analysis will lead to
the veriÔ¨Åcation of a purely algebraic property, the so-called root condition
(see Section 11.6.3).
11.3.2
Convergence Analysis
DeÔ¨Ånition 11.5 A method is said to be convergent if
‚àÄn = 0, . . . , Nh,
|un ‚àíyn| ‚â§C(h)
where C(h) is an inÔ¨Ånitesimal with respect to h. In that case, it is said to
be convergent with order p if ‚àÉC > 0 such that C(h) = Chp.
‚ñ†
We can prove the following theorem.
Theorem 11.2 (Convergence) Under the same assumptions as in The-
orem 11.1, we have
|yn ‚àíun| ‚â§(|y0 ‚àíu0| + nhœÑ(h)) enhŒõ,
1 ‚â§n ‚â§Nh.
(11.20)
Therefore, if the consistency assumption (11.13) holds and |y0 ‚àíu0| ‚Üí0
as h ‚Üí0, then the method is convergent. Moreover, if |y0 ‚àíu0| = O(hp)
and the method has order p, then it is also convergent with order p.
Proof. Setting wj = yj ‚àíuj, subtracting (11.11) from (11.12) and proceed-
ing as in the proof of the previous theorem yields inequality (11.19), with the
understanding that
w0 = y0 ‚àíu0, and Œ¥j+1 = œÑj+1(h).
The estimate (11.20) is then obtained by applying again the discrete Gronwall
lemma. From the fact that nh ‚â§T and œÑ(h) = O(hp), we can conclude that
|yn ‚àíun| ‚â§Chp with C depending on T and Œõ but not on h.
3
A consistent and zero-stable method is thus convergent. This property is
known as the Lax-Richtmyer theorem or equivalence theorem (the converse:
‚Äúa convergent method is zero-stable‚Äù being obviously true). This theorem,
which is proven in [IK66], was already advocated in Section 2.2.1 and is a
central result in the analysis of numerical methods for ODEs (see [Dah56] or

478
11. Numerical Solution of Ordinary DiÔ¨Äerential Equations
[Hen62] for linear multistep methods, [But66], [MNS74] for a wider classes
of methods). It will be considered again in Section 11.5 for the analysis of
multistep methods.
We carry out in detail the convergence analysis in the case of the forward
Euler method, without resorting to the discrete Gronwall lemma. In the
Ô¨Årst part of the proof we assume that any operation is performed in exact
arithmetic and that u0 = y0.
Denote by en+1 = yn+1 ‚àíun+1 the error at node tn+1 with n = 0, 1, . . .
and notice that
en+1 = (yn+1 ‚àíu‚àó
n+1) + (u‚àó
n+1 ‚àíun+1),
(11.21)
where u‚àó
n+1 = yn + hf(tn, yn) is the solution obtained after one step of
the forward Euler method starting from the initial datum yn (see Figure
11.1). The Ô¨Årst addendum in (11.21) accounts for the consistency error, the
second one for the cumulation of these errors. Then
yn+1 ‚àíu‚àó
n+1 = hœÑn+1(h),
u‚àó
n+1 ‚àíun+1 = en + h [f(tn, yn) ‚àíf(tn, un)] .
y(x)
yn
un
tn
tn+1
un+1
u‚àó
n+1
yn+1
hœÑn+1
en+1
FIGURE 11.1. Geometrical interpretation of the local and global truncation er-
rors at node tn+1 for the forward Euler method
As a consequence,
|en+1| ‚â§h|œÑn+1(h)| + |en| + h|f(tn, yn) ‚àíf(tn, un)| ‚â§hœÑ(h) + (1 + hL)|en|,
L being the Lipschitz constant of f. By recursion on n, we Ô¨Ånd
|en+1|
‚â§[1 + (1 + hL) + . . . + (1 + hL)n] hœÑ(h)
= (1 + hL)n+1 ‚àí1
L
œÑ(h) ‚â§eL(tn+1‚àít0) ‚àí1
L
œÑ(h).

11.3 Analysis of One-Step Methods
479
The last inequality follows from noticing that 1+hL ‚â§ehL and (n+1)h =
tn+1 ‚àít0.
On the other hand, if y ‚ààC2(I), the LTE for the forward Euler method
is (see Section 10.10.1)
œÑn+1(h) = h
2 y‚Ä≤‚Ä≤(Œæ),
Œæ ‚àà(tn, tn+1),
and thus, œÑ(h) ‚â§(M/2)h, where M = maxŒæ‚ààI |y‚Ä≤‚Ä≤(Œæ)|. In conclusion,
|en+1| ‚â§eL(tn+1‚àít0) ‚àí1
L
M
2 h,
‚àÄn ‚â•0,
(11.22)
from which it follows that the global error tends to zero with the same
order as the local truncation error.
If also the rounding errors are accounted for, we can assume that the
solution ¬Øun+1, actually computed by the forward Euler method at time
tn+1, is such that
¬Øu0 = y0 + Œ∂0,
¬Øun+1 = ¬Øun + hf(tn, ¬Øun) + Œ∂n+1,
(11.23)
having denoted the rounding error by Œ∂j, for j ‚â•0.
Problem (11.23) is an instance of (11.16), provided that we identify Œ∂n+1
and ¬Øun with hŒ¥n+1 and z(h)
n
in (11.16), respectively. Combining Theorems
11.1 and 11.2 we get, instead of (11.22), the following error estimate
|yn+1 ‚àí¬Øun+1| ‚â§eL(tn+1‚àít0)

|Œ∂0| + 1
L
M
2 h + Œ∂
h

,
where Œ∂ = max1‚â§j‚â§n+1 |Œ∂j|. The presence of rounding errors does not allow,
therefore, to conclude that as h ‚Üí0, the error goes to zero. Actually,
there exists an optimal (non null) value of h, hopt, for which the error is
minimized. For h < hopt, the rounding error dominates the truncation error
and the global error increases.
11.3.3
The Absolute Stability
The property of absolute stability is in some way specular to zero-stability,
as far as the roles played by h and I are concerned. Heuristically, we say that
a numerical method is absolutely stable if, for h Ô¨Åxed, un remains bounded
as tn ‚Üí+‚àû. This property, thus, deals with the asymptotic behavior of
un, as opposed to a zero-stable method for which, for a Ô¨Åxed integration
interval, un remains bounded as h ‚Üí0.
For a precise deÔ¨Ånition, consider the linear Cauchy problem (that from now
on, we shall refer to as the test problem)
" y‚Ä≤(t) = Œªy(t),
t > 0,
y(0) = 1,
(11.24)

480
11. Numerical Solution of Ordinary DiÔ¨Äerential Equations
with Œª ‚ààC, whose solution is y(t) = eŒªt. Notice that
lim
t‚Üí+‚àû|y(t)| = 0 if
Re(Œª) < 0.
DeÔ¨Ånition 11.6 A numerical method for approximating (11.24) is abso-
lutely stable if
|un| ‚àí‚Üí0
as
tn ‚àí‚Üí+‚àû.
(11.25)
Let h be the discretization stepsize. The numerical solution un of (11.24)
obviously depends on h and Œª. The region of absolute stability of the nu-
merical method is the subset of the complex plane
A = {z = hŒª ‚ààC : (11.25) is satisÔ¨Åed } .
(11.26)
Thus, A is the set of the values of the product hŒª for which the numerical
method furnishes solutions that decay to zero as tn tends to inÔ¨Ånity.
‚ñ†
Let us check whether the one-step methods introduced previously are ab-
solutely stable.
1. Forward Euler method: applying (11.7) to problem (11.24) yields un+1 =
un + hŒªun for n ‚â•0, with u0 = 1. Proceeding recursively on n we get
un = (1 + hŒª)n,
n ‚â•0.
Therefore, condition (11.25) is satisÔ¨Åed iÔ¨Ä|1 + hŒª| < 1, that is, if hŒª lies
within the unit circle with center at (‚àí1, 0) (see Figure 11.3). This amounts
to requiring that
hŒª ‚ààC‚àí
and
0 < h < ‚àí2Re(Œª)
|Œª|2
(11.27)
where
C‚àí= {z ‚ààC : Re(z) < 0} .
Example 11.1 For the Cauchy problem y‚Ä≤(x) = ‚àí5y(x) for x > 0 and y(0) = 1,
condition (11.27) implies 0 < h < 2/5. Figure 11.2 (left) shows the behavior of
the computed solution for two values of h which do not fulÔ¨Åll this condition, while
on the right we show the solutions for two values of h that do. Notice that in this
second case the oscillations, if present, damp out as t grows.
‚Ä¢
2. Backward Euler method: proceeding as before, we get this time
un =
1
(1 ‚àíhŒª)n ,
n ‚â•0.
The absolute stability property (11.25) is satisÔ¨Åed for any value of hŒª that
does not belong to the unit circle of center (1, 0) (see Figure 11.3, right).

11.3 Analysis of One-Step Methods
481
0
1
2
3
4
5
6
7
8
‚àí3
‚àí2
‚àí1
0
1
2
3
0
1
2
3
4
5
6
7
8
‚àí1
‚àí0.8
‚àí0.6
‚àí0.4
‚àí0.2
0
0.2
0.4
0.6
0.8
1
FIGURE 11.2. Left: computed solutions for h = 0.41 > 2/5 (dashed line) and
h = 2/5 (solid line). Notice how, in the limiting case h = 2/5, the oscillations
remain unmodiÔ¨Åed as t grows. Right: two solutions are reported for h = 0.39
(solid line) and h = 0.15 (dashed line)
Example 11.2 The numerical solution given by the backward Euler method in
the case of Example 11.1 does not exhibit any oscillation for any value of h. On
the other hand, the same method, if applied to the problem y‚Ä≤(t) = 5y(t) for t > 0
and with y(0) = 1, computes a solution that decays anyway to zero as t ‚Üí‚àûif
h > 2/5, despite the fact that the exact solution of the Cauchy problem tends to
inÔ¨Ånity.
‚Ä¢
3. Trapezoidal (or Crank-Nicolson) method: we get
un =

1 + 1
2Œªh

/

1 ‚àí1
2Œªh
n
,
n ‚â•0,
hence (11.25) is fulÔ¨Ålled for any hŒª ‚ààC‚àí.
4. Heun‚Äôs method: applying (11.10) to problem (11.24) and proceeding
by recursion on n, we obtain
un =

1 + hŒª + (hŒª)2
2
n
,
n ‚â•0.
As shown in Figure 11.3 the region of absolute stability of Heun‚Äôs method
is larger than the corresponding one of Euler‚Äôs method. However, its re-
striction to the real axis is the same.
We say that a method is A-stable if A ‚à©C‚àí= C‚àí, i.e., if for Re(Œª) < 0,
condition (11.25) is satisÔ¨Åed for all values of h.
The backward Euler and Crank-Nicolson methods are A-stable, while
the forward Euler and Heun methods are conditionally stable.
Remark 11.2 Notice that the implicit one-step methods examined so far
are unconditionally absolutely stable, while explicit schemes are condition-

482
11. Numerical Solution of Ordinary DiÔ¨Äerential Equations
FE
H
Re
BE
‚àí1
1
Im
1.75
‚àí1.75
FIGURE 11.3. Regions of absolute stability for the forward (FE) and backward
Euler (BE) methods and for Heun‚Äôs method (H). Notice that the region of abso-
lute stability of the BE method lies outside the unit circle of center (1, 0) (shaded
area)
ally absolutely stable. This is, however, not a general rule: in fact, there ex-
ist implicit unstable or only conditionally stable schemes. On the contrary,
there are no explicit unconditionally absolutely stable schemes [Wid67]. ‚ñ†
11.4
DiÔ¨Äerence Equations
For any integer k ‚â•1, an equation of the form
un+k + Œ±k‚àí1un+k‚àí1 + . . . + Œ±0un = œïn+k,
n = 0, 1, . . .
(11.28)
is called a linear diÔ¨Äerence equation of order k. The coeÔ¨Écients Œ±0 Ã∏= 0,
Œ±1, . . . , Œ±k‚àí1 may or may not depend on n. If, for any n, the right side
œïn+k is equal to zero, the equation is said homogeneous, while if the Œ±‚Ä≤
js
are independent of n it is called linear diÔ¨Äerence equation with constant
coeÔ¨Écients.
DiÔ¨Äerence equations arise for instance in the discretization of ordinary dif-
ferential equations. Regarding this, we notice that all the numerical meth-
ods examined so far end up with equations like (11.28). More generally,
equations like (11.28) are encountered when quantities are deÔ¨Åned through
linear recursive relations. Another relevant application is concerned with
the discretization of boundary value problems (see Chapter 12). For fur-
ther details on the subject, we refer to Chapters 2 and 5 of [BO78] and to
Chapter 6 of [Gau97].

11.4 DiÔ¨Äerence Equations
483
Any sequence {un, n = 0, 1, . . . } of values that satisfy (11.28) is called
a solution to the equation (11.28). Given k initial values u0, . . . , uk‚àí1, it
is always possible to construct a solution of (11.28) by computing (sequen-
tially)
un+k = [œïn+k ‚àí(Œ±k‚àí1un+k‚àí1 + . . . + Œ±0un)],
n = 0, 1, . . .
However, our interest is to Ô¨Ånd an expression of the solution un+k which
depends only on the coeÔ¨Écients and on the initial values.
We start by considering the homogeneous case with constant coeÔ¨Écients,
un+k + Œ±k‚àí1un+k‚àí1 + . . . + Œ±0un = 0,
n = 0, 1, . . .
(11.29)
and associate with (11.29) the characteristic polynomial Œ† ‚ààPk deÔ¨Åned as
Œ†(r) = rk + Œ±k‚àí1rk‚àí1 + . . . + Œ±1r + Œ±0.
(11.30)
Denoting its roots by rj, j = 0, . . . , k ‚àí1, any sequence of the form

rn
j , n = 0, 1, . . .

,
for j = 0, . . . , k ‚àí1
(11.31)
is a solution of (11.29), since
rn+k
j
+ Œ±k‚àí1rn+k‚àí1
j
+ . . . + Œ±0rn
j
= rn
j

rk
j + Œ±k‚àí1rk‚àí1
j
+ . . . + Œ±0

= rn
j Œ†(rj) = 0.
We say that the k sequences deÔ¨Åned in (11.31) are the fundamental solutions
of the homogeneous equation (11.29). Any sequence of the form
un = Œ≥0rn
0 + Œ≥1rn
1 + . . . + Œ≥k‚àí1rn
k‚àí1,
n = 0, 1, . . .
(11.32)
is still a solution to (11.29), since it is a linear equation.
The coeÔ¨Écients Œ≥0, . . . , Œ≥k‚àí1 can be determined by imposing the k initial
conditions u0, . . . , uk‚àí1. Moreover, it can be proved that if all the roots
of Œ† are simple, then all the solutions of (11.29) can be cast in the form
(11.32).
This last statement no longer holds if there are roots of Œ† with multi-
plicity greater than 1. If, for a certain j, the root rj has multiplicity m ‚â•2,
in order to obtain a system of fundamental solutions that generate all the
solutions of (11.29), it suÔ¨Éces to replace the corresponding fundamental
solution

rn
j , n = 0, 1, . . .

with the m sequences

rn
j , n = 0, 1, . . .

,

nrn
j , n = 0, 1, . . .

, . . . ,

nm‚àí1rn
j , n = 0, 1, . . .

.
More generally, assuming that r0, . . . , rk‚Ä≤ are distinct roots of Œ†, with mul-
tiplicities equal to m0, . . . , mk‚Ä≤, respectively, we can write the solution of
(11.29) as
un =
k‚Ä≤

j=0
mj‚àí1

s=0
Œ≥sjns

rn
j ,
n = 0, 1, . . . .
(11.33)

484
11. Numerical Solution of Ordinary DiÔ¨Äerential Equations
Notice that even in presence of complex conjugate roots one can still obtain
a real solution (see Exercise 3).
Example 11.3 For the diÔ¨Äerence equation un+2‚àíun = 0, we have Œ†(r) = r2‚àí1,
then r0 = ‚àí1 and r1 = 1, therefore the solution is given by un = Œ≥00(‚àí1)n + Œ≥01.
In particular, enforcing the initial conditions u0 and u1 gives Œ≥00 = (u0 ‚àíu1)/2,
Œ≥01 = (u0 + u1)/2.
‚Ä¢
Example 11.4 For the diÔ¨Äerence equation un+3 ‚àí2un+2 ‚àí7un+1 ‚àí4un = 0,
Œ†(r) = r3 ‚àí2r2 ‚àí7r ‚àí4. Its roots are r0 = ‚àí1 (with multiplicity 2), r1 = 4 and
the solution is un = (Œ≥00 + nŒ≥10)(‚àí1)n + Œ≥014n. Enforcing the initial conditions
we can compute the unknown coeÔ¨Écients as the solution of the following linear
system
Ô£±
Ô£≤
Ô£≥
Œ≥00 + Œ≥01
= u0,
‚àíŒ≥00 ‚àíŒ≥10 + 4Œ≥01
= u1,
Œ≥00 + 2Œ≥10 + 16Œ≥01
= u2
that yields Œ≥00 = (24u0 ‚àí2u1 ‚àíu2)/25, Œ≥10 = (u2 ‚àí3u1 ‚àí4u0)/5 and Œ≥01 =
(2u1 + u0 + u2)/25.
‚Ä¢
The expression (11.33) is of little practical use since it does not outline
the dependence of un on the k initial conditions. A more convenient rep-
resentation is obtained by introducing a new set
2
œà(n)
j
, n = 0, 1, . . .
3
of
fundamental solutions that satisfy
œà(i)
j
= Œ¥ij,
i, j = 0, 1, . . . , k ‚àí1.
(11.34)
Then, the solution of (11.29) subject to the initial conditions u0, . . . , uk‚àí1
is given by
un =
k‚àí1

j=0
ujœà(n)
j
,
n = 0, 1, . . . .
(11.35)
The new fundamental solutions
2
œà(n)
j
, n = 0, 1, . . .
3
can be represented in
terms of those in (11.31) as follows
œà(n)
j
=
k‚àí1

m=0
Œ≤j,mrn
m
for j = 0, . . . , k ‚àí1, n = 0, 1, . . .
(11.36)
By requiring (11.34), we obtain the k linear systems
k‚àí1

m=0
Œ≤j,mri
m = Œ¥ij,
i, j = 0, . . . , k ‚àí1,
whose matrix form is
Rbj = ej,
j = 0, . . . , k ‚àí1.
(11.37)

11.4 DiÔ¨Äerence Equations
485
Here ej denotes the unit vector of Rk, R = (rim) = (ri
m) and bj =
(Œ≤j,0, . . . , Œ≤j,k‚àí1)T . If all r‚Ä≤
js are simple roots of Œ†, the matrix R is nonsin-
gular (see Exercise 5).
The general case where Œ† has k‚Ä≤ +1 distinct roots r0, . . . , rk‚Ä≤ with multi-
plicities m0, . . . , mk‚Ä≤ respectively, can be dealt with by replacing in (11.36)

rn
j ,
n = 0, 1, . . .

with

rn
j ns,
n = 0, 1, . . .

, where j = 0, . . . , k‚Ä≤ and
s = 0, . . . , mj ‚àí1.
Example 11.5 We consider again the diÔ¨Äerence equation of Example 11.4. Here
we have {rn
0 , nrn
0 , rn
1 , n = 0, 1, . . . } so that the matrix R becomes
R =
Ô£Æ
Ô£∞
r0
0
0
r0
2
r1
0
r1
0
r1
2
r2
0
2r2
0
r2
2
Ô£π
Ô£ª=
Ô£Æ
Ô£∞
1
0
1
‚àí1
‚àí1
4
1
2
16
Ô£π
Ô£ª.
Solving the three systems (11.37) yields
œà(n)
0
= 24
25(‚àí1)n ‚àí4
5n(‚àí1)n + 1
254n,
œà(n)
1
= ‚àí2
25(‚àí1)n ‚àí3
5n(‚àí1)n + 2
254n,
œà(n)
2
= ‚àí1
25(‚àí1)n + 1
5n(‚àí1)n + 1
254n,
from which it can be checked that the solution un = 2
j=0 ujœà(n)
j
coincides with
the one already found in Example 11.4.
‚Ä¢
Now we return to the case of nonconstant coeÔ¨Écients and consider the
following homogeneous equation
un+k +
k

j=1
Œ±k‚àíj(n)un+k‚àíj = 0,
n = 0, 1, . . . .
(11.38)
The goal is to transform it into an ODE by means of a function F, called
the generating function of the equation (11.38). F depends on the real
variable t and is derived as follows. We require that the n-th coeÔ¨Écient
of the Taylor series of F around t = 0 can be written as Œ≥nun, for some
unknown constant Œ≥n, so that
F(t) =
‚àû

n=0
Œ≥nuntn.
(11.39)
The coeÔ¨Écients {Œ≥n} are unknown and must be determined in such a way
that
k

j=0
cjF (k‚àíj)(t) =
‚àû

n=0
Ô£Æ
Ô£∞un+k +
k

j=1
Œ±k‚àíj(n)un+k‚àíj
Ô£π
Ô£ªtn,
(11.40)

486
11. Numerical Solution of Ordinary DiÔ¨Äerential Equations
where cj are suitable unknown constants not depending on n. Note that
owing to (11.39) we obtain the ODE
k

j=0
cjF (k‚àíj)(t) = 0
to which we must add the initial conditions F (j)(0) = Œ≥juj for j = 0, . . . , k‚àí
1. Once F is available, it is simple to recover un through the deÔ¨Ånition of
F itself.
Example 11.6 Consider the diÔ¨Äerence equation
(n + 2)(n + 1)un+2 ‚àí2(n + 1)un+1 ‚àí3un = 0,
n = 0, 1, . . .
(11.41)
with the initial conditions u0 = u1 = 2. We look for a generating function of the
form (11.39). By term-to-term derivation of the series, we get
F ‚Ä≤(t) =
‚àû

n=0
Œ≥nnuntn‚àí1,
F ‚Ä≤‚Ä≤(t) =
‚àû

n=0
Œ≥nn(n ‚àí1)untn‚àí2,
and, after some algebra, we Ô¨Ånd
F ‚Ä≤(t) =
‚àû

n=0
Œ≥nnuntn‚àí1 =
‚àû

n=0
Œ≥n+1(n + 1)un+1tn,
F ‚Ä≤‚Ä≤(t) =
‚àû

n=0
Œ≥nn(n ‚àí1)untn‚àí2 =
‚àû

n=0
Œ≥n+2(n + 2)(n + 1)un+2tn.
As a consequence, (11.40) becomes
‚àû

n=0
(n + 1)(n + 2)un+2tn ‚àí2
‚àû

n=0
(n + 1)un+1tn ‚àí3
‚àû

n=0
untn
= c0
‚àû

n=0
Œ≥n+2(n + 2)(n + 1)un+2tn + c1
‚àû

n=0
Œ≥n+1(n + 1)un+1tn + c2
‚àû

n=0
Œ≥nuntn,
so that, equating both sides, we Ô¨Ånd
Œ≥n = 1 ‚àÄn ‚â•0,
c0 = 1, c1 = ‚àí2, c2 = ‚àí3.
We have thus associated with the diÔ¨Äerence equation the following ODE with
constant coeÔ¨Écients
F ‚Ä≤‚Ä≤(t) ‚àí2F ‚Ä≤(t) ‚àí3F(t) = 0,
with the initial condition F(0) = F ‚Ä≤(0) = 2. The n-th coeÔ¨Écient of the solution
F(t) = e3t + e‚àít is
1
n!F (n)(0) = 1
n! [(‚àí1)n + 3n] ,
so that un = (1/n!) [(‚àí1)n + 3n] is the solution of (11.41).
‚Ä¢

11.5 Multistep Methods
487
The nonhomogeneous case (11.28) can be tackled by searching for solutions
of the form
un = u(0)
n
+ u(œï)
n ,
where u(0)
n
is the solution of the associated homogeneous equation and u(œï)
n
is a particular solution of the nonhomogeneous equation. Once the solution
of the homogeneous equation is available, a general technique to obtain
the solution of the nonhomogeneous equation is based on the method of
variation of parameters, combined with a reduction of the order of the
diÔ¨Äerence equation (see [BO78]).
In the special case of diÔ¨Äerence equations with constant coeÔ¨Écients, with
œïn of the form cnQ(n), where c is a constant and Q is a polynomial of degree
p with respect to the variable n, a possible approach is that of undetermined
coeÔ¨Écients, where one looks for a particular solution that depends on some
undetermined constants and has a known form for some classes of right
sides œïn. It suÔ¨Éces to look for a particular solution of the form
u(œï)
n
= cn(bpnp + bp‚àí1np‚àí1 + . . . + b0),
where bp, . . . , b0 are constants to be determined in such a way that u(œï)
n
is
actually a solution of (11.28).
Example 11.7 Consider the diÔ¨Äerence equation un+3‚àíun+2+un+1‚àíun = 2nn2.
The particular solution is of the form un = 2n(b2n2 + b1n + b0). Substituting this
solution into the equation, we Ô¨Ånd 5b2n2+(36b2+5b1)n+(58b2+18b1+5b0) = n2,
from which, recalling the principle of identity for polynomials, one gets b2 = 1/5,
b1 = ‚àí36/25 and b0 = 358/125.
‚Ä¢
Analogous to the homogeneous case, it is possible to express the solution
of (11.28) as
un =
k‚àí1

j=0
ujœà(n)
j
+
n

l=k
œïlœà(n‚àíl+k‚àí1)
k‚àí1
,
n = 0, 1, . . .
(11.42)
where we deÔ¨Åne œà(i)
k‚àí1 ‚â°0 for all i < 0 and œïj ‚â°0 for all j < k.
11.5
Multistep Methods
Let us now introduce some examples of multistep methods (shortly, MS).
DeÔ¨Ånition 11.7 (q-steps methods) A q-step method (q ‚â•1) is one
which, ‚àÄn ‚â•q ‚àí1, un+1 depends on un+1‚àíq, but not on the values uk
with k < n + 1 ‚àíq.
‚ñ†

488
11. Numerical Solution of Ordinary DiÔ¨Äerential Equations
A well-known two-step explicit method can be obtained by using the
centered Ô¨Ånite diÔ¨Äerence (10.61) to approximate the Ô¨Årst order derivative
in (11.1). This yields the midpoint method
un+1 = un‚àí1 + 2hfn,
n ‚â•1
(11.43)
where u0 = y0, u1 is to be determined and fk denotes the value f(tk, uk).
An example of an implicit two-step scheme is the Simpson method, ob-
tained from (11.2) with t0 = tn‚àí1 and t = tn+1 and by using the Cavalieri-
Simpson quadrature rule to approximate the integral of f
un+1 = un‚àí1 + h
3 [fn‚àí1 + 4fn + fn+1],
n ‚â•1
(11.44)
where u0 = y0, and u1 is to be determined.
From these examples, it is clear that a multistep method requires q initial
values u0, . . . , uq‚àí1 for ‚Äútaking oÔ¨Ä‚Äù. Since the Cauchy problem provides
only one datum (u0), one way to assign the remaining values consists of
resorting to explicit one-step methods of high order. An example is given by
Heun‚Äôs method (11.10), other examples are provided by the Runge-Kutta
methods, which will be introduced in Section 11.8.
In this section we deal with linear multistep methods
un+1 =
p

j=0
ajun‚àíj + h
p

j=0
bjfn‚àíj + hb‚àí1fn+1, n = p, p + 1, . . . (11.45)
which are p + 1-step methods, p ‚â•0. For p = 0, we recover one-step
methods.
The coeÔ¨Écients aj, bj are real and fully identify the scheme; they are
such that ap Ã∏= 0 or bp Ã∏= 0. If b‚àí1 Ã∏= 0 the scheme is implicit, otherwise it
is explicit.
We can reformulate (11.45) as follows
p+1

s=0
Œ±sun+s = h
p+1

s=0
Œ≤sf(tn+s, un+s), n = 0, 1, . . . , Nh ‚àí(p + 1) (11.46)
having set Œ±p+1 = 1, Œ±s = ‚àíap‚àís for s = 0, . . . , p and Œ≤s = bp‚àís for
s = 0, . . . , p+1. Relation (11.46) is a special instance of the linear diÔ¨Äerence
equation (11.28), where we set k = p + 1 and œïn+j = hŒ≤jf(tn+j, un+j), for
j = 0, . . . , p + 1.
Also for MS methods we can characterize consistency in terms of the local
truncation error, according to the following deÔ¨Ånition.
DeÔ¨Ånition 11.8 The local truncation error (LTE) œÑn+1(h) introduced by
the multistep method (11.45) at tn+1 (for n ‚â•p) is deÔ¨Åned through the

11.5 Multistep Methods
489
following relation
hœÑn+1(h) = yn+1 ‚àí
Ô£Æ
Ô£∞
p

j=0
ajyn‚àíj + h
p

j=‚àí1
bjy‚Ä≤
n‚àíj
Ô£π
Ô£ª,
n ‚â•p,
(11.47)
where yn‚àíj = y(tn‚àíj) and y‚Ä≤
n‚àíj = y‚Ä≤(tn‚àíj) for j = ‚àí1, . . . , p.
‚ñ†
Analogous to one-step methods, the quantity hœÑn+1(h) is the residual gen-
erated at tn+1 if we pretend that the exact solution ‚ÄúsatisÔ¨Åes‚Äù the numerical
scheme. Letting œÑ(h) = max
n |œÑn(h)|, we have the following deÔ¨Ånition.
DeÔ¨Ånition 11.9 (Consistency) The multistep method (11.45) is consis-
tent if œÑ(h) ‚Üí0 as h ‚Üí0. Moreover, if œÑ(h) = O(hq), for some q ‚â•1, then
the method is said to have order q.
‚ñ†
A more precise characterization of the LTE can be given by introducing the
following linear operator L associated with the linear MS method (11.45)
L[w(t); h] = w(t + h) ‚àí
p

j=0
ajw(t ‚àíjh) ‚àíh
p

j=‚àí1
bjw‚Ä≤(t ‚àíjh),
(11.48)
where w ‚ààC1(I) is an arbitrary function. Notice that the LTE is exactly
L[y(tn); h]. If we assume that w is suÔ¨Éciently smooth and expand w(t‚àíjh)
and w‚Ä≤(t ‚àíjh) about t ‚àíph, we obtain
L[w(t); h] = C0w(t ‚àíph) + C1hw(1)(t ‚àíph) + . . . + Ckhkw(k)(t ‚àíph) + . . .
Consequently, if the MS method has order q and y ‚ààCq+1(I), we obtain
œÑn+1(h) = Cq+1hq+1y(q+1)(tn‚àíp) + O(hq+2).
The term Cq+1hq+1y(q+1)(tn‚àíp) is the so-called principal local truncation
error (PLTE) while Cq+1 is the error constant. The PLTE is widely em-
ployed in devising adaptive strategies for MS methods (see [Lam91], Chap-
ter 3).
Program 92 provides an implementation of the multistep method in the
form (11.45) for the solution of a Cauchy problem on the interval (t0, T).
The input parameters are: the column vector a containing the p + 1 co-
eÔ¨Écients ai; the column vector b containing the p + 2 coeÔ¨Écients bi; the
discretization stepsize h; the vector of initial data u0 at the corresponding
time instants t0; the macros fun and dfun containing the functions f and
‚àÇf/‚àÇy. If the MS method is implicit, a tolerance tol and a maximum num-
ber of admissible iterations itmax must be provided. These two parameters
monitor the convergence of Newton‚Äôs method that is employed to solve the

490
11. Numerical Solution of Ordinary DiÔ¨Äerential Equations
nonlinear equation (11.45) associated with the MS method. In output the
code returns the vectors u and t containing the computed solution at the
time instants t.
Program 92 - multistep : Linear multistep methods
function [t,u] = multistep (a,b,tf,t0,u0,h,fun,dfun,tol,itmax)
y = u0; t = t0; f = eval (fun); p = length(a) - 1; u = u0;
nt = Ô¨Åx((tf - t0 (1) )/h);
for k = 1:nt
lu=length(u);
G = a‚Äô *u (lu:-1:lu-p)+ h * b(2:p+2)‚Äô * f(lu:-1:lu-p);
lt = length(t0); t0 = [t0; t0(lt)+h]; unew = u (lu);
t = t0 (lt+1); err = tol + 1; it = 0;
while (err > tol) & (it <= itmax)
y = unew; den = 1 - h * b (1) * eval(dfun);
fnew = eval (fun);
if den == 0
it = itmax + 1;
else
it = it + 1;
unew = unew - (unew - G - h * b (1) * fnew)/den;
err = abs (unew - y);
end
end
u = [u; unew]; f = [f; fnew];
end
t = t0;
In the forthcoming sections we examine some families of multistep methods.
11.5.1
Adams Methods
These methods are derived from the integral form (11.2) through an ap-
proximate evaluation of the integral of f between tn and tn+1. We suppose
that the discretization nodes are equally spaced, i.e., tj = t0 + jh, with
h > 0 and j ‚â•1, and then we integrate, instead of f, its interpolating
polynomial on p + 1 distinct nodes. The resulting schemes are thus consis-
tent by construction and have the following form
un+1 = un + h
p

j=‚àí1
bjfn‚àíj,
n ‚â•p.
(11.49)
The interpolation nodes can be either:
1. tn, tn‚àí1, . . . , tn‚àíp (in this case b‚àí1 = 0 and the resulting method is
explicit);
or

11.5 Multistep Methods
491
2. tn+1, tn, . . . , tn‚àíp+1 (in this case b‚àí1 Ã∏= 0 and the scheme is implicit).
The implicit schemes are called Adams-Moulton methods, while the explicit
ones are called Adams-Bashforth methods.
Adams-Bashforth methods (AB)
Taking p = 0 we recover the forward Euler method, since the interpolating
polynomial of degree zero at node tn is given by Œ†0f = fn. For p = 1, the
linear interpolating polynomial at the nodes tn‚àí1 and tn is
Œ†1f(t) = fn + (t ‚àítn)fn‚àí1 ‚àífn
tn‚àí1 ‚àítn
.
Since Œ†1f(tn) = fn and Œ†1f(tn+1) = 2fn ‚àífn‚àí1, we get
tn+1
>
tn
Œ†1f(t) = h
2 [Œ†1f(tn) + Œ†1f(tn+1)] = h
2 [3fn ‚àífn‚àí1] .
Therefore, the two-step AB method is
un+1 = un + h
2 [3fn ‚àífn‚àí1] .
(11.50)
With a similar procedure, if p = 2, we Ô¨Ånd the three-step AB method
un+1 = un + h
12 [23fn ‚àí16fn‚àí1 + 5fn‚àí2] ,
while for p = 3 we get the four-step AB scheme
un+1 = un + h
24 (55fn ‚àí59fn‚àí1 + 37fn‚àí2 ‚àí9fn‚àí3) .
In general, q-step Adams-Bashforth methods have order q. The error con-
stants C‚àó
q+1 of these methods are collected in Table 11.1.
Adams-Moulton methods (AM)
If p = ‚àí1, the Backward Euler scheme is recovered, while if p = 0, we
construct the linear polynomial interpolating f at the nodes tn and tn+1
to recover the Crank-Nicolson scheme (11.9). In the case of the two-step
method, the polynomial of degree 2 interpolating f at the nodes tn‚àí1, tn,
tn+1 is generated, yielding the following scheme
un+1 = un + h
12 [5fn+1 + 8fn ‚àífn‚àí1] .
(11.51)

492
11. Numerical Solution of Ordinary DiÔ¨Äerential Equations
The methods corresponding to p = 3 and 4 are respectively given by
un+1 = un + h
24 (9fn+1 + 19fn ‚àí5fn‚àí1 + fn‚àí2)
un+1 = un +
h
720 (251fn+1 + 646fn ‚àí264fn‚àí1 + 106fn‚àí2 ‚àí19fn‚àí3) .
The q-step Adams-Moulton methods have order q + 1 and their error con-
stants Cq+1 are summarized in Table 11.1.
q
C‚àó
q+1
Cq+1
q
C‚àó
q+1
Cq+1
1
1
2
‚àí1
2
3
3
8
‚àí1
24
2
5
12
‚àí1
12
4
251
720
‚àí19
720
TABLE 11.1. Error constants for Adams-Bashforth methods (having order q) and
Adams-Moulton methods (having order q + 1)
11.5.2
BDF Methods
The so-called backward diÔ¨Äerentiation formulae (henceforth denoted by
BDF) are implicit MS methods derived from a complementary approach
to the one followed for the Adams methods. In fact, for the Adams meth-
ods we have resorted to numerical integration for the source function f,
whereas in BDF methods we directly approximate the value of the Ô¨Årst
derivative of y at node tn+1 through the Ô¨Årst derivative of the polynomial
interpolating y at the p + 1 nodes tn+1, tn, . . . , tn‚àíp+1.
By doing so, we get schemes of the form
un+1 =
p

j=0
ajun‚àíj + hb‚àí1fn+1
with b‚àí1 Ã∏= 0. Method (11.8) represents the most elementary example,
corresponding to the coeÔ¨Écients a0 = 1 and b‚àí1 = 1.
We summarize in Table 11.2 the coeÔ¨Écients of BDF methods that are
zero-stable. In fact, we shall see in Section 11.6.3 that only for p ‚â§5 are
BDF methods zero-stable (see [Cry73]).
11.6
Analysis of Multistep Methods
Analogous to what has been done for one-step methods, in this section
we provide algebraic conditions that ensure consistency and stability of
multistep methods.

11.6 Analysis of Multistep Methods
493
p
a0
a1
a2
a3
a4
a5
b‚àí1
0
1
0
0
0
0
0
1
1
4
3
- 1
3
0
0
0
0
2
3
2
18
11
- 9
11
2
11
0
0
0
6
11
3
48
25
- 36
25
16
25
-
3
25
0
0
12
25
4
300
137
- 300
137
200
137
- 75
137
12
137
0
60
137
5
360
147
- 450
147
400
147
- 225
147
72
147
- 10
147
60
137
TABLE 11.2. CoeÔ¨Écients of zero-stable BDF methods for p = 0, 1, . . . , 5
11.6.1
Consistency
The property of consistency of a multistep method introduced in DeÔ¨Ånition
11.9 can be veriÔ¨Åed by checking that the coeÔ¨Écients satisfy certain algebraic
equations, as stated in the following theorem.
Theorem 11.3 The multistep method (11.45) is consistent iÔ¨Äthe following
algebraic relations among the coeÔ¨Écients are satisÔ¨Åed
p

j=0
aj = 1,
‚àí
p

j=0
jaj +
p

j=‚àí1
bj = 1.
(11.52)
Moreover, if y ‚ààCq+1(I) for some q ‚â•1, where y is the solution of the
Cauchy problem (11.1), then the method is of order q iÔ¨Ä(11.52) holds and
the following additional conditions are satisÔ¨Åed
p

j=0
(‚àíj)iaj + i
p

j=‚àí1
(‚àíj)i‚àí1bj = 1, i = 2, . . . , q.
Proof. Expanding y and f in a Taylor series yields, for any n ‚â•p
yn‚àíj = yn ‚àíjhy‚Ä≤
n + O(h2),
fn‚àíj = fn + O(h).
(11.53)
Plugging these values back into the multistep scheme and neglecting the terms
in h of order higher than 1 gives
yn+1 ‚àí
p

j=0
ajyn‚àíj ‚àíh
p

j=‚àí1
bjfn‚àíj
= yn+1 ‚àí
p

j=0
ajyn + h
p

j=0
jajy‚Ä≤
n ‚àíh
p

j=‚àí1
bjfn ‚àíO(h2)

p

j=0
aj ‚àí
p

j=‚àí1
bj

= yn+1 ‚àí
p

j=0
ajyn ‚àíhy‚Ä≤
n

‚àí
p

j=0
jaj +
p

j=‚àí1
bj

‚àíO(h2)

p

j=0
aj ‚àí
p

j=‚àí1
bj

where we have replaced y‚Ä≤
n by fn. From the deÔ¨Ånition (11.47) we then obtain
hœÑn+1(h) = yn+1 ‚àí
p

j=0
ajyn ‚àíhy‚Ä≤
n

‚àí
p

j=0
jaj +
p

j‚àí1
bj

‚àíO(h2)

p

j=0
aj ‚àí
p

j=‚àí1
bj


494
11. Numerical Solution of Ordinary DiÔ¨Äerential Equations
hence the local truncation error is
œÑn+1(h) = yn+1 ‚àíyn
h
+ yn
h

1 ‚àí
p

j=0
aj

+y‚Ä≤
n

p

j=0
jaj ‚àí
p

j=‚àí1
bj

‚àíO(h)

p

j=0
aj ‚àí
p

j=‚àí1
bj

.
Since, for any n, (yn+1 ‚àíyn)/h ‚Üíy‚Ä≤
n, as h ‚Üí0, it follows that œÑn+1(h) tends to
0 as h goes to 0 iÔ¨Äthe algebraic conditions (11.52) are satisÔ¨Åed. The rest of the
proof can be carried out in a similar manner, accounting for terms of progressively
higher order in the expansions (11.53).
3
11.6.2
The Root Conditions
Let us employ the multistep method (11.45) to approximately solve the
model problem (11.24). The numerical solution satisÔ¨Åes the linear diÔ¨Äerence
equation
un+1 =
p

j=0
ajun‚àíj + hŒª
p

j=‚àí1
bjun‚àíj,
(11.54)
which Ô¨Åts the form (11.29). We can therefore apply the theory devel-
oped in Section 11.4 and look for fundamental solutions of the form uk =
[ri(hŒª)]k, k = 0, 1, . . . , where ri(hŒª), for i = 0, . . . , p, are the roots of the
polynomial Œ† ‚ààPp+1
Œ†(r) = œÅ(r) ‚àíhŒªœÉ(r).
(11.55)
We have denoted by
œÅ(r) = rp+1 ‚àí
p

j=0
ajrp‚àíj,
œÉ(r) = b‚àí1rp+1 +
p

j=0
bjrp‚àíj
the Ô¨Årst and second characteristic polynomials of the multistep method
(11.45), respectively. The polynomial Œ†(r) is the characteristic polynomial
associated with the diÔ¨Äerence equation (11.54), and rj(hŒª) are its charac-
teristic roots.
The roots of œÅ are ri(0), i = 0, . . . , p, and will be abbreviated henceforth
by ri. From the Ô¨Årst condition in (11.52) it follows that if a multistep
method is consistent then 1 is a root of œÅ. We shall assume that such a root
(the consistency root) is labelled as r0(0) = r0 and call the corresponding
root r0(hŒª) the principal root.
DeÔ¨Ånition 11.10 (Root condition) The multistep method (11.45) is said
to satisfy the root condition if all roots ri are contained within the unit

11.6 Analysis of Multistep Methods
495
circle centered at the origin of the complex plane, otherwise, if they fall on
its boundary, they must be simple roots of œÅ. Equivalently,
"
|rj| ‚â§1,
j = 0, . . . , p;
furthermore, for those j such that |rj| = 1, then œÅ‚Ä≤(rj) Ã∏= 0.
(11.56)
‚ñ†
DeÔ¨Ånition 11.11 (Strong root condition) The multistep method (11.45)
is said to satisfy the strong root condition if it satisÔ¨Åes the root condition
and r0 = 1 is the only root lying on the boundary of the unit circle. Equiv-
alently,
|rj| < 1
j = 1, . . . , p.
(11.57)
‚ñ†
DeÔ¨Ånition 11.12 (Absolute root condition) The multistep method
(11.45) satisÔ¨Åes the absolute root condition if there exists h0 > 0 such that
|rj(hŒª)| < 1
j = 0, . . . , p,
‚àÄh ‚â§h0.
‚ñ†
11.6.3
Stability and Convergence Analysis for Multistep
Methods
Let us now examine the relation between root conditions and the stability
of multistep methods. Generalizing the DeÔ¨Ånition 11.4, we can get the
following.
DeÔ¨Ånition 11.13 (Zero-stability of multistep methods) The multi-
step method (11.45) is zero-stable if
‚àÉh0 > 0, ‚àÉC > 0 :
‚àÄh ‚àà(0, h0], |z(h)
n
‚àíu(h)
n | ‚â§CŒµ, 0 ‚â§n ‚â§Nh,
(11.58)
where Nh = max {n : tn ‚â§t0 + T} and z(h)
n
and u(h)
n
are, respectively, the
solutions of problems
Ô£±
Ô£¥
Ô£≤
Ô£¥
Ô£≥
z(h)
n+1 =
p

j=0
ajz(h)
n‚àíj + h
p

j=‚àí1
bjf(tn‚àíj, z(h)
n‚àíj) + hŒ¥n+1,
z(h)
k
= w(h)
k
+ Œ¥k,
k = 0, . . . , p
(11.59)

496
11. Numerical Solution of Ordinary DiÔ¨Äerential Equations
Ô£±
Ô£¥
Ô£≤
Ô£¥
Ô£≥
u(h)
n+1 =
p

j=0
aju(h)
n‚àíj + h
p

j=‚àí1
bjf(tn‚àíj, u(h)
n‚àíj),
u(h)
k
= w(h)
k ,
k = 0, . . . , p
(11.60)
for p ‚â§n ‚â§Nh ‚àí1, where |Œ¥k| ‚â§Œµ, 0 ‚â§k ‚â§Nh, w(h)
0
= y0 and w(h)
k ,
k = 1, . . . , p, are p initial values generated by using another numerical
scheme.
‚ñ†
Theorem 11.4 (Equivalence of zero-stability and root condition)
For a consistent multistep method, the root condition is equivalent to zero-
stability.
Proof. Let us begin by proving that the root condition is necessary for the zero-
stability to hold. We proceed by contradiction and assume that the method is
zero-stable and there exists a root ri which violates the root condition.
Since the method is zero-stable, condition (11.58) must be satisÔ¨Åed for any
Cauchy problem, in particular for the problem y‚Ä≤(t) = 0 with y(0) = 0, whose
solution is, clearly, the null function. Similarly, the solution u(h)
n
of (11.60) with
f = 0 and w(h)
k
= 0 for k = 0, . . . , p is identically zero.
Consider Ô¨Årst the case |ri| > 1. Then, deÔ¨Åne
Œ¥n =
Ô£±
Ô£≤
Ô£≥
Œµrn
i
if ri ‚ààR,
Œµ(ri + ¬Øri)n
if ri ‚ààC,
for Œµ > 0. It is simple to check that the sequence z(h)
n
= Œ¥n for n = 0, 1, . . .
is a solution of (11.59) with initial conditions z(h)
k
= Œ¥k and that |Œ¥k| ‚â§Œµ for
k = 0, 1, . . . , p. Let us now choose ¬Øt in (t0, t0 + T) and let xn be the nearest grid
node to ¬Øt. Clearly, n is the integral part of ¬Øt/h and limh‚Üí0 |z(h)
n | = limh‚Üí0 |u(h)
n ‚àí
z(h)
n | ‚Üí‚àûas h ‚Üí0. This proves that |u(h)
n
‚àíz(h)
n | cannot be uniformly bounded
with respect to h as h ‚Üí0, which contradicts the assumption that the method
is zero-stable.
A similar proof can be carried out if |ri| = 1 but has multiplicity greater than
1, provided that one takes into account the form of the solution (11.33).
Let us now prove that the root condition is suÔ¨Écient for method (11.45) to
be zero-stable. Recalling (11.46) and denoting by z(h)
n+j and u(h)
n+j the solutions
to (11.59) and (11.60), respectively, for j ‚â•1, it turns out that the function
w(h)
n+j = z(h)
n+j ‚àíu(h)
n+j satisÔ¨Åes the following diÔ¨Äerence equation
p+1

j=0
Œ±jw(h)
n+j = œïn+p+1,
n = 0, . . . , Nh ‚àí(p + 1),
(11.61)
having set
œïn+p+1 = h
p+1

j=0
Œ≤j
6
f(tn+j, z(h)
n+j) ‚àíf(tn+j, u(h)
n+j)
7
+ hŒ¥n+p+1.
(11.62)

11.6 Analysis of Multistep Methods
497
Denote by
2
œà(n)
j
3
a sequence of fundamental solutions to the homogeneous equa-
tion associated with (11.61). Recalling (11.42), the general solution of (11.61) is
given by
w(h)
n
=
p

j=0
w(h)
j
œà(n)
j
+
n

l=p+1
œà(n‚àíl+p)
p
œïl,
n = p + 1, . . .
The following result expresses the connection between the root condition and the
boundedness of the solution of a diÔ¨Äerence equation (for the proof, see [Gau97],
Theorem 6.3.2).
Lemma 11.3 There exists a constant M > 0 for any solution {un} of the dif-
ference equation (11.28) such that
|un| ‚â§M
"
max
j=0,... ,k‚àí1|uj| +
n

l=k
|œïl|
#
,
n = 0, 1, . . .
(11.63)
iÔ¨Äthe root condition is satisÔ¨Åed for the polynomial (11.30), i.e., (11.56) holds for
the zeros of the polynomial (11.30).
Let us now recall that, for any j, {œà(n)
j
} is solution of a homogeneous diÔ¨Äerence
equation whose initial data are œà(i)
j
= Œ¥ij, i, j = 0, . . . , p. On the other hand, for
any l, œà(n‚àíl+p)
p
is solution of a diÔ¨Äerence equation with zero initial conditions and
right-hand sides equal to zero except for the one corresponding to n = l which is
œà(p)
p
= 1.
Therefore, Lemma 11.3 can be applied in both cases so we can conclude that
there exists a constant M > 0 such that |œà(n)
j
| ‚â§M and |œà(n‚àíl+p)
p
| ‚â§M,
uniformly with respect to n and l. The following estimate thus holds
|w(h)
n | ‚â§M
Ô£±
Ô£≤
Ô£≥(p + 1) max
j=0,... ,p|w(h)
j
| +
n

l=p+1
|œïl|
Ô£º
Ô£Ω
Ô£æ,
n = 0, 1, . . . , Nh.
(11.64)
If L denotes the Lipschitz constant of f, from (11.62) we get
|œïn+p+1| ‚â§h
max
j=0,... ,p+1|Œ≤j|L
p+1

j=0
|w(h)
n+j| + h|Œ¥n+p+1|.
Let Œ≤ =
max
j=0,... ,p+1|Œ≤j| and ‚àÜ[q,r] =
max
j=q,... ,r|Œ¥j+q|, q and r being some integers
with q ‚â§r. From (11.64), the following estimate is therefore obtained
|w(h)
n |
‚â§
M
Ô£±
Ô£≤
Ô£≥(p + 1)‚àÜ[0,p] + hŒ≤L
n

l=p+1
p+1

j=0
|w(h)
l‚àíp‚àí1+j| + Nhh‚àÜ[p+1,n]
Ô£º
Ô£Ω
Ô£æ
‚â§
M
"
(p + 1)‚àÜ[0,p] + hŒ≤L(p + 2)
n

m=0
|w(h)
m | + T‚àÜ[p+1,n]
#
.

498
11. Numerical Solution of Ordinary DiÔ¨Äerential Equations
Let Q = 2(p + 2)Œ≤LM and h0 = 1/Q, so that 1 ‚àíh Q
2 ‚â•1
2 if h ‚â§h0. Then
1
2|w(h)
n |
‚â§
|w(h)
n |(1 ‚àíh Q
2 )
‚â§
M
"
(p + 1)‚àÜ[0,p] + hŒ≤L(p + 2)
n‚àí1

m=0
|w(h)
m | + T‚àÜ[p+1,n]
#
.
Letting R = 2M

(p + 1)‚àÜ[0,p] + T‚àÜ[p+1,n]

, we Ô¨Ånally obtain
|w(h)
n | ‚â§hQ
n‚àí1

m=0
|w(h)
m | + R.
Applying Lemma 11.2 with the following identiÔ¨Åcations: œïn = |w(h)
n |, g0 = R,
ps = 0 and ks = hQ for any s = 0, . . . , n ‚àí1, yields
|w(h)
n | ‚â§2MeT Q 
(p + 1)‚àÜ[0,p] + T‚àÜ[p+1,n]

.
(11.65)
Method (11.45) is thus zero-stable for any h ‚â§h0.
3
Theorem 11.4 allows for characterizing the stability behavior of several
families of discretization methods.
In the special case of consistent one-step methods, the polynomial œÅ ad-
mits only the root r0 = 1. They thus automatically satisfy the root condition
and are zero-stable.
For the Adams methods (11.49), the polynomial œÅ is always of the form
œÅ(r) = rp+1 ‚àírp. Thus, its roots are r0 = 1 and r1 = 0 (with multiplicity
p) so that all Adams methods are zero-stable.
Also the midpoint method (11.43) and Simpson method (11.44) are zero-
stable: for both of them, the Ô¨Årst characteristic polynomial is œÅ(r) = r2 ‚àí1,
so that r0 = 1 and r1 = ‚àí1.
Finally, the BDF methods of Section 11.5.2 are zero-stable provided that
p ‚â§5, since in such a case the root condition is satisÔ¨Åed (see [Cry73]).
We are in position to give the following convergence result.
Theorem 11.5 (Convergence) A consistent multistep method is conver-
gent iÔ¨Äit satisÔ¨Åes the root condition and the error on the initial data tends
to zero as h ‚Üí0. Moreover, the method converges with order q if it has
order q and the error on the initial data tends to zero as O(hq).
Proof. Suppose that the MS method is consistent and convergent. To prove that
the root condition is satisÔ¨Åed, we refer to the problem y‚Ä≤(t) = 0 with y(0) = 0
and on the interval I = (0, T). Convergence means that the numerical solution
{un} must tend to the exact solution y(t) = 0 for any converging set of initial
data uk, k = 0, . . . , p, i.e.
max
k=0,... ,p|uk| ‚Üí0 as h ‚Üí0. From this observation, the
proof follows by contradiction along the same lines as the proof of Theorem 11.4,
where the parameter Œµ is now replaced by h.

11.6 Analysis of Multistep Methods
499
Let us now prove that consistency, together with the root condition, implies
convergence under the assumption that the error on the initial data tends to zero
as h ‚Üí0. We can apply Theorem 11.4, setting u(h)
n
= un (approximate solution
of the Cauchy problem) and z(h)
n
= yn (exact solution), and from (11.47) it turns
out that Œ¥m = œÑm(h). Then, due to (11.65), for any n ‚â•p + 1 we obtain
|un ‚àíyn| ‚â§2MeT Q
%
(p + 1) max
j=0,... ,p|uj ‚àíyj| + T
max
j=p+1,... ,n|œÑj(h)|
&
.
Convergence holds by noticing that the right-hand side of this inequality tends
to zero as h ‚Üí0.
3
A remarkable consequence of the above theorem is the following equivalence
Lax-Richtmyer theorem.
Corollary 11.1 (Equivalence theorem) A consistent multistep method
is convergent iÔ¨Äit is zero-stable and if the error on the initial data tends
to zero as h tends to zero.
We conclude this section with the following result, which establishes an
upper limit for the order of multistep methods (see [Dah63]).
Property 11.1 (First Dahlquist barrier) There isn‚Äôt any zero-stable,
p-step linear multistep method with order greater than p + 1 if p is odd,
p + 2 if p is even.
11.6.4
Absolute Stability of Multistep Methods
Consider again the diÔ¨Äerence equation (11.54), which was obtained by ap-
plying the MS method (11.45) to the model problem (11.24). According to
(11.33), its solution takes the form
un =
k‚Ä≤

j=1
mj‚àí1

s=0
Œ≥sjns

[rj(hŒª)]n,
n = 0, 1, . . .
where rj(hŒª), j = 1, . . . , k‚Ä≤, are the distinct roots of the characteristic
polynomial (11.55), and having denoted by mj the multiplicity of rj(hŒª).
In view of (11.25), it is clear that the absolute root condition introduced
by DeÔ¨Ånition 11.12 is necessary and suÔ¨Écient to ensure that the multistep
method (11.45) is absolutely stable as h ‚â§h0.
Among the methods enjoying the absolute stability property, the pref-
erence should go to those for which the region of absolute stability A,
introduced in (11.26), is as wide as possible or even unbounded. Among
these are the A-stable methods introduced at the end of Section 11.3.3 and

500
11. Numerical Solution of Ordinary DiÔ¨Äerential Equations
the œë-stable methods, for which A contains the angular region deÔ¨Åned by
z ‚ààC such that ‚àíœë < œÄ ‚àíarg(z) < œë, with œë ‚àà(0, œÄ/2). A-stable meth-
ods are of remarkable importance when solving stiÔ¨Äproblems (see Section
11.10).
                                                                                                                                                                                                                                                              




















                                                                                                                                                                                                                                                              




















                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     








































Im
Re
Im
Re
œë
œë
FIGURE 11.4. Regions of absolute stability for A-stable (left) and (right) œë-stable
methods
The following result, whose proof is given in [Wid67], establishes a relation
between the order of a multistep method, the number of its steps and its
stability properties.
Property 11.2 (Second Dahlquist barrier) A linear explicit multistep
method can be neither A-stable, nor œë-stable. Moreover, there is no A-
stable linear multistep method with order greater than 2. Finally, for any
œë ‚àà(0, œÄ/2), there only exist œë-stable p-step linear multistep methods of
order p for p = 3 and p = 4.
Let us now examine the region of absolute stability of several MS methods.
The regions of absolute stability of both explicit and implicit Adams schemes
reduce progressively as the order of the method increases. In Figure 11.5
(left) we show the regions of absolute stability for the AB methods exam-
ined in Section 11.5.1, with exception of the Forward Euler method whose
region is shown in Figura 11.3.
The regions of absolute stability of the Adams-Moulton schemes, except
for the Crank-Nicolson method which is A-stable, are represented in Figure
11.5 (right).
In Figure 11.6 the regions of absolute stability of some of the BDF meth-
ods introduced in Section 11.5.2 are drawn. They are unbounded and al-

11.6 Analysis of Multistep Methods
501
‚àí2
‚àí1.5
‚àí1
‚àí0.5
0
‚àí0.8
‚àí0.6
‚àí0.4
‚àí0.2
0
0.2
0.4
0.6
0.8
AB2
AB3
AB4
‚àí6
‚àí4
‚àí2
0
‚àí4
‚àí3
‚àí2
‚àí1
0
1
2
3
4
AM3
AM4
AM5
FIGURE
11.5.
Outer
contours
of
the
regions
of
absolute
stability
for
Adams-Bashforth methods (left) ranging from second to fourth-order (AB2, AB3
and AB4) and for Adams-Moulton methods (right), from third to Ô¨Åfth-order
(AM3, AM4 and AM5). Notice that the region of the AB3 method extends into
the half-plane with positive real part. The region for the explicit Euler (AB1)
method was drawn in Figure 11.3
ways contain the negative real numbers. These stability features make BDF
methods quite attractive for solving stiÔ¨Äproblems (see Section 11.10).
‚àí4
‚àí2
0
2
4
6
8
10
12
14
‚àí6
‚àí4
‚àí2
0
2
4
6
BDF6
BDF5
BDF3
FIGURE 11.6. Inner contours of regions of absolute stability for three-step
(BDF3), Ô¨Åve-step (BDF5) and six-step (BDF6) BDF methods. Unlike Adams
methods, these regions are unbounded and extend outside the limited portion
that is shown in the Ô¨Ågure
Remark 11.3 Some authors (see, e.g., [BD74]) adopt an alternative deÔ¨Å-
nition of absolute stability by replacing (11.25) with the milder property
‚àÉC > 0 : |un| ‚â§C, as tn ‚Üí+‚àû.
According to this new deÔ¨Ånition, the absolute stability of a numerical
method should be regarded as the counterpart of the asymptotic stabil-

502
11. Numerical Solution of Ordinary DiÔ¨Äerential Equations
ity (11.6) of the Cauchy problem. The new region of absolute stability A‚àó
would be
A‚àó= {z ‚ààC : ‚àÉC > 0, |un| ‚â§C, ‚àÄn ‚â•0}
and it would not necessarily coincide with A. For example, in the case of
the midpoint method A is empty (thus, it is unconditionally absolutely
unstable), while A‚àó= {z = Œ±i, Œ± ‚àà[‚àí1, 1]}.
In general, if A is nonempty, then A‚àóis its closure. We notice that zero-
stable methods are those for which the region A‚àócontains the origin z = 0
of the complex plane.
‚ñ†
To conclude, let us notice that the strong root condition (11.57) implies,
for a linear problem, that
‚àÄh ‚â§h0, ‚àÉC > 0 : |un| ‚â§C(|u0| + . . . + |up|),
‚àÄn ‚â•p + 1.
(11.66)
We say that a method is relatively stable if it satisÔ¨Åes (11.66). Clearly,
(11.66) implies zero-stability, but the converse does not hold.
Figure 11.7 summarizes the main conclusions that have been drawn in this
section about stability, convergence and root-conditions, in the particular
case of a consistent method applied to the model problem (11.24).
Root
‚áê=
Strong root
‚áê=
Absolute root
condition
condition
condition
CDE
DDE
CDE
Convergence
‚áê‚áí
Zero
‚áê=
(11.66)
‚áê=
Absolute
stability
stability
FIGURE 11.7. Relations between root conditions, stability and convergence for
a consistent method applied to the model problem (11.24)
11.7
Predictor-Corrector Methods
When solving a nonlinear Cauchy problem of the form (11.1), at each time
step implicit schemes require dealing with a nonlinear equation. For in-
stance, if the Crank-Nicolson method is used, we get the nonlinear equation
un+1 = un + h
2 [fn + fn+1] = Œ®(un+1),

11.7 Predictor-Corrector Methods
503
that can be cast in the form Œ¶(un+1) = 0, where Œ¶(un+1) = un+1 ‚àí
Œ®(un+1). To solve this equation the Newton method would give
u(k+1)
n+1
= u(k)
n+1 ‚àíŒ¶(u(k)
n+1)/Œ¶‚Ä≤(u(k)
n+1),
for k = 0, 1, . . . , until convergence and require an initial datum u(0)
n+1 suÔ¨É-
ciently close to un+1. Alternatively, one can resort to Ô¨Åxed-point iterations
u(k+1)
n+1
= Œ®(u(k)
n+1)
(11.67)
for k = 0, 1, . . . , until convergence. In such a case, the global convergence
condition for the Ô¨Åxed-point method (see Theorem 6.1) sets a constraint
on the discretization stepsize of the form
h <
1
|b‚àí1|L
(11.68)
where L is the Lipschitz constant of f with respect to y. In practice, ex-
cept for the case of stiÔ¨Äproblems (see Section 11.10), this restriction on
h is not signiÔ¨Åcant since considerations of accuracy put a much more re-
strictive constraint on h. However, each iteration of (11.67) requires one
evaluation of the function f and the computational cost can be reduced by
providing a good initial guess u(0)
n+1. This can be done by taking one step of
an explicit MS method and then iterating on (11.67) for a Ô¨Åxed number m
of iterations. By doing so, the implicit MS method that is employed in the
Ô¨Åxed-point scheme ‚Äúcorrects‚Äù the value of un+1 ‚Äúpredicted‚Äù by the explicit
MS method. A procedure of this sort is called a predictor-corrector method,
or PC method. There are many ways in which a predictor-corrector method
can be implemented.
In its basic version, the value u(0)
n+1 is computed by an explicit Àúp + 1-step
method, called the predictor (here identiÔ¨Åed by the coeÔ¨Écients {Àúaj, Àúbj})
[P]
u(0)
n+1 =
Àúp

j=0
Àúaju(1)
n‚àíj + h
Àúp

j=0
Àúbjf (0)
n‚àíj,
where f (0)
k
= f(tk, u(0)
k ) and u(1)
k
are the solutions computed by the PC
method at the previous steps or are the initial conditions. Then, we evaluate
the function f at the new point (tn+1, u(0)
n+1) (evaluation step)
[E]
f (0)
n+1 = f(tn+1, u(0)
n+1),
and Ô¨Ånally, one single Ô¨Åxed-point iteration is carried out using an implicit
MS scheme of the form (11.45)
[C]
u(1)
n+1 =
p

j=0
aju(1)
n‚àíj + hb‚àí1f (0)
n+1 + h
p

j=0
bjf (0)
n‚àíj.

504
11. Numerical Solution of Ordinary DiÔ¨Äerential Equations
This second step of the procedure, which is actually explicit, is called the
corrector. The overall procedure is shortly denoted by PEC or P(EC)1
method, in which P and C denote one application at time tn+1 of the
predictor and the corrector methods, respectively, while E indicates one
evaluation of the function f.
This strategy above can be generalized supposing to perform m > 1 iter-
ations at each step tn+1. The corresponding methods are called predictor-
multicorrector schemes and compute u(0)
n+1 at time step tn+1 using the pre-
dictor in the following form
[P]
u(0)
n+1 =
Àúp

j=0
Àúaju(m)
n‚àíj + h
Àúp

j=0
Àúbjf (m‚àí1)
n‚àíj
.
(11.69)
Here m ‚â•1 denotes the (Ô¨Åxed) number of corrector iterations that are
carried out in the following steps [E], [C]: for k = 0, 1, . . . , m ‚àí1
[E]
f (k)
n+1 = f(tn+1, u(k)
n+1),
[C]
u(k+1)
n+1
=
p

j=0
aju(m)
n‚àíj + hb‚àí1f (k)
n+1 + h
p

j=0
bjf (m‚àí1)
n‚àíj
.
These implementations of the predictor-corrector technique are referred to
as P(EC)m. Another implementation, denoted by P(EC)mE, consists of
updating at the end of the process also the function f and is given by
[P]
u(0)
n+1 =
Àúp

j=0
Àúaju(m)
n‚àíj + h
Àúp

j=0
Àúbjf (m)
n‚àíj,
and for k = 0, 1, . . . , m ‚àí1,
[E]
f (k)
n+1 = f(tn+1, u(k)
n+1),
[C]
u(k+1)
n+1
=
p

j=0
aju(m)
n‚àíj + hb‚àí1f (k)
n+1 + h
p

j=0
bjf (m)
n‚àíj,
followed by
[E]
f (m)
n+1 = f(tn+1, u(m)
n+1).
Example 11.8 Heun‚Äôs method (11.10) can be regarded as a predictor-corrector
method whose predictor is the forward Euler method, while the corrector is the
Crank-Nicolson method.
Another example is provided by the Adams-Bashforth method of order 2
(11.50) and the Adams-Moulton method of order 3 (11.51). Its corresponding

11.7 Predictor-Corrector Methods
505
PEC implementation is: given u(0)
0
= u(1)
0
= u0, u(0)
1
= u(1)
1
= u1 and f (0)
0
=
f(t0, u(0)
0 ), f (0)
1
= f(t1, u(0)
1 ), compute for n = 1, 2, . . . ,
[P]
u(0)
n+1 = u(1)
n
+ h
2
6
3f (0)
n
‚àíf (0)
n‚àí1
7
,
[E]
f (0)
n+1 = f(tn+1, u(0)
n+1),
[C]
u(1)
n+1 = u(1)
n
+ h
12
6
5f (0)
n+1 + 8f (0)
n
‚àíf (0)
n‚àí1
7
,
while the PECE implementation is: given u(0)
0
= u(1)
0
= u0, u(0)
1
= u(1)
1
= u1 and
f (1)
0
= f(t0, u(1)
0 ), f (1)
1
= f(t1, u(1)
1 ), compute for n = 1, 2, . . . ,
[P]
u(0)
n+1 = u(1)
n
+ h
2
6
3f (1)
n
‚àíf (1)
n‚àí1
7
,
[E]
f (0)
n+1 = f(tn+1, u(0)
n+1),
[C]
u(1)
n+1 = u(1)
n
+ h
12
6
5f (0)
n+1 + 8f (1)
n
‚àíf (1)
n‚àí1
7
,
[E]
f (1)
n+1 = f(tn+1, u(1)
n+1).
‚Ä¢
Before studying the convergence of predictor-corrector methods, we intro-
duce a simpliÔ¨Åcation in the notation. Usually the number of steps of the
predictor is greater than those of the corrector, so that we deÔ¨Åne the num-
ber of steps of the predictor-corrector pair as being equal to the number of
steps of the predictor. This number will be denoted henceforth by p. Owing
to this deÔ¨Ånition we no longer demand that the coeÔ¨Écients of the corrector
satisfy |ap| + |bp| Ã∏= 0. Consider for example the predictor-corrector pair
[P]
u(0)
n+1 = u(1)
n
+ hf(tn‚àí1, u(0)
n‚àí1),
[C]
u(1)
n+1 = u(1)
n
+ h
2
6
f(tn, u(0)
n ) + f(tn+1, u(0)
n+1)
7
,
for which p = 2 (even though the corrector is a one-step method). Conse-
quently, the Ô¨Årst and the second characteristic polynomials of the corrector
method will be œÅ(r) = r2 ‚àír and œÉ(r) = (r2 + r)/2 instead of œÅ(r) = r ‚àí1
and œÉ(r) = (r + 1)/2.
In any predictor-corrector method, the truncation error of the predictor
combines with the one of the corrector, generating a new truncation error
which we are going to examine. Let Àúq and q be, respectively, the orders of the
predictor and the corrector and assume that y ‚ààCq+1, where q = max(Àúq, q).

506
11. Numerical Solution of Ordinary DiÔ¨Äerential Equations
Then
y(tn+1)
‚àí
p

j=0
Àúajy(tn‚àíj) ‚àíh
p

j=0
Àúbjf(tn‚àíj, yn‚àíj)
=
ÀúCÀúq+1hÀúq+1y(Àúq+1)(tn) + O(hÀúq+2),
y(tn+1)
‚àí
p

j=0
ajy(tn‚àíj) ‚àíh
p

j=‚àí1
bjf(tn‚àíj, yn‚àíj)
=
Cq+1hq+1y(q+1)(tn) + O(hq+2),
where ÀúCÀúq+1, Cq+1 are the error constants of the predictor and the corrector
method respectively. The following result holds.
Property 11.3 Let the predictor method have order Àúq and the corrector
method have order q. Then:
If Àúq ‚â•q (or Àúq < q with m > q ‚àíÀúq), then the predictor-corrector method
has the same order and the same PLTE as the corrector.
If Àúq < q and m = q ‚àíÀúq, then the predictor-corrector method has the same
order as the corrector, but diÔ¨Äerent PLTE.
If Àúq < q and m ‚â§q ‚àíÀúq ‚àí1, then the predictor-corrector method has order
equal to Àúq + m (thus less than q).
In particular, notice that if the predictor has order q ‚àí1 and the corrector
has order q, the PEC suÔ¨Éces to get a method of order q. Moreover, the
P(EC)mE and P(EC)m schemes have always the same order and the same
PLTE.
Combining the Adams-Bashforth method of order q with the correspond-
ing Adams-Moulton method of the same order we obtain the so-called ABM
method of order q. It is possible to estimate its PLTE as
Cq+1
C‚àó
q+1 ‚àíCq+1
+
u(m)
n+1 ‚àíu(0)
n+1
,
,
where Cq+1 and C‚àó
q+1 are the error constants given in Table 11.1. Accord-
ingly, the steplength h can be decreased if the estimate of the PLTE exceeds
a given tolerance and increased otherwise (for the adaptivity of the step
length in a predictor-corrector method, see [Lam91], pp.128‚Äì147).
Program 93 provides an implementation of the P(EC)mE methods. The
input parameters at, bt, a, b contain the coeÔ¨Écients Àúaj,Àúbj (j = 0, . . . , Àúp)
of the predictor and the coeÔ¨Écients aj (j = 0, . . . , p), bj (j = ‚àí1, . . . , p) of
the corrector. Moreover, f is a string containing the expression of f(t, y),

11.7 Predictor-Corrector Methods
507
h is the stepsize, t0 and tf are the end points of the time integration
interval, u0 is the vector of the initial data, m is the number of the corrector
inner iterations. The input variable pece must be set equal to ‚Äôy‚Äô if the
P(EC)mE is selected, conversely the P(EC)m scheme is chosen.
Program 93 - predcor : Predictor-corrector scheme
function [u,t]=predcor(a,b,at,bt,h,f,t0,u0,tf,pece,m)
p = max(length(a),length(b)-1); pt = max(length(at),length(bt));
q = max(p,pt); if length(u0) < q, break, end;
t = [t0:h:t0+(q-1)*h]; u = u0; y = u0; fe = eval(f);
k = q;
for t = t0+q*h:h:tf
ut = sum(at.*u(k:-1:k-pt+1))+h*sum(bt.*fe(k:-1:k-pt+1));
y = ut; foy = eval(f);
uv = sum(a.*u(k:-1:k-p+1))+h*sum(b(2:p+1).*fe(k:-1:k-p+1));
k = k+1;
for j = 1:m
fy = foy; up = uv + h*b(1)*fy; y = up; foy = eval(f);
end
if (pece==‚Äôy‚Äô|pece==‚ÄôY‚Äô)
fe = [fe, foy];
else
fe = [fe, fy];
end
u = [u, up];
end
t = [t0:h:tf];
Example 11.9 Let us check the performance of the P(EC)mE method on the
Cauchy problem y‚Ä≤(t) = e‚àíy(t) for t ‚àà[0, 1] with y(0) = 1. The exact solution is
y(t) = log(1 + t). In all the numerical experiments, the corrector method is the
Adams-Moulton third-order scheme (AM3), while the explicit Euler (AB1) and
the Adams-Bashforth second-order (AB2) methods are used as predictors. Figure
11.8 shows that the pair AB2-AM3 (m = 1) yields third-order convergence rate,
while AB1-AM3 (m = 1) has a Ô¨Årst-order accuracy. Taking m = 2 allows to
recover the third-order convergence rate of the corrector for the AB1-AM3 pair.
‚Ä¢
As for the absolute stability, the characteristic polynomial of P(EC)m
methods reads
Œ†P (EC)m(r) = b‚àí1rp (œÅ(r) ‚àíhŒªœÉ(r)) + Hm(1 ‚àíH)
1 ‚àíHm
(ÀúœÅ(r)œÉ(r) ‚àíœÅ(r)ÀúœÉ(r))
while for P(EC)mE we have
Œ†P (EC)mE(r) = œÅ(r) ‚àíhŒªœÉ(r) + Hm(1 ‚àíH)
1 ‚àíHm
(ÀúœÅ(r) ‚àíhŒªÀúœÉ(r)) .

508
11. Numerical Solution of Ordinary DiÔ¨Äerential Equations
10
‚àí3
10
‚àí2
10
‚àí1
10
‚àí12
10
‚àí10
10
‚àí8
10
‚àí6
10
‚àí4
10
‚àí2
FIGURE 11.8. Convergence rate for P(EC)mE methods as a function of log(h).
The symbol ‚àárefers to the AB2-AM3 method (m = 1), ‚ó¶to AB1-AM3 (m = 1)
and ‚ñ°to AB1-AM3 with m = 2
We have set H = hŒªb‚àí1 and denoted by ÀúœÅ and ÀúœÉ the Ô¨Årst and second charac-
teristic polynomial of the predictor method, respectively. The polynomials
œÅ and œÉ are related to the Ô¨Årst and second characteristic polynomials of the
corrector, as previously explained after Example 11.8. Notice that in both
cases the characteristic polynomial tends to the corresponding polynomial
of the corrector method, since the function Hm(1 ‚àíH)/(1 ‚àíHm) tends to
zero as m tends to inÔ¨Ånity.
Example 11.10 If we consider the ABM methods with a number of steps p, the
characteristic polynomials are œÅ(r) = ÀúœÅ(r) = r(rp‚àí1 ‚àírp‚àí2), while œÉ(r) = rœÉ(r),
where œÉ(r) is the second characteristic polynomial of the corrector. In Figure 11.9
(right) the stability regions for the ABM methods of order 2 are plotted. In the
case of the ABM methods of order 2, 3 and 4, the corresponding stability regions
can be ordered by size, namely, from the largest to the smallest one the regions of
PECE, P(EC)2E, the predictor and PEC methods are plotted in Figure 11.9,
left. The one-step ABM method is an exception to the rule and the largest region
is the one corresponding to the predictor method (see Figure 11.9, left).
‚Ä¢
11.8
Runge-Kutta (RK) Methods
When evolving from the forward Euler method (11.7) toward higher-order
methods, linear multistep methods (MS) and Runge-Kutta methods (RK)
adopt two opposite strategies.
Like the Euler method, MS schemes are linear with respect to both un
and fn = f(tn, un), require only one functional evaluation at each time
step and their accuracy can be increased at the expense of increasing the
number of steps. On the other hand, RK methods maintain the structure
of one-step methods, and increase their accuracy at the price of an increase
of functional evaluations at each time level, thus sacrifying linearity.

11.8 Runge-Kutta (RK) Methods
509
‚àí1.5
‚àí1
‚àí0.5
0
0.5
‚àí1.5
‚àí1
‚àí0.5
0
0.5
1
1.5
PECE
P(EC)2E
PEC
P(EC)2
‚àí2
‚àí1.5
‚àí1
‚àí0.5
0
0.5
‚àí2
‚àí1.5
‚àí1
‚àí0.5
0
0.5
1
1.5
2
PECE
P(EC)2E
PEC
FIGURE 11.9. Stability regions for the ABM methods of order 1 (left) and 2
(right)
A consequence is that RK methods are more suitable than MS methods
at adapting the stepsize, whereas estimating the local error for RK methods
is more diÔ¨Écult than it is in the case of MS methods.
In its most general form, an RK method can be written as
un+1 = un + hF(tn, un, h; f),
n ‚â•0
(11.70)
where F is the increment function deÔ¨Åned as follows
F(tn, un, h; f) =
s

i=1
biKi,
Ki = f(tn + cih, un + h
s

j=1
aijKj),
i = 1, 2, . . . , s
(11.71)
and s denotes the number of stages of the method. The coeÔ¨Écients {aij},
{ci} and {bi} fully characterize an RK method and are usually collected in
the so-called Butcher array
c1
a11
a12
. . .
a1s
c2
a21
a22
a2s
...
...
...
...
cs
as1
as2
. . .
ass
b1
b2
. . .
bs
or
c
A
bT
where A = (aij) ‚ààRs√ós, b = (b1, . . . , bs)T ‚ààRs and c = (c1, . . . , cs)T ‚àà
Rs. We shall henceforth assume that the following condition holds
ci =
s

j=1
aij
i = 1, . . . , s.
(11.72)

510
11. Numerical Solution of Ordinary DiÔ¨Äerential Equations
If the coeÔ¨Écients aij in A are equal to zero for j ‚â•i, with i = 1, 2, . . . , s,
then each Ki can be explicitly computed in terms of the i ‚àí1 coeÔ¨Écients
K1, . . . , Ki‚àí1 that have already been determined. In such a case the RK
method is explicit. Otherwise, it is implicit and solving a nonlinear system
of size s is necessary for computing the coeÔ¨Écients Ki.
The increase in the computational eÔ¨Äort for implicit schemes makes their
use quite expensive; an acceptable compromise is provided by RK semi-
implicit methods, in which case aij = 0 for j > i so that each Ki is the
solution of the nonlinear equation
Ki = f
Ô£´
Ô£≠tn + cih, un + haii Ki + h
i‚àí1

j=1
aijKj
Ô£∂
Ô£∏.
A semi-implicit scheme thus requires s nonlinear independent equations to
be solved.
The local truncation error œÑn+1(h) at node tn+1 of the RK method
(11.70) is deÔ¨Åned through the residual equation
hœÑn+1(h) = yn+1 ‚àíyn ‚àíhF(tn, yn, h; f),
where y(t) is the exact solution to the Cauchy problem (11.1). Method
(11.70) is consistent if œÑ(h) = maxn |œÑn(h)| ‚Üí0 as h ‚Üí0. It can be shown
(see [Lam91]) that this happens iÔ¨Ä
s

i=1
bi = 1.
As usual, we say that (11.70) is a method of order p (‚â•1) with respect to
h if œÑ(h) = O(hp) as h ‚Üí0.
As for convergence, since RK methods are one-step methods, consistency
implies stability and, in turn, convergence. As happens for MS methods,
estimates of œÑ(h) can be derived; however, these estimates are often too
complicated to be proÔ¨Åtably used. We only mention that, as for MS meth-
ods, if an RK scheme has a local truncation error œÑn(h) = O(hp), for any
n, then also the convergence order will be equal to p.
The following result establishes a relation between order and number of
stages of explicit RK methods.
Property 11.4 The order of an s-stage explicit RK method cannot be
greater than s. Also, there do not exist s-stage explicit RK methods with
order s ‚â•5.
We refer the reader to [But87] for the proofs of this result and the results we
give below. In particular, for orders ranging between 1 and 10, the minimum

11.8 Runge-Kutta (RK) Methods
511
number of stages smin required to get a method of corresponding order is
shown below
order
1
2
3
4
5
6
7
8
smin
1
2
3
4
6
7
9
11
Notice that 4 is the maximum number of stages for which the order of the
method is not less than the number of stages itself. An example of a fourth-
order RK method is provided by the following explicit 4-stage method
un+1 = un + h
6 (K1 + 2K2 + 2K3 + K4)
K1 = fn,
K2 = f(tn + h
2 , un + h
2 K1),
K3 = f(tn + h
2 , un + h
2 K2),
K4 = f(tn+1, un + hK3).
(11.73)
As far as implicit schemes are concerned, the maximum achievable order
using s stages is equal to 2s.
Remark 11.4 (The case of systems of ODEs) An RK method can be
readily extended to systems of ODEs. However, the order of an RK method
in the scalar case does not necessarily coincide with that in the vector
case. In particular, for p ‚â•4, a method having order p in the case of the
autonomous system y‚Ä≤ = f(y), with f : Rm ‚ÜíRn maintains order p even
when applied to an autonomous scalar equation y‚Ä≤ = f(y), but the converse
is not true. Regarding this concern, see [Lam91], Section 5.8.
‚ñ†
11.8.1
Derivation of an Explicit RK Method
The standard technique for deriving an explicit RK method consists of en-
forcing that the highest number of terms in Taylor‚Äôs expansion of the exact
solution yn+1 about tn coincide with those of the approximate solution
un+1, assuming that we take one step of the RK method starting from the
exact solution yn. We provide an example of this technique in the case of
an explicit 2-stage RK method.
Let us consider a 2-stage explicit RK method and assume to dispose at
the n-th step of the exact solution yn. Then
un+1 = yn + hF(tn, yn, h; f) = yn + h(b1K1 + b2K2),
K1 = fn,
K2 = f(tn + hc2, yn + hc2K1),
having assumed that (11.72) is satisÔ¨Åed. Expanding K2 in a Taylor series
in a neighborhood of tn and truncating the expansion at the second order,

512
11. Numerical Solution of Ordinary DiÔ¨Äerential Equations
we get
K2 = fn + hc2(fn,t + K1fn,y) + O(h2).
We have denoted by fn,z (for z = t or z = y) the partial derivative of f
with respect to z evaluated at (tn, yn). Then
un+1 = yn + hfn(b1 + b2) + h2c2b2(fn,t + fnfn,y) + O(h3).
If we perform the same expansion on the exact solution, we Ô¨Ånd
yn+1 = yn + hy‚Ä≤
n + h2
2 y‚Ä≤‚Ä≤
n + O(h3) = yn + hfn + h2
2 (fn,t + fnfn,y) + O(h3).
Forcing the coeÔ¨Écients in the two expansions above to agree, up to higher-
order terms, we obtain that the coeÔ¨Écients of the RK method must satisfy
b1 + b2 = 1, c2b2 = 1
2.
Thus, there are inÔ¨Ånitely many 2-stage explicit RK methods with second-
order accuracy. Two examples are the Heun method (11.10) and the modi-
Ô¨Åed Euler method (11.91). Of course, with similar (and cumbersome) com-
putations in the case of higher-stage methods, and accounting for a higher
number of terms in Taylor‚Äôs expansion, one can generate higher-order RK
methods. For instance, retaining all the terms up to the Ô¨Åfth one, we get
scheme (11.73).
11.8.2
Stepsize Adaptivity for RK Methods
Since RK schemes are one-step methods, they are well-suited to adapting
the stepsize h, provided that an eÔ¨Écient estimator of the local error is
available. Usually, a tool of this kind is an a posteriori error estimator,
since the a priori local error estimates are too complicated to be used in
practice. The error estimator can be constructed in two ways:
- using the same RK method, but with two diÔ¨Äerent stepsizes (typically 2h
and h);
- using two RK methods of diÔ¨Äerent order, but with the same number s of
stages.
In the Ô¨Årst case, if an RK method of order p is being used, one pretends
that, starting from an exact datum un = yn (which would not be available
if n ‚â•1), the local error at tn+1 is less than a Ô¨Åxed tolerance. The following
relation holds
yn+1 ‚àíun+1 = Œ¶(yn)hp+1 + O(hp+2),
(11.74)
where Œ¶ is an unknown function evaluated at yn. (Notice that, in this
special case, yn+1 ‚àíun+1 = hœÑn+1(h)).

11.8 Runge-Kutta (RK) Methods
513
Carrying out the same computation with a stepsize of 2h, starting from
tn‚àí1, and denoting by un+1 the computed solution, yields
yn+1 ‚àíun+1 = Œ¶(yn‚àí1)(2h)p+1 + O(hp+2) = Œ¶(yn)(2h)p+1 + O(hp+2)
(11.75)
having expanded also yn‚àí1 with respect to tn. Subtracting (11.74) from
(11.75), we get
(2p+1 ‚àí1)hp+1Œ¶(yn) = un+1 ‚àíun+1 + O(hp+2),
from which
yn+1 ‚àíun+1 ‚âÉun+1 ‚àíun+1
(2p+1 ‚àí1)
= E.
If |E| is less than the Ô¨Åxed tolerance Œµ, the scheme moves to the next time
step, otherwise the estimate is repeated with a halved stepsize. In general,
the stepsize is doubled whenever |E| is less than Œµ/2p+1.
This approach yields a considerable increase in the computational eÔ¨Äort,
due to the s ‚àí1 extra functional evaluations needed to generate the value
un+1. Moreover, if one needs to half the stepsize, the value un must also
be computed again.
An alternative that does not require extra functional evaluations consists
of using simultaneously two diÔ¨Äerent RK methods with s stages, of order
p and p + 1, respectively, which share the same set of values Ki. These
methods are synthetically represented by the modiÔ¨Åed Butcher array
c
A
bT 2
bT 2
ET 2
(11.76)
where the method of order p is identiÔ¨Åed by the coeÔ¨Écients c, A and b,
while that of order p + 1 is identiÔ¨Åed by c, A and b, and where E = b ‚àíb.
Taking the diÔ¨Äerence between the approximate solutions at tn+1 pro-
duced by the two methods provides an estimate of the local truncation
error for the scheme of lower order. On the other hand, since the coeÔ¨É-
cients Ki coincide, this diÔ¨Äerence is given by h s
i=1 EiKi and thus it does
not require extra functional evaluations.
Notice that, if the solution un+1 computed by the scheme of order p is
used to initialize the scheme at time step n+2, the method will have order
p, as a whole. If, conversely, the solution computed by the scheme of order
p+1 is employed, the resulting scheme would still have order p+1 (exactly
as happens with predictor-corrector methods).
The Runge-Kutta Fehlberg method of fourth-order is one of the most
popular schemes of the form (11.76) and consists of a fourth-order RK

514
11. Numerical Solution of Ordinary DiÔ¨Äerential Equations
scheme coupled with a Ô¨Åfth-order RK method (for this reason, it is known
as the RK45 method). The modiÔ¨Åed Butcher array for this method is shown
below
0
0
0
0
0
0
0
1
4
1
4
0
0
0
0
0
3
8
3
32
9
32
0
0
0
0
12
13
1932
2197
‚àí7200
2197
7296
2197
0
0
0
1
439
216
‚àí8
3680
513
‚àí845
4104
0
0
1
2
‚àí8
27
2
‚àí3544
2565
1859
4104
‚àí11
40
0
25
216
0
1408
2565
2197
4104
‚àí1
5
0
16
135
0
6656
12825
28561
56430
‚àí9
50
2
55
1
360
0
‚àí128
4275
‚àí2197
75240
1
50
2
55
This method tends to underestimate the error. As such, its use is not com-
pletely reliable when the stepsize h is large.
Remark 11.5 MATLAB provides a package tool funfun, which, besides
the two classical Runge-Kutta Fehlberg methods, RK23 (second-order and
third-order pair) and RK45 (fourth-order and Ô¨Åfth-order pair), also imple-
ments other methods suitable for solving stiÔ¨Äproblems. These methods are
derived from BDF methods (see [SR97]) and are included in the MATLAB
program ode15s.
‚ñ†
11.8.3
Implicit RK Methods
Implicit RK methods can be derived from the integral formulation of the
Cauchy problem (11.2). In fact, if a quadrature formula with s nodes in
(tn, tn+1) is employed to approximate the integral of f (which we assume,
for simplicity, to depend only on t), we get
tn+1
>
tn
f(œÑ) dœÑ ‚âÉh
s

j=1
bjf(tn + cjh)
having denoted by bj the weights and by tn + cjh the quadrature nodes. It
can be proved (see [But64]) that for any RK formula (11.70)-(11.71), there
exists a correspondence between the coeÔ¨Écients bj, cj of the formula and
the weights and nodes of a Gauss quadrature rule.
In particular, the coeÔ¨Écients c1, . . . , cs are the roots of the Legendre
polynomial Ls in the variable x = 2c ‚àí1, so that x ‚àà[‚àí1, 1]. Once the

11.8 Runge-Kutta (RK) Methods
515
s coeÔ¨Écients cj have been found, we can construct RK methods of order
2s, by determining the coeÔ¨Écients aij and bj as being the solutions of the
linear systems
s

j=1
ck‚àí1
j
aij = (1/k)ck
i ,
k = 1, 2, . . . , s,
s

j=1
ck‚àí1
j
bj = 1/k,
k = 1, 2, . . . , s.
The following families can be derived:
1. Gauss-Legendre RK methods, if Gauss-Legendre quadrature nodes are
used. These methods, for a Ô¨Åxed number of stages s, attain the maximum
possible order 2s. Remarkable examples are the one-stage method (implicit
midpoint method) of order 2
un+1 = un + hf

tn + 1
2h, 1
2(un + un+1)

,
1
2
1
2
1
and the 2-stage method of order 4, described by the following Butcher array
3‚àí
‚àö
3
6
1
4
3‚àí2
‚àö
3
12
3+
‚àö
3
6
3+2
‚àö
3
12
1
4
1
2
1
2
2. Gauss-Radau methods, which are characterized by the fact that the
quadrature nodes include one of the two endpoints of the interval (tn, tn+1).
The maximum order that can be achieved by these methods is 2s‚àí1, when
s stages are used. Elementary examples correspond to the following Butcher
arrays
0
1
1 ,
1
1
1 ,
1
3
5
12
‚àí1
12
1
3
4
1
4
3
4
1
4
and have order 1, 1 and 3, respectively. The Butcher array in the middle
represents the backward Euler method.
3. Gauss-Lobatto methods, where both the endpoints tn and tn+1 are quadra-
ture nodes. The maximum order that can be achieved using s stages is
2s ‚àí2. We recall the methods of the family corresponding to the following

516
11. Numerical Solution of Ordinary DiÔ¨Äerential Equations
Butcher arrays
0
0
0
1
1
2
1
2
1
2
1
2
,
0
1
2
0
1
1
2
0
1
2
1
2
,
0
1
6
‚àí1
3
1
6
1
2
1
6
5
12
‚àí1
12
1
1
6
2
3
1
6
1
6
2
3
1
6
which have order 2, 2 and 3, respectively. The Ô¨Årst array represents the
Crank-Nicolson method.
As for semi-implicit RK methods, we limit ourselves to mentioning the
case of DIRK methods (diagonally implicit RK), which, for s = 3, are
represented by the following Butcher array
1+¬µ
2
1+¬µ
2
0
0
1
2
‚àí¬µ
2
1+¬µ
2
0
1‚àí¬µ
2
1 + ¬µ
‚àí1 ‚àí2¬µ
1+¬µ
2
1
6¬µ2
1 ‚àí
1
3¬µ2
1
6¬µ2
The parameter ¬µ represents one of the three roots of 3¬µ3 ‚àí3¬µ‚àí1 = 0 (i.e.,
(2/
‚àö
3) cos(10o), ‚àí(2/
‚àö
3) cos(50o), ‚àí(2/
‚àö
3) cos(70o)). The maximum or-
der that has been determined in the literature for these methods is 4.
11.8.4
Regions of Absolute Stability for RK Methods
Applying an s-stage RK method to the model problem (11.24) yields
Ki = un + hŒª
s

i=1
aijKj,
un+1 = un + hŒª
s

i=1
biKi,
(11.77)
that is, a Ô¨Årst-order diÔ¨Äerence equation. If K and 1 are the vectors of com-
ponents (K1, . . . , Ks)T and (1, . . . , 1)T , respectively, then (11.77) becomes
K = un1 + hŒªAK,
un+1 = un + hŒªbT K,
from which, K = (I ‚àíhŒªA)‚àí11un and thus
un+1 =
0
1 + hŒªbT (I ‚àíhŒªA)‚àí11
1
un = R(hŒª)un
where R(hŒª) is the so-called stability function.
The RK method is absolutely stable, i.e., the sequence {un} satisÔ¨Åes
(11.25), iÔ¨Ä|R(hŒª)| < 1. Its region of absolute stability is given by
A = {z = hŒª ‚ààC such that |R(hŒª)| < 1} .

11.9 Systems of ODEs
517
If the method is explicit, A is strictly lower triangular and the function R
can be written in the following form (see [DV84])
R(hŒª) = det(I ‚àíhŒªA + hŒª1bT )
det(I ‚àíhŒªA)
.
Thus since det(I‚àíhŒªA) = 1, R(hŒª) is a polynomial function in the variable
hŒª, |R(hŒª)| can never be less than 1 for all values of hŒª. Consequently, A
can never be unbounded for an explicit RK method.
In the special case of an explicit RK of order s = 1, . . . , 4, one gets (see
[Lam91])
R(hŒª) =
s

k=0
1
k!(hŒª)k.
The corresponding regions of absolute stability are drawn in Figure 11.10.
Notice that, unlike MS methods, the regions of absolute stability of RK
methods increase in size as the order grows.
‚àí4
‚àí3
‚àí2
‚àí1
0
1
2
0
0.5
1
1.5
2
2.5
3
3.5
4
s=1
s=2
s=4
s=3
FIGURE 11.10. Regions of absolute stability for s-stage explicit RK methods,
with s = 1, . . . , 4. The plot only shows the portion Im(hŒª) ‚â•0 since the regions
are symmetric about the real axis
We Ô¨Ånally notice that the regions of absolute stability for explicit RK meth-
ods can fail to be connected; an example is given in Exercise 14.
11.9
Systems of ODEs
Let us consider the system of Ô¨Årst-order ODEs
y‚Ä≤ = F(t, y),
(11.78)
where F : R √ó Rn ‚ÜíRn is a given vector function and y ‚ààRn is the
solution vector which depends on n arbitrary constants set by the n initial

518
11. Numerical Solution of Ordinary DiÔ¨Äerential Equations
conditions
y(t0) = y0.
(11.79)
Let us recall the following property (see [PS91], p. 209).
Property 11.5 Let F : R √ó Rn ‚ÜíRn be a continuous function on D =
[t0, T] √ó Rn, with t0 and T Ô¨Ånite. Then, if there exists a positive constant
L such that
‚à•F(t, y) ‚àíF(t, ¬Øy)‚à•‚â§L‚à•y ‚àí¬Øy‚à•
(11.80)
holds for any (t, y) and (t, ¬Øy) ‚ààD, then, for any y0 ‚ààRn there exists a
unique y, continuous and diÔ¨Äerentiable with respect to t for any (t, y) ‚ààD,
which is a solution of the Cauchy problem (11.78)-(11.79).
Condition (11.80) expresses the fact that F is Lipschitz continuous with
respect to the second argument.
It is seldom possible to write out in closed form the solution to system
(11.78). A special case is where the system takes the form
y‚Ä≤(t) = Ay(t),
(11.81)
with A‚ààRn√ón. Assume that A has n distinct eigenvalues Œªj, j = 1, . . . , n;
therefore, the solution y can be written as
y(t) =
n

j=1
CjeŒªjtvj,
(11.82)
where C1, . . . , Cn are some constants and {vj} is a basis formed by the
eigenvectors of A, associated with the eigenvalues Œªj for j = 1, . . . , n. The
solution is determined by setting n initial conditions.
From the numerical standpoint, the methods introduced in the scalar
case can be extended to systems. A delicate matter is how to generalize the
theory developed about absolute stability.
With this aim, let us consider system (11.81). As previously seen, the
property of absolute stability is concerned with the behavior of the numer-
ical solution as t grows to inÔ¨Ånity, in the case where the solution of problem
(11.78) satisÔ¨Åes
‚à•y(t)‚à•‚Üí0
as t ‚Üí‚àû.
(11.83)
Condition (11.83) is satisÔ¨Åed if all the real parts of the eigenvalues of A are
negative since this ensures that
eŒªjt = eReŒªjt(cos(ImŒªj) + i sin(ImŒªi)) ‚Üí0,
as t ‚Üí‚àû,
(11.84)

11.10 StiÔ¨ÄProblems
519
from which (11.83) follows recalling (11.82). Since A has n distinct eigen-
values, there exists a nonsingular matrix Q such that Œõ = Q‚àí1AQ, Œõ being
the diagonal matrix whose entries are the eigenvalues of A (see Section 1.8).
Introducing the auxiliary variable z = Q‚àí1y, the initial system can there-
fore be transformed into
z‚Ä≤ = Œõz.
(11.85)
Since Œõ is a diagonal matrix, the results holding in the scalar case immedi-
ately apply to the vector case as well, provided that the analysis is repeated
on all the (scalar) equations of system (11.85).
11.10
StiÔ¨ÄProblems
Consider a nonhomogeneous linear system of ODEs with constant coeÔ¨É-
cients
y‚Ä≤(t) = Ay(t) + œï(t),
with A ‚ààRn√ón,
œï(t) ‚ààRn,
and assume that A has n distinct eigenvalues Œªj, j = 1, . . . , n. Then
y(t) =
n

j=1
CjeŒªjtvj + œà(t)
where C1, . . . , Cn, are n constants, {vj} is a basis formed by the eigenvec-
tors of A and œà(t) is a particular solution of the ODE at hand. Throughout
the section, we assume that ReŒªj < 0 for all j.
As t ‚Üí‚àû, the solution y tends to the particular solution œà. We can
therefore interpret œà as the steady-state solution (that is, after an inÔ¨Ånite
time) and
n

j=1
CjeŒªjt as being the transient solution (that is, for t Ô¨Ånite).
Assume that we are interested only in the steady-state. If we employ a
numerical scheme with a bounded region of absolute stability, the stepsize h
is subject to a constraint that depends on the maximum module eigenvalue
of A. On the other hand, the greater this module, the shorter the time
interval where the corresponding component in the solution is meaningful.
We are thus faced with a sort of paradox: the scheme is forced to employ
a small integration stepsize to track a component of the solution that is
virtually Ô¨Çat for large values of t.
Precisely, if we assume that
œÉ ‚â§ReŒªj ‚â§œÑ < 0,
‚àÄj = 1, . . . , n
(11.86)
and introduce the stiÔ¨Äness quotient rs = œÉ/œÑ, we say that a linear system
of ODEs with constant coeÔ¨Écients is stiÔ¨Äif the eigenvalues of matrix A all
have negative real parts and rs ‚â´1.

520
11. Numerical Solution of Ordinary DiÔ¨Äerential Equations
However, referring only to the spectrum of A to characterize the stiÔ¨Äness of
a problem might have some drawbacks. For instance, when œÑ ‚âÉ0, the stiÔ¨Ä-
ness quotient can be very large while the problem appears to be ‚Äúgenuinely‚Äù
stiÔ¨Äonly if |œÉ| is very large. Moreover, enforcing suitable initial conditions
can aÔ¨Äect the stiÔ¨Äness of the problem (for example, selecting the data in
such a way that the constants multiplying the ‚ÄústiÔ¨Ä‚Äù components of the
solution vanish).
For this reason, several authors Ô¨Ånd the previous deÔ¨Ånition of a stiÔ¨Ä
problem unsatisfactory, and, on the other hand, they agree on the fact that
it is not possible to exactly state what it is meant by a stiÔ¨Äproblem. We
limit ourselves to quoting only one alternative deÔ¨Ånition, which is of some
interest since it focuses on what is observed in practice to be a stiÔ¨Äproblem.
DeÔ¨Ånition 11.14 (from [Lam91], p. 220) A system of ODEs is stiÔ¨Äif,
when approximated by a numerical scheme characterized by a region of
absolute stability with Ô¨Ånite size, it forces the method, for any initial con-
dition for which the problem admits a solution, to employ a discretization
stepsize excessively small with respect to the smoothness of the exact so-
lution.
‚ñ†
From this deÔ¨Ånition, it is clear that no conditionally absolute stable method
is suitable for approximating a stiÔ¨Äproblem. This prompts resorting to
implicit methods, such as MS or RK, which are more expensive than explicit
schemes, but have regions of absolute stability of inÔ¨Ånite size. However, it
is worth recalling that, for nonlinear problems, implicit methods lead to
nonlinear equations, for which it is thus crucial to select iterative numerical
methods free of limitations on h for convergence.
For instance, in the case of MS methods, we have seen that using Ô¨Åxed-
point iterations sets the constraint (11.68) on h in terms of the Lipschitz
constant L of f. In the case of a linear system this constraint is
L ‚â•
max
i=1,... ,n|Œªi|,
so that (11.68) would imply a strong limitation on h (which could even
be more stringent than those required for an explicit scheme to be stable).
One way of circumventing this drawback consists of resorting to Newton‚Äôs
method or some variants. The presence of Dahlquist barriers imposes a
strong limitation on the use of MS methods, the only exception being BDF
methods, which, as already seen, are Œ∏-stable for p ‚â§5 (for a larger number
of steps they are even not zero-stable). The situation becomes deÔ¨Ånitely
more favorable if implicit RK methods are considered, as observed at the
end of Section 11.8.4.
The theory developed so far holds rigorously if the system is linear. In
the nonlinear case, let us consider the Cauchy problem (11.78), where the

11.11 Applications
521
function F : R √ó Rn ‚ÜíRn is assumed to be diÔ¨Äerentiable. To study its
stability a possible strategy consists of linearizing the system as
y‚Ä≤(t) = F(œÑ, y(œÑ)) + JF(œÑ, y(œÑ)) [y(t) ‚àíy(œÑ)] ,
in a neighborhood (œÑ, y(œÑ)), where œÑ is an arbitrarily chosen value of t
within the time integration interval.
The above technique might be dangerous since the eigenvalues of JF do
not suÔ¨Éce in general to describe the behavior of the exact solution of the
original problem. Actually, some counterexamples can be found where:
1. JF has complex conjugate eigenvalues, while the solution of (11.78)
does not exhibit oscillatory behavior;
2. JF has real nonnegative eigenvalues, while the solution of (11.78) does
not grow monotonically with t;
3. JF has eigenvalues with negative real parts, but the solution of (11.78)
does not decay monotonically with t.
As an example of the case at item 3. let us consider the system of
ODEs
y‚Ä≤ =
Ô£Æ
Ô£ØÔ£∞
‚àí1
2t
2
t3
‚àít
2
‚àí1
2t
Ô£π
Ô£∫Ô£ªy
=
A(t)y.
For t ‚â•1 its solution is
y(t) = C1
 t‚àí3/2
‚àí1
2t1/2

+ C2
 2t‚àí3/2 log t
t1/2(1 ‚àílog t)

whose Euclidean norm diverges monotonically for t > (12)1/4 ‚âÉ1.86
when C1 = 1, C2 = 0, whilst the eigenvalues of A(t), equal to (‚àí1 ¬±
2i)/(2t), have negative real parts.
Therefore, the nonlinear case must be tackled using ad hoc techniques, by
suitably reformulating the concept of stability itself (see [Lam91], Chapter
7).
11.11
Applications
We consider two examples of dynamical systems that are well-suited to
checking the performances of several numerical methods introduced in the
previous sections.

522
11. Numerical Solution of Ordinary DiÔ¨Äerential Equations
11.11.1
Analysis of the Motion of a Frictionless Pendulum
Let us consider the frictionless pendulum in Figure 11.11 (left), whose
motion is governed by the following system of ODEs
" y‚Ä≤
1 =
y2,
y‚Ä≤
2 =
‚àíK sin(y1),
(11.87)
for t > 0, where y1(t) and y2(t) represent the position and angular velocity
of the pendulum at time t, respectively, while K is a positive constant
depending on the geometrical-mechanical parameters of the pendulum. We
consider the initial conditions: y1(0) = Œ∏0, y2(0) = 0.
y
weight
1
A
A‚Äô
‚àíœÄ K
1/2
œÄ K
1/2
FIGURE 11.11. Left: frictionless pendulum; right: orbits of system (11.87) in the
phase space
Denoting by y = (y1, y2)T the solution to system (11.87), this admits
inÔ¨Ånitely many equilibrium conditions of the form y = (nœÄ, 0)T for n ‚ààZ,
corresponding to the situations where the pendulum is vertical with zero
velocity. For n even, the equilibrium is stable, while for n odd it is unstable.
These conclusions can be drawn by analyzing the linearized system
y‚Ä≤ = Aey =
.
0
1
‚àíK
0
/
y,
y‚Ä≤ = Aoy =
.
0
1
K
0
/
y.
If n is even, matrix Ae has complex conjugate eigenvalues Œª1,2 = ¬±i
‚àö
K
and associated eigenvectors y1,2 = (‚àìi/
‚àö
K, 1)T , while for n odd, Ao
has real and opposite eigenvalues Œª1,2 = ¬±
‚àö
K and eigenvectors y1,2 =
(1/
‚àö
K, ‚àì1)T .
Let us consider two diÔ¨Äerent sets of initial data: y(0) = (Œ∏0, 0)T and
y(0) = (œÄ + Œ∏0, 0)T , where |Œ∏0| ‚â™1. The solutions of the corresponding
linearized system are, respectively,
"
y1(t) =
Œ∏0 cos(
‚àö
Kt)
y2(t) =
‚àí
‚àö
KŒ∏0 sin(
‚àö
Kt)
,
"
y1(t) =
(œÄ + Œ∏0) cosh(
‚àö
Kt)
y2(t) =
‚àö
K(œÄ + Œ∏0) sinh(
‚àö
Kt),

11.11 Applications
523
and will be henceforth denoted as ‚Äústable‚Äù and ‚Äúunstable‚Äù, respectively, for
reasons that will be clear later on. To these solutions we associate in the
plane (y1, y2), called the phase space, the following orbits (i.e., the graphs
obtained plotting the curve (y1(t), y2(t)) in the phase space).
y1
Œ∏0
2
+

y2
‚àö
KŒ∏0
2
= 1,
(stable case)

y1
œÄ + Œ∏0
2
‚àí

y2
‚àö
K(œÄ + Œ∏0)
2
= 1,
(unstable case).
In the stable case, the orbits are ellipses with period 2œÄ/
‚àö
K and are cen-
tered at (0, 0)T , while in the unstable case they are hyperbolae centered at
(0, 0)T and asymptotic to the straight lines y2 = ¬±
‚àö
Ky1.
The complete picture of the motion of the pendulum in the phase space
is shown in Figure 11.11 (right). Notice that, letting v = |y2| and Ô¨Åxing
the initial position y1(0) = 0, there exists a limit value vL = 2
‚àö
K which
corresponds in the Ô¨Ågure to the points A and A‚Äô. For v(0) < vL, the orbits
are closed, while for v(0) > vL they are open, corresponding to a continuous
revolution of the pendulum, with inÔ¨Ånite passages (with periodic and non
null velocity) through the two equilibrium positions y1 = 0 and y1 = œÄ.
The limit case v(0) = vL yields a solution such that, thanks to the total
energy conservation principle, y2 = 0 when y1 = œÄ. Actually, these two
values are attained asymptotically only as t ‚Üí‚àû.
The Ô¨Årst-order nonlinear diÔ¨Äerential system (11.87) has been numerically
solved using the forward Euler method (FE), the midpoint method (MP)
and the Adams-Bashforth second-order scheme (AB). In Figure 11.12 we
show the orbits in the phase space that have been computed by the two
methods on the time interval (0, 30) and taking K = 1 and h = 0.1. The
crosses denote initial conditions.
As can be noticed, the orbits generated by FE do not close. This kind
of instability is due to the fact that the region of absolute stability of the
FE method completely excludes the imaginary axis. On the contrary, the
MP method describes accurately the closed system orbits due to the fact
that its region of asymptotic stability (see Section 11.6.4) includes pure
imaginary eigenvalues in the neighborhood of the origin of the complex
plane. It must also be noticed that the MP scheme gives rise to oscillating
solutions as v0 gets larger. The second-order AB method, instead, describes
correctly all kinds of orbits.
11.11.2
Compliance of Arterial Walls
An arterial wall subject to blood Ô¨Çow can be modelled by a compliant
circular cylinder of length L and radius R0 with walls made by an incom-
pressible, homogeneous, isotropic, elastic tissue of thickness H. A simple

524
11. Numerical Solution of Ordinary DiÔ¨Äerential Equations
‚àí10
‚àí5
0
5
10
‚àí2
0
2
‚àí10
‚àí5
0
5
10
‚àí2
0
2
‚àí10
‚àí5
0
5
10
‚àí2
0
2
FIGURE 11.12. Orbits for system (11.87) in the case K = 1 and h = 0.1, com-
puted using the FE method (upper plot), the MP method (central plot) and the
AB method (lower plot), respectively. The initial conditions are Œ∏0 = œÄ/10 and
v0 = 0 (thin solid line), v0 = 1 (dashed line), v0 = 2 (dash-dotted line) and
v0 = ‚àí2 (thick solid line)
model describing the mechanical behavior of the walls interacting with the
blood Ô¨Çow is the so called ‚Äúindependent-rings‚Äù model according to which
the vessel wall is regarded as an assembly of rings which are not inÔ¨Çuenced
one by the others.
This amounts to neglecting the longitudinal (or axial) inner actions along
the vessel, and to assuming that the walls can deform only in the radial
direction. Thus, the vessel radius R is given by R(t) = R0 +y(t), where y is
the radial deformation of the ring with respect to a reference radius R0 and
t is the time variable. The application of Newton‚Äôs law to the independent-
ring system yields the following equation modeling the time mechanical

11.11 Applications
525
behavior of the wall
y‚Ä≤‚Ä≤(t) + Œ≤y‚Ä≤(t) + Œ±y(t) = Œ≥(p(t) ‚àíp0)
(11.88)
where Œ± = E/(œÅwR2
0), Œ≥ = 1/(œÅwH) and Œ≤ is a positive constant. The
physical parameters œÅw and E denote the vascular wall density and the
Young modulus of the vascular tissue, respectively. The function p ‚àíp0
is the forcing term acting on the wall due to the pressure drop between
the inner part of the vessel (where the blood Ô¨Çows) and its outer part
(surrounding organs). At rest, if p = p0, the vessel conÔ¨Åguration coincides
with the undeformed circular cylinder having radius equal exactly to R0
(y = 0).
Equation (11.88) can be formulated as y‚Ä≤(t) = Ay(t) + b(t) where y =
(y, y‚Ä≤)T , b = (0, ‚àíŒ≥(p ‚àíp0))T and
A =

0
1
‚àíŒ±
‚àíŒ≤

.
(11.89)
The eigenvalues of A are Œª¬± = (‚àíŒ≤ ¬±

Œ≤2 ‚àí4Œ±)/2; therefore, if Œ≤ ‚â•2‚àöŒ±
both the eigenvalues are real and negative and the system is asymptotically
stable with y(t) decaying exponentially to zero as t ‚Üí‚àû. Conversely, if 0 <
Œ≤ < 2‚àöŒ± the eigenvalues are complex conjugate and damped oscillations
arise in the solution which again decays exponentially to zero as t ‚Üí‚àû.
Numerical approximations have been carried out using both the backward
Euler (BE) and Crank-Nicolson (CN) methods. We have set y(t) = 0
and used the following (physiological) values of the physical parameters:
L = 5 ¬∑ 10‚àí2[m], R0 = 5 ¬∑ 10‚àí3[m], œÅw = 103[Kgm‚àí3], H = 3 ¬∑ 10‚àí4[m] and
E = 9 ¬∑ 105[Nm‚àí2], from which Œ≥ ‚âÉ3.3[Kg‚àí1m‚àí2] and Œ± = 36 ¬∑ 106[s‚àí2].
A sinusoidal function p ‚àíp0 = x‚àÜp(a + b cos(œâ0t)) has been used to model
the pressure variation along the vessel direction x and time, where ‚àÜp =
0.25 ¬∑ 133.32 [Nm‚àí2], a = 10 ¬∑ 133.32 [Nm‚àí2], b = 133.32 [Nm‚àí2] and the
pulsation œâ0 = 2œÄ/0.8 [rad s‚àí1] corresponds to a heart beat.
The results reported below refer to the ring coordinate x = L/2. The
two (very diÔ¨Äerent) cases (1) Œ≤ = ‚àöŒ± [s‚àí1] and (2) Œ≤ = Œ± [s‚àí1] have been
analyzed; it is easily seen that in case (2) the stiÔ¨Äness quotient (see Section
11.10) is almost equal to Œ±, thus the problem is highly stiÔ¨Ä. We notice also
that in both cases the real parts of the eigenvalues of A are very large,
so that an appropriately small time step should be taken to accurately
describe the fast transient of the problem.
In case (1) the diÔ¨Äerential system has been studied on the time interval
[0, 2.5¬∑10‚àí3] with a time step h = 10‚àí4. We notice that the two eigenvalues
of A have modules equal to 6000, thus our choice of h is compatible with
the use of an explicit method as well.
Figure 11.13 (left) shows the numerical solutions as functions of time.
The solid (thin) line is the exact solution while the thick dashed and solid

526
11. Numerical Solution of Ordinary DiÔ¨Äerential Equations
lines are the solutions given by the CN and BE methods, respectively. A far
better accuracy of the CN method over the BE is clearly demonstrated; this
is conÔ¨Årmed by the plot in Figure 11.13 (right) which shows the trajectories
of the computed solutions in the phase space. In this case the diÔ¨Äerential
system has been integrated on the time interval [0, 0.25] with a time step
h = 2.5 ¬∑ 10‚àí4. The dashed line is the trajectory of the CN method while
the solid line is the corresponding one obtained using the BE scheme. A
strong dissipation is clearly introduced by the BE method with respect to
the CN scheme; the plot also shows that both methods converge to a limit
cycle which corresponds to the cosine component of the forcing term.
0
0.5
1
1.5
2
2.5
x 10‚àí3
‚àí2
0
2
4
6
8
10
12
14x 10‚àí5
0
0.5
1
1.5
x 10‚àí4
‚àí0.1
0
0.1
0.2
0.3
0.4
0.5
0.6
FIGURE 11.13. Transient simulation (left) and phase space trajectories (right)
In the second case (2) the diÔ¨Äerential system has been integrated on the
time interval [0, 10] with a time step h = 0.1. The stiÔ¨Äness of the problem is
demonstrated by the plot of the deformation velocities z shown in Figure
11.14 (left). The solid line is the solution computed by the BE method
while the dashed line is the corresponding one given by the CN scheme; for
the sake of graphical clarity, only one third of the nodal values have been
plotted for the CN method. Strong oscillations arise since the eigenvalues of
matrix A are Œª1 = ‚àí1, Œª2 = ‚àí36¬∑106 so that the CN method approximates
the Ô¨Årst component y of the solution y as
yCN
k
=
1 + (hŒª1)/2
1 ‚àí(hŒª1)/2
k
‚âÉ(0.9048)k,
k ‚â•0,
which is clearly stable, while the approximate second component z(= y‚Ä≤) is
zCN
k
=
1 + (hŒª2)/2
1 ‚àí(hŒª2)/2
k
‚âÉ(‚àí0.9999)k,
k ‚â•0
which is obviously oscillating. On the contrary, the BE method yields
yBE
k
=

1
1 ‚àíhŒª1
k
‚âÉ(0.9090)k,
k ‚â•0,

11.12 Exercises
527
and
zCN
k
=

1
1 ‚àíhŒª2
k
‚âÉ(0.2777)k,
k ‚â•0
which are both stable for every h > 0. According to these conclusions the
Ô¨Årst component y of the vector solution y is correctly approximated by
both the methods as can be seen in Figure 11.14 (right) where the solid
line refers to the BE scheme while the dashed line is the solution computed
by the CN method.
0
1
2
3
4
5
6
7
8
9
10
‚àí1.5
‚àí1
‚àí0.5
0
0.5
1
1.5
2 x 10
‚àí4
0
1
2
3
4
5
6
7
8
9
10
0
0.2
0.4
0.6
0.8
1
1.2 x 10
‚àí4
FIGURE 11.14. Long-time behavior of the solution: velocities (left) and displace-
ments (right)
11.12
Exercises
1. Prove that Heun‚Äôs method has order 2 with respect to h.
[Suggerimento : notice that hœÑn+1 = yn+1 ‚àíyn ‚àíhŒ¶(tn, yn; h) = E1 + E2,
where E1 =
2 tn+1
tn
f(s, y(s))ds ‚àíh
2 [f(tn, yn) + f(tn+1, yn+1)]
3
and E2 =
h
2 {[f(tn+1, yn+1) ‚àíf(tn+1, yn + hf(tn, yn))]}, where E1 is the error due to
numerical integration with the trapezoidal method and E2 can be bounded
by the error due to using the forward Euler method.]
2. Prove that the Crank-Nicoloson method has order 2 with respect to h.
[Solution : using (9.12) we get, for a suitable Œæn in (tn, tn+1)
yn+1 = yn + h
2 [f(tn, yn) + f(tn+1, yn+1)] ‚àíh3
12f ‚Ä≤‚Ä≤(Œæn, y(Œæn))
or, equivalently,
yn+1 ‚àíyn
h
= 1
2 [f(tn, yn) + f(tn+1, yn+1)] ‚àíh2
12f ‚Ä≤‚Ä≤(Œæn, y(Œæn)).
(11.90)
Therefore, relation (11.9) coincides with (11.90) up to an inÔ¨Ånitesimal of
order 2 with respect to h, provided that f ‚ààC2(I).]

528
11. Numerical Solution of Ordinary DiÔ¨Äerential Equations
3. Solve the diÔ¨Äerence equation un+4 ‚àí6un+3 + 14un+2 ‚àí16un+1 + 8un = n
subject to the initial conditions u0 = 1, u1 = 2, u2 = 3 and u3 = 4.
[Solution : un = 2n(n/4 ‚àí1) + 2(n‚àí2)/2 sin(œÄ/4) + n + 2.]
4. Prove that if the characteristic polynomial Œ† deÔ¨Åned in (11.30) has simple
roots, then any solution of the associated diÔ¨Äerence equation can be written
in the form (11.32).
[Hint : notice that a generic solution un+k is completely determined by
the initial values u0, . . . , uk‚àí1. Moreover, if the roots ri of Œ† are distinct,
there exist unique k coeÔ¨Écients Œ±i such that Œ±1rj
1 + . . . + Œ±krj
k = uj with
j = 0, . . . , k ‚àí1 . . . ]
5. Prove that if the characteristic polynomial Œ† has simple roots, the matrix
R deÔ¨Åned in (11.37) is not singular.
[Hint: it coincides with the transpose of the Vandermonde matrix where
xj
i is replaced by ri
j (see Exercise 2, Chapter 8).]
6. The Legendre polynomials Li satisfy the diÔ¨Äerence equation
(n + 1)Ln+1(x) ‚àí(2n + 1)xLn(x) + nLn‚àí1(x) = 0
with L0(x) = 1 and L1(x) = x (see Section 10.1.2). DeÔ¨Åning the generating
function F(z, x) = ‚àû
n=0 Pn(x)zn, prove that F(z, x) = (1‚àí2zx+z2)‚àí1/2.
7. Prove that the gamma function
Œì(z) =
‚àû
>
0
e‚àíttz‚àí1dt,
z ‚ààC,
Rez > 0
is the solution of the diÔ¨Äerence equation Œì(z + 1) = zŒì(z)
[Hint : integrate by parts.]
8. Study, as functions of Œ± ‚ààR, stability and order of the family of linear
multistep methods
un+1 = Œ±un + (1 ‚àíŒ±)un‚àí1 + 2hfn + hŒ±
2 [fn‚àí1 ‚àí3fn] .
9. Consider the following family of linear multistep methods depending on
the real parameter Œ±
un+1 = un + h[(1 ‚àíŒ±
2 )f(xn, un) + Œ±
2 f(xn+1, un+1)].
Study their consistency as a function of Œ±; then, take Œ± = 1 and use the
corresponding method to solve the Cauchy problem
y‚Ä≤(x) = ‚àí10y(x),
x > 0,
y(0) = 1.
Determine the values of h in correspondance of which the method is abso-
lutely stable.
[Solution : the only consistent method of the family is the Crank-Nicolson
method (Œ± = 1).]

11.12 Exercises
529
10. Consider the family of linear multistep methods
un+1 = Œ±un + h
2 (2(1 ‚àíŒ±)fn+1 + 3Œ±fn ‚àíŒ±fn‚àí1)
where Œ± is a real parameter.
(a) Analyze consistency and order of the methods as functions of Œ±, de-
termining the value Œ±‚àófor which the resulting method has maximal
order.
(b) Study the zero-stability of the method with Œ± = Œ±‚àó, write its charac-
teristic polynomial Œ†(r; hŒª) and, using MATLAB, draw its region of
absolute stability in the complex plane.
11. Adams methods can be easily generalized, integrating between tn‚àír and
tn+1 with r ‚â•1. Show that, by doing so, we get methods of the form
un+1 = un‚àír + h
p

j=‚àí1
bjfn‚àíj
and prove that for r = 1 the midpoint method introduced in (11.43) is
recovered (the methods of this family are called Nystron methods.)
12. Check that Heun‚Äôs method (11.10) is an explicit two-stage RK method and
write the Butcher arrays of the method. Then, do the same for the modiÔ¨Åed
Euler method, given by
un+1 = un + hf(tn + h
2 , un + h
2 fn),
n ‚â•0.
(11.91)
[Solution : the methods have the following Butcher arrays
0
0
0
1
1
0
2 1
2
2
1
2
0
0
0
2
3
1
2 2
3
1
2
0
0
1
.]
13. Check that the Butcher array for method (11.73) is given by
0
0
0
0
0
1
2
2 1
2
2
0
0
0
1
2
0
2 1
2
2
0
0
1
0
0
1
0
1
6
2 1
3
2
1
3
1
6
14. Write a MATLAB program to draw the regions of absolute stability for a
RK method, for which the function R(hŒª) is available. Check the code in
the special case of
R(hŒª) = 1 + hŒª + (hŒª)2/2 + (hŒª)3/6 + (hŒª)4/24 + (hŒª)5/120 + (hŒª)6/600
and verify that such a region is not connected.

530
11. Numerical Solution of Ordinary DiÔ¨Äerential Equations
15. Determine the function R(hŒª) associated with the Merson method, whose
Butcher array is
0
0
0
0
0
0
1
3
1
3
0
0
0
0
1
3
1
6
1
6
0
0
0
1
2
1
8
0
3
8
0
0
1
1
2
0
‚àí3
2
2
0
1
6
0
0
2
3
1
6
[Solution : one gets R(hŒª) = 1 + 4
i=1(hŒª)i/i! + (hŒª)5/144.]

12
Two-Point Boundary Value Problems
This chapter is devoted to the analysis of approximation methods for two-
point boundary value problems for diÔ¨Äerential equations of elliptic type.
Finite diÔ¨Äerences, Ô¨Ånite elements and spectral methods will be considered.
A short account is also given on the extension to elliptic boundary value
problems in two-dimensional regions.
12.1
A Model Problem
To start with, let us consider the two-point boundary value problem
‚àíu‚Ä≤‚Ä≤(x) = f(x),
0 < x < 1,
(12.1)
u(0) = u(1) = 0.
(12.2)
From the fundamental theorem of calculus, if u ‚ààC2([0, 1]) and satisÔ¨Åes
the diÔ¨Äerential equation (12.1) then
u(x) = c1 + c2x ‚àí
x
>
0
F(s) ds
where c1 and c2 are arbitrary constants and F(s) =
 s
0 f(t) dt. Using
integration by parts one has
x
>
0
F(s) ds = [sF(s)]x
0 ‚àí
x
>
0
sF ‚Ä≤(s) ds =
x
>
0
(x ‚àís)f(s) ds.

532
12. Two-Point Boundary Value Problems
The constants c1 and c2 can be determined by enforcing the boundary
conditions. The condition u(0) = 0 implies that c1 = 0, and then u(1) = 0
yields c2 =
 1
0 (1 ‚àís)f(s) ds. Consequently, the solution of (12.1)-(12.2)
can be written in the following form
u(x) = x
1
>
0
(1 ‚àís)f(s) ds ‚àí
x
>
0
(x ‚àís)f(s) ds
or, more compactly,
u(x) =
1
>
0
G(x, s)f(s) ds,
(12.3)
where, for any Ô¨Åxed x, we have deÔ¨Åned
G(x, s) =
" s(1 ‚àíx)
if 0 ‚â§s ‚â§x,
x(1 ‚àís)
if x ‚â§s ‚â§1.
(12.4)
The function G is called Green‚Äôs function for the boundary value problem
(12.1)-(12.2). It is a piecewise linear function of x for Ô¨Åxed s, and vice versa.
It is continuous, symmetric (i.e., G(x, s) = G(s, x) for all x, s ‚àà[0, 1]), non
negative, null if x or s are equal to 0 or 1, and
 1
0 G(x, s) ds = 1
2x(1 ‚àíx).
The function is plotted in Figure 12.1.
We can therefore conclude that for every f ‚ààC0([0, 1]) there is a unique
solution u ‚ààC2([0, 1]) of the boundary value problem (12.1)-(12.2) which
admits the representation (12.3). Further smoothness of u can be derived
by (12.1); indeed, if f ‚ààCm([0, 1]) for some m ‚â•0 then u ‚ààCm+2([0, 1]).
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.05
0.1
0.15
0.2
0.25
FIGURE 12.1. Green‚Äôs function for three diÔ¨Äerent values of x: x = 1/4 (solid
line), x = 1/2 (dashed line), x = 3/4 (dash-dotted line)

12.2 Finite DiÔ¨Äerence Approximation
533
An interesting property of the solution u is that if f ‚ààC0([0, 1]) is a
nonnegative function, then u is also nonnegative. This is referred to as the
monotonicity property, and follows directly from (12.3), since G(x, s) ‚â•0
for all x, s ‚àà[0, 1]. The next property is called the maximum principle and
states that if f ‚ààC0(0, 1),
‚à•u‚à•‚àû‚â§1
8‚à•f‚à•‚àû
(12.5)
where ‚à•u‚à•‚àû= max
0‚â§x‚â§1|u(x)| is the maximum norm. Indeed, since G is non-
negative,
|u(x)| ‚â§
1
>
0
G(x, s)|f(s)| ds ‚â§‚à•f‚à•‚àû
1
>
0
G(x, s) ds = 1
2x(1 ‚àíx)‚à•f‚à•‚àû
from which the inequality (12.5) follows.
12.2
Finite DiÔ¨Äerence Approximation
We introduce on [0, 1] the grid points {xj}n
j=0 given by xj = jh where
n ‚â•2 is an integer and h = 1/n is the grid spacing. The approximation to
the solution u is a Ô¨Ånite sequence {uj}n
j=0 deÔ¨Åned only at the grid points
(with the understanding that uj approximates u(xj)) by requiring that
‚àíuj+1 ‚àí2uj + uj‚àí1
h2
= f(xj),
for j = 1, . . . , n ‚àí1
(12.6)
and u0 = un = 0. This corresponds to having replaced u‚Ä≤‚Ä≤(xj) by its second
order centred Ô¨Ånite diÔ¨Äerence (10.65) (see Section 10.10.1).
If we set u = (u1, . . . , un‚àí1)T and f = (f1, . . . , fn‚àí1)T , with fi = f(xi),
it is a simple matter to see that (12.6) can be written in the more compact
form
Afdu = f,
(12.7)
where Afd is the symmetric (n‚àí1)√ó(n‚àí1) Ô¨Ånite diÔ¨Äerence matrix deÔ¨Åned
as
Afd = h‚àí2tridiagn‚àí1(‚àí1, 2, ‚àí1).
(12.8)
This matrix is diagonally dominant by rows; moreover, it is positive deÔ¨Ånite
since for any vector x ‚ààRn‚àí1
xT Afdx = h‚àí2
.
x2
1 + x2
n‚àí1 +
n‚àí1

i=2
(xi ‚àíxi‚àí1)2
/
.

534
12. Two-Point Boundary Value Problems
This implies that (12.7) admits a unique solution. Another interesting prop-
erty is that Afd is an M-matrix (see DeÔ¨Ånition 1.25 and Exercise 2), which
guarantees that the Ô¨Ånite diÔ¨Äerence solution enjoys the same monotonic-
ity property as the exact solution u(x), namely u is nonnegative if f is
nonnegative. This property is called discrete maximum principle.
In order to rewrite (12.6) in operator form, let Vh be a collection of discrete
functions deÔ¨Åned at the grid points xj for j = 0, . . . , n. If vh ‚ààVh, then
vh(xj) is deÔ¨Åned for all j and we sometimes use the shorthand notation vj
instead of vh(xj). Next, we let V 0
h be the subset of Vh containing discrete
functions that are zero at the endpoints x0 and xn. For a function wh we
deÔ¨Åne the operator Lh by
(Lhw)(xj) = ‚àíwj+1 ‚àí2wj + wj‚àí1
h2
,
j = 1, . . . , n ‚àí1
(12.9)
and reformulate the Ô¨Ånite diÔ¨Äerence problem (12.6) equivalently as: Ô¨Ånd
uh ‚ààV 0
h such that
(Lhuh)(xj) = f(xj)
for j = 1, . . . , n ‚àí1.
(12.10)
Notice that, in this formulation, the boundary conditions are taken care of
by the requirement that uh ‚ààV 0
h .
Finite diÔ¨Äerences can be used to provide approximations of higher-order
diÔ¨Äerential operators than the one considered in this section. An example
is given in Section 4.7.2 where the Ô¨Ånite diÔ¨Äerence centred discretization of
the fourth-order derivative ‚àíu(iv)(x) is carried out by applying twice the
discrete operator Lh (see also Exercise 11). Again, extra care is needed to
properly handle the boundary conditions.
12.2.1
Stability Analysis by the Energy Method
For two discrete functions wh, vh ‚ààVh we deÔ¨Åne the discrete inner product
(wh, vh)h = h
n

k=0
ckwkvk,
with c0 = cn = 1/2 and ck = 1 for k = 1, . . . , n‚àí1. This is nothing but the
composite trapezoidal rule (9.13) which is here used to evaluate the inner
product (w, v) =
 1
0 w(x)v(x)dx. Clearly,
‚à•vh‚à•h = (vh, vh)1/2
h
is a norm on Vh.
Lemma 12.1 The operator Lh is symmetric, i.e.
(Lhwh, vh)h = (wh, Lhvh)h
‚àÄwh, vh ‚ààV 0
h ,

12.2 Finite DiÔ¨Äerence Approximation
535
and is positive deÔ¨Ånite, i.e.
(Lhvh, vh)h ‚â•0
‚àÄvh ‚ààV 0
h ,
with equality only if vh ‚â°0.
Proof. From the identity
wj+1vj+1 ‚àíwjvj = (wj+1 ‚àíwj)vj + (vj+1 ‚àívj)wj+1,
upon summation over j from 0 to n ‚àí1 we obtain the following relation for all
wh, vh ‚ààVh
n‚àí1

j=0
(wj+1 ‚àíwj)vj = wnvn ‚àíw0v0 ‚àí
n‚àí1

j=0
(vj+1 ‚àívj)wj+1
which is referred to as summation by parts. Using summation by parts twice, and
setting for ease of notation w‚àí1 = v‚àí1 = 0, for all wh, vh ‚ààV 0
h we obtain
(Lhwh, vh)h
= ‚àíh‚àí1
n‚àí1

j=0
[(wj+1 ‚àíwj) ‚àí(wj ‚àíwj‚àí1)] vj
= h‚àí1
n‚àí1

j=0
(wj+1 ‚àíwj)(vj+1 ‚àívj).
From this relation we deduce that (Lhwh, vh)h = (wh, Lhvh)h; moreover, taking
wh = vh we obtain
(Lhvh, vh)h = h‚àí1
n‚àí1

j=0
(vj+1 ‚àívj)2.
(12.11)
This quantity is always positive, unless vj+1 = vj for j = 0, . . . , n ‚àí1, in which
case vj = 0 for j = 0, . . . , n since v0 = 0.
3
For any grid function vh ‚ààV 0
h we deÔ¨Åne the following norm
|||vh|||h =
Ô£±
Ô£≤
Ô£≥h
n‚àí1

j=0
vj+1 ‚àívj
h
2
Ô£º
Ô£Ω
Ô£æ
1/2
.
(12.12)
Thus, (12.11) is equivalent to
(Lhvh, vh)h = |||vh|||2
h
for all vh ‚ààV 0
h .
(12.13)
Lemma 12.2 The following inequality holds for any function vh ‚ààV 0
h
‚à•vh‚à•h ‚â§
1
‚àö
2|||vh|||h.
(12.14)

536
12. Two-Point Boundary Value Problems
Proof. Since v0 = 0, we have
vj = h
j‚àí1

k=0
vk+1 ‚àívk
h
for all j = 1, . . . , n ‚àí1.
Then,
v2
j = h2
.j‚àí1

k=0
+ vk+1 ‚àívk
h
,/2
.
Using the Minkowski inequality
 m

k=1
pk
2
‚â§m
 m

k=1
p2
k

(12.15)
which holds for every integer m ‚â•1 and every sequence {p1, . . . , pm} of real
numbers (see Exercise 4), we obtain
n‚àí1

j=1
v2
j ‚â§h2
n‚àí1

j=1
j
j‚àí1

k=0
+ vk+1 ‚àívk
h
,2
.
Then for every vh ‚ààV 0
h we get
‚à•vh‚à•2
h = h
n‚àí1

j=1
v2
j ‚â§h2
n‚àí1

j=1
jh
n‚àí1

k=0
+ vk+1 ‚àívk
h
,2
= h2 (n ‚àí1)n
2
|||vh|||2
h.
Inequality (12.14) follows since h = 1/n.
3
Remark 12.1 For every vh ‚ààV 0
h , the grid function v(1)
h
whose grid values
are (vj+1 ‚àívj)/h, j = 0, . . . , n‚àí1, can be regarded as a discrete derivative
of vh (see Section 10.10.1). Inequality (12.14) can thus be rewritten as
‚à•vh‚à•h ‚â§
1
‚àö
2‚à•v(1)
h ‚à•h
‚àÄvh ‚ààV 0
h .
It can be regarded as the discrete counterpart in [0, 1] of the following
Poincar¬¥e inequality: for every interval [a, b] there exists a constant CP > 0
such that
‚à•v‚à•L2(a,b) ‚â§CP ‚à•v(1)‚à•L2(a,b)
(12.16)
for all v ‚ààC1([a, b]) such that v(a) = v(b) = 0 and where ‚à•¬∑ ‚à•L2(a,b) is the
norm in L2(a, b) (see (8.25)).
‚ñ†
Inequality (12.14) has an interesting consequence. If we multiply every
equation of (12.10) by uj and then sum for j from 0 on n ‚àí1, we obtain
(Lhuh, uh)h = (f, uh)h.

12.2 Finite DiÔ¨Äerence Approximation
537
Applying to (12.13) the Cauchy-Schwarz inequality (1.14) (valid in the
Ô¨Ånite dimensional case), we obtain
|||uh|||2
h ‚â§‚à•fh‚à•h‚à•uh‚à•h
where fh ‚ààVh is the grid function such that fh(xj) = f(xj) for all j =
1, . . . , n.
Owing to (12.14) we conclude that
‚à•uh‚à•h ‚â§1
2‚à•fh‚à•h
(12.17)
from which we deduce that the Ô¨Ånite diÔ¨Äerence problem (12.6) has a unique
solution (equivalently, the only solution corresponding to fh = 0 is uh = 0).
Moreover, (12.17) is a stability result, as it states that the Ô¨Ånite diÔ¨Äerence
solution is bounded by the given datum fh.
To prove convergence, we Ô¨Årst introduce the notion of consistency. Ac-
cording to our general deÔ¨Ånition (2.13), if f ‚ààC0([0, 1]) and u ‚ààC2([0, 1])
is the corresponding solution of (12.1)-(12.2), the local truncation error is
the grid function œÑh deÔ¨Åned by
œÑh(xj) = (Lhu)(xj) ‚àíf(xj),
j = 1, . . . , n ‚àí1.
(12.18)
By Taylor series expansion and recalling (10.66), one obtains
œÑh(xj)
=
‚àíh‚àí2 [u(xj‚àí1) ‚àí2u(xj) + u(xj+1)] ‚àíf(xj)
=
‚àíu‚Ä≤‚Ä≤(xj) ‚àíf(xj) + h2
24(u(iv)(Œæj) + u(iv)(Œ∑j))
=
h2
24(u(iv)(Œæj) + u(iv)(Œ∑j))
(12.19)
for suitable Œæj ‚àà(xj‚àí1, xj) and Œ∑j ‚àà(xj, xj+1). Upon deÔ¨Åning the discrete
maximum norm as
‚à•vh‚à•h,‚àû= max
0‚â§j‚â§n |vh(xj)|,
we obtain from (12.19)
‚à•œÑh‚à•h,‚àû‚â§‚à•f ‚Ä≤‚Ä≤‚à•‚àû
12
h2
(12.20)
provided that f ‚ààC2([0, 1]). In particular, lim
h‚Üí0‚à•œÑh‚à•h,‚àû= 0 and there-
fore the Ô¨Ånite diÔ¨Äerence scheme is consistent with the diÔ¨Äerential problem
(12.1)-(12.2).
Remark 12.2 Taylor‚Äôs expansion of u around xj can also be written as
u(xj ¬± h) = u(xj) ¬± hu‚Ä≤(xj) + h2
2 u‚Ä≤‚Ä≤(xj) ¬± h3
6 u‚Ä≤‚Ä≤‚Ä≤(xj) + R4(xj ¬± h)

538
12. Two-Point Boundary Value Problems
with the following integral form of the remainder
R4(xj + h) =
xj+h
>
xj
(u‚Ä≤‚Ä≤‚Ä≤(t) ‚àíu‚Ä≤‚Ä≤‚Ä≤(xj)) (xj + h ‚àít)2
2
dt,
R4(xj ‚àíh) = ‚àí
xj
>
xj‚àíh
(u‚Ä≤‚Ä≤‚Ä≤(t) ‚àíu‚Ä≤‚Ä≤‚Ä≤(xj)) (xj ‚àíh ‚àít)2
2
dt.
Using the two formulae above, by inspection on (12.18) it is easy to see
that
œÑh(xj) = 1
h2 (R4(xj + h) + R4(xj ‚àíh)) .
(12.21)
For any integer m ‚â•0, we denote by Cm,1(0, 1) the space of all functions
in Cm(0, 1) whose m-th derivative is Lipschitz continuous, i.e.
max
x,y‚àà(0,1),xÃ∏=y
|v(m)(x) ‚àív(m)(y)|
|x ‚àíy|
‚â§M < ‚àû.
Looking at (12.21) we see that it suÔ¨Éces to assuming that u ‚ààC3,1(0, 1)
to conclude that
‚à•œÑh‚à•h,‚àû‚â§Mh2
which shows that the Ô¨Ånite diÔ¨Äerence scheme is consistent with the diÔ¨Äer-
ential problem (12.1)-(12.2) even under a slightly weaker regularity of the
exact solution u.
‚ñ†
Remark 12.3 Let e = u ‚àíuh be the discretization error grid function.
Then,
Lhe = Lhu ‚àíLhuh = Lhu ‚àífh = œÑh.
(12.22)
It can be shown (see Exercise 5) that
‚à•œÑh‚à•2
h ‚â§3
+
‚à•f‚à•2
h + ‚à•f‚à•2
L2(0,1)
,
(12.23)
from which it follows that the norm of the discrete second-order derivative
of the discretization error is bounded, provided that the norms of f at the
right-hand side of (12.23) are also bounded.
‚ñ†
12.2.2
Convergence Analysis
The Ô¨Ånite diÔ¨Äerence solution uh can be characterized by a discrete Green‚Äôs
function as follows. For a given grid point xk deÔ¨Åne a grid function Gk ‚ààV 0
h
as the solution to the following problem
LhGk = ek,
(12.24)

12.2 Finite DiÔ¨Äerence Approximation
539
where ek ‚ààV 0
h satisÔ¨Åes ek(xj) = Œ¥kj, 1 ‚â§j ‚â§n ‚àí1. It is easy to see
that Gk(xj) = hG(xj, xk), where G is the Green‚Äôs function introduced in
(12.4) (see Exercise 6). For any grid function g ‚ààV 0
h we can deÔ¨Åne the grid
function
wh = Thg,
wh =
n‚àí1

k=1
g(xk)Gk.
(12.25)
Then
Lhwh =
n‚àí1

k=1
g(xk)LhGk =
n‚àí1

k=1
g(xk)ek = g.
In particular, the solution uh of (12.10) satisÔ¨Åes uh = Thf, therefore
uh =
n‚àí1

k=1
f(xk)Gk,
and
uh(xj) = h
n‚àí1

k=1
G(xj, xk)f(xk).
(12.26)
Theorem 12.1 Assume that f ‚ààC2([0, 1]). Then, the nodal error e(xj) =
u(xj) ‚àíuh(xj) satisÔ¨Åes
‚à•u ‚àíuh‚à•h,‚àû‚â§h2
96‚à•f ‚Ä≤‚Ä≤‚à•‚àû,
(12.27)
i.e. uh converges to u (in the discrete maximum norm) with second order
with respect to h.
Proof. We start by noticing that, thanks to the representation (12.25), the
following discrete counterpart of (12.5) holds
‚à•uh‚à•h,‚àû‚â§1
8‚à•f‚à•h,‚àû.
(12.28)
Indeed, we have
|uh(xj)|
‚â§h
n‚àí1

k=1
G(xj, xk)|f(xk)| ‚â§‚à•f‚à•h,‚àû

h
n‚àí1

k=1
G(xj, xk)

= ‚à•f‚à•h,‚àû1
2xj(1 ‚àíxj) ‚â§1
8‚à•f‚à•h,‚àû
since, if g = 1, then Thg is such that Thg(xj) = 1
2xj(1 ‚àíxj) (see Exercise 7).
Inequality (12.28) provides a result of stability in the discrete maximum norm
for the Ô¨Ånite diÔ¨Äerence solution uh. Using (12.22), by the same argument used to
prove (12.28) we obtain
‚à•e‚à•h,‚àû‚â§1
8‚à•œÑh‚à•h,‚àû.
Finally, the thesis (12.27) follows owing to (12.20).
3
Observe that for the derivation of the convergence result (12.27) we have
used both stability and consistency. In particular, the discretization error
is of the same order (with respect to h) as the consistency error œÑh.

540
12. Two-Point Boundary Value Problems
12.2.3
Finite DiÔ¨Äerences for Two-Point Boundary Value
Problems with Variable CoeÔ¨Écients
A two-point boundary value problem more general than (12.1)-(12.2) is the
following one
Lu(x) = ‚àí(J(u)(x))‚Ä≤ + Œ≥(x)u(x) = f(x)
0 < x < 1,
u(0) = d0,
u(1) = d1
(12.29)
where
J(u)(x) = Œ±(x)u‚Ä≤(x),
(12.30)
d0 and d1 are assigned constants and Œ±, Œ≥ and f are given functions that
are continuous in [0, 1]. Finally, Œ≥(x) ‚â•0 in [0, 1] and Œ±(x) ‚â•Œ±0 > 0 for a
suitable Œ±0. The auxiliary variable J(u) is the Ô¨Çux associated with u and
very often has a precise physical meaning.
For the approximation, it is convenient to introduce on [0, 1] a new grid
made by the midpoints xj+1/2 = (xj + xj+1)/2 of the intervals [xj, xj+1]
for j = 0, . . . , n ‚àí1. Then, a Ô¨Ånite diÔ¨Äerence approximation of (12.29) is
given by: Ô¨Ånd uh ‚ààVh such that
Lhuh(xj) = f(xj)
for all j = 1, . . . , n ‚àí1,
uh(x0) = d0,
uh(xn) = d1,
(12.31)
where Lh is deÔ¨Åned for j = 1, . . . , n ‚àí1 as
Lhw(xj) = ‚àíJj+1/2(wh) ‚àíJj‚àí1/2(wh)
h
+ Œ≥jwj.
(12.32)
We have deÔ¨Åned Œ≥j = Œ≥(xj) and, for j = 0, . . . , n ‚àí1, the approximate
Ô¨Çuxes are given by
Jj+1/2(wh) = Œ±j+1/2
wj+1 ‚àíwj
h
(12.33)
with Œ±j+1/2 = Œ±(xj+1/2).
The Ô¨Ånite diÔ¨Äerence scheme (12.31)-(12.32) with the approximate Ô¨Çuxes
(12.33) can still be cast in the form (12.7) by setting
Afd = h‚àí2tridiagn‚àí1(a, d, a) + diagn‚àí1(c)
(12.34)
where
a =

Œ±1/2, Œ±3/2, . . . , Œ±n‚àí1/2
T ‚ààRn‚àí2,
d =

Œ±1/2 + Œ±3/2, . . . , Œ±n‚àí3/2 + Œ±n‚àí1/2
T ‚ààRn‚àí1,
c = (Œ≥1, . . . , Œ≥n‚àí1)T ‚ààRn‚àí1.

12.2 Finite DiÔ¨Äerence Approximation
541
The matrix (12.34) is symmetric positive deÔ¨Ånite and is also strictly diag-
onally dominant if Œ≥ > 0.
The convergence analysis of the scheme (12.31)-(12.32) can be carried
out by extending straightforwardly the techniques developed in Sections
12.2.1 and 12.2.2.
We conclude this section by addressing boundary conditions that are more
general than those considered in (12.29). For this purpose we assume that
u(0) = d0,
J(u(1)) = g1,
where d0 and g1 are two given data. The boundary condition at x = 1 is
called a Neumann condition while the condition at x = 0 (where the value
of u is assigned) is a Dirichlet boundary condition. The Ô¨Ånite diÔ¨Äerence
discretization of the Neumann boundary condition can be performed by
using the mirror imaging technique. For any suÔ¨Éciently smooth function
œà we write its truncated Taylor‚Äôs expansion at xn as
œàn = œàn‚àí1/2 + œàn+1/2
2
‚àíh2
16(œà‚Ä≤‚Ä≤(Œ∑n) + œà‚Ä≤‚Ä≤(Œæn))
for suitable Œ∑n ‚àà(xn‚àí1/2, xn), Œæn ‚àà(xn, xn+1/2). Taking œà = J(u) and
neglecting the term containing h2 yields
Jn+1/2(uh) = 2g1 ‚àíJn‚àí1/2(uh).
(12.35)
Notice that the point xn+1/2 = xn +h/2 and the corresponding Ô¨Çux Jn+1/2
do not really exist (indeed, xn+1/2 is called a ‚Äúghost‚Äù point), but it is
generated by linear extrapolation of the Ô¨Çux at the nearby nodes xn‚àí1/2
and xn. The Ô¨Ånite diÔ¨Äerence equation (12.32) at the node xn reads
Jn‚àí1/2(uh) ‚àíJn+1/2(uh)
h
+ Œ≥nun = fn.
Using (12.35) to obtain Jn+1/2(uh) we Ô¨Ånally get the second-order accurate
approximation
‚àíŒ±n‚àí1/2
un‚àí1
h2
+
+Œ±n‚àí1/2
h2
+ Œ≥n
2
,
un = g1
h + fn
2 .
This formula suggests easy modiÔ¨Åcation of the matrix and right-hand side
entries in the Ô¨Ånite diÔ¨Äerence system (12.7).
For a further generalization of the boundary conditions in (12.29) and
their discretization using Ô¨Ånite diÔ¨Äerences we refer to Exercise 10 where
boundary conditions of the form Œªu + ¬µu‚Ä≤ = g at both the endpoints of
(0, 1) are considered for u (Robin boundary conditions).
For a thorough presentation and analysis of Ô¨Ånite diÔ¨Äerence approxima-
tions of two-point boundary value problems, see, e.g., [Str89] and [HGR96].

542
12. Two-Point Boundary Value Problems
12.3
The Spectral Collocation Method
Other discretization schemes can be derived which exhibit the same struc-
ture as the Ô¨Ånite diÔ¨Äerence problem (12.10), with a discrete operator Lh
being deÔ¨Åned in a diÔ¨Äerent manner, though.
Actually, numerical approximations of the second derivative other than
the centred Ô¨Ånite diÔ¨Äerence one can be set up, as described in Section
10.10.3. A noticeable instance is provided by the spectral collocation method.
In that case we assume the diÔ¨Äerential equation (12.1) to be set on the in-
terval (‚àí1, 1) and choose the nodes {x0, . . . , xn} to coincide with the n+1
Legendre-Gauss-Lobatto nodes introduced in Section 10.4. Besides, uh is a
polynomial of degree n. For coherence, we will use throughout the section
the index n instead of h.
The spectral collocation problem reads
Ô¨Ånd un ‚ààP0
n : Lnun(xj) = f(xj),
j = 1, . . . , n ‚àí1
(12.36)
where P0
n is the set of polynomials p ‚ààPn([0, 1]) such that p(0) = p(1) = 0.
Besides, Lnv = LInv for any continuous function v where Inv ‚ààPn is the
interpolant of v at the nodes {x0, . . . , xn} and L denotes the diÔ¨Äerential
operator at hand, which, in the case of equation (12.1), coincides with
‚àíd2/dx2. Clearly, if v ‚ààPn then Lnv = Lv.
The algebraic form of (12.36) becomes
Aspu = f,
where uj = un(xj), fj = f(xj) j = 1, . . . , n‚àí1 and the spectral collocation
matrix Asp ‚ààR(n‚àí1)√ó(n‚àí1) is equal to ÀúD2, where ÀúD is the matrix obtained
from the pseudo-spectral diÔ¨Äerentiation matrix (10.73) by eliminating the
Ô¨Årst and the n + 1-th rows and columns.
For the analysis of (12.36) we can introduce the following discrete scalar
product
(u, v)n =
n

j=0
u(xj)v(xj)wj,
(12.37)
where wj are the weights of the Legendre-Gauss-Lobatto quadrature for-
mula (see Section 10.4). Then (12.36) is equivalent to
(Lnun, vn)n = (f, vn)n
‚àÄvn ‚ààP0
n.
(12.38)
Since (12.37) is exact for u, v such that uv ‚ààP2n‚àí1 (see Section 10.2) then
(Lnvn, vn)n = (Lnvn, vn) = ‚à•v‚Ä≤
n‚à•2
L2(‚àí1,1),
‚àÄvn ‚ààP0
n.
Besides,
(f, vn)n ‚â§‚à•f‚à•n‚à•vn‚à•n ‚â§
‚àö
6‚à•f‚à•‚àû‚à•vn‚à•L2(‚àí1,1),

12.3 The Spectral Collocation Method
543
where ‚à•f‚à•‚àûdenotes the maximum of f in [‚àí1, 1] and we have used the
fact that ‚à•f‚à•n ‚â§
‚àö
2‚à•f‚à•‚àûand the result of equivalence
‚à•vn‚à•L2(‚àí1,1) ‚â§‚à•vn‚à•n ‚â§
‚àö
3‚à•vn‚à•L2(‚àí1,1),
‚àÄvn ‚ààPn
(see [CHQZ88], p. 286).
Taking vn = un in (12.38) and using the Poincar¬¥e inequality (12.16) we
Ô¨Ånally obtain
‚à•u‚Ä≤
n‚à•L2(‚àí1,1) ‚â§
‚àö
6CP ‚à•f‚à•‚àû
which ensures that problem (12.36) has a unique solution which is stable.
As for consistency, we can notice that
œÑn(xj) = (Lnu ‚àíf)(xj) = (‚àí(Inu)‚Ä≤‚Ä≤ ‚àíf)(xj) = (u ‚àíInu)‚Ä≤‚Ä≤(xj)
and this right-hand side tends to zero as n ‚Üí‚àûprovided that u ‚àà
C2([‚àí1, 1]).
Let us now establish a convergence result for the spectral collocation
approximation of (12.1). In the following, C is a constant independent of
n that can assume diÔ¨Äerent values at diÔ¨Äerent places.
Moreover, we denote by Hs(a, b), for s ‚â•1, the space of the functions f ‚àà
Cs‚àí1(a, b) such that f (s‚àí1) is continuous and piecewise diÔ¨Äerentiable, so
that f (s) exists unless for a Ô¨Ånite number of points and belongs to L2(a, b).
The space Hs(a, b) is known as the Sobolev function space of order s and is
endowed with the norm ‚à•¬∑ ‚à•Hs(a,b) deÔ¨Åned in (10.35).
Theorem 12.2 Let f ‚ààHs(‚àí1, 1) for some s ‚â•1. Then
‚à•u‚Ä≤ ‚àíu‚Ä≤
n‚à•L2(‚àí1,1) ‚â§Cn‚àís 
‚à•f‚à•Hs(‚àí1,1) + ‚à•u‚à•Hs+1(‚àí1,1)

.
(12.39)
Proof. Note that un satisÔ¨Åes
(u‚Ä≤
n, v‚Ä≤
n) = (f, vn)n
where (u, v) =
 1
‚àí1 uvdx is the scalar product of L2(‚àí1, 1). Similarly, u satisÔ¨Åes
(u‚Ä≤, v‚Ä≤) = (f, v)
‚àÄv ‚ààC1([0, 1]) such that v(0) = v(1) = 0
(see (12.43) of Section 12.4). Then
((u ‚àíun)‚Ä≤, v‚Ä≤
n) = (f, vn) ‚àí(f, vn)n =: E(f, vn),
‚àÄvn ‚ààP0
n.
It follows that
((u ‚àíun)‚Ä≤, (u ‚àíun)‚Ä≤)
= ((u ‚àíun)‚Ä≤, (u ‚àíInu)‚Ä≤) + ((u ‚àíun)‚Ä≤, (Inu ‚àíun)‚Ä≤)
= ((u ‚àíun)‚Ä≤, (u ‚àíInu)‚Ä≤) + E(f, Inu ‚àíun).
We recall the following result (see (10.36))
|E(f, vn)| ‚â§Cn‚àís‚à•f‚à•Hs(‚àí1,1)‚à•vn‚à•L2(‚àí1,1).

544
12. Two-Point Boundary Value Problems
Then
|E(f, Inu ‚àíun)| ‚â§Cn‚àís‚à•f‚à•Hs(‚àí1,1)

‚à•Inu ‚àíu‚à•L2(‚àí1,1) + ‚à•u ‚àíun‚à•L2(‚àí1,1)

.
We recall now the following Young‚Äôs inequality (see Exercise 8)
ab ‚â§Œµa2 + 1
4Œµb2,
‚àÄa, b ‚ààR,
‚àÄŒµ > 0.
(12.40)
Using this inequality we obtain

(u ‚àíun)‚Ä≤, (u ‚àíInu)‚Ä≤
‚â§1
4‚à•(u ‚àíun)‚Ä≤‚à•2
L2(‚àí1,1) + ‚à•(u ‚àíInu)‚Ä≤‚à•2
L2(‚àí1,1),
and also (using the Poincar¬¥e inequality (12.16))
Cn‚àís‚à•f‚à•Hs(‚àí1,1)‚à•u ‚àíun‚à•L2(‚àí1,1) ‚â§C CP n‚àís‚à•f‚à•Hs(‚àí1,1)‚à•(u ‚àíun)‚Ä≤‚à•L2(‚àí1,1)
‚â§(CCP )2n‚àí2s‚à•f‚à•2
Hs(‚àí1,1) + 1
4‚à•(u ‚àíun)‚Ä≤‚à•2
L2(‚àí1,1).
Finally,
Cn‚àís‚à•f‚à•Hs(‚àí1,1)‚à•Inu ‚àíu‚à•L2(‚àí1,1) ‚â§1
2C2n‚àí2s‚à•f‚à•2
Hs(‚àí1,1) + 1
2‚à•Inu ‚àíu‚à•2
L2(‚àí1,1).
Using the interpolation error estimate (10.22) for u ‚àíInu we Ô¨Ånally obtain the
desired error estimate (12.39).
3
12.4
The Galerkin Method
We now derive the Galerkin approximation of problem (12.1)-(12.2), which
is the basic ingredient of the Ô¨Ånite element method and the spectral method,
widely employed in the numerical approximation of boundary value prob-
lems.
12.4.1
Integral Formulation of Boundary Value Problems
We consider a problem which is slightly more general than (12.1), namely
‚àí(Œ±u‚Ä≤)‚Ä≤(x) + (Œ≤u‚Ä≤)(x) + (Œ≥u)(x) = f(x)
0 < x < 1,
(12.41)
with u(0) = u(1) = 0, where Œ±, Œ≤ and Œ≥ are continuous functions on [0, 1]
with Œ±(x) ‚â•Œ±0 > 0 for any x ‚àà[0, 1]. Let us now multiply (12.41) by
a function v ‚ààC1([0, 1]), hereafter called a ‚Äútest function‚Äù, and integrate
over the interval [0, 1]
1
>
0
Œ±u‚Ä≤v‚Ä≤ dx +
1
>
0
Œ≤u‚Ä≤v dx +
1
>
0
Œ≥uv dx =
1
>
0
fv dx + [Œ±u‚Ä≤v]1
0,

12.4 The Galerkin Method
545
where we have used integration by parts on the Ô¨Årst integral. If the function
v is required to vanish at x = 0 and x = 1 we obtain
1
>
0
Œ±u‚Ä≤v‚Ä≤ dx +
1
>
0
Œ≤u‚Ä≤v dx +
1
>
0
Œ≥uv dx =
1
>
0
fv dx.
We will denote by V the test function space. This consists of all functions v
that are continuous, vanish at x = 0 and x = 1 and whose Ô¨Årst derivative is
piecewise continuous, i.e., continuous everywhere except at a Ô¨Ånite number
of points in [0, 1] where the left and right limits v‚Ä≤
‚àíand v‚Ä≤
+ exist but do not
necessarily coincide.
V is actually a vector space which is denoted by H1
0(0, 1). Precisely,
H1
0(0, 1) =

v ‚ààL2(0, 1) : v‚Ä≤ ‚ààL2(0, 1), v(0) = v(1) = 0

(12.42)
where v‚Ä≤ is the distributional derivative of v whose deÔ¨Ånition is given in
Section 12.4.2.
We have therefore shown that if a function u ‚ààC2([0, 1]) satisÔ¨Åes (12.41),
then u is also a solution of the following problem
Ô¨Ånd u ‚ààV : a(u, v) = (f, v) for all v ‚ààV,
(12.43)
where now (f, v) =
 1
0 fv dx denotes the scalar product of L2(0, 1) and
a(u, v) =
1
>
0
Œ±u‚Ä≤v‚Ä≤ dx +
1
>
0
Œ≤u‚Ä≤v dx +
1
>
0
Œ≥uv dx
(12.44)
is a bilinear form, i.e. it is linear with respect to both arguments u and
v. Problem (12.43) is called the weak formulation of problem (12.1). Since
(12.43) contains only the Ô¨Årst derivative of u it might cover cases in which
a classical solution u ‚ààC2([0, 1]) of (12.41) does not exist although the
physical problem is well deÔ¨Åned.
If for instance, Œ± = 1, Œ≤ = Œ≥ = 0, the solution u(x) denotes of the
displacement at point x of an elastic cord having linear density equal to f,
whose position at rest is u(x) = 0 for all x ‚àà[0, 1] and which remains Ô¨Åxed
at the endpoints x = 0 and x = 1. Figure 12.2 (right) shows the solution
u(x) corresponding to a function f which is discontinuous (see Figure 12.2,
left). Clearly, u‚Ä≤‚Ä≤ does not exist at the points x = 0.4 and x = 0.6 where f
is discontinuous.
If (12.41) is supplied with non homogeneous boundary conditions, say
u(0) = u0, u(1) = u1, we can still obtain a formulation like (12.43) by
proceeding as follows. Let ¬Øu(x) = xu1 + (1 ‚àíx)u0 be the straight line that
interpolates the data at the endpoints, and set
0u= u(x)‚àí¬Øu(x). Then
0u‚ààV

546
12. Two-Point Boundary Value Problems
1
0.6
0.4
0
‚àí1
f(x)
x
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
‚àí0.05
‚àí0.045
‚àí0.04
‚àí0.035
‚àí0.03
‚àí0.025
‚àí0.02
‚àí0.015
‚àí0.01
‚àí0.005
0
u(x)
FIGURE 12.2. Elastic cord Ô¨Åxed at the endpoints and subject to a discontinuous
load f (left). The vertical displacement u is shown on the right
satisÔ¨Åes the following problem
Ô¨Ånd
0u‚ààV : a(
0u, v) = (f, v) ‚àía(¬Øu, v) for all v ‚ààV.
A similar problem is obtained in the case of Neumann boundary conditions,
say u‚Ä≤(0) = u‚Ä≤(1) = 0. Proceeding as we did to obtain (12.43), we see
that the solution u of this homogeneous Neumann problem satisÔ¨Åes the
same problem (12.43) provided the space V is now H1(0, 1). More general
boundary conditions of mixed type can be considered as well (see Exercise
12).
12.4.2
A Quick Introduction to Distributions
Let X be a Banach space, i.e., a normed and complete vector space. We
say that a functional T : X ‚ÜíR is continuous if limx‚Üíx0 T(x) = T(x0)
for all x0 ‚ààX and linear if T(x + y) = T(x) + T(y) for any x, y ‚ààX and
T(Œªx) = ŒªT(x) for any x ‚ààX and Œª ‚ààR.
Usually, a linear continuous functional is denoted by ‚ü®T, x‚ü©and the sym-
bol ‚ü®¬∑, ¬∑‚ü©is called duality. As an example, let X = C0([0, 1]) be endowed
with the maximum norm ‚à•¬∑ ‚à•‚àûand consider on X the two functionals
deÔ¨Åned as
‚ü®T, x‚ü©= x(0),
‚ü®S, x‚ü©=
> 1
0
x(t) sin(t)dt.
It is easy to check that both T and S are linear and continuous functionals
on X. The set of all linear continuous functionals on X identiÔ¨Åes an abstract
space which is called the dual space of X and is denoted by X‚Ä≤.
We then introduce the space C‚àû
0 (0, 1) (or D(0, 1)) of inÔ¨Ånitely diÔ¨Äer-
entiable functions having compact support in [0, 1], i.e., vanishing outside
a bounded open set (a, b) ‚äÇ(0, 1) with 0 < a < b < 1. We say that
vn ‚ààD(0, 1) converges to v ‚ààD(0, 1) if there exists a closed bounded set
K ‚äÇ(0, 1) such that vn vanishes outside K for each n and for any k ‚â•0
the derivative v(k)
n
converges to v(k) uniformly in (0, 1).

12.4 The Galerkin Method
547
The space of linear functionals on D(0, 1) which are continuous with
respect to the convergence introduced above is denoted by D‚Ä≤(0, 1) (the
dual space of D(0, 1)) and its elements are called distributions.
We are now in position to introduce the derivative of a distribution. Let
T be a distribution, i.e. an element of D‚Ä≤(0, 1). Then, for any k ‚â•0, T (k)
is also a distribution, deÔ¨Åned as
‚ü®T (k), œï‚ü©= (‚àí1)k‚ü®T, œï(k)‚ü©,
‚àÄœï ‚ààD(0, 1).
(12.45)
As an example, consider the Heaviside function
H(x) =
"
1
x ‚â•0,
0
x < 0.
The distributional derivative of H is the Dirac mass Œ¥ at the origin, deÔ¨Åned
as
v ‚ÜíŒ¥(v) = v(0),
v ‚ààD(R).
From the deÔ¨Ånition (12.45), it turns out that any distribution is inÔ¨Ånitely
diÔ¨Äerentiable; moreover, if T is a diÔ¨Äerentiable function its distributional
derivative coincides with the usual one.
12.4.3
Formulation and Properties of the Galerkin Method
Unlike the Ô¨Ånite diÔ¨Äerence method which stems directly from the diÔ¨Äer-
ential (or strong) form (12.41), the Galerkin method is based on the weak
formulation (12.43). If Vh is a Ô¨Ånite dimensional vector subspace of V , the
Galerkin method consists of approximating (12.43) by the problem
Ô¨Ånd uh ‚ààVh : a(uh, vh) = (f, vh)
‚àÄvh ‚ààVh.
(12.46)
This is a Ô¨Ånite dimensional problem. Actually, let {œï1, . . . , œïN} denote a
basis of Vh, i.e. a set of N linearly independent functions of Vh. Then we
can write
uh(x) =
N

j=1
ujœïj(x).
The integer N denotes the dimension of the vector space Vh. Taking vh = œïi
in (12.46), it turns out that the Galerkin problem (12.46) is equivalent to
seeking N unknown coeÔ¨Écients {u1, . . . , uN} such that
N

j=1
uja(œïj, œïi) = (f, œïi)
‚àÄi = 1, . . . , N.
(12.47)

548
12. Two-Point Boundary Value Problems
We have used the linearity of a(¬∑, ¬∑) with respect to its Ô¨Årst argument, i.e.
a(
N

j=1
ujœïj, œïi) =
N

j=1
uja(œïj, œïi).
If we introduce the matrix AG = (aij), aij = a(œïj, œïi) (called the stiÔ¨Äness
matrix), the unknown vector u = (u1, . . . , uN) and the right-hand side
vector fG = (f1, . . . , fN), with fi = (f, œïi), we see that (12.47) is equivalent
to the linear system
AGu = fG.
(12.48)
The structure of AG, as well as the degree of accuracy of uh, depends on
the form of the basis functions {œïi}, and therefore on the choice of Vh.
We will see two remarkable instances, the Ô¨Ånite element method, where
Vh is a space of piecewise polynomials over subintervals of [0, 1] of length not
greater than h which are continuous and vanish at the endpoints x = 0 and
1, and the spectral method in which Vh is a space of algebraic polynomials
still vanishing at the endpoints x = 0, 1.
However, before speciÔ¨Åcally addressing those cases, we state a couple of
general results that hold for any Galerkin problem (12.46).
12.4.4
Analysis of the Galerkin Method
We endow the space H1
0(0, 1) with the following norm
|v|H1(0,1) =
Ô£±
Ô£≤
Ô£≥
1
>
0
|v‚Ä≤(x)|2 dx
Ô£º
Ô£Ω
Ô£æ
1/2
.
(12.49)
We will address the special case where Œ≤ = 0 and Œ≥(x) ‚â•0. In the most
general case given by the diÔ¨Äerential problem (12.41) we shall assume that
the coeÔ¨Écients satisfy
‚àí1
2Œ≤‚Ä≤ + Œ≥ ‚â•0,
‚àÄx ‚àà[0, 1].
(12.50)
This ensures that the Galerkin problem (12.46) admits a unique solution
depending continuously on the data. Taking vh = uh in (12.46) we obtain
Œ±0|uh|2
H1(0,1) ‚â§
1
>
0
Œ±u‚Ä≤
hu‚Ä≤
h dx +
1
>
0
Œ≥uhuh dx = (f, uh) ‚â§‚à•f‚à•L2(0,1)‚à•uh‚à•L2(0,1),
where we have used the Cauchy-Schwarz inequality (8.29) to set the right-
hand side inequality. Owing to the Poincar¬¥e inequality (12.16) we conclude
that
|uh|H1(0,1) ‚â§CP
Œ±0
‚à•f‚à•L2(0,1).
(12.51)

12.4 The Galerkin Method
549
Thus, the norm of the Galerkin solution remains bounded (uniformly with
respect to the dimension of the subspace Vh) provided that f ‚ààL2(0, 1).
Inequality (12.51) therefore represents a stability result for the solution of
the Galerkin problem.
As for convergence, we can prove the following result.
Theorem 12.3 Let C = Œ±‚àí1
0 (‚à•Œ±‚à•‚àû+ C2
P ‚à•Œ≥‚à•‚àû); then, we have
|u ‚àíuh|H1(0,1) ‚â§C min
wh‚ààVh|u ‚àíwh|H1(0,1).
(12.52)
Proof. Subtracting (12.46) from (12.43) (where we use vh ‚ààVh ‚äÇV ), owing to
the bilinearity of the form a(¬∑, ¬∑) we obtain
a(u ‚àíuh, vh) = 0
‚àÄvh ‚ààVh.
(12.53)
Then, setting e(x) = u(x) ‚àíuh(x), we deduce
Œ±0|e|2
H1(0,1) ‚â§a(e, e) = a(e, u ‚àíwh) + a(e, wh ‚àíuh)
‚àÄwh ‚ààVh.
The last term is null due to (12.53). On the other hand, still by the Cauchy-
Schwarz inequality we obtain
a(e, u ‚àíwh)
=
1
>
0
Œ±e‚Ä≤(u ‚àíwh)‚Ä≤ dx +
1
>
0
Œ≥e(u ‚àíwh) dx
‚â§‚à•Œ±‚à•‚àû|e|H1(0,1)|u ‚àíwh|H1(0,1) + ‚à•Œ≥‚à•‚àû‚à•e‚à•L2(0,1)‚à•u ‚àíwh‚à•L2(0,1).
The desired result (12.52) now follows by using again the Poincar¬¥e inequality for
both ‚à•e‚à•L2(0,1) and ‚à•u ‚àíwh‚à•L2(0,1).
3
The previous results can be obtained under more general hypotheses on
problems (12.43) and (12.46). Precisely, we can assume that V is a Hilbert
space, endowed with norm ‚à•¬∑‚à•V , and that the bilinear form a : V √óV ‚ÜíR
satisÔ¨Åes the following properties:
‚àÉŒ±0 > 0 : a(v, v) ‚â•Œ±0‚à•v‚à•2
V
‚àÄv ‚ààV (coercivity),
(12.54)
‚àÉM > 0 : |a(u, v)| ‚â§M‚à•u‚à•V ‚à•v‚à•V
‚àÄu, v ‚ààV (continuity).
(12.55)
Moreover, the right hand side (f, v) satisÔ¨Åes the following inequality
|(f, v)| ‚â§K‚à•v‚à•V
‚àÄv ‚ààV.
Then both problems (12.43) and (12.46) admit unique solutions that satisfy
‚à•u‚à•V ‚â§K
Œ±0
,
‚à•uh‚à•V ‚â§K
Œ±0
.

550
12. Two-Point Boundary Value Problems
This is a celebrated result which is known as the Lax-Milgram Lemma (for
its proof see, e.g., [QV94]). Besides, the following error inequality holds
‚à•u ‚àíuh‚à•V ‚â§M
Œ±0
min
wh‚ààVh‚à•u ‚àíwh‚à•V .
(12.56)
The proof of this last result, which is known as C¬¥ea‚Äôs Lemma, is very similar
to that of (12.52) and is left to the reader.
We now wish to notice that, under the assumption (12.54), the matrix
introduced in (12.48) is positive deÔ¨Ånite. To show this, we must check that
vT Bv ‚â•0 ‚àÄv ‚ààRN and that vT Bv = 0 ‚áîv = 0 (see Section 1.12).
Let us associate with a generic vector v = (vi) of RN the function vh =
N
j=1 vjœïj ‚ààVh. Since the form a(¬∑, ¬∑) is bilinear and coercive we get
vT AGv
=
N

j=1
N

i=1
viaijvj =
N

j=1
N

i=1
via(œïj, œïi)vj
=
N

j=1
N

i=1
a(vjœïj, viœïi) = a
Ô£´
Ô£≠
N

j=1
vjœïj,
N

i=1
viœïi
Ô£∂
Ô£∏
=
a(vh, vh) ‚â•Œ±‚à•vh‚à•2
V ‚â•0.
Moreover, if vT AGv = 0 then also ‚à•vh‚à•2
V = 0 which implies vh = 0 and
thus v = 0.
It is also easy to check that the matrix AG is symmetric iÔ¨Äthe bilinear
form a(¬∑, ¬∑) is symmetric.
For example, in the case of problem (12.41) with Œ≤ = Œ≥ = 0 the ma-
trix AG is symmetric and positive deÔ¨Ånite (s.p.d.) while if Œ≤ and Œ≥ are
nonvanishing, AG is positive deÔ¨Ånite only under the assumption (12.50).
If AG is s.p.d. the numerical solution of the linear system (12.48) can be
eÔ¨Éciently carried out using direct methods like the Cholesky factorization
(see Section 3.4.2) as well as iterative methods like the conjugate gradient
method (see Section 4.3.4). This is of particular interest in the solution of
boundary value problems in more than one space dimension (see Section
12.6).
12.4.5
The Finite Element Method
The Ô¨Ånite element method (FEM) is a special technique for constructing
a subspace Vh in (12.46) based on the piecewise polynomial interpolation
considered in Section 8.3. With this aim, we introduce a partition Th of
[0,1] into n subintervals Ij = [xj, xj+1], n ‚â•2, of width hj = xj+1 ‚àíxj,
j = 0, . . . , n ‚àí1, with
0 = x0 < x1 < . . . < xn‚àí1 < xn = 1

12.4 The Galerkin Method
551
and let h = max
Th (hj). Since functions in H1
0(0, 1) are continuous it makes
sense to consider for k ‚â•1 the family of piecewise polynomials Xk
h intro-
duced in (8.22) (where now [a, b] must be replaced by [0, 1]). Any function
vh ‚ààXk
h is a continuous piecewise polynomial over [0, 1] and its restriction
over each interval Ij ‚ààTh is a polynomial of degree ‚â§k. In the following
we shall mainly deal with the cases k = 1 and k = 2.
Then, we set
Vh = Xk,0
h
=

vh ‚ààXk
h : vh(0) = vh(1) = 0

.
(12.57)
The dimension N of the Ô¨Ånite element space Vh is equal to nk ‚àí1. In the
following the two cases k = 1 and k = 2 will be examined.
To assess the accuracy of the Galerkin FEM we Ô¨Årst notice that, thanks
to C¬¥ea‚Äôs lemma (12.56), we have
min
wh‚ààVh‚à•u ‚àíwh‚à•H1
0(0,1) ‚â§‚à•u ‚àíŒ†k
hu‚à•H1
0(0,1)
(12.58)
where Œ†k
hu is the interpolant of the exact solution u ‚ààV of (12.43) (see
Section 8.3). From inequality (12.58) we conclude that the matter of esti-
mating the Galerkin approximation error ‚à•u ‚àíuh‚à•H1
0(0,1) is turned into the
estimate of the interpolation error ‚à•u ‚àíŒ†k
hu‚à•H1
0(0,1). When k = 1, using
(12.56) and (8.27) we obtain
‚à•u ‚àíuh‚à•H1
0(0,1) ‚â§M
Œ±0
Ch‚à•u‚à•H2(0,1)
provided that u ‚ààH2(0, 1). This estimate can be extended to the case k > 1
as stated in the following convergence result (for its proof we refer, e.g., to
[QV94], Theorem 6.2.1).
Property 12.1 Let u ‚ààH1
0(0, 1) be the exact solution of (12.43) and uh ‚àà
Vh its Ô¨Ånite element approximation using continuous piecewise polynomials
of degree k ‚â•1. Assume also that u ‚ààHs(0, 1) for some s ‚â•2. Then the
following error estimate holds
‚à•u ‚àíuh‚à•H1
0(0,1) ‚â§M
Œ±0
Chl‚à•u‚à•Hl+1(0,1)
(12.59)
where l = min(k, s ‚àí1). Under the same assumptions, one can also prove
that
‚à•u ‚àíuh‚à•L2(0,1) ‚â§Chl+1‚à•u‚à•Hl+1(0,1).
(12.60)
The estimate (12.59) shows that the Galerkin method is convergent, i.e. the
approximation error tends to zero as h ‚Üí0 and the order of convergence is

552
12. Two-Point Boundary Value Problems
k. We also see that there is no convenience in increasing the degree k of the
Ô¨Ånite element approximation if the solution u is not suÔ¨Éciently smooth. In
this respect l is called a regularity threshold. The obvious alternative to gain
accuracy in such a case is to reduce the stepzise h. Spectral methods, which
will be considered in Section 12.4.7, instead pursue the opposite strategy
(i.e. increasing the degree k) and are thus ideally suited to approximating
problems with highly smooth solutions.
An interesting situation is that where the exact solution u has the min-
imum regularity (s = 1). In such a case, C¬¥ea‚Äôs lemma ensures that the
Galerkin FEM is still convergent since as h ‚Üí0 the subspace Vh becomes
dense into V . However, the estimate (12.59) is no longer valid so that
it is not possible to establish the order of convergence of the numerical
method. Table 12.1 summarizes the orders of convergence of the FEM for
k = 1, . . . , 4 and s = 1, . . . , 5.
k
s = 1
s = 2
s = 3
s = 4
s = 5
1
only convergence
h1
h1
h1
h1
2
only convergence
h1
h2
h2
h2
3
only convergence
h1
h2
h3
h3
4
only convergence
h1
h2
h3
h4
TABLE 12.1. Order of convergence of the FEM as a function of k (the degree of
interpolation) and s (the Sobolev regularity of the solution u)
Let us now focus on how to generate a suitable basis {œïj} for the Ô¨Ånite
element space Xk
h in the special cases k = 1 and k = 2. The basic point is
to choose appropriately a set of degrees of freedom for each element Ij of
the partition Th (i.e., the parameters which permit uniquely identifying a
function in Xk
h). The generic function vh in Xk
h can therefore be written as
vh(x) =
nk

i=0
viœïi(x)
where {vi} denote the set of the degrees of freedom of vh and the basis
functions œïi (which are also called shape functions) are assumed to satisfy
the Lagrange interpolation property œïi(xj) = Œ¥ij, i, j = 0, . . . , n, where Œ¥ij
is the Kronecker symbol.
The space X1
h
This space consists of all continuous and piecewise linear functions over
the partition Th. Since a unique straight line passes through two distinct
nodes the number of degrees of freedom for vh is equal to the number
n + 1 of nodes in the partition. As a consequence, n + 1 shape functions

12.4 The Galerkin Method
553
œïi, i = 0, . . . , n, are needed to completely span the space X1
h. The most
natural choice for œïi, i = 1, . . . , n ‚àí1, is
œïi(x) =
Ô£±
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≥
x ‚àíxi‚àí1
xi ‚àíxi‚àí1
for xi‚àí1 ‚â§x ‚â§xi,
xi+1 ‚àíx
xi+1 ‚àíxi
for xi ‚â§x ‚â§xi+1,
0
elsewhere.
(12.61)
The shape function œïi is thus piecewise linear over Th, its value is 1 at
the node xi and 0 at all the other nodes of the partition. Its support (i.e.,
the subset of [0, 1] where œïi is nonvanishing) consists of the union of the
intervals Ii‚àí1 and Ii if 1 ‚â§i ‚â§n ‚àí1 while it coincides with the interval I0
(respectively In‚àí1) if i = 0 (resp., i = n). The plots of œïi, œï0 and œïn are
shown in Figure 12.3.
1
x1
x0
xi‚àí1 xi
xi+1
xn‚àí1 xn = 1
œïi
œïn
œï0
FIGURE 12.3. Shape functions of X1
h associated with internal and boundary
nodes
For any interval Ii = [xi, xi+1], i = 0, . . . , n ‚àí1, the two basis functions œïi
and œïi+1 can be regarded as the images of two ‚Äúreference‚Äù shape functions
œï0 and œï1 (deÔ¨Åned over the reference interval [0, 1]) through the linear
aÔ¨Éne mapping œÜ : [0, 1] ‚ÜíIi
x = œÜ(Œæ) = xi + Œæ(xi+1 ‚àíxi),
i = 0, . . . , n ‚àí1.
(12.62)
DeÔ¨Åning œï0(Œæ) = 1 ‚àíŒæ, œï1(Œæ) = Œæ, the two shape functions œïi and œïi+1
can be constructed over the interval Ii as
œïi(x) = œï0(Œæ(x)),
œïi+1(x) = œï1(Œæ(x))
where Œæ(x) = (x ‚àíxi)/(xi+1 ‚àíxi) (see Figure 12.4).

554
12. Two-Point Boundary Value Problems
0
1
œï1
Œæ
1
xi
xi+1
œïi+1
x
1
œÜ
‚àí‚Üí
FIGURE 12.4. Linear aÔ¨Éne mapping œÜ from the reference interval to the generic
interval of the partition
The space X2
h
The generic function vh ‚ààX2
h is a piecewise polynomial of degree 2 over
each interval Ii. As such, it can be uniquely determined once three values
of it at three distinct points of Ii are assigned. To ensure continuity of
vh over [0, 1] the degrees of freedom are chosen as the function values at
the nodes xi of Th, i = 0, . . . , n, and at the midpoints of each interval Ii,
i = 0, . . . , n ‚àí1, for a total number equal to 2n + 1. It is convenient to
label the degrees of freedom and the corresponding nodes in the partition
starting from x0 = 0 until x2n = 1 in such a way that the midpoints of
each interval correspond to the nodes with odd index while the endpoints
of each interval correspond to the nodes with even index.
The explicit expression of the single shape function is
(i even)
œïi(x) =
Ô£±
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≥
(x ‚àíxi‚àí1)(x ‚àíxi‚àí2)
(xi ‚àíxi‚àí1)(xi ‚àíxi‚àí2)
for xi‚àí2 ‚â§x ‚â§xi,
(xi+1 ‚àíx)(xi+2 ‚àíx)
(xi+1 ‚àíxi)(xi+2 ‚àíxi)
for xi ‚â§x ‚â§xi+2,
0
elsewhere,
(12.63)
(i odd)
œïi(x) =
Ô£±
Ô£¥
Ô£≤
Ô£¥
Ô£≥
(xi+1 ‚àíx)(x ‚àíxi‚àí1)
(xi+1 ‚àíxi)(xi ‚àíxi‚àí1)
for xi‚àí1 ‚â§x ‚â§xi+1,
0
elsewhere.
(12.64)
Each basis function enjoys the property that œï(xj) = Œ¥ij, i, j = 0, . . . , 2n.
The shape functions for X2
h on the reference interval [0, 1] are
œï0(Œæ) = (1 ‚àíŒæ)(1 ‚àí2Œæ),
œï1(Œæ) = 4(1 ‚àíŒæ)Œæ,
œï2(Œæ) = Œæ(2Œæ ‚àí1)(12.65)

12.4 The Galerkin Method
555
and are shown in Figure 12.5. As in the case of piecewise linear Ô¨Ånite
elements of X1
h the shape functions (12.63) and (12.64) are the images of
(12.65) through the aÔ¨Éne mapping (12.62). Notice that the support of the
basis function œï2i+1 associated with the midpoint x2i+1 coincides with the
interval to which the midpoint belongs. Due to its shape œï2i+1 is usually
referred to as bubble function.
œï2
œï1
œï0
Œæ
0
0.5
1
FIGURE 12.5. Basis functions of X2
h on the reference interval
So far, we have considered only lagrangian-type shape functions. If this
constraint is removed other kind of bases can be derived. A notable example
(on the reference interval) is given by
œà0(Œæ) = 1 ‚àíŒæ,
œà1(Œæ) = (1 ‚àíŒæ)Œæ,
œà2(Œæ) = Œæ.
(12.66)
This basis is called hierarchical since it is generated using the shape func-
tions of the subspace having an immediately lower dimension than X2
h (i.e.
X1
h). Precisely, the bubble function œà1 ‚ààX2
h is added to the shape func-
tions œà0 and œà2 which belong to X1
h. Hierarchical bases can be of some
interest in numerical computations if the local degree of the interpolation
is adaptively increased (p-type adaptivity).
To check that (12.66) forms a basis for X2
h we must verify that its func-
tions are linearly independent, i.e.
Œ±0 œà0(Œæ) + Œ±1 œà1(Œæ) + Œ±2 œà2(Œæ) = 0,
‚àÄŒæ ‚àà[0, 1]
‚áî
Œ±0 = Œ±1 = Œ±2 = 0.
In our case this holds true since if
2

i=0
Œ±i œài(Œæ) = Œ±0 + Œæ(Œ±1 ‚àíŒ±0 + Œ±2) ‚àíŒ±1Œæ2 = 0,
‚àÄŒæ ‚àà[0, 1]
then necessarily Œ±0 = 0, Œ±1 = 0 and thus Œ±2 = 0.

556
12. Two-Point Boundary Value Problems
A procedure analogous to that examined in the sections above can be used
in principle to construct a basis for every subspace Xk
h with k being arbi-
trary. However, it is important to remember that an increase in the degree k
of the polynomial approximation gives rise to an increase of the number of
degrees of freedom of the FEM and, as a consequence, of the computational
cost required for the solution of the linear system (12.48).
Let us now examine the structure and the basic properties of the stiÔ¨Äness
matrix associated with system (12.48) in the case of the Ô¨Ånite element
method (AG = Afe).
Since the Ô¨Ånite element basis functions for Xk
h have a local support, Afe is
sparse. In the particular case k = 1, the support of the shape function œïi is
the union of the intervals Ii‚àí1 and Ii if 1 ‚â§i ‚â§n‚àí1, and it coincides with
the interval I0 (respectively In‚àí1) if i = 0 (resp., i = n). As a consequence,
for a Ô¨Åxed i = 1, . . . , n ‚àí1, only the shape functions œïi‚àí1 and œïi+1 have a
nonvanishing support intersection with that of œïi, which implies that Afe
is tridiagonal since aij = 0 if j Ã∏‚àà{i ‚àí1, i, i + 1}. In the case k = 2 one
concludes with an analogous argument that Afe is a pentadiagonal matrix.
The condition number of Afe is a function of the grid size h; indeed,
K2(Afe) = ‚à•Afe‚à•2‚à•A‚àí1
fe ‚à•2 = O(h‚àí2)
(for the proof, see [QV94], Section 6.3.2), which demonstrates that the
conditioning of the Ô¨Ånite element system (12.48) grows rapidly as h ‚Üí
0. This is clearly conÔ¨Çicting with the need of increasing the accuracy of
the approximation and, in multidimensional problems, demands suitable
preconditioning techniques if iterative solvers are used (see Section 4.3.2).
Remark 12.4 (Elliptic problems of higher order) The Galerkin me-
thod in general, and the Ô¨Ånite element method in particular, can also be
applied to other type of elliptic equations, for instance to those of fourth
order. In that case, the numerical solution (as well as the test functions)
should be continuous together with their Ô¨Årst derivative. An example has
been illustrated in Section 8.8.1.
‚ñ†
12.4.6
Implementation Issues
In this section we implement the Ô¨Ånite element (FE) approximation with
piecewise linear elements (k = 1) of the boundary value problem (12.41)
(shortly, BVP) with non homogeneous Dirichlet boundary conditions.
Here is the list of the input parameters of Program 94: Nx is the number
of grid subintervals; I is the interval [a, b], alpha, beta, gamma and f are
the macros corresponding to the coeÔ¨Écients in the equation, bc=[ua,ub]
is a vector containing the Dirichlet boundary conditions for u at x = a and
x = b and stabfun is an optional string variable. It can assume diÔ¨Äerent

12.4 The Galerkin Method
557
values, allowing the user to select the desired type of artiÔ¨Åcial viscosity that
may be needed for dealing with the problems addressed in Section 12.5.
Program 94 - ellfem : Linear FE for two-point BVPs
function [uh,x] = ellfem(Nx,I,alpha,beta,gamma,f,bc,stabfun)
a = I(1); b = I(2); h = (b-a)/Nx; x = [a+h/2:h:b-h/2];
alpha = eval(alpha); beta = eval(beta); gamma = eval(gamma);
f
= eval(f);
rhs = 0.5*h*(f(1:Nx-1)+f(2:Nx));
if nargin == 8
[Afe,rhsbc] = femmatr(Nx,h,alpha,beta,gamma,stabfun);
else
[Afe,rhsbc] = femmatr(Nx,h,alpha,beta,gamma);
end
[L,U,P]
= lu(Afe);
rhs(1)
= rhs(1)-bc(1)*(-alpha(1)/h-beta(1)/2+h*gamma(1)/3+rhsbc(1));
rhs(Nx-1) = rhs(Nx-1)-bc(2)*(-alpha(Nx)/h+beta(Nx)/2+h*gamma(Nx)/3+rhsbc(2));
rhs = P*rhs‚Äô; z = L \ rhs;
w = U \ z;
uh = [bc(1), w‚Äô, bc(2)]; x = [a:h:b];
Program 95 computes the stiÔ¨Äness matrix Afe; with this aim, the coeÔ¨Écients
Œ±, Œ≤ and Œ≥ and the forcing term f are replaced by piecewise constant
functions on each mesh subinterval and the remaining integrals in (12.41),
involving the basis functions and their derivatives, are evaluated exactly.
Program 95 - femmatr : Construction of the stiÔ¨Äness matrix
function [Afe,rhsbc] = femmatr(Nx,h,alpha,beta,gamma,stabfun)
for i=2:Nx
dd(i-1)=(alpha(i-1)+alpha(i))/h; dc(i-1)=-(beta(i)-beta(i-1))/2;
dr(i-1)=h*(gamma(i-1)+gamma(i))/3;
if i > 2
ld(i-2) = -alpha(i-1)/h; lc(i-2)=-beta(i-1)/2;
lr(i-2) = h*gamma(i-1)/6;
end
if i < Nx
ud(i-1) = -alpha(i)/h; uc(i-1)=beta(i)/2;
ur(i-1) = h*gamma(i)/6;
end
end
Kd=spdiags([[ld 0]‚Äô,dd‚Äô,[0 ud]‚Äô],-1:1,Nx-1,Nx-1);
Kc=spdiags([[lc 0]‚Äô,dc‚Äô,[0 uc]‚Äô],-1:1,Nx-1,Nx-1);
Kr=spdiags([[lr 0]‚Äô,dr‚Äô,[0 ur]‚Äô],-1:1,Nx-1,Nx-1);
Afe=Kd+Kc+Kr;
if nargin == 6
s=[‚Äô[Ks,rhsbc]=‚Äô,stabfun,‚Äô(Nx,h,alpha,beta);‚Äô]; eval(s)
Afe = Afe + Ks;
else

558
12. Two-Point Boundary Value Problems
rhsbc = [0, 0];
end
The H1-norm of the error can be computed by calling Program 96, which
must be supplied by the macros u and ux containing the expression of the
exact solution u and of u‚Ä≤. The computed numerical solution is stored in
the output vector uh, while the vector coord contains the grid coordinates
and h is the mesh size. The integrals involved in the computation of the
H1-norm of the error are evaluated using the composite Simpson formula
(9.17).
Program 96 - H1error : Computation of the H1-norm of the error
function [L2err,H1err]=H1error(coord,h,uh,u,udx)
nvert=max(size(coord)); x=[]; k=0;
for i = 1:nvert-1
xm = (coord(i+1)+coord(i))*0.5;
x = [x, coord(i),xm];
k = k + 2;
end
ndof = k+1; x (ndof) = coord (nvert);
uq = eval(u); uxq = eval(udx); L2err = 0; H1err = 0;
for i=1:nvert-1
L2err = L2err + (h/6)*((uh(i)-uq(2*i-1))ÀÜ2+...
4*(0.5*uh(i)+0.5*uh(i+1)-uq(2*i))ÀÜ2+(uh(i+1)-uq(2*i+1))ÀÜ2);
H1err = H1err + (1/(6*h))*((uh(i+1)-uh(i)-h*uxq(2*i-1))ÀÜ2+...
4*(uh(i+1)-uh(i)-h*uxq(2*i))ÀÜ2+(uh(i+1)-uh(i)-h*uxq(2*i+1))ÀÜ2);
end
H1err = sqrt(H1err + L2err); L2err = sqrt(L2err);
Example 12.1 We assess the accuracy of the Ô¨Ånite element solution of the fol-
lowing problem. Consider a thin rod of length L whose temperature at x = 0 is
Ô¨Åxed to t0 while the other endpoint x = L is thermally isolated. Assume that the
rod has a cross-section with constant area equal to A and that the perimeter of
A is p.
The temperature u of the rod at a generic point x ‚àà(0, L) is governed by the
following boundary value problem with mixed Dirichlet-Neumann conditions
Ô£±
Ô£≤
Ô£≥
‚àí¬µAu‚Ä≤‚Ä≤ + œÉpu = 0
x ‚àà(0, L),
u(0) = u0,
u‚Ä≤(L) = 0,
(12.67)
where ¬µ denotes the thermal conductivity and œÉ is the convective transfer coef-
Ô¨Åcient. The exact solution of the problem is the (smooth) function
u(x) = u0 cosh[m(L ‚àíx)]
cosh(mL)
,
where m =

œÉp/¬µA. We solve the problem by using linear and quadratic Ô¨Ånite
elements (k = 1 and k = 2) on a grid with uniform size. In the numerical

12.4 The Galerkin Method
559
computations we assume that the length of the rod is L = 100cm and that the
rod has a circular cross-section of radius 2cm (and thus, A = 4œÄcm2, p = 4œÄcm).
We also set u0 = 10‚ó¶C, œÉ = 2 and ¬µ = 200.
Figure 12.6 (left) shows the behavior of the error in the L2 and H1 norms for
the linear and quadratic elements, respectively. Notice the excellent agreement
between the numerical results and the expected theoretical estimates (12.59) and
(12.60), i.e., the orders of convergence in the L2 norm and the H1 norm tend
respectively to k + 1 and k if Ô¨Ånite elements of degree k are employed, since the
exact solution is smooth.
‚Ä¢
10
‚àí1
10
0
10
1
10
‚àí6
10
‚àí5
10
‚àí4
10
‚àí3
10
‚àí2
10
‚àí1
10
0
10
1
10
20
30
40
50
60
10
‚àí14
10
‚àí12
10
‚àí10
10
‚àí8
10
‚àí6
10
‚àí4
10
‚àí2
10
0
10
2
FIGURE 12.6. Left: error curves for linear and quadratic elements. The dashed
and solid lines denote the H1(0, L) and L2(0, L) norms of the error in the case
k = 1, while the dot-line and dotted line denote the corresponding norms in the
case k = 2. Right: error curves for the spectral collocation method. The dashed
and solid lines denote the H1(0, L) and L2(0, L) norms of the error, respectively
12.4.7
Spectral Methods
It turns out that the spectral collocation method of Section 12.3 can be
regarded as a Galerkin method where the subspace is P0
n and the integrals
are approximated by the Gauss-Lobatto quadrature formula. As a matter
of fact, the approximation of problem (12.38) is
Ô¨Ånd un ‚ààP0
n : an(un, vn) = (f, vn)n ‚àÄvn ‚ààP0
n,
(12.68)
where an is the bilinear form that is obtained from the bilinear form a
by replacing exact integrals by the Gauss-Lobatto formula (12.37). For
problem (12.41) the associated bilinear form a was introduced in (12.44).
We would therefore obtain
an(un, vn) = (Œ±u‚Ä≤
n, v‚Ä≤
n)n + (Œ≤u‚Ä≤
n, vn)n + (Œ≥un, vn)n.
(12.69)
This is no longer a Galerkin method, but is called a generalized Galerkin
approximation. Its analysis requires more care than for Galerkin methods,

560
12. Two-Point Boundary Value Problems
as already seen in Section 12.3 and depends on the Strang lemma (see
[QV94]). However, the same kind of error estimate (12.39) can be proven
in this case as well.
A further generalization combining the Ô¨Ånite element approach with
piecewise polynomials of high degree and Gauss-Lobatto integration on
each element yields the so-called spectral element method and the h ‚àíp
version of the Ô¨Ånite element method (here p stands for the polynomial de-
gree that we have denoted with n). In these cases convergence is achieved
letting simultaneously (or independently) h go to zero and p go to inÔ¨Ånity.
(See, e.g., [BM92], [SS98]).
Example 12.2 We consider again the two-point boundary value problem (12.67)
and employ the spectral collocation method for its numerical approximation.
We show in Figure 12.6 (right) the error curves in the L2(0, L) (solid line) and
H1(0, L) (dashed line) norms as functions of the spectral degree n, with n = 4‚àík,
k = 1, . . . , 5. Notice the high accuracy that is achieved, even when a small value
of n is used, due to the smoothness of the exact solution. Notice also that for
n ‚â•32 the accuracy is actually bounded by the eÔ¨Äect of rounding errors.
‚Ä¢
12.5
Advection-DiÔ¨Äusion Equations
Boundary value problems of the form (12.41) are used to describe processes
of diÔ¨Äusion, advection and absorption (or reaction) of a certain quantity
which is identiÔ¨Åed with u(x). The term ‚àí(Œ±u‚Ä≤)‚Ä≤ is responsible for the diÔ¨Äu-
sion, Œ≤u‚Ä≤ for the advection (or transport), Œ≥u for the absorption (if Œ≥ > 0).
In this section we focus on the case where Œ± is small compared with Œ≤ (or
Œ≥). In these cases, the Galerkin method that we introduced earlier might
be unsuitable for providing accurate numerical results. A heuristic explana-
tion can be drawn from the inequality (12.56), noticing that in this case the
constant M/Œ±0 can be very large, hence the error estimate can be mean-
ingless unless h is much smaller than (M/Œ±0)‚àí1. For instance, if Œ± = Œµ,
Œ≥ = 0 and Œ≤ = const ‚â´1, then Œ±0 = Œµ and M = Œµ + CP Œ≤. Similarly, if
Œ± = Œµ, Œ≤ = 0 and Œ≥ = const ‚â´1 then Œ±0 = Œµ and M = Œµ + C2
P Œ≥.
To keep our analysis at the simplest possible level, we will consider the
following elementary two-point boundary value problem
" ‚àíŒµu‚Ä≤‚Ä≤ + Œ≤u‚Ä≤ = 0,
0 < x < 1,
u(0) = 0,
u(1) = 1,
(12.70)
where Œµ and Œ≤ are two positive constants such that Œµ/Œ≤ ‚â™1. Despite
its simplicity, (12.70) provides an interesting paradigm of an advection-
diÔ¨Äusion problem in which advection dominates diÔ¨Äusion.

12.5 Advection-DiÔ¨Äusion Equations
561
We deÔ¨Åne the global P¬¥eclet number as
Pegl = |Œ≤|L
2Œµ
(12.71)
where L is the size of the domain (equal to 1 in our case). The global P¬¥eclet
number measures the dominance of the advective term over the diÔ¨Äusive
one.
Let us Ô¨Årst compute the exact solution of problem (12.70). The charac-
teristic equation associated to the diÔ¨Äerential equation is ‚àíŒµŒª2 + Œ≤Œª = 0
and admits the roots Œª1 = 0 and Œª2 = Œ≤/Œµ. Then
u(x) = C1eŒª1x + C2eŒª2x = C1 + C2e
Œ≤
Œµ x,
where C1 and C2 are arbitrary constants. Imposing the boundary conditions
yields C1 = ‚àí1/(eŒ≤/Œµ ‚àí1) = ‚àíC2, therefore
u(x) = (exp(Œ≤x/Œµ) ‚àí1) / (exp(Œ≤/Œµ) ‚àí1) .
If Œ≤/Œµ ‚â™1 we can expand the exponentials up to Ô¨Årst order obtaining
u(x) = (1 + Œ≤
Œµ x + ¬∑ ¬∑ ¬∑ ‚àí1)/(1 + Œ≤
Œµ + ¬∑ ¬∑ ¬∑ ‚àí1) ‚âÉ(Œ≤ x/Œµ)/(Œ≤/Œµ) = x,
thus the solution is close to the solution of the limit problem ‚àíŒµu‚Ä≤‚Ä≤ = 0,
which is a straight line interpolating the boundary data.
However, if Œ≤/Œµ ‚â´1 the exponentials attain big values so that
u(x) ‚âÉexp(Œ≤/Œµx)
exp(Œ≤/Œµ) = exp

‚àíŒ≤
Œµ (1 ‚àíx)

.
Since the exponent is big and negative the solution is almost equal to zero
everywhere unless a small neighborhood of the point x = 1 where the
term 1 ‚àíx becomes very small and the solution joins the value 1 with an
exponential behaviour. The width of the neighbourhood is of the order of
Œµ/Œ≤ and thus it is quite small: in such an event, we say that the solution
exhibits a boundary layer of width O (Œµ/Œ≤) at x = 1.
12.5.1
Galerkin Finite Element Approximation
Let us discretize problem (12.70) using the Galerkin Ô¨Ånite element method
introduced in Section 12.4.5 with k = 1 (piecewise linear Ô¨Ånite elements).
The approximation to the problem is: Ô¨Ånd uh ‚ààX1
h such that
Ô£±
Ô£≤
Ô£≥
a(uh, vh) = 0
‚àÄvh ‚ààX1,0
h ,
uh(0) = 0, uh(1) = 1,
(12.72)

562
12. Two-Point Boundary Value Problems
where the Ô¨Ånite element spaces X1
h and X1,0
h
have been introduced in (8.22)
and (12.57) and the bilinear form a(¬∑, ¬∑) is
a(uh, vh) =
> 1
0
(Œµu‚Ä≤
hv‚Ä≤
h + Œ≤u‚Ä≤
hvh) dx.
(12.73)
Remark 12.5 (Advection-diÔ¨Äusion problems in conservation form)
Sometimes, the advection-diÔ¨Äusion problem (12.70) is written in the follow-
ing conservation form
" ‚àí(J(u))‚Ä≤ = 0,
0 < x < 1,
u(0) = 0,
u(1) = 1,
(12.74)
where J(u) = Œµu‚Ä≤‚àíŒ≤u is the Ô¨Çux (already introduced in the Ô¨Ånite diÔ¨Äerence
context in Section 12.2.3), Œµ and Œ≤ are given functions with Œµ(x) ‚â•Œµ0 > 0
for all x ‚àà[0, 1]. The Galerkin approximation of (12.74) using piecewise
linear Ô¨Ånite elements reads: Ô¨Ånd uh ‚ààX1
h such that
b(uh, vh) = 0,
‚àÄvh ‚ààX1,0
h
where b(uh, vh) =
1
>
0
(Œµu‚Ä≤
h ‚àíŒ≤uh)v‚Ä≤
h dx. The bilinear form b(¬∑, ¬∑) coincides
with the corresponding one in (12.73) when Œµ and Œ≤ are constant.
‚ñ†
Taking vh as a test function the generic basis function œïi, (12.72) yields
1
>
0
Œµu‚Ä≤
hœï‚Ä≤
i dx +
1
>
0
Œ≤u‚Ä≤
hœïi dx = 0
i = 1, . . . , n ‚àí1.
Setting uh(x) = n
j=0 ujœïj(x), and noting that supp(œïi) = [xi‚àí1, xi+1] the
above integral, for i = 1, . . . , n ‚àí1, reduces to
Œµ
Ô£Æ
Ô£∞ui‚àí1
xi
>
xi‚àí1
œï‚Ä≤
i‚àí1œï‚Ä≤
i dx + ui
xi+1
>
xi‚àí1
(œï‚Ä≤
i)2 dx + ui+1
xi+1
>
xi
œï‚Ä≤
iœï‚Ä≤
i+1 dx
Ô£π
Ô£ª
+Œ≤
Ô£Æ
Ô£∞ui‚àí1
xi
>
xi‚àí1
œï‚Ä≤
i‚àí1œïi dx + ui
xi+1
>
xi‚àí1
œï‚Ä≤
iœïi dx + ui+1
xi+1
>
xi
œï‚Ä≤
i+1œïi dx
Ô£π
Ô£ª= 0.
Assuming a uniform partition of [0, 1] with xi = xi‚àí1 + h for i = 1, . . . , n,
h = 1/n, and noting that œï‚Ä≤
j(x) =
1
h if xj‚àí1 ‚â§x ‚â§xj, œï‚Ä≤
j(x) = ‚àí1
h if
xj ‚â§x ‚â§xj+1, we deduce that
Œµ
h (‚àíui‚àí1 + 2ui ‚àíui+1) + 1
2Œ≤ (ui+1 ‚àíui‚àí1) = 0,
i = 1, . . . , n ‚àí1.
(12.75)

12.5 Advection-DiÔ¨Äusion Equations
563
Multiplying by h/Œµ and deÔ¨Åning the local P¬¥eclet number to be
Pe = |Œ≤|h
2Œµ
we Ô¨Ånally obtain
(Pe ‚àí1) ui+1 + 2ui ‚àí(Pe + 1) ui‚àí1 = 0.
i = 1, . . . , n ‚àí1.
(12.76)
This is a linear diÔ¨Äerence equation which admits a solution of the form
ui = A1œÅi
1 + A2œÅi
2 for suitable constants A1, A2 (see Section 11.4), where
œÅ1 and œÅ2 are the two roots of the following characteristic equation
(Pe ‚àí1) œÅ2 + 2œÅ ‚àí(Pe + 1) = 0.
Thus
œÅ1,2 = ‚àí1 ¬±
‚àö
1 + Pe2 ‚àí1
Pe ‚àí1
=
Ô£±
Ô£≤
Ô£≥
1 + Pe
1 ‚àíPe,
1.
Imposing the boundary conditions at x = 0 and x = 1 gives
A1 = 1/(1 ‚àí
+
1+Pe
1‚àíPe
,n
),
A2 = ‚àíA1,
so that the solution of (12.76) is
ui =

1 ‚àí
1 + Pe
1 ‚àíPe
i
/

1 ‚àí
1 + Pe
1 ‚àíPe
n
i = 0, . . . , n.
We notice that if Pe > 1, a power with a negative base appears at the
numerator which gives rise to an oscillating solution. This is clearly visible
in Figure 12.7 where for several values of the local P¬¥eclet number, the
solution of (12.76) is compared with the exact solution (sampled at the
mesh nodes) corresponding to a value of the global P¬¥eclet equal to 50.
The simplest remedy for preventing the oscillations consists of choosing
a suÔ¨Éciently small mesh stepsize h in such a way that Pe < 1. However this
approach is often impractical: for example, if Œ≤ = 1 and Œµ = 5 ¬∑ 10‚àí5 one
should take h < 10‚àí4 which amounts to dividing [0, 1] into 10000 subin-
tervals, a strategy that becomes unfeasible when dealing with multidimen-
sional problems. Other strategies can be pursued, as will be addressed in
the next sections.
12.5.2
The Relationship between Finite Elements and Finite
DiÔ¨Äerences; the Numerical Viscosity
To examine the behaviour of the Ô¨Ånite diÔ¨Äerence method (FD) when applied
to the solution of advection-diÔ¨Äusion problems and its relationship with the

564
12. Two-Point Boundary Value Problems
0.75
0.8
0.85
0.9
0.95
1
‚àí0.5
0
0.5
1
FIGURE 12.7. Finite diÔ¨Äerence solution of the advection-diÔ¨Äusion problem
(12.70) (with Pegl = 50) for several values of the local P¬¥eclet number. Solid
line: exact solution, dot-dashed line: Pe = 2.63, dotted line: Pe = 1.28, dashed
line: Pe = 0.63
Ô¨Ånite element method (FE), we again consider the one-dimensional problem
(12.70) with a uniform meshsize h.
To ensure that the local discretization error is of second order we approx-
imate u‚Ä≤(xi) and u‚Ä≤‚Ä≤(xi), i = 1, . . . , n ‚àí1, by the centred Ô¨Ånite diÔ¨Äerences
(10.61) and (10.65) respectively (see Section 10.10.1). We obtain the fol-
lowing FD problem
Ô£±
Ô£≤
Ô£≥
‚àíŒµui+1 ‚àí2ui + ui‚àí1
h2
+ Œ≤ ui+1 ‚àíui‚àí1
2h
= 0,
i = 1, . . . , n ‚àí1
u0 = 0,
un = 1.
(12.77)
If we multiply by h for every i = 1, . . . , n ‚àí1, we obtain exactly the same
equation (12.75) that was obtained using piecewise linear Ô¨Ånite elements.
The equivalence between FD and FE can be proÔ¨Åtably employed to de-
vise a cure for the oscillations arising in the approximate solution of (12.75)
when the local P¬¥eclet number is larger than 1. The important observation
here is that the instability in the FD solution is due to the fact that the
discretization scheme is a centred one. A possible remedy consists of ap-
proximating the Ô¨Årst derivative by a one-sided Ô¨Ånite diÔ¨Äerence according
to the direction of the transport Ô¨Åeld. Precisely, we use the backward dif-
ference if the convective coeÔ¨Écient Œ≤ is positive and the forward diÔ¨Äerence
otherwise. The resulting scheme when Œ≤ > 0 is
‚àíŒµui+1 ‚àí2ui + ui‚àí1
h2
+ Œ≤ ui ‚àíui‚àí1
h
= 0
i = 1, . . . , n ‚àí1
(12.78)
which, for Œµ = 0 reduces to ui = ui‚àí1 and therefore yields the desired
constant solution of the limit problem Œ≤u‚Ä≤ = 0. This one-side discretization

12.5 Advection-DiÔ¨Äusion Equations
565
of the Ô¨Årst derivative is called upwind diÔ¨Äerencing: the price to be paid for
the enhanced stability is a loss of accuracy since the upwind Ô¨Ånite diÔ¨Äerence
introduces a local discretization error of O(h) and not of O(h2) as happens
using centred Ô¨Ånite diÔ¨Äerences.
Noting that
ui ‚àíui‚àí1
h
= ui+1 ‚àíui‚àí1
2h
‚àíh
2
ui+1 ‚àí2ui + ui‚àí1
h2
,
the upwind Ô¨Ånite diÔ¨Äerence can be interpreted as the sum of a centred Ô¨Ånite
diÔ¨Äerence approximating the Ô¨Årst derivative and of a term proportional to
the discretization of the second-order derivative. Consequently, (12.78) is
equivalent to
‚àíŒµh
ui+1 ‚àí2ui + ui‚àí1
h2
+ Œ≤ ui+1 ‚àíui‚àí1
2h
= 0
i = 1, . . . , n ‚àí1
(12.79)
where Œµh = Œµ(1 + Pe). This amounts to having replaced the diÔ¨Äerential
equation (12.72) with the perturbed one
‚àíŒµhu‚Ä≤‚Ä≤ + Œ≤u‚Ä≤ = 0,
(12.80)
then using centred Ô¨Ånite diÔ¨Äerences to approximate both u‚Ä≤ and u‚Ä≤‚Ä≤. The
perturbation
‚àíŒµ Pe u‚Ä≤‚Ä≤ = ‚àíŒ≤h
2
u‚Ä≤‚Ä≤
(12.81)
is called the numerical viscosity (or artiÔ¨Åcial diÔ¨Äusion). In Figure 12.8
a comparison between centred and upwinded discretizations of problem
(12.72) is shown.
More generally, we can resort to a centred scheme of the form (12.80)
with the following viscosity
Œµh = Œµ(1 + œÜ(Pe))
(12.82)
where œÜ is a suitable function of the local P¬¥eclet number satisfying
lim
t‚Üí0+œÜ(t) = 0.
Notice that when œÜ(t) = 0 for all t, one recovers the centred Ô¨Ånite diÔ¨Äerence
method (12.77), while if œÜ(t) = t the upwind Ô¨Ånite diÔ¨Äerence scheme (12.78)
(or, equivalently, (12.79)) is obtained. Other choices are admissible as well.
For instance, taking
œÜ(t) = t ‚àí1 + B(2t)
(12.83)
where B(t) is the Bernoulli function deÔ¨Åned as B(t) = t/(et ‚àí1) for t Ã∏= 0
and B(0) = 1, yields the so called exponential Ô¨Åtting Ô¨Ånite diÔ¨Äerence scheme
which is also well known as the Scharfetter-Gummel (SG) method [SG69].

566
12. Two-Point Boundary Value Problems
0.7
0.75
0.8
0.85
0.9
0.95
1
‚àí0.4
‚àí0.2
0
0.2
0.4
0.6
0.8
1
1.2
FIGURE 12.8. Finite diÔ¨Äerence solution of (12.72) (with Œµ = 1/100) using a
centred discretization (dashed and dotted lines) and the artiÔ¨Åcial viscosity (12.81)
(dot-dashed and starred lines). The solid line denotes the exact solution. Notice
the eÔ¨Äect of eliminating the oscilations when the local P¬¥eclet number is large;
conversely, notice also the corresponding loss of accuracy for low values of the
local P¬¥eclet number
0
0.5
1
1.5
2
2.5
0
0.5
1
1.5
2
2.5
FIGURE 12.9. The functions œÜUP (dash-dotted line) and œÜSG (solid line)
Remark 12.6 Denoting respectively by œÜC, œÜUP and œÜSG the previous
three functions, i.e. œÜC = 0, œÜUP (t) = t and œÜSG(t) = t ‚àí1 + B(2t) (see
Figure 12.9), we notice that œÜSG ‚âÉœÜUP as Pe ‚Üí+‚àûwhile œÜSG = O(h2)
and œÜUP = O(h) if Pe ‚Üí0+. Therefore, the SG method is second-order
accurate with respect to h and for this reason it is an optimal viscosity
upwind method. Actually, one can show (see [HGR96], pp. 44-45) that
if f is piecewise constant over the grid partition the SG scheme yields a
numerical solution uSG
h
which is nodally exact, i.e., uSG
h (xi) = u(xi) for
each node xi, irrespectively of h (and, thus, of the size of the local P¬¥eclet
number). This is demonstrated in Figure 12.10.
‚ñ†

12.5 Advection-DiÔ¨Äusion Equations
567
0.9
0.95
1
0
0.5
1
FIGURE 12.10. Comparison between the numerical solutions of problem (12.72)
(with Œµ = 1/200) obtained using the artiÔ¨Åcial viscosity (12.81) (dashed line where
the symbol ‚ñ†denotes the nodal values) and with the optimal viscosity (12.83)
(dotted line where the symbol ‚ó¶denotes the nodal values) in the case where
Pe = 1.25. The solid line denotes the exact solution
The new local P¬¥eclet number associated with the scheme (12.79)-(12.82) is
deÔ¨Åned as
Pe‚àó= |Œ≤|h
2Œµh
=
Pe
(1 + œÜ(Pe)).
For both the upwind and the SG schemes we have Pe‚àó< 1 for any value
of h. This implies that the matrix associated with these methods is a M-
matrix for any h (see DeÔ¨Ånition 1.25 and Exercise 13), and, in turn, that the
numerical solution uh satisÔ¨Åes a discrete maximum principle (see Section
12.2.2).
12.5.3
Stabilized Finite Element Methods
In this section we extend the use of numerical viscosity introduced in
the previous section for Ô¨Ånite diÔ¨Äerences to the Galerkin method using
Ô¨Ånite elements of arbitrary degree k ‚â•1. For this purpose we consider
the advection-diÔ¨Äusion problem (12.70) where the viscosity coeÔ¨Écient Œµ is
replaced by (12.82). This yields the following modiÔ¨Åcation of the original
Galerkin problem (12.72):
Ô¨Ånd
0uh‚ààXk,0
h
=

vh ‚ààXk
h : vh(0) = vh(1) = 0

such that
ah(
0uh, vh) = ‚àí
1
>
0
Œ≤vh dx
‚àÄvh ‚ààXk,0
h ,
(12.84)
where
ah(u, v) = a(u, v) + b(u, v),

568
12. Two-Point Boundary Value Problems
and
b(u, v) = Œµ œÜ(Pe)
1
>
0
u‚Ä≤v‚Ä≤ dx
is called the stabilization term. Since ah(v, v) = Œµh|v|2
1 for all v ‚ààH1
0(0, 1)
and Œµh/Œµ = (1 + œÜ(Pe)) ‚â•1, the modiÔ¨Åed problem (12.84) enjoys more
favorable monotonicity properties than the corresponding non stabilized
Galerkin formulation (12.75).
To prove convergence, it is suÔ¨Écient to show that
0uh tends to
0u as h ‚Üí0,
where
0u (x) = u(x) ‚àíx. This is done in the following theorem, where we
assume that
0u (and henceforth u) has the required regularity.
Theorem 12.4 If k = 1 then
|
0u ‚àí
0uh |H1(0,1) ‚â§Ch G(
0u)
(12.85)
where C > 0 is a suitable constant independent of h and
0u, and
G(
0u) =
Ô£±
Ô£¥
Ô£≤
Ô£¥
Ô£≥
|
0u |H1(0,1) + |
0u |H2(0,1)
for the upwind method
|
0u |H2(0,1)
for the SG method.
Further, if k = 2 the SG method yields the improved error estimate
|
0u ‚àí
0uh |H1(0,1) ‚â§Ch2(|
0u |H1(0,1) + |
0u |H3(0,1)).
(12.86)
Proof. From (12.70) we obtain
a(
0u, vh) = ‚àí
1
>
0
Œ≤vhdx,
‚àÄvh ‚ààXk,0
h .
By comparison with (12.84) we get
ah(
0u ‚àí
0uh, vh) = b(
0u, vh),
‚àÄvh ‚ààXk,0
h .
(12.87)
Denote by Eh =
0u ‚àí
0uh the discretization error and recall that the space H1
0(0, 1)
is endowed with the norm (12.49). Then,
Œµh|Eh|2
H1(0,1)
= ah(Eh, Eh) = ah(Eh,
0u ‚àíŒ†k
h
0u) + ah(Eh, Œ†k
h
0u ‚àí
0uh)
= ah(Eh,
0u ‚àíŒ†k
h
0u) + b(
0u, Œ†k
h
0u ‚àí
0uh)
where we have applied (12.87) with vh = Œ†k
h
0u ‚àí
0uh. Using the Cauchy-Schwarz
inequality we get
Œµh|Eh|2
H1(0,1)
‚â§Mh|Eh|H1(0,1)|
0u ‚àíŒ†k
h
0u |H1(0,1)
+ŒµœÜ(Pe)
1
>
0
0
u‚Ä≤(Œ†k
h
0u ‚àí
0uh)‚Ä≤dx
(12.88)

12.5 Advection-DiÔ¨Äusion Equations
569
where Mh = Œµh + |Œ≤|CP is the continuity constant of the bilinear form ah(¬∑, ¬∑)
and CP is the Poincar¬¥e constant introduced in (12.16).
Notice that if k = 1 (corresponding to piecewise linear Ô¨Ånite elements) and
œÜ = œÜSG (SG optimal viscosity) the quantity in the second integral is identically
zero since
0uh= Œ†1
h
0u, as pointed out in Remark 12.6. Then, from (12.88) we get
|Eh|H1(0,1) ‚â§

1 + |Œ≤|CP
Œµh

|
0u ‚àíŒ†1
h
0u |H1(0,1).
Noting that Œµh > Œµ, using (12.71) and the interpolation estimate (8.27), we Ô¨Ånally
obtain the error bound
|Eh|H1(0,1) ‚â§C(1 + 2PeglCP )h|
0u |H2(0,1).
In the general case the error inequality (12.88) can be further manipulated. Using
the Cauchy-Schwarz and triangular inequalities we obtain
1
>
0
0
u‚Ä≤(Œ†k
h
0u ‚àí
0uh)‚Ä≤dx ‚â§|
0u |H1(0,1)(|Œ†k
h
0u ‚àí
0u |H1(0,1) + |Eh|H1(0,1))
from which
Œµh|Eh|2
H1(0,1)
‚â§|Eh|H1(0,1)
+
Mh|
0u ‚àíŒ†k
h
0u |H1(0,1)
+ŒµœÜ(Pe)|
0u |H1(0,1)
,
+ ŒµœÜ(Pe)|
0u |H1(0,1)|
0u ‚àíŒ†k
h
0u |H1(0,1).
Using again the interpolation estimate (8.27) yields
Œµh|Eh|2
H1(0,1)
‚â§|Eh|H1(0,1)
+
MhChk|
0u |Hk+1(0,1) + ŒµœÜ(Pe)|
0u |H1(0,1)
,
+CŒµœÜ(Pe)|
0u |H1(0,1)hk|
0u |Hk+1(0,1).
Using Young‚Äôs inequality (12.40) gives
Œµh|Eh|2
H1(0,1)
‚â§
Œµh|Eh|2
H1(0,1)
2
+ 3
4Œµh
6
(MhChk|
0u |Hk+1(0,1))2 + (ŒµœÜ(Pe)|
0u |H1(0,1))27
from which it follows that
|Eh|2
H1(0,1)
‚â§3
2
 Mh
Œµh
2
C2h2k|
0u |2
Hk+1(0,1) + 3
2
 Œµ
Œµh
2
œÜ(Pe)2|
0u |2
H1(0,1)
+2Œµ
Œµh œÜ(Pe)|
0u |H1(0,1)Chk|
0u |Hk+1(0,1).
Again using the fact that Œµh > Œµ and the deÔ¨Ånition (12.71) we get (Mh/Œµh) ‚â§
(1 + 2CP Pegl) and then
|Eh|2
H1(0,1)
‚â§3
2C2(1 + 2CP Pegl)2h2k|
0u |2
Hk+1(0,1)
+2œÜ(Pe)Chk|
0u |H1(0,1)|
0u |Hk+1(0,1) + 3
2œÜ(Pe)2|
0u |2
H1(0,1),

570
12. Two-Point Boundary Value Problems
which can be bounded further as
|Eh|2
H1(0,1)
‚â§
M
6
h2k|
0u |2
Hk+1(0,1)
+
œÜ(Pe)hk|
0u |H1(0,1)|
0u |Hk+1(0,1) + œÜ(Pe)2|
0u |2
H1(0,1)
7
(12.89)
for a suitable positive constant M.
If œÜUP = CŒµh, where CŒµ = Œ≤/Œµ, we obtain
|Eh|2
H1(0,1)
‚â§Ch2 6
h2k‚àí2|
0u |2
Hk+1(0,1)
+hk‚àí1|
0u |H1(0,1)|
0u |Hk+1(0,1) + |
0u |2
H1(0,1)
7
which shows that using piecewise linear Ô¨Ånite elements (i.e., k = 1) plus the
upwind artiÔ¨Åcial viscosity gives the linear convergence estimate (12.85).
In the case œÜ = œÜSG, assuming that for h suÔ¨Éciently small œÜSG ‚â§Kh2, for a
suitable positive constant K, we get
|Eh|2
H1(0,1)
‚â§Ch4 6
h2(k‚àí2)|
0u |2
Hk+1(0,1)
+hk‚àí2|
0u |H1(0,1)|
0u |Hk+1(0,1) + |
0u |2
H1(0,1)
7
which shows that using quadratic Ô¨Ånite elements (i.e., k = 2) plus the optimal
artiÔ¨Åcial viscosity gives the second-order convergence estimate (12.86).
3
Programs 97 and 98 implement the computation of the artiÔ¨Åcial and opti-
mal artiÔ¨Åcial viscosities (12.81) and (12.83), respectively. These viscosities
can be selected by the user setting the input parameter stabfun in Pro-
gram 94 equal to artvisc or sgvisc. The function sgvisc employs the
function bern to evaluate the Bernoulli function in (12.83).
Program 97 - artvisc : ArtiÔ¨Åcial viscosity
function [Kupw,rhsbc] = artvisc(Nx, h, nu, beta)
Peclet=0.5*h*abs(beta);
for i=2:Nx,
dd(i-1)=(Peclet(i-1)+Peclet(i))/h;
if i > 2, ld(i-2)=-Peclet(i-1)/h; end
if i < Nx, ud(i-1)=-Peclet(i)/h;
end
end
Kupw=spdiags([[ld 0]‚Äô,dd‚Äô,[0 ud]‚Äô],-1:1,Nx-1,Nx-1);
rhsbc = - [Peclet(1)/h, Peclet(Nx)/h];
Program 98 - sgvisc : Optimal artiÔ¨Åcial viscosity
function [Ksg,rhsbc] = sgvisc(Nx, h, nu, beta)
Peclet=0.5*h*abs(beta)./nu;
[bp, bn]=bern(2*Peclet);
Peclet=Peclet-1+bp;

12.5 Advection-DiÔ¨Äusion Equations
571
for i=2:Nx,
dd(i-1)=(nu(i-1)*Peclet(i-1)+nu(i)*Peclet(i))/h;
if i > 2, ld(i-2)=-nu(i-1)*Peclet(i-1)/h; end
if i < Nx, ud(i-1)=-nu(i)*Peclet(i)/h;
end
end
Ksg=spdiags([[ld 0]‚Äô,dd‚Äô,[0 ud]‚Äô],-1:1,Nx-1,Nx-1);
rhsbc = - [nu(1)*Peclet(1)/h, nu(Nx)*Peclet(Nx)/h];
Program 99 - bern : Evaluation of the Bernoulli function
function [bp,bn]=bern(x)
xlim=1e-2; ax=abs(x);
if (ax == 0), bp=1.; bn=1.; return; end;
if (ax > 80),
if (x > 0), bp=0.; bn=x; return;
else,
bp=-x; bn=0.; return; end;
end;
if (ax > xlim),
bp=x/(exp(x)-1); bn=x+bp; return;
else
ii=1; fp=1.;fn=1.; df=1.; s=1.;
while (abs(df) > eps),
ii=ii+1; s=-s; df=df*x/ii;
fp=fp+df; fn=fn+s*df;
bp=1./fp; bn=1./fn;
end;
return;
end
Example 12.3 We use Program 94 supplied with Programs 97 and 98 for the
numerical approximation of problem (12.70) in the case Œµ = 10‚àí2. Figure 12.11
shows the convergence behavior as a function of log(h) of the Galerkin method
without (G) and with numerical viscosity (upwind (UP) and SG methods are
employed). The Ô¨Ågure shows the logarithmic plots of the L2(0, 1) and H1(0, 1)-
norms, where the solid line denotes the UP method and the dashed and dotted
lines denote the G and SG methods, respectively. It is interesting to notice that
the UP and SG schemes exhibit the same (linear) convergence rate as the pure
Galerkin method in the H1-norm, while the accuracy of the UP scheme in the
L2-norm deteriorates dramatically because of the eÔ¨Äect of the artiÔ¨Åcial viscosity
which is O(h). Conversely, the SG converges quadratically since the introduced
numerical viscosity is in this case O(h2) as h tends to zero.
‚Ä¢

572
12. Two-Point Boundary Value Problems
10‚àí3
10‚àí2
10‚àí1
10‚àí4
10‚àí3
10‚àí2
10‚àí1
100
101
102
||u ‚àí u h ||
L2(0,1)
||u ‚àí u h ||
H1(0,1)
FIGURE 12.11. Convergence analysis for an advection-diÔ¨Äusion problem
12.6
A Quick Glance to the Two-Dimensional Case
The game that we want to play is to extend (in a few pages) the basic ideas
illustrated so far to the two-dimensional case. The obvious generalization of
problem (12.1)-(12.2) is the celebrated Poisson problem with homogeneous
Dirichlet boundary condition
" ‚àí‚ñ≥u = f
in ‚Ñ¶,
u = 0
on ‚àÇ‚Ñ¶,
(12.90)
where ‚ñ≥u = ‚àÇ2u/‚àÇx2 + ‚àÇ2u/‚àÇy2 is the Laplace operator and ‚Ñ¶is a two-
dimensional bounded domain whose boundary is ‚àÇ‚Ñ¶. If we allow ‚Ñ¶to be
the unit square ‚Ñ¶= (0, 1)2, the Ô¨Ånite diÔ¨Äerence approximation of (12.90)
that mimics (12.10) is
" Lhuh(xi,j) = f(xi,j)
for i, j = 1, . . . , N ‚àí1,
uh(xi,j) = 0
if i = 0 or N,
j = 0 or N,
(12.91)
where xi,j = (ih, jh) (h = 1/N > 0) are the grid points and uh is a grid
function. Finally, Lh denotes any consistent approximation of the operator
L = ‚àí‚ñ≥. The classical choice is
Lhuh(xi,j) = 1
h2 (4ui,j ‚àíui+1,j ‚àíui‚àí1,j ‚àíui,j+1 ‚àíui,j‚àí1) ,
(12.92)
where ui,j = uh(xi,j), which amounts to adopting the second-order centred
discretization of the second derivative (10.65) in both x and y directions
(see Figure 12.12, left).
The resulting scheme is known as the Ô¨Åve-point discretization of the
Laplacian. It is an easy (and useful) exercise for the reader to check that
the associated matrix Afd has (N ‚àí1)2 rows and is pentadiagonal, with the

12.6 A Quick Glance to the Two-Dimensional Case
573
xij xi,j+1
xi+1,j
xi,j‚àí1
xi‚àí1,j
FIGURE 12.12. Left: Ô¨Ånite diÔ¨Äerence grid and stencil for a squared domain.
Right: Ô¨Ånite element triangulation of a region around a hole
i-th row given by
(afd)ij = 1
h2
Ô£±
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£≥
4
if j = i,
‚àí1
if j = i ‚àíN ‚àí1, i ‚àí1, i + 1, i + N + 1,
0
otherwise.
(12.93)
Moreover, Afd is symmetric positive deÔ¨Ånite and it is also an M-matrix (see
Exercise 14). As expected, the consistency error associated with (12.92) is
second-order with respect to h, and the same holds for the discretization
error ‚à•u ‚àíuh‚à•h,‚àûof the method. More general boundary conditions than
the one considered in (12.90) can be dealt with by properly extending to
the two-dimensional case the mirror imaging technique described in Section
12.2.3 and in Exercise 10 (for a thorough discussion of this subject, see, e.g.,
[Smi85]).
The extension of the Galerkin method is (formally speaking) even more
straightforward and actually is still readable as (12.43) with, however, the
implicit understanding that both the function space Vh and the bilinear
form a(¬∑, ¬∑) be adapted to the problem at hand. The Ô¨Ånite element method
corresponds to taking
Vh =

vh ‚ààC0(‚Ñ¶) : vh|T ‚ààPk(T) ‚àÄT ‚ààTh, vh|‚àÇ‚Ñ¶= 0

,
(12.94)
where Th denotes here a triangulation of the domain ‚Ñ¶as previously intro-
duced in Section 8.5.2, while Pk (k ‚â•1) is the space of piecewise polyno-
mials deÔ¨Åned in (8.35). Note that ‚Ñ¶needs not to be a rectangular domain
(see Figure 12.12, right).

574
12. Two-Point Boundary Value Problems
As of the bilinear form a(¬∑, ¬∑), the same kind of mathematical manipula-
tions performed in Section 12.4.1 lead to
a(uh, vh) =
>
‚Ñ¶
‚àáuh ¬∑ ‚àávh dxdy,
where we have used the following Green‚Äôs formula that generalizes the
formula of integration by parts
>
‚Ñ¶
‚àí‚ñ≥u v dxdy =
>
‚Ñ¶
‚àáu ¬∑ ‚àáv dxdy ‚àí
>
‚àÇ‚Ñ¶
‚àáu ¬∑ n v dŒ≥,
(12.95)
for any u, v smooth enough and where n is the outward normal unit vector
on ‚àÇ‚Ñ¶(see Exercise 15).
The error analysis for the two-dimensional Ô¨Ånite element approximation
of (12.90) can still be performed through the combined use of Ce`a‚Äôs lemma
and interpolation error estimates as in Section 12.4.5 and is summarized in
the following result, which is the two-dimensional counterpart of Property
12.1 (for its proof we refer, e.g., to [QV94], Theorem 6.2.1).
Property 12.2 Let u ‚ààH1
0(‚Ñ¶) be the exact solution of (12.90) and uh ‚ààVh
be its Ô¨Ånite element approximation using continuous piecewise polynomials
of degree k ‚â•1. Assume also that u ‚ààHs(‚Ñ¶) for some s ‚â•2. Then the
following error estimate holds
‚à•u ‚àíuh‚à•H1
0(‚Ñ¶) ‚â§M
Œ±0
Chl‚à•u‚à•Hl+1(‚Ñ¶)
(12.96)
where l = min(k, s ‚àí1). Under the same assumptions, one can also prove
that
‚à•u ‚àíuh‚à•L2(‚Ñ¶) ‚â§Chl+1‚à•u‚à•Hl+1(‚Ñ¶).
(12.97)
We notice that, for any integer s ‚â•0, the Sobolev space Hs(‚Ñ¶) introduced
above is deÔ¨Åned as the space of functions with the Ô¨Årst s partial derivatives
(in the distributional sense) belonging to L2(‚Ñ¶). Moreover, H1
0(‚Ñ¶) is the
space of functions of H1(‚Ñ¶) such that u = 0 on ‚àÇ‚Ñ¶. The precise mathemat-
ical meaning of this latter statement has to be carefully addressed since,
for instance, a function belonging to H1
0(‚Ñ¶) does not necessarily mean to
be continuous everywhere. For a comprehensive presentation and analysis
of Sobolev spaces we refer to [Ada75] and [LM68].
Following the same procedure as in Section 12.4.3, we can write the Ô¨Ånite
element solution uh as
uh(x, y) =
N

j=1
ujœïj(x, y)

12.7 Applications
575
where {œïj}N
j=1 is a basis for Vh. An example of such a basis in the case
k = 1 is provided by the so-called hat functions introduced in Section 8.5.2
(see Figure 8.7, right). The Galerkin Ô¨Ånite element method leads to the
solution of the linear system Afeu = f, where (afe)ij = a(œïj, œïi).
Exactly as happens in the one-dimensional case, the matrix Afe is sym-
metric positive deÔ¨Ånite and, in general, sparse, the sparsity pattern being
strongly dependent on the topology of Th and the numbering of its nodes.
Moreover, the spectral condition number of Afe is still O(h‚àí2), which im-
plies that solving iteratively the linear system demands for the use of a
suitable preconditioner (see Section 4.3.2). If, instead, a direct solver is
used, one should resort to a suitable renumbering procedure, as explained
in Section 3.9.
12.7
Applications
In this section we employ the Ô¨Ånite element method for the numerical
approximation of two problems arising in Ô¨Çuid mechanics and in the sim-
ulation of biological systems.
12.7.1
Lubrication of a Slider
Let us consider a rigid slider moving in the direction x along a physical
support from which it is separated by a thin layer of a viscous Ô¨Çuid (which
is the lubricant). Suppose that the slider, of length L, moves with a ve-
locity U with respect to the plane support which is supposed to have an
inÔ¨Ånite length. The surface of the slider that is faced towards the support
is described by the function s (see Figure 12.13, left).
Denoting by ¬µ the viscosity of the lubricant, the pressure p acting on the
slider can be modeled by the following Dirichlet problem
Ô£±
Ô£¥
Ô£≤
Ô£¥
Ô£≥
‚àí
 s3
6¬µp‚Ä≤
‚Ä≤
= ‚àí(Us)‚Ä≤
x ‚àà(0, L),
p(0) = 0,
p(L) = 0.
Assume in the numerical computations that we are working with a conver-
gent-divergent slider of unit length, whose surface is s(x) = 1‚àí3/2x+9/8x2
with ¬µ = 1.
Figure 12.13 (right) shows the solution obtained using linear and quadratic
Ô¨Ånite elements with an uniform grid size h = 0.2. The linear system has
been solved by the nonpreconditioned conjugate gradient method. To re-
duce the Euclidean norm of the residual below 10‚àí10, 4 iterations are
needed in the case of linear Ô¨Ånite elements while 9 are required for quadratic
Ô¨Ånite elements.

576
12. Two-Point Boundary Value Problems
L
s(x)
U
x
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
‚àí0.01
0
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0.09
FIGURE 12.13. Left: geometrical parameters of the slider considered in Section
12.7.1; right: pressure on a converging-diverging slider. The solid line denotes
the solution obtained used linear Ô¨Ånite elements (the symbol ‚ó¶denotes the nodal
values), while the dashed line denotes the solution obtained using quadratic Ô¨Ånite
elements
Table 12.2 reports a numerical study of the condition number K2(Afe) as
a function of h. In the case of linear Ô¨Ånite elements we have denoted the
matrix by A1, while A2 is the corresponding matrix for quadratic elements.
Here we assume that the condition number approaches h‚àíp as h tends to
zero; the numbers pi are the estimated values of p. As can be seen, in
both cases the condition number grows like h‚àí2, however, for every Ô¨Åxed
h, K2(A2) is much bigger than K2(A1).
h
K2(A1)
p1
K2(A2)
p2
0.10000
63.951
‚Äì
455.24
0.05000
348.21
2.444
2225.7
2.28
0.02500
1703.7
2.290
10173.3
2.19
0.01250
7744.6
2.184
44329.6
2.12
0.00625
33579
2.116
187195.2
2.07
TABLE 12.2. Condition number of the stiÔ¨Äness matrix for linear and quadratic
Ô¨Ånite elements
12.7.2
Vertical Distribution of Spore Concentration over
Wide Regions
In this section we are concerned with diÔ¨Äusion and transport processes of
spores in the air, such as the endspores of bacteria and the pollen of Ô¨Çow-
ering plants. In particular, we study the vertical concentration distribution
of spores and pollen grains over a wide area. These spores, in addition to
settling under the inÔ¨Çuence of gravity, diÔ¨Äuse passively in the atmosphere.

12.7 Applications
577
The basic model assumes the diÔ¨Äusivity ŒΩ and the settling velocity Œ≤ to
be given constants, the averaging procedure incorporating various physical
processes such as small-scale convection and horizontal advection-diÔ¨Äusion
which can be neglected over a wide horizontal area. Denoting by x ‚â•0 the
vertical direction, the steady-state distribution u(x) of the spore concen-
tration is the solution of
"
‚àíŒΩu‚Ä≤‚Ä≤ + Œ≤u‚Ä≤ = 0
0 < x < H,
u(0) = u0,
‚àíŒΩu‚Ä≤(H) + Œ≤u(H) = 0,
(12.98)
where H is a Ô¨Åxed height at which we assume a vanishing Neumann condi-
tion for the advective-diÔ¨Äusive Ô¨Çux ‚àíŒΩu‚Ä≤+Œ≤u (see Section 12.4.1). Realistic
values of the coeÔ¨Écients are ŒΩ = 10 m2s‚àí1 and Œ≤ = ‚àí0.03 ms‚àí1; as for
u0, a reference concentration of 1 pollen grain per m3 has been used in the
numerical experiments, while the height H has been set equal to 10 km.
The global P¬¥eclet number is therefore Pegl = 15.
The Galerkin Ô¨Ånite element method with piecewise linear Ô¨Ånite elements
has been used for the approximation of (12.98). Figure 12.14 (left) shows
the solution computed by running Program 94 on a uniform grid with step-
size h = H/10. The solution obtained using the (non stabilized) Galerkin
formulation (G) is denoted by the solid line, while the dash-dotted and
dashed lines refer to the Scharfetter-Gummel (SG) and upwind (UP) sta-
bilized methods. Spurious oscillations can be noticed in the G solution while
the one obtained using UP is clearly overdiÔ¨Äused with respect to the SG
solution that is nodally exact. The local P¬¥eclet number is equal to 1.5 in
this case. Taking h = H/100 yields for the pure Galerkin scheme a stable
result as shown in Figure 12.14 (right) where the solutions furnished by G
(solid line) and UP (dashed line) are compared.
0
2000
4000
6000
8000
10000
‚àí0.2
0
0.2
0.4
0.6
0.8
1
1.2
0
200
400
600
800
1000
1200
1400
1600
1800
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
FIGURE 12.14. Vertical concentration of spores: G, SG and UP solutions with
h = H/10 (left) and h = H/100 (right, where only the portion [0, 2000] is shown).
The x-axis represents the vertical coordinate

578
12. Two-Point Boundary Value Problems
12.8
Exercises
1. Consider the boundary value problem (12.1)-(12.2) with f(x) = 1/x. Using
(12.3) prove that u(x) = ‚àíx log(x). This shows that u ‚ààC2(0, 1) but u(0)
is not deÔ¨Åned and u‚Ä≤, u‚Ä≤‚Ä≤ do not exist at x = 0 (‚áí: if f ‚ààC0(0, 1), but not
f ‚ààC0([0, 1]), then u does not belong to C0([0, 1])).
2. Prove that the matrix Afd introduced in (12.8) is an M-matrix.
[Hint: check that Afdx ‚â•0 ‚áíx ‚â•0. To do this, for any Œ± > 0 set Afd,Œ± =
Afd+Œ±In‚àí1. Then, compute w = Afd,Œ±x and prove that min1‚â§i‚â§(n‚àí1) wi ‚â•
0. Finally, since the matrix Afd,Œ± is invertible, being symmetric and positive
deÔ¨Ånite, and since the entries of A‚àí1
fd,Œ± are continuous functions of Œ± ‚â•0,
one concludes that A‚àí1
fd,Œ± is a nonnegative matrix as Œ± ‚Üí0.]
3. Prove that (12.13) deÔ¨Ånes a norm for V 0
h .
4. Prove (12.15) by induction on m.
5. Prove the estimate (12.23).
[Hint: for each internal node xj, j = 1, . . . , n‚àí1, integrate by parts (12.21)
to get
œÑh(xj)
= ‚àíu‚Ä≤‚Ä≤(xj) ‚àí1
h2
Ô£Æ
Ô£ØÔ£∞
xj
>
xj‚àíh
u‚Ä≤‚Ä≤(t)(xj ‚àíh ‚àít)2 dt ‚àí
xj+h
>
xj
u‚Ä≤‚Ä≤(t)(xj + h ‚àít)2 dt
Ô£π
Ô£∫Ô£ª.
Then, pass to the squares and sum œÑh(xj)2 for j = 1, . . . , n ‚àí1. On noting
that (a+b+c)2 ‚â§3(a2 +b2 +c2), for any real numbers a, b, c, and applying
the Cauchy-Schwarz inequality yields the desired result.]
6. Prove that Gk(xj) = hG(xj, xk), where G is Green‚Äôs function introduced in
(12.4) and Gk is its corresponding discrete counterpart solution of (12.4).
[Solution: we prove the result by verifying that LhG = hek. Indeed, for a
Ô¨Åxed xk the function G(xk, s) is a straight line on the intervals [0, xk] and
[xk, 1] so that LhG = 0 at every node xl with l = 0, . . . , k ‚àí1 and l =
k+1, . . . , n+1. Finally, a direct computation shows that (LhG)(xk) = 1/h
which concludes the proof.]
7. Let g = 1 and prove that Thg(xj) = 1
2xj(1 ‚àíxj).
[Solution: use the deÔ¨Ånition (12.25) with g(xk) = 1, k = 1, . . . , n ‚àí1 and
recall that Gk(xj) = hG(xj, xk) from the exercise above. Then
Thg(xj) = h
Ô£Æ
Ô£∞
j

k=1
xk(1 ‚àíxj) +
n‚àí1

k=j+1
xj(1 ‚àíxk)
Ô£π
Ô£ª
from which, after straightforward computations, one gets the desired re-
sult.]
8. Prove Young‚Äôs inequality (12.40).
9. Show that ‚à•vh‚à•h ‚â§‚à•vh‚à•h,‚àû‚àÄvh ‚ààVh.

12.8 Exercises
579
10. Consider the two-point boundary value problem (12.29) with the following
boundary conditions
Œª0u(0) + ¬µ0u(0) = g0,
Œª1u(1) + ¬µ1u(1) = g1,
where Œªj, ¬µj and gj are given data (j = 0, 1). Using the mirror imaging
technique described in Section 12.2.3 write the Ô¨Ånite diÔ¨Äerence discretiza-
tion of the equations corresponding to the nodes x0 and xn.
[Solution:
node x0 :
 Œ±1/2
h2
+ Œ≥0
2 + Œ±0Œª0
¬µ0h

u0 ‚àíŒ±1/2
u1
h2 = Œ±0g0
¬µ0h + f0
2 ,
node xn :
 Œ±n‚àí1/2
h2
+ Œ≥n
2 + Œ±nŒª1
¬µ1h

un ‚àíŒ±n‚àí1/2
un‚àí1
h2
= Œ±ng1
¬µ1h + fn
2 .]
11. Discretize the fourth-order diÔ¨Äerential operator Lu(x) = ‚àíu(iv)(x) using
centred Ô¨Ånite diÔ¨Äerences.
[Solution : apply twice the second order centred Ô¨Ånite diÔ¨Äerence operator
Lh deÔ¨Åned in (12.9).]
12. Consider problem (12.41) with non homogeneous Neumann boundary con-
ditions Œ±u‚Ä≤(0) = w0, Œ±u‚Ä≤(1) = w1. Show that the solution satisÔ¨Åes prob-
lem (12.43) where V = H1(0, 1) and the right-hand side is replaced by
(f, v)+w1v(1)‚àíw0v(0). Derive the formulation in the case of mixed bound-
ary conditions Œ±u‚Ä≤(0) = w0, u(1) = u1.
13. Using Property 1.19 prove that the matrices corresponding to the stabilized
Ô¨Ånite element method (12.79) using the upwind and SG artiÔ¨Åcial viscosities
œÜUP and œÜSG (see Section 12.5.2) are M-matrices irrespective of h.
[Hint: let us denote respectively by AUP and ASG the two stiÔ¨Äness matrices
corrsponding to œÜUP and œÜSG. Take v(x) = 1 + x and set vi = 1 + xi,
i = 0, . . . , n, being xi = ih, h = 1/n. Then, by a direct computation check
that (AUP v)i ‚â•Œ≤ > 0. As for the matrix ASG the same result can be
proved by noting that B(‚àít) = t + B(t) for any t ‚ààR.]
14. Prove that the matrix Afd with entries given by (12.93) is symmetric pos-
itive deÔ¨Ånite and it is also an M-matrix.
[Solution: to show that Afd is positive deÔ¨Ånite, proceed as in the corre-
sponding proof in Section 12.2, then proceed as in Exercise 2.]
15. Prove the Green‚Äôs formula (12.95).
[Solution: Ô¨Årst, notice that for any u, v suÔ¨Éciently smooth, div(v‚àáu) =
v‚ñ≥u+‚àáu¬∑‚àáv. Then, integrate this relation over ‚Ñ¶and use the divergence
theorem
>
‚Ñ¶
div(v‚àáu) dxdy =
>
‚àÇ‚Ñ¶
‚àÇu
‚àÇnv dŒ≥.]

13
Parabolic and Hyperbolic Initial
Boundary Value Problems
The Ô¨Ånal chapter of this book is devoted to the approximation of time-
dependent partial diÔ¨Äerential equations. Parabolic and hyperbolic initial-
boundary value problems will be addressed and either Ô¨Ånite diÔ¨Äerences and
Ô¨Ånite elements will be considered for their discretization.
13.1
The Heat Equation
The problem we are considering is how to Ô¨Ånd a function u = u(x, t) for
x ‚àà[0, 1] and t > 0 that satisÔ¨Åes the partial diÔ¨Äerential equation
‚àÇu
‚àÇt + Lu = f
0 < x < 1, t > 0,
(13.1)
subject to the boundary conditions
u(0, t) = u(1, t) = 0,
t > 0
(13.2)
and the initial condition
u(x, 0) = u0(x)
0 ‚â§x ‚â§1.
(13.3)
The diÔ¨Äerential operator L is deÔ¨Åned as
Lu = ‚àíŒΩ ‚àÇ2u
‚àÇx2 .
(13.4)

582
13. Parabolic and Hyperbolic Initial Boundary Value Problems
Equation (13.1) is called the heat equation. In fact, u(x, t) describes the
temperature at the point x and time t of a metallic bar of unit length
that occupies the interval [0, 1], under the following conditions. Its thermal
conductivity is constant and equal to ŒΩ > 0, its extrema are kept at a
constant temperature of zero degrees, at time t = 0 its temperature at
point x is described by u0(x), and f(x, t) represents the heat production
per unit length supplied at point x at time t. Here we are supposing that
the volumetric density œÅ and the speciÔ¨Åc heat per unit mass cp are both
constant and unitary. Otherwise, the temporal derivative ‚àÇu/‚àÇt should be
multiplied by the product œÅcp in (13.1).
A solution of problem (13.1)-(13.3) is provided by a Fourier series. For
instance, if ŒΩ = 1 and f ‚â°0, it is given by
u(x, t) =
‚àû

n=1
cne‚àí(nœÄ)2t sin(nœÄx)
(13.5)
where the coeÔ¨Écients cn are the Fourier sine coeÔ¨Écients of the initial datum
u0(x), i.e.
cn = 2
1
>
0
u0(x) sin(nœÄx) dx,
n = 1, 2 . . .
If instead of (13.2) we consider the Neumann conditions
ux(0, t) = ux(1, t) = 0,
t > 0,
(13.6)
the corresponding solution (still in the case where f ‚â°0 and ŒΩ = 1) would
be
u(x, t) = d0
2 +
‚àû

n=1
dne‚àí(nœÄ)2t cos(nœÄx),
where the coeÔ¨Écients dn are the Fourier cosine coeÔ¨Écients of u0(x), i.e.
dn = 2
1
>
0
u0(x) cos(nœÄx) dx,
n = 1, 2 . . .
These expressions show that the solution decays exponentially fast in time.
A more general result can be stated concerning the behavior in time of the
energy
E(t) =
1
>
0
u2(x, t) dx.

13.1 The Heat Equation
583
Indeed, if we multiply (13.1) by u and integrate with respect to x over the
interval [0, 1], we obtain
1
>
0
‚àÇu
‚àÇt (x, t)u(x, t) dx
‚àí
ŒΩ
1
>
0
‚àÇ2u
‚àÇx2 (x, t)u(x, t) dx = 1
2
1
>
0
‚àÇu2
‚àÇt (x, t) dx
+
ŒΩ
1
>
0
‚àÇu
‚àÇx
2
(x, t) dx ‚àíŒΩ
‚àÇu
‚àÇx(x, t)u(x, t)
x=1
x=0
=
1
2E‚Ä≤(t) + ŒΩ
1
>
0
‚àÇu
‚àÇx(x, t)
2
dx,
having used integration by parts, the boundary conditions (13.2) or (13.6),
and interchanged diÔ¨Äerentiation and integration.
Using the Cauchy-Schwarz inequality (8.29) yields
1
>
0
f(x, t)u(x, t) dx ‚â§(F(t))1/2(E(t))1/2
where F(t) =
 1
0 f 2(x, t) dx. Then
E‚Ä≤(t) + 2ŒΩ
1
>
0
‚àÇu
‚àÇx(x, t)
2
dx ‚â§2(F(t))1/2(E(t))1/2.
Owing to the Poincar¬¥e inequality (12.16) with (a, b) = (0, 1) we obtain
E‚Ä≤(t) + 2
ŒΩ
(CP )2 E(t) ‚â§2(F(t))1/2(E(t))1/2.
By Young‚Äôs inequality (12.40) we have
2(F(t))1/2(E(t))1/2 ‚â§Œ≥E(t) + 1
Œ≥ F(t),
having set Œ≥ = ŒΩ/C2
P . Therefore, E‚Ä≤(t) + Œ≥E(t) ‚â§1
Œ≥ F(t), or, equivalently,
(eŒ≥tE(t))‚Ä≤ ‚â§1
Œ≥ eŒ≥tF(t). Then, integrating from 0 to t we get
E(t) ‚â§e‚àíŒ≥tE(0) + 1
Œ≥
t
>
0
eŒ≥(s‚àít)F(s)ds.
(13.7)
In particular, when f ‚â°0, (13.7) shows that the energy E(t) decays expo-
nentially fast in time.

584
13. Parabolic and Hyperbolic Initial Boundary Value Problems
13.2
Finite DiÔ¨Äerence Approximation of the Heat
Equation
To solve the heat equation numerically we have to discretize both the x
and t variables. We can start by dealing with the x-variable, following the
same approach as in Section 12.2. We denote by ui(t) an approximation of
u(xi, t), i = 0, . . . , n and approximate the Dirichlet problem (13.1)-(13.3)
by the scheme
.ui (t) ‚àíŒΩ
h2 (ui‚àí1(t) ‚àí2ui(t) + ui+1(t)) = fi(t),
i = 1, . . . , n ‚àí1, ‚àÄt > 0,
u0(t) = un(t) = 0,
‚àÄt > 0,
ui(0) = u0(xi),
i = 0, . . . , n,
where the upper dot indicates derivation with respect to time, and fi(t) =
f(xi, t). This is actually a semi-discretization of problem (13.1)-(13.3), and
is a system of ordinary diÔ¨Äerential equations of the following form
" Àôu(t) = ‚àíŒΩAfdu(t) + f(t),
‚àÄt > 0,
u(0) = u0
(13.8)
where u(t) = (u1(t), . . . , un‚àí1(t))T is the vector of unknowns, f(t) =
(f1(t), . . . , fn‚àí1(t))T , u0 = (u0(x1), . . . , u0(xn‚àí1))T and Afd is the tridi-
agonal matrix introduced in (12.8). Note that for the derivation of (13.8)
we have assumed that u0(x0) = u0(xn) = 0, which is coherent with the
boundary condition (13.2).
A popular scheme for the integration of (13.8) with respect to time is the
so-called Œ∏‚àímethod. To construct the scheme, we denote by vk the value
of the variable v at time tk = k‚àÜt, for ‚àÜt > 0; then, the Œ∏-method for the
time-integration of (13.8) is
Ô£±
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£≥
uk+1 ‚àíuk
‚àÜt
= ‚àíŒΩAfd(Œ∏uk+1 + (1 ‚àíŒ∏)uk) + Œ∏f k+1 + (1 ‚àíŒ∏)f k,
k = 0, 1, . . .
u0 = u0
(13.9)
or, equivalently,
(I + ŒΩŒ∏‚àÜtAfd) uk+1 = (I ‚àíŒΩ‚àÜt(1 ‚àíŒ∏)Afd) uk + gk+1,
(13.10)
where gk+1 = ‚àÜt(Œ∏f k+1 + (1 ‚àíŒ∏)f k) and I is the identity matrix of order
n ‚àí1.
For suitable values of the parameter Œ∏, from (13.10) we can recover some
familiar methods that have been introduced in Chapter 11. For example, if
Œ∏ = 0 the method (13.10) coincides with the forward Euler scheme and we

13.2 Finite DiÔ¨Äerence Approximation of the Heat Equation
585
can get uk+1 explicitly; otherwise, a linear system (with constant matrix
I + ŒΩŒ∏‚àÜtAfd) needs be solved at each time-step.
Regarding stability, assume that f ‚â°0 (henceforth gk = 0 ‚àÄk > 0),
so that from (13.5) the exact solution u(x, t) tends to zero for every x
as t ‚Üí‚àû. Then we would expect the discrete solution to have the same
behaviour, in which case we would call our scheme (13.10) asymptotically
stable, this being coherent with what we did in Chapter 11, Section 11.1
for ordinary diÔ¨Äerential equations.
If Œ∏ = 0, from (13.10) it follows that
uk = (I ‚àíŒΩ‚àÜtAfd)ku0,
k = 1, 2, . . .
From the analysis of convergent matrices (see Section 1.11.2) we deduce
that uk ‚Üí0 as k ‚Üí‚àûiÔ¨Ä
œÅ(I ‚àíŒΩ‚àÜtAfd) < 1.
(13.11)
On the other hand, the eigenvalues of Afd are given by (see Exercise 3)
¬µi = 4
h2 sin2(iœÄh/2),
i = 1, . . . , n ‚àí1.
Then (13.11) is satisÔ¨Åed iÔ¨Ä
‚àÜt < 1
2ŒΩ h2.
As expected, the forward Euler method is conditionally stable, and the
time-step ‚àÜt should decay as the square of the grid spacing h.
In the case of the backward Euler method (Œ∏ = 1), we would have from
(13.10)
uk =
0
(I + ŒΩ‚àÜtAfd)‚àí11k u0,
k = 1, 2, . . .
Since all the eigenvalues of the matrix (I+ŒΩ‚àÜtAfd)‚àí1 are real, positive and
strictly less than 1 for every value of ‚àÜt, this scheme is unconditionally
stable. More generally, the Œ∏-scheme is unconditionally stable for all the
values 1/2 ‚â§Œ∏ ‚â§1, and conditionally stable if 0 ‚â§Œ∏ < 1/2 (see Section
13.3.1).
As far as the accuracy of the Œ∏-method is concerned, its local truncation
error is of the order of ‚àÜt+h2 if Œ∏ Ã∏= 1
2 while it is of the order of ‚àÜt2 +h2 if
Œ∏ = 1
2. The method corresponding to Œ∏ = 1/2 is frequently called the Crank-
Nicolson scheme and is therefore unconditionally stable and second-order
accurate with respect to both ‚àÜt and h.

586
13. Parabolic and Hyperbolic Initial Boundary Value Problems
13.3
Finite Element Approximation of the Heat
Equation
The space discretization of (13.1) can also be accomplished using the Galerkin
Ô¨Ånite element method by proceeding as in Chapter 12 in the elliptic case.
First, for all t > 0 we multiply (13.1) by a test function v = v(x) and
integrate over (0, 1). Then, we let V = H1
0(0, 1) and ‚àÄt > 0 we look for a
function t ‚Üíu(x, t) ‚ààV (brieÔ¨Çy, u(t) ‚ààV ) such that
1
>
0
‚àÇu(t)
‚àÇt vdx + a(u(t), v) = F(v)
‚àÄv ‚ààV,
(13.12)
with u(0) = u0. Here, a(u(t), v) =
 1
0 ŒΩ(‚àÇu(t)/‚àÇx) (‚àÇv/‚àÇx) dx and F(v) =
 1
0 f(t)vdx are the bilinear form and the linear functional respectively as-
sociated with the elliptic operator L and the right hand side f. Notice that
a(¬∑, ¬∑) is a special case of (12.44) and that the dependence of u and f on
the space variable x will be understood henceforth.
Let Vh be a suitable Ô¨Ånite dimensional subspace of V . We consider the
following Galerkin formulation: ‚àÄt > 0, Ô¨Ånd uh(t) ‚ààVh such that
1
>
0
‚àÇuh(t)
‚àÇt
vhdx + a(uh(t), vh) = F(vh)
‚àÄvh ‚ààVh
(13.13)
where uh(0) = u0h and u0h ‚ààVh is a convenient approximation of u0.
Problem (13.13) is referred to as a semi-discretization of (13.12) since it is
only a space discretization of the heat equation.
Proceeding in a manner similar to that used to obtain the energy estimate
(13.7), we get the following a priori estimate for the discrete solution uh(t)
of (13.13)
Eh(t) ‚â§e‚àíŒ≥tEh(0) + 1
Œ≥
t
>
0
eŒ≥(s‚àít)F(s)ds,
where Eh(t) =
 1
0 u2
h(x, t) dx.
As for the Ô¨Ånite element discretization of (13.13), we introduce the Ô¨Ånite
element space Vh deÔ¨Åned in (12.57) and consequently a basis {œïj} for Vh
as already done in Section 12.4.5. Then, the solution uh of (13.13) can be
sought under the form
uh(t) =
Nh

j=1
uj(t)œïj,

13.3 Finite Element Approximation of the Heat Equation
587
where {uj(t)} are the unknown coeÔ¨Écients and Nh is the dimension of Vh.
Then, from (13.13) we obtain
1
>
0
Nh

j=1.uj (t)œïjœïidx + a
Ô£´
Ô£≠
Nh

j=1
uj(t)œïj, œïi
Ô£∂
Ô£∏= F(œïi),
i = 1, . . . , Nh
that is,
Nh

j=1.uj (t)
1
>
0
œïjœïidx +
Nh

j=1
uj(t)a(œïj, œïi) = F(œïi),
i = 1, . . . , Nh.
Using the same notation as in (13.8) we obtain
M.u(t) + Afeu(t) = ffe(t)
(13.14)
where Afe = (a(œïj, œïi)), ffe(t) = (F(œïi)) and M = (mij) = (
 1
0 œïjœïidx) for
i, j = 1, . . . , Nh. M is called the mass matrix. Since it is nonsingular, the
system of ODEs (13.14) can be written in normal form as
.u(t) = ‚àíM‚àí1Afeu(t) + M‚àí1ffe(t).
(13.15)
To solve (13.15) approximately we can still apply the Œ∏-method and obtain
Muk+1 ‚àíuk
‚àÜt
+ Afe
0
Œ∏uk+1 + (1 ‚àíŒ∏)uk1
= Œ∏f k+1
fe
+ (1 ‚àíŒ∏)f k
fe.
(13.16)
As usual, the upper index k means that the quantity at hand is computed at
time tk. As in the Ô¨Ånite diÔ¨Äerence case, for Œ∏ = 0, 1 and 1/2 we respectively
obtain the forward Euler, backward Euler and Crank-Nicolson methods,
where the Crank-Nicolson method is the only one which is second-order
accurate with respect to ‚àÜt.
For each k, (13.16) is a linear system whose matrix is
K = 1
‚àÜtM + Œ∏Afe.
Since M and Afe are symmetric and positive deÔ¨Ånite, the matrix K is also
symmetric and positive deÔ¨Ånite. Thus, its Cholesky decomposition K =
HT H where H is upper triangular (see Section 3.4.2) can be carried out at
t = 0. Consequently, at each time step the following two linear triangular
systems, each of size equal to Nh, must be solved, with a computational
cost of N 2
h/2 Ô¨Çops
Ô£±
Ô£≤
Ô£≥
HT y =
 1
‚àÜtM ‚àí(1 ‚àíŒ∏)Afe

uk + Œ∏f k+1
fe
+ (1 ‚àíŒ∏)f k
fe,
Huk+1 = y.

588
13. Parabolic and Hyperbolic Initial Boundary Value Problems
When Œ∏ = 0, a suitable diagonalization of M would allow to decouple
the system equations (13.16). The procedure is carried out by the so-called
mass-lumping in which we approximate M by a nonsingular diagonal matrix
$M. In the case of piecewise linear Ô¨Ånite elements $M can be obtained using
the composite trapezoidal formula over the nodes {xi} to evaluate the
integrals
 1
0 œïjœïi dx, obtaining Àúmij = hŒ¥ij, i, j = 1, . . . , Nh (see Exercise
2).
13.3.1
Stability Analysis of the Œ∏-Method
Applying the Œ∏-method to the Galerkin problem (13.13) yields

uk+1
h
‚àíuk
h
‚àÜt
, vh

+
a

Œ∏uk+1
h
+ (1 ‚àíŒ∏)uk
h, vh

=
Œ∏F k+1(vh) + (1 ‚àíŒ∏)F k(vh)
‚àÄvh ‚ààVh
(13.17)
for k ‚â•0 and with u0
h = u0h, F k(vh) =
 1
0 f(tk)vh(x)dx. Since we are
interested in the stability analysis, we can consider the special case where
F = 0; moreover, for the time being, we focus on the case Œ∏ = 1 (implicit
Euler scheme), i.e.

uk+1
h
‚àíuk
h
‚àÜt
, vh

+ a

uk+1
h
, vh

= 0
‚àÄvh ‚ààVh.
Letting vh = uk+1
h
, we get

uk+1
h
‚àíuk
h
‚àÜt
, uk+1
h

+ a(uk+1
h
, uk+1
h
) = 0.
From the deÔ¨Ånition of a(¬∑, ¬∑), it follows that
a

uk+1
h
, uk+1
h

= ŒΩ
!!!!!
‚àÇuk+1
h
‚àÇx
!!!!!
2
L2(0,1)
.
(13.18)
Moreover, we remark that (see Exercise 3 for the proof of this result)
‚à•uk+1
h
‚à•2
L2(0,1) + 2ŒΩ‚àÜt
!!!!!
‚àÇuk+1
h
‚àÇx
!!!!!
2
L2(0,1)
‚â§‚à•uk
h‚à•2
L2(0,1).
(13.19)
It follows that, ‚àÄn ‚â•1
n‚àí1

k=0
‚à•uk+1
h
‚à•2
L2(0,1) + 2ŒΩ‚àÜt
n‚àí1

k=0
!!!!!
‚àÇuk+1
h
‚àÇx
!!!!!
2
L2(0,1)
‚â§
n‚àí1

k=0
‚à•uk
h‚à•2
L2(0,1).

13.3 Finite Element Approximation of the Heat Equation
589
Since these are telescopic sums, we get
‚à•un
h‚à•2
L2(0,1) + 2ŒΩ‚àÜt
n‚àí1

k=0
!!!!!
‚àÇuk+1
h
‚àÇx
!!!!!
2
L2(0,1)
‚â§‚à•u0h‚à•2
L2(0,1),
(13.20)
which shows that the scheme is unconditionally stable. Proceeding similarly
if f Ã∏= 0, it can be shown that
‚à•un
h‚à•2
L2(0,1)
+
2ŒΩ‚àÜt
n‚àí1

k=0
!!!!!
‚àÇuk+1
h
‚àÇx
!!!!!
2
L2(0,1)
‚â§
C(n)

‚à•u0h‚à•2
L2(0,1) +
n

k=1
‚àÜt‚à•f k‚à•2
L2(0,1)

,
(13.21)
where C(n) is a constant independent of both h and ‚àÜt.
Remark 13.1 The same kind of stability inequalities (13.20) and (13.21)
can be obtained if a(¬∑, ¬∑) is a more general bilinear form provided that it is
continuous and coercive (see Exercise 4).
‚ñ†
To carry out the stability analysis of the Œ∏-method for every Œ∏ ‚àà[0, 1] we
need deÔ¨Åning the eigenvalues and eigenvectors of a bilinear form.
DeÔ¨Ånition 13.1 We say that Œª is an eigenvalue and w ‚ààV is the associ-
ated eigenvector for the bilinear form a(¬∑, ¬∑) : V √ó V "‚ÜíR if
a(w, v) = Œª(w, v)
‚àÄv ‚ààV,
where (¬∑, ¬∑) denotes the usual scalar product in L2(0, 1).
‚ñ†
If the bilinear form a(¬∑, ¬∑) is symmetric and coercive, it has inÔ¨Ånitely many
real positive eigenvalues that form an unbounded sequence; moreover, its
eigenvectors (called also eigenfunctions) form a basis for the space V .
At a discrete level the corresponding pair Œªh ‚ààR, wh ‚ààVh satisÔ¨Åes
a(wh, vh) = Œªh(wh, vh)
‚àÄvh ‚ààVh.
(13.22)
From the algebraic standpoint, problem (13.22) can be formulated as
Afew = ŒªhMw
(where w is the vector of the gridvalues of wh) and can be regarded
as a generalized eigenvalue problem (see Section 5.9). All the eigenvalues
Œª1
h, . . . , ŒªNh
h
are positive. The corresponding eigenvectors w1
h, . . . , wNh
h
form

590
13. Parabolic and Hyperbolic Initial Boundary Value Problems
a basis for the subspace Vh and can be chosen in such a way as to be or-
thonormal, i.e., such that (wi
h, wj
h) = Œ¥ij, ‚àÄi, j = 1, . . . , Nh. In particular,
any function vh ‚ààVh can be represented as
vh(x) =
Nh

j=1
vjwj
h(x).
Let us now assume that Œ∏ ‚àà[0, 1] and focus on the case where the bilinear
form a(¬∑, ¬∑) is symmetric. Although the Ô¨Ånal stability result still holds in
the nonsymmetric case, the proof that follows cannot apply since in that
case the eigenvectors would no longer form a basis for Vh. Let

wi
h

be the
eigenvectors of a(¬∑, ¬∑) whose span forms an orthonormal basis for Vh. Since
at each time step uk
h ‚ààVh, we can express uk
h as
uk
h(x) =
Nh

j=1
uk
j wj
h(x).
Letting F = 0 in (13.17) and taking vh = wi
h, we Ô¨Ånd
1
‚àÜt
Nh

j=1
0
uk+1
j
‚àíuk
j
1 +
wj
h, wi
h
,
+
Nh

j=1
0
Œ∏uk+1
j
+ (1 ‚àíŒ∏)uk
j
1
a(wj
h, wi
h) = 0,
i = 1, . . . , Nh.
Since wj
h are eigenfunctions of a(¬∑, ¬∑) we obtain
a(wj
h, wi
h) = Œªj
h(wj
h, wi
h) = Œªj
hŒ¥ij = Œªi
h,
so that
uk+1
i
‚àíuk
i
‚àÜt
+
0
Œ∏uk+1
i
+ (1 ‚àíŒ∏)uk
i
1
Œªi
h = 0.
Solving this equation with respect to uk+1
i
gives
uk+1
i
= uk
i
0
1 ‚àí(1 ‚àíŒ∏)Œªi
h‚àÜt
1
0
1 + Œ∏Œªi
h‚àÜt
1
.
In order for the method to be unconditionally stable we must have (see
Chapter 11)

1 ‚àí(1 ‚àíŒ∏)Œªi
h‚àÜt
1 + Œ∏Œªi
h‚àÜt
 < 1,
that is
2Œ∏ ‚àí1 > ‚àí
2
Œªi
h‚àÜt.

13.3 Finite Element Approximation of the Heat Equation
591
If Œ∏ ‚â•1/2, this inequality is satisÔ¨Åed for any value of ‚àÜt. Conversely, if
Œ∏ < 1/2 we must have
‚àÜt <
2
(1 ‚àí2Œ∏)Œªi
h
.
Since this relation must hold for all the eigenvalues Œªi
h of the bilinear form,
it suÔ¨Éces requiring that it is satisÔ¨Åed for the largest of them, which we
assume to be ŒªNh
h .
We therefore conclude that if Œ∏ ‚â•1/2 the Œ∏-method is unconditionally
stable (i.e., it is stable ‚àÄ‚àÜt), whereas if 0 ‚â§Œ∏ < 1/2 the Œ∏-method is stable
only if
‚àÜt ‚â§
2
(1 ‚àí2Œ∏)ŒªNh
h
.
It can be shown that there exist two positive constants c1 and c2, indepen-
dent of h, such that
c1h‚àí2 ‚â§ŒªNh
h
= c2h‚àí2
(see for the proof, [QV94], Section 6.3.2). Accounting for this, we obtain
that if 0 ‚â§Œ∏ < 1/2 the method is stable only if
‚àÜt ‚â§C1(Œ∏)h2,
(13.23)
for a suitable constant C1(Œ∏) independent of both h and ‚àÜt.
With an analogous proof, it can be shown that if a pseudo-spectral Galerkin
approximation is used for problem (13.12), the Œ∏‚àímethod is uncondition-
ally stable if Œ∏ ‚â•1
2, while for 0 ‚â§Œ∏ < 1
2 stability holds only if
‚àÜt ‚â§C2(Œ∏)N ‚àí4,
(13.24)
for a suitable constant C2(Œ∏) independent of both N and ‚àÜt. The diÔ¨Äerence
between (13.23) and (13.24) is due to the fact that the largest eigenvalue
of the spectral stiÔ¨Äness matrix grows like O(N 4) with respect to the degree
of the approximating polynomial.
Comparing the solution of the globally discretized problem (13.17) with
that of the semi-discrete problem (13.13), by a suitable use of the stability
result (13.21) and of the truncation time discretization error, the following
convergence result can be proved
‚à•u(tk) ‚àíuk
h‚à•L2(0,1) ‚â§C(u0, f, u)(‚àÜtp(Œ∏) + hr+1),
‚àÄk ‚â•1
where r denotes the piecewise polynomial degree of the Ô¨Ånite element space
Vh, p(Œ∏) = 1 if Œ∏ Ã∏= 1/2 while p(1/2) = 2 and C is a constant that depends
on its arguments (assuming that they are suÔ¨Éciently smooth) but not on

592
13. Parabolic and Hyperbolic Initial Boundary Value Problems
h and ‚àÜt. In particular, if f ‚â°0 on can obtain the following improved
estimates
‚à•u(tk) ‚àíuk
h‚à•L2(0,1) ‚â§C
. h
‚àö
tk
r+1
+
‚àÜt
tk
p(Œ∏)/
‚à•u0‚à•L2(0,1),
for k ‚â•1, Œ∏ = 1 or Œ∏ = 1/2. (For the proof of these results, see [QV94], pp.
394-395).
Program 100 provides an implementation of the Œ∏-method for the solution
of the heat equation on the space-time domain (a, b) √ó (t0, T). The dis-
cretization in space is based on piecewise-linear Ô¨Ånite elements. The input
parameters are: the column vector I containing the endpoints of the space
interval (a = I(1), b = I(2)) and of the time interval (t0 = I(3), T = I(4));
the column vector n containing the number of steps in space and time; the
macros u0 and f containing the functions u0h and f, the constant viscosity
nu, the Dirichlet boundary conditions bc(1) and bc(2), and the value of
the parameter theta.
Program 100 - thetameth : Œ∏-method for the heat equation
function [u,x] = thetameth(I,n,u0,f,bc,nu,theta)
nx = n(1); h = (I(2)-I(1))/nx;
x = [I(1):h:I(2)]; t = I(3);
uold = (eval(u0))‚Äô;
nt = n(2); k = (I(4)-I(3))/nt; e = ones(nx+1,1);
K = spdiags([(h/(6*k)-nu*theta/h)*e, (2*h/(3*k)+2*nu*theta/h)*e, ...
(h/(6*k)-nu*theta/h)*e],-1:1,nx+1,nx+1);
B = spdiags([(h/(6*k)+nu*(1-theta)/h)*e, (2*h/(3*k)-nu*2*(1-theta)/h)*e, ...
(h/(6*k)+nu*(1-theta)/h)*e],-1:1,nx+1,nx+1);
K(1,1)
= 1; K(1,2)
= 0; B(1,1)
= 0; B(1,2)
= 0;
K(nx+1,nx+1) = 1; K(nx+1,nx) = 0; B(nx+1,nx+1) = 0; B(nx+1,nx) = 0;
[L,U]=lu(K);
t = I(3); x=[I(1)+h:h:I(2)-h]; fold = (eval(f))‚Äô;
fold = h*fold;
fold = [bc(1); fold; bc(2)];
for time = I(3)+k:k:I(4)
t = time;
fnew = (eval(f))‚Äô;
fnew = h*fnew;
fnew = [bc(1); fnew; bc(2)];
b = theta*fnew+(1-theta)*fold + B*uold;
y = L \ b;
u = U \ y;
uold = u;
end
x = [I(1):h:I(2)];
Example 13.1 Let us assess the time-accuracy of the Œ∏-method in the solution
of the heat equation (13.1) on the space-time domain (0, 1) √ó (0, 1) where f is

13.4 Space-Time Finite Element Methods for the Heat Equation
593
chosen in such a way that the exact solution is u = sin(2œÄx) cos(2œÄt). A Ô¨Åxed
spatial grid size h = 1/500 has been used while the time step ‚àÜt is equal to
(10k)‚àí1, k = 1, . . . , 4. Finally, piecewise Ô¨Ånite elements are used for the space
discretization. Figure 13.1 shows the convergence behavior in the L2(0, 1) norm
(evaluated at time t = 1), as ‚àÜt tends to zero, of the backward Euler method
(BE) (Œ∏ = 1, solid line) and of the Crank-Nicolson scheme (CN) (Œ∏ = 1/2, dashed
line). As expected, the CN method is far more accurate than the BE method. ‚Ä¢
10
1
10
2
10
‚àí7
10
‚àí6
10
‚àí5
10
‚àí4
10
‚àí3
10
‚àí2
10
‚àí1
FIGURE 13.1. Convergence analysis of the Œ∏-method as a function of the number
1/‚àÜt of time steps (represented on the x-axis): Œ∏ = 1 (solid line) and Œ∏ = 0.5
(dashed line)
13.4
Space-Time Finite Element Methods for the
Heat Equation
An alternative approach for time discretization is based on the use of a
Galerkin method to discretize both space and time variables.
Suppose to solve the heat equation for x ‚àà[0, 1] and t ‚àà[0, T]. Let us
denote by Ik = [tk‚àí1, tk] the k-th time interval for k = 1, . . . , n with ‚àÜtk =
tk ‚àítk‚àí1; moreover, we let ‚àÜt = maxk ‚àÜtk; the rectangle Sk = [0, 1]√óIk is
the so called space-time slab. At each time level tk, we consider a partition
Thk of (0, 1) into mk subintervals Kk
j = [xk
j , xk
j+1], j = 0, . . . , mk ‚àí1. We
let hk
j = xk
j+1 ‚àíxk
j and denote by hk = maxj hk
j and by h = maxk hk.
Let us now associate with Sk a space-time partition Sk = ‚à™mk
j=1Rk
j where
Rk
j = Kk
j √ó Ik and Kk
j ‚ààThk. The space-time slab Sk is thus decomposed
into rectangles Rk
j (see Figure 13.2).
For each time slab Sk we introduce the space-time Ô¨Ånite element space
Qq(Sk) =
2
v ‚ààC0(Sk), v|Rk
j ‚ààP1(Kk
j ) √ó Pq(Ik), j = 0, . . . , mk ‚àí1
3

594
13. Parabolic and Hyperbolic Initial Boundary Value Problems
tk‚àí1
Rj
k
Sk
t
0
1 x
tk
FIGURE 13.2. Space-time Ô¨Ånite element discretization
where, usually, q = 0 or q = 1. Then, the space-time Ô¨Ånite element space
over [0, 1] √ó [0, T] is deÔ¨Åned as follows
Vh,‚àÜt =

v : [0, 1] √ó [0, T] ‚ÜíR : v|Sk ‚ààYh,k, k = 1, . . . , n

,
where
Yh,k = {v ‚ààQq(Sk) : v(0, t) = v(1, t) = 0 ‚àÄt ‚ààIk} .
The number of degrees of freedom of Vh,‚àÜt is equal to (q + 1)(mk ‚àí1).
The functions in Vh,‚àÜt are linear and continuous in space while they are
piecewise polynomials of degree q in time. These functions are in general
discontinuous across the time levels tk and the partitions T k
h do not match
at the interface between contiguous time levels (see Figure 13.2). For this
reason, we adopt henceforth the following notation
vk
¬± = lim
œÑ‚Üí0v(tk ¬± œÑ),
[vk] = vk
+ ‚àívk
‚àí.
The discretization of problem (13.12) using continuous Ô¨Ånite elements in
space of degree 1 and discontinuous Ô¨Ånite elements of degree q in time
(abbreviated by cG(1)dG(q) method) is: Ô¨Ånd U ‚ààVh,‚àÜt such that
n

k=1
>
Ik
‚àÇU
‚àÇt , V

+ a(U, V )

dt +
n‚àí1

k=1
([U k], V k
+)
+(U 0
+, V 0
+) =
T
>
0
(f, V ) dt,
‚àÄV ‚àà
0
V h,‚àÜt,
where
0
V h,‚àÜt = {v ‚ààVh,‚àÜt : v(0, t) = v(1, t) = 0 ‚àÄt ‚àà[0, T]} ,

13.4 Space-Time Finite Element Methods for the Heat Equation
595
U 0
‚àí= u0h, U k = U(x, tk) and (u, v) =
 1
0 uv dx denotes the scalar product
of L2(0, 1). The continuity of U at each point tk is therefore imposed only
in a weak sense.
To construct the algebraic equations for the unknown U we need expand-
ing it over a basis in time and in space. The single space-time basis func-
tion œïk
jl(x, t) can be written as œïk
jl(x, t) = œïk
j (x)œàl(t), j = 1, . . . , mk ‚àí1,
l = 0, . . . , q, where œïk
j is the usual piecewise linear basis function and œàl is
the l-th basis function of Pq(Ik).
When q = 0 the solution U is piecewise constant in time. In that case
U k(x, t) =
N k
h

j=1
U k
j œïk
j (x), x ‚àà[0, 1], t ‚ààIk,
where U k
j = U k(xj, t) ‚àÄt ‚ààIk. Let
Ak = (aij) = (a(œïk
j , œïk
i )),
Mk = (mij) = ((œïk
j , œïk
i )),
fk = (fi) =
Ô£´
Ô£≠
>
Sk
f(x, t)œïk
i (x)dx dt
Ô£∂
Ô£∏,
Bk,k‚àí1 = (bij) = ((œïk
j , œïk‚àí1
i
)),
denote the stiÔ¨Äness matrix, the mass matrix, the data vector and the pro-
jection matrix between V k‚àí1
h
and V k
h , respectively, at the time level tk.
Then, letting Uk = (U k
j ), at each k-th time level the cG(1)dG(0) method
requires solving the following linear system

Mk + ‚àÜtkAk

Uk = Bk,k‚àí1Uk‚àí1 + fk,
which is nothing else than the Euler backward discretization scheme with
a modiÔ¨Åed right hand side.
When q = 1, the solution is piecewise linear in time. For ease of notation
we let U k(x) = U‚àí(x, tk) and U k‚àí1(x) = U+(x, tk‚àí1). Moreover, we assume
that the spatial partition Thk does not change with the time level and we
let mk = m for every k = 0, . . . , n. Then, we can write
U|Sk = U k‚àí1(x)tk ‚àít
‚àÜtk + U k(x)t ‚àítk‚àí1
‚àÜtk
.
Thus the cG(1)dG(1) method leads to the solution of the following 2 √ó
2 block-system in the unknowns Uk = (U k
i ) and Uk‚àí1 = (U k‚àí1
i
), i =
1, . . . , m ‚àí1
Ô£±
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£≥

‚àí1
2Mk + ‚àÜtk
3 Ak

Uk‚àí1 +
1
2Mk + ‚àÜtk
6 Ak

Uk = fk‚àí1 + Bk,k‚àí1Uk‚àí1
‚àí
,
1
2Mk + ‚àÜtk
6 Ak

Uk‚àí1 +
1
2Mk + ‚àÜtk
3 Ak

Uk = fk

596
13. Parabolic and Hyperbolic Initial Boundary Value Problems
where
fk‚àí1 =
>
Sk
f(x, t)œïk
i (x)œàk
1(t)dx dt,
fk =
>
Sk
f(x, t)œïk
i (x)œàk
2(t)dx dt
and œàk
1(t) = (tk ‚àít)/‚àÜtk, œàk
2(t)(t ‚àítk‚àí1)/‚àÜtk are the two basis functions
of P1(Ik).
Assuming that Vh,k‚àí1 Ã∏‚äÇVh,k, it is possible to prove that (see for the
proof [EEHJ96])
‚à•u(tn) ‚àíU n‚à•L2(0,1) ‚â§C(u0h, f, u, n)(‚àÜt2 + h2),
(13.25)
where C is a constant that depends on its arguments (assuming that they
are suÔ¨Éciently smooth) but not on h and ‚àÜt.
An advantage in using space-time Ô¨Ånite elements is the possibility to
perform a space-time grid adaptivity on each time-slab based on a posteriori
error estimates (the interested reader is referred to [EEHJ96] where the
analysis of this method is carried out in detail).
Program 101 provides an implementation of the dG(1)cG(1) method for
the solution of the heat equation on the space-time domain (a, b) √ó (t0, T).
The input parameters are the same as in Program 100.
Program 101 - pardg1cg1 : dG(1)cG(1) method for the heat equation
function [u,x]=pardg1cg1(I,n,u0,f,nu,bc)
nx = n(1);
h = (I(2)-I(1))/nx;
x = [I(1):h:I(2)]; t = I(3);
um = (eval(u0))‚Äô;
nt = n(2);
k = (I(4)-I(3))/nt;
e = ones(nx+1,1);
Add = spdiags([(h/12-k*nu/(3*h))*e, (h/3+2*k*nu/(3*h))*e, ...
(h/12-k*nu/(3*h))*e],-1:1,nx+1,nx+1);
Aud = spdiags([(h/12-k*nu/(6*h))*e, (h/3+k*nu/(3*h))*e, ...
(h/12-k*nu/(6*h))*e],-1:1,nx+1,nx+1);
Ald = spdiags([(-h/12-k*nu/(6*h))*e, (-h/3+k*nu/(3*h))*e, ...
(-h/12-k*nu/(6*h))*e],-1:1,nx+1,nx+1);
B = spdiags([h*e/6, 2*h*e/3, h*e/6],-1:1,nx+1,nx+1);
Add(1,1) = 1; Add(1,2) = 0; B(1,1)
= 0; B(1,2)
= 0;
Aud(1,1) = 0; Aud(1,2) = 0; Ald(1,1) = 0; Ald(1,2) = 0;
Add(nx+1,nx+1)=1; Add(nx+1,nx)=0;
B(nx+1,nx+1)=0; B(nx+1,nx) = 0;
Ald(nx+1,nx+1)=0; Ald(nx+1,nx)=0;
Aud(nx+1,nx+1)=0; Aud(nx+1,nx)=0;
[L,U]=lu([Add Aud; Ald Add]);
x = [I(1)+h:h:I(2)-h]; xx=[I(1),x,I(2)];
for time = I(3)+k:k:I(4)

13.5 Hyperbolic Equations: A Scalar Transport Problem
597
t = time;
fq1 = 0.5*k*h*eval(f);
t = time-k;
fq0 = 0.5*k*h*eval(f);
rhs0 = [bc(1), fq0, bc(2)];
rhs1 = [bc(1), fq1, bc(2)];
b = [rhs0‚Äô; rhs1‚Äô] + [B*um; zeros(nx+1,1)];
y = L \ b;
u = U \ y; um = u(nx+2:2*nx+2,1);
end
x = [I(1):h:I(2)]; u = um;
Example 13.2 We assess the accuracy of the dG(1)cG(1) method on the same
test problem considered in Example 13.1. In order to neatly identify both spatial
and temporal contributions in the error estimate (13.25) we have performed the
numerical computations using Program 101 by varying either the time step or
the space discretization step only, having chosen in each case the discretization
step in the other variable suÔ¨Éciently small that the corresponding error can be
neglected. The convergence behavior in Figure 13.3 shows perfect agreement with
the theoretical results (second-order accuracy in both space and time).
‚Ä¢
10
1
10
2
10
3
10
‚àí5
10
‚àí4
10
‚àí3
10
‚àí2
10
‚àí1
FIGURE 13.3. Convergence analysis for the dG(1)cG(1) method. The solid line
is the time discretization error while the dashed line is the space discretization
error. In the Ô¨Årst case the x-axis denotes the number of time steps while in second
case it represents the number of space subintervals
13.5
Hyperbolic Equations: A Scalar Transport
Problem
Let us consider the following scalar hyperbolic problem
Ô£±
Ô£≤
Ô£≥
‚àÇu
‚àÇt + a‚àÇu
‚àÇx = 0
x ‚ààR, t > 0,
u(x, 0) = u0(x)
x ‚ààR,
(13.26)

598
13. Parabolic and Hyperbolic Initial Boundary Value Problems
where a is a positive real number. Its solution is given by
u(x, t) = u0(x ‚àíat)
t ‚â•0,
and represents a travelling wave with velocity a. The curves (x(t), t) in the
plane (x, t), that satisfy the following scalar ordinary diÔ¨Äerential equation
Ô£±
Ô£≤
Ô£≥
dx(t)
dt
= a
t > 0
x(0) = x0,
(13.27)
are called characteristic curves. They are the straight lines x(t) = x0 + at,
t > 0. The solution of (13.26) remains constant along them since
du
dt = ‚àÇu
‚àÇt + ‚àÇu
‚àÇx
dx
dt = 0
on (x(t), t).
For the more general problem
Ô£±
Ô£≤
Ô£≥
‚àÇu
‚àÇt + a‚àÇu
‚àÇx + a0u = f
x ‚ààR, t > 0,
u(x, 0) = u0(x)
x ‚ààR,
(13.28)
where a, a0 and f are given functions of the variables (x, t), the charac-
teristic curves are still deÔ¨Åned as in (13.27). In this case, the solutions of
(13.28) satisfy along the characteristics the following diÔ¨Äerential equation
du
dt = f ‚àía0u
on (x(t), t).
P0
Q
P
Œ≤
Œ±
x
t
¬Øt
x
t
t = 1
1
0
FIGURE 13.4. Left: examples of characteristics which are straight lines issuing
from the points P and Q. Right: characteristic straight lines for the Burgers
equation
Let us now consider problem (13.26) on a bounded interval. For example,
assume that x ‚àà[Œ±, Œ≤] and a > 0. Since u is constant along the character-
istics, from Figure 13.4 (left) we deduce that the value of the solution at P

13.6 Systems of Linear Hyperbolic Equations
599
attains the value of u0 at P0, the foot of the characteristic issuing from P.
On the other hand, the characteristic issuing from Q intersects the straight
line x(t) = Œ± at a certain time t = ¬Øt > 0. Thus, the point x = Œ± is an
inÔ¨Çow point and it is necessary to assign a boundary value for u at x = Œ±
for every t > 0. Notice that if a < 0 then the inÔ¨Çow point is x = Œ≤.
Referring to problem (13.26) it is worth noting that if u0 is discontinuous
at a point x0, then such a discontinuity propagates along the characteristics
issuing from x0. This process can be rigorously formalized by introducing
the concept of weak solutions of hyperbolic problems, see e.g. [GR96]. An-
other reason for introducing weak solutions is that in the case of nonlinear
hyperbolic problems the characteristic lines can intersect: in this case the
solution cannot be continuous and no classical solution does exist.
Example 13.3 (Burgers equation) Let us consider the Burgers equation
‚àÇu
‚àÇt + u‚àÇu
‚àÇx = 0
x ‚ààR
(13.29)
which is perhaps the simplest nontrivial example of a nonlinear hyperbolic equa-
tion. Taking as initial condition
u(x, 0) = u0(x) =
Ô£±
Ô£≤
Ô£≥
1
x ‚â§0,
1 ‚àíx
0 ‚â§x ‚â§1,
0
x ‚â•1,
the characteristic line issuing from the point (x0, 0) is given by
x(t) = x0 + tu0(x0) =
Ô£±
Ô£≤
Ô£≥
x0 + t
x0 ‚â§0,
x0 + t(1 ‚àíx0)
0 ‚â§x0 ‚â§1,
x0
x0 ‚â•1.
Notice that the characteristic lines do not intersect only if t < 1 (see Figure 13.4,
right).
‚Ä¢
13.6
Systems of Linear Hyperbolic Equations
Consider the linear hyperbolic systems of the form
‚àÇu
‚àÇt + A‚àÇu
‚àÇx = 0
x ‚ààR, t > 0,
(13.30)
where u : R √ó [0, ‚àû) ‚ÜíRp and A ‚ààRp√óp is a matrix with constant
coeÔ¨Écients.
This system is said hyperbolic if A is diagonalizable and has real eigen-
values, that is, if there exists a nonsingular matrix T ‚ààRp√óp such that
A = TŒõT‚àí1,

600
13. Parabolic and Hyperbolic Initial Boundary Value Problems
where Œõ = diag(Œª1, ..., Œªp) is the diagonal matrix of the real eigenvalues of
A, while T = (œâ1, œâ2, . . . , œâp) is the matrix whose column vectors are the
right eigenvectors of A (see Section 1.7). The system is said to be strictly
hyperbolic if it is hyperbolic with distinct eigenvalues. Thus
Aœâk = Œªkœâk,
k = 1, . . . , p.
Introducing the characteristic variables w = T‚àí1u, system (13.30) becomes
‚àÇw
‚àÇt + Œõ‚àÇw
‚àÇx = 0.
This is a system of p independent scalar equations of the form
‚àÇwk
‚àÇt + Œªk
‚àÇwk
‚àÇx = 0,
k = 1, . . . , p.
Proceeding as in Section 13.5 we obtain wk(x, t) = wk(x‚àíŒªkt, 0), and thus
the solution u = Tw of problem (13.30) can be written as
u(x, t) =
p

k=1
wk(x ‚àíŒªkt, 0)œâk.
The curve (xk(t), t) in the plane (x, t) that satisÔ¨Åes x‚Ä≤
k(t) = Œªk is the k-th
characteristic curve and wk is constant along it. A strictly hyperbolic sys-
tem enjoys the property that p distinct characteristic curves pass through
any point of the plane (x, t), for any Ô¨Åxed x and t. Then u(x, t) depends
only on the initial datum at the points x‚àíŒªkt. For this reason, the set of p
points that form the feet of the characteristics issuing from the point (x, t)
D(t, x) =

x ‚ààR : x = x ‚àíŒªkt , k = 1, ..., p

,
(13.31)
is called the domain of dependence of the solution u(x, t).
If (13.30) is set on a bounded interval (Œ±, Œ≤) instead of on the whole real
line, the inÔ¨Çow point for each characteristic variable wk is determined by the
sign of Œªk. Correspondingly, the number of positive eigenvalues determines
the number of boundary conditions that can be assigned at x = Œ±, whereas
at x = Œ≤ it is admissible to assign a number of conditions which equals the
number of negative eigenvalues. An example is discussed in Section 13.6.1.
Remark 13.2 (The nonlinear case) Let us consider the following non-
linear system of Ô¨Årst-order equations
‚àÇu
‚àÇt + ‚àÇ
‚àÇxg(u) = 0
(13.32)
where g = (g1, . . . , gp)T is called the Ô¨Çux function. The system is hyperbolic
if the jacobian matrix A(u) whose elements are aij = ‚àÇgi(u)/‚àÇuj, i, j =
1, . . . , p, is diagonalizable and has p real eigenvalues.
‚ñ†

13.6 Systems of Linear Hyperbolic Equations
601
13.6.1
The Wave Equation
Consider the second-order hyperbolic equation
‚àÇ2u
‚àÇt2 ‚àíŒ≥2 ‚àÇ2u
‚àÇx2 = f
x ‚àà(Œ±, Œ≤),
t > 0,
(13.33)
with initial data
u(x, 0) = u0(x)
and
‚àÇu
‚àÇt (x, 0) = v0(x),
x ‚àà(Œ±, Œ≤),
and boundary data
u(Œ±, t) = 0
and
u(Œ≤, t) = 0,
t > 0.
(13.34)
In this case, u may represent the transverse displacement of an elastic
vibrating string of length Œ≤‚àíŒ±, Ô¨Åxed at the endpoints, and Œ≥ is a coeÔ¨Écient
depending on the speciÔ¨Åc mass of the string and on its tension. The string
is subject to a vertical force of density f.
The functions u0(x) and v0(x) denote respectively the initial displace-
ment and the initial velocity of the string.
The change of variables
œâ1 = ‚àÇu
‚àÇx,
œâ2 = ‚àÇu
‚àÇt ,
transforms (13.33) into the following Ô¨Årst-order system
‚àÇœâ
‚àÇt + A‚àÇœâ
‚àÇx = 0
x ‚àà(Œ±, Œ≤),
t > 0,
(13.35)
where
œâ =

œâ1
œâ2

,
A =

0
‚àí1
‚àíŒ≥2
0

,
and the initial conditions are œâ1(x, 0) = u‚Ä≤
0(x) and œâ2(x, 0) = v0(x).
Since the eigenvalues of A are the two distinct real numbers ¬±Œ≥ (rep-
resenting the propagation velocities of the wave) we conclude that system
(13.35) is hyperbolic. Moreover, one boundary condition needs to be pre-
scribed at every end-point, as in (13.34). Notice that, also in this case,
smooth solutions correspond to smooth initial data, while any discontinuity
that is present in the initial data will propagate along the characteristics.
Remark 13.3 Notice that replacing ‚àÇ2u
‚àÇt2 by t2, ‚àÇ2u
‚àÇx2 by x2 and f by 1, the
wave equation becomes
t2 ‚àíŒ≥2x2 = 1

602
13. Parabolic and Hyperbolic Initial Boundary Value Problems
which represents an hyperbola in the (x, t) plane. Proceeding analogously
in the case of the heat equation (13.1), we end up with
t ‚àíŒΩx2 = 1
which represents a parabola in the (x, t) plane. Finally, for the Poisson
equation (12.90), replacing ‚àÇ2u
‚àÇx2 by x2, ‚àÇ2u
‚àÇy2 by y2 and f by 1, we get
x2 + y2 = 1
which represents an ellipse in the (x, y) plane.
Due to the geometric interpretation above, the corresponding diÔ¨Äerential
operators are classiÔ¨Åed as hyperbolic, parabolic and elliptic.
‚ñ†
13.7
The Finite DiÔ¨Äerence Method for Hyperbolic
Equations
Let us discretize the hyperbolic problem (13.26) by space-time Ô¨Ånite dif-
ferences. To this aim, the half-plane {(x, t) : ‚àí‚àû< x < ‚àû, t > 0} is dis-
cretized by choosing a spatial grid size ‚àÜx, a temporal step ‚àÜt and the
grid points (xj, tn) as follows
xj = j‚àÜx,
j ‚ààZ,
tn = n‚àÜt,
n ‚ààN.
Let us set
Œª = ‚àÜt/‚àÜx,
and deÔ¨Åne xj+1/2 = xj + ‚àÜx/2. We look for discrete solutions un
j which
approximate the values u(xj, tn) of the exact solution for any j, n.
Quite often, explicit methods are employed for advancing in time in
hyperbolic initial-value problems, even though they require restrictions on
the value of Œª, unlike what typically happens with implicit methods.
Let us focus our attention on problem (13.26). Any explicit Ô¨Ånite-diÔ¨Äerence
method can be written in the form
un+1
j
= un
j ‚àíŒª(hn
j+1/2 ‚àíhn
j‚àí1/2),
(13.36)
where hn
j+1/2 = h(un
j , un
j+1) for every j and h(¬∑, ¬∑) is a particular function
that is called the numerical Ô¨Çux.
13.7.1
Discretization of the Scalar Equation
We illustrate several instances of explicit methods, and provide the corre-
sponding numerical Ô¨Çux.

13.7 The Finite DiÔ¨Äerence Method for Hyperbolic Equations
603
1. Forward Euler/centred
un+1
j
= un
j ‚àíŒª
2 a(un
j+1 ‚àíun
j‚àí1)
(13.37)
which can be cast in the form (13.36) by setting
hj+1/2 = 1
2a(uj+1 + uj).
(13.38)
2. Lax-Friedrichs
un+1
j
= 1
2(un
j+1 + un
j‚àí1) ‚àíŒª
2 a(un
j+1 ‚àíun
j‚àí1)
(13.39)
which is of the form (13.36) with
hj+1/2 = 1
2[a(uj+1 + uj) ‚àíŒª‚àí1(uj+1 ‚àíuj)].
3. Lax-WendroÔ¨Ä
un+1
j
= un
j ‚àíŒª
2 a(un
j+1 ‚àíun
j‚àí1) + Œª2
2 a2(un
j+1 ‚àí2un
j + un
j‚àí1) (13.40)
which can be written in the form (13.36) provided that
hj+1/2 = 1
2[a(uj+1 + uj) ‚àíŒªa2(uj+1 ‚àíuj)].
4. Upwind (or forward Euler/uncentred)
un+1
j
= un
j ‚àíŒª
2 a(un
j+1 ‚àíun
j‚àí1) + Œª
2 |a|(un
j+1 ‚àí2un
j + un
j‚àí1)
(13.41)
which Ô¨Åts the form (13.36) when the numerical Ô¨Çux is deÔ¨Åned to be
hj+1/2 = 1
2[a(uj+1 + uj) ‚àí|a|(uj+1 ‚àíuj)].
The last three methods can be obtained from the forward Euler/centred
method by adding a term proportional to a numerical diÔ¨Äusion, so that
they can be written in the equivalent form
un+1
j
= un
j ‚àíŒª
2 a(un
j+1 ‚àíun
j‚àí1) + 1
2k un
j+1 ‚àí2un
j + un
j‚àí1
(‚àÜx)2
,
(13.42)
where the artiÔ¨Åcial viscosity k is given for the three cases in Table 13.1.

604
13. Parabolic and Hyperbolic Initial Boundary Value Problems
methods
k
hdiff
j+1/2
œÑ(‚àÜt, ‚àÜx)
Lax-Friedrichs
‚àÜx2
‚àí1
2Œª(uj+1 ‚àíuj)
O
‚àÜx2
‚àÜt + ‚àÜt + ‚àÜx

Lax-WendroÔ¨Ä
a2‚àÜt2
‚àíŒªa2
2 (uj+1 ‚àíuj)
O

‚àÜt2 + ‚àÜx2
Upwind
|a|‚àÜx‚àÜt
‚àí|a|
2 (uj+1 ‚àíuj)
O(‚àÜt + ‚àÜx)
TABLE
13.1.
ArtiÔ¨Åcial
viscosity,
artiÔ¨Åcial
Ô¨Çux
and
truncation
error
for
Lax-Friedrichs, Lax-WendroÔ¨Äand Upwind methods
As a consequence, the numerical Ô¨Çux for each scheme can be written equiv-
alently as
hj+1/2 = hF E
j+1/2 + hdiff
j+1/2
where hF E
j+1/2 is the numerical Ô¨Çux of the forward Euler/centred scheme
(which is given in (13.38)) and the artiÔ¨Åcial diÔ¨Äusion Ô¨Çux hdiff
j+1/2 is given
for the three cases in Table 13.1.
An example of an implicit method is the backward Euler/centred scheme
un+1
j
+ Œª
2 a(un+1
j+1 ‚àíun+1
j‚àí1 ) = un
j .
(13.43)
It can still be written in the form (13.36) provided that hn is replaced by
hn+1. In the example at hand, the numerical Ô¨Çux is the same as for the
Forward Euler/centred method, and so is the artiÔ¨Åcial viscosity.
Finally, we report the following schemes for the approximation of the
second-order wave equation (13.33):
1. Leap-Frog
un+1
j
‚àí2un
j + un‚àí1
j
= (Œ≥Œª)2(un
j+1 ‚àí2un
j + un
j‚àí1)
(13.44)
2. Newmark
un+1
j
‚àíun
j = ‚àÜtvn
j + (Œ≥Œª)2 0
Œ≤wn+1
j
+
 1
2 ‚àíŒ≤

wn
j
1
,
vn+1
j
‚àívn
j = (Œ≥Œª)2
‚àÜt
0
Œ∏wn+1
j
+ (1 ‚àíŒ∏)wn
j
1
(13.45)
with wj = uj+1‚àí2uj+uj‚àí1 and where the parameters Œ≤ and Œ∏ satisfy
0 ‚â§Œ≤ ‚â§1
2, 0 ‚â§Œ∏ ‚â§1.

13.8 Analysis of Finite DiÔ¨Äerence Methods
605
13.8
Analysis of Finite DiÔ¨Äerence Methods
Let us analyze the properties of consistency, stability and convergence, as
well as those of dissipation and dispersion, of the Ô¨Ånite diÔ¨Äerence methods
introduced above.
13.8.1
Consistency
As illustrated in Section 11.3, the local truncation error of a numerical
scheme is the residual that is generated by pretending the exact solution
to satisfy the numerical method itself.
Denoting by u the solution of the exact problem (13.26), in the case of
method (13.37) the local truncation error at (xj, tn) is deÔ¨Åned as follows
œÑ n
j = u(xj, tn+1) ‚àíu(xj, tn)
‚àÜt
‚àíau(xj+1, tn) ‚àíu(xj‚àí1, tn)
2‚àÜx
.
The truncation error is
œÑ(‚àÜt, ‚àÜx) = max
j,n |œÑ n
j |.
When œÑ(‚àÜt, ‚àÜx) goes to zero as ‚àÜt and ‚àÜx tend to zero independently,
the numerical scheme is said to be consistent.
Moreover, we say that it is of order p in time and of order q in space (for
suitable integers p and q), if, for a suÔ¨Éciently smooth solution of the exact
problem, we have
œÑ(‚àÜt, ‚àÜx) = O(‚àÜtp + ‚àÜxq).
Using Taylor‚Äôs expansion conveniently we can characterize the truncation
error of the methods previously introduced as indicated in Table 13.1. The
Leap-frog and Newmark methods are both second order accurate if ‚àÜt =
‚àÜx, while the forward (or backward) Euler centred method is O(‚àÜt+‚àÜx2).
Finally, we say that a numerical scheme is convergent if
lim
‚àÜt,‚àÜx‚Üí0max
j,n |u(xj, tn) ‚àíun
j | = 0.
13.8.2
Stability
A numerical method for a hyperbolic problem (linear or nonlinear) is said
to be stable if, for any time T, there exist two constants CT > 0 (possibly
depending on T) and Œ¥0 > 0, such that
‚à•un‚à•‚àÜ‚â§CT ‚à•u0‚à•‚àÜ,
(13.46)
for any n such that n‚àÜt ‚â§T and for any ‚àÜt, ‚àÜx such that 0 < ‚àÜt ‚â§Œ¥0,
0 < ‚àÜx ‚â§Œ¥0. We have denoted by ‚à•¬∑ ‚à•‚àÜa suitable discrete norm, for

606
13. Parabolic and Hyperbolic Initial Boundary Value Problems
instance one of those indicated below
‚à•v‚à•‚àÜ,p =
Ô£´
Ô£≠‚àÜx
‚àû

j=‚àí‚àû
|vj|p
Ô£∂
Ô£∏
1
p
p = 1, 2,
‚à•v‚à•‚àÜ,‚àû= sup
j
|vj|.
(13.47)
Note that ‚à•¬∑‚à•‚àÜ,p is an approximation of the norm of Lp(R). For instance, the
implicit Bacward Euler/centred scheme (13.43) is unconditionally stable
with respect to the norm ‚à•¬∑ ‚à•‚àÜ,2 (see Exercise 7).
13.8.3
The CFL Condition
Courant, Friedrichs and Lewy [CFL28] have shown that a necessary and
suÔ¨Écient condition for any explicit scheme of the form (13.36) to be stable
is that the time and space discretization steps must obey the following
condition
|aŒª| =
a ‚àÜt
‚àÜx
 ‚â§1
(13.48)
which is known as the CFL condition. The number aŒª, which is an adi-
mensional number since a is a velocity, is commonly referred to as the CFL
number. If a is not constant the CFL condition becomes
‚àÜt ‚â§
‚àÜx
sup
x‚ààR, t>0
|a(x, t)|,
while, in the case of the hyperbolic system (13.30), the stability condition
becomes
Œªk
‚àÜt
‚àÜx
 ‚â§1
k = 1, . . . , p,
where {Œªk : k = 1 . . . , p} are the eigenvalues of A.
The CFL stability condition has the following geometric interpretation.
In a Ô¨Ånite diÔ¨Äerence scheme the value un+1
j
depends, in general, on the
values of un at the three points xj+i, i = ‚àí1, 0, 1. Thus, at the time t = 0
the solution un+1
j
will depend only on the initial data at the points xj+i,
for i = ‚àí(n + 1), . . . , (n + 1) (see Figure 13.5).
Let us deÔ¨Åne numerical domain of dependence D‚àÜt(xj, tn) to be the set
of values at time t = 0 the numerical solution un
j depends on, that is
D‚àÜt(xj, tn) ‚äÇ
%
x ‚ààR : |x ‚àíxj| ‚â§n‚àÜx = tn
Œª
&
.

13.8 Analysis of Finite DiÔ¨Äerence Methods
607
xj+(n+1)
t
tn+1
tn
xj‚àí1
xj‚àí(n+1)
xj
xj+1
x
t1
t0
FIGURE 13.5. The numerical domain of dependence D‚àÜt(xj, tn+1)
Consequently, for any Ô¨Åxed point (x, t) we have
D‚àÜt(x, t) ‚äÇ
%
x ‚ààR : |x ‚àíx| ‚â§t
Œª
&
.
In particular, taking the limit as ‚àÜt ‚Üí0 for a Ô¨Åxed Œª, the numerical domain
of dependence becomes
D0(x, t) =
%
x ‚ààR : |x ‚àíx| ‚â§t
Œª
&
.
The condition (13.48) is thus equivalent to the inclusion
D(x, t) ‚äÇD0(x, t),
(13.49)
where D(x, t) is the domain of dependence deÔ¨Åned in (13.31).
In the case of an hyperbolic system, thanks to (13.49), we can conclude
that the CFL condition requires that any straight line x = x ‚àíŒªk(t ‚àít),
k = 1, . . . , p, must intersect the temporal straight line t = t ‚àí‚àÜt at some
point x belonging to the domain of dependence (see Figure 13.6).
Let us analyze the stability properties of some of the methods introduced
in the previous section.
Assuming that a > 0, the upwind scheme (13.41) can be reformulated as
un+1
j
= un
j ‚àíŒªa(un
j ‚àíun
j‚àí1).
(13.50)
Therefore
‚à•un+1‚à•‚àÜ,1 ‚â§‚àÜx

j
|(1 ‚àíŒªa)un
j | + ‚àÜx

j
|Œªaun
j‚àí1|.

608
13. Parabolic and Hyperbolic Initial Boundary Value Problems
¬Øx + ‚àÜx
¬Øt
(¬Øx, ¬Øt)
¬Øx ‚àí‚àÜx
¬Øx
¬Øx + ‚àÜx
(¬Øx, ¬Øt)
r1
r2
r2
r1
¬Øt
¬Øt ‚àí‚àÜt
¬Øx ‚àí‚àÜx
¬Øx
FIGURE 13.6. Geometric interpretation of the CFL condition for a system with
p = 2, where ri = ¬Øx ‚àíŒªi(t ‚àí¬Øt) i = 1, 2. The CFL condition is satisÔ¨Åed for the
left-hand case, while it is violated for the right-hand case
Both Œªa and 1 ‚àíŒªa are nonnegative if (13.48) holds. Thus
‚à•un+1‚à•‚àÜ,1 ‚â§‚àÜx(1 ‚àíŒªa)

j
|un
j | + ‚àÜxŒªa

j
|un
j‚àí1| = ‚à•un‚à•‚àÜ,1.
Inequality (13.46) is therefore satisÔ¨Åed by taking CT = 1 and ‚à•¬∑‚à•‚àÜ= ‚à•¬∑‚à•‚àÜ,1.
The Lax-Friedrichs scheme is also stable, upon assuming (13.48). Indeed,
from (13.39) we get
un+1
j
= 1
2(1 ‚àíŒªa)un
j+1 + 1
2(1 + Œªa)un
j‚àí1.
Therefore,
‚à•un+1‚à•‚àÜ,1
‚â§
1
2‚àÜx
Ô£Æ
Ô£∞


j
(1 ‚àíŒªa)un
j+1

+


j
(1 + Œªa)un
j‚àí1

Ô£π
Ô£ª
‚â§
1
2(1 ‚àíŒªa)‚à•un‚à•‚àÜ,1 + 1
2(1 + Œªa)‚à•un‚à•‚àÜ,1 = ‚à•un‚à•‚àÜ,1.
Also the Lax-WendroÔ¨Äscheme is stable under the usual assumption (13.48)
on ‚àÜt (for the proof see, e.g., [QV94] Chapter 14).
13.8.4
Von Neumann Stability Analysis
Let us now show that the condition (13.48) is not suÔ¨Écient to ensure that
the forward Euler/centred scheme (13.37) is stable. For this purpose, we
make the assumption that the function u0(x) is 2œÄ-periodic so that it can
be expanded in a Fourier series as
u0(x) =
‚àû

k=‚àí‚àû
Œ±keikx
(13.51)

13.8 Analysis of Finite DiÔ¨Äerence Methods
609
where
Œ±k = 1
2œÄ
2œÄ
>
0
u0(x) e‚àíikx dx
is the k-th Fourier coeÔ¨Écient of u0 (see Section 10.9). Therefore,
u0
j = u0(xj) =
‚àû

k=‚àí‚àû
Œ±keikjh
j = 0, ¬±1, ¬±2, ¬∑ ¬∑ ¬∑
where we have set h = ‚àÜx for ease of notation. Applying (13.37) with n = 0
we get
u1
j
=
‚àû

k=‚àí‚àû
Œ±keikjh

1 ‚àía‚àÜt
2h (eikh ‚àíe‚àíikh)

=
‚àû

k=‚àí‚àû
Œ±keikjh

1 ‚àía‚àÜt
h i sin(kh)

.
Setting
Œ≥k = 1 ‚àía‚àÜt
h i sin(kh),
and proceeding recursively on n yields
un
j =
‚àû

k=‚àí‚àû
Œ±keikjhŒ≥n
k
j = 0, ¬±1, ¬±2, . . . ,
n ‚â•1.
(13.52)
The number Œ≥k ‚ààC is said to be the ampliÔ¨Åcation coeÔ¨Écient of the k-th
frequency (or harmonic) at each time step. Since
|Œ≥k| =
"
1 +
a‚àÜt
h
sin(kh)
2# 1
2
,
we deduce that
|Œ≥k| > 1 if
a Ã∏= 0 and
k Ã∏= mœÄ
h ,
m = 0, ¬±1, ¬±2, . . .
Correspondingly, the nodal values |un
j | continue to grow as n ‚Üí‚àûand the
numerical solution ‚Äùblows-up‚Äù whereas the exact solution satisÔ¨Åes
|u(x, t)| = |u0(x ‚àíat)| ‚â§max
s‚ààR |u0(s)|
‚àÄx ‚ààR,
‚àÄt > 0.
The centred discretization scheme (13.37) is thus unconditionally unstable,
i.e., it is unstable for any choice of the parameters ‚àÜt and ‚àÜx.

610
13. Parabolic and Hyperbolic Initial Boundary Value Problems
The previous analysis is based on the Fourier series expansion and is
called von Neumann analysis. It can be applied to studying the stability of
any numerical scheme with respect to the norm ‚à•¬∑‚à•‚àÜ,2 and for establishing
the dissipation and dispersion of the method.
Any explicit Ô¨Ånite diÔ¨Äerence numerical scheme for problem (13.26) satis-
Ô¨Åes a recursive relation analogous to (13.52), where Œ≥k depends a priori on
‚àÜt and h and is called the k-th ampliÔ¨Åcation coeÔ¨Écient of the numerical
scheme at hand.
Theorem 13.1 Assume that for a suitable choice of ‚àÜt and h, |Œ≥k| ‚â§1
‚àÄk; then, the numerical scheme is stable with respect to the ‚à•¬∑ ‚à•‚àÜ,2 norm.
Proof. Take an initial datum with a Ô¨Ånite Fourier expansion
u0(x) =
N
2 ‚àí1

k=‚àíN
2
Œ±keikx
where N is a positive integer. Without loss of generality we can assume that
problem (13.26) is well-posed on [0, 2œÄ] since u0 is a 2œÄ-periodic function. Take
in this interval N equally spaced nodes
xj = jh
j = 0, . . . , N ‚àí1,
with
h = 2œÄ
N ,
at which the numerical scheme (13.36) is applied. We get
u0
j = u0(xj) =
N
2 ‚àí1

k=‚àíN
2
Œ±keikjh,
un
j =
N
2 ‚àí1

k=‚àíN
2
Œ±kŒ≥n
k eikjh.
Notice that
‚à•un‚à•2
‚àÜ,2 = h
N‚àí1

j=0
N
2 ‚àí1

k,m=‚àíN
2
Œ±kŒ±m(Œ≥kŒ≥m)nei(k‚àím)jh.
Recalling Lemma 10.1 we have
h
N‚àí1

j=0
ei(k‚àím)jh = 2œÄŒ¥km,
‚àíN
2 ‚â§k, m ‚â§N
2 ‚àí1,
which yields
‚à•un‚à•2
‚àÜ,2 = 2œÄ
N
2 ‚àí1

k=‚àíN
2
|Œ±k|2|Œ≥k|2n.
As a consequence, since |Œ≥k| ‚â§1 ‚àÄk, it turns out that
‚à•un‚à•2
‚àÜ,2 ‚â§2œÄ
N
2 ‚àí1

k=‚àíN
2
‚à•Œ±k‚à•2 = ‚à•u0‚à•2
‚àÜ,2,
‚àÄn ‚â•0,

13.9 Dissipation and Dispersion
611
which proves that the scheme is stable with respect to the ‚à•¬∑ ‚à•‚àÜ,2 norm.
3
In the case of the upwind scheme (13.41), proceeding as was done for
the centred scheme, we Ô¨Ånd the following ampliÔ¨Åcation coeÔ¨Écients (see
Exercise 6)
Œ≥k =
Ô£±
Ô£¥
Ô£≤
Ô£¥
Ô£≥
1 ‚àía‚àÜt
h (1 ‚àíe‚àíikh)
if a > 0,
1 ‚àía‚àÜt
h (e‚àíikh ‚àí1)
if a < 0.
Therefore
‚àÄk,
|Œ≥k| ‚â§1 if
‚àÜt ‚â§h
|a|,
which is nothing but the CFL condition.
Thanks to Theorem 13.1, if the CFL condition is satisÔ¨Åed, the upwind
scheme is stable with respect to the ‚à•¬∑ ‚à•‚àÜ,2 norm.
We conclude by noting that the upwind scheme (13.50) satisÔ¨Åes
un+1
j
= (1 ‚àíŒªa)un
j + Œªaun
j‚àí1.
Owing to (13.48), either Œªa or 1 ‚àíŒªa are nonnegative, thus
min(un
j , un
j‚àí1) ‚â§un+1
j
‚â§max(un
j , un
j‚àí1).
It follows that
inf
l‚ààZ

u0
l

‚â§un
j ‚â§sup
l‚ààZ

u0
l

‚àÄj ‚ààZ, ‚àÄn ‚â•0,
that is,
‚à•un‚à•‚àÜ,‚àû‚â§‚à•u0‚à•‚àÜ,‚àû
‚àÄn ‚â•0,
(13.53)
which proves that if (13.48) is satisÔ¨Åed, the upwind scheme is stable in the
norm ‚à•¬∑‚à•‚àÜ,‚àû. The relation (13.53) is called the discrete maximum principle
(see also Section 12.2.2).
Remark 13.4 For the approximation of the wave equation (13.33) the
Leap-Frog method (13.44) is stable under the CFL restriction ‚àÜt ‚â§‚àÜx/|Œ≥|,
while the Newmark method (13.45) is unconditionally stable if 2Œ≤ ‚â•Œ∏ ‚â•1
2
(see [Joh90]).
‚ñ†
13.9
Dissipation and Dispersion
The von Neumann analysis on the ampliÔ¨Åcation coeÔ¨Écients enlightens the
study of the stability and dissipation of a numerical scheme.

612
13. Parabolic and Hyperbolic Initial Boundary Value Problems
Consider the exact solution to problem (13.26); the following relation
holds
u(x, tn) = u0(x ‚àían‚àÜt),
‚àÄn ‚â•0,
‚àÄx ‚ààR.
In particular, from applying (13.51) it follows that
u(xj, tn) =
‚àû

k=‚àí‚àû
Œ±keikjhgn
k ,
where
gk = e‚àíiak‚àÜt.
(13.54)
Letting
œïk = k‚àÜx,
we have k‚àÜt = Œªœïk and thus
gk = e‚àíiaŒªœïk.
(13.55)
The real number œïk, here expressed in radians, is called the phase angle of
the k-th harmonic. Comparing (13.54) with (13.52) we can see that Œ≥k is
the counterpart of gk which is generated by the speciÔ¨Åc numerical method
at hand. Moreover, |gk| = 1, whereas |Œ≥k| ‚â§1, in order to ensure stability.
Thus, Œ≥k is a dissipation coeÔ¨Écient; the smaller |Œ≥k|, the higher the reduc-
tion of the amplitude Œ±k, and, as a consequence, the higher the numerical
dissipation.
The ratio œµa(k) = |Œ≥k|
|gk| is called the ampliÔ¨Åcation error of the k-th harmonic
associated with the numerical scheme (in our case it coincides with the
ampliÔ¨Åcation coeÔ¨Écient). On the other hand, writing
Œ≥k = |Œ≥k|e‚àíiœâ‚àÜt = |Œ≥k|e
‚àíiœâ
k Œªœïk,
and comparing this relation with (13.55), we can identify the velocity of
propagation of the numerical solution, relative to its k-th harmonic, as
being œâ
k . The ratio between this velocity and the velocity a of the exact
solution is called the dispersion error œµd relative to the k-th harmonic
œµd(k) = œâ
ka = œâ‚àÜx
œïka .
The ampliÔ¨Åcation and dispersion errors for the numerical schemes exam-
ined so far are functions of the phase angle œïk and the CFL number aŒª.
This is shown in Figure 13.7 where we have only considered the interval
0 ‚â§œïk ‚â§œÄ and we have used degrees instead of radians to denote the
values of œïk.
In Figure 13.8 the numerical solutions of equation (13.26) with a = 1
and the initial datum u0 given by a packet of two sinusoidal waves of equal
wavelength l and centred at the origin x = 0 are shown. The plots on the
left-side of the Ô¨Ågure refer to the case l = 10‚àÜx while on the right-side

13.9 Dissipation and Dispersion
613
0
20
40
60
80
100
120
140
160
180
0.2
0.4
0.6
0.8
1
Amplification error for Lax‚àíFriedrichs
œÜ
Œµ
a
0
20
40
60
80
100
120
140
160
180
1
2
3
4
5
Dispersion error for Lax‚àíFriedrichs
œÜ
Œµ
œÜ
CFL=  0.25
CFL=  0.50
CFL=  0.75
CFL=  1.00
CFL=  0.25
CFL=  0.50
CFL=  0.75
CFL=  1.00
0
20
40
60
80
100
120
140
160
180
0
0.2
0.4
0.6
0.8
1
1.2
Amplification error for Lax‚àíWendroff
œÜ
Œµ
a
0
20
40
60
80
100
120
140
160
180
0
0.2
0.4
0.6
0.8
1
1.2
1.4
Dispersion error for Lax‚àíWendroff
œÜ
Œµ
œÜ
CFL=  0.25
CFL=  0.50
CFL=  0.75
CFL=  1.00
CFL=  0.25
CFL=  0.50
CFL=  0.75
CFL=  1.00
0
20
40
60
80
100
120
140
160
180
‚àí0.2
0
0.2
0.4
0.6
0.8
1
1.2
Amplification error for Upwind
œÜ
Œµ
a
0
20
40
60
80
100
120
140
160
180
0
0.2
0.4
0.6
0.8
1
1.2
1.4
Dispersion error for Upwind
œÜ
Œµ
œÜ
CFL=  0.25
CFL=  0.50
CFL=  0.75
CFL=  1.00
CFL=  0.25
CFL=  0.50
CFL=  0.75
CFL=  1.00
FIGURE 13.7. AmpliÔ¨Åcation and dispersion errors for several numerical schemes
we have l = 4‚àÜx. Since k = (2œÄ)/l we get œïk = ((2œÄ)/l)‚àÜx, so that
œïk = œÄ/10 in the left-side pictures and œïk = œÄ/4 in the right-side ones.
All numerical solutions have been computed for a CFL number equal to
0.75, using the schemes introduced above. Notice that the dissipation eÔ¨Äect
is quite relevant at high frequencies (œïk = œÄ/4), especially for Ô¨Årst-order
methods (such as the upwind and the Lax-Friedrichs methods).
In order to highlight the eÔ¨Äects of the dispersion, the same computa-
tions have been repeated for œïk = œÄ/3 and diÔ¨Äerent values of the CFL
number. The numerical solutions after 5 time steps are shown in Figure
13.9. The Lax-WendroÔ¨Ämethod is the least dissipative for all the consid-
ered CFL numbers. Moreover, a comparison of the positions of the peaks
of the numerical solutions with respect to the corresponding ones in the

614
13. Parabolic and Hyperbolic Initial Boundary Value Problems
‚àí4
‚àí3
‚àí2
‚àí1
0
1
2
3
4
‚àí1
‚àí0.5
0
0.5
1
x
u
Lax‚àíWendroff CFL=  0.75 œÜ=œÄ/10
Computed at t=1
Exact
‚àí4
‚àí3
‚àí2
‚àí1
0
1
2
3
4
‚àí1
‚àí0.5
0
0.5
1
x
u
Lax‚àíFriedrichs CFL=  0.75 œÜ=œÄ/10
Computed at t=1
Exact
‚àí4
‚àí3
‚àí2
‚àí1
0
1
2
3
4
‚àí1
‚àí0.5
0
0.5
1
x
u
Upwind CFL=  0.75 œÜ=œÄ/10
Computed at t=1
Exact
‚àí4
‚àí3
‚àí2
‚àí1
0
1
2
3
4
‚àí1
‚àí0.5
0
0.5
1
x
u
Lax‚àíWendroff CFL=  0.75 œÜ=œÄ/4
Computed at t=1
Exact
‚àí4
‚àí3
‚àí2
‚àí1
0
1
2
3
4
‚àí1
‚àí0.5
0
0.5
1
x
u
Lax‚àíFriedrichs CFL=  0.75 œÜ=œÄ/4
Computed at t=1
Exact
‚àí4
‚àí3
‚àí2
‚àí1
0
1
2
3
4
‚àí1
‚àí0.5
0
0.5
1
x
u
Upwind CFL=  0.75 œÜ=œÄ/4
Computed at t=1
Exact
FIGURE 13.8. Numerical solutions corresponding to the transport of a sinusoidal
wave packet with diÔ¨Äerent wavelengths
exact solution shows that the Lax-Friedrichs scheme is aÔ¨Äected by a posi-
tive dispersion error, since the ‚Äùnumerical‚Äù wave advances faster than the
exact one. Also, the upwind scheme exhibits a slight dispersion error for a
CFL number of 0.75 which is absent for a CFL number of 0.5. The peaks
are well aligned with those of the numerical solution, although they have
been reduced in amplitude due to numerical dissipation. Finally, the Lax-
WendroÔ¨Ämethod exhibits a small negative dispersion error; the numerical
solution is indeed slightly late with respect to the exact one.
13.9.1
Equivalent Equations
Using Taylor‚Äôs expansion to the third order to represent the truncation er-
ror, it is possible to associate with any of the numerical schemes introduced

13.9 Dissipation and Dispersion
615
‚àí1
‚àí0.5
0
0.5
1
1.5
2
‚àí1
‚àí0.5
0
0.5
1
u
Lax‚àíWendroff CFL=0.50 œÜ=œÄ/3, t=4‚àÜ t
Computed
Exact
‚àí1
‚àí0.5
0
0.5
1
1.5
2
‚àí1
‚àí0.5
0
0.5
1
u
Lax‚àíWendroff CFL=0.75 œÜ=œÄ/3, t=4‚àÜ t
Computed
Exact
‚àí1
‚àí0.5
0
0.5
1
1.5
2
‚àí1
‚àí0.5
0
0.5
1
u
Lax‚àíFriedrichs CFL=0.50 œÜ=œÄ/3, t=5‚àÜ t
Computed
Exact
‚àí1
‚àí0.5
0
0.5
1
1.5
2
‚àí1
‚àí0.5
0
0.5
1
u
Lax‚àíFriedrichs CFL=0.75 œÜ=œÄ/3, t=5‚àÜ t
Computed
Exact
‚àí1
‚àí0.5
0
0.5
1
1.5
2
‚àí1
‚àí0.5
0
0.5
1
u
Upwind CFL=0.50 œÜ=œÄ/3, t=4‚àÜ t
Computed
Exact
‚àí1
‚àí0.5
0
0.5
1
1.5
2
‚àí1
‚àí0.5
0
0.5
1
u
Upwind CFL=0.75 œÜ=œÄ/3, t=4‚àÜ t
Computed
Exact
FIGURE 13.9. Numerical solutions corresponding to the transport of a sinusoidal
wave packet, for diÔ¨Äerent CFL numbers
so far an equivalent diÔ¨Äerential equation of the form
vt + avx = ¬µvxx + ŒΩvxxx
(13.56)
where the terms ¬µvxx and ŒΩvxxx represent dissipation and dispersion, re-
spectively. Table 13.2 shows the values of ¬µ and ŒΩ for the various methods.
Let us give a proof of this procedure in the case of the upwind scheme.
Let v(x, t) be a smooth function which satisÔ¨Åes the diÔ¨Äerence equation
(13.41); then, assuming that a > 0, we have
v(x, t + ‚àÜt) ‚àív(x, t)
‚àÜt
+ av(x, t) ‚àív(x ‚àí‚àÜx, t)
‚àÜx
= 0.
Truncating the Taylor expansions of v around (x, t) at the Ô¨Årst and second
order, respectively, we obtain
vt + O(‚àÜt) + avx + O(‚àÜx) = 0
(13.57)

616
13. Parabolic and Hyperbolic Initial Boundary Value Problems
Method
¬µ
ŒΩ
Upwind
a‚àÜx
2
‚àía2‚àÜt
2
‚àía
6

‚àÜx2 ‚àí3a‚àÜx‚àÜt + 2a2‚àÜt2
Lax-Friedrichs
‚àÜx2
2‚àÜt

1 ‚àí(aŒª)2
a‚àÜx2
3

1 ‚àí(aŒª)2
Lax-WendroÔ¨Ä
0
a‚àÜx2
6

(aŒª)2 ‚àí1

TABLE 13.2. Values of dissipation and dispersion coeÔ¨Écients for several numer-
ical methods
and
vt + ‚àÜt
2 vtt + O(‚àÜt2) + a

vx + ‚àÜx
2 vxx + O(‚àÜx2)

= 0,
(13.58)
where vt = ‚àÇv
‚àÇt and vx = ‚àÇv
‚àÇx.
DiÔ¨Äerentiating (13.57) with respect to t and then with respect to x, we
get
vtt + avxt = O(‚àÜx + ‚àÜt),
and
vtx + avxx = O(‚àÜx + ‚àÜt).
Thus, it follows that
vtt = a2vxx + O(‚àÜx + ‚àÜt),
which, after substituting into (13.58), yields the following equation
vt + avx = ¬µvxx
(13.59)
where
¬µ = a‚àÜx
2
‚àía2‚àÜt
2
,
and having neglected the term O(‚àÜx2+‚àÜt2). Relation (13.59) is the equiv-
alent diÔ¨Äerential equation up to second order of the upwind scheme.
Following the same procedure and truncating the Taylor expansion at
third order, yields
vt + avx = ¬µvxx + ŒΩvxxx
(13.60)
where
ŒΩ = a
6

a2‚àÜt2 ‚àí‚àÜx2
.
We can give a heuristic explanation of the meaning of the dissipative
and dispersive terms in the equivalent equation (13.56) by studying the
following problem
" vt + avx = ¬µvxx + ŒΩvxxx
x ‚ààR, t > 0
v(x, 0) = eikx,
(k ‚ààZ)
(13.61)

13.9 Dissipation and Dispersion
617
Applying the Fourier transform yields, if ¬µ = ŒΩ = 0,
v(x, t) = eik(x‚àíat),
(13.62)
while if ¬µ and ŒΩ are arbitrary real numbers (with ¬µ > 0) we get
v(x, t) = e‚àí¬µk2teik[x‚àí(a+ŒΩk2)t].
(13.63)
Comparing (13.62) with (13.63) we can see that the module of the solution
diminishes as ¬µ grows and this becomes more relevant if the frequency k
gets larger. Therefore, the term ¬µvxx in (13.61) has a dissipative eÔ¨Äect on
the solution. A further comparison between (13.62) and (13.63) shows that
the presence of the term ŒΩ modiÔ¨Åes the velocity of the propagation of the
solution; the velocity is increased if ŒΩ > 0 whereas it is diminuished if ŒΩ < 0.
Even in this case the eÔ¨Äect is ampliÔ¨Åed at high frequencies. Therefore, the
third-order diÔ¨Äerential term ŒΩvxxx introduces a dispersive eÔ¨Äect.
Generally speaking, even-order derivatives in the equivalent equation rep-
resent diÔ¨Äusive terms, while odd-order derivatives mean dispersive eÔ¨Äects.
In the case of Ô¨Årst-order schemes (like the upwind method) the dispersive
eÔ¨Äect is often only slightly visible since it is hidden by the dissipative one.
Actually, taking ‚àÜt and ‚àÜx of the same order, we have that ŒΩ << ¬µ as
‚àÜx ‚Üí0, since ŒΩ = O(‚àÜx2) and ¬µ = O(‚àÜx). In particular, for a CFL
number of 1
2, the equivalent equation of the upwind method exhibits null
dispersion, truncated at second order, according to the results of the pre-
vious section.
On the other hand, the dispersive eÔ¨Äect is strikingly visible in the Lax-
Friedrichs and in the Lax-WendroÔ¨Äschemes; the latter, being second-order
accurate, does not exhibit a dissipative term of the form ¬µvxx. However,
it ought to be dissipative in order to be stable; actually, the equivalent
equation (truncated at fourth order) for the Lax-WendroÔ¨Äscheme reads
vt + avx = a‚àÜx2
6
[(aŒª)2 ‚àí1]vxxx ‚àía‚àÜx3
6
aŒª[1 ‚àí(aŒª)2]vxxxx
where the last term is dissipative if |aŒª| < 1. We thus recover the CFL
condition for the Lax-WendroÔ¨Ämethod.

618
13. Parabolic and Hyperbolic Initial Boundary Value Problems
13.10
Finite Element Approximation of Hyperbolic
Equations
Let us consider the following Ô¨Årst-order linear, scalar hyperbolic problem
in the interval ‚Ñ¶= (Œ±, Œ≤) ‚äÇR
Ô£±
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≥
‚àÇu
‚àÇt + a‚àÇu
‚àÇx + a0u = f
in QT = ‚Ñ¶√ó (0, T)
u(Œ±, t) = œï(t)
t ‚àà(0, T)
u(x, 0) = u0(x)
x ‚àà‚Ñ¶,
(13.64)
where a = a(x), a0 = a0(x, t), f = f(x, t), œï = œï(t) and u0 = u0(x) are
given functions.
We assume that a(x) > 0 ‚àÄx ‚àà[Œ±, Œ≤]. In particular, this implies that
the point x = Œ± is the inÔ¨Çow boundary, and the boundary value has to be
speciÔ¨Åed there.
13.10.1
Space Discretization with Continuous and
Discontinuous Finite Elements
A semi-discrete approximation of problem (13.64) can be carried out by
means of the Galerkin method (see Section 12.4). DeÔ¨Åne the spaces
Vh = Xr
h =

vh ‚ààC0(‚Ñ¶) : vh|Ij ‚ààPr(Ij), ‚àÄIj ‚ààTh

and
V in
h
= {vh ‚ààVh : vh(Œ±) = 0} ,
where Th is a partition of ‚Ñ¶(see Section 12.4.5) into n ‚â•2 subintervals
Ij = [xj, xj+1], for j = 0, . . . , n ‚àí1.
Let u0,h be a suitable Ô¨Ånite element approximation of u0 and consider
the problem: for any t ‚àà(0, T) Ô¨Ånd uh(t) ‚ààVh such that
Ô£±
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≥
Œ≤
>
Œ±
‚àÇuh(t)
‚àÇt
vh dx
+
Œ≤
>
Œ±

a‚àÇuh(t)
‚àÇx
+ a0(t)uh(t)

vh dx
=
Œ≤
>
Œ±
f(t)vh dx
‚àÄvh ‚ààV in
h
uh(t) = œïh(t)
at
x = Œ±,
(13.65)
with uh(0) = u0,h ‚ààVh.

13.10 Finite Element Approximation of Hyperbolic Equations
619
If œï is equal to zero, uh(t) ‚ààV in
h , and we are allowed to taking vh = uh(t)
and get the following inequality
‚à•uh(t)‚à•2
L2(Œ±,Œ≤)
+
t
>
0
¬µ0‚à•uh(œÑ)‚à•2
L2(Œ±,Œ≤) dœÑ + a(Œ≤)
t
>
0
u2
h(œÑ) dœÑ
‚â§
‚à•u0,h‚à•2
L2(Œ±,Œ≤) +
t
>
0
1
¬µ0
‚à•f(œÑ)‚à•2
L2(Œ±,Œ≤)dœÑ ,
for any t ‚àà[0, T], where we have assumed that
0 < ¬µ0 ‚â§a0(x, t) ‚àí1
2a‚Ä≤(x).
(13.66)
Notice that in the special case in which both f and a0 are identically zero,
we obtain
‚à•uh(t)‚à•L2(Œ±,Œ≤) ‚â§‚à•u0,h‚à•L2(Œ±,Œ≤)
which expresses the conservation of the energy of the system. When (13.66)
does not hold (for example, if a is a constant convective term and a0 = 0),
then an application of Gronwall‚Äôs lemma 11.1 yields
‚à•uh(t)‚à•2
L2(Œ±,Œ≤) + a(Œ≤)
t
>
0
u2
h(œÑ) dœÑ
‚â§
Ô£´
Ô£≠‚à•u0,h‚à•2
L2(Œ±,Œ≤) +
t
>
0
‚à•f(œÑ)‚à•2
L2(Œ±,Œ≤) dœÑ
Ô£∂
Ô£∏exp
t
>
0
[1 + 2¬µ‚àó(œÑ)] dœÑ,
(13.67)
where ¬µ‚àó(t) = max
[Œ±,Œ≤]|¬µ(x, t)|.
An alternative approach to the semi-discrete approximation of problem
(13.64) is based on the use of discontinuous Ô¨Ånite elements. This choice
is motivated by the fact that, as previously pointed out, the solutions of
hyperbolic problems (even in the linear case) may exhibit discontinuities.
The Ô¨Ånite element space can be deÔ¨Åned as follows
Wh = Y r
h =

vh ‚ààL2(Œ±, Œ≤) : vh|Ij ‚ààPr(Ij), ‚àÄIj ‚ààTh

,
i.e., the space of piecewise polynomials of degree less than or equal to r,
which are not necessarily continuous at the Ô¨Ånite element nodes.
Then, the Galerkin discontinuous Ô¨Ånite element space discretization reads:
for any t ‚àà(0, T) Ô¨Ånd uh(t) ‚ààWh such that
Ô£±
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≥
Œ≤
>
Œ±
‚àÇuh(t)
‚àÇt
vh dx +
n‚àí1

i=0
Ô£±
Ô£≤
Ô£≥
xi+1
>
xi

a‚àÇuh(t)
‚àÇx
+ a0(x)uh(t)

vh dx
+a(u+
h ‚àíU ‚àí
h )(xi, t)v+
h (xi)

=
Œ≤
>
Œ±
f(t)vh dx
‚àÄvh ‚ààWh,
(13.68)

620
13. Parabolic and Hyperbolic Initial Boundary Value Problems
where {xi} are the nodes of Th with x0 = Œ±, and for each node xi, v+
h (xi)
denotes the right-value of vh at xi while v‚àí
h (xi) is its left-value. Finally,
U ‚àí
h (xi, t) = u‚àí
h (xi, t) if i = 1, . . . , n ‚àí1, while U ‚àí
h (x0, t) = œï(t) ‚àÄt > 0.
If a is positive, xj is the inÔ¨Çow boundary of Ij for every j and we set
[u]j = u+(xj) ‚àíu‚àí(xj),
u¬±(xj) = lim
s‚Üí0¬±u(xj + sa),
j = 1, . . . , n ‚àí1.
Then, for any t ‚àà[0, T] the stability estimate for problem (13.68) reads
‚à•uh(t)‚à•2
L2(Œ±,Œ≤) +
t
>
0
Ô£´
Ô£≠‚à•uh(œÑ)‚à•2
L2(Œ±,Œ≤) +
n‚àí1

j=0
a(xj)[uh(œÑ)]2
j
Ô£∂
Ô£∏dœÑ
‚â§C
Ô£Æ
Ô£∞‚à•u0,h‚à•2
L2(Œ±,Œ≤) +
t
>
0
+
‚à•f(œÑ)‚à•2
L2(Œ±,Œ≤) + aœï2(œÑ)
,
dœÑ
Ô£π
Ô£ª.
(13.69)
As for the convergence analysis, the following error estimate can be proved
for continuous Ô¨Ånite elements of degree r, r ‚â•1 (see [QV94], Section 14.3.1)
max
t‚àà[0,T ]‚à•u(t) ‚àíuh(t)‚à•L2(Œ±,Œ≤)
+
Ô£´
Ô£≠
t
>
0
a|u(Œ±, œÑ) ‚àíuh(Œ±, œÑ)|2 dœÑ
Ô£∂
Ô£∏
1/2
=
O(‚à•u0 ‚àíu0,h‚à•L2(Œ±,Œ≤) + hr),
If, instead, discontinuous Ô¨Ånite elements of degree r are used, r ‚â•0, the
convergence estimate becomes (see [QV94], Section 14.3.3 and the refer-
ences therein)
max
t‚àà[0,T ]‚à•u(t) ‚àíuh(t)‚à•L2(Œ±,Œ≤) +
Ô£´
Ô£≠
T
>
0
‚à•u(t) ‚àíuh(t)‚à•2
L2(Œ±,Œ≤) dt
+
T
>
0
n‚àí1

j=0
a(xj) [u(t) ‚àíuh(t)]2
j dt
Ô£∂
Ô£∏
1/2
= O(‚à•u0 ‚àíu0,h‚à•L2(Œ±,Œ≤) + hr+1/2).
13.10.2
Time Discretization
The time discretization of the Ô¨Ånite element schemes introduced in the
previous section can be carried out by resorting either to Ô¨Ånite diÔ¨Äerences
or Ô¨Ånite elements. If an implicit Ô¨Ånite diÔ¨Äerence scheme is adopted, both
method (13.65) and (13.68) are unconditionally stable.
As an example, let us use the backward Euler method for the time dis-
cretization of problem (13.65). We obtain for each n ‚â•0: Ô¨Ånd un+1
h
‚ààVh

13.10 Finite Element Approximation of Hyperbolic Equations
621
such that
1
‚àÜt
Œ≤
>
Œ±
(un+1
h
‚àíun
h)vh dx +
Œ≤
>
Œ±
a‚àÇun+1
h
‚àÇx
vh dx
+
Œ≤
>
Œ±
an+1
0
un+1
h
vh dx =
Œ≤
>
Œ±
f n+1vh dx
‚àÄvh ‚ààV in
h
(13.70)
with un+1
h
(Œ±) = œïn+1 and u0
h = u0h. If f ‚â°0 and œï ‚â°0, taking vh = un+1
h
in (13.70) we can obtain
1
2‚àÜt
+
‚à•un+1
h
‚à•2
L2(Œ±,Œ≤) ‚àí‚à•un
h‚à•2
L2(Œ±,Œ≤)
,
+ a(Œ≤)(un+1
h
(Œ≤))2 + ¬µ0‚à•un+1
h
‚à•2
L2(Œ±,Œ≤) ‚â§0
‚àÄn ‚â•0. Summing for n from 0 to m ‚àí1 yields for m ‚â•1
‚à•um
h ‚à•2
L2(Œ±,Œ≤) + 2‚àÜt
Ô£´
Ô£≠
m

j=1
‚à•uj
h‚à•2
L2(Œ±,Œ≤) +
m

j=1
a(Œ≤)(uj+1
h
(Œ≤))2
Ô£∂
Ô£∏‚â§‚à•u0
h‚à•2
L2(Œ±,Œ≤).
In particular, we conclude that
‚à•um
h ‚à•L2(Œ±,Œ≤) ‚â§‚à•u0
h‚à•L2(Œ±,Œ≤)
‚àÄm ‚â•0.
On the other hand, explicit schemes for the hyperbolic equations are
subject to a stability condition: for example, in the case of the forward
Euler method the stability condition is ‚àÜt = O(‚àÜx). In practice, this
restriction is not as severe as happens in the case of parabolic equations
and for this reason explicit schemes are often used in the approximation of
hyperbolic equations.
Programs 102 and 103 provide an implementation of the discontinous
Galerkin-Ô¨Ånite element method of degree 0 (dG(0)) and 1 (dG(1)) in space
coupled with the backward Euler method in time for the solution of (13.26)
on the space-time domain (Œ±, Œ≤) √ó (t0, T).
Program 102 - ipeidg0 : dG(0) implicit Euler
function [u,x]=ipeidg0(I,n,a,u0,bc)
nx = n(1); h = (I(2)-I(1))/nx;
x = [I(1)+h/2:h:I(2)]; t = I(3); u = (eval(u0))‚Äô;
nt = n(2); k = (I(4)-I(3))/nt;
lambda = k/h; e = ones(nx,1);
A=spdiags([-a*lambda*e, (1+a*lambda)*e],-1:0,nx,nx);
[L,U]=lu(A);
for t = I(3)+k:k:I(4)

622
13. Parabolic and Hyperbolic Initial Boundary Value Problems
f = u;
if a > 0
f(1) = a*bc(1)+f(1);
elseif a <= 0
f(nx) = a*bc(2)+f(nx);
end
y = L \ f; u = U \ y;
end
Program 103 - ipeidg1 : dG(1) implicit Euler
function [u,x]=ipeidg1(I,n,a,u0,bc)
nx = n(1); h = (I(2)-I(1))/nx;
x = [I(1):h:I(2)]; t = I(3); um = (eval(u0))‚Äô;
u = []; xx=[];
for i = 1:nx+1
u = [u, um(i), um(i)];
xx = [xx, x(i), x(i)];
end
u = u‚Äô; nt = n(2); k = (I(4)-I(3))/nt;
lambda = k/h; e = ones(2*nx+2,1);
B = spdiags([1/6*e,1/3*e,1/6*e],-1:1,2*nx+2,2*nx+2);
dd = 1/3+0.5*a*lambda; du = 1/6+0.5*a*lambda;
dl = 1/6-0.5*a*lambda; A=sparse([]);
A(1,1) = dd; A(1,2) = du; A(2,1) = dl; A(2,2) = dd;
for i=3:2:2*nx+2
A(i,i-1)
=-a*lambda;
A(i,i)
= dd;
A(i,i+1)
= du;
A(i+1,i)
= dl;
A(i+1,i+1) = A(i,i);
end
[L,U]=lu(A);
for t = I(3)+k:k:I(4)
f = B*u;
if a > 0
f(1) = a*bc(1)+f(1);
elseif a <= 0
f(nx) = a*bc(2)+f(nx);
end
y = L \ f;
u = U \ y;
end
x = xx;

13.11 Applications
623
13.11
Applications
13.11.1
Heat Conduction in a Bar
Consider a homogeneous bar of unit length with thermal conductivity ŒΩ,
which is connected at the endpoints to an external thermal source at a Ô¨Åxed
temperature, say u = 0. Let u0(x) be the temperature distribution along
the bar at time t = 0 and f = f(x, t) be a given heat production term.
Then, the initial-boundary value problem (13.1)-(13.4) provides a model of
the time evolution of the temperature u = u(x, t) throughout the bar.
In the following, we study the case where f ‚â°0 and the temperature of
the bar is suddenly raised at the points around 1/2. A rough mathematical
model for this situation is provided, for instance, by taking u0 = K in
a certain subinterval [a, b] ‚äÜ[0, 1] and equal to 0 outside, where K is a
given positive constant. The initial condition is therefore a discontinuous
function.
We have used the Œ∏-method with Œ∏ = 0.5 (Crank-Nicolson method, CN)
and Œ∏ = 1 (Backward Euler method, BE). Program 100 has been run
with h = 1/20, ‚àÜt = 1/40 and the obtained solutions at time t = 2 are
shown in Figure 13.10. The results show that the CN method suÔ¨Äers a clear
instability due to the low smoothness of the initial datum (about this point,
see also [QV94], Chapter 11). On the contrary, the BE method provides a
stable solution which decays correctly to zero as t grows since the source
term f is null.
0
0.5
1
0
0.5
1
1.5
2
‚àí0.2
0
0.2
0.4
0.6
0.8
0
0.2
0.4
0.6
0.8
1
0
0.5
1
1.5
2
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
FIGURE 13.10. Solutions for a parabolic problem with discontinuous initial da-
tum: CN method (left) and BE method (right)
13.11.2
A Hyperbolic Model for Blood Flow Interaction with
Arterial Walls
Let us consider again the problem of the Ô¨Çuid-structure interaction in a
cylindrical artery considered in Section 11.11.2, where the simple indepen-
dent rings model (11.88) was adopted.

624
13. Parabolic and Hyperbolic Initial Boundary Value Problems
If the axial action due to the tension between the diÔ¨Äerent rings is
no longer neglected, denoting by z the longitudinal coordinate, equation
(11.88) modiÔ¨Åes into
œÅwH ‚àÇ2Œ∑
‚àÇt2 ‚àíœÉz
‚àÇ2Œ∑
‚àÇz2 + HE
R2
0
Œ∑ = P ‚àíP0,
t > 0,
0 < z < L
(13.71)
where œÉz is the radial component of the axial stress and L is the length of
the cylindrical arterial district which is considered. In particular, neglecting
the third term on the left hand side and letting Œ≥2 = œÉz/(œÅwH), f =
(P ‚àíP0)/(œÅwH), we recover the wave equation (13.33).
We have performed two sets of numerical experiments using the Leap-
Frog (LF) and Newmark (NW) methods. In the Ô¨Årst example the space-
time domain of integration is the cylinder (0, 1) √ó (0, 1) and the source
term is f = (1 + œÄ2Œ≥2)e‚àít sin(œÄx) in such a way that the exact solution is
u(x, t) = e‚àít sin(œÄx). Table 13.3 shows the estimated orders of convergence
of the two methods, denoted by pLF and pNW respectively.
To compute these quantities we have Ô¨Årst solved the wave equation on
four grids with sizes ‚àÜx = ‚àÜt = 1/(2k ¬∑ 10), k = 0, . . . , 3. Let u(k)
h
denote
the numerical solution corresponding to the space-time grid at the k-th
reÔ¨Åning level. Moreover, for j = 1, . . . , 10, let t(0)
j
= j/10 be the time
discretization nodes of the grid at the coarsest level k = 0. Then, for each
level k, the maximum nodal errors ek
j on the k-th spatial grid have been
evaluated at each time t(0)
j
in such a way that the convergence order p(k)
j
can be estimated as
p(k)
j
= log(e0
j/ek
j )
log(2k)
,
k = 1, 2, 3.
The results clearly show second-order convergence for both the methods,
as theoretically expected.
In the second example we have taken the following expressions for the
coeÔ¨Écient and source term: Œ≥2 = œÉz/(œÅwH), with œÉz = 1 [Kgs‚àí2], f =
(x‚àÜp ¬∑ sin(œâ0t))/(œÅwH). The parameters œÅw, H and the length L of the
vessel are as in Section 11.11.2. The space-time computational domain is
(0, L) √ó (0, T), with T = 1 [s].
The Newmark method has been Ô¨Årst used with ‚àÜx = L/10 and ‚àÜt =
T/100; the corresponding value of Œ≥Œª is 3.6515, where Œª = ‚àÜt/‚àÜx. Since the
Newmark method is unconditionally stable, no spurious oscillations arise as
is conÔ¨Årmed by Figure 13.11, left. Notice the correct periodical behaviour
of the solution with a period corresponding to one heart beat; notice also
that with the present values of ‚àÜt and ‚àÜx the Leap-Frog method cannot
be employed since the CFL condition is not satisÔ¨Åed. To overcome this

13.12 Exercises
625
t(0)
j
p(1)
LF
p(2)
LF
p(3)
LF
0.1
2.0344
2.0215
2.0151
0.2
2.0223
2.0139
2.0097
0.3
2.0170
2.0106
2.0074
0.4
2.0139
2.0087
2.0061
0.5
2.0117
2.0073
2.0051
0.6
2.0101
2.0063
2.0044
0.7
2.0086
2.0054
2.0038
0.8
2.0073
2.0046
2.0032
0.9
2.0059
2.0037
2.0026
1.0
2.0044
2.0028
2.0019
t(0)
j
p(1)
NW
p(2)
NW
p(3)
NW
0.1
1.9549
1.9718
1.9803
0.2
1.9701
1.9813
1.9869
0.3
1.9754
1.9846
1.9892
0.4
1.9791
1.9869
1.9909
0.5
1.9827
1.9892
1.9924
0.6
1.9865
1.9916
1.9941
0.7
1.9910
1.9944
1.9961
0.8
1.9965
1.9979
1.9985
0.9
2.0034
2.0022
2.0015
1.0
2.0125
2.0079
2.0055
TABLE 13.3. Estimated orders of convergence for the Leap-Frog (LF) and New-
mark (NW) methods
problem, we have therefore chosen a much smaller time-step ‚àÜt = T/400,
in such a way that Œ≥Œª ‚âÉ0.9129 and the Leap-Frog scheme can be applied.
The obtained result is shown in Figure 13.11, right; a similar solution has
been computed by using the Newmark method with the same values of the
discretization parameters.
0
0.01 0.02 0.03 0.04
0
0.2
0.4
0.6
0.8
1
‚àí3
‚àí2
‚àí1
0
1
2
3
x 10‚àí4
0
0.02
0.04
0.06
0
0.2
0.4
0.6
0.8
1
‚àí3
‚àí2
‚àí1
0
1
2
3
x 10‚àí4
FIGURE 13.11. Computed solutions using the NM method on a space-time grid
with ‚àÜt = T/100 and ‚àÜx = L/10 (left) and the LF scheme on a space-time grid
with the same value of ‚àÜx but with ‚àÜt = T/400 (right)
13.12
Exercises
1. Apply the Œ∏-method (13.9) to the approximate solution of the scalar Cauchy
problem (11.1) and using the analysis of Section 11.3 prove that the local
truncation error is of the order of ‚àÜt + h2 if Œ∏ Ã∏= 1
2 while it is of the order
of ‚àÜt2 + h2 if Œ∏ = 1
2.
2. Prove that in the case of piecewise linear Ô¨Ånite elements, the mass-lumping
process described in Section 13.3 is equivalent to computing the integrals

626
13. Parabolic and Hyperbolic Initial Boundary Value Problems
mij =
 1
0 œïjœïi dx by the trapezoidal quadrature formula (9.11). This, in
particular, shows that the diagonal matrix $M is nonsingular.
[Hint: Ô¨Årst, verify that exact integration yields
mij = h
6
Ô£±
Ô£≤
Ô£≥
1
2
i Ã∏= j,
1
i = j.
Then, apply the trapezoidal rule to compute mij recalling that œïi(xj) =
Œ¥ij.]
3. Prove inequality (13.19).
[Hint: using the Cauchy-Schwarz and Young inequalities, prove Ô¨Årst that
1
>
0
(u ‚àív)u dx ‚â•1
2

‚à•u‚à•2
L2(0,1) ‚àí‚à•v‚à•2
L2(0,1)

,
‚àÄu, v ‚ààL2(0, 1).
Then, use (13.18). ]
4. Assume that the bilinear form a(¬∑, ¬∑) in problem (13.12) is continuous and
coercive over the function space V (see (12.54)-(12.55)) with continuity and
coercivity constants M and Œ±, respectively. Then, prove that the stability
inequalities (13.20) and (13.21) still hold provided that ŒΩ is replaced by Œ±.
5. Show that the methods (13.39), (13.40) and (13.41) can be written in the
form (13.42). Then, show that the corresponding expressions of the artiÔ¨Åcial
viscosity K and artiÔ¨Åcial diÔ¨Äusion Ô¨Çux hdiff
j+1/2 are as in Table (13.1).
6. Determine the CFL condition for the upwind scheme.
7. Show that for the scheme (13.43) one has ‚à•un+1‚à•‚àÜ,2 ‚â§‚à•un‚à•‚àÜ,2 for all
n ‚â•0.
[Hint: multiply equation (13.43) by un+1
j
, and notice that
(un+1
j
‚àíun
j )un+1
j
‚â•1
2

|un+1
j
|2 ‚àí|un
j |2
.
Then, sum on j the resulting inequalities, and note that
Œªa
2
‚àû

j=‚àí‚àû

un+1
j+1 ‚àíun+1
j‚àí1

un+1
j
= 0
since this sum is telescopic.]
8. Show how to Ô¨Ånd the values ¬µ and ŒΩ in Table 13.2 for Lax-Friedrichs and
Lax-WendroÔ¨Ämethods.
9. Prove (13.67).
10. Prove (13.69) when f = 0.
[Hint: take ‚àÄt > 0, vh = uh(t) in (13.68).]

References
[Aas71]
Aasen J. (1971) On the Reduction of a Symmetric Matrix to
Tridiagonal Form. BIT 11: 233‚Äì242.
[ABB+92]
Anderson E., Bai Z., Bischof C., Demmel J., Dongarra J., Croz
J. D., Greenbaum A., Hammarling S., McKenney A., Ous-
trouchov S., and Sorensen D. (1992) LAPACK User‚Äôs Guide,
Release 1.0. SIAM, Philadelphia.
[Ada75]
Adams D. (1975) Sobolev Spaces. Academic Press, New York.
[ADR92]
Arioli M., DuÔ¨ÄI., and Ruiz D. (1992) Stopping Criteria for
Iterative Solvers. SIAM J. Matrix Anal. Appl. 1(13).
[AF83]
Alonso M. and Finn E. (1983) Fundamental University
Physics, volume 3. Addison-Wesley, Reading, Massachusetts.
[Arm66]
Armijo L. (1966) Minimization of Functions Having Continu-
ous Partial Derivatives. PaciÔ¨Åc Jour. Math. 16: 1‚Äì3.
[Arn73]
Arnold V. I. (1973) Ordinary DiÔ¨Äerential Equations. The MIT
Press, Cambridge, Massachusetts.
[Atk89]
Atkinson K. E. (1989) An Introduction to Numerical Analysis.
John Wiley, New York.
[Avr76]
Avriel M. (1976) Non Linear Programming: Analysis and
Methods. Prentice-Hall, Englewood CliÔ¨Äs, New Jersey.

628
References
[Axe94]
Axelsson O. (1994) Iterative Solution Methods.
Cambridge
University Press, New York.
[Bar89]
Barnett S. (1989) Leverrier‚Äôs Algorithm: A New Proof and
Extensions. Numer. Math. 7: 338‚Äì352.
[Bat90]
Batterson S. (1990) Convergence of the Shifted QR Algorithm
on 3 by 3 Normal Matrices. Numer. Math. 58: 341‚Äì352.
[BBC+94]
Barrett R., Berry M., Chan T., Demmel J., Donato J., Don-
garra J., Eijkhout V., Pozo V., Romine C., and van der Vorst
H. (1994) Templates for the Solution of Linear Systems: Build-
ing Blocks for Iterative Methods. SIAM, Philadelphia.
[BD74]
Bj¬®orck A. and Dahlquist G. (1974) Numerical Methods.
Prentice-Hall, Englewood CliÔ¨Äs, N.J.
[BDMS79]
Bunch J., Dongarra J., Moler C., and Stewart G. (1979) LIN-
PACK User‚Äôs Guide. SIAM, Philadelphia.
[Ber82]
Bertsekas D. P. (1982) Constrained Optimization and La-
grange Multiplier Methods. Academic Press. Inc., San Diego,
California.
[Bj¬®o88]
Bj¬®orck A. (1988) Least Squares Methods: Handbook of Numer-
ical Analysis Vol. 1 Solution of Equations in RN.
Elsevier
North Holland.
[BM92]
Bernardi C. and Maday Y. (1992) Approximations Spectrales
des Probl¬¥emes aux Limites Elliptiques. Springer-Verlag, Paris.
[BMW67]
Barth W., Martin R. S., and Wilkinson J. H. (1967) Calcula-
tion of the Eigenvalues of a Symmetric Tridiagonal Matrix by
the Method of Bisection. Numer. Math. 9: 386‚Äì393.
[BO78]
Bender C. M. and Orszag S. A. (1978) Advanced Mathemati-
cal Methods for Scientists and Engineers. McGraw-Hill, New
York.
[Boe80]
Boehm W. (1980) Inserting New Knots into B-spline Curves.
Computer Aided Design 12: 199‚Äì201.
[Bos93]
Bossavit A. (1993) Electromagnetisme, en vue de la modelisa-
tion. Springer-Verlag, Paris.
[BR81]
Bank R. E. and Rose D. J. (1981) Global Approximate Newton
Methods. Numer. Math. 37: 279‚Äì295.
[Bra75]
Bradley G. (1975) A Primer of Linear Algebra. Prentice-Hall,
Englewood CliÔ¨Äs, New York.

References
629
[Bre73]
Brent R. (1973) Algorithms for Minimization Without Deriva-
tives. Prentice-Hall, Englewood CliÔ¨Äs, New York.
[Bri74]
Brigham E. O. (1974) The Fast Fourier Transform. Prentice-
Hall, Englewood CliÔ¨Äs, New York.
[BS90]
Brown P. and Saad Y. (1990) Hybrid Krylov Methods for Non-
linear Systems of equations. SIAM J. Sci. and Stat. Comput.
11(3): 450‚Äì481.
[BSG96]
B. Smith P. B. and Gropp P. (1996) Domain Decomposition,
Parallel Multilevel Methods for Elliptic Partial DiÔ¨Äerential
Equations. Univ. Cambridge Press, Cambridge.
[But64]
Butcher J. C. (1964) Implicit Runge-Kutta Processes. Math.
Comp. 18: 233‚Äì244.
[But66]
Butcher J. C. (1966) On the Convergence of Numerical So-
lutions to Ordinary DiÔ¨Äerential Equations. Math. Comp. 20:
1‚Äì10.
[But87]
Butcher J. (1987) The Numerical Analysis of Ordinary DiÔ¨Äer-
ential Equations: Runge-Kutta and General Linear Methods.
Wiley, Chichester.
[CCP70]
Cannon M., Cullum C., and Polak E. (1970) Theory and Op-
timal Control and Mathematical Programming. McGraw-Hill,
New York.
[CFL28]
Courant R., Friedrichs K., and Lewy H. (1928) ¬®Uber die
partiellen diÔ¨Äerenzengleichungen der mathematischen physik.
Math. Ann. 100: 32‚Äì74.
[CHQZ88]
Canuto C., Hussaini M. Y., Quarteroni A., and Zang T. A.
(1988) Spectral Methods in Fluid Dynamics. Springer, New
York.
[CI95]
Chandrasekaren S. and Ipsen I. (1995) On the Sensitivity of
Solution Components in Linear Systems of equations. SIAM
J. Matrix Anal. Appl. 16: 93‚Äì112.
[CL91]
Ciarlet P. G. and Lions J. L. (1991) Handbook of Numerical
Analysis: Finite Element Methods (Part 1). North-Holland,
Amsterdam.
[CM94]
Chan T. and Mathew T. (1994) Domain Decomposition Algo-
rithms. Acta Numerica pages 61‚Äì143.

630
References
[CMSW79]
Cline A., Moler C., Stewart G., and Wilkinson J. (1979) An
Estimate for the Condition Number of a Matrix. SIAM J. Sci.
and Stat. Comput. 16: 368‚Äì375.
[Col66]
Collin R. E. (1966) Foundations for Microwave Engineering.
McGraw-Hill Book Co., Singapore.
[Com95]
Comincioli V. (1995) Analisi Numerica Metodi Modelli Appli-
cazioni. McGraw-Hill Libri Italia, Milano.
[Cox72]
Cox M. (1972) The Numerical Evaluation of B-splines. Jour-
nal of the Inst. of Mathematics and its Applications 10: 134‚Äì
149.
[Cry73]
Cryer C. W. (1973) On the Instability of High Order
Backward-DiÔ¨Äerence Multistep Methods. BIT 13: 153‚Äì159.
[CT65]
Cooley J. and Tukey J. (1965) An Algorithm for the Machine
Calculation of Complex Fourier Series. Math. Comp. 19: 297‚Äì
301.
[Dah56]
Dahlquist G. (1956) Convergence and Stability in the Nu-
merical Integration of Ordinary DiÔ¨Äerential Equations. Math.
Scand. 4: 33‚Äì53.
[Dah63]
Dahlquist G. (1963) A Special Stability Problem for Linear
Multistep Methods. BIT 3: 27‚Äì43.
[Dat95]
Datta B. (1995) Numerical Linear Algebra and Applications.
Brooks/Cole Publishing, PaciÔ¨Åc Grove, CA.
[Dau88]
Daubechies I. (1988) Orthonormal bases of compactly sup-
ported wavelets. Commun. on Pure and Appl. Math. XLI.
[Dav63]
Davis P. (1963) Interpolation and Approximation. Blaisdell
Pub., New York.
[Day96]
Day D. (1996) How the QR algorithm Fails to Converge and
How to Fix It. Technical Report 96-0913J, Sandia National
Laboratory, Albuquerque.
[dB72]
de Boor C. (1972) On Calculating with B-splines. Journal of
Approximation Theory 6: 50‚Äì62.
[dB83]
de Boor C. (1983) A Practical Guide to Splines. In Applied
Mathematical Sciences. (27), Springer-Verlag, New York.
[dB90]
de Boor C. (1990) SPLINE TOOLBOX for use with MAT-
LAB. The Math Works, Inc., South Natick.

References
631
[DD95]
Davis
T.
and
DuÔ¨Ä
I.
(1995)
A
combined
unifrontal/multifrontal
method
for
unsymmetric
sparse
matrices.
Technical Report TR-95-020, Computer and
Information Sciences Department, University of Florida.
[Dek69]
Dekker T. (1969) Finding a Zero by means of Successive Linear
Interpolation. In Dejon B. and Henrici P. (eds) Constructive
Aspects of the Fundamental Theorem of Algebra, pages 37‚Äì51.
Wiley, New York.
[Dek71]
Dekker T. (1971) A Floating-Point Technique for Extending
the Available Precision. Numer. Math. 18: 224‚Äì242.
[Dem97]
Demmel J. (1997) Applied Numerical Linear Algebra. SIAM,
Philadelphia.
[DGK84]
Dongarra J., Gustavson F., and Karp A. (1984) Implementing
Linear Algebra Algorithms for Dense Matrices on a Vector
Pipeline Machine. SIAM Review 26(1): 91‚Äì112.
[Die87a]
Dierckx P. (1987) FITPACK User Guide part 1: Curve Fitting
Routines. TW Report, Dept. of Computer Science, Katholieke
Universiteit, Leuven, Belgium.
[Die87b]
Dierckx P. (1987) FITPACK User Guide part 2: Surface
Fitting Routines.
TW Report, Dept. of Computer Science,
Katholieke Universiteit, Leuven, Belgium.
[Die93]
Dierckx P. (1993) Curve and Surface Fitting with Splines.
Claredon Press, New York.
[DL92]
DeVore R. and Lucier J. (1992) Wavelets.
Acta Numerica
pages 1‚Äì56.
[DR75]
Davis P. and Rabinowitz P. (1975) Methods of Numerical In-
tegration. Academic Press, New York.
[DS83]
Dennis J. and Schnabel R. (1983) Numerical Methods for Un-
constrained Optimization and Nonlinear Equations. Prentice-
Hall, Englewood CliÔ¨Äs, New York.
[Dun85]
Dunavant D. (1985) High Degree EÔ¨Écient Symmetrical Gaus-
sian Quadrature Rules for the Triangle. Internat. J. Numer.
Meth. Engrg. 21: 1129‚Äì1148.
[Dun86]
Dunavant D. (1986) EÔ¨Écient Symmetrical Cubature Rules for
Complete Polynomials of High Degree over the Unit Cube.
Internat. J. Numer. Meth. Engrg. 23: 397‚Äì407.

632
References
[DV84]
Dekker K. and Verwer J. (1984) Stability of Runge-Kutta
Methods for StiÔ¨ÄNonlinear DiÔ¨Äerential Equations.
North-
Holland, Amsterdam.
[dV89]
der Vorst H. V. (1989) High Performance Preconditioning.
SIAM J. Sci. Stat. Comput. 10: 1174‚Äì1185.
[EEHJ96]
Eriksson K., Estep D., Hansbo P., and Johnson C. (1996)
Computational DiÔ¨Äerential Equations.
Cambridge Univ.
Press, Cambridge.
[Elm86]
Elman H. (1986) A Stability Analisys of Incomplete LU Fac-
torization. Math. Comp. 47: 191‚Äì218.
[Erd61]
Erd¬®os P. (1961) Problems and Results on the Theory of Inter-
polation. Acta Math. Acad. Sci. Hungar. 44: 235‚Äì244.
[Erh97]
Erhel J. (1997) About Newton-Krylov Methods. In Periaux J.
and al. (eds) Computational Science for 21st Century, pages
53‚Äì61. Wiley, New York.
[Fab14]
Faber G. (1914)
¬®Uber die interpolatorische Darstellung
stetiger Funktionem. Jber. Deutsch. Math. Verein. 23: 192‚Äì
210.
[FF63]
Faddeev D. K. and Faddeeva V. N. (1963) Computational
Methods of Linear Algebra. Freeman, San Francisco and Lon-
don.
[Fle75]
Fletcher R. (1975) Conjugate gradient methods for indeÔ¨Ånite
systems. In Springer-Verlag (ed) Numerical Analysis, pages
73‚Äì89. New York.
[FM67]
Forsythe G. E. and Moler C. B. (1967) Computer Solution
of Linear Algebraic Systems. Prentice-Hall, Englewood CliÔ¨Äs,
New York.
[Fra61]
Francis J. G. F. (1961) The QR Transformation: A Unitary
Analogue to the LR Transformation. Parts I and II. Comp. J.
pages 265‚Äì272,332‚Äì334.
[FRL55]
F. Richtmyer E. K. and Lauritsen T. (1955) Introduction to
Modern Physics. McGraw-Hill, New York.
[Gas83]
Gastinel N. (1983) Linear Numerical Analysis. Kershaw Pub-
lishing, London.

References
633
[Gau94]
Gautschi W. (1994) Algorithm 726: ORTHPOL - A Package of
Routines for Generating Orthogonal Polynomials and Gauss-
type Quadrature Rules. ACM Trans. Math. Software 20: 21‚Äì
62.
[Gau96]
Gautschi W. (1996) Orthogonal Polynomials: Applications
and Computation. Acta Numerica pages 45‚Äì119.
[Gau97]
Gautschi W. (1997) Numerical Analysis. An Introduction.
Birkh¬®auser, Berlin.
[Geo73]
George A. (1973) Nested Dissection of a Regular Finite Ele-
ment Mesh. SIAM J. Num. Anal. 10: 345‚Äì363.
[Giv54]
Givens W. (1954) Numerical Computation of the Character-
istic Values of a Real Symmetric Matrix. Oak Ridge National
Laboratory ORNL-1574.
[GL81]
George A. and Liu J. (1981) Computer Solution of Large
Sparse Positive DeÔ¨Ånite Systems. Prentice-Hall, Englewood
CliÔ¨Äs, New York.
[GL89]
Golub G. and Loan C. V. (1989) Matrix Computations. The
John Hopkins Univ. Press, Baltimore and London.
[GM83]
Golub G. and Meurant G. (1983) Resolution Numerique des
Grands Systemes Lineaires. Eyrolles, Paris.
[GMW81]
Gill P., Murray W., and Wright M. (1981) Practical Optimiza-
tion. Academic Press, London.
[God66]
Godeman R. (1966) Algebra. Kershaw, London.
[Gol91]
Goldberg D. (1991) What Every Computer Scientist Should
Know about Floating-point Arithmetic.
ACM Computing
Surveys 23(1): 5‚Äì48.
[GP67]
Goldstein A. A. and Price J. B. (1967) An EÔ¨Äective Algorithm
for Minimization. Numer. Math 10: 184‚Äì189.
[GR96]
Godlewski E. and Raviart P. (1996) Numerical Approximation
of Hyperbolic System of Conservation Laws, volume 118 of
Applied Mathematical Sciences. Springer-Verlag, New York.
[Hac94]
Hackbush W. (1994) Iterative Solution of Large Sparse Sys-
tems of Equations. Springer-Verlag, New York.
[Hah67]
Hahn W. (1967) Stability of Motion. Springer-Verlag, Berlin.

634
References
[Hal58]
Halmos P. (1958) Finite-Dimensional Vector Spaces. Van Nos-
trand, Princeton, New York.
[Hen62]
Henrici P. (1962) Discrete Variable Methods in Ordinary Dif-
ferential Equations. Wiley, New York.
[Hen74]
Henrici P. (1974) Applied and Computational Complex Anal-
ysis, volume 1. Wiley, New York.
[HGR96]
H-G. Roos M. Stynes L. T. (1996) Numerical Methods for
Singularly Perturbed DiÔ¨Äerential Equations. Springer-Verlag,
Berlin Heidelberg.
[Hig88]
Higham N. (1988) The Accuracy of Solutions to Triangular
Systems. University of Manchester, Dep. of Mathematics 158:
91‚Äì112.
[Hig89]
Higham N. (1989) The Accuracy of Solutions to Triangular
Systems. SIAM J. Numer. Anal. 26(5): 1252‚Äì1265.
[Hig96]
Higham N. (1996) Accuracy and Stability of Numerical Algo-
rithms. SIAM Publications, Philadelphia, PA.
[Hil87]
Hildebrand F. (1987) Introduction to Numerical Analysis.
McGraw-Hill, New York.
[Hou75]
Householder A. (1975) The Theory of Matrices in Numerical
Analysis. Dover Publications, New York.
[HP94]
Hennessy J. and Patterson D. (1994) Computer Organiza-
tion and Design - The Hardware/Software Interface. Morgan
Kaufmann, San Mateo.
[HW76]
Hammarling S. and Wilkinson J. (1976) The Practical Be-
haviour of Linear Iterative Methods with Particular Reference
to S.O.R. Technical Report Report NAC 69, National Physi-
cal Laboratory, Teddington, UK.
[IK66]
Isaacson E. and Keller H. (1966) Analysis of Numerical Meth-
ods. Wiley, New York.
[Inm94]
Inman D. (1994) Engineering Vibration. Prentice-Hall, Engle-
wood CliÔ¨Äs, NJ.
[Iro70]
Irons B. (1970) A Frontal Solution Program for Finite Element
Analysis. Int. J. for Numer. Meth. in Engng. 2: 5‚Äì32.
[Jac26]
Jacobi C. (1826) Uber GauŒ≤ neue Methode, die Werthe der
Integrale n¬®aherungsweise zu Ô¨Ånden. J. Reine Angew. Math.
30: 127‚Äì156.

References
635
[Jer96]
Jerome J. J. (1996) Analysis of Charge Transport. A Math-
ematical Study of Semiconductor Devices.
Springer, Berlin
Heidelberg.
[Jia95]
Jia Z. (1995) The Convergence of Generalized Lanczos Meth-
ods for Large Unsymmetric Eigenproblems. SIAM J. Matrix
Anal. Applic. 16: 543‚Äì562.
[JM92]
Jennings A. and McKeown J. (1992) Matrix Computation.
Wiley, Chichester.
[Joh90]
Johnson C. (1990) Numerical Solution of Partial DiÔ¨Äerential
Equations by the Finite Element Method. Cambridge Univ.
Press.
[JW77]
Jankowski M. and Wozniakowski M. (1977) Iterative ReÔ¨Åne-
ment Implies Numerical Stability. BIT 17: 303‚Äì311.
[Kah66]
Kahan W. (1966) Numerical Linear Algebra. Canadian Math.
Bull. 9: 757‚Äì801.
[Kan66]
Kaniel S. (1966) Estimates for Some Computational Tech-
niques in Linear Algebra. Math. Comp. 20: 369‚Äì378.
[Kea86]
Keast P. (1986) Moderate-Degree Tetrahedral Quadrature
Formulas. Comp. Meth. Appl. Mech. Engrg. 55: 339‚Äì348.
[Kel99]
Kelley C. (1999) Iterative Methods for Optimization, vol-
ume 18 of Frontiers in Applied Mathematics. SIAM, Philadel-
phia.
[KT51]
Kuhn H. and Tucker A. (1951) Nonlinear Programming. In
Second Berkeley Symposium on Mathematical Statistics and
Probability, pages 481‚Äì492. Univ. of California Press, Berkeley
and Los Angeles.
[Lam91]
Lambert J. (1991) Numerical Methods for Ordinary DiÔ¨Äeren-
tial Systems. John Wiley and Sons, Chichester.
[Lan50]
Lanczos C. (1950) An Iteration Method for the Solution of the
Eigenvalue Problem of Linear DiÔ¨Äerential and Integral Oper-
ator. J. Res. Nat. Bur. Stand. 45: 255‚Äì282.
[Lax65]
Lax P. (1965) Numerical Solution of Partial DiÔ¨Äerential Equa-
tions. Amer. Math. Monthly 72(2): 74‚Äì84.
[Lel92]
Lele S. (1992) Compact Finite DiÔ¨Äerence Schemes with
Spectral-like Resolution. Journ. of Comp. Physics 103(1): 16‚Äì
42.

636
References
[Lem89]
Lemarechal C. (1989) NondiÔ¨Äerentiable Optimization.
In
Nemhauser G., Kan A. R., and Todd M. (eds) Handbooks
Oper. Res. Management Sci., volume 1. Optimization, pages
529‚Äì572. North-Holland, Amsterdam.
[LH74]
Lawson C. and Hanson R. (1974) Solving Least Squares Prob-
lems. Prentice-Hall, Englewood CliÔ¨Äs, New York.
[LM68]
Lions J. L. and Magenes E. (1968) Problemes aux limit`es non-
homog`enes et applications. Dunod, Paris.
[LS96]
Lehoucq R. and Sorensen D. (1996) DeÔ¨Çation Techniques for
an Implicitly Restarted Iteration. SIAM J. Matrix Anal. Ap-
plic. 17(4): 789‚Äì821.
[Lue73]
Luenberger D. (1973) Introduction to Linear and Non Linear
Programming. Addison-Wesley, Reading, Massachusetts.
[Man69]
Mangasarian O. (1969) Non Linear Programming. Prentice-
Hall, Englewood CliÔ¨Äs, New Jersey.
[Man80]
ManteuÔ¨Äel T. (1980) An Incomplete Factorization Technique
for Positive DeÔ¨Ånite Linear Systems. Math. Comp. 150(34):
473‚Äì497.
[Mar86]
Markowich P. (1986) The Stationary Semiconductor Device
Equations. Springer-Verlag, Wien and New York.
[McK62]
McKeeman W. (1962) Crout with Equilibration and Iteration.
Comm. ACM 5: 553‚Äì555.
[MdV77]
Meijerink J. and der Vorst H. V. (1977) An Iterative Solution
Method for Linear Systems of Which the CoeÔ¨Écient Matrix is
a Symmetric M-matrix. Math. Comp. 137(31): 148‚Äì162.
[MM71]
MaxÔ¨Åeld J. and MaxÔ¨Åeld M. (1971) Abstract Algebra and So-
lution by Radicals. Saunders, Philadelphia.
[MMG87]
Martinet R., Morlet J., and Grossmann A. (1987) Analysis of
sound patterns through wavelet transforms. Int. J. of Pattern
Recogn. and ArtiÔ¨Åcial Intellig. 1(2): 273‚Äì302.
[MNS74]
M¬®akela M., Nevanlinna O., and Sipil¬®a A. (1974) On the Con-
cept of Convergence, Consistency and Stability in Connection
with Some Numerical Methods. Numer. Math. 22: 261‚Äì274.
[Mor84]
Morozov V. (1984) Methods for Solving Incorrectly Posed
Problems. Springer-Verlag, New York.

References
637
[Mul56]
Muller D. (1956) A Method for Solving Algebraic Equations
using an Automatic Computer. Math. Tables Aids Comput.
10: 208‚Äì215.
[ NAG95]
NAG (1995) NAG Fortran Library Manual - Mark 17. NAG
Ltd., Oxford.
[Nat65]
Natanson I. (1965) Constructive Function Theory, volume III.
Ungar, New York.
[NM65]
Nelder J. and Mead R. (1965) A simplex method for function
minimization. The Computer Journal 7: 308‚Äì313.
[Nob69]
Noble B. (1969) Applied Linear Algebra. Prentice-Hall, Engle-
wood CliÔ¨Äs, New York.
[OR70]
Ortega J. and Rheinboldt W. (1970) Iterative Solution of Non-
linear Equations in Several Variables. Academic Press, New
York and London.
[Pap62]
Papoulis A. (1962) The Fourier Integral and its Application.
McGraw-Hill, New York.
[Pap87]
Papoulis
A.
(1987)
Probability, Random Variables, and
Stochastic Processes. McGraw-Hill, New York.
[Par80]
Parlett
B.
(1980)
The
Symmetric
Eigenvalue
Problem.
Prentice-Hall, Englewood CliÔ¨Äs, NJ.
[PdK¬®UK83] Piessens R., deDoncker Kapenga E., ¬®Uberhuber C. W., and
Kahaner D. K. (1983) QUADPACK: A Subroutine Package
for Automatic Integration. Springer-Verlag, Berlin and Hei-
delberg.
[PJ55]
Peaceman D. and Jr. H. R. (1955) The numerical solution of
parabolic and elliptic diÔ¨Äerential equations. J. Soc. Ind. Appl.
Math. 3: 28‚Äì41.
[Pou96]
Poularikas A. (1996) The Transforms and Applications Hand-
book. CRC Press, Inc., Boca Raton, Florida.
[PR70]
Parlett B. and Reid J. (1970) On the Solution of a System of
Linear Equations Whose Matrix is Symmetric but not DeÔ¨Å-
nite. BIT 10: 386‚Äì397.
[PS91]
Pagani C. and Salsa S. (1991) Analisi Matematica, volume II.
Masson, Milano.

638
References
[PW79]
Peters G. and Wilkinson J. (1979) Inverse iteration, ill-
conditioned equations, and newton‚Äôs method. SIAM Review
21: 339‚Äì360.
[QV94]
Quarteroni A. and Valli A. (1994) Numerical Approximation
of Partial DiÔ¨Äerential Equations. Springer, Berlin and Heidel-
berg.
[QV99]
Quarteroni A. and Valli A. (1999) Domain Decomposition
Methods for Partial DiÔ¨Äerential Equations.
Oxford Science
Publications, New York.
[Ral65]
Ralston A. (1965) A First Course in Numerical Analysis.
McGraw-Hill, New York.
[Red86]
Reddy B. D. (1986) Applied Functional Analysis and Varia-
tional Methods in Engineering. McGraw-Hill, New York.
[Ric81]
Rice J. (1981) Matrix Computations and Mathematical Soft-
ware. McGraw-Hill, New York.
[Riv74]
Rivlin T. (1974) The Chebyshev Polynomials. John Wiley and
Sons, New York.
[RM67]
Richtmyer R. and Morton K. (1967) DiÔ¨Äerence Methods for
Initial Value Problems. Wiley, New York.
[RR78]
Ralston A. and Rabinowitz P. (1978) A First Course in Nu-
merical Analysis. McGraw-Hill, New York.
[Rud83]
Rudin W. (1983) Real and Complex Analysis. Tata McGraw-
Hill, New Delhi.
[Rut58]
Rutishauser H. (1958) Solution of Eigenvalue Problems with
the LR Transformation. Nat. Bur. Stand. Appl. Math. Ser.
49: 47‚Äì81.
[Saa90]
Saad Y. (1990) Sparskit: A basic tool kit for sparse matrix
computations. Technical Report 90-20, Research Institute for
Advanced Computer Science, NASA Ames Research Center,
MoÔ¨Äet Field, CA.
[Saa92]
Saad Y. (1992) Numerical Methods for Large Eigenvalue Prob-
lems. Halstead Press, New York.
[Saa96]
Saad Y. (1996) Iterative Methods for Sparse Linear Systems.
PWS Publishing Company, Boston.
[Sch67]
Schoenberg I. (1967) On Spline functions. In Shisha O. (ed)
Inequalities, pages 255‚Äì291. Academic Press, New York.

References
639
[Sch81]
Schumaker L. (1981) Splines Functions: Basic Theory. Wiley,
New York.
[Sel84]
Selberherr S. (1984) Analysis and Simulation of Semiconduc-
tor Devices. Springer-Verlag, Wien and New York.
[SG69]
Scharfetter D. and Gummel H. (1969) Large-signal analysis of
a silicon Read diode oscillator. IEEE Trans. on Electr. Dev.
16: 64‚Äì77.
[Ske79]
Skeel R. (1979) Scaling for Numerical Stability in Gaussian
Elimination. J. Assoc. Comput. Mach. 26: 494‚Äì526.
[Ske80]
Skeel R. (1980) Iterative ReÔ¨Ånement Implies Numerical Sta-
bility for Gaussian Elimination. Math. Comp. 35: 817‚Äì832.
[SL89]
Su B. and Liu D. (1989) Computational Geometry: Curve and
Surface Modeling. Academic Press, New York.
[Sla63]
Slater J. (1963) Introduction to Chemical Physics. McGraw-
Hill Book Co.
[Smi85]
Smith G. (1985) Numerical Solution of Partial DiÔ¨Äerential
Equations: Finite DiÔ¨Äerence Methods.
Oxford University
Press, Oxford.
[Son89]
Sonneveld P. (1989) Cgs, a fast lanczos-type solver for non-
symmetric linear systems.
SIAM Journal on ScientiÔ¨Åc and
Statistical Computing 10(1): 36‚Äì52.
[SR97]
Shampine L. F. and Reichelt M. W. (1997) The MATLAB
ODE Suite. SIAM J. Sci. Comput. 18: 1‚Äì22.
[SS90]
Stewart G. and Sun J. (1990) Matrix Perturbation Theory.
Academic Press, New York.
[SS98]
Schwab C. and Sch¬®otzau D. (1998) Mixed hp-FEM on
Anisotropic Meshes.
Mat. Models Methods Appl. Sci. 8(5):
787‚Äì820.
[Ste71]
Stetter H. (1971) Stability of discretization on inÔ¨Ånite inter-
vals. In Morris J. (ed) Conf. on Applications of Numerical
Analysis, pages 207‚Äì222. Springer-Verlag, Berlin.
[Ste73]
Stewart G. (1973) Introduction to Matrix Computations. Aca-
demic Press, New York.
[Str69]
Strassen V. (1969) Gaussian Elimination is Not Optimal. Nu-
mer. Math. 13: 727‚Äì764.

640
References
[Str80]
Strang G. (1980) Linear Algebra and Its Applications. Aca-
demic Press, New York.
[Str89]
Strikwerda J. (1989) Finite DiÔ¨Äerence Schemes and Partial
DiÔ¨Äerential Equations. Wadsworth and Brooks/Cole, PaciÔ¨Åc
Grove.
[Sze67]
Szeg¬®o G. (1967) Orthogonal Polynomials. AMS, Providence,
R.I.
[Tit37]
Titchmarsh E. (1937) Introduction to the Theory of Fourier
Integrals. Oxford.
[Var62]
Varga R. (1962) Matrix Iterative Analysis. Prentice-Hall, En-
glewood CliÔ¨Äs, New York.
[vdV92]
van der Vorst H. (1992) Bi-cgstab: a fast and smoothly con-
verging variant of bi-cg for the solution of non-symmetric lin-
ear systems. SIAM Jour. on Sci. and Stat. Comp. 12: 631‚Äì644.
[Ver96]
Verf¬®urth R. (1996) A Review of a Posteriori Error Estimation
and Adaptive Mesh ReÔ¨Ånement Techniques. Wiley, Teubner,
Germany.
[Wac66]
Wachspress E. (1966) Iterative Solutions of Elliptic Systems.
Prentice-Hall, Englewood CliÔ¨Äs, New York.
[Wal75]
Walsh G. (1975) Methods of Optimization. Wiley.
[Wal91]
Walker J. (1991) Fast Fourier Transforms. CRC Press, Boca
Raton.
[Wen66]
WendroÔ¨ÄB. (1966) Theoretical Numerical Analysis. Academic
Press, New York.
[Wid67]
Widlund O. (1967) A Note on Unconditionally Stable Linear
Multistep Methods. BIT 7: 65‚Äì70.
[Wil62]
Wilkinson J. (1962) Note on the Quadratic Convergence of
the Cyclic Jacobi Process. Numer. Math. 6: 296‚Äì300.
[Wil63]
Wilkinson J. (1963) Rounding Errors in Algebraic Processes.
Prentice-Hall, Englewood CliÔ¨Äs, New York.
[Wil65]
Wilkinson J. (1965) The Algebraic Eigenvalue Problem.
Clarendon Press, Oxford.
[Wil68]
Wilkinson J. (1968) A priori Error Analysis of Algebraic Pro-
cesses. In Intern. Congress Math., volume 19, pages 629‚Äì639.
Izdat. Mir, Moscow.

References
641
[Wol69]
Wolfe P. (1969) Convergence Conditions for Ascent Methods.
SIAM Review 11: 226‚Äì235.
[Wol71]
Wolfe P. (1971) Convergence Conditions for Ascent Methods.
II: Some Corrections. SIAM Review 13: 185‚Äì188.
[Wol78]
Wolfe M. (1978) Numerical Methods for Unconstrained Opti-
mization. Van Nostrand Reinhold Company, New York.
[You71]
Young D. (1971) Iterative Solution of Large Linear Systems.
Academic Press, New York.
[Zie77]
Zienkiewicz O. C. (1977) The Finite Element Method (Third
Edition). McGraw Hill, London.

Index of MATLAB Programs
forward row
Forward substitution: row-oriented version
.
66
forward col
Forward substitution: column-oriented version
66
backward col
Backward substitution: column-oriented version 66
lu kji
LU factorization of matrix A. kji version . .
77
lu jki
LU factorization of matrix A. jki version . .
77
lu ijk
LU factorization of the matrix A: ijk version
79
chol2
Cholesky factorization . . . . . . . . . . . . .
81
mod grams
ModiÔ¨Åed Gram-Schmidt method . . . . . . .
84
LUpivtot
LU factorization with complete pivoting . . .
88
lu band
LU factorization for a banded matrix
. . . .
92
forw band
Forward substitution for a banded matrix L
92
back band
Backward substitution for a banded matrix U
92
mod thomas
Thomas algorithm, modiÔ¨Åed version . . . . .
93
cond est
Algorithm for the approximation of K1(A) .
109
JOR
JOR method . . . . . . . . . . . . . . . . . .
135
SOR
SOR method . . . . . . . . . . . . . . . . . .
136
basicILU
Incomplete LU factorization
. . . . . . . . .
141
ilup
ILU(p) factorization . . . . . . . . . . . . . .
143
gradient
Gradient method with dynamic parameter
.
149
conjgrad
Preconditioned conjugate gradient method .
157
arnoldi alg
The Arnoldi algorithm
. . . . . . . . . . . .
161
arnoldi met
The Arnoldi method for linear systems
. . .
164
GMRES
The GMRES method for linear systems . . .
166
Lanczos
The Lanczos method for linear systems . . .
167
Lanczosnosym
The Lanczos method for unsymmetric systems170

644
Index of MATLAB Programs
powerm
Power method . . . . . . . . . . . . . . . . .
197
invpower
Inverse power method . . . . . . . . . . . . .
198
basicqr
Basic QR iteration . . . . . . . . . . . . . . .
203
houshess
Hessenberg-Householder method . . . . . . .
208
hessqr
Hessenberg-QR method . . . . . . . . . . . .
210
givensqr
QR factorization with Givens rotations . . .
211
vhouse
Construction of the Householder vector . . .
213
givcos
Computation of Givens cosine and sine . . .
214
garow
Product G(i, k, Œ∏)T M
. . . . . . . . . . . . .
214
gacol
Product MG(i, k, Œ∏) . . . . . . . . . . . . . .
214
qrshift
QR iteration with single shift . . . . . . . . .
217
qr2shift
QR iteration with double shift . . . . . . . .
220
psinorm
Evaluation of Œ®(A)
. . . . . . . . . . . . . .
229
symschur
Evaluation of c and s . . . . . . . . . . . . .
229
cycjacobi
Cyclic Jacobi method for symmetric matrices 229
sturm
Sturm sequence evaluation . . . . . . . . . .
232
givsturm
Givens method using the Sturm sequence . .
232
chcksign
Sign changes in the Sturm sequence . . . . .
232
bound
Calculation of the interval J = [Œ±, Œ≤]
. . . .
232
eiglancz
Extremal eigenvalues of a symmetric matrix
234
bisect
Bisection method
. . . . . . . . . . . . . . .
250
chord
The chord method . . . . . . . . . . . . . . .
254
secant
The secant method
. . . . . . . . . . . . . .
255
regfalsi
The Regula Falsi method . . . . . . . . . . .
255
newton
Newton‚Äôs method
. . . . . . . . . . . . . . .
255
Ô¨Åxpoint
Fixed-point method . . . . . . . . . . . . . .
260
horner
Synthetic division algorithm
. . . . . . . . .
263
newthorn
Newton-Horner method with reÔ¨Ånement . . .
266
mulldeÔ¨Ç
Muller‚Äôs method with reÔ¨Ånement . . . . . . .
269
aitken
Aitken‚Äôs extrapolation . . . . . . . . . . . . .
274
adptnewt
Adaptive Newton‚Äôs method . . . . . . . . . .
276
newtonxsys
Newton‚Äôs method for nonlinear systems . . .
285
broyden
Broyden‚Äôs method for nonlinear systems . . .
290
Ô¨Åxposys
Fixed-point method for nonlinear systems . .
293
hookejeeves
The method of Hooke and Jeeves (HJ)
. . .
296
explore
Exploration step in the HJ method
. . . . .
297
backtrackr
Backtraking for line search . . . . . . . . . .
303
lagrpen
Penalty method
. . . . . . . . . . . . . . . .
316
lagrmult
Method of Lagrange multipliers
. . . . . . .
319
interpol
Lagrange polynomial using Newton‚Äôs formula 334
dividif
Newton divided diÔ¨Äerences . . . . . . . . . .
336
hermpol
Osculating polynomial . . . . . . . . . . . . .
342
par spline
Parametric splines . . . . . . . . . . . . . . .
359
bernstein
Bernstein polynomials . . . . . . . . . . . . .
361
bezier
B¬¥ezier curves . . . . . . . . . . . . . . . . . .
361

Index of MATLAB Programs
645
midpntc
Midpoint composite formula
. . . . . . . . .
375
trapezc
Composite trapezoidal formula . . . . . . . .
376
simpsonc
Composite Cavalieri-Simpson formula . . . .
377
newtcot
Closed Newton-Cotes formulae . . . . . . . .
383
trapmodc
Composite corrected trapezoidal formula
. .
387
romberg
Romberg integration . . . . . . . . . . . . . .
391
simpadpt
Adaptive Cavalieri-Simpson formula . . . . .
397
redmidpt
Midpoint reduction formula . . . . . . . . . .
404
redtrap
Trapezoidal reduction formula . . . . . . . .
404
midptr2d
Midpoint rule on a triangle . . . . . . . . . .
406
traptr2d
Trapezoidal rule on a triangle
. . . . . . . .
406
coeÔ¨Çege
CoeÔ¨Écients of Legendre polynomials . . . . .
430
coeÔ¨Çagu
CoeÔ¨Écients of Laguerre polynomials . . . . .
430
coefherm
CoeÔ¨Écients of Hermite polynomials . . . . .
430
zplege
CoeÔ¨Écients of Gauss-Legendre formulae . . .
430
zplagu
CoeÔ¨Écients of Gauss-Laguerre formulae . . .
430
zpherm
CoeÔ¨Écients of Gauss-Hermite formulae . . .
430
dft
Discrete Fourier transform
. . . . . . . . . .
439
idft
Inverse discrete Fourier transform . . . . . .
439
Ô¨Ätrec
FFT algorithm in the recursive version
. . .
441
compdiÔ¨Ä
Compact diÔ¨Äerence schemes
. . . . . . . . .
446
multistep
Linear multistep methods . . . . . . . . . . .
490
predcor
Predictor-corrector scheme . . . . . . . . . .
507
ellfem
Linear FE for two-point BVPs . . . . . . . .
557
femmatr
Construction of the stiÔ¨Äness matrix . . . . .
557
H1error
Computation of the H1-norm of the error
.
558
artvisc
ArtiÔ¨Åcial viscosity . . . . . . . . . . . . . . .
570
sgvisc
Optimal artiÔ¨Åcial viscosity
. . . . . . . . . .
570
bern
Evaluation of the Bernoulli function . . . . .
571
thetameth
Œ∏-method for the heat equation . . . . . . . .
592
pardg1cg1
dG(1)cG(1) method for the heat equation . .
596
ipeidg0
dG(0) implicit Euler . . . . . . . . . . . . . .
621
ipeidg1
dG(1) implicit Euler . . . . . . . . . . . . . .
622

Index
A-conjugate directions, 151
A-stability, 481
absolute value notation, 62
adaptive error control, 43
adaptivity, 43
Newton‚Äôs method, 275
Runge-Kutta methods, 512
algorithm
Arnoldi, 160, 164
Cuthill-McKee, 98
Dekker-Brent, 256
Remes, 435
synthetic division, 262
Thomas, 91
ampliÔ¨Åcation
coeÔ¨Écient, 609
error, 612
analysis
a priori
for an iterative method, 132
a posteriori, 42
a priori, 42
backward, 42
forward, 41
B-splines, 353
parametric, 361
backward substitution, 65
bandwidth, 452
Bernoulli
function, 565
numbers, 389
bi-orthogonal bases, 168
binary digits, 46
boundary condition
Dirichlet, 541
Neumann, 541, 582
Robin, 579
breakdown, 160, 165
B¬¥ezier curve, 360
B¬¥ezier polygon, 359
CFL
condition, 606
number, 606
characteristic
curves, 598
variables, 600
characteristic polygon, 359
chopping, 51

648
Index
cofactor, 9
condition number, 34
asymptotic, 38
interpolation, 332
of a matrix, 36, 58
of a nonlinear equation, 246
of an eigenvalue, 189
of an eigenvector, 190
Skeel, 111
spectral, 59
consistency, 37, 124, 474, 493, 510
convex function, 295, 321
strongly, 312
convex hull, 98
critical point, 295
Dahlquist
Ô¨Årst barrier, 499
second barrier, 500
decomposition
real Schur, 201, 210, 211
generalized, 225
Schur, 14
singular value, 16
computation of the, 222
spectral, 15
deÔ¨Çation, 207, 216, 263
degree
of exactness, 380
of a vector, 160
of exactness, 372, 380, 405,
420
of freedom, 552
determinant of a matrix, 8
discrete
truncation of Fourier series,
417
Chebyshev transform, 426
Fourier transform, 438
Laplace transform, 458
Legendre transform, 428
maximum principle, 567, 611
scalar product, 425
dispersion, 448, 612
dissipation, 612
distribution, 547
derivative of a, 547
divided diÔ¨Äerence, 267, 334
domain of dependence, 600
numerical, 606
eigenfunctions, 589
eigenvalue, 12
algebraic multiplicity of an,
13
geometric multiplicity of an,
13
eigenvector, 12
elliptic
operator, 602
equation
characteristic, 12
diÔ¨Äerence, 482, 483, 499
heat, 581, 592
error
absolute, 40
cancellation, 39
global truncation, 474
interpolation, 329
local truncation, 474, 605
quadrature, 372
rounding, 45
estimate
a posteriori, 64, 195, 196, 381,
392, 395
a priori, 60, 381, 392, 395
exponential Ô¨Åtting, 565
factor
asymptotic convergence, 125
convergence, 125, 245, 259
growth, 104
factorization
block LU, 94
Cholesky, 80
compact forms, 78
Crout, 78
Doolittle, 78
incomplete, 140
LDMT , 79

Index
649
LU, 68
QR, 82, 209
Ô¨Åll-in, 98, 141
level, 141
Ô¨Ånite diÔ¨Äerences, 118, 177, 237,
533
backward, 443
centered, 443, 444
compact, 444
forward, 442
Ô¨Ånite elements, 118, 347
discontinuous, 594, 619
Ô¨Åxed-point iterations, 257
Ô¨Çop, 53
FOM, 163, 164
form
divided diÔ¨Äerence, 334
Lagrange, 329
formula
Armijo‚Äôs, 304
Goldstein‚Äôs, 304
Sherman-Morrison, 95
forward substitution, 65
Fourier coeÔ¨Écients, 436
discrete, 437
function
gamma, 528
Green‚Äôs, 532
Haar, 460
stability, 516
weight, 415
Galerkin
Ô¨Ånite element method, 364,
550
stabilized, 568
generalized method, 559
method, 544
pseudo-spectral approximation,
591
Gauss elimination
method, 68
multipliers in the, 69
GAXPY, 77
generalized inverse, 17
Gershgorin circles, 184
Gibbs phenomenon, 439
gradient, 294
graph, 97
oriented, 97, 186
Gronwall lemma, 471, 476
hyperbolic
operator, 602
hypernorms, 63
ILU, 140
inequality
Cauchy-Schwarz, 340, 568
H¬®older, 19
Kantorovich, 305
Poincar¬¥e, 536, 569
triangular, 569
Young‚Äôs, 544
integration
adaptive, 391
automatic, 391
multidimensional, 402
non adaptive, 391
interpolation
Hermite, 341
in two dimensions, 343
osculatory, 342
piecewise, 338
Taylor, 369
interpolation nodes, 328
piecewise, 345
IOM, 164
Jordan
block, 15
canonical form, 15
kernel of a matrix, 10
Krylov
method, 160
subspace, 159
Lagrange
interpolation, 328
multiplier, 312, 317

650
Index
Lagrangian function, 312
augmented, 317
penalized, 315
Laplace operator, 572
least-squares, 417
discrete, 431
Lebesgue
constant, 331, 332
linear map, 7
linear regression, 433
linearly independent vectors, 3
LU factorization, 72
M-matrix, 29, 145
machine epsilon, 49
machine precision, 51
mass-lumping, 588
matrix, 3
block, 4
companion, 242
convergent, 26
defective, 13
diagonalizable, 15
diagonally dominant, 29, 145
Gaussian transformation, 73
Givens, 206
Hessenberg, 12, 203, 211, 212
Hilbert, 70
Householder, 204
interpolation, 330
irreducible, 185
iteration, 124
mass, 587
norm, 21
normal, 7
orthogonal, 6
permutation, 5
preconditioning, 126
reducible, 185
rotation, 7
similar, 14
stiÔ¨Äness, 548
transformation, 204
trapezoidal, 11
triangular, 11
unitary, 7
Vandermonde, 368
matrix balancing, 110
maximum principle, 533, 534
discrete, 29
method
Œ∏‚àí, 584
Regula Falsi, 252
conjugate gradient, 153
Aitken, 272
alternating-direction, 158
backward Euler, 473
backward Euler/centred, 604
BiCG, 171
BiCGSTab, 171
bisection, 248
Broyden‚Äôs, 289
CGS, 171
chord, 252, 260
conjugate gradient, 168
with restart, 156
CR, 168
Crank-Nicolson, 473, 593
cyclic Jacobi, 228
damped Newton, 321
damped Newton‚Äôs, 308
Ô¨Ånite element, 573
Ô¨Åxed-point, 290
Fletcher-Reeves, 306
forward Euler, 473
forward Euler/centred, 603
forward Euler/uncentred, 603
frontal, 102
Gauss Seidel
symmetric, 133
Gauss-Jordan, 121
Gauss-Seidel, 128
nonlinear, 324
Givens, 230
GMRES, 166
with restart, 166
gradient, 300
Gram-Schmidt, 83
Heun, 473
Horner, 262

Index
651
Householder, 207
inverse power, 195
Jacobi, 127
JOR, 127
Lanczos, 167, 233
Lax-Friedrichs, 603, 608
Lax-WendroÔ¨Ä, 603, 608
Leap-Frog, 604, 611
Merson, 530
modiÔ¨Åed Euler, 529
modiÔ¨Åed Newton‚Äôs, 284
Monte Carlo, 407
Muller, 267
Newmark, 604, 611
Newton‚Äôs, 253, 261, 283
Newton-Horner, 263, 264
Nystron, 529
ORTHOMIN, 168
Polak-Ribi¬¥ere, 307
Powell-Broyden
symmetric, 311
power, 192
QMR, 171
QR, 200
with double shift, 218
with single shift, 215, 216
quasi-Newton, 288
reduction formula, 403
Richardson, 136
Richardson extrapolation, 387
Romberg integration, 389, 409
Rutishauser, 202
secant, 252, 257, 288
secant-like, 309
Simplex, 299
SSOR, 134
steepest descent, 305
SteÔ¨Äensen, 280
successive over-relaxation, 128
upwind, 603, 607
minimax
property, 418
minimizer
global, 294, 311
local, 294, 311
model
computational, 43
module of continuity, 386
nodes
Gauss, 426
Gauss-Lobatto, 424, 426
norm
absolute, 32
compatible, 21, 22
consistent, 21
energy, 29
equivalent, 20
essentially strict, 432
Frobenius, 22
H¬®older, 19
matrix, 21
maximum, 19, 330
spectral, 23
normal equations, 112
numbers
de-normalized, 48
Ô¨Åxed-point, 46
Ô¨Çoating-point, 47
numerical Ô¨Çux, 602
numerical method, 37
adaptive, 43
consistent, 37
convergent, 39
eÔ¨Éciency, 44
ill conditioned, 38
reliability, 44
stable, 38
well posed, 38
orbit, 523
overÔ¨Çow, 51
P¬¥eclet number, 561
local, 563
Pad¬¥e approximation, 370
parabolic
operator, 602
pattern of a matrix, 97, 575
penalty parameter, 315

652
Index
phase angle, 612
pivoting, 85
complete, 86
partial, 86
Poisson equation, 572
polyalgorithm, 277
polynomial
Bernstein, 359
best approximation, 330, 433
characteristic, 12, 329
Fourier, 435
Hermite, 429
interpolating, 328
Lagrange piecewise, 346
Laguerre, 428
nodal, 329
orthogonal, 415
preconditioner, 126
block, 139
diagonal, 140
ILU, 142
least-squares, 145
MILU, 144
point, 139
polynomial, 145
principal root of unity, 437
problem
Cauchy, 469
generalized eigenvalue, 146, 224,
238, 589
ill posed, 34, 35
ill-conditioned, 34
stiÔ¨Ä, 520
well conditioned, 34
well posed, 33
programming
linear, 282
nonlinear, 282, 313
pseudo-inverse, 17, 114
pseudo-spectral
derivative, 449
diÔ¨Äerentiation matrix, 449
quadrature formula, 371
Cavalieri-Simpson, 377, 385,
400, 401, 409
composite Cavalieri-Simpson,
377
composite midpoint, 374
composite Newton-Cotes, 383
composite trapezoidal, 376
corrected trapezoidal, 386
Gauss, 421
on triangles, 406
Gauss-Kronrod, 393
Gauss-Lobatto, 422, 425
Gauss-Radau
on triangles, 406
Hermite, 372, 386
Lagrange, 372
midpoint, 373, 385
on triangles, 405
Newton-Cotes, 378
on triangles, 404
pseudo-random, 408
trapezoidal, 375, 385, 438
on triangles, 405
quotient
Rayleigh, 12
generalized, 146
QZ iteration, 225
rank of a matrix, 9
rate
asymptotic convergence, 125
convergence, 259
reduction formula
midpoint, 403
trapezoidal, 404
reference triangle, 345
regularization, 34
representation
Ô¨Çoating-point, 47
positional, 45
residual, 247
resolvent, 35
restart, 164
round digit, 53
rounding, 51

Index
653
roundoÔ¨Äunit, 51
rule
Cramer‚Äôs, 58
Descartes, 262
Laplace, 9
Runge‚Äôs counterexample, 331, 344,
353
SAXPY, 77
saxpy, 77
scalar product, 18
scaling, 110
by rows, 110
Schur
complement, 102
decomposition, 14
semi-discretization, 584, 586
series
Chebyshev, 418
Fourier, 416, 582
Legendre, 419
set
bi-orthogonal, 189
similarity transformation, 14
singular integrals, 398
singular values, 16
space
normed, 19
phase, 523
Sobolev, 543
vector, 1
spectral radius, 13
spectrum of a matrix, 12
spline
cardinal, 351
interpolatory cubic, 349
natural, 349
not-a-knot, 350
one-dimensional, 348
parametric, 358
periodic, 348
splitting, 126
stability
absolute, 479, 480, 499, 502
region of, 480
asymptotic, 471
factors, 42
Liapunov, 471
of interpolation, 332
relative, 502
zero, 477, 495, 502
standard deviation, 298
statistic mean value, 407
stencil, 445
stopping tests, 171, 269
strong formulation, 547
Sturm sequences, 230
subspace
generated, 2
invariant, 13
vector, 2
substructures, 100
Sylvester criterion, 29
system
hyperbolic, 599
strictly, 600
overdetermined, 112
underdetermined, 115
theorem
Abel, 262
Bauer-Fike, 187
Cauchy, 262
Cayley-Hamilton, 13
Courant-Fisher, 146, 233
de la Vall¬¥ee-Poussin, 434
equioscillation, 433
Gershgorin, 184
Ostrowski, 259
polynomial division, 263
Schur, 14
trace of a matrix, 8
transform
fast Fourier, 426
Fourier, 450
Laplace, 455
Zeta, 457
triangulation, 344, 573
underÔ¨Çow, 51

654
Index
upwind Ô¨Ånite diÔ¨Äerence, 565
weak
formulation, 545
solution, 545, 599
wobbling precision, 49



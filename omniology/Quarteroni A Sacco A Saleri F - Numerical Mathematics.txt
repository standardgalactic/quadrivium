Numerical Mathematics
Alfio Quarteroni
Riccardo Sacco
Fausto Saleri
Springer

Texts in Applied Mathematicsm37
Springer
New York
Berlin
Heidelberg
Barcelona
Hong Kong
London
Milan
Paris
Singapore
Tokyo
Editors
J.E. Marsden
L. Sirovich
M. Golubitsky
W. Jäger
Advisors
G. Iooss
P. Holmes
D. Barkley
M. Dellnitz
P. Newton


Alfio QuarteroniMMRiccardo Sacco
Fausto Saleri
123
Numerical Mathematics
With 134 Illustrations

Alfio Quarteroni
Department of Mathematics
Ecole Polytechnique
MFe´de´rale de Lausanne 
CH-1015 Lausanne
Switzerland
alfio.quarteroni@epfl.ch
Riccardo Sacco
Dipartimento di Matematica
Politecnico di Milano
Piazza Leonardo da Vinci 32
20133 Milan
Italy
ricsac@mate.polimi.it
Fausto Saleri
Dipartimento di Matematica,
M“F. Enriques”
Università degli Studi di
MMilano
Via Saldini 50
20133 Milan
Italy
fausto.saleri@unimi.it
Series Editors
J.E. Marsden 
Control and Dynamical Systems, 107–81
California Institute of Technology
Pasadena, CA 91125
USA
M. Golubitsky
Department of Mathematics
University of Houston
Houston, TX 77204-3476
USA
L. Sirovich
Division of Applied Mathematics
Brown University 
Providence, RI 02912
USA
W. J¨ager
Department of Applied Mathematics
Universit ¨at Heidelberg
Im Neuenheimer Feld 294
69120 Heidelberg 
Germany
Library of Congress Cataloging-in-Publication Data
Quarteroni, Alfio.
Numerical mathematics/Alfio Quarteroni, Riccardo Sacco, Fausto Saleri.
p.Mcm. — (Texts in applied mathematics; 37)
Includes bibliographical references and index.
ISBN 0-387-98959-5 (alk. paper)
1. Numerical analysis.MI. Sacco, Riccardo.MII. Saleri, Fausto.MIII. Title.MIV. Series.
I. Title.MMII. Series.
QA297.Q83M2000
519.4—dc21
99-059414
© 2000 Springer-Verlag New York, Inc.
All rights reserved. This work may not be translated or copied in whole or in part without
the written permission of the publisher (Springer-Verlag New York, Inc., 175 Fifth Avenue,
New York, NY 10010, USA), except for brief excerpts in connection with reviews or scholarly
analysis. Use in connection with any form of information storage and retrieval, electronic
adaptation, computer software, or by similar or dissimilar methodology now known or heraf-
ter developed is forbidden.
The use of general descriptive names, trade names, trademarks, etc., in this publication, even
if the former are not especially identified, is not to be taken as a sign that such names, as
understood by the Trade Marks and Merchandise Marks Act, may accordingly be used freely
by anyone.
ISBN 0-387-98959-5nSpringer-VerlagnNew YorknBerlinnHeidelbergMSPIN 10747955
Mathematics Subject Classification (1991): 15-01, 34-01, 35-01, 65-01


Preface
Numerical mathematics is the branch of mathematics that proposes, de-
velops, analyzes and applies methods from scientiﬁc computing to several
ﬁelds including analysis, linear algebra, geometry, approximation theory,
functional equations, optimization and diﬀerential equations. Other disci-
plines such as physics, the natural and biological sciences, engineering, and
economics and the ﬁnancial sciences frequently give rise to problems that
need scientiﬁc computing for their solutions.
As such, numerical mathematics is the crossroad of several disciplines of
great relevance in modern applied sciences, and can become a crucial tool
for their qualitative and quantitative analysis. This role is also emphasized
by the continual development of computers and algorithms, which make it
possible nowadays, using scientiﬁc computing, to tackle problems of such
a large size that real-life phenomena can be simulated providing accurate
responses at aﬀordable computational cost.
The corresponding spread of numerical software represents an enrichment
for the scientiﬁc community. However, the user has to make the correct
choice of the method (or the algorithm) which best suits the problem at
hand. As a matter of fact, no black-box methods or algorithms exist that
can eﬀectively and accurately solve all kinds of problems.
One of the purposes of this book is to provide the mathematical foun-
dations of numerical methods, to analyze their basic theoretical proper-
ties (stability, accuracy, computational complexity), and demonstrate their
performances on examples and counterexamples which outline their pros

viii
Preface
and cons. This is done using the MATLAB® 1 software environment. This
choice satisﬁes the two fundamental needs of user-friendliness and wide-
spread diﬀusion, making it available on virtually every computer.
Every chapter is supplied with examples, exercises and applications of
the discussed theory to the solution of real-life problems. The reader is
thus in the ideal condition for acquiring the theoretical knowledge that is
required to make the right choice among the numerical methodologies and
make use of the related computer programs.
This book is primarily addressed to undergraduate students, with partic-
ular focus on the degree courses in Engineering, Mathematics, Physics and
Computer Science. The attention which is paid to the applications and the
related development of software makes it valuable also for graduate stu-
dents, researchers and users of scientiﬁc computing in the most widespread
professional ﬁelds.
The content of the volume is organized into four parts and 13 chapters.
Part I comprises two chapters in which we review basic linear algebra and
introduce the general concepts of consistency, stability and convergence of
a numerical method as well as the basic elements of computer arithmetic.
Part II is on numerical linear algebra, and is devoted to the solution of
linear systems (Chapters 3 and 4) and eigenvalues and eigenvectors com-
putation (Chapter 5).
We continue with Part III where we face several issues about functions
and their approximation. Speciﬁcally, we are interested in the solution of
nonlinear equations (Chapter 6), solution of nonlinear systems and opti-
mization problems (Chapter 7), polynomial approximation (Chapter 8) and
numerical integration (Chapter 9).
Part IV, which is the more demanding as a mathematical background, is
concerned with approximation, integration and transforms based on orthog-
onal polynomials (Chapter 10), solution of initial value problems (Chap-
ter 11), boundary value problems (Chapter 12) and initial-boundary value
problems for parabolic and hyperbolic equations (Chapter 13).
Part I provides the indispensable background. Each of the remaining
Parts has a size and a content that make it well suited for a semester
course.
A guideline index to the use of the numerous MATLAB Programs de-
veloped in the book is reported at the end of the volume. These programs
are also available at the web site address:
http://www1.mate.polimi.it/˜calnum/programs.html
For the reader’s ease, any code is accompanied by a brief description of
its input/output parameters.
We express our thanks to the staﬀat Springer-Verlag New York for their
expert guidance and assistance with editorial aspects, as well as to Dr.
1MATLAB is a registered trademark of The MathWorks, Inc.

Preface
ix
Martin Peters from Springer-Verlag Heidelberg and Dr. Francesca Bonadei
from Springer-Italia for their advice and friendly collaboration all along
this project.
We gratefully thank Professors L. Gastaldi and A. Valli for their useful
comments on Chapters 12 and 13.
We also wish to express our gratitude to our families for their forbearance
and understanding, and dedicate this book to them.
Lausanne, Switzerland
Alﬁo Quarteroni
Milan, Italy
Riccardo Sacco
Milan, Italy
Fausto Saleri
January 2000

Contents
Series Preface
v
Preface
vii
PART I: Getting Started
1.
Foundations of Matrix Analysis
1
1.1
Vector Spaces . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.2
Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
1.3
Operations with Matrices . . . . . . . . . . . . . . . . . . .
5
1.3.1
Inverse of a Matrix . . . . . . . . . . . . . . . . . .
6
1.3.2
Matrices and Linear Mappings
. . . . . . . . . . .
7
1.3.3
Operations with Block-Partitioned Matrices . . . .
7
1.4
Trace and Determinant of a Matrix . . . . . . . . . . . . .
8
1.5
Rank and Kernel of a Matrix
. . . . . . . . . . . . . . . .
9
1.6
Special Matrices . . . . . . . . . . . . . . . . . . . . . . . .
10
1.6.1
Block Diagonal Matrices . . . . . . . . . . . . . . .
10
1.6.2
Trapezoidal and Triangular Matrices . . . . . . . .
11
1.6.3
Banded Matrices . . . . . . . . . . . . . . . . . . .
11
1.7
Eigenvalues and Eigenvectors
. . . . . . . . . . . . . . . .
12
1.8
Similarity Transformations . . . . . . . . . . . . . . . . . .
14
1.9
The Singular Value Decomposition (SVD)
. . . . . . . . .
16
1.10
Scalar Product and Norms in Vector Spaces . . . . . . . .
17
1.11
Matrix Norms . . . . . . . . . . . . . . . . . . . . . . . . .
21

xii
Contents
1.11.1
Relation Between Norms and the
Spectral Radius of a Matrix . . . . . . . . . . . . .
25
1.11.2
Sequences and Series of Matrices . . . . . . . . . .
26
1.12
Positive Deﬁnite, Diagonally Dominant and M-Matrices
.
27
1.13
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . .
30
2.
Principles of Numerical Mathematics
33
2.1
Well-Posedness and Condition Number of a Problem
. . .
33
2.2
Stability of Numerical Methods . . . . . . . . . . . . . . .
37
2.2.1
Relations Between Stability and Convergence . . .
40
2.3
A priori and a posteriori Analysis . . . . . . . . . . . . . .
41
2.4
Sources of Error in Computational Models . . . . . . . . .
43
2.5
Machine Representation of Numbers
. . . . . . . . . . . .
45
2.5.1
The Positional System . . . . . . . . . . . . . . . .
45
2.5.2
The Floating-Point Number System
. . . . . . . .
46
2.5.3
Distribution of Floating-Point Numbers . . . . . .
49
2.5.4
IEC/IEEE Arithmetic . . . . . . . . . . . . . . . .
49
2.5.5
Rounding of a Real Number in Its
Machine Representation . . . . . . . . . . . . . . .
50
2.5.6
Machine Floating-Point Operations . . . . . . . . .
52
2.6
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . .
54
PART II: Numerical Linear Algebra
3.
Direct Methods for the Solution of Linear Systems
57
3.1
Stability Analysis of Linear Systems
. . . . . . . . . . . .
58
3.1.1
The Condition Number of a Matrix
. . . . . . . .
58
3.1.2
Forward a priori Analysis . . . . . . . . . . . . . .
60
3.1.3
Backward a priori Analysis . . . . . . . . . . . . .
63
3.1.4
A posteriori Analysis . . . . . . . . . . . . . . . . .
64
3.2
Solution of Triangular Systems
. . . . . . . . . . . . . . .
65
3.2.1
Implementation of Substitution Methods
. . . . .
65
3.2.2
Rounding Error Analysis
. . . . . . . . . . . . . .
67
3.2.3
Inverse of a Triangular Matrix
. . . . . . . . . . .
67
3.3
The Gaussian Elimination Method (GEM) and
LU Factorization
. . . . . . . . . . . . . . . . . . . . . . .
68
3.3.1
GEM as a Factorization Method . . . . . . . . . .
72
3.3.2
The Eﬀect of Rounding Errors
. . . . . . . . . . .
76
3.3.3
Implementation of LU Factorization . . . . . . . .
77
3.3.4
Compact Forms of Factorization
. . . . . . . . . .
78
3.4
Other Types of Factorization . . . . . . . . . . . . . . . . .
79
3.4.1
LDMT Factorization . . . . . . . . . . . . . . . . .
79
3.4.2
Symmetric and Positive Deﬁnite Matrices:
The Cholesky Factorization . . . . . . . . . . . . .
80
3.4.3
Rectangular Matrices: The QR Factorization
. . .
82

Contents
xiii
3.5
Pivoting . . . . . . . . . . . . . . . . . . . . . . . . . . . .
85
3.6
Computing the Inverse of a Matrix
. . . . . . . . . . . . .
89
3.7
Banded Systems . . . . . . . . . . . . . . . . . . . . . . . .
90
3.7.1
Tridiagonal Matrices . . . . . . . . . . . . . . . . .
91
3.7.2
Implementation Issues . . . . . . . . . . . . . . . .
92
3.8
Block Systems . . . . . . . . . . . . . . . . . . . . . . . . .
93
3.8.1
Block LU Factorization
. . . . . . . . . . . . . . .
94
3.8.2
Inverse of a Block-Partitioned Matrix
. . . . . . .
95
3.8.3
Block Tridiagonal Systems . . . . . . . . . . . . . .
95
3.9
Sparse Matrices . . . . . . . . . . . . . . . . . . . . . . . .
97
3.9.1
The Cuthill-McKee Algorithm
. . . . . . . . . . .
98
3.9.2
Decomposition into Substructures
. . . . . . . . .
100
3.9.3
Nested Dissection . . . . . . . . . . . . . . . . . . .
103
3.10
Accuracy of the Solution Achieved Using GEM
. . . . . .
103
3.11
An Approximate Computation of K(A) . . . . . . . . . . .
106
3.12
Improving the Accuracy of GEM
. . . . . . . . . . . . . .
109
3.12.1
Scaling
. . . . . . . . . . . . . . . . . . . . . . . .
110
3.12.2
Iterative Reﬁnement . . . . . . . . . . . . . . . . .
111
3.13
Undetermined Systems . . . . . . . . . . . . . . . . . . . .
112
3.14
Applications . . . . . . . . . . . . . . . . . . . . . . . . . .
115
3.14.1
Nodal Analysis of a Structured Frame . . . . . . .
115
3.14.2
Regularization of a Triangular Grid
. . . . . . . .
118
3.15
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . .
121
4.
Iterative Methods for Solving Linear Systems
123
4.1
On the Convergence of Iterative Methods . . . . . . . . . .
123
4.2
Linear Iterative Methods . . . . . . . . . . . . . . . . . . .
126
4.2.1
Jacobi, Gauss-Seidel and Relaxation Methods . . .
127
4.2.2
Convergence Results for Jacobi and
Gauss-Seidel Methods . . . . . . . . . . . . . . . .
129
4.2.3
Convergence Results for the Relaxation Method
.
131
4.2.4
A priori Forward Analysis . . . . . . . . . . . . . .
132
4.2.5
Block Matrices . . . . . . . . . . . . . . . . . . . .
133
4.2.6
Symmetric Form of the Gauss-Seidel and
SOR Methods . . . . . . . . . . . . . . . . . . . . .
133
4.2.7
Implementation Issues . . . . . . . . . . . . . . . .
135
4.3
Stationary and Nonstationary Iterative Methods . . . . . .
136
4.3.1
Convergence Analysis of the Richardson Method .
137
4.3.2
Preconditioning Matrices
. . . . . . . . . . . . . .
139
4.3.3
The Gradient Method . . . . . . . . . . . . . . . .
146
4.3.4
The Conjugate Gradient Method . . . . . . . . . .
150
4.3.5
The Preconditioned Conjugate Gradient Method .
156
4.3.6
The Alternating-Direction Method . . . . . . . . .
158
4.4
Methods Based on Krylov Subspace Iterations . . . . . . .
159
4.4.1
The Arnoldi Method for Linear Systems . . . . . .
162

xiv
Contents
4.4.2
The GMRES Method
. . . . . . . . . . . . . . . .
165
4.4.3
The Lanczos Method for Symmetric Systems . . .
167
4.5
The Lanczos Method for Unsymmetric Systems . . . . . .
168
4.6
Stopping Criteria . . . . . . . . . . . . . . . . . . . . . . .
171
4.6.1
A Stopping Test Based on the Increment
. . . . .
172
4.6.2
A Stopping Test Based on the Residual
. . . . . .
174
4.7
Applications . . . . . . . . . . . . . . . . . . . . . . . . . .
174
4.7.1
Analysis of an Electric Network . . . . . . . . . . .
174
4.7.2
Finite Diﬀerence Analysis of Beam Bending . . . .
177
4.8
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . .
179
5.
Approximation of Eigenvalues and Eigenvectors
183
5.1
Geometrical Location of the Eigenvalues . . . . . . . . . .
183
5.2
Stability and Conditioning Analysis . . . . . . . . . . . . .
186
5.2.1
A priori Estimates . . . . . . . . . . . . . . . . . .
186
5.2.2
A posteriori Estimates . . . . . . . . . . . . . . . .
190
5.3
The Power Method . . . . . . . . . . . . . . . . . . . . . .
192
5.3.1
Approximation of the Eigenvalue of
Largest Module . . . . . . . . . . . . . . . . . . . .
192
5.3.2
Inverse Iteration
. . . . . . . . . . . . . . . . . . .
195
5.3.3
Implementation Issues . . . . . . . . . . . . . . . .
196
5.4
The QR Iteration . . . . . . . . . . . . . . . . . . . . . . .
200
5.5
The Basic QR Iteration . . . . . . . . . . . . . . . . . . . .
201
5.6
The QR Method for Matrices in Hessenberg Form . . . . .
203
5.6.1
Householder and Givens Transformation Matrices
204
5.6.2
Reducing a Matrix in Hessenberg Form
. . . . . .
207
5.6.3
QR Factorization of a Matrix in Hessenberg Form
209
5.6.4
The Basic QR Iteration Starting from
Upper Hessenberg Form . . . . . . . . . . . . . . .
210
5.6.5
Implementation of Transformation Matrices . . . .
212
5.7
The QR Iteration with Shifting Techniques . . . . . . . . .
215
5.7.1
The QR Method with Single Shift
. . . . . . . . .
215
5.7.2
The QR Method with Double Shift . . . . . . . . .
218
5.8
Computing the Eigenvectors and the SVD of a Matrix
. .
221
5.8.1
The Hessenberg Inverse Iteration . . . . . . . . . .
221
5.8.2
Computing the Eigenvectors from the
Schur Form of a Matrix . . . . . . . . . . . . . . .
221
5.8.3
Approximate Computation of the SVD of a Matrix 222
5.9
The Generalized Eigenvalue Problem . . . . . . . . . . . .
224
5.9.1
Computing the Generalized Real Schur Form . . .
225
5.9.2
Generalized Real Schur Form of
Symmetric-Deﬁnite Pencils
. . . . . . . . . . . . .
226
5.10
Methods for Eigenvalues of Symmetric Matrices . . . . . .
227
5.10.1
The Jacobi Method
. . . . . . . . . . . . . . . . .
227
5.10.2
The Method of Sturm Sequences . . . . . . . . . .
230

Contents
xv
5.11
The Lanczos Method . . . . . . . . . . . . . . . . . . . . .
233
5.12
Applications . . . . . . . . . . . . . . . . . . . . . . . . . .
235
5.12.1
Analysis of the Buckling of a Beam . . . . . . . . .
236
5.12.2
Free Dynamic Vibration of a Bridge . . . . . . . .
238
5.13
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . .
240
PART III: Around Functions and Functionals
6.
Rootﬁnding for Nonlinear Equations
245
6.1
Conditioning of a Nonlinear Equation . . . . . . . . . . . .
246
6.2
A Geometric Approach to Rootﬁnding
. . . . . . . . . . .
248
6.2.1
The Bisection Method . . . . . . . . . . . . . . . .
248
6.2.2
The Methods of Chord, Secant and Regula Falsi
and Newton’s Method . . . . . . . . . . . . . . . .
251
6.2.3
The Dekker-Brent Method
. . . . . . . . . . . . .
256
6.3
Fixed-Point Iterations for Nonlinear Equations . . . . . . .
257
6.3.1
Convergence Results for
Some Fixed-Point Methods . . . . . . . . . . . . .
260
6.4
Zeros of Algebraic Equations . . . . . . . . . . . . . . . . .
261
6.4.1
The Horner Method and Deﬂation . . . . . . . . .
262
6.4.2
The Newton-Horner Method
. . . . . . . . . . . .
263
6.4.3
The Muller Method
. . . . . . . . . . . . . . . . .
267
6.5
Stopping Criteria . . . . . . . . . . . . . . . . . . . . . . .
269
6.6
Post-Processing Techniques for Iterative Methods . . . . .
272
6.6.1
Aitken’s Acceleration
. . . . . . . . . . . . . . . .
272
6.6.2
Techniques for Multiple Roots
. . . . . . . . . . .
275
6.7
Applications . . . . . . . . . . . . . . . . . . . . . . . . . .
276
6.7.1
Analysis of the State Equation for a Real Gas
. .
276
6.7.2
Analysis of a Nonlinear Electrical Circuit
. . . . .
277
6.8
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . .
279
7.
Nonlinear Systems and Numerical Optimization
281
7.1
Solution of Systems of Nonlinear Equations
. . . . . . . .
282
7.1.1
Newton’s Method and Its Variants . . . . . . . . .
283
7.1.2
Modiﬁed Newton’s Methods . . . . . . . . . . . . .
284
7.1.3
Quasi-Newton Methods . . . . . . . . . . . . . . .
288
7.1.4
Secant-Like Methods . . . . . . . . . . . . . . . . .
288
7.1.5
Fixed-Point Methods . . . . . . . . . . . . . . . . .
290
7.2
Unconstrained Optimization . . . . . . . . . . . . . . . . .
294
7.2.1
Direct Search Methods . . . . . . . . . . . . . . . .
295
7.2.2
Descent Methods . . . . . . . . . . . . . . . . . . .
300
7.2.3
Line Search Techniques
. . . . . . . . . . . . . . .
302
7.2.4
Descent Methods for Quadratic Functions . . . . .
304
7.2.5
Newton-Like Methods for Function Minimization .
307
7.2.6
Quasi-Newton Methods . . . . . . . . . . . . . . .
308

xvi
Contents
7.2.7
Secant-Like Methods . . . . . . . . . . . . . . . . .
309
7.3
Constrained Optimization
. . . . . . . . . . . . . . . . . .
311
7.3.1
Kuhn-Tucker Necessary Conditions for
Nonlinear Programming . . . . . . . . . . . . . . .
313
7.3.2
The Penalty Method . . . . . . . . . . . . . . . . .
315
7.3.3
The Method of Lagrange Multipliers . . . . . . . .
317
7.4
Applications . . . . . . . . . . . . . . . . . . . . . . . . . .
319
7.4.1
Solution of a Nonlinear System Arising from
Semiconductor Device Simulation . . . . . . . . . .
320
7.4.2
Nonlinear Regularization of a Discretization Grid .
323
7.5
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . .
325
8.
Polynomial Interpolation
327
8.1
Polynomial Interpolation . . . . . . . . . . . . . . . . . . .
328
8.1.1
The Interpolation Error . . . . . . . . . . . . . . .
329
8.1.2
Drawbacks of Polynomial Interpolation on Equally
Spaced Nodes and Runge’s Counterexample . . . .
330
8.1.3
Stability of Polynomial Interpolation . . . . . . . .
332
8.2
Newton Form of the Interpolating Polynomial . . . . . . .
333
8.2.1
Some Properties of Newton Divided Diﬀerences . .
335
8.2.2
The Interpolation Error Using Divided Diﬀerences
337
8.3
Piecewise Lagrange Interpolation
. . . . . . . . . . . . . .
338
8.4
Hermite-BirkoﬀInterpolation
. . . . . . . . . . . . . . . .
341
8.5
Extension to the Two-Dimensional Case
. . . . . . . . . .
343
8.5.1
Polynomial Interpolation
. . . . . . . . . . . . . .
343
8.5.2
Piecewise Polynomial Interpolation . . . . . . . . .
344
8.6
Approximation by Splines
. . . . . . . . . . . . . . . . . .
348
8.6.1
Interpolatory Cubic Splines . . . . . . . . . . . . .
349
8.6.2
B-Splines . . . . . . . . . . . . . . . . . . . . . . .
353
8.7
Splines in Parametric Form
. . . . . . . . . . . . . . . . .
357
8.7.1
B´ezier Curves and Parametric B-Splines . . . . . .
359
8.8
Applications . . . . . . . . . . . . . . . . . . . . . . . . . .
362
8.8.1
Finite Element Analysis of a Clamped Beam
. . .
363
8.8.2
Geometric Reconstruction Based on
Computer Tomographies . . . . . . . . . . . . . . .
366
8.9
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . .
368
9.
Numerical Integration
371
9.1
Quadrature Formulae . . . . . . . . . . . . . . . . . . . . .
371
9.2
Interpolatory Quadratures . . . . . . . . . . . . . . . . . .
373
9.2.1
The Midpoint or Rectangle Formula . . . . . . . .
373
9.2.2
The Trapezoidal Formula . . . . . . . . . . . . . .
375
9.2.3
The Cavalieri-Simpson Formula . . . . . . . . . . .
377
9.3
Newton-Cotes Formulae
. . . . . . . . . . . . . . . . . . .
378
9.4
Composite Newton-Cotes Formulae . . . . . . . . . . . . .
383

Contents
xvii
9.5
Hermite Quadrature Formulae . . . . . . . . . . . . . . . .
386
9.6
Richardson Extrapolation
. . . . . . . . . . . . . . . . . .
387
9.6.1
Romberg Integration . . . . . . . . . . . . . . . . .
389
9.7
Automatic Integration
. . . . . . . . . . . . . . . . . . . .
391
9.7.1
Non Adaptive Integration Algorithms
. . . . . . .
392
9.7.2
Adaptive Integration Algorithms . . . . . . . . . .
394
9.8
Singular Integrals . . . . . . . . . . . . . . . . . . . . . . .
398
9.8.1
Integrals of Functions with Finite
Jump Discontinuities . . . . . . . . . . . . . . . . .
398
9.8.2
Integrals of Inﬁnite Functions . . . . . . . . . . . .
398
9.8.3
Integrals over Unbounded Intervals . . . . . . . . .
401
9.9
Multidimensional Numerical Integration
. . . . . . . . . .
402
9.9.1
The Method of Reduction Formula . . . . . . . . .
403
9.9.2
Two-Dimensional Composite Quadratures . . . . .
404
9.9.3
Monte Carlo Methods for
Numerical Integration . . . . . . . . . . . . . . . .
407
9.10
Applications . . . . . . . . . . . . . . . . . . . . . . . . . .
408
9.10.1
Computation of an Ellipsoid Surface . . . . . . . .
408
9.10.2
Computation of the Wind Action on a
Sailboat Mast . . . . . . . . . . . . . . . . . . . . .
410
9.11
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . .
412
PART IV: Transforms, Diﬀerentiation
and Problem Discretization
10. Orthogonal Polynomials in Approximation Theory
415
10.1
Approximation of Functions by Generalized Fourier Series
415
10.1.1
The Chebyshev Polynomials . . . . . . . . . . . . .
417
10.1.2
The Legendre Polynomials
. . . . . . . . . . . . .
419
10.2
Gaussian Integration and Interpolation . . . . . . . . . . .
419
10.3
Chebyshev Integration and Interpolation . . . . . . . . . .
424
10.4
Legendre Integration and Interpolation . . . . . . . . . . .
426
10.5
Gaussian Integration over Unbounded Intervals
. . . . . .
428
10.6
Programs for the Implementation of Gaussian Quadratures 429
10.7
Approximation of a Function in the Least-Squares Sense .
431
10.7.1
Discrete Least-Squares Approximation . . . . . . .
431
10.8
The Polynomial of Best Approximation . . . . . . . . . . .
433
10.9
Fourier Trigonometric Polynomials
. . . . . . . . . . . . .
435
10.9.1
The Gibbs Phenomenon . . . . . . . . . . . . . . .
439
10.9.2
The Fast Fourier Transform . . . . . . . . . . . . .
440
10.10 Approximation of Function Derivatives . . . . . . . . . . .
442
10.10.1 Classical Finite Diﬀerence Methods . . . . . . . . .
442
10.10.2 Compact Finite Diﬀerences . . . . . . . . . . . . .
444
10.10.3 Pseudo-Spectral Derivative
. . . . . . . . . . . . .
448
10.11 Transforms and Their Applications . . . . . . . . . . . . .
450

xviii
Contents
10.11.1 The Fourier Transform . . . . . . . . . . . . . . . .
450
10.11.2 (Physical) Linear Systems and Fourier Transform .
453
10.11.3 The Laplace Transform
. . . . . . . . . . . . . . .
455
10.11.4 The Z-Transform . . . . . . . . . . . . . . . . . . .
457
10.12 The Wavelet Transform . . . . . . . . . . . . . . . . . . . .
458
10.12.1 The Continuous Wavelet Transform
. . . . . . . .
458
10.12.2 Discrete and Orthonormal Wavelets
. . . . . . . .
461
10.13 Applications . . . . . . . . . . . . . . . . . . . . . . . . . .
463
10.13.1 Numerical Computation of Blackbody Radiation .
463
10.13.2 Numerical Solution of Schr¨odinger Equation . . . .
464
10.14 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . .
467
11. Numerical Solution of Ordinary Diﬀerential Equations
469
11.1
The Cauchy Problem . . . . . . . . . . . . . . . . . . . . .
469
11.2
One-Step Numerical Methods
. . . . . . . . . . . . . . . .
472
11.3
Analysis of One-Step Methods . . . . . . . . . . . . . . . .
473
11.3.1
The Zero-Stability . . . . . . . . . . . . . . . . . .
475
11.3.2
Convergence Analysis
. . . . . . . . . . . . . . . .
477
11.3.3
The Absolute Stability . . . . . . . . . . . . . . . .
479
11.4
Diﬀerence Equations
. . . . . . . . . . . . . . . . . . . . .
482
11.5
Multistep Methods
. . . . . . . . . . . . . . . . . . . . . .
487
11.5.1
Adams Methods
. . . . . . . . . . . . . . . . . . .
490
11.5.2
BDF Methods
. . . . . . . . . . . . . . . . . . . .
492
11.6
Analysis of Multistep Methods . . . . . . . . . . . . . . . .
492
11.6.1
Consistency . . . . . . . . . . . . . . . . . . . . . .
493
11.6.2
The Root Conditions . . . . . . . . . . . . . . . . .
494
11.6.3
Stability and Convergence Analysis for
Multistep Methods . . . . . . . . . . . . . . . . . .
495
11.6.4
Absolute Stability of Multistep Methods . . . . . .
499
11.7
Predictor-Corrector Methods . . . . . . . . . . . . . . . . .
502
11.8
Runge-Kutta Methods
. . . . . . . . . . . . . . . . . . . .
508
11.8.1
Derivation of an Explicit RK Method
. . . . . . .
511
11.8.2
Stepsize Adaptivity for RK Methods . . . . . . . .
512
11.8.3
Implicit RK Methods
. . . . . . . . . . . . . . . .
514
11.8.4
Regions of Absolute Stability for RK Methods
. .
516
11.9
Systems of ODEs . . . . . . . . . . . . . . . . . . . . . . .
517
11.10 StiﬀProblems . . . . . . . . . . . . . . . . . . . . . . . . .
519
11.11 Applications . . . . . . . . . . . . . . . . . . . . . . . . . .
521
11.11.1 Analysis of the Motion of a Frictionless Pendulum
522
11.11.2 Compliance of Arterial Walls . . . . . . . . . . . .
523
11.12 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . .
527
12. Two-Point Boundary Value Problems
531
12.1
A Model Problem . . . . . . . . . . . . . . . . . . . . . . .
531
12.2
Finite Diﬀerence Approximation . . . . . . . . . . . . . . .
533

Contents
xix
12.2.1
Stability Analysis by the Energy Method
. . . . .
534
12.2.2
Convergence Analysis
. . . . . . . . . . . . . . . .
538
12.2.3
Finite Diﬀerences for Two-Point Boundary
Value Problems with Variable Coeﬃcients . . . . .
540
12.3
The Spectral Collocation Method . . . . . . . . . . . . . .
542
12.4
The Galerkin Method . . . . . . . . . . . . . . . . . . . . .
544
12.4.1
Integral Formulation of Boundary-Value Problems
544
12.4.2
A Quick Introduction to Distributions . . . . . . .
546
12.4.3
Formulation and Properties of the
Galerkin Method . . . . . . . . . . . . . . . . . . .
547
12.4.4
Analysis of the Galerkin Method . . . . . . . . . .
548
12.4.5
The Finite Element Method . . . . . . . . . . . . .
550
12.4.6
Implementation Issues . . . . . . . . . . . . . . . .
556
12.4.7
Spectral Methods . . . . . . . . . . . . . . . . . . .
559
12.5
Advection-Diﬀusion Equations . . . . . . . . . . . . . . . .
560
12.5.1
Galerkin Finite Element Approximation . . . . . .
561
12.5.2
The Relationship Between Finite Elements and
Finite Diﬀerences; the Numerical Viscosity
. . . .
563
12.5.3
Stabilized Finite Element Methods . . . . . . . . .
567
12.6
A Quick Glance to the Two-Dimensional Case . . . . . . .
572
12.7
Applications . . . . . . . . . . . . . . . . . . . . . . . . . .
575
12.7.1
Lubrication of a Slider . . . . . . . . . . . . . . . .
575
12.7.2
Vertical Distribution of Spore
Concentration over Wide Regions . . . . . . . . . .
576
12.8
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . .
578
13. Parabolic and Hyperbolic Initial Boundary
Value Problems
581
13.1
The Heat Equation . . . . . . . . . . . . . . . . . . . . . .
581
13.2
Finite Diﬀerence Approximation of the Heat Equation
. .
584
13.3
Finite Element Approximation of the Heat Equation
. . .
586
13.3.1
Stability Analysis of the θ-Method . . . . . . . . .
588
13.4
Space-Time Finite Element Methods for the
Heat Equation . . . . . . . . . . . . . . . . . . . . . . . . .
593
13.5
Hyperbolic Equations: A Scalar Transport Problem . . . .
597
13.6
Systems of Linear Hyperbolic Equations
. . . . . . . . . .
599
13.6.1
The Wave Equation
. . . . . . . . . . . . . . . . .
601
13.7
The Finite Diﬀerence Method for Hyperbolic Equations . .
602
13.7.1
Discretization of the Scalar Equation . . . . . . . .
602
13.8
Analysis of Finite Diﬀerence Methods . . . . . . . . . . . .
605
13.8.1
Consistency . . . . . . . . . . . . . . . . . . . . . .
605
13.8.2
Stability . . . . . . . . . . . . . . . . . . . . . . . .
605
13.8.3
The CFL Condition
. . . . . . . . . . . . . . . . .
606
13.8.4
Von Neumann Stability Analysis . . . . . . . . . .
608
13.9
Dissipation and Dispersion . . . . . . . . . . . . . . . . . .
611

xx
Contents
13.9.1
Equivalent Equations
. . . . . . . . . . . . . . . .
614
13.10 Finite Element Approximation of Hyperbolic Equations . .
618
13.10.1 Space Discretization with Continuous and
Discontinuous Finite Elements
. . . . . . . . . . .
618
13.10.2 Time Discretization
. . . . . . . . . . . . . . . . .
620
13.11 Applications . . . . . . . . . . . . . . . . . . . . . . . . . .
623
13.11.1 Heat Conduction in a Bar . . . . . . . . . . . . . .
623
13.11.2 A Hyperbolic Model for Blood Flow
Interaction with Arterial Walls . . . . . . . . . . .
623
13.12 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . .
625
References
627
Index of MATLAB Programs
643
Index
647

1
Foundations of Matrix Analysis
In this chapter we recall the basic elements of linear algebra which will be
employed in the remainder of the text. For most of the proofs as well as
for the details, the reader is referred to [Bra75], [Nob69], [Hal58]. Further
results on eigenvalues can be found in [Hou75] and [Wil65].
1.1
Vector Spaces
Deﬁnition 1.1 A vector space over the numeric ﬁeld K (K = R or K = C)
is a nonempty set V , whose elements are called vectors and in which two
operations are deﬁned, called addition and scalar multiplication, that enjoy
the following properties:
1. addition is commutative and associative;
2. there exists an element 0 ∈V (the zero vector or null vector) such
that v + 0 = v for each v ∈V ;
3. 0 · v = 0, 1 · v = v, where 0 and 1 are respectively the zero and the
unity of K;
4. for each element v ∈V there exists its opposite, −v, in V such that
v + (−v) = 0;

2
1. Foundations of Matrix Analysis
5. the following distributive properties hold
∀α ∈K, ∀v, w ∈V, α(v + w) = αv + αw,
∀α, β ∈K, ∀v ∈V, (α + β)v = αv + βv;
6. the following associative property holds
∀α, β ∈K, ∀v ∈V, (αβ)v = α(βv).
■
Example 1.1 Remarkable instances of vector spaces are:
- V = Rn (respectively V = Cn): the set of the n-tuples of real (respectively
complex) numbers, n ≥1;
- V = Pn: the set of polynomials pn(x) = n
k=0 akxk with real (or complex)
coeﬃcients ak having degree less than or equal to n, n ≥0;
- V = Cp([a, b]): the set of real (or complex)-valued functions which are con-
tinuous on [a, b] up to their p-th derivative, 0 ≤p < ∞.
•
Deﬁnition 1.2 We say that a nonempty part W of V is a vector subspace
of V iﬀW is a vector space over K.
■
Example 1.2 The vector space Pn is a vector subspace of C∞(R), which is the
space of inﬁnite continuously diﬀerentiable functions on the real line. A trivial
subspace of any vector space is the one containing only the zero vector.
•
In particular, the set W of the linear combinations of a system of p vectors
of V , {v1, . . . , vp}, is a vector subspace of V , called the generated subspace
or span of the vector system, and is denoted by
W
= span {v1, . . . , vp}
= {v = α1v1 + . . . + αpvp
with αi ∈K, i = 1, . . . , p} .
The system {v1, . . . , vp} is called a system of generators for W.
If W1, . . . , Wm are vector subspaces of V , then the set
S = {w : w = v1 + . . . + vm with vi ∈Wi, i = 1, . . . , m}
is also a vector subspace of V . We say that S is the direct sum of the
subspaces Wi if any element s ∈S admits a unique representation of the
form s = v1 + . . . + vm with vi ∈Wi and i = 1, . . . , m. In such a case, we
shall write S = W1 ⊕. . . ⊕Wm.

1.2 Matrices
3
Deﬁnition 1.3 A system of vectors {v1, . . . , vm} of a vector space V is
called linearly independent if the relation
α1v1 + α2v2 + . . . + αmvm = 0
with α1, α2, . . . , αm ∈K implies that α1 = α2 = . . . = αm = 0. Otherwise,
the system will be called linearly dependent.
■
We call a basis of V any system of linearly independent generators of V .
If {u1, . . . , un} is a basis of V , the expression v = v1u1 + . . . + vnun is
called the decomposition of v with respect to the basis and the scalars
v1, . . . , vn ∈K are the components of v with respect to the given basis.
Moreover, the following property holds.
Property 1.1 Let V be a vector space which admits a basis of n vectors.
Then every system of linearly independent vectors of V has at most n el-
ements and any other basis of V has n elements. The number n is called
the dimension of V and we write dim(V ) = n.
If, instead, for any n there always exist n linearly independent vectors of
V , the vector space is called inﬁnite dimensional.
Example 1.3 For any integer p the space Cp([a, b]) is inﬁnite dimensional. The
spaces Rn and Cn have dimension equal to n. The usual basis for Rn is the set of
unit vectors {e1, . . . , en} where (ei)j = δij for i, j = 1, . . . n, where δij denotes
the Kronecker symbol equal to 0 if i ̸= j and 1 if i = j. This choice is of course
not the only one that is possible (see Exercise 2).
•
1.2
Matrices
Let m and n be two positive integers. We call a matrix having m rows and
n columns, or a matrix m × n, or a matrix (m, n), with elements in K, a
set of mn scalars aij ∈K, with i = 1, . . . , m and j = 1, . . . n, represented
in the following rectangular array
A =


a11
a12
. . .
a1n
a21
a22
. . .
a2n
...
...
...
am1
am2
. . .
amn

.
(1.1)
When K = R or K = C we shall respectively write A ∈Rm×n or A ∈
Cm×n, to explicitly outline the numerical ﬁelds which the elements of A
belong to. Capital letters will be used to denote the matrices, while the
lower case letters corresponding to those upper case letters will denote the
matrix entries.

4
1. Foundations of Matrix Analysis
We shall abbreviate (1.1) as A = (aij) with i = 1, . . . , m and j = 1, . . . n.
The index i is called row index, while j is the column index. The set
(ai1, ai2, . . . , ain) is called the i-th row of A; likewise, (a1j, a2j, . . . , amj)
is the j-th column of A.
If n = m the matrix is called squared or having order n and the set of
the entries (a11, a22, . . . , ann) is called its main diagonal.
A matrix having one row or one column is called a row vector or column
vector respectively. Unless otherwise speciﬁed, we shall always assume that
a vector is a column vector. In the case n = m = 1, the matrix will simply
denote a scalar of K.
Sometimes it turns out to be useful to distinguish within a matrix the set
made up by speciﬁed rows and columns. This prompts us to introduce the
following deﬁnition.
Deﬁnition 1.4 Let A be a matrix m × n. Let 1 ≤i1 < i2 < . . . < ik ≤m
and 1 ≤j1 < j2 < . . . < jl ≤n two sets of contiguous indexes. The matrix
S(k × l) of entries spq = aipjq with p = 1, . . . , k, q = 1, . . . , l is called a
submatrix of A. If k = l and ir = jr for r = 1, . . . , k, S is called a principal
submatrix of A.
■
Deﬁnition 1.5 A matrix A(m × n) is called block partitioned or said to
be partitioned into submatrices if
A =


A11
A12
. . .
A1l
A21
A22
. . .
A2l
...
...
...
...
Ak1
Ak2
. . .
Akl

,
where Aij are submatrices of A.
■
Among the possible partitions of A, we recall in particular the partition by
columns
A = (a1, a2, . . . , an),
ai being the i-th column vector of A. In a similar way the partition by rows
of A can be deﬁned. To ﬁx the notations, if A is a matrix m × n, we shall
denote by
A(i1 : i2, j1 : j2) = (aij)
i1 ≤i ≤i2, j1 ≤j ≤j2
the submatrix of A of size (i2 −i1 + 1) × (j2 −j1 + 1) that lies between the
rows i1 and i2 and the columns j1 and j2. Likewise, if v is a vector of size
n, we shall denote by v(i1 : i2) the vector of size i2 −i1 + 1 made up by
the i1-th to the i2-th components of v.
These notations are convenient in view of programming the algorithms
that will be presented throughout the volume in the MATLAB language.

1.3 Operations with Matrices
5
1.3
Operations with Matrices
Let A = (aij) and B = (bij) be two matrices m × n over K. We say that
A is equal to B, if aij = bij for i = 1, . . . , m, j = 1, . . . , n. Moreover, we
deﬁne the following operations:
- matrix sum: the matrix sum is the matrix A+B = (aij +bij). The neutral
element in a matrix sum is the null matrix, still denoted by 0 and
made up only by null entries;
- matrix multiplication by a scalar: the multiplication of A by λ ∈K, is a
matrix λA = (λaij);
- matrix product: the product of two matrices A and B of sizes (m, p)
and (p, n) respectively, is a matrix C(m, n) whose entries are cij =
p

k=1
aikbkj, for i = 1, . . . , m, j = 1, . . . , n.
The matrix product is associative and distributive with respect to the ma-
trix sum, but it is not in general commutative. The square matrices for
which the property AB = BA holds, will be called commutative.
In the case of square matrices, the neutral element in the matrix product
is a square matrix of order n called the unit matrix of order n or, more
frequently, the identity matrix given by In = (δij). The identity matrix
is, by deﬁnition, the only matrix n × n such that AIn = InA = A for all
square matrices A. In the following we shall omit the subscript n unless it
is strictly necessary. The identity matrix is a special instance of a diagonal
matrix of order n, that is, a square matrix of the type D = (diiδij). We will
use in the following the notation D = diag(d11, d22, . . . , dnn).
Finally, if A is a square matrix of order n and p is an integer, we deﬁne Ap
as the product of A with itself iterated p times. We let A0 = I.
Let us now address the so-called elementary row operations that can be
performed on a matrix. They consist of:
- multiplying the i-th row of a matrix by a scalar α; this operation is
equivalent to pre-multiplying A by the matrix D = diag(1, . . . , 1, α,
1, . . . , 1), where α occupies the i-th position;
- exchanging the i-th and j-th rows of a matrix; this can be done by pre-
multiplying A by the matrix P(i,j) of elements
p(i,j)
rs
=









1
if r = s = 1, . . . , i −1, i + 1, . . . , j −1, j + 1, . . . n,
1
if r = j, s = i or r = i, s = j,
0
otherwise,
(1.2)

6
1. Foundations of Matrix Analysis
where Ir denotes the identity matrix of order r = j −i −1 if j >
i (henceforth, matrices with size equal to zero will correspond to
the empty set). Matrices like (1.2) are called elementary permutation
matrices. The product of elementary permutation matrices is called
a permutation matrix, and it performs the row exchanges associated
with each elementary permutation matrix. In practice, a permutation
matrix is a reordering by rows of the identity matrix;
- adding α times the j-th row of a matrix to its i-th row. This operation
can also be performed by pre-multiplying A by the matrix I + N(i,j)
α
,
where N(i,j)
α
is a matrix having null entries except the one in position
i, j whose value is α.
1.3.1
Inverse of a Matrix
Deﬁnition 1.6 A square matrix A of order n is called invertible (or regular
or nonsingular) if there exists a square matrix B of order n such that
A B = B A = I. B is called the inverse matrix of A and is denoted by A−1.
A matrix which is not invertible is called singular.
■
If A is invertible its inverse is also invertible, with (A−1)−1 = A. Moreover,
if A and B are two invertible matrices of order n, their product AB is also
invertible, with (A B)−1 = B−1A−1. The following property holds.
Property 1.2 A square matrix is invertible iﬀits column vectors are lin-
early independent.
Deﬁnition 1.7 We call the transpose of a matrix A∈Rm×n the matrix
n × m, denoted by AT , that is obtained by exchanging the rows of A with
the columns of A.
■
Clearly, (AT )T = A, (A + B)T = AT + BT , (AB)T = BT AT and (αA)T =
αAT ∀α ∈R. If A is invertible, then also (AT )−1 = (A−1)T = A−T .
Deﬁnition 1.8 Let A ∈Cm×n; the matrix B = AH ∈Cn×m is called the
conjugate transpose (or adjoint) of A if bij = ¯aji, where ¯aji is the complex
conjugate of aji.
■
In analogy with the case of the real matrices, it turns out that (A+B)H =
AH + BH, (AB)H = BHAH and (αA)H = ¯αAH ∀α ∈C.
Deﬁnition 1.9 A matrix A ∈Rn×n is called symmetric if A = AT , while
it is antisymmetric if A = −AT . Finally, it is called orthogonal if AT A =
AAT = I, that is A−1 = AT .
■
Permutation matrices are orthogonal and the same is true for their prod-
ucts.

1.3 Operations with Matrices
7
Deﬁnition 1.10 A matrix A ∈Cn×n is called hermitian or self-adjoint if
AT = ¯A, that is, if AH = A, while it is called unitary if AHA = AAH = I.
Finally, if AAH = AHA, A is called normal.
■
As a consequence, a unitary matrix is one such that A−1 = AH.
Of course, a unitary matrix is also normal, but it is not in general her-
mitian. For instance, the matrix of the Example 1.4 is unitary, although
not symmetric (if s ̸= 0). We ﬁnally notice that the diagonal entries of an
hermitian matrix must necessarily be real (see also Exercise 5).
1.3.2
Matrices and Linear Mappings
Deﬁnition 1.11 A linear map from Cn into Cm is a function f : Cn −→
Cm such that f(αx + βy) = αf(x) + βf(y), ∀α, β ∈K and ∀x, y ∈Cn. ■
The following result links matrices and linear maps.
Property 1.3 Let f : Cn −→Cm be a linear map. Then, there exists a
unique matrix Af ∈Cm×n such that
f(x) = Afx
∀x ∈Cn.
(1.3)
Conversely, if Af ∈Cm×n then the function deﬁned in (1.3) is a linear
map from Cn into Cm.
Example 1.4 An important example of a linear map is the counterclockwise
rotation by an angle ϑ in the plane (x1, x2). The matrix associated with such a
map is given by
G(ϑ) =

c
s
−s
c

,
c = cos(ϑ), s = sin(ϑ)
and it is called a rotation matrix.
•
1.3.3
Operations with Block-Partitioned Matrices
All the operations that have been previously introduced can be extended
to the case of a block-partitioned matrix A, provided that the size of each
single block is such that any single matrix operation is well-deﬁned.
Indeed, the following result can be shown (see, e.g., [Ste73]).
Property 1.4 Let A and B be the block matrices
A =


A11
. . .
A1l
...
...
...
Ak1
. . .
Akl

,
B =


B11
. . .
B1n
...
...
...
Bm1
. . .
Bmn


where Aij and Bij are matrices (ki × lj) and (mi × nj). Then we have

8
1. Foundations of Matrix Analysis
1.
λA =


λA11
. . .
λA1l
...
...
...
λAk1
. . .
λAkl

,
λ ∈C;
AT =


AT
11
. . .
AT
k1
...
...
...
AT
1l
. . .
AT
kl

;
2. if k = m, l = n, mi = ki and nj = lj, then
A + B =


A11 + B11
. . .
A1l + B1l
...
...
...
Ak1 + Bk1
. . .
Akl + Bkl

;
3. if l = m, li = mi and ki = ni, then, letting Cij =
m

s=1
AisBsj,
AB =


C11
. . .
C1l
...
...
...
Ck1
. . .
Ckl

.
1.4
Trace and Determinant of a Matrix
Let us consider a square matrix A of order n. The trace of a matrix is the
sum of the diagonal entries of A, that is tr(A) =
n

i=1
aii.
We call the determinant of A the scalar deﬁned through the following for-
mula
det(A) =

π∈P
sign(π)a1π1a2π2 . . . anπn,
where P =

π = (π1, . . . , πn)T 
is the set of the n! vectors that are ob-
tained by permuting the index vector i = (1, . . . , n)T and sign(π) equal to
1 (respectively, −1) if an even (respectively, odd) number of exchanges is
needed to obtain π from i.
The following properties hold
det(A) = det(AT ),
det(AB) = det(A)det(B),
det(A−1) = 1/det(A),
det(AH) = det(A),
det(αA) = αndet(A), ∀α ∈K.
Moreover, if two rows or columns of a matrix coincide, the determinant
vanishes, while exchanging two rows (or two columns) produces a change

1.5 Rank and Kernel of a Matrix
9
of sign in the determinant. Of course, the determinant of a diagonal matrix
is the product of the diagonal entries.
Denoting by Aij the matrix of order n −1 obtained from A by elimi-
nating the i-th row and the j-th column, we call the complementary minor
associated with the entry aij the determinant of the matrix Aij. We call
the k-th principal (dominating) minor of A, dk, the determinant of the
principal submatrix of order k, Ak = A(1 : k, 1 : k). If we denote by
∆ij = (−1)i+jdet(Aij) the cofactor of the entry aij, the actual computa-
tion of the determinant of A can be performed using the following recursive
relation
det(A) =









a11
if n = 1,
n

j=1
∆ijaij,
for n > 1,
(1.4)
which is known as the Laplace rule. If A is a square invertible matrix of
order n, then
A−1 =
1
det(A)C
where C is the matrix having entries ∆ji, i, j = 1, . . . , n.
As a consequence, a square matrix is invertible iﬀits determinant is non-
vanishing. In the case of nonsingular diagonal matrices the inverse is still
a diagonal matrix having entries given by the reciprocals of the diagonal
entries of the matrix.
Every orthogonal matrix is invertible, its inverse is given by AT , moreover
det(A) = ±1.
1.5
Rank and Kernel of a Matrix
Let A be a rectangular matrix m × n. We call the determinant of order
q (with q ≥1) extracted from matrix A, the determinant of any square
matrix of order q obtained from A by eliminating m −q rows and n −q
columns.
Deﬁnition 1.12 The rank of A (denoted by rank(A)) is the maximum
order of the nonvanishing determinants extracted from A. A matrix has
complete or full rank if rank(A) = min(m,n).
■
Notice that the rank of A represents the maximum number of linearly
independent column vectors of A that is, the dimension of the range of A,
deﬁned as
range(A) = {y ∈Rm : y = Ax for x ∈Rn} .
(1.5)

10
1. Foundations of Matrix Analysis
Rigorously speaking, one should distinguish between the column rank of A
and the row rank of A, the latter being the maximum number of linearly
independent row vectors of A. Nevertheless, it can be shown that the row
rank and column rank do actually coincide.
The kernel of A is deﬁned as the subspace
ker(A) = {x ∈Rn : Ax = 0} .
The following relations hold
1.
rank(A) = rank(AT )
(if A ∈Cm×n, rank(A) = rank(AH))
2.
rank(A) + dim(ker(A)) = n.
In general, dim(ker(A)) ̸= dim(ker(AT )). If A is a nonsingular square ma-
trix, then rank(A) = n and dim(ker(A)) = 0.
Example 1.5 Let
A =
 1
1
0
1
−1
1

.
Then, rank(A) = 2, dim(ker(A)) = 1 and dim(ker(AT )) = 0.
•
We ﬁnally notice that for a matrix A ∈Cn×n the following properties are
equivalent:
1. A is nonsingular;
2. det(A) ̸= 0;
3. ker(A) = {0};
4. rank(A) = n;
5. A has linearly independent rows and columns.
1.6
Special Matrices
1.6.1
Block Diagonal Matrices
These are matrices of the form D = diag(D1, . . . , Dn), where Di are square
matrices with i = 1, . . . , n. Clearly, each single diagonal block can be of
diﬀerent size. We shall say that a block diagonal matrix has size n if n
is the number of its diagonal blocks. The determinant of a block diagonal
matrix is given by the product of the determinants of the single diagonal
blocks.

1.6 Special Matrices
11
1.6.2
Trapezoidal and Triangular Matrices
A matrix A(m × n) is called upper trapezoidal if aij = 0 for i > j, while it
is lower trapezoidal if aij = 0 for i < j. The name is due to the fact that,
in the case of upper trapezoidal matrices, with m < n, the nonzero entries
of the matrix form a trapezoid.
A triangular matrix is a square trapezoidal matrix of order n of the form
L =


l11
0
. . .
0
l21
l22
. . .
0
...
...
...
ln1
ln2
. . .
lnn


or
U =


u11
u12
. . .
u1n
0
u22
. . .
u2n
...
...
...
0
0
. . .
unn

.
The matrix L is called lower triangular while U is upper triangular.
Let us recall some algebraic properties of triangular matrices that are easy
to check.
- The determinant of a triangular matrix is the product of the diagonal
entries;
- the inverse of a lower (respectively, upper) triangular matrix is still lower
(respectively, upper) triangular;
- the product of two lower triangular (respectively, upper trapezoidal) ma-
trices is still lower triangular (respectively, upper trapezodial);
- if we call unit triangular matrix a triangular matrix that has diagonal
entries equal to 1, then, the product of lower (respectively, upper) unit
triangular matrices is still lower (respectively, upper) unit triangular.
1.6.3
Banded Matrices
The matrices introduced in the previous section are a special instance of
banded matrices. Indeed, we say that a matrix A ∈Rm×n (or in Cm×n)
has lower band p if aij = 0 when i > j + p and upper band q if aij = 0
when j > i+q. Diagonal matrices are banded matrices for which p = q = 0,
while trapezoidal matrices have p = m−1, q = 0 (lower trapezoidal), p = 0,
q = n −1 (upper trapezoidal).
Other banded matrices of relevant interest are the tridiagonal matrices
for which p = q = 1 and the upper bidiagonal (p = 0, q = 1) or lower bidiag-
onal (p = 1, q = 0). In the following, tridiagn(b, d, c) will denote the triadi-
agonal matrix of size n having respectively on the lower and upper principal
diagonals the vectors b = (b1, . . . , bn−1)T and c = (c1, . . . , cn−1)T , and on
the principal diagonal the vector d = (d1, . . . , dn)T . If bi = β, di = δ and
ci = γ, β, δ and γ being given constants, the matrix will be denoted by
tridiagn(β, δ, γ).

12
1. Foundations of Matrix Analysis
We also mention the so-called lower Hessenberg matrices (p = m −1,
q = 1) and upper Hessenberg matrices (p = 1, q = n −1) that have the
following structure
H =


h11
h12
0
h21
h22
...
...
...
hm−1n
hm1
. . .
. . .
hmn


or H =


h11
h12
. . .
h1n
h21
h22
h2n
...
...
...
0
hmn−1
hmn


.
Matrices of similar shape can obviously be set up in the block-like format.
1.7
Eigenvalues and Eigenvectors
Let A be a square matrix of order n with real or complex entries; the number
λ ∈C is called an eigenvalue of A if there exists a nonnull vector x ∈Cn
such that Ax = λx. The vector x is the eigenvector associated with the
eigenvalue λ and the set of the eigenvalues of A is called the spectrum of A,
denoted by σ(A). We say that x and y are respectively a right eigenvector
and a left eigenvector of A, associated with the eigenvalue λ, if
Ax = λx,
yHA = λyH.
The eigenvalue λ corresponding to the eigenvector x can be determined by
computing the Rayleigh quotient λ = xHAx/(xHx). The number λ is the
solution of the characteristic equation
pA(λ) = det(A −λI) = 0,
where pA(λ) is the characteristic polynomial. Since this latter is a polyno-
mial of degree n with respect to λ, there certainly exist n eigenvalues of A
not necessarily distinct. The following properties can be proved
det(A) =
n

i=1
λi,
tr(A) =
n

i=1
λi,
(1.6)
and since det(AT −λI) = det((A −λI)T ) = det(A −λI) one concludes that
σ(A) = σ(AT ) and, in an analogous way, that σ(AH) = σ(¯A).
From the ﬁrst relation in (1.6) it can be concluded that a matrix is
singular iﬀit has at least one null eigenvalue, since pA(0) = det(A) =
Πn
i=1λi.
Secondly, if A has real entries, pA(λ) turns out to be a real-coeﬃcient
polynomial so that complex eigenvalues of A shall necessarily occur in com-
plex conjugate pairs.

1.7 Eigenvalues and Eigenvectors
13
Finally, due to the Cayley-Hamilton Theorem if pA(λ) is the charac-
teristic polynomial of A, then pA(A) = 0, where pA(A) denotes a matrix
polynomial (for the proof see, e.g., [Axe94], p. 51).
The maximum module of the eigenvalues of A is called the spectral radius
of A and is denoted by
ρ(A) = max
λ∈σ(A)|λ|.
(1.7)
Characterizing the eigenvalues of a matrix as the roots of a polynomial
implies in particular that λ is an eigenvalue of A ∈Cn×n iﬀ¯λ is an eigen-
value of AH. An immediate consequence is that ρ(A) = ρ(AH). Moreover,
∀A ∈Cn×n, ∀α ∈C, ρ(αA) = |α|ρ(A), and ρ(Ak) = [ρ(A)]k ∀k ∈N.
Finally, assume that A is a block triangular matrix
A =


A11
A12
. . .
A1k
0
A22
. . .
A2k
...
...
...
0
. . .
0
Akk

.
As pA(λ) = pA11(λ)pA22(λ) · · · pAkk (λ), the spectrum of A is given by the
union of the spectra of each single diagonal block. As a consequence, if A
is triangular, the eigenvalues of A are its diagonal entries.
For each eigenvalue λ of a matrix A the set of the eigenvectors associated
with λ, together with the null vector, identiﬁes a subspace of Cn which is
called the eigenspace associated with λ and corresponds by deﬁnition to
ker(A-λI). The dimension of the eigenspace is
dim [ker(A −λI)] = n −rank(A −λI),
and is called geometric multiplicity of the eigenvalue λ. It can never be
greater than the algebraic multiplicity of λ, which is the multiplicity of
λ as a root of the characteristic polynomial. Eigenvalues having geometric
multiplicity strictly less than the algebraic one are called defective. A matrix
having at least one defective eigenvalue is called defective.
The eigenspace associated with an eigenvalue of a matrix A is invariant
with respect to A in the sense of the following deﬁnition.
Deﬁnition 1.13 A subspace S in Cn is called invariant with respect to a
square matrix A if AS ⊂S, where AS is the transformed of S through A.
■

14
1. Foundations of Matrix Analysis
1.8
Similarity Transformations
Deﬁnition 1.14 Let C be a square nonsingular matrix having the same
order as the matrix A. We say that the matrices A and C−1AC are similar,
and the transformation from A to C−1AC is called a similarity transfor-
mation. Moreover, we say that the two matrices are unitarily similar if C
is unitary.
■
Two similar matrices share the same spectrum and the same characteris-
tic polynomial. Indeed, it is easy to check that if (λ, x) is an eigenvalue-
eigenvector pair of A, (λ, C−1x) is the same for the matrix C−1AC since
(C−1AC)C−1x = C−1Ax = λC−1x.
We notice in particular that the product matrices AB and BA, with A ∈
Cn×m and B ∈Cm×n, are not similar but satisfy the following property
(see [Hac94], p.18, Theorem 2.4.6)
σ(AB)\ {0} = σ(BA)\ {0}
that is, AB and BA share the same spectrum apart from null eigenvalues
so that ρ(AB) = ρ(BA).
The use of similarity transformations aims at reducing the complexity
of the problem of evaluating the eigenvalues of a matrix. Indeed, if a given
matrix could be transformed into a similar matrix in diagonal or triangular
form, the computation of the eigenvalues would be immediate. The main
result in this direction is the following theorem (for the proof, see [Dem97],
Theorem 4.2).
Property 1.5 (Schur decomposition) Given A∈Cn×n, there exists U
unitary such that
U−1AU = UHAU =


λ1
b12
. . .
b1n
0
λ2
b2n
...
...
...
0
. . .
0
λn

= T,
where λi are the eigenvalues of A.
It thus turns out that every matrix A is unitarily similar to an upper
triangular matrix. The matrices T and U are not necessarily unique [Hac94].
The Schur decomposition theorem gives rise to several important results;
among them, we recall:
1. every hermitian matrix is unitarily similar to a diagonal real ma-
trix, that is, when A is hermitian every Schur decomposition of A is
diagonal. In such an event, since
U−1AU = Λ = diag(λ1, . . . , λn),

1.8 Similarity Transformations
15
it turns out that AU = UΛ, that is, Aui = λiui for i = 1, . . . , n so
that the column vectors of U are the eigenvectors of A. Moreover,
since the eigenvectors are orthogonal two by two, it turns out that
an hermitian matrix has a system of orthonormal eigenvectors that
generates the whole space Cn. Finally, it can be shown that a matrix
A of order n is similar to a diagonal matrix D iﬀthe eigenvectors of
A form a basis for Cn [Axe94];
2. a matrix A ∈Cn×n is normal iﬀit is unitarily similar to a diagonal
matrix. As a consequence, a normal matrix A ∈Cn×n admits the
following spectral decomposition: A = UΛUH = n
i=1 λiuiuH
i
being
U unitary and Λ diagonal [SS90];
3. let A and B be two normal and commutative matrices; then, the
generic eigenvalue µi of A+B is given by the sum λi + ξi, where
λi and ξi are the eigenvalues of A and B associated with the same
eigenvector.
There are, of course, nonsymmetric matrices that are similar to diagonal
matrices, but these are not unitarily similar (see, e.g., Exercise 7).
The Schur decomposition can be improved as follows (for the proof see,
e.g., [Str80], [God66]).
Property 1.6 (Canonical Jordan Form) Let A be any square matrix.
Then, there exists a nonsingular matrix X which transforms A into a block
diagonal matrix J such that
X−1AX = J = diag (Jk1(λ1), Jk2(λ2), . . . , Jkl(λl)) ,
which is called canonical Jordan form, λj being the eigenvalues of A and
Jk(λ) ∈Ck×k a Jordan block of the form J1(λ) = λ if k = 1 and
Jk(λ) =


λ
1
0
. . .
0
0
λ
1
· · ·
...
...
...
...
1
0
...
...
λ
1
0
. . .
. . .
0
λ


,
for k > 1.
If an eigenvalue is defective, the size of the corresponding Jordan block
is greater than one. Therefore, the canonical Jordan form tells us that a
matrix can be diagonalized by a similarity transformation iﬀit is nonde-
fective. For this reason, the nondefective matrices are called diagonalizable.
In particular, normal matrices are diagonalizable.

16
1. Foundations of Matrix Analysis
Partitioning X by columns, X = (x1, . . . , xn), it can be seen that the
ki vectors associated with the Jordan block Jki(λi) satisfy the following
recursive relation
Axl = λixl,
l =
i−1

j=1
mj + 1,
Axj = λixj + xj−1,
j = l + 1, . . . , l −1 + ki, if ki ̸= 1.
(1.8)
The vectors xi are called principal vectors or generalized eigenvectors of A.
Example 1.6 Let us consider the following matrix
A =


7/4
3/4
−1/4
−1/4
−1/4
1/4
0
2
0
0
0
0
−1/2
−1/2
5/2
1/2
−1/2
1/2
−1/2
−1/2
−1/2
5/2
1/2
1/2
−1/4
−1/4
−1/4
−1/4
11/4
1/4
−3/2
−1/2
−1/2
1/2
1/2
7/2


.
The Jordan canonical form of A and its associated matrix X are given by
J =


2
1
0
0
0
0
0
2
0
0
0
0
0
0
3
1
0
0
0
0
0
3
1
0
0
0
0
0
3
0
0
0
0
0
0
2


,
X =


1
0
0
0
0
1
0
1
0
0
0
1
0
0
1
0
0
1
0
0
0
1
0
1
0
0
0
0
1
1
1
1
1
1
1
1


.
Notice that two diﬀerent Jordan blocks are related to the same eigenvalue (λ =
2). It is easy to check property (1.8). Consider, for example, the Jordan block
associated with the eigenvalue λ2 = 3; we have
Ax3 = [0 0 3 0 0 3]T = 3 [0 0 1 0 0 1]T = λ2x3,
Ax4 = [0 0 1 3 0 4]T = 3 [0 0 0 1 0 1]T + [0 0 1 0 0 1]T = λ2x4 + x3,
Ax5 = [0 0 0 1 3 4]T = 3 [0 0 0 0 1 1]T + [0 0 0 1 0 1]T = λ2x5 + x4.
•
1.9
The Singular Value Decomposition (SVD)
Any matrix can be reduced in diagonal form by a suitable pre and post-
multiplication by unitary matrices. Precisely, the following result holds.
Property 1.7 Let A∈Cm×n. There exist two unitary matrices U∈Cm×m
and V∈Cn×n such that
UHAV = Σ = diag(σ1, . . . , σp) ∈Cm×n
with p = min(m, n)
(1.9)
and σ1 ≥. . . ≥σp ≥0. Formula (1.9) is called Singular Value Decompo-
sition or (SVD) of A and the numbers σi (or σi(A)) are called singular
values of A.

1.10 Scalar Product and Norms in Vector Spaces
17
If A is a real-valued matrix, U and V will also be real-valued and in (1.9)
UT must be written instead of UH. The following characterization of the
singular values holds
σi(A) =

λi(AHA),
i = 1, . . . , n.
(1.10)
Indeed, from (1.9) it follows that A = UΣVH, AH = VΣUH so that, U and
V being unitary, AHA = VΣ2VH, that is, λi(AHA) = λi(Σ2) = (σi(A))2.
Since AAH and AHA are hermitian matrices, the columns of U, called the
left singular vectors of A, turn out to be the eigenvectors of AAH (see
Section 1.8) and, therefore, they are not uniquely deﬁned. The same holds
for the columns of V, which are the right singular vectors of A.
Relation (1.10) implies that if A ∈Cn×n is hermitian with eigenvalues given
by λ1, λ2, . . . , λn, then the singular values of A coincide with the modules
of the eigenvalues of A. Indeed because AAH = A2, σi =

λ2
i = |λi| for
i = 1, . . . , n. As far as the rank is concerned, if
σ1 ≥. . . ≥σr > σr+1 = . . . = σp = 0,
then the rank of A is r, the kernel of A is the span of the column vectors
of V, {vr+1, . . . , vn}, and the range of A is the span of the column vectors
of U, {u1, . . . , ur}.
Deﬁnition 1.15 Suppose that A∈Cm×n has rank equal to r and that it
admits a SVD of the type UHAV = Σ. The matrix A† = VΣ†UH is called
the Moore-Penrose pseudo-inverse matrix, being
Σ† = diag
 1
σ1
, . . . , 1
σr
, 0, . . . , 0

.
(1.11)
■
The matrix A† is also called the generalized inverse of A (see Exercise 13).
Indeed, if rank(A) = n < m, then A† = (AT A)−1AT , while if n = m =
rank(A), A† = A−1. For further properties of A†, see also Exercise 12.
1.10
Scalar Product and Norms in Vector Spaces
Very often, to quantify errors or measure distances one needs to compute
the magnitude of a vector or a matrix. For that purpose we introduce in
this section the concept of a vector norm and, in the following one, of a
matrix norm. We refer the reader to [Ste73], [SS90] and [Axe94] for the
proofs of the properties that are reported hereafter.

18
1. Foundations of Matrix Analysis
Deﬁnition 1.16 A scalar product on a vector space V deﬁned over K
is any map (·, ·) acting from V × V into K which enjoys the following
properties:
1. it is linear with respect to the vectors of V, that is
(γx + λz, y) = γ(x, y) + λ(z, y), ∀x, z ∈V, ∀γ, λ ∈K;
2. it is hermitian, that is, (y, x) = (x, y), ∀x, y ∈V ;
3. it is positive deﬁnite, that is, (x, x) > 0, ∀x ̸= 0 (in other words,
(x, x) ≥0, and (x, x) = 0 if and only if x = 0).
■
In the case V = Cn (or Rn), an example is provided by the classical Eu-
clidean scalar product given by
(x, y) = yHx =
n

i=1
xi¯yi,
where ¯z denotes the complex conjugate of z.
Moreover, for any given square matrix A of order n and for any x, y∈Cn
the following relation holds
(Ax, y) = (x, AHy).
(1.12)
In particular, since for any matrix Q ∈Cn×n, (Qx, Qy) = (x, QHQy), one
gets
Property 1.8 Unitary matrices preserve the Euclidean scalar product, that
is, (Qx, Qy) = (x, y) for any unitary matrix Q and for any pair of vectors
x and y.
Deﬁnition 1.17 Let V be a vector space over K. We say that the map
∥· ∥from V into R is a norm on V if the following axioms are satisﬁed:
1. (i) ∥v∥≥0 ∀v ∈V and (ii) ∥v∥= 0 if and only if v = 0;
2. ∥αv∥= |α|∥v∥∀α ∈K, ∀v ∈V (homogeneity property);
3. ∥v + w∥≤∥v∥+ ∥w∥∀v, w ∈V (triangular inequality),
where |α| denotes the absolute value of α if K = R, the module of α if
K = C.
■

1.10 Scalar Product and Norms in Vector Spaces
19
The pair (V, ∥· ∥) is called a normed space. We shall distinguish among
norms by a suitable subscript at the margin of the double bar symbol. In
the case the map | · | from V into R enjoys only the properties 1(i), 2 and
3 we shall call such a map a seminorm. Finally, we shall call a unit vector
any vector of V having unit norm.
An example of a normed space is Rn, equipped for instance by the p-norm
(or H¨older norm); this latter is deﬁned for a vector x of components {xi}
as
∥x∥p =
 n

i=1
|xi|p
1/p
,
for 1 ≤p < ∞.
(1.13)
Notice that the limit as p goes to inﬁnity of ∥x∥p exists, is ﬁnite, and equals
the maximum module of the components of x. Such a limit deﬁnes in turn
a norm, called the inﬁnity norm (or maximum norm), given by
∥x∥∞= max
1≤i≤n|xi|.
When p = 2, from (1.13) the standard deﬁnition of Euclidean norm is
recovered
∥x∥2 = (x, x)1/2 =
 n

i=1
|xi|2
1/2
=

xT x
1/2 ,
for which the following property holds.
Property 1.9 (Cauchy-Schwarz inequality) For any pair x, y ∈Rn,
|(x, y)| = |xT y| ≤∥x∥2 ∥y∥2,
(1.14)
where strict equality holds iﬀy = αx for some α ∈R.
We recall that the scalar product in Rn can be related to the p-norms
introduced over Rn in (1.13) by the H¨older inequality
|(x, y)| ≤∥x∥p∥y∥q,
with 1
p + 1
q = 1.
In the case where V is a ﬁnite-dimensional space the following property
holds (for a sketch of the proof, see Exercise 14).
Property 1.10 Any vector norm ∥·∥deﬁned on V is a continuous function
of its argument, namely, ∀ε > 0, ∃C > 0 such that if ∥x −x∥≤ε then
| ∥x∥−∥x∥| ≤Cε, for any x, x ∈V .
New norms can be easily built using the following result.

20
1. Foundations of Matrix Analysis
Property 1.11 Let ∥· ∥be a norm of Rn and A ∈Rn×n be a matrix with
n linearly independent columns. Then, the function ∥· ∥A2 acting from Rn
into R deﬁned as
∥x∥A2 = ∥Ax∥
∀x ∈Rn,
is a norm of Rn.
Two vectors x, y in V are said to be orthogonal if (x, y) = 0. This statement
has an immediate geometric interpretation when V = R2 since in such a
case
(x, y) = ∥x∥2∥y∥2 cos(ϑ),
where ϑ is the angle between the vectors x and y. As a consequence, if
(x, y) = 0 then ϑ is a right angle and the two vectors are orthogonal in the
geometric sense.
Deﬁnition 1.18 Two norms ∥· ∥p and ∥· ∥q on V are equivalent if there
exist two positive constants cpq and Cpq such that
cpq∥x∥q ≤∥x∥p ≤Cpq∥x∥q
∀x ∈V.
■
In a ﬁnite-dimensional normed space all norms are equivalent. In particular,
if V = Rn it can be shown that for the p-norms, with p = 1, 2, and ∞, the
constants cpq and Cpq take the value reported in Table 1.1.
cpq
q = 1
q = 2
q = ∞
p = 1
1
1
1
p = 2
n−1/2
1
1
p = ∞
n−1
n−1/2
1
Cpq
q = 1
q = 2
q = ∞
p = 1
1
n1/2
n
p = 2
1
1
n1/2
p = ∞
1
1
1
TABLE 1.1. Equivalence constants for the main norms of Rn
In this book we shall often deal with sequences of vectors and with their
convergence. For this purpose, we recall that a sequence of vectors

x(k)
in a vector space V having ﬁnite dimension n, converges to a vector x, and
we write lim
k→∞x(k) = x if
lim
k→∞x(k)
i
= xi,
i = 1, . . . , n
(1.15)
where x(k)
i
and xi are the components of the corresponding vectors with
respect to a basis of V . If V = Rn, due to the uniqueness of the limit of a

1.11 Matrix Norms
21
sequence of real numbers, (1.15) implies also the uniqueness of the limit, if
existing, of a sequence of vectors.
We further notice that in a ﬁnite-dimensional space all the norms are topo-
logically equivalent in the sense of convergence, namely, given a sequence
of vectors x(k),
|||x(k)||| →0 ⇔∥x(k)∥→0 if k →∞,
where ||| · ||| and ∥· ∥are any two vector norms. As a consequence, we can
establish the following link between norms and limits.
Property 1.12 Let ∥· ∥be a norm in a space ﬁnite dimensional space V .
Then
lim
k→∞x(k) = x
⇔
lim
k→∞∥x −x(k)∥= 0,
where x ∈V and

x(k)
is a sequence of elements of V .
1.11
Matrix Norms
Deﬁnition 1.19 A matrix norm is a mapping ∥·∥: Rm×n →R such that:
1. ∥A∥≥0 ∀A ∈Rm×n and ∥A∥= 0 if and only if A = 0;
2. ∥αA∥= |α|∥A∥∀α ∈R, ∀A ∈Rm×n (homogeneity);
3. ∥A + B∥≤∥A∥+ ∥B∥∀A, B ∈Rm×n (triangular inequality).
■
Unless otherwise speciﬁed we shall employ the same symbol ∥·∥, to denote
matrix norms and vector norms.
We can better characterize the matrix norms by introducing the concepts
of compatible norm and norm induced by a vector norm.
Deﬁnition 1.20 We say that a matrix norm ∥·∥is compatible or consistent
with a vector norm ∥· ∥if
∥Ax∥≤∥A∥∥x∥,
∀x ∈Rn.
(1.16)
More generally, given three norms, all denoted by ∥· ∥, albeit deﬁned on
Rm, Rn and Rm×n, respectively, we say that they are consistent if ∀x ∈Rn,
Ax = y ∈Rm, A ∈Rm×n, we have that ∥y∥≤∥A∥∥x∥.
■
In order to single out matrix norms of practical interest, the following
property is in general required

22
1. Foundations of Matrix Analysis
Deﬁnition 1.21 We say that a matrix norm ∥· ∥is sub-multiplicative if
∀A ∈Rn×m, ∀B ∈Rm×q
∥AB∥≤∥A∥∥B∥.
(1.17)
■
This property is not satisﬁed by any matrix norm. For example (taken from
[GL89]), the norm ∥A∥∆= max |aij| for i = 1, . . . , n, j = 1, . . . , m does
not satisfy (1.17) if applied to the matrices
A = B =
 1
1
1
1

,
since 2 = ∥AB∥∆> ∥A∥∆∥B∥∆= 1.
Notice that, given a certain sub-multiplicative matrix norm ∥· ∥α, there
always exists a consistent vector norm. For instance, given any ﬁxed vector
y ̸= 0 in Cn, it suﬃces to deﬁne the consistent vector norm as
∥x∥= ∥xyH∥α
x ∈Cn.
As a consequence, in the case of sub-multiplicative matrix norms it is no
longer necessary to explicitly specify the vector norm with respect to the
matrix norm is consistent.
Example 1.7 The norm
∥A∥F =




n

i,j=1
|aij|2 = tr(AAH)
(1.18)
is a matrix norm called the Frobenius norm (or Euclidean norm in Cn2) and is
compatible with the Euclidean vector norm ∥· ∥2. Indeed,
∥Ax∥2
2 =
n

i=1

n

j=1
aijxj

2
≤
n

i=1
 n

j=1
|aij|2
n

j=1
|xj|2

= ∥A∥2
F ∥x∥2
2.
Notice that for such a norm ∥In∥F = √n.
•
In view of the deﬁnition of a natural norm, we recall the following theorem.
Theorem 1.1 Let ∥·∥be a vector norm. The function
∥A∥= sup
x̸=0
∥Ax∥
∥x∥
(1.19)
is a matrix norm called induced matrix norm or natural matrix norm.

1.11 Matrix Norms
23
Proof. We start by noticing that (1.19) is equivalent to
∥A∥= sup
∥x∥=1
∥Ax∥.
(1.20)
Indeed, one can deﬁne for any x ̸= 0 the unit vector u = x/∥x∥, so that (1.19)
becomes
∥A∥= sup
∥u∥=1
∥Au∥= ∥Aw∥
with ∥w∥= 1.
This being taken as given, let us check that (1.19) (or, equivalently, (1.20)) is
actually a norm, making direct use of Deﬁnition 1.19.
1. If ∥Ax∥≥0, then it follows that ∥A∥= sup
∥x∥=1
∥Ax∥≥0. Moreover
∥A∥= sup
x̸=0
∥Ax∥
∥x∥
= 0 ⇔∥Ax∥= 0 ∀x ̸= 0
and Ax = 0 ∀x ̸= 0 if and only if A=0; therefore ∥A∥= 0 ⇔A = 0.
2. Given a scalar α,
∥αA∥= sup
∥x∥=1
∥αAx∥= |α| sup
∥x∥=1
∥Ax∥= |α| ∥A∥.
3. Finally, triangular inequality holds. Indeed, by deﬁnition of supremum, if
x ̸= 0 then
∥Ax∥
∥x∥
≤∥A∥
⇒
∥Ax∥≤∥A∥∥x∥,
so that, taking x with unit norm, one gets
∥(A + B)x∥≤∥Ax∥+ ∥Bx∥≤∥A∥+ ∥B∥,
from which it follows that ∥A + B∥= sup
∥x∥=1
∥(A + B)x∥≤∥A∥+ ∥B∥.
3
Relevant instances of induced matrix norms are the so-called p-norms de-
ﬁned as
∥A∥p = sup
x̸=0
∥Ax∥p
∥x∥p
The 1-norm and the inﬁnity norm are easily computable since
∥A∥1 =
max
j=1,... ,n
m

i=1
|aij|,
∥A∥∞=
max
i=1,... ,m
n

j=1
|aij|
and they are called the column sum norm and the row sum norm, respec-
tively.
Moreover, we have ∥A∥1 = ∥AT ∥∞and, if A is self-adjoint or real sym-
metric, ∥A∥1 = ∥A∥∞.
A special discussion is deserved by the 2-norm or spectral norm for which
the following theorem holds.

24
1. Foundations of Matrix Analysis
Theorem 1.2 Let σ1(A) be the largest singular value of A. Then
∥A∥2 =

ρ(AHA) =

ρ(AAH) = σ1(A).
(1.21)
In particular, if A is hermitian (or real and symmetric), then
∥A∥2 = ρ(A),
(1.22)
while, if A is unitary, ∥A∥2 = 1.
Proof. Since AHA is hermitian, there exists a unitary matrix U such that
UHAHAU = diag(µ1, . . . , µn),
where µi are the (positive) eigenvalues of AHA. Let y = UHx, then
∥A∥2
=
sup
x̸=0

(AHAx, x)
(x, x)
= sup
y̸=0

(UHAHAUy, y)
(y, y)
=
sup
y̸=0




n

i=1
µi|yi|2/
n

i=1
|yi|2 =

max
i=1,... ,n|µi|,
from which (1.21) follows, thanks to (1.10).
If A is hermitian, the same considerations as above apply directly to A.
Finally, if A is unitary
∥Ax∥2
2 = (Ax, Ax) = (x, AHAx) = ∥x∥2
2
so that ∥A∥2 = 1.
3
As a consequence, the computation of ∥A∥2 is much more expensive than
that of ∥A∥∞or ∥A∥1. However, if only an estimate of ∥A∥2 is required,
the following relations can be proﬁtably employed in the case of square
matrices
max
i,j |aij| ≤∥A∥2 ≤n max
i,j |aij|,
1
√n∥A∥∞≤∥A∥2 ≤√n∥A∥∞,
1
√n∥A∥1 ≤∥A∥2 ≤√n∥A∥1,
∥A∥2 ≤

∥A∥1 ∥A∥∞.
For other estimates of similar type we refer to Exercise 17. Moreover, if A
is normal then ∥A∥2 ≤∥A∥p for any n and all p ≥2.
Theorem 1.3 Let ||| · ||| be a matrix norm induced by a vector norm ∥· ∥.
Then
1. ∥Ax∥≤|||A||| ∥x∥, that is, ||| · ||| is a norm compatible with ∥· ∥;

1.11 Matrix Norms
25
2. |||I||| = 1;
3. |||AB||| ≤|||A||| |||B|||, that is, ||| · ||| is sub-multiplicative.
Proof. Part 1 of the theorem is already contained in the proof of Theorem 1.1,
while part 2 follows from the fact that |||I||| = sup
x̸=0
∥Ix∥/∥x∥= 1. Part 3 is simple
to check.
3
Notice that the p-norms are sub-multiplicative. Moreover, we remark that
the sub-multiplicativity property by itself would only allow us to conclude
that |||I||| ≥1. Indeed, |||I||| = |||I · I||| ≤|||I|||2.
1.11.1
Relation between Norms and the Spectral Radius of a
Matrix
We next recall some results that relate the spectral radius of a matrix to
matrix norms and that will be widely employed in Chapter 4.
Theorem 1.4 Let ∥· ∥be a consistent matrix norm; then
ρ(A) ≤∥A∥
∀A ∈Cn×n.
Proof. Let λ be an eigenvalue of A and v ̸= 0 an associated eigenvector. As a
consequence, since ∥· ∥is consistent, we have
|λ| ∥v∥= ∥λv∥= ∥Av∥≤∥A∥∥v∥
so that |λ| ≤∥A∥.
3
More precisely, the following property holds (see for the proof [IK66], p.
12, Theorem 3).
Property 1.13 Let A ∈Cn×n and ε > 0. Then, there exists a consistent
matrix norm ∥· ∥A,ε (depending on ε) such that
∥A∥A,ε ≤ρ(A) + ε.
As a result, having ﬁxed an arbitrarily small tolerance, there always exists
a matrix norm which is arbitrarily close to the spectral radius of A, namely
ρ(A) = inf
∥·∥∥A∥,
(1.23)
the inﬁmum being taken on the set of all the consistent norms.
For the sake of clarity, we notice that the spectral radius is a sub-
multiplicative seminorm, since it is not true that ρ(A) = 0 iﬀA = 0.
As an example, any triangular matrix with null diagonal entries clearly has
spectral radius equal to zero. Moreover, we have the following result.

26
1. Foundations of Matrix Analysis
Property 1.14 Let A be a square matrix and let ∥·∥be a consistent norm.
Then
lim
m→∞∥Am∥1/m = ρ(A).
1.11.2
Sequences and Series of Matrices
A sequence of matrices

A(k)
∈Rn×n is said to converge to a matrix
A ∈Rn×n if
lim
k→∞∥A(k) −A∥= 0.
The choice of the norm does not inﬂuence the result since in Rn×n all norms
are equivalent. In particular, when studying the convergence of iterative
methods for solving linear systems (see Chapter 4), one is interested in the
so-called convergent matrices for which
lim
k→∞Ak = 0,
0 being the null matrix. The following theorem holds.
Theorem 1.5 Let A be a square matrix; then
lim
k→∞Ak = 0 ⇔ρ(A) < 1.
(1.24)
Moreover, the geometric series
∞

k=0
Ak is convergent iﬀρ(A) < 1. In such a
case
∞

k=0
Ak = (I −A)−1.
(1.25)
As a result, if ρ(A) < 1 the matrix I −A is invertible and the following
inequalities hold
1
1 + ∥A∥≤∥(I −A)−1∥≤
1
1 −∥A∥
(1.26)
where ∥· ∥is an induced matrix norm such that ∥A∥< 1.
Proof. Let us prove (1.24). Let ρ(A) < 1, then ∃ε > 0 such that ρ(A) < 1 −ε
and thus, thanks to Property 1.13, there exists a consistent matrix norm ∥·∥such
that ∥A∥≤ρ(A) + ε < 1. From the fact that ∥Ak∥≤∥A∥k < 1 and from the
deﬁnition of convergence it turns out that as k →∞the sequence

Ak
tends
to zero. Conversely, assume that lim
k→∞Ak = 0 and let λ denote an eigenvalue of
A. Then, Akx = λkx, being x(̸=0) an eigenvector associated with λ, so that

1.12 Positive Deﬁnite, Diagonally Dominant and M-matrices
27
lim
k→∞λk = 0. As a consequence, |λ| < 1 and because this is true for a generic
eigenvalue one gets ρ(A) < 1 as desired. Relation (1.25) can be obtained noting
ﬁrst that the eigenvalues of I−A are given by 1 −λ(A), λ(A) being the generic
eigenvalue of A. On the other hand, since ρ(A) < 1, we deduce that I−A is
nonsingular. Then, from the identity
(I −A)(I + A + . . . + An) = (I −An+1)
and taking the limit for n tending to inﬁnity the thesis follows since
(I −A)
∞

k=0
Ak = I.
Finally, thanks to Theorem 1.3, the equality ∥I∥= 1 holds, so that
1 = ∥I∥≤∥I −A∥∥(I −A)−1∥≤(1 + ∥A∥) ∥(I −A)−1∥,
giving the ﬁrst inequality in (1.26). As for the second part, noting that I =
I−A+A and multiplying both sides on the right by (I−A)−1, one gets (I−A)−1 =
I + A(I −A)−1. Passing to the norms, we obtain
∥(I −A)−1∥≤1 + ∥A∥∥(I −A)−1∥,
and thus the second inequality, since ∥A∥< 1.
3
Remark 1.1 The assumption that there exists an induced matrix norm
such that ∥A∥< 1 is justiﬁed by Property 1.13, recalling that A is conver-
gent and, therefore, ρ(A) < 1.
■
Notice that (1.25) suggests an algorithm to approximate the inverse of a
matrix by a truncated series expansion.
1.12
Positive Deﬁnite, Diagonally Dominant and
M-matrices
Deﬁnition 1.22 A matrix A ∈Cn×n is positive deﬁnite in Cn if the num-
ber (Ax, x) is real and positive ∀x ∈Cn, x ̸= 0. A matrix A ∈Rn×n is
positive deﬁnite in Rn if (Ax, x) > 0 ∀x ∈Rn, x ̸= 0. If the strict in-
equality is substituted by the weak one (≥) the matrix is called positive
semideﬁnite.
■
Example 1.8 Matrices that are positive deﬁnite in Rn are not necessarily sym-
metric. An instance is provided by matrices of the form
A =

2
α
−2 −α
2

(1.27)

28
1. Foundations of Matrix Analysis
for α ̸= −1. Indeed, for any non null vector x = (x1, x2)T in R2
(Ax, x) = 2(x2
1 + x2
2 −x1x2) > 0.
Notice that A is not positive deﬁnite in C2. Indeed, if we take a complex vector
x we ﬁnd out that the number (Ax, x) is not real-valued in general.
•
Deﬁnition 1.23 Let A ∈Rn×n. The matrices
AS = 1
2(A + AT ),
ASS = 1
2(A −AT )
are respectively called the symmetric part and the skew-symmetric part
of A. Obviously, A = AS + ASS. If A ∈Cn×n, the deﬁnitions modify as
follows: AS = 1
2(A + AH) and ASS = 1
2(A −AH).
■
The following property holds
Property 1.15 A real matrix A of order n is positive deﬁnite iﬀits sym-
metric part AS is positive deﬁnite.
Indeed, it suﬃces to notice that, due to (1.12) and the deﬁnition of ASS,
xT ASSx = 0 ∀x ∈Rn. For instance, the matrix in (1.27) has a positive
deﬁnite symmetric part, since
AS = 1
2(A + AT ) =

2
−1
−1
2

.
This holds more generally (for the proof see [Axe94]).
Property 1.16 Let A ∈Cn×n (respectively, A ∈Rn×n); if (Ax, x) is real-
valued ∀x ∈Cn, then A is hermitian (respectively, symmetric).
An immediate consequence of the above results is that matrices that are
positive deﬁnite in Cn do satisfy the following characterizing property.
Property 1.17 A square matrix A of order n is positive deﬁnite in Cn
iﬀit is hermitian and has positive eigenvalues. Thus, a positive deﬁnite
matrix is nonsingular.
In the case of positive deﬁnite real matrices in Rn, results more speciﬁc
than those presented so far hold only if the matrix is also symmetric (this is
the reason why many textbooks deal only with symmetric positive deﬁnite
matrices). In particular
Property 1.18 Let A ∈Rn×n be symmetric. Then, A is positive deﬁnite
iﬀone of the following properties is satisﬁed:
1. (Ax, x) > 0 ∀x ̸= 0 with x∈Rn;

1.12 Positive Deﬁnite, Diagonally Dominant and M-matrices
29
2. the eigenvalues of the principal submatrices of A are all positive;
3. the dominant principal minors of A are all positive (Sylvester crite-
rion);
4. there exists a nonsingular matrix H such that A = HT H.
All the diagonal entries of a positive deﬁnite matrix are positive. Indeed,
if ei is the i-th vector of the canonical basis of Rn, then eT
i Aei = aii > 0.
Moreover, it can be shown that if A is symmetric positive deﬁnite, the
entry with the largest module must be a diagonal entry (these last two
properties are therefore necessary conditions for a matrix to be positive
deﬁnite).
We ﬁnally notice that if A is symmetric positive deﬁnite and A1/2 is
the only positive deﬁnite matrix that is a solution of the matrix equation
X2 = A, the norm
∥x∥A = ∥A1/2x∥2 = (Ax, x)1/2
(1.28)
deﬁnes a vector norm, called the energy norm of the vector x. Related to
the energy norm is the energy scalar product given by (x, y)A = (Ax, y).
Deﬁnition 1.24 A matrix A∈Rn×n is called diagonally dominant by rows
if
|aii| ≥
n

j=1,j̸=i
|aij|, with i = 1, . . . , n,
while it is called diagonally dominant by columns if
|aii| ≥
n

j=1,j̸=i
|aji|, with i = 1, . . . , n.
If the inequalities above hold in a strict sense, A is called strictly diagonally
dominant (by rows or by columns, respectively).
■
A strictly diagonally dominant matrix that is symmetric with positive di-
agonal entries is also positive deﬁnite.
Deﬁnition 1.25 A nonsingular matrix A ∈Rn×n is an M-matrix if aij ≤0
for i ̸= j and if all the entries of its inverse are nonnegative.
■
M-matrices enjoy the so-called discrete maximum principle, that is, if A is
an M-matrix and Ax ≤0, then x ≤0 (where the inequalities are meant
componentwise). In this connection, the following result can be useful.
Property 1.19 (M-criterion) Let a matrix A satisfy aij ≤0 for i ̸= j.
Then A is an M-matrix if and only if there exists a vector w > 0 such that
Aw > 0.

30
1. Foundations of Matrix Analysis
Finally, M-matrices are related to strictly diagonally dominant matrices
by the following property.
Property 1.20 A matrix A ∈Rn×n that is strictly diagonally dominant
by rows and whose entries satisfy the relations aij ≤0 for i ̸= j and aii > 0,
is an M-matrix.
For further results about M-matrices, see for instance [Axe94] and [Var62].
1.13
Exercises
1. Let W1 and W2 be two subspaces of Rn. Prove that if V = W1 ⊕W2, then
dim(V ) = dim(W1) + dim(W2), while in general
dim(W1 + W2) = dim(W1) + dim(W2) −dim(W1 ∩W2).
[Hint : Consider a basis for W1 ∩W2 and ﬁrst extend it to W1, then to
W2, verifying that the basis formed by the set of the obtained vectors is a
basis for the sum space.]
2. Check that the following set of vectors
vi =

xi−1
1
, xi−1
2
, . . . , xi−1
n

,
i = 1, 2, . . . , n,
forms a basis for Rn, x1, . . . , xn being a set of n distinct points of R.
3. Exhibit an example showing that the product of two symmetric matrices
may be nonsymmetric.
4. Let B be a skew-symmetric matrix, namely, BT = −B. Let A = (I+B)(I−
B)−1 and show that A−1 = AT .
5. A matrix A ∈Cn×n is called skew-hermitian if AH = −A. Show that the
diagonal entries of A must be purely imaginary numbers.
6. Let A, B and A+B be invertible matrices of order n. Show that also A−1 +
B−1 is nonsingular and that

A−1 + B−1−1 = A (A + B)−1 B = B (A + B)−1 A.
[Solution :

A−1 + B−1−1 = A

I + B−1A
−1 = A (B + A)−1 B. The sec-
ond equality is proved similarly by factoring out B and A, respectively from
left and right.]
7. Given the non symmetric real matrix
A =


0
1
1
1
0
−1
−1
−1
0

,
check that it is similar to the diagonal matrix D = diag(1, 0, −1) and ﬁnd
its eigenvectors. Is this matrix normal?
[Solution : the matrix is not normal.]

1.13 Exercises
31
8. Let A be a square matrix of order n. Check that if P(A) =
n

k=0
ckAk and
λ(A) are the eigenvalues of A, then the eigenvalues of P(A) are given by
λ(P(A)) = P(λ(A)). In particular, prove that ρ(A2) = [ρ(A)]2.
9. Prove that a matrix of order n having n distinct eigenvalues cannot be
defective. Moreover, prove that a normal matrix cannot be defective.
10. Commutativity of matrix product. Show that if A and B are square matri-
ces that share the same set of eigenvectors, then AB = BA. Prove, by a
counterexample, that the converse is false.
11. Let A be a normal matrix whose eigenvalues are λ1, . . . , λn. Show that the
singular values of A are |λ1|, . . . , |λn|.
12. Let A ∈Cm×n with rank(A) = n. Show that A† = (AT A)−1AT enjoys the
following properties
(1) A†A = In;
(2) A†AA† = A†, AA†A = A;
(3) if m = n, A† = A−1.
13. Show that the Moore-Penrose pseudo-inverse matrix A† is the only matrix
that minimizes the functional
min
X∈Cn×m∥AX −Im∥F,
where ∥· ∥F is the Frobenius norm.
14. Prove Property 1.10.
[Solution : For any x, x ∈V show that | ∥x∥−∥x∥| ≤∥x −x∥. Assuming
that dim(V ) = n and expanding the vector w = x −x on a basis of V,
show that ∥w∥≤C∥w∥∞, from which the thesis follows by imposing in
the ﬁrst obtained inequality that ∥w∥∞≤ε.]
15. Prove Property 1.11 in the case A ∈Rn×m with m linearly independent
columns.
[Hint : First show that ∥· ∥A fulﬁlls all the properties characterizing a
norm: positiveness (A has linearly independent columns, thus if x ̸= 0, then
Ax ̸= 0, which proves the thesis), homogeneity and triangular inequality.]
16. Show that for a rectangular matrix A ∈Rm×n
∥A∥2
F = σ2
1 + . . . + σ2
p,
where p is the minimum between m and n, σi are the singular values of A
and ∥· ∥F is the Frobenius norm.
17. Assuming p, q = 1, 2, ∞, F, recover the following table of equivalence con-
stants cpq such that ∀A ∈Rn×n, ∥A∥p ≤cpq∥A∥q.
cpq
q = 1
q = 2
q = ∞
q = F
p = 1
1
√n
n
√n
p = 2
√n
1
√n
1
p = ∞
n
√n
1
√n
p = F
√n
√n
√n
1

32
1. Foundations of Matrix Analysis
18. A matrix norm for which ∥A∥= ∥|A| ∥is called absolute norm, having
denoted by |A| the matrix of the absolute values of the entries of A. Prove
that ∥· ∥1, ∥· ∥∞and ∥· ∥F are absolute norms, while ∥· ∥2 is not. Show
that for this latter
1
√n∥A∥2 ≤∥|A| ∥2 ≤√n∥A∥2.

2
Principles of Numerical Mathematics
The basic concepts of consistency, stability and convergence of a numerical
method will be introduced in a very general context in the ﬁrst part of
the chapter: they provide the common framework for the analysis of any
method considered henceforth. The second part of the chapter deals with
the computer ﬁnite representation of real numbers and the analysis of error
propagation in machine operations.
2.1
Well-posedness and Condition Number of a
Problem
Consider the following problem: ﬁnd x such that
F(x, d) = 0
(2.1)
where d is the set of data which the solution depends on and F is the
functional relation between x and d. According to the kind of problem
that is represented in (2.1), the variables x and d may be real numbers,
vectors or functions. Typically, (2.1) is called a direct problem if F and d
are given and x is the unknown, inverse problem if F and x are known
and d is the unknown, identiﬁcation problem when x and d are given while
the functional relation F is the unknown (these latter problems will not be
covered in this volume).
Problem (2.1) is well posed if it admits a unique solution x which depends
with continuity on the data. We shall use the terms well posed and stable in

34
2. Principles of Numerical Mathematics
an interchanging manner and we shall deal henceforth only with well-posed
problems.
A problem which does not enjoy the property above is called ill posed or
unstable and before undertaking its numerical solution it has to be regular-
ized, that is, it must be suitably transformed into a well-posed problem (see,
for instance [Mor84]). Indeed, it is not appropriate to pretend the numerical
method can cure the pathologies of an intrinsically ill-posed problem.
Example 2.1 A simple instance of an ill-posed problem is ﬁnding the number
of real roots of a polynomial. For example, the polynomial p(x) = x4 −x2(2a −
1) + a(a −1) exhibits a discontinuous variation of the number of real roots as a
continuously varies in the real ﬁeld. We have, indeed, 4 real roots if a ≥1, 2 if
a ∈[0, 1) while no real roots exist if a < 0.
•
Continuous dependence on the data means that small perturbations on
the data d yield “small” changes in the solution x. Precisely, denoting by δd
an admissible perturbation on the data and by δx the consequent change
in the solution, in such a way that
F(x + δx, d + δd) = 0,
(2.2)
then
∀η > 0, ∃K(η, d) : ∥δd∥< η ⇒∥δx∥≤K(η, d)∥δd∥.
(2.3)
The norms used for the data and for the solution may not coincide, when-
ever d and x represent variables of diﬀerent kinds.
With the aim of making this analysis more quantitative, we introduce the
following deﬁnition.
Deﬁnition 2.1 For problem (2.1) we deﬁne the relative condition number
to be
K(d) = sup
δd∈D
∥δx∥/∥x∥
∥δd∥/∥d∥,
(2.4)
where D is a neighborhood of the origin and denotes the set of admissible
perturbations on the data for which the perturbed problem (2.2) still makes
sense. Whenever d = 0 or x = 0, it is necessary to introduce the absolute
condition number, given by
Kabs(d) = sup
δd∈D
∥δx∥
∥δd∥.
(2.5)
■
Problem (2.1) is called ill-conditioned if K(d) is “big” for any admissible
datum d (the precise meaning of “small” and “big” is going to change
depending on the considered problem).

2.1 Well-posedness and Condition Number of a Problem
35
The property of a problem of being well-conditioned is independent of
the numerical method that is being used to solve it. In fact, it is possible
to generate stable as well as unstable numerical schemes for solving well-
conditioned problems. The concept of stability for an algorithm or for a
numerical method is analogous to that used for problem (2.1) and will be
made precise in the next section.
Remark 2.1 (Ill-posed problems) Even in the case in which the condi-
tion number does not exist (formally, it is inﬁnite), it is not necessarily true
that the problem is ill-posed. In fact there exist well posed problems (for
instance, the search of multiple roots of algebraic equations, see Example
2.2) for which the condition number is inﬁnite, but such that they can be
reformulated in equivalent problems (that is, having the same solutions)
with a ﬁnite condition number.
■
If problem (2.1) admits a unique solution, then there necessarily exists a
mapping G, that we call resolvent, between the sets of the data and of the
solutions, such that
x = G(d),
that is
F(G(d), d) = 0.
(2.6)
According to this deﬁnition, (2.2) yields x + δx = G(d + δd). Assuming
that G is diﬀerentiable in d and denoting formally by G′(d) its derivative
with respect to d (if G : Rn →Rm, G′(d) will be the Jacobian matrix of
G evaluated at the vector d), a Taylor’s expansion of G truncated at ﬁrst
order ensures that
G(d + δd) −G(d) = G′(d)δd + o(∥δd∥)
for δd →0,
where ∥· ∥is a suitable norm for δd and o(·) is the classical inﬁnitesimal
symbol denoting an inﬁnitesimal term of higher order with respect to its
argument. Neglecting the inﬁnitesimal of higher order with respect to ∥δd∥,
from (2.4) and (2.5) we respectively deduce that
K(d) ≃∥G′(d)∥
∥d∥
∥G(d)∥,
Kabs(d) ≃∥G′(d)∥,
(2.7)
the symbol ∥·∥denoting the matrix norm associated with the vector norm
(deﬁned in (1.19)). The estimates in (2.7) are of great practical usefulness
in the analysis of problems in the form (2.6), as shown in the forthcoming
examples.
Example 2.2 (Algebraic equations of second degree) The solutions to the
algebraic equation x2 −2px + 1 = 0, with p ≥1, are x± = p ±

p2 −1. In this
case, F(x, p) = x2 −2px+1, the datum d is the coeﬃcient p, while x is the vector
of components {x+, x−}. As for the condition number, we notice that (2.6) holds

36
2. Principles of Numerical Mathematics
by taking G : R →R2, G(p) = {x+, x−}. Letting G±(p) = x±, it follows that
G′
±(p) = 1 ± p/

p2 −1. Using (2.7) with ∥· ∥= ∥· ∥2 we get
K(p) ≃
|p|

p2 −1
,
p > 1.
(2.8)
From (2.8) it turns out that in the case of separated roots (say, if p ≥
√
2)
problem F(x, p) = 0 is well conditioned. The behavior dramatically changes in
the case of multiple roots, that is when p = 1. First of all, one notices that the
function G±(p) = p ±

p2 −1 is no longer diﬀerentiable for p = 1, which makes
(2.8) meaningless. On the other hand, equation (2.8) shows that, for p close to
1, the problem at hand is ill conditioned. However, the problem is not ill posed.
Indeed, following Remark 2.1, it is possible to reformulate it in an equivalent
manner as F(x, t) = x2 −((1 + t2)/t)x + 1 = 0, with t = p +

p2 −1, whose
roots x−= t and x+ = 1/t coincide for t = 1. The change of parameter thus
removes the singularity that is present in the former representation of the roots
as functions of p. The two roots x−= x−(t) and x+ = x+(t) are now indeed
regular functions of t in the neighborhood of t = 1 and evaluating the condition
number by (2.7) yields K(t) ≃1 for any value of t. The transformed problem is
thus well conditioned.
•
Example 2.3 (Systems of linear equations) Consider the linear system Ax
= b, where x and b are two vectors in Rn, while A is the matrix (n × n) of the
real coeﬃcients of the system. Suppose that A is nonsingular; in such a case x
is the unknown solution x, while the data d are the right-hand side b and the
matrix A, that is, d = {bi, aij, 1 ≤i, j ≤n}.
Suppose now that we perturb only the right-hand side b. We have d = b,
x = G(b) = A−1b so that, G′(b) = A−1, and (2.7) yields
K(d) ≃∥A−1∥∥b∥
∥A−1b∥
= ∥Ax∥
∥x∥∥A−1∥≤∥A∥∥A−1∥= K(A),
(2.9)
where K(A) is the condition number of matrix A (see Section 3.1.1) and the use
of a consistent matrix norm is understood. Therefore, if A is well conditioned,
solving the linear system Ax=b is a stable problem with respect to perturbations
of the right-hand side b. Stability with respect to perturbations on the entries of
A will be analyzed in Section 3.10.
•
Example 2.4 (Nonlinear equations) Let f : R →R be a function of class
C1 and consider the nonlinear equation
F(x, d) = f(x) = ϕ(x) −d = 0,
where ϕ : R →R is a suitable function and d ∈R a datum (possibly equal
to zero). The problem is well deﬁned only if ϕ is invertible in a neighborhood
of d: in such a case, indeed, x = ϕ−1(d) and the resolvent is G = ϕ−1. Since
(ϕ−1)′(d) = [ϕ′(x)]−1, the ﬁrst relation in (2.7) yields, for d ̸= 0,
K(d) ≃|d|
|x||[ϕ′(x)]−1|,
(2.10)

2.2 Stability of Numerical Methods
37
while if d = 0 or x = 0 we have
Kabs(d) ≃|[ϕ′(x)]−1|.
(2.11)
The problem is thus ill posed if x is a multiple root of ϕ(x)−d; it is ill conditioned
when ϕ′(x) is “small”, well conditioned when ϕ′(x) is “large”. We shall further
address this subject in Section 6.1.
•
In view of (2.8), the quantity ∥G′(d)∥is an approximation of Kabs(d) and
is sometimes called ﬁrst order absolute condition number. This latter rep-
resents the limit of the Lipschitz constant of G (see Section 11.1) as the
perturbation on the data tends to zero.
Such a number does not always provide a sound estimate of the condition
number Kabs(d). This happens, for instance, when G′ vanishes at a point
whilst G is non null in a neighborhood of the same point. For example,
take x = G(d) = cos(d) −1 for d ∈(−π/2, π/2); we have G′(0) = 0, while
Kabs(0) = 2/π.
2.2
Stability of Numerical Methods
We shall henceforth suppose the problem (2.1) to be well posed. A numer-
ical method for the approximate solution of (2.1) will consist, in general,
of a sequence of approximate problems
Fn(xn, dn) = 0
n ≥1
(2.12)
depending on a certain parameter n (to be deﬁned case by case). The
understood expectation is that xn →x as n →∞, i.e. that the numerical
solution converges to the exact solution. For that, it is necessary that dn →
d and that Fn “approximates” F, as n →∞. Precisely, if the datum d of
problem (2.1) is admissible for Fn, we say that (2.12) is consistent if
Fn(x, d) = Fn(x, d) −F(x, d) →0 for n →∞
(2.13)
where x is the solution to problem (2.1) corresponding to the datum d.
The meaning of this deﬁnition will be made precise in the next chapters
for any single class of considered problems.
A method is said to be strongly consistent if Fn(x, d) = 0 for any value
of n and not only for n →∞.
In some cases (e.g., when iterative methods are used) problem (2.12)
could take the following form
Fn(xn, xn−1, . . . , xn−q, dn) = 0
n ≥q
(2.14)
where x0, x1, . . . , xq−1 are given. In such a case, the property of strong
consistency becomes Fn(x, x, . . . , x, d) = 0 for all n ≥q.

38
2. Principles of Numerical Mathematics
Example 2.5 Let us consider the following iterative method (known as New-
ton’s method and discussed in Section 6.2.2) for approximating a simple root α
of a function f : R →R,
given x0,
xn = xn−1 −f(xn−1)
f ′(xn−1),
n ≥1.
(2.15)
The method (2.15) can be written in the form (2.14) by setting Fn(xn, xn−1, f) =
xn −xn−1 + f(xn−1)/f ′(xn−1) and is strongly consistent since Fn(α, α, f) = 0
for all n ≥1.
Consider now the following numerical method (known as the composite mid-
point rule discussed in Section 9.2) for approximating x =
 b
a f(t) dt,
xn = H
n

k=1
f
 tk + tk+1
2

,
n ≥1
where H = (b −a)/n and tk = a + (k −1)H, k = 1, . . . , (n + 1). This method
is consistent; it is also strongly consistent provided thet f is a piecewise linear
polynomial.
More generally, all numerical methods obtained from the mathematical prob-
lem by truncation of limit operations (such as integrals, derivatives, series, . . . )
are not strongly consistent.
•
Recalling what has been previously stated about problem (2.1), in order
for the numerical method to be well posed (or stable) we require that for any
ﬁxed n, there exists a unique solution xn corresponding to the datum dn,
that the computation of xn as a function of dn is unique and, furthermore,
that xn depends continuously on the data, i.e.
∀η > 0, ∃Kn(η, dn) : ∥δdn∥< η ⇒∥δxn∥≤Kn(η, dn)∥δdn∥.
(2.16)
As done in (2.4), we introduce for each problem in the sequence (2.12) the
quantities
Kn(dn) =
sup
δdn∈Dn
∥δxn∥/∥xn∥
∥δdn∥/∥dn∥,
Kabs,n(dn) =
sup
δdn∈Dn
∥δxn∥
∥δdn∥,
(2.17)
and then deﬁne
Knum(dn) = lim
k→∞sup
n≥k
Kn(dn),
Knum
abs (dn) = lim
k→∞sup
n≥k
Kabs,n(dn).
We call Knum(dn) the relative asymptotic condition number of the numer-
ical method (2.12) and Knum
abs (dn) absolute asymptotic condition number,
corresponding to the datum dn.
The numerical method is said to be well conditioned if Knum is “small”
for any admissible datum dn, ill conditioned otherwise. As in (2.6), let us

2.2 Stability of Numerical Methods
39
consider the case where, for each n, the functional relation (2.1) deﬁnes a
mapping Gn between the sets of the numerical data and the solutions
xn = Gn(dn),
that is Fn(Gn(dn), dn) = 0.
(2.18)
Assuming that Gn is diﬀerentiable, we can obtain from (2.17)
Kn(dn) ≃∥G′
n(dn)∥
∥dn∥
∥Gn(dn)∥,
Kabs,n(dn) ≃∥G′
n(dn)∥.
(2.19)
Example 2.6 (Sum and subtraction) The function f : R2 →R, f(a, b) =
a + b, is a linear mapping whose gradient is the vector f ′(a, b) = (1, 1)T . Using
the vector norm ∥· ∥1 deﬁned in (1.13) yields K(a, b) ≃(|a| + |b|)/(|a + b|), from
which it follows that summing two numbers of the same sign is a well conditioned
operation, being K(a, b) ≃1. On the other hand, subtracting two numbers almost
equal is ill conditioned, since |a + b| ≪|a| + |b|. This fact, already pointed out in
Example 2.2, leads to the cancellation of signiﬁcant digits whenever numbers can
be represented using only a ﬁnite number of digits (as in ﬂoating-point arithmetic,
see Section 2.5).
•
Example 2.7 Consider again the problem of computing the roots of a polyno-
mial of second degree analyzed in Example 2.2. When p > 1 (separated roots),
such a problem is well conditioned. However, we generate an unstable algorithm
if we evaluate the root x−by the formula x−= p −

p2 −1. This formula is
indeed subject to errors due to numerical cancellation of signiﬁcant digits (see
Section 2.4) that are introduced by the ﬁnite arithmetic of the computer. A pos-
sible remedy to this trouble consists of computing x+ = p +

p2 −1 at ﬁrst,
then x−= 1/x+. Alternatively, one can solve F(x, p) = x2 −2px + 1 = 0 using
Newton’s method (proposed in Example 2.5)
xn = xn−1 −(x2
n−1 −2pxn−1 + 1)/(2xn−1 −2p) = fn(p),
n ≥1,
x0 given.
Applying (2.19) for p > 1 yields Kn(p) ≃|p|/|xn −p|. To compute Knum(p)
we notice that, in the case when the algorithm converges, the solution xn would
converge to one of the roots x+ or x−; therefore, |xn −p| →

p2 −1 and thus
Kn(p) →Knum(p) ≃|p|/

p2 −1, in perfect agreement with the value (2.8) of
the condition number of the exact problem.
We can conclude that Newton’s method for the search of simple roots of a
second order algebraic equation is ill conditioned if |p| is very close to 1, while it
is well conditioned in the other cases.
•
The ﬁnal goal of numerical approximation is, of course, to build, through
numerical problems of the type (2.12), solutions xn that “get closer” to the
solution of problem (2.1) as much as n gets larger. This concept is made
precise in the next deﬁnition.
Deﬁnition 2.2 The numerical method (2.12) is convergent iﬀ
∀ε > 0 ∃n0(ε), ∃δ(n0, ε) > 0 :
∀n > n0(ε), ∀∥δdn∥< δ(n0, ε)
⇒∥x(d) −xn(d + δdn)∥≤ε,
(2.20)

40
2. Principles of Numerical Mathematics
where d is an admissible datum for the problem (2.1), x(d) is the corre-
sponding solution and xn(d+δdn) is the solution of the numerical problem
(2.12) with datum d + δdn.
■
To verify the implication (2.20) it suﬃces to check that under the same
assumptions
∥x(d + δdn) −xn(d + δdn)∥≤ε
2.
(2.21)
Indeed, thanks to (2.3) we have
∥x(d) −xn(d + δdn)∥≤∥x(d) −x(d + δdn)∥
+∥x(d + δdn) −xn(d + δdn)∥≤K(δ(n0, ε), d)∥δdn∥+ ε
2.
Choosing δdn such that K(δ(n0, ε), d)∥δdn∥< ε
2, one obtains (2.20).
Measures of the convergence of xn to x are given by the absolute error
or the relative error, respectively deﬁned as
E(xn) = |x −xn|,
Erel(xn) = |x −xn|
|x|
,
(if x ̸= 0).
(2.22)
In the cases where x and xn are matrix or vector quantities, in addition
to the deﬁnitions in (2.22) (where the absolute values are substituted by
suitable norms) it is sometimes useful to introduce the error by component
deﬁned as
Ec
rel(xn) = max
i,j
|(x −xn)ij|
|xij|
.
(2.23)
2.2.1
Relations between Stability and Convergence
The concepts of stability and convergence are strongly connected.
First of all, if problem (2.1) is well posed, a necessary condition in order
for the numerical problem (2.12) to be convergent is that it is stable.
Let us thus assume that the method is convergent, and prove that it is
stable by ﬁnding a bound for ∥δxn∥. We have
∥δxn∥
=
∥xn(d + δdn) −xn(d)∥≤∥xn(d) −x(d)∥
+
∥x(d) −x(d + δdn)∥+ ∥x(d + δdn) −xn(d + δdn)∥
≤
K(δ(n0, ε), d)∥δdn∥+ ε,
(2.24)
having used (2.3) and (2.21) twice. From (2.24) we can conclude that, for n
suﬃciently large, ∥δxn∥/∥δdn∥can be bounded by a constant of the order

2.3 A priori and a posteriori Analysis
41
of K(δ(n0, ε), d), so that the method is stable. Thus, we are interested in
stable numerical methods since only these can be convergent.
The stability of a numerical method becomes a suﬃcient condition for
the numerical problem (2.12) to converge if this latter is also consistent
with problem (2.1). Indeed, under these assumptions we have
∥x(d + δdn) −xn(d + δdn)∥
≤
∥x(d + δdn) −x(d)∥
+
∥x(d) −xn(d)∥+ ∥xn(d) −xn(d + δdn)∥.
Thanks to (2.3), the ﬁrst term at right-hand side can be bounded by ∥δdn∥
(up to a multiplicative constant independent of δdn). A similar bound holds
for the third term, due to the stability property (2.16). Finally, concerning
the remaining term, if Fn is diﬀerentiable with respect to the variable x,
an expansion in a Taylor series gives
Fn(x(d), d) −Fn(xn(d), d) = ∂Fn
∂x |(x,d)(x(d) −xn(d)),
for a suitable x “between” x(d) and xn(d). Assuming also that ∂Fn/∂x is
invertible, we get
x(d) −xn(d) =
∂Fn
∂x
−1
|(x,d)
[Fn(x(d), d) −Fn(xn(d), d)].
(2.25)
On the other hand, replacing Fn(xn(d), d) with F(x(d), d) (since both terms
are equal to zero) and passing to the norms, we ﬁnd
∥x(d) −xn(d)∥≤
!!!!!
∂Fn
∂x
−1
|(x,d)
!!!!! ∥Fn(x(d), d) −F(x(d), d)∥.
Thanks to (2.13) we can thus conclude that ∥x(d)−xn(d)∥→0 for n →∞.
The result that has just been proved, although stated in qualitative terms,
is a milestone in numerical analysis, known as equivalence theorem (or
Lax-Richtmyer theorem): “for a consistent numerical method, stability is
equivalent to convergence”. A rigorous proof of this theorem is available in
[Dah56] for the case of linear Cauchy problems, or in [Lax65] and in [RM67]
for linear well-posed initial value problems.
2.3
A priori and a posteriori Analysis
The stability analysis of a numerical method can be carried out following
diﬀerent strategies:
1. forward analysis, which provides a bound to the variations ∥δxn∥on
the solution due to both perturbations in the data and to errors that
are intrinsic to the numerical method;

42
2. Principles of Numerical Mathematics
2. backward analysis, which aims at estimating the perturbations that
should be “impressed” to the data of a given problem in order to
obtain the results actually computed under the assumption of working
in exact arithmetic. Equivalently, given a certain computed solution
xn, backward analysis looks for the perturbations δdn on the data
such that Fn(xn, dn + δdn) = 0. Notice that, when performing such
an estimate, no account at all is taken into the way xn has been
obtained (that is, which method has been employed to generate it).
Forward and backward analyses are two diﬀerent instances of the so
called a priori analysis. This latter can be applied to investigate not only
the stability of a numerical method, but also its convergence. In this case
it is referred to as a priori error analysis, which can again be performed
using either a forward or a backward technique.
A priori error analysis is distincted from the so called a posteriori error
analysis, which aims at producing an estimate of the error on the grounds
of quantities that are actually computed by a speciﬁc numerical method.
Typically, denoting by xn the computed numerical solution, approximation
to the solution x of problem (2.1), the a posteriori error analysis aims at
evaluating the error x −xn as a function of the residual rn = F(xn, d) by
means of constants that are called stability factors (see [EEHJ96]).
Example 2.8 For the sake of illustration, consider the problem of ﬁnding the
zeros α1, . . . , αn of a polynomial pn(x) = n
k=0 akxk of degree n.
Denoting by ˜pn(x) = n
k=0 ˜akxk a perturbed polynomial whose zeros are ˜αi,
forward analysis aims at estimating the error between two corresponding zeros
αi and ˜αi, in terms of the variations on the coeﬃcients ak −˜ak, k = 0, 1, . . . , n.
On the other hand, let {ˆαi} be the approximate zeros of pn (computed some-
how). Backward analysis provides an estimate of the perturbations δak which
should be impressed to the coeﬃcients so that n
k=0(ak +δak)ˆαk
i = 0, for a ﬁxed
ˆαi. The goal of a posteriori error analysis would rather be to provide an estimate
of the error αi −ˆαi as a function of the residual value pn(ˆαi).
This analysis will be carried out in Section 6.1.
•
Example 2.9 Consider the linear system Ax=b, where A∈Rn×n is a nonsin-
gular matrix.
For the perturbed system ˜A˜x = ˜b, forward analysis provides an estimate of
the error x −˜x in terms of A −˜A and b −˜b, while backward analysis estimates
the perturbations δA = (δaij) and δb = (δbi) which should be impressed to the
entries of A and b in order to get (A + δA)xn = b + δb, xn being the solution of
the linear system (computed somehow). Finally, a posteriori error analysis looks
for an estimate of the error x −xn as a function of the residual rn = b −Axn.
We will develop this analysis in Section 3.1.
•
It is important to point out the role played by the a posteriori analysis in
devising strategies for adaptive error control. These strategies, by suitably
changing the discretization parameters (for instance, the spacing between

2.4 Sources of Error in Computational Models
43
nodes in the numerical integration of a function or a diﬀerential equation),
employ the a posteriori analysis in order to ensure that the error does not
exceed a ﬁxed tolerance.
A numerical method that makes use of an adaptive error control is called
adaptive numerical method. In practice, a method of this kind applies in the
computational process the idea of feedback, by activating on the grounds of
a computed solution a convergence test which ensures the control of error
within a ﬁxed tolerance. In case the convergence test fails, a suitable strat-
egy for modifying the discretization parameters is automatically adopted
in order to enhance the accuracy of the solution to be newly computed,
and the overall procedure is iterated until the convergence check is passed.
2.4
Sources of Error in Computational Models
Whenever the numerical problem (2.12) is an approximation to the math-
ematical problem (2.1) and this latter is in turn a model of a physical
problem (which will be shortly denoted by PP), we shall say that (2.12) is
a computational model for PP.
In this process the global error, denoted by e, is expressed by the dif-
ference between the actually computed solution, xn, and the physical so-
lution, xph, of which x provides a model. The global error e can thus be
interpreted as being the sum of the error em of the mathematical model,
given by x−xph, and the error ec of the computational model, xn −x, that
is e = em + ec (see Figure 2.1).
PP : xph
F(x, d) = 0
Fn(xn, dn) = 0
em
xn
e
ea
ec
en
FIGURE 2.1. Errors in computational models
The error em will in turn take into account the error of the mathematical
model in strict sense (that is, the extent at which the functional equation
(2.1) does realistically describe the problem PP) and the error on the data
(that is, how much accurately does d provide a measure of the real physical

44
2. Principles of Numerical Mathematics
data). In the same way, ec turns out to be the combination of the numerical
discretization error en = xn −x, the error ea introduced by the numerical
algorithm and the roundoﬀerror introduced by the computer during the
actual solution of problem (2.12) (see Section 2.5).
In general, we can thus outline the following sources of error:
1. errors due to the model, that can be controlled by a proper choice of
the mathematical model;
2. errors in the data, that can be reduced by enhancing the accuracy in
the measurement of the data themselves;
3. truncation errors, arising from having replaced in the numerical model
limits by operations that involve a ﬁnite number of steps;
4. rounding errors.
The errors at the items 3. and 4. give rise to the computational error. A
numerical method will thus be convergent if this error can be made arbi-
trarily small by increasing the computational eﬀort. Of course, convergence
is the primary, albeit not unique, goal of a numerical method, the others
being accuracy, reliability and eﬃciency.
Accuracy means that the errors are small with respect to a ﬁxed tol-
erance. It is usually quantiﬁed by the order of inﬁnitesimal of the error
en with respect to the discretization characteristic parameter (for instance
the largest grid spacing between the discretization nodes). By the way, we
notice that machine precision does not limit, on theoretical grounds, the
accuracy.
Reliability means it is likely that the global error can be guaranteed to be
below a certain tolerance. Of course, a numerical model can be considered
to be reliable only if suitably tested, that is, successfully applied to several
test cases.
Eﬃciency means that the computational complexity that is needed to
control the error (that is, the amount of operations and the size of the
memory required) is as small as possible.
Having encountered the term algorithm several times in this section, we
cannot refrain from providing an intuitive description of it. By algorithm
we mean a directive that indicates, through elementary operations, all the
passages that are needed to solve a speciﬁc problem. An algorithm can in
turn contain sub-algorithms and must have the feature of terminating after
a ﬁnite number of elementary operations. As a consequence, the executor
of the algorithm (machine or human being) must ﬁnd within the algorithm
itself all the instructions to completely solve the problem at hand (provided
that the necessary resources for its execution are available).
For instance, the statement that a polynomial of second degree surely
admits two roots in the complex plane does not characterize an algorithm,

2.5 Machine Representation of Numbers
45
whereas the formula yielding the roots is an algorithm (provided that the
sub-algorithms needed to correctly execute all the operations have been
deﬁned in turn).
Finally, the complexity of an algorithm is a measure of its executing
time. Calculating the complexity of an algorithm is therefore a part of the
analysis of the eﬃciency of a numerical method. Since several algorithms,
with diﬀerent complexities, can be employed to solve the same problem P,
it is useful to introduce the concept of complexity of a problem, this latter
meaning the complexity of the algorithm that has minimum complexity
among those solving P. The complexity of a problem is typically measured
by a parameter directly associated with P. For instance, in the case of
the product of two square matrices, the computational complexity can be
expressed as a function of a power of the matrix size n (see, [Str69]).
2.5
Machine Representation of Numbers
Any machine operation is aﬀected by rounding errors or roundoﬀ. They are
due to the fact that on a computer only a ﬁnite subset of the set of real
numbers can be represented. In this section, after recalling the positional
notation of real numbers, we introduce their machine representation.
2.5.1
The Positional System
Let a base β ∈N be ﬁxed with β ≥2, and let x be a real number with
a ﬁnite number of digits xk with 0 ≤xk < β for k = −m, . . . , n. The
notation (conventionally adopted)
xβ = (−1)s [xnxn−1 . . . x1x0.x−1x−2 . . . x−m] ,
xn ̸= 0
(2.26)
is called the positional representation of x with respect to the base β. The
point between x0 and x−1 is called decimal point if the base is 10, binary
point if the base is 2, while s depends on the sign of x (s = 0 if x is positive,
1 if negative). Relation (2.26) actually means
xβ = (−1)s

n

k=−m
xkβk

.
Example 2.10 The conventional writing x10 = 425.33 denotes the number x =
4 · 102 + 2 · 10 + 5 + 3 · 10−1 + 3 · 10−2, while x6 = 425.33 would denote the
real number x = 4 · 62 + 2 · 6 + 5 + 3 · 6−1 + 3 · 6−2. A rational number can of
course have a ﬁnite number of digits in a base and an inﬁnite number of digits in
another base. For example, the fraction 1/3 has inﬁnite digits in base 10, being
x10 = 0.¯3, while it has only one digit in base 3, being x3 = 0.1.
•

46
2. Principles of Numerical Mathematics
Any real number can be approximated by numbers having a ﬁnite repre-
sentation. Indeed, having ﬁxed the base β, the following property holds
∀ε > 0, ∀xβ ∈R, ∃yβ ∈R such that |yβ −xβ| < ε,
where yβ has ﬁnite positional representation.
In fact, given the positive number xβ = xnxn−1 . . . x0.x−1 . . . x−m . . . with
a number of digits, ﬁnite or inﬁnite, for any r ≥1 one can build two
numbers
x(l)
β =
r−1

k=0
xn−kβn−k,
x(u)
β
= x(l)
β + βn−r+1,
having r digits, such that x(l)
β
< xβ < x(u)
β
and x(u)
β
−x(l)
β
= βn−r+1. If
r is chosen in such a way that βn−r+1 < ϵ, then taking yβ equal to x(l)
β
or x(u)
β
yields the desired inequality. This result legitimates the computer
representation of real numbers (and thus by a ﬁnite number of digits).
Although theoretically speaking all the bases are equivalent, in the com-
putational practice three are the bases generally employed: base 2 or binary,
base 10 or decimal (the most natural) and base 16 or hexadecimal. Almost
all modern computers use base 2, apart from a few which traditionally
employ base 16. In what follows, we will assume that β is an even integer.
In the binary representation, digits reduce to the two symbols 0 and 1,
called bits (binary digits), while in the hexadecimal case the symbols used
for the representation of the digits are 0,1,...,9,A,B,C,D,E,F. Clearly,
the smaller the adopted base, the longer the string of characters needed to
represent the same number.
To simplify notations, we shall write x instead of xβ, leaving the base β
understood.
2.5.2
The Floating-point Number System
Assume a given computer has N memory positions in which to store any
number. The most natural way to make use of these positions in the rep-
resentation of a real number x diﬀerent from zero is to ﬁx one of them for
its sign, N −k −1 for the integer digits and k for the digits beyond the
point, in such a way that
x = (−1)s · [aN−2aN−3 . . . ak . ak−1 . . . a0]
(2.27)
s being equal to 1 or 0. Notice that one memory position is equivalent to
one bit storage only when β = 2. The set of numbers of this kind is called
ﬁxed-point system. Equation (2.27) stands for
x = (−1)s · β−k
N−2

j=0
ajβj
(2.28)

2.5 Machine Representation of Numbers
47
and therefore this representation amounts to ﬁxing a scaling factor for all
the representable numbers.
The use of ﬁxed point strongly limits the value of the minimum and maxi-
mum numbers that can be represented on the computer, unless a very large
number N of memory positions is employed. This drawback can be easily
overcome if the scaling in (2.28) is allowed to be varying. In such a case,
given a non vanishing real number x, its ﬂoating-point representation is
given by
x = (−1)s · (0.a1a2 . . . at) · βe = (−1)s · m · βe−t
(2.29)
where t ∈N is the number of allowed signiﬁcant digits ai (with 0 ≤ai ≤
β −1), m = a1a2 . . . at an integer number called mantissa such that 0 ≤
m ≤βt −1 and e an integer number called exponent. Clearly, the exponent
can vary within a ﬁnite interval of admissible values: we let L ≤e ≤U
(typically L < 0 and U > 0). The N memory positions are now distributed
among the sign (one position), the signiﬁcant digits (t positions) and the
digits for the exponent (the remaining N −t −1 positions). The number
zero has a separate representation.
Typically, on the computer there are two formats available for the ﬂoating-
point number representation: single and double precision. In the case of bi-
nary representation, these formats correspond in the standard version to
the representation with N = 32 bits (single precision)
1
s
8 bits
e
23 bits
m
and with N = 64 bits (double precision)
1
s
11 bits
e
52 bits
m
Let us denote by
F(β, t, L, U) = {0} ∪
"
x ∈R : x = (−1)sβe
t

i=1
aiβ−i
#
the set of ﬂoating-point numbers with t signiﬁcant digits, base β ≥2, 0 ≤
ai ≤β −1, and range (L,U) with L ≤e ≤U.
In order to enforce uniqueness in a number representation, it is typi-
cally assumed that a1 ̸= 0 and m ≥βt−1. In such an event a1 is called
the principal signiﬁcant digit, while at is the last signiﬁcant digit and the
representation of x is called normalized. The mantissa m is now varying
between βt−1 and βt −1.
For instance, in the case β = 10, t = 4, L = −1 and U = 4, without
the assumption that a1 ̸= 0, the number 1 would admit the following
representations
0.1000 · 101,
0.0100 · 102,
0.0010 · 103,
0.0001 · 104.

48
2. Principles of Numerical Mathematics
To always have uniqueness in the representation, it is assumed that also
the number zero has its own sign (typically s = 0 is assumed).
It can be immediately noticed that if x ∈F(β, t, L, U) then also −x ∈
F(β, t, L, U). Moreover, the following lower and upper bounds hold for the
absolute value of x
xmin = βL−1 ≤|x| ≤βU(1 −β−t) = xmax.
(2.30)
The cardinality of F(β, t, L, U) (henceforth shortly denoted by F) is
card F = 2(β −1)βt−1(U −L + 1) + 1.
From (2.30) it turns out that it is not possible to represent any number
(apart from zero) whose absolute value is less than xmin. This latter limi-
tation can be overcome by completing F by the set FD of the ﬂoating-point
de-normalized numbers obtained by removing the assumption that a1 is
non null, only for the numbers that are referred to the minimum exponent
L. In such a way the uniqueness in the representation is not lost and it is
possible to generate numbers that have mantissa between 1 and βt−1 −1
and belong to the interval (−βL−1, βL−1). The smallest number in this set
has absolute value equal to βL−t.
Example 2.11 The positive numbers in the set F(2, 3, −1, 2) are
(0.111) · 22 = 7
2,
(0.110) · 22 = 3,
(0.101) · 22 = 5
2,
(0.100) · 22 = 2,
(0.111) · 2 = 7
4,
(0.110) · 2 = 3
2,
(0.101) · 2 = 5
4,
(0.100) · 2 = 1,
(0.111) = 7
8,
(0.110) = 3
4,
(0.101) = 5
8,
(0.100) = 1
2,
(0.111) · 2−1 = 7
16,
(0.110) · 2−1 = 3
8,
(0.101) · 2−1 = 5
16,
(0.100) · 2−1 = 1
4.
They are included between xmin = βL−1 = 2−2 = 1/4 and xmax = βU(1−β−t) =
22(1−2−3) = 7/2. As a whole, we have (β −1)βt−1(U −L+1) = (2−1)23−1(2+
1 + 1) = 16 strictly positive numbers. Their opposites must be added to them,
as well as the number zero. We notice that when β = 2, the ﬁrst signiﬁcant digit
in the normalized representation is necessarily equal to 1 and thus it may not be
stored in the computer (in such an event, we call it hidden bit).
When considering also the positive de-normalized numbers, we should complete
the above set by adding the following numbers
(.011)2 · 2−1 = 3
16,
(.010)2 · 2−1 = 1
8,
(.001)2 · 2−1 = 1
16.
According to what previously stated, the smallest de-normalized number is βL−t =
2−1−3 = 1/16.
•

2.5 Machine Representation of Numbers
49
2.5.3
Distribution of Floating-point Numbers
The ﬂoating-point numbers are not equally spaced along the real line, but
they get dense close to the smallest representable number. It can be checked
that the spacing between a number x ∈F and its next nearest y ∈F, where
both x and y are assumed to be non null, is at least β−1ϵM|x| and at most
ϵM|x|, being ϵM = β1−t the machine epsilon. This latter represents the
distance between the number 1 and the nearest ﬂoating-point number, and
therefore it is the smallest number of F such that 1 + ϵM > 1.
Having instead ﬁxed an interval of the form [βe, βe+1], the numbers of F
that belong to such an interval are equally spaced and have distance equal
to βe−t. Decreasing (or increasing) by one the exponent gives rise to a
decrement (or increment) of a factor β of the distance between consecutive
numbers.
Unlike the absolute distance, the relative distance between two consecu-
tive numbers has a periodic behavior which depends only on the mantissa
m. Indeed, denoting by (−1)sm(x)βe−t one of the two numbers, the dis-
tance ∆x from the successive one is equal to (−1)sβe−t, which implies that
the relative distance is
∆x
x
=
(−1)sβe−t
(−1)sm(x)βe−t =
1
m(x).
(2.31)
Within the interval [βe, βe+1], the ratio in (2.31) is decreasing as x increases
since in the normalized representation the mantissa varies from βt−1 to βt−
1 (not included). However, as soon as x = βe+1, the relative distance gets
back to the value β−t+1 and starts decreasing on the successive intervals,
as shown in Figure 2.2. This oscillatory phenomenon is called wobbling
precision and the greater the base β, the more pronounced the eﬀect. This
is another reason why small bases are preferably employed in computers.
2.5.4
IEC/IEEE Arithmetic
The possibility of building sets of ﬂoating-point numbers that diﬀer in base,
number of signiﬁcant digits and range of the exponent has prompted in the
past the development, for almost any computer, of a particular system F. In
order to avoid this proliferation of numerical systems, a standard has been
ﬁxed that is nowadays almost universally accepted. This standard was de-
veloped in 1985 by the Institute of Electrical and Electronics Engineers
(shortly, IEEE) and was approved in 1989 by the International Electroni-
cal Commission (IEC) as the international standard IEC559 and it is now
known by this name (IEC is an organization analogue to the International
Standardization Organization (ISO) in the ﬁeld of electronics). The stan-
dard IEC559 endorses two formats for the ﬂoating-point numbers: a basic
format, made by the system F(2, 24, −125, 128) for the single precision,
and by F(2, 53, −1021, 1024) for the double precision, both including the

50
2. Principles of Numerical Mathematics
2
-126
2
-125
2
-124
2
-123
2
2
-24
-23
FIGURE
2.2.
Variation
of
relative
distance
for
the
set
of
numbers
F(2, 24, −125, 128) IEC/IEEE in single precision
de-normalized numbers, and an extended format, for which only the main
limitations are ﬁxed (see Table 2.1).
single
double
single
double
N
≥43 bits
≥79 bits
t
≥32
≥64
L
≤−1021
≤16381
U
≥1024
≥16384
TABLE 2.1. Lower or upper limits in the standard IEC559 for the extended
format of ﬂoating-point numbers
Almost all the computers nowadays satisfy the requirements above. We
summarize in Table 2.2 the special codings that are used in IEC559 to
deal with the values ±0, ±∞and with the so-called non numbers (shortly,
NaN, that is not a number), which correspond for instance to 0/0 or to
other exceptional operations.
value
exponent
mantissa
±0
L −1
0
±∞
U + 1
0
NaN
U + 1
̸= 0
TABLE 2.2. IEC559 codings of some exceptional values
2.5.5
Rounding of a Real Number in its Machine
Representation
The fact that on any computer only a subset F(β, t, L, U) of R is actually
available poses several practical problems, ﬁrst of all the representation in F

2.5 Machine Representation of Numbers
51
of any given real number. To this concern, notice that, even if x and y were
two numbers in F, the result of an operation on them does not necessarily
belong to F. Therefore, we must deﬁne an arithmetic also on F.
The simplest approach to solve the ﬁrst problem consists of rounding
x ∈R in such a way that the rounded number belongs to F. Among all
the possible rounding operations, let us consider the following one. Given
x ∈R in the normalized positional notation (2.29) let us substitute x by
its representant fl(x) in F, deﬁned as
fl(x) = (−1)s(0. a1a2 . . . $at) · βe,
˜at =
%
at
if at+1 < β/2
at + 1
if at+1 ≥β/2. (2.32)
The mapping fl : R →F is the most commonly used and is called rounding
(in the chopping one would take more trivially $at = at). Clearly, fl(x) = x
if x ∈F and moreover fl(x) ≤fl(y) if x ≤y ∀x, y ∈R (monotonicity
property).
Remark 2.2 (Overﬂow and underﬂow) Everything written so far holds
only for the numbers that in (2.29) have exponent e within the range of F.
If, indeed, x ∈(−∞, −xmax) ∪(xmax, ∞) the value fl(x) is not deﬁned,
while if x ∈(−xmin, xmin) the operation of rounding is deﬁned anyway
(even in absence of de-normalized numbers). In the ﬁrst case, if x is the
result of an operation on numbers of F, we speak about overﬂow, in the
second case about underﬂow (or graceful underﬂow if de-normalized num-
bers are accounted for). The overﬂow is handled by the system through an
interrupt of the executing program.
■
Apart from exceptional situations, we can easily quantify the error, ab-
solute and relative, that is made by substituting fl(x) for x. The following
result can be shown (see for instance [Hig96], Theorem 2.2).
Property 2.1 If x ∈R is such that xmin ≤|x| ≤xmax, then
fl(x) = x(1 + δ) with |δ| ≤u
(2.33)
where
u = 1
2β1−t = 1
2ϵM
(2.34)
is the so-called roundoﬀunit (or machine precision).
As a consequence of (2.33), the following bound holds for the relative error
Erel(x) = |x −fl(x)|
|x|
≤u,
(2.35)
while, for the absolute error, one gets
E(x) = |x −fl(x)| ≤βe−t|(a1 . . . at.at+1 . . . ) −(a1 . . . ˜at)|.

52
2. Principles of Numerical Mathematics
From (2.32), it follows that
|(a1 . . . at.at+1 . . . ) −(a1 . . . ˜at)| ≤β−1 β
2 ,
from which
E(x) ≤1
2β−t+e.
Remark 2.3 In the MATLAB environment it is possible to know imme-
diately the value of ϵM, which is given by the system variable eps.
■
2.5.6
Machine Floating-point Operations
As previously stated, it is necessary to deﬁne on the set of machine numbers
an arithmetic which is analogous, as far as possible, to the arithmetic in R.
Thus, given any arithmetic operation ◦: R × R →R on two operands in
R (the symbol ◦may denote sum, subtraction, multiplication or division),
we shall denote by ◦the corresponding machine operation
◦: R × R →F,
x ◦y = fl(fl(x) ◦fl(y)).
From the properties of ﬂoating-point numbers one could expect that for the
operations on two operands, whenever well deﬁned, the following property
holds: ∀x, y ∈F, ∃δ ∈R such that
x ◦y = (x ◦y)(1 + δ)
with |δ| ≤u.
(2.36)
In order for (2.36) to be satisﬁed when ◦is the operator of subtraction, it
will require an additional assumption on the structure of the numbers in F,
that is the presence of the so-called round digit (which is addressed at the
end of this section). In particular, when ◦is the sum operator, it follows
that for all x, y ∈F (see Exercise 11)
|x + y −(x + y)|
|x + y|
≤u(1 + u)|x| + |y|
|x + y| + u,
(2.37)
so that the relative error associated with every machine operation will be
small, unless x + y is not small by itself. An aside comment is deserved by
the case of the sum of two numbers close in module, but opposite in sign.
In fact, in such a case x+y can be quite small, this generating the so-called
cancellation errors (as evidenced in Example 2.6).
It is important to notice that, together with properties of standard arith-
metic that are preserved when passing to ﬂoating-point arithmetic (like, for
instance, the commutativity of the sum of two addends, or the product of

2.5 Machine Representation of Numbers
53
two factors), other properties are lost. An example is given by the associa-
tivity of sum: it can indeed be shown (see Exercise 12) that in general
x + (y + z) ̸= (x + y) + z.
We shall denote by ﬂop the single elementary ﬂoating-point operation (sum,
subtraction, multiplication or division) (the reader is warned that in some
texts ﬂop identiﬁes an operation of the form a + b · c). According to the
previous convention, a scalar product between two vectors of length n will
require 2n −1 ﬂops, a product matrix-vector 2(m −1)n ﬂops if the matrix
is n × m and ﬁnally, a product matrix-matrix 2(r −1)mn ﬂops if the two
matrices are m × r and r × n respectively.
Remark 2.4 (IEC559 arithmetic) The IEC559 standard also deﬁnes a
closed arithmetic on F, this meaning that any operation on it produces
a result that can be represented within the system itself, although not
necessarily being expected from a pure mathematical standpoint. As an
example, in Table 2.3 we report the results that are obtained in exceptional
situations.
exception
examples
result
non valid operation
0/0, 0 · ∞
NaN
overflow
±∞
division by zero
1/0
±∞
underflow
subnormal numbers
TABLE 2.3. Results for some exceptional operations
The presence of a NaN (Not a Number) in a sequence of operations au-
tomatically implies that the result is a NaN. General acceptance of this
standard is still ongoing.
■
We mention that not all the ﬂoating-point systems satisfy (2.36). One of
the main reasons is the absence of the round digit in subtraction, that is,
an extra-bit that gets into action on the mantissa level when the subtrac-
tion between two ﬂoating-point numbers is performed. To demonstrate the
importance of the round digit, let us consider the following example with
a system F having β = 10 and t = 2. Let us subtract 1 and 0.99. We have
101 · 0.1
101 · 0.10
100 · 0.99
⇒
101 · 0.09
101 · 0.01
−→
100 · 0.10
that is, the result diﬀers from the exact one by a factor 10. If we now
execute the same subtraction using the round digit, we obtain the exact

54
2. Principles of Numerical Mathematics
result. Indeed
101 · 0.1
101 · 0.10
100 · 0.99
⇒
101 · 0.09 9
101 · 0.00 1
−→
100 · 0.01
In fact, it can be shown that addition and subtraction, if executed without
round digit, do not satisfy the property
fl(x ± y) = (x ± y)(1 + δ) with |δ| ≤u,
but the following one
fl(x ± y) = x(1 + α) ± y(1 + β) with |α| + |β| ≤u.
An arithmetic for which this latter event happens is called aberrant. In some
computers the round digit does not exist, most of the care being spent on
velocity in the computation. Nowadays, however, the trend is to use even
two round digits (see [HP94] for technical details about the subject).
2.6
Exercises
1. Use (2.7) to compute the condition number K(d) of the following expres-
sions
(1)
x −ad = 0, a > 0
(2)
d −x + 1 = 0,
d being the datum, a a parameter and x the “unknown”.
[Solution : (1) K(d) ≃|d|| log a|, (2) K(d) = |d|/|d + 1|.]
2. Study the well posedness and the conditioning in the inﬁnity norm of the
following problem as a function of the datum d: ﬁnd x and y such that
% x + dy = 1,
dx + y = 0.
[Solution : the given problem is a linear system whose matrix is A =
 1
d
d
1

. It is well-posed if A is nonsingular, i.e., if d ̸= ±1. In such a
case, K∞(A) = |(|d| + 1)/(|d| −1)|.]
3. Study the conditioning of the solving formula x± = −p ±

p2 + q for
the second degree equation x2 + 2px −q with respect to changes in the
parameters p and q separately.
[Solution : K(p) = |p|/

p2 + q, K(q) = |q|/(2|x±|

p2 + q).]
4. Consider the following Cauchy problem
% x′(t) = x0eat (a cos(t) −sin(t)) ,
t > 0
x(0) = x0
(2.38)

2.6 Exercises
55
whose solution is x(t) = x0eat cos(t) (a is a given real number). Study the
conditioning of (2.38) with respect to the choice of the initial datum and
check that on unbounded intervals it is well conditioned if a < 0, while it
is ill conditioned if a > 0.
[Hint : consider the deﬁnition of Kabs(a).]
5. Let x ̸= 0 be an approximation of a non null quantity x. Find the relation
between the relative error ϵ = |x −x|/|x| and ˜E = |x −x|/|x|.
6. Find a stable formula for evaluating the square root of a complex number.
7. Determine all the elements of the set F = (10, 6, −9, 9), in both normalized
and de-normalized cases.
8. Consider the set of the de-normalized numbers FD and study the behavior
of the absolute distance and of the relative distance between two of these
numbers. Does the wobbling precision eﬀect arise again?
[Hint : for these numbers, uniformity in the relative density is lost. As a
consequence, the absolute distance remains constant (equal to βL−t), while
the relative one rapidly grows as x tends to zero.]
9. What is the value of 00 in IEEE arithmetic?
[Solution : ideally, the outcome should be NaN. In practice, IEEE systems
recover the value 1. A motivation of this result can be found in [Gol91].]
10. Show that, due to cancellation errors, the following sequence
I0 = log 6
5,
Ik + 5Ik−1 = 1
k ,
k = 1, 2, . . . , n,
(2.39)
is not well suited to ﬁnite arithmetic computations of the integral In =
 1
0
xn
x + 5dx when n is suﬃciently large, although it works in inﬁnite arith-
metic.
[Hint : consider the initial perturbed datum ˜I0 = I0 + µ0 and study the
propagation of the error µ0 within (2.39).]
11. Prove (2.37).
[Solution : notice that
|x + y −(x + y)|
|x + y|
≤
|x + y −(fl(x) + fl(y))|
|x + y|
+ |fl(x) −x + fl(y) −y|
|x + y|
.
Then, use (2.36) and (2.35).]
12. Given x, y, z ∈F with x + y, y + z, x + y + z that fall into the range of F,
show that
|(x + y) + z −(x + y + z)| ≤C1 ≃(2|x + y| + |z|)u
|x + (y + z) −(x + y + z)| ≤C2 ≃(|x| + 2|y + z|)u.
13. Which among the following approximations of π,
π = 4

1 −1
3 + 1
5 −1
7 + 1
9 −. . .

,
π = 6

0.5 + (0.5)3
2 · 3 + 3(0.5)5
2 · 4 · 5 + 3 · 5(0.5)7
2 · 4 · 6 · 7 + . . .

(2.40)

56
2. Principles of Numerical Mathematics
better limits the propagation of rounding errors? Compare using MATLAB
the obtained results as a function of the number of the terms in each sum
in (2.40).
14. Analyze the stability, with respect to propagation of rounding errors, of the
following two MATLAB codes to evaluate f(x) = (ex −1)/x for |x| ≪1
% Algorithm 1
if x == 0
f = 1;
else
f = (exp(x) - 1) / x;
end
% Algorithm 2
y = exp (x);
if y == 1
f = 1;
else
f = (y - 1) / log (y);
end
[Solution : the ﬁrst algorithm is inaccurate due to cancellation errors, while
the second one (in presence of round digit) is stable and accurate.]
15. In binary arithmetic one can show [Dek71] that the rounding error in the
sum of two numbers a and b, with a ≥b, can be computed as
((a + b) −a) −b).
Based on this property, a method has been proposed, called Kahan com-
pensated sum, to compute the sum of n addends ai in such a way that the
rounding errors are compensated. In practice, letting the initial rounding
error e1 = 0 and s1 = a1, at the i-th step, with i ≥2, the algorithm
evaluates yi = xi −ei−1, the sum is updated setting si = si−1 + yi and
the new rounding error is computed as ei = (si −si−1) −yi. Implement
this algorithm in MATLAB and check its accuracy by evaluating again the
second expression in (2.40).
16. The area A(T) of a triangle T with sides a, b and c, can be computed using
the following formula
A(T) =

p(p −a)(p −b)(p −c),
where p is half the perimeter of T. Show that in the case of strongly de-
formed triangles (a ≃b + c), this formula lacks accuracy and check this
experimentally.

3
Direct Methods for the Solution of
Linear Systems
A system of m linear equations in n unknowns consists of a set of algebraic
relations of the form
n

j=1
aijxj = bi,
i = 1, . . . , m
(3.1)
where xj are the unknowns, aij are the coeﬃcients of the system and bi
are the components of the right hand side. System (3.1) can be more con-
veniently written in matrix form as
Ax = b,
(3.2)
where we have denoted by A = (aij) ∈Cm×n the coeﬃcient matrix, by
b=(bi) ∈Cm the right side vector and by x=(xi) ∈Cn the unknown
vector, respectively. We call a solution of (3.2) any n-tuple of values xi
which satisﬁes (3.1).
In this chapter we shall be mainly dealing with real-valued square systems
of order n, that is, systems of the form (3.2) with A ∈Rn×n and b ∈Rn.
In such cases existence and uniqueness of the solution of (3.2) are ensured
if one of the following (equivalent) hypotheses holds:
1. A is invertible;
2. rank(A)=n;
3. the homogeneous system Ax=0 admits only the null solution.

58
3. Direct Methods for the Solution of Linear Systems
The solution of system (3.2) is formally provided by Cramer’s rule
xj =
∆j
det(A),
j = 1, . . . , n,
(3.3)
where ∆j is the determinant of the matrix obtained by substituting the
j-th column of A with the right hand side b. This formula is, however,
of little practical use. Indeed, if the determinants are evaluated by the
recursive relation (1.4), the computational eﬀort of Cramer’s rule is of the
order of (n + 1)! ﬂops and therefore turns out to be unacceptable even for
small dimensions of A (for instance, a computer able to perform 109 ﬂops
per second would take 9.6 · 1047 years to solve a linear system of only 50
equations).
For this reason, numerical methods that are alternatives to Cramer’s rule
have been developed. They are called direct methods if they yield the so-
lution of the system in a ﬁnite number of steps, iterative if they require
(theoretically) an inﬁnite number of steps. Iterative methods will be ad-
dressed in the next chapter. We notice from now on that the choice between
a direct and an iterative method does not depend only on the theoretical ef-
ﬁciency of the scheme, but also on the particular type of matrix, on memory
storage requirements and, ﬁnally, on the architecture of the computer.
3.1
Stability Analysis of Linear Systems
Solving a linear system by a numerical method invariably leads to the
introduction of rounding errors. Only using stable numerical methods can
keep away the propagation of such errors from polluting the accuracy of the
solution. In this section two aspects of stability analysis will be addressed.
Firstly, we will analyze the sensitivity of the solution of (3.2) to changes
in the data A and b (forward a priori analysis). Secondly, assuming that
an approximate solution x of (3.2) is available, we shall quantify the per-
turbations on the data A and b in order for x to be the exact solution
of a perturbed system (backward a priori analysis). The size of these per-
turbations will in turn allow us to measure the accuracy of the computed
solution x by the use of a posteriori analysis.
3.1.1
The Condition Number of a Matrix
The condition number of a matrix A ∈Cn×n is deﬁned as
K(A) = ∥A∥∥A−1∥,
(3.4)
where ∥· ∥is an induced matrix norm. In general K(A) depends on the
choice of the norm; this will be made clear by introducing a subscript

3.1 Stability Analysis of Linear Systems
59
into the notation, for instance, K∞(A) = ∥A∥∞∥A−1∥∞. More generally,
Kp(A) will denote the condition number of A in the p-norm. Remarkable
instances are p = 1, p = 2 and p = ∞(we refer to Exercise 1 for the
relations among K1(A), K2(A) and K∞(A)).
As already noticed in Example 2.3, an increase in the condition number
produces a higher sensitivity of the solution of the linear system to changes
in the data. Let us start by noticing that K(A) ≥1 since
1 = ∥AA−1∥≤∥A∥∥A−1∥= K(A).
Moreover, K(A−1) = K(A) and ∀α ∈C with α ̸= 0, K(αA) = K(A).
Finally, if A is orthogonal, K2(A) = 1 since ∥A∥2 =

ρ(AT A) =

ρ(I) = 1
and A−1 = AT . The condition number of a singular matrix is set equal to
inﬁnity.
For p = 2, K2(A) can be characterized as follows. Starting from (1.21),
it can be proved that
K2(A) = ∥A∥2 ∥A−1∥2 = σ1(A)
σn(A)
where σ1(A) and σn(A) are the maximum and minimum singular values of
A (see Property 1.7). As a consequence, in the case of symmetric positive
deﬁnite matrices we have
K2(A) = λmax
λmin
= ρ(A)ρ(A−1)
(3.5)
where λmax and λmin are the maximum and minimum eigenvalues of A.
To check (3.5), notice that
∥A∥2 =

ρ(AT A) =

ρ(A2) =

λ2max = λmax.
Moreover, since λ(A−1) = 1/λ(A), one gets ∥A−1∥2 = 1/λmin from which
(3.5) follows. For that reason, K2(A) is called spectral condition number.
Remark 3.1 Deﬁne the relative distance of A ∈Cn×n from the set of
singular matrices with respect to the p-norm by
distp(A) = min
%∥δA∥p
∥A∥p
: A + δA is singular
&
.
It can then be shown that ([Kah66], [Gas83])
distp(A) =
1
Kp(A).
(3.6)
Equation (3.6) suggests that a matrix A with a high condition number
can behave like a singular matrix of the form A+δA. In other words, null

60
3. Direct Methods for the Solution of Linear Systems
perturbations in the right hand side do not necessarily yield non vanishing
changes in the solution since, if A+δA is singular, the homogeneous system
(A + δA)z = 0 does no longer admit only the null solution. From (3.6) it
also follows that if A+δA is nonsingular then
∥δA∥p∥A∥p < 1.
(3.7)
■
Relation (3.6) seems to suggest that a natural candidate for measuring
the ill-conditioning of a matrix is its determinant, since from (3.3) one is
prompted to conclude that small determinants mean nearly-singular matri-
ces. However this conclusion is wrong, as there exist examples of matrices
with small (respectively, high) determinants and small (respectively, high)
condition numbers (see Exercise 2).
3.1.2
Forward a priori Analysis
In this section we introduce a measure of the sensitivity of the system to
changes in the data. These changes will be interpreted in Section 3.10 as
being the eﬀects of rounding errors induced by the numerical method used
to solve the system. For a more comprehensive analysis of the subject we
refer to [Dat95], [GL89], [Ste73] and [Var62].
Due to rounding errors, a numerical method for solving (3.2) does not
provide the exact solution but only an approximate one, which satisﬁes a
perturbed system. In other words, a numerical method yields an (exact)
solution x + δx of the perturbed system
(A + δA)(x + δx) = b + δb.
(3.8)
The next result provides an estimate of δx in terms of δA and δb.
Theorem 3.1 Let A ∈Rn×n be a nonsingular matrix and δA ∈Rn×n be
such that (3.7) is satisﬁed for a matrix norm ∥· ∥. Then, if x∈Rn is the
solution of Ax=b with b ∈Rn (b ̸= 0) and δx ∈Rn satisﬁes (3.8) for
δb ∈Rn,
∥δx∥
∥x∥≤
K(A)
1 −K(A)∥δA∥/∥A∥
∥δb∥
∥b∥+ ∥δA∥
∥A∥

.
(3.9)
Proof. From (3.7) it follows that the matrix A−1δA has norm less than 1. Then,
due to Theorem 1.5, I + A−1δA is invertible and from (1.26) it follows that
∥(I + A−1δA)−1∥≤
1
1 −∥A−1δA∥≤
1
1 −∥A−1∥∥δA∥.
(3.10)
On the other hand, solving for δx in (3.8) and recalling that Ax = b, one gets
δx = (I + A−1δA)−1A−1(δb −δAx),

3.1 Stability Analysis of Linear Systems
61
from which, passing to the norms and using (3.10), it follows that
∥δx∥≤
∥A−1∥
1 −∥A−1∥∥δA∥(∥δb∥+ ∥δA∥∥x∥) .
Finally, dividing both sides by ∥x∥(which is nonzero since b ̸= 0 and A is
nonsingular) and noticing that ∥x∥≥∥b∥/∥A∥, the result follows.
3
Well-conditioning alone is not enough to yield an accurate solution of the
linear system. It is indeed crucial, as pointed out in Chapter 2, to resort to
stable algorithms. Conversely, ill-conditioning does not necessarily exclude
that for particular choices of the right side b the overall conditioning of the
system is good (see Exercise 4).
A particular case of Theorem 3.1 is the following.
Theorem 3.2 Assume that the conditions of Theorem 3.1 hold and let
δA = 0. Then
1
K(A)
∥δb∥
∥b∥≤∥δx∥
∥x∥≤K(A)∥δb∥
∥b∥.
(3.11)
Proof. We will prove only the ﬁrst inequality since the second one directly
follows from (3.9). Relation δx = A−1δb yields ∥δb∥≤∥A∥∥δx∥. Multiplying
both sides by ∥x∥and recalling that ∥x∥≤∥A−1∥∥b∥it follows that ∥x∥∥δb∥≤
K(A)∥b∥∥δx∥, which is the desired inequality.
3
In order to employ the inequalities (3.10) and (3.11) in the analysis of
propagation of rounding errors in the case of direct methods, ∥δA∥and
∥δb∥should be bounded in terms of the dimension of the system and of
the characteristics of the ﬂoating-point arithmetic that is being used.
It is indeed reasonable to expect that the perturbations induced by a
method for solving a linear system are such that ∥δA∥≤γ∥A∥and ∥δb∥≤
γ∥b∥, γ being a positive number that depends on the roundoﬀunit u (for
example, we shall assume henceforth that γ = β1−t, where β is the base
and t is the number of digits of the mantissa of the ﬂoating-point system
F). In such a case (3.9) can be completed by the following theorem.
Theorem 3.3 Assume that ∥δA∥≤γ∥A∥, ∥δb∥≤γ∥b∥with γ ∈R+ and
δA ∈Rn×n, δb ∈Rn. Then, if γK(A) < 1 the following inequalities hold
∥x + δx∥
∥x∥
≤1 + γK(A)
1 −γK(A),
(3.12)
∥δx∥
∥x∥≤
2γ
1 −γK(A)K(A).
(3.13)

62
3. Direct Methods for the Solution of Linear Systems
Proof. From (3.8) it follows that (I + A−1δA)(x + δx) = x + A−1δb. Moreover,
since γK(A) < 1 and ∥δA∥≤γ∥A∥it turns out that I + A−1δA is nonsingular.
Taking the inverse of such a matrix and passing to the norms we get ∥x + δx∥≤
∥(I + A−1δA)−1∥

∥x∥+ γ∥A−1∥∥b∥

. From Theorem 1.5 it then follows that
∥x + δx∥≤
1
1 −∥A−1δA∥

∥x∥+ γ∥A−1∥∥b∥

,
which implies (3.12), since ∥A−1δA∥≤γK(A) and ∥b∥≤∥A∥∥x∥.
Let us prove (3.13). Subtracting (3.2) from (3.8) it follows that
Aδx = −δA(x + δx) + δb.
Inverting A and passing to the norms, the following inequality is obtained
∥δx∥
≤
∥A−1δA∥∥x + δx∥+ ∥A−1∥∥δb∥
≤
γK(A)∥x + δx∥+ γ∥A−1∥∥b∥.
(3.14)
Dividing both sides by ∥x∥and using the triangular inequality ∥x+δx∥≤∥δx∥+
∥x∥, we ﬁnally get (3.13).
3
Remarkable instances of perturbations δA and δb are those for which
|δA| ≤γ|A| and |δb| ≤γ|b| with γ ≥0. Hereafter, the absolute value
notation B = |A| denotes the matrix n × n having entries bij = |aij| with
i, j = 1, . . . , n and the inequality C ≤D, with C, D ∈Rm×n has the
following meaning
cij ≤dij for i = 1, . . . , m, j = 1, . . . , n.
If ∥· ∥∞is considered, from (3.14) it follows that
∥δx∥∞
∥x∥∞
≤γ ∥|A−1| |A| |x| + |A−1| |b| ∥∞
(1 −γ∥|A−1| |A| ∥∞)∥x∥∞
≤
2γ
1 −γ∥|A−1| |A| ∥∞
∥|A−1| |A| ∥∞.
(3.15)
Estimate (3.15) is generally too pessimistic; however, the following compo-
nentwise error estimates of δx can be derived from (3.15)
|δxi| ≤γ|rT
(i)| |A| |x + δx|,
i = 1, . . . , n
if δb = 0,
|δxi|
|xi| ≤γ
|rT
(i)| |b|
|rT
(i)b| ,
i = 1, . . . , n
if δA = 0,
(3.16)
being rT
(i) the row vector eT
i A−1. Estimates (3.16) are more stringent than
(3.15), as can be seen in Example 3.1. The ﬁrst inequality in (3.16) can be
used when the perturbed solution x+δx is known, being henceforth x+δx
the solution computed by a numerical method.

3.1 Stability Analysis of Linear Systems
63
In the case where |A−1| |b| = |x|, the parameter γ in (3.15) is equal
to 1. For such systems the components of the solution are insensitive to
perturbations to the right side. A slightly worse situation occurs when A
is a triangular M-matrix and b has positive entries. In such a case γ is
bounded by 2n −1, since
|rT
(i)| |A| |x| ≤(2n −1)|xi|.
For further details on the subject we refer to [Ske79], [CI95] and [Hig89].
Results linking componentwise estimates to normwise estimates through
the so-called hypernorms can be found in [ADR92].
Example 3.1 Consider the linear system Ax=b with
A =


α
1
α
0
1
α

,
b =


α2 + 1
α
1
α


which has solution xT = (α, 1), where 0 < α < 1. Let us compare the results
obtained using (3.15) and (3.16). From
|A−1| |A| |x| = |A−1| |b| =

α + 2
α2 , 1
T
(3.17)
it follows that the supremum of (3.17) is unbounded as α →0, exactly as it
happens in the case of ∥A∥∞. On the other hand, the ampliﬁcation factor of
the error in (3.16) is bounded. Indeed, the component of the maximum absolute
value, x2, of the solution, satisﬁes |rT
(2)| |A| |x|/|x2| = 1.
•
3.1.3
Backward a priori Analysis
The numerical methods that we have considered thus far do not require the
explicit computation of the inverse of A to solve Ax=b. However, we can
always assume that they yield an approximate solution of the form x = Cb,
where the matrix C, due to rounding errors, is an approximation of A−1.
In practice, C is very seldom constructed; in case this should happen, the
following result yields an estimate of the error that is made substituting C
for A−1 (see [IK66], Chapter 2, Theorem 7).
Property 3.1 Let R = AC −I; if ∥R∥< 1, then A and C are nonsingular
and
∥A−1∥≤
∥C∥
1 −∥R∥,
∥R∥
∥A∥≤∥C −A−1∥≤∥C∥∥R∥
1 −∥R∥.
(3.18)
In the frame of backward a priori analysis we can interpret C as being the
inverse of A + δA (for a suitable unknown δA). We are thus assuming that
C(A + δA) = I. This yields
δA = C−1 −A = −(AC −I)C−1 = −RC−1

64
3. Direct Methods for the Solution of Linear Systems
and, as a consequence, if ∥R∥< 1 it turns out that
∥δA∥≤∥R∥∥A∥
1 −∥R∥,
(3.19)
having used the ﬁrst inequality in (3.18), where A is assumed to be an
approximation of the inverse of C (notice that the roles of C and A can be
interchanged).
3.1.4
A posteriori Analysis
Having approximated the inverse of A by a matrix C turns into having an
approximation of the solution of the linear system (3.2). Let us denote by
y a known approximate solution. The aim of the a posteriori analysis is to
relate the (unknown) error e = y −x to quantities that can be computed
using y and C.
The starting point of the analysis relies on the fact that the residual
vector r = b −Ay is in general nonzero, since y is just an approximation
to the unknown exact solution. The residual can be related to the error
through Property 3.1 as follows. We have e = A−1(Ay −b) = −A−1r and
thus, if ∥R∥< 1 then
∥e∥≤∥r∥∥C∥
1 −∥R∥.
(3.20)
Notice that the estimate does not necessarily require y to coincide with
the solution x = Cb of the backward a priori analysis. One could therefore
think of computing C only for the purpose of using the estimate (3.20) (for
instance, in the case where (3.2) is solved through the Gauss elimination
method, one can compute C a posteriori using the LU factorization of A,
see Sections 3.3 and 3.3.1).
We conclude by noticing that if δb is interpreted in (3.11) as being the
residual of the computed solution y = x + δx, it also follows that
∥e∥
∥x∥≤K(A) ∥r∥
∥b∥.
(3.21)
The estimate (3.21) is not used in practice since the computed residual
is aﬀected by rounding errors. A more signiﬁcant estimate (in the ∥· ∥∞
norm) is obtained letting r = fl(b −Ay) and assuming that r = r + δr
with |δr| ≤γn+1(|A| |y| + |b|), where γn+1 = (n + 1)u/(1 −(n + 1)u) > 0,
from which we have
∥e∥∞
∥y∥∞
≤∥|A−1|(|r| + γn+1(|A||y| + |b|))∥∞
∥y∥∞
.
Formulae like this last one are implemented in the library for linear algebra
LAPACK (see [ABB+92]).

3.2 Solution of Triangular Systems
65
3.2
Solution of Triangular Systems
Consider the nonsingular 3×3 lower triangular system


l11
0
0
l21
l22
0
l31
l32
l33




x1
x2
x3

=


b1
b2
b3

.
Since the matrix is nonsingular, its diagonal entries lii, i = 1, 2, 3, are
non vanishing, hence we can solve sequentially for the unknown values
xi, i = 1, 2, 3 as follows
x1 = b1/l11,
x2 = (b2 −l21x1)/l22,
x3 = (b3 −l31x1 −l32x2)/l33.
This algorithm can be extended to systems n × n and is called forward
substitution. In the case of a system Lx=b, with L being a nonsingular
lower triangular matrix of order n (n ≥2), the method takes the form
x1 = b1
l11
,
xi = 1
lii

bi −
i−1

j=1
lijxj

,
i = 2, . . . , n.
(3.22)
The number of multiplications and divisions to execute the algorithm is
equal to n(n+1)/2, while the number of sums and subtractions is n(n−1)/2.
The global operation count for (3.22) is thus n2 ﬂops.
Similar conclusions can be drawn for a linear system Ux=b, where U
is a nonsingular upper triangular matrix of order n (n ≥2). In this case
the algorithm is called backward substitution and in the general case can
be written as
xn = bn
unn
,
xi = 1
uii

bi −
n

j=i+1
uijxj

,
i = n −1, . . . , 1.
(3.23)
Its computational cost is still n2 ﬂops.
3.2.1
Implementation of Substitution Methods
Each i-th step of algorithm (3.22) requires performing the scalar product
between the row vector L(i, 1 : i −1) (this notation denoting the vector
extracted from matrix L taking the elements of the i-th row from the ﬁrst

66
3. Direct Methods for the Solution of Linear Systems
to the (i-1)-th column) and the column vector x(1 : i −1). The access to
matrix L is thus by row; for that reason, the forward substitution algorithm,
when implemented in the form above, is called row-oriented.
Its coding is reported in Program 1 (the Program mat square that is
called by forward row merely checks that L is a square matrix).
Program 1 - forward row : Forward substitution: row-oriented version
function [x]=forward row(L,b)
[n]=mat square(L);
x(1) = b(1)/L(1,1);
for i = 2:n, x (i) = (b(i)-L(i,1:i-1)*(x(1:i-1))’)/L(i,i); end
x=x’;
To obtain a column-oriented version of the same algorithm, we take ad-
vantage of the fact that i-th component of the vector x, once computed,
can be conveniently eliminated from the system.
An implementation of such a procedure, where the solution x is over-
written on the right vector b, is reported in Program 2.
Program 2 - forward col : Forward substitution: column-oriented version
function [b]=forward col(L,b)
[n]=mat square(L);
for j=1:n-1,
b(j)= b(j)/L(j,j); b(j+1:n)=b(j+1:n)-b(j)*L(j+1:n,j);
end; b(n) = b(n)/L(n,n);
Implementing the same algorithm by a row-oriented rather than a column-
oriented approach, might dramatically change its performance (but of course,
not the solution). The choice of the form of implementation must therefore
be subordinated to the speciﬁc hardware that is used.
Similar considerations hold for the backward substitution method, pre-
sented in (3.23) in its row-oriented version.
In Program 3 only the column-oriented version of the algorithm is coded.
As usual, the vector x is overwritten on b.
Program 3 - backward col : Backward substitution: column-oriented ver-
sion
function [b]=backward col(U,b)
[n]=mat square(U);
for j = n:-1:2,
b(j)=b(j)/U(j,j); b(1:j-1)=b(1:j-1)-b(j)*U(1:j-1,j);
end; b(1) = b(1)/U(1,1);
When large triangular systems must be solved, only the triangular portion
of the matrix should be stored leading to considerable saving of memory
resources.

3.2 Solution of Triangular Systems
67
3.2.2
Rounding Error Analysis
The analysis developed so far has not accounted for the presence of round-
ing errors. When including these, the forward and backward substitution
algorithms no longer yield the exact solutions to the systems Lx=b and
Uy=b, but rather provide approximate solutions x that can be regarded
as being exact solutions to the perturbed systems
(L + δL)x = b,
(U + δU)x = b,
where δL = (δlij) and δU = (δuij) are perturbation matrices. In order
to apply the estimates (3.9) carried out in Section 3.1.2, we must provide
estimates of the perturbation matrices, δL and δU, as a function of the
entries of L and U, of their size and of the characteristics of the ﬂoating-
point arithmetic. For this purpose, it can be shown that
|δT| ≤
nu
1 −nu|T|,
(3.24)
where T is equal to L or U, u = 1
2β1−t is the roundoﬀunit deﬁned in (2.34).
Clearly, if nu < 1 from (3.24) it turns out that, using a Taylor expansion,
|δT| ≤nu|T| + O(u2). Moreover, from (3.24) and (3.9) it follows that, if
nuK(T) < 1, then
∥x −x∥
∥x∥
≤
nuK(T)
1 −nuK(T) = nuK(T) + O(u2)
(3.25)
for the norms ∥· ∥1, ∥· ∥∞and the Frobenius norm. If u is suﬃciently
small (as typically happens), the perturbations introduced by the rounding
errors in the solution of a triangular system can thus be neglected. As
a consequence, the accuracy of the solution computed by the forward or
backward substitution algorithm is generally very high.
These results can be improved by introducing some additional assump-
tions on the entries of L or U. In particular, if the entries of U are such
that |uii| ≥|uij| for any j > i, then
|xi −xi| ≤2n−i+1
nu
1 −nu max
j≥i |xj|,
1 ≤i ≤n.
The same result holds if T=L, provided that |lii| ≥|lij| for any j < i, or if
L and U are diagonally dominant. The previous estimates will be employed
in Sections 3.3.1 and 3.4.2.
For the proofs of the results reported so far, see [FM67], [Hig89] and
[Hig88].
3.2.3
Inverse of a Triangular Matrix
The algorithm (3.23) can be employed to explicitly compute the inverse
of an upper triangular matrix. Indeed, given an upper triangular matrix

68
3. Direct Methods for the Solution of Linear Systems
U, the column vectors vi of the inverse V=(v1, . . . , vn) of U satisfy the
following linear systems
Uvi = ei,
i = 1, . . . , n
(3.26)
where {ei} is the canonical basis of Rn (deﬁned in Example 1.3). Solving
for vi thus requires the application of algorithm (3.23) n times to (3.26).
This procedure is quite ineﬃcient since at least half the entries of the
inverse of U are null. Let us take advantage of this as follows. Denote by
v′
k = (v′
1k, . . . , v′
kk)T the vector of size k such that
U(k)v′
k = lk
k = 1, . . . , n
(3.27)
where U(k) is the principal submatrix of U of order k and lk the vector of
Rk having null entries, except the ﬁrst one which is equal to 1. Systems
(3.27) are upper triangular, but have order k and can be again solved using
the method (3.23). We end up with the following inversion algorithm for
upper triangular matrices: for k = n, n −1, . . . , 1 compute
v′
kk = u−1
kk ,
v′
ik = −u−1
ii
k

j=i+1
uijv′
jk,
for i = k −1, k −2, . . . , 1.
(3.28)
At the end of this procedure the vectors v′
k furnish the non vanishing entries
of the columns of U−1. The algorithm requires about n3/3 + (3/4)n2 ﬂops.
Once again, due to rounding errors, the algorithm (3.28) no longer yields
the exact solution, but an approximation of it. The error that is introduced
can be estimated using the backward a priori analysis carried out in Section
3.1.3.
A similar procedure can be constructed from (3.22) to compute the in-
verse of a lower triangular system.
3.3
The Gaussian Elimination Method (GEM) and
LU Factorization
The Gaussian elimination method aims at reducing the system Ax=b to an
equivalent system (that is, having the same solution) of the form Ux=b,
where U is an upper triangular matrix and b is an updated right side
vector. This latter system can then be solved by the backward substitution
method. Let us denote the original system by A(1)x = b(1). During the
reduction procedure we basically employ the property which states that
replacing one of the equations by the diﬀerence between this equation and
another one multiplied by a non null constant yields an equivalent system
(i.e., one with the same solution).

3.3 The Gaussian Elimination Method (GEM) and LU Factorization
69
Thus, consider a nonsingular matrix A ∈Rn×n, and suppose that the
diagonal entry a11 is non vanishing. Introducing the multipliers
mi1 = a(1)
i1
a(1)
11
,
i = 2, 3, . . . , n,
where a(1)
ij
denote the elements of A(1), it is possible to eliminate the un-
known x1 from the rows other than the ﬁrst one by simply subtracting
from row i, with i = 2, . . . , n, the ﬁrst row multiplied by mi1 and doing
the same on the right side. If we now deﬁne
a(2)
ij = a(1)
ij −mi1a(1)
1j ,
i, j = 2, . . . , n,
b(2)
i
= b(1)
i
−mi1b(1)
1 ,
i = 2, . . . , n,
where b(1)
i
denote the components of b(1), we get a new system of the form


a(1)
11
a(1)
12
. . .
a(1)
1n
0
a(2)
22
. . .
a(2)
2n
...
...
...
0
a(2)
n2
. . .
a(2)
nn




x1
x2
...
xn


=


b(1)
1
b(2)
2...
b(2)
n


,
which we denote by A(2)x = b(2), that is equivalent to the starting one.
Similarly, we can transform the system in such a way that the unknown
x2 is eliminated from rows 3, . . . , n. In general, we end up with the ﬁnite
sequence of systems
A(k)x = b(k),
1 ≤k ≤n,
(3.29)
where, for k ≥2, matrix A(k) takes the following form
A(k) =


a(1)
11
a(1)
12
. . .
. . .
. . .
a(1)
1n
0
a(2)
22
a(2)
2n
...
...
...
0
. . .
0
a(k)
kk
. . .
a(k)
kn
...
...
...
...
0
. . .
0
a(k)
nk
. . .
a(k)
nn


,

70
3. Direct Methods for the Solution of Linear Systems
having assumed that a(i)
ii ̸= 0 for i = 1, . . . , k −1. It is clear that for k = n
we obtain the upper triangular system A(n)x = b(n)


a(1)
11
a(1)
12
. . .
. . .
a(1)
1n
0
a(2)
22
a(2)
2n
...
...
...
0
...
...
0
a(n)
nn




x1
x2
...
...
xn


=


b(1)
1
b(2)
2...
...
b(n)
n


.
Consistently with the notations that have been previously introduced, we
denote by U the upper triangular matrix A(n). The entries a(k)
kk are called
pivots and must obviously be non null for k = 1, . . . , n −1.
In order to highlight the formulae which transform the k-th system into
the k + 1-th one, for k = 1, . . . , n −1 we assume that a(k)
kk ̸= 0 and deﬁne
the multiplier
mik = a(k)
ik
a(k)
kk
,
i = k + 1, . . . , n.
(3.30)
Then we let
a(k+1)
ij
= a(k)
ij −mika(k)
kj ,
i, j = k + 1, . . . , n
b(k+1)
i
= b(k)
i
−mikb(k)
k ,
i = k + 1, . . . , n.
(3.31)
Example 3.2 Let us use GEM to solve the following system
(A(1)x = b(1))









x1
+
1
2x2
+
1
3x3
=
11
6
1
2x1
+
1
3x2
+
1
4x3
=
13
12
1
3x1
+
1
4x2
+
1
5x3
=
47
60
,
which admits the solution x=(1, 1, 1)T . At the ﬁrst step we compute the mul-
tipliers m21 = 1/2 and m31 = 1/3, and subtract from the second and third
equation of the system the ﬁrst row multiplied by m21 and m31, respectively. We
obtain the equivalent system
(A(2)x = b(2))









x1
+
1
2x2
+
1
3x3
=
11
6
0
+
1
12x2
+
1
12x3
=
1
6
0
+
1
12x2
+
4
45x3
=
31
180
.
If we now subtract the second row multiplied by m32 = 1 from the third one, we
end up with the upper triangular system
(A(3)x = b(3))









x1
+
1
2x2
+
1
3x3
=
11
6
0
+
1
12x2
+
1
12x3
=
1
6
0
+
0
+
1
180x3
=
1
180
,

3.3 The Gaussian Elimination Method (GEM) and LU Factorization
71
from which we immediately compute x3 = 1 and then, by back substitution, the
remaining unknowns x1 = x2 = 1.
•
Remark 3.2 The matrix in Example 3.2 is called the Hilbert matrix of
order 3. In the general n × n case, its entries are
hij = 1/(i + j −1),
i, j = 1, . . . , n.
(3.32)
As we shall see later on, this matrix provides the paradigm of an ill-
conditioned matrix.
■
To complete Gaussian elimination 2(n −1)n(n + 1)/3 + n(n −1) ﬂops are
required, plus n2 ﬂops to backsolve the triangular system U x = b(n).
Therefore, about (2n3/3 + 2n2) ﬂops are needed to solve the linear sys-
tem using GEM. Neglecting the lower order terms, we can state that the
Gaussian elimination process has a cost of 2n3/3 ﬂops.
As previously noticed, GEM terminates safely iﬀthe pivotal elements a(k)
kk ,
for k = 1, . . . , n −1, are non vanishing. Unfortunately, having non null
diagonal entries in A is not enough to prevent zero pivots to arise during
the elimination process. For example, matrix A in (3.33) is nonsingular and
has nonzero diagonal entries
A =


1
2
3
2
4
5
7
8
9

,
A(2) =


1
2
3
0
0
−1
0
−6
−12

.
(3.33)
Nevertheless, when GEM is applied, it is interrupted at the second step
since a(2)
22 = 0.
More restrictive conditions on A are thus needed to ensure the appli-
cability of the method. We shall see in Section 3.3.1 that if the leading
dominating minors di of A are nonzero for i = 1, . . . , n −1, then the corre-
sponding pivotal entries a(i)
ii must necessarily be non vanishing. We recall
that di is the determinant of Ai, the i-th principal submatrix made by the
ﬁrst i rows and columns of A. The matrix in the previous example does
not satisfy this condition, having d1 = 1 and d2 = 0.
Classes of matrices exist such that GEM can be always safely employed in
its basic form (3.31). Among them, we recall the following ones:
1. matrices diagonally dominant by rows;
2. matrices diagonally dominant by columns. In such a case one can even
show that the multipliers are in module less than or equal to 1 (see
Property 3.2);
3. matrices symmetric and positive deﬁnite (see Theorem 3.6).
For a rigorous derivation of these results, we refer to the forthcoming sec-
tions.

72
3. Direct Methods for the Solution of Linear Systems
3.3.1
GEM as a Factorization Method
In this section we show how GEM is equivalent to performing a factorization
of the matrix A into the product of two matrices, A=LU, with U=A(n).
Since L and U depend only on A and not on the right hand side, the same
factorization can be reused when solving several linear systems having the
same matrix A but diﬀerent right hand side b, with a considerable reduction
of the operation count (indeed, the main computational eﬀort, about 2n3/3
ﬂops, is spent in the elimination procedure).
Let us go back to Example 3.2 concerning the Hilbert matrix H3. In
practice, to pass from A(1)=H3 to the matrix A(2) at the second step, we
have multiplied the system by the matrix
M1 =


1
0
0
−1
2
1
0
−1
3
0
1

=


1
0
0
−m21
1
0
−m31
0
1

.
Indeed,
M1A = M1A(1) =


1
1
2
1
3
0
1
12
1
12
0
1
12
4
45

= A(2).
Similarly, to perform the second (and last) step of GEM, we must multiply
A(2) by the matrix
M2 =


1
0
0
0
1
0
0
−1
1

=


1
0
0
0
1
0
0
−m32
1

,
where A(3) = M2A(2). Therefore
M2M1A = A(3) = U.
(3.34)
On the other hand, matrices M1 and M2 are lower triangular, their product
is still lower triangular, as is their inverse; thus, from (3.34) one gets
A = (M2M1)−1U = LU,
which is the desired factorization of A.
This identity can be generalized as follows. Setting
mk = (0, . . . , 0, mk+1,k, . . . , mn,k)T ∈Rn

3.3 The Gaussian Elimination Method (GEM) and LU Factorization
73
and deﬁning
Mk =


1
. . .
0
0
. . .
0
...
...
...
...
...
0
1
0
0
0
−mk+1,k
1
0
...
...
...
...
...
...
0
. . .
−mn,k
0
. . .
1


= In −mkeT
k
as the k-th Gaussian transformation matrix, one ﬁnds out that
(Mk)ip = δip −(mkeT
k )ip = δip −mikδkp,
i, p = 1, . . . , n.
On the other hand, from (3.31) we have that
a(k+1)
ij
= a(k)
ij −mikδkka(k)
kj =
n

p=1
(δip −mikδkp)a(k)
pj ,
i, j = k + 1, . . . , n,
or, equivalently,
A(k+1) = MkA(k).
(3.35)
As a consequence, at the end of the elimination process the matrices Mk,
with k = 1, . . . , n −1, and the matrix U have been generated such that
Mn−1Mn−2 . . . M1A = U.
The matrices Mk are unit lower triangular with inverse given by
M−1
k
= 2In −Mk = In + mkeT
k ,
(3.36)
where (mieT
i )(mjeT
j ) are equal to the null matrix if i ̸= j. As a consequence
A
=
M−1
1 M−1
2
. . . M−1
n−1U
=
(In + m1eT
1 )(In + m2eT
2 ) . . . (In + mn−1eT
n−1)U
=

In +
n−1

i=1
mieT
i

U
=


1
0
. . .
. . .
0
m21
1
...
...
m32
...
...
...
...
...
0
mn1
mn2
. . .
mn,n−1
1


U.
(3.37)

74
3. Direct Methods for the Solution of Linear Systems
Deﬁning L = (Mn−1Mn−2 . . . M1)−1 = M−1
1
. . . M−1
n−1, it follows that
A = LU.
We notice that, due to (3.37), the subdiagonal entries of L are the multi-
pliers mik produced by GEM, while the diagonal entries are equal to one.
Once the matrices L and U have been computed, solving the linear system
consists only of solving successively the two triangular systems
Ly = b
Ux = y.
The computational cost of the factorization process is obviously the same
as that required by GEM.
The following result establishes a link between the leading dominant
minors of a matrix and its LU factorization induced by GEM.
Theorem 3.4 Let A ∈Rn×n. The LU factorization of A with lii = 1 for
i = 1, . . . , n exists and is unique iﬀthe principal submatrices Ai of A of
order i = 1, . . . , n −1 are nonsingular.
Proof. The existence of the LU factorization can be proved following the steps
of the GEM. Here we prefer to pursue an alternative approach, which allows for
proving at the same time both existence and uniqueness and that will be used
again in later sections.
Let us assume that the leading minors Ai of A are nonsingular for i = 1, . . . , n−
1 and prove, by induction on i, that under this hypothesis the LU factorization
of A(= An) with lii = 1 for i = 1, . . . , n, exists and is unique.
The property is obviously true if i = 1. Assume therefore that there exists an
unique LU factorization of Ai−1 of the form Ai−1 = L(i−1)U(i−1) with l(i−1)
kk
= 1
for k = 1, . . . , i −1, and show that there exists an unique factorization also for
Ai. We partition Ai by block matrices as
Ai =


Ai−1
c
dT
aii


and look for a factorization of Ai of the form
Ai = L(i)U(i) =


L(i−1)
0
lT
1




U(i−1)
u
0T
uii

,
(3.38)
having also partitioned by blocks the factors L(i) and U(i). Computing the prod-
uct of these two factors and equating by blocks the elements of Ai, it turns out
that the vectors l and u are the solutions to the linear systems L(i−1)u = c,
lT U(i−1) = dT .

3.3 The Gaussian Elimination Method (GEM) and LU Factorization
75
On the other hand, since 0 ̸= det(Ai−1) = det(L(i−1))det(U(i−1)), the matrices
L(i−1) and U(i−1) are nonsingular and, as a result, u and l exist and are unique.
Thus, there exists a unique factorization of Ai, where uii is the unique solution
of the equation uii = aii −lT u. This completes the induction step of the proof.
It now remains to prove that, if the factorization at hand exists and is unique,
then the ﬁrst n−1 leading minors of A must be nonsingular. We shall distinguish
the case where A is singular and when it is nonsingular.
Let us start from the second one and assume that the LU factorization of A
with lii = 1 for i = 1, . . . , n, exists and is unique. Then, due to (3.38), we have
Ai = L(i)U(i) for i = 1, . . . , n. Thus
det(Ai) = det(L(i))det(U(i)) = det(U(i)) = u11u22 . . . uii,
(3.39)
from which, taking i = n and A nonsingular, we obtain u11u22 . . . unn ̸= 0, and
thus, necessarily, det(Ai) = u11u22 . . . uii ̸= 0 for i = 1, . . . , n −1.
Now let A be a singular matrix and assume that (at least) one diagonal entry
of U is equal to zero. Denote by ukk the null entry of U with minimum index k.
Thanks to (3.38), the factorization can be computed without troubles until the
k + 1-th step. From that step on, since the matrix U(k) is singular, existence and
uniqueness of the vector lT are certainly lost, and, thus, the same holds for the
uniqueness of the factorization. In order for this not to occur before the process
has factorized the whole matrix A, the ukk entries must all be nonzero up to the
index k = n−1 included, and thus, due to (3.39), all the leading minors Ak must
be nonsingular for k = 1, . . . , n −1.
3
From the above theorem we conclude that, if an Ai, with i = 1, . . . , n −1,
is singular, then the factorization may either not exist or not be unique.
Example 3.3 Consider the matrices
B =
 1
2
1
2

,
C =
 0
1
1
0

,
D =
 0
1
0
2

.
According to Theorem 3.4, the singular matrix B, having nonsingular leading
minor B1 = 1, admits a unique LU factorization. The remaining two examples
outline that, if the assumptions of the theorem are not fulﬁlled, the factorization
may fail to exist or be unique.
Actually, the nonsingular matrix C, with C1 singular, does not admit any
factorization, while the (singular) matrix D, with D1 singular, admits an inﬁnite
number of factorizations of the form D = LβUβ, with
Lβ =
 1
0
β
1

,
Uβ =
 0
1
0
2 −β

,
∀β ∈R.
•
In the case where the LU factorization is unique, we point out that, because
det(A) = det(LU) = det(L) det(U) = det(U), the determinant of A is given

76
3. Direct Methods for the Solution of Linear Systems
by
det(A) = u11 · · · unn.
Let us now recall the following property (referring for its proof to [GL89]
or [Hig96]).
Property 3.2 If A is a matrix diagonally dominant by rows or by columns,
then the LU factorization of A exists. In particular, if A is diagonally dom-
inant by columns, then |lij| ≤1 ∀i, j.
In the proof of Theorem 3.4 we exploited the fact the the diagonal entries
of L are equal to 1. In a similar manner, we could have ﬁxed to 1 the
diagonal entries of the upper triangular matrix U, obtaining a variant of
GEM that will be considered in Section 3.3.4.
The freedom in setting up either the diagonal entries of L or those of U,
implies that several LU factorizations exist which can be obtained one from
the other by multiplication with a suitable diagonal matrix (see Section
3.4.1).
3.3.2
The Eﬀect of Rounding Errors
If rounding errors are taken into account, the factorization process induced
by GEM yields two matrices, L and U, such that LU = A+δA, δA being a
perturbation matrix. The size of such a perturbation can be estimated by
|δA| ≤
nu
1 −nu|L| |U|,
(3.40)
where u is the roundoﬀunit (for the proof of this result we refer to [Hig89]).
From (3.40) it is seen that the presence of small pivotal entries can make
the right side of the inequality virtually unbounded, with a consequent loss
of control on the size of the perturbation matrix δA. The interest is thus
in ﬁnding out estimates like (3.40) of the form
|δA| ≤g(u)|A|,
where g(u) is a suitable function of u. For instance, assuming that L and
U have nonnegative entries, then since |L| |U| = |LU| one gets
|L| |U| = |LU| = |A + δA| ≤|A| + |δA| ≤|A| +
nu
1 −nu|L| |U|,
(3.41)
from which the desired bound is achieved by taking g(u) = nu/(1 −2nu).
The technique of pivoting, examined in Section 3.5, keeps the size of the
pivotal entries under control and makes it possible to obtain estimates like
(3.41) for any matrix.

3.3 The Gaussian Elimination Method (GEM) and LU Factorization
77
3.3.3
Implementation of LU Factorization
Since L is a lower triangular matrix with diagonal entries equal to 1 and U
is upper triangular, it is possible (and convenient) to store the LU factor-
ization directly in the same memory area that is occupied by the matrix A.
More precisely, U is stored in the upper triangular part of A (including the
diagonal), whilst L occupies the lower triangular portion of A (the diagonal
entries of L are not stored since they are implicitly assumed to be 1).
A coding of the algorithm is reported in Program 4. The output matrix
A contains the overwritten LU factorization.
Program 4 - lu kji : LU factorization of matrix A. kji version
function [A] = lu kji (A)
[n,n]=size(A);
for k=1:n-1
A(k+1:n,k)=A(k+1:n,k)/A(k,k);
for j=k+1:n, for i=k+1:n
A(i,j)=A(i,j)-A(i,k)*A(k,j);
end,
end
end
This implementation of the factorization algorithm is commonly referred
to as the kji version, due to the order in which the cycles are executed.
In a more appropriate notation, it is called the SAXPY −kji version,
due to the fact that the basic operation of the algorithm, which consists of
multiplying a scalar A by a vector X, summing another vector Y and then
storing the result, is usually called SAXPY (i.e. Scalar A X Plus Y ).
The factorization can of course be executed by following a diﬀerent order.
In general, the forms in which the cycle on index i precedes the cycle on
j are called row-oriented, whilst the others are called column-oriented. As
usual, this terminology refers to the fact that the matrix is accessed by
rows or by columns.
An example of LU factorization, jki version and column-oriented, is given
in Program 5. This version is commonly called GAXPY −jki, since the
basic operation (a product matrix-vector), is called GAXPY which stands
for Generalized sAXPY (see for further details [DGK84]). In the GAXPY
operation the scalar A of the SAXPY operation is replaced by a matrix.
Program 5 - lu jki : LU factorization of matrix A. jki version
function [A] = lu jki (A)
[n,n]=size(A);
for j=1:n
for k=1:j-1,
for i=k+1:n
A(i,j)=A(i,j)-A(i,k)*A(k,j);
end,
end
for i=j+1:n,
A(i,j)=A(i,j)/A(j,j); end
end

78
3. Direct Methods for the Solution of Linear Systems
3.3.4
Compact Forms of Factorization
Remarkable variants of LU factorization are the Crout factorization and
Doolittle factorization, and are known also as compact forms of the Gauss
elimination method. This name is due to the fact that these approaches
require less intermediate results than the standard GEM to generate the
factorization of A.
Computing the LU factorization of A is formally equivalent to solving
the following nonlinear system of n2 equations
aij =
min(i,j)

r=1
lirurj,
(3.42)
the unknowns being the n2 +n coeﬃcients of the triangular matrices L and
U. If we arbitrarily set n coeﬃcients to 1, for example the diagonal entries
of L or U, we end up with the Doolittle and Crout methods, respectively,
which provide an eﬃcient way to solve system (3.42).
In fact, supposing that the ﬁrst k −1 columns of L and U are available
and setting lkk = 1 (Doolittle method), the following equations are obtained
from (3.42)
akj =
k−1

r=1
lkrurj + ukj ,
j = k, . . . , n
aik =
k−1

r=1
lirurk + lik ukk,
i = k + 1, . . . , n.
Note that these equations can be solved in a sequential way with respect
to the boxed variables ukj and lik. From the Doolittle compact method
we thus obtain ﬁrst the k-th row of U and then the k-th column of L, as
follows: for k = 1, . . . , n
ukj = akj −
k−1

r=1
lkrurj
j = k, . . . , n
lik =
1
ukk

aik −
k−1

r=1
lirurk

i = k + 1, . . . , n.
(3.43)
The Crout factorization is generated similarly, computing ﬁrst the k-th
column of L and then the k-th row of U: for k = 1, . . . , n
lik = aik −
k−1

r=1
lirurk
i = k, . . . , n
ukj = 1
lkk

akj −
k−1

r=1
lkrurj

j = k + 1, . . . , n,

3.4 Other Types of Factorization
79
where we set ukk = 1. Recalling the notations introduced above, the Doolit-
tle factorization is nothing but the ijk version of GEM.
We provide in Program 6 the implementation of the Doolittle scheme.
Notice that now the main computation is a dot product, so this scheme is
also known as the DOT −ijk version of GEM.
Program 6 - lu ijk : LU factorization of the matrix A: ijk version
function [A] = lu ijk (A)
[n,n]=size(A);
for i=1:n
for j=2:i
A(i,j-1)=A(i,j-1)/A(j-1,j-1);
for k=1:j-1,
A(i,j)=A(i,j)-A(i,k)*A(k,j); end
end
for j=i+1:n
for k=1:i-1,
A(i,j)=A(i,j)-A(i,k)*A(k,j); end
end
end
3.4
Other Types of Factorization
We now address factorizations suitable for symmetric and rectangular ma-
trices.
3.4.1
LDMT Factorization
It is possible to devise other types of factorizations of A removing the
hypothesis that the elements of L are equal to one. Speciﬁcally, we will
address some variants where the factorization of A is of the form
A = LDMT .
where L, MT and D are lower triangular, upper triangular and diagonal
matrices, respectively.
After the construction of this factorization, the resolution of the system
can be carried out solving ﬁrst the lower triangular system Ly=b, then the
diagonal one Dz=y, and ﬁnally the upper triangular system MT x=z, with
a cost of n2 + n ﬂops. In the symmetric case, we obtain M = L and the
LDLT factorization can be computed with half the cost (see Section 3.4.2).
The LDLT factorization enjoys a property analogous to the one in The-
orem 3.4 for the LU factorization. In particular, the following result holds.
Theorem 3.5 If all the principal minors of a matrix A∈Rn×n are nonzero
then there exist a unique diagonal matrix D, a unique unit lower triangu-
lar matrix L and a unique unit upper triangular matrix MT , such that
A = LDMT .

80
3. Direct Methods for the Solution of Linear Systems
Proof. By Theorem 3.4 we already know that there exists a unique LU factor-
ization of A with lii = 1 for i = 1, . . . , n. If we set the diagonal entries of D
equal to uii (nonzero because U is nonsingular), then A = LU = LD(D−1U).
Upon deﬁning MT = D−1U, the existence of the LDMT factorization follows,
where D−1U is a unit upper triangular matrix. The uniqueness of the LDMT
factorization is a consequence of the uniqueness of the LU factorization.
3
The above proof shows that, since the diagonal entries of D coincide
with those of U, we could compute L, MT and D starting from the LU
factorization of A. It suﬃces to compute MT as D−1U. Nevertheless, this
algorithm has the same cost as the standard LU factorization. Likewise,
it is also possible to compute the three matrices of the factorization by
enforcing the identity A=LDMT entry by entry.
3.4.2
Symmetric and Positive Deﬁnite Matrices: The
Cholesky Factorization
As already pointed out, the factorization LDMT simpliﬁes considerably
when A is symmetric because in such a case M=L, yielding the so-called
LDMT factorization. The computational cost halves, with respect to the
LU factorization, to about (n3/3) ﬂops.
As an example, the Hilbert matrix of order 3 admits the following LDLT
factorization
H3 =


1
1
2
1
3
1
2
1
3
1
4
1
3
1
4
1
5

=


1
0
0
1
2
1
0
1
3
1
1




1
0
0
0
1
12
0
0
0
1
180




1
1
2
1
3
0
1
1
0
0
1

.
In the case that A is also positive deﬁnite, the diagonal entries of D in the
LDLT factorization are positive. Moreover, we have the following result.
Theorem 3.6 Let A ∈Rn×n be a symmetric and positive deﬁnite matrix.
Then, there exists a unique upper triangular matrix H with positive diagonal
entries such that
A = HTH.
(3.44)
This factorization is called Cholesky factorization and the entries hij of HT
can be computed as follows: h11 = √a11 and, for i = 2, . . . , n,
hij =

aij −
j−1

k=1
hikhjk

/hjj,
j = 1, . . . , i −1,
hii =

aii −
i−1

k=1
h2
ik
1/2
.
(3.45)

3.4 Other Types of Factorization
81
Proof. Let us prove the theorem proceeding by induction on the size i of the
matrix (as done in Theorem 3.4), recalling that if Ai ∈Ri×i is symmetric positive
deﬁnite, then all its principal submatrices enjoy the same property.
For i = 1 the result is obviously true. Thus, suppose that it holds for i −1 and
prove that it also holds for i. There exists an upper triangular matrix Hi−1 such
that Ai−1 = HT
i−1Hi−1. Let us partition Ai as
Ai =
 Ai−1
v
vT
α

,
with α ∈R+, vT ∈Ri−1 and look for a factorization of Ai of the form
Ai = HT
i Hi =
 HT
i−1
0
hT
β
  Hi−1
h
0T
β

.
Enforcing the equality with the entries of Ai yields the equations HT
i−1h = v
and hT h + β2 = α. The vector h is thus uniquely determined, since HT
i−1 is
nonsingular. As for β, due to the properties of determinants
0 < det(Ai) = det(HT
i ) det(Hi) = β2(det(Hi−1))2,
we can conclude that it must be a real number. As a result, β =
√
α −hT h is
the desired diagonal entry and this concludes the inductive argument.
Let us now prove formulae (3.45). The fact that h11 = √a11 is an immediate
consequence of the induction argument for i = 1. In the case of a generic i,
relations (3.45)1 are the forward substitution formulae for the solution of the
linear system HT
i−1h = v = (a1i, a2i, . . . , ai−1,i)T , while formulae (3.45)2 state
that β =
√
α −hT h, where α = aii.
3
The algorithm which implements (3.45) requires about (n3/3) ﬂops and it
turns out to be stable with respect to the propagation of rounding errors.
It can indeed be shown that the upper triangular matrix ˜H is such that
˜HT ˜H = A + δA, where δA is a pertubation matrix such that ∥δA∥2 ≤
8n(n + 1)u∥A∥2, when the rounding errors are considered and assuming
that 2n(n + 1)u ≤1 −(n + 1)u (see [Wil68]).
Also, for the Cholesky factorization it is possible to overwrite the matrix
HT in the lower triangular portion of A, without any further memory stor-
age. By doing so, both A and the factorization are preserved, noting that
A is stored in the upper triangular section since it is symmetric and that
its diagonal entries can be computed as a11 = h2
11, aii = h2
ii + i−1
k=1 h2
ik,
i = 2, . . . , n.
An example of implementation of the Cholesky factorization is coded in
Program 7.
Program 7 - chol2 : Cholesky factorization
function [A] = chol2 (A)
[n,n]=size(A);

82
3. Direct Methods for the Solution of Linear Systems
for k=1:n-1
A(k,k)=sqrt(A(k,k));
A(k+1:n,k)=A(k+1:n,k)/A(k,k);
for j=k+1:n,
A(j:n,j)=A(j:n,j)-A(j:n,k)*A(j,k);
end
end
A(n,n)=sqrt(A(n,n));
3.4.3
Rectangular Matrices: The QR Factorization
Deﬁnition 3.1 A matrix A ∈Rm×n, with m ≥n, admits a QR fac-
torization if there exist an orthogonal matrix Q ∈Rm×m and an upper
trapezoidal matrix R ∈Rm×n with null rows from the n + 1-th one on,
such that
A = QR.
(3.46)
■
This factorization can be constructed either using suitable transformation
matrices (Givens or Householder matrices, see Section 5.6.1) or using the
Gram-Schmidt orthogonalization algorithm discussed below.
It is also possible to generate a reduced version of the QR factorization
(3.46), as stated in the following result.
Property 3.3 Let A ∈Rm×n be a matrix of rank n for which a QR fac-
torization is known. Then there exists a unique factorization of A of the
form
A = $Q$R
(3.47)
where $Q and $R are submatrices of Q and R given respectively by
$Q = Q(1 : m, 1 : n),
$R = R(1 : n, 1 : n).
(3.48)
Moreover, $Q has orthonormal vector columns and $R is upper triangular
and coincides with the Cholesky factor H of the symmetric positive deﬁnite
matrix AT A, that is, AT A = $RT $R.
If A has rank n (i.e., full rank), then the column vectors of ˜Q form an
orthonormal basis for the vector space range(A) (deﬁned in (1.5)). As a
consequence, constructing the QR factorization can also be interpreted as
a procedure for generating an orthonormal basis for a given set of vectors.
If A has rank r < n, the QR factorization does not necessarily yield an
orthonormal basis for range(A). However, one can obtain a factorization of
the form
QT AP =

R11
R12
0
0

,

3.4 Other Types of Factorization
83
n
A
m
=
m −n
˜Q
˜R
0
n
n
n
m −n
FIGURE 3.1. The reduced factorization. The matrices of the QR factorization
are drawn in dashed lines
where Q is orthogonal, P is a permutation matrix and R11 is a nonsingular
upper triangular matrix of order r.
In general, when using the QR factorization, we shall always refer to its
reduced form (3.47) as it ﬁnds a remarkable application in the solution of
overdetermined systems (see Section 3.13).
The matrix factors ˜Q and ˜R in (3.47) can be computed using the Gram-
Schmidt orthogonalization. Starting from a set of linearly independent vec-
tors, x1, . . . , xn, this algorithm generates a new set of mutually orthogonal
vectors, q1, . . . , qn, given by
q1 = x1,
qk+1 = xk+1 −
k

i=1
(qi, xk+1)
(qi, qi) qi,
k = 1, . . . , n −1.
(3.49)
Denoting by a1, . . . , an the column vectors of A, we set ˜q1 = a1/∥a1∥2
and, for k = 1, . . . , n −1, compute the column vectors of ˜Q as
˜qk+1 = qk+1/∥qk+1∥2,
where
qk+1 = ak+1 −
k

j=1
(˜qj, ak+1)˜qj.
Next, imposing that A=˜Q˜R and exploiting the fact that ˜Q is orthogonal
(that is, ˜Q−1 = ˜QT ), the entries of ˜R can easily be computed. The overall
computational cost of the algorithm is of the order of mn2 ﬂops.
It is also worth noting that if A has full rank, the matrix AT A is sym-
metric and positive deﬁnite (see Section 1.9) and thus it admits a unique
Cholesky factorization of the form HT H. On the other hand, since the or-
thogonality of ˜Q implies
HT H = AT A = ˜RT ˜QT ˜Q˜R = ˜RT ˜R,

84
3. Direct Methods for the Solution of Linear Systems
we conclude that ˜R is actually the Cholesky factor H of AT A. Thus, the
diagonal entries of ˜R are all nonzero only if A has full rank.
The Gram-Schmidt method is of little practical use since the generated
vectors lose their linear independence due to rounding errors. Indeed, in
ﬂoating-point arithmetic the algorithm produces very small values of ∥qk+1∥2
and ˜rkk with a consequent numerical instability and loss of orthogonality
for matrix ˜Q (see Example 3.4).
These drawbacks suggest employing a more stable version, known as
modiﬁed Gram-Schmidt method. At the beginning of the k + 1-th step, the
projections of the vector ak+1 along the vectors ˜q1, . . . , ˜qk are progressively
subtracted from ak+1. On the resulting vector, the orthogonalization step
is then carried out. In practice, after computing (˜q1, ak+1)˜q1 at the k+1-th
step, this vector is immediately subtracted from ak+1. As an example, one
lets
a(1)
k+1 = ak+1 −(˜q1, ak+1)˜q1.
This new vector a(1)
k+1 is projected along the direction of ˜q2 and the obtained
projection is subtracted from a(1)
k+1, yielding
a(2)
k+1 = a(1)
k+1 −(˜q2, a(1)
k+1)˜q2
and so on, until a(k)
k+1 is computed.
It can be checked that a(k)
k+1 coincides with the corresponding vector qk+1
in the standard Gram-Schmidt process, since, due to the orthogonality of
vectors ˜q1, ˜q2, . . . , ˜qk,
a(k)
k+1
=
ak+1 −(˜q1, ak+1)˜q1 −(˜q2, ak+1 −(˜q1, ak+1)˜q1) ˜q2 + . . .
=
ak+1 −
k

j=1
(˜qj, ak+1)˜qj.
Program 8 implements the modiﬁed Gram-Schmidt method. Notice that
it is not possible to overwrite the computed QR factorization on the ma-
trix A. In general, the matrix $R is overwritten on A, whilst $Q is stored
separately. The computational cost of the modiﬁed Gram-Schmidt method
has the order of 2mn2 ﬂops.
Program 8 - mod grams : Modiﬁed Gram-Schmidt method
function [Q,R] = mod grams(A)
[m,n]=size(A);
Q=zeros(m,n);
Q(1:m,1) = A(1:m,1);
R=zeros(n);
R(1,1)=1;
for k = 1:n
R(k,k) = norm (A(1:m,k));
Q(1:m,k) = A(1:m,k)/R(k,k);

3.5 Pivoting
85
for j=k+1:n
R (k,j) = Q (1:m,k)’ * A(1:m,j);
A (1:m,j) = A (1:m,j) - Q(1:m,k)*R(k,j);
end
end
Example 3.4 Let us consider the Hilbert matrix H4 of order 4 (see (3.32)). The
matrix ˜Q, generated by the standard Gram-Schmidt algorithm, is orthogonal up
to the order of 10−10, being
I −˜QT ˜Q = 10−10


0.0000
−0.0000
0.0001
−0.0041
−0.0000
0
0.0004
−0.0099
0.0001
0.0004
0
−0.4785
−0.0041
−0.0099
−0.4785
0


and ∥I −˜QT ˜Q∥∞= 4.9247 · 10−11. Using the modiﬁed Gram-Schmidt method,
we would obtain
I −˜QT ˜Q = 10−12


0.0001
−0.0005
0.0069
−0.2853
−0.0005
0
−0.0023
0.0213
0.0069
−0.0023
0.0002
−0.0103
−0.2853
0.0213
−0.0103
0


and this time ∥I −˜QT ˜Q∥∞= 3.1686 · 10−13.
An improved result can be obtained using, instead of Program 8, the intrinsic
function QR of MATLAB. This function can be properly employed to generate
both the factorization (3.46) as well as its reduced version (3.47).
•
3.5
Pivoting
As previously pointed out, the GEM process breaks down as soon as a zero
pivotal entry is computed. In such an event, one needs to resort to the so-
called pivoting technique, which amounts to exchanging rows (or columns)
of the system in such a way that non vanishing pivots are obtained.
Example 3.5 Let us go back to matrix (3.33) for which GEM furnishes at the
second step a zero pivotal element. By simply exchanging the second row with
the third one, we can execute one step further of the elimination method, ﬁnding
a nonzero pivot. The generated system is equivalent to the original one and it
can be noticed that it is already in upper triangular form. Indeed
A(2) =


1
2
3
0
−6
−12
0
0
−1

= U,
while the transformation matrices are given by
M(1) =


1
0
0
−2
1
0
−7
0
1

,
M(2) =


1
0
0
0
1
0
0
0
1

.

86
3. Direct Methods for the Solution of Linear Systems
From an algebraic standpoint, a permutation of the rows of A has been performed.
In fact, it now no longer holds that A=M−1
1 M−1
2 U, but rather A=M−1
1
P M−1
2 U,
P being the permutation matrix
P =


1
0
0
0
0
1
0
1
0

.
(3.50)
•
The pivoting strategy adopted in Example 3.5 can be generalized by look-
ing, at each step k of the elimination procedure, for a nonzero pivotal entry
by searching within the entries of the subcolumn A(k)(k : n, k). For that
reason, it is called partial pivoting (by rows).
From (3.30) it can be seen that a large value of mik (generated for ex-
ample by a small value of the pivot a(k)
kk ) might amplify the rounding errors
aﬀecting the entries a(k)
kj . Therefore, in order to ensure a better stability,
the pivotal element is chosen as the largest entry (in module) of the column
A(k)(k : n, k) and partial pivoting is generally performed at every step of
the elimination procedure, even if not strictly necessary (that is, even if
nonzero pivotal entries are found).
Alternatively, the searching process could have been extended to the
whole submatrix A(k)(k : n, k : n), ending up with a complete pivoting
(see Figure 3.2). Notice, however, that while partial pivoting requires an
additional cost of about n2 searches, complete pivoting needs about 2n3/3,
with a considerable increase of the computational cost of GEM.
                                                                                                                                        







                                                                                                                                







0
r
k
r
k
q
k
k
0
FIGURE 3.2. Partial pivoting by row (left) or complete pivoting (right). Shaded
areas of the matrix are those involved in the searching for the pivotal entry
Example 3.6 Let us consider the linear system Ax = b with
A =
 10−13
1
1
1


3.5 Pivoting
87
and where b is chosen in such a way that x = (1, 1)T is the exact solution.
Suppose we use base 2 and 16 signiﬁcant digits. GEM without pivoting would
give xMEG = (0.99920072216264, 1)T , while GEM plus partial pivoting furnishes
the exact solution up to the 16th digit.
•
Let us analyze how partial pivoting aﬀects the LU factorization induced
by GEM. At the ﬁrst step of GEM with partial pivoting, after ﬁnding
out the entry ar1 of maximum module in the ﬁrst column, the elementary
permutation matrix P1 which exchanges the ﬁrst row with the r-th row is
constructed (if r = 1, P1 is the identity matrix). Next, the ﬁrst Gaussian
transformation matrix M1 is generated and we set A(2) = M1P1A(1). A
similar approach is now taken on A(2), searching for a new permutation
matrix P2 and a new matrix M2 such that
A(3) = M2P2A(2) = M2P2M1P1A(1).
Executing all the elimination steps, the resulting upper triangular matrix
U is now given by
U = A(n) = Mn−1Pn−1 . . . M1P1A(1).
(3.51)
Letting M = Mn−1Pn−1 . . . M1P1 and P = Pn−1 . . . P1, we obtain that
U=MA and, thus, U = (MP−1)PA. It can easily be checked that the matrix
L = PM−1 is unit lower triangular, so that the LU factorization reads
PA = LU.
(3.52)
One should not be worried by the presence of the inverse of M, since M−1 =
P−1
1 M−1
1
. . . P−1
n−1M−1
n−1 and P−1
i
= PT
i while M−1
i
= 2In −Mi.
Once L, U and P are available, solving the initial linear system amounts
to solving the triangular systems Ly = Pb and Ux = y. Notice that the
entries of the matrix L coincide with the multipliers computed by LU fac-
torization, without pivoting, when applied to the matrix PA.
If complete pivoting is performed, at the ﬁrst step of the process, once the
element aqr of largest module in submatrix A(1 : n, 1 : n) has been found,
we must exchange the ﬁrst row and column with the q-th row and the
r-th column. This generates the matrix P1A(1)Q1, where P1 and Q1 are
permutation matrices by rows and by columns, respectively.
As a consequence, the action of matrix M1 is now such that A(2) =
M1P1A(1)Q1. Repeating the process, at the last step, instead of (3.51) we
obtain
U = A(n) = Mn−1Pn−1 . . . M1P1A(1)Q1 . . . Qn−1.
In the case of complete pivoting the LU factorization becomes
PAQ = LU,

88
3. Direct Methods for the Solution of Linear Systems
where Q = Q1 . . . Qn−1 is a permutation matrix accounting for all permu-
tations that have been operated. By construction, matrix L is still lower
triangular, with module entries less than or equal to 1. As happens in
partial pivoting, the entries of L are the multipliers produced by the LU
factorization process without pivoting, when applied to the matrix PAQ.
Program 9 is an implementation of the LU factorization with complete
pivoting. For an eﬃcient computer implementation of the LU factorization
with partial pivoting, we refer to the MATLAB intrinsic function lu.
Program 9 - LUpivtot : LU factorization with complete pivoting
function [L,U,P,Q] = LUpivtot(A,n)
P=eye(n); Q=P; Minv=P;
for k=1:n-1
[Pk,Qk]=pivot(A,k,n);
A=Pk*A*Qk;
[Mk,Mkinv]=MGauss(A,k,n);
A=Mk*A;
P=Pk*P;
Q=Q*Qk;
Minv=Minv*Pk*Mkinv;
end
U=triu(A);
L=P*Minv;
function [Mk,Mkinv]=MGauss(A,k,n)
Mk=eye(n);
for i=k+1:n,
Mk(i,k)=-A(i,k)/A(k,k);
end
Mkinv=2*eye(n)-Mk;
function [Pk,Qk]=pivot(A,k,n)
[y,i]=max(abs(A(k:n,k:n))); [piv,jpiv]=max(y);
ipiv=i(jpiv);
jpiv=jpiv+k-1;
ipiv=ipiv+k-1;
Pk=eye(n); Pk(ipiv,ipiv)=0; Pk(k,k)=0; Pk(k,ipiv)=1; Pk(ipiv,k)=1;
Qk=eye(n); Qk(jpiv,jpiv)=0; Qk(k,k)=0; Qk(k,jpiv)=1; Qk(jpiv,k)=1;
Remark 3.3 The presence of large pivotal entries is not in itself suﬃcient
to guarantee accurate solutions, as demonstrated by the following example
(taken from [JM92]). For the linear system Ax = b


−4000
2000
2000
2000
0.78125
0
2000
0
0




x1
x2
x3

=


400
1.3816
1.9273


at the ﬁrst step the pivotal entry coincides with the diagonal entry −4000
itself. However, executing GEM on such a matrix yields the solution
x = [0.00096365, −0.698496, 0.90042329]T
whose ﬁrst component drastically diﬀers from that of the exact solution
x = [1.9273, −0.698496, 0.9004233]T . The cause of this behaviour should

3.6 Computing the Inverse of a Matrix
89
be ascribed to the wide variations among the system coeﬃcients. Such cases
can be remedied by a suitable scaling of the matrix (see Section 3.12.1). ■
Remark 3.4 (Pivoting for symmetric matrices) As already noticed,
pivoting is not strictly necessary if A is symmetric and positive deﬁnite.
A separate comment is deserved when A is symmetric but not positive
deﬁnite, since pivoting could destroy the symmetry of the matrix. This
can be avoided by employing a complete pivoting of the form PAPT , even
though this pivoting can only turn out into a reordering of the diagonal
entries of A. As a consequence, the presence on the diagonal of A of small
entries might inhibit the advantages of the pivoting. To deal with matrices
of this kind, special algorithms are needed (like the Parlett-Reid method
[PR70] or the Aasen method [Aas71]) for whose description we refer to
[GL89], and to [JM92] for the case of sparse matrices.
■
3.6
Computing the Inverse of a Matrix
The explicit computation of the inverse of a matrix can be carried out using
the LU factorization as follows. Denoting by X the inverse of a nonsingular
matrix A∈Rn×n, the column vectors of X are the solutions to the linear
systems Axi = ei, for i = 1, . . . , n.
Supposing that PA=LU, where P is the partial pivoting permutation
matrix, we must solve 2n triangular systems of the form
Lyi = Pei,
Uxi = yi
i = 1, . . . , n,
i.e., a succession of linear systems having the same coeﬃcient matrix but
diﬀerent right hand sides. The computation of the inverse of a matrix is a
costly procedure which can sometimes be even less stable than MEG (see
[Hig88]).
An alternative approach for computing the inverse of A is provided by
the Faddev or Leverrier formula, which, letting B0=I, recursively computes
αk = 1
k tr(ABk−1),
Bk = −ABk−1 + αkI,
k = 1, 2, . . . , n.
Since Bn = 0, if αn ̸= 0 we get
A−1 = 1
αn
Bn−1,
and the computational cost of the method for a full matrix is equal to
(n −1)n3 ﬂops (for further details see [FF63], [Bar89]).

90
3. Direct Methods for the Solution of Linear Systems
3.7
Banded Systems
Discretization methods for boundary value problems often lead to solving
linear systems with matrices having banded, block or sparse forms. Ex-
ploiting the structure of the matrix allows for a dramatic reduction in the
computational costs of the factorization and of the substitution algorithms.
In the present and forthcoming sections, we shall address special variants
of MEG or LU factorization that are properly devised for dealing with ma-
trices of this kind. For the proofs and a more comprehensive treatment, we
refer to [GL89] and [Hig88] for banded or block matrices, while we refer to
[JM92], [GL81] and [Saa96] for sparse matrices and the techniques for their
storage.
The main result for banded matrices is the following.
Property 3.4 Let A∈Rn×n. Suppose that there exists a LU factorization
of A. If A has upper bandwidth q and lower bandwidth p, then L has lower
bandwidth p and U has upper bandwidth q.
In particular, notice that the same memory area used for A is enough to
also store its LU factorization. Consider, indeed, that a matrix A having
upper bandwidth q and lower bandwidth p is usually stored in a matrix B
(p + q + 1) × n, assuming that
bi−j+q+1,j = aij
for all the indices i, j that fall into the band of the matrix. For instance, in
the case of the tridiagonal matrix A=tridiag5(−1, 2, −1) (where q = p = 1),
the compact storage reads
B =


0
−1
−1
−1
−1
2
2
2
2
2
−1
−1
−1
−1
0

.
The same format can be used for storing the factorization LU of A. It is
clear that this storage format can be quite inconvenient in the case where
only a few bands of the matrix are large. In the limit, if only one column
and one row were full, we would have p = q = n and thus B would be a
full matrix with a lot of zero entries.
Finally, we notice that the inverse of a banded matrix is generally full
(as happens for the matrix A considered above).

3.7 Banded Systems
91
3.7.1
Tridiagonal Matrices
Consider the particular case of a linear system with nonsingular tridiagonal
matrix A given by
A =


a1
c1
0
b2
a2
...
...
cn−1
0
bn
an


.
In such an event, the matrices L and U of the LU factorization of A are
bidiagonal matrices of the form
L =


1
0
β2
1
...
...
0
βn
1


U =


α1
c1
0
α2
...
...
cn−1
0
αn


.
The coeﬃcients αi and βi can easily be computed by the following relations
α1 = a1,
βi =
bi
αi−1
,
αi = ai −βici−1, i = 2, . . . , n.
(3.53)
This is known as the Thomas algorithm and can be regarded as a particular
instance of the Doolittle factorization, without pivoting. When one is not
interested in storing the coeﬃcients of the original matrix, the entries αi
and βi can be overwritten on A.
The Thomas algorithm can also be extended to solve the whole tridi-
agonal system Ax = f. This amounts to solving two bidiagonal systems
Ly = f and Ux = y, for which the following formulae hold
(Ly = f)
y1 = f1,
yi = fi −βiyi−1,
i = 2, . . . , n,
(3.54)
(Ux = y) xn = yn
αn
,
xi = (yi −cixi+1) /αi,
i = n −1, . . . , 1.
(3.55)
The algorithm requires only 8n −7 ﬂops: precisely, 3(n −1) ﬂops for the
factorization (3.53) and 5n −4 ﬂops for the substitution procedure (3.54)-
(3.55).
As for the stability of the method, if A is a nonsingular tridiagonal matrix
and L and U are the factors actually computed, then
|δA| ≤(4u + 3u2 + u3)|L| |U|,

92
3. Direct Methods for the Solution of Linear Systems
where δA is implicitly deﬁned by the relation A + δA = LU while u is the
roundoﬀunit. In particular, if A is also symmetric and positive deﬁnite or
it is an M-matrix, we have
|δA| ≤4u + 3u2 + u3
1 −u
|A|,
which implies the stability of the factorization procedure in such cases. A
similar result holds even if A is diagonally dominant.
3.7.2
Implementation Issues
An implementation of the LU factorization for banded matrices is shown
in Program 10.
Program 10 - lu band : LU factorization for a banded matrix
function [A] = lu band (A,p,q)
[n,n]=size(A);
for k = 1:n-1
for i = k+1:min(k+p,n), A(i,k)=A(i,k)/A(k,k); end
for j = k+1:min(k+q,n)
for i = k+1:min(k+p,n), A(i,j)=A(i,j)-A(i,k)*A(k,j); end
end
end
In the case where n ≫p and n ≫q, this algorithm approximately takes
2npq ﬂops, with a considerable saving with respect to the case in which A
is a full matrix.
Similarly, ad hoc versions of the substitution methods can be devised
(see Programs 11 and 12). Their costs are, respectively, of the order of 2np
ﬂops and 2nq ﬂops, always assuming that n ≫p and n ≫q.
Program 11 - forw band : Forward substitution for a banded matrix L
function [b] = forw band (L, p, b)
[n,n]=size(L);
for j = 1:n
for i=j+1:min(j+p,n); b(i) = b(i) - L(i,j)*b(j); end
end
Program 12 - back band : Backward substitution for a banded matrix U
function [b] = back band (U, q, b)
[n,n]=size(U);
for j=n:-1:1
b (j) = b (j) / U (j,j);
for i = max(1,j-q):j-1, b(i)=b(i)-U(i,j)*b(j); end
end

3.8 Block Systems
93
The programs assume that the whole matrix is stored (including also the
zero entries).
Concerning the tridiagonal case, the Thomas algorithm can be imple-
mented in several ways. In particular, when implementing it on computers
where divisions are more costly than multiplications, it is possible (and
convenient) to devise a version of the algorithm without divisions in (3.54)
and (3.55), by resorting to the following form of the factorization
A = LDMT =


γ−1
1
0
0
b2
γ−1
2
...
...
...
0
0
bn
γ−1
n




γ1
0
γ2
...
0
γn




γ−1
1
c1
0
0
γ−1
2
...
...
...
cn−1
0
0
γ−1
n


The coeﬃcients γi can be recursively computed by the formulae
γi = (ai −biγi−1ci−1)−1,
for i = 1, . . . , n
where γ0 = 0, b1 = 0 and cn = 0 have been assumed. The forward and
backward substitution algorithms respectively read
(Ly = f)
y1 = γ1f1,
yi = γi(fi −biyi−1),
i = 2, . . . , n
(Ux = y)
xn = yn
xi = yi −γicixi+1,
i = n −1, . . . , 1.
(3.56)
In Program 13 we show an implementation of the Thomas algorithm in
the form (3.56), without divisions. The input vectors a, b and c contain
the coeﬃcients of the tridiagonal matrix {ai}, {bi} and {ci}, respectively,
while the vector f contains the components fi of the right-hand side f.
Program 13 - mod thomas : Thomas algorithm, modiﬁed version
function [x] = mod thomas (a,b,c,f)
n = size(a); b = [0; b]; c = [c; 0];
gamma (1) = 1/a (1);
for i =2:n, gamma(i)=1/(a(i)-b(i)*gamma(i-1)*c(i-1)); end
y (1) = gamma (1) * f (1);
for i = 2:n, y(i)=gamma(i)*(f(i)-b(i)*y(i-1)); end
x (n) = y (n);
for i = n-1:-1:1, x(i)=y(i)-gamma(i)*c(i)*x(i+1); end
3.8
Block Systems
In this section we deal with the LU factorization of block-partitioned matri-
ces, where each block can possibly be of a diﬀerent size. Our aim is twofold:
optimizing the storage occupation by suitably exploiting the structure of
the matrix and reducing the computational cost of the solution of the sys-
tem.

94
3. Direct Methods for the Solution of Linear Systems
3.8.1
Block LU Factorization
Let A∈Rn×n be the following block partitioned matrix
A =
 A11
A12
A21
A22

,
where A11 ∈Rr×r is a nonsingular square matrix whose factorization
L11D1R11 is known, while A22 ∈R(n−r)×(n−r). In such a case it is possible
to factorize A using only the LU factorization of the block A11. Indeed, it
is true that

A11
A12
A21
A22

=
 L11
0
L21
In−r
  D1
0
0
∆2
  R11
R12
0
In−r

,
where
L21 = A21R−1
11 D−1
1 , R12 = D−1
1 L−1
11 A12,
∆2 = A22 −L21D1R12.
If necessary, the reduction procedure can be repeated on the matrix ∆2,
thus obtaining a block-version of the LU factorization.
If A11 were a scalar, the above approach would reduce by one the size of
the factorization of a given matrix. Applying iteratively this method yields
an alternative way of performing the Gauss elimination.
We also notice that the proof of Theorem 3.4 can be extended to the
case of block matrices, obtaining the following result.
Theorem 3.7 Let A ∈Rn×n be partitioned in m × m blocks Aij with
i, j = 1, . . . , m. A admits a unique LU block factorization (with L having
unit diagonal entries) iﬀthe m −1 dominant principal block minors of A
are nonzero.
Since the block factorization is an equivalent formulation of the standard
LU factorization of A, the stability analysis carried out for the latter holds
for its block-version as well. Improved results concerning the eﬃcient use
in block algorithms of fast forms of matrix-matrix product are dealt with
in [Hig88]. In the forthcoming section we focus solely on block-tridiagonal
matrices.
3.8.2
Inverse of a Block-partitioned Matrix
The inverse of a block matrix can be constructed using the LU factorization
introduced in the previous section. A remarkable application is when A is
a block matrix of the form
A = C + UBV,

3.8 Block Systems
95
where C is a block matrix that is “easy” to invert (for instance, when C
is given by the diagonal blocks of A), while U, B and V take into account
the connections between the diagonal blocks. In such an event A can be
inverted by using the Sherman-Morrison or Woodbury formula
A−1 = (C + UBV)−1 = C−1 −C−1U

I + BVC−1U
−1 BVC−1,
(3.57)
having assumed that C and I + BVC−1U are two nonsingular matrices.
This formula has several practical and theoretical applications, and is par-
ticularly eﬀective if connections between blocks are of modest relevance.
3.8.3
Block Tridiagonal Systems
Consider block tridiagonal systems of the form
Anx =


A11
A12
0
A21
A22
...
...
...
An−1,n
0
An,n−1
Ann




x1
...
...
xn


=


b1
...
...
bn


,
(3.58)
where Aij are matrices of order ni ×nj and xi and bi are column vectors of
size ni, for i, j = 1, . . . , n. We assume that the diagonal blocks are squared,
although not necessarily of the same size. For k = 1, . . . , n, set
Ak =


In1
0
L1
In2
...
...
0
Lk−1
Ink




U1
A12
0
U2
...
...
Ak−1,k
0
Uk


.
Equating for k = n the matrix above with the corresponding blocks of An,
it turns out that U1 = A11, while the remaining blocks can be obtained
solving sequentially, for i = 2, . . . , n, the systems Li−1Ui−1 = Ai,i−1 for
the columns of L and computing Ui = Aii −Li−1Ai−1,i.
This procedure is well deﬁned only if all the matrices Ui are nonsingular,
which is the case if, for instance, the matrices A1, . . . , An are nonsingular.
As an alternative, one could resort to factorization methods for banded
matrices, even if this requires the storage of a large number of zero entries
(unless a suitable reordering of the rows of the matrix is performed).
A remarkable instance is when the matrix is block tridiagonal and sym-
metric, with symmetric and positive deﬁnite blocks. In such a case (3.58)

96
3. Direct Methods for the Solution of Linear Systems
takes the form


A11
AT
21
0
A21
A22
...
...
...
AT
n,n−1
0
An,n−1
Ann




x1
...
...
xn


=


b1
...
...
bn


.
Here we consider an extension to the block case of the Thomas algorithm,
which aims at transforming A into a block bidiagonal matrix. To this pur-
pose, we ﬁrst have to eliminate the block corresponding to matrix A21.
Assume that the Cholesky factorization of A11 is available and denote by
H11 the Cholesky factor. If we multiply the ﬁrst row of the block system
by H−T
11 , we ﬁnd
H11x1 + H−T
11 AT
21x2 = H−T
11 b1.
Letting H21 = H−T
11 AT
21 and c1 = H11b1, it follows that A21 = HT
21H11 and
thus the ﬁrst two rows of the system are
H11x1 + H21x2 = c1,
HT
21H11x1 + A22x2 + AT
32x3 = b2.
As a consequence, multiplying the ﬁrst row by HT
21 and subtracting it from
the second one, the unknown x1 is eliminated and the following equivalent
equation is obtained
A(1)
22 x2 + AT
32x3 = b2 −H21c1,
with A(1)
22 = A22 −HT
21H21. At this point, the factorization of A(1)
22 is carried
out and the unknown x3 is eliminated from the third row of the system,
and the same is repeated for the remaining rows of the system. At the end
of the procedure, which requires solving (n −1) n−1
j=1 nj linear systems to
compute the matrices Hi+1,i, i = 1, . . . , n−1, we end up with the following
block bidiagonal system


H11
H21
0
H22
...
...
Hn,n−1
0
Hnn




x1
...
...
xn


=


c1
...
...
cn


which can be solved with a (block) back substitution method. If all blocks
have the same size p, then the number of multiplications required by the
algorithm is about (7/6)(n−1)p3 ﬂops (assuming both p and n very large).

3.9 Sparse Matrices
97
3.9
Sparse Matrices
In this section we brieﬂy address the numerical solution of linear sparse
systems, that is, systems where the matrix A∈Rn×n has a number of
nonzero entries of the order of n (and not n2). We call a pattern of a sparse
matrix the set of its nonzero coeﬃcients.
Banded matrices with suﬃciently small bands are sparse matrices. Ob-
viously, for a sparse matrix the matrix structure itself is redundant and it
can be more conveniently substituted by a vector-like structure by means
of matrix compacting techniques, like the banded matrix format discussed
in Section 3.7.
x
x
x x x
x
x x x
x x x x
x x
x
x
x
x x
x x x
x
x x x
x
x
x
x
x
x
x
x
x
x
x
x x x
x
x
x
x x
x
x
x
x
x
x
x
x
1
2
3
4
5
6
7
8
9
10
11
12
FIGURE 3.3. Pattern of a symmetric sparse matrix (left) and of its associated
graph (right). For the sake of clarity, the loops have not been drawn; moreover,
since the matrix is symmetric, only one of the two sides associated with each
aij ̸= 0 has been reported
For sake of convenience, we associate with a sparse matrix A an oriented
graph G(A). A graph is a pair (V, X) where V is a set of p points and X
is a set of q ordered pairs of elements of V that are linked by a line. The
elements of V are called the vertices of the graph, while the connection lines
are called the paths of the graph.
The graph G(A) associated with a matrix A∈Rm×n can be constructed
by identifying the vertices with the set of the indices from 1 to the maximum
between m and n and supposing that a path exists which connects two
vertices i and j if aij ̸= 0 and is directed from i to j, for i = 1, . . . , m and
j = 1, . . . , n. For a diagonal entry aii ̸= 0, the path joining the vertex i
with itself is called a loop. Since an orientation is associated with each side,
the graph is called oriented (or ﬁnite directed). As an example, Figure 3.3
displays the pattern of a symmetric and sparse 12 × 12 matrix, together
with its associated graph.
As previously noticed, during the factorization procedure, nonzero entries
can be generated in memory positions that correspond to zero entries in

98
3. Direct Methods for the Solution of Linear Systems
the starting matrix. This action is referred to as ﬁll-in. Figure 3.4 shows the
eﬀect of ﬁll-in on the sparse matrix whose pattern is shown in Figure 3.3.
Since use of pivoting in the factorization process makes things even more
complicated, we shall only consider the case of symmetric positive deﬁnite
matrices for which pivoting is not necessary.
A ﬁrst remarkable result concerns the amount of ﬁll-in. Let mi(A) =
i −min {j < i : aij ̸= 0} and denote by E(A) the convex hull of A, given
by
E(A) = {(i, j) : 0 < i −j ≤mi(A)} .
(3.59)
For a symmetric positive deﬁnite matrix,
E(A) = E(H + HT )
(3.60)
where H is the Cholesky factor, so that ﬁll-in is conﬁned within the convex
hull of A (see Figure 3.4). Moreover, if we denote by lk(A) the number of
active rows at the k-th step of the factorization (i.e., the number of rows
of A with i > k and aik ̸= 0), the computational cost of the factorization
process is
1
2
n

k=1
lk(A) (lk(A) + 3)
ﬂops,
(3.61)
having accounted for all the nonzero entries of the convex hull. Conﬁnement
of ﬁll-in within E(A) ensures that the LU factorization of A can be stored
without extra memory areas simply by storing all the entries of E(A) (in-
cluding the null elements). However, such a procedure might still be highly
ineﬃcient due to the large number of zero entries in the hull (see Exercise
11).
On the other hand, from (3.60) one gets that the reduction in the convex
hull reﬂects a reduction of ﬁll-in, and in turn, due to (3.61), of the number
of operations needed to perform the factorization. For this reason several
strategies for reordering the graph of the matrix have been devised. Among
them, we recall the Cuthill-McKee method, which will be addressed in the
next section.
An alternative consists of decomposing the matrix into sparse subma-
trices, with the aim of reducing the original problem to the solution of
subproblems of reduced size, where matrices can be stored in full format.
This approach leads to submatrix decomposition methods which will be
addressed in Section 3.9.2.
3.9.1
The Cuthill-McKee Algorithm
The Cuthill-McKee algorithm is a simple and eﬀective method for reorder-
ing the system variables.

3.9 Sparse Matrices
99
                



         


      

       
 
x
x
x x
x x
x x
x
x
x
x
x
x
x
x x
x
x
x
x
x
x
x
x
x
x x
x
x
x
x
x
x
x
x x x
x
x x x
x x x x
x x
x
x
x
x
x x
x x x
x
x x x
x
x
x
x
x
x
x
x
x
x
x x x
x
x
x
x x
x
x
x
x
x
x
x
x
FIGURE 3.4. The shaded regions in the left ﬁgure show the areas of the matrix
that can be aﬀected by ﬁll-in, for the matrix considered in Figure 3.3. Solid
lines denote the boundary of E(A). The right ﬁgure displays the factors that
have been actually computed. Black dots denote the elements of A that were
originarily equal to zero
The ﬁrst step of the algorithm consists of associating with each vertex of
the graph the number of its connections with neighboring vertices, called
the degree of the vertex. Next, the following steps are taken:
1. a vertex with a low number of connections is chosen as the ﬁrst vertex
of the graph;
2. the vertices connected to it are progressively re-labeled starting from
those having lower degrees;
3. the procedure is repeated starting from the vertices connected to the
second vertex in the updated list. The nodes already re-labeled are
ignored. Then, a third new vertex is considered, and so on, until all
the vertices have been explored.
The usual way to improve the eﬃciency of the algorithm is based on the
so-called reverse form of the Cuthill-McKee method. This consists of ex-
ecuting the Cuthill-McKee algorithm described above where, at the end,
the i-th vertex is moved into the n −i + 1-th position of the list, n being
the number of nodes in the graph. Figure 3.5 reports, for comparison, the
graphs obtained using the direct and reverse Cuthill-McKee reordering in
the case of the matrix pattern represented in Figure 3.3, while in Figure
3.6 the factors L and U are compared. Notice the absence of ﬁll-in when
the reverse Cuthill-McKee method is used.
Remark 3.5 For an eﬃcient solution of linear systems with sparse ma-
trices, we mention the public domain libraries SPARSKIT [Saa90], UMF-
PACK [DD95] and the MATLAB sparfun package.
■

100
3. Direct Methods for the Solution of Linear Systems
1 (3)
2 (4)
3 (2)
4 (5)
5 (6)
6 (1)
7 (12)
8 (8)
9 (10)
10 (9)
11 (11)
12 (7)
1 (10)
2 (9)
3 (11)
4 (8)
5 (12)
6 (7)
7 (6)
8 (5)
9 (3)
10 (4)
11 (2)
12 (1)
FIGURE 3.5. Reordered graphs using the direct (left) and reverse (right)
Cuthill-McKee algorithm. The label of each vertex, before reordering is per-
formed, is reported in braces
x
x
x x x
x
x x
x x
x x
x
x
x x
x
x x
x
x x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x x x
x x
x x
x x
x
x x
x x
x x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x x
x
x
x
x
x x
x
x
x
x x
x x
x
x
x
x
FIGURE 3.6. Factors L and U after the direct (left) and reverse (right)
Cuthill-McKee reordering. In the second case, ﬁll-in is absent
3.9.2
Decomposition into Substructures
These methods have been developed in the framework of numerical ap-
proximation of partial diﬀerential equations. Their basic strategy consists
of splitting the solution of the original linear system into subsystems of
smaller size which are almost independent from each other and can be
easily interpreted as a reordering technique.
We describe the methods on a special example, referring for a more
comprehensive presentation to [BSG96]. Consider the linear system Ax=b,
where A is a symmetric positive deﬁnite matrix whose pattern is shown in
Figure 3.3. To help develop an intuitive understanding of the method, we
draw the graph of A in the form as in Figure 3.7.

3.9 Sparse Matrices
101
We then partition the graph of A into the two subgraphs (or substruc-
tures) identiﬁed in the ﬁgure and denote by xk, k = 1, 2, the vectors of the
unknowns relative to the nodes that belong to the interior of the k-th sub-
structure. We also denote by x3 the vector of the unknowns that lie along
the interface between the two substructures. Referring to the decomposi-
tion in Figure 3.7, we have x1 = (2, 3, 4, 6)T , x2 = (8, 9, 10, 11, 12)T
and x3 = (1, 5, 7)T .
As a result of the decomposition of the unknowns, matrix A will be
partitioned in blocks, so that the linear system can be written in the form
substructure II
6
7
3
2
4
5
1
12
8
9
11
10
substructure I
FIGURE 3.7. Decomposition into two substructures


A11
0
A13
0
A22
A23
AT
13
AT
23
A33




x1
x2
x3

=


b1
b2
b3

,
having reordered the unknowns and partitioned accordingly the right hand
side of the system. Suppose that A33 is decomposed into two parts, A′
33
and A′′
33, which represent the contributions to A33 of each substructure.
Similarly, let the right hand side b3 be decomposed as b′
3+b′′
3. The original
linear system is now equivalent to the following pair
 A11
A13
AT
13
A′
33
  x1
x3

=
 b1
b′
3 + γ3

,
 A22
A23
AT
23
A′′
33
  x2
x3

=
 b2
b′′
3 −γ3

having denoted by γ3 a vector that takes into account the coupling between
the substructures. A typical way of proceeding in decomposition techniques
consists of eliminating γ3 to end up with independent systems, one for each

102
3. Direct Methods for the Solution of Linear Systems
substructure. Let us apply this strategy to the example at hand. The linear
system for the ﬁrst substructure is
 A11
A13
AT
13
A′
33
  x1
x3

=
 b1
b′
3 + γ3

.
(3.62)
Let us now factorize A11 as HT
11H11 and proceed with the reduction method
already described in Section 3.8.3 for block tridiagonal matrices. We obtain
the system
 H11
H21
0
A′
33 −H21HT
21
  x1
x3

=
 c1
b′
3 + γ3 −H21c1

where H21 = H−T
11 A13 and c1 = H−T
11 b1. The second equation of this system
yields γ3 explicitly as
γ3 =

A′
33 −HT
21H21

x3 −b′
3 + HT
21c1.
Substituting this equation into the system for the second substructure, one
ends up with a system only in the unknowns x2 and x3
 A22
A23
AT
23
A′′′
33
  x2
x3

=
 b2
b′′′
3

,
(3.63)
where A′′′
33 = A33 −HT
21H21 and b′′′
3 = b3 −HT
21c1. Once (3.63) has been
solved, it will be possible, by backsubstitution into (3.62), to compute also
x1.
The technique described above can be easily extended to the case of
several substructures and its eﬃciency will increase the more the substruc-
tures are mutually independent. It reproduces in nuce the so-called frontal
method (introduced by Irons [Iro70]), which is quite popular in the solution
of ﬁnite element systems (for an implementation, we refer to the UMF-
PACK library [DD95]).
Remark 3.6 (The Schur complement) An approach that is dual to
the above method consists of reducing the starting system to a system
acting only on the interface unknowns x3, passing through the assembling
of the Schur complement of matrix A, deﬁned in the 3×3 case at hand as
S = A33 −AT
13A−1
11 A13 −AT
23A−1
22 A23.
The original problem is thus equivalent to the system
Sx3 = b3 −AT
13A−1
11 b1 −AT
23A−1
22 b2.
This system is full (even if the matrices Aij were sparse) and can be solved
using either a direct or an iterative method, provided that a suitable pre-
conditioner is available. Once x3 has been computed, one can get x1 and

3.10 Accuracy of the Solution Achieved Using GEM
103
x2 by solving two systems of reduced size, whose matrices are A11 and A22,
respectively.
We also notice that if the block matrix A is symmetric and positive
deﬁnite, then the linear system on the Schur complement S is no more
ill-conditioned than the original system on A, since
K2(S) ≤K2(A)
(for a proof, see Lemma 3.12, [Axe94]. See also [CM94] and [QV99]).
■
3.9.3
Nested Dissection
This is a renumbering technique quite similar to substructuring. In practice,
it consists of repeating the decomposition process several times at each
substructure level, until the size of each single block is made suﬃciently
small. In Figure 3.8 a possible nested dissection is shown in the case of the
matrix considered in the previous section. Once the subdivision procedure
has been completed, the vertices are renumbered starting with the nodes
belonging to the latest substructuring level and moving progressively up to
the ﬁrst level. In the example at hand, the new node ordering is 11, 9, 7,
6, 12, 8, 4, 2, 1, 5, 3.
This procedure is particularly eﬀective if the problem has a large size and
the substructures have few connections between them or exhibit a repetitive
pattern [Geo73].
3.10
Accuracy of the Solution Achieved Using
GEM
Let us analyze the eﬀects of rounding errors on the accuracy of the solution
yielded by GEM. Suppose that A and b are a matrix and a vector of
ﬂoating-point numbers. Denoting by L and U, respectively, the matrices
of the LU factorization induced by GEM and computed in ﬂoating-point
arithmetic, the solution x yielded by GEM can be regarded as being the
solution (in exact arithmetic) of the perturbed system (A + δA)x = b,
where δA is a perturbation matrix such that
|δA| ≤nu
+
3|A| + 5|L||U|
,
+ O(u2),
(3.64)
where u is the roundoﬀunit and the matrix absolute value notation has
been used (see [GL89], Section 3.4.6). As a consequence, the entries of δA
will be small in size if the entries of L and U are small. Using partial
pivoting allows for bounding below 1 the module of the entries of L in such
a way that, passing to the inﬁnity norm and noting that ∥L∥∞≤n, the

104
3. Direct Methods for the Solution of Linear Systems
1
A
2
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                






























                                                 






1
                         




A
A
2
1
2
      


    

B
A
6
5
4
3
C
         


         


         


    

                                                  









                                                                                           












A
4
A
                                                                                                                                                                                                                                                                                                                                                                                                                                                                  





























C
3
4
C
B
6B
5
6
5
3
FIGURE 3.8. Two steps of nested dissection. Graph partitioning (left) and matrix
reordering (right)
estimate (3.64) becomes
∥δA∥∞≤nu
+
3∥A∥∞+ 5n∥U∥∞
,
+ O(u2).
(3.65)
The bound for ∥δA∥∞in (3.65) is of practical use only if it is possible to
provide an estimate for ∥U∥∞. With this aim, backward analysis can be
carried out introducing the so-called growth factor
ρn =
max
i,j,k |a(k)
ij |
max
i,j |aij| .
(3.66)
Taking advantage of the fact that |uij| ≤ρnmax
i,j |aij|, the following result
due to Wilkinson can be drawn from (3.65),
∥δA∥∞≤8un3ρn∥A∥∞+ O(u2).
(3.67)
The growth factor can be bounded by 2n−1 and, although in most of the
cases it is of the order of 10, there exist matrices for which the inequality in
(3.67) becomes an equality (see, for instance, Exercise 5). For some special
classes of matrices, a sharp bound for ρn can be found:

3.10 Accuracy of the Solution Achieved Using GEM
105
1. for banded matrices with upper and lower bands equal to p, ρn ≤
22p−1 −(p −1)2p−2. As a consequence, in the tridiagonal case one
gets ρn ≤2;
2. for Hessenberg matrices, ρn ≤n;
3. for symmetric positive deﬁnite matrices, ρn = 1;
4. for matrices strictly diagonally dominant by columns, ρn ≤2.
To achieve better stability when using GEM for arbitrary matrices, re-
sorting to complete pivoting would seem to be mandatory, since it ensures
that ρn ≤n1/2 
2 · 31/2 · . . . · n1/(n−1)1/2. Indeed, this growth is slower
than 2n−1 as n increases.
However, apart from very special instances, GEM with only partial piv-
oting exhibits acceptable growth factors. This make it the most commonly
employed method in the computational practice.
Example 3.7 Consider the linear system (3.2) with
A =
 ε
1
1
0

,
b =
 1 + ε
1

,
(3.68)
which admits the exact solution x=1 for any value of ε. The matrix is well-
conditioned, having K∞(A) = (1 + ε)2. Attempting to solve the system for ε =
10−15 by the LU factorization with 16 signiﬁcant digits, and using the Programs
5, 2 and 3, yields the solution x = [0.8881784197001253, 1.000000000000000]T ,
with an error greater than 11% on the ﬁrst component. Some insight into the
causes of the inaccuracy of the computed solution can be drawn from (3.64).
Indeed this latter does not provide a uniformly small bound for all the entries of
matrix δA, rather
|δA| ≤
 3.55 · 10−30
1.33 · 10−15
1.33 · 10−15
2.22

.
Notice that the entries of the corresponding matrices L and U are quite large in
module. Conversely, resorting to GEM with partial or complete pivoting yields
the exact solution of the system (see Exercise 6).
•
Let us now address the role of the condition number in the error analysis
for GEM. GEM yields a solution x that is typically characterized by having
a small residual r = b −Ax (see [GL89]). This feature, however, does not
ensure that the error x −x is small when K(A) ≫1 (see Example 3.8). In
fact, if δb in (3.11) is regarded as being the residual, then
∥x −x∥
∥x∥
≤K(A)∥r∥
1
∥A∥∥x∥≤K(A) ∥r∥
∥b∥.
This result will be applied to devise methods, based on the a posteriori
analysis, for improving the accuracy of the solution of GEM (see Section
3.12).

106
3. Direct Methods for the Solution of Linear Systems
Example 3.8 Consider the linear system Ax = b with
A =

1
1.0001
1.0001
1

,
b =
 1
1

,
which admits the solution x = (0.499975 . . . , 0.499975 . . . )T . Assuming as an ap-
proximate solution the vector x = (−4.499775, 5.5002249)T , one ﬁnds the residual
r ≃(−0.001, 0)T , which is small although x is quite diﬀerent from the exact so-
lution. The reason for this is due to the ill-conditioning of matrix A. Indeed in
this case K∞(A) = 20001.
•
An estimate of the number of exact signiﬁcant digits of a numerical
solution of a linear system can be given as follows. From (3.13), letting
γ = u and assuming that uK∞(A) ≤1/2 we get
∥δx∥∞
∥x∥∞
≤
2uK∞(A)
1 −uK∞(A) ≤4uK∞(A).
As a consequence
∥x −x∥∞
∥x∥∞
≃uK∞(A).
(3.69)
Assuming that u ≃β−t and K∞(A) ≃βm, one gets that the solution x
computed by GEM will have at least t−m exact digits, t being the number
of digits available for the mantissa. In other words, the ill-conditioning of a
system depends both on the capability of the ﬂoating-point arithmetic that
is being used and on the accuracy that is required in the solution.
3.11
An Approximate Computation of K(A)
Suppose that the linear system (3.2) has been solved by a factorization
method. To determine the accuracy of the computed solution, the analy-
sis carried out in Section 3.10 can be used if an estimate of the condition
number K(A) of A, which we denote by K(A), is available. Indeed, al-
though evaluating ∥A∥can be an easy task if a suitable norm is chosen
(for instance, ∥· ∥1 or ∥· ∥∞), it is by no means reasonable (or compu-
tationally convenient) to compute A−1 if the only purpose is to evaluate
∥A−1∥. For this reason, we describe in this section a procedure (proposed
in [CMSW79]) that approximates ∥A−1∥with a computational cost of the
order of n2 ﬂops.
The basic idea of the algorithm is as follows: ∀d ∈Rn with d ̸= 0, thanks
to the deﬁnition of matrix norm, ∥A−1∥≥∥y∥/∥d∥= γ(d) with Ay = d.
Thus, we look for d in such a way that γ(d) is as large as possible and
assume the obtained value as an estimate of ∥A−1∥.
For the method to be eﬀective, the selection of d is crucial. To explain
how to do this, we start by assuming that the QR factorization of A has

3.11 An Approximate Computation of K(A)
107
been computed and that K2(A) is to be approximated. In such an event,
since K2(A) = K2(R) due to Property 1.8, it suﬃces to estimate ∥R−1∥2
instead of ∥A−1∥2. Considerations related to the SVD of R induce approx-
imating ∥R−1∥2 by the following algorithm:
compute the vectors x and y, solutions to the systems
RT x = d,
Ry = x,
(3.70)
then estimate ∥R−1∥2 by the ratio γ2 = ∥y∥2/∥x∥2. The vector d appearing
in (3.70) should be determined in such a way that γ2 is as close as possible
to the value actually attained by ∥R−1∥2. It can be shown that, except in
very special cases, γ2 provides for any choice of d a reasonable (although
not very accurate) estimate of ∥R−1∥2 (see Exercise 15). As a consequence,
a proper selection of d can encourage this natural trend.
Before going on, it is worth noting that computing K2(R) is not an easy
matter even if an estimate of ∥R−1∥2 is available. Indeed, it would remain
to compute ∥R∥2 =

ρ(RT R). To overcome this diﬃculty, we consider
henceforth K1(R) instead of K2(R) since ∥R∥1 is easily computable. Then,
heuristics allows us to assume that the ratio γ1 = ∥y∥1/∥x∥1 is an estimate
of ∥R−1∥1, exactly as γ2 is an estimate of ∥R−1∥2.
Let us now deal with the choice of d. Since RT x = d, the generic compo-
nent xk of x can be formally related to x1, . . . , xk−1 through the formulae
of forward substitution as
r11x0 = d1,
rkkxk = dk −(r1kx1 + . . . + rk−1,kxk−1),
k ≥1.
(3.71)
Assume that the components of d are of the form dk = ±θk, where θk
are random numbers and set arbitrarily d1 = θ1. Then, x1 = θ1/r11 is
completely determined, while x2 = (d2 −r12x1)/r22 depends on the sign of
d2. We set the sign of d2 as the opposite of r12x1 in such a way to make
∥x(1 : 2)∥1 = |x1| + |x2|, for a ﬁxed x1, the largest possible. Once x2 is
known, we compute x3 following the same criterion, and so on, until xn.
This approach sets the sign of each component of d and yields a vector
x with a presumably large ∥· ∥1. However, it can fail since it is based on
the idea (which is in general not true) that maximizing ∥x∥1 can be done
by selecting at each step k in (3.71) the component xk which guarantees
the maximum increase of ∥x(1 : k −1)∥1 (without accounting for the fact
that all the components are related).
Therefore, we need to modify the method by including a sort of “look-
ahead” strategy, which accounts for the way of choosing dk aﬀects all later
values xi, with i > k, still to be computed. Concerning this point, we notice
that for a generic row i of the system it is always possible to compute at

108
3. Direct Methods for the Solution of Linear Systems
step k the vector p(k−1) with components
p(k−1)
i
= 0
i = 1, . . . , k −1,
p(k−1)
i
= r1ix1 + . . . + rk−1,ixk−1
i = k, . . . , n.
Thus xk = (±θk −p(k−1)
k
)/rkk. We denote the two possible values of xk by
x+
k and x−
k . The choice between them is now taken not only accounting for
which of the two most increases ∥x(1 : k)∥1, but also evaluating the increase
of ∥p(k)∥1. This second contribution accounts for the eﬀect of the choice of
dk on the components that are still to be computed. We can include both
criteria in a unique test. Denoting by
p(k)+
i
= 0,
p(k)−
i
= 0,
i = 1, . . . , k,
p(k)+
i
= p(k−1)
i
+ rkix+
k ,
p(k)−
i
= p(k−1)
i
+ rkix−
k ,
i = k + 1, . . . , n,
the components of the vectors p(k)+ and p(k)−respectively, we set each
k-th step dk = +θk or dk = −θk according to whether |rkkx+
k | + ∥p(k)+∥1
is greater or less than |rkkx−
k | + ∥p(k)−∥1.
Under this choice d is completely determined and the same holds for x.
Now, solving the system Ry = x, we are warranted that ∥y∥1/∥x∥1 is a reli-
able approximation to ∥R−1∥1, so that we can set K1(A) = ∥R∥1∥y∥1/∥x∥1.
In practice the PA=LU factorization introduced in Section 3.5 is usually
available. Based on the previous considerations and on some heuristics, an
analogous procedure to that shown above can be conveniently employed
to approximate ∥A−1∥1. Precisely, instead of systems (3.70), we must now
solve
(LU)T x = d,
LUy = x.
We set ∥y∥1/∥x∥1 as the approximation of ∥A−1∥1 and, consequently, we
deﬁne K1(A). The strategy for selecting d can be the same as before;
indeed, solving (LU)T x = d amounts to solving
UT z = d,
LT x = z,
(3.72)
and thus, since UT is lower triangular, we can proceed as in the previous
case. A remarkable diﬀerence concerns the computation of x. Indeed, while
the matrix RT in the second system of (3.70) has the same condition number
as R, the second system in (3.72) has a matrix LT which could be even more
ill-conditioned than UT . If this were the case, solving for x could lead to
an inaccurate outcome, thus making the whole process useless.
Fortunately, resorting to partial pivoting prevents this circumstance from
occurring, ensuring that any ill-condition in A is reﬂected in a correspond-
ing ill-condition in U. Moreover, picking θk randomly between 1/2 and 1

3.12 Improving the Accuracy of GEM
109
guarantees accurate results even in the special cases where L turns out to
be ill-conditioned.
The algorithm presented below is implemented in the LINPACK library
[BDMS79] and in the MATLAB function rcond. This function, in order
to avoid rounding errors, returns as output parameter the reciprocal of
K1(A). A more accurate estimator, described in [Hig88], is implemented in
the MATLAB function condest.
Program 14 implements the approximate evaluation of K1 for a matrix
A of generic form. The input parameters are the size n of the matrix A, the
matrix A, the factors L, U of its PA=LU factorization and the vector theta
containing the random numbers θk, for k = 1, . . . , n.
Program 14 - cond est : Algorithm for the approximation of K1(A)
function [k1] = cond est(n,A,L,U,theta)
for i=1:n, p(i)=0; end
for k=1:n
zplus=(theta(k)-p(k))/U(k,k);
zminu=(-theta(k)-p(k))/U(k,k);
splus=abs(theta(k)-p(k));
sminu=abs(-theta(k)-p(k));
for i=(k+1):n
splus=splus+abs(p(i)+U(k,i)*zplus);
sminu=sminu+abs(p(i)+U(k,i)*zminu);
end
if splus >= sminu, z(k)=zplus;
else, z(k)=zminu; end
for i=(k+1):n, p(i)=p(i)+U(k,i)*z(k); end
end
z = z’; x = backward col(L’,z);
w = forward col(L,x);
y = backward col(U,w);
k1=norm(A,1)*norm(y,1)/norm(x,1);
Example 3.9 Let us consider the Hilbert matrix H4. Its condition number
K1(H4), computed using the MATLAB function invhilb which returns the exact
inverse of H4, is 2.8375 · 104. Running Program 14 with theta=(1, 1, 1, 1)T gives
the reasonable estimate K1(H4) = 2.1523 · 104 (which is the same as the output
of rcond), while the function condest returns the exact result.
•
3.12
Improving the Accuracy of GEM
As previously noted if the matrix of the system is ill-conditioned, the so-
lution generated by GEM could be inaccurate even though its residual is
small. In this section, we mention two techniques for improving the accu-
racy of the solution computed by GEM.

110
3. Direct Methods for the Solution of Linear Systems
3.12.1
Scaling
If the entries of A vary greatly in size, it is likely that during the elimination
process large entries are summed to small entries, with a consequent onset
of rounding errors. A remedy consists of performing a scaling of the matrix
A before the elimination is carried out.
Example 3.10 Consider again the matrix A of Remark 3.3. Multiplying it on
the right and on the left with matrix D=diag(0.0005, 1, 1), we obtain the scaled
matrix
˜A = DAD =


−0.0001
1
1
1
0.78125
0
1
0
0

.
Applying GEM to the scaled system ˜A˜x = Db = (0.2, 1.3816, 1.9273)T , we get
the correct solution x = D˜x.
•
Row scaling of A amounts to ﬁnding a diagonal nonsingular matrix D1
such that the diagonal entries of D1A are of the same size. The linear
system Ax = b transforms into
D1Ax = D1b.
When both rows and columns of A are to be scaled, the scaled version of
(3.2) becomes
(D1AD2)y = D1b
with y = D−1
2 x,
having also assumed that D2 is invertible. Matrix D1 scales the equations
while D2 scales the unknowns. Notice that, to prevent rounding errors, the
scaling matrices are chosen in the form
D1 = diag(βr1, . . . , βrn), D2 = diag(βc1, . . . , βcn),
where β is the base of the used ﬂoating-point arithmetic and the exponents
r1, . . . , rn, c1, . . . , cn must be determined. It can be shown that
∥D−1
2 (x −x)∥∞
∥D−1
2 x∥∞
≃uK∞(D1AD2).
Therefore, scaling will be eﬀective if K∞(D1AD2) is much less than K∞(A).
Finding convenient matrices D1 and D2 is not in general an easy matter.
A strategy consists, for instance, of picking up D1 and D2 in such a way
that ∥D1AD2∥∞and ∥D1AD2∥1 belong to the interval [1/β, 1], where β is
the base of the used ﬂoating-point arithmetic (see [McK62] for a detailed
analysis in the case of the Crout factorization).

3.12 Improving the Accuracy of GEM
111
Remark 3.7 (The Skeel condition number) The Skeel condition num-
ber, deﬁned as cond(A) = ∥|A−1| |A| ∥∞, is the supremum over the set
x∈Rn, with x ̸= 0, of the numbers
cond(A, x) = ∥|A−1| |A| |x| ∥∞
∥x∥∞
.
Unlike what happens for K(A), cond(A,x) is invariant with respect to a
scaling by rows of A, that is, to transformations of A of the form DA, where
D is a nonsingular diagonal matrix. As a consequence, cond(A) provides a
sound indication of the ill-conditioning of a matrix, irrespectively of any
possible row diagonal scaling.
■
3.12.2
Iterative Reﬁnement
Iterative reﬁnement is a technique for improving the accuracy of a solution
yielded by a direct method. Suppose that the linear system (3.2) has been
solved by means of LU factorization (with partial or complete pivoting),
and denote by x(0) the computed solution. Having ﬁxed an error tolerance,
toll, the iterative reﬁnement performs as follows: for i = 0, 1, . . . , until
convergence:
1. compute the residual r(i) = b −Ax(i);
2. solve the linear system Az = r(i) using the LU factorization of A;
3. update the solution setting x(i+1) = x(i) + z;
4. if ∥z∥/∥x(i+1)∥< toll, then terminate the process returning the solu-
tion x(i+1). Otherwise, the algorithm restarts at step 1.
In absence of rounding errors, the process would stop at the ﬁrst step,
yielding the exact solution. The convergence properties of the method
can be improved by computing the residual r(i) in double precision, while
computing the other quantities in single precision. We call this procedure
mixed-precision iterative reﬁnement (shortly, MPR), as compared to ﬁxed-
precision iterative reﬁnement (FPR).
It can be shown that, if ∥|A−1| |L| |U| ∥∞is suﬃciently small, then at
each step i of the algorithm, the relative error ∥x−x(i)∥∞/∥x∥∞is reduced
by a factor ρ, which is given by
ρ ≃2 n cond(A, x)u
(FPR),
ρ ≃u
(MPR),
where ρ is independent of the condition number of A in the case of MPR.
Slow convergence of FPR is a clear indication of the ill-conditioning of the

112
3. Direct Methods for the Solution of Linear Systems
matrix, as it can be shown that, if p is the number of iterations for the
method to converge, then K∞(A) ≃βt(1−1/p).
Even if performed in ﬁxed precision, iterative reﬁnement is worth using
since it improves the overall stability of any direct method for solving the
system. We refer to [Ric81], [Ske80], [JW77] [Ste73], [Wil63] and [CMSW79]
for an overview of this subject.
3.13
Undetermined Systems
We have seen that the solution of the linear system Ax=b exists and is
unique if n = m and A is nonsingular. In this section we give a meaning
to the solution of a linear system both in the overdetermined case, where
m > n, and in the underdetermined case, corresponding to m < n. We
notice that an underdetermined system generally has no solution unless
the right side b is an element of range(A).
For a detailed presentation, we refer to [LH74], [GL89] and [Bj¨o88].
Given A∈Rm×n with m ≥n, b∈Rm, we say that x∗∈Rn is a solution
of the linear system Ax=b in the least-squares sense if
Φ(x∗) = ∥Ax∗−b∥2
2 ≤min
x∈Rn∥Ax −b∥2
2 = min
x∈RnΦ(x).
(3.73)
The problem thus consists of minimizing the Euclidean norm of the resid-
ual. The solution of (3.73) can be found by imposing the condition that the
gradient of the function Φ in (3.73) must be equal to zero at x∗. From
Φ(x) = (Ax −b)T (Ax −b) = xT AT Ax −2xT AT b + bT b,
we ﬁnd that
∇Φ(x∗) = 2AT Ax∗−2AT b = 0,
from which it follows that x∗must be the solution of the square system
AT Ax∗= AT b
(3.74)
known as the system of normal equations. The system is nonsingular if
A has full rank and in such a case the least-squares solution exists and
is unique. We notice that B = AT A is a symmetric and positive deﬁnite
matrix. Thus, in order to solve the normal equations, one could ﬁrst com-
pute the Cholesky factorization B = HT H and then solve the two systems
HT y = AT b and Hx∗= y. However, due to roundoﬀerrors, the com-
putation of AT A may be aﬀected by a loss of signiﬁcant digits, with a
consequent loss of positive deﬁniteness or nonsingularity of the matrix, as
happens in the following example (implemented in MATLAB) where for a

3.13 Undetermined Systems
113
matrix A with full rank, the corresponding matrix fl(AT A) turns out to
be singular
A =


1
1
2−27
0
0
2−27

,
fl(AT A) =

1
1
1
1

.
Therefore, in the case of ill-conditioned matrices it is more convenient to
utilize the QR factorization introduced in Section 3.4.3. Indeed, the follow-
ing result holds.
Theorem 3.8 Let A ∈Rm×n, with m ≥n, be a full rank matrix. Then
the unique solution of (3.73) is given by
x∗= ˜R−1 ˜QT b
(3.75)
where ˜R ∈Rn×n and ˜Q ∈Rm×n are the matrices deﬁned in (3.48) starting
from the QR factorization of A. Moreover, the minimum of Φ is given by
Φ(x∗) =
m

i=n+1
[(QT b)i]2.
Proof. The QR factorization of A exists and is unique since A has full rank.
Thus, there exist two matrices, Q∈Rm×m and R∈Rm×n such that A=QR, where
Q is orthogonal. Since orthogonal matrices preserve the Euclidean scalar product
(see Property 1.8), it follows that
∥Ax −b∥2
2 = ∥Rx −QT b∥2
2.
Recalling that R is upper trapezoidal, we have
∥Rx −QT b∥2
2 = ∥˜Rx −˜QT b∥2
2 +
m

i=n+1
[(QT b)i]2,
so that the minimum is achieved when x = x∗.
3
For more details about the analysis of the computational cost the algo-
rithm (which depends on the actual implementation of the QR factoriza-
tion), as well as for results about its stability, we refer the reader to the
texts quoted at the beginning of the section.
If A does not have full rank, the solution techniques above fail, since in
this case if x∗is a solution to (3.73), the vector x∗+ z, with z ∈ker(A), is
a solution too. We must therefore introduce a further constraint to enforce
the uniqueness of the solution. Typically, one requires that x∗has minimal
Euclidean norm, so that the least-squares problem can be formulated as
ﬁnd x∗∈Rn with minimal Euclidean norm such that
∥Ax∗−b∥2
2 ≤min
x∈Rn∥Ax −b∥2
2.
(3.76)

114
3. Direct Methods for the Solution of Linear Systems
This problem is consistent with (3.73) if A has full rank, since in this case
(3.73) has a unique solution which necessarily must have minimal Euclidean
norm.
The tool for solving (3.76) is the singular value decomposition (or SVD,
see Section 1.9), for which the following theorem holds.
Theorem 3.9 Let A ∈Rm×n with SVD given by A = UΣVT . Then the
unique solution to (3.76) is
x∗= A†b
(3.77)
where A† is the pseudo-inverse of A introduced in Deﬁnition 1.15.
Proof. Using the SVD of A, problem (3.76) is equivalent to ﬁnding w = VT x
such that w has minimal Euclidean norm and
∥Σw −UT b∥2
2 ≤∥Σy −UT b∥2
2,
∀y ∈Rn.
If r is the number of nonzero singular values σi of A, then
∥Σw −UT b∥2
2 =
r

i=1
+
σiwi −(UT b)i
,2
+
m

i=r+1
+
(UT b)i
,2
,
which is minimum if wi = (UT b)i/σi for i = 1, . . . , r. Moreover, it is clear that
among the vectors w of Rn having the ﬁrst r components ﬁxed, the one with
minimal Euclidean norm has the remaining n −r components equal to zero.
Thus the solution vector is w∗= Σ†UT b, that is, x∗= VΣ†UT b = A†b, where
Σ† is the diagonal matrix deﬁned in (1.11).
3
As for the stability of problem (3.76), we point out that if the matrix
A does not have full rank, the solution x∗is not necessarily a continuous
function of the data, so that small changes on these latter might produce
large variations in x∗. An example of this is shown below.
Example 3.11 Consider the system Ax = b with
A =


1
0
0
0
0
0

,
b =


1
2
3

,
rank(A) = 1.
Using the MATLAB function svd we can compute the SVD of A. Then computing
the pseudo-inverse, one ﬁnds the solution vector x∗= (1, 0)T . If we perturb the
null entry a22, with the value 10−12, the perturbed matrix has (full) rank 2
and the solution (which is unique in the sense of (3.73)) is now given by x∗=

1, 2 · 1012T .
•
We refer the reader to Section 5.8.3 for the approximate computation of
the SVD of a matrix.

3.14 Applications
115
In the case of underdetermined systems, for which m < n, if A has full
rank the QR factorization can still be used. In particular, when applied
to the transpose matrix AT , the method yields the solution of minimal
euclidean norm. If, instead, the matrix has not full rank, one must resort
to SVD.
Remark 3.8 If m = n (square system), both SVD and QR factorization
can be used to solve the linear system Ax=b, as alternatives to GEM.
Even though these algorithms require a number of ﬂops far superior to
GEM (SVD, for instance, requires 12n3 ﬂops), they turn out to be more
accurate when the system is ill-conditioned and nearly singular.
■
Example 3.12 Compute the solution to the linear system H15x=b, where H15
is the Hilbert matrix of order 15 (see (3.32)) and the right side is chosen in
such a way that the exact solution is the unit vector x = 1. Using GEM with
partial pivoting yields a solution aﬀected by a relative error larger than 100%. A
solution of much better quality is obtained by passing through the computation
of the pseudo-inverse, where the entries in Σ that are less than 10−13 are set
equal to zero.
•
3.14
Applications
In this section we present two problems, suggested by structural mechan-
ics and grid generation in ﬁnite element analysis, whose solutions require
solving large linear systems.
3.14.1
Nodal Analysis of a Structured Frame
Let us consider a structured frame which is made by rectilinear beams con-
nected among them through hinges (referred to as the nodes) and suitably
constrained to the ground. External loads are assumed to be applied at
the nodes of the frame and for any beam in the frame the internal actions
amount to a unique force of constant strength and directed as the beam
itself. If the normal stress acting on the beam is a traction we assume
that it has positive sign, otherwise the action has negative sign. Structured
frames are frequently employed as covering structures for large size public
buildings like exhibition stands, railway stations or airport halls.
To determine the internal actions in the frame, that are the unknowns
of the mathematical problem, a nodal analysis is used (see [Zie77]): the
equilibrium with respect to translation is imposed at every node of the
frame yielding a sparse and large-size linear system. The resulting matrix
has a sparsity pattern which depends on the numbering of the unknowns
and that can strongly aﬀect the computational eﬀort of the LU factorization

116
3. Direct Methods for the Solution of Linear Systems
due to ﬁll-in. We will show that the ﬁll-in can be dramatically reduced by
a suitable reordering of the unknowns.
The structure shown in Figure 3.9 is arc-shaped and is symmetric with
respect to the origin. The radii r and R of the inner and outer circles are
equal to 1 and 2, respectively. An external vertical load of unit size directed
downwards is applied at (0, 1) while the frame is constrained to ground
through a hinge at (−(r + R), 0) and a bogie at (r + R, 0). To generate
the structure we have partitioned the half unit circle in nθ uniform slices,
resulting in a total number of n = 2(nθ + 1) nodes and a matrix size of
m = 2n. The structure in Figure 3.9 has nθ = 7 and the unknowns are
numbered following a counterclockwise labeling of the beams starting from
the node at (1, 0).
We have represented the structure along with the internal actions com-
puted by solving the nodal equilibrium equations where the width of the
beams is proportional to the strength of the computed action. Black is
used to identify tractions whereas gray is associated with compressions. As
expected the maximum traction stress is attained at the node where the
external load is applied.
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
−0.5
0
0.5
1
1.5
2
2.5
FIGURE 3.9. A structured frame loaded at the point (0, 1)
We show in Figure 3.10 the sparsity pattern of matrix A (left) and that
of the L-factor of its LU factorization with partial pivoting (right) in the
case nθ = 40 which corresponds to a size of 164 × 164. Notice the large
ﬁll-in eﬀect arising in the lower part of L which results in an increase of
the nonzero entries from 645 (before the factorization) to 1946 (after the
factorization).

3.14 Applications
117
0
20
40
60
80
100
120
140
160
0
20
40
60
80
100
120
140
160
nz = 645
0
20
40
60
80
100
120
140
160
0
20
40
60
80
100
120
140
160
nz = 1946
FIGURE 3.10. Sparsity pattern of matrix A (left) and of the L-factor of the LU
factorization with partial pivoting (right) in the case nθ = 40
In view of the solution of the linear system by a direct method, the
increase of the nonzero entries demands for a suitable reordering of the
unknowns. For this purpose we use the MATLAB function symrcm which
implements the symmetric reverse Cuthill-McKee algorithm described in
Section 3.9.1. The sparsity pattern, after reordering, is shown in Figure 3.11
(left) while the L-factor of the LU factorization of the reordered matrix
is shown in Figure 3.11 (right). The results indicate that the reordering
procedure has “scattered” the sparsity pattern throughout the matrix with
a relatively modest increase of the nonzero entries from 645 to 1040.
0
20
40
60
80
100
120
140
160
0
20
40
60
80
100
120
140
160
nz = 645
0
20
40
60
80
100
120
140
160
0
20
40
60
80
100
120
140
160
nz = 1040
FIGURE 3.11. Sparsity pattern of matrix A (left) after a reordering with the sym-
metric reverse Cuthill-McKee algorithm and the L-factor of the LU factorization
of the reordered matrix with partial pivoting (right) in the case nθ = 40
The eﬀectiveness of the symmetric reverse Cuthill-McKee reordering pro-
cedure is demonstrated in Figure 3.12 which shows the number of nonzero
entries nz in the L-factor of A as a function of the size m of the matrix
(represented on the x-axis). In the reordered case (solid line) a linear in-

118
3. Direct Methods for the Solution of Linear Systems
crease of nz with m can be clearly appreciated at the expense of a dramatic
ﬁll-in growing with m if no reordering is performed (dashed line).
0
100
200
300
400
500
600
700
800
900
1000
0
1
2
3
4
5
6 x 10
4
FIGURE 3.12. Number of nonzero entries in the L-factor of A as a function of
the size m of the matrix, with (solid line) and without (dashed line) reordering
3.14.2
Regularization of a Triangular Grid
The numerical solution of a problem in a two-dimensional domain D of
polygonal form, for instance by ﬁnite element or ﬁnite diﬀerence methods,
very often requires that D be decomposed in smaller subdomains, usually
of triangular form (see for instance Section 9.9.2).
Suppose that D =
-
T ∈Th
T, where Th is the considered triangulation (also
called computational grid) and h is a positive parameter which characterizes
the triangulation. Typically, h denotes the maximum length of the triangle
edges. We shall also assume that two triangles of the grid, T1 and T2, have
either null intersection or share a vertex or a side.
The geometrical properties of the computational grid can heavily aﬀect the
quality of the approximate numerical solution. It is therefore convenient to
devise a suﬃciently regular triangulation, such that, for any T ∈Th, the
ratio between the maximum length of the sides of T (the diameter of T)
and the diameter of the circle inscribed within T (the sphericity of T) is
bounded by a constant independent of T. This latter requirement can be
satisﬁed employing a regularization procedure, applied to an existing grid.
We refer to [Ver96] for further details on this subject.
Let us assume that Th contains NT triangles and N vertices, of which Nb,
lying on the boundary ∂D of D, are kept ﬁxed and having coordinates
x(∂D)
i
= (x(∂D)
i
, y(∂D)
i
). We denote by Nh the set of grid nodes, excluding
the boundary nodes, and for each node xi = (xi, yi)T ∈Nh, let Pi and Zi
respectively be the set of triangles T ∈Th sharing xi (called the patch of

3.14 Applications
119
T
k
xi
xj
x
FIGURE 3.13. An example of a decomposition into triangles of a polygonal do-
main D (left), and the eﬀect of the barycentric regularization on a patch of
triangles (right). The newly generated grid is plotted in dashed line
xi) and the set of nodes of Pi except node xi itself (see Figure 3.13, right).
We let ni = dim(Zi).
The regularization procedure consists of moving the generic node xi to
a new position which is determined by the center of gravity of the polygon
generated by joining the nodes of Zi, and for that reason it is called a
barycentric regularization. The eﬀect of such a procedure is to force all the
triangles that belong to the interior of the domain to assume a shape that
is as regular as possible (in the limit, each triangle should be equilateral).
In practice, we let
xi =


xj∈Zi
xj

/ni,
∀xi ∈Nh,
xi = x(∂D)
i
if xi ∈∂D.
Two systems must then be solved, one for the x-components {xi} and the
other for the y-components {yi}. Denoting by zi the generic unknown, the
i-th row of the system, in the case of internal nodes, reads
nizi −

zj∈Zi
zj = 0,
∀i ∈Nh,
(3.78)
while for the boundary nodes the identities zi = z(∂D)
i
hold. Equations
(3.78) yield a system of the form Az = b, where A is a symmetric and pos-
itive deﬁnite matrix of order N −Nb which can be shown to be an M-matrix
(see Section 1.12). This property ensures that the new grid coordinates sat-
isfy minimum and maximum discrete principles, that is, they take a value
which is between the minimum and the maximum values attained on the
boundary.
Let us apply the regularization technique to the triangulation of the unit
square in Figure 3.14, which is aﬀected by a severe non uniformity of the
triangle size. The grid consists of NT = 112 triangles and N = 73 vertices,

120
3. Direct Methods for the Solution of Linear Systems
0
0.2
0.4
0.6
0.8
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.2
0.4
0.6
0.8
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
FIGURE 3.14. Triangulation before (left) and after (right) the regularization
of which Nb = 32 are on the boundary. The size of each of the two linear
systems (3.78) is thus equal to 41 and their solution is carried out by the
LU factorization of matrix A in its original form (1) and using its sparse
format (2), obtained using the Cuthill-McKee inverse reordering algorithm
described in Section 3.9.1.
In Figure 3.15 the sparsity patterns of A are displayed, without and with
reordering; the integer nz = 237 denotes the number of nonzero entries
in the matrix. Notice that in the second case there is a decrease in the
bandwidth of the matrix, to which corresponds a large reduction in the
operation count from 61623 to 5552. The ﬁnal conﬁguration of the grid is
displayed in Figure 3.14 (right), which clearly shows the eﬀectiveness of the
regularization procedure.
0
5
10
15
20
25
30
35
40
0
5
10
15
20
25
30
35
40
nz = 237
0
5
10
15
20
25
30
35
40
0
5
10
15
20
25
30
35
40
nz = 237
FIGURE 3.15. Sparsity patterns of matrix A without and with reordering (left
and right, respectively)

3.15 Exercises
121
3.15
Exercises
1. For any square matrix A∈Rn×n, prove the following relations
1
nK2(A) ≤K1(A) ≤nK2(A),
1
nK∞(A) ≤K2(A) ≤nK∞(A),
1
n2 K1(A) ≤K∞(A) ≤n2K1(A).
They allow us to conclude that if a matrix is ill-conditioned in a certain
norm it remains so even in another norm, up to a factor depending on n.
2. Check that the matrix B ∈Rn×n: bii = 1, bij = −1 if i < j, bij = 0 if
i > j, has determinant equal to 1, yet K∞(B) is large (equal to n2n−1).
3. Prove that K(AB) ≤K(A)K(B), for any two square nonsingular matrices
A,B∈Rn×n.
4. Given the matrix A ∈R2×2, a11 = a22 = 1, a12 = γ, a21 = 0, check that
for γ ≥0, K∞(A) = K1(A) = (1 + γ)2. Next, consider the linear system
Ax = b where b is such that x = (1 −γ, 1)T is the solution. Find a bound
for ∥δx∥∞/∥x∥∞in terms of ∥δb∥∞/∥b∥∞when δb = (δ1, δ2)T . Is the
problem well- or ill-conditioned?
5. Consider the matrix A ∈Rn×n, with entries aij = 1 if i = j or j = n,
aij = −1 if i > j, zero otherwise. Show that A admits an LU factorization,
with |lij| ≤1 and unn = 2n−1.
6. Consider matrix (3.68) in Example 3.7. Prove that the matrices L and U
have entries very large in module. Check that using GEM with complete
pivoting yields the exact solution.
7. Devise a variant of GEM that transforms a nonsingular matrix A ∈Rn×n
directly into a diagonal matrix D. This process is commonly known as the
Gauss-Jordan method. Find the Gauss-Jordan transformation matrices Gi,
i = 1, . . . , n, such that Gn . . . G1A = D.
8. Let A be a sparse matrix of order n. Prove that the computational cost of
the LU factorization of A is given by (3.61). Prove also that it is always
less than
1
2
n

k=1
mk(A) (mk(A) + 3) .
9. Prove that, if A is a symmetric and positive deﬁnite matrix, solving the
linear system Ax = b amounts to computing x= n
i=1(ci/λi)vi, where λi
are the eigenvalues of A and vi are the corresponding eigenvectors.
10. (From [JM92]). Consider the following linear system
 1001
1000
1000
1001
  x1
x2

=
 b1
b2

.
Using Exercise 9, explain why, when b = (2001, 2001)T , a small change
δb = (1, 0)T produces large variations in the solution, while, conversely,

122
3. Direct Methods for the Solution of Linear Systems
when b = (1, −1)T , a small variation δx = (0.001, 0)T in the solution
induces a large change in b.
[Hint : expand the right hand side on the basis of the eigenvectors of the
matrix.]
11. Characterize the ﬁll-in for a matrix A ∈Rn×n having nonzero entries only
on the main diagonal and on the ﬁrst column and last row. Propose a
permutation that minimizes the ﬁll-in.
[Hint : it suﬃces to exchange the ﬁrst row and the ﬁrst column with the
last row and the last column, respectively.]
12. Consider the linear system Hnx = b, where Hn is the Hilbert matrix of
order n. Estimate, as a function of n, the maximum number of signiﬁcant
digits that are expected when solving the system by GEM.
13. Given the vectors
v1 = [1, 1, 1, −1]T ,
v2 = [2, −1, −1, 1]T
v3 = [0, 3, 3, −3]T ,
v4 = [−1, 2, 2, 1]T
generate an orthonormal system using the Gram-Schmidt algorithm, in
either its standard and modiﬁed versions, and compare the obtained results.
What is the dimension of the space generated by the given vectors?
14. Prove that if A=QR then
1
nK1(A) ≤K1(R) ≤nK1(A),
while K2(A) = K2(R).
15. Let A ∈Rn×n be a nonsingular matrix. Determine the conditions under
which the ratio ∥y∥2/∥x∥2, with x and y as in (3.70), approximates ∥A−1∥2.
[Solution : let UΣVT be the singular value decomposition of A. Denote
by ui, vi the column vectors of U and V, respectively, and expand the
vector d in (3.70) on the basis spanned by {vi}. Then d = n
i=1 ˜divi and,
from (3.70), x = n
i=1( ˜di/σi)ui, y = n
i=1( ˜di/σ2
i )vi, having denoted the
singular values of A by σ1, . . . , σn.
The ratio
∥y∥2/∥x∥2 =
. n

i=1
( ˜di/σ2
i )2/
n

i=1
( ˜di/σi)2
/1/2
is about equal to σ−1
n
= ∥A−1∥2 if: (i) y has a relevant component in the
direction of vn (i.e., if ˜dn is not excessively small), and (ii) the ratio ˜dn/σn
is not negligible with respect to the ratios ˜di/σi for i = 1, . . . , n −1. This
last circumstance certainly occurs if A is ill-conditioned in the ∥· ∥2-norm
since σn ≪σ1.]

4
Iterative Methods for Solving Linear
Systems
Iterative methods formally yield the solution x of a linear system after an
inﬁnite number of steps. At each step they require the computation of the
residual of the system. In the case of a full matrix, their computational
cost is therefore of the order of n2 operations for each iteration, to be
compared with an overall cost of the order of 2
3n3 operations needed by
direct methods. Iterative methods can therefore become competitive with
direct methods provided the number of iterations that are required to con-
verge (within a prescribed tolerance) is either independent of n or scales
sublinearly with respect to n.
In the case of large sparse matrices, as discussed in Section 3.9, direct
methods may be unconvenient due to the dramatic ﬁll-in, although ex-
tremely eﬃcient direct solvers can be devised on sparse matrices featuring
special structures like, for example, those encountered in the approximation
of partial diﬀerential equations (see Chapters 12 and 13).
Finally, we notice that, when A is ill-conditioned, a combined use of direct
and iterative methods is made possible by preconditioning techniques that
will be addressed in Section 4.3.2.
4.1
On the Convergence of Iterative Methods
The basic idea of iterative methods is to construct a sequence of vectors
x(k) that enjoy the property of convergence
x = lim
k→∞x(k),
(4.1)

124
4. Iterative Methods for Solving Linear Systems
where x is the solution to (3.2). In practice, the iterative process is stopped
at the minimum value of n such that ∥x(n) −x∥< ε, where ε is a ﬁxed
tolerance and ∥· ∥is any convenient vector norm. However, since the exact
solution is obviously not available, it is necessary to introduce suitable
stopping criteria to monitor the convergence of the iteration (see Section
4.6).
To start with, we consider iterative methods of the form
x(0) given,
x(k+1) = Bx(k) + f,
k ≥0,
(4.2)
having denoted by B an n × n square matrix called the iteration matrix
and by f a vector that is obtained from the right hand side b.
Deﬁnition 4.1 An iterative method of the form (4.2) is said to be consis-
tent with (3.2) if f and B are such that x = Bx + f. Equivalently,
f = (I −B)A−1b.
■
Having denoted by
e(k) = x(k) −x
(4.3)
the error at the k-th step of the iteration, the condition for convergence
(4.1) amounts to requiring that lim
k→∞e(k) = 0 for any choice of the initial
datum x(0) (often called the initial guess).
Consistency alone does not suﬃce to ensure the convergence of the iter-
ative method (4.2), as shown in the following example.
Example 4.1 To solve the linear system 2Ix = b, consider the iterative method
x(k+1) = −x(k) + b,
which is obviously consistent. This scheme is not convergent for any choice of
the initial guess. If, for instance, x(0) = 0, the method generates the sequence
x(2k) = 0, x(2k+1) = b, k = 0, 1, . . . .
On the other hand, if x(0) = 1
2b the method is convergent.
•
Theorem 4.1 Let (4.2) be a consistent method. Then, the sequence of vec-
tors

x(k)
converges to the solution of (3.2) for any choice of x(0) iﬀ
ρ(B) < 1.
Proof. From (4.3) and the consistency assumption, the recursive relation e(k+1) =
Be(k) is obtained. Therefore,
e(k) = Bke(0),
∀k = 0, 1, . . .
(4.4)

4.1 On the Convergence of Iterative Methods
125
Thus, thanks to Theorem 1.5, it follows that lim
k→∞Bke(0) = 0 for any e(0) iﬀ
ρ(B) < 1.
Conversely, suppose that ρ(B) > 1, then there exists at least one eigenvalue
λ(B) with module greater than 1. Let e(0) be an eigenvector associated with λ;
then Be(0) = λe(0) and, therefore, e(k) = λke(0). As a consequence, e(k) cannot
tend to 0 as k →∞, since |λ| > 1.
3
From (1.23) and Theorem 1.5 it follows that a suﬃcient condition for con-
vergence to hold is that ∥B∥< 1, for any matrix norm. It is reasonable
to expect that the convergence is faster when ρ(B) is smaller so that an
estimate of ρ(B) might provide a sound indication of the convergence of
the algorithm. Other remarkable quantities in convergence analysis are con-
tained in the following deﬁnition.
Deﬁnition 4.2 Let B be the iteration matrix. We call:
1. ∥Bm∥the convergence factor after m steps of the iteration;
2. ∥Bm∥1/m the average convergence factor after m steps;
3. Rm(B) = −1
m log ∥Bm∥the average convergence rate after m steps.
■
These quantities are too expensive to compute since they require evaluating
Bm. Therefore, it is usually preferred to estimate the asymptotic conver-
gence rate, which is deﬁned as
R(B) = lim
k→∞Rk(B) = −log ρ(B)
(4.5)
where Property 1.13 has been accounted for. In particular, if B were sym-
metric, we would have
Rm(B) = −1
m log ∥Bm∥2 = −log ρ(B).
In the case of nonsymmetric matrices, ρ(B) sometimes provides an overop-
timistic estimate of ∥Bm∥1/m (see [Axe94], Section 5.1). Indeed, although
ρ(B) < 1, the convergence to zero of the sequence ∥Bm∥might be non-
monotone (see Exercise 1). We ﬁnally notice that, due to (4.5), ρ(B) is
the asymptotic convergence factor. Criteria for estimating the quantities
deﬁned so far will be addressed in Section 4.6.
Remark 4.1 The iterations introduced in (4.2) are a special instance of
iterative methods of the form
x(0) = f0(A, b),
x(n+1) = fn+1(x(n), x(n−1), . . . , x(n−m), A, b), for n ≥m,

126
4. Iterative Methods for Solving Linear Systems
where fi and x(m), . . . , x(1) are given functions and vectors, respectively.
The number of steps which the current iteration depends on is called the
order of the method. If the functions fi are independent of the step index i,
the method is called stationary, otherwise it is nonstationary. Finally, if fi
depends linearly on x(0), . . . , x(m), the method is called linear, otherwise
it is nonlinear.
In the light of these deﬁnitions, the methods considered so far are there-
fore stationary linear iterative methods of ﬁrst order. In Section 4.3, exam-
ples of nonstationary linear methods will be provided.
■
4.2
Linear Iterative Methods
A general technique to devise consistent linear iterative methods is based
on an additive splitting of the matrix A of the form A=P−N, where P
and N are two suitable matrices and P is nonsingular. For reasons that
will be clear in the later sections, P is called preconditioning matrix or
preconditioner.
Precisely, given x(0), one can compute x(k) for k ≥1, solving the systems
Px(k+1) = Nx(k) + b,
k ≥0.
(4.6)
The iteration matrix of method (4.6) is B = P−1N, while f = P−1b. Alter-
natively, (4.6) can be written in the form
x(k+1) = x(k) + P−1r(k),
(4.7)
where
r(k) = b −Ax(k)
(4.8)
denotes the residual vector at step k. Relation (4.7) outlines the fact that
a linear system, with coeﬃcient matrix P, must be solved to update the
solution at step k+1. Thus P, besides being nonsingular, ought to be easily
invertible, in order to keep the overall computational cost low. (Notice that,
if P were equal to A and N=0, method (4.7) would converge in one iteration,
but at the same cost of a direct method).
Let us mention two results that ensure convergence of the iteration (4.7),
provided suitable conditions on the splitting of A are fulﬁlled (for their
proof, we refer to [Hac94]).
Property 4.1 Let A = P −N, with A and P symmetric and positive def-
inite. If the matrix 2P −A is positive deﬁnite, then the iterative method
deﬁned in (4.7) is convergent for any choice of the initial datum x(0) and
ρ(B) = ∥B∥A = ∥B∥P < 1.

4.2 Linear Iterative Methods
127
Moreover, the convergence of the iteration is monotone with respect to the
norms ∥· ∥P and ∥· ∥A (i.e., ∥e(k+1)∥P < ∥e(k)∥P and ∥e(k+1)∥A < ∥e(k)∥A
k = 0, 1, . . . ).
Property 4.2 Let A = P −N with A symmetric and positive deﬁnite. If
the matrix P+PT −A is positive deﬁnite, then P is invertible, the iterative
method deﬁned in (4.7) is monotonically convergent with respect to norm
∥· ∥A and ρ(B) ≤∥B∥A < 1.
4.2.1
Jacobi, Gauss-Seidel and Relaxation Methods
In this section we consider some classical linear iterative methods.
If the diagonal entries of A are nonzero, we can single out in each equation
the corresponding unknown, obtaining the equivalent linear system
xi = 1
aii

bi −
n

j=1
j̸=i
aijxj

,
i = 1, . . . , n.
(4.9)
In the Jacobi method, once an arbitrarily initial guess x0 has been chosen,
x(k+1) is computed by the formulae
x(k+1)
i
= 1
aii

bi −
n

j=1
j̸=i
aijx(k)
j

,
i = 1, . . . , n.
(4.10)
This amounts to performing the following splitting for A
P = D,
N = D −A = E + F,
where D is the diagonal matrix of the diagonal entries of A, E is the lower
triangular matrix of entries eij = −aij if i > j, eij = 0 if i ≤j, and F is
the upper triangular matrix of entries fij = −aij if j > i, fij = 0 if j ≤i.
As a consequence, A=D-(E+F).
The iteration matrix of the Jacobi method is thus given by
BJ = D−1(E + F) = I −D−1A.
(4.11)
A generalization of the Jacobi method is the over-relaxation method
(or JOR), in which, having introduced a relaxation parameter ω, (4.10) is
replaced by
x(k+1)
i
= ω
aii

bi −
n

j=1
j̸=i
aijx(k)
j

+ (1 −ω)x(k)
i
,
i = 1, . . . , n.

128
4. Iterative Methods for Solving Linear Systems
The corresponding iteration matrix is
BJω = ωBJ + (1 −ω)I.
(4.12)
In the form (4.7), the JOR method corresponds to
x(k+1) = x(k) + ωD−1r(k).
This method is consistent for any ω ̸= 0 and for ω = 1 it coincides with
the Jacobi method.
The Gauss-Seidel method diﬀers from the Jacobi method in the fact that
at the k + 1-th step the available values of x(k+1)
i
are being used to update
the solution, so that, instead of (4.10), one has
x(k+1)
i
= 1
aii

bi −
i−1

j=1
aijx(k+1)
j
−
n

j=i+1
aijx(k)
j

,
i = 1, . . . , n.
(4.13)
This method amounts to performing the following splitting for A
P = D −E,
N = F,
and the associated iteration matrix is
BGS = (D −E)−1F.
(4.14)
Starting from Gauss-Seidel method, in analogy to what was done for
Jacobi iterations, we introduce the successive over-relaxation method (or
SOR method)
x(k+1)
i
= ω
aii

bi −
i−1

j=1
aijx(k+1)
j
−
n

j=i+1
aijx(k)
j

+ (1 −ω)x(k)
i
,
(4.15)
for i = 1, . . . , n. The method (4.15) can be written in vector form as
(I −ωD−1E)x(k+1) = [(1 −ω)I + ωD−1F]x(k) + ωD−1b
(4.16)
from which the iteration matrix is
B(ω) = (I −ωD−1E)−1[(1 −ω)I + ωD−1F].
(4.17)
Multiplying by D both sides of (4.16) and recalling that A = D −(E + F)
yields the following form (4.7) of the SOR method
x(k+1) = x(k) +
 1
ω D −E
−1
r(k).
It is consistent for any ω ̸= 0 and for ω = 1 it coincides with Gauss-Seidel
method. In particular, if ω ∈(0, 1) the method is called under-relaxation,
while if ω > 1 it is called over-relaxation.

4.2 Linear Iterative Methods
129
4.2.2
Convergence Results for Jacobi and Gauss-Seidel
Methods
There exist special classes of matrices for which it is possible to state a
priori some convergence results for the methods examined in the previous
section. The ﬁrst result in this direction is the following.
Theorem 4.2 If A is a strictly diagonally dominant matrix by rows, the
Jacobi and Gauss-Seidel methods are convergent.
Proof. Let us prove the part of the theorem concerning the Jacobi method, while
for the Gauss-Seidel method we refer to [Axe94]. Since A is strictly diagonally
dominant by rows, |aii| > n
j=1 |aij| for j ̸= i and i = 1, . . . , n. As a consequence,
∥BJ∥∞=
max
i=1,... ,n
n

j=1,j̸=i
|aij|/|aii| < 1, so that the Jacobi method is convergent.
3
Theorem 4.3 If A and 2D−A are symmetric and positive deﬁnite matri-
ces, then the Jacobi method is convergent and ρ(BJ) = ∥BJ∥A = ∥BJ∥D.
Proof. The theorem follows from Property 4.1 taking P=D.
3
In the case of the JOR method, the assumption on 2D−A can be removed,
yielding the following result.
Theorem 4.4 If A if symmetric positive deﬁnite, then the JOR method is
convergent if 0 < ω < 2/ρ(D−1A).
Proof. The result immediately follows from (4.12) and noting that A has real
positive eigenvalues.
3
Concerning the Gauss-Seidel method, the following result holds.
Theorem 4.5 If A is symmetric positive deﬁnite, the Gauss-Seidel method
is monotonically convergent with respect to the norm ∥· ∥A.
Proof. We can apply Property 4.2 to the matrix P=D−E, upon checking that
P + PT −A is positive deﬁnite. Indeed
P + PT −A = 2D −E −F −A = D,
having observed that (D −E)T = D −F. We conclude by noticing that D is
positive deﬁnite, since it is the diagonal of A.
3
Finally, if A is positive deﬁnite and tridiagonal, it can be shown that also
the Jacobi method is convergent and
ρ(BGS) = ρ2(BJ).
(4.18)

130
4. Iterative Methods for Solving Linear Systems
In this case, the Gauss-Seidel method is more rapidly convergent than the
Jacobi method. Relation (4.18) holds even if A enjoys the following A-
property.
Deﬁnition 4.3 A consistently ordered matrix M ∈Rn×n (that is, a matrix
such that αD−1E+α−1D−1F, for α ̸= 0, has eigenvalues that do not depend
on α, where M=D-E-F, D = diag(m11, . . . , mnn), E and F are strictly lower
and upper triangular matrices, respectively) enjoys the A-property if it can
be partitioned in the 2 × 2 block form
M =
 ˜D1
M12
M21
˜D2

,
where ˜D1 and ˜D2 are diagonal matrices.
■
When dealing with general matrices, no a priori conclusions on the conver-
gence properties of the Jacobi and Gauss-Seidel methods can be drawn, as
shown in Example 4.2.
Example 4.2 Consider the 3 × 3 linear systems of the form Aix = bi, where bi
is always taken in such a way that the solution of the system is the unit vector,
and the matrices Ai are
A1 =


3
0
4
7
4
2
−1
1
2

,
A2 =


−3
3
−6
−4
7
−8
5
7
−9

,
A3 =


4
1
1
2
−9
0
0
−8
−6

,
A4 =


7
6
9
4
5
−4
−7
−3
8

.
It can be checked that the Jacobi method does fail to converge for A1 (ρ(BJ) =
1.33), while the Gauss-Seidel scheme is convergent. Conversely, in the case of
A2, the Jacobi method is convergent, while the Gauss-Seidel method fails to
converge (ρ(BGS) = 1.¯1). In the remaining two cases, the Jacobi method is more
slowly convergent than the Gauss-Seidel method for matrix A3 (ρ(BJ) = 0.44
against ρ(BGS) = 0.018), and the converse is true for A4 (ρ(BJ) = 0.64 while
ρ(BGS) = 0.77).
•
We conclude the section with the following result.
Theorem 4.6 If the Jacobi method is convergent, then the JOR method
converges if 0 < ω ≤1.
Proof. From (4.12) we obtain that the eigenvalues of BJω are
µk = ωλk + 1 −ω,
k = 1, . . . , n,

4.2 Linear Iterative Methods
131
where λk are the eigenvalues of BJ. Then, recalling the Euler formula for the
representation of a complex number, we let λk = rkeiθk and get
|µk|2 = ω2r2
k + 2ωrk cos(θk)(1 −ω) + (1 −ω)2 ≤(ωrk + 1 −ω)2,
which is less than 1 if 0 < ω ≤1.
3
4.2.3
Convergence Results for the Relaxation Method
The following result provides a necessary condition on ω in order the SOR
method to be convergent.
Theorem 4.7 For any ω ∈R we have ρ(B(ω)) ≥|ω −1|; therefore, the
SOR method fails to converge if ω ≤0 or ω ≥2.
Proof. If {λi} denote the eigenvalues of the SOR iteration matrix, then

n

i=1
λi
 =
det
0
(1 −ω)I + ωD−1F
1 = |1 −ω|n.
Therefore, at least one eigenvalue λi must exist such that |λi| ≥|1−ω| and thus,
in order for convergence to hold, we must have |1 −ω| < 1, that is 0 < ω < 2. 3
Assuming that A is symmetric and positive deﬁnite, the condition 0 < ω <
2, besides being necessary, becomes also suﬃcient for convergence. Indeed
the following result holds (for the proof, see [Hac94]).
Property 4.3 (Ostrowski) If A is symmetric and positive deﬁnite, then
the SOR method is convergent iﬀ0 < ω < 2. Moreover, its convergence is
monotone with respect to ∥· ∥A.
Finally, if A is strictly diagonally dominant by rows, the SOR method
converges if 0 < ω ≤1.
The results above show that the SOR method is more or less rapidly
convergent, depending on the choice of the relaxation parameter ω. The
question of how to determine the value ωopt for which the convergence rate
is the highest possible can be given a satisfactory answer only in special
cases (see, for instance, [Axe94], [You71], [Var62] or [Wac66]). Here we limit
ourselves to quoting the following result (whose proof is in [Axe94]).
Property 4.4 If the matrix A enjoys the A-property and if BJ has real
eigenvalues, then the SOR method converges for any choice of x(0) iﬀ
ρ(BJ) < 1 and 0 < ω < 2. Moreover,
ωopt =
2
1 +

1 −ρ(BJ)2
(4.19)

132
4. Iterative Methods for Solving Linear Systems
and the corresponding asymptotic convergence factor is
ρ(B(ωopt)) = 1 −

1 −ρ(BJ)2
1 +

1 −ρ(BJ)2 .
4.2.4
A priori Forward Analysis
In the previous analysis we have neglected the rounding errors. However, as
shown in the following example (taken from [HW76]), they can dramatically
aﬀect the convergence rate of the iterative method.
Example 4.3 Let A be a lower bidiagonal matrix of order 100 with entries
aii = 1.5 and ai,i−1 = 1, and let b ∈R100 be the right-side with bi = 2.5. The
exact solution of the system Ax = b has components xi = 1 −(−2/3)i. The
SOR method with ω = 1.5 should be convergent, working in exact arithmetic,
since ρ(B(1.5)) = 0.5 (far below one). However, running Program 16 with x(0) =
ﬂ(x)+ϵM, which is extremely close to the exact value, the sequence x(k) diverges
and after 100 iterations the algorithm yields a solution with ∥x(100)∥∞= 1013.
The ﬂaw is due to rounding error propagation and must not be ascribed to a
possible ill-conditioning of the matrix since K∞(A) ≃5.
•
To account for rounding errors, let us denote by x(k) the solution (in ﬁnite
arithmetic) generated by an iterative method of the form (4.6) after k steps.
Due to rounding errors, x(k) can be regarded as the exact solution to the
problem
Px(k+1) = Nx(k) + b −ζk,
(4.20)
with
ζk = δPk+1x(k+1) −gk.
The matrix δPk+1 accounts for the rounding errors in the solution of (4.6),
while the vector gk includes the errors made in the evaluation of Nx(k) +b.
From (4.20), we obtain
x(k+1) = Bk+1x(0) +
k

j=0
BjP−1(b −ζk−j)
and for the absolute error e(k+1) = x −x(k+1)
e(k+1) = Bk+1e(0) +
k

j=0
BjP−1ζk−j.
The ﬁrst term represents the error that is made by the iterative method
in exact arithmetic; if the method is convergent, this error is negligible for
suﬃciently large values of k. The second term refers instead to rounding
error propagation; its analysis is quite technical and is carried out, for
instance, in [Hig88] in the case of Jacobi, Gauss-Seidel and SOR methods.

4.2 Linear Iterative Methods
133
4.2.5
Block Matrices
The methods of the previous sections are also referred to as point (or line)
iterative methods, since they act on single entries of matrix A. It is possible
to devise block versions of the algorithms, provided that D denotes the block
diagonal matrix whose entries are the m × m diagonal blocks of matrix A
(see Section 1.6).
The block Jacobi method is obtained taking again P=D and N=D-A. The
method is well-deﬁned only if the diagonal blocks of D are nonsingular. If
A is decomposed in p × p square blocks, the block Jacobi method is
Aiix(k+1)
i
= bi −
p

j=1
j̸=i
Aijx(k)
j ,
i = 1, . . . , p,
having also decomposed the solution vector and the right side in blocks of
size p, denoted by xi and bi, respectively. As a result, at each step, the block
Jacobi method requires solving p linear systems of matrices Aii. Theorem
4.3 is still valid, provided that D is substituted by the corresponding block
diagonal matrix.
In a similar manner, the block Gauss-Seidel and block SOR methods can
be introduced.
4.2.6
Symmetric Form of the Gauss-Seidel and SOR Methods
Even if A is a symmetric matrix, the Gauss-Seidel and SOR methods gen-
erate iteration matrices that are not necessarily symmetric. For that, we
introduce in this section a technique that allows for symmetrizing these
schemes. The ﬁnal aim is to provide an approach for generating symmetric
preconditioners (see Section 4.3.2).
Firstly, let us remark that an analogue of the Gauss-Seidel method can
be constructed, by simply exchanging E with F. The following iteration
can thus be deﬁned, called the backward Gauss-Seidel method
(D −F)x(k+1) = Ex(k) + b
with iteration matrix given by BGSb = (D −F)−1E.
The symmetric Gauss-Seidel method is obtained by combining an itera-
tion of Gauss-Seidel method with an iteration of backward Gauss-Seidel
method. Precisely, the k-th iteration of the symmetric Gauss-Seidel method
is
(D −E)x(k+1/2) = Fx(k) + b,
(D −F)x(k+1) = Ex(k+1/2) + b.

134
4. Iterative Methods for Solving Linear Systems
Eliminating x(k+1/2), the following scheme is obtained
x(k+1) = BSGSx(k) + bSGS,
BSGS = (D −F)−1E(D −E)−1F,
bSGS = (D −F)−1[E(D −E)−1 + I]b.
(4.21)
The preconditioning matrix associated with (4.21) is
PSGS = (D −E)D−1(D −F).
The following result can be proved (see [Hac94]).
Property 4.5 If A is a symmetric positive deﬁnite matrix, the symmet-
ric Gauss-Seidel method is convergent, and, moreover, BSGS is symmetric
positive deﬁnite.
In a similar manner, deﬁning the backward SOR method
(D −ωF)x(k+1) = [ωE + (1 −ω)D] x(k) + ωb,
and combining it with a step of SOR method, the following symmetric SOR
method or SSOR, is obtained
x(k+1) = Bs(ω)x(k) + bω
where
Bs(ω) = (D −ωF)−1(ωE + (1 −ω)D)(D −ωE)−1(ωF + (1 −ω)D),
bω = ω(2 −ω)(D −ωF)−1D(D −ωE)−1b.
The preconditioning matrix of this scheme is
PSSOR(ω) =
 1
ω D −E

ω
2 −ω D−1
 1
ω D −F

.
(4.22)
If A is symmetric and positive deﬁnite, the SSOR method is convergent if
0 < ω < 2 (see [Hac94] for the proof). Typically, the SSOR method with an
optimal choice of the relaxation parameter converges more slowly than the
corresponding SOR method. However, the value of ρ(Bs(ω)) is less sensitive
to a choice of ω around the optimal value (in this respect, see the behavior
of the spectral radii of the two iteration matrices in Figure 4.1). For this
reason, the optimal value of ω that is chosen in the case of SSOR method
is usually the same used for the SOR method (for further details, we refer
to [You71]).

4.2 Linear Iterative Methods
135
0
0.5
1
1.5
2
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
ω
ρ
SSOR
SOR
FIGURE 4.1. Spectral radius of the iteration matrix of SOR and SSOR methods,
as a function of the relaxation parameter ω for the matrix tridiag10(−1, 2, −1)
4.2.7
Implementation Issues
We provide the programs implementing the Jacobi and Gauss-Seidel meth-
ods in their point form and with relaxation.
In Program 15 the JOR method is implemented (the Jacobi method is
obtained as a special case setting omega = 1). The stopping test monitors
the Euclidean norm of the residual at each iteration, normalized to the
value of the initial residual.
Notice that each component x(i) of the solution vector can be computed
independently; this method can thus be easily parallelized.
Program 15 - JOR : JOR method
function [x, iter]= jor ( a, b, x0, nmax, toll, omega)
[n,n]=size(a);
iter = 0; r = b - a * x0; r0 = norm(r); err = norm (r); x = x0;
while err > toll & iter < nmax
iter = iter + 1;
for i=1:n
s = 0;
for j = 1:i-1,
s = s + a (i,j) * x (j);
end
for j = i+1:n,
s = s + a (i,j) * x (j);
end
x (i) = omega * ( b(i) - s) / a(i,i) + (1 - omega) * x(i);
end
r = b - a * x;
err = norm (r) / r0;
end
Program 16 implements the SOR method. Taking omega=1 yields the
Gauss-Seidel method.

136
4. Iterative Methods for Solving Linear Systems
Unlike the Jacobi method, this scheme is fully sequential. However, it can
be eﬃciently implemented without storing the solution of the previous step,
with a saving of memory storage.
Program 16 - SOR : SOR method
function [x, iter]= sor ( a, b, x0, nmax, toll, omega)
[n,n]=size(a);
iter = 0; r = b - a * x0; r0 = norm (r); err = norm (r); xold = x0;
while err > toll & iter < nmax
iter = iter + 1;
for i=1:n
s = 0;
for j = 1:i-1,
s = s + a (i,j) * x (j);
end
for j = i+1:n
s = s + a (i,j) * xold (j);
end
x (i) = omega * ( b(i) - s) / a(i,i) + (1 - omega) * xold (i);
end
x = x’; xold = x;
r = b - a * x;
err = norm (r) / r0;
end
4.3
Stationary and Nonstationary Iterative
Methods
Denote by
RP = I −P−1A
the iteration matrix associated with (4.7). Proceeding as in the case of
relaxation methods, (4.7) can be generalized introducing a relaxation (or
acceleration) parameter α. This leads to the following stationary Richard-
son method
x(k+1) = x(k) + αP−1r(k),
k ≥0.
(4.23)
More generally, allowing α to depend on the iteration index, the nonsta-
tionary Richardson method or semi-iterative method given by
x(k+1) = x(k) + αkP−1r(k),
k ≥0.
(4.24)
The iteration matrix at the k-th step for these methods (depending on k)
is
R(αk) = I −αkP−1A,

4.3 Stationary and Nonstationary Iterative Methods
137
with αk = α in the stationary case. If P=I, the methods will be called
nonpreconditioned. The Jacobi and Gauss-Seidel methods can be regarded
as stationary Richardson methods with α = 1, P = D and P = D −E,
respectively.
We can rewrite (4.24) (and, thus, also (4.23)) in a form of greater inter-
est for computation. Letting z(k) = P−1r(k) (the so-called preconditioned
residual), we get x(k+1) = x(k) + αkz(k) and r(k+1) = b −Ax(k+1) =
r(k)−αkAz(k). To summarize, a nonstationary Richardson method requires
at each k + 1-th step the following operations:
solve the linear system Pz(k) = r(k);
compute the acceleration parameter αk;
update the solution x(k+1) = x(k) + αkz(k);
update the residual r(k+1) = r(k) −αkAz(k).
(4.25)
4.3.1
Convergence Analysis of the Richardson Method
Let us ﬁrst consider the stationary Richardson methods for which αk = α
for k ≥0. The following convergence result holds.
Theorem 4.8 For any nonsingular matrix P, the stationary Richardson
method (4.23) is convergent iﬀ
2Reλi
α|λi|2 > 1
∀i = 1, . . . , n,
(4.26)
where λi ∈C are the eigenvalues of P−1A.
Proof. Let us apply Theorem 4.1 to the iteration matrix Rα = I −αP−1A. The
condition |1 −αλi| < 1 for i = 1, . . . , n yields the inequality
(1 −αReλi)2 + α2(Imλi)2 < 1
from which (4.26) immediately follows.
3
Let us notice that, if the sign of the real parts of the eigenvalues of P−1A
is not constant, the stationary Richardson method cannot converge.
More speciﬁc results can be obtained provided that suitable assumptions
are made on the spectrum of P−1A.
Theorem 4.9 Assume that P is a nonsingular matrix and that P−1A has
positive real eigenvalues, ordered in such a way that λ1 ≥λ2 ≥. . . ≥
λn > 0. Then, the stationary Richardson method (4.23) is convergent iﬀ
0 < α < 2/λ1. Moreover, letting
αopt =
2
λ1 + λn
(4.27)

138
4. Iterative Methods for Solving Linear Systems
the spectral radius of the iteration matrix Rα is minimum if α = αopt, with
ρopt = min
α [ρ(Rα)] = λ1 −λn
λ1 + λn
.
(4.28)
Proof. The eigenvalues of Rα are given by λi(Rα) = 1 −αλi, so that (4.23) is
convergent iﬀ|λi(Rα)| < 1 for i = 1, . . . , n, that is, if 0 < α < 2/λ1. It follows
(see Figure 4.2) that ρ(Rα) is minimum when 1 −αλn = αλ1 −1, that is, for
α = 2/(λ1 + λn), which furnishes the desired value for αopt. By substitution, the
desired value of ρopt is obtained.
3
1
λn
1
λ1
αopt
2
λ1
ρ = 1
|1 −αλ1|
|1 −αλn|
ρopt
|1 −αλk|
α
FIGURE 4.2. Spectral radius of Rα as a function of the eigenvalues of P−1A
If P−1A is symmetric positive deﬁnite, it can be shown that the convergence
of the Richardson method is monotone with respect to either ∥·∥2 and ∥·∥A.
In such a case, using (4.28), we can also relate ρopt to K2(P−1A) as follows
ρopt = K2(P−1A) −1
K2(P−1A) + 1,
αopt =
2∥A−1P∥2
K2(P−1A) + 1.
(4.29)
The choice of a suitable preconditioner P is, therefore, of paramount im-
portance for improving the convergence of a Richardson method. Of course,
such a choice should also account for the need of keeping the computational
eﬀort as low as possible. In Section 4.3.2, some preconditioners of common
use in practice will be described.
Corollary 4.1 Let A be a symmetric positive deﬁnite matrix. Then, the
non preconditioned stationary Richardson method is convergent and
∥e(k+1)∥A ≤ρ(Rα)∥e(k)∥A,
k ≥0.
(4.30)

4.3 Stationary and Nonstationary Iterative Methods
139
The same result holds for the preconditioned Richardson method, provided
that the matrices P, A and P−1A are symmetric positive deﬁnite.
Proof. The convergence is a consequence of Theorem 4.8. Moreover, we notice
that
∥e(k+1)∥A = ∥Rαe(k)∥A = ∥A1/2Rαe(k)∥2 ≤∥A1/2RαA−1/2∥2∥A1/2e(k)∥2.
The matrix Rα is symmetric positive deﬁnite and is similar to A1/2RαA−1/2.
Therefore,
∥A1/2RαA−1/2∥2 = ρ(Rα).
The result (4.30) follows by noting that ∥A1/2e(k)∥2 = ∥e(k)∥A. A similar proof
can be carried out in the preconditioned case, provided we replace A with P−1A.
3
Finally, the inequality (4.30) holds even if only P and A are symmetric
positive deﬁnite (for the proof, see [QV94], Chapter 2).
4.3.2
Preconditioning Matrices
All the methods introduced in the previous sections can be cast in the form
(4.2), so that they can be regarded as being methods for solving the system
(I −B)x = f = P−1b.
On the other hand, since B=P−1N, system (3.2) can be equivalently refor-
mulated as
P−1Ax = P−1b.
(4.31)
The latter is the preconditioned system, being P the preconditioning matrix
or left preconditioner. Right and centered preconditioners can be introduced
as well, if system (3.2) is transformed, respectively, as
AP−1y = b,
y = Px,
or
P−1
L AP−1
R y = P−1
L b,
y = PRx.
There are point preconditioners or block preconditioners, depending on
whether they are applied to the single entries of A or to the blocks of
a partition of A. The iterative methods considered so far correspond to
ﬁxed-point iterations on a left-preconditioned system. As stressed by (4.25),
computing the inverse of P is not mandatory; actually, the role of P is to
“preconditioning” the residual r(k) through the solution of the additional
system Pz(k) = r(k).

140
4. Iterative Methods for Solving Linear Systems
Since the preconditioner acts on the spectral radius of the iteration ma-
trix, it would be useful to pick up, for a given linear system, an optimal
preconditioner, i.e., a preconditioner which is able to make the number of
iterations required for convergence independent of the size of the system.
Notice that the choice P=A is optimal but, trivially, “ineﬃcient”; some
alternatives of greater computational interest will be examined below.
There is a lack of general theoretical results that allow to devise optimal
preconditioners. However, an established “rule of thumb” is that P is a
good preconditioner for A if P−1A is near to being a normal matrix and if
its eigenvalues are clustered within a suﬃciently small region of the com-
plex ﬁeld. The choice of a preconditioner must also be guided by practical
considerations, noticeably, its computational cost and its memory require-
ments.
Preconditioners can be divided into two main categories: algebraic and
functional preconditioners, the diﬀerence being that the algebraic precon-
ditioners are independent of the problem that originated the system to
be solved, and are actually constructed via algebraic procedure, while the
functional preconditioners take advantage of the knowledge of the problem
and are constructed as a function of it. In addition to the preconditioners
already introduced in Section 4.2.6, we give a description of other algebraic
preconditioners of common use.
1. Diagonal preconditioners: choosing P as the diagonal of A is generally
eﬀective if A is symmetric positive deﬁnite. A usual choice in the non
symmetric case is to set
pii =


n

j=1
a2
ij


1/2
.
Block diagonal preconditioners can be constructed in a similar man-
ner. We remark that devising an optimal diagonal preconditioner is
far from being trivial, as previously noticed in Section 3.12.1 when
dealing with the scaling of a matrix.
2. Incomplete LU factorization (shortly ILU) and Incomplete Cholesky
factorization (shortly IC).
An incomplete factorization of A is a process that computes P =
LinUin, where Lin is a lower triangular matrix and Uin is an upper
triangular matrix. These matrices are approximations of the exact
matrices L, U of the LU factorization of A and are chosen in such a
way that the residual matrix R = A−LinUin satisﬁes some prescribed
requirements, such as having zero entries in speciﬁed locations.
For a given matrix M, the L-part (U-part) of M will mean henceforth
the lower (upper) triangular part of M. Moreover, we assume that the
factorization process can be carried out without resorting to pivoting.

4.3 Stationary and Nonstationary Iterative Methods
141
The basic approach to incomplete factorization, consists of requiring
the approximate factors Lin and Uin to have the same sparsity pat-
tern as the L-part and U-part of A, respectively. A general algorithm
for constructing an incomplete factorization is to perform Gauss elim-
ination as follows: at each step k, compute mik = a(k)
ik /a(k)
kk only if
aik ̸= 0 for i = k + 1, . . . , n. Then, compute for j = k + 1, . . . , n
a(k+1)
ij
only if aij ̸= 0. This algorithm is implemented in Program 17
where the matrices Lin and Uin are progressively overwritten onto
the L-part and U-part of A.
Program 17 - basicILU : Incomplete LU factorization
function [a] = basicILU(a)
[n,n]=size(a);
for k=1:n-1, for i=k+1:n,
if a(i,k) ˜= 0
a(i,k) = a(i,k) / a(k,k);
for j=k+1:n
if a(i,j) ˜= 0
a(i,j) = a(i,j) -a(i,k)*a(k,j);
end
end
end
end, end
We notice that having Lin and Uin with the same patterns as the
L and U-parts of A, respectively, does not necessarily imply that R
has the same sparsity pattern as A, but guarantees that rij = 0 if
aij ̸= 0, as is shown in Figure 4.3.
The resulting incomplete factorization is known as ILU(0), where “0”
means that no ﬁll-in has been introduced in the factorization process.
An alternative strategy might be to ﬁx the structure of Lin and Uin
irrespectively of that of A, in such a way that some computational
criteria are satisﬁed (for example, that the incomplete factors have
the simplest possible structure).
The accuracy of the ILU(0) factorization can obviously be improved
by allowing some ﬁll-in to arise, and thus, by accepting nonzero entries
in the factorization whereas A has elements equal to zero. To this
purpose, it is convenient to introduce a function, which we call ﬁll-
in level, that is associated with each entry of A and that is being
modiﬁed during the factorization process. If the ﬁll-in level of an

142
4. Iterative Methods for Solving Linear Systems
0
1
2
3
4
5
6
7
8
9
10
11
0
1
2
3
4
5
6
7
8
9
10
11
FIGURE 4.3. The sparsity pattern of the original matrix A is represented by the
squares, while the pattern of R = A−LinUin, computed by Program 17, is drawn
by the bullets
element is greater than an admissible value p ∈N, the corresponding
entry in Uin or Lin is set equal to zero.
Let us explain how this procedure works, assuming that the matri-
ces Lin and Uin are progressively overwritten to A (as happens in
Program 4). The ﬁll-in level of an entry a(k)
ij
is denoted by levij,
where the dependence on k is understood, and it should provide a
reasonable estimate of the size of the entry during the factorization
process. Actually, we are assuming that if levij = q then |aij| ≃δq
with δ ∈(0, 1), so that q is greater when |a(k)
ij | is smaller.
At the starting step of the procedure, the level of the nonzero entries
of A and of the diagonal entries is set equal to 0, while the level of
the null entries is set equal to inﬁnity. For any row i = 2, . . . , n, the
following operations are performed: if levik ≤p, k = 1, . . . , i −1, the
entry mik of Lin and the entries a(k+1)
ij
of Uin, j = i + 1, . . . , n, are
updated. Moreover, if a(k+1)
ij
̸= 0 the value levij is updated as being
the minimum between the available value of levij and levik+levkj +1.
The reason of this choice is that |a(k+1)
ij
| = |a(k)
ij −mika(k)
kj | ≃|δlevij −
δlevik+levkj+1|, so that one can assume that the size of |a(k+1)
ij
| is the
maximum between δlevij and δlevik+levkj+1.
The above factorization process is called ILU(p) and turns out to be
extremely eﬃcient (with p small) provided that it is coupled with a
suitable matrix reordering (see Section 3.9).
Program 18 implements the ILU(p) factorization; it returns in out-
put the approximate matrices Lin and Uin (overwritten to the input
matrix a), with the diagonal entries of Lin equal to 1, and the ma-

4.3 Stationary and Nonstationary Iterative Methods
143
trix lev containing the ﬁll-in level of each entry at the end of the
factorization.
Program 18 - ilup : ILU(p) factorization
function [a,lev] = ilup (a,p)
[n,n]=size(a);
for i=1:n, for j=1:n
if (a(i,j) ˜= 0) | (i==j)
lev(i,j)=0;
else
lev(i,j)=Inf;
end
end, end
for i=2:n,
for k=1:i-1
if lev(i,k) <= p
a(i,k)=a(i,k)/a(k,k);
for j=k+1:n
a(i,j)=a(i,j)-a(i,k)*a(k,j);
if a(i,j) ˜= 0
lev(i,j)=min(lev(i,j),lev(i,k)+lev(k,j)+1);
end
end
end
end
for j=1:n, if lev(i,j) > p, a(i,j) = 0; end, end
end
Example 4.4 Consider the matrix A ∈R46×46 associated with the ﬁnite
diﬀerence approximation of the Laplace operator ∆· =
∂2·
∂x2 +
∂2·
∂y2 (see
Section 12.6). This matrix can be generated with the following MATLAB
commands: G=numgrid(’B’,10); A=delsq(G) and corresponds to the dis-
cretization of the diﬀerential operator on a domain having the shape of the
exterior of a butterﬂy and included in the square [−1, 1]2 (see Section 12.6).
The number of nonzero entries of A is 174. Figure 4.4 shows the pattern of
matrix A (drawn by the bullets) and the entries in the pattern added by
the ILU(1) and ILU(2) factorizations due to ﬁll-in (denoted by the squares
and the triangles, respectively). Notice that these entries are all contained
within the envelope of A since no pivoting has been performed.
•
The ILU(p) process can be carried out without knowing the actual
values of the entries of A, but only working on their ﬁll-in levels.
Therefore, we can distinguish between a symbolic factorization (the
generation of the levels) and an actual factorization (the computation
of the entries of ILU(p) starting from the informations contained in

144
4. Iterative Methods for Solving Linear Systems
0
5
10
15
20
25
30
35
40
45
0
5
10
15
20
25
30
35
40
45
FIGURE 4.4. Pattern of the matrix A in Example 4.4 (bullets); entries added by
the ILU(1) and ILU(2) factorizations (squares and triangles, respectively)
the level function). The scheme is thus particularly eﬀective when
several linear systems must be solved, with matrices having the same
structure but diﬀerent entries.
On the other hand, for certain classes of matrices, the ﬁll-in level
does not always provide a sound indication of the actual size attained
by the entries. In such cases, it is better to monitor the size of the
entries of R by neglecting each time the entries that are too small.
For instance, one can drop out the entries a(k+1)
ij
such that
|a(k+1)
ij
| ≤c|a(k+1)
ii
a(k+1)
jj
|1/2,
i, j = 1, . . . , n,
with 0 < c < 1 (see [Axe94]).
In the strategies considered so far, the entries of the matrix that are
dropped out can no longer be recovered in the incomplete factoriza-
tion process. Some remedies exist for this drawback: for instance, at
the end of each k-th step of the factorization, one can sum, row by
row, the discarded entries to the diagonal entries of Uin. By doing
so, an incomplete factorization known as MILU (Modiﬁed ILU) is
obtained, which enjoys the property of being exact with respect to
the constant vectors, i.e., such that R1T = 0T (see [Axe94] for other
formulations). In the practice, this simple trick provides, for a wide
class of matrices, a better preconditioner than obtained with the ILU
method. In the case of symmetric positive deﬁnite matrices one can
resort to the Modiﬁed Incomplete Cholesky Factorization (MICh).
We conclude by mentioning the ILUT factorization, which collects the
features of ILU(p) and MILU. This factorization can also include par-
tial pivoting by columns with a slight increase of the computational

4.3 Stationary and Nonstationary Iterative Methods
145
cost. For an eﬃcient implementation of incomplete factorizations, we
refer to the MATLAB function luinc in the toolbox sparfun.
The existence of the ILU factorization is not guaranteed for all non-
singular matrices (see for an example [Elm86]) and the process stops
if zero pivotal entries arise. Existence theorems can be proved if A is
an M-matrix [MdV77] or diagonally dominant [Man80]. It is worth
noting that sometimes the ILU factorization turns out to be more
stable than the complete LU factorization [GM83].
3. Polynomial preconditioners: the preconditioning matrix is deﬁned as
P−1 = p(A),
where p is a polynomial in A, usually of low degree.
A remarkable example is given by Neumann polynomial precondi-
tioners. Letting A = D −C, we have A = (I −CD−1)D, from which
A−1 = D−1(I −CD−1)−1 = D−1(I + CD−1 + (CD−1)2 + . . . ).
A preconditioner can then be obtained by truncating the series above
at a certain power p. This method is actually eﬀective only if ρ(CD−1)
< 1, which is the necessary condition in order the series to be con-
vergent.
4. Least-squares preconditioners: A−1 is approximated by a least-squares
polynomial ps(A) (see Section 3.13). Since the aim is to make ma-
trix I −P−1A as close as possible to the null matrix, the least-
squares approximant ps(A) is chosen in such a way that the function
ϕ(x) = 1−ps(x)x is minimized. This preconditioning technique works
eﬀectively only if A is symmetric and positive deﬁnite.
For further results on preconditioners, see [dV89] and [Axe94].
Example 4.5 Consider the matrix A∈R324×324 associated with the ﬁnite diﬀer-
ence approximation of the Laplace operator on the square [−1, 1]2. This matrix
can be generated with the following MATLAB commands: G=numgrid(’N’,20);
A=delsq(G). The condition number of the matrix is K2(A) = 211.3. In Table
4.1 we show the values of K2(P−1A) computed using the ILU(p) and Neumann
preconditioners, with p = 0, 1, 2, 3. In the last case D is the diagonal part of A. •
Remark 4.2 Let A and P be real symmetric matrices of order n, with P
positive deﬁnite. The eigenvalues of the preconditioned matrix P−1A are
solutions of the algebraic equation
Ax = λPx,
(4.32)

146
4. Iterative Methods for Solving Linear Systems
p
ILU(p)
Neumann
0
22.3
211.3
1
12
36.91
2
8.6
48.55
3
5.6
18.7
TABLE 4.1. Spectral condition numbers of the preconditioned matrix A of Ex-
ample 4.5 as a function of p
where x is an eigenvector associated with the eigenvalue λ. Equation (4.32)
is an example of generalized eigenvalue problem (see Section 5.9 for a thor-
ough discussion) and the eigenvalue λ can be computed through the fol-
lowing generalized Rayleigh quotient
λ = (Ax, x)
(Px, x) .
Applying the Courant-Fisher Theorem (see Section 5.11) yields
λmin(A)
λmax(P) ≤λ ≤λmax(A)
λmin(P) .
(4.33)
Relation (4.33) provides a lower and upper bound for the eigenvalues of the
preconditioned matrix as a function of the extremal eigenvalues of A and
P, and therefore it can be proﬁtably used to estimate the condition number
of P−1A.
■
4.3.3
The Gradient Method
The expression of the optimal parameter that has been provided in Theo-
rem 4.9 is of limited usefulness in practical computations, since it requires
the knowledge of the extremal eigenvalues of the matrix P−1A. In the spe-
cial case of symmetric and positive deﬁnite matrices, however, the optimal
acceleration parameter can be dynamically computed at each step k as
follows.
We ﬁrst notice that, for such matrices, solving system (3.2) is equivalent
to ﬁnding the minimizer x ∈Rn of the quadratic form
Φ(y) = 1
2yT Ay −yT b,
which is called the energy of system (3.2). Indeed, the gradient of Φ is given
by
∇Φ(y) = 1
2(AT + A)y −b = Ay −b.
(4.34)
As a consequence, if ∇Φ(x) = 0 then x is a solution of the original system.
Conversely, if x is a solution, then
Φ(y) = Φ(x + (y −x)) = Φ(x) + 1
2(y −x)T A(y −x),
∀y ∈Rn

4.3 Stationary and Nonstationary Iterative Methods
147
and thus, Φ(y) > Φ(x) if y ̸= x, i.e. x is a minimizer of the functional Φ.
Notice that the previous relation is equivalent to
1
2∥y −x∥2
A = Φ(y) −Φ(x)
(4.35)
where ∥· ∥A is the A-norm or energy norm, deﬁned in (1.28).
The problem is thus to determine the minimizer x of Φ starting from a
point x(0) ∈Rn and, consequently, to select suitable directions along which
moving to get as close as possible to the solution x. The optimal direction,
that joins the starting point x(0) to the solution point x, is obviously un-
known a priori. Therefore, we must take a step from x(0) along another
direction d(0), and then ﬁx along this latter a new point x(1) from which
to iterate the process until convergence.
Thus, at the generic step k, x(k+1) is computed as
x(k+1) = x(k) + αkd(k),
(4.36)
where αk is the value which ﬁxes the length of the step along d(k). The most
natural idea is to take the descent direction of maximum slope ∇Φ(x(k)),
which yields the gradient method or steepest descent method.
On the other hand, due to (4.34), ∇Φ(x(k)) = Ax(k) −b = −r(k), so that
the direction of the gradient of Φ coincides with that of residual and can
be immediately computed using the current iterate. This shows that the
gradient method, as well as the Richardson method, moves at each step k
along the direction d(k) = r(k).
To compute the parameter αk let us write explicitly Φ(x(k+1)) as a func-
tion of a parameter α
Φ(x(k+1)) = 1
2(x(k) + αr(k))T A(x(k) + αr(k)) −(x(k) + αr(k))T b.
Diﬀerentiating with respect to α and setting it equal to zero, yields the
desired value of αk
αk = r(k)T r(k)
r(k)T Ar(k)
(4.37)
which depends only on the residual at the k-th step. For this reason, the
nonstationary Richardson method employing (4.37) to evaluate the acceler-
ation parameter, is also called the gradient method with dynamic parameter
(shortly, gradient method), to distinguish it from the stationary Richardson
method (4.23) or gradient method with constant parameter, where αk = α
is a constant for any k ≥0.
Summarizing, the gradient method can be described as follows:

148
4. Iterative Methods for Solving Linear Systems
given x(0) ∈Rn, for k = 0, 1, . . . until convergence, compute
r(k) = b −Ax(k)
αk = r(k)T r(k)
r(k)T Ar(k)
x(k+1) = x(k) + αkr(k).
Theorem 4.10 Let A be a symmetric and positive deﬁnite matrix; then
the gradient method is convergent for any choice of the initial datum x(0)
and
∥e(k+1)∥A ≤K2(A) −1
K2(A) + 1∥e(k)∥A,
k = 0, 1, . . . ,
(4.38)
where ∥· ∥A is the energy norm deﬁned in (1.28).
Proof. Let x(k) be the solution generated by the gradient method at the k-th
step. Then, let x(k+1)
R
be the vector generated by taking one step of the non
preconditioned Richardson method with optimal parameter starting from x(k),
i.e., x(k+1)
R
= x(k) + αoptr(k).
Due to Corollary 4.1 and (4.28), we have
∥e(k+1)
R
∥A ≤K2(A) −1
K2(A) + 1∥e(k)∥A,
where e(k+1)
R
= x(k+1)
R
−x. Moreover, from (4.35) we have that the vector x(k+1),
generated by the gradient method, is the one that minimizes the A-norm of
the error among all vectors of the form x(k) + θr(k), with θ ∈R. Therefore,
∥e(k+1)∥A ≤∥e(k+1)
R
∥A which is the desired result.
3
We notice that the line through x(k) and x(k+1) is tangent at the point
x(k+1) to the ellipsoidal level surface

x ∈Rn : Φ(x) = Φ(x(k+1))

(see
also Figure 4.5).
Relation (4.38) shows that convergence of the gradient method can be
quite slow if K2(A) = λ1/λn is large. A simple geometric interpretation of
this result can be given in the case n = 2. Suppose that A=diag(λ1, λ2),
with 0 < λ2 ≤λ1 and b = (b1, b2)T .
In such a case, the curves corresponding to Φ(x1, x2) = c, as c varies
in R+, form a sequence of concentric ellipses whose semi-axes have length
inversely proportional to the values λ1 and λ2. If λ1 = λ2, the ellipses
degenerate into circles and the direction of the gradient crosses the center
directly, in such a way that the gradient method converges in one iteration.
Conversely, if λ1 ≫λ2, the ellipses become strongly eccentric and the
method converges quite slowly, as shown in Figure 4.5, moving along a
“zig-zag” trajectory.

4.3 Stationary and Nonstationary Iterative Methods
149
−2
0
2
−2
−1
0
1
2
x
(0)
x
(1)
−1
−0.5
0
0.5
1
−0.5
0
0.5
1
x
(2)
x
(3)
FIGURE 4.5. The ﬁrst iterates of the gradient method on the level curves of Φ
Program 19 provides an implementation of the gradient method with dy-
namic parameter. Here and in the programs reported in the remainder of
the section, the input parameters A, x, b, M, maxit and tol respectively
represent the coeﬃcient matrix of the linear system, the initial datum x(0),
the right side, a possible preconditioner, the maximum number of admis-
sible iterations and a tolerance for the stopping test. This stopping test
checks if the ratio ∥r(k)∥2/∥b∥2 is less than tol. The output parameters of
the code are the the number of iterations niter required to fulﬁll the stop-
ping test, the vector x with the solution computed after niter iterations
and the normalized residual error = ∥r(niter)∥2/∥b∥2. A null value of the
parameter flag warns the user that the algorithm has actually satisﬁed
the stopping test and it has not terminated due to reaching the maximum
admissible number of iterations.
Program 19 - gradient : Gradient method with dynamic parameter
function [x, error, niter, ﬂag] = gradient(A, x, b, M, maxit, tol)
ﬂag = 0;
niter = 0;
bnrm2 = norm( b );
if ( bnrm2 == 0.0 ), bnrm2 = 1.0; end
r = b - A*x; error = norm( r ) / bnrm2;
if ( error < tol ) return, end
for niter = 1:maxit
z = M \ r; rho = (r’*z);
q = A*z;
alpha = rho / (z’*q );
x = x + alpha * z;
r = r - alpha*q;
error = norm( r ) / bnrm2;
if ( error <= tol ), break, end
end
if ( error > tol ) ﬂag = 1; end

150
4. Iterative Methods for Solving Linear Systems
Example 4.6 Let us solve with the gradient method the linear system with ma-
trix Am ∈Rm×m generated with the MATLAB commands G=numgrid(’S’,n);
A=delsq(G) where m = (n −2)2. This matrix is associated with the discretiza-
tion of the diﬀerential Laplace operator on the domain [−1, 1]2. The right-hand
side bm is selected in such a way that the exact solution is the vector 1T ∈Rm.
The matrix Am is symmetric and positive deﬁnite for any m and becomes ill-
conditioned for large values of m. We run Program 19 in the cases m = 16 and
m = 400, with x(0) = 0T , tol=10−10 and maxit=200. If m = 400, the method
fails to satisfy the stopping test within the admissible maximum number of it-
erations and exhibits an extremely slow reduction of the residual (see Figure
4.6). Actually, K2(A400) ≃258. If, however, we precondition the system with the
matrix P = RT
inRin, where Rin is the lower triangular matrix in the Cholesky
incomplete factorization of A, the algorithm fulﬁlls the convergence within the
maximum admissible number of iterations (indeed, now K2(P−1A400) ≃38).
•
0
50
100
150
200
250
10
−14
10
−12
10
−10
10
−8
10
−6
10
−4
10
−2
10
0
(a)
(b)
(c)
(d)
FIGURE 4.6. The residual normalized to the starting one, as a function of the
number of iterations, for the gradient method applied to the systems in Example
4.6. The curves labelled (a) and (b) refer to the case m = 16 with the non precon-
ditioned and preconditioned method, respectively, while the curves labelled (c)
and (d) refer to the case m = 400 with the non preconditioned and preconditioned
method, respectively
4.3.4
The Conjugate Gradient Method
The gradient method consists essentially of two phases: choosing a descent
direction (the one of the residual) and picking up a point of local minimum
for Φ along that direction. The second phase is independent of the ﬁrst one
since, for a given direction p(k), we can determine αk as being the value
of the parameter α such that Φ(x(k) + αp(k)) is minimized. Diﬀerentiating
with respect to α and setting to zero the derivative at the minimizer, yields
αk =
p(k)T r(k)
p(k)T Ap(k) ,
(4.39)

4.3 Stationary and Nonstationary Iterative Methods
151
instead of (4.37). The question is how to determine p(k). A diﬀerent ap-
proach than the one which led to identify p(k) with r(k) is suggested by the
following deﬁnition.
Deﬁnition 4.4 A direction x(k) is said to be optimal with respect to a
direction p ̸= 0 if
Φ(x(k)) ≤Φ(x(k) + λp),
∀λ ∈R.
(4.40)
If x(k) is optimal with respect to any direction in a vector space V, we say
that x(k) is optimal with respect to V.
■
From the deﬁnition of optimality, it turns out that p must be orthogonal
to the residual r(k). Indeed, from (4.40) we conclude that Φ admits a local
minimum along p for λ = 0, and thus the partial derivative of Φ with
respect to λ must vanish at λ = 0. Since
∂Φ
∂λ (x(k) + λp) = pT (Ax(k) −b) + λpT Ap,
we therefore have
∂Φ
∂λ (x(k))|λ=0 = 0
iﬀ
pT (r(k)) = 0,
that is, p ⊥r(k). Notice that the iterate x(k+1) of the gradient method
is optimal with respect to r(k) since, due to the choice of αk, we have
r(k+1) ⊥r(k), but this property no longer holds for the successive iterate
x(k+2) (see Exercise 12). It is then natural to ask whether there exist descent
directions that maintain the optimality of iterates. Let
x(k+1) = x(k) + q,
and assume that x(k) is optimal with respect to a direction p (thus, r(k) ⊥
p). Let us impose that x(k+1) is still optimal with respect to p, that is,
r(k+1) ⊥p. We obtain
0 = pT r(k+1) = pT (r(k) −Aq) = −pT Aq.
The conclusion is that, in order to preserve optimality between succes-
sive iterates, the descent directions must be mutually A-orthogonal or A-
conjugate, i.e.
pT Aq = 0.
A method employing A-conjugate descent directions is called conjugate.
The next step is how to generate automatically a sequence of conjugate

152
4. Iterative Methods for Solving Linear Systems
directions. This can be done as follows. Let p(0) = r(0) and search for the
directions of the form
p(k+1) = r(k+1) −βkp(k),
k = 0, 1, . . .
(4.41)
where βk ∈R must be determined in such a way that
(Ap(j))T p(k+1) = 0,
j = 0, 1, . . . , k.
(4.42)
Requiring that (4.42) is satisﬁed for j = k, we get from (4.41)
βk = (Ap(k))T r(k+1)
(Ap(k))T p(k) ,
k = 0, 1, . . .
We must now verify that (4.42) holds also for j = 0, 1, . . . , k−1. To do this,
let us proceed by induction on k. Due to the choice of β0, relation (4.42)
holds for k = 0; let us thus assume that the directions p(0), . . . , p(k−1) are
mutually A-orthogonal and, without losing generality, that
(p(j))T r(k) = 0,
j = 0, 1, . . . , k −1,
k ≥1.
(4.43)
Then, from (4.41) it follows that
(Ap(j))T p(k+1) = (Ap(j))T r(k+1),
j = 0, 1, . . . , k −1.
Moreover, due to (4.43) and by the assumption of of A-orthogonality we
get
(p(j))T r(k+1) = (p(j))T r(k) −αk(p(j))T Ap(k) = 0,
j = 0, . . . , k −1(4.44)
i.e., we conclude that r(k+1) is orthogonal to every vector of the space Vk =
span(p(0), . . . , p(k−1)). Since p(0) = r(0), from (4.41) it follows that Vk is
also equal to span(r(0), . . . , r(k−1)). Then, (4.41) implies that Ap(j) ∈Vj+1
and thus, due to (4.44)
(Ap(j))T r(k+1) = 0,
j = 0, 1, . . . , k −1.
As a consequence, (4.42) holds for j = 0, . . . , k.
The conjugate gradient method (CG) is the method obtained by choosing
the descent directions p(k) given by (4.41) and the acceleration parameter
αk as in (4.39). As a consequence, setting r(0) = b −Ax(0) and p(0) = r(0),
the k-th iteration of the conjugate gradient method takes the following

4.3 Stationary and Nonstationary Iterative Methods
153
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
−0.2
0
0.2
0.4
0.6
0.8
1
1.2
1.4
CG
G
FIGURE 4.7. Descent directions for the conjugate gradient method (denoted by
CG, dashed line) and the gradient method (denoted by G, solid line). Notice that
the CG method reaches the solution after two iterations
form
αk =
p(k)T r(k)
p(k)T Ap(k)
x(k+1) = x(k) + αkp(k)
r(k+1) = r(k) −αkAp(k)
βk = (Ap(k))T r(k+1)
(Ap(k))T p(k)
p(k+1) = r(k+1) −βkp(k).
It can also be shown (see Exercise 13) that the two parameters αk and βk
may be alternatively expressed as
αk =
∥r(k)∥2
2
p(k)T Ap(k) ,
βk = ∥r(k+1)∥2
2
∥r(k)∥2
2
.
(4.45)
We ﬁnally notice that, eliminating the descent directions from r(k+1) =
r(k) −αkAp(k), the following recursive three-terms relation is obtained for
the residuals (see Exercise 14)
Ar(k) = −1
αk
r(k+1) +
 1
αk
−βk−1
αk−1

r(k) +
βk
αk−1
r(k−1).
(4.46)
As for the convergence of the CG method, we have the following results.
Theorem 4.11 Let A be a symmetric and positive deﬁnite matrix. Any
method which employs conjugate directions to solve (3.2) terminates after
at most n steps, yielding the exact solution.

154
4. Iterative Methods for Solving Linear Systems
Proof. The directions p(0), p(1), . . . , p(n−1) form an A-orthogonal basis in Rn.
Moreover, since x(k) is optimal with respect to all the directions p(j), j =
0, . . . , k−1, it follows that r(k) is orthogonal to the space Sk−1 = span(p(0), p(1),
. . . , p(k−1)). As a consequence, r(n) ⊥Sn−1 = Rn and thus r(n) = 0 which
implies x(n) = x.
3
Theorem 4.12 Let A be a symmetric and positive deﬁnite matrix and
let λ1, λn be its maximum and minimum eigenvalues, respectively. The
conjugate gradient method for solving (3.2) converges after at most n steps.
Moreover, the error e(k) at the k-th iteration (with k < n) is orthogonal to
p(j), for j = 0, . . . , k −1 and
∥e(k)∥A ≤
2ck
1 + c2k ∥e(0)∥A,
with c =

K2(A) −1

K2(A) + 1
.
(4.47)
Proof. The convergence of the CG method in n steps is a consequence of The-
orem 4.11.
Let us prove the error estimate, assuming for simplicity that x(0) = 0. Notice
ﬁrst that, for ﬁxed k
x(k+1) =
k

j=0
γjAjb,
for suitable γj ∈R. Moreover, by construction, x(k+1) is the vector which min-
imizes the A-norm of the error at step k + 1, among all vectors of the form
z = k
j=0 δjAjb = pk(A)b, where pk(ξ) = k
j=0 δjξj is a polynomial of degree
k and pk(A) denotes the corresponding matrix polynomial. As a consequence
∥e(k+1)∥2
A ≤(x −z)T A(x −z) = xT qk+1(A)Aqk+1(A)x,
(4.48)
where qk+1(ξ) = 1 −pk(ξ)ξ ∈P0,1
k+1, being P0,1
k+1 = {q ∈Pk+1 : q(0) = 1} and
qk+1(A) the associated matrix polynomial. From (4.48) we get
∥e(k+1)∥2
A =
min
qk+1∈P0,1
k+1
xT qk+1(A)Aqk+1(A)x.
(4.49)
Since A is symmetric positive deﬁnite, there exists an orthogonal matrix Q
such that A = QΛQT with Λ = diag(λ1, . . . , λn). Noticing that qk+1(A) =
Qqk+1(Λ)QT , we get from (4.49)
∥e(k+1)∥2
A
=
min
qk+1∈P0,1
k+1
xT Qqk+1(Λ)QT QΛQT Qqk+1(Λ)QT x
=
min
qk+1∈P0,1
k+1
xT Qqk+1(Λ)Λqk+1(Λ)QT x
=
min
qk+1∈P0,1
k+1
yT diag(qk+1(λi)λiqk+1(λi))y
=
min
qk+1∈P0,1
k+1
n

i=1
y2
i λi(qk+1(λi))2

4.3 Stationary and Nonstationary Iterative Methods
155
having set y = Qx. Thus, we can conclude that
∥e(k+1)∥2
A ≤
.
min
qk+1∈P0,1
k+1
max
λi∈σ(A)(qk+1(λi))2
/
n

i=1
y2
i λi.
Recalling that
n

i=1
y2
i λi = ∥e(0)∥2
A, we have
∥e(k+1)∥A
∥e(0)∥A
≤
min
qk+1∈P0,1
k+1
max
λi∈σ(A)|qk+1(λi)|.
Let us now recall the following property
Property 4.6 The problem of minimizing
max
λn≤z≤λ1|q(z)| over the space
P0,1
k+1([λn, λ1]) admits a unique solution, given by the polynomial
pk+1(ξ) = Tk+1
 λ1 + λn −2ξ
λ1 −λn

/Ck+1,
ξ ∈[λn, λ1],
where Ck+1 = Tk+1( λ1+λn
λ1−λn ) and Tk+1 is the Chebyshev polynomial of degree k+1
(see Section 10.10). The value of the minimum is 1/Ck+1.
Using this property we get
∥e(k+1)∥A
∥e(0)∥A
≤
1
Tk+1
 λ1 + λn
λ1 −λn

from which the thesis follows since in the case of a symmetric positive deﬁnite
matrix
1
Ck+1 =
2ck+1
1 + c2(k+1) .
3
The generic k-th iteration of the conjugate gradient method is well deﬁned
only if the descent direction p(k) is non null. Besides, if p(k) = 0, then the
iterate x(k) must necessarily coincide with the solution x of the system.
Moreover, irrespectively of the choice of the parameters βk, one can show
(see [Axe94], p. 463) that the sequence x(k) generated by the CG method
is such that either x(k) ̸= x, p(k) ̸= 0, αk ̸= 0 for any k, or there must exist
an integer m such that x(m) = x, where x(k) ̸= x, p(k) ̸= 0 and αk ̸= 0 for
k = 0, 1, . . . , m −1.
The particular choice made for βk in (4.45) ensures that m ≤n. In ab-
sence of rounding errors, the CG method can thus be regarded as being a
direct method, since it terminates after a ﬁnite number of steps. However,
for matrices of large size, it is usually employed as an iterative scheme,

156
4. Iterative Methods for Solving Linear Systems
where the iterations are stopped when the error gets below a ﬁxed toler-
ance. In this respect, the dependence of the error reduction factor on the
condition number of the matrix is more favorable than for the gradient
method. We also notice that estimate (4.47) is often overly pessimistic and
does not account for the fact that in this method, unlike what happens for
the gradient method, the convergence is inﬂuenced by the whole spectrum
of A, and not only by its extremal eigenvalues.
Remark 4.3 (Eﬀect of rounding errors) The termination property of
the CG method is rigorously valid only in exact arithmetic. The cumulating
rounding errors prevent the descent directions from being A-conjugate and
can even generate null denominators in the computation of coeﬃcients αk
and βk. This latter phenomenon, known as breakdown, can be avoided by
introducing suitable stabilization procedures; in such an event, we speak
about stabilized gradient methods.
Despite the use of these strategies, it may happen that the CG method
fails to converge (in ﬁnite arithmetic) after n iterations. In such a case,
the only reasonable possibility is to restart the iterative process, taking
as residual the last computed one. By so doing, the cyclic CG method or
CG method with restart is obtained, for which, however, the convergence
properties of the original CG method are no longer valid.
■
4.3.5
The Preconditioned Conjugate Gradient Method
If P is a symmetric and positive deﬁnite preconditioning matrix, the pre-
conditioned conjugate gradient method (PCG) consists of applying the CG
method to the preconditioned system
P−1/2AP−1/2y = P−1/2b,
with y = P1/2x.
In practice, the method is implemented without explicitly requiring the
computation of P1/2 or P−1/2. After some algebra, the following scheme is
obtained:
given x(0) and setting r(0) = b −Ax(0), z(0) = P−1r(0) e p(0) = z(0), the
k-th iteration reads

4.3 Stationary and Nonstationary Iterative Methods
157
αk =
p(k)T r(k)
p(k)T Ap(k)
x(k+1) = x(k) + αkp(k)
r(k+1) = r(k) −αkAp(k)
Pz(k+1) = r(k+1)
βk = (Ap(k))T z(k+1)
(Ap(k))T p(k)
p(k+1) = z(k+1) −βkp(k).
The computational cost is increased with respect to the CG method, as
one needs to solve at each step the linear system Pz(k+1) = r(k+1). For this
system the symmetric preconditioners examined in Section 4.3.2 can be
used. The error estimate is the same as for the nonpreconditioned method,
provided to replace the matrix A by P−1A.
In Program 20 an implementation of the PCG method is reported. For
a description of the input/output parameters, see Program 19.
Program 20 - conjgrad : Preconditioned conjugate gradient method
function [x, error, niter, ﬂag] = conjgrad(A, x, b, P, maxit, tol)
ﬂag = 0; niter = 0; bnrm2 = norm( b );
if ( bnrm2 == 0.0 ), bnrm2 = 1.0; end
r = b - A*x; error = norm( r ) / bnrm2;
if ( error < tol ) return, end
for niter = 1:maxit
z = P \ r; rho = (r’*z);
if niter > 1
beta = rho / rho1;
p = z + beta*p;
else
p = z;
end
q = A*p;
alpha = rho / (p’*q );
x = x + alpha * p;
r = r - alpha*q;
error = norm( r ) / bnrm2;
if ( error <= tol ), break, end
rho1 = rho;
end
if ( error > tol ) ﬂag = 1; end

158
4. Iterative Methods for Solving Linear Systems
Example 4.7 Let us consider again the linear system of Example 4.6. The CG
method has been run with the same input data as in the previous example. It
converges in 3 iterations for m = 16 and in 45 iterations for m = 400. Using the
same preconditioner as in Example 4.6, the number of iterations decreases from
45 to 26, in the case m = 400.
•
0
5
10
15
20
25
30
35
40
45
10
−14
10
−12
10
−10
10
−8
10
−6
10
−4
10
−2
10
0
FIGURE 4.8. Behavior of the residual, normalized to the right-hand side, as a
function of the number of iterations for the conjugate gradient method applied
to the systems of Example 4.6 in the case m = 400. The curve in dashed line
refers to the non preconditioned method, while the curve in solid line refers to
the preconditioned one
4.3.6
The Alternating-Direction Method
Assume that A = A1+A2, with A1 and A2 symmetric and positive deﬁnite.
The alternating direction method (ADI), as introduced by Peaceman and
Rachford [PJ55], is an iterative scheme for (3.2) which consists of solving
the following systems ∀k ≥0
(I + α1A1)x(k+1/2) = (I −α1A2)x(k) + α1b,
(I + α2A2)x(k+1) = (I −α2A1)x(k+1/2) + α2b
(4.50)
where α1 and α2 are two real parameters. The ADI method can be cast in
the form (4.2) setting
B = (I + α2A2)−1(I −α2A1)(I + α1A1)−1(I −α1A2),
f =
0
α1(I −α2A1)(I + α1A1)−1 + α2I
1
b.
Both B and f depend on α1 and α2. The following estimate holds
ρ(B) ≤
max
i=1,... ,n

1 −α2λ(1)
i
1 + α1λ(1)
i
 max
i=1,... ,n

1 −α1λ(2)
i
1 + α2λ(2)
i
 ,

4.4 Methods Based on Krylov Subspace Iterations
159
where λ(i)
1
and λ(i)
2 , for i = 1, . . . , n, are the eigenvalues of A1 and A2,
respectively. The method converges if ρ(B) < 1, which is always veriﬁed if
α1 = α2 = α > 0. Moreover (see [Axe94]) if γ ≤λ(j)
i
≤δ ∀i = 1, . . . , n,
∀j = 1, 2, for suitable γ and δ then the ADI method converges with the
choice α1 = α2 = 1/√δγ, provided that γ/δ tends to 0 as the size of A
grows. In such an event the corresponding spectral radius satisﬁes
ρ(B) ≤

1 −

γ/δ
1 +

γ/δ
2
.
4.4
Methods Based on Krylov Subspace Iterations
In this section we introduce iterative methods based on Krylov subspace
iterations. For the proofs and further analysis, we refer to [Saa96], [Axe94]
and [Hac94].
Consider the Richardson method (4.24) with P=I; the residual at the
k-th step can be related to the initial residual as
r(k) =
k−1

j=0
(I −αjA)r(0)
(4.51)
so that r(k) = pk(A)r(0), where pk(A) is a polynomial in A of degree k. If
we introduce the space
Km(A; v) = span

v, Av, . . . , Am−1v

,
(4.52)
it immediately appears from (4.51) that r(k) ∈Kk+1(A; r(0)). The space
deﬁned in (4.52) is called the Krylov subspace of order m. It is a subspace
of the set spanned by all the vectors u ∈Rn that can be written as u =
pm−1(A)v, where pm−1 is a polynomial in A of degree ≤m −1.
In an analogous manner as for (4.51), it is seen that the iterate x(k) of
the Richardson method is given by
x(k) = x(0) +
k−1

j=0
αjr(j)
so that x(k) belongs to the following space
Wk =
2
v = x(0) + y, y ∈Kk(A; r(0))
3
.
(4.53)
Notice also that k−1
j=0 αjr(j) is a polynomial in A of degree less than k −1.
In the non preconditioned Richardson method we are thus looking for an

160
4. Iterative Methods for Solving Linear Systems
approximate solution to x in the space Wk. More generally, we can think
of devising methods that search for approximate solutions of the form
x(k) = x(0) + qk−1(A)r(0),
(4.54)
where qk−1 is a polynomial selected in such a way that x(k) be, in a sense
that must be made precise, the best approximation of x in Wk. A method
that looks for a solution of the form (4.54) with Wk deﬁned as in (4.53) is
called a Krylov method.
A ﬁrst question concerning Krylov subspace iterations is whether the
dimension of Km(A; v) increases as the order m grows. A partial answer is
provided by the following result.
Property 4.7 Let A ∈Rn×n and v ∈Rn. The Krylov subspace Km(A; v)
has dimension equal to m iﬀthe degree of v with respect to A, denoted by
degA(v), is not less than m, where the degree of v is deﬁned as the minimum
degree of a monic non null polynomial p in A, for which p(A)v = 0.
The dimension of Km(A; v) is thus equal to the minimum between m and
the degree of v with respect to A and, as a consequence, the dimension
of the Krylov subspaces is certainly a nondecreasing function of m. Notice
that the degree of v cannot be greater than n due to the Cayley-Hamilton
Theorem (see Section 1.7).
Example 4.8 Consider the matrix A = tridiag4(−1, 2, −1). The vector v =
(1, 1, 1, 1)T has degree 2 with respect to A since p2(A)v = 0 with p2(A) = I4 −
3A+A2, while there is no monic polynomial p1 of degree 1 for which p1(A)v = 0.
As a consequence, all Krylov subspaces from K2(A; v) on, have dimension equal
to 2. The vector w = (1, 1, −1, 1)T has, instead, degree 4 with respect to A.
•
For a ﬁxed m, it is possible to compute an orthonormal basis for Km(A; v)
using the so-called Arnoldi algorithm.
Setting v1 = v/∥v∥2, this method generates an orthonormal basis {vi}
for Km(A; v1) using the Gram-Schmidt procedure (see Section 3.4.3). For
k = 1, . . . , m, the Arnoldi algorithm computes
hik = vT
i Avk,
i = 1, 2, . . . , k,
wk = Avk −
k

i=1
hikvi,
hk+1,k = ∥wk∥2.
(4.55)
If wk = 0 the process terminates and in such a case we say that a breakdown
of the algorithm has occurred; otherwise, we set vk+1 = wk/∥wk∥2 and the
algorithm restarts, incrementing k by 1.

4.4 Methods Based on Krylov Subspace Iterations
161
It can be shown that if the method terminates at the step m then the
vectors v1, . . . , vm form a basis for Km(A; v). In such a case, if we denote
by Vm ∈Rn×m the matrix whose columns are the vectors vi, we have
VT
mAVm = Hm,
VT
m+1AVm = Hm,
(4.56)
where Hm ∈R(m+1)×m is the upper Hessenberg matrix whose entries hij
are given by (4.55) and Hm ∈Rm×m is the restriction of Hm to the ﬁrst m
rows and m columns.
The algorithm terminates at an intermediate step k < m iﬀdegA(v1) =
k. As for the stability of the procedure, all the considerations valid for the
Gram-Schmidt method hold. For more eﬃcient and stable computational
variants of (4.55), we refer to [Saa96].
The functions arnoldi alg and GSarnoldi, invoked by Program 21, pro-
vide an implementation of the Arnoldi algorithm. In output, the columns
of V contain the vectors of the generated basis, while the matrix H stores
the coeﬃcients hik computed by the algorithm. If m steps are carried out,
V = Vm and H(1 : m, 1 : m) = Hm.
Program 21 - arnoldi alg : The Arnoldi algorithm
function [V,H]=arnoldi alg(A,v,m)
v=v/norm(v,2); V=[v1];
H=[];
k=0;
while k <= m-1
[k,V,H] = GSarnoldi(A,m,k,V,H);
end
function [k,V,H]=GSarnoldi(A,m,k,V,H)
k=k+1;
H=[H,V(:,1:k)’*A*V(:,k)];
s=0; for i=1:k,
s=s+H(i,k)*V(:,i);
end
w=A*V(:,k)-s;
H(k+1,k)=norm(w,2);
if ( H(k+1,k) <= eps ) & ( k < m )
V=[V,w/H(k+1,k)];
else
k=m+1;
end
Having introduced an algorithm for generating the basis for a Krylov sub-
space of any order, we can now solve the linear system (3.2) by a Krylov
method. As already noticed, for all of these methods the iterate x(k) is
always of the form (4.54) and, for a given r(0), the vector x(k) is selected as
being the unique element in Wk which satisﬁes a criterion of minimal dis-
tance from x. Thus, the feature distinguishing two diﬀerent Krylov methods
is the criterion for selecting x(k).
The most natural idea consists of searching for x(k) ∈Wk as the vector
which minimizes the Euclidean norm of the error. This approach, how-

162
4. Iterative Methods for Solving Linear Systems
ever, does not work in practice since x(k) would depend on the (unknown)
solution x.
Two alternative strategies can be pursued:
1. compute x(k) ∈Wk enforcing that the residual r(k) is orthogonal to
any vector in Kk(A; r(0)), i.e., we look for x(k) ∈Wk such that
vT (b −Ax(k)) = 0
∀v ∈Kk(A; r(0));
(4.57)
2. compute x(k) ∈Wk minimizing the Euclidean norm of the residual
∥r(k)∥2, i.e.
∥b −Ax(k)∥2 = min
v∈Wk∥b −Av∥2.
(4.58)
Satisfying (4.57) leads to the Arnoldi method for linear systems (more
commonly known as FOM, full orthogonalization method), while satisfying
(4.58) yields the GMRES (generalized minimum residual) method.
In the two forthcoming sections we shall assume that k steps of the
Arnoldi algorithm have been carried out, in such a way that an orthonormal
basis for Kk(A; r(0)) has been generated and stored into the column vectors
of the matrix Vk with v1 = r(0)/∥r(0)∥2. In such a case the new iterate x(k)
can always be written as
x(k) = x(0) + Vkz(k),
(4.59)
where z(k) must be selected according to a ﬁxed criterion.
4.4.1
The Arnoldi Method for Linear Systems
Let us enforce that r(k) be orthogonal to Kk(A; r(0)) by requiring that
(4.57) holds for all the basis vectors vi, i.e.
VT
k r(k) = 0.
(4.60)
Since r(k) = b−Ax(k) with x(k) of the form (4.59), relation (4.60) becomes
VT
k (b −Ax(0)) −VT
k AVkz(k) = VT
k r(0) −VT
k AVkz(k) = 0.
(4.61)
Due to the orthonormality of the basis and the choice of v1, VT
k r(0) =
∥r(0)∥2e1, e1 being the ﬁrst unit vector of Rm. Recalling (4.56), from (4.61)
it turns out that z(k) is the solution to the linear system
Hkz(k) = ∥r(0)∥2e1.
(4.62)
Once z(k) is known, we can compute x(k) from (4.59). Since Hk is an upper
Hessenberg matrix, the linear system in (4.62) can be easily solved, for
instance, resorting to the LU factorization of Hk.

4.4 Methods Based on Krylov Subspace Iterations
163
We notice that the method, if working in exact arithmetic, cannot execute
more than n steps and that it terminates after m < n steps only if a
breakdown in the Arnoldi algorithm occurs. As for the convergence of the
method, the following result holds.
Theorem 4.13 In exact arithmetic the Arnoldi method yields the solution
of (3.2) after at most n iterations.
Proof. If the method terminates at the n-th iteration, then it must necessarily
be x(n) = x since Kn(A; r(0)) = Rn. Conversely, if a breakdown occurs after m
iterations, for a suitable m < n, then x(m) = x. Indeed, inverting the ﬁrst relation
in (4.56), we get
x(m) = x(0) + Vmz(m) = x(0) + VmH−1
m VT
mr(0) = A−1b.
3
In its naive form, FOM does not require an explicit computation of the
solution or the residual, unless a breakdown occurs. Therefore, monitoring
its convergence (by computing, for instance, the residual at each step) might
be computationally expensive. The residual, however, is available without
explicitly requiring to compute the solution since at the k-th step we have
∥b −Ax(k)∥2 = hk+1,k|eT
k zk|
and, as a consequence, one can decide to stop the method if
hk+1,k|eT
k zk|/∥r(0)∥2 ≤ε
(4.63)
ε > 0 being a ﬁxed tolerance.
The most relevant consequence of Theorem 4.13 is that FOM can be
regarded as a direct method, since it yields the exact solution after a ﬁnite
number of steps. However, this fails to hold when working in ﬂoating point
arithmetic due to the cumulating rounding errors. Moreover, if we also
account for the high computational eﬀort, which, for a number of m steps
and a sparse matrix of order n with nz nonzero entries, is of the order of
2(nz + mn) ﬂops, and the large memory occupation needed to store the
matrix Vm, we conclude that the Arnoldi method cannot be used in the
practice, except for small values of m.
Several remedies to this drawback are available, one of which consisting
of preconditioning the system (using, for instance, one of the precondition-
ers proposed in Section 4.3.2). Alternatively, we can also introduce some
modiﬁed versions of the Arnoldi method following two approaches:
1. no more than m consecutive steps of FOM are taken, m being a small
ﬁxed number (usually, m ≃10). If the method fails to converge, we set

164
4. Iterative Methods for Solving Linear Systems
x(0) = x(m) and FOM is repeated for other m steps. This procedure
is carried out until convergence is achieved. This method, known as
FOM(m) or FOM with restart, reduces the memory occupation, only
requiring to store matrices with m columns at most;
2. a limitation is set on the number of directions involved in the orthog-
onalization procedure in the Arnoldi algorithm, yielding the incom-
plete orthogonalization method or IOM. In the practice, the k-th step
of the Arnoldi algorithm generates a vector vk+1 which is orthonor-
mal, at most, to the q preceding vectors, where q is ﬁxed according
to the amount of available memory.
It is worth noticing that Theorem 4.13 does no longer hold for the methods
stemming from the two strategies above.
Program 22 provides an implementation of the FOM algorithm with a
stopping criterion based on the residual (4.63). The input parameter m is
the maximum admissible size of the Krylov subspace that is being gener-
ated and represents, as a consequence, the maximum admissible number of
iterations.
Program 22 - arnoldi met : The Arnoldi method for linear systems
function [x,k]=arnoldi met(A,b,m,x0,toll)
r0=b-A*x0; nr0=norm(r0,2);
if nr0 ˜= 0
v1=r0/nr0; V=[v1]; H=[]; k=0; istop=0;
while (k <= m-1) & (istop == 0)
[k,V,H] = GSarnoldi(A,m,k,V,H);
[nr,nc]=size(H); e1=eye(nc);
y=(e1(:,1)’*nr0)/H(1:nc,:);
residual = H(nr,nc)*abs(y*e1(:,nc));
if residual <= toll
istop = 1; y=y’;
end
end
if istop==0
[nr,nc]=size(H); e1=eye(nc);
y=(e1(:,1)’*nr0)/H(1:nc,:); y=y’;
end
x=x0+V(:,1:nc)*y;
else
x=x0;
end
Example 4.9 Let us solve the linear system Ax = b with A = tridiag100(−1, 2,
−1) and b such that the solution is x = 1T . The initial vector is x(0) = 0T
and toll=10−10. The method converges in 50 iterations and Figure 4.9 reports
its convergence history. Notice the sudden, dramatic, reduction of the residual,

4.4 Methods Based on Krylov Subspace Iterations
165
which is a typical warning that the last generated subspace Wk is suﬃciently rich
to contain the exact solution of the system.
•
0
10
20
30
40
50
60
10
−16
10
−14
10
−12
10
−10
10
−8
10
−6
10
−4
10
−2
10
0
10
2
FIGURE 4.9. The behavior of the residual as a function of the number of itera-
tions for the Arnoldi method applied to the linear system in Example 4.9
4.4.2
The GMRES Method
This method is characterized by selecting x(k) in such a way to minimize
the Euclidean norm of the residual at each k-th step. Recalling (4.59) we
have
r(k) = r(0) −AVkz(k),
(4.64)
but, since r(0) = v1∥r(0)∥2 and (4.56) holds, relation (4.64) becomes
r(k) = Vk+1(∥r(0)∥2e1 −Hkz(k)),
(4.65)
where e1 is the ﬁrst unit vector of Rk+1. Therefore, in the GMRES method
the solution at step k can be computed through (4.59) as
z(k) chosen in such a way to minimize ∥∥r(0)∥2e1 −Hkz(k)∥2
(4.66)
(the matrix Vk+1 appearing in (4.65) does not change the value of ∥· ∥2
since it is orthogonal). Having to solve at each step a least-squares problem
of size k, the GMRES method will be the more eﬀective the smaller is
the number of iterations. Exactly as for the Arnoldi method, the GMRES
method terminates at most after n iterations, yielding the exact solution.
Premature stops are due to a breakdown in the orthonormalization Arnoldi
algorithm. More precisely, we have the following result.
Property 4.8 A breakdown occurs for the GMRES method at a step m
(with m < n) iﬀthe computed solution x(m) coincides with the exact solu-
tion to the system.

166
4. Iterative Methods for Solving Linear Systems
A basic implementation of the GMRES method is provided in Program 23.
This latter requires in input the maximum admissible size m for the Krylov
subspace and the tolerance toll on the Euclidean norm of the residual
normalized to the initial residual. This implementation of the method com-
putes the solution x(k) at each step in order to evaluate the residual, with
a consequent increase of the computational eﬀort.
Program 23 - GMRES : The GMRES method for linear systems
function [x,k]=gmres(A,b,m,toll,x0)
r0=b-A*x0; nr0=norm(r0,2);
if nr0 ˜= 0
v1=r0/nr0; V=[v1]; H=[]; k=0; residual=1;
while k <= m-1 & residual > toll,
[k,V,H] = GSarnoldi(A,m,k,V,H);
[nr,nc]=size(H);
y=(H’*H) \ (H’*nr0*[1;zeros(nr-1,1)]);
x=x0+V(:,1:nc)*y;
residual = norm(b-A*x,2)/nr0;
end
else
x=x0;
end
To improve the eﬃciency of the GMRES algorithm it is necessary to devise
a stopping criterion which does not require the explicit evaluation of the
residual at each step. This is possible, provided that the linear system with
upper Hessenberg matrix Hk is appropriately solved.
In practice, Hk is transformed into an upper triangular matrix Rk ∈
R(k+1)×k with rk+1,k = 0 such that QT
k Rk = Hk, where Qk is a matrix
obtained as the product of k Givens rotations (see Section 5.6.3). Then,
since Qk is orthogonal, it can be seen that minimizing ∥∥r(0)∥2e1−Hkz(k)∥2
is equivalent to minimize ∥fk −Rkz(k)∥2, with fk = Qk∥r(0)∥2e1. It can
also be shown that the k + 1-th component of fk is, in absolute value, the
Euclidean norm of the residual at the k-th step.
As FOM, the GMRES method entails a high computational eﬀort and
a large amount of memory, unless convergence occurs after few iterations.
For this reason, two variants of the algorithm are available, one named
GMRES(m) and based on the restart after m steps, the other named Quasi-
GMRES or QGMRES and based on stopping the Arnoldi orthogonalization
process. It is worth noting that these two methods do not enjoy Property
4.8.
Remark 4.4 (Projection methods) Denoting by Yk and Lk two generic
m-dimensional subspaces of Rn, we call projection method a process which
generates an approximate solution x(k) at step k, enforcing that x(k) ∈Yk

4.4 Methods Based on Krylov Subspace Iterations
167
and that the residual r(k) = b−Ax(k) be orthogonal to Lk. If Yk = Lk, the
projection process is said to be orthogonal, oblique otherwise (see [Saa96]).
The Krylov subspace iterations can be regarded as being projection
methods. For instance, the Arnoldi method is an orthogonal projection
method where Lk = Yk = Kk(A; r(0)), while the GMRES method is an
oblique projection method with Yk = Kk(A; r(0)) and Lk = AYk. It is
worth noticing that some classical methods introduced in previous sections
fall into this category. For example, the Gauss-Seidel method is an orthogo-
nal projection method where at the k-th step Kk(A; r(0)) = span(ek), with
k = 1, . . . , n. The projection steps are carried out cyclically from 1 to n
until convergence.
■
4.4.3
The Lanczos Method for Symmetric Systems
The Arnoldi algorithm simpliﬁes considerably if A is symmetric since the
matrix Hm is tridiagonal and symmetric (indeed, from (4.56) it turns out
that Hm must be symmetric, so that, being upper Hessenberg by construc-
tion, it must necessarily be tridiagonal). In such an event the method is
more commonly known as the Lanczos algorithm. For ease of notation, we
henceforth let αi = hii and βi = hi−1,i.
An implementation of the Lanczos algorithm is provided in Program 24.
Vectors alpha and beta contain the coeﬃcients αi and βi computed by the
scheme.
Program 24 - Lanczos : The Lanczos method for linear systems
function [V,alpha,beta]=lanczos(A,m)
n=size(A); V=[0*[1:n]’,[1,0*[1:n-1]]’];
beta(1)=0; normb=1; k=1;
while k <= m & normb >= eps
vk = V(:,k+1);
w = A*vk-beta(k)*V(:,k);
alpha(k)= w’*vk;
w = w - alpha(k)*vk
normb = norm(w,2);
if normb ˜= 0
beta(k+1)=normb;
V=[V,w/normb];
k=k+1;
end
end
[n,m]=size(V); V=V(:,2:m-1);
alpha=alpha(1:n); beta=beta(2:n);
The algorithm, which is far superior to Arnoldi’s one as far as memory
saving is concerned, is not numerically stable since only the ﬁrst generated
vectors are actually orthogonal. For this reason, several stable variants have
been devised.
As in previous cases, also the Lanczos algorithm can be employed as a
solver for linear systems, yielding a symmetric form of the FOM method. It

168
4. Iterative Methods for Solving Linear Systems
can be shown that r(k) = γkvk+1, for a suitable γk (analogously to (4.63))
so that the residuals are all mutually orthogonal.
Remark 4.5 (The conjugate gradient method) If A is symmetric and
positive deﬁnite, starting from the Lanczos method for linear systems it is
possible to derive the conjugate gradient method already introduced in Sec-
tion 4.3.4 (see [Saa96]). The conjugate gradient method is a variant of the
Lanczos method where the orthonormalization process remains incomplete.
As a matter of fact, the A-conjugate directions of the CG method can
be characterized as follows. If we carry out at the generic k-th step the
LU factorization Hk = LkUk, with Lk (Uk) lower (upper) bidiagonal, the
iterate x(k) of the Lanczos method for systems reads
x(k) = x(0) + PkL−1
k ∥r(0)∥2e1,
with Pk = VkU−1
k . The column vectors of Pk are mutually A-conjugate.
Indeed, PT
k APk is symmetric and bidiagonal since
PT
k APk = U−T
k
HkU−1
k
= U−T
k
Lk,
so that it must necessarily be diagonal. As a result, pT
j Api = 0 if i ̸= j,
having denoted by pi the i-th column vector of matrix Pk.
■
As happens for the FOM method, also the GMRES method simpliﬁes
if A is symmetric. The resulting scheme is called conjugate residuals or
CR method since it enjoys the property that the residuals are mutually
A-conjugate. Variants of this method are the generalized conjugate resid-
uals method (GCR) and the method commonly known as ORTHOMIN
(obtained by truncation of the orthonormalization process as done for the
IOM method).
4.5
The Lanczos Method for Unsymmetric Systems
The Lanczos orthogonalization process can be extended to deal with un-
symmetric matrices through a bi-orthogonalization procedure as follows.
Two bases, {vi}m
i=1 and {zi}m
i=1, are generated for the subspaces Km(A; v1)
and Km(AT ; z1), respectively, with zT
1 v1 = 1, such that
zT
i vj = δij,
i, j = 1, . . . , m.
(4.67)
Two sets of vectors satisfying (4.67) are said to be bi-orthogonal and can
be obtained through the following algorithm: setting β1 = γ1 = 0 and z0 =
v0 = 0T , at the generic k-th step, with k = 1, . . . , m, we set αk = zT
k Avk,
then we compute
˜vk+1 = Avk −αkvk −βkvk−1,
˜zk+1 = AT zk −αkzk −γkzk−1.

4.5 The Lanczos Method for Unsymmetric Systems
169
If γk+1 =

|˜zT
k+1˜vk+1| = 0 the algorithm is stopped, otherwise we set
βk+1 = ˜zT
k+1˜vk+1/γk+1 and generate two new vectors in the basis as
vk+1 = ˜vk+1/γk+1,
zk+1 = ˜zk+1/βk+1.
If the process terminates after m steps, denoting by Vm and Zm the ma-
trices whose columns are the vectors of the basis that has been generated,
we have
ZT
mAVm = Tm,
Tm being the following tridiagonal matrix
Tm =


α1
β2
0
γ2
α2
...
...
...
βm
0
γm
αm


.
As in the symmetric case, the bi-orthogonalization Lanczos algorithm can
be utilized to solve the linear system (3.2). For this purpose, for m ﬁxed,
once the bases {vi}m
i=1 and {zi}m
i=1 have been constructed, it suﬃces to set
x(m) = x(0) + Vmy(m),
where y(m) is the solution to the linear system Tmy(m) = ∥r(0)∥2e1. It
is also possible to introduce a stopping criterion based on the residual,
without computing it explicitly, since
∥r(m)∥2 = |γm+1eT
mym| ∥vm+1∥2.
An implementation of the Lanczos method for unsymmetric systems is
given in Program 25. If a breakdown of the algorithm occurs, i.e., if γk+1 =
0, the method stops returning in output a negative value of the variable
niter which denotes the number of iterations necessary to reduce the initial
residual by a factor toll.

170
4. Iterative Methods for Solving Linear Systems
Program 25 - Lanczosnosym
: The Lanczos method for unsymmetric
systems
function [xk,nres,niter]=lanczosnosym(A,b,x0,m,toll)
r0=b-A*x0; nres0=norm(r0,2);
if nres0 ˜= 0
V=r0/nres0; Z=V; gamma(1)=0; beta(1)=0; k=1; nres=1;
while k <= m & nres > toll
vk=V(:,k); zk=Z(:,k);
if
k==1, vk1=0*vk;
zk1=0*zk;
else, vk1=V(:,k-1); zk1=Z(:,k-1); end
alpha(k)=zk’*A*vk;
tildev=A*vk-alpha(k)*vk-beta(k)*vk1;
tildez=A’*zk-alpha(k)*zk-gamma(k)*zk1;
gamma(k+1)=sqrt(abs(tildez’*tildev));
if gamma(k+1) == 0,
k=m+2;
else
beta(k+1)=tildez’*tildev/gamma(k+1);
Z=[Z,tildez/beta(k+1)];
V=[V,tildev/gamma(k+1)];
end
if k˜=m+2
if k==1
Tk = alpha;
else
Tk=diag(alpha)+diag(beta(2:k),1)+diag(gamma(2:k),-1);
end
yk=Tk \ (nres0*[1,0*[1:k-1]]’);
xk=x0+V(:,1:k)*yk;
nres=abs(gamma(k+1)*[0*[1:k-1],1]*yk)*norm(V(:,k+1),2)/nres0;
k=k+1;
end
end
else
x=x0;
end
if k==m+2, niter=-k; else, niter=k-1; end
Example 4.10 Let us solve the linear system with matrix A = tridiag100(−0.5, 2,
−1) and right-side b selected in such a way that the exact solution is x = 1T .
Using Program 25 with toll= 10−13 and a randomly generated x0, the algorithm
converges in 59 iterations. Figure 4.10 shows the convergence history reporting
the graph of ∥r(k)∥2/∥r(0)∥2 as a function of the number of iterations.
•
We conclude recalling that some variants of the unsymmetric Lanczos
method have been devised, that are characterized by a reduced compu-
tational cost. We refer the interested reader to the bibliography below for a

4.6 Stopping Criteria
171
0
10
20
30
40
50
60
10
−14
10
−12
10
−10
10
−8
10
−6
10
−4
10
−2
10
0
FIGURE 4.10. Graph of the residual normalized to the initial residual as a func-
tion of the number of iterations for the Lanczos method applied to the system in
Example 4.10
complete description of the algorithms and to the programs included in the
MATLAB version of the public domain library templates for their eﬃcient
implementation [BBC+94].
1. The bi-conjugate gradient method (BiCG): it can be derived by the
unsymmetric Lanczos method in the same way as the conjugate gra-
dient method is obtained from the FOM method [Fle75];
2. the Quasi-Minimal Residual method (QMR): it is analogous to the
GMRES method, the only diﬀerence being the fact that the Arnoldi
orthonormalization process is replaced by the Lanczos bi-orthogona-
lization;
3. the conjugate gradient squared method (CGS): the matrix-vector prod-
ucts involving the transposed matrix AT are removed. A variant of
this method, known as BiCGStab, is characterized by a more reg-
ular convergence than provided by the CGS method (see [Son89],
[vdV92]).
4.6
Stopping Criteria
In this section we address the problem of how to estimate the error intro-
duced by an iterative method and the number kmin of iterations needed to
reduce the initial error by a factor ε.
In practice, kmin can be obtained by estimating the convergence rate of
(4.2), i.e. the rate at which ∥e(k)∥→0 as k tends to inﬁnity. From (4.4),
we get
∥e(k)∥
∥e(0)∥≤∥Bk∥,

172
4. Iterative Methods for Solving Linear Systems
so that ∥Bk∥is an estimate of the reducing factor of the norm of the error
after k steps. Typically, the iterative process is continued until ∥e(k)∥has
reduced with respect to ∥e(0)∥by a certain factor ε < 1, that is
∥e(k)∥≤ε∥e(0)∥.
(4.68)
If we assume that ρ(B) < 1, then Property 1.13 implies that there exists
a suitable matrix norm ∥· ∥such that ∥B∥< 1. As a consequence, ∥Bk∥
tends to zero as k tends to inﬁnity, so that (4.68) can be satisﬁed for a
suﬃciently large k such that ∥Bk∥≤ε holds. However, since ∥Bk∥< 1, the
previous inequality amounts to requiring that
k ≥log(ε)/
1
k log ∥Bk∥

= −log(ε)/Rk(B),
(4.69)
where Rk(B) is the average convergence rate introduced in Deﬁnition 4.2.
From a practical standpoint, (4.69) is useless, being nonlinear in k; if, how-
ever, the asymptotic convergence rate is adopted, instead of the average
one, the following estimate for kmin is obtained
kmin ≃−log(ε)/R(B).
(4.70)
This latter estimate is usually rather optimistic, as conﬁrmed by Example
4.11.
Example 4.11 For the matrix A3 of Example 4.2, in the case of Jacobi method,
letting ε = 10−5, condition (4.69) is satisﬁed with kmin = 16, while (4.70) yields
kmin = 15, with a good agreement between the two estimates. Instead, on the
matrix A4 of Example 4.2, we ﬁnd that (4.69) is satisﬁed with kmin = 30, while
(4.70) yields kmin = 26.
•
4.6.1
A Stopping Test Based on the Increment
From the recursive error relation e(k+1) = Be(k), we get
∥e(k+1)∥≤∥B∥∥e(k)∥.
(4.71)
Using the triangular inequality we get
∥e(k+1)∥≤∥B∥(∥e(k+1)∥+ ∥x(k+1) −x(k)∥),
from which it follows that
∥x −x(k+1)∥≤
∥B∥
1 −∥B∥∥x(k+1) −x(k)∥.
(4.72)
In particular, taking k = 0 in (4.72) and applying recursively (4.71) we also
get
∥x −x(k+1)∥≤∥B∥k+1
1 −∥B∥∥x(1) −x(0)∥,

4.6 Stopping Criteria
173
which can be used to estimate the number of iterations necessary to fulﬁll
the condition ∥e(k+1)∥≤ε, for a given tolerance ε.
In the practice, ∥B∥can be estimated as follows: since
x(k+1) −x(k) = −(x −x(k+1)) + (x −x(k)) = B(x(k) −x(k−1)),
a lower bound of ∥B∥is provided by c = δk+1/δk, where δj+1 = ∥x(j+1) −
x(j)∥, with j = k −1, k. Replacing ∥B∥by c, the right-hand side of (4.72)
suggests using the following indicator for ∥e(k+1)∥
ϵ(k+1) =
δ2
k+1
δk −δk+1
.
(4.73)
Due to the kind of approximation of ∥B∥that has been used, the reader is
warned that ϵ(k+1) should not be regarded as an upper bound for ∥e(k+1)∥.
However, often ϵ(k+1) provides a reasonable indication about the true error
behavior, as we can see in the following example.
Example 4.12 Consider the linear system Ax=b with
A =


4
1
1
2
−9
0
0
−8
−6

,
b =


6
−7
−14

,
which admits the unit vector as exact solution. Let us apply the Jacobi method
and estimate the error at each step by using (4.73). Figure 4.11 shows an ac-
ceptable agreement between the behavior of the error ∥e(k+1)∥∞and that of its
estimate ϵ(k+1).
•
0
5
10
15
20
25
10
−8
10
−7
10
−6
10
−5
10
−4
10
−3
10
−2
10
−1
10
0
10
1
FIGURE 4.11. Absolute error (in solid line) versus the error estimated by (4.73)
(dashed line). The number of iterations is indicated on the x-axis

174
4. Iterative Methods for Solving Linear Systems
4.6.2
A Stopping Test Based on the Residual
A diﬀerent stopping criterion consists of continuing the iteration until
∥r(k)∥≤ε, ε being a ﬁxed tolerance. Note that
∥x −x(k)∥= ∥A−1b −x(k)∥= ∥A−1r(k)∥≤∥A−1∥ε.
Considering instead a normalized residual, i.e. stopping the iteration as
soon as ∥r(k)∥/∥b∥≤ε, we obtain the following control on the relative
error
∥x −x(k)∥
∥x∥
≤∥A−1∥∥r(k)∥
∥x∥
≤K(A)∥∥r(k)∥
∥b∥
≤εK(A).
In the case of preconditioned methods, the residual is replaced by the pre-
conditioned residual, so that the previous criterion becomes
∥P−1r(k)∥
∥P−1r(0)∥≤ε,
where P is the preconditioning matrix.
4.7
Applications
In this section we consider two examples arising in electrical network anal-
ysis and structural mechanics which lead to the solution of large sparse
linear systems.
4.7.1
Analysis of an Electric Network
We consider a purely resistive electric network (shown in Figure 4.12, left)
which consists of a connection of n stages S (Figure 4.12, right) through
the series resistances R. The circuit is completed by the driving current
generator I0 and the load resistance RL. As an example, a purely resistive
network is a model of a signal attenuator for low-frequency applications
where capacitive and inductive eﬀects can be neglected. The connecting
points between the electrical components will be referred to henceforth as
nodes and are progressively labeled as drawn in the ﬁgure. For n ≥1, the
total number of nodes is 4n. Each node is associated with a value of the
electric potential Vi, i = 0, . . . , 4n, which are the unknowns of the problem.
The nodal analysis method is employed to solve the problem. Precisely,
the Kirchhoﬀcurrent law is written at any node of the network leading
to the linear system ˜Y ˜V = ˜I, where ˜V ∈RN+1 is the vector of nodal
potentials, ˜I ∈RN+1 is the load vector and the entries of the matrix ˜Y ∈

4.7 Applications
175
R
I
S
S
R
RL
2
4
1
3
5
n
n-1
6
R
R
R
R
R
R
1
2
3
4
5
FIGURE 4.12. Resistive electric network (left) and resistive stage S (right)
R(N+1)×(N+1), for i, j = 0, . . . , 4n, are given by
˜Yij =






j∈k(i)
Gij,
for i = j,
−Gij,
for i ̸= j,
where k(i) is the index set of the neighboring nodes of node i and Gij =
1/Rij is the admittance between node i and node j, provided Rij denotes
the resistance between the two nodes i and j. Since the potential is deﬁned
up to an additive constant, we arbitrarily set V0 = 0 (ground potential). As
a consequence, the number of independent nodes for potential diﬀerence
computations is N = 4n −1 and the linear system to be solved becomes
YV = I, where Y ∈RN×N, V ∈RN and I ∈RN are obtained eliminating
the ﬁrst row and column in ˜Y and the ﬁrst entry in ˜V and ˜I, respectively.
The matrix Y is symmetric, diagonally dominant and positive deﬁnite. This
last property follows by noting that
˜VT ˜Y ˜V =
N

i=1
GiiV 2
i +
N

i,j=1
Gij(Vi −Vj)2,
which is always a positive quantity, being equal to zero only if ˜V = 0. The
sparsity pattern of Y in the case n = 3 is shown in Figure 4.13 (left) while
the spectral condition number of Y as a function of the number of blocks n
is reported in Figure 4.13 (right). Our numerical computations have been
carried out setting the resistance values equal to 1 Ωwhile I0 = 1 A.
In Figure 4.14 we report the convergence history of several non precondi-
tioned iterative methods in the case n = 5 corresponding to a matrix size of
19 × 19. The plots show the Euclidean norms of the residual normalized to
the initial residual. The dashed curve refers to the Gauss-Seidel method, the
dash-dotted line refers to the gradient method, while the solid and circled
lines refer respectively to the conjugate gradient (CG) and SOR method
(with an optimal value of the relaxation parameter ω ≃1.76 computed
according to (4.19) since Y is block tridiagonal symmetric positive deﬁ-
nite). The SOR method converges in 109 iterations, while the CG method
converges in 10 iterations.
We have also considered the solution of the system at hand by the conju-
gate gradient (CG) method using the Cholesky version of the ILU(0) and

176
4. Iterative Methods for Solving Linear Systems
0
2
4
6
8
10
12
0
2
4
6
8
10
12
0
5
10
15
20
25
30
35
40
45
50
100
101
102
103
104
105
FIGURE 4.13. Sparsity pattern of Y for n = 3 (left) and spectral condition
number of Y as a function of n (right)
0
50
100
150
200
250
10
−16
10
−14
10
−12
10
−10
10
−8
10
−6
10
−4
10
−2
10
0
10
2
FIGURE 4.14. Convergence history of several non preconditioned iterative meth-
ods
MILU(0) preconditioners, where drop tolerances equal to ε = 10−2, 10−3
have been chosen for the MILU(0) preconditioner (see Section 4.3.2). Cal-
culations with both preconditioners have been done using the MATLAB
functions cholinc and michol. Table 4.2 shows the convergence iterations
of the method for n = 5, 10, 20, 40, 80, 160 and for the considered values of
ε. We report in the second column the number of nonzero entries in the
Cholesky factor of matrix Y, in the third column the number of iterations
for the CG method without preconditioning to converge, while the columns
ICh(0) and MICh(0) with ε = 10−2 and ε = 10−3 show the same infor-
mation for the CG method using the incomplete Cholesky and modiﬁed
incomplete Cholesky preconditioners, respectively.
The entries in the table are the number of iterations to converge and the
number in the brackets are the nonzero entries of the L-factor of the cor-
responding preconditioners. Notice the decrease of the iterations as ε de-
creases, as expected. Notice also the increase of the number of iterations
with respect to the increase of the size of the problem.

4.7 Applications
177
n
nz
CG
ICh(0)
MICh(0) ε = 10−2
MICh(0) ε = 10−3
5
114
10
9 (54)
6 (78)
4 (98)
10
429
20
15 (114)
7 (173)
5 (233)
20
1659
40
23 (234)
10 (363)
6 (503)
40
6519
80
36 (474)
14 (743)
7 (1043)
80
25839
160
62 (954)
21 (1503)
10 (2123)
160
102879
320
110 (1914)
34 (3023)
14 (4283)
TABLE 4.2. Convergence iterations for the preconditioned CG method
4.7.2
Finite Diﬀerence Analysis of Beam Bending
Consider the beam clamped at the endpoints that is drawn in Figure 4.15
(left). The structure, of length L, is subject to a distributed load P, varying
along the free coordinate x and expressed in [Kgm−1]. We assume hence-
forth that the beam has uniform rectangular section, of width r and depth
s, momentum of inertia J = rs3/12 and Young’s module E, expressed in
[m4] and [Kg m−2], respectively.
P(x)
                                                















                                                















u(x)
x
0
20
40
60
80
100
120
10
−20
10
−15
10
−10
10
−5
10
0
10
5
n=10
n=60
n=110
FIGURE 4.15. Clamped beam (left); convergence histories for the preconditioned
conjugate gradient method in the solution of system (4.76) (right)
The transverse bending of the beam, under the assumption of small dis-
placements, is governed by the following fourth-order diﬀerential equation
(EJu′′)′′(x) = P(x),
0 < x < L,
(4.74)
where u = u(x) denotes the vertical displacement. The following boundary
conditions (at the endpoints x = 0 and x = L)
u(0) = u(L) = 0,
u′(0) = u′(L) = 0,
(4.75)
model the eﬀect of the two clampings (vanishing displacements and rota-
tions). To solve numerically the boundary-value problem (4.74)-(4.75), we
use the ﬁnite diﬀerence method (see Section 10.10.1 and Exercise 11 of
Chapter 12).

178
4. Iterative Methods for Solving Linear Systems
With this aim, let us introduce the discretization nodes xj = jh, with
h = L/Nh and j = 0, . . . , Nh, and substitute at each node xj the fourth-
order derivative with an approximation through centered ﬁnite diﬀerences.
Letting f(x) = P(x)/(EJ), fj = f(xj) and denoting by ηj the (approx-
imate) nodal displacement of the beam at node xj, the ﬁnite diﬀerence
discretization of (4.74)-(4.75) is
"
ηj−2 −4ηj−1 + 6ηj −4ηj+1 + ηj+2 = h4fj, ∀j = 2, . . . , Nh −2,
η0 = η1 = ηNh−1 = ηNh = 0.
(4.76)
The null displacement boundary conditions in (4.76) that have been im-
posed at the ﬁrst and the last two nodes of the grid, require that Nh ≥4.
Notice that a fourth-order scheme has been used to approximate the fourth-
order derivative, while, for sake of simplicity, a ﬁrst-order approximation
has been employed to deal with the boundary conditions (see Section
10.10.1).
The Nh −3 discrete equations (4.76) yield a linear system of the form
Ax = b where the unknown vector x ∈RNh−3 and the load vector
b ∈RNh−3 are given respectively by x = (η2, η3, . . . , ηNh−2)T and b =
(f2, f3, . . . , fNh−2)T , while the coeﬃcient matrix A ∈R(Nh−3)×(Nh−3) is
pentadiagonal and symmetric, given by A = pentadiagNh−3(1, −4, 6, −4, 1).
The matrix A is symmetric and positive deﬁnite. Therefore, to solve
system Ax = b, the SSOR preconditioned conjugated gradient method (see
Section 4.21) and the Cholesky factorization method have been employed.
In the remainder of the section, the two methods are identiﬁed by the
symbols (CG) and (CH).
The convergence histories of CG are reported in Figure 4.15 (right),
where the sequences ∥r(k)∥2/∥b(k)∥2, for the values n = 10, 60, 110, are
plotted, r(k) = b −Ax(k) being the residual at the k-th step. The results
have been obtained using Program 20, with toll=10−15 and ω = 1.8 in
(4.22). The initial vector x(0) has been set equal to the null vector.
As a comment to the graphs, it is worth noting that CG has required 7, 33
and 64 iterations to converge, respectively, with a maximum absolute error
of 5·10−15 with respect to the solution produced by CH. This latter has an
overall computational cost of 136, 1286 and 2436 ﬂops respectively, to be
compared with the corresponding 3117, 149424 and 541647 ﬂops of method
CG. As for the performances of the SSOR preconditioner, we remark that
the spectral condition number of matrix A is equal to 192, 3.8 · 105 and
4.5 · 106, respectively, while the corresponding values in the preconditioned
case are 65, 1.2 · 104 and 1.3 · 105.

4.8 Exercises
179
4.8
Exercises
1. The spectral radius of the matrix
B =
 a
4
0
a

is ρ(B) = a. Check that if 0 < a < 1, then ρ(B) < 1, while ∥Bm∥1/m
2
can
be greater than 1.
2. Let A ∈Rn×n be a strictly diagonally dominant matrix by rows. Show
that the Gauss-Seidel method for the solution of the linear system (3.2) is
convergent.
3. Check that the matrix A = tridiag(−1, α, −1), with α ∈R, has eigenvalues
given by
λj = α −2 cos(jθ),
j = 1, . . . , n
where θ = π/(n + 1) and the corresponding eigenvectors are
qj = [sin(jθ), sin(2jθ), . . . , sin(njθ)]T .
Under which conditions on α is the matrix positive deﬁnite?
[Solution : α ≥2.]
4. Consider the pentadiagonal matrix A = pentadiagn(−1, −1, 10, −1, −1).
Assume n = 10 and A = M + N + D, with D = diag(8, . . . , 8) ∈R10×10,
M = pentadiag10(−1, −1, 1, 0, 0) and N = MT . To solve Ax = b, analyze
the convergence of the following iterative methods
(a)
(M + D)x(k+1) = −Nx(k) + b,
(b)
Dx(k+1) = −(M + N)x(k) + b,
(c)
(M + N)x(k+1) = −Dx(k) + b.
[Solution : denoting respectively by ρa, ρb and ρc the spectral radii of the
iteration matrices of the three methods, we have ρa = 0.1450, ρb = 0.5
and ρc = 12.2870 which implies convergence for methods (a) and (b) and
divergence for method (c).]
5. For the solution of the linear system Ax = b with
A =
 1
2
2
3

,
b =
 3
5

,
consider the following iterative method
x(k+1) = B(θ)x(k) + g(θ),
k ≥0,
with x(0) given,
where θ is a real parameter and
B(θ) = 1
4

2θ2 + 2θ + 1
−2θ2 + 2θ + 1
−2θ2 + 2θ + 1
2θ2 + 2θ + 1

,
g(θ) =

1
2 −θ
1
2 −θ

.

180
4. Iterative Methods for Solving Linear Systems
Check that the method is consistent ∀θ ∈R. Then, determine the values
of θ for which the method is convergent and compute the optimal value
of θ (i.e., the value of the parameter for which the convergence rate is
maximum).
[Solution : the method is convergent iﬀ−1 < θ < 1/2 and the convergence
rate is maximum if θ = (1 −
√
3)/2.]
6. To solve the following block linear system
 A1
B
B
A2
  x
y

=
 b1
b2

,
consider the two methods
(1)
A1x(k+1) + By(k) = b1,
Bx(k) + A2y(k+1) = b2;
(2)
A1x(k+1) + By(k) = b1,
Bx(k+1) + A2y(k+1) = b2.
Find suﬃcient conditions in order for the two schemes to be convergent for
any choice of the initial data x(0), y(0).
[Solution : method (1) is a decoupled system in the unknowns x(k+1) and
y(k+1). Assuming that A1 and A2 are invertible, method (1) converges if
ρ(A−1
1 B) < 1 and ρ(A−1
2 B) < 1. In the case of method (2) we have a coupled
system to solve at each step in the unknowns x(k+1) and y(k+1). Solving
formally the ﬁrst equation with respect to x(k+1) (which requires A1 to be
invertible) and substituting into the second one we see that method (2) is
convergent if ρ(A−1
2 BA−1
1 B) < 1 (again A2 must be invertible).]
7. Consider the linear system Ax = b with
A =


62
24
1
8
15
23
50
7
14
16
4
6
58
20
22
10
12
19
66
3
11
18
25
2
54


,
b =


110
110
110
110
110


.
(1) Check if the Jacobi and Gauss-Seidel methods can be applied to solve
the system. (2) Check if the stationary Richardson method with optimal
parameter can be applied with P = I and P = D, where D is the diagonal
part of A, and compute the corresponding values of αopt and ρopt.
[Solution : (1): matrix A is neither diagonally dominant nor symmetric
positive deﬁnite, so that we must compute the spectral radii of the itera-
tion matrices of the Jacobi and Gauss-Seidel methods to verify if they are
convergent. It turns out that ρJ = 0.9280 and ρGS = 0.3066 which implies
convergence for both methods. (2): in the case P = I all the eigenvalues
of A are positive so that the Richardson method can be applied yielding
αopt = 0.015 and ρopt = 0.6452. If P = D the method is still applicable
and αopt = 0.8510, ρopt = 0.6407.]
8. Consider the linear system Ax = b with
A =


5
7
6
5
7
10
8
7
6
8
10
9
5
7
9
10

,
b =


23
32
33
31

.

4.8 Exercises
181
Analyze the convergence properties of the Jacobi and Gauss-Seidel methods
applied to the system above in their point and block forms (for a 2×2 block
partition of A).
[Solution : both methods are convergent, the block form being the faster
one. Moreover, ρ2(BJ) = ρ(BGS).]
9. To solve the linear system Ax = b, consider the iterative method (4.6),
with P = D + ωF and N = −βF −E, ω and β being real numbers. Check
that the method is consistent only if β = 1 −ω. In such a case, express
the eigenvalues of the iteration matrix as a function of ω and determine for
which values of ω the method is convergent, as well as the value of ωopt,
assuming that A = tridiag10(−1, 2, −1).
[Hint : Take advantage of the result in Exercise 3.]
10. Let A ∈Rn×n be such that A = (1+ω)P−(N+ωP), with P−1N nonsingular
and with real eigenvalues 1 > λ1 ≥λ2 ≥. . . ≥λn. Find the values of ω ∈R
for which the following iterative method
(1 + ω)Px(k+1) = (N + ωP)x(k) + b,
k ≥0,
converges ∀x(0) to the solution of the linear system (3.2). Determine also
the value of ω for which the convergence rate is maximum.
[Solution : ω > −(1 + λn)/2; ωopt = −(λ1 + λn)/2.]
11. Consider the linear system
Ax = b
with A =
 3
2
2
6

,
b =

2
−8

.
Write the associated functional Φ(x) and give a graphical interpretation of
the solution of the linear system. Perform some iterations of the gradient
method, after proving convergence for it.
12. Check that in the gradient method x(k+2) is not an optimal direction with
respect to r(k).
13. Show that the coeﬃcients αk and βk in the conjugate gradient method can
be written in the alternative form (4.45).
[Solution: notice Ap(k) = (r(k) −r(k+1))/αk and thus (Ap(k))T r(k+1) =
−∥r(k+1)∥2
2/αk. Moreover, αk(Ap(k))T p(k) = −∥r(k)∥2
2.]
14. Prove the three-terms recursive relation (4.46) for the residual in the con-
jugate gradient method.
[Solution: subtract from both sides of Ap(k) = (r(k) −r(k+1))/αk the quan-
tity βk−1/αkr(k) and recall that Ap(k) = Ar(k) −βk−1Ap(k−1). Then, ex-
pressing the residual r(k) as a function of r(k−1) one immediately gets the
desired relation.]

5
Approximation of Eigenvalues and
Eigenvectors
In this chapter we deal with approximations of the eigenvalues and eigen-
vectors of a matrix A ∈Cn×n. Two main classes of numerical methods
exist to this purpose, partial methods, which compute the extremal eigen-
values of A (that is, those having maximum and minimum module), or
global methods, which approximate the whole spectrum of A.
It is worth noting that methods which are introduced to solve the matrix
eigenvalue problem are not necessarily suitable for calculating the matrix
eigenvectors. For example, the power method (a partial method, see Section
5.3) provides an approximation to a particular eigenvalue/eigenvector pair.
The QR method (a global method, see Section 5.5) instead computes the
real Schur form of A, a canonical form that displays all the eigenvalues of
A but not its eigenvectors. These eigenvectors can be computed, starting
from the real Schur form of A, with an extra amount of work, as described
in Section 5.8.2.
Finally, some ad hoc methods for dealing eﬀectively with the special case
where A is a symmetric (n × n) matrix are considered in Section 5.10.
5.1
Geometrical Location of the Eigenvalues
Since the eigenvalues of A are the roots of the characteristic polynomial
pA(λ) (see Section 1.7), iterative methods must be used for their approxi-
mation when n ≥5. Knowledge of eigenvalue location in the complex plane
can thus be helpful in accelerating the convergence of the process.

184
5. Approximation of Eigenvalues and Eigenvectors
A ﬁrst estimate is provided by Theorem 1.4,
|λ| ≤∥A∥,
∀λ ∈σ(A),
(5.1)
for any consistent matrix norm ∥· ∥. Inequality (5.1), which is often quite
rough, states that all the eigenvalues of A are contained in a circle of radius
R∥A∥= ∥A∥centered at the origin of the Gauss plane.
Another result is obtained by extending the Decomposition Property 1.23
to complex-valued matrices.
Theorem 5.1 If A ∈Cn×n, let
H =

A + AH
/2
and
iS =

A −AH
/2
be the hermitian and skew-hermitian parts of A, respectively, i being the
imaginary unit. For any λ ∈σ(A)
λmin(H) ≤Re(λ) ≤λmax(H),
λmin(S) ≤Im(λ) ≤λmax(S).
(5.2)
Proof. From the deﬁnition of H and S it follows that A = H + iS. Let u ∈Cn,
∥u∥2 = 1, be the eigenvector associated with the eigenvalue λ; the Rayleigh
quotient (introduced in Section 1.7) reads
λ = uHAu = uHHu + iuHSu.
(5.3)
Notice that both H and S are hermitian matrices, whilst iS is skew-hermitian.
Matrices H and S are thus unitarily similar to a real diagonal matrix (see Section
1.7), and therefore their eigenvalues are real. In such a case, (5.3) yields
Re(λ) = uHHu,
Im(λ) = uHSu,
from which (5.2) follows.
3
An a priori bound for the eigenvalues of A is given by the following result.
Theorem 5.2 (of the Gershgorin circles) Let A ∈Cn×n. Then
σ(A) ⊆SR =
n
4
i=1
Ri,
Ri = {z ∈C : |z −aii| ≤
n

j=1
j̸=i
|aij|}.
(5.4)
The sets Ri are called Gershgorin circles.
Proof. Let us decompose A as A = D + E, where D is the diagonal part of
A, whilst eii = 0 for i = 1, . . . , n. For λ ∈σ(A) (with λ ̸= aii, i = 1, . . . , n),
let us introduce the matrix Bλ = A −λI = (D −λI) + E. Since Bλ is singular,
there exists a non-null vector x ∈Cn such that Bλx = 0. This means that
((D −λI) + E) x = 0, that is, passing to the ∥· ∥∞norm,
x = −(D −λI)−1Ex,
∥x∥∞≤∥(D −λI)−1E∥∞∥x∥∞,

5.1 Geometrical Location of the Eigenvalues
185
and thus
1 ≤∥(D −λI)−1E∥∞=
n

j=1
|ekj|
|akk −λ| =
n

j=1
j̸=k
|akj|
|akk −λ|
(5.5)
for a certain k, 1 ≤k ≤n. Inequality (5.5) implies λ ∈Rk and thus (5.4).
3
The bounds (5.4) ensure that any eigenvalue of A lies within the union
of the circles Ri. Moreover, since A and AT share the same spectrum,
Theorem 5.2 also holds in the form
σ(A) ⊆SC =
n
4
j=1
Cj,
Cj = {z ∈C : |z −ajj| ≤
n

i=1
i̸=j
|aij|}.
(5.6)
The circles Ri in the complex plane are called row circles, and Cj column
circles. The immediate consequence of (5.4) and (5.6) is the following.
Property 5.1 (First Gershgorin theorem) For a given matrix A ∈
Cn×n,
∀λ ∈σ(A),
λ ∈SR
5
SC.
(5.7)
The following two location theorems can also be proved (see [Atk89], pp.
588-590 and [Hou75], pp. 66-67).
Property 5.2 (Second Gershgorin theorem) Let
S1 =
m
4
i=1
Ri,
S2 =
n
4
i=m+1
Ri.
If S1 ∩S2 = ∅, then S1 contains exactly m eigenvalues of A, each one being
accounted for with its algebraic multiplicity, while the remaining eigenvalues
are contained in S2.
Remark 5.1 Properties 5.1 and 5.2 do not exclude the possibility that
there exist circles containing no eigenvalues, as happens for the matrix in
Exercise 1.
■
Deﬁnition 5.1 A matrix A ∈Cn×n is called reducible if there exists a
permutation matrix P such that
PAPT =
.
B11
B12
0
B22
/
,
where B11 and B22 are square matrices; A is irreducible if it is not reducible.
■

186
5. Approximation of Eigenvalues and Eigenvectors
To check if a matrix is reducible, the oriented graph of the matrix can be
conveniently employed. Recall from Section 3.9 that the oriented graph of a
real matrix A is obtained by joining n points (called vertices of the graph)
P1, . . . , Pn through a line oriented from Pi to Pj if the corresponding
matrix entry aij ̸= 0. An oriented graph is strongly connected if for any
pair of distinct vertices Pi and Pj there exists an oriented path from Pi to
Pj. The following result holds (see [Var62] for the proof).
Property 5.3 A matrix A ∈Rn×n is irreducible iﬀits oriented graph is
strongly connected.
Property 5.4 (Third Gershgorin theorem) Let A ∈Cn×n be an irre-
ducible matrix. An eigenvalue λ ∈σ(A) cannot lie on the boundary of SR
unless it belongs to the boundary of every circle Ri, for i = 1, . . . , n.
Example 5.1 Let us consider the matrix
A =


10
2
3
−1
2
−1
0
1
3


whose spectrum is (to four signiﬁcant ﬁgures) σ(A) = {9.687, 2.656±i0.693}. The
following values of the norm of A: ∥A∥1 = 11, ∥A∥2 = 10.72, ∥A∥∞= 15 and
∥A∥F = 11.36 can be used in the estimate (5.1). Estimate (5.2) provides instead
1.96 ≤Re(λ(A)) ≤10.34, −2.34 ≤Im(λ(A)) ≤2.34, while the row and column
circles are given respectively by R1 = {|z| : |z−10| ≤5}, R2 = {|z| : |z−2| ≤2},
R3 = {|z| : |z −3| ≤1} and C1 = {|z| : |z −10| ≤1}, C2 = {|z| : |z −2| ≤3},
C3 = {|z| : |z −3| ≤4}.
In Figure 5.1, for i = 1, 2, 3 the Ri and Ci circles and the intersection SR ∩SC
(shaded areas) are drawn. In agreement with Property 5.2, we notice that an
eigenvalue is contained in C1, which is disjoint from C2 and C3, while the remaining
eigenvalues, thanks to Property 5.1, lie within the set R2 ∪{C3 ∩R1}.
•
5.2
Stability and Conditioning Analysis
In this section we introduce some a priori and a posteriori estimates that
are relevant in the stability analysis of the matrix eigenvalue and eigenvec-
tor problem. The presentation follows the guidelines that have been traced
in Chapter 2.
5.2.1
A priori Estimates
Assume that A ∈Cn×n is a diagonalizable matrix and denote by X =
(x1, . . . , xn) ∈Cn×n the matrix of its right eigenvectors, where xk ∈Cn

5.2 Stability and Conditioning Analysis
187
2
Re(z)
3
10
C3
C2
C1
R3
R2
R1
Im(z)
FIGURE 5.1. Row and column circles for matrix A in Example 5.1
for k = 1, . . . , n, such that D = X−1AX = diag(λ1, . . . , λn), λi being the
eigenvalues of A, i = 1, . . . , n. Moreover, let E ∈Cn×n be a perturbation
of A. The following theorem holds.
Theorem 5.3 (Bauer-Fike) Let µ be an eigenvalue of the matrix A+E ∈
Cn×n; then
min
λ∈σ(A) |λ −µ| ≤Kp(X)∥E∥p
(5.8)
where ∥· ∥p is any matrix p-norm and Kp(X) = ∥X∥p∥X−1∥p is called the
condition number of the eigenvalue problem for matrix A.
Proof. We ﬁrst notice that if µ ∈σ(A) then (5.8) is trivially veriﬁed, since
∥X∥p∥X−1∥p∥E∥p ≥0. Let us thus assume henceforth that µ ̸∈σ(A). From the
deﬁnition of eigenvalue it follows that matrix (A+E−µI) is singular, which means
that, since X is invertible, the matrix X−1(A + E −µI)X = D + X−1EX −µI is
singular. Therefore, there exists a non-null vector x ∈Cn such that

(D −µI) + X−1EX

x = 0.
Since µ ̸∈σ(A), the diagonal matrix (D −µI) is invertible and the previous
equation can be written in the form

I + (D −µI)−1(X−1EX)

x = 0.
Passing to the ∥· ∥p norm and proceeding as in the proof of Theorem 5.2, we get
1 ≤∥(D −µI)−1∥pKp(X)∥E∥p,
from which the estimate (5.8) follows, since
∥(D −µI)−1∥p = ( min
λ∈σ(A) |λ −µ|)−1.
3

188
5. Approximation of Eigenvalues and Eigenvectors
If A is a normal matrix, from the Schur decomposition theorem (see Section
1.8) it follows that the similarity transformation matrix X is unitary so that
Kp(X) = 1. This implies that
∀µ ∈σ(A + E),
min
λ∈σ(A) |λ −µ| ≤∥E∥p,
(5.9)
hence the eigenvalue problem is well-conditioned with respect to the abso-
lute error. This, however, does not prevent the matrix eigenvalue problem
from being aﬀected by signiﬁcant relative errors, especially when A has a
widely spread spectrum.
Example 5.2 Let us consider, for 1 ≤n ≤10, the calculation of the eigenvalues
of the Hilbert matrix Hn ∈Rn×n (see Example 3.2, Chapter 3). It is symmetric
(thus, in particular, normal) and exhibits, for n ≥4, a very large condition
number. Let En ∈Rn×n be a matrix having constant entries equal to η = 10−3.
We show in Table 5.1 the results of the computation of the minimum in (5.9),
taking p = 2 (that is, ∥En∥2 = nη). Notice how the absolute error is decreasing,
since the eigenvalue of minimum module tends to zero, whilst the relative error
is increasing as the size n of the matrix increases, due to the higher sensitivity of
“small” eigenvalues with respect to rounding errors.
•
n
Abs. Err.
Rel. Err.
∥En∥2
K2(Hn)
K2(Hn + En)
1
1 · 10−3
1 · 10−3
1 · 10−3
1 · 10−3
1
2
1.677 · 10−4
1.446 · 10−3
2 · 10−3
19.28
19.26
4
5.080 · 10−7
2.207 · 10−3
4 · 10−3
1.551 · 104
1.547 · 104
8
1.156 · 10−12
3.496 · 10−3
8 · 10−3
1.526 · 1010
1.515 · 1010
10
1.355 · 10−15
4.078 · 10−3
1 · 10−2
1.603 · 1013
1.589 · 1013
TABLE 5.1. Relative and absolute errors in the calculation of the eigenvalues of
the Hilbert matrix (using the MATLAB intrinsic function eig). “Abs. Err.” and
“Rel. Err.” denote respectively the absolute and relative errors (with respect to
λ)
The Bauer-Fike theorem states that the matrix eigenvalue problem is well-
conditioned if A is a normal matrix. Failure to fulﬁl this property, however,
does not necessarily imply that A must exhibit a “strong” numerical sen-
sitivity to the computation of every one of its eigenvalues. In this respect,
the following result holds, which can be regarded as an a priori estimate of
the conditioning of the calculation of a particular eigenvalue of a matrix.
Theorem 5.4 Let A ∈Cn×n be a diagonalizable matrix; let λ, x and y
be a simple eigenvalue of A and its associated right and left eigenvectors,
respectively, with ∥x∥2 = ∥y∥2 = 1. Moreover, for ε > 0, let A(ε) =
A + εE, with E ∈Cn×n such that ∥E∥2 = 1. Denoting by λ(ε) and x(ε) the

5.2 Stability and Conditioning Analysis
189
eigenvalue and the corresponding eigenvector of A(ε), such that λ(0) = λ
and x(0) = x,

∂λ
∂ε (0)
 ≤
1
|yHx|.
(5.10)
Proof. Let us ﬁrst prove that yHx ̸= 0. Setting Y = (y1, . . . , yn) = (XH)−1,
with yk ∈Cn for k = 1, . . . , n, it follows that yH
k A = λkyH
k , i.e., the rows
of X−1 = YH are left eigenvectors of A. Then, since YHX = I, yH
i xj = δij
for i, j = 1, . . . , n, δij being the Kronecker symbol. This result is equivalent to
saying that the eigenvectors {x} of A and the eigenvectors {y} of AH form a
bi-orthogonal set (see (4.67)).
Let us now prove (5.10). Since the roots of the characteristic equation are
continuous functions of the coeﬃcients of the characteristic polynomial associated
with A(ε), it follows that the eigenvalues of A(ε) are continuous functions of ε
(see, for instance, [Hen74], p. 281). Therefore, in a neighborhood of ε = 0,
(A + εE)x(ε) = λ(ε)x(ε).
Diﬀerentiating the previous equation with respect to ε and setting ε = 0 yields
A∂x
∂ε (0) + Ex = ∂λ
∂ε (0)x + λ∂x
∂ε (0),
from which, left-multiplying both sides by yH and recalling that yH is a left
eigenvector of A,
∂λ
∂ε (0) = yHEx
yHx .
Using the Cauchy-Schwarz inequality gives the desired estimate (5.10).
3
Notice that |yHx| = | cos(θλ)|, where θλ is the angle between the eigenvec-
tors yH and x (both having unit Euclidean norm). Therefore, if these two
vectors are almost orthogonal the computation of the eigenvalue λ turns
out to be ill-conditioned. The quantity
κ(λ) =
1
|yHx| =
1
| cos(θλ)|
(5.11)
can thus be taken as the condition number of the eigenvalue λ. Obviously,
κ(λ) ≥1; when A is a normal matrix, since it is unitarily similar to a
diagonal matrix, the left and right eigenvectors y and x coincide, yielding
κ(λ) = 1/∥x∥2
2 = 1.
Inequality (5.10) can be roughly interpreted as stating that perturbations
of the order of δε in the entries of matrix A induce changes of the order of
δλ = δε/| cos(θλ)| in the eigenvalue λ. If normal matrices are considered,
the calculation of λ is a well-conditioned problem; the case of a generic non-
symmetric matrix A can be conveniently dealt with using methods based
on similarity transformations, as will be seen in later sections.

190
5. Approximation of Eigenvalues and Eigenvectors
It is interesting to check that the conditioning of the matrix eigenvalue
problem remains unchanged if the transformation matrices are unitary. To
this end, let U ∈Cn×n be a unitary matrix and let $A = UHAU. Also let
λj be an eigenvalue of A and denote by κj the condition number (5.11).
Moreover, let $κj be the condition number of λj when it is regarded as an
eigenvalue of $A. Finally, let {xk}, {yk} be the right and left eigenvectors of
A respectively. Clearly, {UHxk}, {UHyk} are the right and left eigenvectors
of $A. Thus, for any j = 1, . . . , n,
$κj =
yH
j UUHxj
−1 = κj,
from which it follows that the stability of the computation of λj is not
aﬀected by performing similarity transformations using unitary matrices.
It can also be checked that unitary transformation matrices do not change
the Euclidean length and the angles between vectors in Cn. Moreover, the
following a priori estimate holds (see [GL89], p. 317)
fl

X−1AX

= X−1AX + E,
with ∥E∥2 ≃uK2(X)∥A∥2
(5.12)
where fl(M) is the machine representation of matrix M and u is the roundoﬀ
unit (see Section 2.5). From (5.12) it follows that using nonunitary trans-
formation matrices in the eigenvalue computation can lead to an unstable
process with respect to rounding errors.
We conclude this section with a stability result for the approximation of
the eigenvector associated with a simple eigenvalue. Under the same as-
sumptions of Theorem 5.4, the following result holds (see for the proof,
[Atk89], Problem 6, pp. 649-650).
Property 5.5 The eigenvectors xk and xk(ε) of the matrices A and A(ε) =
A + εE, with ∥xk(ε)∥2 = ∥xk∥2 = 1 for k = 1, . . . , n, satisfy
∥xk(ε) −xk∥2 ≤
ε
minj̸=k |λk −λj|∥E∥2 + O(ε2),
∀k = 1, . . . , n.
Analogous to (5.11), the quantity
κ(xk) =
1
minj̸=k |λk −λj|
can be regarded as being the condition number of the eigenvector xk. Com-
puting xk might be an ill-conditioned operation if some eigenvalues λj are
“very close” to the eigenvalue λk associated with xk.
5.2.2
A posteriori Estimates
The a priori estimates examined in the previous section characterize the
stability properties of the matrix eigenvalue and eigenvector problem. From

5.2 Stability and Conditioning Analysis
191
the implementation standpoint, it is also important to dispose of a pos-
teriori estimates that allow for a run-time control of the quality of the
approximation that is being constructed. Since the methods that will be
considered later are iterative processes, the results of this section can be
usefully employed to devise reliable stopping criteria for these latter.
Theorem 5.5 Let A ∈Cn×n be an hermitian matrix and let (λ, x) be
the computed approximations of an eigenvalue/eigenvector pair (λ, x) of
A. Deﬁning the residual as
r = Ax −λx,
x ̸= 0,
it then follows that
min
λi∈σ(A) |λ −λi| ≤∥r∥2
∥x∥2
.
(5.13)
Proof. Since A is hermitian, it admits a system of orthonormal eigenvectors {uk}
which can be taken as a basis of Cn. In particular, x =
n

i=1
αiui with αi = uH
i x,
and thus r =
n

i=1
αi(λi −λ)ui. As a consequence
 ∥r∥2
∥x∥2
2
=
n

i=1
βi(λi −λ)2,
with βi = |αk|2/(
n

j=1
|αj|2).
(5.14)
Since
n

i=1
βi = 1, the inequality (5.13) immediately follows from (5.14).
3
The estimate (5.13) ensures that a small absolute error corresponds to a
small relative residual in the computation of the eigenvalue of the matrix
A which is closest to λ.
Let us now consider the following a posteriori estimate for the eigenvector
x (for the proof, see [IK66], pp. 142-143).
Property 5.6 Under the same assumptions of Theorem 5.5, suppose that
|λi −λ| ≤∥r∥2 for i = 1, . . . , m and that |λi −λ| ≥δ > 0 for i =
m + 1, . . . , n. Then
d(x, Um) ≤∥r∥2
δ
(5.15)
where d(x, Um) is the Euclidean distance between x and the space Um gen-
erated by the eigenvectors ui, i = 1, . . . , m associated with the eigenvalues
λi of A.

192
5. Approximation of Eigenvalues and Eigenvectors
Notice that the a posteriori estimate (5.15) ensures that a small absolute
error corresponds to a small residual in the approximation of the eigenvec-
tor associated with the eigenvalue of A that is closest to λ, provided that
the eigenvalues of A are well-separated (that is, if δ is suﬃciently large).
In the general case of a nonhermitian matrix A, an a posteriori estimate
can be given for the eigenvalue λ only when the matrix of the eigenvectors
of A is available. We have the following result (for the proof, we refer to
[IK66], p. 146).
Property 5.7 Let A ∈Cn×n be a diagonalizable matrix, with matrix of
eigenvectors X = [x1, . . . , xn]. If, for some ε > 0,
∥r∥2 ≤ε∥x∥2,
then
min
λi∈σ(A) |λ −λi| ≤ε∥X−1∥2∥X∥2.
This estimate is of little practical use, since it requires the knowledge of all
the eigenvectors of A. Examples of a posteriori estimates that can actually
be implemented in a numerical algorithm will be provided in Sections 5.3.1
and 5.3.2.
5.3
The Power Method
The power method is very good at approximating the extremal eigenvalues
of the matrix, that is, the eigenvalues having largest and smallest module,
denoted by λ1 and λn respectively, as well as their associated eigenvectors.
Solving such a problem is of great interest in several real-life applications
(geosysmic, machine and structural vibrations, electric network analysis,
quantum mechanics, . . . ) where the computation of λn (and its associated
eigenvector xn) arises in the determination of the proper frequency (and
the corresponding fundamental mode) of a given physical system. We shall
come back to this point in Section 5.12.
Having approximations of λ1 and λn can also be useful in the analysis of
numerical methods. For instance, if A is symmetric and positive deﬁnite,
one can compute the optimal value of the acceleration parameter of the
Richardson method and estimate its error reducing factor (see Chapter
4), as well as perform the stability analysis of discretization methods for
systems of ordinary diﬀerential equations (see Chapter 11).
5.3.1
Approximation of the Eigenvalue of Largest Module
Let A ∈Cn×n be a diagonalizable matrix and let X ∈Cn×n be the matrix of
its eigenvectors xi, for i = 1, . . . , n. Let us also suppose that the eigenvalues

5.3 The Power Method
193
of A are ordered as
|λ1| > |λ2| ≥|λ3| . . . ≥|λn|,
(5.16)
where λ1 has algebraic multiplicity equal to 1. Under these assumptions,
λ1 is called the dominant eigenvalue of matrix A.
Given an arbitrary initial vector q(0) ∈Cn of unit Euclidean norm, consider
for k = 1, 2, . . . the following iteration based on the computation of powers
of matrices, commonly known as the power method:
z(k) = Aq(k−1)
q(k) = z(k)/∥z(k)∥2
ν(k) = (q(k))HAq(k).
(5.17)
Let us analyze the convergence properties of method (5.17). By induction
on k one can check that
q(k) =
Akq(0)
∥Akq(0)∥2
,
k ≥1.
(5.18)
This relation explains the role played by the powers of A in the method.
Because A is diagonalizable, its eigenvectors xi form a basis of Cn; it is
thus possible to represent q(0) as
q(0) =
n

i=1
αixi,
αi ∈C,
i = 1, . . . , n.
(5.19)
Moreover, since Axi = λixi, we have
Akq(0) = α1λk
1

x1 +
n

i=2
αi
α1
 λi
λ1
k
xi

, k = 1, 2, . . .
(5.20)
Since |λi/λ1| < 1 for i = 2, . . . , n, as k increases the vector Akq(0) (and
thus also q(k), due to (5.18)), tends to assume an increasingly signiﬁcant
component in the direction of the eigenvector x1, while its components in
the other directions xj decrease. Using (5.18) and (5.20), we get
q(k) =
α1λk
1(x1 + y(k))
∥α1λk
1(x1 + y(k))∥2
= µk
x1 + y(k)
∥x1 + y(k)∥2
,
where µk is the sign of α1λk
1 and y(k) denotes a vector that vanishes as
k →∞.
As k →∞, the vector q(k) thus aligns itself along the direction of eigen-
vector x1, and the following error estimate holds at each step k.

194
5. Approximation of Eigenvalues and Eigenvectors
Theorem 5.6 Let A ∈Cn×n be a diagonalizable matrix whose eigenvalues
satisfy (5.16). Assuming that α1 ̸= 0, there exists a constant C > 0 such
that
∥˜q(k) −x1∥2 ≤C

λ2
λ1

k
,
k ≥1,
(5.21)
where
˜q(k) = q(k)∥Akq(0)∥2
α1λk
1
= x1 +
n

i=2
αi
α1
 λi
λ1
k
xi,
k = 1, 2, . . .
(5.22)
Proof. Since A is diagonalizable, without losing generality, we can pick up the
nonsingular matrix X in such a way that its columns have unit Euclidean length,
that is ∥xi∥2 = 1 for i = 1, . . . , n. From (5.20) it thus follows that
∥x1 +
n

i=2
.
αi
α1
 λi
λ1
k
xi
/
−x1∥2 = ∥
n

i=2
αi
α1
 λi
λ1
k
xi∥2
≤
 n

i=2
 αi
α1
2  λi
λ1
2k1/2
≤

λ2
λ1

k  n

i=2
 αi
α1
21/2
,
that is (5.21) with C =
 n

i=2
(αi/α1)2
1/2
.
3
Estimate (5.21) expresses the convergence of the sequence ˜q(k) towards x1.
Therefore the sequence of Rayleigh quotients
((˜q(k))HA˜q(k))/∥˜q(k)∥2
2 =
+
q(k),H
Aq(k) = ν(k)
will converge to λ1. As a consequence, limk→∞ν(k) = λ1, and the conver-
gence will be faster when the ratio |λ2/λ1| is smaller.
If the matrix A is real and symmetric it can be proved, always assuming
that α1 ̸= 0, that (see [GL89], pp. 406-407)
|λ1 −ν(k)| ≤|λ1 −λn| tan2(θ0)

λ2
λ1

2k
,
(5.23)
where cos(θ0) = |xT
1 q(0)| ̸= 0. Inequality (5.23) outlines that the conver-
gence of the sequence ν(k) to λ1 is quadratic with respect to the ratio |λ2/λ1|
(we refer to Section 5.3.3 for numerical results).
We conclude the section by providing a stopping criterion for the iteration
(5.17). For this purpose, let us introduce the residual at step k
r(k) = Aq(k) −ν(k)q(k),
k ≥1,
and, for ε > 0, the matrix εE(k) = −r(k) 0
q(k)1H ∈Cn×n with ∥E(k)∥2 = 1.
Since
εE(k)q(k) = −r(k),
k ≥1,
(5.24)

5.3 The Power Method
195
we obtain

A + εE(k)
q(k) = ν(k)q(k). As a result, at each step of the
power method ν(k) is an eigenvalue of the perturbed matrix A + εE(k).
From (5.24) and from deﬁnition (1.20) it also follows that ε = ∥r(k)∥2 for
k = 1, 2, . . . . Plugging this identity back into (5.10) and approximating the
partial derivative in (5.10) by the incremental ratio |λ1 −ν(k)|/ε, we get
|λ1 −ν(k)| ≃∥r(k)∥2
| cos(θλ)|,
k ≥1,
(5.25)
where θλ is the angle between the right and the left eigenvectors, x1 and y1,
associated with λ1. Notice that, if A is an hermitian matrix, then cos(θλ) =
1, so that (5.25) yields an estimate which is analogue to (5.13).
In practice, in order to employ the estimate (5.25) it is necessary at each
step k to replace | cos(θλ)| with the module of the scalar product between
two approximations q(k) and w(k) of x1 and y1, computed by the power
method. The following a posteriori estimate is thus obtained
|λ1 −ν(k)| ≃
∥r(k)∥2
|(w(k))Hq(k)|,
k ≥1.
(5.26)
Examples of applications of (5.26) will be provided in Section 5.3.3.
5.3.2
Inverse Iteration
In this section we look for an approximation of the eigenvalue of a matrix
A ∈Cn×n which is closest to a given number µ ∈C, where µ ̸∈σ(A).
For this, the power iteration (5.17) can be applied to the matrix (Mµ)−1 =
(A −µI)−1, yielding the so-called inverse iteration or inverse power method.
The number µ is called a shift.
The eigenvalues of M−1
µ
are ξi = (λi −µ)−1; let us assume that there
exists an integer m such that
|λm −µ| < |λi −µ|,
∀i = 1, . . . , n
and i ̸= m.
(5.27)
This amounts to requiring that the eigenvalue λm which is closest to µ has
multiplicity equal to 1. Moreover, (5.27) shows that ξm is the eigenvalue of
M−1
µ
with largest module; in particular, if µ = 0, λm turns out to be the
eigenvalue of A with smallest module.
Given an arbitrary initial vector q(0) ∈Cn of unit Euclidean norm, for
k = 1, 2, . . . the following sequence is constructed:
(A −µI) z(k) = q(k−1)
q(k) = z(k)/∥z(k)∥2
σ(k) = (q(k))HAq(k).
(5.28)

196
5. Approximation of Eigenvalues and Eigenvectors
Notice that the eigenvectors of Mµ are the same as those of A since Mµ =
X (Λ −µIn) X−1, where Λ = diag(λ1, . . . , λn). For this reason, the Rayleigh
quotient in (5.28) is computed directly on the matrix A (and not on M−1
µ ).
The main diﬀerence with respect to (5.17) is that at each step k a linear
system with coeﬃcient matrix Mµ = A −µI must be solved. For numerical
convenience, the LU factorization of Mµ is computed once for all at k = 1,
so that at each step only two triangular systems are to be solved, with a
cost of the order of n2 ﬂops.
Although being more computationally expensive than the power method
(5.17), the inverse iteration has the advantage that it can converge to any
desired eigenvalue of A (namely, the one closest to the shift µ). Inverse iter-
ation is thus ideally suited for reﬁning an initial estimate µ of an eigenvalue
of A, which can be obtained, for instance, by applying the localization tech-
niques introduced in Section 5.1. Inverse iteration can be also eﬀectively
employed to compute the eigenvector associated with a given (approximate)
eigenvalue, as described in Section 5.8.1.
In view of the convergence analysis of the iteration (5.28) we assume that
A is diagonalizable, so that q(0) can be represented in the form (5.19).
Proceeding in the same way as in the power method, we let
˜q(k) = xm +
n

i=1,i̸=m
αi
αm
 ξi
ξm
k
xi,
where xi are the eigenvectors of M−1
µ
(and thus also of A), while αi are as
in (5.19). As a consequence, recalling the deﬁnition of ξi and using (5.27),
we get
lim
k→∞˜q(k) = xm,
lim
k→∞σ(k) = λm.
Convergence will be faster when µ is closer to λm. Under the same assump-
tions made for proving (5.26), the following a posteriori estimate can be
obtained for the approximation error on λm
|λm −σ(k)| ≃
∥r(k)∥2
|(w(k))Hq(k)|,
k ≥1,
(5.29)
where r(k) = Aq(k) −σ(k)q(k) and w(k) is the k-th iterate of the inverse
power method to approximate the left eigenvector associated with λm.
5.3.3
Implementation Issues
The convergence analysis of Section 5.3.1 shows that the eﬀectiveness of
the power method strongly depends on the dominant eigenvalues being
well-separated (that is, |λ2|/|λ1| ≪1). Let us now analyze the behavior of
iteration (5.17) when two dominant eigenvalues of equal module exist (that
is, |λ2| = |λ1|). Three cases must be distinguished:

5.3 The Power Method
197
1. λ2 = λ1: the two dominant eigenvalues are coincident. The method
is still convergent, since for k suﬃciently large (5.20) yields
Akq(0) ≃λk
1 (α1x1 + α2x2)
which is an eigenvector of A. For k →∞, the sequence ˜q(k) (after
a suitable redeﬁnition) converges to a vector lying in the subspace
spanned by the eigenvectors x1 and x2, while the sequence ν(k) still
converges to λ1.
2. λ2 = −λ1: the two dominant eigenvalues are opposite. In this case
the eigenvalue of largest module can be approximated by applying the
power method to the matrix A2. Indeed, for i = 1, . . . , n, λi(A2) =
[λi(A)]2, so that λ2
1 = λ2
2 and the analysis falls into the previous case,
where the matrix is now A2.
3. λ2 = λ1: the two dominant eigenvalues are complex conjugate. Here,
undamped oscillations arise in the sequence of vectors q(k) and the
power method is not convergent (see [Wil65], Chapter 9, Section 12).
As for the computer implementation of (5.17), it is worth noting that nor-
malizing the vector q(k) to 1 keeps away from overﬂow (when |λ1| > 1) or
underﬂow (when |λ1| < 1) in (5.20). We also point out that the requirement
α1 ̸= 0 (which is a priori impossible to fulﬁl when no information about
the eigenvector x1 is available) is not essential for the actual convergence
of the algorithm.
Indeed, although it can be proved that, working in exact arithmetic, the
sequence (5.17) converges to the pair (λ2, x2) if α1 = 0 (see Exercise 10),
the arising of (unavoidable) rounding errors ensures that in practice the
vector q(k) contains a non-null component also in the direction of x1. This
allows for the eigenvalue λ1 to “show-up” and the power method to quickly
converge to it.
An implementation of the power method is given in Program 26. Here
and in the following algorithm, the convergence check is based on the a
posteriori estimate (5.26).
Here and in the remainder of the chapter, the input data z0, toll and
nmax are the initial vector, the tolerance for the stopping test and the
maximum admissible number of iterations, respectively. In output, the vec-
tors nu1 and err contain the sequences {ν(k)} and {∥r(k)∥2/| cos(θλ)|} (see
(5.26)), whilst x1 and niter are the approximation of the eigenvector x1
and the number of iterations taken by the algorithm to converge, respec-
tively.
Program 26 - powerm : Power method
function [nu1,x1,niter,err]=powerm(A,z0,toll,nmax)

198
5. Approximation of Eigenvalues and Eigenvectors
q=z0/norm(z0); q2=q; err=[]; nu1=[]; res=toll+1; niter=0; z=A*q;
while (res >= toll & niter <= nmax)
q=z/norm(z); z=A*q; lam=q’*z; x1=q;
z2=q2’*A; q2=z2/norm(z2); q2=q2’;
y1=q2; costheta=abs(y1’*x1);
if (costheta >= 5e-2),
niter=niter+1; res=norm(z-lam*q)/costheta;
err=[err; res]; nu1=[nu1; lam];
else
disp(’ Multiple eigenvalue ’); break;
end
end
A coding of the inverse power method is provided in Program 27. The
input parameter mu is the initial approximation of the eigenvalue. In output,
the vectors sigma and err contain the sequences {σ(k)} and

∥r(k)∥2/|(w(k))H
q(k)|

(see (5.29)). The LU factorization (with partial pivoting) of the ma-
trix Mµ is carried out using the MATLAB intrinsic function lu.
Program 27 - invpower : Inverse power method
function [sigma,x,niter,err]=invpower(A,z0,mu,toll,nmax)
n=max(size(A)); M=A-mu*eye(n); [L,U,P]=lu(M);
q=z0/norm(z0); q2=q’; err=[]; sigma=[]; res=toll+1; niter=0;
while (res >= toll & niter <= nmax)
niter=niter+1; b=P*q; y=L\b; z=U\y;
q=z/norm(z); z=A*q; lam=q’*z;
b=q2’; y=U’\b; w=L’\y;
q2=(P’*w)’; q2=q2/norm(q2); costheta=abs(q2*q);
if (costheta >= 5e-2),
res=norm(z-lam*q)/costheta; err=[err; res]; sigma=[sigma; lam];
else,
disp(’ Multiple eigenvalue ’); break;
end
x=q;
end
Example 5.3 The matrix A in (5.30)
A =


15
−2
2
1
10
−3
−2
1
0

,
V =


−0.944
0.393
−0.088
−0.312
0.919
0.309
0.112
0.013
0.947


(5.30)
has the following eigenvalues (to ﬁve signiﬁcant ﬁgures): λ1 = 14.103, λ2 = 10.385
and λ3 = 0.512, while the corresponding eigenvectors are the vector columns of
matrix V.

5.3 The Power Method
199
0
10
20
30
40
50
60
70
80
10
−12
10
−10
10
−8
10
−6
10
−4
10
−2
10
0
0
6
12
18
10
−14
10
−12
10
−10
10
−8
10
−6
10
−4
10
−2
10
0
(S)
(NS)
FIGURE 5.2. Comparison between the a posteriori error estimate and the actual
absolute error for matrix A in (5.30) (left); convergence curves for the power
method applied to matrix A in (5.31) in its symmetric (S) and nonsymmetric
(NS) forms (right)
To approximate the pair (λ1, x1), we have run the Program 26 with initial datum
z(0) = (1, 1, 1)T . After 71 iterations of the power method the absolute errors are
|λ1 −ν(71)| = 7.91 · 10−11 and ∥x1 −x(71)
1
∥∞= 1.42 · 10−11.
In a second run, we have used z(0) = x2 + x3 (notice that with this choice
α1 = 0). After 215 iterations the absolute errors are |λ1 −ν(215)| = 4.26 · 10−14
and ∥x1 −x(215)
1
∥∞= 1.38 · 10−14.
Figure 5.2 (left) shows the reliability of the a posteriori estimate (5.26). The
sequences |λ1 −ν(k)| (solid line) and the corresponding a posteriori estimates
(5.26) (dashed line) are plotted as a function of the number of iterations (in
abscissae). Notice the excellent agreement between the two curves.
The symmetric matrix A in (5.31)
A =


1
3
4
3
1
2
4
2
1

,
T =


8
1
6
3
5
7
4
9
2


(5.31)
has the following spectrum: λ1 = 7.047, λ2 = −3.1879 and λ3 = −0.8868 (to ﬁve
signiﬁcant ﬁgures).
It is interesting to compare the behaviour of the power method when computing
λ1 for the symmetric matrix A and for its similar matrix M = T−1AT, where T
is the nonsingular (and nonorthogonal) matrix in (5.31).
Running Program 26 with z(0) = (1, 1, 1)T , the power method converges to the
eigenvalue λ1 in 18 and 30 iterations, for matrices A and M, respectively. The
sequence of absolute errors |λ1 −ν(k)| is plotted in Figure 5.2 (right) where (S)
and (NS) refer to the computations on A and M, respectively. Notice the rapid
error reduction in the symmetric case, according to the quadratic convergence
properties of the power method (see Section 5.3.1).
We ﬁnally employ the inverse power method (5.28) to compute the eigenvalue
of smallest module λ3 = 0.512 of matrix A in (5.30). Running Program 27 with
q(0) = (1, 1, 1)T /
√
3, the method converges in 9 iterations, with absolute errors
|λ3 −σ(9)| = 1.194 · 10−12 and ∥x3 −x(9)
3 ∥∞= 4.59 · 10−13.
•

200
5. Approximation of Eigenvalues and Eigenvectors
5.4
The QR Iteration
In this section we present some iterative techniques for simultaneously ap-
proximating all the eigenvalues of a given matrix A. The basic idea consists
of reducing A, by means of suitable similarity transformations, into a form
for which the calculation of the eigenvalues is easier than on the starting
matrix.
The problem would be satisfactorily solved if the unitary matrix U of the
Schur decomposition theorem 1.5, such that T = UHAU, T being upper
triangular and with tii = λi(A) for i = 1, . . . , n, could be determined in a
direct way, that is, with a ﬁnite number of operations. Unfortunately, it is
a consequence of Abel’s theorem that, for n ≥5, the matrix U cannot be
computed in an elementary way (see Exercise 8). Thus, our problem can
be solved only resorting to iterative techniques.
The reference algorithm in this context is the QR iteration method, that is
here examined only in the case of real matrices. (For some remarks on the
extension of the algorithms to the complex case, see [GL89], Section 5.2.10
and [Dem97], Section 4.2.1).
Let A ∈Rn×n; given an orthogonal matrix Q(0) ∈Rn×n and letting
T(0) = (Q(0))T AQ(0), for k = 1, 2, . . . , until convergence, the QR iteration
consists of:
determine Q(k), R(k) such that
Q(k)R(k) = T(k−1)
(QR factorization);
then, let
T(k) = R(k)Q(k).
(5.32)
At each step k ≥1, the ﬁrst phase of the iteration is the factorization of
the matrix T(k−1) into the product of an orthogonal matrix Q(k) with an
upper triangular matrix R(k) (see Section 5.6.3). The second phase is a
simple matrix product. Notice that
T(k)
= R(k)Q(k) = (Q(k))T (Q(k)R(k))Q(k) = (Q(k))T T(k−1)Q(k)
= (Q(0)Q(1) . . . Q(k))T A(Q(0)Q(1) . . . Q(k)),
k ≥0,
(5.33)
i.e., every matrix T(k) is orthogonally similar to A. This is particularly
relevant for the stability of the method, since, as shown in Section 5.2, the
conditioning of the matrix eigenvalue problem for T(k) is not worse than it
is for A (see also [GL89], p. 360).
A basic implementation of the QR iteration (5.32), assuming Q(0) = In,
is examined in Section 5.5, while a more computationally eﬃcient version,
starting from T(0) in upper Hessenberg form, is described in detail in Sec-
tion 5.6.

5.5 The Basic QR Iteration
201
If A has real eigenvalues, distinct in module, it will be seen in Section 5.5
that the limit of T(k) is an upper triangular matrix (with the eigenvalues of
A on the main diagonal). However, if A has complex eigenvalues the limit
of T(k) cannot be an upper triangular matrix T. Indeed if it were T would
necessarily have real eigenvalues, although it is similar to A.
Failure to converge to a triangular matrix may also happen in more
general situations, as addressed in Example 5.9.
For this, it is necessary to introduce variants of the QR iteration (5.32),
based on deﬂation and shift techniques (see Section 5.7 and, for a more
detailed discussion of the subject, [GL89], Chapter 7, [Dat95], Chapter 8
and [Dem97], Chapter 4).
These techniques allow for T(k) to converge to an upper quasi-triangular
matrix, known as the real Schur decomposition of A, for which the following
result holds (for the proof we refer to [GL89], pp. 341-342).
Property 5.8 Given a matrix A ∈Rn×n, there exists an orthogonal ma-
trix Q ∈Rn×n such that
QT AQ =


R11
R12
. . .
R1m
0
R22
. . .
R2m
...
...
...
...
0
0
. . .
Rmm


,
(5.34)
where each block Rii is either a real number or a matrix of order 2 having
complex conjugate eigenvalues, and
Q = lim
k→∞
6
Q(0)Q(1) · · · Q(k)7
(5.35)
Q(k) being the orthogonal matrix generated by the k-th factorization step of
the QR iteration (5.32).
The QR iteration can be also employed to compute all the eigenvectors
of a given matrix. For this purpose, we describe in Section 5.8 two possi-
ble approaches, one based on the coupling between (5.32) and the inverse
iteration (5.28), the other working on the real Schur form (5.34).
5.5
The Basic QR Iteration
In the basic version of the QR method, one sets Q(0) = In in such a way
that T(0) = A. At each step k ≥1 the QR factorization of the matrix T(k−1)

202
5. Approximation of Eigenvalues and Eigenvectors
can be carried out using the modiﬁed Gram-Schmidt procedure introduced
in Section 3.4.3, with a cost of the order of 2n3 ﬂops (for a full matrix A).
The following convergence result holds (for the proof, see [GL89], Theorem
7.3.1, or [Wil65], pp. 517-519).
Property 5.9 (Convergence of QR method) Let A ∈Rn×n be a ma-
trix such that
|λ1| > |λ2| > . . . > |λn|.
Then
lim
k→+∞T(k) =


λ1
t12
. . .
t1n
0
λ2
t23
. . .
...
...
...
...
0
0
. . .
λn


.
(5.36)
As for the convergence rate, we have
|t(k)
i,i−1| = O

λi
λi−1

k
,
i = 2, . . . , n,
for k →+∞.
(5.37)
Under the additional assumption that A is symmetric, the sequence {T(k)}
tends to a diagonal matrix.
If the eigenvalues of A, although being distinct, are not well-separated, it
follows from (5.37) that the convergence of T(k) towards a triangular matrix
can be quite slow. With the aim of accelerating it, one can resort to the
so-called shift technique, which will be addressed in Section 5.7.
Remark 5.2 It is always possible to reduce the matrix A into a triangular
form by means of an iterative algorithm employing nonorthogonal similarity
transformations. In such a case, the so-called LR iteration (known also as
Rutishauser method, [Rut58]) can be used, from which the QR method has
actually been derived (see also [Fra61], [Wil65]). The LR iteration is based
on the factorization of the matrix A into the product of two matrices L
and R, respectively unit lower triangular and upper triangular, and on the
(nonorthogonal) similarity transformation
L−1AL = L−1(LR)L = RL.
The rare use of the LR method in practical computations is due to the loss
of accuracy that can arise in the LR factorization because of the increase
in module of the upper diagonal entries of R. This aspect, together with
the details of the implementation of the algorithm and some comparisons
with the QR method, is examined in [Wil65], Chapter 8.
■

5.6 The QR Method for Matrices in Hessenberg Form
203
Example 5.4 We apply the QR method to the symmetric matrix A∈R4×4 such
that aii = 4, for i = 1, . . . , 4, and aij = 4 + i −j for i < j ≤4, whose eigenvalues
are (to three signiﬁcant ﬁgures) λ1 = 11.09, λ2 = 3.41, λ3 = 0.90 and λ4 = 0.59.
After 20 iterations, we get
T(20) =


11.09
6.44 · 10−10
−3.62 · 10−15
9.49 · 10−15
6.47 · 10−10
3.41
1.43 · 10−11
4.60 · 10−16
1.74 · 10−21
1.43 · 10−11
0.90
1.16 · 10−4
2.32 · 10−25
2.68 · 10−15
1.16 · 10−4
0.58


.
Notice the “almost-diagonal” structure of the matrix T(20) and, at the same
time, the eﬀect of rounding errors which slightly alter its expected symmetry.
Good agreement can also be found between the under-diagonal entries and the
estimate (5.37).
•
A computer implementation of the basic QR iteration is given in Program
28. The QR factorization is executed using the modiﬁed Gram-Schmidt
method (Program 8). The input parameter niter denotes the maximum
admissible number of iterations, while the output parameters T, Q and R
are the matrices T, Q and R in (5.32) after niter iterations of the QR
procedure.
Program 28 - basicqr : Basic QR iteration
function [T,Q,R]=basicqr(A,niter)
T=A;
for i=1:niter,
[Q,R]=mod grams(T);
T=R*Q;
end
5.6
The QR Method for Matrices in Hessenberg
Form
The naive implementation of the QR method discussed in the previous
section requires (for a full matrix) a computational eﬀort of the order of
n3 ﬂops per iteration. In this section we illustrate a variant for the QR
iteration, known as Hessenberg-QR iteration, with a greatly reduced com-
putational cost. The idea consists of starting the iteration from a matrix
T(0) in upper Hessenberg form, that is, t(0)
ij = 0 for i > j + 1. Indeed, it can
be checked that with this choice the computation of T(k) in (5.32) requires
only an order of n2 ﬂops per iteration.

204
5. Approximation of Eigenvalues and Eigenvectors
To achieve maximum eﬃciency and stability of the algorithm, suitable
transformation matrices are employed. Precisely, the preliminary reduc-
tion of matrix A into upper Hessenberg form is realized with Householder
matrices, whilst the QR factorization of T(k) is carried out using Givens
matrices, instead of the modiﬁed Gram-Schmidt procedure introduced in
Section 3.4.3.
We brieﬂy describe Householder and Givens matrices in the next section,
referring to Section 5.6.5 for their implementation. The algorithm and ex-
amples of computations of the real Schur form of A starting from its upper
Hessenberg form are then discussed in Section 5.6.4.
5.6.1
Householder and Givens Transformation Matrices
For any vector v ∈Rn, let us introduce the orthogonal and symmetric
matrix
P = I −2vvT /∥v∥2
2.
(5.38)
Given a vector x ∈Rn, the vector y = Px is the reﬂection of x with respect
to the hyperplane π = span{v}⊥formed by the set of the vectors that are
orthogonal to v (see Figure 5.3, left). Matrix P and the vector v are called
the Householder reﬂection matrix and the Householder vector, respectively.
v
x
y
π
y
xi
x
θ
xk
FIGURE 5.3. Reﬂection across the hyperplane orthogonal to v (left); rotation by
an angle θ in the plane (xi, xk) (right)
Householder matrices can be used to set to zero a block of components of
a given vector x ∈Rn. If, in particular, one would like to set to zero all the
components of x, except the m-th one, the Householder vector ought to be
chosen as
v = x ± ∥x∥2em,
(5.39)

5.6 The QR Method for Matrices in Hessenberg Form
205
em being the m-th unit vector of Rn. The matrix P computed by (5.38)
depends on the vector x itself, and it can be checked that
Px =

0, 0, . . . , ±∥x∥2
8 9: ;
m
, 0, . . . , 0


T
.
(5.40)
Example 5.5 Let x = [1, 1, 1, 1]T and m = 3; then
v =


1
1
3
1


,
P = 1
6


5
−1
−3
−1
−1
5
−3
−1
−3
−3
−3
−3
−1
−1
−3
5


,
Px =


0
0
−2
0


.
•
If, for some k ≥1, the ﬁrst k components of x must remain unaltered,
while the components from k + 2 on are to be set to zero, the Householder
matrix P = P(k) takes the following form
P(k) =


Ik
0
0
Rn−k

,
Rn−k = In−k −2w(k)(w(k))T
∥w(k)∥2
2
.
(5.41)
As usual, Ik is the identity matrix of order k, while Rn−k is the elementary
Householder matrix of order n −k associated with the reﬂection across the
hyperplane orthogonal to the vector w(k) ∈Rn−k. According to (5.39), the
Householder vector is given by
w(k) = x(n−k) ± ∥x(n−k)∥2e(n−k)
1
,
(5.42)
where x(n−k) ∈Rn−k is the vector formed by the last n −k components
of x and e(n−k)
1
is the ﬁrst unit vector of the canonical basis of Rn−k. We
notice that P(k) is a function of x through w(k). The criterion for ﬁxing
the sign in the deﬁnition of w(k) will be discussed in Section 5.6.5.
The components of the transformed vector y = P(k) x read







yj = xj
j = 1, · · · , k,
yj = 0
j = k + 2, · · · , n,
yk+1 = ±∥x(n−k)∥2.
The Householder matrices will be employed in Section 5.6.2 to carry out the
reduction of a given matrix A to a matrix H(0) in upper Hessenberg form.
This is the ﬁrst step for an eﬃcient implementation of the QR iteration
(5.32) with T(0) = H(0) (see Section 5.6).

206
5. Approximation of Eigenvalues and Eigenvectors
Example 5.6 Let x=[1, 2, 3, 4, 5]T and k = 1 (this means that we want to set to
zero the components xj, with j = 3, 4, 5). The matrix P(1) and the transformed
vector y=P(1) x are given by
P(1) =


1
0
0
0
0
0
0.2722
0.4082
0.5443
0.6804
0
0.4082
0.7710
−0.3053
−0.3816
0
0.5443
−0.3053
0.5929
−0.5089
0
0.6804
−0.3816
−0.5089
0.3639


,
y =


1
7.3485
0
0
0


.
•
The Givens elementary matrices are orthogonal rotation matrices that al-
low for setting to zero in a selective way the entries of a vector or matrix.
For a given pair of indices i and k, and a given angle θ, these matrices are
deﬁned as
G(i, k, θ) = In −Y
(5.43)
where Y∈Rn×n is a null matrix except for the following entries: yii =
ykk = 1 −cos(θ), yik = −sin(θ) = −yki. A Givens matrix is of the form
i
k
G(i, k, θ)
=


1
0
1
...
cos(θ)
sin(θ)
...
−sin(θ)
cos(θ)
...
1
0
1


.
i
k
For a given vector x ∈Rn, the product y = (G(i, k, θ))T x is equivalent to
rotating x counterclockwise by an angle θ in the coordinate plane (xi, xk)
(see Figure 5.3, right). After letting c = cos θ, s = sin θ, it follows that
yj =







xj
j ̸= i, k,
cxi −sxk
j = i,
sxi + cxk
j = k.
(5.44)

5.6 The QR Method for Matrices in Hessenberg Form
207
Let αik =

x2
i + x2
k and notice that if c and s satisfy c = xi/αik, s =
−xk/αik (in such a case, θ = arctan(−xk/xi)), we get yk = 0, yi = αik
and yj = xj for j ̸= i, k. Similarly, if c = xk/αik, s = xi/αik (that is,
θ = arctan(xi/xk)), then yi = 0, yk = αik and yj = xj for j ̸= i, k.
The Givens rotation matrices will be employed in Section 5.6.3 to carry
out the QR factorization step in the algorithm (5.32) and in Section 5.10.1
where the Jacobi method for symmetric matrices is considered.
Remark 5.3 (Householder deﬂation for power iterations) The ele-
mentary Householder tranformations can be conveniently employed to com-
pute the ﬁrst (largest or smallest) eigenvalues of a given matrix A ∈Rn×n.
Assume that the eigenvalues of A are ordered as in (5.16) and suppose
that the eigenvalue/eigenvector pair (λ1, x1) has been computed using the
power method. Then the matrix A can be transformed into the following
block form (see for the proof [Dat95], Theorem 8.5.4, p. 418)
A1 = HAH =

λ1
bT
0
A2

where b ∈Rn−1, H is the Householder matrix such that Hx1 = αx1 for
some α ∈R, the matrix A2 ∈R(n−1)×(n−1) and the eigenvalues of A2 are
the same as those of A except for λ1. The matrix H can be computed using
(5.38) with v = x1 ± ∥x1∥2e1.
The deﬂation procedure consists of computing the second dominant (sub-
dominant) eigenvalue of A by applying the power method to A2 provided
that |λ2| ̸= |λ3|. Once λ2 is available, the corresponding eigenvector x2
can be computed by applying the inverse power iteration to the matrix A
taking µ = λ2 (see Section 5.3.2) and proceeding in the same manner with
the remaining eigenvalue/eigenvector pairs. An example of deﬂation will
be presented in Section 5.12.2.
■
5.6.2
Reducing a Matrix in Hessenberg Form
A given matrix A∈Rn×n can be transformed by similarity transforma-
tions into upper Hessenberg form with a cost of the order of n3 ﬂops. The
algorithm takes n −2 steps and the similarity transformation Q can be
computed as the product of Householder matrices P(1) · · · P(n−2). For this,
the reduction procedure is commonly known as the Householder method.
Precisely, the k-th step consists of a similarity transformation of A through
the Householder matrix P(k) which aims at setting to zero the elements in
positions k + 2, . . . , n of the k-th column of A, for k = 1, . . . , (n −2) (see

208
5. Approximation of Eigenvalues and Eigenvectors
Section 5.6.1). For example, in the case n = 4 the reduction process yields


•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•


−→
P(1)


•
•
•
•
•
•
•
•
0
•
•
•
0
•
•
•


−→
P(2)


•
•
•
•
•
•
•
•
0
•
•
•
0
0
•
•


,
having denoted by • the entries of the matrices that are a priori non zero.
Given A(0) = A, the method generates a sequence of matrices A(k) that
are orthogonally similar to A
A(k) = PT
(k)A(k−1)P(k) = (P(k) · · · P(1))T A(P(k) · · · P(1))
= QT
(k)AQ(k),
k ≥1.
(5.45)
For any k ≥1 the matrix P(k) is given by (5.41), where x is substituted
by the k-th column vector in matrix A(k−1). From the deﬁnition (5.41) it
is easy to check that the operation PT
(k) A(k−1) leaves the ﬁrst k rows of
A(k−1) unchanged, whilst PT
(k) A(k−1) P(k) = A(k) does the same on the
ﬁrst k columns. After n −2 steps of the Householder reduction, we obtain
a matrix H = A(n−2) in upper Hessenberg form.
Remark 5.4 (The symmetric case) If A is symmetric, the transforma-
tion (5.45) maintains such a property. Indeed
(A(k))T = (QT
(k)AQ(k))T = A(k),
∀k ≥1,
so that H must be tridiagonal. Its eigenvalues can be eﬃciently computed
using the method of Sturm sequences with a cost of the order of n ﬂops, as
will be addressed in Section 5.10.2.
■
A coding of the Householder reduction method is provided in Program
29. To compute the Householder vector, Program 32 is employed. In output,
the two matrices H and Q, respectively in Hessenberg form and orthogonal,
are such that H = QT AQ.
Program 29 - houshess : Hessenberg-Householder method
function [H,Q]=houshess(A)
n=max(size(A)); Q=eye(n); H=A;
for k=1:(n-2),
[v,beta]=vhouse(H(k+1:n,k)); I=eye(k); N=zeros(k,n-k);
m=length(v); R=eye(m)-beta*v*v’; H(k+1:n,k:n)=R*H(k+1:n,k:n);
H(1:n,k+1:n)=H(1:n,k+1:n)*R; P=[I, N; N’, R]; Q=Q*P;
end

5.6 The QR Method for Matrices in Hessenberg Form
209
The algorithm coded in Program 29 requires a cost of 10n3/3 ﬂops and
is well-conditioned with respect to rounding errors. Indeed, the following
estimate holds (see [Wil65], p. 351)
H = QT (A + E) Q,
∥E∥F ≤cn2u∥A∥F
(5.46)
where H is the Hessenberg matrix computed by Program 29, Q is an or-
thogonal matrix, c is a constant, u is the roundoﬀunit and ∥· ∥F is the
Frobenius norm (see (1.18)).
Example 5.7 Consider the reduction in upper Hessenberg form of the Hilbert
matrix H4 ∈R4×4. Since H4 is symmetric, its Hessenberg form should be a
triadigonal symmetric matrix. Program 29 yields the following results
Q =


1.00
0
0
0
0
0.77
−0.61
0.20
0
0.51
0.40
−0.76
0
0.38
0.69
0.61


,
H =


1.00
0.65
0
0
0.65
0.65
0.06
0
0
0.06
0.02
0.001
0
0
0.001
0.0003


.
The accuracy of the transformation procedure (5.45) can be measured by com-
puting the ∥· ∥F norm of the diﬀerence between H and QT H4Q. This yields
∥H −QT H4Q∥F = 3.38 · 10−17, which conﬁrms the stability estimate (5.46).
•
5.6.3
QR Factorization of a Matrix in Hessenberg Form
In this section we explain how to eﬃciently implement the generic step of
the QR iteration, starting from a matrix T(0) = H(0) in upper Hessenberg
form.
For any k ≥1, the ﬁrst phase consists of computing the QR factorization
of H(k−1) by means of n −1 Givens rotations
+
Q(k),T
H(k−1) =
+
G(k)
n−1
,T
. . .
+
G(k)
1
,T
H(k−1) = R(k),
(5.47)
where, for any j = 1, . . . , n −1, G(k)
j
= G(j, j + 1, θj)(k) is, for any k ≥1,
the j-th Givens rotation matrix (5.43) in which θj is chosen according
to (5.44) in such a way that the entry of indices (j + 1, j) of the matrix
+
G(k)
j
,T
· · ·
+
G(k)
1
,T
H(k−1) is set equal to zero. The product (5.47) requires
a computational cost of the order of 3n2 ﬂops.
The next step consists of completing the orthogonal similarity transfor-
mation
H(k) = R(k)Q(k) = R(k) +
G(k)
1
. . . G(k)
n−1
,
.
(5.48)

210
5. Approximation of Eigenvalues and Eigenvectors
The orthogonal matrix Q(k) =
+
G(k)
1
. . . G(k)
n−1
,
is in upper Hessenberg
form. Indeed, taking for instance n = 3, and recalling Section 5.6.1, we get
Q(k) = G(k)
1 G(k)
2
=


•
•
0
•
•
0
0
0
1




1
0
0
0
•
•
0
•
•

=


•
•
•
•
•
•
0
•
•

.
Also (5.48) requires a cost of the order of 3n2 operations, for an overall eﬀort
of the order of 6n2 ﬂops. In conclusion, performing the QR factorization
with elementary Givens rotations on a starting matrix in upper Hessenberg
form yields a reduction of the operation count of one order of magnitude
with respect to the corresponding factorization with the modiﬁed Gram-
Schmidt procedure of Section 5.5.
5.6.4
The Basic QR Iteration starting from Upper Hessenberg
Form
A basic implementation of the QR iteration to generate the real Schur
decomposition of a matrix A is given in Program 30.
This program uses Program 29 to reduce A in upper Hessenberg form;
then each QR factorization step in (5.32) is carried out with Program 31
which utilizes Givens rotations. The overall eﬃciency of the algorithm is
ensured by pre- and post-multiplying with Givens matrices as explained in
Section 5.6.5, and by constructing the matrix Q(k) = G(k)
1
. . . G(k)
n−1 in the
function prodgiv, with a cost of n2 −2 ﬂops and without explicitly forming
the Givens matrices G(k)
j , for j = 1, . . . , n −1.
As for the stability of the QR iteration with respect to rounding er-
ror propagation, it can be shown that the computed real Schur form T is
orthogonally similar to a matrix “close” to A, i.e.
T = QT (A + E)Q
where Q is orthogonal and ∥E∥2 ≃u∥A∥2, u being the machine roundoﬀ
unit.
Program 30 returns in output, after niter iterations of the QR proce-
dure, the matrices T, Q and R in (5.32).
Program 30 - hessqr : Hessenberg-QR method
function [T,Q,R]=hessqr(A,niter)
n=max(size(A));
[T,Qhess]=houshess(A);
for j=1:niter
[Q,R,c,s]= qrgivens(T);

5.6 The QR Method for Matrices in Hessenberg Form
211
T=R;
for k=1:n-1,
T=gacol(T,c(k),s(k),1,k+1,k,k+1);
end
end
Program 31 - givensqr : QR factorization with Givens rotations
function [Q,R,c,s]= qrgivens(H)
[m,n]=size(H);
for k=1:n-1
[c(k),s(k)]=givcos(H(k,k),H(k+1,k));
H=garow(H,c(k),s(k),k,k+1,k,n);
end
R=H; Q=prodgiv(c,s,n);
function Q=prodgiv(c,s,n)
n1=n-1; n2=n-2;
Q=eye(n); Q(n1,n1)=c(n1); Q(n,n)=c(n1);
Q(n1,n)=s(n1); Q(n,n1)=-s(n1);
for k=n2:-1:1,
k1=k+1; Q(k,k)=c(k); Q(k1,k)=-s(k);
q=Q(k1,k1:n); Q(k,k1:n)=s(k)*q;
Q(k1,k1:n)=c(k)*q;
end
Example 5.8 Consider the matrix A (already in Hessenberg form)
A =


3
17
−37
18
−40
1
0
0
0
0
0
1
0
0
0
0
0
1
0
0
0
0
0
1
0


.
To compute its eigenvalues, given by −4, ±i, 2 and 5, we apply the QR method
and we compute the matrix T(40) after 40 iterations of Program 30. Notice that
the algorithm converges to the real Schur decomposition of A (5.34), with three
blocks Rii of order 1 (i = 1, 2, 3) and with the block R44 = T(40)(4 : 5, 4 : 5)
having eigenvalues equal to ±i
T(40) =


4.9997
18.9739
−34.2570
32.8760
−28.4604
0
−3.9997
6.7693
−6.4968
5.6216
0
0
2
−1.4557
1.1562
0
0
0
0.3129
−0.8709
0
0
0
1.2607
−0.3129


.

212
5. Approximation of Eigenvalues and Eigenvectors
•
Example 5.9 Let us now employ the QR method to generate the Schur real
decomposition of the matrix A below, after reducing it to upper Hessenberg form
A =


17
24
1
8
15
23
5
7
14
16
4
6
13
20
22
10
12
19
21
3
11
18
25
2
9


.
The eigenvalues of A are real and given (to four signiﬁcant ﬁgures) by λ1 =
65, λ2,3 = ±21.28 and λ4,5 = ±13.13. After 40 iterations of Program 30, the
computed matrix reads
T(40) =


65
0
0
0
0
0
14.6701
14.2435
4.4848
−3.4375
0
16.6735
−14.6701
−1.2159
2.0416
0
0
0
−13.0293
−0.7643
0
0
0
−3.3173
13.0293


.
It is not upper triangular, but block upper triangular, with a diagonal block
R11 = 65 and the two blocks
R22 =
. 14.6701
14.2435
16.6735
−14.6701
/
,
R33 =
. −13.0293
−0.7643
−3.3173
13.0293
/
,
having spectrums given by σ(R22) = λ2,3 and σ(R33) = λ4,5 respectively.
It is important to recognize that matrix T(40) is not the real Schur decomposi-
tion of A, but only a “cheating” version of it. In fact, in order for the QR method
to converge to the real Schur decomposition of A, it is mandatory to resort to
the shift techniques introduced in Section 5.7.
•
5.6.5
Implementation of Transformation Matrices
In the deﬁnition (5.42) it is convenient to choose the minus sign, obtaining
w(k) = x(n−k)−∥x(n−k)∥2e(n−k)
1
, in such a way that the vector Rn−kx(n−k)
is a positive multiple of e(n−k)
1
. If xk+1 is positive, in order to avoid nu-
merical cancellations, the computation can be rationalized as follows
w(k)
1
= x2
k+1 −∥x(n−k)∥2
2
xk+1 + ∥x(n−k)∥2
=
−
n

j=k+2
x2
j
xk+1 + ∥x(n−k)∥2
.

5.6 The QR Method for Matrices in Hessenberg Form
213
The construction of the Householder vector is performed by Program 32,
which takes as input a vector p ∈Rn−k (formerly, the vector x(n−k)) and
returns a vector q ∈Rn−k (the Householder vector w(k)), with a cost of
the order of n ﬂops.
If M ∈Rm×m is the generic matrix to which the Householder matrix P
(5.38) is applied (where I is the identity matrix of order m and v∈Rm),
letting w = MT v, then
PM = M −βvwT ,
β = 2/∥v∥2
2.
(5.49)
Therefore, performing the product PM amounts to a matrix-vector product
(w = MT v) plus an external product vector-vector (vwT ). The overall
computational cost of the product PM is thus equal to 2(m2 + m) ﬂops.
Similar considerations hold in the case where the product MP is to be
computed; deﬁning w = Mv, we get
MP = M −βwvT .
(5.50)
Notice that (5.49) and (5.50) do not require the explicit construction of
the matrix P. This reduces the computational cost to an order of m2 ﬂops,
whilst executing the product PM without taking advantage of the special
structure of P would increase the operation count to an order of m3 ﬂops.
Program 32 - vhouse : Construction of the Householder vector
function [v,beta]=vhouse(x)
n=length(x); x=x/norm(x); s=x(2:n)’*x(2:n); v=[1; x(2:n)];
if (s==0), beta=0;
else
mu=sqrt(x(1)ˆ2+s);
if (x(1) <= 0), v(1)=x(1)-mu;
else,
v(1)=-s/(x(1)+mu); end
beta=2*v(1)ˆ2/(s+v(1)ˆ2); v=v/v(1);
end
Concerning the Givens rotation matrices, the computation of c and s is
carried out as follows. Let i and k be two ﬁxed indices and assume that
the k-th component of a given vector x ∈Rn must be set to zero. Letting
r =

x2
i + x2
k, relation (5.44) yields
.
c
−s
s
c
/ .
xi
xk
/
=
.
r
0
/
(5.51)
hence there is no need of explicitly computing θ, nor evaluating any trigono-
metric function.
Executing Program 33 to solve system (5.51), requires 5 ﬂops, plus the
evaluation of a square root. As already noticed in the case of Householder

214
5. Approximation of Eigenvalues and Eigenvectors
matrices, even for Givens rotations we don’t have to explicitly compute the
matrix G(i, k, θ) to perform its product with a given matrix M∈Rm×m.
For that purpose Programs 34 and 35 are used, both at the cost of 6m
ﬂops. Looking at the structure (5.43) of matrix G(i, k, θ), it is clear that
the ﬁrst algorithm only modiﬁes rows i and k of M, whilst the second one
only changes columns i and k of M.
We conclude by noticing that the computation of the Householder vector
v and of the Givens sine and cosine (c, s), are well-conditioned operations
with respect to rounding errors (see [GL89], pp. 212-217 and the references
therein).
The solution of system (5.51) is implemented in Program 33. The input
parameters are the vector components xi and xk, whilst the output data
are the Givens cosine and sine c and s.
Program 33 - givcos : Computation of Givens cosine and sine
function [c,s]=givcos(xi, xk)
if (xk==0), c=1; s=0; else,
if abs(xk) > abs(xi)
t=-xi/xk; s=1/sqrt(1+tˆ2); c=s*t;
else
t=-xk/xi; c=1/sqrt(1+tˆ2); s=c*t;
end
end
Programs 34 and 35 compute G(i, k, θ)T M and MG(i, k, θ) respectively.
The input parameters c and s are the Givens cosine and sine. In Program
34, the indices i and k identify the rows of the matrix M that are being
aﬀected by the update M ←G(i, k, θ)T M, while j1 and j2 are the indices
of the columns involved in the computation. Similarly, in Program 35 i
and k identify the columns eﬀected by the update M ←MG(i, k, θ), while
j1 and j2 are the indices of the rows involved in the computation.
Program 34 - garow : Product G(i, k, θ)T M
function [M]=garow(M,c,s,i,k,j1,j2)
for j=j1:j2
t1=M(i,j);
t2=M(k,j);
M(i,j)=c*t1-s*t2;
M(k,j)=s*t1+c*t2;
end
Program 35 - gacol : Product MG(i, k, θ)
function [M]=gacol(M,c,s,j1,j2,i,k)
for j=j1:j2
t1=M(j,i);

5.7 The QR Iteration with Shifting Techniques
215
t2=M(j,k);
M(j,i)=c*t1-s*t2;
M(j,k)=s*t1+c*t2;
end
5.7
The QR Iteration with Shifting Techniques
Example 5.9 reveals that the QR iteration does not always converge to the
real Schur form of a given matrix A. To make this happen, an eﬀective
approach consists of incorporating in the QR iteration (5.32) a shifting
technique similar to that introduced for inverse iteration in Section 5.3.2.
This leads to the QR method with single shift described in Section 5.7.1,
which is used to accelerate the convergence of the QR iteration when A has
eigenvalues with moduli very close to each other.
In Section 5.7.2, a more sophisticated shifting technique is considered,
which guarantees the convergence of the QR iteration to the (approximate)
Schur form of matrix A (see Property 5.8). The resulting method (known as
QR iteration with double shift) is the most popular version of the QR iter-
ation (5.32) for solving the matrix eigenvalue problem, and is implemented
in the MATLAB intrinsic function eig.
5.7.1
The QR Method with Single Shift
Given µ ∈R, the shifted QR iteration is deﬁned as follows. For k = 1, 2, . . . ,
until convergence:
determine Q(k), R(k) such that
Q(k)R(k) = T(k−1) −µI
(QR factorization);
then, let
T(k) = R(k)Q(k) + µI.
(5.52)
where T(0) =

Q(0)T AQ(0) is in upper Hessenberg form. Since the QR
factorization in (5.52) is performed on the shifted matrix T(k−1) −µI, the
scalar µ is called shift. The sequence of matrices T(k) generated by (5.52)
is still similar to the initial matrix A, since for any k ≥1
R(k)Q(k) + µI
=

Q(k)T 
Q(k)R(k)Q(k) + µQ(k)
=

Q(k)T 
Q(k)R(k) + µI

Q(k) =

Q(k)T T(k−1)Q(k)
=
(Q(0)Q(1) . . . Q(k))T A(Q(0)Q(1) . . . Q(k)),
k ≥0.

216
5. Approximation of Eigenvalues and Eigenvectors
Assume µ is ﬁxed and that the eigenvalues of A are ordered in such a way
that
|λ1 −µ| ≥|λ2 −µ| ≥. . . ≥|λn −µ|.
Then it can be shown that, for 1 < j ≤n, the subdiagonal entry t(k)
j,j−1
tends to zero with a rate that is proportional to the ratio
|(λj −µ)/(λj−1 −µ)|k.
This extends the convergence result (5.37) to the shifted QR method (see
[GL89], Sections 7.5.2 and 7.3).
The result above suggests that if µ is chosen in such a way that
|λn −µ| < |λi −µ|,
i = 1, . . . , n −1,
then the matrix entry t(k)
n,n−1 in the iteration (5.52) tends rapidly to zero
as k increases. (In the limit, if µ were equal to an eigenvalue of T(k), that
is of A, then t(k)
n,n−1 = 0 and t(k)
n,n = µ). In practice one takes
µ = t(k)
n,n,
(5.53)
yielding the so called QR iteration with single shift. Correspondingly, the
convergence to zero of the sequence
2
t(k)
n,n−1
3
is quadratic in the sense that
if |t(k)
n,n−1|/∥T(0)∥2 = ηk < 1, for some k ≥0, then |t(k+1)
n,n−1|/∥T(0)∥2 = O(η2
k)
(see [Dem97], pp. 161-163 and [GL89], pp. 354-355).
This can be proﬁtably taken into account when programming the QR
iteration with single shift by monitoring the size of the subdiagonal entry
|t(k)
n,n−1|. In practice, t(k)
n,n−1 is set equal to zero if
|t(k)
n,n−1| ≤ε(|t(k)
n−1,n−1| + |t(k)
n,n|),
k ≥0,
(5.54)
for a prescribed ε, in general of the order of the roundoﬀunit. (This con-
vergence test is adopted in the library EISPACK). If A is an Hessenberg
matrix, when for a certain k a(k)
n,n−1 is set to zero, t(k)
n,n provides the desired
approximation of λn. Then the QR iteration with shift can continue on the
matrix T(k)(1 : n −1, 1 : n −1), and so on. This is a deﬂation algorithm
(for another example see Remark 5.3).
Example 5.10 We consider again the matrix A as in Example 5.9. Program 36,
with toll equal to the roundoﬀunit, converges in 14 iterations to the following
approximate real Schur form of A, which displays the correct eigenvalues of matrix
A on its diagonal (to six signiﬁcant ﬁgures)
T(40) =


65
0
0
0
0
0
−21.2768
2.5888
−0.0445
−4.2959
0
0
−13.1263
−4.0294
−13.079
0
0
0
21.2768
−2.6197
0
0
0
0
13.1263


.

5.7 The QR Iteration with Shifting Techniques
217
We also report in Table 5.2 the convergence rate p(k) of the sequence
2
t(k)
n,n−1
3
(n = 5) computed as
p(k) = 1 +
1
log(ηk) log |t(k)
n,n−1|
|t(k−1)
n,n−1|
,
k ≥1.
The results show good agreement with the expected quadratic rate.
k
|t(k)
n,n−1|/∥T(0)∥2
p(k)
0
0.13865
1
1.5401 · 10−2
2.1122
2
1.2213 · 10−4
2.1591
3
1.8268 · 10−8
1.9775
4
8.9036 · 10−16
1.9449
TABLE 5.2. Convergence rate of the sequence
2
t(k)
n,n−1
3
in the QR iteration with
single shift
•
The coding of the QR iteration with single shift (5.52) is given in Pro-
gram 36. The code utilizes Program 29 to reduce the matrix A in upper
Hessenberg form and Program 31 to perform the QR factorization step.
The input parameters toll and itmax are the tolerance ε in (5.54) and
the maximum admissible number of iterations, respectively. In output, the
program returns the (approximate) real Schur form of A and the number
of iterations needed for its computation.
Program 36 - qrshift : QR iteration with single shift
function [T,iter]=qrshift(A,toll,itmax)
n=max(size(A)); iter=0; [T,Q]=houshess(A);
for k=n:-1:2
I=eye(k);
while abs(T(k,k-1)) > toll*(abs(T(k,k))+abs(T(k-1,k-1)))
iter=iter+1;
if (iter > itmax),
return
end
mu=T(k,k); [Q,R,c,s]=qrgivens(T(1:k,1:k)-mu*I);
T(1:k,1:k)=R*Q+mu*I;
end
T(k,k-1)=0;
end

218
5. Approximation of Eigenvalues and Eigenvectors
5.7.2
The QR Method with Double Shift
The single-shift QR iteration (5.52) with the choice (5.53) for µ is eﬀective
if the eigenvalues of A are real, but not necessarily when complex conjugate
eigenvalues are present, as happens in the following example.
Example 5.11 The matrix A ∈R4×4 (reported below to ﬁve signiﬁcant ﬁgures)
A =


1.5726
−0.6392
3.7696
−1.3143
0.2166
−0.0420
0.4006
−1.2054
0.0226
0.3592
0.2045
−0.1411
−0.1814
1.1146
−3.2330
1.2648


has eigenvalues {±i, 1, 2}, i being the imaginary unit. Running Program 36 with
toll equal to the roundoﬀunit yields after 100 iterations
T(101) =


2
1.1999
0.5148
4.9004
0
−0.0001
−0.8575
0.7182
0
1.1662
0.0001
−0.8186
0
0
0
1


.
The obtained matrix is the real Schur form of A, where the 2×2 block T(101)(2:3,
2:3) has complex conjugate eigenvalues ±i. These eigenvalues cannot be computed
by the algorithm (5.52)-(5.53) since µ is real.
•
The problem with this example is that working with real matrices neces-
sarily yields a real shift, whereas a complex one would be needed. The QR
iteration with double shift is set up to account for complex eigenvalues and
allows for removing the 2×2 diagonal blocks of the real Schur form of A.
Precisely, suppose that the QR iteration with single shift (5.52) detects
at some step k a 2×2 diagonal block R(k)
kk that cannot be reduced into
upper triangular form. Since the iteration is converging to the real Schur
form of the matrix A the two eigenvalues of R(k)
kk are complex conjugate
and will be denoted by λ(k) and ¯λ(k). The double shift strategy consists of

5.7 The QR Iteration with Shifting Techniques
219
the following steps:
determine Q(k), R(k) such that
Q(k)R(k) = T(k−1) −λ(k)I
(ﬁrst QR factorization);
then, let
T(k) = R(k)Q(k) + λ(k)I;
determine Q(k+1), R(k+1) such that
Q(k+1)R(k+1) = T(k) −¯λ(k)I
(second QR factorization);
then, let
T(k+1) = R(k+1)Q(k+1) + ¯λ(k)I.
(5.55)
Once the double shift has been carried out the QR iteration with single shift
is continued until a situation analogous to the one above is encountered.
The QR iteration incorporating the double shift strategy is the most eﬀec-
tive algorithm for computing eigenvalues and yields the approximate Schur
form of a given matrix A. Its actual implementation is far more sophisti-
cated than the outline above and is called QR iteration with Francis shift
(see [Fra61], and, also, [GL89], Section 7.5 and [Dem97], Section 4.4.5). As
for the case of the QR iteration with single shift, quadratic convergence can
also be proven for the QR method with Francis shift. However, special ma-
trices have recently been found for which the method fails to converge (see
for an example Exercise 14 and Remark 5.13). We refer for some analysis
and remedies to [Bat90], [Day96], although the ﬁnding of a shift strategy
that guarantees convergence of the QR iteration for all matrices is still an
open problem.
Example 5.12 Let us apply the QR iteration with double shift to the matrix
A in Example 5.11. After 97 iterations of Program 37, with toll equal to the
roundoﬀunit, we get the following (approximate) Schur form of A, which displays
on its diagonal the four eigenvalues of A
T(97) =


2
1 + 2i
−2.33 + 0.86i
4.90
0
5.02 · 10−14 + i
−2.02 + 6.91 · 10−14i
0.72
t(97)
31
0
−1.78 · 10−14 −i
−0.82
t(97)
41
t(97)
42
0
1


where t(97)
31
= 2.06 · 10−17 + 7.15 · 10−49i, t(97)
41
= −5.59 · 10−17 and t(97)
42
=
−4.26 · 10−18, respectively.
•
Example 5.13 Consider the pseudo-spectral diﬀerentiation matrix (10.73) of
order 5. This matrix is singular, with a unique eigenvalue λ = 0 of algebraic

220
5. Approximation of Eigenvalues and Eigenvectors
multiplicity equal to 5 (see [CHQZ88], p. 44). In this case the QR method with
double shift provides an inaccurate approximation of the spectrum of the ma-
trix. Indeed, using Program 37, with toll=eps, the method converges after 59
iterations to an upper triangular matrix with diagonal entries given by 0.0020,
0.0006 ± 0.0019i and −0.0017 ± 0.0012i, respectively. Using the MATLAB intrin-
sic function eig yields instead the eigenvalues −0.0024, −0.0007 ± 0.0023i and
0.0019 ± 0.0014i.
•
A basic implementation of the QR iteration with double shift is provided
in Program 37. The input/output parameters are the same as those of
Program 36. The output matrix T is the approximate Schur form of matrix
A.
Program 37 - qr2shift : QR iteration with double shift
function [T,iter]=qr2shift(A,toll,itmax)
n=max(size(A)); iter=0; [T,Q]=houshess(A);
for k=n:-1:2
I=eye(k);
while abs(T(k,k-1)) > toll*(abs(T(k,k))+abs(T(k-1,k-1)))
iter=iter+1; if (iter > itmax), return, end
mu=T(k,k); [Q,R,c,s]=qrgivens(T(1:k,1:k)-mu*I);
T(1:k,1:k)=R*Q+mu*I;
if (k > 2),
Tdiag2=abs(T(k-1,k-1))+abs(T(k-2,k-2));
if abs(T(k-1,k-2)) ¡= toll*Tdiag2;
[lambda]=eig(T(k-1:k,k-1:k));
[Q,R,c,s]=qrgivens(T(1:k,1:k)-lambda(1)*I);
T(1:k,1:k)=R*Q+lambda(1)*I;
[Q,R,c,s]=qrgivens(T(1:k,1:k)-lambda(2)*I);
T(1:k,1:k)=R*Q+lambda(2)*I;
end
end
end, T(k,k-1)=0;
end
I=eye(2);
while (abs(T(2,1)) > toll*(abs(T(2,2))+abs(T(1,1)))) & (iter <= itmax)
iter=iter+1; mu=T(2,2);
[Q,R,c,s]=qrgivens(T(1:2,1:2)-mu*I); T(1:2,1:2)=R*Q+mu*I;
end

5.8 Computing the Eigenvectors and the SVD of a Matrix
221
5.8
Computing the Eigenvectors and the SVD of a
Matrix
The power and inverse iterations described in Section 5.3.2 can be used
to compute a selected number of eigenvalue/eigenvector pairs. If all the
eigenvalues and eigenvectors of a matrix are needed, the QR iteration can be
proﬁtably employed to compute the eigenvectors as shown in Sections 5.8.1
and 5.8.2. In Section 5.8.3 we deal with the computation of the singular
value decomposition (SVD) of a given matrix.
5.8.1
The Hessenberg Inverse Iteration
For any approximate eigenvalue λ computed by the QR iteration as de-
scribed in Section 5.7.2, the inverse iteration (5.28) can be applied to the
matrix H = QT AQ in Hessenberg form, yielding an approximate eigenvec-
tor q. Then, the eigenvector x associated with λ is computed as x = Qq.
Clearly, one can take advantage of the structure of the Hessenberg matrix
for an eﬃcient solution of the linear system at each step of (5.28). Typi-
cally, only one iteration is required to produce an adequate approximation
of the desired eigenvector x (see [GL89], Section 7.6.1 and [PW79] for more
details).
5.8.2
Computing the Eigenvectors from the Schur Form of a
Matrix
Suppose that the (approximate) Schur form QHAQ=T of a given matrix
A∈Rn×n has been computed by the QR iteration with double shift, Q
being a unitary matrix and T being upper triangular.
Then, if Ax=λx, we have QHAQQHx= QH λx, i.e., letting y=QHx, T
y=λy holds. Therefore y is an eigenvector of T, so that to compute the
eigenvectors of A we can work directly on the Schur form T.
Assume for simplicity that λ = tkk ∈C is a simple eigenvalue of A. Then
the upper triangular matrix T can be decomposed as
T =


T11
v
T13
0
λ
wT
0
0
T33

,
where T11 ∈C(k−1)×(k−1) and T33 ∈C(n−k)×(n−k) are upper triangular
matrices, v∈Ck−1, w∈Cn−k and λ ̸∈σ(T11) ∪σ(T33).

222
5. Approximation of Eigenvalues and Eigenvectors
Thus, letting y =

yT
k−1, y, yT
n−k

, with yk−1 ∈Ck−1, y ∈C and yn−k ∈
Cn−k, the matrix eigenvector problem (T - λI) y=0 can be written as







(T11 −λIk−1)yk−1+
vy+
T13yn−k
=
0
wT yn−k
=
0
(T33 −λIn−k)yn−k
=
0.
(5.56)
Since λ is simple, both matrices T11 −λIk−1 and T33 −λIn−k are nonsin-
gular, so that the third equation in (5.56) yields yn−k = 0 and the ﬁrst
equation becomes
(T11 −λIk−1)yk−1 = −vy.
Setting arbitrarily y = 1 and solving the triangular system above for yk−1
yields (formally)
y =




−(T11 −λIk−1)−1v
1
0



.
The desired eigenvector x can then be computed as x=Qy.
An eﬃcient implementation of the above procedure is carried out in the
intrinsic MATLAB function eig. Invoking this function with the format [V,
D]= eig(A) yields the matrix V whose columns are the right eigenvectors
of A and the diagonal matrix D contains its eigenvalues. Further details can
be found in the strvec subroutine in the LAPACK library, while for the
computation of eigenvectors in the case where A is symmetric, we refer to
[GL89], Chapter 8 and [Dem97], Section 5.3.
5.8.3
Approximate Computation of the SVD of a Matrix
In this section we describe the Golub-Kahan-Reinsch algorithm for the
computation of the SVD of a matrix A ∈Rm×n with m ≥n (see [GL89],
Section 5.4). The method consists of two phases, a direct one and an iter-
ative one.
In the ﬁrst phase A is transformed into an upper trapezoidal matrix of
the form
UT AV =

B
0

,
(5.57)
where U and V are two orthogonal matrices and B ∈Rn×n is upper bidi-
agonal. The matrices U and V are generated using n + m −3 Householder
matrices U1, . . . , Um−1, V1, . . . , Vn−2 as follows.

5.8 Computing the Eigenvectors and the SVD of a Matrix
223
The algorithm initially generates U1 in such a way that the matrix A(1) =
U1A has a(1)
i1 = 0 if i > 1. Then, V1 is determined so that A(2) = A(1)V1
has a(2)
1j = 0 for j > 2, preserving at the same time the null entries of the
previous step. The procedure is repeated starting from A(2), and taking U2
such that A(3) = U2A(2) has a(3)
i2 = 0 for i > 2 and V2 in such a way that
A(4) = A(3)V2 has a(4)
2j = 0 for j > 3, yet preserving the null entries already
generated. For example, in the case m = 5, n = 4 the ﬁrst two steps of the
reduction process yield
A(1) = U1A =


•
•
•
•
0
•
•
•
0
•
•
•
0
•
•
•
0
•
•
•


−→A(2) = A(1)V1 =


•
•
0
0
0
•
•
•
0
•
•
•
0
•
•
•
0
•
•
•


,
having denoted by • the entries of the matrices that in principle are diﬀerent
than zero. After at most m −1 steps, we ﬁnd (5.57) with
U = U1U2 . . . Um−1,
V = V1V2 . . . Vn−2.
In the second phase, the obtained matrix B is reduced into a diagonal
matrix Σ using the QR iteration. Precisely, a sequence of upper bidiagonal
matrices B(k) are constructed such that, as k →∞, their oﬀ-diagonal
entries tend to zero quadratically and the diagonal entries tend to the
singular values σi of A. In the limit, the process generates two orthogonal
matrices W and Z such that
WT BZ = Σ = diag(σ1, . . . , σn).
The SVD of A is then given by
UT AV =

Σ
0

,
with U = Udiag(W, Im−n) and V = VZ.
The computational cost of this procedure is 2m2n + 4mn2 + 9
2n3 ﬂops,
which reduces to 2mn2−2
3n3 ﬂops if only the singular values are computed.
In this case, recalling what was stated in Section 3.13 about AT A, the
method described in the present section is preferable to computing directly
the eigenvalues of AT A and then taking their square roots.
As for the stability of this procedure, it can be shown that the computed
σi turn out to be the singular values of the matrix A + δA with
∥δA∥2 ≤Cmnu∥A∥2,

224
5. Approximation of Eigenvalues and Eigenvectors
Cmn being a constant dependent on n, m and the roundoﬀunit u. For other
approaches to the computation of the SVD of a matrix, see [Dat95] and
[GL89].
5.9
The Generalized Eigenvalue Problem
Let A, B ∈Cn×n be two given matrices; for any z ∈C, we call A −zB a
matrix pencil and denote it by (A,B). The set σ(A,B) of the eigenvalues of
(A,B) is deﬁned as
σ(A, B) = {µ ∈C : det(A −µB) = 0} .
The generalized matrix eigenvalue problem can be formulated as: ﬁnd λ ∈
σ(A,B) and a nonnull vector x ∈Cn such that
Ax = λBx.
(5.58)
The pair (λ, x) satisfying (5.58) is an eigenvalue/eigenvector pair of the
pencil (A,B). Note that by setting B=In in (5.58) we recover the standard
matrix eigenvalue problem considered thus far.
Problems like (5.58) arise frequently in engineering applications, e.g.,
in the study of vibrations of structures (buildings, aircrafts and bridges)
or in the mode analysis for waveguides (see [Inm94] and [Bos93]). Another
example is the computation of the extremal eigenvalues of a preconditioned
matrix P−1A (in which case B = P in (5.58)) when solving a linear system
with an iterative method (see Remark 4.2).
Let us introduce some deﬁnitions. We say that the pencil (A,B) is regular
if det(A-zB) is not identically zero, otherwise the pencil is singular. When
(A,B) is regular, p(z) = det(A −zB) is the characteristic polynomial of the
pencil; denoting by k the degree of p, the eigenvalues of (A,B) are deﬁned
as:
1. the roots of p(z) = 0, if k = n;
2. ∞if k < n (with multiplicity equal to n −k).
Example 5.14 (Taken from [Par80], [Saa92] and [GL89])
A =
 −1
0
0
1

,
B =
 0
1
1
0

p(z) = z2 + 1
=⇒
σ(A, B) = ±i
A =
 −1
0
0
0

,
B =
 0
0
0
1

p(z) = z
=⇒
σ(A, B) = {0, ∞}
A =
 1
2
0
0

,
B =
 1
0
0
0

p(z) = 0
=⇒
σ(A, B) = C.
The ﬁrst pair of matrices shows that symmetric pencils, unlike symmetric ma-
trices, may exhibit complex conjugate eigenvalues. The second pair is a regular
pencil displaying an eigenvalue equal to inﬁnity, while the third pair is an example
of singular pencil.
•

5.9 The Generalized Eigenvalue Problem
225
5.9.1
Computing the Generalized Real Schur Form
The deﬁnitions and examples above imply that the pencil (A,B) has n ﬁnite
eigenvalues iﬀB is nonsingular.
In such a case, a possible approach to the solution of problem (5.58) is
to transform it into the equivalent eigenvalue problem Cx = λx, where the
matrix C is the solution of the system BC = A, then apply the QR iteration
to C. For actually computing the matrix C, one can use Gauss elimination
with pivoting or the techniques shown in Section 3.6. This procedure can
yield inaccurate results if B is ill-conditioned, since computing C is aﬀected
by rounding errors of the order of u ∥A∥2∥B−1∥2 (see [GL89], p. 376).
A more attractive approach is based on the following result, which gen-
eralizes the Schur decomposition theorem 1.5 to the case of regular pencils
to (for a proof, see [Dat95], p. 497).
Property 5.10 (Generalized Schur decomposition) Let (A,B) be a
regular pencil. Then, there exist two unitary matrices U and Z such that
UHAZ = T, UHBZ = S, where T and S are upper triangular. For i =
1, . . . , n the eigenvalues of (A,B) are given by
λi = tii/sii,
if sii ̸= 0,
λi = ∞,
if tii ̸= 0, sii = 0.
Exactly as in the matrix eigenvalue problem, the generalized Schur form
cannot be explicitly computed, so the counterpart of the real Schur form
(5.34) has to be computed. Assuming that the matrices A and B are real, it
can be shown that there exist two orthogonal matrices ˜U and ˜Z such that
˜T = ˜UT A˜Z is upper quasi-triangular and ˜S = ˜UT B˜Z is upper triangular.
This decomposition is known as the generalized real Schur decomposition
of a pair (A,B) and can be computed by a suitably modiﬁed version of the
QR algorithm, known as QZ iteration, which consists of the following steps
(for a more detailed description, see [GL89], Section 7.7, [Dat95], Section
9.3):
1. reduce A and B into upper Hessenberg form and upper triangular
form, respectively, i.e., ﬁnd two orthogonal matrices Q and Z such
that A = QT AZ is upper Hessenberg and B = QT BZ is upper trian-
gular;
2. the QR iteration is applied to the matrix AB−1 to reduce it to real
Schur form.
To save computational resources, the QZ algorithm overwrites the matrices
A and B on their upper Hessenberg and triangular forms and requires 30n3
ﬂops; an additional cost of 36n3 operations is required if Q and Z are

226
5. Approximation of Eigenvalues and Eigenvectors
also needed. The method is implemented in the LAPACK library in the
subroutine sgges and can be invoked in the MATLAB environment with
the command eig(A,B).
5.9.2
Generalized Real Schur Form of Symmetric-Deﬁnite
Pencils
A remarkable situation occurs when both A and B are symmetric, and one
of them, say B, is also positive deﬁnite. In such a case, the pair (A,B) forms
a symmetric-deﬁnite pencil for which the following result holds.
Theorem 5.7 The symmetric-deﬁnite pencil (A,B) has real eigenvalues
and linearly independent eigenvectors. Moreover, the matrices A and B can
be simultaneously diagonalized. Precisely, there exists a nonsingular matrix
X ∈Rn×n such that
XT AX = Λ = diag(λ1, λ2, . . . , λn),
XT BX = In,
where for i = 1, . . . , n, λi are the eigenvalues of the pencil (A, B).
Proof. Since B is symmetric positive deﬁnite, it admits a unique Cholesky fac-
torization B = HT H, where H is upper triangular (see Section 3.4.2). From (5.58)
we deduce that Cz = λz with C = H−T AH−1, z = Hx, where (λ, x) is an
eigenvalue/eigenvector pair of (A,B).
The matrix C is symmetric; therefore, its eigenvalues are real and a set of
orthonormal eigenvectors (y1, . . . , yn) = Y exists. As a consequence, letting X =
H−1Y allows for simultaneously diagonalizing both A and B since
XT AX = YT H−T AH−1Y = YT CY = Λ = diag(λ1, . . . , λn),
XT BX = YT H−T BH−1Y = YT Y = In.
3
The following QR-Cholesky algorithm computes the eigenvalues λi and
the corresponding eigenvectors xi of a symmetric-deﬁnite pencil (A,B), for
i = 1, . . . , n (see for more details [GL89], Section 8.7, [Dat95], Section 9.5):
1. compute the Cholesky factorization B = HT H;
2. compute C = H−T AH−1;
3. for i = 1, . . . , n, compute the eigenvalues λi and eigenvectors zi of the
symmetric matrix C using the QR iteration. Then construct from the
set {zi} an orthonormal set of eigenvectors {yi} (using, for instance,
the modiﬁed Gram-Schmidt procedure of Section 3.4.3);
4. for i = 1, . . . , n, compute the eigenvectors xi of the pencil (A,B) by
solving the systems Hxi = yi.

5.10 Methods for Eigenvalues of Symmetric matrices
227
This algorithm requires an order of 14n3 ﬂops and it can be shown (see
[GL89], p. 464) that, if ˆλ is a computed eigenvalue, then
ˆλ ∈σ(H−T AH−1 + E),
with ∥E∥2 ≃u∥A∥2∥B−1∥2.
Thus, the generalized eigenvalue problem in the symmetric-deﬁnite case
may become unstable with respect to rounding errors propagation if B is
ill-conditioned. For a stabilized version of the QR-Cholesky method, see
[GL89], p. 464 and the references cited therein.
5.10
Methods for Eigenvalues of Symmetric
matrices
In this section we deal with the computation of the eigenvalues of a sym-
metric matrix A ∈Rn×n. Besides the QR method previously examined,
speciﬁc algorithms which take advantage of the symmetry of A are avail-
able.
Among these, we ﬁrst consider the Jacobi method, which generates a
sequence of matrices orthogonally similar to A and converging to the diag-
onal Schur form of A. Then, the Sturm sequence and Lanczos procedures
are presented, for handling the case of tridiagonal matrices and large sparse
matrices respectively.
5.10.1
The Jacobi Method
The Jacobi method generates a sequence of matrices A(k) that are orthog-
onally similar to matrix A and converge to a diagonal matrix whose entries
are the eigenvalues of A. This is done using the Givens similarity transfor-
mations (5.43) as follows.
Given A(0) = A, for any k = 1, 2, . . . , a pair of indices p and q is ﬁxed,
with 1 ≤p < q ≤n. Next, letting Gpq = G(p, q, θ), the matrix A(k) =
(Gpq)T A(k−1)Gpq, orthogonally similar to A, is constructed in such a way
that
a(k)
ij = 0
if
(i, j) = (p, q).
(5.59)
Letting c = cos θ and s = sin θ, the procedure for computing the entries of
A(k) that are changed with respect to those of A(k−1), can be written as

a(k)
pp
a(k)
pq
a(k)
pq
a(k)
qq

=
.
c
s
−s
c
/T 
a(k−1)
pp
a(k−1)
pq
a(k−1)
pq
a(k−1)
qq


.
c
s
−s
c
/
.
(5.60)
If a(k−1)
pq
= 0, we can satisfy (5.59) by taking c = 1 and s = 0. If a(k−1)
pq
̸=
0, letting t = s/c, (5.60) requires the solution of the following algebraic

228
5. Approximation of Eigenvalues and Eigenvectors
equation
t2 + 2ηt −1 = 0,
η = a(k−1)
qq
−a(k−1)
pp
2a(k−1)
pq
.
(5.61)
The root t = 1/(η +

1 + η2) is chosen in (5.61) if η ≥0, otherwise we
take t = −1/(−η +

1 + η2); next, we let
c =
1
√
1 + t2 ,
s = ct.
(5.62)
To examine the rate at which the oﬀ-diagonal entries of A(k) tend to zero,
it is convenient to introduce, for any matrix M ∈Rn×n, the nonnegative
quantity
Ψ(M) =



n

i,j=1
i̸=j
m2
ij



1/2
=

∥M∥2
F −
n

i=1
m2
ii
1/2
.
(5.63)
The Jacobi method ensures that Ψ(A(k)) ≤Ψ(A(k−1)) for any k ≥1.
Indeed, the computation of (5.63) for matrix A(k) yields
(Ψ(A(k)))2 = (Ψ(A(k−1)))2 −2
+
a(k−1)
pq
,2
≤(Ψ(A(k−1)))2.
(5.64)
The estimate (5.64) suggests that, at each step k, the optimal choice of the
indices p and q is that corresponding to the entry in A(k−1) such that
|a(k−1)
pq
| = max
i̸=j |a(k−1)
ij
|.
The computational cost of this strategy is of the order of n2 ﬂops for the
search of the maximum module entry, while the updating step A(k) =
(Gpq)T A(k−1)Gpq requires only a cost of the order of n ﬂops, as already
noticed in Section 5.6.5. It is thus convenient to resort to the so called row
cyclic Jacobi method, in which the choice of the indices p and q is done by
a row-sweeping of the matrix A(k) according to the following algorithm: for
any k = 1, 2, . . . and for any i-th row of A(k) (i = 1, . . . , n −1), we set
p = i and q = (i+1), . . . , n. Each complete sweep requires N = n(n−1)/2
Jacobi transformations. Assuming that |λi −λj| ≥δ for i ̸= j, it can be
shown that the cyclic Jacobi method converges quadratically, that is (see
[Wil65], [Wil62])
Ψ(A(k+N)) ≤
1
δ
√
2(Ψ(A(k)))2,
k = 1, 2, . . .
For further details of the algorithm, we refer to [GL89], Section 8.4.

5.10 Methods for Eigenvalues of Symmetric matrices
229
Example 5.15 Let us apply the cyclic Jacobi method to the Hilbert matrix H4,
whose eigenvalues read (to ﬁve signiﬁcant ﬁgures) λ1 = 1.5002, λ2 = 1.6914·10−1,
λ3 = 6.7383 · 10−3 and λ4 = 9.6702 · 10−5. Running Program 40 with toll =
10−15, the method converges in 3 sweeps to a matrix whose diagonal entries
coincide with the eigenvalues of H4 unless 4.4409 · 10−16. As for the oﬀ-diagonal
entries, the values attained by Ψ(H(k)
4 ) are reported in Table 5.3.
•
Sweep
Ψ(H(k)
4 )
Sweep
Ψ(H(k)
4 )
Sweep
Ψ(H(k)
4 )
1
5.262 · 10−2
2
3.824 · 10−5
3
5.313 · 10−16
TABLE 5.3. Convergence of the cyclic Jacobi algorithm
Formulae (5.63) and (5.62) are implemented in Programs 38 and 39.
Program 38 - psinorm : Evaluation of Ψ(A)
function [psi]=psinorm(A)
n=max(size(A)); psi=0;
for i=1:(n-1), for j=(i+1):n, psi=psi+A(i,j)ˆ2+A(j,i)ˆ2; end; end; psi=sqrt(psi);
Program 39 - symschur : Evaluation of c and s
function [c,s]=symschur(A,p,q)
if (A(p,q)==0), c=1; s=0; else,
eta=(A(q,q)-A(p,p))/(2*A(p,q));
if (eta >= 0), t=1/(eta+sqrt(1+etaˆ2));
else, t=-1/(-eta+sqrt(1+etaˆ2)); end; c=1/sqrt(1+tˆ2); s=c*t;
end
A coding of the cyclic Jacobi method is implemented in Program 40.
This program gets as input parameters the symmetric matrix A ∈Rn×n
and a tolerance toll. The program returns a matrix D = GT AG, G be-
ing orthogonal, such that Ψ(D) ≤toll∥A∥F , the value of Ψ(D) and the
number of sweeps to achieve convergence.
Program 40 - cycjacobi : Cyclic Jacobi method for symmetric matrices
function [D,sweep,psi]=cycjacobi(A,toll)
n=max(size(A)); D=A; psiD=norm(A,’fro’);
epsi=toll*psiD; psiD=psinorm(D); [psi]=psiD; sweep=0;
while (psiD > epsi), sweep=sweep+1;
for p=1:(n-1), for q=(p+1):n
[c,s]=symschur(D,p,q); [D]=gacol(D,c,s,1,n,p,q); [D]=garow(D,c,s,p,q,1,n);
end; end; psiD=psinorm(D); psi=[psi; psiD];
end

230
5. Approximation of Eigenvalues and Eigenvectors
5.10.2
The Method of Sturm Sequences
In this section we deal with the calculation of the eigenvalues of a real,
tridiagonal and symmetric matrix T. Typical instances of such a problem
arise when applying the Householder transformation to a given symmetric
matrix A (see Section 5.6.2) or when solving boundary value problems in
one spatial dimension (see for an example Section 5.12.1).
We analyze the method of Sturm sequences, or Givens method, introduced
in [Giv54]. For i = 1, . . . , n, we denote by di the diagonal entries of T and by
bi, i = 1, . . . , n−1, the elements of the upper and lower subdiagonals of T.
We shall assume that bi ̸= 0 for any i. Otherwise, indeed, the computation
reduces to problems of less complexity.
Letting Ti be the principal minor of order i of matrix T and p0(x) = 1,
we deﬁne for i = 1, . . . , n the following sequence of polynomials pi(x) =
det(Ti −xIi)
p1(x) = d1 −x
pi(x) = (di −x)pi−1(x) −b2
i−1pi−2(x),
i = 2, . . . , n.
(5.65)
It can be checked that pn is the characteristic polynomial of T; the com-
putational cost of its evaluation at point x is of the order of 2n ﬂops. The
sequence (5.65) is called the Sturm sequence owing to the following result,
for whose proof we refer to [Wil65], Chapter 2, Section 47 and Chapter 5,
Section 37.
Property 5.11 (of Sturm sequence) For i = 2, . . . , n the eigenvalues
of Ti−1 strictly separate those of Ti, that is
λi(Ti) < λi−1(Ti−1) < λi−1(Ti) < . . . < λ2(Ti) < λ1(Ti−1) < λ1(Ti).
Moreover, letting for any real number µ
Sµ = {p0(µ), p1(µ), . . . , pn(µ)},
the number s(µ) of sign changes in Sµ yields the number of eigenvalues of T
that are strictly less than µ, with the convention that pi(µ) has opposite sign
to pi−1(µ) if pi(µ) = 0 (two consecutive elements in the sequence cannot
vanish at the same value of µ).
Example 5.16 Let T be the tridiagonal part of the Hilbert matrix H4 ∈R4×4,
having entries hij = 1/(i + j −1). The eigenvalues of T are (to ﬁve signiﬁcant
ﬁgures) λ1 = 1.2813, λ2 = 0.4205, λ3 = −0.1417 and λ4 = 0.1161. Taking µ = 0,
Program 41 computes the following Sturm sequence
S0 = {p0(0), p1(0), p2(0), p3(0), p4(0)} = {1, 1, 0.0833, −0.0458, −0.0089}
from which, applying Property 5.11, one concludes that matrix T has one eigen-
value less than 0. In the case of matrix T = tridiag4(−1, 2, −1), with eigenvalues

5.10 Methods for Eigenvalues of Symmetric matrices
231
{0.38, 1.38, 2.62, 3.62} (to three signiﬁcant ﬁgures), we get, taking µ = 3
{p0(3), p1(3), p2(3), p3(3), p4(3)} = {1, −1, 0, 1, −1}
which shows that matrix T has three eigenvalues less than 3, since three sign
changes occur.
•
The Givens method for the calculation of the eigenvalues of T proceeds as
follows. Letting b0 = bn = 0, Theorem 5.2 yields the interval J = [α, β]
which contains the spectrum of T, where
α = min
1≤i≤n [di −(|bi−1| + |bi|)] ,
β = max
1≤i≤n [di + (|bi−1| + |bi|)] .
The set J is used as an initial guess in the search for generic eigenvalues
λi of matrix T, for i = 1, . . . , n, using the bisection method (see Chapter
6).
Precisely, given a(0) = α and b(0) = β, we let c(0) = (α + β)/2 and
compute s(c(0)); then, recalling Property 5.11, we let b(1) = c(0) if s(c(0)) >
(n −i), otherwise we set a(1) = c(0). After r iterations, the value c(r) =
(a(r) + b(r))/2 provides an approximation of λi within (|α| + |β|) · 2−(r+1),
as is shown in (6.9).
A systematic procedure can be set up to store any information about
the position within the interval J of the eigenvalues of T that are being
computed by the Givens method. The resulting algorithm generates a se-
quence of neighboring subintervals a(r)
j , b(r)
j , for j = 1, . . . , n, each one of
arbitrarily small length and containing one eigenvalue λj of T (for further
details, see [BMW67]).
Example 5.17 Let us employ the Givens method to compute the eigenvalue
λ2 ≃2.62 of matrix T considered in Example 5.16. Letting toll=10−4 in Program
42 we obtain the results reported in Table 5.4, which demonstrate the convergence
of the sequence c(k) to the desired eigenvalue in 13 iterations. We have denoted
for brevity, s(k) = s(c(k)). Similar results are obtained by running Program 42 to
compute the remaining eigenvalues of T.
•
k
a(k)
b(k)
c(k)
s(k)
k
a(k)
b(k)
c(k)
s(k)
0
0
4.000
2.0000
2
7
2.5938
2.625
2.6094
2
1
2.0000
4.000
3.0000
3
8
2.6094
2.625
2.6172
2
2
2.0000
3.000
2.5000
2
9
2.6094
2.625
2.6172
2
3
2.5000
3.000
2.7500
3
10
2.6172
2.625
2.6211
3
4
2.5000
2.750
2.6250
3
11
2.6172
2.621
2.6191
3
5
2.5000
2.625
2.5625
2
12
2.6172
2.619
2.6182
3
6
2.5625
2.625
2.5938
2
13
2.6172
2.618
2.6177
2
TABLE 5.4. Convergence of the Givens method for the calculation of the eigen-
value λ2 of the matrix T in Example 5.16

232
5. Approximation of Eigenvalues and Eigenvectors
An implementation of the polynomial evaluation (5.65) is given in Pro-
gram 41. This program receives in input the vectors dd and bb containing
the main and the upper diagonals of T. The output values pi(x) are stored,
for i = 0, . . . , n, in the vector p.
Program 41 - sturm : Sturm sequence evaluation
function [p]=sturm(dd,bb,x)
n=length(dd); p(1)=1; p(2)=d(1)-x;
for i=2:n, p(i+1)=(dd(i)-x)*p(i)-bb(i-1)ˆ2*p(i-1); end
A basic implementation of the Givens method is provided in Program
42. In input, ind is the pointer to the searched eigenvalue, while the other
parameters are similar to those in Program 41. In output the values of
the elements of sequences a(k), b(k) and c(k) are returned, together with
the required number of iterations niter and the sequence of sign changes
s(c(k)).
Program 42 - givsturm : Givens method using the Sturm sequence
function [ak,bk,ck,nch,niter]=givsturm(dd,bb,ind,toll)
[a, b]=bound(dd,bb); dist=abs(b-a); s=abs(b)+abs(a);
n=length(d); niter=0; nch=[];
while (dist > (toll*s)),
niter=niter+1; c=(b+a)/2;
ak(niter)=a; bk(niter)=b; ck(niter)=c;
nch(niter)=chcksign(dd,bb,c);
if (nch(niter) > (n-ind)), b=c;
else, a=c; end; dist=abs(b-a); s=abs(b)+abs(a);
end
Program 43 - chcksign : Sign changes in the Sturm sequence
function nch=chcksign(dd,bb,x)
[p]=sturm(dd,bb,x); n=length(dd); nch=0; s=0;
for i=2:(n+1),
if ((p(i)*p(i-1)) <= 0), nch=nch+1; end
if (p(i)==0), s=s+1; end
end
nch=nch-s;
Program 44 - bound : Calculation of the interval J = [α, β]
function [alfa,beta]=bound(dd,bb)
n=length(dd); alfa=dd(1)-abs(bb(1)); temp=dd(n)-abs(bb(n-1));
if (temp < alfa), alfa=temp; end;
for i=2:(n-1),
temp=dd(i)-abs(bb(i-1))-abs(bb(i));

5.11 The Lanczos Method
233
if (temp < alfa), alfa=temp; end;
end
beta=dd(1)+abs(bb(1)); temp=dd(n)+abs(bb(n-1));
if (temp > beta), beta=temp; end;
for i=2:(n-1),
temp=dd(i)+abs(bb(i-1))+abs(bb(i));
if (temp > beta), beta=temp; end;
end
5.11
The Lanczos Method
Let A ∈Rn×n be a symmetric sparse matrix, whose (real) eigenvalues are
ordered as
λ1 ≥λ2 ≥. . . ≥λn−1 ≥λn.
(5.66)
When n is very large, the Lanczos method [Lan50] described in Section
4.4.3 can be applied to approximate the extremal eigenvalues λn and λ1. It
generates a sequence of tridiagonal matrices Hm whose extremal eigenvalues
rapidly converge to the extremal eigenvalues of A.
To estimate the convergence of the tridiagonalization process, we intro-
duce the Rayleigh quotient r(x) = (xT Ax)/(xT x) associated with a nonnull
vector x ∈Rn. The following result, known as Courant-Fisher Theorem,
holds (for the proof see [GL89], p. 394)
λ1(A) = max
x∈Rn
x̸=0
r(x),
λn(A) = min
x∈Rn
x̸=0
r(x).
Its application to the matrix Hm = VT
mAVm, yields
λ1(Hm) = maxx∈Rn
x̸=0
(Vmx)T A(Vmx)
xT x
= max
∥x∥2=1 r(Hmx) ≤λ1(A)
λm(Hm) = minx∈Rn
x̸=0
(Vmx)T A(Vmx)
xT x
= min
∥x∥2=1 r(Hmx) ≥λn(A).
(5.67)
At each step of the Lanczos method, the estimates (5.67) provide a lower
and upper bound for the extremal eigenvalues of A. The convergence of the
sequences {λ1(Hm)} and {λm(Hm)} to λ1 and λn, respectively, is governed
by the following property, for whose proof we refer to [GL89], pp. 475-477.
Property 5.12 Let A ∈Rn×n be a symmetric matrix with eigenvalues
ordered as in (5.66) and let u1, . . . , un be the corresponding orthonormal
eigenvectors. If η1, . . . , ηm denote the eigenvalues of Hm, with η1 ≥η2 ≥
. . . ≥ηm, then
λ1 ≥η1 ≥λ1 −(λ1 −λn)(tan φ1)2
(Tm−1(1 + 2ρ1))2 ,

234
5. Approximation of Eigenvalues and Eigenvectors
where cos φ1 = |(q(1))T u1|, ρ1 = (λ1 −λ2)/(λ2 −λn) and Tm−1(x) is the
Chebyshev polynomial of degree m −1 (see Section 10.1.1).
A similar result holds of course for the convergence estimate of the eigen-
values ηm to λn
λn ≤ηm ≤λn + (λ1 −λn)(tan φn)2
(Tm−1(1 + 2ρn))2 ,
where ρn = (λn−1 −λn)/(λ1 −λn−1) and cos φn = |(q(n))T un|.
A naive implementation of the Lanczos algorithm can be aﬀected by nu-
merical instability due to propagation of rounding errors. In particular, the
Lanczos vectors will not verify the mutual orthogonality relation, making
the extremal properties (5.67) false. This requires careful programming of
the Lanczos iteration by incorporating suitable reorthogonalization proce-
dures as described in [GL89], Sections 9.2.3-9.2.4.
Despite this limitation, the Lanczos method has two relevant features:
it preserves the sparsity pattern of the matrix (unlike Householder tridiag-
onalization), and such a property makes it quite attractive when dealing
with large size matrices; furthermore, it converges to the extremal eigen-
values of A much more rapidly than the power method does (see [Kan66],
[GL89], p. 477).
The Lanczos method can be generalized to compute the extremal eigen-
values of an unsymmetric matrix along the same lines as in Section 4.5
in the case of the solution of a linear system. Details on the practical im-
plementation of the algorithm and a theoretical convergence analysis can
be found in [LS96] and [Jia95], while some documentation of the latest
software can be found in NETLIB/scalapack/readme.arpack (see also the
MATLAB command eigs).
An implementation of the Lanczos algorithm is provided in Program 45.
The input parameter m is the size of the Krylov subspace in the tridiago-
nalization procedure, while toll is a tolerance monitoring the size of the
increment of the computed eigenvalues between two successive iterations.
The output vectors lmin, lmax and deltaeig contain the sequences of the
approximate extremal eigenvalues and of their increments between succes-
sive iterations. Program 42 is invoked for computing the eigenvalues of the
tridiagonal matrix Hm.
Program 45 - eiglancz : Extremal eigenvalues of a symmetric matrix
function [lmin,lmax,deltaeig,k]=eiglancz(A,m,toll)
n=size(A); V=[0*[1:n]’,[1,0*[1:n-1]]’];
beta(1)=0; normb=1; k=1; deltaeig(1)=1;
while k <= m & normb >= eps & deltaeig(k) < toll

5.12 Applications
235
vk = V(:,k+1);
w = A*vk-beta(k)*V(:,k);
alpha(k)= w’*vk; w = w - alpha(k)*vk;
normb = norm(w,2); beta(k+1)=normb;
if normb ˜= 0
V=[V,w/normb];
if k==1
lmin(1)=alpha; lmax(1)=alpha;
k=k+1; deltaeig(k)=1;
else
d=alpha; b=beta(2:length(beta)-1);
[ak,bk,ck,nch,niter]=givsturm(d,b,1,toll);
lmax(k)=(ak(niter)+bk(niter))/2;
[ak,bk,ck,nch,niter]=givsturm(d,b,k,toll);
lmin(k)=(ak(niter)+bk(niter))/2;
deltaeig(k+1)=max(abs(lmin(k)-lmin(k-1)),abs(lmax(k)-lmax(k-1)));
k=k+1;
end
else
disp(’Breakdown’);
d=alpha; b=beta(2:length(beta)-1);
[ak,bk,ck,nch,niter]=givsturm(d,b,1,toll);
lmax(k)=(ak(niter)+bk(niter))/2;
[ak,bk,ck,nch,niter]=givsturm(d,b,k,toll);
lmin(k)=(ak(niter)+bk(niter))/2;
deltaeig(k+1)=max(abs(lmin(k)-lmin(k-1)),abs(lmax(k)-lmax(k-1)));
k=k+1;
end
end
k=k-1;
return
Example 5.18 Consider the eigenvalue problem for the matrix A∈Rn×n with
n = 100, having diagonal entries equal to 2 and oﬀ-diagonal entries equal to -1
on the upper and lower tenth diagonal. Program 45, with m=100 and toll=eps,
takes 10 iterations to approximate the extremal eigenvalues of A with an absolute
error of the order of the machine precision.
•
5.12
Applications
A classical problem in engineering is to determine the proper or natural
frequencies of a system (mechanical, structural or electric). Typically, this
leads to solving a matrix eigenvalue problem. Two examples coming from
structural applications are presented in the forthcoming sections where the
buckling problem of a beam and the study of the free vibrations of a bridge
are considered.

236
5. Approximation of Eigenvalues and Eigenvectors
5.12.1
Analysis of the Buckling of a Beam
Consider the homogeneous and thin beam of length L shown in Figure
5.4. The beam is simply supported at the end and is subject to a normal
compression load P at x = L. Denote by y(x) the vertical displacement
of the beam; the structure constraints demand that y(0) = y(L) = 0. Let
y
L
P
x
FIGURE 5.4. A simply supported beam subject to a normal compression load
us consider the problem of the buckling of the beam. This amounts to
determining the critical load Pcr, i.e. the smallest value of P such that an
equilibrium conﬁguration of the beam exists which is diﬀerent from being
rectilinear. Reaching the condition of critical load is a warning of structure
instability, so that it is quite important to determine its value accurately.
The explicit computation of the critical load can be worked out under
the assumption of small displacements, writing the equilibrium equation
for the structure in its deformed conﬁguration (drawn in dashed line in
Figure 5.4)
"
−E (J(x)y′(x))′ = Me(x),
0 < x < L
y(0) = y(L) = 0,
(5.68)
where E is the constant Young’s modulus of the beam and Me(x) = Py(x)
is the momentum of the load P with respect to a generic point of the beam
of abscissa x. In (5.68) we are assuming that the momentum of inertia
J can be varying along the beam, which indeed happens if the beam has
nonuniform cross-section.
Equation (5.68) expresses the equilibrium between the external momen-
tum Me and the internal momentum Mi = −E(Jy′)′ which tends to restore
the rectilinear equilibrium conﬁguration of the beam. If the stabilizing re-
action Mi prevails on the unstabilizing action Me, the equilibrium of the
initial rectilinear conﬁguration is stable. The critical situation (buckling of
the beam) clearly arises when Mi = Me.
Assume that J is constant and let α2 = P/(EJ); solving the boundary value
problem (5.68), we get the equation C sin αL = 0, which admits nontrivial

5.12 Applications
237
solutions α = (kπ)/L, k = 1, 2, . . . . Taking k = 1 yields the value of the
critical load Pcr = π2EJ
L2 .
To solve numerically the boundary value problem (5.68) it is conve-
nient to introduce for n ≥1, the discretization nodes xj = jh, with
h = L/(n + 1) and j = 1, . . . , n, thus deﬁning the vector of nodal ap-
proximate displacements uj at the internal nodes xj (where u0 = y(0) = 0,
un+1 = y(L) = 0). Then, using the ﬁnite diﬀerence method (see Section
12.2), the calculation of the critical load amounts to determining the small-
est eigenvalue of the tridiagonal symmetric and positive deﬁnite matrix
A = tridiagn(−1, 2, −1) ∈Rn×n.
It can indeed be checked that the ﬁnite diﬀerence discretization of prob-
lem (5.68) by centered diﬀerences leads to the following matrix eigenvalue
problem
Au = α2h2u,
where u ∈Rn is the vector of nodal displacements uj. The discrete coun-
terpart of condition C sin(α) = 0 requires that Ph2/(EJ) coincides with
the eigenvalues of A as P varies.
Denoting by λmin and P h
cr, the smallest eigenvalue of A and the (approx-
imate) value of the critical load, respectively, then P h
cr = (λminEJ)/h2.
Letting θ = π/(n + 1), it can be checked (see Exercise 3, Chapter 4) that
the eigenvalues of matrix A are
λj = 2(1 −cos(jθ)),
j = 1, . . . , n.
(5.69)
The numerical calculation of λmin has been carried out using the Givens
algorithm described in Section 5.10.2 and assuming n = 10. Running the
Program 42 with an absolute tolerance equal to the roundoﬀunit, the
solution λmin ≃0.081 has been obtained after 57 iterations.
It is also interesting to analyze the case where the beam has nonuniform
cross-section, since the value of the critical load, unlike the previous situa-
tion, is not exactly known a priori. We assume that, for each x ∈[0, L], the
section of the beam is rectangular, with depth a ﬁxed and height σ that
varies according to the rule
σ(x) = s

1 +
S
s −1
 + x
L −1
,2
,
0 ≤x ≤L,
where S and s are the values at the ends, with S ≥s > 0. The momentum
of inertia, as a function of x, is given by J(x) = (1/12)aσ3(x); proceeding
similarly as before, we end up with a system of linear algebraic equations
of the form
˜Au = (P/E)h2u,
where this time ˜A = tridiagn(b, d, b) is a tridiagonal, symmetric and posi-
tive deﬁnite matrix having diagonal entries di = J(xi−1/2) + J(xi+1/2), for
i = 1, . . . , n, and oﬀ-diagonal entries bi = −J(xi+1/2), for i = 1, . . . , n −1.

238
5. Approximation of Eigenvalues and Eigenvectors
Assume the following values of the parameters: a = 0.4 [m], s = a, S =
0.5 [m] and L = 10 [m]. To ensure a correct dimensional comparison, we
have multiplied by ¯J = a4/12 the smallest eigenvalue of the matrix A in the
uniform case (corresponding to S = s = a), obtaining λmin = 1.7283·10−4.
Running Program 42, with n = 10, yields in the nonuniform case the value
λmin = 2.243 · 10−4. This result conﬁrms that the critical load increases
for a beam having a wider section at x = 0, that is, the structure enters
the instability regime for higher values of the load than in the uniform
cross-section case.
5.12.2
Free Dynamic Vibration of a Bridge
We are concerned with the analysis of the free response of a bridge whose
schematic structure is shown in Figure 5.5. The number of the nodes of
the structure is equal to 2n while the number of the beams is 5n. Each
horizontal and vertical beam has a mass equal to m while the diagonal
beams have mass equal to m
√
2. The stiﬀness of each beam is represented
by the spring constant κ. The nodes labeled by “0” and “2n + 1” are
constrained to ground.
2n + 1
n + 2
2n −2
2n −1
n + 3
n + 1
2n
0
1
2
3
n −2
n −1
n
FIGURE 5.5. Schematic structure of a bridge
Denoting by x and y the vectors of the 2n nodal horizontal and vertical
displacements the free response of the bridge can be studied by solving the
generalized eigenvalue problems
Mx = λKx,
My = λKy,
(5.70)
where M = mdiag2n(α, b, α, γ, b, γ), where α = 3 +
√
2, b = (β, . . . , β)T ∈
Rn−2 with β = 3/2 +
√
2 and γ = 1 +
√
2,
K = κ

K11
K12
K12
K11

for a positive constant κ and where K12 = tridiagn(−1, −1, −1), K11 =
tridiagn(−1, d, −1) with d = (4, 5, . . . , 5, 4)T ∈Rn. The diagonal matrix
M is the mass matrix while the symmetric and positive deﬁnite matrix K
is the stiﬀness matrix.

5.12 Applications
239
0
10
20
30
40
50
60
70
80
90
100
0
10
20
30
40
50
60
70
80
90
100
FIGURE 5.6. Iterations number of the Lanczos method and of the inverse power
method versus the size 2n of matrix C. The solid and the dash-dotted curves
refer to the inverse power method (for ˜λ2n and ˜λ2n−1 respectively), while the
dashed and the dotted curves refer to the Lanczos method (still for ˜λ2n and
˜λ2n−1, respectively)
For k = 1, . . . , 2n we denote by (λk, zk) any eigenvalue/eigenvector pair
of (5.70) and call ωk = √λk the natural frequencies and zk the modes of
vibration of the bridge. The study of the free vibrations is of primary im-
portance in the design of a structure like a bridge or a multi-story building.
Indeed, if the excitation frequency of an external force (vehicles, wind or,
even worse, an earthquake) coincides with one of the natural frequencies of
the structure then a condition of resonance occurs and, as a result, large
oscillations may dangerously arise.
Let us now deal with the numerical solution of the matrix eigenvalue
problem (5.70). For this purpose we introduce the change of variable z =
M1/2x (or z = M1/2y) so that each generalized eigenvalue problem in (5.70)
can be conveniently reformulated as
Cz = ˜λz
where ˜λ = 1/λ and the matrix C = M−1/2KM−1/2 is symmetric positive
deﬁnite. This property allows us to use the Lanczos method described in
Section 5.11 and also ensures quadratic convergence of the power iterations
(see Section 5.11).
We approximate the ﬁrst two subdominant eigenvalues ˜λ2n and ˜λ2n−1
of the matrix C (i.e., its smallest and second smallest eigenvalues) in the
case m = κ = 1 using the deﬂation procedure considered in Remark 5.3.
The inverse power iteration and the Lanczos method are compared in the
computation of ˜λ2n and ˜λ2n−1 in Figure 5.6.
The results show the superiority of the Lanczos method over the inverse
iterations only when the matrix C is of small size. This is to be ascribed
to the fact that, as n grows, the progressive inﬂuence of the rounding er-

240
5. Approximation of Eigenvalues and Eigenvectors
rors causes a loss of mutual orthogonality of the Lanczos vectors and, in
turn, an increase in the number of iterations for the method to converge.
Suitable reorthogonalization procedures are thus needed to improve the
performances of the Lanczos iteration as pointed out in Section 5.11.
We conclude the free response analysis of the bridge showing in Figure
5.7 (in the case n = 5, m = 10 and κ = 1) the modes of vibration z8
and z10 corresponding to the natural frequencies ω8 = 990.42 and ω10 =
2904.59. The MATLAB built-in function eig has been employed to solve
the generalized eigenvalue problems (5.70) as explained in Section 5.9.1.
−1
0
1
2
3
4
5
6
7
−0.5
0
0.5
1
1.5
−1
0
1
2
3
4
5
6
7
−0.5
0
0.5
1
1.5
FIGURE 5.7. Modes of vibration corresponding to the natural frequencies ω8
(left) and ω10 (right). The undeformed conﬁguration of the bridge is drawn in
dotted line
5.13
Exercises
1. Using the Gershgorin theorems, localize the eigenvalues of the matrix A
which is obtained setting A = (P−1DP)T and then a13 = 0, a23 = 0, where
D=diag3(1, 50, 100) and
P =


1
1
1
10
20
30
100
50
60

.
[Solution : σ(A) = {−151.84, 80.34, 222.5}.]
2. Localize the spectrum of the matrix
A =


1
2
−1
2
7
0
−1
0
5

.
[Solution : σ(A) ⊂[−2, 9].]

5.13 Exercises
241
3. Draw the oriented graph of the matrix
A =


1
3
0
0
2
−1
−1
0
2

.
4. Check if the following matrices are reducible.
A1 =


1
0
−1
0
2
3
−2
1
−1
0
−2
0
1
−1
1
4


,
A2 =


0
0
1
0
0
0
0
1
0
1
0
0
1
0
0
0


.
[Solution : A1, reducible; A2, irreducible.]
5. Provide an estimate of the number of complex eigenvalues of the matrix
A =


−4
0
0
0.5
0
2
2
4
−3
1
0.5
0
−1
0
0
0.5
0
0.2
3
0
2
0.5
−1
3
4


.
[Hint : Check that A can be reduced to the form
A =
. M1
M2
0
M3
/
where M1 ∈R2×2 and M2 ∈R3×3. Then, study the eigenvalues of blocks M1
and M2 using the Gershgorin theorems and check that A has no complex
eigenvalues.]
6. Let A ∈Cn×n be a diagonal matrix and let $A = A + E be a perturbation
of A with eii = 0 for i = 1, . . . , n. Show that
|λi($A) −λi(A)| ≤
n

j=1
|eij|,
i = 1, . . . , n.
(5.71)
7. Apply estimate (5.71) to the case in which A and E are, for ε ≥0, the
matrices
A =
. 1
0
0
2
/
,
E =
. 0
ε
ε
0
/
.
[Solution : σ(A) = {1, 2} and σ($A) = {(3 ∓
√
1 + 4ε2)/2}.]

242
5. Approximation of Eigenvalues and Eigenvectors
8. Check that ﬁnding the zeros of a polynomial of degree ≤n with real coef-
ﬁcients
pn(x) =
n

k=0
akxk = a0 + a1x + ... + anxn,
an ̸= 0,
ak ∈R, k = 0, . . . n
is equivalent to determining the spectrum of the Frobenius matrix C ∈
Rn×n associated with pn (known as the companion matrix)
C =


−(an−1/an)
−(an−2/an)
. . .
−(a1/an)
−(a0/an)
1
0
. . .
0
0
0
1
. . .
0
0
...
...
...
...
...
0
0
. . .
1
0


.(5.72)
An important consequence of the result above is that, due to Abel’s theo-
rem, there exist in general no direct methods for computing the eigenvalues
of a given matrix, for n ≥5.
9. Show that if matrix A ∈Cn×n admits eigenvalue/eigenvector pairs (λ, x),
then the matrix UHAU, with U unitary, admits eigenvalue/eigenvector
pairs

λ, UHx

. (Similarity transformation using an orthogonal matrix).
10. Suppose that all the assumptions needed to apply the power method are
satisﬁed except for the requirement α1 ̸= 0 (see Section 5.3.1). Show that
in such a case the sequence (5.17) converges to the eigenvalue/eigenvector
pair (λ2, x2). Then, study experimentally the behaviour of the method,
computing the pair (λ1, x1) for the matrix
A =


1
−1
2
−2
0
5
6
−3
6

.
For this, use Program 26, taking q(0) = 1T /
√
3 and q(0) = w(0)/∥w(0)∥2,
respectively, where w(0) = (1/3)x2 −(2/3)x3.
[Solution : λ1 = 5, λ2 = 3, λ3 = −1 and x1 = [5, 16, 18]T , x2 = [1, 6, 4]T ,
x3 = [5, 16, 18]T .]
11. Show that the companion matrix associated with the polynomial pn(x) =
xn + anxn−1 + . . . + a1, can be written in the alternative form (5.72)
A =


0
a1
0
−1
0
a2
...
...
...
−1
0
an−1
0
−1
an


.
12. (From [FF63]) Suppose that a real matrix A ∈Rn×n has two maximum
module complex eigenvalues given by λ1 = ρeiθ and λ2 = ρe−iθ, with

5.13 Exercises
243
θ ̸= 0. Assume, moreover, that the remaining eigenvalues have modules
less than ρ. The power method can then be modiﬁed as follows:
let q(0) be a real vector and q(k) be the vector provided by the power
method without normalization. Then, set xk = q(k)
n0 for some n0, with
1 ≤n0 ≤n. Prove that
ρ2 = xkxk+2 −x2
k+1
xk−1xk+1 −x2
k
+ O

∥λ3
ρ ∥k

,
cos(θ) = ρxk−1 + r−1xk+1
2xk
+ O

∥λ3
ρ ∥k

.
[Hint : ﬁrst, show that
xk = C(ρk cos(kθ + α)) + O

∥λ3
ρ ∥k

,
where α depends on the components of the initial vector along the direc-
tions of the eigenvectors associated with λ1 and λ2.]
13. Apply the modiﬁed power method of Exercise 12 to the matrix
A =


1
−1
4
1
4
1
0
0
0
1
0

,
and compare the obtained results with those yielded by the standard power
method.
14. (Taken from [Dem97]). Apply the QR iteration with double shift to com-
pute the eigenvalues of the matrix
A =


0
0
1
1
0
0
0
1
0

.
Run Program 37 setting toll=eps, itmax=100 and comment about the
form of the obtained matrix T(iter) after iter iterations of the algorithm.
[Solution : the eigenvalues of A are the solution of λ3 −1 = 0, i.e.,
σ(A) =

1, −1/2 ±
√
3/2i

. After iter=100 iterations, Program 37 yields
the matrix
T(100) =


0
0
−1
1
0
0
0
−1
0

,
which means that the QR iteration leaves A unchanged (except for sign
changes that are non relevant for eigenvalues computation). This is a simple
but glaring example of matrix for which the QR method with double shift
fails to converge.]

6
Rootﬁnding for Nonlinear Equations
This chapter deals with the numerical approximation of the zeros of a real-
valued function of one variable, that is
given f : I = (a, b) ⊆R →R, ﬁnd α ∈C such that f(α) = 0.
(6.1)
The analysis of problem (6.1) in the case of systems of nonlinear equations
will be addressed in Chapter 7.
Methods for the numerical approximation of a zero of f are usually iter-
ative. The aim is to generate a sequence of values x(k) such that
lim
k→∞x(k) = α.
The convergence of the iteration is characterized by the following deﬁnition.
Deﬁnition 6.1 A sequence

x(k)
generated by a numerical method is
said to converge to α with order p ≥1 if
∃C > 0 : |x(k+1) −α|
|x(k) −α|p ≤C, ∀k ≥k0,
(6.2)
where k0 ≥0 is a suitable integer. In such a case, the method is said to be
of order p. Notice that if p is equal to 1, in order for x(k) to converge to
α it is necessary that C < 1 in (6.2). In such an event, the constant C is
called the convergence factor of the method.
■
Unlike the case of linear systems, convergence of iterative methods for
rootﬁnding of nonlinear equations depends in general on the choice of the

246
6. Rootﬁnding for Nonlinear Equations
initial datum x(0). This allows for establishing only local convergence re-
sults, that is, holding for any x(0) which belongs to a suitable neighborhood
of the root α. Methods for which convergence to α holds for any choice of
x(0) in the interval I, are said to be globally convergent to α.
6.1
Conditioning of a Nonlinear Equation
Consider the nonlinear equation f(x) = ϕ(x) −d = 0 and assume that f
is a continuously diﬀerentiable function. Let us analyze the sensitivity of
ﬁnding the roots of f with respect to changes in the datum d.
The problem is well posed only if the function ϕ is invertible. In such a
case, indeed, one gets α = ϕ−1(d) from which, using the notation of Chapter
2, the resolvent G is ϕ−1. On the other hand, (ϕ−1)′(d) = 1/ϕ′(α), so that
formula (2.7) for the approximate condition number (relative and absolute)
yields
K(d) ≃
|d|
|α||f ′(α)|,
Kabs(d) ≃
1
|f ′(α)|.
(6.3)
The problem is thus ill-conditioned when f ′(α) is “small” and well-condi-
tioned if f ′(α) is “large”.
The analysis which leads to (6.3) can be generalized to the case in which
α is a root of f with multiplicity m > 1 as follows. Expanding ϕ in a Taylor
series around α up to the m-th order term, we get
d + δd = ϕ(α + δα) = ϕ(α) +
m

k=1
ϕ(k)(α)
k!
(δα)k + o((δα)m).
Since ϕ(k)(α) = 0 for k = 1, . . . , m −1, we obtain
δd = f (m)(α)(δα)m/m!
so that an approximation to the absolute condition number is
Kabs(d) ≃

m!δd
f (m)(α)

1/m
1
|δd|.
(6.4)
Notice that (6.3) is the special case of (6.4) where m = 1. From this it also
follows that, even if δd is suﬃciently small to make |m!δd/f (m)(α)| < 1,
Kabs(d) could nevertheless be a large number. We therefore conclude that
the problem of rootﬁnding of a nonlinear equation is well-conditioned if α
is a simple root and |f ′(α)| is deﬁnitely diﬀerent from zero, ill-conditioned
otherwise.
Let us now consider the following problem, which is closely connected
with the previous analysis. Assume d = 0 and let α be a simple root of f;

6.1 Conditioning of a Nonlinear Equation
247
moreover, for ˆα ̸= α, let f(ˆα) = ˆr ̸= 0. We seek a bound for the diﬀerence
ˆα −α as a function of the residual ˆr. Applying (6.3) yields
Kabs(0) ≃
1
|f ′(α)|.
Therefore, letting δx = ˆα −α and δd = ˆr in the deﬁnition of Kabs (see
(2.5)), we get
|ˆα −α|
|α|
≲
|ˆr|
|f ′(α)||α|,
(6.5)
where the following convention has been adopted: if a ≤b and a ≃c, then
we write a ≲c. If α has multiplicity m > 1, using (6.4) instead of (6.3) and
proceeding as above, we get
|ˆα −α|
|α|
≲

m!
|f (m)(α)||α|m
1/m
|ˆr|1/m.
(6.6)
These estimates will be useful in the analysis of stopping criteria for itera-
tive methods (see Section 6.5).
A remarkable example of a nonlinear problem is when f is a polynomial
pn of degree n, in which case it admits exactly n roots αi, real or complex,
each one counted with its multiplicity. We want to investigate the sensitivity
of the roots of pn with respect to the changes of its coeﬃcients.
To this end, let ˆpn = pn + qn, where qn is a perturbation polynomial of
degree n, and let ˆαi be the corresponding roots of ˆpn. A direct use of (6.6)
yields for any root αi the following estimate
Ei
rel = |ˆαi −αi|
|αi|
≲

m!
|p(m)
n
(αi)||αi|m
1/m
|qn(ˆαi)|1/m = Si,
(6.7)
where m is the multiplicity of the root at hand and qn(ˆαi) = −pn(ˆαi) is
the “residual” of the polynomial pn evaluated at the perturbed root.
Remark 6.1 A formal analogy exists between the a priori estimates so
far obtained for the nonlinear problem ϕ(α) = d and those developed in
Section 3.1.2 for linear systems, provided that A corresponds to ϕ and b
to d. More precisely, (6.5) is the analogue of (3.9) if δA=0, and the same
holds for (6.7) (for m = 1) if δb = 0.
■
Example 6.1 Let p4(x) = (x−1)4, and let ˆp4(x) = (x−1)4 −ε, with 0 < ε ≪1.
The roots of the perturbed polynomial are simple and equal to ˆαi = αi +
4√ε,
where αi = 1 are the (coincident) zeros of p4. They lie with intervals of π/2 on
the circle of radius
4√ε and center z = (1, 0) in the complex plane.

248
6. Rootﬁnding for Nonlinear Equations
The problem is stable (that is limε→0 ˆαi = 1), but is ill-conditioned since
|ˆαi −αi|
|αi|
=
4√ε,
i = 1, . . . 4,
For example, if ε = 10−4 the relative change is 10−1. Notice that the right-side
of (6.7) is just
4√ε, so that, in this case, (6.7) becomes an equality.
•
Example 6.2 (Wilkinson). Consider the following polynomial
p10(x) = Π10
k=1(x + k) = x10 + 55x9 + . . . + 10!.
Let ˆp10 = p10 + εx9, with ε = 2−23 ≃1.2 · 10−7. Let us study the conditioning of
ﬁnding the roots of p10. Using (6.7) with m = 1, we report for i = 1, . . . , 10 in
Table 6.1 the relative errors Ei
rel and the corresponding estimates Si.
These results show that the problem is ill-conditioned, since the maximum
relative error for the root α8 = −8 is three orders of magnitude larger than
the corresponding absolute perturbation. Moreover, excellent agreement can be
observed between the a priori estimate and the actual relative error.
•
i
Ei
rel
Si
i
Ei
rel
Si
1
3.039 · 10−13
3.285 · 10−13
6
6.956 · 10−5
6.956 · 10−5
2
7.562 · 10−10
7.568 · 10−10
7
1.589 · 10−4
1.588 · 10−4
3
7.758 · 10−8
7.759 · 10−8
8
1.984 · 10−4
1.987 · 10−4
4
1.808 · 10−6
1.808 · 10−6
9
1.273 · 10−4
1.271 · 10−4
5
1.616 · 10−5
1.616 · 10−5
10
3.283 · 10−5
3.286 · 10−5
TABLE 6.1. Relative error and estimated error using (6.7) for the Wilkinson
polynomial of degree 10
6.2
A Geometric Approach to Rootﬁnding
In this section we introduce the following methods for ﬁnding roots: the
bisection method, the chord method, the secant method, the false position
(or Regula Falsi) method and Newton’s method. The order of the presen-
tation reﬂects the growing complexity of the algorithms. In the case of the
bisection method, indeed, the only information that is being used is the sign
of the function f at the end points of any bisection (sub)interval, whilst
the remaining algorithms also take into account the values of the function
and/or its derivative.
6.2.1
The Bisection Method
The bisection method is based on the following property.

6.2 A Geometric Approach to Rootﬁnding
249
Property 6.1 (theorem of zeros for continuous functions) Given a
continuous function f : [a, b] →R, such that f(a)f(b) < 0, then ∃α ∈(a, b)
such that f(α) = 0.
Starting from I0 = [a, b], the bisection method generates a sequence of
subintervals Ik = [a(k), b(k)], k ≥0, with Ik ⊂Ik−1, k ≥1, and enjoys the
property that f(a(k))f(b(k)) < 0. Precisely, we set a(0) = a, b(0) = b and
x(0) = (a(0) + b(0))/2; then, for k ≥0:
set a(k+1) = a(k), b(k+1) = x(k)
if f(x(k))f(a(k)) < 0;
set a(k+1) = x(k), b(k+1) = b(k)
if f(x(k))f(b(k)) < 0;
ﬁnally, set x(k+1) = (a(k+1) + b(k+1))/2.
x(1)
x
y
f(x)
a
b
α
I1
I0
x(0)
0
5
10
15
20
25
30
10
−12
10
−10
10
−8
10
−6
10
−4
10
−2
10
0
FIGURE 6.1. The bisection method. The ﬁrst two steps (left); convergence history
for the Example 6.3 (right). The number of iterations and the absolute error as
a function of k are reported on the x- and y-axis, respectively
The bisection iteration terminates at the m-th step for which |x(m)−α| ≤
|Im| ≤ε, where ε is a ﬁxed tolerance and |Im| is the length of Im. As for
the speed of convergence of the bisection method, notice that |I0| = b −a,
while
|Ik| = |I0|/2k = (b −a)/2k,
k ≥0.
(6.8)
Denoting by e(k) = x(k) −α the absolute error at step k, from (6.8) it
follows that |e(k)| ≤(b −a)/2k, k ≥0, which implies limk→∞|e(k)| = 0.
The bisection method is therefore globally convergent. Moreover, to get
|x(m) −α| ≤ε we must take
m ≥log2(b −a) −log2(ε) = log((b −a)/ε)
log(2)
≃log((b −a)/ε)
0.6931
.
(6.9)
In particular, to gain a signiﬁcant ﬁgure in the accuracy of the approxi-
mation of the root (that is, to have |x(k) −α| = |x(j) −α|/10), one needs

250
6. Rootﬁnding for Nonlinear Equations
k−j = log2(10) ≃3.32 bisections. This singles out the bisection method as
an algorithm of certain, but slow, convergence. We must also point out that
the bisection method does not generally guarantee a monotone reduction
of the absolute error between two successive iterations, that is, we cannot
ensure a priori that
|e(k+1)| ≤Mk|e(k)|,
for any k ≥0,
(6.10)
with Mk < 1. For this purpose, consider the situation depicted in Figure
6.1 (left), where clearly |e(1)| > |e(0)|. Failure to satisfy (6.10) does not
allow for qualifying the bisection method as a method of order 1, in the
sense of Deﬁnition 6.1.
Example 6.3 Let us check the convergence properties of the bisection method
in the approximation of the root α ≃0.9062 of the Legendre polynomial of degree
5
L5(x) = x
8 (63x4 −70x2 + 15),
whose roots lie within the interval (−1, 1) (see Section 10.1.2). Program 46 has
been run taking a = 0.6, b = 1 (whence, L5(a) · L5(b) < 0), nmax = 100,
toll = 10−10 and has reached convergence in 32 iterations, this agrees with
the theoretical estimate (6.9) (indeed, m ≥31.8974). The convergence history is
reported in Figure 6.1 (right) and shows an (average) reduction of the error by a
factor of two, with an oscillating behavior of the sequence {x(k)}.
•
The slow reduction of the error suggests employing the bisection method
as an “approaching” technique to the root. Indeed, taking few bisection
steps, a reasonable approximation to α is obtained, starting from which a
higher order method can be successfully used for a rapid convergence to
the solution within the ﬁxed tolerance. An example of such a procedure
will be addressed in Section 6.7.1.
The bisection algorithm is implemented in Program 46. The input pa-
rameters, here and in the remainder of this chapter, have the following
meaning: a and b denote the end points of the search interval, fun is the
variable containing the expression of the function f, toll is a ﬁxed toler-
ance and nmax is the maximum admissible number of steps for the iterative
process.
In the output vectors xvect, xdif and fx the sequences {x(k)}, {|x(k+1)−
x(k)|} and {f(x(k))}, for k ≥0, are respectively stored, while nit denotes
the number of iterations needed to satisfy the stopping criteria. In the case
of the bisection method, the code returns as soon as the half-length of the
search interval is less than toll.
Program 46 - bisect : Bisection method
function [xvect,xdif,fx,nit]=bisect(a,b,toll,nmax,fun)
err=toll+1; nit=0; xvect=[]; fx=[]; xdif=[];
while (nit < nmax & err > toll)

6.2 A Geometric Approach to Rootﬁnding
251
nit=nit+1; c=(a+b)/2; x=c; fc=eval(fun); xvect=[xvect;x];
fx=[fx;fc]; x=a; if (fc*eval(fun) > 0), a=c; else, b=c; end;
err=abs(b-a); xdif=[xdif;err];
end;
6.2.2
The Methods of Chord, Secant and Regula Falsi and
Newton’s Method
In order to devise algorithms with better convergence properties than the
bisection method, it is necessary to include information from the values
attained by f and, possibly, also by its derivative f ′ (if f is diﬀerentiable)
or by a suitable approximation.
For this purpose, let us expand f in a Taylor series around α and truncate
the expansion at the ﬁrst order. The following linearized version of problem
(6.1) is obtained
f(α) = 0 = f(x) + (α −x)f ′(ξ),
(6.11)
for a suitable ξ between α and x. Equation (6.11) prompts the following
iterative method: for any k ≥0, given x(k), determine x(k+1) by solving
equation f(x(k)) + (x(k+1) −x(k))qk = 0, where qk is a suitable approxima-
tion of f ′(x(k)).
The method described here amounts to ﬁnding the intersection between
the x-axis and the straight line of slope qk passing through the point
(x(k), f(x(k))), and thus can be more conveniently set up in the form
x(k+1) = x(k) −q−1
k f(x(k)),
∀k ≥0.
We consider below four particular choices of qk.
f(x)
x(1)
y
b
a
f(x)
x(0)
x
y
b
a
x(1)
x
x(2)
α
α
x(3)
FIGURE 6.2. The ﬁrst step of the chord method (left) and the ﬁrst three steps
of the secant method (right). For this method we set x(−1) = b and x(0) = a

252
6. Rootﬁnding for Nonlinear Equations
The chord method. We let
qk = q = f(b) −f(a)
b −a
,
∀k ≥0
from which, given an initial value x(0), the following recursive relation is
obtained
x(k+1) = x(k) −
b −a
f(b) −f(a)f(x(k)),
k ≥0.
(6.12)
In Section 6.3.1, we shall see that the sequence {x(k)} generated by (6.12)
converges to the root α with order of convergence p = 1.
The secant method. We let
qk = f(x(k)) −f(x(k−1))
x(k) −x(k−1)
,
∀k ≥0
(6.13)
from which, giving two initial values x(−1) and x(0), we obtain the following
relation
x(k+1) = x(k) −
x(k) −x(k−1)
f(x(k)) −f(x(k−1))f(x(k)),
k ≥0.
(6.14)
If compared with the chord method, the iterative process (6.14) requires
an extra initial point x(−1) and the corresponding function value f(x(−1)),
as well as, for any k, computing the incremental ratio (6.13). The beneﬁt
due to the increase in the computational cost is the higher speed of con-
vergence of the secant method, as stated in the following property which
can be regarded as a ﬁrst example of the local convergence theorem (for the
proof see [IK66], pp. 99-101).
Property 6.2 Let f ∈C2(J ), J being a suitable neighborhood of the root
α and assume that f ′′(α) ̸= 0. Then, if the initial data x(−1) and x(0) are
chosen in J suﬃciently close to α, the sequence (6.14) converges to α with
order p = (1 +
√
5)/2 ≃1.63.
The Regula Falsi (or false position) method. This is a variant of
the secant method in which, instead of selecting the secant line through
the values (x(k), f(x(k)) and (x(k−1), f(x(k−1)), we take the one through
(x(k), f(x(k)) and (x(k′), f(x(k′)), k′ being the maximum index less than k
such that f(x(k′)) · f(x(k)) < 0. Precisely, once two values x(−1) and x(0)
have been found such that f(x(−1)) · f(x(0)) < 0, we let
x(k+1) = x(k) −
x(k) −x(k′)
f(x(k)) −f(x(k′))f(x(k)),
k ≥0.
(6.15)

6.2 A Geometric Approach to Rootﬁnding
253
Having ﬁxed an absolute tolerance ε, the iteration (6.15) terminates at the
m-th step such that |f(x(m))| < ε. Notice that the sequence of indices k′
is nondecreasing; therefore, in order to ﬁnd at step k the new value of k′,
it is not necessary to sweep all the sequence back, but it suﬃces to stop at
the value of k′ that has been determined at the previous step. We show in
Figure 6.3 (left) the ﬁrst two steps of (6.15) in the special case in which
x(k′) coincides with x(−1) for any k ≥0.
The Regula Falsi method, though of the same complexity as the secant
method, has linear convergence order (see, for example, [RR78], pp. 339-
340). However, unlike the secant method, the iterates generated by (6.15)
are all contained within the starting interval [x(−1), x(0)].
In Figure 6.3 (right), the ﬁrst two iterations of both the secant and Regula
Falsi methods are shown, starting from the same initial data x(−1) and x(0).
Notice that the iterate x(1) computed by the secant method coincides with
that computed by the Regula Falsi method, while the value x(2) computed
by the former method (and denoted in the ﬁgure by x(2)
Sec) falls outside the
searching interval [x(−1), x(0)].
In this respect, the Regula Falsi method, as well as the bisection method,
can be regarded as a globally convergent method.
f(x)
y
f(x)
x(−1)
x(0)
x
x(1)
x(2)
y
x(0)
x
x(−1)
x(1)
x(2)
x(2)
Sec
FIGURE 6.3. The ﬁrst two steps of the Regula Falsi method for two diﬀerent
functions
Newton’s method.
Assuming that f ∈C1(I) and that f ′(α) ̸= 0 (i.e., α is a simple root of f),
if we let
qk = f ′(x(k)),
∀k ≥0
and assign the initial value x(0), we obtain the so called Newton’s method
x(k+1) = x(k) −f(x(k))
f ′(x(k)),
k ≥0.
(6.16)

254
6. Rootﬁnding for Nonlinear Equations
x(2)
a
b
x
f(x)
y
x(1)
x(0)
0
5
10
15
20
25
30
35
10
−15
10
−10
10
−5
10
0
(1)
(2)
(3)
(4)
FIGURE 6.4. The ﬁrst two steps of Newton’s method (left); convergence histories
in Example 6.4 for the chord method (1), bisection method (2), secant method
(3) and Newton’s method (4) (right). The number of iterations and the absolute
error as a function of k are shown on the x-axis and y-axis, respectively
At the k-th iteration, Newton’s method requires the two functional evalu-
ations f(x(k)) and f ′(x(k)). The increasing computational cost with respect
to the methods previously considered is more than compensated for by a
higher order of convergence, Newton’s method being of order 2 (see Section
6.3.1).
Example 6.4 Let us compare the methods introduced so far for the approxima-
tion of the root α ≃0.5149 of the function f(x) = cos2(2x) −x2 in the interval
(0, 1.5). The tolerance ε on the absolute error has been taken equal to 10−10 and
the convergence histories are drawn in Figure 6.4 (right). For all methods, the
initial guess x(0) has been set equal to 0.75. For the secant method we chose
x(−1) = 0.
The analysis of the results singles out the slow convergence of the chord
method. The error curve for the Regula Falsi method is similar to that of se-
cant method, thus it was not reported in Figure 6.4.
It is interesting to compare the performances of Newton’s and secant methods
(both having order p > 1), in terms of their computational eﬀort. It can indeed
be proven that it is more convenient to employ the secant method whenever the
number of ﬂoating point operations to evaluate f ′ are about twice those needed
for evaluating f (see [Atk89], pp. 71-73). In the example at hand, Newton’s
method converges to α in 6 iterations, instead of 7, but the secant method takes
94 ﬂops instead of 177 ﬂops required by Newton’s method.
•
The chord, secant, Regula Falsi and Newton’s methods are implemented
in Programs 47, 48, 49 and 50, respectively. Here and in the rest of the
chapter, x0 and xm1 denote the initial data x(0) and x(−1). In the case of
the Regula Falsi method the stopping test checks is |f(x(k))| < toll, while
for the other methods the test is |x(k+1) −x(k)| < toll. The string dfun
contains the expression of f ′ to be used in the Newton method.

6.2 A Geometric Approach to Rootﬁnding
255
Program 47 - chord : The chord method
function [xvect,xdif,fx,nit]=chord(a,b,x0,nmax,toll,fun)
x=a; fa=eval(fun); x=b; fb=eval(fun); r=(fb-fa)/(b-a);
err=toll+1; nit=0; xvect=x0; x=x0; fx=eval(fun); xdif=[];
while (nit < nmax & err > toll),
nit=nit+1; x=xvect(nit); xn=x-fx(nit)/r; err=abs(xn-x);
xdif=[xdif; err]; x=xn; xvect=[xvect;x]; fx=[fx;eval(fun)];
end;
Program 48 - secant : The secant method
function [xvect,xdif,fx,nit]=secant(xm1,x0,nmax,toll,fun)
x=xm1; fxm1=eval(fun); xvect=[x]; fx=[fxm1]; x=x0; fx0=eval(fun);
xvect=[xvect;x]; fx=[fx;fx0]; err=toll+1; nit=0; xdif=[];
while (nit < nmax & err > toll),
nit=nit+1; x=x0-fx0*(x0-xm1)/(fx0-fxm1); xvect=[xvect;x];
fnew=eval(fun); fx=[fx;fnew]; err=abs(x0-x); xdif=[xdif;err];
xm1=x0; fxm1=fx0; x0=x; fx0=fnew;
end;
Program 49 - regfalsi : The Regula Falsi method
function [xvect,xdif,fx,nit]=regfalsi(xm1,x0,toll,nmax,fun)
nit=0; x=xm1; f=eval(fun); fx=[f]; x=x0; f=eval(fun); fx=[fx, f];
xvect=[xm1,x0]; xdif=[]; f=toll+1; kprime=1;
while (nit < nmax & (abs(f) > toll),
nit=nit+1; dim=length(xvect);
x=xvect(dim); fxk=eval(fun); xk=x; i=dim;
while (i >= kprime), i=i-1; x=xvect(i); fxkpr=eval(fun);
if ((fxkpr*fxk) < 0), xkpr=x; kprime=i; break; end;
end;
x=xk-fxk*(xk-xkpr)/(fxk-fxkpr); xvect=[xvect, x]; f=eval(fun);
fx=[fx, f]; err=abs(x-xkpr); xdif=[xdif, err];
end;
Program 50 - newton : Newton’s method
function [xvect,xdif,fx,nit]=newton(x0,nmax,toll,fun,dfun)
err=toll+1; nit=0; xvect=x0; x=x0; fx=eval(fun); xdif=[];
while (nit < nmax & err > toll),
nit=nit+1; x=xvect(nit); dfx=eval(dfun);
if (dfx == 0), err=toll*1.e-10;
disp(’ Stop for vanishing dfun ’);
else,
xn=x-fx(nit)/dfx; err=abs(xn-x); xdif=[xdif; err];
x=xn; xvect=[xvect;x]; fx=[fx;eval(fun)];

256
6. Rootﬁnding for Nonlinear Equations
end;
end;
6.2.3
The Dekker-Brent Method
The Dekker-Brent method combines the bisection and secant methods, pro-
viding a synthesis of the advantages of both. This algorithm carries out an
iteration in which three abscissas a, b and c are present at each stage. Nor-
mally, b is the latest iterate and closest approximation to the zero, a is
the previous iterate and c is the previous or an older iterate so that f(b)
and f(c) have opposite signs. At all times b and c bracket the zero and
|f(b)| ≤|f(c)|.
Once an interval [a, b] containing at least one root α of the function
y = f(x) is found with f(a)f(b) < 0, the algorithm generates a sequence of
values a, b and c such that α always lies between b and c and, at convergence,
the half-length |c −b|/2 is less than a ﬁxed tolerance. If the function f is
suﬃciently smooth around the desired root, then the order of convergence
of the algorithm is more than linear (see [Dek69], [Bre73] Chapter 4 and
[Atk89], pp. 91-93).
In the following we describe the main lines of the algorithm as imple-
mented in the MATLAB function fzero. Throughout the parameter d will
be a correction to the point b since it is best to arrange formulae so that
they express the desired quantity as a small correction to a good approx-
imation. For example, if the new value of b were computed as (b + c)/2
(bisection step) a numerical cancellation might occur, while computing b
as b + (c −b)/2 gives a more stable formula.
Denote by ε a suitable tolerance (usually the machine precision) and let
c = b; then, the Dekker-Brent method proceeds as follows:
First, check if f(b) = 0. Should this be the case, the algorithm terminates
and returns b as the approximate zero of f. Otherwise, the following steps
are executed:
1. if f(b)f(c) > 0, set c = a, d = b −a and e = d.
2. If |f(c)| < |f(b)|, perform the exchanges a = b, b = c and c = a.
3. Set δ = 2ε max {|b|, 1} and m = (c −b)/2. If |m| ≤δ or f(b) = 0 then
the algorithm terminates and returns b as the approximate zero of f.
4. Choose bisection or interpolation.
(a) If |e| < δ or |f(a)| ≤|f(b)| then a bisection step is taken, i.e., set
d = m and e = m; otherwise, the interpolation step is executed.
(b) if a = c execute linear interpolation, i.e., compute the zero of the
straight line passing through the points (b, f(b)) and (c, f(c)) as

6.3 Fixed-point Iterations for Nonlinear Equations
257
a correction δb to the point b. This amounts to taking a step of
the secant method on the interval having b and c as end points.
If a ̸= c execute inverse quadratic interpolation, i.e., construct
the second-degree polynomial with respect to y, that interpo-
lates at the points (f(a), a), (f(b), b) and (f(c), c) and its value
at y = 0 is computed as a correction δb to the point b. Notice
that at this stage the values f(a), f(b) and f(c) are diﬀerent one
from the others, being |f(a)| > |f(b)|, f(b)f(c) < 0 and a ̸= c.
Then the algorithm checks whether the point b + δb can be ac-
cepted. This is a rather technical issue but essentially it amounts
to ascertaining if the point is inside the current interval and not
too close to the end points. This guarantees that the length of
the interval decreases by a large factor when the function is well
behaved. If the point is accepted then e = d and d = δb, i.e.,
the interpolation is actually carried out, else a bisection step is
executed by setting d = m and e = m.
5. The algorithm now updates the current iterate. Set a = b and if
|d| > δ then b = b + d else b = b + δsign(m) and go back to step 1.
Example 6.5 Let us consider the ﬁnding of roots of the function f considered in
Example 6.4, taking ε equal to the roundoﬀunit. The MATLAB function fzero
has been employed. It automatically determines the values a and b, starting from
a given initial guess ξ provided by the user. Starting from ξ = 1.5, the algorithm
ﬁnds the values a = 0.3 and b = 2.1; convergence is achieved in 5 iterations and
the sequences of the values a, b, c and f(b) are reported in Table 6.2.
Notice that the tabulated values refer to the state of the algorithm before step
3., and thus, in particular, after possible exchanges between a and b.
•
k
a
b
c
f(b)
0
2.1
0.3
2.1
0.5912
1
0.3
0.5235
0.3
−2.39 · 10−2
2
0.5235
0.5148
0.5235
3.11 · 10−4
3
0.5148
0.5149
0.5148
−8.8 · 10−7
4
0.5149
0.5149
0.5148
−3.07 · 10−11
TABLE 6.2. Solution of the equation cos2(2x) −x2 = 0 using the Dekker-Brent
algorithm. The integer k denotes the current iteration
6.3
Fixed-point Iterations for Nonlinear Equations
In this section a completely general framework for ﬁnding the roots of a
nonlinear function is provided. The method is based on the fact that, for a
given f : [a, b] →R, it is always possible to transform the problem f(x) = 0

258
6. Rootﬁnding for Nonlinear Equations
into an equivalent problem x −φ(x) = 0, where the auxiliary function
φ : [a, b] →R has to be chosen in such a way that φ(α) = α whenever
f(α) = 0. Approximating the zeros of a function has thus become the
problem of ﬁnding the ﬁxed points of the mapping φ, which is done by the
following iterative algorithm:
given x(0), let
x(k+1) = φ(x(k)),
k ≥0.
(6.17)
We say that (6.17) is a ﬁxed-point iteration and φ is its associated iteration
function. Sometimes, (6.17) is also referred to as Picard iteration or func-
tional iteration for the solution of f(x) = 0. Notice that by construction
the methods of the form (6.17) are strongly consistent in the sense of the
deﬁnition given in Section 2.2.
The choice of φ is not unique. For instance, any function of the form
φ(x) = x + F(f(x)), where F is a continuous function such that F(0) = 0,
is an admissible iteration function.
The next two results provide suﬃcient conditions in order for the ﬁxed-
point method (6.17) to converge to the root α of problem (6.1). These
conditions are stated precisely in the following theorem.
Theorem 6.1 (convergence of ﬁxed-point iterations) Consider the se-
quence x(k+1) = φ(x(k)), for k ≥0, being x(0) given. Assume that:
1. φ : [a, b] →[a, b];
2. φ ∈C1([a, b]);
3. ∃K < 1 : |φ′(x)| ≤K ∀x ∈[a, b].
Then, φ has a unique ﬁxed point α in [a, b] and the sequence {x(k)} con-
verges to α for any choice of x(0) ∈[a, b]. Moreover, we have
lim
k→∞
x(k+1) −α
x(k) −α
= φ′(α).
(6.18)
Proof. The assumption 1. and the continuity of φ ensure that the iteration
function φ has at least one ﬁxed point in [a, b]. Assumption 3. states that φ is
a contraction mapping and ensures the uniqueness of the ﬁxed point. Indeed,
suppose that there exist two distinct values α1, α2 ∈[a, b] such that φ(α1) = α1
and φ(α2) = α2. Expanding φ in a Taylor series around α1 and truncating it at
ﬁrst order, it follows that
|α2 −α1| = |φ(α2) −φ(α1)| = |φ′(η)(α2 −α1)| ≤K|α2 −α1| < |α2 −α1|,
for η ∈(α1, α2), from which it must necessarily be that α2 = α1.
The convergence analysis for the sequence {x(k)} is again based on a Taylor
series expansion. Indeed, for any k ≥0 there exists a value η(k) between α and
x(k) such that
x(k+1) −α = φ(x(k)) −φ(α) = φ′(η(k))(x(k) −α)
(6.19)

6.3 Fixed-point Iterations for Nonlinear Equations
259
from which |x(k+1) −α| ≤K|x(k) −α| ≤Kk+1|x(0) −α| →0 for k →∞. Thus,
x(k) converges to α and (6.19) implies that
lim
k→∞
x(k+1) −α
x(k) −α
= lim
k→∞φ′(η(k)) = φ′(α),
that is (6.18).
3
The quantity |φ′(α)| is called the asymptotic convergence factor and, in
analogy with the case of iterative methods for linear systems, the asymp-
totic convergence rate can be deﬁned as
R = −log
1
|φ′(α)|.
(6.20)
Theorem 6.1 ensures convergence of the sequence {x(k)} to the root α for
any choice of the initial value x(0) ∈[a, b]. As such, it represents an example
of a global convergence result.
In practice, however, it is often quite diﬃcult to determine a priori the
width of the interval [a, b]; in such a case the following convergence result
can be useful (see for the proof, [OR70]).
Property 6.3 (Ostrowski theorem) Let α be a ﬁxed point of a func-
tion φ, which is continuous and diﬀerentiable in a neighborhood J of α. If
|φ′(α)| < 1 then there exists δ > 0 such that the sequence {x(k)} converges
to α, for any x(0) such that |x(0) −α| < δ.
Remark 6.2 If |φ′(α)| > 1 it follows from (6.19) that if x(n) is suﬃciently
close to α, so that |φ′(x(n))| > 1, then |α −x(n+1)| > |α −x(n)|, thus
no convergence is possible. In the case |φ′(α)| = 1 no general conclusion
can be stated since both convergence and nonconvergence may be possible,
depending on the problem at hand.
■
Example 6.6 Let φ(x) = x −x3, which admits α = 0 as ﬁxed point. Although
φ′(α) = 1, if x(0) ∈[−1, 1] then x(k) ∈(−1, 1) for k ≥1 and it converges (very
slowly) to α (if x(0) = ±1, we even have x(k) = α for any k ≥1). Starting from
x(0) = 1/2 the absolute error after 2000 iterations is 0.0158. Let now φ(x) = x+x3
having also α = 0 as ﬁxed point. Again, φ′(α) = 1 but in this case the sequence
x(k) diverges for any choice x(0) ̸= 0.
•
We say that a ﬁxed-point method has order p (p non necessarily being an
integer) if the sequence that is generated by the method converges to the
ﬁxed point α with order p according to Deﬁnition 6.1.

260
6. Rootﬁnding for Nonlinear Equations
Property 6.4 If φ ∈Cp+1(J ) for a suitable neighborhood J of α and an
integer p ≥0, and if φ(i)(α) = 0 for 0 ≤i ≤p and φ(p+1)(α) ̸= 0, then the
ﬁxed-point method with iteration function φ has order p + 1 and
lim
k→∞
x(k+1) −α
(x(k) −α)p+1 = φ(p+1)(α)
(p + 1)! ,
p ≥0.
(6.21)
Proof. Let us expand φ in a Taylor series around x = α obtaining
x(k+1) −α =
p

i=0
φ(i)(α)
i!
(x(k) −α)i + φ(p+1)(η)
(p + 1)! (x(k) −α)p+1,
for a certain η between x(k) and α. Thus, we have
lim
k→∞
x(k+1) −α
(x(k) −α)p+1 = lim
k→∞
φ(p+1)(η)
(p + 1)! = φ(p+1)(α)
(p + 1)! .
3
The convergence of the sequence to the root α will be faster, for a ﬁxed
order p, when the quantity at right-side in (6.21) is smaller.
The ﬁxed-point method (6.17) is implemented in Program 51. The variable
phi contains the expression of the iteration function φ.
Program 51 - ﬁxpoint : Fixed-point method
function [xvect,xdif,fx,nit]=ﬁxpoint(x0,nmax,toll,fun,phi)
err=toll+1; nit=0; xvect=x0; x=x0; fx=eval(fun); xdif=[];
while (nit < nmax & err > toll),
nit=nit+1; x=xvect(nit); xn=eval(phi); err=abs(xn-x);
xdif=[xdif; err]; x=xn; xvect=[xvect;x]; fx=[fx;eval(fun)];
end;
6.3.1
Convergence Results for Some Fixed-point Methods
Theorem 6.1 provides a theoretical tool for analyzing some of the iterative
methods introduced in Section 6.2.2.
The chord method. Equation (6.12) is a special instance of (6.17), in
which we let φ(x) = φchord(x) = x−q−1f(x) = x−(b−a)/(f(b)−f(a))f(x).
If f ′(α) = 0, φ′
chord(α) = 1 and the method is not guaranteed to converge.
Otherwise, the condition |φ′
chord(α)| < 1 is equivalent to requiring that
0 < q−1f ′(α) < 2.
Therefore, the slope q of the chord must have the same sign as f ′(α),
and the search interval [a, b] has to satisfy the constraint
(b −a) < 2f(b) −f(a)
f ′(α)
.

6.4 Zeros of Algebraic Equations
261
The chord method converges in one iteration if f is a straight line, otherwise
it converges linearly, apart the (lucky) case when f ′(α) = (f(b)−f(a))/(b−
a), for which φ′
chord(α) = 0.
Newton’s method. Equation (6.16) can be cast in the general framework
(6.17) letting
φNewt(x) = x −f(x)
f ′(x).
Assuming f ′(α) ̸= 0 (that is, α is a simple root)
φ′
Newt(α) = 0,
φ′′
Newt(α) = f ′′(α)
f ′(α) .
If the root α has multiplicity m > 1, then the method (6.16) is no longer
second-order convergent. Indeed we have (see Exercise 2)
φ′
Newt(α) = 1 −1
m.
(6.22)
If the value of m is known a priori, then the quadratic convergence of
Newton’s method can be recovered by resorting to the so-called modiﬁed
Newton’s method
x(k+1) = x(k) −m f(x(k))
f ′(x(k)),
k ≥0.
(6.23)
To check the convergence order of the iteration (6.23), see Exercise 2.
6.4
Zeros of Algebraic Equations
In this section we address the special case in which f is a polynomial of
degree n ≥0, i.e., a function of the form
pn(x) =
n

k=0
akxk,
(6.24)
where ak ∈R are given coeﬃcients.
The above representation of pn is not the only one possible. Actually,
one can also write
pn(x) = an(x −α1)m1...(x −αk)mk,
k

l=1
ml = n
where αi and mi denote the i-th root of pn and its multiplicity, respectively.
Other representations are available as well, see Section 6.4.1.

262
6. Rootﬁnding for Nonlinear Equations
Notice that, since the coeﬃcients ak are real, if α is a zero of pn, then
its complex conjugate ¯α is a zero of pn too.
Abel’s theorem states that for n ≥5 there does not exist an explicit
formula for the zeros of pn (see, for instance, [MM71], Theorem 10.1). This,
in turn, motivates numerical solutions of the nonlinear equation pn(x) = 0.
Since the methods introduced so far must be provided by a suitable search
interval [a, b] or an initial guess x(0), we recall two results that can be useful
to localize the zeros of a polynomial.
Property 6.5 (Descartes’ rule of signs) Let pn ∈Pn. Denote by ν the
number of sign changes in the set of coeﬃcients {aj} and by k the number
of real positive roots of pn (each counted with its multiplicity). Then, k ≤ν
and ν −k is an even number.
Property 6.6 (Cauchy’s Theorem) All zeros of pn are contained in the
circle Γ in the complex plane
Γ = {z ∈C : |z| ≤1 + ηk} ,
where ηk =
max
0≤k≤n−1|ak/an|.
This second property is of little use if ηk ≫1. In such an event, it is con-
venient to perform a translation through a suitable change of coordinates.
6.4.1
The Horner Method and Deﬂation
In this section we describe the Horner method for eﬃciently evaluating a
polynomial (and its derivative) at a given point z. The algorithm allows for
generating automatically a procedure, called deﬂation, for the sequential
approximation of all the roots of a polynomial.
Horner’s method is based on the observation that any polynomial pn ∈
Pn can be written as
pn(x) = a0 + x(a1 + x(a2 + . . . + x(an−1 + anx) . . . )).
(6.25)
Formulae (6.24) and (6.25) are completely equivalent from an algebraic
standpoint; nevertheless, (6.24) requires n sums and 2n −1 multiplications
to evaluate pn(x), while (6.25) requires n sums and n multiplications. The
second expression, known as nested multiplications algorithm, is the basic
ingredient of Horner’s method. This method eﬃciently evaluates the poly-
nomial pn at a point z through the following synthetic division algorithm
bn = an,
bk = ak + bk+1z,
k = n −1, n −2, ..., 0,
(6.26)
which is implemented in Program 52. The coeﬃcients aj of the polynomial
are stored in vector a ordered from an back to a0.

6.4 Zeros of Algebraic Equations
263
Program 52 - horner : Synthetic division algorithm
function [pnz,b] = horner(a,n,z)
b(1)=a(1); for j=2:n+1, b(j)=a(j)+b(j-1)*z; end; pnz=b(n+1);
All the coeﬃcients bk in (6.26) depend on z and b0 = pn(z). The polynomial
qn−1(x; z) = b1 + b2x + ... + bnxn−1 =
n

k=1
bkxk−1
(6.27)
has degree n−1 in the variable x and depends on the parameter z through
the coeﬃcients bk; it is called the associated polynomial of pn.
Let us now recall the following property of polynomial division:
given two polynomials hn ∈Pn and gm ∈Pm with m ≤n, there exist
an unique polynomial δ ∈Pn−m and an unique polynomial ρ ∈Pm−1 such
that
hn(x) = gm(x)δ(x) + ρ(x).
(6.28)
Then, dividing pn by x −z, from (6.28) it follows that
pn(x) = b0 + (x −z)qn−1(x; z),
having denoted by qn−1 the quotient and by b0 the remainder of the di-
vision. If z is a zero of pn, then b0 = pn(z) = 0 and thus pn(x) =
(x −z)qn−1(x; z). In such a case, the algebraic equation qn−1(x; z) = 0
yields the n −1 remaining roots of pn(x). This observation suggests adopt-
ing the following deﬂation procedure for ﬁnding the roots of pn. For m =
n, n −1, . . . , 1:
1. ﬁnd a root r of pm using a suitable approximation method;
2. evaluate qm−1(x; r) by (6.26);
3. let pm−1 = qm−1.
In the two forthcoming sections some deﬂation methods will be ad-
dressed, making a precise choice for the scheme at point 1.
6.4.2
The Newton-Horner Method
A ﬁrst example of deﬂation employs Newton’s method for computing the
root r at step 1. of the procedure in the previous section. Implement-
ing Newton’s method fully beneﬁts from Horner’s algorithm (6.26). In-
deed, if qn−1 is the associated polynomial of pn deﬁned in (6.27), since

264
6. Rootﬁnding for Nonlinear Equations
p′
n(x) = qn−1(x; z) + (x −z)q′
n−1(x; z) then p′
n(z) = qn−1(z; z). Thanks to
this identity, the Newton-Horner method for the approximation of a root
(real or complex) rj of pn (j = 1, . . . , n) takes the following form:
given an initial estimate r(0)
j
of the root, solve for any k ≥0
r(k+1)
j
= r(k)
j
−
pn(r(k)
j
)
p′n(r(k)
j
)
= r(k)
j
−
pn(r(k)
j
)
qn−1(r(k)
j
; r(k)
j
)
.
(6.29)
Once convergence has been achieved for the iteration (6.29), polynomial
deﬂation is performed, this deﬂation being helped by the fact that pn(x) =
(x −rj)pn−1(x). Then, the approximation of a root of pn−1(x) is carried
out until all the roots of pn have been computed.
Denoting by nk = n −k the degree of the polynomial that is obtained at
each step of the deﬂation process, for k = 0, . . . , n −1, the computational
cost of each Newton-Horner iteration (6.29) is equal to 4nk. If rj ∈C, it
is necessary to work in complex arithmetic and take r(0)
j
∈C; otherwise,
indeed, the Newton-Horner method (6.29) would yield a sequence {r(k)
j
} of
real numbers.
The deﬂation procedure might be aﬀected by rounding error propagation
and, as a consequence, can lead to inaccurate results. For the sake of stabil-
ity, it is therefore convenient to approximate ﬁrst the root r1 of minimum
module, which is the most sensitive to ill-conditioning of the problem (see
Example 2.7, Chapter 2) and then to continue with the successive roots
r2, . . . , until the root of maximum module is computed. To localize r1, the
techniques described in Section 5.1 or the method of Sturm sequences can
be used (see [IK66], p. 126).
A further increase in accuracy can be obtained, once an approximation $rj
of the root rj is available, by going back to the original polynomial pn and
generating through the Newton-Horner method (6.29) a new approximation
to rj, taking as initial guess r(0)
j
= $rj. This combination of deﬂation and
successive correction of the root is called the Newton-Horner method with
reﬁnement.
Example 6.7 Let us examine the performance of the Newton-Horner method in
two cases: in the ﬁrst one, the polynomial admits real roots, while in the second
one there are two pairs of complex conjugate roots. To single out the importance
of reﬁnement, we have implemented (6.29) both switching it on and oﬀ(methods
NwtRef and Nwt, respectively). The approximate roots obtained using method Nwt
are denoted by rj, while sj are those computed by method NwtRef. As for the
numerical experiments, the computations have been done in complex arithmetic,
with x(0) = 0+i 0, i being the imaginary unit, nmax = 100 and toll = 10−5. The
tolerance for the stopping test in the reﬁnement cycle has been set to 10−3toll.
1) p5(x) = x5 + x4 −9x3 −x2 + 20x −12 = (x −1)2(x −2)(x + 2)(x + 3).

6.4 Zeros of Algebraic Equations
265
We report in Tables 6.3(a) and 6.3(b) the approximate roots rj (j = 1, . . . , 5)
and the number of Newton iterations (Nit) needed to get each of them; in the
case of method NwtRef we also show the number of extra Newton iterations for
the reﬁnement (Extra).
rj
Nit
0.99999348047830
17
1 −i3.56 · 10−25
6
2 −i2.24 · 10−13
9
−2 −i1.70 · 10−10
7
−3 + i5.62 · 10−6
1
(a)
sj
Nit
Extra
0.9999999899210124
17
10
1 −i2.40 · 10−28
6
10
2 + i1.12 · 10−22
9
1
−2 + i8.18 · 10−22
7
1
−3 −i7.06 · 10−21
1
2
(b)
TABLE 6.3. Roots of the polynomial p5. Roots computed by the Newton-Horner
method without reﬁnement (left), and with reﬁnement (right)
Notice a neat increase in the accuracy of rootﬁnding due to reﬁnement, even with
few extra iterations.
2) p6(x) = x6 −2x5 + 5x4 −6x3 + 2x2 + 8x −8.
The zeros of p6 are the complex numbers {1, −1, 1 ± i, ±2i}. We report below,
denoting them by rj, (j = 1, . . . , 6), the approximations to the roots of p6 ob-
tained using method Nwt, with a number of iterations equal to 2, 1, 1, 7, 7 and 1,
respectively. Beside, we also show the corresponding approximations sj computed
by method NwtRef and obtained with a maximum number of 2 extra iterations.
•
rj
Nwt
sj
NwtRef
r1
1
s1
1
r2
−0.99 −i9.54 · 10−17
s2
−1 + i1.23 · 10−32
r3
1+i
s3
1+i
r4
1-i
s4
1-i
r5
-1.31 · 10−8 + i2
s5
−5.66 · 10−17 + i2
r6
-i2
s6
-i2
TABLE 6.4. Roots of the polynomial p6 obtained using the Newton-Horner
method without (left) and with (right) reﬁnement
A coding of the Newton-Horner algorithm is provided in Program 53. The
input parameters are A (a vector containing the polynomial coeﬃcients), n
(the degree of the polynomial), toll (tolerance on the maximum variation
between successive iterates in Newton’s method), x0 (initial value, with
x(0) ∈R), nmax (maximum number of admissible iterations for Newton’s

266
6. Rootﬁnding for Nonlinear Equations
method) and iref (if iref = 1, then the reﬁnement procedure is activated).
For dealing with the general case of complex roots, the initial datum is
automatically converted into the complex number z = x(0) + ix(0), where
i = √−1.
The program returns as output the variables xn (a vector containing the
sequence of iterates for each zero of pn(x)), iter (a vector containing the
number of iterations needed to approximate each root), itrefin (a vector
containing the Newton iterations required to reﬁne each estimate of the
computed root) and root (vector containing the computed roots).
Program 53 - newthorn : Newton-Horner method with reﬁnement
function [xn,iter,root,itreﬁn]=newthorn(A,n,toll,x0,nmax,iref)
apoly=A;
for i=1:n, it=1; xn(it,i)=x0+sqrt(-1)*x0; err=toll+1; Ndeg=n-i+1;
if (Ndeg == 1), it=it+1; xn(it,i)=-A(2)/A(1);
else
while (it < nmax & err > toll),
[px,B]=horner(A,Ndeg,xn(it,i));
[pdx,C]=horner(B,Ndeg-1,xn(it,i));
it=it+1; if (pdx ˜=0), xn(it,i)=xn(it-1,i)-px/pdx;
err=max(abs(xn(it,i)-xn(it-1,i)),abs(px));
else,
disp(’ Stop due to a vanishing p’’ ’);
err=0; xn(it,i)=xn(it-1,i);
end
end
end
A=B;
if (iref==1), alfa=xn(it,i); itr=1; err=toll+1;
while ((err > toll*1e-3) & (itr < nmax))
[px,B]=horner(apoly,n,alfa);
[pdx,C]=horner(B,n-1,alfa); itr=itr+1;
if (pdx˜=0)
alfa2=alfa-px/pdx;
err=max(abs(alfa2-alfa),abs(px)); alfa=alfa2;
else,
disp(’ Stop due to a vanishing p’’ ’); err=0;
end
end; itreﬁn(i)=itr-1; xn(it,i)=alfa;
end
iter(i)=it-1; root(i)=xn(it,i); x0=root(i);
end

6.4 Zeros of Algebraic Equations
267
6.4.3
The Muller Method
A second example of deﬂation employs Muller’s method for ﬁnding an
approximation to the root r at step 1. of the procedure described in Section
6.4.1 (see [Mul56]). Unlike Newton’s or secant methods, Muller’s method is
able to compute complex zeros of a given function f, even starting from a
real initial datum; moreover, its order of convergence is almost quadratic.
The action of Muller’s method is drawn in Figure 6.5. The scheme ex-
tends the secant method, substituting the linear polynomial introduced in
(6.13) with a second-degree polynomial as follows. Given three distinct
values x(0), x(1) and x(2), the new point x(3) is determined by setting
p2(x(3)) = 0, where p2 ∈P2 is the unique polynomial that interpolates
f at the points x(i), i = 0, 1, 2, that is, p2(x(i)) = f(x(i)) for i = 0, 1, 2.
Therefore,
x(3)
f
p2
x(0)x(1) x(2)
FIGURE 6.5. The ﬁrst step of Muller’s method
p2(x) = f(x(2)) + (x −x(2))f[x(2), x(1)] + (x −x(2))(x −x(1))f[x(2), x(1), x(0)]
where
f[ξ, η] = f(η) −f(ξ)
η −ξ
,
f[ξ, η, τ] = f[η, τ] −f[ξ, η]
τ −ξ
are the divided diﬀerences of order 1 and 2 associated with the points ξ, η
and τ (see Section 8.2.1). Noticing that x−x(1) = (x−x(2))+(x(2) −x(1)),
we get
p2(x) = f(x(2)) + w(x −x(2)) + f[x(2), x(1), x(0)](x −x(2))2
having deﬁned
w
=
f[x(2), x(1)] + (x(2) −x(1))f[x(2), x(1), x(0)]
=
f[x(2), x(1)] + f[x(2), x(0)] −f[x(0), x(1)].

268
6. Rootﬁnding for Nonlinear Equations
Requiring that p2(x(3)) = 0 it follows that
x(3) = x(2) + −w ±

w2 −4f(x(2))f[x(2), x(1), x(0)]
1/2
2f[x(2), x(1), x(0)]
.
Similar computations must be done for getting x(4) starting from x(1), x(2)
and x(3) and, more generally, to ﬁnd x(k+1) starting from x(k−2), x(k−1)
and x(k), with k ≥2, according with the following formula (notice that the
numerator has been rationalized)
x(k+1) = x(k) −
2f(x(k))
w ∓

w2 −4f(x(k))f[x(k), x(k−1), x(k−2)]
1/2 .
(6.30)
The sign in (6.30) is chosen in such a way that the module of the denomina-
tor is maximized. Assuming that f ∈C3(J ) in a suitable neighborhood J
of the root α, with f ′(α) ̸= 0, the order of convergence is almost quadratic.
Precisely, the error e(k) = α −x(k) obeys the following relation (see for the
proof [Hil87])
lim
k→∞
|e(k+1)|
|e(k)|p = 1
6

f ′′′(α)
f ′(α)
 ,
p ≃1.84.
Example 6.8 Let us employ Muller’s method to approximate the roots of the
polynomial p6 examined in Example 6.7. The tolerance on the stopping test
is toll = 10−6, while x(0) = −5, x(1) = 0 and x(2) = 5 are the inputs to
(6.30). We report in Table 6.5 the approximate roots of p6, denoted by sj and
rj (j = 1, . . . , 5), where, as in Example 6.7, sj and rj have been obtained by
switching the reﬁnement procedure on and oﬀ, respectively. To compute the roots
rj, 12, 11, 9, 9, 2 and 1 iterations are needed, respectively, while only one extra
iteration is taken to reﬁne all the roots.
rj
sj
r1
1 + i2.2 · 10−15
s1
1 + i9.9 · 10−18
r2
−1 −i8.4 · 10−16
s2
-1
r3
0.99 + i
s3
1 + i
r4
0.99 −i
s4
1 −i
r5
−1.1 · 10−15 + i1.99
s5
i2
r6
−1.0 · 10−15 −i2
s6
-i2
TABLE 6.5. Roots of polynomial p6 with Muller’s method without (rj) and with
(sj) reﬁnement
Even in this example, one can notice the eﬀectiveness of the reﬁnement procedure,
based on Newton’s method, on the accuracy of the solution yielded by (6.30). •

6.5 Stopping Criteria
269
The Muller method is implemented in Program 54, in the special case
where f is a polynomial of degree n. The deﬂation process also includes a
reﬁnement phase; the evaluation of f(x(k−2)), f(x(k−1)) and f(x(k)), with
k ≥2, is carried out using Program 52. The input/output parameters are
analogous to those described in Program 53.
Program 54 - mulldeﬂ: Muller’s method with reﬁnement
function [xn,iter,root,itreﬁn]=mulldeﬂ(A,n,toll,x0,x1,x2,nmax,iref)
apoly=A;
for i=1:n
xn(1,i)=x0; xn(2,i)=x1; xn(3,i)=x2; it=0; err=toll+1; k=2; Ndeg=n-i+1;
if (Ndeg == 1), it=it+1; k=0; xn(it,i)=-A(2)/A(1);
else
while ((err > toll) & (it < nmax)),
k=k+1; it=it+1; [f0,B]=horner(A,Ndeg,xn(k-2,i));
[f1,B]=horner(A,Ndeg,xn(k-1,i)); [f2,B]=horner(A,Ndeg,xn(k,i));
f01=(f1-f0)/(xn(k-1,i)-xn(k-2,i)); f12=(f2-f1)/(xn(k,i)-xn(k-1,i));
f012=(f12-f01)/(xn(k,i)-xn(k-2,i)); w=f12+(xn(k,i)-xn(k-1,i))*f012;
arg=wˆ2-4*f2*f012; d1=w-sqrt(arg); d2=w+sqrt(arg); den=max(d1,d2);
if (den˜=0); xn(k+1,i)=xn(k,i)-(2*f2)/den;
err=abs(xn(k+1,i)-xn(k,i));
else
disp(’ Vanishing denominator ’); return; end;
end; end; radix=xn(k+1,i);
if (iref==1),
alfa=radix; itr=1; err=toll+1;
while ((err > toll*1e-3) & (itr < nmax)),
[px,B]=horner(apoly,n,alfa); [pdx,C]=horner(B,n-1,alfa);
if (pdx == 0), disp(’ Vanishing derivative ’); err=0; end;
itr=itr+1; if (pdx˜=0), alfa2=alfa-px/pdx;
err=abs(alfa2-alfa); alfa=alfa2; end;
end; itreﬁn(i)=itr-1; xn(k+1,i)=alfa; radix=alfa;
end
iter(i)=it; root(i)=radix; [px,B]=horner(A,Ndeg-1,xn(k+1,i)); A=B;
end
6.5
Stopping Criteria
Suppose that {x(k)} is a sequence converging to a zero α of the function
f. In this section we provide some stopping criteria for terminating the
iterative process that approximates α. Analogous to Section 4.6, where
the case of iterative methods for linear systems has been examined, there
are two possible criteria: a stopping test based on the residual and on the
increment. Below, ε is a ﬁxed tolerance on the approximate calculation of

270
6. Rootﬁnding for Nonlinear Equations
α and e(k) = α−x(k) denotes the absolute error. We shall moreover assume
that f is continuously diﬀerentiable in a suitable neighborhood of the root.
1. Control of the residual: the iterative process terminates at the ﬁrst
step k such that |f(x(k))| < ε.
Situations can arise where the test turns out to be either too restrictive or
excessively optimistic (see Figure 6.6). Applying the estimate (6.6) to the
case at hand yields
|e(k)|
|α|
≲

m!
|f (m)(α)||α|m
1/m
|f(x(k))|1/m.
In particular, in the case of simple roots, the error is bound to the residual
by the factor 1/|f ′(α)| so that the following conclusions can be drawn:
1. if |f ′(α)| ≃1, then |e(k)| ≃ε; therefore, the test provides a satisfac-
tory indication of the error;
2. if |f ′(α)| ≪1, the test is not reliable since |e(k)| could be quite large
with respect to ε;
3. if, ﬁnally, |f ′(α)| ≫1, we get |e(k)| ≪ε and the test is too restrictive.
We refer to Figure 6.6 for an illustration of the last two cases.
f(x)
x(k)
α
α
f(x)
x(k)
FIGURE 6.6. Two situations where the stopping test based on the residual
is either too restrictive (when |e(k)| ≪|f(x(k))|, left) or too optimistic (when
|e(k)| ≫|f(x(k))|, right)
The conclusions that we have drawn agree with those in Example 2.4.
Indeed, when f ′(α) ≃0, the condition number of the problem f(x) = 0 is
very high and, as a consequence, the residual does not provide a signiﬁcant
indication of the error.
2. Control of the increment: the iterative process terminates as soon as
|x(k+1) −x(k)| < ε.

6.5 Stopping Criteria
271
Let

x(k)
be generated by the ﬁxed-point method x(k+1) = φ(x(k)). Using
the mean value theorem, we get
e(k+1) = φ(α) −φ(x(k)) = φ′(ξ(k))e(k),
where ξ(k) lies between x(k) and α. Then,
x(k+1) −x(k) = e(k) −e(k+1) =
+
1 −φ′(ξ(k))
,
e(k)
so that, assuming that we can replace φ′(ξ(k)) with φ′(α), it follows that
e(k) ≃
1
1 −φ′(α)(x(k+1) −x(k)).
(6.31)
-1
1 φ′(α)
0
1
1
2
γ
FIGURE 6.7. Behavior of γ = 1/(1 −φ′(α)) as a function of φ′(α)
As shown in Figure 6.7, we can conclude that the test:
- is unsatisfactory if φ′(α) is close to 1;
- provides an optimal balancing between increment and error in the case
of methods of order 2 for which φ′(α) = 0 as is the case for Newton’s
method;
- is still satisfactory if −1 < φ′(α) < 0.
Example 6.9 The zero of the function f(x) = e−x −η is given by α = −log(η).
For η = 10−9, α ≃20.723 and f ′(α) = −e−α ≃−10−9. We are thus in the case
where |f ′(α)| ≪1 and we wish to examine the behaviour of Newton’s method in
the approximation of α when the two stopping criteria above are adopted in the
computations.
We show in Tables 6.6 and 6.7 the results obtained using the test based on the
control of the residual (1) and of the increment (2), respectively. We have taken
x(0) = 0 and used two diﬀerent values of the tolerance. The number of iterations
required by the method is denoted by nit.
According to (6.31), since φ′(α) = 0, the stopping test based on the increment
reveals to be reliable for both the values (which are quite diﬀering) of the stop
tolerance ε. The test based on the residual, instead, yields an acceptable estimate
of the root only for very small tolerances, while it is completely wrong for large
values of ε.
•

272
6. Rootﬁnding for Nonlinear Equations
ε
nit
|f(x(nit))|
|α −x(nit)|
|α −x(nit)|/α
10−10
22
5.9 · 10−11
5.7 · 10−2
0.27
10−3
7
9.1 · 10−4
13.7
66.2
TABLE
6.6.
Newton’s
method
for
the
approximation
of
the
root
of
f(x) = e−x −η = 0. The stopping test is based on the control of the residual
ε
nit
|x(nit) −x(nit−1)|
|α −x(nit)|
|α −x(nit)|/α
10−10
26
8.4 · 10−13
≃0
≃0
10−3
25
1.3 · 10−6
8.4 · 10−13
4 · 10−12
TABLE
6.7.
Newton’s
method
for
the
approximation
of
the
root
of
f(x) = e−x −η = 0. The stopping test is based on the control of the incre-
ment
6.6
Post-processing Techniques for Iterative
Methods
We conclude this chapter by introducing two algorithms that aim at ac-
celerating the convergence of iterative methods for ﬁnding the roots of a
function.
6.6.1
Aitken’s Acceleration
We describe this technique in the case of linearly convergent ﬁxed-point
methods, referring to [IK66], pp. 104–108, for the case of methods of higher
order.
Consider a ﬁxed-point iteration that is linearly converging to a zero α of
a given function f. Denoting by λ an approximation of φ′(α) to be suitably
determined and recalling (6.18) we have, for k ≥1
α
≃x(k) −λx(k−1)
1 −λ
= x(k) −λx(k) + λx(k) −λx(k−1)
1 −λ
= x(k) +
λ
1 −λ(x(k) −x(k−1)).
(6.32)
Aitken’s method provides a simple way of computing λ that is able to
accelerate the convergence of the sequence {x(k)} to the root α. With this
aim, let us consider for k ≥2 the following ratio
λ(k) =
x(k) −x(k−1)
x(k−1) −x(k−2) ,
(6.33)
and check that
lim
k→∞λ(k) = φ′(α).
(6.34)

6.6 Post-processing Techniques for Iterative Methods
273
Indeed, for k suﬃciently large
x(k+2) −α ≃φ′(α)(x(k+1) −α)
and thus, elaborating (6.33), we get
lim
k→∞λ(k) = lim
k→∞
x(k) −x(k−1)
x(k−1) −x(k−2) = lim
k→∞
(x(k) −α) −(x(k−1) −α)
(x(k−1) −α) −(x(k−2) −α)
= lim
k→∞
x(k) −α
x(k−1) −α −1
1 −x(k−2) −α
x(k−1) −α
= φ′(α) −1
1 −
1
φ′(α)
= φ′(α)
which is (6.34). Substituting in (6.32) λ with its approximation λ(k) given
by (6.33), yields the updated estimate of α
α ≃x(k) +
λ(k)
1 −λ(k) (x(k) −x(k−1))
(6.35)
which, rigorously speaking, is signiﬁcant only for a suﬃciently large k.
However, assuming that (6.35) holds for any k ≥2, we denote by x(k) the
new approximation of α that is obtained by plugging (6.33) back into (6.35)
x(k) = x(k) −
(x(k) −x(k−1))2
(x(k) −x(k−1)) −(x(k−1) −x(k−2)),
k ≥2.
(6.36)
This relation is known as Aitken’s extrapolation formula.
Letting, for k ≥2,
△x(k) = x(k) −x(k−1),
△2x(k) = △(△x(k)) = △x(k+1) −△x(k),
formula (6.36) can be written as
x(k) = x(k) −(△x(k))2
△2x(k−1) ,
k ≥2.
(6.37)
Form (6.37) explains the reason why method (6.36) is more commonly
known as Aitken’s △2 method.
For the convergence analysis of Aitken’s method, it is useful to write (6.36)
as a ﬁxed-point method in the form (6.17), by introducing the iteration
function
φ△(x) =
xφ(φ(x)) −φ2(x)
φ(φ(x)) −2φ(x) + x.
(6.38)
This function is indeterminate at x = α since φ(α) = α; however, by
applying L’Hospital’s rule one can easily check that limx→α φ△(x) = α

274
6. Rootﬁnding for Nonlinear Equations
under the assumption that φ is diﬀerentiable at α and φ′(α) ̸= 1. Thus, φ△
is consistent and has a continuos extension at α, the same being also true
if α is a multiple root of f. Moreover, it can be shown that the ﬁxed points
of (6.38) coincide with those of φ even in the case where α is a multiple
root of f (see [IK66], pp. 104-106).
From (6.38) we conclude that Aitken’s method can be applied to a ﬁxed-
point method x = φ(x) of arbitrary order. Actually, the following conver-
gence result holds.
Property 6.7 (convergence of Aitken’s method) Let x(k+1) = φ(x(k))
be a ﬁxed-point iteration of order p ≥1 for the approximation of a simple
zero α of a function f. If p = 1, Aitken’s method converges to α with order
2, while if p ≥2 the convergence order is 2p −1. In particular, if p = 1,
Aitken’s method is convergent even if the ﬁxed-point method is not. If α
has multiplicity m ≥2 and the method x(k+1) = φ(x(k)) is ﬁrst-order con-
vergent, then Aitken’s method converges linearly, with convergence factor
C = 1 −1/m.
Example 6.10 Consider the computation of the simple zero α = 1 for the func-
tion f(x) = (x −1)ex. For this, we use three ﬁxed-point methods whose iteration
functions are, respectively, φ0(x) = log(xex), φ1(x) = (ex + x)/(ex + 1) and
φ2(x) = (x2 −x + 1)/x (for x ̸= 0). Notice that, since |φ′
0(1)| = 2, the corre-
sponding ﬁxed-point method is not convergent, while in the other two cases the
methods have order 1 and 2, respectively.
Let us check the performance of Aitken’s method, running Program 55 with
x(0) = 2, toll = 10−10 and working in complex arithmetic. Notice that in the case
of φ0 this produces complex numbers if x(k) happens to be negative. According
to Property 6.7, Aitken’s method applied to the iteration function φ0 converges
in 8 steps to the value x(8) = 1.000002 + i 0.000002. In the other two cases, the
method of order 1 converges to α in 18 iterations, to be compared with the 4
iterations required by Aitken’s method, while in the case of the iteration function
φ2 convergence holds in 7 iterations against 5 iterations required by Aitken’s
method.
•
Aitken’s method is implemented in Program 55. The input/output pa-
rameters are the same as those of previous programs in this chapter.
Program 55 - aitken : Aitken’s extrapolation
function [xvect,xdif,fx,nit]=aitken(x0,nmax,toll,phi,fun)
nit=0; xvect=[x0]; x=x0; fxn=eval(fun);
fx=[fxn]; xdif=[]; err=toll+1;
while err >= toll & nit <= nmax
nit=nit+1; xv=xvect(nit); x=xv; phix=eval(phi);
x=phix; phixx=eval(phi); den=phixx-2*phix+xv;
if den == 0, err=toll*1.e-01;
else, xn=(xv*phixx-phixˆ2)/den; xvect=[xvect; xn];
xdif=[xdif; abs(xn-xv)]; x=xn; fxn=abs(eval(fun));

6.6 Post-processing Techniques for Iterative Methods
275
fx=[fx; fxn]; err=fxn;
end
end
6.6.2
Techniques for Multiple Roots
As previously noticed in deriving Aitken’s acceleration, taking the incre-
mental ratios of successive iterates λ(k) in (6.33) provides a way to estimate
the asymptotic convergence factor φ′(α).
This information can be employed also to estimate the multiplicity of the
root of a nonlinear equation and, as a consequence, it provides a tool for
modifying Newton’s method in order to recover its quadratic convergence
(see (6.23)). Indeed, deﬁne the sequence m(k) through the relation λ(k) =
1 −1/m(k), and recalling (6.22), it follows that m(k) tends to m as k →∞.
If the multiplicity m is known a priori, it is clearly convenient to use the
modiﬁed Newton method (6.23). In other cases, the following adaptive New-
ton algorithm can be used
x(k+1) = x(k) −m(k) f(x(k))
f ′(x(k)),
k ≥2,
(6.39)
where we have set
m(k) =
1
1 −λ(k) =
x(k−1) −x(k−2)
2x(k−1) −x(k) −x(k−2) .
(6.40)
Example 6.11 Let us check the performances of Newton’s method in its three
versions proposed so far (standard (6.16), modiﬁed (6.23) and adaptive (6.39)),
to approximate the multiple zero α = 1 of the function f(x) = (x2 −1)p log x
(for p ≥1 and x > 0). The desired root has multiplicity m = p + 1. The values
p = 2, 4, 6 have been considered and x(0) = 0.8, toll=10−10 have always been
taken in numerical computations.
The obtained results are summarized in Table 6.8, where for each method
the number of iterations nit required to converge are reported. In the case of
the adaptive method, beside the value of nit we have also shown in braces the
estimate m(nit) of the multiplicity m that is yielded by Program 56.
•
m
standard
adaptive
modiﬁed
3
51
13 (2.9860)
4
5
90
16 (4.9143)
5
7
127
18 (6.7792)
5
TABLE 6.8. Solution of problem (x2 −1)p log x = 0 in the interval [0.5, 1.5], with
p = 2, 4, 6

276
6. Rootﬁnding for Nonlinear Equations
In Example 6.11, the adaptive Newton method converges more rapidly
than the standard method, but less rapidly than the modiﬁed Newton
method. It must be noticed, however, that the adaptive method yields as
a useful by-product a good estimate of the multiplicity of the root, which
can be proﬁtably employed in a deﬂation procedure for the approximation
of the roots of a polynomial.
The algorithm 6.39, with the adaptive estimate (6.40) of the multiplicity
of the root, is implemented in Program 56. To avoid the onset of numerical
instabilities, the updating of m(k) is performed only when the variation be-
tween two consecutive iterates is suﬃciently diminished. The input/output
parameters are the same as those of previous programs in this chapter.
Program 56 - adptnewt : Adaptive Newton’s method
function [xvect,xdif,fx,nit,m] = adptnewt(x0,nmax,toll,fun,dfun)
xvect=x0; nit=0; r=[1]; err=toll+1; m=[1]; xdif=[];
while (nit < nmax) & (err > toll)
nit=nit+1; x=xvect(nit); fx(nit)=eval(fun); f1x=eval(dfun);
if (f1x == 0), disp(’ Stop due to vanishing derivative ’); return; end;
x=x-m(nit)*fx(nit)/f1x; xvect=[xvect;x]; fx=[fx;eval(fun)];
rd=err; err=abs(xvect(nit+1)-xvect(nit)); xdif=[xdif;err];
ra=err/rd; r=[r;ra]; diﬀ=abs(r(nit+1)-r(nit));
if (diﬀ< 1.e-3) & (r(nit+1) > 1.e-2),
m(nit+1)=max(m(nit),1/abs(1-r(nit+1)));
else, m(nit+1)=m(nit); end
end
6.7
Applications
We apply iterative methods for nonlinear equations considered so far in the
solution of two problems arising in the study of the thermal properties of
gases and electronics, respectively.
6.7.1
Analysis of the State Equation for a Real Gas
For a mole of a perfect gas, the state equation Pv = RT establishes a re-
lation between the pressure P of the gas (in Pascals [Pa]), the speciﬁc vol-
ume v (in cubic meters per kilogram [m3Kg−1]) and its temperature T (in
Kelvin [K]), R being the universal gas constant, expressed in [JKg−1K−1]
(joules per kilogram per Kelvin).
For a real gas, the deviation from the state equation of perfect gases is
due to van der Waals and takes into account the intermolecular interaction
and the space occupied by molecules of ﬁnite size (see [Sla63]).

6.7 Applications
277
Denoting by α and β the gas constants according to the van der Waals
model, in order to determine the speciﬁc volume v of the gas, once P and
T are known, we must solve the nonlinear equation
f(v) = (P + α/v2)(v −β) −RT = 0.
(6.41)
With this aim, let us consider Newton’s method (6.16) in the case of carbon
dioxide (CO2), at the pressure of P = 10[atm] (equal to 1013250[Pa]) and
at the temperature of T = 300[K]. In such a case, α = 188.33[Pam6Kg−2]
and β = 9.77 · 10−4[m3Kg−1]; as a comparison, the solution computed by
assuming that the gas is perfect is ˜v ≃0.056[m3Kg−1].
We report in Table 6.9 the results obtained by running Program 50 for
diﬀerent choices of the initial guess v(0). We have denoted by Nit the num-
ber of iterations needed by Newton’s method to converge to the root v∗of
f(v) = 0 using an absolute tolerance equal to the roundoﬀunit.
v(0)
Nit
v(0)
Nit
v(0)
Nit
v(0)
Nit
10−4
47
10−2
7
10−3
21
10−1
5
TABLE 6.9. Convergence of Newton’s method to the root of equation (6.41)
The computed approximation of v∗is vNit ≃0.0535. To analyze the causes
of the strong dependence of Nit on the value of v(0), let us examine the
derivative f ′(v) = P −αv−2 + 2αβv−3. For v > 0, f ′(v) = 0 at vM ≃
1.99·10−3[m3Kg−1] (relative maximum) and at vm ≃1.25·10−2[m3Kg−1]
(relative minimum), as can be seen in the graph of Figure 6.8 (left).
A choice of v(0) in the interval (0, vm) (with v(0) ̸= vM) thus necessarily
leads to a slow convergence of Newton’s method, as demonstrated in Figure
6.8 (right), where, in solid circled line, the sequence {|v(k+1) −v(k)|} is
shown, for k ≥0.
A possible remedy consists of resorting to a polyalgorithmic approach,
based on the sequential use of the bisection method and Newton’s method
(see Section 6.2.1). Running the bisection-Newton’s method with the end-
points of the search interval equal to a = 10−4[m3Kg−1] and b = 0.1[m3Kg−1]
and an absolute tolerance of 10−3[m3Kg−1], yields an overall convergence
of the algorithm to the root v∗in 11 iterations, with an accuracy of the
order of the roundoﬀunit. The plot of the sequence {|v(k+1) −v(k)|}, for
k ≥0, is shown in solid and starred lines in Figure 6.8 (right).
6.7.2
Analysis of a Nonlinear Electrical Circuit
Let us consider the electrical circuit in Figure 6.9 (left), where v and j
denote respectively the voltage drop across the device D (called a tunneling
diode) and the current ﬂowing through D, while R and E are a resistor and
a voltage generator of given values.

278
6. Rootﬁnding for Nonlinear Equations
0
0.02
0.04
0.06
0.08
0.1
−6
−4
−2
0
2
4
6 x 10
4
0
10
20
30
40
50
10
−18
10
−16
10
−14
10
−12
10
−10
10
−8
10
−6
10
−4
10
−2
10
0
FIGURE 6.8. Graph of the function f in (6.41) (left); increments |v(k+1) −v(k)|
computed by the Newton’s method (circled curve) and bisection-Newton’s
method (starred curve)
The circuit is commonly employed as a biasing circuit for electronic de-
vices working at high frequency (see [Col66]). In such applications the pa-
rameters R and E are designed in such a way that v attains a value internal
to the interval for which g′(v) < 0, where g is the function which describes
the bound between current and voltage for D and is drawn in Figure 6.9
(right). Explicitly, g = α(ev/β −1) −µv(v −γ), for suitable constants α, β,
γ and µ.
_+
j
E
R
D
v
0
0.1
0.2
0.3
0.4
0.5
−4
−2
0
2
4
6
8
10
12 x 10
−5
g(v)
FIGURE 6.9. Tunneling diode circuit (left) and working point computation
(right)
Our aim is to determine the working point of the circuit at hand, that
is, the values attained by v and j for given parameters R and E. For that,
we write Kirchhoﬀ’s law for the voltages across the loop, obtaining the
following nonlinear equation
f(v) = v
 1
R + µγ

−µv2 + α(ev/β −1) −E
R = 0.
(6.42)
From a graphical standpoint, ﬁnding out the working point of the circuit
amounts to determining the intersection between the function g and the

6.8 Exercises
279
straight line of equation j = (E −v)/R, as shown in Figure 6.9 (right).
Assume the following (real-life) values for the parameters of the problem:
E/R = 1.2·10−4 [A], α = 10−12 [A], β−1 = 40 [V −1], µ = 10−3 [AV −2] and
γ = 0.4 [V ]. The solution of (6.42), which is also unique for the considered
values of the parameters, is v∗≃0.3 [V ].
To approximate v∗, we compare the main iterative methods introduced
in this chapter. We have taken v(0) = 0 [V ] for Newton’s method, ξ = 0
for the Dekker-Brent algorithm (for the meaning of ξ, see Example 6.5),
while for all the other schemes the search interval has been taken equal to
[0, 0.5]. The stopping tolerance toll has been set to 10−10. The obtained
results are reported in Table 6.10 where nit and f (nit) denote respectively
the number of iterations needed by the method to converge and the value
of f at the computed solution.
Notice the extremely slow convergence of the Regula Falsi method, due
to the fact that the value v(k′) always coincides with the right end-point
v = 0.5 and the function f around v∗has derivative very close to zero. An
analogous interpretation holds for the chord method.
Method
nit
f (nit)
Method
nit
f (nit)
bisection
33
−1.12 · 10−15
Dekker-Brent
11
1.09 · 10−14
Regula Falsi
225
−9.77 · 10−11
secant
11
2.7 · 10−20
chord
186
−9.80 · 10−14
Newton’s
8
−1.35 · 10−20
TABLE 6.10. Convergence of the methods for the approximation of the root of
equation (6.42)
6.8
Exercises
1. Derive geometrically the sequence of the ﬁrst iterates computed by bisec-
tion, Regula Falsi, secant and Newton’s methods in the approximation of
the zero of the function f(x) = x2 −2 in the interval [1, 3].
2. Let f be a continuous function that is m-times diﬀerentiable (m ≥1), such
that f(α) = . . . = f (m−1)(α) = 0 and f (m)(α) ̸= 0. Prove (6.22) and check
that the modiﬁed Newton method (6.23) has order of convergence equal to
2.
[Hint: let f(x) = (x −α)mh(x), h being a function such that h(α) ̸= 0].
3. Let f(x) = cos2(2x) −x2 be the function in the interval 0 ≤x ≤1.5
examined in Example 6.4. Having ﬁxed a tolerance ε = 10−10 on the abso-
lute error, determine experimentally the subintervals for which Newton’s
method is convergent to the zero α ≃0.5149.
[Solution: for 0 < x(0) ≤0.02, 0.94 ≤x(0) ≤1.13 and 1.476 ≤x(0) ≤1.5,
the method converges to the solution −α. For any other value of x(0) in
[0, 1.5], the method converges to α].

280
6. Rootﬁnding for Nonlinear Equations
4. Check the following properties:
(a) 0 < φ′(α) < 1: monotone convergence, that is, the error x(k) −α
maintains a constant sign as k varies;
(b) −1 < φ′(α) < 0: oscillatory convergence that is, x(k) −α changes sign
as k varies;
(c) |φ′(α)| > 1: divergence. More precisely, if φ′(α) > 1, the sequence
is monotonically diverging, while for φ′(α) < −1 it diverges with
oscillatory sign.
5. Consider for k ≥0 the ﬁxed-point method, known as Steﬀensen’s method
x(k+1) = x(k) −f(x(k))
ϕ(x(k)),
ϕ(x(k)) = f(x(k) + f(x(k))) −f(x(k))
f(x(k))
,
and prove that it is a second-order method. Implement the Steﬀensen
method in a MATLAB code and employ it to approximate the root of
the nonlinear equation e−x −sin(x) = 0.
6. Analyze the convergence of the ﬁxed-point method x(k+1) = φj(x(k)) for
computing the zeros α1 = −1 and α2 = 2 of the function f(x) = x2 −x−2,
when the following iteration functions are used: φ1(x) = x2 −2, φ2(x) =
√2 + x φ3(x) = −√2 + x and φ4(x) = 1 + 2/x, x ̸= 0.
[Solution: the method is non convergent with φ1, it converges only to α2,
with φ2 and φ4, while it converges only to α1 with φ3].
7. For the approximation of the zeros of the function f(x) = (2x2 −3x −
2)/(x −1), consider the following ﬁxed-point methods:
(1) x(k+1) = g(x(k)), where g(x) = (3x2 −4x −2)/(x −1);
(2) x(k+1) = h(x(k)), where h(x) = x −2 + x/(x −1).
Analyze the convergence properties of the two methods and determine
in particular their order. Check the behavior of the two schemes using
Program 51 and provide, for the second method, an experimental estimate
of the interval such that if x(0) is chosen in the interval then the method
converges to α = 2.
[Solution: zeros: α1 = −1/2 and α2 = 2. Method (1) is not convergent,
while (2) can approximate only α2 and is second-order. Convergence holds
for any x(0) > 1].
8. Propose at least two ﬁxed-point methods for approximating the root α ≃
0.5885 of equation e−x −sin(x) = 0 and analyze their convergence.
9. Using Descartes’s rule of signs, determine the number of real roots of the
polynomials p6(x) = x6 −x −1 and p4(x) = x4 −x3 −x2 + x −1.
[Solution: both p6 and p4 have one negative and one positive real root].
10. Let g : R →R be deﬁned as g(x) =
√
1 + x2. Show that the iterates of
Newton’s method for the equation g′(x) = 0 satisfy the following proper-
ties:
(a)
|x(0)| < 1 ⇒g(x(k+1)) < g(x(k)), k ≥0, lim
k→∞x(k) = 0,
(b)
|x(0)| > 1 ⇒g(x(k+1)) > g(x(k)), k ≥0, lim
k→∞|x(k)| = +∞.

7
Nonlinear Systems and Numerical
Optimization
In this chapter we address the numerical solution of systems of nonlinear
equations and the minimization of a function of several variables.
The ﬁrst problem generalizes to the n-dimensional case the search for
the zeros of a function, which was considered in Chapter 6, and can be
formulated as follows: given F : Rn →Rn,
ﬁnd x∗∈Rn such that F(x∗) = 0.
(7.1)
Problem (7.1) will be solved by extending to several dimensions some of
the schemes that have been proposed in Chapter 6.
The basic formulation of the second problem reads: given f : Rn →R,
called an objective function,
minimize f(x) in Rn,
(7.2)
and is called an unconstrained optimization problem.
A typical example consists of determining the optimal allocation of n
resources, x1, x2, . . . , xn, in competition with each other and ruled by a
speciﬁc law. Generally, such resources are not unlimited; this circumstance,
from a mathematical standpoint, amounts to requiring that the minimizer
of the objective function lies within a subset Ω⊂Rn, and, possibly, that
some equality or inequality constraints must be satisﬁed.
When these constraints exist the optimization problem is called con-
strained and can be formulated as follows: given the objective function f,
minimize f(x) in Ω⊂Rn.
(7.3)

282
7. Nonlinear Systems and Numerical Optimization
Remarkable instances of (7.3) are those in which Ωis characterized by con-
ditions like h(x) = 0 (equality constraints) or h(x) ≤0 (inequality con-
straints), where h : Rn →Rm, with m ≤n, is a given function, called cost
function, and the condition h(x) ≤0 means hi(x) ≤0, for i = 1, . . . , m.
If the function h is continuous and Ωis connected, problem (7.3) is
usually referred to as a nonlinear programming problem. Notable examples
in this area are:
convex programming if f is a convex function and h has convex compo-
nents (see (7.21));
linear programming if f and h are linear;
quadratic programming if f is quadratic and h is linear.
Problems (7.1) and (7.2) are strictly related to one another. Indeed, if we
denote by Fi the components of F, then a point x∗, a solution of (7.1),
is a minimizer of the function f(x) = n
i=1 F 2
i (x). Conversely, assuming
that f is diﬀerentiable and setting the partial derivatives of f equal to
zero at a point x∗at which f is minimum leads to a system of nonlinear
equations. Thus, any system of nonlinear equations can be associated with
a suitable minimization problem, and vice versa. We shall take advantage
of this observation when devising eﬃcient numerical methods.
7.1
Solution of Systems of Nonlinear Equations
Before considering problem (7.1), let us set some notation which will be
used throughout the chapter.
For k ≥0, we denote by Ck(D) the set of k-continuously diﬀerentiable
functions from D to Rn, where D ⊆Rn is a set that will be made precise
from time to time. We shall always assume that F ∈C1(D), i.e., F : Rn →
Rn is a continuously diﬀerentiable function on D.
We denote also by JF(x) the Jacobian matrix associated with F and
evaluated at the point x = (x1, . . . , xn)T of Rn, deﬁned as
(JF(x))ij =
∂Fi
∂xj

(x),
i, j = 1, . . . , n.
Given any vector norm ∥·∥, we shall henceforth denote the sphere of radius
R with center x∗by
B(x∗; R) = {y ∈Rn : ∥y −x∗∥< R} .

7.1 Solution of Systems of Nonlinear Equations
283
7.1.1
Newton’s Method and Its Variants
An immediate extension to the vector case of Newton’s method (6.16) for
scalar equations can be formulated as follows:
given x(0) ∈Rn, for k = 0, 1, . . . , until convergence:
solve
JF(x(k))δx(k) = −F(x(k));
set
x(k+1) = x(k) + δx(k).
(7.4)
Thus, at each step k the solution of a linear system with matrix JF(x(k))
is required.
Example 7.1 Consider the nonlinear system



ex2
1+x2
2 −1
= 0,
ex2
1−x2
2 −1
= 0,
which admits the unique solution x∗= 0. In this case, F(x) = (ex2
1+x2
2 −
1, ex2
1−x2
2 −1). Running Program 57, leads to convergence in 15 iterations to the
pair (0.61 · 10−5, 0.61 · 10−5)T , starting from the initial datum x(0) = (0.1, 0.1)T ,
thus demonstrating a fairly rapid convergence rate. The results, however, dra-
matically change as the choice of the initial guess is varied. For instance, picking
up x(0) = (10, 10)T , 220 iterations are needed to obtain a solution comparable to
the previous one, while, starting from x(0) = (20, 20)T , Newton’s method fails to
converge.
•
The previous example points out the high sensitivity of Newton’s method
on the choice of the initial datum x(0), as conﬁrmed by the following local
convergence result.
Theorem 7.1 Let F : Rn →Rn be a C1 function in a convex open set
D of Rn that contains x∗. Suppose that J−1
F (x∗) exists and that there exist
positive constants R, C and L, such that ∥J−1
F (x∗)∥≤C and
∥JF(x) −JF(y)∥≤L∥x −y∥
∀x, y ∈B(x∗; R),
having denoted by the same symbol ∥· ∥two consistent vector and matrix
norms. Then, there exists r > 0 such that, for any x(0) ∈B(x∗; r), the
sequence (7.4) is uniquely deﬁned and converges to x∗with
∥x(k+1) −x∗∥≤CL∥x(k) −x∗∥2.
(7.5)
Proof. Proceeding by induction on k, let us check (7.5) and, moreover, that
x(k+1) ∈B(x∗; r), where r = min(R, 1/(2CL)). First, we prove that for any
x(0) ∈B(x∗; r), the inverse matrix J−1
F (x(0)) exists. Indeed
∥J−1
F (x∗)[JF(x(0)) −JF(x∗)]∥≤∥J−1
F (x∗)∥∥JF(x(0)) −JF(x∗)∥≤CLr ≤1
2,

284
7. Nonlinear Systems and Numerical Optimization
and thus, thanks to Theorem 1.5, we can conclude that J−1
F (x(0)) exists, since
∥J−1
F (x(0))∥≤
∥J−1
F (x∗)∥
1 −∥J−1
F (x∗)[JF(x(0)) −JF(x∗)]∥≤2∥J−1
F (x∗)∥≤2C.
As a consequence, x(1) is well deﬁned and
x(1) −x∗= x(0) −x∗−J−1
F (x(0))[F(x(0)) −F(x∗)].
Factoring out J−1
F (x(0)) on the right hand side and passing to the norms, we get
∥x(1) −x∗∥
≤∥J−1
F (x(0))∥∥F(x∗) −F(x(0)) −JF(x(0))[x∗−x(0)]∥
≤2C L
2 ∥x∗−x(0)∥2
where the remainder of Taylor’s series of F has been used. The previous relation
proves (7.5) in the case k = 0; moreover, since x(0) ∈B(x∗; r), we have ∥x∗−
x(0)∥≤1/(2CL), from which ∥x(1) −x∗∥≤1
2∥x∗−x(0)∥.
This ensures that x(1) ∈B(x∗; r).
By a similar proof, one can check that, should (7.5) be true for a certain k,
then the same inequality would follow also for k + 1 in place of k. This proves
the theorem.
3
Theorem 7.1 thus conﬁrms that Newton’s method is quadratically conver-
gent only if x(0) is suﬃciently close to the solution x∗and if the Jacobian
matrix is nonsingular. Moreover, it is worth noting that the computational
eﬀort needed to solve the linear system (7.4) can be excessively high as n
gets large. Also, JF(x(k)) could be ill-conditioned, which makes it quite diﬃ-
cult to obtain an accurate solution. For these reasons, several modiﬁcations
to Newton’s method have been proposed, which will be brieﬂy considered
in the later sections, referring to the specialized literature for further details
(see [OR70], [DS83], [Erh97], [BS90] and the references therein).
7.1.2
Modiﬁed Newton’s Methods
Several modiﬁcations of Newton’s method have been proposed in order
to reduce its cost when the computed solution is suﬃciently close to x∗.
Further variants, that are globally convergent, will be introduced for the
solution of the minimization problem (7.2).
1. Cyclic updating of the Jacobian matrix
An eﬃcient alternative to method (7.4) consists of keeping the Jacobian
matrix (more precisely, its factorization) unchanged for a certain number,
say p ≥2, of steps. Generally, a deterioration of convergence rate is accom-
panied by a gain in computational eﬃciency.

7.1 Solution of Systems of Nonlinear Equations
285
Program 57 implements Newton’s method in the case in which the LU
factorization of the Jacobian matrix is updated once every p steps. The pro-
grams used to solve the triangular systems have been described in Chapter
3.
Here and in later codings in this chapter, we denote by x0 the initial
vector, by F and J the variables containing the functional expressions of F
and of its Jacobian matrix JF, respectively. The parameters toll and nmax
represent the stopping tolerance in the convergence of the iterative process
and the maximum admissible number of iterations, respectively. In output,
the vector x contains the approximation to the searched zero of F, while
nit denotes the number of iterations necessary to converge.
Program 57 - newtonxsys : Newton’s method for nonlinear systems
function [x, nit] = newtonsys(F, J, x0, toll, nmax, p)
[n,m]=size(F); nit=0; Fxn=zeros(n,1); x=x0; err=toll+1;
for i=1:n, for j=1:n, Jxn(i,j)=eval(J((i-1)*n+j,:)); end; end
[L,U,P]=lu(Jxn); step=0;
while err > toll
if step == p
step = 0;
for i=1:n;
Fxn(i)=eval(F(i,:));
for j=1:n; Jxn(i,j)=eval(J((i-1)*n+j,:)); end
end
[L,U,P]=lu(Jxn);
else
for i=1:n, Fxn(i)=eval(F(i,:)); end
end
nit=nit+1; step=step+1; Fxn=-P*Fxn; y=forward col(L,Fxn);
deltax=backward col(U,y); x = x + deltax; err=norm(deltax);
if nit > nmax
disp(’ Fails to converge within maximum number of iterations ’);
break
end
end
2. Inexact solution of the linear systems
Another possibility consists of solving the linear system (7.4) by an iter-
ative method where the maximum number of admissible iterations is ﬁxed
a priori. The resulting schemes are identiﬁed as Newton-Jacobi, Newton-
SOR or Newton-Krylov methods, according to the iterative process that is
used for the linear system (see [BS90], [Kel99]). Here, we limit ourselves to
describing the Newton-SOR method.

286
7. Nonlinear Systems and Numerical Optimization
In analogy with what was done in Section 4.2.1, let us decompose the
Jacobian matrix at step k as
JF(x(k)) = Dk −Ek −Fk
(7.6)
where Dk = D(x(k)), −Ek = −E(x(k)) and −Fk = −F(x(k)), the diagonal
part and the lower and upper triangular portions of the matrix JF(x(k)),
respectively. We suppose also that Dk is nonsingular. The SOR method for
solving the linear system in (7.4) is organized as follows: setting δx(k)
0
= 0,
solve
δx(k)
r
= Mkδx(k)
r−1 −ωk(Dk −ωkEk)−1F(x(k)),
r = 1, 2, . . . ,
(7.7)
where Mk is the iteration matrix of SOR method
Mk = [Dk −ωkEk]−1 [(1 −ωk)Dk + ωkFk] ,
and ωk is a positive relaxation parameter whose optimal value can rarely
be determined a priori. Assume that only r = m steps of the method are
carried out. Recalling that δx(k)
r
= x(k)
r
−x(k) and still denoting by x(k+1)
the approximate solution computed after m steps, we ﬁnd that this latter
can be written as (see Exercise 1)
x(k+1) = x(k) −ωk

Mm−1
k
+ · · · + I

(Dk −ωkEk)−1 F(x(k)).
(7.8)
This method is thus a composite iteration, in which at each step k, starting
from x(k), m steps of the SOR method are carried out to solve approxi-
mately system (7.4).
The integer m, as well as ωk, can depend on the iteration index k; the
simplest choice amounts to performing, at each Newton’s step, only one
iteration of the SOR method, thus obtaining for r = 1 from (7.7) the one-
step Newton-SOR method
x(k+1) = x(k) −ωk (Dk −ωkEk)−1 F(x(k)).
In a similar way, the preconditioned Newton-Richardson method with ma-
trix Pk, if truncated at the m-th iteration, is
x(k+1) = x(k) −
0
I + Mk + . . . + Mm−1
k
1
P−1
k F(x(k)),
where Pk is the preconditioner of JF and
Mk = P−1
k Nk,
Nk = Pk −JF(x(k)).
For an eﬃcient implementation of these techniques we refer to the MAT-
LAB software package developed in [Kel99].

7.1 Solution of Systems of Nonlinear Equations
287
3. Diﬀerence approximations of the Jacobian matrix
Another possibility consists of replacing JF(x(k)) (whose explicit compu-
tation is often very expensive) with an approximation through n-dimensional
diﬀerences of the form
(J(k)
h )j =
F(x(k) + h(k)
j ej) −F(x(k))
h(k)
j
,
∀k ≥0,
(7.9)
where ej is the j-th vector of the canonical basis of Rn and h(k)
j
> 0 are
increments to be suitably chosen at each step k of the iteration (7.4). The
following result can be shown.
Property 7.1 Let F and x∗be such that the hypotheses of Theorem 7.1
are fulﬁlled, where ∥·∥denotes the ∥·∥1 vector norm and the corresponding
induced matrix norm. If there exist two positive constants ε and h such
that x(0) ∈B(x∗, ε) and 0 < |h(k)
j | ≤h for j = 1, . . . , n then the sequence
deﬁned by
x(k+1) = x(k) −
6
J(k)
h
7−1
F(x(k)),
(7.10)
is well deﬁned and converges linearly to x∗. Moreover, if there exists a
positive constant C such that max
j |h(k)
j | ≤C∥x(k) −x∗∥or, equivalently,
there exists a positive constant c such that max
j |h(k)
j | ≤c∥F(x(k))∥, then
the sequence (7.10) is convergent quadratically.
This result does not provide any constructive indication as to how to com-
pute the increments h(k)
j . In this regard, the following remarks can be made.
The ﬁrst-order truncation error with respect to h(k)
j , which arises from the
divided diﬀerence (7.10), can be reduced by reducing the sizes of h(k)
j . On
the other hand, a too small value for h(k)
j
can lead to large rounding er-
rors. A trade-oﬀmust therefore be made between the need of limiting the
truncation errors and ensuring a certain accuracy in the computations.
A possible choice is to take
h(k)
j
= √ϵM max
2
|x(k)
j |, Mj
3
sign(xj),
where Mj is a parameter that characterizes the typical size of the com-
ponent xj of the solution. Further improvements can be achieved using
higher-order divided diﬀerences to approximate derivatives, like
(J(k)
h )j =
F(x(k) + h(k)
j ej) −F(x(k) −h(k)
j ej)
2h(k)
j
,
∀k ≥0.
For further details on this subject, see, for instance, [BS90].

288
7. Nonlinear Systems and Numerical Optimization
7.1.3
Quasi-Newton Methods
By this term, we denote all those schemes in which globally convergent
methods are coupled with Newton-like methods that are only locally con-
vergent, but with an order greater than one.
In a quasi-Newton method, given a continuously diﬀerentiable function
F : Rn →Rn, and an initial value x(0) ∈Rn, at each step k one has to
accomplish the following operations:
1. compute F(x(k));
2. choose ˜JF(x(k)) as being either the exact JF(x(k)) or an approxima-
tion of it;
3. solve the linear system ˜JF(x(k))δx(k) = −F(x(k));
4. set x(k+1) = x(k) + αkδx(k), where αk are suitable damping parame-
ters.
Step 4. is thus the characterizing element of this family of methods. It will
be addressed in Section 7.2.6, where a criterion for selecting the “direction”
δx(k) will be provided.
7.1.4
Secant-like Methods
These methods are constructed starting from the secant method introduced
in Section 6.2 for scalar functions. Precisely, given two vectors x(0) and x(1),
at the generic step k ≥1 we solve the linear system
Qkδx(k+1) = −F(x(k))
(7.11)
and we set x(k+1) = x(k) + δx(k+1). Qk is an n × n matrix such that
Qkδx(k) = F(x(k)) −F(x(k−1)) = b(k),
k ≥1,
and is obtained by a formal generalization of (6.13). However, the algebraic
relation above does not suﬃce to uniquely determine Qk. For this purpose
we require Qk for k ≥n to be a solution to the following set of n systems
Qk
+
x(k) −x(k−j),
= F(x(k)) −F(x(k−j)),
j = 1, . . . , n.
(7.12)
If the vectors x(k−j), . . . , x(k) are linearly independent, system (7.12) allows
for calculating all the unknown coeﬃcients {(Qk)lm, l, m = 1, . . . , n} of
Qk. Unfortunately, in practice the above vectors tend to become linearly
dependent and the resulting scheme is unstable, not to mention the need
for storing all the previous n iterates.
For these reasons, an alternative approach is pursued which aims at pre-
serving the information already provided by the method at step k. Precisely,

7.1 Solution of Systems of Nonlinear Equations
289
Qk is looked for in such a way that the diﬀerence between the following
linear approximants to F(x(k−1)) and F(x(k)), respectively
F(x(k)) + Qk(x −x(k)),
F(x(k−1)) + Qk−1(x −x(k−1)),
is minimized jointly with the constraint that Qk satisﬁes system (7.12).
Using (7.12) with j = 1, the diﬀerence between the two approximants is
found to be
dk = (Qk −Qk−1)
+
x −x(k−1),
.
(7.13)
Let us decompose the vector x −x(k−1) as
x −x(k−1) = αδx(k) + s,
where α ∈R and sT δx(k) = 0. Therefore, (7.13) becomes
dk = α (Qk −Qk−1) δx(k) + (Qk −Qk−1) s.
Only the second term in the relation above can be minimized since the ﬁrst
one is independent of Qk, being
(Qk −Qk−1)δx(k) = b(k) −Qk−1δx(k).
The problem has thus become: ﬁnd the matrix Qk such that (Qk −Qk−1) s
is minimized ∀s orthogonal to δx(k) with the constraint that (7.12) holds.
It can be shown that such a matrix exists and can be recursively computed
as follows
Qk = Qk−1 + (b(k) −Qk−1δx(k))δx(k)T
δx(k)T δx(k)
.
(7.14)
The method (7.11), with the choice (7.14) of matrix Qk is known as the
Broyden method. To initialize (7.14), we set Q0 equal to the matrix JF(x(0))
or to any approximation of it, for instance, the one yielded by (7.9). As for
the convergence of Broyden’s method, the following result holds.
Property 7.2 If the assumptions of Theorem 7.1 are satisﬁed and there
exist two positive constants ε and γ such that
∥x(0) −x∗∥≤ε,
∥Q0 −JF(x∗)∥≤γ,
then the sequence of vectors x(k) generated by Broyden’s method is well
deﬁned and converges superlinearly to x∗, that is
∥x(k) −x∗∥≤ck∥x(k−1) −x∗∥
(7.15)
where the constants ck are such that lim
k→∞ck = 0.

290
7. Nonlinear Systems and Numerical Optimization
Under further assumptions, it is also possible to prove that the sequence
Qk converges to JF(x∗), a property that does not necessarily hold for the
above method as demonstrated in Example 7.3.
There exist several variants to Broyden’s method which aim at reducing
its computational cost, but are usually less stable (see [DS83], Chapter 8).
Program 58 implements Broyden’s method (7.11)-(7.14). We have denoted
by Q the initial approximation Q0 in (7.14).
Program 58 - broyden : Broyden’s method for nonlinear systems
function [x,it]=broyden(x,Q,nmax,toll,f)
[n,m]=size(f); it=0;
err=1;
fk=zeros(n,1); fk1=fk;
for i=1:n,
fk(i)=eval(f(i,:)); end
while it < nmax & err > toll
s=-Q \ fk; x=s+x; err=norm(s,inf);
if err > toll
for i=1:n, fk1(i)=eval(f(i,:)); end
Q=Q+1/(s’*s)*fk1*s’
end
it=it+1; fk=fk1;
end
Example 7.2 Let us solve using Broyden’s method the nonlinear system of Ex-
ample 7.1. The method converges in 35 iterations to the value (0.7 · 10−8, 0.7 ·
10−8)T compared with the 26 iterations required by Newton’s method starting
from the same initial guess (x(0) = (0.1, 0.1)T ). The matrix Q0 has been set equal
to the Jacobian matrix evaluated at x(0). Figure 7.1 shows the behavior of the
Euclidean norm of the error for both methods.
•
Example 7.3 Suppose we wish to solve using the Broyden method the nonlinear
system F(x) = (x1+x2−3; x2
1+x2
2−9)T = 0. This system admits the two solutions
(0, 3)T and (3, 0)T . Broyden’s method converges in 8 iterations to the solution
(0, 3)T starting from x(0) = (2, 4)T . However, the sequence of Qk, stored in the
variable Q of Program 58, does not converge to the Jacobian matrix, since
lim
k→∞Q(k) =
 1
1
1.5
1.75

̸= JF[(0, 3)T ] =
 1
1
0
6

.
•
7.1.5
Fixed-point Methods
We conclude the analysis of methods for solving systems of nonlinear equa-
tions by extending to n-dimensions the ﬁxed-point techniques introduced
in the scalar case. For this, we reformulate problem (7.1) as
given G : Rn →Rn, ﬁnd x∗∈Rn such that G(x∗) = x∗
(7.16)

7.1 Solution of Systems of Nonlinear Equations
291
0
5
10
15
20
25
30
35
40
10
−9
10
−8
10
−7
10
−6
10
−5
10
−4
10
−3
10
−2
10
−1
FIGURE 7.1. Euclidean norm of the error for the Newton method (solid line)
and the Broyden method (dashed line) in the case of the nonlinear system of
Example 7.1
where G is related to F through the following property: if x∗is a ﬁxed
point of G, then F(x∗) = 0.
Analogously to what was done in Section 6.3, we introduce iterative meth-
ods for the solution of (7.16) of the form:
given x(0) ∈Rn, for k = 0, 1, . . . until convergence, ﬁnd
x(k+1) = G(x(k)).
(7.17)
In order to analyze the convergence of the ﬁxed-point iteration (7.17) the
following deﬁnition will be useful.
Deﬁnition 7.1 A mapping G : D ⊂Rn →Rn is contractive on a set
D0 ⊂D if there exists a constant α < 1 such that ∥G(x) −G(y)∥≤
α∥x −y∥for all x, y in D0 where ∥· ∥is a suitable vector norm.
■
The existence and uniqueness of a ﬁxed point for G is ensured by the
following theorem.
Theorem 7.2 (contraction-mapping theorem) Suppose that G : D ⊂
Rn →Rn is contractive on a closed set D0 ⊂D and that G(x) ⊂D0 for
all x ∈D0. Then G has a unique ﬁxed point in D0.
Proof. Let us ﬁrst prove the uniqueness of the ﬁxed point. For this, assume that
there exist two distinct ﬁxed points, x∗, y∗. Then
∥x∗−y∗∥= ∥G(x∗) −G(y∗)∥≤α∥x∗−y∗∥
from which (1 −α)∥x∗−y∗∥≤0. Since (1 −α) > 0, it must necessarily be that
∥x∗−y∗∥= 0, i.e., x∗= y∗.

292
7. Nonlinear Systems and Numerical Optimization
To prove the existence we show that x(k) given by (7.17) is a Cauchy sequence.
This in turn implies that x(k) is convergent to a point x(∗) ∈D0. Take x(0)
arbitrarily in D0. Then, since the image of G is included in D0, the sequence x(k)
is well deﬁned and
∥x(k+1) −x(k)∥= ∥G(x(k)) −G(x(k−1))∥≤α∥x(k) −x(k−1)∥.
After p steps, p ≥1, we obtain
∥x(k+p) −x(k)∥
≤
p

i=1
∥x(k+i) −x(k+i−1)∥≤

αp−1 + . . . + 1

∥x(k+1) −x(k)∥
≤
αk
1 −α∥x(1) −x(0)∥.
Owing to the continuity of G it follows that lim
k→∞G(x(k)) = G(x(∗)) which proves
that x(∗) is a ﬁxed point for G.
3
The following result provides a suﬃcient condition for the iteration (7.17) to
converge (for the proof see [OR70], pp. 299-301), and extends the analogous
Theorem 6.3 in the scalar case.
Property 7.3 Suppose that G : D ⊂Rn →Rn has a ﬁxed point x∗in the
interior of D and that G is continuously diﬀerentiable in a neighborhood of
x∗. Denote by JG the Jacobian matrix of G and assume that ρ(JG(x(∗))) <
1. Then there exists a neighborhood S of x∗such that S ⊂D and, for any
x(0) ∈S, the iterates deﬁned by (7.17) all lie in D and converge to x∗.
As usual, since the spectral radius is the inﬁmum of the induced matrix
norms, in order for convergence to hold it suﬃces to check that ∥JG(x)∥< 1
for some matrix norm.
Example 7.4 Consider the nonlinear system
F(x) =

x2
1 + x2
2 −1, 2x1 + x2 −1
T = 0,
whose solutions are x∗
1 = (0, 1)T and x∗
2 = (4/5, −3/5)T . To solve it, let us use
two ﬁxed-point schemes, respectively deﬁned by the following iteration functions
G1(x) =


1 −x2
2

1 −x2
1

,
G2(x) =


1 −x2
2
−

1 −x2
1

.
(7.18)
It can be checked that Gi(x∗
i ) = x∗
i for i = 1, 2 and that the Jacobian matrices
of G1 and G2, evaluated at x∗
1 and x∗
2 respectively, are
JG1(x∗
1) =


0
−1
2
0
0

,
JG2(x∗
2) =


0
−1
2
4
3
0

.

7.1 Solution of Systems of Nonlinear Equations
293
The spectral radii are ρ(JG1(x∗
1)) = 0 and ρ(JG2(x∗
2)) =

2/3 ≃0.817 < 1 so
that both methods are convergent in a suitable neighborhood of their respective
ﬁxed points.
Running Program 59, with a tolerance of 10−10 on the maximum absolute
diﬀerence between two successive iterates, the ﬁrst scheme converges to x∗
1 in 9
iterations, starting from x(0) = (−0.9, 0.9)T , while the second one converges to
x∗
2 in 115 iterations, starting from x(0) = (0.9, 0.9)T . The dramatic change in
the convergence behavior of the two methods can be explained in view of the
diﬀerence between the spectral radii of the corresponding iteration matrices.
•
Remark 7.1 Newton’s method can be regarded as a ﬁxed-point method
with iteration function
GN(x) = x −J−1
F (x)F(x).
(7.19)
If we denote by r(k) = F(x(k)) the residual at step k, from (7.19) it turns
out that Newton’s method can be alternatively formulated as
+
I −JGN (x(k))
, +
x(k+1) −x(k),
= −r(k).
This equation allows us to interpret Newton’s method as a preconditioned
stationary Richardson method. This prompts introducing a parameter αk
in order to accelerate the convergence of the iteration
+
I −JGN (x(k))
, +
x(k+1) −x(k),
= −αkr(k).
The problem of how to select αk will be addressed in Section 7.2.6.
■
An implementation of the ﬁxed-point method (7.17) is provided in Pro-
gram 59. We have denoted by dim the size of the nonlinear system and
by Phi the variables containing the functional expressions of the iteration
function G. In output, the vector alpha contains the approximation of the
sought zero of F and the vector res contains the sequence of the maximum
norms of the residuals of F(x(k)).
Program 59 - ﬁxposys : Fixed-point method for nonlinear systems
function [alpha, res, nit]=ﬁxposys(dim, x0, nmax, toll, Phi, F)
x = x0; alpha=[x’]; res = 0;
for k=1:dim,
r=abs(eval(F(k,:))); if (r > res), res = r; end
end;
nit = 0; residual(1)=res;
while ((nit <= nmax) & (res >= toll)),
nit = nit + 1;
for k = 1:dim, xnew(k) = eval(Phi(k,:)); end
x = xnew; res = 0; alpha=[alpha;x]; x=x’;
for k = 1:dim,

294
7. Nonlinear Systems and Numerical Optimization
r = abs(eval(F(k,:)));
if (r > res), res=r; end,
end
residual(nit+1)=res;
end
res=residual’;
7.2
Unconstrained Optimization
We turn now to minimization problems. The point x∗, the solution of (7.2),
is called a global minimizer of f, while x∗is a local minimizer of f if ∃R > 0
such that
f(x∗) ≤f(x),
∀x ∈B(x∗; R).
Throughout this section we shall always assume that f ∈C1(Rn), and
we refer to [Lem89] for the case in which f is non diﬀerentiable. We shall
denote by
∇f(x) =
 ∂f
∂x1
(x), . . . , ∂f
∂xn
(x)
T
,
the gradient of f at a point x. If d is a non null vector in Rn, then the
directional derivative of f with respect to d is
∂f
∂d(x) = lim
α→0
f(x + αd) −f(x)
α
and satisﬁes ∂f(x)/∂d = [∇f(x)]T d. Moreover, denoting by (x, x + αd)
the segment in Rn joining the points x and x + αd, with α ∈R, Taylor’s
expansion ensures that ∃ξ ∈(x, x + αd) such that
f(x + αd) −f(x) = α∇f(ξ)T d.
(7.20)
If f ∈C2(Rn), we shall denote by H(x) (or ∇2f(x)) the Hessian matrix of
f evaluated at a point x, whose entries are
hij(x) = ∂2f(x)
∂xi∂xj
,
i, j = 1, . . . , n.
In such a case it can be shown that, if d ̸= 0, the second-order directional
derivative exists and we have
∂2f
∂d2 (x) = dT H(x)d.

7.2 Unconstrained Optimization
295
For a suitable ξ ∈(x, x + d) we also have
f(x + d) −f(x) = ∇f(x)T d + 1
2dT H(ξ)d.
Existence and uniqueness of solutions for (7.2) are not guaranteed in Rn.
Nevertheless, the following optimality conditions can be proved.
Property 7.4 Let x∗∈Rn be a local minimizer of f and assume that
f ∈C1(B(x∗; R)) for a suitable R > 0. Then ∇f(x∗) = 0. Moreover,
if f ∈C2(B(x∗; R)) then H(x∗) is positive semideﬁnite. Conversely, if
x∗∈B(x∗; R) and H(x∗) is positive deﬁnite, then x∗is a local minimizer
of f in B(x∗; R).
A point x∗such that ∇f(x∗) = 0, is said to be a critical point for f. This
condition is necessary for optimality to hold. However, this condition also
becomes suﬃcient if f is a convex function on Rn, i.e., such that ∀x, y ∈Rn
and for any α ∈[0, 1]
f[αx + (1 −α)y] ≤αf(x) + (1 −α)f(y).
(7.21)
For further and more general existence results, see [Ber82].
7.2.1
Direct Search Methods
In this section we deal with direct methods for solving problem (7.2), which
only require f to be continuous. In later sections, we shall introduce the
so-called descent methods, which also involve values of the derivatives of f
and have, in general, better convergence properties.
Direct methods are employed when f is not diﬀerentiable or if the com-
putation of its derivatives is a nontrivial task. They can also be used to
provide an approximate solution to employ as an initial guess for a descent
method. For further details, we refer to [Wal75] and [Wol78].
The Hooke and Jeeves Method
Assume we are searching for the minimizer of f starting from a given initial
point x(0) and requiring that the error on the residual is less than a certain
ﬁxed tolerance ϵ. The Hooke and Jeeves method computes a new point x(1)
using the values of f at suitable points along the orthogonal coordinate
directions around x(0). The method consists of two steps: an exploration
step and an advancing step.
The exploration step starts by evaluating f(x(0) + h1e1), where e1 is the
ﬁrst vector of the canonical basis of Rn and h1 is a positive real number to
be suitably chosen.
If f(x(0) + h1e1) < f(x(0)), then a success is recorded and the starting
point is moved in x(0) +h1e1, from which an analogous check is carried out
at point x(0) + h1e1 + h2e2 with h2 ∈R+.

296
7. Nonlinear Systems and Numerical Optimization
If, instead, f(x(0) + h1e1) ≥f(x(0)), then a failure is recorded and a
similar check is performed at x(0) −h1e1. If a success is registered, the
method explores, as previously, the behavior of f in the direction e2 starting
from this new point, while, in case of a failure, the method passes directly
to examining direction e2, keeping x(0) as starting point for the exploration
step.
To achieve a certain accuracy, the step lengths hi must be selected in
such a way that the quantities
|f(x(0) ± hjej) −f(x(0)|,
j = 1, . . . , n
(7.22)
have comparable sizes.
The exploration step terminates as soon as all the n Cartesian directions
have been examined. Therefore, the method generates a new point, y(0),
after at most 2n+1 functional evaluations. Only two possibilities may arise:
1. y(0) = x(0). In such a case, if
max
i=1,... ,nhi ≤ϵ the method terminates
and yields the approximate solution x(0). Otherwise, the step lengths
hi are halved and another exploration step is performed starting from
x(0);
2. y(0) ̸= x(0). If
max
i=1,... ,n|hi| < ϵ, then the method terminates yielding
y(0) as an approximate solution, otherwise the advancing step starts.
The advancing step consists of moving further from y(0) along the
direction y(0) −x(0) (which is the direction that recorded the maxi-
mum decrease of f during the exploration step), rather then simply
setting y(0) as a new starting point x(1).
This new starting point is instead set equal to 2y(0) −x(0). From this
point a new series of exploration moves is started. If this exploration
leads to a point y(1) such that f(y(1)) < f(y(0) −x(0)), then a new
starting point for the next exploration step has been found, otherwise
the initial guess for further explorations is set equal to y(1) = y(0) −
x(0).
The method is now ready to restart from the point x(1) just com-
puted.
Program 60 provides an implementation of the Hooke and Jeeves method.
The input parameters are the size n of the problem, the vector h of the
initial steps along the Cartesian directions, the variable f containing the
functional expression of f in terms of the components x(1), . . . , x(n), the
initial point x0 and the stopping tolerance toll equal to ϵ. In output, the
code returns the approximate minimizer of f, x, the value minf attained by
f at x and the number of iterations needed to compute x up to the desired
accuracy. The exploration step is performed by Program 61.

7.2 Unconstrained Optimization
297
Program 60 - hookejeeves : The method of Hooke and Jeeves (HJ)
function [x,minf,nit]=hookejeeves(n,h,f,x0,toll)
x = x0; minf = eval(f); nit = 0;
while h > toll
[y] = explore(h,n,f,x);
if y == x, h = h/2; else
x = 2*y-x;
[z] = explore(h,n,f,x);
if z == x,
x = y; else,
x = z; end
end
nit = nit +1;
end
minf = eval(f);
Program 61 - explore : Exploration step in the HJ method
function [x]=explore(h,n,f,x0)
x = x0; f0 = eval(f);
for i=1:n
x(i) = x(i) + h(i); ﬀ= eval(f);
if ﬀ< f0,
f0 = ﬀ;
else
x(i) = x0(i) - h(i);
ﬀ= eval(f);
if ﬀ< f0,
f0 = ﬀ;
else,
x(i) = x0 (i);
end
end
end
The Method of Nelder and Mead
This method, proposed in [NM65], employs local linear approximants of f
to generate a sequence of points x(k), approximations of x∗, starting from
simple geometrical considerations. To explain the details of the algorithm,
we begin by noticing that a plane in Rn is uniquely determined by ﬁxing
n + 1 points that must not be lying on a hyperplane.
Denote such points by x(k), for k = 0, . . . , n. They could be generated as
x(k) = x(0) + hkek,
k = 1, . . . , n
(7.23)
having selected the steplengths hk ∈R+ in such a way that the variations
(7.22) are of comparable size.
Let us now denote by x(M), x(m) and x(µ) those points of the set

x(k)
at which f respectively attains its maximum and minimum value and the
value immediately preceding the maximum. Moreover, denote by x(k)
c
the
centroid of point x(k) deﬁned as
x(k)
c
= 1
n
n

j=0,j̸=k
x(j).

298
7. Nonlinear Systems and Numerical Optimization
The method generates a sequence of approximations of x∗, starting from
x(k), by employing only three possible transformations: reﬂections with
respect to centroids, dilations and contractions. Let us examine the details
of the algorithm assuming that n + 1 initial points are available.
1. Determine the points x(M), x(m) and x(µ).
2. Compute as an approximation of x∗the point
¯x =
1
n + 1
n

i=0
x(i)
and check if ¯x is suﬃciently close (in a sense to be made precise) to
x∗. Typically, one requires that the standard deviation of the values
f(x(0)), . . . , f(x(n)) from
¯f =
1
n + 1
n

i=0
f(x(i))
are less than a ﬁxed tolerance ε, that is
1
n
n

i=0
+
f(x(i)) −¯f
,2
< ε.
Otherwise, x(M) is reﬂected with respect to x(M)
c
, that is, the follow-
ing new point xr is computed
xr = (1 + α)x(M)
c
−αx(M),
where α ≥0 is a suitable reﬂection factor. Notice that the method
has moved along the “opposite” direction to x(M). This statement has
a geometrical interpretation in the case n = 2, since the points x(k)
coincide with x(M), x(m) and x(µ). They thus deﬁne a plane whose
slope points from x(M) towards x(m) and the method provides a step
along this direction.
3. If f(x(m)) ≤f(x(r)) ≤f(x(µ)), the point x(M) is replaced by x(r)
and the algorithm returns to step 2.
4. If f(x(r)) < f(x(m)) then the reﬂection step has produced a new
minimizer. This means that the minimizer could lie outside the set
deﬁned by the convex hull of the considered points. Therefore, this
set must be expanded by computing the new vertex
x(e) = βx(r) + (1 −β)x(M)
c
,
where β > 1 is an expansion factor. Then, before coming back to step
2., two possibilities arise:

7.2 Unconstrained Optimization
299
4a. if f(x(e)) < f(x(m)) then x(M) is replaced by x(e);
4b. f(x(e)) ≥f(x(m)) then x(M) is replaced by x(r) since f(x(r)) <
f(x(m)).
5. If f(x(r)) > f(x(µ)) then the minimizer probably lies within a subset
of the convex hull of points

x(k)
and, therefore, two diﬀerent ap-
proaches can be pursued to contract this set. If f(x(r)) < f(x(M)),
the contraction generates a new point of the form
x(co) = γx(r) + (1 −γ)x(M)
c
,
γ ∈(0, 1),
otherwise,
x(co) = γx(M) + (1 −γ)x(M)
c
,
γ ∈(0, 1),
Finally, before returning to step 2., if f(x(co)) < f(x(M)) and f(x(co)) <
f(x(r)), the point x(M) is replaced by x(co), while if f(x(co)) ≥f(x(M))
or if f(x(co)) > f(x(r)), then n new points x(k) are generated, with
k = 1, . . . , n, by halving the distances between the original points
and x(0).
As far as the choice of the parameters α, β and γ is concerned, the following
values are empirically suggested in [NM65]: α = 1, β = 2 and γ = 1/2. The
resulting scheme is known as the Simplex method (that must not be con-
fused with a method sharing the same name used in linear programming),
since the set of the points x(k), together with their convex combinations,
form a simplex in Rn.
The convergence rate of the method is strongly aﬀected by the orientation
of the starting simplex. To address this concern, in absence of information
about the behavior of f, the initial choice (7.23) turns out to be satisfactory
in most cases.
We ﬁnally mention that the Simplex method is the basic ingredient of
the MATLAB function fmins for function minimization in n dimensions.
Example 7.5 Let us compare the performances of the Simplex method with the
Hooke and Jeeves method, in the minimization of the Rosembrock function
f(x) = 100(x2 −x2
1)2 + (1 −x1)2.
(7.24)
This function has a minimizer at (1, 1)T and represents a severe benchmark for
testing numerical methods in minimization problems. The starting point for both
methods is set equal to x(0) = (−1.2, 1)T , while the step sizes are taken equal
to h1 = 0.6 and h2 = 0.5, in such a way that (7.23) is satisﬁed. The stopping
tolerance on the residual is set equal to 10−4. For the implementation of Simplex
method, we have used the MATLAB function fmins.
Figure 7.2 shows the iterates computed by the Hooke and Jeeves method (of
which one in every ten iterates have been reported, for the sake of clarity) and by

300
7. Nonlinear Systems and Numerical Optimization
−1.5
−1
−0.5
0
0.5
1
1.5
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
FIGURE
7.2.
Convergence
histories
of
the
Hooke
and
Jeeves
method
(crossed-line) and the Simplex method (circled-line). The level curves of the min-
imized function (7.24) are reported in dashed line
the Simplex method, superposed to the level curves of the Rosembrock function.
The graph demonstrates the diﬃculty of this benchmark: actually, the function
is like a curved, narrow valley, which attains its minimum along the parabola of
equation x2
1 −x2 = 0.
The Simplex method converges in only 165 iterations, while 935 are needed for
the Hooke and Jeeves method to converge. The former scheme yields a solution
equal to (0.999987, 0.999978)T , while the latter gives the vector (0.9655, 0.9322)T .
•
7.2.2
Descent Methods
In this section we introduce iterative methods that are more sophisticated
than those examined in Section 7.2.1. They can be formulated as follows:
given an initial vector x(0) ∈Rn, compute for k ≥0 until convergence
x(k+1) = x(k) + αkd(k),
(7.25)
where d(k) is a suitably chosen direction and αk is a positive parameter
(called stepsize) that measures the step along the direction d(k). This di-
rection d(k) is a descent direction if
d(k)T ∇f(x(k)) < 0
if ∇f(x(k)) ̸= 0,
d(k) = 0
if ∇f(x(k)) = 0.
(7.26)
A descent method is a method like (7.25), in which the vectors d(k) are
descent directions.
Property (7.20) ensures that there exists αk > 0, suﬃciently small, such
that
f(x(k) + αkd(k)) < f(x(k)),
(7.27)

7.2 Unconstrained Optimization
301
provided that f is continuously diﬀerentiable. Actually, taking in (7.20)
ξ = x(k) + ϑαkd(k) with ϑ ∈(0, 1), and employing the continuity of ∇f,
we get
f(x(k) + αkd(k)) −f(x(k)) = αk∇f(x(k))T d(k) + ε,
(7.28)
where ε tends to zero as αk tends to zero. As a consequence, if αk > 0 is
suﬃciently small, the sign of the left-side of (7.28) coincides with the sign
of ∇f(x(k))T d(k), so that (7.27) is satisﬁed if d(k) is a descent direction.
Diﬀerent choices of d(k) correspond to diﬀerent methods. In particular,
we recall the following ones:
- Newton’s method, in which
d(k) = −H−1(x(k))∇f(x(k)),
provided that H is positive deﬁnite within a suﬃciently large neigh-
borhood of point x∗;
- inexact Newton’s methods, in which
d(k) = −B−1
k ∇f(x(k)),
where Bk is a suitable approximation of H(x(k));
- the gradient method or steepest descent method, corresponding to setting
d(k) = −∇f(x(k)). This method is thus an inexact Newton’s method,
in which Bk = I. It can also be regarded as a gradient-like method,
since d(k)T ∇f(x(k)) = −∥∇f(x(k))∥2
2;
- the conjugate gradient method, for which
d(k) = −∇f(x(k)) + βkd(k−1),
where βk is a scalar to be suitably selected in such a way that the
directions

d(k)
turn out to be mutually orthogonal with respect to
a suitable scalar product.
Selecting d(k) is not enough to completely identify a descent method,
since it remains an open problem how to determine αk in such a way that
(7.27) is fulﬁlled without resorting to excessively small stepsizes αk (and,
thus, to methods with a slow convergence).
A method for computing αk consists of solving the following minimiza-
tion problem in one dimension:
ﬁnd α such that φ(α) = f(x(k) + αd(k)) is minimized.
(7.29)
In such a case we have the following result.

302
7. Nonlinear Systems and Numerical Optimization
Theorem 7.3 Consider the descent method (7.25). If at the generic step
k, the parameter αk is set equal to the exact solution of (7.29), then the
following orthogonality property holds
∇f(x(k+1))T d(k) = 0.
Proof. Let αk be a solution to (7.29). Then, the ﬁrst derivative of φ, given by
φ′(α) =
n

i=1
∂f
∂xi (x(k) + αkd(k)) ∂
∂α(x(k)
i
+ αd(k)
i
) = ∇f(x(k) + αkd(k))T d(k),
vanishes at α = αk. The thesis then follows, recalling the deﬁnition of x(k+1). 3
Unfortunately, except for in special cases (which are nevetherless quite
relevant, see Section 7.2.4), providing an exact solution of (7.29) is not fea-
sible, since this is a nonlinear problem. One possible strategy consists of
approximating f along the straight line x(k) + αd(k) through an interpo-
lating polynomial and then minimizing this polynomial (see the quadratic
interpolation Powell methods and cubic interpolation Davidon methods in
[Wal75]).
Generally speaking, a process that leads to an approximate solution to
(7.29) is said to be a line search technique and is addressed in the next
section.
7.2.3
Line Search Techniques
The methods that we are going to deal with in this section, are iterative
techniques that terminate as soon as some accuracy stopping criterion on
αk is satisﬁed. We shall assume that (7.26) holds.
Practical experience reveals that it is not necessary to solve accurately
for (7.29) in order to devise eﬃcient methods, rather, it is crucial to enforce
some limitation on the step lengths (and, thus, on the admissible values for
αk). Actually, without introducing any limitation, a reasonable request on
αk would seem be that the new iterate x(k+1) satisﬁes the inequality
f(x(k+1)) < f(x(k)),
(7.30)
where x(k) and d(k) have been ﬁxed. For this purpose, the procedure based
on starting from a (suﬃciently large) value of the step length αk and halve
this value until (7.30) is fulﬁlled, can yield completely wrong results (see,
[DS83]).
More stringent criteria than (7.30) should be adopted in the choice of
possible values for αk. To this end, we notice that two kinds of diﬃculties
arise with the above examples: a slow descent rate of the sequence and the
use of small stepsizes.

7.2 Unconstrained Optimization
303
The ﬁrst diﬃculty can be overcome by requiring that
0 ≥vM(x(k+1))
=
1
αk
6
f(x(k)) −f(x(k) + αkd(k))
7
≥
−σ∇f(x(k))T d(k),
(7.31)
with σ ∈(0, 1/2). This amounts to requiring that the average descent
rate vM of f along d(k), evaluated at x(k+1), be at least equal to a given
fraction of the initial descent rate at x(k). To avoid the generation of too
small stepsizes, we require that the descent rate in the direction d(k) at
x(k+1) is not less than a given fraction of the descent rate at x(k)
|∇f(x(k) + αkd(k))T d(k)| ≤β|∇f(x(k))T d(k)|,
(7.32)
with β ∈(σ, 1) in such a way as to also satisfy (7.31). In computational
practice, σ ∈[10−5, 10−1] and β ∈[10−1, 1
2] are usual choices. Sometimes,
(7.32) is replaced by the milder condition
∇f(x(k) + αkd(k))T d(k) ≥β∇f(x(k))T d(k)
(7.33)
(recall that ∇f(x(k))T d(k) is negative, since d(k) is a descent direction).
The following property ensures that, under suitable assumptions, it is pos-
sible to ﬁnd out values of αk which satisfy (7.31)-(7.32) or (7.31)-(7.33).
Property 7.5 Assume that f(x) ≥M for any x ∈Rn. Then there exists
an interval I = [c, C] for the descent method, with 0 < c < C, such that
∀αk ∈I, (7.31), (7.32) (or (7.31)-(7.33)) are satisﬁed, with σ ∈(0, 1/2)
and β ∈(σ, 1).
Under the constraint of fulﬁlling conditions (7.31) and (7.32), several
choices for αk are available. Among the most up-to-date strategies, we re-
call here the backtracking techniques: having ﬁxed σ ∈(0, 1/2), then start
with αk = 1 and then keep on reducing its value by a suitable scale factor
ρ ∈(0, 1) (backtrack step) until (7.31) is satisﬁed. This procedure is im-
plemented in Program 62, which requires as input parameters the vector x
containing x(k), the macros f and J of the functional expressions of f and
its Jacobian, the vector d of the direction d(k), and a value for σ (usually
of the order of 10−4) and the scale factor ρ. In output, the code returns
the vector x(k+1), computed using a suitable value of αk.
Program 62 - backtrackr : Backtraking for line search
function [xnew]= backtrackr(sigma,rho,x,f,J,d)
alphak = 1; fk = eval(f); Jfk = eval (J);
xx = x; x = x + alphak * d; fk1 = eval (f);
while fk1 > fk + sigma * alphak * Jfk’*d

304
7. Nonlinear Systems and Numerical Optimization
alphak = alphak*rho;
x = xx + alphak*d;
fk1 = eval(f);
end
Other commonly used strategies are those developed by Armijo and Gold-
stein (see [Arm66], [GP67]). Both use σ ∈(0, 1/2). In the Armijo formula,
one takes αk = βmk ¯α, where β ∈(0, 1), ¯α > 0 and mk is the ﬁrst non-
negative integer such that (7.31) is satisﬁed. In the Goldstein formula, the
parameter αk is determined in such a way that
σ ≤f(x(k) + αkd(k)) −f(x(k))
αk∇f(x(k))T d(k)
≤1 −σ.
(7.34)
A procedure for computing αk that satisﬁes (7.34) is provided in [Ber82],
Chapter 1. Of course, one can even choose αk = ¯α for any k, which is
clearly convenient when evaluating f is a costly task.
In any case, a good choice of the value ¯α is mandatory. In this respect,
one can proceed as follows. For a given value ¯α, the second degree poly-
nomial Π2 along the direction d(k) is constructed, subject to the following
interpolation constraints
Π2(x(k)) = f(x(k)),
Π2(x(k) + ¯αd(k)) = f(x(k) + ¯αd(k)),
Π′
2(x(k)) = ∇f(x(k))T d(k).
Next, the value ˜α is computed such that Π2 is minimized, then, we let
¯α = ˜α.
7.2.4
Descent Methods for Quadratic Functions
A case of remarkable interest, where the parameter αk can be exactly com-
puted, is the problem of minimizing the quadratic function
f(x) = 1
2xT Ax −bT x,
(7.35)
where A∈Rn×n is a symmetric and positive deﬁnite matrix and b ∈Rn.
In such a case, as already seen in Section 4.3.3, a necessary condition for
x∗to be a minimizer for f is that x∗is the solution of the linear system
(3.2). Actually, it can be checked that if f is a quadratic function
∇f(x) = Ax −b = −r,
H(x) = A.
As a consequence, all gradient-like iterative methods developed in Section
4.3.3 for linear systems, can be extended tout-court to solve minimization
problems.

7.2 Unconstrained Optimization
305
In particular, having ﬁxed a descent direction d(k), we can determine
the optimal value of the acceleration parameter αk that appears in (7.25),
in such a way as to ﬁnd the point where the function f, restricted to the
direction d(k), is minimized. Setting to zero the directional derivative, we
get
d
dαk
f(x(k) + αkd(k)) = −d(k)T r(k) + αkd(k)T Ad(k) = 0
from which the following expression for αk is obtained
αk =
d(k)T r(k)
d(k)T Ad(k) .
(7.36)
The error introduced by the iterative process (7.25) at the k-th step is
∥x(k+1) −x∗∥2
A =

x(k+1) −x∗T A

x(k+1) −x∗
= ∥x(k) −x∗∥2
A + 2αkd(k)T A

x(k) −x∗
+ α2
kd(k)T Ad(k).
(7.37)
On the other hand ∥x(k) −x∗∥2
A = r(k)T A−1r(k), so that from (7.37) it
follows that
∥x(k+1) −x∗∥2
A = ρk∥x(k) −x∗∥2
A
(7.38)
having denoted by ρk = 1 −σk, with
σk = (d(k)T r(k))2/
+
d(k),T
Ad(k) +
r(k),T
A−1r(k)

.
Since A is symmetric and positive deﬁnite, σk is always positive. Moreover,
it can be directly checked that ρk is strictly less than 1, except when d(k)
is orthogonal to r(k), in which case ρk = 1.
The choice d(k) = r(k), which leads to the steepest descent method, pre-
vents this last circumstance from arising. In such a case, from (7.38) we
get
∥x(k+1) −x∗∥A ≤λmax −λmin
λmax + λmin
∥x(k) −x∗∥A
(7.39)
having employed the following result.
Lemma 7.1 (Kantorovich inequality) Let A ∈Rn×n be a symmetric
positive deﬁnite matrix whose eigenvalues with largest and smallest module
are given by λmax and λmin, respectively. Then, ∀y ∈Rn, y ̸= 0,
(yT y)2
(yT Ay)(yT A−1y) ≥
4λmaxλmin
(λmax + λmin)2 .

306
7. Nonlinear Systems and Numerical Optimization
It follows from (7.39) that, if A is ill-conditioned, the error reducing factor
for the steepest descent method is close to 1, yielding a slow convergence to
the minimizer x∗. As done in Chapter 4, this drawback can be overcome
by introducing directions d(k) that are mutually A-conjugate, i.e.
d(k)T Ad(m) = 0
if k ̸= m.
The corresponding methods enjoy the following ﬁnite termination property.
Property 7.6 A method for computing the minimizer x∗of the quadratic
function (7.35) which employs A-conjugate directions terminates after at
most n steps if the acceleration parameter αk is selected as in (7.36). More-
over, for any k, x(k+1) is the minimizer of f over the subspace generated
by the vectors x(0), d(0), . . . , d(k) and
r(k+1)T d(m) = 0
∀m ≤k.
The A-conjugate directions can be determined by following the proce-
dure described in Section 4.3.4. Letting d(0) = r(0), the conjugate gradient
method for function minimization is
d(k+1) = r(k) + βkd(k),
βk = −r(k+1)T Ad(k)
d(k)T Ad(k)
= r(k+1)T r(k+1)
r(k)T r(k)
,
x(k+1) = x(k) + αkd(k).
It satisﬁes the following error estimate
∥x(k) −x∗∥A ≤2

K2(A) −1

K2(A) + 1
k
∥x(0) −x∗∥A,
which can be improved by lowering the condition number of A, i.e., resort-
ing to the preconditioning techniques that have been dealt with in Section
4.3.2.
Remark 7.2 (The nonquadratic case) The conjugate gradient method
can be extended to the case in which f is a non quadratic function. However,
in such an event, the acceleration parameter αk cannot be exactly deter-
mined a priori, but requires the solution of a local minimization problem.
Moreover, the parameters βk can no longer be uniquely found. Among the
most reliable formulae, we recall the one due to Fletcher-Reeves,
β1 = 0,
βk =
∥∇f(x(k))∥2
2
∥∇f(x(k−1))∥2
2
,
for k > 1

7.2 Unconstrained Optimization
307
and the one due to Polak-Ribi´ere
β1 = 0,
βk = ∇f(x(k))
T (∇f(x(k)) −∇f(x(k−1)))
∥∇f(x(k−1))∥2
2
,
for k > 1.
■
7.2.5
Newton-like Methods for Function Minimization
An alternative is provided by Newton’s method, which diﬀers from its ver-
sion for nonlinear systems in that now it is no longer applied to f, but to
its gradient.
Using the notation of Section 7.2.2, Newton’s method for function mini-
mization amounts to computing, for k = 0, 1, . . . , until convergence
d(k) = −H−1
k ∇f(x(k)),
x(k+1) = x(k) + d(k),
(7.40)
where x(0) ∈Rn is a given initial vector and having set Hk = H(x(k)). The
method can be derived by truncating Taylor’s expansion of f(x(k)) at the
second-order
f(x(k) + p) ≃f(x(k)) + ∇f(x(k))T p + 1
2pT Hkp.
(7.41)
Selecting p in (7.41) in such a way that the new vector x(k+1) = x(k) + p
satisﬁes ∇f(xk+1) = 0, we end up with method (7.40), which thus con-
verges in one step if f is quadratic.
In the general case, a result analogous to Theorem 7.1 also holds for func-
tion minimization. Method (7.40) is therefore locally quadratically conver-
gent to the minimizer x∗. However, it is not convenient to use Newton’s
method from the beginning of the computation, unless x(0) is suﬃciently
close to x∗. Otherwise, indeed, Hk could not be invertible and the direc-
tions d(k) could fail to be descent directions. Moreover, if Hk is not positive
deﬁnite, nothing prevents the scheme (7.40) from converging to a saddle
point or a maximizer, which are points where ∇f is equal to zero. All these
drawbacks, together with the high computational cost (recall that a linear
system with matrix Hk must be solved at each iteration), prompt suit-
ably modifying method (7.40), which leads to the so-called quasi-Newton
methods.
A ﬁrst modiﬁcation, which applies to the case where Hk is not posi-
tive deﬁnite, yields the so-called Newton’s method with shift. The idea is
to prevent Newton’s method from converging to non-minimizers of f, by
applying the scheme to a new Hessian matrix ˜Hk = Hk + µkIn, where, as

308
7. Nonlinear Systems and Numerical Optimization
usual, In denotes the identity matrix of order n and µk is selected in such
a way that ˜Hk is positive deﬁnite. The problem is to determine the shift
µk with a reduced eﬀort. This can be done, for instance, by applying the
Gershgorin theorem to the matrix ˜Hk (see Section 5.1). For further details
on the subject, see [DS83] and [GMW81].
7.2.6
Quasi-Newton Methods
At the generic k-th iteration, a quasi-Newton method for function mini-
mization performs the following steps:
1. compute the Hessian matrix Hk, or a suitable approximation Bk;
2. ﬁnd a descent direction d(k) (not necessarily coinciding with the di-
rection provided by Newton’s method), using Hk or Bk;
3. compute the acceleration parameter αk;
4. update the solution, setting x(k+1) = x(k) + αkd(k), according to a
global convergence criterion.
In the particular case where d(k) = −H−1
k ∇f(x(k)), the resulting scheme is
called the damped Newton’s method. To compute Hk or Bk, one can resort
to either Newton’s method or secant-like methods, which will be considered
in Section 7.2.7.
The criteria for selecting the parameter αk, that have been discussed in
Section 7.2.3, can now be usefully employed to devise globally convergent
methods. Property 7.5 ensures that there exist values of αk satisfying (7.31),
(7.33) or (7.31), (7.32).
Let us then assume that a sequence of iterates x(k), generated by a
descent method for a given x(0), converge to a vector x∗. This vector will
not be, in general, a critical point for f. The following result gives some
conditions on the directions d(k) which ensure that the limit x∗of the
sequence is also a critical point of f.
Property 7.7 (Convergence) Let f : Rn →R be a continuously diﬀer-
entiable function, and assume that there exists L > 0 such that
∥∇f(x) −∇f(y)∥2 ≤L∥x −y∥2.
Then, if

x(k)
is a sequence generated by a gradient-like method which
fulﬁlls (7.31) and (7.33), then, one (and only one) of the following events
can occur:
1. ∇f(x(k)) = 0 for some k;
2.
lim
k→∞f(x(k)) = −∞;

7.2 Unconstrained Optimization
309
3.
lim
k→∞
∇f(x(k))T d(k)
∥d(k)∥2
= 0.
Thus, unless the pathological cases where the directions d(k) become too
large or too small with respect to ∇f(x(k)) or, even, are orthogonal to
∇f(x(k)), any limit of the sequence

x(k)
is a critical point of f.
The convergence result for the sequence x(k) can also be extended to the
sequence f(x(k)). Indeed, the following result holds.
Property 7.8 Let

x(k)
be a convergent sequence generated by a gradient-
like method, i.e., such that any limit of the sequence is also a critical point
of f. If the sequence

x(k)
is bounded, then ∇f(x(k)) tends to zero as
k →∞.
For the proofs of the above results, see [Wol69] and [Wol71].
7.2.7
Secant-like methods
In quasi-Newton methods the Hessian matrix H is replaced by a suitable
approximation. Precisely, the generic iterate is
x(k+1) = x(k) −B−1
k ∇f(x(k)) = x(k) + s(k).
Assume that f : Rn →R is of class C2 on an open convex set D ⊂Rn.
In such a case, H is symmetric and, as a consequence, approximants Bk
of H ought to be symmetric. Moreover, if Bk were symmetric at a point
x(k), we would also like the next approximant Bk+1 to be symmetric at
x(k+1) = x(k) + s(k).
To generate Bk+1 starting from Bk, consider the Taylor expansion
∇f(x(k)) = ∇f(x(k+1)) + Bk+1(x(k) −x(k+1)),
from which we get
Bk+1s(k) = y(k),
with y(k) = ∇f(x(k+1)) −∇f(x(k)).
Using again a series expansion of B, we end up with the following ﬁrst-order
approximation of H
Bk+1 = Bk + (y(k) −Bks(k))cT
cT s(k)
,
(7.42)
where c ∈Rn and having assumed that cT s(k) ̸= 0. We notice that taking
c = s(k) yields Broyden’s method, already discussed in Section 7.1.4 in the
case of systems of nonlinear equations.
Since (7.42) does not guarantee that Bk+1 is symmetric, it must be
suitably modiﬁed. A way for constructing a symmetric approximant Bk+1

310
7. Nonlinear Systems and Numerical Optimization
consists of choosing c = y(k) −Bks(k) in (7.42), assuming that (y(k) −
Bks(k))T s(k) ̸= 0. By so doing, the following symmetric ﬁrst-order approx-
imation is obtained
Bk+1 = Bk + (y(k) −Bks(k))(y(k) −Bks(k))T
(y(k) −Bks(k))T s(k)
.
(7.43)
From a computational standpoint, disposing of an approximation for H
is not completely satisfactory, since the inverse of the approximation of
H appears in the iterative methods that we are dealing with. Using the
Sherman-Morrison formula (3.57), with Ck = B−1
k , yields the following
recursive formula for the computation of the inverse
Ck+1 = Ck + (s(k) −Cky(k))(s(k) −Cky(k))T
(s(k) −Cky(k))T y(k)
, k = 0, 1, . . .
(7.44)
having assumed that y(k) = Bs(k), where B is a symmetric nonsingular
matrix, and that (s(k) −Cky(k))T y(k) ̸= 0.
An algorithm that employs the approximations (7.43) or (7.44), is po-
tentially unstable when (s(k) −Cky(k))T y(k) ≃0, due to rounding errors.
For this reason, it is convenient to set up the previous scheme in a more
stable form. To this end, instead of (7.42), we introduce the approximation
B(1)
k+1 = Bk + (y(k) −Bks(k))cT
cT s(k)
,
then, we deﬁne B(2)
k+1 as being the symmetric part
B(2)
k+1 = B(1)
k+1 + (B(1)
k+1)T
2
.
The procedure can be iterated as follows
B(2j+1)
k+1
= B(2j)
k+1 + (y(k) −B(2j)
k+1s(k))cT
cT s
,
B(2j+2)
k+1
= B(2j+1)
k+1
+ (B(2j+1)
k+1
)T
2
(7.45)
with k = 0, 1, . . . and having set B(0)
k+1 = Bk. It can be shown that the limit
as j tends to inﬁnity of (7.45) is
lim
j→∞B(j)
= Bk+1 = Bk + (y(k) −Bks(k))cT + c(y(k) −Bks(k))T
cT s(k)
−(y(k) −Bks(k))T s(k)
(cT s(k))2
ccT ,
(7.46)

7.3 Constrained Optimization
311
having assumed that cT s(k) ̸= 0. If c = s(k), the method employing (7.46)
is known as the symmetric Powell-Broyden method. Denoting by BSP B
the corresponding matrix Bk+1, it can be shown that BSP B is the unique
solution to the problem:
ﬁnd ¯B such that ∥¯B −B∥F is minimized,
where ¯Bs(k) = y(k) and ∥· ∥F is the Frobenius norm.
As for the error made approximating H(x(k+1)) with BSP B, it can be proved
that
∥BSP B −H(x(k+1))∥F ≤∥Bk −H(x(k))∥F + 3L∥s(k)∥,
where it is assumed that H is Lipschitz continuous, with Lipschitz constant
L, and that the iterates x(k+1) and x(k) belong to D.
To deal with the particular case in which the Hessian matrix is not only
symmetric but also positive deﬁnite, we refer to [DS83], Section 9.2.
7.3
Constrained Optimization
The simplest case of constrained optimization can be formulated as follows.
Given f : Rn →R,
minimize f(x), with x ∈Ω⊂Rn.
(7.47)
More precisely, the point x∗is said to be a global minimizer in Ωif it
satisﬁes (7.47), while it is a local minimizer if ∃R > 0 such that
f(x∗) ≤f(x),
∀x ∈B(x∗; R) ⊂Ω.
Existence of solutions to problem (7.47) is, for instance, ensured by the
Weierstrass theorem, in the case in which f is continuous and Ωis a closed
and bounded set. Under the assumption that Ωis a convex set, the following
optimality conditions hold.
Property 7.9 Let Ω⊂Rn be a convex set, x∗∈Ωand f ∈C1(B(x∗; R)),
for a suitable R > 0. Then:
1. if x∗is a local minimizer of f then
∇f(x∗)T (x −x∗) ≥0,
∀x ∈Ω;
(7.48)
2. moreover, if f is convex on Ω(see (7.21)) and (7.48) is satisﬁed, then
x∗is a global minimizer of f.

312
7. Nonlinear Systems and Numerical Optimization
We recall that f : Ω→R is a strongly convex function if ∃ρ > 0 such
that
f[αx + (1 −α)y] ≤αf(x) + (1 −α)f(y) −α(1 −α)ρ∥x −y∥2
2,
(7.49)
∀x, y ∈Ωand ∀α ∈[0, 1]. The following result holds.
Property 7.10 Let Ω⊂Rn be a closed and convex set and f be a strongly
convex function in Ω. Then there exists a unique local minimizer x∗∈Ω.
Throughout this section, we refer to [Avr76], [Ber82], [CCP70], [Lue73] and
[Man69], for the proofs of the quoted results and further details.
A remarkable instance of (7.47) is the following problem: given f : Rn →R,
minimize f(x), under the constraint that h(x) = 0,
(7.50)
where h : Rn →Rm, with m ≤n, is a given function of components
h1, . . . , hm. The analogues of critical points in problem (7.50) are called
the regular points.
Deﬁnition 7.2 A point x∗∈Rn, such that h(x∗) = 0, is said to be
regular if the column vectors of the Jacobian matrix Jh(x∗) are linearly
independent, having assumed that hi ∈C1(B(x∗; R)), for a suitable R > 0
and i = 1, . . . , m.
■
Our aim now is to convert problem (7.50) into an unconstrained minimiza-
tion problem of the form (7.2), to which the methods introduced in Section
7.2 can be applied.
For this purpose, we introduce the Lagrangian function L : Rn+m →R
L(x, λ) = f(x) + λT h(x),
where the vector λ is called the Lagrange multiplier. Moreover, let us de-
note by JL the Jacobian matrix associated with L, but where the partial
derivatives are only taken with respect to the variables x1, . . . , xn. The link
between (7.2) and (7.50) is then expressed by the following result.
Property 7.11 Let x∗be a local minimizer for (7.50) and suppose that,
for a suitable R > 0, f, hi ∈C1(B(x∗; R)), for i = 1, . . . , m. Then there
exists a unique vector λ∗∈Rm such that JL(x∗, λ∗) = 0.
Conversely, assume that x∗∈Rn satisﬁes h(x∗) = 0 and that, for a
suitable R > 0 and i = 1, . . . , m, f, hi ∈C2(B(x∗; R)). Let HL be the
matrix of entries ∂2L/∂xi∂xj for i, j = 1, . . . , n. If there exists a vector
λ∗∈Rm such that JL(x∗, λ∗) = 0 and
zT HL(x∗, λ∗)z > 0
∀z ̸= 0,
with
∇h(x∗)T z = 0,
then x∗is a strict local minimizer of (7.50).

7.3 Constrained Optimization
313
The last class of problems that we are going to deal with includes the case
where inequality constraints are also present, i.e.: given f : Rn →R,
minimize f(x), under the constraint that h(x) = 0 and g(x) ≤0,(7.51)
where h : Rn →Rm, with m ≤n, and g : Rn →Rr are two given functions.
It is understood that g(x) ≤0 means gi(x) ≤0 for i = 1, . . . , r. Inequality
constraints give rise to some extra formal complication with respect to the
case previously examined, but do not prevent converting the solution of
(7.51) into the minimization of a suitable Lagrangian function.
In particular, Deﬁnition 7.2 becomes
Deﬁnition 7.3 Assume that hi, gj ∈C1(B(x∗; R)) for a suitable R >
0 with i = 1, . . . , m and j = 1, . . . , r, and denote by J (x∗) the set of
indices j such that gj(x∗) = 0. A point x∗∈Rn such that h(x∗) = 0 and
g(x∗) ≤0 is said to be regular if the column vectors of the Jacobian matrix
Jh(x∗) together with the vectors ∇gj(x∗), j ∈J (x∗) form a set of linearly
independent vectors.
■
Finally, an analogue of Property 7.11 holds, provided that the following
Lagrangian function is used
M(x, λ, µ) = f(x) + λT h(x) + µT g(x)
instead of L and that further assumptions on the constraints are made.
For the sake of simplicity, we report in this case only the following nec-
essary condition for optimality of problem (7.51) to hold.
Property 7.12 Let x∗be a regular local minimizer for (7.51) and suppose
that, for a suitable R > 0, f, hi, gj ∈C1(B(x∗; R)) with i = 1, . . . , m,
j = 1, . . . , r. Then, there exist only two vectors λ∗∈Rm and µ∗∈Rr,
such that JM(x∗, λ∗, µ∗) = 0 with µ∗
j ≥0 and µ∗
jgj(x∗) = 0 ∀j = 1, . . . , r.
7.3.1
Kuhn-Tucker Necessary Conditions for Nonlinear
Programming
In this section we recall some results, known as Kuhn-Tucker conditions
[KT51], that ensure in general the existence of a local solution for the non-
linear programming problem. Under suitable assumptions they also guar-
antee the existence of a global solution. Throughout this section we suppose
that a minimization problem can always be reformulated as a maximization
one.

314
7. Nonlinear Systems and Numerical Optimization
Let us consider the general nonlinear programming problem:
given f : Rn →R,
maximize f(x), subject to
gi(x) ≤bi
i = 1, . . . , l,
gi(x) ≥bi
i = l + 1, . . . , k,
gi(x) = bi
i = k + 1, . . . , m,
x ≥0.
(7.52)
A vector x that satisﬁes the constraints above is called a feasible solution of
(7.52) and the set of the feasible solutions is called the feasible region. We
assume henceforth that f, gi ∈C1(Rn), i = 1, . . . , m, and deﬁne the sets
I= = {i : gi(x∗) = bi}, I̸= = {i : gi(x∗) ̸= bi}, J= = {i : x∗
i = 0}, J> =
{i : x∗
i > 0}, having denoted by x∗a local maximizer of f. We associate
with (7.52) the following Lagrangian
L(x, λ) = f(x) +
m

i=1
λi [bi −gi(x)] −
m+n

i=m+1
λixi−m.
The following result can be proved.
Property 7.13 (Kuhn-Tucker conditions I and II) If f has a con-
strained local maximum at the point x = x∗, it is necessary that a vector
λ∗∈Rm+n exists such that (ﬁrst Kuhn-Tucker condition)
∇xL(x∗, λ∗) ≤0,
where strict equality holds for every component i ∈J>. Moreover (second
Kuhn-Tucker condition)
(∇xL(x∗, λ∗))T x∗= 0.
The other two necessary Kuhn-Tucker conditions are as follows.
Property 7.14 Under the same hypothesis as in Property 7.13, the third
Kuhn-Tucker condition requires that:
∇λL(x∗, λ∗) ≥0
i = 1, . . . , l,
∇λL(x∗, λ∗) ≤0
i = l + 1, . . . , k,
∇λL(x∗, λ∗) = 0
i = k + 1, . . . , m.
Moreover (fourth Kuhn-Tucker condition)
(∇λL(x∗, λ∗))T x∗= 0.

7.3 Constrained Optimization
315
It is worth noticing that the Kuhn-Tucker conditions hold provided that
the vector λ∗exists. To ensure this, it is necessary to introduce a further
geometric condition that is known as constraint qualiﬁcation (see [Wal75],
p. 48).
We conclude this section by the following fundamental theorem which
establishes when the Kuhn-Tucker conditions become also suﬃcient for the
existence of a global maximizer for f.
Property 7.15 Assume that the function f in (7.52) is a concave func-
tion (i.e., −f is convex) in the feasible region. Suppose also that the point
(x∗, λ∗) satisﬁes all the Kuhn-Tucker necessary conditions and that the
functions gi for which λ∗
i > 0 are convex while those for which λ∗
i < 0 are
concave. Then f(x∗) is the constrained global maximizer of f for problem
(7.52).
7.3.2
The Penalty Method
The basic idea of this method is to eliminate, partly or completely, the
constraints in order to transform the constrained problem into an uncon-
strained one. This new problem is characterized by the presence of a pa-
rameter that yields a measure of the accuracy at which the constraint is
actually imposed.
Let us consider the constrained problem (7.50), assuming we are search-
ing for the solution x∗only in Ω⊂Rn. Suppose that such a problem admits
at least one solution in Ωand write it in the following penalized form
minimize Lα(x)
for x ∈Ω,
(7.53)
where
Lα(x) = f(x) + 1
2α∥h(x)∥2
2.
The function Lα : Rn →R is called the penalized Lagrangian, and α is
called the penalty parameter. It is clear that if the constraint was exactly
satisﬁed then minimizing f would be equivalent to minimizing Lα.
The penalty method is an iterative technique for solving (7.53).
For k = 0, 1, . . . , until convergence, one must solve the sequence of prob-
lems
minimize Lαk(x)
with x ∈Ω,
(7.54)
where {αk} is an increasing monotonically sequence of positive penalty
parameters, such that αk →∞as k →∞. As a consequence, after choosing
αk, at each step of the penalty process we have to solve a minimization
problem with respect to the variable x, leading to a sequence of values x∗
k,
solutions to (7.54). By doing so, the objective function Lαk(x) tends to
inﬁnity, unless h(x) is equal to zero.

316
7. Nonlinear Systems and Numerical Optimization
The minimization problems can then be solved by one of the methods
introduced in Section 7.2. The following property ensures the convergence
of the penalty method in the form (7.53).
Property 7.16 Assume that f : Rn →R and h : Rn →Rm, with m ≤n,
are continuous functions on a closed set Ω⊂Rn and suppose that the
sequence of penalty parameters αk > 0 is monotonically divergent. Finally,
let x∗
k be the global minimizer of problem (7.54) at step k. Then, taking
the limit as k →∞, the sequence x∗
k converges to x∗, which is a global
minimizer of f in Ωand satisﬁes the constraint h(x∗) = 0.
Regarding the selection of the parameters αk, it can be shown that large
values of αk make the minimization problem in (7.54) ill-conditioned, thus
making its solution quite prohibitive unless the initial guess is particularly
close to x∗. On the other hand, the sequence αk must not grow too slowly,
since this would negatively aﬀect the overall convergence of the method.
A choice that is commonly made in practice is to pick up a not too large
value of α0 and then set αk = βαk−1 for k > 0, where β is an integer
number between 4 and 10 (see [Ber82]). Finally, the starting point for the
numerical method used to solve the minimization problem (7.54) can be
set equal to the last computed iterate.
The penalty method is implemented in Program 63. This requires as
input parameters the functions f, h, an initial value alpha0 for the penalty
parameter and the number beta.
Program 63 - lagrpen : Penalty method
function [x,vinc,nit]=lagrpen(x0,alpha0,beta,f,h,toll)
x = x0; [r,c]=size(h); vinc = 0;
for i=1:r,
vinc = max(vinc,eval(h(i,1:c)));
end
norm2h=[’(’,h(1,1:c),’)ˆ2’];
for i=2:r,
norm2h=[norm2h,’+(’,h(i,1:c),’)ˆ2’];
end
alpha = alpha0; options(1)=0; options(2)=toll*0.1; nit = 0;
while vinc > toll
g=[f,’+0.5*’,num2str(alpha,16),’*’,norm2h];
[x]=fmins(g,x,options);
vinc=0; nit = nit + 1;
for i=1:r, vinc = max(vinc,eval(h(i,1:c))); end
alpha=alpha*beta;
end
Example 7.6 Let us employ the penalty method to compute the minimizer of
f(x) = 100(x2 −x2
1)2 + (1 −x1)2 under the constraint h(x) = (x1 + 0.5)2 +
(x2 + 0.5)2 −0.25 = 0. The crosses in Figure 7.3 denote the sequence of iterates
computed by Program 63 starting from x(0) = (1, 1)T and choosing α0 = 0.1, β =
6. The method converges in 12 iterations to the value x = (−0.2463, −0.0691)T ,
satisfying the constraint up to a tolerance of 10−4.
•

7.3 Constrained Optimization
317
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
FIGURE 7.3. Convergence history of the penalty method in Example 7.6
7.3.3
The Method of Lagrange Multipliers
A variant of the penalty method makes use of (instead of Lα(x) in (7.53))
the augmented Lagrangian function Gα : Rm × Rn →R given by
Gα(x, λ) = f(x) + λT h(x) + 1
2α∥h(x)∥2
2,
(7.55)
where λ ∈Rm is a Lagrange multiplier. Clearly, if x∗is a solution to prob-
lem (7.50), then it will also be a solution to (7.55), but with the advantage,
with respect to (7.53), of disposing of the further degree of freedom λ. The
penalty method applied to (7.55) reads: for k = 0, 1, . . . , until convergence,
solve the sequence of problems
minimize Gαk(x, λk)
for x ∈Ω,
(7.56)
where {λk} is a bounded sequence of unknown vectors in Rn, and the
parameters αk are deﬁned as above (notice that if λk were zero, then we
would recover method (7.54)).
Property 7.16 also holds for method (7.56), provided that the multipliers
are assumed to be bounded. Notice that the existence of the minimizer
of (7.56) is not guaranteed, even in the case where f has a unique global
minimizer (see Example 7.7). This circumstance can be overcome by adding
further non quadratic terms to the augmented Lagrangian function (e.g.,
of the form ∥h∥p
2, with p large).

318
7. Nonlinear Systems and Numerical Optimization
Example 7.7 Let us ﬁnd the minimizer of f(x) = −x4 under the constraint
x = 0. Such problem clearly admits the solution x∗= 0. If, instead, one considers
the augmented Lagrangian function
Lαk(x, λk) = −x4 + λkx + 1
2αkx2,
one ﬁnds that it no longer admits a minimum at x = 0, though vanishing there,
for any αk diﬀerent from zero.
•
As far as the choice of the multipliers is concerned, the sequence of vectors
λk is typically assigned by the following formula
λk+1 = λk + αkh(x(k)),
where λ0 is a given value while the sequence of αk can be set a priori or
modiﬁed during run-time.
As for the convergence properties of the method of Lagrange multipliers,
the following local result holds.
Property 7.17 Assume that x∗is a regular strict local minimizer of (7.50)
and that:
1. f, hi ∈C2(B(x∗; R)) with i = 1, . . . , m and for a suitable R > 0;
2. the pair (x∗, λ∗) satisﬁes zT HG0(x∗, λ∗)z > 0, ∀z ̸= 0 such that
Jh(x∗)T z = 0;
3. ∃¯α > 0 such that HG ¯
α(x∗, λ∗) > 0.
Then, there exist three positive scalars δ, γ and M such that, for any pair
(λ, α) ∈V =

(λ, α) ∈Rm+1 : ∥λ −λ∗∥2 < δα, α ≥¯α

, the problem
minimize Gα(x, λ),
with x ∈B(x∗; γ),
admits a unique solution x(λ, α), diﬀerentiable with respect to its argu-
ments. Moreover, ∀(λ, α) ∈V
∥x(λ, α) −x∗∥2 ≤M∥λ −λ∗∥2.
Under further assumptions (see [Ber82], Proposition 2.7), it can be proved
that the Lagrange multipliers method converges. Moreover, if αk →∞, as
k →∞, then
lim
k→∞
∥λk+1 −λ∗∥2
∥λk −λ∗∥2
= 0.
and the convergence of the method is more than linear. In the case where
the sequence αk has an upper bound, the method converges linearly.

7.4 Applications
319
Finally, we notice that, unlike the penalty method, it is no longer nec-
essary that the sequence of αk tends to inﬁnity. This, in turn, limits the
ill-conditioning of problem (7.56) as αk is growing. Another advantage con-
cerns the convergence rate of the method, which turns out to be indepen-
dent of the growth rate of the penalty parameter, in the case of the Lagrange
multipliers technique. This of course implies a considerable reduction of the
computational cost.
The method of Lagrange multipliers is implemented in Program 64. Com-
pared with Program 63, this further requires in input the initial value
lambda0 of the multiplier.
Program 64 - lagrmult : Method of Lagrange multipliers
function [x,vinc,nit]=lagrmult(x0,lambda0,alpha0,beta,f,h,toll)
x = x0; [r,c]=size(h); vinc = 0; lambda = lambda0;
for i=1:r,
vinc = max(vinc,eval(h(i,1:c)));
end
norm2h=[’(’,h(1,1:c),’)ˆ2’];
for i=2:r,
norm2h=[norm2h,’+(’,h(i,1:c),’)ˆ2’];
end
alpha = alpha0; options(1)=0; options(2)=toll*0.1; nit = 0;
while vinc > toll
lh=[’(’,h(1,1:c),’)*’,num2str(lambda(1))];
for i=2:r, lh=[lh,’+(’,h(i,1:c),’)*’,num2str(lambda(i))];
end
g=[f,’+0.5*’,num2str(alpha,16),’*’,norm2h,’+’,lh];
[x]=fmins(g,x,options);
vinc=0; nit = nit + 1;
for i=1:r, vinc = max(vinc,eval(h(i,1:c))); end
alpha=alpha*beta;
for i=1:r, lambda(i)=lambda(i)+alpha*eval(h(i,1:c)); end
end
Example 7.8 We use the method of Lagrange multipliers to solve the prob-
lem presented in Example 7.6. Set λ = 10 and leave the remaining parameters
unchanged. The method converges in 6 iterations and the crosses in Figure 7.4
show the iterates computed by Program 64. The constraint is here satisﬁed up
to machine precision.
•
7.4
Applications
The two applications of this section are concerned with nonlinear systems
arising in the simulation of the electric potential in a semiconductor device
and in the triangulation of a two-dimensional polygon.

320
7. Nonlinear Systems and Numerical Optimization
−1
−0.5
0
0.5
1
−1
−0.5
0
0.5
1
FIGURE 7.4. Convergence history for the method of Lagrange multipliers in
Example 7.8
7.4.1
Solution of a Nonlinear System Arising from
Semiconductor Device Simulation
Let us consider the nonlinear system in the unknown u ∈Rn
F(u) = Au + φ(u) −b = 0,
(7.57)
where A = (λ/h)2tridiagn(−1, 2−1), for h = 1/(n+1), φi(u) = 2K sinh(ui)
for i = 1, . . . , n, where λ and K are two positive constants and b ∈Rn is
a given vector. Problem (7.57) arises in the numerical simulation of semi-
conductor devices in microelectronics, where u and b represent electric
potential and doping proﬁle, respectively.
In Figure 7.5 (left) we show schematically the particular device consid-
ered in the numerical example, a p −n junction diode of unit normalized
length, subject to an external bias △V = Vb −Va, together with the doping
proﬁle of the device, normalized to 1 (right). Notice that bi = b(xi), for
i = 1, . . . , n, where xi = ih. The mathematical model of the problem at
hand comprises a nonlinear Poisson equation for the electric potential and
two continuity equations of advection-diﬀusion type, as those addressed in
Chapter 12, for the current densities. For the complete derivation of the
model and its analysis see, for instance, [Mar86] and [Jer96].
Solving system (7.57) corresponds to ﬁnding the minimizer in Rn of the
function f : Rn →R deﬁned as
f(u) = 1
2uT Au + 2
n

i=1
cosh(ui)) −bT u.
(7.58)

7.4 Applications
321
+
p
n
∆V
−
0
L
x
b(x)
−1
1
FIGURE 7.5. Scheme of a semiconductor device (left); doping proﬁle (right)
It can be checked (see Exercise 5) that for any u, v ∈Rn with u ̸= v and
for any λ ∈(0, 1)
λf(u) + (1 −λ)f(v) −f(λu + (1 −λ)v) > (1/2)λ(1 −λ)∥u −v∥2
A,
where ∥· ∥A denotes the energy norm introduced in (1.28). This implies
that f(u) is an uniformly convex function in Rn, that is, it strictly satisﬁes
(7.49) with ρ = 1/2.
Property 7.10 ensures, in turn, that the function in (7.58) admits a unique
minimizer u∗∈Rn and it can be shown (see Theorem 14.4.3, p. 503 [OR70])
that there exists a sequence {αk} such that the iterates of the damped
Newton method introduced in Section 7.2.6 converge to u∗∈Rn (at least)
superlinearly.
Thus, using the damped Newton method for solving system (7.57) leads to
the following sequence of linearized problems:
given u(0) ∈Rn, ∀k ≥0 solve
6
A + 2K diagn(cosh(u(k)
i
))
7
δu(k) = b −
+
Au(k) + φ(u(k))
,
,
(7.59)
then set u(k+1) = u(k) + αkδu(k).
Let us now address two possible choices of the acceleration parameters
αk. The ﬁrst one has been proposed in [BR81] and is
αk =
1
1 + ρk ∥F(u(k))∥,
k = 0, 1, . . . ,
(7.60)
where ∥· ∥denotes a vector norm, for instance ∥· ∥= ∥· ∥∞, and the
coeﬃcients ρk ≥0 are suitable acceleration parameters picked in such a
way that the descent condition ∥F(u(k) + αkδu(k))∥∞< ∥F(u(k))∥∞is
satisﬁed (see [BR81] for the implementation details of the algorithm).

322
7. Nonlinear Systems and Numerical Optimization
We notice that, as ∥F(u(k))∥∞→0, (7.60) yields αk →1, thus recov-
ering the full (quadratic) convergence of Newton’s method. Otherwise, as
typically happens in the ﬁrst iterations, ∥F(u(k))∥∞≫1 and αk is quite
close to zero, with a strong reduction of the Newton variation (damping).
As an alternative to (7.60), the sequence {αk} can be generated using the
simpler formula, suggested in [Sel84], Chapter 7
αk = 2−i(i−1)/2,
k = 0, 1, . . . ,
(7.61)
where i is the ﬁrst integer in the interval [1, Itmax] such that the descent
condition above is satisﬁed, Itmax being the maximum admissible number
of damping cycles for any Newton’s iteration (ﬁxed equal to 10 in the
numerical experiments).
As a comparison, both damped and standard Newton’s methods have been
implemented, the former one with both choices (7.60) and (7.61) for the
coeﬃcients αk. In the case of Newton’s method, we have set in (7.59) αk = 1
for any k ≥0.
The numerical examples have been performed with n = 49, bi = −1 for
i ≤n/2 and the remaining values bi equal to 1. Moreover, we have taken
λ2 = 1.67 · 10−4, K = 6.77 · 10−6 and ﬁxed the ﬁrst n/2 components of the
initial vector u(0) equal to Va and the remaining ones equal to Vb, where
Va = 0 and Vb = 10.
The tolerance on the maximum change between two successive iterates,
which monitors the convergence of damped Newton’s method (7.59), has
been set equal to 10−4.
10
0
10
1
10
2
10
−6
10
−4
10
−2
10
0
10
2
10
4
(1)
(2)
(3)
0
2
4
6
8
10
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
FIGURE 7.6. Absolute error (left) and damping parameters αk (right). The error
curve for standard Newton’s method is denoted by (1), while (2) and (3) refer to
damped Newton’s method with the choices (7.61) and (7.60) for the coeﬃcients
αk, respectively
Figure 7.6 (left) shows the log-scale absolute error for the three algorithms
as functions of the iteration number. Notice the rapid convergence of the

7.4 Applications
323
damped Newton’s method (8 and 10 iterations in the case of (7.60) and
(7.61), respectively), compared with the extremely slow convergence of the
standard Newton’s method (192 iterations). Moreover, it is interesting to
analyze in Figure 7.6 (right) the plot of the sequences of parameters αk as
functions of the iteration number.
The starred and the circled curves refer to the choices (7.60) and (7.61)
for the coeﬃcients αk, respectively. As previously observed, the αk’s start
from very small values, to converge quickly to 1 as the damped Newton
method (7.59) enters the attraction region of the minimizer x∗.
7.4.2
Nonlinear Regularization of a Discretization Grid
In this section we go back to the problem of regularizing a discretization
grid that has been introduced in Section 3.14.2. There, we considered the
technique of barycentric regularization, which leads to solving a linear sys-
tem, typically of large size and featuring a sparse coeﬃcient matrix.
In this section we address two alternative techniques, denoted as reg-
ularization by edges and by areas. The main diﬀerence with respect to
the method described in Section 3.14.2 lies in the fact that these new ap-
proaches lead to systems of nonlinear equations.
Using the notation of Section 3.14.2, for each pair of nodes xj, xk ∈Zi,
denote by ljk the edge on the boundary ∂Pi of Pi which connects them
and by xjk the midpoint of ljk, while for each triangle T ∈Pi we denote
by xb,T the centroid of T. Moreover, let ni = dim(Zi) and denote for any
geometric entity (side or triangle) by | · | its measure in R1 or R2.
In the case of regularization by edges, we let
xi =


ljk∈∂Pi
xjk|ljk|

/|∂Pi|,
∀xi ∈Nh,
(7.62)
while in the case of regularization by areas, we let
xi =
 
T ∈Pi
xb,T |T|

/|Pi|,
∀xi ∈Nh.
(7.63)
In both the regularization procedures we assume that xi = x(∂D)
i
if xi ∈
∂D, that is, the nodes lying on the boundary of the domain D are ﬁxed. Let-
ting n = N−Nb be the number of internal nodes, relation (7.62) amounts to
solving the following two systems of nonlinear equations for the coordinates

324
7. Nonlinear Systems and Numerical Optimization
{xi} and {yi} of the internal nodes, with i = 1, . . . , n
xi −1
2


ljk∈∂Pi
(xj + xk)|ljk|

/

ljk∈∂Pi
|ljk| = 0,
yi −1
2


ljk∈∂Pi
(yj + yk)|ljk|

/

ljk∈∂Pi
|ljk| = 0.
(7.64)
Similarly, (7.63) leads to the following nonlinear systems, for i = 1, . . . , n
xi −1
3
 
T ∈Pi
(x1,T + x2,T + x3,T )|T|

/

T ∈Pi
|T| = 0,
yi −1
3
 
T ∈Pi
(y1,T + y2,T + y3,T )|T|

/

T ∈Pi
|T| = 0,
(7.65)
where xs,T = (xs,T , ys,T ), for s = 1, 2, 3, are the coordinates of the vertices
of each triangle T ∈Pi. Notice that the nonlinearity of systems (7.64) and
(7.65) is due to the presence of terms |ljk| and |T|.
Both systems (7.64) and (7.65) can be cast in the form (7.1), denoting,
as usual, by fi the i-th nonlinear equation of the system, for i = 1, . . . , n.
The complex functional dependence of fi on the unknowns makes it pro-
hibitive to use Newton’s method (7.4), which would require the explicit
computation of the Jacobian matrix JF.
A convenient alternative is provided by the nonlinear Gauss-Seidel method
(see [OR70], Chapter 7), which generalizes the corresponding method pro-
posed in Chapter 4 for linear systems and can be formulated as follows.
Denote by zi, for i = 1, . . . , n, either of the unknown xi or yi. Given the
initial vector z(0) = (z(0)
1 , . . . , z(0)
n )T , for k = 0, 1, . . . until convergence,
solve
fi(z(k+1)
1
, . . . , z(k+1)
i−1
, ξ, z(k)
i+1, . . . , z(k)
n ) = 0,
i = 1, . . . , n,
(7.66)
then, set z(k+1)
i
= ξ. Thus, the nonlinear Gauss-Seidel method converts
problem (7.1) into the successive solution of n scalar nonlinear equations.
In the case of system (7.64), each of these equations is linear in the unknown
z(k+1)
i
(since ξ does not explicitly appear in the bracketed term at the right
side of (7.64)). This allows for its exact solution in one step.
In the case of system (7.65), the equation (7.66) is genuinely nonlinear
with respect to ξ, and is solved taking one step of a ﬁxed-point iteration.
The nonlinear Gauss-Seidel (7.66) has been implemented in MATLAB
to solve systems (7.64) and (7.65) in the case of the initial triangulation
shown in Figure 7.7 (left). Such a triangulation covers the external region
of a two dimensional wing section of type NACA 2316. The grid contains
NT = 534 triangles and n = 198 internal nodes.

7.5 Exercises
325
The algorithm reached convergence in 42 iterations for both kinds of reg-
ularization, having used as stopping criterion the test ∥z(k+1) −z(k)∥∞≤
10−4. In Figure 7.7 (right) the discretization grid obtained after the reg-
ularization by areas is shown (a similar result has been provided by the
regularization by edges). Notice the higher uniformity of the triangles with
respect to those of the starting grid.
FIGURE 7.7. Triangulation before (left) and after (right) the regularization
7.5
Exercises
1. Prove (7.8) for the m-step Newton-SOR method.
[Hint: use the SOR method for solving a linear system Ax=b with A=D-
E-F and express the k-th iterate as a function of the initial datum x(0),
obtaining
x(k+1) = x(0) + (Mk+1 −I)x(0) + (Mk + . . . + I)B−1b,
where B= ω−1(D −ωE) and M = B−1ω−1 [(1 −ω)D + ωF]. Since B−1A =
I −M and
(I + . . . + Mk)(I −M) = I −Mk+1
then (7.8) follows by suitably identifying the matrix and the right-side of
the system.]
2. Prove that using the gradient method for minimizing f(x) = x2 with the
directions p(k) = −1 and the parameters αk = 2−k+1, does not yield the
minimizer of f.
3. Show that for the steepest descent method applied to minimizing a quadratic
functional f of the form (7.35) the following inequality holds
f(x(k+1)) ≤
 λmax −λmin
λmax + λmin
2
f(x(k)),

326
7. Nonlinear Systems and Numerical Optimization
where λmax, λmin are the eigenvalues of maximum and minimum module,
respectively, of the matrix A that appears in (7.35).
[Hint: proceed as done for (7.38).]
4. Check that the parameters αk of Exercise 2 do not fulﬁll the conditions
(7.31) and (7.32).
5. Consider the function f : Rn →R introduced in (7.58) and check that it is
uniformly convex on Rn, that is
λf(u) + (1 −λ)f(v) −f(λu + (1 −λ)v) > (1/2)λ(1 −λ)∥u −v∥2
A
for any u, v ∈Rn with u ̸= v and 0 < λ < 1.
[Hint: notice that cosh(·) is a convex function.]
6. To solve the nonlinear system









−1
81 cos x1 + 1
9x2
2 + 1
3 sin x3 = x1
1
3 sin x1 + 1
3 cos x3 = x2
−1
9 cos x1 + 1
3x2 + 1
6 sin x3 = x3,
use the ﬁxed-point iteration x(n+1) = Ψ(x(n)), where x = (x1, x2, x3)T and
Ψ(x) is the left-hand side of the system. Analyze the convergence of the
iteration to compute the ﬁxed point α = (0, 1/3, 0)T .
[Solution: the ﬁxed-point method is convergent since ∥Ψ(α)∥∞= 1/2.]
7. Using Program 50 implementing Newton’s method, determine the global
maximizer of the function
f(x) = e−x2
2 −1
4 cos(2x)
and analyze the performance of the method (input data: xv=1; toll=1e-6;
nmax=500). Solve the same problem using the following ﬁxed-point iteration
x(k+1) = g(xk)
with
g(x) = sin(2x)

e
x2
2 (x sin(2x) + 2 cos(2x)) −2
2 (x sin(2x) + 2 cos(2x))

.
Analyze the performance of this second scheme, both theoretically and
experimentally, and compare the results obtained using the two methods.
[Solution: the function f has a global maximum at x = 0. This point is
a double zero for f ′. Thus, Newton’s method is only linearly convergent.
Conversely, the proposed ﬁxed-point method is third-order convergent.]

8
Polynomial Interpolation
This chapter is addressed to the approximation of a function which is known
through its nodal values.
Precisely, given m+1 pairs (xi, yi), the problem consists of ﬁnding a func-
tion Φ = Φ(x) such that Φ(xi) = yi for i = 0, . . . , m, yi being some given
values, and say that Φ interpolates {yi} at the nodes {xi}. We speak about
polynomial interpolation if Φ is an algebraic polynomial, trigonometric ap-
proximation if Φ is a trigonometric polynomial or piecewise polynomial
interpolation (or spline interpolation) if Φ is only locally a polynomial.
The numbers yi may represent the values attained at the nodes xi by a
function f that is known in closed form, as well as experimental data. In the
former case, the approximation process aims at replacing f with a simpler
function to deal with, in particular in view of its numerical integration
or derivation. In the latter case, the primary goal of approximation is to
provide a compact representation of the available data, whose number is
often quite large.
Polynomial interpolation is addressed in Sections 8.1 and 8.2, while piece-
wise polynomial interpolation is introduced in Sections 8.3, 8.4 and 8.5. Fi-
nally, univariate and parametric splines are addressed in Sections 8.6 and
8.7. Interpolation processes based on trigonometric or algebraic orthogonal
polynomials will be considered in Chapter 10.

328
8. Polynomial Interpolation
8.1
Polynomial Interpolation
Let us consider n + 1 pairs (xi, yi). The problem is to ﬁnd a polynomial
Πm ∈Pm, called an interpolating polynomial, such that
Πm(xi) = amxm
i + . . . + a1xi + a0 = yi
i = 0, . . . , n.
(8.1)
The points xi are called interpolation nodes. If n ̸= m the problem is over
or under-determined and will be addressed in Section 10.7.1. If n = m, the
following result holds.
Theorem 8.1 Given n+1 distinct points x0, . . . , xn and n+1 correspond-
ing values y0, . . . , yn, there exists a unique polynomial Πn ∈Pn such that
Πn(xi) = yi for i = 0, . . . , n.
Proof. To prove existence, let us use a constructive approach, providing an
expression for Πn. Denoting by {li}n
i=0 a basis for Pn, then Πn admits a repre-
sentation on such a basis of the form Πn(x) = n
i=0 bili(x) with the property
that
Πn(xi) =
n

j=0
bjlj(xi) = yi,
i = 0, . . . , n.
(8.2)
If we deﬁne
li ∈Pn :
li(x) =
n

j=0
j̸=i
x −xj
xi −xj
i = 0, . . . , n,
(8.3)
then li(xj) = δij and we immediately get from (8.2) that bi = yi.
The polynomials {li, i = 0, . . . , n} form a basis for Pn (see Exercise 1). As a con-
sequence, the interpolating polynomial exists and has the following form (called
Lagrange form)
Πn(x) =
n

i=0
yili(x).
(8.4)
To prove uniqueness, suppose that another interpolating polynomial Ψm of de-
gree m ≤n exists, such that Ψm(xi) = yi for i = 0, ..., n. Then, the diﬀerence
polynomial Πn −Ψm vanishes at n + 1 distinct points xi and thus coincides with
the null polynomial. Therefore, Ψm = Πn.
An alternative approach to prove existence and uniqueness of Πn is provided
in Exercise 2.
3
It can be checked that (see Exercise 3)
Πn(x) =
n

i=0
ωn+1(x)
(x −xi)ω′
n+1(xi)yi
(8.5)

8.1 Polynomial Interpolation
329
where ωn+1 is the nodal polynomial of degree n + 1 deﬁned as
ωn+1(x) =
n

i=0
(x −xi).
(8.6)
Formula (8.4) is called the Lagrange form of the interpolating polynomial,
while the polynomials li(x) are the characteristic polynomials. In Figure
8.1 we show the characteristic polynomials l2(x), l3(x) and l4(x), in the
case of degree n = 6, on the interval [-1,1] where equally spaced nodes are
taken, including the end points.
−1
−0.5
0
0.5
1
−1.5
−1
−0.5
0
0.5
1
1.5
l
l
l
2
3
4
FIGURE 8.1. Lagrange characteristic polynomials
Notice that |li(x)| can be greater than 1 within the interpolation interval.
If yi = f(xi) for i = 0, . . . , n, f being a given function, the interpolating
polynomial Πn(x) will be denoted by Πnf(x).
8.1.1
The Interpolation Error
In this section we estimate the interpolation error that is made when re-
placing a given function f with its interpolating polynomial Πnf at the
nodes x0, x1, . . . , xn (for further results, we refer the reader to [Wen66],
[Dav63]).
Theorem 8.2 Let x0, x1, . . . , xn be n+1 distinct nodes and let x be a point
belonging to the domain of a given function f. Assume that f ∈Cn+1(Ix),
where Ix is the smallest interval containing the nodes x0, x1, . . . , xn and x.
Then the interpolation error at the point x is given by
En(x) = f(x) −Πnf(x) = f (n+1)(ξ)
(n + 1)! ωn+1(x),
(8.7)
where ξ ∈Ix and ωn+1 is the nodal polynomial of degree n + 1.

330
8. Polynomial Interpolation
Proof. The result is obviously true if x coincides with any of the interpola-
tion nodes. Otherwise, deﬁne, for any t ∈Ix, the function G(t) = En(t) −
ωn+1(t)En(x)/ωn+1(x). Since f ∈C(n+1)(Ix) and ωn+1 is a polynomial, then
G ∈C(n+1)(Ix) and it has n + 2 distinct zeros in Ix, since
G(xi) = En(xi) −ωn+1(xi)En(x)/ωn+1(x) = 0,
i = 0, . . . , n
G(x) = En(x) −ωn+1(x)En(x)/ωn+1(x) = 0.
Then, thanks to the mean value theorem, G′ has n + 1 distinct zeros and, by
recursion, G(j) admits n + 2 −j distinct zeros. As a consequence, G(n+1) has a
unique zero, which we denote by ξ. On the other hand, since E(n+1)
n
(t) = f (n+1)(t)
and ω(n+1)
n+1 (x) = (n + 1)! we get
G(n+1)(t) = f (n+1)(t) −(n + 1)!
ωn+1(x)En(x),
which, evaluated at t = ξ, gives the desired expression for En(x).
3
8.1.2
Drawbacks of Polynomial Interpolation on Equally
Spaced Nodes and Runge’s Counterexample
In this section we analyze the behavior of the interpolation error (8.7) as
n tends to inﬁnity. For this purpose, for any function f ∈C0([a, b]), deﬁne
its maximum norm
∥f∥∞= max
x∈[a,b]|f(x)|.
(8.8)
Then, let us introduce a lower triangular matrix X of inﬁnite size, called the
interpolation matrix on [a, b], whose entries xij, for i, j = 0, 1, . . . , represent
points of [a, b], with the assumption that on each row the entries are all
distinct.
Thus, for any n ≥0, the n + 1-th row of X contains n + 1 distinct
values that we can identify as nodes, so that, for a given function f, we
can uniquely deﬁne an interpolating polynomial Πnf of degree n at those
nodes (any polynomial Πnf depends on X, as well as on f).
Having ﬁxed f and an interpolation matrix X, let us deﬁne the interpo-
lation error
En,∞(X) = ∥f −Πnf∥∞,
n = 0, 1, . . .
(8.9)
Next, denote by p∗
n ∈Pn the best approximation polynomial, for which
E∗
n = ∥f −p∗
n∥∞≤∥f −qn∥∞
∀qn ∈Pn.
The following comparison result holds (for the proof, see [Riv74]).

8.1 Polynomial Interpolation
331
Property 8.1 Let f ∈C0([a, b]) and X be an interpolation matrix on [a, b].
Then
En,∞(X) ≤E∗
n (1 + Λn(X)) ,
n = 0, 1, . . .
(8.10)
where Λn(X) denotes the Lebesgue constant of X, deﬁned as
Λn(X) =
!!!!!!
n

j=0
|l(n)
j
|
!!!!!!
∞
,
(8.11)
and where l(n)
j
∈Pn is the j-th characteristic polynomial associated with
the n + 1-th row of X, that is, satisfying l(n)
j
(xnk) = δjk, j, k = 0, 1, . . .
Since E∗
n does not depend on X, all the information concerning the eﬀects
of X on En,∞(X) must be looked for in Λn(X). Although there exists an
interpolation matrix X∗such that Λn(X) is minimized, it is not in general a
simple task to determine its entries explicitly. We shall see in Section 10.3,
that the zeros of the Chebyshev polynomials provide on the interval [−1, 1]
an interpolation matrix with a very small value of the Lebesgue constant.
On the other hand, for any possible choice of X, there exists a constant
C > 0 such that (see [Erd61])
Λn(X) > 2
π log(n + 1) −C,
n = 0, 1, . . .
This property shows that Λn(X) →∞as n →∞. This fact has important
consequences: in particular, it can be proved (see [Fab14]) that, given an
interpolation matrix X on an interval [a, b], there always exists a continuous
function f in [a, b], such that Πnf does not converge uniformly (that is, in
the maximum norm) to f. Thus, polynomial interpolation does not allow for
approximating any continuous function, as demonstrated by the following
example.
Example 8.1 (Runge’s counterexample) Suppose we approximate the fol-
lowing function
f(x) =
1
1 + x2 ,
−5 ≤x ≤5
(8.12)
using Lagrange interpolation on equally spaced nodes. It can be checked that
some points x exist within the interpolation interval such that
lim
n→∞|f(x) −Πnf(x)| ̸= 0.
In particular, Lagrange interpolation diverges for |x| > 3.63 . . . . This phenomenon
is particularly evident in the neighborhood of the end points of the interpolation
interval, as shown in Figure 8.2, and is due to the choice of equally spaced nodes.
We shall see in Chapter 10 that resorting to suitably chosen nodes will allow for
uniform convergence of the interpolating polynomial to the function f to hold. •

332
8. Polynomial Interpolation
−5
−4
−3
−2
−1
0
1
2
3
4
5
−0.5
0
0.5
1
1.5
2
FIGURE 8.2. Lagrange interpolation on equally spaced nodes for the function
f(x) = 1/(1 + x2): the interpolating polynomials Π5f and Π10f are shown in
dotted and dashed line, respectively
8.1.3
Stability of Polynomial Interpolation
Let us consider a set of function values
2
$f(xi)
3
which is a perturbation
of the data f(xi) relative to the nodes xi, with i = 0, . . . , n, in an interval
[a, b]. The perturbation may be due, for instance, to the eﬀect of rounding
errors, or may be caused by an error in the experimental measure of the
data.
Denoting by Πn $f the interpolating polynomial on the set of values $f(xi),
we have
∥Πnf −Πn $f∥∞
= max
a≤x≤b

n

j=0
(f(xj) −$f(xj))lj(x)

≤Λn(X) max
i=0,...,n|f(xi) −$f(xi)|.
As a consequence, small changes on the data give rise to small changes
on the interpolating polynomial only if the Lebesgue constant is small.
This constant plays the role of the condition number for the interpolation
problem.
As previously noticed, Λn grows as n →∞and in particular, in the case
of Lagrange interpolation on equally spaced nodes, it can be proved that
(see [Nat65])
Λn(X) ≃
2n+1
en log n
where e ≃2.7183 is the naeperian number. This shows that, for n large,
this form of interpolation can become unstable. Notice also that so far we
have completely neglected the errors generated by the interpolation process
in constructing Πnf. However, it can be shown that the eﬀect of such errors
is generally negligible (see [Atk89]).

8.2 Newton Form of the Interpolating Polynomial
333
−1
−0.5
0
0.5
1
−1.5
−1
−0.5
0
0.5
1
1.5
2
2.5
FIGURE 8.3. Instability of Lagrange interpolation. In solid line Π21f, on unper-
turbed data, in dashed line Π21 $f, on perturbed data, for Example 8.2
Example 8.2 On the interval [−1, 1] let us interpolate the function f(x) =
sin(2πx) at 22 equally spaced nodes xi. Next, we generate a perturbed set of val-
ues $f(xi) of the function evaluations f(xi) = sin(2πxi) with maxi=0,...,21 |f(xi)−
$f(xi)| ≃9.5 · 10−4. In Figure 8.3 we compare the polynomials Π21f and Π21 $f:
notice how the diﬀerence between the two interpolating polynomials, around the
end points of the interpolation interval, is quite larger than the impressed per-
turbation (actually, ∥Π21f −Π21 $f∥∞≃2.1635 and Λ21 ≃24000).
•
8.2
Newton Form of the Interpolating Polynomial
The Lagrange form (8.4) of the interpolating polynomial is not the most
convenient from a practical standpoint. In this section we introduce an
alternative form characterized by a cheaper computational cost. Our goal
is the following:
given n + 1 pairs {xi, yi}, i = 0, . . . , n, we want to represent Πn (with
Πn(xi) = yi for i = 0, . . . , n) as the sum of Πn−1 (with Πn−1(xi) = yi for
i = 0, . . . , n−1) and a polynomial of degree n which depends on the nodes
xi and on only one unknown coeﬃcient. We thus set
Πn(x) = Πn−1(x) + qn(x),
(8.13)
where qn ∈Pn. Since qn(xi) = Πn(xi) −Πn−1(xi) = 0 for i = 0, . . . , n −1,
it must necessarily be that
qn(x) = an(x −x0) . . . (x −xn−1) = anωn(x).

334
8. Polynomial Interpolation
To determine the unknown coeﬃcient an, suppose that yi = f(xi), i =
0, . . . , n, where f is a suitable function, not necessarily known in explicit
form. Since Πnf(xn) = f(xn), from (8.13) it follows that
an = f(xn) −Πn−1f(xn)
ωn(xn)
.
(8.14)
The coeﬃcient an is called n-th the Newton divided diﬀerence and is gen-
erally denoted by
an = f[x0, x1, . . . , xn]
(8.15)
for n ≥1. As a consequence, (8.13) becomes
Πnf(x) = Πn−1f(x) + ωn(x)f[x0, x1, . . . , xn].
(8.16)
If we let y0 = f(x0) = f[x0] and ω0 = 1, by recursion on n we can obtain
from (8.16) the following formula
Πnf(x) =
n

k=0
ωk(x)f[x0, . . . , xk].
(8.17)
Uniqueness of the interpolating polynomial ensures that the above expres-
sion yields the same interpolating polynomial generated by the Lagrange
form. Form (8.17) is commonly known as the Newton divided diﬀerence
formula for the interpolating polynomial.
Program 65 provides an implementation of Newton’s formula. The input
vectors x and y contain the interpolation nodes and the corresponding func-
tional evaluations of f, respectively, while vector z contains the abscissae
where the polynomial Πnf is to be evaluated. This polynomial is stored in
the output vector f.
Program 65 - interpol : Lagrange polynomial using Newton’s formula
function [f] = interpol (x,y,z)
[m n] = size(y);
for j = 1:m
a (:,1) = y (j,:)’;
for i = 2:n
a (i:n,i) = ( a(i:n,i-1)-a(i-1,i-1) )./(x(i:n)-x(i-1))’;
end
f(j,:) = a(n,n).*(z-x(n-1)) + a(n-1,n-1);
for i = 2:n-1
f(j,:) = f(j,:).*(z-x(n-i))+a(n-i,n-i);
end
end

8.2 Newton Form of the Interpolating Polynomial
335
8.2.1
Some Properties of Newton Divided Diﬀerences
The n-th divided diﬀerence f[x0, . . . , xn] = an can be further characterized
by noticing that it is the coeﬃcient of xn in Πnf. Isolating such a coeﬃcient
from (8.5) and equating it with the corresponding coeﬃcient in the Newton
formula (8.17), we end up with the following explicit representation
f[x0, . . . , xn] =
n

i=0
f(xi)
ω′
n+1(xi).
(8.18)
This formula has remarkable consequences:
1. the value attained by the divided diﬀerence is invariant with respect
to permutations of the indexes of the nodes. This instance can be
proﬁtably employed when stability problems suggest exchanging the
indexes (for example, if x is the point where the polynomial must be
computed, it is convenient to introduce a permutation of the indexes
such that |x −xk| ≤|x −xk−1| with k = 1, . . . , n);
2. if f = αg + βh for some α, β ∈R, then
f[x0, . . . , xn] = αg[x0, . . . , xn] + βh[x0, . . . , xn];
3. if f = gh, the following formula (called the Leibniz formula) holds
(see [Die93])
f[x0, . . . , xn] =
n

j=0
g[x0, . . . , xj]h[xj, . . . , xn];
4. an algebraic manipulation of (8.18) (see Exercise 7) yields the follow-
ing recursive formula for computing divided diﬀerences
f[x0, . . . , xn] = f[x1, . . . , xn] −f[x0, . . . , xn−1]
xn −x0
,
n ≥1.
(8.19)
Program 66 implements the recursive formula (8.19). The evaluations of f
at the interpolation nodes x are stored in vector y, while the output matrix
d (lower triangular) contains the divided diﬀerences, which are stored in
the following form
x0
f[x0]
x1
f[x1]
f[x0, x1]
x2
f[x2]
f[x1, x2]
f[x0, x1, x2]
...
...
...
...
xn
f[xn]
f[xn−1, xn]
f[xn−2, xn−1, xn]
. . .
f[x0, . . . , xn]

336
8. Polynomial Interpolation
The coeﬃcients involved in the Newton formula are the diagonal entries of
the matrix.
Program 66 - dividif : Newton divided diﬀerences
function [d]=dividif(x,y)
[n,m]=size(y);
if n == 1, n = m; end
n = n-1;
d = zeros (n+1,n+1);
d (:,1) = y’;
for j = 2:n+1
for i = j:n+1
d (i,j) = ( d (i-1,j-1)-d (i,j-1))/(x (i-j+1)-x (i));
end
end
Using (8.19), n(n + 1) sums and n(n + 1)/2 divisions are needed to gen-
erate the whole matrix. If a new evaluation of f were available at a new
node xn+1, only the calculation of a new row of the matrix would be re-
quired (f[xn, xn+1], . . . , f[x0, x1, . . . , xn+1]). Thus, in order to construct
Πn+1f from Πnf, it suﬃces to add to Πnf the term an+1ωn+1(x), with a
computational cost of (n + 1) divisions and 2(n + 1) sums. For the sake of
notational simplicity, we write below Drfi = f[xi, xi+1, . . . , xr].
Example 8.3 In Table 8.1 we show the divided diﬀerences on the interval (0,2)
for the function f(x) = 1+sin(3x). The values of f and the corresponding divided
diﬀerences have been computed using 16 signiﬁcant ﬁgures, although only the ﬁrst
5 ﬁgures are reported. If the value of f were available at node x = 0.2, updating
the divided diﬀerence table would require only to computing the entries denoted
by italics in Table 8.1.
•
xi
f(xi)
f [xi, xi−1]
D2fi
D3fi
D4fi
D5fi
D6fi
0
1.0000
0.2
1.5646
2.82
0.4
1.9320
1.83
-2.46
0.8
1.6755
-0.64
-4.13
-2.08
1.2
0.5575
-2.79
-2.69
1.43
2.93
1.6
0.0038
-1.38
1.76
3.71
1.62
-0.81
2.0
0.7206
1.79
3.97
1.83
-1.17
-1.55
-0.36
TABLE 8.1. Divided diﬀerences for the function f(x) = 1 + sin(3x) in the case
in which the evaluation of f at x = 0.2 is also available. The newly computed
values are denoted by italics

8.2 Newton Form of the Interpolating Polynomial
337
Notice that f[x0, . . . , xn] = 0 for any f ∈Pn−1. This property, how-
ever, is not always veriﬁed numerically, since the computation of divided
diﬀerences might be highly aﬀected by rounding errors.
Example 8.4 Consider again the divided diﬀerences for the function f(x) =
1 + sin(3x) on the interval (0, 0.0002). The function behaves like 1 + 3x in a
suﬃciently small neighbourhood of 0, so that we expect to ﬁnd smaller numbers as
the order of divided diﬀerences increases. However, the results obtained running
Program 66, and shown in Table 8.2 in exponential notation up to the ﬁrst 4
signiﬁcant ﬁgures (although 16 digits have been employed in the calculations),
exhibit a substantially diﬀerent pattern. The small rounding errors introduced in
the computation of divided diﬀerences of low order have dramatically propagated
on the higher order divided diﬀerences.
•
xi
f(xi)
f[xi, xi−1]
D2fi
D3fi
D4fi
D5fi
0
1.0000
4.0e-5
1.0001
3.000
8.0e-5
1.0002
3.000
-5.39e-4
1.2e-4
1.0004
3.000
-1.08e-3
-4.50
1.6e-4
1.0005
3.000
-1.62e-3
-4.49
1.80e+1
2.0e-4
1.0006
3.000
-2.15e-3
-4.49
-7.23
−1.2e + 5
TABLE 8.2. Divided diﬀerences for the function f(x) = 1+sin(3x) on the interval
(0,0.0002). Notice the completely wrong value in the last column (it should be
approximately equal to 0), due to the propagation of rounding errors throughout
the algorithm
8.2.2
The Interpolation Error Using Divided Diﬀerences
Consider the nodes x0, . . . , xn and let Πnf be the interpolating polynomial
of f on such nodes. Now let x be a node distinct from the previous ones;
letting xn+1 = x, we denote by Πn+1f the interpolating polynomial of f
at the nodes xk, k = 0, . . . , n + 1. Using the Newton divided diﬀerences
formula, we get
Πn+1f(t) = Πnf(t) + (t −x0) . . . (t −xn)f[x0, . . . , xn, t].
Since Πn+1f(x) = f(x), we obtain the following formula for the interpola-
tion error at t = x
En(x)
=
f(x) −Πnf(x) = Πn+1f(x) −Πnf(x)
=
(x −x0) . . . (x −xn)f[x0, . . . , xn, x]
=
ωn+1(x)f[x0, . . . , xn, x].
(8.20)

338
8. Polynomial Interpolation
Assuming f ∈C(n+1)(Ix) and comparing (8.20) with (8.7), yields
f[x0, . . . , xn, x] = f (n+1)(ξ)
(n + 1)!
(8.21)
for a suitable ξ ∈Ix. Since (8.21) resembles the remainder of the Tay-
lor series expansion of f, the Newton formula (8.17) for the interpolating
polynomial is often regarded as being a truncated expansion around x0
provided that |xn −x0| is not too big.
8.3
Piecewise Lagrange Interpolation
In Section 8.1.1 we have outlined the fact that, for equally spaced inter-
polating nodes, uniform convergence of Πnf to f is not guaranteed as
n →∞. On the other hand, using equally spaced nodes is clearly computa-
tionally convenient and, moreover, Lagrange interpolation of low degree is
suﬃciently accurate, provided suﬃciently small interpolation intervals are
considered.
Therefore, it is natural to introduce a partition Th of [a, b] into K subin-
tervals Ij = [xj, xj+1] of length hj, with h = max0≤j≤K−1 hj, such that
[a, b] = ∪K−1
j=0
Ij and then to employ Lagrange interpolation on each Ij
using n + 1 equally spaced nodes
2
x(i)
j , 0 ≤i ≤n
3
with a small n.
For k ≥1, we introduce on Th the piecewise polynomial space
Xk
h =

v ∈C0([a, b]) : v|Ij ∈Pk(Ij) ∀Ij ∈Th

(8.22)
which is the space of the continuous functions over [a, b] whose restric-
tions on each Ij are polynomials of degree ≤k. Then, for any continuous
function f in [a, b], the piecewise interpolation polynomial Πk
hf coincides
on each Ij with the interpolating polynomial of f|Ij at the n + 1 nodes
2
x(i)
j , 0 ≤i ≤n
3
. As a consequence, if f ∈Ck+1([a, b]), using (8.7) within
each interval we obtain the following error estimate
∥f −Πk
hf∥∞≤Chk+1 ∥f (k+1)∥∞.
(8.23)
Note that a small interpolation error can be obtained even for low k pro-
vided that h is suﬃciently “small”.
Example 8.5 Let us go back to the function of Runge’s counterexample. Now,
piecewise polynomials of degree k = 1 and k = 2 are employed. We check ex-
perimentally for the behavior of the error as h decreases. In Table 8.3 we show
the absolute errors measured in the maximum norm over the interval [−5, 5] and
the corresponding estimates of the convergence order p with respect to h. Except
when using an excessively small number of subintervals, the results conﬁrm the
theoretical estimate (8.23), that is p = k + 1.
•

8.3 Piecewise Lagrange Interpolation
339
h
∥f −Πh
1∥∞
p
∥f −Πh
2∥∞
p
5
0.4153
0.0835
2.5
0.1787
1.216
0.0971
-0.217
1.25
0.0631
1.501
0.0477
1.024
0.625
0.0535
0.237
0.0082
2.537
0.3125
0.0206
1.374
0.0010
3.038
0.15625
0.0058
1.819
1.3828e-04
2.856
0.078125
0.0015
1.954
1.7715e-05
2.964
TABLE 8.3. Interpolation error for Lagrange piecewise interpolation of degree
k = 1 and k = 2, in the case of Runge’s function (8.12); p denotes the trend of
the exponent of h. Notice that, as h →0, p →k + 1, as predicted by (8.23)
Besides estimate (8.23), convergence results in integral norms exist (see
[QV94], [EEHJ96]). For this purpose, we introduce the following space
L2(a, b) =


f : (a, b) →R,
b
>
a
|f(x)|2dx < +∞


,
(8.24)
with
∥f∥L2(a,b) =


b
>
a
|f(x)|2dx


1/2
.
(8.25)
Formula (8.25) deﬁnes a norm for L2(a, b). (We recall that norms and semi-
norms of functions can be deﬁned in a manner similar to what was done in
Deﬁnition 1.17 in the case of vectors). We warn the reader that the integral
of the function |f|2 in (8.24) has to be intended in the Lebesgue sense (see,
e.g., [Rud83]). In particular, f needs not be continuous everywhere.
Theorem 8.3 Let 0 ≤m ≤k + 1, with k ≥1 and assume that f (m) ∈
L2(a, b) for 0 ≤m ≤k + 1; then there exists a positive constant C, inde-
pendent of h, such that
∥(f −Πk
hf)(m)∥L2(a,b) ≤Chk+1−m∥f (k+1)∥L2(a,b).
(8.26)
In particular, for k = 1, and m = 0 or m = 1, we obtain
∥f −Π1
hf∥L2(a,b) ≤C1h2∥f ′′∥L2(a,b),
∥(f −Π1
hf)′∥L2(a,b) ≤C2h∥f ′′∥L2(a,b),
(8.27)
for two suitable positive constants C1 and C2.
Proof. We only prove (8.27) and refer to [QV94], Chapter 3 for the proof of
(8.26) in the general case.

340
8. Polynomial Interpolation
Deﬁne e = f −Π1
hf. Since e(xj) = 0 for all j = 0, . . . , K, Rolle’s theorem
infers the existence of ξj ∈(xj, xj+1), for j = 0, . . . , K −1 such that e′(ξj) = 0.
Since Π1
hf is a linear function on each Ij, for x ∈Ij we obtain
e′(x) =
> x
ξj
e′′(s)ds =
> x
ξj
f ′′(s)ds,
whence
|e′(x)| ≤
> xj+1
xj
|f ′′(s)|ds,
for x ∈[xj, xj+1].
(8.28)
We recall the Cauchy-Schwarz inequality

> β
α
u(x)v(x)dx
 ≤
> β
α
u2(x)dx
1/2 > β
α
v2(x)dx
1/2
(8.29)
which holds if u, v ∈L2(α, β). If we apply this inequality to (8.28) we obtain
|e′(x)|
≤



xj+1
>
xj
12dx



1/2 


xj+1
>
xj
|f ′′(s)|2ds



1/2
≤h1/2



xj+1
>
xj
|f ′′(s)|2ds



1/2
.
(8.30)
To ﬁnd a bound for |e(x)|, we notice that
e(x) =
> x
xj
e′(s)ds,
so that, applying (8.30), we get
|e(x)| ≤
> xj+1
xj
|e′(s)|ds ≤h3/2
> xj+1
xj
|f ′′(s)|2ds
1/2
.
(8.31)
Then
xj+1
>
xj
|e′(x)|2dx ≤h2
xj+1
>
xj
|f ′′(s)|2ds
and
xj+1
>
xj
|e(x)|2dx ≤h4
xj+1
>
xj
|f ′′(s)|2ds,
from which, summing over the index j from 0 to K −1 and taking the square
root of both sides, we obtain
> b
a
|e′(x)|2dx
1/2
≤h
> b
a
|f ′′(x)|2dx
1/2
,
and
> b
a
|e(x)|2dx
1/2
≤h2
> b
a
|f ′′(x)|2dx
1/2
,
which is the desired estimate (8.27), with C1 = C2 = 1.
3

8.4 Hermite-BirkoﬀInterpolation
341
8.4
Hermite-BirkoﬀInterpolation
Lagrange polynomial interpolation can be generalized to the case in which
also the values of the derivatives of a function f are available at some (or
all) of the nodes xi.
Let us then suppose that (xi, f (k)(xi)) are given data, with i = 0, . . . , n,
k = 0, . . . , mi and mi ∈N. Letting N = n
i=0(mi + 1), it can be proved
(see [Dav63]) that, if the nodes {xi} are distinct, there exists a unique
polynomial HN−1 ∈PN−1, called the Hermite interpolation polynomial,
such that
H(k)
N−1(xi) = y(k)
i
,
i = 0, . . . , n
k = 0, . . . , mi,
of the form
HN−1(x) =
n

i=0
mi

k=0
y(k)
i
Lik(x)
(8.32)
where y(k)
i
= f (k)(xi), i = 0, . . . , n, k = 0, . . . , mi.
The functions Lik ∈PN−1 are called the Hermite characteristic polynomials
and are deﬁned through the relations
dp
dxp (Lik)(xj) =
"
1
if i = j and k = p,
0
otherwise.
Deﬁning the polynomials
lij(x) = (x −xi)j
j!
n

k=0
k̸=i
 x −xk
xi −xk
mk+1
, i = 0, . . . , n, j = 0, . . . , mi,
and letting Limi(x) = limi(x) for i = 0, . . . , n, we have the following recur-
sive formula for the polynomials Lij
Lij(x) = lij(x) −
mi

k=j+1
l(k)
ij (xi)Lik(x)
j = mi −1, mi −2, . . . , 0.
As for the interpolation error, the following estimate holds
f(x) −HN−1(x) = f (N)(ξ)
N!
ΩN(x)
∀x ∈R
where ξ ∈I(x; x0, . . . , xn) and ΩN is the polynomial of degree N deﬁned
by
ΩN(x) = (x −x0)m0+1(x −x1)m1+1 . . . (x −xn)mn+1.
(8.33)

342
8. Polynomial Interpolation
Example 8.6 (osculatory interpolation) Let us set mi = 1 for i = 0, . . . , n.
In this case N = 2n + 2 and the interpolating Hermite polynomial is called the
osculating polynomial, and it is given by
HN−1(x) =
n

i=0
+
yiAi(x) + y(1)
i
Bi(x)
,
where Ai(x) = (1 −2(x −xi)l′
i(xi))li(x)2 and Bi(x) = (x −xi)li(x)2, for i =
0, . . . , n, with
l′
i(xi) =
n

k=0,k̸=i
1
xi −xk ,
i = 0, . . . , n.
As a comparison, we use Programs 65 and 67 to compute the Lagrange and
Hermite interpolating polynomials of the function f(x) = sin(4πx) on the interval
[0, 1] taking four equally spaced nodes (n = 3). Figure 8.4 shows the superposed
graphs of the function f (dashed line) and of the two polynomials Πnf (dotted
line) and HN−1 (solid line).
•
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
−1.5
−1
−0.5
0
0.5
1
1.5
FIGURE
8.4.
Lagrange
and
Hermite
interpolation
for
the
function
f(x) = sin(4πx) on the interval [0, 1]
Program 67 computes the values of the osculating polynomial at the ab-
scissae contained in the vector z. The input vectors x, y and dy contain the
interpolation nodes and the corresponding function evaluations of f and
f ′, respectively.
Program 67 - hermpol : Osculating polynomial
function [herm] = hermite(x,y,dy,z)
n = max(size(x)); m = max(size(z)); herm = [];
for j = 1:m
xx = z(j); hxv = 0;
for i = 1:n,
den = 1; num = 1; xn = x(i); derLi = 0;
for k = 1:n,

8.5 Extension to the Two-Dimensional Case
343
if k ˜= i, num = num*(xx-x(k)); arg = xn-x(k);
den = den*arg; derLi = derLi+1/arg;
end
end
Lix2 = (num/den)ˆ2; p = (1-2*(xx-xn)*derLi)*Lix2;
q = (xx-xn)*Lix2; hxv = hxv+(y(i)*p+dy(i)*q);
end
herm = [herm, hxv];
end
8.5
Extension to the Two-Dimensional Case
In this section we brieﬂy address the extension of the previous concepts to
the two-dimensional case, referring to [SL89], [CHQZ88], [QV94] for more
details. We denote by Ωa bounded domain in R2 and by x = (x, y) the
coordinate vector of a point in Ω.
8.5.1
Polynomial Interpolation
A particularly simple situation occurs when Ω= [a, b] × [c, d], i.e., the
interpolation domain Ωis the tensor product of two intervals. In such a
case, introducing the nodes a = x0 < x1 < . . . < xn = b and c = y0 <
y1 < . . . < ym = d, the interpolating polynomial Πn,mf can be written as
Πn,mf(x, y) = n
i=0
m
j=0 αijli(x)lj(y), where li ∈Pn, i = 0, . . . , n, and
lj ∈Pm, j = 0, . . . , m, are the characteristic one-dimensional Lagrange
polynomials with respect to the x and y variables respectively, and where
αij = f(xi, yj).
The drawbacks of one-dimensional Lagrange interpolation are inherited
by the two-dimensional case, as conﬁrmed by the example in Figure 8.5.
Remark 8.1 (The general case) If Ωis not a rectangular domain or if
the interpolation nodes are not uniformly distributed over a Cartesian grid,
the interpolation problem is diﬃcult to solve, and, generally speaking, it is
preferable to resort to a least-squares solution (see Section 10.7). We also
point out that in d dimensions (with d ≥2) the problem of ﬁnding an
interpolating polynomial of degree n with respect to each space variable on
n + 1 distinct nodes might be ill-posed.
Consider, for example, a polynomial of degree 1 with respect to x and y
of the form p(x, y) = a3xy+a2x+a1y+a0 to interpolate a function f at the
nodes (−1, 0), (0, −1), (1, 0) and (0, 1). Although the nodes are distinct, the
problem (which is nonlinear) does not in general admit a unique solution;
actually, imposing the interpolation constraints, we end up with a system
that is satisﬁed by any value of the coeﬃcient a3.
■

344
8. Polynomial Interpolation
−5
0
5
−5
0
5
−0.1
0
0.1
0.2
0.3
0.4
0.5
−5
0
5
−5
0
5
−2
0
2
4
6
8
FIGURE 8.5. Runge’s counterexample extended to the two-dimensional case:
interpolating polynomial on a 6 × 6 nodes grid (left) and on a 11 × 11 nodes grid
(right). Notice the change in the vertical scale between the two plots
8.5.2
Piecewise Polynomial Interpolation
In the multidimensional case, the higher ﬂexibility of piecewise interpola-
tion allows for easy handling of domains of complex shape. Let us suppose
that Ωis a polygon in R2. Then, Ωcan be partitioned into K nonover-
lapping triangles (or elements) T, which deﬁne the so called triangulation
of the domain which will be denoted by Th. Clearly, Ω=
-
T ∈Th
T. Suppose
that the maximum length of the edges of the triangles is less than a positive
number h. As shown in Figure 8.6 (left), not any arbitrary triangulation is
allowed. Precisely, the admissible ones are those for which any pair of non
disjoint triangles may have a vertex or an edge in common.
T
T2
1
2
T
T
2
1
T1
T2
T1
T
1
0
1
FT
T
x
y
y
x
aT
1
T
aT
3
aT
2
FIGURE 8.6. The left side picture shows admissible (above) and non admissible
(below) triangulations while the right side picture shows the aﬃne map from the
reference triangle ˆT to the generic element T ∈Th
Any element T ∈Th, of area equal to |T|, is the image through the aﬃne
map x = FT (ˆx) = BT ˆx + bT of the reference triangle T, of vertices (0,0),

8.5 Extension to the Two-Dimensional Case
345
(1,0) and (0,1) in the ˆx = (ˆx, ˆy) plane (see Figure 8.6, right), where the
invertible matrix BT and the right-hand side bT are given respectively by
BT =
.
x2 −x1
x3 −x1
y2 −y1
y3 −y1
/
,
bT = (x1, y1)T ,
(8.34)
while the coordinates of the vertices of T are denoted by a(l)
T
= (xl, yl)T
for l = 1, 2, 3.
(x)
l (x,y)
i
iz
z
1
i
li
1
z
iz
i
1
li(x,y)
1
li(x)
FIGURE 8.7. Characteristic piecewise Lagrange polynomial, in one and two space
dimensions. Left, k = 0; right, k = 1
The aﬃne map (8.34) is of remarkable importance in practical computa-
tions, since, once a basis has been generated for representing the piecewise
polynomial interpolant on ˆT, it is possible, applying the change of coor-
dinates x = FT (ˆx), to reconstruct the polynomial on each element T of
Th. We are thus interested in devising local basis functions, which can be
fully described over each triangle without needing any information from
adjacent triangles.
For this purpose, let us introduce on Th the set Z of the piecewise interpo-
lation nodes zi = (xi, yi)T , for i = 1, . . . , N, and denote by Pk(Ω), k ≥0,
the space of algebraic polynomials of degree ≤k in the space variables x, y
Pk(Ω) =





p(x, y) =
k

i,j=0
i+j≤k
aijxiyj, x, y ∈Ω





.
(8.35)
Finally, for k ≥0, let Pc
k(Ω) be the space of piecewise polynomials of degree
≤k, such that, for any p ∈Pc
k(Ω), p|T ∈Pk(T) for any T ∈Th. An ele-
mentary basis for Pc
k(Ω) consists of the Lagrange characteristic polynomials
li = li(x, y), such that li ∈Pc
k(Ω) and
li(zj) = δij,
i, j = 1, . . . , N,
(8.36)

346
8. Polynomial Interpolation
where δij is the Kronecker symbol. We show in Figure 8.7 the functions li for
k = 0, 1, together with their corresponding one-dimensional counterparts.
In the case k = 0, the interpolation nodes are collocated at the centers of
gravity of the triangles, while in the case k = 1 the nodes coincide with
the vertices of the triangles. This choice, that we are going to maintain
henceforth, is not the only one possible. The midpoints of the edges of the
triangles could be used as well, giving rise to a discontinuous piecewise
polynomial over Ω.
For k ≥0, the Lagrange piecewise interpolating polynomial of f, Πk
hf ∈
Pc
k(Ω), is deﬁned as
Πk
hf(x, y) =
N

i=1
f(zi)li(x, y).
(8.37)
Notice that Π0
hf is a piecewise constant function, while Π1
hf is a linear
function over each triangle, continuous at the vertices, and thus globally
continuous.
For any T ∈Th, we shall denote by Πk
T f the restriction of the piecewise
interpolating polynomial of f over the element T. By deﬁnition, Πk
T f ∈
Pk(T); noticing that dk = dimPk(T) = (k + 1)(k + 2)/2, we can therefore
write
Πk
T f(x, y) =
dk−1

m=0
f(˜z(m)
T
)lm,T (x, y),
∀T ∈Th.
(8.38)
In (8.38), we have denoted by ˜z(m)
T
, for m = 0, . . . , dk −1, the piecewise
interpolation nodes on T and by lm,T (x, y) the restriction to T of the La-
grange characteristic polynomial having index i in (8.37) which corresponds
in the list of the “global” nodes zi to that of the “local” node ˜z(m)
T
.
Keeping on with this notation, we have lj,T (x) = ˆlj ◦F −1
T (x), where
ˆlj = ˆlj(ˆx) is, for j = 0, . . . , dk −1, the j-th Lagrange basis function for
Pk( ˆT) generated on the reference element ˆT. We notice that if k = 0 then
d0 = 1, that is, only one local interpolation node exists (coinciding with
the center of gravity of the triangle T), while if k = 1 then d1 = 3, that is,
three local interpolation nodes exist, coinciding with the vertices of T. In
Figure 8.8 we draw the local interpolation nodes on ˆT for k = 0, 1 and 2.
As for the interpolation error estimate, denoting for any T ∈Th by hT the
maximum length of the edges of T, the following result holds (see for the
proof, [CL91], Theorem 16.1, pp. 125-126 and [QV94], Remark 3.4.2, pp.
89-90)
∥f −Πk
T f∥∞,T ≤Chk+1
T
∥f (k+1)∥∞,T ,
k ≥0,
(8.39)
where for every g ∈C0(T), ∥g∥∞,T = maxx∈T |g(x)|. In (8.39), C is a
positive constant independent of hT and f.

8.5 Extension to the Two-Dimensional Case
347
FIGURE 8.8. Local interpolation nodes on ˆT; left, k = 0, center k = 1, right,
k = 2
Let us assume that the triangulation Th is regular, i.e., there exists a
positive constant σ such that
max
T ∈Th
hT
ρT
≤σ,
where ∀T ∈Th, ρT is the diameter of the inscribed circle to T, Then, it
is possible to derive from (8.39) the following interpolation error estimate
over the whole domain Ω
∥f −Πk
hf∥∞,Ω≤Chk+1∥f (k+1)∥∞,Ω,
k ≥0,
∀f ∈Ck+1(Ω).
(8.40)
The theory of piecewise interpolation is a basic tool of the ﬁnite element
method, a computational technique that is widely used in the numerical
approximation of partial diﬀerential equations (see Chapter 12 for the one-
dimensional case and [QV94] for a complete presentation of the method).
Example 8.7 We compare the convergence of the piecewise polynomial interpo-
lation of degree 0, 1 and 2, on the function f(x, y) = e−(x2+y2) on Ω= (−1, 1)2.
We show in Table 8.4 the error Ek = ∥f −Πk
hf∥∞,Ω, for k = 0, 1, 2, and the order
of convergence pk as a function of the mesh size h = 2/N for N = 2, . . . , 32.
Clearly, linear convergence is observed for interpolation of degree 0 while the
order of convergence is quadratic with respect to h for interpolation of degree 1
and cubic for interpolation of degree 2.
•
h
E0
p0
E1
p1
E2
p2
1
0.4384
0.2387
0.016
1
2
0.2931
0.5809
0.1037
1.2028
1.6678 · 10−3
3.2639
1
4
0.1579
0.8924
0.0298
1.7990
2.8151 · 10−4
2.5667
1
8
0.0795
0.9900
0.0077
1.9524
3.5165 · 10−5
3.001
1
16
0.0399
0.9946
0.0019
2.0189
4.555 · 10−6
2.9486
TABLE 8.4. Convergence rates and orders for piecewise interpolations of degree
0, 1 and 2

348
8. Polynomial Interpolation
8.6
Approximation by Splines
In this section we address the matter of approximating a given function us-
ing splines, which allow for a piecewise interpolation with a global smooth-
ness.
Deﬁnition 8.1 Let x0, . . . , xn, be n + 1 distinct nodes of [a, b], with a =
x0 < x1 < . . . < xn = b. The function sk(x) on the interval [a,b] is a spline
of degree k relative to the nodes xj if
sk|[xj,xj+1] ∈Pk,
j = 0, 1, . . . , n −1
(8.41)
sk ∈Ck−1[a, b].
(8.42)
■
Denoting by Sk the space of splines sk on [a, b] relative to n + 1 distinct
nodes, then dim Sk = n+k. Obviously, any polynomial of degree k on [a, b]
is a spline; however, in the practice a spline is represented by a diﬀerent
polynomial on each subinterval and for this reason there could be a discon-
tinuity in its k-th derivative at the internal nodes x1, . . . , xn−1. The nodes
for which this actually happens are called active nodes.
It is simple to check that conditions (8.41) and (8.42) do not suﬃce to
characterize a spline of degree k. Indeed, the restriction sk,j = sk|[xj,xj+1]
can be represented as
sk,j(x) =
k

i=0
sij(x −xj)i,
if x ∈[xj, xj+1]
(8.43)
so that (k + 1)n coeﬃcients sij must be determined. On the other hand,
from (8.42) it follows that
s(m)
k,j−1(xj) = s(m)
k,j (xj),
j = 1, . . . , n −1,
m = 0, ..., k −1
which amounts to setting k(n −1) conditions. As a consequence, the re-
maining degrees of freedom are (k + 1)n −k(n −1) = k + n.
Even if the spline were interpolatory, that is, such that sk(xj) = fj for
j = 0, . . . , n, where f0, . . . , fn are given values, there would still be k −1
unsaturated degrees of freedom. For this reason further constraints are
usually imposed, which lead to:
1. periodic splines, if
s(m)
k
(a) = s(m)
k
(b),
m = 0, 1, . . . , k −1;
(8.44)

8.6 Approximation by Splines
349
2. natural splines, if for k = 2l −1, with l ≥2
s(l+j)
k
(a) = s(l+j)
k
(b) = 0,
j = 0, 1, . . . , l −2.
(8.45)
From (8.43) it turns out that a spline can be conveniently represented using
k +n spline basis functions, such that (8.42) is automatically satisﬁed. The
simplest choice, which consists of employing a suitably enriched monomial
basis (see Exercise 10), is not satisfactory from the numerical standpoint,
since it is ill-conditioned. In Sections 8.6.1 and 8.6.2 possible examples of
spline basis functions will be provided: cardinal splines for the speciﬁc case
k = 3 and B-splines for a generic k.
8.6.1
Interpolatory Cubic Splines
Interpolatory cubic splines are particularly signiﬁcant since: i. they are
the splines of minimum degree that yield C2 approximations; ii. they are
suﬃciently smooth in the presence of small curvatures.
Let us thus consider, in [a, b], n + 1 ordered nodes a = x0 < x1 < . . . <
xn = b and the corresponding evaluations fi, i = 0, . . . , n. Our aim is to
provide an eﬃcient procedure for constructing the cubic spline interpolating
those values. Since the spline is of degree 3, its second-order derivative must
be continuous. Let us introduce the following notation
fi = s3(xi),
mi = s′
3(xi),
Mi = s′′
3(xi),
i = 0, . . . , n.
Since s3,i−1 ∈P3, s′′
3,i−1 is linear and
s′′
3,i−1(x) = Mi−1
xi −x
hi
+ Mi
x −xi−1
hi
for x ∈[xi−1, xi]
(8.46)
where hi = xi −xi−1. Integrating (8.46) twice we get
s3,i−1(x) = Mi−1
(xi −x)3
6hi
+ Mi
(x −xi−1)3
6hi
+ Ci−1(x −xi−1) + $Ci−1,
and the constants Ci−1 and $Ci−1 are determined by imposing the end point
values s3(xi−1) = fi−1 and s3(xi) = fi. This yields, for i = 1, . . . , n −1
$Ci−1 = fi−1 −Mi−1
h2
i
6 ,
Ci−1 = fi −fi−1
hi
−hi
6 (Mi −Mi−1).
Let us now enforce the continuity of the ﬁrst derivatives at xi; we get
s′
3(x−
i )
= hi
6 Mi−1 + hi
3 Mi + fi −fi−1
hi
= −hi+1
3
Mi −hi+1
6
Mi+1 + fi+1 −fi
hi+1
= s′
3(x+
i ),

350
8. Polynomial Interpolation
where s′
3(x±
i ) = lim
t→0s′
3(xi ± t). This leads to the following linear system
(called M-continuity system)
µiMi−1 + 2Mi + λiMi+1 = di
i = 1, . . . , n −1
(8.47)
where we have set
µi =
hi
hi + hi+1
,
λi =
hi+1
hi + hi+1
,
di =
6
hi + hi+1
fi+1 −fi
hi+1
−fi −fi−1
hi

,
i = 1, . . . , n −1.
System (8.47) has n + 1 unknowns and n −1 equations; thus, 2(= k −1)
conditions are still lacking. In general, these conditions can be of the form
2M0 + λ0M1 = d0,
µnMn−1 + 2Mn = dn,
with 0 ≤λ0, µn ≤1 and d0, dn given values. For instance, in order to
obtain the natural splines (satisfying s′′
3(a) = s′′
3(b) = 0), we must set the
above coeﬃcients equal to zero. A popular choice sets λ0 = µn = 1 and
d0 = d1, dn = dn−1, which corresponds to prolongating the spline outside
the end points of the interval [a, b] and treating a and b as internal points.
This strategy produces a spline with a “smooth” behavior. In general, the
resulting linear system is tridiagonal of the form


2
λ0
0
. . .
0
µ1
2
λ1
...
0
...
...
...
0
...
µn−1
2
λn−1
0
. . .
0
µn
2




M0
M1
...
Mn−1
Mn


=


d0
d1
...
dn−1
dn


(8.48)
and it can be eﬃciently solved using the Thomas algorithm (3.53).
A closure condition for system (8.48), which can be useful when the
derivatives f ′(a) and f ′(b) are not available, consists of enforcing the con-
tinuity of s′′′
3 (x) at x1 and xn−1. Since the nodes x1 and xn−1 do not
actually contribute in constructing the cubic spline, it is called a not-a-
knot spline, with “active” knots {x0, x2, . . . , xn−2, xn} and interpolating f
at all the nodes {x0, x1, x2, . . . , xn−2, xn−1, xn}.
Remark 8.2 (Speciﬁc software) Several packages exist for dealing with
interpolating splines. In the case of cubic splines, we mention the command
spline, which uses the not-a-knot condition introduced above, or, in gen-
eral, the spline toolbox of MATLAB [dB90] and the library FITPACK
[Die87a], [Die87b].
■

8.6 Approximation by Splines
351
A completely diﬀerent approach for generating s3 consists of providing
a basis {ϕi} for the space S3 of cubic splines, whose dimension is equal to
n + 3. We consider here the case in which the n + 3 basis functions ϕi have
global support in the interval [a, b], referring to Section 8.6.2 for the case
of a basis with local support.
Functions ϕi, for i, j = 0, . . . , n, are deﬁned through the following inter-
polation constraints
ϕi(xj) = δij,
ϕ′
i(x0) = ϕ′
i(xn) = 0,
and two suitable splines must be added, ϕn+1 and ϕn+2. For instance, if
the spline must satisfy some assigned conditions on the derivative at the
end points, we ask that
ϕn+1(xj) = 0,
j = 0, ..., n
ϕ′
n+1(x0) = 1,
ϕ′
n+1(xn) = 0,
ϕn+2(xj) = 0,
j = 0, ..., n
ϕ′
n+2(x0) = 0,
ϕ′
n+2(xn) = 1.
By doing so, the spline takes the form
s3(x) =
n

i=0
fiϕi(x) + f ′
0ϕn+1(x) + f ′
nϕn+2(x),
where f ′
0 and f ′
n are two given values. The resulting basis {ϕi, i = 0, ..., n + 2}
is called a cardinal spline basis and is frequently employed in the numerical
solution of diﬀerential or integral equations. Figure 8.9 shows a generic car-
dinal spline, which is computed over a virtually unbounded interval where
the interpolation nodes xj are the integers. The spline changes sign in any
adjacent intervals [xj−1, xj] and [xj, xj+1] and rapidly decays to zero.
Restricting ourselves to the positive axis, it can be shown (see [SL89])
that the extremant of the function on the interval [xj, xj+1] is equal to
the extremant on the interval [xj+1, xj+2] multiplied by a decaying factor
λ ∈(0, 1). In such a way, possible errors arising over an interval are rapidly
damped on the next one, thus ensuring the stability of the algorithm.
Let us summarize the main properties of interpolating cubic splines, re-
ferring to [Sch81] and [dB83] for the proofs and more general results.
Property 8.2 Let f ∈C2([a, b]), and let s3 be the natural cubic spline
interpolating f. Then
b
>
a
[s′′
3(x)]2dx ≤
b
>
a
[f ′′(x)]2dx,
(8.49)
where equality holds if and only if f = s3.
The above result is known as the minimum norm property and has the
meaning of the minimum energy principle in mechanics. Property (8.49)

352
8. Polynomial Interpolation
−4
−3
−2
−1
0
1
2
3
4
5
−0.2
0
0.2
0.4
0.6
0.8
FIGURE 8.9. Cardinal spline
still holds if conditions on the ﬁrst derivative of the spline at the end points
are assigned instead of natural conditions (in such a case, the spline is called
constrained, see Exercise 11).
The cubic interpolating spline sf of a function f ∈C2([a, b]), with
s′
f(a) = f ′(a) and s′
f(b) = f ′(b), also satisﬁes the following property
b
>
a
[f ′′(x) −s′′
f(x)]2dx ≤
b
>
a
[f ′′(x) −s′′(x)]2dx, ∀s ∈S3.
As far as the error estimate is concerned, the following result holds.
Property 8.3 Let f ∈C4([a, b]) and ﬁx a partition of [a, b] into subinter-
vals of width hi such that h = maxi hi and β = h/ mini hi. Let s3 be the
cubic spline interpolating f. Then
∥f (r) −s(r)
3 ∥∞≤Crh4−r∥f (4)∥∞,
r = 0, 1, 2, 3,
(8.50)
with C0 = 5/384, C1 = 1/24, C2 = 3/8 and C3 = (β + β−1)/2.
As a consequence, spline s3 and its ﬁrst and second order derivatives
uniformly converge to f and to its derivatives, as h tends to zero. The third
order derivative converges as well, provided that β is uniformly bounded.
Example 8.8 Figure 8.10 shows the cubic spline approximating the function in
the Runge’s example, and its ﬁrst, second and third order derivatives, on a grid
of 11 equally spaced nodes, while in Table 8.5 the error ∥s3 −f∥∞is reported as
a function of h together with the computed order of convergence p. The results
clearly demonstrate that p tends to 4 (the theoretical order) as h tends to zero.
•

8.6 Approximation by Splines
353
h
1
0.5
0.25
0.125
0.0625
∥s3 −f∥∞
0.022
0.0032
2.7741e-4
1.5983e-5
9.6343e-7
p
–
2.7881
3.5197
4.1175
4.0522
TABLE 8.5. Experimental interpolation error for Runge’s function using cubic
splines
−5
−4
−3
−2
−1
0
1
2
3
4
5
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
(a)
−5
−4
−3
−2
−1
0
1
2
3
4
5
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
(b)
−5
−4
−3
−2
−1
0
1
2
3
4
5
−2
−1.5
−1
−0.5
0
0.5
1
(c)
−5
−4
−3
−2
−1
0
1
2
3
4
5
−5
−4
−3
−2
−1
0
1
2
3
4
5
(d)
FIGURE 8.10. Interpolating spline (a) and its ﬁrst (b), second (c) and third (d)
order derivatives (in solid line) for the function of Runge’s example (in dashed
line)
8.6.2
B-splines
Let us go back to splines of a generic degree k, and consider the B-spline
(or bell-spline) basis, referring to divided diﬀerences introduced in Section
8.2.1.
Deﬁnition 8.2 The normalized B-spline Bi,k+1 of degree k relative to the
distinct nodes xi, . . . , xi+k+1 is deﬁned as
Bi,k+1(x) = (xi+k+1 −xi)g[xi, . . . , xi+k+1],
(8.51)
where
g(t) = (t −x)k
+ =
" (t −x)k
if x ≤t,
0
otherwise.
(8.52)

354
8. Polynomial Interpolation
■
Substituting (8.18) into (8.51) yields the following explicit representation
Bi,k+1(x) = (xi+k+1 −xi)
k+1

j=0
(xj+i −x)k
+
k+1

l=0
l̸=j
(xi+j −xi+l)
.
(8.53)
From (8.53) it turns out that the active nodes of Bi,k+1(x) are xi, . . . , xi+k+1
and that Bi,k+1(x) is non null only within the interval [xi, xi+k+1].
Actually, it can be proved that it is the unique non null spline of min-
imum support relative to nodes xi, . . . , xi+k+1 [Sch67]. It can also be
shown that Bi,k+1(x) ≥0 [dB83] and |B(l)
i,k+1(xi)| = |B(l)
i,k+1(xi+k+1)| for
l = 0, . . . , k−1 [Sch81]. B-splines admit the following recursive formulation
([dB72], [Cox72])
Bi,1(x) =
" 1
if x ∈[xi, xi+1],
0
otherwise,
Bi,k+1(x) =
x −xi
xi+k −xi
Bi,k(x) +
xi+k+1 −x
xi+k+1 −xi+1
Bi+1,k(x), k ≥1,
(8.54)
which is usually preferred to (8.53) when evaluating a B-spline at a given
point.
Remark 8.3 It is possible to deﬁne B-splines even in the case of partially
coincident nodes, by suitably extending the deﬁnition of divided diﬀerences.
This leads to a new recursive form of Newton divided diﬀerences given by
(see for further details [Die93])
f[x0, . . . , xn] =







f[x1, . . . , xn] −f[x0, . . . , xn−1]
xn −x0
if x0 < x1 < . . . < xn
f (n+1)(x0)
(n + 1)!
if x0 = x1 = . . . = xn.
Assuming that m (with 1 < m < k +2) of the k +2 nodes xi, . . . , xi+k+1
are coincident and equal to λ, then (8.46) will contain a linear combination
of the functions (λ −x)k+1−j
+
, for j = 1, . . . , m. As a consequence, the
B-spline can have continuous derivatives at λ only up to order k −m and,
therefore, it is discontinuous if m = k + 1. It can be checked [Die93] that,
if xi−1 < xi = . . . = xi+k < xi+k+1, then
Bi,k+1(x) =





 xi+k+1 −x
xi+k+1 −xi
k
if x ∈[xi, xi+k+1],
0
otherwise,

8.6 Approximation by Splines
355
while for xi < xi+1 = . . . = xi+k+1 < xi+k+2
Bi,k+1(x) =






x −xi
xi+k+1 −xi
k
if x ∈[xi, xi+k+1],
0
otherwise.
Combining these formulae with the recursive relation (8.54) allows for con-
structing B-splines with coincident nodes.
■
Example 8.9 Let us examine the special case of cubic B-splines on equally
spaced nodes xi+1 = xi + h for i = 0, ..., n −1. Equation (8.53) becomes
6h3Bi,4(x) =























(x −xi)3,
if x ∈[xi, xi+1],
h3 + 3h2(x −xi+1) + 3h(x −xi+1)2 −3(x −xi+1)3, if x ∈[xi+1, xi+2],
h3 + 3h2(xi+3 −x) + 3h(xi+3 −x)2 −3(xi+3 −x)3, if x ∈[xi+2, xi+3],
(xi+4 −x)3,
if x ∈[xi+3, xi+4],
0
otherwise.
In Figure 8.11 the graph of Bi,4 is shown in the case of distinct nodes and of
partially coincident nodes.
•
−2
−1
0
1
2
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
FIGURE 8.11. B-spline with distinct nodes (in solid line) and with three coin-
cident nodes at the origin (in dashed line). Notice the discontinuity of the ﬁrst
derivative
Given n + 1 distinct nodes xj, j = 0, . . . , n, n −k linearly independent
B-splines of degree k can be constructed, though 2k degrees of freedom are

356
8. Polynomial Interpolation
still available to generate a basis for Sk. One way of proceeding consists of
introducing 2k ﬁctitious nodes
x−k ≤x−k+1 ≤. . . ≤x−1 ≤x0 = a,
b = xn ≤xn+1 ≤. . . ≤xn+k
(8.55)
which the B-splines Bi,k+1, with i = −k, . . . , −1 and i = n −k, . . . , n −1,
are associated with. By doing so, any spline sk ∈Sk can be uniquely written
as
sk(x) =
n−1

i=−k
ciBi,k+1(x).
(8.56)
The real numbers ci are the B-spline coeﬃcients of sk. Nodes (8.55) are
usually chosen as coincident or periodic.
1. Coincident: this choice is suitable for enforcing the values attained
by a spline at the end points of its deﬁnition interval. In such a case,
indeed, thanks to Remark 8.3 about B-splines with coincident nodes,
we get
sk(a) = c−k,
sk(b) = cn−1.
(8.57)
2. Periodic, that is
x−i = xn−i −b + a,
xi+n = xi + b −a,
i = 1, . . . , k.
This choice is useful if the periodicity conditions (8.44) have to be
imposed.
Remark 8.4 (Inserting nodes) Using B-splines instead of cardinal spli-
nes is advantageous when handling, with a reduced computational eﬀort, a
given conﬁguration of nodes for which a spline sk is known. In particular,
assume that the coeﬃcients ci of sk (in form (8.56)) are available over the
nodes x−k, x−k+1, . . . , xn+k, and that we wish to add to these a new node
$x.
The spline $sk ∈Sk, deﬁned over the new set of nodes, admits the follow-
ing representation with respect to a new B-spline basis
2
˜Bi,k+1
3
$sk(x) =
n−1

i=−k
di $Bi,k+1(x).
The new coeﬃcients di can be computed starting from the known coeﬃ-
cients ci using the following algorithm [Boe80]:

8.7 Splines in Parametric Form
357
let $x ∈[xj, xj+1); then, construct a new set of nodes {yi} such that
yi = xi for i = −k, . . . , j,
yj+1 = $x,
yi = xi−1 for i = j + 2, . . . , n + k + 1;
deﬁne
ωi =









1
for i = −k, . . . , j −k,
yj+1 −yi
yi+k+1 −yi
for i = j −k + 1, . . . , j,
0
for i = j + 1, . . . , n;
compute
di = ωici + (1 −ωi)ci
i = −k, ..., n −1.
This algorithm has good stability properties and can be generalized to the
case where more than one node is inserted at the same time (see [Die93]).
■
8.7
Splines in Parametric Form
Using interpolating splines presents the following two drawbacks:
1. the resulting approximation is of good quality only if the function
f does not exhibit large derivatives (in particular, we require that
|f ′(x)| < 1 for every x). Otherwise, oscillating behaviors may arise
in the spline, as demonstrated by the example considered in Figure
8.12 which shows, in solid line, the cubic interpolating spline over the
following set of data (from [SL89])
xi
8.125
8.4
9
9.845
9.6
9.959
10.166
10.2
fi
0.0774
0.099
0.28
0.6
0.708
1.3
1.8
2.177
2. sk depends on the choice of the coordinate system. In fact, performing
a clockwise rotation of 36 degrees of the coordinate system in the
above example, would lead to the spline without spurious oscillations
reported in the boxed frame in Figure 8.12.
All the interpolation procedures considered so far depend on the cho-
sen Cartesian reference system, which is a negative feature if the
spline is used for a graphical representation of a given ﬁgure (for in-
stance, an ellipse). Indeed, we would like such a representation to
be independent of the reference system, that is, to have a geometric
invariance property.

358
8. Polynomial Interpolation
8
8.5
9
9.5
10
10.5
0
0.5
1
1.5
2
2.5
6
7
8
9
10
−5.2
−5
−4.8
−4.6
−4.4
−4.2
FIGURE 8.12. Geometric noninvariance for an interpolating cubic spline s3: the
set of data for s3 in the boxed frame is the same as in the main ﬁgure, rotated
by 36 degrees. The rotation diminishes the slope of the interpolated curve and
eliminates any oscillation from s3. Notice that resorting to a parametric spline
(dashed line) removes the oscillations in s3 without any rotation of the reference
system
A solution is provided by parametric splines, in which any component of the
curve, written in parametric form, is approximated by a spline function.
Consider a plane curve in parametric form P(t) = (x(t), y(t)), with t ∈
[0, T], then take the set of the points in the plane of coordinates Pi =
(xi, yi), for i = 0, . . . , n, and introduce a partition onto [0, T]: 0 = t0 <
t1 < . . . < tn = T.
Using the two sets of values {ti, xi} and {ti, yi} as interpolation data,
we obtain the two splines sk,x and sk,y, with respect to the independent
variable t, that interpolate x(t) and y(t), respectively. The parametric curve
Sk(t) = (sk,x(t), sk,y(t)) is called the parametric spline. Obviously, diﬀerent
parameterizations of the interval [0, T] yield diﬀerent splines (see Figure
8.13).
A reasonable choice of the parameterization makes use of the length of
each segment Pi−1Pi,
li =

(xi −xi−1)2 + (yi −yi−1)2,
i = 1, . . . , n.
Setting t0 = 0 and ti = i
k=1 lk for i = 1, . . . , n, every ti represents the
cumulative length of the piecewise line that joins the points from P0 to
Pi. This function is called the cumulative length spline and approximates
satisfactorily even those curves with large curvature. Moreover, it can also
be proved (see [SL89]) that it is geometrically invariant.
Program 68 implements the construction of cumulative parametric cu-
bic splines in two dimensions (it can be easily generalized to the three-

8.7 Splines in Parametric Form
359
−2
0
2
4
6
−4
−2
0
2
4
FIGURE 8.13. Parametric splines for a spiral-like node distribution. The spline
of cumulative length is drawn in solid line
dimensional case). Composite parametric splines can be generated as well
by enforcing suitable continuity conditions (see [SL89]).
Program 68 - par spline : Parametric splines
function [xi,yi] = par spline (x, y)
t (1) = 0;
for i = 1:length (x)-1
t (i+1) = t (i) + sqrt ( (x(i+1)-x(i))ˆ2 + (y(i+1)-y(i))ˆ2 );
end
z = [t(1):(t(length(t))-t(1))/100:t(length(t))];
xi = spline (t,x,z);
yi = spline (t,y,z);
8.7.1
B´ezier Curves and Parametric B-splines
The B´ezier curves and parametric B-splines are widely employed in graph-
ical applications, where the nodes’ locations might be aﬀected by some
uncertainty.
Let P0, P1, . . . , Pn be n + 1 points ordered in the plane. The oriented
polygon formed by them is called the characteristic polygon or B´ezier poly-
gon. Let us introduce the Bernstein polynomials over the interval [0, 1]
deﬁned as
bn,k(t) =

n
k

tk(1 −t)n−k =
n!
k!(n −k)!tk(1 −t)n−k,

360
8. Polynomial Interpolation
for n = 0, 1, . . . and k = 0, . . . , n. They can be obtained by the following
recursive formula
"
bn,0(t) = (1 −t)n
bn,k(t) = (1 −t)bn−1,k(t) + tbn−1,k−1(t),
k = 1, . . . , n, t ∈[0, 1].
It is easily seen that bn,k ∈Pn, for k = 0, . . . , n. Also, {bn,k, k = 0, . . . , n}
provides a basis for Pn. The B´ezier curve is deﬁned as follows
Bn(P0, P1, . . . , Pn, t) =
n

k=0
Pkbn,k(t),
0 ≤t ≤1.
(8.58)
This expression can be regarded as a weighted average of the points Pk,
with weights bn,k(t).
The B´ezier curves can also be obtained by a pure geometric approach
starting from the characteristic polygon. Indeed, for any ﬁxed t ∈[0, 1],
we deﬁne Pi,1(t) = (1 −t)Pi + tPi+1 for i = 0, . . . , n −1 and, for t
ﬁxed, the piecewise line that joins the new nodes Pi,1(t) forms a polygon
of n −1 edges. We can now repeat the procedure by generating the new
vertices Pi,2(t) (i = 0, . . . , n −2), and terminating as soon as the polygon
comprises only the vertices P0,n−1(t) and P1,n−1(t). It can be shown that
P0,n(t) = (1 −t)P0,n−1(t) + tP1,n−1(t) = Bn(P0, P1, . . . , Pn, t),
that is, P0,n(t) is equal to the value of the B´ezier curve Bn at the points
corresponding to the ﬁxed value of t. Repeating the process for several val-
ues of the parameter t yields the construction of the curve in the considered
region of the plane.
2
4
6
8
10
12
14
16
−4
−2
0
2
4
6
8
FIGURE 8.14. Computation of the value of B3 relative to the points (0,0), (4,7),
(14,7), (17,0) for t = 0.5, using the graphical method described in the text
Notice that, for a given node conﬁguration, several curves can be con-
structed according to the ordering of points Pi. Moreover, the B´ezier curve

8.7 Splines in Parametric Form
361
Bn(P0, P1, . . . , Pn, t) coincides with Bn(Pn, Pn−1, . . . , P0, t), apart from
the orientation.
Program 69 computes bn,k at the point x for x ∈[0, 1].
Program 69 - bernstein : Bernstein polynomials
function [bnk]=bernstein (n,k,x)
if k == 0,
C = 1;
else,
C = prod ([1:n])/( prod([1:k])*prod([1:n-k]));
end
bnk = C * xˆk * (1-x)ˆ(n-k);
Program 70 plots the B´ezier curve relative to the set of points (x, y).
Program 70 - bezier : B´ezier curves
function [bezx,bezy] = bezier (x, y, n)
i = 0; k = 0;
for t = 0:0.01:1,
i = i + 1; bnk = bernstein (n,k,t); ber(i) = bnk;
end
bezx = ber * x (1); bezy = ber * y (1);
for k = 1:n
i = 0;
for t = 0:0.01:1
i = i + 1; bnk = bernstein (n,k,t); ber(i) = bnk;
end
bezx = bezx + ber * x (k+1); bezy = bezy + ber * y (k+1);
end
plot(bezx,bezy)
In practice, the B´ezier curves are rarely used since they do not provide a
suﬃciently accurate approximation to the characteristic polygon. For this
reason, in the 70’s the parametric B-splines were introduced, and they are
used in (8.58) instead of the Bernstein polynomials. Parametric B-splines
are widely employed in packages for computer graphics since they enjoy
the following properties:
1. perturbing a single vertex of the characteristic polygon yields a local
perturbation of the curve only around the vertex itself;
2. the parametric B-spline better approximates the control polygon than
the corresponding B´ezier curve does, and it is always contained within
the convex hull of the polygon.

362
8. Polynomial Interpolation
In Figure 8.15 a comparison is made between B´ezier curves and para-
metric B-splines for the approximation of a given characteristic polygon.
FIGURE 8.15. Comparison of a B´ezier curve (left) and a parametric B-spline
(right). The vertices of the characteristic polygon are denoted by ×
We conclude this section by noticing that parametric cubic B-splines
allow for obtaining locally straight lines by aligning four consecutive ver-
tices (see Figure 8.16) and that a parametric B-spline can be constrained
at a speciﬁc point of the characteristic polygon by simply making three
consecutive points of the polygon coincide with the desired point.
FIGURE 8.16. Some parametric B-splines as functions of the number and posi-
tions of the vertices of the characteristic polygon. Notice in the last ﬁgure (right)
the localization eﬀects due to moving a single vertex
8.8
Applications
In this section we consider two problems arising from the solution of fourth-
order diﬀerential equations and from the reconstruction of images in axial
tomographies.

8.8 Applications
363
8.8.1
Finite Element Analysis of a Clamped Beam
Let us employ piecewise Hermite polynomials (see Section 8.4) for the nu-
merical approximation of the transversal bending of a clamped beam. This
problem was already considered in Section 4.7.2 where centered ﬁnite dif-
ferences were used.
The mathematical model is the fourth-order boundary value problem
(4.74), here presented in the following general formulation
"
(α(x)u′′(x))′′ = f(x),
0 < x < L
u(0) = u(L) = 0,
u′(0) = u′(L) = 0.
(8.59)
In the particular case of (4.74) we have α = EJ and f = P; we assume
henceforth that α is a positive and bounded function over (0, L) and that
f ∈L2(0, L).
We multiply (8.59) by a suﬃciently smooth arbitrary function v, then,
we integrate by parts twice, to obtain
L
>
0
αu′′v′′dx −[αu′′′v]L
0 + [αu′′v′]L
0 =
L
>
0
fvdx.
Problem (8.59) is then replaced by the following problem in integral form
ﬁnd u ∈V such that
L
>
0
αu′′v′′dx =
L
>
0
fvdx,
∀v ∈V,
(8.60)
where
V =
2
v : v(k) ∈L2(0, L), k = 0, 1, 2, v(k)(0) = v(k)(L) = 0, k = 0, 1
3
.
Problem (8.60) admits a unique solution, which represents the deformed
conﬁguration that minimizes the total potential energy of the beam over
the space V (see, for instance, [Red86], p. 156)
J(u) =
L
>
0
1
2α(u′′)2 −fu

dx.
In view of the numerical solution of problem (8.60), we introduce a partition
Th of [0, L] into K subintervals Tk = [xk−1, xk], (k = 1, . . . , K) of uniform
length h = L/K, with xk = kh, and the ﬁnite dimensional space
Vh =

vh ∈C1([0, L]), vh|T ∈P3(T)
∀T ∈Th, v(k)
h (0) = v(k)
h (L) = 0, k = 0, 1
3
.
(8.61)

364
8. Polynomial Interpolation
Let us equip Vh with a basis. For this purpose, we associate with each
internal node xi (i = 1, . . . , K −1) a support σi = Ti ∪Ti+1 and two
functions ϕi, ψi deﬁned as follows: for any k, ϕi|Tk ∈P3(Tk), ψi|Tk ∈P3(Tk)
and for any j = 0, . . . , K,



ϕi(xj) = δij,
ϕ′
i(xj) = 0,
ψi(xj) = 0,
ψ′
i(xj) = δij.
(8.62)
Notice that the above functions belong to Vh and deﬁne a basis
Bh = {ϕi, ψi, i = 1, . . . , K −1}.
(8.63)
These basis functions can be brought back to the reference interval ˆT =
[0, 1] for 0 ≤ˆx ≤1, by the aﬃne maps x = hˆx + xk−1 between ˆT and Tk,
for k = 1, . . . , K.
Therefore, let us introduce on the interval ˆT the basis functions ˆϕ(0)
0
and ˆϕ(1)
0 , associated with the node ˆx = 0, and ˆϕ(0)
1
and ˆϕ(1)
1 , associated
with node ˆx = 1. Each of these is of the form ˆϕ = a0 + a1ˆx + a2ˆx2 +
a3ˆx3; in particular, the functions with superscript “0” must satisfy the
ﬁrst two conditions in (8.62), while those with superscript “1” must fulﬁll
the remaining two conditions. Solving the (4×4) associated system, we get
ˆϕ(0)
0 (ˆx) = 1 −3ˆx2 + 2ˆx3,
ˆϕ(1)
0 (ˆx) = ˆx −2ˆx2 + ˆx3,
ˆϕ(0)
1 (ˆx) = 3ˆx2 −2ˆx3,
ˆϕ(1)
1 (ˆx) = −ˆx2 + ˆx3.
(8.64)
The graphs of the functions (8.64) are drawn in Figure 8.17 (left), where
(0), (1), (2) and (3) denote ˆϕ(0)
0 , ˆϕ(0)
1 , ˆϕ(1)
0
and ˆϕ(1)
1 , respectively.
The function uh ∈Vh can be written as
uh(x) =
K−1

i=1
uiϕi(x) +
K−1

i=1
u(1)
i ψi(x).
(8.65)
The coeﬃcients and the degrees of freedom of uh have the following mean-
ing: ui = uh(xi), u(1)
i (xi) = u′
h(xi) for i = 1, . . . , K −1. Notice that (8.65)
is a special instance of (8.32), having set mi = 1.
The discretization of problem (8.60) reads
ﬁnd uh ∈Vh such that
L
>
0
αu′′
hv′′
hdx =
L
>
0
fvhdx,
∀vh ∈Bh.
(8.66)
This is called the Galerkin ﬁnite element approximation of the diﬀerential
problem (8.59). We refer to Chapter 12, Sections 12.4 and 12.4.5, for a
more comprehensive discussion and analysis of the method.

8.8 Applications
365
0
0.2
0.4
0.6
0.8
1
−0.2
0
0.2
0.4
0.6
0.8
1
(0)
(1)
(2)
(3)
0
20
40
60
80
100
120
140
160
10
−20
10
−15
10
−10
10
−5
10
0
10
5
No Prec.
Prec.
FIGURE 8.17. Canonical Hermite basis on the reference interval 0 ≤ˆx ≤1 (left);
convergence histories for the conjugate gradient method in the solution of system
(8.69) (right). On the x-axis the number of iterations k is shown, while the y-axis
represents the quantity ∥r(k)∥2/∥b1∥2, where r is the residual of system (8.69)
Using the representation (8.65) we end up with the following system in
the 2K −2 unknowns u1, u2, . . . , uK−1, u(1)
1 , u(1)
2 , . . . u(1)
K−1















K−1

j=1


uj
L
>
0
αϕ′′
j ϕ′′
i dx + u(1)
j
L
>
0
αψ′′
j ϕ′′
i dx


=
L
>
0
fϕidx,
K−1

j=1


uj
L
>
0
αϕ′′
j ψ′′
i dx + u(1)
j
L
>
0
αψ′′
j ψ′′
i dx


=
L
>
0
fψidx,
(8.67)
for i = 1, . . . , K−1. Assuming, for the sake of simplicity, that the beam has
unit length L, that α and f are two constants and computing the integrals
in (8.67), the ﬁnal system reads in matrix form
"
Au + Bp =
b1
BT u + Cp =
0,
(8.68)
where the vectors u, p ∈RK−1 contain the nodal unknowns ui and u(1)
i ,
b1 ∈RK−1 is the vector of components equal to h4f/α, while
A = tridiagK−1(−12, 24, −12),
B = tridiagK−1(−6, 0, 6),
C = tridiagK−1(2, 8, 2).
System (8.68) has size equal to 2(K −1); eliminating the unknown p from
the second equation, we get the reduced system (of size K −1)

A −BC−1BT 
u = b1.
(8.69)

366
8. Polynomial Interpolation
Since B is skew-symmetric and A is symmetric and positive deﬁnite (s.p.d.),
the matrix M = A−BC−1BT is s.p.d. too. Using Cholesky factorization for
solving system (8.69) is impractical as C−1 is full. An alternative is thus the
conjugate gradient method (CG) supplied with a suitable preconditioner
as the spectral condition number of M is of the order of h−4 = K4.
We notice that computing the residual at each step k ≥0 requires solv-
ing a linear system whose right side is the vector BT u(k), u(k) being the
current iterate of CG method, and whose coeﬃcient matrix is matrix C.
This system can be solved using the Thomas algorithm (3.53) with a cost
of the order of K ﬂops.
The CG algorithm terminates in correspondence to the lowest value of k
for which ∥r(k)∥2 ≤u∥b1∥2, where r(k) is the residual of system (8.69) and
u is the roundoﬀunit.
The results obtained running the CG method in the case of a uniform
partition of [0, 1] with K = 50 elements and setting α = f = 1 are sum-
marized in Figure 8.17 (right), which shows the convergence histories of
the method in both nonpreconditioned form (denoted by “Non Prec.”) and
with SSOR preconditioner (denoted by “Prec.”), having set the relaxation
parameter ω = 1.95.
We notice that the CG method does not converge within K −1 steps
due to the eﬀect of the rounding errors. Notice also the eﬀectiveness of the
SSOR preconditioner in terms of the reduction of the number of iterations.
However, the high computational cost of this preconditioner prompts us to
devise another choice. Looking at the structure of the matrix M a natural
preconditioner is M = A−B$C−1BT , where $C is the diagonal matrix whose
entries are $cii = K−1
j=1 |cij|. The matrix M is banded so that its inversion
requires a strongly reduced cost than for the SSOR preconditioner. More-
over, as shown in Table 8.6, using M provides a dramatic decrease of the
number of iterations to converge.
K
Without Precond.
SSOR
M
25
51
27
12
50
178
61
25
100
685
118
33
200
2849
237
34
TABLE 8.6. Number of iterations as a function of K
8.8.2
Geometric Reconstruction Based on Computer
Tomographies
A typical application of the algorithms presented in Section 8.7 deals with
the reconstruction of the three-dimensional structure of internal organs of
human body based on computer tomographies (CT).

8.8 Applications
367
FIGURE 8.18. Cross-section of a blood vessel (left) and an associated character-
istic polygon using 16 points Pi (right)
The CT usually provides a sequence of images which represent the sections
of an organ at several horizontal planes; as a convention, we say that the
CT produces sections of the x, y plane in correspondance of several values of
z. The result is analogous to what we would get by sectioning the organ at
diﬀerent values of z and taking the picture of the corresponding sections.
Obviously, the great advantage in using the CT is that the organ under
investigation can be visualized without being hidden by the neighboring
ones, as happens in other kinds of medical images, e.g., angiographies.
The image that is obtained for each section is coded into a matrix of
pixels (abbreviation of pictures elements) in the x, y plane; a certain value
is associated with each pixel expressing the level of grey of the image at
that point. This level is determined by the density of X rays which are
collected by a detector after passing through the human body. In practice,
the information contained in a CT at a given value of z is expressed by a
set of points (xi, yi) which identify the boundary of the organ at z.
To improve the diagnostics it is often useful to reconstruct the three-
dimensional structure of the organ under examination starting from the
sections provided by the CT. With this aim, it is necessary to convert the
information coded by pixels into a parametric representation which can be
expressed by suitable functions interpolating the image at some signiﬁcant
points on its boundary. This reconstruction can be carried out by using the
methods described in Section 8.7 as shown in Figure 8.19.
A set of curves like those shown in Figure 8.19 can be suitably stacked to
provide an overall three-dimensional view of the organ under examination.

368
8. Polynomial Interpolation
(a)
(b)
(c)
FIGURE 8.19. Reconstruction of the internal vessel of Figure 8.18 using diﬀerent
interpolating splines with the same characteristic polygon: (a) B´ezier curves, (b)
parametric splines and (c) parametric B-splines
8.9
Exercises
1. Prove that the characteristic polynomials li ∈Pn deﬁned in (8.3) form a
basis for Pn.
2. An alternative approach to the method in Theorem 8.1, for constructing
the interpolating polynomial, consists of directly enforcing the n + 1 in-
terpolation constraints on Πn and then computing the coeﬃcients ai. By
doing so, we end up with a linear system Xa= y, with a = (a0, . . . , an)T ,
y = (y0, . . . , yn)T and X = [xj
i]. X is called Vandermonde matrix. Prove
that X is nonsingular if the nodes xi are distinct.
[Hint: show that det(X)=

0≤j<i≤n
(xi −xj) by recursion on n.]
3. Prove that ω′
n+1(xi) =
n

j=0
j̸=i
(xi −xj) where ωn+1 is the nodal polynomial
(8.6). Then, check (8.5).
4. Provide an estimate of ∥ωn+1∥∞, in the cases n = 1 and n = 2, for a
distribution of equally spaced nodes.
5. Prove that
(n −1)!hn−1|(x −xn−1)(x −xn)| ≤|ωn+1(x)| ≤n!hn−1|(x −xn−1)(x −xn)|,
where n is even, −1 = x0 < x1 < . . . < xn−1 < xn = 1, x ∈(xn−1, xn) and
h = 2/n.

8.9 Exercises
369
[Hint : let N = n/2 and show ﬁrst that
ωn+1(x) = (x + Nh)(x + (N −1)h) . . . (x + h)x
(x −h) . . . (x −(N −1)h)(x −Nh).
(8.70)
Then, take x = rh with N −1 < r < N.]
6. Under the assumptions of Exercise 5, show that |ωn+1| is maximum if
x ∈(xn−1, xn) (notice that |ωn+1| is an even function).
[Hint : use (8.70) to prove that |ωn+1(x + h)/ωn+1(x)| > 1 for any x ∈
(0, xn−1) with x not coinciding with any interpolation node.]
7. Prove the recursive relation (8.19) for Newton divided diﬀerences.
8. Determine an interpolating polynomial Hf ∈Pn such that
(Hf)(k)(x0) = f (k)(x0),
k = 0, . . . , n,
and check that
Hf(x) =
n

j=0
f (j)(x0)
j!
(x −x0)j,
that is, the Hermite interpolating polynomial on one node coincides with
the Taylor polynomial.
9. Given the following set of data

f0 = f(−1) = 1, f1 = f ′(−1) = 1, f2 = f ′(1) = 2, f3 = f(2) = 1

,
prove that the Hermite-Birkoﬀinterpolating polynomial H3 does not exist
for them.
[Solution : letting H3(x) = a3x3 + a2x2 + a1x + a0, one must check that
the matrix of the linear system H3(xi) = fi for i = 0, . . . , 3 is singular.]
10. Check that any sk ∈Sk[a, b] admits a representation of the form
sk(x) =
k

i=0
bixi +
g

i=1
ci(x −xi)k
+,
that is, 1, x, x2, . . . , xk, (x −x1)k
+, . . . , (x −xg)k
+ form a basis for Sk[a, b].
11. Prove Property 8.2 and check its validity even in the case where the spline
s satisﬁes conditions of the form s′(a) = f ′(a), s′(b) = f ′(b).
[Hint: start from
b
>
a
0
f ′′(x) −s′′(x)
1
s′′(x)dx =
n

i=1
xi
>
xi−1
0
f ′′(x) −s′′(x)
1
s′′dx
and integrate by parts twice.]

370
8. Polynomial Interpolation
12. Let f(x) = cos(x) = 1 −x2
2! + x4
4! −x6
6! + . . . ; then, consider the following
rational approximation
r(x) = a0 + a2x2 + a4x4
1 + b2x2
,
(8.71)
called the Pad´e approximation. Determine the coeﬃcients of r in such a
way that
f(x) −r(x) = γ8x8 + γ10x10 + . . .
[Solution: a0 = 1, a2 = −7/15, a4 = 1/40, b2 = 1/30.]
13. Assume that the function f of the previous exercise is known at a set of n
equally spaced points xi ∈(−π/2, π/2) with i = 0, . . . , n. Repeat Exercise
12, determining, by using MATLAB, the coeﬃcients of r in such a way
that the quantity n
i=0 |f(xi) −r(xi)|2 is minimized. Consider the cases
n = 5 and n = 10.

9
Numerical Integration
In this chapter we present the most commonly used methods for numer-
ical integration. We will mainly consider one-dimensional integrals over
bounded intervals, although in Sections 9.8 and 9.9 an extension of the tech-
niques to integration over unbounded intervals (or integration of functions
with singularities) and to the multidimensional case will be considered.
9.1
Quadrature Formulae
Let f be a real integrable function over the interval [a, b]. Computing ex-
plicitly the deﬁnite integral I(f) =
 b
a f(x)dx may be diﬃcult or even
impossible. Any explicit formula that is suitable for providing an approxi-
mation of I(f) is said to be a quadrature formula or numerical integration
formula.
An example can be obtained by replacing f with an approximation fn,
depending on the integer n ≥0, then computing I(fn) instead of I(f).
Letting In(f) = I(fn), we have
In(f) =
b
>
a
fn(x)dx,
n ≥0.
(9.1)
The dependence on the end points a, b is always understood, so we write
In(f) instead of In(f; a, b).

372
9. Numerical Integration
If f ∈C0([a, b]), the quadrature error En(f) = I(f) −In(f) satisﬁes
|En(f)| ≤
b
>
a
|f(x) −fn(x)|dx ≤(b −a)∥f −fn∥∞.
Therefore, if for some n, ∥f −fn∥∞< ε, then |En(f)| ≤ε(b −a).
The approximant fn must be easily integrable, which is the case if, for
example, fn ∈Pn. In this respect, a natural approach consists of using
fn = Πnf, the interpolating Lagrange polynomial of f over a set of n + 1
distinct nodes {xi}, with i = 0, . . . , n. By doing so, from (9.1) it follows
that
In(f) =
n

i=0
f(xi)
b
>
a
li(x)dx,
(9.2)
where li is the characteristic Lagrange polynomial of degree n associated
with node xi (see Section 8.1). We notice that (9.2) is a special instance of
the following quadrature formula
In(f) =
n

i=0
αif(xi),
(9.3)
where the coeﬃcients αi of the linear combination are given by
 b
a li(x)dx.
Formula (9.3) is a weighted sum of the values of f at the points xi, for
i = 0, . . . , n. These points are said to be the nodes of the quadrature
formula, while the numbers αi ∈R are its coeﬃcients or weights. Both
weights and nodes depend in general on n; again, for notational simplicity,
this dependence is always understood.
Formula (9.2), called the Lagrange quadrature formula, can be generalized
to the case where also the values of the derivative of f are available. This
leads to the Hermite quadrature formula (see Section 9.5)
In(f) =
1

k=0
n

i=0
αikf (k)(xi)
(9.4)
where the weights are now denoted by αik.
Both (9.2) and (9.4) are interpolatory quadrature formulae, since the
function f has been replaced by its interpolating polynomial (Lagrange
and Hermite polynomials, respectively). We deﬁne the degree of exactness
of a quadrature formula as the maximum integer r ≥0 for which
In(f) = I(f),
∀f ∈Pr.
Any interpolatory quadrature formula that makes use of n + 1 distinct
nodes has degree of exactness equal to at least n. Indeed, if f ∈Pn, then

9.2 Interpolatory Quadratures
373
Πnf = f and thus In(Πnf) = I(Πnf). The converse statement is also true,
that is, a quadrature formula using n + 1 distinct nodes and having degree
of exactness equal at least to n is necessarily of interpolatory type (for the
proof see [IK66], p. 316).
As we will see in Section 10.2, the degree of exactness of a Lagrange
quadrature formula can be as large as 2n + 1 in the case of the so-called
Gaussian quadrature formulae.
9.2
Interpolatory Quadratures
We consider three remarkable instances of formula (9.2), corresponding to
n = 0, 1 and 2.
9.2.1
The Midpoint or Rectangle Formula
This formula is obtained by replacing f over [a, b] with the constant function
equal to the value attained by f at the midpoint of [a, b] (see Figure 9.1,
left). This yields
I0(f) = (b −a)f
a + b
2

(9.5)
with weight α0 = b −a and node x0 = (a + b)/2. If f ∈C2([a, b]), the
quadrature error is
E0(f) = h3
3 f ′′(ξ),
h = b −a
2
,
(9.6)
where ξ lies within the interval (a, b).
b
a
f(x)
x0
x
f(x)
xm−1
xk
x
x0
FIGURE 9.1. The midpoint formula (left); the composite midpoint formula
(right)
Indeed, expanding f in a Taylor’s series around c = (a + b)/2 and trun-
cating at the second-order, we get
f(x) = f(c) + f ′(c)(x −c) + f ′′(η(x))(x −c)2/2,

374
9. Numerical Integration
from which, integrating on (a, b) and using the mean-value theorem, (9.6)
follows. From this, it turns out that (9.5) is exact for constant and aﬃne
functions (since in both cases f ′′(ξ) = 0 for any ξ ∈(a, b)), so that the
midpoint rule has degree of exactness equal to 1.
It is worth noting that if the width of the integration interval [a, b] is
not suﬃciently small, the quadrature error (9.6) can be quite large. This
drawback is common to all the numerical integration formulae that will
be described in the three forthcoming sections and can be overcome by
resorting to their composite counterparts as discussed in Section 9.4.
Suppose now that we approximate the integral I(f) by replacing f over
[a, b] with its composite interpolating polynomial of degree zero, constructed
on m subintervals of width H = (b −a)/m, for m ≥1 (see Figure 9.1,
right). Introducing the quadrature nodes xk = a + (2k + 1)H/2, for k =
0, . . . , m −1, we get the composite midpoint formula
I0,m(f) = H
m−1

k=0
f(xk),
m ≥1.
(9.7)
The quadrature error E0,m(f) = I(f) −I0,m(f) is given by
E0,m(f) = b −a
24 H2f ′′(ξ),
H = b −a
m
(9.8)
provided that f ∈C2([a, b]) and where ξ ∈(a, b). From (9.8) we conclude
that (9.7) has degree of exactness equal to 1; (9.8) can be proved by recalling
(9.6) and using the additivity of integrals. Indeed, for k = 0, . . . , m−1 and
ξk ∈(a + kH, a + (k + 1)H),
E0,m(f) =
m−1

k=0
f ′′(ξk)(H/2)3/3 =
m−1

k=0
f ′′(ξk)H2
24
b −a
m
= b −a
24 H2f ′′(ξ).
The last equality is a consequence of the following theorem, that is applied
letting u = f ′′ and δj = 1 for j = 0, . . . , m −1.
Theorem 9.1 (discrete mean-value theorem) Let u ∈C0([a, b]) and
let xj be s + 1 points in [a, b] and δj be s + 1 constants, all having the same
sign. Then there exists η ∈[a, b] such that
s

j=0
δju(xj) = u(η)
s

j=0
δj.
(9.9)
Proof. Let um = minx∈[a,b] u(x) = u(¯x) and uM = maxx∈[a,b] u(x) = u(¯¯x),
where ¯x and ¯¯x are two points in (a, b). Then
um
s

j=0
δj ≤
s

j=0
δju(xj) ≤uM
s

j=0
δj.
(9.10)

9.2 Interpolatory Quadratures
375
Let σs = s
j=0 δju(xj) and consider the continuous function U(x) = u(x) s
j=0 δj.
Thanks to (9.10), U(¯x) ≤σs ≤U(¯¯x). Applying the mean-value theorem, there
exists a point η between a and b such that U(η) = σs, which is (9.9). A similar
proof can be carried out if the coeﬃcients δj are negative.
3
The composite midpoint formula is implemented in Program 71. Through-
out this chapter, we shall denote by a and b the end points of the integration
interval and by m the number of quadrature subintervals. The variable fun
contains the expression of the function f, while the output variable int
contains the value of the approximate integral.
Program 71 - midpntc : Midpoint composite formula
function int = midpntc(a,b,m,fun)
h=(b-a)/m; x=[a+h/2:h:b]; dim = max(size(x)); y=eval(fun);
if size(y)==1, y=diag(ones(dim))*y; end; int=h*sum(y);
9.2.2
The Trapezoidal Formula
This formula is obtained by replacing f with Π1f, its Lagrange interpolat-
ing polynomial of degree 1, relative to the nodes x0 = a and x1 = b (see
Figure 9.2, left). The resulting quadrature, having nodes x0 = a, x1 = b
and weights α0 = α1 = (b −a)/2, is
I1(f) = b −a
2
[f(a) + f(b)] .
(9.11)
If f ∈C2([a, b]), the quadrature error is given by
E1(f) = −h3
12f ′′(ξ),
h = b −a
(9.12)
where ξ is a point within the integration interval.
f(x)
a = x0
b = x1
x
b = x2
a+b
2
= x1
x
f(x)
a = x0
FIGURE 9.2. Trapezoidal formula (left) and Cavalieri-Simpson formula (right)

376
9. Numerical Integration
Indeed, from the expression of the interpolation error (8.7) one gets
E1(f) =
b
>
a
(f(x) −Π1f(x))dx = −1
2
b
>
a
f ′′(ξ(x))(x −a)(b −x)dx.
Since ω2(x) = (x −a)(x −b) < 0 in (a, b), the mean-value theorem yields
E1(f) = (1/2)f ′′(ξ)
b
>
a
ω2(x)dx = −f ′′(ξ)(b −a)3/12,
for some ξ ∈(a, b), which is (9.12). The trapezoidal quadrature therefore
has degree of exactness equal to 1, as is the case with the midpoint rule.
To obtain the composite trapezoidal formula, we proceed as in the case
where n = 0, by replacing f over [a, b] with its composite Lagrange polyno-
mial of degree 1 on m subintervals, with m ≥1. Introduce the quadrature
nodes xk = a + kH, for k = 0, . . . , m and H = (b −a)/m, getting
I1,m(f) = H
2
m−1

k=0
(f(xk) + f(xk+1)) ,
m ≥1.
(9.13)
Each term in (9.13) is counted twice, except the ﬁrst and the last one, so
that the formula can be written as
I1,m(f) = H
1
2f(x0) + f(x1) + . . . + f(xm−1) + 1
2f(xm)

.
(9.14)
As was done for (9.8), it can be shown that the quadrature error associated
with (9.14) is
E1,m(f) = −b −a
12 H2f ′′(ξ),
provided that f ∈C2([a, b]), where ξ ∈(a, b). The degree of exactness is
again equal to 1.
The composite trapezoidal rule is implemented in Program 72.
Program 72 - trapezc : Composite trapezoidal formula
function int = trapezc(a,b,m,fun)
h=(b-a)/m; x=[a:h:b]; dim = max(size(x)); y=eval(fun);
if size(y)==1, y=diag(ones(dim))*y; end;
int=h*(0.5*y(1)+sum(y(2:m))+0.5*y(m+1));

9.2 Interpolatory Quadratures
377
9.2.3
The Cavalieri-Simpson Formula
The Cavalieri-Simpson formula can be obtained by replacing f over [a, b]
with its interpolating polynomial of degree 2 at the nodes x0 = a, x1 =
(a + b)/2 and x2 = b (see Figure 9.2, right). The weights are given by
α0 = α2 = (b −a)/6 and α1 = 4(b −a)/6, and the resulting formula reads
I2(f) = b −a
6

f(a) + 4f
a + b
2

+ f(b)

.
(9.15)
It can be shown that the quadrature error is
E2(f) = −h5
90f (4)(ξ),
h = b −a
2
(9.16)
provided that f ∈C4([a, b]), and where ξ lies within (a, b). From (9.16) it
turns out that (9.15) has degree of exactness equal to 3.
Replacing f with its composite polynomial of degree 2 over [a, b] yields
the composite formula corresponding to (9.15). Introducing the quadrature
nodes xk = a + kH/2, for k = 0, . . . , 2m and letting H = (b −a)/m, with
m ≥1 gives
I2,m = H
6
.
f(x0) + 2
m−1

r=1
f(x2r) + 4
m−1

s=0
f(x2s+1) + f(x2m)
/
.
(9.17)
The quadrature error associated with (9.17) is
E2,m(f) = −b −a
180 (H/2)4f (4)(ξ),
provided that f ∈C4([a, b]) and where ξ ∈(a, b); the degree of exactness
of the formula is 3.
The composite Cavalieri-Simpson quadrature is implemented in Program
73.
Program 73 - simpsonc : Composite Cavalieri-Simpson formula
function int = simpsonc(a,b,m,fun)
h=(b-a)/m; x=[a:h/2:b]; dim = max(size(x)); y=eval(fun);
if size(y)==1, y=diag(ones(dim))*y; end;
int=(h/6)*(y(1)+2*sum(y(3:2:2*m-1))+4*sum(y(2:2:2*m))+y(2*m+1));
Example 9.1 Let us employ the midpoint, trapezoidal and Cavalieri-Simpson
composite formulae to compute the integral
2π
>
0
xe−x cos(2x)dx =
0
3(e−2π −1) −10πe−2π1
25
≃−0.122122.
(9.18)

378
9. Numerical Integration
Table 9.1 shows in even columns the behavior of the absolute value of the er-
ror when halving H (thus, doubling m), while in odd columns the ratio Rm =
|Em|/|E2m| between two consecutive errors is given. As predicted by the previous
theoretical analysis, Rm tends to 4 for the midpoint and trapezoidal rules and
to 16 for the Cavalieri-Simpson formula.
•
m
|E0,m|
Rm
|E1,m|
Rm
|E2,m|
Rm
1
0.9751
1.589e-01
7.030e-01
2
1.037
0.9406
0.5670
0.2804
0.5021
1.400
4
0.1221
8.489
0.2348
2.415
3.139 · 10−3
159.96
8
2.980 · 10−2
4.097
5.635 · 10−2
4.167
1.085 · 10−3
2.892
16
6.748 · 10−3
4.417
1.327 · 10−2
4.245
7.381 · 10−5
14.704
32
1.639 · 10−3
4.118
3.263 · 10−3
4.068
4.682 · 10−6
15.765
64
4.066 · 10−4
4.030
8.123 · 10−4
4.017
2.936 · 10−7
15.946
128
1.014 · 10−4
4.008
2.028 · 10−4
4.004
1.836 · 10−8
15.987
256
2.535 · 10−5
4.002
5.070 · 10−5
4.001
1.148 · 10−9
15.997
TABLE 9.1. Absolute error for midpoint, trapezoidal and Cavalieri-Simpson com-
posite formulae in the approximate evaluation of integral (9.18)
9.3
Newton-Cotes Formulae
These formulae are based on Lagrange interpolation with equally spaced
nodes in [a, b]. For a ﬁxed n ≥0, let us denote the quadrature nodes
by xk = x0 + kh, k = 0, . . . , n. The midpoint, trapezoidal and Simpson
formulae are special instances of the Newton-Cotes formulae, taking n = 0,
n = 1 and n = 2 respectively. In the general case, we deﬁne:
- closed formulae, those where x0 = a, xn = b and h = b −a
n
(n ≥1);
- open formulae, those where x0 = a+h, xn = b−h and h = b −a
n + 2 (n ≥0).
A signiﬁcant property of the Newton-Cotes formulae is that the quadra-
ture weights αi depend explicitly only on n and h, but not on the integration
interval [a, b]. To check this property in the case of closed formulae, let us
introduce the change of variable x = Ψ(t) = x0 +th. Noting that Ψ(0) = a,
Ψ(n) = b and xk = a + kh, we get
x −xk
xi −xk
= a + th −(a + kh)
a + ih −(a + kh) = t −k
i −k .
Therefore, if n ≥1
li(x) =
n

k=0,k̸=i
t −k
i −k = ϕi(t),
0 ≤i ≤n.

9.3 Newton-Cotes Formulae
379
The following expression for the quadrature weights is obtained
αi =
b
>
a
li(x)dx =
n
>
0
ϕi(t)hdt = h
n
>
0
ϕi(t)dt,
from which we get the formula
In(f) = h
n

i=0
wif(xi),
wi =
n
>
0
ϕi(t)dt.
Open formulae can be interpreted in a similar manner. Actually, using again
the mapping x = Ψ(t), we get x0 = a+h, xn = b−h and xk = a+h(k +1)
for k = 1, . . . , n −1. Letting, for sake of coherence, x−1 = a, xn+1 = b and
proceeding as in the case of closed formulae, we get αi = h
 n+1
−1
ϕi(t)dt,
and thus
In(f) = h
n

i=0
wif(xi),
wi =
n+1
>
−1
ϕi(t)dt.
In the special case where n = 0, since l0(x) = ϕ0(t) = 1, we get w0 = 2.
The coeﬃcients wi do not depend on a, b, h and f, but only depend on n,
and can therefore be tabulated a priori. In the case of closed formulae, the
polynomials ϕi and ϕn−i, for i = 0, . . . , n −1, have by symmetry the same
integral, so that also the corresponding weights wi and wn−i are equal for
i = 0, . . . , n−1. In the case of open formulae, the weights wi and wn−i are
equal for i = 0, . . . , n. For this reason, we show in Table 9.2 only the ﬁrst
half of the weights.
Notice the presence of negative weights in open formulae for n ≥2. This can
be a source of numerical instability, in particular due to rounding errors.
n
1
2
3
4
5
6
w0
1
2
1
3
3
8
14
45
95
288
41
140
w1
0
4
3
9
8
64
45
375
288
216
140
w2
0
0
0
24
45
250
288
27
140
w3
0
0
0
0
0
272
140
n
0
1
2
3
4
5
w0
2
3
2
8
3
55
24
66
20
4277
1440
w1
0
0
−4
3
5
24
−84
20
−3171
1440
w2
0
0
0
0
156
20
3934
1440
TABLE 9.2. Weights of closed (left) and open Newton-Cotes formulae (right)
Besides its degree of exactness, a quadrature formula can also be qualiﬁed
by its order of inﬁnitesimal with respect to the integration stepsize h, which
is deﬁned as the maximum integer p such that |I(f) −In(f)| = O(hp).
Regarding this, the following result holds

380
9. Numerical Integration
Theorem 9.2 For any Newton-Cotes formula corresponding to an even
value of n, the following error characterization holds
En(f) =
Mn
(n + 2)!hn+3f (n+2)(ξ),
(9.19)
provided f ∈Cn+2([a, b]), where ξ ∈(a, b) and
Mn =















n
>
0
t πn+1(t)dt < 0
for closed formulae,
n+1
>
−1
t πn+1(t)dt > 0
for open formulae,
having deﬁned πn+1(t) = Bn
i=0(t −i). From (9.19), it turns out that the
degree of exactness is equal to n + 1 and the order of inﬁnitesimal is n + 3.
Similarly, for odd values of n, the following error characterization holds
En(f) =
Kn
(n + 1)!hn+2f (n+1)(η),
(9.20)
provided f ∈Cn+1([a, b]), where η ∈(a, b) and
Kn =















n
>
0
πn+1(t)dt < 0
for closed formulae,
n+1
>
−1
πn+1(t)dt > 0
for open formulae.
The degree of exactness is thus equal to n and the order of inﬁnitesimal is
n + 2.
Proof. We give a proof in the particular case of closed formulae with n even,
referring to [IK66], pp. 308-314, for a complete demonstration of the theorem.
Thanks to (8.20), we have
En(f) = I(f) −In(f) =
b
>
a
f[x0, . . . , xn, x]ωn+1(x)dx.
(9.21)
Set W(x) =
 x
a ωn+1(t)dt. Clearly, W(a) = 0; moreover, ωn+1(t) is an odd func-
tion with respect to the midpoint (a + b)/2 so that W(b) = 0. Integrating by

9.3 Newton-Cotes Formulae
381
parts (9.21) we get
En(f)
=
b
>
a
f[x0, . . . , xn, x]W ′(x)dx = −
b
>
a
d
dxf[x0, . . . , xn, x]W(x)dx
=
−
b
>
a
f (n+2)(ξ(x))
(n + 2)!
W(x)dx.
In deriving the formula above we have used the following identity (see Exercise
4)
d
dxf[x0, . . . , xn, x] = f[x0, . . . , xn, x, x].
(9.22)
Since W(x) > 0 for a < x < b (see [IK66], p. 309), using the mean-value theorem
we obtain
En(f) = −f (n+2)(ξ)
(n + 2)!
b
>
a
W(x)dx = −f (n+2)(ξ)
(n + 2)!
b
>
a
x
>
a
ωn+1(t) dt dx
(9.23)
where ξ lies within (a, b). Exchanging the order of integration, letting s = x0+τh,
for 0 ≤τ ≤n, and recalling that a = x0, b = xn, yields
b
>
a
W(x)dx
=
b
>
a
b
>
s
(s −x0) . . . (s −xn)dxds
=
xn
>
x0
(s −x0) . . . (s −xn−1)(s −xn)(xn −s)ds
=
−hn+3
n
>
0
τ(τ −1) . . . (τ −n + 1)(τ −n)2dτ.
Finally, letting t = n −τ and combining this result with (9.23), we get (9.19). 3
Relations (9.19) and (9.20) are a priori estimates for the quadrature error
(see Chapter 2, Section 2.3). Their use in generating a posteriori estimates
of the error in the frame of adaptive algorithms will be examined in Section
9.7.
In the case of closed Newton-Cotes formulae, we show in Table 9.3, for
1 ≤n ≤6, the degree of exactness (that we denote henceforth by rn) and
the absolute value of the constant Mn = Mn/(n + 2)! (if n is even) or
Kn = Kn/(n + 1)! (if n is odd).
Example 9.2 The purpose of this example is to assess the importance of the
regularity assumption on f for the error estimates (9.19) and (9.20). Consider
the closed Newton-Cotes formulae, for 1 ≤n ≤6, to approximate the integral
 1
0 x5/2dx = 2/7 ≃0.2857. Since f is only C2([0, 1]), we do not expect a substan-
tial increase of the accuracy as n gets larger. Actually, this is conﬁrmed by Table
9.4, where the results obtained by running Program 74 are reported.

382
9. Numerical Integration
n
rn
Mn
Kn
n
rn
Mn
Kn
n
rn
Mn
Kn
1
1
1
12
3
3
3
80
5
5
275
12096
2
3
1
90
4
5
8
945
6
7
9
1400
TABLE 9.3. Degree of exactness and error constants for closed Newton-Cotes
formulae
For n = 1, . . . , 6, we have denoted by Ec
n(f) the module of the absolute error,
by qc
n the computed order of inﬁnitesimal and by qs
n the corresponding theoretical
value predicted by (9.19) and (9.20) under optimal regularity assumptions for f.
As is clearly seen, qc
n is deﬁnitely less than the potential theoretical value qs
n. •
n
Ec
n(f)
qc
n
qs
n
n
Ec
n(f)
qc
n
qs
n
1
0.2143
3
3
4
5.009 · 10−5
4.7
7
2
1.196 · 10−3
3.2
5
5
3.189 · 10−5
2.6
7
3
5.753 · 10−4
3.8
5
6
7.857 · 10−6
3.7
9
TABLE 9.4. Error in the approximation of
 1
0 x5/2dx
Example 9.3 From a brief analysis of error estimates (9.19) and (9.20), we could
be led to believe that only non-smooth functions can be a source of trouble when
dealing with Newton-Cotes formulae. Thus, it is a little surprising to see results
like those in Table 9.5, concerning the approximation of the integral
I(f) =
5
>
−5
1
1 + x2 dx = 2 arctan 5 ≃2.747,
(9.24)
where f(x) = 1/(1+x2) is Runge’s function (see Section 8.1.2), which belongs to
C∞(R). The results clearly demonstrate that the error remains almost unchanged
as n grows. This is due to the fact that singularities on the imaginary axis may
also aﬀect the convergence properties of a quadrature formula. This is indeed the
case with the function at hand, which exhibits two singularities at ±√−1 (see
[DR75], pp. 64-66).
•
n
En(f)
n
En(f)
n
En(f)
1
0.8601
3
0.2422
5
0.1599
2
-1.474
4
0.1357
6
-0.4091
TABLE 9.5. Relative error En(f) = [I(f) −In(f)]/In(f) in the approximate
evaluation of (9.24) using closed Newton-Cotes formulae
To increase the accuracy of an interpolatory quadrature rule, it is by
no means convenient to increase the value of n. By doing so, the same

9.4 Composite Newton-Cotes Formulae
383
drawbacks of Lagrange interpolation on equally spaced nodes would arise.
For example, the weights of the closed Newton-Cotes formula with n = 8
do not have the same sign (see Table 9.6 and recall that wi = wn−i for
i = 0, . . . , n −1).
n
w0
w1
w2
w3
w4
rn
Mn
8
3956
14175
23552
14175
−3712
14175
41984
14175
−18160
14175
9
2368
467775
TABLE 9.6. Weights of the closed Newton-Cotes formula with 9 nodes
This can give rise to numerical instabilities, due to rounding errors (see
Chapter 2), and makes this formula useless in the practice, as happens for
all the Newton-Cotes formulae using more than 8 nodes. As an alternative,
one can resort to composite formulae, whose error analysis is addressed in
Section 9.4, or to Gaussian formulae, which will be dealt with in Chapter
10 and which yield maximum degree of exactness with a non equally spaced
nodes distribution.
The closed Newton-Cotes formulae, for 1 ≤n ≤6, are implemented in
Program 74.
Program 74 - newtcot : Closed Newton-Cotes formulae
function int = newtcot(a,b,n,fun)
h=(b-a)/n; n2=ﬁx(n/2);
if n > 6, disp(’maximum value of n equal to 6 ’); return; end
a03=1/3; a08=1/8; a45=1/45; a288=1/288; a140=1/140;
alpha=[0.5
0
0
0; ...
a03
4*a03
0
0; ...
3*a08
9*a08
0
0; ...
14*a45 64*a45
24*a45
0; ...
95*a288 375*a288 250*a288 0; ...
41*a140 216*a140 27*a140 272*a140];
x=a; y(1)=eval(fun);
for j=2:n+1,
x=x+h; y(j)=eval(f); end;
int=0;
for j=1:n2+1,
int=int+y(j)*alpha(n,j);
end;
for j=n2+2:n+1, int=int+y(j)*alpha(n,n-j+2); end; int=int*h;
9.4
Composite Newton-Cotes Formulae
The examples of Section 9.2 have already pointed out that composite
Newton-Cotes formulae can be constructed by replacing f with its com-
posite Lagrange interpolating polynomial, introduced in Section 8.1.

384
9. Numerical Integration
The general procedure consists of partitioning the integration interval
[a, b] into m subintervals Tj = [yj, yj+1] such that yj = a + jH, where H =
(b −a)/m for j = 0, . . . , m. Then, over each subinterval, an interpolatory
formula with nodes {x(j)
k , 0 ≤k ≤n} and weights {α(j)
k , 0 ≤k ≤n} is
used. Since
I(f) =
b
>
a
f(x)dx =
m−1

j=0
>
Tj
f(x)dx,
a composite interpolatory quadrature formula is obtained by replacing I(f)
with
In,m(f) =
m−1

j=0
n

k=0
α(j)
k f(x(j)
k ).
(9.25)
The quadrature error is deﬁned as En,m(f) = I(f)−In,m(f). In particular,
over each subinterval Tj one can resort to a Newton-Cotes formula with
n + 1 equally spaced nodes: in such a case, the weights α(j)
k
= hwk are still
independent of Tj.
Using the same notation as in Theorem 9.2, the following convergence
result holds for composite formulae.
Theorem 9.3 Let a composite Newton-Cotes formula, with n even, be
used. If f ∈Cn+2([a, b]), then
En,m(f) =
b −a
(n + 2)!
Mn
(n + 2)n+3 Hn+2f (n+2)(ξ)
(9.26)
where ξ ∈(a, b). Therefore, the quadrature error is an inﬁnitesimal in H
of order n + 2 and the formula has degree of exactness equal to n + 1.
For a composite Newton-Cotes formula, with n odd, if f ∈Cn+1([a, b])
En,m(f) =
b −a
(n + 1)!
Kn
nn+2 Hn+1f (n+1)(η)
(9.27)
where η ∈(a, b). Thus, the quadrature error is an inﬁnitesimal in H of
order n + 1 and the formula has degree of exactness equal to n.
Proof. We only consider the case where n is even. Using (9.19), and noticing
that Mn does not depend on the integration interval, we get
En,m(f) =
m−1

j=0
0
I(f)|Tj −In(f)|Tj
1
=
Mn
(n + 2)!
m−1

j=0
hn+3
j
f (n+2)(ξj),
where, for j = 0, . . . , (m −1), hj = |Tj|/(n + 2) = (b −a)/(m(n + 2)); this time,
ξj is a suitable point of Tj. Since (b −a)/m = H, we obtain
En,m(f) =
Mn
(n + 2)!
b −a
m(n + 2)n+3 Hn+2
m−1

j=0
f (n+2)(ξj),

9.4 Composite Newton-Cotes Formulae
385
from which, applying Theorem 9.1 with u(x) = f (n+2)(x) and δj = 1 for j =
0, . . . , m −1, (9.26) immediately follows. A similar procedure can be followed to
prove (9.27).
3
We notice that, for n ﬁxed, En,m(f) →0 as m →∞(i.e., as H →0).
This ensures the convergence of the numerical integral to the exact value
I(f). We notice also that the degree of exactness of composite formulae
coincides with that of simple formulae, whereas its order of inﬁnitesimal
(with respect to H) is reduced by 1 with respect to the order of inﬁnitesimal
(in h) of simple formulae.
In practical computations, it is convenient to resort to a local interpolation
of low degree (typically n ≤2, as done in Section 9.2), this leads to com-
posite quadrature rules with positive weights, with a minimization of the
rounding errors.
Example 9.4 For the same integral (9.24) considered in Example 9.3, we show
in Table 9.7 the behavior of the absolute error as a function of the number of
subintervals m, in the case of the composite midpoint, trapezoidal and Cavalieri-
Simpson formulae. Convergence of In,m(f) to I(f) as m increases can be clearly
observed. Moreover, we notice that E0,m(f) ≃E1,m(f)/2 for m ≥32 (see Exer-
cise 1).
m
|E0,m|
|E1,m|
|E2,m|
1
7.253
2.362
4.04
2
1.367
2.445
9.65 · 10−2
8
3.90 · 10−2
3.77 · 10−2
1.35 · 10−2
32
1.20 · 10−4
2.40 · 10−4
4.55 · 10−8
128
7.52 · 10−6
1.50 · 10−5
1.63 · 10−10
512
4.70 · 10−7
9.40 · 10−7
6.36 · 10−13
TABLE 9.7. Absolute error for composite quadratures in the computation of
(9.24)
•
Convergence of In,m(f) to I(f) can be established under less stringent
regularity assumptions on f than those required by Theorem 9.3. In this
regard, the following result holds (see for the proof [IK66], pp. 341-343).
Property 9.1 Let f ∈C0([a, b]) and assume that the weights α(j)
k
in (9.25)
are nonnegative. Then
lim
m→∞In,m(f) =
> b
a
f(x)dx,
∀n ≥0.
Moreover

> b
a
f(x)dx −In,m(f)
 ≤2(b −a)Ω(f; H),

386
9. Numerical Integration
where
Ω(f; H) = sup{|f(x) −f(y)|, x, y ∈[a, b], x ̸= y, |x −y| ≤H}
is the module of continuity of function f.
9.5
Hermite Quadrature Formulae
Thus far we have considered quadrature formulae based on Lagrange inter-
polation (simple or composite). More accurate formulae can be devised by
resorting to Hermite interpolation (see Section 8.4).
Suppose that 2(n + 1) values f(xk), f ′(xk) are available at n + 1 distinct
points x0, . . . , xn, then the Hermite interpolating polynomial of f is given
by
H2n+1f(x) =
n

i=0
f(xi)Li(x) +
n

i=0
f ′(xi)Mi(x),
(9.28)
where the polynomials Lk, Mk ∈P2n+1 are deﬁned, for k = 0, . . . , n, as
Lk(x) =

1 −ω′′
n+1(xk)
ω′
n+1(xk)(x −xk)

l2
k(x),
Mk(x) = (x −xk)l2
k(x).
Integrating (9.28) over [a, b], we get the quadrature formula of type (9.4)
In(f) =
n

k=0
αkf(xk) +
n

k=0
βkf ′(xk)
(9.29)
where
αk = I(Lk),
βk = I(Mk),
k = 0, . . . , n.
Formula (9.29) has degree of exactness equal to 2n + 1. Taking n = 1, the
so-called corrected trapezoidal formula is obtained
Icorr
1
(f) = b −a
2
[f(a) + f(b)] + (b −a)2
12
[f ′(a) −f ′(b)]
(9.30)
with weights α0 = α1 = (b−a)/2, β0 = (b−a)2/12 and β1 = −β0. Assuming
f ∈C4([a, b]), the quadrature error associated with (9.30) is
Ecorr
1
(f) = h5
720f (4)(ξ),
h = b −a
(9.31)
with ξ ∈(a, b). Notice the increase of accuracy from O(h3) to O(h5) with
respect to the corresponding expression (9.12) (of the same order as the

9.6 Richardson Extrapolation
387
Cavalieri-Simpson formula (9.15)). The composite formula can be generated
in a similar manner
Icorr
1,m (f) = b −a
m
%1
2 [f(x0) + f(xm)]
+f(x1) + . . . + f(xm−1)} + (b −a)2
12
[f ′(a) −f ′(b)] ,
(9.32)
where the assumption that f ∈C1([a, b]) gives rise to the cancellation of
the ﬁrst derivatives at the nodes xk, with k = 1, . . . , m −1.
Example 9.5 Let us check experimentally the error estimate (9.31) in the simple
(m = 1) and composite (m > 1) cases, running Program 75 for the approximate
computation of integral (9.18). Table 9.8 reports the behavior of the module of
the absolute error as H is halved (that is, m is doubled) and the ratio Rm between
two consecutive errors. This ratio, as happens in the case of Cavalieri-Simpson
formula, tends to 16, demonstrating that formula (9.32) has order of inﬁnitesimal
equal to 4. Comparing Table 9.8 with the corresponding Table 9.1, we can also
notice that |Ecorr
1,m (f)| ≃4|E2,m(f)| (see Exercise 9).
•
m
Ecorr
1,m (f)
Rm
m
Ecorr
1,m (f)
Rm
m
Ecorr
1,m (f)
Rm
1
3.4813
8
4.4 · 10−3
6.1
64
1.1 · 10−6
15.957
2
1.398
2.4
16
2.9 · 10−4
14.9
128
7.3 · 10−8
15.990
4
2.72 · 10−2
51.4
32
1.8 · 10−5
15.8
256
4.5 · 10−9
15.997
TABLE 9.8. Absolute error for the corrected trapezoidal formula in the compu-
tation of I(f) =
 2π
0
xe−x cos(2x)dx
The corrected composite trapezoidal quadrature is implemented in Pro-
gram 75, where dfun contains the expression of the derivative of f.
Program 75 - trapmodc : Composite corrected trapezoidal formula
function int = trapmodc(a,b,m,fun,dfun)
h=(b-a)/m; x=[a:h:b]; y=eval(fun);
f1a=feval(dfun,a); f1b=feval(dfun,b);
int=h*(0.5*y(1)+sum(y(2:m))+0.5*y(m+1))+(hˆ2/12)*(f1a-f1b);
9.6
Richardson Extrapolation
The Richardson extrapolation method is a procedure which combines several
approximations of a certain quantity α0 in a smart way to yield a more
accurate approximation of α0. More precisely, assume that a method is
available to approximate α0 by a quantity A(h) that is computable for any

388
9. Numerical Integration
value of the parameter h ̸= 0. Moreover, assume that, for a suitable k ≥0,
A(h) can be expanded as follows
A(h) = α0 + α1h + . . . + αkhk + Rk+1(h),
(9.33)
where |Rk+1(h)| ≤Ck+1hk+1. The constants Ck+1 and the coeﬃcients αi,
for i = 0, . . . , k, are independent of h. Henceforth, α0 = limh→0 A(h).
Writing (9.33) with δh instead of h, for 0 < δ < 1 (typically, δ = 1/2),
we get
A(δh) = α0 + α1(δh) + . . . + αk(δh)k + Rk+1(δh).
Subtracting (9.33) multiplied by δ from this expression then yields
B(h) = A(δh) −δA(h)
1 −δ
= α0 + $α2h2 + . . . + $αkhk + $Rk+1(h),
having deﬁned, for k ≥2, $αi = αi(δi −δ)/(1 −δ), for i = 2, . . . , k and
$Rk+1(h) = [Rk+1(δh) −δRk+1(h)] /(1 −δ).
Notice that $αi ̸= 0 iﬀαi ̸= 0. In particular, if α1 ̸= 0, then A(h) is a ﬁrst-
order approximation of α0, while B(h) is at least second-order accurate.
More generally, if A(h) is an approximation of α0 of order p, then the
quantity B(h) = [A(δh) −δpA(h)] /(1 −δp) approximates α0 up to order
p + 1 (at least).
Proceeding by induction, the following Richardson extrapolation algorithm
is generated: setting n ≥0, h > 0 and δ ∈(0, 1), we construct the sequences
Am,0 = A(δmh),
m = 0, . . . , n,
Am,q+1 = Am,q −δq+1Am−1,q
1 −δq+1
,
q = 0, . . . , n −1,
m = q + 1, . . . , n,
(9.34)
which can be represented by the diagram below
A0,0
↘
A1,0
→
A1,1
↘
↘
A2,0
→
A2,1
→
A2,2
↘
↘
↘
A3,0
→
A3,1
→
A3,2
→
A3,3
↘
↘
↘
↘
...
...
...
...
...
↘
↘
↘
↘
An,0
→
An,1
→
An,2
→
An,3
. . .
→
An,n
where the arrows indicate the way the terms which have been already
computed contribute to the construction of the “new” ones.
The following result can be proved (see [Com95], Proposition 4.1).

9.6 Richardson Extrapolation
389
Property 9.2 For n ≥0 and δ ∈(0, 1)
Am,n = α0 + O((δmh)n+1),
m = 0, . . . , n.
(9.35)
In particular, for the terms in the ﬁrst column (n = 0) the convergence
rate to α0 is O((δmh)), while for those of the last one it is O((δmh)n+1),
i.e., n times higher.
Example 9.6 Richardson extrapolation has been employed to approximate at
x = 0 the derivative of the function f(x) = xe−x cos(2x), introduced in Ex-
ample 9.1. For this purpose, algorithm (9.34) has been executed with A(h) =
[f(x + h) −f(x)] /h, δ = 0.5, n = 5 and h = 0.1. Table 9.9 reports the sequence
of absolute errors Em,k = |α0 −Am,k|. The results demonstrate that the error
decays as predicted by (9.35).
•
Em,0
Em,1
Em,2
Em,3
Em,4
Em,5
0.113
–
–
–
–
–
5.3 · 10−2
6.1 · 10−3
–
–
–
–
2.6 · 10−2
1.7 · 10−3
2.2 · 10−4
–
–
–
1.3 · 10−2
4.5 · 10−4
2.8 · 10−5
5.5 · 10−7
–
–
6.3 · 10−3
1.1 · 10−4
3.5 · 10−6
3.1 · 10−8
3.0 · 10−9
–
3.1 · 10−3
2.9 · 10−5
4.5 · 10−7
1.9 · 10−9
9.9 · 10−11
4.9 · 10−12
TABLE 9.9. Errors in the Richardson extrapolation for the approximate evalua-
tion of f ′(0) where f(x) = xe−x cos(2x)
9.6.1
Romberg Integration
The Romberg integration method is an application of Richardson extrap-
olation to the composite trapezoidal rule. The following result, known as
the Euler-MacLaurin formula, will be useful (for its proof see, e.g., [Ral65],
pp. 131-133, and [DR75], pp. 106-111).
Property 9.3 Let f ∈C2k+2([a, b]), for k ≥0, and let us approximate
α0 =
 b
a f(x)dx by the composite trapezoidal rule (9.14). Letting hm =
(b −a)/m for m ≥1,
I1,m(f) = α0
+
k

i=1
B2i
(2i)!h2i
m
+
f (2i−1)(b) −f (2i−1)(a)
,
+ B2k+2
(2k + 2)!h2k+2
m
(b −a)f (2k+2)(η),
(9.36)
where η ∈(a, b) and B2j = (−1)j−1
.+∞

n=1
2/(2nπ)2j
/
(2j)!, for j ≥1, are
the Bernoulli numbers.

390
9. Numerical Integration
Equation (9.36) is a special case of (9.33) where h = h2
m and A(h) =
I1,m(f); notice that only even powers of the parameter h appear in the
expansion.
The Richardson extrapolation algorithm (9.34) applied to (9.36) gives
Am,0 = A(δmh),
m = 0, . . . , n,
Am,q+1 = Am,q −δ2(q+1)Am−1,q
1 −δ2(q+1)
,
q = 0, . . . , n −1,
m = q + 1, . . . , n.
(9.37)
Setting h = b −a and δ = 1/2 into (9.37) and denoting by T(hs) = I1,s(f)
the composite trapezoidal formula (9.14) over s = 2m subintervals of width
hs = (b −a)/2m, for m ≥0, the algorithm (9.37) becomes
Am,0 = T((b −a)/2m),
m = 0, . . . , n,
Am,q+1 = 4q+1Am,q −Am−1,q
4q+1 −1
,
q = 0, . . . , n −1,
m = q + 1, . . . , n.
This is the Romberg numerical integration algorithm. Recalling (9.35), the
following convergence result holds for Romberg integration
Am,n =
b
>
a
f(x)dx + O(h2(n+1)
s
),
n ≥0.
Example 9.7 Table 9.10 shows the results obtained by running Program 76 to
compute the quantity α0 in the two cases α(1)
0
=
 π
0 ex cos(x)dx = −(eπ + 1)/2
and α(2)
0
=
 1
0
√xdx = 2/3.
The maximum size n has been set equal to 9. In the second and third columns
we show the modules of the absolute errors E(r)
k
= |α(r)
0
−A(r)
k+1,k+1|, for r = 1, 2
and k = 0, . . . , 6.
The convergence to zero is much faster for E(1)
k
than for E(2)
k . Indeed, the ﬁrst
integrand function is inﬁnitely diﬀerentiable whereas the second is only continu-
ous.
•
k
E(1)
k
E(2)
k
k
E(1)
k
E(2)
k
0
22.71
0.1670
4
8.923 · 10−7
1.074 · 10−3
1
0.4775
2.860 · 10−2
5
6.850 · 10−11
3.790 · 10−4
2
5.926 · 10−2
8.910 · 10−3
6
5.330 · 10−14
1.340 · 10−4
3
7.410 · 10−5
3.060 · 10−3
7
0
4.734 · 10−5
TABLE
9.10.
Romberg
integration
for
the
approximate
evaluation
of
 π
0 ex cos(x)dx (error E(1)
k ) and
 1
0
√xdx (error E(2)
k )
The Romberg algorithm is implemented in Program 76.

9.7 Automatic Integration
391
Program 76 - romberg : Romberg integration
function [A]=romberg(a,b,n,fun);
for i=1:(n+1), A(i,1)=trapezc(a,b,2ˆ(i-1),fun); end;
for j=2:(n+1), for i=j:(n+1),
A(i,j)=(4ˆ(j-1)*A(i,j-1)-A(i-1,j-1))/(4ˆ(j-1)-1); end; end;
9.7
Automatic Integration
An automatic numerical integration program, or automatic integrator, is
a set of algorithms which yield an approximation of the integral I(f) =
 b
a f(x)dx, within a given tolerance, εa, or relative tolerance, εr, prescribed
by the user.
With this aim, the program generates a sequence {Ik, Ek}, for k =
1, . . . , N, where Ik is the approximation of I(f) at the k-th step of the
computational process, Ek is an estimate of the error I(f) −Ik, and is N
a suitable ﬁxed integer.
The sequence terminates at the s-th level, with s ≤N, such that the
automatic integrator fulﬁlls the following requirement on the accuracy
max
2
εa, εr|$I(f)|
3
≥|Es|(≃|I(f) −Is|),
(9.38)
where $I(f) is a reasonable guess of the integral I(f) provided as an input
datum by the user. Otherwise, the integrator returns the last computed
approximation IN, together with a suitable error message that warns the
user of the algorithm’s failure to converge.
Ideally, an automatic integrator should:
(a) provide a reliable criterion for determining |Es| that allows for moni-
toring the convergence check (9.38);
(b) ensure an eﬃcient implementation, which minimizes the number of
functional evaluations for yielding the desired approximation Is.
In computational practice, for each k ≥1, moving from level k to level
k + 1 of the automatic integration process can be done according to two
diﬀerent strategies, which we deﬁne as non adaptive or adaptive.
In the non adaptive case, the law of distribution of the quadrature nodes
is ﬁxed a priori and the quality of the estimate Ik is reﬁned by increas-
ing the number of nodes corresponding to each level of the computational
process. An example of an automatic integrator that is based on such a
procedure is provided by the composite Newton-Cotes formulae on m and

392
9. Numerical Integration
2m subintervals, respectively, at levels k and k + 1, as described in Section
9.7.1.
In the adaptive case, the positions of the nodes is not set a priori, but at
each level k of the process they depend on the information that has been
stored during the previous k −1 levels. An adaptive automatic integration
algorithm is performed by partitioning the interval [a, b] into successive
subdivisions which are characterized by a nonuniform density of the nodes,
this density being typically higher in a neighborhood of strong gradients
or singularities of f. An example of an adaptive integrator based on the
Cavalieri-Simpson formula is described in Section 9.7.2.
9.7.1
Non Adaptive Integration Algorithms
In this section, we employ the composite Newton-Cotes formulae. Our aim
is to devise a criterion for estimating the absolute error |I(f) −Ik| by
using Richardson extrapolation. From (9.26) and (9.27) it turns out that,
for m ≥1 and n ≥0, In,m(f) has order of inﬁnitesimal equal to Hn+p,
with p = 2 for n even and p = 1 for n odd, where m, n and H = (b −a)/m
are the number of partitions of [a, b], the number of quadrature nodes over
each subinterval and the constant length of each subinterval, respectively.
By doubling the value of m (i.e., halving the stepsize H) and proceeding
by extrapolation, we get
I(f) −In,2m(f) ≃
1
2n+p [I(f) −In,m(f)] .
(9.39)
The use of the symbol ≃instead of = is due to the fact that the point ξ
or η, where the derivative in (9.26) and (9.27) must be evaluated, changes
when passing from m to 2m subintervals. Solving (9.39) with respect to
I(f) yields the following absolute error estimate for In,2m(f)
I(f) −In,2m(f) ≃In,2m(f) −In,m(f)
2n+p −1
.
(9.40)
If the composite Simpson rule is considered (i.e., n = 2), (9.40) predicts a
reduction of the absolute error by a factor of 15 when passing from m to 2m
subintervals. Notice also that only 2m−1 extra functional evaluations are
needed to compute the new approximation I1,2m(f) starting from I1,m(f).
Relation (9.40) is an instance of an a posteriori error estimate (see Chapter
2, Section 2.3). It is based on the combined use of an a priori estimate (in
this case, (9.26) or (9.27)) and of two evaluations of the quantity to be ap-
proximated (the integral I(f)) for two diﬀerent values of the discretization
parameter (that is, H = (b −a)/m).

9.7 Automatic Integration
393
Example 9.8 Let us employ the a posteriori estimate (9.40) in the case of the
composite Simpson formula (n = p = 2), for the approximation of the integral
π
>
0
(ex/2 + cos 4x)dx = 2(eπ −1) ≃7.621,
where we require the absolute error to be less than 10−4. For k = 0, 1, . . . , set
hk = (b−a)/2k and denote by I2,m(k)(f) the integral of f which is computed using
the composite Simpson formula on a grid of size hk with m(k) = 2k intervals. We
can thus assume as a conservative estimate of the quadrature error the following
quantity
|Ek| = |I(f) −I2,m(k)(f)| ≃1
10|I2,2m(k)(f) −I2,m(k)(f)| = |Ek|,
k ≥1.
(9.41)
Table 9.11 shows the sequence of the estimated errors |Ek| and of the correspond-
ing absolute errors |Ek| that have been actually made by the numerical integration
process. Notice that, when convergence has been achieved, the error estimated
by (9.41) is deﬁnitely higher than the actual error, due to the conservative choice
above.
•
k
|Ek|
|Ek|
k
|Ek|
|Ek|
0
3.156
2
0.10
4.52 · 10−5
1
0.42
1.047
3
5.8 · 10−6
2 · 10−9
TABLE 9.11. Non adaptive automatic Simpson rule for the approximation of
 π
0 (ex/2 + cos 4x)dx
An alternative approach for fulﬁlling the constraints (a) and (b) con-
sists of employing a nested sequence of special Gaussian quadratures Ik(f)
(see Chapter 10), having increasing degree of exactness for k = 1, . . . , N.
These formulae are constructed in such a way that, denoting by Snk =
{x1, . . . , xnk} the set of quadrature nodes relative to quadrature Ik(f),
Snk ⊂Snk+1 for any k = 1, . . . , N −1. As a result, for k ≥1, the formula
at the k + 1-th level employs all the nodes of the formula at level k and
this makes nested formulae quite eﬀective for computer implementation.
As an example, we recall the Gauss-Kronrod formulae with 10, 21, 43
and 87 points, that are available in [PdK¨UK83] (in this case, N = 4). The
Gauss-Kronrod formulae have degree of exactness rnk (optimal) equal to
2nk−1, where nk is the number of nodes for each formula, with n1 = 10 and
nk+1 = 2nk+1 for k = 1, 2, 3. The criterion for devising an error estimate is
based on comparing the results given by two successive formulae Ink(f) and
Ink+1(f) with k = 1, 2, 3, and then terminating the computational process
at the level k such that (see also [DR75], p. 321)
|Ik+1 −Ik| ≤max {εa, εr|Ik+1|} .

394
9. Numerical Integration
9.7.2
Adaptive Integration Algorithms
The goal of an adaptive integrator is to yield an approximation of I(f)
within a ﬁxed tolerance ε by a non uniform distribution of the integration
stepsize along the interval [a, b]. An optimal algorithm is able to adapt
automatically the choice of the steplength according to the behavior of the
integrand function, by increasing the density of the quadrature nodes where
the function exhibits stronger variations.
In view of describing the method, it is convenient to restrict our attention
to a generic subinterval [α, β] ⊆[a, b]. Recalling the error estimates for the
Newton-Cotes formulae, it turns out that the evaluation of the derivatives
of f, up to a certain order, is needed to set a stepsize h such that a ﬁxed
accuracy is ensured, say ε(β−α)/(b−a). This procedure, which is unfeasible
in practical computations, is carried out by an automatic integrator as
follows. We consider throughout this section the Cavalieri-Simpson formula
(9.15), although the method can be extended to other quadrature rules.
Set If(α, β) =
 β
α f(x)dx, h = h0 = (β −α)/2 and
Sf(α, β) = (h0/3) [f(α) + 4f(α + h0) + f(β)] .
From (9.16) we get
If(α, β) −Sf(α, β) = −h5
0
90f (4)(ξ),
(9.42)
where ξ is a point in (α, β). To estimate the error If(α, β) −Sf(α, β)
without using explicitly the function f (4) we employ again the Cavalieri-
Simpson formula over the union of the two subintervals [α, (α + β)/2] and
[(α + β)/2, β], obtaining, for h = h0/2 = (β −α)/4
If(α, β) −Sf,2(α, β) = −(h0/2)5
90
+
f (4)(ξ) + f (4)(η)
,
,
where ξ ∈(α, (α + β)/2), η ∈((α + β)/2, β) and Sf,2(α, β) = Sf(α, (α +
β)/2) + Sf((α + β)/2, β).
Let us now make the assumption that f (4)(ξ) ≃f (4)(η) (which is true,
in general, only if the function f (4) does not vary “too much” on [α, β]).
Then,
If(α, β) −Sf,2(α, β) ≃−1
16
h5
0
90f (4)(ξ),
(9.43)
with a reduction of the error by a factor 16 with respect to (9.42), corre-
sponding to the choice of a steplength of doubled size. Comparing (9.42)
and (9.43), we get the estimate
h5
0
90f (4)(ξ) ≃16
15Ef(α, β),

9.7 Automatic Integration
395
where Ef(α, β) = Sf(α, β) −Sf,2(α, β). Then, from (9.43), we have
|If(α, β) −Sf,2(α, β)| ≃|Ef(α, β)|
15
.
(9.44)
We have thus obtained a formula that allows for easily computing the error
made by using composite Cavalieri-Simpson numerical integration on the
generic interval [α, β]. Relation (9.44), as well as (9.40), is another instance
of an a posteriori error estimate. It combines the use of an a priori es-
timate (in this case, (9.16)) and of two evaluations of the quantity to be
approximated (the integral I(f)) for two diﬀerent values of the discretiza-
tion parameter h.
In the practice, it might be convenient to assume a more conservative
error estimate, precisely
|If(α, β) −Sf,2(α, β)| ≃|Ef(α, β)|/10.
Moreover, to ensure a global accuracy on [a, b] equal to the ﬁxed tolerance
ε, it will suﬃce to enforce that the error Ef(α, β) satisﬁes on each single
subinterval [α, β] ⊆[a, b] the following constraint
|Ef(α, β)|
10
≤εβ −α
b −a .
(9.45)
The adaptive automatic integration algorithm can be described as follows.
Denote by:
1. A: the active integration interval, i.e., the interval where the integral
is being computed;
2. S: the integration interval already examined, for which the error test
(9.45) has been successfully passed;
3. N: the integration interval yet to be examined.
At the beginning of the integration process we have N = [a, b], A = N
and S = ∅, while the situation at the generic step of the algorithm is
depicted in Figure 9.3. Set JS(f) ≃
 α
a f(x)dx, with JS(f) = 0 at the
beginning of the process; if the algorithm successfully terminates, JS(f)
yields the desired approximation of I(f). We also denote by J(α,β)(f) the
approximate integral of f over the “active” interval [α, β]. This interval
is drawn in bold in Figure 9.3. At each step of the adaptive integration
method the following decisions are taken:
1. if the local error test (9.45) is passed, then:
(i) JS(f) is increased by J(α,β)(f), that is, JS(f) ←JS(f)+J(α,β)(f);
(ii) we let S ←S ∪A, A = N (corresponding to the path (I) in
Figure 9.3), β = b.

396
9. Numerical Integration
a
α′
b
b
b
α
β
S
A
N
(I)
S
a
a
α
A
α
S
A
N
(II)
FIGURE 9.3. Distribution of the integration intervals at the generic step of the
adaptive algorithm and updating of the integration grid
2. If the local error test (9.45) fails, then:
(j) A is halved, and the new active interval is set to A = [α, α′] with
α′ = (α + β)/2 (corresponding to the path (II) in Figure 9.3);
(jj) we let N ←N ∪[α′, β], β ←α′;
(jjj) a new error estimate is provided.
In order to prevent the algorithm from generating too small stepsizes, it
is convenient to monitor the width of A and warn the user, in case of
an excessive reduction of the steplength, about the presence of a possible
singularity in the integrand function (see Section 9.8).
Example 9.9 Let us employ Cavalieri-Simpson adaptive integration for com-
puting the integral
I(f)
=
4
>
−3
tan−1(10x)dx
= 4tan−1(40) + 3tan−1(−30) −(1/20) log(16/9) ≃1.54201193.
Running Program 77 with tol = 10−4 and hmin = 10−3 yields an approximation
of the integral with an absolute error of 2.104 · 10−5. The algorithm performs
77 functional evaluations, corresponding to partitioning the interval [a, b] into
38 nonuniform subintervals. We notice that the corresponding composite formula
with uniform stepsize would have required 128 subintervals with an absolute error
of 2.413 · 10−5.
In Figure 9.4 (left) we show, together with the plot of the integrand function,
the distribution of the quadrature nodes as a function of x, while on the right
the integration step density (piecewise constant) ∆h(x) is shown, deﬁned as the
inverse of the step size h over each active interval A. Notice the high value attained
by ∆h at x = 0, where the derivative of the integrand function is maximum.
•
The adaptive algorithm described above is implemented in Program 77.
Among the input parameters, hmin is the minimum admissible value of
the integration steplength. In output the program returns the approximate

9.7 Automatic Integration
397
−3
−2
−1
0
1
2
3
4
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
−3
−2
−1
0
1
2
3
4
0
10
20
30
40
50
60
70
80
FIGURE 9.4. Distribution of quadrature nodes (left); density of the integration
stepsize in the approximation of the integral of Example 9.9 (right)
value of the integral integ, the total number of functional evaluations nfv
and the set of integration points xfv.
Program 77 - simpadpt : Adaptive Cavalieri-Simpson formula
function [integ,xfv,nfv]=simpadpt(a,b,tol,fun,hmin);
integ=0; level=0; i=1; alfa(i)=a; beta(i)=b;
step=(beta(i)-alfa(i))/4; nfv=0;
for k=1:5, x=a+(k-1)*step; f(i,k)=eval(fun); nfv=nfv+1; end
while (i > 0),
S=0; S2=0; h=(beta(i)-alfa(i))/2; S=(h/3)*(f(i,1)+4*f(i,3)+f(i,5));
h=h/2; S2=(h/3)*(f(i,1)+4*f(i,2)+f(i,3));
S2=S2+(h/3)*(f(i,3)+4*f(i,4)+f(i,5));
tolrv=tol*(beta(i)-alfa(i))/(b-a); errrv=abs(S-S2)/10;
if (errrv > tolrv)
i=i+1; alfa(i)=alfa(i-1); beta(i)=(alfa(i-1)+beta(i-1))/2;
f(i,1)=f(i-1,1);f(i,3)=f(i-1,2);f(i,5)=f(i-1,3);len=abs(beta(i)-alfa(i));
if (len >= hmin),
if (len <= 11*hmin)
disp(’ Steplength close to hmin ’),
str=sprintf(’The approximate integral is %12.7e’,integ);disp(str),end;
step=len/4; x=alfa(i)+step; f(i,2)=eval(fun);
nfv=nfv+1; x=beta(i)-step; f(i,4)=eval(fun); nfv=nfv+1;
else, xfv=xfv’; disp(’ Too small steplength
’)
str=sprintf(’The approximate integral is %12.7e’,integ);
disp(str), return
end, else
integ=integ+S2; level=level+1; if (level==1),
for k=1:5, xfv(k)=alfa(i)+(k-1)*h; end; ist=5;
else, for k=1:4, xfv(ist+k)=alfa(i)+k*h; end; ist=ist+4; end;
if (beta(i)==b), xfv=xfv’;
str=sprintf(’The approximate integral is %12.7e’,integ);
disp(str), return, end; i=i-1; alfa(i)=beta(i+1);
f(i,1)=f(i+1,5); f(i,3)=f(i,4); step=abs(beta(i)-alfa(i))/4;
x=alfa(i)+step; f(i,2)=eval(fun); nfv=nfv+1; x=beta(i)-step;
f(i,4)=eval(fun); nfv=nfv+1;

398
9. Numerical Integration
end
end
9.8
Singular Integrals
In this section we extend our analysis to deal with singular integrals, arising
when f has ﬁnite jumps or is even inﬁnite at some point. Besides, we
will consider the case of integrals of bounded functions over unbounded
intervals. We brieﬂy address the most relevant numerical techniques for
properly handling these integrals.
9.8.1
Integrals of Functions with Finite Jump Discontinuities
Let c be a known point within [a, b] and assume that f is a continuous and
bounded function in [a, c) and (c, b], with ﬁnite jump f(c+) −f(c−). Since
I(f) =
> b
a
f(x)dx =
> c
a
f(x)dx +
> b
c
f(x)dx,
(9.46)
any integration formula of the previous sections can be used on [a, c−] and
[c+, b] to furnish an approximation of I(f). We proceed similarly if f admits
a ﬁnite number of jump discontinuities within [a, b].
When the position of the discontinuity points of f is not known a priori,
a preliminary analysis of the graph of the function should be carried out.
Alternatively, one can resort to an adaptive integrator that is able to detect
the presence of discontinuities when the integration steplength falls below
a given tolerance (see Section 9.7.2).
9.8.2
Integrals of Inﬁnite Functions
Let us deal with the case in which limx→a+ f(x) = ∞; similar consider-
ations hold when f is inﬁnite as x →b−, while the case of a point of
singularity c internal to the interval [a, b] can be recast to one of the pre-
vious two cases owing to (9.46). Assume that the integrand function is of
the form
f(x) =
φ(x)
(x −a)µ ,
0 ≤µ < 1,
where φ is a function whose absolute value is bounded by M. Then
|I(f)| ≤M lim
t→a+
b
>
t
1
(x −a)µ dx = M (b −a)1−µ
1 −µ
.

9.8 Singular Integrals
399
Suppose we wish to approximate I(f) up to a ﬁxed tolerance δ. For this, let
us describe the following two methods (for further details, see also [IK66],
Section 7.6, and [DR75], Section 2.12 and Appendix 1).
Method 1. For any ε such that 0 < ε < (b −a), we write the singular
integral as I(f) = I1 + I2, where
I1 =
a+ε
>
a
φ(x)
(x −a)µ dx,
I2 =
b
>
a+ε
φ(x)
(x −a)µ dx.
The computation of I2 is not troublesome. After replacing φ by its p-th
order Taylor’s expansion around x = a, we obtain
φ(x) = Φp(x) + (x −a)p+1
(p + 1)! φ(p+1)(ξ(x)),
p ≥0
(9.47)
where Φp(x) = p
k=0 φ(k)(a)(x −a)k/k!. Then
I1 = ε1−µ
p

k=0
εkφ(k)(a)
k!(k + 1 −µ) +
1
(p + 1)!
a+ε
>
a
(x −a)p+1−µφ(p+1)(ξ(x))dx.
Replacing I1 by the ﬁnite sum, the corresponding error E1 can be bounded
as
|E1| ≤
εp+2−µ
(p + 1)!(p + 2 −µ)
max
a≤x≤a+ε|φ(p+1)(x)|,
p ≥0.
(9.48)
For ﬁxed p, the right side of (9.48) is an increasing function of ε. On the
other hand, taking ε < 1 and assuming that the successive derivatives of φ
do not grow too much as p increases, the same function is decreasing as p
grows.
Let us next approximate I2 using a composite Newton-Cotes formula
with m subintervals and n quadrature nodes for each subinterval, n being
an even integer. Recalling (9.26) and aiming at equidistributing the error
δ between I1 and I2, it turns out that
|E2| ≤M(n+2)(ε)b −a −ε
(n + 2)!
|Mn|
nn+3
b −a −ε
m
n+2
= δ/2,
(9.49)
where
M(n+2)(ε) =
max
a+ε≤x≤b

dn+2
dxn+2

φ(x)
(x −a)µ
 .
The value of the constant M(n+2)(ε) grows rapidly as ε tends to zero; as
a consequence, (9.49) might require such a large number of subintervals
mε = m(ε) to make the method at hand of little practical use.

400
9. Numerical Integration
Example 9.10 Consider the singular integral (known as the Fresnel integral)
I(f) =
π/2
>
0
cos(x)
√x dx.
(9.50)
Expanding the integrand function in a Taylor’s series around the origin and
applying the theorem of integration by series, we get
I(f) =
∞

k=0
(−1)k
(2k)!
1
(2k + 1/2)(π/2)2k+1/2.
Truncating the series at the ﬁrst 10 terms, we obtain an approximate value of
the integral equal to 1.9549.
Using the composite Cavalieri-Simpson formula, the a priori estimate (9.49)
yields, as ε tends to zero and letting n = 2, |M2| = 4/15,
mε ≃
0.018
δ
+π
2 −ε
,5
ε−9/2
1/4
.
For δ = 10−4, taking ε = 10−2, it turns out that 1140 (uniform) subintervals are
needed, while for ε = 10−4 and ε = 10−6 the number of subintervals is 2 · 105
and 3.6 · 107, respectively.
As a comparison, running Program 77 (adaptive integration with Cavalieri-
Simpson formula) with a = ε = 10−10, hmin = 10−12 and tol = 10−4, we
get the approximate value 1.955 for the integral at the price of 1057 functional
evaluations, which correspond to 528 nonuniform subdivisions of the interval
[0, π/2].
•
Method 2. Using the Taylor expansion (9.47) we obtain
I(f) =
b
>
a
φ(x) −Φp(x)
(x −a)µ
dx +
b
>
a
Φp(x)
(x −a)µ dx = I1 + I2.
Exact computation of I2 yields
I2 = (b −a)1−µ
p

k=0
(b −a)kφ(k)(a)
k!(k + 1 −µ) .
(9.51)
The integral I1 is, for p ≥0
I1 =
b
>
a
(x −a)p+1−µ φ(p+1)(ξ(x))
(p + 1)!
dx =
b
>
a
g(x)dx.
(9.52)
Unlike the case of method 1, the integrand function g does not blow up
at x = a, since its ﬁrst p derivatives are ﬁnite at x = a. As a consequence,
assuming we approximate I1 using a composite Newton-Cotes formula, it is
possible to give an estimate of the quadrature error, provided that p ≥n+2,
for n ≥0 even, or p ≥n + 1, for n odd.

9.8 Singular Integrals
401
Example 9.11 Consider again the singular Fresnel integral (9.50), and assume
we use the composite Cavalieri-Simpson formula for approximating I1. We will
take p = 4 in (9.51) and (9.52). Computing I2 yields the value (π/2)1/2(2 −
(1/5)(π/2)2 + (1/108)(π/2)4) ≃1.9588. Applying the error estimate (9.26) with
n = 2 shows that only 2 subdivisions of [0, π/2] suﬃce for approximating I1 up
to an error δ = 10−4, obtaining the value I1 ≃−0.0173. As a whole, method 2
returns for (9.50) the approximate value 1.9415.
•
9.8.3
Integrals over Unbounded Intervals
Let f ∈C0([a, +∞)); should it exist and be ﬁnite, the following limit
lim
t→+∞
t
>
a
f(x)dx
is taken as being the value of the singular integral
I(f) =
> ∞
a
f(x)dx =
lim
t→+∞
t
>
a
f(x)dx.
(9.53)
An analogous deﬁnition holds if f is continuous over (−∞, b], while for a
function f : R →R, integrable over any bounded interval, we let
> ∞
−∞
f(x)dx =
> c
−∞
f(x)dx +
> +∞
c
f(x)dx
(9.54)
if c is any real number and the two singular integrals on the right hand side
of (9.54) are convergent. This deﬁnition is correct since the value of I(f)
does not depend on the choice of c.
A suﬃcient condition for f to be integrable over [a, +∞) is that
∃ρ > 0, such that
lim
x→+∞x1+ρf(x) = 0,
that is, we require f to be inﬁnitesimal of order > 1 with respect to 1/x
as x →∞. For the numerical approximation of (9.53) up to a tolerance δ,
we consider the following methods, referring for further details to [DR75],
Chapter 3.
Method 1. To compute (9.53), we can split I(f) as I(f) = I1 + I2, where
I1 =
 c
a f(x)dx and I2 =
 ∞
c
f(x)dx.
The end-point c, which can be taken arbitrarily, is chosen in such a way
that the contribution of I2 is negligible. Precisely, taking advantage of the
asymptotic behavior of f, c is selected to guarantee that I2 equals a fraction
of the ﬁxed tolerance, say, I2 = δ/2.
Then, I1 will be computed up to an absolute error equal to δ/2. This
ensures that the global error in the computation of I1 + I2 is below the
tolerance δ.

402
9. Numerical Integration
Example 9.12 Compute up to an error δ = 10−3 the integral
I(f) =
∞
>
0
cos2(x)e−xdx = 3/5.
For any given c > 0, we have I2 =
∞
>
c
cos2(x)e−xdx ≤
> ∞
c
e−xdx = e−c; re-
quiring that e−c = δ/2, one gets c ≃7.6. Then, assuming we use the compos-
ite trapezoidal formula for approximating I1, thanks to (9.27) with n = 1 and
M = max0≤x≤c |f ′′(x)| ≃1.04, we obtain m ≥

Mc3/(6δ)
1/2 = 277.
Program 72 returns the value I1 ≃0.599905, instead of the exact value I1 =
3/5 −e−c(cos2(c) −(sin(2c) + 2 cos(2c))/5) ≃0.599842, with an absolute error of
about 6.27 · 10−5. The global numerical outcome is thus I1 + I2 ≃0.600405, with
an absolute error with respect to I(f) equal to 4.05 · 10−4.
•
Method 2. For any real number c, we let I(f) = I1 + I2, as for method 1,
then we introduce the change of variable x = 1/t in order to transform I2
into an integral over the bounded interval [0, 1/c]
I2 =
1/c
>
0
f(t)t−2dt =
1/c
>
0
g(t)dt.
(9.55)
If g(t) is not singular at t = 0, (9.55) can be treated by any quadrature
formula introduced in this chapter. Otherwise, one can resort to the inte-
gration methods considered in Section 9.8.2.
Method 3. Gaussian interpolatory formulae are used, where the integra-
tion nodes are the zeros of Laguerre and Hermite orthogonal polynomials
(see Section 10.5).
9.9
Multidimensional Numerical Integration
Let Ωbe a bounded domain in R2 with a suﬃciently smooth boundary. We
consider the problem of approximating the integral I(f) =
 
Ωf(x, y)dxdy,
where f is a continuous function in Ω. For this purpose, in Sections 9.9.1
and 9.9.2 we address two methods.
The ﬁrst method applies when Ωis a normal domain with respect to a
coordinate axis. It is based on the reduction formula for double integrals
and consists of using one-dimensional quadratures along both coordinate
direction. The second method, which applies when Ωis a polygon, consists
of employing composite quadratures of low degree on a triangular decom-
position of the domain Ω. Section 9.9.3 brieﬂy addresses the Monte Carlo

9.9 Multidimensional Numerical Integration
403
method, which is particularly well-suited to integration in several dimen-
sions.
9.9.1
The Method of Reduction Formula
Let Ωbe a normal domain with respect to the x axis, as drawn in Figure
9.5, and assume for the sake of simplicity that φ2(x) > φ1(x), ∀x ∈[a, b].
FIGURE 9.5. Normal domain with respect to x axis
The reduction formula for double integrals gives (with obvious choice of
notation)
I(f) =
b
>
a
φ2(x)
>
φ1(x)
f(x, y)dydx =
b
>
a
Ff(x)dx.
(9.56)
The integral I(Ff) =
 b
a Ff(x)dx can be approximated by a composite
quadrature rule using Mx subintervals {Jk, k = 1, . . . , Mx}, of width H =
(b −a)/Mx, and in each subinterval n(k)
x
+ 1 nodes {xk
i , i = 0, . . . , n(k)
x }.
Thus, in the x direction we can write
Ic
nx(f) =
Mx

k=1
n(k)
x

i=0
αk
i Ff(xk
i ),
where the coeﬃcients αk
i are the quadrature weights on each subinterval Jk.
For each node xk
i , the approximate evaluation of the integral Ff(xk
i ) is then
carried out by a composite quadrature using My subintervals {Jm, m =
1, . . . , My}, of width hk
i = (φ2(xk
i ) −φ1(xk
i ))/My and in each subinterval
n(m)
y
+ 1 nodes {yi,k
j,m, j = 0, . . . , n(m)
y
}.
In the particular case Mx = My = M, n(k)
x
= n(m)
y
= 0, for k, m =
1, . . . , M, the resulting quadrature formula is the midpoint reduction for-
mula
Ic
0,0(f) = H
M

k=1
hk
0
M

m=1
f(xk
0, y0,k
0,m),

404
9. Numerical Integration
where H = (b −a)/M, xk
0 = a + (k −1/2)H for k = 1, . . . , M and
y0,k
0,m = φ1(xk
0) + (m −1/2)hk
0 for m = 1, . . . , M. With a similar procedure
the trapezoidal reduction formula can be constructed along the coordinate
directions (in that case, n(k)
x
= n(m)
y
= 1, for k, m = 1, . . . , M).
The eﬃciency of the approach can obviously be increased by employing
the adaptive method described in Section 9.7.2 to suitably allocate the
quadrature nodes xk
i and yi,k
j,m according to the variations of f over the
domain Ω. The use of the reduction formulae above becomes less and less
convenient as the dimension d of the domain Ω⊂Rd gets larger, due to
the large increase in the computational eﬀort. Indeed, if any simple integral
requires N functional evaluations, the overall cost would be equal to N d.
The midpoint and trapezoidal reduction formulae for approximating the
integral (9.56) are implemented in Programs 78 and 79. For the sake of
simplicity, we set Mx = My = M. The variables phi1 and phi2 contain
the expressions of the functions φ1 and φ2 which delimitate the integration
domain.
Program 78 - redmidpt : Midpoint reduction formula
function inte=redmidpt(a,b,phi1,phi2,m,fun)
H=(b-a)/m; xx=[a+H/2:H:b]; dim=max(size(xx));
for i=1:dim, x=xx(i); d=eval(phi2); c=eval(phi1); h=(d-c)/m;
y=[c+h/2:h:d]; w=eval(fun); psi(i)=h*sum(w(1:m)); end;
inte=H*sum(psi(1:m));
Program 79 - redtrap : Trapezoidal reduction formula
function inte=redtrap(a,b,phi1,phi2,m,fun)
H=(b-a)/m; xx=[a:H:b]; dim=max(size(xx));
for i=1:dim, x=xx(i); d=eval(phi2); c=eval(phi1); h=(d-c)/m;
y=[c:h:d]; w=eval(fun); psi(i)=h*(0.5*w(1)+sum(w(2:m))+0.5*w(m+1));
end; inte=H*(0.5*psi(1)+sum(psi(2:m))+0.5*psi(m+1));
9.9.2
Two-Dimensional Composite Quadratures
In this section we extend to the two-dimensional case the composite inter-
polatory quadratures that have been considered in Section 9.4. We assume
that Ωis a convex polygon on which we introduce a triangulation Th of NT
triangles or elements, such that Ω=
-
T ∈Th
T, where the parameter h > 0 is
the maximum edge-length in Th (see Section 8.5.2).
Exactly as happens in the one-dimensional case, interpolatory composite
quadrature rules on triangles can be devised by replacing
 
Ωf(x, y)dxdy
with
 
ΩΠk
hf(x, y)dxdy, where, for k ≥0, Πk
hf is the composite interpolat-
ing polynomial of f on the triangulation Th introduced in Section 8.5.2.

9.9 Multidimensional Numerical Integration
405
For an eﬃcient evaluation of this last integral, we employ the property of
additivity which, combined with (8.38), leads to the following interpolatory
composite rule
Ic
k(f) =
>
Ω
Πk
hf(x, y)dxdy =

T ∈Th
>
T
Πk
T f(x, y)dxdy =

T ∈Th
IT
k (f)
=

T ∈Th
dk−1

j=0
f(˜zT
j )
>
T
lT
j (x, y)dxdy =

T ∈Th
dk−1

j=0
αT
j f(˜zT
j ).
(9.57)
The coeﬃcients α(j)
T
and the points ˜z(j)
T
are called the local weights and
nodes of the quadrature formula (9.57), respectively.
The weights α(j)
T
can be computed on the reference triangle ˆT of vertices
(0, 0), (1, 0) and (0, 1), as follows
α(j)
T
=
>
T
lj,T (x, y)dxdy = 2|T|
>
ˆT
ˆlj(ˆx, ˆy)dˆxdˆy,
j = 0, . . . , dk −1,
where |T| is the area of T. If k = 0, we get α(0)
T
= |T|, while if k = 1 we
have α(j)
T
= |T|/3, for j = 0, 1, 2.
Denoting respectively by a(j)
T
and aT = 3
j=1(a(j)
T )/3, for j = 1, 2, 3, the
vertices and the center of gravity of the triangle T ∈Th, the following
formulae are obtained.
Composite midpoint formula
Ic
0(f) =

T ∈Th
|T|f(aT ).
(9.58)
Composite trapezoidal formula
Ic
1(f) = 1
3

T ∈Th
|T|
3

j=1
f(a(j)
T ).
(9.59)
In view of the analysis of the quadrature error Ec
k(f) = I(f) −Ic
k(f), we
introduce the following deﬁnition.
Deﬁnition 9.1 The quadrature formula (9.57) has degree of exactness
equal to n, with n ≥0, if I T
k (p) =
 
T pdxdy for any p ∈Pn( T), where
Pn( T) is deﬁned in (8.35).
■
The following result can be proved (see [IK66], pp. 361–362).

406
9. Numerical Integration
Property 9.4 Assume that the quadrature rule (9.57) has degree of exact-
ness on Ωequal to n, with n ≥0, and that its weights are all nonnegative.
Then, there exists a positive constant Kn, independent of h, such that
|Ec
k(f)| ≤Knhn+1|Ω|Mn+1,
for any function f ∈Cn+1(Ω), where Mn+1 is the maximum value of the
modules of the derivatives of order n + 1 of f and |Ω| is the area of Ω.
The composite formulae (9.58) and (9.59) both have degrees of exactness
equal to 1; then, due to Property 9.4, their order of inﬁnitesimal with
respect to h is equal to 2.
An alternative family of quadrature rules on triangles is provided by the so-
called symmetric formulae. These are Gaussian formulae with n nodes and
high degree of exactness, and exhibit the feature that the quadrature nodes
occupy symmetric positions with respect to all corners of the reference
triangle T or, as happens for Gauss-Radau formulae, with respect to the
straight line y = x.
Considering the generic triangle T ∈Th and denoting by aT
(j), j = 1, 2, 3,
the midpoints of the edges of T, two examples of symmetric formulae,
having degree of exactness equal to 2 and 3, respectively, are the following
I3(f) = |T|
3
3

j=1
f(aT
(j)),
n = 3,
I7(f) = |T|
60

3
3

i=1
f(a(i)
T ) + 8
3

j=1
f(aT
(j)) + 27f(aT )

,
n = 7.
For a description and analysis of symmetric formulae for triangles, see
[Dun85], while we refer to [Kea86] and [Dun86] for their extension to tetra-
hedra and cubes.
The composite quadrature rules (9.58) and (9.59) are implemented in
Programs 80 and 81 for the approximate evaluation of the integral of f(x, y)
over a single triangle T ∈Th. To compute the integral over Ωit suﬃces
to sum the result provided by the program over each triangle of Th. The
coordinates of the vertices of the triangle T are stored in the arrays xv and
yv.
Program 80 - midptr2d : Midpoint rule on a triangle
function inte=midptr2d(xv,yv,fun)
y12=yv(1)-yv(2); y23=yv(2)-yv(3); y31=yv(3)-yv(1);
areat=0.5*abs(xv(1)*y23+xv(2)*y31+xv(3)*y12);
x=sum(xv)/3; y=sum(yv)/3; inte=areat*eval(fun);
Program 81 - traptr2d : Trapezoidal rule on a triangle

9.9 Multidimensional Numerical Integration
407
function inte=traptr2d(xv,yv,fun)
y12=yv(1)-yv(2); y23=yv(2)-yv(3); y31=yv(3)-yv(1);
areat=0.5*abs(xv(1)*y23+xv(2)*y31+xv(3)*y12); inte=0;
for i=1:3, x=xv(i); y=yv(i); inte=inte+eval(fun); end;
inte=inte*areat/3;
9.9.3
Monte Carlo Methods for Numerical Integration
Numerical integration methods based on Monte Carlo techniques are a valid
tool for approximating multidimensional integrals when the space dimen-
sion of Rn gets large. These methods diﬀer from the approaches considered
thus far, since the choice of quadrature nodes is done statistically accord-
ing to the values attained by random variables having a known probability
distribution.
The basic idea of the method is to interpret the integral as a statistic
mean value
>
Ω
f(x)dx = |Ω|
>
Rn
|Ω|−1χΩ(x)f(x)dx = |Ω|µ(f),
where x = (x1, x2, . . . , xn)T and |Ω| denotes the n-dimensional volume of
Ω, χΩ(x) is the characteristic function of the set Ω, equal to 1 for x ∈Ωand
to 0 elsewhere, while µ(f) is the mean value of the function f(X), where
X is a random variable with uniform probability density |Ω|−1χΩover Rn.
We recall that the random variable X ∈Rn (or, more properly, random
vector) is an n-tuple of real numbers X1(ζ), . . . , Xn(ζ) assigned to every
outcome ζ of a random experiment (see [Pap87], Chapter 4).
Having ﬁxed a vector x ∈Rn, the probability P{X ≤x} of the random
event {X1 ≤x1, . . . , Xn ≤xn} is given by
P{X ≤x} =
> x1
−∞
. . .
> xn
−∞
f(X1, . . . , Xn)dX1 . . . dXn
where f(X) = f(X1, . . . , Xn) is the probability density of the random vari-
able X ∈Rn, such that
f(X1, . . . , Xn) ≥0,
>
Rn f(X1, . . . , Xn)dX = 1.
The numerical computation of the mean value µ(f) is carried out by taking
N independent samples x1, . . . , xN ∈Rn with probability density |Ω|−1χΩ
and evaluating their average
f N = 1
N
N

i=1
f(xi) = IN(f).
(9.60)

408
9. Numerical Integration
From a statistical standpoint, the samples x1, . . . , xN can be regarded as
the realizations of a sequence of N random variables {X1, . . . , XN}, mu-
tually independent and each with probability density |Ω|−1χΩ.
For such a sequence the strong law of large numbers ensures with prob-
ability 1 the convergence of the average IN(f) =
+N
i=1 f(Xi)
,
/N to
the mean value µ(f) as N →∞. In computational practice the sequence
of samples x1, . . . , xN is deterministically produced by a random-number
generator, giving rise to the so-called pseudo-random integration formulae.
The quadrature error EN(f) = µ(f) −IN(f) as a function of N can be
characterized through the variance
σ(IN(f)) =

µ (IN(f) −µ(f))2.
Interpreting again f as a function of the random variable X, distributed
with uniform probability density |Ω|−1 in Ω⊆Rn and variance σ(f), we
have
σ(IN(f)) = σ(f)
√
N
,
(9.61)
from which, as N →∞, a convergence rate of O(N −1/2) follows for the
statistical estimate of the error σ(IN(f)). Such convergence rate does not
depend on the dimension n of the integration domain, and this is a most
relevant feature of the Monte Carlo method. However, it is worth noting
that the convergence rate is independent of the regularity of f; thus, un-
like interpolatory quadratures, Monte Carlo methods do not yield more
accurate results when dealing with smooth integrands.
The estimate (9.61) is extremely weak and in practice one does often
obtain poorly accurate results. A more eﬃcient implementation of Monte
Carlo methods is based on composite approach or semi-analytical methods;
an example of these techniques is provided in [ NAG95], where a composite
Monte Carlo method is employed for the computation of integrals over
hypercubes in Rn.
9.10
Applications
We consider in the next sections the computation of two integrals suggested
by applications in geometry and the mechanics of rigid bodies.
9.10.1
Computation of an Ellipsoid Surface
Let E be the ellipsoid obtained by rotating the ellipse in Figure 9.6 around
the x axis, where the radius ρ is described as a function of the axial coor-

9.10 Applications
409
x )
ρ
1/β
-
1/β
(
x
E
FIGURE 9.6. Section of the ellipsoid
dinate by the equation
ρ2(x) = α2(1 −β2x2),
−1
β ≤x ≤1
β ,
α and β being given constants, assigned in such a way that α2β2 < 1.
We set the following values for the parameters: α2 = (3 −2
√
2)/100 and
β2 = 100. Letting K2 = β2
1 −α2β2, f(x) =
√
1 −K2x2 and θ =
cos−1(K/β), the computation of the surface of E requires evaluating the
integral
I(f) = 4πα
1/β
>
0
f(x)dx = 2πα
K [(π/2 −θ) + sin(2θ)/2] .
(9.62)
Notice that f ′(1/β) = −100; this prompts us to use a numerical adaptive
formula able to provide a nonuniform distribution of quadrature nodes,
with a possible reﬁnement of these nodes around x = 1/β.
Table 9.12 summarizes the results obtained using the composite midpoint,
trapezoidal and Cavalieri-Simpson rules (respectively denoted by (MP),
(TR) and (CS)), which are compared with Romberg integration (RO) and
with the adaptive Cavalieri-Simpson quadrature introduced in Section 9.7.2
and denoted by (AD).
In the table, m is the number of subintervals, while Err and flops denote
the absolute quadrature error and the number of ﬂoating-point operations
required by each algorithm, respectively. In the case of the AD method,
we have run Program 77 taking hmin=10−5 and tol=10−8, while for the
Romberg method we have used Program 76 with n=9.
The results demonstrate the advantage of using the composite adaptive
Cavalieri-Simpson formula, both in terms of computational eﬃciency and
accuracy, as can be seen in the plots in Figure 9.7 which allow to check
the successful working of the adaptivity procedure. In Figure 9.7 (left),
we show, together with the graph of f, the nonuniform distribution of
the quadrature nodes on the x axis, while in Figure 9.7 (right) we plot
the logarithmic graph of the integration step density (piecewise constant)
∆h(x), deﬁned as the inverse of the value of the stepsize h on each active
interval A (see Section 9.7.2).

410
9. Numerical Integration
Notice the high value of ∆h at x = 1/β, where the derivative of the
integrand function is maximum.
(PM)
(TR)
(CS)
(RO)
(AD)
m
4000
5600
250
50
Err
3.24e −10
3.30e −10
2.98e −10
3.58e −11
3.18e −10
flops
20007
29013
2519
5772
3540
TABLE 9.12. Methods for approximating I(f) = 4πα
 1/β
0
√
1 −K2x2dx, with
α2 = (3 −2
√
2)/100, β = 10 and K2 =

β2(1 −α2β2)
0
0.02
0.04
0.06
0.08
0.1
−0.2
0
0.2
0.4
0.6
0.8
1
0
0.02
0.04
0.06
0.08
0.1
10
2
10
3
10
4
10
5
FIGURE 9.7. Distribution of quadrature nodes (left); integration stepsize density
in the approximation of integral (9.62) (right)
9.10.2
Computation of the Wind Action on a Sailboat Mast
Let us consider the sailboat schematically drawn in Figure 9.8 (left) and
subject to the action of the wind force. The mast, of length L, is denoted by
the straight line AB, while one of the two shrouds (strings for the side stiﬀ-
ening of the mast) is represented by the straight line BO. Any inﬁnitesimal
element of the sail transmits to the corresponding element of length dx of
the mast a force of magnitude equal to f(x)dx. The change of f along with
the height x, measured from the point A (basis of the mast), is expressed
by the following law
f(x) =
αx
x + β e−γx,
where α, β and γ are given constants.

9.10 Applications
411
The resultant R of the force f is deﬁned as
R =
L
>
0
f(x)dx ≡I(f),
(9.63)
and is applied at a point at distance equal to b (to be determined) from
the basis of the mast.
                              


dx
wind
direction
mast
shroud
T
A
O
dx
B
f
L
A
O
V
B
T
M
H
b
R
FIGURE 9.8. Schematic representation of a sailboat (left); forces acting on the
mast (right)
Computing R and the distance b, given by b = I(xf)/I(f), is crucial for the
structural design of the mast and shroud sections. Indeed, once the values
of R and b are known, it is possible to analyze the hyperstatic structure
mast-shroud (using for instance the method of forces), thus allowing for the
computation of the reactions V , H and M at the basis of the mast and the
traction T that is transmitted by the shroud, and are drawn in Figure 9.8
(right). Then, the internal actions in the structure can be found, as well as
the maximum stresses arising in the mast AB and in the shroud BO, from
which, assuming that the safety veriﬁcations are satisﬁed, one can ﬁnally
design the geometrical parameters of the sections of AB and BO.
For the approximate computation of R we have considered the compos-
ite midpoint, trapezoidal and Cavalieri-Simpson rules, denoted henceforth
by (MP), (TR) and (CS), and, for a comparison, the adaptive Cavalieri-
Simpson quadrature formula introduced in Section 9.7.2 and denoted by
(AD). Since a closed-form expression for the integral (9.63) is not available,
the composite rules have been applied taking mk = 2k uniform partitions
of [0, L], with k = 0, . . . , 15.
We have assumed in the numerical experiments α = 50, β = 5/3 and
γ = 1/4 and we have run Program 77 taking tol=10−4 and hmin=10−3.
The sequence of integrals computed using the composite formulae has been
stopped at k = 12 (corresponding to mk = 212 = 4096) since the remaining

412
9. Numerical Integration
0
20
40
60
80
100
120
10
−9
10
−8
10
−7
10
−6
10
−5
10
−4
10
−3
10
−2
10
−1
10
0
(TR)
(PM)
(CS)
(AD)
FIGURE 9.9. Relative errors in the approximate computation of the integral
 L
0 (αxe−γx)/(x + β)dx
elements, in the case of formula CS, diﬀer among them only up to the last
signiﬁcant ﬁgure. Therefore, we have assumed as the exact value of I(f)
the outcome I(CS)
12
= 100.0613683179612 provided by formula CS.
We report in Figure 9.9 the logarithmic plots of the relative error |I(CS)
12
−
Ik|/I12, for k = 0, . . . , 7, Ik being the generic element of the sequence for
the three considered formulae. As a comparison, we also display the graph
of the relative error in the case of formula AD, applied on a number of
(nonuniform) partitions equivalent to that of the composite rules.
Notice how, for the same number of partitions, formula AD is more accu-
rate, with a relative error of 2.06 · 10−7 obtained using 37 (nonuniform)
partitions of [0, L]. Methods PM and TR achieve a comparable accuracy
employing 2048 and 4096 uniform subintervals, respectively, while formula
CS requires about 64 partitions. The eﬀectiveness of the adaptivity pro-
cedure is demonstrated by the plots in Figure 9.10, which show, together
with the graph of f, the distribution of the quadrature nodes (left) and the
function ∆h(x) (right) that expresses the (piecewise constant) density of
the integration stepsize h, deﬁned as the inverse of the value of h over each
active interval A (see Section 9.7.2).
Notice also the high value of ∆h at x = 0, where the derivatives of f are
maximum.
9.11
Exercises
1. Let E0(f) and E1(f) be the quadrature errors in (9.6) and (9.12). Prove
that |E1(f)| ≃2|E0(f)|.
2. Check that the error estimates for the midpoint, trapezoidal and Cavalieri-
Simpson formulae, given respectively by (9.6), (9.12) and (9.16), are special
instances of (9.19) or (9.20). In particular, show that M0 = 2/3, K1 =

9.11 Exercises
413
0
2
4
6
8
10
−5
0
5
10
15
20
0
2
4
6
8
10
0
5
10
15
20
25
30
FIGURE 9.10. Distribution of quadrature nodes (left); integration step density
in the approximation of the integral
 L
0 (αxe−γx)/(x + β)dx (right)
−1/6 and M2 = −4/15 and determine, using the deﬁnition, the degree of
exactness r of each formula.
[Hint: ﬁnd r such that In(xk) =
 b
a xkdx, for k = 0, . . . , r, and In(xj) ̸=
 b
a xjdx, for j > r.]
3. Let In(f) = n
k=0 αkf(xk) be a Lagrange quadrature formula on n + 1
nodes. Compute the degree of exactness r of the formulae:
(a) I2(f) = (2/3)[2f(−1/2) −f(0) + 2f(1/2)],
(b) I4(f) = (1/4)[f(−1) + 3f(−1/3) + 3f(1/3) + f(1)].
Which is the order of inﬁnitesimal p for (a) and (b)?
[Solution: r = 3 and p = 5 for both I2(f) and I4(f).]
4. Compute df[x0, . . . , xn, x]/dx by checking (9.22).
[Hint: proceed by computing directly the derivative at x as an incremental
ratio, in the case where only one node x0 exists, then upgrade progressively
the order of the divided diﬀerence.]
5. Let Iw(f) =
 1
0 w(x)f(x)dx with w(x) = √x, and consider the quadrature
formula Q(f) = af(x1). Find a and x1 in such a way that Q has maximum
degree of exactness r.
[Solution: a = 2/3, x1 = 3/5 and r = 1.]
6. Let us consider the quadrature formula Q(f) = α1f(0) + α2f(1) + α3f ′(0)
for the approximation of I(f) =
 1
0 f(x)dx, where f ∈C1([0, 1]). Determine
the coeﬃcients αj, for j = 1, 2, 3 in such a way that Q has degree of
exactness r = 2.
[Solution: α1 = 2/3, α2 = 1/3 and α3 = 1/6.]
7. Apply the midpoint, trapezoidal and Cavalieri-Simpson composite rules to
approximate the integral
> 1
−1
|x|exdx,
and discuss their convergence as a function of the size H of the subintervals.

414
9. Numerical Integration
8. Consider the integral I(f) =
 1
0 exdx and estimate the minimum number m
of subintervals that is needed for computing I(f) up to an absolute error
≤5 · 10−4 using the composite trapezoidal (TR) and Cavalieri-Simpson
(CS) rules. Evaluate in both cases the absolute error Err that is actually
made.
[Solution: for TR, we have m = 17 and Err = 4.95 · 10−4, while for CS,
m = 2 and Err = 3.70 · 10−5.]
9. Consider the corrected trapezoidal formula (9.30) and check that |Ecorr
1
(f)| ≃
4|E2(f)|, where Ecorr
1
(f) and E2(f) are deﬁned in (9.31) and (9.16), re-
spectively.
10. Compute, with an error less than 10−4, the following integrals:
(a)
 ∞
0
sin(x)/(1 + x4)dx;
(b)
 ∞
0
e−x(1 + x)−5dx;
(c)
 ∞
−∞cos(x)e−x2dx.
11. Use the reduction midpoint and trapezoidal formulae for computing the
double integral I(f) =
 
Ω
y
(1 + xy)dxdy over the domain Ω= (0, 1)2. Run
Programs 78 and 79 with M = 2i, for i = 0, . . . , 10 and plot in log-scale
the absolute error in the two cases as a function of M. Which method is
the most accurate? How many functional evaluations are needed to get an
(absolute) accuracy of the order of 10−6?
[Solution: the exact integral is I(f) = log(4) −1, and almost 2002 = 40000
functional evaluations are needed.]

10
Orthogonal Polynomials in
Approximation Theory
Trigonometric polynomials, as well as other orthogonal polynomials like
Legendre’s and Chebyshev’s, are widely employed in approximation theory.
This chapter addresses the most relevant properties of orthogonal poly-
nomials, and introduces the transforms associated with them, in particular
the discrete Fourier transform and the FFT, but also the Zeta and Wavelet
transforms.
Application to interpolation, least-squares approximation, numerical dif-
ferentiation and Gaussian integration are addressed.
10.1
Approximation of Functions by Generalized
Fourier Series
Let w = w(x) be a weight function on the interval (−1, 1), i.e., a nonneg-
ative integrable function in (−1, 1). Let us denote by {pk, k = 0, 1, . . . } a
system of algebraic polynomials, with pk of degree equal to k for each k,
mutually orthogonal on the interval (−1, 1) with respect to w. This means
that
1
>
−1
pk(x)pm(x)w(x)dx = 0
if k ̸= m.

416
10. Orthogonal Polynomials in Approximation Theory
Set (f, g)w =
 1
−1 f(x)g(x)w(x)dx and ∥f∥w = (f, f)1/2
w ; (·, ·)w and ∥· ∥w
are respectively the scalar product and the norm for the function space
L2
w = L2
w(−1, 1) =
%
f : (−1, 1) →R,
> 1
−1
f 2(x)w(x)dx < ∞
&
.
(10.1)
For any function f ∈L2
w the series
Sf =
+∞

k=0
fkpk,
with fk = (f, pk)w
∥pk∥2w
,
is called the generalized Fourier series of f, and fk is the k-th Fourier
coeﬃcient. As is well-known, Sf converges in average (or in the sense of
L2
w) to f. This means that, letting for any integer n
fn(x) =
n

k=0
fkpk(x)
(10.2)
(fn ∈Pn is the truncation of order n of the generalized Fourier series of
f), the following convergence result holds
lim
n→+∞∥f −fn∥w = 0.
Thanks to Parseval’s equality, we have
∥f∥2
w =
+∞

k=0
f 2
k∥pk∥2
w
and, for any n, ∥f−fn∥2
w = +∞
k=n+1 f 2
k∥pk∥2
w is the square of the remainder
of the generalized Fourier series.
The polynomial fn ∈Pn satisﬁes the following minimization property
∥f −fn∥w = min
q∈Pn∥f −q∥w.
(10.3)
Indeed, since f −fn = +∞
k=n+1 fkpk, the property of orthogonality of
polynomials {pk} implies (f −fn, q)w = 0 ∀q ∈Pn. Then, the Cauchy-
Schwarz inequality (8.29) yields
∥f −fn∥2
w
=
(f −fn, f −fn)w = (f −fn, f −q)w + (f −fn, q −fn)w
=
(f −fn, f −q)w ≤∥f −fn∥w∥f −q∥w,
∀q ∈Pn,
and (10.3) follows since q is arbitrary in Pn. In such a case, we say that fn
is the orthogonal projection of f over Pn in the sense of L2
w. It is therefore
interesting to compute the coeﬃcients fk of fn. As will be seen in later

10.1 Approximation of Functions by Generalized Fourier Series
417
sections, this is usually done by suitably approximating the integrals that
appear in the deﬁnition of fk. By doing so, one gets the so-called discrete
coeﬃcients ˜fk of f, and, as a consequence, the new polynomial
f ∗
n(x) =
n

k=0
˜fkpk(x)
(10.4)
which is called the discrete truncation of order n of the Fourier series of f.
Typically,
˜fk = (f, pk)n
∥pk∥2n
,
(10.5)
where, for any pair of continuous functions f and g, (f, g)n is the approxi-
mation of the scalar product (f, g)w and ∥g∥n =

(g, g)n is the seminorm
associated with (·, ·)w. In a manner analogous to what was done for fn, it
can be checked that
∥f −f ∗
n∥n = min
q∈Pn∥f −q∥n
(10.6)
and we say that f ∗
n is the approximation to f in Pn in the least-squares
sense (the reason for using this name will be made clear later on).
We conclude this section by recalling that, for any family of monic orthog-
onal polynomials {pk}, the following recursive three-term formula holds (for
the proof, see for instance [Gau96])
"
pk+1(x) = (x −αk)pk(x) −βkpk−1(x)
k ≥0,
p−1(x) = 0,
p0(x) = 1,
(10.7)
where
αk = (xpk, pk)w
(pk, pk)w
,
βk+1 = (pk+1, pk+1)w
(pk, pk)w
,
k ≥0.
(10.8)
Since p−1 = 0, the coeﬃcient β0 is arbitrary and is chosen according to
the particular family of orthogonal polynomials at hand. The recursive
three-term relation is generally quite stable and can thus be conveniently
employed in the numerical computation of orthogonal polynomials, as will
be seen in Section 10.6.
In the forthcoming sections we introduce two relevant families of orthogonal
polynomials.
10.1.1
The Chebyshev Polynomials
Consider the Chebyshev weight function w(x) = (1−x2)−1/2 on the interval
(−1, 1), and, according to (10.1), introduce the space of square-integrable

418
10. Orthogonal Polynomials in Approximation Theory
functions with respect to the weight w
L2
w(−1, 1) =
%
f : (−1, 1) →R :
> 1
−1
f 2(x)(1 −x2)−1/2dx < ∞
&
.
A scalar product and a norm for this space are deﬁned as
(f, g)w =
1
>
−1
f(x)g(x)(1 −x2)−1/2dx,
∥f∥w =



1
>
−1
f 2(x)(1 −x2)−1/2dx



1/2
.
(10.9)
The Chebyshev polynomials are deﬁned as follows
Tk(x) = cos kθ,
θ = arccos x,
k = 0, 1, 2, . . .
(10.10)
They can be recursively generated by the following formula (a consequence
of (10.7), see [DR75], pp. 25-26)



Tk+1(x) = 2xTk(x) −Tk−1(x)
k = 1, 2, . . .
T0(x) = 1,
T1(x) = x.
(10.11)
In particular, for any k ≥0, we notice that Tk ∈Pk, i.e., Tk(x) is an alge-
braic polynomial of degree k with respect to x. Using well-known trigono-
metric relations, we have
(Tk, Tn)w = 0 if k ̸= n,
(Tn, Tn)w =
" c0 = π
if n = 0,
cn = π/2
if n ̸= 0,
which expresses the orthogonality of the Chebyshev polynomials with re-
spect to the scalar product (·, ·)w. Therefore, the Chebyshev series of a
function f ∈L2
w takes the form
Cf =
∞

k=0
fkTk,
with
fk = 1
ck
1
>
−1
f(x)Tk(x)(1 −x2)−1/2dx.
Notice that ∥Tn∥∞= 1 for every n and the following minimax property
holds
∥21−nTn∥∞≤min
p∈P1
n
∥p∥∞,
where P1
n = {p(x) = n
k=0 akxk, an = 1} denotes the subset of polynomials
of degree n with leading coeﬃcient equal to 1.

10.2 Gaussian Integration and Interpolation
419
10.1.2
The Legendre Polynomials
The Legendre polynomials are orthogonal polynomials over the interval
(−1, 1) with respect to the weight function w(x) = 1. For these polynomials,
L2
w is the usual L2(−1, 1) space introduced in (8.25), while (·, ·)w and ∥·∥w
coincide with the scalar product and norm in L2(−1, 1), respectively given
by
(f, g) =
1
>
−1
f(x)g(x) dx,
∥f∥L2(−1,1) =


1
>
−1
f 2(x) dx


1
2
.
The Legendre polynomials are deﬁned as
Lk(x) = 1
2k
[k/2]

l=0
(−1)l
 k
l
  2k −2l
k

xk−2l
k = 0, 1, . . . (10.12)
where [k/2] is the integer part of k/2, or, recursively, through the three-
term relation





Lk+1(x) = 2k + 1
k + 1 xLk(x) −
k
k + 1Lk−1(x)
k = 1, 2 . . .
L0(x) = 1,
L1(x) = x.
For every k = 0, 1 . . . , Lk ∈Pk and (Lk, Lm) = δkm(k + 1/2)−1 for k, m =
0, 1, 2, . . . . For any function f ∈L2(−1, 1), its Legendre series takes the
following form
Lf =
∞

k=0
fkLk,
with
fk =

k + 1
2
−1
1
>
−1
f(x)Lk(x)dx.
Remark 10.1 (The Jacobi polynomials) The polynomials previously
introduced belong to the wider family of Jacobi polynomials {Jαβ
k , k =
0, . . . , n}, that are orthogonal with respect to the weight w(x) = (1 −
x)α(1 + x)β, for α, β > −1. Indeed, setting α = β = 0 we recover the
Legendre polynomials, while choosing α = β = −1/2 gives the Chebyshev
polynomials.
■
10.2
Gaussian Integration and Interpolation
Orthogonal polynomials play a crucial role in devising quadrature formulae
with maximal degrees of exactness. Let x0, . . . , xn be n + 1 given distinct
points in the interval [−1, 1]. For the approximation of the weighted integral

420
10. Orthogonal Polynomials in Approximation Theory
Iw(f) =
 1
−1 f(x)w(x)dx, being f ∈C0([−1, 1]), we consider quadrature
rules of the type
In,w(f) =
n

i=0
αif(xi)
(10.13)
where αi are coeﬃcients to be suitably determined. Obviously, both nodes
and weights depend on n, however this dependence will be understood.
Denoting by
En,w(f) = Iw(f) −In,w(f)
the error between the exact integral and its approximation (10.13), if
En,w(p) = 0 for any p ∈Pr (for a suitable r ≥0) we shall say that for-
mula (10.13) has degree of exactness r with respect to the weight w. This
deﬁnition generalizes the one given for ordinary integration with weight
w = 1.
Clearly, we can get a degree of exactness equal to (at least) n taking
In,w(f) =
1
>
−1
Πnf(x)w(x)dx
where Πnf ∈Pn is the Lagrange interpolating polynomial of the function
f at the nodes {xi, i = 0, . . . , n}, given by (8.4). Therefore, (10.13) has
degree of exactness at least equal to n taking
αi =
1
>
−1
li(x)w(x)dx,
i = 0, . . . , n,
(10.14)
where li ∈Pn is the i-th characteristic Lagrange polynomial such that
li(xj) = δij, for i, j = 0, . . . , n.
The question that arises is whether suitable choices of the nodes exist
such that the degree of exactness is greater than n, say, equal to r = n+m
for some m > 0. The answer to this question is furnished by the following
theorem, due to Jacobi [Jac26].
Theorem 10.1 For a given m > 0, the quadrature formula (10.13) has
degree of exactness n + m iﬀit is of interpolatory type and the nodal poly-
nomial ωn+1 (8.6) associated with the nodes {xi} is such that
1
>
−1
ωn+1(x)p(x)w(x)dx = 0,
∀p ∈Pm−1.
(10.15)
Proof. Let us prove that these conditions are suﬃcient. If f ∈Pn+m then
there exist a quotient πm−1 ∈Pm−1 and a remainder qn ∈Pn, such that f =

10.2 Gaussian Integration and Interpolation
421
ωn+1πm−1 + qn. Since the degree of exactness of an interpolatory formula with
n + 1 nodes is equal to n (at least), we get
n

i=0
αiqn(xi) =
1
>
−1
qn(x)w(x)dx =
1
>
−1
f(x)w(x)dx −
1
>
−1
ωn+1(x)πm−1(x)w(x)dx.
As a consequence of (10.15), the last integral is null, thus
1
>
−1
f(x)w(x)dx =
n

i=0
αiqn(xi) =
n

i=0
αif(xi).
Since f is arbitrary, we conclude that En,w(f) = 0 for any f ∈Pn+m. Proving
that the conditions are also necessary is an exercise left to the reader.
3
Corollary 10.1 The maximum degree of exactness of the quadrature for-
mula (10.13) is 2n + 1.
Proof. If this would not be true, one could take m ≥n + 2 in the previous
theorem. This, in turn, would allow us to choose p = ωn+1 in (10.15) and come
to the conclusion that ωn+1 is identically zero, which is absurd.
3
Setting m = n + 1 (the maximum admissible value), from (10.15) we get
that the nodal polynomial ωn+1 satisﬁes the relation
1
>
−1
ωn+1(x)p(x)w(x)dx = 0,
∀p ∈Pn.
Since ωn+1 is a polynomial of degree n + 1 orthogonal to all the polyno-
mials of lower degree, we conclude that ωn+1 is the only monic polynomial
multiple of pn+1 (recall that {pk} is the system of orthogonal polynomials
introduced in Section 10.1). In particular, its roots {xj} coincide with those
of pn+1, that is
pn+1(xj) = 0,
j = 0, . . . , n.
(10.16)
The abscissae {xj} are the Gauss nodes associated with the weight func-
tion w(x). We can thus conclude that the quadrature formula (10.13) with
coeﬃcients and nodes given by (10.14) and (10.16), respectively, has degree
of exactness 2n + 1, the maximum value that can be achieved using inter-
polatory quadrature formulae with n + 1 nodes, and is called the Gauss
quadrature formula.
Its weights are all positive and the nodes are internal to the interval
(−1, 1) (see, for instance, [CHQZ88], p. 56). However, it is often useful to
also include the end points of the interval among the quadrature nodes. By

422
10. Orthogonal Polynomials in Approximation Theory
doing so, the Gauss formula with the highest degree of exactness is the one
that employs as nodes the n + 1 roots of the polynomial
ωn+1(x) = pn+1(x) + apn(x) + bpn−1(x),
(10.17)
where the constants a and b are selected in such a way that ωn+1(−1) =
ωn+1(1) = 0.
Denoting these roots by x0 = −1, x1, . . . , xn = 1, the coeﬃcients {αi, i =
0, . . . , n} can then be obtained from the usual formulae (10.14), that is
αi =
1
>
−1
li(x)w(x)dx,
i = 0, . . . , n,
where li ∈Pn is the i-th characteristic Lagrange polynomial such that
li(xj) = δij, for i, j = 0, . . . , n. The quadrature formula
IGL
n,w(f) =
n

i=0
αif(xi)
(10.18)
is called the Gauss-Lobatto formula with n + 1 nodes, and has degree of
exactness 2n −1. Indeed, for any f ∈P2n−1, there exist a polynomial
πn−2 ∈Pn−2 and a remainder qn ∈Pn such that f = ωn+1πn−2 + qn.
The quadrature formula (10.18) has degree of exactness at least equal to
n (being interpolatory with n + 1 distinct nodes), thus we get
n

j=0
αjqn(xj) =
1
>
−1
qn(x)w(x)dx =
1
>
−1
f(x)w(x)dx −
1
>
−1
ωn+1(x)πn−2(x)w(x)dx.
From (10.17) we conclude that ¯ωn+1 is orthogonal to all the polynomials
of degree ≤n −2, so that the last integral is null. Moreover, since f(xj) =
qn(xj) for j = 0, . . . , n, we conclude that
1
>
−1
f(x)w(x)dx =
n

i=0
αif(xi),
∀f ∈P2n−1.
Denoting by ΠGL
n,wf the polynomial of degree n that interpolates f at the
nodes {xj, j = 0, . . . , n}, we get
ΠGL
n,wf(x) =
n

i=0
f(xi)li(x)
(10.19)
and thus IGL
n,w(f) =
 1
−1 ΠGL
n,wf(x)w(x)dx.

10.2 Gaussian Integration and Interpolation
423
Remark 10.2 In the special case where the Gauss-Lobatto quadrature is
considered with respect to the Jacobi weight w(x) = (1 −x)α(1 −x)β,
with α, β > −1, the internal nodes x1, . . . , xn−1 can be identiﬁed as the
roots of the polynomial (J(α,β)
n
)′, that is, the extremants of the n-th Jacobi
polynomial J(α,β)
n
(see [CHQZ88], pp. 57-58).
■
The following convergence result holds for Gaussian integration (see [Atk89],
Chapter 5)
lim
n→+∞

1
>
−1
f(x)w(x)dx −
n

j=0
αjf(xj)

= 0,
∀f ∈C0([−1, 1]).
A similar result also holds for Gauss-Lobatto integration. If the integrand
function is not only continuous, but also diﬀerentiable up to the order
p ≥1, we shall see that Gaussian integration converges with an order of
inﬁnitesimal with respect to 1/n that is larger when p is greater. In the
forthcoming sections, the previous results will be speciﬁed in the cases of
the Chebyshev and Legendre polynomials.
Remark 10.3 (Integration over an arbitrary interval) A quadrature
formula with nodes ξj and coeﬃcients βj, j = 0, . . . , n over the interval
[−1, 1] can be mapped on any interval [a, b]. Indeed, let ϕ : [−1, 1] →[a, b]
be the aﬃne map x = ϕ(ξ) = a+b
2 ξ + b−a
2 . Then
b
>
a
f(x)dx = a + b
2
1
>
−1
(f ◦ϕ)(ξ)dξ.
Therefore, we can employ on the interval [a, b] the quadrature formula with
nodes xj = ϕ(ξj) and weights αj = a+b
2 βj. Notice that this formula main-
tains on the interval [a, b] the same degree of exactness of the generating
formula over [−1, 1]. Indeed, assuming that
1
>
−1
p(ξ)dξ =
n

j=0
p(ξj)βj
for any polynomial p of degree r over [−1, 1] (for a suitable integer r), for
any polynomial q of the same degree on [a, b] we get
n

j=0
q(xj)αj = a + b
2
n

j=0
(q ◦ϕ)(ξj)βj = a + b
2
1
>
−1
(q ◦ϕ)(ξ)dξ =
b
>
a
q(x)dx,
having recalled that (q ◦ϕ)(ξ) is a polynomial of degree r on [−1, 1].
■

424
10. Orthogonal Polynomials in Approximation Theory
10.3
Chebyshev Integration and Interpolation
If Gaussian quadratures are considered with respect to the Chebyshev
weight w(x) = (1 −x2)−1/2, Gauss nodes and coeﬃcients are given by
xj = −cos (2j + 1)π
2(n + 1) ,
αj =
π
n + 1,
0 ≤j ≤n,
(10.20)
while Gauss-Lobatto nodes and weights are
xj = −cos πj
n ,
αj =
π
djn,
0 ≤j ≤n,
n ≥1,
(10.21)
where d0 = dn = 2 and dj = 1 for j = 1, . . . , n −1. Notice that the Gauss
nodes (10.20) are, for a ﬁxed n ≥0, the zeros of the Chebyshev polynomial
Tn+1 ∈Pn+1, while, for n ≥1, the internal nodes {¯xj, j = 1, . . . , n −1}
are the zeros of T ′
n, as anticipated in Remark 10.2.
Denoting by ΠGL
n,wf the polynomial of degree n + 1 that interpolates f
at the nodes (10.21), it can be shown that the interpolation error can be
bounded as
∥f −ΠGL
n,wf∥w ≤Cn−s∥f∥s,w,
for s ≥1,
(10.22)
where ∥· ∥w is the norm in L2
w deﬁned in (10.9), provided that for some
s ≥1 the function f has derivatives f (k) of order k = 0, . . . , s in L2
w. In
such a case
∥f∥s,w =
 s

k=0
∥f (k)∥2
w
 1
2
.
(10.23)
Here and in the following, C is a constant independent of n that can assume
diﬀerent values at diﬀerent places. In particular, for any continuous function
f the following pointwise error estimate can be derived (see Exercise 3)
∥f(x) −ΠGL
n,wf(x)∥∞≤Cn1/2−s∥f∥s,w.
(10.24)
Thus, ΠGL
n,wf converges pointwise to f as n →∞, for any f ∈C1([−1, 1]).
The same kind of results (10.22) and (10.24) hold if ΠGL
n,wf is replaced with
the polynomial ΠG
n f of degree n that interpolates f at the n+1 Gauss nodes
xj in (10.20). (For the proof of these results see, for instance, [CHQZ88],
p. 298, or [QV94], p. 112). We have also the following result (see [Riv74],
p.13)
∥f −ΠG
n f∥∞≤(1 + Λn)E∗
n(f),
with Λn ≤2
π log(n + 1) + 1, (10.25)
where ∀n, E∗
n(f) = inf
p∈Pn∥f −p∥∞is the best approximation error for f
in Pn and Λn is the Lebesgue constant associated with the Chebyshev

10.3 Chebyshev Integration and Interpolation
425
nodes (10.20). As far as the numerical integration error is concerned, let
us consider, for instance, the Gauss-Lobatto quadrature rule (10.18) with
nodes and weights given in (10.21). First of all, notice that
1
>
−1
f(x)(1 −x2)−1/2dx = lim
n→∞IGL
n,w(f)
for any function f whose left integral is ﬁnite (see [Sze67], p. 342). If,
moreover, ∥f∥s,w is ﬁnite for some s ≥1, we have

1
>
−1
f(x)(1 −x2)−1/2dx −IGL
n,w(f)

≤Cn−s∥f∥s,w.
(10.26)
This result follows from the more general one
|(f, vn)w −(f, vn)n| ≤Cn−s∥f∥s,w∥vn∥w,
∀vn ∈Pn,
(10.27)
where the so-called discrete scalar product has been introduced
(f, g)n =
n

j=0
αjf(xj)g(xj) = IGL
n,w(fg).
(10.28)
Actually, (10.26) follows from (10.27) setting vn ≡1 and noticing that
∥vn∥w =
+ 1
−1(1 −x2)−1/2dx
,1/2
= √π. Thanks to (10.26) we can thus
conclude that the (Chebyshev) Gauss-Lobatto formula has degree of ex-
actness 2n −1 and order of accuracy (with respect to n−1) equal to s,
provided that ∥f∥s,w < ∞. Therefore, the order of accuracy is only limited
by the regularity threshold s of the integrand function. Completely similar
considerations can be drawn for (Chebyshev) Gauss formulae with n + 1
nodes.
Let us ﬁnally determine the coeﬃcients ˜fk, k = 0, . . . , n, of the interpolat-
ing polynomial ΠGL
n,wf at the n + 1 Gauss-Lobatto nodes in the expansion
with respect to the Chebyshev polynomials (10.10)
ΠGL
n,wf(x) =
n

k=0
˜fkTk(x).
(10.29)
Notice that ΠGL
n,wf coincides with the discrete truncation of the Chebyshev
series f ∗
n deﬁned in (10.4). Enforcing the equality ΠGL
n,wf(xj) = f(xj), j =
0, . . . , n, we ﬁnd
f(xj) =
n

k=0
cos
kjπ
n

˜fk,
j = 0, . . . , n.
(10.30)

426
10. Orthogonal Polynomials in Approximation Theory
Recalling the exactness of the Gauss-Lobatto quadrature, it can be checked
that (see Exercise 2)
˜fk =
2
ndk
n

j=0
1
dj
cos
kjπ
n

f(xj),
k = 0, . . . , n.
(10.31)
Relation (10.31) yields the discrete coeﬃcients { ˜fk, k = 0, . . . , n} in terms
of the nodal values {f(xj), j = 0, . . . , n}. For this reason it is called
the Chebyshev discrete transform (CDT) and, thanks to its trigonomet-
ric structure, it can be eﬃciently computed using the FFT algorithm (Fast
Fourier transform) with a number of ﬂoating-point operations of the order
of n log2 n (see Section 10.9.2). Of course, (10.30) is the inverse of the CDT,
and can be computed using the FFT.
10.4
Legendre Integration and Interpolation
As previously noticed, the Legendre weight is w(x) ≡1. For n ≥0, the
Gauss nodes and the related coeﬃcients are given by
xj zeros of Ln+1(x), αj =
2
(1 −x2
j)[L′
n+1(xj)]2 , j = 0, . . . , n, (10.32)
while the Gauss-Lobatto ones are, for n ≥1
x0 = −1, xn = 1,
xj zeros of L′
n(x),
j = 1, . . . , n −1
(10.33)
αj =
2
n(n + 1)
1
[Ln(xj)]2 ,
j = 0, . . . , n
(10.34)
where Ln is the n-th Legendre polynomial deﬁned in (10.12). It can be
checked that, for a suitable constant C independent of n,
2
n(n + 1) ≤αj ≤C
n ,
∀j = 0, . . . , n
(see [BM92], p. 76). Then, letting ΠGL
n f be the polynomial of degree n that
interpolates f at the n+1 nodes xj given by (10.33), it can be proved that
it fulﬁlls the same error estimates as those reported in (10.22) and (10.24)
in the case of the corresponding Chebyshev polynomial.
Of course, the norm ∥·∥w must here be replaced by the norm ∥·∥L2(−1,1),
while ∥f∥s,w becomes
∥f∥s =
 s

k=0
∥f (k)∥2
L2(−1,1)
 1
2
.
(10.35)

10.4 Legendre Integration and Interpolation
427
The same kinds of results are ensured if ΠGL
n f is replaced by the polynomial
of degree n that interpolates f at the n + 1 nodes xj given by (10.32).
Referring to the discrete scalar product deﬁned in (10.28), but taking
now the nodes and coeﬃcients given by (10.33) and (10.34), we see that
(·, ·)n is an approximation of the usual scalar product (·, ·) of L2(−1, 1).
Actually, the equivalent relation to (10.27) now reads
|(f, vn) −(f, vn)n| ≤Cn−s∥f∥s∥vn∥L2(−1,1),
∀vn ∈Pn
(10.36)
and holds for any s ≥1 such that ∥f∥s < ∞. In particular, setting vn ≡1,
we get ∥vn∥=
√
2, and from (10.36) it follows that

1
>
−1
f(x)dx −IGL
n
(f)

≤Cn−s∥f∥s
(10.37)
which demonstrates a convergence of the Gauss-Legendre-Lobatto quadra-
ture formula to the exact integral of f with order of accuracy s with respect
to n−1 provided that ∥f∥s < ∞. A similar result holds for the Gauss-
Legendre quadrature formulae.
Example 10.1 Consider the approximate evaluation of the integral of f(x) =
|x|α+ 3
5 over [−1, 1] for α = 0, 1, 2. Notice that f has “piecewise” derivatives up
to order s = s(α) = α + 1 in L2(−1, 1). Figure 10.1 shows the behavior of the
error as a function of n for the Gauss-Legendre quadrature formula. According
to (10.37), the convergence rate of the formula increases by one when α increases
by one.
•
10
0
10
1
10
2
10
3
10
−10
10
−8
10
−6
10
−4
10
−2
10
0
10
2
FIGURE 10.1. The quadrature error in logarithmic scale as a function of n in the
case of a function with the ﬁrst s derivatives in L2(−1, 1) for s = 1 (solid line),
s = 2 (dashed line), s = 3 (dotted line)

428
10. Orthogonal Polynomials in Approximation Theory
The interpolating polynomial at the nodes (10.33) is given by
ΠGL
n f(x) =
n

k=0
˜fkLk(x).
(10.38)
Notice that also in this case ΠGL
n f coincides with the discrete truncation
of the Legendre series f ∗
n deﬁned in (10.4). Proceeding as in the previous
section, we get
f(xj) =
n

k=0
˜fkLk(xj),
j = 0, . . . , n,
(10.39)
and also
˜fk =











2k + 1
n(n + 1)
n

j=0
Lk(xj)
1
L2n(xj)f(xj),
k = 0, . . . , n −1,
1
n + 1
n

j=0
1
Ln(xj)f(xj),
k = n
(10.40)
(see Exercise 6). Formulae (10.40) and (10.39) provide, respectively, the
discrete Legendre transform (DLT) and its inverse.
10.5
Gaussian Integration over Unbounded
Intervals
We consider integration on both half and on the whole of real axis. In both
cases we use interpolatory Gaussian formulae whose nodes are the zeros of
Laguerre and Hermite orthogonal polynomials, respectively.
The Laguerre polynomials. These are algebraic polynomials, orthogonal
on the interval [0, +∞) with respect to the weight function w(x) = e−x.
They are deﬁned by
Ln(x) = ex dn
dxn (e−xxn),
n ≥0,
and satisfy the following three-term recursive relation
"
Ln+1(x) = (2n + 1 −x)Ln(x) −n2Ln−1(x)
n ≥0,
L−1 = 0,
L0 = 1.
For any function f, deﬁne ϕ(x) = f(x)ex. Then, I(f) =
 ∞
0
f(x)dx =
 ∞
0
e−xϕ(x)dx, so that it suﬃces to apply to this last integral the Gauss-
Laguerre quadratures, to get, for n ≥1 and f ∈C2n([0, +∞))
I(f) =
n

k=1
αkϕ(xk) + (n!)2
(2n)!ϕ(2n)(ξ),
0 < ξ < +∞,
(10.41)

10.6 Programs for the Implementation of Gaussian Quadratures
429
where the nodes xk, for k = 1, . . . , n, are the zeros of Ln and the weights
are αk = (n!)2xk/[Ln+1(xk)]2. From (10.41), one concludes that Gauss-
Laguerre formulae are exact for functions f of the type ϕe−x, where ϕ ∈
P2n−1. In a generalized sense, we can then state that they have optimal
degrees of exactness equal to 2n −1.
Example 10.2 Using a Gauss-Laguerre quadrature formula with n = 12 to com-
pute the integral in Example 9.12 we obtain the value 0.5997 with an absolute
error with respect to exact integration equal to 2.96 · 10−4. For the sake of com-
parison, the composite trapezoidal formula would require 277 nodes to obtain the
same accuracy.
•
The Hermite polynomials. These are orthogonal polynomials on the
real line with respect to the weight function w(x) = e−x2. They are deﬁned
by
Hn(x) = (−1)nex2 dn
dxn (e−x2),
n ≥0.
Hermite polynomials can be recursively generated as
"
Hn+1(x) = 2xHn(x) −2nHn−1(x)
n ≥0,
H−1 = 0,
H0 = 1.
As in the previous case, letting ϕ(x) = f(x)ex2, we have I(f) =
 ∞
−∞f(x)dx =
 ∞
−∞e−x2ϕ(x)dx. Applying to this last integral the Gauss-Hermite quadra-
tures we obtain, for n ≥1 and f ∈C2n(R)
I(f) =
∞
>
−∞
e−x2ϕ(x)dx =
n

k=1
αkϕ(xk) + (n!)√π
2n(2n)!ϕ(2n)(ξ),
ξ ∈R,
(10.42)
where the nodes xk, for k = 1, . . . , n, are the zeros of Hn and the weights
are αk = 2n+1n!√π/[Hn+1(xk)]2. As for Gauss-Laguerre quadratures, the
Gauss-Hermite rules also are exact for functions f of the form ϕe−x2, where
ϕ ∈P2n−1; therefore, they have optimal degrees of exactness equal to 2n−1.
More details on the subject can be found in [DR75], pp. 173-174.
10.6
Programs for the Implementation of Gaussian
Quadratures
Programs 82, 83 and 84 compute the coeﬃcients {αk} and {βk}, introduced
in (10.8), in the cases of the Legendre, Laguerre and Hermite polynomials.
These programs are then called by Program 85 for the computation of nodes

430
10. Orthogonal Polynomials in Approximation Theory
and weights (10.32), in the case of the Gauss-Legendre formulae, and by
Programs 86, 87 for computing nodes and weights in the Gauss-Laguerre
and Gauss-Hermite quadrature rules (10.41) and (10.42). All the codings
reported in this section are excerpts from the library ORTHPOL [Gau94].
Program 82 - coeﬂege : Coeﬃcients of Legendre polynomials
function [a, b] = coeﬂege(n)
if (n <= 1), disp(’ n must be > 1 ’); return; end
for k=1:n, a(k)=0; b(k)=0; end; b(1)=2;
for k=2:n, b(k)=1/(4-1/(k-1)ˆ2); end
Program 83 - coeﬂagu : Coeﬃcients of Laguerre polynomials
function [a, b] = coeﬂagu(n)
if (n <= 1), disp(’ n must be > 1 ’); return; end
a=zeros(n,1); b=zeros(n,1); a(1)=1; b(1)=1;
for k=2:n, a(k)=2*(k-1)+1; b(k)=(k-1)ˆ2; end
Program 84 - coefherm : Coeﬃcients of Hermite polynomials
function [a, b] = coefherm(n)
if (n <= 1), disp(’ n must be > 1 ’); return; end
a=zeros(n,1); b=zeros(n,1); b(1)=sqrt(4.*atan(1.));
for k=2:n, b(k)=0.5*(k-1); end
Program 85 - zplege : Coeﬃcients of Gauss-Legendre formulae
function [x,w]=zplege(n)
if (n <= 1), disp(’ n must be > 1 ’); return; end
[a,b]=coeﬂege(n);
JacM=diag(a)+diag(sqrt(b(2:n)),1)+diag(sqrt(b(2:n)),-1);
[w,x]=eig(JacM); x=diag(x); scal=2; w=w(1,:)’.ˆ2*scal;
[x,ind]=sort(x); w=w(ind);
Program 86 - zplagu : Coeﬃcients of Gauss-Laguerre formulae
function [x,w]=zplagu(n)
if (n <= 1), disp(’ n must be > 1 ’); return; end
[a,b]=coeﬂagu(n);
JacM=diag(a)+diag(sqrt(b(2:n)),1)+diag(sqrt(b(2:n)),-1);
[w,x]=eig(JacM); x=diag(x); w=w(1,:)’.ˆ2;
Program 87 - zpherm : Coeﬃcients of Gauss-Hermite formulae
function [x,w]=zpherm(n)
if (n <= 1), disp(’ n must be > 1 ’); return; end

10.7 Approximation of a Function in the Least-Squares Sense
431
[a,b]=coefherm(n);
JacM=diag(a)+diag(sqrt(b(2:n)),1)+diag(sqrt(b(2:n)),-1);
[w,x]=eig(JacM); x=diag(x); scal=sqrt(pi); w=w(1,:)’.ˆ2*scal;
[x,ind]=sort(x); w=w(ind);
10.7
Approximation of a Function in the
Least-Squares Sense
Given a function f ∈L2(a, b), we look for a polynomial rn of degree ≤n
that satisﬁes
∥f −rn∥w = min
pn∈Pn∥f −pn∥w,
where w is a ﬁxed weight function in (a, b). Should it exist, rn is called a
least-squares polynomial. The name derives from the fact that, if w ≡1, rn
is the polynomial that minimizes the mean-square error E = ∥f −rn∥L2(a,b)
(see Exercise 8).
As seen in Section 10.1, rn coincides with the truncation fn of order
n of the Fourier series (see (10.2) and (10.3)). Depending on the choice
of the weight w(x), diﬀerent least-squares polynomials arise with diﬀerent
convergence properties.
Analogous to Section 10.1, we can introduce the discrete truncation f ∗
n
(10.4) of the Chebyshev series (setting pk = Tk) or the Legendre series
(setting pk = Lk). If the discrete scalar product induced by the Gauss-
Lobatto quadrature rule (10.28) is used in (10.5) then the ˜fk’s coincide with
the coeﬃcients of the expansion of the interpolating polynomial ΠGL
n,wf (see
(10.29) in the Chebyshev case, or (10.38) in the Legendre case).
Consequently, f ∗
n = ΠGL
n,wf, i.e., the discrete truncation of the (Cheby-
shev or Legendre) series of f turns out to coincide with the interpolating
polynomial at the n + 1 Gauss-Lobatto nodes. In particular, in such a case
(10.6) is trivially satisﬁed, since ∥f −f ∗
n∥n = 0.
10.7.1
Discrete Least-Squares Approximation
Several applications require representing in a synthetic way, using elemen-
tary functions, a large set of data that are available at a discrete level, for
instance, the results of experimental measurements. This approximation
process, often referred to as data ﬁtting, can be satisfactorily solved using
the discrete least-squares technique that can be formulated as follows.
Assume we are given m + 1 pairs of data
{(xi, yi), i = 0, . . . , m}
(10.43)

432
10. Orthogonal Polynomials in Approximation Theory
where yi may represent, for instance, the value of a physical quantity mea-
sured at the position xi. We assume that all the abscissae are distinct.
We look for a polynomial pn(x) =
n

i=0
aiϕi(x) such that
m

j=0
wj|pn(xj) −yj|2 ≤
m

j=0
wj|qn(xj) −yj|2
∀qn ∈Pn,
(10.44)
for suitable coeﬃcients wj > 0. If n = m the polynomial pn clearly co-
incides with the interpolating polynomial of degree n at the nodes {xi}.
Problem (10.44) is called a discrete least-squares problem since a discrete
scalar product is involved, and is the discrete counterpart of the contin-
uous least-squares problem. The solution pn is therefore referred to as a
least-squares polynomial. Notice that
|||q||| =



m

j=0
wj[q(xj)]2



1/2
(10.45)
is an essentially strict seminorm on Pn (see, Exercise 7). By deﬁnition
a discrete norm (or seminorm) ∥· ∥∗is essentially strict if ∥f + g∥∗=
∥f∥∗+ ∥g∥∗implies there exist nonnull α, β such that αf(xi) + βg(xi) = 0
for i = 0, . . . , m. Since ||| · ||| is an essentially strict seminorm, problem
(10.44) admits a unique solution (see, [IK66], Section 3.5). Proceeding as
in Section 3.13, we ﬁnd
n

k=0
ak
m

j=0
wjϕk(xj)ϕi(xj) =
m

j=0
wjyjϕi(xj),
∀i = 0, . . . , n,
which is called a system of normal equations, and can be conveniently
written in the form
BT Ba = BT y,
(10.46)
where B is the rectangular matrix (m+1)×(n+1) of entries bij = ϕj(xi), i =
0, . . . , m, j = 0, . . . , n, a ∈Rn+1 is the vector of the unknown coeﬃcients
and y ∈Rm+1 is the vector of data.
Notice that the system of normal equations obtained in (10.46) is of
the same nature as that introduced in Section 3.13 in the case of over-
determined systems. Actually, if wj = 1 for j = 0, . . . , m, the above system
can be regarded as the solution in the least-squares sense of the system
n

k=0
akϕk(xi) = yi,
i = 0, 1, . . . , m,

10.8 The Polynomial of Best Approximation
433
which would not admit a solution in the classical sense, since the number of
rows is greater than the number of columns. In the case n = 1, the solution
to (10.44) is a linear function, called linear regression for the data ﬁtting
of (10.43). The associated system of normal equations is
1

k=0
m

j=0
wjϕi(xj)ϕk(xj)ak =
m

j=0
wjϕi(xj)yj,
i = 0, 1.
Setting (f, g)m =
m

j=0
f(xj)g(xj) the previous system becomes
" (ϕ0, ϕ0)ma0 + (ϕ1, ϕ0)ma1 = (y, ϕ0)m,
(ϕ0, ϕ1)ma0 + (ϕ1, ϕ1)ma1 = (y, ϕ1)m,
where y(x) is a function that takes the value yi at the nodes xi, i =
0, . . . , m. After some algebra, we get this explicit form for the coeﬃcients
a0 = (y, ϕ0)m(ϕ1, ϕ1)m −(y, ϕ1)m(ϕ1, ϕ0)m
(ϕ1, ϕ1)m(ϕ0, ϕ0)m −(ϕ0, ϕ1)2m
,
a1 = (y, ϕ1)m(ϕ0, ϕ0)m −(y, ϕ0)m(ϕ1, ϕ0)m
(ϕ1, ϕ1)m(ϕ0, ϕ0)m −(ϕ0, ϕ1)2m
.
Example 10.3 As already seen in Example 8.2, small changes in the data can
give rise to large variations on the interpolating polynomial of a given function
f. This doesn’t happen for the least-squares polynomial where m is much larger
than n. As an example, consider the function f(x) = sin(2πx) in [−1, 1] and
evaluate it at the 22 equally spaced nodes xi = 2i/21, i = 0, . . . , 21, setting
fi = f(xi). Then, suppose to add to the data fi a random perturbation of the
order of 10−3 and denote by p5 and ˜p5 the least-squares polynomials of degree
5 approximating the data fi and ˜fi, respectively. The maximum norm of the
diﬀerence p5 −˜p5 over [−1, 1] is of the order of 10−3, i.e., it is of the same order
as the perturbation on the data. For comparison, the same diﬀerence in the case
of Lagrange interpolation is about equal to 2 as can be seen in Figure 10.2.
•
10.8
The Polynomial of Best Approximation
Consider a function f ∈C0([a, b]). A polynomial p∗
n ∈Pn is said to be the
polynomial of best approximation of f if it satisﬁes
∥f −p∗
n∥∞= min
pn∈Pn∥f −pn∥∞,
∀pn ∈Pn
(10.47)
where ∥g∥∞= maxa≤x≤b |g(x)|. This problem is referred to as a minimax
approximation, as we are looking for the minimum error measured in the
maximum norm.

434
10. Orthogonal Polynomials in Approximation Theory
−1
−0.5
0
0.5
1
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
FIGURE 10.2. The perturbed data (circles), the associated least-squares polyno-
mial of degree 5 (solid line) and the Lagrange interpolating polynomial (dashed
line)
Property 10.1 (Chebyshev equioscillation theorem) For any n ≥0,
the polynomial of best approximation p∗
n of f exists and is unique. More-
over, in [a, b] there exist n + 2 points x0 < x1 < . . . < xn+1 such that
f(xj) −p∗
n(xj) = σ(−1)jE∗
n(f),
j = 0, . . . , n + 1
with σ = 1 or σ = −1 depending on f and n, and E∗
n(f) = ∥f −p∗
n∥∞.
(For the proof, see [Dav63], Chapter 7). As a consequence, there exist n+1
points ˜x0 < ˜x1 < . . . < ˜xn, with xk < ˜xk < xk+1 for k = 0, . . . , n, to be
determined in [a, b] such that
p∗
n(˜xj) = f(˜xj),
j = 0, 1, . . . , n,
so that the best approximation polynomial is a polynomial of degree n that
interpolates f at n + 1 unknown nodes.
The following result yields an estimate of E∗
n(f) without explicitly com-
puting p∗
n (we refer for the proof to [Atk89], Chapter 4).
Property 10.2 (de la Vall´ee-Poussin theorem) Let n ≥0 and let x0 <
x1 < . . . < xn+1 be n + 2 points in [a, b]. If there exists a polynomial qn of
degree ≤n such that
f(xj) −qn(xj) = (−1)jej
j = 0, 1, . . . , n + 1
where all ej have the same sign and are non null, then
min
0≤j≤n+1|ej| ≤E∗
n(f).
We can now relate E∗
n(f) with the interpolation error. Indeed,
∥f −Πnf∥∞≤∥f −p∗
n∥∞+ ∥p∗
n −Πnf∥∞.

10.9 Fourier Trigonometric Polynomials
435
On the other hand, using the Lagrange representation of p∗
n we get
∥p∗
n −Πnf∥∞= ∥
n

i=0
(p∗
n(xi) −f(xi))li∥∞≤∥p∗
n −f∥∞∥
n

i=0
|li|∥∞,
from which it follows
∥f −Πnf∥∞≤(1 + Λn)E∗
n(f),
where Λn is the Lebesgue constant (8.11) associated with the nodes {xi}.
Thanks to (10.25) we can conclude that the Lagrange interpolating poly-
nomial on the Chebyshev nodes is a good approximation of p∗
n. The above
results yield a characterization of the best approximation polynomial, but
do not provide a constructive way for generating it. However, starting from
the Chebyshev equioscillation theorem, it is possible to devise an algorithm,
called the Remes algorithm, that is able to construct an arbitrarily good
approximation of the polynomial p∗
n (see [Atk89], Section 4.7).
10.9
Fourier Trigonometric Polynomials
Let us apply the theory developed in the previous sections to a particular
family of orthogonal polynomials which are no longer algebraic polynomials
but rather trigonometric. The Fourier polynomials on (0, 2π) are deﬁned
as
ϕk(x) = eikx,
k = 0, ±1, ±2, . . .
where i is the imaginary unit. These are complex-valued periodic functions
with period equal to 2π. We shall use the notation L2(0, 2π) to denote the
complex-valued functions that are square integrable over (0, 2π). Therefore
L2(0, 2π) =
%
f : (0, 2π) →C such that
> 2π
0
|f(x)|2dx < ∞
&
with scalar product and norm deﬁned respectively by
(f, g) =
 2π
0
f(x)g(x)dx,
∥f∥L2(0,2π) =

(f, f).
If f ∈L2(0, 2π), its Fourier series is
Ff =
∞

k=−∞
fkϕk,
with fk = 1
2π
2π
>
0
f(x)e−ikxdx = 1
2π (f, ϕk). (10.48)
If f is complex-valued we set f(x) = α(x) + iβ(x) for x ∈[0, 2π], where
α(x) is the real part of f and β(x) is the imaginary one. Recalling that

436
10. Orthogonal Polynomials in Approximation Theory
e−ikx = cos(kx) −i sin(kx) and letting
ak = 1
2π
2π
>
0
[α(x) cos(kx) + β(x) sin(kx)] dx
bk = 1
2π
2π
>
0
[−α(x) sin(kx) + β(x) cos(kx)] dx,
the Fourier coeﬃcients of the function f can be written as
fk = ak + ibk
∀k = 0, ±1, ±2, . . . .
(10.49)
We shall assume henceforth that f is a real-valued function; in such a case
f−k = fk for any k.
Let N be an even positive integer. Analogously to what was done in
Section 10.1, we call the truncation of order N of the Fourier series the
function
f ∗
N(x) =
N
2 −1

k=−N
2
fkeikx.
The use of capital N instead of small n is to conform with the notation usu-
ally adopted in the analysis of discrete Fourier series (see [Bri74], [Wal91]).
To simplify the notations we also introduce an index shift so that
f ∗
N(x) =
N−1

k=0
fkei(k−N
2 )x,
where now
fk = 1
2π
2π
>
0
f(x)e−i(k−N/2)xdx = 1
2π (f, $ϕk), k = 0, . . . , N −1
(10.50)
and $ϕk = e−i(k−N/2)x. Denoting by
SN = span{$ϕk, 0 ≤k ≤N −1},
if f ∈L2(0, 2π) its truncation of order N satisﬁes the following optimal
approximation property in the least-squares sense
∥f −f ∗
N∥L2(0,2π) = min
g∈SN∥f −g∥L2(0,2π).
Set h = 2π/N and xj = jh, for j = 0, . . . , N −1, and introduce the
following discrete scalar product
(f, g)N = h
N−1

j=0
f(xj)g(xj).
(10.51)

10.9 Fourier Trigonometric Polynomials
437
Replacing (f, $ϕk) in (10.50) with (f, $ϕk)N, we get the discrete Fourier
coeﬃcients of the function f
$fk = 1
N
N−1

j=0
f(xj)e−ikjheijπ = 1
N
N−1

j=0
f(xj)W
(k−N
2 )j
N
(10.52)
for k = 0, . . . , N −1, where
WN = exp

−i2π
N

is the principal root of order N of unity. According to (10.4), the trigono-
metric polynomial
ΠF
Nf(x) =
N−1

k=0
$fkei(k−N
2 )x
(10.53)
is called the discrete Fourier series of order N of f.
Lemma 10.1 The following property holds
(ϕl, ϕj)N = h
N−1

k=0
e−ik(l−j)h = 2πδjl,
0 ≤l, j ≤N −1,
(10.54)
where δjl is the Kronecker symbol.
Proof. For l = j the result is immediate. Thus, assume l ̸= j; we have that
N−1

k=0
e−ik(l−j)h =
1 −
+
e−i(l−j)h,N
1 −e−i(l−j)h
= 0.
Indeed, the numerator is 1 −(cos(2π(l −j)) −i sin(2π(l −j))) = 1 −1 = 0,
while the denominator cannot vanish. Actually, it vanishes iﬀ(j −l)h = 2π, i.e.,
j −l = N, which is impossible.
3
Thanks to Lemma 10.1, the trigonometric polynomial ΠF
Nf is the Fourier
interpolate of f at the nodes xj, that is
ΠF
Nf(xj) = f(xj),
j = 0, 1, . . . , N −1.
Indeed, using (10.52) and (10.54) in (10.53) it follows that
ΠF
Nf(xj) =
N−1

k=0
$fkeikjhe−ijh N
2 =
N−1

l=0
f(xl)
.
1
N
N−1

k=0
e−ik(l−j)h
/
= f(xj).

438
10. Orthogonal Polynomials in Approximation Theory
Therefore, looking at the ﬁrst and last equality, we get
f(xj) =
N−1

k=0
$fkeik(j−N
2 )h =
N−1

k=0
$fkW
−(j−N
2 )k
N
, j = 0, . . . , N −1. (10.55)
The mapping {f(xj)} →{ $fk} described by (10.52) is called the Discrete
Fourier Transform (DFT), while the mapping (10.55) from { $fk} to {f(xj)}
is called the inverse transform (IDFT). Both DFT and IDFT can be written
in matrix form as { $fk} = T{f(xj)} and {f(xj)} = C{ $fk} where T ∈
CN×N, C denotes the inverse of T and
Tkj = 1
N W
(k−N
2 )j
N
,
k, j = 0, . . . , N −1,
Cjk = W
−(j−N
2 )k
N
,
j, k = 0, . . . , N −1.
A naive implementation of the matrix-vector computation in the DFT and
IDFT would require N 2 operations. Using the FFT (Fast Fourier Trans-
form) algorithm only O(N log2 N) ﬂops are needed, provided that N is a
power of 2, as will be explained in Section 10.9.2.
The function ΠF
Nf ∈SN introduced in (10.53) is the solution of the
minimization problem ∥f −ΠF
Nf∥N ≤∥f −g∥N for any g ∈SN, where
∥· ∥N = (·, ·)1/2
N
is a discrete norm for SN. In the case where f is periodic
with all its derivatives up to order s (s ≥1), an error estimate analogous
to that for Chebyshev and Legendre interpolation holds
∥f −ΠF
Nf∥L2(0,2π) ≤CN −s∥f∥s
and also
max
0≤x≤2π|f(x) −ΠF
Nf(x)| ≤CN 1/2−s∥f∥s.
In a similar manner, we also have
|(f, vN) −(f, vN)N| ≤CN −s∥f∥s∥vN∥
for any vN ∈SN, and in particular, setting vN = 1 we have the following
error for the quadrature formula (10.51)

2π
>
0
f(x)dx −h
N−1

j=0
f(xj)

≤CN −s∥f∥s
(see for the proof [CHQZ88], Chapter 2).
Notice that h
N−1

j=0
f(xj) is nothing else than the composite trapezoidal
rule for approximating the integral
 2π
0
f(x)dx. Therefore, such a formula

10.9 Fourier Trigonometric Polynomials
439
turns out to be extremely accurate when dealing with periodic and smooth
integrands.
Programs 88 and 89 provide an implementation of the DFT and IDFT. The
input parameter f is a string containing the function f to be transformed
while fc is a vector of size N containing the values $fk.
Program 88 - dft : Discrete Fourier transform
function fc = dft(N,f)
h = 2*pi/N; x=[0:h:2*pi*(1-1/N)]; fx = eval(f); wn = exp(-i*h);
for k=0:N-1,
s = 0;
for j=0:N-1
s = s + fx(j+1)*wnˆ((k-N/2)*j);
end
fc (k+1) = s/N;
end
Program 89 - idft : Inverse discrete Fourier transform
function fv = idft(N,fc)
h = 2*pi/N; wn = exp(-i*h);
for k=0:N-1
s = 0;
for j=0:N-1
s = s + fc(j+1)*wnˆ(-k*(j-N/2));
end
fv (k+1) = s;
end
10.9.1
The Gibbs Phenomenon
Consider the discontinuous function f(x) = x/π for x ∈[0, π] and equal to
x/π −2 for x ∈(π, 2π], and compute its DFT using Program 88. The inter-
polate ΠF
Nf is shown in Figure 10.3 (above) for N = 8, 16, 32. Notice the
spurious oscillations around the point of discontinuity of f whose maximum
amplitude, however, tends to a ﬁnite limit. The arising of these oscillations
is known as Gibbs phenomenon and is typical of functions with isolated
jump discontinuities; it aﬀects the behavior of the truncated Fourier series
not only in the neighborhood of the discontinuity but also over the entire
interval, as can be clearly seen in the ﬁgure. The convergence rate of the
truncated series for functions with jump discontinuities is linear in N −1 at
every given non-singular point of the interval of deﬁnition of the function
(see [CHQZ88], Section 2.1.4).

440
10. Orthogonal Polynomials in Approximation Theory
0
1
2
3
4
5
6
−1
−0.5
0
0.5
1
0
1
2
3
4
5
6
−1
−0.5
0
0.5
1
FIGURE 10.3. Above: Fourier interpolate of the sawtooth function (thick solid
line) for N = 8 (dash-dotted line), 16 (dashed line) and 32 (thin solid line).
Below: the same informations are plotted in the case of the Lanczos smoothing
Since the Gibbs phenomenon is related to the slow decay of the Fourier
coeﬃcients of a discontinuous function, smoothing procedures can be prof-
itably employed to attenuate the higher-order Fourier coeﬃcients. This can
be done by multiplying each coeﬃcient $fk by a factor σk such that σk is a
decreasing function of k. An example is provided by the Lanczos smoothing
σk = sin(2(k −N/2)(π/N))
2(k −N/2)(π/N)
,
k = 0, . . . , N −1.
(10.56)
The eﬀect of applying the Lanczos smoothing to the computation of the
DFT of the above function f is represented in Figure 10.3 (below), which
shows that the oscillations have almost completely disappeared.
For a deeper analysis of this subject we refer to [CHQZ88], Chapter 2.
10.9.2
The Fast Fourier Transform
As pointed out in the previous section, computing the discrete Fourier
transform (DFT) or its inverse (IDFT) as a matrix-vector product, would
require N 2 operations. In this section we illustrate the basic steps of the
Cooley-Tukey algorithm [CT65], commonly known as Fast Fourier Trans-
form (FFT). The computation of a DFT of order N is split into DFTs of
order p0, . . . , pm, where {pi} are the prime factors of N. If N is a power of
2, the computational cost has the order of N log2 N ﬂops.
A recursive algorithm to compute the DFT when N is a power of 2
is described in the following. Let f = (fi)T , i = 0, . . . , N −1 and set
p(x) =
1
N
N−1

j=0
fjxj. Then, computing the DFT of the vector f amounts to

10.9 Fourier Trigonometric Polynomials
441
evaluating p(W
k−N
2
N
) for k = 0, . . . , N −1. Let us introduce the polynomials
pe(x) = 1
N
6
f0 + f2x + . . . + fN−2x
N
2 −17
,
po(x) = 1
N
6
f1 + f3x + . . . + fN−1x
N
2 −17
.
Notice that
p(x) = pe(x2) + xpo(x2)
from which it follows that the computation of the DFT of f can be carried
out by evaluating the polynomials pe and po at the points W
2(k−N
2 )
N
, k =
0, . . . , N −1. Since
W
2(k−N
2 )
N
= W 2k−N
N
= exp

−i 2πk
N/2

exp(i2π) = W k
N/2,
it turns out that we must evaluate pe and po at the principal roots of unity
of order N/2. In this manner the DFT of order N is rewritten in terms
of two DFTs of order N/2; of course, we can recursively apply again this
procedure to po and pe. The process is terminated when the degree of the
last generated polynomials is equal to one.
In Program 90 we propose a simple implementation of the FFT recursive
algorithm. The input parameters are the vector f containing the NN values
fk, where NN is a power of 2.
Program 90 - ﬀtrec : FFT algorithm in the recursive version
function [ﬀtv]=ﬀtrec(f,NN)
N = length(f);
w = exp(-2*pi*sqrt(-1)/N);
if N == 2
ﬀtv = f(1)+w.ˆ[-NN/2:NN-1-NN/2]*f(2);
else
a1 = f(1:2:N);
b1 = f(2:2:N);
a2 = ﬀtrec(a1,NN); b2 = ﬀtrec(b1,NN);
for k=-NN/2:NN-1-NN/2
ﬀtv(k+1+NN/2) = a2(k+1+NN/2) + b2(k+1+NN/2)*wˆk;
end
end
Remark 10.4 A FFT procedure can also be set up when N is not a power
of 2. The simplest approach consists of adding some zero samples to the
original sequence {fi} in such a way to obtain a total number of ˜N = 2p
values. This technique, however, does not necessarily yield the correct re-
sult. Therefore, an eﬀective alternative is based on partitioning the Fourier
matrix C into subblocks of smaller size. Practical FFT implementations
can handle both strategies (see, for instance, the fft package available in
MATLAB).
■

442
10. Orthogonal Polynomials in Approximation Theory
10.10
Approximation of Function Derivatives
A problem which is often encountered in numerical analysis is the ap-
proximation of the derivative of a function f(x) on a given interval [a, b].
A natural approach to it consists of introducing in [a, b] n + 1 nodes
{xk, k = 0, . . . , n}, with x0 = a, xn = b and xk+1 = xk+h, k = 0, . . . , n−1
where h = (b −a)/n. Then, we approximate f ′(xi) using the nodal values
f(xk) as
h
m

k=−m
αkui−k =
m′

k=−m′
βkf(xi−k),
(10.57)
where {αk}, {βk} ∈R are m + m′ + 1 coeﬃcients to be determined and uk
is the desired approximation to f ′(xk).
A non negligible issue in the choice of scheme (10.57) is the computa-
tional eﬃciency. Regarding this concern, it is worth noting that, if m ̸= 0,
determining the values {ui} requires the solution of a linear system.
The set of nodes which are involved in constructing the derivative of f at
a certain node, is called a stencil. The band of the matrix associated with
system (10.57) increases as the stencil gets larger.
10.10.1
Classical Finite Diﬀerence Methods
The simplest way to generate a formula like (10.57) consists of resorting to
the deﬁnition of the derivative. If f ′(xi) exists, then
f ′(xi) = lim
h→0+
f(xi + h) −f(xi)
h
.
(10.58)
Replacing the limit with the incremental ratio, with h ﬁnite, yields the
approximation
uF D
i
= f(xi+1) −f(xi)
h
,
0 ≤i ≤n −1.
(10.59)
Relation (10.59) is a special instance of (10.57) setting m = 0, α0 = 1,
m′ = 1, β−1 = 1, β0 = −1, β1 = 0.
The right side of (10.59) is called the forward ﬁnite diﬀerence and the
approximation that is being used corresponds to replacing f ′(xi) with
the slope of the straight line passing through the points (xi, f(xi)) and
(xi+1, f(xi+1)), as shown in Figure 10.4.
To estimate the error that is made, it suﬃces to expand f in Taylor’s
series, obtaining
f(xi+1) = f(xi) + hf ′(xi) + h2
2 f ′′(ξi)
with ξi ∈(xi, xi+1).

10.10 Approximation of Function Derivatives
443
We assume henceforth that f has the required regularity, so that
f ′(xi) −uF D
i
= −h
2 f ′′(ξi).
(10.60)
f(xi)
f(xi−1)
f(xi+1)
xi−1
xi
xi+1
FIGURE 10.4. Finite diﬀerence approximation of f ′(xi): backward (solid line),
forward (pointed line) and centred (dashed line)
Obviously, instead of (10.58) we could employ a centred incremental
ratio, obtaining the following approximation
uCD
i
= f(xi+1) −f(xi−1)
2h
,
1 ≤i ≤n −1.
(10.61)
Scheme (10.61) is a special instance of (10.57) setting m = 0, α0 = 1,
m′ = 1, β−1 = 1/2, β0 = 0, β1 = −1/2.
The right side of (10.61) is called the centred ﬁnite diﬀerence and geometri-
cally amounts to replacing f ′(xi) with the slope of the straight line passing
through the points (xi−1, f(xi−1)) and (xi+1, f(xi+1)) (see Figure 10.4).
Resorting again to Taylor’s series, we get
f ′(xi) −uCD
i
= −h2
6 f ′′′(ξi).
(10.62)
Formula (10.61) thus provides a second-order approximation to f ′(xi) with
respect to h.
Finally, with a similar procedure, we can derive a backward ﬁnite diﬀer-
ence scheme, where
uBD
i
= f(xi) −f(xi−1)
h
,
1 ≤i ≤n,
(10.63)

444
10. Orthogonal Polynomials in Approximation Theory
which is aﬀected by the following error
f ′(xi) −uBD
i
= h
2 f ′′(ξi).
(10.64)
The values of the parameters in (10.57) are m = 0, α0 = 1, m′ = 1 and
β−1 = 0, β0 = 1, β1 = −1.
Higher-order schemes, as well as ﬁnite diﬀerence approximations of higher-
order derivatives of f, can be constructed using Taylor’s expansions of
higher order. A remarkable example is the approximation of f ′′; if f ∈
C4([a, b]) we easily get
f ′′(xi)
= f(xi+1) −2f(xi) + f(xi−1)
h2
−h2
24
+
f (4)(xi + θih) + f (4)(xi −ωih)
,
,
0 < θi, ωi < 1.
The following centred ﬁnite diﬀerence scheme can thus be derived
u′′
i = f(xi+1) −2f(xi) + f(xi−1)
h2
,
1 ≤i ≤n −1
(10.65)
which is aﬀected by the error
f ′′(xi) −u′′
i = −h2
24
+
f (4)(xi + θih) + f (4)(xi −ωih)
,
.
(10.66)
Formula (10.65) provides a second-order approximation to f ′′(xi) with re-
spect to h.
10.10.2
Compact Finite Diﬀerences
More accurate approximations are provided by using the following formula
(which we call compact diﬀerences)
αui−1 + ui + αui+1 = β
2h(fi+1 −fi−1) + γ
4h(fi+2 −fi−2)
(10.67)
for i = 2, . . . , n −1. We have set, for brevity, fi = f(xi).
The coeﬃcients α, β and γ are to be determined in such a way that the
relations (10.67) yield values ui that approximate f ′(xi) up to the highest
order with respect to h. For this purpose, the coeﬃcients are selected in
such a way as to minimize the consistency error (see Section 2.2)
σi(h)
= αf (1)
i−1 + f (1)
i
−αf (1)
i+1
−
 β
2h(fi+1 −fi−1) + γ
4h(fi+2 −fi−2)

(10.68)

10.10 Approximation of Function Derivatives
445
which comes from “forcing” f to satisfy the numerical scheme (10.67). For
brevity, we set f (k)
i
= f (k)(xi), k = 1, 2, . . . .
Precisely, assuming that f ∈C5([a, b]) and expanding it in a Taylor’s
series around xi, we ﬁnd
fi±1 = fi ± hf (1)
i
+ h2
2 f (2)
i
± h3
6 f (3)
i
+ h4
24 f (4)
i
± h5
120f (5)
i
+ O(h6),
f (1)
i±1 = f (1)
i
± hf (2)
i
+ h2
2 f (3)
i
± h3
6 f (4)
i
+ h4
24 f (5)
i
+ O(h5).
Substituting into (10.68) we get
σi(h) = (2α + 1)f (1)
i
+ αh2
2 f (3)
i
+ αh4
12f (5)
i
−(β + γ)f (1)
i
−h2
2
β
6 + 2γ
3

f (3)
i
−h4
60
β
2 + 8γ

f (5)
i
+ O(h6).
Second-order methods are obtained by equating to zero the coeﬃcient of
f (1)
i
, i.e., if 2α + 1 = β + γ, while we obtain schemes of order 4 by equating
to zero also the coeﬃcient of f (3)
i
, yielding 6α = β+4γ and ﬁnally, methods
of order 6 are obtained by setting to zero also the coeﬃcient of f (5)
i
, i.e.,
10α = β + 16γ.
The linear system formed by these last three equations has a nonsingular
matrix. Thus, there exists a unique scheme of order 6 that corresponds to
the following choice of the parameters
α = 1/3,
β = 14/9,
γ = 1/9,
(10.69)
while there exist inﬁnitely many methods of second and fourth order.
Among these inﬁnite methods, a popular scheme has coeﬃcients α = 1/4,
β = 3/2 and γ = 0. Schemes of higher order can be generated at the
expense of furtherly expanding the computational stencil.
Traditional ﬁnite diﬀerence schemes correspond to setting α = 0 and
allow for computing explicitly the approximant of the ﬁrst derivative of f
at a node, in contrast with compact schemes which are required in any case
to solve a linear system of the form Au = Bf (where the notation has the
obvious meaning).
To make the system solvable, it is necessary to provide values to the vari-
ables ui with i < 0 and i > n. A particularly favorable instance is that
where f is a periodic function of period b −a, in which case ui+n = ui
for any i ∈Z. In the nonperiodic case, system (10.67) must be supplied
by suitable relations at the nodes near the boundary of the approximation
interval. For example, the ﬁrst derivative at x0 can be computed using the
relation
u0 + αu1 = 1
h(Af1 + Bf2 + Cf3 + Df4),

446
10. Orthogonal Polynomials in Approximation Theory
and requiring that
A = −3 + α + 2D
2
,
B = 2 + 3D,
C = −1 −α + 6D
2
,
in order for the scheme to be at least second-order accurate (see [Lel92]
for the relations to enforce in the case of higher-order methods). Finally,
we notice that, for any given order of accuracy, compact schemes have a
stencil smaller than the one of standard ﬁnite diﬀerences.
Program 91 provides an implementation of the compact ﬁnite diﬀerence
schemes (10.67) for the approximation of the derivative of a given function f
which is assumed to be periodic on the interval [a, b). The input parameters
alpha, beta and gamma contain the coeﬃcients of the scheme, a and b are
the endpoints of the interval, f is a string containing the expression of f
and n denotes the number of subintervals in which [a, b] is partitioned. The
output vectors u and x contain the computed approximate values ui and
the node coordinates. Notice that setting alpha=gamma=0 and beta=1 we
recover the centered ﬁnite diﬀerence approximation (10.61).
Program 91 - compdiﬀ: Compact diﬀerence schemes
function [u, x] = compdiﬀ(alpha,beta,gamma,a,b,n,f)
h=(b-a)/(n+1); x=[a:h:b]; fx = eval(f);
A
= eye(n+2)+alpha*diag(ones(n+1,1),1)+alpha*diag(ones(n+1,1),-1);
rhs = 0.5*beta/h*(fx(4:n+1)-fx(2:n-1))+0.25*gamma/h*(fx(5:n+2)-fx(1:n-2));
if gamma == 0
rhs=[0.5*beta/h*(fx(3)-fx(1)), rhs, 0.5*beta/h*(fx(n+2)-fx(n))];
A(1,1:n+2) = zeros(1,n+2);
A(1,1) = 1; A(1,2)=alpha; A(1,n+1)=alpha;
rhs=[0.5*beta/h*(fx(2)-fx(n+1)), rhs];
A(n+2,1:n+2) = zeros(1,n+2);
A(n+2,n+2) = 1; A(n+2,n+1)=alpha; A(n+2,2)=alpha;
rhs=[rhs, 0.5*beta/h*(fx(2)-fx(n+1))];
else
rhs=[0.5*beta/h*(fx(3)-fx(1))+0.25*gamma/h*(fx(4)-fx(n+1)), rhs];
A(1,1:n+2) = zeros(1,n+2);
A(1,1) = 1; A(1,2)=alpha; A(1,n+1)=alpha;
rhs=[0.5*beta/h*(fx(2)-fx(n+1))+0.25*gamma/h*(fx(3)-fx(n)), rhs];
rhs=[rhs,0.5*beta/h*(fx(n+2)-fx(n))+0.25*gamma/h*(fx(2)-fx(n-1))];
A(n+2,1:n+2) = zeros(1,n+2);
A(n+2,n+2) = 1; A(n+2,n+1)=alpha; A(n+2,2)=alpha;
rhs=[rhs,0.5*beta/h*(fx(2)-fx(n+1))+0.25*gamma/h*(fx(3)-fx(n))];
end
u = A \ rhs’;
return
Example 10.4 Let us consider the approximate evaluation of the derivative of
the function f(x) = sin(x) on the interval [0, 2π]. Figure 10.5 shows the loga-

10.10 Approximation of Function Derivatives
447
rithm of the maximum nodal errors for the second-order centered ﬁnite diﬀer-
ence scheme (10.61) and of the fourth and sixth-order compact diﬀerence schemes
introduced above, as a function of p = log(n).
•
4
8
16
32
64
10
−10
10
−8
10
−6
10
−4
10
−2
10
0
FIGURE 10.5. Maximum nodal errors for the second-order centred ﬁnite diﬀer-
ence scheme (solid line) and for the fourth (dashed line) and sixth-order (dotted
line) compact diﬀerence schemes as functions of p = log(n)
Another nice feature of compact schemes is that they maximize the range
of well-resolved waves as we are going to explain. Assume that f is a real
and periodic function on [0, 2π], that is, f(0) = f(2π). Using the same
notation as in Section 10.9, we let N be an even positive integer and set
h = 2π/N. Then replace f by its truncated Fourier series
f ∗
N(x) =
N/2−1

k=−N/2
fkeikx.
Since the function f is real-valued, fk = ¯f −k for k = 1, . . . , N/2 and
f0 = ¯f 0. For sake of convenience, introduce the normalized wave number
wk = kh = 2πk/N and perform a scaling of the coordinates setting s = x/h.
As a consequence, we get
f ∗
N(x(s)) =
N/2−1

k=−N/2
fkeiksh =
N/2−1

k=−N/2
fkeiwks.
(10.70)
Taking the ﬁrst derivative of (10.70) with respect to s yields a function
whose Fourier coeﬃcients are f ′
k = iwk fk. We can thus estimate the ap-
proximation error on (f ∗
N)′ by comparing the exact coeﬃcients f ′
k with the
corresponding ones obtained by an approximate derivative, in particular,
by comparing the exact wave number wk with the approximate one, say
wk,app.

448
10. Orthogonal Polynomials in Approximation Theory
Let us neglect the subscript k and perform the comparison over the whole
interval [0, π) where wk is varying. It is clear that methods based on the
Fourier expansion have wapp = w if w ̸= π (wapp = 0 if w = π). The family
of schemes (10.67) is instead characterized by the wave number
wapp(z) = a sin(z) + (b/2) sin(2z) + (c/3) sin(3z)
1 + 2α cos(z) + 2β cos(2z)
,
z ∈[0, π)
(see [Lel92]). Figure 10.6 displays a comparison among wave numbers of
several schemes, of compact and non compact type.
The range of values for which the wave number computed by the numer-
ical scheme adequately approximates the exact wave number, is the set of
well-resolved waves. As a consequence, if wmin is the smallest well-resolved
wave, the diﬀerence 1 −wmin/π represents the fraction of waves that are
unresolved by the numerical scheme. As can be seen in Figure 10.6, the
standard ﬁnite diﬀerence schemes approximate correctly the exact wave
number only for small wave numbers.
0.5
1
1.5
2
2.5
3
0
0.5
1
1.5
2
2.5
3
(a)
(b)
(c)
(d)
FIGURE 10.6. Computed wave numbers for centred ﬁnite diﬀerences (10.61) (a)
and for compact schemes of fourth (b), sixth (c) and tenth (d) order, compared
with the exact wave number (the straight line). On the x axis the normalized
coordinate s is represented
10.10.3
Pseudo-Spectral Derivative
An alternative way for numerical diﬀerentiation consists of approximating
the ﬁrst derivative of a function f with the exact ﬁrst derivative of the
polynomial Πnf interpolating f at the nodes {x0, . . . , xn}.
Exactly as happens for Lagrange interpolation, using equally spaced
nodes does not yield stable approximations to the ﬁrst derivative of f for
n large. For this reason, we limit ourselves to considering the case where
the nodes are nonuniformly distributed according to the Gauss-Lobatto-
Chebyshev formula.

10.10 Approximation of Function Derivatives
449
For simplicity, assume that I = [a, b] = [−1, 1] and for n ≥1, take in
I the Gauss-Lobatto-Chebyshev nodes as in (10.21). Then, consider the
Lagrange interpolating polynomial ΠGL
n,wf, introduced in Section 10.3. We
deﬁne the pseudo-spectral derivative of f ∈C0(I) to be the derivative of
the polynomial ΠGL
n,wf
Dnf = (ΠGL
n,wf)′ ∈Pn−1(I).
The error made in replacing f ′ with Dnf is of exponential type, that is, it
only depends on the smoothness of the function f. More precisely, there
exists a constant C > 0 independent of n such that
∥f ′ −Dnf∥w ≤Cn1−m∥f∥m,w,
(10.71)
for any m ≥2 such that the norm ∥f∥m,w, introduced in (10.23), is ﬁnite.
Recalling (10.19) and using (10.27) yields
(Dnf)(¯xi) =
n

j=0
f(¯xj)¯l′
j(¯xi),
i = 0, . . . , n,
(10.72)
so that the pseudo-spectral derivative at the interpolation nodes can be
computed knowing only the nodal values of f and of ¯l′
j. These values can
be computed once for all and stored in a matrix D ∈R(n+1)×(n+1): Dij =
¯l′
j(¯xi) for i, j = 0, ..., n, called a pseudo-spectral diﬀerentiation matrix.
Relation (10.72) can thus be cast in matrix form as f ′ = Df, letting
f = [f(¯xi)] and f ′ = [(Dnf)(¯xi)] for i = 0, ..., n.
The entries of D have the following explicit form (see [CHQZ88], p. 69)
Dlj =























dl
dj
(−1)l+j
¯xl −¯xj
,
l ̸= j,
−¯xj
2(1 −¯x2
j),
1 ≤l = j ≤n −1,
−2n2 + 1
6
,
l = j = 0,
2n2 + 1
6
,
l = j = n,
(10.73)
where the coeﬃcients dl have been deﬁned in Section 10.3 (see also Example
5.13 concerning the approximation of the multiple eigenvalue λ = 0 of D).
To compute the pseudo-spectral derivative of a function f over the generic
interval [a, b], we only have to resort to the change of variables considered
in Remark 10.3.
The second-order pseudo-spectral derivative can be computed as the
product of the matrix D and the vector f ′, that is, f ′′ = Df ′, or by di-
rectly applying matrix D2 to the vector f.

450
10. Orthogonal Polynomials in Approximation Theory
10.11
Transforms and Their Applications
In this section we provide a short introduction to the most relevant integral
transforms and discuss their basic analytical and numerical properties.
10.11.1
The Fourier Transform
Deﬁnition 10.1 Let L1(R) denote the space of real or complex functions
deﬁned on the real line such that
∞
>
−∞
|f(t)| dt < +∞.
For any f ∈L1(R) its Fourier transform is a complex-valued function
F = F[f] deﬁned as
F(ν) =
∞
>
−∞
f(t)e−i2πνt dt.
■
Should the independent variable t denote time, then ν would have the
meaning of frequency. Thus, the Fourier transform is a mapping that to a
function of time (typically, real-valued) associates a complex-valued func-
tion of frequency.
The following result provides the conditions under which an inversion for-
mula exists that allows us to recover the function f from its Fourier trans-
form F (for the proof see [Rud83], p. 199).
Property 10.3 (inversion theorem) Let f be a given function in L1(R),
F ∈L1(R) be its Fourier transform and g be the function deﬁned by
g(t) =
∞
>
−∞
F(ν)ei2πνt dν,
t ∈R.
(10.74)
Then g ∈C0(R), with lim|x|→∞g(x) = 0, and f(t) = g(t) almost every-
where in R (i.e., for any t unless possibly a set of zero measure).
The integral at right hand side of (10.74) is to be meant in the Cauchy
principal value sense, i.e., we let
∞
>
−∞
F(ν)ei2πνt dν = lim
a→∞
a
>
−a
F(ν)ei2πνt dν

10.11 Transforms and Their Applications
451
and we call it the inverse Fourier transform or inversion formula of the
Fourier transform. This mapping that associates to the complex function
F the generating function f will be denoted by F−1[F], i.e., F = F[f] iﬀ
f = F−1[F].
Let us brieﬂy summarize the main properties of the Fourier transform and
its inverse.
1. F and F−1 are linear operators, i.e.
F[αf + βg] = αF[f] + βF[g],
∀α, β ∈C,
F−1[αF + βG] = αF−1[F] + βF−1[G],
∀α, β ∈C;
(10.75)
2. scaling: if α is any nonzero real number and fα is the function fα(t) =
f(αt), then
F[fα] = 1
|α|F 1
α
where F 1
α (ν) = F(ν/α);
3. duality: let f(t) be a given function and F(ν) be its Fourier trans-
form. Then the function g(t) = F(−t) has a Fourier transform given
by f(ν). Thus, once an associated function-transform pair is found,
another dual pair is automatically generated. An application of this
property is provided by the pair r(t)-F[r] in Example 10.5;
4. parity: if f(t) is a real even function then F(ν) is real and even, while
if f(t) is a real and odd function then F(ν) is imaginary and odd.
This property allows one to work only with nonnegative frequencies;
5. convolution and product: for any given functions f, g ∈L1(R), we
have
F[f ∗g] = F[f]F[g],
F[fg] = F ∗G,
(10.76)
where the convolution integral of two functions φ and ψ is given by
(φ ∗ψ)(t) =
∞
>
−∞
φ(τ)ψ(t −τ) dτ.
(10.77)
Example 10.5 We provide two examples of the computation of the Fourier
transforms of functions that are typically encountered in signal processing.
Let us ﬁrst consider the square wave (or rectangular) function r(t) deﬁned as
r(t) =
" A
if −T
2 ≤t ≤T
2 ,
0
otherwise,

452
10. Orthogonal Polynomials in Approximation Theory
where T and A are two given positive numbers. Its Fourier transform F[r] is the
function
F(ν) =
T/2
>
−T/2
Ae−i2πνt dt = AT sin(πνT)
πνT
,
ν ∈R
where AT is the area of the rectangular function.
Let us consider the sawtooth function
s(t) =



2At
T
if −T
2 ≤t ≤T
2 ,
0
otherwise,
whose DFT is shown in Figure 10.3 and whose Fourier transform F[s] is the
function
F(ν) = i AT
πνT

cos(πνT) −sin(πνT)
πνT

,
ν ∈R
and is purely imaginary since s is an odd real function. Notice also that the
functions r and s have a ﬁnite support whereas their transforms have an inﬁnite
support (see Figure 10.7). In signal theory this corresponds to saying that the
transform has an inﬁnite bandwidth.
•
−10
−8
−6
−4
−2
0
2
4
6
8
10
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
,
−10
−8
−6
−4
−2
0
2
4
6
8
10
−0.5
−0.4
−0.3
−0.2
−0.1
0
0.1
0.2
0.3
0.4
0.5
FIGURE 10.7. Fourier transforms of the rectangular (left) and the sawtooth
(right) functions
Example 10.6 The Fourier transform of a sinusoidal function is of paramount
interest in signal and communication systems. To start with, consider the constant
function f(t) = A for a given A ∈R. Since it has an inﬁnite time duration its
Fourier transform F[A] is the function
F(ν) = lim
a→∞
a
>
−a
Ae−i2πνt dt = A lim
a→∞
sin(2πνa)
πν
,
where the integral above is again the Cauchy principal value of the corresponding
integral over (−∞, ∞). It can be proved that the limit exists and is unique in the
sense of distributions (see Section 12.4) yielding
F(ν) = Aδ(ν),
(10.78)

10.11 Transforms and Their Applications
453
where δ is the so-called Dirac mass, i.e., a distribution that satisﬁes
> ∞
−∞
δ(ξ)φ(ξ) dξ = φ(0)
(10.79)
for any function φ continuous at the origin. From (10.78) we see that the trans-
form of a function with inﬁnite time duration has a ﬁnite bandwidth.
Let us now consider the computation of the Fourier transform of the function
f(t) = A cos(2πν0t) where ν0 is a ﬁxed frequency. Recalling Euler’s formula
cos(θ) = eiθ + e−iθ
2
,
θ ∈R,
and applying (10.78) twice we get
F[A cos(2πν0t)] = A
2 δ(ν −ν0) + A
2 δ(ν + ν0),
which shows that the spectrum of a sinusoidal function with frequency ν0 is
centred around ±ν0 (notice that the transform is even and real since the same
holds for the function f(t)).
•
It is worth noting that in real-life there do not exist functions (i.e. signals)
with inﬁnite duration or bandwidth. Actually, if f(t) is a function whose
value may be considered as “negligible” outside of some interval (ta, tb),
then we can assume that the eﬀective duration of f is the length ∆t =
tb −ta. In a similar manner, if F(ν) is the Fourier transform of f and
it happens that F(ν) may be considered as “negligible” outside of some
interval (νa, νb), then the eﬀective bandwidth of f is ∆ν = νb−νa. Referring
to Figure 10.7, we clearly see that the eﬀective bandwidth of the rectangular
function can be taken as (−10, 10).
10.11.2
(Physical) Linear Systems and Fourier Transform
Mathematically speaking, a physical linear system (LS) can be regarded
as a linear operator S that enjoys the linearity property (10.75). Denoting
by i(t) and u(t) an admissible input function for S and its corresponding
output function respectively, the LS can be represented as u(t) = S(i(t))
or S : i →u. A special category of LS are the so-called shift invariant (or
time-invariant) linear systems (ILS) which satisfy the property
S(i(t −t0)) = u(t −t0),
∀t0 ∈R
and for any admissible input function i.
Let S be an ILS system and let f and g be two admissible input functions
for S with w = S(g). An immediate consequence of the linearity and shift-
invariance is that
S((f ∗g)(t)) = (f ∗S(g))(t) = (f ∗w)(t)
(10.80)

454
10. Orthogonal Polynomials in Approximation Theory
where ∗is the convolution operator deﬁned in (10.77).
Assume we take as input function the impulse function δ(t) introduced in
the previous section and denote by h(t) = S(δ(t)) the corresponding output
through S (usually referred to as the system impulse response function).
Property (10.79) implies that for any function φ, (φ ∗δ)(t) = φ(t), so that,
recalling (10.80) and taking φ(t) = i(t) we have
u(t) = S(i(t)) = S(i ∗δ)(t) = (i ∗S(δ))(t) = (i ∗h)(t).
Thus, S can be completely described through its impulse response function.
Equivalently, we can pass to the frequency domain by means of the ﬁrst
relation in (10.76) obtaining
U(ν) = I(ν)H(ν),
(10.81)
where I, U and H are the Fourier transforms of i(t), u(t) and h(t), respec-
tively; H is the so-called system transfer function.
Relation (10.81) plays a central role in the analysis of linear time-invariant
systems as it is simpler to deal with the system transfer function than with
the corresponding impulse response function, as demonstrated in the fol-
lowing example.
Example 10.7 (ideal low-pass ﬁlter) An ideal low-pass ﬁlter is an ILS char-
acterized by the transfer function
H(ν) =
% 1,
if |ν| ≤ν0/2,
0,
otherwise.
Using the duality property, the impulse response function F−1[H] is
h(t) = ν0 sin(πν0t)
πν0t
.
Given an input signal i(t) with Fourier transform I(ν), the corresponding output
u(t) has a spectrum given by (10.81)
I(ν)H(ν) =
% I(ν),
if |ν| ≤ν0/2,
0
otherwise.
The eﬀect of the ﬁlter is to cut oﬀthe input frequencies that lie outside the
window |ν| ≤ν0/2.
•
The input/output functions i(t) and u(t) usually denote signals and
the linear system described by H(ν) is typically a communication system.
Therefore, as pointed out at the end of Section 10.11.1, we are legitimated
in assuming that both i(t) and u(t) have a ﬁnite eﬀective duration. In

10.11 Transforms and Their Applications
455
particular, referring to i(t) we suppose i(t) = 0 if t ̸∈[0, T0). Then, the
computation of the Fourier transform of i(t) yields
I(ν) =
T0
>
0
i(t)e−i2πνt dt.
Letting ∆t = T0/n for n ≥1 and approximating the integral above by the
composite trapezoidal formula (9.14), we get
˜I(ν) = ∆t
n−1

k=0
i(k∆t)e−i2πνk∆t.
It can be proved (see, e.g., [Pap62]) that ˜I(ν)/∆t is the Fourier transform
of the so-called sampled signal
is(t) =
∞

k=−∞
i(k∆t)δ(t −k∆t),
where δ(t −k∆t) is the Dirac mass at k∆t. Then, using the convolution
and the duality properties of the Fourier transform, we get
˜I(ν) =
∞

j=−∞
I

ν −j
∆t

,
(10.82)
which amounts to replacing I(ν) by its periodic repetition with period
1/∆t. Let J∆t = [−
1
2∆t,
1
2∆t]; then, it suﬃces to compute (10.82) for ν ∈
J∆t. This can be done numerically by introducing a uniform discretization
of J∆t with frequency step ν0 = 1/(m∆t) for m ≥1. By doing so, the
computation of ˜I(ν) requires evaluating the following m+1 discrete Fourier
transforms (DFT)
˜I(jν0) = ∆t
n−1

k=0
i(k∆t)e−i2πjν0k∆t,
j = −m
2 , . . . , m
2 .
For an eﬃcient computation of each DFT in the formula above it is crucial
to use the FFT algorithm described in Section 10.9.2.
10.11.3
The Laplace Transform
The Laplace transform can be employed to solve ordinary diﬀerential equa-
tions with constant coeﬃcients as well as partial diﬀerential equations.
Deﬁnition 10.2 Let f ∈L1
loc([0, ∞)) i.e., f ∈L1([0, T]) for any T > 0.
Let s = σ + iω be a complex variable. The Laplace integral of f is deﬁned

456
10. Orthogonal Polynomials in Approximation Theory
as
∞
>
0
f(t)e−st dt = lim
T →∞
T
>
0
f(t)e−st dt.
If this integral exists for some s, it turns out to be a function of s; then,
the Laplace transform L[f] of f is the function
L(s) =
∞
>
0
f(t)e−st dt.
■
The following relation between Laplace and Fourier transforms holds
L(s) = F(e−σt ˜f(t)),
where ˜f(t) = f(t) if t ≥0 while ˜f(t) = 0 if t < 0.
Example 10.8 The Laplace transform of the unit step function f(t) = 1 if t > 0,
f(t) = 0 otherwise, is given by
L(s) =
∞
>
0
e−st dt = 1
s .
We notice that the Laplace integral exists if σ > 0.
•
In Example 10.8 the convergence region of the Laplace integral is the half-
plane {Re(s) > 0} of the complex ﬁeld. This property is quite general, as
stated by the following result.
Property 10.4 If the Laplace transform exists for s = ¯s then it exists
for all s with Re(s) > Re(¯s). Moreover, let E be the set of the real parts
of s such that the Laplace integral exists and denote by λ the inﬁmum of
E. If λ happens to be ﬁnite, the Laplace integral exists in the half-plane
Re(s) > λ. If λ = −∞then it exists for all s ∈C; λ is called the abscissa
of convergence.
We recall that the Laplace transform enjoys properties completely analo-
gous to those of the Fourier transform. The inverse Laplace transform is
denoted formally as L−1 and is such that
f(t) = L−1[L(s)].

10.11 Transforms and Their Applications
457
Example 10.9 Let us consider the ordinary diﬀerential equation y′(t)+ay(t) =
g(t) with y(0) = y0. Multiplying by est, integrating between 0 and ∞and passing
to the Laplace transform, yields
sY (s) −y0 + aY (s) = G(s).
(10.83)
Should G(s) be easily computable, (10.83) would furnish Y (s) and then, by ap-
plying the inverse Laplace transform, the generating function y(t). For instance,
if g(t) is the unit step function, we obtain
y(t) = L−1
% 1
a
1
s −
1
s + a

+
y0
s + a
&
= 1
a(1 −e−at) + y0e−at.
•
For an extensive presentation and analysis of the Laplace transform see,
e.g., [Tit37]. In the next section we describe a discrete version of the Laplace
transform, known as the Z-transform.
10.11.4
The Z-Transform
Deﬁnition 10.3 Let f be a given function, deﬁned for any t ≥0, and
∆t > 0 be a given time step. The function
Z(z) =
∞

n=0
f(n∆t)z−n,
z ∈C
(10.84)
is called the Z-transform of the sequence {f(n∆t)} and is denoted by
Z[f(n∆t)].
■
The parameter ∆t is the sampling time step of the sequence of samples
f(n∆t). The inﬁnite sum (10.84) converges if
|z| > R = lim sup
n→∞
n
|f(n∆t)|.
It is possible to deduce the Z-transform from the Laplace transform as
follows. Denoting by f0(t) the piecewise constant function such that f0(t) =
f(n∆t) for t ∈(n∆t, (n + 1)∆t), the Laplace transform L[f0] of f0 is the
function
L(s)
=
∞
>
0
f0(t)e−st dt =
∞

n=0
(n+1)∆t
>
n∆t
e−stf(n∆t) dt
=
∞

n=0
f(n∆t)e−ns∆t −e−(n+1)s∆t
s
=
1 −e−s∆t
s
 ∞

n=0
f(n∆t)e−ns∆t.

458
10. Orthogonal Polynomials in Approximation Theory
The discrete Laplace transform Zd[f0] of f0 is the function
Zd(s) =
∞

n=0
f(n∆t)e−ns∆t.
Then, the Z-transform of the sequence {f(n∆t), n = 0, . . . , ∞} coincides
with the discrete Laplace transform of f0 up to the change of variable
z = e−s∆t. The Z-transform enjoys similar properties (linearity, scaling,
convolution and product) to those already seen in the continuous case.
The inverse Z-transform is denoted by Z−1 and is deﬁned as
f(n∆t) = Z−1[Z(z)].
The practical computation of Z−1 can be carried out by resorting to classi-
cal techniques of complex analysis (for example, using the Laurent formula
or the Cauchy theorem for residual integral evaluation) coupled with an
extensive use of tables (see, e.g., [Pou96]).
10.12
The Wavelet Transform
This technique, originally developed in the area of signal processing, has
successively been extended to many diﬀerent branches of approximation
theory, including the solution of diﬀerential equations. It is based on the
so-called wavelets, which are functions generated by an elementary wavelet
through traslations and dilations. We shall limit ourselves to a brief intro-
duction of univariate wavelets and their transform in both the continuous
and discrete cases referring to [DL92], [Dau88] and to the references cited
therein for a detailed presentation and analysis.
10.12.1
The Continuous Wavelet Transform
Any function
hs,τ(t) = 1
√sh
t −τ
s

,
t ∈R
(10.85)
that is obtained from a reference function h ∈L2(R) by means of traslations
by a traslation factor τ and dilations by a positive scaling factor s is called
a wavelet. The function h is called an elementary wavelet.
Its Fourier transform, written in terms of ω = 2πν, is
Hs,τ(ω) = √sH(sω)e−iωτ,
(10.86)
where i denotes the imaginary unit and H(ω) is the Fourier transform of
the elementary wavelet. A dilation t/s (s > 1) in the real domain produces

10.12 The Wavelet Transform
459
therefore a contraction sω in the frequency domain. Therefore, the factor
1/s plays the role of the frequency ν in the Fourier transform (see Section
10.11.1). In wavelets theory s is usually referred to as the scale. Formula
(10.86) is known as the ﬁlter of the wavelet transform.
Deﬁnition 10.4 Given a function f ∈L2(R), its continuous wavelet trans-
form Wf = W[f] is a decomposition of f(t) onto a wavelet basis {hs,τ(t)},
that is
Wf(s, τ) =
∞
>
−∞
f(t)¯hs,τ(t) dt,
(10.87)
where the overline bar denotes complex conjugate.
■
When t denotes the time-variable, the wavelet transform of f(t) is a func-
tion of the two variables s (scale) and τ (time shift); as such, it is a repre-
sentation of f in the time-scale space and is usually referred to as time-scale
joint representation of f. The time-scale representation is the analogue of
the time-frequency representation introduced in the Fourier analysis. This
latter representation has an intrinsic limitation: the product of the res-
olution in time ∆t and the resolution in frequency ∆ω must satisfy the
following constraint (Heisenberg inequality)
∆t∆ω ≥1
2
(10.88)
which is the counterpart of the Heisenberg uncertainty principle in quantum
mechanics. This inequality states that a signal cannot be represented as
a point in the time-frequency space. We can only determine its position
within a rectangle of area ∆t∆ω in the time-frequency space.
The wavelet transform (10.87) can be rewritten in terms of the Fourier
transform F(ω) of f as
Wf(s, τ) =
√s
2π
∞
>
−∞
F(ω) ¯H(sω)eiωτ dω,
which shows that the wavelets transform is a bank of wavelet ﬁlters char-
acterized by diﬀerent scales. More precisely, if the scale is small the wavelet
is concentrated in time and the wavelet transform provides a detailed de-
scription of f(t) (which is the signal). Conversely, if the scale is large, the
wavelet transform is able to resolve only the large-scale details of f. Thus,
the wavelet transform can be regarded as a bank of multiresolution ﬁlters.
The theoretical properties of this transform do not depend on the partic-
ular elementary wavelet that is considered. Hence, speciﬁc bases of wavelets
can be derived for speciﬁc applications. Some examples of elementary wave-
lets are reported below.

460
10. Orthogonal Polynomials in Approximation Theory
Example 10.10 (Haar wavelets) These functions can be obtained by choos-
ing as the elementary wavelet the Haar function deﬁned as
h(x) =



1
if x ∈(0, 1
2),
−1
if x ∈( 1
2, 1),
0
otherwise.
Its Fourier transform is the complex-valued function
H(ω) = 4ie−iω/2 +
1 −cos(ω
2 )
,
/ω,
which has symmetric module with respect to the origin (see Figure 10.8). The
bases that are obtained from this wavelet are not used in practice due to their
ineﬀective localization properties in the frequency domain.
•
−0.5
0
0.5
1
1.5
−1.5
−1
−0.5
0
0.5
1
1.5
−80
−60
−40
−20
0
20
40
60
80
0
0.5
1
1.5
FIGURE 10.8. The Haar wavelet (left) and the module of its Fourier transform
(right)
Example 10.11 (Morlet wavelets) The Morlet wavelet is deﬁned as follows
(see [MMG87])
h(x) = eiω0xe−x2/2.
Thus, it is a complex-valued function whose real part has a real positive Fourier
transform, symmetric with respect to the origin, given by
H(ω) = √π
6
e−(ω−ω0)2/2 + e−(ω+ω0)2/27
.
•
We point out that the presence of the dilation factor allows for the wavelets
to easily handle possible discontinuities or singularities in f. Indeed, using
the multi-resolution analysis, the signal, properly divided into frequency
bandwidths, can be processed at each frequency by suitably tuning up the
scale factor of the wavelets.

10.12 The Wavelet Transform
461
−10
−8
−6
−4
−2
0
2
4
6
8
10
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
−10
−8
−6
−4
−2
0
2
4
6
8
10
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
FIGURE 10.9. The real part of the Morlet wavelet (left) and the real part of the
corresponding Fourier transforms (right) for ω0 = 1 (solid line), ω0 = 2.5 (dashed
line) and ω0 = 5 (dotted line)
Recalling what was already pointed out in Section 10.11.1, the time lo-
calization of the wavelet gives rise to a ﬁlter with inﬁnite bandwidth. In
particular, deﬁning the bandwidth ∆ω of the wavelet ﬁlter as
∆ω =


∞
>
−∞
ω2|H(ω)|2 dω/
∞
>
−∞
|H(ω)|2 dω


2
,
then the bandwidth of the wavelet ﬁlter with scale equal to s is
∆ωs =


∞
>
−∞
ω2|H(sω)|2 dω/
∞
>
−∞
|H(sω)|2 dω


2
= 1
s∆ω.
Consequently, the quality factor Q of the wavelet ﬁlter, deﬁned as the in-
verse of the bandwidth of the ﬁlter, is independent of s since
Q = 1/s
∆ωs
= ∆ω
provided that (10.88) holds. At low frequencies, corresponding to large
values of s, the wavelet ﬁlter has a small bandwidth and a large temporal
width (called window) with a low resolution. Conversely, at high frequencies
the ﬁlter has a large bandwidth and a small temporal window with a high
resolution. Thus, the resolution furnished by the wavelet analysis increases
with the frequency of the signal. This property of adaptivity makes the
wavelets a crucial tool in the analysis of unsteady signals or signals with
fast transients for which the standard Fourier analysis turns out to be
ineﬀective.
10.12.2
Discrete and Orthonormal Wavelets
The continuous wavelet transform maps a function of one variable into a bi-
dimensional representation in the time-scale domain. In many applications

462
10. Orthogonal Polynomials in Approximation Theory
this description is excessively rich. Resorting to the discrete wavelets is
an attempt to represent a function using a ﬁnite (and small) number of
parameters.
A discrete wavelet is a continuous wavelet that is generated by using
discrete scale and translation factors. For s0 > 1, denote by s = sj
0 the
scale factors; the dilation factors usually depend on the scale factors by
setting τ = kτ0sj
0, τ0 ∈R. The corresponding discrete wavelet is
hj,k(t) = s−j/2
0
h(s−j
0 (t −kτ0sj
0)) = s−j/2
0
h(s−j
0 t −kτ0).
The scale factor sj
0 corresponds to the magniﬁcation or the resolution of
the observation, while the translation factor τ0 is the location where the
observations are made. If one looks at very small details, the magniﬁcation
must be large, which corresponds to large negative index j. In this case the
step of translation is small and the wavelet is very concentrated around the
observation point. For large and positive j, the wavelet is spread out and
large translation steps are used.
The behavior of the discrete wavelets depends on the steps s0 and τ0.
When s0 is close to 1 and τ0 is small, the discrete wavelets are close to the
continuous ones. For a ﬁxed scale s0 the localization points of the discrete
wavelets along the scale axis are logarithmic as log s = j log s0. The choice
s0 = 2 corresponds to the dyadic sampling in frequency. The discrete time-
step is τ0sj
0 and, typically, τ0 = 1. Hence, the time-sampling step is a
function of the scale and along the time axis the localization points of the
wavelet depend on the scale.
For a given function f ∈L1(R), the corresponding discrete wavelet trans-
form is
Wf(j, k) =
∞
>
−∞
f(t)¯hj,k(t) dt.
It is possible to introduce an orthonormal wavelet basis using discrete di-
lation and traslation factors, i.e.
∞
>
−∞
hi,j¯hk,l(t) dt = δikδjl,
∀i, j, k, l ∈Z.
With an orthogonal wavelet basis, an arbitrary function f can be recon-
structed by the expansion
f(t) = A

j,k∈Z
Wf(j, k)hj,k(t),
where A is a constant that does not depend on f.
As of the computational standpoint, the wavelet discrete transform can
be implemented at even a cheaper cost than the FFT algorithm for com-
puting the Fourier transform.

10.13 Applications
463
10.13
Applications
In this section we apply the theory of orthogonal polynomials to solve two
problems arising in quantum physics. In the ﬁrst example we deal with
Gauss-Laguerre quadratures, while in the second case the Fourier analysis
and the FFT are considered.
10.13.1
Numerical Computation of Blackbody Radiation
The monochromatic energy density E(ν) of blackbody radiation as a func-
tion of frequency ν is expressed by the following law
E(ν) = 8πh
c3
ν3
ehν/KBT −1,
where h is the Planck constant, c is the speed of light, KB is the Boltz-
mann constant and T is the absolute temperature of the blackbody (see,
for instance, [AF83]).
To compute the total density of monochromatic energy that is emitted
by the blackbody (that is, the emitted energy per unit volume) we must
evaluate the integral
E =
∞
>
0
E(ν)dν = αT 4
∞
>
0
x3
ex −1dx,
where x = hν/KBT and α = (8πK4
B)/(ch)3 ≃1.16 · 10−16 [J][K−4][m−3].
We also let f(x) = x3/(ex −1) and I(f) =
 ∞
0
f(x)dx.
To approximate I(f) up to a previously ﬁxed absolute error ≤δ, we com-
pare method 1. introduced in Section 9.8.3 with Gauss-Laguerre quadra-
tures.
In the case of method 1. we proceed as follows. For any a > 0 we let
I(f) =
 a
0 f(x)dx +
 ∞
a f(x)dx and try to ﬁnd a function φ such that
∞
>
a
f(x)dx ≤
∞
>
a
φ(x)dx ≤δ
2,
(10.89)
the integral
 ∞
a φ(x)dx being “easy” to compute. Once the value of a
has been found such that (10.89) is fulﬁlled, we compute the integral
I1(f) =
 a
0 f(x)dx using for instance the adaptive Cavalieri-Simpson for-
mula introduced in Section 9.7.2 and denoted in the following by AD.
A natural choice of a bounding function for f is φ(x) = Kx3e−x, for a
suitable constant K > 1. Thus, we have K ≥ex/(ex −1), for any x > 0,
that is, letting x = a, K = ea/(ea−1). Substituting back into (10.89) yields
∞
>
a
f(x)dx ≤a3 + 3a2 + 6a + 6
ea −1
= η(a) ≤δ
2.

464
10. Orthogonal Polynomials in Approximation Theory
0
2
4
6
8
10
12
14
16
−0.2
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
FIGURE 10.10. Distribution of quadrature nodes and graph of the integrand
function
Letting δ = 10−3, we see that (10.89) is satisﬁed by taking a ≃16. Pro-
gram 77 for computing I1(f) with the AD method, setting hmin=10−3 and
tol=5 · 10−4, yields the approximate value I1 ≃6.4934 with a number of
(nonuniform) partitions equal to 25.
The distribution of the quadrature nodes produced by the adaptive algo-
rithm is plotted in Figure 10.10. Globally, using method 1. yields an approx-
imation of I(f) equal to 6.4984. Table 10.1 shows, for sake of comparison,
some approximate values of I(f) obtained using the Gauss-Laguerre formu-
lae with the number of nodes varying between 2 to 20. Notice that, taking
n = 4 nodes, the accuracy of the two computational procedures is roughly
equivalent.
n
In(f)
2
6.413727469517582
3
6.481130171540022
4
6.494535639802632
5
6.494313365790864
10
6.493939967652101
15
6.493939402671590
20
6.493939402219742
TABLE 10.1. Approximate evaluation of I(f)
=
 ∞
0
x3/(ex −1)dx with
Gauss-Laguerre quadratures
10.13.2
Numerical Solution of Schr¨odinger Equation
Let us consider the following diﬀerential equation arising in quantum me-
chanics known as the Schr¨odinger equation
i∂ψ
∂t = −ℏ
2m
∂2ψ
∂x2 ,
x ∈R
t > 0.
(10.90)
The symbols i and ℏdenote the imaginary unit and the reduced Planck
constant, respectively. The complex-valued function ψ = ψ(x, t), the solu-

10.13 Applications
465
tion of (10.90), is called a wave function and the quantity |ψ(x, t)|2 deﬁnes
the probability density in the space x of a free electron of mass m at time
t (see [FRL55]).
The corresponding Cauchy problem may represent a physical model for
describing the motion of an electron in a cell of an inﬁnite lattice (for more
details see, e.g., [AF83]).
Consider the initial condition ψ(x, 0) = w(x), where w is the step func-
tion that takes the value 1/
√
2b for |x| ≤b and is zero for |x| > b, with
b = a/5, and where 2a represents the inter-ionic distance in the lattice.
Therefore, we are searching for periodic solutions, with period equal to 2a.
Solving problem (10.90) can be carried out using Fourier analysis as
follows. We ﬁrst write the Fourier series of w and ψ (for any t > 0)
w(x) =
N/2−1

k=−N/2
wkeiπkx/a,
wk = 1
2a
a
>
−a
w(x)e−iπkx/adx,
ψ(x, t) =
N/2−1

k=−N/2
ψk(t)eiπkx/a,
ψk(t) = 1
2a
a
>
−a
ψ(x, t)e−iπkx/adx.
(10.91)
Then, we substitute back (10.91) into (10.90), obtaining the following
Cauchy problem for the Fourier coeﬃcients ψk, for k = −N/2, . . . , N/2−1





ψ′
k(t) = −i ℏ
2m
kπ
a
2
ψk(t),
ψk(0) = $wk.
(10.92)
The coeﬃcients {$wk} have been computed by regularizing the coeﬃcients
{ wk} of the step function w using the Lanczos smoothing (10.56) in order
to avoid the Gibbs phenomenon arising around the discontinuities of w (see
Section 10.9.1).
After solving (10.92), we ﬁnally get, recalling (10.91), the following expres-
sion for the wave function
ψN(x, t) =
N/2−1

k=−N/2
$wke−iEkt/ℏeiπkx/a,
(10.93)
where the coeﬃcients Ek = (k2π2ℏ2)/(2ma2) represent, from the physical
standpoint, the energy levels that the electron may assume in its motion
within the potential well.
To compute the coeﬃcients wk (and, as a consequence, $wk), we have used
the MATLAB intrinsic function fft (see Section 10.9.2), employing N =
26 = 64 points and letting a = 10
◦
A= 10−9[m]. Time analysis has been
carried out up to T = 10 [s], with time steps of 1 [s]; in all the reported

466
10. Orthogonal Polynomials in Approximation Theory
−10
−5
0
5
10
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
−10
−5
0
5
10
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
FIGURE 10.11. Probability density |ψ(x, t)|2 at t = 0, 2, 5 [s], corresponding to
a step function as initial datum: solution without ﬁltering (left), with Lanczos
ﬁltering (right)
graphs, the x-axis is measured in [
◦
A], while the y-axes are respectively in
units of 105 [m−1/2] and 1010 [m−1].
In Figure 10.11 we draw the probability density |ψ(x, t)|2 at t = 0, 2
and 5 [s]. The result obtained without the regularizing procedure above is
shown on the left, while the same calculation with the “ﬁltering” of the
Fourier coeﬃcients is reported on the right. The second plot demonstrates
the smoothing eﬀect on the solution by the regularization, at the price of
a slight enlargement of the step-like initial probability distribution.
Finally, it is interesting to apply Fourier analysis to solve problem (10.90)
starting from a smooth initial datum. For this, we choose an initial probabil-
ity density w of Gaussian form such that ∥w∥2 = 1. The solution |ψ(x, t)|2,
this time computed without regularization, is shown in Figure 10.12, at
t = 0, 2, 5, 7, 9[s]. Notice the absence of spurious oscillations with respect
to the previous case.
−10
−5
0
5
10
0
0.02
0.04
0.06
0.08
0.1
0.12
0.14
0.16
0.18
0.2
FIGURE 10.12. Probability density |ψ(x, t)|2 at t = 0, 2, 5, 7, 9[s], corresponding
to an initial datum with Gaussian form

10.14 Exercises
467
10.14
Exercises
1. Prove the three-term relation (10.11).
[Hint: set x = cos(θ), for 0 ≤θ ≤π.]
2. Prove (10.31).
[Hint: ﬁrst prove that ∥vn∥n = (vn, vn)1/2, ∥Tk∥n = ∥Tk∥w for k < n and
∥Tn∥2
n = 2∥Tn∥2
w (see [QV94], formula (4.3.16)). Then, the thesis follows
from (10.29) multiplying by Tl (l ̸= k) and taking (·, ·)n.]
3. Prove (10.24) after showing that ∥(f −ΠGL
n f)′∥ω ≤Cn1−s∥f∥s,ω.
[Hint: use the Gagliardo-Nirenberg inequality
max
−1≤x≤1|f(x)| ≤∥f∥1/2∥f ′∥1/2
valid for any f ∈L2 with f ′ ∈L2. Next, use the relation that has been just
shown to prove (10.24).]
4. Prove that the discrete seminorm ∥f∥n = (f, f)1/2
n
is a norm for Pn.
5. Compute weights and nodes of the following quadrature formulae
b
>
a
w(x)f(x)dx =
n

i=0
ωif(xi),
in such a way that the order is maximum, setting
ω(x) = √x,
a = 0,
b = 1,
n = 1;
ω(x) = 2x2 + 1,
a = −1,
b = 1,
n = 0;
ω(x) =
% 2
if 0 < x ≤1,
1
if
−1 ≤x ≤0
a = −1,
b = 1,
n = 1.
[Solution: for ω(x) = √x, the nodes x1 = 5
9 + 2
9

10/7, x2 = 5
9 −2
9

10/7
are obtained, from which the weights can be computed (order 3); for ω(x) =
2x2 + 1, we get x1 = 3/5 and ω1 = 5/3 (order 1); for ω(x) = 2x2 + 1, we
have x1 =
1
22 +
1
22
√
155, x2 =
1
22 −
1
22
√
155 (order 3).]
6. Prove (10.40).
[Hint: notice that (ΠGL
n f, Lj)n = 
k f ∗
k(Lk, Lj)n = . . . , distinguishing the
case j < n from the case j = n.]
7. Show that ||| · |||, deﬁned in (10.45), is an essentially strict seminorm.
[Solution : use the Cauchy-Schwarz inequality (1.14) to check that the
triangular inequality is satisﬁed. This proves that |||·||| is a seminorm. The
second part of the exercise follows after a direct computation.]
8. Consider in an interval [a, b] the nodes
xj = a +

j −1
2
  b −a
m

j = 1, 2, . . . , m

468
10. Orthogonal Polynomials in Approximation Theory
for m ≥1. They are the midpoints of m equally spaced intervals in [a, b].
Let f be a given function; prove that the least-squares polynomial rn with
respect to the weight w(x) = 1 minimizes the error average, deﬁned as
E = lim
m→∞
"
1
m
m

j=1
[f(xj) −rn(xj)]2
#1/2
.
9. Consider the function
F(a0, a1, . . . , an) =
1
>
0
.
f(x) −
n

j=0
ajxj
/2
dx
and determine the coeﬃcients a0, a1, . . . , an in such a way that F is mini-
mized. Which kind of linear system is obtained?
[Hint: enforce the conditions ∂F/∂ai = 0 with i = 0, 1, . . . , n. The matrix
of the ﬁnal linear system is the Hilbert matrix (see Example 3.2, Chapter
3) which is strongly ill-conditioned.]

11
Numerical Solution of Ordinary
Diﬀerential Equations
In this chapter we deal with the numerical solutions of the Cauchy problem
for ordinary diﬀerential equations (henceforth abbreviated by ODEs). After
a brief review of basic notions about ODEs, we introduce the most widely
used techniques for the numerical approximation of scalar equations. The
concepts of consistency, convergence, zero-stability and absolute stability
will be addressed. Then, we extend our analysis to systems of ODEs, with
emphasis on stiﬀproblems.
11.1
The Cauchy Problem
The Cauchy problem (also known as the initial-value problem) consists of
ﬁnding the solution of an ODE, in the scalar or vector case, given suitable
initial conditions. In particular, in the scalar case, denoting by I an interval
of R containing the point t0, the Cauchy problem associated with a ﬁrst
order ODE reads:
ﬁnd a real-valued function y ∈C1(I), such that
" y′(t) = f(t, y(t)),
t ∈I,
y(t0) = y0,
(11.1)
where f(t, y) is a given real-valued function in the strip S = I ×(−∞, +∞),
which is continuous with respect to both variables. Should f depend on t
only through y, the diﬀerential equation is called autonomous.

470
11. Numerical Solution of Ordinary Diﬀerential Equations
Most of our analysis will be concerned with one single diﬀerential equa-
tion (scalar case). The extension to the case of systems of ﬁrst-order ODEs
will be addressed in Section 11.9.
If f is continuous with respect to t, then the solution to (11.1) satisﬁes
y(t) −y0 =
t
>
t0
f(τ, y(τ))dτ.
(11.2)
Conversely, if y is deﬁned by (11.2), then it is continuous in I and y(t0) =
y0. Moreover, since y is a primitive of the continuous function f(·, y(·)),
y ∈C1(I) and satisﬁes the diﬀerential equation y′(t) = f(t, y(t)).
Thus, if f is continuous the Cauchy problem (11.1) is equivalent to the
integral equation (11.2). We shall see later on how to take advantage of
this equivalence in the numerical methods.
Let us now recall two existence and uniqueness results for (11.1).
1. Local existence and uniqueness.
Suppose that f(t, y) is locally Lipschitz continuous at (t0, y0) with
respect to y, that is, there exist two neighborhoods, J ⊆I of t0 of
width rJ, and Σ of y0 of width rΣ, and a constant L > 0, such that
|f(t, y1) −f(t, y2)| ≤L|y1 −y2|
∀t ∈J, ∀y1, y2 ∈Σ.
(11.3)
Then, the Cauchy problem (11.1) admits a unique solution in a neigh-
borhood of t0 with radius r0 with 0 < r0 < min(rJ, rΣ/M, 1/L),
where M is the maximum of |f(t, y)| on J ×Σ. This solution is called
the local solution.
Notice that condition (11.3) is automatically satisﬁed if f has con-
tinuous derivative with respect to y: indeed, in such a case it suﬃces
to choose L as the maximum of |∂f(t, y)/∂y| in J × Σ.
2. Global existence and uniqueness. The problem admits a unique
global solution if one can take J = I and Σ = R in (11.3), that is, if
f is uniformly Lipschitz continuous with respect to y.
In view of the stability analysis of the Cauchy problem, we consider the
following problem
" z′(t) = f(t, z(t)) + δ(t),
t ∈I,
z(t0) = y0 + δ0,
(11.4)
where δ0 ∈R and δ is a continuous function on I. Problem (11.4) is derived
from (11.1) by perturbing both the initial datum y0 and the source func-
tion f. Let us now characterize the sensitivity of the solution z to those
perturbations.

11.1 The Cauchy Problem
471
Deﬁnition 11.1 ([Hah67], [Ste71] or [PS91]). Let I be a bounded set. The
Cauchy problem (11.1) is stable in the sense of Liapunov (or stable) on I
if, for any perturbation (δ0, δ(t)) satisfying
|δ0| < ε,
|δ(t)| < ε
∀t ∈I,
with ε > 0 suﬃciently small to guarantee that the solution to the perturbed
problem (11.4) does exist, then
∃C > 0 independent of ε such that
|y(t) −z(t)| < Cε,
∀t ∈I.
(11.5)
If I has no upperly bound we say that (11.1) is asymptotically stable if, as
well as being Liapunov stable in any bounded interval I, the following limit
also holds
|y(t) −z(t)| →0,
for t →+∞.
(11.6)
■
The requirement that the Cauchy problem is stable is equivalent to requir-
ing that it is well-posed in the sense stated in Chapter 2.
The uniform Lipschitz-continuity of f with respect to y suﬃces to ensure
the stability of the Cauchy problem. Indeed, letting w(t) = z(t) −y(t), we
have
w′(t) = f(t, z(t)) −f(t, y(t)) + δ(t).
Therefore,
w(t) = δ0 +
t
>
t0
[f(s, z(s)) −f(s, y(s))] ds +
t
>
t0
δ(s)ds,
∀t ∈I.
Thanks to previous assumptions, it follows that
|w(t)| ≤(1 + |t −t0|) ε + L
t
>
t0
|w(s)|ds.
Applying the Gronwall lemma (which we include below for the reader’s
ease) yields
|w(t)| ≤(1 + |t −t0|) εeL|t−t0|,
∀t ∈I
and, thus, (11.5) with C = (1 + KI)eLKI where KI = maxt∈I |t −t0|.
Lemma 11.1 (Gronwall) Let p be an integrable function nonnegative on
the interval (t0, t0 + T), and let g and ϕ be two continuous functions on
[t0, t0 + T], g being nondecreasing. If ϕ satisﬁes the inequality
ϕ(t) ≤g(t) +
t
>
t0
p(τ)ϕ(τ)dτ,
∀t ∈[t0, t0 + T],

472
11. Numerical Solution of Ordinary Diﬀerential Equations
then
ϕ(t) ≤g(t) exp


t
>
t0
p(τ)dτ

,
∀t ∈[t0, t0 + T].
For the proof, see, for instance, [QV94], Lemma 1.4.1.
The constant C that appears in (11.5) could be very large and, in general,
depends on the upper extreme of the interval I, as in the proof above.
For that reason, the property of asymptotic stability is more suitable for
describing the behavior of the dynamical system (11.1) as t →+∞(see
[Arn73]).
As is well-known, only a restricted number of nonlinear ODEs can be
solved in closed form (see, for instance, [Arn73]). Moreover, even when
this is possible, it is not always a straightforward task to ﬁnd an explicit
expression of the solution; for example, consider the (very simple) equation
y′ = (y−t)/(y+t), whose solution is only implicitly deﬁned by the relation
(1/2) log(t2 + y2) + tan−1(y/t) = C, where C is a constant depending on
the initial condition.
For this reason we are interested in numerical methods, since these can
be applied to any ODE under the sole condition that it admits a unique
solution.
11.2
One-Step Numerical Methods
Let us address the numerical approximation of the Cauchy problem (11.1).
Fix 0 < T < +∞and let I = (t0, t0 + T) be the integration interval and,
correspondingly, for h > 0, let tn = t0 + nh, with n = 0, 1, 2, . . . , Nh, be
the sequence of discretization nodes of I into subintervals In = [tn, tn+1].
The width h of such subintervals is called the discretization stepsize. Notice
that Nh is the maximum integer such that tNh ≤t0 + T. Let uj be the
approximation at node tj of the exact solution y(tj); this solution will be
henceforth shortly denoted by yj. Similarly, fj denotes the value f(tj, uj).
We obviously set u0 = y0.
Deﬁnition 11.2 A numerical method for the approximation of problem
(11.1) is called a one-step method if ∀n ≥0, un+1 depends only on un.
Otherwise, the scheme is called a multistep method.
■
For now, we focus our attention on one-step methods. Here are some of
them:

11.3 Analysis of One-Step Methods
473
1. forward Euler method
un+1 = un + hfn;
(11.7)
2. backward Euler method
un+1 = un + hfn+1.
(11.8)
In both cases, y′ is approximated through a ﬁnite diﬀerence: forward and
backward diﬀerences are used in (11.7) and (11.8), respectively. Both ﬁnite
diﬀerences are ﬁrst-order approximations of the ﬁrst derivative of y with
respect to h (see Section 10.10.1).
3. trapezoidal (or Crank-Nicolson) method
un+1 = un + h
2 [fn + fn+1] .
(11.9)
This method stems from approximating the integral on the right side of
(11.2) by the trapezoidal quadrature rule (9.11).
4. Heun method
un+1 = un + h
2 [fn + f(tn+1, un + hfn)].
(11.10)
This method can be derived from the trapezoidal method substituting
f(tn+1, un + hf(tn, un)) for f(tn+1, un+1) in (11.9) (i.e., using the forward
Euler method to compute un+1).
In this last case, we notice that the aim is to transform an implicit method
into an explicit one. Addressing this concern, we recall the following.
Deﬁnition 11.3 (explicit and implicit methods) A method is called
explicit if un+1 can be computed directly in terms of (some of) the previous
values uk, k ≤n. A method is said to be implicit if un+1 depends implicitly
on itself through f.
■
Methods (11.7) and (11.10) are explicit, while (11.8) and (11.9) are implicit.
These latter require at each time step to solving a nonlinear problem if f
depends nonlinearly on the second argument.
A remarkable example of one-step methods are the Runge-Kutta meth-
ods, which will be analyzed in Section 11.8.
11.3
Analysis of One-Step Methods
Any one-step explicit method for the approximation of (11.1) can be cast
in the concise form
un+1 = un + hΦ(tn, un, fn; h),
0 ≤n ≤Nh −1,
u0 = y0,
(11.11)

474
11. Numerical Solution of Ordinary Diﬀerential Equations
where Φ(·, ·, ·; ·) is called an increment function. Letting as usual yn = y(tn),
analogously to (11.11) we can write
yn+1 = yn + hΦ(tn, yn, f(tn, yn); h) + εn+1,
0 ≤n ≤Nh −1, (11.12)
where εn+1 is the residual arising at the point tn+1 when we pretend that
the exact solution “satisﬁes” the numerical scheme. Let us write the residual
as
εn+1 = hτn+1(h).
The quantity τn+1(h) is called the local truncation error (LTE) at the node
tn+1. We thus deﬁne the global truncation error to be the quantity
τ(h) =
max
0≤n≤Nh−1|τn+1(h)|
Notice that τ(h) depends on the solution y of the Cauchy problem (11.1).
The forward Euler’s method is a special instance of (11.11), where
Φ(tn, un, fn; h) = fn,
while to recover Heun’s method we must set
Φ(tn, un, fn; h) = 1
2 [fn + f(tn + h, un + hfn)] .
A one-step explicit scheme is fully characterized by its increment function
Φ. This function, in all the cases considered thus far, is such that
lim
h→0Φ(tn, yn, f(tn, yn); h) = f(tn, yn),
∀tn ≥t0
(11.13)
Property (11.13), together with the obvious relation yn+1 −yn = hy′(tn) +
O(h2), ∀n ≥0, allows one to obtain from (11.12) that lim
h→0τn(h) = 0,
0 ≤n ≤Nh −1. In turn, this condition ensures that
lim
h→0τ(h) = 0
which expresses the consistency of the numerical method (11.11) with the
Cauchy problem (11.1). In general, a method is said to be consistent if its
LTE is inﬁnitesimal with respect to h. Moreover, a scheme has order p if,
∀t ∈I, the solution y(t) of the Cauchy problem (11.1) fulﬁlls the condition
τ(h) = O(hp)
for h →0.
(11.14)
Using Taylor expansions, as was done in Section 11.2, it can be proved that
the forward Euler method has order 1, while the Heun method has order 2
(see Exercises 1 and 2).

11.3 Analysis of One-Step Methods
475
11.3.1
The Zero-Stability
Let us formulate a requirement analogous to the one for Liapunov stability
(11.5), speciﬁcally for the numerical scheme. If (11.5) is satisﬁed with a
constant C independent of h, we shall say that the numerical problem is
zero-stable. Precisely:
Deﬁnition 11.4 (zero-stability of one-step methods) The numerical
method (11.11) for the approximation of problem (11.1) is zero-stable if
∃h0 > 0, ∃C > 0 : ∀h ∈(0, h0], |z(h)
n
−u(h)
n | ≤Cε, 0 ≤n ≤Nh, (11.15)
where z(h)
n , u(h)
n
are respectively the solutions of the problems



z(h)
n+1 = z(h)
n
+ h
6
Φ(tn, z(h)
n , f(tn, z(h)
n ); h) + δn+1
7
,
z0 = y0 + δ0,
(11.16)



u(h)
n+1 = u(h)
n
+ hΦ(tn, u(h)
n , f(tn, u(h)
n ); h),
u0 = y0,
(11.17)
for 0 ≤n ≤Nh −1, under the assumption that |δk| ≤ε, 0 ≤k ≤Nh.
■
Zero-stability thus requires that, in a bounded interval, (11.15) holds for
any value h ≤h0. This property deals, in particular, with the behavior
of the numerical method in the limit case h →0 and this justiﬁes the
name of zero-stability. This latter is therefore a distinguishing property of
the numerical method itself, not of the Cauchy problem (which, indeed,
is stable due to the uniform Lipschitz continuity of f). Property (11.15)
ensures that the numerical method has a weak sensitivity with respect to
small changes in the data and is thus stable in the sense of the general
deﬁnition given in Chapter 2.
Remark 11.1 The constant C in (11.15) is independent of h (and thus
of Nh), but it can depend on the width T of the integration interval I.
Actually, (11.15) does not exclude a priori the constant C from being an
unbounded function of T.
■
The request that a numerical method be stable arises, before anything else,
from the need of keeping under control the (unavoidable) errors introduced
by the ﬁnite arithmetic of the computer. Indeed, if the numerical method
were not zero-stable, the rounding errors made on y0 as well as in the pro-
cess of computing f(tn, un) would make the computed solution completely
useless.

476
11. Numerical Solution of Ordinary Diﬀerential Equations
Theorem 11.1 (Zero-stability) Consider the explicit one-step method
(11.11) for the numerical solution of the Cauchy problem (11.1). Assume
that the increment function Φ is Lipschitz continuous with respect to the
second argument, with constant Λ independent of h and of the nodes tj ∈
[t0, t0 + T], that is
∃h0 > 0, ∃Λ > 0 : ∀h ∈(0, h0]
|Φ(tn, u(h)
n , f(tn, u(h)
n ); h) −Φ(tn, z(h)
n , f(tn, z(h)
n ); h)|
≤Λ|u(h)
n
−z(h)
n |, 0 ≤n ≤Nh.
(11.18)
Then, method (11.11) is zero-stable.
Proof. Setting w(h)
j
= z(h)
j
−u(h)
j
, by subtracting (11.17) from (11.16) we obtain,
for j = 0, . . . , Nh −1,
w(h)
j+1 = w(h)
j
+ h
6
Φ(tj, z(h)
j
, f(tj, z(h)
j
); h) −Φ(tj, u(h)
j
, f(tj, u(h)
j
); h)
7
+ hδj+1.
Summing over j gives, for n = 1, . . . , Nh,
w(h)
n
= w(h)
0
+h
n−1

j=0
δj+1 + h
n−1

j=0
+
Φ(tj, z(h)
j
, f(tj, z(h)
j
); h) −Φ(tj, u(h)
j
, f(tj, u(h)
j
); h)
,
,
so that, by (11.18)
|w(h)
n | ≤|w0| + h
n−1

j=0
|δj+1| + hΛ
n−1

j=0
|w(h)
j
|,
1 ≤n ≤Nh.
(11.19)
Applying the discrete Gronwall lemma, given below, we obtain
|w(h)
n | ≤(1 + hn) εenhΛ,
1 ≤n ≤Nh.
Then (11.15) follows from noticing that hn ≤T and setting C = (1 + T) eΛT . 3
Notice that zero-stability implies the boundedness of the solution when f
is linear with respect to the second argument.
Lemma 11.2 (discrete Gronwall) Let kn be a nonnegative sequence and
ϕn a sequence such that







ϕ0 ≤g0
ϕn ≤g0 +
n−1

s=0
ps +
n−1

s=0
ksφs,
n ≥1.

11.3 Analysis of One-Step Methods
477
If g0 ≥0 and pn ≥0 for any n ≥0, then
ϕn ≤

g0 +
n−1

s=0
ps

exp
n−1

s=0
ks

,
n ≥1.
For the proof, see, for instance, [QV94], Lemma 1.4.2. In the speciﬁc case
of the Euler method, checking the property of zero-stability can be done
directly using the Lipschitz continuity of f (we refer the reader to the end
of Section 11.3.2). In the case of multistep methods, the analysis will lead to
the veriﬁcation of a purely algebraic property, the so-called root condition
(see Section 11.6.3).
11.3.2
Convergence Analysis
Deﬁnition 11.5 A method is said to be convergent if
∀n = 0, . . . , Nh,
|un −yn| ≤C(h)
where C(h) is an inﬁnitesimal with respect to h. In that case, it is said to
be convergent with order p if ∃C > 0 such that C(h) = Chp.
■
We can prove the following theorem.
Theorem 11.2 (Convergence) Under the same assumptions as in The-
orem 11.1, we have
|yn −un| ≤(|y0 −u0| + nhτ(h)) enhΛ,
1 ≤n ≤Nh.
(11.20)
Therefore, if the consistency assumption (11.13) holds and |y0 −u0| →0
as h →0, then the method is convergent. Moreover, if |y0 −u0| = O(hp)
and the method has order p, then it is also convergent with order p.
Proof. Setting wj = yj −uj, subtracting (11.11) from (11.12) and proceed-
ing as in the proof of the previous theorem yields inequality (11.19), with the
understanding that
w0 = y0 −u0, and δj+1 = τj+1(h).
The estimate (11.20) is then obtained by applying again the discrete Gronwall
lemma. From the fact that nh ≤T and τ(h) = O(hp), we can conclude that
|yn −un| ≤Chp with C depending on T and Λ but not on h.
3
A consistent and zero-stable method is thus convergent. This property is
known as the Lax-Richtmyer theorem or equivalence theorem (the converse:
“a convergent method is zero-stable” being obviously true). This theorem,
which is proven in [IK66], was already advocated in Section 2.2.1 and is a
central result in the analysis of numerical methods for ODEs (see [Dah56] or

478
11. Numerical Solution of Ordinary Diﬀerential Equations
[Hen62] for linear multistep methods, [But66], [MNS74] for a wider classes
of methods). It will be considered again in Section 11.5 for the analysis of
multistep methods.
We carry out in detail the convergence analysis in the case of the forward
Euler method, without resorting to the discrete Gronwall lemma. In the
ﬁrst part of the proof we assume that any operation is performed in exact
arithmetic and that u0 = y0.
Denote by en+1 = yn+1 −un+1 the error at node tn+1 with n = 0, 1, . . .
and notice that
en+1 = (yn+1 −u∗
n+1) + (u∗
n+1 −un+1),
(11.21)
where u∗
n+1 = yn + hf(tn, yn) is the solution obtained after one step of
the forward Euler method starting from the initial datum yn (see Figure
11.1). The ﬁrst addendum in (11.21) accounts for the consistency error, the
second one for the cumulation of these errors. Then
yn+1 −u∗
n+1 = hτn+1(h),
u∗
n+1 −un+1 = en + h [f(tn, yn) −f(tn, un)] .
y(x)
yn
un
tn
tn+1
un+1
u∗
n+1
yn+1
hτn+1
en+1
FIGURE 11.1. Geometrical interpretation of the local and global truncation er-
rors at node tn+1 for the forward Euler method
As a consequence,
|en+1| ≤h|τn+1(h)| + |en| + h|f(tn, yn) −f(tn, un)| ≤hτ(h) + (1 + hL)|en|,
L being the Lipschitz constant of f. By recursion on n, we ﬁnd
|en+1|
≤[1 + (1 + hL) + . . . + (1 + hL)n] hτ(h)
= (1 + hL)n+1 −1
L
τ(h) ≤eL(tn+1−t0) −1
L
τ(h).

11.3 Analysis of One-Step Methods
479
The last inequality follows from noticing that 1+hL ≤ehL and (n+1)h =
tn+1 −t0.
On the other hand, if y ∈C2(I), the LTE for the forward Euler method
is (see Section 10.10.1)
τn+1(h) = h
2 y′′(ξ),
ξ ∈(tn, tn+1),
and thus, τ(h) ≤(M/2)h, where M = maxξ∈I |y′′(ξ)|. In conclusion,
|en+1| ≤eL(tn+1−t0) −1
L
M
2 h,
∀n ≥0,
(11.22)
from which it follows that the global error tends to zero with the same
order as the local truncation error.
If also the rounding errors are accounted for, we can assume that the
solution ¯un+1, actually computed by the forward Euler method at time
tn+1, is such that
¯u0 = y0 + ζ0,
¯un+1 = ¯un + hf(tn, ¯un) + ζn+1,
(11.23)
having denoted the rounding error by ζj, for j ≥0.
Problem (11.23) is an instance of (11.16), provided that we identify ζn+1
and ¯un with hδn+1 and z(h)
n
in (11.16), respectively. Combining Theorems
11.1 and 11.2 we get, instead of (11.22), the following error estimate
|yn+1 −¯un+1| ≤eL(tn+1−t0)

|ζ0| + 1
L
M
2 h + ζ
h

,
where ζ = max1≤j≤n+1 |ζj|. The presence of rounding errors does not allow,
therefore, to conclude that as h →0, the error goes to zero. Actually,
there exists an optimal (non null) value of h, hopt, for which the error is
minimized. For h < hopt, the rounding error dominates the truncation error
and the global error increases.
11.3.3
The Absolute Stability
The property of absolute stability is in some way specular to zero-stability,
as far as the roles played by h and I are concerned. Heuristically, we say that
a numerical method is absolutely stable if, for h ﬁxed, un remains bounded
as tn →+∞. This property, thus, deals with the asymptotic behavior of
un, as opposed to a zero-stable method for which, for a ﬁxed integration
interval, un remains bounded as h →0.
For a precise deﬁnition, consider the linear Cauchy problem (that from now
on, we shall refer to as the test problem)
" y′(t) = λy(t),
t > 0,
y(0) = 1,
(11.24)

480
11. Numerical Solution of Ordinary Diﬀerential Equations
with λ ∈C, whose solution is y(t) = eλt. Notice that
lim
t→+∞|y(t)| = 0 if
Re(λ) < 0.
Deﬁnition 11.6 A numerical method for approximating (11.24) is abso-
lutely stable if
|un| −→0
as
tn −→+∞.
(11.25)
Let h be the discretization stepsize. The numerical solution un of (11.24)
obviously depends on h and λ. The region of absolute stability of the nu-
merical method is the subset of the complex plane
A = {z = hλ ∈C : (11.25) is satisﬁed } .
(11.26)
Thus, A is the set of the values of the product hλ for which the numerical
method furnishes solutions that decay to zero as tn tends to inﬁnity.
■
Let us check whether the one-step methods introduced previously are ab-
solutely stable.
1. Forward Euler method: applying (11.7) to problem (11.24) yields un+1 =
un + hλun for n ≥0, with u0 = 1. Proceeding recursively on n we get
un = (1 + hλ)n,
n ≥0.
Therefore, condition (11.25) is satisﬁed iﬀ|1 + hλ| < 1, that is, if hλ lies
within the unit circle with center at (−1, 0) (see Figure 11.3). This amounts
to requiring that
hλ ∈C−
and
0 < h < −2Re(λ)
|λ|2
(11.27)
where
C−= {z ∈C : Re(z) < 0} .
Example 11.1 For the Cauchy problem y′(x) = −5y(x) for x > 0 and y(0) = 1,
condition (11.27) implies 0 < h < 2/5. Figure 11.2 (left) shows the behavior of
the computed solution for two values of h which do not fulﬁll this condition, while
on the right we show the solutions for two values of h that do. Notice that in this
second case the oscillations, if present, damp out as t grows.
•
2. Backward Euler method: proceeding as before, we get this time
un =
1
(1 −hλ)n ,
n ≥0.
The absolute stability property (11.25) is satisﬁed for any value of hλ that
does not belong to the unit circle of center (1, 0) (see Figure 11.3, right).

11.3 Analysis of One-Step Methods
481
0
1
2
3
4
5
6
7
8
−3
−2
−1
0
1
2
3
0
1
2
3
4
5
6
7
8
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
FIGURE 11.2. Left: computed solutions for h = 0.41 > 2/5 (dashed line) and
h = 2/5 (solid line). Notice how, in the limiting case h = 2/5, the oscillations
remain unmodiﬁed as t grows. Right: two solutions are reported for h = 0.39
(solid line) and h = 0.15 (dashed line)
Example 11.2 The numerical solution given by the backward Euler method in
the case of Example 11.1 does not exhibit any oscillation for any value of h. On
the other hand, the same method, if applied to the problem y′(t) = 5y(t) for t > 0
and with y(0) = 1, computes a solution that decays anyway to zero as t →∞if
h > 2/5, despite the fact that the exact solution of the Cauchy problem tends to
inﬁnity.
•
3. Trapezoidal (or Crank-Nicolson) method: we get
un =

1 + 1
2λh

/

1 −1
2λh
n
,
n ≥0,
hence (11.25) is fulﬁlled for any hλ ∈C−.
4. Heun’s method: applying (11.10) to problem (11.24) and proceeding
by recursion on n, we obtain
un =

1 + hλ + (hλ)2
2
n
,
n ≥0.
As shown in Figure 11.3 the region of absolute stability of Heun’s method
is larger than the corresponding one of Euler’s method. However, its re-
striction to the real axis is the same.
We say that a method is A-stable if A ∩C−= C−, i.e., if for Re(λ) < 0,
condition (11.25) is satisﬁed for all values of h.
The backward Euler and Crank-Nicolson methods are A-stable, while
the forward Euler and Heun methods are conditionally stable.
Remark 11.2 Notice that the implicit one-step methods examined so far
are unconditionally absolutely stable, while explicit schemes are condition-

482
11. Numerical Solution of Ordinary Diﬀerential Equations
FE
H
Re
BE
−1
1
Im
1.75
−1.75
FIGURE 11.3. Regions of absolute stability for the forward (FE) and backward
Euler (BE) methods and for Heun’s method (H). Notice that the region of abso-
lute stability of the BE method lies outside the unit circle of center (1, 0) (shaded
area)
ally absolutely stable. This is, however, not a general rule: in fact, there ex-
ist implicit unstable or only conditionally stable schemes. On the contrary,
there are no explicit unconditionally absolutely stable schemes [Wid67]. ■
11.4
Diﬀerence Equations
For any integer k ≥1, an equation of the form
un+k + αk−1un+k−1 + . . . + α0un = ϕn+k,
n = 0, 1, . . .
(11.28)
is called a linear diﬀerence equation of order k. The coeﬃcients α0 ̸= 0,
α1, . . . , αk−1 may or may not depend on n. If, for any n, the right side
ϕn+k is equal to zero, the equation is said homogeneous, while if the α′
js
are independent of n it is called linear diﬀerence equation with constant
coeﬃcients.
Diﬀerence equations arise for instance in the discretization of ordinary dif-
ferential equations. Regarding this, we notice that all the numerical meth-
ods examined so far end up with equations like (11.28). More generally,
equations like (11.28) are encountered when quantities are deﬁned through
linear recursive relations. Another relevant application is concerned with
the discretization of boundary value problems (see Chapter 12). For fur-
ther details on the subject, we refer to Chapters 2 and 5 of [BO78] and to
Chapter 6 of [Gau97].

11.4 Diﬀerence Equations
483
Any sequence {un, n = 0, 1, . . . } of values that satisfy (11.28) is called
a solution to the equation (11.28). Given k initial values u0, . . . , uk−1, it
is always possible to construct a solution of (11.28) by computing (sequen-
tially)
un+k = [ϕn+k −(αk−1un+k−1 + . . . + α0un)],
n = 0, 1, . . .
However, our interest is to ﬁnd an expression of the solution un+k which
depends only on the coeﬃcients and on the initial values.
We start by considering the homogeneous case with constant coeﬃcients,
un+k + αk−1un+k−1 + . . . + α0un = 0,
n = 0, 1, . . .
(11.29)
and associate with (11.29) the characteristic polynomial Π ∈Pk deﬁned as
Π(r) = rk + αk−1rk−1 + . . . + α1r + α0.
(11.30)
Denoting its roots by rj, j = 0, . . . , k −1, any sequence of the form

rn
j , n = 0, 1, . . .

,
for j = 0, . . . , k −1
(11.31)
is a solution of (11.29), since
rn+k
j
+ αk−1rn+k−1
j
+ . . . + α0rn
j
= rn
j

rk
j + αk−1rk−1
j
+ . . . + α0

= rn
j Π(rj) = 0.
We say that the k sequences deﬁned in (11.31) are the fundamental solutions
of the homogeneous equation (11.29). Any sequence of the form
un = γ0rn
0 + γ1rn
1 + . . . + γk−1rn
k−1,
n = 0, 1, . . .
(11.32)
is still a solution to (11.29), since it is a linear equation.
The coeﬃcients γ0, . . . , γk−1 can be determined by imposing the k initial
conditions u0, . . . , uk−1. Moreover, it can be proved that if all the roots
of Π are simple, then all the solutions of (11.29) can be cast in the form
(11.32).
This last statement no longer holds if there are roots of Π with multi-
plicity greater than 1. If, for a certain j, the root rj has multiplicity m ≥2,
in order to obtain a system of fundamental solutions that generate all the
solutions of (11.29), it suﬃces to replace the corresponding fundamental
solution

rn
j , n = 0, 1, . . .

with the m sequences

rn
j , n = 0, 1, . . .

,

nrn
j , n = 0, 1, . . .

, . . . ,

nm−1rn
j , n = 0, 1, . . .

.
More generally, assuming that r0, . . . , rk′ are distinct roots of Π, with mul-
tiplicities equal to m0, . . . , mk′, respectively, we can write the solution of
(11.29) as
un =
k′

j=0
mj−1

s=0
γsjns

rn
j ,
n = 0, 1, . . . .
(11.33)

484
11. Numerical Solution of Ordinary Diﬀerential Equations
Notice that even in presence of complex conjugate roots one can still obtain
a real solution (see Exercise 3).
Example 11.3 For the diﬀerence equation un+2−un = 0, we have Π(r) = r2−1,
then r0 = −1 and r1 = 1, therefore the solution is given by un = γ00(−1)n + γ01.
In particular, enforcing the initial conditions u0 and u1 gives γ00 = (u0 −u1)/2,
γ01 = (u0 + u1)/2.
•
Example 11.4 For the diﬀerence equation un+3 −2un+2 −7un+1 −4un = 0,
Π(r) = r3 −2r2 −7r −4. Its roots are r0 = −1 (with multiplicity 2), r1 = 4 and
the solution is un = (γ00 + nγ10)(−1)n + γ014n. Enforcing the initial conditions
we can compute the unknown coeﬃcients as the solution of the following linear
system



γ00 + γ01
= u0,
−γ00 −γ10 + 4γ01
= u1,
γ00 + 2γ10 + 16γ01
= u2
that yields γ00 = (24u0 −2u1 −u2)/25, γ10 = (u2 −3u1 −4u0)/5 and γ01 =
(2u1 + u0 + u2)/25.
•
The expression (11.33) is of little practical use since it does not outline
the dependence of un on the k initial conditions. A more convenient rep-
resentation is obtained by introducing a new set
2
ψ(n)
j
, n = 0, 1, . . .
3
of
fundamental solutions that satisfy
ψ(i)
j
= δij,
i, j = 0, 1, . . . , k −1.
(11.34)
Then, the solution of (11.29) subject to the initial conditions u0, . . . , uk−1
is given by
un =
k−1

j=0
ujψ(n)
j
,
n = 0, 1, . . . .
(11.35)
The new fundamental solutions
2
ψ(n)
j
, n = 0, 1, . . .
3
can be represented in
terms of those in (11.31) as follows
ψ(n)
j
=
k−1

m=0
βj,mrn
m
for j = 0, . . . , k −1, n = 0, 1, . . .
(11.36)
By requiring (11.34), we obtain the k linear systems
k−1

m=0
βj,mri
m = δij,
i, j = 0, . . . , k −1,
whose matrix form is
Rbj = ej,
j = 0, . . . , k −1.
(11.37)

11.4 Diﬀerence Equations
485
Here ej denotes the unit vector of Rk, R = (rim) = (ri
m) and bj =
(βj,0, . . . , βj,k−1)T . If all r′
js are simple roots of Π, the matrix R is nonsin-
gular (see Exercise 5).
The general case where Π has k′ +1 distinct roots r0, . . . , rk′ with multi-
plicities m0, . . . , mk′ respectively, can be dealt with by replacing in (11.36)

rn
j ,
n = 0, 1, . . .

with

rn
j ns,
n = 0, 1, . . .

, where j = 0, . . . , k′ and
s = 0, . . . , mj −1.
Example 11.5 We consider again the diﬀerence equation of Example 11.4. Here
we have {rn
0 , nrn
0 , rn
1 , n = 0, 1, . . . } so that the matrix R becomes
R =


r0
0
0
r0
2
r1
0
r1
0
r1
2
r2
0
2r2
0
r2
2

=


1
0
1
−1
−1
4
1
2
16

.
Solving the three systems (11.37) yields
ψ(n)
0
= 24
25(−1)n −4
5n(−1)n + 1
254n,
ψ(n)
1
= −2
25(−1)n −3
5n(−1)n + 2
254n,
ψ(n)
2
= −1
25(−1)n + 1
5n(−1)n + 1
254n,
from which it can be checked that the solution un = 2
j=0 ujψ(n)
j
coincides with
the one already found in Example 11.4.
•
Now we return to the case of nonconstant coeﬃcients and consider the
following homogeneous equation
un+k +
k

j=1
αk−j(n)un+k−j = 0,
n = 0, 1, . . . .
(11.38)
The goal is to transform it into an ODE by means of a function F, called
the generating function of the equation (11.38). F depends on the real
variable t and is derived as follows. We require that the n-th coeﬃcient
of the Taylor series of F around t = 0 can be written as γnun, for some
unknown constant γn, so that
F(t) =
∞

n=0
γnuntn.
(11.39)
The coeﬃcients {γn} are unknown and must be determined in such a way
that
k

j=0
cjF (k−j)(t) =
∞

n=0

un+k +
k

j=1
αk−j(n)un+k−j

tn,
(11.40)

486
11. Numerical Solution of Ordinary Diﬀerential Equations
where cj are suitable unknown constants not depending on n. Note that
owing to (11.39) we obtain the ODE
k

j=0
cjF (k−j)(t) = 0
to which we must add the initial conditions F (j)(0) = γjuj for j = 0, . . . , k−
1. Once F is available, it is simple to recover un through the deﬁnition of
F itself.
Example 11.6 Consider the diﬀerence equation
(n + 2)(n + 1)un+2 −2(n + 1)un+1 −3un = 0,
n = 0, 1, . . .
(11.41)
with the initial conditions u0 = u1 = 2. We look for a generating function of the
form (11.39). By term-to-term derivation of the series, we get
F ′(t) =
∞

n=0
γnnuntn−1,
F ′′(t) =
∞

n=0
γnn(n −1)untn−2,
and, after some algebra, we ﬁnd
F ′(t) =
∞

n=0
γnnuntn−1 =
∞

n=0
γn+1(n + 1)un+1tn,
F ′′(t) =
∞

n=0
γnn(n −1)untn−2 =
∞

n=0
γn+2(n + 2)(n + 1)un+2tn.
As a consequence, (11.40) becomes
∞

n=0
(n + 1)(n + 2)un+2tn −2
∞

n=0
(n + 1)un+1tn −3
∞

n=0
untn
= c0
∞

n=0
γn+2(n + 2)(n + 1)un+2tn + c1
∞

n=0
γn+1(n + 1)un+1tn + c2
∞

n=0
γnuntn,
so that, equating both sides, we ﬁnd
γn = 1 ∀n ≥0,
c0 = 1, c1 = −2, c2 = −3.
We have thus associated with the diﬀerence equation the following ODE with
constant coeﬃcients
F ′′(t) −2F ′(t) −3F(t) = 0,
with the initial condition F(0) = F ′(0) = 2. The n-th coeﬃcient of the solution
F(t) = e3t + e−t is
1
n!F (n)(0) = 1
n! [(−1)n + 3n] ,
so that un = (1/n!) [(−1)n + 3n] is the solution of (11.41).
•

11.5 Multistep Methods
487
The nonhomogeneous case (11.28) can be tackled by searching for solutions
of the form
un = u(0)
n
+ u(ϕ)
n ,
where u(0)
n
is the solution of the associated homogeneous equation and u(ϕ)
n
is a particular solution of the nonhomogeneous equation. Once the solution
of the homogeneous equation is available, a general technique to obtain
the solution of the nonhomogeneous equation is based on the method of
variation of parameters, combined with a reduction of the order of the
diﬀerence equation (see [BO78]).
In the special case of diﬀerence equations with constant coeﬃcients, with
ϕn of the form cnQ(n), where c is a constant and Q is a polynomial of degree
p with respect to the variable n, a possible approach is that of undetermined
coeﬃcients, where one looks for a particular solution that depends on some
undetermined constants and has a known form for some classes of right
sides ϕn. It suﬃces to look for a particular solution of the form
u(ϕ)
n
= cn(bpnp + bp−1np−1 + . . . + b0),
where bp, . . . , b0 are constants to be determined in such a way that u(ϕ)
n
is
actually a solution of (11.28).
Example 11.7 Consider the diﬀerence equation un+3−un+2+un+1−un = 2nn2.
The particular solution is of the form un = 2n(b2n2 + b1n + b0). Substituting this
solution into the equation, we ﬁnd 5b2n2+(36b2+5b1)n+(58b2+18b1+5b0) = n2,
from which, recalling the principle of identity for polynomials, one gets b2 = 1/5,
b1 = −36/25 and b0 = 358/125.
•
Analogous to the homogeneous case, it is possible to express the solution
of (11.28) as
un =
k−1

j=0
ujψ(n)
j
+
n

l=k
ϕlψ(n−l+k−1)
k−1
,
n = 0, 1, . . .
(11.42)
where we deﬁne ψ(i)
k−1 ≡0 for all i < 0 and ϕj ≡0 for all j < k.
11.5
Multistep Methods
Let us now introduce some examples of multistep methods (shortly, MS).
Deﬁnition 11.7 (q-steps methods) A q-step method (q ≥1) is one
which, ∀n ≥q −1, un+1 depends on un+1−q, but not on the values uk
with k < n + 1 −q.
■

488
11. Numerical Solution of Ordinary Diﬀerential Equations
A well-known two-step explicit method can be obtained by using the
centered ﬁnite diﬀerence (10.61) to approximate the ﬁrst order derivative
in (11.1). This yields the midpoint method
un+1 = un−1 + 2hfn,
n ≥1
(11.43)
where u0 = y0, u1 is to be determined and fk denotes the value f(tk, uk).
An example of an implicit two-step scheme is the Simpson method, ob-
tained from (11.2) with t0 = tn−1 and t = tn+1 and by using the Cavalieri-
Simpson quadrature rule to approximate the integral of f
un+1 = un−1 + h
3 [fn−1 + 4fn + fn+1],
n ≥1
(11.44)
where u0 = y0, and u1 is to be determined.
From these examples, it is clear that a multistep method requires q initial
values u0, . . . , uq−1 for “taking oﬀ”. Since the Cauchy problem provides
only one datum (u0), one way to assign the remaining values consists of
resorting to explicit one-step methods of high order. An example is given by
Heun’s method (11.10), other examples are provided by the Runge-Kutta
methods, which will be introduced in Section 11.8.
In this section we deal with linear multistep methods
un+1 =
p

j=0
ajun−j + h
p

j=0
bjfn−j + hb−1fn+1, n = p, p + 1, . . . (11.45)
which are p + 1-step methods, p ≥0. For p = 0, we recover one-step
methods.
The coeﬃcients aj, bj are real and fully identify the scheme; they are
such that ap ̸= 0 or bp ̸= 0. If b−1 ̸= 0 the scheme is implicit, otherwise it
is explicit.
We can reformulate (11.45) as follows
p+1

s=0
αsun+s = h
p+1

s=0
βsf(tn+s, un+s), n = 0, 1, . . . , Nh −(p + 1) (11.46)
having set αp+1 = 1, αs = −ap−s for s = 0, . . . , p and βs = bp−s for
s = 0, . . . , p+1. Relation (11.46) is a special instance of the linear diﬀerence
equation (11.28), where we set k = p + 1 and ϕn+j = hβjf(tn+j, un+j), for
j = 0, . . . , p + 1.
Also for MS methods we can characterize consistency in terms of the local
truncation error, according to the following deﬁnition.
Deﬁnition 11.8 The local truncation error (LTE) τn+1(h) introduced by
the multistep method (11.45) at tn+1 (for n ≥p) is deﬁned through the

11.5 Multistep Methods
489
following relation
hτn+1(h) = yn+1 −


p

j=0
ajyn−j + h
p

j=−1
bjy′
n−j

,
n ≥p,
(11.47)
where yn−j = y(tn−j) and y′
n−j = y′(tn−j) for j = −1, . . . , p.
■
Analogous to one-step methods, the quantity hτn+1(h) is the residual gen-
erated at tn+1 if we pretend that the exact solution “satisﬁes” the numerical
scheme. Letting τ(h) = max
n |τn(h)|, we have the following deﬁnition.
Deﬁnition 11.9 (Consistency) The multistep method (11.45) is consis-
tent if τ(h) →0 as h →0. Moreover, if τ(h) = O(hq), for some q ≥1, then
the method is said to have order q.
■
A more precise characterization of the LTE can be given by introducing the
following linear operator L associated with the linear MS method (11.45)
L[w(t); h] = w(t + h) −
p

j=0
ajw(t −jh) −h
p

j=−1
bjw′(t −jh),
(11.48)
where w ∈C1(I) is an arbitrary function. Notice that the LTE is exactly
L[y(tn); h]. If we assume that w is suﬃciently smooth and expand w(t−jh)
and w′(t −jh) about t −ph, we obtain
L[w(t); h] = C0w(t −ph) + C1hw(1)(t −ph) + . . . + Ckhkw(k)(t −ph) + . . .
Consequently, if the MS method has order q and y ∈Cq+1(I), we obtain
τn+1(h) = Cq+1hq+1y(q+1)(tn−p) + O(hq+2).
The term Cq+1hq+1y(q+1)(tn−p) is the so-called principal local truncation
error (PLTE) while Cq+1 is the error constant. The PLTE is widely em-
ployed in devising adaptive strategies for MS methods (see [Lam91], Chap-
ter 3).
Program 92 provides an implementation of the multistep method in the
form (11.45) for the solution of a Cauchy problem on the interval (t0, T).
The input parameters are: the column vector a containing the p + 1 co-
eﬃcients ai; the column vector b containing the p + 2 coeﬃcients bi; the
discretization stepsize h; the vector of initial data u0 at the corresponding
time instants t0; the macros fun and dfun containing the functions f and
∂f/∂y. If the MS method is implicit, a tolerance tol and a maximum num-
ber of admissible iterations itmax must be provided. These two parameters
monitor the convergence of Newton’s method that is employed to solve the

490
11. Numerical Solution of Ordinary Diﬀerential Equations
nonlinear equation (11.45) associated with the MS method. In output the
code returns the vectors u and t containing the computed solution at the
time instants t.
Program 92 - multistep : Linear multistep methods
function [t,u] = multistep (a,b,tf,t0,u0,h,fun,dfun,tol,itmax)
y = u0; t = t0; f = eval (fun); p = length(a) - 1; u = u0;
nt = ﬁx((tf - t0 (1) )/h);
for k = 1:nt
lu=length(u);
G = a’ *u (lu:-1:lu-p)+ h * b(2:p+2)’ * f(lu:-1:lu-p);
lt = length(t0); t0 = [t0; t0(lt)+h]; unew = u (lu);
t = t0 (lt+1); err = tol + 1; it = 0;
while (err > tol) & (it <= itmax)
y = unew; den = 1 - h * b (1) * eval(dfun);
fnew = eval (fun);
if den == 0
it = itmax + 1;
else
it = it + 1;
unew = unew - (unew - G - h * b (1) * fnew)/den;
err = abs (unew - y);
end
end
u = [u; unew]; f = [f; fnew];
end
t = t0;
In the forthcoming sections we examine some families of multistep methods.
11.5.1
Adams Methods
These methods are derived from the integral form (11.2) through an ap-
proximate evaluation of the integral of f between tn and tn+1. We suppose
that the discretization nodes are equally spaced, i.e., tj = t0 + jh, with
h > 0 and j ≥1, and then we integrate, instead of f, its interpolating
polynomial on p + 1 distinct nodes. The resulting schemes are thus consis-
tent by construction and have the following form
un+1 = un + h
p

j=−1
bjfn−j,
n ≥p.
(11.49)
The interpolation nodes can be either:
1. tn, tn−1, . . . , tn−p (in this case b−1 = 0 and the resulting method is
explicit);
or

11.5 Multistep Methods
491
2. tn+1, tn, . . . , tn−p+1 (in this case b−1 ̸= 0 and the scheme is implicit).
The implicit schemes are called Adams-Moulton methods, while the explicit
ones are called Adams-Bashforth methods.
Adams-Bashforth methods (AB)
Taking p = 0 we recover the forward Euler method, since the interpolating
polynomial of degree zero at node tn is given by Π0f = fn. For p = 1, the
linear interpolating polynomial at the nodes tn−1 and tn is
Π1f(t) = fn + (t −tn)fn−1 −fn
tn−1 −tn
.
Since Π1f(tn) = fn and Π1f(tn+1) = 2fn −fn−1, we get
tn+1
>
tn
Π1f(t) = h
2 [Π1f(tn) + Π1f(tn+1)] = h
2 [3fn −fn−1] .
Therefore, the two-step AB method is
un+1 = un + h
2 [3fn −fn−1] .
(11.50)
With a similar procedure, if p = 2, we ﬁnd the three-step AB method
un+1 = un + h
12 [23fn −16fn−1 + 5fn−2] ,
while for p = 3 we get the four-step AB scheme
un+1 = un + h
24 (55fn −59fn−1 + 37fn−2 −9fn−3) .
In general, q-step Adams-Bashforth methods have order q. The error con-
stants C∗
q+1 of these methods are collected in Table 11.1.
Adams-Moulton methods (AM)
If p = −1, the Backward Euler scheme is recovered, while if p = 0, we
construct the linear polynomial interpolating f at the nodes tn and tn+1
to recover the Crank-Nicolson scheme (11.9). In the case of the two-step
method, the polynomial of degree 2 interpolating f at the nodes tn−1, tn,
tn+1 is generated, yielding the following scheme
un+1 = un + h
12 [5fn+1 + 8fn −fn−1] .
(11.51)

492
11. Numerical Solution of Ordinary Diﬀerential Equations
The methods corresponding to p = 3 and 4 are respectively given by
un+1 = un + h
24 (9fn+1 + 19fn −5fn−1 + fn−2)
un+1 = un +
h
720 (251fn+1 + 646fn −264fn−1 + 106fn−2 −19fn−3) .
The q-step Adams-Moulton methods have order q + 1 and their error con-
stants Cq+1 are summarized in Table 11.1.
q
C∗
q+1
Cq+1
q
C∗
q+1
Cq+1
1
1
2
−1
2
3
3
8
−1
24
2
5
12
−1
12
4
251
720
−19
720
TABLE 11.1. Error constants for Adams-Bashforth methods (having order q) and
Adams-Moulton methods (having order q + 1)
11.5.2
BDF Methods
The so-called backward diﬀerentiation formulae (henceforth denoted by
BDF) are implicit MS methods derived from a complementary approach
to the one followed for the Adams methods. In fact, for the Adams meth-
ods we have resorted to numerical integration for the source function f,
whereas in BDF methods we directly approximate the value of the ﬁrst
derivative of y at node tn+1 through the ﬁrst derivative of the polynomial
interpolating y at the p + 1 nodes tn+1, tn, . . . , tn−p+1.
By doing so, we get schemes of the form
un+1 =
p

j=0
ajun−j + hb−1fn+1
with b−1 ̸= 0. Method (11.8) represents the most elementary example,
corresponding to the coeﬃcients a0 = 1 and b−1 = 1.
We summarize in Table 11.2 the coeﬃcients of BDF methods that are
zero-stable. In fact, we shall see in Section 11.6.3 that only for p ≤5 are
BDF methods zero-stable (see [Cry73]).
11.6
Analysis of Multistep Methods
Analogous to what has been done for one-step methods, in this section
we provide algebraic conditions that ensure consistency and stability of
multistep methods.

11.6 Analysis of Multistep Methods
493
p
a0
a1
a2
a3
a4
a5
b−1
0
1
0
0
0
0
0
1
1
4
3
- 1
3
0
0
0
0
2
3
2
18
11
- 9
11
2
11
0
0
0
6
11
3
48
25
- 36
25
16
25
-
3
25
0
0
12
25
4
300
137
- 300
137
200
137
- 75
137
12
137
0
60
137
5
360
147
- 450
147
400
147
- 225
147
72
147
- 10
147
60
137
TABLE 11.2. Coeﬃcients of zero-stable BDF methods for p = 0, 1, . . . , 5
11.6.1
Consistency
The property of consistency of a multistep method introduced in Deﬁnition
11.9 can be veriﬁed by checking that the coeﬃcients satisfy certain algebraic
equations, as stated in the following theorem.
Theorem 11.3 The multistep method (11.45) is consistent iﬀthe following
algebraic relations among the coeﬃcients are satisﬁed
p

j=0
aj = 1,
−
p

j=0
jaj +
p

j=−1
bj = 1.
(11.52)
Moreover, if y ∈Cq+1(I) for some q ≥1, where y is the solution of the
Cauchy problem (11.1), then the method is of order q iﬀ(11.52) holds and
the following additional conditions are satisﬁed
p

j=0
(−j)iaj + i
p

j=−1
(−j)i−1bj = 1, i = 2, . . . , q.
Proof. Expanding y and f in a Taylor series yields, for any n ≥p
yn−j = yn −jhy′
n + O(h2),
fn−j = fn + O(h).
(11.53)
Plugging these values back into the multistep scheme and neglecting the terms
in h of order higher than 1 gives
yn+1 −
p

j=0
ajyn−j −h
p

j=−1
bjfn−j
= yn+1 −
p

j=0
ajyn + h
p

j=0
jajy′
n −h
p

j=−1
bjfn −O(h2)

p

j=0
aj −
p

j=−1
bj

= yn+1 −
p

j=0
ajyn −hy′
n

−
p

j=0
jaj +
p

j=−1
bj

−O(h2)

p

j=0
aj −
p

j=−1
bj

where we have replaced y′
n by fn. From the deﬁnition (11.47) we then obtain
hτn+1(h) = yn+1 −
p

j=0
ajyn −hy′
n

−
p

j=0
jaj +
p

j−1
bj

−O(h2)

p

j=0
aj −
p

j=−1
bj


494
11. Numerical Solution of Ordinary Diﬀerential Equations
hence the local truncation error is
τn+1(h) = yn+1 −yn
h
+ yn
h

1 −
p

j=0
aj

+y′
n

p

j=0
jaj −
p

j=−1
bj

−O(h)

p

j=0
aj −
p

j=−1
bj

.
Since, for any n, (yn+1 −yn)/h →y′
n, as h →0, it follows that τn+1(h) tends to
0 as h goes to 0 iﬀthe algebraic conditions (11.52) are satisﬁed. The rest of the
proof can be carried out in a similar manner, accounting for terms of progressively
higher order in the expansions (11.53).
3
11.6.2
The Root Conditions
Let us employ the multistep method (11.45) to approximately solve the
model problem (11.24). The numerical solution satisﬁes the linear diﬀerence
equation
un+1 =
p

j=0
ajun−j + hλ
p

j=−1
bjun−j,
(11.54)
which ﬁts the form (11.29). We can therefore apply the theory devel-
oped in Section 11.4 and look for fundamental solutions of the form uk =
[ri(hλ)]k, k = 0, 1, . . . , where ri(hλ), for i = 0, . . . , p, are the roots of the
polynomial Π ∈Pp+1
Π(r) = ρ(r) −hλσ(r).
(11.55)
We have denoted by
ρ(r) = rp+1 −
p

j=0
ajrp−j,
σ(r) = b−1rp+1 +
p

j=0
bjrp−j
the ﬁrst and second characteristic polynomials of the multistep method
(11.45), respectively. The polynomial Π(r) is the characteristic polynomial
associated with the diﬀerence equation (11.54), and rj(hλ) are its charac-
teristic roots.
The roots of ρ are ri(0), i = 0, . . . , p, and will be abbreviated henceforth
by ri. From the ﬁrst condition in (11.52) it follows that if a multistep
method is consistent then 1 is a root of ρ. We shall assume that such a root
(the consistency root) is labelled as r0(0) = r0 and call the corresponding
root r0(hλ) the principal root.
Deﬁnition 11.10 (Root condition) The multistep method (11.45) is said
to satisfy the root condition if all roots ri are contained within the unit

11.6 Analysis of Multistep Methods
495
circle centered at the origin of the complex plane, otherwise, if they fall on
its boundary, they must be simple roots of ρ. Equivalently,
"
|rj| ≤1,
j = 0, . . . , p;
furthermore, for those j such that |rj| = 1, then ρ′(rj) ̸= 0.
(11.56)
■
Deﬁnition 11.11 (Strong root condition) The multistep method (11.45)
is said to satisfy the strong root condition if it satisﬁes the root condition
and r0 = 1 is the only root lying on the boundary of the unit circle. Equiv-
alently,
|rj| < 1
j = 1, . . . , p.
(11.57)
■
Deﬁnition 11.12 (Absolute root condition) The multistep method
(11.45) satisﬁes the absolute root condition if there exists h0 > 0 such that
|rj(hλ)| < 1
j = 0, . . . , p,
∀h ≤h0.
■
11.6.3
Stability and Convergence Analysis for Multistep
Methods
Let us now examine the relation between root conditions and the stability
of multistep methods. Generalizing the Deﬁnition 11.4, we can get the
following.
Deﬁnition 11.13 (Zero-stability of multistep methods) The multi-
step method (11.45) is zero-stable if
∃h0 > 0, ∃C > 0 :
∀h ∈(0, h0], |z(h)
n
−u(h)
n | ≤Cε, 0 ≤n ≤Nh,
(11.58)
where Nh = max {n : tn ≤t0 + T} and z(h)
n
and u(h)
n
are, respectively, the
solutions of problems





z(h)
n+1 =
p

j=0
ajz(h)
n−j + h
p

j=−1
bjf(tn−j, z(h)
n−j) + hδn+1,
z(h)
k
= w(h)
k
+ δk,
k = 0, . . . , p
(11.59)

496
11. Numerical Solution of Ordinary Diﬀerential Equations





u(h)
n+1 =
p

j=0
aju(h)
n−j + h
p

j=−1
bjf(tn−j, u(h)
n−j),
u(h)
k
= w(h)
k ,
k = 0, . . . , p
(11.60)
for p ≤n ≤Nh −1, where |δk| ≤ε, 0 ≤k ≤Nh, w(h)
0
= y0 and w(h)
k ,
k = 1, . . . , p, are p initial values generated by using another numerical
scheme.
■
Theorem 11.4 (Equivalence of zero-stability and root condition)
For a consistent multistep method, the root condition is equivalent to zero-
stability.
Proof. Let us begin by proving that the root condition is necessary for the zero-
stability to hold. We proceed by contradiction and assume that the method is
zero-stable and there exists a root ri which violates the root condition.
Since the method is zero-stable, condition (11.58) must be satisﬁed for any
Cauchy problem, in particular for the problem y′(t) = 0 with y(0) = 0, whose
solution is, clearly, the null function. Similarly, the solution u(h)
n
of (11.60) with
f = 0 and w(h)
k
= 0 for k = 0, . . . , p is identically zero.
Consider ﬁrst the case |ri| > 1. Then, deﬁne
δn =



εrn
i
if ri ∈R,
ε(ri + ¯ri)n
if ri ∈C,
for ε > 0. It is simple to check that the sequence z(h)
n
= δn for n = 0, 1, . . .
is a solution of (11.59) with initial conditions z(h)
k
= δk and that |δk| ≤ε for
k = 0, 1, . . . , p. Let us now choose ¯t in (t0, t0 + T) and let xn be the nearest grid
node to ¯t. Clearly, n is the integral part of ¯t/h and limh→0 |z(h)
n | = limh→0 |u(h)
n −
z(h)
n | →∞as h →0. This proves that |u(h)
n
−z(h)
n | cannot be uniformly bounded
with respect to h as h →0, which contradicts the assumption that the method
is zero-stable.
A similar proof can be carried out if |ri| = 1 but has multiplicity greater than
1, provided that one takes into account the form of the solution (11.33).
Let us now prove that the root condition is suﬃcient for method (11.45) to
be zero-stable. Recalling (11.46) and denoting by z(h)
n+j and u(h)
n+j the solutions
to (11.59) and (11.60), respectively, for j ≥1, it turns out that the function
w(h)
n+j = z(h)
n+j −u(h)
n+j satisﬁes the following diﬀerence equation
p+1

j=0
αjw(h)
n+j = ϕn+p+1,
n = 0, . . . , Nh −(p + 1),
(11.61)
having set
ϕn+p+1 = h
p+1

j=0
βj
6
f(tn+j, z(h)
n+j) −f(tn+j, u(h)
n+j)
7
+ hδn+p+1.
(11.62)

11.6 Analysis of Multistep Methods
497
Denote by
2
ψ(n)
j
3
a sequence of fundamental solutions to the homogeneous equa-
tion associated with (11.61). Recalling (11.42), the general solution of (11.61) is
given by
w(h)
n
=
p

j=0
w(h)
j
ψ(n)
j
+
n

l=p+1
ψ(n−l+p)
p
ϕl,
n = p + 1, . . .
The following result expresses the connection between the root condition and the
boundedness of the solution of a diﬀerence equation (for the proof, see [Gau97],
Theorem 6.3.2).
Lemma 11.3 There exists a constant M > 0 for any solution {un} of the dif-
ference equation (11.28) such that
|un| ≤M
"
max
j=0,... ,k−1|uj| +
n

l=k
|ϕl|
#
,
n = 0, 1, . . .
(11.63)
iﬀthe root condition is satisﬁed for the polynomial (11.30), i.e., (11.56) holds for
the zeros of the polynomial (11.30).
Let us now recall that, for any j, {ψ(n)
j
} is solution of a homogeneous diﬀerence
equation whose initial data are ψ(i)
j
= δij, i, j = 0, . . . , p. On the other hand, for
any l, ψ(n−l+p)
p
is solution of a diﬀerence equation with zero initial conditions and
right-hand sides equal to zero except for the one corresponding to n = l which is
ψ(p)
p
= 1.
Therefore, Lemma 11.3 can be applied in both cases so we can conclude that
there exists a constant M > 0 such that |ψ(n)
j
| ≤M and |ψ(n−l+p)
p
| ≤M,
uniformly with respect to n and l. The following estimate thus holds
|w(h)
n | ≤M


(p + 1) max
j=0,... ,p|w(h)
j
| +
n

l=p+1
|ϕl|


,
n = 0, 1, . . . , Nh.
(11.64)
If L denotes the Lipschitz constant of f, from (11.62) we get
|ϕn+p+1| ≤h
max
j=0,... ,p+1|βj|L
p+1

j=0
|w(h)
n+j| + h|δn+p+1|.
Let β =
max
j=0,... ,p+1|βj| and ∆[q,r] =
max
j=q,... ,r|δj+q|, q and r being some integers
with q ≤r. From (11.64), the following estimate is therefore obtained
|w(h)
n |
≤
M


(p + 1)∆[0,p] + hβL
n

l=p+1
p+1

j=0
|w(h)
l−p−1+j| + Nhh∆[p+1,n]



≤
M
"
(p + 1)∆[0,p] + hβL(p + 2)
n

m=0
|w(h)
m | + T∆[p+1,n]
#
.

498
11. Numerical Solution of Ordinary Diﬀerential Equations
Let Q = 2(p + 2)βLM and h0 = 1/Q, so that 1 −h Q
2 ≥1
2 if h ≤h0. Then
1
2|w(h)
n |
≤
|w(h)
n |(1 −h Q
2 )
≤
M
"
(p + 1)∆[0,p] + hβL(p + 2)
n−1

m=0
|w(h)
m | + T∆[p+1,n]
#
.
Letting R = 2M

(p + 1)∆[0,p] + T∆[p+1,n]

, we ﬁnally obtain
|w(h)
n | ≤hQ
n−1

m=0
|w(h)
m | + R.
Applying Lemma 11.2 with the following identiﬁcations: ϕn = |w(h)
n |, g0 = R,
ps = 0 and ks = hQ for any s = 0, . . . , n −1, yields
|w(h)
n | ≤2MeT Q 
(p + 1)∆[0,p] + T∆[p+1,n]

.
(11.65)
Method (11.45) is thus zero-stable for any h ≤h0.
3
Theorem 11.4 allows for characterizing the stability behavior of several
families of discretization methods.
In the special case of consistent one-step methods, the polynomial ρ ad-
mits only the root r0 = 1. They thus automatically satisfy the root condition
and are zero-stable.
For the Adams methods (11.49), the polynomial ρ is always of the form
ρ(r) = rp+1 −rp. Thus, its roots are r0 = 1 and r1 = 0 (with multiplicity
p) so that all Adams methods are zero-stable.
Also the midpoint method (11.43) and Simpson method (11.44) are zero-
stable: for both of them, the ﬁrst characteristic polynomial is ρ(r) = r2 −1,
so that r0 = 1 and r1 = −1.
Finally, the BDF methods of Section 11.5.2 are zero-stable provided that
p ≤5, since in such a case the root condition is satisﬁed (see [Cry73]).
We are in position to give the following convergence result.
Theorem 11.5 (Convergence) A consistent multistep method is conver-
gent iﬀit satisﬁes the root condition and the error on the initial data tends
to zero as h →0. Moreover, the method converges with order q if it has
order q and the error on the initial data tends to zero as O(hq).
Proof. Suppose that the MS method is consistent and convergent. To prove that
the root condition is satisﬁed, we refer to the problem y′(t) = 0 with y(0) = 0
and on the interval I = (0, T). Convergence means that the numerical solution
{un} must tend to the exact solution y(t) = 0 for any converging set of initial
data uk, k = 0, . . . , p, i.e.
max
k=0,... ,p|uk| →0 as h →0. From this observation, the
proof follows by contradiction along the same lines as the proof of Theorem 11.4,
where the parameter ε is now replaced by h.

11.6 Analysis of Multistep Methods
499
Let us now prove that consistency, together with the root condition, implies
convergence under the assumption that the error on the initial data tends to zero
as h →0. We can apply Theorem 11.4, setting u(h)
n
= un (approximate solution
of the Cauchy problem) and z(h)
n
= yn (exact solution), and from (11.47) it turns
out that δm = τm(h). Then, due to (11.65), for any n ≥p + 1 we obtain
|un −yn| ≤2MeT Q
%
(p + 1) max
j=0,... ,p|uj −yj| + T
max
j=p+1,... ,n|τj(h)|
&
.
Convergence holds by noticing that the right-hand side of this inequality tends
to zero as h →0.
3
A remarkable consequence of the above theorem is the following equivalence
Lax-Richtmyer theorem.
Corollary 11.1 (Equivalence theorem) A consistent multistep method
is convergent iﬀit is zero-stable and if the error on the initial data tends
to zero as h tends to zero.
We conclude this section with the following result, which establishes an
upper limit for the order of multistep methods (see [Dah63]).
Property 11.1 (First Dahlquist barrier) There isn’t any zero-stable,
p-step linear multistep method with order greater than p + 1 if p is odd,
p + 2 if p is even.
11.6.4
Absolute Stability of Multistep Methods
Consider again the diﬀerence equation (11.54), which was obtained by ap-
plying the MS method (11.45) to the model problem (11.24). According to
(11.33), its solution takes the form
un =
k′

j=1
mj−1

s=0
γsjns

[rj(hλ)]n,
n = 0, 1, . . .
where rj(hλ), j = 1, . . . , k′, are the distinct roots of the characteristic
polynomial (11.55), and having denoted by mj the multiplicity of rj(hλ).
In view of (11.25), it is clear that the absolute root condition introduced
by Deﬁnition 11.12 is necessary and suﬃcient to ensure that the multistep
method (11.45) is absolutely stable as h ≤h0.
Among the methods enjoying the absolute stability property, the pref-
erence should go to those for which the region of absolute stability A,
introduced in (11.26), is as wide as possible or even unbounded. Among
these are the A-stable methods introduced at the end of Section 11.3.3 and

500
11. Numerical Solution of Ordinary Diﬀerential Equations
the ϑ-stable methods, for which A contains the angular region deﬁned by
z ∈C such that −ϑ < π −arg(z) < ϑ, with ϑ ∈(0, π/2). A-stable meth-
ods are of remarkable importance when solving stiﬀproblems (see Section
11.10).
                                                                                                                                                                                                                                                              




















                                                                                                                                                                                                                                                              




















                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     








































Im
Re
Im
Re
ϑ
ϑ
FIGURE 11.4. Regions of absolute stability for A-stable (left) and (right) ϑ-stable
methods
The following result, whose proof is given in [Wid67], establishes a relation
between the order of a multistep method, the number of its steps and its
stability properties.
Property 11.2 (Second Dahlquist barrier) A linear explicit multistep
method can be neither A-stable, nor ϑ-stable. Moreover, there is no A-
stable linear multistep method with order greater than 2. Finally, for any
ϑ ∈(0, π/2), there only exist ϑ-stable p-step linear multistep methods of
order p for p = 3 and p = 4.
Let us now examine the region of absolute stability of several MS methods.
The regions of absolute stability of both explicit and implicit Adams schemes
reduce progressively as the order of the method increases. In Figure 11.5
(left) we show the regions of absolute stability for the AB methods exam-
ined in Section 11.5.1, with exception of the Forward Euler method whose
region is shown in Figura 11.3.
The regions of absolute stability of the Adams-Moulton schemes, except
for the Crank-Nicolson method which is A-stable, are represented in Figure
11.5 (right).
In Figure 11.6 the regions of absolute stability of some of the BDF meth-
ods introduced in Section 11.5.2 are drawn. They are unbounded and al-

11.6 Analysis of Multistep Methods
501
−2
−1.5
−1
−0.5
0
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
AB2
AB3
AB4
−6
−4
−2
0
−4
−3
−2
−1
0
1
2
3
4
AM3
AM4
AM5
FIGURE
11.5.
Outer
contours
of
the
regions
of
absolute
stability
for
Adams-Bashforth methods (left) ranging from second to fourth-order (AB2, AB3
and AB4) and for Adams-Moulton methods (right), from third to ﬁfth-order
(AM3, AM4 and AM5). Notice that the region of the AB3 method extends into
the half-plane with positive real part. The region for the explicit Euler (AB1)
method was drawn in Figure 11.3
ways contain the negative real numbers. These stability features make BDF
methods quite attractive for solving stiﬀproblems (see Section 11.10).
−4
−2
0
2
4
6
8
10
12
14
−6
−4
−2
0
2
4
6
BDF6
BDF5
BDF3
FIGURE 11.6. Inner contours of regions of absolute stability for three-step
(BDF3), ﬁve-step (BDF5) and six-step (BDF6) BDF methods. Unlike Adams
methods, these regions are unbounded and extend outside the limited portion
that is shown in the ﬁgure
Remark 11.3 Some authors (see, e.g., [BD74]) adopt an alternative deﬁ-
nition of absolute stability by replacing (11.25) with the milder property
∃C > 0 : |un| ≤C, as tn →+∞.
According to this new deﬁnition, the absolute stability of a numerical
method should be regarded as the counterpart of the asymptotic stabil-

502
11. Numerical Solution of Ordinary Diﬀerential Equations
ity (11.6) of the Cauchy problem. The new region of absolute stability A∗
would be
A∗= {z ∈C : ∃C > 0, |un| ≤C, ∀n ≥0}
and it would not necessarily coincide with A. For example, in the case of
the midpoint method A is empty (thus, it is unconditionally absolutely
unstable), while A∗= {z = αi, α ∈[−1, 1]}.
In general, if A is nonempty, then A∗is its closure. We notice that zero-
stable methods are those for which the region A∗contains the origin z = 0
of the complex plane.
■
To conclude, let us notice that the strong root condition (11.57) implies,
for a linear problem, that
∀h ≤h0, ∃C > 0 : |un| ≤C(|u0| + . . . + |up|),
∀n ≥p + 1.
(11.66)
We say that a method is relatively stable if it satisﬁes (11.66). Clearly,
(11.66) implies zero-stability, but the converse does not hold.
Figure 11.7 summarizes the main conclusions that have been drawn in this
section about stability, convergence and root-conditions, in the particular
case of a consistent method applied to the model problem (11.24).
Root
⇐=
Strong root
⇐=
Absolute root
condition
condition
condition
CDE
DDE
CDE
Convergence
⇐⇒
Zero
⇐=
(11.66)
⇐=
Absolute
stability
stability
FIGURE 11.7. Relations between root conditions, stability and convergence for
a consistent method applied to the model problem (11.24)
11.7
Predictor-Corrector Methods
When solving a nonlinear Cauchy problem of the form (11.1), at each time
step implicit schemes require dealing with a nonlinear equation. For in-
stance, if the Crank-Nicolson method is used, we get the nonlinear equation
un+1 = un + h
2 [fn + fn+1] = Ψ(un+1),

11.7 Predictor-Corrector Methods
503
that can be cast in the form Φ(un+1) = 0, where Φ(un+1) = un+1 −
Ψ(un+1). To solve this equation the Newton method would give
u(k+1)
n+1
= u(k)
n+1 −Φ(u(k)
n+1)/Φ′(u(k)
n+1),
for k = 0, 1, . . . , until convergence and require an initial datum u(0)
n+1 suﬃ-
ciently close to un+1. Alternatively, one can resort to ﬁxed-point iterations
u(k+1)
n+1
= Ψ(u(k)
n+1)
(11.67)
for k = 0, 1, . . . , until convergence. In such a case, the global convergence
condition for the ﬁxed-point method (see Theorem 6.1) sets a constraint
on the discretization stepsize of the form
h <
1
|b−1|L
(11.68)
where L is the Lipschitz constant of f with respect to y. In practice, ex-
cept for the case of stiﬀproblems (see Section 11.10), this restriction on
h is not signiﬁcant since considerations of accuracy put a much more re-
strictive constraint on h. However, each iteration of (11.67) requires one
evaluation of the function f and the computational cost can be reduced by
providing a good initial guess u(0)
n+1. This can be done by taking one step of
an explicit MS method and then iterating on (11.67) for a ﬁxed number m
of iterations. By doing so, the implicit MS method that is employed in the
ﬁxed-point scheme “corrects” the value of un+1 “predicted” by the explicit
MS method. A procedure of this sort is called a predictor-corrector method,
or PC method. There are many ways in which a predictor-corrector method
can be implemented.
In its basic version, the value u(0)
n+1 is computed by an explicit ˜p + 1-step
method, called the predictor (here identiﬁed by the coeﬃcients {˜aj, ˜bj})
[P]
u(0)
n+1 =
˜p

j=0
˜aju(1)
n−j + h
˜p

j=0
˜bjf (0)
n−j,
where f (0)
k
= f(tk, u(0)
k ) and u(1)
k
are the solutions computed by the PC
method at the previous steps or are the initial conditions. Then, we evaluate
the function f at the new point (tn+1, u(0)
n+1) (evaluation step)
[E]
f (0)
n+1 = f(tn+1, u(0)
n+1),
and ﬁnally, one single ﬁxed-point iteration is carried out using an implicit
MS scheme of the form (11.45)
[C]
u(1)
n+1 =
p

j=0
aju(1)
n−j + hb−1f (0)
n+1 + h
p

j=0
bjf (0)
n−j.

504
11. Numerical Solution of Ordinary Diﬀerential Equations
This second step of the procedure, which is actually explicit, is called the
corrector. The overall procedure is shortly denoted by PEC or P(EC)1
method, in which P and C denote one application at time tn+1 of the
predictor and the corrector methods, respectively, while E indicates one
evaluation of the function f.
This strategy above can be generalized supposing to perform m > 1 iter-
ations at each step tn+1. The corresponding methods are called predictor-
multicorrector schemes and compute u(0)
n+1 at time step tn+1 using the pre-
dictor in the following form
[P]
u(0)
n+1 =
˜p

j=0
˜aju(m)
n−j + h
˜p

j=0
˜bjf (m−1)
n−j
.
(11.69)
Here m ≥1 denotes the (ﬁxed) number of corrector iterations that are
carried out in the following steps [E], [C]: for k = 0, 1, . . . , m −1
[E]
f (k)
n+1 = f(tn+1, u(k)
n+1),
[C]
u(k+1)
n+1
=
p

j=0
aju(m)
n−j + hb−1f (k)
n+1 + h
p

j=0
bjf (m−1)
n−j
.
These implementations of the predictor-corrector technique are referred to
as P(EC)m. Another implementation, denoted by P(EC)mE, consists of
updating at the end of the process also the function f and is given by
[P]
u(0)
n+1 =
˜p

j=0
˜aju(m)
n−j + h
˜p

j=0
˜bjf (m)
n−j,
and for k = 0, 1, . . . , m −1,
[E]
f (k)
n+1 = f(tn+1, u(k)
n+1),
[C]
u(k+1)
n+1
=
p

j=0
aju(m)
n−j + hb−1f (k)
n+1 + h
p

j=0
bjf (m)
n−j,
followed by
[E]
f (m)
n+1 = f(tn+1, u(m)
n+1).
Example 11.8 Heun’s method (11.10) can be regarded as a predictor-corrector
method whose predictor is the forward Euler method, while the corrector is the
Crank-Nicolson method.
Another example is provided by the Adams-Bashforth method of order 2
(11.50) and the Adams-Moulton method of order 3 (11.51). Its corresponding

11.7 Predictor-Corrector Methods
505
PEC implementation is: given u(0)
0
= u(1)
0
= u0, u(0)
1
= u(1)
1
= u1 and f (0)
0
=
f(t0, u(0)
0 ), f (0)
1
= f(t1, u(0)
1 ), compute for n = 1, 2, . . . ,
[P]
u(0)
n+1 = u(1)
n
+ h
2
6
3f (0)
n
−f (0)
n−1
7
,
[E]
f (0)
n+1 = f(tn+1, u(0)
n+1),
[C]
u(1)
n+1 = u(1)
n
+ h
12
6
5f (0)
n+1 + 8f (0)
n
−f (0)
n−1
7
,
while the PECE implementation is: given u(0)
0
= u(1)
0
= u0, u(0)
1
= u(1)
1
= u1 and
f (1)
0
= f(t0, u(1)
0 ), f (1)
1
= f(t1, u(1)
1 ), compute for n = 1, 2, . . . ,
[P]
u(0)
n+1 = u(1)
n
+ h
2
6
3f (1)
n
−f (1)
n−1
7
,
[E]
f (0)
n+1 = f(tn+1, u(0)
n+1),
[C]
u(1)
n+1 = u(1)
n
+ h
12
6
5f (0)
n+1 + 8f (1)
n
−f (1)
n−1
7
,
[E]
f (1)
n+1 = f(tn+1, u(1)
n+1).
•
Before studying the convergence of predictor-corrector methods, we intro-
duce a simpliﬁcation in the notation. Usually the number of steps of the
predictor is greater than those of the corrector, so that we deﬁne the num-
ber of steps of the predictor-corrector pair as being equal to the number of
steps of the predictor. This number will be denoted henceforth by p. Owing
to this deﬁnition we no longer demand that the coeﬃcients of the corrector
satisfy |ap| + |bp| ̸= 0. Consider for example the predictor-corrector pair
[P]
u(0)
n+1 = u(1)
n
+ hf(tn−1, u(0)
n−1),
[C]
u(1)
n+1 = u(1)
n
+ h
2
6
f(tn, u(0)
n ) + f(tn+1, u(0)
n+1)
7
,
for which p = 2 (even though the corrector is a one-step method). Conse-
quently, the ﬁrst and the second characteristic polynomials of the corrector
method will be ρ(r) = r2 −r and σ(r) = (r2 + r)/2 instead of ρ(r) = r −1
and σ(r) = (r + 1)/2.
In any predictor-corrector method, the truncation error of the predictor
combines with the one of the corrector, generating a new truncation error
which we are going to examine. Let ˜q and q be, respectively, the orders of the
predictor and the corrector and assume that y ∈Cq+1, where q = max(˜q, q).

506
11. Numerical Solution of Ordinary Diﬀerential Equations
Then
y(tn+1)
−
p

j=0
˜ajy(tn−j) −h
p

j=0
˜bjf(tn−j, yn−j)
=
˜C˜q+1h˜q+1y(˜q+1)(tn) + O(h˜q+2),
y(tn+1)
−
p

j=0
ajy(tn−j) −h
p

j=−1
bjf(tn−j, yn−j)
=
Cq+1hq+1y(q+1)(tn) + O(hq+2),
where ˜C˜q+1, Cq+1 are the error constants of the predictor and the corrector
method respectively. The following result holds.
Property 11.3 Let the predictor method have order ˜q and the corrector
method have order q. Then:
If ˜q ≥q (or ˜q < q with m > q −˜q), then the predictor-corrector method
has the same order and the same PLTE as the corrector.
If ˜q < q and m = q −˜q, then the predictor-corrector method has the same
order as the corrector, but diﬀerent PLTE.
If ˜q < q and m ≤q −˜q −1, then the predictor-corrector method has order
equal to ˜q + m (thus less than q).
In particular, notice that if the predictor has order q −1 and the corrector
has order q, the PEC suﬃces to get a method of order q. Moreover, the
P(EC)mE and P(EC)m schemes have always the same order and the same
PLTE.
Combining the Adams-Bashforth method of order q with the correspond-
ing Adams-Moulton method of the same order we obtain the so-called ABM
method of order q. It is possible to estimate its PLTE as
Cq+1
C∗
q+1 −Cq+1
+
u(m)
n+1 −u(0)
n+1
,
,
where Cq+1 and C∗
q+1 are the error constants given in Table 11.1. Accord-
ingly, the steplength h can be decreased if the estimate of the PLTE exceeds
a given tolerance and increased otherwise (for the adaptivity of the step
length in a predictor-corrector method, see [Lam91], pp.128–147).
Program 93 provides an implementation of the P(EC)mE methods. The
input parameters at, bt, a, b contain the coeﬃcients ˜aj,˜bj (j = 0, . . . , ˜p)
of the predictor and the coeﬃcients aj (j = 0, . . . , p), bj (j = −1, . . . , p) of
the corrector. Moreover, f is a string containing the expression of f(t, y),

11.7 Predictor-Corrector Methods
507
h is the stepsize, t0 and tf are the end points of the time integration
interval, u0 is the vector of the initial data, m is the number of the corrector
inner iterations. The input variable pece must be set equal to ’y’ if the
P(EC)mE is selected, conversely the P(EC)m scheme is chosen.
Program 93 - predcor : Predictor-corrector scheme
function [u,t]=predcor(a,b,at,bt,h,f,t0,u0,tf,pece,m)
p = max(length(a),length(b)-1); pt = max(length(at),length(bt));
q = max(p,pt); if length(u0) < q, break, end;
t = [t0:h:t0+(q-1)*h]; u = u0; y = u0; fe = eval(f);
k = q;
for t = t0+q*h:h:tf
ut = sum(at.*u(k:-1:k-pt+1))+h*sum(bt.*fe(k:-1:k-pt+1));
y = ut; foy = eval(f);
uv = sum(a.*u(k:-1:k-p+1))+h*sum(b(2:p+1).*fe(k:-1:k-p+1));
k = k+1;
for j = 1:m
fy = foy; up = uv + h*b(1)*fy; y = up; foy = eval(f);
end
if (pece==’y’|pece==’Y’)
fe = [fe, foy];
else
fe = [fe, fy];
end
u = [u, up];
end
t = [t0:h:tf];
Example 11.9 Let us check the performance of the P(EC)mE method on the
Cauchy problem y′(t) = e−y(t) for t ∈[0, 1] with y(0) = 1. The exact solution is
y(t) = log(1 + t). In all the numerical experiments, the corrector method is the
Adams-Moulton third-order scheme (AM3), while the explicit Euler (AB1) and
the Adams-Bashforth second-order (AB2) methods are used as predictors. Figure
11.8 shows that the pair AB2-AM3 (m = 1) yields third-order convergence rate,
while AB1-AM3 (m = 1) has a ﬁrst-order accuracy. Taking m = 2 allows to
recover the third-order convergence rate of the corrector for the AB1-AM3 pair.
•
As for the absolute stability, the characteristic polynomial of P(EC)m
methods reads
ΠP (EC)m(r) = b−1rp (ρ(r) −hλσ(r)) + Hm(1 −H)
1 −Hm
(˜ρ(r)σ(r) −ρ(r)˜σ(r))
while for P(EC)mE we have
ΠP (EC)mE(r) = ρ(r) −hλσ(r) + Hm(1 −H)
1 −Hm
(˜ρ(r) −hλ˜σ(r)) .

508
11. Numerical Solution of Ordinary Diﬀerential Equations
10
−3
10
−2
10
−1
10
−12
10
−10
10
−8
10
−6
10
−4
10
−2
FIGURE 11.8. Convergence rate for P(EC)mE methods as a function of log(h).
The symbol ∇refers to the AB2-AM3 method (m = 1), ◦to AB1-AM3 (m = 1)
and □to AB1-AM3 with m = 2
We have set H = hλb−1 and denoted by ˜ρ and ˜σ the ﬁrst and second charac-
teristic polynomial of the predictor method, respectively. The polynomials
ρ and σ are related to the ﬁrst and second characteristic polynomials of the
corrector, as previously explained after Example 11.8. Notice that in both
cases the characteristic polynomial tends to the corresponding polynomial
of the corrector method, since the function Hm(1 −H)/(1 −Hm) tends to
zero as m tends to inﬁnity.
Example 11.10 If we consider the ABM methods with a number of steps p, the
characteristic polynomials are ρ(r) = ˜ρ(r) = r(rp−1 −rp−2), while σ(r) = rσ(r),
where σ(r) is the second characteristic polynomial of the corrector. In Figure 11.9
(right) the stability regions for the ABM methods of order 2 are plotted. In the
case of the ABM methods of order 2, 3 and 4, the corresponding stability regions
can be ordered by size, namely, from the largest to the smallest one the regions of
PECE, P(EC)2E, the predictor and PEC methods are plotted in Figure 11.9,
left. The one-step ABM method is an exception to the rule and the largest region
is the one corresponding to the predictor method (see Figure 11.9, left).
•
11.8
Runge-Kutta (RK) Methods
When evolving from the forward Euler method (11.7) toward higher-order
methods, linear multistep methods (MS) and Runge-Kutta methods (RK)
adopt two opposite strategies.
Like the Euler method, MS schemes are linear with respect to both un
and fn = f(tn, un), require only one functional evaluation at each time
step and their accuracy can be increased at the expense of increasing the
number of steps. On the other hand, RK methods maintain the structure
of one-step methods, and increase their accuracy at the price of an increase
of functional evaluations at each time level, thus sacrifying linearity.

11.8 Runge-Kutta (RK) Methods
509
−1.5
−1
−0.5
0
0.5
−1.5
−1
−0.5
0
0.5
1
1.5
PECE
P(EC)2E
PEC
P(EC)2
−2
−1.5
−1
−0.5
0
0.5
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
PECE
P(EC)2E
PEC
FIGURE 11.9. Stability regions for the ABM methods of order 1 (left) and 2
(right)
A consequence is that RK methods are more suitable than MS methods
at adapting the stepsize, whereas estimating the local error for RK methods
is more diﬃcult than it is in the case of MS methods.
In its most general form, an RK method can be written as
un+1 = un + hF(tn, un, h; f),
n ≥0
(11.70)
where F is the increment function deﬁned as follows
F(tn, un, h; f) =
s

i=1
biKi,
Ki = f(tn + cih, un + h
s

j=1
aijKj),
i = 1, 2, . . . , s
(11.71)
and s denotes the number of stages of the method. The coeﬃcients {aij},
{ci} and {bi} fully characterize an RK method and are usually collected in
the so-called Butcher array
c1
a11
a12
. . .
a1s
c2
a21
a22
a2s
...
...
...
...
cs
as1
as2
. . .
ass
b1
b2
. . .
bs
or
c
A
bT
where A = (aij) ∈Rs×s, b = (b1, . . . , bs)T ∈Rs and c = (c1, . . . , cs)T ∈
Rs. We shall henceforth assume that the following condition holds
ci =
s

j=1
aij
i = 1, . . . , s.
(11.72)

510
11. Numerical Solution of Ordinary Diﬀerential Equations
If the coeﬃcients aij in A are equal to zero for j ≥i, with i = 1, 2, . . . , s,
then each Ki can be explicitly computed in terms of the i −1 coeﬃcients
K1, . . . , Ki−1 that have already been determined. In such a case the RK
method is explicit. Otherwise, it is implicit and solving a nonlinear system
of size s is necessary for computing the coeﬃcients Ki.
The increase in the computational eﬀort for implicit schemes makes their
use quite expensive; an acceptable compromise is provided by RK semi-
implicit methods, in which case aij = 0 for j > i so that each Ki is the
solution of the nonlinear equation
Ki = f

tn + cih, un + haii Ki + h
i−1

j=1
aijKj

.
A semi-implicit scheme thus requires s nonlinear independent equations to
be solved.
The local truncation error τn+1(h) at node tn+1 of the RK method
(11.70) is deﬁned through the residual equation
hτn+1(h) = yn+1 −yn −hF(tn, yn, h; f),
where y(t) is the exact solution to the Cauchy problem (11.1). Method
(11.70) is consistent if τ(h) = maxn |τn(h)| →0 as h →0. It can be shown
(see [Lam91]) that this happens iﬀ
s

i=1
bi = 1.
As usual, we say that (11.70) is a method of order p (≥1) with respect to
h if τ(h) = O(hp) as h →0.
As for convergence, since RK methods are one-step methods, consistency
implies stability and, in turn, convergence. As happens for MS methods,
estimates of τ(h) can be derived; however, these estimates are often too
complicated to be proﬁtably used. We only mention that, as for MS meth-
ods, if an RK scheme has a local truncation error τn(h) = O(hp), for any
n, then also the convergence order will be equal to p.
The following result establishes a relation between order and number of
stages of explicit RK methods.
Property 11.4 The order of an s-stage explicit RK method cannot be
greater than s. Also, there do not exist s-stage explicit RK methods with
order s ≥5.
We refer the reader to [But87] for the proofs of this result and the results we
give below. In particular, for orders ranging between 1 and 10, the minimum

11.8 Runge-Kutta (RK) Methods
511
number of stages smin required to get a method of corresponding order is
shown below
order
1
2
3
4
5
6
7
8
smin
1
2
3
4
6
7
9
11
Notice that 4 is the maximum number of stages for which the order of the
method is not less than the number of stages itself. An example of a fourth-
order RK method is provided by the following explicit 4-stage method
un+1 = un + h
6 (K1 + 2K2 + 2K3 + K4)
K1 = fn,
K2 = f(tn + h
2 , un + h
2 K1),
K3 = f(tn + h
2 , un + h
2 K2),
K4 = f(tn+1, un + hK3).
(11.73)
As far as implicit schemes are concerned, the maximum achievable order
using s stages is equal to 2s.
Remark 11.4 (The case of systems of ODEs) An RK method can be
readily extended to systems of ODEs. However, the order of an RK method
in the scalar case does not necessarily coincide with that in the vector
case. In particular, for p ≥4, a method having order p in the case of the
autonomous system y′ = f(y), with f : Rm →Rn maintains order p even
when applied to an autonomous scalar equation y′ = f(y), but the converse
is not true. Regarding this concern, see [Lam91], Section 5.8.
■
11.8.1
Derivation of an Explicit RK Method
The standard technique for deriving an explicit RK method consists of en-
forcing that the highest number of terms in Taylor’s expansion of the exact
solution yn+1 about tn coincide with those of the approximate solution
un+1, assuming that we take one step of the RK method starting from the
exact solution yn. We provide an example of this technique in the case of
an explicit 2-stage RK method.
Let us consider a 2-stage explicit RK method and assume to dispose at
the n-th step of the exact solution yn. Then
un+1 = yn + hF(tn, yn, h; f) = yn + h(b1K1 + b2K2),
K1 = fn,
K2 = f(tn + hc2, yn + hc2K1),
having assumed that (11.72) is satisﬁed. Expanding K2 in a Taylor series
in a neighborhood of tn and truncating the expansion at the second order,

512
11. Numerical Solution of Ordinary Diﬀerential Equations
we get
K2 = fn + hc2(fn,t + K1fn,y) + O(h2).
We have denoted by fn,z (for z = t or z = y) the partial derivative of f
with respect to z evaluated at (tn, yn). Then
un+1 = yn + hfn(b1 + b2) + h2c2b2(fn,t + fnfn,y) + O(h3).
If we perform the same expansion on the exact solution, we ﬁnd
yn+1 = yn + hy′
n + h2
2 y′′
n + O(h3) = yn + hfn + h2
2 (fn,t + fnfn,y) + O(h3).
Forcing the coeﬃcients in the two expansions above to agree, up to higher-
order terms, we obtain that the coeﬃcients of the RK method must satisfy
b1 + b2 = 1, c2b2 = 1
2.
Thus, there are inﬁnitely many 2-stage explicit RK methods with second-
order accuracy. Two examples are the Heun method (11.10) and the modi-
ﬁed Euler method (11.91). Of course, with similar (and cumbersome) com-
putations in the case of higher-stage methods, and accounting for a higher
number of terms in Taylor’s expansion, one can generate higher-order RK
methods. For instance, retaining all the terms up to the ﬁfth one, we get
scheme (11.73).
11.8.2
Stepsize Adaptivity for RK Methods
Since RK schemes are one-step methods, they are well-suited to adapting
the stepsize h, provided that an eﬃcient estimator of the local error is
available. Usually, a tool of this kind is an a posteriori error estimator,
since the a priori local error estimates are too complicated to be used in
practice. The error estimator can be constructed in two ways:
- using the same RK method, but with two diﬀerent stepsizes (typically 2h
and h);
- using two RK methods of diﬀerent order, but with the same number s of
stages.
In the ﬁrst case, if an RK method of order p is being used, one pretends
that, starting from an exact datum un = yn (which would not be available
if n ≥1), the local error at tn+1 is less than a ﬁxed tolerance. The following
relation holds
yn+1 −un+1 = Φ(yn)hp+1 + O(hp+2),
(11.74)
where Φ is an unknown function evaluated at yn. (Notice that, in this
special case, yn+1 −un+1 = hτn+1(h)).

11.8 Runge-Kutta (RK) Methods
513
Carrying out the same computation with a stepsize of 2h, starting from
tn−1, and denoting by un+1 the computed solution, yields
yn+1 −un+1 = Φ(yn−1)(2h)p+1 + O(hp+2) = Φ(yn)(2h)p+1 + O(hp+2)
(11.75)
having expanded also yn−1 with respect to tn. Subtracting (11.74) from
(11.75), we get
(2p+1 −1)hp+1Φ(yn) = un+1 −un+1 + O(hp+2),
from which
yn+1 −un+1 ≃un+1 −un+1
(2p+1 −1)
= E.
If |E| is less than the ﬁxed tolerance ε, the scheme moves to the next time
step, otherwise the estimate is repeated with a halved stepsize. In general,
the stepsize is doubled whenever |E| is less than ε/2p+1.
This approach yields a considerable increase in the computational eﬀort,
due to the s −1 extra functional evaluations needed to generate the value
un+1. Moreover, if one needs to half the stepsize, the value un must also
be computed again.
An alternative that does not require extra functional evaluations consists
of using simultaneously two diﬀerent RK methods with s stages, of order
p and p + 1, respectively, which share the same set of values Ki. These
methods are synthetically represented by the modiﬁed Butcher array
c
A
bT 2
bT 2
ET 2
(11.76)
where the method of order p is identiﬁed by the coeﬃcients c, A and b,
while that of order p + 1 is identiﬁed by c, A and b, and where E = b −b.
Taking the diﬀerence between the approximate solutions at tn+1 pro-
duced by the two methods provides an estimate of the local truncation
error for the scheme of lower order. On the other hand, since the coeﬃ-
cients Ki coincide, this diﬀerence is given by h s
i=1 EiKi and thus it does
not require extra functional evaluations.
Notice that, if the solution un+1 computed by the scheme of order p is
used to initialize the scheme at time step n+2, the method will have order
p, as a whole. If, conversely, the solution computed by the scheme of order
p+1 is employed, the resulting scheme would still have order p+1 (exactly
as happens with predictor-corrector methods).
The Runge-Kutta Fehlberg method of fourth-order is one of the most
popular schemes of the form (11.76) and consists of a fourth-order RK

514
11. Numerical Solution of Ordinary Diﬀerential Equations
scheme coupled with a ﬁfth-order RK method (for this reason, it is known
as the RK45 method). The modiﬁed Butcher array for this method is shown
below
0
0
0
0
0
0
0
1
4
1
4
0
0
0
0
0
3
8
3
32
9
32
0
0
0
0
12
13
1932
2197
−7200
2197
7296
2197
0
0
0
1
439
216
−8
3680
513
−845
4104
0
0
1
2
−8
27
2
−3544
2565
1859
4104
−11
40
0
25
216
0
1408
2565
2197
4104
−1
5
0
16
135
0
6656
12825
28561
56430
−9
50
2
55
1
360
0
−128
4275
−2197
75240
1
50
2
55
This method tends to underestimate the error. As such, its use is not com-
pletely reliable when the stepsize h is large.
Remark 11.5 MATLAB provides a package tool funfun, which, besides
the two classical Runge-Kutta Fehlberg methods, RK23 (second-order and
third-order pair) and RK45 (fourth-order and ﬁfth-order pair), also imple-
ments other methods suitable for solving stiﬀproblems. These methods are
derived from BDF methods (see [SR97]) and are included in the MATLAB
program ode15s.
■
11.8.3
Implicit RK Methods
Implicit RK methods can be derived from the integral formulation of the
Cauchy problem (11.2). In fact, if a quadrature formula with s nodes in
(tn, tn+1) is employed to approximate the integral of f (which we assume,
for simplicity, to depend only on t), we get
tn+1
>
tn
f(τ) dτ ≃h
s

j=1
bjf(tn + cjh)
having denoted by bj the weights and by tn + cjh the quadrature nodes. It
can be proved (see [But64]) that for any RK formula (11.70)-(11.71), there
exists a correspondence between the coeﬃcients bj, cj of the formula and
the weights and nodes of a Gauss quadrature rule.
In particular, the coeﬃcients c1, . . . , cs are the roots of the Legendre
polynomial Ls in the variable x = 2c −1, so that x ∈[−1, 1]. Once the

11.8 Runge-Kutta (RK) Methods
515
s coeﬃcients cj have been found, we can construct RK methods of order
2s, by determining the coeﬃcients aij and bj as being the solutions of the
linear systems
s

j=1
ck−1
j
aij = (1/k)ck
i ,
k = 1, 2, . . . , s,
s

j=1
ck−1
j
bj = 1/k,
k = 1, 2, . . . , s.
The following families can be derived:
1. Gauss-Legendre RK methods, if Gauss-Legendre quadrature nodes are
used. These methods, for a ﬁxed number of stages s, attain the maximum
possible order 2s. Remarkable examples are the one-stage method (implicit
midpoint method) of order 2
un+1 = un + hf

tn + 1
2h, 1
2(un + un+1)

,
1
2
1
2
1
and the 2-stage method of order 4, described by the following Butcher array
3−
√
3
6
1
4
3−2
√
3
12
3+
√
3
6
3+2
√
3
12
1
4
1
2
1
2
2. Gauss-Radau methods, which are characterized by the fact that the
quadrature nodes include one of the two endpoints of the interval (tn, tn+1).
The maximum order that can be achieved by these methods is 2s−1, when
s stages are used. Elementary examples correspond to the following Butcher
arrays
0
1
1 ,
1
1
1 ,
1
3
5
12
−1
12
1
3
4
1
4
3
4
1
4
and have order 1, 1 and 3, respectively. The Butcher array in the middle
represents the backward Euler method.
3. Gauss-Lobatto methods, where both the endpoints tn and tn+1 are quadra-
ture nodes. The maximum order that can be achieved using s stages is
2s −2. We recall the methods of the family corresponding to the following

516
11. Numerical Solution of Ordinary Diﬀerential Equations
Butcher arrays
0
0
0
1
1
2
1
2
1
2
1
2
,
0
1
2
0
1
1
2
0
1
2
1
2
,
0
1
6
−1
3
1
6
1
2
1
6
5
12
−1
12
1
1
6
2
3
1
6
1
6
2
3
1
6
which have order 2, 2 and 3, respectively. The ﬁrst array represents the
Crank-Nicolson method.
As for semi-implicit RK methods, we limit ourselves to mentioning the
case of DIRK methods (diagonally implicit RK), which, for s = 3, are
represented by the following Butcher array
1+µ
2
1+µ
2
0
0
1
2
−µ
2
1+µ
2
0
1−µ
2
1 + µ
−1 −2µ
1+µ
2
1
6µ2
1 −
1
3µ2
1
6µ2
The parameter µ represents one of the three roots of 3µ3 −3µ−1 = 0 (i.e.,
(2/
√
3) cos(10o), −(2/
√
3) cos(50o), −(2/
√
3) cos(70o)). The maximum or-
der that has been determined in the literature for these methods is 4.
11.8.4
Regions of Absolute Stability for RK Methods
Applying an s-stage RK method to the model problem (11.24) yields
Ki = un + hλ
s

i=1
aijKj,
un+1 = un + hλ
s

i=1
biKi,
(11.77)
that is, a ﬁrst-order diﬀerence equation. If K and 1 are the vectors of com-
ponents (K1, . . . , Ks)T and (1, . . . , 1)T , respectively, then (11.77) becomes
K = un1 + hλAK,
un+1 = un + hλbT K,
from which, K = (I −hλA)−11un and thus
un+1 =
0
1 + hλbT (I −hλA)−11
1
un = R(hλ)un
where R(hλ) is the so-called stability function.
The RK method is absolutely stable, i.e., the sequence {un} satisﬁes
(11.25), iﬀ|R(hλ)| < 1. Its region of absolute stability is given by
A = {z = hλ ∈C such that |R(hλ)| < 1} .

11.9 Systems of ODEs
517
If the method is explicit, A is strictly lower triangular and the function R
can be written in the following form (see [DV84])
R(hλ) = det(I −hλA + hλ1bT )
det(I −hλA)
.
Thus since det(I−hλA) = 1, R(hλ) is a polynomial function in the variable
hλ, |R(hλ)| can never be less than 1 for all values of hλ. Consequently, A
can never be unbounded for an explicit RK method.
In the special case of an explicit RK of order s = 1, . . . , 4, one gets (see
[Lam91])
R(hλ) =
s

k=0
1
k!(hλ)k.
The corresponding regions of absolute stability are drawn in Figure 11.10.
Notice that, unlike MS methods, the regions of absolute stability of RK
methods increase in size as the order grows.
−4
−3
−2
−1
0
1
2
0
0.5
1
1.5
2
2.5
3
3.5
4
s=1
s=2
s=4
s=3
FIGURE 11.10. Regions of absolute stability for s-stage explicit RK methods,
with s = 1, . . . , 4. The plot only shows the portion Im(hλ) ≥0 since the regions
are symmetric about the real axis
We ﬁnally notice that the regions of absolute stability for explicit RK meth-
ods can fail to be connected; an example is given in Exercise 14.
11.9
Systems of ODEs
Let us consider the system of ﬁrst-order ODEs
y′ = F(t, y),
(11.78)
where F : R × Rn →Rn is a given vector function and y ∈Rn is the
solution vector which depends on n arbitrary constants set by the n initial

518
11. Numerical Solution of Ordinary Diﬀerential Equations
conditions
y(t0) = y0.
(11.79)
Let us recall the following property (see [PS91], p. 209).
Property 11.5 Let F : R × Rn →Rn be a continuous function on D =
[t0, T] × Rn, with t0 and T ﬁnite. Then, if there exists a positive constant
L such that
∥F(t, y) −F(t, ¯y)∥≤L∥y −¯y∥
(11.80)
holds for any (t, y) and (t, ¯y) ∈D, then, for any y0 ∈Rn there exists a
unique y, continuous and diﬀerentiable with respect to t for any (t, y) ∈D,
which is a solution of the Cauchy problem (11.78)-(11.79).
Condition (11.80) expresses the fact that F is Lipschitz continuous with
respect to the second argument.
It is seldom possible to write out in closed form the solution to system
(11.78). A special case is where the system takes the form
y′(t) = Ay(t),
(11.81)
with A∈Rn×n. Assume that A has n distinct eigenvalues λj, j = 1, . . . , n;
therefore, the solution y can be written as
y(t) =
n

j=1
Cjeλjtvj,
(11.82)
where C1, . . . , Cn are some constants and {vj} is a basis formed by the
eigenvectors of A, associated with the eigenvalues λj for j = 1, . . . , n. The
solution is determined by setting n initial conditions.
From the numerical standpoint, the methods introduced in the scalar
case can be extended to systems. A delicate matter is how to generalize the
theory developed about absolute stability.
With this aim, let us consider system (11.81). As previously seen, the
property of absolute stability is concerned with the behavior of the numer-
ical solution as t grows to inﬁnity, in the case where the solution of problem
(11.78) satisﬁes
∥y(t)∥→0
as t →∞.
(11.83)
Condition (11.83) is satisﬁed if all the real parts of the eigenvalues of A are
negative since this ensures that
eλjt = eReλjt(cos(Imλj) + i sin(Imλi)) →0,
as t →∞,
(11.84)

11.10 StiﬀProblems
519
from which (11.83) follows recalling (11.82). Since A has n distinct eigen-
values, there exists a nonsingular matrix Q such that Λ = Q−1AQ, Λ being
the diagonal matrix whose entries are the eigenvalues of A (see Section 1.8).
Introducing the auxiliary variable z = Q−1y, the initial system can there-
fore be transformed into
z′ = Λz.
(11.85)
Since Λ is a diagonal matrix, the results holding in the scalar case immedi-
ately apply to the vector case as well, provided that the analysis is repeated
on all the (scalar) equations of system (11.85).
11.10
StiﬀProblems
Consider a nonhomogeneous linear system of ODEs with constant coeﬃ-
cients
y′(t) = Ay(t) + ϕ(t),
with A ∈Rn×n,
ϕ(t) ∈Rn,
and assume that A has n distinct eigenvalues λj, j = 1, . . . , n. Then
y(t) =
n

j=1
Cjeλjtvj + ψ(t)
where C1, . . . , Cn, are n constants, {vj} is a basis formed by the eigenvec-
tors of A and ψ(t) is a particular solution of the ODE at hand. Throughout
the section, we assume that Reλj < 0 for all j.
As t →∞, the solution y tends to the particular solution ψ. We can
therefore interpret ψ as the steady-state solution (that is, after an inﬁnite
time) and
n

j=1
Cjeλjt as being the transient solution (that is, for t ﬁnite).
Assume that we are interested only in the steady-state. If we employ a
numerical scheme with a bounded region of absolute stability, the stepsize h
is subject to a constraint that depends on the maximum module eigenvalue
of A. On the other hand, the greater this module, the shorter the time
interval where the corresponding component in the solution is meaningful.
We are thus faced with a sort of paradox: the scheme is forced to employ
a small integration stepsize to track a component of the solution that is
virtually ﬂat for large values of t.
Precisely, if we assume that
σ ≤Reλj ≤τ < 0,
∀j = 1, . . . , n
(11.86)
and introduce the stiﬀness quotient rs = σ/τ, we say that a linear system
of ODEs with constant coeﬃcients is stiﬀif the eigenvalues of matrix A all
have negative real parts and rs ≫1.

520
11. Numerical Solution of Ordinary Diﬀerential Equations
However, referring only to the spectrum of A to characterize the stiﬀness of
a problem might have some drawbacks. For instance, when τ ≃0, the stiﬀ-
ness quotient can be very large while the problem appears to be “genuinely”
stiﬀonly if |σ| is very large. Moreover, enforcing suitable initial conditions
can aﬀect the stiﬀness of the problem (for example, selecting the data in
such a way that the constants multiplying the “stiﬀ” components of the
solution vanish).
For this reason, several authors ﬁnd the previous deﬁnition of a stiﬀ
problem unsatisfactory, and, on the other hand, they agree on the fact that
it is not possible to exactly state what it is meant by a stiﬀproblem. We
limit ourselves to quoting only one alternative deﬁnition, which is of some
interest since it focuses on what is observed in practice to be a stiﬀproblem.
Deﬁnition 11.14 (from [Lam91], p. 220) A system of ODEs is stiﬀif,
when approximated by a numerical scheme characterized by a region of
absolute stability with ﬁnite size, it forces the method, for any initial con-
dition for which the problem admits a solution, to employ a discretization
stepsize excessively small with respect to the smoothness of the exact so-
lution.
■
From this deﬁnition, it is clear that no conditionally absolute stable method
is suitable for approximating a stiﬀproblem. This prompts resorting to
implicit methods, such as MS or RK, which are more expensive than explicit
schemes, but have regions of absolute stability of inﬁnite size. However, it
is worth recalling that, for nonlinear problems, implicit methods lead to
nonlinear equations, for which it is thus crucial to select iterative numerical
methods free of limitations on h for convergence.
For instance, in the case of MS methods, we have seen that using ﬁxed-
point iterations sets the constraint (11.68) on h in terms of the Lipschitz
constant L of f. In the case of a linear system this constraint is
L ≥
max
i=1,... ,n|λi|,
so that (11.68) would imply a strong limitation on h (which could even
be more stringent than those required for an explicit scheme to be stable).
One way of circumventing this drawback consists of resorting to Newton’s
method or some variants. The presence of Dahlquist barriers imposes a
strong limitation on the use of MS methods, the only exception being BDF
methods, which, as already seen, are θ-stable for p ≤5 (for a larger number
of steps they are even not zero-stable). The situation becomes deﬁnitely
more favorable if implicit RK methods are considered, as observed at the
end of Section 11.8.4.
The theory developed so far holds rigorously if the system is linear. In
the nonlinear case, let us consider the Cauchy problem (11.78), where the

11.11 Applications
521
function F : R × Rn →Rn is assumed to be diﬀerentiable. To study its
stability a possible strategy consists of linearizing the system as
y′(t) = F(τ, y(τ)) + JF(τ, y(τ)) [y(t) −y(τ)] ,
in a neighborhood (τ, y(τ)), where τ is an arbitrarily chosen value of t
within the time integration interval.
The above technique might be dangerous since the eigenvalues of JF do
not suﬃce in general to describe the behavior of the exact solution of the
original problem. Actually, some counterexamples can be found where:
1. JF has complex conjugate eigenvalues, while the solution of (11.78)
does not exhibit oscillatory behavior;
2. JF has real nonnegative eigenvalues, while the solution of (11.78) does
not grow monotonically with t;
3. JF has eigenvalues with negative real parts, but the solution of (11.78)
does not decay monotonically with t.
As an example of the case at item 3. let us consider the system of
ODEs
y′ =


−1
2t
2
t3
−t
2
−1
2t

y
=
A(t)y.
For t ≥1 its solution is
y(t) = C1
 t−3/2
−1
2t1/2

+ C2
 2t−3/2 log t
t1/2(1 −log t)

whose Euclidean norm diverges monotonically for t > (12)1/4 ≃1.86
when C1 = 1, C2 = 0, whilst the eigenvalues of A(t), equal to (−1 ±
2i)/(2t), have negative real parts.
Therefore, the nonlinear case must be tackled using ad hoc techniques, by
suitably reformulating the concept of stability itself (see [Lam91], Chapter
7).
11.11
Applications
We consider two examples of dynamical systems that are well-suited to
checking the performances of several numerical methods introduced in the
previous sections.

522
11. Numerical Solution of Ordinary Diﬀerential Equations
11.11.1
Analysis of the Motion of a Frictionless Pendulum
Let us consider the frictionless pendulum in Figure 11.11 (left), whose
motion is governed by the following system of ODEs
" y′
1 =
y2,
y′
2 =
−K sin(y1),
(11.87)
for t > 0, where y1(t) and y2(t) represent the position and angular velocity
of the pendulum at time t, respectively, while K is a positive constant
depending on the geometrical-mechanical parameters of the pendulum. We
consider the initial conditions: y1(0) = θ0, y2(0) = 0.
y
weight
1
A
A’
−π K
1/2
π K
1/2
FIGURE 11.11. Left: frictionless pendulum; right: orbits of system (11.87) in the
phase space
Denoting by y = (y1, y2)T the solution to system (11.87), this admits
inﬁnitely many equilibrium conditions of the form y = (nπ, 0)T for n ∈Z,
corresponding to the situations where the pendulum is vertical with zero
velocity. For n even, the equilibrium is stable, while for n odd it is unstable.
These conclusions can be drawn by analyzing the linearized system
y′ = Aey =
.
0
1
−K
0
/
y,
y′ = Aoy =
.
0
1
K
0
/
y.
If n is even, matrix Ae has complex conjugate eigenvalues λ1,2 = ±i
√
K
and associated eigenvectors y1,2 = (∓i/
√
K, 1)T , while for n odd, Ao
has real and opposite eigenvalues λ1,2 = ±
√
K and eigenvectors y1,2 =
(1/
√
K, ∓1)T .
Let us consider two diﬀerent sets of initial data: y(0) = (θ0, 0)T and
y(0) = (π + θ0, 0)T , where |θ0| ≪1. The solutions of the corresponding
linearized system are, respectively,
"
y1(t) =
θ0 cos(
√
Kt)
y2(t) =
−
√
Kθ0 sin(
√
Kt)
,
"
y1(t) =
(π + θ0) cosh(
√
Kt)
y2(t) =
√
K(π + θ0) sinh(
√
Kt),

11.11 Applications
523
and will be henceforth denoted as “stable” and “unstable”, respectively, for
reasons that will be clear later on. To these solutions we associate in the
plane (y1, y2), called the phase space, the following orbits (i.e., the graphs
obtained plotting the curve (y1(t), y2(t)) in the phase space).
y1
θ0
2
+

y2
√
Kθ0
2
= 1,
(stable case)

y1
π + θ0
2
−

y2
√
K(π + θ0)
2
= 1,
(unstable case).
In the stable case, the orbits are ellipses with period 2π/
√
K and are cen-
tered at (0, 0)T , while in the unstable case they are hyperbolae centered at
(0, 0)T and asymptotic to the straight lines y2 = ±
√
Ky1.
The complete picture of the motion of the pendulum in the phase space
is shown in Figure 11.11 (right). Notice that, letting v = |y2| and ﬁxing
the initial position y1(0) = 0, there exists a limit value vL = 2
√
K which
corresponds in the ﬁgure to the points A and A’. For v(0) < vL, the orbits
are closed, while for v(0) > vL they are open, corresponding to a continuous
revolution of the pendulum, with inﬁnite passages (with periodic and non
null velocity) through the two equilibrium positions y1 = 0 and y1 = π.
The limit case v(0) = vL yields a solution such that, thanks to the total
energy conservation principle, y2 = 0 when y1 = π. Actually, these two
values are attained asymptotically only as t →∞.
The ﬁrst-order nonlinear diﬀerential system (11.87) has been numerically
solved using the forward Euler method (FE), the midpoint method (MP)
and the Adams-Bashforth second-order scheme (AB). In Figure 11.12 we
show the orbits in the phase space that have been computed by the two
methods on the time interval (0, 30) and taking K = 1 and h = 0.1. The
crosses denote initial conditions.
As can be noticed, the orbits generated by FE do not close. This kind
of instability is due to the fact that the region of absolute stability of the
FE method completely excludes the imaginary axis. On the contrary, the
MP method describes accurately the closed system orbits due to the fact
that its region of asymptotic stability (see Section 11.6.4) includes pure
imaginary eigenvalues in the neighborhood of the origin of the complex
plane. It must also be noticed that the MP scheme gives rise to oscillating
solutions as v0 gets larger. The second-order AB method, instead, describes
correctly all kinds of orbits.
11.11.2
Compliance of Arterial Walls
An arterial wall subject to blood ﬂow can be modelled by a compliant
circular cylinder of length L and radius R0 with walls made by an incom-
pressible, homogeneous, isotropic, elastic tissue of thickness H. A simple

524
11. Numerical Solution of Ordinary Diﬀerential Equations
−10
−5
0
5
10
−2
0
2
−10
−5
0
5
10
−2
0
2
−10
−5
0
5
10
−2
0
2
FIGURE 11.12. Orbits for system (11.87) in the case K = 1 and h = 0.1, com-
puted using the FE method (upper plot), the MP method (central plot) and the
AB method (lower plot), respectively. The initial conditions are θ0 = π/10 and
v0 = 0 (thin solid line), v0 = 1 (dashed line), v0 = 2 (dash-dotted line) and
v0 = −2 (thick solid line)
model describing the mechanical behavior of the walls interacting with the
blood ﬂow is the so called “independent-rings” model according to which
the vessel wall is regarded as an assembly of rings which are not inﬂuenced
one by the others.
This amounts to neglecting the longitudinal (or axial) inner actions along
the vessel, and to assuming that the walls can deform only in the radial
direction. Thus, the vessel radius R is given by R(t) = R0 +y(t), where y is
the radial deformation of the ring with respect to a reference radius R0 and
t is the time variable. The application of Newton’s law to the independent-
ring system yields the following equation modeling the time mechanical

11.11 Applications
525
behavior of the wall
y′′(t) + βy′(t) + αy(t) = γ(p(t) −p0)
(11.88)
where α = E/(ρwR2
0), γ = 1/(ρwH) and β is a positive constant. The
physical parameters ρw and E denote the vascular wall density and the
Young modulus of the vascular tissue, respectively. The function p −p0
is the forcing term acting on the wall due to the pressure drop between
the inner part of the vessel (where the blood ﬂows) and its outer part
(surrounding organs). At rest, if p = p0, the vessel conﬁguration coincides
with the undeformed circular cylinder having radius equal exactly to R0
(y = 0).
Equation (11.88) can be formulated as y′(t) = Ay(t) + b(t) where y =
(y, y′)T , b = (0, −γ(p −p0))T and
A =

0
1
−α
−β

.
(11.89)
The eigenvalues of A are λ± = (−β ±

β2 −4α)/2; therefore, if β ≥2√α
both the eigenvalues are real and negative and the system is asymptotically
stable with y(t) decaying exponentially to zero as t →∞. Conversely, if 0 <
β < 2√α the eigenvalues are complex conjugate and damped oscillations
arise in the solution which again decays exponentially to zero as t →∞.
Numerical approximations have been carried out using both the backward
Euler (BE) and Crank-Nicolson (CN) methods. We have set y(t) = 0
and used the following (physiological) values of the physical parameters:
L = 5 · 10−2[m], R0 = 5 · 10−3[m], ρw = 103[Kgm−3], H = 3 · 10−4[m] and
E = 9 · 105[Nm−2], from which γ ≃3.3[Kg−1m−2] and α = 36 · 106[s−2].
A sinusoidal function p −p0 = x∆p(a + b cos(ω0t)) has been used to model
the pressure variation along the vessel direction x and time, where ∆p =
0.25 · 133.32 [Nm−2], a = 10 · 133.32 [Nm−2], b = 133.32 [Nm−2] and the
pulsation ω0 = 2π/0.8 [rad s−1] corresponds to a heart beat.
The results reported below refer to the ring coordinate x = L/2. The
two (very diﬀerent) cases (1) β = √α [s−1] and (2) β = α [s−1] have been
analyzed; it is easily seen that in case (2) the stiﬀness quotient (see Section
11.10) is almost equal to α, thus the problem is highly stiﬀ. We notice also
that in both cases the real parts of the eigenvalues of A are very large,
so that an appropriately small time step should be taken to accurately
describe the fast transient of the problem.
In case (1) the diﬀerential system has been studied on the time interval
[0, 2.5·10−3] with a time step h = 10−4. We notice that the two eigenvalues
of A have modules equal to 6000, thus our choice of h is compatible with
the use of an explicit method as well.
Figure 11.13 (left) shows the numerical solutions as functions of time.
The solid (thin) line is the exact solution while the thick dashed and solid

526
11. Numerical Solution of Ordinary Diﬀerential Equations
lines are the solutions given by the CN and BE methods, respectively. A far
better accuracy of the CN method over the BE is clearly demonstrated; this
is conﬁrmed by the plot in Figure 11.13 (right) which shows the trajectories
of the computed solutions in the phase space. In this case the diﬀerential
system has been integrated on the time interval [0, 0.25] with a time step
h = 2.5 · 10−4. The dashed line is the trajectory of the CN method while
the solid line is the corresponding one obtained using the BE scheme. A
strong dissipation is clearly introduced by the BE method with respect to
the CN scheme; the plot also shows that both methods converge to a limit
cycle which corresponds to the cosine component of the forcing term.
0
0.5
1
1.5
2
2.5
x 10−3
−2
0
2
4
6
8
10
12
14x 10−5
0
0.5
1
1.5
x 10−4
−0.1
0
0.1
0.2
0.3
0.4
0.5
0.6
FIGURE 11.13. Transient simulation (left) and phase space trajectories (right)
In the second case (2) the diﬀerential system has been integrated on the
time interval [0, 10] with a time step h = 0.1. The stiﬀness of the problem is
demonstrated by the plot of the deformation velocities z shown in Figure
11.14 (left). The solid line is the solution computed by the BE method
while the dashed line is the corresponding one given by the CN scheme; for
the sake of graphical clarity, only one third of the nodal values have been
plotted for the CN method. Strong oscillations arise since the eigenvalues of
matrix A are λ1 = −1, λ2 = −36·106 so that the CN method approximates
the ﬁrst component y of the solution y as
yCN
k
=
1 + (hλ1)/2
1 −(hλ1)/2
k
≃(0.9048)k,
k ≥0,
which is clearly stable, while the approximate second component z(= y′) is
zCN
k
=
1 + (hλ2)/2
1 −(hλ2)/2
k
≃(−0.9999)k,
k ≥0
which is obviously oscillating. On the contrary, the BE method yields
yBE
k
=

1
1 −hλ1
k
≃(0.9090)k,
k ≥0,

11.12 Exercises
527
and
zCN
k
=

1
1 −hλ2
k
≃(0.2777)k,
k ≥0
which are both stable for every h > 0. According to these conclusions the
ﬁrst component y of the vector solution y is correctly approximated by
both the methods as can be seen in Figure 11.14 (right) where the solid
line refers to the BE scheme while the dashed line is the solution computed
by the CN method.
0
1
2
3
4
5
6
7
8
9
10
−1.5
−1
−0.5
0
0.5
1
1.5
2 x 10
−4
0
1
2
3
4
5
6
7
8
9
10
0
0.2
0.4
0.6
0.8
1
1.2 x 10
−4
FIGURE 11.14. Long-time behavior of the solution: velocities (left) and displace-
ments (right)
11.12
Exercises
1. Prove that Heun’s method has order 2 with respect to h.
[Suggerimento : notice that hτn+1 = yn+1 −yn −hΦ(tn, yn; h) = E1 + E2,
where E1 =
2 tn+1
tn
f(s, y(s))ds −h
2 [f(tn, yn) + f(tn+1, yn+1)]
3
and E2 =
h
2 {[f(tn+1, yn+1) −f(tn+1, yn + hf(tn, yn))]}, where E1 is the error due to
numerical integration with the trapezoidal method and E2 can be bounded
by the error due to using the forward Euler method.]
2. Prove that the Crank-Nicoloson method has order 2 with respect to h.
[Solution : using (9.12) we get, for a suitable ξn in (tn, tn+1)
yn+1 = yn + h
2 [f(tn, yn) + f(tn+1, yn+1)] −h3
12f ′′(ξn, y(ξn))
or, equivalently,
yn+1 −yn
h
= 1
2 [f(tn, yn) + f(tn+1, yn+1)] −h2
12f ′′(ξn, y(ξn)).
(11.90)
Therefore, relation (11.9) coincides with (11.90) up to an inﬁnitesimal of
order 2 with respect to h, provided that f ∈C2(I).]

528
11. Numerical Solution of Ordinary Diﬀerential Equations
3. Solve the diﬀerence equation un+4 −6un+3 + 14un+2 −16un+1 + 8un = n
subject to the initial conditions u0 = 1, u1 = 2, u2 = 3 and u3 = 4.
[Solution : un = 2n(n/4 −1) + 2(n−2)/2 sin(π/4) + n + 2.]
4. Prove that if the characteristic polynomial Π deﬁned in (11.30) has simple
roots, then any solution of the associated diﬀerence equation can be written
in the form (11.32).
[Hint : notice that a generic solution un+k is completely determined by
the initial values u0, . . . , uk−1. Moreover, if the roots ri of Π are distinct,
there exist unique k coeﬃcients αi such that α1rj
1 + . . . + αkrj
k = uj with
j = 0, . . . , k −1 . . . ]
5. Prove that if the characteristic polynomial Π has simple roots, the matrix
R deﬁned in (11.37) is not singular.
[Hint: it coincides with the transpose of the Vandermonde matrix where
xj
i is replaced by ri
j (see Exercise 2, Chapter 8).]
6. The Legendre polynomials Li satisfy the diﬀerence equation
(n + 1)Ln+1(x) −(2n + 1)xLn(x) + nLn−1(x) = 0
with L0(x) = 1 and L1(x) = x (see Section 10.1.2). Deﬁning the generating
function F(z, x) = ∞
n=0 Pn(x)zn, prove that F(z, x) = (1−2zx+z2)−1/2.
7. Prove that the gamma function
Γ(z) =
∞
>
0
e−ttz−1dt,
z ∈C,
Rez > 0
is the solution of the diﬀerence equation Γ(z + 1) = zΓ(z)
[Hint : integrate by parts.]
8. Study, as functions of α ∈R, stability and order of the family of linear
multistep methods
un+1 = αun + (1 −α)un−1 + 2hfn + hα
2 [fn−1 −3fn] .
9. Consider the following family of linear multistep methods depending on
the real parameter α
un+1 = un + h[(1 −α
2 )f(xn, un) + α
2 f(xn+1, un+1)].
Study their consistency as a function of α; then, take α = 1 and use the
corresponding method to solve the Cauchy problem
y′(x) = −10y(x),
x > 0,
y(0) = 1.
Determine the values of h in correspondance of which the method is abso-
lutely stable.
[Solution : the only consistent method of the family is the Crank-Nicolson
method (α = 1).]

11.12 Exercises
529
10. Consider the family of linear multistep methods
un+1 = αun + h
2 (2(1 −α)fn+1 + 3αfn −αfn−1)
where α is a real parameter.
(a) Analyze consistency and order of the methods as functions of α, de-
termining the value α∗for which the resulting method has maximal
order.
(b) Study the zero-stability of the method with α = α∗, write its charac-
teristic polynomial Π(r; hλ) and, using MATLAB, draw its region of
absolute stability in the complex plane.
11. Adams methods can be easily generalized, integrating between tn−r and
tn+1 with r ≥1. Show that, by doing so, we get methods of the form
un+1 = un−r + h
p

j=−1
bjfn−j
and prove that for r = 1 the midpoint method introduced in (11.43) is
recovered (the methods of this family are called Nystron methods.)
12. Check that Heun’s method (11.10) is an explicit two-stage RK method and
write the Butcher arrays of the method. Then, do the same for the modiﬁed
Euler method, given by
un+1 = un + hf(tn + h
2 , un + h
2 fn),
n ≥0.
(11.91)
[Solution : the methods have the following Butcher arrays
0
0
0
1
1
0
2 1
2
2
1
2
0
0
0
2
3
1
2 2
3
1
2
0
0
1
.]
13. Check that the Butcher array for method (11.73) is given by
0
0
0
0
0
1
2
2 1
2
2
0
0
0
1
2
0
2 1
2
2
0
0
1
0
0
1
0
1
6
2 1
3
2
1
3
1
6
14. Write a MATLAB program to draw the regions of absolute stability for a
RK method, for which the function R(hλ) is available. Check the code in
the special case of
R(hλ) = 1 + hλ + (hλ)2/2 + (hλ)3/6 + (hλ)4/24 + (hλ)5/120 + (hλ)6/600
and verify that such a region is not connected.

530
11. Numerical Solution of Ordinary Diﬀerential Equations
15. Determine the function R(hλ) associated with the Merson method, whose
Butcher array is
0
0
0
0
0
0
1
3
1
3
0
0
0
0
1
3
1
6
1
6
0
0
0
1
2
1
8
0
3
8
0
0
1
1
2
0
−3
2
2
0
1
6
0
0
2
3
1
6
[Solution : one gets R(hλ) = 1 + 4
i=1(hλ)i/i! + (hλ)5/144.]

12
Two-Point Boundary Value Problems
This chapter is devoted to the analysis of approximation methods for two-
point boundary value problems for diﬀerential equations of elliptic type.
Finite diﬀerences, ﬁnite elements and spectral methods will be considered.
A short account is also given on the extension to elliptic boundary value
problems in two-dimensional regions.
12.1
A Model Problem
To start with, let us consider the two-point boundary value problem
−u′′(x) = f(x),
0 < x < 1,
(12.1)
u(0) = u(1) = 0.
(12.2)
From the fundamental theorem of calculus, if u ∈C2([0, 1]) and satisﬁes
the diﬀerential equation (12.1) then
u(x) = c1 + c2x −
x
>
0
F(s) ds
where c1 and c2 are arbitrary constants and F(s) =
 s
0 f(t) dt. Using
integration by parts one has
x
>
0
F(s) ds = [sF(s)]x
0 −
x
>
0
sF ′(s) ds =
x
>
0
(x −s)f(s) ds.

532
12. Two-Point Boundary Value Problems
The constants c1 and c2 can be determined by enforcing the boundary
conditions. The condition u(0) = 0 implies that c1 = 0, and then u(1) = 0
yields c2 =
 1
0 (1 −s)f(s) ds. Consequently, the solution of (12.1)-(12.2)
can be written in the following form
u(x) = x
1
>
0
(1 −s)f(s) ds −
x
>
0
(x −s)f(s) ds
or, more compactly,
u(x) =
1
>
0
G(x, s)f(s) ds,
(12.3)
where, for any ﬁxed x, we have deﬁned
G(x, s) =
" s(1 −x)
if 0 ≤s ≤x,
x(1 −s)
if x ≤s ≤1.
(12.4)
The function G is called Green’s function for the boundary value problem
(12.1)-(12.2). It is a piecewise linear function of x for ﬁxed s, and vice versa.
It is continuous, symmetric (i.e., G(x, s) = G(s, x) for all x, s ∈[0, 1]), non
negative, null if x or s are equal to 0 or 1, and
 1
0 G(x, s) ds = 1
2x(1 −x).
The function is plotted in Figure 12.1.
We can therefore conclude that for every f ∈C0([0, 1]) there is a unique
solution u ∈C2([0, 1]) of the boundary value problem (12.1)-(12.2) which
admits the representation (12.3). Further smoothness of u can be derived
by (12.1); indeed, if f ∈Cm([0, 1]) for some m ≥0 then u ∈Cm+2([0, 1]).
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.05
0.1
0.15
0.2
0.25
FIGURE 12.1. Green’s function for three diﬀerent values of x: x = 1/4 (solid
line), x = 1/2 (dashed line), x = 3/4 (dash-dotted line)

12.2 Finite Diﬀerence Approximation
533
An interesting property of the solution u is that if f ∈C0([0, 1]) is a
nonnegative function, then u is also nonnegative. This is referred to as the
monotonicity property, and follows directly from (12.3), since G(x, s) ≥0
for all x, s ∈[0, 1]. The next property is called the maximum principle and
states that if f ∈C0(0, 1),
∥u∥∞≤1
8∥f∥∞
(12.5)
where ∥u∥∞= max
0≤x≤1|u(x)| is the maximum norm. Indeed, since G is non-
negative,
|u(x)| ≤
1
>
0
G(x, s)|f(s)| ds ≤∥f∥∞
1
>
0
G(x, s) ds = 1
2x(1 −x)∥f∥∞
from which the inequality (12.5) follows.
12.2
Finite Diﬀerence Approximation
We introduce on [0, 1] the grid points {xj}n
j=0 given by xj = jh where
n ≥2 is an integer and h = 1/n is the grid spacing. The approximation to
the solution u is a ﬁnite sequence {uj}n
j=0 deﬁned only at the grid points
(with the understanding that uj approximates u(xj)) by requiring that
−uj+1 −2uj + uj−1
h2
= f(xj),
for j = 1, . . . , n −1
(12.6)
and u0 = un = 0. This corresponds to having replaced u′′(xj) by its second
order centred ﬁnite diﬀerence (10.65) (see Section 10.10.1).
If we set u = (u1, . . . , un−1)T and f = (f1, . . . , fn−1)T , with fi = f(xi),
it is a simple matter to see that (12.6) can be written in the more compact
form
Afdu = f,
(12.7)
where Afd is the symmetric (n−1)×(n−1) ﬁnite diﬀerence matrix deﬁned
as
Afd = h−2tridiagn−1(−1, 2, −1).
(12.8)
This matrix is diagonally dominant by rows; moreover, it is positive deﬁnite
since for any vector x ∈Rn−1
xT Afdx = h−2
.
x2
1 + x2
n−1 +
n−1

i=2
(xi −xi−1)2
/
.

534
12. Two-Point Boundary Value Problems
This implies that (12.7) admits a unique solution. Another interesting prop-
erty is that Afd is an M-matrix (see Deﬁnition 1.25 and Exercise 2), which
guarantees that the ﬁnite diﬀerence solution enjoys the same monotonic-
ity property as the exact solution u(x), namely u is nonnegative if f is
nonnegative. This property is called discrete maximum principle.
In order to rewrite (12.6) in operator form, let Vh be a collection of discrete
functions deﬁned at the grid points xj for j = 0, . . . , n. If vh ∈Vh, then
vh(xj) is deﬁned for all j and we sometimes use the shorthand notation vj
instead of vh(xj). Next, we let V 0
h be the subset of Vh containing discrete
functions that are zero at the endpoints x0 and xn. For a function wh we
deﬁne the operator Lh by
(Lhw)(xj) = −wj+1 −2wj + wj−1
h2
,
j = 1, . . . , n −1
(12.9)
and reformulate the ﬁnite diﬀerence problem (12.6) equivalently as: ﬁnd
uh ∈V 0
h such that
(Lhuh)(xj) = f(xj)
for j = 1, . . . , n −1.
(12.10)
Notice that, in this formulation, the boundary conditions are taken care of
by the requirement that uh ∈V 0
h .
Finite diﬀerences can be used to provide approximations of higher-order
diﬀerential operators than the one considered in this section. An example
is given in Section 4.7.2 where the ﬁnite diﬀerence centred discretization of
the fourth-order derivative −u(iv)(x) is carried out by applying twice the
discrete operator Lh (see also Exercise 11). Again, extra care is needed to
properly handle the boundary conditions.
12.2.1
Stability Analysis by the Energy Method
For two discrete functions wh, vh ∈Vh we deﬁne the discrete inner product
(wh, vh)h = h
n

k=0
ckwkvk,
with c0 = cn = 1/2 and ck = 1 for k = 1, . . . , n−1. This is nothing but the
composite trapezoidal rule (9.13) which is here used to evaluate the inner
product (w, v) =
 1
0 w(x)v(x)dx. Clearly,
∥vh∥h = (vh, vh)1/2
h
is a norm on Vh.
Lemma 12.1 The operator Lh is symmetric, i.e.
(Lhwh, vh)h = (wh, Lhvh)h
∀wh, vh ∈V 0
h ,

12.2 Finite Diﬀerence Approximation
535
and is positive deﬁnite, i.e.
(Lhvh, vh)h ≥0
∀vh ∈V 0
h ,
with equality only if vh ≡0.
Proof. From the identity
wj+1vj+1 −wjvj = (wj+1 −wj)vj + (vj+1 −vj)wj+1,
upon summation over j from 0 to n −1 we obtain the following relation for all
wh, vh ∈Vh
n−1

j=0
(wj+1 −wj)vj = wnvn −w0v0 −
n−1

j=0
(vj+1 −vj)wj+1
which is referred to as summation by parts. Using summation by parts twice, and
setting for ease of notation w−1 = v−1 = 0, for all wh, vh ∈V 0
h we obtain
(Lhwh, vh)h
= −h−1
n−1

j=0
[(wj+1 −wj) −(wj −wj−1)] vj
= h−1
n−1

j=0
(wj+1 −wj)(vj+1 −vj).
From this relation we deduce that (Lhwh, vh)h = (wh, Lhvh)h; moreover, taking
wh = vh we obtain
(Lhvh, vh)h = h−1
n−1

j=0
(vj+1 −vj)2.
(12.11)
This quantity is always positive, unless vj+1 = vj for j = 0, . . . , n −1, in which
case vj = 0 for j = 0, . . . , n since v0 = 0.
3
For any grid function vh ∈V 0
h we deﬁne the following norm
|||vh|||h =


h
n−1

j=0
vj+1 −vj
h
2



1/2
.
(12.12)
Thus, (12.11) is equivalent to
(Lhvh, vh)h = |||vh|||2
h
for all vh ∈V 0
h .
(12.13)
Lemma 12.2 The following inequality holds for any function vh ∈V 0
h
∥vh∥h ≤
1
√
2|||vh|||h.
(12.14)

536
12. Two-Point Boundary Value Problems
Proof. Since v0 = 0, we have
vj = h
j−1

k=0
vk+1 −vk
h
for all j = 1, . . . , n −1.
Then,
v2
j = h2
.j−1

k=0
+ vk+1 −vk
h
,/2
.
Using the Minkowski inequality
 m

k=1
pk
2
≤m
 m

k=1
p2
k

(12.15)
which holds for every integer m ≥1 and every sequence {p1, . . . , pm} of real
numbers (see Exercise 4), we obtain
n−1

j=1
v2
j ≤h2
n−1

j=1
j
j−1

k=0
+ vk+1 −vk
h
,2
.
Then for every vh ∈V 0
h we get
∥vh∥2
h = h
n−1

j=1
v2
j ≤h2
n−1

j=1
jh
n−1

k=0
+ vk+1 −vk
h
,2
= h2 (n −1)n
2
|||vh|||2
h.
Inequality (12.14) follows since h = 1/n.
3
Remark 12.1 For every vh ∈V 0
h , the grid function v(1)
h
whose grid values
are (vj+1 −vj)/h, j = 0, . . . , n−1, can be regarded as a discrete derivative
of vh (see Section 10.10.1). Inequality (12.14) can thus be rewritten as
∥vh∥h ≤
1
√
2∥v(1)
h ∥h
∀vh ∈V 0
h .
It can be regarded as the discrete counterpart in [0, 1] of the following
Poincar´e inequality: for every interval [a, b] there exists a constant CP > 0
such that
∥v∥L2(a,b) ≤CP ∥v(1)∥L2(a,b)
(12.16)
for all v ∈C1([a, b]) such that v(a) = v(b) = 0 and where ∥· ∥L2(a,b) is the
norm in L2(a, b) (see (8.25)).
■
Inequality (12.14) has an interesting consequence. If we multiply every
equation of (12.10) by uj and then sum for j from 0 on n −1, we obtain
(Lhuh, uh)h = (f, uh)h.

12.2 Finite Diﬀerence Approximation
537
Applying to (12.13) the Cauchy-Schwarz inequality (1.14) (valid in the
ﬁnite dimensional case), we obtain
|||uh|||2
h ≤∥fh∥h∥uh∥h
where fh ∈Vh is the grid function such that fh(xj) = f(xj) for all j =
1, . . . , n.
Owing to (12.14) we conclude that
∥uh∥h ≤1
2∥fh∥h
(12.17)
from which we deduce that the ﬁnite diﬀerence problem (12.6) has a unique
solution (equivalently, the only solution corresponding to fh = 0 is uh = 0).
Moreover, (12.17) is a stability result, as it states that the ﬁnite diﬀerence
solution is bounded by the given datum fh.
To prove convergence, we ﬁrst introduce the notion of consistency. Ac-
cording to our general deﬁnition (2.13), if f ∈C0([0, 1]) and u ∈C2([0, 1])
is the corresponding solution of (12.1)-(12.2), the local truncation error is
the grid function τh deﬁned by
τh(xj) = (Lhu)(xj) −f(xj),
j = 1, . . . , n −1.
(12.18)
By Taylor series expansion and recalling (10.66), one obtains
τh(xj)
=
−h−2 [u(xj−1) −2u(xj) + u(xj+1)] −f(xj)
=
−u′′(xj) −f(xj) + h2
24(u(iv)(ξj) + u(iv)(ηj))
=
h2
24(u(iv)(ξj) + u(iv)(ηj))
(12.19)
for suitable ξj ∈(xj−1, xj) and ηj ∈(xj, xj+1). Upon deﬁning the discrete
maximum norm as
∥vh∥h,∞= max
0≤j≤n |vh(xj)|,
we obtain from (12.19)
∥τh∥h,∞≤∥f ′′∥∞
12
h2
(12.20)
provided that f ∈C2([0, 1]). In particular, lim
h→0∥τh∥h,∞= 0 and there-
fore the ﬁnite diﬀerence scheme is consistent with the diﬀerential problem
(12.1)-(12.2).
Remark 12.2 Taylor’s expansion of u around xj can also be written as
u(xj ± h) = u(xj) ± hu′(xj) + h2
2 u′′(xj) ± h3
6 u′′′(xj) + R4(xj ± h)

538
12. Two-Point Boundary Value Problems
with the following integral form of the remainder
R4(xj + h) =
xj+h
>
xj
(u′′′(t) −u′′′(xj)) (xj + h −t)2
2
dt,
R4(xj −h) = −
xj
>
xj−h
(u′′′(t) −u′′′(xj)) (xj −h −t)2
2
dt.
Using the two formulae above, by inspection on (12.18) it is easy to see
that
τh(xj) = 1
h2 (R4(xj + h) + R4(xj −h)) .
(12.21)
For any integer m ≥0, we denote by Cm,1(0, 1) the space of all functions
in Cm(0, 1) whose m-th derivative is Lipschitz continuous, i.e.
max
x,y∈(0,1),x̸=y
|v(m)(x) −v(m)(y)|
|x −y|
≤M < ∞.
Looking at (12.21) we see that it suﬃces to assuming that u ∈C3,1(0, 1)
to conclude that
∥τh∥h,∞≤Mh2
which shows that the ﬁnite diﬀerence scheme is consistent with the diﬀer-
ential problem (12.1)-(12.2) even under a slightly weaker regularity of the
exact solution u.
■
Remark 12.3 Let e = u −uh be the discretization error grid function.
Then,
Lhe = Lhu −Lhuh = Lhu −fh = τh.
(12.22)
It can be shown (see Exercise 5) that
∥τh∥2
h ≤3
+
∥f∥2
h + ∥f∥2
L2(0,1)
,
(12.23)
from which it follows that the norm of the discrete second-order derivative
of the discretization error is bounded, provided that the norms of f at the
right-hand side of (12.23) are also bounded.
■
12.2.2
Convergence Analysis
The ﬁnite diﬀerence solution uh can be characterized by a discrete Green’s
function as follows. For a given grid point xk deﬁne a grid function Gk ∈V 0
h
as the solution to the following problem
LhGk = ek,
(12.24)

12.2 Finite Diﬀerence Approximation
539
where ek ∈V 0
h satisﬁes ek(xj) = δkj, 1 ≤j ≤n −1. It is easy to see
that Gk(xj) = hG(xj, xk), where G is the Green’s function introduced in
(12.4) (see Exercise 6). For any grid function g ∈V 0
h we can deﬁne the grid
function
wh = Thg,
wh =
n−1

k=1
g(xk)Gk.
(12.25)
Then
Lhwh =
n−1

k=1
g(xk)LhGk =
n−1

k=1
g(xk)ek = g.
In particular, the solution uh of (12.10) satisﬁes uh = Thf, therefore
uh =
n−1

k=1
f(xk)Gk,
and
uh(xj) = h
n−1

k=1
G(xj, xk)f(xk).
(12.26)
Theorem 12.1 Assume that f ∈C2([0, 1]). Then, the nodal error e(xj) =
u(xj) −uh(xj) satisﬁes
∥u −uh∥h,∞≤h2
96∥f ′′∥∞,
(12.27)
i.e. uh converges to u (in the discrete maximum norm) with second order
with respect to h.
Proof. We start by noticing that, thanks to the representation (12.25), the
following discrete counterpart of (12.5) holds
∥uh∥h,∞≤1
8∥f∥h,∞.
(12.28)
Indeed, we have
|uh(xj)|
≤h
n−1

k=1
G(xj, xk)|f(xk)| ≤∥f∥h,∞

h
n−1

k=1
G(xj, xk)

= ∥f∥h,∞1
2xj(1 −xj) ≤1
8∥f∥h,∞
since, if g = 1, then Thg is such that Thg(xj) = 1
2xj(1 −xj) (see Exercise 7).
Inequality (12.28) provides a result of stability in the discrete maximum norm
for the ﬁnite diﬀerence solution uh. Using (12.22), by the same argument used to
prove (12.28) we obtain
∥e∥h,∞≤1
8∥τh∥h,∞.
Finally, the thesis (12.27) follows owing to (12.20).
3
Observe that for the derivation of the convergence result (12.27) we have
used both stability and consistency. In particular, the discretization error
is of the same order (with respect to h) as the consistency error τh.

540
12. Two-Point Boundary Value Problems
12.2.3
Finite Diﬀerences for Two-Point Boundary Value
Problems with Variable Coeﬃcients
A two-point boundary value problem more general than (12.1)-(12.2) is the
following one
Lu(x) = −(J(u)(x))′ + γ(x)u(x) = f(x)
0 < x < 1,
u(0) = d0,
u(1) = d1
(12.29)
where
J(u)(x) = α(x)u′(x),
(12.30)
d0 and d1 are assigned constants and α, γ and f are given functions that
are continuous in [0, 1]. Finally, γ(x) ≥0 in [0, 1] and α(x) ≥α0 > 0 for a
suitable α0. The auxiliary variable J(u) is the ﬂux associated with u and
very often has a precise physical meaning.
For the approximation, it is convenient to introduce on [0, 1] a new grid
made by the midpoints xj+1/2 = (xj + xj+1)/2 of the intervals [xj, xj+1]
for j = 0, . . . , n −1. Then, a ﬁnite diﬀerence approximation of (12.29) is
given by: ﬁnd uh ∈Vh such that
Lhuh(xj) = f(xj)
for all j = 1, . . . , n −1,
uh(x0) = d0,
uh(xn) = d1,
(12.31)
where Lh is deﬁned for j = 1, . . . , n −1 as
Lhw(xj) = −Jj+1/2(wh) −Jj−1/2(wh)
h
+ γjwj.
(12.32)
We have deﬁned γj = γ(xj) and, for j = 0, . . . , n −1, the approximate
ﬂuxes are given by
Jj+1/2(wh) = αj+1/2
wj+1 −wj
h
(12.33)
with αj+1/2 = α(xj+1/2).
The ﬁnite diﬀerence scheme (12.31)-(12.32) with the approximate ﬂuxes
(12.33) can still be cast in the form (12.7) by setting
Afd = h−2tridiagn−1(a, d, a) + diagn−1(c)
(12.34)
where
a =

α1/2, α3/2, . . . , αn−1/2
T ∈Rn−2,
d =

α1/2 + α3/2, . . . , αn−3/2 + αn−1/2
T ∈Rn−1,
c = (γ1, . . . , γn−1)T ∈Rn−1.

12.2 Finite Diﬀerence Approximation
541
The matrix (12.34) is symmetric positive deﬁnite and is also strictly diag-
onally dominant if γ > 0.
The convergence analysis of the scheme (12.31)-(12.32) can be carried
out by extending straightforwardly the techniques developed in Sections
12.2.1 and 12.2.2.
We conclude this section by addressing boundary conditions that are more
general than those considered in (12.29). For this purpose we assume that
u(0) = d0,
J(u(1)) = g1,
where d0 and g1 are two given data. The boundary condition at x = 1 is
called a Neumann condition while the condition at x = 0 (where the value
of u is assigned) is a Dirichlet boundary condition. The ﬁnite diﬀerence
discretization of the Neumann boundary condition can be performed by
using the mirror imaging technique. For any suﬃciently smooth function
ψ we write its truncated Taylor’s expansion at xn as
ψn = ψn−1/2 + ψn+1/2
2
−h2
16(ψ′′(ηn) + ψ′′(ξn))
for suitable ηn ∈(xn−1/2, xn), ξn ∈(xn, xn+1/2). Taking ψ = J(u) and
neglecting the term containing h2 yields
Jn+1/2(uh) = 2g1 −Jn−1/2(uh).
(12.35)
Notice that the point xn+1/2 = xn +h/2 and the corresponding ﬂux Jn+1/2
do not really exist (indeed, xn+1/2 is called a “ghost” point), but it is
generated by linear extrapolation of the ﬂux at the nearby nodes xn−1/2
and xn. The ﬁnite diﬀerence equation (12.32) at the node xn reads
Jn−1/2(uh) −Jn+1/2(uh)
h
+ γnun = fn.
Using (12.35) to obtain Jn+1/2(uh) we ﬁnally get the second-order accurate
approximation
−αn−1/2
un−1
h2
+
+αn−1/2
h2
+ γn
2
,
un = g1
h + fn
2 .
This formula suggests easy modiﬁcation of the matrix and right-hand side
entries in the ﬁnite diﬀerence system (12.7).
For a further generalization of the boundary conditions in (12.29) and
their discretization using ﬁnite diﬀerences we refer to Exercise 10 where
boundary conditions of the form λu + µu′ = g at both the endpoints of
(0, 1) are considered for u (Robin boundary conditions).
For a thorough presentation and analysis of ﬁnite diﬀerence approxima-
tions of two-point boundary value problems, see, e.g., [Str89] and [HGR96].

542
12. Two-Point Boundary Value Problems
12.3
The Spectral Collocation Method
Other discretization schemes can be derived which exhibit the same struc-
ture as the ﬁnite diﬀerence problem (12.10), with a discrete operator Lh
being deﬁned in a diﬀerent manner, though.
Actually, numerical approximations of the second derivative other than
the centred ﬁnite diﬀerence one can be set up, as described in Section
10.10.3. A noticeable instance is provided by the spectral collocation method.
In that case we assume the diﬀerential equation (12.1) to be set on the in-
terval (−1, 1) and choose the nodes {x0, . . . , xn} to coincide with the n+1
Legendre-Gauss-Lobatto nodes introduced in Section 10.4. Besides, uh is a
polynomial of degree n. For coherence, we will use throughout the section
the index n instead of h.
The spectral collocation problem reads
ﬁnd un ∈P0
n : Lnun(xj) = f(xj),
j = 1, . . . , n −1
(12.36)
where P0
n is the set of polynomials p ∈Pn([0, 1]) such that p(0) = p(1) = 0.
Besides, Lnv = LInv for any continuous function v where Inv ∈Pn is the
interpolant of v at the nodes {x0, . . . , xn} and L denotes the diﬀerential
operator at hand, which, in the case of equation (12.1), coincides with
−d2/dx2. Clearly, if v ∈Pn then Lnv = Lv.
The algebraic form of (12.36) becomes
Aspu = f,
where uj = un(xj), fj = f(xj) j = 1, . . . , n−1 and the spectral collocation
matrix Asp ∈R(n−1)×(n−1) is equal to ˜D2, where ˜D is the matrix obtained
from the pseudo-spectral diﬀerentiation matrix (10.73) by eliminating the
ﬁrst and the n + 1-th rows and columns.
For the analysis of (12.36) we can introduce the following discrete scalar
product
(u, v)n =
n

j=0
u(xj)v(xj)wj,
(12.37)
where wj are the weights of the Legendre-Gauss-Lobatto quadrature for-
mula (see Section 10.4). Then (12.36) is equivalent to
(Lnun, vn)n = (f, vn)n
∀vn ∈P0
n.
(12.38)
Since (12.37) is exact for u, v such that uv ∈P2n−1 (see Section 10.2) then
(Lnvn, vn)n = (Lnvn, vn) = ∥v′
n∥2
L2(−1,1),
∀vn ∈P0
n.
Besides,
(f, vn)n ≤∥f∥n∥vn∥n ≤
√
6∥f∥∞∥vn∥L2(−1,1),

12.3 The Spectral Collocation Method
543
where ∥f∥∞denotes the maximum of f in [−1, 1] and we have used the
fact that ∥f∥n ≤
√
2∥f∥∞and the result of equivalence
∥vn∥L2(−1,1) ≤∥vn∥n ≤
√
3∥vn∥L2(−1,1),
∀vn ∈Pn
(see [CHQZ88], p. 286).
Taking vn = un in (12.38) and using the Poincar´e inequality (12.16) we
ﬁnally obtain
∥u′
n∥L2(−1,1) ≤
√
6CP ∥f∥∞
which ensures that problem (12.36) has a unique solution which is stable.
As for consistency, we can notice that
τn(xj) = (Lnu −f)(xj) = (−(Inu)′′ −f)(xj) = (u −Inu)′′(xj)
and this right-hand side tends to zero as n →∞provided that u ∈
C2([−1, 1]).
Let us now establish a convergence result for the spectral collocation
approximation of (12.1). In the following, C is a constant independent of
n that can assume diﬀerent values at diﬀerent places.
Moreover, we denote by Hs(a, b), for s ≥1, the space of the functions f ∈
Cs−1(a, b) such that f (s−1) is continuous and piecewise diﬀerentiable, so
that f (s) exists unless for a ﬁnite number of points and belongs to L2(a, b).
The space Hs(a, b) is known as the Sobolev function space of order s and is
endowed with the norm ∥· ∥Hs(a,b) deﬁned in (10.35).
Theorem 12.2 Let f ∈Hs(−1, 1) for some s ≥1. Then
∥u′ −u′
n∥L2(−1,1) ≤Cn−s 
∥f∥Hs(−1,1) + ∥u∥Hs+1(−1,1)

.
(12.39)
Proof. Note that un satisﬁes
(u′
n, v′
n) = (f, vn)n
where (u, v) =
 1
−1 uvdx is the scalar product of L2(−1, 1). Similarly, u satisﬁes
(u′, v′) = (f, v)
∀v ∈C1([0, 1]) such that v(0) = v(1) = 0
(see (12.43) of Section 12.4). Then
((u −un)′, v′
n) = (f, vn) −(f, vn)n =: E(f, vn),
∀vn ∈P0
n.
It follows that
((u −un)′, (u −un)′)
= ((u −un)′, (u −Inu)′) + ((u −un)′, (Inu −un)′)
= ((u −un)′, (u −Inu)′) + E(f, Inu −un).
We recall the following result (see (10.36))
|E(f, vn)| ≤Cn−s∥f∥Hs(−1,1)∥vn∥L2(−1,1).

544
12. Two-Point Boundary Value Problems
Then
|E(f, Inu −un)| ≤Cn−s∥f∥Hs(−1,1)

∥Inu −u∥L2(−1,1) + ∥u −un∥L2(−1,1)

.
We recall now the following Young’s inequality (see Exercise 8)
ab ≤εa2 + 1
4εb2,
∀a, b ∈R,
∀ε > 0.
(12.40)
Using this inequality we obtain

(u −un)′, (u −Inu)′
≤1
4∥(u −un)′∥2
L2(−1,1) + ∥(u −Inu)′∥2
L2(−1,1),
and also (using the Poincar´e inequality (12.16))
Cn−s∥f∥Hs(−1,1)∥u −un∥L2(−1,1) ≤C CP n−s∥f∥Hs(−1,1)∥(u −un)′∥L2(−1,1)
≤(CCP )2n−2s∥f∥2
Hs(−1,1) + 1
4∥(u −un)′∥2
L2(−1,1).
Finally,
Cn−s∥f∥Hs(−1,1)∥Inu −u∥L2(−1,1) ≤1
2C2n−2s∥f∥2
Hs(−1,1) + 1
2∥Inu −u∥2
L2(−1,1).
Using the interpolation error estimate (10.22) for u −Inu we ﬁnally obtain the
desired error estimate (12.39).
3
12.4
The Galerkin Method
We now derive the Galerkin approximation of problem (12.1)-(12.2), which
is the basic ingredient of the ﬁnite element method and the spectral method,
widely employed in the numerical approximation of boundary value prob-
lems.
12.4.1
Integral Formulation of Boundary Value Problems
We consider a problem which is slightly more general than (12.1), namely
−(αu′)′(x) + (βu′)(x) + (γu)(x) = f(x)
0 < x < 1,
(12.41)
with u(0) = u(1) = 0, where α, β and γ are continuous functions on [0, 1]
with α(x) ≥α0 > 0 for any x ∈[0, 1]. Let us now multiply (12.41) by
a function v ∈C1([0, 1]), hereafter called a “test function”, and integrate
over the interval [0, 1]
1
>
0
αu′v′ dx +
1
>
0
βu′v dx +
1
>
0
γuv dx =
1
>
0
fv dx + [αu′v]1
0,

12.4 The Galerkin Method
545
where we have used integration by parts on the ﬁrst integral. If the function
v is required to vanish at x = 0 and x = 1 we obtain
1
>
0
αu′v′ dx +
1
>
0
βu′v dx +
1
>
0
γuv dx =
1
>
0
fv dx.
We will denote by V the test function space. This consists of all functions v
that are continuous, vanish at x = 0 and x = 1 and whose ﬁrst derivative is
piecewise continuous, i.e., continuous everywhere except at a ﬁnite number
of points in [0, 1] where the left and right limits v′
−and v′
+ exist but do not
necessarily coincide.
V is actually a vector space which is denoted by H1
0(0, 1). Precisely,
H1
0(0, 1) =

v ∈L2(0, 1) : v′ ∈L2(0, 1), v(0) = v(1) = 0

(12.42)
where v′ is the distributional derivative of v whose deﬁnition is given in
Section 12.4.2.
We have therefore shown that if a function u ∈C2([0, 1]) satisﬁes (12.41),
then u is also a solution of the following problem
ﬁnd u ∈V : a(u, v) = (f, v) for all v ∈V,
(12.43)
where now (f, v) =
 1
0 fv dx denotes the scalar product of L2(0, 1) and
a(u, v) =
1
>
0
αu′v′ dx +
1
>
0
βu′v dx +
1
>
0
γuv dx
(12.44)
is a bilinear form, i.e. it is linear with respect to both arguments u and
v. Problem (12.43) is called the weak formulation of problem (12.1). Since
(12.43) contains only the ﬁrst derivative of u it might cover cases in which
a classical solution u ∈C2([0, 1]) of (12.41) does not exist although the
physical problem is well deﬁned.
If for instance, α = 1, β = γ = 0, the solution u(x) denotes of the
displacement at point x of an elastic cord having linear density equal to f,
whose position at rest is u(x) = 0 for all x ∈[0, 1] and which remains ﬁxed
at the endpoints x = 0 and x = 1. Figure 12.2 (right) shows the solution
u(x) corresponding to a function f which is discontinuous (see Figure 12.2,
left). Clearly, u′′ does not exist at the points x = 0.4 and x = 0.6 where f
is discontinuous.
If (12.41) is supplied with non homogeneous boundary conditions, say
u(0) = u0, u(1) = u1, we can still obtain a formulation like (12.43) by
proceeding as follows. Let ¯u(x) = xu1 + (1 −x)u0 be the straight line that
interpolates the data at the endpoints, and set
0u= u(x)−¯u(x). Then
0u∈V

546
12. Two-Point Boundary Value Problems
1
0.6
0.4
0
−1
f(x)
x
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
−0.05
−0.045
−0.04
−0.035
−0.03
−0.025
−0.02
−0.015
−0.01
−0.005
0
u(x)
FIGURE 12.2. Elastic cord ﬁxed at the endpoints and subject to a discontinuous
load f (left). The vertical displacement u is shown on the right
satisﬁes the following problem
ﬁnd
0u∈V : a(
0u, v) = (f, v) −a(¯u, v) for all v ∈V.
A similar problem is obtained in the case of Neumann boundary conditions,
say u′(0) = u′(1) = 0. Proceeding as we did to obtain (12.43), we see
that the solution u of this homogeneous Neumann problem satisﬁes the
same problem (12.43) provided the space V is now H1(0, 1). More general
boundary conditions of mixed type can be considered as well (see Exercise
12).
12.4.2
A Quick Introduction to Distributions
Let X be a Banach space, i.e., a normed and complete vector space. We
say that a functional T : X →R is continuous if limx→x0 T(x) = T(x0)
for all x0 ∈X and linear if T(x + y) = T(x) + T(y) for any x, y ∈X and
T(λx) = λT(x) for any x ∈X and λ ∈R.
Usually, a linear continuous functional is denoted by ⟨T, x⟩and the sym-
bol ⟨·, ·⟩is called duality. As an example, let X = C0([0, 1]) be endowed
with the maximum norm ∥· ∥∞and consider on X the two functionals
deﬁned as
⟨T, x⟩= x(0),
⟨S, x⟩=
> 1
0
x(t) sin(t)dt.
It is easy to check that both T and S are linear and continuous functionals
on X. The set of all linear continuous functionals on X identiﬁes an abstract
space which is called the dual space of X and is denoted by X′.
We then introduce the space C∞
0 (0, 1) (or D(0, 1)) of inﬁnitely diﬀer-
entiable functions having compact support in [0, 1], i.e., vanishing outside
a bounded open set (a, b) ⊂(0, 1) with 0 < a < b < 1. We say that
vn ∈D(0, 1) converges to v ∈D(0, 1) if there exists a closed bounded set
K ⊂(0, 1) such that vn vanishes outside K for each n and for any k ≥0
the derivative v(k)
n
converges to v(k) uniformly in (0, 1).

12.4 The Galerkin Method
547
The space of linear functionals on D(0, 1) which are continuous with
respect to the convergence introduced above is denoted by D′(0, 1) (the
dual space of D(0, 1)) and its elements are called distributions.
We are now in position to introduce the derivative of a distribution. Let
T be a distribution, i.e. an element of D′(0, 1). Then, for any k ≥0, T (k)
is also a distribution, deﬁned as
⟨T (k), ϕ⟩= (−1)k⟨T, ϕ(k)⟩,
∀ϕ ∈D(0, 1).
(12.45)
As an example, consider the Heaviside function
H(x) =
"
1
x ≥0,
0
x < 0.
The distributional derivative of H is the Dirac mass δ at the origin, deﬁned
as
v →δ(v) = v(0),
v ∈D(R).
From the deﬁnition (12.45), it turns out that any distribution is inﬁnitely
diﬀerentiable; moreover, if T is a diﬀerentiable function its distributional
derivative coincides with the usual one.
12.4.3
Formulation and Properties of the Galerkin Method
Unlike the ﬁnite diﬀerence method which stems directly from the diﬀer-
ential (or strong) form (12.41), the Galerkin method is based on the weak
formulation (12.43). If Vh is a ﬁnite dimensional vector subspace of V , the
Galerkin method consists of approximating (12.43) by the problem
ﬁnd uh ∈Vh : a(uh, vh) = (f, vh)
∀vh ∈Vh.
(12.46)
This is a ﬁnite dimensional problem. Actually, let {ϕ1, . . . , ϕN} denote a
basis of Vh, i.e. a set of N linearly independent functions of Vh. Then we
can write
uh(x) =
N

j=1
ujϕj(x).
The integer N denotes the dimension of the vector space Vh. Taking vh = ϕi
in (12.46), it turns out that the Galerkin problem (12.46) is equivalent to
seeking N unknown coeﬃcients {u1, . . . , uN} such that
N

j=1
uja(ϕj, ϕi) = (f, ϕi)
∀i = 1, . . . , N.
(12.47)

548
12. Two-Point Boundary Value Problems
We have used the linearity of a(·, ·) with respect to its ﬁrst argument, i.e.
a(
N

j=1
ujϕj, ϕi) =
N

j=1
uja(ϕj, ϕi).
If we introduce the matrix AG = (aij), aij = a(ϕj, ϕi) (called the stiﬀness
matrix), the unknown vector u = (u1, . . . , uN) and the right-hand side
vector fG = (f1, . . . , fN), with fi = (f, ϕi), we see that (12.47) is equivalent
to the linear system
AGu = fG.
(12.48)
The structure of AG, as well as the degree of accuracy of uh, depends on
the form of the basis functions {ϕi}, and therefore on the choice of Vh.
We will see two remarkable instances, the ﬁnite element method, where
Vh is a space of piecewise polynomials over subintervals of [0, 1] of length not
greater than h which are continuous and vanish at the endpoints x = 0 and
1, and the spectral method in which Vh is a space of algebraic polynomials
still vanishing at the endpoints x = 0, 1.
However, before speciﬁcally addressing those cases, we state a couple of
general results that hold for any Galerkin problem (12.46).
12.4.4
Analysis of the Galerkin Method
We endow the space H1
0(0, 1) with the following norm
|v|H1(0,1) =



1
>
0
|v′(x)|2 dx



1/2
.
(12.49)
We will address the special case where β = 0 and γ(x) ≥0. In the most
general case given by the diﬀerential problem (12.41) we shall assume that
the coeﬃcients satisfy
−1
2β′ + γ ≥0,
∀x ∈[0, 1].
(12.50)
This ensures that the Galerkin problem (12.46) admits a unique solution
depending continuously on the data. Taking vh = uh in (12.46) we obtain
α0|uh|2
H1(0,1) ≤
1
>
0
αu′
hu′
h dx +
1
>
0
γuhuh dx = (f, uh) ≤∥f∥L2(0,1)∥uh∥L2(0,1),
where we have used the Cauchy-Schwarz inequality (8.29) to set the right-
hand side inequality. Owing to the Poincar´e inequality (12.16) we conclude
that
|uh|H1(0,1) ≤CP
α0
∥f∥L2(0,1).
(12.51)

12.4 The Galerkin Method
549
Thus, the norm of the Galerkin solution remains bounded (uniformly with
respect to the dimension of the subspace Vh) provided that f ∈L2(0, 1).
Inequality (12.51) therefore represents a stability result for the solution of
the Galerkin problem.
As for convergence, we can prove the following result.
Theorem 12.3 Let C = α−1
0 (∥α∥∞+ C2
P ∥γ∥∞); then, we have
|u −uh|H1(0,1) ≤C min
wh∈Vh|u −wh|H1(0,1).
(12.52)
Proof. Subtracting (12.46) from (12.43) (where we use vh ∈Vh ⊂V ), owing to
the bilinearity of the form a(·, ·) we obtain
a(u −uh, vh) = 0
∀vh ∈Vh.
(12.53)
Then, setting e(x) = u(x) −uh(x), we deduce
α0|e|2
H1(0,1) ≤a(e, e) = a(e, u −wh) + a(e, wh −uh)
∀wh ∈Vh.
The last term is null due to (12.53). On the other hand, still by the Cauchy-
Schwarz inequality we obtain
a(e, u −wh)
=
1
>
0
αe′(u −wh)′ dx +
1
>
0
γe(u −wh) dx
≤∥α∥∞|e|H1(0,1)|u −wh|H1(0,1) + ∥γ∥∞∥e∥L2(0,1)∥u −wh∥L2(0,1).
The desired result (12.52) now follows by using again the Poincar´e inequality for
both ∥e∥L2(0,1) and ∥u −wh∥L2(0,1).
3
The previous results can be obtained under more general hypotheses on
problems (12.43) and (12.46). Precisely, we can assume that V is a Hilbert
space, endowed with norm ∥·∥V , and that the bilinear form a : V ×V →R
satisﬁes the following properties:
∃α0 > 0 : a(v, v) ≥α0∥v∥2
V
∀v ∈V (coercivity),
(12.54)
∃M > 0 : |a(u, v)| ≤M∥u∥V ∥v∥V
∀u, v ∈V (continuity).
(12.55)
Moreover, the right hand side (f, v) satisﬁes the following inequality
|(f, v)| ≤K∥v∥V
∀v ∈V.
Then both problems (12.43) and (12.46) admit unique solutions that satisfy
∥u∥V ≤K
α0
,
∥uh∥V ≤K
α0
.

550
12. Two-Point Boundary Value Problems
This is a celebrated result which is known as the Lax-Milgram Lemma (for
its proof see, e.g., [QV94]). Besides, the following error inequality holds
∥u −uh∥V ≤M
α0
min
wh∈Vh∥u −wh∥V .
(12.56)
The proof of this last result, which is known as C´ea’s Lemma, is very similar
to that of (12.52) and is left to the reader.
We now wish to notice that, under the assumption (12.54), the matrix
introduced in (12.48) is positive deﬁnite. To show this, we must check that
vT Bv ≥0 ∀v ∈RN and that vT Bv = 0 ⇔v = 0 (see Section 1.12).
Let us associate with a generic vector v = (vi) of RN the function vh =
N
j=1 vjϕj ∈Vh. Since the form a(·, ·) is bilinear and coercive we get
vT AGv
=
N

j=1
N

i=1
viaijvj =
N

j=1
N

i=1
via(ϕj, ϕi)vj
=
N

j=1
N

i=1
a(vjϕj, viϕi) = a


N

j=1
vjϕj,
N

i=1
viϕi


=
a(vh, vh) ≥α∥vh∥2
V ≥0.
Moreover, if vT AGv = 0 then also ∥vh∥2
V = 0 which implies vh = 0 and
thus v = 0.
It is also easy to check that the matrix AG is symmetric iﬀthe bilinear
form a(·, ·) is symmetric.
For example, in the case of problem (12.41) with β = γ = 0 the ma-
trix AG is symmetric and positive deﬁnite (s.p.d.) while if β and γ are
nonvanishing, AG is positive deﬁnite only under the assumption (12.50).
If AG is s.p.d. the numerical solution of the linear system (12.48) can be
eﬃciently carried out using direct methods like the Cholesky factorization
(see Section 3.4.2) as well as iterative methods like the conjugate gradient
method (see Section 4.3.4). This is of particular interest in the solution of
boundary value problems in more than one space dimension (see Section
12.6).
12.4.5
The Finite Element Method
The ﬁnite element method (FEM) is a special technique for constructing
a subspace Vh in (12.46) based on the piecewise polynomial interpolation
considered in Section 8.3. With this aim, we introduce a partition Th of
[0,1] into n subintervals Ij = [xj, xj+1], n ≥2, of width hj = xj+1 −xj,
j = 0, . . . , n −1, with
0 = x0 < x1 < . . . < xn−1 < xn = 1

12.4 The Galerkin Method
551
and let h = max
Th (hj). Since functions in H1
0(0, 1) are continuous it makes
sense to consider for k ≥1 the family of piecewise polynomials Xk
h intro-
duced in (8.22) (where now [a, b] must be replaced by [0, 1]). Any function
vh ∈Xk
h is a continuous piecewise polynomial over [0, 1] and its restriction
over each interval Ij ∈Th is a polynomial of degree ≤k. In the following
we shall mainly deal with the cases k = 1 and k = 2.
Then, we set
Vh = Xk,0
h
=

vh ∈Xk
h : vh(0) = vh(1) = 0

.
(12.57)
The dimension N of the ﬁnite element space Vh is equal to nk −1. In the
following the two cases k = 1 and k = 2 will be examined.
To assess the accuracy of the Galerkin FEM we ﬁrst notice that, thanks
to C´ea’s lemma (12.56), we have
min
wh∈Vh∥u −wh∥H1
0(0,1) ≤∥u −Πk
hu∥H1
0(0,1)
(12.58)
where Πk
hu is the interpolant of the exact solution u ∈V of (12.43) (see
Section 8.3). From inequality (12.58) we conclude that the matter of esti-
mating the Galerkin approximation error ∥u −uh∥H1
0(0,1) is turned into the
estimate of the interpolation error ∥u −Πk
hu∥H1
0(0,1). When k = 1, using
(12.56) and (8.27) we obtain
∥u −uh∥H1
0(0,1) ≤M
α0
Ch∥u∥H2(0,1)
provided that u ∈H2(0, 1). This estimate can be extended to the case k > 1
as stated in the following convergence result (for its proof we refer, e.g., to
[QV94], Theorem 6.2.1).
Property 12.1 Let u ∈H1
0(0, 1) be the exact solution of (12.43) and uh ∈
Vh its ﬁnite element approximation using continuous piecewise polynomials
of degree k ≥1. Assume also that u ∈Hs(0, 1) for some s ≥2. Then the
following error estimate holds
∥u −uh∥H1
0(0,1) ≤M
α0
Chl∥u∥Hl+1(0,1)
(12.59)
where l = min(k, s −1). Under the same assumptions, one can also prove
that
∥u −uh∥L2(0,1) ≤Chl+1∥u∥Hl+1(0,1).
(12.60)
The estimate (12.59) shows that the Galerkin method is convergent, i.e. the
approximation error tends to zero as h →0 and the order of convergence is

552
12. Two-Point Boundary Value Problems
k. We also see that there is no convenience in increasing the degree k of the
ﬁnite element approximation if the solution u is not suﬃciently smooth. In
this respect l is called a regularity threshold. The obvious alternative to gain
accuracy in such a case is to reduce the stepzise h. Spectral methods, which
will be considered in Section 12.4.7, instead pursue the opposite strategy
(i.e. increasing the degree k) and are thus ideally suited to approximating
problems with highly smooth solutions.
An interesting situation is that where the exact solution u has the min-
imum regularity (s = 1). In such a case, C´ea’s lemma ensures that the
Galerkin FEM is still convergent since as h →0 the subspace Vh becomes
dense into V . However, the estimate (12.59) is no longer valid so that
it is not possible to establish the order of convergence of the numerical
method. Table 12.1 summarizes the orders of convergence of the FEM for
k = 1, . . . , 4 and s = 1, . . . , 5.
k
s = 1
s = 2
s = 3
s = 4
s = 5
1
only convergence
h1
h1
h1
h1
2
only convergence
h1
h2
h2
h2
3
only convergence
h1
h2
h3
h3
4
only convergence
h1
h2
h3
h4
TABLE 12.1. Order of convergence of the FEM as a function of k (the degree of
interpolation) and s (the Sobolev regularity of the solution u)
Let us now focus on how to generate a suitable basis {ϕj} for the ﬁnite
element space Xk
h in the special cases k = 1 and k = 2. The basic point is
to choose appropriately a set of degrees of freedom for each element Ij of
the partition Th (i.e., the parameters which permit uniquely identifying a
function in Xk
h). The generic function vh in Xk
h can therefore be written as
vh(x) =
nk

i=0
viϕi(x)
where {vi} denote the set of the degrees of freedom of vh and the basis
functions ϕi (which are also called shape functions) are assumed to satisfy
the Lagrange interpolation property ϕi(xj) = δij, i, j = 0, . . . , n, where δij
is the Kronecker symbol.
The space X1
h
This space consists of all continuous and piecewise linear functions over
the partition Th. Since a unique straight line passes through two distinct
nodes the number of degrees of freedom for vh is equal to the number
n + 1 of nodes in the partition. As a consequence, n + 1 shape functions

12.4 The Galerkin Method
553
ϕi, i = 0, . . . , n, are needed to completely span the space X1
h. The most
natural choice for ϕi, i = 1, . . . , n −1, is
ϕi(x) =















x −xi−1
xi −xi−1
for xi−1 ≤x ≤xi,
xi+1 −x
xi+1 −xi
for xi ≤x ≤xi+1,
0
elsewhere.
(12.61)
The shape function ϕi is thus piecewise linear over Th, its value is 1 at
the node xi and 0 at all the other nodes of the partition. Its support (i.e.,
the subset of [0, 1] where ϕi is nonvanishing) consists of the union of the
intervals Ii−1 and Ii if 1 ≤i ≤n −1 while it coincides with the interval I0
(respectively In−1) if i = 0 (resp., i = n). The plots of ϕi, ϕ0 and ϕn are
shown in Figure 12.3.
1
x1
x0
xi−1 xi
xi+1
xn−1 xn = 1
ϕi
ϕn
ϕ0
FIGURE 12.3. Shape functions of X1
h associated with internal and boundary
nodes
For any interval Ii = [xi, xi+1], i = 0, . . . , n −1, the two basis functions ϕi
and ϕi+1 can be regarded as the images of two “reference” shape functions
ϕ0 and ϕ1 (deﬁned over the reference interval [0, 1]) through the linear
aﬃne mapping φ : [0, 1] →Ii
x = φ(ξ) = xi + ξ(xi+1 −xi),
i = 0, . . . , n −1.
(12.62)
Deﬁning ϕ0(ξ) = 1 −ξ, ϕ1(ξ) = ξ, the two shape functions ϕi and ϕi+1
can be constructed over the interval Ii as
ϕi(x) = ϕ0(ξ(x)),
ϕi+1(x) = ϕ1(ξ(x))
where ξ(x) = (x −xi)/(xi+1 −xi) (see Figure 12.4).

554
12. Two-Point Boundary Value Problems
0
1
ϕ1
ξ
1
xi
xi+1
ϕi+1
x
1
φ
−→
FIGURE 12.4. Linear aﬃne mapping φ from the reference interval to the generic
interval of the partition
The space X2
h
The generic function vh ∈X2
h is a piecewise polynomial of degree 2 over
each interval Ii. As such, it can be uniquely determined once three values
of it at three distinct points of Ii are assigned. To ensure continuity of
vh over [0, 1] the degrees of freedom are chosen as the function values at
the nodes xi of Th, i = 0, . . . , n, and at the midpoints of each interval Ii,
i = 0, . . . , n −1, for a total number equal to 2n + 1. It is convenient to
label the degrees of freedom and the corresponding nodes in the partition
starting from x0 = 0 until x2n = 1 in such a way that the midpoints of
each interval correspond to the nodes with odd index while the endpoints
of each interval correspond to the nodes with even index.
The explicit expression of the single shape function is
(i even)
ϕi(x) =















(x −xi−1)(x −xi−2)
(xi −xi−1)(xi −xi−2)
for xi−2 ≤x ≤xi,
(xi+1 −x)(xi+2 −x)
(xi+1 −xi)(xi+2 −xi)
for xi ≤x ≤xi+2,
0
elsewhere,
(12.63)
(i odd)
ϕi(x) =





(xi+1 −x)(x −xi−1)
(xi+1 −xi)(xi −xi−1)
for xi−1 ≤x ≤xi+1,
0
elsewhere.
(12.64)
Each basis function enjoys the property that ϕ(xj) = δij, i, j = 0, . . . , 2n.
The shape functions for X2
h on the reference interval [0, 1] are
ϕ0(ξ) = (1 −ξ)(1 −2ξ),
ϕ1(ξ) = 4(1 −ξ)ξ,
ϕ2(ξ) = ξ(2ξ −1)(12.65)

12.4 The Galerkin Method
555
and are shown in Figure 12.5. As in the case of piecewise linear ﬁnite
elements of X1
h the shape functions (12.63) and (12.64) are the images of
(12.65) through the aﬃne mapping (12.62). Notice that the support of the
basis function ϕ2i+1 associated with the midpoint x2i+1 coincides with the
interval to which the midpoint belongs. Due to its shape ϕ2i+1 is usually
referred to as bubble function.
ϕ2
ϕ1
ϕ0
ξ
0
0.5
1
FIGURE 12.5. Basis functions of X2
h on the reference interval
So far, we have considered only lagrangian-type shape functions. If this
constraint is removed other kind of bases can be derived. A notable example
(on the reference interval) is given by
ψ0(ξ) = 1 −ξ,
ψ1(ξ) = (1 −ξ)ξ,
ψ2(ξ) = ξ.
(12.66)
This basis is called hierarchical since it is generated using the shape func-
tions of the subspace having an immediately lower dimension than X2
h (i.e.
X1
h). Precisely, the bubble function ψ1 ∈X2
h is added to the shape func-
tions ψ0 and ψ2 which belong to X1
h. Hierarchical bases can be of some
interest in numerical computations if the local degree of the interpolation
is adaptively increased (p-type adaptivity).
To check that (12.66) forms a basis for X2
h we must verify that its func-
tions are linearly independent, i.e.
α0 ψ0(ξ) + α1 ψ1(ξ) + α2 ψ2(ξ) = 0,
∀ξ ∈[0, 1]
⇔
α0 = α1 = α2 = 0.
In our case this holds true since if
2

i=0
αi ψi(ξ) = α0 + ξ(α1 −α0 + α2) −α1ξ2 = 0,
∀ξ ∈[0, 1]
then necessarily α0 = 0, α1 = 0 and thus α2 = 0.

556
12. Two-Point Boundary Value Problems
A procedure analogous to that examined in the sections above can be used
in principle to construct a basis for every subspace Xk
h with k being arbi-
trary. However, it is important to remember that an increase in the degree k
of the polynomial approximation gives rise to an increase of the number of
degrees of freedom of the FEM and, as a consequence, of the computational
cost required for the solution of the linear system (12.48).
Let us now examine the structure and the basic properties of the stiﬀness
matrix associated with system (12.48) in the case of the ﬁnite element
method (AG = Afe).
Since the ﬁnite element basis functions for Xk
h have a local support, Afe is
sparse. In the particular case k = 1, the support of the shape function ϕi is
the union of the intervals Ii−1 and Ii if 1 ≤i ≤n−1, and it coincides with
the interval I0 (respectively In−1) if i = 0 (resp., i = n). As a consequence,
for a ﬁxed i = 1, . . . , n −1, only the shape functions ϕi−1 and ϕi+1 have a
nonvanishing support intersection with that of ϕi, which implies that Afe
is tridiagonal since aij = 0 if j ̸∈{i −1, i, i + 1}. In the case k = 2 one
concludes with an analogous argument that Afe is a pentadiagonal matrix.
The condition number of Afe is a function of the grid size h; indeed,
K2(Afe) = ∥Afe∥2∥A−1
fe ∥2 = O(h−2)
(for the proof, see [QV94], Section 6.3.2), which demonstrates that the
conditioning of the ﬁnite element system (12.48) grows rapidly as h →
0. This is clearly conﬂicting with the need of increasing the accuracy of
the approximation and, in multidimensional problems, demands suitable
preconditioning techniques if iterative solvers are used (see Section 4.3.2).
Remark 12.4 (Elliptic problems of higher order) The Galerkin me-
thod in general, and the ﬁnite element method in particular, can also be
applied to other type of elliptic equations, for instance to those of fourth
order. In that case, the numerical solution (as well as the test functions)
should be continuous together with their ﬁrst derivative. An example has
been illustrated in Section 8.8.1.
■
12.4.6
Implementation Issues
In this section we implement the ﬁnite element (FE) approximation with
piecewise linear elements (k = 1) of the boundary value problem (12.41)
(shortly, BVP) with non homogeneous Dirichlet boundary conditions.
Here is the list of the input parameters of Program 94: Nx is the number
of grid subintervals; I is the interval [a, b], alpha, beta, gamma and f are
the macros corresponding to the coeﬃcients in the equation, bc=[ua,ub]
is a vector containing the Dirichlet boundary conditions for u at x = a and
x = b and stabfun is an optional string variable. It can assume diﬀerent

12.4 The Galerkin Method
557
values, allowing the user to select the desired type of artiﬁcial viscosity that
may be needed for dealing with the problems addressed in Section 12.5.
Program 94 - ellfem : Linear FE for two-point BVPs
function [uh,x] = ellfem(Nx,I,alpha,beta,gamma,f,bc,stabfun)
a = I(1); b = I(2); h = (b-a)/Nx; x = [a+h/2:h:b-h/2];
alpha = eval(alpha); beta = eval(beta); gamma = eval(gamma);
f
= eval(f);
rhs = 0.5*h*(f(1:Nx-1)+f(2:Nx));
if nargin == 8
[Afe,rhsbc] = femmatr(Nx,h,alpha,beta,gamma,stabfun);
else
[Afe,rhsbc] = femmatr(Nx,h,alpha,beta,gamma);
end
[L,U,P]
= lu(Afe);
rhs(1)
= rhs(1)-bc(1)*(-alpha(1)/h-beta(1)/2+h*gamma(1)/3+rhsbc(1));
rhs(Nx-1) = rhs(Nx-1)-bc(2)*(-alpha(Nx)/h+beta(Nx)/2+h*gamma(Nx)/3+rhsbc(2));
rhs = P*rhs’; z = L \ rhs;
w = U \ z;
uh = [bc(1), w’, bc(2)]; x = [a:h:b];
Program 95 computes the stiﬀness matrix Afe; with this aim, the coeﬃcients
α, β and γ and the forcing term f are replaced by piecewise constant
functions on each mesh subinterval and the remaining integrals in (12.41),
involving the basis functions and their derivatives, are evaluated exactly.
Program 95 - femmatr : Construction of the stiﬀness matrix
function [Afe,rhsbc] = femmatr(Nx,h,alpha,beta,gamma,stabfun)
for i=2:Nx
dd(i-1)=(alpha(i-1)+alpha(i))/h; dc(i-1)=-(beta(i)-beta(i-1))/2;
dr(i-1)=h*(gamma(i-1)+gamma(i))/3;
if i > 2
ld(i-2) = -alpha(i-1)/h; lc(i-2)=-beta(i-1)/2;
lr(i-2) = h*gamma(i-1)/6;
end
if i < Nx
ud(i-1) = -alpha(i)/h; uc(i-1)=beta(i)/2;
ur(i-1) = h*gamma(i)/6;
end
end
Kd=spdiags([[ld 0]’,dd’,[0 ud]’],-1:1,Nx-1,Nx-1);
Kc=spdiags([[lc 0]’,dc’,[0 uc]’],-1:1,Nx-1,Nx-1);
Kr=spdiags([[lr 0]’,dr’,[0 ur]’],-1:1,Nx-1,Nx-1);
Afe=Kd+Kc+Kr;
if nargin == 6
s=[’[Ks,rhsbc]=’,stabfun,’(Nx,h,alpha,beta);’]; eval(s)
Afe = Afe + Ks;
else

558
12. Two-Point Boundary Value Problems
rhsbc = [0, 0];
end
The H1-norm of the error can be computed by calling Program 96, which
must be supplied by the macros u and ux containing the expression of the
exact solution u and of u′. The computed numerical solution is stored in
the output vector uh, while the vector coord contains the grid coordinates
and h is the mesh size. The integrals involved in the computation of the
H1-norm of the error are evaluated using the composite Simpson formula
(9.17).
Program 96 - H1error : Computation of the H1-norm of the error
function [L2err,H1err]=H1error(coord,h,uh,u,udx)
nvert=max(size(coord)); x=[]; k=0;
for i = 1:nvert-1
xm = (coord(i+1)+coord(i))*0.5;
x = [x, coord(i),xm];
k = k + 2;
end
ndof = k+1; x (ndof) = coord (nvert);
uq = eval(u); uxq = eval(udx); L2err = 0; H1err = 0;
for i=1:nvert-1
L2err = L2err + (h/6)*((uh(i)-uq(2*i-1))ˆ2+...
4*(0.5*uh(i)+0.5*uh(i+1)-uq(2*i))ˆ2+(uh(i+1)-uq(2*i+1))ˆ2);
H1err = H1err + (1/(6*h))*((uh(i+1)-uh(i)-h*uxq(2*i-1))ˆ2+...
4*(uh(i+1)-uh(i)-h*uxq(2*i))ˆ2+(uh(i+1)-uh(i)-h*uxq(2*i+1))ˆ2);
end
H1err = sqrt(H1err + L2err); L2err = sqrt(L2err);
Example 12.1 We assess the accuracy of the ﬁnite element solution of the fol-
lowing problem. Consider a thin rod of length L whose temperature at x = 0 is
ﬁxed to t0 while the other endpoint x = L is thermally isolated. Assume that the
rod has a cross-section with constant area equal to A and that the perimeter of
A is p.
The temperature u of the rod at a generic point x ∈(0, L) is governed by the
following boundary value problem with mixed Dirichlet-Neumann conditions



−µAu′′ + σpu = 0
x ∈(0, L),
u(0) = u0,
u′(L) = 0,
(12.67)
where µ denotes the thermal conductivity and σ is the convective transfer coef-
ﬁcient. The exact solution of the problem is the (smooth) function
u(x) = u0 cosh[m(L −x)]
cosh(mL)
,
where m =

σp/µA. We solve the problem by using linear and quadratic ﬁnite
elements (k = 1 and k = 2) on a grid with uniform size. In the numerical

12.4 The Galerkin Method
559
computations we assume that the length of the rod is L = 100cm and that the
rod has a circular cross-section of radius 2cm (and thus, A = 4πcm2, p = 4πcm).
We also set u0 = 10◦C, σ = 2 and µ = 200.
Figure 12.6 (left) shows the behavior of the error in the L2 and H1 norms for
the linear and quadratic elements, respectively. Notice the excellent agreement
between the numerical results and the expected theoretical estimates (12.59) and
(12.60), i.e., the orders of convergence in the L2 norm and the H1 norm tend
respectively to k + 1 and k if ﬁnite elements of degree k are employed, since the
exact solution is smooth.
•
10
−1
10
0
10
1
10
−6
10
−5
10
−4
10
−3
10
−2
10
−1
10
0
10
1
10
20
30
40
50
60
10
−14
10
−12
10
−10
10
−8
10
−6
10
−4
10
−2
10
0
10
2
FIGURE 12.6. Left: error curves for linear and quadratic elements. The dashed
and solid lines denote the H1(0, L) and L2(0, L) norms of the error in the case
k = 1, while the dot-line and dotted line denote the corresponding norms in the
case k = 2. Right: error curves for the spectral collocation method. The dashed
and solid lines denote the H1(0, L) and L2(0, L) norms of the error, respectively
12.4.7
Spectral Methods
It turns out that the spectral collocation method of Section 12.3 can be
regarded as a Galerkin method where the subspace is P0
n and the integrals
are approximated by the Gauss-Lobatto quadrature formula. As a matter
of fact, the approximation of problem (12.38) is
ﬁnd un ∈P0
n : an(un, vn) = (f, vn)n ∀vn ∈P0
n,
(12.68)
where an is the bilinear form that is obtained from the bilinear form a
by replacing exact integrals by the Gauss-Lobatto formula (12.37). For
problem (12.41) the associated bilinear form a was introduced in (12.44).
We would therefore obtain
an(un, vn) = (αu′
n, v′
n)n + (βu′
n, vn)n + (γun, vn)n.
(12.69)
This is no longer a Galerkin method, but is called a generalized Galerkin
approximation. Its analysis requires more care than for Galerkin methods,

560
12. Two-Point Boundary Value Problems
as already seen in Section 12.3 and depends on the Strang lemma (see
[QV94]). However, the same kind of error estimate (12.39) can be proven
in this case as well.
A further generalization combining the ﬁnite element approach with
piecewise polynomials of high degree and Gauss-Lobatto integration on
each element yields the so-called spectral element method and the h −p
version of the ﬁnite element method (here p stands for the polynomial de-
gree that we have denoted with n). In these cases convergence is achieved
letting simultaneously (or independently) h go to zero and p go to inﬁnity.
(See, e.g., [BM92], [SS98]).
Example 12.2 We consider again the two-point boundary value problem (12.67)
and employ the spectral collocation method for its numerical approximation.
We show in Figure 12.6 (right) the error curves in the L2(0, L) (solid line) and
H1(0, L) (dashed line) norms as functions of the spectral degree n, with n = 4−k,
k = 1, . . . , 5. Notice the high accuracy that is achieved, even when a small value
of n is used, due to the smoothness of the exact solution. Notice also that for
n ≥32 the accuracy is actually bounded by the eﬀect of rounding errors.
•
12.5
Advection-Diﬀusion Equations
Boundary value problems of the form (12.41) are used to describe processes
of diﬀusion, advection and absorption (or reaction) of a certain quantity
which is identiﬁed with u(x). The term −(αu′)′ is responsible for the diﬀu-
sion, βu′ for the advection (or transport), γu for the absorption (if γ > 0).
In this section we focus on the case where α is small compared with β (or
γ). In these cases, the Galerkin method that we introduced earlier might
be unsuitable for providing accurate numerical results. A heuristic explana-
tion can be drawn from the inequality (12.56), noticing that in this case the
constant M/α0 can be very large, hence the error estimate can be mean-
ingless unless h is much smaller than (M/α0)−1. For instance, if α = ε,
γ = 0 and β = const ≫1, then α0 = ε and M = ε + CP β. Similarly, if
α = ε, β = 0 and γ = const ≫1 then α0 = ε and M = ε + C2
P γ.
To keep our analysis at the simplest possible level, we will consider the
following elementary two-point boundary value problem
" −εu′′ + βu′ = 0,
0 < x < 1,
u(0) = 0,
u(1) = 1,
(12.70)
where ε and β are two positive constants such that ε/β ≪1. Despite
its simplicity, (12.70) provides an interesting paradigm of an advection-
diﬀusion problem in which advection dominates diﬀusion.

12.5 Advection-Diﬀusion Equations
561
We deﬁne the global P´eclet number as
Pegl = |β|L
2ε
(12.71)
where L is the size of the domain (equal to 1 in our case). The global P´eclet
number measures the dominance of the advective term over the diﬀusive
one.
Let us ﬁrst compute the exact solution of problem (12.70). The charac-
teristic equation associated to the diﬀerential equation is −ελ2 + βλ = 0
and admits the roots λ1 = 0 and λ2 = β/ε. Then
u(x) = C1eλ1x + C2eλ2x = C1 + C2e
β
ε x,
where C1 and C2 are arbitrary constants. Imposing the boundary conditions
yields C1 = −1/(eβ/ε −1) = −C2, therefore
u(x) = (exp(βx/ε) −1) / (exp(β/ε) −1) .
If β/ε ≪1 we can expand the exponentials up to ﬁrst order obtaining
u(x) = (1 + β
ε x + · · · −1)/(1 + β
ε + · · · −1) ≃(β x/ε)/(β/ε) = x,
thus the solution is close to the solution of the limit problem −εu′′ = 0,
which is a straight line interpolating the boundary data.
However, if β/ε ≫1 the exponentials attain big values so that
u(x) ≃exp(β/εx)
exp(β/ε) = exp

−β
ε (1 −x)

.
Since the exponent is big and negative the solution is almost equal to zero
everywhere unless a small neighborhood of the point x = 1 where the
term 1 −x becomes very small and the solution joins the value 1 with an
exponential behaviour. The width of the neighbourhood is of the order of
ε/β and thus it is quite small: in such an event, we say that the solution
exhibits a boundary layer of width O (ε/β) at x = 1.
12.5.1
Galerkin Finite Element Approximation
Let us discretize problem (12.70) using the Galerkin ﬁnite element method
introduced in Section 12.4.5 with k = 1 (piecewise linear ﬁnite elements).
The approximation to the problem is: ﬁnd uh ∈X1
h such that



a(uh, vh) = 0
∀vh ∈X1,0
h ,
uh(0) = 0, uh(1) = 1,
(12.72)

562
12. Two-Point Boundary Value Problems
where the ﬁnite element spaces X1
h and X1,0
h
have been introduced in (8.22)
and (12.57) and the bilinear form a(·, ·) is
a(uh, vh) =
> 1
0
(εu′
hv′
h + βu′
hvh) dx.
(12.73)
Remark 12.5 (Advection-diﬀusion problems in conservation form)
Sometimes, the advection-diﬀusion problem (12.70) is written in the follow-
ing conservation form
" −(J(u))′ = 0,
0 < x < 1,
u(0) = 0,
u(1) = 1,
(12.74)
where J(u) = εu′−βu is the ﬂux (already introduced in the ﬁnite diﬀerence
context in Section 12.2.3), ε and β are given functions with ε(x) ≥ε0 > 0
for all x ∈[0, 1]. The Galerkin approximation of (12.74) using piecewise
linear ﬁnite elements reads: ﬁnd uh ∈X1
h such that
b(uh, vh) = 0,
∀vh ∈X1,0
h
where b(uh, vh) =
1
>
0
(εu′
h −βuh)v′
h dx. The bilinear form b(·, ·) coincides
with the corresponding one in (12.73) when ε and β are constant.
■
Taking vh as a test function the generic basis function ϕi, (12.72) yields
1
>
0
εu′
hϕ′
i dx +
1
>
0
βu′
hϕi dx = 0
i = 1, . . . , n −1.
Setting uh(x) = n
j=0 ujϕj(x), and noting that supp(ϕi) = [xi−1, xi+1] the
above integral, for i = 1, . . . , n −1, reduces to
ε

ui−1
xi
>
xi−1
ϕ′
i−1ϕ′
i dx + ui
xi+1
>
xi−1
(ϕ′
i)2 dx + ui+1
xi+1
>
xi
ϕ′
iϕ′
i+1 dx


+β

ui−1
xi
>
xi−1
ϕ′
i−1ϕi dx + ui
xi+1
>
xi−1
ϕ′
iϕi dx + ui+1
xi+1
>
xi
ϕ′
i+1ϕi dx

= 0.
Assuming a uniform partition of [0, 1] with xi = xi−1 + h for i = 1, . . . , n,
h = 1/n, and noting that ϕ′
j(x) =
1
h if xj−1 ≤x ≤xj, ϕ′
j(x) = −1
h if
xj ≤x ≤xj+1, we deduce that
ε
h (−ui−1 + 2ui −ui+1) + 1
2β (ui+1 −ui−1) = 0,
i = 1, . . . , n −1.
(12.75)

12.5 Advection-Diﬀusion Equations
563
Multiplying by h/ε and deﬁning the local P´eclet number to be
Pe = |β|h
2ε
we ﬁnally obtain
(Pe −1) ui+1 + 2ui −(Pe + 1) ui−1 = 0.
i = 1, . . . , n −1.
(12.76)
This is a linear diﬀerence equation which admits a solution of the form
ui = A1ρi
1 + A2ρi
2 for suitable constants A1, A2 (see Section 11.4), where
ρ1 and ρ2 are the two roots of the following characteristic equation
(Pe −1) ρ2 + 2ρ −(Pe + 1) = 0.
Thus
ρ1,2 = −1 ±
√
1 + Pe2 −1
Pe −1
=



1 + Pe
1 −Pe,
1.
Imposing the boundary conditions at x = 0 and x = 1 gives
A1 = 1/(1 −
+
1+Pe
1−Pe
,n
),
A2 = −A1,
so that the solution of (12.76) is
ui =

1 −
1 + Pe
1 −Pe
i
/

1 −
1 + Pe
1 −Pe
n
i = 0, . . . , n.
We notice that if Pe > 1, a power with a negative base appears at the
numerator which gives rise to an oscillating solution. This is clearly visible
in Figure 12.7 where for several values of the local P´eclet number, the
solution of (12.76) is compared with the exact solution (sampled at the
mesh nodes) corresponding to a value of the global P´eclet equal to 50.
The simplest remedy for preventing the oscillations consists of choosing
a suﬃciently small mesh stepsize h in such a way that Pe < 1. However this
approach is often impractical: for example, if β = 1 and ε = 5 · 10−5 one
should take h < 10−4 which amounts to dividing [0, 1] into 10000 subin-
tervals, a strategy that becomes unfeasible when dealing with multidimen-
sional problems. Other strategies can be pursued, as will be addressed in
the next sections.
12.5.2
The Relationship between Finite Elements and Finite
Diﬀerences; the Numerical Viscosity
To examine the behaviour of the ﬁnite diﬀerence method (FD) when applied
to the solution of advection-diﬀusion problems and its relationship with the

564
12. Two-Point Boundary Value Problems
0.75
0.8
0.85
0.9
0.95
1
−0.5
0
0.5
1
FIGURE 12.7. Finite diﬀerence solution of the advection-diﬀusion problem
(12.70) (with Pegl = 50) for several values of the local P´eclet number. Solid
line: exact solution, dot-dashed line: Pe = 2.63, dotted line: Pe = 1.28, dashed
line: Pe = 0.63
ﬁnite element method (FE), we again consider the one-dimensional problem
(12.70) with a uniform meshsize h.
To ensure that the local discretization error is of second order we approx-
imate u′(xi) and u′′(xi), i = 1, . . . , n −1, by the centred ﬁnite diﬀerences
(10.61) and (10.65) respectively (see Section 10.10.1). We obtain the fol-
lowing FD problem



−εui+1 −2ui + ui−1
h2
+ β ui+1 −ui−1
2h
= 0,
i = 1, . . . , n −1
u0 = 0,
un = 1.
(12.77)
If we multiply by h for every i = 1, . . . , n −1, we obtain exactly the same
equation (12.75) that was obtained using piecewise linear ﬁnite elements.
The equivalence between FD and FE can be proﬁtably employed to de-
vise a cure for the oscillations arising in the approximate solution of (12.75)
when the local P´eclet number is larger than 1. The important observation
here is that the instability in the FD solution is due to the fact that the
discretization scheme is a centred one. A possible remedy consists of ap-
proximating the ﬁrst derivative by a one-sided ﬁnite diﬀerence according
to the direction of the transport ﬁeld. Precisely, we use the backward dif-
ference if the convective coeﬃcient β is positive and the forward diﬀerence
otherwise. The resulting scheme when β > 0 is
−εui+1 −2ui + ui−1
h2
+ β ui −ui−1
h
= 0
i = 1, . . . , n −1
(12.78)
which, for ε = 0 reduces to ui = ui−1 and therefore yields the desired
constant solution of the limit problem βu′ = 0. This one-side discretization

12.5 Advection-Diﬀusion Equations
565
of the ﬁrst derivative is called upwind diﬀerencing: the price to be paid for
the enhanced stability is a loss of accuracy since the upwind ﬁnite diﬀerence
introduces a local discretization error of O(h) and not of O(h2) as happens
using centred ﬁnite diﬀerences.
Noting that
ui −ui−1
h
= ui+1 −ui−1
2h
−h
2
ui+1 −2ui + ui−1
h2
,
the upwind ﬁnite diﬀerence can be interpreted as the sum of a centred ﬁnite
diﬀerence approximating the ﬁrst derivative and of a term proportional to
the discretization of the second-order derivative. Consequently, (12.78) is
equivalent to
−εh
ui+1 −2ui + ui−1
h2
+ β ui+1 −ui−1
2h
= 0
i = 1, . . . , n −1
(12.79)
where εh = ε(1 + Pe). This amounts to having replaced the diﬀerential
equation (12.72) with the perturbed one
−εhu′′ + βu′ = 0,
(12.80)
then using centred ﬁnite diﬀerences to approximate both u′ and u′′. The
perturbation
−ε Pe u′′ = −βh
2
u′′
(12.81)
is called the numerical viscosity (or artiﬁcial diﬀusion). In Figure 12.8
a comparison between centred and upwinded discretizations of problem
(12.72) is shown.
More generally, we can resort to a centred scheme of the form (12.80)
with the following viscosity
εh = ε(1 + φ(Pe))
(12.82)
where φ is a suitable function of the local P´eclet number satisfying
lim
t→0+φ(t) = 0.
Notice that when φ(t) = 0 for all t, one recovers the centred ﬁnite diﬀerence
method (12.77), while if φ(t) = t the upwind ﬁnite diﬀerence scheme (12.78)
(or, equivalently, (12.79)) is obtained. Other choices are admissible as well.
For instance, taking
φ(t) = t −1 + B(2t)
(12.83)
where B(t) is the Bernoulli function deﬁned as B(t) = t/(et −1) for t ̸= 0
and B(0) = 1, yields the so called exponential ﬁtting ﬁnite diﬀerence scheme
which is also well known as the Scharfetter-Gummel (SG) method [SG69].

566
12. Two-Point Boundary Value Problems
0.7
0.75
0.8
0.85
0.9
0.95
1
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
1.2
FIGURE 12.8. Finite diﬀerence solution of (12.72) (with ε = 1/100) using a
centred discretization (dashed and dotted lines) and the artiﬁcial viscosity (12.81)
(dot-dashed and starred lines). The solid line denotes the exact solution. Notice
the eﬀect of eliminating the oscilations when the local P´eclet number is large;
conversely, notice also the corresponding loss of accuracy for low values of the
local P´eclet number
0
0.5
1
1.5
2
2.5
0
0.5
1
1.5
2
2.5
FIGURE 12.9. The functions φUP (dash-dotted line) and φSG (solid line)
Remark 12.6 Denoting respectively by φC, φUP and φSG the previous
three functions, i.e. φC = 0, φUP (t) = t and φSG(t) = t −1 + B(2t) (see
Figure 12.9), we notice that φSG ≃φUP as Pe →+∞while φSG = O(h2)
and φUP = O(h) if Pe →0+. Therefore, the SG method is second-order
accurate with respect to h and for this reason it is an optimal viscosity
upwind method. Actually, one can show (see [HGR96], pp. 44-45) that
if f is piecewise constant over the grid partition the SG scheme yields a
numerical solution uSG
h
which is nodally exact, i.e., uSG
h (xi) = u(xi) for
each node xi, irrespectively of h (and, thus, of the size of the local P´eclet
number). This is demonstrated in Figure 12.10.
■

12.5 Advection-Diﬀusion Equations
567
0.9
0.95
1
0
0.5
1
FIGURE 12.10. Comparison between the numerical solutions of problem (12.72)
(with ε = 1/200) obtained using the artiﬁcial viscosity (12.81) (dashed line where
the symbol ■denotes the nodal values) and with the optimal viscosity (12.83)
(dotted line where the symbol ◦denotes the nodal values) in the case where
Pe = 1.25. The solid line denotes the exact solution
The new local P´eclet number associated with the scheme (12.79)-(12.82) is
deﬁned as
Pe∗= |β|h
2εh
=
Pe
(1 + φ(Pe)).
For both the upwind and the SG schemes we have Pe∗< 1 for any value
of h. This implies that the matrix associated with these methods is a M-
matrix for any h (see Deﬁnition 1.25 and Exercise 13), and, in turn, that the
numerical solution uh satisﬁes a discrete maximum principle (see Section
12.2.2).
12.5.3
Stabilized Finite Element Methods
In this section we extend the use of numerical viscosity introduced in
the previous section for ﬁnite diﬀerences to the Galerkin method using
ﬁnite elements of arbitrary degree k ≥1. For this purpose we consider
the advection-diﬀusion problem (12.70) where the viscosity coeﬃcient ε is
replaced by (12.82). This yields the following modiﬁcation of the original
Galerkin problem (12.72):
ﬁnd
0uh∈Xk,0
h
=

vh ∈Xk
h : vh(0) = vh(1) = 0

such that
ah(
0uh, vh) = −
1
>
0
βvh dx
∀vh ∈Xk,0
h ,
(12.84)
where
ah(u, v) = a(u, v) + b(u, v),

568
12. Two-Point Boundary Value Problems
and
b(u, v) = ε φ(Pe)
1
>
0
u′v′ dx
is called the stabilization term. Since ah(v, v) = εh|v|2
1 for all v ∈H1
0(0, 1)
and εh/ε = (1 + φ(Pe)) ≥1, the modiﬁed problem (12.84) enjoys more
favorable monotonicity properties than the corresponding non stabilized
Galerkin formulation (12.75).
To prove convergence, it is suﬃcient to show that
0uh tends to
0u as h →0,
where
0u (x) = u(x) −x. This is done in the following theorem, where we
assume that
0u (and henceforth u) has the required regularity.
Theorem 12.4 If k = 1 then
|
0u −
0uh |H1(0,1) ≤Ch G(
0u)
(12.85)
where C > 0 is a suitable constant independent of h and
0u, and
G(
0u) =





|
0u |H1(0,1) + |
0u |H2(0,1)
for the upwind method
|
0u |H2(0,1)
for the SG method.
Further, if k = 2 the SG method yields the improved error estimate
|
0u −
0uh |H1(0,1) ≤Ch2(|
0u |H1(0,1) + |
0u |H3(0,1)).
(12.86)
Proof. From (12.70) we obtain
a(
0u, vh) = −
1
>
0
βvhdx,
∀vh ∈Xk,0
h .
By comparison with (12.84) we get
ah(
0u −
0uh, vh) = b(
0u, vh),
∀vh ∈Xk,0
h .
(12.87)
Denote by Eh =
0u −
0uh the discretization error and recall that the space H1
0(0, 1)
is endowed with the norm (12.49). Then,
εh|Eh|2
H1(0,1)
= ah(Eh, Eh) = ah(Eh,
0u −Πk
h
0u) + ah(Eh, Πk
h
0u −
0uh)
= ah(Eh,
0u −Πk
h
0u) + b(
0u, Πk
h
0u −
0uh)
where we have applied (12.87) with vh = Πk
h
0u −
0uh. Using the Cauchy-Schwarz
inequality we get
εh|Eh|2
H1(0,1)
≤Mh|Eh|H1(0,1)|
0u −Πk
h
0u |H1(0,1)
+εφ(Pe)
1
>
0
0
u′(Πk
h
0u −
0uh)′dx
(12.88)

12.5 Advection-Diﬀusion Equations
569
where Mh = εh + |β|CP is the continuity constant of the bilinear form ah(·, ·)
and CP is the Poincar´e constant introduced in (12.16).
Notice that if k = 1 (corresponding to piecewise linear ﬁnite elements) and
φ = φSG (SG optimal viscosity) the quantity in the second integral is identically
zero since
0uh= Π1
h
0u, as pointed out in Remark 12.6. Then, from (12.88) we get
|Eh|H1(0,1) ≤

1 + |β|CP
εh

|
0u −Π1
h
0u |H1(0,1).
Noting that εh > ε, using (12.71) and the interpolation estimate (8.27), we ﬁnally
obtain the error bound
|Eh|H1(0,1) ≤C(1 + 2PeglCP )h|
0u |H2(0,1).
In the general case the error inequality (12.88) can be further manipulated. Using
the Cauchy-Schwarz and triangular inequalities we obtain
1
>
0
0
u′(Πk
h
0u −
0uh)′dx ≤|
0u |H1(0,1)(|Πk
h
0u −
0u |H1(0,1) + |Eh|H1(0,1))
from which
εh|Eh|2
H1(0,1)
≤|Eh|H1(0,1)
+
Mh|
0u −Πk
h
0u |H1(0,1)
+εφ(Pe)|
0u |H1(0,1)
,
+ εφ(Pe)|
0u |H1(0,1)|
0u −Πk
h
0u |H1(0,1).
Using again the interpolation estimate (8.27) yields
εh|Eh|2
H1(0,1)
≤|Eh|H1(0,1)
+
MhChk|
0u |Hk+1(0,1) + εφ(Pe)|
0u |H1(0,1)
,
+Cεφ(Pe)|
0u |H1(0,1)hk|
0u |Hk+1(0,1).
Using Young’s inequality (12.40) gives
εh|Eh|2
H1(0,1)
≤
εh|Eh|2
H1(0,1)
2
+ 3
4εh
6
(MhChk|
0u |Hk+1(0,1))2 + (εφ(Pe)|
0u |H1(0,1))27
from which it follows that
|Eh|2
H1(0,1)
≤3
2
 Mh
εh
2
C2h2k|
0u |2
Hk+1(0,1) + 3
2
 ε
εh
2
φ(Pe)2|
0u |2
H1(0,1)
+2ε
εh φ(Pe)|
0u |H1(0,1)Chk|
0u |Hk+1(0,1).
Again using the fact that εh > ε and the deﬁnition (12.71) we get (Mh/εh) ≤
(1 + 2CP Pegl) and then
|Eh|2
H1(0,1)
≤3
2C2(1 + 2CP Pegl)2h2k|
0u |2
Hk+1(0,1)
+2φ(Pe)Chk|
0u |H1(0,1)|
0u |Hk+1(0,1) + 3
2φ(Pe)2|
0u |2
H1(0,1),

570
12. Two-Point Boundary Value Problems
which can be bounded further as
|Eh|2
H1(0,1)
≤
M
6
h2k|
0u |2
Hk+1(0,1)
+
φ(Pe)hk|
0u |H1(0,1)|
0u |Hk+1(0,1) + φ(Pe)2|
0u |2
H1(0,1)
7
(12.89)
for a suitable positive constant M.
If φUP = Cεh, where Cε = β/ε, we obtain
|Eh|2
H1(0,1)
≤Ch2 6
h2k−2|
0u |2
Hk+1(0,1)
+hk−1|
0u |H1(0,1)|
0u |Hk+1(0,1) + |
0u |2
H1(0,1)
7
which shows that using piecewise linear ﬁnite elements (i.e., k = 1) plus the
upwind artiﬁcial viscosity gives the linear convergence estimate (12.85).
In the case φ = φSG, assuming that for h suﬃciently small φSG ≤Kh2, for a
suitable positive constant K, we get
|Eh|2
H1(0,1)
≤Ch4 6
h2(k−2)|
0u |2
Hk+1(0,1)
+hk−2|
0u |H1(0,1)|
0u |Hk+1(0,1) + |
0u |2
H1(0,1)
7
which shows that using quadratic ﬁnite elements (i.e., k = 2) plus the optimal
artiﬁcial viscosity gives the second-order convergence estimate (12.86).
3
Programs 97 and 98 implement the computation of the artiﬁcial and opti-
mal artiﬁcial viscosities (12.81) and (12.83), respectively. These viscosities
can be selected by the user setting the input parameter stabfun in Pro-
gram 94 equal to artvisc or sgvisc. The function sgvisc employs the
function bern to evaluate the Bernoulli function in (12.83).
Program 97 - artvisc : Artiﬁcial viscosity
function [Kupw,rhsbc] = artvisc(Nx, h, nu, beta)
Peclet=0.5*h*abs(beta);
for i=2:Nx,
dd(i-1)=(Peclet(i-1)+Peclet(i))/h;
if i > 2, ld(i-2)=-Peclet(i-1)/h; end
if i < Nx, ud(i-1)=-Peclet(i)/h;
end
end
Kupw=spdiags([[ld 0]’,dd’,[0 ud]’],-1:1,Nx-1,Nx-1);
rhsbc = - [Peclet(1)/h, Peclet(Nx)/h];
Program 98 - sgvisc : Optimal artiﬁcial viscosity
function [Ksg,rhsbc] = sgvisc(Nx, h, nu, beta)
Peclet=0.5*h*abs(beta)./nu;
[bp, bn]=bern(2*Peclet);
Peclet=Peclet-1+bp;

12.5 Advection-Diﬀusion Equations
571
for i=2:Nx,
dd(i-1)=(nu(i-1)*Peclet(i-1)+nu(i)*Peclet(i))/h;
if i > 2, ld(i-2)=-nu(i-1)*Peclet(i-1)/h; end
if i < Nx, ud(i-1)=-nu(i)*Peclet(i)/h;
end
end
Ksg=spdiags([[ld 0]’,dd’,[0 ud]’],-1:1,Nx-1,Nx-1);
rhsbc = - [nu(1)*Peclet(1)/h, nu(Nx)*Peclet(Nx)/h];
Program 99 - bern : Evaluation of the Bernoulli function
function [bp,bn]=bern(x)
xlim=1e-2; ax=abs(x);
if (ax == 0), bp=1.; bn=1.; return; end;
if (ax > 80),
if (x > 0), bp=0.; bn=x; return;
else,
bp=-x; bn=0.; return; end;
end;
if (ax > xlim),
bp=x/(exp(x)-1); bn=x+bp; return;
else
ii=1; fp=1.;fn=1.; df=1.; s=1.;
while (abs(df) > eps),
ii=ii+1; s=-s; df=df*x/ii;
fp=fp+df; fn=fn+s*df;
bp=1./fp; bn=1./fn;
end;
return;
end
Example 12.3 We use Program 94 supplied with Programs 97 and 98 for the
numerical approximation of problem (12.70) in the case ε = 10−2. Figure 12.11
shows the convergence behavior as a function of log(h) of the Galerkin method
without (G) and with numerical viscosity (upwind (UP) and SG methods are
employed). The ﬁgure shows the logarithmic plots of the L2(0, 1) and H1(0, 1)-
norms, where the solid line denotes the UP method and the dashed and dotted
lines denote the G and SG methods, respectively. It is interesting to notice that
the UP and SG schemes exhibit the same (linear) convergence rate as the pure
Galerkin method in the H1-norm, while the accuracy of the UP scheme in the
L2-norm deteriorates dramatically because of the eﬀect of the artiﬁcial viscosity
which is O(h). Conversely, the SG converges quadratically since the introduced
numerical viscosity is in this case O(h2) as h tends to zero.
•

572
12. Two-Point Boundary Value Problems
10−3
10−2
10−1
10−4
10−3
10−2
10−1
100
101
102
||u − u h ||
L2(0,1)
||u − u h ||
H1(0,1)
FIGURE 12.11. Convergence analysis for an advection-diﬀusion problem
12.6
A Quick Glance to the Two-Dimensional Case
The game that we want to play is to extend (in a few pages) the basic ideas
illustrated so far to the two-dimensional case. The obvious generalization of
problem (12.1)-(12.2) is the celebrated Poisson problem with homogeneous
Dirichlet boundary condition
" −△u = f
in Ω,
u = 0
on ∂Ω,
(12.90)
where △u = ∂2u/∂x2 + ∂2u/∂y2 is the Laplace operator and Ωis a two-
dimensional bounded domain whose boundary is ∂Ω. If we allow Ωto be
the unit square Ω= (0, 1)2, the ﬁnite diﬀerence approximation of (12.90)
that mimics (12.10) is
" Lhuh(xi,j) = f(xi,j)
for i, j = 1, . . . , N −1,
uh(xi,j) = 0
if i = 0 or N,
j = 0 or N,
(12.91)
where xi,j = (ih, jh) (h = 1/N > 0) are the grid points and uh is a grid
function. Finally, Lh denotes any consistent approximation of the operator
L = −△. The classical choice is
Lhuh(xi,j) = 1
h2 (4ui,j −ui+1,j −ui−1,j −ui,j+1 −ui,j−1) ,
(12.92)
where ui,j = uh(xi,j), which amounts to adopting the second-order centred
discretization of the second derivative (10.65) in both x and y directions
(see Figure 12.12, left).
The resulting scheme is known as the ﬁve-point discretization of the
Laplacian. It is an easy (and useful) exercise for the reader to check that
the associated matrix Afd has (N −1)2 rows and is pentadiagonal, with the

12.6 A Quick Glance to the Two-Dimensional Case
573
xij xi,j+1
xi+1,j
xi,j−1
xi−1,j
FIGURE 12.12. Left: ﬁnite diﬀerence grid and stencil for a squared domain.
Right: ﬁnite element triangulation of a region around a hole
i-th row given by
(afd)ij = 1
h2







4
if j = i,
−1
if j = i −N −1, i −1, i + 1, i + N + 1,
0
otherwise.
(12.93)
Moreover, Afd is symmetric positive deﬁnite and it is also an M-matrix (see
Exercise 14). As expected, the consistency error associated with (12.92) is
second-order with respect to h, and the same holds for the discretization
error ∥u −uh∥h,∞of the method. More general boundary conditions than
the one considered in (12.90) can be dealt with by properly extending to
the two-dimensional case the mirror imaging technique described in Section
12.2.3 and in Exercise 10 (for a thorough discussion of this subject, see, e.g.,
[Smi85]).
The extension of the Galerkin method is (formally speaking) even more
straightforward and actually is still readable as (12.43) with, however, the
implicit understanding that both the function space Vh and the bilinear
form a(·, ·) be adapted to the problem at hand. The ﬁnite element method
corresponds to taking
Vh =

vh ∈C0(Ω) : vh|T ∈Pk(T) ∀T ∈Th, vh|∂Ω= 0

,
(12.94)
where Th denotes here a triangulation of the domain Ωas previously intro-
duced in Section 8.5.2, while Pk (k ≥1) is the space of piecewise polyno-
mials deﬁned in (8.35). Note that Ωneeds not to be a rectangular domain
(see Figure 12.12, right).

574
12. Two-Point Boundary Value Problems
As of the bilinear form a(·, ·), the same kind of mathematical manipula-
tions performed in Section 12.4.1 lead to
a(uh, vh) =
>
Ω
∇uh · ∇vh dxdy,
where we have used the following Green’s formula that generalizes the
formula of integration by parts
>
Ω
−△u v dxdy =
>
Ω
∇u · ∇v dxdy −
>
∂Ω
∇u · n v dγ,
(12.95)
for any u, v smooth enough and where n is the outward normal unit vector
on ∂Ω(see Exercise 15).
The error analysis for the two-dimensional ﬁnite element approximation
of (12.90) can still be performed through the combined use of Ce`a’s lemma
and interpolation error estimates as in Section 12.4.5 and is summarized in
the following result, which is the two-dimensional counterpart of Property
12.1 (for its proof we refer, e.g., to [QV94], Theorem 6.2.1).
Property 12.2 Let u ∈H1
0(Ω) be the exact solution of (12.90) and uh ∈Vh
be its ﬁnite element approximation using continuous piecewise polynomials
of degree k ≥1. Assume also that u ∈Hs(Ω) for some s ≥2. Then the
following error estimate holds
∥u −uh∥H1
0(Ω) ≤M
α0
Chl∥u∥Hl+1(Ω)
(12.96)
where l = min(k, s −1). Under the same assumptions, one can also prove
that
∥u −uh∥L2(Ω) ≤Chl+1∥u∥Hl+1(Ω).
(12.97)
We notice that, for any integer s ≥0, the Sobolev space Hs(Ω) introduced
above is deﬁned as the space of functions with the ﬁrst s partial derivatives
(in the distributional sense) belonging to L2(Ω). Moreover, H1
0(Ω) is the
space of functions of H1(Ω) such that u = 0 on ∂Ω. The precise mathemat-
ical meaning of this latter statement has to be carefully addressed since,
for instance, a function belonging to H1
0(Ω) does not necessarily mean to
be continuous everywhere. For a comprehensive presentation and analysis
of Sobolev spaces we refer to [Ada75] and [LM68].
Following the same procedure as in Section 12.4.3, we can write the ﬁnite
element solution uh as
uh(x, y) =
N

j=1
ujϕj(x, y)

12.7 Applications
575
where {ϕj}N
j=1 is a basis for Vh. An example of such a basis in the case
k = 1 is provided by the so-called hat functions introduced in Section 8.5.2
(see Figure 8.7, right). The Galerkin ﬁnite element method leads to the
solution of the linear system Afeu = f, where (afe)ij = a(ϕj, ϕi).
Exactly as happens in the one-dimensional case, the matrix Afe is sym-
metric positive deﬁnite and, in general, sparse, the sparsity pattern being
strongly dependent on the topology of Th and the numbering of its nodes.
Moreover, the spectral condition number of Afe is still O(h−2), which im-
plies that solving iteratively the linear system demands for the use of a
suitable preconditioner (see Section 4.3.2). If, instead, a direct solver is
used, one should resort to a suitable renumbering procedure, as explained
in Section 3.9.
12.7
Applications
In this section we employ the ﬁnite element method for the numerical
approximation of two problems arising in ﬂuid mechanics and in the sim-
ulation of biological systems.
12.7.1
Lubrication of a Slider
Let us consider a rigid slider moving in the direction x along a physical
support from which it is separated by a thin layer of a viscous ﬂuid (which
is the lubricant). Suppose that the slider, of length L, moves with a ve-
locity U with respect to the plane support which is supposed to have an
inﬁnite length. The surface of the slider that is faced towards the support
is described by the function s (see Figure 12.13, left).
Denoting by µ the viscosity of the lubricant, the pressure p acting on the
slider can be modeled by the following Dirichlet problem





−
 s3
6µp′
′
= −(Us)′
x ∈(0, L),
p(0) = 0,
p(L) = 0.
Assume in the numerical computations that we are working with a conver-
gent-divergent slider of unit length, whose surface is s(x) = 1−3/2x+9/8x2
with µ = 1.
Figure 12.13 (right) shows the solution obtained using linear and quadratic
ﬁnite elements with an uniform grid size h = 0.2. The linear system has
been solved by the nonpreconditioned conjugate gradient method. To re-
duce the Euclidean norm of the residual below 10−10, 4 iterations are
needed in the case of linear ﬁnite elements while 9 are required for quadratic
ﬁnite elements.

576
12. Two-Point Boundary Value Problems
L
s(x)
U
x
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
−0.01
0
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0.09
FIGURE 12.13. Left: geometrical parameters of the slider considered in Section
12.7.1; right: pressure on a converging-diverging slider. The solid line denotes
the solution obtained used linear ﬁnite elements (the symbol ◦denotes the nodal
values), while the dashed line denotes the solution obtained using quadratic ﬁnite
elements
Table 12.2 reports a numerical study of the condition number K2(Afe) as
a function of h. In the case of linear ﬁnite elements we have denoted the
matrix by A1, while A2 is the corresponding matrix for quadratic elements.
Here we assume that the condition number approaches h−p as h tends to
zero; the numbers pi are the estimated values of p. As can be seen, in
both cases the condition number grows like h−2, however, for every ﬁxed
h, K2(A2) is much bigger than K2(A1).
h
K2(A1)
p1
K2(A2)
p2
0.10000
63.951
–
455.24
0.05000
348.21
2.444
2225.7
2.28
0.02500
1703.7
2.290
10173.3
2.19
0.01250
7744.6
2.184
44329.6
2.12
0.00625
33579
2.116
187195.2
2.07
TABLE 12.2. Condition number of the stiﬀness matrix for linear and quadratic
ﬁnite elements
12.7.2
Vertical Distribution of Spore Concentration over
Wide Regions
In this section we are concerned with diﬀusion and transport processes of
spores in the air, such as the endspores of bacteria and the pollen of ﬂow-
ering plants. In particular, we study the vertical concentration distribution
of spores and pollen grains over a wide area. These spores, in addition to
settling under the inﬂuence of gravity, diﬀuse passively in the atmosphere.

12.7 Applications
577
The basic model assumes the diﬀusivity ν and the settling velocity β to
be given constants, the averaging procedure incorporating various physical
processes such as small-scale convection and horizontal advection-diﬀusion
which can be neglected over a wide horizontal area. Denoting by x ≥0 the
vertical direction, the steady-state distribution u(x) of the spore concen-
tration is the solution of
"
−νu′′ + βu′ = 0
0 < x < H,
u(0) = u0,
−νu′(H) + βu(H) = 0,
(12.98)
where H is a ﬁxed height at which we assume a vanishing Neumann condi-
tion for the advective-diﬀusive ﬂux −νu′+βu (see Section 12.4.1). Realistic
values of the coeﬃcients are ν = 10 m2s−1 and β = −0.03 ms−1; as for
u0, a reference concentration of 1 pollen grain per m3 has been used in the
numerical experiments, while the height H has been set equal to 10 km.
The global P´eclet number is therefore Pegl = 15.
The Galerkin ﬁnite element method with piecewise linear ﬁnite elements
has been used for the approximation of (12.98). Figure 12.14 (left) shows
the solution computed by running Program 94 on a uniform grid with step-
size h = H/10. The solution obtained using the (non stabilized) Galerkin
formulation (G) is denoted by the solid line, while the dash-dotted and
dashed lines refer to the Scharfetter-Gummel (SG) and upwind (UP) sta-
bilized methods. Spurious oscillations can be noticed in the G solution while
the one obtained using UP is clearly overdiﬀused with respect to the SG
solution that is nodally exact. The local P´eclet number is equal to 1.5 in
this case. Taking h = H/100 yields for the pure Galerkin scheme a stable
result as shown in Figure 12.14 (right) where the solutions furnished by G
(solid line) and UP (dashed line) are compared.
0
2000
4000
6000
8000
10000
−0.2
0
0.2
0.4
0.6
0.8
1
1.2
0
200
400
600
800
1000
1200
1400
1600
1800
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
FIGURE 12.14. Vertical concentration of spores: G, SG and UP solutions with
h = H/10 (left) and h = H/100 (right, where only the portion [0, 2000] is shown).
The x-axis represents the vertical coordinate

578
12. Two-Point Boundary Value Problems
12.8
Exercises
1. Consider the boundary value problem (12.1)-(12.2) with f(x) = 1/x. Using
(12.3) prove that u(x) = −x log(x). This shows that u ∈C2(0, 1) but u(0)
is not deﬁned and u′, u′′ do not exist at x = 0 (⇒: if f ∈C0(0, 1), but not
f ∈C0([0, 1]), then u does not belong to C0([0, 1])).
2. Prove that the matrix Afd introduced in (12.8) is an M-matrix.
[Hint: check that Afdx ≥0 ⇒x ≥0. To do this, for any α > 0 set Afd,α =
Afd+αIn−1. Then, compute w = Afd,αx and prove that min1≤i≤(n−1) wi ≥
0. Finally, since the matrix Afd,α is invertible, being symmetric and positive
deﬁnite, and since the entries of A−1
fd,α are continuous functions of α ≥0,
one concludes that A−1
fd,α is a nonnegative matrix as α →0.]
3. Prove that (12.13) deﬁnes a norm for V 0
h .
4. Prove (12.15) by induction on m.
5. Prove the estimate (12.23).
[Hint: for each internal node xj, j = 1, . . . , n−1, integrate by parts (12.21)
to get
τh(xj)
= −u′′(xj) −1
h2


xj
>
xj−h
u′′(t)(xj −h −t)2 dt −
xj+h
>
xj
u′′(t)(xj + h −t)2 dt

.
Then, pass to the squares and sum τh(xj)2 for j = 1, . . . , n −1. On noting
that (a+b+c)2 ≤3(a2 +b2 +c2), for any real numbers a, b, c, and applying
the Cauchy-Schwarz inequality yields the desired result.]
6. Prove that Gk(xj) = hG(xj, xk), where G is Green’s function introduced in
(12.4) and Gk is its corresponding discrete counterpart solution of (12.4).
[Solution: we prove the result by verifying that LhG = hek. Indeed, for a
ﬁxed xk the function G(xk, s) is a straight line on the intervals [0, xk] and
[xk, 1] so that LhG = 0 at every node xl with l = 0, . . . , k −1 and l =
k+1, . . . , n+1. Finally, a direct computation shows that (LhG)(xk) = 1/h
which concludes the proof.]
7. Let g = 1 and prove that Thg(xj) = 1
2xj(1 −xj).
[Solution: use the deﬁnition (12.25) with g(xk) = 1, k = 1, . . . , n −1 and
recall that Gk(xj) = hG(xj, xk) from the exercise above. Then
Thg(xj) = h


j

k=1
xk(1 −xj) +
n−1

k=j+1
xj(1 −xk)


from which, after straightforward computations, one gets the desired re-
sult.]
8. Prove Young’s inequality (12.40).
9. Show that ∥vh∥h ≤∥vh∥h,∞∀vh ∈Vh.

12.8 Exercises
579
10. Consider the two-point boundary value problem (12.29) with the following
boundary conditions
λ0u(0) + µ0u(0) = g0,
λ1u(1) + µ1u(1) = g1,
where λj, µj and gj are given data (j = 0, 1). Using the mirror imaging
technique described in Section 12.2.3 write the ﬁnite diﬀerence discretiza-
tion of the equations corresponding to the nodes x0 and xn.
[Solution:
node x0 :
 α1/2
h2
+ γ0
2 + α0λ0
µ0h

u0 −α1/2
u1
h2 = α0g0
µ0h + f0
2 ,
node xn :
 αn−1/2
h2
+ γn
2 + αnλ1
µ1h

un −αn−1/2
un−1
h2
= αng1
µ1h + fn
2 .]
11. Discretize the fourth-order diﬀerential operator Lu(x) = −u(iv)(x) using
centred ﬁnite diﬀerences.
[Solution : apply twice the second order centred ﬁnite diﬀerence operator
Lh deﬁned in (12.9).]
12. Consider problem (12.41) with non homogeneous Neumann boundary con-
ditions αu′(0) = w0, αu′(1) = w1. Show that the solution satisﬁes prob-
lem (12.43) where V = H1(0, 1) and the right-hand side is replaced by
(f, v)+w1v(1)−w0v(0). Derive the formulation in the case of mixed bound-
ary conditions αu′(0) = w0, u(1) = u1.
13. Using Property 1.19 prove that the matrices corresponding to the stabilized
ﬁnite element method (12.79) using the upwind and SG artiﬁcial viscosities
φUP and φSG (see Section 12.5.2) are M-matrices irrespective of h.
[Hint: let us denote respectively by AUP and ASG the two stiﬀness matrices
corrsponding to φUP and φSG. Take v(x) = 1 + x and set vi = 1 + xi,
i = 0, . . . , n, being xi = ih, h = 1/n. Then, by a direct computation check
that (AUP v)i ≥β > 0. As for the matrix ASG the same result can be
proved by noting that B(−t) = t + B(t) for any t ∈R.]
14. Prove that the matrix Afd with entries given by (12.93) is symmetric pos-
itive deﬁnite and it is also an M-matrix.
[Solution: to show that Afd is positive deﬁnite, proceed as in the corre-
sponding proof in Section 12.2, then proceed as in Exercise 2.]
15. Prove the Green’s formula (12.95).
[Solution: ﬁrst, notice that for any u, v suﬃciently smooth, div(v∇u) =
v△u+∇u·∇v. Then, integrate this relation over Ωand use the divergence
theorem
>
Ω
div(v∇u) dxdy =
>
∂Ω
∂u
∂nv dγ.]

13
Parabolic and Hyperbolic Initial
Boundary Value Problems
The ﬁnal chapter of this book is devoted to the approximation of time-
dependent partial diﬀerential equations. Parabolic and hyperbolic initial-
boundary value problems will be addressed and either ﬁnite diﬀerences and
ﬁnite elements will be considered for their discretization.
13.1
The Heat Equation
The problem we are considering is how to ﬁnd a function u = u(x, t) for
x ∈[0, 1] and t > 0 that satisﬁes the partial diﬀerential equation
∂u
∂t + Lu = f
0 < x < 1, t > 0,
(13.1)
subject to the boundary conditions
u(0, t) = u(1, t) = 0,
t > 0
(13.2)
and the initial condition
u(x, 0) = u0(x)
0 ≤x ≤1.
(13.3)
The diﬀerential operator L is deﬁned as
Lu = −ν ∂2u
∂x2 .
(13.4)

582
13. Parabolic and Hyperbolic Initial Boundary Value Problems
Equation (13.1) is called the heat equation. In fact, u(x, t) describes the
temperature at the point x and time t of a metallic bar of unit length
that occupies the interval [0, 1], under the following conditions. Its thermal
conductivity is constant and equal to ν > 0, its extrema are kept at a
constant temperature of zero degrees, at time t = 0 its temperature at
point x is described by u0(x), and f(x, t) represents the heat production
per unit length supplied at point x at time t. Here we are supposing that
the volumetric density ρ and the speciﬁc heat per unit mass cp are both
constant and unitary. Otherwise, the temporal derivative ∂u/∂t should be
multiplied by the product ρcp in (13.1).
A solution of problem (13.1)-(13.3) is provided by a Fourier series. For
instance, if ν = 1 and f ≡0, it is given by
u(x, t) =
∞

n=1
cne−(nπ)2t sin(nπx)
(13.5)
where the coeﬃcients cn are the Fourier sine coeﬃcients of the initial datum
u0(x), i.e.
cn = 2
1
>
0
u0(x) sin(nπx) dx,
n = 1, 2 . . .
If instead of (13.2) we consider the Neumann conditions
ux(0, t) = ux(1, t) = 0,
t > 0,
(13.6)
the corresponding solution (still in the case where f ≡0 and ν = 1) would
be
u(x, t) = d0
2 +
∞

n=1
dne−(nπ)2t cos(nπx),
where the coeﬃcients dn are the Fourier cosine coeﬃcients of u0(x), i.e.
dn = 2
1
>
0
u0(x) cos(nπx) dx,
n = 1, 2 . . .
These expressions show that the solution decays exponentially fast in time.
A more general result can be stated concerning the behavior in time of the
energy
E(t) =
1
>
0
u2(x, t) dx.

13.1 The Heat Equation
583
Indeed, if we multiply (13.1) by u and integrate with respect to x over the
interval [0, 1], we obtain
1
>
0
∂u
∂t (x, t)u(x, t) dx
−
ν
1
>
0
∂2u
∂x2 (x, t)u(x, t) dx = 1
2
1
>
0
∂u2
∂t (x, t) dx
+
ν
1
>
0
∂u
∂x
2
(x, t) dx −ν
∂u
∂x(x, t)u(x, t)
x=1
x=0
=
1
2E′(t) + ν
1
>
0
∂u
∂x(x, t)
2
dx,
having used integration by parts, the boundary conditions (13.2) or (13.6),
and interchanged diﬀerentiation and integration.
Using the Cauchy-Schwarz inequality (8.29) yields
1
>
0
f(x, t)u(x, t) dx ≤(F(t))1/2(E(t))1/2
where F(t) =
 1
0 f 2(x, t) dx. Then
E′(t) + 2ν
1
>
0
∂u
∂x(x, t)
2
dx ≤2(F(t))1/2(E(t))1/2.
Owing to the Poincar´e inequality (12.16) with (a, b) = (0, 1) we obtain
E′(t) + 2
ν
(CP )2 E(t) ≤2(F(t))1/2(E(t))1/2.
By Young’s inequality (12.40) we have
2(F(t))1/2(E(t))1/2 ≤γE(t) + 1
γ F(t),
having set γ = ν/C2
P . Therefore, E′(t) + γE(t) ≤1
γ F(t), or, equivalently,
(eγtE(t))′ ≤1
γ eγtF(t). Then, integrating from 0 to t we get
E(t) ≤e−γtE(0) + 1
γ
t
>
0
eγ(s−t)F(s)ds.
(13.7)
In particular, when f ≡0, (13.7) shows that the energy E(t) decays expo-
nentially fast in time.

584
13. Parabolic and Hyperbolic Initial Boundary Value Problems
13.2
Finite Diﬀerence Approximation of the Heat
Equation
To solve the heat equation numerically we have to discretize both the x
and t variables. We can start by dealing with the x-variable, following the
same approach as in Section 12.2. We denote by ui(t) an approximation of
u(xi, t), i = 0, . . . , n and approximate the Dirichlet problem (13.1)-(13.3)
by the scheme
.ui (t) −ν
h2 (ui−1(t) −2ui(t) + ui+1(t)) = fi(t),
i = 1, . . . , n −1, ∀t > 0,
u0(t) = un(t) = 0,
∀t > 0,
ui(0) = u0(xi),
i = 0, . . . , n,
where the upper dot indicates derivation with respect to time, and fi(t) =
f(xi, t). This is actually a semi-discretization of problem (13.1)-(13.3), and
is a system of ordinary diﬀerential equations of the following form
" ˙u(t) = −νAfdu(t) + f(t),
∀t > 0,
u(0) = u0
(13.8)
where u(t) = (u1(t), . . . , un−1(t))T is the vector of unknowns, f(t) =
(f1(t), . . . , fn−1(t))T , u0 = (u0(x1), . . . , u0(xn−1))T and Afd is the tridi-
agonal matrix introduced in (12.8). Note that for the derivation of (13.8)
we have assumed that u0(x0) = u0(xn) = 0, which is coherent with the
boundary condition (13.2).
A popular scheme for the integration of (13.8) with respect to time is the
so-called θ−method. To construct the scheme, we denote by vk the value
of the variable v at time tk = k∆t, for ∆t > 0; then, the θ-method for the
time-integration of (13.8) is







uk+1 −uk
∆t
= −νAfd(θuk+1 + (1 −θ)uk) + θf k+1 + (1 −θ)f k,
k = 0, 1, . . .
u0 = u0
(13.9)
or, equivalently,
(I + νθ∆tAfd) uk+1 = (I −ν∆t(1 −θ)Afd) uk + gk+1,
(13.10)
where gk+1 = ∆t(θf k+1 + (1 −θ)f k) and I is the identity matrix of order
n −1.
For suitable values of the parameter θ, from (13.10) we can recover some
familiar methods that have been introduced in Chapter 11. For example, if
θ = 0 the method (13.10) coincides with the forward Euler scheme and we

13.2 Finite Diﬀerence Approximation of the Heat Equation
585
can get uk+1 explicitly; otherwise, a linear system (with constant matrix
I + νθ∆tAfd) needs be solved at each time-step.
Regarding stability, assume that f ≡0 (henceforth gk = 0 ∀k > 0),
so that from (13.5) the exact solution u(x, t) tends to zero for every x
as t →∞. Then we would expect the discrete solution to have the same
behaviour, in which case we would call our scheme (13.10) asymptotically
stable, this being coherent with what we did in Chapter 11, Section 11.1
for ordinary diﬀerential equations.
If θ = 0, from (13.10) it follows that
uk = (I −ν∆tAfd)ku0,
k = 1, 2, . . .
From the analysis of convergent matrices (see Section 1.11.2) we deduce
that uk →0 as k →∞iﬀ
ρ(I −ν∆tAfd) < 1.
(13.11)
On the other hand, the eigenvalues of Afd are given by (see Exercise 3)
µi = 4
h2 sin2(iπh/2),
i = 1, . . . , n −1.
Then (13.11) is satisﬁed iﬀ
∆t < 1
2ν h2.
As expected, the forward Euler method is conditionally stable, and the
time-step ∆t should decay as the square of the grid spacing h.
In the case of the backward Euler method (θ = 1), we would have from
(13.10)
uk =
0
(I + ν∆tAfd)−11k u0,
k = 1, 2, . . .
Since all the eigenvalues of the matrix (I+ν∆tAfd)−1 are real, positive and
strictly less than 1 for every value of ∆t, this scheme is unconditionally
stable. More generally, the θ-scheme is unconditionally stable for all the
values 1/2 ≤θ ≤1, and conditionally stable if 0 ≤θ < 1/2 (see Section
13.3.1).
As far as the accuracy of the θ-method is concerned, its local truncation
error is of the order of ∆t+h2 if θ ̸= 1
2 while it is of the order of ∆t2 +h2 if
θ = 1
2. The method corresponding to θ = 1/2 is frequently called the Crank-
Nicolson scheme and is therefore unconditionally stable and second-order
accurate with respect to both ∆t and h.

586
13. Parabolic and Hyperbolic Initial Boundary Value Problems
13.3
Finite Element Approximation of the Heat
Equation
The space discretization of (13.1) can also be accomplished using the Galerkin
ﬁnite element method by proceeding as in Chapter 12 in the elliptic case.
First, for all t > 0 we multiply (13.1) by a test function v = v(x) and
integrate over (0, 1). Then, we let V = H1
0(0, 1) and ∀t > 0 we look for a
function t →u(x, t) ∈V (brieﬂy, u(t) ∈V ) such that
1
>
0
∂u(t)
∂t vdx + a(u(t), v) = F(v)
∀v ∈V,
(13.12)
with u(0) = u0. Here, a(u(t), v) =
 1
0 ν(∂u(t)/∂x) (∂v/∂x) dx and F(v) =
 1
0 f(t)vdx are the bilinear form and the linear functional respectively as-
sociated with the elliptic operator L and the right hand side f. Notice that
a(·, ·) is a special case of (12.44) and that the dependence of u and f on
the space variable x will be understood henceforth.
Let Vh be a suitable ﬁnite dimensional subspace of V . We consider the
following Galerkin formulation: ∀t > 0, ﬁnd uh(t) ∈Vh such that
1
>
0
∂uh(t)
∂t
vhdx + a(uh(t), vh) = F(vh)
∀vh ∈Vh
(13.13)
where uh(0) = u0h and u0h ∈Vh is a convenient approximation of u0.
Problem (13.13) is referred to as a semi-discretization of (13.12) since it is
only a space discretization of the heat equation.
Proceeding in a manner similar to that used to obtain the energy estimate
(13.7), we get the following a priori estimate for the discrete solution uh(t)
of (13.13)
Eh(t) ≤e−γtEh(0) + 1
γ
t
>
0
eγ(s−t)F(s)ds,
where Eh(t) =
 1
0 u2
h(x, t) dx.
As for the ﬁnite element discretization of (13.13), we introduce the ﬁnite
element space Vh deﬁned in (12.57) and consequently a basis {ϕj} for Vh
as already done in Section 12.4.5. Then, the solution uh of (13.13) can be
sought under the form
uh(t) =
Nh

j=1
uj(t)ϕj,

13.3 Finite Element Approximation of the Heat Equation
587
where {uj(t)} are the unknown coeﬃcients and Nh is the dimension of Vh.
Then, from (13.13) we obtain
1
>
0
Nh

j=1.uj (t)ϕjϕidx + a


Nh

j=1
uj(t)ϕj, ϕi

= F(ϕi),
i = 1, . . . , Nh
that is,
Nh

j=1.uj (t)
1
>
0
ϕjϕidx +
Nh

j=1
uj(t)a(ϕj, ϕi) = F(ϕi),
i = 1, . . . , Nh.
Using the same notation as in (13.8) we obtain
M.u(t) + Afeu(t) = ffe(t)
(13.14)
where Afe = (a(ϕj, ϕi)), ffe(t) = (F(ϕi)) and M = (mij) = (
 1
0 ϕjϕidx) for
i, j = 1, . . . , Nh. M is called the mass matrix. Since it is nonsingular, the
system of ODEs (13.14) can be written in normal form as
.u(t) = −M−1Afeu(t) + M−1ffe(t).
(13.15)
To solve (13.15) approximately we can still apply the θ-method and obtain
Muk+1 −uk
∆t
+ Afe
0
θuk+1 + (1 −θ)uk1
= θf k+1
fe
+ (1 −θ)f k
fe.
(13.16)
As usual, the upper index k means that the quantity at hand is computed at
time tk. As in the ﬁnite diﬀerence case, for θ = 0, 1 and 1/2 we respectively
obtain the forward Euler, backward Euler and Crank-Nicolson methods,
where the Crank-Nicolson method is the only one which is second-order
accurate with respect to ∆t.
For each k, (13.16) is a linear system whose matrix is
K = 1
∆tM + θAfe.
Since M and Afe are symmetric and positive deﬁnite, the matrix K is also
symmetric and positive deﬁnite. Thus, its Cholesky decomposition K =
HT H where H is upper triangular (see Section 3.4.2) can be carried out at
t = 0. Consequently, at each time step the following two linear triangular
systems, each of size equal to Nh, must be solved, with a computational
cost of N 2
h/2 ﬂops



HT y =
 1
∆tM −(1 −θ)Afe

uk + θf k+1
fe
+ (1 −θ)f k
fe,
Huk+1 = y.

588
13. Parabolic and Hyperbolic Initial Boundary Value Problems
When θ = 0, a suitable diagonalization of M would allow to decouple
the system equations (13.16). The procedure is carried out by the so-called
mass-lumping in which we approximate M by a nonsingular diagonal matrix
$M. In the case of piecewise linear ﬁnite elements $M can be obtained using
the composite trapezoidal formula over the nodes {xi} to evaluate the
integrals
 1
0 ϕjϕi dx, obtaining ˜mij = hδij, i, j = 1, . . . , Nh (see Exercise
2).
13.3.1
Stability Analysis of the θ-Method
Applying the θ-method to the Galerkin problem (13.13) yields

uk+1
h
−uk
h
∆t
, vh

+
a

θuk+1
h
+ (1 −θ)uk
h, vh

=
θF k+1(vh) + (1 −θ)F k(vh)
∀vh ∈Vh
(13.17)
for k ≥0 and with u0
h = u0h, F k(vh) =
 1
0 f(tk)vh(x)dx. Since we are
interested in the stability analysis, we can consider the special case where
F = 0; moreover, for the time being, we focus on the case θ = 1 (implicit
Euler scheme), i.e.

uk+1
h
−uk
h
∆t
, vh

+ a

uk+1
h
, vh

= 0
∀vh ∈Vh.
Letting vh = uk+1
h
, we get

uk+1
h
−uk
h
∆t
, uk+1
h

+ a(uk+1
h
, uk+1
h
) = 0.
From the deﬁnition of a(·, ·), it follows that
a

uk+1
h
, uk+1
h

= ν
!!!!!
∂uk+1
h
∂x
!!!!!
2
L2(0,1)
.
(13.18)
Moreover, we remark that (see Exercise 3 for the proof of this result)
∥uk+1
h
∥2
L2(0,1) + 2ν∆t
!!!!!
∂uk+1
h
∂x
!!!!!
2
L2(0,1)
≤∥uk
h∥2
L2(0,1).
(13.19)
It follows that, ∀n ≥1
n−1

k=0
∥uk+1
h
∥2
L2(0,1) + 2ν∆t
n−1

k=0
!!!!!
∂uk+1
h
∂x
!!!!!
2
L2(0,1)
≤
n−1

k=0
∥uk
h∥2
L2(0,1).

13.3 Finite Element Approximation of the Heat Equation
589
Since these are telescopic sums, we get
∥un
h∥2
L2(0,1) + 2ν∆t
n−1

k=0
!!!!!
∂uk+1
h
∂x
!!!!!
2
L2(0,1)
≤∥u0h∥2
L2(0,1),
(13.20)
which shows that the scheme is unconditionally stable. Proceeding similarly
if f ̸= 0, it can be shown that
∥un
h∥2
L2(0,1)
+
2ν∆t
n−1

k=0
!!!!!
∂uk+1
h
∂x
!!!!!
2
L2(0,1)
≤
C(n)

∥u0h∥2
L2(0,1) +
n

k=1
∆t∥f k∥2
L2(0,1)

,
(13.21)
where C(n) is a constant independent of both h and ∆t.
Remark 13.1 The same kind of stability inequalities (13.20) and (13.21)
can be obtained if a(·, ·) is a more general bilinear form provided that it is
continuous and coercive (see Exercise 4).
■
To carry out the stability analysis of the θ-method for every θ ∈[0, 1] we
need deﬁning the eigenvalues and eigenvectors of a bilinear form.
Deﬁnition 13.1 We say that λ is an eigenvalue and w ∈V is the associ-
ated eigenvector for the bilinear form a(·, ·) : V × V "→R if
a(w, v) = λ(w, v)
∀v ∈V,
where (·, ·) denotes the usual scalar product in L2(0, 1).
■
If the bilinear form a(·, ·) is symmetric and coercive, it has inﬁnitely many
real positive eigenvalues that form an unbounded sequence; moreover, its
eigenvectors (called also eigenfunctions) form a basis for the space V .
At a discrete level the corresponding pair λh ∈R, wh ∈Vh satisﬁes
a(wh, vh) = λh(wh, vh)
∀vh ∈Vh.
(13.22)
From the algebraic standpoint, problem (13.22) can be formulated as
Afew = λhMw
(where w is the vector of the gridvalues of wh) and can be regarded
as a generalized eigenvalue problem (see Section 5.9). All the eigenvalues
λ1
h, . . . , λNh
h
are positive. The corresponding eigenvectors w1
h, . . . , wNh
h
form

590
13. Parabolic and Hyperbolic Initial Boundary Value Problems
a basis for the subspace Vh and can be chosen in such a way as to be or-
thonormal, i.e., such that (wi
h, wj
h) = δij, ∀i, j = 1, . . . , Nh. In particular,
any function vh ∈Vh can be represented as
vh(x) =
Nh

j=1
vjwj
h(x).
Let us now assume that θ ∈[0, 1] and focus on the case where the bilinear
form a(·, ·) is symmetric. Although the ﬁnal stability result still holds in
the nonsymmetric case, the proof that follows cannot apply since in that
case the eigenvectors would no longer form a basis for Vh. Let

wi
h

be the
eigenvectors of a(·, ·) whose span forms an orthonormal basis for Vh. Since
at each time step uk
h ∈Vh, we can express uk
h as
uk
h(x) =
Nh

j=1
uk
j wj
h(x).
Letting F = 0 in (13.17) and taking vh = wi
h, we ﬁnd
1
∆t
Nh

j=1
0
uk+1
j
−uk
j
1 +
wj
h, wi
h
,
+
Nh

j=1
0
θuk+1
j
+ (1 −θ)uk
j
1
a(wj
h, wi
h) = 0,
i = 1, . . . , Nh.
Since wj
h are eigenfunctions of a(·, ·) we obtain
a(wj
h, wi
h) = λj
h(wj
h, wi
h) = λj
hδij = λi
h,
so that
uk+1
i
−uk
i
∆t
+
0
θuk+1
i
+ (1 −θ)uk
i
1
λi
h = 0.
Solving this equation with respect to uk+1
i
gives
uk+1
i
= uk
i
0
1 −(1 −θ)λi
h∆t
1
0
1 + θλi
h∆t
1
.
In order for the method to be unconditionally stable we must have (see
Chapter 11)

1 −(1 −θ)λi
h∆t
1 + θλi
h∆t
 < 1,
that is
2θ −1 > −
2
λi
h∆t.

13.3 Finite Element Approximation of the Heat Equation
591
If θ ≥1/2, this inequality is satisﬁed for any value of ∆t. Conversely, if
θ < 1/2 we must have
∆t <
2
(1 −2θ)λi
h
.
Since this relation must hold for all the eigenvalues λi
h of the bilinear form,
it suﬃces requiring that it is satisﬁed for the largest of them, which we
assume to be λNh
h .
We therefore conclude that if θ ≥1/2 the θ-method is unconditionally
stable (i.e., it is stable ∀∆t), whereas if 0 ≤θ < 1/2 the θ-method is stable
only if
∆t ≤
2
(1 −2θ)λNh
h
.
It can be shown that there exist two positive constants c1 and c2, indepen-
dent of h, such that
c1h−2 ≤λNh
h
= c2h−2
(see for the proof, [QV94], Section 6.3.2). Accounting for this, we obtain
that if 0 ≤θ < 1/2 the method is stable only if
∆t ≤C1(θ)h2,
(13.23)
for a suitable constant C1(θ) independent of both h and ∆t.
With an analogous proof, it can be shown that if a pseudo-spectral Galerkin
approximation is used for problem (13.12), the θ−method is uncondition-
ally stable if θ ≥1
2, while for 0 ≤θ < 1
2 stability holds only if
∆t ≤C2(θ)N −4,
(13.24)
for a suitable constant C2(θ) independent of both N and ∆t. The diﬀerence
between (13.23) and (13.24) is due to the fact that the largest eigenvalue
of the spectral stiﬀness matrix grows like O(N 4) with respect to the degree
of the approximating polynomial.
Comparing the solution of the globally discretized problem (13.17) with
that of the semi-discrete problem (13.13), by a suitable use of the stability
result (13.21) and of the truncation time discretization error, the following
convergence result can be proved
∥u(tk) −uk
h∥L2(0,1) ≤C(u0, f, u)(∆tp(θ) + hr+1),
∀k ≥1
where r denotes the piecewise polynomial degree of the ﬁnite element space
Vh, p(θ) = 1 if θ ̸= 1/2 while p(1/2) = 2 and C is a constant that depends
on its arguments (assuming that they are suﬃciently smooth) but not on

592
13. Parabolic and Hyperbolic Initial Boundary Value Problems
h and ∆t. In particular, if f ≡0 on can obtain the following improved
estimates
∥u(tk) −uk
h∥L2(0,1) ≤C
. h
√
tk
r+1
+
∆t
tk
p(θ)/
∥u0∥L2(0,1),
for k ≥1, θ = 1 or θ = 1/2. (For the proof of these results, see [QV94], pp.
394-395).
Program 100 provides an implementation of the θ-method for the solution
of the heat equation on the space-time domain (a, b) × (t0, T). The dis-
cretization in space is based on piecewise-linear ﬁnite elements. The input
parameters are: the column vector I containing the endpoints of the space
interval (a = I(1), b = I(2)) and of the time interval (t0 = I(3), T = I(4));
the column vector n containing the number of steps in space and time; the
macros u0 and f containing the functions u0h and f, the constant viscosity
nu, the Dirichlet boundary conditions bc(1) and bc(2), and the value of
the parameter theta.
Program 100 - thetameth : θ-method for the heat equation
function [u,x] = thetameth(I,n,u0,f,bc,nu,theta)
nx = n(1); h = (I(2)-I(1))/nx;
x = [I(1):h:I(2)]; t = I(3);
uold = (eval(u0))’;
nt = n(2); k = (I(4)-I(3))/nt; e = ones(nx+1,1);
K = spdiags([(h/(6*k)-nu*theta/h)*e, (2*h/(3*k)+2*nu*theta/h)*e, ...
(h/(6*k)-nu*theta/h)*e],-1:1,nx+1,nx+1);
B = spdiags([(h/(6*k)+nu*(1-theta)/h)*e, (2*h/(3*k)-nu*2*(1-theta)/h)*e, ...
(h/(6*k)+nu*(1-theta)/h)*e],-1:1,nx+1,nx+1);
K(1,1)
= 1; K(1,2)
= 0; B(1,1)
= 0; B(1,2)
= 0;
K(nx+1,nx+1) = 1; K(nx+1,nx) = 0; B(nx+1,nx+1) = 0; B(nx+1,nx) = 0;
[L,U]=lu(K);
t = I(3); x=[I(1)+h:h:I(2)-h]; fold = (eval(f))’;
fold = h*fold;
fold = [bc(1); fold; bc(2)];
for time = I(3)+k:k:I(4)
t = time;
fnew = (eval(f))’;
fnew = h*fnew;
fnew = [bc(1); fnew; bc(2)];
b = theta*fnew+(1-theta)*fold + B*uold;
y = L \ b;
u = U \ y;
uold = u;
end
x = [I(1):h:I(2)];
Example 13.1 Let us assess the time-accuracy of the θ-method in the solution
of the heat equation (13.1) on the space-time domain (0, 1) × (0, 1) where f is

13.4 Space-Time Finite Element Methods for the Heat Equation
593
chosen in such a way that the exact solution is u = sin(2πx) cos(2πt). A ﬁxed
spatial grid size h = 1/500 has been used while the time step ∆t is equal to
(10k)−1, k = 1, . . . , 4. Finally, piecewise ﬁnite elements are used for the space
discretization. Figure 13.1 shows the convergence behavior in the L2(0, 1) norm
(evaluated at time t = 1), as ∆t tends to zero, of the backward Euler method
(BE) (θ = 1, solid line) and of the Crank-Nicolson scheme (CN) (θ = 1/2, dashed
line). As expected, the CN method is far more accurate than the BE method. •
10
1
10
2
10
−7
10
−6
10
−5
10
−4
10
−3
10
−2
10
−1
FIGURE 13.1. Convergence analysis of the θ-method as a function of the number
1/∆t of time steps (represented on the x-axis): θ = 1 (solid line) and θ = 0.5
(dashed line)
13.4
Space-Time Finite Element Methods for the
Heat Equation
An alternative approach for time discretization is based on the use of a
Galerkin method to discretize both space and time variables.
Suppose to solve the heat equation for x ∈[0, 1] and t ∈[0, T]. Let us
denote by Ik = [tk−1, tk] the k-th time interval for k = 1, . . . , n with ∆tk =
tk −tk−1; moreover, we let ∆t = maxk ∆tk; the rectangle Sk = [0, 1]×Ik is
the so called space-time slab. At each time level tk, we consider a partition
Thk of (0, 1) into mk subintervals Kk
j = [xk
j , xk
j+1], j = 0, . . . , mk −1. We
let hk
j = xk
j+1 −xk
j and denote by hk = maxj hk
j and by h = maxk hk.
Let us now associate with Sk a space-time partition Sk = ∪mk
j=1Rk
j where
Rk
j = Kk
j × Ik and Kk
j ∈Thk. The space-time slab Sk is thus decomposed
into rectangles Rk
j (see Figure 13.2).
For each time slab Sk we introduce the space-time ﬁnite element space
Qq(Sk) =
2
v ∈C0(Sk), v|Rk
j ∈P1(Kk
j ) × Pq(Ik), j = 0, . . . , mk −1
3

594
13. Parabolic and Hyperbolic Initial Boundary Value Problems
tk−1
Rj
k
Sk
t
0
1 x
tk
FIGURE 13.2. Space-time ﬁnite element discretization
where, usually, q = 0 or q = 1. Then, the space-time ﬁnite element space
over [0, 1] × [0, T] is deﬁned as follows
Vh,∆t =

v : [0, 1] × [0, T] →R : v|Sk ∈Yh,k, k = 1, . . . , n

,
where
Yh,k = {v ∈Qq(Sk) : v(0, t) = v(1, t) = 0 ∀t ∈Ik} .
The number of degrees of freedom of Vh,∆t is equal to (q + 1)(mk −1).
The functions in Vh,∆t are linear and continuous in space while they are
piecewise polynomials of degree q in time. These functions are in general
discontinuous across the time levels tk and the partitions T k
h do not match
at the interface between contiguous time levels (see Figure 13.2). For this
reason, we adopt henceforth the following notation
vk
± = lim
τ→0v(tk ± τ),
[vk] = vk
+ −vk
−.
The discretization of problem (13.12) using continuous ﬁnite elements in
space of degree 1 and discontinuous ﬁnite elements of degree q in time
(abbreviated by cG(1)dG(q) method) is: ﬁnd U ∈Vh,∆t such that
n

k=1
>
Ik
∂U
∂t , V

+ a(U, V )

dt +
n−1

k=1
([U k], V k
+)
+(U 0
+, V 0
+) =
T
>
0
(f, V ) dt,
∀V ∈
0
V h,∆t,
where
0
V h,∆t = {v ∈Vh,∆t : v(0, t) = v(1, t) = 0 ∀t ∈[0, T]} ,

13.4 Space-Time Finite Element Methods for the Heat Equation
595
U 0
−= u0h, U k = U(x, tk) and (u, v) =
 1
0 uv dx denotes the scalar product
of L2(0, 1). The continuity of U at each point tk is therefore imposed only
in a weak sense.
To construct the algebraic equations for the unknown U we need expand-
ing it over a basis in time and in space. The single space-time basis func-
tion ϕk
jl(x, t) can be written as ϕk
jl(x, t) = ϕk
j (x)ψl(t), j = 1, . . . , mk −1,
l = 0, . . . , q, where ϕk
j is the usual piecewise linear basis function and ψl is
the l-th basis function of Pq(Ik).
When q = 0 the solution U is piecewise constant in time. In that case
U k(x, t) =
N k
h

j=1
U k
j ϕk
j (x), x ∈[0, 1], t ∈Ik,
where U k
j = U k(xj, t) ∀t ∈Ik. Let
Ak = (aij) = (a(ϕk
j , ϕk
i )),
Mk = (mij) = ((ϕk
j , ϕk
i )),
fk = (fi) =


>
Sk
f(x, t)ϕk
i (x)dx dt

,
Bk,k−1 = (bij) = ((ϕk
j , ϕk−1
i
)),
denote the stiﬀness matrix, the mass matrix, the data vector and the pro-
jection matrix between V k−1
h
and V k
h , respectively, at the time level tk.
Then, letting Uk = (U k
j ), at each k-th time level the cG(1)dG(0) method
requires solving the following linear system

Mk + ∆tkAk

Uk = Bk,k−1Uk−1 + fk,
which is nothing else than the Euler backward discretization scheme with
a modiﬁed right hand side.
When q = 1, the solution is piecewise linear in time. For ease of notation
we let U k(x) = U−(x, tk) and U k−1(x) = U+(x, tk−1). Moreover, we assume
that the spatial partition Thk does not change with the time level and we
let mk = m for every k = 0, . . . , n. Then, we can write
U|Sk = U k−1(x)tk −t
∆tk + U k(x)t −tk−1
∆tk
.
Thus the cG(1)dG(1) method leads to the solution of the following 2 ×
2 block-system in the unknowns Uk = (U k
i ) and Uk−1 = (U k−1
i
), i =
1, . . . , m −1








−1
2Mk + ∆tk
3 Ak

Uk−1 +
1
2Mk + ∆tk
6 Ak

Uk = fk−1 + Bk,k−1Uk−1
−
,
1
2Mk + ∆tk
6 Ak

Uk−1 +
1
2Mk + ∆tk
3 Ak

Uk = fk

596
13. Parabolic and Hyperbolic Initial Boundary Value Problems
where
fk−1 =
>
Sk
f(x, t)ϕk
i (x)ψk
1(t)dx dt,
fk =
>
Sk
f(x, t)ϕk
i (x)ψk
2(t)dx dt
and ψk
1(t) = (tk −t)/∆tk, ψk
2(t)(t −tk−1)/∆tk are the two basis functions
of P1(Ik).
Assuming that Vh,k−1 ̸⊂Vh,k, it is possible to prove that (see for the
proof [EEHJ96])
∥u(tn) −U n∥L2(0,1) ≤C(u0h, f, u, n)(∆t2 + h2),
(13.25)
where C is a constant that depends on its arguments (assuming that they
are suﬃciently smooth) but not on h and ∆t.
An advantage in using space-time ﬁnite elements is the possibility to
perform a space-time grid adaptivity on each time-slab based on a posteriori
error estimates (the interested reader is referred to [EEHJ96] where the
analysis of this method is carried out in detail).
Program 101 provides an implementation of the dG(1)cG(1) method for
the solution of the heat equation on the space-time domain (a, b) × (t0, T).
The input parameters are the same as in Program 100.
Program 101 - pardg1cg1 : dG(1)cG(1) method for the heat equation
function [u,x]=pardg1cg1(I,n,u0,f,nu,bc)
nx = n(1);
h = (I(2)-I(1))/nx;
x = [I(1):h:I(2)]; t = I(3);
um = (eval(u0))’;
nt = n(2);
k = (I(4)-I(3))/nt;
e = ones(nx+1,1);
Add = spdiags([(h/12-k*nu/(3*h))*e, (h/3+2*k*nu/(3*h))*e, ...
(h/12-k*nu/(3*h))*e],-1:1,nx+1,nx+1);
Aud = spdiags([(h/12-k*nu/(6*h))*e, (h/3+k*nu/(3*h))*e, ...
(h/12-k*nu/(6*h))*e],-1:1,nx+1,nx+1);
Ald = spdiags([(-h/12-k*nu/(6*h))*e, (-h/3+k*nu/(3*h))*e, ...
(-h/12-k*nu/(6*h))*e],-1:1,nx+1,nx+1);
B = spdiags([h*e/6, 2*h*e/3, h*e/6],-1:1,nx+1,nx+1);
Add(1,1) = 1; Add(1,2) = 0; B(1,1)
= 0; B(1,2)
= 0;
Aud(1,1) = 0; Aud(1,2) = 0; Ald(1,1) = 0; Ald(1,2) = 0;
Add(nx+1,nx+1)=1; Add(nx+1,nx)=0;
B(nx+1,nx+1)=0; B(nx+1,nx) = 0;
Ald(nx+1,nx+1)=0; Ald(nx+1,nx)=0;
Aud(nx+1,nx+1)=0; Aud(nx+1,nx)=0;
[L,U]=lu([Add Aud; Ald Add]);
x = [I(1)+h:h:I(2)-h]; xx=[I(1),x,I(2)];
for time = I(3)+k:k:I(4)

13.5 Hyperbolic Equations: A Scalar Transport Problem
597
t = time;
fq1 = 0.5*k*h*eval(f);
t = time-k;
fq0 = 0.5*k*h*eval(f);
rhs0 = [bc(1), fq0, bc(2)];
rhs1 = [bc(1), fq1, bc(2)];
b = [rhs0’; rhs1’] + [B*um; zeros(nx+1,1)];
y = L \ b;
u = U \ y; um = u(nx+2:2*nx+2,1);
end
x = [I(1):h:I(2)]; u = um;
Example 13.2 We assess the accuracy of the dG(1)cG(1) method on the same
test problem considered in Example 13.1. In order to neatly identify both spatial
and temporal contributions in the error estimate (13.25) we have performed the
numerical computations using Program 101 by varying either the time step or
the space discretization step only, having chosen in each case the discretization
step in the other variable suﬃciently small that the corresponding error can be
neglected. The convergence behavior in Figure 13.3 shows perfect agreement with
the theoretical results (second-order accuracy in both space and time).
•
10
1
10
2
10
3
10
−5
10
−4
10
−3
10
−2
10
−1
FIGURE 13.3. Convergence analysis for the dG(1)cG(1) method. The solid line
is the time discretization error while the dashed line is the space discretization
error. In the ﬁrst case the x-axis denotes the number of time steps while in second
case it represents the number of space subintervals
13.5
Hyperbolic Equations: A Scalar Transport
Problem
Let us consider the following scalar hyperbolic problem



∂u
∂t + a∂u
∂x = 0
x ∈R, t > 0,
u(x, 0) = u0(x)
x ∈R,
(13.26)

598
13. Parabolic and Hyperbolic Initial Boundary Value Problems
where a is a positive real number. Its solution is given by
u(x, t) = u0(x −at)
t ≥0,
and represents a travelling wave with velocity a. The curves (x(t), t) in the
plane (x, t), that satisfy the following scalar ordinary diﬀerential equation



dx(t)
dt
= a
t > 0
x(0) = x0,
(13.27)
are called characteristic curves. They are the straight lines x(t) = x0 + at,
t > 0. The solution of (13.26) remains constant along them since
du
dt = ∂u
∂t + ∂u
∂x
dx
dt = 0
on (x(t), t).
For the more general problem



∂u
∂t + a∂u
∂x + a0u = f
x ∈R, t > 0,
u(x, 0) = u0(x)
x ∈R,
(13.28)
where a, a0 and f are given functions of the variables (x, t), the charac-
teristic curves are still deﬁned as in (13.27). In this case, the solutions of
(13.28) satisfy along the characteristics the following diﬀerential equation
du
dt = f −a0u
on (x(t), t).
P0
Q
P
β
α
x
t
¯t
x
t
t = 1
1
0
FIGURE 13.4. Left: examples of characteristics which are straight lines issuing
from the points P and Q. Right: characteristic straight lines for the Burgers
equation
Let us now consider problem (13.26) on a bounded interval. For example,
assume that x ∈[α, β] and a > 0. Since u is constant along the character-
istics, from Figure 13.4 (left) we deduce that the value of the solution at P

13.6 Systems of Linear Hyperbolic Equations
599
attains the value of u0 at P0, the foot of the characteristic issuing from P.
On the other hand, the characteristic issuing from Q intersects the straight
line x(t) = α at a certain time t = ¯t > 0. Thus, the point x = α is an
inﬂow point and it is necessary to assign a boundary value for u at x = α
for every t > 0. Notice that if a < 0 then the inﬂow point is x = β.
Referring to problem (13.26) it is worth noting that if u0 is discontinuous
at a point x0, then such a discontinuity propagates along the characteristics
issuing from x0. This process can be rigorously formalized by introducing
the concept of weak solutions of hyperbolic problems, see e.g. [GR96]. An-
other reason for introducing weak solutions is that in the case of nonlinear
hyperbolic problems the characteristic lines can intersect: in this case the
solution cannot be continuous and no classical solution does exist.
Example 13.3 (Burgers equation) Let us consider the Burgers equation
∂u
∂t + u∂u
∂x = 0
x ∈R
(13.29)
which is perhaps the simplest nontrivial example of a nonlinear hyperbolic equa-
tion. Taking as initial condition
u(x, 0) = u0(x) =



1
x ≤0,
1 −x
0 ≤x ≤1,
0
x ≥1,
the characteristic line issuing from the point (x0, 0) is given by
x(t) = x0 + tu0(x0) =



x0 + t
x0 ≤0,
x0 + t(1 −x0)
0 ≤x0 ≤1,
x0
x0 ≥1.
Notice that the characteristic lines do not intersect only if t < 1 (see Figure 13.4,
right).
•
13.6
Systems of Linear Hyperbolic Equations
Consider the linear hyperbolic systems of the form
∂u
∂t + A∂u
∂x = 0
x ∈R, t > 0,
(13.30)
where u : R × [0, ∞) →Rp and A ∈Rp×p is a matrix with constant
coeﬃcients.
This system is said hyperbolic if A is diagonalizable and has real eigen-
values, that is, if there exists a nonsingular matrix T ∈Rp×p such that
A = TΛT−1,

600
13. Parabolic and Hyperbolic Initial Boundary Value Problems
where Λ = diag(λ1, ..., λp) is the diagonal matrix of the real eigenvalues of
A, while T = (ω1, ω2, . . . , ωp) is the matrix whose column vectors are the
right eigenvectors of A (see Section 1.7). The system is said to be strictly
hyperbolic if it is hyperbolic with distinct eigenvalues. Thus
Aωk = λkωk,
k = 1, . . . , p.
Introducing the characteristic variables w = T−1u, system (13.30) becomes
∂w
∂t + Λ∂w
∂x = 0.
This is a system of p independent scalar equations of the form
∂wk
∂t + λk
∂wk
∂x = 0,
k = 1, . . . , p.
Proceeding as in Section 13.5 we obtain wk(x, t) = wk(x−λkt, 0), and thus
the solution u = Tw of problem (13.30) can be written as
u(x, t) =
p

k=1
wk(x −λkt, 0)ωk.
The curve (xk(t), t) in the plane (x, t) that satisﬁes x′
k(t) = λk is the k-th
characteristic curve and wk is constant along it. A strictly hyperbolic sys-
tem enjoys the property that p distinct characteristic curves pass through
any point of the plane (x, t), for any ﬁxed x and t. Then u(x, t) depends
only on the initial datum at the points x−λkt. For this reason, the set of p
points that form the feet of the characteristics issuing from the point (x, t)
D(t, x) =

x ∈R : x = x −λkt , k = 1, ..., p

,
(13.31)
is called the domain of dependence of the solution u(x, t).
If (13.30) is set on a bounded interval (α, β) instead of on the whole real
line, the inﬂow point for each characteristic variable wk is determined by the
sign of λk. Correspondingly, the number of positive eigenvalues determines
the number of boundary conditions that can be assigned at x = α, whereas
at x = β it is admissible to assign a number of conditions which equals the
number of negative eigenvalues. An example is discussed in Section 13.6.1.
Remark 13.2 (The nonlinear case) Let us consider the following non-
linear system of ﬁrst-order equations
∂u
∂t + ∂
∂xg(u) = 0
(13.32)
where g = (g1, . . . , gp)T is called the ﬂux function. The system is hyperbolic
if the jacobian matrix A(u) whose elements are aij = ∂gi(u)/∂uj, i, j =
1, . . . , p, is diagonalizable and has p real eigenvalues.
■

13.6 Systems of Linear Hyperbolic Equations
601
13.6.1
The Wave Equation
Consider the second-order hyperbolic equation
∂2u
∂t2 −γ2 ∂2u
∂x2 = f
x ∈(α, β),
t > 0,
(13.33)
with initial data
u(x, 0) = u0(x)
and
∂u
∂t (x, 0) = v0(x),
x ∈(α, β),
and boundary data
u(α, t) = 0
and
u(β, t) = 0,
t > 0.
(13.34)
In this case, u may represent the transverse displacement of an elastic
vibrating string of length β−α, ﬁxed at the endpoints, and γ is a coeﬃcient
depending on the speciﬁc mass of the string and on its tension. The string
is subject to a vertical force of density f.
The functions u0(x) and v0(x) denote respectively the initial displace-
ment and the initial velocity of the string.
The change of variables
ω1 = ∂u
∂x,
ω2 = ∂u
∂t ,
transforms (13.33) into the following ﬁrst-order system
∂ω
∂t + A∂ω
∂x = 0
x ∈(α, β),
t > 0,
(13.35)
where
ω =

ω1
ω2

,
A =

0
−1
−γ2
0

,
and the initial conditions are ω1(x, 0) = u′
0(x) and ω2(x, 0) = v0(x).
Since the eigenvalues of A are the two distinct real numbers ±γ (rep-
resenting the propagation velocities of the wave) we conclude that system
(13.35) is hyperbolic. Moreover, one boundary condition needs to be pre-
scribed at every end-point, as in (13.34). Notice that, also in this case,
smooth solutions correspond to smooth initial data, while any discontinuity
that is present in the initial data will propagate along the characteristics.
Remark 13.3 Notice that replacing ∂2u
∂t2 by t2, ∂2u
∂x2 by x2 and f by 1, the
wave equation becomes
t2 −γ2x2 = 1

602
13. Parabolic and Hyperbolic Initial Boundary Value Problems
which represents an hyperbola in the (x, t) plane. Proceeding analogously
in the case of the heat equation (13.1), we end up with
t −νx2 = 1
which represents a parabola in the (x, t) plane. Finally, for the Poisson
equation (12.90), replacing ∂2u
∂x2 by x2, ∂2u
∂y2 by y2 and f by 1, we get
x2 + y2 = 1
which represents an ellipse in the (x, y) plane.
Due to the geometric interpretation above, the corresponding diﬀerential
operators are classiﬁed as hyperbolic, parabolic and elliptic.
■
13.7
The Finite Diﬀerence Method for Hyperbolic
Equations
Let us discretize the hyperbolic problem (13.26) by space-time ﬁnite dif-
ferences. To this aim, the half-plane {(x, t) : −∞< x < ∞, t > 0} is dis-
cretized by choosing a spatial grid size ∆x, a temporal step ∆t and the
grid points (xj, tn) as follows
xj = j∆x,
j ∈Z,
tn = n∆t,
n ∈N.
Let us set
λ = ∆t/∆x,
and deﬁne xj+1/2 = xj + ∆x/2. We look for discrete solutions un
j which
approximate the values u(xj, tn) of the exact solution for any j, n.
Quite often, explicit methods are employed for advancing in time in
hyperbolic initial-value problems, even though they require restrictions on
the value of λ, unlike what typically happens with implicit methods.
Let us focus our attention on problem (13.26). Any explicit ﬁnite-diﬀerence
method can be written in the form
un+1
j
= un
j −λ(hn
j+1/2 −hn
j−1/2),
(13.36)
where hn
j+1/2 = h(un
j , un
j+1) for every j and h(·, ·) is a particular function
that is called the numerical ﬂux.
13.7.1
Discretization of the Scalar Equation
We illustrate several instances of explicit methods, and provide the corre-
sponding numerical ﬂux.

13.7 The Finite Diﬀerence Method for Hyperbolic Equations
603
1. Forward Euler/centred
un+1
j
= un
j −λ
2 a(un
j+1 −un
j−1)
(13.37)
which can be cast in the form (13.36) by setting
hj+1/2 = 1
2a(uj+1 + uj).
(13.38)
2. Lax-Friedrichs
un+1
j
= 1
2(un
j+1 + un
j−1) −λ
2 a(un
j+1 −un
j−1)
(13.39)
which is of the form (13.36) with
hj+1/2 = 1
2[a(uj+1 + uj) −λ−1(uj+1 −uj)].
3. Lax-Wendroﬀ
un+1
j
= un
j −λ
2 a(un
j+1 −un
j−1) + λ2
2 a2(un
j+1 −2un
j + un
j−1) (13.40)
which can be written in the form (13.36) provided that
hj+1/2 = 1
2[a(uj+1 + uj) −λa2(uj+1 −uj)].
4. Upwind (or forward Euler/uncentred)
un+1
j
= un
j −λ
2 a(un
j+1 −un
j−1) + λ
2 |a|(un
j+1 −2un
j + un
j−1)
(13.41)
which ﬁts the form (13.36) when the numerical ﬂux is deﬁned to be
hj+1/2 = 1
2[a(uj+1 + uj) −|a|(uj+1 −uj)].
The last three methods can be obtained from the forward Euler/centred
method by adding a term proportional to a numerical diﬀusion, so that
they can be written in the equivalent form
un+1
j
= un
j −λ
2 a(un
j+1 −un
j−1) + 1
2k un
j+1 −2un
j + un
j−1
(∆x)2
,
(13.42)
where the artiﬁcial viscosity k is given for the three cases in Table 13.1.

604
13. Parabolic and Hyperbolic Initial Boundary Value Problems
methods
k
hdiff
j+1/2
τ(∆t, ∆x)
Lax-Friedrichs
∆x2
−1
2λ(uj+1 −uj)
O
∆x2
∆t + ∆t + ∆x

Lax-Wendroﬀ
a2∆t2
−λa2
2 (uj+1 −uj)
O

∆t2 + ∆x2
Upwind
|a|∆x∆t
−|a|
2 (uj+1 −uj)
O(∆t + ∆x)
TABLE
13.1.
Artiﬁcial
viscosity,
artiﬁcial
ﬂux
and
truncation
error
for
Lax-Friedrichs, Lax-Wendroﬀand Upwind methods
As a consequence, the numerical ﬂux for each scheme can be written equiv-
alently as
hj+1/2 = hF E
j+1/2 + hdiff
j+1/2
where hF E
j+1/2 is the numerical ﬂux of the forward Euler/centred scheme
(which is given in (13.38)) and the artiﬁcial diﬀusion ﬂux hdiff
j+1/2 is given
for the three cases in Table 13.1.
An example of an implicit method is the backward Euler/centred scheme
un+1
j
+ λ
2 a(un+1
j+1 −un+1
j−1 ) = un
j .
(13.43)
It can still be written in the form (13.36) provided that hn is replaced by
hn+1. In the example at hand, the numerical ﬂux is the same as for the
Forward Euler/centred method, and so is the artiﬁcial viscosity.
Finally, we report the following schemes for the approximation of the
second-order wave equation (13.33):
1. Leap-Frog
un+1
j
−2un
j + un−1
j
= (γλ)2(un
j+1 −2un
j + un
j−1)
(13.44)
2. Newmark
un+1
j
−un
j = ∆tvn
j + (γλ)2 0
βwn+1
j
+
 1
2 −β

wn
j
1
,
vn+1
j
−vn
j = (γλ)2
∆t
0
θwn+1
j
+ (1 −θ)wn
j
1
(13.45)
with wj = uj+1−2uj+uj−1 and where the parameters β and θ satisfy
0 ≤β ≤1
2, 0 ≤θ ≤1.

13.8 Analysis of Finite Diﬀerence Methods
605
13.8
Analysis of Finite Diﬀerence Methods
Let us analyze the properties of consistency, stability and convergence, as
well as those of dissipation and dispersion, of the ﬁnite diﬀerence methods
introduced above.
13.8.1
Consistency
As illustrated in Section 11.3, the local truncation error of a numerical
scheme is the residual that is generated by pretending the exact solution
to satisfy the numerical method itself.
Denoting by u the solution of the exact problem (13.26), in the case of
method (13.37) the local truncation error at (xj, tn) is deﬁned as follows
τ n
j = u(xj, tn+1) −u(xj, tn)
∆t
−au(xj+1, tn) −u(xj−1, tn)
2∆x
.
The truncation error is
τ(∆t, ∆x) = max
j,n |τ n
j |.
When τ(∆t, ∆x) goes to zero as ∆t and ∆x tend to zero independently,
the numerical scheme is said to be consistent.
Moreover, we say that it is of order p in time and of order q in space (for
suitable integers p and q), if, for a suﬃciently smooth solution of the exact
problem, we have
τ(∆t, ∆x) = O(∆tp + ∆xq).
Using Taylor’s expansion conveniently we can characterize the truncation
error of the methods previously introduced as indicated in Table 13.1. The
Leap-frog and Newmark methods are both second order accurate if ∆t =
∆x, while the forward (or backward) Euler centred method is O(∆t+∆x2).
Finally, we say that a numerical scheme is convergent if
lim
∆t,∆x→0max
j,n |u(xj, tn) −un
j | = 0.
13.8.2
Stability
A numerical method for a hyperbolic problem (linear or nonlinear) is said
to be stable if, for any time T, there exist two constants CT > 0 (possibly
depending on T) and δ0 > 0, such that
∥un∥∆≤CT ∥u0∥∆,
(13.46)
for any n such that n∆t ≤T and for any ∆t, ∆x such that 0 < ∆t ≤δ0,
0 < ∆x ≤δ0. We have denoted by ∥· ∥∆a suitable discrete norm, for

606
13. Parabolic and Hyperbolic Initial Boundary Value Problems
instance one of those indicated below
∥v∥∆,p =

∆x
∞

j=−∞
|vj|p


1
p
p = 1, 2,
∥v∥∆,∞= sup
j
|vj|.
(13.47)
Note that ∥·∥∆,p is an approximation of the norm of Lp(R). For instance, the
implicit Bacward Euler/centred scheme (13.43) is unconditionally stable
with respect to the norm ∥· ∥∆,2 (see Exercise 7).
13.8.3
The CFL Condition
Courant, Friedrichs and Lewy [CFL28] have shown that a necessary and
suﬃcient condition for any explicit scheme of the form (13.36) to be stable
is that the time and space discretization steps must obey the following
condition
|aλ| =
a ∆t
∆x
 ≤1
(13.48)
which is known as the CFL condition. The number aλ, which is an adi-
mensional number since a is a velocity, is commonly referred to as the CFL
number. If a is not constant the CFL condition becomes
∆t ≤
∆x
sup
x∈R, t>0
|a(x, t)|,
while, in the case of the hyperbolic system (13.30), the stability condition
becomes
λk
∆t
∆x
 ≤1
k = 1, . . . , p,
where {λk : k = 1 . . . , p} are the eigenvalues of A.
The CFL stability condition has the following geometric interpretation.
In a ﬁnite diﬀerence scheme the value un+1
j
depends, in general, on the
values of un at the three points xj+i, i = −1, 0, 1. Thus, at the time t = 0
the solution un+1
j
will depend only on the initial data at the points xj+i,
for i = −(n + 1), . . . , (n + 1) (see Figure 13.5).
Let us deﬁne numerical domain of dependence D∆t(xj, tn) to be the set
of values at time t = 0 the numerical solution un
j depends on, that is
D∆t(xj, tn) ⊂
%
x ∈R : |x −xj| ≤n∆x = tn
λ
&
.

13.8 Analysis of Finite Diﬀerence Methods
607
xj+(n+1)
t
tn+1
tn
xj−1
xj−(n+1)
xj
xj+1
x
t1
t0
FIGURE 13.5. The numerical domain of dependence D∆t(xj, tn+1)
Consequently, for any ﬁxed point (x, t) we have
D∆t(x, t) ⊂
%
x ∈R : |x −x| ≤t
λ
&
.
In particular, taking the limit as ∆t →0 for a ﬁxed λ, the numerical domain
of dependence becomes
D0(x, t) =
%
x ∈R : |x −x| ≤t
λ
&
.
The condition (13.48) is thus equivalent to the inclusion
D(x, t) ⊂D0(x, t),
(13.49)
where D(x, t) is the domain of dependence deﬁned in (13.31).
In the case of an hyperbolic system, thanks to (13.49), we can conclude
that the CFL condition requires that any straight line x = x −λk(t −t),
k = 1, . . . , p, must intersect the temporal straight line t = t −∆t at some
point x belonging to the domain of dependence (see Figure 13.6).
Let us analyze the stability properties of some of the methods introduced
in the previous section.
Assuming that a > 0, the upwind scheme (13.41) can be reformulated as
un+1
j
= un
j −λa(un
j −un
j−1).
(13.50)
Therefore
∥un+1∥∆,1 ≤∆x

j
|(1 −λa)un
j | + ∆x

j
|λaun
j−1|.

608
13. Parabolic and Hyperbolic Initial Boundary Value Problems
¯x + ∆x
¯t
(¯x, ¯t)
¯x −∆x
¯x
¯x + ∆x
(¯x, ¯t)
r1
r2
r2
r1
¯t
¯t −∆t
¯x −∆x
¯x
FIGURE 13.6. Geometric interpretation of the CFL condition for a system with
p = 2, where ri = ¯x −λi(t −¯t) i = 1, 2. The CFL condition is satisﬁed for the
left-hand case, while it is violated for the right-hand case
Both λa and 1 −λa are nonnegative if (13.48) holds. Thus
∥un+1∥∆,1 ≤∆x(1 −λa)

j
|un
j | + ∆xλa

j
|un
j−1| = ∥un∥∆,1.
Inequality (13.46) is therefore satisﬁed by taking CT = 1 and ∥·∥∆= ∥·∥∆,1.
The Lax-Friedrichs scheme is also stable, upon assuming (13.48). Indeed,
from (13.39) we get
un+1
j
= 1
2(1 −λa)un
j+1 + 1
2(1 + λa)un
j−1.
Therefore,
∥un+1∥∆,1
≤
1
2∆x




j
(1 −λa)un
j+1

+


j
(1 + λa)un
j−1



≤
1
2(1 −λa)∥un∥∆,1 + 1
2(1 + λa)∥un∥∆,1 = ∥un∥∆,1.
Also the Lax-Wendroﬀscheme is stable under the usual assumption (13.48)
on ∆t (for the proof see, e.g., [QV94] Chapter 14).
13.8.4
Von Neumann Stability Analysis
Let us now show that the condition (13.48) is not suﬃcient to ensure that
the forward Euler/centred scheme (13.37) is stable. For this purpose, we
make the assumption that the function u0(x) is 2π-periodic so that it can
be expanded in a Fourier series as
u0(x) =
∞

k=−∞
αkeikx
(13.51)

13.8 Analysis of Finite Diﬀerence Methods
609
where
αk = 1
2π
2π
>
0
u0(x) e−ikx dx
is the k-th Fourier coeﬃcient of u0 (see Section 10.9). Therefore,
u0
j = u0(xj) =
∞

k=−∞
αkeikjh
j = 0, ±1, ±2, · · ·
where we have set h = ∆x for ease of notation. Applying (13.37) with n = 0
we get
u1
j
=
∞

k=−∞
αkeikjh

1 −a∆t
2h (eikh −e−ikh)

=
∞

k=−∞
αkeikjh

1 −a∆t
h i sin(kh)

.
Setting
γk = 1 −a∆t
h i sin(kh),
and proceeding recursively on n yields
un
j =
∞

k=−∞
αkeikjhγn
k
j = 0, ±1, ±2, . . . ,
n ≥1.
(13.52)
The number γk ∈C is said to be the ampliﬁcation coeﬃcient of the k-th
frequency (or harmonic) at each time step. Since
|γk| =
"
1 +
a∆t
h
sin(kh)
2# 1
2
,
we deduce that
|γk| > 1 if
a ̸= 0 and
k ̸= mπ
h ,
m = 0, ±1, ±2, . . .
Correspondingly, the nodal values |un
j | continue to grow as n →∞and the
numerical solution ”blows-up” whereas the exact solution satisﬁes
|u(x, t)| = |u0(x −at)| ≤max
s∈R |u0(s)|
∀x ∈R,
∀t > 0.
The centred discretization scheme (13.37) is thus unconditionally unstable,
i.e., it is unstable for any choice of the parameters ∆t and ∆x.

610
13. Parabolic and Hyperbolic Initial Boundary Value Problems
The previous analysis is based on the Fourier series expansion and is
called von Neumann analysis. It can be applied to studying the stability of
any numerical scheme with respect to the norm ∥·∥∆,2 and for establishing
the dissipation and dispersion of the method.
Any explicit ﬁnite diﬀerence numerical scheme for problem (13.26) satis-
ﬁes a recursive relation analogous to (13.52), where γk depends a priori on
∆t and h and is called the k-th ampliﬁcation coeﬃcient of the numerical
scheme at hand.
Theorem 13.1 Assume that for a suitable choice of ∆t and h, |γk| ≤1
∀k; then, the numerical scheme is stable with respect to the ∥· ∥∆,2 norm.
Proof. Take an initial datum with a ﬁnite Fourier expansion
u0(x) =
N
2 −1

k=−N
2
αkeikx
where N is a positive integer. Without loss of generality we can assume that
problem (13.26) is well-posed on [0, 2π] since u0 is a 2π-periodic function. Take
in this interval N equally spaced nodes
xj = jh
j = 0, . . . , N −1,
with
h = 2π
N ,
at which the numerical scheme (13.36) is applied. We get
u0
j = u0(xj) =
N
2 −1

k=−N
2
αkeikjh,
un
j =
N
2 −1

k=−N
2
αkγn
k eikjh.
Notice that
∥un∥2
∆,2 = h
N−1

j=0
N
2 −1

k,m=−N
2
αkαm(γkγm)nei(k−m)jh.
Recalling Lemma 10.1 we have
h
N−1

j=0
ei(k−m)jh = 2πδkm,
−N
2 ≤k, m ≤N
2 −1,
which yields
∥un∥2
∆,2 = 2π
N
2 −1

k=−N
2
|αk|2|γk|2n.
As a consequence, since |γk| ≤1 ∀k, it turns out that
∥un∥2
∆,2 ≤2π
N
2 −1

k=−N
2
∥αk∥2 = ∥u0∥2
∆,2,
∀n ≥0,

13.9 Dissipation and Dispersion
611
which proves that the scheme is stable with respect to the ∥· ∥∆,2 norm.
3
In the case of the upwind scheme (13.41), proceeding as was done for
the centred scheme, we ﬁnd the following ampliﬁcation coeﬃcients (see
Exercise 6)
γk =





1 −a∆t
h (1 −e−ikh)
if a > 0,
1 −a∆t
h (e−ikh −1)
if a < 0.
Therefore
∀k,
|γk| ≤1 if
∆t ≤h
|a|,
which is nothing but the CFL condition.
Thanks to Theorem 13.1, if the CFL condition is satisﬁed, the upwind
scheme is stable with respect to the ∥· ∥∆,2 norm.
We conclude by noting that the upwind scheme (13.50) satisﬁes
un+1
j
= (1 −λa)un
j + λaun
j−1.
Owing to (13.48), either λa or 1 −λa are nonnegative, thus
min(un
j , un
j−1) ≤un+1
j
≤max(un
j , un
j−1).
It follows that
inf
l∈Z

u0
l

≤un
j ≤sup
l∈Z

u0
l

∀j ∈Z, ∀n ≥0,
that is,
∥un∥∆,∞≤∥u0∥∆,∞
∀n ≥0,
(13.53)
which proves that if (13.48) is satisﬁed, the upwind scheme is stable in the
norm ∥·∥∆,∞. The relation (13.53) is called the discrete maximum principle
(see also Section 12.2.2).
Remark 13.4 For the approximation of the wave equation (13.33) the
Leap-Frog method (13.44) is stable under the CFL restriction ∆t ≤∆x/|γ|,
while the Newmark method (13.45) is unconditionally stable if 2β ≥θ ≥1
2
(see [Joh90]).
■
13.9
Dissipation and Dispersion
The von Neumann analysis on the ampliﬁcation coeﬃcients enlightens the
study of the stability and dissipation of a numerical scheme.

612
13. Parabolic and Hyperbolic Initial Boundary Value Problems
Consider the exact solution to problem (13.26); the following relation
holds
u(x, tn) = u0(x −an∆t),
∀n ≥0,
∀x ∈R.
In particular, from applying (13.51) it follows that
u(xj, tn) =
∞

k=−∞
αkeikjhgn
k ,
where
gk = e−iak∆t.
(13.54)
Letting
ϕk = k∆x,
we have k∆t = λϕk and thus
gk = e−iaλϕk.
(13.55)
The real number ϕk, here expressed in radians, is called the phase angle of
the k-th harmonic. Comparing (13.54) with (13.52) we can see that γk is
the counterpart of gk which is generated by the speciﬁc numerical method
at hand. Moreover, |gk| = 1, whereas |γk| ≤1, in order to ensure stability.
Thus, γk is a dissipation coeﬃcient; the smaller |γk|, the higher the reduc-
tion of the amplitude αk, and, as a consequence, the higher the numerical
dissipation.
The ratio ϵa(k) = |γk|
|gk| is called the ampliﬁcation error of the k-th harmonic
associated with the numerical scheme (in our case it coincides with the
ampliﬁcation coeﬃcient). On the other hand, writing
γk = |γk|e−iω∆t = |γk|e
−iω
k λϕk,
and comparing this relation with (13.55), we can identify the velocity of
propagation of the numerical solution, relative to its k-th harmonic, as
being ω
k . The ratio between this velocity and the velocity a of the exact
solution is called the dispersion error ϵd relative to the k-th harmonic
ϵd(k) = ω
ka = ω∆x
ϕka .
The ampliﬁcation and dispersion errors for the numerical schemes exam-
ined so far are functions of the phase angle ϕk and the CFL number aλ.
This is shown in Figure 13.7 where we have only considered the interval
0 ≤ϕk ≤π and we have used degrees instead of radians to denote the
values of ϕk.
In Figure 13.8 the numerical solutions of equation (13.26) with a = 1
and the initial datum u0 given by a packet of two sinusoidal waves of equal
wavelength l and centred at the origin x = 0 are shown. The plots on the
left-side of the ﬁgure refer to the case l = 10∆x while on the right-side

13.9 Dissipation and Dispersion
613
0
20
40
60
80
100
120
140
160
180
0.2
0.4
0.6
0.8
1
Amplification error for Lax−Friedrichs
φ
ε
a
0
20
40
60
80
100
120
140
160
180
1
2
3
4
5
Dispersion error for Lax−Friedrichs
φ
ε
φ
CFL=  0.25
CFL=  0.50
CFL=  0.75
CFL=  1.00
CFL=  0.25
CFL=  0.50
CFL=  0.75
CFL=  1.00
0
20
40
60
80
100
120
140
160
180
0
0.2
0.4
0.6
0.8
1
1.2
Amplification error for Lax−Wendroff
φ
ε
a
0
20
40
60
80
100
120
140
160
180
0
0.2
0.4
0.6
0.8
1
1.2
1.4
Dispersion error for Lax−Wendroff
φ
ε
φ
CFL=  0.25
CFL=  0.50
CFL=  0.75
CFL=  1.00
CFL=  0.25
CFL=  0.50
CFL=  0.75
CFL=  1.00
0
20
40
60
80
100
120
140
160
180
−0.2
0
0.2
0.4
0.6
0.8
1
1.2
Amplification error for Upwind
φ
ε
a
0
20
40
60
80
100
120
140
160
180
0
0.2
0.4
0.6
0.8
1
1.2
1.4
Dispersion error for Upwind
φ
ε
φ
CFL=  0.25
CFL=  0.50
CFL=  0.75
CFL=  1.00
CFL=  0.25
CFL=  0.50
CFL=  0.75
CFL=  1.00
FIGURE 13.7. Ampliﬁcation and dispersion errors for several numerical schemes
we have l = 4∆x. Since k = (2π)/l we get ϕk = ((2π)/l)∆x, so that
ϕk = π/10 in the left-side pictures and ϕk = π/4 in the right-side ones.
All numerical solutions have been computed for a CFL number equal to
0.75, using the schemes introduced above. Notice that the dissipation eﬀect
is quite relevant at high frequencies (ϕk = π/4), especially for ﬁrst-order
methods (such as the upwind and the Lax-Friedrichs methods).
In order to highlight the eﬀects of the dispersion, the same computa-
tions have been repeated for ϕk = π/3 and diﬀerent values of the CFL
number. The numerical solutions after 5 time steps are shown in Figure
13.9. The Lax-Wendroﬀmethod is the least dissipative for all the consid-
ered CFL numbers. Moreover, a comparison of the positions of the peaks
of the numerical solutions with respect to the corresponding ones in the

614
13. Parabolic and Hyperbolic Initial Boundary Value Problems
−4
−3
−2
−1
0
1
2
3
4
−1
−0.5
0
0.5
1
x
u
Lax−Wendroff CFL=  0.75 φ=π/10
Computed at t=1
Exact
−4
−3
−2
−1
0
1
2
3
4
−1
−0.5
0
0.5
1
x
u
Lax−Friedrichs CFL=  0.75 φ=π/10
Computed at t=1
Exact
−4
−3
−2
−1
0
1
2
3
4
−1
−0.5
0
0.5
1
x
u
Upwind CFL=  0.75 φ=π/10
Computed at t=1
Exact
−4
−3
−2
−1
0
1
2
3
4
−1
−0.5
0
0.5
1
x
u
Lax−Wendroff CFL=  0.75 φ=π/4
Computed at t=1
Exact
−4
−3
−2
−1
0
1
2
3
4
−1
−0.5
0
0.5
1
x
u
Lax−Friedrichs CFL=  0.75 φ=π/4
Computed at t=1
Exact
−4
−3
−2
−1
0
1
2
3
4
−1
−0.5
0
0.5
1
x
u
Upwind CFL=  0.75 φ=π/4
Computed at t=1
Exact
FIGURE 13.8. Numerical solutions corresponding to the transport of a sinusoidal
wave packet with diﬀerent wavelengths
exact solution shows that the Lax-Friedrichs scheme is aﬀected by a posi-
tive dispersion error, since the ”numerical” wave advances faster than the
exact one. Also, the upwind scheme exhibits a slight dispersion error for a
CFL number of 0.75 which is absent for a CFL number of 0.5. The peaks
are well aligned with those of the numerical solution, although they have
been reduced in amplitude due to numerical dissipation. Finally, the Lax-
Wendroﬀmethod exhibits a small negative dispersion error; the numerical
solution is indeed slightly late with respect to the exact one.
13.9.1
Equivalent Equations
Using Taylor’s expansion to the third order to represent the truncation er-
ror, it is possible to associate with any of the numerical schemes introduced

13.9 Dissipation and Dispersion
615
−1
−0.5
0
0.5
1
1.5
2
−1
−0.5
0
0.5
1
u
Lax−Wendroff CFL=0.50 φ=π/3, t=4∆ t
Computed
Exact
−1
−0.5
0
0.5
1
1.5
2
−1
−0.5
0
0.5
1
u
Lax−Wendroff CFL=0.75 φ=π/3, t=4∆ t
Computed
Exact
−1
−0.5
0
0.5
1
1.5
2
−1
−0.5
0
0.5
1
u
Lax−Friedrichs CFL=0.50 φ=π/3, t=5∆ t
Computed
Exact
−1
−0.5
0
0.5
1
1.5
2
−1
−0.5
0
0.5
1
u
Lax−Friedrichs CFL=0.75 φ=π/3, t=5∆ t
Computed
Exact
−1
−0.5
0
0.5
1
1.5
2
−1
−0.5
0
0.5
1
u
Upwind CFL=0.50 φ=π/3, t=4∆ t
Computed
Exact
−1
−0.5
0
0.5
1
1.5
2
−1
−0.5
0
0.5
1
u
Upwind CFL=0.75 φ=π/3, t=4∆ t
Computed
Exact
FIGURE 13.9. Numerical solutions corresponding to the transport of a sinusoidal
wave packet, for diﬀerent CFL numbers
so far an equivalent diﬀerential equation of the form
vt + avx = µvxx + νvxxx
(13.56)
where the terms µvxx and νvxxx represent dissipation and dispersion, re-
spectively. Table 13.2 shows the values of µ and ν for the various methods.
Let us give a proof of this procedure in the case of the upwind scheme.
Let v(x, t) be a smooth function which satisﬁes the diﬀerence equation
(13.41); then, assuming that a > 0, we have
v(x, t + ∆t) −v(x, t)
∆t
+ av(x, t) −v(x −∆x, t)
∆x
= 0.
Truncating the Taylor expansions of v around (x, t) at the ﬁrst and second
order, respectively, we obtain
vt + O(∆t) + avx + O(∆x) = 0
(13.57)

616
13. Parabolic and Hyperbolic Initial Boundary Value Problems
Method
µ
ν
Upwind
a∆x
2
−a2∆t
2
−a
6

∆x2 −3a∆x∆t + 2a2∆t2
Lax-Friedrichs
∆x2
2∆t

1 −(aλ)2
a∆x2
3

1 −(aλ)2
Lax-Wendroﬀ
0
a∆x2
6

(aλ)2 −1

TABLE 13.2. Values of dissipation and dispersion coeﬃcients for several numer-
ical methods
and
vt + ∆t
2 vtt + O(∆t2) + a

vx + ∆x
2 vxx + O(∆x2)

= 0,
(13.58)
where vt = ∂v
∂t and vx = ∂v
∂x.
Diﬀerentiating (13.57) with respect to t and then with respect to x, we
get
vtt + avxt = O(∆x + ∆t),
and
vtx + avxx = O(∆x + ∆t).
Thus, it follows that
vtt = a2vxx + O(∆x + ∆t),
which, after substituting into (13.58), yields the following equation
vt + avx = µvxx
(13.59)
where
µ = a∆x
2
−a2∆t
2
,
and having neglected the term O(∆x2+∆t2). Relation (13.59) is the equiv-
alent diﬀerential equation up to second order of the upwind scheme.
Following the same procedure and truncating the Taylor expansion at
third order, yields
vt + avx = µvxx + νvxxx
(13.60)
where
ν = a
6

a2∆t2 −∆x2
.
We can give a heuristic explanation of the meaning of the dissipative
and dispersive terms in the equivalent equation (13.56) by studying the
following problem
" vt + avx = µvxx + νvxxx
x ∈R, t > 0
v(x, 0) = eikx,
(k ∈Z)
(13.61)

13.9 Dissipation and Dispersion
617
Applying the Fourier transform yields, if µ = ν = 0,
v(x, t) = eik(x−at),
(13.62)
while if µ and ν are arbitrary real numbers (with µ > 0) we get
v(x, t) = e−µk2teik[x−(a+νk2)t].
(13.63)
Comparing (13.62) with (13.63) we can see that the module of the solution
diminishes as µ grows and this becomes more relevant if the frequency k
gets larger. Therefore, the term µvxx in (13.61) has a dissipative eﬀect on
the solution. A further comparison between (13.62) and (13.63) shows that
the presence of the term ν modiﬁes the velocity of the propagation of the
solution; the velocity is increased if ν > 0 whereas it is diminuished if ν < 0.
Even in this case the eﬀect is ampliﬁed at high frequencies. Therefore, the
third-order diﬀerential term νvxxx introduces a dispersive eﬀect.
Generally speaking, even-order derivatives in the equivalent equation rep-
resent diﬀusive terms, while odd-order derivatives mean dispersive eﬀects.
In the case of ﬁrst-order schemes (like the upwind method) the dispersive
eﬀect is often only slightly visible since it is hidden by the dissipative one.
Actually, taking ∆t and ∆x of the same order, we have that ν << µ as
∆x →0, since ν = O(∆x2) and µ = O(∆x). In particular, for a CFL
number of 1
2, the equivalent equation of the upwind method exhibits null
dispersion, truncated at second order, according to the results of the pre-
vious section.
On the other hand, the dispersive eﬀect is strikingly visible in the Lax-
Friedrichs and in the Lax-Wendroﬀschemes; the latter, being second-order
accurate, does not exhibit a dissipative term of the form µvxx. However,
it ought to be dissipative in order to be stable; actually, the equivalent
equation (truncated at fourth order) for the Lax-Wendroﬀscheme reads
vt + avx = a∆x2
6
[(aλ)2 −1]vxxx −a∆x3
6
aλ[1 −(aλ)2]vxxxx
where the last term is dissipative if |aλ| < 1. We thus recover the CFL
condition for the Lax-Wendroﬀmethod.

618
13. Parabolic and Hyperbolic Initial Boundary Value Problems
13.10
Finite Element Approximation of Hyperbolic
Equations
Let us consider the following ﬁrst-order linear, scalar hyperbolic problem
in the interval Ω= (α, β) ⊂R













∂u
∂t + a∂u
∂x + a0u = f
in QT = Ω× (0, T)
u(α, t) = ϕ(t)
t ∈(0, T)
u(x, 0) = u0(x)
x ∈Ω,
(13.64)
where a = a(x), a0 = a0(x, t), f = f(x, t), ϕ = ϕ(t) and u0 = u0(x) are
given functions.
We assume that a(x) > 0 ∀x ∈[α, β]. In particular, this implies that
the point x = α is the inﬂow boundary, and the boundary value has to be
speciﬁed there.
13.10.1
Space Discretization with Continuous and
Discontinuous Finite Elements
A semi-discrete approximation of problem (13.64) can be carried out by
means of the Galerkin method (see Section 12.4). Deﬁne the spaces
Vh = Xr
h =

vh ∈C0(Ω) : vh|Ij ∈Pr(Ij), ∀Ij ∈Th

and
V in
h
= {vh ∈Vh : vh(α) = 0} ,
where Th is a partition of Ω(see Section 12.4.5) into n ≥2 subintervals
Ij = [xj, xj+1], for j = 0, . . . , n −1.
Let u0,h be a suitable ﬁnite element approximation of u0 and consider
the problem: for any t ∈(0, T) ﬁnd uh(t) ∈Vh such that



















β
>
α
∂uh(t)
∂t
vh dx
+
β
>
α

a∂uh(t)
∂x
+ a0(t)uh(t)

vh dx
=
β
>
α
f(t)vh dx
∀vh ∈V in
h
uh(t) = ϕh(t)
at
x = α,
(13.65)
with uh(0) = u0,h ∈Vh.

13.10 Finite Element Approximation of Hyperbolic Equations
619
If ϕ is equal to zero, uh(t) ∈V in
h , and we are allowed to taking vh = uh(t)
and get the following inequality
∥uh(t)∥2
L2(α,β)
+
t
>
0
µ0∥uh(τ)∥2
L2(α,β) dτ + a(β)
t
>
0
u2
h(τ) dτ
≤
∥u0,h∥2
L2(α,β) +
t
>
0
1
µ0
∥f(τ)∥2
L2(α,β)dτ ,
for any t ∈[0, T], where we have assumed that
0 < µ0 ≤a0(x, t) −1
2a′(x).
(13.66)
Notice that in the special case in which both f and a0 are identically zero,
we obtain
∥uh(t)∥L2(α,β) ≤∥u0,h∥L2(α,β)
which expresses the conservation of the energy of the system. When (13.66)
does not hold (for example, if a is a constant convective term and a0 = 0),
then an application of Gronwall’s lemma 11.1 yields
∥uh(t)∥2
L2(α,β) + a(β)
t
>
0
u2
h(τ) dτ
≤

∥u0,h∥2
L2(α,β) +
t
>
0
∥f(τ)∥2
L2(α,β) dτ

exp
t
>
0
[1 + 2µ∗(τ)] dτ,
(13.67)
where µ∗(t) = max
[α,β]|µ(x, t)|.
An alternative approach to the semi-discrete approximation of problem
(13.64) is based on the use of discontinuous ﬁnite elements. This choice
is motivated by the fact that, as previously pointed out, the solutions of
hyperbolic problems (even in the linear case) may exhibit discontinuities.
The ﬁnite element space can be deﬁned as follows
Wh = Y r
h =

vh ∈L2(α, β) : vh|Ij ∈Pr(Ij), ∀Ij ∈Th

,
i.e., the space of piecewise polynomials of degree less than or equal to r,
which are not necessarily continuous at the ﬁnite element nodes.
Then, the Galerkin discontinuous ﬁnite element space discretization reads:
for any t ∈(0, T) ﬁnd uh(t) ∈Wh such that















β
>
α
∂uh(t)
∂t
vh dx +
n−1

i=0



xi+1
>
xi

a∂uh(t)
∂x
+ a0(x)uh(t)

vh dx
+a(u+
h −U −
h )(xi, t)v+
h (xi)

=
β
>
α
f(t)vh dx
∀vh ∈Wh,
(13.68)

620
13. Parabolic and Hyperbolic Initial Boundary Value Problems
where {xi} are the nodes of Th with x0 = α, and for each node xi, v+
h (xi)
denotes the right-value of vh at xi while v−
h (xi) is its left-value. Finally,
U −
h (xi, t) = u−
h (xi, t) if i = 1, . . . , n −1, while U −
h (x0, t) = ϕ(t) ∀t > 0.
If a is positive, xj is the inﬂow boundary of Ij for every j and we set
[u]j = u+(xj) −u−(xj),
u±(xj) = lim
s→0±u(xj + sa),
j = 1, . . . , n −1.
Then, for any t ∈[0, T] the stability estimate for problem (13.68) reads
∥uh(t)∥2
L2(α,β) +
t
>
0

∥uh(τ)∥2
L2(α,β) +
n−1

j=0
a(xj)[uh(τ)]2
j

dτ
≤C

∥u0,h∥2
L2(α,β) +
t
>
0
+
∥f(τ)∥2
L2(α,β) + aϕ2(τ)
,
dτ

.
(13.69)
As for the convergence analysis, the following error estimate can be proved
for continuous ﬁnite elements of degree r, r ≥1 (see [QV94], Section 14.3.1)
max
t∈[0,T ]∥u(t) −uh(t)∥L2(α,β)
+


t
>
0
a|u(α, τ) −uh(α, τ)|2 dτ


1/2
=
O(∥u0 −u0,h∥L2(α,β) + hr),
If, instead, discontinuous ﬁnite elements of degree r are used, r ≥0, the
convergence estimate becomes (see [QV94], Section 14.3.3 and the refer-
ences therein)
max
t∈[0,T ]∥u(t) −uh(t)∥L2(α,β) +


T
>
0
∥u(t) −uh(t)∥2
L2(α,β) dt
+
T
>
0
n−1

j=0
a(xj) [u(t) −uh(t)]2
j dt


1/2
= O(∥u0 −u0,h∥L2(α,β) + hr+1/2).
13.10.2
Time Discretization
The time discretization of the ﬁnite element schemes introduced in the
previous section can be carried out by resorting either to ﬁnite diﬀerences
or ﬁnite elements. If an implicit ﬁnite diﬀerence scheme is adopted, both
method (13.65) and (13.68) are unconditionally stable.
As an example, let us use the backward Euler method for the time dis-
cretization of problem (13.65). We obtain for each n ≥0: ﬁnd un+1
h
∈Vh

13.10 Finite Element Approximation of Hyperbolic Equations
621
such that
1
∆t
β
>
α
(un+1
h
−un
h)vh dx +
β
>
α
a∂un+1
h
∂x
vh dx
+
β
>
α
an+1
0
un+1
h
vh dx =
β
>
α
f n+1vh dx
∀vh ∈V in
h
(13.70)
with un+1
h
(α) = ϕn+1 and u0
h = u0h. If f ≡0 and ϕ ≡0, taking vh = un+1
h
in (13.70) we can obtain
1
2∆t
+
∥un+1
h
∥2
L2(α,β) −∥un
h∥2
L2(α,β)
,
+ a(β)(un+1
h
(β))2 + µ0∥un+1
h
∥2
L2(α,β) ≤0
∀n ≥0. Summing for n from 0 to m −1 yields for m ≥1
∥um
h ∥2
L2(α,β) + 2∆t


m

j=1
∥uj
h∥2
L2(α,β) +
m

j=1
a(β)(uj+1
h
(β))2

≤∥u0
h∥2
L2(α,β).
In particular, we conclude that
∥um
h ∥L2(α,β) ≤∥u0
h∥L2(α,β)
∀m ≥0.
On the other hand, explicit schemes for the hyperbolic equations are
subject to a stability condition: for example, in the case of the forward
Euler method the stability condition is ∆t = O(∆x). In practice, this
restriction is not as severe as happens in the case of parabolic equations
and for this reason explicit schemes are often used in the approximation of
hyperbolic equations.
Programs 102 and 103 provide an implementation of the discontinous
Galerkin-ﬁnite element method of degree 0 (dG(0)) and 1 (dG(1)) in space
coupled with the backward Euler method in time for the solution of (13.26)
on the space-time domain (α, β) × (t0, T).
Program 102 - ipeidg0 : dG(0) implicit Euler
function [u,x]=ipeidg0(I,n,a,u0,bc)
nx = n(1); h = (I(2)-I(1))/nx;
x = [I(1)+h/2:h:I(2)]; t = I(3); u = (eval(u0))’;
nt = n(2); k = (I(4)-I(3))/nt;
lambda = k/h; e = ones(nx,1);
A=spdiags([-a*lambda*e, (1+a*lambda)*e],-1:0,nx,nx);
[L,U]=lu(A);
for t = I(3)+k:k:I(4)

622
13. Parabolic and Hyperbolic Initial Boundary Value Problems
f = u;
if a > 0
f(1) = a*bc(1)+f(1);
elseif a <= 0
f(nx) = a*bc(2)+f(nx);
end
y = L \ f; u = U \ y;
end
Program 103 - ipeidg1 : dG(1) implicit Euler
function [u,x]=ipeidg1(I,n,a,u0,bc)
nx = n(1); h = (I(2)-I(1))/nx;
x = [I(1):h:I(2)]; t = I(3); um = (eval(u0))’;
u = []; xx=[];
for i = 1:nx+1
u = [u, um(i), um(i)];
xx = [xx, x(i), x(i)];
end
u = u’; nt = n(2); k = (I(4)-I(3))/nt;
lambda = k/h; e = ones(2*nx+2,1);
B = spdiags([1/6*e,1/3*e,1/6*e],-1:1,2*nx+2,2*nx+2);
dd = 1/3+0.5*a*lambda; du = 1/6+0.5*a*lambda;
dl = 1/6-0.5*a*lambda; A=sparse([]);
A(1,1) = dd; A(1,2) = du; A(2,1) = dl; A(2,2) = dd;
for i=3:2:2*nx+2
A(i,i-1)
=-a*lambda;
A(i,i)
= dd;
A(i,i+1)
= du;
A(i+1,i)
= dl;
A(i+1,i+1) = A(i,i);
end
[L,U]=lu(A);
for t = I(3)+k:k:I(4)
f = B*u;
if a > 0
f(1) = a*bc(1)+f(1);
elseif a <= 0
f(nx) = a*bc(2)+f(nx);
end
y = L \ f;
u = U \ y;
end
x = xx;

13.11 Applications
623
13.11
Applications
13.11.1
Heat Conduction in a Bar
Consider a homogeneous bar of unit length with thermal conductivity ν,
which is connected at the endpoints to an external thermal source at a ﬁxed
temperature, say u = 0. Let u0(x) be the temperature distribution along
the bar at time t = 0 and f = f(x, t) be a given heat production term.
Then, the initial-boundary value problem (13.1)-(13.4) provides a model of
the time evolution of the temperature u = u(x, t) throughout the bar.
In the following, we study the case where f ≡0 and the temperature of
the bar is suddenly raised at the points around 1/2. A rough mathematical
model for this situation is provided, for instance, by taking u0 = K in
a certain subinterval [a, b] ⊆[0, 1] and equal to 0 outside, where K is a
given positive constant. The initial condition is therefore a discontinuous
function.
We have used the θ-method with θ = 0.5 (Crank-Nicolson method, CN)
and θ = 1 (Backward Euler method, BE). Program 100 has been run
with h = 1/20, ∆t = 1/40 and the obtained solutions at time t = 2 are
shown in Figure 13.10. The results show that the CN method suﬀers a clear
instability due to the low smoothness of the initial datum (about this point,
see also [QV94], Chapter 11). On the contrary, the BE method provides a
stable solution which decays correctly to zero as t grows since the source
term f is null.
0
0.5
1
0
0.5
1
1.5
2
−0.2
0
0.2
0.4
0.6
0.8
0
0.2
0.4
0.6
0.8
1
0
0.5
1
1.5
2
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
FIGURE 13.10. Solutions for a parabolic problem with discontinuous initial da-
tum: CN method (left) and BE method (right)
13.11.2
A Hyperbolic Model for Blood Flow Interaction with
Arterial Walls
Let us consider again the problem of the ﬂuid-structure interaction in a
cylindrical artery considered in Section 11.11.2, where the simple indepen-
dent rings model (11.88) was adopted.

624
13. Parabolic and Hyperbolic Initial Boundary Value Problems
If the axial action due to the tension between the diﬀerent rings is
no longer neglected, denoting by z the longitudinal coordinate, equation
(11.88) modiﬁes into
ρwH ∂2η
∂t2 −σz
∂2η
∂z2 + HE
R2
0
η = P −P0,
t > 0,
0 < z < L
(13.71)
where σz is the radial component of the axial stress and L is the length of
the cylindrical arterial district which is considered. In particular, neglecting
the third term on the left hand side and letting γ2 = σz/(ρwH), f =
(P −P0)/(ρwH), we recover the wave equation (13.33).
We have performed two sets of numerical experiments using the Leap-
Frog (LF) and Newmark (NW) methods. In the ﬁrst example the space-
time domain of integration is the cylinder (0, 1) × (0, 1) and the source
term is f = (1 + π2γ2)e−t sin(πx) in such a way that the exact solution is
u(x, t) = e−t sin(πx). Table 13.3 shows the estimated orders of convergence
of the two methods, denoted by pLF and pNW respectively.
To compute these quantities we have ﬁrst solved the wave equation on
four grids with sizes ∆x = ∆t = 1/(2k · 10), k = 0, . . . , 3. Let u(k)
h
denote
the numerical solution corresponding to the space-time grid at the k-th
reﬁning level. Moreover, for j = 1, . . . , 10, let t(0)
j
= j/10 be the time
discretization nodes of the grid at the coarsest level k = 0. Then, for each
level k, the maximum nodal errors ek
j on the k-th spatial grid have been
evaluated at each time t(0)
j
in such a way that the convergence order p(k)
j
can be estimated as
p(k)
j
= log(e0
j/ek
j )
log(2k)
,
k = 1, 2, 3.
The results clearly show second-order convergence for both the methods,
as theoretically expected.
In the second example we have taken the following expressions for the
coeﬃcient and source term: γ2 = σz/(ρwH), with σz = 1 [Kgs−2], f =
(x∆p · sin(ω0t))/(ρwH). The parameters ρw, H and the length L of the
vessel are as in Section 11.11.2. The space-time computational domain is
(0, L) × (0, T), with T = 1 [s].
The Newmark method has been ﬁrst used with ∆x = L/10 and ∆t =
T/100; the corresponding value of γλ is 3.6515, where λ = ∆t/∆x. Since the
Newmark method is unconditionally stable, no spurious oscillations arise as
is conﬁrmed by Figure 13.11, left. Notice the correct periodical behaviour
of the solution with a period corresponding to one heart beat; notice also
that with the present values of ∆t and ∆x the Leap-Frog method cannot
be employed since the CFL condition is not satisﬁed. To overcome this

13.12 Exercises
625
t(0)
j
p(1)
LF
p(2)
LF
p(3)
LF
0.1
2.0344
2.0215
2.0151
0.2
2.0223
2.0139
2.0097
0.3
2.0170
2.0106
2.0074
0.4
2.0139
2.0087
2.0061
0.5
2.0117
2.0073
2.0051
0.6
2.0101
2.0063
2.0044
0.7
2.0086
2.0054
2.0038
0.8
2.0073
2.0046
2.0032
0.9
2.0059
2.0037
2.0026
1.0
2.0044
2.0028
2.0019
t(0)
j
p(1)
NW
p(2)
NW
p(3)
NW
0.1
1.9549
1.9718
1.9803
0.2
1.9701
1.9813
1.9869
0.3
1.9754
1.9846
1.9892
0.4
1.9791
1.9869
1.9909
0.5
1.9827
1.9892
1.9924
0.6
1.9865
1.9916
1.9941
0.7
1.9910
1.9944
1.9961
0.8
1.9965
1.9979
1.9985
0.9
2.0034
2.0022
2.0015
1.0
2.0125
2.0079
2.0055
TABLE 13.3. Estimated orders of convergence for the Leap-Frog (LF) and New-
mark (NW) methods
problem, we have therefore chosen a much smaller time-step ∆t = T/400,
in such a way that γλ ≃0.9129 and the Leap-Frog scheme can be applied.
The obtained result is shown in Figure 13.11, right; a similar solution has
been computed by using the Newmark method with the same values of the
discretization parameters.
0
0.01 0.02 0.03 0.04
0
0.2
0.4
0.6
0.8
1
−3
−2
−1
0
1
2
3
x 10−4
0
0.02
0.04
0.06
0
0.2
0.4
0.6
0.8
1
−3
−2
−1
0
1
2
3
x 10−4
FIGURE 13.11. Computed solutions using the NM method on a space-time grid
with ∆t = T/100 and ∆x = L/10 (left) and the LF scheme on a space-time grid
with the same value of ∆x but with ∆t = T/400 (right)
13.12
Exercises
1. Apply the θ-method (13.9) to the approximate solution of the scalar Cauchy
problem (11.1) and using the analysis of Section 11.3 prove that the local
truncation error is of the order of ∆t + h2 if θ ̸= 1
2 while it is of the order
of ∆t2 + h2 if θ = 1
2.
2. Prove that in the case of piecewise linear ﬁnite elements, the mass-lumping
process described in Section 13.3 is equivalent to computing the integrals

626
13. Parabolic and Hyperbolic Initial Boundary Value Problems
mij =
 1
0 ϕjϕi dx by the trapezoidal quadrature formula (9.11). This, in
particular, shows that the diagonal matrix $M is nonsingular.
[Hint: ﬁrst, verify that exact integration yields
mij = h
6



1
2
i ̸= j,
1
i = j.
Then, apply the trapezoidal rule to compute mij recalling that ϕi(xj) =
δij.]
3. Prove inequality (13.19).
[Hint: using the Cauchy-Schwarz and Young inequalities, prove ﬁrst that
1
>
0
(u −v)u dx ≥1
2

∥u∥2
L2(0,1) −∥v∥2
L2(0,1)

,
∀u, v ∈L2(0, 1).
Then, use (13.18). ]
4. Assume that the bilinear form a(·, ·) in problem (13.12) is continuous and
coercive over the function space V (see (12.54)-(12.55)) with continuity and
coercivity constants M and α, respectively. Then, prove that the stability
inequalities (13.20) and (13.21) still hold provided that ν is replaced by α.
5. Show that the methods (13.39), (13.40) and (13.41) can be written in the
form (13.42). Then, show that the corresponding expressions of the artiﬁcial
viscosity K and artiﬁcial diﬀusion ﬂux hdiff
j+1/2 are as in Table (13.1).
6. Determine the CFL condition for the upwind scheme.
7. Show that for the scheme (13.43) one has ∥un+1∥∆,2 ≤∥un∥∆,2 for all
n ≥0.
[Hint: multiply equation (13.43) by un+1
j
, and notice that
(un+1
j
−un
j )un+1
j
≥1
2

|un+1
j
|2 −|un
j |2
.
Then, sum on j the resulting inequalities, and note that
λa
2
∞

j=−∞

un+1
j+1 −un+1
j−1

un+1
j
= 0
since this sum is telescopic.]
8. Show how to ﬁnd the values µ and ν in Table 13.2 for Lax-Friedrichs and
Lax-Wendroﬀmethods.
9. Prove (13.67).
10. Prove (13.69) when f = 0.
[Hint: take ∀t > 0, vh = uh(t) in (13.68).]

References
[Aas71]
Aasen J. (1971) On the Reduction of a Symmetric Matrix to
Tridiagonal Form. BIT 11: 233–242.
[ABB+92]
Anderson E., Bai Z., Bischof C., Demmel J., Dongarra J., Croz
J. D., Greenbaum A., Hammarling S., McKenney A., Ous-
trouchov S., and Sorensen D. (1992) LAPACK User’s Guide,
Release 1.0. SIAM, Philadelphia.
[Ada75]
Adams D. (1975) Sobolev Spaces. Academic Press, New York.
[ADR92]
Arioli M., DuﬀI., and Ruiz D. (1992) Stopping Criteria for
Iterative Solvers. SIAM J. Matrix Anal. Appl. 1(13).
[AF83]
Alonso M. and Finn E. (1983) Fundamental University
Physics, volume 3. Addison-Wesley, Reading, Massachusetts.
[Arm66]
Armijo L. (1966) Minimization of Functions Having Continu-
ous Partial Derivatives. Paciﬁc Jour. Math. 16: 1–3.
[Arn73]
Arnold V. I. (1973) Ordinary Diﬀerential Equations. The MIT
Press, Cambridge, Massachusetts.
[Atk89]
Atkinson K. E. (1989) An Introduction to Numerical Analysis.
John Wiley, New York.
[Avr76]
Avriel M. (1976) Non Linear Programming: Analysis and
Methods. Prentice-Hall, Englewood Cliﬀs, New Jersey.

628
References
[Axe94]
Axelsson O. (1994) Iterative Solution Methods.
Cambridge
University Press, New York.
[Bar89]
Barnett S. (1989) Leverrier’s Algorithm: A New Proof and
Extensions. Numer. Math. 7: 338–352.
[Bat90]
Batterson S. (1990) Convergence of the Shifted QR Algorithm
on 3 by 3 Normal Matrices. Numer. Math. 58: 341–352.
[BBC+94]
Barrett R., Berry M., Chan T., Demmel J., Donato J., Don-
garra J., Eijkhout V., Pozo V., Romine C., and van der Vorst
H. (1994) Templates for the Solution of Linear Systems: Build-
ing Blocks for Iterative Methods. SIAM, Philadelphia.
[BD74]
Bj¨orck A. and Dahlquist G. (1974) Numerical Methods.
Prentice-Hall, Englewood Cliﬀs, N.J.
[BDMS79]
Bunch J., Dongarra J., Moler C., and Stewart G. (1979) LIN-
PACK User’s Guide. SIAM, Philadelphia.
[Ber82]
Bertsekas D. P. (1982) Constrained Optimization and La-
grange Multiplier Methods. Academic Press. Inc., San Diego,
California.
[Bj¨o88]
Bj¨orck A. (1988) Least Squares Methods: Handbook of Numer-
ical Analysis Vol. 1 Solution of Equations in RN.
Elsevier
North Holland.
[BM92]
Bernardi C. and Maday Y. (1992) Approximations Spectrales
des Probl´emes aux Limites Elliptiques. Springer-Verlag, Paris.
[BMW67]
Barth W., Martin R. S., and Wilkinson J. H. (1967) Calcula-
tion of the Eigenvalues of a Symmetric Tridiagonal Matrix by
the Method of Bisection. Numer. Math. 9: 386–393.
[BO78]
Bender C. M. and Orszag S. A. (1978) Advanced Mathemati-
cal Methods for Scientists and Engineers. McGraw-Hill, New
York.
[Boe80]
Boehm W. (1980) Inserting New Knots into B-spline Curves.
Computer Aided Design 12: 199–201.
[Bos93]
Bossavit A. (1993) Electromagnetisme, en vue de la modelisa-
tion. Springer-Verlag, Paris.
[BR81]
Bank R. E. and Rose D. J. (1981) Global Approximate Newton
Methods. Numer. Math. 37: 279–295.
[Bra75]
Bradley G. (1975) A Primer of Linear Algebra. Prentice-Hall,
Englewood Cliﬀs, New York.

References
629
[Bre73]
Brent R. (1973) Algorithms for Minimization Without Deriva-
tives. Prentice-Hall, Englewood Cliﬀs, New York.
[Bri74]
Brigham E. O. (1974) The Fast Fourier Transform. Prentice-
Hall, Englewood Cliﬀs, New York.
[BS90]
Brown P. and Saad Y. (1990) Hybrid Krylov Methods for Non-
linear Systems of equations. SIAM J. Sci. and Stat. Comput.
11(3): 450–481.
[BSG96]
B. Smith P. B. and Gropp P. (1996) Domain Decomposition,
Parallel Multilevel Methods for Elliptic Partial Diﬀerential
Equations. Univ. Cambridge Press, Cambridge.
[But64]
Butcher J. C. (1964) Implicit Runge-Kutta Processes. Math.
Comp. 18: 233–244.
[But66]
Butcher J. C. (1966) On the Convergence of Numerical So-
lutions to Ordinary Diﬀerential Equations. Math. Comp. 20:
1–10.
[But87]
Butcher J. (1987) The Numerical Analysis of Ordinary Diﬀer-
ential Equations: Runge-Kutta and General Linear Methods.
Wiley, Chichester.
[CCP70]
Cannon M., Cullum C., and Polak E. (1970) Theory and Op-
timal Control and Mathematical Programming. McGraw-Hill,
New York.
[CFL28]
Courant R., Friedrichs K., and Lewy H. (1928) ¨Uber die
partiellen diﬀerenzengleichungen der mathematischen physik.
Math. Ann. 100: 32–74.
[CHQZ88]
Canuto C., Hussaini M. Y., Quarteroni A., and Zang T. A.
(1988) Spectral Methods in Fluid Dynamics. Springer, New
York.
[CI95]
Chandrasekaren S. and Ipsen I. (1995) On the Sensitivity of
Solution Components in Linear Systems of equations. SIAM
J. Matrix Anal. Appl. 16: 93–112.
[CL91]
Ciarlet P. G. and Lions J. L. (1991) Handbook of Numerical
Analysis: Finite Element Methods (Part 1). North-Holland,
Amsterdam.
[CM94]
Chan T. and Mathew T. (1994) Domain Decomposition Algo-
rithms. Acta Numerica pages 61–143.

630
References
[CMSW79]
Cline A., Moler C., Stewart G., and Wilkinson J. (1979) An
Estimate for the Condition Number of a Matrix. SIAM J. Sci.
and Stat. Comput. 16: 368–375.
[Col66]
Collin R. E. (1966) Foundations for Microwave Engineering.
McGraw-Hill Book Co., Singapore.
[Com95]
Comincioli V. (1995) Analisi Numerica Metodi Modelli Appli-
cazioni. McGraw-Hill Libri Italia, Milano.
[Cox72]
Cox M. (1972) The Numerical Evaluation of B-splines. Jour-
nal of the Inst. of Mathematics and its Applications 10: 134–
149.
[Cry73]
Cryer C. W. (1973) On the Instability of High Order
Backward-Diﬀerence Multistep Methods. BIT 13: 153–159.
[CT65]
Cooley J. and Tukey J. (1965) An Algorithm for the Machine
Calculation of Complex Fourier Series. Math. Comp. 19: 297–
301.
[Dah56]
Dahlquist G. (1956) Convergence and Stability in the Nu-
merical Integration of Ordinary Diﬀerential Equations. Math.
Scand. 4: 33–53.
[Dah63]
Dahlquist G. (1963) A Special Stability Problem for Linear
Multistep Methods. BIT 3: 27–43.
[Dat95]
Datta B. (1995) Numerical Linear Algebra and Applications.
Brooks/Cole Publishing, Paciﬁc Grove, CA.
[Dau88]
Daubechies I. (1988) Orthonormal bases of compactly sup-
ported wavelets. Commun. on Pure and Appl. Math. XLI.
[Dav63]
Davis P. (1963) Interpolation and Approximation. Blaisdell
Pub., New York.
[Day96]
Day D. (1996) How the QR algorithm Fails to Converge and
How to Fix It. Technical Report 96-0913J, Sandia National
Laboratory, Albuquerque.
[dB72]
de Boor C. (1972) On Calculating with B-splines. Journal of
Approximation Theory 6: 50–62.
[dB83]
de Boor C. (1983) A Practical Guide to Splines. In Applied
Mathematical Sciences. (27), Springer-Verlag, New York.
[dB90]
de Boor C. (1990) SPLINE TOOLBOX for use with MAT-
LAB. The Math Works, Inc., South Natick.

References
631
[DD95]
Davis
T.
and
Duﬀ
I.
(1995)
A
combined
unifrontal/multifrontal
method
for
unsymmetric
sparse
matrices.
Technical Report TR-95-020, Computer and
Information Sciences Department, University of Florida.
[Dek69]
Dekker T. (1969) Finding a Zero by means of Successive Linear
Interpolation. In Dejon B. and Henrici P. (eds) Constructive
Aspects of the Fundamental Theorem of Algebra, pages 37–51.
Wiley, New York.
[Dek71]
Dekker T. (1971) A Floating-Point Technique for Extending
the Available Precision. Numer. Math. 18: 224–242.
[Dem97]
Demmel J. (1997) Applied Numerical Linear Algebra. SIAM,
Philadelphia.
[DGK84]
Dongarra J., Gustavson F., and Karp A. (1984) Implementing
Linear Algebra Algorithms for Dense Matrices on a Vector
Pipeline Machine. SIAM Review 26(1): 91–112.
[Die87a]
Dierckx P. (1987) FITPACK User Guide part 1: Curve Fitting
Routines. TW Report, Dept. of Computer Science, Katholieke
Universiteit, Leuven, Belgium.
[Die87b]
Dierckx P. (1987) FITPACK User Guide part 2: Surface
Fitting Routines.
TW Report, Dept. of Computer Science,
Katholieke Universiteit, Leuven, Belgium.
[Die93]
Dierckx P. (1993) Curve and Surface Fitting with Splines.
Claredon Press, New York.
[DL92]
DeVore R. and Lucier J. (1992) Wavelets.
Acta Numerica
pages 1–56.
[DR75]
Davis P. and Rabinowitz P. (1975) Methods of Numerical In-
tegration. Academic Press, New York.
[DS83]
Dennis J. and Schnabel R. (1983) Numerical Methods for Un-
constrained Optimization and Nonlinear Equations. Prentice-
Hall, Englewood Cliﬀs, New York.
[Dun85]
Dunavant D. (1985) High Degree Eﬃcient Symmetrical Gaus-
sian Quadrature Rules for the Triangle. Internat. J. Numer.
Meth. Engrg. 21: 1129–1148.
[Dun86]
Dunavant D. (1986) Eﬃcient Symmetrical Cubature Rules for
Complete Polynomials of High Degree over the Unit Cube.
Internat. J. Numer. Meth. Engrg. 23: 397–407.

632
References
[DV84]
Dekker K. and Verwer J. (1984) Stability of Runge-Kutta
Methods for StiﬀNonlinear Diﬀerential Equations.
North-
Holland, Amsterdam.
[dV89]
der Vorst H. V. (1989) High Performance Preconditioning.
SIAM J. Sci. Stat. Comput. 10: 1174–1185.
[EEHJ96]
Eriksson K., Estep D., Hansbo P., and Johnson C. (1996)
Computational Diﬀerential Equations.
Cambridge Univ.
Press, Cambridge.
[Elm86]
Elman H. (1986) A Stability Analisys of Incomplete LU Fac-
torization. Math. Comp. 47: 191–218.
[Erd61]
Erd¨os P. (1961) Problems and Results on the Theory of Inter-
polation. Acta Math. Acad. Sci. Hungar. 44: 235–244.
[Erh97]
Erhel J. (1997) About Newton-Krylov Methods. In Periaux J.
and al. (eds) Computational Science for 21st Century, pages
53–61. Wiley, New York.
[Fab14]
Faber G. (1914)
¨Uber die interpolatorische Darstellung
stetiger Funktionem. Jber. Deutsch. Math. Verein. 23: 192–
210.
[FF63]
Faddeev D. K. and Faddeeva V. N. (1963) Computational
Methods of Linear Algebra. Freeman, San Francisco and Lon-
don.
[Fle75]
Fletcher R. (1975) Conjugate gradient methods for indeﬁnite
systems. In Springer-Verlag (ed) Numerical Analysis, pages
73–89. New York.
[FM67]
Forsythe G. E. and Moler C. B. (1967) Computer Solution
of Linear Algebraic Systems. Prentice-Hall, Englewood Cliﬀs,
New York.
[Fra61]
Francis J. G. F. (1961) The QR Transformation: A Unitary
Analogue to the LR Transformation. Parts I and II. Comp. J.
pages 265–272,332–334.
[FRL55]
F. Richtmyer E. K. and Lauritsen T. (1955) Introduction to
Modern Physics. McGraw-Hill, New York.
[Gas83]
Gastinel N. (1983) Linear Numerical Analysis. Kershaw Pub-
lishing, London.

References
633
[Gau94]
Gautschi W. (1994) Algorithm 726: ORTHPOL - A Package of
Routines for Generating Orthogonal Polynomials and Gauss-
type Quadrature Rules. ACM Trans. Math. Software 20: 21–
62.
[Gau96]
Gautschi W. (1996) Orthogonal Polynomials: Applications
and Computation. Acta Numerica pages 45–119.
[Gau97]
Gautschi W. (1997) Numerical Analysis. An Introduction.
Birkh¨auser, Berlin.
[Geo73]
George A. (1973) Nested Dissection of a Regular Finite Ele-
ment Mesh. SIAM J. Num. Anal. 10: 345–363.
[Giv54]
Givens W. (1954) Numerical Computation of the Character-
istic Values of a Real Symmetric Matrix. Oak Ridge National
Laboratory ORNL-1574.
[GL81]
George A. and Liu J. (1981) Computer Solution of Large
Sparse Positive Deﬁnite Systems. Prentice-Hall, Englewood
Cliﬀs, New York.
[GL89]
Golub G. and Loan C. V. (1989) Matrix Computations. The
John Hopkins Univ. Press, Baltimore and London.
[GM83]
Golub G. and Meurant G. (1983) Resolution Numerique des
Grands Systemes Lineaires. Eyrolles, Paris.
[GMW81]
Gill P., Murray W., and Wright M. (1981) Practical Optimiza-
tion. Academic Press, London.
[God66]
Godeman R. (1966) Algebra. Kershaw, London.
[Gol91]
Goldberg D. (1991) What Every Computer Scientist Should
Know about Floating-point Arithmetic.
ACM Computing
Surveys 23(1): 5–48.
[GP67]
Goldstein A. A. and Price J. B. (1967) An Eﬀective Algorithm
for Minimization. Numer. Math 10: 184–189.
[GR96]
Godlewski E. and Raviart P. (1996) Numerical Approximation
of Hyperbolic System of Conservation Laws, volume 118 of
Applied Mathematical Sciences. Springer-Verlag, New York.
[Hac94]
Hackbush W. (1994) Iterative Solution of Large Sparse Sys-
tems of Equations. Springer-Verlag, New York.
[Hah67]
Hahn W. (1967) Stability of Motion. Springer-Verlag, Berlin.

634
References
[Hal58]
Halmos P. (1958) Finite-Dimensional Vector Spaces. Van Nos-
trand, Princeton, New York.
[Hen62]
Henrici P. (1962) Discrete Variable Methods in Ordinary Dif-
ferential Equations. Wiley, New York.
[Hen74]
Henrici P. (1974) Applied and Computational Complex Anal-
ysis, volume 1. Wiley, New York.
[HGR96]
H-G. Roos M. Stynes L. T. (1996) Numerical Methods for
Singularly Perturbed Diﬀerential Equations. Springer-Verlag,
Berlin Heidelberg.
[Hig88]
Higham N. (1988) The Accuracy of Solutions to Triangular
Systems. University of Manchester, Dep. of Mathematics 158:
91–112.
[Hig89]
Higham N. (1989) The Accuracy of Solutions to Triangular
Systems. SIAM J. Numer. Anal. 26(5): 1252–1265.
[Hig96]
Higham N. (1996) Accuracy and Stability of Numerical Algo-
rithms. SIAM Publications, Philadelphia, PA.
[Hil87]
Hildebrand F. (1987) Introduction to Numerical Analysis.
McGraw-Hill, New York.
[Hou75]
Householder A. (1975) The Theory of Matrices in Numerical
Analysis. Dover Publications, New York.
[HP94]
Hennessy J. and Patterson D. (1994) Computer Organiza-
tion and Design - The Hardware/Software Interface. Morgan
Kaufmann, San Mateo.
[HW76]
Hammarling S. and Wilkinson J. (1976) The Practical Be-
haviour of Linear Iterative Methods with Particular Reference
to S.O.R. Technical Report Report NAC 69, National Physi-
cal Laboratory, Teddington, UK.
[IK66]
Isaacson E. and Keller H. (1966) Analysis of Numerical Meth-
ods. Wiley, New York.
[Inm94]
Inman D. (1994) Engineering Vibration. Prentice-Hall, Engle-
wood Cliﬀs, NJ.
[Iro70]
Irons B. (1970) A Frontal Solution Program for Finite Element
Analysis. Int. J. for Numer. Meth. in Engng. 2: 5–32.
[Jac26]
Jacobi C. (1826) Uber Gauβ neue Methode, die Werthe der
Integrale n¨aherungsweise zu ﬁnden. J. Reine Angew. Math.
30: 127–156.

References
635
[Jer96]
Jerome J. J. (1996) Analysis of Charge Transport. A Math-
ematical Study of Semiconductor Devices.
Springer, Berlin
Heidelberg.
[Jia95]
Jia Z. (1995) The Convergence of Generalized Lanczos Meth-
ods for Large Unsymmetric Eigenproblems. SIAM J. Matrix
Anal. Applic. 16: 543–562.
[JM92]
Jennings A. and McKeown J. (1992) Matrix Computation.
Wiley, Chichester.
[Joh90]
Johnson C. (1990) Numerical Solution of Partial Diﬀerential
Equations by the Finite Element Method. Cambridge Univ.
Press.
[JW77]
Jankowski M. and Wozniakowski M. (1977) Iterative Reﬁne-
ment Implies Numerical Stability. BIT 17: 303–311.
[Kah66]
Kahan W. (1966) Numerical Linear Algebra. Canadian Math.
Bull. 9: 757–801.
[Kan66]
Kaniel S. (1966) Estimates for Some Computational Tech-
niques in Linear Algebra. Math. Comp. 20: 369–378.
[Kea86]
Keast P. (1986) Moderate-Degree Tetrahedral Quadrature
Formulas. Comp. Meth. Appl. Mech. Engrg. 55: 339–348.
[Kel99]
Kelley C. (1999) Iterative Methods for Optimization, vol-
ume 18 of Frontiers in Applied Mathematics. SIAM, Philadel-
phia.
[KT51]
Kuhn H. and Tucker A. (1951) Nonlinear Programming. In
Second Berkeley Symposium on Mathematical Statistics and
Probability, pages 481–492. Univ. of California Press, Berkeley
and Los Angeles.
[Lam91]
Lambert J. (1991) Numerical Methods for Ordinary Diﬀeren-
tial Systems. John Wiley and Sons, Chichester.
[Lan50]
Lanczos C. (1950) An Iteration Method for the Solution of the
Eigenvalue Problem of Linear Diﬀerential and Integral Oper-
ator. J. Res. Nat. Bur. Stand. 45: 255–282.
[Lax65]
Lax P. (1965) Numerical Solution of Partial Diﬀerential Equa-
tions. Amer. Math. Monthly 72(2): 74–84.
[Lel92]
Lele S. (1992) Compact Finite Diﬀerence Schemes with
Spectral-like Resolution. Journ. of Comp. Physics 103(1): 16–
42.

636
References
[Lem89]
Lemarechal C. (1989) Nondiﬀerentiable Optimization.
In
Nemhauser G., Kan A. R., and Todd M. (eds) Handbooks
Oper. Res. Management Sci., volume 1. Optimization, pages
529–572. North-Holland, Amsterdam.
[LH74]
Lawson C. and Hanson R. (1974) Solving Least Squares Prob-
lems. Prentice-Hall, Englewood Cliﬀs, New York.
[LM68]
Lions J. L. and Magenes E. (1968) Problemes aux limit`es non-
homog`enes et applications. Dunod, Paris.
[LS96]
Lehoucq R. and Sorensen D. (1996) Deﬂation Techniques for
an Implicitly Restarted Iteration. SIAM J. Matrix Anal. Ap-
plic. 17(4): 789–821.
[Lue73]
Luenberger D. (1973) Introduction to Linear and Non Linear
Programming. Addison-Wesley, Reading, Massachusetts.
[Man69]
Mangasarian O. (1969) Non Linear Programming. Prentice-
Hall, Englewood Cliﬀs, New Jersey.
[Man80]
Manteuﬀel T. (1980) An Incomplete Factorization Technique
for Positive Deﬁnite Linear Systems. Math. Comp. 150(34):
473–497.
[Mar86]
Markowich P. (1986) The Stationary Semiconductor Device
Equations. Springer-Verlag, Wien and New York.
[McK62]
McKeeman W. (1962) Crout with Equilibration and Iteration.
Comm. ACM 5: 553–555.
[MdV77]
Meijerink J. and der Vorst H. V. (1977) An Iterative Solution
Method for Linear Systems of Which the Coeﬃcient Matrix is
a Symmetric M-matrix. Math. Comp. 137(31): 148–162.
[MM71]
Maxﬁeld J. and Maxﬁeld M. (1971) Abstract Algebra and So-
lution by Radicals. Saunders, Philadelphia.
[MMG87]
Martinet R., Morlet J., and Grossmann A. (1987) Analysis of
sound patterns through wavelet transforms. Int. J. of Pattern
Recogn. and Artiﬁcial Intellig. 1(2): 273–302.
[MNS74]
M¨akela M., Nevanlinna O., and Sipil¨a A. (1974) On the Con-
cept of Convergence, Consistency and Stability in Connection
with Some Numerical Methods. Numer. Math. 22: 261–274.
[Mor84]
Morozov V. (1984) Methods for Solving Incorrectly Posed
Problems. Springer-Verlag, New York.

References
637
[Mul56]
Muller D. (1956) A Method for Solving Algebraic Equations
using an Automatic Computer. Math. Tables Aids Comput.
10: 208–215.
[ NAG95]
NAG (1995) NAG Fortran Library Manual - Mark 17. NAG
Ltd., Oxford.
[Nat65]
Natanson I. (1965) Constructive Function Theory, volume III.
Ungar, New York.
[NM65]
Nelder J. and Mead R. (1965) A simplex method for function
minimization. The Computer Journal 7: 308–313.
[Nob69]
Noble B. (1969) Applied Linear Algebra. Prentice-Hall, Engle-
wood Cliﬀs, New York.
[OR70]
Ortega J. and Rheinboldt W. (1970) Iterative Solution of Non-
linear Equations in Several Variables. Academic Press, New
York and London.
[Pap62]
Papoulis A. (1962) The Fourier Integral and its Application.
McGraw-Hill, New York.
[Pap87]
Papoulis
A.
(1987)
Probability, Random Variables, and
Stochastic Processes. McGraw-Hill, New York.
[Par80]
Parlett
B.
(1980)
The
Symmetric
Eigenvalue
Problem.
Prentice-Hall, Englewood Cliﬀs, NJ.
[PdK¨UK83] Piessens R., deDoncker Kapenga E., ¨Uberhuber C. W., and
Kahaner D. K. (1983) QUADPACK: A Subroutine Package
for Automatic Integration. Springer-Verlag, Berlin and Hei-
delberg.
[PJ55]
Peaceman D. and Jr. H. R. (1955) The numerical solution of
parabolic and elliptic diﬀerential equations. J. Soc. Ind. Appl.
Math. 3: 28–41.
[Pou96]
Poularikas A. (1996) The Transforms and Applications Hand-
book. CRC Press, Inc., Boca Raton, Florida.
[PR70]
Parlett B. and Reid J. (1970) On the Solution of a System of
Linear Equations Whose Matrix is Symmetric but not Deﬁ-
nite. BIT 10: 386–397.
[PS91]
Pagani C. and Salsa S. (1991) Analisi Matematica, volume II.
Masson, Milano.

638
References
[PW79]
Peters G. and Wilkinson J. (1979) Inverse iteration, ill-
conditioned equations, and newton’s method. SIAM Review
21: 339–360.
[QV94]
Quarteroni A. and Valli A. (1994) Numerical Approximation
of Partial Diﬀerential Equations. Springer, Berlin and Heidel-
berg.
[QV99]
Quarteroni A. and Valli A. (1999) Domain Decomposition
Methods for Partial Diﬀerential Equations.
Oxford Science
Publications, New York.
[Ral65]
Ralston A. (1965) A First Course in Numerical Analysis.
McGraw-Hill, New York.
[Red86]
Reddy B. D. (1986) Applied Functional Analysis and Varia-
tional Methods in Engineering. McGraw-Hill, New York.
[Ric81]
Rice J. (1981) Matrix Computations and Mathematical Soft-
ware. McGraw-Hill, New York.
[Riv74]
Rivlin T. (1974) The Chebyshev Polynomials. John Wiley and
Sons, New York.
[RM67]
Richtmyer R. and Morton K. (1967) Diﬀerence Methods for
Initial Value Problems. Wiley, New York.
[RR78]
Ralston A. and Rabinowitz P. (1978) A First Course in Nu-
merical Analysis. McGraw-Hill, New York.
[Rud83]
Rudin W. (1983) Real and Complex Analysis. Tata McGraw-
Hill, New Delhi.
[Rut58]
Rutishauser H. (1958) Solution of Eigenvalue Problems with
the LR Transformation. Nat. Bur. Stand. Appl. Math. Ser.
49: 47–81.
[Saa90]
Saad Y. (1990) Sparskit: A basic tool kit for sparse matrix
computations. Technical Report 90-20, Research Institute for
Advanced Computer Science, NASA Ames Research Center,
Moﬀet Field, CA.
[Saa92]
Saad Y. (1992) Numerical Methods for Large Eigenvalue Prob-
lems. Halstead Press, New York.
[Saa96]
Saad Y. (1996) Iterative Methods for Sparse Linear Systems.
PWS Publishing Company, Boston.
[Sch67]
Schoenberg I. (1967) On Spline functions. In Shisha O. (ed)
Inequalities, pages 255–291. Academic Press, New York.

References
639
[Sch81]
Schumaker L. (1981) Splines Functions: Basic Theory. Wiley,
New York.
[Sel84]
Selberherr S. (1984) Analysis and Simulation of Semiconduc-
tor Devices. Springer-Verlag, Wien and New York.
[SG69]
Scharfetter D. and Gummel H. (1969) Large-signal analysis of
a silicon Read diode oscillator. IEEE Trans. on Electr. Dev.
16: 64–77.
[Ske79]
Skeel R. (1979) Scaling for Numerical Stability in Gaussian
Elimination. J. Assoc. Comput. Mach. 26: 494–526.
[Ske80]
Skeel R. (1980) Iterative Reﬁnement Implies Numerical Sta-
bility for Gaussian Elimination. Math. Comp. 35: 817–832.
[SL89]
Su B. and Liu D. (1989) Computational Geometry: Curve and
Surface Modeling. Academic Press, New York.
[Sla63]
Slater J. (1963) Introduction to Chemical Physics. McGraw-
Hill Book Co.
[Smi85]
Smith G. (1985) Numerical Solution of Partial Diﬀerential
Equations: Finite Diﬀerence Methods.
Oxford University
Press, Oxford.
[Son89]
Sonneveld P. (1989) Cgs, a fast lanczos-type solver for non-
symmetric linear systems.
SIAM Journal on Scientiﬁc and
Statistical Computing 10(1): 36–52.
[SR97]
Shampine L. F. and Reichelt M. W. (1997) The MATLAB
ODE Suite. SIAM J. Sci. Comput. 18: 1–22.
[SS90]
Stewart G. and Sun J. (1990) Matrix Perturbation Theory.
Academic Press, New York.
[SS98]
Schwab C. and Sch¨otzau D. (1998) Mixed hp-FEM on
Anisotropic Meshes.
Mat. Models Methods Appl. Sci. 8(5):
787–820.
[Ste71]
Stetter H. (1971) Stability of discretization on inﬁnite inter-
vals. In Morris J. (ed) Conf. on Applications of Numerical
Analysis, pages 207–222. Springer-Verlag, Berlin.
[Ste73]
Stewart G. (1973) Introduction to Matrix Computations. Aca-
demic Press, New York.
[Str69]
Strassen V. (1969) Gaussian Elimination is Not Optimal. Nu-
mer. Math. 13: 727–764.

640
References
[Str80]
Strang G. (1980) Linear Algebra and Its Applications. Aca-
demic Press, New York.
[Str89]
Strikwerda J. (1989) Finite Diﬀerence Schemes and Partial
Diﬀerential Equations. Wadsworth and Brooks/Cole, Paciﬁc
Grove.
[Sze67]
Szeg¨o G. (1967) Orthogonal Polynomials. AMS, Providence,
R.I.
[Tit37]
Titchmarsh E. (1937) Introduction to the Theory of Fourier
Integrals. Oxford.
[Var62]
Varga R. (1962) Matrix Iterative Analysis. Prentice-Hall, En-
glewood Cliﬀs, New York.
[vdV92]
van der Vorst H. (1992) Bi-cgstab: a fast and smoothly con-
verging variant of bi-cg for the solution of non-symmetric lin-
ear systems. SIAM Jour. on Sci. and Stat. Comp. 12: 631–644.
[Ver96]
Verf¨urth R. (1996) A Review of a Posteriori Error Estimation
and Adaptive Mesh Reﬁnement Techniques. Wiley, Teubner,
Germany.
[Wac66]
Wachspress E. (1966) Iterative Solutions of Elliptic Systems.
Prentice-Hall, Englewood Cliﬀs, New York.
[Wal75]
Walsh G. (1975) Methods of Optimization. Wiley.
[Wal91]
Walker J. (1991) Fast Fourier Transforms. CRC Press, Boca
Raton.
[Wen66]
WendroﬀB. (1966) Theoretical Numerical Analysis. Academic
Press, New York.
[Wid67]
Widlund O. (1967) A Note on Unconditionally Stable Linear
Multistep Methods. BIT 7: 65–70.
[Wil62]
Wilkinson J. (1962) Note on the Quadratic Convergence of
the Cyclic Jacobi Process. Numer. Math. 6: 296–300.
[Wil63]
Wilkinson J. (1963) Rounding Errors in Algebraic Processes.
Prentice-Hall, Englewood Cliﬀs, New York.
[Wil65]
Wilkinson J. (1965) The Algebraic Eigenvalue Problem.
Clarendon Press, Oxford.
[Wil68]
Wilkinson J. (1968) A priori Error Analysis of Algebraic Pro-
cesses. In Intern. Congress Math., volume 19, pages 629–639.
Izdat. Mir, Moscow.

References
641
[Wol69]
Wolfe P. (1969) Convergence Conditions for Ascent Methods.
SIAM Review 11: 226–235.
[Wol71]
Wolfe P. (1971) Convergence Conditions for Ascent Methods.
II: Some Corrections. SIAM Review 13: 185–188.
[Wol78]
Wolfe M. (1978) Numerical Methods for Unconstrained Opti-
mization. Van Nostrand Reinhold Company, New York.
[You71]
Young D. (1971) Iterative Solution of Large Linear Systems.
Academic Press, New York.
[Zie77]
Zienkiewicz O. C. (1977) The Finite Element Method (Third
Edition). McGraw Hill, London.

Index of MATLAB Programs
forward row
Forward substitution: row-oriented version
.
66
forward col
Forward substitution: column-oriented version
66
backward col
Backward substitution: column-oriented version 66
lu kji
LU factorization of matrix A. kji version . .
77
lu jki
LU factorization of matrix A. jki version . .
77
lu ijk
LU factorization of the matrix A: ijk version
79
chol2
Cholesky factorization . . . . . . . . . . . . .
81
mod grams
Modiﬁed Gram-Schmidt method . . . . . . .
84
LUpivtot
LU factorization with complete pivoting . . .
88
lu band
LU factorization for a banded matrix
. . . .
92
forw band
Forward substitution for a banded matrix L
92
back band
Backward substitution for a banded matrix U
92
mod thomas
Thomas algorithm, modiﬁed version . . . . .
93
cond est
Algorithm for the approximation of K1(A) .
109
JOR
JOR method . . . . . . . . . . . . . . . . . .
135
SOR
SOR method . . . . . . . . . . . . . . . . . .
136
basicILU
Incomplete LU factorization
. . . . . . . . .
141
ilup
ILU(p) factorization . . . . . . . . . . . . . .
143
gradient
Gradient method with dynamic parameter
.
149
conjgrad
Preconditioned conjugate gradient method .
157
arnoldi alg
The Arnoldi algorithm
. . . . . . . . . . . .
161
arnoldi met
The Arnoldi method for linear systems
. . .
164
GMRES
The GMRES method for linear systems . . .
166
Lanczos
The Lanczos method for linear systems . . .
167
Lanczosnosym
The Lanczos method for unsymmetric systems170

644
Index of MATLAB Programs
powerm
Power method . . . . . . . . . . . . . . . . .
197
invpower
Inverse power method . . . . . . . . . . . . .
198
basicqr
Basic QR iteration . . . . . . . . . . . . . . .
203
houshess
Hessenberg-Householder method . . . . . . .
208
hessqr
Hessenberg-QR method . . . . . . . . . . . .
210
givensqr
QR factorization with Givens rotations . . .
211
vhouse
Construction of the Householder vector . . .
213
givcos
Computation of Givens cosine and sine . . .
214
garow
Product G(i, k, θ)T M
. . . . . . . . . . . . .
214
gacol
Product MG(i, k, θ) . . . . . . . . . . . . . .
214
qrshift
QR iteration with single shift . . . . . . . . .
217
qr2shift
QR iteration with double shift . . . . . . . .
220
psinorm
Evaluation of Ψ(A)
. . . . . . . . . . . . . .
229
symschur
Evaluation of c and s . . . . . . . . . . . . .
229
cycjacobi
Cyclic Jacobi method for symmetric matrices 229
sturm
Sturm sequence evaluation . . . . . . . . . .
232
givsturm
Givens method using the Sturm sequence . .
232
chcksign
Sign changes in the Sturm sequence . . . . .
232
bound
Calculation of the interval J = [α, β]
. . . .
232
eiglancz
Extremal eigenvalues of a symmetric matrix
234
bisect
Bisection method
. . . . . . . . . . . . . . .
250
chord
The chord method . . . . . . . . . . . . . . .
254
secant
The secant method
. . . . . . . . . . . . . .
255
regfalsi
The Regula Falsi method . . . . . . . . . . .
255
newton
Newton’s method
. . . . . . . . . . . . . . .
255
ﬁxpoint
Fixed-point method . . . . . . . . . . . . . .
260
horner
Synthetic division algorithm
. . . . . . . . .
263
newthorn
Newton-Horner method with reﬁnement . . .
266
mulldeﬂ
Muller’s method with reﬁnement . . . . . . .
269
aitken
Aitken’s extrapolation . . . . . . . . . . . . .
274
adptnewt
Adaptive Newton’s method . . . . . . . . . .
276
newtonxsys
Newton’s method for nonlinear systems . . .
285
broyden
Broyden’s method for nonlinear systems . . .
290
ﬁxposys
Fixed-point method for nonlinear systems . .
293
hookejeeves
The method of Hooke and Jeeves (HJ)
. . .
296
explore
Exploration step in the HJ method
. . . . .
297
backtrackr
Backtraking for line search . . . . . . . . . .
303
lagrpen
Penalty method
. . . . . . . . . . . . . . . .
316
lagrmult
Method of Lagrange multipliers
. . . . . . .
319
interpol
Lagrange polynomial using Newton’s formula 334
dividif
Newton divided diﬀerences . . . . . . . . . .
336
hermpol
Osculating polynomial . . . . . . . . . . . . .
342
par spline
Parametric splines . . . . . . . . . . . . . . .
359
bernstein
Bernstein polynomials . . . . . . . . . . . . .
361
bezier
B´ezier curves . . . . . . . . . . . . . . . . . .
361

Index of MATLAB Programs
645
midpntc
Midpoint composite formula
. . . . . . . . .
375
trapezc
Composite trapezoidal formula . . . . . . . .
376
simpsonc
Composite Cavalieri-Simpson formula . . . .
377
newtcot
Closed Newton-Cotes formulae . . . . . . . .
383
trapmodc
Composite corrected trapezoidal formula
. .
387
romberg
Romberg integration . . . . . . . . . . . . . .
391
simpadpt
Adaptive Cavalieri-Simpson formula . . . . .
397
redmidpt
Midpoint reduction formula . . . . . . . . . .
404
redtrap
Trapezoidal reduction formula . . . . . . . .
404
midptr2d
Midpoint rule on a triangle . . . . . . . . . .
406
traptr2d
Trapezoidal rule on a triangle
. . . . . . . .
406
coeﬂege
Coeﬃcients of Legendre polynomials . . . . .
430
coeﬂagu
Coeﬃcients of Laguerre polynomials . . . . .
430
coefherm
Coeﬃcients of Hermite polynomials . . . . .
430
zplege
Coeﬃcients of Gauss-Legendre formulae . . .
430
zplagu
Coeﬃcients of Gauss-Laguerre formulae . . .
430
zpherm
Coeﬃcients of Gauss-Hermite formulae . . .
430
dft
Discrete Fourier transform
. . . . . . . . . .
439
idft
Inverse discrete Fourier transform . . . . . .
439
ﬀtrec
FFT algorithm in the recursive version
. . .
441
compdiﬀ
Compact diﬀerence schemes
. . . . . . . . .
446
multistep
Linear multistep methods . . . . . . . . . . .
490
predcor
Predictor-corrector scheme . . . . . . . . . .
507
ellfem
Linear FE for two-point BVPs . . . . . . . .
557
femmatr
Construction of the stiﬀness matrix . . . . .
557
H1error
Computation of the H1-norm of the error
.
558
artvisc
Artiﬁcial viscosity . . . . . . . . . . . . . . .
570
sgvisc
Optimal artiﬁcial viscosity
. . . . . . . . . .
570
bern
Evaluation of the Bernoulli function . . . . .
571
thetameth
θ-method for the heat equation . . . . . . . .
592
pardg1cg1
dG(1)cG(1) method for the heat equation . .
596
ipeidg0
dG(0) implicit Euler . . . . . . . . . . . . . .
621
ipeidg1
dG(1) implicit Euler . . . . . . . . . . . . . .
622

Index
A-conjugate directions, 151
A-stability, 481
absolute value notation, 62
adaptive error control, 43
adaptivity, 43
Newton’s method, 275
Runge-Kutta methods, 512
algorithm
Arnoldi, 160, 164
Cuthill-McKee, 98
Dekker-Brent, 256
Remes, 435
synthetic division, 262
Thomas, 91
ampliﬁcation
coeﬃcient, 609
error, 612
analysis
a priori
for an iterative method, 132
a posteriori, 42
a priori, 42
backward, 42
forward, 41
B-splines, 353
parametric, 361
backward substitution, 65
bandwidth, 452
Bernoulli
function, 565
numbers, 389
bi-orthogonal bases, 168
binary digits, 46
boundary condition
Dirichlet, 541
Neumann, 541, 582
Robin, 579
breakdown, 160, 165
B´ezier curve, 360
B´ezier polygon, 359
CFL
condition, 606
number, 606
characteristic
curves, 598
variables, 600
characteristic polygon, 359
chopping, 51

648
Index
cofactor, 9
condition number, 34
asymptotic, 38
interpolation, 332
of a matrix, 36, 58
of a nonlinear equation, 246
of an eigenvalue, 189
of an eigenvector, 190
Skeel, 111
spectral, 59
consistency, 37, 124, 474, 493, 510
convex function, 295, 321
strongly, 312
convex hull, 98
critical point, 295
Dahlquist
ﬁrst barrier, 499
second barrier, 500
decomposition
real Schur, 201, 210, 211
generalized, 225
Schur, 14
singular value, 16
computation of the, 222
spectral, 15
deﬂation, 207, 216, 263
degree
of exactness, 380
of a vector, 160
of exactness, 372, 380, 405,
420
of freedom, 552
determinant of a matrix, 8
discrete
truncation of Fourier series,
417
Chebyshev transform, 426
Fourier transform, 438
Laplace transform, 458
Legendre transform, 428
maximum principle, 567, 611
scalar product, 425
dispersion, 448, 612
dissipation, 612
distribution, 547
derivative of a, 547
divided diﬀerence, 267, 334
domain of dependence, 600
numerical, 606
eigenfunctions, 589
eigenvalue, 12
algebraic multiplicity of an,
13
geometric multiplicity of an,
13
eigenvector, 12
elliptic
operator, 602
equation
characteristic, 12
diﬀerence, 482, 483, 499
heat, 581, 592
error
absolute, 40
cancellation, 39
global truncation, 474
interpolation, 329
local truncation, 474, 605
quadrature, 372
rounding, 45
estimate
a posteriori, 64, 195, 196, 381,
392, 395
a priori, 60, 381, 392, 395
exponential ﬁtting, 565
factor
asymptotic convergence, 125
convergence, 125, 245, 259
growth, 104
factorization
block LU, 94
Cholesky, 80
compact forms, 78
Crout, 78
Doolittle, 78
incomplete, 140
LDMT , 79

Index
649
LU, 68
QR, 82, 209
ﬁll-in, 98, 141
level, 141
ﬁnite diﬀerences, 118, 177, 237,
533
backward, 443
centered, 443, 444
compact, 444
forward, 442
ﬁnite elements, 118, 347
discontinuous, 594, 619
ﬁxed-point iterations, 257
ﬂop, 53
FOM, 163, 164
form
divided diﬀerence, 334
Lagrange, 329
formula
Armijo’s, 304
Goldstein’s, 304
Sherman-Morrison, 95
forward substitution, 65
Fourier coeﬃcients, 436
discrete, 437
function
gamma, 528
Green’s, 532
Haar, 460
stability, 516
weight, 415
Galerkin
ﬁnite element method, 364,
550
stabilized, 568
generalized method, 559
method, 544
pseudo-spectral approximation,
591
Gauss elimination
method, 68
multipliers in the, 69
GAXPY, 77
generalized inverse, 17
Gershgorin circles, 184
Gibbs phenomenon, 439
gradient, 294
graph, 97
oriented, 97, 186
Gronwall lemma, 471, 476
hyperbolic
operator, 602
hypernorms, 63
ILU, 140
inequality
Cauchy-Schwarz, 340, 568
H¨older, 19
Kantorovich, 305
Poincar´e, 536, 569
triangular, 569
Young’s, 544
integration
adaptive, 391
automatic, 391
multidimensional, 402
non adaptive, 391
interpolation
Hermite, 341
in two dimensions, 343
osculatory, 342
piecewise, 338
Taylor, 369
interpolation nodes, 328
piecewise, 345
IOM, 164
Jordan
block, 15
canonical form, 15
kernel of a matrix, 10
Krylov
method, 160
subspace, 159
Lagrange
interpolation, 328
multiplier, 312, 317

650
Index
Lagrangian function, 312
augmented, 317
penalized, 315
Laplace operator, 572
least-squares, 417
discrete, 431
Lebesgue
constant, 331, 332
linear map, 7
linear regression, 433
linearly independent vectors, 3
LU factorization, 72
M-matrix, 29, 145
machine epsilon, 49
machine precision, 51
mass-lumping, 588
matrix, 3
block, 4
companion, 242
convergent, 26
defective, 13
diagonalizable, 15
diagonally dominant, 29, 145
Gaussian transformation, 73
Givens, 206
Hessenberg, 12, 203, 211, 212
Hilbert, 70
Householder, 204
interpolation, 330
irreducible, 185
iteration, 124
mass, 587
norm, 21
normal, 7
orthogonal, 6
permutation, 5
preconditioning, 126
reducible, 185
rotation, 7
similar, 14
stiﬀness, 548
transformation, 204
trapezoidal, 11
triangular, 11
unitary, 7
Vandermonde, 368
matrix balancing, 110
maximum principle, 533, 534
discrete, 29
method
θ−, 584
Regula Falsi, 252
conjugate gradient, 153
Aitken, 272
alternating-direction, 158
backward Euler, 473
backward Euler/centred, 604
BiCG, 171
BiCGSTab, 171
bisection, 248
Broyden’s, 289
CGS, 171
chord, 252, 260
conjugate gradient, 168
with restart, 156
CR, 168
Crank-Nicolson, 473, 593
cyclic Jacobi, 228
damped Newton, 321
damped Newton’s, 308
ﬁnite element, 573
ﬁxed-point, 290
Fletcher-Reeves, 306
forward Euler, 473
forward Euler/centred, 603
forward Euler/uncentred, 603
frontal, 102
Gauss Seidel
symmetric, 133
Gauss-Jordan, 121
Gauss-Seidel, 128
nonlinear, 324
Givens, 230
GMRES, 166
with restart, 166
gradient, 300
Gram-Schmidt, 83
Heun, 473
Horner, 262

Index
651
Householder, 207
inverse power, 195
Jacobi, 127
JOR, 127
Lanczos, 167, 233
Lax-Friedrichs, 603, 608
Lax-Wendroﬀ, 603, 608
Leap-Frog, 604, 611
Merson, 530
modiﬁed Euler, 529
modiﬁed Newton’s, 284
Monte Carlo, 407
Muller, 267
Newmark, 604, 611
Newton’s, 253, 261, 283
Newton-Horner, 263, 264
Nystron, 529
ORTHOMIN, 168
Polak-Ribi´ere, 307
Powell-Broyden
symmetric, 311
power, 192
QMR, 171
QR, 200
with double shift, 218
with single shift, 215, 216
quasi-Newton, 288
reduction formula, 403
Richardson, 136
Richardson extrapolation, 387
Romberg integration, 389, 409
Rutishauser, 202
secant, 252, 257, 288
secant-like, 309
Simplex, 299
SSOR, 134
steepest descent, 305
Steﬀensen, 280
successive over-relaxation, 128
upwind, 603, 607
minimax
property, 418
minimizer
global, 294, 311
local, 294, 311
model
computational, 43
module of continuity, 386
nodes
Gauss, 426
Gauss-Lobatto, 424, 426
norm
absolute, 32
compatible, 21, 22
consistent, 21
energy, 29
equivalent, 20
essentially strict, 432
Frobenius, 22
H¨older, 19
matrix, 21
maximum, 19, 330
spectral, 23
normal equations, 112
numbers
de-normalized, 48
ﬁxed-point, 46
ﬂoating-point, 47
numerical ﬂux, 602
numerical method, 37
adaptive, 43
consistent, 37
convergent, 39
eﬃciency, 44
ill conditioned, 38
reliability, 44
stable, 38
well posed, 38
orbit, 523
overﬂow, 51
P´eclet number, 561
local, 563
Pad´e approximation, 370
parabolic
operator, 602
pattern of a matrix, 97, 575
penalty parameter, 315

652
Index
phase angle, 612
pivoting, 85
complete, 86
partial, 86
Poisson equation, 572
polyalgorithm, 277
polynomial
Bernstein, 359
best approximation, 330, 433
characteristic, 12, 329
Fourier, 435
Hermite, 429
interpolating, 328
Lagrange piecewise, 346
Laguerre, 428
nodal, 329
orthogonal, 415
preconditioner, 126
block, 139
diagonal, 140
ILU, 142
least-squares, 145
MILU, 144
point, 139
polynomial, 145
principal root of unity, 437
problem
Cauchy, 469
generalized eigenvalue, 146, 224,
238, 589
ill posed, 34, 35
ill-conditioned, 34
stiﬀ, 520
well conditioned, 34
well posed, 33
programming
linear, 282
nonlinear, 282, 313
pseudo-inverse, 17, 114
pseudo-spectral
derivative, 449
diﬀerentiation matrix, 449
quadrature formula, 371
Cavalieri-Simpson, 377, 385,
400, 401, 409
composite Cavalieri-Simpson,
377
composite midpoint, 374
composite Newton-Cotes, 383
composite trapezoidal, 376
corrected trapezoidal, 386
Gauss, 421
on triangles, 406
Gauss-Kronrod, 393
Gauss-Lobatto, 422, 425
Gauss-Radau
on triangles, 406
Hermite, 372, 386
Lagrange, 372
midpoint, 373, 385
on triangles, 405
Newton-Cotes, 378
on triangles, 404
pseudo-random, 408
trapezoidal, 375, 385, 438
on triangles, 405
quotient
Rayleigh, 12
generalized, 146
QZ iteration, 225
rank of a matrix, 9
rate
asymptotic convergence, 125
convergence, 259
reduction formula
midpoint, 403
trapezoidal, 404
reference triangle, 345
regularization, 34
representation
ﬂoating-point, 47
positional, 45
residual, 247
resolvent, 35
restart, 164
round digit, 53
rounding, 51

Index
653
roundoﬀunit, 51
rule
Cramer’s, 58
Descartes, 262
Laplace, 9
Runge’s counterexample, 331, 344,
353
SAXPY, 77
saxpy, 77
scalar product, 18
scaling, 110
by rows, 110
Schur
complement, 102
decomposition, 14
semi-discretization, 584, 586
series
Chebyshev, 418
Fourier, 416, 582
Legendre, 419
set
bi-orthogonal, 189
similarity transformation, 14
singular integrals, 398
singular values, 16
space
normed, 19
phase, 523
Sobolev, 543
vector, 1
spectral radius, 13
spectrum of a matrix, 12
spline
cardinal, 351
interpolatory cubic, 349
natural, 349
not-a-knot, 350
one-dimensional, 348
parametric, 358
periodic, 348
splitting, 126
stability
absolute, 479, 480, 499, 502
region of, 480
asymptotic, 471
factors, 42
Liapunov, 471
of interpolation, 332
relative, 502
zero, 477, 495, 502
standard deviation, 298
statistic mean value, 407
stencil, 445
stopping tests, 171, 269
strong formulation, 547
Sturm sequences, 230
subspace
generated, 2
invariant, 13
vector, 2
substructures, 100
Sylvester criterion, 29
system
hyperbolic, 599
strictly, 600
overdetermined, 112
underdetermined, 115
theorem
Abel, 262
Bauer-Fike, 187
Cauchy, 262
Cayley-Hamilton, 13
Courant-Fisher, 146, 233
de la Vall´ee-Poussin, 434
equioscillation, 433
Gershgorin, 184
Ostrowski, 259
polynomial division, 263
Schur, 14
trace of a matrix, 8
transform
fast Fourier, 426
Fourier, 450
Laplace, 455
Zeta, 457
triangulation, 344, 573
underﬂow, 51

654
Index
upwind ﬁnite diﬀerence, 565
weak
formulation, 545
solution, 545, 599
wobbling precision, 49



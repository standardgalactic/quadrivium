Daniel W. Stroock
An Introduction
to Markov Processes
4y Springer

Daniel W. Stroock
MIT
Department of Mathematics, Rm. 272
Massachusetts Ave 77
02139-4307 Cambridge, USA
dws @math.mit.edu
Editorial Board
S. Axler 
F. W. Gehring 
K. A. Ribet
Mathematics Department Mathematics Department 
Mathematics Department
San Francisco 
East Hall 
University of California
State University 
University of Michigan 
at Berkeley
San Francisco, CA 94132 Ann Arbor, MI 48109 
Berkeley, CA 94720-3840
e
d
u 
U S A 
U S A
fgehring@math.lsa.umich.edu 
ribet@math.berkeley.edu
Mathematics Subject Classification (2000): 60-01, 60J10, 60J27
ISSN 0072-5285
ISBN 3-540-23499-3 Springer Berlin Heidelberg New York
Library of Congress Control Number: 20041113930
This work is subject to copyright. All rights are reserved, whether the whole or part of the material is concerned,
specifically the rights of translation, reprinting, reuse of illustrations, recitation, broadcasting, reproduction on
microfilms or in any other way, and storage in databanks. Duplication of this publication or parts thereof is permitted
only under the provisions of the German Copyright Law of September 9, 1965, in its current version, and permission
for use must always be obtained from Springer. Violations are liable for prosecution under the German Copyright Law.
Springer is a part of Springer Science+Business Media
springeronline.com
© Springer-Verlag Berlin Heidelberg 2005
Printed in Germany
The use of general descriptive names, registered names, trademarks, etc. in this publication does not imply, even in the
absence of a specific statement, that such names are exempt from the relevant protective laws and regulations and
therefore free for general use.
Typesetting: Camera-ready by the translator
Cover design: design & production GmbH, Heidelberg
Printed on acid-free paper 
41/3142 XT - 5 4 3 2 1 0

This book is dedicated to my longtime colleague:
Richard A. Holley

Contents
Preface 
xi
Chapter 1 Random Walks A Good Place to Begin 
1
1.1. Nearest Neighbor Random Walks on Z 
1
1.1.1. Distribution at Time n 
2
1.1.2. Passage Times via the Reflection Principle 
3
1.1.3. Some Related Computations 
4
1.1.4. Time of First Return 
6
1.1.5. Passage Times via Functional Equations 
7
1.2. Recurrence Properties of Random Walks 
8
1.2.1. Random Walks on Zd 
9
1.2.2. An Elementary Recurrence Criterion 
9
1.2.3. Recurrence of Symmetric Random Walk in Z2 
11
1.2.4. Transience in Z3 
13
1.3. Exercises 
16
Chapter 2 Doeblin's Theory for Markov Chains 
23
2.1. Some Generalities 
23
2.1.1. Existence of Markov Chains 
24
2.1.2. Transition Probabilities & Probability Vectors 
24
2.1.3. Transition Probabilities and Functions 
26
2.1.4. The Markov Property 
27
2.2. Doeblin's Theory 
27
2.2.1. Doeblin's Basic Theorem 
28
2.2.2. A Couple of Extensions 
30
2.3. Elements of Ergodic Theory 
32
2.3.1. The Mean Ergodic Theorem 
33
2.3.2. Return Times 
34
2.3.3. Identification of n 
38
2.4. Exercises 
40
Chapter 3 More about the Ergodic Theory of Markov Chains 
45
3.1. Classification of States 
46
3.1.1. Classification, Recurrence, and Transience 
46
3.1.2. Criteria for Recurrence and Transience 
48
3.1.3. Periodicity 
51
3.2. Ergodic Theory without Doeblin 
53
3.2.1. Convergence of Matrices 
53

viii 
Contents
3.2.2. Abel Convergence 
55
3.2.3. Structure of Stationary Distributions 
57
3.2.4. A Small Improvement 
59
3.2.5. The Mean Ergodic Theorem Again 
61
3.2.6. A Refinement in The Aperiodic Case 
62
3.2.7. Periodic Structure 
65
3.3. Exercises 
67
Chapter 4 Markov Processes in Continuous Time 
75
4.1. Poisson Processes 
75
4.1.1. The Simple Poisson Process 
75
4.1.2. Compound Poisson Processes on 7Ld 
77
4.2. Markov Processes with Bounded Rates 
80
4.2.1. Basic Construction 
80
4.2.2. The Markov Property 
83
4.2.3. The Q-Matrix and Kolmogorov's Backward Equation 
. . . . 
85
4.2.4. Kolmogorov's Forward Equation 
86
4.2.5. Solving Kolmogorov's Equation 
86
4.2.6. A Markov Process from its Infinitesimal Characteristics 
. . . 
88
4.3. Unbounded Rates 
89
4.3.1. Explosion 
90
4.3.2. Criteria for Non-explosion or Explosion 
92
4.3.3. What to Do When Explosion Occurs 
94
4.4. Ergodic Properties 
95
4.4.1. Classification of States 
95
4.4.2. Stationary Measures and Limit Theorems 
98
4.4.3. Interpreting %a 
101
4.5. Exercises 
102
Chapter 5 Reversible Markov Processes 
107
5.1. Reversible Markov Chains 
107
5.1.1. Reversibility from Invariance 
108
5.1.2. Measurements in Quadratic Mean 
108
5.1.3. The Spectral Gap 
110
5.1.4. Reversibility and Periodicity 
112
5.1.5. Relation to Convergence in Variation 
113
5.2. Dirichlet Forms and Estimation of /3 
115
5.2.1. The Dirichlet Form and Poincare's Inequality 
115
5.2.2. Estimating /?+ 
117
5.2.3. Estimating /?_ 
119
5.3. Reversible Markov Processes in Continuous Time 
120
5.3.1. Criterion for Reversibility 
120
5.3.2. Convergence in L2(TV) for Bounded Rates 
121
5.3.3. £2(-7r)-Convergence Rate in General 
122

Contents 
ix
5.3.4. Estimating A 
125
5.4. Gibbs States and Glauber Dynamics 
126
5.4.1. Formulation 
126
5.4.2. The Dirichlet Form 
127
5.5. Simulated Annealing 
130
5.5.1. The Algorithm 
131
5.5.2. Construction of the Transition Probabilities 
132
5.5.3. Description of the Markov Process 
134
5.5.4. Choosing a Cooling Schedule 
134
5.5.5. Small Improvements 
137
5.6. Exercises 
138
Chapter 6 Some Mild Measure Theory 
145
6.1. A Description of Lebesgue's Measure Theory 
145
6.1.1. Measure Spaces 
145
6.1.2. Some Consequences of Countable Additivity 
147
6.1.3. Generating a-Algebras 
148
6.1.4. Measurable Functions 
149
6.1.5. Lebesgue Integration 
150
6.1.6. Stability Properties of Lebesgue Integration 
151
6.1.7. Lebesgue Integration in Countable Spaces 
153
6.1.8. Fubini's Theorem 
155
6.2. Modeling Probability 
157
6.2.1. Modeling Infinitely Many Tosses of a Fair Coin 
158
6.3. Independent Random Variables 
162
6.3.1. Existence of Lots of Independent Random Variables 
163
6.4. Conditional Probabilities and Expectations 
165
6.4.1. Conditioning with Respect to Random Variables 
166
Notation 
167
References 
168
Index 
169

Preface
To some extent, it would be accurate to summarize the contents of this
book as an intolerably protracted description of what happens when either
one raises a transition probability matrix P (i.e., all entries (P)»j are non-
negative and each row of P sums to 1) to higher and higher powers or one
exponentiates R(P — I), where R is a diagonal matrix with non-negative
entries. Indeed, when it comes right down to it, that is all that is done in
this book. However, I, and others of my ilk, would take offense at such a
dismissive characterization of the theory of Markov chains and processes with
values in a countable state space, and a primary goal of mine in writing this
book was to convince its readers that our offense would be warranted.
The reason why I, and others of my persuasion, refuse to consider the theory
here as no more than a subset of matrix theory is that to do so is to ignore the
pervasive role that probability plays throughout. Namely, probability theory
provides a model which both motivates and provides a context for what we
are doing with these matrices. To wit, even the term "transition probability
matrix" lends meaning to an otherwise rather peculiar set of hypotheses to
make about a matrix. Namely, it suggests that we think of the matrix entry
(P)ij as giving the probability that, in one step, a system in state i will make
a transition to state j . Moreover, if we adopt this interpretation for (P)ij,
then we must interpret the entry (Pn)jj of P n as the probability of the same
transition in n steps. Thus, as n —> oo, P" is encoding the long time behavior
of a randomly evolving system for which P encodes the one-step behavior,
and, as we will see, this interpretation will guide us to an understanding of
limn_>oo(Pn)jj. In addition, and perhaps even more important, is the role
that probability plays in bridging the chasm between mathematics and the
rest of the world. Indeed, it is the probabilistic metaphor which allows one to
formulate mathematical models of various phenomena observed in both the
natural and social sciences. Without the language of probability, it is hard to
imagine how one would go about connecting such phenomena to P n.
In spite of the propaganda at the end of the preceding paragraph, this
book is written from a mathematician's perspective. Thus, for the most part,
the probabilistic metaphor will be used to elucidate mathematical concepts
rather than to provide mathematical explanations for non-mathematical phe-
nomena. There are two reasons for my having chosen this perspective. First,
and foremost, is my own background. Although I have occasionally tried to
help people who are engaged in various sorts of applications, I have not accu-
mulated a large store of examples which are easily translated into terms which
are appropriate for a book at this level. In fact, my experience has taught
me that people engaged in applications are more than competent to handle
the routine problems which they encounter, and that they come to someone
like me only as a last resort. As a consequence, the questions which they

xii 
Preface
ask me tend to be quite difficult and the answers to those few which I can
solve usually involve material which is well beyond the scope of the present
book. The second reason for my writing this book in the way that I have
is that I think the material itself is of sufficient interest to stand on its own.
In spite of what funding agencies would have us believe, mathematics qua
mathematics is a worthy intellectual endeavor, and I think there is a place
for a modern introduction to stochastic processes which is unabashed about
making mathematics its top priority.
I came to this opinion after several semesters during which I taught the
introduction to stochastic processes course offered by the M.I.T. department
of mathematics. The clientele for that course has been an interesting mix of
undergraduate and graduate students, less than half of whom concentrate in
mathematics. Nonetheless, most of the students who stay with the course
have considerable talent and appreciation for mathematics, even though they
lack the formal mathematical training which is requisite for a modern course
in stochastic processes, at least as such courses are now taught in mathematics
departments to their own graduate students. As a result, I found no ready-
made choice of text for the course. On the one hand, the most obvious choice is
the classic text A First Course in Stochastic Processes, either the original one
by S. Karlin or the updated version [4] by S. Karlin and H. Taylor. Their book
gives a no nonsense introduction to stochastic processes, especially Markov
processes, on a countable state space, and its consistently honest, if not al-
ways easily assimilated, presentation of proofs is complemented by a daunting
number of examples and exercises. On the other hand, when I began, I feared
that adopting Karlin and Taylor for my course would be a mistake of the same
sort as adopting Feller's book for an undergraduate introduction to probabil-
ity, and this fear prevailed the first two times I taught the course. However,
after using, and finding wanting, two derivatives of Karlin's classic, I took the
plunge and assigned Karlin and Taylor's book. The result was very much the
one which I predicted: I was far more enthusiastic about the text than were
my students.
In an attempt to make Karlin and Taylor's book more palatable for the
students, I started supplementing their text with notes in which I tried to
couch the proofs in terms which I hoped they would find more accessible, and
my efforts were rewarded with a quite positive response from my students.
In fact, as my notes became more and more extensive and began to diminish
the importance of the book, I decided to convert them into what is now this
book, although I realize that my decision to do so may have been stupid. For
one thing, the market is already close to glutted with books which purport
to cover this material. Moreover, some of these books are quite popular, al-
though my experience with them leads me to believe that their popularity
is not always correlated with the quality of the mathematics they contained.
Having made that pejorative comment, I will not make public which are the
books which led me to this conclusion. Instead, I will only mention the books
on this topic, besides Karlin and Taylor's, which I very much liked. Namely,

Preface 
xiii
J. Norris's book [5] is an excellent introduction to Markov processes which,
at the same time, provides its readers with a good place to exercise their
measure-theoretic skills. Of course, Norris's book is only appropriate for stu-
dents who have measure-theoretic skills to exercise. On the other hand, for
students who possess those skills, Norris's book is a place where they can
see measure theory put to work in an attractive way. In addition, Norris
has included many interesting examples and exercises which illustrate how
the subject can be applied. The present book includes most of the math-
ematical material contained in [5], but the proofs here demand much less
measure theory than his do. In fact, although I have systematically employed
measure theoretic terminology (Lebesgue's Dominated Convergence Theorem,
the Monotone Convergence Theorem, etc.), which is explained in Chapter 6,
I have done so only to familiarize my readers with the jargon which they will
encounter if they delve more deeply into the subject. In fact, because the
state spaces in this book are countable, the applications which I have made of
Lebesgue's theory are, with one notable exception, entirely trivial. The one
exception, which is made in § 6.2, is that I have included a proof that there
exist countably infinite families of mutually independent random variables.
Be that as it may, the reader who is ready to accept that such families exist
has no need to consult Chapter 6 except for terminology and the derivation of
a few essentially obvious facts about series. For more advanced students, an
excellent treatment of Markov chains on a general state space can be found
in the book [6] by D. Revuz.
The organization of this book should be more or less self-evident from the
table of contents. In Chapter 1, I give a bare hands treatment of the basic
facts, with particular emphasis on recurrence and transience, about nearest
neighbor random walks on the square, d-dimensional lattice Zd. Chapter 2
introduces the study of ergodic properties, and this becomes the central theme
which ties together Chapters 2 through 5. In Chapter 2, the systems under
consideration are Markov chains (i.e., the time parameter is discrete), and the
driving force behind the development there is an idea which was introduced
by Doeblin. Restricted as the applicability of Doeblin's idea may be, it has
the enormous advantage over the material in Chapters 3 and 4 that it provides
an estimate on the rate at which the chain is converging to its equilibrium
distribution. After giving a reasonably thorough account of Doeblin's theory,
in Chapter 3 I study the ergodic properties of Markov chains which do not
necessarily satisfy Doeblin's condition. The main result here is the one sum-
marized in equation (3.2.15). Even though it is completely elementary, the
derivation of (3.2.15), is, without doubt, the most demanding piece of analy-
sis in the entire book. So far as I know, every proof of (3.2.15) requires work
at some stage. In supposedly "simpler" proofs, the work is hidden elsewhere
(either measure theory, as in [5] and [6], or in operator theory, as in [2]). The
treatment given here, which is a re-working of the one in [4] based on Feller's
renewal theorem, demands nothing more of the reader than a thorough un-
derstanding of arguments involving limits superior, limits inferior, and their

xiv 
Preface
role in proving that limits exist. In Chapter 4, Markov chains are replaced by
continuous-time Markov processes (still on a countable state space). I do this
first in the case when the rates are bounded and therefore problems of possible
explosion do not arise. Afterwards, I allow for unbounded rates and develop
criteria, besides boundedness, which guarantee non-explosion. The remainder
of the chapter is devoted to transferring the results obtained for Markov chains
in Chapter 3 to the continuous-time setting. Aside from Chapter 6, which is
more like an appendix than an integral part of the book, the book ends with
Chapter 5. The goal in Chapter 5 is to obtain quantitative results, reminis-
cent of, if not as strong as, those in Chapter 2, when Doeblin's theory either
fails entirely or yields rather poor estimates. The new ingredient in Chapter
5 is the assumption that the chain or process is reversible (i.e., the transition
probability is self-adjoint in the Z2-space of its stationary distribution), and
the engine which makes everything go is the associated Dirichlet form. In
the final section, the power of the Dirichlet form methodology is tested in an
analysis of the Metropolis (a.k.a. as simulated annealing) algorithm. Finally,
as I said before, Chapter 6 is an appendix in which the ideas and terminol-
ogy of Lebesgue's theory of measure and integration are reviewed. The one
substantive part of Chapter 6 is the construction, alluded to earlier, in § 6.2.1.
Finally, I have reached the traditional place reserved for thanking those
individuals who, either directly or indirectly, contributed to this book. The
principal direct contributors are the many students who suffered with various
and spontaneously changing versions of this book. I am particularly grateful
to Adela Popescu whose careful reading brought to light many minor and a few
major errors which have been removed and, perhaps, replaced by new ones.
Thanking, or even identifying, the indirect contributors is trickier. Indeed,
they include all the individuals, both dead and alive, from whom I received
my education, and I am not about to bore you with even a partial list of who
they were or are. Nonetheless, there is one person who, over a period of more
than ten years, patiently taught me to appreciate the sort of material treated
here. Namely, Richard A. Holley, to whom I have dedicated this book, is a
true probabilist. To wit, for Dick, intuitive understanding usually precedes
his mathematically rigorous comprehension of a probabilistic phenomenon.
This statement should lead no one to to doubt Dick's powers as a rigorous
mathematician. On the contrary, his intuitive grasp of probability theory not
only enhances his own formidable mathematical powers, it has saved me and
others from blindly pursuing flawed lines of reasoning. As all who have worked
with him know, reconsider what you are saying if ever, during some diatribe
into which you have launched, Dick quietly says "I don't follow that."
In addition to his mathematical prowess, every one of Dick's many students
will attest to his wonderful generosity. I was not his student, but I was
his colleague, and I can assure you that his generosity is not limited to his
students.
Daniel W. Stroock, August 2004

CHAPTER 1
Random Walks
A Good Place to Begin
The purpose of this chapter is to discuss some examples of Markov processes
which can be understood even before the term "Markov process" is. Indeed,
anyone who has been introduced to probability theory will recognize that
these processes all derive from consideration of elementary "coin tossing."
1.1 Nearest Neighbor Random Walks on Z
Let p be a fixed number from the open interval (0,1), and suppose that1
{Bn : n € Z+} is a sequence of {—1, l}-valued, identically distributed Bernoul-
li random variables2 which are 1 with probability p. That is, for any n G Z+
P(Si = ei,..., Bn = en) = p»Wqn-"W 
where q = 1 - p and
iV(£) = #{m : em = 1} = n + ^ E ) 
when Sn(E) = ^ em.
2 
l
Next, set
n
(1.1.2) 
Xo = 0 and Xn = ^ 
Bm for n G Z+.
m=l
The existence of the family {£>„ : n G Z+} is the content of § 6.2.1.
The above family of random variables {Xn : n E N} is often called a nearest
neighbor random walk on Z. Nearest neighbor random walks are examples
of Markov processes, but the description which we have just given is the
one which would be given in elementary probability theory, as opposed to a
course, like this one, devoted to stochastic processes. Namely, in the study
of stochastic processes the description should emphasize the dynamic aspects
1 Z is used to denote the set of all integers, of which N and Z+ are, respectively, the non-
negative and positive members.
2 For historical reasons, mutually independent random variables which take only two values
are often said to be Bernoulli random variables.

2 
1 RANDOM WALKS
of the family. Thus, a stochastic process oriented description might replace
(1-1-2) by
p(X0 = 0) = 1 and
i if e = 1
X0,...,-..-., 
, q 
. u = _ h
where P(Xra — Xra_i = e Xo,... ,Xra_i) denotes the conditional probability
(cf. §6.4.1) that Xn -X r a_: = e given CT({X0, ... ,Xn_i}). Notice that (1.1.3)
is indeed more dynamic a description than the one in (1.1.2). Specifically, it
says that the process starts from 0 at time n = 0 and proceeds so that, at
each time n € Z+, it moves one step forward with probability p or one step
backward with probability q, independent of where it has been before time n.
1.1.1. Distribution at Time n: In this subsection, we will present two
approaches to computing P(Xn = TO). The first computation is based on the
description given in (1.1.2). Namely, from (1.1.2) it is clear that P(|Xn| <
n) = 1. In addition, it is clear that
n odd => P(Xn is odd) = 1 and n even => P(Xn is even) = 1.
Finally, given TO 6 {—n,... ,n} with the same parity as n and a string E =
(ei,...,en) e {-l,l} n with (cf. (1.1.1)) Sn(E) = TO, N(E) = ^ 
and so
Hence, because, when (fe) = fc,,^_fcs, is the binomial coefficient u£ choose fc,"
there are (m+n) such strings E, we see that
(1.1.4)
if TO s Z, |TO| < n, and TO has the same parity as n
and is 0 otherwise.
Our second computation of the same probability will be based on the more
dynamic description given in (1.1.3). To do this, we introduce the notation
(Pn)m = P(Xn = fri)- Obviously, (P°)m = <5o,m> where 8k,e is the Kronecker
symbol which is 1 when k = £ and 0 otherwise. Further, from (1.1.3), we see
that P(Xn = TO) equals
P(Xn_i = TO - 1 & Xn = TO) + P(Xn_i = TO + 1 & Xn = TO)
= pP(Xra_i = T O - 1 ) +qF(Xn_1 =TO + 1).
That is,
(1.1.5) 
(P°)m = 60,m 
and 
1
1

1.1 Nearest Neighbor Random Walks on Z 
3
Obviously, (1.1.5) provides a complete, albeit implicit, prescription for com-
puting the numbers (Pn)m, and one can easily check that the numbers given
by (1.1.4) satisfy this prescription. Alternatively, one can use (1.1.5) plus in-
duction on n to see that {Pn)m — 0 unless m = 2£ — n for some 0 < £ < n and
that (Cn)j> = (C"){_i + (C")^i when (Cn)e = p-eqn~e(Pn)2i_n. 
In other
words, the coefficients {(Cn)i : n£NSzO<£<n} 
are given by Pascal's
triangle and are therefore the binomial coefficients.
1.1.2. Passage Times via the Reflection Principle: More challenging
than the computation in §1.1.1 is finding the distribution of the first passage
time to a point a G Z. That is, given a £ Z \ {0}, set3
(1.1.6) 
Ca = inf{n > 1 : Xn = a} (= oo when Xn ^ a for any n > 1).
Then (a is the first passage time to a, and our goal here is to find its distribu-
tion. Equivalently, we want an expression for P(£a = n), and clearly, by the
considerations in §1.1.1, we need only worry about n's which satisfy n > \a
and have the same parity as a.
Again we will present two approaches to this problem, here based on (1.1.2)
and in §1.1.5 on (1.1.3). To carry out the one based on (1.1.2), assume that
a £ Z +, suppose that n G Z + has the same parity as a, and observe first that
P(Ca = n) = F(Xn =a&Ca > n - 1) = PF((a > n - 1 & Xn^ 
= a - l).
Hence, it suffices for us to compute P(Co > n — I $z Xn_\ = a — l). For this
purpose, note that for any E £ {—1,1}""1 with Sn-\{E) 
= a — 1, the event
{(i?i,... ,Bn_i) = E} has probability p~ 
1q~^~. Thus,
(*) 
P(Ca = n) = Af(n, 
d)p^i^
where Af(n,a) is the number of E £ {—ljl}""1 with the properties that
Se(E) < a - 1 for 0 < t < n - 1 and Sn-i(E) 
= a - 1. That is, everything
comes down to the computation of Af(n,a). 
Alternatively, since Af(n,a) =
(2±r^1)-A/''(n,a), where A/7(n, a) is the number of E £ { - M } " " 1 such that
Sn-i(E) 
= a — 1 and Sg.{E) > a for some £ < n — 1, we need only compute
Af'(n,a). 
For this purpose we will use a beautiful argument known as the
reflection principle. Namely, consider the set P(n, a) of paths (So, • • •, SVi-i) 6
Zn with the properties that So = 0, Se - 5m_i £ {-1,1} for 1 < m < n - 1,
and Sm > a for some 1 < m < n — 1. Clearly, Af'(n,a) is the numbers of
paths in the set L(n, a) consisting of those (So, • • •, Sn-i) £ P(n, a) for which
Sn-i = a — 1, and, as an application of the reflection principle, we will show
that the set L(n, a) has the same number of elements as the set U(n, a) whose
elements are those paths (So,- • • ,Sn-i) 
e P(n,a) for which Sn-i = a + 1.
Since (SQ,..., Sn-i) £ U(n, a) if and only if So = 0, Sm - Sm^i G {-1,1}
1 As the following indicates, we take the infemum over the empty set to be +oo.

4 
1 RANDOM WALKS
for all 1 < m < n — 1, and Sn-\ = a + 1, we already know how to count
them: there are ("+<>) of them. Hence, all that remains is to provide the
advertised application of the reflection principle. To this end, for a given
5 = (5 0,..., Sn-i) e P(n, a), let £(S) be the smallest 0 < k < n - 1 for which
Sk > a, and define the reflection 5H(S) = (So,..., Sn-i) of S so that Sm = Sm
if 0 < TO < g(S) and Sk = 2a - Sk if ^(S) < m < n - 1. Clearly, 9t maps
L(n,a) into U(n,a) and U(n,a) into L(n,a). In addition, 9^ is idempotent:
its composition with itself is the identity map. Hence, as a map from £(n, a)
to U(n, a), fH it must be both one-to-one and onto, and so L(n, a) and U{n, a)
have the same numbers of elements.
We have now shown that Af'(n,a) = f"+a) and therefore that
Finally, after plugging this into (*), we arrive at
n - l \ 
fn-l
— n> — ii n+a 
i / 
I 
n+a
2 
L
which simplifies to the remarkably simple expression
Ca = n) = I ( i ) P ^ ^ = ^P(Xn = a).
The computation when a < 0 can be carried out either by repeating the
argument just given or, after reversing the roles of p and q, applying the
preceding result to —a . However one arrives at it, the general result is that
a
(1.1.7) 
a * 0 => F((a = n) = f 
^ 
p^q^ 
= -F(Xn 
= a)
for n > \a\ with the same parity as a and is 0 otherwise.
1.1.3. Some Related Computations: Although the formula in (1.1.7) is
elegant, it is not particularly transparent. In particular, it is not at all evident
how one can use it to determine whether P(C« < oo) = 1. To carry out this
computation, let a > 0 be given, and write of £a = fa(Bi,..., 
Bn,...), 
where
fa is the function which maps {—1,1}Z into Z+ U {oo} so that, for each
/ a(ei,..., en,...) > n <=> > €£ < a for 1 < m < n.
Because the event {£a = TO} depends only on (Bi,..., Bm) and
(118) 
(a = m = > Ca+1 = TO + Cl o E™
where Ci o S m = / i ( - B m + i , . . . , B m + n , . . . ) ,

1.1 Nearest Neighbor Random Walks on Z 
5
{Ca = rn & Co+i < 00} = {Co = m} n {Ci o E r a < 00}, and {Co = rn} is
independent of {Ci o Em < 00}. In particular, this leads to
00
P(Co+l < 00) = Y, P(Ca = m & Ca+1 < OO)
m=l
00
= £ P(Ca = m)P(Ci o Sm < 00)
m=l
00
= P(Cl < CO) Y, 
F(Ca = m) = P(Cl < 00)P(Co < 00),
m=l
since (Bm+\,..., £?m_j_n,...) and (£?i,..., Z?n,...) have the same distribution
and therefore so do Ci ° S m and Ci • The same reasoning applies equally well
when a < 0, only now with —1 playing the role of 1. In other words, we have
proved that
(1.1.9) 
P(Ca<oo) = P(Csgn(a)<oo)H 
foraeZ\{0},
where sgn(a), the signum of a, is 1 or —1 according to whether a > 0 or
a < 0. In particular, this shows that P(Ci < 00) = 1 => P(Co < 00) = 1 and
P(C-i < 00) = 1 = » P(C-o < 00) = 1 for all a e Z+.
In view of the preceding, we need only look at P(Ci < 00). Moreover, by
the Monotone Convergence Theorem, Theorem 6.1.9,
00
P(Ci < 00) = limE|sCll = UrnVs^-'PfG = 2 n - l ) .
Applying (1.1.7) with a = 1, we know that
1 
/ 2 n - l
Next, note that
' 2 7 1 - 1
71 / 
n\{n-L)l 
nl 
m_1
_
n! m = 1
where4, for any a £ R ,
'a\ 
f 1 
if 71 = 0
71
1 In the preceding, we have adopted the convention that TJ _fc a^ = 1 if £ < k.

6 
1 RANDOM WALKS
is the generalized binomial coefficient which gives the coefficient of xn in the
Taylor's expansion of (1 + x)a around x — 0. Hence,
n=\ 
^ n=l
and so
(1.1.10) 
E[«Ci] = i ^ Z M f ! 
for|s|<l.
Of course, by symmetry, one can reverse the roles of p and g to obtain
(1.1.11) 
E[aC-i] = ! VI 4W* 
for|s|<1.
By letting s /* 1 in (1.1.10) and noting that 1 — 4pq = (p + q)2 - Apq
> — q)2, we see that5
2q 
q
and so
iip>q
Of course, P(C-i < CXD) is given by the same formula, only with the roles of p
and q reversed. Thus,
(I 
if a eZ+ kp>qov -a£Z+ kp<q
(1.1.12) 
P(C < oo) = ^
1.1.4. Time of First Return: Having gone to so much trouble to arrive at
(1.1.12), it is only reasonable to draw from it a famous conclusion about the
recurrence properties of nearest neighbor random walks on Z. Namely, let
p0 = inf{n > 1 : Xn = 0} (= oo if Xn ^ 0 for all n > 1)
be the time of first return to 0. Then, by precisely the same sort of reasoning
which allowed us to arrive at (1.1.9), we see that P(Xi = 1 & p$ < oo) =
C-i < oo) and P(Xi = - 1 & p0 < oo) = qP(Ci < oo), and so, by (1.1.12),
(1.1.13)
' We use a A b to denote the minimum min{o, b} of a, 6 6

1.1 Nearest Neighbor Random Walks on Z 
7
In other words, the random walk {Xn : n > 0} will return to 0 with probability
1 if and only if it is symmetric in the sense that p = \.
By sharpening the preceding a little, one sees that P(XL — 1 & po — 2n) =
pP(C-i = 2n - 1) and P(Xi = - 1 & /90 = 2n) = #(Ci = 2n - 1), and so, by
(1.1.10) and (1.1.11),
(1.1.14) 
E[sPo] = 1 - -y/1 -Apqs2 
for |s| < 1.
Hence,
E[po/°] = s^E[S"°] 
= 
4pqs 
for |s| < 1,
1
1 
ds 
l 
J 
^/l - 4pgs2
and therefore, since6 E[p0sPo] Z1 E[po, Po < oo] as s /" 1,
4?7/7
E[po, Po < oo] = 
,
which, in conjunction with (1.1.13), means that7
(1.1.15) 
E [ A ) | A ) < o 
^ 
^
' " ^ 
i~\p~q\~ 
' 
\p~q\-
The conclusions drawn in the preceding provide significant insight into the
behavior of nearest neighbor random walks on Z. In the first place, they say
that when the random walk is symmetric, it returns to 0 with probability 1
but the expected amount of time it takes to do so is infinite. Secondly, when
the random walk is not symmetric, it will, with positive probability, fail to
return. On the other hand, in the non-symmetric case, the behavior of the
trajectories is interesting. Namely, (1.1.13) in combination with (1.1.15) say
that either they fail to return at all or they return relatively quickly.
1.1.5. Passage Times via Functional Equations: We close this discus-
sion of passage times for nearest neighbor random walks with a less computa-
tional derivation of (1.1.10). For this purpose, set ua(s) = E[s^°] for a G Z\{0}
and s G (—1,1). Given a G Z +, we use the ideas in §1.1.3, especially (1.1.8),
to arrive at
oo 
oo
„ 
(„) _ V ^ mmr Ci°£m (• — m] — V ^ „
^OT1\^/ 
/ 
^ 
Ji-J 
o 
, 
l^(j 
111/] 
7 
O
m=l 
m=l
5(Ca = m)ui(s) = 
ua(s)ui(s).
oo£
m=l
6 When X is a random variable and A is an event, we will often use E[X, A] to denote
E[X1^].
7 a V 6 is used to denote the maximum max{a, 6} of a, 6 € M.

8 
1 RANDOM WALKS
Similarly, if —a € Z+, then ua-\{s) = ua(s)u-i(s). 
Hence
(1.1.16) 
ua(s) = Msgn(a)(s)|a| foraeZ\{0} and \s\ < 1.
Continuing with the same line of reasoning and using (1.1.16) with a = 1, we
also have
ui(s) =E[sCl,X1 = 1] +E[s c\ Xx = -1]
= ps + gsE[s&oS , X\ = -l] = ps + qsu2(s) = ps + qsu1(s)2.
Hence, by the quadratic formula,
Because
is odd) = 1, ui(—s) = —ui(s). At the same time,
U) =
1 + \/l — Apqs2 
1 + A/1 - 4pg pVg
^_ -L-
Hence, since s e (0,1) => u\(s) < 1, we can eliminate the "+" solution and
thereby arrive at a second derivation of (1.1.10). In fact, after combining this
with (1.1.16), we have shown that
(1.1.17) 
E[s
Ca] = <
/l-Apqs2
2qs
/l-4pqs2
2ps
if a e Z+
if - a G Z+
for \s\ < 1.
1.2 Recurrence Properties of Random Walks
In §1.1.4, we studied the time po of first return of a nearest neighbor random
walk to 0. As we will see in Chapters 2 and 3, times of first return are critical
(cf. §2.3.2) for an understanding of the long time behavior of random walks
and related processes. Indeed, when the random walk returns to 0, it starts
all over again. Thus, if it returns with probability 1, then the entire history of
the walk will consist of epochs, each epoch being a sojourn which begins and
ends at 0. Because it marks the time at which one epoch ends and a second,
identically distributed, one begins, a time of first return is often called a
recurrence time, and the walk is said to be recurrent if P(po < °°) = 1- Walks
which are not recurrent are said to be transient.
In this section, we will discuss the recurrence properties of nearest neigh-
bor random walks. Of course, we already know (cf. (1.1.13)) that a nearest
neighbor random on Z is recurrent if and only if it is symmetric. Thus, our
interest here will be in higher dimensional analogs. In particular, in the hope
that it will be convincing evidence that recurrence is subtle, we will show that
the recurrence of the nearest neighbor, symmetric random walk on Z persists
when Z is replaced by Z2 but disappears in Z3.

1.2 Recurrence Properties of Random Walks 
9
1.2.1. Random Walks on Zd: To describe the analog on 1d of a nearest
neighbor random walk on Z, we begin by thinking of Ni = { — 1,1} as the
set of nearest neighbors in Z of 0. It should then be clear why the set of
nearest neighbors of the origin in Zd consists of the 2d points in Zd for which
(d — 1) coordinates are 0 and the remaining coordinate is in Ni. Next, we re-
place the Ni-valued Bernoulli random variables in §1.1 by their d-dimensional
analogs, namely: independent, identically distributed N^-valued random vari-
ables Bi,..., B n,... .8 Finally, a nearest neighbor random walk on Zd is a
family {Xn : n > 0} of the form
n
Xo = 0 and Xn = J ^ B m 
for n > 1.
m=l
The equivalent, stochastic process oriented description of {Xn : n > 0} is
P(X0 = 0) = 1 and, for n > 1 and e € Nd,
P(Xn — Xn-i = e I Xo,..., Xn_iJ = p6,
where pe = P(Bi = e). When Bi is uniformly distributed on Nd, the random
walk is said to be symmetric.
In keeping with the notation and terminology introduced above, we define
the time po of first return to the origin equal to n if n > 1, Xra = 0, and
XTO j^ 0 for 1 < m < n, and we take po = oo if no such n > 1 exists.
Also, we will say that the walk is recurrent or transient according to whether
P(po < oo) is 1 or strictly less than 1.
1.2.2. An Elementary Recurrence Criterion: Given n > 1, let PQ be
the time of the nth return to 0. That is, p£' — p0 and, for n > 2,
p£-V < oo = ^ pM = inf{TO > p(n-D : X m = 0}
and P(Q"1] = oo => po
n) = oo. Equivalently, if g : (Nd)z+ —> Z+ U {ex)} is
determined so that
m
g(e\,... eg,...) > n if 22ei ¥" ° for 1 < m < n,
then po = <?(Bi,..., B^,...), and pQ = m =^> pQ
n 
= m + po o S m where
Po ° S m is equal to g(Bm+i,..., Bm+£,...). In particular, this leads to
oo
'"+1> < oo) = ^2 F(P(on) =mk 
p0oT,m <oo)
= F(po
n) < oo)¥{p0 < oo),
8 The existence of the Bn's can be seen as a consequence of Theorem 6.3.2. Namely, let
{Un : n £ Z+} be a family of mutually independent random variables which are uniformly
distributed on [0,1). Next, let (ki,...,k2d) be an ordering of the elements of N^, set
/?o = 0 and pm = Y17=i P^ B l = 
k^ 
f o r l - 
m - 
2d' d e f i n e F '• [°> !) —" Nd s o 
t h a t
F \ [0m-i,0m) = km, and set Bn = F{Un).

10 
1 RANDOM WALKS
since {p0 = m} depends only on (Bi,..., Bm), and is therefore independent
of po ° ^m> and the distribution of po ° S m is the same as that of po- Thus,
we have proved that
(1.2.2) 
P(po
n) < oo) = P(p0 < oo)n for n > 1.
One dividend of (1.2.2) is that it supports the epochal picture given above
about the structure of recurrent walks. Namely, it says that if the walk returns
once to 0 with probability 1, then, with probability 1, it will do so infinitely
often. This observation has many applications. For example, it shows that if
the mean value of ith coordinate of Bi is different from 0, then {Xn : n > 0}
must be transient. To see this, use Yn to denote the ith coordinate of B n,
and observe that {Yn — Yn_i : n > 1} is a sequence of mutually independent,
identically distributed {—1,0,1}-valued random variables with mean value
fi T<= 0. But, by the Strong Law of Large Numbers (cf. Exercise 1.3.4 below),
this means that ^ 
—> /i / 0 with probability 1, which is possible only
if |Xn| > \Yn\ —> oo with probability 1, and clearly this eliminates the
possibility that, even with positive probability, Xn = 0 infinitely often.
A second dividend of (1-2.2) is the following. Define
To =
n=0
to be the total time that {X.n : n > 0} spends at the origin. Since Xo = 0,
To > 1. Moreover, for n > 1, To > n <=^> po
n) < oo. Hence, by (1.2.2),
E[T0] = 2 ^ P(T0 > n) = 1 + }_^ IP(Po < oo) = 1 + 2 ^ P(p0 <
n=0 
n=l 
n=l
and so
1 
1
(1.2.3) 
E[T0] = —P(p0<oo)
Before applying (1.2.3) to the problem of recurrence, it is interesting to
note that To is a random variable for which the following peculiar dichotomy
holds:
P(T0 < oo) > 0 => E[T0] < oo
1 ' ' ] 
E[T0] - oo => P(T0 = oo) = 1.
Indeed, if P(To < oo) > 0, then, with positive probability, Xn cannot be 0
infinitely often and so, by (1.1.13), P(po < oo) < 1, which, by (1.2.3), means
that E[T0] < oo. On the other hand, if E[T0] = oo, then (1.2.3) implies that
P(p0 < oo) = 1 and therefore, by (1.2.2), that P(T0 > n) = P(po
n) < oo) = 1
for all n > 1. Hence (cf. (6.1.3)), P(T0 = oo) = 1.

1.2 Recurrence Properties of Random Walks 
11
1.2.3. Recurrence of Symmetric Random Walk in Z2: The most fre-
quent way that (1.2.3) gets applied to determine recurrence is in conjunction
with the formula
oo
(1.2.5) 
E[T0] = E F(Xn = °)-
n=0
Although the proof of (1.2.5) is essentially trivial (cf. Theorem 6.1.15):
E[T0] = E 
_
.n=0 
J 
n=0 
n=0
in conjunction with (1.2.3) it becomes powerful. Namely, it says that
oo
(1.2.6) 
{Xn : n > 0} is recurrent if and only if E 
FP^n = 0) = oo,
n=0
and, since P(Xn = 0) is more amenable to estimation than quantities which
involve knowing the trajectory at more than one time, this is valuable infor-
mation.
In order to apply (1.2.6) to symmetric random walks, it is important to
know that when the walk is symmetric, then 0 is the most likely place for the
walk to be at any even time. To verify this, note that if k e Zd,
n = 
*-£)
Xn = £)P(X2n - X,, = k - I) =
where, in the passage to the last line, we have applied Schwarz's inequality (cf.
Exercise 1.3.1 below). Up to this point we have not used symmetry. However,
if the walk is symmetric, then P(Xn = £) = P(Xn = —£), and so the last line
of the preceding can by continued as
= E 
P(Xn = 0P(X2n " Xn = -t) = P(X2n = 0).
eezd
Thus,
(1.2.7) 
{Xn : n > 0} symmetric ==> F(X2n = 0) = maxP(X2n = k).

12 
1 RANDOM WALKS
To develop a feeling for how these considerations get applied, we begin
by using them to give a second derivation of the recurrence of the nearest
neighbor, symmetric random walk on Z. For this purpose, note that, because
< n) = 1, (1.2.7) implies that
In
1 = Y, 
¥(X2n = e)< (4n + l)P(X2n = 0),
i=-2n
and therefore, since the harmonic series diverges, that Y^=o ^(Xn = 0) = oo.
The analysis for the symmetric, nearest neighbor random walk in Z2 re-
quires an additional ingredient. Namely, the d-dimensional analog of the
preceding line of reasoning would lead to P(X2n = 0) > (4n + l)~d, which
is inconclusive except when d = 1. In order to do better, we need to use the
fact that
(1.2.8) 
{Xn : n > 0} symmetric => E[|XJ 2] = n.
To prove (1.2.8), note that each coordinate of B n is a random variable with
mean value 0 and variance ^- Hence, because the Bn's are mutually indepen-
dent, the second moment of each coordinate of Xra is ^.
Knowing (1.2.8), Markov's inequality (6.1.12) says that
|X2nj > 2Vn) < ^E[|X 2 n|] = i,
which allows us to sharpen the preceding argument to give9
2 - 
vi -
< {Ay/n + l)dP(X2n = 0) < 2d~1(Ani + l)P(X2n = 0).
That is, we have now shown that
(1.2.9) 
P(X2n = 0) >2-d(4ni 
+l)^
for the symmetric, nearest neighbor random walk on Zd. In particular, when
d = 2, this proves that the symmetric, nearest neighbor random walk on Z2 is
recurrent.
9 For any a, b G [0, oo) and p G [l,oo), (a + b)P < 2P~1(aP + V). This can be seen as
an application of Jensen's inequality (cf. Exercise 5.6.2), which, in this case, is simply the
statement that x G [0, oo) >—* xp is convex.

1.2 Recurrence Properties of Random Walks
13
1.2.4. Transience in Z3: Although (1.2.9) was sufficient to prove recurrence
for the symmetric, nearest neighbor random walk in Z2, it only leaves open
the possibility of transience in 7Ld for d > 3. Thus, in order to nail down the
question when d > 3, we will need to see how good an estimate (1.2.9) really
is. In particular, it would suffice to prove that there is an upper bound of the
same form.
To get an upper bound which complements the lower bound in (1.2.9), we
first do so in the case when d ~ 1. For this purpose, let 0 < £ < n be given,
and observe that
P(X2n = It)
(n!!)2
n(n - 1) • • • (n - I + 1)
i 1
£)\(n~£)\
£
£-k
(
I
1 "
Now recall that
(1.2.10)
log(l -x) = -^2 
for \x
m = l
m
and therefore that log(l — x) > — ^f for 0 < x < |. Hence, the preceding
shows that
P(X2n = 2£)
P(X2n = 0) >
> e
as long as 0 < £ < ^ . Because ¥{X2n = -2t) - W(X2n = 2t), we can now
say that
F(X2n = 0) < eip(X2n = 21) for \£\ < y/K.
But, because
e5, and so
(1.2.11)
= 2t) = 1, this means that (2^/n - l)¥(X2n = 0) <
" 1
P(X2n = 0) <e5(2Vn-l)" 1, 
n > 1,
when {Xn : n > 0} is the symmetric, nearest neighbor random walk on Z.
If, as they most definitely are not, the coordinates of the symmetric, nearest
neighbor random walk were independent, then (1.2.11) would yield the sort
of upper bound for which we are looking. Thus it is reasonable to examine
to what extent we can relate the symmetric, nearest neighbor random walk
on Zd to d mutually independent symmetric, nearest neighbor random walks
{Xi<n : n > 0}, 1 < i < d, on Z. To this end, refer to (1.2.1) which p€ = ^ ,
and think of choosing Xn — Xn_i in two steps: first choose the coordinate
which is to be non-zero and then choose whether it is to be +1 or —1. With
this in mind, let {/„ : n > 0} be a sequence of {1,... ,d}-valued, mutually

14 
1 RANDOM WALKS
independent, uniformly distributed random variables which are independent
of {Xi>n : 1 < i < d & n > 0}, set, for 1 < % < d, Ni>0 = 0 and JVi>n =
Sm=i l{i}(^m) w n e n n — 1; a n (l consider the sequence {Yn : n > 0} given
by
(1-2-12) 
Yn = 
(XhNln,...,XdtNdJ.
Without too much effort, one can check that {Yn : n > 0} satisfies the
conditions in (1-2.1) for the symmetric, nearest neighbor random walk on "Ed
and therefore has the same distribution as {Xn : n > 0}. In particular, by
(1.2.11),
P(X2n = 0) = Yl p(J.,2m, = 0 & JVi>2n = 2m8 for 1 < i < d)
( d 
\
= 
22 
I I ^(xi,2mi = 0) TP(Nii2n = Zrrii for 1 < i < d)
i=l
= 0) j F(Ni,2n = 2mi for 1 < i < d)
§ - l ) 
+ P(iVi,2n < f for some 
\<i<d).
Thus, we will have proved that there is a constant A{d) < oo such that
(1.2.13) 
P(X2n = 0) <A(d)n~i, 
n > 1,
once we show that there is a constant B(d) < oo such that
(1.2.14) 
F(Nii2n < § for some 1 <i < d) < B(d)n~i, 
n > 1.
In particular, this will complete the proof that
oo
d > 3 = > ^ 
P ( X 2 n = 0) < OO
n=0
and therefore that the symmetric, 
nearest neighbor random walk in Zd is
transient when d > 3.
To prove (1.2.14), first note that
i>2n < ^ for some 1 < i < d) < dP(iY1?2n < §)•

1.2 Recurrence Properties of Random Walks
15
Next, write iViin = Y^i zm where Zm = l{i}(/m), and observe that {Zm :
m > 1} is a sequence of {0, l}-valued Bernoulli random variables such that
P(Zm = 1) = p = 2- I n particular, for any A € R,
E
exp I A
and so
E
exp 
A I np -
= eni!{x) where i>{\) = \og(pe-x<1 + qeXp).
Since V(0) = y/(0) = 0, and
where x\ =
(1.2.15)
(qexP + pe~xi)2 
{qxx+px^)2 
4
, Taylor's formula allows us to conclude that
E
it
exp I A I np —
AG
Starting from (1.2.15), there are many ways to arrive at (1.2.14). For ex-
ample, for any A > 0 and R > 0, Markov's inequality (6.1.12) plus (1.2.15)
say that
< np - nR ) = P I exp (A (np - ] T Zm ) J > ,n\R
which, when A = AnR, gives
(1.2.16)
<np — nR 
< e
Returning to the notation used earlier and using the remark with which our
discussion of (1.2.14) began, one see from (1.2.16) that
P(Ni,2n < ^ for some 1 < i < d) < de'^Sr,
which is obviously far more than is required by (1.2.14).
The argument which we have used in this subsection is an example of an
extremely powerful method known as coupling. Loosely speaking, the coupling
method entails writing a random variable about which one wants to know more
(in our case X2n) as a function of random variables about which one knows
quite a bit (in our case {(-X»im,iVj)m) : l<i<dk,m> 
0}). Of course,
the power of the method reflects the power of its user: there are lots of ways
in which to couple a random variable to other random variables, but most of
them are useless.

16
1 RANDOM WALKS
1.3 Exercises
EXERCISE 1.3.1. Schwarz's inequality comes in many forms, the most ele-
mentary of which is the statement that, for any {an : n 6 Z} C R and
{bn : n e Z} C R,
Moreover, when the right hand side is finite, then
if and only if there is an a G K for which either bn = aan, n G Z, or an =
abn, n £ Z. Here is an outline of one proof of these statements.
(a) Begin by showing that it suffices to treat the case in which an = 0 = bn
for all but a finite number of n's.
(b) Given a real, quadratic polynomial P{x) = Ax2 + 2Bx + C, use the
quadratic formula to see that P > 0 everywhere if and only if C > 0 and
B2 < AC. Similarly, show that P > 0 everywhere if and only if C > 0 and
B2 < AC.
(c) Assuming that an = 0 = bn for all but a finite number of n's, set
P(x) = ^Zn(anx + bn)2, and apply (b) to get the desired conclusions. Finally,
use (a) to remove the restriction of the an's and 6ra's.
EXERCISE 1.3.2. Let {Yn : n > 1} be a sequence of mutually independent,
identically distributed random variables satisfying E[|Yi|] < oo. Set Xn =
]Cm=i ^m for n > 1. The Weak Law of Large Numbers says that
> 
g
In fact,
(1.3.3)
lim E
n—»oo
0 for all e > 0.
= o,
from which the above follows as an application of Markov's inequality. Here
are steps which lead to (1.3.3).
(a) First reduce to the case when E[Yi] = 0. Next, assume that EfY^] < oo,
and show that
W\V2]
2.
n
I2
< E
VL
n
2
Hence the result is proved when Y\ has a finite second moment.

1.3 Exercises
17
(b) Given R>0, set Y^
m=i Y™R)- 
N o t e 
t h a t > f o r
= Ynl[0,R)(\Yn\) 
- E[Yn, \Yn\ < R] and X(
n
R) =
R > °'
E
^ | | <E
n
n
+ E
n
\
E
<4+2E[|y!|, |Fi|>-R],
712
and use this, together with the Monotone Convergence Theorem, to complete
the proof of (1.3.3).
EXERCISE 1.3.4. Refer to Exercise 1.3.2. The Strong Law of Large Numbers
says that the statement in the Weak Law can be improved to the statement
that ^ —> E[Yi] with probability 1. The proof of the Strong Law when
one assumes only that E[|Yi|] < oo is a bit tricky. However, if one is willing
to assume that EJY^] < oo, then a proof can be based on the same type
argument which leads to the Weak Law.
Let {Yn}i° be a sequence of mutually independent random variables with
the properties that M = supn E[|Y^|4] < oo, and prove that, with probability
1, limn_^oo ^ ^ = 1 ( 7 m — E[ym]) = 0. Note that we have not assume yet
that they are identically distributed, but when we add this assumption we get
lim^oo ^ Y^n=i ~ E[yil w i t n probability 1.
Here is an outline.
(a) Begin by reducing to the case when E[yn] = 0 for all n G Z+.
(b) After writing
E
E[yfcl...yfc4]
fc1,...,fc4=i
and noting that the only terms which do not vanish are those for which each
index is equal to at least one other index, conclude that
E
Hence, since E[Y^}2 < E[Yfc
4],
(*) 
E
• E i
l<k<£<n
< 3Mn2.

18 
1 RANDOM WALKS
(c) Starting from (*), show that
1
3M
0
for all e > 0. This is the Weak Law of Large Numbers for independent random
variables with bounded fourth moments. Of course, the use of four moments
here is somewhat ridiculous since the argument using only two moments is
easier.
(d) Starting again from (*) and using (6.1.4), show that
sup
n>m
<
n
AM
>e < V I
7 
n=m+l
^ 
1 
4M
> ne
0 as m —» oo for all e > 0.
n=rn+l
(e) Use the definition of convergence plus (6.1.4) to show that
n
oo
0 
=
u
N—l 
\m—l n>m
u n u
N=l m—1 n>m
:?n
n
i
A7
Finally, apply the second line of (6.1.3) plus (d) above to justify
n 
~~ N
\n>m
p n
F(I I
for each AT € Z+. Hence, with probability 1, ^ E " - ^ —y ^' which is the
Strong Law of Large Numbers for independent random variables with bounded
fourth moments.
EXERCISE 1.3.5. Readers who know DeMoivre's proof of the Central Limit
Theorem will have realized that the estimate in (1.2.11) is a poor man's sub-
stitute for what one can get as a consequence of Stirling's formula
(1.3.6)
n!
i 
/n
as n —> oo,
meaning that the ratio of the quantities on the two sides of "~" tends to 1.
Indeed, given (1.3.6), show that
Next, give a proof of (1.3.6) based on the following line of reasoning.

1.3 Exercises 
19
(a) Let TI, ..., rn be a mutually independent, unit exponential random vari-
ables,10 and show that for any 0 < R < y/n
(n - 1 ) ! J-
(b) Make a change of variables followed by elementary manipulations to
show that
p^nti+n 
pn
J-^/nR+n 
J-R
n-1
exp (-^-+ 
En(a)\ da,
where
En(a)^(n-l)log(l 
+ ~
)
-
(c) As an application of the Taylor's series for log(l+x) (cf. (1.2.10)), show
that En{a) —> 0 uniformly for \a\ < R when n —» oo, and combine this with
the results in (a) and (b) to arrive at
— n+ie 
fR 
„*
hm 
j 
/ 
e" ~ da < 1
n! 
J
and
lim 
r — 
/ 
e~ ^ da > 1 ——r.
i?2
Because / ^ e °z da = \/27r, it is clear that (1.3.6) follows after one lets
R / oo. 
°°
EXERCISE 1.3.7. The argument in §1.2.3 is quite robust. Indeed, let {Xn :
n > 0} be any symmetric random walk on 1? whose jumps have finite second
moment. That is, Xo = 0, {Xn — Xn_i : n > 1} are mutually independent,
identically distributed, symmetric (Xi has the same distribution as —Xi), Z2-
valued random variables with finite second moment. Show that {Xn : n > 0}
is recurrent in the sense that P(3n > 1 Xn = 0) = 1.
10 A unit exponential random variable is a random variable T for which ¥(T > t) = e

20 
1 RANDOM WALKS
EXERCISE 1.3.8. Let {Xra : n > 0} be a random walk on Zd: Xo = 0,
{Xn — Xn^i : n > 1} are mutually independent, identically distributed, Zd-
valued random variables. Further, for each 1 < i < d, let (Xn)j be the ith
coordinate of Xn, and assume that
min P((Xi)i ^ 0) > 0 but 
P(3 i ± j ( X i ) ^ ) , - ^ 0) = 0.
l<.i<.d
If, for some C < oo and (au...,ad) 
£ [0, oo)d with J2iai 
> x> F(( Xn)i =
0) < Cn~ai, n > 1, show that {Xn : n > 0} is transient in the sense that
P(3n > 1 X n = 0) < 1.
EXERCISE 1.3.9. Let {Xn : n > 0} be a random walk on Zd, as in the
preceding. Given k £ Zd, set
oo
Tk = ^ l { k } ( X n ) 
and 
Ck = inf{n > 0 : Xre = k}.
n=0
Show that
P(Ck < OO)
'O - 
OO) '
(1.3.10) 
E[Tk] = P(Ck < co)E[T0] =
where po = inf {n > 1 : Xn = 0} is the time of first return to 0. In particular,
if {Xn : n > 0} is transient in the sense described in the preceding exercise,
show that
oo
E ^ 
lB(r)(Xn) 
< oo for all r £ (0,co),
.m=0
where B{r) = {k : |k| < r}; and from this conclude that |Xn| —> oo with
probability 1. On the other hand, if {Xra : n > 0} is recurrent, show that
Xra = 0 infinitely often with probability 1. Hence, either {Xra : n > 0} is
recurrent and Xra = 0 infinitely often with probability 1 or it is transient and
X n| —> oo with probability 1.
EXERCISE 1.3.11. Take d = 1 in the preceding, Xo = 0, and {Xn - Xn_i :
n > 1} to be mutually independent, identically distributed random variables
for which 0 < E[|Xi|] < oo and E[Xi] = 0. By a slight variation on the
argument given in §1.2.1, we will show here that this random walk is recurrent
but that
lim Xn = oo and 
lim Xn = — oo with probability 1.
(a) First show that it suffices to prove that supra Xn = oo and that infn Xn =
—oo. Next, use the Weak Law of Large Numbers (cf. Exercise 1.3.2) to show
that
lim max —-—— = 0.
n

1.3 Exercises 
21
(b) For n > 1, set T(
k
n) = ^m=o l{fc}(*m), show that E[Tfc
(n)] < E[T0
(n)]
for all k G Z, and use this to arrive at
(4/x(n) + l)E[T0
(n)] > - 
where fj,(n) = max E[|X
2 
0<m<ro1
Finally, apply part (a) to conclude that E[T0] — oo. Hence, by (1.3.10),
P(po < oo) = 1, and so {Xn : n > 0} is recurrent.
(c) To complete the program, proceed as in the derivation of (1.2.2) to pass
from (b) to
(*) 
F(p{
o
m) < oo) = 1 for all m > 1,
where p$ 
is the time of the mth return to 0. Next, for r £ Z+, set r/r =
inf{n > 0 : Xn > r}, show that e = P(r?i > po) < 1, and conclude that
p(»?i > />om)) ^ e?n- 
Now> combine this with (*) to get P(r/i < oo) = 1.
Finally, argue that
P(r?r+i < oo) > P(r?r < oo)P(r?1 < oo)
and therefore that P(r?r < oo) = 1 for all r > 1. Since this means that, with
probability 1, supn Xn > r for all r > 1, it follows that supn Xn = oo with
probability 1. To prove that infra Xn = — oo with probability 1, simply replace
{Xn : n > 0} by {-Xn : n > 0}.
EXERCISE 1.3.12. 1J Here is an interesting application of one dimensional
random walks to elementary queuing theory. Queuing theory deals with the
distribution of the number of people waiting to be served (i.e., the length
of the queue) when, during each time interval, the number of people who
arrive and the number of people who are served are random. The queuing
model which we will consider here is among the simplest. Namely, we will
assume that, during the time interval [n — l,n), the number of people who
arrive minus the number who can be served is given by a Z-valued random
variable Bn. Further, we assume that the Bn's are mutually independent
and identically distributed random variables satisfying 0 < E[|_Bij] < oo. The
associated queue is, apart from the fact that there are never a negative number
of people waiting, the random walk {Xn : n > 0} determined by the £?n's:
XQ = 0 and Xn = 5Zm=i -^m.- To take into account the prohibition against
having a queue of negative length, the queuing model {Qn : n > 0} is given
by the prescription
Qo = O and Qn = (Qn-i + Bn)+ 
for n > 1.
(a) Show that
Qn = Xn - min Xm = max (Xn - Xm),
0<m<n 
0<m<n
and conclude that, for each n > 0, the distribution of Qn is the same as that
of Mn = maxo<m<ra Xm.
11 So far as I know, this example was invented by Wm. Feller.

22 
1 RANDOM WALKS
(b) Set MQO = limn-^oo Mn s NU {00}, and, as a consequence of (a), arrive
at
Jirn^ F(Qn = 3) = P(Moc = 3) 
for j e N.
(c) Set ji = E[i?i]. The Weak Law of Large Numbers says that, for each
e > 0, P(|Xn — nfi\ > ne) —> 0 as n —» 00. In particular, when /J, > 0, show
that P(Moo = ex)) = 1. When fi = 0, use Exercise 1.3.11 to reach the same
conclusion. Hence, when E[Bj] > 0, F(Qn = j) —> 0 for all j e N. That
is, when the expected number of arrivals is a least as large of the expected
number of people served, then, with probability 1, the queue grows infinitely
long.
(d) Now assume that fj, = E[£?i] < 0. Then the Strong Law of Large
Numbers (cf. Exercise 1.3.4 for the case when B\ has a finite fourth moment
and Theorem 1.4.11 in [9] for the general case) says that ^ 
—> \i with
probability 1. In particular, conclude that M m < 00 with probability 1 and
therefore that J2jen vo = 1 when Vj = limn^oo F(Qn = j) = P(Moo — j).
(e) Specialize to the case when the .Bm's are {—1,1}-valued Bernoulli ran-
dom variables with p = P(J5i = 1) G (0,1), and set q = 1 — p. Use the
calculations in (1.1.12) to show that
( 0 
if p > q
if p < q.
(f) Generalize (e) to the case when Bm e {-1,0,1}, p = F(B1 = 1), and
q = P(i?i = —1). The idea is that M^ in this case has the same distribution
as supn Yn, where {Yn : n > 0} is the random walk corresponding to {—1,1}-
valued Bernoulli random variables which are 1 with probability
? •

CHAPTER 2
Doeblin's Theory for Markov Chains
In this chapter we begin in earnest our study of Markov processes. Like the
random walks in Chapter 1, the processes with which we will be dealing here
take only countably many values and have a discrete (as opposed to continu-
ous) time parameter. In fact, in many ways, these processes are the simplest
generalizations of random walks. To be precise, random walks proceed in such
a way that the distribution of their increments are independent of everything
which has happened before the increment takes place. The processes at which
we will be looking now proceed in such a way that the distribution of their
increments depends on where they are at the time of the increment but not on
where they were in the past. A process with this sort of dependence property
is said to have the Markov property and is called a Markov chain.1
The set § in which a process takes its values is called its state space, and,
as we said, our processes will have state spaces which are either finite or
countably infinite. Thus, at least for theoretical purposes, there is no reason
for us not to think of S as the set {1,..., N} or Z +, depending on whether S
if finite or countably infinite. On the other hand, always taking § to be one
of these has the disadvantage that it may mask important properties. For
example, it would have been a great mistake to describe the nearest neighbor
random walk on Z2 after mapping Z2 isomorphically onto Z+.
2.1 Some Generalities
Before getting started, there are a few general facts which we will need to
know about Markov chains.
A Markov chain on a finite or countably infinite state space S is a family of
S-valued random variables {Xn : n > 0} with the property that, for all n > 0
and ( z o , . . . , w ) G Sn+2,
(2.1.1) 
V(Xn+1=j\X0 
= i0,...,Xn=in) 
=(P)inj,
where P is a matrix all of whose entries are non-negative and each of whose
rows sums to 1. Equivalently (cf. §6.4.1)
(2.1.2) 
P ( X n + 1 
=j\X0,...,Xn)=
i 3-
1 The term "chain" is commonly applied to processes with a time discrete parameter.

24 
2 MARKOV CHAINS
It should be clear that (2.1.2) is a mathematically precise expression of the
idea that, when a Markov chain jumps, the distribution of where it lands
depends only on where it was at the time when it jumped and not on where
it was in the past.
2.1.1. Existence of Markov Chains: For obvious reasons, a matrix whose
entries are non-negative and each of whose rows sum to 1 is called a transition
probability matrix: it gives the probability that the Markov chain will move
to the state j at time n + 1 given that it is at state i at time n, independent
of where it was prior to time n. Further, it is clear that only a transition
probability matrix could appear on the right of (2.1.1). What may not be
so immediate is that one can go in the opposite direction. Namely, let n
be a probability vector2 and P a transition probability matrix. Then there
exists a Markov chain {Xn : n > 0} with initial distribution fx. and transition
probability matrix P. That is, ¥(X0 = i) = (/z); and (2.1.1) holds.
To prove the preceding existence statement, one can proceed as follows.
Begin by assuming, without loss in generality, that § is either {1,..., N} or
Z+. Next, given i e S, set fi(i,0) = 0 and 0(i,j) = ELi( p)ifc f o r J ^ *>
and define F : S x [0,1) —> § so that F(i, u) = j if (3(i,j - 1) < u < j3{i,j).
In addition, set a(0) = 0 and a(i) = X^=i (/•*)«; f°r * — 1) a nd define / :
[0,1) —>• S so that f(u) = i if a(i - 1) < u < a(i). Finally, let {Un : n > 0}
be a sequence of mutually independent random variables (cf. Theorem 6.3.2)
which are uniformly distributed on [0,1), and set
f(U0) 
if n = 0
We will now show that the sequence {Xn : n > 0} in (2.1.3) is a Markov
chain with the required properties. For this purpose, suppose that (io, • • •, in) £
Sra+1, and observe that
= 
i0,...,Xn=in)
o€ 
[a(io-l),a(io))
k,Ume 
[/3(im_i,im - l),/3(im_i,im)) for 1 < m < nj
2.1.2. Transition Probabilities & Probability Vectors: Notice that the
use of matrix notation here is clever. To wit, if /x is the row vector with ith
entry (/i), = V{XQ = i), then fi is called the initial distribution of the chain
and
(2.1.4) 
(/zP™),- = F(Xn = j), 
n > 0 and j e S,
' A probability vector is a row vector whose coordinates are non-negative and sum to 1.

2.1 Some Generalities 
25
where we have adopted the convention that P° is the identity matrix and
P
n = p p n l
 n > I.3 To check (2.1.4), let n > 1 be given, and note that, by
(2.1.1) and induction,
¥(X0 =i o,...,X n_i =in-i,Xn=j) 
=(M)ao(p)to«i-"(Pk-ij-
Hence (2.1.4) results after one sums with respect to (J,Q, ...,in_i). 
Obviously,
(2.1.4) is the statement that the row vector /zPn is the distribution of the
Markov chain at time n if fi is its initial distribution (i.e., its distribution at
time 0). Alternatively, P n is the n-step transition probability matrix: (Pn)»j
is the conditional probability that Xm+n = j given that Xm = i.
For future reference, we will introduce here an appropriate way in which
to measure the length of row vectors when they are being used to represent
measures. Namely, given a row vector p, we set
(2-1.5) 
IIPIIV
where the subscript "v" is used in recognition that this is the notion of length
which corresponds to the variation norm on the space of measures. The basic
reason for our making this choice of norm is that
(2.1.6) 
||pP||v < ||p||v,
since, by Theorem 6.1.15,
<£
Notice that this is a quite different way of measuring the length from the way
Euclid would have: he would have used
(2.1.7)
On the other hand, at least when S is finite, these two norms are comparable.
Namely,
llplb < ||p||v < \/#S||p||2, 
where #§ denotes the cardinality of S.
The first inequality is easily seen by squaring both sides, and the second is an
application of Schwarz's inequality (cf. Exercise 1.3.1). Moreover, || • ||v is a
3 The reader should check for itself that P n is again a transition probability matrix for all
n £ N: all entries are non-negative and each row sums to 1.

26 
2 MARKOV CHAINS
good norm (i.e., measure of length) in the sense that ||p||v = 0 if and only if
p = 0 and that it satisfies the triangle inequality: \\p + p'\\ v < ||p||v + llp'ilv-
Finally, Cauchy's convergence criterion holds for || • ||v. That is, if {pn}i° is
a sequence in Ms, then there exists p € Ks for which \\pn — p|| v —> 0 if and
only {pra}i° is Cauchy convergent
lim sup \\pn - pm||v = 0.
As usual, the "only if" direction is an easy application of the triangle inequal-
ity:
\\pn ~ Pm||v < \\Pn ~ p\\v + \\p ~ Pm\\v
To go the other direction, suppose that {pn}f 
is Cauchy convergent, and
observe that each coordinate of {pn}i° must be Cauchy convergent as real
numbers. Hence, by Cauchy's criterion for real numbers, there exists a p to
which {pn}i° converges in the sense that each coordinate of the p n's tends to
the corresponding coordinate of p. Thus, by Fatou's Lemma, Theorem 6.1.10,
as TO —> oo,
y)i-(Pm)i\< 
Hm Y]\ (pn)i-(pm)i 
• 0.
Z /
2.1.3. Transition Probabilities and Functions: As we saw in §2.1.2,
the representation of the transition probability as a matrix and the initial
distributions as a row vector facilitates the representation of the distribution
at later times. In order to understand how to get the analogous benefit when
computing expectation values of functions, think of a function / on the state
space § as the column vector f whose jth coordinate is the value of the function
/ at j . Clearly, if p, is the row vector which represents the probability measure
fj, on {1,..., N} and f is the column vector which represents a function / which
is either non-negative or bounded, then /j,£ = Sies /W/-*(W) ls ^ne expected
value of / with respect to \i. Similarly, the column vector Pnf represents that
function whose value at i is the conditional expectation value of f(Xn) given
that XQ = i. Indeed,
E[f(Xn) \X0=i]=J2 ftiMXn =j\X0=i)
ies
More generally, if / is either a non-negative or bounded function on S and f
is the column vector which it determines, then, for 0 < m < n,
(218) 
E [f(Xn) \XQ = io,...,Xm 
= im]= 
(P n~ mf ) i m ,
or, equivalents E[f(Xn) \X0,...,Xm] 
= ( P « — f ) X m

2.2 Doeblin's Theory 
27
since
— ) W = ( P — f ) , m .
In particular, if /i is the initial distribution of {Xn : n > 0}, then
(2.1.9) 
E[f{Xn)] = viPnf,
since E[f(Xn)] = Ei(A*)iE[/(^n)l^o = t]- '
Notice that, just as || • ||v was the appropriate way to measure the length of
row vectors when we were using them to represent measures, the appropriate
way to measure the length of column vectors which represent functions is with
the uniform norm || • ||u:
(2.1.10) 
||f||u = sup|(f)j|.
The reason why || • ||u is the norm of choice here is that |/if| < ||/x||v||f ||u,
since
In particular, we have the complement to (2.1.6):
(2.1.H) 
||Pf ||u < ||f ||u.
2.1 A. The Markov Property: By definition, if /j, is the initial distribution
of {Xn : n > 0}, then
(2.1.12) 
¥(X0 =io,...,Xn=in)= 
(n)io(P)ioil 
• • • (P) i n_ l i n•
Hence, if m,n > 1 and F : Sra+1 —> R is either bounded or non-negative,
then
F(im,jl,---Jn)Vio(P)ioii--
0, ...,Xn)\X0 
= im]V(X0 
=io,...,Xm=im).
Equivalently, we have now proved the Markov property in the form
,... ,Xm+n) 
I Xo = io, • • • ,Xm 
= im]
= 
E[F(X0,...,Xn)\X0=im].
2.2 Doeblin's Theory
In this section we will introduce an elementary but basic technique, due to
Doeblin, which will allow us to study the long time distribution of a Markov
chain, particularly ones on a finite state space.

28 
2 MARKOV CHAINS
2.2.1. Doeblin's Basic Theorem: For many purposes, what one wants to
know about a Markov chain is its distribution after a long time, and, at least
when the state space is finite, it is reasonable to think that the distribution of
the chain will stabilize. To be more precise, if one is dealing with a chain which
can go in a single step from some state i to any state j with positive probability,
then, because there are only a finite number of states, a pigeon hole argument
shows that this state is going to visited again and again and that, after a while,
the chain's initial distribution is going to get "forgotten." In other words, we
are predicting for such a chain that /zPn will, for sufficiently large n, be nearly
independent of /i.. In particular, this would mean that /LtP™ = (pJ?n~m)Pm
is very nearly equal to /zPm when m is large and therefore, by Cauchy's
convergence criterion, that TT — linin^oo /xPn exists. In addition, if this were
the case, then we would have that TT = linv^oo fiPn+1 = limn^oo(/i.Pn)P =
TTP. That is, TT would have to be a left eigenvector for P with eigenvalue 1.
A probability vector TT is, for obvious reasons, called a stationary distribution
for the transition probability matrix P if TT = TTP.
Although we were thinking about finite state spaces in the preceding dis-
cussion, there are situations in which these musings apply even to infinite
state spaces. Namely, if, no matter where the chain starts, it has a positive
probability of visiting some fixed state, then, as the following theorem shows,
it will stabilize.
2.2.1 DOEBLIN'S THEOREM. 
Let P be a transition probability matrix with
the property that, for some state jo G S and e > 0, (P)ij0 > e for all i s S .
Then P has a unique stationary probability vector TT, (TT)J0 > e, and, for all
initial distributions \A,
||/iPn — TTJjv < 2(1 - e ) n , 
n > 0.
PROOF: The key to the proof lies the observations that if p G 8 s is a row
vector with ||p||v < oo, then
2_](pP)j = yj(p)» 
and
(2.2.2)
Y,(p)i = 0 = * l|pP"||v < (1 - e)"||p||v 
for n > 1.
The first of these is trivial, because, by Theorem 6.1.15,
E^p); =
As for the second, we note that, by an easy induction argument, it suffices to

2.2 Doeblin's Theory 
29
check it when n = 1. Next, suppose that J2i(p)i = 0, and observe that
JGS
and therefore that
Now let /x be a probability vector, and set /j,n = ijiPr
'>n = Mn-mPm and X^i((A*n-m)j ~~ Mi) = 1 — 1 = 0 ,
Then, because
< (1 "
-m " A*l|v < 2(1 - e)
for 1 < m < n. Hence, {Atn}i° is Cauchy convergent; and therefore there exists
a TV for which \\/j,n — TT||v —> 0. Since each /i.n is a probability vector, it is clear
that 7T must also be a probability vector. In addition, TV = limn^oo /i.P™+1 =
limn_>oo(/i.P")P = TTP, and so TV is stationary. In particular,
J6S
Finally, if u is any probability vector, then
7T v 
=
which, of course, proves both the stated convergence result and the uniqueness
of TV as the only stationary probability vector for P. 
•
It is instructive to understand what Doeblin's Theorem says in the language
of spectral theory. Namely, as an operator on the space of bounded functions
(a.k.a. column vectors with finite uniform norm), P has the function 1 as a
right eigenfunction with eigenvalue 1: P I = 1. Thus, at least if S is finite,
general principles say that there should exist a row vector which is a left
eigenvector of P with eigenvalue 1. Moreover, because 1 and the entries of
P are real, this left eigenvector can be taken to have real components. Thus,
from the spectral point of view, it is no surprise that there is a non-zero
row vector /i € Rs with the property that fiP = /i.. On the other hand,

30 
2 MARKOV CHAINS
standard spectral theory would not predict that /x can be chosen to have non-
negative components, and this is the first place where Doeblin's Theorem gives
information which is not readily available from spectral theory, even when §
is finite. To interpret the estimate in Doeblin's Theorem, let Mi(S; C) denote
the space of row vectors i / £ C s with ||f||v = 1- Then,
H^PIIv < 1 for all u £ Mi(S; C),
and so
sup{|a| : a £ C& 3 u e Mi(S;C) uP = au} < 1.
Moreover, if uP = au for some a / 1 , then u\ = u(Pl) = (uP)l = aul,
and therefore ul = 0. Thus, the estimate in (2.2.2) says that all eigenvalues of
P which are different from 1 have absolute value dominated by 1 — e. That is,
the entire spectrum of P lies in the complex unit disk, 1 is a simple eigenvalue,
and all the other eigenvalues lie in the disk of radius 1 — e. Finally, although
general spectral theory fails to predict Doeblin's Theorem, it should be said
that there is a spectral theory, the one initiated by Frobenius and developed
further by Kakutani, which does cover Doeblin's results. The interested reader
should consult Chapter VIII in [2].
2.2.2. A Couple of Extensions: An essentially trivial extension of Theo-
rem 2.2.1 is provided by the observation that, for any M > 1 and e > 0,4
(2.2.3) 
supinf(PM)ii > e = > ||/xP" - TT||V < 2(1 - e ) ^
for all probability vectors [i and a unique stationary probability vector TT. TO
see this, let TT be the stationary probability vector for P M , the one guaranteed
by Theorem 2.2.1, and note that, for any probability vector [i, any m € N,
and any 0 < r < M,
II pmM+r _ 
II - ll('iiPr-ir'lPmMll 
< 9(1 
Am
IIH
1 
II v — || vM' 
/ 
II v — V — /
Thus (2.2.3) has been proved, and from (2.2.3) the argument needed to show
that 7T is the one and only stationary measure for P is the same as the one
given in the proof of Theorem 2.2.1.
The next extension is a little less trivial. In order to appreciate the point
which it is addressing, one should keep in mind the following example. Namely,
consider the transition probability matrix
0 1
1 0
Obviously, this two state chain goes in a single step from one state to the other.
Thus, it certainly visits all its states. On the other hand, it does not satisfy
4 Here and elsewhere, we use [s] to denote the integer part [s] of s of s € K. That is, [s] is
the largest integer dominated by s.

2.2 Doeblin's Theory
31
the hypothesis in (2.2.3): (Pn)jj = 0 if either i = j and n is odd or if i ^ j
and n is even. Thus, it should not be surprising that the conclusion in (2.2.3)
fails to hold for this P. Indeed, it is easy to check that although (|, 5) is the
one and only stationary probability vector for P, ||(l,0)Pn — (|, |)|| v = 1 for
all n > 0. As we will see later (cf. §3.1.3), the problems encountered here
stem from the fact that (Pn)n > 0 only if n is even.
In spite of the problems raised by the preceding example, one should expect
that the chain corresponding to this P does equilibrate in some sense. To
describe what we have in mind, set
n-l
(2.2.4)
•**-n 
—
' m=0
Although the matrix An is again a transition probability matrix, it is not
describing transitions but instead it is giving the average amount of time that
the chain will visit states. To be precise, because
1 n - l
x 
Hi1/ v
m = 0
m = j Xo = i) = E
' 
n-l
— y^ 
l{j}(Xm) XQ = i
(An)ij is the expected value of the average time spent at state j during the
time interval [0, n — 1] given that i was the state from which the chain started.
Experience teaches us that data becomes much more forgiving when it is
averaged, and the present situation is no exception. Indeed, continuing with
the example given above, observe that, for any probability vector ft,
- ( i , i ) 
< - 
forn>l.
What follows is a statement which shows that this sort of conclusion is quite
general.
2.2.5 THEOREM. 
Suppose that P is a transition probability matrix on S.
If for some M G Z +, j 0 G S, and e > 0, (AM)ij0 
> e for all i G S, then there
is precisely one stationary probability vector TT for P, (TT)J0 > e, and
< M - l
ne
for any probability vector fi.
To get started, let TT be the unique stationary probability which Theorem
(2.2.3) guarantees for AM- Then, because any /j, which is stationary for P
is certainly stationary for AM, it is clear that vr is the only candidate for
P-stationarity. Moreover, to see that TT is P-stationary, observe that, because
P commutes with AM, (TTP)AM = (TTAM)P = TTP. Hence, TTP is stationary
for AM and therefore, by uniqueness, must be equal to TT. That is, 7r = TTP.

32
2 MARKOV CHAINS
In order to prove the asserted convergence result, we will need an elementary
property of averaging procedures. Namely, for any probability vector fj,,
(2.2.6)
\\/j,AnAm 
— /itAn||v <
1
n
for all m,n > 1.
To check this, first note that, by the triangle inequality,
||/xAnAm - /iAn||v = —
m-l
k=0
m — 1
<
m k=0
Second, for each k > 0,
n - l
£=0
{n+k-1 
n-l
£=0
and so ||/i.PfcAra — /xAn||v < ^ - Hence, after combining this with the first
observation, we are lead to
m—1
||/iAnAm - A*An||v <
ran
k =
..
fc=0
which is what we wanted.
To complete the proof of Theorem 2.2.5 from here, assume that (Ajn)y0 > e
for all i, and, as above, let TT be the unique stationary probability vector for P.
Then, TT is also the unique stationary probability vector for AM, and so, by
the estimate in the second line of (2.2.2) applied to AM, ||/iAnAM - 7r||v =
||(/xAra — 7T)AM!|V < (1 — e)\\fiA.n — 7r||v, which, in conjunction with (2.2.6),
leads to
||/LtAn - 7T||V <
- 7r||
M - l
~ 
n
Finally, after elementary rearrangement, this gives the required result.
2.3 Elements of Ergodic Theory
In the preceding section we saw that, under suitable conditions, either /i.Pra
or fiAn converge and that the limit is the unique stationary probability vector
7T for P. In the present section, we will provide a more probabilistically ori-
ented interpretation of these results. In particular, we will give a probabilistic

2.3 Elements of Ergodic Theory 
33
interpretation of TT. This will be done again, by entirely different methods, in
Chapter 3.
Before going further, it will be useful to have summarized our earlier results
in the form (cf. (2.2.3) and remember that |/zf| < ||MI|V||/||U)5
(2.3.1) 
supinf(PM)ij>e => ||Pf - Trf ||u < 2(1 - c)W||f ||u
3 
*
and (cf. Theorem 2.2.5)
M - 1
(2.3.2) 
supinf(AM)y > e ==> ||Anf - Trf||u < 
||f||u
i 
i 
ne
when f is a bounded column vector.
2.3.1. The Mean Ergodic Theorem: Let {Xn : n > 0} be a Markov
chain with transition probability P. Obviously,
(2.3.3) 
T(? 
=l-Y^l{3}{Xm)
m=0
is that average amount of time that the chain spends at j before time n.
Thus, if fj, is the initial distribution of the chain (i.e., (fi)i = ¥(X0 = i)),
then (fj,An)j = E[Tj], and so, when it applies, Theorem 2.2.5 implies that
E\Tj'} 
—> (TT)J as n —> oo. Here we will be proving that the random
variables Tj 
themselves, not just their expected values, tend to (TT)J as
n —> oo. Such results come under the heading of ergodic theory. Ergodic
theory is the mathematics of the principle, first enunciated by the physicist
J.W. Gibbs in connection with the kinetic theory of gases, which asserts that
the time-average over a particular trajectory of a random dynamical system
will approximate the equilibrium state of that system. Unfortunately, in spite
of results, like those given here, confirming this principle, even now, nearly
150 years after Gibbs, there are essentially no physically realistic situations
in which Gibbs's principle has been mathematically confirmed.
2.3.4 MEAN ERGODIC THEOREM. Under the hypotheses in Theorem 2.2.5,
- (TT),)2! < WLL}1 
for all „ > i.
3> J 
ne
ne
(See (2.3.10) below for a more refined, less quantitative version.) More gen-
erally, for any bounded function f on S and all n > 1:
, 21
E
n - l
n 771=0 
/
where f denotes the column vector determined by f.
n e
5 Here, and elsewhere, we abuse notation by using a constant to stand for the associated
constant function.

34
2 MARKOV CHAINS
PROOF: Let f be the column vector determined by the function f = f — Trf.
Obviously,
n - l
n - l
m = 0
m = 0
and so
n - l
n / ^
m=0
- 
TTf
r)2 
/ 
J
n - l
Hence,
E 
-[k\
n - lE
m = 0
/(*m)
^
- T r f
J
2
)
2
~ n2
2
n - l
E E
fe=O
n - l
/ 
IPJ
ra-fc-1
"
€=0
fe=0
But, by (2.3.2), ||An_fcfj|u < #^-||f|| u, and so, since ||f||u < ||f||u,
(n-k)E[f(Xk)(An_k£)Xk] <
After plugging this into the preceding, we get the second result. To get the
first, simply take / = l ^ j and observe that, in this case, ||f ||u < 1. D
2.3.2. Return Times: As the contents of §§1.1 and 1.2 already indicate,
return times ought to play an important role in the analysis of the long time
behavior of Markov chains. In particular, if
j = 0 and, for m > 1, the time
of rath return to j is denned so that
p™ =
= oo if Pj~ 
= oo or Xn ^ j for
™
and p™ = inf{n > p™ 
: Xn — j} otherwise, then we
every n > p^-171
say that j is recurrent or transient depending on whether
< oo|Xo =
j) — 1 or not; and we can hope that when j is recurrent, then the history of
the chain breaks into epochs which are punctuated by the successive returns
to j . In this subsection we will provide evidence which bolsters that hope.

2.3 Elements of Ergodic Theory
35
Notice that Pj = p^ > 1 and, for n > 1,
(2.3.5)
w h e r e F n > j ( i 0 , ...,in) 
=
(n,oo] (Pj) — Fnj(X0, . . . , Xn)
1 if im ^ j for 1 < m < n
0 otherwise
In particular, this shows that the event {pj > n} is a measurable function of
(Xo,..., Xn). More generally, because
n-l
1=1
an easy inductive argument shows that, for each m £ N and n G N, {p™ >
is a measurable function of (Xo,..., Xn).
2.3.6 THEOREM. 
For all m G Z+ and (i, j) G S2,
< oo
= i) = P(/Oj < oo | Xo = i)P(pj < oo | Xo = j)
.\ m — 1
In particular, if j is recurrent, then
r m ' < oo|Xo = j) = 1 for a7i ?TT. £ N.
Jn fact, if j is recurrent, then, conditional on XQ = j , {Pj
i 
f 
ll i d d 
d 
bl
— P~
Pj
• m > 1}
j 
j
is a sequence of mutually independent random variables each of which has the
same distribution as pj.
PROOF: TO prove the first statement, we apply (2.1.13) and the Monotone
Convergence Theorem, Theorem 6.1.9, to justify
oo
lim
JV
n=l
~ FN,j{Xn,... ,Xn+jv),
= n
lim E l -
n=l
oo
n=l ^°°
= F(pj < oo
3,...,X J V)|Xo=j]P(pV
Xo = i
(m-l)
1 
= n X0 = i)
= n
oo I XQ = i).
Turning to the second statement, note that it sufnces for us prove that
^>>n + nm X0=j,Py=:n1,...,p) 
' = nm)
F(Pj 
>n\X0=j).

36 
2 MARKOV CHAINS
But, again by (2.1.13), the expression on the left is equal to
E[FnJ(Xnm,.. 
.,Xnm+n) 
| Xo = j,pf> = m,... ,pjm
= n
= E[FnJ(X0, 
...,Xn)\XQ=j]= 
¥(PJ >n\X0= 
j ) . D
Reasoning as we did in §1.2.2, we can derive from the first part of Theorem
2.3.6:
(2.3.7)
E[Tj\X0 = i] =5itj
r0 = j] =00
E|T, X 0=jl <oo
oo|X0 = i)
= oo|X0=j)
F(Tj 
=oo\X0=j)=l
F(Tj<oo\X0=j)=l,
where Tj = ]Cm=o l{j}(Xm) 
is the total time the chain spends in the state j .
Indeed, because
V(Tj>m\X0=i) =
all three parts of (2.3.7) follow immediately from the first part of Theorem
2.3.6.
Of course, from (2.3.7) we know that j is recurrent if and only if E[2}|Xo ==
j] = oo. In particular, under the conditions in Theorem 2.2.5, we know that
(An)j0j0 
> (TT)JO > °> a n d s o
E[Tjo I Xo = jo] = J2 (pm)^oio = lim n(
m=0
= oo.
That is, the conditions in Theorem 2.2.5 imply that jo is recurrent, and as we
are about to demonstrate, we can say much more.
To facilitate the statement of the next result, we will say that j is accessible
from i and will write i—>j if (Pn)ij > 0 for some n > 0. Equivalently, i—>j if
and only if i = j or P(/9j < oo|Xo = i) > 0.
2.3.8 THEOREM. 
Assume that infj(AM)r,0 > e for some M > 1, j 0 , and
e > 0. Tien j is recurrent if and only if jo—\7- Moreover, if jo-^-j, tien
E[^|X0 = j] < 00 for aiJ p € (0, 00).
PROOF: First suppose that jo~/>j- Equivalently, P(pj = oo|Xo = jo) = 1. At
the same time, because (AM)JJ0 > e; there exists an 1 < m < M such that
(Pm)jj0 > 0, and so
(m) _= 00 & Xm = jo I Xo = j)
X
\ V 
' "V 
"1
m+N)y -A-m = JO *A-0 = J\
= lim E[FNtj(X0,.. .,XN) | Xo = jo]P(Xm = j 0 | Xo = j)

2.3 Elements of Ergodic Theory 
37
Hence, by Theorem 2.3.6, j cannot be recurrent.
We next show that
(*) 
jo->j => inf(AM')u > 0 for some M' > 1.
To this end, choose m G N so that (Pm)joj > 0. Then, for all i G S,
M+m-l 
n 
M-l
(A m 
M)i 
= 
y ^ 
(p^)t > 
y ^ (pf)» (r*m)
m 
e=o 
m 
e=o
=
 
M
 
(AM) 
r ) . . > i
n . 
.>0.
In view of (*) and what we have already shown, it suffices to show that
Xo = j] < oo if infj(AM)y > e for some e > 0 and M G Z +. For this
purpose, set u(n,i) = P(/?j > nM|Xo = f) for n G Z + and i e § . Then, by
(2.1.13),
u(n + l,i) = 2_,F[pj > (n + l)M & XnM = k\X0 = i)
kes
w,j(XnM, 
• • • ,X(n+1)M), 
Pj > nM 
& XnM 
= k |
M | Xo = fc)P(/9j >nM h XnM = k\X0 = i
Hence, u(n + l,i) < Uu(n,i) where U = maxfc6§M(l, k). Finally, since
u(l, fe) = 1 - P{pj < M\X0 = k) and
j < M \ Xo = k) > max {Pm)kj > (AM)kj > e,
U < 1 — e. In particular, this means that u(n + l,j) < (1 — t)u{n,j), 
and
therefore that P(pj > nM\X0 = j) < (1 - e)n, from which
oo
V—V
E[p? I Xo = j] = V nPP(Pi = n|X0 = j)
n = l
n=(m-l)M+l
oo
Y2 m
pF(Pj > (m -
m=l
oo
771—1

38 
2 MARKOV CHAINS
follows immediately. 
•
2.3.3. Identification of TT: Under the conditions in Theorem 2.2.5, we know
that there is precisely one P-stationary probability vector TT. In this section,
we will give a probabilistic interpretation of (TT)J- Namely, we will show that
sup supinf(AM)ij > 0
M>I 
s »es
(2.3.9)
1S t r a n s i e n t ) '
= ^ (7r)* = E[Pj\X0=j] 
(^ °
The idea for the proof of (2.3.9) is that, on the one hand, (cf. (2.3.3))
while, on the other hand,
Pj 
£=0
mutually
preceding combined with the Weak Law of Large Numbers should lead
Thus, since p™ is the sum of m mutually independent copies of Pj, the
d 
b d 
i h h W k L 
f L 
N b 
h l d 
l d
To carry out the program suggested above, we will actually prove a stronger
result. Namely, we will show that, for each j e §,6
(2.3.10) 
P f lim Tf} = E[Pj 
X0=j]
Xo=j\=l.
In particular, because 0 < T 
< 1, Lebesgue's Dominated Convergence
Theorem, Theorem 6.1.11, says that
(TT)J = lim (A,,)^- = lim E[Tf; | Xo = j] =
follows from (2.3.10). Thus, we need only prove (2.3.10). To this end, choose
jo, M, and e > 0 so that ( A M ) ^ 0 — e f°r &H *• ^ jo ~hji then, by Theorem
2.3.8, j is transient, and so, by (2.3.7), ¥(Tj < oo\X0 = j) = 1. Hence,
6 Statements like the one which follows are called individual ergodic theorems because they,
as distinguished from the first part of Theorem 2.3.4, are about convergence with probability
1 as opposed to convergence in mean. See Exercise 3.3.9 below for more information.

2.3 Elements of Ergodic Theory
39
conditional on Xo = j , T™ < ^Tj —> 0 with probability 1. At the same
time, because j is transient, P(pj = oo | X$ = j) > 0, and so M[pj \ XQ = j] =
oo. Hence, we have proved (2.3.10) in the case when jo -/>j.
Next assume that jo—>j- Then, again by Theorem 2.3.8, E^jXo = j] < oo
and, conditional on Xo = j , {p3• — Pj 
'• m > 1} is a sequence of mutually
independent random variables with the same distribution as pj. In particular,
by the Strong Law of Large Numbers (cf. Exercise 1.3.4)
hm —— = Tj
m—>oo rn
Xo=j) 
= 1 where Tj ~ E[pj\X0 = j].
On the other hand, for any m > 1,
P W 
_ 
- 1
" 3 
T3
T(n) - T (
3 
3
and
j 
l 3
< 2
< 2
n
2m
n
- r.
m
while, since pj 
> m,
Hence,
m
iT^-r"1 <2
/2m 
1
V n 
r3
m
Finally, by taking mn — U1 we get
2
n
mr,
0 
as n —> (X).
Notice that (2.3.10) is precisely the sort of statement for which Gibbs was
looking. That is, it says that, with probability 1, when one observes an
individual path, the average time which it spends in each state tends, as one
observes for a longer and longer time, to the probability which the equilibrium
(i.e., stationary) distribution assigns to that state.

40 
2 MARKOV CHAINS
2.4 Exercises
EXERCISE 2.4.1. In this exercise we will give a probabilistic interpretation
of the adjoint of a transition probability matrix with respect to a stationary
distribution. That is, suppose that the transition probability matrix P admits
a stationary distribution fx, assume {yi)i > 0 for each ! £ § , and determine
the matrix P T by (P7)^ = {gf (P)j
(a) Show that P T is a transition probability matrix for which /j, is again a
stationary distribution.
(b) Use P and P T to denote probabilities computed for the chains deter-
mined, respectively, by P and P T with initial distribution /x, and show that
these chains are the reverse of one another in the sense that, for each n > 0
the distribution of (Xo,..., Xn) under P T is the same as the distribution of
(Xn,...,Xo) 
under P. That is,
P
T ( X 0 = io,...,Xn 
= in)= P(Xn =io,...,Xo 
= in)
for all n > 0 and (i0,... ,in) e Sn+1.
EXERCISE 2.4.2. The Doeblin theory applies particularly well to chains on a
finite state. For example, suppose that P a transition probability matrix on
an N element state space §, and show that there exists an e > 0 such that
(Ajv)rj0 > e for alH e S if and only if i—>jo for all «£§. In particular, if such
a jo exists, conclude that, for all probability vectors /x,
where TV is the unique stationary probability vector for P.
EXERCISE 2.4.3. Here is a version of Doeblin's Theorem which sometimes
gives a slightly better estimate. Namely, assume that (P)ij > £j for all (i,j),
and set e = ^ , ej. If e > 0, show that the conclusion of Theorem 2.2.1 holds
and that (TT), > e, for each t g S .
EXERCISE 2.4.4. Assume that P is a transition probability matrix on the
finite state space S, and show that j G S is recurrent if and only if E[/0,-|Xo =
j] < oo. Of course, the "if" part is trivial and has nothing to do with the
finiteness of the state space.
EXERCISE 2.4.5. Again assume that P is a transition probability matrix on
the finite state space S. In addition, assume that P is doubly stochastic in the
sense that each of its columns as well as each of its rows sums to 1. Under
the condition that every state is accessible from every other state, show that
E[pj\Xo = j] = #§ for each j e S.

2.4 Exercises 
41
EXERCISE 2.4.6. In order to test how good Doeblin's Theorem is, consider
the case when S = {1,2} and
R 
, 
n 
for 
some (a,/?) e (0,1).
Show that 7T = (a + p1)"^/?, a) and that
max{|ji/P — 7r||v : v is a probability vector} = — 
—— 
-.
EXERCISE 2.4.7. One of the earliest examples of Markov processes are the
branching processes introduced, around the end of the nineteenth century,
by Galton and Watson to model demographics. In this model, S = N, the
state i e N representing the number of members in the population, and the
process evolves so that, at each stage, every individual, independently of all
other members of the population, dies and is replaced by a random number
of offspring. 
Thus, 0 is an absorbing state, and, given that there are i >
1 individuals alive at time n, the number of individuals alive at time n +
1 will be distributed like —i plus the sum of i mutually independent, re-
valued, identically distributed random variables. To be more precise, if /i. =
(yitO; • • • i Mfej • • •) is the probability vector giving the number of offspring each
individual produces, define the m-fold convolution power /j,*m so that (n*°)j =
8oj and, for m > 1,
i=0
Then the transition probability matrix P is given by (P)ij = (^**)j.
The first interesting question which one should ask about this model is what
it predicts will be the probability of eventual extinction. That is, what is
limn^oo P(Xn = 0)? A naive guess is that eventual extinction should occur or
should not occur depending on whether the expected number 7 = Y^kLo ^^k
of progeny is strictly less or strictly greater than 1, with the case when the
expected number is precisely 1 being more ambiguous. In order to verify
this guess and remove trivial special cases, we make the assumptions that
(fi)o > 0, O) 0 + O)i < 1, and 7 = J2^L0 fc(/x)fc < 00.
(a) Set f(s) = J2^Loskiik for s G [0,1], and define f°n(s) 
inductively so
that /o0(s) = s and fon = f o f° t""1) for n > 1. Show that 7 = /'(I) and
that
00
f°n(sY = E[sx" \X0=i]=J2si(p")u 
for s e [0,1] and i > 0.
3=0
Hint: Begin by showing that /(s)1 =

42 
2 MARKOV CHAINS
(b) Observe that s G [0,1] i—> /(s) — s is a continuous function which is
positive at s = 0, zero at s — 1, smooth and strictly convex (i.e., / " > 0) on
(0,1). Conclude that either 7 < 1 and f(s) > s for all s G [0,1) or 7 > 1 and
there is exactly one a G (0,1) at which f(a) = a.
(c) Referring to the preceding, show that
7 < 1 ==> lim Ej>x" Xo = i] = 1 for all s G (0,1]
and that
7 > 1 => 
lim E[sXn \X0 = i] = a1 
for all s G (0,1)
(d) Based on (c), conclude that 7 < 1 => P(Xn = 0|X0 = i) —> 1 and
that 7 > 1 => lim^oo P(Xn = 0|X0 = i) = a1 and
l i m P ( l < Xn < L\X0 = i ) = 0 for all 
L > 1 .
n—>oo
The last conclusion has the ominous implication that, when the expected
number of progeny is larger than 1, then the population either becomes extinct
or, what may be worse, grows indefinitely.
EXERCISE 2.4.8. Continue with the setting and notion in Exercise 2.4.7. We
want to show in this exercise that there are significant differences between the
cases when 7 < 1 and 7 = 1.
(a) Show that E[Xn | XQ = z] = 27™. Hence, when 7 < 1, the expected
size of the population goes to 0 at an exponential rate. On the other hand,
when 7 = 1, the expected size remains constant, this in spite of the fact that
F(Xn = Q\XQ = i) —> 1. Thus, when 7 = 1, we have a typical situation of
the sort which demonstrates why Lebesgue had to make the hypotheses he
did in his dominated convergence theorem, Theorem 6.1.11. In the present
case, the explanation is simple: as n —> 00, with large probability Xn = 0
but, nonetheless, with positive probability Xn is enormous.
(b) Let po be the time of first return to 0. Show that
oo < n\X0 = i)= F(Xn = 0\X0 = i) = (f°«
and use this to get the estimate
In particular, this shows that E[/9o|^o = i] < 00 when 7 < 1.
(c) Now assume that 7 = 1. Under the additional condition that /? =
/"(I) = Efc>2 k(k ~ l)/x* < 00, start from ¥(p0 < n\X0 = 1) = / ^ " ^
and show that E[po\Xo = i] = 00 for all i > 1.

2.4 Exercises 
43
Hint: Begin by showing that
i - r"(MO) >
for n > TO. Next, use this to show that
oo
(X) > E[Poi^o = i] = i + y v i - r
would lead to a contradiction.
(d) Here we want to show that the conclusion in (c) will, in general, be
false without the finiteness condition on the second derivative. To see this, let
9 G (0,1) be given, and check that f(s) = s + ' ~yg— = Y^kLo sfeA*fc> where
H = (^Q, ..., /Lij.,...) is a probability vector for which ^ 
> 0 unless k = 1.
Now use this choice of /x to see that, when the second derivative condition in
(c) fails, E[po|-Xo = 1] can be finite even though 7 = 1 .
Hint: Set an = 1 - /°™(/xo), note that an — an+\ = /xo^+e', and use this first
to see that ^ ^ —> 1 and then that there exist 0 < C2 < c^ < oo such that
c\ < a^+i — a>Z® < C2 for all n > 1. Conclude that P(po > n\XQ = 1) tends to
0 like rC«.
EXERCISE 2.4.9. The idea underlying this exercise was introduced by J.L.
Doob and is called7 Doob's h-transformation. 
Let P is a transition probability
matrix on the state space S. Next, let 0 ^ F C § be given, set pr = inf{n >
1 : Xn 6 F}, and assume that
h{i) = P(/or = oo | Xo = i) > 0 for alii e S = S \ T.
(a) Show that h(i) = 2-es(^>)»j'l(j) ^or a^ * e ^> an<^ conclude that the
matrix P given by (P)y = j^hr(P)ijh(j) 
for (i,j) G (S)2 is a transition
probability matrix on S.
(b) For all n G N and (j0, • • •, jn) e (S)ra+1, show that, for each i G S,
~ ¥(X0 = j 0 , . . . , Xn = j n I p r = oo & Xo = i),
where P is used here to denote probabilities computed for the Markov chain
on S whose transition probability matrix is P. That is, the Markov chain
determined by P is the Markov chain determined by P conditioned to never
wtr.
7 The "h" comes from the connection with harmonic functions.

44 
2 MARKOV CHAINS
EXERCISE 2.4.10. Here is another example of an /i-transform. Namely, as-
sume that jo G S is transient but that i^jo for all i s §.8 Set h(i) = F(/oJO <
oo|Xo = i) for i T^ jo and /i(jo) = 1.
(a) After checking that h(i) > 0 for all i e S , define P so that
(P)joj 
if i = Jo
Show that P is again a transition probability matrix.
(b) Using P to denote probabilities computed relative to the chain deter-
mined by P, show that
P(pJO > n | Xo = i) = T77rP(n < pJO 
<oo\X0=i)
for all n € N and % ^ jo •
(c) Starting from the result in (b), show that jo is recurrent for the chain
determined by P.
! By Exercise 2.4.2, this is possible only if S in infinite.

CHAPTER 3
More about the Ergodic Theory
of Markov Chains
In Chapter 2, all of our considerations centered around one form or another
of the Doeblin condition which says that there is a state which can be reached
from any other state at a uniformly fast rate. Although there are lots of
chains on an infinite state space which satisfy his condition, most do not.
As a result, many chains on an infinite state space will not even admit a
stationary probability distribution. Indeed, the fact that there are infinitely
many states means that there is enough space for the chain to "get lost and
disappear." There are two ways in which this can happen. Namely, the chain
can disappear because, like the a nearest neighbor, non-symmetric random
walk in Z (cf. (1.1.13)) or even the symmetric one in Z3 (cf. §1.2.4), it may
have no recurrent states and, as a consequence, will spend a finite amount of
time in any given state. A more subtle way for the chain to disappear is for
it to be recurrent but not sufficiently recurrent for there to exist a stationary
distribution. Such an example is the symmetric, nearest neighbor random
walk in Z which is recurrent, but just barely so. In particular, although this
random walk returns infinitely often to the place where is begins, it does so
at too sparse a set of times. More precisely, by (1.2.7) and (1.2.13), if P is
the transition probability matrix for the symmetric, nearest neighbor random
walk on Z, then
(P2")^ < (P2n)u = P(X2n = 0) < A(l)n-$ -^ 0,
and so, if /z were a probability vector which was stationary for P, then, by
Lebesgue's Dominated Convergence Theorem, Theorem 6.1.11, we would have
the contradiction
(M)j- = Jm^ J2(v)i(p2nh 
= ° for a11 3 e Z.
In this chapter, we will see that ergodic properties can exist in the absence
of Doeblin's condition. However, as we will see, what survives does so in a
weaker form. Specifically, we will no longer be looking for convergence in the
|| • ||v-norm and instead will settle for pointwise convergence. That is, we
will be looking for results of the form (fiP)j —> (TT)^ for each j rather than

46 
3 MORE ERGODIC THEORY OF MARKOV CHAINS
3.1 Classification of States
In this section we deal with a topic which was hinted at but not explic-
itly discussed in Chapter 2. Namely, because we will no longer be making
an assumption like Doeblin's, it will be necessary to take into account the
possibility that the chain sees the state space as a union of disjoint parts. To
be precise, given a pair (i,j) of states, recall that we write i—>j and say that
j is accessible from i if, with positive probability, the chain can go from state
i to state j . That is, (Pn)ij > 0 for some n € N. Notice that accessibility is
transitive in the sense that
(3.1.1) 
i->j and j-M? => 
i^£.
Indeed, if (P"%- > 0 and (P n)^ > 0, then
If i and j are accessible from one another in the sense that %—>j and j—>i,
then we write i<-*j and say that i communicates with j . It should be clear
that <-» is an equivalence relation. To wit, because (P°)u — 1, i<-n, and it
is trivial that j<-*i if i<->j. Finally, if i-^j and j<-+£, then (3.1.1) makes it
obvious that i+->£ Thus, "<->" leads to a partitioning of the state space into
equivalence classes made up of communicating states. That is, for each state
i, the communicating equivalence class [i] of i is the set of states j such that
z<->j; and, for every pair (i,j), either [i] = [j] or [i] n [j] — 0. In the case
when every state communicates with every other state, we say that the chain
is irreducible.
3.1.1. Classification, Recurrence, and Transience: In this subsection,
we will show that recurrence and transience are communicating class prop-
erties. That is, either all members of a communicating equivalence class are
recurrent or all members are transient.
Recall (cf. §2.3.2) that pj is the time of first return to j and that we say j
is recurrent or transient according to whether P(pj < oo|Xo = j) is equal to
or strictly less than 1.
3.1.2 THEOREM. 
Assume that i is recurrent and that j ^ i. 
Theni-^jifand
only if¥(pj < pi\X0 = i) > 0. Moreover, ifi^j, 
then F(pk < oo|X0 = £) = 1
for any (k,£) € {z,j}2. In particular, i—>j implies that i<->j and that j is
recurrent.
PROOF: Given j ^ i and n > 1, set (cf. (2.3.5))
Gn\kQ,..., 
kn) = (Fn—\ti(ko, • • •, kn-\) — Fn>i[ko,..., 
kn)jFnj[ko,..., 
kn).
If {/ojm) : m > 0} are defined as in §2.3.2, then, by (2.1.2),

3.1 Classification of States
47
[Pi
(m+1)
= 1 n=X
E Gn(X^,... 
,Xi+n),
ooE
£,71=1
OO
E
£,71=1
E'
i,n=l
= £ < Pi
Xo = i)
= i
E
(X 0,..., Xn) | Xo = i
= n<Pj\X0 = i)¥(p[m) =£<Pj\X0 = i)
t ) 
<Pj\XQ=i),
and so
(3.1.3)
(m) <Pj\X0=i) 
= ¥(Pi <
= i)
Now suppose that i-^j but P(p^ < Pi\X0 = i) = 0. Then Pf^ < /9j|X0 =
i) = 1, and so, because P(pj ^ pi|X0 = i) > F(Pi < oo|X0 = i) = 1, P(pi <
Pj|X0 = i) = 1. Hence, by (3.1.3), this means that P(p-m) < Pj|X0 = i) = 1
for all m > 1, which, since p(m) > TO, leads to W(pj = oo|Xo = i) = 1 and
therefore rules out i—>j. That is, we have now shown that i—>j ^^>
/9j|Xo = z) > 0, and the opposite implication needs no comment.
To prove that i—>j =^> P(pj < OO|XQ = j) = 1, first observe that
f(pi < Pi < oo Xo = i) = lim
? T i = l
OO
= lim
n~>oo
= TO
TO=1
Xo =
Thus, after combining this with
< Pi
m = l
j =m< 
Pi
Xo - i
!To = i ) E [ l - .
yo xo = j).
0, ...,Xn)\X0= 
j]
< oo|Xo = i) — 1, we have
<Pi\Xo = i
oo
= j),
which, because V(pj < Pi\X0 = i) > 0, is possible only if P(pi < oo[X0 = j) —
1. In particular, we have now proved that j—>i and therefore that i<->j.

3 MORE ERGODIC THEORY OF MARKOV CHAINS
Similarly,
< oo
and so
< Pi Xo = i)
Pj
Xo = i
oo Xo = i)
oo |
Hence, i-+
Finally,
oo Xo = ijFlpj < pi Xo = i\ =
=> F(Pj < oo|X0 = i) = 1.
oo \ Xo =
< ft < oo | Xo = j) = P(pj < oo j Xo = i)P(pi < Pj Xo = j).
>j < oo|X0 =
Hence, because we now know that F(pi < oo|Xo = j) —
i) when i—*j, we see that i—>j implies
<Pi\X0= 
j) + F(pj <
<Pl\X0= 
j) + F(pz < pj < oo | Xo = j)
o = i)F(Pi < Pj | Xo - j) = 1,
since F(Pi = Pj |X0 = j) < F(Pi = oo|X0 = j) = 0. D
As an immediate consequence of Theorem 3.1.2 we have the following corol-
lary.
3.1.4 COROLLARY. If z<->j, then j is recurrent (transient) if and only ifi is.
Moreover, if i is recurrent, then F(pj < oo|Xo = i) is either 1 or 0 according
whether or not i communicates with j . In particular, ifi is recurrent, then
(Pn)ij = 0 for all n > 0 and all j which do not communicate with i.
When a chain is irreducible, all or none of its states possess any particular
communicating class property. Hence, when a chain is irreducible, we will say
that it is recurrent or transient if any one, and therefore all, of its states is.
3.1.2. Criteria for Recurrence and Transience: There are many tests
which can help determine whether a state is recurrent, but no one of them
works in all circumstances. In this subsection, we will develop a few of the
most common of these tests. Throughout, we will use u to denote the column
vector determined by a function u : S —> R.
We begin with a criterion for transience.
3.1.5 THEOREM. 
If U is a non-negative function on S with the property
that (Pu)j < (u)j for all i g S , then (Pu)j < (u)j for some j e § implies j is
transient.

3.1 Classification of States
PROOF: Set f = u - Pu, and note that, for all n > 1,
49
m = 0
ra-1
n-\
In •
m = 0
m = 0
Thus E[Tj|X0 = j] = Em=o(pm)ii ^ # F < °°' w h i c h' b y (2-3-7)' m e a n s
that j is transient. 
•
In order to prove our next criterion, we will need the following special case
of a general result known as Doob's Stopping Time Theorem.
LEMMA 3.1.6. Assume that u : § —> K is bounded below and that F is a non-
empty subset of S. If (Pu)j < u(i) for alii £ T and pr = inf{n > 1 : Xn e T},
then
E [u(XnApr) | Xo = i] < u(i) for alln>0 
and % G S.
Moreover, if the inequality in the hypothesis is replaced by equality, then the
inequality in the conclusion can be replaced by equality.
PROOF: Set An = {pr > n}. Then, An is measurable with respect to
(Xo,... ,Xn), and so, by (2.1.1), for any i,
E[u(X(n+1)Apr) 
\X0=i]= 
E[u(XnApr),
= E[u{XnApr), AnZ \X0 =
), An n {xn = k}\xo 
= i]
, An n {Xn = k} \ Xo = i]
< E[u{XnApr), AnC \X0=i] 
+ E[u(XnApr), An\X0 = i]
= E[u(XnApr)\X0 
= i].
Clearly, the same argument works just as well in the case of equality. 
•
THEOREM 3.1.7. Assume that j is recurrent, and set C = {i : i<->j}. If
u : S —> [0, oo) is a bounded function and either u(i) = (Pu)j or u(j) >
u(i) > (Pu), for all i G C\ {j}, then u is constant on C. On the other hand,
if j is transient, then the function u given by
1 
ifi= 
j
j < OO|XQ = i) 
if i 7^ j
is a bounded, non-constant solution to u(i) = (Pu); for all i

50 
3 MORE ERGODIC THEORY OF MARKOV CHAINS
PROOF: In proving the first part, we will assume, without loss in generality,
that C = S. Now suppose that j is recurrent and that u(i) = (Pu)j for i =^ j .
By applying Lemma 3.1.6 with F = {j}, we see that, for i ^ j ,
u{i) = u(j)¥(Pj 
< n j Xo = i) +E[u(Xn), 
Pj>n\X0 
= i].
Hence, since, by Theorem 3.1.2, P(pj < OO|XQ = i) = 1 and u is bounded, we
get u(i) = u(j) after letting n —» oo. Next assume that u(j) > u(i) > (Pu)j
for all i ^ j . Then, again by Lemma 3.1.6, we have
u(j) > u(i) > u{j)F(Pj <n\X0=i) 
+E[u{Xn), 
pj>n\X0 
= i],
which leads to the required conclusion when n —> oo.
To prove the second part, let u be given by the prescription described, and
begin by observing that, because j is transient,
1 > P(pj < oo | Xo = j) = Pjj + ^2Pjiu(i) 
> Pjj + (1 - Pjj) infu(i).
From this, one sees first that Pjj < 1 and then that inf,^ u{i) < 1 = u(j).
That is, u is bounded and non-constant. At the same time, when i ^ j , by
conditioning on what happens at time 1, we know that
u(i) = ¥(Pj < oo | Xo = i) = Py + J2 pik]P(Pj < oo | Xo = k) = (Pu);. D
3.1.8 THEOREM. 
Let {Bm : m > 0} be a non-decreasing sequence of
non-empty subsets of S with the property that
P(3n G N Xn $ Bm | Xo = j) = 1 for some j G Bo and all m > 0.
If there to exists a non-negative function u satisfying (Pu)j < u(i), i ^ j and
am = inf.rfn Ui —> oo as m —> oo, then j is recurrent.
PROOF: For each m > 0, set FTO = {j} U BmC, and take prm = inf{n > 1 :
Xn G Fm} = pj A rm, where Tm = inf{n > 1 : Xn g Bm}. By Lemma 3.1.6,
u(j) > E[u(XnAprm 
)\X0=j}> 
a m P ( r m < n A Pj \ Xo = j)
for all n > 0. Hence, because P(rm < oo|Xo = j) = 1, we conclude, after
letting n —> oo, that u{j) > amF(rm < Pj\Xo = j) for all m > 0, and therefore
limm^ooF(Tm < Pj\Xo = j) = 0. But this means that
j < oo
Xo = j) > V(Pj <Tm\X0= 
j)
= 1 - P(rm < Pj | Xo = j) / 1,
and so j is recurrent. 
•

3.1 Classification of States 
51
COROLLARY 3.1.9. Assume that P is irreducible, and let {Fm : m > 0} be
a non-decreasing sequence of non-empty, finite subsets of the state space. If
j G FQ and there exists u a non-negative function which satisfies (Pu)j <
u(i), i ^ j and infi(£pm Ui —> oo, then j is recurrent.
PROOF: In view of Theorem 3.1.8, it suffices for us to check that P(3ra e
N Xn i Fm\X0 = j) = 1 for all m > 0. To this end, let rm = inf{n >
1 : Xn <£ Fm}. By irreducibility, P(rm < oo|X0 = i) > 0 for all m and i.
Hence, because Fm is finite, for each m there exists a 6m G (0,1) and Nm > 1
such that P(rm > Nm\Xo = i) < 9m for all i G Fm. But this means that
P(rm >{£ + l)Nm | Xo = j) equals
Y, P(rm > {£ + l)Nm & XlNm 
=i\X0=j)
= J2 F(Tm >Nm\X0 = i)P(rm > lNm & XiNm 
=i\X0=j)
<0mV(jm > £Nm\X0 
=j).
Thus, P(rTO > £Nm\X0 = j) < 9e
m, and so P(rm = oo|X0 = j) = 0. 
•
Remark: The preceding criteria are examples, of which there are many oth-
ers, which relate recurrence of j G S to the existence or non-existence of
certain types of functions which satisfy either Pu = u or Pu < u on S \ {j}.
All these criteria can be understood as mathematical implementations of the
intuitive idea that
u(i) = (Pu), or (Pu)j < u(i) for i ^ j ,
implies that as long as Xn j^ j , u(Xn) will be "nearly constant" or "nearly
non-increasing" as n increases. The sense in which these "nearly's" should be
interpreted is the subject of martingale theory, and our proofs of these criteria
would have been simplified had we been able to call on martingale theory.
3.1.3. Periodicity: Periodicity is another important communicating class
property. In order to describe this property, we must recall Euclid's concept
of the greatest common divisor gcd(S) of a non-empty subset SCZ. Namely,
we say that d G Z+ is a common divisor of S and write d\S if ^ G Z for
every s G S. Clearly, if 5" = {0}, then d\S for every d G Z+, and so we take
gcd(5f) = oo. On the other hand, if S ^ {0}, then no common divisor of S
can be larger than min{|s| : s G S \ {0}}, and so we know that gcd(S') < oo.
Our interest in this concept comes from the role it plays in the ergodic
theory of Markov chains. Namely, as we will see below, it allows us to distin-
guish between the chains for which powers of the transition probability matrix
converge and those for which it is necessary to take averages. More precisely,
given a state i, set
(3.1.10) 
S{i) = {n > 0 : (Pn)u > 0} and d(i) = gcd(Sr(i)).

52 
3 MORE ERGODIC THEORY OF MARKOV CHAINS
Then d(i) is called the period of the state i, and, i is said to be aperiodic if
d{i) = 1.
As we will see, averaging is required unless i is aperiodic. However, before
we get into this connection with ergodic theory, we need to take care of a
few mundane matters. In the first place, the period is a communicating class
property:
(3.1.11) 
i<r+j => d(i) = d(j).
To see this, assume that (Pm)jj > 0 and (P*)^ > 0, and let d be a common
divisor of S{i). Then for any k £ S(j), 
\vm+k+n)il 
> 
(Pm)ij(Pk)jj(Pn)ji
> 0, and so m + k + n G S(i). Hence d\{m + k + n : k G S(j)}. 
But, because
m + n £ S(i), and therefore d\(m + n), this is possible only if d divides S(j),
and so we now know that d(i) < d(j). After reversing the roles of i and j ,
one sees that d(j) < d(i), which means that d(i) must equal d(j).
We next need the following elementary fact from number theory.
3.1.12 THEOREM. 
Given 0 ^ S C Z with S ^ {0}, gcd(5) < min{|s| : s G
5 \ {0}} and equality holds if and only if {gcd(S), -gcd(5)} n S ^= $. More
generally, there always exists an M G Z +, { a m } ^ C Z, and {s m}f C S such
that gcd(S) = ]Tf amsm. 
Finally, ifSCN 
and (sl,s2) 
e S2 = > si + s2 G
5, then tiiere exists an M G Z + sucii
{mgcd(S') : m > M } = { s £ S : s > Mgcd(5)}.
PROOF: The first assertion needs no comment. To prove the second assertion,
let S be the smallest subset of Z which contains S and has the property that
(si, S2) G 512 =>- si ±S2 G 5. As is easy to check, S coincides with the subset
of Z whose elements can be expressed in the form ]T^ amsm for some M > 1,
{a m}f C Z, and {sm}^ 
C 5. In particular, this means that gcd(5)|5r, and
so gcd(S') < gcd(5). On the other hand, because S C S, gcd(5)|5. Hence,
gcd(5) = gcd(S'), and so, by the first part, we will be done once we show that
gcd(5) G S. To this end, let m = min{s G Z + : s £ S}. We already know
that gcd(iS) < m. Thus, to prove the equality, we need only check that m\S.
But, by the Euclidean algorithm, for any s G S, we can write s = am + r for
some (a, r) G Z2 with 0 < r < m. In particular, r = s — am G S. Hence, if
r ^ O , then r would contradict the condition that m is the smallest positive
element of S.
To prove the final assertion, first note that it suffices to prove that there
is an M G Z+ such that {mgcd(S) 
: m > M} C S. To this end, begin by
checking that, under the stated hypothesis, S = {s2 — «i : (si, S2) 6 SU{0}}.
Thus, gcd(S') = s2 - si for some s ^ S U {0} and s2 G 5 \ {0}. If si = 0,
then mgcd(S) 
= ms2 G S for all m G Z +, and so we can take M = 1. If
si 7^ 0, choose a G Z + so that Si = agcd(S'). Then, for any (m,r) G N2 with
0 < r < o,
(a2 + ma + r)gcd(S') = ms\ + rs2 + (a — r)s\ = (m + a — r)s\ + rs2 G S.

3.2 Ergodic Theory without Doeblin 
53
Hence, after another application of the Euclidean algorithm, we see that we
can take M = a2. 
•
As an immediate consequence of Theorem 3.1.12, we see that
(3.1.13) 
d{i) < oo ==> (Pnd(i))ri > 0 for all sufficiently large n e Z+.
In particular,1
(3.1.14) 
i is aperiodic •<=>• (Pn)u > 0 for all sufficiently large n G Z+.
We close this subsection with an application of these considerations to the
ergodic theory of Markov chains on a finite state space.
3.1.15 COROLLARY. Suppose that P is an transition probability matrix on
a finite state space S. If there is an aperiodic state jo € S such that i~*jo for
every i £ § , then there exists an M G Z+ and an e > 0 such that (PM)ij0 > e
for all ieS. 
In particular, (cf. (2.3.9))
/j,Pn -TV 
< 2(1 - C)[M] for all n £ Z+ and initial distributions
PROOF: Because jo is aperiodic, we know that there is an Mo G N such that
(Pn)jojo > 0 for all n > Mo- Further, because i—>jo, there exists an m(i) G Z+
such that (Pm(i))ij0 > 0. Hence, (Pn)^0 > 0 for all n > m(i) + Mo. Finally,
take M = Mo + maxie§m(i), e = mmi^§(PM)ij0, and apply (2.2.3). 
•
3.2 Ergodic Theory without Doeblin
In this section, we will see to what extent the results obtained in Chap-
ter 2 for Markov chains which satisfy Doeblin's condition can be reproduced
without Doeblin's condition. The progression which we will adopt here runs
in the opposite direction from that in Chapter 2. That is, here we will start
with the most general but weakest form of the result and afterwards will see
what can be done to refine it.
3.2.1. Convergence of Matrices: Because we will be looking at power
series involving matrices which may have infinitely many entries, it will be
important for us to be precise about what is the class of matrices with which
we are dealing and in what sense our series are converging. For our purposes,
the most natural class will be of matrices M for which
(3.2.1) 
||M|| u, v=sup^|(M) 0-|
is finite, and the set of all such matrices will be denoted by MU)V(S). An easy
calculation shows that MU)V(S) is a vector space over R and that || • ||UiV is a
good norm on Mu,„(§). That is,
||M||UjV = 0 if and only if M = 0,
1 The "if" part of the following statement depends on the existence of infinitely many prime
numbers.

54 
3 MORE ERGODIC THEORY OF MARKOV CHAINS
||aM||UiV = H||M||Uit) 
foraeM,
and
||M + M'||u,v < ||M||U]V + ||M'||UiV.
Slightly less obvious is the fact that
„ 
MM' exists and
(M.J) 
(M.M^M^f 
^ 
| | M M , | k v £ | | M | | u v | | M 1 |
To see this, observe first that, since
]T|(M)ifc||(M')fci| < (X)l(M)ifc| J sup|(M')fei| <oo,
k 
V k 
/ ^
the sum in
(MM')o- = X)(M)ifc(M')fej
k
is absolutely convergent. In addition, for each i,
and so the inequality in (3.2.2) follows.
We next want to show that the metric which || • || u v determines on MU)V(§)
is complete. In order to do this, it will be useful to know that if {Mn}o° C
MUiV(§), then
Mij = lim (Mn)j7- for each (i,j)
n~>oo
Indeed, by Fatou's Lemma, Theorem 6.1.10,
E
^^ \Mij\ < lim V" |(Mn)jj| for each i £ S,
and so (3.2.3) is proved.
Knowing (3.2.3), the proof of completeness goes as follows. Assume that
{M«}g° C MU)V(S) is || • ||UiV-Cauchy convergent: lim^^oo supra>TO ||Mn -
Mm||UiV = 0. Obviously, for each (i,j) G S2, {(Mn)y}o° is Cauchy convergent
as a sequence in R. Hence, there is a matrix M such that, for each 
(i,j),
(M)ij = limn^oo(M.n)ij. 
Furthermore, by (3.2.3),
||M - Mm||UjV < lim ||Mn - Mm||u,v < sup ||Mn - Mm||u,v.
n—too 
n>m
Hence, ||M-M m|| u > v - ^ 0.

3.2 Ergodic Theory without Doeblin
55
3.2.2. Abel Convergence: As we said in the introduction, we will begin
with the weakest form of the convergence results at which we are aiming.
That is, rather than attempting to prove the convergence of (Pn)ij or even
the Cesaro means •- Yl^n=o0^m)ij a s n ~~* °°> w e w1^ begin by studying the
Abel sums (1 - s) f^=0 sm(Pm)ij 
a s s / 1 .
We will say that a bounded sequence {xn}^ C K is Abel convergent to x if
lim(l - s
n=l
It should be clear that Abel convergence is weaker than (i.e., is implied by)
ordinary convergence.2 Indeed, if xn —> x, then, since (1 — s) Y^f sn = 1,
for any N:
- (1 - s)
n=0
= (1 - S)
OOE
n=0
sn(x-xn)
< (1 — s) y sn\x — xn\ < N(l — s) sup \x — xn\ + sup \x — xr
^ 
neN 
n>N
Hence,
lim x-(l 
- s)
< lim sup |a; — xn\ = 0
if xn —> x. On the other hand, although {(—l)n}f° fails to converge to
anything,
n = 0
+ s
• 0 ass/1.
That is, Abel convergence does not, in general imply ordinary convergence.
With the preceding in mind, we set
(3.2.4)
R(s) = (1 - s) ]T snPn 
for se [0,1).
However, before accepting this definition, it is necessary to check that the
above series converges. To this end, first note that, because Pra is a transition
probability matrix for each n > 0, ||Pn||UiV = 1. Hence, for 0 < m < n,
se\\P%,v<sm,
£=0
e=o
u,v
t=m
2 In Exercise 3.3.1 below, it is shown that Abel convergence is also weaker than Cesaro
convergence.

56
3 MORE ERGODIC THEORY OF MARKOV CHAINS
and so, by the completeness proved above, the series in (3.2.4) converges with
respect to the || • ||UjV-norm.
Our goal here is to prove that
(3.2.5)
and the key to our doing so lies in the renewal equation
\XQ = j \ ) 
II I = J
>J < oo|X 0 = i)-Kjj 
if i ^ j ,
(3.2.6)
)n
for n > 1,
m=l
where
= m
-^0 = *)'
which is an elementary application of (2.1.13):
n 
=jkPj=m\X0=i)
m=l
n
= 53'
m=l
Next, for s G [0,1), set
_ m = j I Xo = j
=m
X0 = i).
= i],
m = l
and, starting from (3.2.6), conclude that
oo 
/ n
(R(s)).. = (1 - s)^- + (1 - s) J2 s" E •
n=l 
\m=l
oo
— H — s^<5- • 4- M — <*) S~^ 
<imf(m)-
m=l
53 
3
n-
m(P
n~
m)jj
)
That is,
(3.2.7) 
(R(s)) .. = (1 - 8)5^- + /(
Given (3.2.7), (3.2.5) is easy. Namely,
for s e [0,1).
1 - s
-/(«),

3.2 Ergodic Theory without Doeblin 
57
Hence, if j is transient, and therefore f(l)jj < 1,
On the other hand, if j is recurrent, then, since
1 
cm 
rn-i
X 
•& 
\ 
^ 
f 
-x
-j 
= / , s 
/ Tl
~ s 
e=o
the Monotone Convergence Theorem says that
= E -Y
1 - S 
, - 
-
m=l 
m = l
as s / 1. At the same time, when i ^ j ,
(R(s))y = /(s)y (R(*))# /* P(ft
3.2.3. Structure of Stationary Distributions: We will say that a prob-
ability vector /j, is P-stationary and will write \i G Stat(P) if fj, — /xP. Obvi-
ously, if n is stationary, then fi = /LtR(s) for each s G [0,1). Hence, by (3.2.5)
and Lebesgue's Dominated Convergence Theorem,
If j is transient, then TT^ = 0. On the other hand, if j is recurrent, then,
by Theorem 3.1.2, TTJJ is either TTJJ or 0, according to whether i*-+j or i */>j.
Hence, in either case, we have that
(3.2.8) 
M G Stat(P)
We next want to show that
"33-
-Kjj > 0 and C - {i : i^j} 
=^> TTC G Stat(P)
when (irc)i = lc(i)Tru-
(3-2-9) 
, 
, n , -
To do this, first note that TTJJ > 0 only if j is recurrent. Thus, all i s C
are recurrent and, for each s e (0,1), (R(s))fc£ > 0 <^=> (fc,^) G C2.
In particular, (vrc)i = limsy<i(R(s)) .. for all i, and therefore, by Fatou's
Lemma,
S / 1

58 
3 MORE ERGODIC THEORY OF MARKOV CHAINS
Similarly, for any i,
kec
since
( ) ) i f c ( ) « 
^ 
) ) " — ^ 
= (TTC)« 
as s / 
1.
fcec
But if strict inequality were to hold for some i, then, by Fubini's Theorem,
Theorem 6.1.15, we would have the contradiction
Hence, we now know that irc = TTCP. Finally, to prove that TVC G Stat(P),
we still have to check that ^2i('KC)i = 1- However, we have already checked
that TTC = TT^P, and so we know that TVC = 7TCR(s). Therefore, since, as we
already showed, ^2i('KC)i < 1, Lebesgue's Dominated Convergence Theorem
justifies
0 < TTtf 
^
i
which is possible only if Yli(nC)i 
~ 1-
Before summarizing the preceding as a theorem, we need to recall that a
subset A of a linear space is said to be a convex set if (1 — 9)a + 9a' € A for all
a, a' e A and 0 s [0,1] and that b € A is an extreme point of the convex set A
if 6 = (1 — 6)a + 8a' for some 6 e (0,1) and a, a' e ^4 implies that b = a' — a".
In addition, we need the notion of positive recurrence. Namely, we say that
j € S is positive recurrent if E[pj|Xo = j] < oo. Obviously, only recurrent
states can be positive recurrent. On the other hand, (1.1.13) together with
(1.1.15) show that that there can exist null recurrent states, those which are
recurrent but not positive recurrent.
3.2.10 THEOREM. 
Stat(P) is a convexsubset of Rs. Moreover, Stat(P) ^ 0
if and only if there is at least one positive recurrent state j G S. In fact, for
any fi e Stat(P), (3.2.8) holds, and JU is an extreme point in Stat(P) if and
only if there is a communicating class C of positive recurrent states for which
(cf. (3.2.9)) [i — nG. In particular, (fi)j = 0 for any transient state j and, for
any recurrent state j , either (/i)j is strictly positive or it is 0 simultaneously
for all states i's which communicate with j .
PROOF: The only statements not already covered are the characterization of
the extreme points of Stat(P) and the final assertion in the case when j is
recurrent.

3.2 Ergodic Theory without Doeblin 
59
In view of (3.2.8), the final assertion when j is recurrent comes down to
showing that if j is positive recurrent and *<->j, then i is positive recurrent.
To this end, suppose that j is positive recurrent, set C = {i : i<-+j}, and let
i G C be given. Then TT^P" = TVG for all n > 0, and therefore, by choosing
n so that (P71)^ > 0, we see that (-rvc)i > (irc)j(Pn)ji 
> 0.
To handle the characterization of extreme points, first suppose that fi ^
irc for any communicating class C of positive recurrent states. Then, by
(3.2.8), there must exist non-communicating, positive recurrent states j and
j ' for which (/J,)J > 0 < {fi)y- But, again by (3.2.8), this means that /J. =
9TTC + (1 - 0)v, where C = {i : i*^j}, 9 = Ej6c(A*)i £ (0,1), and (i/);
equals 0 or (1 — 9)~1(/j,)i depending on whether i is or is not in C. Clearly
v G Stat(P) and, because uy > 0 = (Ttc)y, v 7^ TTC"• Hence, [i cannot be
extreme. Equivalently, every extreme /i. is TVC for some communicating class
C of positive recurrent states.
Conversely, given a communicating class C of positive recurrent states,
suppose that TT° = (1— 9)n+9v for some 9 6 (0,1) and pair (/x, u) € Stat(P)2.
Then (/x), = 0 for all i £ C, and so, by (3.2.8), we see first that ^ = TTC and
then, as a consequence, that v = TVC as well. 
•
The last part of Theorem 3.2.10 shows that positive recurrence is a commu-
nicating class property. Indeed, if j is positive recurrent and C — {i : «<->j},
then irc G Stat(P), and so, since (TTC)J > 0, TTU = (7rc)i > 0. In particular,
if a chain is irreducible, we are justified in saying it is positive recurrent if any
one of its states is. See Exercise 3.3.3 below for a criterion which guarantees
positive recurrence.
3.2.4. A Small Improvement: The next step in our program will be the
replacement of Abel convergence by Cesaro convergence. That is, we will
show that (3.2.5) can be replaced by
1 
n-i
(3.2.11) 
lim (An)i7- = 7ri7, 
where Ara = - V P m .
rra=0
As is shown in Exercise 3.3.1, Cesaro convergence does not, in general, follow
from Abel convergence. In fact, general results which say when Abel con-
vergence implies Cesaro convergence can be quite delicate and, because the
original one was proved by a man named Tauber, they are known as Taube-
rian theorems. Fortunately, the Tauberian theorem required here is quite
straight-forward.
A key role in our proof will be played by the following easy estimate:
{am}S° C [0,1] & An = - } 
at
(3.2.12) 
n o
771
= > \An - An-m\ 
< — 
for 0 < m < n.
n

60 
3 MORE ERGODIC THEORY OF MARKOV CHAINS
The proof is:
I
n
n — \
ae
m
n — m — l
n(n - m)
> - ^
3.2.13 LEMMA. 
For all (i,j), limn^0o(An)^ < eyr^. In addition, for any j
and any subsequence {ri£ : £ > 0} C N,
lim {Ant)jj = a => lim (Ane)ij = ¥(pj < oo
PROOF: TO prove the first part, observe that
n - l
&>r ail i.
n 
n
- 
V1 
n i
m=0
which, together with (1.2.10) and (3.2.5), shows that limn_^oo(ATj)jj < eirtj <
e-Kjj.
To handle the second part, use (3.2.6) to arrive at
n - l
(An)l3 =
for i
Hence,
n |
n - l
TTl—l
n - l
m— 1
m
n
where, in the second inequality, we have used (3.2.12) plus X)m=i /(r7i)«j <
1. Finally, by Lebesgue's Dominated Convergence Theorem, 5^Q~ 
l^f(m)ij
tends to 0 as n —» oo, and therefore, by applying the above with n = ng and
letting £ —> oo, we get the desired conclusion. 
•
We can now complete the proof of (3.2.11). Namely, if TTJJ = 0, then the
first part of Lemma 3.2.13 guarantees that limn^oo(An)jj = 0 = 7!YJ for all
i. Thus, assume that TTJJ > 0. In this case Theorem 3.2.10 says that j must
be positive recurrent and TVC £ Stat(P) when C = {i : i<->j}. In particular,
TTjj = '}2iec('n'C)i(-^n)ij- 
At the same time, if a + = \ircin^oo(An)jj 
and the
subsequence {nt : £ > 0} is chosen so that (Ant)jj 
—> a+, then, by the
second part of Lemma 3.2.13 and Corollary 3.1.4,
i e C
lim
= a+.

3.2 Ergodic Theory without Doeblin
Hence, after putting these two remarks together, we arrive at
61
iec 
iec
Similarly, if a~~ = limr_^oc(Ara),-,-, we can show that a~ = TTJJ , and so we
now know that TTJJ > 0 =>• limn^00(Arj)j;,- = TTJJ, which, after another
application of the second part of Lemma 3.2.13, means that we have proved
(3.2.11).
3.2.5. The Mean Ergodic Theorem Again: Just as we were able to use
Theorem 2.2.5 in §2.3.1 to prove Theorem 2.3.4, so here we can use (3.2.11)
to prove the following version of the mean ergodic theorem.
3.2.14 THEOREM. Let C a communicating class of positive recurrent states.
Iff(X0 e C) = 1, then
lim E
n - l
= 0.
See Exercises 3.3.9 and 3.3.11 below for a more refined statement.
PROOF: Since P(Xm e C for all m e N) = 1, without loss in generality we
may and will assume that C is the whole state space. In keeping with this
assumption, we will set TT = TT .
Next note that if /z, = P(X0 = i), then
E
n-l
n m = 0
— TTii
n - l
"33
m = 0
X0=i
and so it suffices to prove that
lim E
• 
n-l
1 
V^
m = 0
Xn=i
= 0 for each i e S.
But, because TTU > 0 for all i e S and

62
3 MORE ERGODIC THEORY OF MARKOV CHAINS
it is enough to prove the result when TT is the initial distribution of the Markov
chain. Hence, from now on, we will be making this assumption along with
C = §.
Now let f the column vector whose ith component is l^y(i) — TTJJ. Then,
just as in the proof of Theorem 2.3.4,
E
n
n-1E
'33
Since n e Stat(P),
and therefore the preceding becomes
E
n - i
m7r(/Amf),
m = l
where (/ATOf)i = (f)i(Amf)i.
Finally, by (3.2.11), for each e > 0 there exists an Ne G Z+ such that
|7r(/Anf)| <
< e for all n >
Hence, we find that
lim E
m=0
3
< lim ^ 5 - 7 m 7r(/Amf)|+ lim
2e
nz E
m < e. 
•
3.2.6. A Refinement in The Aperiodic Case: Our goal in this subsection
is to prove that (cf. (3.2.5))
(3.2.15) 
If j is transient or aperiodic, then lim (Pn)j7- = TTU for all i s S.
n—»oo
Of course, the case when j is transient requires very little effort. Namely, by
(2.3.7), we have that
j transient
(Pn)ij<E[Tj\X0=j]<oo,
n=0

3.2 Ergodic Theory without Doeblin
63
and therefore that limn_^oo(Pn)iJ- = 0. At the same time, because F(pj =
oo|Xo = j) > 0 when j is transient, TTJ,- < TTJJ = 0. Thus, from now on, we
will concentrate on the case when j is recurrent and aperiodic.
Our first step is the observation that (cf. §2.3.2)
(3-2'16)
if j is aperiodic, then there exists an N € Z+ such that
max
\<m<n
<m)
p<m) =n Xo = j) > 0 for all 
n>N.
J
To check this, use (3.1.14) to produce an N e Z+ such that (Pn)jj > 0 for
all n > N. Then, since
=n
X0=j) 
for n > 1,
m = l
(3.2.16) is clear.
The second, and key, step is contained in the following lemma.
3.2.17 LEMMA. 
Assume that j is aperiodic and recurrent, and set o~ =
limri^oo(P")7-7- and OL^ = limn^oo(Pn)jj. Then there exist subsequences
{nj : i > 1} and {n/ : £ > 1} such that
of = lim (Pn
for all r > 0.
PROOF: Choose a subsequence {n^ : £ > 1} so that (Pne)jj —> Q!j", and,
using (3.2.16), choose iV > 1 so that maxi<m<n F(pj 
= n\X0 = j) > 0 for all
n>N. 
Given r > N, choose 1 < m < r so that <5 = P(/^m) = r|X0 = j) > 0.
Now for any M £ Z+, observe that, when nt > M + r, (Pn')jj is equal to
= j &
= j &
Furthermore,
P(Xn, = j & n, - M > p$
np—M
-M> p{™
o = j)
Xo=.
- M l
< (1 — S) sup
= j)
while
V{Xni=j
- M | Xo = j) <
> ne - M\X0 = j).

64 
3 MORE ERGODIC THEORY OF MARKOV CHAINS
Hence, since j is recurrent and therefore ¥(pj 
< oojXo = j) = 1, we get
a+ < 6 lim (P"'- r)jj + (1 - 8) sup (P")^
i^oo 
n>M
after letting £ —> oo. Since this is true for all M > 1, it leads to
a t < 5 lim ( P ^ - r ) i i + (1 - S)a+,
1-+CO
which implies 
\jml^00(Pnt~r)jj > at.. But obviously Iim<!_+oo(Pra*"r)ij <
a t , and so we have shown that lim^oo(Pra€~r)jj = a~j~ for all r > N.
Now choose L so that ni > N, take n^ = n^+x, — AT, and conclude that
lim^00(P™^r)jj = a+ for all r > 0.
The construction of {n^~ : £ > 1} is essentially the same and is left as an
exercise. •
3.2.18 LEMMA. If j is aperiodic and recurrent, then limn_>oo(Pn);,-:,- < -Kjj.
Furthermore, if the subsequences {nj : £ > 1} are the ones described in
Lemma 3.2.17, then lim^ o o(P r^ )y = a^ for any i with i<-+j.
PROOF: TO prove the second assertion, simply note that, by Lemma 3.2.17
and Lebesgue's Dominated Convergence Theorem,
i)af.
Turning to the first assertion, we again use the result in Lemma 3.2.17 to
obtain
N
aj 2.^1
for all TV > 1
then we will
. Thus, if we show
N
r = l
know that
= lin:
that
- J (
AT
13 -
oo
r=l
• X 0=j)(P r
Xo=j)<l,
which is equivalent to the first assertion.

3.2 Ergodic Theory without Doeblin
To prove (*), note that, for any n > 1,
65
r = l
>r
X0=j) (P"
r = l
r=l
and so, since
>
o = i) =
X0=j) (Pn
>r
for all n > 1. But X]™=i P(/3j >
we have now proved that
r = l
r=1
r = l
= J)(Pn~r)jj = 1 when n = 1, and so
r | Xo = j)
r = l
r = l
for all n > AT > 1. •
We can now complete the proof of (3.2.15) when j is recurrent and aperiodic.
By the first part of Lemma 3.2.18, we know that limn_»oo(Prl)J.?- = 0 if TTJJ = 0.
Thus, if Ttjj = 0, then, for any i,
lim (Pn)i7- = lim
>j = r\X0 = i)(Pn-r)jj 
= 0 =
r = l
In order to handle the case when j is positive recurrent, set C = {i : i<^>j},
and take TTC accordingly. Then, TVC G Stat(P). In particular, by the last part
of Lemma 3.2.18 and Lebesgue's Dominated Convergence Theorem,
tec
and so (Pn)jj
theorem,
- Finally, if i ^ j , then, again by the Lebesgue's
r=l
3.2.7. Periodic Structure: The preceding result allows us to give a finer
analysis even in the case when the period is not 1. Namely, consider a Markov

66 
3 MORE ERGODIC THEORY OF MARKOV CHAINS
chain with transition probability matrix P which is irreducible and recurrent
on §, and assume that its period is d > 2. The basic result in this subsection
is that there exists a partition of S into subsets Sr, 0 < r < d, with the
properties that
(!) (Pmd+r)jk 
> 0 =>• r(k) - r(j) = r mod d,
(2) r(k) - r{j) = r mod d = ^ 
{Pmd+r)jk 
> 0 < (Pmd~r)fcj 
for 
a11
sufficiently large m > 1,
(3) for each 0 < r < d, the restriction of P d to §r is an aperiodic, recurrent,
irreducible transition probability matrix,
where we have used r(j) to denote the 0 < r < d for which j &Sr.
To prove that this decomposition exists, we begin by noting that, for 0 <
r < d,
(*) 
3 m > 0 (Pmd+r)zj 
> 0 => 3 n > 1 (Pnd~r)ii > 0.
Indeed, by irreducibihty and the Euclidean algorithm, we know that there
exists an m' > 0 and 0 < r' < d such that that {Pm'd+r')ji 
> 0. Fur-
thermore, by (3.1.13), (p(rn'+m")d+r'}.. > (pm'd+r'-j. .(pm'V).. > Q for &n
sufficiently large m"'s, and so we may and will assume that m' > 1. But then
(P(m+m')d+(r+r')).. > 0 ; a n d SQ d\(j. + r>^ whjch, because 0 < r,r' < d, means
that r' = 0 if r = 0 and that r' = d - r if r > 1.
Starting from (*), it is easy to see that, for each pair (i,j) € S2, there
is a unique 0 < r < d such that (P m d + r)y > 0 for some m > 0. Namely,
suppose that (Pmd+%- > 0 < (P m' d + r')y for some m,m' £ N and 0 <
r,r' < d. Then, by (*), there exists an n > 1 such that (Pnd~r)ji 
> 0 and
so (p(m'+n)d+(r'-r)^u > Q 
S i n c e t h i s m e a n s t h a t d\(r> _ r ) ; w e have proved
that r = r'.
Now, let IQ be a fixed reference point in S, and, for each 0 < r < d, define Sr
to be the set of j such that there exists an m > 0 for which (Pmd+r)ioj 
> 0.
By the preceding, we know that the Sr's are mutually disjoint. In addi-
tion, by irreducibihty and the Euclidean algorithm, S = Ur=o ^ - Turning
to the proof of property (1), use (*) to choose n > 0 and n' > 1 so that
> 0 < (Pn'<i-r<fe>)feio. Then
and so d\{r(j) + r — r{k)). Equivalently, r[k) — r(j) = r mod d. Conversely,
if r(k) - r(J) = r mod d, choose n > 0 and n' > 1 so that (Pnd+rik))iok 
> 0
and (Pn'd-r(j))jio 
> 0. Then (p(n+mW)d+r^jk 
> 0 for a n y m > i satisfying
(Pmd)i0i0 > °- S i n c e. by (3.1.13), (Pmd)ioio 
> 0 for all sufficiently large m's,
this completes the left hand inequality in (2), and the right hand inequality
in (2) is proved in the same way. Finally, to check (3), note that, from (1),
the restriction of Pd to Sr is a transition probability matrix, and by (2), it is
both irreducible and aperiodic.
The existence of such a partition has several interesting consequences. In
the first place, it says that the chain proceeds through the state space in a

3.3 Exercises 
67
cyclic way: if it starts from i, then after n steps it is in §r(i)+n, where the
addition in the subscript should be interpreted modulo d. In fact, with this
convention for addition, we have that
(3.2.19) 
P n l S r = lBr+n.
To see this, simply observe that, on the one hand, Pnl§r,(z) = 0 unless i G
§r_i_n, while, on the other hand, J2r'=o^>n^r' 
= •"•• Hence, i <£ §r+n ==>
(P nlsji = 0, whereas i G § r + n => 1 = D ^ P l s , . , ) * = (Pl^+Ji-
Secondly, because, for each 0 < r < d, the restriction of P d to Sr is an
irreducible, recurrent, and aperiodic, transition probability matrix, we know
that, for each 0 < r < d and j G Sr, there exists a irjj G [0,1] with the
property that (Pmd)ij —> n^ for all (i,j) G S2. More generally,
(3.2.20) 
lim (Pmd+%- = \ " 
^r[J) 
r[l) 
- s m o d d
m^°° 
[ 0 
otherwise.
In particular, if (i,j) G (Sr)2
; then
m=0 s=0 
771=0
Hence, since we already know that (Ara)jj —> TTJJ, it follows that
(3.2.21) 
7rjr) = d7Tjj for 0 < r < d and j € Sr.
In the case when P is positive recurrent on S, so is the restriction of P d to
each Sr, and therefore Yljesr ^jj = ^ Thus, in the positive recurrent case,
(3.2.21) leads to the interesting conclusion that TTS assigns probability ^ to
each Sr. See Exercise 3.3.12 and 5.6.7 below for other applications of these
considerations.
3.3 Exercises
EXERCISE 3.3.1. Just as Cesaro convergence is strictly weaker (i.e., it is im-
plied by but does not imply) than ordinary convergence, so, in this exercise, we
will show that Abel convergence is strictly weaker than Cesaro convergence.
(a) Assume that the radius of convergence of {an}o° Q K is less than or
equal to 1. That is, linin^oo |an|" < 1. Set R(s) = (1 — s) ^^° snan for
s G [0,1) and An = ^ Xlo 
am f°r n ^ 1> a nd show that, for s G [0,1),
R(s) = (1 - s)2 YT nsn~lAn. 
Use this to conclude that
lim An = a G K ==> lim R(s) = a.
n^oo 
s/1
(b) Take an = (—l)ra+1n for n > 0, check that the radius of convergence of
{ara}o° is 1, and show that
\ 
if n is even 
^
m 
<\ - s)
and 
(1 - s) > s a m = -^ 
-jr.
j ^ 
m 
(1 + S) 2
, 
i
-i + i 
ifnisodd
Hence, {ara}o° is Abel convergent to 0 but is Cesaro divergent.

68 
3 MORE ERGODIC THEORY OF MARKOV CHAINS
EXERCISE 3.3.2. Recall the queuing model in Exercise 1.3.12. Show that
{Qn '• n > 0} is an N-valued Markov chain conditioned to start from 0, and
write down the transition probability matrix for this chain. Further, for this
chain: show that 0 is transient if E[i?i] > 0, 0 is null recurrent if E[i?i] = 0,
and that 0 is positive recurrent if E[i?i] < 0. In order to handle the case when
E[i?i] = 0, you might want to refer to Exercise 1.3.11.
EXERCISE 3.3.3. Here is a test for positive recurrence. Namely, given a
transition probability matrix P and an element j of the state space S, set
C = {i £ S : i^j}- 
Assume u is a non-negative function on C with the
property that
u(i) > (Pu)j + e for alii e C \ {j}
for some e > 0.
(a) Begin by showing that
E[u(X{n+1]Ap.) 
| Xo = j] < E[u(XnApj) 
| Xo = j] - eF(Pj 
>n\X0=j),
and use this to conclude that j is positive recurrent.
(b) Suppose that S = Z and that \i\ > £ \ \J\(P)ij + e for all i e Z \ {0}.
Show that 0 is positive recurrent for the chain determined by P.
EXERCISE 3.3.4. Consider the nearest neighbor random walk on Z which
moves forward with probability p € (|;l) and backward with probability
q = 1 — p. In other words, we are looking at the Markov chain on Z whose
transition probability matrix P is given by (P)y = p if j = i + 1, (P)»j = q if
j = i — 1, and (P)^- = 0 if \j — z| ^ 1. Obviously, this chain is irreducible, and
the results in §§1.2.2-1.2.1 show that 0 is transient. Thus, the considerations
in Exercise 2.4.10 apply.
(a) Let P be constructed from P by the prescription in Exercise 2.4.10
when jo = 0, and, using (1.1.12), show that
otherwise.
(b) On the basis of Exercise 2.4.10, we know that 0 is recurrent for the chain
determined by P. Moreover, by part (b) of Exercise 3.3.3, one can check that
it is positive recurrent. In fact, by combining part (b) of Exercise 2.4.10 with
the computations in §1.1.4, show that
p-q
where the superscript IP is used to indicate that the expectation value is taken
relative to the chain determined by P.

3.3 Exercises
69
(c) Since P is irreducible, so is P. Hence, since 0 is positive recurrent for
the chain determined by P, there is a unique stationary probability vector for
P. Find this vector and use it to show that
if
EXERCISE 3.3.5. In section §1.2.4, we worked quite hard to prove that the
nearest neighbor, symmetric random walk on 1? is transient, and one might
hope that the criteria provided in §3.1.2 would allow us to avoid working
so hard. However, even if one knows which function u ought to be plugged
into Theorem 3.1.5 or 3.1.7, the computation to show that it works is rather
delicate. Namely, show that if a > 0 is sufficiently large and
u(k) =
for k G Z3
then (Pu)k < w(k) < u(0) when u is the column vector determined by the
function u and P is the transition probability matrix for the nearest neighbor
random walk on Z3. That is,
( P ) w = / S 
if Di=
t 0 otherwise.
What follows are some hints,
(a) Let k e Z3 be given, and set
3
M =
a2
a n d
M
for l ~ l ~
Show that (Pu)k < w(k) if and only if
2(l-4x?)i
(b) Show that (l - i ) " ^ > 1 + -^ and that
and conclude that (Pu)k < w(k) if
2M
6 '
where |x| = A/X^I^I is the Euclidean length of x = (£1,3:2,£3)

70 
3 MORE ERGODIC THEORY OF MARKOV CHAINS
(c) Show that there is a constant C < oo such that, as long as a > 1,
and put this together with the preceding to conclude that we can take any
a > 0 with a2 + 1 > 2C.
An analogous computation shows that, for each d > 3 and all sufficiently
large a > 0, the function fa2 + Xa(k)?) 
c a n be used to prove that
the nearest neighbor, symmetric random walk is transient in Zd. The reason
why one does not get a contradiction when d = 2 is that the non-constant,
non-negative functions which satisfies PM < u when d = 2 are of the form
log (a2 + (k)f + (k)2) and therefore do not achieve their maximum value at 0.
EXERCISE 3.3.6. As we said at the beginning of this chapter, we would be
content here with statements about the convergence of either {(Pn)jj : n > 0}
or {(An)ij 
: n > 0} for each (i,j) G §2. However, as we are about to see,
there are circumstances in which the pointwise results we have just obtained
self-improve.
(a) Assume that j is positive recurrent, and set C — {i : i*-+j}. Given
a probability vector fj, with the property that ^2it±c{n)i = 0, show that, in
general, (/zAn)i —> na and, when j is aperiodic, (/LtPn)j —> TCH for each
i€C.
(b) Here is an interesting fact about convergence of series. Namely, for each
m € N, let {am,n '• n > 0} be & sequence of real numbers which converges
to a real number bm as n —> oo. Further, assume that, for each n G N, the
sequence {am)Tl : m > 0} is absolutely summable. Finally, assume that
ooE
\bm\ < oo as n -> oo.
Show that
|am,n -
m 
/ 
j 
\um,n
n—>oo z—J
m = 0
= 0.
Hint: Using the triangle inequality, show that
\\am,n\ - \bm\ - |am,n - bm\ < 2|6n
and apply Lebesgue's Dominated Convergence Theorem to conclude that
OOE
m = 0
am,n-&m| <
OO
E da-.«
m—0
- \bm\) +E
m = 0
am,n -\bm
-
as n
oo .

3.3 Exercises
71
(c) Return to the setting in (a), and use (b) together with the result in (a)
to show that, in general, ||//An—7rc||v —> 0, and ||/iPn — irc\\v —>0whenj
is aperiodic. In particular, for each probability vector /J, with X^ec (/•*)* = 1>
lim sup{|/xP"f-7rcf|: 
| | / | | u < l } = 0 ,
n—>oo 
*• 
'
where f is the column vector determined by a function /. Of course, this is
still far less than what we had under Doeblin's condition since his condition
provided us with a rate of convergence which was independent of /x. In general,
no such uniform rate of convergence will exist.
EXERCISE 3.3.7. Here is an important interpretation of TVC when C is a
positive recurrent communicating class. Namely, let i be a recurrent state
and, for k e S, let //& be the expected number of times the chain visits k
before returning to i given that the chain started from i:
m = 0
X0=i
e [o,oo].
Determine the row vector /j, e [0,oo]s by (n)k = fJ*k-
(a) Show that, for all j e S,
= E
Pi
Thus, without any further assumptions about i, /x is P-stationary in the sense
that /i = /J.P.
(b) Clearly jii = 1 and J ^ /ij = oo unless % is positive recurrent. Nonethe-
less, show that fij = 0 unless i<->j and that fij s (0, oo) if i<->j.
Hint: Show that
Xo = i) = F(Pj 
<Pi\X0=j
(c) If i is positive recurrent, show that
.\ m— 1
Xo - i).
Equivalently, when i is positive recurrent,
E
v" ^ 
E\pi\X0 = i]
In words, (Tr0)^ is the relative expected amount of time the chains spends at
j before returning to i.

72
3 MORE ERGODIC THEORY OF MARKOV CHAINS
EXERCISE 3.3.8. We continue with the program initiated in Exercise 3.3.7 but
assume now that the reference point i is null recurrent. In this case, Yljit^j 
=
oo when /x e [0, oo)s is the P-stationary measure introduced in Exercise 3.3.8.
In this exercise we will show that, up to a multiplicative constant, fj, is the
only P-stationary v e [0, oo)s with the property that (y)j = 0 unless i—>j
(and therefore «<->j). Equivalently, given such a u, v — (v)ifi.
(a) Assume that v e [0, oo)s satisfies v = i/P. If {u)i = 1, show that, for
all j e § and n > 0,
= j &
n | Xo = k)
raA(pi-l)
E :
m=0
Hint: Work by induction on n > 0. When n = 0 there is nothing to do. To
carry out the inductive step, use (2.1.1) and Fubini's Theorem to show that
E(
=jkp%>n
X0 =
£ u)eF(Xn+1 = j & p% > n + 11 Xo = £).
(b) Assuming that v = vP and (u)j = 0 unless i—*j, show that v = (v)i/J,.
Hint: First show that v = 0 if (i/)j = 0, and thereby reduce to the case when
{u)i = 1. Starting from the result in (a), apply the Monotone Convergence
Theorem to see that the right hand side tends to [n)j as n —> oo.
EXERCISE 3.3.9. Let C be a communicating class of positive recurrent states.
The reason why Theorem 3.2.14 is called a "mean ergodic theorem" is that the
asserted convergence is taking place in the sense of mean square convergence.
Of course, mean square convergence implies convergence in probability, but, in
general, it cannot be used to get convergence with probability 1. Nonetheless,
as we will show here, when P(XQ 6 C) = 1 and j G C,
(3.3.10)
lim —
n^oo n
1{j}(xm) 
= Kjj with probability 1.
m=0
Observe that, in §2.3.3, we proved the individual ergodic theorem (2.3.10)
under Doeblin's condition holds, and (3.3.10) says that the same sort of in-
dividual ergodic theorem holds even when Doeblin's condition is not present.
In fact, there is a very general result, of which (3.3.10) is a very special case,

3.3 Exercises
73
which was proved originally by G.D. Birkhoff. However, we will not follow
Birkhoff and instead, as we did in § 2.3.3, we base our proof on the Strong Law
of Large Numbers, although' this time we need the full statement which holds
(cf. Theorem 1.4.11 in [9])) for averages of mutually independent, identically
distributed, integrable random variables.
(a) Show that it suffices to prove the result when P(XQ = i) = 1 for some
i eC.
(b) Set p\ = 0, and use p\ 
to denote the time of the roth return to i. If
and Ym =
l{j}(Xe),
show that, conditional o n l 0 = i, both {rm : ro > 1} and {Ym : m > 1}
are sequences of mutually independent, identically distributed, non-negative,
integrable random variables. In particular, as an application of the Strong
Law of Large Numbers and (3.2.5) plus the result in Exercise 3.3.7, conclude
that, conditional on XQ = i,
(*) 
lim —— = — and lim —
m—>oo 
ro 
7Tj2 
rn—>°° ro
lfj}(Xm)=Triiirij
e=o
'
p
= 
wij = ^ij with
with probability 1. Hence, limTO
probability 1.
(c) In view of the results in (a) and (b), we will be done once we check
that, conditional on XQ = i,
lim
n—too
7 1 - 1
*£
r\
with probability 1, where mn is the Z+-valued random variable determined
so that p\mn ' < n < p\mn . To this end, first show that
n - 1
Pi
n)
mn
Next, from the first part of (*), show that P(limn^0Ororl = oo|Xo = i)
Finally, check that, for any e > 0,
= 1.
sup — > e
in
m>M
m=M
X0 = i
>me\X0=i
, Pi>Me\X0 
= i],
and use this to complete the proof of (3.3.10).

74
3 MORE ERGODIC THEORY OF MARKOV CHAINS
(d) Introduce the empirical measure Ln, which is the random probabil-
ity vector measuring the average time spent at points. That is, (Ln)j =
^ Y^o~ l{«}(-^m)- By combining the result proved here with the one in (b)
of Exercise 3.3.6, conclude that lin^^oo ||Ln — TT^HV = 0 with probability 1
when P(X0 G C) = 1.
EXERCISE 3.3.11. Although the statement in Exercise 3.3.9 applies only to
positive recurrent states, it turns out that there is a corresponding limit theo-
rem for states which are not positive recurrent. Namely, show that if j is not
positive recurrent, then, no matter what the initial distribution,
/ 
n-i 
\
P lim - Vl{i}(Xm)=0 =1.
\ 
m=0 
/
When j is transient, E [5^o° -*-{i}(^™)] < °° anc' therefore the result is trivial.
To handle j are null recurrent, begin by noting that it suffices to handle
the case when P(Xo = j) = 1. Next, note that, conditional on Xo = j ,
{Pj 
— Pj '• TO > 0} is a sequence of independent, identically distributed,
Z+-valued random variables, and apply the Strong Law of Large Numbers to
see that, for any R > 1,
(m)
lim ^— > H(R)
TO
m
X0=j
>H(R) X0=j]=l,
where H(r) = |E[/>j A R\Xo = j] / oo as i? / oo. Hence, given XQ = j ,
-^ 
> oo with probability 1. Finally, check that, for any e > 0,
/ 
•, n-x
. n>N
Xo=j)<
sup
i n>N
X0=j
sup
TO
Xo=j\,
and combine this with the preceding to reach the desired conclusion.
EXERCISE 3.3.12. When j is aperiodic for P, we know that limn_>00(Pn)i:;-
exists for alii G S and is 0 unless j is positive recurrent. When d(j) > 1 and j
is positive recurrent, show that limn_»o
hand, even if d(j) > 1, show that lim,,
not positive recurrent.
will fail to exist. On the other
Pn)ij = 0 for any j e § which is
133

CHAPTER 4
Markov Processes in Continuous Time
Up until now we have been dealing with Markov processes for which time
has a discrete parameter n € N. In this chapter we will introduce Markov
processes for which time has a continuous parameter t S [0, oo), even though
our processes will continue to take their values in a countable state space S.
4.1 Poisson Processes
Just as Markov processes with independent, identically distributed incre-
ments (a.k.a. random walks) on Zd are the simplest discrete parameter Markov
processes, so the simplest continuous time Markov processes are those whose
increments are mutually independent and homogeneous in the sense that the
distribution of an increment depends only on the length of the time interval
over which the increment is taken. More precisely, we will be dealing in this
section with Zd-valued stochastic processes {X(t) : t > 0} with the property
that P(X(0) = 0) = 1 and
- X(t0) = j u .. .,X(tn) - X{tn.x) = jn)
m=l
for all n > 1, 0 < t0 < • • • < tn, a n d (ji, ...,jn)e 
( Z
d )
n .
4.1.1. The Simple Poisson Process: The simple Poisson process is the re-
valued stochastic process {N(t) : t > 0} which starts from 0 (i.e., N(0) = 0),
sits at 0 for a unit exponential holding time Ei (i.e., N(t) = 0 for 0 < t <
Ei and P(Ei > t) = e~t), at time E\ moves to 1 where it remains for an
independent unit exponential holding time E^, moves to 2 at time E\ + E<z,
etc. More precisely, if {En : n > 1} is a sequence of mutually independent,
unit exponential random variables and
f 0 
when n = 0
{ 2_^m=i Em 
when n > 1,
then the stochastic process {N(t) : t > 0} given by
(4.1.1) 
N(t) = max{n > 0 : Jn < i)

76 
4 MARKOV PROCESSES IN CONTINUOUS TIME
is a simple Poisson process. When thinking about the meaning of (4.1.1),
keep in mind that, because,
with probability 1, En > 0 for all n > 1 and 2_. Em = oo,
m = l
with probability 1 the path t G [0, oo) i—> N(t) is piecewise constant, right
continuous, and, when it jumps, it jumps by +1: N(t) — N(t—) G {0,1} for
all t > 0, where N(t-) = lims/>t N(s) is the left limit of N( •) at t.
We now want to show that {N(t) : t > 0} moves along in independent,
homogeneous increments.1 That is, we want to show that, for each s, t G
[0,oo), N(t) - N(s) is independent of (cf. §6.1.3) CT({N(T) : r G [0,s]}) and
has the same distribution as N(i):
(4.1.2) F(N(s + t)-N{s) 
= n\ N(T), T G [0,S]) = P(iV(t) = n), 
n € N.
Equivalently, what we have to check is that when (s,t) G [0, oo)2 and A G
<J({N(T) : r G [0,s]}), P({iV(s + i) - AT(s) > n} n A) = F(N(t) > n)F(A)
for all n G N. Since this is trivial when n — 0, we will assume that n G Z + .
In addition, since we can always write A as the disjoint union of the sets
Af]{N(s) 
= m}, m G N, and each of these is again in a({N(r) 
: r G [0, s]}),
we may and will assume that, for some m, N(s) = m on A. But in that
case we can write A = {Jm+i > s} D B where B G a{{E\,... 
,-Em}) and
4 
< s on B. Hence, since N(s + t) > m + n <S=> Jm+n < s + t, and
a({Ji — Jm : £ > m}) is independent of cr({Ek : k < m}), an application of
(6.4.2) shows that
) - N{s) > n} n A) = P({J m + n < s +1} n {J m +i > s } n B )
m+n -Jm<s 
+ t-Jm}n 
{Jm+1 
-Jm>s-Jm}nB)
= E[v{Jm),B],
where, for £ G [0,s],
v(0 = F({Jm+n -Jm<s 
+ t-Z}n {Jm+1 
~Jm>s-Z})
= F({Jn <s + t-$}n{E1>s-£})= 
P({ Jn <s + 
t-£}n{En>s-£})
= P({Jn_i +En<s + t-Z}n{En>s-£})= 
E[w(£, En), 
En>s-£]
when w(£, r/) = P(7 n-i < s +1 - ^ - ry) for ^ G [0, s] and 7? G [s - C, s + * - C]-
Up to this point we have not used any property of exponential random
variables other than that they of positive. However, in our next step we will
1 Actually, our primary goal here is to develop a line of reasoning which will serve us well
later on. A more straight-forward, but less revealing, proof that the simple Poisson process
has independent, homogeneous increments is given in Exercise 4.5.1 below.

4.1Poisson Processes 
77
use their characteristic property, namely, the fact that an exponential random
variable E "has no memory." That is, F(E > a + b\E > a) = F(E > b), from
which it is an easy step to
(4.1.3) 
E[/(£), E > a] = e-aE[f(a + E)]
for any non-negative, /?[o]00)-measurable function / . In particular, this means
that
+ s - £)]
_! <t-En) 
= e~(s-4)P(Jn < t) = e"{s'^P(N(t) > n).
Hence, we have now shown that
F({N(s + t)-N(s) >n}ni) =E[e-(s-Jm), B]P(N(t) >n).
Finally, since
F(A) = F({Jm+1 >s}nB)= F({Em+1 > s - 4 } f l B ) = Efe"^-7™), B],
the proof of (4.1.2) is complete. Hence, we now know that the simple Poisson
process {N(t) : t > 0} has homogeneous and mutually independent incre-
ments.
Before moving on, we must still find out what is the distribution of N(t).
But, the sum of n mutually independent, unit exponential random variables
has a r(n)-distribution, and so
F(N(t) 
=n)= F(jn < t < Jn+1) = F(Jn <t)- 
F(Jn+1 < t)
* 
1 /"* 
tn
- A / 
rndr=-..
n\ Jo 
n\'
That is, N(t) is a Poisson random variable with mean t. More generally, when
we combine this with (4.1.2), we get that, for all s, t G [0, oo) and n G N,
(4.1.4) 
F(N(s + t)- N(s) = n | N(T), T G [0, s}) = e"*^.
Alternatively, again starting from (4.1.2), we can now give the following
Markovian description of {N(t) : t > 0}:
j.n-N(s)
(4.1.5) 
F(N(s + t)=n\ N(T), T G [0, s]) = . ~*
_ 
l[0
4.1.2. 
Compound Poisson Processes on Zd: Having constructed the
simplest Poisson process, we can easily construct a rich class of processes which
are the continuous time analogs of random walks on Zd. Namely, suppose that

78 
4 MARKOV PROCESSES IN CONTINUOUS TIME
fi is a probability vector on Zd which gives 0 mass to the origin 0. Then the
Poisson process with jump distribution fi and rate R £ (0, oo) is the stochastic
process {X(i) : t > 0} which starts at 0, sits there for an exponential holding
time having mean value i?"1, at which time it jumps by the amount k € Zd
with probability (/i)k, sits where it lands for another, independent holding
time with mean R~1, jumps again a random amount with distribution /i.,
etc. Thus, the simple Poisson process is the case when d = 1, (/z)i = 1, and
R = 1. In particular, the amount by which the simple Poisson process jumps
is deterministic whereas the amount by which a compound Poisson process
jumps will, in general, be random.
To construct a compound Poisson process, let {Bn : n > 1} be a sequence
of mutually independent Zd-valued random variables with distribution fi, in-
troduce the random walk Xo = 0 and Xn = ^Zm=i ^m for n > 1, and define
{K(t) : t > 0} so that X(i) = XAr(i?t), where {N(t) : t > 0} is a sim-
ple Poisson process which is independent of the Bm's. The existence of all
these random variables is guaranteed (cf. the footnote in § 1.2.1) by Theorem
6.3.2. Obviously, X(0) = 0 and t € [0, oo) i—> X(i) £ Zd is a piecewise
constant, right continuous Zd-valued path. In addition, because (/i)o = 0,
it is clear2 that the number of jumps that £~-»X(£) makes during a time in-
terval (s,t\ is precisely N(Ri) — N(Rs) and that Xn — Xn_i is the amount
of the nth jump of £~+X(£). Thus, if Jo = 0 and, for n > 1, Jn is the time
of the nth jump of t~-+X(t), then N(Rt) = n <=> Jn < Rt < Jn+i, and
X(Jn) - X(Jn_!) = X n - X n_i. Equivalently, if {En : n > 1} denotes the
sequence of unit exponential random variables out of which {N(t) : t > 0}
is built, then Jn - Jn_i = ^f, X.(t) - X(i-) = 0 for t e (Jn-i,Jn), 
and
X(Jn) — X( Jn_i) = Bra. Hence, {X(i) : t > 0} is indeed a compound Poisson
process with jump distribution ft and rate R.
We next want to show that a compound process moves along in homoge-
neous, mutually independent increments:
(4.1.6) P(X(a + t ) - X ( s ) = k|X(r), re[0,s])=P(X(t)=k), 
k € Zd.
For this purpose, we use the representation X(t) = ^-N{m) introduced above.
Given A £ CT({X(T) : r £ [0,s]}), we need to show that
P({X(s +1) - X(s) = k } n A ) = P({X(s +1) - X(s) = k})P(A),
and, just as in the derivation of (4.1.2), we will, without loss in generality,
assume that, for some m £ N, N(Rs) = m on A. But then A is independent
2 This is the reason for our having assumed that (M)O = 0- However, one should realize
that this assumption causes no loss in generality. Namely, if (/n.)o = 1, then the resulting
compound process would be trivial: it would never move. On the other hand, if (/i)o S (0,1),
then we could replace fj, by p,, where (/i)o = 0 and (A)k = (1 — (/*)o)~1(A*)k when k ^ O ,
and R by R = (1 — {n)o)R. The compound Poisson process corresponding to p. and R
would have exactly the same distribution of the one corresponding to /Lt and _R.

4.1Poisson Processes 
79
of a({Km+n - Xm : n > 0} U {N(R(s + t)) - N(Rs)}), and so
P({X(s +1) - X(s) = k} n A)
oo
= J2 F({ x0 + *) - x(«) = k & N(R(s + t)) - N(Rs) =n}DA)
n=0
oo
= ^2 F({X m + n - XTO = k & JV(E(s + i)) - N(Rs) 
=n}nA)
n=0
oo
— 
7 
i r \ - * * - j i 
— 
J v l J r 
l i V 
\ A h i ) 
— 
I I I l L \ S i )
n=0
oo
= ^2 P(x« = k & N(Rt) = n)F(A) = P(X(t) = k)F(A).
n=0
Hence, (4.1.6) is proved.
Finally, to compute the distribution of X(i), begin by recalling that the dis-
tribution of the sum of n independent, identically distributed random variables
is the n-fold convolution of their distribution. Thus, P(Xn = k) = (/j*n)k,
where (/i*°)k = <5o,k *s *ne P°int mass at 0 and
for n > 1. Hence,
P(X(s + t) = k) =J2
 
F (
X n 
= k & N(Rt) = n)= 
e~m
n=0 
n=0
Putting this together with (4.1.6), we now see that for A G a[{N(r) : r
p({x(s + () = k}nil) = ^ 
P({X(s +1) = k}n An {X(s) = j})
= J2 p ( W s + *) - x( s) = k - j} n A n {X(S) = j})
jezrf
= E(p(*))jkp(^n{x(s) = j» = E[(pw)x(s)k. 4 .
jezd
where
m = 0

80 
4 MARKOV PROCESSES IN CONTINUOUS TIME
Equivalently, we have proved that {X(i) : t > 0} is a continuous time Markov
process with transition probability i~~+P(t) in the sense that
(4.1.8) 
P(X(s + *) = k | X(a), a G [0, s]) = (P(t))x(s)k-
Observe that, as a consequence of (4.1.8), we find that {P(t) : t > 0} is a
semigroup. That is, it satisfies the Chapman-Kolmogorov equation
(4.1.9) 
P(s + i) =P(s)P(i), 
s,te[0,oo).
Indeed,
(p(s + t))ok = ]T P(X( S + *) = k & x(s) = j)
from which the asserted matrix equality follows immediately when one re-
members that (P(r))M = (P(T))0(*_k)-
4.2 Markov Processes with Bounded Rates
There are two directions in which one can generalize the preceding without
destroying the Markov property. For one thing, one can make the distribution
of jumps depend on where the process is at the time it makes the jump. This
change comes down to replacing the random walk in the compound Poisson
process by more a general Markov chain. The second way to increase the
randomness is to make the rate of jumping depend on the place where the
process is waiting before it jumps. That is, instead of the holding times all
having the same mean value, the holding time at a particular state will depend
on that state.
4.2.1. Basic Construction: Let S be a countable state space and P a
transition probability matrix with the property that (P)u = 0 for all i G S.3
Further, let £R = {Ri : i G S} C [0, oo) be a family of rates. Then a continuous
time Markov process on S with rates 9^ and transition probability matrix P
is an S-valued family {X(t) : t > 0} of random variables with the properties
that
(a) t-^>X(t) is piecewise constant and right continuous,
(b) If Jo = 0 and, for n > 1, Jn is the time of the nth
(4.2.1) 
jump of t~*X(t), then
P(Jn > Jn_i + t & X(Jn) = j | X(T), 
T G [0, Jnj)
t
R
i
)
j 
on {Jn_! < oo}.
3 We make this assumption for the same reason as we assumed in §4.1.2 that (p-)o = 0, and,
just as it resulted in no loss in generality there, so it does not reduce the generality here.

4.2 Markov Processes with Bounded Rates
81
Our first task is to see that, together with the initial distribution, the pre-
ceding completely determines the distribution of {X(t) : t > 0}, and, for
reasons which will become clear soon, we will restrict our attention through-
out this section to the case when the rates D\ are bounded in the sense that
supj Ri < oo. In fact, for the moment, we will also assume that the rates
91 are non-degenerate in the sense that [R C (0, oo). Since it is clear that
non-degeneracy implies that F(Jn < oo) = 1 for each n > 0, we may (cf.
(6.1.5)) and will assume that Jn < oo for all n > 0. Now set Xn = X(Jn)
and En =
"^ "
for n > 1, and observe that, by (b) in (4.2.1),
F(En >tkXn=j\ 
{Eu .. .,£„_!} U {Xo,.. .,
= e~t(P)Xnj.
Hence, {Xn : n > 0} is a Markov chain with transition probability matrix P
and the same initial distribution as the distribution of X(0), {En : n > 1}
is a sequence of mutually independent, unit exponential random variables,
and a({Xn : n > 0}) is independent of <r({En : n > 1}). Thus, the joint
distribution of {Xn : n > 0} and {En : n > 1} is uniquely determined.
Moreover, {X(t) : t > 0} can be recovered from {Xn : n > 0}U{En : n > 1}.
Namely, given (ex,..., en,...) 
G (0, oo)z+ and (j0,..., 
jn,...) 
G SN, define
£;(ei,...,e n,...),(j 0,...,j n,...)) = j n for £n < t
(4.2.2)
Clearly,
(4.2.3)
where ^o = 0 and £n =
when n > 1.
7 7 1 = 1
X(t) =
forO<i<
Thus, we will know that the distribution of {X(t) : t > 0} is uniquely de-
termined once we check that J^m=i -^x1 .-i-^" = °° w^Ci probability 1. But
this is precisely why we made the assumption that 9i is bounded. Namely,
by either the Strong Law of Large Numbers or explicit calculation (e.g. of
E [exp (— ^^° Em)]), we know that YlT Em = oo with probability 1. Hence,
the boundedness of £H is more than enough to guarantee that ^
oo with probability 1.
To handle4 the degenerate case, the one when some of the
set So = {i : Ri = 0} and determine £H so that Ri = Ri if i £
R^1 
Em =
Ri's may be 0,
So and Ri = 1
if i € So- Our goal is to show that the distribution of {X(t) : t > 0} is the
same as that of {X(t/\( : t > 0}, where {X(t) : t > 0} is a process with rates
4 The discussion with follows is a little technical and need not be fully assimilated in order
to proceed.

82 
4 MARKOV PROCESSES IN CONTINUOUS TIME
!>K and transition probability matrix P and £ = inf{£ > 0 : X(t) € §o} is the
first time t~^X(t) hits So- That is, we are claiming that the distribution of
the process {X(t) : t > 0} is that of the process {X(t) : t > 0} stopped when
it hits §o-
To verify the preceding assertion, let {Xn : n > O & i e § o } b e a family
of S-valued random variables and {En : n > 1} a family of (0, oo)-valued
random variables with the properties that
(1) CT({XP : n > 0 & i e So}), a({En : n > 1}), and a({X(t) : t > 0})
are mutually independent,
(2) for each i 6 So, {Xn '• n > 0} is a Markov chain starting from i with
transition probability matrix P,
(3) {En : n > 1} are mutually independent, unit exponential random
variables.
Then, for each i £ S, the process {X^ (t) : t > 0} given by
starts at i and is associated with the rates SH and transition probability matrix
P. Finally, define {X(t) : t > 0} so that
[ i t < c
Obviously, X(t) = J ( t A (). Hence, we will be done once we show that
the distribution of {X(t) : t > 0} is that of a process associated with the
rates Eft and transition probability matrix P. To see this, set Jo = 0 and, for
m > 1, let Jm be the time of the mth jump of t~+X(t). Next, suppose that
A e a({X(r) : T e[0, Jn)})- We need to show that
F({Jn > Jn_1 + t&i(Jn)=J}ni)=E[e-^<v.'(%(Jii_lb, A
and because we can always write A is the disjoint union of sets of the form
{X( Jm) = j m for 0 < TO < n}, we may and will assume that A itself is of this
form. If Rjm > 0 for each 0 < m < n, then A e a{{X{a) : a e [0, Jn)}), and
(Je,X(Je)) 
= (je,X(Je)) 
for 0 < £ < n on A. Thus (*) holds in this case.
If je € So for some 0 < £ < n, use m to denote the first such £, set i = j m ,
and let {Jf : I > 0} be the jump times of t~^X^%\t). Then we can write
A = BC\C, where B = {X(Je) = j e for 0 < £ < m} and
/ {X« (Jflm) = j e for m < £ < n - 1} if 0 < TO < n - 1
\ Q 
if m = n — 1.

4.2 Markov Processes with Bounded Rates 
83
In addition, on A, (Je,X(Je)) = (J^m, X™'{jflj) for I > m. Hence,
^(jwm) = j} n c)v(B)
( i ) ( J( !) m i ) ., C]P(
and so (*) holds in this case also.
In view of the preceding, we know that the distribution of {X(t) : t > 0} is
uniquely determined by (4.2.1) plus its initial distribution, and, in fact, that
its distribution is the same as the process determined by (4.2.3). Of course, in
the degenerate case, although (4.2.3) holds, the exponential random variables
{En : n > 1} and the Markov chain {Xn : n > 0} will not be functions of the
process {X(t) : t > 0}. However, now that we have proved the uniqueness of
their distribution, there is no need to let this bother us, and so we may and
will always assume that our process is given by (4.2.3).
4.2.2. The Markov Property: We continue with the assumption that the
rates are bounded, and we now want to check that the process {X{t) : t > 0}
described above possesses the Markov property:
, . +t)=j\X(r),re[O,s]) 
= 
(P(t))XMj
where (P(t)).. = F(X(t) = j \ X(0) = i).
For this purpose, it will be useful to have checked that
"£ ' (t; ( e m + i — Rjm(s 
— £,rn),£m+2 • • • , Sm+m 
• • • )> O'mj • • • > jm+n, 
• • • ))
Now, let A s cr({X(r) : r 6 [0, s]}) be given, and assume that X(s) = j on
A. What we need to do is verify that
(*) 
F({X(s + t)= j} n A) = 
(P(t))..F(A).
To this end, set Am = A n {iV(s) = m} = {Em+i > Ri(s - Jm)} D i?m, where
{Jm <s}DBme 
<r({Eu. ..,Em}U 
{Xo,.. .,Xm}). 
Then
CK)
P({X(s + <)=i}ni) = ^ ] P({X(s + i) = j}n Am)
m=0
oo
= ^ 
F({X(s + t)=jk 
Em+1 > Rt(s - Jm)} n Bm)
m=0

84 
4 MARKOV PROCESSES IN CONTINUOUS TIME
and, by (4.2.3), (4.2.5), and (4.1.3),
F({X(s + t) = j & Em+1 > Ri(s - Jm)} n Bm)
f . m t p f + . / T p 
p /•„ 
T \ 
p 
p 
-i
^ 
Vh l,-c'm+l 
JT-i^S 
J m ; , -C/m-f.2, • • • , ^m+m 
• • • )i
= j | X(0) = i)E[e-*<a-H Bm] = (P(*)).P(^m),
from which (*) is now an immediate consequence.
Just as (4.1.8) implied the semigroup property (cf. (4.1.9)) for the transition
probability matrices there, so (4.2.4) implies the family {P(t) : t > 0} is a
semigroup. In fact, the proof is the same as it was there:
fees
In addition, it is important to realize that, at least in principle, the distribution
of a Markov process is uniquely determined by its initial distribution together
with the semigroup of transition probability matrices {P(t) : t > 0}. To
be precise, suppose that {X(i) : t > 0} is a family of S-valued random
variables for which (4.2.4) holds, and let /J, be its initial distribution (i.e., the
distribution of X(0).) 
Then, for all n > 1, 0 = to < t\ < • • • < tn, and
jo,---,jn S S,
¥(X(tm) = j m for 0 < m < n)
( 4 2 ' 6 ) 
= (M)io (P(*i " to))hjl 
• • • (P(*» - tn-i))jn_ljn 
•
To verify this, first note that, by (4.2.4),
F(X(t0) = j 0 & X(tx) = h) = {V{h))joji{»)jo 
= (/*)*,(P(*i " to))Joji-
Thus (4.2.6) holds for n = 1. Now let n > 2 be given, assume that (4.2.6)
holds for n - 1 , and set A = {X{tm) = j m : 0 < m < n - 1 } . Then, by (4.2.4),
¥(X(tm) = j m for 0 < m < n)
= ¥({X(tn) 
= jn} nA) = (P(tn - tn_i))in_iiBP(,4),
and so (4.2.6) follows from the induction hypothesis. Finally, by the ap-
plication of Theorem 6.1.6 given at the end of §6.1.4, the distribution of
{X(t) : t > 0} is completely determined by the probabilities it assigns to sets
of the form {X(tm) = j m for 0 < m < n}, and so our uniqueness assertion
has now been justified.

4.2 Markov Processes with Bounded Rates 
85
4.2.3. The Q-Matrix and Kolmogorov's Backward Equation: As we
saw at the end of the preceding section, apart from its initial distribution, the
distribution of a Markov process is completely determined by the semigroup
{P(t) : t > 0}. Thus, it is important to develop methods for calculating the
transition probabilities P(i) directly from the data contained in the rates £R
and the transition probability P.
Based on ones experience with real valued functions, one should suspect
that (4.1.9) means that P(t) must to be expressible as etCi for some Q. In
fact, Q ought to be obtainable by differentiating i~~»P(£) at t = 0.
As the first step in our program to give the substance to the preceding
speculations, we will prove that
" ' - ^ ( P P l t - r ^ . d r .
Clearly, there is nothing to do when Ri = 0, and so we now assume that
Ri > 0. Because
y 
> tRi | X(0) =i)+ ¥(E! < tRi & X{t) = j \ X(0) = i),
and (cf. (4.2.3) and (4.2.5))
W(E1 < tRi & Xit) = j I X(0) = i)
- R-1EV, 
(E2, ...,En,...), 
(Xu 
...,Xn,...))=j
Xo = i
[(P(t
= E[(P(t - RT^Ex))^., 
Ex < Rib
= Ri
we have completed the proof of (*).
The expression in (*) is an integrated version of a renowned equation due
to Kolmogorov. Namely, when one differentiates (*) with respect to t, one
arrives at Kolmogorov's backward equation :
which, when written in matrix notation, becomes
(4.2.7) 
-^P(t) = QP(i) withP(0)=I when Q = R(P - I),
where R is the diagonal matrix whose ith diagonal entry is i?j. The reason for
the adjective "backward" is that (4.2.7) describes the evolution of £~~>(P(t))..

86
4 MARKOV PROCESSES IN CONTINUOUS TIME
as a function of time t and its backward variable i, so called because, if one
adopts the perspective of someone traveling along the path t-~*X(t), then i,
being the place where he started, is the variable he sees when he is "looking
backward." Unfortunately, the terminology for the matrix Q is much less
inspired. Namely, probabilists call any matrix whose off-diagonal entries are
non-negative and whose rows sum to 0 a Q-matrix.
4.2.4. Kolmogorov's Forward Equation: Recall the norm || • || u v intro-
duced at the beginning of §3.2.1.
Starting from (4.2.7), we have
P(t) = 1+1 
QP(r) dr = I + tQ + f (t- r)Q 2P(r) dr,
Jo 
Jo
and so
(4.2.8)
Because, by the semigroup property (cf. (4.1.9)),
P(i + h)- P(i) - hQP(t) = P(t)(P(h) 
-I-hQ),
we can pass from the above to Kolmogorov's forward equation
(4.2.9)
4p(t) = p0)Q with p(°) = I.
at
so called because it describes the evolution of £~~>P(£) as a function of the for-
ward variable j : the variable which the traveler sees when he "looks forward"
in time.
4.2.5. Solving Kolmogorov's Equation: As we mentioned earlier, (4.1.9)
suggests that P(£) = etV^\ and, thanks to (4.2.7), we now know that P(0) =
Q. That is, we are guessing that P(t) = etCi, where the meaning of the
exponential is given by the power series
(4.2.10)
e M EE
m=0
f
for M e AfU|V(S).
Because ||Mm||u>v < ||M||™V, there is no question that the preceding series
converges for every M e MU>V(S). In fact, because
n - l
e M -
M71
m = 0
ml
^ 
ml
m=n

4.2 Markov Processes with Bounded Rates
ra-l
87
(4.2.11)
rvT
m=0
ml
nl
J|M||u,v
u,v
Also, it is easy (cf. Exercise 4.5.2 below) to check that
(4.2.12) 
e
Mi+M2 = e
M le M 2 
for M1; M2 G MU,V(S) which commute.
In particular, this means that £~~»etc* has the semigroup property: e
esQetQ. Finally, because
hQ - I -
u,v
we see that
e(t+h)Q _
—Qe*
and so ^e*Q =
With these preparations, it is an easy matter to complete the identification
(4.2.13)
P(t) = e*Q =
m = 0
m!
for all t G [0, oo).
Namely, by the preceding and (4.2.7), we see that, for any t > 0 and r G [0, t],
- Q)P(r) = 0,
III
and so r G [0,t] i—> e(*"T)QP(r) G MU,V(S) is constant.
Before closing this discussion, there is an important point which should be
addressed. Namely, we have proved, via (4.2.13), that, for each t > 0, etc& is a
transition probability matrix, although this fact is not at all evident from the
power series expression for etCi. In particular, without further considerations,
it is far from clear why the entries of etCi are non-negative or why ||etc* ||U)V = 1,
independent of ||Q||u,v Thus, we will now provide another way to see these
properties, one that does not require the identification of etCl as P(i). Namely,
set
Pjj = < M
 
o 
where M = sup i?i = sup(-(Q)ij).
Then P is a transition probability matrix, and Q = M(P —I). Hence, because
I commutes with P,
= 
e-tMietMp
m = 0
ml

4 MARKOV PROCESSES IN CONTINUOUS TIME
The non-negativity of the entries of eiC* is obvious from this representation as
is the fact that
-i
In order to appreciate what is going on here, it is well to notice that this line
of reasoning works only because t > 0: when t < 0, the entries of etc* need
not be non-negative and ||etc*||U)V may be as large as e'*Hlc^u'v.
4.2.6. A Markov Process from its Infinitesimal Characteristics: In
some applications the most natural way to describe a Markov process {X(t) :
t > 0} in terms of its rates EH and the underlying transition probability P is
to specify its initial distribution and say that, given its past <J({X(T) : r G
[0,i]}) up until time t > 0, the probability of its having moved away from
X{t) at time t + h (where h > 0 is small) will be approximately hRX(t), and
given O({X(T) 
: T £ [0,t]} U {X(t + h) ^ X(t)}) (i.e., both its past and that
it has moved) the probability that X{t + h) = j is approximately (P)x(t)j-
In such a description, %X and P become the infinitesimal characteristics of
the process and the question is whether the distribution of the process can
be reconstructed from this description. However, before attempting such a
reconstruction, we will make the description more quantitative by insisting
that there exist an e : (0, oo) —> (0, oo) which tends to 0 at 0 and for which
and
X(t) I X(T), T
+ h)=j\ 
X(T), 
T e [0,t], & X(t + h)^ 
X(t)) 
- (P)X(t)j\ 
< e(h).
Clearly, the first of these is equivalent to
¥(X(t 
+ h) = X(t) 
| X(T), 
T e [0, t])-l- 
h(Q)x{t)x(t) 
| < 
he(h),
whereas the two together lead to
¥(X(t 
+ h)=j\ 
X(T), 
T € [0,t]) -
< he(h)(Rx(t) 
+ e(h))
when j ^ X(t). Hence, because we are assuming that the rates are bounded,
the above description implies that
*)
F(X(t 
+ 
h)=j
), r e
5x{t)tj - h(Q)x{t)j 
< he'(h),
where, again, e'(h) —> 0 when 
h\0.

4.3 Unbounded Rates 
89
We will now show that (*) is sufficient to show that (4.2.4) is satisfied
with the P(£) = etc* and therefore, by the result discussed toward the end
of §4.2.2, that {X(t) : t > 0} is a Markov process corresponding to rates
[H and transition probability matrix P. To this end, given s > 0 and A <G
O{{X(T) 
: T G [0,s]}), determine the row vectors {n(t) : t > 0} so that
(n(t)) . = P({X(s + t) = j} n A) for each j e S. Then, because
(/*(* + h)). = E [P(X(t + h)=j\ 
X(T), T e [0, t}), A]
and (n(t)) . = E[5X(t),j, A], (*) tells us that
h)). - (n(t)). - hE[(Q)x(s+t)j, 
A]
he'(h) >
• 
j 
• 
j
/i(t)Q) • 
for t > 0 and h>0.
Hence, -^fJ-(t) = fJ-(t)Q for t > 0, and so
—/x(T)e(t~r)Q = (M(T)Q - /z(T)Q)e(t~r)Q = 0 for r G (0,t),
from which we conclude that
F({X(s + t) = j} n J4) = /i(t) = n(0)etQ = /x(0)P(i),
which is equivalent to
That is, we have now shown that (*) implies (4.2.4), and therefore, by the
final part of §4.2.2, we see that the distribution of {X(t) : t > 0} is that of a
Markov process corresponding to rates D\ and transition probability P.
4.3 Unbounded Rates
Thus far we have been assuming that the rates 5K — {i?j : i £ §} are
bounded, and the essential application which we made of this assumption
came in §4.2.1. Namely, a bound on the rates guaranteed that the Jn f oo
with probability 1 and therefore that our Markov process was completely
determined for all time. As we will see below (cf. Exercises 4.5.5 and 4.5.6),
when the rates are unbounded, JQO = Imin^^ Jn may be finite with positive
probability, in which case our description fails to tell the process what to do
during the time interval [700,00). In this section, we will give conditions,
other than a bound on £H, which guarantee that J^ = 00 with probability
1. In addition, we will give a very cursory discussion of what one can do to
salvage the situation when J^ < 00 with positive probability.

90 
4 MARKOV PROCESSES IN CONTINUOUS TIME
4.3.1. Explosion: The setting here is the same as the one in §4.2.1, only
now we are no longer assuming that the rates are bounded. Thus, if t~^>X(t)
is given by the prescription in (4.2.3), Jn denotes the time of the nth jump
(i.e., the nth t for which X(t) ^ X(t—)), and J^ = linin^oo Jn, then the
distribution of t~*X(t) is uniquely determined only until time Joo-
Our first step is to show that, with probability 1, J^ coincides with the
time when the process explodes out of the state space. To be precise, choose
an exhaustion {FN : N > 1} of § by non-empty, finite subsets. That is, for
each N, FN is a non-empty, finite subset of §, FN C FJV+I, and S = (J^ FN-
Next, take StW to be the set of rates given by
(N) _ ( Ri 
Hi eFN
i 
~{0 
if i£ FN.
Then, for each N > 1, (4.2.3) with £R(Ar) replacing UK determines a Markov
process {XW(i) : t > 0}. Moreover, if CAT = inf{t > 0 : X(")(t) ^ FN}, then
X(N+l)(t) 
= X^N\t) 
for t e [O,CAT] n [0,oo). Hence, (N < £N+1, and so the
explosion time e = limAr^oo ^JV exists (in [0, oo]).
4.3.1 THEOREM. 
e = J^ with probability 1, and, for each N > 1,
X(N\t) 
= X(t A CN) for t e [0, oo). In particular, if P(e = oo) = 1, then the
distribution of X(0) together with the description in (4.2.1) uniquely deter-
mine the distribution of a process {X(t) : t > 0}.
PROOF: Because {e ^ JQO} can be written as the union of the sets {e >
T > Joo} U {Joo > T > e} as T runs over the positive rationals, in order
to prove that e = J^ with probability 1, (6.1.5) says that it suffices for us
to show that P(e > T > J^) = 0 = P(Joo > T > e) for each T > 0.
To this end, first suppose that P(e > T > J^) > 0 for some T. 
Then
there exists an N such that P(C/v > T > J^) > 0. On the other hand,
Ov > T > Joo =>• T > Joo > r^1 J2T Em, where rN — supieFjv Rj, and
therefore we are led to the contradiction
0 < P(CJV > T > Joo) < P j 2 ^ Em 
< rNT 
= 0.
,m=l 
/
Next suppose that P( Joo > T > e) > 0. Then there exists an n > 1 such that
P( Jn > T > e) > 0. On the other hand, if /z is the distribution of X(0), then
E L o EjtFN (A*pm)i " ^ 0 as N - , oo, and so,
n > T > e) < P(j n > T > (N) < P(3 0 < TO < n Xm ^ FN)
—> 0 as N —> oo.
That is, we know that P( Jn > T > e) = 0.

4.3 Unbounded Rates 
91
Given the preceding, it should be clear that X(t) = X^N\t) 
as long as t G
[0,6v) and that X(CN) = X(N\(N) 
if CAT < oo. Hence, X(t A (N) = X<-N\t)
for all N G N and t > 0. Finally, because (4.2.1), with Dt^ replacing
1ft, together with the initial distribution uniquely determine the distribution
of (Xflff) : t > 0}, it follows that, for each N G N, the distribution of
{X{t A C/v) : i > 0} is uniquely determined by the initial distribution and
(4.2.1), and therefore, when P(e = oo) = 1, so is the distribution of {X(t) :
t > 0}. 
•
4.3.2 COROLLARY. 
If P(e = oo|X(0) = z) = 1 for allies 
and (P{t)) 
=
P(X(t) = j|X(0) = i), then, for each initial distribution fj, and T > 0,
(4.3.3) 
lim 
sup ||/zP(iV)(£)-,uP(t)llv = 0,
N
°
°
}
where {pW : t > 0} is the semigroup determined by fftW and P. Moreover,
{X(t) : t > 0} satisfies the Markov property in (4.2.4). Finally {P(t) : t > 0}
is a semigroup which satisfies Kolmogorov's backward equation in the sense
that, for each (i,j) 6 S2,
(4-3.4) 
(P(i)) 
= 5i,j + f (QP(T)) 
dr.
Jo
PROOF: First note that
P(e = oo) = £ > ) i P ( e = oo | X(0) 
=i)=l,
and therefore that limjv^oo P(Cw < 71) = 0 for each T € [0, oo). At the same
time, by Theorem 4.3.1,
| 
j & Civ < T)
) . 
(MP(t)) ,
for all 0 < t < T and j G S. Thus,
sup |/iP ( N )(t)-/xP(t)
te(o,T]
To prove the Markov property (4.2.4), let A G a({X(r) 
: r G [0,s]})
be given, and assume that X(s) = i on A. Then, since A n {CAT > s} G
cr^X^ir) 
: r G [0,s]}), the Markov property for {X^N\t) 
: t > 0} plus
the result in Theorem 4.3.1 justify
P({X(s + t)=j}nA)= 
limoV({X(N\s 
+ 
t)=j}nAn{(N>s})
= lim ( P ^ ) ( i ) ) i / ( ^ n {CAT

92 
4 MARKOV PROCESSES IN CONTINUOUS TIME
Of course, once we know that (4.2.4) holds, then, by exactly the same argu-
ment with which we derived (4.1.9) in the bounded case, we know that the
{P(t) : t > 0} here is also a semigroup.
To check (4.3.4), note that, by (4.2.7) applied to {pW(i) : t > 0},
as soon as N is large enough that i € F/v- Hence, since 1 > (pW(r))fc. —>
(P(r))fci while ]T/c l(Q)ifcl = 2Ri < °°> (4.3.4) follows by Lebesgue's Domi-
nated Convergence Theorem. 
•
4.3.2. Criteria for Non-explosion or Explosion: In this subsection we
will first develop two criteria which guarantee non-explosion: e = oo with
probability 1. We will also give a condition which guarantees explosion with
probability 1.
4.3.5 THEOREM. Let P be a transition probability matrix satisfying (P)u =
0 for all i € S, and let /J tea probability vector with the property that
(fj,)i = 0 unless % is recurrent for P. Then for every choice of rates *H, there is
no explosion of the process described in (4.2.1) with initial distribution fi.
PROOF: First observe that it is enough to handle the case when the rates
are non-degenerate. Indeed, because what we are trying to check is that
^oo = 12T ^-lirJ^rn = oo with probability 1, it is clear that making the rates
smaller can only make explosion less likely. In addition, because P(e = oo) =
]T\e§(/z)jP(e = oo|X(0) = i), it suffices for us to show that P(c = oo|X(0) =
i) = 1 whenever i is recurrent for P.
Because we are assuming that the rates are non-degenerate, (4.2.3) guar-
antees that the points visited by {X(t) : t s [0, Joo)} will be the same as the
points visited by {Xn : n > 0}. In particular, for any N > 1,
= 9N =
n > 1 Xn = i and Xm e .F/v for 0 < m < n Xo = i),
where (a^)i 
is the first time after j[ 
that t-^+X^^t) returns to i. At the
same time, essentially the same argument as was used to prove Theorem 2.3.6
shows that P((cr(Ar))l
M < CN\X{0) 
= i) = 9%, where (a^)\m) 
is defined
inductively so that ((7^)1 
= (a^N%))i and
{ oo 
if {&
( >)] — o o
and {Jn 
• n > 0} are the jump times of t-^>X^N\t). Thus, if i is recurrent
for P and therefore 9N —> 1 as N —> oo, we know that
lim P((o-(Ar))l
W < (N | X(0) = i) = 1 for each m > 1.

4.3 Unbounded Rates 
93
On the other hand, since ( C J W ) ^ > j[N) and
+l) _ , (AT)\(m) . 
j(N) 
j(N) _
< T | X(0) =i)<( 
( r ^ ) m
But, for all m > 1 and TV > 1,
< T | X(0) = i)< F((N <
) 
<T\X(0)=i),
and so, after first letting N —> oo and then m -^ oo, we see that, when i is
recurrent for P, P(e < T | X(0) = i) = 0. D
We group our second non-explosion criterion together with our criterion for
explosion as two parts of the same theorem. In this theorem, the process is
determined by (4.2.1) with rates fH and transition probability P.
4.3.6 THEOREM. 
If there exists a non-negative function u on S with the
properties that UN = inij^pN u(j) —• oo as N —> oo and, for some a £
[0,oo),
j) < ( 1 + 77- ) u(i) 
whenever i e S and Ri > 0,
then P(e = oo|X(0) = i) = 1 for all i G S. On the other hand, if, for some
i e S, Rj > 0 whenever i^j and there exists a non-negative function u on §
with the property that, for some e > 0,
k) < u(j) - — 
whenever i->j,
{k:i*k} 
i
then P(e = oo|X(0) 
=i)=Q.
PROOF: TO prove the first part, for each N > 1, set u^N\j) 
= u(j) when
j € FN and u(N\j) 
= UN when j ^ F^. It is an easy matter to check that
if QW = RW(P - I), where (R(Ar))y = i?4
(JV)8^, and u^> is the column
vector determined by (uW), = u^(i), 
then, for all i e S, (QW UW
au^N\i). 
Hence, by Kolmogorov's forward equation (4.2.9) applied to
t>0},
and so (p(Af)(T)u(]V)). < eaTw(JV)(i). But, since
) > ^ 
if Civ <

94 
4 MARKOV PROCESSES IN CONTINUOUS TIME
this means that
p(Civ < T
and so, by the Monotone Convergence Theorem, we have now proved that
P(e < T\X(0) = i)< limAr^oo P(Ov < T\X(0) = i) = 0.
In proving the second assertion, we may and will assume that i—>j for all
j e S. Now take w(jv^(j) = u(j) if j G FN and u(N\j) 
= 0 if j g FN, and use
u^) to denote the associated column vector. Clearly (QWu^)). is either
less than or equal to — e or equal to 0 according to whether i G FJV or i £ F^.
Hence, again by Kolmogorov's forward equation,
~~~ ix 
ytjvL 
) . ^ 6 y I 
V J J • '
and so
E[u^(JfW(r))|jfW(o)=»] -uc)(i)
<-eE / lFN(xW(t))dt Xm(0) =i
But, since «W > 0, this means that E [ C J V | ^ W ( ° ) = *] < ^r f o r a11 ^^ a n d
so E[e|X(0) = i] < ^f- < 00. D
4.3.3. What to Do When Explosion Occurs: Although I do not intend
to repent, I feel compelled to admit that we have ignored what, from the
mathematical standpoint, is the most interesting aspect of the theory under
consideration. Namely, so far we have said nothing about the options one has
when explosion occurs with positive probability, and in this subsection we will
discuss only the most banal of the many choices available.
If one thinks of {e < 00} as the event that the process escapes its state
space in a finite amount of time, then continuation of the process after e
might entail the introduction of at least one new state. Indeed, the situation
here is very similar to the one encountered by topologists when they want to
"compactify" a space. The simplest compactification of a separable, locally
compact space is the one point compactification, the compactification which
recognizes escape to infinity but ignores all details about the route taken. The
analog of one point compactification in the present context is, at the time
of explosion, to send the process to an absorbing point A ^ S. That is, one
defines t G [0,00) 1—> X(t) € §U{A} by the prescription in (4.2.3) (remember
that, by Theorem 4.3.1, e = JQO with probability 1) as long as t s [0, J^) and

4.4 Ergodic Properties 
95
takes X(t) — A for t € [JQCOO). For various reasons, none of which I will
explain, this extension is called that the minimal extension of the process. The
minimal extension has the virtue that it always works. On the other hand, it,
like the one point compactification, has the disadvantage that it completely
masks all the fine structure of any particular case. For example, when S = Z,
explosion can occur because, given that e < oo, limt^e X(t) = +00 with
probability 1 or because, although limtye \X(t)\ = 00 with probability 1, both
linityc X(t) = +00 and lim.tyt X(t) = —00 occur with positive probability.
In the latter case, one might want to record which of the two possibilities
occurred, and this could be done by introducing two new absorbing states,
A+ for those paths which escape via +00 and A_ for those which escape via
- 0 0 .
Alternatively, rather than thinking of the explosion time as a time to banish
the process from S, one can turn it into a time of renewal by redistributing
the process over S at time e and running it again until it explodes, etc.
Obviously, there is an infinity of possibilities. Suffice it to say that the
preceding discussion hardly scratches the surface.
4.4 Ergodic Properties
In this section we will examine the ergodic behavior of the Markov processes
which we have been discussing in this chapter. Our running assumption will
be that the process with which we are dealing is a continuous time Markov
process of the sort described in §4,2.1 under the condition that there is no
explosion.
4.4.1. Classification of States: Just as in §3.1, we begin by classifying the
states of S.
In order to make our first observation, write Q = R(P — I), where R is
the diagonal matrix of rates from £K and P is a transition probability matrix
whose diagonal entries are 0. Next, define Pm to be the transition probability
matrix given by5
Obviously, it is again true that Q = R(P K — I). In addition,
i^j relative to Pm <^=> 3 n > 0 (Q")y > 0
<y4A'2' 
<^=> (!*(*))»• > ° for a11 * > °-
Because of (4.2.13), this is obvious when 5K is bounded, and, in general, it fol-
lows from the bounded case plus (cf. the notation in §4.3.1) limjv^oo ( p W (i)) ..
5 It is worth noting that, as distinguished from P, P " is completely determined by Q.

96
4 MARKOV PROCESSES IN CONTINUOUS TIME
On the basis of (4.4.2), we will write i—>j when any one of the equivalent
conditions in (4.4.2) holds, and we will say that i Q-communicates with j and
will write i<->j when i—>j and j—>i. In this connection, S will be said to be
Q-irreducible if all states communicate with all other states.
We turn next to the question of recurrence and transience. Because
= 0
F(X(t) = i for all t > 0 | X (0) = i) = 1,
it is obvious that i should be considered recurrent when Ri = 0. On the other
hand, since in the continuous time setting there is no "first positive time," it is
less obvious what should be meant by recurrence of i when Ri > 0. However,
if one adopts the attitude that time 1 in the discrete time context represents
the first time that the chain can move, then it becomes clear that the role of
1 there should be played here by the first jump time J\, and therefore that
the role of pi is played by
(4.4.3)
i = inf{t > J-L : X(t) = i}.
Hence, we will say that i is Q-recurrent if either Ri = 0 or P(CTJ < oo|X(0) =
i) = 1, and we will say that it is Q-transient if it is not Q-recurrent.
Our next observation is that i is Q-recurrent if and only if it is recurrent
with respect to the transition probability matrix P3^ in (4.4.1). Indeed, as
is evident from the discussion in §4.2.1, the points visited by t-~+X(t) are
exactly the same as those visited by n~^Xn = X(Jn), and {Xn : n > 0} is
a Markov chain with transition probability matrix P31. As a consequence of
this observation, we see that both Theorem 3.1.2 and Corollary 3.1.4 apply
to the present setting. In particular, Q-recurrence and Q-transience are Q-
communicating class properties.
We next develop the analog for continuous time of the relations given in
(2.3.7). Namely, set af] = a, and, for m > 1, aj m + 1 ) = oo if a^m) = oo and
(TJm+1) = inf{£ > Je+1 : X(t) = j} if a^m) = Je < oo. Then, just as in the
proof of Theorem 2.3.6, one sees that
¥(a\m) < oo | X(0) 
=i)=
In addition, one can easily check that
oo | X(0) = i)m.
E
( m )
(Ji
0
i X(0)
t, a);"' < oo X(0) = i
X(0) = i I P(cr4
(m) < oo | X(0) = i)
m) < oo X(0) = i) =

4.4 Ergodic Properties
97
and
X(0) = i
Hence, by exactly the argument with which we passed from Theorem 2.3.6 to
(2.3.7), we now know that
X(0) = i
X(0) = i
1
oo|X(0) = i)
=OO|X(0) 
=j)
= oo
l{iy(X(t))dt 
=
X(0) = % I = 1
X(0) = i\<oo
l{il(X(t))dt<oo
E
E
(4.4.4)
/•CO
E| / 
l{i](X{t))dt
/ r°°
X(0) = i\ =1.
Our final goal in this subsection is to prove the following statement.
4.4.5 THEOREM. 
For any given state i e § , the following are equivalent:
(1) i is Q-recurrent.
(2) There is at € (0, oo) such that % is recurrent relative to the transition
probability matrix P(t).
(3) i is recurrent relative to P(£) for all t £ (0, oo).
PROOF: We will prove this equivalence by checking that the same statement
holds when "recurrent" is replaced throughout by "transient." To this end,
first observe that
Efy°°l{i}(A:(*))dt X(0) = i] =
and therefore, from the first line of (4.4.4), that
/•OO
(4.4.6) 
i is Q-transient <(=> / 
(P(t)) dt < oo.
Jo
Next, notice that for 0 < s < t,
(4.4.7) 
(P(£)).. > (P(t-s))..

98 
4 MARKOV PROCESSES IN CONTINUOUS TIME
since (P(h))i. > P(Ji > h\X(0) = i)= e~hRi. Hence, for any t > 0 and
n£N,
(P(t)n+1)u 
= (P((n + 1)*)) .. > e~tRi (P(r)).. for all r e [nt, (n + l)t]
and
e-tRi (p(t)n).. = e-«fc (P(nt)) .. < (P(r))^ for all r € [nt, (n + l)t].
Since this means that
oo 
-oo 
oo
ie"** $>(«)")„ < / (P(r))«dr < te*«* E(P(*)B+1)«'
n=0 
- 7 0 
n=0
the asserted implications are now immediate from (4.4.6). •
4.4.2. Stationary Measures and Limit Theorems: In this subsection,
we will complete our program by proving the following basic result.
4.4.8 THEOREM. 
For each j e S
itii = lim (P(t)) .. exists
and
lim (P(i)).. = itij = F(<Tj < ool X(0) = i ) ^ - for % ± j .
Moreover, ifitjj > 0, then nu > 0 for all i G C = {i : i<->j}, and when the
jj
row vector -kG is determined by (7tc)i = ^-c{i)^u, then, for each s > 0, 7C
C is
the unique probability vector /J, 6 Stat(P(s)) for which (fi)k = 0 when k <£ C.
In fact, if n 6 Stat(P(s)) for some s > 0, then, for each j £ C,
PROOF: We begin with the following continuous-time version of the renewal
equation (cf. (3.2.6)):
(4.4.9) 
( P ( t ) ) . . = e -
t R * 5 i t j + E [ ( P ( t - a ^ ) . . , a , < t \ X { 0 ) = i ] .
The proof of (4.4.9) runs as follows. First, write (P(i)) as
F(X(t) 
=jkJ1>t\ 
X(0) =i)+ F(X(t) 
= j & J i < 11 X(0) = i).

4.4 Ergodic Properties 
99
Clearly, the first term on the right is 0 unless i — j , in which case it is equal
e~tRi. To handle the second term, write it as
= j' & <Tj = Jm < t X(0) = ij,
m=l
and observe that (cf. (4.2.3) and (4.2.5))
F(X(t) =j&<Jj = Jm<t\ 
X(0) = i)
= Pf $ ' (t — Jm; (Em+i,..., 
Em+n,...), 
(j,Xm+i,... 
,Xm+n,.
& <7j = Jrn < t
[(P(t - Jm)).., 
aj = Jm<t\ X(0) =
Hence, after summing this over m > 1 and combining this result with the
preceding, one arrives at (4.4.9).
Knowing (4.4.9), we see that the first part of the theorem will be proved once
we treat the case when i = j . To this end, we begin by observing that, because,
by (4.4.7), (P(s)).. > e~sRi > 0 for all s > 0 and i e § , each i is aperiodic
relative to P(s), and so, by (3.2.15), we know that ir(s)u = limra^oo(P(s)rl)ii
exists for all s > 0 and i e S. We want to show that TT(1)JJ = limt_>oo(P(t))ii,
and when TT(1)JJ = 0, this is easy. Indeed, by (4.4.7),
Em (P(t))« < eR' Em (P([t] + 1)).. = efl%(l)«,
where [t] denotes the integer part of t.
In view of the preceding, what remains to be proved in the first assertion
is that limt_>oo(P(£)).. = n(l)u when TT(1)U > 0, and the key step in our
proof will be the demonstration that ir(s)u = TT(1)JJ for all s > 0, a fact which
we already know when TT(1)M = 0. Thus, assume that TT(1)JJ > 0, and let
C be the Q-communicating class of i relative to P(l). By (4.4.2), C is also
the communicating class of i relative to P(s) for every s > 0. In addition,
because TT(1)JJ > 0, i is recurrent, in fact positive recurrent, relative to P(l),
and therefore, by Theorem 4.4.5, it is also recurrent relative to P(s) for all
s > 0. Now determine the row vector TT(1) 
SO that (TT(1) )J = lc(j)7r(l)jj.
Then, because n(l)u > 0, we know, by Theorem 3.2.10, that TT(1) is the one
and only /i. £ Stat(P(l)) which vanishes off of C. Next, given s > 0, consider
[i = TT(1)C'P(S). Then \i is a probability vector and, because (P(l)) k = 0
when j e C and k <£ C, fj, vanishes off of C. Also, because
= 7r(l)CP(S)P(l) = 7r(l)CP(s + 1)
= TT(1) CP(1)P( S) = TT(1)CP(S) = M )

100 
4 MARKOV PROCESSES IN CONTINUOUS TIME
H is stationary for P(l). Hence, by uniqueness, we conclude that TT(1) P(S)
= 7r(l) , and therefore that TT(1)C is a stationary measure for P(s) which
vanishes off of C. But, by (3.2.8) and the fact that C is the communicating
class of i relative to P(s), this means that
To complete the proof of the first part from here, note that, by (4.4.7),
e-sRj (p(ns)) .. < (P(t)) • • < e 5^ (P((n + l)s)) .. when ns < t < {n + l)s,
and so, since TT(S)JJ = TT(1)JJ and P(ni) = P(t)n,
Now let s \ 0, and simply define TTJJ = TT(1)JJ.
Given the first part, the proof of the second part is easy. Namely, if itjj > 0
and C = {i : i<->j}, then, for each s > 0, we know that C is the communicating
class of j relative to P(s) and that itc = TT(S)C G Stat(P(s)). Conversely, by
(3.2.8), we know that if /x 6 Stat(P(s)) for some s > 0, then
With the preceding result in mind, we will say that i is Q-positive recurrent
if TTjj > 0 and will say that i is Q-null recurrent if i is Q-recurrent but not
Q-positive recurrent. From the preceding, we already know that Q-positive
recurrence is a Q-communicating class property.
The following corollary comes at very little additional cost.
4.4.10 COROLLARY. Mean Ergodic Theorem Assume that j is Q-positive
recurrent and that P(X(0)S.j) = 1. Then,
lim E
T-»oo
See Exercise 4.5.10 for the more refined version.
= 0.

4.4 Ergodic Properties
101
PROOF: The proof is really just an obvious transcription to the continuous
setting of the argument used to prove Theorem 3.2.14. Namely, set C = {i :
j<-n}. By precisely the argument given there, it suffices to handle the case
when TTC is the initial distribution. Thus, if / = l^y — TXJJ, what we need to
show is that
lim E
T—>oo
= 0,
hen TTG is the distribution of X(0). But, because TT17 is P(£)-stationary,
E[f(X(s))f{X(t))] ds\ dt
where a(t) = 7TC(/P(t)f), f being the column vector corresponding to the
function / and /P(t)f being the column vector determined by (/P(i)f)j =
/(i)(P(i)f)j. Finally, since, for each i e C , limt_>oo(P(i)f)j = 0, lim^oo &(t)
= 0, and therefore
r
T 
2 rT
; / 
(l - ±) a(t) dt < — 
\a(t)\dt—>0 
as T -> oo. D
Jo 
^ Jo
4.4.3. 
Interpreting ir^: Although we have shown that the limit TT^ =
limt^oo(P(i)).. exists, we have yet to give an expression, analogous to the
one in (3.2.5), for TTJJ. However, as we are about to see, such an expression is
readily available from (4.4.9). Namely, for each a > 0, set
2
T
L(a)ij = oE
— at!{3}{X{t))dt
=i] =aJ\-at{P(t))..dt.
Because Tin = limt^Oo(I>(^)) ••> the second of these representations makes it
is easy to identify nu as l i m ^ o L(a)u. 
At the same time, from (4.4.9), we
see that
L(a)u = ——5- + E[e~a<Ti | X(0) -
Hence, we have now shown that
( 1 
if Ri = 0
^7T 
ifi*i>0,
which, in conjunction with the second equality in Theorem 4.4.8, proves that
r Sij + ¥{aj < oo|X(0) = i) if Rj = 0
()
Of course, as an immediate corollary of (4.4.11), we now know that i is positive
recurrent if and only if either Ri = 0 or E[CTJ|X(0) = i] < oo.

102 
4 MARKOV PROCESSES IN CONTINUOUS TIME
4.5 Exercises
EXERCISE 4.5.1. The purpose of this exercise is to give another derivation of
(4.1.5). Thus, let {En : n > 1} be a sequence of mutually independent, unit
exponential random variables, and define {Jn : n > 0} and {N(t) : t > 0}
accordingly, as in §4.1.1. Given 0 = to < • • • < tg and 0 < n\ < • • • < ri£, use
the change of variables formula for multi-dimensional integrals to justify:
P(N(t1)=n1,...,N(te) 
= ne)
( 
< H < Jm + 1, • • • , Jnt < te < 
)
f 
f 
(
 
n<r? \
i=i 
0=1
where
??« < Vi+i for 1 < i < n^ & ^ 
< tj < r?nj+1 for 1 < j < £},
with the obvious modifications when rij = 0 for 1 < j' < i.
EXERCISE 4.5.2. Let Mi and M2 be commuting elements of MU)V(§). After
checking that
m
(Mi + M2)m = J2
for all m e N , verify (4.2.12).
EXERCISE 4.5.3. Given a Q-recurrent state i, set C ~ {j : i<->j}, and show
that Ri > 0 =^> i?j > 0 for all j e C.
EXERCISE 4.5.4. In this exercise we will give the continuous-time version of
the ideas in Exercise 2.4.1. For this purpose, assume that S is irreducible
and positive recurrent with respect to Q, and use it to denote the unique

4.5 Exercises 
103
probability vector which is stationary for each P(t), t > 0. Next, determine
the adjoint semigroup {P(t)T : t > 0} so that
%3 
(7r)i
(a) Show that P(t)T is a transition probability matrix and that TTP(£)T = it
for each t > 0. In addition, check that {P(£)T : t > 0} is the semigroup
determined by the Q-matrix Q T, where
(Q~% =
(b) Let P and P T denote the probabilities computed for the Markov pro-
cesses corresponding, respectively, to Q and Q T with initial distribution
•it. Show that P T is the reverse of P in the sense that, for each n <S N,
0 = t0 < h < • • • < tn, and (j 0, ...,jn)e 
S n + 1,
P T (X{tm) = j m for 0 < m < n) = P(X(tn - tm) = j m for 0 < m < n).
(c) Define P T so that
Show that P T is again a transition probability matrix, and check that Q T =
R ( P T - I ) .
EXERCISE 4.5.5. Take S = N and (P) y equal to 1 or 0 according to whether
j — i + 1 or not. Given a set of strictly positive rates D\, show that, no
matter what its initial distribution, the Markov process determined by £R and
P explodes with probability 1 if X^GN-^T1 < °° an<^ does not explode if
EXERCISE 4.5.6. Here is a more interesting example of explosion. Take S =
Z3, and let (cf. Exercise 3.3.5) P be the transition probability matrix for
the symmetric, nearest neighbor random walk on 1?. Given a set of positive
rates 9t with the property that X^kez3 -^k * < °°' show that, starting at every
k e Z3, explosion occurs with probability 1 when (Q)k^ = i?k((P)w — <^k/)-
Hint: Apply the criterion in the second half of Theorem 4.3.6 to a function
of the form
and use the computation in Exercise 3.3.5.

104
4 MARKOV PROCESSES IN CONTINUOUS TIME
EXERCISE 4.5.7. Even when § is finite, writing down a reasonably explicit
expression for the solution to (4.2.7) is seldom easy and often impossible.
Nonetheless, a little linear algebra often does quite a lot of good. Throughout
this exercise, Q is a Q-matrix on the state space S, and it is assumed that the
associated Markov process exists (i.e., does not explode) starting from any
point.
(a) If u G Cs is a bounded, non-zero, right eigenvector for Q with eigenvalue
« £ C , show that the real part of a must be less than or equal to 0.
(b) Assume that N = #§ < oo and that Q admits a complete set of linearly
independent, right eigenvectors ui,..., u^ e Cs with associated eigenvalues
ai,..., aN. Let U be the matrix whose mth column is um, and show that
etQ _ UA(t)U~1, where A(t) is the diagonal matrix whose m diagonal entry
is e,tan
EXERCISE 4.5.8. Here is the continuous analog of the result in Exercise 3.3.7.
Namely, assume that i is Q-recurrent and i?, > 0, set
X(0)=i\ 
e [0,oo] for j
= E
and let /i be the row vector given by (p,)j = (ij for each j e S.
(a) Show that /Lj = -^-, p,j < oo for all j € §, and that p,j > 0 if and only
•t Q •
if 
i^j.
(b) Show that p, = fiP(t) for all t > 0.
Hint: Check that
a\{j}(x(t))dt
X(0) = i
and that
E
dt X(0) = i\ = E
J
dt
X(0)=i\.
• Q .,
(c) In particular, if i is Q-positive recurrent and C — {j : i<->j}, show that
ft =
• Equivalently,
X(0) = i
E[a x(o) = i]
EXERCISE 4.5.9. Having given the continuous-time analog of Exercise 3.3.7,
we now want to give the continuous-time analog of Exercise 3.3.8. For this
purpose, again let i be a Q-recurrent element of § with i?j > 0, and define the

4.5 Exercises
105
measure /x accordingly, as in Exercise 4.5.8. Next, assume that i> G [0, oo)
satisfies the conditions that: (y)i > 0, (v)j = 0 unless P(<7j < oo | X(0)
i) = 1, and i>Q = 0 in the sense that Rj(i>)j = Z^j^HQfc; f°r a u 3 G ^-
The goal is to show that v = Ri{i>)i\i. Equivalently,
X(0)=i\ 
foralljeS.
In particular, by Exercise 4.5.8, this will mean that v = uP(t) for all t > 0.
In the following, the transition probability P is chosen so that its diagonal
entries vanish and Q = R(P — I), where, as usual, R is the diagonal matrix
whose jth diagonal entry is Rj for each j s S.
(a) Begin by showing that % is P-recurrent.
(b) Define the row vector v so that (u)j = R ul3. , and show that v = i/P.
(c) By combining (b) with Exercise 3.3.7, show that
(y)i =
m=0
X0=i
where {Xn : n > 0} is the Markov chain with transition probability matrix
P.
(d) Show that
X(0) =i 
= E Ei
.m=0
and, after combining this with (c), arrive at the desired conclusion.
EXERCISE 4.5.10. Following the strategy used in Exercise 3.3.9, show that,
under the hypotheses in Corollary 4.4.10, one has the following continuous
version of the individual ergodic theorem:
r l i m j | L T - 7 r c | | v = 0 ) = l ,
where C = {i : j<-n} and Ly is the empirical measure determined by
i f
1 Jo
In addition, following the strategy in Exercise 3.3.11, show that, for any
initial distribution,
P ( lirn^ - ^ 
l{j} (X(t)) dt = 0 j = 1
when j is not Q-positive recurrent.
6 The reason why the condition z>Q = 0 needs amplification is that, because Q has negative
diagonal entries, possible infinities could cause ambiguities.

106 
4 MARKOV PROCESSES IN CONTINUOUS TIME
EXERCISE 4.5.11. Given a Markov process {X(t) : t > 0} with Q-matrix Q,
one can produce a Markov process with Q-matrix MQ by speeding up the
clock of t~>X(t) by a factor of M. That is, {X(Mt) 
: t > 0} is a Markov
process with Q-matrix MQ. The purpose of this exercise is to show how to
carry out the analogous procedure, known in the literature as random time
change, for variable rates. To be precise, let P be a transition probability
matrix on S with (P)« = 0 for all i. Choose and fix an i G S, let {Xn : n > 0}
be a Markov chain starting from i with transition probability matrix P and
{N(t) : t > 0} a simple Poisson process which is independent of {Xn : n > 0},
and set X°(t) - X^(t) for t > 0. In particular, {X°(t) : t > 0} is a Markov
process with Q-matrix (P — I) starting from i. Finally, let D\ be a set of
positive rates, and take Q = R(P — I) accordingly.
(a) Define
r 
1
A{t) = / — 
dr 
for t e [0, oo),
observe that t~^>A{t) is strictly increasing, set A{oo) = limtyoo A(t) s (0, oo],
and use s e [0,^4(oo)) i—> A"1(s) e [0, oo) to denote the inverse of
Show that
fs
= 
Rxo{A-Ha))da, 
se[0,A(oo)).
Jo
(b) Set X{s) = X°(A-1(s)) 
for s e [0, A(oo)). Define J$ = 0 = Jo and, for
n > 1, J° and Jn to be the times of the nth jump of t~^>X°(t) and s~-+X(s),
respectively. After noting that Jn = A(J°), conclude that, for each n > 1,
s > 0, and j G S,
P(j n - Jn_! > s & X( Jn) = j | X(a), a e [0, Jn))
^
i
)
. 
on {Jn_! < oo}.
(c) Show that the explosion time for the Markov process starting from i with
Q-matrix Q has the same distribution as A(oo). In particular, if A(oo) = oo
with probability 1, use (b) to conclude that {X(s) : s > 0} is a Markov
process starting from i with Q-matrix Q.
(d) As a consequence of these considerations, show that if the Markov
process corresponding to Q does not explode, then neither does the one cor-
responding to Q', where Q' is related to Q by (Q')ij = cti{Q)ij and the
{ai : i £ §} is a bounded subset of (0, oo).

CHAPTER 5
Reversible Markov Processes
This is devoted to the study of a class of Markov processes which admit
an initial distribution with respect to which they are reversible in the sense
that, on every time interval, the distribution of the process is the same when
it is run backwards as when it is run forwards. That is, for any n > 1 and
(io,..., in) G En+1, in the discrete time setting,
(5.0.1) 
P(XTO = im for 0 < m < n) = P(Xn_m = im for 0 < m < n)
and in the continuous time setting,
(5.0.2) ¥(X(tm) = im for 0 < m < n) = W(X(tn - tm) = im for 0 < m < n)
whenever 0 = to < • • • < tn. Notice that the initial distribution of such a
process is necessarily stationary. Indeed, depending on whether the setting is
that of discrete or continuous time, we have
P(X0 = i & Xn = j) = F(Xn 
=ikX0=j)
or P(X(0) = i & X(t) = j) = F(X(t) = i & X(0) = j),
from which stationarity follows after one sums over j . In fact, what the
preceding argument reveals is that reversibility says that the joint distribution
of, depending on the setting, (Xo,Xn) or (X(0),X(£)) is the same as that
of (Xn,Xo) or (X(£),X(0)). This should be contrasted with the stationarity
which gives equality only for the marginal distribution of the first components
of these.
In view of the preceding, one should suspect that reversible Markov pro-
cesses have ergodic properties which are better than those of general stationary
processes, and in this chapter we will examine some of these special properties.
5.1 Reversible Markov Chains
In this section we will discuss irreducible, reversible Markov chains. Because
the initial distribution of such a chain is stationary, we know (cf. Theorem
3.2.10) that the chain must be positive recurrent and that the initial distribu-
tion must be the probability vector (cf. (3.2.9)) TT = TTS whose ith component

108 
5 REVERSIBLE MARKOV PROCESSES
is (TT)J = E[/?j|Xo = J]"1- Thus, if P is the transition probability matrix,
then, by taking n = 1 in (5.0.1), we see that
WiWij 
= P(X0 = ibX1=j)= 
P(X0 = jkXl=i) 
= (ir)j{p)ji.
That is, P satisfies1
(5.1.1) 
(7r)i(P)ij = (7T)J(P)JJ, 
the condition of detailed balance.
Conversely, (5.1.1) implies reversibility. To see this, one works by induction
on n > 1 to check that
7Tio(P)ioh 
• • • ( P ) i n _ l i n = TtiJP)^^ 
• • • (P) i l < 0,
which is equivalent to (5.0.1).
5.1.1. Reversibility from Invariance: As we have already seen, reversibil-
ity implies invariance, and it should be clear that the converse is false. On
the other hand, there are two canonical ways in which one can pass from an
irreducible transition probability P with stationary distribution TT to a tran-
sition probability for which TT is reversible. Namely, define the adjoint P T of
P so that
/cio-) 
CPTV _ ( 7 r)j( P)i»
(5.1.2) 
(P ) X J - 
{7v) 
.
Obviously, TT is reversible for P if and only if P = P T . More generally,
because TTP = TT, P T is again a transition probability. In addition, one can
easily verify that both
(5.1.3) 
P + P 
and 
P T P
are transition probabilities which are reversible with respect to TT. AS is
explained in Exercise 5.6.9 below, each of these constructions has its own
virtue.
5.1.2. Measurements in Quadratic Mean: For reasons which will be-
come increasingly clear, it turns out that we will here want to measure the
size of functions using a Euclidean norm rather than the uniform norm ||/||u.
Namely, we will use the norm
(5.1.4) 
||/||2>w = VWWU 
where {g)n
1 The reader who did Exercise 2.4.1 should recognize that the condition below is precisely
the same as the statement that P = P T . In particular, if one knows the conclusion of that
exercise, then one has no need for the discussion which follows.

5.1 Reversible Markov Chains 
109
is the expected value of g with respect to vr. Because (TT)* > 0 for each J e S ,
it is clear that ||/||2,7r = 0 -<=>• / = 0. In addition, if we define the inner
product (f,g)n to be (fg)n, then, for any t > 0,
0 < ||«/ ± t-Vlll,. = *
2H/llilW ± 2(7,5). + t^WgWl^,
and so \{f,g)n\ < ^ll/lli.Tr+^IMIi.Tr for a11 * > °- T o Set t h e b e s t estimate,
we minimize the right hand side with respect to t > 0. When either / or g
is identically 0, then we see that {f,g)n = 0 by letting t —> oo or t —->• 0. If
neither / nor g vanishes identically, we can do best by taking t = i .....
Hence, in any case, we arrive at Schwarz's inequality
(5-1.5) 
\(f,gU < H/lk.lMk..
Given Schwarz's inequality, we know
11/ + 3llL = II/IIL + 2(/)5>ir + ||5||2
2j7r <
That is, we have the triangle inequality:
Thus, if L2{-K) denotes the space of / for which ||/||2,7r < °°, then L2(n) is a
linear space for which (/,g)~*\\f — g\\2,n is a metric. In fact, this metric space
is complete since, if linv^oo supn>TO \\fn - fmh,-* = 0, then {fn(i) • n > 0}
is Cauchy convergent in M for each t 6 § , and therefore there exists a limit
function / such that fn(i) —> f(i) for each t £ S. Moreover, by Fatou's
Lemma,
11/ - fmh,-K < lim \\fn - /m||2,7r 
> 0 as m -> 00.
n^oo
In this chapter, we will use the notation
where f is the column vector determined by /. 
When / is bounded, it
is clear that P/(z) is well-defined and that ||P/||U < II/I|UJ where ||<;||u =
supi6§ \g(i)\ = ||g||u when g is the column vector determined by g. We next
want show that, even when / e L2(TT), P/(i) is well-defined for each j e §
and that P is a contraction in L2(TT): ||P/||2,7r < \\fh,n- To check that P/(i)
is well-defined, we will show that the the series J2j€§f(j)(P)ij 
is absolutely
convergent. But, by the form of Schwarz's inequality in Exercise 1.3.1,
jes

110 
5 REVERSIBLE MARKOV PROCESSES
and, by (5.1.1),
<cx>
with ^2j(P)ij = 1 to see that (P/(i)) < P/2(i) for each i. Thus, since n is
As for the estimate ||P/|[2.7r < ||/||2,7r> we use Exercise 5.6.2 below together
with ^2j(P)ij
P-stationary,
(5.1-7)
An important consequence of (5.1.7) is the fact that, in general (cf. (2.2.4)),
(5.1.8) 
lim ||An/ -{/)„||2,w 
= 0 for all / € L2(TT)
n—>oo
and
(5.1.9) P is aperiodic => lim ||P"/ - (f)^ 
n = 0 for all / G L2(TT).
n—^oo 
'
To see these, first observe that, by (3.2.11), (3.2.15), and Lebesgue's Domi-
nated Convergence Theorem, there is nothing to do when / vanishes off of a
finite set. Thus, if {F/v : N > 1} is an exhaustion of S by finite sets and if,
for / G L2{TT), fN = lFjv/, then, for each N G Z+,
||An/ - </)w||2lW < ||An(/ - fN)\\2<ir 
+ \\AnfN - (M^h,* 
+ (|/iV " f\)n
" fN\\2,n + HAn/jv " </iv)W||2,W,
where, in the passage to the second line, we have used ||An<7||2;7r < ||3||2l7r,
which follows immediately from (5.1.7). Thus, for each N, limn^oo \\Anf -
(/)ir||2,ir < 2||/ — /jv||2.ir> which gives (5.1.8) when N —> oo. The argument
for (5.1.9) is essentially the same, and, as is shown in Exercise 5.6.6 below, all
these results hold even in the non-reversible setting.
Finally, (5.1.1) leads to
In other words, P is symmetric on L2(TT) in the sense that
(5.1.10) 
(fl,P/)w = (Pff,/)w for (/,<?) e(L2(7r))2
5.1.3. The Spectral Gap: The equation (5.1.7) combined with (5.1.10)
say that P a self-adjoint contraction on the Hilbert space2 is L2(n). For the
2 A Hilbert space is a vector space equipped with an inner product which determines a
norm for which the associated metric is complete.

5.1 Reversible Markov Chains 
111
reader who is unfamiliar with these concepts at this level of abstraction, think
about the case when § = {1,..., N}. Then the space of functions / : § —> M.
can be identified with RN. (Indeed, we already made this identification when
we gave, in §2.1.3, the relation between functions and column vectors.) After
making this identification, the inner product on L2(TT) becomes the inner
product Yli (7r)i(v)«(w)i f°r column vectors v and w in M.N. Hence (5.1.7)
says that the matrix P acts as a symmetric, contraction on RN with this
inner product. Alternatively, if P = Il2Pn~2 ; where II is the diagonal
matrix whose ith diagonal entry is (TT),, then, by (5.1.1), P is symmetric
with respect to the standard inner product (V,W)KN = J2i (v)j(w)i o n KN.
Moreover, because, by (5.1.7),
HPfllJ^ = (pf,Pf)B~ = 
i ;
where g is the function determined by the column vector TI~2{, we see
that, as an operator on H&N, P is length contracting. Now, by the stan-
dard theory of symmetric matrices on RN, we know that P admits eigen-
values 1 > Ai > • • • AJV > —1 with associated eigenvectors (ei,...,ejv)
which are orthonormal for (•, -)R«: ( e ^ e ^ ) ^ = 5k,e- Moreover, because
\/(iv)i = Yl7=i(P)ii\/(7r)i' 
w e know that Ai = 1 and can take (ei)j = A/(TT)J.
Finally, by setting g£ = (II)~2e^ and letting gg be the associated function
on S, we see that Pgt = Xege, gi = 1, and {gi^ge)™ — Sk,e- To summarize,
when S has N elements, we have shown that P on L2(TV) has eigenvalues
1 = Ai > • • • > A AT > — 1 with corresponding eigenfunctions gi,. • •, <7JV which
are orthonormal with respect to (•, • )„•. Of course, since L2{-n) has dimension
N and, by ortho-normality, the g^s are linearly independent, (gi, • • •,ffjv) is
an orthonormal basis in L2(TT). In particular,
(5.1
and
.11)
so
IIP
pn
" / "
/ "
..in,
1=2
N
1=2
W
\ 2 n
for
< ( 1
all n > 0 ai
! n||/~
where /? = (1 — A2) A (1 + AJV) is the spectral gap between {—1,1} and
{A2, •.., AJV}. In other words,
(5.1.12)
for all n > 0 and / e 
L2(-K).
When § is not finite, it will not be true in general that one can find an
orthonormal basis of eigenfunctions for P. Instead, the closest approximation

112 
5 REVERSIBLE MARKOV PROCESSES
to the preceding development requires a famous result, known as the Spectral
Theorem (cf. §107 in [7]), about bounded, symmetric operators on a Hilbert
space. Nonetheless, seeing as it is the estimate (5.1.12) in which we are most
interested, we can get away without having to invoke the Spectral Theorem.
To be more precise, observe that (5.1.12) holds when
(5.1.13) 
13 = 1- sup{||P/ - (f)n\\2tV • f e L2(TT) with ||/||2,w = l}.
To see this, first note that, when (3 is given by (5.1.13),
Vll/lk,
II2.7T
2,7T
„. 
whenfeL2(ir)\{0},
and that ||P/ - (fj-nh-n < Wfhn trivially when / = 0. Next, because
<PnJ% = (f)n for all n> 0,
-\ II 
_ \\T>(~P
nf 
— CPnf\ "ill
'
n \ \ 2 , n 
II V x J 
\ x 
J I"*) Il2,7r
p n/ - (Pn/)
Thus, by induction on n, ||Pn/-(/)7r||2,7r < (1 - t3)n\\f\\2 •* for all n > 0 and
/ e L2(ir). Hence, if / e L2(TV) and / = / - </)„, then ||P«/ - (/)w||2>w =
||Pn/||2,7r < (1 - PTWfhn = (1 " P)n\\f ~ {f)*h,n- That is, (5.1.12) holds
with the (3 in (5.1.13). Observe that when S is finite the (3 in (5.1.13) coincides
with the one in the preceding paragraph. Hence, we have made a first step
toward generalizing the contents of that paragraph to situations when (5.1.11)
does not apply.
5.1.4. Reversibility and Periodicity: Clearly, the constant (3 in (5.1.13)
can be as small as 0, in which case (5.1.12) tells us nothing. There are
three ways in which this might happen. One way is that there exist an
/ e L2(n) with the property that, ||/||2,w = 1, </>„• = 0, and P / = /.
However, irreducibility rules out the existence of such an /. Indeed, be-
cause of irreducibility, we would have (cf. (5.1.8)) the contradiction that
0 = (f)^ = limn_+0O(Anf)j = f(i) for all i € § and that f(i) ^ 0 for some
j e §. Thus, we can ignore this possibility because it never occurs. A sec-
ond possibility is that there exists an / 6 L2(TT) with ||/||2,ir = 1 such that
P / = - / . In fact, if / is such a function, then (f)n = (P/)^ = — (/)w, and
so (/)„- = 0. Hence, we would have that ||P/ — (/^Ikx = 1 and therefore
that {3 = 0. The third possibility is that there is no non-zero solution to
P / = —/ but that, nonetheless, there exists a sequence {/ra}i° Q L2(-n) with
||/n||2,ir = 1 and (fn)n = 0 such that ||P/ra||2)7r tends to 1.
Because the analysis of this last possibility requires the Spectral Theorem,
we will not deal with it. However, as the next theorem shows, the second
possibility has a pleasing and simple probabilistic interpretation. See Exercise
5.6.7 below for an extension of these considerations to non-reversible P's.

5.1 Reversible Markov Chains 
113
5.1.14 THEOREM. 
If P is an irreducible transition probability for which
there is a reversible initial distribution, which is necessarily vr, then the period
of P is either 1 or 2. Moreover, the period is 2 if and only if there exists an
f e L2(ir) \ {0} for which f = - P / .
PROOF: We begin by showing that the period d must be less than or equal to
2. To this end, remember that, because of irreducibility, (TT), > 0 for all i's.
Hence, the detailed balance condition, (5.1.1), implies that (P)»j > 0 4=>
(P)ji > 0. In particular, since, for each i, (P)»j > 0 for some j and therefore
(P2)ji = J2j(P)ij(P)ji > 0> w e s e e that the period must divide 2.
To complete the proof at this point, first suppose that d= 1. If / £ L2(TV)
satisfies / = —P/, then, as noted before, {/}•* = 0, and yet, because of
aperiodicity and (5.1.9), limn^oo Pn/(i) = (/)TT = 0 for each i e §. Since
/ = p2nj for au n > 0, this means that / = 0. Conversely, if d = 2, take
§o and §i accordingly, as in §3.2.7, and consider / = l§0 — 1§J. Because of
(3.2.19), P / = - / , and clearly ||/||2>w = 1. D
As an immediate corollary of the preceding, we can give the following graph
theoretic picture of aperiodicity for irreducible, reversible Markov chains.
Namely, if we use P to define a graph structure in which the elements of S are
the "vertices" and an "edge" between i and j exists if and only if (P)jj > 0,
then the first part of Theorem 5.1.14, in combination with the considerations
in §3.2.7, says that the resulting graph is bipartite (i.e., splits into two parts
in such a way that all edges run from one part to the other) if and only if the
chain fails to be aperiodic, and the second part says that this is possible if
and only if there exists an / e L2(TT) \ {0} satisfying P / = —/.
5.1.5. Relation to Convergence in Variation: Before discussing meth-
ods for finding or estimating the (3 in (5.1.13), it might be helpful to compare
the sort of convergence result contained in (5.1.12) to the sort of results we
have been getting heretofore. To this end, first observe that
\\Pnf - </>w||2 „ < ||P n/ - </>w||u < sup H^P" - 7r||v||/||u.
' 
i
In particular, if one knows, as one does when Theorem 2.2.1 applies, that
for some C < oo and e G (0,1], then one has that
which looks a lot like (5.1.12). Indeed, the only difference is that on the right
hand side of (5.1.12), C — 1 and the norm is ||/||2,ir instead of ||/||u. Thus, one
should suspect that (*) implies that the /3 in (5.1.12) is at least as large as the
e in (*). In, as will always be the case when § is finite, there exists & g E L2(TV)

114
5 REVERSIBLE MARKOV PROCESSES
with the properties that ]|g||2,7r = 1, {g)it = 0, and either Pg = (1 — f3)g or
~Pg = —(1 — /3)g, this suspicion is easy to verify. Namely, set / = gl[-R,R](g)
where R > 0 is chosen so that a = {f,g)n > \, and set f = f — (/)«•• Then,
after writing / = ag + (/ — ag) and noting that (g, f — ag)n = 0, we see that,
for any n > 0,
= a\l - pfn ±2a(l ~
>
\2n
ag) li.
since
±{g,Pn(f~ag))n 
= {PngJ-ag)n 
= (1 - p)n{g,f- ag)^ = 0.
On the other hand, [|P"/|||)7I. < C2(l - e)2™||/||2 < (OR)2(1 - e)2n. Thus,
|(1 - /?)2n < (Ci?)2(l - e)2n for all n > 0, which is possible only if /3 > e.
When no such g exists, the same conclusion holds, only one has to invoke the
Spectral Theorem in order to arrive at it.
As the preceding shows, uniform estimates on the variation distance be-
tween /xPn and 7T imply estimates like the one in (5.1.12). However, going in
the opposite direction is not always possible. To examine what can be done,
let a probability vector /x be given, and define / so that f(i) = 7^-. Then,
since (f)^ = 1 and (g)"^ < {g2}^ for all g G L2(n),
E
3
Hence, (5.1.12) implies that
(5.1.15)
<
(M)?
(1-/?)".
In the case when S is finite, and therefore there exists an A G (0,1) for which
(TT)J > A, (5.1.15) yields the Doeblin type estimate
(1-/3)".

5.2 Dirichlet Forms and Estimation of (5 
115
However, when S is infinite, (5.1.15), as distinguished from Doeblin, does
not give a rate of convergence which is independent of /i. In fact, unless
J2i j^fr < °°> it gives no information at all. Thus, one might ask why we are
considering estimates like (5.1.15) when Doeblin does as well and sometimes
does better. The answer is that, although Doeblin may do well when it works,
it seldom works when S is infinite and, even when S is finite, it usually gives
a far less than optimal rate of convergence. See, for example, Exercise 5.6.15
below.
5.2 
Dirichlet Forms and Estimation of (3
Our purpose in this section will be to find methods for estimating the op-
timal (i.e., largest) value of/3 for which (5.1.12) holds. Again, we will assume
that the chain is reversible and irreducible. Later, we will add the assumption
that it is aperiodic.
5.2.1. The Dirichlet Form and Poincare's Inequality: Our first step
requires us to find other expressions the right hand side of (5.1.13). To this
end, we begin with the observation that
(521) 
1 " 0 = sup{||P/||2i7r : / G £§(TT) with ||/||2,w = 1}
where L2
0(n) = {/ e L2(TT) : </>w = 0}.
It is obvious that the supremum on the right hand side of (5.1.13) dominates
the supremum on the right above. On the other hand, if / S L2(TV) with
||/||2,w = 1, then either ||/ - (/)7r||2,. = 0 and therefore ||P/ - (/)7r||2,7r = 0,
or 1 > ||/ - (/)w||2,7r > 0, in which case
i 
<
n/-(/) wii2, w;i 2 i W
is also dominated by the right hand side of (5.2.1).
To go further, we need to borrow the following simple result from the theory
of symmetric operators on a Hilbert space. Namely,
(5 2 2)
= sup{|(/,P/) w| : / € L&n) k ||/||2,w = l}.
That the right hand side of (5.2.2) is dominated by the left is Schwarz's
inequality: \{f,Pf)n\ 
< ||/||2,ir||P/||2,7r- To prove the opposite inequality,
let / e io(7r) w i t n ll/lkir = 1 be given, assume that ||P/||2,TT > 0, and
set g = TTpTif—• Then g s LQ(TT) and ||^||2,-w = 1- Hence, if 7 denotes the
IIP/II
supremum on the right hand side of (5.2.2), then, from the symmetry of P,
4||P/||2>W = 4(g, P/>w = ((/ + g), P ( / + g))v - ((/ - g),P(f - g))n
< 7(11/ + ffllL + 11/ " <7lli>w) = 27(11/11!,* + llfflll,w) = 47.

116 
5 REVERSIBLE MARKOV PROCESSES
The advantage won for us by (5.2.2) is that, in conjunction with (5.2.1), it
shows that
(3 = (3+ A /?_
(5'2'3) 
where /3± = inf {(/, (I T P)/)* : / € L§(TT) & ||/||2,w = 1}
Notice that, because (I - P ) / = (I - P)(/ - (/)„),
/?+ = inf {{/, (I - P)/) w : / G L2(TT)
At the same, because
((/ + c),(I + P)(/ + c))7r = (/ m(I + P)/ r i) 7 r+c 2 for / G L2(TT) and c G K,
it is clear that the infemum in the definition of /3_ is the same whether we
consider all / G L2(-K) or just those in £2(TT). Hence, another expression for
/?_ is
(5.2.5) 
/3_ = inf {(/, (I + P)/) w : / G X2(TT) & Var^/) = l}.
Two comments are in order here. First, when P is non-negative definite,
abbreviated by P > 0, in the sense that (/, P/)^ > 0 for all / G L2(TT), then
(5+ < 1 < /3_, and so /3 = /3_|_. Hence,
(5.2.6) P > 0 = » /? - inf{(/, (I - P)/) w : / G L2(TT) & Var7r(/) = l}.
Second, by Theorem 5.1.14, we know that, in general, /?_ = 0 unless the chain
is aperiodic, and (cf. (5.1.11) and the discussion at the beginning of §5.1.4),
when S is finite, that f3 > 0 if and only if § is aperiodic.
The expressions for /?+ and /?_ in (5.2.4) and (5.2.5) lead to important
calculational tools. Namely, observe that, by (5.1.1),
(/, (I - P)/)w = Y, /WWi(P)«(/« -
Hence, when we add the second expression to the last, we find that
(5.2.7) 
(/, (I - P)/>w = £(f, f) = \ J2(irUPh(f(J) - /«)2-
Because the quadratic form £(/, /) is a discrete analog of the famous quadratic
form \ j \Vf\2(x)dx 
introduced by Dirichlet, it is called a Dirichlet form.
Extending this metaphor, one interprets /?+ as the Poincare constant
(5.2.8) 
p+ = ini{£(/,/) 
: / G L2(TV) & Var^/) = 1}

5.2 Dirichlet Forms and Estimation of (3 
117
in the Poincare inequality
(5.2.9) 
/3+Var^/) <£(/,/), 
/ e L2(TT).
To make an analogous application of (5.2.5), observe that
(id)
and therefore
i X>)(P)(/C/) + /(i))2
(5.2.10) 
(/, (I + P)/>w = £(/, /) = i X>)i(P)i,-(/C/) + /(i))2.
Hence
2 n 
/3- 
mf{£(/,/) : / G L2(*r) & Var7r(/) = 1}
= inf {£(/, /) : / e Lg(Tr) & ||/||2>7r = 1}.
In order to give an immediate application of (5.2.9) and (5.2.11), suppose
(cf. Exercise 2.4.3) that (P)ij > £j for all (i,j), set e = ]T\ e,, assume that
e > 0, and define the probability vector /i so that (/i)* = ^-. Then, by
Schwarz's inequality for expectations with respect to [i and the variational
characterization of Var,^/) as the minimum value of a~-»((/ — a)2)^,
and, similarly, £(/,/) > fVarw(/). Hence, by (5.2.9), (5.2.11), and (5.2.3),
(3 > |. Of course, this result is a significantly less strong than the one we get
by combining Exercise 2.4.3 with the reasoning in §5.1.4. Namely, by that
exercise we know that ||^P" — TT||V £ 2(1 —e)n, and so the reasoning in §5.1.4
tells us that j3 > e, which is twice as good as the estimate we are getting here.
On the other hand, as we already advertised, there are circumstances when
the considerations here yield results when a Doeblin-type approach does not.
5.2.2. 
Estimating /?+: The origin of many applications of (5.2.9) and
(5.2.11) to estimate /?+ and /3_ is the simple observation that
(5.2.12) 
Var^/) = \

118 
5 REVERSIBLE MARKOV PROCESSES
which is easily checked by expanding (/(«) — /(j)) and seeing that the sum
on the right equals 2{/2)7r - 2(/)2
T.
The importance of (5.2.12) is that it expresses Vav.K(f) in terms of difference
between the values of / at different points in S, and clearly £(f, /) is also given
in terms of such differences. However, the differences which appear in £(f, f)
are only between the values of / at pairs of points (i, j) for which (P)jj > 0,
whereas the right hand of side of (5.2.12) entails sampling all pairs (i,j). Thus,
in order to estimate Var7r(/) in terms of £(f, /), it is necessary to choose, for
each (i,j) with i / j , a path p(i,j) = (ko,... ,kn) G Sra+1 with ko = i and
kn = j , which is allowable in the sense (P)fcro_1 km > 0 for each 1 < m < n,
and to write
where the summation in e is taken over the oriented segments 
(km-i,km)
in the path p(i,j), and, for e = (k,£), A e/ = f(£) — f(k). 
At this point
there are various ways in which one can proceed. For example, given any
{a(e) : e e p(i,j)} Q (0, oo), Schwarz's inequality (cf. Exercise 1.3.1) shows
that the quantity on the right is dominated by
where p(e) = {n)k{P)ki when e = (k,£). Thus, for any selection of paths V =
{p(hj) '• (hJ) G S2 \ £>} (D here denotes the diagonal {(i,j) G S2 : i = j})
and coefficients A — {a(e,p) : e € p G V} C (0, oo),
where
\eeP a(e,p)J f^ 
p{e)
and
W(V,A) =

5.2 Dirichlet Forms and Estimation of j3 
119
Hence, we have now shown that
(5.2.4) 
, + >
^ 
where ^ 
• • • • , £ £
for every choice of allowable paths V and coefficients A.
The most effective applications of (5.2.13) depend on making a choice of T
and A which takes advantage of the particular situation under consideration.
Given a selection V of paths, one of the most frequently made choices of A is
to take a(e,p) = 1. In this case, (5.2.13) gives
•*—' 
p(e')
e'ep 
ry 
'
where (TT(P))_ = (TT)J and (TT(P))_|_ = (TT)J when p begins at i and ends at j .
Finally, it should be recognized that, in general, (5.2.13) gives no informa-
tion. Indeed, although irreducibility guarantees that there is always at least
one path connecting every pair of points, when § is infinite there is no guar-
antee that V and A can be chosen so that W(V, A) < oo. Moreover, even
when S is finite, and therefore W(V, A) < oo for every choice, only a judicious
choice will make (5.2.13) yield a good estimate.
5.2.3. Estimating /?_: The estimation of (3- starting from (5.2.11) is a bit
more contrived than the one of /3+ starting from (5.2.9). For one thing, we
already know (cf. Theorem 5.1.14) that /?„ = 0 unless the chain is aperiodic.
Thus, we will now require that the chain be aperiodic. As a consequence of
aperiodicity and irreducibility, we know (cf. (3.1.13)) that, for each i £ § , there
always is a path p(i) = (ko, • • •, &2n+i) which is allowable (i.e. (P)fcTO_1/tm > 0
for each 1 < m < 2n + 1), is closed (i.e., ko = fc2n+i), and starts at i (i.e.,
ko = i)- Note our insistence that this path have an odd number of steps. The
reason for our doing so is that when the number of steps is odd, an elementary
exercise in telescoping sums shows that
In
m=0
Thus, if V be a selection of such paths, one path p(i) for each i e §, and we
make an associated choice of coefficients A = {a(e,p) : e 6 p G V} C (0, oo).
Then, just as in the preceding section,
2 
p(e)
a(e,p)

120 
5 REVERSIBLE MARKOV PROCESSES
where, when p = (ko, • • •, fen+i), TT(P) = (7r)fc0! and, for 0 < m < 2n, m(e)
m and A e/ = f(km) + f(km+i) 
if e = (fcm,fem+i). Hence, if
zD(p) = Tv(p) I max —;—
\ e€p 
a(e,i
then
2||/HL<W0M)£~(/,/) where
and, by (5.2.11), this proves that
" W(V,A)
for any choice of paths V and coefficients A satisfying the stated requirements.
When we take a(e,p) = 1, then this specializes to
(5.2.15) 
P-^^f] 
^
|
|
It should be emphasized that the preceding method for getting estimates
on f3- is inherently flawed in that it appears incapable of recognizing spectral
properties of P like non-negative definiteness. That is, when P is non-negative
definite, then /?_ > 1, but it seems unlikely that one could get that conclusion
out of the arguments being used here. On the other hand, because our real
interest is in (3 = /?+ A/3_ and the estimates given by (5.2.14) and (5.2.15) are
likely to be comparable, the inadequacy of (5.2.15) causes less damage than
one otherwise might fear.
5.3 Reversible Markov Processes in Continuous Time
Here we will see what the preceding theory looks like in the continuous time
context and will learn that it is both easier and more aesthetically pleasing
there.
We will be working with the notation and theory developed in Chapter 4.
In particular, Q will denote a matrix of the form R(P — I), where R is a
diagonal matrix whose diagonal entries are the rates 91 = {i?j : i G §} and
P is a transition probability matrix. We will assume throughout that Q is
irreducible in the sense that (cf. (4.4.2)), for each pair (i,j) G S2, (Qn)»j > 0
for some n > 0.
5.3.1. Criterion for Reversibility: Let Q be given, assume that the as-
sociated Markov process never explodes (cf. §4.3.1), and use {P(t) : t > 0}
to denote the semigroup determined by Q (cf. Corollary 4.3.2). Our purpose

5.3 Reversible Markov Processes in Continuous Time 
121
in this subsection is to show that if fi is a probability vector for which the
detailed balance condition
(5.3.1) 
(iJ-UQh = (£MQ)ji 
forall(i,i)eS2,
holds relative to Q, then the detailed balance condition
(5.3.2) 
(A)i(P(t)).. = (/i)j(P(t)).. 
for all* > 0 and (i,j) e S2
also holds.
The proof that (5.3.1) implies (5.3.2) is trivial in the case when the rates 9\
are bounded. Indeed, all that we need to do in that case is first use a simple
inductive argument to check that (5.3.1) implies (jl)i(Qn)ij = (fi)j(Q,n)ji for
all n > 0 and then use the expression for P(4) given in (4.2.13). When the
rates are unbounded, we will use the approximation procedure introduced in
§4.3.1. Namely, refer to §4.3.1, and take Q ^ corresponding to the choice
rates D t ^ described there. Equivalently, take ( Q ^ ) y to be (Q)^ if i 6 FN
and 0 when i ^ F^. Using induction, one finds first that ((Q^N^)n)ij = 0 for
all n > 1 and i <£ FN and second that
^ for all n > 0 and (i,j) e F2
N.
Hence, if {P(N\t) 
: t > 0} is the semigroup determined by Q^N\ then, since
the rates for QW are bounded, (4.2.13) shows that
(5 3 3)
In particular, because, by (4.3.3), (P^^t)).. —> (P(i)).., we are done.
As a consequence of the preceding, we now know that (5.3.1) implies that /i
is stationary for P(£). Hence, because we are assuming that Q is irreducible,
the results in §4.4.2 and §4.4.3 allow us to identify fi as the probability vec-
tor 7T = TTS introduced in Theorem 4.4.8 and discussed in §4.4.3, especially
(4.4.11). To summarize, if (i is a probability vector for which [5.3.1) holds,
then jl = 7T.
5.3.2. Convergence in L2{-k) for Bounded Rates: In view of the results
just obtained, from now on we will be assuming that vr is a probability vector
for which (5.3.1) holds when fi = it. In particular, this means that
(5.3.4) 
(iv)i(P(t))i. = (it)j(l>(t))jt 
foralH>Oand(i,j)eS2.
Knowing (5.3.4), one is tempted to use the ideas in §5.2 to get an estimate
on the rate, as measured by convergence in L2(TT), at which P(i)/ tends to
(/}#• To be precise, first note that, for each h > 0,

122 
5 REVERSIBLE MARKOV PROCESSES
and therefore (cf. (5.1.13),(5.2.6), and (5.2.7)) that
(3{h) = 1 - sup{ | (/, P(h)f)^ | : / e Lg(Tr) with ||/||2>* = 1}
= inf{(/,I - P(/i)j% : Var#(/) = 1} = inf{£,(/,/) : Var*(/) = l } ,
where
£h(f,f) = l^n)i(P(h))..(fU) 
-f(i)f
is the Dirichlet form for P(/i) on £2(TT). Hence (cf. (5.1.12)), for any * > 0
and n e Z +,
||P(*)/ - (fh\\m*) < (i -/5(^))"ll/ - </>*lk*.
To take the next step, we add the assumption that the rates *H are bounded.
One can then use (4.2.8) to see that, uniformly in / satisfying Var^(/) = 1,
l i m
h\o 
h
where
(5.3.5) 
e<*{f,f) = l^inUQhiHj) 
-/(i))2;
and from this it follows that the limit lim^x^o h~1(3{h) exists and is equal to
(5.3.6) 
A = inf{£Q(/, /) : / e L2(TV) k Var*(/) = l}.
Thus, at least when the rates are bounded, we know that
(5.3.7)
5.3.3. L2(7r)-Convergence Rate in General: When the rates are un-
bounded, the preceding line of reasoning is too naive. In order to treat the
unbounded case, one needs to make some additional observations, all of which
have their origins in the following lemma.3
5.3.8 LEMMA. 
Given f e L2(iv), the function t e [0, oo) >—> ||P(t)/|||>#
is continuous, non-increasing, non-negative, and convex. In particular, t G
(0, oo) i—> (/•(I-P(f))/>* is non-increasing, and therefore
v 
H/lil*HP(ft)/ll!,* 
. f . rn ,
hm 
: 
— exists m 0, oo .
h\o 
h
3 If one knows spectral theory, especially Stone's Theorem, the rather cumbersome argument
which follows can be avoided.

5.3 Reversible Markov Processes in Continuous Time 
123
In fact (cf. (5.3.5)),
ii/llU - IIPW/II!,
h
PROOF: Let / e L2(TC) be given, and (cf. the notation in §4.3.1) define
/iv = IFJV/- Then, by Lebesgue's Dominated Convergence Theorem, ||/ —
/ATIk* —> ° a s N ~* °°. Because ||P(t)g||2^ < \\g\\2,* for all t > 0 and
g e L2(ir), we know that
/iv)||2,* < ||/ - fffh,* 
— 0
uniformly in t G (0, oo) as N —> oo. Hence, by part (a) of Exercise 5.6.1 below,
in order to prove the initial statement, it suffices to do so when / vanishes off
of FM for some M. Now let / be a function which vanishes off of FM, and set
V(t) = ||P(t)/||l,#. At the same time, set ^N(t) = HP^W/II',* for N > M.
Then, because by (4.3.3), ipw —> \j) uniformly on finite intervals, another
application of part (a) in Exercise 5.6.1 allows us to restrict our attention to
the V'iv's. That is, we will have proved that tp is a continuous, non-increasing,
non-negative, convex function as soon as we show that each ^JV is. The non-
negativity requires no comment. To prove the other properties, we apply
(4.2.7) to see that
Next, by the first line of (5.3.3), we know that, because N > M, p("'(t)/
vanishes off of FAT, and so, because (TT)JQ^- ' = (TT^Q^ 
for (i,j) € F^, the
preceding becomes
Similarly, we see that
# > 0.
Clearly, the second of these proves the convexity of ^jy. In order to see that
the first implies that tpN is non-increasing, we will show that
if g = 0 off FAT 
and V(N) {€} = ^ 
Qij 
for

124 
5 REVERSIBLE MARKOV PROCESSES
To check (*), first observe that
(g, -Q{N)gh = - J2 W
Next, use (it)i(Q)ij = (Tr)j(Q,)ji for (z, j) € i7^ to see that
and thereby arrive at (*). Finally, apply (*) with g = P(N\t)f 
to conclude
that tpN < 0.
Turning to the second and third assertions, let / be any element of L2(-fr).
Now that we know that the corresponding tp is a continuous, non-increasing,
non-negative, convex function, it is easy (cf. part (d) in Exercise 5.6.1) to
check that t~^> ^ >-vW j g non-increasing and therefore that lim^\o
exists in [0, oo]. Next, remember (5.2.7), apply it when P = P(2/i), and
conclude that
Hence, since, by (4.3.4),
lim -—' , ''lJ = 2(Q)a 
for i ^ j ,
the required inequality follows after an application of Fatou's Lemma. 
•
5.3.9 LEMMA. 
If 0 < s < t, then for any f 6 L2(-k)
mu , IPM/III* - iipw/iii, , 2£«(P(t)/,p(t)/).
5 
~C S
PROOF: Set ip{i) = ||P(i)/ll2 7r- ^ e know that ip is a continuous, non-
increasing, non-negative, convex function. Hence, by part (a) of Exercise
5.6.1,
V>(0) > V>(0) - ^(s) > ^(a) - ^ft) ^_ ^(t) - ^{t + h)
s 
~ 
s 
~ 
t — s 
~ 
h

5.3 Reversible Markov Processes in Continuous Time 
125
for any h > 0. Moreover, because,
h 
h
the last part of Lemma 5.3.8 applied with P(t)f replacing / yields the second
asserted estimate. 
•
With the preceding at hand, we can now complete our program. Namely,
by writing
ra-l
II f I!2 
— \\~P(t\\\2 
— \~^ 
(\\~P(?2±)\2
\\J Il2,ir 
lFWIl2,7r — /__/ \ \ \ r \ n ) \ 2,-fr
m=0
we can use the result in Lemma 5.3.9 to obtain the estimate
ii fii2 
iipmfii2 
> — V^ 
f
rn=l
Hence, if A is defined as in (5.3.6), then, for any / G LQ(TT),
m=l
which, when n —> oo, leads to
II f\\2 
— HPff'l f II2 - > 2A
Finally, by Gronwall's inequality (cf. Exercise 5.6.4), the preceding yields the
estimate ||P(i)/|||^. < e~2A*||/||2 #• After replacing a general / G L2(n) by
/ — (/)#, we have now proved that (5.3.7) holds even when £R is unbounded.
5.3.4. Estimating A: Proceeding in the exactly the same way that we did
in §5.2.2, we can estimate the A in (5.3.6) in the same way as we estimated
/?+ there. Namely, we make a selection V consisting of paths p(i,j), one for
each from pair (i,j) G S\ D, with the properties that if p(i, j) = (ko,... ,kn),
then ko = i, kn = j , and p(i,j) is allowable in the sense that (Q)/^^ fcm > 0
for each 1 < m < n. Then, just as in §5.2.2, we can say that
(5*10) 
A
>
^ 
where 
^
^
;
V 
' 
p3e e'ep 
' X 
'
where the supremum is over oriented edges e = (k,£) with (Q)fc^ > 0, the
first sum is over p G V in which the edge e appears, the second sum is over
edges e' which appear in the path p, (TT(P))_ = (TT)J if the path p starts at i,
(?r(p))+ = (TT)J if the path p ends at j , and p(e') = (7r)fc(Q)/« if e' = (fc,^).

126 
5 REVERSIBLE MARKOV PROCESSES
5.4 Gibbs States and Glauber Dynamics
Loosely speaking, the physical principle underlying statistical mechanics
can be summarized in the statement that, when a system is in equilibrium,
states with lower energy are more likely than those with higher energy. In
fact, J.W. Gibbs sharpened this statement by saying that the probability of
a state i will be proportional to e~~^r~, where k is the Boltzmann constant,
T is temperature, and H(i) is the energy of the system when it is in state
i. For this reason, a distribution which assigns probabilities in this Gibbsian
manner is called a Gibbs state.
Since a Gibbs state is to be a model of equilibrium, it is only reasonable to
ask what is the dynamics for which it is the equilibrium. From our point of
view, this means that we should seek a Markov process for which the Gibbs
state is the stationary distribution. Further, because dynamics in physics
should be reversible, we should be looking for Markov processes which are
reversible with respect to the Gibbs state, and, because such processes were
introduced in this context by R. Glauber, we will call a Markov process which
is reversible with respect to a Gibbs state a Glauber dynamics for that Gibbs
state.
In this section, we will give a rather simplistic treatment of Gibbs states
and their associated Glauber dynamics.
5.4.1. Formulation: Throughout this section, we will be working in the
following setting. As usual, S is either a finite or countably infinite space. On
§ there is given some "natural" background assignment v G (0, oo)s of (not
necessarily summable) weights, which should be thought of as a row vector.
In many applications, v is uniform: it assigns each i weight 1, but in other
situations it is convenient to not have to assume that it is uniform. Next,
there is a function H : § —> [0, oo) (alias, the energy function) with the
property that
(5.4.1) 
Z(/3) = J2e~l3H(i)(»h 
<°° for each/3 e (0,oo).
In the physics metaphor, j3 — -^ is, apart from Boltzmann's constant k,
the reciprocal temperature, and physicists would call /3~>Z(/3) the partition
function. Finally, for each (3 e (0, oo), the Gibbs state /y(/3) is the probability
vector given by
(5.4.2) 
7 ( / 3 )) 
e - ^ W M i 
forieS.
From a physical standpoint, everything of interest is encoded in the parti-
tion function. For example, it is elementary to compute both the average and
variance of the energy by taking logarithmic derivatives:
(5.4.3) 
(-H"}7(/3) = —rz;logZ(P) 
and VarT(/3) (H) - —^ log Z(f3).

5.4 Gibbs States and Glauber Dynamics 
127
The final ingredient is the description of the Glauber dynamics. For this
purpose, we start with a matrix A all of whose entries are non-negative and
whose diagonal entries are 0. Further, we assume that A is irreducible in the
sense that
(5.4.4) 
sup(A n) i j>0 
for all (i,j) G S2
n>0
and that it is reversible in the sense that
(5.4.5) 
Mi(A)y = ( ^ ( A ) ^ 
for all (i,j) e S2.
Finally, we insist that
(5.4.6) 
^2 e~0HUHA)ij 
< °° 
for e a c h « e S and /? > 0.
je§
At this point there are many ways in which to construct a Glauber dynam-
ics. However, for our purposes, the one which will serve us best is the one
whose Q-matrix is given by
(
)
y 
when j
where a+ = a V 0 is the non-negative part of the number O E K . Because,
(5.4.8) 
(7(/3)) .(Q(/J))y = Z03)-ie-W>i'lJ)(I;).(A)y for i ± j ,
clearly is reversible for Q(/3). There are many other possibilities, and
the optimal choice is often dictated by special features of the situation under
consideration. However, whatever choice is made, it should be made in such
a way that, for each /3 > 0, Q(/3) determines a Markov process which never
explodes.
5.4.2. The Dirichlet Form: In this subsection we will modify the ideas
developed in §5.2.3 to get a lower bound on
(5-4"9) 
where Sp(f, /) EE \ E f r G ^ W ) ) ^ (/(j) - /(*))'
and Varjg(/) is shorthand for Var7(/3)(/), the variance of / with respect to
7(/3). For this purpose, we introduce the notation
Elev(p) = max H(im) 
and e(p) = Elev(p) - H(i0) — H(in)
0<m<n

128 
5 REVERSIBLE MARKOV PROCESSES
for a path p = (to,... ,in). 
Then, when Q(/J) is given by (5.4.7), one sees
that, when p = (io,..., in)
where w(p) =
Hence, for any choice of paths V, we know that (cf. (5.3.10))
where W(V) = sup^^io(p) and E(V) = supe(p),
and therefore that
On the one hand, it is clear that (5.4.10) gives information only when
W{V) < oo. At the same time, it shows that, at least if ones interest is
in large /?'s, then it is important to choose V so that E(V) is as small as
possible. When S is finite, reconciling these two creates no problem. Indeed,
the finiteness of S guarantees that W(V) will be finite for every choice of
allowable paths. In addition, finiteness allows one to find for each (i,j) a
path p(i,j) which minimizes Elev(p) among allowable paths from i to j , and
clearly any V consisting of such paths will minimize E(V). Of course, it is
sensible to choose such a V so as to minimize W(V) as well. In any case,
whenever S is finite and V consists of paths p(i,j) which minimize Elev(p)
among paths p between i and j , E(V) has a nice interpretation. Namely,
think of S as being sites on a map and of H as giving the altitude of the
sites. That is, in this metaphor, H(i) is the distance of i "above sea level."
Without loss in generality, we will assume that at least one site ko is at sea
level: H(ko) = 0.4 When such an ko exists, the metaphorical interpretation of
EiT1) is as the least upper bound on the altitude a hiker must gain, no matter
where he starts or what allowable path he chooses to follow, in order to reach
the sea. To see this, first observe that if p and p' are a pair of allowable paths
and if the end point of p is the initial point of p\ then the path q is allowable
and Elev(g) < Elev(p) V Elev(p') when q is obtained by concatenating p and
4 If that is not already so, we can make it so by choosing ko to be a point at which H takes
its minimum value and replacing H by H — H(ko). Such a replacement leaves both
and Q(/3) as will as the quantity on the right hand side of (5.4.10) unchanged.

5.4 Gibbs States and Glauber Dynamics 
129
p ' : if p = (io,...,in) 
andp' 
= (i'o,... 
, i ' n , ) , t h e n q = (i0,... 
,in,i'lt... 
, i ' n , ) .
Hence, for any (i,j), e(p(i,j)) = e(p(i, fco)) Ve(p(j,fco)); from which it should
be clear that E(V) = maxj e(p(i, ko))- Finally, since, for each i, e(p(i, fco)) =
H(£)—H(i), where £ is a highest point along the pathp(i, fco), the explanation
is complete. When § is infinite, the same interpretation is valid in various
circumstances. For example, it applies when H "tends to infinity at infinity"
in the sense that {i : H(i) < M} is finite for each M < oo.
When § is finite, we can show that, at least for large j3, (5.4.10) is quite
good. To be precise, we have the following result:
5.4.11 THEOREM. Assume that E> is finite and that Q([3) isgiven by (5.4.7).
Set m = minjgg H(i) and §o = {i : H(i) = m}, and let e be the minimum value
E(V) takes as V runs over all selections of allowable paths. Then e > — m,
and e = —m if and only if for each (i,j) £ §x§o there is an allowable path
p from i to j with Elev(p) = H(i). (See also Exercise 5.6.5 below.) More
generally, whatever the value of e, there exist constants 0 < c_ < c+ < oo,
which are independent of H, such that
c_e-/3(e+m) < A/3 < c+e-/3(*+™) 
for 
a\\ Q > Q.
PROOF: Because neither f((3) nor Q(/3) is changed if H is replaced by H — m
whereas e changes to e + m, we may and will assume that m = 0.
Choose a collection V = {p(i,j) : (i,j) S S2} of allowable paths so that,
for each (i,j), e(p(i,j)) minimizes e(p) over allowable paths from i to j . Next
choose and fix a fc0 G So- By the reasoning given above,
(*) 
e = maxe(p(i,fc0)).
In particular, since e(p(i,fco)) = Elev(p(i,fco)) — H(i) > 0 for all i € S,
this proves that e > 0 and makes it clear that e = 0 if and only if H(i) =
Elev (p(i,k0)) ior alii e S.
Turning to the lower bound for A^, observe that, because m = 0, Z(0) >
(v)k0 > 0 a nd therefore, by (5.4.10), that we can take c_ = •$$&•
Finally, to prove the upper bound, choose 4 £ § \ {^o} so that e(po) = e
when po = p(£o, fco), let T be the set of i G S with the property that either
i = fco or Elev(p(i)) < Elev(po) for the path p(i) = p(i,ko) g V from i to fco,
and set / = l r . Then, because fco G F and £0 £ F,
e-0(H(ko)+H(eo))_
At the same time
•-/)= E 
W)),
Z(3) 
£—<
(i.i)srxrC 
(i.j)erxrC

130 
5 REVERSIBLE MARKOV PROCESSES
But if i e T \ {k0}, j £ r, and (A)^- > 0, then H(j) > Ehv(p0). 
To see
this, consider the path q obtained by going in one step from j to i and then
following p(i) from i to fco- Clearly, q is an allowable path from j to fco, and
therefore Elev(gr) > Elev(p(j)) > Elev(po)- But this means that
Elev(po) < Elev(p(j)) < Elev(g) = Elev(p(i)) V H(j),
which, together with Elev(p(z)) < Elev(po), forces the conclusion that H(j) >
Elev(po)- Even easier is the observation that H{j) > Elev(po) if J ^ T and
j > 0, since in that case the path (j, fco) is allowable and
j) = Elev(O\fco)) > Elev(p(j)) > Elev(po).
Hence, after plugging this into the preceding expression for £p (/, /), we get
£P(f,f)<e
 z(0) 
E
which, because e = Elev(po) — H(ko) — H(£Q), means that
Finally, because Z{(3) < Iji'llvj the upper bound follows. 
•
5.5 Simulated Annealing
This concluding section deals with an application of the ideas in the preced-
ing section. Namely, given a function H : S —> [0, oo), we want to describe
a procedure, variously known as the simulated annealing or the Metropolis
algorithm, for locating a place where H achieves its minimum value.
In order to understand the intuition which underlies this procedure, let A
be a matrix of the sort discussed in §5.4, assume that 0 is the minimum value
of H, set So = {i : H(i) = 0}, and think about dynamic procedures which
would lead you from any initial point to So y i a paths which are allowable
according to A (i.e., A ^ > 0 if k and (. are successive points along the path).
One procedure is based on the steepest decent strategy. That is, if one is at fc,
one moves to any one of the points £ for which A^e > 0 and H(£) is minimal
if H{£) < H(k) for at least one such point, and one stays put if H{£) > H(k)
for every £ with (A)^ > 0. This procedure works beautifully as long as you
avoid, in the metaphor suggested at the end of §5.4, getting trapped in some
"mountain valley." The point is that the steepest decent procedure is the most
efficient strategy for getting to some local minimum of H. However, if that
minimum is not global, then, in general, you will get stuck! Thus, if you are
going to avoid this fate, occasionally you will have to go "up hill" even when

5.5 Simulated Annealing 
131
you have the option to go "down hill." However, unless you have a detailed
a priori knowledge of the whole terrain, there is no way to know when you
should decide to do so. For this reason, it may be best to abandon rationality
and let the decision be made randomly. Of course, after a while, you should
hope that you will have worked your way out of the mountain valleys and that
a steepest decent strategy should become increasingly reasonable.
5.5.1. The Algorithm: In order to eliminate as many technicalities as
possible, we will assume throughout that § is finite and has at least 2 elements.
Next, let H : § —• [0, oo) be the function for which we want to locate a place
where it achieves its minimum, and, without loss in generality, we will assume
that 0 is its minimum. Now take v so that (i/)j = 1 for all i € S, and choose
a matrix A so that (A)a = 0 for all i G S, (A)^- = (A)jj > 0 if j ^ i, and
A is irreducible (cf. (5.4.4)) on S. In practice, the selection of A should be
made so that the evaluation of H(j) — H(i) when (A)^- > 0 is as "easy as
possible." For example, if S has some sort of natural neighborhood structure
with respect to which S is connected and the computation of H{j) — H(i)
when j is a neighbor of i requires very little time, then it is reasonable to take
A so that (A)ij = 0 unless j =^ i is a neighbor of i.
Now define -y((3) as in (5.4.2) and Q(/3) as in (5.4.7). Clearly, 7(0) is just
the normalized, uniform distribution on S: (7(0)) ^ = L~1, where L = #§
> 2 is the number of elements in S. On the one hand, as /3 gets larger, "f((3)
becomes more concentrated on So- More precisely, since Z{(3) > #§0 > 1
(5.5.1) 
<lsoc)7(/3) < Le-?s, 
where S = mln{H(j) : j $ So}.
On the other hand, as (3 gets larger, Theorem 5.4.11 says that, at least when
e > 0, \((3) will be getting smaller. Thus, we are confronted by a conflict.
In view of the introductory discussion, this conflict between the virtues of
taking (3 large, which is tantamount to adopting an approximately steepest
decent strategy, versus those of taking /? small, which is tantamount to keeping
things fluid and thereby diminishing the danger of getting trapped, should be
expected. Moreover, a resolution is suggested at the end of that discussion.
Namely, in order to maximize the advantages of each, one should start with
(3 = 0 and allow j3 to increase with time.5 That is, we will make (3 an
increasing, continuous function t~~>/3(t) with (3(Q) = 0. In the interest of
unencumbering our formulae, we will adopt the notation
Vart=Var7t, 
Q(t) = Q(/3(t)), 
€t = £p{t), 
and Xt = \0{t).
5 Actually, there is good reason to doubt that monotonically increasing 8 is the best way to
go. Indeed, the name "simulated annealing" derives from the idea that what one wants to do
is simulate the annealing process familiar to chemists, material scientists, skilled carpenters,
and followers of Metropolis. Namely, what these people do is alternately heat and cool
to achieve their goal, and there is reason to believe we should be following their example.
However, I have chosen not to follow them on the unforgivable, but understandable, grounds
that my analysis is capable of handling only the monotone case.

132
5 REVERSIBLE MARKOV PROCESSES
Because, in the physical model, (3 is proportional to the reciprocal of tem-
perature and j3 increases with time, t~*0(t) is called the cooling schedule.
5.5.2. Construction of the Transition Probabilities: Because the Q
matrix here is time dependent, the associated transition probabilities will be
time-inhomogeneous. Thus, instead of £--~>Q(£) determining a one parameter
family of transition probability matrices, for each s s [0, oo) it will determine
a map t~-»P(s,t) from [s,oo) into transition probability matrices by the time-
inhomogeneous Kolmogorov forward equation
(5.5.2)
— P(s,t) =P{s,t)Q(t) 
on (s, oo) with P (s,s) = I.
Although (5.5.2) is not exactly covered by our earlier analysis of Kolmogorov
equations, it nearly is. To see this, we solve (5.5.2) via an approximation
procedure in which Q(t) is replaced on the right hand side by
m
QW(t) = Q([t]N) where [t]N = - f o r t e [f,
AT 
;•
The solution t-~+P(N\s,t) to the resulting equation is then given by the pre-
scription I*(N\s,s) = I and
s, t) =
s, s V
for
As this construction makes obvious, T?(N\s,t) is a transition probability ma-
trix for each TV > 1 and t>s, and (s,i)~-»P^JV-'(s, t) is continuous. Moreover,
|Q(Hiv)-Q([T] M,
S,T) 
dr
' 
' l l u , v
But
| | Q ( T ) | | U I V < ||A||UiV and
and so
- Q(r)
\P([T}N)-p({r)M)\dT

5.5 Simulated Annealing 
133
Hence, after an application of Gronwall's inequality, we find that
sup ||PW(M)-P(M>(S)*)|
0<s<t<T
UV
< ||A||U,V||JJ||uellAll"-T / 
\P([T]N) - (3([T]M) I dr.
Jo
/
o
Because T~~+/3(T) is continuous, this proves that the sequence
JV > 1} is Cauchy convergent in the sense that, for each T > 0,
lim sup 
sup |P ( A r )(s,t)-P ( M )(s,t)| 
=0.
M-+oo N>M 0<s<t<T 
U>V
As a consequence, we know that there exists a continuous (s,t)~-*P(s, i) to
which the P(N\s,t)'s 
converge with respect to || • ||U)V uniformly on finite
intervals. In particular, for each t > s, P(s,t) is a transition probability
matrix and t G [s, oo) i—> P(s,t) is a continuous solution to
ft
P(s,t)=I+ 
P(s,r)Q(r)dr, 
te[s,oo),
Js
which is the equivalent, integrated form (5.5.2). Furthermore, if £ 6 [s, oo) i—>
l^t £ Mi (S) is continuously differentiable, then
(5.5.3) fit = —tit = tHQ(t) for t e [s, oo) <^=^ /2t = /xsP(s,£) for t G [s, oo).
Since the "if" assertion is trivial, we turn to the "only if" statement. Thus,
suppose that t G [s, oo) i—> /zt G Mi(S) satisfying /i.j = /xtQ(£) is given, and
set u>t = fit — fiaP(s,t). Then
and so,
<||A||U.V / llwJIv dr.
||A||U;V A K
J s
Hence, after another application of Gronwall's inequality, we see that u)t = 0
for all t > s.
Of course, by applying this uniqueness result when fit = 6iP(s,t) for each
i G S, we learn that (s, £)~~>P(s, t) is the one and only solution to (5.5.2). In ad-
dition, it leads to the following time-inhomogeneous version of the Chapman-
Kolmogorov equation:
(5.5.4) 
P(r,t) =P(r,s)P(s,t) 
for 0 < r < s < t.
Indeed, set Ht = 6iP(r,t) for t > s, note that t-^tH satisfies (5.5.3) with
/is = <$jP(r, s), and conclude that fit = <$jP(r, s)P(s,£).

134 
5 REVERSIBLE MARKOV PROCESSES
5.5.3. Description of the Markov Process: Given a probability vector
jit, we now want to construct a Markov process {X(t) : t > 0} which has n as
its initial distribution and (s,£)~»P(s, t) as its transition mechanism, in the
sense that
(5.5.5) 
P(X(0) = i)= (fi)i & F(X(t) = j [ X(a), u e [0, s]) = P(s, 
t)x(s)j.
The idea which we will use is basically the same as the one which we used in
§4.2.1 & 2.1.1. However, life here is made more complicated by the fact that
the time inhomogeneity forces us to have an uncountable number of random
variables at hand: a pair for each (i, i) G [0, oo) x S. To handle this situation,
take, without loss in generality, S = {1,..., L} and, for (t, i,j) S [0, oo) x S2,
set S{t,i,j) = E L i e-^W(/rW-ffW)+(A)tf) take S(t,i,O) = 0, and define
f „• if S(t,i,j~l) 
< 
S(t,i,j)
tt(M,«) = < 
s(tA'L) 
~ 
s ( M l i )
[ i if u > 1.
Also, determine T : [0, oo) x S x [0, oo) —> [0, oo) by
Next, let Xo be an S-valued random variable with distribution fj,, let {En :
n > 1} be a sequence of unit exponential random variables which are indepen-
dent of each other and of Xo, and let {Un : n > 1} be a sequence of random
variables which are uniformly distributed on [0,1) and independent of each
other and of cr({X0} U {En : n > 1}). Finally, set Jo = 0 and X(0) = Xo,
and, when n > 1, use induction to define
Jn - Jn_! = Tfa^XiJ^En), 
X(Jn) =
and X(t) = X( Jn_i) 
for Jn_x <t < Jn-
Without substantial change, the reasoning given in §2.1.1 combined with that
in §4.2.2 allows one to show that (5.5.5) holds.
5.5.4. Choosing a Cooling Schedule: In this section, we will give a ratio-
nal basis on which to choose the cooling schedule t-^/3(t). For this purpose, it
is essential to keep in mind what it is that we are attempting to do. Namely,
we are trying to have the Markov process {X(t) : t > 0} seek out the set
So = {j : H(j) = 0} in the sense that, as t -> oo, P(X(t) £ So) should tend
to 0 as fast as possible, and the way we hope to accomplish this is by making
the distribution of X(t) look as much like -yt as possible. Thus, on the one
hand, we need to give {X(t) : t > 0} enough time to equilibrate, so that the
distribution of X(t) will look a lot like -ft- On the other hand, in spite of the
fact that it may inhibit equilibration, unless we make /?(£) increase to infinity,

5.5 Simulated Annealing 
135
there is no reason for our wanting to make the distribution of X(t) look like
It-
In order to understand how to deal with the concerns raised above, let fx
be a fixed initial distribution, and let /j,t be the distribution at time t > 0
of the Markov process {X(t) : t > 0} described in the §5.5.3 with initial
distribution /J,. Equivalently, fxt — fiP(0,t), where {P(s,t) : 0 < s < t < oo}
is the family of transition probability matrices constructed in §5.5.2. Next,
define ft : S —> [0, oo) SO that
ft(i) = <^± for t > 0 and i e S.
V~ft)i
It should be obvious that the size of ft provides a good measure of the extent
to which \it resembles "ft- For example, by Schwarz's inequality and (5.5.1),
F(X(t) i So) = (lSoC)Mt = </tlSoC}t
(
)
and so we will have made progress if we can keep ||/t||2,t under control.
With the preceding in mind, assume that £~+/3(£) is continuously differen-
tiable, note that this assumption makes i-^H/tH^t a^so continuously differen-
tiable, and, in fact, that
d ,, . ,,o 
d I 1
since, by (5.4.3), Z{t) = —$(t)Z(t){H)t. On the other hand, we can compute
this same derivative another way. Namely, because (P(0,£)g)M = {g)y.t =
(ft,g)t for any function g,
and so we can use (5.5.2) to see that
ft\\ft\\h = <P(o,t)Q(t)/*>M + <P(o,t)/t)M = -St(fuft) + {ftJth.
Thus, after combining these to eliminate the term containing ft, we arrive at

136 
5 REVERSIBLE MARKOV PROCESSES
where, in passing to the second line, we have used the fact that (ft)t = 1 and
therefore that Vart(/) = H/tlUt - 1- Putting all this together, we now know
that
The preceding differential inequality for H/H^t is easy to integrate. Namely,
it says that
i 
(eA(t)||/t||i>t) < 2AteA« where A(t) = f XTdr.
Hence,
II £ i | 2 
^ 
„ — A f t ) it £ 
H
2 
i o
/
i 
^, — A ( t ) \ ^ 
\\ £ | | 2 
\
/
o
ll/tlb.t < e 
Il/o|l2,o + 2^1 ~ e 
J < ll/o||2,o V2-
Moreover, since (7o)j = L~x, where L = # § > 2, ||/o|||jO ^ ^) an(i s o
(5.5.7) 
\\H\\J(t)<Xt =» \\fth,t<Li.
The final step is to find out how to choose t-^/3(t) so that it satisfies the
condition in (5.5.7); and, of course, we are only interested in the case when
So ^ S or, equivalently, ||i?||u > 0. Next, by Theorem 5.4.11, we know that
At > c_e~^*)e. Hence, we can take
(5.5.8)
the case when e = 0 being obtained by an obvious limit procedure. After
putting this together with (5.5.7) and (5.5.6), we have now proved that when
f3(t) is given by (5.5.8), then
C-tt\
1 + ,,rTII 
when c > 0
e 2!i"ii« 
when e = 0.
Remark: The result in (5.5.9) when e = 0 deserves some further comment.
In particular, it should be observed that e = 0 does not guarantee success
for a steepest decent strategy. Indeed, e = 0 only means that each i can be
connected to So by an allowable path along which H is non-increasing (cf.
Exercise 5.6.5), it does not rule out the possibility that, when using steepest
decent, one will choose bad path and get stuck. Thus, even in this situation,
one needs enough randomness to hunt around until one finds a good path.

5.5 Simulated Annealing 
137
5.5.5. Small Improvements: An observation, which is really only of inter-
est when e > 0, is that one carry out the same sort of analysis to control the
||/t||g,t = ((\ft\q)t)* for each q G [2, oo). As a result, one can show that there
is, for each 9 e (0,1), a cooling schedule which makes ¥(X(t) £ So) go to 0 at
least as fast as t~~^. The relationship between 9 and q is given by 9 — 1 — -.
To carry this out, one begins by computing ^||/t||g)t twice, once for each
each of the following expressions:
\\ft\\9
g,t = {f?)t and ||/t||*,t = (P((M)/rV
One then eliminates ft from the resulting equations and thereby arrives at
where q' — -^y and
At this point one has to show that
and a little thought makes it clear that this inequality comes down to checking
that, for any pair (a, b) 6 [0, oo)2,
which, when looked at correctly, follows from the Fundamental Theorem of
Calculus plus Schwarz's inequality. Hence, in conjunction with the preceding
and (5.3.6), we find that
In order to proceed further, we must learn how to control (ft
2 )2 in terms of
H/tllq t. In the case when q = 2, this quantity caused no problem because we
knew it was equal to 1. When q > 2, we no longer have so much control over
q 
q 
-.
it. Nonetheless, by first writing (/t
2 )| = (/t
2 
}^t and then using part (c) of
Exercise 5.6.2 to see that
q 
q-2 
q-2
\Jt 
//i t — wt 
/**t 
— \Jt It 
»

138 
5 REVERSIBLE MARKOV PROCESSES
we arrive at (ft
2)^ 
< (fi)^1. 
Armed with this estimate, we obtain the
differential inequality
Finally, by taking (3(t) = |log(l + Jjnjnr), the preceding inequality can be
replaced by
which, after integration, can be made to yield
Remark: Actually, it is possible to do even better if one is prepared to
combine the preceding line of reasoning, which is basically a consequence
of Poincare's inequality, with analytic ideas which come under the general
heading of Sobolev inequalities. The interested reader might want to consult
[3], which is the source from which the contents of this whole section are
derived.
5.6 Exercises
EXERCISE 5.6.1. A function ip : [0, oo) —> M is said to be convex if the graph
of xp lies below the secant connecting any pair of points on its graph. That is,
if it satisfies
(*) V((l " 6)s + 6t) < (1 - 9)ip{s) + 6ip(t) for all 0 < s < t and 9 e [0,1].
This exercise deals with various properties of convex functions, all of which
turn on the property that the slope of a convex function is non-decreasing.
(a) If {ipn}f U {ip} are functions on [0, oo) and ipn(t) —> ip{t) for each
t 6 [0, oo), show that ip is non-increasing if each of the tpn's is and that xp is
convex if each of the V'n's is.
(b) If ip : [0, oo) —> M. is continuous and twice continuously differentiable
on (0, oo), show that ip is convex on [0, oo) if and only if ip > 0 on (0, oo).
Hint: The "only if" part is an easy consequence of
7u, 
y 
0(t + ft) + -ip(t -h) 
2iP{s)
ip(t) = hm — 
—^ 
— 
for t e (0, oo).
To prove the "if" statement, let 0 < s < t be given, and for e > 0 set
- 9)s + 9t) - (1 - 9)xP{s) - 9xP(s) - e9{\ -9), 
9e [0,1].
Note that <pe(0) = 0 = (p€(l) and that <pe > 0 on (0,1). Hence, by the second
derivative test, tpe cannot achieve a maximum value in (0,1). Now let e \ 0.

5.6 Exercises 
139
(c) If tp is convex on [0, oo), show that, for each s £ [0, oo),
ip(s) ~ ip(t)
t 6 (s, oo) i—> —— 
— 
is non-mcreasmg.
t — s
(d) If %j) is convex on [0, oo) and 0<s<t<u<w, 
show that
ip(s) - ip(t) > ip(u) - 4>(w)
t — s 
~ 
w — u
Hint: Reduce to the case when u = t.
EXERCISE 5.6.2. Given a probability vector \i £ [0, l]s, there are many ways
to prove that (/)^ < (/2)M for any / G L2(/i.). For example, one can get this
inequality as an application of Schwarz's inequality |(/,5)^| < ||/||2,/Li||5r||2,^
by taking g = 1. Alternatively, one can use 0 < VarM(/) = {/2)M — (/)^.
However, neither of these approaches reveals the essential role that convexity
plays here. Namely, the purpose of this exercise is to show that for any
non-decreasing, continuous, convex function tp : [0, oo) —> [0, oo) and any
(5.6.3)
where the meaning of the left hand side when (/)M = oo is given by taking
ip(oo) = luntyooi()(t). The inequality (5.6.3) is an example of more general
statement known as Jensen's inequality (cf. Theorem 6.1.1 in [8]).
(a) Use induction on n > 2 to show that
^ ( ) 
for 
all
\m=l 
/ 
m—1
n
(0i,..., 9n) G [0, l] n with ^ 
5fc = 1 and (an,..., xn) G [0, oo)n.
m=l
(b) Let {-FJV}I° be a non-decreasing exhaustion of § by finite sets satisfying
P a r t (a) t o s e e t n a t
for each TV, and get the asserted result after letting N —> oo.
(c) As an application of (5.6.3), show that, for any 0 < p < q < oo and

140 
5 REVERSIBLE MARKOV PROCESSES
EXERCISE 5.6.4. Gronwall's is an inequality which has many forms, the most
elementary of which states that if u : [0,T] —> [0, oo) is a continuous function
which satisfies
u(t) < A + B 
U(T) dr 
forte[0,T],
Jo
then u(t) < AeBt for t G [0,T]. Prove this form of Gronwall's inequality.
Hint: Set Uit) = /0* U(T) dr, show that U(t) < A + BU(t), and conclude that
EXERCISE 5.6.5. Define e as in Theorem 5.4.11, and show that e = —m if and
only if for each (i,j) € S x So there is an allowable path (jo,..., in) starting
at i and ending at j along which H is non-increasing. That is, io = i, in = j ,
and, for each 1 < m < n, Aim__lim 
> 0 and H(im) < i?(zTO_i).
EXERCISE 5.6.6. This exercise deals with the material in §5.1.3 and demon-
strates that, with the exception of (5.1.10), more or less everything in that
section extends to general irreducible, positive recurrent P's, whether or not
they are reversible. Again let TT = ns be the unique P-stationary probabil-
ity vector. In addition, for the exercise which follows, it will be important
to consider the space L2(TT;C) consisting of those / : S —> C for which
l/ieiV).
(a) Define P T as in (5.1.2), show that 1 > (PTP)ii = (TT), J2J€§ ^ | , and
conclude that the series in the definition P/(i) = Sjes /(j)(P)ij is absolutely
convergent for each % G S and / € L2(n; C).
(b) Show that ||P/||2,,r < \\fh,n for all / £ L2(TT;C), and conclude that
(5.1.8) and (5.1.9) extend to the present setting for all / 6 £2(TT; C).
EXERCISE 5.6.7. Continuing with the program initiated in Exercise 5.6.6, we
will now see that reversibility plays only a minor role in §5.1.4. Thus, let P
be any irreducible transition probability on S which is positive recurrent, let
d be its period, and set Qd = e v^ 2' r d 
.
(a) Show that for each 0 < m < d there is a function fm : S —> C with the
properties that \fm\ = 1 and P / m = 
9fjm.
Hint: Choose a cyclic decomposition (So, • • • ,§d-i) as in §3.2.7, and use
(3.2.19).
(b) Given a G R \ {0}, set 9a = e^12™"1, 
and show that there exists
an / € L2(TT; C) \ {0} satisfying P / = 9af if and only if d = ma for some
m' G 1 \ {0}.
Hint: By part (a), it suffices to show that no / exists unless d = ma for
some non-zero m G Z. Thus, suppose that / exists for some a which is not
a rational number of the form ^ , choose i G S so that f(i) ^ 0, and get a
contradiction with the fact that lim^^co ~Pndf(i) exists.

5.6 Exercises 
141
(c) Suppose that / E £2(S;C) is a non-trivial, bounded solution to P / =
6™f for some m E Z, and let (Eo,..., Sd_i) be a cyclic decomposition of S.
Show that, for each 0 < r < d, f \ Sr = 9r
d
mc0, where c0 E C \ {0}. In
particular, up to a multiplicative constant, for each integer 0 < m < d there
is exactly one non-trivial, bounded / E L2(S;C) satisfying P / = 0™f.
(d) Let H denote the subspace of / E L2(7r;C) satisfying P / = Of for
some 9 E C with |0| = 1. By combining parts (b) and (c), show that d is the
dimension of H as a vector space over C.
(e) Assume that (P)^- > 0 ==» (P)^ > 0 for all (i,j) E §2. Show that
d < 2 and that d = 2 if and only if there is a non-trivial, bounded / : S —> R
satisfying P / = —/. Thus, this is the only property of reversible transition
probability matrices of which we made essential use in Theorem 5.1.14.
EXERCISE 5.6.8. This exercise provides another way to think about the re-
lationship between non-negative definiteness and aperiodicity. Namely, let P
be a not necessarily irreducible transition probability matrix on S, and as-
sume that /j, is a probability vector for which the detailed balance condition
(n)i(P)ij = (/i)j(P)jj, (i,j) E S2 holds. Further, assume that P is non-
negative definite in L2(ju): (/,P/)M > 0 for all bounded / : S —> K. Show
that (n)i > 0 ^=> (P)u > 0 and therefore that i is aperiodic if (/x)j > 0.
What follows are steps which lead to this conclusion.
(a) Define the matrix A so that (A)^- = (l^jPl^-j)^, and show that
A is symmetric (i.e., (A)^- = (A)j-j) and non-negative definite in the sense
that 2^«(A)ij(x)i(x).7 > 0 for any x £ l s with only a finite number of non-
vanishing entries.
(b) Given i ^ j , consider the plane {alj + /?lj : a, j3 E R} in 
L2(TV),
and, using the argument with which we derived (5.1.5), show that
(A)ii(A)jj. In particular, if (A)a = 0, then (A)^ = 0 for all j £ S.
(c) Complete the proof by noting that ~^2j
(d) After examining the argument, show that we did not need P to be non-
negative definite but only that, for a given ?£§, each of the 2x2 submatrices
be.
EXERCISE 5.6.9. Let P be an irreducible, positive recurrent transition prob-
ability matrix with stationary distribution TT. Refer to (5.1.2), and show
that the first construction in (5.1.3) is again irreducible whereas the second
one need not be. Also, show that the period of the first construction is never
greater than that of P and that the second construction is always non-negative
definite. Thus, if P T P is irreducible, it is necessarily aperiodic.

142 
5 REVERSIBLE MARKOV PROCESSES
EXERCISE 5.6.10. In many applications, the structure of a Markov chain is
determined by a finite, simple graph (S, E). To be precise, the state space S is
set of vertices of the graph and E is a symmetric subset of S2, the set of edges;
and simplicity means that there are no loops or double edges. Next, we say
that j is a nearest neighbor of i, denoted by j G A/"(i), if and only if (i,j) G E.
Throughout, we assume that the degree d(i) = #A/"(i) is positive for each
t e S . The transition probability P associated with (S, E) is determined so
that (P)ij = j?jy if j G Af(i) and 0 otherwise. The interested reader will find
more examples in [1].
(a) Show that P is irreducible if and only the graph is connected. That
is, if and only if to each pair (i,j) e S2 there corresponds a finite sequence
(i0, ...,in)e 
S n + 1 such that i = i0, j = in, and (im-i,im) 
€ E for 1 < m < n.
In addition, assuming irreducibility, show that the chain is aperiodic if and
only if the graph is not bipartite. That is, if and only if there is no non-empty
S' C S with the property that every edge connects a point in §' to one in
S\S'.
(b) Determine the probability vector n by (TT)J = 2#~E' a nd show that -K is
a reversible for P.
(c) Assuming that the graph is connected, choose a set V = {p(i, j) : (i,j) G
S2 \ D} of allowable paths, as in §5.2.2, and show that
(5.6.11) 
/?+<
H+ ~ 
D2L{V)B{V)'
where D = maxjgg d(i), L(V) is the maximal length (i.e., number of edges) of
the paths in V, and B(V), the bottleneck coefficient, is the maximal number
of paths p G V which cross over an edge e G E. Obviously, if one chooses
V to consist of geodesies (i.e., paths of minimal length connecting their end
points), then L{V) is just the diameter of (S,E), and, as such, is as small as
possible. On the other hand, because it may force there to be bad bottlenecks
(i.e., many paths traversing a given edge), choosing geodesies may not be the
optimal.
(d) Assuming that the graph is connected and not bipartite, choose V =
{p(i) : i G S} to be of set of allowable closed paths of odd length, and show
that
(5.6.12) 
/?_ > - 1 + DL{V)B(V)'
EXERCISE 5.6.13. Here is an example to which the considerations in Exercise
5.6.10 apply and give close to optimal results. Namely, let N > 2, and consider
the set S in the complex plain C consisting of the N roots of unity. Next,
take E be the collection of pairs of adjacent roots of unity. That is, pairs of
. 
y'—127r(m—1) 
^/ — 127rm .
the form (e 
« 
, e 
« 
). Finally, take P and TT accordingly, as in the
preceding.

5.6 Exercises 
143
(a) As an application of (c) in the preceding, show that /3+ < 1 — tN_1
(b) Assume that N is odd, and use (d) above to show that /3_ > — 1 + j ^ .
EXERCISE 5.6.14. In this exercise we will give a very cursory introduction to a
class of reversible Markov processes which provide somewhat naive mathemat-
ical models of certain physical systems. In the literature, these are often called,
for reasons which will be clear shortly, spin-flip systems, and they are among
the earliest examples of Glauber dynamics. Here the state space S = {—1,1}^
is to be thought of as the configuration space for a system of N particles, each
of which has "spin" +1 or —1. Because it is more conventional, we will use
u> = (a>i,..., wjy) or T] = (r/i,..., r?jv) to denote generic elements of S. Given
u) 6 § and 1 < k < N, Cjk will be the configuration obtained from u> by
"flipping" its fcth spin. That is, the Qjk = (UJI, ...,uik-i, 
—cJk,Wk+i, • • • ,<^N)-
Next, given M-valued functions / and g on S, define
N
T(/, g)(u) = ]T(/(£fc) - / M ) (g(u;k) - g(cj)),
fe=i
which is a discrete analog of the dot product of the gradient of / with the
gradient of g. Finally, given a probability vector /x with (/x)w > 0 for all
W G S , define
^ H 
if rj =
and
(a) Check that QM is an irreducible Q-matrix on S and that the detailed
balance condition (/x)(JQ^7? = (^vQriu 
n°lds. In addition, show that
-{g,<F 
!)=£»(!,g)-
(b) Let A be the uniform probability vector on S. That is, (A)w = 2~~N for
each oj G S. Show that
VarM (/) < MM VarA (/), 
where MM = 2N max ^
and
SX(fJ) 
< —S»(f,f) 
where m» = 2JvminMw.
(c) For each S C {1,.. .,N}, define xs • § —» {-1,1} so that Xs{u) =
Tikes^k- 
I n particular, %0 = 1- Show that {xs : 5 C {1,...,7V}} is an
orthonormal basis in L2(X) and that Qxxs = —2(#5')x5, and conclude from
these that VarA(/) < 
\£x(fJ).

144 
5 REVERSIBLE MARKOV PROCESSES
(d) By combining (b) with (c), show that /^Var^ < £**(/,/) where /3M =
-JJ^- In particular, if {P£* : t > 0} is the semigroup of transition probability
matrices determined by Q^, conclude that ||P^/ — (/)M||2/x < e~P>*\\f —
EXERCISE 5.6.15. Refer to parts (b) and (c) in Exercise 5.6.14. It is some-
what surprising that the spectral gap for the uniform probability A is 2, in-
dependent of N. In particular, this means that if {p(N\t) 
: t > 0} is the
semigroup determined by the Q-matrix
1 
if r) = u>
k
(Q ( J V )U=<| -N 
if»/ = u>
0 
otherwise
on {-1,1}", then | | P w ( t ) / - (/)AW||2IA(JV) < e^2t||/||2iA(M) for t > 0 and
/ G L2(A'JV^)), where A^"' is the uniform probability measure on { — 1,1}".
In fact, the situation here provides convincing evidence of that the theory
developed in this chapter works in situations where Doeblin's theory is doomed
to failure. Indeed, the purpose of this exercise is to prove that, for any t > 0
and LJ & {-1,1}", limjv^oo \\^N\t,u;) 
^ X^\\v = 2 when ( ^ ' ( t . w ) ) ^ =
(a) Begin by showing that \\(i(N\t,u>) — A^jlv is independent of w.
(b) Show that for any two probability vectors v and u1 on a countable space
§, 2 > || i/ - i/||v > 2\v{A) - u'(A)\ for all ACS.
(c) For 1 < k < N, define the random variable X^ on {—1,1}" so that
Xk{ri) = Tjk if V = (liT • • IVN)- Show that, under \(N\ them's are mutually
independent, {—1, l}-valued Bernoulli random variables with expectation 0.
Next, let t > 0 be given, and set //(") = /j,(N\t,u)), where u> is the element
of {—1,1} whose coordinates are all —1. Show that, under H^N\ the X^s
are mutually independent, {—1, l}-valued Bernoulli random variables with
expectation value -e~2t.
(d) Continuing in the setting of (c), let A^ be the set of i) for which
jfZiXkiv) 
< ~\e~2t, and show that ^ w»(#') - \(N\A^) 
> 1 -
^jj-- In fact, by using the sort of estimate developed at the end of §1.2.4,
especially (1.2.16), one can sharpen this and get
Hint: Use the usual Chebychev estimate with which the Weak Law is
proved.
(e) By combining the preceding, conclude that \\^N\t,u}) 
—
2(1 - ^-) for allt > 0, N G Z+, and u G {-1,1}".

CHAPTER 6
Some Mild Measure Theory
On Easter 1933, A.N. Kolmogorov published Foundations of Probability, a
book which set out the foundations on which most of probability theory has
rested ever since. Because Kolmogorov's model is given in terms of Lebesgue's
theory of measures and integration, its full appreciation requires a thorough
understanding that theory. Thus, although it is far too sketchy to provide
anything approaching a thorough understanding, this chapter is an attempt
to provide an introduction to Lebesgue's ideas and Kolmogorov's application
of them in his model of probability theory.
6.1 A Description of Lebesgue's Measure Theory
In this section, we will introduce the terminology used in Lebesgue's the-
ory. However, we will systematically avoid giving rigorous proofs of any hard
results. There are many places in which these proofs can be found, one of
them being [8].
6.1.1. Measure Spaces: The essential components in measure theory are
a set Q, the space, a collection T of subsets of Vt, the collection of measurable,
subsets, and a function /i from T into [0, oo], called the measure. Being a
space on which a measure might exist, the pair (fi, J-) is called a measurable,
space, and when a measurable space (fl, T~) comes equipped with a measure
fi, the triple (fl, T, \x) is called a measure space.
In order to avoid stupid trivialities, we will always assume that the space fl
is non-empty. Also, we will assume that the collection J- of measurable sets
forms a a-algebra over fi:
OO
and {An}^° C T => [JAn e T.
It is important to emphasize that, as distinguished from point-set topology
(i.e., the description of open and closed sets) only finite or countable set
theoretic operations are permitted in measure theory.

146 
6 SOME MILD MEASURE THEORY
Using elementary set theoretic manipulations, it is easy to check that
A, B eF => inBe/andB\ief
Finally, the measure /i will be function which assigns1 0 to 0 and is countably
additive in the sense that
{An)T Q F and Am n An = 0 when m^n
In particular, for A, B 6 !F,
ACB 
=> ix{B) = fi(A) + ii{B \ A) >
li(Ar\B)<oo => fi(AuB) = n(A) + II(B) -/j,( An B).
The first line comes from writing B as the union of the disjoint sets A and
B \ A, and the second line comes from writing A U B as the union of the
disjoint sets A and B \ (A n B) and then applying the first line to B and
AnB. 
The fimteness condition is needed when one moves the term /i(An-B)
to the left hand side in fi(B) = fj,(AnB) +/j.(B\(AnB)). 
That is, one wants
to avoid having to subtract oo from oo.
When the set ft is finite or countable, there is no problem constructing
measure spaces. Namely, one can take T = {A : A C fi}, the set of all
subsets of fl, make any assignment of w e fi i—> /I({UJ}) € [0, oo], at which
point countable additivity demands that we take
Y^ KM) for AQQ.
However, when Q is uncountable, it is far from obvious that interesting mea-
sures can be constructed on a non-trivial collection of measurable sets. Indeed,
it is reasonable to think that Lebesgue's most significant achievement was his
construction of a measure space in which fl = K, T is a cr-algebra of which
every interval (open, closed, or semi-closed) is an element, and \x assigns each
interval its length (i.e., fj,(I) = b — a if I is an interval whose right and left
end points are 6 and a).
Although the terminology is misleading, a measure space (w, J7, fi) is said
to be finite if fj,(Q.) < oo. That is, the "finiteness" here is not determined by
1 In view of additivity, it is clear that either /J(0) = 0 or n(A) = oo for all A 6 T. Indeed,
by additivity, /u(0) = /i(0 U 0) = 2/K(0), and therefore /i(0) is either 0 or oo. Moreover, if
= oo, then n(A) = n(A U 0) = fi(A) + p(0) = oo for all A e f .

6.1 A Description of Lebesgue's Measure Theory 
147
the size of Q. but instead by how large Q, looks to fj,. Even if a measure space is
not finite, it may be decomposable into a countable number of finite pieces, in
which case it is said to be a-finite. Equivalently, (fi, T, n) is cr-fmite if there
exists {£ln}T ^ F s u c n that2 ttn / Q, and fj,(£ln) < oo for each n > 1. Thus,
for example, both Lebesgue's measure on K. and counting measure on Z are
cr-finite but not finite.
6.1.2. Some Consequences of Countable Additivity: Countable addi-
tivity is the sine qua non in this subject. In particular, it leads to the following
continuity properties of measures:
i 
{An}™ C T and An / A 
=
j 
{A}? 
C f, ^{Ay) < oo, and An \ A
Although we do not intend to prove many of the results discussed in this
chapter, the proofs of those in (6.1.3) are too basic and easy to omit. Indeed,
to prove the first line, simply take B\ = A\ and Bn+\ = An+\ \ An. Then
Bm n Bn = 0 when m ^ n, \J" Bm = An for all n > 1, and U r B™ = A.
Hence, by (6.1.1),
To prove the second line, begin by noting that fi(Ai) = fJ,(An) + fi(Ai \ An)
and fi(Ai) = fi(A) + fi(A1\A). 
Hence, since Ai\An 
/ A\\A and>(Ai) < oo,
we have n{A{) — n{An) y n{A{) — /i(A) and therefore that /J,(An) \ 
JJL{A).
Just as in the proof of second line in (6.1.2), we need the fmiteness condition
here to avoid being forced to subtract oo from oo.
Another important consequence of countable additivity is countable subad-
ditivity:
Like the preceding, this is easy. Namely, if B\ = A\ and Bn+i = An+i \
Ui Am, then /i(Bn) < ^{An) and
A particularly important consequence of (6.1.4) is the fact that
/oo 
\
(6.1.5) 
(j, M J An 
= 0 if fj,(An) = 0 for each 
n>\.
2 We write An f A when An C An+i for all n > 1 and A = IJ?0 An. Similarly, An \ ^4
means that An D An_|_i for all n > 1 and /I = p|?° An. Obviously, An /" A if and only if
AnC \ AC.

148 
6 SOME MILD MEASURE THEORY
That is, the countable union of sets each of which has measure 0 is again a set
having measure 0. Here one begins to see the reason for restricting oneself to
countable operations in measure theory. Namely, it is certainly not true that
the uncountable union of sets having measure 0 will necessarily have measure
0. For example, in the case of Lebesgue's measure on R, the second line of
(6.1.3) implies
ii({x}) = lim u((x -6,x + 5)) = lim 26 = 0
s\o 
y 
' <5\o
for each point x £ R, and yet (0,1) = Uzefo i)ix} n a s m e a s u r e 1-
6.1.3. Generating cr-Algebras: Very often one wants to make sure that
a certain collection of subsets will be among the measurable subsets, and for
this reason it is important to know the following constructions. First, suppose
that C is a collection of subsets of O. Then there is a smallest <r-algebra cr(C),
called the cr-algebra generated byC, over £7 which contains C. Namely, consider
the collection of all the cr-algebras over Cl which contain C. This collection
is non-empty because {^4 : A C £2} is an element. In addition, as is easily
verified, the intersection of any collection of u-algebras is again a cr-algebra.
Hence, cr(C) is the intersection of all the cr-algebras which contain C. When
fl is a topological space and C is the collection of all open subsets of £2, then
cr(C) is called the Borel a-algebra and is denoted by BQ..
One the most important reasons for knowing how a cr-algebra is generated
is that one can often check properties of measures on cr(C) by making sure
that the property holds on C. An important example of such a result is the
one in the following uniqueness theorem.
6.1.6 THEOREM. 
Suppose that (£2, JF) is a measurable space and that C C T
includes £2 and is closed under intersection (i.e., AC\B s C whenever A,B £ C).
If fi and v are a pair of Unite measures on (£2, T) and if /j,(A) — v(A) for each
AeC, 
then n(A) = v(A) for all A e a(C).
PROOF: We will say that S C T is good if
(i) 
A,B eS a,nd AC B 
=> 
B\AeS.
(ii) 
A,B eS and AnB = 9 => AuB eS.
(iii) 
{An}f C5 wdAn/ 
A =^ 
AeS.
Notice that if S is good, D e 5 , and A,BeS 
= > An B £ S, then S is a
cr-algebra. Indeed, because of (i) and (iii), all that one has to do is check that
AUB £ S whenever A,B £ S. But, because AllB = 
(A\(AnB))uB,
this is clear from (i), (ii), and the fact that S is closed under intersections. In
addition, observe that if S is good, then, for any V C JF,
5' = {A £ S : A n B £ S for all B £ V}
is again good.

6.1 A Description of Lebesgue's Measure Theory 
149
Now set B = {A G T : jd{A) = is (A)}. 
From the properties of fi-
nite measures, in particular, (6.1.2) and (6.1.3), it is easy to check that
B is good. Moreover, by assumption, C C B. Thus, if B' = {A G B :
A C\ C G B for all C G C}, then, by the preceding observation, B' is again
good. In addition, because C is closed under intersection, C C £>'. Simi-
larly, B" = {A G B' : AnB 
e B' for all B e B'} is also good, and, by
the definition of B', C C B". Finally, if A, A' G B" and B G B', then
( i n A ' ) f l 5 = A n ( i ' n B ) e B', and so An A' G B". Hence, B" is a
cr-algebra which contains C, B" C B, and therefore // equals i/ on cr(C). D
6.1.4. Measurable Functions: Given a pair (fii,.Fi) and (^2,-^2) of mea-
surable spaces, we will say that the map F : Qi —> Q2 is measurable if the
inverse image of sets in T2 are elements of .Fi: F - 1(r) G T\ for every F G J-^2-3
Notice that if T2 = CT(C), F is measurable if F'^C) G J"i for each C & C.
In particular, if fli and JI2 are topological spaces and J-i = B^i, then every
continuous map from Qi to Q2 is measurable.
It is important to know that when Q.% = K. and Ti = BR, measurability
is preserved under sequential limit operations. To be precise, if {/n}i° is a
sequence of E-valued measurable functions from (fl,J-) to (K, BH), then
(6.1.7) 
" 
are measurable.
w—> lim /n(w), 
and UJ~* hm /n(w)
For example, the first of these can be proved by the following line of reasoning.
Begin with the observation that BR = a{C) when C = {(a, oo) : a € K}. Hence
/ : ft —> K will be measurable if and only if4 {/ > a} for each a e l , and so
the first function in (6.1.7) is measurable because
SUP fn > a \ = I \{fn > a} f°r every a G K.
As a consequence the second line in (6.1.7), we know that the set A of points u>
at which the limit limn^co /ra(u;) exists is measurable, and LU~^ lirrin^oo fn(u>)
is measurable if A = £1.
Finally, measurable functions give rise to important instances of the con-
struction in the preceding subsection. Namely, suppose that the space Qi
and the measurable space (f^;-^) are given, and let $ be some collection of
maps from $7i into ^2- Then the a-algebra cr($) over Qi generated by $ is the
smallest cr-algebra over Q\ with respect to which every element of $ is mea-
surable. Equivalent!^ cr($) = a(C) when C = {-F"1^) : F G # & F G T2}.
3 The reader should notice the striking similarity between this definition and the one for
continuity in terms of inverse images of open sets.
4 When there is no ambiguity caused by doing so, we use {F G F} to stand for {ai : F(u>) €
r}.

150 
6 SOME MILD MEASURE THEORY
Finally, suppose that J-2 = cr(C2), where Ci C T2 contains Q2 and is closed
under intersection (i.e., A n B E Ci if A, B E C2), and let Ci be the collection
of sets of the form F f 1 ^ ) n • • • D i^CAx) for n G Z+, {Fx,..., Fn} C £,
and {Ai,.. .,An} Q C2. Then a($) = cr(Ci), Qi e Ci, and Ci is closed under
intersection. In particular, by Theorem 6.1.6, if /i and v are a pair of finite
measures on (fii,.Fi) and
G ^i,...,F n(wi) G An})
= V({LOI : F^wi) eA1,...,Fn(ui) 
e An})
for all n e Z +, {Fi,..., Fn} C ^ and {/ii,..., An} C C2, then /i equals v on
6.1.5. Lebesgue Integration: Given a measure space (£l,J-,/j,), Lebes-
gue's theory of integration begins by giving a prescription for defining the
integral with respect to /J of all non-negative, measurable functions (i.e., all
measurable maps / from (f2, T) into5 ([0, 00]; £?[0iOOj). Namely, his theory says
that when 1^ is the indicator function of a set A & T and a G [0, 00), then
the integral of the function6 O\.A should be equal to o/x(A). He then insists
that the integral should be additive in the sense that the integral of /1 + $2
should be sum of the integrals of f\ and fi- In particular, this means that if
/ is a non-negative, measurable function which is simple, in the sense that it
takes on only a finite number of values, then the integral J f d/i of / must be
where, because /~1({a:}) = 0 for all but a finite number of x, the sum involves
only a finite number of non-zero terms. Of course, before one can insist on this
additivity of the integral, one is obliged to check that additivity is consistent.
Specifically, it is necessary to show that J^" amn(Am) 
= Y^i a'm'M-^m') when
Y^l o-mXAm — f = Xa a'm}-A' , • However, once this consistency has been
established, one knows how to define the integral of any non-negative, mea-
surable, simple function in such a way that the integral is additive and gives
the obvious answer for indicator functions. In particular, additivity implies
monotonicity: J f dv < J g d/j, when / < g.
To complete Lebesgue's program for non-negative functions, one has to first
observe that if / : E —> [0,00] is measurable, then there exists a sequence
{VnJT of non-negative, measurable, simple functions with the property that
(pn(to) y f(u>) for each u> G fl. For example, one can take
Vn=y] 
^Um 
„ where Am,n = {w : m2~n < f(u) < (m + 1)2""}.
5 In this context, we are thinking of [0, oo] as the compact metric space obtained by mapping
[0,1] onto [0,oo] via the map t £ [0,1] i—> tan(f t).
6 In measure theory, the convention which works best is to take Ooo = 0.

6.1 A Description of Lebesgue's Measure Theory 
151
Given {<yJra}i°, what Lebesgue says is that J f dfi = lim-r^oo J Lpn dfj. Indeed,
by monotonicity, J ipn dfi is non-decreasing in n, and therefore the indicated
limit necessarily exists. On the other hand, just as there was earlier, there is
a consistency problem which we must resolve before we can adopt Lebesgue's
definition. This time the problem comes from the fact that there are myr-
iad ways in which to construct the approximating simple functions <pn, and
one must make sure that the limit does not depend on which approximation
scheme one chooses. That is, it is necessary to check that if {</?n}i° and
{ipn}T a r e two non-decreasing sequences of non-negative, simple, measurable
functions such that lirrijj^oo <pn(u)) = limn^oo ipn(OJ) for each w e fi, then
limji-,00 J (pn dfi = limn^oo f ipn dfj; and it is at this step that the full power
of countable additivity must be brought to bear.
Having denned f f dfi for all non-negative, measurable /'s, one must check
that the resulting integral is homogeneous and additive: J af dfj = a f f dfj
for a e [0,oo] and /(/1 + f2) dfi = J fidfi + J f2dfi. However, both these
properties are easily seen to be inherited from the case of simple /'s. Thus, the
only remaining challenge in Lebesgue's construction is to get away from the
restriction to non-negative functions and extend the integral to measurable
functions which can take both signs. On the other hand, if one wants the
resulting theory to be linear, then there is no doubt about how this extension
must be made. Namely, given a measurable function / : E —> [—00, 00], it is
not hard to show that / + ~ /VO and /"" = —(/AO) = (—/)+ are non-negative
measurable functions. Hence, because f = f+ — f~, linearity demands that
J f dfi = J f+ dfi — J f~ dfi; and this time there are two problems which
have to be confronted. In the first place, at the very least, it is necessary to
restrict ones attention to functions / for which at least one of the numbers
/ / + dfj or J f~ dfi is finite, otherwise one ends up having to deal with 00 — 00.
Secondly, one must, once again, check consistency. That is, if / = /1 — /2,
where f\ and fi are non-negative and measurable, then one has to show that
/ fidfi- 
J f2dfi = J f+dfj- 
f f~ dfi.
In most applications, the measurable functions which one integrates are
either non-negative or have the property that J \f\dfi < 00, in which case / is
said to be an integrable function. Because |«i/i+«2/2| < |<2i||/i| + |fl2||/2|, the
set of integrable functions forms a vector space over M. on which integration
with respect to fi acts as a linear function. Finally, if / is a measurable function
which is either non-negative or integrable and A e J7, then the product 1^/
is again a measurable function which is either non-negative or integrable and
so
(6.1.8)
is well defined.
6.1.6. 
Stability Properties of Lebesgue Integration: The power of
Lebesgue's theory derives from the stability of the integral it defines, and its

152 
6 SOME MILD MEASURE THEORY
stability is made manifest in the following three famous theorems. Through-
out, (ft, T, JJL) is a measure space to which all references about measurability
and integration refer. Also, the functions here are assumed to take their values
in (—00, 00].
6.1.9 THEOREM. (Monotone Convergence) Suppose that {fn}f is a non-
decreasing sequence of measurable functions, and, for each us 6 ft, set /(w) =
limn^oo fn(u>). If there exists a fixed integrable function g which is dominated
by each fn, then f fndp, /* J f d/j,. If, instead, {/n}i° is non-increasing and
if there exists a fixed integrable function g which dominates each fn, then
Jfndn\Jf dfi.
6.1.10 THEOREM. (Fatou's Lemmas) Given any sequence {/n}i° of mea-
surable functions, all of which dominate some fixed integrable function g,
f 
f
hm 
fndu.> 
lim /„
n^oo J 
J n—xx)
djl.
If, instead, there is some fixed integrable function g which dominates all of
the fn 's, then
f 
f
lim 
fndfj, < / lim fn do..
6.1.11 THEOREM. (Lebesgue's Dominated Convergence) Suppose that
{/n}f° JS a sequence of measurable functions, and assume that there is a fixed
integrable function g with the property that fj,({uj : \fn(u))\ > g(cj)}) = 0 for
each n > 1. Further, assume that there exists a measurable function f to
which {fn} converges in either one of the following two senses:
(a) 
/i (^|w : /(w) ^ JKm^a;) j j = 0
(b) 
lim LL{{UJ: | / n M -/(w)| > e)) = 0 for aii e > 0.
ra—IOO
Then
/
/" 
/*
fndii— 
If du < I \fn — f\du —> 0 as n —> oo.
Before proceeding, a word should be said about the conditions in Lebes-
gue's Dominated Convergence Theorem. Because integrals cannot "see" what
is happening on a set of measure 0 (i.e., an A e T for which n(A) = 0)
it is natural that conditions which guarantee certain behavior of integrals
should be conditions which need only hold on the complement of a set of
measure 0. Thus, condition (a) says that fn(y>) —> /( w) for all u> outside
of a set of measure 0. In the jargon, conditions which hold off of a set of
measure 0 are said to hold almost everywhere. In this terminology, the first
hypothesis is that |/ n| < g almost everywhere and (a) is saying that {/n}f°
converges to / almost everywhere, and these statements would be abbreviated

6.1 A Description of Lebesgue's Measure Theory 
153
in the literature by something like \fn\ < g a.e. and /„ —> / a.e.. The
condition in (b) is related to, but significantly different from, the one in (a).
In particular, it does not guarantee that {fn{u)}f 
converges for any w G Q.
For example, take fj, to be the measure of Lebesgue on K. described above,
and take / m + 2 « H = l[0j2-»)(w - m2~n) for n > 0 and 0 < m < 2™. Then
M({^ : /m+2»M 7^ 0}) = 2" n, and so lim^oo (J,({CJJ_ 1/nHI > e}) = 0
for all e > 0. On the other hand, for each to G [0,1), limra_^oo fn(uj) — 1 but
liSln^oo / n H = 0- Thus, (b) most definitely does not imply (a). Conversely,
although (a) implies (b) when /z(f2) < oo, (a) does not imply (b) when /u(D) =
oo. To wit, again when fj, is Lebesgue's measure, and consider fn = lR\[_n>n].
In connection with the preceding discussion, there is a basic estimate, known
as Markov's inequality, which plays a central role in all measure theoretic
analysis. Namely, because, for any A > 0, Al[^jOO] o / < /1[A,OO] ° / < |/|,
(6.1.12) 
n({u: /(w)>A})<| / 
fdn<\ 
f\f\diM.
A J{uj-.f(ui)>\} 
A J
In particular, this leads to the conclusion that
\fn - f\ dfi = 0 = > M({c : |/n(w) - / H | > e}) = 0
for all e > 0. That is, the condition (b) is necessary for the conclusion in
Lebesgue's theorem. In addition, (6.1.12) proves that
[\f\dn 
= O => ^{{w : I / H I > e}) = 0 for all e > 0,
and therefore, by (6.1.3),
(6.1.13) 
J\f\dn 
= O =» fi({cu : / H + 0}) = 0.
Finally, the role of the Lebesgue dominant g is made clear by considering
{/n}i° when either /„ = nl[0 ±^ or fn = l[n_in) and /J, is Lebesgue's measure.
6.1.7. Lebesgue Integration in Countable Spaces: In this subsection
we will see what Lebesgue's theory looks like in the relatively trivial case
when Q, is countable, J- is the collection of all subsets of Q, and the measure
\i is (7-finite. As we pointed out in §6.1.1, specifying a measure on (ft,^) is
tantamount to assigning a non-negative number to each element of fl, and,
because we want our measures to be cr-fmite, no element is assigned oo.
Because the elements of Q can be counted, there is no reason to not count
them. Thus, in the case when S7 is finite, there is no loss in generality if we
write Q, = {1,..., JV}, where N — #£2 is the number of elements in O, and,
similarly, when Q, is countably infinite, we might as well, at least for abstract
purposes, think of its being the set Z + of positive integers. In fact, in order to

154 
6 SOME MILD MEASURE THEORY
avoid having to worry about the finite and countably infinite cases separately,
we will embed the finite case into the infinite one by simply noting that the
theory for {1,..., N} is exactly the same as the theory for Z + restricted to
measures \JL for which fj,({uj}) = 0 when u> > N. Finally, in order to make the
notation here conform with the notation in the rest of the book, we will use
§ in place of ft, i, j , or k to denote generic elements of S, and we will identify
a measure \i with the row vector \i 6 [0, oo)s given by (/x)j = /x({i}).
The first thing to observe is that
(6.1.14) 
f
whenever either / /+ d\i or / /~ d/j, < oo is finite. Indeed, if ip > 0 is simple,
a\,...,a,L 
are the distinct values / takes, and At = {i : ip(i) = at}, then
L
e=l i€A€ 
i€§
Second, given any / > 0 on §, set (fn(i) = f(i) if 1 < i < n and <pn(i) = 0 if
% > n. Then,
= lim
n—>oo \<i<n
fd/i= 
lim / ip
Finally, if either j f+ d/j, or J f~ djj, is finite, then it is clear that
E
We next want to see what the "big three" look like in this context.
The Monotone Convergence Theorem: First observe that it suffices to treat
the case when 0 < fn f f. Indeed, one can reduce each of these statements
to that case by replacing /„ with fn — g or g — fn. When 0 < / „ / " / , it is
obvious that
0 < E fn 
" 
"~^
Thus, all that remains is to note that
L
lim y^/«(*)(/*)« 
— ^ m / ^
n—^oo ^-—' 
n—nx) -^—'
for each L G Z+, and therefore that the desired result follows after one lets
L f oo.

6.1 A Description of Lebesgue's Measure Theory 
155
Fatou's Lemma: Again one can reduce to the case when fn > 0 and the
limit being taken is the limit inferior. But in this case,
> lim } 
inf /n(i)(u)« = > lim /„
where the last equality follows from the Monotone Convergence Theorem ap-
plied to 0 < infn>m /„ / l i m ^ ^ /„.
Lebesgue's Dominated Convergence Theorem: First note that, if we elimi-
nate those i G S for which (/z), = 0, none of the conclusions change. Thus, we
will, from now on assume that (/LI)J > 0 for all i G S. Next observe that, under
this assumption, the hypotheses become supn |/n(i)| < g(i) and fn(i) —> f(i)
for each i G S. In particular, |/| < g. Hence, by considering fn — f instead of
{/n}i° an<i replacing g by 2g, we may and will assume that / = 0. Now let
e > 0 be given, and choose L so that 2 J > L 5r(*)(/Lt)i < e- Then
', \fn{i)\(n)i < Y] \fn{i)\{v)i + e —> e as n -^ oo.
6.1.8. Fubini's Theorem: Fubini's Theorem deals with products of mea-
sure spaces. Namely, given measurable spaces (fii,^) and (02,^2); the
product of T\ and Ti is the a-algebra T\ x T2 over Qi x JI2 which is gen-
erated by the set {A\ x Ai : A\ G T\ h, A2 G T-i\ of measurable rectangles.
An important technical fact about this construction is that, if / is a measur-
able function on (fii x O2,J"i x ^2), then, for each u)\ £ Qi and u>2 € O2,
6^2*^/(^1)^2) and wi~»/(ti;i,CL>2) are measurable functions on, respectively,
In the following statement, it is important to emphasize exactly which vari-
able is being integrated. For this reason, we use the more detailed notation
JQ f(u>) n(dui) instead of the more abbreviated J f dfj,.
6.1.15 THEOREM. (Fubini)7 Let (Oi,.Fi,/zi) and (^2,^2,^2) be a pair of
a-finite measure spaces, and set Q = Qj x Q2 and T = T\ x T2- Then
there is a unique measure [i = /J,I x /i2 on (il,!F) with the property that
ji{A\ x A2) = ni(Ai)fi2(A2) for all A\ e Tx and A2 G T2. Moreover, if f is a
non-negative, measurable function on (f2,^r), then both
f
wi-^ / 
f(u>i,u)2) I22(du)2) 
and 
u>2-
JQ.2
7 Although this theorem is usually attributed Fubini, it seems that Tonelli deserves, but
seldom receives, a good deal of credit for it.

156 
6 SOME MILD MEASURE THEORY
are measurable functions, and
= / 
fd/j=
Ju 
Jn2
Finally, for any integrable function f on
oo
and
are integrable, and
f 
f 
f
/ 
/i(wi)/ii(dwi) = / f(u)(j,((L))= 
/
JUi 
JO. 
Jo,2
In the case when fii and f22 are countable, all this becomes very easy.
Namely, in the notation which we used in §6.1.6, the measure /J,I x /j2 cor-
responds to row vector /xi x /j,2 G [0,oo)SlxS2 given by (//1 x fJ-2)(ilti2) =
(/Li)i1(/it)i2, and so, by (6.1.14), Fubini's Theorem reduces to the statement
that
when {OJJ^ : (?i,i2) G Si x S2} C (—00,00] satisfies either a^i2 > 0 for all
(ii,i2) or X)(tx j2) la«i*2l < °°- I11 proving this, we may and will assume that
Si = Z + = §2 throughout and will start with the case when a^^ > 0. Given
any pair (ni,n2) G (Z +) 2,
)
ii<ni & J2<«2
Hence, by first letting n\ —> 00 and then letting n2 —> 00, we arrive at

6.2 Modeling Probability 
157
Similarly, for any n G Z+,
n 
/ n 
\
= £ £*i« p £
»2 = 1 \ l l = l 
/ 
i2€S!
i2G§2 
/
and so, after letting n —> oo, we get the opposite inequality. Next, when
< oo for all 12 € §2,
and so
E <* -
(ii,i2)eSixS2 
(ii,i2)£SixS2
a*l*2 
/ 
j 
"*1«2 
/ 
y
Finally, after reversing the roles 1 and 2, we get the relation with the order
of summation reversed.
6.2 Modeling Probability
To understand how these considerations relate to probability theory, think
about the problem of modeling the tosses of a fair coin. When the game ends
after the nth toss, a Kolmogorov model is provided by taking fi = {0, l} n, T
the set of all subsets of Q, and setting /x({u;}) = 2~n for each u € fl. More
generally, any measure space in which il has total measure 1 can be thought
of as a model of probability, for which reason, such a measure space is called a
probability space, and the measure \i is called a probability measure and is often
denoted by P. In this connection, when dealing with probability spaces, ones
intuition is aided by extending the metaphor to other objects. Namely, one
calls fi the sample space, its elements are called sample points, the elements
of T are called events, and the number that P assigns an event is called the
probability of that event. In addition, a measurable map is called a random
variable, it tends to be denoted by X instead of F, and, when it is R-valued, its
integral is called its expected value. Moreover, the latter convention is reflected
in the use of E[X], or, when more precision is required, EP[X] to denote
JXdV. Also, E[X, A] or EP[X, A] is used to denote JAXdF, the expected
value of X on the event A. Finally, the distribution of a random variable whose
values lie in the measurable space (E,B) is the probability measure X*P on

158 
6 SOME MILD MEASURE THEORY
(E,B) given by fi(B) = ¥(X e B). In particular, when X is R-valued, its
distribution function Fx is defined so that Fx (x) — (X*P)((—00,3;]) = P(X <
x). Obviously, Fx is non-increasing. Moreover, as an application of (6.1.3),
one can show that Fx is continuous from the right, in the sense that Fx{x) =
limy\^ Fx(y) for each x e l , lim^-co Fx(x) = 0, and lim^oc, Fx{x) = 1.
At the same time, one sees that F(X < x) = Fx(x-) 
= lim,,/^ Fx(y), and,
as a consequence, that ¥(X = x) = Fx(x) — Fx(x-) 
is the jump in Fx at x.
6.2.1. Modeling Infinitely Many Tosses of a Fair Coin: Just as in
the general theory of measure spaces, the construction of probability spaces
presents no analytic (as opposed to combinatorial) problems as long as the
sample space is finite or countable. The only change from the general theory
is that the assignment of probabilities to the sample points must satisfy the
condition that J^wen ^({w}) = 1- The technical analytic problems arise when
ft is uncountable. For example, suppose that, instead of stopping the game
after n tosses, one thinks about a coin tossing of indefinite duration. Clearly,
the sample space will now be ft = {0,1}Z+. In addition, when A C ft depends
only on tosses 1 through n, then A should be a measurable event and the
probability assigned to A should be the same as the probability that would
have been assigned had the game stopped after the nth toss. That is, if
T C {0, l } n and8 A = {iv e ft : (w(l),..., to(n)) e T}, then ¥(A) should
equal 2~ n#r.
Continuing with the example of an infinite coin tossing game, one sees (cf.
(6.1.3)) that, for any fixed r] G fl, ¥({r]}) is equal to
Jam^jcc; : (w(l),..., w(n)) = (r?(l),..., V(n)) }) = Jm^ 2"n = 0.
Hence, in this case, nothing is learned from the way in which probability is
assigned to points: every sample points has probability 0. In fact, it is far
from obvious that there exists a probability measure on Q, with the required
properties. Nonetheless, as we will now prove such a measure does exist.
To get started, for each n > 1, let IIn : ft —> £tn = {0, l } n be the projection
map given by w~*(w(l),..., w(n)), and set
Equivalently, A e An if and only if A depends on the first n coordinates, in
the sense that: ui G A and IIra(w') = IIn(w) => w' G A. Next, define Pra on
An so that Fn(A) = 2"™#nn(^). Clearly An C An+i. In fact, if A e An,
then nn+1(A) = (n n(^) x {0,1})), and so Pn+1(^) = Pn(A). Hence, we can
unambiguously define P on A = \Jn°=1 An so that F(A) = Fn(A) when A e An.
Moreover, if A, A' & A are disjoint, then, by choosing n so that A, A' G An,
we see that ¥{A U A') = Pn(A U A') = ¥n{A) + ¥n{A') = ¥{A) + ¥(A').
8 It is convenient here to identify Q with the set a mappings ui from Z + into {0,1}. Thus,
we will use w{n) to denote the "nth coordinate" of w.

6.2 Modeling Probability 
159
Before taking the next step, it will be convenient to introduce a metric on
for which all elements of A are open sets. Namely, we take
n=l
It is an easy matter to check that p is a metric Q. In fact, it is a metric for
which the notion of convergence corresponds to convergence of each coordi-
nate. In addition, if u> G A G An, then p(u>',u>) < 2~~n ==> u/ G A Hence,
each A e A is open relative to topology induced by p. At the same time,
each A € A is also closed in p-topology. Indeed, if A G An, {wfc}f° C A, and
p(wfc,w) —> 0, then there is a fc G Z + for which p(u>,u>k) < 2~n, and so, for
this k, Hn(cj) = Un(uik). Another fact which we will need is that Q is compact
in the p-topology. To see this, suppose that {wfc}^ is a sequence in Q. To
produce a convergent subsequence, we employ a diagonalization procedure.
That is, because, for each m, either u>k{m) = 0 for infinitely many k G Z +
or Wfc(m) = 1 for infinitely many k G Z +, we can use induction to construct
{km,e • {m, I) G (Z+)2} so that {kite : £ G Z +} is an increasing enumeration of
{fc : Wfe(l) = 0} or of {fc : Wfe(l) = 1} depending on whether Wfc(l) = 0 for in-
finitely or finitely many fc's, and, when m > 1, {/cm/ : £ G Z +} is an increasing
enumeration of {km-ite 
: ojkm_1 e(m) = 0} or of {km-\ti 
: Wfcm_1 £(m) = 1}
depending on whether oj/.m_1 e(m) = 0 for infinitely or finitely many £'s. Now
set ki = ki}£, and check that {w,k£ : t G Z +} a convergent subsequence of
{wfc}f°. Finally, there is another property which we will need to know about.
Namely, for each open set G C SI there is a sequence {An}f° C A of mutually
disjoint sets such that Am G Am and G = \J^° Am. To produce {Am}^° one
can proceed as follows. Choose A\ to be the largest element of A G A\ with
the property that A C G. Equivalently, A\ is the union of all the A E Ai
contained in G. Next, given Aj for 1 < £ < m, choose Am+\ 
to the largest
A G Am+i contained in G \ 1J™ Am. Obviously, these A^'s are mutually dis-
joint. To see that they cover G, suppose that u> G G, and choose n > 2 so that
w' G G whenever p(u',u) 
< 2~n+1. 
Then A = {LJ' : Un(cj') = nn(w)} C G,
and either u> G |J™~ Am or A (~i IJ"~ A^ = 0, in which case W G I C 
An.
Having made these preparations, we can continue our construction. First,
define P(F) for F C Q to the infemum of Yyj° W(Am) over all countable covers
{A^}i° C A of F. Equivalently, since all elements of A are open and all open
sets can be written as the union of countably many elements of A,
(6.2.1) 
P(F) = inf{P(G) : G D F and G open}.
The following lemma contains several elementary facts about P.
6.2.2 LEMMA. 
For each A e A, P(A) = P(A). More generally, for all Fi C
F2 C O, P(Fi < P(F2). Moreover, for any open G C il and any countable, exact
cover {Am}f 
of G by mutually disjoint elements of A, P(G) = E r p ( A r n ) ,

160 
6 SOME MILD MEASURE THEORY
and, in general, for any sequence {rk}f 
of subsets
(
oo 
\ 
oo
\jTk\ < ^
Finally, if F and F' are disjoint closed subsets ofn, then f(FuF') 
= ¥(F) +
F(F').
PROOF: The second assertion is obvious, since any cover of ^ is also a cover of
I\. Next, suppose that A G A. Obviously, ¥(A) < ¥(A). On the other hand, if
{Am}±° is a cover of A by elements of A, then, by the Heine-Borel property of
compact sets, we can find (remember that A is closed and therefore compact)
an n such that A C (J™ Am. 
Now choose N so that {^4} U {An}™ Q AN-
Then
n 
n 
oo
¥(A) = ¥N(A) < J
l
and so we can conclude that F(A) < F(A).
Next let G be open, and suppose that {^4m}i° is an exact cover of G by
mutually disjoint elements of A. By definition, ¥(G) < J2™P(Am). On the
other hand, it is clear that, because G I) |J™ Am G A, we know that
where N is chosen so that {^4m}™ f= AN- Hence, the desired conclusion follows
after one lets n —* oo.
Now suppose that {Tk}f is a sequence of subsets of f2. Given e > 0,
choose for each k G Z + a countable cover {Ak,e : I G Z+} C A of Tk so that
Y,tHAk,i>) < P(Tk) + 2~ke. Then {AKt : (kj) G (Z+)2} C ^l is a countable
cover of \Jk Tk, and so (by Fubini's Theorem for countable measure spaces)
Finally, given disjoint, closed subsets F and F' of O, the preceding implies
that P(FUF') < ¥(F)+¥(F'). 
To get the opposite inequality, first note that,
because they are compact, there is an N > 2 such that p(u>, to') > 2~N+1 for all
to G F and w' G F'. Hence, if S = n^ 1 (UN(F)) and B' = n^ 1 (liN{F')), then
F C B e ^ F ' C B ' e i , and BOB' = 0, since ry G 5 n B ' would imply there

6.2 Modeling Probability 
161
exist LU £ F and u' £ F' such that p{uj, to') < p(u,r]) + p(r],uj') < 2~N+1. Now
suppose that {Am}f° C A is a countable cover of FUF', and set Bm = BtlAm
and B'm=B'r\A'm. 
Then {Bm}f C A and {B'm}f C A are countable covers
of F and i7". In addition, because Bm and 5 ^ are disjoint elements of A,
lm) > P(^m U B^) = ¥(Bm) + P(B'm). Hence,
E
The final ingredient in our construction is the specification of which subsets
of 0, are to be measurable. Although it may not be immediately apparent
why, we will say that F C 57 is measurable and will write F e B if, for each
e > 0, there exists an open G D F such that P(G \ F) < e. Obviously, every
open set is measurable. At the same time, it should be clear that F £ B if
P(F) = 0. Indeed, by (6.2.1), if P(F) = 0, then, for each e > 0 we can find a
open G 2 F such that F(G \ F) < P(G) < e. However, it is less obvious that
every closed set F C ft is measurable. To see this, let e > 0 be given, and
choose an open set G D F so that P(G) < ¥(F) + e. Then G \ F is open,
and so we can write G\ F = \Jm Am, where the Am's are mutually disjoint
elements of A Hence, by Lemma 6.2.2, ¥(G\F) = YTV(A™)- 
O n t n e o t n e r
hand, for each n > 1, Bn = |J™ Am is a closed subset of G\F, and so, by the
first and last parts of Lemma 6.2.2,
n
) = P(Bn) = P(Bn) = P(F U Bn) - ¥{F) < P(G) - ¥(F) < e.
Thus, F(G\F)=J2™F(Am)<e.
Now that we know open sets, closed sets, and sets of P-measure 0 are all
measurable, we can show that B is a a-algebra. To this end, first suppose that
{Ffc}f° C B, and, given e > 0, choose open sets G& 2 Ffc so that P(Gfc \ F^) <
2~ke. Then G = \J™ Gk is open, G D F = Ui° rfe. and
C
oo 
\ 
oo
\J(Gk\Tk)\ <]T
Hence, F e S , and so B is closed under countable unions. To see that it is
also closed under complementation, let F £ 8 be given, and, for each n > 1,
choose an open set Gn D F so that P(Gn \ F) < £. Then £> = f|i° G n D T
and P(D \ F) < P(Gn \ F) < £ for all n > 1. Thus, P(£> \ F) = 0, and so
D\F £ B. Now set Fn = GnC and C = \J™ Fn. Because each Fn is closed,
and therefore measurable, C £ B. Hence, since FC = C U (D \ F) £ B, we are
done.
6.2.4 THEOREM. 
Referring to the preceding, BQ = a (A) and T £ B if
and only if there exist C,D £ BQ such that C C T C D and ¥(D \ C) = 0.
Next, again use P to denote restriction ¥ \BofFtoB. 
Then, (fl,B,F) is a
probability space with the property that ¥ \ An = ¥n for every n > 1.

162 
6 SOME MILD MEASURE THEORY
PROOF: We have already established that B is a a-algebra. Since all elements
of A are open, the equality BQ = <r{A) follows from the fact that every open
set can be written as the countable union of elements of A. To prove the
characterization of B in terms of BQ, first observe that, since open sets are
in B, BQ C B. Moreover, since sets of P-measure 0 are also in B, C C F C
D for C,D G BQ with F(D \ C) = 0 implies that F = C n (F \ C) G B,
since P(F \ C) < P(D \ C) = 0. Conversely, if F G B, then we can find
sequences {Gn}™ and {-Hn}i° of open sets such that Gn D F, Hn D FC, and
P(Gn \ F) V F(Hn \ FC) < i . Thus, if D = flf Gn and C = |Jf iJnC, then
C,fl66fi,CcrcD, and
P(£> \ C) < F(Gn \ HnZ) < F(Gn \ F) + F(Hn \ FC) < -
n
for all n > 1. Hence, P(L> \ C) = 0.
All that remains is to check that P is countably additive on B. To this end,
let {Ffe}]50 C B be a sequence of mutually disjoint sets, and set F = {J^Tk.
By the second part of Lemma 6.2.2, we know that P(F) < ^^°P(F/t). To get
the opposite inequality, let e > 0 be given, and, for each k, choose an open
Gk D FfeC so that F(Gk \ FfcC) < 2~ke. Then each Fk = GkC is closed, and
P(Ffe) < F(Fk) + F(Gk \ FfcC) < F(Fk) + 2~ke.
In addition, because the F^'s are, the Fk's are mutually disjoint. Hence, by
the last part of Lemma 6.2.2, we know first that
_ / n 
\ 
n
P(F) > P I \Fk \ = VP(Ffc) 
for all n > 1,
vV ) i
and then that
oo 
oo
^ ~ " 
" " " 
" 
- e . D
6.3 Independent Random Variables
In Kolmogorov's model, independence is best described in terms of a-
algebras. Namely, if (f2, J7, P) is a probability space and T\ and JF2 are o-
subalgebras (i.e., are cr-algebras which are subsets) of J-, then we say that
Tx and JT2 are independent if P(r\ n T2) = P(ri)P(r2) for all r\ e T\ and
F2 e Ti- It should be comforting to recognize that, when A\, A^ € T and, for
i G {1,2}, !Fi = a({Ai}) = {0, Ai, AjC,f2}, then, as is easily checked, T\ is in-
dependent of JF2 precisely when, in the terminology of elementary probability
theory, "Ai is independent of A2": P(Ai n A2) — F(Ai)F(A2).
The notion of independence gets inherited by random variables. Namely,
the members of a collection {Xa : a & X} of random variables on (ft, J7, P)

6.3 Independent Random Variables 
163
are said to be mutually independent if, for each pair of disjoint subsets J\
and J2 of X, the cr-algebras a{{Xa 
: a e Ji}) and a({Xa 
: a e J2})
are independent. One can use Theorem 6.1.6 to show that this definition
is equivalent to saying that if Xa takes its values in the measurable space
(Ea,Ba), 
then, for every finite subset {am}™ of distinct elements of X and
choice of BUm G Bam, 1 < m < n,
n
P(XQm G Bam for 1 < m < n) = \\_V{Xam G Bam).
1
As a dividend of this definition, it is essentially obvious that if {Xa : a e X}
are mutually independent and if, for each a G X, Fa is a measurable map on
the range of Xa, then {Fa(Xa) 
: a € X} are again mutually independent.
Finally, by starting with simple functions, one can show that if {Xm}™ are
mutually independent and, for each 1 < m < n, fm is a measurable K-valued
function on the range of Xm, then
n
E [h (Xi )•••/„ (Xn)] = H E [fm (Xm)]
1
whenever the fm are all bounded or are all non-negative.
6.3.1. Existence of Lots of Independent Random Variables: In the
preceding section, we constructed a countable family of mutually independent
random variables. Namely, if (0, B, P) is the probability space discussed in
Theorem 6.2.4 and Xm(cu) = u){m) is the mth coordinate of to, then, for any
choice of n > 1 and (771, ...,ryn) G {0,1}, F(Xm — rjm, 1 < m < n) =
2~~n — n"P(XTO = rfm). Thus, the random variables {Xm}f° are mutually
independent. Mutually independent random variables, like these, which take
on only two values are called Bernoulli random variables.
As we are about to see, Bernoulli random variables can be used as build-
ing blocks to construct many other families of mutually independent random
variables. The key to such constructions is contained in the following lemma.
6.3.1 LEMMA. 
Given any family {Bm}f 
of mutually independent, {0,1}-
valued Bernoulli random variables satisfying F(Bm = 0) = ^ = V(Bm = 1)
for all m e Z+, set U = J2T 2~mBm- 
Then U is uniformly distributed on
[0,1). That is, ¥(U < u) is 0ifu<0,u 
ifu G [0,1), and 1 if u > 1.
PROOF: Given N > 1 and 0 < n < 2N, we want to show that
(*) 
P(n2-N 
<U <{n + l)2~N) 
=2'N.
To this end, note that 712"^ < U < (n + 1)2"^ if and only if J]f 
^~mBm
= (n + l)2- w and Bm = 0 for m > N, or J2i 2~mBm = n2~N and Bm = 1
for some m > N. Hence, since W(Bm = 0 for all m > N) = 0, the left hand

164 
6 SOME MILD MEASURE THEORY
side of (*) is equal to the probability that J ^ 2~mBm = n2~N. However,
elementary considerations show that, for any 0 < n < 2N there is exactly one
choice of (%,..., r]N) G {0,1}^ for which *£i 2~mVm = n2~N. Hence,
N 
\
%Bm = n2~N 
= ¥(Bm = r]m for 1 < m < N) = 2~N.
Having proved (*), the rest is easy. Namely, since ¥(U — 0) = ¥(Bm =
0 for all m > 1) = 0, (*) tells us that, for any 1 < k < 2N
fc-i
¥(U < k2~N) = Y^ W(rn2-N < U < (m + 1)2"^) = k2~N.
m=0
Hence, because u-^*¥(U < u) is continuous from the right, it is now clear that
Fu(u) = u for all u € [0,1). Finally, since ¥{U e [0,1]) = 1, this completes
the proof. 
•
Now let X a non-empty, finite or countably infinite set. Then X x Z + is
countable, and so we can construct a 1-to-l map (a, n)~^N(a, n) from X x Z +
onto Z+. Next, for each a e X, define w e li = {0,1}Z+ i—> Xa(w) € ft so
that the nth coordinate of Xa(a>) is ui(N(a, n)). Then, as random variables on
the probability space (ft, B, P) in Theorem 6.2.4, {Xa : a 6 X} are mutually
independent and each has distribution P. Hence, if $ : ft —> [0,1] is the
continuous map given by
oo
$(77) = ^2 2~mri{m) 
for 77 G ft
and if Ua = $(XQ), then the random variables {Ua = $(XQ) : a G X} are
mutually independent and, by Lemma 6.3.1, each is uniformly distributed on
[0,1].
The final step in this section combines the preceding construction with the
well-known fact that any M-valued random variable can be represented in
terms of a uniform random variable. More precisely, a map F : K —> [0,1]
is called a distribution function if F is non-decreasing, continuous from the
right, and tends to 0 at —00 and 1 at +00. Given such an F, define
F" 2(M) = inf{:r 6 R : F(x) > u} for u G [0,1].
Notice that, by right continuity, F{x) > u <=^ F~l(u) < x. Hence, if U
is uniformly distributed on [0,1], then F"1^) 
is a random variable whose
distribution function is F.
6.3.2 THEOREM. 
Let (Cl,B,¥) be the probability space in Theorem 6.2.4.
Given any finite or countably infinite index set X and a collection {Fa : a € X}
of distribution functions, there exist mutually independent random variables
{Xa : a G X} on (ft, B, P) with the property that, for each a G X, Fa is the
distribution of Xa.

6.4 Conditional Probabilities and Expectations 
165
6.4 Conditional Probabilities and Expectations
Just as they are to independence, cr-algebras are central to Kolmogorov's
definition of conditioning. Namely, given a probability space (fi, J-, P), a sub-
cr-algebra E, and a random variable X which is non-negative or integrable,
Kolmogorov says that the random variable Xs is a conditional expectation of
X given E if X-% is a non-negative random variable which is measurable with
respect to E (i.e., <J({X}) C S) and satisfies
(6.4.1) 
E[X2, T] = E[X, T] for all T G E.
When X is the indicator function of a set B G J-, then the term conditional
expectation is replaced to conditional probability.
To understand that this definition is an extension of the one given in elemen-
tary probability courses, begin by considering the case when E is the trivial
cr-algebra {0,57}. Because only constant random variables are measurable
with respect to {0, Q,}, it is clear that the one an only conditional expectation
of X will be E[X}. Next, suppose that S = a({A}) = {0, A, AC, Q} for some
A G T with P(A) G (0,1). In this situation, it is an easy matter to check that,
for any B G T,
is a conditional probability of B given E. That is, the quantity P(£?|A) =
p,^ 
, which in elementary probability theory would be called "the condi-
tional probability of B given A" appears here as the value on A of the map
a>~~>P(.B|£)(a;). More generally, if S is generated by a finite or countable par-
tition V C T of f2, then, for any non-negative or integrable random variable
X,
^ 
E[X, A]
will be a conditional expectation of X given E.
Of course, Kolmogorov's definition brings up two essential questions: exis-
tence and uniqueness. A proof of existence in general can be done in any one
of many ways. For instance, when E[X2] < oo, one can easily see that, just
as E[X]1 is the minimum value of E[(X — X')2] among all constant random
variables X', so X2 will have to be the minimum of E[(X — X')2] among all
S-measurable random variables X'. In this way, the problem of existence can
be related to a problem of orthogonal projection in the space of all square-
integrable random variables, and, although they are outside the scope of this
book, such projection results are familiar to anyone who has studied the the-
ory Hilbert spaces.
Uniqueness, on the other hand, is both easier and more subtle than ex-
istence. Namely, there is no naive uniqueness statement here, because, in

166 
6 SOME MILD MEASURE THEORY
general, there will be uncountably many ways to take X^,.9 On the other
hand, every choice differs from any other choice on a set of measure at most
0. To see this, suppose that X^ is a second non-negative random variable
which satisfies (6.4.1). Then A = {X^ > X%} £ S, and so the only way
that (6.4.1) can hold is if ¥{A) = 0. Similarly, P(X2 > X^) = 0, and so
P(XS ^ X£ = 0.
In spite of the ambiguity caused by the sort of uniqueness problems just
discussed, it is common to ignore, in so far as possible, this ambiguity and
proceed as if a random variable possesses only one conditional expectation
with respect to a given cr-algebra. In this connection, the standard notation
for a conditional expectation of X given S is E[X|S] or, when X = 1B,
P(.B|£), which is the notation which we adopted in the earlier chapters of this
book.
6.4.1. Conditioning with Respect to Random Variables: In this book,
essentially all conditioning is done when E = cr($) (cf. § 6.1.4) for some family
# of measurable functions on (ft, J-, P). When S has this form, the conditional
expectation of a random variable X will be a measurable functions of the
functions in $. For example, if $ = {F\,... ,Fn} and the functions Fm all take
their values in a countable space S, a conditional expectation value E [X j u(3r)]
of X given a($) has the form $(Fi,..., Fn), where 3>(ii,..., in) is equal to
E[X,F1=i1,...,Fn=in] 
^
¥(F1=i1,...,Fn=in)
according to whether P(iri = i\,...,Fn 
= in) is positive or 0. In order to
emphasize that conditioning with respect to <J($) results in a function of J,
we use the notation E[X|£] or P(-B|#) instead of E[X|o-(#)] or f>(B\a{$)).
To give more concrete examples of what we are talking about, first suppose
that X and Y are independent random variables with values in some countable
space S, and set Z = F(X, Y), where F : §2 —> R is bounded. Then
(6.4.2) 
E[Z\X] = v(X) where v{i) = E[F(i, Y)} for i e S.
A less trivial example is provided by our discussion of Markov chains. In
Chapter 2, we encoded the Markov property in equations like
¥(Xn+1=j\X0,...Xn)=(P)Xnj,
which displays this conditioning as a function, namely (zo, • • •, zn)~^(P)jTlj, of
the random variables in terms of which the condition is made. (Of course, the
distinguishing feature of the Markov property is that the function depends
only on in and not (io, • • •, in-i)-) Similarly, when we discussed Markov pro-
cesses with a continuous time parameter, we wrote
¥{X(t) = j | X{a), a E [0, s]) = (P(t - 
s))x{s)j,
which again makes it explicit that the conditioned quantity is a function ran-
dom variables on which the condition is imposed.
9 This non-uniqueness is the reason for our use of the article "a" instead of "the" in front
of "conditional expectation."

Notation
Notation
z& z+
N
AC
l A
F \ S
aAd&oVb
a & a~
i —> j & i <-> j
<5i
E[X, A]
E[X | A] & E[X | E]
<V?,V}w&||¥>,V'l|2>7r
Stat(P)
IIMIIV
ll/llu
l|M||u,v
Vai>(/)
Description
set of all integers and the subset of positive integers
set of non-negative integers
number of elements in the set S
complement of the set A
indicator function of the set A: 1,4(0:) = 1 if x £ A
and XA{x) = 0 if x £ A
restriction of the function F to the set S
minimum and the maximum of a, b £ R
positive part aVO & negative part (-a)VO of a G K
state j is accessible from i & communicates with i
Kronecker delta: 5ij is 1 or 0 depending on whether
i is equal or unequal to j
point measure at i € S: (Si)j = 5ij for j 6 S
expected value of X on the event A
conditional expectatation value of X given the
event A & the cr-algebra £
alternative notations for Y"\ «.¥>(0('r)i
inner product and norm in L2(n)
set of stationary distribution for the transition
probability matrix P
variation norm of the row vector fi
uniform norm of the function /
uniform-variation norm of the matrix M
variance of / relative to the probability vector /^
See
§3.1
§6.2
§6.4
(5.1.4)
§5.1.2
§3.2.3
(2.1.5)
(2.1.10)
(3.2.1)

References
1. Diaconis, P. & Stroock, D., Geometric bounds for eigenvalues of Markov
chains, Ann. Appl. Probab. 1 # 1 (1991), 36-61.
2. Dunford, N. & Schwartz, J., Linear Operators, part I, Wiley Classics
Lib., Wiley-Interscience, NY, 1988.
3. Holley, R. & Stroock, D., Simulated annealing via Sobolev inequalities,
Comm. Math. Phys. 115 # 4 (1988), 553-569.
4. Karlin, S. & Taylor, H, A First Course in Stochastic Processes, 2nd
ed., Academic Press, NY, 1975.
5. Norris, J.R., Markov Chains, Cambridge Series in Statistical & Proba-
bilistic Mathematics, Cambridge Univ. Press, Cambridge, U.K., 1997.
6. Revuz, D., Markov Chains, Mathematical Library, vol. 11, North Hol-
land, Amsterdam & New York, 1984.
7. Riesz, F. & Sz.-Nagy, B., Functional Analysis, translated from the
French edition by F. Boron, reprint of 1955 original, Dover Books, NY,
1990.
8. Stroock, D., A Concise Introduction to the Theory of Integration, 3rd
ed., Birkhauser-Boston, Cambridge, MA, USA, 1998.
9. Stroock, D., Probability Theory, An Analytic View, 2nded., Cambridge
Univ. Press, NY, 2000.

Index
accessible, 36, 46
adjoint, 40, 103, 108
allowable path, 118, 125
almost everywhere, 152
aperiodic, 52
backward variable, 86
Bernoulli random variable, 1
Bernoulli random variable, 163
binomial coefficient, 2
generalized, 6
branching process, 41
extinction, 41
Cauchy convergent, 26
Chapman-Kolmogorov equation, 80
time-inhomogeneous, 133
communicates, 46
Q-communicates, 96
communicating class properties, 46
conditional expectation, 165
conditional probability, 2, 165
continuity properties of measures, 147
convex function, 138
convex set, 58
convolution, 79
cooling schedule, 132
countably additive, 146
coupling, 15
D
Sij, the Kronecker symbol, 2
detailed balance condition, 108, 121
Dirichlet form, 116
distribution
function, 158
initial, 24
of a random variable, 157
distribution function, 164
Doeblin's Theorem, 28
basic, 28
sharpened, 40
spectral theory, 29
Doob's /i-transformation, 43
Doob's Stopping Time Theorem, 49
doubly stochastic, 40
E
empirical measure, 74, 105
ergodic theorem
empirical measure, 74, 105
individual, 72, 105
mean, 33, 61, 100
ergodic theory, 33
event, 157
exhaustion, 90
expected value, 157
explosion time, 90
extreme point, 58
Fatou's Lemma, 155
finite measure space, 146
first passage time, 3
forward variable, 86
Fubini's Theorem, 155
G
generalized binomial coefficient, 6
Gibbs state, 126
Glauber dynamics, 126
greatest common divisor, 51
Gronwall's inequality, 125, 140
H
homogeneous increments, 75

170
Index
independence
of ex-algebras, 162
of random variables, 163
individual ergodic theorem, 38, 72, 105
infinitesimal characteristics, 88
initial distribution, 24
inner product, 109
integer part [s] of s, 30
integrable function, 151
irreducible, 46
Q-irreducible, 96
Jensen's inequality, 139
K
Kolmogorov's equation
backward, 85, 91
forward, 86
time-inhomogeneous, 132
Kronecker symbol, 2
Lebesgue's Dominated
Convergence Theorem, 155
M
Markov chain, 23
initial distribution, 24
Markov process, 80
rates for, 80
transition probability for, 80
Markov property, 23, 27, 83
Markov's inequality, 153
measurable
map, 149
space, 145
subsets, 145
measure, 145
measure space, 145
(7-finite, 147
finite, 146
Metropolis algorithm, 130
minimal extension, 95
Monotone Convergence Theorem, 154
monotonicity of integral, 150
mutually independent
random variables, 163
N
non-explosion, 92
non-negative definite, 116
norm, 26
null recurrent, 58
partition function, 126
period of a state, 52
Poincare inequality, 117
Poincare constant, 116
Poisson process
compound with jump distribution
and rate R, 78
simple, 75
positive recurrent, 58, 101
probability, 157
measure, 157
space, 157
vector, 24
P-stationary, 57
Q
Q-null recurrent, 100
Q-positive recurrent, 100
Q-matrix, 86
queuing model, 21, 68
queuing theory, 21
R
random time change, 106
random variable, 157
Bernoulli, 163
random walk
symmetric, 7, 9
rates, 80
bounded, 81
degenerate, 81
non-degenerate, 81
recurrence, 6
recurrence time, 8
recurrent, 8, 9, 20, 34, 46, 48
null, 58
positive, 58
reflection principle, 3
renewal equation, 56
reverse, 40, 103
reversible, 107

Index
171
sample point, 157
sample space, 157
Schwarz's inequality, 16, 109, 139
semigroup, 80
set of measure 0, 152
a-algebra, 145
Borel, 148
generated by, 148, 149
smallest containing, 148
or-finite measure space, 147
signum, 5
simple function, 150
simple Poisson process, 75
simulated annealing algorithm, 130
spectral gap, 111
spectral theory, 29, 112
spin-flip systems, 143
state space, 23
stationary distribution, 28, 57, 71
null recurrent case, 72, 104
uniqueness, 72, 104
Stirling's formula, 18
Strong Law of Large Numbers, 17
subadditivity of measures, 147
symmetric on L2, 110
time of first return, 6, 9, 46
time-inhomogeneous, 132
Chapman—Kolmogorov equation, 133
Kolmogorov's forward equation, 132
transition probability, 132
transient, 8, 9, 20, 34, 46, 48
transition probability matrix, 24
time inhomogeneous, 132
triangle inequality, 26
uniform norm
U
llu, 27
variation norm ||p||vi 25
W
Weak Law of Large Numbers, 16, 18

(continued from page ii)
64 
EDWARDS. Fourier Series. Vol. I. 2nd ed.
65 
WELLS. Differential Analysis on Complex
Manifolds. 2nd ed.
66 
WATERHOUSE. Introduction to Affine
Group Schemes.
67 
SERRE. Local Fields.
68 
WEIDMANN. Linear Operators in Hilbert
Spaces.
69 
LANG. Cyclotomic Fields II.
70 
MASSEY. Singular Homology Theory.
71 
FARKAS/KRA. Riemann Surfaces. 2nd ed.
72 
STILLWELL. Classical Topology and
Combinatorial Group Theory. 2nd ed.
73 
HUNGERFORD. Algebra.
74 
DAVENPORT. Multiplicative Number
Theory. 3rd ed.
75 
HOCHSCHILD. Basic Theory of Algebraic
Groups and Lie Algebras.
76 
IITAKA. Algebraic Geometry.
77 
HECKE. Lectures on the Theory of
Algebraic Numbers.
78 
BURRIS/SANKAPPANAVAR. A Course in
Universal Algebra.
79 
WALTERS. An Introduction to Ergodic
Theory.
80 
ROBINSON. A Course in the Theory of
Groups 2nd ed.
81 
FORSTER. Lectures on Riemann Surfaces.
82 
BOTT/TU. Differential Forms in Algebraic
Topology.
83 
WASHINGTON. Introduction to Cyclotomic
Fields. 2nd ed.
84 
IRELAND/ROSEN. A Classical Introduction
to Modern Number Theory. 2nd ed.
85 
EDWARDS. Fourier Series. Vol. II. 2nd ed.
86 
VAN LINT. Introduction to Coding Theory.
2nd ed.
87 
BROWN. Cohomology of Groups.
88 
PIERCE. Associative Algebras.
89 
LANG. Introduction to Algebraic and
Abelian Functions. 2nd ed.
90 
BRONDSTED. An Introduction to Convex
Polytopes.
91 
BEARDON. On the Geometry of Discrete
Groups.
92 
DIESTEL. Sequences and Series in Banach
Spaces.
93 
DUBROVIN/FOMENKO/NOVIKOV. Modem
Geometry—Methods and Applications.
Part I. 2nd ed.
94 
WARNER. Foundations of Differentiable
Manifolds and Lie Groups.
95 
SHIRYAEV. Probability. 2nd ed.
96 
CONWAY. A Course in Functional
Analysis. 2nd ed.
97 
KOBLITZ. Introduction to Elliptic Curves
and Modular Forms. 2nd ed.
98 
BROCKER/TOM DIECK. Representations of
Compact Lie Groups.
99 
GROVE/BENSON. Finite Reflection Groups.
2nd ed.
100 BERG/CHRISTENSEN/RESSEL. Harmonic
Analysis on Semigroups: Theory of
Positive Definite and Related Functions
101 EDWARDS. Galois Theory.
102 VARADARAJAN. Lie Groups, Lie Algebras
and Their Representations.
103 LANG. Complex Analysis. 3rd ed.
104 DUBROVIN/FOMENKO/NOVIKOV. Modern
Geometry—Methods and Applications.
Part II.
105 LANG. SL2(R).
106 SILVERMAN. The Arithmetic of Elliptic
Curves.
107 OLVER. Applications of Lie Groups to
Differential Equations. 2nd ed.
108 RANGE. Holomorphic Functions and
Integral Representations in Several
Complex Variables.
109 LEHTO. Univalent Functions and
Teichmuller Spaces.
110 LANG. Algebraic Number Theory.
111 HUSEMOLLER. Elliptic Curves. 2nd ed.
112 LANG. Elliptic Functions.
113 KARATZAS/SHREVE. Brownian Motion and
Stochastic Calculus. 2nd ed.
114 KOBLITZ. A Course in Number Theory and
Cryptography. 2nd ed.
115 BERGER/GOSTIAUX. Differential Geometry:
Manifolds, Curves, and Surfaces.
116 KELLEY/SRINIVASAN. Measure and
Integral. Vol. I.
117 J.-P. SERRE. Algebraic Groups and Class
Fields.
118 PEDERSEN. Analysis Now.
119 ROTMAN. An Introduction to Algebraic
Topology.
120 ZIEMER. Weakly Differentiable Functions:
Sobolev Spaces and Functions of Bounded
Variation.
121 LANG. Cyclotomic Fields I and II.
Combined 2nd ed.
122 REMMERT. Theory of Complex Functions.
Readings in Mathematics
123 EBBINGHAUS/HERMES et al. Numbers.
Readings in Mathematics

124 DUBROVIN/FOMENKO/NOVIKOV. Modern
Geometry—Methods and Applications.
Part III
125 BERENSTEIN/GAY. Complex Variables:
An Introduction.
126 BOREL. Linear Algebraic Groups. 2nd ed.
127 MASSEY. A Basic Course in Algebraic
Topology.
128 RAUCH. Partial Differential Equations.
129 FULTON/HARRIS. Representation Theory: A
First Course.
Readings in Mathematics
130 DODSON/POSTON. Tensor Geometry.
131 LAM. A First Course in Noncommutative
Rings. 2nd ed.
132 BEARDON. Iteration of Rational Functions.
133 HARRIS. Algebraic Geometry: A First
Course.
134 ROMAN. Coding and Information Theory.
135 ROMAN. Advanced Linear Algebra.
136 ADKINS/WEINTRAUB. Algebra: An
Approach via Module Theory.
137 AXLER/BOURDON/RAMEY. Harmonic
Function Theory. 2nd ed.
138 COHEN. A Course in Computational
Algebraic Number Theory.
139 BREDON. Topology and Geometry.
140 AUBIN. Optima and Equilibria. An
Introduction to Nonlinear Analysis.
141 BECKER/WEISPFENMNG/KREDEL. Grobner
Bases. A Computational Approach to
Commutative Algebra.
142 LANG. Real and Functional Analysis.
3rd ed.
143 DOOB. Measure Theory.
144 DENNIS/FARB. Noncommutative
Algebra.
145 VICK. Homology Theory. An
Introduction to Algebraic Topology.
2nd ed.
146 BRIDGES. Computability: A
Mathematical Sketchbook.
147 ROSENBERG. Algebraic A:-Theory
and Its Applications.
148 ROTMAN. An Introduction to the
Theory of Groups. 4th ed.
149 RATCLIFFE. Foundations of
Hyperbolic Manifolds.
150 ElSENBUD. Commutative Algebra
with a View Toward Algebraic
Geometry.
151 SlLVERMAN. Advanced Topics in
the Arithmetic of Elliptic Curves.
152 ZIEGLER. Lectures on Polytopes.
153 FULTON. Algebraic Topology: A
First Course.
154 BROWN/PEARCY. An Introduction to
Analysis.
155 KASSEL. Quantum Groups.
156 KECHRIS. Classical Descriptive Set
Theory.
157 MALLIAVIN. Integration and
Probability.
158 ROMAN. Field Theory.
159 CONWAY. Functions of One
Complex Variable II.
160 LANG. Differential and Riemannian
Manifolds.
161 BORWEIN/ERDELYI. Polynomials and
Polynomial Inequalities.
162 ALPERIN/BELL. Groups and
Representations.
163 DIXON/MORTIMER. Permutation Groups.
164 NATHANSON. Additive Number Theory:
The Classical Bases.
165 NATHANSON. Additive Number Theory:
Inverse Problems and the Geometry of
Sumsets.
166 SHARPE. Differential Geometry: Cartan's
Generalization of Klein's Erlangen
Program.
167 MORANDI. Field and Galois Theory.
168 EWALD. Combinatorial Convexity and
Algebraic Geometry.
169 BHATIA. Matrix Analysis.
170 BREDON. Sheaf Theory. 2nd ed.
171 PETERSEN. Riemannian Geometry.
172 REMMERT. Classical Topics in Complex
Function Theory.
173 DIESTEL. Graph Theory. 2nd ed.
174 BRIDGES. Foundations of Real and
Abstract Analysis.
175 LICKORISH. An Introduction to Knot
Theory.
176 LEE. Riemannian Manifolds.
177 NEWMAN. Analytic Number Theory.
178 
CLARKE/LEDYAEV/STERN/WOLENSKI.
Nonsmooth Analysis and Control
Theory.
179 DOUGLAS. Banach Algebra Techniques in
Operator Theory. 2nd ed.
180 SRIVASTAVA. A Course on Borel Sets.
181 KRESS. Numerical Analysis.
182 WALTER. Ordinary Differential
Equations.

183 MEGGINSON. An Introduction to Banach
Space Theory.
184 BOLLOBAS. Modern Graph Theory.
185 Cox/LlTTLE/O'SHEA. Using Algebraic
Geometry.
186 RAMAKRISHNAN/VALENZA. Fourier
Analysis on Number Fields.
187 HARRIS/MORRISON. Moduli of Curves.
188 GOLDBLATT. Lectures on the Hyperreals:
An Introduction to Nonstandard Analysis.
189 LAM. Lectures on Modules and Rings.
190 ESMONDE/MURTY. Problems in Algebraic
Number Theory. 2nd ed.
191 LANG. Fundamentals of Differential
Geometry.
192 HIRSCH/LACOMBE. Elements of
Functional Analysis.
193 COHEN. Advanced Topics in
Computational Number Theory.
194 ENGEL/NAGEL. One-Parameter Semigroups
for Linear Evolution Equations.
195 NATHANSON. Elementary Methods in
Number Theory.
196 OSBORNE. Basic Homological Algebra.
197 ElSENBUD/HARRlS. The Geometry of
Schemes.
198 ROBERT A Course in /?-adic Analysis.
199 HEDENMALM/KORENBLUM/ZHU. Theory
of Bergman Spaces.
200 
BAO/CHERN/SHEN. An Introduction to
Riemann-Finsler Geometry.
201 
HINDRY/SILVERMAN. Diophantine
Geometry: An Introduction.
202 LEE. Introduction to Topological
Manifolds.
203 SAGAN. The Symmetric Group:
Representations, Combinatorial
Algorithms, and Symmetric Functions.
204 
ESCOFIER. Galois Theory.
205 FELIX/HALPERIN/THOMAS. Rational
Homotopy Theory. 2nd ed.
206 MuRTY. Problems in Analytic Number
Theory.
Readings in Mathematics
207 GODSIL/ROYLE. Algebraic Graph Theory.
208 CHENEY. Analysis for Applied
Mathematics.
209 ARVESON. A Short Course on Spectral
Theory.
210 ROSEN. Number Theory in Function
Fields.
211 LANG. Algebra. Revised 3rd ed.
212 MATOUSEK. Lectures on Discrete
Geometry.
213 FRITZSCHE/GRAUERT. From Holomorphic
Functions to Complex Manifolds.
214 JoST. Partial Differential Equations.
215 GOLDSCHMIDT. Algebraic Functions and
Projective Curves.
216 D. SERRE. Matrices: Theory and
Applications.
217 MARKER. Model Theory: An Introduction.
218 LEE. Introduction to Smooth Manifolds.
219 MACLACHLAN/REID. The Arithmetic of
Hyperbolic 3-Manifolds.
220 NESTRUEV. Smooth Manifolds and
Observables.
221 GRUNBAUM. Convex Polytopes. 2nd ed.
222 HALL. Lie Groups, Lie Algebras, and
Representations: An Elementary
Introduction.
223 VRETBLAD. Fourier Analysis and
Its Applications.
224 WALSCHAP. Metric Structures in
Differential Geometry.
225 BUMP: Lie Groups
226 ZHU. Spaces of Holomorphic Functions
in the Unit Ball.
227 MILLER/STURMFELS. Combinatorial
Communicative Algebra.
228 DIAMOND/SHURMAN. A First Course
in Modular Forms.
229 EISENBUD. The Geometry of Syzygies.
230 STROOCK. An Introduction to Markov
Processes.


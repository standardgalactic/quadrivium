Volume: 05 | Issue: 08 | Pages: 108 | May 2017
ISSN-2456-4885
` 120
An Introduction 
To gRPC
Using Ansible To Deploy 
Cacti For Monitoring
Sharpen Your
Programming 
SkillS
Programming With 
Shell Scripts
A Guide To Programming In 
Perl, PHP And Python
Dart: An Easy, Scalable 
And Multi-purpose 
Programming Language
An Interview With  
Bjarne Stroustrup, Creator Of C++
Case Study: Vodafone Deploys Open 
Source To Reduce Vendor Lock-in
“I built C++ 
primarily for myself 
and my colleagues”



Admin
24 
Handling Large Files  
Using Open Source Tools
29 
DevOps: Using Ansible to 
Deploy Cacti for Monitoring
36 
The Basics of Vulnerability 
Assessment and Penetration 
Testing
Developers
38  Exploring Sinatra
41  Scala: The Powerhouse of 
Apache Spark
44  An Introduction to gRPC
50 
Qt Programming for HTTP 
REST Clients
54 
A Quick Look at 
Programming with Shell 
Scripts
62  A Guide to Programming in 
Perl, PHP and Python
69 
Python: The User Friendly 
Language for Coding
73 
An Introduction to HTML5
78 
Top IDEs for Raspberry Pi
83 
Using State Machines  
to Build Better Software
85 
Qt: The Cross-Platform 
Development Toolkit
87 
Creating the Sliding Number 
Game in App Inventor 2
91 
Searching Text Strings from 
Files Using Python
Open Gurus
94  Hack the Bootsector  
and Write Your Own!
102  Beaglebone Black: Flashing 
eMMC through an SD Card
REGULAR FEATURES
06 
FOSSBytes
  14  New Products
  104  Tips & Tricks
65
58
Dart: An Easy, Scalable and Multi-
purpose Programming Language
Programming the Internet of 
Things Using Contiki and Cooja
ISSN-2456-4885
4 | May 2017 | OPEN SOURCE FOR yOU | www.OpenSourceForU.com

In c
ase 
this
 DV
D do
es n
ot w
ork 
prop
erly,
 wri
te t
o us
 at s
upp
ort@
efy.i
n fo
r a f
ree 
repl
ace
men
t.
CD T
eam
 e-m
ail: 
cdtea
m@
efy.i
n
Rec
omm
ende
d Sy
stem
 Req
uire
ment
s: P
4, 1
GB R
AM,
 DV
D-RO
M Dr
ive
May 2017
17.04 
Live
Here’s the latest version of 
Ubuntu for your desktop.
DVD of The MonTh
• Ubuntu 17.04 Desktop
• Lubuntu 17.04
• KDE Neon
• Rocksor 3.9.0
106
Bjarne Stroustrup, creator of C++
“I built C++ 
primarily for 
myself and my 
colleagues”
20
99
Vodafone Deploys Open Source  
to Reduce Vendor Lock-in
Columns
16 
CodeSport 
18 
Exploring Software: 
Diaspora: The Free, 
Distributed Social 
Network
Editor
Rahul chopRa
Editorial, SubScriptionS & advErtiSing
Delhi (hQ)
d-87/1, okhla industrial area, phase i, new delhi 110020
ph: (011) 26810602, 26810603; Fax: 26817563
E-mail: info@efy.in
MiSSing iSSuES
e-mail: support@efy.in
back iSSuES
Kits ‘n’ Spares
new delhi 110020 
ph: (011) 26371661,  26371662
E-mail: info@kitsnspares.com
nEwSStand diStribution
ph: 011-40596600
E-mail: efycirc@efy.in 
advErtiSEMEntS 
mumbai
ph: (022) 24950047, 24928520 
E-mail: efymum@efy.in
beNGaluRu
ph: (080) 25260394, 25260023 
E-mail: efyblr@efy.in
PuNe
ph: 08800295610/ 09870682995 
E-mail: efypune@efy.in
GuJaRaT
ph: (079) 61344948 
E-mail: efyahd@efy.in
chiNa
power pioneer group inc.  
ph: (86 755) 83729797, (86) 13923802595 
E-mail: powerpioneer@efy.in
JaPaN
tandem inc., ph: 81-3-3541-4166 
E-mail: tandem@efy.in
SiNGaPORe
publicitas Singapore pte ltd 
ph: +65-6836 2272 
E-mail: publicitas@efy.in
TaiwaN 
J.k. Media, ph: 886-2-87726780 ext. 10 
E-mail: jkmedia@efy.in
uNiTeD STaTeS
E & tech Media 
ph: +1 860 536 6677 
E-mail: veroniquelamarque@gmail.com
printed, published and owned by ramesh chopra. printed at tara 
art printers pvt ltd, a-46,47, Sec-5, noida, on 28th of the previous 
month, and published from d-87/1, okhla industrial area, phase i, new 
delhi 110020. copyright © 2017. all articles in this issue, except for 
interviews, verbatim quotes, or unless otherwise explicitly mentioned, 
will be released under creative commons attribution-noncommercial 
3.0 unported license a month after the date of publication. refer to 
http://creativecommons.org/licenses/by-nc/3.0/  for a copy of the 
licence. although every effort is made to ensure accuracy, no responsi-
bility whatsoever is taken for any loss due to publishing errors. articles 
that cannot be used are returned to the authors if accompanied by a 
self-addressed and sufficiently stamped envelope. but no responsibility 
is taken for any loss or delay in returning the material. disputes, if any, 
will be settled in a new delhi court only.
SubScRiPTiON RaTeS 
Year 
Newstand Price 
You Pay 
Overseas
 
(`) 
(`)
Five 
7200 
4320 
—
three 
4320 
3030 
—
one 
1440 
1150  
uS$ 120
kindly add ` 50/- for outside delhi cheques.
please send payments only in favour of eFY enterprises Pvt ltd.
non-receipt of copies may be reported to support@efy.in—do mention 
your subscription number.
www.OpenSourceForU.com | OPEN SOURCE FOR yOU | May 2017 | 5

Compiled by: 
Jagmeet Singh
It was all about the future of 
containers at the Bangalore 
Container Conference
Bangalore Container 
Conference (BCC), held 
on April 7 at Bengaluru, 
was India’s first conference 
focused on container 
technologies, and attracted 
more than 250 delegates.
“Containers are eating 
the world. It has been 
Docker, Docker, everywhere 
for the last few years,” said 
Hari Kiran, one of the organisers of the Bangalore Container Conference (BCC) 
and co-founder of CodeOps Technologies.
When asked about what prompted him to conduct an event like BCC, Kiran 
told Open Source For You, “Given the fact that there is substantial interest in 
container technologies among developers in India, we realised that the time is ripe 
for organising India’s first conference on this topic.”
All the tickets for BCC 2017 were sold out a month before the conference 
date. The conference hosted Evan Powell, chairman of OpenEBS, and Ian Lewis, 
developer advocate at Google, who travelled from the US and Japan, respectively. 
Both the experts were featured at the keynote sessions.
In his opening address, Powell spoke about the relationship between 
DevOps and containers. He explained how the DevOps industry is expected to 
significantly benefit from the container ecosystem and its technologies.
Lewis began his keynote on ‘standards-based containerisation with 
Kubernetes’ and explained why standards are important. He elaborated on how 
they allow interoperability between systems and the right level of abstraction at 
each part of the stack.
Organisations are increasingly adopting containers and related technologies in 
production and want to learn from others’ successes, failures and best practices. 
Hence, the theme chosen for the conference was ‘Containers in production’.
Following the talks by the experts, there was a panel discussion on using 
containers in production. The conference also featured two workshops. Open 
Source For You was the media partner for the event.
Microsoft fixes OneDrive performance issue on Linux
Microsoft has finally fixed the infamous OneDrive performance issue that was 
widely reported by Linux users. The 
slow access to OneDrive on Linux-based 
distros had downgraded the experience 
for many on the open source platform.
The performance issue was due to a 
forced Windows user-agent string on 
Chrome and Firefox based browsers 
on Linux. Some users even felt that 
Microsoft was deliberately causing performance hiccups for Linux users.
FOSSBYTES
Red Hat Gluster Storage 3.2 
lowers data integrity costs
Red Hat has announced the new 
version of Gluster Storage 3.2. The 
update comes preloaded with several 
performance improvements as well 
as deeper integration with Red Hat 
OpenShift Container Platform.
Focusing on performance 
improvements, Red Hat Gluster 
Storage 3.2 provides data integrity 
at a lower cost. There is enhanced 
integration with OpenShift Container 
Platform that brings native support for 
advanced storage services like geo-
replication and in-flight encryption for 
applications deployed in containers.
“Red Hat Gluster Storage 3.2 
marks a milestone in the life of the 
product. It brings together a number 
of innovations that are relevant to the 
modern CIO,” said Ranga Rangachari, 
vice president and general manager of 
storage, Red Hat.
Red Hat claims that the upgraded 
OpenShift Container Platform 
support can enable three times as 
many persistent volumes per cluster. 
The newly added improvements also 
offer a smaller hardware footprint 
through arbiter volumes and shrink 
infrastructure costs while maintaining 
data integrity. They also fix the 
problem of data mismatch between 
two nodes.
Customers running traditional NAS 
use cases for backup/restore can achieve 
data integrity, while hyper-converged 
configurations in branch offices can 
save on  hardware costs, power and data 
centre space. The software package also 
comes with the Nagios framework to 
monitor storage operations.
The latest Gluster Storage version 
has also fixed to scale metadata-
intensive operations for large files. 
Red Hat believes that faster metadata-
intensive operations can improve daily 
operations by 8x. The new release 
is focused on the software-defined 
storage industry.
6 | May 2017 | OPEN SOURCE FOR yOU | www.OpenSourceForU.com

FOSSBYTES
Infosys embraces open source, 
joins network community
Indian IT services giant, Infosys, has 
joined the Open Invention Network (OIN) 
as a community member. The new move 
will enable the Bengaluru-headquartered 
company to share its developments with 
over 2,000 member companies in the OIN 
community, which is funded by Google, 
IBM, NEC, Philips, Red Hat and SUSE.
“We appreciate Infosys’ leadership 
in joining OIN and believe that those 
who look to the company for inspiration, 
both in and outside of India, will see 
the wisdom it has demonstrated by 
supporting patent non-aggression 
in Linux and adjacent open source 
technologies,” said Keith Bergelt, CEO of 
Open Invention Network.
Infosys considers open source software 
as the ‘primary engine of innovation’ 
that will help boost areas such as cloud 
computing, Big Data, artificial intelligence, 
DevOps and modern Web frameworks. 
Joining OIN will be vital for the company 
to expand its IT presence by not just sharing 
its developments with other community 
members but also enabling it access to the 
intellectual property of member companies 
without paying a royalty fee.
“Infosys is actively taking open source 
and open source based products to all our 
enterprise customers, so we are extremely 
excited to become a part of OIN. We 
encourage others to join it as well and 
support this very important initiative,” 
said Navin Budhiraja, SVP and head, 
architecture and technology, Infosys.
North Carolina-based OIN is the 
leading patent non-aggression community 
to promote the deployments of Linux and 
other related open source technologies. 
Patents owned by the OIN community are 
licensed royalty-free to help companies 
expand and enhance their existing 
offerings and promote open source.
Prior to Infosys, German automaker 
Daimler had joined OIN. The parent 
company of Mercedes-Benz is aiming 
to leverage open patents to upgrade 
intelligent vehicle technologies.
Microsoft has responded to a thread in Hacker News announcing the fix. 
According to Edgar Hernandez from the OneDrive team, the StaticLoad.aspx page 
that pre-fetches resources in the background for Office apps was a less efficient 
technique causing the earlier-mentioned issue. He also clarified that the issue was 
not intentional.
“The pre-fetching optimisation was disabled, and it will be enabled again soon 
after an update for StaticLoad.aspx has been tested on Linux and released. We 
apologise for the inconvenience this may have caused,” Hernandez wrote.
Alongside announcing the fix, Hernandez promised thorough Linux testing in 
the future. Microsoft apparently wants to make sure that OneDrive is a productive 
service for as many users as possible.
SAP joins Hyperledger Project to embrace open source 
blockchain developments
SAP SE, the company known for its proprietary enterprise software offerings, is 
finally moving on the open source path and has joined the Hyperledger Project as 
a premier member. The new development will allow SAP to utilise the advanced 
blockchain technology driven by open source collaboration.
As a part of the new collaboration, Dominik Heere, vice president of innovation 
engineering, SAP Innovation Centre Network, will represent SAP’s content on the 
Hyperledger governing board. Based in Walldorf, Germany, SAP is set to enable 
blockchain technology in “existing and new business scenarios” for its enterprise 
clients through the Hyperledger 
Project. The community-driven 
blockchain developments will be 
implemented across SAP’s entire 
portfolio, including the SAP Ariba 
network.
“In joining Hyperledger, we 
plan to share our expertise and 
knowledge to help bring open 
distributed ledger technology 
to all businesses,” said Juergen Mueller, chief innovation officer, SAP, in a joint 
statement. “We believe blockchain is a transformative technology for enterprise 
businesses. We are committed to advancing its adoption via the creation of new 
standards, use cases, platforms and open applications,” he added.
Led by the Linux Foundation, the Hyperledger Project aims to develop a model 
of a commonly distributed ledger that lets organisations build and run industry-
specific applications, platforms and hardware systems to support their industry’s 
business transactions. More than 122 members have already joined the project, 
which is what seems to have persuaded SAP to also do so.
“Having support from an enterprise software and cloud leader like SAP is an 
important step in the right direction. The diversity of our members is a real strength, 
as we look to advance open blockchain technology POCs, pilots and production 
deployments across many industries this year,” said Brian Behlendorf, executive 
director, Hyperledger.
SAP is planning to enable use cases like provenance scenarios, digital object 
representations and collaborative transaction executions.
IBM also recently joined the Hyperledger Project to enter the growing  
blockchain space. The IT giant has announced its own blockchain ecosystem called 
the IBM Blockchain that is backed by the collaborative consortium.
www.OpenSourceForU.com | OPEN SOURCE FOR yOU | May 2017 | 7

FOSSBYTES
Huawei’s KunLun server to run SUSE Linux Enterprise Server
Huawei has announced its partnership with SUSE to bring SUSE Linux Enterprise 
Server as the preferred standard OS for its KunLun RAS 2.0. The open source 
platform supposedly comes with reliability, availability and serviceability features 
for the server, and the combination brings an ‘always online’ offering for enterprises.
At CeBIT 2016, Huawei and SUSE jointly launched KunLun as the world’s 
first 32-socket mission critical server. That tie-up influenced both the companies 
to move a step ahead and ultimately made SUSE Linux Enterprise Server the 
preferred standard operating system.
“Now we bring our partnership to a new level by releasing the industry’s first 
standard Linux OS that supports KunLun RAS 2.0. The OS will help KunLun 
deliver reliability that is parallel to, and even surpasses that of traditional high-end 
UNIX servers. These benefits will translate into a better return on investment for our 
customers,” said Wang Zhen, vice president for IT, server product line, Huawei.
The operating system for KunLun is based on SUSE Linux Enterprise Server 
12 Service Pack 2. It supports RAS 2.0 features that allow customers to add or 
remove CPU and memory resources without shutting down the system.
“SUSE leads the industry by releasing the preferred standard OS that supports 
KunLun RAS 2.0 features, including CPU and memory hot swap,” said Ralf Flaxa, 
president of engineering at SUSE. The SUSE team led by Flaxa is also developing 
an in-service kernel upgrade feature in order to deliver all the latest updates to the 
Linux-backed platform.
Huawei has designed KunLun to address the high reliability, performance and 
scalability requirements of mission-critical environments by leveraging the x86 
ecosystem. The company claims that the server can overcome the longstanding 
restrictions of enclosed architectures.
Google releases Android O and aims to improve battery life
While you might be waiting to test your apps on Android Nougat, Google has 
moved a step ahead and released the first developer preview of Android O. The 
focus of the new open source project is supposedly to improve battery life and 
enhance performance on compatible devices.
Battery life is one big concern on Android devices. Though Android engineers 
at Google had tried to fix various bugs on its Lollipop, Marshmallow and the recent 
Nougat versions, the issue persists — on almost all the smartphones and tablets.
But with Android O, the Sunder Pichai-led team is planning to deliver a better 
experience by enabling some automatic limits on apps running in the background. 
This advances the power optimisation that was a part of the Nougat release last year.
“Building on the work we began in Nougat, Android O places a big priority 
on improving a user’s battery life and a device’s interactive performance. To 
make this possible, we have put additional automatic limits on what apps can 
do in the background, in three main areas — implicit broadcasts, background 
services and location updates,” explains Dave Burke, vice president of 
engineering at Google, in a blog post.
The latest changes supposedly enable apps to have minimal impact on the 
hardware and this enhances the battery life.
Developers are recommended to learn the tweaks to make their apps 
compatible with the new background limits. Google has already got documentation 
ready on background execution limits and background location limits to make 
things easier for app makers.
Alongside the enhancements to improve battery life, Android O is all about an 
Google set to speed up 
Android security update 
releases
Google is working on upgrading the 
security update system for Android. 
The search giant has started working 
with third-party manufacturers and 
cell phone carriers to improve the 
speed of security updates and make 
the open source platform more secure.
Almost half of Android users 
do not receive important security 
updates. Google pushed updates to 
735 million devices last year, but 
half of them did not receive the 
patch from device manufacturers. 
Therefore, the company is aiming to 
push the updates within just a few 
days from release by working with 
carriers and manufacturers.
“We provided monthly security 
updates for all supported Pixel and 
Nexus devices throughout 2016, and 
we are thrilled to see our partners 
invest significantly in regular 
updates as well. There is still a lot of 
room for improvement, however,” 
Google’s Android security team 
members Adrian Ludwig and Mel 
Miller, wrote in a blog post.
Pushing the updates to Android 
devices, except the Nexus and Pixel 
models, is a complex process. Device 
manufacturers and carriers have 
to compile the patch to match the 
device’s requirements. 
Working with manufacturers 
and carriers to restructure the 
updates release system can certainly 
speed up the delivery schedule 
of security updates for Android. 
However, it is impossible to ensure 
the protection of the over 1.4 billion 
Android users worldwide.
8 | May 2017 | OPEN SOURCE FOR yOU | www.OpenSourceForU.com

FOSSBYTES
LibreELEC adds support for 
Raspberry Pi Zero W
The Linux community has received 
a new LibreELEC update that comes 
with native support for Raspberry Pi 
Zero W. The latest release also comes 
with a new Kodi version.
Dubbed as LibreELEC 8.0.1, the 
new version is specifically designed 
to upgrade the experience across a 
variety of hardware. The very first 
noticeable change in the update is 
the support for Raspberry Pi Zero W, 
which was recently launched with 
wireless connectivity options.
Additionally, the LibreELEC 
team has provided improvements for 
the Raspberry Pi 3 board. You can 
experience the improved software 
HEVC decoding, as well as support 
for Fe Pi audio cards and Cirrus 
Logic DAC. Users opting for the 
Kodi media player can also enjoy the 
advantage of the new maintenance 
release and receive its fresh build 
with fixes to upgrade the user 
experience.
The LibreELEC 8.0.1 release 
includes Linux 4.9.13 on generic, 
Raspberry Pi and Raspberry Pi 2 
devices. However, on the Amlogic 
aarch64, the update comes with the 
dated Linux 3.14 kernel.
You can download LibreELEC 
8.0.1 directly from the official 
LibreELEC website. It can be 
installed through an SD card or a 
USB drive.
upgraded experience. There are new notification channels that work to enable app-
defined categories for notification content. The operating system also works with auto-
fill APIs to add login details and repetitive information automatically to certain forms.
The Android O additionally includes some root-level changes to improve the 
existing user experience. There are font resources in XML as well as adaptive icons 
and a wide gamut of colour capabilities for apps.
Google has also provided a large number of Java 8 language APIs and runtime 
optimisations to take on other mobile platforms including iOS. You can download 
the very first Android O Developer Preview on your Nexus or Pixel device. The new 
version comes with an updated SDK to make development even easier. Moreover, an 
emulator has been specifically designed to test Android Wear 2.0 apps on Android O.
A manual download and Flash is currently the only available channel to install the 
O release on compatible devices. But you should only install the preview to test your 
development skills, as some hidden bugs can affect the performance of your device.
Orgs, the team collaboration software from npm, becomes free 
for open source developers
npm Inc., the parent company of Node.js, has announced that it has made its 
developer collaboration tool free for open source projects. Called Orgs, the tool is 
popular among JavaScript and Node.js developers.
The free version is only available for open source projects. But those using 
Orgs for private packages still need to 
opt for the premium subscription of US$ 
7 per month/per user.
npm has decided to offer the free 
version to support development in open 
source. “Making it easier to collaborate 
on open source projects benefits the 
whole community, because anything 
that reduces friction in software development makes it easier for everyone to build 
amazing things,” said Isaac Z. Schlueter, CEO of npm Inc., in a statement.
With this development, open source teams can now use advanced capabilities 
like role-based access control, package search and semantic versioning for free. 
npm can simplify the particular kind of dependency between two software packages 
or modules in any project. Besides, Orgs can help users share and manage code 
packages among team members.
Netflix debuts on Linux with HTML5 video playback
A long time after its arrival on proprietary platforms, media streaming service 
Netflix has finally arrived on Linux. Mozilla’s Firefox browser has received 
Netflix’s official support to play premium HTML5 content.
While Google Chrome had enabled Netflix video playback support on Linux 
back in 2014, Firefox has become the first Web browser to deliver premium video 
content in HTML5 on the open source operating system.
“Starting today, users of Firefox can also enjoy Netflix on Linux. This marks 
a huge milestone for us and our partners, including Google, Microsoft, Apple and 
Mozilla, that helped make it possible,” the Netflix team, comprising engineers 
Nicholas Eddy, Matt Trunnell and Kevin Gallagher, reported in a recent blog post.
In 2013, Netflix added HTML5 support for premium videos to deliver an 
improved experience over the previously supported Silverlight. It also omitted the 
need for plugins to enable video playback.
www.OpenSourceForU.com | OPEN SOURCE FOR yOU | May 2017 | 9

FOSSBYTES
“Our excitement about HTML5 video has remained strong over the past four 
years. Plugin-free playback that works seamlessly on all major platforms helps us 
deliver compelling experiences no matter how you choose to watch,” the Netflix 
engineering team reported.
Going forward, Netflix is aiming to enhance video streaming using 4K (Ultra-
HD) resolution as well as HDR support. Hardware such as Netflix-supported 
televisions and Google Chromecast are already supporting Dolby Vision and 
HDR10 formats. These features are likely to reach Linux systems following their 
debut on Windows and Mac machines.
In the meantime, you can start watching Netflix videos on your Linux OS by 
installing at least Firefox 47. The update is initially bringing out content in 720p 
(HD), though it would be expanded to 1080p at a later stage.
NEC joins FIWARE Foundation to add open source flavour to 
smart city systems
NEC Corporation, the company behind major IT-integrating network technologies, 
has  joined the non-profit FIWARE Foundation as a platinum member. This is to 
enable open source technologies for smart city focused systems.
FIWARE works as an open 
source middleware platform – 
designed to promote the cross-
industry use of data and promote 
service collaborations among the 
companies and municipalities that 
provide public services. The solution 
was developed under the European 
Union’s Future Internet Public-
Private Partnership (FI-PPP) project. 
It has been adopted by a large number of cities and companies across Europe and 
some other parts of the world.
“The FIWARE Foundation is a visible commitment of European businesses to 
bring more innovative Internet services to consumers, citizens, businesses and the 
public sector,” said Ulrich Ahle, CEO of the FIWARE Foundation. “We welcome 
NEC as a platinum member of the FIWARE Foundation, and are looking forward to 
writing the future story of FIWARE together with NEC,” he added.
NEC will join the board of directors and technical steering committee of the 
FIWARE Foundation that serves the community as the decision-making authority 
for FIWARE technologies. The NEC team will also contribute to development, 
standardisation and promotion of FIWARE technology to accelerate smart city and 
smart industry businesses utilising Internet of Things (IoT).
“NEC will provide advanced social solutions and contribute to the realisation of a 
super smart society (Society 5.0), for which FIWARE’s unique capabilities, including 
cross-industrial use of data and service collaborations, will play a crucial role,” said 
Yasunori Mochizuki, senior vice president of IoT strategy, NEC Corporation.
The Germany-headquartered FIWARE Foundation is empowering some of 
the leading smart city projects worldwide. There are FIWARE-based solutions in 
Santander, Spain, to advance the waste collection management service in the city. 
Wellington in New Zealand has also deployed the open source software to forecast 
traffic volumes and analyse environmental situations.
NEC is set to leverage FIWARE to develop a model for industrial waste 
collection services in Kawasaki City in Japan. This would be the first project on 
Japanese soil to use the advanced middleware solution.
10 | May 2017 | OPEN SOURCE FOR yOU | www.OpenSourceForU.com

FOSSBYTES
Microsoft brings Black Duck open source tools to Visual Studio
Microsoft is integrating the automated, open source code management program 
Black Duck Hub with its Visual Studio package. Black Duck (based in 
Massachusetts in the US) has also joined the Microsoft Visual Studio Program as a 
premier level partner.
Black Duck Hub is capable of enhancing the code through software 
composition capabilities and offers a large amount of data to identify security holes. 
KnowledgeBase, the database behind Black Duck Hub, includes data on more than 
2 million open source projects and over 79,000 known vulnerabilities. This enables 
the program to scan your code and look for different security issues.
“Microsoft recognises the importance of open source in application 
development and the many economic and productivity reasons for its rapidly 
expanding use. We are pleased that Microsoft also sees value in bringing Black 
Duck’s open source licence and security compliance capabilities to the Microsoft 
Visual Studio continuous integration platform,” said Lou Shipley, CEO, Black 
Duck, in a joint statement.
Black Duck Hub helps you prioritise and track your efforts. Also, the code-
checking service will be able to guide you on whether the open source code used in 
your project has any licensing issues.
The Visual Studio extension of Black Duck Hub scans for open source code 
in your TFS (Team Foundation Service) and TS (Team Services) builds. It scans 
the code for security vulnerabilities and other components for licence compliance 
issues. The tool also allows you to monitor the code written by your employees.
Black Duck and Microsoft are offering Hub support to developers with a 14-day 
free trial. A video tutorial has also been produced to ease the integration process.
If you do not want to analyse your code on Visual Studio, you can use Black Duck 
Hub on programming tools developed by IT giants like HPE and IBM. However, 
Microsoft has also convinced the Black Duck team to join its Visual Studio Partner 
Program to extend the service access for its integrated development environment.
IBM, Red Hat partner to develop open source hybrid cloud
IBM and Red Hat have announced a strategic collaboration to build an open source 
hybrid cloud based on OpenStack. The new partnership will persuade enterprise 
customers to opt for IBM Private Cloud to install the Red Hat OpenStack Platform 
and Red Hat Ceph Storage.
As a result of this move, IBM and Red Hat will jointly market and sell their 
cloud deployments. “Our collaboration with IBM is aimed at helping enterprise 
customers more quickly and easily embrace the hybrid cloud,” said Radhesh 
Balakrishnan, general manager of OpenStack, Red Hat, in a joint statement.
“Now, customers who do not have the in-house expertise to manage OpenStack 
infrastructure can more confidently consume Red Hat OpenStack Platform and Red 
Hat Ceph Storage on IBM Private Cloud,” Balakrishnan added. Through the latest 
deal, IBM and Red Hat are set to offer a hybrid cloud infrastructure with OpenStack 
APIs. Both the companies are also set to enable their clients with a workload 
migration option directly via Red Hat Cloud Access.
“The strategic collaboration between IBM and Red Hat is designed to 
enable clients to more easily adopt open source products and OpenStack cloud 
software while preserving their existing investments and creating new business 
opportunities,” said Zane Adam, vice president for IBM Cloud.  IBM has been 
looking to expand in the cloud space for a long time. The company partnered 
with Dell EMC-owned VMware last year, and is already running the VMware 
management platform on its cloud.
Audacity 2.1.3 comes with 
new scrubbing features
Audacity, the open source audio 
editing software, has been updated to 
version 2.1.3. The new maintenance 
update comes with scrubbing features 
to ease the editing process.
One of the first changes that you 
will notice in Audacity 2.1.3 is the 
Scrub Ruler and Scrub Toolbar to 
improve the scrubbing experience. 
There is also a new generator for 
Sample Data Import, and you can 
create a higher quality audio file 
using Change Pitch and Change 
Tempo effects.
The new Audacity version is 
powered by the SBSMS algorithm 
that ensures lossless content and 
unchanged selection length. There is 
also an updated click track generator 
that comes with Swing amount 
control. In addition to the major new 
features, the updated Audacity has 
included a new timer record function. 
The software also sends you an alert if 
your system is low on disk space.
Additionally, Audacity has added 
a function that can save you from 
accidentally cancelling a recording. 
There are also tweaks to help you mix 
MP3 files while exporting.
The Audacity team has developed 
some new keyboard shortcuts as well. 
You can easily move to the next or 
previous label by pressing the Alt + 
Right or Alt + Left key combinations. 
Similarly, the shortcut for Play/Stop 
and Set Cursor has been changed from 
Shift + A to just X.
www.OpenSourceForU.com | OPEN SOURCE FOR yOU | May 2017 | 11

FOSSBYTES
Likewise, as open source software is dominating the cloud space, IBM aims 
to succeed in the market by partnering with its market leaders. Hybrid cloud 
application management company, Cloudsoft, is already using the combined service 
by deploying its Red Hat Enterprise Linux workloads in IBM’s cloud solution. This 
use case is likely to be expanded over time.
“A cloud-first strategy has become the new normal for a majority of our 
enterprise clients worldwide who are leveraging IBM Cloud as a key driver for 
digital transformation,” Adam said.
Microsoft’s Project Olympus enlightens open source cloud path
About four months after its official launch, Project Olympus by Microsoft has 
finalised its future moves and expanded open source hardware development by 
adding semiconductor manufacturers like 
NVIDIA, Intel, Qualcomm, AMD and 
Cavium. The new development is aimed at 
bringing full-fledged cloud hardware closer 
to the open source community.
Microsoft had announced Project 
Olympus in October last year to deliver a 
next-generation cloud hardware design, based on the company’s massive belief 
in the success of open source. While the initial model is yet to be finalised, the 
Redmond company now plans to make open source cloud hardware a reality.
Microsoft is working closely with the Open Compute Project (OCP) team 
to enable its Project Olympus as an open source cooperative standard for server 
hardware that will provide both cloud and enterprise services. At the 2017 OCP US 
Summit, the software giant announced that Project Olympus had “attracted the latest 
in silicon innovation” — offering an advanced model for emerging cloud workloads 
such as Big Data analytics, machine learning and artificial intelligence.
To reach a large number of projects, Project Olympus standards will be seen 
in next generation Intel Zeon and AMD processors. The hardware standard will 
also support ARM processors by Qualcomm and Cavium. Interestingly, this 
will open up new verticals since ARM-based chips have till now never been 
associated with Microsoft software.
“This is a significant moment as we usher in a new era of open source 
hardware development with the OCP community. We intend Project Olympus to 
provide a blueprint for future hardware development and collaboration at cloud 
speed,” said Kushagra Vaid, general manager for Azure hardware infrastructure, 
Microsoft, in a statement.
Microsoft aims to build hardware that is robust, flexible as well as scalable in 
terms of capacity. The best part is, this standard is CPU independent. Moreover, 
the collaborative agreement between Project Olympus and the chipmakers will 
ensure this independence.
Apart from supporting advanced hardware, Microsoft is projecting a strong 
and quantifiable foundation for cloud software development with Project Olympus. 
Azure is expected to enable this software foundation through some of the recent 
open source projects.
The rapid rate of cloud adoption is already indicative of the fact that the 
future of computing is on the cloud. Therefore, a universal standard like Project 
Olympus is likely to help software makers be less dependent on hardware 
drivers and CPU specifications.
For more news, visit www.opensourceforu.com
Asttecs launches open 
source CRM suite
Months after delivering a community-
backed communication solution, 
Asttecs has launched its open 
source CRM application suite. The 
new development, called AstCRM, 
is designed to enable enterprises 
to manage all their customer 
engagement needs using the power of 
open source.
“Leveraging expertise in open 
source, our objective is to provide 
customers with best-of-breed 
enterprise-grade applications, while 
focusing mainly on the must-have 
features of a customer relationship 
management solution and major 
workflows,” said Manjunath R.J., 
CEO of AstCRM.
Asttecs is targeting the CRM 
package at large enterprises and SMEs 
(small and medium enterprises). 
The solution can be used to improve 
prospect and customer experiences 
by automating sales, marketing 
and support tasks, while delivering 
business-enhancing insights.
To accomplish the customer 
relationship process, AstCRM 
includes major applications for tasks 
such as lead management, contact 
management, account management, 
ticket management, role management 
and activity management.
The suite allows users to access 
customer data via a Web browser. 
There are integrated plugins for 
Outlook, Thunderbird, Firefox and 
Gmail to let users configure the CRM 
using their email accounts.
12 | May 2017 | OPEN SOURCE FOR yOU | www.OpenSourceForU.com


Audio communications equipment 
manufacturer, Plantronics, has 
unveiled its latest over-the-ear 
wireless headphones, the Backbeat 
Pro 2, which offer good audio 
quality and some smart features.  The 
music pauses automatically once the 
headphones are removed and resumes 
when they are put on again.
The headphones come with on-
demand active-noise cancellation, 24-
hour battery life, along with Plantronics’ 
signature audio technologies like rich 
bass, crisp highs and natural mid-
tones. They are equipped with Class 1 
Bluetooth, which gives an impressive 
100 metre range when paired with any 
Class 1 Bluetooth device.
The headphones’ smart power 
management technologies offer up 
to 24 hours of non-stop playback 
on a single charge, up to 21 days 
of standby power and up to six 
new products
Wireless headphones 
from Plantronics
Server SSDs from 
Kingston
months of deep sleep hibernation if 
inadvertently left on.
The Backbeat Pro 2 comes with 
additional features like multi-device 
connectivity to connect up to two devices 
simultaneously and to switch between 
them easily. Built-in dual microphones 
enable the user to switch into a phone call 
while using the headphones.
The Plantronics headphones are 
travel-friendly as they are designed with 
fold-flat ear-cups and a protective sleeve. 
They are available via online retail stores.
Address: Plantronics India Pvt Ltd, 
608, 6th Floor, Vipul Square, Sushant 
Lok, Phase 1, Gurugram – 122002;  
Ph: 91-124 4105192/93
Waterproof 
Bluetooth 
speakers from 
Ultimate Ears
Custom in-ear headphones manufacturer 
and Logitech brand Ultimate Ears has 
launched its latest Bluetooth portable 
speaker, the UE Wonderboom. The 
device is said to offer 360-degree 
sound for up to 10 hours on a single 
charge. It enables users to enjoy non-
stop music and can be fully charged in 
less than three hours.
Company sources claim that the 
speaker can withstand a drop from 
a height of up to 1.52m (5 feet). It 
features a hanging loop on top, through 
which it can be attached to a bag, cycle, 
etc. The music can be controlled via 
the speaker itself, which lets the user 
control the volume and skip a song. The 
speaker comes with IP67 certification, 
making it dust- and water-resistant. It 
can be immersed in up to 1 metre of 
liquid for 30 minutes, making it perfect 
for poolsides, beaches or rainy days.
Supporting a mobile range of 
up to 33 metres, the device can be 
paired with approximately eight other 
Bluetooth enabled Wonderboom 
speakers, and can be connected to two 
source devices.
The UE Wonderboom speakers are 
available in grey, black, red, blue and 
pink via online and retail stores.
Address: Logitech Electronics India 
Pvt Ltd,  601, Raja House, 30-31, 
Nehru Place, New Delhi – 110019;  
Ph: 011- 47306600
Kingston, the multinational computer 
technology corporation that develops 
and manufactures Flash memory 
products and other memory devices, has 
unveiled its latest cloud based storage 
device – the DC400 SSD.
Ideal for read-intensive applications 
such as boot, Web servers and lower data 
rate, operational databases and analytics, 
the DC 400 SSD offers good quality of 
service for data centre customers.
The device features enterprise-class 
reliability with end-to-end data path 
protection and firmware implemented 
power-loss-protection (pFAIL). The SSD 
is a good front-loading server storage 
option, being a combination of high 
IOPS, low latency and advanced data 
protection. It is available in capacities 
Address: Kingston India, Liaison 
Office, 808, Palm Spring Centre, 
Above Croma, Link Road, Malad West, 
Mumbai – 400064; Ph: 91-22-42230300
of 400GB, 480GB, 800GB, 960GB, 
1.6TB and 1.8TB.  The 400GB, 800GB 
and 1.6TB capacities are performance 
optimised with greater IOPS for faster 
application performance.
The SSDs are available online and 
at retail stores.
Price:  
 ` 7,000 
Price:  
 ` 19,999 for 480GB  
` 35,000 for 960GB and 
` 36,000 for 1.6TB
Price:  
 ` 13,000
14 | May 2017 | OPEN SOURCE FOR yOU | www.OpenSourceForU.com

edges, which gives the phone a 
premium look.
The Dual 5 is powered with an 
octa-core Qualcomm Snapdragon 652 
processor, coupled with 4GB of RAM 
The prices, features and specifications are based on information provided to us, or as available on various 
websites and portals. OSFY cannot vouch for their accuracy. 
Dual camera smartphone from Micromax
Compiled by: Aashima Sharma
Address: Karbonn Mobiles Pvt Ltd, 
D-170, Okhla Industrial Area, Phase 1, 
New Delhi-110020; 
Website: support@jainaindia.com 
Mobile device manufacturer, Karbonn 
Mobiles, has introduced two new 4G 
enabled smartphones — the Karbonn 
Aura Sleek 4G and Aura Note 4G, 
both offering the latest features at an 
affordable price, making them quite 
popular among the youth.
The Aura Sleek 4G comes with 
a 12.7cm (5 inch) FWVGA IPS full 
lamination display, perfect for video 
streaming and games. It is powered 
by a 1.1GHz quad-core processor 
with an internal memory of 1GB 
RAM and 8GB ROM.  Backed 
with a 2000mAh battery, the device 
runs on Android 6.0 Marshmallow, 
comes with a microSD card port 
and supports up to 32GB of external 
memory. As for the camera, the Aura 
Karbonn’s pocket-friendly 
4G smartphones
Indian consumer electronics company, 
Micromax, has made a strong 
comeback with the launch of its latest 
smartphone, the Dual 5, which sports 
a dual camera on the rear. The device 
sports twin 13 megapixel sensors on 
the rear, both with an f/1.8 aperture. 
One sensor captures in monochrome 
and the other in colour. Apart from 
this, there is the 13 megapixel front 
camera for selfies.
Apart from its imaging 
capabilities, the flagship device 
features a 13.9cm (5.5 inch) full 
HD super AMOLED display with 
2.5D Corning Gorilla Glass 3 for 
protection. A fingerprint sensor on 
the rear panel recognises fingerprints 
within 0.2 seconds, according to 
company sources. It sports an all-
metal unibody design with chamfered 
Sleek comes with a 5 megapixel rear 
and 2 megapixel front camera.
The Karbonn Aura Note 4G is the 
first fingerprint sensor device from the 
company. It comes with a 13.9cm (5.5 
inch) HD screen, 720x1280 display and 
IPS full lamination. With a 1.25GHz  
quad core processor, the device is 
backed with a huge 2800mAh battery, a 
flashlight, 2GB RAM and 16GB ROM. 
It runs on Android 6.0 Marshmallow 
and has an SD card port with memory 
expandable up to 32GB. It offers front 
and rear 5 megapixel cameras.
The Aura Sleek is available in 
champagne white, black and grey, while 
the Aura Note 4G is available in premium 
finishes of matte black and metallic 
champagne via online and retail stores.
and 128GB of onboard storage, which is 
further expandable via a microSD card. It 
runs on Micromax’s Qiku 360 interface 
with Android 6.0 Marshmallow. 
Powered by a 3,200mAh battery, 
the smartphone comes with Quick 
Charge 3.0 which can charge the 
phone to full capacity in just 45 
minutes. The connectivity options of 
the device include 4G VOLTE, Wi-Fi 
802.11ac, Bluetooth v4.1, a 3.5mm 
audio jack and USB Type-C along 
with sensors like an accelerometer, 
an ambient light sensor, a gyroscope, 
infrared sensor, etc.
The Micromax Dual 5 is available 
via online and retail stores.
Address: Micromax House, 90 B, 
Sector 18, Gurugram – 122015; Ph: 
91-124-4811000
Price:  
 ` 24,999 
Price:  
 ` 5,290 for Aura
Sleek and ` 6,890 
for Aura Note
www.OpenSourceForU.com | OPEN SOURCE FOR yOU | May 2017 | 15

CODE
SPORT
Sandya Mannarswamy
16 | May 2017 | OPEN SOURCE FOR yOU | www.OpenSourceForU.com
W
hile we have been discussing deep 
learning in the last few columns, a few 
readers asked me to share interview 
questions on this topic as well as on general machine 
learning. So, in this month’s column, we focus on 
some of the common interview questions, and I 
request readers to send me your email replies to these 
questions. I will feature them in our next column 
when we discuss the answers to these questions.
1.  In last month’s column, we had discussed 
auto encoders and how they can be used for 
dimensionality reduction.  I had mentioned that it 
is possible to achieve a dimensionality reduction 
of the input representation by having a hidden 
layer of smaller dimensions compared to the input 
representation dimensions. Now the question to 
our readers is the following: Is it always necessary 
to have a hidden layer of smaller dimensions in 
an auto-encoder? If you build an auto-encoder 
in which you have the hidden layer whose 
dimensions are larger than the input layer, can you 
justify why you chose to do so?
2. Continuing on the subject of auto-encoders, why do 
we really need them for dimensionality reduction? 
Most of you would be familiar with dimensionality 
reduction techniques such as Principal Components 
Analysis (PCA).  What is the advantage of using 
auto-encoders for dimensionality reduction of data 
compared to a PCA based approach? (Clue: Think 
of linear vs non-linear.)
3. Given that deep neural networks need large 
amounts of training data to generalise well, 
can you explain methods by which you can 
artificially increase the size of your training data? 
(Clue: Think of transformations such as scaling, 
rotation and translation. While it may be obvious 
in the case of image data sets to employ these 
transformations to increase the size of training 
data, what kind of transformations can you 
employ in the case of text data?)
4. Regularisation techniques are widely used 
to avoid over-fitting of the neural network 
parameters to training data so that the 
constructed network can still generalise well for 
unseen test data. Can you explain the differences 
between L1 regularisation and L2 regularisation 
methods? When would you use the first option 
instead of the second?  If you change the network 
learning rate, does it impact the choice of your 
regularisation parameter lambda?
5. You have been given a neural network and 
training data of size N.  How would you 
determine whether the constructed network 
suffers from the problem of over-fitting? 
6. One of the several other techniques used to avoid 
over-fitting is employing drop-out while training a 
neural network. In simple terms, drop-out implies 
having a percentage of neurons inactive while 
training the network for a fixed number of mini-
epochs and then repeating this process a number 
of times, having a different set of neurons inactive 
each time.  After completing training, the learnt 
weights of the neurons are scaled down. Can you 
explain why this is needed? Also, can you explain 
the intuition behind drop-out in terms of why it 
can help avoid over-fitting?
7. One way of thinking about the drop-out technique 
is that by having a certain percentage of neurons 
inactive for each learning phase, we actually train 
a different neural network and then combine these 
different networks to obtain the final network.  
Can you think of any other technique in machine 
learning whereby you can combine different weak 
learners to build a strong learner? 
8. Different activation functions can be employed 
for the neurons such as rectilinear, tan-h, 
sigmoid, etc. Can you explain the factors 
involved in the choice of activation functions? 
Are there any criteria that a function needs to 
satisfy, for it to be considered as an activation 
In this month’s column, we continue our discussion on deep learning. 

Guest Column
CodeSport
www.OpenSourceForU.com | OPEN SOURCE FOR yOU | May 2017 | 17
By: Sandya Mannarswamy
The author is an expert in systems software and is currently 
working as a research scientist at Xerox India Research 
Centre. Her interests include compilers, programming 
languages, file systems and natural language processing. 
If you are preparing for systems software interviews, you 
may find it useful to visit Sandya’s  LinkedIn group Computer 
Science Interview Training India at http://www.linkedin.com/
groups?home=HYPERLINK “http://www.linkedin.com/group
s?home=&gid=2339182”&HYPERLINK “http://www.linkedin.
com/groups?home=&gid=2339182”gid=2339182
function in a neural network trained using back 
propagation?
9. In a neural network built for classification problems, 
the output layer typically has neurons whose activation 
function is a softmax activation function. Can you explain 
why you should use a softmax function for the output layer 
of a classification network? (Clue: Think about how the 
softmax function is defined and how it can be interpreted 
as a probability.)
10. The back-propagation algorithm is the anchor behind the 
ability to train very large and deep neural networks. Can 
you explain the intuition behind the back-propagation 
algorithm? Instead of using back-propagation, I design a 
method wherein I disturb the weight of one of the arbitrary 
neurons in the network and measure its impact on the cost 
function, to determine the gradient of the cost function with 
respect to that weight.  While this approach certainly seems 
simple compared to the complicated back-propagation 
algorithm, explain why this is not a scalable approach? 
11. Recall the fact that back-propagation is built on the 
premise of computing the gradient of the cost function with 
respect to each of the weights, and using the computed 
gradient to update the weights for the next pass. What are 
the criteria a reasonable cost function needs to satisfy for 
back-propagation to be applied? 
12. Different cost functions can be applied for a neural network 
such as the quadratic cost function, the cross entropy cost 
function, etc. Can you explain why you may prefer a cross-
entropy cost function over a quadratic cost function? (Clue: 
Think of the slowdown in learning for a quadratic cost 
function when a neuron is near saturation.)
13. Continuing the question of cost functions, very often people 
use a sigmoid activation function for the output layer along 
with a cross-entropy cost function; or a softmax output layer 
with log likelihood cost function. Can you use a softmax 
layer with the cross-entropy cost function? Explain the 
reasons behind your choice. (Clue: Think of interpreting the 
softmax layer output as a probability distribution.)
14. How can you compare the performance of two different 
machine learning algorithms? For example, consider that 
you have been given two learners – one based on support 
vector machines and another that uses neural networks for 
classifying documents. Let us also assume that the SVM 
based learner performs better than the neural network based 
learner when the training data size is M, whereas the neural 
network performs better than SVM when the training data 
size is much less than M, but is poorer in performance by 
a few points when comparing training data sizes of M and 
above. Which training algorithm would you prefer in that 
case, and can you explain the rationale behind your choice?
15. When we discussed neural networks in our previous 
column, I did not go into much detail about the hyper-
parameters of the learning framework such as the learning 
rate, number of mini-epochs, etc. Can you explain some 
of these hyper-parameters and how they impact the 
classification test accuracy? How would you choose the 
appropriate values for the hyper-parameters?
One of the best resources for learning about the 
basic concepts of neural networks is the free online book 
from Michael Nielsen available at http://neuralnetwork 
sanddeeplearning.com/index.html.  I would suggest that our 
readers go through this short book before they attempt to read 
through the voluminous deep learning text book from Prof. 
Bengio et al, available at http://www.deeplearningbook.org/. I 
like Nielsen’s book, especially because he has provided short 
Python code snippets for different concepts and you can try 
them out quickly as you read through the chapters.  Another 
great resource to get  familiar with deep learning concepts 
is the repository of articles and tutorials available at https://
deeplearning4j.org/tutorials. 
Given the enormous number of papers and articles 
available in this space, it can be very confusing for a novice 
computer science student who wants to gain expertise in 
deep learning. While there are online courses available 
from Coursera (Prof. Hinton’s course on neural networks) 
and Udacity (a deep learning nano-degree), I would suggest 
that you  also read through one of the above-mentioned 
text books in order to get the mathematical concepts clear, 
instead of just viewing neural networks as a black-box 
machine learning algorithm. This would allow you to 
understand and appreciate the application of different neural 
networks to different problems. For instance, you may want 
to dig deeper into why convolutional networks, which were 
originally envisaged for image recognition, are also useful 
in text processing tasks such as sentence classification or 
document classification. More importantly, as you use some 
of the standard neural net architectures for specific text 
processing problems such as sentiment analysis or topic 
modelling, you will understand how different architectures 
need to be tuned for different tasks.  We will discuss more 
about this in our next column.
If you have any favourite programming questions/
software topics that you would like to discuss on this forum, 
please send them to me, along with your solutions and 
feedback, at sandyasm_AT_yahoo_DOT_com. Till we meet 
again next month, wishing all our readers a wonderful and 
productive month ahead! 

Guest Column
Exploring Software
18 | May 2017 | OPEN SOURCE FOR yOU | www.OpenSourceForU.com
 BalancerMember unix:///home/diaspora/diaspora/tmp/
diaspora.sock|http:// 
</Proxy>
# SSLEngine On
 SSLCertificateFile /etc/pki/tls/certs/localhost.crt
 SSLCACertificateFile /etc/pki/tls/certs/ca-bundle.crt
 SSLCertificateKeyFile /etc/pki/tls/certs/localhost.key
Once the installation is successful, you can start the 
browser and explore further.
Usage
Sharing concepts are very similar to how it is done on G+ 
(Google Plus). Relationships are asymmetric. You create 
‘aspects’, which are identical to circles in G+. You add 
people you know to one or more aspects. They will be 
notified that you are sharing posts with them. However, it 
is up to them to decide whether to include you in an aspect 
or not. If they don’t, the posts you share with them will not 
show up in their stream.
You can share your posts with one or more aspects, or 
the public. However, your post will go to a user’s stream, 
provided this user is following you. This means that you 
need to be one of this user’s aspects.
Re-sharing of posts is tighter. Only public posts can be 
re-shared. You need to keep in mind that someone may re-
share your private post by using copy-paste.
In addition to seeing posts from people you follow, you 
may also view posts based on your interests. This is similar 
to the micro-blogging sites. You can specify your interests 
by selecting a list of hashtags. A public post that contains 
one of the hashtags in your list of interests will also be 
shown on your stream.
Networking 
Diaspora pods do not synchronise, which is fine. There is 
no reason for a pod to keep track of all the traffic. If a user 
on one pod is following a user on another pod, then the 
relevant posts will be forwarded. However, if the receiving 
This month, the author explores Diaspora software, which 
is free and can be used to set up non-profit distributed 
social networks. A group of independently owned nodes can 
interoperate to form a network. 
Diaspora: The Free, 
Distributed Social Network
D
iaspora (wiki.diasporafoundation.org) is a Ruby on 
Rails application for social networking, designed 
to run as a federated system. Each Diaspora server 
is called a ‘pod’, and you can join one of the public pods. 
There are a few with the ‘in’ domain as well. You can select 
a pod using the guidelines on the Diaspora wiki or use the 
site https://podupti.me/.
In this article, we will explore how to set up a personal 
pod. This may be useful in an organisation or a group, as it 
can encourage sharing and networking between colleagues. 
An additional advantage is that, as it is open source, code 
can be added to make it more useful for any special needs 
of the common interest group.
Installation
The installation of Diaspora is complex but not very 
difficult. Instructions are available on the wiki for various 
Linux distributions as well as for MacOS and for Windows.
The instructions for Fedora 22 are fine for Fedora 25 as 
well, with some minor differences.
The following line created a little confusion:
curl -L https://s.diaspora.software/1t | bash
I misread ‘1’  in ‘1t’ as the letter L and not the 
number 1!  
For testing, you can use a dummy URL and self-signed 
certificates. However, for production, you will need valid 
certificates and a valid domain. The dummy URL should be 
in the /etc/hosts file so that the browser can find it.
The configuration files are in .yml and, hence, 
indentation is as critical as in Python code.
To get the Apache reverse proxy to work, you need 
to make sure that mod_ssl is installed. Modification 
of the sample configuration file for Apache is fairly 
straightforward. You need to take care of the file names, 
e.g., for the proxy and SSL settings, as follows:
<Proxy balancer://upstream> 
Anil Seth

Guest Column
Exploring Software
www.OpenSourceForU.com | OPEN SOURCE FOR yOU | May 2017 | 19
pod is not operational for an extended period, these posts 
may not be delivered.
You will see the public posts related to hashtags from 
your own pod. In addition, your pod can be configured to 
receive public posts from a relay server. The scope can be 
for all the posts or just for a list of hashtags. The pod may 
also opt to send public posts to a relay server. The default 
options are false.
Alternatives
The primary open source alternative to Diaspora is 
Friendica. It is written using the LAMP stack and has 
built-in support for the Diaspora protocol. Hence, users 
on Friendica and Diaspora can share messages with 
each other. The site https://the-federation.info/ gives 
information about the servers running the Diaspora 
protocol. Another interesting project supporting the 
Diaspora protocol is Hubzilla.
Is there any hope for the alternatives? Probably yes, 
By: Dr Anil Seth
The author has earned the right to do what interests him. You 
can find him online at http://sethanil.com, http://sethanil.
blogspot.com, and reach him via email at anil@sethanil.com.
as the ‘List of social networking websites’ on Wikipedia 
illustrates. Other social networks may be small but can still 
serve a significant social need.
If pods are hosted by common interest groups but are 
still part of a federated environment, it would make it 
easy for people to join and find others whom they wish to 
share their interests with. Since advertising revenue is not 
at stake, the numbers do not need to be absurdly huge in 
order to be successful.
Diaspora and Facebook were created by college 
students! So, colleges seem to be the ideal places to set up 
federated servers and change the world. 

When we think about programming, it’s C++ that first comes to mind. That’s because of 
the immense popularity and broad applicability of the general-purpose programming 
language that  debuted back in 1983. Since then, it has influenced many modern 
languages, including C#, D and even Java. But what makes C++ important in the world 
of generation-next computers and devices like smartphones and embedded hardware? 
Jagmeet Singh of OSFY tries to get the answer to this question in a conversation 
with none other than the creator of C++, Bjarne Stroustrup. Edited excerpts...
and my colleagues. However, the 
range of projects, the demands and the 
hardware at Bell Labs were very wide, 
so the language had to be very flexible 
to cope with all of that.
Q 
What were the prime 
challenges you faced when 
building C++?
There were many challenges because 
I was building a tool for practical use, 
rather than as an academic project 
for publication. A tool has to be good 
enough for everything its users need; 
just being the best in the world for 
one or two things is not sufficient 
for success.
The very first challenge that I 
faced was the language’s design. The 
question that arose was what features 
do my colleagues and I need in order to 
simplify our code. Implementing those 
features so that they were affordable 
to use in real-world development and 
execution environments was also quite 
hard, initially.
Once the language was ready, 
its installation and portability on a 
variety of hardware and operating 
systems was quite tough. Similarly, 
educating people on how to use the 
new techniques and language features 
was also a big challenge.
It was not all that easy because for 
the first years I was the only person 
working on ‘C with Classes’. My 
colleagues were most supportive, but 
there was no official C++ project with a 
budget; basically, the help I offered to a 
range of Bell Labs projects allowed me 
to develop C++.
Q 
Why was there a need for 
C++ when C already existed 
in the computing world?
I built C++ on C because I did not 
want to build from scratch. That 
would not have resulted in a useful 
tool within a reasonable time frame. 
However, C could not, and still cannot, 
manage complexity as well as C++. 
The C language, and how it is used, 
Q 
What prompted you to 
develop C++?
I needed a language that could be 
used to manipulate hardware directly 
and use all the available hardware 
resources well. I also needed a 
language that allowed me to handle 
complexity. Though C could be used 
in manipulating hardware and Simula 
could handle complexity, there wasn’t a 
language that could do both. Therefore, 
I started to build one, by adding 
Simula’s class ideas to C.
Q 
For whom did you build the 
initial C++ model?
I wanted that language for myself, to 
help build a distributed system based 
on UNIX. Before I even finished my 
language, my friends and colleagues 
at Bell Labs started to use it for their 
projects, often simulation projects 
because the very first library I built 
for ‘C with Classes’ (as I called my 
language) was a co-routine library. 
I built C++ primarily for myself 
“ I built C++ 
primarily for myself and 
my colleagues
For U & Me
Interview
20 | May 2017 | OPEN SOURCE FOR yOU | www.OpenSourceForU.com

 Bjarne Stroustrup,  
creator of C++
has evolved over the years -- often 
under the influence of C++. When 
I started with ‘C with Classes’, the 
use of sets of functions as opaque 
interfaces (which today is C’s most 
effective abstraction mechanism) was 
not common. It has been conjectured 
that this developed partly in response 
to the classes and virtual functions of 
C++.  K&R C did not offer function 
prototypes, //, const, inline and more; 
those came from my work with 
C++. Even GCC (GNU Compiler 
Collection) that was first released in 
1987 is a C++ program now.
Q 
How has C++ evolved in the 
programming space?
For the first 10 years or so, the user 
population of C++ doubled every 7.5 
months. Currently, there are about 4.5 
million users of C++, and that number is 
growing by about 100,000 a year.
Moreover, C++ is used all over 
the world, and heavily in finance, 
banking, games, front offices, telecom, 
electronics, marketing, manufacturing 
and retail. From my own experience, I 
can add embedded systems, scientific 
computation and graphics to the 
list of use cases.
Q 
Is it the open source practice 
that led to the early success 
for programming languages such 
as C and C++?
No. The early success of C and C++ 
predates the emergence of open source. 
However, AT&T did the next best thing 
and allowed the use of C and C++ 
compilers and libraries for a very low 
price. For non-profit organisations, 
For U & Me
Interview

C++ cost US$ 75, which was the price 
of the magnetic tape on which it was 
shipped (source and binary); organised 
distribution over the Internet was still in 
the future, back then. Soon, AT&T gave 
the specifications of C and C++ to the 
ISO so everyone could use them, and I 
personally helped other organisations 
with getting C++ compilers written.
Nowadays, there are, of course, 
open source and proprietary C++ 
implementations. To ensure the wide 
reach of my work, I deliberately (with 
the agreement of AT&T) refrained from 
patenting anything related to C++.
Q 
Do you see any programming 
languages today that can 
replace C++? Or can we call it 
irreplaceable?
I do not see a current language that could 
replace C++ across its range of uses. 
Its combination of hardware access and 
zero-overhead abstraction is still unique. 
However, nothing lasts forever.
Eventually, a current language will 
acquire sufficient facilities or a new 
language will come along. C++ has 
certainly been at the top of the game 
for almost 30 years. That is not bad, 
especially given that C++ never had a 
powerful owner or a marketing budget.
Q 
What are the major features 
that make C++ an easier 
option compared to C?
C++ offers you a better type system, 
classes with constructors and 
destructors, overloading, native support 
for object-oriented programming, 
support for generic programming as 
well as compile-time programming.
Q 
Why do aspiring developers 
need to learn C++ to 
survive in the growing world of 
computers?
I do not know if they need to learn 
C++, but they should want to. It is one 
of the most widely used languages and 
among the most flexible. It is also one 
of the languages that delivers the best 
performance, is very popular and allows 
you direct access to hardware resources.
Further, C++ is one of the few 
languages that allow you to use a wide 
range of fundamental programming 
techniques. It also allows you to work in 
most industries.
Q 
There are different compilers 
available for C++. Which one 
is the best, in your view, and why?
I do not have a favourite compiler for 
C++. I use several. The major C++ 
compilers are all good. They have 
good standard conformance, generate 
good code and have good supporting 
toolsets and fundamental libraries. 
You can choose a C++ compiler based 
on specific needs such as the ability 
to use specific hardware or specific 
programming techniques, specific 
environments, portability or a certain 
toolset like GDB or Visual Studio.
Q 
Unlike Python and Java, 
C++ has not yet become the 
perfect choice for embedded 
engineers. Do you think the focus 
should now also be on connected 
devices?
Python and Java are not perfect 
choices for embedded systems either. 
C++ is critical when you need to 
squeeze performance or energy 
efficiency (say, enhancing battery 
life) out of a gadget, but no one 
language is the best for everything 
and everybody. There is a lot of C++ 
embedded system code. It is worth 
remembering that ‘embedded systems’ 
include a vast range of things — from 
coffee machines to jet plane flight 
controls, from fuel injectors to stereo 
amplifiers, and from lithium ion battery 
controllers to self-driving cars. There 
is a corresponding range of needs for 
programming techniques and tools.
Q 
How important, do you 
think, is readability in a 
programming language?
Readability is essential. If you cannot 
read the code, you cannot maintain it 
and cannot discuss or argue about how 
correct it is. Today’s C++ is so much 
more readable than older C++ or C.
I am working on an ambitious 
project to define what modern C++ 
code -- using C++11, C++14, C++17 
and beyond —should look like. It is 
called the Core Guidelines, and is an 
open source project with editors from 
Morgan Stanley, Microsoft, Red Hat 
and Facebook.
We are aiming for completely 
type-safe and resource-safe code, 
without limitations on expressibility 
or performance. Code conforming 
to the Core Guidelines is far more 
regular and readable than most current 
code. Moreover, we are working on 
tools for the automatic enforcement 
of these guidelines.
Q 
Do you see any big 
difference between 
computer systems and 
embedded devices that are 
designed for the Internet of 
Things (IoT) ecosystem?
There are differences such as 
application binary interfaces (ABIs) and 
security concerns, but the fundamental 
programming needs and constraints are 
the same, and I think they favour C++.
Q 
How do you plan the 
revisions of C++ standards?
We just moved C++17 out of a national 
vote. It will be the ISO C++ standard later 
this year. C++20 will be the standard 
after that in 2020. For C++20, we will 
code for concepts, modules and possibly 
contracts and co-routines. It could be 
an exciting major revision and change 
the way we program, but all of this will 
depend on the standards committee’s 
willingness to accept change.
Concepts are shipping in GCC, while 
modules are shipping in Microsoft and 
co-routines are available in Microsoft 
and Clang. More implementations are 
 
 
For U & Me
Interview
22 | May 2017 | OPEN SOURCE FOR yOU | www.OpenSourceForU.com

in the works, so my dreams have some 
concrete foundation.
It is extremely difficult for a group 
of 200 to make plans and stick to them. 
Some of us in the committee try, though.
Q 
What are all the scheduled 
features in the C++17 
standard that will advance 
the present programming 
experience?
Compared to C++11 and C++14, 
C++17 does not offer anything that 
will fundamentally change the way you 
program. It does not change the way 
you think about constructing a program. 
Thus, by my usual definition, it is a 
minor update. It does, however, offer a 
little for everybody. Importantly, most 
C++17 features are already shipping in 
the major implementations; I suspect 
these implementations will all be 
feature-complete before 2017 is out.
What matters with the new features 
is how they can be used to write better 
code, by which I mean code that more 
directly expresses its intent and runs 
faster using fewer resources.
Q 
Spending hours on the code 
is not an easy task for young 
programmers. What valuable tips 
can they get from you that will 
help them work comfortably on 
new projects?
The code is where ideas turn into 
practical results. We cannot just write 
papers or manuals. In fact, we must 
produce code that actually works.
Before adding to a code base, you 
have to understand some of it. You can 
get a general understanding of some 
code top-down, but to be able to make 
a change (such as to fix a bug) you 
need to understand the code inside-out, 
particularly what affects this line of 
code and in turn, what this line of code 
affects? Anything that helps read the 
code and navigate through it, helps. 
Linear reading or trying to understand 
everything just does not work. It is 
not feasible for large code bases and 
inefficient for small ones.
So, look for good code navigation 
tools like IDEs and, of course, hope for 
helpful colleagues who will spend time 
introducing you to the code base and 
its associated tools (debuggers, build 
systems and test suites).
If you want to use C++, take care to 
learn it well. In particular, learn to use 
modern C++ well. Do not just repeat the 
errors of the past. You can write so much 
better C++11 than you could write in the 
styles of the 1990s.
Q 
Finally, where do you see the 
programming world in the 
near future?
Hopefully, it will be easier to read, write 
and maintain code. I do not want to 
write science fiction, so I will not go into 
details. But I expect that C++ and I will 
make a significant positive contribution 
to that future. 
Would You
Like More
DIY Circuits?
For U & Me
Interview
www.OpenSourceForU.com | OPEN SOURCE FOR yOU | May 2017 | 23

24 | May 2017 | OPEN SOURCE FOR yOU | www.OpenSourceForU.com
Admin
Overview
I
n almost every project we work on, we need to deal with 
files. Consider the following scenarios:
 
As a systems administrator, you need to deal with large 
system log files every day (in  gigabytes), which are very 
difficult to handle.
 
As a developer, you may have to deal with huge 
application log files or application source code files.
 
As a tester, you may have to deal with the large-sized data 
files generated as the output of some process, and need to 
validate a particular attribute present in those files.
There may be various other scenarios as well but, 
generally, all these data and log files are text files. The 
difference between a log file, a text file and a data file is 
that a log file is generated automatically to keep track of 
some specific application, a text file is created by the user 
in a word processor, and a data file stores data pertaining 
to a specific application for later use. Data files can be 
stored as text files or as binary files, and are particularly 
helpful when debugging a program.
The need to handle large files carefully
The files generated in the situations mentioned earlier 
could be of any size, format or encoding. Besides, they 
may or may not contain the text in specific formats, such as 
comma-separated values (CSV), log data files (LDF) and 
master data files (MDF). These large text files may also vary 
with respect to the lines they have or the reason they need 
to be opened for. For example, the event log files for some 
Handling Large Files  
Using Open Source Tools
Handling large log files is problematic. Systems administrators must be able to open them to 
view and act upon the vital information they contain. Text editors are the best option for this 
task. Here is a review of a selection of these editors for our readers.

www.OpenSourceForU.com | OPEN SOURCE FOR yOU | May 2017 | 25
Admin
Overview
common applications that generate a large amount of data 
could be more than 1GB in size and we may need to read 
them. As the file size increases, handling them becomes 
a tedious task. Generally, we come across the following 
issues while attempting to handle large files:
 
The system may hang while attempting to open large 
(GB) log files.
 
The system will slow down due to full RAM usage by the 
opened large text file while the editor is in use.
 
While attempting to open some 1GB sized text files, the 
built-in text editor of the Windows application, Notepad, 
can show some serious error messages like The file is too 
large for Notepad. In such cases, we need to use another 
editor to edit the file.
To interpret the information of these files carefully 
without facing such issues, we need some good editors that 
allow us to open, read and edit the files. So let’s discuss some 
basic editor tools, which could be handy for these tasks. 
Open source tools to handle large files
Notepad++ 
Notepad++ is an advanced and feature-rich sibling of the 
Notepad text editor that you find in the Windows OS. It is 
for those who want a very simple and easy UI with a great 
feature set, as it is clean, very fast and an excellent way to 
get work done. It is a lightweight replacement for Notepad, 
and is better than Microsoft Notepad in every possible way. 
It supports multiple tabs to open different files in a single 
window. It also supports multiple programming languages. 
Some of the languages supported by Notepad++ are C, C++, 
Java, C#, XML, HTML, PHP,  JavaScript, VB/VBS, SQL, 
Perl, Python, UNIX shell script, etc.
Notepad++ is commonly known as the best HTML editor. 
It supports coloured lines and reports code error at the same 
instant. Besides, its functionality can be extended by using 
the hundreds of available plugins.
Key features
 
WYSIWYG (What You See Is What You Get).
 
User defined syntax highlighting and auto-completion.
 
Has the multi-document and multi-view feature.
 
Regular expression and the ‘multiple file search, mark and 
replace’ feature is supported.
 
Full drag-and-drop support.
 
Support for a multi-language environment support.
 
Being built for the Windows platform, it can also run on 
Linux, UNIX and Mac OS X (using Wine).
 
Macro recording and playback.
 
Zooming for those afflicted with sore eyes from staring at 
computer screens.
 
Comes in both ‘installer’ and ‘zipped’ versions for people 
who don't have admin rights on their work computers.
Disadvantages
 
Third party program (Wine) needed to run the application 
on Mac OS X.
 
Remote file editing does not support HTTP, SSH or 
WebDAV.
Glogg
Glogg is a multi-platform, open source GUI application 
tool for viewing and searching large,  complex log and 
data files. It is a very quick tool as it reads data directly 
from disk and does not load it entirely into memory, which 
Figure 2: Notepad++ text editor logo (Image source: www.google.com)
Figure 3: Log file opened with Glogg (Image source: www.google.com)
Figure 1: Error with Notepad for a file size greater than 1GB 
(Image source: www.google.com)
User another editor to edit the file.
The C:\User\Abder-Rahman\Desktop\hg38.txt file is too large for 
Notepad.
Notepad
OK

26 | May 2017 | OPEN SOURCE FOR yOU | www.OpenSourceForU.com
Admin
Overview
enables it to open very large files for viewing. Glogg is a 
read-only editor and does not come with a lot of features. 
However, it is very useful in the overall handling of large 
files. It can be interpreted as a graphical combination of 
Grep with fewer commands.
Key features
 
It is very quick while handling large files as it directly 
reads from the disk without loading into memory.
 
Runs on UNIX-like systems, Windows and Mac (using 
the Qt software development framework).
 
It automatically updates the log with auto-refresh features 
in real-time.
 
Lines in a file can be marked while reading and they can 
be combined later.
 
It supports keyboard commands like vim/less to move 
around the file.
Disadvantages
 
You can’t edit files as it is a read-only text editor.
Download link: http://glogg.bonnefon.org/download.html
Vim
VI (Visual Interactive) is one of the main editors for UNIX 
systems and Vim is a multi-platform clone of this text editor. 
It has been written by the 
Dutch programmer, Bram 
Moolenaar, who is an active 
member of the open source 
software community. Vim 
stands for Vi Improved. It is 
considered one of the most 
popular text editors among 
developers and is perfectly 
customisable. It is famous 
for two reasons. First, it 
supports complete keyboard 
operations without any need 
for the mouse and, second, 
Vim is present in almost every UNIX-based machine. It is a 
free text editor, which needs a terminal shell environment, and 
it is also available as GVim, which is Vim with a built-in GUI.
For new users, it is sometimes hard to interact with Vim 
because this editor does not prompt the user for the next 
instructions. To learn the basic commands through gaming, 
you can refer to http://vim-adventures.com/.
Key features
 
It supports keyboard based operations fully.
 
Its performance with handling large files is very good.
 
Portability and ubiquity is a key feature of Vim.
 
Vim is customisable. You can Google for .vimrc, dotfile 
and VimScript to find examples of preconfigured Vim 
configuration files.
 
It has a good ability to work with files on a remote server 
using a terminal over SSH.
 
Vim is open source and free to use.
 
Its core functionalities can be extended using various 
plugins with support for Vim.
Disadvantages
 
Vim is generally for advanced users. It is not easy to 
master it quickly but, once done, it provides you with the 
power that no other text editor can give you.
ConTEXT
ConTEXT is a small, fast and very useful open source text 
editor for Microsoft Windows. It has been developed to serve 
as a powerful tool for software developers.
Key features
 
It is very good at handling large files.
 
It has powerful syntax highlighting for multiple languages 
like C#, C/C++, Java, etc.
 
It can open multiple files at once.
 
It supports more than 20 languages like English, 
Spanish, French, etc.
 
We can compare multiple files in it.
 
Search and replace with regular expressions.
 
It has user definable execution keys, depending on 
the file type.
 
Powerful command line handler.
 
Customisable with syntax highlighter colours, cursors, 
margins, gutter, line spacing, etc.
 
Search and replace text in all open files at one go.
Download link: http://www.contexteditor.org/downloads/
Source code repository path: https://code.google.com/
archive/p/contexteditor/
Figure 4: Vim text editor logo (Image 
source: www.google.com)
Figure 5: Tool options with ConTEXT editor (Image source: www.google.com)

www.OpenSourceForU.com | OPEN SOURCE FOR yOU | May 2017 | 27
Admin
Overview
EditPad Lite
EditPad Lite is a general-purpose text editor. It can 
be used to easily edit any kind of plain text file. It 
displays content instantly because it uses pointers to 
access the file directly, rather than to read the entire 
file into memory at once. EditPad Lite has all the 
essential features to make text editing easy.
Key features
 
Full Unicode support, including complex scripts 
and right-to-left scripts.
 
It does editing of files directly in Windows, 
UNIX and Mac with text encoding (code pages) 
and line breaks.
 
Working with multiple files is very easy using 
the tabbed interface.
 
You can undo and redo all open files infinitely, 
even after you save the file.
 
It supports large files and long lines very well.
 
It prevents any data loss by automatic backup and 
saves working copies of files.
 
Its powerful search-and-replace option with 
literal search terms is a worthy feature.
 
Regular expressions that can span multiple lines.
Disadvantages
 
 It supports Windows only.
 
It is only free for personal use.
EmEditor
This is a powerful editor with some very great features 
like Unicode support and coloured syntax highlighting. 
It supports multi-platform files, and many files can 
be opened concurrently. It uses an effective approach 
to read large files as it spills the content onto the disk 
Figure 6: EditPad is very good with regular expressions
Figure 7: EmEditor tool logo (Image source: www.google.com)
•  www.lulu.com
•  www.magzter.com
•  Createspace.com
•  www.readwhere.com

28 | May 2017 | OPEN SOURCE FOR yOU | www.OpenSourceForU.com
Admin
Overview
rather than loading it to memory. This helps it to open very 
large files with ease. One major feature of EmEditor is that 
it auto-detects if the CSV file has an uneven number of 
columns, and it tries to fix it at the same time. Its large file 
controller helps it to read large files at once.
Key features
 
Easily handles files up to 248 GB.
 
 It can split large files and can also 
combine files into one.
 
 Syntax highlighting and regular expressions are some 
of its main coding features.
 
 Customisable interface and quick launch makes the 
user experience better.
 
 Various free plugins make this tool more productive.
 
 Powerful and scriptable macros.
 
 The word count plug-in allows you to count 
specific terms or characters.
 
 Split window (up to four panes).
 
 Has the auto-save and auto-indent features.
 
 Support for external tools includes launching external 
programs via keyboard shortcuts or toolbar buttons.
Disadvantages
 
Some of its features are available only on the pro 
edition, which is not free.
LogExpert
This is a Windows tail program, which serves as a GUI 
replacement for the UNIX tail command. It is very good 
at reading large files, and is free for commercial as well as 
non-commercial use.
Key features
 
It has a plugin API for log file data sources.
 
 It supports the tail program and Unicode.
 
 Multiple plugin support is available from third party.
 
 MDI-interface with tabs.
 
 It has a search function including regular expressions.
 
 We can bookmark the search result and can add 
comments to bookmarks.
 
Any change in the log file automatically gets 
updated in the same log file that is being used by the 
LogExpert too.
 
 It has a very flexible filter view.
 
 We can highlight lines with search criteria.
 
 Columnizer is its key feature, which helps it to split log 
file lines into several multiple columns for some well-
defined log file formats.
Other alternatives to consider
 
Atom: This is a freeware open source solution that 
By: Aakash Beniwal 
The author works with Infosys Limited, Pune, as a testing 
engineer and has an experience of 2.5 years in that domain. 
He can be reached at aakashnavodaya@gmail.com.
[1] https://en.wikipedia.org/wiki/
[2] https://www.petri.com/
[3] http://www.technoreply.com/
References
supports C++, HTML, CSS, and JavaScript. 
 
Visual Studio Code: This is another free text editor 
created by Microsoft under the MIT licence.
 
GNU Emacs: This is a very popular text editor 
derivative of the Emacs family, created by Richard 
Stallman for the GNU projects.
Tips to handle large files
 
Do not close large files immediately after viewing 
them. You will be dealing with a lot of information 
and you wouldn’t want to close a file only to realise 
that you need it open again. And besides, large files 
take time to open but, after that, it’s all fine.
 
It is good to have complete documentation of all the 
findings of large log files, as we may forget these later 
when we need to use them and end up wasting time in 
reopening the same large file.
 
 It is a good practice to keep your RAM as free as 
possible as most text editors load the files into the 
RAM to read them.
 
 More than a text editor, it's a RAM issue. If you want 
to edit a 5GB file and you do not have enough free 
RAM, then you will have to close any other open 
application to handle the opening of the large file, 
carefully.
 
 Instead of using a text editor tool, you may go ahead 
with the good old Grep command, or you could use 
the split function to split the large file into multiple 
files of smaller sizes, which are easy to handle.
 
You can use cmd to open large files in Windows. 
First, open cmd  with Start>Run. Navigate to your file 
location and then type the following command:
type <filename.extension> | more
Now the cmd window will show you a screen with 
the contents of the file. This happens quickly for large 
files also, without any time lag. You can copy the 
details from there as well. 

www.OpenSourceForU.com | OPEN SOURCE FOR yOU | May 2017 | 29
Admin
Let’s Try
C
acti is written in PHP and uses the MySQL database 
as a backend. It uses the RRDtool (Round-Robin 
Database tool) to handle time series data and has built-
in SNMP support. Cacti has been released under the GNU 
General Public License. 
Setting up Cacti 
We will use a CentOS 6.8 virtual machine (VM) running on 
KVM to set up Cacti. Just for this demonstration, we will 
disable SELinux. You will need to set the following in /etc/
selinux/config and reboot the VM: 
SELINUX=disabled
When used in production, it is essential that you enable 
SELinux. You should then test for Internet connectivity from 
within the VM. 
The Ansible version used on the host Parabola GNU/
Linux-libre x86_64 is 2.2.1.0. The ansible/inventory/kvm/ 
directory structure is shown below: 
ansible/inventory/kvm/inventory
ansible/inventory/kvm/group_vars/all/all.yml
The IP address of the guest CentOS 6.8 VM is provided in 
the inventory file as shown below: 
centos ansible_host=192.168.122.98 ansible_connection=ssh 
ansible_user=root ansible_password=password
Add an entry for ‘centos’  in the /etc/hosts file as 
indicated below: 
192.168.122.98 centos
The contents of the all.yml for use with the playbook are 
as follows: 
---
mysql_cacti_password_hash: "{{ vault_mysql_cacti_password_
hash }}"
mysql_username: "{{ vault_mysql_user }}"
mysql_password: "{{ vault_mysql_password }}"
The Cacti.yml playbook is located in the ansible/
playbooks/configuration folder. 
Vault 
Ansible provides the Vault feature, which allows you to store 
sensitive information like passwords in encrypted files. You 
can set the EDITOR environment variable to the text editor of 
your choice, as shown below: 
$ export EDITOR=nano
In order to store our MySQL database credentials, we will 
DevOps: Using Ansible to Deploy 
Cacti for Monitoring
In this third article in the DevOps series, we will install and set up Cacti, a free and open 
source Web-based network monitoring and graphing tool, using Ansible. 

30 | May 2017 | OPEN SOURCE FOR yOU | www.OpenSourceForU.com
Admin
Let’s Try
create a vault.yml file as indicated below: 
$ ansible-vault create inventory/kvm/group_vars/all/vault.yml
Provide a password when prompted, following which, 
the Nano text editor will open. You can enter the following 
credentials and save the file: 
---
vault_mysql_cacti_password_hash: 
"*528573A4E6FE4F3E8B455F2F060EB6F63ECBECAA"
vault_mysql_user: "cacti"
vault_mysql_password: "cacti123"
You can edit the same file, if you wish, using the 
following command: 
$ ansible-vault edit inventory/kvm/group_vars/all/vault.yml
It will prompt you for a password, and on successful 
authentication, your text editor will open with the decrypted 
file contents for editing. 
Apache 
Cacti has many dependency packages, and the first software 
that we will install is the Apache HTTP server. 
---
- name: Install web server
  hosts: centos
  gather_facts: true
  tags: [httpd]
  tasks:
    - name: Update the software package repository
      yum:
        name: '*'
        update_cache: yes
    - name: Install HTTP packages
      package:
        name: "{{ item }}"
        state: latest
      with_items:
        - wget
        - nano
        - httpd
        - httpd-devel
    - name: Start the httpd server
      service:
        name: httpd
        state: started
    - wait_for:
        port: 80
A ‘yum update’ is first performed to sync with the 
package repositories. The httpd Web server and a few other 
packages are then installed. The server is started, and the 
Ansible playbook waits for the server to listen on port 80. 
MySQL and PHP 
The MySQL, PHP and RRDTool packages are then installed, 
following which the SNMP and MySQL servers are started as 
shown below: 
- name: Install MySQL, PHP packages
  hosts: centos
  become: yes
  become_method: sudo
  gather_facts: true
  tags: [database-web]
  tasks:
    - name: Install database/web packages
      package:
        name: "{{ item }}"
        state: latest
      with_items:
        - mysql
        - mysql-server
        - MySQL-python
        - php-mysql
        - php-pear
        - php-common
        - php-gd
        - php-devel
        - php
        - php-mbstring
        - php-cli
        - php-process
        - php-snmp
        - net-snmp-utils
        - net-snmp-libs
        - rrdtool
    - name: Start snmpd server
      service:
        name: snmpd
        state: started
    - name: Start mysqld server
      service:
        name: mysqld
        state: started
    - wait_for:

www.OpenSourceForU.com | OPEN SOURCE FOR yOU | May 2017 | 31
Admin
Let’s Try
        port: 3306
Cacti 
Cacti is available in the EPEL repository for CentOS. The 
GPG key for the CentOS repositories is enabled before 
installing the EPEL repository. A‘yum update’ is performed 
and the Cacti package is installed. A Cacti user is then created 
in the MySQL database. 
- name: Install Cacti
  hosts: centos
  become: yes
  become_method: sudo
  gather_facts: true
  tags: [cacti]
  tasks:
    - name: Import EPEL GPG key
      rpm_key:
        key: http://dl.fedoraproject.org/pub/epel/RPM-GPG-
KEY-EPEL-6
        state: present
    - name: Add YUM repo
      yum_repository:
        name: epel
        description: EPEL YUM repo
        baseurl: https://dl.fedoraproject.org/pub/
epel/$releasever/$basearch/
        gpgcheck: yes
    - name: Update the software package repository
      yum:
        name: '*'
        update_cache: yes
    - name: Install cacti
      package:
        name: "{{ item }}"
        state: latest
      with_items:
        - cacti
    - name: Create cacti database user
      mysql_user:
        name: cacti
        password: "{{ mysql_cacti_password_hash }}"
        encrypted: yes
        priv: '*.*:ALL,GRANT'
        state: present
Fixing a bug 
The time zone data is missing in this MySQL version (5.1.73-
8). In order to resolve this bug, the mysql_test_data_timezone.
sql file needs to be imported and the ‘cacti’ user needs to be 
given the SELECT privilege to do this. 
- name: For bug https://github.com/Cacti/cacti/issues/242
  hosts: centos
  become: yes
  become_method: sudo
  gather_facts: true
  tags: [bug]
  tasks:
    - name: Import mysql_test_data_timezone.sql
      mysql_db:
        state: import
        name: mysql
        target: /usr/share/mysql/mysql_test_data_timezone.sql
    - name: Grant privileges
      mysql_user:
        name: cacti
        append_privs: true
        priv: 'mysql.time_zone_name:SELECT'
        state: present
It is a good practice to have a separate playbook for such 
exceptional cases. In future, when you upgrade to newer 
versions that have bug fixes, you can simply skip this step. 
Configuration 
The last step involves configuring Cacti. 
- name: Configuration
  hosts: centos
  become: yes
  become_method: sudo
  gather_facts: true
  tags: [config]
  tasks:
    - name: Create a database for cacti
      mysql_db:
        name: cacti
        state: present
    - name: Import cacti.sql
      mysql_db:
        state: import
        name: cacti
        target: /usr/share/doc/cacti-1.0.4/cacti.sql
    - name: Update database credentials in config file
      lineinfile:
        dest: /etc/cacti/db.php
        regexp: “{{ item.regexp }}”



34 | May 2017 | OPEN SOURCE FOR yOU | www.OpenSourceForU.com
Admin
Let’s Try
        line: “{{ item.line }}”
      with_items:
        - { regexp: ‘^\$database_username’, line: “$database_
username = ‘{{ mysql_username }}’;” }
        - { regexp: ‘^\$database_password’, line: “$database_
password = ‘{{ mysql_password }}’;” }
    - name: Allow port 80
      shell: iptables -I INPUT 5 -p tcp --dport 80 -m state 
--state NEW,ESTABLISHED -j ACCEPT
    - name: Update access in cacti.conf for httpd
      replace:
        dest: /etc/httpd/conf.d/cacti.conf
        regexp: “{{ item.regexp }}”
        replace: “{{ item.replace }}”
      with_items:
        - { regexp: ‘Require host localhost’, replace: 
‘Require all granted’ }
        - { regexp: ‘Allow from localhost’, replace: ‘Allow 
from all’ }
    - lineinfile:
        dest: /etc/cron.d/cacti
        regexp: ‘^#(.*)$’
        line: ‘\1’
        backrefs: yes    
    - name: Start mysqld server
      service:
        name: mysqld
        state: restarted
    - wait_for:
        port: 3306
    - name: Start the httpd server
      service:
        name: httpd
        state: restarted
    - wait_for:
        port: 80
A database called ‘cacti’ is created for the application, and 
the cacti.sql file is imported into it. The database credentials 
are updated for the Cacti application. The firewall rules are 
then updated to allow incoming HTTP requests for port 80. 
The periodic cron poller is then enabled in /etc/cron.d/cacti: 
Figure 1: License agreement
Figure 3: Installation type 
Figure 4: Binary location and version 
Figure 2: Pre-installation checks 

www.OpenSourceForU.com | OPEN SOURCE FOR yOU | May 2017 | 35
Admin
Let’s Try
Figure 5: Directory permission checks 
Figure 6: Template set-up 
Figure 8: Changing the password 
Figure 7: User login
configuration/cacti.yml --ask-vault-pass
It will prompt you for the Vault password, following 
which all the playbooks will be completed. You can then 
open http://192.168.122.98/cacti to accept the GNU 
General Public License agreement. After you agree to the 
terms of the licence, click ‘Next’. The Cacti installation 
wizard shows the pre-installation checks, which should 
not have any errors. This is followed by the selection of 
the installation type, binary location, version, and the 
directory permission checks. You can then decide on the 
templates you would like to set up, following which a user 
login is provided. The default user name and password is 
‘admin:admin’ and you will be immediately prompted to 
change the password after logging in. You can then proceed 
to log in to the Cacti dashboard. Figures 1 to 8 give the 
screenshots of the Cacti Web UI installation for reference. 
A screenshot of Cacti graphing for memory usage is 
shown in Figure 9. 
*/5 * * * *     cacti   /usr/bin/php /usr/share/cacti/poller.
php > /dev/null 2>&1
The MySQL and HTTP servers are then restarted. 
The result 
The entire playbook can now be invoked as follows: 
$ ansible-playbook -i inventory/kvm/inventory playbooks/
By: Shakthi Kannan
The author is a free software enthusiast and blogs at 
shakthimaan.com.
Figure 9: Cacti Web UI

36 | May 2017 | OPEN SOURCE FOR yOU | www.OpenSourceForU.com
Admin
Insight
C
yber attacks are increasing every day with the 
increased use of mobile and Web applications. 
Globally, statistics show that more than 70 per 
cent of the applications either have vulnerabilities which 
could potentially be exploited by a hacker, or worse, they 
have already been exploited. The data losses due to this 
are typically of two types. Either the data is confidential to 
the organisation or it is private to an individual. Regardless 
of the category, data losses result in the loss of money or 
reputation. This article explores a technical process that can 
be adopted by industries and organisations to protect their 
intellectual property, and if implemented correctly, will 
result in better risk management.
For those who are new to Vulnerability Assessment and 
Penetration Testing (VAPT), this is a technical assessment 
process to find security bugs in a software program or a 
computer network. The network may be a LAN or WAN, 
while the software program can be a .exe running on a 
server or desktop, a Web/cloud application or a mobile 
application. Before we get into the technical aspects of 
VAPT, let’s look at a few of its benefits.
 
Helps identify programming errors that can lead 
to cyber attacks
 
 Provides a methodical approach to risk management
 
 Secures IT networks from internal and external attacks
 
 Secures applications from business logic flaws
 
 Increased ROI on IT security
 
 Protects the organisation from loss of reputation 
and money
Why are systems vulnerable?
There are primarily two main reasons for systems being 
vulnerable—misconfiguration and incorrect programming 
practices. In the case of networks, devices such as routers, 
switches and servers, as well as firewalls and IPS systems 
are either misconfigured or, in some cases, not configured 
at all, thus running default settings. As an example, almost 
all firewalls have a default built-in user account with the 
name,‘admin’. Typically, the password for it is also set 
to ‘admin,’ by default, or something even easier to guess. 
Looking at the example of servers, installing a database server 
leaves us with an ‘sa’ account, which has a blank password. 
The Basics of  
Vulnerability Assessment and 
Penetration Testing
In these days of widespread Internet usage, security is of prime importance. The almost 
universal use of mobile and Web applications makes systems vulnerable to cyber attacks. 
Vulnerability assessment can help identify the loopholes in a system while penetration 
testing is a proof-of-concept approach to actually explore and exploit a vulnerability. 

www.OpenSourceForU.com | OPEN SOURCE FOR yOU | May 2017 | 37
Admin
Insight
As for programming errors, a user input taken from a 
Web application form may be directly sent to a backend 
database server without parsing it. This can lead to a 
parameter manipulation attack or SQL injection attack. 
Another example of programming errors would be a Web 
service accepting requests without performing adequate 
authentication, thus leaking data inadvertently. This shows us 
that it is human error that leads to vulnerable systems, which 
could be exploited easily by attackers, to compromise data 
confidentiality, integrity and availability. 
What is vulnerability assessment?
Vulnerability assessment (VA) is a systematic technical 
approach to find the security loopholes in a network or 
software system. VA is entirely a process of searching and 
finding, with the objective that none of the loopholes are 
missed. It primarily adopts a scanning approach which is done 
both manually and performed by certain tools. The outcome 
of a VA process is a report showing all vulnerabilities, which 
are categorised based on their severity. This report is further 
used for the next step, which is penetration testing (PT). VA is 
usually a non-intrusive process and can be carried out without 
jeopardising the IT infrastructure or application's operations.
What is penetration testing?
A penetration test (PT) is a proof-of-concept approach to 
actually explore and exploit vulnerabilities. This process 
confirms whether the vulnerability really exists and 
further proves that exploiting it can result in damage to the 
application or network. The PT process is mostly intrusive 
and can actually cause damage to the systems; hence, a lot 
of precautions need to be taken before planning such a test. 
The outcome of a PT is, typically, evidence in the form of a 
screenshot or log, which substantiates the finding and can be a 
useful aid towards remediation. As a summary, shown below 
are the steps involved in the VAPT process.
 
Scanning the network or application
 
 Searching for security flaws
 
 Exploiting the security flaws
 
 Preparing the final report of the test
Differences between VA and PT
VA and PT differ from each other in two aspects. The VA 
process gives a horizontal map into the security position of 
the network and the application, while the PT process does 
a vertical deep dive into the findings. In other words, the 
VA process shows how big a vulnerability is, while the PT 
shows how bad it is. There is one more subtle difference. 
Due to the nature of work involved in each process, a VA can 
be carried out using automated tools, while a PT, in almost 
all cases, is a manual process. This is because PT essentially 
simulates what real hackers would do to your network or 
application. Figures 1 and 2 shows the VAPT process for 
network and Web applications, respectively.
VAPT tools
While there are multiple tools available in the market, those 
listed below are well-known for their usability. Although 
these tools are mentioned as VAPT tools, most of them 
essentially provide VA only and leave the PT part to the 
ethical hackers to be done manually. There are a couple 
of tools, though, which are powerful PT tools, and are 
mentioned as such in the list below.
 
Nmap
 
 Acunetix
 
 Nessus
 
 OpenVAS
 
 Nexpose
 
 BurpSuite (PT)
 
 Metasploit (PT)
There are two important terms that an ethical hacker must 
know, especially while using these tools. These are: false 
positive and false negative. 
A false positive is when a vulnerability actually does 
not exist, but it gets reported. A false negative is when a 
vulnerability actually exists but it is not reported. A false 
positive can be a nuisance resulting in a waste of time for 
an ethical hacker, whereas a false negative can be really 
Figure 1: Network VAPT process
Figure 2: Web VAPT process
Continued on page 40...

38 | May 2017 | OPEN SOURCE FOR yOU | www.OpenSourceForU.com
Developers Let’s Try
Exploring Sinatra
This basic introduction to Sinatra can be used by interested readers as a starting 
point, from where they can move on to the vast resources online. 
R
uby, a mature language, is used in many applications 
but Web development is what  it is mostly used for. 
Whenever someone recommends Ruby for Web 
development, the first thing that comes to mind is Ruby 
on Rails. Granted, it’s a great Web framework for rapid 
prototyping, and has excellent documentation, tooling and 
community, but there are times when using Rails is an 
overkill—like when you want a simple solution without 
having to navigate between files and directories and just 
want the work done. Or, when you want a quick solution to a 
problem but don’t want to fiddle through configurations. For 
such cases, Sinatra might be the right tool for you.  
Sinatra is named after the famous American singer and 
actor, Frank Sinatra. It was designed and developed by Blake 
Mizerany, with a huge list of contributors and backing from 
many companies like Travis CI. Sinatra is a library and not a 
framework, which means it doesn’t do many things under the 
hood and let’s you handle the logic, which is a great thing if 
you want control.
Installation
Sinatra is easy to install. The only obvious requirement is 
Ruby. If you don’t have Ruby installed, try rbenv or rvm, both 
of which let you manage and install multiple Ruby versions. 
Installing it is as easy as using the following command: 
gem install sinatra
Basic use cases
Sinatra is a DSL (domain specific language), which helps in 
reducing the effort to churn out a Web application. To run 
Sinatra locally, there is no configuration needed. 
require ‘sinatra’
# Associates Root of the website with following code block 
when a get request is processed
get ‘/’ do
  “Hello World”
End
Sinatra executes a code block by matching it with an 
HTTP verb like GET, POST and a URL location like /user, /
important. In the above example, when some program makes 
a GET request to the root ‘/’, it will execute the code block and 

www.OpenSourceForU.com | OPEN SOURCE FOR yOU | May 2017 | 39
Developers
Let’s Try
‘Hello World’ will be returned by the server to the user, because 
in Ruby, the last statement is treated as the return statement.
To run the code, type the following command:
ruby hello.rb
The following output should be visible with some variation, 
depending on your version of Ruby and the backend server:
== Sinatra (v1.4.8) has taken the stage on 4567 for 
development with backup from Puma
Puma starting in single mode...
* Version 3.6.2 (ruby 2.2.4-p230), codename: Sleepy Sunday 
Serenity
* Min threads: 0, max threads: 16
* Environment: development
* Listening on tcp://localhost:4567
Use Ctrl-C to stop
If you open http://localhost:4567 you will see a ‘Hello 
World’ message. 
Sinatra has a params hash to capture parameters passed to 
the route. A small excerpt is shown below:
post ‘/secret’ do 
“Message is  #{params[:message]} “
end
The following code displays the parameter named 
message which was POSTed to /secret. All the parameters 
sent to a route are available in hash named params. If we 
make a POST request with the message ‘meet me at Coffee 
Shop’ as follows:
curl --request POST   --url ‘http://localhost:4567/
secret?message=meet%20me%20at%20Coffee%20shop’
…we will get the following output:
Message is meet me at Coffee shop
Sinatra routes can contain regular expressions, named 
parameters and wild card expressions, as well.
get '/send/:room/?' do
 "Sending message to room no #{params[:room]}"
end
This time we don’t need to pass the parameter explicitly; 
it has become a part of the URL itself and it will just 
work, and like in the previous instance, the parameters are 
available via params hash. The interesting bit in the route is 
‘/?’; it is used so that the route will work with and without 
the trailing slash, so that both http://localhost:4567/42 
and http://localhost:456/42/ are legal.
curl --request GET --url http://localhost:4567/send/42/
Sending message to room no 42
Splat (*), or what is commonly known as the 
wildcard operator, will allow anything and is accessible 
via a special parameter named splat which is an array of 
all the values captured by splat against *.
get ‘/mix/*/with/*/?’ do 
“I have #{params[:splat]} and I am going to mix 
#{params[:splat].first} with #{params[:splat].last} “
end 
curl --request GET --url http://localhost:4567/mix/apple/
with/oranges/ 
I have ["apple", "oranges"] and I am going to mix apple 
with oranges
There are many ways to play with routes, but 
covering them all would be beyond the scope of 
this article. 
One cool thing that you can quickly do with Sinatra 
is make APIs, which generally don’t require any interface 
and most of them are stateless. In the following example, 
we will make a simple API that returns all environment 
variables available on the machine in JSON format. For 
JSON conversion, we need a JSON parser and generator, 
which Ruby already ships with. 
Content type tells Sinatra to explicitly specify itself in 
the response header.
# Load the JSON library
require ‘json’ 
get ‘/json’ do 
content_type :json
  ENV.to_h.to_json # To json converts the ENV which was 
converted into hash to json
end
curl --request GET --url http://localhost:4567/json
The output may vary depending on the system and 
configuration:
{
  "PWD": "/home/jatin/Area_51/sinatra_example",

40 | May 2017 | OPEN SOURCE FOR yOU | www.OpenSourceForU.com
Developers Let’s Try
By: Jatin Dhankhar
The author loves modern C++, Ruby, JavaScript and Haskell. 
He can be reached at jatin@jatindhankhar.in.
[1] http://www.sinatrarb.com
[2] https://sinatrarb.slack.com - Sinatra Slack Community
References
Plugins and libraries
There are innumerable libraries available for Ruby, in general, 
and almost of all them will work with Sinatra but there are some, 
like those listed below, that are a ‘must have’.
Shotgun: It lets you reload the server on file changes, 
enabling faster development.
Byebug: This library lets you debug an issue by pausing 
the request and providing a full-blown REPL to investigate.
Sinatra-Cross-Origin: This library lets you handle Cross 
Domain Origin Resource Sharing (CORS), which is very 
useful if you are making a web SDK for others to use.
There are many extensions available at http://www.
sinatrarb.com/extensions-wild.html that might help. 
  "HOME": "/home/jatin",
  "LC_CTYPE": "en_US.UTF-8",
  "BROWSER": "/usr/bin/chromium",
  "JOURNAL_STREAM": "8:20822",
  "XDG_SESSION_TYPE": "x11",
  "XDG_SESSION_DESKTOP": "gnome-xorg",
  "GJS_DEBUG_OUTPUT": "stderr",
  "RBENV_DIR": "/home/jatin/Area_51/sinatra_example",
  "MAIL": "/var/spool/mail/jatin",
  "WINDOWPATH": "2",
  "TERM": "xterm-256color",
  "SHELL": "/usr/bin/zsh
}
Sinatra can handle views and templates as well, and they 
are pretty simple. So, it’s good if you want to make user-
facing applications.
  Note: Most of the code used in the article is also available 
at https://github.com/jatindhankhar/sinatra_example.
Continued from page 37...
dangerous, leaving a network or application susceptible to 
attack, while giving an illusion that everything is alright. It 
has been observed that automated tools tend to exhibit false 
positives as well as false negatives. This brings us to the next 
important question of which method is better—the automated 
VAPT or manual VAPT?
Automated vs manual VAPT
The shortest answer is that the manual VAPT is always better 
and, hence, is a more widely used approach. This is because 
the automated tools are based on simple logic, which checks 
either for signatures or behaviour. To understand this, let’s 
go to the basic difference between a software program and 
the human mind. Listed below are the steps a typical ethical 
hacker performs for a VAPT.
 
Enumerates a vulnerability
 
Performs an attack manually
 
Analyses the results of the attack
 
Performs similar or different attacks based 
on previous findings
 
Assimilates the results to create a customised attack
 
Exploits the vulnerability further to see if more 
attacks are possible
 
Repeats the above steps for all vulnerabilities
Each network or application is different, resulting in a 
very wide range of vulnerability scenarios. From the above 
steps, it becomes clear that there is a lot of complexity 
involved in VAPT, wherein, the results of one test decide 
the actions of the next one. This makes VAPT a process of 
cascaded intelligence, where you cannot predict the next 
step and also need to apply years of experience to reach a 
conclusion. No tool can do this, at least, not as of today, and 
hence it must be performed manually. An ethical hacker’s 
job can be made less stressful by automating certain tasks 
of vulnerability assessment; however, the proof-of-concept 
part in penetration testing mostly relies on manual ways of 
exploiting the loophole and gathering the required evidence. 
Given below are the benefits of manual penetration testing.
 
Mimics the behaviour of real life hackers
 
Brings a great deal of accuracy to the results
 
No false positives
 
Provides evidence, enabling the replication of problems
 
Helps in fixing a product’s security design issues 
VAPT is a methodical approach to risk management. 
CISO's or IT heads should, as a matter of strategy, incorporate 
VAPT in their budgets and risk governance processes. It 
should be a periodically executed process, and the frequency 
should depend upon the data’s confidentiality and risk impact. 
While there are multiple tools to perform vulnerability 
assessment, penetration testing is a manual process, and 
should be handled by professional and highly experienced 
ethical hackers. This will ensure genuine cyber security as 
opposed to an illusion of being secure. 
By: Prashant Phatak 
The author is the founder CEO of Valency Networks, based 
in India (www.valencynetworks.com). He is a subject matter 
expert in IT security penetration testing, designing and audits. 
He can be reached at prashant@valencynetworks.com.

www.OpenSourceForU.com | OPEN SOURCE FOR yOU | May 2017 | 41
Developers
Let’s Try
T
he world is being flooded with data from a wide range 
of sources. The hottest trends in technology currently 
are Big Data and data science, both of which offer 
ways to cope with this data deluge. Many platforms have 
emerged in this space, but Apache Spark and Scala work in 
synergy to address the various challenges this humongous 
data throws up. They are being used on Facebook, Pinterest, 
NetFlix, Conviva and TripAdvisor, among others, for Big 
Data and machine learning applications.
So what is Scala?
Scala stands for Scalable Language. It was developed as 
an object-oriented and functional programming language. 
Everything in Scala is an object, even its primitive data 
types. If you write a snippet of code in Scala, you will 
see that the style is similar to a scripting language. It is 
very powerful, yet compact, and requires only a fraction 
of the lines of code compared to other commercially 
used languages. Due to its characteristics and support 
for distributed/concurrent programming, it is popularly 
used for data streaming, batch processing, AWS Lambda 
Expression and analysis in Apache Spark. It is one of the 
most widely used languages by data scientists, and its 
popularity will soar in the future due to the boom in the 
Big Data and data science domains.
What is Apache Spark?
Apache Spark is a cluster computing framework based on 
Hadoop’s MapReduce framework. Spark has in-memory 
cluster computing, which helps speed up computation by 
reducing the IO transfer time. It is widely used to deal with 
Big Data problems because of its distributed architectural 
support and parallel processing capabilities. It is preferred to 
Hadoop due to its stream processing and interactive query 
features. To provide a wide range of services, it has built-in 
libraries like GraphX, SparkSQL and MLlib. Spark supports 
Python, Scala, Java and R as programming languages,  of 
which Scala is the most preferred. 
Scala: The Powerhouse of 
Apache Spark
Scala, which is an acronym for Scalable Language, is a multi-paradigm, statically-
typed, type-safe programming language focused on Web services. Widely used by 
data scientists today, its popularity is set to soar in the future because of the boom 
in the Big Data and data science domains.

42 | May 2017 | OPEN SOURCE FOR yOU | www.OpenSourceForU.com
Developers Let’s Try
2. To install Scala, type:
$ cd ~/Downloads
$ wget http://www.scala-lang.org/files/archive/scala-
2.11.7.deb
$ sudo dpkg -i scala-2.11.7.deb
$ scala –version
Working with Spark RDD using Scala
Resilient Distributed Datasets (RDD) are the basic data types 
of Spark. They can be created in two ways—from an existing 
source or an external source.
Creating RDD from an existing source: First, switch to 
the home directory and load the sparkcontext as sc.
$ ./bin/spark-shell
Then, create an RDD from existing data stored previously 
in the driver program. First, we will make an array and then use 
the parallelise method to create the Spark RDD from an iterable 
already present in the driver program, using the following code:
val data = Array(2,4,6,8)
val distData = sc.parallelize(data)
To view the content of any RDD, use the collect method, 
as shown below:
distData.collect()
Creating RDD from an external source: An RDD can be 
created from external sources which have the Hadoop Input 
Format such as a shared file system, HDFS, HBase, etc. First, 
load the desired file using the following syntax:
val lines = sc.textFile(“text.txt”);
To display the lines, use the command given below:
lines.take(2)
Basic transformations and actions
Transformations modify your RDD data from one form to 
another. Actions will also give you another RDD, but this 
Reasons to use Scala for Spark
Eighty eight per cent of Spark users code in Scala for the 
following reasons:
1)  Apache Spark is written in Scala and is scalable in 
JVM. Being proficient in Scala helps you dig into the 
source code of Spark, so that you can easily access and 
implement the latter’s newest features. 
2) Spark is implemented in Scala, so it has the maximum 
features available at the earliest release. The features 
are ported from Scala to support other languages 
like Python.
3) Scala’s interoperability with Java is its biggest advantage, 
as experienced Java developers can easily grasp the 
object-oriented concepts quickly. You can also write Java 
code inside a Scala class.
4) Scala is a static typed language. It looks like a dynamic 
typed language because it uses a sophisticated type 
inference mechanism. This leads to better performance.
5) Scala renders the expressive power of a dynamic 
programming language without compromising 
on type safety. 
6) It is designed for parallelism and concurrency to cater 
to Big Data applications. Scala has efficient built-in 
concurrency support and libraries like Akka, which allow 
you to build a scalable application.
7) Scala works well within the MapReduce framework 
because of its functional nature. Many Scala data 
frameworks follow similar abstract data types that are 
consistent with Scala’s collection of APIs. Developers 
just need to learn the basic standard collections, 
which allow them to easily get acquainted with 
other libraries.
Installing Scala
Scala can be installed on Windows or Linux based systems. 
It is mandatory for Java to be installed before Scala. The 
following steps will install Scala 2.11.7 on Ubuntu 14.04 
with Java 7. Type the following commands in a terminal.
1.  To install Java, type:
$ sudo apt-add-repository ppa:webupd8team/java
$ sudo apt-get update
$ sudo apt-get install oracle-java7-installer
Table 1: A comparison between four languages
Metrics
Scala
Java
Python
R
Type 
            
Compiled 
Compiled
Interpreted
Interpreted
JVM based 
Yes 
         
Yes 
       
No 
       
No
Verbosity 
Less           
More  
Less 
       
Less
Code length 
Less           
More  
Less         
Less 
Productivity 
High 
         
Less 
High 
       
High
Scalability 
High 
         
High 
Less 
       
Less
OOPS support
Yes 
         
Yes 
       
Yes 
       
Yes

www.OpenSourceForU.com | OPEN SOURCE FOR yOU | May 2017 | 43
Developers
Let’s Try
operation will trigger all the lined up transformations on 
the base RDD and then execute the action operation on 
the last RDD.
Map transformation: Map applies a function to each 
element of the RDD. The following code checks the length 
of each line.
val Length = lines.map(s => s.length)
Length.collect()
Reduce action: Reduce aggregates the elements 
according to key value. It is applied to the output of the 
Map function. The following code calculates the sum total 
of characters in the file.
val totalLength = Length.reduce((a, b) => a + b)
The DataFrame API
DataFrame is a distributed data collection organised into 
columns that are similar to a relational database system. 
DataFrame can be created from Hive tables, structured data 
files, external databases or existing RDDs. The DataFrame 
API uses a schema to describe the data, allowing Spark 
to manage the schema and only pass data between nodes. 
This is more efficient than using Java serialisation. It is 
helpful when performing computations in a single process, 
as Spark can serialise the data into off-heap storage in a 
binary format and then perform many transformations 
directly on this, reducing the garbage-collection costs of 
constructing individual objects for each row in the data 
set. Because Spark understands the schema, we don’t need 
to use Java serialisation to encode the data. The following 
code will explore some functions related to DataFrames.
First, create a sparkcontext object, as follows:
val sqlcontext = new org.apache.spark.sql.SQLContext(sc)
Then, read an external JSON object and store it 
in DataFrame dfs. Next, show and print it, using the 
following code:
val dfs = sqlContext.read.json(“employee.json”)
dfs.show()
dfs.printSchema()
DataSet API
DataSet API has encoders that translate between JVM 
representations (objects) and Spark’s internal binary format. 
Spark has built-in encoders, which are powerful as they 
generate byte code to mix with off-heap data, and provide 
on-demand access to individual attributes without having 
to de-serialise an entire object. Moreover, the DataSet API 
is designed to work well with Scala. When working with 
Java objects, it is important that they are fully JavaBean-
compliant. The following code will explore some basic functions:
val sc = new SparkContext(conf)
val sqlContext = new SQLContext(sc)
import sqlContext.implicits._
val sampleData: Seq[ScalaPerson] = ScalaData.sampleData()
val dataset = sqlContext.createDataset(sampleData)
The Scala vs Python debate
Scala and Python are both powerful and popular among data 
scientists, many of whom learn and work with both languages. 
Scala is faster and moderately easy to use, while Python is slower 
but very easy to use. But Python usually ranks second because of 
the following reasons:
1.  Scala is usually 10 times faster than Python for processing. 
Python code needs a lot of translation and converting, which 
makes the program a bit slow. Hence, there is a performance 
overhead. Scala has an edge in performance when there are 
fewer processor cores. 
2. Scala is better for concurrency due to its ability to easily 
integrate across several databases and services. It has 
asynchronous libraries and reactive cores. Python doesn’t 
provide heavyweight processes to fork for multi-threading or 
parallel computing.
3. The Scala programming language has several existential 
types, macros and implicits. Its advantages are evident when 
using these powerful features in important frameworks and 
libraries. Scala is the best choice for the Spark streaming 
feature because Python Spark streaming support is not 
advanced and mature like Scala.
Scala has slightly fewer machine learning and natural 
language processing libraries than Python. The library has 
only a few algorithms but they are sufficient for Big Data 
applications. Scala lacks good visualisation and local data 
transformations. Nevertheless, it is preferred since Python 
increases portability for more issues and bugs, as translation is 
tough. That Scala is the winning combination of both object-
oriented and functional programming paradigms might be 
surprising to beginners and they could take some time to pick 
up the new syntax. Scala programming might be a difficult 
language to master for Apache Spark, but the time spent on 
learning it is worth the investment. 
By: Preet Gandhi 
The author is an avid Big Data and data science enthusiast. She 
can be reached at gandhipreet1995@gmail.com
[1] spark.apache.org/
[2] www.tutorialspoint.com/apache_spark/
[3] www.kdnuggets.com/2015/06/introduction-big-data-
apache-spark.html
[4] www.tutorialspoint.com/scala/
References

44 | May 2017 | OPEN SOURCE FOR yOU | www.OpenSourceForU.com
Developers Let’s Try
T
he last decade has seen the rise of public APIs 
and standards like REST for building HTTP based 
services. Both XML and JSON are frequently used 
as the data format for these service methods’ request and 
response data. With the proliferation of these services and 
the emerging trend towards micro-services, latency becomes 
an important consideration when creating high performance 
and efficient services. 
To address this, Google had been working for a while 
to come up with a specification that defines both service 
interfaces and efficient communication protocols, along with 
bindings in multiple languages for both the client and server 
sides. The company later made this available as open source 
to the general public and the gRPC Project was born. The 
project helps to implement services in multiple languages 
with pluggable support for load balancing, health checking 
and authentication. 
The gRPC model is shown in Figure 1. 
The service, its methods and the messages are defined 
via a separate specification known as ProtocolBuffers. This 
specification is then implemented by the server in one of the 
supported languages. Most modern programming languages 
like Java, Python, C#, Node.js and others are supported. You 
can either generate the server bindings via the tools provided 
or even dynamically provide the implementation.
On the client side, you can generate the gRPC stub in a 
client language of your choice, and use that stub directly in 
your client to invoke the service. 
Why use gRPC?
gRPC has several advantages that are worth considering in 
your application. Some of them are:
 
It is built on HTTP/2, which provides us with a high 
speed communication protocol that can take advantage of 
bi-directional streaming, multiplexing and more.
 
The latest version of ProtocolBuffer, i.e., version 3, 
supports many more languages.
 
The ProtocolBuffer data has a binary format and, 
hence, provides a much smaller footprint than JSON/
XML payloads that are currently the most popular. 
This can make a big difference when latency is an 
issue in your API. 
An Introduction to 
Remote Procedure Call (RPC) has been around for decades as a mechanism for inter-
process communication in applications. The client making a call talks to a stub that works 
behind the scenes to handle the marshalling/unmarshalling of the request/response data. 
To the client it appears that the service is residing locally. 

www.OpenSourceForU.com | OPEN SOURCE FOR yOU | May 2017 | 45
Developers
Let’s Try
gRPC and CNCF
gRPC was released by Google more than a year back and 
since then, there has been a lot of momentum towards 
getting the industry to consider this as the glue for micro-
services communication, especially when latency is a 
key factor. Recently, it got adopted as a top level project 
in the Cloud Native Computing Foundation (CNCF), 
which is a big move towards the wider adoption of 
gRPC. It joined the likes of Kubernetes, Prometheus 
and others that are considered to be the foundation of a 
cloud native application.
Wire Protocol - ProtocolBuffers
ProtocolBuffers is both an interface definition language 
and a message format. It is primarily used to define the 
service interface, i.e., what the service methods are and the 
messages that are exchanged. The message formats are also 
defined via their individual parts. 
This protocol definition file can then be used to 
generate both the server side and client side bindings, or 
you can dynamically load the definition and provide the 
implementations. ProtocolBuffers is currently available 
in two versions: proto2 and proto3, but we will go with 
the latter, which is the latest since it has support for a 
wide range of languages. 
The best way to understand ProtocolBuffers is via an 
example that we will build over the course of this article. 
This will include the server and client implementations. 
We are going to build a service called OSFY Stats. 
This service provides two methods, one for retrieving the 
top articles from the OSFY website and another method to 
retrieve the stats on a particular article, like the number of 
views, likes and so on. We are not going to provide a real 
implementation later, but this exercise is more to get an 
idea of how to start building the specification and message 
formats as per the ProtocolBuffers standard.
The proto file (osfy_stats.proto) is shown below: 
syntax = "proto3"; 
package osfy_stats; 
//Service. define the methods that the grpc server can expose 
to the client.
service OSFYStatsService {
  rpc toparticles (TopArticlesRequest) returns 
(TopArticlesResponse);
  rpc articleStats (ArticleStatsRequest) returns 
(ArticleStatsResponse);
}
//Message Type definition for TopArticlesRequest
message TopArticlesRequest{
   int32 numResults = 1;
}
//Message Type definition for TopArticlesResponse
message TopArticlesResponse {
   repeated Article articles = 1;
}
//Message Type definition for Article
message Article {
   int32  id    = 1;
   string title = 2;
   string url   = 3;
}
//Message Type definition for ArticleStatsRequest 
message ArticleStatsRequest {
   int32 id = 1;
}
//Message Type definition for ArticleStatsResponse 
message ArticleStatsResponse {
   int32 id          = 1;
   int32 numViews    = 2;
   int32 numLikes    = 3;
   int32 numComments = 4;
}
Let us go through the proto file in brief:
 
At the top of the file, we specify the version of 
ProtocolBuffers and that the service resides in a package 
named osfy_stats. 
 
We then define the service named OSFYStatsService and 
specify the two methods that it exposes, i.e., toparticles 
and articleStats. 
 
The input and output message formats are specified, 
and the message formats for each of the request and 
response messages are defined in the same file. Note that 
the ProtocolBuffers specification supports various scalar 
types (int32, float and string) and also other complex stuff 
like repeatable, Nested and more. The unique numbers 
Figure 1: gRPC model

46 | May 2017 | OPEN SOURCE FOR yOU | www.OpenSourceForU.com
Developers Let’s Try
    
    }    
  },
  articleStats(call, callback) {
    let article_id = call.request.id;
    //make some calls to actual API
    let numViews = 1000;
    let numLikes = 30;
    let numComments = 5;
    callback(null, {
      id:article_id,
      numViews,
      numLikes,
      numComments
    });
  }
});
//Specify the IP and and port to start the grpc Server, no 
SSL in test environment
server.bind('0.0.0.0:50000', grpc.ServerCredentials.
createInsecure());
//Start the server
server.start();
console.log('OSFY Stats GRPC Server is now running on port-
>', '0.0.0.0:50000');
Let us go through the code in brief:
 
We load the proto definition file and create a server instance.
 
For the server instance, we simply bind the service proto.
osfy_stats.OSFYStatsService.service and provide the 
implementations for the two methods: articleStats and 
toparticles.
 
We use mock implementations for the two methods and, 
in reality, you would have connected to your analytics API 
for this purpose. 
To start the server, all we need to do is the following, 
and it will display that the server is running, via the message 
shown below:
$ node server.js
OSFY Stats GRPC Server is now running on port-> 0.0.0.0:50000
Testing the service
To test the service, we can use the node module grpcli that 
we installed. To launch grpcli and point it to the proto file and 
server that is running, use the command given below:
grpcli -f osfy_stats.proto --ip=127.0.0.1 --port=50000 -i
This will initiate the connection, and we can then use the 
that you see, i.e., 1, 2, 3, etc, are used by the binary format 
while encoding and decoding the messages. 
 
The toparticles method takes in the number of top 
articles that we want as input and returns  an array of 
articles, where each article contains fields like the ID, 
title and a URL.
 
The articleStats method takes the article’s ID as input, 
and returns a message containing stats like the number of 
views, likes and comments.  
Implementing our server
Now that we have defined the service, we can implement 
our server. We will be using Node.js for the task, and it is 
assumed that you have a Node.js environment set up and 
available on your machine. 
We will need a few node libraries to be installed 
and you can use npm install: 
$ npm install grpc
$ npm install grpcli
Now, let us look at the implementation on the server side. 
The file server.js is shown below:
const grpc = require('grpc');
const proto = grpc.load('osfy_stats.proto');
const server = new grpc.Server();
let top_articles = [
    { id:20000 , title: 'T1', url: 'URL1' },
    { id:20001 , title: 'T2', url: 'URL2' },
    { id:20002 , title: 'T3', url: 'URL3' },
    { id:20003 , title: 'T4', url: 'URL4' },
    { id:20004 , title: 'T5', url: 'URL5' }
];
//define the callable methods that correspond to the methods 
defined in the protofile
server.addProtoService(proto.osfy_stats.OSFYStatsService.
service, {
  toparticles(call, callback) {
    if (call.request.numResults < 1 || call.request.
numResults > 5) {
      callback(new Error('Invalid number of Results provided. 
It should be in the range of [1-5]'));
    } else {
    var topresults = top_articles.slice(0, call.request.
numResults);
      callback(null, { articles:topresults });

www.OpenSourceForU.com | OPEN SOURCE FOR yOU | May 2017 | 47
Developers
Let’s Try
By: Romin Irani
The author has been working in the software industry for 
more than 20 years. His passion is to read, write and teach 
about technology, in order to help developers succeed. He 
blogs at www.rominirani.com.
[1] gRPC.io home page: www.grpc.io 
[2] gRPC Documentation: http://www.grpc.io/docs/ 
[3] Protocol Buffers Documentation: https://developers.google.
com/protocol-buffers/docs/overview 
[4] gRPC Community: http://www.grpc.io/community/ 
[5] Cloud Native Computing Foundation: https://www.cncf.io/
References
rpc list and call methods to test out the service. The sample 
calls are shown below:
$ grpcli -f osfy_stats.proto --ip=127.0.0.1 --port=50000 -i
Package: osfy_stats
Service: OSFYStatsService
Host: 127.0.0.1
Port: 50000
Secure: No
[grpc+insecure://127.0.0.1:50000]# rpc list
toparticles(TopArticlesRequest) {
  return TopArticlesResponse;
}
articleStats(ArticleStatsRequest) {
  return ArticleStatsResponse;
}
[grpc+insecure://127.0.0.1:50000]# rpc call articleStats 
{"id":1}
Info: Calling articleStats on OSFYStatsService
Response:
{
  "id": 1,
  "numViews": 1000,
  "numLikes": 30,
  "numComments": 5
}
[grpc+insecure://127.0.0.1:50000]#
Consuming the gRPC service
Now that we have tested our service, we can write our client 
code as shown below. The code is similar in the sense that we 
load the proto file first and then create a client to the server 
that is running. Once the client is connected, we can directly 
invoke the service methods as shown below: 
const grpc = require('grpc');
const proto = grpc.load('osfy_stats.proto');
const client = new proto.osfy_stats.OSFYStatsService('localho
st:50000', grpc.credentials.createInsecure());
client.toparticles({"numResults":2}, (error, response) => {
  if (!error) {
    console.log("Total Articles: " + response.articles.
length);
    for (article of response.articles) {
       console.log(article.id + " " + article.title + " " + 
article.url);  
    }
  } else {
    console.log("Error:", error.message);
  }
});
client.articleStats({"id":2}, (error, response) => {
  if (!error) {
    console.log("Article ID : " + response.id + " Views : 
" + response.numViews + " Likes : " + response.numLikes + " 
Comments : " + response.numComments );
  } else {
    console.log("Error:", error.message);
  }
});
You can execute the client and see that the two actions 
available by the service are invoked. The output is shown below:
$ node client.js
Article ID : 2 Views : 1000 Likes : 30 Comments : 5
Total Articles: 2
20000 T1 URL1
20001 T2 URL2
Other language bindings
We have seen how we can define the service interface via the 
ProtocolBuffers format, and then used Node.js to develop both 
the server side and client side bindings. The advantage of gRPC 
is that your server side could be implemented in a specific 
language, say Node.js, but the client bindings could be in another 
language, like Python, Java or other supported bindings. 
To see the list of languages supported, check out the 
documentation page (http://www.grpc.io/docs/) and select a 
language of your choice.  
Modern architectures suggest breaking up monolithic 
applications into multiple micro-services and to compose your 
applications via those services. This results in an explosion of 
inter-service calls and it is important that latency, which is one 
of the key factors to consider in providing a high performance 
system, is addressed. gRPC provides both an efficient wire 
transfer protocol and multiple language bindings that make 
this a possibility. With the recent adoption of gRPC as a top 
level project in the Cloud Native Computing Foundation, we 
are likely to see an increase in developers exploring and using 
this across a wide range of projects. 



50 | May 2017 | OPEN SOURCE FOR yOU | www.OpenSourceForU.com
Developers Let’s Try
Q
t is a cross-platform development framework 
designed to provide eye candy GUI features and 
to give rich API support for Web communication, 
graph plotting, data exchange, 3D visualisation, multimedia 
handling, location tracking, sensor interfacing, etc. Qt 
applications can be built for various desktop platforms like 
Linux, Mac OS and Windows; for embedded platforms like 
Raspberry Pi and QNX, and also for mobile platforms like 
Android, iOS and Windows Mobile.
This article explores Qt support for connectivity to IoT 
platforms like ThingSpeak using HTTP REST APIs, and 
also focuses on handling JSON data in terms of encoding, 
parsing, URL encoding, forming query strings, etc. The 
assumption is that readers have basic familiarity with Qt 
and RESTful operations using HTTP support. If you are 
new to the Qt environment, do refer to some of the previous 
articles published in earlier editions of OSFY, as listed in the 
References, before proceeding.
About HTTP REST and ThingSpeak
ThingSpeak is an open source platform for building IoT 
applications, which supports connectivity using HTTP 
REST APIs. One can send data using the POST method 
with a request payload or the GET method with a query 
string, and retrieve data in JSON or XML formats using 
the GET method. Recently, support for sending data using 
MQTT Publish has also been added. ThingSpeak has good 
integration support for MATLAB from Mathworks, which 
helps it to offer better analysis and visualisation of data. 
It provides SDKs in various languages for connectivity 
or one can try to connect using any HTTP client of the 
preferred language.
In ThingSpeak, data is stored in terms of channels, and 
each channel is associated with an ID, name, description 
and various applicable fields along with some tags, geo-
location, etc. For simple and better authentication, API 
keys are provided for read/write operations and a channel 
key for meta operations. The API keys of a specific 
channel can be embedded in a query string or post data 
for authentication. Secure HTTP (https) is preferred to 
prevent unauthorised access of API keys by eavesdroppers. 
A channel can also be made public during the creation or 
by changing the settings later, in which case no read key is 
required for viewing the data.
One can log in to ThingSpeak using a Mathworks account, 
which is free of cost, and create any number of channels 
Qt Programming  
for HTTP REST Clients
This article discusses Qt support for connectivity to IoT platforms like ThingSpeak 
using HTTP REST APIs. It also focuses on handling JSON data in terms of encoding, 
parsing, URL encoding, forming query strings, etc. 

www.OpenSourceForU.com | OPEN SOURCE FOR yOU | May 2017 | 51
Developers
Let’s Try
and send maximum feeds—the only limitation on updating 
channels is an interval of 15 seconds. Once the channel is 
created with a suitable description, applicable fields and meta 
information, you can identify the read or write API keys under 
Channel settings and the key for channel meta operations in 
the Profile section.
Let’s now look at how a REST client for connecting with 
ThingSpeak can be built.
Qt HTTP connectivity
In order to talk to any network server using the HTTP 
protocol, Qt provides the QNetworkAccessManager class, 
which has asynchronous methods like get, post, put and 
deleteResource to perform various REST operations. These 
methods take the QNetworkRequest object as an argument, as 
well as an additional argument for request payload in the form 
of QByteArray for POST, PUT operations. QNetworkRequest 
mainly contains the URL in the form of a QUrl object, which 
encapsulates the protocol, host name, port, path, query string, 
etc. The QUrlQuery class can be used to form a QueryString 
effectively. These methods return the response in the form 
of a QNetworkReply, which needs to be explicitly destroyed 
using the deleteLater method in the slot connected to the 
finished signal of QNetworkAccessManager.
Let’s consider a channel with three fields representing 
temperature, humidity and pressure. Let us  assume that 
the various API keys and channel numbers are initialised 
as follows: 
QString RDKey = “XXXXXXXXXXXXXXXX”;
QString WRKey = “XXXXXXXXXXXXXXXX”;
QString CHKey = “XXXXXXXXXXXXXXXX”;
Qstring CHNum = “xxxxxx”;
To send a feed, we need to perform the POST method 
with the steps that follow, using the URL https://api.
thingspeak.com/update.json and the following JSON data 
as request payload:
 
{
 
 “api_key”:”XXXXXXXX XXXXXXXX”,
 
 “field1”:”25”,
 
 “field2”:”72”,
 
 “field3”:”900”
 
}
Step 1: Let’s prepare the data for POST in JSON format, 
assuming that tval, hval and pval represent the temperature, 
humidity and pressure values in integer form.
QVariantMap feed;
feed.insert(“api_key”,WRKey);
feed.insert(“field1”,QVariant(tval).toString());
feed.insert(“field2”,QVariant(hval).toString());
feed.insert(“field3”,Qvariant(pval).toString());
QByteArray payload=QJsonDocument::fromVariant(feed).toJson();
You may verify the prepared JSON data in QString format 
as follows: 
qDebug() << Qvariant(payload).toString();
Or you can display it in a QLineEdit or QTextEdit widget 
for testing purposes.
Step 2: Prepare the URL as follows:
QUrl myurl;
myurl.setScheme(“http”); //https also applicable
myurl.setHost(“api.thingspeak.com”); 
myurl.setPath(“/update.json”); 
We are skipping the user name and password as we are 
using API keys for authentication. We are also skipping 
setPort as the service is running on the default port 80. You 
can verify the prepared URL as follows:
qDebug() << myurl.toString();
Step 3: Prepare the network request, as follows:
QNetworkRequest request;
request.setUrl(myurl);
request.setHeader(QNetworkRequest::ContentTypeHeader,
 
  
 
 
 
 
 
”application/json”);
Step 4: Perform the POST operation using the following 
code in the slot connected to the Publish PushButton click.
QNetworkAccessManager *restclient; 
 
 
  //in class
restclient = new QNetworkAccessManager(this); //constructor
QNetworkReply *reply = restclient->post(request,payload);
qDebug() << reply->readAll();
restclient can be declared in the MainWindow or the 
Figure 1: Sending a feed using POST 

52 | May 2017 | OPEN SOURCE FOR yOU | www.OpenSourceForU.com
Developers Let’s Try
Dialog class and the QNetworkAccessManager object can be 
created in constructor, as one object is sufficient to perform 
all the operations.
The same update operation can be achieved using the 
GET method and data encoded in the URL as a query string:
QUrlQuery querystr; 
querystr.addQueryItem(“api_key”,WRKey);
querystr.addQueryItem(“field1”,”25”);
querystr.addQueryItem(“field2”,”72”);
querystr.addQueryItem(“field3”,”900”); 
myurl.setScheme(“https”);
myurl.setHost(“api.thingspeak.com”);
myurl.setPath(“/update”);
myurl.setQuery(querystr);
request.setUrl(myurl);
reply = restclient->get(myurl);
qDebug() << reply->readAll();
 
url.setHost(“api.thingspeak.com”);
 
url.setPath(“/channels/”+CHNum+”/feeds.json”);
 
url.setQuery(“api_key=”+RDKey+”&results=10”);
 
request.setUrl(url);
 
reply=nwManager->get(request);
To process the responses holding all feeds in the JSON 
format, connect the finished signal of restclient to a suitable 
custom slot.
QObject::connect(restclient, SIGNAL(finished(QNetworkReply*)),  
 
  
 
this, SLOT(replyFinished(QNetworkReply *)));
Parsing JSON data
The returned JSON data has two primary fields with the 
names channel and feeds, and the value of feeds is an array 
of all feeds, where each array element consists of fields like 
entry_id, field1, field2, field3, created_at, etc.
To traverse all array elements and retrieve the specific 
fields in each, you can use the following code to parse the 
JSON data in the replyFinished slot:
 
QJsonDocument jsdoc;
 
jsdoc = QJsonDocument::fromJson(reply->readAll());
 
QJsonObject jsobj = jsdoc.object();
 
QJsonArray jsarr = jsobj[“feeds”].toArray();
 
foreach (const QJsonValue &value, jsarr) {
 
 QJsonObject jsob = value.toObject();
 
 qDebug() << jsob[“entry_id”].toInt();
 
 qDebug() << jsob[“field1”].toString();
 
 qDebug() << jsob[“field2”].toString();
 
 qDebug() << jsob[“field3”].toString();
 
 qDebug() << jsob[“created_at”].toString();
 
}
 
reply->deleteLater();
Rendering data in tabular form
To display the data in tabular form, create a TableWidget 
through TableView with the name tableFeeds and add 
the following code in the slot connected to the Retrieve 
PushButton click.
 
ui->tableFeeds->clearContents();
 
ui->tableFeeds->setColumnCount(5);
 
ui->tableFeeds->setRowCount(jsarr.count());
 
foreach (const QJsonValue &value, jsarr) {
        QJsonObject jsob = value.toObject();
        ui->tableFeeds->setItem(k,0,
 
  
new QTableWidgetItem(jsob[“entry_id”].
toString()));
        ui->tableFeeds->setItem(k,1,   
 
  
 
 
  
 
 
new QTableWidgetItem(jsob[“field1”].toString()));
        ui->tableFeeds->setItem(k,2,   
  
 
 
  
 
 
 
new QTableWidgetItem(jsob[“field2”].toString()));
Figure 2: Sending a feed using GET
Alternatively, data encoded in the URL query string 
format can be used to perform the POST operation using the 
same URL api.thingspeak.com/update.json:
 
//Prepare querystr similar to above steps
 
request.setHeader(QNetworkRequest::ContentTypeHeader,   
 
  
 
“application/x-www-form-urlencoded”);
 
QByteArray postdata = Qvariant(querystr).toByteArray();
 
restclient->post(myurl,postdata);
Retrieving data from ThingSpeak
To retrieve the feeds from ThingSpeak, we need to perform the 
GET operation on http://api.thingspeak.com/channels/xxxxxx/
feeds.json and api_key=XXX..XX  as the query string. For this 
purpose, use the following code in the slot connected to the 
Retrieve PushButton click. You can append results=10 to the 
query string to limit the number of results to the last 10 feeds.
 
QUrl url;
url.setScheme(“http”);

www.OpenSourceForU.com | OPEN SOURCE FOR yOU | May 2017 | 53
Developers
Let’s Try
        ui->tableFeeds->setItem(k,3,   
 
  
 
 
 
 
  
new QTableWidgetItem(jsob[“field3”].toString()));
        ui->tableFeeds->setItem(k,4,    
 
 
 
  
 
 
new QTableWidgetItem(jsob[“created_at”].toString()));
        k++;
     }
By: Rajesh Sola
The author is a faculty member of C-DAC’s Advanced 
Computing Training School, and an evangelist in the 
embedded systems and IoT domains. You can reach him at 
rajeshsola@gmail.com.
[1] doc.qt.io
[2]  in.mathworks.com/help/thingspeak/
[3]  github.com/rajeshsola/qt-examples
  
Please refer to the following articles to get started with Qt:
[4]  http://opensourceforu.com/2015/03/qt5-let%c2%92s-
learn-some-theory/
[5]  http://opensourceforu.com/2015/03/qt5-console-
applications-and-networking/
[6]  http://opensourceforu.com/2012/01/developing-
applications-qt-part-1/
[7]  http://opensourceforu.com/2012/02/developing-
applications-qt-part-2/
[8]  http://opensourceforu.com/2012/03/developing-apps-
qt-part-3/
[9]  http://opensourceforu.com/2012/05/developing-apps-
qt-part-4/
References
Figure 3: Retrieving data and displaying it in tabular form
to other IoT platforms and database servers like InfluxDb, 
which support HTTP REST based connectivity. You can find 
the entire code for this example at github.com/rajeshsola/qt-
examples/tree/master/thingspeak-demo. 
You can also plot the graph with the above feeds using 
the third party library, QCustomPlot (http://www.qcustomplot.
com) or with the help of Qt Charts introduced in version 5.7.0.
As an example, we have discussed connectivity to 
ThingSpeak in this article. You can apply these concepts 

54 | May 2017 | OPEN SOURCE FOR yOU | www.OpenSourceForU.com
Developers Let’s Try
different command line interfaces (CLI) or in a graphical 
manner (using the GUI).  A shell takes input from us in the 
form of commands, processes it and then gives an output. 
It can be accessed by a terminal, which runs it. Whenever 
we run the terminal, the shell actually issues a command 
prompt, where we can type our own input, which is then 
executed when we hit the Enter key. The output is thereafter 
displayed on the terminal. The shell basically wraps around 
the delicate interior of an operating system, protecting it 
from any accidental damage and, hence, the name. There are 
different types of a shell, such as Korn shell (ksh), Bourne 
shell (sh), C shell (csh), Bourne Again shell (bash), a remote 
shell (rsh), etc. A script written in any such environment is 
referred to as a shell script.
The different operations that can be performed by shell 
scripts include file manipulation, execution of programs, and 
printing text. A script that sets up the specific environment, 
runs the program, does clean-ups if necessary, does logging, 
etc, is basically called a wrapper, which refers to the 
automated mode of running a specific operating system shell. 
A Quick Look at  
Programming with Shell Scripts
#!/bin/bash
The power of scripting lies in the fact that you get to program with commands you 
already know, from various computer languages. Scripting provides a means to 
substitute really complicated and convoluted commands and if there is something 
repetitive, then a script can probably remove the tedium in your work. 
I
magine that you are doing some important task on your 
computer system and, suddenly, you get a black screen with 
something being automatically typed on it. I am sure many 
of us have experienced such system reboots that take place 
on their own due to some hardware or software issues.  When 
this happens, our pulse rate shoots up, and we hope and pray 
that the task that was being performed has been saved, or else, 
everything will have to be started afresh. 
Have we ever given a thought to how all this happens on 
its own? Well, it’s shell scripting that drives such processes 
automatically. A newbie in the computer programming arena 
generally gets anxious just seeing the black screen on the 
computer, assuming the programming involved is quite difficult. 
In shell scripts, the term ‘shell’ defines the interface 
between the user and the different services of an operating 
system such as Linux, UNIX, Windows (to some extent) 
or Mac. Apart from the shell, the kernel is the other main 
component of an operating system. It’s the kernel that makes 
the communication between the hardware and software 
possible. The interface can be created by a shell through 

www.OpenSourceForU.com | OPEN SOURCE FOR yOU | May 2017 | 55
Developers
Let’s Try
In different operating systems, shell scripts are referred to 
by different names such as batch files (OS/2, MSDos-Win95 
stream), command procedures (VMS), and shell scripts 
(Windows NT stream and third-party derivatives). Mainframe 
operating systems are associated with a different set of terms.
Shell scripts help us to program different commands in 
chains and have the system execute them as a scripted event 
—the same as batch files. They also allow us to perform far 
more useful functions, like command substitution. We can 
invoke a command, like date, and then use its output as part 
of a file-naming scheme. We can even automate backups, 
and each copied file can have the value of the current date 
appended to the end of its name. Shell scripts are not just 
invocations of different commands either, but are programs in 
their own right. Shell scripting also allows us to use different 
programming functions – such as if/ then/ else statements, 
‘for’ loops, and so on – directly within our operating system’s 
interface. We don’t have to learn another language because we 
are using what we already know—the command line.
Different features of shell scripts
Let’s have a look at the different features of shell scripts.
Running batch jobs: Shell scripts allow different sets 
of commands that are entered manually at a command line 
interface to be executed automatically, without having 
to wait for a user to separately trigger each stage of the 
sequence. For example, in a directory that has three C source 
code files, instead of manually running the four commands 
which are required to build the final program from them, 
one could create a C shell script, and keep it in the directory 
along with the C source code files, which would ultimately 
compile them automatically. The script would further allow 
a user to save the file being edited, pause the editor, and then 
just run the shell script to create the updated program, test 
that, and return to the editor again.
 Different shortcuts to perform specific functions: We 
can use the different available shortcuts in shell scripting 
to perform some specific action. These shortcuts are short 
commands written for specific utilities. They are like 
inbuilt functions, which we use in different programming 
languages. A shell script can also provide convenient 
variations of a system command, where different special 
environment settings, post-processing or command options 
apply automatically, but in such a way that they allow the 
new script to still act as a complete normal UNIX command. 
For example, to create a version of the ls command to list 
different files, it can be given a shorter command name of  
‘l’. This shorter command will be normally saved in the bin 
directory of a user as /home/username/bin/l and a default set 
of different command options will be pre-supplied. The user 
can then use ‘l’ for most commonly used short listings.
Generalisation: Simple batch jobs are usual for isolated 
tasks, but at the same time, using shell loops, variables and tests 
provides much more flexibility to users. Let’s look at a bash file 
(shell script) to convert different JPEG images to PNG images, 
in which the names of images are provided on the command 
line—possibly using wildcards. Instead of each of the names 
being listed within the script, they can be created with this 
file, typically saved in /home/username/bin/jpg2png. So we 
can directly run this command on an entire directory of JPEG 
images with just /home/username/bin/jpg2png *.jpg.
Flavour of programming: Many modern shell scripts 
also supply various features which are usually found only in 
more sophisticated programming languages, such as control-
flow constructs, comments, variables, subroutines, arrays 
and so on. With such features available, it is quite possible 
to write reasonably sophisticated applications using shell 
scripts. However, they are still limited by the fact that most 
shell scripting languages have little or even no support for 
data typing systems, threading, classes, complex math and 
other such common language features. They are also much 
slower than the compiled code or interpreted languages that 
are written with speed as a performance goal. The standard 
UNIX tools, Awk and Sed, provide extra capabilities for shell 
programming. Perl can also be embedded in different shell 
scripts as can other scripting languages such as Tcl, etc.
 Verisimilitude: A key trait of different shell scripts 
is that the invocation of their interpreter is considered as 
a core operating system feature. So, instead of a user's 
shell only being able to run different scripts in that shell's 
language or a shell script only having directives of its 
Figure 1: A shell acts as an interface between the user and the OS 
(Image credits: http://www.guru99.com/)
Hardware
Operating
System
Terminal
User
Kernel
Shell
Figure 2: The different functions of a shell 
(Image credits: googleimages.com)
Figure 3: Path shortcuts in shell scripting 
(Image credits: googleimages.com)
SHELL
pipeline
hookup
I/O
redirection
environment
control
interpreted
programming
language
program
execution
variable and
filename
substitution

56 | May 2017 | OPEN SOURCE FOR yOU | www.OpenSourceForU.com
Developers Let’s Try
different. Various earlier versions of Windows NT are able 
to successfully run contemporary versions of 4OS2 with 
the help of the OS/2 sub-system.
4. Scripting languages can be extended; for example, 
Windows NT and MS-DOS/Windows 95/98 type systems 
allowed for batch/shell programs to call different tools 
like KixTart, QBasic, Rexx; various Basic, Python and 
Perl implementations; and the Windows Script Host 
and all its installed engines. On UNIX and other Posix-
compliant systems, Sed and Awk are used to extend the 
string and the numeric processing ability of shell scripts. 
Tcl, Rexx, Perl and Python have graphics toolkits that 
can be used to code different procedures and functions 
for shell scripts. This poses a speed bottleneck (Fortran, 
C, and Assembly language are much faster still) and 
adds functionality such as sockets and other connectivity 
functions, which are not available in the shell language. 
VBA and VBScript can be easily used to communicate 
with databases, spreadsheets, scriptable program of all 
types, development tools, telecommunications software, 
graphics tools and other software that can be accessed 
using the Component Object Model.
Writing your first shell script
Well, we are now at the most interesting part, which is, 
programming using the shell script. So we will actually follow 
three simple steps in order to successfully write our first shell 
script, which is the famous program to display ‘Hello World’. 
The pre-requisites are: 
 
You need to have Linux installed on your system.
 
You need to have a text editor (select from Vim, kwrite, 
etc), as well, on your system.
Now, let’s follow the three simple steps.
1. Write the script
A shell script is basically a file that contains ASCII text. In 
order to create a shell script, we use a text editor, which is 
nothing but a program, similar to a word processor, which 
reads and writes different ASCII text files. There are many 
text editors available for our Linux system, both for the GUI 
interpreter handled correctly, if it is run from a shell 
(both were limitations in the early Bourne shell's script 
handling), shell scripts are set up and executed by 
the OS itself. Nowadays, a modern shell script is not 
just on the same footing as the system commands, but 
rather, many of the system commands are actually shell 
scripts. This extends to the returning exit codes like 
other system utilities in order to indicate success or 
failure, and also allows them to be called components 
of the larger programs, regardless of how all those 
larger tools are implemented. Like different standard 
system commands, shell scripts omit any kind of file 
name extension unless it is intended to be read into 
a running shell using a special mechanism for this 
purpose (such as csh’s source or sh’s ‘.’).
Shell scripting on various operating systems
It is possible to leverage the various advantages of shell 
scripting on different operating systems (other than UNIX 
and Windows) as well with the help of interoperability 
software. Let’s have a look at some of these options.
1. Interoperability software like Cygwin, Interix (which 
is available in the Microsoft Windows Services for 
UNIX), MKS Toolkit, UWIN (AT&T UNIX for 
Windows), Hamilton C shell, and others, allow UNIX 
shell programs to be run on different Windows NT 
machines and all their successors, with some loss of 
functionality on the MS-DOS-Windows 95 branch, as 
well as earlier MKS Toolkit versions for OS/2. At least 
three DCL implementations for Windows OS – apart 
from XLNT, a multiple-use scripting language package 
(which can be used with the command shell), CGI 
programming and Windows Script Host -- are available 
for these types of systems as well. 
2. In addition to all these tools, some OS/2 and Posix 
functionality can also be used with the corresponding 
environmental sub-systems of the Windows NT OS 
series up to Windows 2000 as well. A third 16-bit 
sub-system, often called the MS-DOS sub-system, uses 
Command.com which is provided with these OSs to 
run all the just-mentioned MS-DOS batch files.
3. Different console alternatives such as 4DOS, 4NT, 
4OS2, Peter Norton's NDOS, FreeDOS, and the 
GUI Take Command, which add the functionality to 
Windows NT-style Cmd.exe, OS/2's Cmd.exe, MS-
DOS/Windows 95 batch files (run by Command.com), 
and 4NT, respectively, are similar to the shells that they 
actually enhance and are much more integrated with the 
Windows script host.  The Windows Script Host comes 
with three engines (VBScript, VBA and JScript ) that 
are pre-installed, and to which a number of third-party 
engines can be added with Perl, Rexx, Python, Tcl and 
Ruby with pre-defined functions in 4NT. PC DOS is 
also quite similar to MS-DOS, while DR DOS is quite 
Figure 4: Output redirection and piping in shell scripting 
(Image credits: googleimages.com)
Stdin
program
Stderr
stdout
I/O
args
storage
network
X11

www.OpenSourceForU.com | OPEN SOURCE FOR yOU | May 2017 | 57
Developers
Let’s Try
By: Vivek Ratan 
The author currently works as an automation test 
engineer at Infosys, Pune. He can be reached at 
ratanvivek14@gmail.com for any suggestions or queries.
environment and the command line environment. Here is a 
list of a few:
1. Vi or Vim
2. Emacs
3. Nano
4. Gedit
5. kwrite
Now, we can type in the first script using any of the 
above text editors, as follows:
#!/bin/bash
# My first script
echo "Hello World!"
The above script has three lines.
 
The first line is a special clue given to the shell in order 
to indicate what program is used to interpret this script. 
In this case, its /bin/bash. Other scripting languages 
such as Awk, Perl, Tcl, Tk, and Python can also use this 
type of mechanism.
 
The second line is just a comment. Everything that 
comes after a ‘#’ symbol is all ignored by bash. As 
our scripts become larger and more complicated, 
comments become important. They are basically used 
by programmers to explain what’s going on, so that 
others can understand. 
 
The last line is the echo command, which just prints 
whatever is given on the display. Once we have written 
the above script, we can save this file using any name. 
Here, we save it as First_Shell_Script.
2.  Give the shell permission to execute it
The next thing we need to do is to give the shell 
permission to execute our script. This is done using the 
chmod command, as follows:
[me@linuxbox me]$ chmod 755 my_script
The ‘755’ will give us permission to read, write and to 
execute. Everybody else will just have read and execute 
permission. If we want our script to be private (so that only 
we can read and execute), we use ‘700’ instead.
3. Put it at the location where the shell can find it
We now need to run our script by typing the 
following command:
[me@linuxbox me]$ ./First_Shell_Script
Now, you should see ‘Hello World!’ displayed. If we 
don’t, then we need to check what directory we saved our 
script in, so that we can go there and try again.
Before we go further, let’s discuss paths a little bit. 
When we type in the name of a command, our system 
does not search the entire computer in order to find where 
the program is located. That would really take a long time. 
You will have noticed that we don't usually have to specify 
the complete path name to the program we want to run; the 
shell just seems to know that. The shell maintains a list of 
different directories where executable files are kept, and it just 
searches for the directories in that list. If it’s not able to find 
the program after searching each of the directories in the list, 
it will throw up the error: Command not found.
This list of directories is called the path. Let us look 
at some of the commands related to this along with their 
possible output.
 
The command for viewing the list of directories 
is as follows:
 [me@linuxbox me]$ echo $PATH
The output: A colon-separated list of directories which 
will be searched if a specific path is not given when a 
command is attempted. 
 
The command for adding directories to the path, where 
directory_name is the name of directory we want to add, is:
[me@linuxbox me]$ export PATH=$PATH:directory_name
The output: directory_name will be added to the path. 
 
The command for creating a bin directory, which is a sub-
directory of our home directory, is as follows:
[me@linuxbox me]$ mkdir bin
The output: The bin directory will be created.
Shell scripting can be used in the following cases:
1. To create our own command or tool.
2. To monitor several tasks like disk usage, etc.
3. To take data backup and snapshots.
4. For automatic system booting.
5. For automatic installation of different application packages.
6. To automate different aspects of computer maintenance 
and user account creation.
7. To create startup scripts for different unattended applications.
8. To kill or start multiple applications together. 
[1] http://www.wikipedia.org/
[2]  http://www.guru99.com/
[3] http://linuxcommand.org
References

58 | May 2017 | OPEN SOURCE FOR yOU | www.OpenSourceForU.com
Developers Insight
W
e are all surrounded by a number of gadgets, mobile 
devices, smartphones, wireless nodes and many other 
objects that are digitally connected in real-time. The 
Internet of Things (IoT) enables  real world objects to interact 
with each other. These objects can share information and 
communicate in real-time to deliver better performance as well as 
security. IoT works by developing and integrating smart objects 
that can be controlled using remote network infrastructure.
The term, the Internet of Things, was coined by 
Kevin Ashton in 1999. The implementation of IoT is 
now widespread because of high performance wireless 
technologies. Radio frequency identification (RFID) tags 
and sensors are the most important components of IoT. 
The RFID tags can be embedded in real world devices and 
objects, which can be monitored remotely using software 
based applications. RFID readers can be used to locate, read 
and sense RFID implanted objects. Very small, micro-sized 
transmitting and receiving chips are integrated with RFID to 
help them communicate with distant objects.
As per reports from Forbes.com, the market for the 
Internet of Things is expected to reach around US$ 267 
billion by 2020. Analysis from Gartner underlines that around 
8.4 billion objects, with investments amounting to US$ 273 
billion, will be interconnected with each other in the current 
year (2017-18). This represents a 31 per cent increase over the 
figures of 2016-17.
Some of the key applications of IoT include: 
 
Smart cities, retail points, smart grids, agriculture and 
farming, homes and offices
 
The Internet of Vehicles (IoV)
 
Connected cars
 
Connected railways infrastructure
 
Wearable devices 
 
Software defined networking
 
The industrial Internet
 
Energy management
Programming the Internet of Things 
Using Contiki and Cooja
Contiki is an operating system with a focus on low power IoT devices. Cooja 
is the Contiki network simulator. Cooja allows the large and small networks of 
Contiki motes to be simulated. This article takes the reader through the process 
of programming IoT with Contiki and Cooja.

www.OpenSourceForU.com | OPEN SOURCE FOR yOU | May 2017 | 59
Developers
Insight
Research domains in IoT
Due to the ever increasing deployment of IoT in the 
government and in the private sector, there is a need to 
analyse various factors that can affect its overall performance 
and efficiency.
The following are some of the key research issues in IoT:
 
Security, privacy and trust architectures
 
Big Data analysis and scalability
 
Device interoperability and compliance
 
Robustness and fault tolerance
 
Cognitive networking
 
Energy aware approaches
 
Virtualisation
 
Ontology models
Free and open source tools for IoT programming
Featured below are some well known open source tools used 
for IoT programming.
OpenIoT (http://www.openiot.eu/): This is a free and 
open source platform to manage and program the sensors on 
the cloud and the Internet based environment. The concept of 
Sensing-as-a-Service is being adopted in OpenIoT.
Zetta (http://www.zettajs.org/): This is a free and open 
source platform that is based on Node.js. Zetta is used to 
create the IoT servers that can control and run the globally 
distributed systems, sensors and computers, including those 
that are on the cloud.
DSA (http://www.iot-dsa.org/):  Distributed Services 
Architecture is a powerful, open source IoT library that is 
available for free distribution. It enhances the performance of 
inter-object communication and makes it very effective. DSA 
provides the toolkit for managing IoT based applications, 
services and objects.
Node-RED (http://nodered.org/): This provides the 
programming interface and APIs for the Internet of Things. 
Using Node-RED, the flow based creation of remote IoT 
objects can be done easily with a Web browser based flow 
editor. In the flow editor of Node-RED, the JavaScript code 
can be executed and remote objects can be programmed easily 
with powerful functionalities.
IoTivity (https://www.iotivity.org/): This is a 
powerful open source library which enables inter-object 
connectivity with enormous speed and performance. It 
is written and programmed in C and C++. Most of the 
performance aware protocols (like ANT+, Bluetooth low 
energy, Wi-Fi Direct, Zigbee and Z-Wave) can be easily 
integrated with IoTivity.
There are other open source implementations for the 
Internet of Things as listed below.
Development toolkits and libraries
 
Arduino
 
Eclipse IoT Project
 
Kinoma
 
M2MLabs Mainspring
 
Node-RED
 
ThingBox
Automation for home and offices
 
Eclipse SmartHome
 
Home Gateway Initiative (HGI)
 
Ninja Blocks
 
openHAB
 
PrivateEyePi
 
RaZberry
 
The Thing System 
Middleware
 
IoTSyS
 
Kaa
 
OpenIoT
 
OpenRemote 
Operating systems
 
AllJoyn
 
Brillo
 
Contiki
 
FreeRTOS
 
Raspbian
 
RIOT
 
Spark
 
TinyOS
IoT integration tools and horizontal platforms
 
Canopy
 
Chimera IoT
 
DeviceHive
 
IoT Toolkit
 
M2MLabs Mainspring
 
Mango
 
Nimbits
 
Open Source Internet of Things (OSIoT)
 
OpenRemote
 
Pico Labs
 
prpl Foundation
 
RabbitMQ
 
SiteWhere
 
SiteWhere
 
ThingSpeak
 
webinos
 
Yaler
Protocols
 
Advanced Message Queuing Protocol (AMQP)
 
Constrained Application Protocol (CoAP)
 
Extensible Messaging and Presence Protocol (XMPP)
 
OASIS Message Queuing Telemetry Transport (MQTT)
 
Very Simple Control Protocol (VSCP)

60 | May 2017 | OPEN SOURCE FOR yOU | www.OpenSourceForU.com
Developers Insight
to get the desired results. Contiki works on IPv4 as well as 
IPv6 networking with the integration of lightweight protocols, 
so that low power chips and radio frequency chips can be 
connected without performance issues.
The URL for downloading Instant Contiki is http://
sourceforge.net/projects/contiki/files/Instant%20Contiki/.
Once the compressed Instant Contiki is downloaded, it 
can be used on any host operating system. Instant Contiki 
is available on sourceforge.net as a compressed file which 
then has to be extracted. The uncompressed or extracted 
Instant Contiki can be executed on VMware Player, which 
is a virtualisation tool that can be downloaded free and is 
available at http://www.vmware.com/go/downloadplayer/.
In the extracted folder of Instant Contiki, there is the 
executable file Instant_Contiki_Ubuntu_12.04_32-bit.vmx.
This file, on executing, will automatically open  in 
VMware Player, and we will be ready to work with Contiki 
using virtualisation software in parallel with any host 
operating system. The default password for the Contiki 
operating system is ‘user’.
After loading the Contiki OS, the commands given 
in Figure 3 are executed in the terminal so that the Cooja 
simulator gets loaded for the implementation of IoT.
Creating a new network in the Cooja simulator
In the file menu of Cooja, select New Simulation as 
shown in Figure 4.
In the dialogue box, the basic network parameters are 
Figure 4: Creating new IoT simulation in Cooja
Figure 5: Set-up of basic simulation properties in Cooja
Figure 1: Directory structure and files in Instant Contiki
Figure 2: Instant Contiki login screen
Figure 3: Loading the Cooja simulator in Contiki
Implementations for engineering
 
Open Garden
 
Open Source Robotics Foundation
 
OpenWSN
Using the Contiki OS with the Cooja 
simulator to program the IoT
Contiki (http://www.contiki-os.org/) is a popular, free 
and open source operating system for IoT programming 
available under the BSD licence with the base code of 
the C programming language. Contiki can be used for 
communication between low powered RFID chips in wireless 
networks with a high degree of performance and security.
The programming on Contiki is done using the Cooja 
network simulator, in which the base libraries of RFID chips 
and sensors are available in C. To program, control and 
monitor the remote IoT devices, the back-end C programs 
and related header files can be customised and recompiled 

www.OpenSourceForU.com | OPEN SOURCE FOR yOU | May 2017 | 61
Developers
Insight
By: Dr Gaurav Kumar
The author is the MD of Magma Research and Consultancy 
Pvt Ltd, Ambala. He is associated with various academic and 
research institutes, where he delivers lectures and conducts 
technical workshops on the latest technologies and tools. 
You can contact him at kumargaurav.in@gmail.com. Website: 
www.gauravkumarindia.com. 
Figure 6: Invoking wireless and RFID motes in the Cooja simulator
Figure 7: Importing C source code to recompile in Cooja
Figure 8: Compiling C source code for the desired behaviour of wireless motes in Cooja
Figure 9: Viewing wireless motes with positions in Cooja
Figure 10: Running simulation and behaviour analytics of motes
imported in the simulation area so that the transmission of 
radio signals can be viewed and analysed.
In this simulation of the IoT network, the scenario 
of a dynamic key exchange between the motes is 
done. Here, the dynamic security key is generated and 
authenticated for communication. In the IoT, for reasons 
of security, it is necessary to devise and implement the 
protocols and algorithms by which the overall privacy 
and security in communication can be enforced to avoid 
any intrusions. As the IoT can be used for military 
applications, it becomes mandatory to work on highly 
secured key exchange algorithms with the dynamic 
cryptography of security keys.
Once the simulation is complete, the network log files, 
which include the source and destination motes, the time, 
and the overall activities performed during simulation are 
analysed. In the Mote Output Window, the log data can be 
copied and further analysed using data mining and machine 
learning tools for predictive analytics. 
set, which include the name of the simulation, radio medium, 
startup delay and random seed.
Once the basic layout and working environment is ready, 
you need to import the RFID tags, sensor nodes or any 
other wireless devices that are to be connected and have to 
communicate over the IoT. In wireless networking and IoT, 
these are known as motes. There are many types of motes in 
Cooja that can be programmed.
Physical motes, too, can be connected by using ports on 
the system so that real-time interfacing can be done. Every 
mote, with the base properties and programming APIs, is 
specified in the C source code at the back-end of Cooja. These 
C source code files can be customised and recompiled to get 
the new or desired output from these motes.
After compiling C code, a number of virtual motes can be 



62 | May 2017 | OPEN SOURCE FOR yOU | www.OpenSourceForU.com
 
Perl code is very short and there is a whole set of one-
line code available, which shows the power of Perl.
 
It should be used, depending upon the use case. It can 
be used as a functional language, sometimes object-
oriented, procedural or imperative. How you want to use 
it depends on you. It handles almost every use case.
 
Perl’s CPAN provides a huge number of modules and 
tested code, and most of them are completely free.
 
Since Perl is a very old language, it has vast 
community support. 
 
Perl is portable and available for all platforms.
Disadvantages
 
Since Perl’s syntax is complex, sometimes new 
programmers find it difficult to understand it, because 
the same code can be written in many different ways. For 
example, print 11 + 12 + 13 != print (11 + 12) + 13
 
It has poor documentation for object-oriented paradigms 
compared to Python.
 
In Perl, there are many ways to do the same things. 
Sometimes, this creates a problem.
 
It is slow compared to other scripting languages.
Popularity
Though Perl has been around for a long time, its growth has 
been slow compared to some other languages and, yet, it has 
survived. Huge community support and built-in modules 
make it very popular.
Popular frameworks
Catalyst, Dancer, Mojolicious, etc, are some 
frameworks that use Perl.
Syntax
Shown below is the basic syntax for Perl. The extension for 
A Guide to Programming in 
Perl, PHP and Python
Perl, PHP and Python are three very popular and easy-to-learn programming 
languages, each with their own advantages and disadvantages. Newbies venturing 
into the field of programming will find this guide interesting and instructive.
T
o be a good programmer, mastering one programming 
language is not enough. In fact, it is very important to 
understand a customer’s requirements and choose the 
best available language for the task. It’s easy to shift from one 
programming language to another language if you know them 
very well. But there are so many programming languages in 
the world. Each has its own advantages and disadvantages.  
By leveraging all the advantages of different languages, a 
programmer can build a robust application. There are different 
factors which need to be considered, such as the functionality 
of the application, support for new features, the platform on 
which the application will run, performance, security, code 
size, community and support. 
Today, let’s examine the three Ps: Perl, PHP and Python. All 
these three languages are dynamic and powerful in their own 
way and can be used in different scenarios, but they are most 
commonly used to build Web applications and for scripting.
Perl 
Perl is a general-purpose, high-level, dynamic programming 
language. Currently, Perl 5 is the most popular version of it, 
while the latest version is Perl 6. Perl is derived from different 
languages like C, AWK, Sed and Shell Script.
Perl is most powerful in string manipulation and regular 
expressions. It is very useful while parsing huge amounts of 
data, line by line or based on specific regex. It is also useful 
for Web development, systems administration, network 
programming and game development.
The biggest advantage of Perl is that there are many ways 
to do the same thing. But, sometimes, this advantage creates 
problems in terms of resulting in non-structural or bad code style.
Advantages
 
Perl is very good at handling regular expressions. Hence, 
it is preferred when there is a lot of work on regex.
Developers Insight

www.OpenSourceForU.com | OPEN SOURCE FOR yOU | May 2017 | 63
Developers
Insight
Perl files is .pl or .pm, which is based on a file or module: 
#!/usr/bin/perl
$abc = 10;
print “Value of abc = $abc\n”;
print ‘program is over’;
PHP 
PHP is also a general-purpose language but is mainly used for 
Web development. It’s a server-side scripting language designed 
by Rasmus Lerdorf. It is very popular for small or medium level 
website development compared to other languages. It is also 
used by some giant firms like Facebook, YouTube, and Yahoo! 
Nowadays, there are a lot of frameworks built on PHP, which 
compete with Python and Perl. Though demand for Python is 
increasing, PHP is still very popular in both small and large 
organisations. The current version of PHP is 7.1.2.
Advantages
 
Most people prefer PHP when it comes to creating 
dynamic Web pages, because it’s easier to set-up in 
local host and most of the hosting providers offer built-
in PHP support.
 
It’s easy to use. The syntax is mostly the same as the C 
language. Anyone can easily adopt it.
 
There are lots of frameworks available for PHP and it has 
immense community support.
 
PHP is stable. Since it’s used by many developers, code 
maintenance is very quick.
 
PHP has strong database support and provides native 
session management.
 
PHP 7 is much faster than any other versions of PHP.
Disadvantages
 
Some of the libraries written in PHP are procedural, so 
it’s difficult for programmers from an object-oriented 
background.
 
Since it is open source, security is the major challenge as 
the code is available to all; hence, intruders can easily find 
weaknesses in the code.
 
It becomes slow after a certain scale. Because of this, 
large scale organisations use their own customised 
frameworks to increase performance. 
 
PHP is not suitable for desktop applications. Besides, its 
error handling capabilities are poor.
Popularity
PHP is still very popular and is one of the top-10 languages used 
by Web developers to create dynamic websites. Web developers 
should definitely learn PHP to kickstart their careers.
Popular frameworks
Laravel, Symfony, CodeIgniter, Phalcon, etc, are some of the 
popular frameworks.
Syntax
PHP scripting code is written using the <?php start tag and 
?> end tag. The file extension is .php, and usually contains 
PHP scripts and HTML code.
<?php // PHP code  ?>
Python 
Python is a general-purpose and high-level programming 
language designed by Guido van Rossum in the late 1980s. 
It’s very elegant and easy to learn. Nowadays, a beginner’s 
first priority is to learn Python. It supports multiple 
paradigms —functional, imperative, procedural and OO 
(object oriented). Python is an all-round language. Since 
it’s often a first choice, it can be found everywhere. It can 
be used in testing, automation, Web development, data 
analytics, cloud computing, mobile development and game 
development. Python uses indentation to identify the block, 
and this eliminates the need for curly brackets and the colon.  
Python’s philosophy is different, which is, “There is only 
one way to do the things and it’s called the Pythonic way.” 
The current version of Python is 3.6.0.
Advantages
 
One of the best things about Python is that it is well 
documented.
 
Python is used in 3D animation and game development, 
which differentiates it from other languages.
 
In school, students learn Python quicker than C or C++ 
because it is easy.
 
Python uses white space indentation, while others use 
curly braces. Comparatively, Python code is neater 
and cleaner.
 
Code written in Python is very short compared to other 
languages, which demonstrates its power.
 
Python also supports JVM so code written in Python can 
easily work with some of the Java objects/APIs. 
 
If you are new to programming, learn Python because 
by learning this one language, you can easily jump 
into any field of interest, whether it’s automation, Web 
development, game development or data analytics. 
Python is everywhere.
Disadvantages
 
Indentation is the biggest issue in Python. The code will 
not work and sometimes people get irritated because of it.
 
Python forces programmers to follow a particular 
convention.
 
It’s slower compared to older languages like C and C++. 
It is also an interpreted language. 
 
It doesn’t perform multi-processor/multi-core work 
efficiently.

64 | May 2017 | OPEN SOURCE FOR yOU | www.OpenSourceForU.com
Developers Insight
By: Maulik Parekh
The author has an M. Tech degree in cloud computing from VIT 
University, Chennai. He can be reached at maulikparekh2@gmail.
com. Website: https://www.linkedin.com/in/maulikparekh2. 
you can do wonders in all those segments. So I prefer Python.
The most important thing is to learn the Pythonic way of 
writing a code. Here are a few examples.
Example 1: The ‘non-Pythonic’ way of writing a code:
def test(a, b):
    a[0] = 2
    b[0] = 7.5
abc = [0]
xyz = [0]
test(abc, xyz)
abc = abc[0]
xyz = xyz[0]
The Pythonic way of writing the same code:
def test():
    return 2, 7.5
alpha, beta = foo()
Example 2: The non-Pythonic way of writing code:
i = 0
while i < mylistleng:
   do(mylist[i])
   i += 1
The Pythonic way of writing the same code:
for i in range(mylistleng):
   do(mylist[i])
Another Pythonic way of writing the same code:
for item in mylist:
   do(item)
Example 3: A short program for reversing a string is 
shown below:
'Hello Python'[::-1]                              
Popularity
Python has been bestowed the TIOBE Programming 
Language of the Year award in 2007 and 2010. The TIOBE 
index measures the growth and popularity of the language 
over the period of one year. 
Popular frameworks
Django, TurboGears, Web2py, Bottle, CherryPy, Flask, 
Pyramid, etc, are some of the popular frameworks.
Syntax
Python syntax is very neat and clean. The extension 
of the file is .py. 
#!/usr/bin/python
print "Hello! Welcome to the world of Python!"
So which is the best programming language 
to learn and use?
There is no specific answer to this question because we can 
perform almost all tasks with any of the three languages. 
It depends on what you want to do and how you can do it 
efficiently. The answer to this question varies based on the 
applications or usage.
If Web development is the first priority, then PHP is the 
first preference. For test automation and scripting, Python and 
Perl are very popular. There are lots of frameworks available 
in all three languages, and since community support is good, 
you will never be stuck with a particular problem. 
PHP has lots of popular CMS frameworks available like 
WordPress, Joomla and Drupal. Any newbie can deploy 
websites easily using these frameworks. There are lots of 
plugins available for these frameworks and almost all the 
major functionalities are covered. Similarly, Python has the 
Django framework, which is very popular and most Python 
lovers use it for Web development.
My advice would be to learn all three languages and use 
them based on the situation. Beginners can start with Python 
as it is very easy, and later jump into learning PHP and 
Perl, in that order. The basic logic for all the programming 
languages is the same and the only difference is in terms of 
the syntax. Usually, the person who knows one language can 
easily migrate to another language very quickly.
These are never ending discussions on all the forums 
about which is the best programming language for career 
growth. My advice would be to learn one language very well. 
Pick the one you feel comfortable with and slowly venture 
into exploring the other two. 
My preference
I will go with Python as it is evolving very fast and is easy to 
learn. The use cases of Python cover a very broad range. It is 
used in almost every IT segment and, by learning this language, 
[1] https://en.wikipedia.org/wiki/Perl
[2] https://wiki.python.org/moin/WebFrameworks 
[3] https://www.python.org/
[4] https://en.wikipedia.org/wiki/PHP
[5] http://php.net/manual/en/intro-whatis.php
References

www.OpenSourceForU.com | OPEN SOURCE FOR yOU | May 2017 | 65
Developers
Insight
D
art is a programming language developed at Google. 
The primary developers behind it are Lark Bak and 
Kasper Lund. Dart was first released in October 2011, 
and the latest stable release is version 1.22, which came out 
in February 2017. The consistent updates to Dart indicate its 
active development.
Dart is cross-platform and supports all major operating 
systems. It is open source and available with a BSD licence. It 
is recognised as an ECMA standard.  
Objectives of Dart
According to the official documentation, Dart is a long-
term project with ambitious objectives. The core objectives, 
illustrated in Figure 1, are:
 
Dart has a strong support base with many libraries and 
tools, which enable very large applications. 
 
 One of the major objectives of Dart is to simplify 
programming tasks. It is designed to make common 
programming tasks simpler. 
 
 Dart is very stable and it can be used to build production 
quality real-time applications. It is an object-oriented 
programming language with support for inheritance, 
interfaces and optional typing features. 
Dart targets
The Dart code can be executed in four different 
ways (see Figure 2): 
 
The code can be trans-compiled into JavaScript using 
source-to-source compilation. This is done with the 
help of the dart2js compiler. As it can be converted to 
JavaScript, all major browsers support it. 
 
 The Dart SDK includes a virtual machine called Dart VM. 
This mode is for standalone execution. The Dart code can 
be executed in command line interface mode. The Dart 
SDK includes a powerful package manager called ‘pub’. 
 
The Dart code can be executed in another mode through 
the Dartium browser. This is a customised Chromium 
Web browser that includes the Dart VM. As this browser 
has direct support for Dart code, there is no need to 
convert it into JavaScript. 
 
 Dart can also be executed in AOT mode. AOT stands for 
Ahead-Of-Time compilation. In this mode, the Dart code 
can be directly converted into native machine code.
DartPad
If you prefer to start writing your first Dart program without 
any installation or configuration, then DartPad is the right 
Dart: An Easy, Scalable  
and Multi-purpose  
Programming Language 
If you are looking for an application programming language that is easy to 
learn, highly scalable and deployable everywhere, then do try Dart. This article 
introduces readers to its exciting array of features.

66 | May 2017 | OPEN SOURCE FOR yOU | www.OpenSourceForU.com
Developers Insight
choice for you.  It can be downloaded from https://dartpad.
dartlang,org.  The DartPad interface is plain and simple. 
Write your code and click ‘Run’ to execute the code. The 
output is also shown within the DartPad window itself, as 
shown in Figure 3. 
DartPad introduces users to the world of Dart 
programming.  The support for libraries in DartPad is 
restricted to the basic level. To include various libraries, users 
can have a local installation of the Dart SDK. 
A simple Dart program
Shown below is a simple Dart program with a user 
defined function: 
// Define a function.
printOSFYIssueNumber(num aNumber) {
  print('Welcome to the OSFY issue Number $aNumber.'); // 
Print to console.
}
// This is where the app starts executing.
main() {
  var issuenumber = 12; // Declare and initialize a variable.
  printOSFYIssueNumber(issuenumber); // Call a function.
}
main() is the function that the application execution 
starts from. This sample program initialises a variable and 
calls a user defined function printOSFYIssueNumber(). The 
function takes one numeric argument. In the function, the 
type of the argument is ‘num’. In Dart, ‘num’ denotes the 
‘number’ type. There are two sub-types to ‘num’. These are 
‘int’ and ‘double’. 
As you might have observed, the function definition doesn’t 
require any specific keyword. It merely involves the use of the 
function name and the optional arguments. The object-oriented 
attribute has been given emphasis in the design of the Dart 
programming language. The functions are objects in Dart. They 
belong to a type called ‘Function’. What this implies is that the 
functions can also be assigned to other variables, and they can 
be supplied as arguments to other functions. 
The comments are prefixed with //. The ‘print()’ function 
is used to output the value to the user. 
The Dart programming language supports various data 
types, as listed below: 
 
Numbers
 
 Strings: These are sequences of UTF-16 code units. 
Strings can be provided with either single quotes or 
double quotes. 
 
 Booleans: These are provided with the ‘bool’ keyword.
 
 Lists: These are used to specify arrays, for example,  
var list = [100,200,300]
 
 Maps: These are used to associate keys with values. For 
example: 
 
var OSFYThemes = {
 
 // Keys      Values   
 
'Jan' : 'Mobile Computing',   
 
'Feb': 'Networking',
 
'Mar' : 'Programming Lang' 
 
};
 
Runes: In Dart, runes are UTF-32 code points of a 
string. As stated earlier, Dart strings are UTF -16.  For 
expressing the 32-bit Unicode values, a specific syntax is 
adopted. An example of how ‘runes’are used follows:
main() {
  var clapping = '\u{1f44f}';
  print(clapping);
  print(clapping.codeUnits);
  print(clapping.runes.toList());
}
 
Symbols: The ‘Symbol’ object in Dart is used to represent 
Figure 2: Dart execution modes
Dart execution 
modes
Trans-Compile in 
to JavaScript
Dartium Browser
AOT Compilation
Stand-alone via
DartVM
Figure 1: Dart’s objectives
Figure 3: DartPad – execute your Dart programs

www.OpenSourceForU.com | OPEN SOURCE FOR yOU | May 2017 | 67
Developers
Insight
is provided at https://webdev.dartlang.org/guides/get-started. 
Dart mobile apps
Dart mobile apps are built using Flutter, which is a SDK for 
building mobile apps. For iOS and Android, the apps can 
be built with a single codebase. Apps built using Flutter are 
high-performance and high-fidelity. The major advantages of 
building mobile apps with Flutter are as follows: 
 
iOS and Android apps can be built using the same 
codebase. 
 
Prototyping is comparatively simpler. 
 
The apps are built with highly customised user 
experiences. The material design widgets for the Flutter 
framework are handy. The custom designs can also be 
implemented using OEM widget sets. 
Flutter includes the following components: 
 
A functional reactive framework
 
A 2D rendering engine
 
Ready-to-use widgets
 
Development tools
In the application development model of Flutter, 
everything is a widget. Flutter has a unified object 
model in which all components are widgets. In Flutter 
terminology, a widget can be used to define all of the 
following: structural elements, style elements, layout, and 
even the business logic. 
Fundamental Dart libraries
The three most fundamental Dart libraries are listed below:
 
dart:core - All basic functionalities of Dart such as 
strings, collections, dates and URIs are processed 
using this library. 
 
dart:html - This library is the wrapper for DOM 
manipulation which is much used with Web apps. 
 
dart:io - This library is used with command line apps. 
The libraries in Dart are imported with the help of 
‘import’, as shown below: 
import 'dart:html'; 
import 'dart:math';
Apart from the above-mentioned core libraries, there are 
various other libraries. Some of them are  listed below: 
 
dart:async - This is a library that supports asynchronous 
programming. 
 
dart:developer - This library is used for interacting with 
developer tools such as the debugger and inspector.
 
dart:js - This library provides support for interoperating 
with JavaScript. 
 
dart:svg – This is used to handle support vector graphics.
 
dart:web_audio - This is used to perform audio 
programming in the browser. 
 
dart:web_gl – This is to perform 3D programming 
in the browser. 
an operator or identifier used in a Dart program. 
Another interesting feature of Dart is ‘Method 
Cascading’. To perform this, the operator used is ‘..’. It is the 
double dot operator. 
An overview of the Dart programming constructs is 
provided at https://www.dartlang.org/guides/language/
language-tour.
As stated in the beginning of this article, Dart is a 
general-purpose programming language. It is used to build 
applications belonging to various types. The three major types 
of apps that can be built with it are listed below: 
 
Dart Web apps
 
Dart Mobile apps
 
Dart Server (or command line) apps 
Dart Web apps
As per the official documentation (https://webdev.dartlang.
org/), Google uses Dart for building critical Web apps. It is 
utilised with AngularDart. Google’s internal CRM is also built 
using Dart. Apart from the search giant, many other reputed 
companies also use Dart for their critical Web apps. 
In the case of Dart Web apps, the contents of the HTML 
elements can be manipulated dynamically from Dart code. A 
simple ‘countdown’ timer code follows. 
In HTML, type: 
<h1 id="countdown"></h1>
In Dart, type:
import 'dart:html';
main() async {
  var countdown = querySelector("#countdown");
  for (int i = 100; i >= 0; i--) {
    countdown.text = "Time: $i";
    await window.animationFrame;
  }
}
Place this code in DartPad and click Run. In the output 
window (HTML), you will notice the countdown from 100 to 0. 
The HTML element ‘<h1>’ is accessed with the querySelector() 
function. Here, the ‘id’ is supplied as input. Then the contents are 
dynamically modified by using the ‘text’ attribute. 
The dart:html library is utilised in the above program. It 
is a wrapper around the DOM (Document Object Model) and 
Window API. Due to the availability of this wrapper library, 
the developer need not worry much about browser support. 
Important advantages of building Web apps with Dart are: 
 
The Dart SDK is of production quality. It is stable 
and maintained. 
 
With Angular, Dart becomes a powerful combination.
Detailed information regarding building Web apps with Dart 

68 | May 2017 | OPEN SOURCE FOR yOU | www.OpenSourceForU.com
Developers Insight
By: Dr K.S. Kuppusamy 
The author is an assistant professor of computer science at 
the Pondicherry Central University, with more than 11 years of 
teaching and research experience in academia and industry. 
He can be reached via mail at kskuppu@gmail.com. 
[1] DartPad: https://dartpad.dartlang.org/
[2]  Dart Language basics: https://www.dartlang.org/guides/
language/language-tour
[3]  Dart SDK: https://api.dartlang.org/stable/1.22.1/index.html
[4]  Dart Package Manager Pub: https://www.dartlang.org/
tools/pub/get-started
References
 
dart:web_sql - This provides an API for storing the data, 
which can be manipulated with SQL. 
Dart ‘pub’
The package manager of Dart is called ‘pub’.  The 
Dart applications specify the ‘pubspec’, which lists the 
dependencies for that application. For example: 
name: my_app 
dependencies:   
js: ^0.3.0   
intl: ^0.12.4
After the specification of ‘pubspec’, navigate to that 
directory and execute ‘pub get’ as shown below: 
cd <path-to-app>
pub get
The dependencies can be upgraded later with the 
following command: 
$ pub upgrade
The Dart tools
In this article, the code written in Dart is executed using 
DartPad. However, as stated earlier, for advanced tasks, 
it would be better to have an IDE installed in the local 
machine. The official documentation has recommended the 
WebStorm IDE. 
In the case of Web app development, the Dartium browser 
can be used. The Dartium browser download is available in 
the link: https://webdev.dartlang.org/tools/dartium.
Apart from the Dartium browser, the other command line 
tools such as pub build (for building a Web app) and pub 
serve (for serving a Web app) can also be used. 
There are a few other important Dart tools such as: 
 
Dartanalyzer - This tool is used to evaluate and report 
potential errors in your Dart code. 
 
Dartfmt - This tool is for formatting the Dart code. 
The official Dart style guide is followed while 
formatting the code. 
To summarise, Dart is an interesting programming 
language with features to facilitate Web, mobile and 
command line apps. Its major advantages are its stability 
and support base. The SDK is constantly updated and, 
at the same time, is very stable. In the case of Web 
applications, the combination with AngularDart makes it 
a very powerful tool. The mobile application development 
platform, Flutter, enables the building of apps for both 
Android and iOS from the same codebase. Overall, Dart is 
a programming language that is easy to learn, and it enables 
the developer to build various types of apps. 
www.electronicsb2b.com
Log on to www.electronicsb2b.com and be in touch with the Electronics B2B Fraternity 24x7
Read more stories on Components in 
• The latest in power converters
• India’s leading component distributors
• Growth of Indian electronics components industry
• The latest launches of components for LEDs
• The latest launches of components for electronics
TOPCOMPONENTS STORIES
ELECTRONICS 
INDUSTRY IS AT A 

www.OpenSourceForU.com | OPEN SOURCE FOR yOU | May 2017 | 69
of hardware with the same user interface; it is scalable, 
supporting very large programs; and it has user interfaces 
for UNIX, Windows and Mac.  It also has efficient high-
level data structures and extensive standards-free libraries, 
which make programming easy; hence, it is considered the 
language for beginners.
In this article, we will learn how to install, set up and use 
Python. The cool thing about learning Python is that there are 
no prerequisites. Even if you are a novice to programming 
you can become a pro coder within a month. To start with, 
download and install Python from https://www.python.org/
downloads/release/python-2712/. For this article, we are using 
Python 2.7. The above link also contains Python for Windows 
and Mac. On most Linux boxes, Python is pre-installed. 
For Windows, download Python and run the .msi file. Upon 
completion, add the environment variable named 'PYTHON' 
and its value as the directory where Python is (in this case,'C:\
Python27'), as shown in Figure 1. We can verify whether 
Python has been installed correctly by running the python - 
version command in the command line. This will give you 
the current version of Python running on your computer, as 
shown in Figure 2. To create Python files, use any text editor 
of your choice. For this article, we will be using the Sublime 
Python: The User 
Friendly Language 
for Coding
This article will delight newbies who want to get a foothold in the 
programming world. It introduces readers to the fascinating world of 
Python, which is a very versatile programming language. 
C
omputers, today, come with immense disk space, 
multi-core processors and high speed Internet 
connectivity. And they use computer languages that 
interact with the operating system and execute processes at 
a high speed. One such language is Python, which is used as 
a high level programming language. It was first released in 
1991 by Guido van Rossum, an ex-employee of Google. It 
is quite easy to learn, as it uses English keywords frequently, 
unlike the heavy syntaxes of other languages. Moreover, 
programmers can code the logic using fewer lines of code 
than what is required in other programming languages like 
C or Java. The unique design, notably, using white space 
indentation to delimit code blocks, makes Python more 
readable and clean. 
Python is an interpreted, interactive and object-oriented 
scripting language. The Python code is processed by the 
processor at run time, i.e., it does not need to be compiled 
as in Java, which is compiled to get the .class file. The term 
interactive implies that you can use the Python prompt to 
write the code, which interacts with the interpreter, and 
by encapsulating the code within objects, it follows the 
OOPS principle. The other features that make Python cool 
are that it is extendable, i.e., it can run on a wide range 
Developers
Insight

70 | May 2017 | OPEN SOURCE FOR yOU | www.OpenSourceForU.com
Developers Insight
text editor, which can be downloaded from https://www.
sublimetext.com/3.
The cliché of programming languages is ‘Hello World’; so 
let’s get started by printing it in the console. Open the command 
prompt, type Python and execute it. Now run the command:
 print (“Hello world”). 
This will print ‘Hello World’ in the console as shown 
in Figure 3. Python, being a scripting language, can also be 
executed by writing a group of individual statements, which 
make a single code block called a ‘suite’ in a file, and can be 
saved with the .py extension.
Now let’s learn about the building blocks of Python, like 
the identifiers, keywords, user input processes, comments, etc.  
Identifiers are the names given to classes, variables, functions, 
etc. These are case sensitive and start with the letters A-Z, 
a-z or an underscore. There are some naming conventions in 
Python, which are listed below:
 
An identifier starting with an underscore means that it’s 
private, and a double underscore means it is strongly private. 
 
The class name must start with upper case. Python 
has some reserved words called keywords which have 
special meanings. Examples of keywords are and, finally, 
Class, def, if, else, elif, etc. Unlike Java or C, which use 
braces to wrap lines of code, Python uses indentation 
to denote the block of code that is tightly mapped. In 
Python, statements usually end with a new line, but if the 
statement gets carried to the new line, then one uses the 
‘\’ backslash to show the continuation of the line. A single 
line comment is denoted by ‘#’ and a multi-line comment 
can be wrapped in triple quotes.
The data types used in Python are numbers, strings, lists, 
tuples and the dictionary. The number data type stores the 
numeric value and is immutable in nature, i.e., upon changing 
the value, a new object is created and the older one is kept 
for automatic garbage collection. For example, variable = 
10. The single quote or double quotes are used to denote the 
string. For example,  varA = ‘Hello World’, varB = “Python 
is cool”. Python does not support character types, but treats 
the characters as strings of length one. Coming to the list, it 
consists of comma-separated values which could be of any 
type and it is represented as [,,,,]. All values are enclosed 
between '[' and ']'. A list object is a mutable data type, which 
means it can't be hashed, and modifying an existing list 
will not result in a new list object—the memory address 
will not be changed either. For example: def_list = [1, 
2,"1","100","Python","Anne"].
 The next is a tuple, which has comma-separated 
values that could be of any type, enclosed between '(' 
and ')'. This, too, is an immutable data type and can be 
used as the key in a dictionary. For example: def_tuple = 
(1,2,"1","100","Python","Anne").  The dictionary in Python 
contains the key value pair in which the key is immutable, 
and the value can be anything; the key value is separated by 
a colon ‘:’. Dictionaries work on the hashing principle. For 
example: dictB = dict(). Let us now create a new file in the 
Sublime text editor with the following commands: 
# An Empty Dict
dictA = dict() # same as dictA = {}
dictB = dict() # same as dictB = {}
# 
Adding values to it 
for i in [1,3,2,4,8,9,5,6,7]:
 
dictB[i] = i
# 
Adding values to a dict
# Let us use a simple for loop to add values to a dict
for i in xrange(10):
 
dictA[i] = i + 10
print dictA,'\n\n',dictB,'\n\n'
'''
 
Define a simple list with multiple datatypes
'''
def_list = [1,2,"1","100","Python","Anne","A!@345<>_()",True,
False,{1:100,2:200,3:300},range(10)]
Figure 1: Setting the environment variable
Figure 2: Checking the correct installation of Python and its version
Figure 3: Getting started with printing ‘Hello World’


72 | May 2017 | OPEN SOURCE FOR yOU | www.OpenSourceForU.com
Developers Insight
By: Ashish Kumar Sinha
The author is a software engineer based in Bengaluru. A software 
enthusiast at heart, he is passionate about using open source 
technology and sharing it with the world. He can be reached at 
ashi.sinha.87@gmail.com. Twitter handle: @sinha_tweet. 
There are a few rules for writing the function, which are:
 a)  It begins with the ‘def’ keyword, followed by the name 
and parentheses, which can be used to pass the input 
parameters. 
b) A code block within every function starts with a colon 
(:) and is indented. A simple example of a function is 
shown below:
#A Function to print the sum of numbers
def funcA(a,b,c):
 
#Add a,b,c and return the result
 
sum_numbers = a + b + c 
 
return sum_numbers
This can be called by executing print funcA(100,200,300), 
which prints the result 600 in the console.
There are many other useful features of Python like 
modules, inheritance, multi-processors, exceptions, file input 
and output, all of which make Python a high level and easy 
programming language. Python also has classes and objects, 
multi-threading, database access, networking, emails and 
many more features, which can be learned while exploring 
the advanced features. 
#Now create a variable 
vara = def_list
 
#Modification of vara will result in modifying def_list
vara.append("Hero")
print "Address of vara and def_list %s and %s 
"%(id(vara),id(def_list)),'\n\n'
print "vara = %s "%(vara),'\n\n'
print “def_list = %s “%(def_list),’\n\n’
# 
Define a simple tuple with multiple datatypes
def_tuple = 1,2,”1”,”100”,”Python”,”Anne”,”A!@345<>_()”,True,
False,{1:100,2:200,3:300},range(10)
# Print and view the tuple
print “Printing the tuple  - “ , def_tuple, ‘\n\n’
print “Printing the tuple’s address - “ , id(def_tuple),’\
n\n’# The address is subject to change
 
Once this is done, save the file with the .py extension. In this 
case, it is saved as PythonPractice.py. In the command prompt, 
browse to the location where the file is saved and execute the 
command Python PythonPractice.py (shown in Figure 4); this 
displays the result on the console (as shown in Figure 5). 
Python is famous for its functional programming.  A 
function is a block of organised, reusable code that is used to 
perform a task. It provides better modularity because of the 
ability to be reused. 
Figure 4: A Python file to test the dictionary, list and tuples
Figure 5: Outcome of executing the PythonPractice.py file
Your favourite Magazine on Open 
Source is now on the Web, too.
OpenSourceForU.com
Follow us on Twitter@LinuxForYou

www.OpenSourceForU.com | OPEN SOURCE FOR yOU | May 2017 | 73
Developers
Overview
H
TML5 has evolved through the cooperation between 
the W3C and the Web Hypertext Application 
Technology Working Group. It is a higher version of 
HTML, and its many new elements make your pages more 
semantic and dynamic. It was developed to provide a greater 
Web experience for everyone. HTML5 offers great features 
that make the Web more dynamic and interactive.
The new features of HTML5 are:
 
New sets of tags such as <header> and <section>
 
<canvas> element for 2D drawing
 
Local storage
 
New form controls like calendar, date and time
 
New media functionality
 
Geo-location
HTML5 is not an official standard as yet; hence, not all 
browsers support it or some of its features. One of the most 
important reasons behind developing HTML5 was to prevent 
users from having to download and install multiple plugins 
like Silverlight and Flash.
New tags and elements
Semantic elements: Figure 1 displays a few useful 
semantic elements.
Form elements: The form elements present in HTML5 
are shown in Figure 2.
Graphic elements: The graphic elements in HTML5 can 
be seen in Figure 3.
Media elements: The new media elements in HTML5 are 
listed in Figure 4.
Advanced features of HTML5
Geo-location
It is an HTML5 API that is used to get the geographical 
location of a website’s user, who has to first permit the site 
to fetch his or her location. This usually happens via a button 
An Introduction to HTML5
HTML5, the fifth and current version of the HTML standard, is a markup 
language used to structure and present content on the World Wide Web. 
This article will help readers get acquainted with it.

74 | May 2017 | OPEN SOURCE FOR yOU | www.OpenSourceForU.com
Developers Overview
 
Job postings can automatically include commute times
How it works: Geo-location works by scanning common 
sources of location information, which include the following:
 
Global Positioning System (GPS), which is the most 
accurate
 
Network signals—IP address, RFID, Wi-Fi and Bluetooth 
MAC addresses
 
GSM/CDMA cell IDs
 
User inputs
The API offers a very handy function to detect geo-
location support in browsers:
if (navigator.geolocation) {
  
 // do stuff
}
The getCurrentPosition API is the main method for using 
geo-location. It retrieves the current geographic location 
of the user’s device. The location is described as a set of 
geographic coordinates along with the heading and speed. The 
location information is returned as a position object.
The syntax is:
getCurrentPosition(showLocation, ErrorHandler, options);
 
showLocation: This defines the callback method that 
retrieves location information. 
 
 ErrorHandler(Optional): This defines the callback method 
that is invoked when an error occurs in processing the 
asynchronous call. 
 
 options (Optional): This defines a set of options for 
retrieving the location information.
Figure 5 incorporates the set of properties returned by a 
position object.
We can present location information to the user in two 
ways—geodetic and civil: 
1. The geodetic way of describing a position refers directly 
to the latitude and longitude. 
2. The civic representation of location data is readable and 
easily understood by humans.
Figure 1: Semantic elements
Figure 2: Form elements
Figure 4: Media elements
Figure 5: Position object properties
Figure 3: Graphic elements
and/or browser popup. All the latest versions of Chrome, 
Firefox, IE, Safari and Opera can use the geo-location 
feature of HTML5.
Some uses of geo-location are:
 
Public transportation websites 
 
Taxi and other transportation websites
 
To calculate shipping costs on an e-commerce site
 
Travel agency websites
 
Real estate websites
 
Movie theatre websites can find movies playing nearby
 
Online gaming
 
For sites to feature local headlines and weather on 
their front page

www.OpenSourceForU.com | OPEN SOURCE FOR yOU | May 2017 | 75
Developers
Overview
As shown in Table 1, each attribute/parameter has both 
a geodetic representation and a civic representation.
Table 1
Attribute
Geodetic 
Civic
Position
59.3, 18.6 
Stockholm
Elevation
8 metres 
Third  floor
Heading
142 degrees
To the city 
Speed 
      
12 km/h
Running
Orientation
55 degrees
North-West
Web storage
In HTML, to store user data on a local machine, we were 
using JavaScript cookies. To avoid that, HTML5 has 
introduced Web storage, with which websites themselves 
store user data on a local machine.
The advantages of Web storage, as compared to 
cookies, are:
 
More secure
 
Faster
 
Stores a larger amount of data
 
The stored data is not sent with every server request. 
It is only included when asked for. This is a big 
advantage of HTML5 Web storage over cookies.
There are two types of Web storage objects:
1) Local – this stores data with no expiration date.
2) Session – this stores data for one session only.
How it works: The localStorage and sessionStorage 
objects create a key = value pair.
An example is: key=“Name”, value=“Palak”
These are stored as strings but can be converted, if 
required, by using JavaScript functions like parseInt() and 
parseFloat().
Given below is the syntax for using Web 
storage objects:
Storing a Value:
• 
localStorage.setItem(“key1”, “value1”);             
• 
localStorage[“key1”] = “value1”;                          
Getting a Value:
• 
alert(localStorage.getItem(“key1”));  
• 
alert(localStorage[“key1”]);  
Remove a Value:
• 
removeItem(“key1”);  
Remove All Values:
• 
localStorage.clear();  
Application Cache (AppCache)
Using HTML5 Apache, we can make a Web application 
work offline without an Internet connection. All browsers, 
besides IE, can use AppCache (at this point in time).
The advantages of Application Cache are:
 
Enables browsing Web pages offline
 
Pages load faster
 
Results in less load for servers
The cache manifest file is a simple text file that lists the 
resources the browser should cache for offline access. The 
manifest attribute can be included on the document’s HTML 
tag, as follows:
<html manifest=”test.appcache”> 
 
... 
</html>
It should be on all the pages that you want to cache. 
The application pages that are cached will remain unless:
1. 
The user clears them out.
2. 
The manifest has been modified.
3. 
The cache is updated.
Video
Until HTML5 was launched, there was no uniform standard 
for showing video on Web pages. Most of the videos were 
shown through different plugins like Flash. But HTML5 
specifies a standard way to show the video on a Web page by 
using a video element.
Currently, it supports three video formats for the video 
element, as shown in Table 2.
Table 2
Format 
IE
Firefox
Opera
Chrome Safari
Ogg 
No
3.5+ 
 10.5+
5.0+
No
MPEG4 
No
No 
 No
5.0+
3.0+
WebM 
No
No 
 10.6+
 6.0+
No
The example given below shows the use of this video 
element: 
<! DOCTYPE HTML>
<html>
<body>
<video src=” vdeo.ogg” width=”320” height=”240” 
controls=”controls”>
This browser does not support the video element.
</video>
</body>
</html>
This example uses an Ogg file, and will work in 
Firefox, Opera and Chrome. To make the video work in 
Safari and future versions of Chrome, we must add an 
MPEG4 and WebM file.

76 | May 2017 | OPEN SOURCE FOR yOU | www.OpenSourceForU.com
Developers Overview
The video element allows multiple source elements. 
Source elements can link to different video files. The browser 
will use the first recognised format, as follows:
<video width=”320” height=”240” controls=”controls”>
  <source src=”vdeo.ogg” type=”video/ogg” />
  <source src=” vdeo.mp4” type=”video/mp4” />
  <source src=” vdeo.webm” type=”video/webm” />
This browser does not support the video element.
</video>
Audio
With audio, the case is similar to video. Until HTML5 was 
launched, there wasn’t any uniform standard for playing 
audio on Web pages. Most of the audio was also played 
through different plugins like Flash. But HTML5 specifies a 
standard way to play audio on a Web page by using an audio 
element. The audio element is used to play sound files and to 
stream audio.
Currently, HTML5 supports three audio formats for the 
audio element, as shown in Table 3.
Table 3
Format  IE 8
Firefox 
3.5
Opera  
10.5
Chrome 
3.0
Safari 
3.0
Ogg  
Vorbis
No
Yes 
   Yes 
Yes 
         No
MP3 
 No
No 
    No 
Yes 
         Yes
Wav 
 No
Yes 
   Yes 
 No 
         Yes
              
The use of the audio element is demonstrated below:
<! DOCTYPE HTML>
<html>
<body>
<audio src=” song.ogg” controls=”controls”>
This browser does not support the audio element.
</video>
</body>
</html>
This example uses an Ogg file and will work in Firefox, 
Opera and Chrome. To make the audio work in Safari and 
future versions of Chrome, we must add an MP3 and Wav file.
The audio element allows multiple source elements, 
which can link to different audio files. The browser will use 
the first recognised format, as follows:
<audio controls=”controls”>
  <source src=”song.ogg” type=”audio/ogg” />
  <source src=”song.mp3” type=”audio/mpeg” />
This browser does not support the audio element.
</audio>
Canvas
To create graphics on a Web page, HTML5 uses the Canvas 
API. We can draw anything with it and it uses JavaScript. 
This can improve the performance of our website by avoiding 
the need to download images off the network. With Canvas, 
we can draw shapes and lines, arcs and text, gradients and 
patterns. In addition, Canvas gives us the power to manipulate 
pixels in the images and even in video. You can add the 
Canvas element to an HTML page as shown below: 
<canvas id=”myCanvas” width=”200” height=”100”></canvas> 
The Canvas element doesn’t have capabilities to draw 
elements. We can achieve this by using JavaScript. All 
drawings should be inside JavaScript.
<script type=”text/javascript”>
var c=document.getElementById(“myCanvas”);
var cxt=c.getContext(“2d”);
cxt.fillStyle=”blue”;
cxt.storkeStyle = “red”;
cxt.fillRect(10,10,100,100);
cxt.storkeRect(10,10,100,100);
</script>
The output of the above script is seen in Figure 6.
Figure 6: Output of Canvas
 
You can draw many objects like arcs, circles, linear/
vertical gradients, etc.
HTML5 tools
To operate effectively, all skilled or amateur Web developers/

www.OpenSourceForU.com | OPEN SOURCE FOR yOU | May 2017 | 77
Developers
Overview
designers should use HTML5 tools, which are a great help 
when one needs to set up workflows/websites or perform 
repetitive tasks.  They improve the usability of Web 
designs. Featured below are some essential tools which 
help in creating amazing websites.
HTML5 Maker: This is used to interact with content in 
websites with the help of HTML, JavaScript and CSS. It is 
very easy to use. It also allows us to develop slide shows, 
sliders, HTML5 animation and more.
Liveweave: This is used to test the code. It cuts down 
on the time spent on saving the code and loading it on 
screen. Pasting the code in the editor gives instant results. 
It is very easy to use and also provides the auto-complete 
feature for some of the code, which makes development 
and testing faster and easier. 
Font dragr: This previews the custom Web fonts in 
a browser. It loads the font instantly, so you get to know 
whether it looks good or not. It also offers a drag-and-drop 
interface, allowing you to drop your typefaces, Web open 
fonts and vector graphics to test all of them immediately.
HTML5 Please: This allows us to find anything 
related to HTML5. If you want to know how to use any 
feature, you can search for it on HTML Please. It provides 
lists of useful resources about supporting browsers and 
devices, syntax, general recommendations of how to use 
the element, etc.
Modernizr: This is an open source tool which is used 
to provide the best experience possible, given the visitor’s 
browser. With this tool, you can detect whether the 
visitor’s browser supports HTML5 features, and load the 
scripts accordingly. 
By: Palak Shah
The author is a senior software engineer. She loves to explore 
new technologies, learn innovative concepts, and is also fond 
of philosophy. She can be reached at palak311@gmail.com.
[1] https://www.w3schools.com/html/html5_intro.asp
[2]  http://www.html-5-tutorial.com/
[3]  https://www.tutorialspoint.com/html5/
[4]  https://www.udemy.com/learn-html5-programming-
from-scratch/learn/v4/overview
References
Adobe Edge Animate: This is a useful tool for HTML5 
developers who have to deal with interactive HTML 
animations. It is used in digital publishing, the Web and in 
the advertising fields. This tool allows users to create flawless 
animations, which run across multiple devices.
Video.js: This is a HTML5 video player based on JavaScript. 
If you want to add videos to your website, you should use this 
tool. It makes videos look good and a part of the website. 
The W3 Validator: The W3 Validator tests the validity of 
the markup of websites in HTML, XHTML, SMIL, MathML, 
etc. To test the markup validity of any website, you have to 
choose the document type as HTML5 and enter your Web 
page’s URL. By doing this, your code will get checked, and 
all errors and warnings will be mentioned.
HTML5 Reset: This tool allows developers to rewrite the 
code of their old websites in HTML5.
You can use these tools to provide a great Web experience 
for visitors to your website. 
Month
theMe
March 2017
Open Source Firewall, Network security and Monitoring
April 2017
Databases management and Optimisation
May 2017
Open Source Programming (Languages and tools)
June 2017
Open Source and IoT
July 2017
Mobile App Development and Optimisation
August 2017
Docker and Containers
September 2017
Web and desktop app Development
October 2017
Artificial Intelligence, Deep learning and Machine Learning
November 2017
Open Source on Windows
December 2017
BigData, Hadoop, PaaS, SaaS, Iaas and Cloud
January 2018
Data Security, Storage and Backup
February 2018
Best in the world of Open Source (Tools and Services)
oSFY Magazine Attractions During 2017-18

78 | May 2017 | OPEN SOURCE FOR yOU | www.OpenSourceForU.com
Developers Overview
R
aspberry Pi, a small development board 
minicomputer that runs the Linux operating 
system, was developed in the United Kingdom 
by the Raspberry Pi Foundation to promote the teaching 
of basic computer science in schools in the UK and in 
developing countries. Raspberry Pi has USB sockets, 
which support various peripheral plug-and-play devices 
like the keyboard, the mouse, the printer, etc. It contains 
ports like HDMI (High Definition Multimedia Interface) 
to provide users with video output. Its credit-card-like size 
makes it extremely portable and affordable. It requires just 
a 5V micro-USB power supply, similar to the one used 
to charge a mobile phone.
Over the years, the Raspberry Pi Foundation has released 
a few different versions of the Pi board. The first version was 
Raspberry Pi 1 Model B, which was followed by a simple and 
cheap Model A. In 2014, the Foundation released a significant 
and improved version of the board —Raspberry Pi 1 Model 
B+. In 2015, the Foundation revolutionised the design of the 
board by releasing a small form factor edition costing US$ 
5 (about ` 323) called Raspberry Pi Zero. In February 2016, 
Raspberry Pi 3 Model B was launched, which is currently 
the main product available. In 2017, the Foundation released 
the updated model of Raspberry Pi Zero named Raspberry Pi 
Zero W (W = wireless). 
In the near future, a model that has improved technical 
specifications will arrive, offering a robust platform for 
embedded systems enthusiasts, researchers, hobbyists and 
engineers to use it in multi-functional ways to develop real-
time applications. 
Raspberry Pi as an efficient programming device
After getting the Pi powered up and the LXDE WM up 
and running, the user gets a full-fledged Linux box running 
a Debian based operating system, i.e., Raspbian. The 
Raspbian operating system comes with tons of free and open 
source utilities for users, covering programming, gaming, 
applications and even education. 
The official programming language of Raspberry Pi is 
Python, which comes preloaded with the Raspbian operating 
system. The combination of Raspberry Pi and IDLE3, 
a Python integrated development environment, enables 
Top IDEs for Raspberry Pi
The Raspberry Pi, a tiny single-board computer, has revolutionised the way in which 
computer science is being taught in schools. It has also turned out to be a boon 
for software developers. Currently, it has gained popularity much beyond its target 
market and is being used in robotics projects. 

www.OpenSourceForU.com | OPEN SOURCE FOR yOU | May 2017 | 79
Developers
Overview
BlueJ provides an efficient way for learning object-
oriented programming concepts and the GUI provides a class 
structure for applications like UML diagram. Every OOPS 
based concept, like class, objects and function calling, can be 
represented via interaction based design.
Features
 
Simple and interactive interface: The user interface is 
simple and easy to learn as compared to other professional 
interfaces like NetBeans or Eclipse. Developers can focus 
mainly on programming rather than the environment.
 
Portable: BlueJ supports multiple platforms like 
Windows, Linux and Mac OS X, and can even run 
without any installation.
 
New innovations: BlueJ IDE is filled with innovations in 
terms of the object bench, code pad and scope colouring, 
which makes development fun even for newbies.
 
Strong technical support: BlueJ has a hard-core functioning 
team that responds to queries and offers solutions to all 
sorts of developer problems within 24 hours.
Latest version: 4.0.1
Official website: https://www.bluej.org/
Geany IDE
Geany IDE is regarded as a very lightweight GUI based text 
editor that uses Scintilla and GTK+ with IDE environment 
support. The unique thing about Geany is that it is designed to 
be independent of a special desktop environment and requires 
only a few dependencies on other packages. It only requires 
GTK2 runtime libraries for execution. Geany IDE supports 
tons of programming languages like C, C++, C#, Java, 
HTML, PHP, Python, Perl, Ruby, Erlang and even LaTeX.
Features
 
Auto-completion of code and simple code navigation.
 
Efficient syntax highlighting and code folding.
 
Supports embedded terminal emulator, and is highly 
extensible and feature-rich since lots of plugins are 
available for free download.
Figure 1: Raspberry Pi 
Figure 2: The BlueJ GUI interface
Game
Compile
New Class...
Command
Parser
Parser1:
Parser
room1:
Room
Creating object.. Done.
Room
programmers to develop all sorts of Python based programs. 
In addition to Python, various other languages are 
supported by Raspberry Pi. A number of IDEs (integrated 
development environments) that are free and open source 
are also available. These allow programmers, developers 
and application engineers to develop programs and 
applications on Pi.
Top IDEs for Raspberry Pi
As a programmer and developer, the first thing you require 
is an IDE, which is regarded as a comprehensive software 
suite that integrates the basic tools that developers and 
programmers require to write, compile and test their software. 
An IDE contains a code editor, a compiler or interpreter and 
a debugger, which the developer can access via a graphical 
user interface (GUI). One of the main aims of an IDE is to 
reduce the configuration necessary to piece together multiple 
development utilities, and provide the same set of capabilities 
as a cohesive unit. 
An IDE’s user interface is similar to that of a word 
processor, for which tools in the toolbar support colour-
coding, formatting of source code, error diagnostics, 
reporting and intelligent code completion. IDEs are designed 
to integrate with third-party version control libraries like 
GitHub or Apache Subversion. Some IDEs are dedicated to 
a particular programming language, allowing a feature set 
that matches the programming language, while some support 
multiple languages.
Raspberry Pi has a wide range of IDEs that provide 
programmers with good interfaces to develop source code, 
applications and system programs.
Let’s explore the top IDEs for Raspberry Pi.
BlueJ
BlueJ is an IDE that is dedicated to the Java Programming 
Language and was mainly developed for educational 
purposes. It also supports short software development 
projects. Michael Kolling and John Rosenburg at Monash 
University, Australia, started BlueJ development in 2000 as a 
powerful successor to the Blue system, and BlueJ became free 
and open source in March 2009.

80 | May 2017 | OPEN SOURCE FOR yOU | www.OpenSourceForU.com
Developers Overview
 
Simple project management and supports multiple file 
types, which include C, Java, PHP, HTML, Python, Perl, 
and many more.
 
Highly customised interface for adding or removing 
options, bars and windows.
Latest version: 1.30.1
Official website: http://www.geany.org
AlgoIDE
AlgoIDE is a combination of a scripting language and an 
IDE environment, designed to function together to take 
programming to the next paradigm. It incorporates a powerful 
debugger, real-time scope explorer and executes the code, 
step by step. It is basically designed for all age groups to 
design programs and do extensive research on algorithms.
It supports various types of languages like C, C++, Python, 
Java, Smalltalk, Objective C, ActionScript, and many more.
Features
 
Automatic indentation and completion of source code.
 
Effective syntax highlighting and error management.
 
Contains a debugger, scope explorer and dynamic help 
system.
 
Supports GUI and traditional Logo programming 
language Turtle for the development of source code.
Latest version: 2016-12-08 (when it was last updated)
Official website: http://www.algoid.net/
Figure 3: The Geany IDE GUI interface
Figure 4: The Adafruit WebIDE GUI interface
Figure 5:  The AlgoIDE GUI interface
Adafruit WebIDE
Adafruit WebIDE provides a Web based interface for 
Raspberry Pi users to perform programming functions, and 
allows developers to compile the source code of various 
languages like Python, Ruby, JavaScript and many others. 
Adafruit IDE allows developers to put the code in a GIT 
repository, which can be accessed anywhere via GitHub.
Features
 
Can be accessed via Web browser on ports 8080 or 80.
 
Supports the easy compilation and running of source code.
 
Bundled with a debugger and visualiser for proper 
tracking, the navigation of code and to test source code.
Official website: https://learn.adafruit.com/webide/overview
Ninja IDE
Ninja IDE (Not Just Another IDE), which was designed 
by Diego Sarmentero, Horacio Duranm Gabriel Acosta, 
Pedro Mourelle and Jose Rostango, is written purely in 
Python and supports multiple platforms like Linux, Mac 
OS X and Windows, for execution. It is regarded as a 
cross-platform IDE software, especially designed to build 
Python based applications.
Ninja IDE is very lightweight and performs various 
functions like file handling, code locating, going to lines, tabs, 
automatic indentation of code and editor zoom. Apart from 
Python, several other languages are supported by this IDE. 
Features
 
An efficient code editor: Ninja-IDE is regarded as 
the most efficient code editor as it performs various 
functions like code completion and code indentation, and 
functions as an assistant.

www.OpenSourceForU.com | OPEN SOURCE FOR yOU | May 2017 | 81
Developers
Overview
 
Errors and PEP8 finder: It highlights static and PEP8 
errors in the file.
 
Code locator: With this feature, quick and direct access 
to a file can be made. The user can just make use of the 
‘CTRL+K’ shortcut to type anything, and the IDE will 
locate the specific text.
 
Its unique project management features and tons of 
plugins make Ninja-IDE highly extensible.
Latest version: 2.3
Official website: http://ninja-ide.org
 
IDE through Lazarus package files.
 
Makes use of Free Pascal, which is highly enhanced 
with new features and is even used in Android app 
development.
 
Highly extensible, open source and supports various 
frameworks to compile additional languages.
Latest version: 1.6.4
Official website: http://www.lazarus-ide.org 
Figure 6: The Ninja IDE GUI interface
Figure 7: The Lazarus IDE GUI interface
Lazarus IDE
Lazarus IDE was developed by Cliff Baeseman, Shane 
Miller and Michael A. Hess in February 1999. It is regarded 
as a cross-platform GUI based IDE for rapid application 
development, and it uses the Free Pascal Compiler. It 
inherits three primary features—compilation speed, 
execution speed and cross-compilation. Applications can be 
cross-compiled from Windows to other operating systems 
like Linux, Mac OS X, etc. 
This IDE consists of the Lazarus component library, 
which provides varied facilities to developers in the form of 
a single and unified interface with different platform-specific 
implementations. It supports the principle of ‘Write once and 
compile anywhere’.
Features
 
Powerful and fast enough to handle any sort of source 
code, and supports performance testing.
 
Easy to use GUI, which supports drag-and-drop 
components. Additional components can be added to the 
Codeblock IDE
Codeblock IDE was written in C++ using wxWidgets as a 
GUI toolkit and was released in 2005. It is a free, open source 
and cross-platform IDE supporting multiple compilers like 
GCC, Clang and Visual C++. 
Codeblock IDE is highly intelligent and performs various 
functions like Syntax highlighting, code folding, code 
completion and indentation, and has a number of external 
plugins for varied customisations. It can run on Windows, 
Mac OS X and Linux operating systems.
Features
 
Supports multiple compilers like GCC, Visual C++, 
Borland C++, Watcom, Intel C++ and many more. 
Basically designed for C++, but today supports 
many languages.
 
Intelligent debugger, which allows users to debug 
programs via access to the local function symbol and 
argument display, user defined watches, call stack, 
custom memory dump, thread switching and GNU 
debugger interface.
 
Supports varied features for migrating code from 
Dev-C++, Visual C++ and others.
 
Makes use of custom-built systems and stores information 
in XML extension files.
Latest version: 16.01
Official website: www.codeblocks.org 


www.OpenSourceForU.com | OPEN SOURCE FOR yOU | May 2017 | 83
Developers
Overview
A
lmost every software becomes complex as it evolves 
over a period of time, and can therefore suffer from 
frequent non-functional problems. These problems 
are linked to its maintainability, code manageability, 
extensibility, modifiability, determinism, etc. However, a 
good plan and design carved out at the initial stages could 
help developers enjoy the smoothness of the development 
process, as the software matures over a period of time. 
The use of state machines is one of the oldest and 
best known techniques for modelling the behaviour of 
a system. And its adoption dates way back to the study 
of physical matter. As a reference, water (H20) can exist 
in three different states – solid, liquid and gaseous — 
and in each state it exhibits a specific behaviour; also, 
there are defined events and actions that lead to water, 
steam or ice transitioning through its different states. 
Software designers have been using this example to 
model and partition systems into well-defined states, 
with specific actions and events that define how these 
systems move through these states. 
In this article, we look at the benefits of using state 
machines for modelling software along with some 
examples, and see what the open source world has 
to offer in this space.
Benefits of state machines
Some of the benefits of adopting state machines are listed below.
 
A complex problem can be broken down into smaller and 
manageable pieces called states, with each state being 
delegated a narrowly defined task. Modelling the system 
with a state machine ensures a self-documenting and 
compact representation of the software that automatically 
satisfies non-functional demands like manageability, 
extensibility, modifiability and determinism. It might 
seem like a lot of work in the beginning but as the system 
grows, this technique helps to dramatically reduce the 
overall complexity. Enabling a state machine for code that 
is already developed and has evolved quite a bit can be 
painful. So, the one-time investment in state machines for 
any software module right at the beginning of the software 
Using State Machines  
to Build Better Software
Often, an application runs in an environment of asynchronous, unordered and 
unpredictable events and the software needs to be robust enough to work 
seamlessly under these conditions. In this article, targeted at software designers 
and developers, we explore the benefits of using state machines to make robust 
software and the open source options available for this purpose. 
SMC

84 | May 2017 | OPEN SOURCE FOR yOU | www.OpenSourceForU.com
Developers Overview
Your favourite Magazine on  
Open Source is now on the Web, too.
OpenSourceForU.com
Follow us on Twitter@LinuxForYou
then define your state’s actions and various transitions using 
the auto generated code and StatefulJ APIs. You can refer 
to http://www.statefulj.org/framework/#installation for the 
installation and usage steps.
SMC
State machine compiler (SMC) provides users with a 
framework for writing a well-defined format to define states, 
transitions and actions. The framework is available for 
multiple programming languages like C, C++, Java, Python, 
Scala, Objective-C, JavaScript, Ruby and VB.NET. The 
code that SMC generates out of the user-defined state model 
follows the state design pattern, with a few variations. The 
application then uses these generated APIs to instantiate and 
run the state machine. You can refer to the SMC manual 
(http://smc.sourceforge.net/SmcManual.htm) for rules 
to define the state model, transitions and other elements. 
Incorrect usage could lead to compile or runtime errors.
The spring state machine
This is an open source framework released under the Apache 
2.0 licence for developers to use state machines in spring 
applications. It uses the builder design pattern for configuration 
and instantiation of states. Spring state machines provide some 
useful features like storing the state machine configurations 
in a persistent storage, UML Eclipse Papyrus modelling, 
Zookeeper based distributed state machine, etc. 
development life cycle (SDLC) is always good.
 
Once a state machine is in place, the developer can 
focus on tuning the behaviour of the system to meet 
functionality requirements and enhance business value, 
rather than spending time on enforcing the rules of its 
behaviour and other crucial non-functional attributes like 
code maintainability, flexibility, etc.
 
When a system has its responsibilities well delegated 
at different states, it also ensures predictability and 
determinism in the system, thereby making it developer-
friendly when troubleshooting.
 
State machines with well-defined states and events can 
keep in control the growth of conditional logic as well 
as ad-hoc code branching out with scattered Boolean 
flags in the software, and thereby control the overall 
cyclomatic complexity.
Digressing a bit into the history of software and 
technology, good designs and implementations backed by 
state machines have always proved excellent in various 
verticals. These include commonly used elements in our 
daily life like a DBMS transaction; TCP, BGP and many 
other networking protocols; system processes and threads, 
gaming software development, etc. All these software 
structures have a state machine in their DNA. Even everyday 
occurrences like traffic lights, vending machines and other 
digital electronic systems could all be quickly modelled using 
Mealy and Moore styled finite state machines. 
With all this said, let us now explore some options that the 
open source world has to offer developers and designers. You 
could enable a state machine for your code, even if it’s proprietary, 
with the open source state machine frameworks given below, and 
reap the benefits of the privileges we discussed earlier.
The StateFulJ framework
StateFulJ is an open source, lightweight, Java based finite 
state machine (FSM) framework that could help you embed 
a state machine into your application. It is a useful aid 
when your application has to deal with many synchronous 
and asynchronous events and requests, while handling 
concurrency in a reliable way. The user has to define the 
state machine model (states, events and transitions) using 
Java annotations, and the framework auto generates the 
necessary FSM infrastructure for the application. You can 
By: Shravan I.V.
The author currently works as a software developer at Cisco 
Systems India. He is interested in open source technologies, 
and can be reached at iv.shravan@gmail.com. 
[1] http://www.statefulj.org/framework/
[2]  http://smc.sourceforge.net/ 
[3] http://projects.spring.io/spring-statemachine/ 
[4] https://ocw.mit.edu/courses/electrical-engineering-
and-computer-science/6-01sc-introduction-to-
electrical-engineering-and-computer-science-i-
spring-2011/unit-1-software-engineering/state-
machines/MIT6_01SCS11_chap04.pdf 
References

www.OpenSourceForU.com | OPEN SOURCE FOR yOU | May 2017 | 85
Developers
Overview
Q
t, which is pronounced as ‘cute’, is a cross-platform 
development framework which was originally created 
by Haavard Nord and Eirik Chambe. It was first 
released on  May 20, 1995. The latest version, Qt 5.8, was 
released on  January 23, 2017.  Qt comes in four editions— 
Community, Indie Mobile, Professional and Enterprise. The 
Community version is under the open source licence, while 
the other three are commercially sold by the company, Qt.
Qt is not a programming language at all but a cross-platform 
development framework for the desktop, embedded systems 
and mobiles. It is basically written in C++.  The ‘t’ in it stands 
for ‘toolkit’ which describes it much better. It is, in fact, a set 
of tools. Remember that it is written as ‘Qt’ and not as ‘QT’. 
The first letter stands for Apple’s QuickTime and has very little 
to do with programming. Thus the framework itself and the 
applications/libraries using it can be compiled by any standards-
compliant C++ compiler like GCC, ICC, MSVC and Clang.  
Its purpose and capabilities 
Qt is used for developing multi-platform applications and 
graphical user interfaces (GUIs). However, programs without 
a GUI can also be developed, such as command line tools and 
consoles for servers. An example of a non-GUI program using 
Qt is the Cutelyst Web framework. GUI programs created 
with Qt can have a native-looking interface, in which case, Qt 
is classified as a widget toolkit.
It uses standard C++ with extensions, including signals 
and slots that simplify the handling of events, and this helps 
in the development of both GUI and server applications. Qt 
supports compilers like GCC and the Visual Studio suite. It 
also provides Qt Quick, which allows the use of scripting 
languages like JavaScript. Non-GUI features include SQL 
database access, XML parsing and JSON parsing.
Downloading and installing Qt
Download the copy of the Qt SDK or if you are using a Linux 
system, then use a copy of Qt and a compiler. Remember that 
if you are starting off, you might want to consider the open 
source LGPL version.
Qt can be installed in two ways:
1.  Through the Qt installers, whereby you download and 
install it.
2. Through the Qt source.  
Creating a new project
The following steps will help you create a project.
Start up Qt Creator (refer to Figure 1). Go to the File – 
New File or Project menu entry.
Choose Qt GUI Application and choose a name for it (see 
Figure 2). Enter a project name, say ‘qt-tutorial-01’ (Figure 3).
Select one or more versions of Qt to target. A desktop 
build is fine for this tutorial (Figure 4).
Select the base class to be QWidget and for everything else, 
leave it as the default settings (Figure 5). Check the project 
creation options on Summary and click Finish (Figure 6).
This will create a simple project consisting of four files.
 
main.cpp
 
widget.h
 
widget.cpp
 
widget.ui
Qt: The Cross-Platform 
Development Toolkit
Check out Qt, a cross-platform development framework, which can be used to 
develop application software that runs on various software and hardware platforms. 

86 | May 2017 | OPEN SOURCE FOR yOU | www.OpenSourceForU.com
Developers Overview
Figure 1: Qt Creator
Figure 4: Select path
Figure 3: Qt GUI
Figure 6: Summary
Figure 2: New project
Figure 5: Select Baseclass
Facts and features
 
There were one million downloads of Qt 5.3 in just over a 
month after its launch.
 
There are 250+ commits by over 60 contributors per week.
 
Companies that use Qt include AMD, Autodesk, Siemens, 
Panasonic, etc.
 
Alongside the support for traditional desktop user 
interfaces, Qt includes support for declarative UI 
development with Qt Quick.
By: Neetesh Mehrotra 
The author works as a systems engineer in TCS, and his areas 
of interest are Java development and automation testing. He 
can be reached at mehrotra.neetesh@gmail.com.
https://wiki.qt.io
Reference
 
Widgets can be used for painting, printing and rendering.
 
Integration between Qt and WebKit makes it possible for 
developers to use a fully-featured Web browser engine. 
The Qt framework is simple and user-friendly. Once 
you understand how this simple app works, you can start 
adding some more bells and whistles like signal/slot 
connections. 

Developers
How To
www.OpenSourceForU.com | OPEN SOURCE FOR yOU | May 2017 | 87
If you’ve been faithfully 
following this series, 
then get ready to create 
a fun game. This is the 
popular sliding number 
puzzle which contains 
numbered tiles within 
a square frame, with 
one blank space for 
a tile. The idea is to 
move a tile to the blank 
space, and rearrange 
the tiles in sequence in 
the shortest possible 
time and with the least 
number of moves.
that are available in the palette, and 
moving between designer and block 
editor is so much fun.
In this tutorial, I will give you the 
rather challenging task of building 
yet another interesting and powerful 
game with the resources available in 
App Inventor 2. I will be making a 
15-number sliding puzzle which is also 
known by other names such as Gem 
Puzzle, Boss Puzzle, Game of Fifteen, 
Mystic Square, etc. It also comes 
in various sizes and variations. The 
objective is to arrange the numbers in 
sequence, leaving only the corner block 
empty to allow the number blocks to be 
moved around.  If the size of the puzzle 
is 3×3 tiles, it is called the 8-puzzle 
or 9-puzzle, and if it is made up of 
4×4 tiles, it is called the 15-puzzle or 
16-puzzle—named, respectively, for the 
number of tiles or the number of spaces.
Theme of the application 
The theme is pretty simple and you 
W
e started Android app 
development with App 
Inventor, picking up some 
pretty basic ideas to get used to the 
concepts, and we have moved towards 
more challenging and engaging 
application ideas.
I have been getting regular 
feedback, suggestions and debugging 
questions. By now, most readers 
have gained enough experience and 
hands-on practice with all the features 
of the tool. If you are a first-time 
reader of this series, you too can very 
well learn from this point onwards 
without having any prior programming 
experience or any specialised 
knowledge. Apps developed with 
App Inventor can be uploaded to 
the Google Play Store and I will 
be delighted to see the good work 
done by readers.
If we talk about App Inventor 
specifically, then we can proudly say 
that we have mastered the components 
Creating the Sliding Number Game 
in App Inventor 2
probably have already got the idea 
from descriptions in earlier articles in 
this series. We will make an Android 
application that will present a square 
board, which is covered with numbers 
on tiles (in a series starting from 1 
but in jumbled order) with one empty 
space. The objective is to arrange the 
numbers in sequence. To make it more 
interesting, you can count the time 
and the number of moves it takes you 
to solve the puzzle so that you can 
challenge your friends as well. 
GUI requirement 
For every application, we have a 
graphical user interface or GUI, which 
helps the user to interact with the on-
screen components. The block editor 
section defines how each component 
responds to user actions.  
GUI requirements for Screen1
1. Label: This is a static text 
component that is used to display 

Developers How To
88 | May 2017 | OPEN SOURCE FOR yOU | www.OpenSourceForU.com
some headings or markings on to 
the screen. 
2. Button: This will let you trigger 
the event and is a very essential 
component.
3. Horizontal arrangement: This 
comprises special components 
that keep all child components 
aligned within themselves.
4. Table arrangement: This 
comprises special components 
that keep all child components, 
within itself, well arranged in 
rows and columns as specified in 
the properties.
5. Clock: This is the timer 
component that provides the time 
control of the events happening. 
Using this, we can set certain 
events to happen after a particular 
interval.
6. Notifier: This is used to display 
some instructions or give controls 
over your existing components. 
You will see its functionality in 
more detail as we implement it in 
our game.
7. Canvas: This is the component 
for drawing and animation. You 
can place various animation 
objects on the canvas to control 
them via user actions. You will 
be able to see more details on 
this aspect as we work on the 
application.
8. Image: The image component 
is used when you need to show 
some pictures or special effects 
on screen.
9. TinyDB: This is the database 
component for your application. 
Figure 1: Designer screen
Figure 1: Designer screen
Figure 2: How the application looks
If we need to store some variables 
and values that we want to persist 
even after the application is closed, 
we use TinyDB. 
The components that we need for 
this application are given in Table 1. We 
will drag them on to the designer from 
the left hand side palette. 
1. Drag and drop the components 
mentioned in Table 1 to the viewer.
2. Visible components will be seen by 
you while the non-visible components 
will be located beneath the viewer 
under the tag ‘Non-visible’.
3. For the application’s name, set 
the ‘Screen1’ title property to 
‘SlidingPuzzle’.
4. All images need to be put within the 
vertical arrangement so as to keep 
them aligned in a square view.
5. If you have dragged and placed 
everything, the layout will look 
something like what’s shown 
in Figure 1.
6. Make the necessary property 
changes like we did in changing 
the text property for the label and 
button components. 
7. Renaming the components helps to 
identify them in the block editor.
8. Your graphical user interface is 
ready. The application will look 
exactly like what’s shown in Figure 
2, after installation in the device.
9. The hierarchy of the components 
dragged to the designer is shown 
in Figure 3.
If you are confused by the designer 
and the components viewer, let me 
discuss it a bit more. Here is the 
hierarchy that we have placed for 
our application.
Table 1
Component name
Purpose 
Location
1
Label                         
To display a label                    
Palette-->User Interface-->Label
2
Button                            
To trigger events                   
Palette-->User Interface-->Button
3
Horizontal arrangement
To arrange the child components
Palette-->Layout-->Horizontal Arrangement
4
Table arrangement 
To arrange the child components
Palette-->Layout-->Table Arrangement
5
Clock
To have time control of events
Palette-->Sensors-->Clock
6
Notifier
To display on-screen information
Palette-->User Interface-->Notifier
7
Canvas    
To enable drawing 
                  
Palette--> Drawing and Animation--> Canvas
8
Image
To display images                   
Palette-->User Interface-->Image
9
TinyDB
To store data  
                              
Palette-->Storage-->TinyDB

Developers
How To
www.OpenSourceForU.com | OPEN SOURCE FOR yOU | May 2017 | 89
1. At the top, we have the title for 
our application. It’s always a good 
practice to name your application 
and show it on the screen as well. 
We have set the title property 
of Screen1 as the name of the 
application, so the top grey bar 
will be the application title or you 
can make it within the screen by 
providing a label.
2. Below that, we have a score card 
which is hidden in the beginning 
and will be loaded as the leader 
board at the end of the game.
3. In the middle, we have labels of the 
time and the moves. We will display 
the time taken and the steps count to 
solve the puzzle.
4. Beneath that, we have our actual 
play area. We have placed 16 image 
components on the canvas. Assign a 
number to the 15 numbers and keep 
one block empty.
5. At the end, the ‘Shuffle the tiles’ 
button will begin a new session 
each time to randomly change the 
numbers displayed on the board. 
Now, let’s head towards the block 
editor to define the behaviour. Let’s 
discuss the actual functionality that we 
are expecting from our application.
1. The board will be loaded with 
the numbers 1 to 15, in sequence. 
On pressing the ‘Shuffle the tiles’ 
button, numbers will be randomly 
distributed on the square board.
2. If there is an empty block adjacent 
to any block that is touched, then 
this block should move to this 
empty position, and a blank should 
be created in the place from which it 
has moved. This is just a swapping 
of positions.
3. Each move will increase the 
move count.
4. Our clock timer will be running in 
the background and will count the 
seconds. Make sure to check the 
‘Timer always fires’ checkbox in the 
clock properties.
5. Once the blocks are arranged in 
sequence, we need to display a 
message on the screen, and the 
move count and the time taken 
should be recorded to be presented 
on the leader board.
So let’s move on and add these 
behaviours using the block editor. I 
hope you remember how to switch 
from designer to block editor. There is a 
button right above the Properties pane 
to switch between them.
Block editor blocks 
I have already prepared the blocks for 
you. All you need to do is drag the 
relevant blocks from the left side palette 
and drop them on the viewer. Arrange 
the blocks the same way as shown in 
the image. I will explain what each does 
and how it is called.
As soon as Screen1 is loaded, 
certain variables to control events later 
on will be declared. In Figure 4, all the 
necessary variables are declared with 
their initial state.
Before we present anything to the 
user, we need to reset everything by 
keeping all the blocks at their default 
position and setting the lists. How the 
Screen1.Initialize block takes care of 
that is displayed in Figure 5.
The Shuffle button will re-arrange 
the blocks and start the game. Figure 6 
shows the blocks for events when you 
click on the Shuffle button.
The ‘tile_shuffle’ procedure is a 
recursive call to shuffle the blocks; we 
have put that line of blocks within a 
procedure name so that we can call it 
any number of times.
The counter will be triggered as 
Figure 3: A view of the components
ScoresDB
Rename
Delete
Game
Notifier1
BtnShuffle
Tray
LblMoves
Label1
LblTimer
HorizontalArrangement1
TblScores
Screen1
Components
LblScoreHdr
Figure 4: Block editor image 1
Figure 5: Block editor image 2
Figure 6: Block editor image 3
Figure 7: Block editor image 4

Developers How To
90 | May 2017 | OPEN SOURCE FOR yOU | www.OpenSourceForU.com
By: Meghraj Singh Beniwal
The author is a B. Tech in electronics and communication, a freelance writer and 
an Android app developer. He is currently working as an automation engineer 
at Infosys, Pune. He can be contacted at meghrajsingh01@rediffmail.com or 
meghrajwithandroid@gmail.com 
soon as your play board is ready to use. 
It will increase by one second as we 
have set the interval property to ‘1000 
milliseconds’ in the designer for the 
clock component. 
Now, when a tile is touched, how 
it should move is depicted in Figure 9. 
I have posted for two blocks. It will be 
the same for the others as well, keeping 
a separate event handler for all; so 
please create accordingly.
Application code
It was not possible to snap each and 
every block of the logic, so I am 
providing the complete code as a bar 
code. Scan the code (see QRCode 1) 
to obtain the .aia file, which you can 
upload to your projects in App Inventor 
2 and view the complete code.
If you want to see how the 
application looks after installation, scan 
the code given in QRCode 2 to get the 
application apk.
Packaging and testing 
To test the app, you need to get it 
on your phone. First, you have to 
download the application to your 
computer and then move it to your 
phone via Bluetooth or USB cable. I’ll 
tell you how to download it.
1.  On the top row, click on the ‘Build’ 
button. It will show you the option 
to download the apk to your 
computer.
2.  The progress of the download can be 
seen, and after it’s been successfully 
done, the application will be placed 
in the Download folder of your 
directory or the preferred location 
you have set for it.
3.  Now you need to get this apk 
file to your mobile phone either 
via Bluetooth or USB cable. 
Once you have placed the apk 
file to your SD card, you need to 
install it. Follow the on-screen 
instructions to install it. You 
might get some notification or 
warning saying,‘Install from 
Figure 8: Block Editor image 5
Figure 9: Block editor image 6
QRCode 1: Project source code file (.aia)
QRCode 2:  Application install File (.apk)
un-trusted source’. Allow this 
from the settings and after the 
successful installation, you will 
see the icon of your application 
in the menu of your mobile. Here, 
you will see the default icon, 
which can be changed and we will 
tell you how to do so as we move 
ahead in this course.
I hope your application is working 
exactly as per the requirements 
you have given. Now, depending 
upon your usage requirements and 
customisation, you can change various 
things like the image, sound and 
behaviour, too.
Debugging the application 
We have just created the prototype 
of the application with very basic 
functionality, but what else might 
the user be interested in? Now come 
various use cases which require 
attention in order not to annoy the 
user. Your app should be able to 
address the following use cases:
1. Can we think of a way to share our 
moves and puzzle-solving time 
with our friends?
2. Can we display two boards 
simultaneously for two users and 
make it a dual player game?
3. What will other versions be like 
if we think of expanding the size 
of the square?
These are some of the scenarios 
that might occur and users will be 
pretty happy seeing these features 
implemented.
Think about all these scenarios 
and how you could integrate their 
solutions into the application. Do ask 
me if you fail to address any of the 
above cases.
You have successfully built 
another useful Android app for 
yourself. Happy inventing! 

www.OpenSourceForU.com | OPEN SOURCE FOR yOU | May 2017 | 91
Developers
How To
H
ave you ever thought of searching a string in the files 
of a given folder? If you are a Linux lover, you must 
be thinking about the grep command. But in Windows, 
there is no grep command. By using Python programming, 
you can make your own command which will search the 
string pattern from the given files. The program also offers 
you the power of regular expressions to search the pattern.
In this article, I am going to show you an amazing utility, 
which will help you to find the string from a number of files.
The program, see.py, will search for the string pattern 
provided by the user, from the files presented in the directory, 
also given by the user. This is equivalent to the grep command 
in the Linux OS. Here, we will use Python 2.7. 
The program expects the string pattern and directory from 
the user. Let us examine the code and discuss it.
Import the mandatory modules 
import os
import re
import sys
import argparse
In the following code I have declared a class Text_search 
class Text_search :
 
def __init__(self, string2, path1,i=None):
 
 self.path1= path1
 
 self.string1 = string2
 
 self.i=i
 
 if self.i:
 
  
string2 = string2.lower()
 
 self.string2= re.compile(string2)
The following method gives the file’s name in which the 
given string is found: 
 
def txt_search(self):
 
 file_number = 0
 
 files = [f for f in os.listdir(self.path1) if os.path.
isfile(self.path1+”/”+f)]
 
 for file in files:
 
  
file_t = open(self.path1+”/”+file)
 
  
file_text= file_t.read()
 
  
if self.i:
 
  
 
file_text=file_text.lower()
 
  
file_t.close()
 
  
if re.search(self.string2, file_text):
 
  
 
print “The text “+self.string1+” found in “, 
file
 
  
 
file_number=file_number+1
 
 print “total files are “,file_number
Searching Text Strings from 
Files Using Python
Searching text strings from files in a given folder is easily accomplished by using Python 
in Windows. While Linux has the grep command, Windows does not have an equivalent. 
The only alternative, then, is to make a command that will search the string. This article 
introduces see.py, which helps in accomplishing this task.

92 | May 2017 | OPEN SOURCE FOR yOU | www.OpenSourceForU.com
Developers How To
The following method returns the file’s name as well as 
the line numbers in which the given string is matched.
 
 
def txt_search_m(self):
 
 files = [f for f in os.listdir(self.path1) if os.path.
isfile(self.path1+"/"+f)]
 
 file_number = 0
 
 for file in files:
 
  
file_t = open(self.path1+"/"+file)
 
  
line_number=1
 
  
flag_file = 0
 
  
for line1 in file_t:
 
  
 
if self.i:
 
  
 
 
line1 = line1.lower()
 
  
 
if re.search(self.string2, line1):
 
  
 
 
flag_file= 1
 
  
 
 
print "The text "+self.string1+" found in 
", file, " at line number ",line_number
 
  
 
line_number=line_number+1
 
  
if flag_file == 1:
 
  
 
file_number=file_number+1
 
  
 
flag_file=0
 
  
file_t.close() 
 
 print "total files are ",file_number
The following method also returns the file’s name as well 
as the line numbers in which the given string is matched. 
This method works in recursive mode. 
 
def txt_search_r(self):
 
 file_number = 0
 
 for root, dir, files in os.walk(self.path1, topdown = 
True):
 
  
files = [f for f in files if os.path.
isfile(root+”/”+f)]
 
  
for file in files:
 
  
 
file= root+”/”+file
 
  
 
file_t = open(file)
 
  
 
line_number=1
 
  
 
flag_file = 0
 
  
 
for line1 in file_t:
 
  
 
 
if self.i:
 
  
 
 
 
line1=line1.lower()
 
  
 
 
 
 
  
 
 
if re.search(self.string2, line1):
 
  
 
 
 
flag_file= 1
 
  
 
 
 
print “The text “+self.string1+” found 
in “, file, “ at line number “,line_number
 
  
 
 
 
line_number=line_number+1
 
  
 
if flag_file == 1:
 
  
 
 
file_number=file_number+1
 
  
 
 
flag_file=0
 
  
 
file_t.close() 
 
  
 
 print “total files are “,file_number
This is the main function of the program which handles 
all the options. The program offers you six options. The 
–m option gives the number of the file and the line. –mi 
is case-insensitive. You can use the –h option to get help 
for all options.
def main():
 
parser = argparse.ArgumentParser(version=’1.0’)
 
parser.add_argument(‘-m’, nargs = 2, help = ‘To get 
files as well as line number of files ‘)
 
parser.add_argument(‘-s’, nargs = 2, help = ‘To get the 
files contain string ‘)
 
parser.add_argument(‘-r’, nargs = 2, help = ‘To search 
in recusrive order ‘)
 
parser.add_argument(‘-mi’, nargs = 2, help = ‘-m option 
with case insensitive ‘)
 
parser.add_argument(‘-si’, nargs = 2, help = ‘-s option 
with case insensitive ‘)
 
parser.add_argument(‘-ri’, nargs = 2, help = ‘-r option 
with case insensitive ‘)
 
 
args = parser.parse_args() 
 
If you select option –m, then it will call the txt_
search_m() method of class Text_search(). 
 
try:
 
 if args.m:
 
  
dir = args.m[1]
 
  
obj1 = Text_search(args.m[0],dir)
 
  
obj1.txt_search_m()
 
 
If you select option –s, then it will call the method 
txt_search().
 
 elif args.s:
 
  
if args.s[1]:
 
  
 
dir = args.s[1]
 
  
 
obj1 = Text_search(args.s[0],dir)
 
  
 
obj1.txt_search()
 
 
If you select the –r option, then it will call the method 
txt_search_r().
 
 elif args.r:
 
  
if args.r[1]:
 
  
 
dir = args.r[1]
 
  
 
obj1 = Text_search(args.r[0],dir)
 
  
 
obj1.txt_search_r()
If you select the –mi option, then it will call the txt_
search_m() method in case-insensitive mode.

www.OpenSourceForU.com | OPEN SOURCE FOR yOU | May 2017 | 93
Developers
How To
 
 elif args.mi:
 
  
dir = args.mi[1]
 
  
obj1 = Text_search(args.mi[0],dir,i=1)
 
  
obj1.txt_search_m()
If you select option –s, then it will call the method txt_
search() in case-insensitive mode.
 
  
 
 
 elif args.si:
 
  
if args.si[1]:
 
  
 
dir = args.si[1]
 
  
 
obj1 = Text_search(args.si[0],dir,i=1)
 
  
 
obj1.txt_search()
If you select the –r option, then it will call the txt_
search_r() method in case-insensitive mode.
 
 
 
 
 
 
 elif args.ri:
 
  
if args.ri[1]:
 
  
 
dir = args.ri[1]
 
  
 
obj1 = Text_search(args.ri[0],dir,i=1)
 
  
 
obj1.txt_search_r()
 
  
 
 
 
 
 print “\nThanks for using L4wisdom.com”
 
 print “Email id mohitraj.cs@gmail.com”
 
 print “URL: http://l4wisdom.com/see_go.php”
 
 
 
except Exception as e:
 
 print e
 
 print “Please use proper format to search a file use 
following instructions”
 
 print “see file-name”
 
 print “Use <see -h >  For help”
main()
Let’s make exe files using pyinstaller modules as 
shown in Figure 1.
After conversion, it will make a directory called see\dist. Get 
the see.exe files from the directory see\dist and put them in the 
Windows folder. In this way, see.exe is added to the system path.
see.exe works like a DOS command. 
Let us use the program see.
Use the option –s as in Figure 2. You can see that only file 
names are returned.
Use the option –m as shown in Figure 3. You can see that 
file names and lines are returned.
Use the option –r as shown in Figure 4. In this option, –m 
works in recursive mode.
Figure 1: Python program to make exe file
Figure 3: Option –m, which is case-sensitive 
Figure 4: Option –r, which is case-sensitive
Figure 5: Option –si, which is case-insensitive
Figure 6: Option –mi, which is case-insensitive
Figure 2: Option –s, which is case-sensitive
Continued on page 98...

94 | May 2017 | OPEN SOURCE FOR yOU | www.OpenSourceForU.com
OpenGurus How To
M
any young programmers dream of writing the code 
for their own operating system. But when they 
realise they have to write everything from scratch, 
including the code for the device and file system drivers, 
they give up the dream.
Of course, one can just write the core components and 
port all the existing stuff to the new system. But even getting 
a kernel ready – one that supports only very basic features 
—would be a long process. Moreover, porting the existing 
components will not give you that ‘developed-by-me’ feeling.
So here is a solution: start off by writing something that 
just boots, rather than an entire OS, or even a kernel. Later, 
the same steps can be followed to develop an advanced 
bootloader, and eventually a kernel, which essentially lays the 
foundation for your own OS.
Before starting, let’s first address the following question: 
why develop a new operating system?
This article doesn’t assume that everybody is going to 
develop a brand new operating system. We have many OSs 
already, and our time and resources could be donated to such 
existing community projects.
However, one reason why you could consider writing 
your own OS is that it's a great learning experience. 
You are at the lowest level of software (just above the 
firmware), and in contact with bare metal. Any hardware 
component is ready to obey your orders. And all that 
matters is your capability to give the correct orders (which 
is what is called drivers). Despite being time-consuming, 
bootsector experiments can give you the confidence that 
doing an entire Java or PHP course can't.
 Note:  The code snippets and explanations given in this 
article target the Intel x86 architecture. This means you can 
try them on Intel 8086 to Core i7, any x86 compatible CPU 
from AMD or other vendor, or on an emulator like QEMU.
Also, modern technologies including GPT, EFI and GNU 
Multiboot have been avoided in favour of the traditional 
BIOS-based booting technology. It seems to be the best 
starting point, and is still supported by all modern machines.
The boot process and the bootsector
The steps involved in the boot process primarily depend on 
the hardware components, firmware, BIOS, disks and the 
operating system itself.
Hack the Bootsector  
and Write Your Own!
This article is a tutorial on writing your own bootsector. It is a good exercise in 
understanding how the bootsector works and trying your hand at writing something that 
boots. Who knows? You might be able to write your own OS some day!


96 | May 2017 | OPEN SOURCE FOR yOU | www.OpenSourceForU.com
OpenGurus How To
Generally speaking, when a system is turned on, the 
CPU starts and executes the startup code from the ROM 
chip. After the Power-On-Self-Test (POST), the BIOS 
locates the boot disk, loads its bootsector (the first sector 
or 512 bytes) code into the RAM (location 0x7c00), and 
transfers the controls to it.
The program that has got the controls now is called the 
first bootstrap loader (or the first stage bootloader). This is 
the program that we are going to write and test in this article.
With a size constraint of 512 bytes, its only purpose 
would be to load the next stages in the bootloading process. 
But this limited space is sufficient for our experiments.
The ‘Hello World’ program
The Assembly code for a simple ‘Hello World’ program that 
works on the x86 architecture follows. There is no space 
for a detailed explanation. However, the core idea is to take 
each letter from a string literal (msg, here), and display it 
using the BIOS function in order to print a character.
Now just enter the following code using a text editor 
and save it as hello.asm.
; Set DS (data segment base) as 0x7c0 
mov ax, 0x7c0 
mov ds, ax 
 
mov cx, MSGLEN 
mov si, msg 
mov ah, 0xe      ; BIOS 10h function code for tty output 
putchar: 
 
mov al, [si] ; Character to be displayed 
 
int 0x10     ; BIOS interrupt for video service 
 
inc si 
 
loop putchar 
 
jmp $ ; Jump here (i.e, loop forever) 
 
msg: db 'Hello, world!' 
 
; Let MSGLEN = Length of msg 
MSGLEN: EQU ($ - msg) 
 
; We need the boot signature as the last two bytes. 
; That's why the remaining space is padded off. 
padding: times (510 - ($ - $$)) db 0 
 
BOOT_SIGN: db 0x55, 0xaa
One might ask: why Assembly language? There is no 
escaping from Assembly, at least, not in the early stages of 
developing an OS. Assembly is highly hardware-dependent 
and less productive, but it gives greater control. Also, we 
have a limited space of 446 bytes for our bootsector code, 
which is too short for a high-level program.
Getting it assembled using NASM
Although there are many assemblers available (including 
GNU Assembler), I prefer NASM, the Netwide Assembler, 
for its simplicity. Let's use the following command to 
assemble this code (before that, make sure you have the 
package nasm installed on your computer):
nasm -o hello.bin -f hello.asm
I am assuming the command shell is in the same directory 
as the source code. If not, use the command cd to navigate.
hello.asm is the input file, and the option -o specifies 
the output file, which is hello.bin (an extension has no 
significance, actually).
The option -f says that the format of the output file should 
be flat (plain or raw) binary. Usually, assemblers and linkers 
choose high-level executable formats like ELF and PE, which 
cannot be executed by the CPU without help from an OS. But 
we need something that can be directly executed by the CPU. 
This is why we assemble our program as flat binary.
Testing it with QEMU
We can use emulators and virtual machine monitors to test 
our bootable code without restarting the actual machine and 
getting out of the current OS. Let’s choose QEMU as the 
emulator to be used in this article, for its portability. Most 
GNU/Linux distros provide the package qemu, and you can 
install it directly.
Now, simply run the following command (again, if the 
Shell is in a different directory, use cd to navigate first). This 
command instructs QEMU to start a virtual machine with 
the disk image hello.bin considered to be the boot disk.
qemu-system-i386 hello.bin
Now it works!
If you’ve got KVM installed, you can use the following 
command also: 
kvm hello.bin
 
Figure 1: QEMU runs the Hello World bootable program 

www.OpenSourceForU.com | OPEN SOURCE FOR yOU | May 2017 | 97
OpenGurus
How To
skip at the start of the output; and skip is the number of blocks 
to skip at the start of the input. conv=notrunc ensures that the 
output file is not truncated (i.e., all other sectors in the USB 
Flash drive are preserved).
Now let’s test it on the real BIOS. Connect the USB drive 
and restart the computer. If USB booting is enabled and given 
high preference, the system should directly load your boot 
code. If it doesn’t, restart again and enter the BIOS settings. 
Ensure the following, save the settings and restart again:
 
USB booting is enabled and given preference over other 
drives like HDD.
 
Legacy booting is enabled and is preferred over UEFI.
Now you should see your ‘Hello World’ program running. 
When you've finished enjoying your own bootable program, 
just press the power button on the tower to shut down, or use 
Ctrl + Alt + Del to restart.
Using a hex editor
A hex editor is a program that lets you view/edit the 
contents of any computer file as a plain stream of bytes, 
and it is usually represented in hexadecimal values. While 
developing bootable programs, we can use a hex editor to 
review the internal byte patterns in a disk image or pseudo-
files like /dev/sdb. In our case, for example, you can use a 
hex editor to ensure that we haven't exceeded the limit of 
446 bytes for our boot code.
The popular command hexdump might be already 
available on your GNU/Linux system. However, I recommend 
installing the package <i>bless</i>, which provides the easy 
and full-featured Bless hex editor.
After installation, launch Bless from the menu or by using 
the command bless. If you are not the root user, you might not 
be able to open system files like /dev/sdb (which represents 
the hard disk). You'll need the assistance of commands like su, 
sudo or gksudo. For example:
sudo bless /dev/sdb 
gksudo bless /dev/sdb
 Caution:
 Never open actual disks (especially your primary hard 
disk) if you are not sure what you are doing. Editing files 
like /dev/sda can even break the booting, OS and the 
partition table. However, trying disk images (e.g., .img and 
.iso files) is safe if you have their backup copies.
Another example: A simple typewriter
When we run hello.bin (our first example), the keyboard does 
not respond. Given below is the code for a simple typewriter, 
which displays whatever you type.
mov ax, 0x7c0 
mov ds, ax 
Getting it set on an actual disk
Let’s get our code written onto the bootsector of a USB Flash 
drive. First, find the drive name of the USB Flash drive. If 
you have just a single SATA hard disk installed, the next drive 
you connect (here, the USB Flash drive) would be called /dev/
sdb. To verify this, launch the Disk Utility or try the command 
lsblk after connecting the drive.
 Caution:
Incorrect identification of drive names can cause 
unexpected data loss. Also, take a backup of important files 
before you start experimenting with a disk.
We are not performing a simple copy-paste. So it is better 
to unmount the drive (in case it got mounted automatically) 
by using the following command: 
sudo umount /dev/sdb
Now, we can use the following command to copy the 
bootsector. But wait! Don’t even think about executing it until 
you complete the next paragraph.
sudo dd if=hello.bin of=/dev/sdb
This command will copy the data in the file hello.bin 
to the Flash drive, directly (i.e., not as a file). To make it 
useful again, you’ll have to format it. Alternatively, the 
following commands copy the necessary parts only, leaving 
all other bytes intact.
sudo dd bs=446 count=1 conv=notrunc if=hello.in of=/dev/sdb 
sudo dd bs=1 count=2 seek=510 skip=510 conv=notrunc if=hello. 
bin of=/dev/sdb
The first line copies the boot code and the second line 
copies the boot signature bytes (0x55, 0xaa). bs means the 
block size is to be considered while copying; count is the 
number of blocks to copy; seek is the number of blocks to 
Figure 2: Bless hex editor displays the contents of a USB Flash drive 

98 | May 2017 | OPEN SOURCE FOR yOU | www.OpenSourceForU.com
OpenGurus How To
By: Nandakumar Edamana
The author is a free software user, developer, hacker and 
activist who has developed a few software packages including 
Sammaty Election Software. He writes for mainstream media 
and composes music as a passion. Website: nandakumar.
co.in. You can contact him at nandakumar96@gmail.com.
read_and_display: 
 
mov ah, 0; BIOS 10h function code for keyboard read 
 
int 0x16; BIOS interrupt for keyboard service 
 
; Waits for a keypress. 
 
; Now we have the ASCII code of the pressed key in AL. 
 
 
mov ah, 0xe; BIOS 10h function code for tty output 
 
            ; Anything in AL will be displayed. 
 
int 0x10    ; BIOS interrupt for video service 
 
 
jmp read_and_display 
 
padding: times (510 - ($ - $$)) db 0 
BOOT_SIGN: db 0x55, 0xaa
Figure 3: QEMU runs the typewriter program 
The way ahead
All examples discussed in this article are based on the Intel 
x86 Real Mode, which has a limited memory of 1MB, and 
no protection walls between applications. Modern systems 
make use of the Protected Mode, which is a must to write a 
general-purpose OS. However, for backward compatibility 
and simplicity, modern CPUs still start in the Real Mode. 
That means the examples we discussed are still useful, 
avoiding the need for a complete reboot.
You can find many websites that can help you in all 
these processes, among which is the remarkable wiki.
osdev.org. However, as the term bootstrap loading 
suggests, you are pulling yourself up with your own 
bootstraps, which means that original experimentation 
and a lot of patience are required. 
By: Mohit 
The author is a certified ethical hacker and EC Council 
certified security analyst. He has a master’s degree in 
computer science from Thapar University, and is the author 
of ‘Python Penetration Testing Essentials’. You can contact 
him at mohitraj.cs@gmail.com and https://in.linkedin.com/
in/mohit-raj-990a852a. 
Figure 7: Option –ri, which is case-insensitive
Figure 8: Help option
Figure 9: Regular expressions 
Figure 10: The power of regular expressions
Use the option –si as shown in Figure 5. You can see 
that only file names are returned, and text searching is 
impervious to upper and lower case. 
Use the option –mi as shown in Figure 6. Use the 
option –ri as shown in Figure 7.
In order to get help, use the option –h as shown  
in Figure 8.
The program offers you the power of regular 
expressions. Figure 9 shows the file 1.txt, which contains 
text. Let us use the regular expression, ‘+’ operator. 
See Figure 10, which shows the power of regular 
expressions. 
Continued from page 93...

www.OpenSourceForU.com | OPEN SOURCE FOR YOU | MaY 2017 | 99
For U & Me
Case Study
Vodafone Deploys Open Source  
to Reduce Vendor Lock-in
Developers at Vodafone India also take support from 
product sites. Additionally, the telco has some partner-
managed solutions that are taken care of by third parties.
Moving away from proprietary technologies
Vodafone India had initially relied on proprietary 
technologies. But with the massive advances in the open 
source world, the operator has decided to make the shift to 
community solutions.
“Growth, cost reduction and architecture improvement 
initiatives compel us to keep transforming our IT state 
year-over-year. This allows us to review our technology 
stack, avoid vendor lock-in and analyse cost implications, 
resulting in wider implementation and induction of open 
source technologies,” Gupta told Open Source For You.
Customer Point of Sale (CPOS)
One of the technologies to have recently received open source 
treatment by Vodafone engineers is a customer onboarding 
Vodafone India is the second largest player in the Indian telecom sector after 
Bharti Airtel, with a base of over 200 million subscribers. The telco giant uses 
digital GSM technologies on 900MHz and 1800MHz frequencies to offer traditional 
2G and 3G services alongside an advanced 4G LTE network.
A
part from offering mobile device users in the country 
with seamless connectivity, Vodafone India is a large 
consumer of open source software. The subsidiary of 
the UK-based Vodafone Group Plc has a talent pool of over 
100 people involved in major open source developments. 
Moreover, it considers open source as the key factor when it 
comes to deciding the architecture for all its partners.
“Vodafone India ensures that all partner contracts have 
open source as the key architecture deciding point,” says Vikas 
Grover, former chief information officer, Vodafone India.
Vodafone's developer team uses traditional open 
source solutions such as Java, Drools, Eclipse Mars and 
Apache Hadoop as well as the new versions of R, Python, 
Cassandra and Struts, among various others.
To enhance existing offerings and to discover new 
developments, the team accesses well-known repositories. 
“We sometimes take help from the contributors available 
on GitHub and Stack Overflow,” says Rahul Gupta, VP for 
solution and planning of IT, Vodafone India.

100 | MaY 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com
For U & Me
Case Study
in order to shift the solution to open source software. The 
major bottleneck in the transformation process was to enable 
real-time communication between the app server and JPS 
server, which was based on the Java state machine. The 
engineers were also required to optimise Enterprise Java 
Beans (EJB) and Web service calls. 
“We designed XML-based flow and translated BPEL 
rules to Java XML to begin the transformation. Our 
engineers also used a caching mechanism to optimise 
Appin server calls to JPS. After that, all transactions were 
thoroughly tested with bucket movement, and Mini Sanity 
(for bulk testing of post-paid connections) was simulated 
to ensure proper performance,” says Gupta.
Open source campaign management
In addition to the customer onboarding system, Vodafone 
India has an open source-powered Campaign Management 
System that helps the telecom giant create unique offers and 
promotions for customers, based on data-led segmentation 
and real-time triggers including recharges.
The offers made through the Campaign Management 
System are available to customers at certain interventions. The 
system lists promotional activities when a customer contacts 
the operator for service needs. Also, the system circulates 
pre-defined real-time triggers. “These offers not only help 
Vodafone India to grow revenues but also let customers enjoy 
a better and more personalised experience,” asserts Bhardwaj.
Proprietary vs open source solutions
The IT engineers at Vodafone India make certain value-
based evaluations to pick the right mix between proprietary 
technologies and open source software. “There are parameters 
including the technological edge, support, the roadmap and 
costs that help us decide between commercial and open 
source options,” says Gupta.
Having said that, the team focuses more on open source. 
“As a principle, we are committed to moving towards using 
open source, thereby reducing vendor lock-in to the extent 
possible,” says Bhardwaj.
Need for local competencies among Indian 
systems integrators
While open source is liberating the telco from the licensing 
needed for proprietary solutions, the systems integrators 
in India also need to create their own competencies and 
contribute to the community. This would help to reduce the 
monopoly of a few support providers in the country.
“Open source adoption is no more a choice to be made 
in the future. It is happening now and here,” Bhardwaj 
emphasises. 
system. Internally called Customer Point of Sale or CPOS, 
the technology has been moved from proprietary Websphere 
Process Server (WPS) to a Drools-based custom workflow 
solution. The CPOS solution by Vodafone India helps in 
acquiring new customers and offers efficient inventory 
management. “Our CPOS is effectively handling the smallest 
of performance dips and availability issues that impact the 
market directly, often leading to irrecoverable scenarios like a 
customer moving away to other avenues,” explains Abhilekh 
Bhardwaj, head of IT delivery excellence, Vodafone India.
Apart from easing customer acquisition, Bhardwaj states 
that the open source code used in CPOS enables engineers to 
scale the application and increase its resilience.
Vodafone India is also leveraging the features of CPOS 
for various eKYC (know your customer) services that offer 
Aadhaar-based instant activation. “Open source adoption 
within our CPOS has doubled the performance of activating 
new prepaid, postpaid and enterprise customers, and has 
delivered higher stability,” Gupta comments. Today, the 
CPOS solution is accessed by all Vodafone-authorised 
distributors and retailers across the country. And it results 
in over 10 million activations per month.
WPS Replacement
The IT experts at Vodafone India accomplished the 
transformation project dubbed as ‘WPS Replacement’ in 
2014, wherein a WPS (Wi-Fi Protected Setup) backed bucket 
movement was replaced with a custom Java state machine.
Gupta points out that more than 10 critical transactions 
impacting over 40 bucket movements had to be redesigned 
By: Jagmeet Singh
The author is an assistant editor at OSFY.
Major open source solutions used at Vodafone India
• CPOS (Customer Point of Sale) – A customer on-
boarding application used for all types of new clients. 
It uses Apache Tomcat, BIRT, Java, JAXWS, Open 
JPA, Spring and Struts.
• UPSS – A system to support prepaid offers for over 
200 million customers. It uses Axis, Hibernate, Java, 
Spring and Struts.
• A campaign management system – An application to 
generate campaigns based on customer segmenta-
tion and value management. It uses Hadoop, Hive, 
Pig and Red Hat Enterprise Linux.
Key challenges involved in the WPS  
Replacement project
• Complex workflows, which were as per TRAI regula-
tions, had to be migrated from BPEL to Java Engine
• More than 10 critical transactions impacting over 40 
bucket movements had to be redesigned
• Java-based state machine (JPS) was hosted on a new 
server to ensure handling of high capacity activations
• Real-time communication between app server and 
JPS server
• Optimisation of EJB and Web service calls


102 | May 2017 | OPEN SOURCE FOR yOU | www.OpenSourceForU.com
OpenGurus
How To
The Beaglebone Black is a low-power open source single-board computer. eMMC is 
short for ‘embedded multi media controller’ which refers to a package integrating 
Flash memory and a memory controller in the same silicon die.  In this article, we 
flash the eMMC on to the Beaglebone Black using an SD card. 
Beaglebone Black:  
Flashing eMMC Through an SD Card
the trick. The best option is not to use the MMC1_CLK and 
MMC1_CMD signals at all and tie them ‘LOW’.
 Note:  If the board is powered off without using 
the onboard power button, then there is a chance of 
data corruption in the eMMC. The data in the eMMC 
can be restored by re-flashing the board. It is always 
recommended to use the latest version of the software.
Steps to flash the board
To flash the board, you need an 
SD card with a capacity of 4GB or 
more. Using an SD card smaller 
than 4GB will not boot the board. 
Before starting, do remember to take 
a backup of all the contents of the 
SD card that you are going to use, as 
flashing will erase them completely. 
Now, ensure to download the latest 
images for the Beaglebone Black 
from the official Beagleboard 
website (Figure 2).
The image can also be directly 
downloaded from the system by using the following command:
$ wget -c http://debian.beagleboard.org/images/BBB-eMMC-
T
he Beaglebone Black Rev C features a Sitara 
AM3358BZ100, 1GHz processor and 512MB DDR3L 
RAM. Its significant features are lower costs, and an 
increase in both performance and memory size. In this low 
power device, EEPROM has been reduced from 32KB to 
4KB, so that the cost falls. The board features a 4GB NAND 
(eMMC), and has an HDMI and a USB interface. In addition, 
it has a GPIO3_21 with a 24.576MHz clock. It also carries 
a microSD card. On-board Flash memory of 2GB acts as the 
hard drive for the board to host a Linux operating system and 
other software development tools. 
With a user-friendly, browser-based Bonescript 
programming environment called Cloud9, a learner can 
easily program the Beaglebone Black (BBB) board to rapidly 
prototype electronic systems that interface with real-world 
applications. As user knowledge develops, the board provides 
more complicated interfaces including C/C++ functions to 
access digital and analogue pins aboard the ARM Cortex A8 
microprocessor. The full power and capability of the BBB 
board can be programmed in the underlying onboard Linux 
operating system, such as Angstrom or Ubuntu. 
Onboard eMMC
The onboard eMMC uses MMC1 signals.  It is also 
connected to the expansion header. The eMMC device on 
the Beaglebone Black will need to be disabled by activating 
the reset line to this device. To do this, you have to write to 
eMMC first and instruct it to enable the reset. At this point 
in time, it is not totally clear whether the reset line will do 
Figure 1: Beaglebone Black
Figure 2: Latest image source

www.OpenSourceForU.com | OPEN SOURCE FOR yOU | May 2017 | 103
OpenGurus
How To
Figure 3: Command showing mounting of the SD card
By: S. Pardhu Pavan 
The author currently works at AdeptChips Pvt Ltd, 
Bengaluru, as an embedded systems engineer in the 
areas of communications and Internet enabled systems. 
He is an open source enthusiast, and has experience in C 
programming and device driver development.
[1]  https://en.wikipedia.org/wiki/BeagleBoard
[2]  http://derekmolloy.ie/
[3] http://tinypic.com/1r55gh7o
References
Figure 4: Copying the image to the SD card
Figure 5: After the SD card is inserted, the board starts to flash
Figure 6: LED status after flashing
The above command is shown in Figure 3.
In this case, I have created a mount point bbb_flashing and 
have mounted the SD card. You can go to Devices to confirm 
whether the SD card has mounted properly or not.
I am going to use the dd command to copy the image file 
to the SD card. Figure 4 shows the image files being copied to 
the SD card, in my case.
$sudo dd if=/<path/to/image/file> of=/dev/<SD Cardname>
After the image has been successfully copied on to the SD 
card, unmount the SD card and insert it into the board, which 
is in the switched off state.
The command used for unmounting the SD 
card is as follows: 
$sudo umount /dev/(Device name) Eg: sdb1
We have to provide an external power supply by holding 
switch2 on the board. The SD card present in the board 
functions and tries to flash the board, as shown in Figure 5.
As soon as the LEDs present on the board begin to flash, 
you can release switch2.
It may take around 45 minutes to one hour to flash the 
board. When the LEDs present on the board stop blinking 
and are ON, the board has been flashed completely. In 
the screenshot (Figure 6), we can see the LEDs glowing 
constantly after the completion of the flashing process.
As soon as the flashing is over, disconnect the power and 
try restarting the board by pressing the S2 button.
Finally, press S3 to power up the board and you will have 
the latest image installed on it.
Beaglebone Black is used in robotics, motor drivers, 
Twitter printers, data backups, SDR base stations, USB data 
acquisition and more. It offers great customisation, speed 
and MATLAB compatibility. It runs Simulink models as 
standalone applications and the off-course JTAG debugger. 
The incredible number of pins on the Beaglebone Black and 
the many bus options allow users to easily interface it with 
pretty much any device out there. 
flasher-debian-7.5-2014-05-14-2gb.img.xz
After the image is downloaded, unzip it by using the 
following command:
$ xz -d BBB*.xz
In the above command, BBB* denotes the full name of the 
downloaded image.
After unzipping the downloaded image, mount the SD 
card and copy the image on to it.
The command used to mount the SD card is given below:
$ sudo mount /dev/<Device name> E.g. sdb. 

TIPS
TRICKS
&
Record audio from your microphone via 
the command line
First, download the 'sox' tool suite which is extremely 
versatile for audio file manipulation. Use the following 
command on Debian systems:
# apt-get install sox libsox-fmt-all
'libsox-fmt-all' package enables playback of mp3 files.
The 'sox' suite includes two tools called 'rec' and 'play'.
To record audio, insert a microphone into your audio 
jack and type the following command:
# rec -c 1 test_file.mp3
The recording session begins and whatever you speak 
through the microphone is recorded. Once you are done 
recording, press Ctrl+C.
To play the recorded file, type the following:
#play test_file.mp3
'sox' can also handle audio files of formats like 
wav, ogg, etc. 
—Divya Lakshmanan, 
divya.lakshmanan27@gmail.com
Download videos using the YouTube 
downloader in Linux
First, install youtube-dl by following the steps given below.
If you are running an Ubuntu based Linux distribution, 
you can install it using the following:
sudo apt-get install youtube-dl
For any other Linux distro, you can quickly install 
youtube-dl on your system through the command line 
interface with the following command:
sudo wget https://yt-dl.org/downloads/latest/youtube-dl -O/usr/
local/bin/youtube-dl
After fetching the file, you need to set an executable 
permission on the script for it to execute properly.
sudo chmod a+rx /usr/local/bin/youtube-dl
To download a video file, simply run the following 
command, where ‘VIDEO_URL’ is the URL of the 
video that you want to download.
youtube-dl <VIDEO_URL>
You can also download in multiple formats. Run 
the commands below to discover all that can be done 
using this tool:
youtube-dl -h
…or: 
man youtube-dl
—Mounica Revuru, ojaswithamonica@gmail.com
Tips for day-to-day use
Given below are a few handy tips for those 
who use Linux daily.
Resume typing a command after a few others
Sometimes, after typing a long and complex command 
on a Linux terminal, we realise that we have to type 
some other commands, and even examine their results, 
before we hit Enter. Instead of discarding what we have 
typed, or opening another terminal/GUI text editor to 
store it, we can just go to the end of our typed line by 
using Ctrl E, and press Ctrl U to clear the line. Then, we 
can execute the intermediate command and just press 
Ctrl Y. Voila! The deferred command reappears for us 
to modify or execute with the use of Enter. Think of it 
as a kind of cut-and-paste for the contents of the line 
preceding the cursor. 
Docker emergency shutdown
As of release 1.12.1, Docker does not have a single 
command to stop all running containers. But here 
104 | May 2017 | OPEN SOURCE FOR yOU | www.OpenSourceForU.com

is a hack to do that:
docker stop $(docker ps -q)
You may have to use sudo before docker both times 
in the command. This can save you from individually 
stopping dozens of containers before a system shutdown. 
Swing between ‘smart quotes’ and ‘straight quotes’ in 
LibreOffice
Those who have been hunting around for a way to 
quickly use both smart and straight quotes on any PC running 
LibreOffice or OpenOffice, here’s the way to go about it.
Turn on Smart quotes (if turned off) using Tools » 
AutoCorrect Options » Localized Options.
Hit Ctrl+z (or Undo) to turn back curly quotes to 
straight quotes following each individual insertion.
I tested this on LibreOffice, and expect equal success 
with OpenOffice.
—A. Datta, webmaster@aucklandwhich.org
Adding the time stamp to the History 
command
The History command does not output the time stamp with 
the log of the most recently executed commands.
To do so, run the following command in the terminal:
# HISTTIMEFORMAT=”%d/%m/%y %T “
# history
If you want to permanently append this change, add the 
following line to the  ~/.bashrc file:
# export HISTTIMEFORMAT=”%d/%m/%y %T “ 
Then, from the terminal, run the following:
# source ~/.bashrc
Here’s an explanation of the commands and switches:
history – GNU History library
HISTIMEFORMAT – Environmental variable
%d – Day
%m – Month
%y – Year
%T – Time stamp
source – In short, send the contents of the file to the shell
.bashrc – Is a shell script that BASH runs whenever it 
is started interactively 
—Rajeeb Senapati, rajeeb.koomar@gmail.com
Add a welcome message to your terminal
This tip will show you how you can easily add a 
welcome message to your terminal. This message will be 
displayed at the start, whenever you open your terminal.
Here are the steps to follow:
1. You need to install figlet. So open the terminal and type 
the following command:
sudo apt-get install figlet
Then press Enter.
2. Open .bashrc in the Vim editor and press ‘’’ to get into 
Insert mode.
vi .bashrc
3. Enter the following command in .bashrc:
figlet -c your_message
Press the Esc button; then press colon x and Enter.
4.  Now, when you open your terminal, you can see your 
welcome message. 
Here is another tip that I want to share.
Remove Amazon results from Dash in Ubuntu
Ubuntu added Amazon search results into the Unity Dash 
a few releases ago. While many people complained that 
their privacy was at risk since every search was being sent 
to Amazon’s servers, I personally just found the Amazon 
results unnecessary.
To remove the Amazon search results from the Dash, 
run the following command:
sudo apt-get autoremove unity-lens-shopping  
…and restart. This will get rid of the Dash lens that is 
responsible for those results.
Alternatively, you can also go into the System Settings 
–> Security & Privacy, and disable Online Search Results. 
Do note that this toggle will affect not just the Amazon 
search results, but also any other Dash lenses that require 
the Internet to function. 
—Harish Tiwari, 
harishtiwary46@gmail.com
Share Your Linux Recipes!
The joy of using Linux is in finding ways to get around 
problems—take them head on, defeat them! We invite you 
to share your tips and tricks with us for publication in OSFY 
so that they can reach a wider audience. Your tips could be 
related to administration, programming, troubleshooting or 
general tweaking. Submit them at www.opensourceforu.
com. The sender of each published tip will get a T-shirt.
www.OpenSourceForU.com | OPEN SOURCE FOR yOU | May 2017 | 105

106 | May 2017 | OPEN SOURCE FOR yOU | www.OpenSourceForU.com
DVD Of The MOnTh
Here’s the latest version of Ubuntu for your desktop.
In c
ase 
this
 DV
D do
es n
ot w
ork 
prop
erly,
 wri
te t
o us
 at s
upp
ort@
efy.i
n fo
r a f
ree 
repl
ace
men
t.
CD T
eam
 e-m
ail: 
cdtea
m@
efy.i
n
Rec
omm
ende
d Sy
stem
 Req
uire
ment
s: P
4, 1
GB R
AM,
 DV
D-RO
M Dr
ive
May 2017
17.04 
Live
What is a live DVD?
A live CD/DVD or live disk contains a bootable 
operating system, the core program of any computer, 
which is designed to run all your programs and manage 
all your hardware and software.
Live CDs/DVDs have the ability to run a complete, 
modern OS on a computer even without secondary 
storage, such as a hard disk drive. The CD/DVD directly 
runs the OS and other applications from the DVD drive 
itself. Thus, a live disk allows you to try the OS before 
you install it, without erasing or installing anything on 
your current system. Such disks are used to demonstrate 
features or try out a release. They are also used for 
testing hardware functionality, before actual installation. 
To run a live DVD, you need to boot your computer 
using the disk in the ROM drive. To know how to set 
a boot device in BIOS, please refer to the hardware 
documentation for your computer/laptop.
OSFY DVD
Ubuntu 17.04 Desktop
Ubuntu is used on the smallest of devices to the largest 
cloud server. All the essential applications, like an office 
suite, browsers, email and media apps come pre-installed, 
and thousands of games and applications are available in 
the Ubuntu Software Centre. With its 17.04 release, Ubuntu 
switches from swap partitions to using swap files, by default.  
Lubuntu 17.04
Lubuntu is an official Ubuntu flavour based on the 
Lightweight X11 Desktop Environment (LXDE). Lubuntu 
specifically targets older machines with lower resources, but 
also runs very well on newer hardware. The bundled DVD 
comes with the ISO image of the 32-bit edition.
KDE Neon 
This is the latest and supposedly greatest of the KDE 
community’s software packaged on a rock-solid base. Most 
users will want to use the packages built from released 
software, which will be available soon. KDE Neon will 
provide users with more up-to-date packages of Qt and 
cutting-edge KDE software.
Rocksor 3.9.0
You can build and manage your own Linux and BTRFS 
powered advanced NAS and cloud storage with ease—
Rocksor 3.9.0 provides a solid platform and empowers users 
to deploy effective storage solutions.




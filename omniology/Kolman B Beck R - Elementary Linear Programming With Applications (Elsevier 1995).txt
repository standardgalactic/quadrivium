Elementary Linear Programming with 
Applications 
by Bernard Kolman, Robert E. Beck 
 
 
 
• Textbook Hardcover - REV 
• ISBN: 012417910X; ISBN-13: 9780124179103 
• Format: Textbook Hardcover, 449pp 
• Publisher: Elsevier Science & Technology Books 
• Pub. Date: June 1995 
 

Preface 
Classical optimization techniques have been widely used in engineering 
and the physical sciences for a long time. They arose from attempts to 
determine the "best" or "most desirable" solution to a problem. Toward 
the end of World War II, models for many problems in the management 
sciences were formulated and algorithms for their solutions were devel- 
oped. In particular, the new areas of linear, integer, and nonlinear pro- 
gramming and network flows were developed. These new areas of applied 
mathematics have succeeded in saving billions of dollars by enabling the 
model builder to find optimal solutions to large and complex applied 
problems. Of course, the success of these modem optimization techniques 
for real problems is due primarily to the rapid development of computer 
capabilities in the past 40 years. Computational power has doubled every 
12 months since 1964 (Moore's Law, Joy's Law) allowing the routine 
solution today of problems whose complexity was overwhelming even a few 
years ago. 
With the increasing emphasis in mathematics on relevance to real-world 
problems, some of the areas of modem optimization mentioned above 
xi 

xii 
Preface 
have rapidly become part of the undergraduate curriculum for business, 
engineering, computer science, and mathematics students. 
This book presents a survey of the basic ideas in linear programming 
and related areas and is designed for a one-semester or one-quarter course 
that can be taken by business, engineering, computer science, or mathe- 
matics majors. In their professional careers many of these students will 
work with real applied problems; they will have to formulate models for 
these problems and obtain understandable numerical answers. Our pur- 
pose is to provide such students with an impressive illustration of how 
simple mathematics can be used to solve difficult problems that arise in 
real situations and to give them some tools that will prove useful in their 
professional work. 
A significant change that has taken place in the general teaching of this 
course has been the introduction of the personal computer. This edition 
takes due cognizance of this new development. 
WHAT IS NEW IN THE SECOND EDITION 
We have been very pleased by the widespread acceptance of the first 
edition of this book since its publication 15 years ago. Although many 
changes have been made in this edition, our objectives remain the same as 
in the first edition: to provide a textbook that is readable by the student, 
presents the basic notions of linear programming, and illustrates how this 
material is used to solve, some very important problems that arise in our 
daily lives. To achieve these objectives we have made use of many faculty 
and student suggestions and have developed the following features for this 
edition. 
FEATURES 
9 Some more review material on linear algebra has been added in 
Chapter 0. 
9 Chapters 1 and 2 of the first edition have been modified. In the 
revised Chapters 1 and 2, the material on the Extreme Point Theo- 
rem, basic solutions, and the Duality Theorem are now presented in 
separate sections. Moreover, the important elementary aspects of 
linear programming and its applications are covered more quickly and 
more directly. 
9 In Chapter 3, the presentation of the Duality Theorem has been 
rewritten, and now appears as Section 3.2. 
9 In Chapter 5, the presentations of the transportation problem, assign- 
ment problem, and maximal flow problem have been rewritten for 
greater clarity. 

Preface 
xiii 
9 New exercises have been added. 
9 New figures have been added. 
9 Throughout the book, the material on computer aspects has been 
updated. 
9 A computer disk containing the student-oriented linear programming 
code SMPX, written by Professor Evar D. Nering, Arizona State 
University, to be used for experimentation and discovery, is included 
with the book. Its use is described in Appendix C. 
9 Appendix A, new to this edition, provides a very elementary introduc- 
tion to the basic ideas of the Karmarkar algorithm for solving linear 
programming problems. 
9 Appendix B has been added to this edition to provide a guide to some 
of the inexpensive linear programming software available for personal 
computers. 
PRESENTATION 
The Prologue gives a brief survey of operations research and discusses 
the different steps in solving an operations research problem. Although we 
assume that most readers have already had some exposure to linear 
algebra, Chapter 0 provides a quick review of the necessary linear algebra. 
The linear algebra requirements for this book are kept to a minimum. 
Chapter 1 introduces the linear programming problem, provides examples 
of such a problem, introduces matrix notation for this problem, and 
discusses the geometry of linear programming problems. Chapter 2 pre- 
sents the simplex method for solving the linear programming problem. 
Chapter 3 covers further topics in linear programming, including duality 
theory and sensitivity analysis. Chapter 4 presents an introduction to 
integer programming, and Chapter 5 discusses a few of the more important 
topics in network flows. 
The approach in this book is not a rigorous one, and proofs have been 
kept to a minimum. Each idea is motivated, discussed, and carefully 
illustrated with examples. The first edition of this book is based on a 
course developed by one of us (Bernard Kolman) under a College Science 
Improvement Program grant from the National Science Foundation. 
EXERCISES 
The exercises in this book are of three types. First, we give routine 
exercises designed to reinforce the mechanical aspects of the material 
under study. Second, we present realistic problems requiring the student to 
formulate a model and obtain solutions to it. Third, we offer projects, 

xiv 
Preface 
some of which ask the student to familiarize himself or herself with those 
journals that publish papers in the subject under study. Most of the 
projects are realistic problems, and they will often have some vagueness in 
their statement; this vagueness must be resolved by the student as he or 
she formulates a model. 
COMPUTERS 
The majority of students taking this course will find that after having 
solved a few linear programming problems by hand, they will very much 
appreciate being able to use a computer program to solve such problems. 
The computer will reduce the computational effort required to solve linear 
programming problems and will make it possible to solve larger and more 
realistic problems. In this regard, the situation is different from when the 
first edition of this book appeared. Nowadays, there are inexpensive 
programs that will run on modest personal computers. A guide to some of 
these is provided in Appendix B. Moreover, bound with this book is a disk 
containing the program SMPX, developed by Evar D. Nering, Arizona 
State University, as courseware for a typical course in linear programming. 
This courseware allows the student to experiment with the simplex method 
and to discover the significance of algorithm choices. 
Complementing SMPX courseware is LINDO, an inexpensive and pow- 
erful software package designed to solve linear programming problems. It 
was first developed in 1983 and is now available in both PC and Macintosh 
versions. 
The final sections in each of Chapters 3, 4 and 5 discuss computer 
aspects of the material in the chapter. These sections provide an introduc- 
tion to some of the features available in the linear programming codes 
used to solve large real problems and an introduction to the considerations 
that enter into the selection of a particular code. 

Acknowledgments 
We gratefully acknowledge the contributions of the following people 
whose incisive comments greatly helped to improve the manuscript for the 
second edition. 
Wolfgang Bein~University of New Mexico 
Gerald Bergum~South Dakota State University 
Joseph Creegan~Ketron Management Science 
Igor Faynberg~AT & T Bell Laboratories 
Fritz Hartmann~Villanova University 
Betty Hickman~University of Nebraska at Omaha 
Ralph Kallman~Ball State University 
Moshe Kam~Drexel University 
Andr6 K6zdy~University of Louisville 
David Levine~Drexel University 
Michael Levitan~Villanova University 
Anany Levitin~Villanova University 
Douglas McLeod~Philadelphia Board of Education and Drexel University 

7131[ 
Acknowledgments 
Jeffrey PopyackmDrexel University 
Lev Slutsman--AT & T Bell Laboratories 
Kurt Spielberg--IBM Corporation 
Walter StromquistmWagner Associates 
Avi VardimDrexel University 
Ron WatrowMitre Corporation 
Mark Wiley~Lindo Systems 
We thank the students in N655 at Drexel University who, working in 
teams, found the solutions to all the problems, and the many students 
throughout North America and Europe who used the first edition of the 
text in their class and provided feedback to their instructors about the 
quality of the explanations, examples, and exercises. We thank professor 
Evar D. Nering who graciously tailored the SMPX system to our require- 
ments. We also thank Beth Kayros, Villanova University, who checked the 
answers to all odd-numbered exercises, and Stephen M. Kolman, Univer- 
sity of Wisconsin, who carefully prepared the extensive index. Finally, 
thanks are also due to Peter Renz and Craig Panner of Academic Press for 
their interest, encouragement, and cooperation. 

Table of Contents 
 
Preface 
 
Acknowledgments 
 
Prologue 
0 
Review of Linear Algebra (Optional) 
1 
Introduction to Linear Programming 
2 
The Simplex Method 
3 
Further Topics in Linear Programming 
4 
Integer Programming 
5 
Special Types of Linear Programming Problems
 
Appendix A: Karmarkar's Algorithm 
 
Appendix B: Microcomputer Software 
 
Appendix C: SMPX 
 
Answers to Odd-Numbered Exercises 
 
Index 
 
 

Prologue 
Introduction to 
Operations 
Research 
WHAT IS OPERATIONS RESEARCH? 
Many definitions of operations research (frequently called OR) have 
been given. A common thread in these definitions is that OR is a scientific 
method for providing a quantitative basis for decision making that can be 
used in almost any field of endeavor. The techniques of OR give a logical 
and systematic way of formulating a problem so that the tools of mathe- 
matics can be applied to find a solution. However, OR differs from 
mathematics in the following sense. Most often mathematics problems can 
be clearly stated and have a specific answer. OR problems are frequently 
poorly posed: they arise when someone has the vague feeling that the 
established way of doing things can be improved. Engineering, which is 
also engaged in solving problems, frequently uses the methods of OR. A 
central problem in OR is the optimal allocation of scarce resources. In this 
context, scarce resources include raw materials, labor, capital, energy, and 
processing time. For example, a manufacturer could consult an operations 
research analyst to determine which combination of production techniques 
o, 
XVll 

xviii 
Prologue 
should be used to meet market demands and minimize costs. In fact, the 
1975 Nobel Prize in Economics was awarded to T. C. Koopmans and L. V. 
Kantorovich for their contributions to the theory of optimum allocation of 
resources. 
DEVELOPMENT OF OPERATIONS RESEARCH 
The use of scientific methods as an aid in decision making goes back a 
long time, but the discipline that is now called operations research had its 
birth during World War II. Great Britain, which was struggling for its very 
existence, gathered a number of its top scientists and mathematicians to 
study the problem of allocating the country's dwindling resources. The 
United States Air Force became interested in applying this new approach 
to the analysis of military operations and organized a research group. In 
1947 George B. Dantzig, a member of this group, developed the simplex 
algorithm for solving linear programming problems. At approximately the 
same time the programmable digital computer was developed, giving a 
means of solving large-scale linear programming problems. The first solu- 
tion of a linear programming problem on a computer occurred in 1952 on 
the National Bureau of Standards SEAC machine. The rapid development 
of mathematical programming techniques has paralleled the rapid growth 
of computing power. The ability to analyze large and complicated prob- 
lems with operations research techniques has resulted in savings of billions 
of dollars to industry and government. It is remarkable that a newly 
developed discipline such as operations research has had such an impact 
on the science of decision making in such a short time. 
PHASES OF AN OPERATIONS RESEARCH STUDY 
We now look at the steps an operations analyst uses in determining 
information for decision making. In most cases the analyst is employed as 
a consultant, so that management has to first recognize the need for the 
study to be carried out. The consultant can now begin work using the 
following sequence of steps. 
Step 1: Problem definition and formulation. In this phase the goal of 
the study is defined. The consultant's role at this point is one of 
helping management to clarify its objectives in undertaking the 
study. Once an acceptable statement of the goals has been 
made, the consultant must identify the decision alternatives. It 
is likely that there are some options that management will 
refuse to pursue; thus, the consultant will consider only the 

Prologue 
xix 
Step 2: 
Step 3: 
Step 4." 
Step 5." 
Step 6." 
alternatives acceptable to management. Attention must also be 
paid to the limitations, restrictions, and requirements of the 
various alternatives. For example, management might have to 
abide by fair employment laws or antipollution laws. Some 
alternatives may be limited by the available capital, labor, or 
technology. 
Model construction. The consultant now develops the appropri- 
ate mathematical description of the problem. The limitations, 
restrictions, and requirements must be translated into mathe- 
matical terms, which then give rise to the constraints of the 
problem. In many cases the goal of the study can be quantified 
as an expression that is to be maximized or minimized. The 
decision alternatives are represented by the variables in the 
problem. Often the mathematical model developed is one that 
has a familiar form and for which methods of solution are 
available. 
Solution of the model. The mathematical model developed in 
Step 2 must now be solved. The method of solution may be as 
simple as providing the input data for an available computer 
program or it may call for an investigation of an area of 
mathematics that so far has not been studied. There may be no 
method of finding a solution to the mathematical model. In this 
case the consultant may use heuristic methods or approximate 
methods, or it may be necessary to go back to Step 2 and modify 
the model. It should be noted that the solution to the model 
need not be the solution to the original problem. This will be 
further discussed below. 
Sensitivity analysis. Frequently the numbers that are given to 
the consultant are approximate. Obviously, the solution depends 
on the values that are specified for the model, and, because 
these are subject to variation, it is important to know how the 
solution will vary with the variation in the input data. For 
standard types of models these questions have been investi- 
gated, and techniques for determining the sensitivity of the 
solution to changes in the input data are available. 
Model evaluation. At this point the consultant has obtained a 
solution to the model, and often this solution will represent a 
solution to the given problem. The consultant must determine 
whether the answers produced by the model are realistic, ac- 
ceptable to management, and capable of implementation by 
management. As in Step 1, the consultant now needs a thorough 
understanding of the nature of the client's business. 
Implementation of the study. Management must now decide 
how to implement the recommendations of the consultant. 

~I~ 
Prologue 
Sometimes the choice is to ignore all recommendations and do 
something that is politically expedient instead. 
THE STRUCTURE OF MATHEMATICAL MODELS 
When a technical person discusses a model of a situation that is being 
studied, he or she is referring to some idealized representation of a 
real-life system. The model may simply involve a change in scale, such as 
the hobbyist's HO railroad or the architect's display of a newly planned 
community. 
Engineers often use analogue models in which electrical properties 
substitute for mechanical properties. Usually the electrical analogues are 
much easier to deal with than the real objects. For example, resetting a 
dial will change the analogue of the mass of an object. Without the 
analogue one might have to saw off part of the object. 
Mathematical models represent objects by symbols. The variables in the 
model represent the decision alternatives or items that can be varied in the 
real-life situation. There are two types of mathematical models: determin- 
istic and probabilistic. Suppose the process described by the model is 
repeated many times. A deterministic model will always yield the same set 
of output values for a given set of input values, whereas a probabilistic 
model will typically yield many different sets of output values according to 
some probability distribution. In this book we will discuss only determinis- 
tic models. 
The mathematical models that will be considered in this book are 
structured to include the following four basic components: 
(a) 
(b) 
(c) 
(d) 
Decision variables or unknowns. Typically we are seeking values for 
these unknowns, which describe an optimal allocation of the scarce 
resources represented by the model. For example, decision variables 
might represent purchase lot size, number of hours to operate a 
machine, or which of several alternatives to choose. 
Parameters. These are inputs that may or may not be adjustable by 
the analyst, but are known either exactly or approximately. For 
example, purchase price, rate of consumption, and amount of 
spoilage could all be parameters. 
Constraints. These are conditions that limit the values that the 
decision variables can assume. For example, a variable measuring 
units of output cannot be negative; a variable measuring the amount 
to be stored cannot have a value greater than the available capacity. 
Objective function. This expression measures the effectiveness of 
the system as a function of the decision variables. The decision 

Prologue 
1~1~[ 
variables are to be determined so that the objective function will be 
optimized. It is sometimes difficult to determine a quantitative 
measure of the performance of a system. Consequently, several 
objective functions may be tried before choosing one that will 
reflect the goals of the client. 
MATHEMATICAL TECHNIQUES IN OPERATIONS RESEARCH 
The area of mathematicalprogramming plays a prominent role in OR. It 
consists of a variety of techniques and algorithms for solving certain kinds 
of mathematical models. These models call for finding values of the 
decision variables that maximize or minimize the objective function subject 
to a system of inequality and equality constraints. Mathematical program- 
ming is divided into several areas depending on the nature of the con- 
straints, the objective function, and the decision variables. Linear program- 
ming deals with those models in which the constraints and the objective 
function are linear expressions in the decision variables. Integer program- 
ming deals with the special linear programming situation in which the 
decision variables are constrained to take nonnegative integer values. In 
stochastic programming the parameters do not have fixed values but are 
described by probability distributions. In nonlinear programming some or 
all of the constraints and the objective function are nonlinear expressions 
in the decision variables. Special linear programming problems such as 
optimally assigning workers to jobs or optimally choosing routes for 
shipments between plants and warehouses have individually tailored algo- 
rithms for obtaining their solutions. These algorithms make use of the 
techniques of network flow analysis. 
Special models, other than mathematical programming techniques, have 
been developed to handle several important OR problems. These include 
models for inventory analysis to determine how much of each item to keep 
on hand, for analysis of waiting-line situations such as checkout counters 
and tollbooths, and for competitive situations with conflicting goals such as 
those that would arise in a game. 
Standard techniques for solving many of the usual models in OR are 
available. Some of these methods are iterative, obtaining a better solution 
at each successive iteration. Some will produce the optimal solution after a 
finite number of steps. Others converge only after an infinite number of 
steps and consequently must be truncated. Some models do not lend 
themselves to the standard approaches, and thus heuristic techniques--that 
is, techniques improvised for the particular problem and without firm 
mathematical basis--must be used. 

xxii 
Prologue 
Further Reading 
Gale, D. The Theory of Linear Economic Models. McGraw-Hill, NY, 1960. 
Maki, D. P., and Thompson, M. Mathematical Models and Applications. Prentice-Hall, 
Englewood Cliffs, NJ, 1973. 
Roberts, F. S. Discrete Mathematical Models, with Applications to Social, Biological, and 
Environmental Problems. Prentice-Hall, Englewood Cliffs, NJ, 1976. 
Journals 
Computer and Information Systems Abstracts Journal 
Computer Journal 
Decision Sciences 
European Journal of Operational Research 
IEEE Transactions on Automatic Control 
Interfaces 
International Abstracts in Operations Research 
Journal of Computer and System Sciences 
Journal of Research of the National Bureau of Standards 
Journal of the ACM 
Journal of the Canadian Operational Research Society 
Management Science (published by The Institute for Management 
SciencemTIMS) 
Mathematical Programming 
Mathematics in Operations Research 
Naval Research Logistics (published by the Office of Naval 
Research--ONR) 
Operational Research Quarterly 
Operations Research (published by the Operations Research Society of 
AmericamORSA) 
Operations Research Letters 
OR/MS Today 
ORSA Journal on Computing 
SlAM Journal on Computing 
Transportation Science 
Zeitschrifi ftir Operations Research 

Review of 
Linear Algebra 
(Optional) 
W 
E ASSUME MOST readers of this book have already had some 
exposure to linear algebra. We expect that they have learned 
what a matrix is, how to multiply matrices, and how to tell 
whether a set of n-tuples is linearly independent. This chapter provides a 
quick review of the necessary linear algebra material for those readers who 
wish it. The chapter can also serve as a reference for the linear algebra 
encountered later in the text. Exercises are included in this chapter to give 
the student an opportunity to test his or her comprehension of the 
material. 
0.1 MATRICES 
DEFINITION. 
An 
m x n matrix A is a rectangular array of 
mn 
numbers (usually real numbers for linear programming) arranged in 

Chapter 0 
Review of Linear Algebra (Optional) 
m horizontal rows and n vertical columns: 
all 
a12 
... 
aln 
a21 
a22 
... 
a2n 
A = 
. 
. 
. 
. 
(1) 
9 
. 
aml 
am2 
9 .. 
amn 
The i th row of A is 
[ail 
ai2 
"'" 
ain] 
(1 < i < m). 
The j th column of A is 
alj 
a2j 
(1 < j _< n). 
amj 
The number in the ith row and jth column of A is denoted by a u, and is 
called the ijth element of A, or the (i, j) entry of A, and we often write (1) 
as 
A = [a/j]. 
The matrix A is said to be square of order n if m - n. In this case, the 
numbers aaa, a22,..., ann form the main diagonal of A. 
EXAMPLE 1. 
If 
l 
i] 
A-- 
3 
4 , 
B= 
3 
-2 
1 
and 
C= 
-1 
2 
4 
2 
4 
3 
5 ' 
2 
4 ' 
then A is 3 x 2, B is 2 x 3, and C is square of order 2. Moreover, a21 = 3, 
a32 = 2, b23 = 5, and C2a -- --2. 
A 
DEFINITION. 
Two m X n matrices A -[aij] 
and B- [bij] are said to 
be equal if aij-bij 
for each choice of i and j, where l_<i_<m, 
l<_j<_n. 
We now turn to the definition of several operations on matrices. These 
operations will enable us to use matrices as we discuss linear programming 
problems. 
DEFINITION. 
If A = [aij] and B = [bij] are m x n matrices, then the 
sum of A and B is the matrix C = [c i j], defined by 
cij=aij+bij 
(1 <i<m,1 
<j<n). 
That is, C is obtained by adding corresponding entries of A and B. 

0.1 Matrices 
EXAMPLE 2. 
Let 
2 
-3 
A= 
5 
1 
then 
4 ] 
and 
-2 
.=[ 3 3 2] 
-2 
2 
4 " 
[2+3 
-3+3 
4+2] 
[5 
0 
6] 
A+B= 
5+(-2) 
1+2 
-2+4 
= 
3 
3 
2 
" 
/x 
Properties of Matrix Addition. 
(a) A+B=B+A 
(b) A+(B+C)=(A+B)+C 
(c) There is a unique m • n matrix 0, called the m x n zero matrix, such 
that 
A+0=A 
for anym • 
(d) For each m • n matrix A, there is a unique matrix, denoted by -A, 
such that 
A+ (-A) =0. 
The matrix -A is called the negative of A. The ijth element of -A is 
-aq, where A = [aq]. 
DEFINITION. 
If A - [a i j] is an m • p matrix and B - [bij] is a p • n 
matrix, then the product of A and B is the m x n matrix C - [cii], defined 
by 
cq = ailblj + aizb2j + ... +aipbpi 
(l<i_<m,l_<j<n). 
(2) 
EXAMPLE 3. 
If 
1 
3 
-2] 
and 
B = 
A= 
2 
4 
3 
-2 
4] 
3 
-3 
, 
2 
1 
then 
1(-2) + 3.3 + (-2). 2 
AB= 
2(-2)+4 
3+3.2 
3 
-7 
-[14 
1]" 
1-4+3(-3)+(-2).1] 
2 4+4(-3)+3 
1 
] 
A 
Matrix multiplication requires more care than matrix addition. The 
product All can be formed only if the number of columns of A is the same 
as the number of rows of B. Also, unlike multiplication of real numbers, 
we may have AB = 0, the zero matrix, with neither A = 0 nor B = 0, and 
we may have All = AC without B = C. Also, if both A and B are square of 
order n, it is possible that AB 4= BA. 

Chapter 0 Review of Linear Algebra (Optional) 
We digress for a moment to recall the summation notation. When we 
write 
we mean 
n 
~~ ai, 
i=1 
a I + a 2 + ... +a n . 
The letter i is called the index of summation; any other letter can be used 
in place of it. Thus, 
(i) 
(ii) 
(iii) 
n 
n 
n 
Eai = Eaj = Ear. 
i=1 
j=l 
r=l 
The summation notation satisfies the following properties: 
n 
n 
n 
E (ri + si)ai = Eriai + Esiai 
i=1 
i=1 
i=1 
n 
n 
E cai = c E ai 
i=1 
i=1 
m 
n 
n 
m 
E 
Eaij = E 
Eaij 9 
i=lj=l 
j=l i=l 
Using the summation notation, Equation (2) for the (i, j) entry of the 
product All can be written as 
p 
Cij-- E aikbky 
k=l 
(1 <i<m,1 
<j<n). 
Properties of Matrix Multiplication. 
(a) A(BC) = (AB)C 
(b) A(B+C)-AB+AC 
(c) (A+B)C =AC+BC 
DEFINITION. The n x n matrix In, all of whose diagonal elements are 
1 and the rest of whose entries are zero, is called the identity matrix of 
order n. 
If A is an m X n matrix, then 
ImA = AI n = A. 
Sometimes the identity matrix is denoted by I when its size is unimportant 
or unknown. 

0.1 Matrices 
Linear Systems 
The linear system of m equations in n unknowns 
all x I -]- a12 x 2 -~- "" -~- aln Xn 
"- bl 
a21 x 1 --I- a22 x 2 -~- "- q- a2n x n 
-- b 2 
9 
o 
~ 
~ 
~ 
9 
o 
~ 
amlX 1 -F am2X 2 -t- "'" +amnX n 
-- b m 
(3) 
can be written in matrix form as follows. Let 
all 
a12 
... 
aln 
x1 
9 .. 
X 2 
A= 
a21 
a22 
a2n 
x= 
and 
b= 
aml 
am2 
"" 
amn 
Xn 
bl 
b2 
bm 
Then (3)can be written as 
Ax=b. 
The matrix A is called the coefficient matrix of the linear system (3), and 
the matrix 
[A 
' b] = 
all 
a12 
... 
aln 
a21 
a22 
... 
a2n 
aml 
am2 
.'. 
amn 
bl 
b2 
bm 
obtained by adjoining b to A, is called the augmented matrix of (3). 
Sometimes an augmented matrix may be written without the dashed line 
separating A and b. 
EXAMPLE 4. 
Consider the linear system 
3x- 2y + 4z + 5w = 6 
2x+3y-2z+ 
w=7 
x- 5y + 2z 
=8. 
The coefficient matrix of this linear system is 
A 
3 
-2 
4 
5 1 
2 
3 
-2 
1 , 
1 
-5 
2 
0 

6 
Chapter 0 Review of Linear Algebra (Optional) 
and the augmented matrix is 
3 
-2 
4 
5 
[A I b] = 
2 
3 
-2 
1 
1 
-5 
2 
0 6] 
7 , 
8 
where 
Letting 
b 
.__ 
X 
[2] 
x 
Y 
z 
w 
we can write the given linear system in matrix form as 
Ax=b. 
A 
Conversely, every matrix with more than one column can be considered 
as the augmented matrix of a linear system. 
EXAMPLE 5. 
The matrix 
3 
2 
416] 
-2 
5 
614 
is the augmented matrix of the linear system 
3x + 2y + 4z = 6 
2x + 5y + 6z = 4. 
A 
Scalar Multiplication 
DEFINITION. 
If A = [a i j] is an m X n matrix and r is a real number, 
then the scalar multiple of A by r, rA, is the m x n matrix B = [bij] , 
where bij = raij (1 < i < m, 1 _< j _< n). 
EXAMPLE 6. 
If r = --2 and 
A 
.__ 
2 
-3 
5] 
2 
4 
3 , 
0 
6 
-3 

O. 1 Matrices 
7 
then 
~A [ 
L 
-4 
6 
-10 
-4 
-8 
-6 
. 
0 
-12 
6 
Properties of Scalar Multiplication. 
(a) r(sA) = (rs)A 
(b) (r+s)A=rA+sA 
(c) r(A+ B)=rA+rB 
(d) A(rB) = r(AB) 
A 
The Transpose of a Matrix 
DEFINITION. 
If A = [a~j] is an m X n matrix, then the n x m matrix 
A T = [bq], where 
bq-aji 
(l <_i <_m,1 <_j <_n), 
is called the transpose of A. Thus, the transpose of A is obtained by merely 
interchanging the rows and columns of A. 
EXAMPLE 7. 
If 
then 
1 3 2] 
A= 
-2 
6 
5 ' 
T 
~_ 
1 
-2] 
3 
6 . 
2 
5 
A 
Properties of the Transpose. 
then 
If r is a scalar and A and B are matrices, 
(a) (AT) T = A 
(b) (A+B) T=A T+B T 
(c) (A1B) T = BTA T (Note that the order changes.) 
(d) (rA) T = rA T 
If we cross out some rows, or columns, of a given matrix A, we obtain a 
submatrix of A. 
EXAMPLE 8. 
Let 
A 
__. 
2 
3 
5 
-1] 
3 
4 
2 
7 9 
8 
2 
6 
1 

8 
Chapter 0 Review of Linear Algebra (Optional) 
If we cross out the second row and third column, we obtain the submatrix 
2 
3 
-1] 
8 
2 
1 " 
A 
We can now view a given matrix A as being partitioned into submatri- 
ces. Moreover, the partitioning can be carded out in many different ways. 
EXAMPLE 9. 
The matrix 
A 
all 
a12 
a13 
a21 
a22 
a23 
a31 
a32 
a33 
a41 
a42 
a43 
a14 
a15 
a24 
a25 
a34 
a35 
a44 
a45 
is partitioned as 
A 
All 
A21 
A12 
A22 
II 
Another partitioning of A is 
A .__ 
all 
a12 Ia13 
al, Ia15 
i 
i 
a21 
a22 Ii a23 
a24 Ii a25 
1 
T--- 
a31 
a32 I a33 
a34 I a35 
a41 
a42 I a43 
a44 Ii a45 
/x 
Another example of a partitioned matrix is the augmented matrix 
[A i b] of a linear system Ax = b. Partitioned matrices can be multiplied 
by multiplying the corresponding submatrices. This idea is illustrated in 
the following example. 
EXAMPLE 10. 
Consider the partitioned matrices 
A 
all 
a12 
a21 
a22 
a31 
a32 
a41 
a42 
a13 
a14 Ii a15 
i 
a23 
a24 I a25 
.rBm~ 
a33 
a34 II a35 
a43 
a44 Ii a45 
All 
A12 
A13 
A21 
A22 
A23 

0.1 Matrices 
9 
and 
bn 
b21 
B 
= 
b31 
b41 
b51 
b12 Ii b13 
b14 
i 
bEE Ib23 
b24 
! 
b32 tj b33 
b34 
b42 [b43 
b44 
b52 [b53 
b54 
We then find, as the reader should verify, that I 1 
Bll 
B12 
B21 
B22 . 
B31 
B32 
An 
--- 
AllBll + A12B21 + A13B31 
A21Bll + A22B21 + A23B31 
A11B12 + A12B22 + Al_3_B3_2_.]. 
A21B12 + A22B22 + A23B32 J 
A 
Addition of partitioned matrices is carried out in the obvious manner. 
0.1 
EXERCISES 
1. If 
find a, b, c, and d. 
In Exercises 2-4, let 
[~ 
A- 
3 311 
1 
2 ' 
[a+~ c+~] [6 8] 
c-d 
a-b 
10 
2 ' 
.[3 1] 
2 
3 ' 
[20] 
B= 
3 
2 
, 
C= 
1 
2 
E= 
0 
2 
5 
, 
1 
2 
3 
Compute, if possible, the following. 
2. (a) C + E 
(b) AB and BA 
(c) AB + DF 
(d) A(BD) 
(a) A(B + D) 
(b) 3B- 2F 
(C) A T 
(d) (C + E) T 
(a) (An) T 
(b) (a T + A)C 
(c) AT(D + F) 
(d) (2C- 3E)TB 
-1 
2 
2 
and 
F 
__ 
3] 
6 , 
1 
2 [4 
-3 

1 O 
Chapter 0 Review of Linear Algebra (Optional) 
5. Let 
[ 1 
A= 
2 
Show that All ~ BA. 
show that All = O. 
6. If 
7. If 
23] and ~ [32 
-4 ~] 
[ 1 2] and B [2 6J 
A= 
-1 
2 
1 
3' 
[5 
1 1 B [ 1 1] 
and c [1 
4] 
A= 
4 
-2 
' 
-2 
5 ' 
2 
8 ' 
show that AC -- BC. 
8. Consider the following linear system: 
3x 
+2z+2w= 
-8 
2x+3y+5z- 
w=4 
3x + 2y + 4z 
= 6 
x 
+ 
z+ 
w=-6. 
(a) Find the coefficient matrix. 
(b) Write the linear system in matrix form. 
(c) Find the augmented matrix. 
9. Write the linear system that has augmented matrix 
3 
-2 
5 
4 
1] 
4 
2 
1 
0 
-3 
. 
3 
4 
-2 
1 
5 
10. If A is an m • n matrix, show that 
AI n = 
ImA 
-- A. 
11. Show that ( - 1)A = - A. 
12. Consider the matrices 
J 
3 
1 
2 
-1 
2 
1 
3 
2 
1 
2 
-1 
2 
A= 
3 
4 
2 
1 
5 
and 
B= 
3 
2 
-1 
2 
3 
1 
-1 
2 
-1 
1 
4 
2 
2 
-1 
2 
3 
4 
5 
Find All by partitioning A and B in two different ways. 
-1 
2 
3 
-3 
2 
2 
5 
2 
9 
1 
3 

0.2 Gauss-Jordan Reduction 
11 
13. (a) Prove that if A has a row of zeros, then AB has a row of zeros. 
(b) Prove that if B has a column of zeros, then All has a column of zeros. 
14. Show that the jth column of the matrix product AB is equal to the matrix 
product AB i, where Bj is the jth column of B. 
15. Show that if Ax = b has more than one solution, then it has infinitely many 
solutions. (Hint: If x I and x 2 are solutions, consider x 3 = rx I + sx 2, where 
r+s= 
1.) 
0.2 GAUSS-JORDAN REDUCTION 
The reader has undoubtedly solved linear systems of three equations in 
three unknowns by the method of elimination of variables. We now discuss 
a systematic way of eliminating variables that will allow the reader to solve 
larger systems of equations. It is not difficult to see that it is more efficient 
to carry out the operations on the augmented matrix of the given linear 
system instead of performing them on the equations of the linear system. 
Thus, we start with the augmented matrix of the given linear system and 
transform it to a matrix of a certain special form. This new matrix 
represents a linear system that has exactly the same solutions as the given 
system. However, this new linear system can be solved very easily. This 
method is called Gauss-Jordan reduction. 
DEFINITION. An m • n matrix A is said to be in reduced row echelon 
form when it satisfies the following properties. 
(a) All rows consisting entirely of zeros, if any, are at the bottom of the 
matrix. 
(b) The first nonzero entry in each row that does not consist entirely of 
zeros is a 1, called the leading entry of its row. 
(c) If rows i and i + 1 are two successive rows that do not consist 
entirely of zeros, then the leading entry of row i + 1 is to the right of the 
leading entry of row i. 
(d) If a column contains a leading entry of some row, then all other 
entries in that column are zero. 
Notice that a matrix in reduced row echelon form might not have any 
rows that consist entirely of zeros. 
EXAMPLE 1. 
The following matrices are in reduced echelon form: 
[1 oo 2] [100301] 
0 
1 
0 
-5 
0 
0 
1 
2 
0 
2 
0 
0 
1 
3 
0 
0 
0 
0 
1 
3 
1 
0 
3 
0 
4 
0 
1 
2 
0 
1 
0 
0 
0 
1 
2 . 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
A 

1 ~ 
Chapter 0 Review of Linear Algebra (Optional) 
EXAMPLE 2. 
The following matrices are not in reduced row echelon 
[1303][1 
0 2 
1] 
0 
0 
0 
0 
0 
4 
2 
2 
0 
0 
1 
2 
0 
0 
1 
3 
1 
0 
2 
1 
1 
2 
5 
-2 
0 
1 
3 
2 
0 
1 
3 
2 
0 
1 
2 
3 
0 
0 
1 
2 " 
0 
0 
0 
0 
0 
0 
0 
0 
form (why not?): 
A 
We now define three operations on the rows of a matrix that can be used 
to transform it to reduced row echelon form. 
DEFINITION. All elementary row operation on an m • n matrix A = 
[ a~j ] is any of the following operations. 
Type I. Interchange rows r and s of A. That is, the elements 
arl , ar2,...,arn 
replace the elements a~, as2,...,asn 
and the elements 
a,1, as2,..., asn replace the elements arl , ar2, . . . , arn. 
Type II. Multiply row r of A by c g: 0. That is, the elements 
arl , ar2,... , arn are replaced by the elements ca,l, ca,2,..., Car,,. 
Type III. Add a multiple d of row r of A to row s of A, writing the result 
in row s. That is, the elements a,l + darl, as2 4r dar2 , .... a~ -1-darn re- 
place the elements a,1, as2,..., ash. 
EXAMPLE 3. 
Let 
A- 
3 
-2 
1 
5 9 
4 
2 
3 
-4 
If we interchange the first and third rows of A, we obtain 
4 
2 
3 
-4] 
B= 
3 
-2 
1 
5 9 
1 
2 
0 
3 
If we multiply the third row of A by -2, we obtain 
1 
2 
0 
3] 
C= 
3 
-2 
1 
5 . 
-8 
-4 
-6 
8 
If we add (-3) times the first row of A to the second row of A, we obtain 
D 
___ [ 
l 
1 
2 
0 
3 
0 
-8 
1 
-4 
9 
4 
2 
3 
-4 
A 

0.2 Gauss-Jordan Reduction 
1 
An m • n matrix A is said to be row equivalent to an m • n matrix B if 
B can be obtained from A by applying a finite sequence of elementary row 
operations to A. 
EXAMPLE 4. 
Let 
A 
~. 
,] 
1 
2 
0 
3 9 
4 
2 
3 
-4 
Interchanging the first and second rows of A, we obtain 
n 
~_ 
1 
2 
0 
3] 
3 
-2 
2 
5, 
4 
2 
3 
-4 
so B is row equivalent to A. Adding - 3 times row 1 of B to row 2 of B, we 
obtain 
C 
.__ 
o 
0 
-8 
2 
-4 
, 
4 
2 
3 
-4 
1 
so C is row equivalent to B and also to A. Multiplying row 2 of C by 
8, 
we obtain 
I 
1 
2 
0 
3 1 
1 
1 
D= 
0 
1 
-~ 
~ , 
4 
2 
3 
-4 
so D is row equivalent to C, to B, and to A. 
A 
It can be shown (Exercise 17) that 
i. every matrix A is row equivalent to itself; 
ii. if A is row equivalent to B, then B is row equivalent to A; and 
iii. if A is row equivalent to B and B is row equivalent to C, then A is 
row equivalent to C. 
In light of ii, the statements "A is row equivalent to B" and "B is row 
equivalent to A" can be replaced by "A and B are row equivalent." Thus, 
the matrices A, B, C, and D in Example 4 are all row equivalent. 
THEOREM 0.1. 
Every m • n matrix can be transformed to reduced row 
echelon form by a finite sequence of elementary row operations. 
A 
We omit the proof of this theorem and illustrate the method with the 
following example. 

1 ~ 
Chapter 0 Review of Linear Algebra (Optional) 
EXAMPLE 5. 
Let 
A 
0 
2 
5 
-2 
1 
0 
0 
2 
1 
3 
2 
-4 
-7 
8 
-7 
" 
2 
0 
3 
4 
-5 
Step 1. 
zeros; this column is called the pivotal column. 
0 
2 
5 
-2 
A= 
0 
0 
2 
1 
2 
-4 
-7 
8 
2 
0 
3 
4 
pivotal column 
ofn 
Find the first column in A that does not consist entirely of 
1 
3 
-7 
-5 
Step 2. 
element, called the pivot, is circled. 
Find the first nonzero entry in the pivotal column. This 
0 
2 
5 
-2 
1 
0 
0 
2 
1 
3 
(~) 
-4 
-7 
8 
-7 
2 
0 
3 
4 
-5 
A 
Step 3. 
Interchange, if necessary, the first row of A with the row in 
which the pivot is located and call the new matrix A1. Thus, the pivot now 
has been moved to position (1, 1) in A l: 
2 
-4 
-7 
8 
-7 
0 
0 
2 
1 
3 
A1 = 
0 
2 
5 
--2 
1 " 
2 
0 
3 
4 
--5 
Step 4. 
Multiply the first row of A 1 by the reciprocal of the entry in 
position (1, 1). That is, multiply the first row of A 1 by the reciprocal of the 
pivot. The matrix thus obtained is denoted by A2: 
7 
4 
7 
1 
-2 
2 
--2 
A2 = 
0 
0 
2 
1 
3 . 
0 
2 
5 
-2 
1 
2 
0 
3 
4 
-5 
Step 5. 
Add suitable multiples of the first row of A 2 to all its other 
rows so that all entires in the pivotal column, except for the entry in which 

0.2 Gauss-Jordan Reduction 
1~ 
the pivot was located, become zero. Thus, all entries in the pivotal column 
and rows 2, 3,..., m are zero. Call the new matrix A3. 
A 3 --- 
7 
7 
1 
-2 
~ 
4 
-3 
0 
0 
2 
1 
3 
0 
2 
5 
-2 
1 
0 
4 
10 
-4 
2 
- 2 times the first row of A2 was 
added to its fourth row. 
Step 6. 
Ignore, but do not erase, the first row of A 3 and denote the 
resulting (m - 1) • n matrix by B. Now repeat Steps 1-5 on B. 
B 
__ I 
O 
0 
2 
1 
31 
0 
(~) 
5 
-2 
1 
0 
4 
10 
-4 
2 
pivotal column 
of B 
1 
-2 
7 
2 
I 
0 
2 
5 
BI= 
0 
0 
2 
0 
4 
10 
-2 
1 
-4 11 
3 
2 
The first and second rows of B were 
interchanged. 
.2 1 
-2 
7 
~ m  
2 
5 
2 
10 
-1 
1 
-4 11 
3 
2 
The first row of B 1 was multiplied by 1 
5- 
7 
-2 
-~ 
7 
~
m
 
2 
I 
5 
11 
0 
1 
~ 
-1 
B3= 
0 
0 
2 
1 
3 
0 
0 
0 
0 
0 
--4 times the first row of B 2 was added to 
its third row. 

1 ~ 
Chapter 0 Review of Linear Algebra (Optional) 
Step 7. 
Add multiples of the first row of B 3 to all the rows of A 3 above 
the first row of B 3 so that all the entries in the pivotal column, except for 
the entry in which the pivot was located, become zero. 
3 
5 
1 
0 
~ 
2 
2 
0 
1 
7 
-1 
B3 -- 
0 
0 
2 
1 
3 
2 times the first row of B 3 was added to 
the shaded row. 
0 
0 
0 
0 
0 
Step 8. 
Ignore, but do not erase, the first row of B 3 and denote the 
resulting (m - 2) • n matrix by C. Repeat Steps 1-7 on C. 
3 
5 
1 
0 
~ 
2 
2 
5 
1 
0 
1 
~ 
-1 
~- 
0 
0 
0 
0 
0 
1' 
pivotal column 
ore 
The final matrix 
5 
19 
1 
0 
0 
~" 
4 
0 
1 
0 
9 
13 
4 
4 
1 
3 
0 
0 
1 
~ 
0 
0 
0 
0 
0 
is in reduced row echelon form. 
A 

0.2 Gauss-Jordan Reduction 
1 
We now discuss the use of the reduced row echelon form of a matrix in 
solving a linear system of equations. The following theorem provides the 
key result. Its proof, although not difficult, is omitted. 
THEOREM 0.2. 
Let Ax = b and Cx = d be two linear systems, each 
consisting of rn equations in n unknowns. If the augmented matrices [ A [ b ] 
and [ C ~ d ] are row equivalent, then both linear systems have no solutions or 
they have exactly the same solutions. 
/x 
The Gauss-Jordan reduction procedure for solving a linear system 
Ax = b consists of transforming the augmented matrix to reduced row 
echelon form [C ~j d] using elementary row operations. Since [A I b] and 
[ C ~ d ] are row equivalent, it follows from Theorem 0.2 that the given 
linear system and the system Cx = d corresponding to the augmented 
matrix [C ~ d] have exactly the same solutions or neither has any solu- 
tions. It turns out that the linear system Cx = d can be solved very easily 
because its augmented matrix is in reduced row echelon form. More 
specifically, ignore all rows consisting entirely of zeros, since the corre- 
sponding equation is satisfied for any values of the unknowns. For each 
nonzero row of[C tjd ], solve the corresponding equation for the un- 
known that corresponds to the leading nonzero entry in the row. We 
illustrate the method with several examples. 
EXAMPLE 6. 
Consider the linear system 
x+3y+2z=5 
3x+ 
y- 
z=-8 
2x+2y+3z= 
1. 
The augmented matrix of this linear system can be transformed to the 
following matrix in reduced row echelon form (verify), 
1 
0 
0 
- 3] 
0 
1 
0', 
2, 
0 
0 
11 
1 
which represents the linear system 
=-3 
=2 
z=l. 
Thus, the unique solution to the given linear system is 
x--3 
y=2 
z--1. 
A 

1 ~ 
Chapter 0 Review of Linear Algebra (Optional) 
EXAMPLE 7. 
Consider the linear system 
x+ 
y+2z+3w= 
13 
x-2y+ 
z+ 
w=8 
3x+ 
y+ 
z- 
w=l. 
The augmented matrix of this linear system can be transformed to the 
following matrix in reduced row echelon form (verify), 
1 
0 
0 
-1 
0 
1 
0 
0 
0 
0 
1 
2 
0 
0 
0 
0 
which represents the linear system 
-w 
= 
-2 
y 
=-1 
-2 
-1 
8 
' 
0 
z+2w= 
8. 
This linear system can be solved, obtaining 
x=-2+r 
y=-I 
z=8-2r 
w~--r, 
where r is any real number. This solution may be written in matrix form as 
x 
-2 
1 
0 
y 
= 
1 
+r 
. 
z 
8 
-2 
w 
0 
1 
The situation in this example is typical of what occurs in linear pro- 
gramming problems in that the number of unknowns generally exceeds the 
number of equations. As this example shows, there may be infinitely many 
solutions to such a problem. In linear programming we study how to 
choose a "best" solution from among these. 
A 
EXAMPLE 8. 
Consider the linear system 
x + 2y- 3z = 2 
x+3y+ 
z=7 
x+ 
y-7z=3. 
The augmented matrix of this linear system can be transformed to the 
following matrix in reduced row echelon form (verify), 
0 110] 
0 
1 
4 
0 , 
0 
0 
0 
1 

0.2 Gauss-Jordan Reduction 
1 
which represents the linear system 
- 
llz = 0 
y+ 
4z=0 
0=1. 
Since this last system obviously has no solution, neither does the given 
system. 
A 
The last example shows the way in which we recognize that a linear 
system has no solution. That is, the matrix in reduced row echelon form 
that is row equivalent to the augmented matrix of the linear system has a 
row whose first n entries are zero and whose (n + 1)th entry is 1. 
Homogeneous Systems 
A linear system of the form 
a n x 1 + a12 x 2 + 
"" +aln 
x n 
= 0 
a21 x I + a22 x 2 + 
--- +a2n 
x n = 0 
o 
9 
9 
9 
9 
o 
9 
~ 
o 
o 
amlX 1 + am2X 2 + "'" +amnX n 
= 0 
(1) 
is called a homogeneous 
system. Observe that a homogeneous system 
always has the solution 
X 1 ~'X 2 . . . . .  
X n = O, 
which is called the trivial solution. A homogeneous system may also have a 
solution in which not all the x i are zero. Such a solution is called a 
nontrivial solution. 
EXAMPLE 9. 
Consider the homogeneous system 
x- 
y+2z=0 
-x+3y+4z=O 
2x+ 
y+3z=O. 
The augmented matrix of this system 
1 
-1 
-1 
3 
2 
1 
210 
410 
3',0 

20 
Chapter 0 Review of Linear Algebra (Optional) 
is row equivalent to (verify) 
1 
0 
0 
0 
1 
0 
0 
0 
1 
0] 
0 , 
0 
so the only solution to the given homogeneous system is the trivial solution 
EXAMPLE 10. 
x=y 
=z=0. 
Consider the homogeneous system 
x- 
y+2z=0 
-x+3y+4z=O 
x+ 
y+8z=0. 
The augmented matrix of this system 
1 
-1 
-1 
3 
1 
1 
is row equivalent to (verify) 
20] 
4 
0 
8 
0 
1 
0 
5 
0] 
0 
1 
3 
0 , 
0 
0 
0 
0 
so a solution to the given homogeneous system is 
x = -5r 
y = -3r 
z=r, 
where r is any real number. 
A 
A 
0.2 
EXERCISES 
In Exercises 1-4 transform the given matrix to a matrix in reduced row echelon 
form. 
I 
1 
[1 
11 
1 
1 
1 
1. 
2 
3 
5 
2 
2. 
1 
2 
-3 
2 
2 
1 
4 
1 
2 
-1 
1 
4 
2 
3 
1 
5 
3 
2 
-1 
I 
0 
1 
2 
-1 
31 
[ 
1 
0 
3 
4 
1 
2 
-1 
4. 
4 
-4 
3. 
6 
9 
4 
3 
1 
-3 
-1 
5 
-5 
10 
-2 
4 
23 
4] 
1 
1 
3 
3 
5 
1 

0.2 Gauss-Jordan Reduction 
~'1 
In Exercises 5-10, find all solutions to the given linear systems. 
5. (a) 
x+y+ 
z=l 
(b) 
x+ 
y+2z- 
-1 
2x-y+2z= 
-4 
2x- 
y+3z= 
-6 
3x+y+ 
z=3 
5x+2y+9z-2 
6.(a) 
x+ 
y+2z=-3 
(b) 
x+ 
y+2z+ 
w=4 
2x+2y-5z= 
15 
2x-2y+3z-2w=5 
3x+ 
y- 
z=lO 
x+7y+3z+5w=7 
2x+ 
y+2z=5. 
7. (a) 
x + y + 2z + 
w = 5 
(b) 
3x +y- 
2z + 2w = 8 
-x + y + 6z 
= 4 
x+2y+z- 
w=-2 
2x+ 
y+z+2w=6 
3x + 2y 
+ 
w = 6 
x+3y+z+2w= 
-1 
2x-5y+z-2w=14 
8. (a) 
x+3y+ 
z+ 
w=4 
(b) 
2x+ 
y + 2z- 2w= 8 
x+8y+ 
z+5w-4 
3x- 
y+3z-5w- 
12 
x+2y+ 
z=-I 
2x+3y-5z= 
-7 
4x + 5y + 7z = 5 
9. (a) 
2x+ 
y+2z+ 
w-2 
(b) 
x+3y-2z-3w- 
-4 
4x + 2y + 
z 
-- 2 
-2x-6y+ 
z+4w-2 
x+2y- 
z=5 
4x- 
y+4z=8 
-x+7y-7z=7 
10. (a) 
x- 
y- 
z-2 
(b) 
2x+ 
y+3z=-5 
3x+4y- 
z= 14 
x + 2y- 
3z = 4 
2x + 
y - 4z = 5 
x + 5y- 
5z = 6 
ll.(a) 
x+ 
y+8z=O 
(b) 
x+ 
y+8z+ 
w=O 
-x+3y+4z=O 
-x+2y+3z- 
w-O 
2x+ 
y+3z=O 
2x+ 
y- 
z+2w=O 
12. (a) 
x- 
y+2z=O 
(b) 
x- 
y+2z+ 
w=O 
-x+3y+4z=O 
-x+3y 
+4z+2w 
=0 
-x+2y+ 
z=O 
2x+3y-2z+3w=O 
In Exercises 13 and 14 find all values of a for which the resulting linear system has 
(i) no solution, (ii) a unique solution, and (iii) infinitely many solutions. 
13. x+ 
y=3 
x + (a 2 - 8)y = a 
14. x+ 
2y- 
2z--4 
-y+ 
5z=2 
x+ 
y+(a 2- 13)z-a+2 
15. Let A be an n • n matrix in reduced row echelon form. Show that if A 4: I,,, 
then A has a row of zeros. 
16. Consider the linear system Ax - 0. Show that if x i and x 2 are solutions, then 
x -- rx 1 + sx 2 is a solution for any real numbers r and s. 

22 
Chapter 0 Review of Linear Algebra (Optional) 
17. Prove the following. 
(a) Every matrix is row equivalent to itself. 
(b) If A is row equivalent to B, then B is row equivalent to A. 
(c) If A is row equivalent to B and B is row equivalent to C, then A is row 
equivalent to C. 
18. Let 
A 
._.. 
a 
b 
[c 
d]" 
Show that A is row equivalent to I 2 if and only if ad - 
bc 4= O. 
0.3 THE INVERSE OF A MATRIX 
In this section we restrict our attention to square matrices. 
DEFINITION. 
An n • n matrix A is called nonsingular or invertible if 
there exists an n • n matrix B such that 
All= BA= I.. 
The matrix B is called the inverse of A. If no such matrix B exists, then A is 
called singular or noninvertible. If the inverse of A exists, we shall write it 
as A-1. Thus 
EXAMPLE 1. 
Let 
AA -1 = A-1A = I,. 
Since 
[ ] 
[2 
1] 
1 
2 
and 
B = 
3 
1 . 
A= 
3 
4 
~ 
2 
AB= BA= I2, 
it follows that B is the inverse of A or B = A-1 and that A is nonsingular. 
A 
EXAMPLE 2. 
Let 
1 
A=[2 
6 " 
Does A have an inverse? If it does, let such an inverse be denoted by 
B 
___ 
Then 
x 
y]. 
z 
w 
[ 
jig I 
[10] 
1 
3 
y 
=i2= 
. 
All= 
2 
6 
w 
0 
1 

0.3 The Inverse of a Matrix 
23 
Thus, 
[ x+3z 
y+3w]=[a 
0] 
2x+6z 
2y+6w 
0 
1 ' 
which means that we must have 
x+3z=l 
2x + 6z = 0. 
Since this linear system has no solution, we conclude that A has no inverse. 
A 
In practice, if the matrix B is proposed as the inverse of A, we need only 
check that All = I n because it can be shown that if All = In, then also 
BA = I n. 
THEOREM 0.3 
(PROPERTIES 
OF INVERSE). 
(a) If A is nonsingular, then A-1 is nonsingular and 
(A- 1) -1 
--" A. 
(b) If A and B are nonsingular, then All is nonsingular and 
(All) -1 = B- 1A- 1 (note that the order changes). 
(c) If A is nonsingular, then A T is nonsingular and 
(AT) -1 = 
(A-l) 
T. 
/k 
We now develop some results that will yield a practical method for 
computing the inverse of a matrix. The method is based on the properties 
of elementary matrices, which we discuss next. 
DEFINITION. An n • n elementary matrix of type I, type II, or type III 
is a matrix obtained from the identity matrix I n by performing a single 
elementary row operation of type I, type II, or type III, respectively. 
EXAMPLE 3. 
matrices. 
E 1 = 
E 2 = 
E 3 = 
Matrices El, E2, and E 3 as defined below are elementary 
[io o] 
0 
1 
1 
0 
-3 
0 
0 [1 
0 
-4 
The second and third rows were interchanged. 
The first row was multiplied by - 3. 
-4 times the first row was added to the 
third row. 
A 

~ 
Chapter 0 Review of Linear Algebra (Optional) 
THEOREM 0.4. 
Let A be an m • n matrix, and let the matrix B be 
obtained from A by performing an elementary row operation on A. Let E be 
the elementary matrix obtained by performing the same elementary row opera- 
tion on I n as that performed on A. Then B = EA. 
A 
EXAMPLE 4. 
Let 
A 
1 
2 
-1 
3] 
2 
3 
5 
4 . 
-3 
2 
4 
-2 
If we add -2 times row 1 to row 2, we obtain 
1 
2 
-1 
3] 
B = 
0 
-1 
7 
-2 
. 
-3 
2 
4 
2 
Performing the same elementary row operation on I3, we obtain the 
elementary matrix 
1 
0 
0] 
E= 
-2 
1 
0 . 
0 
0 
1 
It is now easy to verify that B = EA. 
A 
The following theorem follows easily from the definition of row equiva- 
lence of matrices and Theorem 0.3. 
THEOREM 0.5. 
The m • n matrices A and B are row equivalent if and 
only if 
B = EkE k_a "'" E2E1A, 
where El, E2,... , E k are elementary matrices. 
A 
THEOREM 0.6. 
An elementary matrix is nonsingular and its inverse is an 
elementary matrix of the same type. 
A 
THEOREM 0.7. 
An n • n matrix A is nonsingular if and only if A is a 
product of elementary matrices. 
A 
COROLLARY 0.1. 
An n X n matrix A is nonsingular if and only if A is 
row equivalent to I n. 
A 
Suppose now that A is nonsingular. Then, by Corollary 0.1 and Theorem 
0.5, there exist elementary matrices E~, E2,... , E k such that 
I~ = EkE k_ 1 "'" E2 EIA. 
Then 
A- 1 = EkEk - 1 "'" E2 El. 

0.3 The Inverse of a Matrix 
25 
Procedure for Computing A- 
We now have an effective algorithm for computing A -1. We use 
elementary row operations to transform A to In; the product of the 
elementary matrices EkEk_ 1 ..-E2E 1 gives A -1. The algorithm can be 
efficiently organized as follows. Form the n x 2n matrix [A i l n] and 
perform elementary row operations to transform this matrix to [I~ i A-l]. 
Every elementary row operation that is performed on a row of A is also 
performed on the corresponding row of I n . 
EXAMPLE 
5. 
Let 
Then 
Thus, 
0 
3 [1 
0 
0 
0 
0 
11 
0 
0 
11 
0 
0 
1 
2 
2 
1 
2 
-1 
1 
1 
-1 
1 
1 
0 
1 
1 
0 
1 
1 
2] 
A= 
0 
2 
5 9 
3 
2 
2 
1 
1 
2 
[A 'I 3] = 
0 
2 
5 
3 
2 
2 100] 
0 
1 
0 . 
0 
0 
1 
21 
1 
5l 
o 
21 
o 
2 
1 
5 
0 
-4 
-3 
2', 
1 
5 
I 
~,, 
o 
-4 ,, -3 
2 
1 
5 
0 
3 
-~ 
-3 
2 
1 
5 
0 
1 
2 
1 
-~ 
1 
5_ 
0 
2 
1 
2 
0 0] 
1 
0 9 
0 
1 
o o] 
1 
0 9 
0 
1 
" 
~ 1 
1 
0 . 
0 
1 
~ ~ 1 
1 
0 . 
1 
1 
O~ 
O- 1 
0 . 
1 
2 
3 
3 
1 
0 
2 
! 
0 
2 
1 
2 
3 
--g 
Add -3 times the first row to the 
third row, obtaining 
1 
Multiply the second row by ~, obtaining 
Add the second row to the third row, 
obtaining 
2 
Multiply the third row by - 3, obtaining 
Add - 1 times the second row to the 
first row, obtaining 
Add - ~ times the third row to the 
second row, obtaining 

~ 
Chapter 0 Review of Linear Algebra (Optional) 
1 
0 
0 
1 
0 
0 
Hence, 
1 
0 
1 
0 
0 
1 
1 
0 
1 
2 
4 
5 
-5 
-~ 
3 9 
1 
2 
2 
3 
g 
2 
1 
2 
3 
4 
5 
-5 
~ 
~ . 
1 
2 
2 
3 
~ 
A 
--1 
__ 
2 
2 
1 
3 
3 
4 
5 
g7 
1 
2 
2 
3 
3 
1 
Add ~ times the third row to the first 
row, obtaining 
A 
It can be shown that if the matrix C in reduced row echelon form 
obtained from A has a row of zeros, then A is singular. However, observe 
that once a matrix obtained from A by this process has a row of zeros, then 
the final matrix C in reduced row echelon form will also have a row of 
zeros. Thus, we can stop our calculations as soon as we get a matrix from A 
with a row of zeros and conclude that A-1 does not exist. 
EXAMPLE 
6. 
Let 
[1 
A= 
2 
1 
To find A-1 we proceed as above: 
[
1
2
1
1
0
0
]
 
2 
3 
-4 
0 
1 
0 . 
1 
3 
7 
0 
0 
1 
1 100] 
0 
-1 
-6 
-2 
1 
0 . 
1 
3 
7 
0 0 1  
[
1
2
1
'
,
1
0
0
]
 
0 
-1 
-6 I -21 
0 . 
0 
1 
6',-101 
[
1
2
1
1
0
0
]
 
0 
-1 
-6 
-2 
1 
0 . 
0 
0 
0 
-31 
1 
21] 
3 
-4 
. 
3 
7 
Add -2 times the first row to the 
second row, obtaining 
Add - 1 times the first row to the third 
row, obtaining 
Add the second row to the third row, 
obtaining 
Since we have a matrix with a row of zeros under A, we stop and conclude 
that A is singular. 
A 

0.3 The Inverse of a Matrix 
27 
The method just presented for calculating A-1 has the nice feature that 
we do not have to know, in advance, whether or not A-1 exists. That is, we 
set out to find A-1, and we either find it or determine that it does not exist. 
Suppose now that A is an n • n matrix, and consider the linear system 
Ax=b. 
(1) 
If A is nonsingular, then A-1 exists. Multiplying both sides of (1) by A-1 on 
the left, we have 
A-lAx = A-lb 
or 
l,x = x = A -lb. 
Thus, if A-1 is known, then the solution to (1) can be simply calculated as 
A-lb. 
0.3 
EXERCISES 
1. If 
A-l= [23 
1 
4 ' 
find A. 
2. If 
,_1__[3 2] and B a [2 3] 
1 
3 
4 
1 ' 
find (AB)- 1. 
3. Let A be a 4 x 3 matrix. Find the elementary matrix E that when it multiplies 
A on the left, performs the following elementary row operation: 
(a) Interchanges the second and fourth rows. 
(b) Multiplies the third row by 2. 
(c) Adds -3 times the third row to the fourth row. 
4. Find all the values of k for which the matrix 
A 
1 
0 
2] 
0 
1 
k 
-1 
1 
0 
is singular. 

~8 
Chapter 0 
Review of Linear Algebra (Optional) 
In Exercises 5-10 find the inverses of the given matrices, if possible. 
[11 2] 
[lOl] 
~ 
~a' 
4~ 
~ 
~ 
4 
~ 
~ 
~ 
2 
5] 
(b) 
6. 
(a) 
3 
2 
1 3 ] 
(b) 
7. 
(a) 
-2 
4 
2 
-2] 
(b) 
8. 
(a) 
4 
3 
2 3 4] 
0 
1 
2 
-2 
5 
1 
1 
2 
-1 
1 
1 
1 
3] 
1 
3 1] 
-1 
2 
2 
1 
3] 
(c) 
4 
6 
2 
-1 
-6 
4 
1 5 3] 
(c) 
2 
5 
1 
1 1 3 
(c) 
1 
0 
2] 
2 
1 
-1 
0 
4 
4 
3 
3 ] 
(b) 
9. 
(a) 
2 
2 
2 
3 ] 
(b) 
10. 
(a) 
4 
-3 
2 
5 
-2 
1 
2 
6 
1 
8 
-8 3] 
0 
4 
3] [1 
3 
(c) 
2 
4 
-4 
(c) 
2 
-6 
5 
3 
251 
4 
10 
3 
-9 
-2 
4 
11. Consider the matrix 
A-[ ac 
b 
Show that if ad- 
bc 4= 0, then 
d 
-b 
ad - bc 
ad - bc 
A-l_ 
-c 
a 
ad - bc 
ad- 
bc 
1 [~ 
b] 
ad - bc 
- c 
a 
" 
12. Prove Theorem 0.4. 
13. Prove Theorem 0.5. 
14. Prove Theorem 0.6. 
15. Let A be an n x n matrix. Show that A is nonsingular if and only if the linear 
system Ax = b has a solution for every n x 1 matrix b. 
16. Show that two m x n matrices A and B are row equivalent if and only if there 
exists a nonsingular matrix P such that B = PA. (Hint: Use Theorems 0.5 and 
0.7.) 
17. Prove Corollary 0.1. 

0.4 Subspaces 
29 
0.4 SUBSPACES 
DEFINITION. 
An n-vector (or n-tuple) is an n x 1 matrix 
x 1 
x 2 
X-- 
whose entries are real numbers, called the components of x. 
The set of all n-vectors is denoted by R" and is called n-space. When 
we do not need to specify the value of n, we merely refer to an n-vector as 
a vector. As a space-saving device, at times some authors write the vector x 
as (x 1, x2,..., x,). However, it is thought of as a column whenever opera- 
tions, and especially matrix multiplication, are performed on it. Through- 
out this book the vector x will be written as a column or as [x I x 2 --. x,] T. 
Since R ~ is the set of n x 1 matrices, the operations of addition and scalar 
multiplication, which we discussed in Section 0.1, are defined in this set. 
EXAMPLE 1. 
Consider the vectors 
-4 
3 
2 
and 
Y = 
- 2 
x= 
3 
5 
4 
3 
in R 4 and let c = -3. Then 
and 
x+y= 
-4+3 
-1 
2+(-2) 
= 
0 
3+5 
8 
4+3 
7 
cx = -3x = 
-3(-4) 
12 
-3(2) 
= 
-6 
-3(3) 
9 " 
-3(4) 
-12 
A 
The reader will recall that R 3 is the world we live in, and we can thus 
visualize vectors in R 3. For example, in Figure 0.1 we show the vectors 
[i] I11 
[0] 
3 
and 
z= 
1 
x= 
, 
Y= 
-3 
, 
2 
0 

30 
Chapter 0 Review of Linear Algebra (Optional) 
E'] 
2 
f!] 
3 
v 
X 
FIGURE 0.1 
We now turn our attention to certain subsets of R', which are used in 
applications. 
DEFINmON. 
A nonempty subset V of R n is called a subspace if the 
following properties are satisfied. 
(a) If x and y are any vectors in V, then x + y is in V. 
(b) If r is any real number and x is any vector in V, then rx is in V. 
EXAMPLE 2. 
The simplest example of a subspace of R n is R n itself. A 
Another example is the subset consisting only of the zero vector, which is 
called the zero subspace. 
EXAMPLE 3. 
Consider the subset V of R 3 consisting of all vectors of 
the form 
IXl 1 
X 2 
. 
X 1 -'t-X 2 
Show that V is a subspace. 
Solution. Let r be a real number and let 
I xl 
IyYll 
x= 
x2 
and 
y= 
Y2 
Xl ~- X2 
1 + Y2 

0.4 Subspaces 
~1 
be vectors in V. Then we must show that x + y and rx are in V. A vector is 
in V if its third component is the sum of the first two components. We now 
have 
x+y= 
I I 
1 
I 
X1 + Yl 
X1 + Yl 
X2 + Y2 
= 
X2 + Y2 
, 
(X1 + X2) + (Yl + Y2) 
(Xl + Yl) + (X2 + Y2) 
so that x + y is in V. Also, 
I 
IX 1 
rx = 
IX2 
r(x 1 +/2 ) 
is in V. Hence, V is a subspace. 
EXAMPLE 4. 
the form 
Is V a subspace of R 39. 
Solution. 
i i 
= 
FX 2 
FX 1 + rx 2 
A 
Consider the subset V of R 3 consisting of all vectors of 
Ixml 
X 2 9 
1 
Let r be a real number and let 
iill 
X'- 
2 
and 
y= 
be vectors in V. Then 
x+y= Ix  yll IXl yal 
+ Y2 
= 
x2 + y2 
, 
+1 
2 
which is not in V, since its third component is 2, whereas the third 
component of every vector in V must be 1. Hence, V is not a subspace. ZX 
EXAMPLE 5. 
Let A be an m x n matrix and consider the linear system 
Ax = 0. The set V of all solutions to this system is a subset of R n. We now 
show that V is a subspace of R n. Thus, let x and y be in V. Then x and y 
are solutions, so that 
Ax=0 
and 
Ay=0. 

32 
Chapter 0 Review of Linear Algebra (Optional) 
We have 
A(x+y) =Ax+Ay=0 
+ 0=0, 
which implies that x + y is a solution or that x + y is in V. Also, if r is any 
real number, then 
A(rx) = r(Ax) = r0 = 0, 
so that rx is in V. Thus, V is a subspace of R n. This subspace is called the 
null space of A and is denoted by n(A). 
A 
0.4 
EXERCISES 
1. Verify that {0} is a subspace of R'. 
2. Which of the following subsets of R 3 are subspaces? The set of all vectors of 
the form 
I x' ] 
(a) 
x2 , where x 3 --" X 1 § 2X 2. 
X 3 [x,] 
(b) 
x2 , where x3 :~: 0. 
x 3 
(c) [xl] 
X 2 
Xl+l 
3. Which of the following subsets of R 4 are subspaces? The set of all vectors of 
the form 
xl 
x2 
wherex l=0andx 
3=x 1 +2. 
(a) 
x3 , 
X4 
X1 
x2 
where x I -- 0 and x 2 = -2x 3. 
(b) 
x3 ' 
X4 
X1 
x2 
where x 1 + x 2 = 0. 
(c) 
x3 ' 
X4 

0.5 Linear Independence and Basis 
33 
4. Which of the following subsets of R 3 are subspaces? The set of all vectors of 
the form 
(a) 
- 
c 
. 
a 
(b) 
b + a . 
a-3 
(c) 
a - c 
. 
b+c 
5. Which of the following subsets of R a are subspaces? The set of all vectors of 
the form 
[ao  1 
(a) [ab+2a b . 
(b) 
O b 
. 
+b 
-3a 
" 
4a + 3b 
6. Show that 0x - 0 for any vector x. 
7. Show that r0 - 0 for any real number r. 
8. Show that -(-x) 
= 
x. 
9. Show that if r x - 0, then r = 0 or x = 0. 
10. Show that (- 1N 
- 
-x. 
11. Show that ifx+y-x+z, 
theny=z. 
12. Let x and y be fixed vectors in R". Show that the set of all vectors rx + sy, 
where r and s are any real numbers, is a subspace of R". 
0.5 LINEAR INDEPENDENCE AND BASIS 
In this section we examine the structure of a subspace of R n. 
DEFINITION. 
A vector v in R n is said to be a linear combination of the 
vectors v 1, v2,..., v k if it can be written as 
V ~-- C1u 1 -~- C2V 2 -b "'" -[-Cku k, 
where Cl, c2,..., Ck are real numbers. 

~ 
Chapter 0 Review of Linear Algebra (Optional) 
EXAMPLE 1. 
Let [4][1] 
[2] 
v = 
7 
, 
V 1 = 
2 , 
and 
v 2 = 
3 
9 
2 
-1 
4 
The vector v is a linear combination of v 1 and v 2 if we can find constants c 1 
and c 2, such that 
ClVl + 
C2V2 - 
V 
or 
[1] 
V 1 -- 
2 
, 
V 2 -- 
1 
Determine whether S spans R 3. 
Solution. 
Let 
[1] [2} [4] 
Cl 
2 
+c2 3 
= 
7 , 
-1 
4 
2 
which leads to the linear system 
c I + 2c e = 4 
2c I + 
3c 2 = 7 
--C 1 + 
4c 2 = 2. 
Using Gauss-Jordan 
reduction, we obtain the solution c 1 - 2 and c 2 - 
1. 
A 
DEFINITION. 
Let S - {v 1, v2,..., v k} be a set of vectors in a subspace V 
of R n. The set S spans V, or V is spanned by S, if every vector in V is a 
linear combination of the vectors in S. 
EXAMPLE 2. 
Let S - {v 1, v 2, v3}, where 
[0] 
1 , 
and 
v 3 = 
1 . 
0 
1 
Ixal 
X = 
X 2 
X 3 
be any vector in R 3. The set S spans R 3 if we can find constants Cl, Ce, 
and c 3, such that 
ClV 1 + 
C2V 2 + 
C3V 3 = 
X. 
We then have the linear system 
C 1 + 2C 2 + 
Oc 3 = x 1 
2c I + 
c 2 + 
c 3 -- x2 
c I + Oc 2 + 
c 3 = x 3. 

0.5 Linear Independence and Basis 
~1~1 
A solution to this linear system can be easily obtained for any choice of x l, 
x 2, and x3: 
C1 -- --Xl + 2X 2 -- 2x3, 
C 2 ---Xl --X 2 + X 3, 
C 3 = X 1 -- 2x 2 + 3X 3 
Thus, S spans R 3. 
EXAMPLE 3. 
Let S -- {Vl, u 
where 
[1] 
[1] 
u 
"-- 
2 
and 
u = 
1 . 
1 
1 
Determine whether S spans R 3. 
Solution. 
Let 
A 
DEFINITION. 
Let S--{V1, u 
Yk} be a set of distinct vectors in a 
subspace V of R ~. The set S is said to be linearly dependent if we can find 
constants Cl, c2,.--, Ck, not all zero, such that 
C1u 1 + C2V 2 + 
"'" +CkV k -~ O. 
(1) 
Otherwise, S is said to be linearly independent. That is, S is linearly 
independent if Equation (1) can be satisfied only with 
c 1 = c 2 ..... 
c k = O. 
Of course, (1) always holds when 
C 1 -- C 2 . . . . .  
C k --" O. 
The essential point in the above definition is whether the equation can 
hold with not all of the constants c 1, c2,..., c k being zero. Equation (1) 
IXll 
X -- 
X 2 
X 3 
be any vector in R 3. We need to find constants c 1 and c 2, such that 
C1u 1 + C2V 2 = X. 
If we transform the augmented matrix of the resulting linear system to 
reduced row echelon form, we obtain 
i10 x Xll 
0 
1 
2x 1 - x 2 . 
0 
0 
X 3 --X 1 
Thus, a solution exists only when x 3 -x I = 0. Since we must find con- 
stants for any choice of x 1, x 2, and x 3, we conclude that S does not span 
R 3. 
/k 

~ 
Chapter 0 Review of Linear Algebra (Optional) 
always leads to a homogeneous system. The set S = 
{v1,v2,...,v 
k} is 
linearly dependent if and only if the resulting homogeneous system has a 
nontrivial solution. 
EXAMPLE 4. 
Consider the vectors 
1 
0 
2 
1 
V1 = 
0 ' 
V2 -- 
1 ' 
1 
2 
and 
V 3 
1 
= 
1 
1 
Determine whether S = 
{v1, v2, v 3} is linearly dependent or linearly inde- 
pendent. 
Solution. 
From Equation (1) we have 
C1V 1 + C2V 2 + 
C3V 3 -- O, 
which yields the linear system 
C1 
2c I + 
+C 3 -'0 
C 2 + C 3 = 
0 
C 2 -- C 3 = 
0 
C 1 + 
2C 2 + C a = 
0. 
Since this linear system has only the trivial solution c 1 -- C 2 = 
C 3 -- 0, we 
conclude that S is linearly independent. 
/x 
EXAMPLE 5. 
Consider the vectors 
[1][1] 
[1] 
Vl- 
-1 
, 
v2= 
0 , 
and 
v 3 = 
-2 
. 
3 
-2 
4 
Determine whether S - {v 1, v 2, v 3} is linearly dependent or linearly inde- 
pendent. 
Solution. 
From Equation (1) we have 
ClV 1 + C2V 2 + C3V 3 = 
O, 
which yields the linear system 
C 1 + 
C 2 + 
C 3 -- 0 
-- C 1 
-- 2C 3 = 
0 
-- 3C 1 -- 2C 2 -- 4C 3 -- 0, 
which has among its solutions (verify) 
c 1 =2, 
c 2= -1, 
and c a- 
-1. 
Hence, S is linearly dependent. 
/X 

0.5 Linear Independence and Basis 
~ 
THEOREM 0.8. 
The set S = {Vl,V2,...,Vk} of vectors in R ~ is linearly 
dependent if and only if one of the vectors in S is a linear combination of the 
other vectors in S. 
Proof. 
Suppose S is linearly dependent. Then we can write 
ClV 1 -~- C2V 2 ~- "'" +CkV k = O, 
where not all the constants c l, c2,... , c k are zero. Suppose cj ~ O. Then 
C 1 
C 2 
Cj__ 1 
Cj+ 1 
Ck 
= 
V 1 -- __ V 2 .
.
.
.
.
.
 
Vj_ 1 
Vj+I .... 
U k. 
Vj 
Cj 
Cj 
Cj 
Cj 
Cj 
Conversely, suppose one of the vectorsmsay vj--is a linear combination of 
the other vectors in S" 
Then 
Vj = 
alv I + 
a2v 2 + 
"" +aj_lVj_ 1 + aj+lVj+ 1 + ... +akV k. 
air 1 + 
a2v 2 + 
"" +aj_lVj_ 1 -- lvj + aj+lVj+ 1 + ... +akV k = 
0. 
(2) 
Since at least one of the coefficients in (2) is nonzero, we conclude that S 
is linearly dependent. 
/x 
Thus, in Example 5 we found that 
2V 1 -- V 2 -- V 3 = 
0, 
so that we could solve for any one vector in terms of the other two. 
However, the theorem does not say that in general every vector in S is a 
linear combination of the other vectors in S. For suppose we have 
5u 1 + 
0V 2 -- 2V 3 + 
5u 4 -- 0. 
Then we cannot solve for v 2 in terms of v 1, v 3, and v 4. 
THEOREM 0.9. 
Let A be an n • n matrix. Then A is nonsingular if and 
only if the columns of A (as vectors in R n) form a linearly independent set. /x 
EXAMPLE 6. 
Let 
[ 
1 
1 
2 
0 
A= 
-1 
1 
1 . 
2 
3 
2 
Since A is nonsingular (verify), the columns of A form the linearly indepen- 
dent set 
{[ 1] [2] [0]) 
-1, 
1, 
1 
2 
3 
2 
in R 3. 
/k 

31] 
Chapter 0 Review of Linear Algebra (Optional) 
DEFINITION. 
A set of vectors S = {v1, v2,... 
, Vk} in a subspace V of R n 
is called a basis for V if S spans V and S is linearly independent. 
EXAMPLE 7. 
The sets 
([1] 
S= 
0 , 
0 
are bases for R 3. 
THEOREM 0.10. 
[~ I~ 
([i] [~ [1]) 
1 , 
0 
and 
T = 
, 
1 , 
1 
0 
1 
1 
0 
/x 
/f S --- {v1, v2,... , Vk} is a basis for a subspace V of R n 
and if T = {w l, w2,... 
, Wr} iS a linearly independent set of vectors in V, then 
r<k. 
A 
COROLLARY 0.2. 
If S = {v1, v2,... , v k} and T = {Wl, w2,... 
, Wr} are 
bases for a subspace V of R n, then k = r. 
/x 
DEFINITION. 
The dimension of a subspace V of R n is the number of 
vectors in a basis for V. Thus, the dimension of R n is n. 
THEOREM 0.11. 
Let V be a vector space of dimension k and let S = 
{v 1, v2,..., v k} be a set of k vectors in V. 
(a) If S is linearly independent, then S is a basis for I~. 
(b) If S spans I~, then S is a basis for IA. 
THEOREM 0.12. 
If S = {v 1, v2,..., v k} is a basis for a subspace V of R ~, 
then every vector x in V can be written in one and only one way as a linear 
combination of the vectors in S. 
Proof. 
Suppose 
and also 
x = alv 1 d- a2v 2 d- --- -Fakv k 
X = blV 1 + b2v 2 + "'" +bkV k. 
Subtracting the second expression from the first, we obtain 
0 = x - x = (al - bl)Vl + (a2 - b2)v2 + "'" +(ak -- bk)Vk. 
(3) 
Since S is a basis, it is a linearly independent set of vectors. Equation (3) is 
the expression that must be tested for linear independence. Therefore, 
aj - bj = 0 (1 < j < k) or aj = bj and the expression for x is unique. 
A 
DEFINITION. 
Suppose that S = {v 1, v2,..., v k} is a basis for a subspace 
V of R ~ and let x be a vector in V. Then 
X--ClV 1 + C2V 2 + "'" +CkV k. 

0.5 Linear Independence and Basis 
~9 
The vector 
Cl 
[x]s = 
c2 
c~ 
is called the coordinate vector of x with respect to S. 
EXAMPLE 8. 
Consider the bases 
([1] [0] [i]} 
([1][0] 
S- 
0 , 
1 , 
and 
T- 
0 , 
1 , 
0 
0 
1 
1 
for R 3. Let 
[~ 
x= 
5 
9 
3 
Expressing x in terms of the given bases, we obtain (verify) 
[11 [~ [~ 
x-0 
0 
+5 
1 
+3 0 
0 
0 
1 
and 
[1] [0] [1] 
x--2 
0 
+3 
1 
+2 
1 , 
1 
1 
1 
so that 
[x]s=[i ] 
and 
[x]r=[-i]. 
An important result that we will use several times in Chapter 1 is given 
THEOREM 0.13. 
ff S--{Vl,V2,...,u k} is a linearly independent set of 
vectors in a subspace V of R ~ of dimension m, then we may find vectors 
Yk + l , u + 2, . . . , u 
SO that {Vl, V2, . . . , u 
u + l , u + E, . . . , u 
is a basis for 111. 
A 
Theorem 0.13 says that a linearly independent set of vectors in an 
m-dimensional subspace V of R n can be extended to a basis for V. 
ZX 
in the following theorem. 

~0 
Chapter 0 
Review of Linear Algebra (Optional) 
Consider the matrix equation 
all 
a12 
... 
aln 
a21 
a22 
... 
a2n 
aml 
am2 
"." 
amn 
X 1 
bl 
x 2 
bE 
Xn 
;m 
Multiplying out the left-hand side, we obtain 
a11x1 + a12x 2 + ... + alnXn 
a21x 1 q- a22x 2 q- ... q_ a2nX n 
amlX 1 -b am2X 2 + "'" +amnXn 
(4) 
b 1 
b2 
bm 
(5) 
Using the properties of matrix addition and scalar multiplication, we may 
rewrite (5) as 
all 
a12 
a21 
a22 
Xl 
. 
+ 
X 2 
. 
+ 
... +X n 
aml 
am2 
aln 
bl 
a2n 
b2 
amn 
b m 
Thus, if (4) is written in compact form as 
Ax=b 
and A1, A2,... ,A n 
be written as 
are the columns of A, then we have shown that (4) can 
xIA 1 + x2A 2 + "'" +xnA n -- b. 
(6) 
That is, writing the matrix equation Ax = b is equivalent to writing b as a 
linear combination of the columns of A. Furthermore, if x is a solution to 
Ax = b, then the components of x are the coefficients of the columns of A 
when b is written as a linear combination of these columns. 
An important application of this discussion occurs in the case when A is 
nonsingular. In this case, the columns of A form a linearly independent set 
of n vectors in R n. Thus, this set is a basis for R n, and b can be uniquely 
written as a linear combination of the columns of A. The solution to 
Ax = b gives the coordinate vector of b with respect to this basis. That is, 
from Equation (6), the coordinates of b with respect to the basis 
{A1,A2,... ,A n} are x 1, x2,..., Xn. Since A is nonsingular, A -1 exists, and 
we may write x = A -1 b. The columns of A -1 also form a basis for R n. In 
the same manner as that for b, we can obtain the coordinates of x with 
respect to the basis of columns of A-1. These coordinates are the compo- 
nents of b. 

0.5 Linear Independence and Basis 
41 
DEFINITION. 
The rank of an m X n matrix A is the number of nonzero 
rows in the matrix in reduced row echelon form that is row equivalent to 
A. 
EXAMPLE 9. 
Find the rank of the matrix 
A .~. 
1 
2 
-1 
1 
2 
2 
3 
-2 
5 
2 
4 
7 
-4 
7 
6 
. 
3 
4 
-3 
9 
2 
-1 
0 
1 
-7 
2 
Solution. 
Transforming A to reduced row echelon form we find (verify) 
that A is row equivalent to 
B 
Hence, the rank of A is 2. 
1 
0 
-1 
7 
-2 
0 
1 
0 
-3 
2 
0 
0 
0 
0 
0 
9 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
A 
0.5 
EXERCISES 
1. Let 
s_ {I-i], I-i], I-i]}. 
Which of the following vectors are linear combinations of the vectors in S? 
[1] [6] 
[1] 
(a) 
-1 
(b) 
2 
(c) 
2 
(d) 
1 
2 
4 
-1 
1 
2. Let 
/1211 I31 
I1 311 I~ 
S= 
3' 
2' 
-8' 
" 
4 
2 
-10 
Which of the following vectors are linear combinations of the vectors in S? 
I~ I~ 
0 
(b) 
(c) 
(d) 
1 
(a) 
6 
0 
6 
-2 

~ 
Chapter 0 Review of Linear Algebra (Optional) 
3. Which of the following sets of vectors span R27 
(a) {[_~], 
[0]} 
(b){[1], 
[0], 
[3]} 
4. Which of the following sets of vectors span R37 
[F31 
1-01 [ 11/ 
/[11 
[ 31/ 

0.5 Linear Independence and Basis 
~ 
([il [i] I il) 
{[i] [i] [i]) 
 b,([il [i] [i]} 
12. Let 
s([~] [11]) 
be a basis for R 2. Find the coordinate vector [x] s of the indicated vector x with 
respect to S. 
3 
(a' x- [4 ] 
13. Let 
(b) x= [-2 
1 
1 
s {[i] [i] [il) 
be a basis for R 3. Find the coordinate vector [x] s of the indicated vector x with 
respect to S. [i] 
[1] 
[3] 
(a) x= 
(b) x-- 
2 
(c) x= 
1 
(d) x= 
2 
3 
1 
1 
14. Suppose that S 1 and S 2 are finite sets of vectors in R n and that S 1 is a subset 
of S 2. Prove the following. 
(a) If S 1 is linearly dependent, so is S 2. 
(b) If S 2 is linearly independent, so is S 1. 
15. Show that any set of vectors in R" that includes the zero vector must be 
linearly dependent. 
16. Show that any set of n + 1 vectors in R" must be linearly dependent. 
17. Show that R" cannot be spanned by a set containing fewer than n vectors. 
18. Find the rank of the matrix 
2 
3 
4 
-1 
2] 
4 
-1 
6 
-7 
-6 
. 
3 
2 
-1 
3 
4 
19. Find the rank of the matrix 
-2 
4 
2 
-10 

44 
Chapter 0 Review of Linear Algebra (Optional) 
20. Let A be an n • n matrix. Show that A is nonsingular if and only if its rank 
is n. 
Further Reading 
Kolman, Bernard. Introductory Linear Algebra with Applications, fifth ed. Macmillan, New 
York, 1993. 
Kolman, Bernard. Elementary Linear Algebra, sixth ed. Macmillan, New York, 1996. 
Strang, Gilbert. Linear Algebra and Its Applications, third ed. Harcourt Brace Jovanovich, 
Orlando, FL, 1988. 

Introduction 
to Linear 
Programming 
T 
HIS CHAPTER AND the next two, which represent the heart of this 
book, introduce the basic ideas and techniques of linear program- 
ming. This area of applied mathematics was developed in the late 
1940s to solve a number of resource allocation problems for the federal 
government. It has become an essential tool in operations research and 
has been applied to a remarkably varied number of real problems, produc- 
ing enormous savings in money and resources. In this chapter we first 
introduce the linear programming problem and then discuss a simple 
geometric solution for small problems. Finally, we connect the algebraic 
and geometric descriptions of the solutions of a linear programming 
problem. 
1.1 THE LINEAR PROGRAMMING PROBLEM 
We start by giving several examples of linear programming problems. 
45 

~ 
Chapter 1 Introduction to Linear Programming 
EXAMPLE 1 (ACTIVITY ANALYSIS OR PRODUCT MIX). 
A lumber mill 
saws both finish-grade and construction-grade boards from the logs that it 
receives. Suppose that it takes 2 hr to rough-saw each 1000 board feet of 
the finish-grade boards and 5 hr to plane each 1000 board feet of these 
boards. Suppose also that it takes 2 hr to rough-saw each 1000 board feet 
of the construction-grade boards, but it takes only 3 hr to plane each 1000 
board feet of these boards. The saw is available 8 hr per day, and the plane 
is available 15 hr per day. If the profit on each 1000 board feet of 
finish-grade boards is $120 and the profit on each 1000 board feet of 
construction-grade boards is $100, how many board feet of each type of 
lumber should be sawed to maximize the profit? 
MATHEMATICAL MODEL. Let x and y denote the amount of finish- 
grade and construction-grade lumber, respectively, to be sawed per day. 
Let the units of x and y be thousands of board feet. The number of hours 
required daily for the saw is 
2x + 2y. 
Since only 8 hours are available daily, x and y must satisfy the inequality 
2x + 2y < 8. 
Similarly, the number of hours required for the plane is 
5x + 3y, 
so x and y must satisfy 
5x + 3y < 15. 
Of course, we must also have 
x>0 
and 
y>0. 
The profit (in dollars) to be maximized is given by 
z = 120x + 100y. 
Thus, our mathematical model is: 
Find values of x and y that will 
maximize 
z = 120x + 100y 
subject to the restrictions 
2x + 2y < 8 
5x + 3y < 15 
x>0 
y>_0. 
A 
EXAMPLE :2 (THE DIET PROBLEM). A nutritionist is planning a menu 
consisting of two main foods A and B. Each ounce of A contains 2 units 

1.1 The Linear Programming Problem 
~ 
of fat, 1 unit of carbohydrates, and 4 units of protein. Each ounce of B 
contains 3 units of fat, 3 units of carbohydrates, and 3 units of protein. The 
nutritionist wants the meal to provide at least 18 units of fat, at least 12 
units of carbohydrates, and at least 24 units of protein. If an ounce of A 
costs 20 cents and an ounce of B costs 25 cents, how many ounces of each 
food should be served to minimize the cost of the meal yet satisfy the 
nutritionist's requirements? 
MATHEMATICAL MODEL. 
Let x and y denote the number of ounces of 
foods A and B, respectively, that are served. The number of units of fat 
contained in the meal is 
2x + 3y, 
so that x and y have to satisfy the inequality 
2x + 3y > 18. 
Similarly, to meet the nutritionist's requirements for carbohydrate and 
protein, we must have x and y satisfy 
x + 3y > 12 
and 
4x + 3y > 24. 
Of course, we also require that 
x>0 
and 
y>0. 
The cost of the meal, which is to be minimized, is 
z = 20x + 25y. 
Thus, our mathematical model is: 
Find values of x and y that will 
minimize 
z = 20x + 25y 
subject to the restrictions 
2x + 3y > 18 
x + 3y > 12 
4x + 3y > 24 
x>0 
y>0. 
A 
EXAMPLE 3 (THE TRANSPORTATION PROBLEM). A manufacturer of sheet 
polyethylene has two plants, one located in Salt Lake City and the other 
located in Denver. There are three distributing warehouses, one in Los 
Angeles, another in Chicago, and the third in New York City. The Salt 

40 
Chapter 1 Introduction to Linear Programming 
Lake City plant can supply 120 tons of the product per week, whereas the 
Denver plant can supply 140 tons per week. The Los Angeles warehouse 
needs 100 tons weekly to meet its demand, the Chicago warehouse needs 
60 tons weekly, and the New York City warehouse needs 80 tons weekly. 
The following tables gives the shipping cost (in dollars) per ton of the 
product: 
From 
To 
Los Angeles 
Chicago 
New York City 
Salt Lake City 
5 
7 
9 
Denver 
6 
7 
10 
How many tons of polyethylene should be shipped from each plant to each 
warehouse to minimize the total shipping cost while meeting the demand? 
MATHEMATICAL MODEL. Let P1 and P2 denote the plants in Salt Lake 
City and in Denver, respectively. Let W 1, W 2, and W 3 denote the ware- 
houses in Los Angeles, Chicago, and New York City, respectively. Let 
x u = number of tons shipped from Pi to Wj 
cq = cost of shipping 1 ton from Pi to 
for i = 1, 2 and j = 1, 2, 3. The total amount of polyethylene sent from 
P1 is 
Xll + X12 + X13. 
Since P1 can supply only 120 tons, we must have 
Xll + X12 + X13 __< 120. 
Similarly, since P2 can supply only 140 tons, we must have 
X21 -[-X22 -[--X23 __< 140. 
The total amount of polyethylene received at W1 is 
Xll -[- X21. 
Since the demand at W1 is 100 tons, we would like to have 
Xll -[- X21 ~__ 100. 
Similarly, since the demands at W E and W 3 are 60 and 80 tons, respec- 
tively, we would like to have 
X12 + X22 >__ 60 
and 
X13 "+" X23 ~__ 80. 

1.1 The Linear Programming Problem 
t~ 
Of course, we must also have 
Xij >__ 0 
for 
i = 1,2 
and 
j = 1,2,3. 
The total transportation cost, which we want to minimize, is 
Z = CllXll q- C12X12 d- C13X13 -~- C21X21 d- C22X22 d- C23X23. 
Thus, our mathematical model is: 
Find values of x11, x12, x13, x21, x22, and x23 that will 
2 
3 
minimize 
z = E 
E cijxij 
i=lj=l 
subject to the restrictions 
3 
E Xij <-- Si, 
i = 1, 2 
j=l 
2 
Exij > dj, j=1,2,3 
i=1 
Xij >_~ O, 
where available supplies are 
s 1 - 120 
and 
s 2 
and where the required demands are 
i= 1,2 
and 
j= 1,2,3 
d 1 -- 100, 
d E = 
60, 
and 
EXAMPLE 4 (A BLENDING PROBLEM). 
= 140 
d 3 -- 80. 
A 
A manufacturer of artificial 
sweetener blends 14 kg of saccharin and 18 kg of dextrose to prepare two 
new products: SWEET and LO-SUGAR. Each kilogram of SWEET con- 
tains 0.4 kg of dextrose and 0.2 kg of saccharin, whereas each kilogram of 
LO-SUGAR contains 0.3 kg of dextrose and 0.4 kg of saccharin. If the 
profit on each kilogram of SWEET is 20 cents and the profit on each 
kilogram of LO-SUGAR is 30 cents, how many kilograms of each product 
should be made to maximize the profit? 
MATHEMATICAL MODEL. Let x and y denote the number of kilograms 
of SWEET and LO-SUGAR, respectively, being made. The number of 
kilograms of dextrose being used is 
so that we must have 
0.4x + 0.3y, 
0.4x + 0.3y < 18. 

~0 
Chapter 1 Introduction to Linear Programming 
Similarly, the number of kilograms of saccharin being used is 
0.2x + 0.4y, 
so that we must have 
0.2x + 0.4y < 14. 
Of course, we also require that 
x>0 
and 
y>0. 
The total profit (in cents), which we seek to maximize, is 
z = 20x + 30y. 
Thus, our mathematical model is: 
Find values of x and y that will 
maximize 
z = 20x + 30y 
subject to the restrictions 
0.4x + 0.3y < 18 
0.2x + 0.4y < 14 
x>0 
y>_0. 
A 
EXAMPLE 5 (A FINANCIAL PROBLEM). Suppose that the financial advi- 
sor of a university's endowment fund must invest exact/y $100,000 in two 
types of securities: bond AAA, paying a dividend of 7%, and stock BB, 
paying a dividend of 9%. The advisor has been told that no more than 
$30,000 can be invested in stock BB, whereas the amount invested in bond 
AAA must be at least twice the amount invested in stock BB. How much 
should be invested in each security to maximize the university's return? 
MATHEMATICAL MODEL. Let x and y denote the amounts invested in 
bond AAA and stock BB, respectively. We must then have 
x + y = 100,000 
x>_2y 
y _< 30,000. 
Of course, we also require that 
x>_0 
and 
y>_0. 
The return to the university, which we seek to maximize, is 
z = 0.07x + 0.09y. 

1.1 The Linear Programming Problem 
~1 
Thus, our mathematical model is: 
Find values of x and y that will 
maximize 
z = 0.07x + 0.09y 
subject to the restrictions 
x + y = 100,000 
x-2y>O 
y < 30,000 
x>0 
y>_0. 
A 
Following the form of the previous examples, the general linear program- 
ming problem can be stated as follows: 
Find values of x 1, x2,..., x n that will 
maximize or minimize 
z --- ClX 1 + CEX 2 + "'" +CnX n 
(1) 
subject to the restrictions 
allX 1 + 
al2x 2 + ... + alnX n <__ (>__)(=)b 1 
a21x 1 + 
a22x 2 + ... +a2nX n <__ (>_.)(=)b 2 
.
.
.
.
 
, 
(2) 
9 
, 
~ 
. 
amlX 1 + am2X 2 + ... +amnX n <__ ( ~ )(= )b m 
where in each inequality in (2) one and only one of the symbols, <, >, 
= occurs. The linear function in (1) is called the objective function. The 
equalities or inequalities in (2) are called constraints. Note that the 
left-hand sides of all the inequalities or equalities in (2) are linear 
functions of the variables x 1, x2,..., x~, just as the objective function is. A 
problem in which not all the constraints or the objective function are 
linear functions of the variables is a nonlinear programming problem. Such 
problems are discussed in more advanced texts. 
We shall say that a linear programming problem is in standard form if 
it is in the following form: 
Find values of x 1, x2,..., x~ that will 
maximize 
z ~- ClX 1 + C2X 2 + "'" +CnX n 
(3) 
subject to the constraints 
allX 1 + 
a12x2 + ... + 
alnXn <--b 1 
a21x I + 
a22x 2 + ... + a2nX n < b 2 
.
.
.
.
 
(4) 
amlX1 + am2X2 -t-- "" +amnX n ~ b m 
xj>0, 
j= 1,2,...,n. 
(5) 
Examples 1 and 4 are in standard form. The other examples are not. Why? 

~ 
Chapter 1 
Introduction to Linear Programming 
EXAMPLE 6. 
form: 
We shall say that a linear programming problem is in canonical form if 
it is in the following form: 
Find values of Xl, x2,.-., xs that will 
maximize 
z--ClX 1 -{-C2X 2 -Jr" "'" "+'CsX s 
subject to the constraints 
allX 1 + 
al2x 2 + "" + a~sX s = b~ 
a21x 1 + 
a22x 2 + ... + a2sX s = b 2 
9 
~ 
9 
. 
amlX 1 + am2X 2 + ... +amsX s = b m 
xj>O, 
j = 1,2,...,s. 
The following linear programming problem is in canonical 
Maximize 
z=3x+2y+3u-4v 
subject to the constraints 
2x+ 
y+2u- 
v= 
4 
5x+3y 
-2v= 
15 
x>_0, 
y>_0, 
u>_0, 
v>_0. 
ZX 
Some other authors use different names for what we call standard and 
canonical linear programming problems. Some also require that all the 
variables be nonnegative in a linear programming problem. The reader 
should carefully check the definitions when referring to other books or 
papers. 
EXAMPLE 7. 
The following linear programming problems are neither 
in standard form nor in canonical form. Why? 
(a) 
Minimize z - 3x + 2y 
subject to the constraints 
2x+ 
y<4 
3x- 2y < 6 
x>_0, 
y>_0. 
(b) 
Maximize z = 2x I + 3x 2 + 4x 3 
subject to the constraints 
3x 1 + 2x 2 - 
3x 3 < 4 
2X 1 + 3x 2 + 2x 3 < 6 
3X 1 -- 
x 2 + 2x 3 > 
--8 
x 1 >__ 0, 
x 2 >_~ 0, 
X 3 >_~ 0. 

1.1 The Linear Programming Problem 
53 
(c) 
Maximize z = 3x + 2y + 3v - 2w 
subject to the constraints 
2x + 6y + 2v - 4w = 7 
3x+2y-5v+ 
w=8 
6x+7y+2v 
+5w <4 
x>O, 
y>O, 
v>O, 
w>_O. 
(d) 
Minimizez=2x+5y+u 
+v+4w 
subject to the constraints 
3x+2y- 
u 
+2w=4 
4x+5y+3u 
+2v 
=7 
x>_0, 
y>_0, 
u>_0, 
v>_0, 
w>_0. 
(e) 
Maximize z = 2x + 5y 
subject to the constraints 
3x + 2y < 6 
2x+9y<8 
x>_0. 
(f) 
Minimize z = 2 xa + 3x2 + x3 
subject to the constraints 
2x I + 
x 2 - x 3 = 4 
3x I + 2x 2 + X 3 --8 
x I - 
x 2 
= 
6 
X 1 >__ O, 
X 2 ~_~ O. 
/~ 
We shall now show that every linear programming problem that has 
unconstrained variables can be solved by solving a corresponding linear 
programming problem in which all the variables are constrained to be 
nonnegative. Moreover, we show that every linear programming problem 
can be formulated as a corresponding standard linear programming prob- 
lem or as a corresponding canonical linear programming problem. That is, 
we can show that there is a standard linear programming problem (or 
canonical linear program problem)whose solution determines a solution 
to the given arbitrary linear programming problem. 
Minimization Problem as a Maximization Problem 
Every minimization problem can be viewed as a maximization problem 
and conversely. This can be seen from the observation that 
min cixi  
i1 

54 
Chapter 1 Introduction to Linear Programming 
That is, to minimize the objective function we could maximize its negative 
instead and then change the sign of the answer. 
Reversing an Inequality 
If we multiply the inequality 
klX 1 + k2x 2 + "'" +k~x, >_ b 
by -1, we obtain the inequality 
-klX 1 - kEX 2 ..... 
knx n <_ -b. 
EXAMPLE 8. 
Consider the linear programming problem given in 
Example 7b. If we multiply the third constant, 
3x 1 --X 2 + 2X 3 > 
--8, 
by -1, we obtain the equivalent linear programming problem: 
Maximize 
z = 2X 1 + 3X 2 + 4X 3 
subject to 
3x I + 2x 2 -- 3X 3 < 4 
2x 1 + 3x 2 + 2x 3 < 6 
-- 3x 1 + 
x 2 -- 2x 3 < 
8 
X 1 >__ 0, 
X 2 >__ 0, 
X 3 >__ 0, 
which is in standard form. 
A 
Changing an Equality to an Inequality 
Observe that we can write the equation x = 6 as the pair of inequalities 
x<6 
and x>6 
and hence as the pair x<6 
and -x< 
-6. In the 
general case the equation 
n 
E aijxj = bi 
j=l 
can be written as the pair of inequalities 
n 
E aijxj <-- bi 
j=l 
tl 
~ 
- 
aijxj <_ - bi. 
]=1 
EXAMPLE 9. 
Consider the linear programming problem given 
Example 7c. It contains the two equality constraints 
2x + 6y + 2v -4w = 7 
3x+2y-5v+ 
w=8. 
in 

1.1 The Linear Programming Problem 
55 
These may be written as the equivalent four inequalities 
2x+6y+2v-4w<7 
2x + 6y + 2v - 4w > 7 
3x+2y-5v+ 
w<8 
3x+2y-5v+ 
w>8. 
Thus, we obtain the equivalent linear programming problem: 
Maximize 
z=3x+2y+3v-2w 
subject to 
2x+6y+2v-4w 
< 7 
-2x-6y-2v+4w< 
-7 
3x+2y-5v+ 
w< 
8 
- 3x- 
2y + 5v- 
w< 
-8 
6x + 7y + 2v + 5w < 4 
y>O, 
v>_O, 
w>_O, 
x>0, 
which is in standard form. 
A 
Unconstrained Variables 
The problems in Examples 7e and 7f have variables that are not 
constrained to be nonnegative. Suppose that xj is not constrained to be 
nonnegative. We replace xj with two new variables, x~ and x}-, letting 
xj = x;- x;, 
where x~ > 0 and x]-> 0. That is, any number is the difference of two 
nonnegative numbers. In this manner we may introduce constraints on 
unconstrained variables. 
EXAMPLE 10. 
Consider the problem in Example 7e. Letting y = 
y +- y-, our problem becomes the following linear programming problem: 
Maximize 
z = 2x + 5y § 
5y- 
subject to 
3x + 2y § 
2y-< 6 
2x + 9y § 
9y-< 8 
x>0, 
y+>0, 
y->0, 
which is in standard form. 
Z~ 
EXAMPLE 11. 
The problem in Example 7f can be converted to a 
maximization problem. We also let x 3 = x~-x 3. With these changes we 

56 
Chapter 1 Introduction to Linear Programming 
obtain the following problem: 
Maximize 
z -- - 2x 1 - 3x 2 - x~ + x 3 
subject to 
2x 1 + 
x 2 - x~+ x 3 = 4 
3x 1 + 2x 2 q-x~--x3= 
8 
Xl - 
x2 
- 6 
Xl >_~ 0 , 
X2 >_~ 0 , 
X~->_~ O, 
X3>_~ 0 , 
which is in canonical form. 
We have thus shown that every linear programming problem that is not 
in standard form can be transformed into an equivalent linear program- 
ming problem that is in standard form. 
Scaling 
It is not difficult to see that if both sides of one or more constraints of a 
linear programming problem are multiplied by constants, then the optimal 
solution to the new problem is identical to the optical solution to the given 
problem. This technique can be used to make all coefficients in a linear 
programming problem approximately the same size. This method, called 
scaling, will be discussed further in Section 3.6. 
A diagrammatic representation of the various types of linear program- 
ming problems is given in Figure 1.1. 
In Section 1.2 we will show how to convert a linear programming 
problem in standard form to one in canonical form. Thus, any linear 
programming problem can be put in either standard form or canonical 
form. 

1.1 The Linear Programming Problem 
57 
1.1 
EXERCISES 
In Exercises 1-11 set up a linear programming model of the situation described. 
Determine if the model is in standard form. If it is not, state what must be changed 
to put the model into standard form. 
1. Blending problem. A new rose dust is being prepared by using two available 
products: PEST and BUG. Each kilogram of PEST contains 30 g of carbaryl 
and 40 g of Malathion, whereas each kilogram of BUG contains 40 g of 
carbaryl and 20 g of Malathion. The final blend must contain at least 120 g of 
carbaryl and at most 80 g of Malathion. If each kilogram of PEST costs $3.00 
and each kilogram of BUG costs $2.50, how many kilograms of each pesticide 
should be used to minimize the cost? 
2. Equipment purchasing problem. A container manufacturer is considering the 
purchase of two different types of cardboard-folding machines: model A and 
model B. Model A can fold 30 boxes per minute and requires 1 attendant, 
whereas model B can fold 50 boxes per minute and requires 2 attendants. 
Suppose the manufacturer must fold at least 320 boxes per minute and cannot 
afford more than 12 employees for the folding operation. If a model A 
machine costs $15,000 and a model B machine costs $20,000, how many 
machines of each type should be bought to minimize the cost? 
3. Disease treatment problem. Dr. R. C. McGonigal treats cases of tactutis with a 
combination of the brand-name compounds Palium and Timade. The Palium 
costs $0.40/pill and the Timade costs $0.30/pi11. Each compound contains 
SND plus an activator. The typical dosage requires at least 10 mg of SND per 
day. Palium contains 4 mg of SND and Timade contains 2 mg of SND. In 
excessive amounts the activators can be harmful. Consequently Dr. McGonigal 
limits the total amount of activator to no more than 2 mg per day. Palium and 
Timade each contain 0.5 mg of activator per pill. How many of each pill per 
day should Dr. McGonigal prescribe to minimize the cost of the medication, 
provide enough SND, and yet not exceed the maximum permissible limit of 
activator? 
4. Agricultural problem. A farmer owns a farm that produces corn, soybeans, and 
oats. There are 12 acres of land available for cultivation. Each crop that is 
planted has certain requirements for labor and capital. These data along with 
the net profit figures are given in the accompanying table. 
,,,,,,, 
Labor (hr) 
Capital ($) 
Net profit ($) 
Corn (per acre) 
6 
36 
40 
Soybeans (per acre) 
6 
24 
30 
Oats (per acre) 
2 
18 
20 
The farmer has $360 available for capital and knows that there are 48 hr 
available for working these crops. How much of each crop should be planted to 
maximize profit? 

58 
Chapter 1 Introduction to Linear Programming 
5. Blending problem. A coffee packer blends Brazilian coffee and Colombian 
coffee to prepare two products: Super and Deluxe brands. Each kilogram of 
Super coffee contains 0.5 kg of Brazilian coffee and 0.5 kg of Colombian 
coffee, whereas each kilogram of Deluxe coffee contains 0.25 kg of Brazilian 
coffee and 0.75 kg of Colombian coffee. The packer has 120 kg of Brazilian 
coffee and 160 kg of Colombian coffee on hand. If the profit on each kilogram 
of Super coffee is 20 cents and the profit on each kilogram of Deluxe coffee is 
30 cents, how many kilograms of each type of coffee should be blended to 
maximize profit? 
6. Air pollution problem. Consider an airshed in which there is one major 
contributor to air pollution--a cement-manufacturing plant whose annual 
production capacity is 2.500,000 barrels of cement. Figures are not available to 
determine whether the plant has been operating at capacity. Although the kilns 
are equipped with mechanical collectors for air pollution control, the plant still 
emits 2.0 lb of dust per barrel of cement produced. There are two types of 
electrostatic precipitators that can be installed to control dust emission. The 
four-field type would reduce emissions by 1.5 lb of dust/barrel and would cost 
$0.14/barrel to operate. The five-field type would reduce emissions by 1.8 lb of 
dust/barrel and would cost $0.18/barrel to operate. The EPA requires that 
particulate emissions be reduced by at least 84%. How many barrels of cement 
should be produced using each new control process to minimize the cost of 
controls and still meet the EPA requirements1? 
7. Mixing problem. The R. H. Lawn Products Co. has available 80 metric tons of 
nitrate and 50 metric tons of phosphate to use in producing its three types of 
fertilizer during the coming week. The mixture ratios and profit figures are 
given in the accompanying table. Determine how the current inventory should 
be used to maximize the profit. 
Metric tons / 1000 bags 
Profit 
Nitrate 
Phosphate 
($/1000 bags) 
Regular lawn 
4 
2 
300 
Super lawn 
4 
3 
500 
Garden 
2 
2 
400 
8. Investment problem. The administrator of a $200,000 trust fund set up by Mr. 
Smith's will must adhere to certain guidelines. The total amount of $200,000 
need not be fully invested at any one time. The money may be invested in three 
different types of securities: a utilities stock paying a 9% dividend, an electron- 
ics stock paying a 4% dividend, and a bond paying 5% interest. Suppose that 
the amount invested in the stocks cannot be more than half the total amount 
invested; the amount invested in the utilities stock cannot exceed $40,000; and 
the amount invested in the bond must be at least $70,000. What investment 
policy should be pursued to maximize the return? 
1Kohn, Robert E. "A Mathematical Programming Model for Air Pollution Control." 
School Sci. Math. (June 1969). 

1.1 The Linear Programming Problem 
~ 
9. A book publisher is planning to bind the latest potential bestseller in three 
different bindings: paperback, book club, and library. Each book goes through 
a sewing and gluing process. The time required for each process is given in the 
accompanying table. 
Paperback 
Book club 
Library 
Sewing (min) 
2 
2 
3 
Gluing (min) 
4 
6 
10 
Suppose the sewing process is available 7 hr per day and the gluing process 10 
hr per day. Assume that the profits are $0.50 on a paperback edition, $0.80 on 
a book club edition, and $1.20 on a library edition. How many books will be 
manufactured in each binding when the profit is maximized? 
10. Major oil companies use linear programming to model many phases of their 
operations. Consider the following simplified version of part of a refinery 
operation. Two kinds of aviation gasoline, high octane and low octane, are 
made by blending four components from the refinery output. For the low- 
octane gasoline the components are augmented with a small amount of 
tetraethyllead (TEL) to give the low-octane ratings shown in the accompanying 
table. The high-octane gasoline is made from the same components when these 
have been augmented with a larger amount of TEL, giving the high-octane 
ratings in the table. Assume that the octane rating (OR) of the mixture is the 
volumetric average of the octane ratings of the components. That is, letting V 
denote volume, we have 
ORmi x - 
OR compl X Vcompl + OR comp2 X Vcomp 2 + "'" 
Vcompl + Vcomp2 + ..- 
The vapor pressure (a measure of the tendency of the gasoline to evaporate) of 
both gasolines must be 7. Assume that the vapor pressure of a mixture is the 
volumetric average of the vapor pressures of the components. Vapor pressure 
and octane rating are the only two physical properties for which there are 
constraints. Data for the components and desired mixtures are given in the 
accompanying table. 
Vapor 
pressure 
OR 
High Low Demand Supply Revenue Cost 
Component 
Alkylate 
5 
Catalytic cracked 
6.5 
Straight run 
4 
Isopentane 
18 
Mixture 
High octane 
7 
Low octane 
7 
108 
98 
94 
87 
87 
80 
108 
100 
100 
90 
1300 
800 
700 
7.20 
600 
4.35 
900 
3.80 
500 
4.30 
6.50 
7.50 

60 
Chapter 1 Introduction to Linear Programming 
Assume that the demands must be met exactly. Measure the profit by using 
revenue less cost. Set up a model of this situation that maximizes this measure 
of profit. 
11. A local health food store packages three types of snack foodsmChewy, 
Crunchy, and Nuttymby mixing sunflower seeds, raisins, and peanuts. The 
specifications for each mixture are given in the accompanying table. 
Selling price 
Mixture 
Sunflower seeds 
Raisins 
Peanuts 
per kilogram ($) 
Chewy 
At least 60% 
At most 20% 
2.00 
Crunchy 
At least 60% 
1.60 
Nutty 
At most 20% 
At least 60% 
1.20 
The suppliers of the ingredients can deliver each week at most 100 kg of 
sunflower seeds at $1.00/kg, 80 kg of raisins at $1.50/kg, and 60 kg of peanuts 
at $0.80/kg. Determine a mixing scheme that will maximize the store's profit. 
1.1 
PROJECTS 
1. Feed problem. A laboratory needs to supply its research dogs a mixture of 
commercially available dog foods that meet the National Research Council 
(NRC) nutrient requirements (Table 1.1). The nutritional composition of each 
of the eight available foods is given in Table 1.2. Note that these data are given 
in terms of percentages. 
(a) Set up a constraint for each of the food constituents listed in Table 1.1 
based on the NRC requirements. 
(b) An additional constraint must be provided, because the requirements are 
given in terms of percentages. It must say that the sum of the amounts used 
is 1. That is, each variable represents a fraction of the total amount to be 
blended. Write this constraint. 
TABLE 1.1 National Research Council Nutrient 
Requirements 
Food must have at least 
(%) 
Food must have at most 
(%) 
Protein 
20 
Fat 
5 
Linoleic acid 
1.4 
Calcium 
1 
Phosphorus 
0.8 
Potassium 
0.5 
Salt 
1 
Magnesium 
0.4 
NFE 
25 
Fiber 
8 
Moisture 
5 

1.1 The Linear Programming Problem 
~1 
TABLE 1.2 Dog Food Constituents (Percentage by Weight) 
Wayne 
Purina 
Purina 
Purina 
Agway 
Wayne 
TW 
Meal 
Chow 
HP 
Gaines 
Burgerbits 
2000 
Protein 
25.0 
24.0 
27.0 
23.8 
26.0 
21.0 
23.0 
25.5 
Fat 
8.0 
9.0 
10.5 
9.4 
10.0 
8.0 
7.0 
10.5 
Linoleic acid 
2.1 
1.6 
1.6 
1.6 
1.6 
0.9 
1.5 
1.5 
Calcium 
2.15 
1.20 
2.50 
1.75 
1.60 
1.0 
1.50 
1.50 
Phosphorus 
1.43 
1.00 
1.40 
1.03 
1.20 
0.80 
0.80 
1.70 
Potassium 
0.73 
0.98 
0.80 
0.71 
0.90 
0.50 
0.50 
0.69 
Salt 
1.15 
1.15 
0.78 
0.64 
1.10 
1.00 
1.50 
1.00 
Magnesium 
0.170 
0.220 
0.290 
0.270 
0.150 
0.036 
0.050 
0.230 
Fiber 
3.5 
4.7 
4.3 
3.7 
4.0 
5.0 
5.0 
2.9 
NFE 
45.77 
46.15 
41.83 
48.10 
41.45 
51.76 
47.15 
45.28 
Moisture 
10.0 
10.0 
9.0 
9.0 
12.0 
10.0 
12.0 
9.2 
Cost ($/kg) 
0.17 
0.17 
0.17 
0.16 
0.21 
0.20 
0.17 
0.16 
(c) Set up the objective function using the cost data given in Table 1.2. 
(d) Consider an arbitrary constraint, 
alx 1 + a2x 2 + .." +a8x 8 > b. 
(1) 
Using the constraint in b, show that the constraint in (1) is automatically 
satisfied if, for all i, a i > b. Similarly, show that (1) is impossible to satisfy if, 
for all i, a i < b. Formulate and prove similar results if the inequality in (1) 
is reversed. 
(e) Using the results in d, identify the redundant constraints and the impossible 
constraints in a. Rewrite the model, eliminating impossible and redundant 
constraints. 
(f) Discuss why Xl = x2 ..... 
x 7 = 0, x 8 = 1, is an optimal solution. 
2. Advertising. The advertising programs of major companies are designed to 
achieve certain goals in the hope of stimulating sales. There are many media 
that accept advertising, and the company must decide how to allocate its 
advertising budget among the different media to achieve the greatest possible 
benefit. To aid in making this type of decision, there are a number of research 
firms that collect data concerning the audience of each medium. Suppose a car 
manufacturer, who requires a four-color one-page unit in a weekly magazine, is 
presented by the research firm with the accompanying table representing 
readership characteristics and advertising limitations of three different weekly 
magazines. 
TV Guide 
Newsweek 
Time 
Cost per four-color one-page unit ($) 
Total male readers per unit 
Men 50 years or older per unit 
Men who are college graduates per unit 
55,000 
35,335 
49,480 
19,089,000 
11,075,000 10,813,000 
4,312,000 
2,808,000 
2,714,000 
2,729,000 
3,387,000 
3,767,000 

~ 
Chapter 1 Introduction to Linear Programming 
The advertising manager has a monthly budget limitation of $200,000 and must 
decide what amount to spend for each magazine. Because she is worried about 
the possible duplication of TV Guide with her television advertising schedule, 
she decides to limit TV Guide to a maximum of two advertising units. She can 
use as many as four advertising units per month in each of Newsweek and Time. 
Each time a person reads a magazine, it is counted as an exposure to the 
advertising in the magazine. The advertising manager wants to obtain at least 
12,000,000 exposures to men who are college graduates, and, because men 50 
years and older are not good prospects for her products, she wants to limit the 
number of exposures to no more than 16,000,000 such men. Set up a linear 
programming model to determine how many advertising units the advertising 
manager should buy in each magazine if she wants to keep within her budget 
and wants to maximize the total number of male readers. 
3. Construction problem. A private contractor has five machines that are capable 
of doing excavation work available at certain times during the day for a period 
of one week. He wants to determine which combination of machines he should 
use in order to get the job done the cheapest way. The size of the excavation is 
1000 cubic yards of material, and the material has to be removed in one week's 
time. His machine operators will work at most an 8-hr day, 5 days per week. In 
the accompanying table is the capacity of each machine, the cycle time for each 
machine (the time it takes the machine to dig to its capacity and move the 
excavated material to a truck), the availability, and the cost (which includes 
wages for the operator). 
Time needed to 
Capacity 
Rate 
Availability 
excavate I unit of 
Machine 
(cubic yard) 
($/hr) 
(hr/day) 
capacity (rain) 
Shovel dozer 
2 
17.50 
6.0 
4.25 
Large backhoe 
2.5 
40.00 
6.0 
1.00 
Backhoe A 
1.5 
27.50 
6.0 
1.00 
Backhoe B 
1 
22.00 
8.0 
1.00 
Crane with clamshell 
1.5 
47.00 
5.5 
2.25 
Set up a linear programming problem to determine what combination of 
machines to use to complete the job at minimum cost. From the data given you 
will have to compute the number of cubic yards each machine can excavate in 1 
hr. Remember to include a constraint that says that the job must be finished. 
(What would the solution be if this last constraint were not included?) 
4. Literature search. Among the many journals that deal with linear programming 
problems are Operations Research, Management Science, Naval Logistics Re- 
search Quarterly, Mathematics in Operations Research, Operational Research Quar- 
terly, and the Journal of the Canadian Operational Research Society. Write a short 
report on a paper that appears in one of these journals and describes a real 
situation. The report should include a description of the situation and a 
discussion of the assumptions that were made in constructing the model. 

1.2 Matrix Notation 
63 
1.2 MATRIX NOTATION 
It is convenient to write linear programming problems in matrix nota- 
tion. Consider the standard linear programming problem: 
Maximize 
z = ClX 1 + c2x 2 + ... +c~x~ 
(1) 
subject to 
allX 1 + 
a12x2 + 
-.. + 
alnX n <_ b 1 
a21x I + 
a22x2 + 
... + a2nX n <_ b 2 
.
.
.
.
 
(2) 
9 
9 
~ 
. 
9 
9 
. 
9 
amlX 1 -Jr- am2X2 + "'" -Jr amnX n ~__ b m 
xj > O, 
j- 
1,2,...,n. 
(3) 
Letting 
all 
a12 
... 
aln 
a21 
a22 
9 
a2n 
A-- 
. 
. 
. 
, 
x- 
aml 
am2 
9 
amn 
bl 
c 1 
b2 
C 2 
b= 
. 
, 
andc= 
. 
, 
b m 
C n 
X 1 
X2 
9 
9 
xn 
we can write our given linear programming problem as: 
Find a vector x ~ R n that will 
maximize 
z = cTx 
(4) 
subject to 
Ax _< b 
(5) 
x > 0. 
(6) 
Here, writing v < w for two vectors v and w means that each entry of v is 
less than or equal to the corresponding entry of w. Specifically, x > 0 
means that each entry of x is nonnegative. 
EXAMPLE 1. 
The linear programming problem in Example 1 of Section 
1.1 can be written in matrix form as follows: 
Find a vector x in R 2 that will 
maximize 
z= [120 
100] y] 

~ 
Chapter 1 Introduction to Linear Programming 
subject to 
2 2Ix 
3 
Y 
ly] 
8 
>_0. 
A 
DEFINITION. 
A vector x ~ R n satisfying the constraints of a linear 
programming problem is called a feasible solution to the problem. A 
feasible solution that maximizes or minimizes the objective function of a 
linear programming problem is called an optimal solution. 
A 
Consider the linear programming problem in Example 1. 
1 
2] 
and 
Ii 3 
' 
X2= 
1 ' 
EXAMPLE 2. 
The vectors 
Xl 
are feasible solutions. For example, 
2 
2 
6 
8 
1]- 
1 
and 
[2] 
>0"1 
- 
Therefore, x 2 is a feasible solution. The vectors x 1 and x 3 can be checked 
in the same manner. Also, the same technique can be used to show that 
[1 
[2] 
3 
and 
x = 
x4- 
1 
s 
2 
are not feasible solutions. Moreover, 
-2 
x6= I 
3] 
is not a feasible solution because one of its entries is negative. Later we 
will show that 
X0 -- 
is an optimal solution to the problem. 
[3] 
5 
A 
We now describe the method for converting a standard linear program- 
ming problem into a problem in canonical form. To do this we must be 
able to change the inequality constraints into equality constraints. In 
canonical form the constraints form a system of linear equations, and we 
can use the methods of linear algebra to solve such systems. In particular, 
we shall be able to employ the steps used in Gauss-Jordan reduction. 

1.2 Matrix Notation 
65 
Changing an Inequality to an Equality 
Consider the constraint 
ailx 1 -Jr- ai2x2 
-k- ... +ainX n ~ b i. 
(7) 
We may convert (7) into an equation by introducing a new variable, u~, and 
writing 
ailXl 
-I- ai2x 2 -Jr- "" +ainXn 
-I- U i ~-- b i. 
(8) 
The variable u i is nonnegative and is called a slack variable because it 
"takes up the slack" between the left side of constraint (7) and its right 
side. 
We now convert the linear programming problem in standard form 
given by (1), (2), and (3) to a problem in canonical form by introducing a 
slack variable in each of the constraints. Note that each constraint will get 
a different slack variable. In the ith constraint 
ailx I -4- ai2x2 
+ ... q-ainXn 
~ bi, 
we introduce the slack variable x~ § i and write 
ailx 1 Jr- ai2x 2 q- ... -I-ainX n + Xn+ i = b i. 
Because of the direction of the inequality, we know that Xn§ ~ > O. There- 
fore, the canonical form of the problem is: 
Maximize 
z = ClX 1 + c2x2 + ... +c,x, 
(9) 
subject to 
allX1 -k- al2x 2 -k- --" q-alnX n -k-Xn+ 1 
= b 1 
a21x I -+- a22x 2 d- "- q- a2nXn 
-Jr-Xn+ 2 
"-" b 2 
.
.
.
.
 
(~0) 
9 
o 
o 
o 
amlX 1 ~ am2X 2 -[- ... -+-amnX n 
-[-Xn+ m --b m 
x 1>_0, 
x 2>_0,..., 
x~ >_0, 
x,+ 1 >_0,..., 
x,+ m >_0. 
(11) 
The new problem has m equations in m + n unknowns in addition to the 
nonnegativity restrictions on the variables x 1, x2,..., x~, x,+ 1,--., Xn+m. 
If y- [Yl 
Y2 "'" y~]T is a feasible solution to the problem given by 
(1), (2), and (3), then we define Yn+i, i - 1,2,...,m, by 
Yn+i = bi - 
ailYl 
- 
ai2Y2 ..... 
ai.Yn, 
That is, Yn+i is the difference between the right side of the ith constraint 
in (2) and the value of the left side of this constraint at the feasible 
solution y. Since each constraint in (2) is of the < form, we conclude that 
Yn+i >-- 0, 
i = 1,2,...,m. 

~ 
Chapter I 
Introduction to Linear Programming 
Thus, [Yl 
Y2 "'" 
Yn 
Yn+l 
"'" 
Yn+m] T satisfies (10) and (11), and the 
... 
]T is a feasible solution to the problem in (9), 
vector~ = [Yl 
Y2 
Yn+m 
(10), and (11). 
... 
]T is a feasible solution to 
Conversely, suppose ~ = [yl 
Y2 
Yn +m 
the linear programming problem in canonical form given by (9), (10), and 
(11). Then clearly 
y~ > O, 
Y2 >- O,...,yn 
> O. Since 
Yn+i > O, 
i = 
1, 2,..., m, we see that 
aiaY 1 + ailY 2 + ... +ainY n <_ b~ 
for 
i = 1,2,...,m. 
Hence, y = [Yl 
Y2 "'" y,]T is a feasible solution to the linear program- 
ming problem in standard form given by (1), (2), and (3). 
The discussion above has shown that a feasible solution to a standard 
linear programming problem yields a feasible solution to a canonical linear 
programming problem by adjoining the values of the slack variables. 
Conversely, a feasible solution to a canonical linear programming problem 
yields a feasible solution to the corresponding standard linear program- 
ming problem by truncating the slack variables. 
EXAMPLE 3. 
Consider Example 1 again. Introducing the slack variables 
u and v, our problem becomes: 
Maximize 
z = 120x + 100y 
subject to 
2x + 2y + u 
= 8 
5x + 3y 
+v = 15 
x>0, 
y>_0, 
u>0, 
v>O. 
In terms of the model, the slack variable u is the difference between the 
total amount of time that the saw is available, 8 hr, and the amount of time 
that it is actually used, 2x + 2y (in hours). Similarly, the slack variable v is 
the difference between the total amount of time that the plane is available, 
15 hr, and the amount of time that it is actually used, 5x + 3y (in hours). 
We showed in Example 2 of this section that x = 2, y = 1 is a feasible 
solution to the problem in standard form. For this feasible solution we 
have 
u= 
8-2-2-2.1=2 
v=15-5.2-3-1=2. 
Thus, 
x=2, 
y=l, 
u=2, 
v=2 
is a feasible solution to the new form of the problem. 
Consider now the values 
x=l, 
y= 1, 
u=4, 
v=7. 
These values are a feasible solution to the new problem, since 
2.1+2.1+4=8 

1.2 Matrix Notation 
~ 
and 
5-1+3.1+7= 
15. 
Consequently, 
x=l, 
y=l 
is a feasible solution to the given problem. 
We will show in Example 3 of Section 1.3 that an optimal solution to 
this problem is 
3 
5 
x=2' 
Y 
2 
In canonical form this feasible solution gives the following values for u and 
U. 
3 
5 
u= 
8-2.~-2.~=0 
3 
5 
v= 15-5.~-3.~=0. 
That is, an optimal feasible solution to the canonical form of the problem 
is 
3 
5 
x=~, 
y=~, 
u=0, 
v=O. 
/x 
The linear programming problem given by (9), (10), and (11) can also be 
written in matrix form as follows. We now let 
all 
a12 
... 
aln 
1 
0 
"'" 
0 
A = 
a21 
a22 
"'" 
azn 
0 
1 
"'" 
0 
9 
~ 
~ 
~ 
~ 
~ 
' 
9 
~ 
~ 
9 
~ 
am1 
am2 
... 
amn 
0 
0 
"'" 
1 
I Xl 
7 
!- 
- 
Cl 
bl 
x2 
c2 
9 
. 
b2 
b= 
. 
, 
x= 
x n 
, 
c= 
cn 
. 
b 
Xn+l 
0 
m 
~ 
~ 
Xn+ m 
0 
_ 
- 
_ 
- 
Then this problem can be written as: 
Maximize 
z = e T 
subject to 
Ax=b 
x>_O. 
(12) 
(13) 
(14) 
Note that this problem is in canonical form. 

68 
Chapter 1 Introduction to Linear Programming 
EXAMPLE 4. 
The linear programming problem that was formulated in 
Example 3 can be written in matrix form as: 
x 
Maximize 
z= [120 
100 
0 
0] y 
u 
9 
2 
2 
1 
0 
5 
3 
0 
1 
subject to 
x 
Y 
u 
V 
Note that this problem is in canonical form. 
x 
8 
]yu ;[1 1 
u 
0 
0 
>-- 
0 " 
0 
A 
1.2 
EXERCISES 
In Exercises 1-4, write the indicated linear programming problem from Section 
1.1 in matrix form. 
1. Example 4 
2. Example 8 
3. Example 9 
4. Exercise 4 
In Exercises 5-10, convert the indicated linear programming problem from Section 
1.1 to canonical form and express this form in matrix notation. 
5. Example 7a 
6. Example 7e 
7. Example 7c 
8. Exercise 1 
9. Exercise 5 
10. Exercise 7 
11. (a) For the linear programming problem in Example 1, show that 
[1] 
and 
x =[1] 
xl= 
2 
3 
3 
are feasible solutions. Also computethe values of the objective function for 
these feasible solutions. 

1.2 Matrix Notation 
~ 
(b) Show that 
X4= 
1 
' 
X5-- 
2' 
X6= 
3 
are not feasible solutions. 
12. Write the following linear programming problem in canonical form. 
[Xl] 
Maximize 
z-[2 
3 
5] x2 
X3 
subject to 
3 
2 
1 
1 
1 
-2 
2 
5 
4 ][Xl] [ 5] 
X2 
---< 
8 
X 3 
10 
[Xl] 
x2 
> O. 
X3 
13. Consider the linear programming problem 
Maximize 
z=x+4y 
subject to 
3x + 4y < 21 
x+2y<12 
x>0, 
y>0. 
Let u and v be the slack variables in the first and second inequalities, 
respectively. 
(a) Determine, if possible, a feasible solution to the canonical form problem 
in which u=3andv=4. 
(b) Determine, if possible, a feasible solution to the canonical form problem 
in which u= 18andv= 
10. 
14. Consider the linear programming problem 
Maximize 
z -- 2x + 5y 
subject to 
2x + 3y < 10 
5x + y < 12 
x + 5y < 15 
x>_O, 
y>_O. 
[ 1 ] is a feasible solution. 
(a) Verify that x = 
2 
(b) For the feasible solution in a, find the corresponding values of the slack 
variables. 

70 
Chapter 1 
Introduction to Linear Programming 
15. Consider the linear programming problem 
Maximize 
z = cTx 
subject to 
Ax<b 
x>_O. 
If x~ and x z are feasible solutions, show that 
1 
2 
X 
-- 
~X 1 -I- 
~X 2 
is a feasible solution. 
16. Generalize the previous exercise to show that 
x ---- rx 1 -I- SX 2 
is a feasible solution if r + s = 1. 
1.3 GEOMETRY OF LINEAR PROGRAMMING PROBLEMS 
In this section we consider the geometry of linear programming prob- 
lems by first looking at the geometric interpretation of a single constraint, 
then at a set of constraints, and finally at the objective function. These 
ideas give rise to a geometric method for solving a linear programming 
problem that is successful only for problems with two or three variables. 
However, the geometric concepts that we discuss can be built into an 
algebraic algorithm that can effectively solve very large problems. After 
casting the geometric ideas in a linear algebra setting, we will present this 
algorithmmthe simplex methodmin Section 2.1. 
Geometry of a Constraint 
A single constraint of a linear programming problem in standard form, 
say the ith one, 
ailx I -t- ai2x2 -t- "" +ainX n < bi, 
can be written as 
where 
aTx < b i, 
a T -- [ail 
ai2 
... 
ain]. 
The set of points x = (x 1, x2,..., x,) in R ~ that satisfy this constraint is 
called a closed half-space. If the inequality is reversed, the set of points 
x = (Xl, x2,..., Xn) in R n satisfying 
aTx > b i 
is also called a closed half-space. 

1.3 Geometry of Linear Programming Problems 
71 
EXAMPLE 1. 
space 
Consider the constraint 2x + 3y < 6 and the closed half- 
- 
([y] 
3 [y] 6 / 
which consists of the points satisfying the constraint. Note that the points 
(3, 0) and (1, 1) satisfy the inequality and therefore are in H. Also, the 
points (3, 4) and (-1, 3) do not satisfy the inequality and therefore are not 
in H. Every point on the line 2x + 3y = 6 satisfies the constraint and thus 
lies in H. 
A 
We can graph a closed half-space in R E by graphing the line and then 
using a test point to decide which side of the line is included in the 
half-space. A simple way to graph the line is to find its x- and y-intercepts. 
By setting y - 0 in the equation of the line and solving for x, we obtain 
the x-intercept and plot it on the x-axis. Similarly, by setting x = 0 in the 
equation of the line and solving for y, we obtain the y-intercept and plot it 
on the y-axis. We now connect the two points to sketch the graph of the 
line. To choose a test point, we check whether the origin is on the line. If it 
is not, we use it as a test point, checking whether the origin satisfies the 
inequality. If it does, the side of the line containing the origin (our test 
point) is the closed half-space H. If it does not, then the other side of the 
line is the closed half-space. If the origin is on the line, some other point 
not on the line must be selected as the test point. Some possible choices 
are (1, 0), (0, 1), or (1, 1). 
EXAMPLE 1 
(continued). 
We compute the x-intercept to be x = 3 
and the y-intercept to be y = 2. These points have been plotted and the 
line connecting them has been drawn in Figure 1.2a. Since the origin does 
not lie on the line 2x + 3y = 6, we use the origin as the test point. The 
coordinates of the origin satisfy the inequality, so that H lies below the 
line and contains the origin as shown in Figure 1.2b. 
A 
FIGURE 1.2 Closed half-space in two dimensions. 

72 
Chapter 1 Introduction to Linear Programming 
T 
z 
~(0,0,4) 
20 
(5,0, 
FIGURE 1.3 Closed half-space in three dimensions. 
EXAMPLE 2. 
The constraint in three variables, 4x + 2y + 5z < 20, 
defines the closed half-space H in R 3, where 
H = 
[4 
2 
5] 
< 20 . 
We can graph H in R 3 by graphing the plane 4x + 2y + 5z = 20 and 
checking a test point. To graph the plane, we graph the intersection of the 
plane with each of the coordinate planes. These intersections are lines in 
the coordinate planes. Thus, letting z = 0 yields the line 4x + 2y = 20 in 
the xy plane, which can be graphed as described in Example 1. Similarly, 
letting y = 0 yields the line 4x + 5z = 20 in the xz plane. Finally, setting 
x = 0 yields the line 2y + 5z = 20 in the yz plane. The graph of the plane 
containing these lines is shown in Figure 1.3. The origin does not lie on the 
plane and thus can be used as a test point. It satisfies the inequality so that 
the closed half-space contains the origin as shown in Figure 1.3. 
A 
In more than three dimensions, it is impossible to sketch a closed 
half-space. However, we can think about the geometry of closed half-spaces 
in any dimension and use the lower dimension examples as models for our 
computations. 
A typical constraint of a linear programming problem in canonical form 
has the equation 
aTx = b. 
(1) 

1.3 Geometry of Linear Programming Problems 
~ 
Its graph in R n is a hyperplane. If this equation were an inequality, 
namely, 
aTx _< b, 
then the set of points satisfying the inequality would be a closed half-space. 
Thus, a hyperplane is the boundary of a closed half-space. Intuitively, it 
consists of the points that are in the half-space, but on its edge. 
EXAMPLE 3. 
The equation 4x + 2y + 5z = 20 defines a hyperplane 
in R 3. The graph of this hyperplane, which is really a plane in this case, is 
shown in Figure 1.3. The hyperplane H is the boundary of the closed 
half-space H 1 defined by the inequality 4x + 2y + 5z < 20, considered in 
^ 
Example 2. The half-space H 1 extends below the hyperplane H and lies 
behind the page. We also see that H is the boundary of the closed 
half-space H 2 defined by the inequality 4x + 2y + 5z > 20. The half-space 
H 2 extends above the hyperplane and reaches out of the page. 
/x 
The hyperplane H defined by (1) divides R ~ into the two closed 
half-spaces 
Hi= {x ~ R" la T _<b} 
and 
H2= {x~RnlaT >b}. 
We also see that H 1 n H 2 = H, the original hyperplane. In other words, a 
hyperplane is the intersection of two closed half-spaces. 
Recall from Section 1.2 that a feasible solution to a linear programming 
problem is a point in R n that satisfies all the constraints of the problem. It 
then follows that this set of feasible solutions is the intersection of all the 
closed half-spaces determined by the constraints. Specifically, the set of 
solutions to an inequality ( < or > ) constraint is a single closed half-space, 
whereas the set of solutions to an equality constraint is the intersection of 
two closed half-spaces. 
Sketch the set of all feasible solutions satisfying the set of 
EXAMPLE 4. 
inequalities 
2x + 3y < 6 
-x+2y<4 
x>0 
y>0. 
Solution. 
The set of solutions to the first inequality, 2x + 3y _< 6, is 
shown as the shaded region in Figure 1.4a and the set of solutions to the 

~ 
Chapter 1 Introduction to Linear Programming 
second inequality, -x + 2y < 4, form the shaded region in Figure 1.4b. In 
determining these regions, we have used the origin as a test point. The 
regions satisfying the third and fourth constraints are shown in Figures 
1.4c and 1.4d, respectively. The point (1, 1) was used as a test point to 
determine these regions. The intersection of the regions in Figures 
1.4a-l.4d is shown in Figure 1.4e; it is the set of all feasible solutions to 
the given set of constraints. 
/x 

1.3 Geometry of Linear Programming Problems 
75 
3 
)" 
FIGURE 1.5 Set of all feasible solutions (three dimensions). 
EXAMPLE 5. 
Using the same technique as in the previous example, we 
find the sketch of the region in R 3 defined by the inequalities 
x>0 
y>0 
z>0 
5x+3y+5z< 
15 
10x + 4y + 5z < 20. 
The first three inequalities limit us to the first octant of R 3. The other two 
inequalities define certain closed half-spaces. The region, which is 
the intersection of these two half-spaces in the first octant, is shown in 
Figure 1.5. 
Geometry of the Objective Function 
The objective function of any linear programming problem can be 
written as 
cTx. 
If k is a constant, then the graph of the equation 
cXx = k 
is a hyperplane. Assume that we have a linear programming problem that 
asks for a maximum value of the objective function. In solving this 
problem, we are searching for points x in the set of feasible solutions for 
which the value of k is as large as possible. Geometrically we are looking 

7~ 
Chapter 1 Introduction to Linear Programming 
for a hyperplane that intersects the set of feasible solutions and for which 
k is a maximum. The value of k measures the distance from the origin to 
the hyperplane. We can think of starting with very large values of k and 
then decreasing them until we find a hyperplane that just touches the set 
of feasible solutions. 
EXAMPLE 6. 
Consider the linear programming problem 
Maximize 
z=4x+3y 
subject to 
x+ 
y<4 
5x + 3y < 15 
x>0, 
y>0 
The set of feasible solutions (the shaded region) and the hyperplanes 
z=9, 
z= 12, 
z= ~, 
and 
z= 15 
are shown in Figure 1.6. Note that it appears that the maximum value of 
5 
the objective function is -~, which is obtained when x = 3, y = 3- This 
conjecture will be verified in a later section. 
A 
A linear programming problem may not have a solution if the set of 
feasible solutions is unbounded. In the following example, we are asked to 
maximize the value of the objective function, but we discover that no such 
maximum exists. 
FIfURE 1.6 Objective function hyperplanes (two dimensions). 

1.3 Geometry of Linear Programming Problems 
~ 
EXAMPLE 7. 
Consider the linear programming problem 
Maximize 
z=2x+5y 
subject to 
-3x + 2y < 6 
x+2y>2 
x>0, 
y>0 
The graph of the set of feasible solutions is shown as the shaded region in 
Figure 1.7. We have also drawn the graphs of the hyperplanes 
z=6, 
z= 14, 
and 
z=20. 
We see that in each case there are points that lie to the right of the 
hyperplane and that are still in the set of feasible solutions. Evidently the 
value of the objective function can be made arbitrarily large. 
A 
EXAMPLE 8. 
Consider a linear programming problem that has the 
same set of constraints as in Example 7. However, assume that it is a 
minimization problem with objective function 
z = 3x + 5y. 
We have drawn the graph of the set of feasible solutions in Figure 1.8 (the 
shaded region) and the hyperplanes 
z=6, 
z=9, 
and 
z= 15. 
It appears that the optimum value of the objective function is z = 5 which 
is obtained when x = 0, y = 1. Smaller values of the objective function, 
such as z = 3, yield graphs of hyperplanes that do not intersect the set of 

78 
Chapter 1 Introduction to Linear Programming 
FIGURE 1.8 Minimization (two dimensions). 
feasible solutions. You may want to sketch the hyperplanes corresponding 
to z = 3 and z = 5. 
A 
Geometry of the Set of Feasible Solutions 
We now explore the question of where in the set of feasible solutions 
we are likely to find a point at which the objective function takes on its 
optimal value. We first show that if x 1 and x 2 are two feasible solutions, 
then any point on the line segment joining these two points is also a 
feasible solution. The line segment joining x I and x 2 is defined as 
{x ~ R n Ix = ,~x I q- (1 -/~)x2, 
0 __< A __< 1}. 
Observe that, if A = 0, we get x 2 and, if A = 1, we get x 1. The points of 
the line segment at which 0 < A < 1 are called the interior points of the 
line segment, and x 1 and x 2 and called its end points. 
Now suppose that x 1 and x 2 are feasible solutions of a linear program- 
ming problem. If 
aTx < b i 
is a constraint of the problem, then we have 
8TXl _~ b i 
and 
aTx2 __< b i. 

1.3 Geometry of Linear Programming Problems 
~ 
For any point x = Ax 1 + (1 - 
)k)x2, 0 _~< ,~ _~< 1, on the line segment joining 
X 1 and x 2, we have 
aTx = aT(AXl + (1 - A)x 2) 
= AaTXl + (1 - A)aTx2 
<_ Ab i+ (1-A)b i 
--- bi . 
Hence, x also satisfies the constraint. This result also holds if the inequal- 
ity in the constraint is reversed or if the constraint is an equality. Thus, the 
line segment joining any two feasible solutions to a linear programming 
problem is contained in the set of feasible solutions. 
Consider now two feasible solutions x I and x 2 to a linear programming 
problem in standard form with objective function cTx. If the objective 
function has the same value k at x I and x 2, then proceeding as above we 
can easily show that it has the value k at any point on the line segment 
joining x I and x 2 (Exercise 32). Suppose that the value of the objective 
function is different at x~ and x 2 and say 
cTx 1 < cTX 2" 
If x = Ax I + (1 - A)x 2, 0 < A < 1, is any interior point of the line seg- 
ment joining xl and x2, then 
cTx--" cT(,~Xl + (1 -- X)X 2) 
= XcTxl + (1 -- A)cTx2 
< AcTx2 + (1 -- A)cTx2 
= cTx2 . 
That is, the value of the objective function at any interior point of the line 
segment is less than its value at one end point. In the same manner we 
may show that the value of the objective function at any interior point of 
the line segment is greater than its value at the other end point (verify). 
Summarizing, we conclude that, on a given line segment joining two 
feasible solutions to a linear programming problem, the objective function 
either is a constant or attains a maximum at one end point and a minimum 
at the other. Thus, the property that a set contains the line segment joining 
any two points in it has strong implications for linear programming. The 
following definition gives a name to this property. 
DEFINITION. A subset S of R n is called convex if for any two distinct 
points x I and x 2 in S the line segment joining x I and x2 lies in S. That is, 

80 
Chapter 1 Introduction to Linear Programming 
S is convex if, whenever x I and x 2 ~ S, so does 
x-- AX 1 + (1 - 
A)x 2 
for 
0 < A < 1. 
A 
EXAMPLE 9. 
The sets in R E in Figures 1.9 and 1.10 are convex. The 
sets in R E in Figure 1.11 are not convex. 
/x 
The following results help to identify convex sets. 
THEOREM 1.1. 
A closed half-space is a convex set. 
Proof. 
Let the half-space H 1 be defined by cTx _< k. Let x 1 and 
X 2 ~ H i and consider x = Ax 1 + (1 - 
A)x2, (0 < A < 1). 
Then 
cTx = cT[,~X 1 "J- (1 - 
,~)x2] 
-- AcTx1 + (1 - 
A)cTx2 . 

1.3 Geometry of Linear Programming Problems 
81 
Since h >_ 0 and 1 - h >_ O, we obtain 
Thus, 
cTx<Ak+ 
(1-A)k=k. 
cTx _< k, 
so that x ~ H 1. 
A 
THEOREM 1.2. 
A hyperplane is a convex set. 
Proof 
Exercise. 
A 
THEOREM 1.3. 
The intersection of a finite collection of convex sets is 
conoex. 
Proof. 
Exercise. 
A 
THEOREM 1.4. 
Let A be an m • n matrix, and let b be a vector in R m. 
The set of solutions to the system of linear equations Ax = b, if it is not empty, 
is a convex set. 
Proof. 
Exercise. 
A 
Convex sets are of two types: bounded and unbounded. To define a 
bounded convex set, we first need the concept of a rectangle. A rectangle 
in R ~ isaset, 
R = {x ~ Rnlai < X i <_~ bi} , 
where a i < b i, i = 1, 2,..., n, are real numbers. A bounded convex set is 
one that can be enclosed in a rectangle in R n. An unbounded convex set 
cannot be so enclosed. The convex sets illustrated in Figure 1.9 are 
bounded; those in Figure 1.10 are unbounded. 
1.3 
EXERCISES 
In Exercises 1-6 sketch the convex set formed by the intersections of the 
half-space determined by the given inequalities. Also indicate whether the convex 
set is bounded. 
1. 
2. 
x+y<5 
x-y<-2 
2x + y < 8 
2x - y < O 
x>0, 
y>0 
3x+y<6 
x>_O, 
y>_O 
3. 
4. 
4x+ 
y>8 
3x+ 
y<6 
3x + 2y > 6 
2x + 3y > 4 
x>0, 
y>0 
x>0, 
y>0 

8~ 
Chapter 1 Introduction to Linear Programming 
2x+5y+5z<20 
4x+ 5y+ 4z<20 
4x+2y+ 
z<8 
20x+ 12y+ 15z<60 
x>0, 
y>0, 
z>0 
x>0, 
y>0, 
z>0 
In Exercises 7-12 sketch the set of feasible solutions to the given set of inequali- 
ties. 
11. 
7. 
8. 
-x +y < 2 
x +y < 3 
2x +y < 4 
2x +y < 4 
x>0, 
y>0 
x>0, 
y>0 
10. 
x+ 
y>3 
-x+y<2 
-3x + 2y < 6 
2x + y < 2 
x>0, 
y>0 
y<l 
x>0 
12. 
6x+4y+9z<36 
12x+6y+ 
16z<84 
2x+5y+4z<20 
8x+5y+ 
12z<60 
x>0, 
y>0, 
z>0 
x>0, 
y>0, 
z>0 
In Exercises 13-16 (a) sketch the set of feasible solutions to the given linear 
programming problem, (b) draw the objective function z = cTx = k, for the indi- 
cated values of k, and (c) conjecture the optimal value of z. 
13. Maximize z = 3x + 4y 
subject to 
k = 6, 8, 10, and 12. 
14. Maximize z = 2x + 3y 
subject to 
k = 4, 6, 8, and 10. 
15. Maximize z = 3x + y 
subject to 
k -- 2, 6, 8, and 12. 
x+3y<6 
4x + 3y < 12 
x>0, 
y>0 
x+y<4 
3x+y<6 
x+3y<6 
x>0, 
y>0 
-2x + 3y < 6 
x+y<4 
3x+y<6 
x>0, 
y>0 

1.3 Geometry of Linear Programming Problems 
8~ 
16. Maximize z = 4x 1 + 8x 2 + x 3 
subject to 
8X 1 -~- 2x 2 + 5x 3 < 68 
5x 1 + 
9x z+ 
7x 3< 120 
13x 1 + llx 2 + 43x 3 < 250 
x 1>0, 
x 2>0, 
x 3>0 
k = 80, 90, 100, and 110. 
In Exercises 17-24 determine whether the given set is convex. 

84 
Chapter 1 
Introduction to Linear Programming 
25. Prove that R" is a convex set. 
26. Prove that a subspace of R n is a convex set. 
27. Show that a rectangle in R" is a convex set. 
28. Let H 2 be the half-space in R" defined by cXx > k. Show that H 2 is convex. 
29. Show that a hyperplane H in R" is convex (Theorem 1.2). 
30. Show that the intersection of a finite collection of convex sets is convex 
(Theorem 1.3). 
31. Give two proofs of Theorem 1.4. One proof should use the definition of convex 
sets and the other should use Theorem 1.3. 
32. Consider the linear programming problem 
Maximize 
z -- cTx 
subject to 
Ax_<b 
x>_O. 
Let x~ and x 2 be feasible solutions to the problem. Show that, if the objective 
function has the value k at both x 1 and x 2, then it has the value k at any point 
on the line segment joining x I and x 2. 
33. Show that the set of all solutions to Ax < b, if it is nonempty, is a convex set. 
34. Show that the set of solutions to Ax > b, if it is nonempty, is a convex set. 
35. A function mapping R" into R m is called a linear transformation if f(u + v) 
= f(u) + f(v), for any u and v in R n, and f(ru) = rf(u), for any u in R" and r 
in R. Prove that if S is a convex set in R n and f is a linear transformation 
mapping R" into R m, then f(S) = {f(v)Iv ~ S} is a convex set. A function f 
defined on a convex set S in R" is called a convex function if 
f(Ax 1 + (1 -- A)x 2) < Af(x 1) + (1 -- A)f(x 2) 
for0_<A_< landanyx~,x2 
~S. 
36. Show that a function f defined on a convex set S in R" is convex if the line 
segment joining any two points (Xl, f(xl)) and (x e, f(xe)) does not lie below its 
graph. (See Figure 1.12.) 
37. Show that the objective function z = cXx of a linear programming problem is a 
convex function. 
f(x2) 
f(xl) 
t 
I 
x 1 
x2 
FIGURE 1.12 

1.4 The Extreme Point Theorem 
85 
1.4 THE EXTREME POINT THEOREM 
We continue in this section toward our goal of understanding the 
geometry of a linear programming problem. We first combine the results 
of the last section to describe the geometry of a general linear program- 
ming problem and then introduce the concept of an extreme point, or 
comer point. These become candidates for solutions to the problem. 
We now consider a general linear programming problem. The graph of 
each constraint defined by an inequality is a closed half-space. The graph 
of each constraint defined by an equality is a hyperplane, or intersection of 
two closed half-spaces. Thus, the set of all points that satisfy all the 
constraints of the linear programming problem is exactly the intersection 
of the closed half-spaces determined by the constraints. From the previous 
results we see that this set of points, if it is nonempty, is a convex set, 
because it is the intersection of a finite number of convex sets. In general, 
the intersection of a finite set of closed half-spaces is called a convex 
polyhedron, and thus, if it is not empty, the set of feasible solutions to a 
general linear programming problem is a convex polyhedron. 
We now turn to describing the points at which an optimal solution to a 
general linear programming problem can occur. We first make the follow- 
ing definition. 
DEFINITION. A point x ~ R" is a convex 
combination 
of the points 
Xl, X2,... ,x r in R" if for some real numbers c 1, c2,..., c r which satisfy 
~C i -- 1 
and 
C i > 0, 
1 < i < r, 
i=l 
we have 
4. 
X -- 
2__, CiX i. 
i=1 
THEOREM 1.5. 
The set of all convex combinations of a finite set of points 
in R n is a convex set. 
Proof 
Exercise. 
/x 
DEFINITION. A point u in a convex set S is called an extreme point of 
S if it is not an interior point of any line segment in S. That is, u is an 
extreme point of S if there are no distinct points x I and x 2 in S such that 
U--" ,~X 1 d- (1 - 
A)x2, 
0 < A < 1. 
EXAMPLE 1. 
The only extreme points of the convex set in Figure 1.13 
are A, B, C, D, and E (verify). 
A 

86 
Chapter 1 Introduction to Linear Programming 
l 
c 
x 
FIGURE 1.13 
EXAMPLE 2. 
The extreme points of the convex sets shown in Figures 
1.9 and 1.10 are given in the following table: 
Figure 
Extreme points 
1.9a 
A, B, C, D 
1.9b 
A, B, C 
1.9c 
The entire edge of the ellipse 
1.9d 
A, B, C, D 
1.10a 
A 
1.10b 
O 
1.10c 
None 
1.10d 
O 
A 
THEOREM 1.6. 
Let S be a convex set in R'. A point u in S is an extreme 
point of S if and only if u is not a convex combination of other points of S. 
Proof. 
Exercise. 
A 
Since the set of all feasible solutions to a general linear programming 
problem is a convex polyhedron, it contains an infinite number of points. 
An optimal solution to the problem occurs at one of these points. But how 
do we find the right point among an infinite number? The following 
theorem, whose proof we do not give, shows that, if a linear programming 
problem has an optimal solution, then this solution must occur at an 
extreme point. Although an optimal solution can occur at a feasible 
solution that is not an extreme point, from the geometry of the situation it 
suffices to consider only extreme points. In fact, there are only a finite 
number of extreme points, but this number may be very large. In the next 

1.4 The Extreme Point Theorem 
87 
chapter we show how to search through the extreme points in an orderly 
manner to find an optimal solution after a relatively small number of steps. 
THEOREM 1.7 (EXTREME POINT). Let S be the set of feasible solutions to 
a general linear programming problem. 
1. If S is nonempty and bounded, then an optimal solution to the problem 
exists and occurs at an extreme point. 
2. If S is nonempty and not bounded and if an optimal solution to the 
problem exists, then an optimal solution occurs at an extreme point. 
3. If an optimal solution to the problem does not exist, then either S is 
empty or S is unbounded. 
/x 
EXAMPLE 3. 
in Section 1.1 
Consider the linear programming problem of Example 1 
Maximize 
z = 120x + 100y 
subject to 
2x + 2y < 8 
5x + 3y < 15 
x>0, 
y>0. 
The convex set of all feasible solutions is shown as the shaded region in 
Figure 1.14. 
The extreme points of the convex set S are (0, 0), (3, 0), (0, 4), and (3, 5). 
Since S is nonempty and bounded, the objective function attains its 
maximum at an extreme point of S (Theorem 1.7). We can find which 
extreme point is the optimal solution by evaluating the objective function 
at each extreme point. This evaluation is shown in Table 1.3. The maxi- 
mum value of z occurs at the extreme point (3, ~). Thus, an optimal 
3 
5 
solution is x = 5 and y = 5. In terms of the model this means that the 
lumber mill should saw 1500 board feet of finish-grade lumber and 2500 
FIGURE 1.14 

88 
Chapter 1 Introduction to Linear Programming 
TABLE 1.3 
Extreme point 
Value of objective function 
(x, y) 
z = 120x + l OOy 
(0,0) 
0 
(3, o) 
360 
(0, 4) 
400 
(3,5) 
430 
board feet of construction-grade lumber per day. These amounts will yield 
the maximum profit of $430 per day. 
A 
Some linear programming problems have no solution. 
EXAMPLE 4. 
Consider the linear programming problem: 
Maximize 
z=2x+5y 
subject to 
2x + 3y >_ 12 
3x + 4y _< 12 
x>0, 
y>0. 
The convex set of all feasible solutions consists of the points that lie in all 
four half-spaces defined by the constraints. The sketch of these half-spaces 
in Figure 1.15 shows that there are no such points. The set of feasible 
solutions is empty. This situation will arise when conflicting constraints are 
put on a problem. The assumptions for the model must be changed to yield 
a nonempty set of feasible solutions. 
A 

1.4 The Extreme Point Theorem 
89 
y 
~~,, 
II 
'~ 
/ 21- 
~ 
3x+5y=15 
~.~] 
I 
~ ~'~ 
-3x+2y=6 / 
~ 
~' 
Y.. 
ill 
.... 
1 
2 
"~3_ 
4 
5 ~ 
x+2~2~~ 
FIGURE 1.16 
~x 
We have already seen in Example 7 in Section 1.3 that a linear 
programming problem with an unbounded convex set of feasible solutions 
may have no finite optimal value for the objective function. On the other 
hand, a linear programming problem with an unbounded convex set of 
feasible solutions may have an optimal solution. 
EXAMPLE 5. 
In our previous discussion of this example (Example 8, 
Section 1.3), it seemed that the optimum value of the objective function 
was z = 5, which occurs when x = 0 and y = 1. We shall now show that 
this is the case. 
We divide the set of feasible solutions into two regions with the 
arbitrarily chosen hyperplane 3x + 5y = 15, as shown in Figure 1.16. All 
the points in region II satisfy 15 < 3x + 5y, and all the points in region I 
satisfy 15 > 3x + 5y. Thus, we need consider only the points in region I to 
solve the minimization problem, since it is only those points at which the 
objective function takes on values smaller than 15. Region I is closed and 
bounded and has a finite number of extreme points: (0,3), (0, 1), (2, 0), 
(5,0). Consequently, Theorem 1.7 applies. By evaluating the objective 
function at these four points, we find that the minimum value is z = 5 at 
(0,1). Note that other choices for the dividing hyperplane are possible. 
ZX 
EXAMPLE 6. 
Consider the linear programming problem 
Maximize 
z = 2x + 3y 
subject to 
x+3y< 
9 
2x + 3y < 12 
x>_O, 
y>_O. 

90 
Chapter 1 Introduction to Linear Programming 
TABLE 1.4 
Extreme point 
Value of z = 2x + 3y 
(0,0) 
0 
(0,3) 
9 
(6,0) 
12 
(3,2) 
12 
The convex set of all feasible solutions is shown in Figure 1.17. The 
extreme points and corresponding values of the objective function are 
given in Table 1.4. We see that both (6, 0) and (3, 2) are optimal solutions 
to the problem. The line segment joining these points is 
(x,y) 
= X(6,0) + (1 - A)(3,2) 
= (6A,0) + (3 - 3A,2- 2A) 
=(3+3A,2-2A) 
for 
0<A<I. 
For any point (x, y) on this line segment we have 
z=2x+3y=2(3+3A) 
+3(2-2A) 
= 6 + 6A + 6 - 6A 
= 12. 
Any point on this line segment is an optimal solution. 
A 
1.4 
EXERCISES 
In Exercises 1-12 (a) find the extreme points of the set of feasible solutions for 
the given linear programming program and (b) find the optimal solution(s). 

1.4 The Extreme Point Theorem 
~1 
1. Maximize 
z=x+2y 
subject to 
3x+ 
y<6 
3x + 4y < 12 
x>0, 
y>0. 
3. Maximize 
z=3x+y 
subject to 
-3x+ 
y>6 
3x + 5y < 15 
x>0, 
y>0. 
5. Minimize 
z=3x+5y 
subject to the same constraints 
as those in Exercise 4. 
7. Maximize 
z=2x+5y 
subject to 
2x+y>2 
x+y<8 
x+y>3 
2x + y < 12 
x>_0, 
y>_0. 
9. Maximize 
z = 2x I + 4x 2 + 3x 3 
subject to 
x I + 
x 2 + 
x 3 < 12 
x I + 3x 2 + 3x 3 < 24 
3x I + 6x 2 + 4x 3 < 90 
X 1 >_~ 0, 
X 2 ~_~ 0, 
X 3 >_~ 0. 
11. Maximize 
z = 5X 1 + 2x 2 + 3x 3 
subject to 
x I + 
x 2 + 
x 3 - 1 
2x I + 5x 2 + 3x 3 < 4 
4x I + 
x 2 + 3x 3 < 2 
X 1 >__ 0, 
X 2 >__ 0, 
X 3 >__ 0. 
13. Prove Theorem 1.5. 
14. Prove Theorem 1.6. 
2. Minimize 
z=5x-3y 
subject to 
x+2y<4 
x+3y>6 
x>0, 
y>0. 
4. Maximize 
z=2x+3y 
subject to 
3x+ 
y<6 
x+ 
y<4 
x+2y<6 
x>_O, 
y>0. 
6. Maximize z--- ix + 
subject to 
x+3y<6 
x+ 
y>4 
x>_0, 
y>_0. 
8. Maximize 
z = 2X 1 + 4X 2 
subject to 
5X 1 "4- 3X 2 + 
5X 3 < 15 
10x I + 8x 2 -1- 15x 3 < 40 
x 1 >_. 0, 
x 2 >_~ 0, 
X 3 >_~ 0~ 
10. Minimize 
z = 2Xl + 3x2 + x3 
subject to the same constraints 
as those in Exercise 9. 
12. Minimize 
z = 2Xl + x3 
subject to 
X 1%- X 2 -~- 
X 3 -- 1 
2x 1 + x 2 + 2x 3 >__ 3 
X 1 >_~ 0, 
X 2 ~_~ 0, 
X 3 ~ 0. 
15. Show that a set S in R" is convex if and only if every convex combination of a 
finite number of points in S is in S. 

92 
Chapter I 
Introduction to Linear Programming 
16. Show that if the optimal value of the objective function of a linear program- 
ming problem is attained at several extreme points, then it is also attained at 
any convex combination of these extreme points. 
1.5 BASIC SOLUTIONS 
In this section we connect the geometric ideas of Section 1.3 and 
Section 1.4 with the algebraic notions developed in Section 1.2. We have 
already seen the important role played by the extreme points of the set of 
feasible solutions in obtaining an optimal solution to a linear programming 
problem. However, the extreme points are difficult to compute geometri- 
cally when there are more than three variables in the problem. In this 
section we give an algebraic description of extreme points that will facili- 
tate their computation. This description uses the concept of a basic 
solution to a linear programming problem. To lay the foundation for the 
definition of a basic solution, we shall now prove two very important 
general theorems about linear programming problems in canonical form. 
Consider the linear programming problem in canonical form 
Maximize 
z = cTx 
(1) 
subject to 
Ax = b 
(2) 
x > 0, 
(3) 
where A is an m • s matrix, c ~ R ~, x ~ R ~, and b ~ R m. Let the columns 
of A be denoted by A1,A2,... ,A~. We can then write (2) as 
x1A 1 + x2A 2 + "'" +xsA s = b. 
(4) 
We make two assumptions about the constraint matrix A. We assume that 
m < s and that there are m columns of A that are linearly independent. 
That is, the rank of A is m. These assumptions are true for a linear 
programming problem in canonical form that arose from a problem in 
standard form as given in Equations (4), (5), and (6) in Section 1.2. 
This set of m columns, and indeed any set of m linearly independent 
columns of A, forms a basis for R m. We can always renumber the columns 
of A (by reordering the components of x), so that the last m columns of A 
are linearly independent. Let S be the convex set of all feasible solutions 
to the problem determined by (1), (2), and (3). 
THEOREM 1.8. 
Suppose that the last m columns of A, which we denote by 
t~1, t~2,..., t~m, are linearly independent and suppose that 
Xtl t~ l "Jr- XP2 t~ 2 "Jr- "'" "[- X m t~ m = b, 
(5) 

1.5 Basic Solutions 
93 
where x' i > 0 for i = 1, 2,..., m. Then the point 
, 
, 
,) 
X = (0,0,...,0, 
Xl, X2,... 
,x m 
is an extreme point of S. 
Proof 
We assumed x > 0 in the statement of the theorem. Equation 
(5) represents Ax = b, since the first s-m 
components of x are zero. 
Thus, x is a feasible solution to the linear programming problem given by 
(1), (2), and (3). Assume that x is not an extreme point of S. Then, x lies in 
the interior of a line segment in S. That is, there are points v and w in S 
both different from x and a number A, 0 < A < 1, such that 
Now 
and 
x= 
Av + 
(1 - 
A)w. 
, 
, 
,) 
u -- (Vl~V2,...,Vs_m~Vl,V2~...~Um 
(6) 
? 
! 
! ) 
W "~ 
W1,W2,...,Ws_m,W1,W2,...~wm 
, 
where all the components of v and w are nonnegative, since v and w are 
feasible solutions. Substituting the expressions for x, v, and w into (6)7 we 
have 
0-- AU i a t- (1 
- 
A)wi, 
1 < i < s-m 
(7) 
! 
t 
xj=Av~+ 
(1-A)w), 
1 <j <m. 
Since all the terms in (7) are nonnegative and A and 1 - 
A are positive, we 
conclude that v i=O and w i=0 
for i= 1,2,...,s-re. 
Since v is a 
feasible solution, we know that Av = b and, because the first s-m 
components of v are zero, this equation can be written as 
l 
l 
l 
! 
u1A~I -1- u2Ar2 "q- "'" q-Umi~ m -" b. 
(8) 
If we now subtract Equation (8) from Equation (5), we have 
(Xq -- V~)/~ 1 "~ (X2 -- v2)At2 -4- ... "4"(Xtm - 
vtm)t~m 
-- O. 
Since we assumed that /~l,Ar2,...,Ar 
m 
were 
linearly independent, we 
conclude that 
' 
' 
for 
1 <i <m 
X i = 
V i 
-- 
_ 
and consequently that x = v. But we had assumed x # v. This contradic- 
tion implies that our assumption that x is not an extreme point of S is 
false. Thus, x is an extreme point of S, as we wanted to show. 
A 
THEOREM 1.9. 
If x = (X 1, X2,..., X,) is an extreme point of S, then the 
columns of A that correspond to positive xj form a linearly independent set of 
vectors in R m. 

94 
Chapter 1 Introduction to Linear Programming 
Proof 
To simplify the notation, renumber the columns of A and the 
components of x, if necessary, so that the last k components, denoted by 
! 
p 
t 
xi, x2,..., Xk, are positive. Thus, Equation (4) can be written as 
x~A' 1 + x~A' 2 + "- +x~,A' k = b. 
(9) 
We must show that A'I, A'2,...,/~k are linearly independent. Suppose they 
are linearly dependent. This means that there are numbers cj, 1 _< j _< k, 
not all of which are zero, such that 
C12~ 1 + CEAr2 + "" +-Ck2~ k -- 0. 
(10) 
Say that c t ~ O. Multiply Equation (10) by a positive scalar d, and first add 
the resulting equation to Equation (9), getting Equation (11), and second 
subtract it from Equation (9), getting Equation (12). We now have 
(X~ + dcl)A' 1 + (x~ + dc2)Ar2 + "'" +(XPk + dCk)t~ k = b 
(11) 
(X~ -- dcl)APl + (x~ - dc2)Ar2 + ... +(X~k -- dCk)t~ k = b. 
(12) 
Now consider the points in R ~, 
! 
! 
p 
V-- (0,0,...,0, 
X 1 + dCl,X 2 + dc2,...,x 
k + dc k) 
and 
' -dc 
' - dc 2 
' -dc k) 
w - (O,O,...,O, Xl 
1,x2 
,...,Xk 
9 
Since d is any positive scalar, we may choose it so that 
! 
0<d< 
min xj 
cj~:0. 
9 
I-~jl ' 
1 
With this choice of d, the last k coordinates of both v and w are positive. 
This fact together with Equations (11) and (12) implies that v and w are 
feasible solutions. But we also have 
1 
1 
x- 
~v + ~w, 
contradicting the hypothesis that x is an extreme point of S. Thus our 
assumption that the last k columns of A are linearly dependent is false; 
they are linearly independent. 
A 
COROLLARY 1.1. 
If X is an extreme point and Xil,...,Xir are the r 
positive components of x, then r < m, and the set of columns All,..., Ai, can 
be extended to a set of rn linearly independent vectors in R m by adjoining a 
suitably chosen set of rn - r columns of A. 
Proof. 
Exercise. 
A 
THEOREM 1.10. 
At most m components of any extreme point of S can be 
positive. The rest must be zero. 

1.5 Basic Solutions 
~ 
Proof. 
Theorem 1.9 says that the columns of A corresponding to the 
positive components of an extreme point x of the set S of feasible 
solutions are linearly independent vectors in R m. But there can be no 
more than m linearly independent vectors in R m. Therefore, at most m of 
the components of x are nonzero. 
A 
An important feature of the canonical form of a linear programming 
problem is that the constraints Ax = b form a system of m equations in s 
unknowns. Our assumption that there are m linearly independent column 
vectors means that the rank of A is m, so the rows of A are also linearly 
independent. That is, redundant equations do not occur in the system of 
constraints. In Theorems 1.8 and 1.9 we showed the relationships between 
the extreme points of the set S of feasible solutions and the linearly 
independent columns of A. We now use information about solutions to a 
system of equations to describe further the points of S. Note that S is just 
the set of solutions to Ax = b with nonnegative components (x > 0). 
Consider a system of m equations in s unknowns (m < s) and write it 
in matrix form as Ax = b. Assume that at least one set of m columns of A 
is linearly independent. Choosing any set T of m linearly independent 
columns of A (which is choosing a basis for Rm), set the s - m variables 
corresponding to the remaining columns equal to zero. The equation 
Ax = b may be written as 
x1A 1 + x2A 2 + "'" +XsA s = b, 
(13) 
where Ai is the ith column of A. But we have set s - m of the variables 
equal to zero. Let il, i2,...,i m be the indices of the variables that were 
not set to zero. They are also the indices of the columns of A in the set T. 
Consequently, (13)reduces to 
xilAil + xi2Ai2 + ... +XimAim -- b, 
which is a system of m equations in m unknowns and has a unique 
solution. (Why?) The values of the m variables obtained from solving this 
system along with the s - m zeros form a vector x that is called a basic 
solution to Ax = b. 
EXAMPLE 1. 
Consider the system of three equations in six unknowns 
x1 
[ 
I 11 
1 
0 
1 
0 
1 
0 
x3 
0 
-1 
-1 
0 
-1 
-1 
- 
b2 . 
1 
2 
2 
1 
1 
1 
x4 
b3 
x 5 
x6 
- 
- 

~ 
Chapter 1 Introduction to Linear Programming 
Setting x 2 - - X  3 
--" X 5 
= 
0, we get the system of three equations in three 
unknowns given by 
[1 0 0]IXl I ibll 
0 
0 
-1 
x4 
"-- 
b2 9 
1 
1 
1 
x 6 
b 3 
The columns of this coefficient matrix are linearly independent, and this 
system has the solution x I = b~, x 4 = b 2 + b 3 - bl, x 6 - 
-b 2. Conse- 
quently, a basic solution to the original system is 
X = 
(bl,0,0, 
b 2 + b 3 - 
bl,0 ,-b2). 
On the other hand, if we set x 1 --X 3 = X 5 "-- 0, we obtain the system 
[o o o]Ix21 ibll 
-1 
0 
-1 
x4 
-- 
b2 9 
2 
1 
1 
x 6 
b 3 
Here the columns of the coefficient matrix are not linearly independent. In 
fact, column 1 is the sum of columns 2 and 3. This system cannot be solved 
if b I ~: 0. Consequently, this choice of variables does not lead to a basic 
solution. 
/x 
In any basic solution, the s - m variables that are set equal to zero are 
called nonbasic variables, and the m variables solved for are called basic 
variables. Although the term basic solution appears in all the literature 
describing linear programming, it can be misleading. A basic solution is a 
solution to the system Ax = b; it does not necessarily satisfy x >_ 0, and 
therefore it is not necessarily a feasible solution to the linear programming 
problem given by (1), (2), and (3). 
DEFINITION. 
A basic feasible solution to the linear programming prob- 
lem given by (1), (2), and (3) is a basic solution that is also feasible. 
THEOREM 1.11. 
For the linear programming problem determined by (1), 
(2), and (3), every basic feasible solution is an extreme point, and, conversely, 
every extreme point is a basic feasible solution. 
Proof. 
Exercise. 
A 
THEOREM 1.12. 
The problem determined by (1), (2), and (3) has a finite 
number of basic feasible solutions. 
Proof 
The number of basic solutions to the problem is 
(s) 
s, 
( )s 
m 
m!(s -m)! 
s -m 

1.5 Basic Solutions 
~ 
because s - m variables must be chosen to be set to zero, out of a total of 
s variables. The number of basic feasible solutions may be smaller than 
the number of basic solutions, since not all basic solutions need to be 
feasible. 
A 
We now examine the relationship between the set S of feasible solu- 
tions to a standard linear programming problem given in Equations (4), 
(5), and (6) in Section 1.2 and the set S' of feasible solutions to the 
associated canonical linear programming problem given in Equations (12), 
(13), and (14) in Section 1.2. We have already discussed the method of 
adding slack variables to go from a point in S to a point in S'. Conversely, 
we truncated variables to move from S' to S. More specifically, we have 
the following theorem, whose proof we leave as an exercise. 
THEOREM 1.13. 
Every extreme point of S yields an extreme point of S' 
when slack variables are added. Conversely, every extreme point of S', when 
truncated, yields an extreme point of S. 
Proof 
Exercise. 
A 
THEOREM 1.14. 
The convex set S of all feasible solutions to a linear 
programming problem in standard form has a finite number of extreme points. 
Proof 
Exercise. 
/x 
By combining the Extreme Point Theorem (Theorem 1.7) and Theorem 
1.14 we can give a procedure for solving a standard linear programming 
problem given by Equations (4), (5), and (6) in Section 1.2. First, set up the 
associated canonical form of the problem. Then find all basic solutions and 
eliminate those that are not feasible. Find those that are optimal among 
the basic feasible solutions. Since the objective function does not change 
between the standard and canonical forms of the problem, any optimal 
solution to the canonical form, found as described above, is an optimal 
solution to the standard problem. 
From the situation under consideration, the number of basic solutions 
to be examined is no more than 
m+n). 
n 
Although this number is finite, it is still too large for actual problems. For 
example, a moderate-size problem with m = 200 and n = 300 would have 
about 10144 basic solutions. 
EXAMPLE 2. 
Consider the linear programming problem given in Exam- 
ple 3 in Section 1.2. In this example we can select two of the four variables 
x, y, u, v as nonbasic variables by setting them equal to zero and then 

98 
Chapter 1 
Introduction to Linear Programming 
solve for the basic variables. If in 
we set 
then 
2 
2 
1 
0 
5 
3 
0 
1 
x 
u 
u 
8 
x = y = 0 
(nonbasic variables), 
u = 8 
and 
v = 15 
(basic variables). 
The vector [0 
0 
8 
15] T is a basic feasible solution to the canonical 
form of the problem and hence an extreme point of S'. By truncating the 
slack variables we get [0 
0] T, which is an extreme point of S and a 
feasible solution to the standard form of the problem. 
If instead we set 
then 
The vector [0 
u is negative. 
x = v = 0 
(nonbasic variables), 
y= 5 
and 
u =-2 
(basic variables). 
5 
-2 
0] T is a basic solution, but it is not feasible, since 
In Table 1.5 we list all the basic solutions, indicate whether they are 
feasible, and give the truncated vectors. The student should locate these 
truncated vectors on Figure 1.14. Once we discard the basic solutions that 
are not feasible, we select a basic feasible solution for which the objective 
function is a maximum. Thus, we again obtain the optimal solution 
3 
5 
x = ~, 
Jv= ~, 
u =v, 
v =v 
TABLE 1.5 
Value of 
Truncated 
x 
y 
u 
v 
Type of solution 
z = 120x + 100y 
vector 
0 
0 
8 
15 
Basic feasible 
0 
(0, 0) 
0 
4 
0 
3 
Basic feasible 
400 
(0, 4) 
0 
5 
- 2 
0 
Basic, not feasible 
m 
(0, 5) 
4 
0 
0 
- 5 
Basic, not feasible 
m 
(4, 0) 
3 
0 
2 
0 
Basic feasible 
360 
(3, 0) 
3 
5 ~ 
0 
0 
Basic feasible 
430 
(3, 5) 

1.5 Basic Solutions 
99 
1.5 
EXERCISES 
1. Suppose the canonical form of a liner programming problem is given by the 
constraint matrix A and resource vector b, where 
A- 
2 
1 
0 
0 
0 
and 
b= 
3 . 
4 
0 
3 
0 
1 
6 
Determine which of the following points is 
(i) a feasible solution to the linear programming problem. 
(ii) an extreme point of the set of feasible solutions. 
(iii) a basic solution. 
(iv) a basic feasible solution. 
For each basic feasible solution x given below, list the basic variables. 
3 
1 
1 
0 
0 
~ 
~ 
1 
3 
3 
0 
1 
1 
(a) 0 
(b) 
5 
(c) 0 
(d) 1 
(e) ~ 
1 
3 
5 
0 
~ 
0 
1 
6 
-9 
0 
2 
In Exercises 2 and 3, set up a linear programming model for the situation 
described. Sketch the set of feasible solutions and find an optimal solution by 
examining the extreme points. 
2. The Savory Potato Chip Company makes pizza-flavored and chili-flavored 
potato chips. These chips must go through three main processes: frying, 
flavoring, and packing. Each kilogram of pizza-flavored chips takes 3 min to fry, 
5 min to flavor, and 2 min to pack. Each kilogram of chili-flavored chips takes 3 
min to fry, 4 min to flavor, and 3 min to pack. The net profit on each kilogram 
of pizza chips is $0.12, whereas the net profit on each kilogram of chili chips is 
$0.10. The fryer is available 4 hr each day, the flavorer is available 8 hr each 
day, and the packer is available 6 hr each day. Maximize the net profit with 
your model. 
3. Sugary Donut Bakers, Inc., is known for its excellent glazed doughnuts. The 
firm also bakes doughnuts, which are then dipped in powdered sugar. It makes 
a profit of $0.07 per glazed doughnut and $0.05 per powdered sugar doughnut. 
The three main operations are baking, dipping (for the powdered sugar 
doughnuts only), and glazing (for the glazed doughnuts only). Each day the 
plant has the capacity to bake 1400 doughnuts, dip 1200 doughnuts, and glaze 
1000 doughnuts. The manager has instructed that at least 600 glazed dough- 
nuts must be made each day. Maximize the total profit with your model. 
4. For Exercise 2, write the linear programming problem in canonical form, 
compute the values of the slack variables for an optimal solution, and give a 
physical interpretation for these values. Also identify the basic variables of the 
optimal solution. 

1 O0 
Chapter 1 Introduction to Linear Programming 
5. For Exercise 3, write the linear programming problem in canonical form, 
compute the values of the slack variables for an optimal solution, and give a 
physical interpretation for these values. Also identify the basic variables of the 
optimal solution. 
6. Consider the system of equations Ax = b, where 
A=[2 
3 
4 
0 
4] 
and 
b=[ 2] 
1 
0 
0 
-2 
1 
0 
" 
Determine whether each of the following 5-tuples is a basic solution to the 
system. 
(a) (1, 0, 1, 0, 0) 
(b) (0, 2, - 1, 0, 0) 
(c) (2, - 2, 3, 0, - 2) 
(d) (0, 0, x 
~,0,0) 
7. Consider the system of equations Ax - b, where 
[23100] 
[1] 
A- 
-1 
1 
0 
2 
1 
and 
b= 
1 . 
0 
6 
1 
0 
3 
4 
Determine which of the following 5-tuples are basic solutions to the system. 
Give reasons. 
(a) (1, 0, - 1, 1, 0) 
(b) (0,2,-5,0,-1) 
(c) (0, 0, 1, 0, 1) 
8. Consider the linear programming problem 
Maximize 
z=3x+2y 
subject to 
2x-y<6 
2x + y < 10 
x>_0, 
y>_0. 
(a) Transform this problem to a problem in canonical form. 
(b) For each extreme point of the new problem, identify the basic variables. 
(c) Solve the problem geometrically. 
9. Consider the linear programming problem 
Maximize 
z = 4x 1 + 2X 2 + 7X 3 
subject to 
2x 1- 
x 2 + 4x 3_< 18 
4x I + 2x 2 + 5X 3 __< 10 
X 1 >_~ 0, 
X 2 >_~ 0, 
X 3 >_~ 0. 
(a) Transform this problem to a problem in canonical form. 
(b) For each extreme point of the new problem, identify the basic variables. 
(c) Which of the extreme points are optimal solutions to the problem? 

1.5 Basic Solutions 
101 
10. Consider the linear programming in standard form 
Maximize 
z = cTx 
subject to 
Ax<b 
x>0. 
Show that the constraints Ax < b may be written as 
[x] 
(i) [A I I] x' 
=b 
or as 
(ii) Ax + Ix'= b, 
where x' is a vector of slack variables. 
11. Prove Corollary 1.1 (Hint: Use Theorem 0.13.) 
12. Prove for a linear programming problem in canonical form that a point in the 
set of feasible solutions is an extreme point if and only if it is a basic feasible 
solution (Theorem 1.11). (Hint: Use Theorem 0.13.) 
13. Prove that the set of feasible solutions to a linear programming problem in 
standard form has a finite number of extreme points (Theorem 1.14). 
Further Reading 
Chvfital, Va~ek. Linear Programming. Freeman, New York, 1980. 
Griinbaum, B. Convex Polytopes. Wiley-Interscience, New York, 1967. 
Hadley, G. Linear Algebra. Addison-Wesley, Reading, MA, 1961. 
Murty, Katta G. Linear Programming. Wiley, New York, 1983. 
Taha, Hamdy A. Operations Research: An Introduction, third ed., Macmillan, New York, 1982. 

The Simplex 
Method 
I 
N THIS CHAPTER we describe an elementary version of the method 
that can be used to solve a linear programming problem systemati- 
cally. In Chapter 1 we developed the algebraic and geometric 
notions that allowed us to characterize the solutions to a linear program- 
ming problem. However, for problems of more than three variables, the 
characterization did not lead to a practical method for actually finding the 
solutions. We know that the solutions are extreme points of the set of 
feasible solutions. The method that we present determines the extreme 
points in the set of feasible solutions in a particular order that allows us to 
find an optimal solution in a small number of trials. We first consider 
problems in standard form because when applying the method to these 
problems it is easy to find a starting point. The second section discusses a 
potential pitfall with the method. However, the difficulty rarely arises and 
has almost never been found when solving practical problems. In the third 
section, we extend the method to arbitrary linear programming problems 
by developing a way of constructing a starting point. 
103 

104 
Chapter 2 
The Simplex Method 
2.1 THE SIMPLEX METHOD FOR PROBLEMS IN STANDARD FORM 
We already know from Section 1.5 that a linear programming problem 
in canonical form can be solved by finding all the basic solutions, discard- 
ing those that are not feasible, and finding an optimal solution among the 
remaining. Since this procedure can still be a lengthy one, we seek a more 
efficient method for solving linear programming problems. The simplex 
algorithm is such a method; in this section we shall describe and carefully 
illustrate it. Even though the method is an algebraic one, it is helpful to 
examine it geometrically. 
Consider a linear programming problem in standard form 
Maximize 
z = cTx 
(1) 
subject to 
Ax<b 
x>_O, 
where A = [a/y] is an m • n matrix and 
bl 
c 1 
x 1 
b2 
c 2 
x 2 
b= 
. 
, 
c= 
. 
, 
and 
x= 
. 
. 
Cn 
X n 
m 
(2) 
(3) 
In this section we shall make the additional assumption that b > 0. In 
Section 2.3 we will describe a procedure for handling problems in which b 
is not nonnegative. 
We now transform each of the constraints in (2) into an equation by 
introducing a slack variable. We obtain the canonical form of the problem, 
namely 
Maximize 
z = cTx 
(4) 
subject to 
Ax=b 
x>_O, 
where in this case A is the m • (n + m) matrix 
(5) 
(6) 
A 
__. 
all 
a12 
... 
aln 
1 
0 
"" 
a21 
a22 
... 
a2n 
0 
1 
... 
9 
: 
9 
; 
9 
9 
o 
. 
o 
aml 
am2 
"'" 
amn 
0 
0 
"'" 

2.1 The Simplex Method for Problems in Standard Form 
1 ~ 
C 1 
r 
Cn 
X ~- 
C-- 
0 
0 
~ 
0 
X 1 
X 2 
Xn 
Xn+ 1 
Xn+m 
and b is as before. 
Recall from Section 1.5 that a basic feasible solution to the canonical 
form of the problem (4), (5), (6) is an extreme point of the convex set S' of 
all feasible solutions to the problem. 
DEFINITION9 Two distinct extreme points in S' are said to be adjacent 
if as basic feasible solutions they have all but one basic variable in 
common. 
A 
EXAMPLE 1. 
Consider Example 2 of Section 1.5 and especially Table 
1.5 in that example. The extreme points (0, 0,8, 15) and (0,4, 0,3) are 
adjacent, since the basic variables in the first extreme point are u and v 
and the basic variables in the second extreme point are y and v. In fact, 
the only extreme point that is not adjacent to (0, 0, 8, 15) is (3, 5, 0, 0). 
A 
The simplex method developed by George B. Dantzig in 1947 is a 
method that proceeds from a given extreme point (basic feasible solution) 
to an adjacent extreme point in such a way that the value of the objective 
function increases or, at worst, remains the same. The method proceeds 
until we either obtain an optimal solution or find that the given problem 
has no finite optimal solution. The simplex algorithm consists of two steps: 
(1) a way of finding out whether a given basic feasible solution is an 
optimal solution and (2) a way of obtaining an adjacent basic feasible 
solution with the same or larger value for the objective function. In actual 
use, the simplex method does not examine every basic feasible solution; it 
checks only a relatively small number of them. However, examples have 
been given in which a large number of basic feasible solutions have been 
examined by the simplex method. 
We shall demonstrate parts of our description of the simplex method on 
the linear programming problem in Example 1 of Section 1.1. The associ- 
ated canonical form of the problem was described in Example 4 of Section 

106 
Chapter 2 The Simplex Method 
1.2. In this form it is: 
Maximize 
z = 120x + 100y 
subject to 
2x+2y+u 
= 
8} 
5x + 3y 
+ v = 15 
x>_O, 
y>_O, 
u>_O, 
v>_O. 
(7) 
(8) 
(9) 
The Initial Basic Feasible Solution 
To start the simplex method, we must find a basic feasible solution. The 
assumption that b >__ 0 allows the following procedure to work. If it is not 
true that b > 0, another procedure (discussed in Section 2.3) must be used. 
We take all the nonslack variables as nonbasic variables; that is, we set all 
the nonslack variables in the system Ax = b equal to zero. The basic 
variables are then just the slack variables. We have 
x I = x 2 ..... 
x,, = 0 
and 
x,,+l = b 1, 
Xn+ 2 -" b2,...,Xn+ 
m --- bm. 
This is a feasible solution, since b > 0; and it is a basic solution, since 
(n + m) - m = n of the variables are zero. 
In our example, we let 
x=y=0. 
Solving for u and v, we obtain 
u=8, 
v=15. 
The initial basic feasible solution constructed by this method is (0, 0, 8, 15). 
The basic feasible solution yields the extreme point (0, 0) in Figure 1.14 
(Section 1.4). 
It is useful to set up our example and its initial basic feasible solution in 
tabular form. To do this, we write (7) as 
-120x- lOOy+z=O, 
(10) 
where z is now viewed as another variable. The initial tableau is now 
formed (Tableau 2.1). At the top we list the variables x, y, u, v, and z as 
labels on the corresponding columns. The last row, called the objective 
row, is Equation (10). The constraints (8) are on the first two rows. Along 
the left side of each row we indicate which variable is basic in the 
corresponding equation. Thus, in the first equation u is the basic variable, 
and v is the basic variable in the second equation. 

2.1 The Simplex Method for Problems in Standard Form 
107 
Tableau 2.1 
x 
y 
u 
v 
z 
u 
2 
2 
1 
0 
9 
8 
v 
5 
3 
0 
1 
0 
15 
120 
100 
0 
0 
1 
0 
In the tableau, a basic variable has the following properties: 
1. It appears in exactly one equation and in that equation it has a 
coefficient of + 1. 
2. The column that it labels has all zeros (including the objective row 
entry) except for the + 1 in the row that is labeled by the basic variable. 
3. The value of a basic variable is the entry in the same row in the 
rightmost column. 
The initial tableau for the general problem (4), (5), (6) is shown in Tableau 
2.2. The value of the objective function 
Z -- ClX 1 + C2X 2 "+" "'" "+'CnX n "+ O'Xn+l 
"[- "'" +O'Xn+ 
m 
for the initial basic feasible solution is 
Z -- C 1 "0 + C2"0 
+ 
"'" +C n "0 + 0-b 
I + 0"b 2 + 
-'" + 
O'b m = O. 
Notice that the entry in the last row and rightmost column is the value of 
the objective function for the initial basic feasible solution. 
Tableau 2.2 
x1 
x2 
"'" 
Xn 
Xn + 1 
Xn + 2 
"'" 
Xn + m 
Z 
Xn+ 1 
all 
a12 
... 
aln 
1 
0 
"" 
0 
0 
b 1 
Xn+ 2 
a21 
a22 
"" 
a2n 
0 
1 
"'" 
0 
0 
b 2 
9 
X'n+mamlam2 
""amnO 
b 
""'1 
bb 
m 
Cl 
C2 
"'" 
% 
0 
0 
"'" 
0 
1 
0 
In our example we have 
z= 120.0+ 100-0+0-8+0-15 
=0. 
At this point the given linear programming problem has been trans- 
formed to the initial tableau. This tableau displays the constraints and 
objective function along with an initial basic feasible solution and the 
corresponding value of the objective function for this basic feasible solu- 
tion. We are now ready to describe the steps in the simplex method that 

108 
Chapter 2 The Simplex Method 
are used repeatedly to create a sequence of tableaux, terminating in a 
tableau that yields an optimal solution to the problem. 
Checking an Optimality Criterion 
We shall now turn to the development of a criterion that will determine 
whether the basic feasible solution represented by a tableau is, in fact, 
optimal. For our example we can increase the value of z from its value of 
0 by increasing any one of the nonbasic variables having a positive 
coefficient from its current value of 0 to some positive value. For our 
example, 
z = 120x + lOOy + O.u + O.v, 
so that z can be increased by increasing either x or y. 
For an arbitrary tableau, if we write the objective function so that the 
coefficients of the basic variables are zero, we then have 
Z -" 
E 
djxj ..~ E O'xi, 
(11) 
nonbasic 
basic 
where the dj's are the negatives of the entries in the objective row of the 
tableau. We see that (11) has some terms with positive coefficients if and 
only if the objective row has negative entries under some of the columns 
labeled by nonbasic variables. Now the value of z can be increased by 
increasing the value of any nonbasic variable with a negative entry in the 
objective row from its current value of O. If this is done, then some basic 
variable must be set to zero since the number of basic variables is to 
remain unchanged. Setting this basic variable to zero will not change the 
value of the objective function since the coefficient of the basic variable 
was zero. We summarize this discussion by stating the following optimality 
criterion for testing whether a feasible solution shown in a tableau is an 
optimal solution. 
Optimality Criterion. If the objective row of a tableau has zero 
entries in the columns labeled by basic variables and no 
negative entries in the columns labeled by nonbasic variables, 
then the solution represented by the tableau is optimal. 
As soon as the optimality criterion has been met, we can stop our 
computations, for we have found an optimal solution. 

2.1 The Simplex Method for Problems in Standard Form 
109 
Selecting the Entering Variable 
Suppose now that the objective row of a tableau has negative entries in 
the labeled columns. Then the solution shown in the tableau is not 
optimal, and some adjustment of the values of the variables must be made. 
The simplex method proceeds from a given extreme point (basic feasible 
solution) to an adjacent extreme point in such a way that the objective 
function increases in value. From the definition of adjacent extreme point, 
it is clear that we reach such a point by increasing a single variable from 
zero to a positive value and decreasing a variable with a positive value to 
zero. The largest increase in z per unit increase in a variable occurs for 
the most negative entry in the objective row. We shall see below that, if 
the feasible set is bounded, there is a limit on the amount by which we can 
increase a variable. Because of this limit, it may turn out that a larger 
increase in z may be achieved by not increasing the variable with the most 
negative entry in the objective row. However, this rule is most commonly 
followed because of its computational simplicity. Some computer imple- 
mentations of the simplex algorithm provide other strategies for choosing 
the variable to be increased, including one as simple as choosing the first 
negative entry. Another compares increases in the objective function for 
several likely candidates for the entering variable. In Tableau 2.1, the most 
negative entry, -120, in the objective row occurs under the x column, so 
that x is chosen to be the variable to be increased from zero to a positive 
value. The variable to be increased is called the entering variable, since in 
the next iteration it will become a basic variable; that is, it will enter the 
set of basic variables. If there are several possible entering variables, 
choose one. (This situation will occur when the most negative entry in the 
objective row occurs in more than one column.) Now an increase in one 
variable must be accompanied by a decrease in some of the other variables 
to maintain a solution to Ax = b. 
Choosing the Departing Variable 
Solving (8) for the basic variables u and v, we have 
u= 
8-2x-2y 
v= 15-5x-3y. 
We increase only x and keep y at zero. We have 
u= 
8-2x) 
(12) 
v = 15 - 5x 
' 
which shows that as x increases both u and v decrease. By how much can 
we increase x? It can be increased until either u or v becomes negative. 

11 ~ 
Chapter 2 The Simplex Method 
That is, from (9) and (12)we have 
O_<u= 
8-2x 
O_<v= 15-5x. 
Solving these inequalities for x, we find 
and 
2x< 
8 
or 
x< 
8/2=4 
5x< 15 
or 
x< 15/5 =3. 
We see that we cannot increase x by more than the smaller of the two 
ratios 8/2 and 15/5. Letting x = 3, we obtain a new feasible solution, 
x=3, 
y=O, 
u=2, 
v=O. 
In fact, this is a basic feasible solution, and it was constructed to be 
adjacent to the previous basic feasible solution, since only one variable 
changed from basic to nonbasic. The new basic variables are x and u; the 
nonbasic variables are y and v. The objective function now has the value 
z = 120.3 + 100.0 + 0-2 + 0.0 = 360, 
which is a considerable improvement over the previous value of zero. 
The new basic feasible solution yields the extreme point (3, 0) in Figure 
1.14, and it is adjacent to (0, 0). In the new basic feasible solution to our 
example, we have the variable v = 0. It is no longer a basic variable 
because it is zero, and it is called a departing variable since it has departed 
from the set of basic variables. The column of the entering variable is 
called the pivotal column; the row that is labeled with the departing 
variable is called the pivotal row. 
We now examine more carefully the selection of the departing variable. 
Recall that the ratios of the rightmost column entries to the corresponding 
entries in the pivotal column were determined by how much we could 
increase the entering variable (x in our example). These ratios are called 
0-ratios. The smallest nonnegative 0-ratio is the largest possible value for 
the entering variable. The basic variable labeling the row where the 
smallest nonnegative 0-ratio occurs is the departing variable, and the row 
is the pivotal row. In our example, 
min{8/2, 15/5} = 3, 
and the second row in Tableau 2.1 is the pivotal row. 
If the smallest nonnegative 0-ratio is not chosen, then the next basic 
solution is not feasible. Suppose we had chosen u as the departing variable 
by choosing the 0-ratio as 4. Then x = 4, and from (12)we have 
u= 
8-2.4=0 
v= 15-5.4= 
-5, 

2.1 The Simplex Method for Problems in Standard Form 
111 
and the next basic solution is 
x=4, 
y=0, 
u=0, 
v= -5, 
which is not feasible. 
In the general case, we have assumed that the rightmost column will 
contain only nonnegative entries. However, the entries in the pivotal 
column may be positive, negative, or zero. Positive entries lead to non- 
negative 0-ratios, which are fine. Negative entries lead to nonpositive 
0-ratios. In this case, there is no restriction imposed on how far the 
entering variable can be increased. For example, suppose the pivotal 
column in our example were 
2 
[ 5-2 
instead of 
_]5] 
Then we would have, instead of (12), 
u= 
8+2x 
v = 15 - 5x. 
Since u must be nonnegative, we find that 
8+2x>0 
or 
x> -4, 
which puts no restriction on how far we can increase x. Thus, in calculat- 
ing 0-ratios we can ignore any negative entries in the pivotal column. 
If an entry in the pivotal column is zero, the corresponding 0-ratio is 
undefined. However, checking the equations corresponding to (12), but 
with one of the entries in the pivotal column equal to zero, will show that 
no restriction is placed on the size of x by the zero entry. Consequently, in 
forming the 0-ratios we use only the positive entries in the pivotal column 
that are above the objective row. 
If all the entries in the pivotal column above the objective row are 
either zero or negative, then the entering variable can be made as large as 
we wish. Hence, the given problem has no finite optimal solution, and we 
can stop. 
Forming a New Tableau 
Having determined the entering and departing variables, we must 
obtain a new tableau showing the new basic variables and the new basic 
feasible solution. We illustrate the procedure with our continuing example. 
Solving the second equation of (8) (it corresponds to the departing vari- 
able) for x, the entering variable, we have 
X=3 
3 
1 
~y - ~v. 
(13) 
Substituting (13) into the first equation of (8), we get 
2(3 - 
3 
1 
~y-~v) 
+2y+u 
=8 

112 
Chapter 2 The Simplex Method 
or 
We also rewrite (13) as 
2 
4y + u 
~v = 2. 
(14) 
3 
1 
x+~y+~v=3. 
(15) 
Substituting (13) into (7), we have 
(- 120)(3 - 3 
1 
~y-~v)- 
100y+z=0 
or 
-28y + 24v + z = 360. 
(16) 
Since in the new basic feasible solution we have y = v = 0, the value of z 
for this solution is 360. This value appears as the entry in the last row and 
rightmost column. Equations (14), (15), and (16) yield the new tableau 
(Tableau 2.3). 
Tableau 2.3 
x 
y 
u 
v 
z 
u 
0 
4 
1 
2 
0 
2 
5 
5 
x 
1 
3 
0 
1- 
0 
3 
5 
5 
0 
28 
0 
24 
1 
360 
Observe that the basic variables in Tableau 2.3 are x and u. By 
comparing Tableaus 2.1 and 2.3, we see that the steps that were used to 
obtain Tableau 2.3 from Tableau 2.1 are as follows. 
Step a. 
Locate and circle the entry at the intersection of the pivotal 
row and pivotal column. This entry is called the pivot. Mark the pivotal 
column by placing an arrow $ above the entering variable, and mark the 
pivotal row by placing an arrow ~ 
to the left of the departing variable. 
Step b. 
If the pivot is k, multiply the pivotal row by l/k, making the 
entry in the pivot position in the new tableau equal to 1. 
Step c. 
Add suitable multiples of the new pivotal row to all other rows 
(including the objective row), so that all other elements in the pivotal 
column become zero. 
Step d. 
In the new tableau, replace the label on the pivotal row by the 
entering variable. 
These four steps constitute a process called pivoting. Steps b and c use 
elementary row operations (see Section 0.2) and form one iteration of the 
procedure used to transform a given matrix to reduced row echelon form. 

2.1 The Simplex Method for Problems in Standard Form 
113 
We now repeat Tableau 2.1 with the arrows placed next to the entering 
and departing variables and with the pivot circled (Tableau 2.1a). 
Tableau 2.1a 
$ 
x 
y 
u 
v 
z 
u 
2 
2 
1 
0 
0 
v 
(~) 
3 
0 
1 
0 
120 
100 
0 
0 
1 
8 
15 
0 
Tableau 2.3 was obtained from Tableau 2.1 by pivoting. We now repeat 
the process with Tableau 2.3. Since the most negative entry in the objective 
row of Tableau 2.3, -28, occurs in the second column, y is the entering 
variable of this tableau and the second column is the pivotal column. To 
find the departing variable we form the 0-ratios, that is, the ratios of the 
entries in the rightmost column (except for the objective row) to the 
corresponding entries of the pivotal column for those entries in the pivotal 
column that are positive. The 0-ratios are 
2 
3 
5 
T =~ 
and 
-3- =5. 
The minimum of these is s ~, which occurs for the first row. Therefore, the 
4 
pivotal row is the first row, the pivot is ~, and the departing variable is u. 
We now show Tableau 2.3 with the pivot, entering, and departing variables 
marked (Tableau 2.3a). 
Tableau 2.3a 
x 
y 
u 
v 
z 
.,, 
G 
2 
u 
0 
1 
~ 
0 
2 
3 
1 
0 
3 
x 
1 
3- 
0 
3- 
0 
28 
0 
24 
1 
360 
We obtain Tableau 2.4 from Tableau 2.3 by pivoting. Since the objective 
row in Tableau 2.4 has no negative entries, we are finished, by the 
optimality criterion. That is, the indicated solution, 
3 
5 
x=~, 
y=~, 
u=O, 
v=O, 

114 
Chapter 2 The Simplex Method 
Tableau 2.4 
x 
y 
u 
v 
z 
y 
0 
1 
5 
1 
5 
~ 
0 
~- 
3 
1 
0 
3 
x 
1 
0 
4 
~ 
~- 
0 
0 
35 
10 
1 
430 
is optimal, and the maximum value of z is 430. Notice from Figure 1.14 
that we moved from the extreme point (0, 0) to the adjacent extreme point 
3 
5 
(3, 0) and then to the adjacent extreme point (3, 3). The value of the 
objective function started at 0, increased to 360, and then to 430, the entry 
in the last row and rightmost column. 
Summary of the Simplex Method 
We assume that the linear programming problem is in standard form 
and that b > 0. In this case the initial basic feasible solution is 
01 
x-[b 
9 
In subsequent sections we will show how to extend the simplex method to 
other linear programming problems. 
Step 1. 
Set up the initial tableau. 
Step 2. 
Apply the optimality test: If the objective row has no negative 
entries in the labeled columns, then the indicated solution is optimal. Stop 
computation. 
Step 3. 
Find the pivotal column by determining the column with the 
most negative entry in the objective row. If there are several possible 
pivotal columns, choose any one. 
Step 4. 
Find the pivotal row. This is done by forming the 0-ratios--the 
ratios formed by dividing the entries of the rightmost column (except for 
the objective row) by the corresponding entries of the pivotal columns 
using only those entries in the pivotal column that are positive. The pivotal 
row is the row for which the minimum ratio occurs. If two or more 0-ratios 
are the same, choose one of the possible rows. If none of the entries in the 
pivotal column above the objective row is positive, the problem has no 
finite optimum. We stop our computation in this case. 
Step 5. 
Obtain a new tableau by pivoting. Then return to Step 2. 
In Figure 2.1 we give a flowchart and in Figure 2.2, a structure diagram 
for the simplex algorithm. 
The reader can use the SMPX courseware described in Appendix C to 
experiment with different choices of pivot, observing how some choices 

2.1 The Simplex Method for Problems in Standard Form 
11 
Set up 
initial tableau 
.... 
~YES 
........... 
[ Ge't ........ 
pi'vota, column~ ] 
J 
[ 
C ,,,ivo,a, 
J 
~ 
,." 
Compute a new 
tableau by pivoting 
I 
NO_ 
v 
Indicated 
solution is 
optimal 
NO 
There is no 
finite optimal 
solution 
O 
FIGURE 2.1 
Flowchart for simplex algorithm (standard form, b > 0). 
lead to infeasible solutions. The courseware will also allow the user to step 
through the iterations of the simplex algorithm so that the intermediate 
tableaux can be examined. 
The reader should note that the z column always appears in the form 
z 
0 
0 
6 
1 
in any simplex tableau. We included it initially to remind the reader that 
each row of a tableau including the objective row represents an equation 

11 ~ 
Chapter 2 The Simplex Method 
Set up initial tableau 
WHILE negative entries in objective row DO 
,, 
Get pivotal column 
~
~
 
Positive entries in pivotal column ~f~" 
~objective 
row 
~ 
TRUE 
~
J
 
FALSE 
Get pivotal row 
No finite optimal 
solution exists 
Compute new tableau 
by pivoting 
STOP 
Present tableau represents optimal solution 
FIGURE 2.2 
Structure diagram of simplex algorithm (standard form, la > 0). 
in the variables x l, x2,... , X s, z. From this point on we will not include the 
z column in tableaux. The student should remember to read the objective 
row of a tableau as an equation that involves z with coefficient + 1. 
EXAMPLE 2. 
We solve the following linear programming problem in 
standard form by using the simplex method: 
Maximize 
z -- 8x 1 + 9x 2 + 5x 3 
subject to 
x I + 
x 2 + 2x 3 < 2 
2X 1 + 3x 2 + 4X 3 < 3 
6x I + 6x 2 + 2x 3 < 8 
X 1 ~_~ 0, 
X 2 ~ 0, 
X 3 >_~ 0. 
We first convert the problem to canonical form by adding slack vari- 
ables, obtaining: 
Maximize 
z-8x 
1 +9x 2+5x 3 
subject to 
x 1 + 
x 2 + 2x 3 + x 4 
= 2 
2x 1 + 3X 2 + 4x 3 
+ x 5 
= 3 
6x I + 6x 2 + 2x 3 
+ x 6 -" 8 
xj>0, 
j= 
1,2, .... 6. 

2.1 The Simplex Method for Problems in Standard Form 
117 
Tableau 2.5 
X 4 
X5 
X6 
Tableau 2.6 
X1 
X2 
X 3 
X4 
X5 
X6 
1 
1 
2 
1 
0 
0 
2 
2 
(~) 
4 
0 
1 
0 
3 
6 
6 
2 
0 
0 
1 
8 
8 
9 
5 
0 
0 
0 
0 
X1 
X2 
X 3 
X4 
X 5 
X6 
1 
0 
2 
1 
1 
X 4 
~ 
~ 
~ 
0 
X2 
2 
1 
4 
0 
! 
0 
3 
3 
3 
x 6 
(~) 
0 
6 
0 
2 
1 
Tableau 2.7 
2 
0 
7 
0 
3 
0 
X 1 
X 2 
X 3 
X4 
X5 
X6 
X 4 
0 
0 
5 
1 
0 
1 
2 
1 
1 
X 2 
0 
1 
~0 
0 
1 
~ 
1 
1 
X 1 
1 
0 
3 
0 
1 
0 
0 
1 
0 
1 
1 
11 
The initial tableau is Tableau 2.5; the succeeding tableaux are Tableaux 
2.6 and 2.7. 
Hence, an optimal solution to the standard form of the problem is 
1 
X 1 -- 1, 
X2 -- 3, 
X 3 -- 0. 
The values of the slack variables are 
2 
X 4 = 
-~, 
X 5 = 
0, 
X 6 -- 0. 
The optimal value of z is 11. 
A 
EXAMPLE 
3. 
Consider the linear programming problem 
Maximize 
z = 2x I + 3x 2 + X 3 + X 4 
subject to 
X 1 -- 
X 2 -- 
X 3 
__< 2 
-- 2x 1 + 5X 2 -- 3X 3 -- 3x 4 < 10 
2x 1 -- 5X 2 
-~- 3x 4 < 
5 
xj>_0, 
j= 
1,2,3,4. 

118 
Chapter 2 The Simplex Method 
To solve this problem 
by the simplex method, we first convert the 
problem to canonical form by adding slack variables obtaining 
Maximize 
z = 2x I + 3x 2 + X 3 q- X 4 
subject to 
X 1 -- 
X 2 -- 
X 3 
+X 
5 
-- 
2 
--2X 
1 + 
5X 2 -- 3X 3 -- 3X 4 
+ 
X 6 
-- 10 
2x I - 5x 2 
+ 3x 4 
+ X 7 -- 
5 
xj>_O, 
j = 1,2,...,7. 
The initial tableau is Tableau 2.8; the following tableaux are Tableaux 2.9 
and 2.10. 
In Tableau 2.10, the most negative entry in the objective row is - 
-~, so 
the departing variable is x3. However, none of the entries in the pivotal 
column (the third column) is positive, so we conclude 
that the given 
problem has no finite optimal solution. 
/~ 
Tableau 2.8 
Xl 
X2 
X 3 
X4 
X 5 
X 6 
X7 
X 5 
1 
1 
1 
0 
1 
0 
0 
2 
X 6 
2 
(~) 
3 
3 
0 
1 
0 
10 
X 7 
2 
5 
0 
3 
0 
0 
1 
5 
2 
3 
1 
1 
0 
0 
0 
0 
Tableau 2.9 
X1 
X2 
X 3 
X4 
X 5 
X6 
X7 
Xs 
3 
0 
8 
3 
1 
0 
4 
5 
~ 
1 
2 
1 
3 
3 
0 
1 
0 
2 
X2 
5 
5 
5 
5 
x 7 
0 
0 
3 
0 
0 
1 
1 
15 
~6 
o 
~ 
~ 
o 
5 
~ 
0 
6 
Tableau 2.10 
X1 
X 2 
X 3 
X4 
X 5 
X 6 
X7 
x 1 
1 
x 2 
0 
x 7 
0 
8 
5 
1 
0 
2O 
0 
~ 
1 
~ 
~ 
3 
5 
2 
1 
0 
1 
~ 
1 
~ 
3 
0 
3 
0 
0 
1 
1 
15 
5 
0 
0 
343 
6 
16 
x 
0 
s~ 

2.1 The Simplex Method for Problems in Standard Form 
119 
2.1 
EXERCISES 
In Exercises 1 and 2, set up the initial simplex tableau. 
1. Maximize z = 2x + 5y 
subject to 
3x+5y 
< 
8 
2x + 7y < 12 
x>0, 
y>0. 
2. Maximize z = x 1 -[- 3x 2 + 5x 3 
subject to 
2x I - 5x 2 + X 3 _~< 3 
X 1 "~- 4x 2 
< 5 
X 1 >__ O, 
X 2 >_~ O, 
X 3 >_~ O. 
3. Consider the following simplex tableau. 
Xl 
X 2 
X 3 
X4 
X 5 
X6 
X7 
5 
6 
X 4 
0 
0 
2 
1 
X 
0 
0 
x 1 
1 
0 
5 
0 
3 
0 
-2 
2_ 
7 
5 
x 6 
0 
0 
3 
0 
4 
1 
4 
y 
X 2 
0 
1 
0 
0 
2 
0 
0 
• 
2 
7 
Determine the departing variable if the entering variable is (a) x5; (b) x3; 
(c) x7. 
In Exercises 4-7 use one iteration of the simplex algorithm to obtain the next 
tableau from the given tableau. 
X1 
X 2 
X 3 
X4 
4 
0 
2 
0 
12 
X4 
3 
0 
5 
7 
3 
1 
6 
X2 
2 
1 
2 
0 
8 
3 

"1 ~0 
Chapter 2 
The Simplex Method 
X1 
X 2 
X3 
X4 
x 1 
1 
2 
0 
1 
1 
1 
-1 
x 3 
0 
0 
4 
0 
4 
X 3 
X2 
X5 
X 1 
X2 
X 3 
X 4 
X 5 
2 
3 
3 
-~ 
0 
1 
~ 
0 
-~ 
3 
1 
0 
1 
0 
! 
2 
2 
2 
2 
5 
0 
0 
-~ 
1 
g 
7 
4 
0 
0 
5 
0 
X1 
X2 
X3 
X4 
x 2 
1 
1 
5 
0 
4 
x 4 
1 
0 
2 
1 
6 
3 
0 
-2 
0 
7 
8. (a) The following tableau arose in the course of using the simplex algorithm to 
solve a linear programming problem. What basic feasible solution does this 
tableau represent? 
X1 
X2 
X3 
X4 
X 5 
X 6 
X 7 
4 
2 
1 
0 
~ 
~ 
0 
1 
0 
1 
2 
1 
0 
~ 
~ 
1 
0 
1 
1 
1 
1 
1 
1 
~ 
~ 
~- 
0 
0 
0 
5 
4 
1 
0 
0 
5 
3 
3 
]" 
4 
10 
4 
12 
(b) Perform one operation of the simplex algorithm on the tableau. What basic 
feasible solution does this new tableau represent? 
9. Consider the following tableau, which arose in solving a linear programming 
problem by the simplex method. 

2.1 The Simplex Method for Problems in Standard Form 
1 ~1 
X 1 
X 2 
X 3 
U 
U 
W 
1 
5 
2 
0 
0 
3 
0 
2 
4 
1 
0 
4 
0 
2 
1 
0 
1 
3 
..... 
0 
5 
3 
0 
0 
3 
20 
6 
12 
12 
(a) Identify the basic feasible solution and basic variables in this tableau. 
(b) Compute the next tableau using the simplex method. 
(c) Identify the basic feasible solution and basic variables in the tableau in (b). 
In Exercises 10-23 solve the indicated linear programming problem using the 
simplex method. 
10. Example 4, Section 1.1. 
11. Example 7a, Section 1.1. 
12. Example 7b, Section 1.1. 
13. Example 10, Section 1.1. 
14. Exercise 4, Section 1.1. 
15. Exercise 5, Section 1.1. 
16. Exercise 7, Section 1.1. 
17. Exercise 9, Section 1.1. 
18. Exercise 2, Section 1.5. 
19. Maximize z = 2x 1 + 3x 2 -x 3 
subject to 
x I + 2x 2 - 
x 3 < 
6 
X 1 -- 3X 2 -- 3X 3 < 10 
xj>_O, 
j- 
1,2,3. 
20. Maximize z = x I + 2x 2 + X 3 "q-X 4 
subject to 
2x I + 
x 2 + 3x 3 + 
x 4 __~ 
8 
2x 1 4- 3x 2 
4- 4x 4 < 
12 
3x I + 
x 2 + 2x 3 
< 18 
xj>0, 
j= 
1,2,3,4. 
21. Maximize z = 5x 1 + 2x 2 + x 3 d-x 4 
subject to 
2x I + 
X 2 -t-X 3 -~- 2x 4 < 
6 
3x I 
+ X 3 
__< 15 
5X 1 -'1- 4X 2 
4- 
X 4 < 24 
xj>0, 
j= 
1,2,3,4. 

122 
Chapter 2 The Simplex Method 
22. Maximize z = -x I + 3x z + X 3 
subject to 
--X 1 4;- 2x 2 - 
7x 3 _<< 6 
x 1 + 
x 2 -- 3x 3 < 
15 
xj> 
O, j= 1,2,3. 
23. Maximize z = 3x 1 + 3x 2 -x 
3 + X 4 
subject to 
2x I -- X 2 -- x 3 + 
x 4 _< 
2 
X 1 --X 2 + X 3 -- 
X 4 __< 
5 
3xa + X 2 
+ 5X 4 __< 12 
xj>_ O, j- 
1,2,3,4. 
24. Suppose a linear programming problem has a constraint of the form 
3x I + 2x 2 + 
5x 3 -- 2x 4 >__ 12. 
Why can we not solve this problem using the simplex method as described up 
to this point? (In Section 2.3 we develop techniques for handling this situation.) 
2.2 DEGENERACY AND CYCLING (OPTIONAL) 
In choosing the departing variable, we computed the minimum 0-ratio. 
If the minimum 0-ratio occurs, say, in the rth row of a tableau, we drop 
the variable that labels that row. Now suppose that there is a tie for 
minimum 0-ratio, so that several variables are candidates for departing 
variable. We choose one of the candidates by using an arbitrary rule such 
as dropping the variable with the smallest subscript. However, there are 
potential difficulties any time such an arbitrary choice must be made. We 
now examine these difficulties. 
Suppose that the 0-ratios for the rth and sth rows of a tableau are the 
same and their value is the minimum value of all the 0-ratios. These two 
rows of the tableau are shown in Tableau 2.11 with the label on the rth 
row marked as the departing variable. The 0-ratios of these two rows are 
br/arj = b~/asj. 
Tableau 2.11 
,1, 
X 1 
X 2 
"'" 
Xj 
"'" 
Xn + m 
Xi r 
ar 1 
ar 2 
"'" 
~ 
"'" 
ar, n + m 
br 
Xi s 
asl 
as2 
"'" 
asj 
"'" 
as, n + m 
bs 
9 

2.2 Degeneracy and Cycling (Optional) 
123 
Tableau 2.12 
X 1 
X 2 
"'" 
Xj 
"" 
Xn+ m 
xj 
a,llarj 
a,21arj 
." 
i 
... 
ar, n+mlarj 
Xis 
* 
* 
""" 
6 
9 "" 
* 
"br/arj 
"b s 
asj . br/arj 
When we pivot in Tableau 2.11, we obtain Tableau 2.12, where 9 
indicates an entry whose value we are not concerned about. Setting the 
nonbasic variables in Tableau 2.12 equal to zero, we find that 
and 
Xj = br//arj 
Xis 
-- b s - 
asj'br/arj 
= asj(bs/asj 
- 
br/arj) 
= O. 
Consequently, the tie among the 0-ratios has produced a basic variable 
whose value is 0. 
DEFINmON. 
A basic feasible solution in which some basic variables 
are zero is called degenerate. 
EXAMPLE 1 (DEGENERACY). Consider the linear programming problem 
in standard form 
Maximize 
z -- 5x I ~- 3x 3 
subject to 
x 1 - 
x 2 < 2 
2x 1 -t- 
x 2 < 4 
-- 3x 1 -i- 2x 2 < 6 
X 1 ~_~ O, 
X 2 >" O. 
The region of all feasible solutions is shown in Figure 2.3. The extreme 
points and corresponding values of the objective function are given in 
Table 2.1. The simplex method leads to the following tableaux. In Tableau 
2.13 we have two candidates for the departing variable: x 3 and x 4 since 
the 0-ratios are equal. Choosing x 3 gives Tableaux 2.13, 2.14, 2.15, and 
2.16. Choosing x 4 gives Tableaux 2.13a, 21.4a, and 2.15a. Note that 
Tableaux 2.15a and 2.16 are the same except for the order of the con- 
straint rows. 

124 
Chapter 2 The Simplex Method 
TABLE 2.1 
Extreme point 
Value ofz = 5x 1 + 3x 2 
(0,0) 
0 
(2,0) 
10 
(0,3) 
9 
(~,~) 
~ 
-7 
Tableau 2.13 
Xl 
X 2 
X3 
X4 
X5 
x3 
C) 
1 
1 
0 
0 
2 
x 4 
2 
1 
0 
1 
0 
4 
x 5 
3 
2 
0 
0 
1 
6 
5 
3 
0 
0 
0 
Tableau 2.14 
X1 
X4 
X5 
Xl 
X 2 
X 3 
X4 
X5 
1 
1 
1 
0 
0 
o 
| 
2 
1 
o 
0 
1 
3 
0 
1 
0 
8 
5 
0 
0 
2 
0 
12 
10 

2.2 Degeneracy and Cycling (Optional) 
125 
Tableau 2.15 
X1 
X2 
X5 
Tableau 2.16 
X1 
X2 
X3 
Tableau 2.13a 
X1 
X2 
X 3 
X4 
X5 
1 
1 
0 
2 
1 
0 
~ 
0 
1 
2 
1 
0 
0 
3 
3 
1 
12 
0 
0 
31 
1 
8 
0 
10 
0 
0 
3 
X1 
X 2 
X3 
X4 
X 5 
2 
1 
2 
1 
0 
0 
y 
~ 
3 
2 
0 
1 
0 
y 
y 
2~ 
1 
3 
36 
0 
0 
1 
y 
y 
7 
0 
0 
0 
19 
1 
82 
7 
7 
7 
Xl 
X2 
X 3 
X4 
X 5 
X 3 
1 
1 
1 
0 
0 
2 
X 4 
Q 
1 
0 
1 
0 
4 
X 5 
3 
2 
0 
0 
1 
6 
5 
3 
0 
0 
0 
0 
Tableau 2.14a 
X1 
X 3 
0 
x I 
1 
x 5 
0 
0 
Tableau 2.15a 
X2 
X3 
X4 
X 5 
3 
1 
1 
2 
~ 
0 
0 
0 
~ 
0 
2 
2 
2 
3 
Q 
0 
~ 
1 
12 
1 
5 
0 
y 
0 
10 
1 
3 
36 
x 3 
0 
0 
1 
~ 
~- 
7 
2 
1 
2 
x~ 
1 
0 
0 
~- 
7 
y 
3 
2 
24 
X 2 
0 
1 
0 
y 
v 
7 
1 
82 
0 
0 
0 
19 
~- 
7 
X1 
X 2 
X3 
X4 
X5 

126 
Chapter 2 
The Simplex Method 
The optimal solution is 
2 
24 
X1 = 
7, 
X2 "- --q-, 
with the optimal value of the objective function being 
82 
Z = 
--q-. 
The slack variables have values 
__ 36 
=0 
X 5 =0. 
X 3 -- --q-, 
X4 
What is happening geometrically? We start with the initial basic feasible 
solution as the origin (0, 0), where z = 0. If we choose to replace x 3 with 
x 1, we move to the adjacent extreme point (2, 0), where z = 10 (Tableau 
2.14). Now we replace x 4 with x 2 and remain at (2, 0) (Tableau 2.15). 
2 
Finally we replace x 5 with x 3 and move to (7,-~), where z = -~. This is 
our optimal solution (Tableau 2.16). 
Alternatively, because the 0-ratios are equal we could replace x4 with 
x 1. The pivot step with this choice again moves us from (0, 0) to (2, 0), 
where z = 10 (Tableau 2.14a). However, at the next stage, x 3, which has 
value 0 and is the degenerate variable, is not a departing variable. Instead, 
x5 is the departing variable, and we move immediately to the optimal 
2 
solution (Tableau 2.15a) at (7, ~). 
A 
In general, in the case of degeneracy, an extreme point is the intersec- 
tion of too many hyperplanes. For example, degeneracy occurs in R 2 when 
three or more lines intersect at a point, degeneracy occurs in R 3 when 
four or more planes intersect at a point, and so on. 
Cycling 
If no degenerate solution occurs in the course of the simplex method, 
then the value of z increases as we go from one basic feasible solution to 
an adjacent basic feasible solution. Since the number of basic feasible 
solutions is finite, the simplex method eventually stops. However, if we 
have a degenerate basic feasible solution and if a basic variable whose 
value is zero departs, then the value of z does not change. To see this, 
observe that z increases by a multiple of the value in the rightmost column 
of the pivotal row. But this value is zero, so that z does not increase. 
Therefore, after several steps of the simplex method we may return to a 
basic feasible solution that we already have encountered. In this case the 
simplex method is said to be cycling and will never terminate by finding an 
optimal solution or concluding that no bounded optimal solution exists. 
Cycling can only occur in the presence of degeneracy, but many linear 
programming problems that are degenerate do not cycle (see Example 1). 
Examples of problems that cycle are difficult to construct and rarely 
occur in practice. However, Kotiah and Steinberg (see Further Reading) 

2.2 Degeneracy and Cycling (Optional) 
127 
have discovered a linear programming problem arising in the solution of a 
practical queuing model that does cycle. Also, Beale (see Further Reading) 
has constructed the following example of a smaller problem that cycles 
after a few steps. 
EXAMPLE 2 (CYCLING). 
Consider the following linear programming 
problem in canonical form. 
Maximize 
z = lOx I -- 57x 2 -- 9x 3 -- 24X 4 
subject to 
1 
m _~X 2 
5 
~-X 1 
--~X 3 -[-9X 4 + X 5 
= 0 
1 
3 
1 
~X 1 -- ~X 2 -- ~X 3 + 
X 4 
-~- X 6 
-- 0 
Xl 
+x 7 = 1 
Xy>_O, 
j= 
1,...,7. 
Using the simplex method we obtain the following sequence of tableaux. 
Tableau 2.17 
X 1 
X 2 
X3 
X4 
X5 
X6 
X7 
5 
x 5 
@ 
89 
~ 
9 
1 
0 
0 
1 
3 
1 
X6 
2 
2 
~ 
1 
0 
1 
0 
x 7 
1 
0 
0 
0 
0 
0 
1 
10 
57 
9 
24 
0 
0 
0 
Tableau 2.18 
X1 
X2 
X 3 
X4 
X5 
X6 
X7 
X 1 
1 
11 
5 
18 
2 
0 
0 
X 6 
0 
@ 
2 
8 
1 
1 
0 
X 7 
0 
11 
5 
18 
2 
0 
1 
0 
53 
41 
204 
20 
0 
0 
Tableau 2.19 
X 1 
X2 
X 3 
X4 
X5 
X 6 
X7 
X 1 
1 
x 2 
0 
x 7 
0 
3 
o 
@ 
o o  
1 
2 
1 
1 
0 
0 
1 
~ 
~ 
1 
3 
~ 
1 
1 
0 
~ 
4 
0 
29 
98 
27 
53 
0 
0 
2 
4 
4 

128 
Chapter 2 The Simplex Method 
Tableau 2.20 
X1 
X 3 
2 
X 2 
1 
X 7 
1 
29 
X 2 
X 3 
X 4 
X 5 
X 6 
X 7 
0 
1 
8 
3 
11 
0 
0 
2 
1 
5 
0 
0 
1 
0 
@ 
~" 
2 
0 
0 
0 
0 
0 
1 
1 
0 
0 
18 
15 
93 
0 
0 
Tableau 2.21 
$ 
X 1 
X2 
X3 
X4 
X 5 
X 6 
X 7 
x 3 
2 
1 
X4 
x 7 
1 
20 
9 
4 
1 
0 
~ 
0 
0 
! 
0 
1 
1 
5 
0 
0 
2 
4 
4 
0 
0 
0 
0 
0 
1 
1 
9 
0 
0 
2~ 
141 
0 
Tableau 2.22 
X1 
x 5 
-4 
1 
X 4 
~" 
x 7 
1 
-22 
$ 
X 2 
X3 
X4 
X5 
X6 
8 
2 
0 
1 
9 
3 
1 
2 
2 
1 
0 
(~) 
0 
0 
0 
0 
0 
93 
21 
0 
0 
24 
X7 
0 
0 
0 
0 
1 
1 
0 
0 
Tableau 2.23 
X 1 
X2 
X3 
X4 
X5 
X6 
1 
~ 
5 
9 
1 
0 
X5 
2 
2 
1 
3 
1 
X6 
2" 
2 
~ 
1 
0 
1 
x 7 
1 
0 
0 
0 
0 
0 
10 
57 
9 
24 
0 
0 
X7 
0 
0 
0 
0 
1 
1 
0 
0 
Observe that Tableau 2.23 is identical to Tableau 2.17, and, thus, the 
simplex method has cycled. 
A 
Computer programs designed for large linear programming problems 
provide several options for dealing with degeneracy and cycling. One 
option is to ignore degeneracy and to assume that cycling will not occur. 
Another option is to use Bland's Rule for choosing entering and departing 
variables to avoid cycling. This rule modifies Step 3 and 4 of the Simplex 
Method. 

2.2 Degeneracy and Cycling (Optional) 
129 
Bland's Rule 
1. Selecting the pivotal column. Choose the column with the smallest 
subscript from among those columns with negative entries in the objective 
row instead of choosing the column with the most negative entry in the 
objective row. 
2. Selecting the pivotal row. If two or more rows have equal 0-ratios, 
choose the row labeled by the basic variable with the smallest subscript, 
instead of making an arbitrary choice. 
Bland showed that if these rules are used, then, in the event of degeneracy, 
cycling will not occur and the simplex method will terminate. 
EXAMPLE 3. 
Referring to the tableaux from Example 2, note that 
Bland's rule affects only the choice of entering variable in Tableau 2.22. 
Applying the rule and rewriting Tableau 2.22 with the new choice of 
entering and departing variables, we obtain Tableau 2.23a. 
Tableau 2.22 
Xl 
X2 
X 3 
X4 
X 5 
X 6 
x 5 
4 
8 
2 
0 
1 
9 
@ 
3 
1 
X4 
2 
~ 
1 
0 
1 
X 7 
1 
0 
0 
0 
0 
0 
22 
93 
21 
0 
0 
24 
X7 
0 
0 
0 
0 
1 
1 
0 
0 
Tableau 2.23a 
X1 
X2 
X 3 
X4 
X 5 
X 6 
X7 
x 5 
0 
4 
-2 
8 
1 
1 
0 
0 
x I 
1 
-3 
-1 
2 
0 
2 
0 
0 
x 7 
0 
3 
C) 
-2 
0 
2 
1 
1 
0 
27 
1 
44 
0 
20 
0 
0 
We perform the pivot step to obtain Tableau 2.24a, which represents an 
optimal solution. The cycling has been broken. 
Tableau 2.24a 
X1 
X2 
X 3 
X4 
X 5 
X6 
X7 
x 5 
0 
2 
0 
4 
1 
5 
2 
2 
x 1 
1 
0 
0 
0 
0 
0 
1 
1 
x 3 
0 
3 
1 
-2 
0 
-2 
1 
1 
0 
30 
0 
42 
0 
18 
1 
1 
A 

130 
Chapter 2 
The Simplex Method 
2.2 
EXERCISES 
In Exercises 1-6 solve the indicated linear programming problem, noting where 
degeneracies occur. Sketch the set of feasible solutions, indicating the order in 
which the extreme points are examined by the simplex algorithm. 
1. Maximize z = 
6x 1 + 5x 2 
subject to 
2. Maximize z = 5x i + 4x2 
subject to 
3x I -- 2x 2 < 0 
3x I + 2x 2 < 15 
x I >_~ O, 
x 2 >_~ O. 
x 1 + 2x 2 < 8 
x I -- 2x e < 4 
3x I + 2x 2 < 12 
x I >_~ O, 
x 2 >_~ O. 
3. Maximize z = 
3x I -t- 2x 2 + 5x 3 
subject to 
2x 1 -- 
x 2 -I- 4X 3 < 12 
4x 1 -t- 3x 2 q- 6X 3 < 18 
X 1 >_~ O, 
X 2 >__ O, 
X 3 >_~ O. 
4. Maximize z = 5x I d- 8x 2 q-x 3 
subject to 
X 1 -~- 
X 2 -~- 
X 3 _< 7 
2x 1 -I- 3x 2 + 3x 3 < 12 
3x 1 q- 6X 2 -I- 5x 3 __< 24 
X 1 >_~ O, 
X 2 >__ O, 
X 3 >_~ O. 
5. Maximize z = 
6x I + 5x 2 
subject to 
4x I -t- 3x 2 < 19 
X 1 -- 
X 2 _<_3 
X 1 -- 2X 2 < 2 
3x 1 d- 4x 2 < 18 
x 1 >_~ O, 
x 2 >_~ O. 

2.3 Artificial Variables 
131 
6. Maximize z = 
5x I -I- 3x 2 
subject to 
2X 1 d-X 2 __~ 6 
2X 1 --x 2 >__ 0 
X1--X 2 ~__0 
xj>_0, 
j= 1,2. 
7. If a degeneracy arose in any of the exercises above, use all choices of the 
departing variable. 
In Exercises 8 and 9, 
(a) Show that cycling occurs when solving the problem using the simplex 
method. 
(b) Use Brand's Rule to terminate cycling and obtain an optimal solution, if one 
exists. 
8. Minimize z = -x I + 7x 2 + x 3 + 2x 4 
subject to 
X 1 + 
X 2 + 
X 3 + 
X 4 + x 5 
= 
1 
1 
_ 
_~X2 
5 
~x I 
-- 7x 3 + 9x 4 
+ x 6 
= 0 
1 
3 
1 
~X 1 -- 
~X 2 -- ~X 3 + 
X 4 
+ X 7 = 
0 
xj~0, 
j= 1 ..... 7 
(due to K. T. Marshall and J. W. Suurballe). 
9. Minimize z = - 2x I - 2x 2 + 9x3 
subject to 
3 
-- -~X 2 -k- -~X 3 -t-X 4 
~x1 
1 
9 
3 
~X 1 -- 
~X 2 -q- 
~X 3 
2 
8 
1 
~X 1 -- 
~X 2 -q- 
~X 3 
X 2 
xj>_0, 
(due to K. T. Marshall and J. W. Suurballe). 
=0 
+x 5 
=0 
+x 6 
=0 
+XT=l 
j= 1,...,7 
2.3 ARTIFICIAL VARIABLES 
In the previous two sections we discussed the simplex algorithm as a 
method of solving certain types of linear programming problems. Recall 
that we restricted our discussion to those linear programming problems 
that could be put in standard form, and we considered only those prob- 
lems in that form that had a nonnegative right-hand side. That is, we have 

1~ 
Chapter 2 The Simplex Method 
assumed that all our constraints were of the form 
n 
E aijxj ~ bi, 
i = 1,2,..., m, 
j=l 
where 
bi>_O. 
(1) 
X = (0, 0,..., 
0, b l, b2,..., b m). 
Furthermore, this solution was feasible since b i >_ 0 for i = 1, 2,..., m. 
Unfortunately, there are many linear programming problems in which 
not all the constraints can be transformed to the form of (1). For example, 
the constraint 
can be changed to 
2x + 3y > 4 
(2) 
-2x-3y 
< -4, 
but then the right-hand side is negative. By adding a slack variable, we 
obtain 
-2x-3y+u= 
-4. 
(3) 
Setting the nonslack variables equal to zero gives us u = -4, which will 
not yield a feasible solution. 
When we examine the procedure described above for finding an initial 
basic feasible solution, we see that it was not important that the procedure 
was applied to a problem in canonical form coming from a problem in 
standard form. It was important that the problem was in canonical form, 
that the right-hand side of Ax = b was nonnegative, and that in each 
equation of the system of constraints there was a variable with coefficient 
+ 1 that appeared in no other equation. Then setting all but these 
"special" variables equal to zero, we again would have an initial basic 
feasible solution. 
However, there may be equations in the canonical form of a problem in 
which no such "special" variable exists. In Equation (3)we can make the 
right-hand side positive so that the equation reads 
2x+3y-u=4. 
But now no coefficient is + 1, and presumably x and y appear in other 
equations of the system of constraints, so that they cannot be chosen as 
"special "variables. 
The assumption that b > 0 enabled us to easily find an initial basic feasible 
solution. For, when we introduced slack variables, we found that we could 
set the nonslack variables equal to zero and obtain the basic solution 

2.3 Artificial Variables 
133 
We now develop a procedure for introducing a variable that can serve 
as a basic variable for an initial basic feasible solution. We start with the 
general linear programming problem 
Maximize 
z = cTx 
(4) 
subject to 
allX 1 + a12x2 + ... +alnX n (<)(=)(<)b 
1 
a21x 1 -+-a22x2 -+-... +a2nXn 
(_<) (--) (>__) b 2 
(5) 
9 
, 
9 
9 
9 
9 
9 
9 
9 
9 
9 
amlX 1 -k- am2X 2 -4r ...-k-amnXn(~)('-)(>__)bm 
xj>0, 
j= 1,2,...,n. 
(6) 
Note that we have assumed that the problem is a maximization problem. 
Any minimization problem can be converted to a maximization problem by 
multiplying the objective function by -1. 
We now write each constraint in (5) so that its right-hand side is 
nonnegative. This can be done by multiplying those constraints with 
negative right-hand sides by -1. The constraints in (5) can now be 
rearranged and renumbered so that the < constraints come first, then the 
> constraints, and finally the = constraints. Assume that there are r 1 < 
constraints, r 2 > constraints and r 3 --- constraints. Then we can assume 
that (5) is in the following form. 
allX 1 d- a12x 2 d- ... d- alnX n ~ b 1, 
a21x1 + a22x 2 d- --- -I- a2nX n <_~ b2, 
9 
~ 
~ 
~ 
9 
~ 
9 
arllX 1 + arl2X 2 -t- "'" -t- arlnX n ~ brl , 
! 
! 
allXld- a'12x 2d- ... d- alnX n>__ b' 1, 
! 
! 
a21xl + a'22x 2 -f- "" + a'z,,X,, > b 2 , 
~ 
9 
9 
9 
o 
9 
~ 
~ 
a' 
' 
a' 
[~' 
rE1X1 ~ arE2X 2 4;- "'" "t- 
rEnXn ~_~ 
rE ~ 
P! 
P! 
f 
PP 
allXld- al2x 2d- "- d- dlnX n= b l, 
~lX 
t 
tt 
~_ 
n 
1 -~- at22x2 -1- "'" -~- aZnXn 
b2, 
9 
~ 
~ 
~ 
9 
~ 
~ 
' 
' 
" 
= 
b" 
anr31Xl-~-at~r32X 2 + --. + ar3nX n 
r 3, 
b 1 >__0 
b 2 >0 j 
brl >___0 
b'l>_O 
b' _ 
2>0 
b' >0 
r 2 
-
-
 
b 
Pp 1~__0 
b" 2>_0 
b" > 0 
r 3 
- -  
(7a) 
(Tb) 
(7c) 
Next we make each inequality in (7a) and (7b) into an equality by 
introducing a slack variable. Each slack variable must be nonnegative so 

1 ~ 
Chapter 2 
The Simplex Method 
that in (7a) it is introduced with a + sign and in (7b) it is introduced with a 
- 
sign. We now write (7a) and (7b) as 
~ aijx ] -t- x n+i -- bi, 
bi > 0, 
i = 1,2,..., r I 
(8a) 
j=l 
aijxj -- Xn +r 1 +i 
"
-
-
 bi, 
b i >_ 0, 
i - 1,2,..., r 2. 
(8b) 
j=l 
EXAMPLE 1. 
Consider the linear programming problem 
Maximize 
z=2x 1+5x 2 
subject to 
2x I + 3x 2 < 6 
- 
2x 1 + 
x 2 < 
-2 
x I - 6x 2 = -2 
X 1 ~ 0, 
X 2 >_~ 0. 
We first rewrite the constraints so that the right-hand sides are non- 
negative. The problem becomes 
Maximize 
z=2x 1+5x 2 
subject to 
2x 1 + 3x 2 < 6 
2X 1 -- 
X 2 >_ 2 
-x 
1 -~- 6x 2 = 2 
x 1 >_~ 0, 
x 2 >_~ 0. 
We now insert slack variables in each of the inequalities, obtaining an 
associated canonical form problem: 
Maximize 
z = 2x 1 + 5X 2 
subject to 
2Xl + 3x2 + x3 
2x I - 
x 2 
--X 1 d- 6x 2 
:6 / 
--X 4 
2 
2 
(9) 
xj>O, 
j= 
1,2,3,4. 
However, we do not have a variable in each equation that can act as a 
basic variable for an initial basic feasible solution. In fact, in (9) both the 
second and third equations do not have such a variable. 
A 

2.3 Artificial Variables 
135 
As Example 1 shows, we can convert the general linear programming 
problem in (4), (5), and (6) to an associated problem in canonical form with 
b > 0. Thus, we may now assume that we have a problem in the following 
form: 
s 
Maximize 
z "-- E cjxj 
(10) 
j=l 
subject to 
~ aijx j -- bi, 
j=l 
i = 1,2,...,m 
(11) 
xj>0, 
j= 1,2 .... ,s 
(12) 
with b~ > 0, i = 1,2,..., m. The following method of finding an initial 
basic feasible solution is widely used in modem computer codes. 
Two-Phase Method 
To enable us to obtain an initial basic feasible solution, we introduce 
another variable into each equation in (11). We denote the variable for the 
ith equation by Yi. The variables yi, i = 1,2,..., m, are called artificial 
variables and have no physical significance. Assuming the profit associated 
with each y~ to be zero, we obtain the problem 
Maximize 
z = ~ CjXy 
(13) 
j=l 
subject to 
~ aijx j Jr- Yi -- bi, 
j=l 
xj>0, 
j= 1,2,...,s; 
with b i > O, i = 1,2,...,m. 
i= 1,2,...,m 
(14) 
Yi >0, 
i= 1,2,...,m 
(15) 
It is easy to see (Exercise 24) that the vector x in R s is a feasible 
solution to the problem given by (10), (11), and (12) if and only if the 
vector 
[;1 
in R s+m is a feasible solution to the problem given by (13), (14), and (15). 
Note that it is easy to find an initial basic feasible solution to the latter 
problem, namely, x = 0, y = b. We now develop a way to use the simplex 
algorithm to change this initial basic feasible solution into a basic feasible 
solution to the same problem in which y = 0. Thus, we will have found a 

136 
Chapter 2 
The Simplex Method 
basic feasible solution to the problem given by (10), (11), and (12). This 
procedure is Phase 1 of the two-phase method. 
Phase 1 
Since each Yi is constrainted to be nonnegative, one way of guarantee- 
ing that each Yi is zero is to make the sum of the yi's zero. Thus, we set up 
an auxiliary problem in which we minimize the sum of the yi's subject to 
the constraints (14) and (15) and hope that this minimum value is zero. If 
this minimum value is not zero, then it must be positive, and at least one 
of the yi's must be positive. Furthermore, the yi's will never all be zero, 
since we have found the minimum value of their sum. Thus, in this case 
the original problem has no feasible solution. 
We convert the canonical linear programming given by (10), (11), and 
(12) to the form involving artificial variables and introduce the new 
objective function. The resulting problem is 
m 
Minimize 
z' --- ~'~ Yi 
(16) 
i=1 
subject to 
s 
ai.ix j -F Yi -- bi, 
j=l 
i = 1,2,...,m 
(17) 
with b/> 0, i = 1, 2,...,m. 
This problem has the initial basic feasible solution 
[0 
0 
"'" 
0 
b 1 
b 2 
bm] T 
obtained by setting 
X 1 "- O, 
X 2 -- O,...,X 
s -~ 0 
as nonbasic variables and solving (17) for 
Yl = bl, 
Y2 = b2,---, Ym -- bm. 
Writing the problem given by (16), (17), and (18) in matrix form, we find 
that the columns corresponding to Y l, Y2,---, Ym are the columns of an 
rn x m identity matrix and, thus, are linearly independent. Therefore, this 
procedure yields a basic solution for the given problem. 
To use the simplex method as it was developed earlier, we must multiply 
(16) by -1 to convert to a maximization problem and then write the result 
as 
m 
z + ~y/= 
0, 
(19) 
i=1 
where z = -z'. Recall that when the simplex method was first described, 
the initial basic variables were the slack variables, and these had zero 
xj > 0, 
j = 1,2,...,s; 
Yi > 0, 
i = 1,2,...,m 
(18) 

2.3 Artificial Variables 
137 
objective function coefficients. Consequently, the entries in the objective 
row in the columns labeled by the basic variables were zero initially and 
remained so after each pivoting step. This was necessary for the use of the 
optimality criterion. Therefore, we must eliminate y/, i = 1, 2,..., m, from 
(19). We can do this by solving (17) for Yi. 
Now 
s 
Yi = bi - 
E 
aijxj, 
j=l 
and, substituting into (19), we obtain 
Z + 
~ 
b i - 
a~jxj 
= O. 
i=l 
j=l 
Rearranging, we can write the objective equation as 
Z -- 
aij xj = -- E 
bi. 
(20) 
j=l 
i=l 
i=1 
We can now solve the problem as given by (20), (17), and (18) using the 
simplex method. 
The reader can experiment with problems that are not in standard form 
using the SMPX courseware described in Appendix C. One must include 
artificial variables when entering the problem and must indicate to SMPX 
that these variables are artificial. 
EXAMPLE 2. 
Consider the linear programming problem in canonical 
form 
Maximize 
z=x 1-2x 2-3x 3-x 4-x 5 +2x 6 
subject to 
x I + 2x 2 + 2x 3 + x 4 + 
x 5 
= 12 
X 1 + 2x 2 + 
x 3 -I--X 4 -t- 2x 5 + X 6 = 18 
3x I + 6x 2 + 2x 3 + x 4 + 3x 5 
= 24 
xj>O, 
j= 1,2,...,6. 
Introducing the artificial variables Yl, Y2, and Y3, we can write the 
auxiliary problem as 
Minimize 
subject to 
x I + 2x 2 + 2x 3 +x 4 + 
x 5 
X 1 "-]- 2x 2 + 
X 3 -'1-X 4 -+- 2x 5 + X 6 
3X 1 + 6X 2 + 2X 3 + X 4 + 3x 5 
z' = Yl + Y2 + Y3 
(21) 
+Yl 
12 / 
+ Y2 
18 
(22) 
+ Y3 
24 
xj>O, 
j= 1,2,...,6; 
y/>0, 
i= 1,2,3 
(23) 

138 
Chapter 2 
The Simplex Method 
After conversion to a maximization problem, the objective function (21) 
can be written as 
z' + Yl + Y2 + Y3 = 0. 
(24) 
To eliminate Yl, Y2, and Y3 from (24) we add -1 times each of the 
constraints in (22) to (24), obtaining 
z' - 5x 1 - 10x 2 -- 
5X 
3 
-- 3X 4 -- 6X 5 --X 6 = 
-54. 
(25) 
The initial basic feasible solution is 
X1 --X 2 -- X 3 -- X 4 --X 5 -- X 6 -- 0 
Yl = 12, 
Y2 -- 18, 
Y3 = 24 
(nonbasic variables) 
(basic variables). 
Using (25), (22), and (23), we can write the initial tableau (Tableau 2.25). 
Tableau 2.25 
X1 
X2 
X3 
X4 
X5 
X6 
Yl 
Y2 
Y3 
Yl 
1 
2 
2 
1 
1 
0 
1 
0 
0 
Y2 
1 
2 
1 
1 
2 
1 
0 
1 
0 
Y3 
3 
(~) 
2 
1 
3 
0 
0 
0 
1 
-5 
10 
5 
3 
6 
1 
0 
0 
0 
12 
18 
24 
54 
The most negative entry in the objective row is -10, so that x 2 is the 
entering variable and the second column is the pivotal column. The 
departing variable is determined by computing the minimum of the 0-ratios. 
We have 
min{ ,2 18 2~ } = 24 
2, 2, 
-6 - = 4 ,  
so that the row labeled by Y3 is the pivotal row and Y3 is the departing 
variable. The pivot is 6. 
We obtain Tableau 2.26 from Tableau 2.25 by pivoting (verify). In 
Tableau 2.26 we choose x 3 as the entering variable and Y l as the departing 
variable. We form Tableau 2.27 by pivoting. 
There is a tie in determining the entering variable in Tableau 2.27. We 
choose x 6 as the entering variable and obtain Tableau 2.28. 

2.3 Artificial Variables 
139 
Tableau 2.26 
x1 
Yl 
0 
Y2 
0 
1 
x 2 
~- 
0 
x2 
x3 
x4 
x5 
x6 
Yl 
Y2 
Y3 
2 
1 
0 
x 
0 
0 
1 
0 
~ 
4 
1 
2 
1 
0 
~ 
~ 
1 
1 
0 
1 
~ 
10 
1 
1 
1 
0 
0 
0 
1 
4 
1 
~ 
~ 
~ 
0 
5 
4 
5 
3 
~ 
1 
1 
0 
0 
14 
Tableau 2.27 
X1 
X2 
X3 
X4 
X5 
X6 
Yl 
Y2 
Y3 
1 
0 
0 
3 
1 
X 3 
O 
O 
1 
y 
~ 
O 
1 
1 
1 
1 
(~) 
~ 
1 
Y2 
0 
0 
0 
~ 
4 
1 
1 
O 
0 
1 
O 
1 
1 
X2 
~ 
~ 
a 
0 
1 
5 
0 
5 
O 
O 
O 
~ 
1 
1 
~ 
Tableau 2.28 
X1 
X2 
X3 
X4 
X5 
X6 
Yl 
Y2 
Y3 
1 
0 
0 
3 
0 
1 
X 3 
O 
O 
1 
~- 
r 
~ 
3 
1 
1 
1 
1 
1 
x 6 
0 
0 
0 
~- 
a 
1 
a 
9 
1 
1 
0 
0 
1 
0 
1 
1 
3 
x 2 
~ 
~ 
~ 
0 
~ 
0 
0 
0 
0 
0 
0 
1 
1 
1 
0 
At this point all the artificial variables are nonbasic variables and have 
value zero. Tableau 2.28 gives the basic feasible solution 
x2=3, 
x 3=3, 
x 6=9 
Xl -- X4 -- X5 = Yl = Y2 = Y3 = 0 
with objective function value z - 0. 
The reader can verify thatx=[0 
3 
3 
0 
0 
9] T is a basic feasible 
solution to the original problem without artificial variables. The introduc- 
tion of the artificial variables gave a systematic way of finding such a basic 
feasible solution. 
A 
If the solution to Phase 1 is a set of values for the variables that makes 
the objective function of the auxiliary problem equal to zero, then we may 
start Phase 2. There are two possibilities: (1) every artificial variable is a 
nonbasic variable in the final tableau of Phase 1, or (2) some artificial 
variables are still basic variables, with value 0, in the final tableau (see 

140 
Chapter 2 The Simplex Method 
Example 5). At this time we shall discuss only the first case. The second 
case is discussed in Section 3.3. 
Phase 2 
We assume that no artificial variable is a basic variable at the end of 
Phase 1 and the value of the objective function of the auxiliary problem is 
zero. The optimal solution obtained in Phase 1 is used to obtain an initial 
basic feasible solution for the original problem (10), (11), (12). By deleting 
the yi's from the optimal solution, we obtain a basic feasible solution to 
the original problem because we have assumed that no artificial variables 
appear in the optimal solution. The initial tableau for Phase 2 is the final 
tableau of Phase 1, with the following modifications. 
(a) Delete the columns labeled with artificial variables. 
(b) Calculate a new objective row as follows. Start with the objective 
function of the original problem (10). For each of the basic variables in the 
final tableau of Phase 1, make the entry in the objective row in the column 
labeled by that basic variable equal to zero by adding a suitable multiple of 
the row labeled by that basic variable. 
These two steps construct an initial tableau for Phase 2. We can now 
apply the simplex algorithm to this tableau. 
EXAMPLE 2 (CONTINUED). We now form the initial tableau for Phase 2 
from Tableau 2.28. We delete the columns labeled with Y l, Y2, and Y3 and 
then use the original objective function, 
Z -" X 1 -- 2x 2 - 3x 3 -- x 4 -- x 5 -I- 2x 6. 
The objective row would be 
- 1 
2 
3 
1 
1 
- 2 
0 
(26) 
But the entries in the second, third, and sixth columns must be zeroed for 
the optimality criterion to hold, since x 2, x 3, and x 6 are basic variables. 
We do this by adding -2 times the x 2 row to (26); also, we add -3 times 
the x 3 row and 2 times the x 6 row to (26). This calculation yields 
-1 
2 
3 
1 
1 
-2 
0 
[Equation (26)] 
-1 
-2 
0 
0 
-1 
0 
-6 
(-2timesx 2row) 
0 
0 
- 3 
3 
0 
0 
- 9 
(- 3 times x3 row) 
2 
0 
0 
0 
1 
2 
2 
18 
(2 times x 6 row) 
1 
- 2 
0 
0 
~ 
2 
0 
3 
(objective row for Phase 2). 
We then have the initial tableau for Phase 2 (Tableau 2.29), in which, as 
usual, we have not included the z column. 
We now continue with the simplex algorithm to find an optimal solu- 
tion. The next tableau is Tableau 2.30 (verify). 

2.3 Artificial Variables 
141 
Since the objective row in Tableau 2.30 has no negative entries, we have 
found an optimal solution, 
X 1 -- 6, 
X 2 -- O, 
X 3 = 3, 
X 4 -~ O, 
X 5 -" O, 
X 6 = 9, 
which gives z = 15 as the value of the objective function. 
/x 
Tableau 2.29 
x1 
x 3 
0 
x 6 
0 
x~ 
(~ 
2 
X2 
X 3 
X4 
X5 
X6 
1 
0 
0 
3 
0 
1 
1 
1 
1 
9 
0 
0 
0 
3 
1 
0 
0 
1 
2 
0 
3 
0 
0 
Tableau 2.30 
X1 
X2 
X 3 
X4 
X 5 
X6 
X 3 
0 
X 6 
0 
X 1 
1 
1 
0 
0 
3 
0 
1 
1 
1 
1 
9 
0 
0 
2 
0 
0 
1 
0 
6 
4 
0 
15 
0 
4 
0 
We had originally put an artificial variable in every equation. This 
method requires no decision steps but may cause more tableaux to be 
computed. If some of the equations have a variable that can be used as a 
basic variable, then it is not necessary to introduce an artificial variable 
into each of those equations. 
EXAMPLE 3. 
In the problem discussed in Example 2, we see that x 6 
can be used as a basic variable. That is, it appears in only one equation 
and, in that equation, has a coefficient of + 1. Consequently, we need to 
introduce artificial variables only into the first and third equations. Doing 
this, we obtain the auxiliary problem 
Minimize 
z' = y 1 + Y2 
subject to 
x 1 + 2x 2 + 2x 3 + X 4 ~- 
X 5 
-k-Yl 
= 12 
X 1 -~- 2x 2 + 
X 3 -~-X 4 -~- 2x 5 + X 6 
-- 18 
3Xl -+-6x2 + 2x3 + X4 +" 3x5 
+ Ye = 12 
xj>0, 
j= 
1,2,...,6; 
Yl >0, 
Y2 >0. 

142 
Chapter 2 The Simplex Method 
As before, we must eliminate the basic variables from the objective 
function, rewritten as in (24) by adding -1 times each of the constraints 
that includes an artificial variable to the rewritten objective function. We 
obtain 
z -- 4X 1 -- 
8x 
2 
-- 4X 3 -- 2X 4 -- 4X 5 = --36. 
This now leads to the sequence of Tableaux 2.31, 2.32, and 2.33. 
In Tableau 2.33 we have an optimal solution to the auxiliary problem in 
which all the artificial variables have value zero and are nonbasic variables. 
Thus, a basic feasible solution to the original problem is 
[0 
3 
3 
0 
0 
9] T, 
as we obtained in Example 2. 
A 
The system of equations (11) that gives the constraints for the canonical 
form of a linear programming problem always must have a solution 
Tableau 2.31 
X1 
Yl 
1 
x 6 
1 
Y2 
3 
4 
X2 
X3 
X4 
X5 
X6 
Yl 
Y2 
2 
2 
1 
1 
0 
1 
0 
2 
1 
1 
2 
1 
0 
0 
(~) 
2 
1 
3 
0 
0 
1 
8 
4 
2 
4 
0 
0 
0 
12 
18 
24 
36 
Tableau 2.32 
Xl 
Yl 
0 
x 6 
0 
1 
X2 
0 
,L 
X2 
X3 
X4 
X5 
X6 
Yl 
Y2 
(~ 
2 
1 
1 
0 
~ 
0 
0 
1 
2 
1 
1 
0 
1 
0 
~ 
~ 
1 
1 
1 
0 
0 
1 
1 
~- 
~ 
~ 
4 
2 
0 
0 
0 
4 
0 
3 
3 
4 
10 
4 
Tableau 2.33 
X1 
x 3 
0 
x 6 
0 
1 
X2 
0 
X2 
X3 
X4 
X5 
X6 
Yl 
Y2 
1 
3 
1 
0 
1 
~ 
0 
0 
~ 
~ 
3 
1 
1 
1 
1 
1 
9 
0 
0 
~ 
~ 
4 
1 
0 
1 
! 
3 
1 
0 
0 
~ 
~ 
0 
0 
0 
0 
0 
1 
1 
0 

2.3 Artificial Variables 
143 
if m < s and if there are m linearly independent columns in the coeffi- 
cient matrix. But it is possible that none of the solutions satisfies the 
nonnegative criterion (12). When the artificial variables are added to the 
system of constraints (14), there is always a solution, 
[ 0 
0 
"'" 
0 
b 1 
b 2 
"'" 
bm ]T, 
that satisfies the nonnegativity conditions (15). A solution to the system in 
(14) is a solution to the system in (11) if all the artificial variables have 
values equal to zero. We use the simplex algorithm to try to find a solution 
to (14) in which all the artificial variables are zero. The simplex algorithm 
is designed to find only solutions that satisfy the nonnegativity require- 
ments. Thus, if one of the y/ is positive when the simplex algorithm 
reaches an optimal solution to the auxiliary problem, this indicates that 
there are no solutions to (11) that satisfy the nonnegativity constraints (12). 
In this case, the original problem (10), (11), (12) has no feasible solutions. 
The following example illustrates this situation. 
EXAMPLE 4. 
Consider the general linear programming problem 
Maximize 
z = 2 Xl + 5X2 
subject to 
2x I + 3x 2 < 6 
X 1 + 
X2>_4 
X 1 >_~ O, 
X 2 ~_~ O~ 
By inserting slack variables x 3 and x4, we can write the problem in 
canonical form as 
Maximize 
z = 2x I + 5x 2 
subject to 
2X 1 -I- 3X 2 + X 3 
--6 
X 1 -~- 
X 2 
-- X 4 -- 4 
xj>_O, 
j= 1,2,3,4. 
We insert an artificial variable y into the second equation; x 3 can serve as 
the basic variable in the first equation. The auxiliary problem then has the 
form 
Minimize 
z' = y 
subject to 
2x I + 3x z + x 3 
= 6 
x I + 
x 2 
-x 4 +y = 4 
xj>0, 
j= 1,2,3,4; 
y>0. 

144 
Chapter 2 The Simplex Method 
After adding -1 times the second constraint to the rewritten objective 
function, we obtain 
Z' 
-- X 1 -- X 2 + 
X 4 -- -4. 
We then calculate the sequence of Tableaux 2.34, 2.35, and 2.36. 
Tableau 2.34 
Xl 
X2 
X 3 
X4 
Y 
X 3 
2 
(~) 
1 
0 
0 
y 
1 
1 
0 
1 
1 
1 
1 
0 
1 
0 
-4 
Tableau 2.35 
X 1 
X 2 
X 3 
X 4 
Y 
0 
0 
2 
X 2 
1 
~1 
1 
1 
y 
~ 
0 
~ 
1 
1 
2 
1 
1 
1 
0 
2 
0 
Tableau 2.36 
X1 
X2 
X 3 
X4 
Y 
3 
1 
0 
0 
3 
x I 
1 
~ 
1 
1 
y 
0 
~ 
~ 
1 
1 
1 
1 
1 
1 
0 
1 
0 
~ 
~- 
Tableau 2.36 represents an optimal solution to Phase 1 in which the 
artificial variable y has the value 1. This means that the given problem has 
no feasible solution. When we look at the graphs of the constraints (Figure 
2.4), we see that there is no intersection between the two half-spaces. The 
set of feasible solutions is empty. 
A 
We have already pointed out that an artificial variable can appear in an 
optimal solution to the auxiliary problem with a value of zero. In this case 
the given problem has a feasible solution. The following example illus- 
trates these ideas. We can complete Phase 1 in this section; in Section 3.3 
we will develop tools for handling Phase 2. 

2.3 Artificial Variables 
145 
EXAMPLE 
5. 
form 
Consider the linear programming problem in canonical 
Maximize 
z--x 
I -~- 2x 2 + x 3 
subject to 
3x~ + 
X 2 -- X 3 -- 15 
8x 1 + 4x 2 -x 3 -- 50 
2x 1 + 2x 2 + X 3 "--20 
X 1 >_~ 0, 
X 2 ~__ 0, 
X 3 >__ 0. 
In this problem we must introduce an artificial variable in each of the 
constraint equations. We obtain the auxiliary problem 
Maximize 
z' = YI + Y2 + Y3 
subject to 
3Xl + 
X2 -- X3 ~- Yl 
= 15 
8Xl -t- 4x2- x3 
-~-Y2 
= 50 
2x 1 + 2x 2 + X 3 
-'1- Y3 = 20 
Xj>_0, 
j-- 1,2,3; 
y~>_0, 
i-- 1,2,3 
Rewriting the objective function in the same manner as before, we obtain 
the initial tableau (Tableau 2.37) for the auxiliary problem. At the end of 
Phase 1 we obtain Tableau 2.38 which represents an optimal solution to 
the auxiliary problem with Y2 -- 0 as a basic variable. 
A 

146 
Chapter 2 
The Simplex Method 
Tableau 2.37 
,1, 
X1 
Yl 
Y2 
Y3 
x2 
x3 
Yl 
Y2 
Y3 
(~) 
1 
1 
1 
0 
0 
15 
8 
4 
1 
0 
1 
0 
50 
2 
2 
1 
0 
0 
1 
20 
13 
7 
1 
0 
0 
0 
85 
Tableau 2.38 
Xl 
X2 
X3 
Yl 
Y2 
Y3 
3 
1 
0 
1 
7 
X 1 
1 
3. 
O 
3. 
3. 
Y2 
0 
0 
0 
2 
1 
1 
0 
4 
1 
2 
0 
3 
6 
X 3 
O 
3" 
5 
3. 
O 
O 
O 
3 
O 
2 
O 
Figure 2.5 gives a flowchart and Figure 2.6, a structure diagram that 
summarizes the two-phase method. 
Phase 1 
Set up initial tableau [ 
for auxiliary problem J 
z = -Yl -Y2 ..... Yk 
_ 
1 
Solve auxiliary 
problem using 
simplex algorithm 
NO 
~ YES 
This path will be 
completed in 
Section 3.2 
FIGURE 2.5 
No feasible 
solutions to 
original 
problem 
CALL 
Phase 2 
algorithm 
Phase 2 
Input is final tableau from Phase 1 
1 
Replace objective row ] 
with original objective I 
function 
1 
Make entries in objective 
row labeled by basic 
variables zero by adding 
suitable multiples of 
other rows of tableau 
1 
Solve problem using 
simplex algorithm 
Flowchart of the two-phase algorithm. 

2.3 Artificial Variables 
Set up initial tableau for auxiliary problem with 
z = -Yl -Y2 
-Yk 
.... 
CALL SIMPLEX 
147 
Artificial variables have value 0 ~
"
 
~
S
o
m
e
 
artificial 
vari.ables.are 
~ 
No feasible solution 
to original problem 
This part of the 
diagram will be 
completed in 
Section 3.2 
Replace objective 
row with original 
objective function 
Make entries in 
objective row 
labeled by basic 
variables zero 
CALL SIMPLEX 
STOP 
FIGUdE 2.6 
Structure diagram of the two-phase algorithm. 
Big M Method (Optional) 
The following method of solving linear programming problems that 
require artificial variables is attributed to Charnes. Historically it precedes 
the two-phase method; it has been replaced by the latter in computer 
codes due to the greater efficiency of the two-phase method. It is still of 
interest for theoretical computations. 
Instead of introducing an auxiliary problem, the big M method ensures 
that the artificial variables are zero in the final solution by assigning to 
each Yi a penalty cost, -M, where M is a very large positive number. This 
means that we use an objective function of the form 
~ 
m 
Z = 
CjXj -- 
~_, My i. 
j=l 
i=1 
If any Yi is positive, the -M serves to decrease z drastically. 
We convert the canonical linear programming problem given by (10), 
(11), and (12) to the form involving artificial variables. We obtain 
rn 
Maximize 
z = 
cjxj - 
E 
MYi 
(27) 
j=l 
i=1 
subject to 
s 
~-~ aijxj + Yi -- bi, 
j=l 
x/>0, 
j= 1,2,...,s; 
with b i > 0, i = 1,2,...,m. 
i = 1,2,...,m 
(28) 
Yi > 0, 
i = 1,2,...,m 
(29) 

1~ 
Chapter 2 
The Simplex Method 
This problem has an initial basic feasible solution, 
[0 
0 
..- 
0 
bl 
b2 
"'" 
obtained by setting 
X 1 = 0, 
X 2 = 0,...,X 
s -- 0 
bm] 
as nonbasic variables and solving (28) for 
Yl = bl, 
Y2 = b2,'", 
Y,, = bm. 
To use the simplex method as it was developed earlier, we must write (27) 
as 
k 
m 
z - 
cjxj + M ~_, Yi = 0 
(30) 
j=l 
i=1 
and eliminate the Yi from this equation. This is done by adding -M times 
each of the constraints in (28) to (30). We obtain 
Z -- 
CjXj + M Y] 
b i - 
aijx j 
= O. 
j=l 
i=1 
j=l 
Rearranging, we can write the previous equation as 
z - 
cj + M ~_. aij xj = -M 
Y'~ b i. 
(31) 
j=l 
i=1 
i=1 
We can now solve the problem as given by (31), (28), and (29) using the 
simplex method. 
EXAMPLE 6. 
Consider the linear programming problem in Example 2. 
Since x 6 appears in only one constraint, and there with a coefficient of 
+ 1, we may use x 6 as a basic variable. Thus, we introduce artificial 
variables into only the first and third equations, obtaining the problem 
Maximize 
z = X 1 -- 2X 2 -- 3X 3 -- X 4 -- X 5 + 2X 6 -- My I - My 2 
subject to 
Xl + 2X2 + 2X3 + X4 + 
X5 
+ Yl 
= 12 
X 1 -+- 2X 2 + 
X 3 -'[-X 4 + 2x 5 + x 6 
-- 18 
3Xl + 6x2 + 2x3 + x4 + 3x5 
+ Y2 = 24 
xj>0, 
j= 1,2 .... ,6; 
Yl >0, 
Y2 >0- 
We rewrite the objective function as 
Z- 
X 1 + 2X 2 + .... 
2x 6 + My 1 + My 2 

2.3 Artificial Variables 
149 
and eliminate the basic variables x6, Yl, and Y2 from this equation. This is 
done by adding 
(-M) 
• first constraint 
2 • second constraint 
(-M) 
x third constraint. 
These operations lead to the equations 
Z -- 
m 
X 1 + 
2x 2 + 
3x 3 -]- 
X 4 + 
x 5 --2x 6 + My 1 -Jr-My 2 --0 
Mx I - 
2 Mx 2 - 
2 Mx 3 - 
Mx 4 - 
Mx 5 
- 
My 1 
=- 
12M 
2x a + 
4x 2 + 
2x 3 + 2x 4 + 
4x 5 + 2x 6 
=36 
3Mx 
1 - 
6Mx 
2 - 
2Mx 
3 - 
Mx 4 - 
3Mx 
5 
- 
My 2 =-24M. 
The result of adding these equations is 
z + (1 - 4M)x I + (6 - 8M)x 2 + (5 - 4M)x 3 + (3 - 2M)x 4 
+(5 -4M)x 
5 = 36- 
36M, 
from which we obtain the initial tableau (Tableau 2.39). 
Tableau 2.39 
X1 
X2 
X3 
X4 
X5 
X6 
YZ 
Y2 
Yl 
1 
2 
2 
1 
1 
0 
1 
0 
x 6 
1 
2 
1 
1 
2 
1 
0 
0 
Y2 
3 
(~) 
2 
1 
3 
0 
0 
1 
1 
12 
18 
24 
4M 6 
8M 5 
4M 3 
2M 5 
4M 
0 
0 
0 
36 
36MI 
Since M is a large positive number, the most negative entry in the 
objective row is 6 - 8M, so that x 2 is the entering variable. The departing 
variable, obtained as usual, is Y2. Pivoting, we obtain Tableau 2.40. Using 
the same reasoning, we obtain Tableaux 2.41 and 2.42. 
Since the objective row has no negative entries, we have found an 
optimal solution: 
x I = 6, 
x 2 = 0, 
x 3 = 3, 
x 4 = 
0, 
X 5 "- 0, 
X 6 = 9 
Ya =0, 
Y2 --0, 
which gives z = 15 as the value of the objective function. This solution 
coincides with the one obtained in Example 2. 
A 

150 
Chapter 2 The Simplex Method 
Tableau 2.40 
X1 
X2 
X3 
X4 
X5 
X6 
Yl 
Y2 
*- 
Yl 
0 
x 6 
0 
1 
X2 
-2 
2 
1 
0 
x 
0 
0 
1 
3 
1 
2 
1 
1 
0 
1 
0 
~ 
~ 
1 
1 
1 
0 
0 
1 
1 
~ 
~ 
~ 
4 
2 
4 
0 
3 
gM 2 
gM 
2 
0 
0 
1 + gM 
12 
4 
10 
4 
4M 
Tableau 2.41 
,1, 
X] 
x 3 
0 
x 6 
0 
x~ 
(~ 
2 
X2 
X3 
X4 
X5 
X6 
Yl 
Y2 
1 
0 
0 
3 
1 
0 
1 
~ 
z 
1 
1 
1 
1 
1 
0 
0 
~ 
~ 
1 
0 
1 
1 
1 
0 
0 
~ 
~ 
1 
2 
0 
9 
1 
0 
0 
y 
~+M 
~+M 
Tableau 2.42 
X1 
x 3 
0 
X 6 
0 
X 1 
1 
2.3 
EXERCISES 
X2 
X3 
X4 
X5 
X6 
Yt 
Y2 
1 
0 
0 
3 
1 
0 
1 
~ 
~ 
x 
3 
1 
1 
1 
1 
1 
0 
0 
~ 
~ 
~ 
9 
z 
1 
6 
2 
0 
0 
1 
0 
~ 
1 
4 
0 
~+M 
3 
0 
4 
0 
~ 
~+M 
15 
In Exercises 1-4 set up the initial simplex tableau (a) for solving the problem using 
the two-phase method and (b) for solving the problem using the big M method. 
1. Maximize z = x 1 + 3x 3 
subject to 
X 1 -4- 2x 2 + 7x 3 --4 
X 1 -~- 3x 2 + 
X 3 "- 5 
X 1 >_~ O, 
X 2 >__ O, 
X 3 >_~ O. 
2. Maximize z = x 1 + 2 x2 + x4 
subject to 
x 1 + 3x 2 - x 3 + X 4 __~ 5 
x I + 7x 2 + x 3 
> 4 
4x 1 + 2x 2 
+ x 4 = 3 
xj>0, 
j=1,2,3,4. 

2.3 Artificial Variables 
151 
3. Minimize z = 3xl 
subject to 
-- 2x 2 
X 1 4- 
X 2 + 2x 3 >__ 7 
2x I + 
x 2 4- 
x 3 >_ 4 
X 1 >__ O, 
X 2 >__ O, 
X 3 >__ O. 
4. Minimize 
z = x l + 2x2 4- 7x3- 
x4 
subject to 
3x I 4- 
X 2 -- 2x 3 --x 4 -- 2 
2x I 4- 4x 2 4- 7x 3 
>_ 3 
xj>_O, 
j- 
1,2,3,4. 
In Exercises 5 and 6 carry out Phase 1 for the given problems. 
-- 4x 3 
5. Maximize z = 3Xl 
subject to 
2x I 4- X 2 4- 3X 3 >_ 5 
X 1 -- X 2 4- 
X 3 >_ 1 
X 1 >__ O, 
X 2 ~ O, 
X 3 >__ O. 
6. Maximize z = x I 4-X 2 4- 2x 4 
subject to 
3x I + 
x 2 + 3x 3 + 2x 4 = 10 
x 1 - 3x 2 + 2x 3 
<_ 7 
x 1 + 2x 2 + 3x 3 + 
X 4 >__ 4 
xj>O, 
j= 1,2,3,4. 
In Exercises 7-9 we give the final tableau for Phase 1 of the two-phase method 
along with the original objective function. In these tableaux we use Y l, Y2 .... to 
denote artificial variables. (a) Form the initial tableau for Phase 2 using the given 
information and (b) apply the simplex method to the tableau in (a) to find an 
optimal solution to the given problem. 
7. Maximize z = 2 x i + x2 + x 3 
X 1 
X2 
X 3 
X4 
X5 
1 
3 
3 
0 
~ 
x 2 
- 1 
1 
10 
7 
1 
1 
1 
11-0 
10 
X 4 
~- 
0 
0 
0 
0 
0 
0 
0 

1 ~ 
Chapter 2 The Simplex Method 
8. Maximize z = x 2 + 3X 3 4-x 4 
X1 
X2 
X 3 
X4 
X 5 
X6 
X 7 
3 
3 
X 7 
~ 
0 
2 
1 
0 
Z 
1 
0 
1 
0 
2 
X 2 
0 
1 
1 
3 
0 
1 
x 5 
1 
0 
3 
0 
1 
~ 
0 
4 
0 
0 
0 
0 
0 
0 
0 
0 
9. Maximize z = 2x I + x 2 4- X 4 
X1 
X2 
X3 
X4 
X5 
X6 
YI 
Y2 
x 2 
3- 
1 
1 
0 
0 
3 
0 
0 
1 
8 
4 
5 
0 
1 
1 
1 
7 
0 
0 
2 
X5 
8 
u 
Yl 
1 
0 
0 
--2 
0 
5 
1 
0 
3 
4 
1 
1 
Y2 
0 
0 
~ 
1 
0 
~ 
0 
1 
0 
1 
3 
3 
1 
0 
~ 
1 
0 
~ 
0 
0 
2 
10. 
11. 
12. 
13. 
14. 
15. 
16. 
17. 
18. 
19. 
20. 
In Exercises 10-23 solve the indicated linear programming problem using 
either the two-phase method or the big M method. 
Example 2, Section 1.1. 
Example 5, Section 1.1. 
Example 6, Section 1.1. 
Example 7(c), Section 1.1. 
Example 7(d), Section 1.1. 
Exercise 1, Section 1.1. 
Exercise 11, Section 1.1. 
Exercise 12, Section 1.4. 
Exercise 3, Section 1.5. 
Exercise 8, Section 1.1. 
Maximize z = 2x 1 + 5x 2 -X 3 
subject to 
-4x I + 2x 2 + 
6x 3 = 4 
6x 1+9x 2+ 12x 3=3 
x 1 >0, 
x 2>0, 
x 3>0. 

2.3 Artificial Variables 
153 
21. Maximize z = 3x I -x 2 + 2x 3 + 4x 4 
subject to 
X 2 + 7X 3 + 2x 4 > 3 
x 1 + 2x 2 + 
X 3 
= 9 
2x 1 + 3x 2 + 
X 3 -- 4x 4 < 7 
Xj>_O, 
j= 1,2,3,4. 
22. Maximize z = 
2x I -x 
2 + x 3 -x 
4 + x 5 
subject to 
X 1 + X 2 -- X 3 + 
X 4 + x 5 = 3 
2x 1 - x 2 + X 3 -- 2x 4 
= 2 
3x 1 
--x 3 + 3x 4 
>__ 2 
xj>O, 
j-- 1,2,3,4. 
23. Maximize z = 3xl + x 2 --X 3 + 2x 4 --x 5 + 2x 6 
subject to 
2x I + x 2 -- 
x 3 
+ x 6 = 3 
3x 1 
+ 2x 3 + X 4 + 2x 5 
= 4 
x 2 -- 3x 3 
+ 
x 5 
= 2 
xj>_O, 
j= 
1,2 ..... 6. 
24. Show that the vector x in R s is a feasible solution to the problem in canonical 
form given by (10), (11), and (12) if and only if the vector 
in R s§ 
is a feasible solution to the auxiliary problem given by (13), (14), and 
(15). 
25. Explain why the coefficients of M in the objective row of Tableau 2.40 are the 
same as the nonzero entries in the objective row of Tableau 2.32. 
Further Reading 
Beale, E. M. L. "Cycling in the Dual Simplex Algorithm." Naval Res. Logistics Q. 2 (1955), 
269-276. 
Kotiah, Thoddi C. T., and Steinberg, D. I. "Occurrences in Cycling and Other Phenomena 
Arising in a Class of Linear Programming Models." Commun. ACM 20 (1977), 107-112. 
Kotiah, Thoddi C. T., and Steinberg, D. I. "On the Possibility of Cycling with the Simplex 
Method." Operations Res. 26 (1978), 374-375. 
Marshall, K. T., and Suurballe, J. W. "A Note on Cycling in the Simplex Method." Naval Res. 
Logistics Q. 16 (1969), 121-137. 

Further Topics 
in Linear 
Programming 
T 
HIS CHAPTER COVERS several topics in linear programming that 
have important computational consequences. The idea of duality, 
which is introduced in the second section, is particularly useful in 
modeling, because it provides economic interpretations of the solution to a 
linear programming problem. We present a brief discussion of sensitivity 
analysis, another tool that is useful in interpreting the solution to a linear 
programming problem. We discuss several variants of the simplex algo- 
rithm, including the one that is used in most computer codes. Finally, we 
deal with computational considerations from the viewpoint of the user of a 
packaged linear programming system. 
3.1 DUALITY 
In this section we shall show how to associate a minimization problem 
with each linear programming problem in standard form. There are some 
very interesting interpretations of the associated problem that we will 
155 

1 ~ 
Chapter 3 Further Topics in Linear Programming 
discuss. Generally, a problem in standard form can be thought of as a 
manufacturing problem, one in which scarce resources are allocated in a 
way that maximizes profit. The associated minimization problem is one 
that seeks to minimize cost. 
Consider the pair of linear programming problems 
Maximize 
z = cTx 
subject to 
(1) 
Ax<b 
x>0 
and 
Minimize 
z' --,bTw 
subject to 
(2) 
ATw>_c 
w>O 
where A is an m x n matrix, c and x are n • 1 column vectors, and b and 
w are m • 1 column vectors. 
These problems are called dual problems. The problem given by (1) is 
called the primal problem; the problem given by (2) is called the dual 
problem. 
EXAMPLE 1. 
If the primal problem is 
then the dual problem is 
Minimize 
Ix1] 
Maximize 
z = [ 2 
3 ] X2 
subject to 
subject to 
3 
-1 
2 
2 
[3 2]ix11 [215 
-1 
2 
x2 
__ 
4 
1 
1 
X 1 ~ O, 
X 2 ~_~ O, 
z' -- [2 
5 
1] IWll 
W2 
W 3 
IWll [] 
4} w2 
2 
1 
>- 
3 
W 3 
W 1 >_~ O, 
W 2 ~ O, 
W 3 >_~ O. 
A 

3.1 Duality 
157 
Observe that, in forming the dual problem, the coefficients of the ith 
constraint of the primal problem became the coefficients of the variable w i 
in the constraints of the dual problem. Conversely, the coefficients of xj 
became the coefficients of the jth constraint in the dual problem. Also, the 
coefficients of the objective function of the primal problem became the 
right-hand sides of the constraints of the dual problem, and conversely. 
THEOREM 3.1. 
Given a primal problem as in (1), the dual of its dual 
problem is again the primal problem. 
Proof 
The dual problem as given by (2) is 
We can rewrite (3) as 
Minimize 
z' = bTw 
subject to 
ATw~c 
w~_O. 
Maximize 
z' 
-- - bTw 
subject to 
--ATw < --c 
w>O. 
Now the dual problem to (4) is 
Minimize 
subject to 
(--AT) T 
This problem can be rewritten as 
Maximize 
subject to 
Z -- --cTx 
x> 
-b 
x>0. 
Z -- cTx 
Ax<b 
x>0, 
which is the primal problem. 
THEOREM 3.2. 
given by 
(3) 
(4) 
A 
The linear programming problem in canonical form 
Maximize 
z = cTx 
subject to 
Ax=b 
x>0 

1 ~ 
Chapter 3 Further Topics in Linear Programming 
has for its dual the linear programming problem 
Minimize 
z' = bTw 
subject to 
ATw>__C 
W unrestricted. 
Proof. 
The primal problem can be written as 
Maximize 
z "- cTx 
subject to 
Ax<b 
-Ax< 
-b 
x>0 
by using the method of converting equalities that we described in Section 
1.1. In matrix form the primal problem is 
Maximize 
z --" cTx 
subject to 
A 
~] 
x>O. 
The dual problem is then 
Minimize 
z' = [ b T 
subject to 
--bTJ[U 1 
-A l[ul >cv - 
u>_O, 
v>O. 
When we multiply out the matrices, we have 
Minimize 
z' = bTu -- bTv -- bT(u -- V) 
subject to 
AT u -- AT v -- AT ( u -- V) >_ C 
u>_O, 
v>_O. 

3.1 Duality 
159 
If we let w = u - v, then the dual problem has the form 
Minimize 
z' = bWw 
subject to 
(5) 
ATw>_c 
w unrestricted 
because any vector may be written as the difference of two nonnegative 
vectors. 
A 
THEOREM 3.3. 
The linear programming problem 
Maximize 
z = cTx 
/ 
subject to 
~ 
(6) 
Ax<b 
/ 
x unrestricted, ) 
has as its dual problem, 
Minimize 
z' = brw 
subject to 
ATw _--- C 
w>0. 
Proof. 
We can rewrite the given problem as 
Minimize 
z = -crx 
subject to 
-Ax > -b 
x unrestricted. 
Comparing this statement of the problem with (5), we see that it is the 
dual of 
Maximize 
z' -- --bXw 
subject to 
--ATw "- --C 
w>0. 
This last problem statement can be written as 
Minimize 
z' = brw 
subject to 
(7) 
ATw----C 
w>0. 
We have shown that the dual of problem (7) is problem (6). Therefore, the 
dual of the dual of problem (7) is the dual of problem (6). Applying 

160 
Chapter 3 Further Topics in Linear Programming 
TABLE 3.1 
Primal problem 
Dual problem 
Maximization 
Coefficients of objective function 
Coefficients of ith constraint 
ith constraint is an inequality < 
ith constraint is an equality 
jth variable is unrestricted 
jth variable is > 0 
Number of variables 
Minimization 
Right-hand sides of constraints 
Coefficients of ith variable, one in 
each constraint 
i th variable is > 0 
i th variable is unrestricted 
jth constraint is an equality 
jth constraint is an inequality > 
Number of constraints 
Theorem 3.1 it follows that problem (7) is the dual of problem (6), as we 
were to show. 
A 
We summarize the relationships between the primal and dual problems 
in Table 3.1. Remember that Theorem 3.1 allows us to also read the table 
from right to left. That is, if the headings "Primal problem" and "Dual 
problem" are interchanged, the resulting table remains valid. For example, 
Table 3.1 shows that if the jth constraint in a minimization problem is a 
>__ inequality, then the jth variable of the dual problem is > 0. Note that 
the table shows how to find the dual of a maximization problem with < 
and 
= 
constraints and of a minimization problem with 
> 
and 
= 
constraints. If we have a maximization problem with a > constraint, this 
constraint must be converted to < (by multiplying by - 1) before the dual 
problem can be constructed. The same procedure must be used on a 
minimization problem with a < constraint. 
EXAMPLE 2. 
If the primal problem is 
Maximize 
z--3x 
I + 2x 2 + x 3 
subject to 
X 1 + 2X 2 --X 3 _< 4 
2x I -- 
x 2 -~- X 3 -- 8 
x 1 -- 
x 2 
__< 6 
x 1 >_ O, 
x 2 > O, 
x 3 unrestricted, 
then the dual problem is 
Minimize 
z' = 4w 1 ~- 8w2 --I- 6w3 
subject to 
w I + 2w 2 + w 3 > 3 
2w I - 
w 2 -w 3 > 2 
--W 1 -[- 
W 2 
=1 
Wl >-~ 0, 
W3 >-~ 0, 
WE unrestricted. 
A 

3.1 Duality 
161 
EXAMPLE 3. 
If the primal problem is 
Minimize 
z = 2Xl 
subject to 
X 1 -'~ 2x 2 'i'-x 3 
__< 7 
X 1 + 4x 2 
-- 
x 4 -- 5 
x 2 + x 3 + 5x 4 > 3 
X 1 ~__ 0, 
X 2 ~__ 0, 
X 3 ~__ 0, 
X 4 ~__ 0, 
then the dual problem is 
Maximize 
z' 
subject to 
--W 1 -Jr- 
W 2 
-- 2W 1 + 
4W 2 + 
--W 1 
-[- 
-- 3x 2 + x 4 
w I ~__ 0, 
= 
--7w I + 5W 2 -}- 3w 3 
_<2 
W 3 _~ -3 
W3_~0 
--W 2 q- 5W 3 __~ 1 
W3 ~-~ 0, 
W2 unrestricted. 
To find this dual problem, we write the first constraint of the primal 
problem as 
--X 1 -- 2x 2 -x 
3 >__ -7. 
An alternate method would be to change the primal problem to a maxi- 
mization problem and multiply the third constraint by - 1. 
A 
Economic Interpretation of the Dual Problem 
The dual problem can have various economic interpretations depending 
upon the point of view one takes. We shall describe two possible interpre- 
tations here. As we discover more relationships between the primal and 
dual problems, we will be able to present some additional interpretations. 
In Example 1 of Section 1.1 we have a model of a sawmill. It is 
Maximize 
z = 
120x 1 + 
100x 2 
subject to 
2x~ + 2x 2 < 8 
5x 1 + 3x 2 < 15 
X 1 ~_~ 0, 
X 2 ~ 0. 
This is a simple example of a typical manufacturing problem that is in 
standard form (1). The first constraint in the sawmill problem deals with 

1 ~ 
Chapter 3 Further Topics in Linear Programming 
the number of hours for which the saw is available. The second constraint 
deals with the number of hours for which the plane is available. In general, 
in the ith constraint of (1), 
~ aijxj ~ b i, 
j=l 
we may think of b i as the total supply of the i th resource, or raw material, 
or input. For the first constraint of the sawmill problem it was sawing time. 
The coefficient a ij in the general problem represents the amount of the 
ith input required per unit of the jth product, or output. For example, 
a21 = 5 in the sawmill example represents the 5 hr of planing time 
required for each 1000 board feet of finish-grade lumber. The variable xj 
is the unknown amount of the jth output that is to be produced. The 
coefficient cj in the objective function represents the profit, or value, 
3 
derived from one unit of the jth output. The optimal solution x I = 3, 
5 
x 2 = ~ maximizes the total value of all outputs 
n 
Z '- E CjXj. 
j=l 
The dual problem to the sawmill example is 
Minimize 
z'= 
8w 1 + 15w 2 
subject to 
2w I + 5w 2 > 120 
2w 1 + 3w 2 >_ 100 
W l >_~ 0, 
W z >_~ 0. 
The coefficients of the first constraint are the amounts of each input that 
are needed to make one unit (1000 board feet) of the first output. That is, 
to make 1000 board feet of finish-grade lumber we need 2 hr of sawing and 
5 hr of planing. The right-hand side of the first constraint is the profit, or 
value of one unit of the first output. Likewise, the second constraint of the 
dual problem of the sawmill example says that to make 1000 board feet of 
construction-grade lumber we need 2 hr of sawing and 3 hr of planing, and 
the value of this amount of lumber is $100. Solving the dual problem we 
discover (verify)that w I = 35 and w 2 = 10. 
The dual problem of the general linear programming problem in 
standard form (1) is 
Minimize 
z' = bTw 
subject to 
ATw>_c 
w>__O. 

3.1 Duality 
163 
The jth constraint of this problem is 
m 
a ijw i >_ cj. 
i=1 
As above, the coefficient a/j represents the amount of input i per unit of 
output j, and the fight-hand side is the value per unit of output j. This 
means that the units of the dual variable w i are "value per unit of input i." 
The dual variables act as prices, or costs, or values of one unit of each of 
the inputs. They are called by several names, including accounting prices, 
fictitious prices, shadow prices, and imputed values. 
At an optimal solution to the primal problem, profit, which is equal to 
cTx, is also equal to bTw, as we will show in Section 3.2. Thus, increasing 
the ith input b i by one unit increases bTw, and hence profit, by w i units. 
Hence, at an optimal solution to the dual problem, w i represents the 
contribution to profit of one unit of the ith input. The values of the dual 
variables are not directly related to the actual costs of the inputs. Just 
because the optimal solution to the dual of the sawmill problem is 
w 1 = 35, w 2 = 10 does not mean that the cost of the saw is $35 per hour 
and the cost of the plane is $10 per hour. The actual costs are hidden 
in whatever computations were done to figure out that the profits were 
$120 per 1000 board feet of finish-grade and $100 per board feet of 
construction-grade lumber. In the sense that the dual variables do not 
represent actual costs, they are fictitious prices. 
The left-hand side of the jth constraint of the dual problem gives the 
total value of the inputs used in making one unit of the jth output. This 
constraint says that this value must be at least as much as the profit of one 
unit of the jth output. But at an optimal solution the value of the left-hand 
side represents the total contribution to profit of one unit of the jth 
output, and it is reasonable to expect to operate when this contribution to 
profit is at least as much as the actual profit. If this were not the case, the 
manufacturer would be well advised to use the available inputs in a better 
way. 
We see then that the dual problem seeks shadow prices for each of the 
inputs that minimize their total price, subject to the restriction that these 
prices, or values, yield a corresponding value for each unit of output that is 
at least the profit for a unit of output. 
Another description of the dual variables comes from the fact that, as 
we will show in Section 3.2, at an optimal solution to the dual problem, 
Profit = bTw. 
To increase this profit the manufacturer must increase the availability of at 
least one of the resources. If b i is increased by one unit, the profit will 
increase by w~. Thus, w i represents the marginal value of the ith input. In 

1 ~ 
Chapter 3 Further Topics in Linear Programming 
the same way, w i is the loss incurred if one unit of the ith resource is not 
used. Thus it can be considered as a replacement value of the ith resource 
for insurance purposes. In fact, an insurance company would want to use 
the dual problem in case of a claim for lost resources; it wants to pay out 
as little as possible to settle the claim. 
It is also interesting to look at an interpretation of the dual of the diet 
problem that was given in Example 2 of Section 1.1. The model of the diet 
problem is 
Minimize 
z = 20Xl + 25x2 
subject to 
2X 1 + 3X 2 >__ 18 
x 1 + 3X 2 >_ 12 
4x I + 3x 2 >__ 24 
X 1 ~__ O, 
X 2 ~" O. 
The dual problem is 
Maximize 
z' 
subject to 
2w 1 + 
= 18w I + 12w 2 + 24w 3 
W 2 + 4W 3 _< 20 
3w I + 3w 2 + 3w 3 __< 25 
W 1 ~_~ O, 
W 2 ~ O, 
W 3 ~" O. 
To discuss the dual problem we introduce some notation. Let N 1, N 2, and 
N3 denote the nutrients fat, carbohydrates, and protein, respectively. It is 
also convenient to denote foods A and B by F 1 and F 2, respectively. Now 
introduce a manufacturer that makes artificial foods P1, P2, and P3 with 
the property that for each i = 1, 2, or 3, one unit of Pi provides one unit 
of nutrient N~. Assume that the manufacturer sets w i as the price of Pi 
(i = 1, 2, or 3). Recall that in the original statement of the problem, a~j is 
the number of units of nutrient N~ in 1 oz of food Fj. For example, a12 = 3 
is the number of units of fat (N1) in 1 oz of food F 2, and a31 -- 4 is the 
number of units of protein (N3) in 1 oz of food F 1. The artificial food 
manufacturer will set its prices so that 
2w I + w 2 --b 4w 3 _< 20 
and 
3w 1 + 3w z + 3w 3 < 25. 
That is, it will set the prices on the foods P~ so that when these foods are 
taken in the proportions necessary to give the same nutrition as foods F 1 
and F 2, the cost of the substitute for F 1 is no greater than the cost of F 1 
itself and the cost of the substitute for F 2 is no greater than the cost of F 2 

3.1 Duality 
165 
itself. Thus, the nutritionist will always find it at least as economical to buy 
the three artificial foods. Since we require 18 units of fat (from P1), 12 
units of carbohydrate (from P2), and 24 units of protein (from P3), the 
manufacturer's revenue is 
z'= 
18w I + 12w 2 + 24w 3. 
It seeks to maximize this revenue. 
Thus, the fictitious prices w 1, w 2, and w 3 of the nutrients are those 
prices that the artificial food manufacturer should charge to maximize the 
revenue, yet still be able to compete with the producer of foods F 1 and F 2. 
Therefore, these fictitious prices represent competitive prices. 
3.1 
EXERCISES 
In Exercises 1-6 find the dual of the given linear programming problem. 
1. Minimize 
subject to 
2. Minimize 
subject to 
3. Maximize 
subject to 
4. Maximize 
subject to 
7. = 3x 1 + 4x 2 
x 1 -b 4x 2 >__ 8 
2x I + 3x 2 >_ 12 
2xa + x 2 >_ 6 
X 1 ~__ O, 
X 2 >_~ O. 
z = 6x 1 + 6x 2 + 8x 3 -+ 9x 4 
x 1+2x 2+ 
x 3+ 
x 4>3 
2x I + 
x 2 + 4x 3 + 9x 4 >__ 8 
X 1 >__ O, 
X 2 >__ O, 
X 3 ~_~ O, 
X 4 ~__ O. 
z = 3x 1 + 2x 2 + 5x 3 d- 7x 4 
3x I + 2x 2 + 
x 3 
__~ 8 
5x 1 + 
x 2 + 2x 3 + 4x 4 = 7 
4x 1 
-t- 
x 3 -- 2x 4 < 12 
X 1 >__ O, 
X 2 >__ O, 
X 3 >__ O, 
X 4 >_~ O. 
z = 2x 1 + x 2 + 3x 3 + 4x 4 
4x 1 + 2x 2 + 5x 3 -~- 5x 4 __~ 10 
4x 1 + 2x 2 + 5x 3 -~- 5x 4 ~__ 5 
3x I + 5x 2 + 4x 3 + 
x 4 >__ 8 
3x 1+ 
5x 2 + 4x 3 + 
x 4 < 15 
X 1 -'[- X 2 -b 
X 3 -[- 
X 4 = 20 
X 1 >_~ O, 
X 2 >_~ O, 
X 3 >_~ O, 
X 4 >__ O. 

166 
Chapter 3 Further Topics in Linear Programming 
5. Maximize 
subject to 
z = 3x I + x 2 + 4x 3 
3x 1+3x 2+ 
x 3< 18 
2x 1 + 2x 2 + 4x 3 = 12 
x I > 0, 
x 3 > 0. 
6. Minimize 
subject to 
z = 5x I + 2x 2 + 6x 3 
4x 1+2x 2+ 
x 3> 12 
3x I + 2x 2 + 3x 3 < 6 
x I > 0, 
x 2 > 0. 
7. The text suggested an alternate method for solving Example 3. 
(a) Use this method to construct the dual problem. 
(b) Verify that the resulting problem is the same as the one given in the text. 
(Hint: Use the fact that w 2 is unrestricted.) 
In Exercises 8-11 formulate the dual problem for the given linear programming 
problem. 
8. Exercise 2, Section 1.1 
9. Exercise 4, Section 1.1. Give an economic interpretation of the dual problem. 
10. Exercise 9, Section 1.1. Give an economic interpretation of the dual problem. 
11. Using the definition of the dual of a problem in standard form, find the dual of 
the linear programming problem 
Maximize 
z = CTx + d Tx ' 
subject to 
Ax + Bx' < b 
x > 0, 
x' unrestricted 
(Hint: Write x'= u- v, u > 0, v > 0, and express the given problem as a 
standard linear programming problem in matrix notation.) 
3.2 THE DUALITY THEOREM 
In his work on game theory, John von Neumann, one of the most 
versatile mathematicians of all time, proved a duality theorem for games. 
In October of 1947 George Dantzig, one of the pioneers of linear pro- 
gramming and the developer of the simplex algorithm, went to see von 
Neumann in Princeton. After hearing the basic ideas in linear program- 
ming, von Neumann indicated to Dantzig his Duality Theorem for games 
and also conjectured and proved an equivalent result for linear program- 
ming. However, this proof was not published, and the first careful proof of 

3.2 The Duality Theorem 
167 
the Duality Theorem, now recognized as the fundamental theorem of 
linear programming, was published in 1950 by A. W. Tucker and his 
students David Gale and Harold W. Kuhn. The Duality Theorem estab- 
lishes conditions for a feasible solution to a linear programming problem 
to be an optimal solution. 
To present and prove the Duality Theorem we first need to develop 
some tools for expressing the solution to a linear programming problem in 
terms of the entries in any tableau that has been constructed while using 
the simplex method to solve the problem. 
Suppose that we are given a general linear programming problem and 
suppose that we have introduced slack and artificial variables to convert 
the problem to canonical form. That is, we take our problem as 
Maximize 
z = cTx 
subject to 
Ax=b 
x>_0, 
(1) 
where A is an m x s matrix that contains an m x m identity submatrix; 
b > 0 is an m x 1 matrix; and c is an s x 1 matrix. Remember that if xj is 
a slack variable, then cj = 0. Also, we use the two-phase method for the 
artificial variables so that, if xj is an artificial variable, then cj = 0. 
We now examine a tableau constructed by the simplex algorithm during 
the solution of our problem. This tableau represents a basic feasible 
solution. Let il be the subscript of the first basic variable in this solution; 
let i 2 be the subscript of the second basic variable. Continuing in this 
manner, we end with i n being the subscript of the mth basic variable. Let 
N denote the set of indices of the nonbasic variables. We also let Aj 
denote the jth column of A. Using this notation, we may write the second 
equality of (1) as 
m 
E xiAi r + 
E xyAy = b. 
(2) 
r-1 
j~N 
Recall that the nonbasic variables are set equal to zero, so that (2) may be 
simplified to 
XilAil + xi2Ai2 -~- ... + XimAim -- b. 
(3) 
We make an m x m matrix out of the m columns Ail , Ai2,... , Aim of A 
corresponding to basic variables and denote this matrix by B. We introduce 
notation for the basic feasible solution expressed as a vector and the 

1 ~8 
Chapter 3 Further Topics in Linear Programming 
corresponding cost vector by letting 
IXim 
Ici 
XB-- [ Xi2 
and 
CB= 
/C/~ 
. 
LXim 
LCi~ 
Then (3) may be written as 
Ux B -- b. 
(4) 
Using Theorem 0.9, the columns of B are linearly independent, so B must 
be a nonsingular matrix. We may write (4) as 
X B -- B-lb. 
(5) 
That is, the m-tuple of basic variables has the value B-lb in any tableau. 
We also note that the m columns of B considered as m-tuples form a 
basis for R m. Thus, the jth column of our initial tableau, Ay, j - 1, 2,..., s, 
can be written as a linear combination of the columns Ail, Ai2,... ,Aim, 
where the indices i l, i2,... , i m are the indices of the basic variables of our 
current tableau in the order of the labels on the left-hand side. 
We have 
A/= tljAi~ + txjAi2 + ... + tmjAim, 
j = 1,2,..., s. 
(6) 
It can be shown that the vector of coefficients in (6), 
tj= 
tlj 
t2j 
tmj 
which is the coordinate vector of A/with respect to the basis 
{Ail,Ai2,... cAi m} 
of R m, is also the jth column of our current tableau. In the same way as in 
(4), we may write (6) as 
Btj = A/ 
or 
t/= B-lAy. 
(7) 

3.2 The Duality Theorem 
1 ~ 
Equation (7) says that any column of a tableau can be found from 
the corresponding column of the initial tableau by multiplying on the left 
byB -1. 
Using the notation that has been developed, we define 
zj = c~ty 
(8) 
or, using (7), 
z: = c~B-1Aj. 
(9) 
From this definition we see that for the 
0 
0 
Zir ~- [ Cil 
Ci2 "'' Cim ] 
1 
~ rth entry 
0 
rth basic variable Xir we have 
0 
-- Cir. 
The objective function for this problem takes on a particularly simple 
form when we use the definition of Zy. Recall that 
z = cTx = ~ CjXj. 
(10) 
j=l 
We separate the sum on the right-hand side of (10) into two parts: one 
part contains the basic variables and the other part contains the nonbasic 
variables. We write 
m 
Z --- E CiXir -~- E 
CjXj. 
(11) 
r=l 
j~N 
In order to apply the 
optimality criterion 
to our tableau, we 
must modify (11) so that the coefficients of the basic variables are zero. To 
make this modification we add (-cil)x 
first row of the 
current 
tableau +(-ci2) x second row + ... "lt-(--Cim ) X mth row to (11). In sym- 
bols, the rth row of the current tableau can be expressed as 
~trjX j =- XBr , 
j=l 
where X Br is the rth component of the vector x B. 
Adding the appropriate multiples of the rows of the tableau to (11), we 
obtain 
m 
m 
Z -- E CirXBr-~ E 
CjXj- 
E Ci r E 
trjXj" 
r=l 
j~N 
r=l 
j~N 

1 ~0 
Chapter 3 Further Topics in Linear Programming 
Simplifying, we have 
or by (8) 
Since Zir 
-
-
 
Z--C~XB = 
E 
(Cy--c~ty)xy 
j~N 
(zj - cj)xj 
+ z = c 
xB. 
j~N 
c i, = 
O, we obtain 
(zj - cj)xj 
+ 
z 
= 
j=l 
This equation shows that the entries in the objective row of a tableau are 
simply 
zj - cj = c~tj - cj. 
(12) 
We may restate the optimality criterion for the simplex method: a tableau 
represents an optimal solution if and only if, for all j, zj - cj > 0. 
EXAMPLE 1. 
Consider the linear programming problem in canonical 
form from Section 2.1, Example 2. 
Maximize 
z=8x 
1+9x 2+5x 3 
subject to 
x I + 
x 2 + 2x 3 + x 4 
= 2 
2x I + 3x 2 + 4x 3 
+ x 5 
= 3 
6x I + 6x 2 + 2x 3 
+x 6 = 8 
xj>0, 
j= 
1,2,...,6, 
where 
8 
- 
9 
A= 
2 
3 
4 
0 
1 
0 , 
b= 
3 , 
and 
c= 
. 
6 
6 
2 
0 
0 
1 
8 
0 
0 
0 
After the first iteration of the simplex method we obtained Tableau 3.2 
as a tableau representing the basic feasible solution in which i~- 4, 
i2 = 2, and i 3 -- 6, and thus 
IX41 I11 
XB= 
X2 
= 
1 . 
X6 
2 

3.2 The Duality Theorem 
171 
Tableau 3.1 
X 4 
X2 
X 6 
X 1 
X2 
X3 
X4 
X5 
X6 
! 
0 
3 
2 
1 
2 
o 
-2 
0 
_ 
1 
2 
1 
~ 
0 
1 
3 
4 
0 
1 
0 
1 
3 
3 
6 
0 
2 
1 
2 
7 
0 
3 
0 
9 
We also have c~ = 
[c 4 
c 2 
can be written as 
C6] -" [0 
9 
Ax=b 
0]. The problem in matrix form 
AIX 1 + 
A2x 2 + 
"" + 
A6x 6 = b 
or 
[1] I1] 
[2] 
2 xl + 
3 x2+"" 
+ 
x 6= 
3 9 
6 
6 
8 
Since the tableau represents x4, x2, and x 6 as basic variables in this order, 
we may rearrange the previous equation as 
[1] [1] [0] [1] 
[0] 
0 x4+ 
3 x2+ 
0 x6 + 
2 Xl + 
4 x 3 + 
1 x5 = 
3 , 
0 
6 
1 
6 
2 
0 
8 
the entries being the coefficients of the basic variables in the objective 
function. Thus, 
[110] 
B= 
0 
3 
0 
0 
6 
1 
and 
Then 
B-1 I~1 31 o- 1 
3 
0 9 
-2 
1 
X B = B-lb = I 1~ 
] 
1 
3 
2 
1 
1 
0 
~ 
0 
3 
= 
1 
0 
-2 
1 
8 
2 
which agrees with the right-hand side of the tableau. Furthermore, t 1 = 
B-1A1 or 
I I I ~ ] 
3 
1 
3 
1 
2 
1 
5 
= 
0 
~ 
0 
2 9 
2 
0 
--2 
1 
6 

172 
Chapter 3 Further Topics in Linear Programming 
The rest of the columns can be checked in the same manner. Finally, the 
entries in the objective row are zj - cj, where zj = c~tj. We have 
5 
2 
--6 
Z 1 -- [0 
9 
O] ~ 
, 
2 
and the first entry in the objective row is 
z1-el 
=6-8= 
-2. 
The other entries can be checked in the same manner. 
A 
Relations between the Solutions to the Primal and Dual Problems 
We saw in Chapter 2 that there are three possible outcomes when 
attempting to solve a linear programming problem. 
1. No feasible solutions exist. 
2. There is a finite optimal solution. 
3. Feasible solutions exist, but the objective function is unbounded. 
Since the dual problem is a linear programming problem, attempting to 
solve it also leads to these three possible outcomes. Consequently in 
considering relationships between solutions to the primal and dual prob- 
lems there are nine alternatives for the pair of solutions. We now present 
theorems that show which of these alternatives actually can occur. 
THEOREM 3.4 (WEAK DUALITY THEOREM). 
the primal problem 
Maximize 
z - cTx 
subject to 
Ax<b 
x>_O, 
and if w o is a feasible solution to the dual problem 
Minimize 
z' - bTw 
subject to 
ATw>_c 
w>_O, 
then 
If x 0 is a feasible solution to 
(13) 
(14) 
That is, the value of the objective function of the dual problem is always 
greater than or equal to the value of the objective function of the primal 
problem. 
eTx0 _< bTw0 
(15) 

3.2 The Duality Theorem 
173 
Proof. 
Since x 0 is a feasible solution to (13), we have 
Ax 0 _< b. 
(16) 
It follows from (16) that 
w~Ax o < woTb = bTWo 
(17) 
since w o >_ 0. The equality in (17) comes from the fact that Wo T b is a 1 • 1 
matrix and consequently is equal to its transpose. 
Since w o is a feasible solution to (14), we have 
ATwo > c 
or, taking transposes, 
w~A >__ C T. 
Again we can multiply by x 0, which is nonnegative, without changing the 
inequality. We get 
woTAxo > cTx0 . 
(18) 
Combining inequalities (17) and (18) gives the desired result. 
A 
An important application of Theorem 3.4 arises in the case in which the 
primal problem has feasible solutions but the objective function is un- 
bounded. This means that, given any number N, we can find a feasible 
solution to the primal problem for which the value of the objective 
function is greater than N. Consequently the objective function of the dual 
problem (using Theorem 3.4) is greater than N for any feasible solution w 0 
to the dual problem. This means that there are no feasible solutions to the 
dual problem. For if w were such a solution, the value of the dual objective 
function would be bTw. But N can be chosen greater than this value, and 
we have 
bXw < N < bXw, 
the second inequality coming from the argument above. This impossible 
pair of inequalities means that w cannot exist. The discussion above has 
proved part (a) of the following theorem. 
THEOREM 3.5. (a) If the primal problem has feasible solutions but the 
objective function is unbounded, then the dual problem has no feasible 
solutions. 
(b) If the dual problem has feasible solutions but the objective function is 
unbounded, then the primal problem has no feasible solutions. 
Proof. 
(b) Combining Theorem 3.1 and part (a)we obtain the desired 
result. 
A 
We now give a condition that a feasible solution to the primal problem 
be an optimal solution. 

1 ~4 
Chapter 3 Further Topics in Linear Programming 
THEOREM 3.6. 
/f x o and w o are feasible solutions to the primal and dual 
problems (13) and (14), respectively, and if CTXo = bTwo, then both x o and 
w o are optimal solutions to their respective problems. 
Proof 
from (15), 
Suppose x I is any feasible solution to the primal problem. Then, 
cTx1 _~< bTw0 -- cTx0" 
Hence, x 0 is an optimal solution. Similarly, if w I is any feasible solution to 
the dual problem, then, from (15), 
bTwo = cTx0 __< bTWl, 
and we see that w o is an optimal solution to the dual problem. 
A 
THEOREM 3.7 (DUALITY THEOREM). 
(a) If either the primal or dual problem has a feasible solution with a finite 
optimal objective value, then the other problem has a feasible solution with the 
same objective value. 
(b) If the primal (13) and dual (14) problems have feasible solutions, then 
(i) the primal problem has an optimal solution--say, x0; 
(ii) the dual problem has an optimal solutionmsay, w0; and 
(iii) CTXo = bTWo . 
Proof. 
(a) We convert the primal problem given in (13) to canonical 
form by introducing the vector of slack variables x' and writing, 
Maximize 
Z=[C~a0]T[ x]x' 
subject to 
[At~I] x] 
x' 
= b 
x>0 
x' >_0. 
Let i be an optimal solution to this problem. Then there is a corre- 
sponding invertible matrix B that gives the values of the basic variables of 
i as i B = B-lb [see Equation (5)]. The objective function value at the 
optimal solution is then 
z = c~ 
n = c'~B-lb. 
We let 
W 
(B-l) T 
= 
c B 
and thus 
W T -- cTB-1, 

3.2 The Duality Theorem 
175 
and by substitution we obtain z = wTb. Since z is a number, 
z = wTb = (wTb) T = bTw. 
Thus, the objective function value for the dual problem, namely, bTw, 
agrees with the objective function value for the primal problem. We now 
show that w is a feasible solution to the dual problem given by (14). 
From (7) we have 
and from (8)we have 
tj = c~B- 1[ AII ]y 
zj = c~tj = c~B-I[A[I]j. 
The optimality criterion of the simplex algorithm implies that 
zj-cy>_O, 
which can be written in vector form as 
i 
-1 
I 
z- [cTI0] =c~B 
[All]- [cT,0] >0. 
Multiplying in the previous equation and separating the partitioned matrix, 
we hax, e 
c~B- 1A > c T 
and 
c~B -~ >0. 
Using the definition of w, these inequalities can be written as 
wXA >_ c x 
w>O 
or 
ATw>__C 
w>_0. 
Hence, w is a feasible solution to the dual problem given by (14) and yields 
the same value for the objective function of the dual problem as does the 
optimal solution x for the objective function of the primal problem. 
(b) Let x be a feasible solution to (13) and w be a feasible solution to 
(14). By Theorem 3.4 
cTx < bTw. 
Since the objective function of the primal problem is bounded by bTw and 
the set of feasible solutions is not empty, there is a finite optimal solution 

176 
Chapter 3 Further Topics in Linear Programming 
x 0. By part (a) there is a feasible solution w 0 to (14) so 
bTWo = eTXo , 
Theorem 3.6 then implies that both x o and w o are optimal. 
A 
The optimality criterion for the simplex method tells us that the jth 
variable of the primal problem is a candidate for an entering variable if 
zj - cj < 0. We have already noted that w i is a value, or cost, attached to 
each unit of resource i. Then zj represents the total value assigned to one 
unit of the jth product. In economics this value is called an imputed value 
because it is not directly determined by the marketplace; it is a value 
assigned using some other basis than an exchange of money. The jth 
variable is a candidate for an entering variable if zj < cj; that is, xj may 
enter if the imputed value zj of the jth product is less than the profit cj 
per unit of the jth product. 
We summarize the results of the Weak Duality Theorem and the 
Duality Theorem in the following table. The column and row labels have 
the following meanings: none, no feasible solution; finite, optimal solution 
with finite objective function value; and unbounded, feasible solutions with 
unbounded objective function value. 
TABLE 3.2 
Primal 
Dual 
None 
Finite 
Unbounded 
None 
Impossible 
(Theorem 3.7) 
Finite 
Impossible 
(Theorem 3.7) 
Unbounded 
Impossible 
(Theorem 3.5) 
Impossible 
(Theorem 3.5) 
Impossible 
(Theorem 3.5) 
We next present examples to show that the missing entries in Table 3.2 are 
all possible. 
EXAMPLE 2. 
Consider the linear programming problem 
Maximize 
z = 2 Xl "+ X2 
subject to 
3X 1 -- 2X 2 < 6 
X 1 -- 2X 2 _< 1 
X 1 ~_~ 0, 
X 2 ~__ 0~ 
(19) 
The set of feasible solutions is shown in Figure 3.1a. It is evident that this 
problem has an unbounded optimal solution. For setting x I = 0 allows x 2 
to be as large as we please. In this case z = x 2 is also as large as we please. 

3. 2 The Duality Theorem 
1 "l'l 
The dual problem to (19) is 
Minimize 
z' = 6w i "~- W2 
subject to 
3w I + 
w E >_ 2 
-- 
2w 
1 -- 2W 2 >__ 1 
W 1 ~__ 0, 
W 2 ~__ 0. 
The constraints are shown in Figure 3.1b. There are no feasible solutions 
to the problem, since the second constraint can never hold for nonnegative 
values of w I and w 2. 
/~ 
It is also possible, as the next example shows, that neither the primal 
problem nor its dual will have a feasible solution. 
EXAMPLE 3. 
Consider the linear programming problem 
Its dual problem is 
Maximize 
z = 3x I + 2x 2 
subject to 
2x 1 - 2x 2 < - 1 
-- 
2x 
I 
+ 
2X 2 _< -4 
X 1 >__ 0, 
X 2 >__ 0. 
Minimize 
z' --- - w1 - 
subject to 
2w 1 - 
2w 2 > 3 
- 2Wl + 2w2 > 2 
W 1 >" O, 
W 2 >" O. 
4W 2 

1711 
Chapter 3 Further Topics in Linear Programming 
FIGURE 3.2 
The graphs of the constraints of the primal problem are shown in Figure 
3.2a, and the graphs for the dual problem are shown in Figure 3.2b. 
Neither of these problems has a feasible solution. 
A 
Complementary Slackness 
In addition to the relations between an optimal solution to the primal 
problem and an optimal solution to the corresponding dual problem, which 
we have already discussed, we can obtain information about which con- 
straints may be "acting" on the solution to the primal problem. Specifi- 
cally, we can show that if an optimal solution to the primal problem makes 
a constraint into a strict inequality, then the corresponding dual variable 
must be zero. 
We consider a linear programming problem in standard form (13) as the 
primal problem. Its dual problem is given in (14). For these problems we 
state and prove the theorem on complementary slackness. 
THEOREM 3.8. 
For any pair of optimal solutions to the primal problem 
and its dual, we have: 
(a) For i = 1, 2,..., m, the product of the ith slack variable for the primal 
problem and the ith dual variable is zero. That is, x n + iwi = O, i = 1, 2,..., m, 
where x n +i is the i th slack variable for the primal problem. 
(b) For j = 1, 2,..., n, the product of the jth slack variable for the dual 
problem and the jth variable for the primal problem is zero. 
Another way of stating the theorem is to say that, if the ith slack 
variable of the primal problem is not zero, then the i th dual variable must 
be zero. Likewise, if the jth slack variable for the dual problem is not zero, 
then the jth primal variable must be zero. Note that it is possible for both 
the slack variable and its corresponding dual variable to be zero. 

3.2 The Duality Theorem 
1 "i~ 
Proof 
where 
We add slack variables to the primal problem and write 
Ax + Ix' = b, 
Xn+l 
Xr 
__ 
Xn + 2 
n+m 
is the vector of slack variables. Then for any vector of dual variables 
W1 
W2 
W -'- 
. 
we have 
wTAx + w T Ix' = w T b. 
This equality is an equality between numbers so that it is preserved when 
the transpose of each side is taken. We obtain 
xTATw -k- x'Tw 
= bTw. 
(20) 
optimal solutions 
x~X~ and Wo, we have, from the duality theorem, 
At the 
CTxo = bTWo . 
(21) 
Using (20), we may rewrite (21) as 
cTx0 = x~ATw0 + x~Tw0 . 
(22) 
In the dual problem, ATwo >__ c; hence, we may write (22) as 
cTx0 >_~ xTc -4- x~)Tw0 . 
(23) 
Now cTx0 is a number, so that CTxo = (cTx0) T = xTc, implying 0 >__ x'Tw0 . 
Since x~ >__ 0 and w o >__ 0 imply that x'oTwo > 0, we have equality in (23), 
and X~Wo = 0. That is, 
X n + 1W1 -~- X n + 2W2 "4- "'" "4- X n +mWm -- O. 
and each term is nonnegative. Therefore, for each i = 1, 2,..., m, we have 
Xn+ iwi = O. The proof of part (b) is similar. 
A 

180 
Chapter 3 Further Topics in Linear Programming 
EXAMPLE 4. 
Consider the linear programming problem 
Maximize 
z = 2Xl + x2 
subject to 
X 1 + 2x 2 < 8 
3x 1 + 4x 2 < 18 
X 1 >_~ 0, 
X 2 >_~ 0. 
The dual of this problem is 
Minimize 
z' = 8w~ + 18w2 
subject to 
W 1 + 3w 2 >_ 2 
2wl + 4w 2 >_ 1 
W 1 >__ 0, 
W 2 ~__ 0. 
The feasible regions for the primal and dual problems are shown in 
Figures 3.3a and 3.3b, respectively. 
An optimal solution to the primal problem is (verify) 
x 1 =6, 
x 2 =0, 
and 
z= 12. 
Since there is slack in the first constraint, the principle of complementary 
slackness says that the first dual variable must be zero in an optimal 
solution to the dual problem. Thus, without evaluating the objective 
function at the extreme points, we see that 
2 
W 1 -- 0, 
W 2 = 
must be an optimal solution to the dual problem. Furthermore, the value 
of the dual objective function at this extreme point must be 12. If there 
FIGURE 3.3 

3.2 The Duality Theorem 
181 
were several points for which w 1 -
-
 0, a point at which the dual objective 
function has value 12 would be an optimal solution to the dual problem by 
the Duality Theorem (Theorem 3.7). 
A 
The economic interpretation of complementary slackness is related to 
the understanding of the dual variables as marginal costs or shadow prices. 
Suppose that in an optimal solution to the primal problem the i th slack 
variable is nonzero. This means that there is more of the i th input 
available than is needed for this optimal solution. The value of the ith 
slack variable is exactly the excess of the ith input. But there is no need 
for any of this excess of the i th input; its marginal value is zero. The 
theorem on complementary slackness tells us that, if the i th primal slack 
variable is positive, then the i th dual variable, which can be thought of as 
the marginal value of the i th input, is zero. On the other hand, if in an 
optimal solution to the dual problem the i th dual variable is nonzero, its 
value can be thought of as the marginal value of the ith input. For in this 
case the ith primal slack variable is zero, indicating that all the ith input 
has been used and that it is desirable to have more. Its marginal value is 
positive. 
The dual variables can also be viewed as a measure of the contribution 
of each resource to the maximum profit. Recall that at optimal solutions 
z=z' 
or 
bTw = cTx = maximum profit. 
(24) 
The constraints of the primal problem in which there is slack at an optimal 
solution correspond to dual variables that have value zero by complemen- 
tary slackness. Thus, the corresponding resources do not contribute to the 
maximum profit. Considering the constraints of the primal problem in 
which there is no slack at an optimal solution, the values of the corre- 
sponding dual variables are nonzero and do indeed affect the maximum 
profit. We see from Equation (23) that the values of the nonzero dual 
variables divide the profit proportionately among the corresponding re- 
sources. 
For example, in the sawmill problem, the saw may be assigned a value 
of $35 per hour (w I = 35) and the plane may be assigned a value of $10 
per hour (w 2 = 10). The maximum profit of $430 is attributable to the saw 
and the plane at the respective rates of $35 per hour and $10 per hour. 
That is, to make this profit, the saw is 31 times more valuable than the 
plane. In this sense the dual variables are accounting costs and would be 
useful for cost-accounting procedures. 
We summarize the discussion and examples of this section by complet- 
ing Table 3.3 by citing an example of the possible combinations of results 
for the primal and dual problem pairs. This summary appears in Table 3.3. 

182 
Chapter 3 Further Topics in Linear Programming 
TABLE 3.3 
Primal 
Dual 
None 
Finite 
Unbounded 
None 
Possible 
Impossible 
Possible 
(Example 3) 
(Theorem 3.7) 
(Example 
2) 
Finite 
Impossible 
Possible 
Impossible 
(Theorem 3.7) 
(Example 
4) 
(Theorem 3.5) 
Unbounded 
Possible 
Impossible 
Impossible 
(Example 2) 
(Theorem 3.5) 
(Theorem 3.5) 
3.2 
EXERCISES 
In Exercises 1-4, properties of solutions to a given linear programming problem 
are specified. Describe in as much detail as possible the solutions to the dual of the 
given problem. (Hint: You may need to use Theorems 3.5-3.8.) 
1. An optimal solution is [0.27 1.83 0.94 
0.5 
0 
0] T with objective function 
value 117.81. 
2. An optimal solution to the dual problem is [0 3 
15 0 
5] T with dual 
objective function value 125. 
3. There are no feasible solutions to the problem. 
4. There are solutions to the dual problem with arbitrarily large dual objective 
function values. 
5. Suppose for the linear programming problem 
Maximize 
z = eTx 
subject to 
Ax<b 
x>0, 
we know that b = [12 21 
8 
2 5]T. Assume that w= [0 4 
5 
0 
3] Tis 
an optimal solution to the dual of the given problem. Calculate the optimal 
value of the objective function for the given problem. 
In Exercises 6 and 7, solve the given linear programming problem using the 
simplex method. Also, set up and solve the dual of the given problem. Finally, 
verify that your solutions satisfy part (b) (iii) of the Duality Theorem. 
6. Minimize 
z=4x+6y 
subject to 
x+3y>5 
2x+ 
y>3 
x>0, 
y>0. 

3.2 The Duality Theorem 
1 ~3 
7. Maximize 
z--8x 
I -1- 9x 2 + 5x 3 
subject to 
x 1 + 
x 2 + 2x 3 < 2 
2x 1 + 3x 2 -t- 4x 3 < 3 
3xl +3x 2+ 
x 3 <4 
xj>O, 
j= 
1,2,3 
8. A health food store packages a nut sampler consisting of walnuts, pecans, and 
almonds. Suppose that each ounce of walnuts contains 12 units of protein and 
3 units of iron and costs 12 cents, that each ounce of pecans contains 1 unit of 
protein and 3 units of iron and costs 9 cents, and that each ounce of almonds 
contains 2 units of protein and 1 unit of iron and costs 6 cents. If each package 
of the nut sampler is to contain at least 24 units of protein and at least 18 units 
of iron, how many ounces of each type of nut should be used to minimize the 
cost of the sampler? (Hint: Set up and solve the dual problem. Then use the 
principle of complementary slackness to solve the given problem.) 
5 
5 
27 
9. Show without using the Simplex Method that x I = ~, x 2 = ~, x 3 -- ~ is an 
optimal solution to the following linear programming problem: 
Maximize 
z = 9x~ + 14x 2 + 7x 3 
subject to 
2x I + 
x 2 + 3x 3 < 6 
5x1+4x2+ 
x 3< 12 
2x 2 
< 5 
x 1, 
x 2, 
x 3 unrestricted. 
(Hint." Formulate the dual of this problem and then find a feasible solution.) 
10. Consider the linear programming problem 
Maximize 
z -- 3x 1 + 4x 2 
subject to 
X 1 -+- 2X 2 < 10 
X 1 -[- 
X2_~8 
3x 1 + 5x 2 __< 26 
X 1 >__ 0, 
X 2 >_~ 0. 
By using the principle of complementary slackness, show that w I - 0 in any 
optimal solution to the dual problem. 

184 
Chapter 3 
Further Topics in Linear Programming 
11. Suppose that x I "~ 2, x 2 = 0, 
X 3 = 4 is an optimal solution to the linear 
programming problem 
Maximize 
z = 4x 1 + 
2X 2 -t- 3x 3 
subject to 
2x 1+3x 2+ x 3< 12 
x I + 4x 2 + 
2x 3 < 
10 
3x 1 + 
x 2 -]- 
x 3 __< 10 
X 1 >_~ 0, 
X 2 >_~ 0, 
X 3 >_~ 0. 
Using the principle of complementary slackness and the duality theorem 
(Theorem 3.7), find an optimal solution to the dual problem. What value will 
the objective function of the dual problem have at this optimal solution? 
3.3 COMPUTATIONAL RELATIONS BETWEEN THE 
PRIMAL AND DUAL PROBLEMS 
We now modify the form of a tableau and the steps in the pivoting 
process to take advantage of the new information we have. We add an 
additional row and column to a tableau. The row is written above the 
column labels and contains the vector c T = 
[c I 
c 2 
..o 
C s]. The column 
is written to the left of the column denoting the basic variables and 
contains the vector 
Ci 1 
Ci 2 
CB -- 
. 
9 
Ci m 
The entries of c a can be determined by copying the values of c/from the 
new top row corresponding to the basic variables of the tableau. 
The pivoting step can be changed to compute the entries of the 
objective row by using (12) of Section 3.2 rather than by using elementary 
row operations. Recall that the entries of the objective row are z/- c/. We 
now rework an example using these new ideas. 
EXAMPLE 1. 
Consider the linear programming problem from Examples 
2 and 3 in Section 2.3. 
Maximize 
z 
-- x I - 
2X 2 -- 3X 3 -- X 4 -- X 5 d- 2X 6 
subject to 
X 1 q- 2x 2 -t- 2x 3 -t-X 4 q- 
X 5 
+ X 7 
"- 12 
x 1 + 2x 2 + 
X 3 q-X 4 -I- 2x 5 + x 6 
~- 18 
3x I + 6x 2 + 2x 3 + x 4 + 3x 5 
+ x 8 = 24 
x/>0, 
j=1,2,...,8. 

3.3 Computational Relations between the Primal and Dual Problems 
185 
We have 
1 
2 
2 
1 
1 
0 
1 
0] 
A= 
1 
2 
1 
1 
2 
1 
0 
0J, 
3 
6 
2 
1 
3 
0 
0 
1 
1 
-2 
b= 
18 , 
and 
c= 
1 " 
24 
2 
0 
0 
Note that this problem is in the form of Equation (1) in Section 3.2. The 
matrix A contains the identity matrix by taking columns 7, 6, and 8 in this 
order. We have denoted the artificial variables by x 7 and x 8 rather than by 
Yl and Y2 to be consistent with (1). 
The initial tableau for the auxiliary problem of the two-phase method is 
shown in Tableau 3.2a, for which the objective row has not been filled in. 
Recall from Section 2.3 that we had to eliminate the initial basic variables 
x7 and x 8 from the objective function by substitution. This substitution 
procedure is replaced by the procedure of computing the objective row as 
zj - cj. We have 
c~= [-1 
0 
-1] 
since i z = 7, i 2 = 6, and i 3 -- 8. The entry in column 1 of the objective row 
is, by (11), 
[1] 
Z 1 -- 
C 1 --- 
[ -1 
0 
-1] 
1 
- 0 = -4. 
3 
Tableau 3.2a 
CB 
1 
X 7 
0 
X 6 
1 
x 8 
0 
0 
0 
0 
0 
0 
1 
-1 
Xl 
X 2 
X3 
X4 
X5 
X6 
X 7 
X8 
X B 
1 
2 
2 
1 
1 
0 
1 
0 
12 
1 
2 
1 
1 
2 
1 
0 
0 
18 
3 
6 
2 
1 
3 
0 
0 
1 
24 
In the same way we compute the other entries obtaining the full initial 
tableau (Tableau 3.2b). Note that it is the same as Tableau 2.23 (Section 
2.3). 

186 
Chapter 3 Further Topics in Linear Programming 
Tableau 3.2b 
CB 
1 
X 7 
0 
X 6 
1 
x s 
o 
o 
o 
o 
o 
o 
1 
1 
Xl 
X2 
X3 
X4 
X5 
X 6 
X 7 
X 8 
1 
2 
2 
1 
1 
0 
1 
0 
1 
2 
1 
1 
2 
1 
0 
0 
3 
(~) 
2 
1 
3 
0 
0 
1 
4 
8 
4 
2 
4 
0 
0 
0 
x B 
12 
18 
24 
36 
The value of the last entry in the objective row is computed as 
Z "- CTXB, 
where x a is the last column of the tableau. For this tableau, 
n 
.__ [100] 
[100] 
0 
1 
0 
and 
B -1 -- 
0 
l 
0 
0 
0 
1 
0 
0 
1 
since 
il --" 7 
and 
i1] 
[0] 
Ail = 
0 , 
i 2 = 6 
and 
Ai2 = 
1 ; 
0 
0 
i 3 --- 8 
and 
Ai3 
The entering and departing variables are determined as they were 
previously. We find x 2 is the entering variable and x 8 is the departing 
variable. In the next tableau (Tableau 3.3), x 7, x 6, and x 2 will be the basic 
variables, in this order, so that i I = 7, i 2 = 6, and i 3 -- 2. Thus, 
[_1] [lO2] 
ca= 
0 
, 
B= 
0 
1 
2 
, 
0 
0 
0 
6 
and 
B-1 
1 
1 
0 
-5 
1 
= 
0 
1 
-~ 
1 
0 
0 
g 

3.3 Computational Relations between the Primal and Dual Problems 
187 
Tableau 3.3 
CB 
1 
X 7 
0 
x 6 
0 
x 2 
0 
0 
0 
0 
0 
0 
1 
1 
Xl 
X 2 
X3 
X4 
X 5 
X6 
X7 
X8 
O 
0 
0 
1 
0 
0 
3"2 
"31 
1 
2 
1 
0 
0 
~ 
~ 
1 
1 
0 
1 
1 
1 
z 
1 
0 
0 
! 
2 
3 
6 
2 
6 
4 
2 
4 
0 
0 
~ 
~ 
0 
0 
0 
X B 
4 
10 
4 
We may verify that 
x B = 
B-lb-- 
1 
10 
[4] 
0 
1 
-5 
18 
= 
10 . 
1 
24 
4 
0 
0 
~ 
We also know that tj 
for example, 
= B-1A j, j = 1, 2,..., 8, and we may verify that, 
t 3 -- B-1A3 
1 
4 
10 
= 
0 
1 
-7 
1 
= 
5 
9 
1 
2 
1 
0 
0 
~ 
5 
The objective row is computed after the usual procedure for pivoting is 
applied to find the other rows. The entries in the objective row are found 
by 
Zj -- Cj = cTtj -- cj. 
For example, the entry in the third column of Tableau 3.3 is 
4 5 
1 
4 
z 3-c 3=cTt3-c 
3=[-1 
0 
0] 5 
-0= 
-5. 
1 5 
We see that Tableau 3.3 is the same as Tableau 2.32 of Section 2.3. We 
continue the simplex algorithm by determining the entering and departing 
variables for Tableau 3.3 and pivoting to form Tableau 3.4. 

1118 
Chapter 3 Further Topics in Linear Programming 
Tableau 3.4 
CB 
0 
X 3 
0 
X 6 
0 
x 2 
0 
0 
0 
0 
0 
0 
-1 
-1 
Xl 
X2 
X3 
X4 
X5 
X6 
X7 
X 8 
X B 
1 
0 
0 
3 
1 
O 
O 
1 
~ 
~ 
~ 
3 
1 
1 
1 
1 
1 
9 
0 
0 
0 
~ 
~ 
4 
' 
1 
0 
0 
1 
0 
1 
1 
3 
0 
0 
0 
0 
0 
0 
1 
1 
0 
., 
Tableau 3.4 gives an optimal solution to the auxiliary problem. The next 
step is to form the initial tableau for Phase 2, as described in Section 2.3. 
The columns corresponding to the artificial variables x 7 and x 8 are 
deleted and the entries in the objective row of this tableau are computed 
as zj- 
cj. Remember to put the coefficients of the original objective 
function at the top of the tableau. Carrying out these steps, we obtain 
Tableau 3.5. 
Tableau 3.5 
CB 
- 3 
X 3 
2 
X 6 
2 
X 2 
1 
2 
3 
1 
1 
2 
X 1 
X2 
X 3 
X4 
X 5 
X6 
X B 
1 
0 
0 
3 
0 
0 
1 
1 
1 
9 
0 
0 
0 
0 
3 
1 
0 
0 
~1 
1 
2 
0 
3 
2 
0 
0 
Pivoting, we obtain Tableau 3.6, which yields an optimal solution to the 
given problem. The reader should check that he or she understands how 
the entries were determined. 
Tableau 3.6 
CB 
3 
x 3 
2 
x 6 
1 
x 1 
2 
3 
1 
1 
2 
X1 
X 2 
X3 
X4 
X 5 
X 6 
XB 
1 
0 
0 
3 
0 
0 
1 
1 
0 
0 
0 
~ 
1 
1 
9 
1 
2 
0 
0 
1 
0 
6 
1 
4 
0 
15 
0 
4 
0 
A 

3.3 Computational Relations between the Primal and Dual Problems 
1119 
We have now defined enough tools to complete our discussion of 
artificial variables. Recall that we had taken a problem in canonical form 
Maximize 
z = cTx 
subject to 
Ax=b 
x>0, 
where b > 0. We introduced artificial variables into each of the constraint 
equations. For Phase 1 we used a different objective function, namely, 
Minimize 
z = Y l + Y2 + "'" + Ym 
where Yi, i = 1,2,..., m are the artificial variables. Suppose that at the 
end of Phase 1, the minimum of this objective function is zero but that 
there are artificial variables which remain basic (at value zero) in the final 
optimal tableau. We now proceed as follows. 
Phase 2 
The initial tableau for Phase 2 is the final tableau of Phase 1 with the 
_ 
following modifications. 
(a) Delete the columns from the final tableau of Phase 1 that are 
labeled with the nonbasic artificial variables. 
(b) Replace the row above the column labels with the coefficients of the 
original objective function, assigning 0 as a cost for each of the basic 
artificial variables. 
(c) Form the vector c B from the new row of objective function coeffi- 
cients. 
(d) Calculate the entries in the new objective row as zj - cj = c~tj - cj. 
As we proceed with the simplex method for Phase 2 we must ensure 
that the remaining artificial variables do not take on positive values. This 
would happen when one of these variables remained basic and the pivotal 
elimination gave a positive entry in x B for the position labeled by the 
artificial variable. Suppose that x k is to be the entering variable and that 
the rows labeled by artificial variables are il, i2,...,ip. 
Denote the kth 
column of the current tableau by 
tlk 
t2k 
tk= 
. 
9 
tmk 
It can be shown (Exercise 18) that if 
til k >__ 0, 
ti2 k >__ 0, 
..., 
tip k >__ 0, 

190 
Chapter 3 
Further Topics in Linear Programming 
then none of the artificial variables that are basic will take on a positive 
value. If, however, we have 
tir k ( 0 
for some r, r = 1, 2,..., p, then the usual simplex procedure could cause 
the artificial variable that labels row 
i r to take on a positive value. 
Consequently, we must modify the usual simplex procedure. The new 
procedure for selecting a departing variable is as follows. If at least one of 
the entries in the entering variable column corresponding to a row labeled 
by an artificial variable is negative, choose one of these artificial variables 
as the departing variable. Otherwise, use the usual simplex procedure to 
obtain a finite optimal solution or to discover that such a solution does not 
exist. 
EXAMPLE 2. 
In Section 2.3 we showed that the problem given in 
Example 5 ended with an artificial variable in the basis. We rework this 
example using the two-phase method. The original problem in canonical 
form is 
Maximize 
z--x 1 + 2x 2 + x 3 
subject to 
3x 1 + 
X 2 -- X 3 : 
15 
8X 1 -~- 4x 2 -x 
3 :- 50 
2x I + 2x 2 + X 3 
-- 20 
X 2 >_~ O, 
X 3 >__ O. 
X 1 ~___ 0, 
The new problem for Phase 1 is 
Minimize 
subject to 
3x I + 
x 2 - x 3 + yl 
8X 1 + 4X 2 -- X 3 
2X 1 + 2X 2 + X 3 
xj>_O, 
j= 1,2,3; 
We rewrite the objective function as 
z =Yl +Y2 +Y3 
= 
15 
+Y2 
= 50 
+Y3 = 20 
Yi >- 0, 
i = 1,2,3. 
Maximize 
z = 
-Yl 
- Y2 - Y3 
to have a maximization problem. 
We now have the following sequence of tableaux (Tableaux 3.7-3.10) 
for Phase 1. The objective row of the initial tableau is computed by using 
zj - %. Since the initial basic variables are Yl, Y2, and Y3, we have 
[1] 
CB-- 
1 
1 

3.3 Computational Relations between the Primal and Dual Problems 
191 
and 
Similarly, 
and 
Z 1 -- C 1 = 
[ -1 
-1 
[3] 
-1] 
8 
-0= 
-13. 
2 
Z2 -- C2 -- 
[ -1 
-1 
[1] 
-1] 
4 
-0=-7 
2 
Z 3 -- C 3 -- 
1. 
For the basic variables Yl, Y2, and Y3 we have [1] 
Z 4 -- C 4 = 
[ -1 
-1 
-1 ] 0 
- 
(-1) 
= 0 
0 
and, similarly, z 5 - c 5 = 0 and z 6 - c 6 = 0. 
The value of the objective function is 
[-1 
-1 
-11 
50 
= -85. 
20 
The results of these computations are shown in Tableaux 3.7-3.10. 
Tableau 3.7 
CB 
1 
Yl 
1 
Y2 
1 
Y3 
0 
0 
0 
1 
1 
1 
x1 
x2 
x3 
Yl 
Y2 
Y3 
(~) 
1 
1 
1 
0 
0 
15 
8 
4 
1 
0 
1 
0 
50 
2 
2 
1 
0 
0 
1 
20 
13 
7 
1 
0 
0 
0 
85 
Tableau 3.8 
CB 
X1 
Y2 
Y3 
0 
0 
0 
-1 
1 
1 
x2 
x3 
Yl 
Y2 
Y3 
X B 
x I 
XB 
1 
1 
1 
0 
O 
5 
1 
~ 
~ 
4 
(t) 
8 
o 
~ 
1 
0 
10 
0 
4 
5 
2 
~ 
~ 
0 
1 
10 
0 
8 
lO 
13 
0 
0 
20 
3 
3 
3 

192 
Chapter 3 Further Topics in Linear Programming 
Tableau 3.9 
CB 
0 
X 1 
0 
x 3 
1 
Y3 
0 
0 
0 
1 
-1 
-1 
x1 
x2 
x3 
Yl 
Y2 
Y3 
XB 
3 
1 
1 
0 
7 
1 
3- 
0 
~ 
3- 
0 
4 
1 
s 
3 
3- 
5 
3- 
O 
6 
0 
0 
0 
(~) 
-1 
1 
0 
0 
0 
0 
1 
2 
0 
0 
Tableau 3.10 
CB 
0 
X 1 
0 
x 3 
1 
Yl 
0 
0 
0 
1 
1 
-1 
x1 
x2 
x3 
Yl 
Y2 
Y3 
Xa 
3 
1 
1 
3- 
0 
0 
~ 
f0 
7 
0 
4 
1 
0 
1 
4 
6 
3- 
3 
1 
1 
0 
0 
0 
0 
1 
2 
3 
1 
0 
0 
0 
0 
0 
~ 
Thus, Phase 1 has an optimal solution with x 1 = 7, x 3 = 6, Y l = 0 and 
value 0 for the objective function. The artificial variable Y l appears as a 
basic variable in the optimal solution. 
The initial tableau for Phase 2 is shown in Tableau 3.11. The columns 
corresponding to Y2 and Y3 have been deleted and a cost of 0 has been 
assigned to Y l- The objective row has been filled in, using zy - cj. 
Tableau 3.11 
C B 
1 
X 1 
1 
x 3 
0 
Yl 
1 
2 
1 
0 
X1 
X2 
X3 
Yl 
XB 
3 
1 
3- 
0 
0 
7 
s(~ 
1 
0 
6 
0 
0 
0 
0 
1 
0 
..... 
0 
3 
0 
0 
13 
5 
We now apply the simplex method to Tableau 3.11. In selecting the 
departing variable we make sure the entry in the row labeled by Y l and the 
pivotal column is nonnegative. If it is not, we will choose Y l as the 
departing variable. We get Tableau 3.12. 

3.3 Computational Relations between the Primal and Dual Problems 
193 
Tableau 3.12 
CB 
1 
X 1 
2 
x 2 
0 
Yl 
1 
2 
1 
0 
x1 
x2 
x3 
Yl 
1 
0 
3 
0 
4 
0 
1 
5_ 
0 
4 
0 
0 
0 
1 
0 
0 
3 
0 
4 
XB 
An optimal solution to the original problem is therefore 
5 
Xl= 
~ 
15 
X2--- ~- 
X 3 --" 0, 
which gives -~ as the optimal value of the objective function. 
A 
Solution to Dual Problem from Final Tableau of Primal Problem 
One of our objectives in this section is to describe how to find an 
optimal solution to the dual problem from the final tableau for the primal 
problem. We discuss the easiest case first, namely, when the primal 
problem is in canonical form, 
Maximize 
z -- cTx 
subject to 
Ax=b 
x>_O, 
where A contains the identity matrix and b > 0. In particular this situation 
arises when the primal problem is given in standard form and has been 
converted to canonical form by adding slack variables. There are two easily 
given descriptions for finding an optimal solution to the dual of the 
problem given above. The dual problem is 
Minimize 
z' = bTw 
subject to 
ATw>c 
w unrestricted. 

1 ~4 
Chapter 3 
Further Topics in Linear Programming 
An optimal solution 
to it is given by 
W "-- 
w 1 
w2 
~ 
Wm 
W T = cTB - 1, 
(1) 
where B is the matrix consisting of certain columns of the initial tableau. 
The columns that are used in B correspond to the basic variables of the 
final tableau of the primal problem. We can find B -1 from the final 
tableau as follows. From our assumptions about A and b we may infer that 
the columns labeled by the initial basic variables in the initial tableau form 
the m • m identity matrix when they are properly ordered. It can be 
shown that the columns in the final tableau with the same labels as the 
initial basic variables and arranged in the same order give B-1. 
It can also be shown that an optimal solution to the dual problem is 
given by 
Wj -- Cij "Jr- (Zij -- Cij), 
(2) 
where the subscript ij ranges over the indices of the initial basic variables. 
Of course, c/j is the entry above the label of the ijth column and zij - cij 
is the corresponding entry in the objective row. This second description 
shows that if an initial basic variable is a slack or artificial variable, the 
value of the corresponding dual variable is the entry in the ijth column of 
the objective row of the final tableau of the primal problem. This fact 
follows, since cij = 0 for any slack or artificial variable. 
EXAMPLE 3. 
Consider as our primal problem the linear programming 
problem 
Maximize 
z = 8x 1 + 9x 2 + 4x 3 
subject to 
X 1 -]- 
X 2 -~- 2X 3 _< 2 
2x I + 3x 2 + 4x 3 _< 3 
7x 1 + 6x 2 + 2x 3 < 8 
X 1 >__ O, 
X 2 >__ O, 
X 3 >__ O. 

3.3 Computational Relations between the Primal and Dual Problems 
1 ~ 
Introducing the slack variables x4, x5, and x6, our primal problem be- 
comes 
Maximize 
subject to 
x I + 
x 2 + 2x 3 + x 4 
2x I + 3x 2 + 4x 3 
7x I + 6x 2 + 2x 3 
xj >_ O, 
z = 8x 1 + 9x 2 + 4x 3 
+ X 5 
+X 6 
j = 1,2,...,6. 
=2 
=3 
=8 
Solving this problem by the simplex method we are led to the sequence of 
Tableaux 3.13-3.15. 
The dual problem in this example is 
Minimize 
z' = 2 Wl + 3w2 "+ 8W3 
subject to 
w 1 + 2w 2 + 7w 3 >__ 8 
w I + 3w 2 + 6w 3 >__ 9 
2w I + 4w 2 + 2w 3 >__ 4 
W 1 ~_~ 0, 
W 2 ~_~ 0, 
W 3 ~__ 0. 
The solution to this problem is found from Tableau 3.15. In Tableau 3.15 
the basic variables are x4, x2, and x l, in that order. Therefore, reading 
from Tableau 3.13, we find 
and 
[1] [1] [1] 
A 4 = 
0 
, 
A 2 = 
3 
, 
A 1 = 
2 
, 
0 
6 
7 
B 
.__ 
1 
1 
1] 
0 
3 
2 . 
0 
6 
7 
Since the initial basic variables are x4, x5, and x6, in that order, we find 
the columns of B -1 under the labels x 4, x 5, and x 6 in Tableau 3.15. Thus, 
B-1 
1 
1 
1 
9 
9 
7 
2 
-- 
0 
~ 
9 
0 
2 
1 
3 

196 
Chapter 3 Further Topics in Linear Programming 
Tableau 3.13 
s 
X 4 
X5 
X 6 
8 
9 
4 
0 
0 
Xl 
X2 
X3 
X4 
X 5 
1 
1 
2 
1 
0 
2 
(~) 
4 
0 
1 
7 
6 
2 
0 
0 
8 
9 
4 
0 
0 
0 
x6 
XB 
0 
2 
0 
3 
1 
8 
0 
0 
Tableau 3.14 
s 
0 
x 4 
9 
x 2 
0 
X 6 
8 
9 
4 
0 
0 
0 
X 1 
X 2 
X 3 
X4 
X 5 
X 6 
X B 
1 
0 
2 
1 
1 
~ 
~ 
0 
1 
2 
4 
0 
1 
0 
1 
(~) 
0 
6 
0 
-2 
1 
2 
2 
0 
8 
0 
3 
0 
9 
Tableau 3.15 
s 
0 
X 4 
9 
x 2 
8 
X 1 
8 
9 
4 
0 
0 
0 
X1 
X2 
X3 
X4 
X 5 
X 6 
4 
1 
1 
1 
7 
0 
0 
~ 
~ 
~ 
8 
7 
2 
5 
0 
1 
-~ 
0 
~ 
~ 
2 
1 
2 
1 
0 
2 
0 
3 
~ 
5 
2 
31 
0 
0 
4 
0 
- 3 
~ 
3 
XB 
Then an optimal solution to the dual problem is, by (1), 
1 
1 
1 
9 
9 
w T=cTB 
-1=[0 
9 
8] 
0 
7 
2 
= [0 
5 
2] 
9 
3 
3 
9 
2 
1 
0 
3 
If (2) is used, an optimal value of W l is 
C 4 -- (Z 4 -- C 4) 
~- 0 
q- 0 
-- 0 
since x 4 was the first initial basic variable. Likewise, an optimal value of 
w 2 comes from the second initial basic variable x 5 and is given as 
5 
5 
w 2=c 5 + (z 5-c 
5) =0+3=7. 

3.3 Computational Relations between the Primal and Dual Problems 
1 ~7 
Finally, 
2 
2 
W 3 -- C 6 -~" (Z 6 -- C 6) "- 0 "~- ~ -- 3" 
Thus, this solution to the dual problem yields the value ~ for the dual 
objective function, which checks with the value of the primal objective 
function. Theorem 3.6 assures us that these solutions are indeed optimal, 
since they yield the same value for the objective functions. 
A 
Now let us consider finding a solution to the dual of an arbitrary 
general linear programming problem. As we discussed in earlier sections, 
such a problem can always be converted to one in the following form: 
Maximize 
z = crx 
subject to 
A(1)x _< b (1) 
A(2)x >__ b (2) 
A(3)x - 
b(3) 
x>__0 
b (1) ~_ 0, 
b (2) > 0, 
b (3) > 0. 
(3) 
This conversion does not change the set of feasible solutions but may 
change the objective function of the original problem by multiplying it by 
-1. A problem in the form above is ready to be solved by the simplex 
method after slack variables and artificial variables are introduced. 
After multiplying the second set of constraints of (3) by - 1, we may set 
up the dual problem. It is 
Minimize 
z' = [ b (1)T 
-- b (2)T 
b (3)T ] 
subject to 
I 
w 1) 1 
[A O)T 
-A (2)T A(3) T] w (2) >_c 
w (3) 
W (1) ~ O, W (2) > O, 
w (3) unrestricted. 
I 
w t) 1 
w (2) 
w (3) 
(4) 
We seek the solution to (4) using the final tableau from the solution to 
(3). If the two-phase method is used, the columns labeled by artificial 
variables must not be discarded at the end of Phase 1. They must be kept 
throughout Phase 2 and must be modified accordingly by each pivoting 
step. However, the entries in the objective row labeled by artificial vari- 
ables must not be used when checking the optimality criterion. This means 

1 ~8 
Chapter 3 
Further Topics in Linear Programming 
that there may be negative numbers in the objective row when an optimal 
solution is reached, but these numbers will only be in columns labeled by 
artificial variables. 
Assuming that all the columns labeled by artificial variables are avail- 
able, an optimal solution to the dual problem (4) can be obtained from (1) 
or (2). If (1) is used, then B -1 is automatically available, as described in 
Example 3, in the final tableau of Phase 2 from the columns labeled by the 
initial basic variables. We then compute 
V~, T -- cTB-1, 
but ~ is not 
an optimal solution to the dual problem (4). Because the 
second set of constraints in (3)was multiplied by -1 to find the dual 
problem but was not changed to use the simplex algorithm, those entries in 
r corresponding to the second set of constraints in (3) must be multiplied 
by -1. The vector thus formed is w, an optimal solution to the dual 
problem (4). 
If (2) is used, we must distinguish between the two-phase and the big M 
methods. If the two-phase method is used, then the cost of an artificial 
variable is 0, so that 
1~ i -- 0 if" ( Z i -- O) -- Z i. 
With the big M method the cost of an artificial variable c/ is -M, and 
z i - 
c i will be of the form k + M, so that 
Wi = -M + (k + M) = k. 
In either case we must, as above, multiply each of the 1~ i by -1, where i 
runs through the indices of the second set of constraints in (3). The set of 
values Wl, W2,...,Wm 
thus obtained is an optimal solution to the dual 
problem. 
EXAMPLE 4. 
problem 
Consider as our primal problem the linear programming 
Maximize 
z = 3x I d- 2x 2 + 5x 3 
subject to 
X 1 -~- 3X 2 d- 2X 3 __< 15 
2x 2 -- 
X 3 >__ 5 
2x 1 d- 
x 2 -- 5x 3 = 
10 
x I >_~ 0, 
x 2 >_~ 0, 
x 3 ~__ 0. 

3.3 Computational Relations between the Primal and Dual Problems 
199 
After multiplying the second constraint of the primal problem by -1, we 
find that the dual of the resulting problem is given as 
Minimize 
z'= 
15w I --5w 2 + 10w 3 
subject to 
W 1 
+ 2W 3 > 3 
3Wl--2W 2+ 
w 3 >_2 
2w 1 + 
W 2 -- 5W 3 >__ 5 
Wl >0, 
w2 > 0, 
w3 unrestricted. 
Introducing the slack variables x 4 and x 5 and the artificial variables Yl 
and Y2, we can formulate the auxiliary problem to the primal problem as 
Minimize 
z = yl + Y2 
subject to 
x I + 3x 2 + 2x 3 +x 4 
= 15 
2x2- 
x3 
-x5 +Yl 
=5 
2x1+ 
x2-5x3 
+Y2= 
10 
x i>0, 
i= 1,2,3,4; 
Yl >0, 
Y2 >0. 
From this statement of the primal problem we can construct our initial 
tableau (Tableau 3.16). 
The initial basic variables are x 4, Y l, and Y2. At the end of Phase 1 we 
have Tableau 3.17. 
Tableau 3.16 
CB 
0 
X 4 
1 
Yl 
1 
Y2 
0 
0 
0 
0 
0 
1 
1 
x1 
x2 
x3 
x4 
x5 
Yl 
Y2 
1 
3 
2 
1 
0 
0 
0 
0 
2 
1 
0 
1 
1 
0 
2 
1 
5 
0 
0 
0 
1 
XB 
15 
5 
10 
2 
3 
6 
0 
1 
0 
0 
15 
Tableau 3.17 
CB 
X4 
X2 
Xl 
0 
0 
0 
0 
0 
1 
1 
Xl 
x2 
x3 
x4 
x5 
Yl 
Y2 
XB 
0 
0 
0 
0 
0 
1 
1 
0 
0 
0 
23 
1 
5 
5 
1 
15 
4 
4" 
4 
2 
4 
1 
1 
1 
0 
5 
0 
1 
~ 
0 
~ 
~ 
9 
1 
1 
1 
15 
1 
0 
~ 
0 
~ 
~ 
~ 
4 

200 
Chapter 3 Further Topics in Linear Programming 
Converting to Phase 2 but keeping the columns labeled with artificial 
variables, we construct Tableau 3.18. 
Tableau 3.18 
CB 
0 
X 4 
2 
x 2 
3 
x 1 
3 
2 
5 
0 
0 
0 
0 
Xl 
x2 
x3 
x4 
x5 
Yl 
Y2 
0 
0 
23 
1 
5 
5 
1 
4 
4" 
4 
2 
1 
1 
1 
0 
0 
1 
~ 
0 
~ 
9 
0 
1 
1 
1 
1 
0 
4 
~ 
~ 
1 
1 
3 
0 
0 
5.1 
0 
~ 
~ 
XB 
Now we perform another iteration of the simplex method, including the 
results of the pivoting step on the columns labeled by artificial variables 
but ignoring these columns when applying the optimality criterion. We 
obtain the final tableau (Tableau 3.19). Using (1) to obtain an optimal 
solution to the dual problem, we first compute 
]~,T __ craB - 1 
or 
4 
5 
23 
23 
9 
fit = [5 
2 
31 
2--3 
9 
17 
23 
23 
= 
~ 
23 
2-5 
9 
23 
7 
23 
As before, 
W 1 -- I~1, 
W 2 ~-~ --1~,'2, 
and 
w 3 -- I~ 3. 
Tableau 3.19 
CB 
5 
x 3 
2 
x 2 
3 
x I 
3 
2 
5 
0 
0 
0 
0 
Xl 
x2 
x3, 
x4 
x5 
Yl 
Y2 
XB 
4 
5 
5 
2 
15 
0 
0 
1 
~ 
23 
23 
23 
23 
2 
9 
9 
~3 
65 
0 
1 
0 
~ 
23 
23 
9 
17 
17 
7 
120 
1 
0 
0 
~ 
23 
23 
23 
0 
0 
0 
51 
58 
58 
2~ 
565 
23 
23 
23 
23 
Thus, an optimal solution to the dual problem is 
wT 
[51 
58 
9] 
= 
~ 
2-5 
2-5 

3.3 Computational Relations between the Primal and Dual Problems 
201 
and the value of the dual objective function is 
z'- 
15(~) - 5(~) + 10(9) 
- 
565 
23 " 
We see from Tableau 3.18 that an optimal solution to the primal 
problem is 
120 
65 
15 
Xl 
= 
2-'-~'~ 
X2 = 
"~ 
X3 -- 
2"3" 
and the value of the objective function is 565 
-~3-, which is the same as the 
value for the dual problem. 
We can also solve the primal problem using the big M method, 
obtaining the final tableau (Tableau 3.20). Using (2)we can obtain an 
optimal solution to the dual problem from Tableau 3.20 as follows. Since 
the first initial basic variable is x4, we then find 
W 1 -" 1~ 1 -- C 4 -'[- (Z 4 -- C 4) 
= 
0 
"[- -- 
The second initial basic variable is Y l, so that 
~) .... 
w2= -M+ 
(M- 
58 
51 
51 
23 
23 9 
58 
23" 
But since the second constraint was multiplied by -1 in forming the dual 
problem, we must multiply if2 by -1 to obtain the value of the dual 
variable at the optimum. We have 
W2 
ml,~ 2 
58 
Proceeding as in the case of the first dual variable, we find 
9 
w3 =w3 
-M+M+ 
9=~. 
Substituting these values into the objective function, we get 
z' = 15(51 
58 
9 
565 
~) - 5( 
) + 90(~) 
= 
23 
which is the same as the value for the primal problem. 
Tableau 3.20 
3 
2 
5 
0 
0 
-M 
-M 
C B 
X1 
X2 
X3 
X4 
X5 
Yl 
Y2 
X B 
4 
5 
5 
2 
15 
5 
X 3 
0 
0 
1 
2-3 
23 
23 
2-3 
23 
2 
65 
2 
x 2 
0 
1 
0 
~3 
9 
9 
2~ 
23 
9 
17 
17 
120 
3 
x 1 
1 
0 
0 
~ 
23 
23 
7 
23 
0 
0 
0 
51 
58 
M 
58 
9 
565 
23 
23 
~" 
M "~" 
23 
A 

202 
Chapter 3 Further Topics in Linear Programming 
3.3 
EXERCISES 
In Exercises 1 and 2 fill in all the missing entries in the given simplex tableaux. 
CB 
2 
x 2 
X6 
1 
0 
0 
5 
X 1 
X 2 
X 3 
X4 
X 5 
X 6 
X B 
3 
1 
2 
2 
2 
0 
1 
3 
6 
7 
6 
1 
CB 
4 
~ 
~ 
3 
1 
0 
2 
3 
3 
3 
X 1 
X 2 
X3 
X4 
X 5 
X6 
X 7 
X B 
0 
4 
2 
0 
1 
0 
1 
~ 
~ 
4 
1 
2 
1 
0 
1 
1 
0 
~ 
~ 
~ 
10 
1 
1 
1 
0 
0 
1 
4 
1 
~ 
~ 
~ 
In Exercises 3 and 4 we give the original objective function of a linear program- 
ming problem and the final tableau at the end of Phase 1. Find the initial tableau 
for Phase 2 and solve the resulting linear programming problem. 
3. Maximize z = 2x 1 + 3x 2 + 5x 3 q-X 4. 
CB 
Xl 
X2 
X3 
X4 
X5 
X6 
X7 
Yl 
Y2 
0 
X 1 
0 
x 4 
0 
X 6 
1 
Yl 
1 
Y2 
1 
~1 
1 
0 
3 
0 
1 
0 
0 
2"1 
1 
0 
1 
~ 
1 
2 
0 
1 
0 
0 
4 
0 
2 
2 
0 
1 
3 
3 
~ 
1 
0 
0 
0 
0 
3 
2 
0 
3 
0 
0 
1 
0 
0 
2 
0 
4 
1 
0 
2 
0 
2 
0 
1 
0 
0 
1~ 
1 
0 
1 
0 
2 
0 
0 
0 
XB 

3.3 Computational Relations between the Primal and Dual Problems 
~0~ 
4. Maximize z = 3x 1 + x 2 + 3x 3. 
CB 
X1 
X2 
X3 
X4 
X5 
X6 
Yl 
0 
x 1 
0 
x 3 
0 
x 5 
1 
Yl 
2 
0 
2 
1 
1 
0 
2 
0 
0 
1 
1 
1 
0 
1 
0 
5 2 
2 
0 
2 
0 
3 
1 
1 
0 
3 
1 
0 
~ 
0 
5 
0 
2 
1 
0 
3 
1 
0 
2 
0 
0 
0 
~ 
0 
X B 
5. Verify the entries in Tableaux 3.4 and 3.5. 
In Exercises 6-12 solve the given linear programming problem calculating zj 
as described in this section and using the new format for the tableaux. 
6. Maximize z = 2x 1 + x 2 + 3x 3 
subject to 
2x 1 - 
x 2 + 3x 3 < 6 
x 1 + 3x 2 + 5x 3 < 10 
2x 1 
+ 
x 3 < 7 
X 1 >_~ 0, 
X 2 >_~ 0, 
X 3 >__ 0. 
-- 
cj 
7. Maximize z = xl + X 2 -k- X 3 d- X 4 
subject to 
X 1 d'- 2X 2 --X 3 -~- 3X 4 < 12 
x 1 + 3x 2 +x 3 + 2x 4 < 8 
2x 1 -- 3x 2 --x 3 d- 2x 4 < 7 
x 1 >_~ 0, 
x 2 >_~ 0, 
x 3 ~__ 0, 
x 4 >__ 0. 
8. Minimize z = 8x 1 + 6x 2 + llx 3 
subject to 
5X 1 + 
X 2 d- 3X 3 < 4 
5x 1 + 
x 2 q- 3x 3 >__ 2 
2x I + 4x 2 + 7x 3 < 5 
2x I -I- 4x e + 7x 3 >__ 3 
Xl+ 
x2+ 
x3= 
1 
X 1 >__ 0, 
X 2 >_~ 0, 
X 3 >__ 0. 
(Exercise 14 will require the use of the columns corresponding to artificial 
variables.) 

~1~ 
Chapter 3 Further Topics in Linear Programming 
13. 
14. 
15. 
16. 
17. 
18. 
9. Minimize z = 4x 1 -k-x 2 -k-x 3 -k- 3x 4 
subject to 
2x I + 
x 2 + 3x 3 + 
X 4 ~__ 12 
3x I + 2x 2 -I- 4x 3 
-- 5 
2x 1 - 
x 2 -k- 2x 3 + 3x 4 --8 
3x I + 4x 2 + 3x 3 + 
x 4 >__ 16 
X 1 ~_~ O, 
X 2 ~__ O, 
X 3 ~ O, 
X 4 >__ O. 
10. Maximize z = 2x I + x 2 + x 3 + x 4 
subject to 
x I + 2x 2 + x 3 + 2x 4 < 7 
x 1 + 2x 2 + x 3 + 2x 4 >__ 3 
2x 1 + 3x 2 --x 3 --4x 4 < 10 
x I + 
X 2 + x 3 + 
x 4 = 1 
X 1 >' 0, 
X 2 >" 0, 
X 3 >_~ 0, 
X 4 ~_~ 0. 
11. Maximize z = 3Xl + x2 + 3x3 + x4 
subject to 
x I + 
x 2 + 
4x 3 + 
x 4 < 6 
2x 1 
+ 
6x 3 + 
2x 4 >__ 8 
20x 1 + 2x 2 + 47x 3 + llx 4 < 48 
X 1 >_~ 0, 
X 2 ~_~ 0, 
X 3 ~ 0, 
X 4 >__ 0 
12. Maximize z = 2x 1 + x 2 -k- 3x 4 
subject to 
9x 1 + 14x 2 - 6x 3 - 
6x 4 < 2 
xa + 
x 2 - 
x 3- 
x 4 > - 1 
- 20x 1 - 
5x 2 + 5x 3 + 13x 4 = 11 
5x I + 10x 2 - 2x 3 + 14x 4 = 6 
X 1 >_~ 0, 
X 2 ~__ 0, 
X 3 ~__ 0, 
X 4 >_~ 0. 
For each of the linear programming 
problems in Exercises 6 and 9 find the 
matrices B and B-1 from 
each of the tableaux that arose in the solution of the 
problem. 
For each of the primal linear programming 
problems in Exercises 6 and 8 find 
an optimal solution to the dual problem using the final tableau determined 
in 
solving the primal problem. 
Solve Example 5 in Section 1.1. 
Solve Project 2 in Section 1.1. 
Solve Project 3 in Section 1.1. 
Verify the remarks preceding Example 2 regarding Phase 2. 

3.4 The Dual Simplex Method 
205 
3.4 THE DUAL SIMPLEX METHOD 
In modeling applied problems as linear programming problems, it 
frequently becomes necessary to add more constraints to the model. These 
constraints are generally used to make the model more accurately repre- 
sent the real problem, and their need becomes evident when the re- 
searcher compares the solution to the linear programming problem with 
the situation being modeled. However, adding one or more constraints 
may cause the existing solution to become infeasible. In this case the dual 
simplex method, discussed in this section, can be used to restore feasibility 
without having to resolve the entire new problem. 
When we use the simplex algorithm on a primal problem we begin with 
a feasible but nonoptimal solution. Each iteration of the simplex algorithm 
finds a feasible solution that is closer to optimality, and this procedure 
continues until an optimal solution is reached. In the meantime, what is 
happening to the dual problem? Let us examine the sawmill problem in 
this context. 
EXAMPLE 1. 
The primal problem in standard form for the model is 
Maximize 
z = 120x + 100y 
subject to 
2x + 2y < 8 
5x + 3y < 15 
x>O, 
y>O. 
The dual problem is 
Minimize 
z' =8s+ 
15t 
subject to 
2s + 5t > 120 
2s + 3t > 100 
s>O, 
t>O. 
The initial tableau for the primal problem, after adding the necessary slack 
variables, is as follows. 
Tableau 3.21 
CB 
0 
U 
0 
v 
120 
100 
0 
0 
x 
y 
u 
v 
x B 
2 
2 
1 
0 
8 
5 
0 
1 
15 
120 
100 
0 
0 
0 

206 
Chapter 3 Further Topics in Linear Programming 
From this tableau we see that 
[0 
and 
B-l=[ 
1 
O] 
s 
0 
0 
1 ' 
and we may compute from the formula w T - cTB - 1 that 
o]_io ol 
~r 
[S 
t] = [0 
0] 0 
1 
Note that this "solution" to the dual problem satisfies the nonnegativity 
conditions but neither of the constraints. 
We now pivot in Tableau 3.21 and obtain Tableau 3.22. 
Tableau 3.22 
s 
0 
U 
120 
x 
120 
100 
0 
0 
x 
y 
u 
v 
0 
4 
1 
2 
3 
3 
0 
1 
0 
28 
0 
24 
XB 
360 
From this tableau we see that 
s 
[2] 
0 ] 
and 
B- 1 __ 
1 
- 
51 
120 
0 
and that 
[ 
[~ 
1 
~r 
[S 
t] = 
120 
0 
1 
= [0 
24]. 
We now have a "solution" to the dual problem that satisfies the nonnega- 
tivity conditions and also satisfies the first but not the second constraint of 
the dual problem. We pivot again, obtaining Tableau 3.23. 
Tableau 3.23 
s 
100 
y 
120 
x 
120 
100 
0 
0 
x 
y 
u 
v 
x B 
5 
1 
5 
0 
1 
~ 
~ 
3 
1 
3 
1 
0 
~ 
7 
7 
0 
0 
35 
10 
430 
From this tableau we see that 
s 
= [100 
120 ] 
and 
B-a 
5 
3 1] 
1 
" 

3.4 The Dual Simplex Method 
~0~ 
Thus, 
w T= [s 
t] = [35 
10]. 
This is a feasible solution to the dual problem: it satisfies the nonnegativity 
conditions and both of the constraints of the problem. The objective 
function value for the dual problem using this solution is the same as the 
objective function value for the primal problem with the corresponding 
solution. From the Duality Theorem, we have found an optimal solution to 
the dual problem. 
A 
From this example we have seen that if the primal problem has a 
solution that is feasible and nonoptimal, then the solution determined for 
the dual problem is infeasible. As the simplex method progresses, the 
solutions determined for the dual problem are all infeasible until the 
optimal solution is attained for the primal problem. The dual solution 
corresponding to the optimal primal solution is both optimal and feasible. 
The goal for the primal problem when using the simplex method is to 
achieve optimality. The goal for a corresponding method for the dual 
problem is to achieve feasibility, that is, to have both nonnegativity 
constraints and resource constraints satisfied. 
The dual simplex method handles problems for which it is easy to 
obtain an initial basic solution that is infeasible but satisfies the optimality 
criterion. That is, the initial tableau has nonnegative entries in the objec- 
tive row but negative entries in the right-hand column. The following 
example will be used as we present our description of the dual simplex 
algorithm. 
EXAMPLE 
2. 
Consider the following linear programming problem: 
Maximize 
z-- 
-x 1 - 
2x 2 
subject to 
x I - 2x 2 + x 3 > 4 
2x I + 
x 2 -x 3 > 6 
X 1 ~__ 0, 
X 2 ~_~ 0, 
X 3 ~__ 0. 
We change each constraint to an < inequality and then introduce slack 
variables x 4 and x 5. The result is a problem in canonical form: 
Maximize 
z = 
--X 1 -- 2X 2 
subject to 
--x 1 + 
2x 2-x 
3 +x 4 
= 
--4 
- 
2x 1 - 
x 2 + x 3 
+ x 5 = 
-6 
xj>0, 
j= 1,2,...,5. 
The initial tableau for the simplex algorithm is given in Tableau 3.24. It 
has x 4 and x 5 as the initial basic variables. The solution that this tableau 

208 
Chapter 3 Further Topics in Linear Programming 
represents is 
X 1 -- 0, 
Tableau 3.24 
X 2 -- 0, 
X 3 -" 0, 
X 4 = -- 4, 
X 5 = - 6. 
CB 
0 
X 4 
0 
x 5 
1 
2 
0 
0 
X1 
X2 
X 3 
X4 
1 
2 
1 
1 
2 
1 
1 
0 
0 
X5 
XB 
0 
-4 
1 
6 
1 
2 
0 
0 
0 
0 
This solution is not feasible, and here z = 0. The entries in the objective 
row show that the optimality criterion is satisfied. 
A 
The dual simplex method consists of two parts: a feasibility criterion 
that tells us whether the current solution (which satisfies the optimality 
criterion) is feasible, and a procedure for getting a new solution that 
removes some of the infeasibilities of the current solution and conse- 
quently drives the current solution toward a feasible solution. The dual 
simplex method consists of the following steps. 
1. Find an initial basic solution such that all entries in the objective row 
are nonnegative and at least one basic variable has a negative value. 
(Tableau 3.24 represents this step for our example.) 
2. Select a departing variable by examining the basic variables and 
choosing the most negative one. This is the departing variable and the row 
it labels is the pivotal row. 
3. Select an entering variable. This selection depends on the ratios of 
the objective row entries to the corresponding pivotal row entries. The 
ratios are formed only for those entries of the pivotal row that are 
negative. If all entries in the pivotal row are nonnegative, the problem has 
no feasible solution. Among all the ratios (which must all be nonpositive), 
select the maximum ratio. The column for which this ratio occurred is the 
pivotal column and the corresponding variable is the entering variable. In 
case of ties among the ratios, choose one column arbitrarily. 
4. Perform pivoting to obtain a new tableau. The objective row can be 
computed as zj -cj = c~tj- 
cj, where tj is the jth column of the new 
tableau. 
5. The process stops when a basic solution that is feasible (all variables 
> 0) is obtained. 
A flowchart for the dual simplex method is given in Figure 3.4 and a 
structure diagram is given in Figure 3.5. 
EXAMPLE 2 (CONTINUED). Continuing with our example, we perform 
Step 2 of the dual simplex algorithm. We see that x 5 = -6 is the most 

3.4 The Dual Simplex Method 
209 
Input is a tableau which satisfies the optimality criterion. 
.................... I~E ~ ........ 
[ 
Get pivotal row ,, 1 
................. 
............................... 1 ~ 
................................................... 
1 
by pivoting 
NO 
NO 
Indicated 
solution 
is feasible 
There are no ] 
'~ | feasible 
I s~176 
] 
III 
~ 
FIGURE 3.4 Flowchart for the dual simplex algorithm. 
Input is a tableau which satisfies the optimality criterion. 
WHILE negative values for basic variables DO 
Get pivotal row 
~
"
~
 
Neg~tivee'ntries in 
pivotal row 
~ 
.............. 
TRUE 
~ 
FALSE 
.............. 
~ 
~A'~,, ~ 
............ 
Get pivotal column 
.............. 
Compute new 
tableau by 
pivoting 
i 
. . . . . . . . . . . .  
No feasible 
solutions exist 
STOP 
Present tableau represents feasible solution 
............. 
FIGURE 3.5 Structure diagram for the dual simplex algorithm. 

210 
Chapter 3 Further Topics in Linear Programming 
negative basic variable, so that x 5 is the departing variable. The ratios of 
the entries in the objective row to corresponding negative entries in the 
pivotal row are 
1 
Column 1" 
2 
Column 2" 
- 2. 
The maximum ratio is 
-- 2,1 so that Xl is the entering variable. We repeat 
Tableau 3.24 with the entering and departing variables and the pivot 
labeled (Tableau 3.24a). 
Tableau 3.24a 
CB 
1 
2 
0 
0 
0 
x1 
x 2 
x 3 
x 4 
x 5 
0 
x 4 
1 
2 
1 
1 
0 
x5 
(-2") 
1 
1 
0 
1 
2 
0 
0 
x B 
0 
4 
1 
6 
0 
0 
We now perform a pivotal elimination to get Tableau 3.25. The basic 
solution given by Tableau 3.25 is 
X 1 -- 3, 
X 2 ~-- 0, 
X 3 = 
0, 
X 4 = --1, 
X 5 = 0. 
This solution is still optimal (the objective row has nonnegative entries) 
but is infeasible. However, it is less infeasible in that only one variable is 
negative. 
Tableau 3.25 
C B 
0 
X 4 
1 
x 1 
1 
2 
0 
0 
0 
X1 
X 2 
X 3 
X 4 
X 5 
X B 
0 
5 
~-2) 
1 
~ 
1 
1 
1 
1 
0 
1 
1 
~- 
2 
~ 
3 
0 
3 
! 
0 
1 
2 
2 
2 
For the next iteration of the dual simplex algorithm, X 4 is the departing 
variable, since it is the only negative basic variable. Forming the ratios of 
the entries in the objective row to the corresponding negative entries of 
the pivotal row, we have 
1 
3 
1 
Column 3" 
~/ 
2 = 
- 
Column 5" 
1 
1 
~/-~ 
= 
-1. 

3.4 The Dual Simplex Method 
211 
1 
is the entering variable. Pivoting, we 
The maximum ratio is - 7, so that x 3 
now obtain Tableau 3.26. 
Tableau 3.26 
CB 
0 
X 3 
1 
x 1 
1 
-2 
0 
0 
0 
X1 
X2 
X 3 
X4 
X 5 
0 
s 
1 
2 
1_ 
3 
3 
3 
1 
1 
1 
1 
~ 
0 
i 
i 
0 
7 
0 
~ 
! 
3 
3 
3 
X B 
2 
The basic solution represented by Tableau 3.26 is 
10 
2 
X 1 -- -~-, 
X 2 -- 0, 
X 3 -- ~, 
X 4 -- 0, 
X 5 -- 0. 
This solution satisfies the optimality criterion, and it is feasible since all 
the variables have nonnegative values. 
A 
While using the dual simplex method in Example 2, we were fortunate 
in finding an initial basic solution to the given problem that satisfied the 
optimality criterion but was not feasible. In general, it is difficult to find 
such a starting point for the dual simplex method. However, the principal 
use of this method is to restore feasibility when additional constraints are 
included in a linear programming problem whose solution is known. The 
following example illustrates this situation. 
EXAMPLE 3. 
There are three typical kinds of dog food that Paws eats 
each day: dry food, canned wet food, and dog biscuits. The nutritional 
analysis of his favorite brands is given in the following table in percent by 
weight. 
Fat 
Protein 
Fiber 
Moisture 
Cost (r 
Dry food 
8.0 
14.0 
5.5 
12.0 
4.1 
Wet food 
6.0 
9.0 
1.5 
78.0 
2.5 
Biscuit 
8.0 
21.0 
4.5 
12.0 
7.3 
The veterinarian has suggested that Paws get at least 5 oz of protein and at 
most 1 oz of fiber each day. His owner has set up the following linear 
programming problem to model the dietary requirements and minimize 
the cost, where x I is the amount of dry food measured in ounces offered 

212 
Chapter 3 Further Topics in Linear Programming 
to Paws. Similarly x 2 denotes the amount of wet food and x 3 denotes the 
amount of biscuits. 
Minimize 
z = 4.1x 1 + 2.5x 2 + 7.3x 3 
subject to 
0.14x 1 + 
0.09x 2 + 
0.21x 3 > 5.0 
0.055X 1 '[- 0.015x 2 + 0.045x 3 < 1.0 
xj>0, 
j= 
1,2,3 
We first transform this problem to standard form and then to canonical 
form introducing the slack variables x 4 and x 5" 
Maximize 
z = 
--4.1x I -- 2.5x 2 -- 7.3x 3 
subject to 
-0.14x 1 - 
0.09x 2 - 
0.21x 3 +X 4 
=--5.0 
0.055X 1 "[- 0.015x 2 + 0.045x 3 
+ x 5 = 1.0 
xj>0, 
j= 
1,2,...,5. 
Solving this linear programming problem using the dual simplex method 
we obtain (verify) Tableau 3.27. 
Tableau 3.27 
CB 
2.5 
x 2 
0 
x 5 
4.1 
2.5 
7.3 
0 
0 
x1 
x 2 
x 3 
x4 
x 5 
1.556 
1 
2.333 
11.111 
0 
0.032 
0 
0.010 
0.167 
1 
0.211 
0 
1.467 
27.778 
0 
x B 
55.556 
0.167 
138.889 
From this tableau we see that the minimum-cost diet for Paws consists of 
55.556 ounces of wet food per day (no dry food and no biscuits, poor Paws) 
at a cost of $1.39 per day. 
At the most recent visit to the veterinarian, she also suggested that 
Paws' fat intake be limited to 2.5 oz per day. The new constraint, 
0.08X 1 -[- 0.06X 2 + 0.08X 3 ~ 2.5, 
expresses this limitation. Introducing the slack variable x 6 the new con- 
straint becomes 
0.08X 1 q- 0.06x 2 + 0.08x 3 q-x 6 -- 2.5. 
We now include this constraint into Tableau 3.27 and form Tableau 3.28. 
Observe that the new slack variable x 6 does not appear in the other two 
constraints nor in the objective function, so that its coefficient in these two 
constraints and in the objective row will be zero. 

3.4 The Dual Simplex Method 
213 
Tableau 3.28 
CB 
- 2.5 
x 2 
0 
x 5 
0 
x 6 
4.1 
2.5 
7.3 
0 
0 
0 
X1 
X 2 
X3 
X4 
X 5 
X6 
..... 
1.556 
1 
2.333 
11.111 
0 
0 
0.032 
0 
0.010 
0.167 
1 
0 
0.08 
0.06 
0.08 
0 
0 
1 
. 
XB 
55.556 
0.167 
2.5 
0.211 
0 
1.467 
27.778 
0 
0 
138.889 
Moreover, the new slack variable will be a basic variable. Since x 2 is a 
basic variable, all the entries in the column labeled by x 2 must be zero 
except for the entry in the row labeled by x 2. Hence, we add (- 0.06) times 
the first row to the third row and obtain Tableau 3.29. 
Tableau 3.29 
CB 
2.5 
x 2 
0 
x 5 
0 
x 6 
4.1 
- 2.5 
7.3 
0 
0 
0 
Xl 
X2 
X3 
X4 
X 5 
X 6 
.... 
1.556 
1 
2.333 
11.111 
0 
0 
0.032 
0 
0.010 
0.167 
1 
0 
0.013 
0 
0.06 
0.667 
0 
1 
0.211 
0 
1.467 
27.778 
0 
0 
X B 
55.556 
0.167 
-0.833 
138.889 
This tableau represents an infeasible solution that satisfies the optimality 
criterion. 
Using the dual simplex method, we may restore feasibility. We see that 
x 6 is the departing variable and that x 1 is the entering variable for this 
step. Completing several iterations of the dual simplex method we obtain 
Tableau 3.30 (verify). 
Tableau 3.30 
CB 
2.5 x 2 
4.1 x 1 
7.3 x 3 
4.1 
2.5 
7.3 
0 
0 
0 
X1 
X2 
X3 
X4 
X 5 
X 6 
0 
1 
0 
5.031 
35.220 
33.019 
1 
0 
0 
9.434 
33.962 
5.660 
X B 
22.170 
0.943 
0 
0 
1 
13.208 
7.547 
17.925 
13.679 
0 
0 
0 
45.157 
3.899 
25.094 
159.151 
_. 
We conclude that after his fat intake was restricted, the minimum-cost 
diet for Paws is 0.943 oz of dry food, 22.17 oz of wet food, and 13.679 oz of 
dog biscuits per day at a total cost of $1.59. 
A 
Much of our discussion of the simplex method centered on finding an 
initial basic feasible solution to a linear programming problem in arbitrary 

214 
Chapter 3 Further Topics in Linear Programming 
form. We developed the concept of artificial variables to provide a method 
for constructing a starting point for such a problem. The situation with the 
dual simplex method is different. In this book we will use the dual simplex 
method to restore feasibility in a tableau that represents a solution that is 
infeasible but satisfies the optimality criterion. Thus, we will not need any 
procedures for finding an initial basic feasible solution when using the dual 
simplex method. 
3.4 
EXERCISES 
In Exercises 1-5 the given tableau represents a solution to a linear programming 
problem that satisfies 
the optimality criterion, but is infeasible. 
Use the dual 
simplex method to restore feasibility. 
CB 
5 
X 1 
6 
x 2 
0 
x 5 
5 
6 
0 
0 
0 
X 1 
X2 
X3 
X4 
X5 
1 
1 
1 
0 
~ 
~ 
0 
0 
1 
12 
5 
0 
12 
1 
7 
0 
0 
~ 
g 
1 
z 
15 
0 
0 
0 
~ 
8 
X B 
161 
CB 
Xl 
X2 
X3 
X 6 
5 
6 
0 
0 
0 
0 
X1 
X2 
X3 
X4 
X 5 
X 6 
XB 
1 
0 
0 
1 
1 
0 
4 
2 
0 
1 
0 
1 
~ 
0 
0 
0 
1 
7 
-8 
0 
2 
1 
1 
0 
0 
0 
0 
~ 
1 
0 
0 
0 
1 
1 
0 
40 
C B 
X3 
X5 
X2 
X7 
4 
5 
3 
0 
0 
0 
0 
X 1 
X2 
X3 
X4 
X5 
X 6 
X7 
X B 
3_ 
0 
1 
1 
0 
0 
0 
5_ 
4 
4 
2 
11 
0 
0 
?6 
1 
1 
16 
~" 
0 
8 
9 
1 
0 
16 
0 
1 
0 
19 
16 
4 
8 
1 
1 
1 
0 
0 
~ 
0 
0 
1 
17 
0 
0 
7 
0 
5 
155 
16 
~" 
0 
a 

3.5 The Revised Simplex Method 
215 
C B 
X1 
X 2 
X 3 
X 6 
5 
6 
0 
0 
0 
0 
X1 
X 2 
X3 
X4 
X5 
X 6 
X B 
..... 
1 
0 
0 
0 
1 
0 
4 
1 
2 
10 
0 
1 
0 
~ 
~ 
0 
3 
0 
0 
1 
1 
8 
0 
2 
1 
2 
1 
1 
0 
0 
0 
-~ 
~ 
0 
0 
0 
2 
1 
0 
40 
, 
CB 
0 
X 3 
7 
x 1 
3 
x 2 
0 
X 6 
7 
3 
0 
0 
0 
0 
X1 
X 2 
X3 
X4 
X5 
X 6 
X B 
. 
1 
17 
0 
19 
0 
0 
1 
a 
4 
2 
1 
3 
1 
1 
0 
0 
~ 
~ 
0 
0 
1 
0 
0 
1 
0 
2 
1 
3 
1 
23 
0 
0 
0 
~ 
s 
4 
.
.
.
.
.
.
.
.
.
.
 
0 
0 
0 
Z 
~ 
0 
31 
8 
8 
4 
6. Use the dual simplex method to verify that Tableau 3.27 is correct, 
7. For Example 3, verify using the dual simplex method that the final tableau 
(Tableau 3.30) is correct. Note that your answer may differ slightly from the 
text due to round-off error. 
8. Use the dual simplex method to find a solution to the linear programming 
problem formed by adding the constraint 
3x I + 5x 3 > 15 
to the problem in Example 2. 
9. Example 3 showed that adding a constraint may change the solution to a linear 
programming problem (i.e., the new solution has different basic variables and 
the basic variables have different values). There are two other possibilities that 
may occur when a constraint is added. Describe them. 
Computing project. Compare the structure 
diagrams for the simplex 
algorithm and the dual simplex algorithm. How is duality exemplified by these 
diagrams? 
10. 
3.5 THE REVISED SIMPLEX METHOD 
The revised simplex method makes use of some of the notation and 
ideas we developed in Section 3.3 to obtain efficiency in computation and 
storage of intermediate results. The simplex method, as we described it, 

~1 ~ 
Chapter 3 Further Topics in Linear Programming 
performs elementary row operations on an m • (n + m) matrix for a 
problem in standard form with m constraints and n variables. The revised 
simplex method works with the much smaller m • m matrix. The savings 
in computation time and storage of arrays can be considerable for large 
problems (n > 1000) typically found in applications. Consequently the 
computer programs for solving linear programming problems, called LP 
codes, always use the revised simplex method. 
We consider a linear programming problem in canonical form 
Maximize 
z = cTx 
subject to 
(1) 
Ax=b 
x~_O. 
In this section we confine our attention to the case in which the canonical 
form above was obtained using only slack variables, not artificial variables. 
A more general procedure for dealing with artificial variables is available, 
but will not be discussed in this book. 
For each tableau in the simplex algorithm we defined the matrix 
B = 
[Ai 1 
Ai 2 ""Aim] , 
where Ai, is the i r column of the constraint matrix A and i r is the index of 
the rth basic variable in the list at the left side of the tableau. The values 
of the basic variables xi~, xi2,..., Xim were represented by the vector 
Xi 
XB --" IXi  
and we showed that 
x B =B-Xb 
or 
b=Bx 
B. 
(2) 
We also defined 
Ci 
CB --- I ci~ 
LC m 
so that 
z-e~xB 
or z-e~xB-0. 
(3) 

3.5 The Revised Simplex Method 
~1 
We can combine equations (2) and (3) into one matrix equation by writing 
[1 c ][z 
0 
B 
XB 
0] 
(4) 
b " 
The coefficient matrix in (4) is (m + 1) x (m + 1), and the vectors are 
(m + 1) x 1. We denote the coefficient matrix in (4) by M; that is, 
M 
___ 
1 
-c~ 
0 
B 
By multiplying the matrices together, the reader may verify that 
o1:[10 c B1]B1 
Hence, the solution represented by any tableau can be succinctly stated as 
Z 
-1 0] 
= 
1 
r 
0 
B -1 
b 
cTB-lb]. 
B-lb 
(5) 
The revised simplex method exploits the form of the solution in (5) by 
working only with the matrix M-1 instead of the entire tableau. In fact, 
because M-1 has a particularly simple form, we need consider only B -1. 
Now the initial tableau is constructed so that B = I m, where I m denotes 
the m x m identity matrix. Thus, initially, B-1 
.
_
 in" The revised simplex 
method uses a procedure to find the B -1 for the next iteration using 
information about the entering and departing variables along with the 
current B-1. We start by writing 
(B -~)new = EB -1, 
(6) 
where E is an m x m matrix that can be obtained as follows. 
(a) Start with I m. Suppose Xp is the entering variable and that Xiq is the 
departing variable. We have previously shown that tp, the pth column of 
the current simplex tableau, which is the pivotal column, is given by 
tp = B-lAp, 

~18 
Chapter 3 Further Topics in Linear Programming 
where Ap is the pth column of the original matrix A. Denote the entries of 
the pivotal column of the current tableau by 
tp 
Itlp 
= l t2p " 
(b) Replace the qth column of I m by the vector 
--tlp/tqp 
--t2p/tqp 
1/tqp 
--tmp/tqp 
qth entry, 
called an eta 
vector. 
This modification of the identity matrix that we have constructed is 
called an eta matrix and is the matrix E that we wanted. Notice that we 
never have to numerically invert a matrix (a procedure that may require 
some care); we simply obtain a sequence of matrices that are B-1 for each 
of our tableaux. 
The revised simplex method consists of the following steps: 
1. Determine the entering variable Xp by choosing the most negative 
z: - cj, j = 1, 2,..., 
s. Pick randomly if there are ties. Recall that z: - c: 
may be computed as 
zj - cj = 
c~tj 
- 
cj = 
c~B-1Aj 
- 
cj9 
In terms of a matrix product, we may write 
[1 cTB_1 ] [ -CJA:]. 
2. Determine the departing variable Xiq9 This is the variable with the 
minimum 0-ratio. The irth basic variable has a 0-ratio 
Xir//tr p , 

3.5 The Revised Simplex Method 
~1 
where Xir is the entry in X B on the right-hand side of the tableau and 
where trp > 0. To find the 0-ratios, we may compute 
tip 
tp --- 
tEp. 
= B- lap 
trap 
and 
x B = B-lb. 
We use only those entries in x B that correspond to positive entries in tp to 
form the set of 0-ratios. 
3. Determine the new B-1 as described above. 
4. Determine the new basic feasible solution and objective function 
value. From Equations (5) and (6), (XB)ne w --(B-1)new b -- EB-lb = Ex B. 
Thus, if x B is available in storage, then 
(XB)ne w -" Ex B. 
This formula is computationally faster than Equation (5), since E is 
sparse (has many zeros). 
As in the simplex method, if none of the zj - cj is negative, an optimal 
solution has been achieved. If none of the entries in the pivotal column t p 
is positive, the optimal solution is unbounded. Observe that in using the 
revised simplex method, no tableau need be computed. 
EXAMPLE 1. 
Consider the linear programming problem in canonical 
form that came from the model for the sawmill: 
Maximize 
z = 
120x I + 
100x 2 
subject to 
2x 1 + 2x 2 + x 3 
--8 
5x 1 -~- 3x 2 
+ x 4 -- 15 
xj>0 
j= 1,2,3,4. 
For this problem 
A 
.__ 
2 
1 o] 
b 
181 
and 
0 
1 ' 
15 ' 
C -- 
120 
100 
0 
" 
0 

~0 
Chapter 3 Further Topics in Linear Programming 
The slack variables x 3 and x 4 are our initial basic variables, so that i 1 
and i 2 --4. 
Consequently, 
[1 
0] 
B_I= [1 
0 
and 
CB= 
0] 
B= 
0 
1 
' 
0 
1 
' 
0 
" 
Therefore, 
c .l 
M1 [1 c .l] [i ~ ~ ] 
- 
= 
= 
= 
1 
0 
, 
0 
B -1 
0 
1 
so that 
[1 o o][ o] [ 
XB 
b 
0 
0 
1 
15 
We first determine the entering variable by computing 
- 
Aj 
--[1 
0 
0] 
Therefore, 
0] 
8. 
15 
:- C j* 9 
=3 
(7) 
E 
[2] 
1 
0 
Therefore, 
Z 1 -- C 1 = 
--C 1 = 
-- 120 
Z 2 -- C 2 = 
--C 2 = 
-- 100 
Z 3 --C 3 ~-- --C 3 -- 0 
Z 4 -- C 4 = 
--C 4 -" 0, 
and Xl (P = 1) is the entering variable. 
To find the departing variable, we form 
[ 
2 
2 
1 
0][5]=[5] 
tl 
= 
B-1A1 
-- [0 
1 
and copy x B from (7). All the entries of t 1 are positive, so we compute 
8 
min{~, ~} = 3. 
The minimum 0-ratio occurs for xi2 = x 4 (q = 2) and, thus, x 4 becomes 
the departing variable. To compute the eta matrix E, we replace the 
second column of 12 with the eta vector 
--g 
1 
~ 
g 

3.5 The Revised Simplex Method 
~1 
and 
(B-1)new = EB -1 = 
1 
-7 
1 
1 
0 
0 
Now we have i I = 3 and i 2 = 1, so that 
0 
CB = [120 ] 
and 
M 
--1 
Therefore, 
and the current solution is 
[1o 
e~B -1=[0 
241. 
I 
1 
1 
0 
24 
2 
0 
1 
-7, 
1 
0 
0 
~ 
2] 
--7 
1 
" 
I!o 241[0] [360] 
[ ] 
[] 
z 
=M-1 
0 
1 
-7 
8 
= 
2 9 
1 
15 
3 
XB 
b 
0 
Next we compute 
so that 
z, - 
cj = [1 c .-11 [ _cj] 
Aj 
=[1 
--cj ] 
24] 
Aj ' 
- 120] 
z 1 -- 
C 1 = 
[ 1 
0 
24] 
2 
= 0 
5 
z 2-c 2- [1 
0 
24] 
2 
= -28 
3 
[o] 
z 3-c 3=[1 
0 
24] 1 
=0 
0 
z 4-c 
4= [1 
0 
24] 
=24. 
(8) 
The entering variable is x 2 (p = 2). To find the departing variable, we 
compute 
_ 
1 
-~ 
2 
1 
3 
3 
t 2 
B 1A2 
0 
~ 

~ 
Chapter 3 Further Topics in Linear Programming 
and copy x B from (8). All the entries of t 2 are positive, so that we compute 
min 
7, 
-5- 
=-i 
=-. 
3 
~ 
2 
The minimum 0-ratio occurs for Xil = x 3 (q = 1) and, thus, x 3 becomes 
the departing variable. We compute the eta matrix E by replacing the first 
column of 12 by the eta vector 
We obtain 
and 
[][5] 
1/~ 
3 
4 
3 
" 
-~/~ 
-~ 
[5] 
E= 
7 
0 
3 
-7 
1 
[5 ][ 2][5 
(B-1 
)new 
-" 
7 
0 
1 
-- ~ 
= 
7 
3 
1 
3 1] 
1 
" 
Now we have i~ = 2 and i 2 = 1, so that 
[100] 
and 
cTB-1=[35 
Ca -- 
120 
Therefore, 
M 
- - 1  
_ _  I 
1 
1 
35 
10 
0 
5 
1 
4 
2 
0 
3 
1 
4 
10]. 
and the current solution is 355101[0 
]1 rt43011 
7 
~ 
8 
= 
- 
. 
3 
1 
15 
4 
(9) 
Next we compute 
We have 
z j-c j=[1 
35 
10][ -cj 
Aj]" 
Z 1 -- 
C 1 -- 
0 
z2 - c2 = 0 
Z 3 -- C 3 -- 35 
Z 4 -- C 4 -- 
10. 
Since zj - cj >_ 0 for all j, the solution given by (9) is optimal. 
A 

3.5 The Revised Simplex Method 
223 
3.5 
EXERCISES 
In Exercises 1 and 2 calculate the eta vector and then the eta matrix E from the 
given information. 
1. The pivotal column tp is 
i11 
-2 
tp= 
0 
3 
and the departing variable labels the fourth row (q = 4). 
2. The pivotal column tp is 
1 
3 
tp 
--~ 
-ig 
and the departing variable labels the second row (q - 2). 
In Exercises 3 and 4, find the new B-1 from the given information. 
3. The current B- 1 is 
1 
0 
2] 
B -1 -- 
-1 
1 
3 
0 
2 
1 
and the pivotal column is given by 
3 
tp = 
2 
and 
x b 
4. The current B- 1 is 
I 
1 
- 1 
2 
1-1 
ll_l= 
0 
1 
0 
1 
-1 
3 
1 
-3 
2 
1 
2 
4 
and the pivotal column is given by 
0 
4 
tp = 
--2 
2 
and 
xb= Iil 

~ 
Chapter 3 Further Topics in Linear Programming 
In Exercises 5-9 solve the given linear programming problem using the revised 
simplex method. 
5. Exercise 6, Section 3.3 
z =x I + 2x 2 + 3x 3 +x 4 
6. Maximize 
subject to 
2x 1 + 
X 2 + 
X 3 + 2X 4 < 18 
3X 1 + 5x 2 + 2x 3 + 3x 4 < 24 
3x I + 2x 2 + 
X 3 + 
X 4 __< 12 
Xj>_O, 
j= 
1,2,3,4. 
7. Maximize 
subject to 
z = 2X 1 + 3X 2 + X 3 + X 4 + 2X 5 
2x I + 
X 2 -- 
X 1 + 4x 2 
3x I 
xj>_O, 
3x 3 + 
x 4 + 
x 5 __< 10 
+ 
X 4 + 2x 5 < 20 
+ 4x 4 -t- 2x 5 < 15 
j-l,2 
..... 5. 
8. Maximize 
subject to 
z = 3x I + 2x 2 + 4x 5 + X 6 + 2x 8 
3x 1 + 
X 2 + 
X 3 + 
X 4 + 2X 5 + 3X 6 
+ 
X 8 < 12 
2X 1 + 
X 2 
+ 2x 4 
+ 5x 6 + x 7 + 2x 8 _< 15 
3x 1 + 2x 2 + X 3 
+ 3x 5 
+ X 7 + 3x 8 < 18 
xj>O, 
j= 
1,2 ..... 8. 
9. Maximize 
subject to 
z = 2x 1 + X 2 + 3x 3 + X 6 + 2x 7 + 3x 8 
2x I + 
x 2 
+ 
x 4 + 3x 5 
+ x 7 
__< 24 
X 1 
+ 3x 3 + 
x 4 + 
x 5 + 2x 6 
+ 3x 8 < 30 
5X 1 + 3x 2 
+ 3x 4 + 2x 5 
+ X 7 + 5X 8 _~< 18 
3x I + 2x 2 + 
X 3 
+ 
X 6 
+ 3x 8 _< 20 
xj>_O, j= 1,2 ..... 8. 
10. If 
verify that 
M 
__ 
M-l= 
1 
0 
B 
' 
1 eTB-1]. 
0 
B -1 

3.6 SensitivityAnalysis 
225 
11. Consider the standard linear programming problem 
Maximize 
z = eTx 
subject to 
Ax<b 
x>__O. 
(a) Show that this problem can be written in canonical form as 
Maximize 
z = eTx + (e')Tx ' 
subject to 
[xl 
[A ', 11 x' 
=b 
ix] 
x' 
> O. 
(Hint." x' will be a vector of slack variables.) 
(b) Show that the initial simplex tableau represents the matrix equation 
[10 
--eTA --(c')T] [ Z 
]
I
 _Xx, 
= [0]b 
(Hint: Proceed as in the derivation of Equation (4).) 
(c) Equation (5) shows that the solution represented by any tableau is ob- 
tained by multiplying the vector 
[01 
by M-1. Show that the system of equations represented by any tableau is 
[1 cTB-1A- c T cTBB-1- (c')T] [ z ] [cTB- lb] 
X 
~ 
0 
B-1A 
B -1 
x' 
B-lb 
(d) Show from part (c) that, at an optimal solution to the problem in part (a), 
cTB - 1A >___ C T and c~B-1 > (e,)T. 
12. (a) Find the dual of the linear programming'problem in Exercise l l(a). 
(b) Show that w = (B-1)TeB is a feasible solution to the dual problem. (Hint: 
Use Exercise 1 ld.) 
(c) Using Exercise 9 in Section 3.3, explain why w = (B-1)Te B is an optimal 
Jsolution to the dual problem. 
13. Computing project. Construct a flowchart or structure diagram for the revised 
simplex method. 
3.6 SENSITIVITY ANALYSIS 
In the Prologue it was pointed out that solving a linear programming 
problem is just one part of mathematically modeling a situation. After the 

~ 
Chapter 3 Further Topics in Linear Programming 
problem is solved, one must ask whether the solution makes sense in the 
actual situation. It is also very likely that the numbers that are used for the 
linear programming problem are not known exactly. In most cases they will 
be estimates of the true numbers, and many times they will not be very 
good estimates. Consequently, it is desirable to have ways of measuring the 
sensitivity of the solution to changes in the values that specify the problem. 
Of course, one way to proceed would be to recompute the solution using 
different values. However, the tableau representing an optimal solution 
contains the information we need to measure the sensitivity. Using the 
final tableau makes it unnecessary to repeat the calculations for a different 
set of values. 
There are five things that can be singled out as subject to variation in 
defining a linear programming problem. 
1. One or more of the objective function coefficients can change. 
2. One or more of the resource values (components of b) can change. 
3. One or more of the entries in the constraint matrix A can change. 
4. A variable might need to be added. This may happen if management 
wants information about the effects of making an additional product. 
5. Addition of a constraint might be necessary, especially if the solution 
to the original problem is somewhat unreasonable. This situation also 
occurs when some of the variables are constrained to take integer values. 
In this section we examine simple cases of the first two possibilities, 
namely, when only one quantity is allowed to change. In Chapter 4 we will 
discuss the fifth case. Cases 3 and 4 and the situation in Cases 1 and 2 in 
which more than one quantity changes simultaneously are discussed in 
more advanced texts (Further Reading). 
Another approach to this study is to assume a change in each entry of 
the objective function coefficient vector, for example, and to assume that 
the amount of the change depends on a parameter. This leads to the study 
of parametric programming. We do not study this topic here but refer the 
interested reader to Further Reading. 
We assume that the original problem has been converted to the form 
Maximize 
z 
-
-
 cTx 
subject to 
A_x=b 
x>0 
and that an optimal solution to the problem has been obtained. We further 
assume that we have available the final tableau for the simplex method. 
As changes are made in the problem statement, there are several things 
that may happen to the old optimal solution. It may remain both feasible 
and optimal, so that no further calculations are necessary. In fact, some 
computer codes for linear programming problems will automatically com- 

3.6 Sensitivity Analysis 
227 
pute a range of values for b and c in which the solution found will remain 
optimal. The solution to the problem may remain feasible but become 
nonoptimal. In this case a few iterations of the simplex algorithm will 
restore the optimality. The optimal solution to the original problem, being 
a basic feasible solution, provides an initial basic feasible solution for these 
iterations. On the other hand, the solution to the given problem may 
remain optimal, as judged by the optimality criterion, but may become 
infeasible. In this case a few iterations of the dual simplex algorithm will 
usually restore the feasibility. We now examine Cases 1 and 2. 
Change in the Objective Function 
Suppose c k changes to t? k = c k + 
Ac k. The old optimal solution must 
remain feasible, since neither A nor b was changed. The optimality 
criterion is stated in terms of 
Zj -- Cj : cTtj -- cj, 
where tj is the jth column of the final tableau (see Section 3.3). If k is the 
index of one of the basic variables, then c B changes and every zj - cj must 
be recomputed. If X k is a nonbasic variable, e a is unchanged and only 
z k - 
c k changes. In this latter case we have 
Therefore, 
if and only if 
zk - 6k = (z~ -- c~) -- Ack. 
Zk -- (~k ~-~ 0 
z k - 
c k > Ac k. 
(1) 
That is, the profit coefficient of the kth variable, c k, can be increased by as 
much as z k - c k and the solution will remain optimal. However, making 
this increase in Ck will not change the value of the objective function, since 
x k = 0 in the optimal solution. 
Now suppose that x k is a basic variable in the optimal solution. Suppose 
k = ir, SO that the new value of c B is 
^ 
CB 
Ci 1 
Ci 2 
% 
+ ACir 
Ci m 
-~ C B d- ACir 
0 
0 
1 
0 
rth entry. 

~l] 
Chapter 3 Further Topics in Linear Programming 
Let e r denote the m • 1 matrix which is all zeros except for a 1 in the rth 
row. Then we may write 
CB = CB --I- ACire r. 
Now for all values of j except j 
= 
ir, we have 
2y - Cy = ~Bty -- Cy = c~ty + AC~reTty - cj = Zy - cy + try Ac~ r. 
(2) 
The reader may show, using a similar argument, that 
~,ir- fir ~" O. 
Recall that basic variables are determined only by the constraints, not by 
the objective function. Furthermore, for each basic variable x~k we must 
have Zik -- Cik "-- 0. This follows from (2), since tri k -- 0 when k ~ r. Conse- 
quently, the old optimal solution remains optimal when the objective 
function coefficient of a basic variable is changed if and only if for all 
nonbasic variables xj, 
zj - cj + try Aci, >_ 0 
or 
If trj 
divide each side of (3) by --try, reversing the inequality, to obtain 
zj - cj > -trj Aci . 
(3) 
= 0, the inequality in (3) holds for all changes Aci. If try > 0, we can 
ACir > 
Zj -- Cj 
_ 
- 
~ 
(4) 
tq 
for those j for which xj is nonbasic and try > 0. If try < 0, we again divide 
both sides of (3) by --try, but do not reverse the inequality, and obtain 
Zj -- Cj 
mCir ~_~ - - ~  
(5) 
try 
for those j for which xj is nonbasic and trj < 0. Combining (4) and (5), we 
find that the old optimal solution remains optimal if the change in c i, 
satisfies 
max (_ 
trj- cy 
( 
Zj -- Cj 
trj > 0 
<_ A C ir ~ min 
-- 
J 
try 
trj <0), 
(6) 
where the index j runs over all nonbasic variables. If there are no j such 
that try > 0, then the left side of (6) gives no restriction on Aci,. It may 
take on arbitrarily large negative values. Likewise, if there are no j such 
that trj < 0, the right side of (6) gives no restriction on Aci. 

3.6 Sensitivity Analysis 
229 
If ACk does not satisfy the inequality in (1) when Xk is a nonbasic 
variable or if Ac k does not satisfy the inequalities in (6)when x k is a basic 
variable, the solution represented by the final tableau is no longer optimal. 
Some iterations of the simplex method will restore optimality. 
EXAMPLE 1. 
2 in Section 2.3: 
Consider the linear programming problem from Example 
Maximize 
subject to 
x 1 + 2x 2 + 2x 3 +x 4 + 
x 5 
x 1 + 2x 2 + 
x 3 d-x 4 + 2x 5 + x 6 
3x I + 6x 2 + 2x 3 +x 4 + 3x 5 
Z -- X 1 -- 2X 2 -- 3X 3 --X 4 -- X 5 -~- 2X 6 
= 
12 
=18 
= 
24 
x j>0, 
j = 1,2,...,6. 
An optimal solution is given by Tableau 3.31. 
Since x 2, x4, and x 5 are nonbasic variables, we use (1) and see that c2 
can be increased by 4 = z z - c2, c 4 can be increased by ~ = z 4 - c a, or c 5 
can be increased by 4 = z 5 -c 5 without changing the optimality of the 
solution. 
Tableau 3.31 
CB 
3 
x 3 
2 
x 6 
1 
x 1 
1 
2 
3 
1 
1 
2 
x1 
x 2 
x 3 
x4 
x 5 
x 6 
1 
0 
0 
0 
0 
1 
1 
1 
1 
0 
0 
0 
~- 
1 
2 
0 
0 
1 
0 
1 
4 
0 
0 
4 
0 
x B 
3 
9 
6 
15 
We now examine the objective function coefficients of the basic vari- 
ables xl, x3, and x 6. To determine the range for a change in c 1, we 
compute 
and 
cJ I 
) 
44 
-- 
t3j > 0 
= max{ 
~, 
i} = --2 
t3j 
Zj -- Cj 
min 
- 
t3j 
t3j < 01 9 

230 
Chapter 3 Further Topics in Linear Programming 
Since there are no j for which t3j < 0, there is no upper bound for the 
change in Cl. That is, if 
-2 
< Ac I < oo, 
the current solution remains optimal. 
To determine the range for a change in c a for which the solution 
remains optimal, we compute 
max 
zj - cj 
- ~ 
- 
tij>O 
=max 
-7-- 
= -1. 
tlj 
Again there are no values of j for which tlj < 0, so that the change in c 3 is 
not bounded above. The solution remains optimal if 
-1 
< mc 3 < m. 
Finally, checking the range for a change in c 6, we see that the solution 
remains optimal if 
-1 < Ac 6 < m. 
Summarizing our results, the optimal solution 
x 1 =6, 
x 2=0, 
x 3=3, 
x4=0, 
x s =0, 
x 6=9 
remains optimal for a change, Ac k, in the kth coefficient of the objective 
function if A c k satisfies the appropriate inequality listed in Table 3.4. This 
assumes that only one coefficient is changed. 
TABLE 3.4 
k 
Ac k 
1 
-2 <_ Ac I ( 
oo 
2 
-oo < Ac2 < 4 
3 
-1 < Ac 3 < oo 
k 
AC k 
1 
4 
-m 
< 
Ac 4 ~_~ 
5 
-oo < Ac 5 <4 
6 
-1 <Ac6 <oo 
A 
EXAMPLE 2. 
Section 1.2) 
The canonical form of the sawmill problem (Example 3, 
Maximize 
z = 120x 1 + lOOx 2 
subject to 
2x I + 2x 2 + x 3 
-- 
8 
5X 1 + 3X 2 
+ X 4 -- 15 
X 1 ~_~ 0, 
X 2 ~__ 0, 
has a final tableau as shown in Tableau 3.32. 

3.6 Sensitivity Analysis 
231 
Tableau 3.32 
CB 
100 
x 2 
120 
x 1 
120 
100 
0 
0 
X1 
X2 
X3 
X4 
5 
1 
5 
0 
1 
~ 
~ 
7 
3 
1 
3 
1 
0 
~ 
~ 
7 
0 
0 
35 
10 
430 
X B 
The owner of the sawmill would like to know how much it is possible to 
increase the price (and hence the profit) on each of the outputs individu- 
ally and still make the same amount of each type of board. 
The change in c 1 that does not affect optimality is 
- 
10 
- 35 
140 
1 
= 
-20 
_< Ac 1 _~< 
3 = 
3 
4 
Likewise, the bounds for the change in c 2 are 
-35 
-10 
5 
= -28 _< Ac 2 < 
1 
= 20. 
n 
4 
2 
These computations show that, if the owner increases prices so that the 
2 (instead of $120) 
profit on 1000 board feet of finish-grade lumber is $166 7 
or so that the profit on the same amount of construction-grade lumber is 
$120 (instead of $100), then the owner can still make 1500 board feet of 
finish-grade and 2500 board feet of construction-grade lumber to maximize 
the profit. If the owner chooses to increase c 2 to $120, the final tableau 
will be Tableau 3.33 and the profit will be $480. 
Tableau 3.33 
CB 
100 
x 2 
1662 
x 1 
1662 
100 
0 
0 
X1 
X2 
X 3 
X4 
0 
1 
5 
z 
3 
1 
1 
0 
~ 
0 
0 
60 
0 
480 
XB 
A 
Changes in the Resource Vector 
Suppose that b k changes to bk = bk q- A bk" The old optimality criterion 
does not depend on b. However, the old solution may no longer be feasible 

~ 
Chapter 3 Further Topics in Linear Programming 
because the values of the basic variables may change. To compute their 
new values, we proceed as follows. Let 
bl 
b2 
bk + Abk 
bm 
bl - 
-0 
i bk 
+ Ab k 
b m 
- 
We may write b = b + Abke k. Then 
kth entry. 
]~B -- B-lb 
= B-lb 
+ AbkB-lek 
and 
XB = XB -+- AbkB- lek" 
(7) 
Now B-lek is the k th column of the matrix B-1. This column appears in 
the final simplex tableau in the column that held the kth column of the 
identity matrix in the initial tableau. If the new solution ~B is to be 
feasible, then A b k must be chosen so that 
X B + AbkB-lek > O. 
EXAMPLE 3. 
Looking again at the sawmill problem, we see that a 
change, A b 1, in the availability of the saw yields 
XB = 
3 
+ 
Abl 
3 
9 
If R B is to remain feasible, A b I must satisfy 
5 
88 
> 0 
2 "[- 
1-- 
3 
3 
2 
~ Abl > 0 
or 
-2 < Ab 1 _< 2. 
(8) 
Likewise, we find that, if the change A b 2 in the available hours of the 
plane satisfies 
-3 < Ab 2 < 5, 
the solution remains feasible (and optimal). On the other hand, if Ab 1 
does not satisfy (8), the old solution becomes infeasible. For example, if 
the availability of the saw is increased from 8 to 12 hr a week (Ab I = 4), 
then using (7) 
= 
3 
+4 
3 
= 
3 9 

3.6 Sensitivity Analysis 
233 
Inserting X n into Tableau 3.32, we obtain Tableau 3.34, which represents 
an infeasible solution that satisfies the optimality criterion. By applying the 
dual simplex method, we may restore feasibility. We then obtain Tableau 
3.35. 
Tableau 3.34 
CB 
100 
120 
X2 
X1 
120 
100 
0 
0 
X1 
X2 
X3 
X4 
0 
1 
5 
1 
@ 
1 
0 
7 
0 
0 
35 
10 
570 
IB 
15 
2 
Tableau 3.35 
CB 
100 
x 2 
0 
x 3 
120 
100 
0 
0 
X1 
X2 
X 3 
X4 
XB 
5-- 
1 
0 
• 
5 
3 
3 
4 
2 
0 
1 
~ 
2 
140 
100 
500 
0 
0 
3 
We see that the mill operator should make only construction-grade 
lumber if the saw is available 12 hr per day. Five thousand board feet can 
be made with this much sawing time for a profit of $500. 
/x 
3.6 
EXERCISES 
1. Consider the linear programming problem 
Maximize 
z --" X 1 -~- 2x 2 + X 3 "~" X 4 
subject to 
2x I + x 2 + 3x 3 + x 4 < 8 
2x~ + 3x 2 
+ 4x 4 _< 12 
3x I + 
x 2 -q- 2x 3 
< 18 
xj > O 
l_<j_<4. 

~4 
Chapter 3 Further Topics in Linear Programming 
After adding slack variables x 5, x6, and x 7 and solving by the simplex method, 
we obtain the final tableau shown below. 
CB 
1 
X 3 
2 
x 2 
0 
x 7 
1 
2 
1 
1 
0 
0 
0 
X 1 
X2 
X 3 
X4 
X 5 
X6 
X7 
X B 
4 
0 
1 
1 
1 
1 
4 
1 
0 
• 
0 
! 
0 
4 
3 
3 
3 
7 
0 
0 
10 
2 
1 
9 
3 
~ 
1 
3~ 
_ 
! 
~ 
0 
2~ 
7 
0 
0 
~4 
3 
9 
9 
(a) For each of the cost coefficients cj, 1 ~ j < 4, find the range of values for 
Acj for which the above solution remains optimal. 
(b) For each of the resources bi, 1 < i < 3, find the range of values for Ab i for 
which the above solution remains feasible. 
2. What will be an optimal solution to the problem in Exercise 1 
(a) if c 1 is changed to 3? 
(b) if b 2 is changed to 26? 
19 
(c) if c 3 is changed to 3. 
(d) if b 3 is changed to 127? 
3. Resolve the linear programming problem in Example 2, Section 2.3, keeping the 
columns for the artificial variables when forming Tableau 2.20. 
(a) For each of the cost coefficients cj, 1 < j < 6, find the range of values for 
Acj for which the solution remains optimal. 
(b) For each of the resources bi, 1 < i < 3, find the range of values for Ab i for 
which the solution remains feasible. 
4. What will be an optimal solution to the problem in Exercise 3 
(a) if c 2 is changed to 5? 
_79 
(b) if c3 is changed to - 2. 
(c) if b 1 is changed to 30? 
(d) if b 2 is changed to 25? 
5. Consider the agricultural problem in Exercise 4, Section 1.1. 
(a) Suppose the farmer is able to hire additional workers who can devote 60 hr 
to working the crops. How much of each crop should be planted? 
(b) Suppose the farmer decides to only plant 10 acres. How much of each crop 
should be planted? 
(c) Suppose the price received for oats increases by $1/acre, so that the profit 
per acre of oats is now $21. How much of each crop should the farmer 
plant? 
(d) What increase in profit for soybeans would induce the farmer to plant 
soybeans? 
6. Consider the investment problem in Exercise 8, Section 1.1. 
(a) Suppose the rate of return on the electronics stock increases to 5.2%. What 
is the optimal investment policy? 

3.6 SensitivityAnalysis 
235 
(b) Suppose the rate of return on the utility stock decreases to 7%. What is the 
optimal investment policy? 
(c) Suppose the amount invested in the bond is at least $90,000. What is the 
optimal investment policy? 
7. The text discusses changing only one component of the resource vector at a 
time. Consider now the situation in which 
Ab 1 
Ab2 
=b+Ab 
and 
Ab= 
. 
. 
Ab m 
That is, several components of the resource vector are changed. Following the 
text discussion, show that 
~B = XB + B-1 Ab 
and that the solution ~B is a feasible solution (and hence optimal) if and only if 
XB + B-lAb 
> O. 
3.6 PROJECT 
A tractor manufacturer in a developing nation subcontracts the task of making 
air filters for the tractors to a small company. The filter consists of a cylindrical 
main chamber with a cylindrical exit duct mounted on top of it. The specifications 
for the filters are as follow. 
1. In order to keep the dust efficiency within permissible limits, the diameter of 
the main chamber and the exit duct should not exceed 16 and 6.5 cm, respectively. 
2. To keep the pressure drop across the air cleaner small enough to prevent 
excessive power loss, the main chamber diameter and exit duct diameter should not 
be less than 9.5 and 3.5 cm, respectively. 
3. The main chamber is to be 24 cm tall, and the exit duct is to be 6.5 cm long. 
4. To maintain acceptable weight and durability, each filter must contain at 
least 1600 cm 2 of metal. 
5. At least 50 air filters must be supplied each month. 
As is typical in such countries, industrial materials such as sheet metal are not 
available in unlimited supply. The government has allocated 9.65 m 2 of metal each 
month to the filter manufacturer. 
A cross-sectional view of the filter is shown in Figure 3.6. Assume that the 
intake port and other interior structures need 40% of the total metal used for the 
main chamber and exit duct. Also assume that unusable scrap accounts for 15% of 
the total metal used for the main chamber and exit duct. 
(a) Set up a linear programming model for this situation to meet the objective of 
minimizing the amount of sheet metal used per air cleaner. 
(b) Solve the model. 
(c) Perform a sensitivity analysis on the model. 
(d) Comment on the solution, noting the meanings of the slack variables, the 
dual variables, and the ranges for "resource" and "cost" components. 

236 
Chapter 3 Further Topics in Linear Programming 
/i//// 
T 
6.5 
FIGURE 3.6 
24 
3.7 COMPUTER ASPECTS (OPTIONAL) 
The development of linear programming and its applications has paral- 
leled the development of the digital computer. The first linear program- 
ming problem to be solved on a computer dealt with a U.S. Air Force 
problem regarding the development and support of aircraft subject to 
strategic and physical requirements. It was solved in January 1952, on a 
machine at the National Bureau of Standards, now called the National 
Institute of Standards and Technology. The large computer programs 
(called LP codes or LP systems) that are available today to solve linear 
programming problems owe their existence to the current generation of 
computers with their extremely large auxiliary storage disks and their fast 
arithmetic operations. These large LP codes can theoretically handle 
problems involving as many as 5 million variables and at least 32,000 
constraints in reasonable (but not short) amounts of time. 
An LP system is typically available from a computer manufacturer as an 
option when leasing a large system, or it can be leased from one of many 
software development companies. Such a system may represent an invest- 
ment of 10-20 person-years of development time. Typical of the commer- 
cially available systems are IBM's Mathematical Programming System 
Extended, MPSX, and Optimization Subroutine Library, OSL; Bonner and 
Moore Software Systems' Functional Mathematical Programming System, 
FMPS; Ketron Management Science's Mathematical Programming System 
III, MPSIII; and CPLEX Optimization's CPLEX. 
Personal computers are now very popular for solving linear program- 
ming problems. Problems with up to 32,000 constraints and as many as 

3.7 ComputerAspects (Optional) 
237 
100,000 variables have been solved on such computers. Typical of PC 
systems are Lindo System's Lindo and Ketron Management Science's 
MPSIII/PC. 
Two new tools for solving LP problems are modeling languages such as 
GAMS and LINGO and spreadsheet-based LP packages such as What's 
Best!. The major advantage of modeling languages is that they allow one 
to develop and state models much more compactly and faster than older 
systems, which describe an LP problem by presenting all of the coefficients 
in tabular form. 
Spreadsheet-based systems have the main advantage of getting the 
results directly in ready-to-use form in the spreadsheet. Also, for many LP 
problems it is much faster to formulate the model in the spreadsheet than 
in old-fashioned tabular form. Further details are provided in Appendix B. 
In this section we will describe typical features of an LP code to give the 
reader some idea of what to look for in the course of solving a linear 
programming problem. The following example will be used as an illustra- 
tion. 
EXAMPLE 1. A health food store packages two types of snack foods, 
Chewy and Nutty, by mixing sunflower seeds, raisins, and peanuts. For the 
coming week the store has available in bulk quantities 90 kg of sunflower 
seeds, 100 kg of raisins, and 60 kg of peanuts. Invoices show that they paid 
$135 for the sunflower seeds, $180 for the raisins, and $60 for the peanuts. 
Chewy consists of two parts sunflower seeds, two parts peanuts, and six 
parts raisins. Nutty consists of three parts sunflower seeds and seven parts 
peanuts. Chewy sells for $2.00/kg, and Nutty sells for $1.50/kg. Deter- 
mine a mixing scheme that will maximize the store's profits, assuming that 
its entire production of Chewy and Nutty will be sold. 
Input 
To solve a general linear programming problem, clearly the LP code 
will need to know the type of problem (maximize or minimize), the 
coefficients of the objective function c, the coefficients of the constraints 
A, the right-hand sides of the constraints b, and the relation (<, =, >) 
for each constraint. There are several tradeoffs that can be made between 
ease of use for the problem solver and ease of programming and standard- 
ization for the programmer. Some codes assume that all problems are 
minimization problems, that all entries in b are nonnegative, and that all 
constraints are equalities. Thus, the user of these codes must put the 
model into a particular form by including slack variables and multiplying 
the objective function and each constraint by -1 if necessary. In this case 
the code provides the artificial variables where necessary. 
In larger problems the majority of the entries of A will be zero. In this 
case the user would not want to have to input all these zeros, so most 

~ 
Chapter 3 Further Topics in Linear Programming 
codes provide for entering only the nonzero elements of A and assume that 
all other entries are zero. Consequently, the input must identify the 
constraint and variable to which the coefficient belongs. This specification 
is accomplished by asking the user to assign a name (perhaps limited to six 
or eight characters) to each constraint and each variable. Then each 
nonzero coefficient can be entered by giving the names of the constraint 
and variable to which it belongs. A drawback to this method is that the 
computer will interpret a misspelled name as a new variable or constraint. 
Some codes try to protect against such errors by keeping the name in two 
pieces and flagging all input where one piece of the name agrees but the 
other does not. Naming the variables and constraints also provides more 
descriptive output, especially when the names are chosen as mnemonics 
for the quantities that the variables and constraints represent. A disci- 
plined naming convention is essential for automated report generation and 
solution analysis. 
Either the objective function is entered separately by giving the name of 
the variable and its objective function coefficient or it is entered as part of 
the constraints. The right-hand sides are entered by giving the constraint 
name and the coefficient value. If the constraints have not been changed 
into a particular form, the type of relation for each constraint must also be 
specified. 
EXAMPLE 1 (CONTINUED). Using mnemonic labels for the variables, we 
can write the mathematical model of the situation as 
OI" 
Maximize 
z = 2 x CHEWY - (1.5) (0.2) • CHEWY 
- (1.8)(0.6) x CHEWY 
-(1)(0.2) x CHEWY + 1.5 x NU'ITY 
-(1.5)(0.3) x NUTTY- 
(1)(0.7) x NUqq'Y 
Maximize 
subject to 
RAISIN: 
PEANUT: 
SUN: 
z = 0.42 x CHEWY + 0.35 x NUTTY 
6 x CHEWY 
< 100 
2 x CHEWY + 7 x NUTI~ < 60 
2 x CHEWY + 3 x NUT/'Y < 90. 
Most modern LP codes consist of three modules that are executed in the 
sequence listed. 
1. The preprocessor attempts to reduce the user's statement of the 
problem to one that can be solved more quickly. The preprocessor searches 
for ways to reduce the size of the problem by eliminating redundant 
constraints and variables. It seeks to set as many entries in the constraint 
matrix to zero as possible to allow this matrix to be stored more compactly 
and to improve the numerical stability of the solution process. The 
preprocessor uses reduction patterns that it tries to match with parts of the 

3. 7 Computer Aspects (Optional) 
239 
given coefficient matrix. Typical, although very simple, reduction patterns 
include the following. 
9 Empty rows or columns 
9 Rows involving only one variable: xj = b k 
9 Generalized upper bounds with only two variables: x~ + xj = b k 
9 Rows with all positive coefficients and a nonpositive right-hand side 
9 Rows with all negative coefficients and a nonnegative right-hand side 
9 Constraints that are implied by other constraints 
2. The optimizer actually solves the reduced linear programming 
problem. 
3. The postprocessor transforms the output back to the setting of the 
original problem. 
Besides specifying the problem, many codes allow for the user to specify 
various error-correcting features and output formats, which we will discuss 
later. They also provide the capability of identifying the problem to the 
code and user by allowing the user to give a title to the problem, which will 
be printed as a heading on each page of output. 
A typical job setup for an LP code is shown in Figure 3.7. Note that the 
various types of data are separated by the key words ROWS, COLUMNS, 
and RHS. 
Control 
Language 
Linear programming problem title 
Options for error diagnosis and correction tolerance, number 
of iterations between basis reinversions, number of 
iterations between row and column error checking 
Options for output formatting: controls for saving final results 
on tape or disk 
Type of Problem" MAX or MIN 
Input 
Data 
ROWS 
List of constraint names, constraint relations 
OBJECT 
(no relation given) 
COLUMNS 
List of variable names, constraint names, coefficient values 
List of variable names, OBJECT, objective function coefficients 
RHS 
List of constraint names, right-hand side values 
End of problem marker 
FIGURE 3.7 

240 
Chapter 3 Further Topics in Linear Programming 
Figure 3.8 shows the input for this example expressed in standard MPS 
format as used by nearly all current optimizers and a procedural control 
program that is typical for batch-oriented mainframe systems. The display 
of the input matrix was produced by a subroutine of the optimizer, 
typically called TRANCOL. 
Typical execution control program 
0001 
PROGRAM MIXTURE 
0 0 0 2  
AMINMAX:'MAX' 
0003 
CALL INPUT 
0004 
CALL SETUP 
0005 
CALL BCDOUT 
0006 
CALL PICTURE 
0007 
CALL TRANCOL 
0008 
CALL TRANROW 
0009 
CALL OPTIMIZE 
0010 
CALL SOLUTION 
0011 
STOP 
0012 
END 
Standard format MPS input 
NAME 
ROWS. 
N PROF I T 
L RAISIN 
L PEANUT 
L SUN 
COLUMNS 
CHEWY 
CHEWY 
NUTTY 
NUTTY 
RHS 
PROFIT 
PEANUT 
PROFIT 
SUN 
RHS 1 
RAI S I N 
RHS1 
SUN 
ENDATA 
.42000 
2.00000 
.35000 
3.00000 
100.00000 
90.00000 
RAISIN 
SUN 
PEANUT 
PEANUT 
6.00000 
2.00000 
7.00000 
60.00000 
Display of input matrix 
CHEWY 
AT 
*LO* 
PROFIT 
.42000 
RAISIN 
6.00000 
PEANUT 
2.00000 
SUN 
2.00000 
NUTTY 
*LO* 
.35000 
.00000 
7.00000 
3.00000 
FIGURE3.8 
RHS1 
.00000 
100.00000 
60.00000 
90.00000 

3. 7 Computer Aspects (Optional) 
241 
Algorithm 
Virtually all LP codes designed for production, rather than teaching, 
use the revised simplex method. This method has several desirable fea- 
tures, including the ability to handle a large number of variables. The real 
limit on the size of a problem is the number of constraints (see Section 
3.5). Other features will be described when we discuss error detection and 
correction. 
Most of the large LP codes provide an option for computing B-1 that is 
based upon a procedure from numerical linear algebra called LU factoriza- 
tion. That is, B is written as LU, the product of a lower triangular matrix L 
and an upper triangular matrix U. The inverses of upper and lower 
triangular matrices are easily calculated. Then B-1 = U-1L-1. Many large 
linear programming models have sparse matrices (ones with few nonzero 
entries). The matrix representations can then be highly compressed and 
L-1 and U-1 can be calculated in RAM, with special routines for sparse 
matrices, resulting in significant time savings. For this reason, more and 
more codes will provide an LU-factorization option. 
The revised simplex algorithm with iterative B -1 calculation is usually 
programmed to check itself at specified intervals. Between checks it 
follows the description we gave in Section 3.4. The check involves comput- 
ing the next B -1 in a manner different from the one we described. The 
matrix B can be constructed from the list of basic variables and the 
original problem as it was read in and stored. Then a very good method of 
numerically inverting B, such as the LU-factorization method described 
above, is used. This procedure of occasionally recomputing B-1 from the 
given problem serves to produce a more accurate basic feasible solution. 
However, in general the procedure is expensive in terms of computation 
time and must be used sparingly. 
As was indicated in Section 2.2 most LP codes provide several options 
for handling degeneracy when it occurs. 
Generalized Upper Bounding 
Earlier commercially available mathematical programming systems typi- 
cally included a procedure called generalized upper bounding (GUB, for 
short). This approach is historically interesting but has been superseded by 
improvements in algorithms and increased hardware speed. 
It is usual for a large linear programming problem to have a substantial 
number of constraints that deal with bounds on certain variables or sums 
of variables or with the balance of materials between two stages of a 
process (the output of the first stage equals the input of the second stage). 
The special structure of these constraints allows them to be treated in a 
way different from that of a general constraint. The GUB procedure may 
be used on a problem whose constraints have the form shown in Figure 
3.9. The GUB constraints are shaded. 

242 
Chapter 3 Further Topics in Linear Programming 
The GUB procedure allows the reduction of the number of basic 
variables from one corresponding to each constraint to one corresponding 
to each general constraint (r variables in Figure 3.9). Thus, a problem with 
1000 constraints, 800 of which are GUB constraints, can be solved with 
about the same effort as a problem with 200 general constraints plus the 
overhead of dealing with the GUB structure. 
Models to which the GUB procedure may be applied arise in a number 
of areas, including production resource allocation (e.g., forest management 
or commercial fishing), multiproduct blending, and large-scale models of 
economic sectors. 
Output 
We have seen in Sections 3.3 and 3.5 that there is a large amount of 
information about the solution to a linear programming problem that is 
contained in the final tableau. A typical output from an LP code will 
summarize this information in a useful form. Some codes also allow the 
option of having the output saved on disk for future input to a report 
generator or to rerun the problem with slight variations. 
The output will give the problem heading, the number of iterations 
used, and the optimal value of the objective function. Then usually a list of 
all the variables originally specified for the problem is produced. Those 
variables which are basic are so labeled. The value of each variable is 
given, along with the value of zj - 
cj from the objective row. 
The next part of the output lists the constraints and notes whether each 
is slack or binding. The value of the slack variable is also given. Because 
each constraint corresponds to a dual variable, the value of each dual 
variable, which may be interpreted as the marginal cost of each right-hand 
side (or resource) value, is also given. 
Optionally, ranges of values for each of the objective function coeffi- 
cients and each of the right-hand side values are given. These ranges come 

3.7 Computer Aspects (Optional) 
243 
from the sensitivity analysis of the optimal solution. One must interpret 
each range as giving the values that that particular coefficient may take, 
assuming that (1) no other coefficients are changed and that (2) the 
computed optimal value remains optimal. The output from MPS/90 for 
our example is shown in Figure 3.10. 
Error Detection and Correction 
2 
Throughout this chapter we have used rational numbers such as ~ and 
8 while solving linear programming problems using the simplex or 
revised simplex algorithms. A computer, however, will convert these num- 
bers to decimal representation and round off in the process. If the 
2 
8 
computer typically carries seven digits, x becomes 0.6666667 and 
11 
becomes -0.7272727. After many calculations the errors made through 
round-off and truncation tend to accumulate. 
It is possible for the 
computer to produce a "solution" to a linear programming problem that is 
not feasible because of the round-off error. 
Fortunately, the revised simplex method allows us to easily detect such 
errors as they accumulate. The key to this detection is that the revised 
simplex algorithm keeps the problem as it was entered and changes only 
the B -1 matrix. Suppose after some iterations the algorithm claims that 
i B = ll-~b is a feasible solution. Since at each iteration B-1 is computed 
from the previous value, round-off errors could have occurred, making 
what we have recorded as B-1 different from the theoretical B-1. 
However, we may take the original constraints in canonical form as 
Ax = b and set all the nonbasic variables to zero. This yields the system 
I 
Xi 
Xi 2 
Bx B = B 
. 
= b. 
[_xi~ 
We then compute B~ B and compare it with b. If the difference between 
the two values exceeds a preset tolerance, we can invoke an error- 
correcting routine. This process is generally referred to as checking row 
sums. Normally, the tolerance is set at 10 -6, although most LP systems 
allow the user to change its value. 
Column sums may also be checked by recalling that the entries in the 
objective row should be 
c~tj - cj = c~B-1A/- c/. 
The error-correcting routine that is used when row or column sum errors 
are detected involves what is usually referred to as reinverting 
the basis. 
The matrix B can be formed exactly for the present list of basic variables 

cO 
'l- 
r~ 
II 
00 I..~1 
"!-1,,4 
x 
<Z 
0 
0 
"" 
II 
O~a 
X 
0 
~m 
E 
.i 
i~1 
~ 
N 
0 
~" 
o o  
"-~oo 
r ~ o o  
f,~ 
~ 
o 
I 
I 
I-- 
Z3 
0 
(~11',m 
(_.) 
LI- 
> 
Z 
LLI 
> 
I"- 
0O'4 
Z 
0 
Z 
1.1..I 
Or,~ 
"r 
-..J o 
P,~ 
,,::C o 
J,~ 
,~ 
~ o P ~  
I..l_J 
rim r.... 0o 
>- 
0 
!.-- 
._1 
r',- ,..-~ c,,~ 
~" 
IJJ 
I-- 
I-- 
-- 
13_ 
0 
1:: 
0 
L.._ 
0 
,m, 
emm 
0 
(8) 
0 
II 
v.....4 
Z 
0 
F-- 
w 
i"- 
....J 
I..1.1 
0 
...J 
I-- 
0 
r~ 
n,n 
Z 
o 
!,r 
o 
o 
o 
r,r 
o 
o 
o 
I',r 
o 
o 
o 
I.t~ I..r o 
o
o
o
o
 
9 
9 
9 
9 
I 
I 
n t n O 0 0  
Z
o
o
o
 
0
o
o
o
 
Z
O
O
O
 
o
o
o
 
9 
9 
9 
i.l.l ot t I..~ n, t 
Z
Z
Z
Z
 
0
0
0
0
 
Z
Z
Z
Z
 
I,r 
o 
o 
o 
1,,r o 
o 
1.-I 
r,r 
o 
o 
0o 
i,,r~ o 
o 
i,,t~ 
I,r 
o 
o 
C'~l 
I 
I~'~ o 
o 
,.-I 
0o o 
o 
...~- 
o~.~" 
tJ) -J 
--J ch 
r,m Z~ ~ 
rn 
l - - Z l - -  
u-ooZ 
O-- 
,,~ Z 
r~" <E ,,, Z3 
O0 
Z 
C3 
.--I 
0 
rJ 
Z 
0 
I-- 
I.LI 
O0 
1.1.1 
::3 
-J 
O 0  
O 0  
O 0  
O 0  
O 0  
LLI IJ.I 
Z Z  
O 0  
Z Z  
o o  
o o  
o o  
o o  
o o  
I-- 
o o  
(./) 
o o  
0 
o o  
C3 
Z 
-- 
coo 
cooo 
(J 
,--4 
nnrn 
Z 
>->- 
0 
uJi-- 
cO 
-r~ 
(JZ 
C3 
Z 
o 
1.1.1 

3. 7 Computer Aspects (Optional) 
245 
from the original problem. Then a very accurate method from numerical 
analysis is used to calculate B -1. From this a better value of x B can be 
found, as x B = B- 1 b. 
In some codes this method of reinverting the basis is done at specified 
intervals irrespective of whether the row and column sums check. The 
length of this interval may be set by the user or may be determined 
automatically as a function of the size of the problem. 
Scaling 
One thing the user can do to minimize the errors generated by the 
algorithm is to make sure that the problem is appropriately scaled. This 
means that all the numbers in the constraint coefficient matrix A should be 
about the same size. If the coefficients of one constraint are more than 100 
times the size of those of another constraint, severe error propagation may 
result. However, the large coefficients can be divided by an appropriate 
number to make them about the right size. 
EXAMPLE 2. 
If tWO constraints in a linear problem are 
2x 1 + 0.12x 2 + x 3 -- 3x 4 = 7 
and 
(1) 
75x I + 250x 2 + 121x 3 + 314x 4 = 500, 
(2a) 
the second can be divided by 100 to yield 
0.75x I + 2.5x 2 + 1.21x 3 + 3.14x 4 -- 5. 
(2b) 
Constraints (1) and (2b) should be used as input to the revised simplex 
algorithm. 
In the same way, if the coefficients of one variable are significantly 
different in magnitude from those of all other variables, the units of the 
variable should be changed to bring the size of the coefficients in line. This 
means that the corresponding objective row coefficient will also have to be 
changed. 
EXAMPLE 3. 
Consider the linear programming problem 
Maximize 
z = 23x I + 2x 2 + 3x 3 
subject to 
140x 1+5x 2- 
x 3 < 10 
210x I + 
x 2 + 3x 3 < 14 
X 1 ~_~ O, 
X 2 ~__ O, 
X 3 >__ O. 

246 
Chapter 3 Further Topics in Linear Programming 
Replacing x 1 with x' 1 = 100Xl, we obtain the following problem: 
Maximize 
z = 0.23x'i + 2x 2 + 3x 3 
subject to 
1.4x' 1+5x 2- 
x 3 <_ 10 
2.1x' 1 + x 2 + 3x 3 < 14 
? 
X 1 ~__ 0, 
X 2 >__ 0, 
X 3 ~__ 0. 
Thus, if the units of x 1 are grams/kilogram, the units of x~ will be 100 
grams/kilogram. 
Restarting 
Most large codes allow the user the option of saving the computations 
that have been done to a certain point. One scheme allows the user to 
specify the maximum number of iterations permitted. When that maximum 
number is reached, the list of basic variables and the original problem 
statement are written to disk for restarting at a later time. Another scheme 
allows the final solution and list of basic variables to be saved along with 
the original problem. This scheme is particularly useful because the 
restarting procedure allows additional constraints or variables to be put 
into the problem. In this case the dual simplex method can be used to 
restore feasibility if necessary. 
The restart procedures are especially useful when analysis shows that 
the original model produced results that are inappropriate to the situation 
that was modeled and the model must be changed to better represent the 
actual situation. This procedure is also used when solving a family of 
related problems. The list of basic variables from the solution to one 
model is used as the starting point for solving the other methods. 
Further Reading 
Beale, E. M. L. Mathematical Programming in Practice. Pitman, London, 1968. 
Dantzig, George B., and Van Slyke, R. M. "Generalized Upper Bounding Techniques for 
Linear Programming." J. Comput. System Sci. 1 (1967), 213-226. 
Gale, David. The Theory of Linear Economic Models. McGraw-Hill, New York, 1960. 
Gale, David, Kuhn, Harold W., and Tucker, Albert W. "On Symmetric Games," in Contribu- 
tions to the Theory of Games (H. W. Kuhn and A. W. Tucker, Eds.), Annals of Mathemati- 
cal Studies, No. 24, Princeton Univ. Press, Princeton, NJ, 1950. 
Gass, Saul I. Linear Programming, fifth ed. McGraw-Hill, New York, 1984. 
Geoffrion, A. M. "Elements of Large-Scale Mathematical Programming." Management Sci. 
16 (1970), 652-691. 
Lenstra, J. K., Rinnooykan, A. H. G., and Schrijver, A. (Eds.). History of Mathematical 
Programming: A Collection of Personal Reminiscences. Elsevier Science Publishers, 
Amsterdam, 1991. 
Murty, Katta. Linear and Combinatorial Programming. Wiley, New York, 1976. 

3. 7 Computer Aspects (Optional) 
247 
Nazareth, J. L. Computer Solution of Linear Programs. Oxford Univ. Press, New York, 1987. 
Orchard-Hays, William. Advanced Linear Programming Computing Techniques. McGraw-Hill, 
New York, 1968. 
Salkin, Harvey S., and Saha, Jahar (Eds.). Studies in Linear Programming. North-Holland, 
Amsterdam, 1975. 
Thesen, Arne. Computer Methods in Operations Research. Academic Press, New York, 1978. 
Wagner, Harvey. Principles of Operations Research. Prentice-Hall, Englewood Cliffs, NJ, 1969. 
White, William W. "A Status Report on Computing Algorithms for Mathematical Program- 
ming." ACM Comput. Surveys 5 (1973), 135-66. 

Integer 
Programming 
I 
N THE LINEAR programming problems considered so far, the vari- 
ables have been permitted to assume all nonnegative real values. 
However, there are many problems in which the variables must 
assume only integer values. For example, it would be meaningless to have 
an answer calling for the manufacture of half a table or for the chartering 
of 1.2 airplanes. In some problems, such as the transportation problem 
with integer values for supply and demand, the simplex method will yield 
integer answers; however, in many other problems it will not. In this 
chapter we formulate a number of problems that require integer variables 
and present three algorithms for solving these integer programming prob- 
lems. 
4.1 EXAMPLES 
EXAMPLE 1 (THE TRANSPORTATION PROBLEM). Suppose a manufac- 
turer making one product has rn factories and n warehouses. The demand 
249 

~0 
Chapter 4 Integer Programming 
at the jth warehouse is dj, j = 1, 2,..., n, and the supply available from 
the ith factory is s i, i = 1, 2,..., m. The cost of shipping one unit from the 
ith factory to the jth warehouse is Ciy. Our problem is to determine the 
amount, xq, of the product to be sent from the ith factory to the jth 
warehouse. 
If we assume that the total supply at least equals the total demand, 
m 
n 
ESi>-~ Edj, 
i=1 
j=l 
so that our problem is feasible, then the mathematical model is 
Minimize 
rn 
n 
Z 
-
-
 
E E CijXij 
i=lj=l 
subject to 
n 
E Xij <-~ S i, 
j=l 
i= 1,2,...,m 
m 
Exij>_~dj, 
j = 
1,2,...,n 
i=1 
xij > 0 
and 
integral, 
i= 1,2,...,m; 
j- 
1,2,...,n. 
If this problem is converted to standard form, then the only entries in 
the constraint matrix are ls, -ls, and 0s. It can be shown using a result of 
Hoffman and Kruskal that in this case the simplex method will automati- 
cally yield integer solutions if the s i and dj are integers. However, the 
simplex method is a rather poor way of solving the transportation problem. 
In Chapter 5 we present a special algorithm for this problem that is rather 
efficient. This algorithm was developed because the transportation model 
arises repeatedly in practice. 
A 
EXAMPLE 2 (THE KNAPSACK PROBLEM). Consider the problem faced by 
a hiker who cannot carry more than k pounds of equipment. She has n 
items that she is considering bringing. To each item she assigns a relative 
value, cj, with the most important items having the highest values. Let aj 
be the weight of the jth item. The hiker's problem is to decide which of 
the n items to carry; she will choose those that maximize the total relative 
value subject to the weight limitation. 

4.1 Examples 
251 
To construct the mathematical model, let xj = 1 if the jth item is 
chosen and let xj = 0 if the jth item is not chosen. Then the model is 
n 
Maximize 
z = ~ cjxj 
j=l 
subject to 
n 
E ajxj <__ k 
j=l 
xj=0 
or 
1, 
j= 1,2,...,n. 
Note that by limiting the value of xj to 0 or 1, the left-hand side of the 
constraint represents just the weight of the items that are chosen. This 
type of an integer programming problem is called a zero-one program- 
ming problem. 
A 
EXAMPLE 3 (THE ASSIGNMENT PROBLEM). Suppose n people, el, 
P2,..., Pn, are being considered for n jobs, J1, J2,..., Jn" Using various 
criteria, including past performance, aptitude, and job ability, we specify a 
value cij that would accrue if the ith person is given the jth job. We 
assume that each person is assigned to exactly one job and that each job is 
assigned to exactly one person. Our problem is to assign the people to the 
jobs so that the total value of the assignment is maximized. 
To construct the mathematical model, define the variables Xiy so that 
1 
if Pi is assigned to Jj 
Xij -- 
0 
otherwise. 
Then the mathematical model is 
Maximize 
subject to 
n 
E Xij 
i=1 
n 
E Xij 
j=l 
=1, 
=1, 
n 
n 
Z = E 
E CijXij 
i=lj=l 
j= 1,2,...,n 
i= 1,2,...,n 
xij=O 
or 
1, i,j=l,2,...,n. 
(1) 
(2) 
Under the condition that xi/has a value of either 0 or 1, exactly one of the 
summands in Equation (1) can be nonzero, and, likewise, exactly one of 
the summands in Equation (2) can be nonzero. Constraint (1) says that job 

~ 
Chapter 4 Integer Programming 
j is assigned to exactly one person; constraint (2) says that person i is 
assigned to exactly one job. Just as in the transportation problem, the 
result of Hoffman and Kruskal applies and the simplex algorithm yields a 
zero-one solution to the assignment problem. However, there is a special 
algorithm that efficiently handles this problem; it will be discussed in 
Chapter 5. 
A 
EXAMPLE 4 (THE TRAVELING SALESMAN PROBLEM). A traveling sales- 
man has to visit each of n cities, Ca, C2,..., Cn. He must start from his 
home office in city C1 and return to Ca after visiting each city exactly 
once. Such a route is called a tour. The order in which he visits cities 
C2, C3,..., Cn does not matter. He knows the distance between each pair 
of cities and wants to choose a tour that minimizes the total distance 
traveled. 
To formulate the mathematical model, let cij be the distance between 
Ci and Cj. Let the variable xij be defined by 
Xij -- 1 
=0 
if the route includes traveling from Ci to Cj 
otherwise. 
The condition that the route must go to exactly one city after leaving C i 
may be written 
n 
~_.xij= 1, 
i= 1,2,...,n. 
j=l 
The condition that the route goes through every city exactly once can be 
phrased by saying that each city must be reached from exactly one city, or 
n 
~xij= 1, 
j= 1,2,...,n. 
i=1 
Our mathematical model is then 
Minimize 
n 
n 
Z = E 
E CijXij 
i=lj=l 
subject to 
n 
EXij 
= 1, 
i-1 
j = 1,2,...,n 
(3) 
n 
E Xij "- 1, i= 1,2,..., n 
(4) 
j=l 
xij=0 
or 
1, i,j= 1,2,...,n. 

4.1 Examples 
253 
Consider the feasible solution for this problem when n = 12: 
and 
X12 --X23 -X34 
~---X45 ~--X56 = X61 = 
1 
X78 -- X89 = X9,10 -- Xl0,11 --- X11,12 --- X12,7 =1 
xij = 0 
for all other values of i and j. 
This solution is feasible, since each index from 1 to 12 occurs exactly once 
in the first position and exactly once in the second position. However, it is 
not an acceptable solution, since there are two disconnected subtours. We 
must design a way to eliminate disconnected routes from our set of 
feasible solutions. 
To this end we introduce n- 
1 new variables, u 2, U3,...,Un, 
and 
(n - 1)2 _ (n - 1) new constraints. The constraints are 
U i -- Uj + rI.Xij ~_ n - 
1, 
i,j -- 2,3,..., 
n, 
and 
i ~ j 
(5) 
ui >0 
and 
integral, 
i=2,3,...,n. 
Before we had 2n constraints and n 2 variables; these variables had values 
of either 0 or 1 and n of them x, were always 0. We now have 
2n + (n - 1) 2 - (n - 1) = n 2 - n + 2 
linear constraints and 
n2+n-1 
integer-valued variables. 
We now show that the constraints (3), (4), and (5) do not permit 
disconnected routes and still include all routes satisfying the original 
problem statement. First we assume that there is a subtour; that is, the 
route leads back to C 1 before visiting all the cities. Then there must be 
another subtour, since each city is visited exactly once. This subtour will 
start and end at some city in the list C 2, C3,..., Cn; it will not include C1; 
and it will include r < n - 1 cities. The r variables xq that describe this 
subtour will be equal to 1. We add up the r constants (5) that correspond 
to these nonzero xij. This new constraint is satisfied by any solution that 
satisfies (5). As we take the sum to form this new constraint, we have -uj 
when the route enters city Cj and +uj when it leaves. Since the route 
enters and leaves each of the r cities exactly once, the uj's cancel out in 
the sum. Thus, the new constraint is 
nr <__ (n - 1)r, 
which is a contradiction of our assumption that there was a subtour of 
length r _< n - 1. 
For example, if we had the subtour starting at C 4, 
C 4 ~ 
C 5 ~ 
C 3 ~ 
C 2 ~ 
C4, 

~ 
Chapter 4 
Integer Programming 
so that 
X45 = X53 -- X32 -- X24 -- 1, 
then we would form our new constraint by adding the constraints 
U 4 -- U 5 9 r/X45 __< n - 1 
U 5 -- U 3 -]- /'/X53 __< n - 1 
U 3 --U 2 --[-nx32 _~< n- 
1 
u2-u4+nx24-<n- 
1 
and obtain 
4n <4(n- 
1). 
We have now shown that constraints (3), (4), and (5) allow no subtours. 
Now we show that these constraints do not exclude any potential routes. 
To do this we show that each u i can be assigned a nonnegative integer 
value for any route and that these values satisfy the constraints given in 
(5). 
Let 
t i be the position in the route at which C i is visited. Thus, t I = 1 for 
C1. If we consider the route that starts C1 ---> Ca --* C6 ---> C2 ---> "", then 
t I = 1, t 4 -- 2, t 6 = 
3, t 2 = 4,.... 
Let u i = ti for i = 2,3,..., n. We show 
that for each i and j, (5) holds. Either x~j = 1 or xi/= 
0. If xij = 1, then 
C/ is visited immediately after Ci, so that 
tj = ti + l. 
Substituting this equation into (5), we have 
U i -- Uj 7 t- nxij = t i -- (t i + 1) + n = n - 1 
as we needed. If X ij -- O, then since u i < n and u/>_ 2, we have 
U i -- Uj _~< n -- 2 < n -- 1, 
so that (5) holds. 
We have shown that a model for the traveling salesman problem is 
n 
n 
Minimize 
z= 
~ 
~ci/xi/ 
i=1 j=l 
subject to 
n 
~xi/=l, 
j = 1,2,...,n 
i=1 
n 
~_, x~j 
j=l 
= 1, 
i= 1,2,...,n 
U i --Uj 
-~- nxij <_~ n - 
1, 
i,j = 2,3,..., n, 
and 
i 4:j 
Xij -- 0 
or 
1, 
i,j = 1,2,...,n 
ui>_O 
and 
integral, 
i-2,3,...,n. 
A 

4.1 Examples 
255 
EXAMPLE 5 (STOCK CUTYING PROBLEM). A plumber can buy plastic 
pipe in 6- and 12-ft lengths. The current job requires eight 4-ft lengths, five 
5-ft lengths, and three 7-ft lengths. The plumber wants to figure out how 
many of each of the two stock lengths should be bought to minimize waste. 
We determine all the possible ways the stock lengths can be cut to yield 
the necessary lengths. A 6-ft piece can be cut to give 
one 4-ft length and 2 ft of scrap 
one 5-ft length and 1 ft of scrap 
(cutting pattern 1) 
(cutting pattern 2). 
A 12-ft piece can be cut to give 
one 4-ft piece and 8 ft of scrap 
two 4-ft pieces and 4 ft of scrap 
three 4-ft pieces 
one 4-ft piece, one 5-ft piece, and 3 ft of scrap 
one 4-ft piece, one 7-ft piece, and 1 ft of scrap 
one 5-ft piece and 7 ft of scrap 
two 5-ft pieces and 2 ft of scrap 
one 7-ft piece and 5 ft of scrap 
one 7-ft piece and one 5-ft piece 
(cutting pattern 3), 
(cutting pattern 4), 
(cutting pattern 5), 
(cutting pattern 6), 
(cutting pattern 7), 
(cutting pattern 8), 
(cutting pattern 9), 
(cutting pattern 10), 
(cutting pattern 11). 
Let piece 1 be of length l I = 4 ft, let piece 2 be of length 12 = 5 ft, and let 
piece 3 be of length 13 -- 7 ft. Let 
aii = number of pieces of length I i in cutting pattern j; 
b i = number of pieces of length I i which are needed; 
cj = waste in cutting pattern j; 
xj = number of times cutting pattern j is used. 
Our mathematical model is 
Minimize 
z = s 
subject to 
Ax=b 
x >_ 0 
and 
integral, 

~ 
Chapter 4 Integer Programming 
where 
1 
0 
1 
2 
3 
1 
1 
0 
0 
0 
0] 
1 
A-- 
0 
1 
0 
0 
0 
1 
0 
1 
2 
0 
1 
0 
0 
0 
0 
0 
0 
1 
0 
0 
1 
1 
C T= [2 
1 
8 
4 
0 
3 
1 
7 
2 
5 
O] 
and 
[sj 
b= 
5 
. 
ZX 
3 
EXAMPLE 6 (FIXED CHARGE PROBLEM). A manufacturing corporation 
makes n products, and naturally the board of directors wants to minimize 
manufacturing costs. Each unit of product j that is made costs cj dollars to 
produce (raw materials, labor, direct machine costs, etc.). Moreover, if any 
units of product j are made, there is a fixed cost of k i dollars, which 
represents the initial cost of setting up the production and distribution 
process. 
Let xj be the number of units of product j that are made. Suppose that 
the production process is constrained by a system of inequalities such as 
that in Exercises 1 or 2 in Section 1.1. Our objective function is 
Minimize 
Z-- ~ (CjXj + kjyj) 
j=l 
where the production constraints not involving the new variables Yi hold 
and in addition 
1 
if 
xj > 0 
YJ = 
0 
if 
Xj = 
O. 
(6) 
That is, yj indicates whether any of the jth product is manufactured. The 
constraints in (6) are nonlinear functions of x/ and Yi" These constraints 
are not defined by hyperplanes as they must be for a linear programming 
problem. 
However, this problem can be cast as a problem with linear constraints 
in which some of the variables are restricted to be integers and others may 
take any value. Such problems are called mixed integer programming 
problems. 
Suppose we know an upper bound on the number of units of Xg that can 
be produced. That is, suppose we know numbers Mj, such that 
xj<_Mj, 
j = 1,2,...,n. 

4.1 Examples 
~7 
We now show that we may reformulate the definition of yj in (6) as 
xj 
/ 
yj>~ 
Mj 
(7) 
yj=0 
or 
1. 
If xj > 0, then yj > xj/Mj > 0 implies that yj = 1 since yj can be only 0 
or 1. If xj ---- 0, then yj > xj/Mj >_ O, so that yj = 0 or 1. But since we are 
minimizing, the objective function will be smaller if yj = 0. Therefore, at 
the minimum value of the objective function, if xj = 0, then yj - 0. The 
constraints given by (7) are now linear. We combine (7)with those 
constraints describing the production process to obtain our mixed integer 
programming model. 
A 
EXAMPLE 7 (EITHER-OR PROBLEM). 
Suppose that we have a situation 
in which either the constraint 
~ aljx j <~ b 1 
(8) 
j=l 
or the constraint 
n 
E a2jxj <~ b2 
j=l 
(9) 
holds. We can convert this condition to one in which the constraints are 
linear if we have available numbers M1 and M2, such that 
n 
Y'~ a~jxj - b 1 < M~ 
j=l 
and 
• a2jx j - b 2 ~ m 2 
j=l 
for all feasible solutions. 
Let y be a zero-one variable. Consider the problem 
n 
aUx j - b I < Mly 
(10) 
j=l 
~ azjX j -- b 2 ~ m2(1 -y) 
(11) 
j=l 
y = 0 
or 
1. 
If y = 0 in our new problem constraint, then (10) is the same as constraint 
(8), and constraint (11) is redundant, since it holds for all feasible solu- 
tions. If y = 1, then constraint (11) is the same as constraint (9), and 
constraint (10) is redundant. 
A 

~1~ 
Chapter 4 Integer Programming 
We now examine a general integer programming problem and describe 
some methods for solving it. We consider the following problem: 
Maximize 
z = cTx 
subject to 
Ax<b 
x>0 
xj=integer 
if 
j~I, 
where I is a subset of {1, 2,..., n}. If I = {1, 2,..., n}, then the problem is 
called a pure integer programming problem. If I is a proper subset of 
{1,2,...,n}, then the problem is called a mixed integer programming 
problem. In a pure integer programming problem every variable is re- 
quired to be an integer. In a mixed integer programming problem only 
some of the variables are required to have integer values. Examples 6 and 
7 are mixed integer programming problems. Examples 1-5 are pure 
integer programming problems. In Examples 2 and 3 the variables are 
restricted to the values 0 or 1. 
One might attempt to solve an integer programming problem by treat- 
ing it as a linear programming problem (that is, by not restricting the 
variables to integer values)and then rounding the answer to the nearest 
integer. Under extremely fortunate circumstances one might not have to 
round at all. But there are other situations in which rounding will produce 
an incorrect answer. 
EXAMPLE 8. 
Consider the integer programming problem 
Maximize 
z=7x+8y 
subject to 
10x + 3y < 52 
2x + 3y < 18 
x>0, 
y>0, 
and integers. 
If we ignore the restriction that x and y are integers, the simplex method 
gives the solution (verify) 
with optimal value 
1 
1 
x=4~-, 
y=3g 
z = 55~. 
If we round the values of x and y to the nearest integer values that are 
feasible, we get 
x=4, 
y-3, 

4.1 Examples 
259 
and 
z= 52. 
However, the solution 
x=3, 
y=4 
is also feasible, and the value of the objective function for this solution is 
z= 53. 
A 
4.1 
EXERCISES 
In Exercises 1-6 formulate the given problem as an integer programming 
problem. 
1. Equipment purchasing problem. A ribbon manufacturer is considering the 
purchase of two different types of printing machines that will be used to emboss 
designs on the ribbon. Machine A can print 100 m per minute and requires 50 
m 2 of floor space, whereas machine B can print 200 m per minute and requires 
140 m 2 of floor space. Suppose that the manufacturer must print at least 600 m 
per minute and has no more than 350 m 2 of floor space. If a model A machine 
costs $22,000 and a model B machine costs $48,000, how many machines of each 
type should be bought to minimize the cost? 
2. A production problem. A chair manufacturer makes three different types of 
chairs, each of which must go through sanding, staining, and varnishing. In 
addition, the model with the vinyl-covered back and seat must go through an 
upholstering process. The following table gives the time required for each 
operation on each type of chair, the available time for each operation in hours 
per month, and the profit per chair for each model. How many chairs of each 
type should be made to maximize the total profit? 
Sanding 
Staining 
Varnishing 
Upholstering Profit 
Model 
(hr) 
(hr) 
(hr) 
(hr) 
($) 
A--solid back 
1.0 
0.5 
0.7 
0 
and seat 
B--ladder back, 
1.2 
0.5 
0.7 
0 
solid seat 
Cwvinyl-covered 
0.7 
0.3 
0.3 
0.7 
back and seat 
Total time available 
600 
300 
300 
140 
per month 
10 
13 
8 
3. Pam Hardy currently has six favorite country and western songs. There are 10 
compact disks that contain different groups of these songs available. Suppose 
that the jth CD costs cj dollars. Set up a model that Pam could use to 

~0 
Chapter 4 Integer Programming 
determine the cheapest selection of CDs to buy to get at least one version of 
each of her favorite songs. 
4. Tommy Jones's mother is planning his 10th birthday party and will serve a 
variety of soft drinks, which will be chosen from the list below. 
Root 
Ginger 
Drink 
Cola 
beer 
Cherry Lemon 
Orange Grape 
ale 
Price per bottle 
69 
59 
62 
62 
65 
55 
65 
(cents) 
From past experience it has been determined that at least 12 bottles of soft 
drinks are needed. Also, at least 2 bottles of ginger ale, at least 2 bottles of cola, 
and no more than 3 bottles of fruit-flavored soft drinks are needed. How many 
bottles of each type should be bought to minimize the total cost? 
5. A manager for a large corporation must prepare a list of projects that her group 
will complete over the next year. She has under consideration 10 such projects 
but will not be able to do all of them because of limits on personnel and budget. 
She has assigned a weight to each project that represents to her the value of 
completing the project. The personnel, capital requirements, and weights for 
each project are given in the following table. 
Project 
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
Person-weeks 
250 195 200 
70 
Cost (thousands 
400 300 350 100 
of dollars) 
Value of completion 
70 
50 
60 
20 
30 
40 100 170 
40 120 
70 
70 250 250 100 200 
10 
20 
30 
45 
10 
40 
The manager has available 1000 person-weeks and $1,500,000 to allocate among 
the projects. Which projects should she choose to complete to maximize the 
value? 
6. Each day at the Graphic Arts Co. the press operator is given a list of jobs to be 
done during the day. He must determine the order in which he does the jobs 
based on the amount of time it takes to change from one job setup to the next. 
Clearly he will arrange the jobs in an order that minimizes the total setup time. 
Assume that each day he starts the press from a rest state and returns it to that 
state at the end of the day. Suppose on a particular day that he must do six jobs 
for which he estimates the changeover times given in the following table. What 
schedule of jobs should the operator use? 

4.1 Examples 
261 
J 
i 
1 
2 
3 
4 
5 
6 
Rest 
1 
2 
3 
4 
5 
6 
Rest 
(From job i to job j (min)) 
0 
10 
5 
15 
10 
20 
5 
10 
0 
10 
10 
20 
15 
10 
5 
5 
0 
5 
10 
10 
15 
8 
10 
3 
0 
9 
14 
10 
4 
7 
8 
6 
0 
10 
10 
10 
5 
10 
15 
10 
0 
8 
7 
7 
9 
12 
10 
8 
0 
4.1 
PROJECTS 
1. Meg Watkins is trying to decide which college to attend. From among the 
applications that she submitted, she has been admitted to four schools. One is a 
large state university that is about 250 miles from her home. At this school she 
may live in a dormitory for two years, but then must find accommodations in the 
community. Another is a small private school about 1000 miles from her home 
that has an excellent reputation. At this school there are dormitory accommoda- 
tions for all students. Meg, under pressure from her father, also applied to and 
was accepted by the private church-related school in her hometown. Since she is 
a local student, the school would expect her to live at home. 
Another possibility open to Meg is to go to the local community college for 
two years and then transfer to another school. The reputation of the community 
college has been improving over the last few years. The state university would 
carry forward her acceptance for two years and transfer all credits. The distant 
private school will also carry forward her acceptance but most likely will transfer 
nine credits fewer than two full years of credit. The local private school has no 
formal statement on transfer from two-year schools. The accompanying table 
gives the cost for attending each school and Meg's assessment of the worth (or 
utility) of having certain blocks of credit from each school. 
Distant 
Local 
Community 
State 
priva te 
private 
college 
Tuition 
$2500/year 
$14,000/year 
$9000/year 
Living 
On campus 
$3500/year 
$3500/year 
$125/month 
Off campus 
$4000/year 
Humanities 
7 
9 
6 
Social science 
6 
8 
5 
Science 
8 
5 
4 
Major 
8 
10 
6 
$125/month 
Set up a model that Meg could use to maximize the future worth of her 
education assuming that she can earn $3000/summer and that her father will 

262 
Chapter 4 Integer Programming 
provide $6000/year for her education and living expenses. You may wish to 
consider allowing Meg the option of working while going to school or of going to 
summer school for certain courses. 
2. Consider the problem of making change in a supermarket. Suppose that the 
cash register shows that your change is to be C cents. Initially, we assume 
C < 100. The coins that are available to make change have values 
w 1= 1, 
w 2=5, 
w 3= 10, 
w 4=25, 
and 
w 5=50. 
(a) Set up an integer programming problem for finding which combination of 
coins yields the correct change using the smallest number of coins. 
(b) Construct a table giving the solution to part (a) for C = 1, 2,..., 99. 
(c) One algorithm for change-making calls for giving as many of the largest-de- 
nomination coins as possible, then using the next largest denomination for 
the remaining amount, and so on. Does this algorithm give the correct 
answer for each C = 1, 2 .... ,99? 
(d) Suppose our monetary system had coins with values 
w 1= 1, 
w 2=5, 
w 3=20, 
and 
w 4=25. 
Use the algorithm in part (c) to make up C = 40. Is this a minimal solution? 
(e) Consider the problem of giving C dollars in change, where C = 1, 2 ..... 9. 
Would it be advantageous to use a $2 bill in addition to the $1 and $5 bills? 
(f) What other situations have models similar to the change-making problem? 
Further Reading 
Chang, S. K., and Gill, A. "Algorithmic Solution of the Change-Making Problem." J. ACM 
17 (1970), 113-122. 
Hoffman, A. J., and Kruskal, J. B. "Integral Boundary Points of Convex Polyhedra," in 
Linear Inequalities and Related Systems (H. W. Kuhn and A. W. Tucker, Eds.), pp. 223-246. 
Princeton Univ. Press, Princeton, NJ, 1956. 
4.2 CUTTING PLANE METHODS 
In this section we discuss one approach that has been used to solve 
integer programming problems. The algorithms for such problems are not 
as nice as those for linear programming problems in the sense that there is 
not one algorithm that works well for all integer programming problems. 
Among the difficulties with these algorithms is their inefficiency for even 
medium-sized problems. For example, computations for the traveling sales- 
man problem (Example 4 in Section 4.1) become prohibitively long for 
over 200 cities. 
Consider the pure integer programming problem 
Maximize 
z = cTx 
(1) 
subject to 
Ax = b 
(2) 
x > 0 
and 
integral, 
(3) 

4.2 Cutting Plane Methods 
~ 
where A is an m • n matrix, c is an n • 1 column vector, and b is an 
m • 1 column vector. 
The cutting plane algorithms were developed by Ralph Gomory in 
1958. The idea behind the algorithms is to start with the problem given by 
(1), (2), and (3); ignore the restriction that x must be a vector with integer 
components; and solve the resulting problem by the simplex method. If the 
resulting solution is integral, we are finished. Otherwise, we add a new 
constraint that "cuts off" (eliminates)some nonintegral solutions, includ- 
ing the one just obtained by the simplex method. However, the new 
constraint is carefully constructed so as not to eliminate any feasible 
integer solutions. We solve the new problem and repeat the simplex 
algorithm. By adding enough constraints, we eventually reach an optimal 
integer solution. 
Suppose the problem has a feasible solution and that a finite optimal 
value exists. We may assume that A, B, and c have integer entries. The ith 
constraint as it appears in the final simplex tableau for the related linear 
programming problem is 
n 
E tijxj = XBi, 
(4) 
j=l 
where XBi is the value of the ith basic variable for the optimal solution of 
the related linear programming problem. We denote by [a] the integer 
part of a. That is, [a] is the largest integer K, such that K < a. For 
example, 
3 
3 
[~1= 1, 
[~]=0 
2 
1 
[-31= 
-1, 
[-371= 
-4 
[31=3, 
[-21= 
-2. 
Since [tij] < tij and xj > 0, we can write from (4) 
n 
E [tij]Xj <_~ XBi. 
j=l 
(5) 
Any integer vector x that satisfies (4) must also satisfy (5). For such x the 
left-hand side of (5) is an integer. Thus, we may write the constraint (6) 
that x must also satisfy: 
n 
E [tij ]Xj <_~ [ XBi ]. 
(6) 
j=l 
We can transform (6) into an equation by introducing the slack variable u/" 
n 
E [tij ]Xj -Jr- U i = [XBi]. 
(7) 
j--1 

~ 
Chapter 4 Integer Programming 
Since u/ is the difference between two integers, we may require that u i be 
an integer. We have shown that any feasible solution to the given integer 
programming problem will also satisfy (7) for some value of u i. 
Now assume that Xni is not an integer. We may write 
[ tij ] + gij = tij 
and 
[XBi] + fi -- XBi' 
where 0 < gij < 1 and 0 < fi < 1. The quantity fi is called the fractional 
part of Xni. Thus, if 
9 
2 
tij - 
-~ , 
then 
gij = v, 
and if 
__. 
1 
ti j 
2 
then 
gij = 3 
37 
If we subtract (4) from (7), we have 
n 
(-g~j)xj + u~ = -L. 
(8) 
j=l 
Equation (8) is the cutting plane constraint to be added to the constraints 
in (2). We now proceed as follows. 
Step 1. 
Solve the related linear programming problem obtained from 
the given integer programming problem by dropping the integrality re- 
quirements. If the solution is a vector with integer components, stop. 
Otherwise, go to Step 2. 
Step 2. 
Generate a cutting plane constraint as in (8). A heuristic rule 
to use for choosing the constraint in the cutting plane construction is 
choose the constraint in the final simplex tableau that gives the largest fi. 
Step 3. 
Consider the new integer programming problem, which consists 
of the same objective function, the constraints in (2), and the cutting plane 
(8), which have been added in Step 2. Return to Step 1. 
Since the coefficients of all the basic variables except the ith one in the 
list will be zero in (4), we may rewrite (4) as 
Xr i "[- E 
tiixj = XBi, 
(9) 
j~N 
where N is the set of indices of the nonbasic variables and where the 
variable labeling this ith constraint is Xr. Suppose that XBi is not an 
integer. The Gomory cutting plane obtained from (9) is 
E 
(--gij)Xj + Ui = 
--L, 
(10) 
j~N 
where 
[ tij ] + gij = tij 
and 
[XBi] + L --- XBi" 

4.2 Cutting Plane Methods 
265 
If we set the nonbasic variables xj equal to zero in (10), we obtain 
u i =-f/< 
0. Since the slack variable 
u i is negative, we see that the 
optimal solution to the related linear programming problem whose i th 
constraint is given by (9) is no longer a feasible solution to the new linear 
programming problem with the added constraint (10). We have cut off the 
current optimal solution. The dual simplex method can now be used to 
solve the new problem; it will remove the infeasibility caused by adding the 
constraint (10). 
We have shown that the cutting plane method fulfills the two criteria. It 
does not remove any integer vectors from the set of feasible solutions, but 
it does remove noninteger optimal solutions. 
EXAMPLE 1. 
Consider the pure integer programming problem 
Maximize 
z 
= 
5X 1 -~- 6X 2 
subject to 
10x I + 3x 2 _< 52 
2x 1 + 3x 2 _< 18 
Xl>0, 
x e>0, 
integers. 
The final simplex tableau for the related linear programming problem is 
given in Tableau 4.1 (verify), where x 3 and x 4 are the slack variables for 
the first and second constraints, respectively. Since the solution is noninte- 
1 
1 
gral, we must add a cutting plane constraint. We find f~ = ~ and f2 = g, 
so that we choose the first row for constructing the cutting plane. We have 
and 
r 
1 
1 
1 
1 
1 
7 
1 
[-~1~ 
~- 
~. 
Tableau 4.1 
CB 
5 
x 1 
6 
x 2 
5 
6 
0 
0 
X 1 
X 2 
X 3 
X 4 
X B 
1 
1 
1 
0 
g 
g 
17 
0 
1 
}2 
5 
1 
15 
161 
0 
0 
~ 
8 
The Gomory cutting plane is then 
1 
7 
~ 
1 
gX 3 -- gX 4 -t- U 1 -- 
--~. 
Our new tableau is Tableau 4.2. We must now use the dual simplex 
method on Tableau 4.2 to restore feasibility. We obtain Tableau 4.3. 

266 
Chapter 4 Integer Programming 
Tableau 4.2 
CB 
5 
X 1 
6 
x 2 
0 
u I 
5 
6 
0 
0 
0 
X 1 
X2 
X3 
X 4 
U 1 
1 
1 
1 
0 
~ 
~ 
0 
5 
0 
0 
1 
}2 
12 
0 
0 
a 
1 
15 
0 
161 
0 
0 
g 
8 
X B 
Tableau 4.3 
CB 
5 
x 1 
6 
x 2 
0 
x 3 
5 
6 
0 
0 
0 
X1 
X2 
X3 
X4 
Ul 
X B 
1 
0 
0 
1 
1 
4 
2 
10 
0 
1 
0 
1 
3 
3 
0 
0 
1 
7 
8 
2 
0 
0 
0 
1 
1 
40 
Since the solution is still nonintegral, we formulate another Gomory 
cutting plane using the x 2 row. We have 
[11 + 0 - 
1 
1 ~ 
2 
The cutting plane equation is 
1 
1 
0X4 
-- 3Ul 
-~- /'/2 -- 
3" 
We show the new tableau in Tableau 4.4. 
Tableau 4.4 
CB 
5 
X 1 
6 
x 2 
0 
X 3 
0 
/A 2 
5 
6 
0 
0 
0 
0 
X1 
X2 
X3 
--X 4 
U 1 
U 2 
1 
0 
0 
1 
1 
0 
2 
0 
1 
0 
1 
3 
0 
0 
0 
1 
7 
8 
0 
0 
0 
0 
0 
(-f) 
1 
0 
0 
0 
1 
1 
0 
XB 
4 
10 
3 
2 
1 
40 

4.2 Cutting Plane Methods 
267 
Tableau 4.5 
5 
6 
0 
0 
0 
0 
c B 
x1 
x2 
x3 
x4 
Ul 
u2 
XB 
5 
X 1 
1 
0 
0 
1 
0 
3 
3 
6 
X 2 
0 
1 
0 
1 
0 
--2 
4 
0 
X 3 
0 
0 
1 
7 
0 
24 
10 
0 
U 1 
0 
0 
0 
0 
1 
3 
1 
0 
0 
0 
1 
0 
3 
39 
Using the dual simplex method on Tableau 4.4, we obtain Tableau 4.5. 
We have obtained the optimal integer solution 
x 1=3, 
x 2=4, 
z=39. 
The first cutting plane, 
1 
7 
1 
--8X3 
-- 8X4 -+- U1 -- 
4, 
can be expressed in terms of x 1 and x 2 by substituting 
X 3 --" 52- 
10x 1 - 
3x 2 
X 4 --" 18- 
2X 1 -- 3X 2, 
which come from the constraints of the original problem. We obtain 
3x I + 3x 2 + u 1 = 22 
or 
The second cutting plane, 
gives 
(11) 
1 
1 
3U1 + U2 -- 
3, 
X 1 -+- X 2 ___< 7. 
(13) 
In this calculation we used (11) to write u 1 in terms of x 1 and x 2. We 
sketch the original feasible region and the cutting planes in Figure 4.1. 
/x 
Several remarks are in order. For a linear programming problem the 
number of positive components in the solution vector is < m, where A is 
m x s. For an integer programming problem this is no longer true. In fact, 
the optimal value may not occur at an extreme point of the convex region 
defined by the constraints. 
For a pure integer programming problem, the set of feasible solutions is 
the set of points with integer coordinates, called lattice points, lying within 
the convex region defined by the constraints (see Figure 4.1). If the convex 
region is bounded, there are just a finite number of feasible solutions. 
X 1 -~-X 2 ___< -~. 
(12) 

268 
Chapter 4 Integer Programming 
.x 2 
7- ~ 
9 
~lOXl+ 3X 2 = 52 
6-~ 
5-q 
9 
o
~
 
~ 
44~- 
, 
9 
'3~ 
3~ 
9 
9 
9 
"ek 
2Xl+ 3x 2 = 18 
24 
9 
9 
14 
9 
9 
22 
9 
9 
~l+X2 = --g- 
(Cutting plane 1) 
" 
- 
XI+X 2 -- 
(Cutting plane 2) 
.,. 
# 
A 
.~t 
,,, 
I 
~-Xl 
1 
2 
3 
4 
5 
6 
7 
8 
9 
FIGURE 4.1 
However, for problems with several variables it is computationally imprac- 
tical to evaluate the objective function at each of these points and then 
choose the point with the largest value. Instead the cutting plane method 
works from an extreme point and closes in on one of the lattice points at 
which the objective function attains its optimal value. 
The cutting plane algorithm that we just gave has several major draw- 
backs. The integral answer does not appear until the very last step. This is 
unlike the simplex method, in which we can stop at any stage and obtain a 
feasible solution that is better than the previous feasible solutions. In the 
Gomory algorithm, if we stop before the end, we have no integral solution. 
Also, the cutting planes are constructed by using fractional parts of the 
coefficients and the right-hand side of a constraint, and, consequently, 
round-off errors may cause the method to converge slowly. 
Another method used for solving integer programming problems occur- 
ring in common applications avoids this difficulty by introducing a pair of 
constraints based on the integer part of one of the basic variables. This 
method, called the branch and bound method, is described in the Section 
4.3. 
Gomory's Method for Mixed Integer Programming 
We can attempt to solve a mixed integer programming problem in the 
same way as a pure integer programming problem. We use the simplex 
method to obtain the optimal solution to the related linear programming 

4.2 Cutting Plane Methods 
~ 
problem. This will be a solution to the mixed integer programming prob- 
lem if those variables appearing in the basis for the optimal solution that 
are required to have integer values actually do have such values. We 
suppose now that Xri, the ith basic variable in the optimal solution of the 
related linear programming problem is required to be integral and that xBi 
is not an integer. We saw that we may write the i th constraint (9) as 
Let XBi 
or 
Xr i -~- 
E 
tijX j --- XBi. 
(14) 
j nonbasic 
-- [XBi] + fi" We may write (14) as 
Xr i + 
E 
tijx j -- [ XBi ] Jr- L 
j nonbasic 
E tijxj--L + ([XBi]- Xri)" 
(15) 
j~N 
We now divide the set of indices for the nonbasic variables into two 
subsets N § and N-, where 
N += {jlj is the index of a nonbasic variable and tij >__ 0} 
N-= {jlj is the index of a nonbasic variable and tij < 0}. 
Then (15) may be written as 
E 
tijxj + 
E 
tijxj--fi + ([XBi]- Xri)" 
j~N + 
j~N- 
(16) 
Using (16), we want to derive a cutting plane constraint that will cut off 
the current optimal solution to the related linear programming problem 
because in that solution Xr, does not have an integer value. The cutting 
plane constraint must also have the property, as before, that no feasible 
solutions for the mixed integer problem should be cut off. We can 
interpret this condition as saying that if Xr, satisfies (16) and if Xr, is an 
integer, then Xr, must satisfy the cutting plane constraint. 
We consider two cases: the right-hand side of (16) is positive, or it is 
negative. Assume, first, that fi + ([xBi] -Xri) < 0. The quantity in paren- 
theses is an integer by assumption and 0 < fz < 1. We see that [xB~] -Xri 
must be a negative integer for all these assumptions to hold. 
Thus, the largest value that the right-hand side of (16) can have while 
still remaining negative is fi- 
1. Our original constraint (14) implies, in 
this case, 
E 
tijxj + 
E 
tijxj <-- fi- 
1. 
(17) 
j~N + 
j~N- 

~0 
Chapter 4 Integer Programming 
We can make the left-hand side of (17) smaller by removing the first sum, 
since it represents a nonnegative quantity. We obtain 
E 
tijxj ~ fi -- 1. 
yeN- 
Dividing by f/- 1 and reversing the inequality, since f,- 1 is negative, 
and then multiplying by fi yields 
~N 
f i 
> 
(18) 
tijxj 
L. 
j 
-fi 
-1 
- 
Since the first sum in (17) is nonnegative, we may add it to (18) to obtain 
L 
E 
tijXj + ~ 
E 
tijXj >-- fi" 
(19) 
j~N + 
L- 1 j ~ N -  
For the other case, we assume that fi + ([XBi] -- Xr i) >-- O. Using reason- 
ing similar to that in the first case, we see that [xni]--Xr, 
must be 
nonnegative. Thus, our original constraint (14) implies that 
E tijxj + E tijxj >-- fi" 
(20) 
j~N- 
j~N- 
We may replace the second sum in (20) by any larger quantity and still 
maintain the inequality. We have 
L 
tijx j < 0 < 
~ 
tijx j. 
j~s- 
- 
- 
fi- 
l j~s- 
Consequently, (20) implies that 
L 
E 
tijxj + ~ 
E 
tijxj >---fi' 
WeN + 
fi- 1 j e N -  
which is the same as (19). 
We have now shown that if (14) is a constraint in a mixed integer 
problem whose corresponding basic variable is supposed to have an integer 
value but does not, then (19) is satisfied by every vector x that satisfies (14), 
assuming that Xr, is an integer. From (19)we can construct the equation of 
the cutting plane. We reverse the inequality in (19) and add a slack 
variable u/, obtaining 
f/ 
-- E 
tijxj 
E 
tijXj + Ui = --fi" 
(21) 
j~N + 
f l - 1 j ~ N -  
Equation (21) is the Gomory cutting plane. For the current optimal 
solution, xj = 0 if j ~ N § or j ~ N-. Therefore, introducing the con- 
straint (21) gives 
Hi--- --L <0 

4.2 Cutting Plane Methods 
271 
for the current optimal solution, and hence it has been cut off, since u i 
must be nonnegative. 
Note that (21) may be written as 
u, = -L + 
djxj, 
j~N 
tij 
if 
tij >__ 0 
dj = 
fi 
f~- ltij 
if 
t~j<O. 
where 
(22) 
It is possible that some of the nonbasic variables are also required to be 
integer valued. We can refine the cutting plane defined by (21) if we use 
this information. The refinement will simply cut off more solutions that do 
not satisfy the integrality conditions. We write 
tij = [ tij ] + gij" 
Then, instead of the definitions of dj given in (22), we have for the refined 
cutting plane 
tij 
if 
tij >__ 0 
f~- 
1 tij 
if 
tij < 0 
gij 
if 
gii < f i 
fi 
(gij- 1) 
if 
gij>f~ 
f/-1 
and xj may be nonintegral 
and xj may be nonintegral 
and xj must be integral 
and xj must be integral. 
(23) 
/x 
EXAMPLE 2. 
Consider the mixed integer programming problem 
Maximize 
z=5x 1 +6x a 
subject to 
10X 1 + 3X 2 < 52 
2x 1 + 3x 2 _< 18 
x 1 > 0 
and 
integral 
X2>__0. 
The final simplex tableau for the related linear programming problem is 
the same as that in Example 1 and is given in Tableau 4.1. Since the value 
for x 1 is not an integer, we must add a cutting plane constraint. The 
nonbasic variables are x 3 and x 4, the slack variables. Neither is con- 
strained to be an integer. We therefore use (21) to define our cutting 
plane. In this case, i = 1 and 
1 
XBi = 4 + ~ , 

272 
Chapter 4 Integer Programming 
so that fl = ~. From Tableau 4.1 we see that N + 
{3} and N-= 
{4}. The 
cutting plane is 
fl 
--t13x3 
-- fl 
-- 1 t14x4 
+ 
ul 
-- 
--fl 
or 
1 
1 
4 
1 
1 
--8X3 
3 (-8)x4 
+ 
Ul 
= 
4 
4 
or 
1 
1 
8X3 -- ~4X4 "J- Ul --" 
4" 
Putting this constraint into Tableau 4.1, we get Tableau 4.6. 
Tableau 4.6 
CB 
5 
X 1 
6 
x 2 
0 
U 1 
5 
6 
0 
0 
0 
X1 
X2 
X3 
X 4 
U 1 
1 
1 
1 
0 
~ 
~ 
0 
0 
1 
liE 
~ 
0 
@ 
1 
0 
0 
i4 
1 
1 
0 
0 
~ 
~5 
0 
X B 
~7 
1 
161 
We use the dual simplex method to restore feasibility in Tableau 4.7. 
The solution that Tableau 4.7 represents, 
10 
X 1 
-- 4, 
x2 "-- 
3 , 
Z = 
40, 
satisfies the integrality conditions so that it is an optimal solution to the 
given mixed integer programming problem. 
Tableau 4.7 
CB 
X1 
X2 
X3 
5 
6 
0 
0 
0 
X 1 
X 2 
X 3 
X 4 
U 1 
XB 
1 
1 
0 
0 
~ 
1 
4 
4 
2 
10 
0 
1 
0 
~ 
3 
3 
1 
8 
2 
0 
0 
1 
0 
0 
0 
11 
1 
40 
A 

4.2 Cutting Plane Methods 
273 
EXAMPLE 3. 
Consider the mixed integer programming problem 
Z--4X 1 + 5X 2 + 3X 3 
Maximize 
subject to 
3x 1 
2x I + 
+ 
4X 3 _< 10 
X 2 + 
X 3 < 7 
3x I + 4x 2 + 
X 3 < 12 
X 1 >_~ 0, 
X 3 >_ 0, 
and 
integral 
X2 >_~ 0. 
The final tableau of the related linear programming problem is Tableau 
4.8 (verify). Since x 1 and x 3 are constrained to be integer valued, the 
solution represented by Tableau 4.8 is not feasible for the given mixed 
integer problem. We introduce a cutting plane from the first constraint in 
Tableau 4.8 (i = 1). Note that x 4 must also be integer valued, since it is 
the difference of two integers. The set of indices of nonbasic variables is 
N- 
{1, 4, 6}. We calculate the value of dj using (23) for each of these 
5 
1 
indices. The fractional part of x 3 - $ is f~ = ~. 
Tableau 4.8 
CB 
3 
X 3 
0 
x 5 
5 
x 2 
4 
5 
3 
0 
0 
0 
X 1 
X 2 
X 3 
X4 
X 5 
X 6 
X B 
3_ 
0 
1 
• 
o 
o 
5_ 
4 
4 
2 
11 
0 
0 
16 
4 
9 16 
1 
0 
1~ 
0 
1716 
0 
0 
?6 
0 
5_. 
4 
155 
8 
For j = 
1, x I must be integer valued, and we have 
3__0+3 
3 
~- 
a 
or 
gll = 7 >fl" 
Therefore, 
1 
1 
_
_
 
3 _ 
2 
1 
1 
(7 
1) = --7-(- 7) = 7. 
dl- 
1 
1 
2 
2 
For j = 4, x 4 must be integer valued, and we have 
1 
0_1_ 1 
1 
= 
~ 
or 
g14 = 7 <fl- 
Therefore, 
1 
d4 =7. 

274 
Chapter 4 Integer Programming 
For j = 6, x 6 may have any value, t16 -- 0, and therefore 
d 6 -- 0. 
The cutting plane constraint is 
1 
1 
1 
Ul 
-- 
--~ 
+ 
~X 1 + 
~X 4 
or 
1 
1 
m 
1 
4X1 -- 4X4 
-~ Ul 
2" 
We add this constraint to Tableau 4.8 to get Tableau 4.9. This tableau 
represents an optimal but infeasible solution to the related linear pro- 
gramming problem. We apply the dual simplex method to restore feasibil- 
ity. The result is shown in Tableau 4.10, which yields the optimal solution 
5 
37 
X 1 -- 0, 
X 2 = 
~, 
X 3 = 
2, 
Z = 
T" 
Tableau 4.9 
CB 
3 
X 3 
0 
x 5 
5 
x 2 
0 
u 1 
4 
5 
3 
0 
0 
0 
0 
X 1 
X 2 
X 3 
X 4 
X 5 
X 6 
U 1 
XB 
3 
0 
1 
1 
0 
0 
0 
5 
4 
a" 
11 
3 
1 
1 
i?, 
0 
0 
16 
~ 
0 
~7 
9 
1 
0 
8 9 
i-6 
1 
0 
16 
0 
~ 
1 
@ 
, 
~ 
0 
0 
0 
0 
1 
~ 
17 
0 
0 
7 6 
0 
5 
0 
16 
4 
155 
Tableau 4.10 
CB 
3 
X 3 
0 
X 5 
5 
X 2 
0 
X 4 
4 
5 
3 
0 
0 
0 
0 
X1 
X 2 
X 3 
X 4 
X 5 
X 6 
U 1 
X B 
! 
0 
1 
0 
0 
0 
1 
2 
2 
7 
1 
3 
5 
0 
0 
0 
1 
~ 
4 
5 
1 
1 
5 
1 
0 
0 
0 
~ 
~ 
1 
0 
0 
1 
0 
0 
4 
2 
0 
0 
0 
0 
~ 
~ 
37 
8 
4 
4 
2 
A 
4.2 
EXERCISES 
In Exercises 1 and 2 assume that every variable is constrained to be an integer. 
Using the given final simplex tableau, find the equation of the cutting plane. 

4.2 Cutting Plane Methods 
~ 
CB 
X4 
X2 
Xl 
2 
3 
1 
0 
0 
X 1 
X2 
X 3 
X4 
X5 
XB 
1 
1 
5 
0 
0 
~ 
1 
~ 
1 
0 
1 
7 
0 
1 
~ 
3 
L 
13 
1 
0 
- 
0 
8 
4 
8 
5 
7 
47 
0 
0 
z 
0 
~ 
s 
CB 
3 
X 2 
1 
x 4 
4 
x 3 
2 
3 
4 
1 
0 
0 
0 
X 1 
X2 
X3 
X4 
X5 
X6 
X7 
It B 
'- 
~ 
2 
s 
1 
0 
0 
~2 
2 
24 
11 
1 
3 
7 
~ 
0 
0 
1 
~ 
1 
~ 
24 
3 
11 
13 
1 
1 
0 
1 
0 
~ 
"12 
12 
3" 
11 
0 
0 
0 
3512 
376 
296 
1838 
In Exercises 3 and 4 solve the given integer programming problem by the 
cutting plane method and sketch the graph of the set of feasible solutions and the 
cutting planes. 
3. Maximize z = x + y 
subject to 
4. Maximize z = x + 4y 
subject to 
2x + 3y < 12 
2x+ 
y<6 
x>0, 
y> 0, 
integers. 
x+6y<36 
3x + 8y < 60 
x>0, 
y>0, 
integers. 
In Exercises 5-12 solve the given integer programming problem using the 
cutting plane algorithm. 
5. Maximize z = 4x + y 
subject to 
x>O, 
3x + 2y < 5 
2x + 6y < 7 
3x+Ty_<6 
y > 0, 
integers. 

276 
Chapter 4 Integer Programming 
6. Maximize z = x 2 -I- 4x 3 
subject to 
X 1 >___0, 
3x I -- 6x 2 + 9x 3 < 9 
3x a + 2x 2 + 
X 3 _< 7 
x 2 >~ O, 
x 3 >_ O, 
integers. 
7. Maximize z = 5x + 2y 
subject to 
12x - 
7y < 84 
6x + 10y < 69 
x>0, 
y>0, 
integers. 
8. Maximize z = x I + 2x 2 + X 3 -'[-X 4 
subject to 
2x I + 
x 2 + 3x 3 + 
X 4 __< 8 
2x 1+3x 2 
+4x 4< 12 
3x I + 
x 2 + 2x 3 
< 18 
xj>0, 
j= 
1,2,3,4, 
integers. 
9. Repeat Exercise 3 under the assumption that only x must be an integer. 
10. Repeat Exercise 6 under the assumption that only x l and x 3 must be integers. 
11. Repeat Exercise 8 under the assumption that only xl and x 2 must be integers. 
12. Repeat Exercise 6 under the assumption that only x2 must be an integer. 
4.2 
PROJECT 
In Step 2 of the procedure for generating a cutting plane constraint, we gave the 
following heuristic rule for choosing the variable to be affected by the cutting 
plane: from among the basic variables constrained to be integers, choose the basic 
variable whose value has the largest fractional part. This arbitrary rule was included 
so that there was an unambiguous procedure for developing the cutting plane 
constraint. But other rules are possible. Consider the following problem. 
Maximize 
z=5x+2y 
subject to 
6x- 
15y < 24 
6x + 10y < 69 
3x + 10y < 60 
x>0, 
y>0, 
integers. 
(a) Sketch a graph of the feasible region of the associated linear programming 
problem and identify the feasible and optimal solutions of the given integer 
programming problem. 

4.3 Branch and Bound Methods 
277 
(b) Solve the integer programming problem using the heuristic rule as given in 
Step 2. (Warning: the computation is lengthy.) 
(c) Solve the integer programming problem using the following as the heuristic 
rule: from among the basic variables constrained to be integers, choose the basic 
variable whose value has the smallest fractional part. 
(d) What other rules can you propose for choosing the basic variable to be 
affected by the cutting plane? 
4.3 BRANCH AND BOUND METHODS 
If the set of feasible solutions to the related linear programming 
problem of a mixed integer programming problem is bounded, then the 
integer valued variables can take on only finitely many values in this 
region. We had previously discussed solving a linear programming problem 
by enumerating the extreme points of the set of feasible solutions and then 
choosing an optimal solution from among these. We dismissed this brute 
force method in favor of the simplex algorithm that listed only some of the 
extreme points and chose an order for the list in which the value of the 
objective function improved each time. However, using cutting planes and 
the simplex algorithm can involve very lengthy computations. Thus, it may 
be advantageous to again consider some enumerating technique. 
We examine the possibility of cleverly enumerating the integer values 
that should be considered for a mixed integer programming problem. The 
clevemess is needed so that the task does not become overwhelming. One 
technique that is used is that of implicit enumeration. This involves 
generating a list of some of the feasible integral solutions and saving the 
best solution in the list for comparison with lists that will be generated 
subsequently. 
A set S of feasible solutions to a mixed integer programming problem is 
said to be implicitly enumerated if S does not contain any solution that is 
better than the best currently known solution. Our strategy will be to 
partition the set of feasible solutions into several subsets and then to 
dismiss many of these subsets because they are implicitly enumerated. We 
can derive certain relations from the constraints of the problem and the 
value of the best current solution. These relations will be violated by any 
set of solutions that is implicitly enumerated; that is, these relations give 
conditions that are necessary for a solution to satisfy if it is to improve the 
value of the objective function. If many subsets of the set of feasible 
solutions can be implicitly enumerated, we can greatly limit the number of 
solutions that have to be explicitly examined. 
It will be easiest to describe the enumeration technique if we initially 
limit ourselves to a zero-one programming problem. Consider such a 
problem and assume that it has n variables. The set of all feasible 

~71] 
Chapter 4 Integer Programming 
solutions, S, can be partitioned into two subsets, S o and S1, where 
S0 = {x ~ Six1 = 0} 
and 
S 1 = (x ~ Six I = 1}. 
Likewise, both So and S 1 can be partitioned into two subsets--say, So is 
divided into S00 and S0a, and S 1 is divided into $10 and Sll. We define S00 
by 
Similarly, 
Soo = {x ~ Six 1 = 0 
and 
X 2 "- 0}. 
Sol = {x ~ Six 1 = 0 
and 
X 2 = 1} 
S~o = {x ~ S lxl = 1 
and 
X 2 ~- O} 
$11 = {x~SIx i = 1 
and 
x 2 = 1}. 
Each of the four subsets Sij , i, j = 0 or 1, can be partitioned further. We 
can describe this procedure with a tree diagram, as shown in Figure 4.2. 
The numbered circles are called nodes; the lines connecting them are 
called branches. Node 1 represents the set S of all feasible solutions. The 
partitioning of S into So and $1 is represented by the branches leading to 
nodes 2 and 3. Node 2 represents S O and node 3 represents S 1. 
A sequence of nodes and branches from node 1 to any other node k is 
called a path to node k. Each branch represents the imposition of one 
constraint on the variables (setting one variable equal to 0 or 1). Node k 
FllliiflE 4,2 

4.3 Branch and Bound Methods 
~'i~ 
represents the set of all solutions to the original constraints that also 
satisfy the constraints imposed by the branches in the path to node k. For 
example, node 9 represents 
{x~SIXl =0, 
x2=0, 
x3 = 1}. 
If two nodes are connected by a path, the lower node represents a subset 
of the solutions that are represented by the higher node. 
If all possible values for the n variables in a zero-one programming 
problem are enumerated, we have to generate 2 n paths. The bottom node 
of each path would correspond to exactly one value for x, and this value 
may be infeasible depending on the nature of the constraints. Our enu- 
meration strategy will try to eliminate as many of these paths as possible 
from consideration. We describe one method that might be used to 
perform this elimination. 
Suppose our zero-one programming problem has an optimal solution 
for which the value of the objective function is z*. Assume that we have 
found a feasible solution to the problem that has an objective function 
value of z c. Obviously, z L < z*. Keep in mind that z* is unknown and is 
precisely what we are trying to find. At this point we know that we do not 
have to consider any set of solutions whose objective function values are 
smaller than z L. Consider the set of solutions to the zero-one problem 
that is represented by node k. We can find an upper bound z~ for the 
values of the objective function at the solutions represented by node k by 
solving a related linear programming problem. If 
Z k ___< Z L 
then no path that includes node k will yield an improved solution; the 
objective function values will all be no larger than z k. Consequently, we 
can eliminate all paths through node k without explicitly evaluating the 
objective function at each solution on the path. This set of solutions has 
been implicitly enumerated. In this case, node k is called a terminal node. 
Node k is also called a terminal node if there are no feasible solutions 
satisfying the constraints imposed at node k. Besides the original con- 
straints and those imposed by the path leading to node k, we often add the 
constraint 
cTx ~_~ Z L 
since we are interested only in solutions that are better than the current 
best feasible solution. Finally, node k is called a terminal node if it 
represents a single feasible solutionmthat is, if all the variables have been 
assigned a unique value. 
Both branch and bound methods and methods that search in the tree of 
solutions generate the nodes of the tree until all paths end in terminal 

~0 
Chapter 4 Integer Programming 
nodes. At that point all solutions will have been explicitly or implicitly 
enumerated and the best solution found is an optimal solution. The 
difference between the two types of methods is the manner in which the 
nodes are generated. Search methods follow a path until it ends at a 
terminal node before examining another path. Branch and bound methods 
examine the nodes in a less straightforward manner. They may generate, 
more or less simultaneously, many different paths. Branch and bound 
methods use more computer storage than search methods do, but they 
have much more flexibility in examining the nodes and thus may be faster. 
We will give the details of one branch and bound method for solving an 
arbitrary mixed integer programming problem. This method was developed 
in 1966 by R. J. Dakin and is a modification of the Land-Doig method. It 
is widely used in the computer codes for integer programming. 
Consider the mixed integer programming problem 
Maximize 
z = cTx 
subject to 
Ax=b 
x>0 
xj, j ~ I, 
integral, 
(1) 
whereAis m xs, bis m x 1, cis sx 1, andxis sx 1. 
Dakin's method will generate a tree very similar to the one we formed 
for the zero-one programming problem. For each node there will be two 
branches to check. If neither of the branches ends in a terminal node, the 
method follows the most promising branch. The other branch is called 
dangling and must be examined before the algorithm terminates. Dakin's 
method proceeds as follows: 
Step 1 (Initial Solution). 
Solve the problem given by (1) as a linear 
programming problem, ignoring the integrality restrictions. If all xj, j ~ I, 
have integral values, we are done. If not, go to Step 2. 
Step 2 (Branching Variable Selection). 
Choose, from among those vari- 
ables, xj, j ~ I, that do not have integral values at this node, one variable 
to be used to form the branching constraints. An easily implemented rule 
for this choice is to use the variable whose value has the largest fractional 
part. There are other more complicated rules that may even involve one 
iteration of the dual simplex method to determine which variable to 
choose. The xj selected must be a basic variable; otherwise, its value would 
be zero. Suppose it is the i th basic variable in the final tableau for the 
node, so that its value is xBi. We can write 

4.3 Branch and Bound Methods 
~1 
where 0 < f/< 1. Since xj must have an integral value, it must satisfy 
either 
Xj <_~ [ XBi ] 
(2) 
or 
xj >__ [ XBi ] + 1. 
(3) 
Step 3 (Formation of New Nodes). We create two new mixed integer 
problems represented by the node under consideration in Step 2. One 
problem is formed by adding constraint (2) and the other problem is 
formed by adding constraint (3). Solve each of these problems as a linear 
programming problem using the dual simplex method. 
Step 4 (Test for Terminal Node). Each of the nodes formed in Step 3 
may be a terminal node for one of two reasons. First, the problem 
represented by the node may have no feasible solutions. Or the values of 
xj, j ~ I, are all integers. In the former case, label the nodes as terminal 
nodes and go to Step 5. In the latter case, besides labeling the nodes, 
compare the values of the objective function with the current best value. If 
the objective function value for the new node is better, replace the current 
best value with it. Go to Step 5. 
Step 5 (Node Selection). 
(a) If both nodes were terminal nodes in Step 4, the next node to be 
considered is the next one on the list of dangling nodes. If this dangling 
node has an objective function value larger than the current best value, 
then use this node and go to Step 2. Otherwise, check the next node in the 
list of dangling nodes. When the list of dangling nodes is exhausted, stop. 
The current best value is the optimal solution. 
(b) If exactly one node in Step 4 was terminal, use the nonterminal node 
and go to Step 2. 
(c) If both nodes in Step 4 were nonterminal, we choose the more 
promising one. Usually the node with the largest objective function value is 
considered more promising. The other node is recorded in the list of 
dangling nodes to be considered later. 
EXAMPLE 1. 
Consider the pure integer programming problem 
Maximize 
z=7x 1+3x 2 
subject to 
2x 1 + 5x 2 _< 30 
8X 1 -]" 3X 2 < 48 
x~ >_0, 
x 2>_0, 
integers. 

282 
Chapter 4 Integer Programming 
We solve the related linear programming problem and obtain the final 
tableau shown in Tableau 4.11. From this tableau we see that an optimal 
solution is 
x 1 -" 4 7, 
x e = 4 4, 
z = 431~. 
We choose x I as the branching variable, since it has the largest fractional 
part. The constraints to be added are 
X 1 _~ 4 
and 
x I >__ 5. 
Tableau 4.11 
CB 
3 
x 2 
7 
x I 
7 
3 
0 
0 
x1 
x 2 
x 3 
x 4 
0 
1 
1~ 
~7 
3 
5 
1 
0 
34 
34 
3 
29 
0 
0 
34 
34 
X B 
741 
17 
To carry out Step 3 we add each of these constraints in turn to the final 
tableau in Step 1. We get the tableaux shown in Tableaux 4.12 and 4.13. 
For Tableau 4.12 we have written 
75 
X 1 -- -~ d- ~4X3 -- 3--~X4 
from the second row of Tableau 4.11 and then introduced a slack variable, 
u l, so that our new constraint is 
X 1 +U 
1 =4 
or 
3 
5 
=4- 
75 
7 
~-~X 3 -- 
X4 -{- U 1 
"i'ff = 
17" 
In the same way the new constraint for Tableau 4.13 becomes 
75 ~__ 
10 
34X 3 + 
3-~X4 "~- U 1 --- --5 
"1" "i'ff 
17" 
Tableau 4.12 
,I, 
CB 
3 
X 2 
7 
X 1 
0 
U 1 
7 
3 
0 
0 
0 
X 1 
X2 
X3 
X 4 
U 1 
4 
0 
1 
i-7 
i~7 
0 
3 
5 
0 
1 
0 
34 
34 
3 
@ 
1 
0 
0 
34 
XB 
3 
29 
0 
71~ 
0 
0 
3-4 
34 

4.3 Branch and Bound Methods 
283 
Tableau 4.13 
CB 
3 
X 2 
7 
x 1 
0 
U 1 
7 
3 
0 
0 
0 
X 1 
X2 
X3 
X 4 
U 1 
XB 
0 
1 
4 
17- 
0 
72~7 
3 
75 
1 
0 
34 
34 
0 
17 
5 
1 
10 
0 
0 
34 
17 
3 
29 
0 
71~ 
0 
0 
34 
34" 
We now apply the dual simplex method to each of these tableaux to obtain 
Tableaux 4.14 and 4.15, respectively. 
Tableau 4.14 
CB 
3 
X 2 
7 
x 1 
0 
x 4 
7 
3 
0 
0 
0 
X1 
X2 
X3 
X4 
U 1 
XB 
1 
0 
2 
22 
0 
1 
~ 
5 
5 
1 
0 
0 
0 
1 
4 
3 
14 
0 
0 
~ 
1 
35 
5 
3 
0 
~ 
206 
0 
0 
~ 
5 
Tableau 4.15 
CB 
3 
x 2 
7 
x 1 
0 
x 3 
7 
3 
0 
0 
0 
x 1 
x 2 
x 3 
x 4 
u 1 
XB 
1 
8 
8 
0 
1 
0 
~ 
~ 
1 
0 
0 
0 
1 
5 
5 
34 
20 
0 
0 
1 
~ 
3 
3 
0 
0 
0 
1 
1 
43 
At this point we have the tree in Figure 4.3. The objective function value 
in node 3 is larger than that in node 2. Consequently, node 2 is recorded in 
our list of dangling nodes and we continue from node 3. We add the 
constraints 
X 1 <2 
and 
X 2 >_ 3 
to the problem represented by node 3 to form two new problems. As 
before, we write 
1 
8 
8 
X2 -- 
---~X 4 -- ~U 1 + 

284 
Chapter 4 Integer Programming 
9 
Q 
z = 4317 ~ 
x 1 = 4 7 
(Tableau 4.11 ) 
X 2 =4 4 
1 
Q 
z = 41~- 
z = 43 
x I = 4 
(Tableau 4.14) 
x 1 - 5 
2 
X~ =22 
x 2 = 4~- 
FIGURE 4.3 
(Tableau 4.15) 
from the first row of Tableau 4.15. Then by adding a slack variable to each 
of the constraints, we have 
X 2 -~- U 2 -- 2 
and 
-x 2 %- u 2 -- 
-3 
or 
1 
8 
8 
2 
(4) 
~X 4 -- ~U 1 -~- U 2 -- 2 - 
3 - 
3 
and 
1 
8 
8 
1 
3X4 + 
gU 1 + U 2 = 
--3 
+ ~ = 
3. 
(5) 
We add each of constraints (4) and (5) to Tableau 4.15 to form Tableaux 
4.16 and 4.17. We use the dual simplex method on each of these tableaux 
to try to restore feasibility. Tableau 4.18 is obtained from Tableau 4.16 by 
this process. However, Tableau 4.17 represents an infeasible solution that 
satisfies the optimality criterion, because there are no negative entries in 
the pivotal row (labeled by u2). Thus, Tableau 4.17 represents a terminal 
node. 
Tableau 4.16 
CB 
3 
X 2 
7 
x 1 
0 
X 3 
0 
u 2 
7 
3 
0 
0 
0 
0 
X1 
X2 
X 3 
X4 
Ul 
U 2 
XB 
1 
8 
0 
8 
0 
1 
0 
~ 
~ 
1 
0 
0 
0 
1 
0 
5 
0 
0 
1 3 5  
~ 
0 
203 
1 
0 
0 
0 
x 
1 
3 
0 
0 
0 
1 
1 
0 
43 

4.3 Branch and Bound Methods 
285 
Tableau 4.17 
CB 
3 
X 2 
7 
x 1 
0 
x 3 
0 
u 2 
7 
3 
0 
0 
0 
0 
X 1 
X2 
X3 
X4 
U 1 
U 2 
1 
8 
0 
1 
0 
~ 
x 
0 
1 
0 
0 
0 
1 
0 
0 
0 
1 
5 
34 
0 
3 
3 
1 
8 
0 
0 
0 
~ 
~ 
1 
0 
0 
0 
1 
1 
0 
XB 
43 
Tableau 4.18 
CB 
3 
X 2 
7 
x 1 
0 
x 3 
0 
u 1 
7 
3 
0 
0 
0 
0 
X 1 
X2 
X3 
X4 
U 1 
U 2 
0 
1 
0 
0 
0 
1 
1 
0 
3 
1 
0 
0 
~ 
8 
1 
0 
0 
1 
~ 
0 
1 
3 
0 
0 
0 
~ 
8 
0 
0 
0 
7 
0 
3 
XB 
171 
At this point we have the tree in Figure 4.4. Note that in node 3, x 1 had 
an integer value, but in node 4, the value of X l is no longer an integer. 
Thus, we cannot expect a variable once it has an integer value to continue 
having an integer value. 
We now use Step 2 on node 4, adding the constraints 
X 1 _~ 5 
and 
x 1 >_ 6. 
Tableau 4.19 
CB 
3 
X 2 
7 
X 1 
0 
X 3 
0 
U 1 
0 
U 3 
7 
3 
0 
0 
0 
0 
0 
Xl 
X2 
X3 
X4 
gl 
U2 
U3 
0 
1 
0 
0 
0 
1 
0 
1 
0 
3 
1 
0 
0 
~ 
~ 
0 
1 
0 
0 
1 
~ 
0 
1~ 
0 
1 
1 
3 
0 
0 
0 
0 
~ 
8 
~ 
3 
1 
0 
0 
0 
0 
7 
0 
3 
0 
171 
0 
0 
0 
~ 
~ 
4 
XB 

286 
Chapter 4 Integer Programming 
9 
Q 
z = 43]0 
x] = 47 
(Tableau 4. l l ) 
x 2 = 4~7 
z-4,  
Q 
z-4 
 
x I - 4 
(Tableau 4.14) 
x ) = 5 
x 2 -42 
Dangling node 
Q 
(Tableau 
x 2 =2} 
xl = 588 
(Tableau 4.18) 
Terminal node 
x 2 =2 
4.15) 
FIGURE 4.4 
4.17) 
Tableau 4.20 
CB 
3 
X 2 
7 
x 1 
0 
x 3 
0 
u 1 
0 
u 3 
7 
3 
0 
0 
0 
0 
0 
X 1 
X 2 
X 3 
X 4 
U 1 
U2 
U3 
X B 
0 
1 
0 
0 
0 
1 
0 
2 
1 
0 
3 
0 
21 
1 
0 
0 
~ 
8 
1 
0 
0 
1 
X 
0 
17 
0 
19 
1 
1 
3 
0 
1 
0 
0 
0 
~ 
a 
0 
(_3") 
1 
O 
O 
O 
81 
"43 
7 
0 
3 
171 
0 
0 
0 
~ 
~ 
0 
4 
We obtain Tableaux 4.19 and 4.20 after introducing the slack variable u 3 
and rewriting these constraints. We use the dual simplex algorithm on 
each of these tableaux to try to restore feasibility. Tableau 4.21 is obtained 
from Tableau 4.19, and Tableau 4.22 is obtained from Tableau 4.20 by this 
process. Since both Tableaux 4.21 and 4.22 give integer solutions, they 
correspond to terminal nodes. The solution 
X 1 = 
6, 
x 2 = 0, 
z = 42 

4.3 Branch and Bound Methods 
287 
Tableau 4.21 
CB 
3 
X 2 
7 
x 1 
0 
x 3 
0 
u 1 
0 
x 4 
7 
3 
0 
0 
0 
0 
0 
X 1 
X2 
X3 
X4 
Ul 
U2 
U3 
0 
1 
0 
0 
0 
1 
0 
1 
0 
0 
0 
0 
0 
1 
0 
0 
1 
0 
0 
5 
2 
0 
0 
0 
0 
1 
0 
1 
0 
0 
0 
1 
0 
3 
8 
0 
0 
0 
0 
0 
3 
7 
XB 
2 
5 
10 
0 
2 
41 
Tableau 4.22 
CB 
3 
X 2 
7 
x 1 
0 
X 3 
0 
U 1 
0 
U 2 
7 
3 
0 
0 
0 
0 
0 
X 1 
X 2 
X 3 
X 4 
U 1 
U2 
U3 
8 
1 
0 
0 
0 
1 
0 
1 
0 
0 
0 
0 
0 
1 
5 
0 
0 
1 
~ 
0 
0 
34 
0 
0 
0 
0 
1 
0 
1 
1 
8 
0 
0 
0 
3 
0 
1 
3 
0 
0 
0 
1 
0 
0 
1 
X B 
0 
6 
18 
1 
2 
42 
from Tableau 4.22 is the better one, since it gives a larger objective 
function value. We then check the list of dangling nodes to see whether 
other branches of the tree must be pursued. The only dangling node has 
1 
the objective function value z = 417, which is smaller than the value 
obtained in Tableau 4.22. Branching at this dangling node will not increase 
the objective function value. Therefore, we have found an optimal solution 
to the original problem: 
X 1 -- 6, 
x 2 = 0, 
z = 42. 
The tree of problems that was constructed by the branch and bound 
algorithm for this example is given in Figure 4.5. 
/x 
The following problem shows an application of the branch and bound 
algorithm to a mixed integer programming problem. 
EXAMPLE 2. 
Consider the mixed integer programming problem 
Maximize 
z = 2x I + X 2 
'[- 3x 3 
subject to 
4x 1 + 3x 2 - 3x 2 < 6 
2x 1 + 3x 2 + 3x 3 < 4 
xj>_0, 
j- 
1, 2, 3; 
Xl, X3, 
integers. 

288 
Chapter 4 Integer Programming 
9 
63 
Q 
z = 4311---07 
x I = 47 
(Tableau 4.11 ) 
x 2 = 4~147 
1 
~ 
z = 43 
z=41~ - 
x z = 4 
(Tableau 4.14) 
x I " 5 
2 
x 2 = 4~- 
Q 
(Tableau 4.15) 
x 1 = 2~ 
x I = 51 
(Tableau 4.18) 
x2=2 
z=41 
Q 
z=42 
x I = 5 
(Tableau 4.21 ) 
x I = 6 
(Tableau 
x 2 = 2 
x 2 = 0 
Optimal solution 
4.22) 
FIGURE 4.5 
(Tableau 4.17) 
Solution. The branch and bound method yields the tree shown in 
Figure 4.6. Observe that node 4 represents a possible solution to the 
problem since x a and x 3 both have integer values. However, node 5 gives 
an objective function value that is larger than the one in node 4, so we 
might have a solution in which x a and x3 have integer values with an 
1 
objective function value greater than 3x. Thus, we must generate nodes 6 
and 7. We do not have to go beyond node 6 since its objective function 
value is smaller than that of node 4 and any further branching will not 
increase the value of the objective function. Thus, node 4 represents the 
optimal solution to the given problem: 
1 
1 
X 1 = 
O, 
X 2 = 
~, 
X 3 = 1, 
Z = 3~. 
A 

4.3 Branch and Bound Methods 
289 
| 
~ I 
(~I 
z=4 
Xl=0 
x2=0 
x3=l" ~ 
z=4 
"1 
@1 
XI'- 89 
X2=0 
1 
x3=l 
, 
z=3-~ 
@ 
z=4 
X 1 =0 
XI = 1 
x2 3 
x 2 =0 
X 3 = 1 
X3 = 2 
| 
Not feasible 
z=3 
(~ 
x I = 189 
Not feasible 
x2=0 
x3=0 
FIGURE 4.6 
4.3 
EXERCISES 
In Exercises 1-10 solve the indicated mixed integer programming problem in 
Section 4.2 and draw the tree of problems used in finding the solution. 
1. Exercise 3 
2. Exercise 4 
3. Exercise 5 
4. Exercise 6 
5. Exercise 7 
6. Exercise 8 
7. Exercise 9 
8. Exercise 10 
9. Exercise 11 
10. Exercise 12 

290 
Chapter 4 Integer Programming 
In Exercises 11-14, use either the cutting plane method or the branch and 
bound method to solve the indicated mixed integer programming problem in 
Section 4.1. 
11. Exercise 1 
12. Exercise 2 
13. Exercise 4 
14. Exercise 5 (If a computer program is available for solving linear programming 
problems, make use of it.) 
4.3 
PROJECT 
Consider the air filter manufacturer in Section 3.5, Project 1. Suppose that there 
is an additional specification for the purposes of standardization: the diameters of 
the main chamber and exit duct must be integer values. Rework the problem with 
this additional specification and comment on its economic impact on the company. 
4.4 COMPUTER ASPECTS (OPTIONAL) 
The computation of a solution to a mixed integer programming problem 
can be an extremely difficult task. We have discussed two algorithms for 
finding such a solution. Experimental evidence indicates, however, that 
there are integer programming problems for which each of these algo- 
rithms would be very inefficient. There have been many other algorithms 
reported in the literature, along with several papers comparing their 
effectiveness. Most researchers now feel that there will never be a univer- 
sally good algorithm for mixed integer programming. This is perhaps 
disappointing, because in the linear programming case the simplex algo- 
rithm with modifications to make it numerically stable is a universal 
algorithm, and one might expect that restricting the range of some of the 
variables to integer values would simplify the computations. In fact, just 
the opposite happens. 
The state of the art for computational methods for integer program- 
ming is as follows. There are commercially available codes, just as for the 
simplex method, virtually all of which use a branch and bound technique. 
At various universities there are also experimental implementations of 
various modifications of the branch and bound algorithm and of cutting 
plane techniques. Several authors have proposed lists of problems against 
which one can test an algorithm. In running these tests, it is found that 
each algorithm is successful on some problems but no algorithm is success- 

4.4 Computer Aspects (Optional) 
291 
ful on all problems. Worse than this is the discovery that by simply 
renumbering the variables or reordering the constraints of a problem, an 
algorithm that solved the original problem well may perform poorly on the 
reformulated problem. 
Recently an extremely important development has taken place in the 
area of preprocessing a mixed integer programming problem to reduce its 
complexity. The problem and an initial solution are assessed as to their 
closeness to be a pure integer model. This approach has succeeded in 
turning many difficult problems into manageable ones. For example, 
Ketron Management System's MIPIII does extensive preprocessing using 
these ideas. 
The size of a mixed integer programming problem that can be success- 
fully solved in a reasonable amount of computer time is much smaller than 
that for a linear programming problem. Mixed integer problems that have 
thousands of integer variables, hundreds of thousands of continuous vari- 
ables, and tens of thousands of constraints can be solved successfully. Of 
course, as the computational speed of new computers increases, larger 
problems will become feasible. 
To be a successful problem solver of mixed integer programming 
problems then, one must be able to do much more than provide data to a 
computer program. First, one must be skilled at formulating problems and 
must know alternative formulations so that when the problem is given to 
the computer it will be in a form on which the code will work efficiently. 
Several factors that should be considered while constructing the model are 
as follows. 
(a) Having a large number of variables constrained to be integers may 
cause the solution procedure to be very complex and lengthy. Each integer 
variable can cause branches in a branch and bound algorithm. A large 
number of these variables will cause a very large tree of problems to be 
constructed. It will require an extensive amount of storage for the tree and 
a large amount of time to process the branches. One way of limiting the 
number of integer variables is to agree that an almost optimum solution 
will be acceptable. Having a solution 10% less than optimal would not be 
bad if uncertainties in input parameters would cause almost that much 
deviation anyway. After settling for an almost optimal solution, one can 
argue that variables with large values need not be constrained to be 
integers. Simply take the noninteger values for these variables and round 
off. Most likely, rounding off will not change the objective function value 
greatly. For example, if a variable represents the number of workers but its 
magnitude will be at least 1000, allowing it to be noninteger and rounding 
off will make an error of less than 0.05% in the variable, and if the model 
is well formulated, this error should not propagate to a very large error in 

292 
Chapter 4 Integer Programming 
the objective function. In fact, some suggest that any variable whose value 
will be larger than 20 should not be constrained to be an integer. 
(b) Tightly bounding integer variables will allow a branch and bound 
algorithm to find terminal nodes more quickly as the extra constraints 
become inconsistent with the bounds. Geometrically, the tight bounds 
reduce the number of lattice points within the convex set of feasible 
solutions to the related linear programming problem that must be consid- 
ered. 
(c) It helps to formulate the problem as a network problem or special- 
type (transportation, assignment) integer programming problem to take 
advantage of the special algorithms that are available. These special 
algorithms exist because they are more efficient than the general algorithm 
on a particular type of problem. Even though the combinatorial problems 
arising in network theory can be difficult, it is generally better to make use 
of the additional information or structure that the problem has. 
Second, the user should be prepared to monitor the progress of the 
computer solution by dividing the problem into stages and carefully 
interpreting the output from each stage. If the user suspects that a 
particular algorithm is not proceeding quickly enough to the solution, he 
or she may choose to change algorithms for a few iterations. Or perhaps 
the structure of the problem and the output will indicate that a different 
selection of decision rules for the algorithm will speed the process. 
Finally, the user should be familiar with the heuristic methods that are 
applicable to his or her type of problem. Perhaps one of these methods will 
determine a good starting solution or good decision rules. 
Further Reading 
Dakin, N. J. "A Tree Search Algorithm for Mixed Integer Programming Problems." Comput. 
J. 9 (1966), 250-255. 
Garfinkel, Robert S., and Nemhauser, George L. Integer Programming. Wiley, New York, 
1972. 
Geoffrion, A. M., and Marsten, R. E. "Integer Programming Algorithms: A Framework and 
State-of-the-Art Survey." Management Sci. 18 (1972), 465-491. 
Gilmore, P. C., and Gomory, Ralph E. "A Linear Programming Approach to the Cutting 
Stock Problem." Operations Res. 9 (1961), 849-859. 
Gilmore, P. C., and Gomory, Ralph E. "A Linear Programming Approach to the Cutting 
Stock Problem--Part II." Operations Res. 11 (1963), 863-888. 
Gilmore, P. C., and Gomory, Ralph E. "Multistage Cutting Stock Problems of Two or More 
Dimensions." Operations Res. 13 (1965), 94-120. 
Kastning, Claus. Integer Programming and Related Areas: A Classified Bibliography. Lecture 
Notes in Economics and Mathematical Systems, No. 128. Springer-Verlag, New York, 
1976. 
Lawler, Eugene L. (Ed.). The Traveling Salesman Problem: A Guided Tour of Combinatorial 
Optimization. Wiley, New York, 1985. 

4.4 Computer Aspects (Optional) 
2g~ 
Nemhauser, George L., and Wolsey, Laurence A. Integer and Combinatorial Optimization. 
Wiley, New York, 1988. 
von Randow, Rabe, (Ed.). Integer Programming and Related Areas: A Classified Bibliography: 
1978-1981. Lecture Notes in Economics and Mathematical Systems, No. 197. Springer- 
Verlag, New York, 1982. 
von Randow, Rabe (Ed.). Integer Programming and Related Areas: A Classified Bibliography: 
1981-1984. Lecture Notes in Economics and Mathematical Systems, No. 243. Springer- 
Verlag, New York, 1985. 
Salkin, Harvey M. Integer Programming. Addison-Wesley, Reading, MA, 1975. 
Taha, Hamdy A. Integer Programming: Theory, Applications. Academic Press, New York, 1975. 

Special Types 
of Linear 
Programming 
Problems 
I 
N THIS CHAPTER we study a number of special types of linear 
programming problems. These problems arise in transportation sys- 
tems, in communication systems, in pipeline systems, in electrical 
networks, in the planning and scheduling of projects, and in many other 
applications. Each of these problems can be formulated and solved as a 
linear programming problem. However, because of their special structure 
these problems can be more effectively solved by other methods that have 
been specifically developed to handle them. 
5.1 THE TRANSPORTATION PROBLEM 
We have previously considered two examples of the transportation 
problem~Example 3 in Section 1.1 and Example 1 in Section 4.1. We 
present another example here that will be used to indicate the ways in 
which the simplex method can be adapted to the special structure of this 
problem. Our eventual goal is the transportation algorithm. 
295 

~ 
Chapter 5 
Special Types of Linear Programming Problems 
EXAMPLE 1. The plastic manufacturing company described in Example 
3 of Section 1.1 has decided to build a combination plant-warehouse in 
San Antonio. This expansion will give it three plants and four warehouses. 
The following shipping costs have been determined for the new plant and 
warehouse: 
To 
From 
Los Angeles 
Chicago 
New York City 
San Antonio 
Salt Lake City 
~ 
~ 
m 
6 
Denver 
~ 
~ 
5 
San Antonio 
7 
6 
8 
1 
The San Antonio plant can supply 100 tons per week, and the warehouse 
needs 120 tons per week to meet its demand. We want to determine how 
many tons of sheet polyethylene should be shipped from each plant to 
each warehouse to minimize the transportation cost. 
MATHEMATICAL MODEL. The cost matrix is given by 
Los 
New York 
San 
Angeles 
Chicago 
City 
Antonio 
[5 
7 
9 
6] 
Salt Lake City 
C = 
6 
7 
10 
5 
Denver 
(1) 
7 
6 
8 
1 
San Antonio, 
where we have labeled the rows with the points of origin and the columns 
with the destinations. The supply vector is 
s = 
140 
100 
and the demand vector is 
d 
__. 
100 
60 
80 " 
120 

5.1 The Transportation Problem 
~ 
We let Xij denote the amount shipped from the ith plant to the jth ware- 
house, where the plants and warehouses are numbered as in Equation (1). 
Our model is to minimize 
subject to 
3 
4 
Z = E 
E CijXij 
i=lj=l 
4 
E Xij ~ Si, 
i = 1,2,3 
j=l 
3 
E xij ~___ 4, 
j = 1,2,3,4 
i=1 
Xij ~_~ O, 
integers. 
(2) 
Clearly the company must be able to provide a supply of polyethylene at 
least equal to the demand. In our example, we have 
3 
4 
Es~ = 360 = E4" 
i=1 
j=l 
/x 
Later we will indicate what to do if supply exceeds demand. We will say 
that the model is infeasible if demand exceeds supply. The student may 
show that, when demand equals supply, each of the constraints in (2) is an 
equality. 
We will now develop an algorithm for solving the general transportation 
problem 
Minimize 
Z = E 
CijXij 
i=lj=l 
subject to 
n 
E xij -- Si, 
j=l 
rn 
~ xij = dj, 
i=1 
i= 1,2,...,m 
j= 1,2,...,n 
(3) 
m 
Esi= 
i=1 
j=l 
(4) 
xij>O, i= 1,2,...,m, 
j= 1,2,...,n. 

~1] 
Chapter 5 Special Types of Linear Programming Problems 
This form of the transportation problem has the following properties. 
1. The problem always has a feasible solution (Exercise 17). 
2. The set of feasible solutions is bounded (Exercise 18). 
3. Because of (4), one of the m + n constraints in (3) is redundant 
(Exercise 15). 
We may substitute the values for the coefficients in our model and write 
it as 
Minimize 
Z = 5Xll + 7X12 + 9X13 4- 6X14 + 6X21 
+ 7X22 + 10X23 + 5X24 -~- 7X31 -4- 6X32 4- 8X33 -~-X34 
subject to 
Xll d- X12 -4- X13 d- X14 
Xll 
-F X21 
X21 -~- X22 -4- X23 -4- X24 
X 12 
"4- X22 
X 13 
-I- X23 
X14 
"~- X24 
= 
120 
= 140 
X31 + X32 + X33 + X34 = 100 
+ x31 
= 100 
+ X32 
-- 60 
+ X33 
-- 80 
q- X34 = 120 
(5) 
xij >0, 
integers, 
i= 1,2,3, 
j= 1,2,3,4. 
Note that each variable appears in exactly two constraints. Since there are 
seven constraints in the problem, we expect that there will be seven 
nonzero variables in a basic feasible solution. Actually, there will be only 
six nonzero variables in any basic feasible solution, because one constraint 
is redundant. This redundancy occurs because the supply is equal to the 
demand. 
The model can be solved by the simplex algorithm, and its solutions will 
always be an integer vector, since the constraint matrix contains only O's 
and l's. However, there is a much more efficient algorithm that uses a 
3 x 4 tableau instead of the 6 x 12 tableau for the simplex algorithm. 
We construct the transportation tableau by writing a 3 x 4 matrix to 
hold the values of xij. The supply vector is placed to the right of this 
matrix, the transpose of the demand vector is written below the matrix, 
and the unit costs are written in the insets. For our problem we obtain 
Tableau 5.1. 

5.1 The Transportation Problem 
299 
Tableau 5.1 
] 
m 
Y 
7] 
__ 
71 
61 
91 
,01 
6] 
51 
81 
,J 
120 
140 
Supply 
100 
100 
60 
80 
120 
Demand 
There are several ways of systematically filling in the tableau with values 
for x ij. For now, we start by allocating as much of the supply in row 1 
(from plant 1) as possible to the cheapest route. This means Xll = 100, 
which satisfies the demand in the first column and leaves 20 units to be 
allocated to the next cheapest route in the first row. We set x14 = 20, 
which exhausts the supply from the first plant. Consequently, x12 = x13 = 0. 
Moving to the second row, we follow the same procedure of allocating as 
much as possible to the cheapest route. We set x24 = 100, since 20 units 
have already been provided in row 1. The 40 remaining units from plant 2 
are shipped to warehouse 2, since that route is the cheapest remaining 
one. The rest of the tableau is filled in using the same reasoning (Tableau 
5.2). Observe that the solution represented by Tableau 5.2 is a basic 
feasible solution. There are six nonzero variables. Later we will develop a 
technique to show that the columns of the simplex tableau corresponding 
to these variables are linearly independent. 
Tableau 5.2 
J 
10o 
Y 
_ 
0 
Y 
_ 
o 
71 
71 
61 
40 
20 
9] 
61 
,01 
81 
0 
20 
0 
80 
'1 
'1 
1 O0 
100 
60 
80 
120 
Demand 
120 
140 
100 
0 _ 
Supply 
Have we found the minimum cost solution? Our scheme for distributing 
the polyethylene represents $2160 in transportation costs. It is most likely 

300 
Chapter 5 Special Types of Linear Programming Problems 
not the minimum, since no goods made in San Antonio are shipped to San 
Antonio. 
We now look for a way of systematically modifying our initial solution to 
arrive at the minimum cost distribution scheme in a small number of steps. 
For each of the unused routes, routes along which no goods were shipped, 
we calculate the change in the total shipping cost due to shipping one unit 
of goods along that route subject to the restriction that the corresponding 
changes must be made in the used routes. For example, shipping one unit 
from plant 1 to warehouse 2 (x12 = 1) leads to the modifications of 
Tableau 5.2 as shown in Tableau 5.3. 
Tableau 5.3 
,_J 
] 
m 
J 
7j 
71 
_ 
61 
+1 
40- 1 
91 
,oi 
81 
__ 
61 
20- 1 
100+ 1 
'1 
120 
140 
Supply 
100 
100 
60 
80 
120 
Demand 
The change in the total shipping cost due to this one modification is 
7-6-7+5=-1. 
That is, the total shipping cost for the modified tableau (Tableau 5.4) is 
$2159. Thus, the cost has decreased and we have improved our solution by 
$1. 
Tableau 5.4 
s__l 
_ 
100 
~ 
_ 
0 
0 
100 
7~ 
1 
39 
20 
60 
91 
61 
r 
y 
51 
80 
19 
80 
8] 
0 
101 
'1 
120 
120 
140 
Supply 
100 
Demand 

5.1 The Transportation Problem 
301 
We now compute the possible improvements for each of the unused 
routes. There are six unused routes shown in Tableau 5.2, namely, (1, 2), 
(1, 3), (2, 1), (2, 3), (3, 1), and (3, 4). In doing these computations remember 
that all changes that are forced by the initial change must be assigned to a 
route that is being used. Thus, in computing the possible improvement for 
route (3, 4), for example, we have two choices for decreasing by 1 the 
amount shipped along a route in use. We may choose either (1, 4), or (2, 4). 
However, only the choice of (2, 4) works, since choosing (1, 4) leads to a 
change in an unused route that cannot be balanced with a corresponding 
change in a route that is being used. These two cases are illustrated in 
Tableaux 5.5 and 5.6. 
Tableau 5.5 
61 
Y 
100 
7J 
m 
7] 
m 
40+ 1 
6] 
20- 1 
60 
9] 
lol 
81 
51 
100- 1 
II 
+l 
80 
120 
120 
140 
Supply 
100 
This case works 
Demand 
Tableau 5.6 
m 
Cannot balance _ 
this increase 
--~ I00+I 
Y 
Y 
7] 
7I 
6} 
9} 
lO] 
61 
20- 1 
81 
'1 
+1 
120 
140 Supply 
I00 
100 
60 
80 
120 
Demand 
We show in Tableaux 5.7a-5.7f (displayed in abbreviated form) the 
modifications that would have to be made to the amounts shipped as 
shown in Tableau 5.2 if each of the unused routes were brought into the 
solution. The corresponding change in cost is noted below each tableau. 
We see that the shipping cost would be substantially reduced if we 

302 
Chapter 5 Special Types of Linear Programming Problems 
Tableau 5.7 
I 
m 
6__1 
100 
71 
71 
_ 
61 
+1 
-1 
I 
m 
,ol 
81 
_ 
61 
5j 
60 
80 
Cost decreases by 1 
(a) 
120 
-1 
+1 
120 
140 
100 
61 
.7-] 
N 
100 
7I 
_ 
7j 
61 
_ 
-1 
+1 
~j 
,o I 
81 
+1 
-1 
61 
sl 
'I 
60 
80 
Cost decreases by 1 
(b) 
120 
-1 
+1 
120 
140 
100 
6~ 
Y 
-1 
+1 
7I 
_ 
71 
_ 
61 
_ 
91 
,ol 
81 
61 
sl 
'I 
t O0 
60 
80 
120 
Cost increases by 2 
(c) 
+1 
-1 
120 
140 
100 

Tableau 5.7 (continued) 
5_~ 
m 71 
~6~ 
Y 
91 
7] 
_ 
-1 
61 
+1 
,ol 
+1 J 
-1 
6j 
5.1 The Transportation Problem 
303 
,J 
1 O0 
60 
80 
120 
Cost increases by 1 
(d) 
120 
140 
100 
,1 
_vJ 
-1 
+1 
71 
6] 
+I 
-1 
,oi 
51 
,] 
1 O0 
60 
80 
1 20 
Cost increases by 4 
(el 
+1 
-I 
120 
_ 
140 
100 
51 
_ 7] 
'I 
+1 
-I 
8J 
' 
6 1 
- 
51 
'1 
I O0 
60 
80 
1 20 
-1 
+! 
Cost decreases by 3 
(1) 
120 
140 
100 

304 
Chapter 5 Special Types of Linear Programming Problems 
allocated more to routes (2, 2) and (3, 4) and then modified routes (3, 2) 
and (2, 4) accordingly. If we set x34 = 20 [the largest amount by which we 
can decrease route (3, 2)] and increase x22 by 20, then we get Tableau 5.8, 
which represents a tableau whose total shipping cost is $2100. Conse- 
quently, Tableau 5.2 did not represent an optimal solution. 
Tableau 5.8 
lOO 
o 
0 
7__j 
71 
61 
60 
91 
,0J 
m 
80 
6I 
51 
'I 
100 
60 
80 
120 
20 
80 
20 
120 
140 
100 
Supply 
Demand 
By developing a way to easily compute the possible improvements, we 
shall see how to reallocate shipments and how to determine when an 
optimal solution has been reached. We first formulate the dual problem to 
the transportation problem. 
Let v~, u2, and u 3 be the dual variables corresponding to the supply 
constraints, and let Wl, w E, w3, and w 4 be the dual variables corresponding 
to the demand constraints. Then the dual problem to the transportation 
problem (5) is 
Maximize 
z' 
subject to 
U i -~ Wj <___ Cij , 
Ui, 
3 
4 
E SiUi -Jr- E djwj 
i=l 
j=l 
i= 1,2,3, 
j= 1,2,3,4 
wj unrestricted. 
Tableau 5.2 represents the initial basic feasible solution to the trans- 
portation problem with basic variables Xll , x14 , x22 , x24 , x32 , and x33. 
Since the variables xij of the transportation problem are doubly indexed, 
so are the imputed values zij, and from the properties of the simplex 
algorithm, zij = cij when xij is a basic variable. It follows from the 
discussion of complementary slackness in Section 3.2 that there are values 
for the dual variables for which the left-hand side of the dual constraint 
U i -Jr- Wj <__ C ij 

5.1 The Transportation Problem 
~0~ 
is equal to Zij , for all i and j. This means that for the pairs (1, 1), (1, 4), 
(2, 2), (2, 4), (3, 2), and (3, 3), we have 
U i + Wj "--Cij. 
(6) 
This gives us six equations in seven unknowns. The values of the dual 
variables can be found from (6) by giving one of the unknownsmsay, the 
one that appears most often--an arbitrary value--say, 0mand then solv- 
ing for the remaining unknowns. Our system is 
Xll" 
U 1 
+ 
W 1 
-- 5 
X14" 
U 1 
+ 
w 4 = 
6 
X22" 
U 2 
+ 
W 2 
= 7 
X24" 
U 2 
+ 
W 4 -- 5 
X32" 
U 3 
+ 
W 2 
= 6 
X33" 
U 3 
+ 
W 3 
-- 8. 
Setting U 1 -- O, we have 
U 2 -- --1, 
U 3 = 
--2, 
W 1 = 
5, 
1412 = 
8, 
W 3 = 10, 
and 
w 4 = 6. 
Now the entries in the objective row of the simplex tableau corresponding 
to the solution in Tableau 5.2 can be determined. The only nonzero entries 
will be those for the nonbasic variables. Each entry will be of the form 
Zij -- Cij = U i + Wj -- Cij 
since z/j is the left-hand side of the (i, j) constraint of the dual problem. 
The entries are 
U 1 + W 2 -- C12 = 
0 + 
8 -- 7 = 1 
v l+w 3-c13 =0+ 
10-9= 
1 
X12 9 
X13 9 
X21: 
U 2 + W 1 -- C21 
X23" 
U 2 + 
W 3 -- C23 
X31: 
U 3 + W 1 -- C31 
X34" 
U 3 + 
W 4 -- C34 
=-1+5-6=-2 
= -1 + 10- 10= -1 
= -2+5-7= 
-4 
= 
- 2 + 6 -  
1=3. 
Since we are dealing with a minimization problem, the largest positive 
value determines the entering variable. In this case it is x34. To determine 
the departing variable, examine route (3, 4). If we try to send one unit 
along unused route (3, 4) and make the corresponding modification in the 
routes that are being used (basic variables), we have Tableau 5.9. The total 
shipping cost is changed by 
7 - 
5 - 
6 + 1 = 
-3 
(Note" 
u 3 + W 4 -- C34 = 3), 

306 
Chapter 5 Special Types of Linear Programming Problems 
Tableau 5.9 
A 
A 
71 
71 
40+1 
6 1 
20- 1 
91 
loj 
81 
61 
51 
100- 
1 
100 
60 
80 
120 
Demand 
+1 
120 
140 
Supply 
100 
so that it has been reduced by $3. We can send as many as 20 units along 
route (3, 4), so that if x34 = 20, we drive x32 to zero. Thus, x32 becomes 
the departing variable. Modifying Tableau 5.2, we obtain Tableau 5.8, as 
we did before. Since each unit sent along route (3, 4) reduced the total 
shipping cost by $3, when 20 units are sent the cost should be reduced by 
$60. Indeed, the cost represented by Tableau 5.8 is $2100. 
We now give the calculations necessary to determine the next tableau. 
The system of equations relating the dual variables is 
Xll: 
U 1 
X14 " 
U1 
X22 9 
U2 
X24 " 
U2 
X33 : 
-+- W 1 
+ 
W 2 
=5 
+w4=6 
=7 
--I- W4 --- 5 
v 3 
+ w 3 
= 8 
X34" 
U 3 
+ w 4 = 1. 
To obtain a solution to this system of six equations in seven unknowns, we 
arbitrarily set w 4 = 0, obtaining 
v I = 6, 
u 2 = 5, 
v 3 = 1, 
w 1 = -1, 
w 2 = 2, 
and 
w 3 = 7. 
Consequently, the values of zij - 
cij for the nonbasic variables are 
X12 ; 
X13 ; 
X21 ; 
X23 ; 
X31 ; 
X32 ; 
U 1 + 
W 2 -- C12 "- 6 + 2 - 7 = 1 
/31 -~- W 3 -- C13 "-- 6 + 7 - 9 = 4 
u 2 + w 1 - c21 -'- 5 - 1 - 6 = -2 
u 2 + w 3 -c23 ~-- 5 + 7- 
10 = 2 
v 3 +w 1-c31 
= 1- 
1-7= 
-7 
v 3 +w 2-c32= 
1+2-6= 
-3. 

5.1 The Transportation Problem 
307 
The entering variable is x13. If we send one unit along the unused route 
(1, 3) and make the modifications in the routes being used (basic variables), 
we have Tableau 5.10. The total shipping cost is changed by 
9- 
6- 
8 + 1 = -4 
(Note" u 1 "l- W 3 --C13 
-
-
 4), 
so that it has been reduced by $4. We can send as many as 20 units along 
route (1,3), so that, if x13 = 20, we drive x14 to 0. Thus, x14 becomes the 
departing variable. Modifying Tableau 5.8, we obtain Tableau 5.11. Since 
each unit sent along route (1,3) reduced the total shipping cost by $4, 
when 20 units are sent the cost should be reduced by $80. Indeed, the cost 
represented by Tableau 5.11 is $2020. 
Tableau 5.10 
s_j 
_ 
61 
7] 
71 
_ 
6j 
91 
_ 
+1 
10J 
80- 1 
61 
20- 1 
'1 
20+1 
100 
60 
80 
Demand 
120 
Tableau 5.11 
A 
_ 
100 
0 
A 
0 
~.. 
71 
71 
6j 
_ 
60 
91 
101 
81 
20 
60 
61 
5} 
11 
0 
80 
40 
100 
60 
80 
120 
Demand 
120 
140 
Supply 
100 
120 
140 
Supply 
100 
Performing these steps several more times, we come to the point at 
which no entering variable can be chosen (all zij- 
cij are negative 
because this is a minimization problem), and the procedure stops. An 

308 
Chapter 5 Special Types of Linear Programming Problems 
optimal solution has been obtained; it is given in Tableau 5.12. The cost of 
this solution is $1900. Note that it chooses the plant in San Antonio to 
supply all the demand of the San Antonio warehouse. 
Tableau 5.12 
100 
0 
71 
_ 
0 
7I 
71 
1 
m 
60 
1 
m 
,oi 
81 
_ 
20 
60 
61 
,I 
0 
20 
1 O0 
100 
60 
80 
120 
Demand 
120 
140 
Supply 
1 O0 
A 
We formalize the procedure used in this example as "the transportation 
algorithm." Given the transportation problem 
Minimize 
subject to 
n 
E Xij --- Si 
j=l 
m 
~_~ xij 
i=1 
m 
n 
7. = E 
E c,jx,j 
i=1 j=l 
i= 1,2,...,m 
=dj, j= 1,2,...,n 
Xij ~ 0, 
integers 
m 
n 
E Si ~-- E dj, 
i=1 
j--1 
we choose an initial basic feasible solution using the minimum cost rule: 
For each row i = 1, 2,..., m assign the maximum possible amount xij of 
the remaining supply to the cheapest route. The transportation algorithm 
then consists of the following steps. 
1. Find the entering variable. 
(a) Solve the dual constraint equations corresponding to the basic 
variables for the remaining m + n - 1 dual variables. The value of one 
dual variable will have to be chosen arbitrarily. 

5.1 The Transportation Problem 
~09 
(b) Evaluate the objective row coefficients by computing Zij- 
Cij-- 
U i -Jr- Wj -- Cij for each pair (i, j), where xij is a nonbasic variable. 
(c) The entering variable is xij, where zij- 
c~j is the largest positive 
value from Step (b). If all zij- 
cij are nonpositive, stop; an optimal 
solution has been obtained. 
2. Find the departing variable (these steps will be modified to cover 
certain degenerate cases). 
(a) Determine which basic variables Xeq will decrease when xi/ is 
increased. 
(b) The departing variable is the one from the list in Step (a) whose 
value is smallest. 
3. Form the new tableau. 
(a) Set the departing variable to 0. 
(b) Set the entering variable equal to the previous value of the depart- 
ing variable. 
(c) Adjust the values of the other basic variables to make the supply and 
demand constraints hold. 
To show that this algorithm works for all possible transportation problems, 
we must check the following points. 
1. The minimum cost rule yields a basic feasible solution. 
2. The dual constraint equations can always be solved. 
3. The values of the objective row entries zij - cij are independent of 
the choice of value for one of the dual variables. 
4. The departing variable can always be computed by the given scheme. 
A more precise statement of this scheme will also be needed. 
We need to introduce some definitions so that we can further discuss 
the points we raised above. We will call each block in the transportation 
tableau a cell. The value of xi/ is recorded in cell (i, j). In our example we 
saw that if we changed the entry in cell (i, j) from 0 to 1, where xij was a 
nonbasic variable, then this change forced changes in the values of some of 
the basic variables. We now systematize the recording of these changes. 
A loop in a transportation tableau is a sequence of cells in the tableau 
that satisfies the following criteria. 
1. The sequence consists of alternating horizontal and vertical segments. 
2. Each segment joins exactly two cells. 
3. The first cell of the sequence is the last, and no other cell is used 
twice. 
Properties 1 and 2 tell us that if (i, j) is a cell in a loop and if we reached it 
horizontally (along row i), then the next cell in the loop must be in column 
j. Likewise, if we reached cell (i, j)vertically, then the next cell in the loop 
must be in row i. Consequently, we use the cells in each row two at a time 

310 
Chapter 5 Special Types of Linear Programming Problems 
when forming a loop. A loop must therefore have an even number of cells 
in it. We sketch some of the loops we found when computing possible 
improvements for our example in Tableau 5.13. The heavy dot in the cell 
indicates that the cell is included in the loop. This loop could be written as 
(1,2), 
(1,4), 
(2,4), 
(2, 2) 
if we proceeded horizontally from (1,2). The example in Tableau 5.14 
shows how a loop can be more complicated. In this example the loop is 
(3,1), 
(1,1), 
(1,4), 
(2,4), 
(2,2), 
(3,2). 
Another example is given in Tableau 5.15. This loop is 
(1,3), 
(3,3), 
(3,2), 
(2,2), 
(2,4), 
(1,4). 
Tableau 5.13 
7J 
__ 
61 
7J 
] 
m 61 
__ 
l~ 
'I 
8J 
11 
100 
60 
80 
120 
120 
140 
100 
Supply 
Demand 
Tableau 5.14 
61 
71 
_ 
7j 
6j 
91 
10j 
61 
'1 
100 
60 
Demand 
80 
120 
120 
140 
100 
Supply 

5.1 The Transportation Problem 
311 
Tableau 5.15 
A 
6_j 
A 
71 
'i 
6] 
,L 
~ 
10J 
81 
61 
"i 
5] 
11 
100 
60 
80 
120 
Demand 
120 
140 
Supply 
100 
Note that cell (2,3) is not in the loop, even though two segments cross 
there. 
Recall that a basic feasible solution to a linear programming problem 
with m constraints, none of which was redundant, had at most m nonzero 
values, and the corresponding columns of the coefficient matrix A were 
linearly independent. Loops give us a very convenient way of determining 
the linear independence of the corresponding columns of A for a trans- 
portation problem. We saw in (3) that each column of the coefficient 
matrix A corresponded to one of the variables xij and hence to one of the 
cells in the transportation tableau. 
THEOREM 5.1. 
The columns of A determined by 
(il, jl)(i2, J2),.--, (ik, J~) 
are linearly dependent if and only if the corresponding cells (or some of them) 
can be arranged in a loop. 
Proof 
Omitted. 
A 
This theorem shows that in computing possible improvements we were 
discovering how to write each column of A that corresponds to a nonbasic 
variable as a linear combination of the columns that correspond to the 
basic variables. It also allows us to conclude that the minimum cost rule 
gives a basic feasible solution--one whose corresponding columns of A are 
linearly independent. 
Theorem 5.1 also allows us to formalize the procedure for computing 
the departing variable. If xij has been chosen as the entering variable, 
then cell (i, j) must belong to a loop consisting of basic cells. That is, the 
column of A corresponding to x ij must be a linear combination of the 
columns of A corresponding to the basic variables. We list the cells of 
the loop for xij in order, starting with (i, j). The cells that appear in the 

~1 ~- 
Chapter 5 Special Types of Linear Programming Problems 
even-numbered positions in the sequence will have their values decreased 
when the value of xiy is increased. To maintain feasibility, these values 
cannot be decreased to a negative number. Therefore, we may choose the 
departing variable Xkl as one that satisfies the following criteria. 
(a) The variable Xkt is a basic variable and cell (k, l) appears in the loop 
for cell (i, j). 
(b) Cell (k, 1) appears in an even-numbered position in the loop. 
(c) The value of Xkl is the smallest of the values of all variables in 
even-numbered positions in the loop. 
For example, going from Tableau 5.2 to 5.8, we found that X34 was the 
entering variable and that it belonged to the loop 
(3,4), 
(2,4), 
(2,2), 
(3,2) 
We have x24 -- 100 and x32 = 20, so that x32 has the smaller value. Thus, 
x32 is the departing variable. 
The special form of the transportation problem allows us to do the 
x 
pivotal elimination in a very concise form. Suppose the value of the 
departing variable xkt is a. Then each variable in an odd-numbered 
position in the loop of the entering variable has, as its new value, a plus 
its old value. Each variable in an even-numbered position in the loop has, 
as its new value, its old value minus a. In this scheme the entering 
variable automatically is set at c~, and the departing variable is set at 0. 
The equations that are the dual constraints can always be solved. There 
are more unknowns than equations, and the equations are consistent. In 
the following example we show a fast technique for solving these equations 
by hand. 
EXAMPLE 2. 
Consider a transportation problem between three sources 
and five destinations in which the supply vector is 
and the demand vector is 
100] 
s = 
160 
140 
d 
~. 
90 
60 
80 . 
100 
70 
The costs are shown in Tableau 5.16. We find an initial basic feasible 
solution by using the minimum-cost rule. It is shown in Tableau 5.17 and 
has a value of z = 1990. Note that this solution was found in the order 

5.1 The Transportation Problem 
313 
shown in Table 5.1 and that each assignment satisfied either a demand 
constraint or a supply constraint but not both (with the exception of the 
last assignment). Also note that the solution has 
3+5-1=7 
nonzero variables. 
Tableau 5.16 
~ 
, 
A 
m 
90 
'! 
sl 
41 
60 
61 
_ 
2j 
91 
80 
71 
.01 
81 
100 
J 
61 
.ol 
70 
100 
160 
140 
Supply 
Demand 
Tableau 5.17 
Y 
0 
Y 
_ 
50 
40 
90 
51 
41 
60 
61 
60 
0 
,1 
0 
80 
91 
0 
0 
80 
71 
.01 
0 
100 
100 
'! 
61 
,ol 
70 
40 
30 
100 
160 
140 
Supply 
Demand 

314 
Chapter 5 
Special Types of Linear Programming Problems 
Table 5.1 
Assignment 
Cell 
Constraint satisfied 
1 
(1, 2) 
Demand 
2 
(1,5) 
Supply 
3 
(2, 3) 
Demand 
4 
(2, 5) 
Demand 
5 
(2, 1) 
Supply 
6 
(3, 1) 
Demand 
7 
(3, 4) 
Demand and supply 
To find the entering variable, we must compute the possible improve- 
ment or zij 
- 
cij for each nonbasic variable. Recall that 
Zij -- Cij -~- U i +" Wj -- Cij. 
For the basic variables we solve the system of equations 
(a) 
v 1 -[- w 2 ~-c12 = 3 
(b) 
v 1 +w 5 =c15 =3 
(c) 
v2+w~ =c2~ =7 
(d) 
v 2+w 3=c23=2 
(e) 
v 2 +w 5 =c25 =6 
(f) 
U 3 -[" W 1 : 
C31 -- 5 
(g) 
U3 -q- W4 -" C34 "- 8. 
We adjoin a row below the tableau for the values of w s and a column to 
the right for the values of u r and leave out the zeros in the nonbasic cells. 
We start by assuming v 1 = 0. We obtain in the following order 
w 2 = 3 
from (a) 
w 5 = 3 
from (b) 
v 2 = 3 
from (e) 
W 3 = 
-- 1 
from (d) 
W 1 = 4 
from (c) 
v 3 = 1 
from (f) 
w 4 = 7 
from (g). 
These results are shown in Tableau 5.18. Now the values for zij - cij can 
be filled in for the blanks in Tableau 5.18. The values of the basic variables 
are circled to distinguish them. We obtain Tableau 5.19. Only z22 - c22 is 

5.1 The Transportation Problem 
315 
positive. Hence, x22 is the entering variable. It belongs to the loop of basic 
variables. 
(2, 2), 
(2, 5), 
(1,5), 
(1,2). 
Tableau 5.18 
9I 
3j 
617__1 
_ 
@ 
7__I5I 
@- 
51 
4j 
@- 
90 
60 
21 
lOj 
@ 
31 
6j 
91 
8j 
lO 1 
Q 
100 
80 
-1 
70 
100 
160 
140 
Supply 
Demand 
Tableau 5.19 
Y 
~ @ 
j 
51 
@ 
j 
41 
@ 
90 
60 
6j 
2j 
-7 
7] 
lol 
@ 
o 
80 
-1 
8I 
0 
61 
100 
31 @ 
9I 
-9 
@ 
1ol 
(~ 
-6 
70 
100 
160 
140 
Supply 
Demand 
We have x25 
= 
30 and x12 = 
60, so that the smaller x25 becomes the 
departing variable. We increase x22 and x15 by 30 and decrease x25 and 

316 
Chapter 5 Special Types of Linear Programming Problems 
X12 by 30. This gives Tableau 5.20 with objective function value z = 1960. 
Solving for v~ and % and then computing zij- c~j, we obtain Tableau 
5.21. 
Tableau 5.20 
1 
m 
6J 
m 
y 
21 @- 
Y 
91 
90 
60 
80 
3j @ 
Q~Q 
4 J 
@- 
7j 
1o] 
m 
• 
100 
31 
61 
1oi 
70 
Q 
100 
160 
140 
Supply 
Tableau 5.21 
Y 
A 
A 
w 
Demand 
--4 
Q 
m 
m 
90 
31 
61 
Q 
51 
2J 
4I 
91 
__ 
60 
-6 
@ 
-1 
-9 
80 
7~ 
lOj 
m 
0 
~J O- 
100 
31 
1 
6I 
1o] 
G 
-1 
-7 
70 
1 O0 
160 
140 
Supply 
Demand 
In Tableau 5.21 the entering variable is x14 and it belongs to the loop 
(1,4), 
(3,4), 
(3,1), 
(2,1), 
(2,2), 
(1,2). 

5.1 The Transportation Problem 
317 
We have 
x34 = 
100, 
x21 = 50, 
and 
x12 = 30, 
so that x12 is the departing variable. Decreasing x34 , x21 , and x12 by 30 
and increasing x14, x31, and x22 by the same amount gives us Tableau 5.22. 
We have also solved for the dual variables and given the values of zij - 
cij. 
Since all these values are nonpositive, we have found an optimal solution. 
Its value is z = 1930. 
Tableau 5.22 
9I 
9 
31 
m 
-5 
@ 
41 
@ 
90 
-1 
-1 
60 
Y 
2I 
9I 
m 
-7 
@ 
-9 
80 
-1 
71 
10] 
m 
8j ~ 
m 
100 
31 @ 
61 
,0I 
-6 
70 
100 
160 
140 
Supply 
Demand 
A 
Degeneracy 
Degeneracy in a transportation problem has the same meaning as it did 
for a general linear programming problem. That is, a basic variable has 
value zero. This means that we have designated a route as being used 
although no goods are being sent along it. In a transportation problem 
degeneracy can occur in two ways. It is possible that while finding an initial 
basic feasible solution both a supply and a demand constraint are satisfied 
simultaneously. On the other hand, there may be a tie for choosing a 
departing variable. In either case at least one basic variable will have value 
zero. As in the simplex method, degeneracy generally causes no difficulties. 
A transportation problem has never been known to cycle. 
An algorithm to find an initial basic feasible solution to the transporta- 
tion problem assigns a value to a variable to satisfy a demand constraint or 

318 
Chapter 5 Special Types of Linear Programming Problems 
a supply constraint. This variable is then an initial basic variable that 
corresponds to the constraints satisfied. If both a demand and a supply 
constraint are simultaneously satisfied by the assignment of a value to Xrs, 
except for the final allocation, we have degeneracy. To maintain the corre- 
spondence between basic variables and satisfied constraints, we must 
designate a variable other than Xrs as also being basic and assign to it the 
value zero. 
EXAMPLE 3. 
Consider the transportation problem defined by Tableau 
5.23. Using the minimum cost rule, the cells are filled in the order 
(1,2), 
(1,5), 
(2, 3), (2, 5). 
Tableau 5.23 
3] 
5] 
4I 
50 
60 
61 
71 
_ 
91 
80 
81 
100 
3J 
61 
101 
70 
100 
160 
100 
Supply 
Demand 
At this point the tableau looks like Tableau 5.24. The next assignment of 
50 units to cell (2, 1)will complete both the first column and the second 
row. 
Thus, we have degeneracy. To systematically choose a variable as basic 
and having value zero, we agree in this case to say that only the second row 
has been completed. Moving to the third row, the smallest available cost is 
for cell (3, 1), and x31 is assigned the value zero. This makes x31 a basic 
variable. We complete the determination of an initial basic feasible solu- 
tion by letting X34 -- 100. Degeneracy during the iterations of the trans- 
portation algorithm may be ignored. 
A 

5.1 The Transportation Problem 
319 
Tableau 5.24 
J 
0 
50 
31 
_ 
51 
_ 
4[ 
60 
60 
61 
21 
91 
80 
71 
_ 
80 
0 
0 
,01 
l~ 
m 
100 
31 
61 
70 
100 
40 
160 
30 
100 
0 
Supply 
Demand 
Starting Procedures 
We have already described the minimum cost rule for obtaining an 
initial basic feasible solution to the transportation problem. A number of 
other methods are available. A desirable starting method is one that will 
provide a starting solution that is not far from the optimal one in the sense 
that only a small number of iterations of the transportation algorithm are 
required. We now describe Vogel's method, which is widely used. 
Vogel's Method 
Let C 
= [Cij ] be the cost matrix of a transportation problem. 
1. For each row and each column of C find the difference between the 
smallest and the next smallest entry. This difference represents the mini- 
mum penalty incurred when one fails to assign goods to the cheapest 
route. 
2. Select the row or column with the largest difference. Ties may be 
broken arbitrarily. 
3. Allocate as much as possible to the cell with the smallest cost in that 
row or column. Let us say that this allocation is made to cell (r,s). 
Decrease the available supply in row r and the required demand in column 
s by the amount allocated. This allocation will satisfy a demand constraint, 
a supply constraint, or perhaps both. The indication of which constraint 
has been satisfied is the reduction of the available supply in row r or the 
required demand in column s to zero. Remove the constraint that is 
satisfied from further consideration by crossing out the corresponding row 
or column of the cost matrix. If both a demand and a supply constraint are 
satisfied simultaneously, remove only one from further consideration. In 

320 
Chapter 5 Special Types of Linear Programming Problems 
this case both the available supply and the required demand have been 
reduced to zero. 
4. Repeat Steps 1, 2, and 3 until either exactly one row or exactly one 
column remains. In doing Step 1, do not compute differences for any row 
with 0 available supply or any column with 0 required demand. When 
exactly one row or one column remains, the entries in that row or column 
are fully determined by the previous allocations and are filled in accord- 
ingly. 
EXAMPLE 4. 
We apply Vogel'smethod to the transportation problem 
with the cost matr~ and supply and demand vectors shown below: 
100 
[863 
9] 
[120] 
C= 
2 
6 
1 
4 
s= 
140 
and 
d= 
60 
' 
' 
80 
" 
7 
8 
6 
3 
100 
120 
We compute the differences according to Step 1 of the algorithm and 
circle the largest. 
Differences: 
Differences 
[
8
6
3
9
]
3
 
2 
6 
1 
4 
1 
7 
8 
6 
3 
3 
(~ 
0 
2 
1 
Thus, we allocate as much as possible (100 units) to the cheapest route 
in the first column (x21 = 100), fill in the rest of this column with zeros, 
and cross out the column. The allocation in x21 is circled to indicate that 
x21 is a basic variable. The revised supplies and demands from Step 3 of 
the algorithm and the new differences are shown in Tableau 5.25. 
Tableau 5.25 
d 
61 
J 
61 
O 
I 
81 
3] 
91 
0 
0 
11 
61 
41 
31 
Demand 
60 
80 
120 
Supply 
Differences 
120 
40 
3 
1 O0 
3 
Differences 
0 
2 
1 

5.1 The Transportation Problem 
321 
We can arbitrarily choose among rows 1, 2, and 3. Choosing the first 
row, we allocate 80 units to the cheapest route in that row (x13 = 80), 
circle the allocation, fill in the rest of the row with zeros, and cross out the 
third column. We revise the supplies and demands, compute the new 
differences, and show the result in Tableau 5.26. 
Tableau 5.26 
~g 
_g 
_
_
 
61 
61 
8] 
J 
91 
@ 
J 
41 
J 
31 
Demand 
60 
120 
0 
0 
0 
0 
Differences 
0 
1 
Supply 
Differences 
40 
3 
40 
2 
100 
@ 
Choosing row 3, we allocate 100 units to the cheapest route in that row 
(X34 -- 100) and cross out the row. Tableau 5.27 shows the consequences of 
this allocation along with the new differences. The largest difference is 
now in column 4, so that we allocate 20 units to the cheapest route in that 
column (x24 = 20), cross out the column, and obtain Tableau 5.28. 
Tableau 5.27 
l 
Demand 
~ 
6 j 
J 
6I 
O 
1 
oi 
__J 
~ 
0 
0 
0 
60 
Differences 
0 
'I 
J 
! 
'1 
_
_
 
0 
0 
91 
4I 
20 
@ 
Supply 
Differences 
40 
2 
0 
40 
3 

322 
Chapter 5 Special Types of Linear Programming Problems 
Tableau 5.28 
! 
6j 
d 
61 
@ 
,I 
__J 
0 
Demand 
0 
60 
.I 
o 
d 
J 
1 
LJ 
@ 
o 
0 
,J 
o 
,J @ 
I 
"-J (D 
o 
Supply 
40 
20 
0 
There is now exactly one column remaining. The allocations for that 
column are completely determined (x21 = 40 and x22-- 20). Thus, the 
initial basic feasible solution is Tableau 5.29, with a cost of $1180. 
Tableau 5.29 
S 
0 
S @ 
0 
61 
61 
81 
m 
@ 
31 @ 
11 
0 
61 
91 
41 
31 
@ 
@ 
100 
60 
80 
120 
Demand 
120 
140 
Supply 
100 
The minimum cost method yields the initial basic feasible solution in 
Tableau 5.30, whose cost is $1240. Thus, in this case the Vogel method 
gives a better starting solution. Applying the transportation algorithm to 
either of the two starting solutions given above, the reader may show that 
the minimum cost for this transportation problem is $1140. 
A 
The effectiveness of Vogel's method can be improved by using a 
procedure due to Roland E. Larson that is, perhaps, too complicated for 
hand computation but can be carried out rapidly on a computer. Instead of 

5.1 The Transportation Problem 
323 
Tableau 5.30 
Y 
Y 
@ 
61 
31 
91 
@ 
@ 
61 
81 
61 
@ 
0 
'I 
41 @ 
__ 
J @ 
100 
60 
80 
120 
120 
140 
Supply 
100 
Demand 
using the given costs Cij for Vogel's method, we use the normalized costs 
c'ij defined by 
1 
n 
1 m 
' -- 
E r 
E r 
cij 
r 
n p=l 
m 
q=l 
That is, we subtract from each C ij the average of the costs of the row and 
column in which it appears. We then apply Vogel's method to the matrix 
[C'ij]. 
EXAMPLE 5. 
We consider the same transportation problem as that in 
Example 4. When we compute the normalized cost matrix, we obtain 
25 
43 
41 
17 
6 
6 
6 
6 
p 
73 
37 
67 
35 
12 
12 
12 
12 
9 
14 
14 
10 
25 
3 
3 
3 
3 
Applying Vogel's method to C', we obtain the starting solution in Tableau 
5.31. This solution is actually an optimal solution, and its cost is $1140. 
Tableau 5.31 
0 
i q @ 
q 
6j 
6] 
8] 
m 3] 
91 
@ 
'! 
41 
@ 
@ 
61 
_ 
120 
100 
60 
Demand 
80 
140 
Supply 
31 
100 
. . .  
120 
/X 

324 
Chapter 5 Special Types of Linear Programming Problems 
Extensions 
If the total supply exceeds the total demand in the original statement of 
the transportation problem, a dummy destination can be set up. That is, we 
create a destination with a demand equal to the difference between the 
total supply and the total demand. The cost of shipping to this destination 
from any source is 0. In fact, we have just added a slack variable to each 
supply constraint. 
EXAMPLE 6. 
The problem originally given in Example 3, Section 2.1, is 
z = 5Xll + 7x12 + 9x13 + 6x21 + 7x22 + 10x23 
Minimize 
subject to 
Xll q- X12 -~- X13 _~< 120 
X21 -q-X22 -~-X23 __< 140 
Xll q-X21 >__ 100 
X12 -I- X22 ~_~ 60 
X13 "~-X23 ~__ 80. 
The difference between supply and demand is 20. We create a destination 
with a demand equal to this amount. The constraints then become equali- 
ties, since demand equals supply. The tableau for the problem is Tableau 
5.32. 
Tableau 5.32 
N 
61 
71 
71 
l01 
01 
01 
120 
140 
Supply 
100 
60 
80 
20 
Demand 
A 
5.1 
EXERCISES 
1. Verify that Tableau 5.12 represents the optimal solution to Example 1 by 
performing the necessary steps to obtain Tableau 5.12 from Tableau 5.11. 
In Exercises 2-4 find an initial basic feasible solution using (a) the minimum 
cost rule, (b) Vogel's method, and (c) Larson's method if a hand calculator is 
available. 

5.1 The Transportation Problem 
~ 
i5236] [1oo] 
I6o 1 
2. C= 
2 
7 
7 
4 , s= 
80 , and d= 
60 
1 3 
6 
9 
140 
80 
120 
I663921 I9o 1 
8 
7 
5 
7 
5 
70 
and d= 
3. C= 
3 
4 
8 
2 
7 ' s= 
110 ' 
6 
0 
0 
2 
9 
150 
146751 I1~176 
4. C= 
4 
4 
7 
8 
60 
and d= 
5 
3 
6 
5' 
s= 
50' 
6 
5 
3 
4 
70 
5. Solve the transportation problem given in Example 3. 
6. Solve the transportation problem given in Example 6. 
7. Solve the transportation problem given in Exercise 3. 
In Exercises 8-13 solve the given transportation problem9 
[2 
5 
6 
3] 
[100] 
I 701 
8. C-- 
9 
6 
2 
1 , s= 
90 , and d= 
50 
7 
7 
2 
4 
130 
30 
120 
13 
2 
5 
41 
I 801 
I701 
9. C= 
6 
5 
7 
8 
60 
and d= 
70 
2 
1 4 
3 ' s= 
50 ' 
70 
4 
3 
5 
2 
100 
80 
100 
60 
120 
I4~ 
70 
120 
50 
60 
I ol 
10. C= 
8 
3 
4 
5 
7 
80 
and d= 
50 
6 
8 
6 
7 
5 ' s= 
60 ' 
4 
3 
5 
2 
4 
120 
70 
100 
I4 2 9 71 
I1 1 
11 C= 
7 
8 
5 
6 
9 
3 
3 
4 
1 
' 
s=d= 
7 
5 
2 
6 
[5 
6 
7 
4] 
[75] 
I451 
12. C= 
2 
9 
7 
5 , s= 
50 , and d= 
50 
8 
5 
8 
7 
60 
25 
50 
[6 
4 
3 
5] 
[100] 
I601 
13. C= 
7 
4 
8 
6 , s= 
60 , and d= 
80 
8 
3 
2 
5 
50 
70 
40 

326 
Chapter 5 Special Types of Linear Programming Problems 
14. Show that, in the m • n transportation problem, if 
m 
~ 
Esi = 
dj, 
i=1 
j=l 
then the constraints are equalities. 
15. Show that, in the m x n transportation problem, if 
m Es/= Eg, 
i=1 
j=l 
then one constraint is redundant. (Hint: First, sum all the supply constraints. 
Second, sum all but one of the demand constraints. Then show that the 
difference between the two sums is the other demand constraint.) 
16. Write the formulation of the dual of the m x n transportation problem. 
In Exercises 17 and 18 consider the general transportation problem in which 
m 
k 
s- Esi= 
g. 
i=1 
j=l 
17. Show that Xij = sidj//S is a feasible solution. 
18. Show that if X = [xij] is a feasible solution, then, for all i and j, 
0 <___ Xij <___ min{si, dj}. 
Further Reading 
Berge, C., and Ghouila-Houri, A. Programming, Games and Transportation Networks. Wiley, 
New York, 1965. 
Larson, R. E. "Normalizing Vogel's Approximation Method." Math. Magazine, 45 (1972), 
266-269. 
Reinfeld, N. V., and Vogel, W. R. Mathematical Programming. Prentice-Hall, Englewood 
Cliffs, NJ, 1958. 
5.2 THE ASSIGNMENT PROBLEM 
We gave an example of the assignment problem in Section 4.1, Example 
3. In this section we will discuss the simplest formulation of the problem. 
We assume that there are exactly as many persons available for assignment 
as there are jobs and that each person is to be assigned exactly one job. In 
the event that there are more persons available than jobs, we can create 
dummy jobs for the excess persons. Being assigned to a dummy job means 
in reality that the person is not assigned. We do not consider the case in 
which there are more jobs than people if all the jobs are to be completed 
because this violates our basic assumption that each person is assigned 
exactly one job. 

5.2 The Assignment Problem 
327 
The assignment problem, like the transportation problem, will also be 
considered as a minimization problem. That is, if we assign person i to job 
j, this assignment will cost c u. Our goal is to minimize the total cost of the 
assignment. Our first model for the problem is 
Minimize 
n 
n 
Z --- E 
E CijXij 
i=lj=l 
subject to 
~ Xij : 
1, 
i=l 
j = 1,2,...,n 
(1) 
n 
E Xij-- 1, 
i = 1,2,..., n 
(2) 
j=l 
Xij "-" 0 or 1, 
i = 1,2,..., n; 
j = 1,2,..., n 
(3) 
We can actually remove the restriction that X ij take on only 0 or 1 as a 
value. If we simply assume that x ij must be an integer, then each of 
constraints (1) and (2) guarantees that x ij can take on only 0 or 1 as a 
value. Thus, in place of (3)we write 
xij > 0, 
integers, 
i = 1,2,...,n; 
j = 1,2,...,n. 
(4) 
The model defined by (1), (2), and (4) is actually a transportation problem 
in which the demands are all equal, the supplies are all equal, the supplies 
equal the demands, and the number of sources is equal to the number of 
destinations. Consequently, this problem could be solved using the trans- 
portation algorithm of Section 5.1. With this algorithm we will find that 
2n- 
1 variables are basic. However, only n of these variables will be 
nonzero, so that the solution is highly degenerate. We would suspect that 
many iterations of the transportation algorithm would simply replace a 
basic variable with a zero value with another variable with a zero value. To 
obtain a better algorithm for the problem it will be helpful to use a slightly 
different point of view. 
Suppose a list of available persons and a list of the jobs are both written 
in some fixed order. Then one possible assignment is to give job i to 
person i. On the other hand, if the job list is reordered, it might be 
possible to reduce the total cost. Assume that person i is still assigned the 
job in the i th position of the new job list, but since the list was shuffled, 
this job is no longer job i. What we need is a shuffling or permutation of 
the job list that yields the minimum total cost. 
One way of recording a permutation of the numbers 1,2,...,n 
is to write a list of these numbers in the desired order. For example, 
4 
2 
1 
3 is apermutationofl 
2 
3 
4. Another way of writing this 
permutation is to give a 4 x 4 matrix M = [mij] with entries that are zeros 

328 
Chapter 5 Special Types of Linear Programming Problems 
and ones. If m ij = 1, it means that i is assigned the jth position in the new 
order. We see that 4 
2 
1 
3 corresponds to the matrix 
0 
0 
1 
0 
0 
1 
0 
0 
0 
0 
0 
1 
1 
0 
0 
0 
since 1 is in third position, 2 is in second position, 3 is in fourth position, 
and 4 is in first position. 
The set of all feasible solutions to the assignment problem is simply the 
set of all n x n permutation matrices. Note that each permutation matrix 
has the property that in each row (or column) there is precisely one entry 
equal to 1. 
THEOREM 5.2. 
If the cost matrix for an assignment problem has nonnega- 
tive entries and at least n zeros, then an optimal solution to the problem exists 
if n of the zeros lie in the positions of the ones of some n x n permutation 
matrix P. The matrix P represents an optimal assignment. 
Proof 
In the situation described, the cost can never be smaller than 
zero, and we have found an assignment for which the cost is zero. 
A 
This theorem provides a goal for our algorithm. We will show that we 
can modify the cost matrix without changing the optimal solution. The 
algorithm will then attempt to carry out this modification to reach a 
situation in which the cost matrix has a zero in each row and in each 
column. 
THEOREM 5.3. 
Suppose the matrix C = [c i j] is the cost matrix for an 
A 
n X n assignment problem. Suppose that X = [xij] is an optimal solution to 
this problem. Let C' be the matrix formed by adding a to each entry in the rth 
row. Then X is an optimal solution to the new assignment problem defined by 
C r" 
Proof 
The objective function for the new problem is 
Z r 
n 
n 
n 
n 
n 
-- E E C;jXij-- E E CijXij "Jr- E 
(Crj "Jr- Ol)Xrj 
i=lj=l 
i=lj=l 
j=l 
i~r 
n 
n 
n 
-- E E CijXij 4;- Ol E 
Xrj 
i=lj=l 
j=l 
n 
= 
E 
CijXij -'F Ol 
i=lj=l 

5.2 The Assignment Problem 
32~ 
since each row sum is 1. Therefore, the smallest value for z 
obtained when 
' will be 
n 
n 
Z -- E 
E CijXij 
i=1 j=l 
is smallest; namely, it is obtained when X = X. 
ZX 
A statement similar to Theorem 5.3 can be made if a constant is added 
to some column of a cost matrix. Thus, our strategy is to modify C by 
adding constants to rows or columns. 
EXAMPLE 1. 
Suppose the cost matrix for an assignment problem is 
C 
4 
5 
2 
5 
3 
1 
1 
4 
12 
3 
6 
3 " 
12 
6 
5 
9 
We can begin to introduce zeros by subtracting the smallest entry in each 
row from that row. We obtain 
2 
3 
0 
3 
2 
0 
0 
3 
9 
0 
3 
0 " 
7 
1 
0 
4 
There is no zero in column 1, but one can be forced there by subtracting 2, 
the smallest entry in column 1, from each entry in that column. We have 
0 
3 
0 
3 
0 
0 
0 
3 
7 
0 
3 
0 " 
5 
1 
0 
4 
We now have at least one zero in each row and column. We attempt to 
assign the locations for the ones in a permutation matrix. 
Actually, we will not produce the permutation matrix but will star the 
zeros in the cost matrix to indicate an assignment. We must assign the zero 
in position (4, 3) since it is the only zero in row 4. However, we will carry 
out the assignment in an order that will anticipate part of the algorithm we 
are developing. Starting with the first row, we assign the first zero in each 
row that does not belong to a previously assigned column. We say a row or 
column is assigned when some zero in it is assigned. Thus, in row 1 we 
assign the zero in column 1; in row 2 we assign the zero in column 2, 
skipping the zero in column 1 because that column was previously as- 
signed; in row 3 we assign the zero in column 4; and in row 4 we assign the 

~0 
Chapter 5 Special Types of Linear Programming Problems 
zero in column 3. The assignment that we obtain is 
0* 
3 
0 
3 
0 
0* 
0 
3 
7 
0 
3 
0* 
5 
1 
0* 
4 
or person 1 is assigned job 1, person 2 is assigned job 2, person 3 is 
assigned job 4, and person 4 is assigned job 3. The cost is 
4+1+3+5=13. 
A 
In Example 1 we were successful in determining an assignment for each 
person. However, as the next example shows, we may not always be as 
fortunate. 
EXAMPLE 2. 
Suppose the cost matrix for an assignment problem is 
C 
__ 
4 
1 
3 
4 
5 
6 
2 
9 
6 
5 
8 
5 " 
7 
6 
2 
3 
Subtracting the minimum entry in each row, we obtain 
3 
0 
2 
3 
3 
4 
0 
7 
1 
0 
3 
0 
" 
5 
4 
0 
1 
Now subtracting the minimum entry in each column, we obtain 
2 
0 
2 
3 
2 
4 
0 
7 
0 
0 
3 
0 
4 
4 
0 
1 
Making the assignments row by row, we have 
2 
0* 
2 
3 
2 
4 
0* 
7 
0* 
0 
3 
0 " 
4 
4 
0 
1 
This matrix does not represent a complete assignment; person 4 has not 
been given a job. Two explanations are available for this situation. Either 
there is no possible complete assignment for the given pattern of zeros or 
there is a complete assignment but the algorithm failed to find it. We 
investigate each of these possibilities. 

5.2 The Assignment Problem 
331 
First notice that any pattern of zeros in an n x n matrix has the 
property that all the zeros can be covered by n lines. For example, choose 
the n lines that each cover one column. Suppose that the zeros in the 
n x n matrix C can be covered with k lines, where k < n. Let a be the 
smallest of the uncovered entries of C. We form a new matrix C' by 
subtracting a from the entries of each uncovered row and adding a to the 
entries of each covered column. Each uncovered entry of C has decreased 
by a, since it belongs to an uncovered row and an uncovered column. Each 
entry covered by one line has remained unchanged: either it belonged to a 
covered row and uncovered column and was not modified, or it belonged 
to an uncovered row and covered column and had a added to it and 
subtracted from it. Each entry in both a covered row and a covered column 
has increased by a. Thus C' has a zero entry in a position in which C did 
not have a zero and it might be possible to finish the assignment. The 
procedure for modifying C can be more simply stated: subtract a from 
each uncovered entry and add a to each doubly covered entry. For 
example, we can cover the last matrix as follows: 
2 
1 
The smallest uncovered entry of C' is 1. Subtracting 1 from each 
uncovered entry and adding 1 to each doubly covered entry, we obtain the 
matrix 
1 
0 
2 
2 
1 
4 
0 
6 
0 
1 
4 
0 " 
3 
4 
0 
0 
Now using the assignment algorithm on this last matrix, we have 
1 
0* 
2 
2 
1 
4 
0* 
6 
0* 
1 
4 
0 
' 
3 
4 
0 
0* 
which is a complete assignment. 
A 
We can be guided in our use of this procedure by the following 
theorem, proved by the graph theorist K6nig. 
THEOREM 5.4. 
The maximum number of zeros that can be assigned is 
equal to the minimum number of lines that are needed to cover all the zeros. 

~ 
Chapter 5 Special Types of Linear Programming Problems 
A 
In Example 2, since we can cover all the zeros of matrix C' with three 
lines, it follows from Theorem 5.4 that at most three zeros can be assigned. 
We have determined such an assignment. It is impossible to assign four 
zeros with the given matrix C', and more zeros must be introduced using 
the procedure described above. The other possibility when we do not 
discover a complete assignment is that the row-searching algorithm has 
failed. 
by 
EXAMPLE 3. 
Consider the cost matrix for an assignment problem given 
C 
~.. 
4 
2 
9 
7 
7 
8 
5 
6 
3 
3 
4 
1 " 
7 
5 
2 
6 
Subtracting the minimum entry in each row from that row and then the 
minimum entry in each column from that column, we obtain 
0 
0 
7 
5 
0 
3 
0 
1 
0 
2 
3 
0 " 
3 
3 
0 
4 
Assigning the first zero entry in each row that does not lie in a previously 
assigned column, we get 
0* 
0 
7 
5 
0 
3 
0* 
1 
0 
2 
3 
0* ' 
3 
3 
0 
4 
which is not a complete assignment. However, there is a complete assign- 
ment for this matrix given by 
0 
0* 
7 
5 
0* 
3 
0 
1 
0 
2 
3 
0* " 
3 
3 
0* 
4 
Consequently, we must develop an algorithm to search it out. Before we do 
this, let us summarize our method of solution for an assignment problem 
as we now have it. 
Step 1. 
Assuming that the cost matrix C has nonnegative entries and 
the problem is a minimization problem, subtract the smallest entry in each 

5.2 The Assignment Problem 
333 
row from that row and then subtract the smallest entry in each column 
from that column. The new matrix C' defines an assignment problem that 
has the same optimal solutions as C, and C' has at least one zero in each 
row and column. 
Step 2. 
For each row assign the first zero not in any previously 
assigned column. If n zeros have been assigned, stop; an optimal solution 
has been found. 
Step 3. 
Assuming that fewer than n zeros have been assigned, deter- 
mine whether reassigning some zeros will give a complete assignment. If it 
will, stop after the reassignment. 
Step 4. 
If reassigning some zeros does not give a complete assignment, 
then find k lines (k < n) that cover all the zeros of C'. 
Step 5. 
Let a be the smallest uncovered entry in C'. Rearrange the 
zeros in C' by subtracting a from each uncovered entry of C' and adding a 
to each doubly covered entry of C'. Go to Step 2 to reassign all zeros. 
We now describe the details of Steps 3 and 4. The algorithm will require 
many searches of the rows and columns of C'. All these searches are to be 
done from left to right or from top to bottom (in order of increasing 
subscripts). 
Details of Step 3 
Suppose that i o is the index of one of the rows for which, in Step 2, we 
fail to find a zero that can be assigned. However, there must be at least 
one zero in row i 0, since every row has a zero in it. Say that one of these 
zeros occurs in column J0- There must be an assigned zero in column J0, 
for otherwise the zero in (i 0, J0) could be assigned. Say that this assigned 
zero occurs in row ia. Starting at cell (i 0, J0), we construct a path consisting 
of alternating vertical and horizontal segments, joining cells that alter- 
nately contain zeros and starred zeros. Specifically, we join 
0 
in 
(io,Jo) 
to 
0* 
in 
(il,J0) 
to 
0 
in 
(il,J1) 
to 
0* 
in 
(i2, Jl) 
..., 

~ 
Chapter 5 Special Types of Linear Programming Problems 
where the column indices J0, Jl,-", Jn must be distinct: 
Jl 
J0 
i 0 
0 
I 
i I 
0 ~0" 
I 
i 2 
0". 
We can represent this path by the sequence of cells (i0, J0), (il, J0), 
(il, jl), (i2, jl), .... The next cell in the sequence is obtained as follows. 
Case A. 
Suppose that we are at a 0 in (ik, Jk). We search column Jk for 
a 0". If a 0* is found, we add its cell to the sequence. If no 0* exists in 
column Jk, then we make the following reassignments in C': each 0 in the 
sequence from (i0, J0) to (ik, Jk) is changed to a 0* and each 0* is changed 
to a 0. Note that we have created a 0* for row i 0 and that a 0* remains in 
every row that previously had one. We now repeat Step 3 for the next row 
in which there is no assigned zero. 
Case B. 
Suppose, on the other hand, that we are at a 0* in cell 
(ik§ 1, Jk)" We search row i k+ 1 for a 0 that does not lie in a column 
appearing previously in the path. If a 0 is found, we add its cell to the 
sequence. If no 0 is found, we will not be able to modify the assignment as 
we did in Case A; we backtrack by labeling column Jk as necessary and 
redirecting our search. This is done by deleting the 0* at (i k § 1, Jk) and the 
0 at (i k, Jk) from the sequence. If k > 1, we return to the 0* at (i k, Jk-1) 
and repeat this process with row i k instead of row i k § 1. That is, we search 
row i k for a 0 other than the 0 in column Jk (the necessary column) that 
does not lie in a column appearing previously in the path: 
O* 
(ik, Jk-1) 
~O(ik, 
jk) 
I 
O* 
(ik+l, jk). 
If k = 0, we search for another 0 in row i0 that does not lie in a necessary 
column. If we find this 0, say in column j~, we construct a path as 
described above starting at cell (i 0, j~). If we cannot find another suitable 0 
in row i 0, a complete assignment has not been made. 
There are two ways in which the construction of this sequence can be 
terminated. One way is when the O's are changed to 0*'s and the 0*'s are 
changed to O's. In this case we have found a 0 in row i 0 that can be 
assigned. The other way is when all the cells are deleted because they lie in 

5.2 The Assignment Problem 
335 
necessary columns. In this case, a complete assignment cannot be made 
with this pattern of zeros, and we must go to Step 4. 
EXAMPLE 4. 
Consider the assignment problem whose cost matrix is 
C 
8 
7 
9 
9 
5 
2 
7 
8 
6 
1 
4 
9 " 
2 
3 
2 
6 
We construct C', which has a zero in each row and column, by performing 
Step 1 of the algorithm. We subtract the smallest entry in each row from 
that row and then subtract the smallest entry in each column from that 
column, obtaining 
C 
f 
1 
0 
2 
0 
= 
3 
0 
5 
4 
5 
0 
3 
6 
0 
1 
0 
2 
We now assign zeros starting in row 1 as in Step 2. We find that row 2 is 
the first row that has no 0 in an unassigned column. At this point, 
C 
! 
1 
0* 
2 
0 
3 
0 
5 
4 
5 
0 
3 
6 " 
0* 
1 
0 
2 
We then start constructing a sequence of O's and 0*'s for Step 3. The first 
cell is (2, 2), which contains 0. We search column 2 for a 0* and find it in 
cell (1, 2). We search 1 for a 0 and find it in cell (1, 4). We search column 4 
for a 0* and find none. Consequently we are in Case A with the sequence 
0 
in 
(2,2) 
O* 
in 
(1,2) 
0 
in 
(1,4), 
which represents the path 
(1, 2) 0"--0 (1, 4). 
o 
(2, 2) o 

336 
Chapter 5 Special Types of Linear Programming Problems 
Changing every 0 to 0* and every 0* to a 0 in the sequence, we obtain the 
matrix 
C 
P 
1 
0 
2 
0* 
3 
0* 
5 
4 
5 
0 
3 
6 
' 
0* 
1 
0 
2 
where we increased the number of assignments by 1. We now repeat Step 
3 with row 3. 
There is no assignable 0 in row 3; therefore, we must construct a 
sequence starting at the 0 in cell (3, 2). The 0* in column 2 is in row 2. But 
there are no other O's in row 2. We are in Case B and column 2 is 
necessary. Our sequence is 
0 
in 
(3, 2) 
0* 
in 
(2,2). 
Since all the cells in our sequence lie in a necessary column, we must go 
to Step 4 to determine the necessary rows. 
A row is called necessary if it contains a 0* in an unnecessary column. 
Starting with row 1, we find that its 0* is in column 4, and consequently 
row 1 is necessary. Row 2 has its 0* in column 2, which is a necessary 
column. Therefore, row 2 is not necessary. Row 3 has no 0* so it is not 
necessary. Row 4 has its 0* in column 1 and consequently is necessary. 
Details of Step 4 
Covering each necessary row and column with a line provides the k 
lines previously described. This procedure automatically covers all zeros 
of C'. 
We find that the C' of our example is covered as follows: 
1 
~ 
2 
9* 
C'= 
5 
4 
3 
6 
" 
~. 
v 
L, 
We are now ready for Step 5. Subtract 3, the smallest uncovered entry, 
from each uncovered entry and add 3 to each doubly covered entry. Our 
new 12', ready for Step 2, is 
1 
3 
2 
0 
12'- 
0 
0 
2 
1 
2 
0 
0 
3 " 
0 
4 
0 
2 

5.2 The Assignment Problem 
~7 
At the completion of the assignment procedure in Step 2 we have 
1 
3 
2 
O* 
O* 
0 
2 
1 
2 
O* 
0 
3 
' 
0 
4 
O* 
2 
which is a complete assignment. 
A 
We can now rewrite Step 3 in the algorithm: Assume that no zero in 
row i 0 has been assigned and that there is a zero in cell (i 0, J0). Construct 
a sequence of vertical and horizontal segments that alternate and join 0 to 
0* to 0 to 0* and so on as follows. 
(A) If we are at 0 in cell (ik, Jk), search column Jk for 0". If 0* is found, 
adjoin its cell to the sequence. If 0* is not found, change each 0 in the 
sequence to 0* and each 0* to 0 and search for the next row without a 0". 
(B) If we are at 0* in cell (ik + 1, Jk), search row ik + 1 for 0. If 0 is found, 
adjoin its cell to the sequence. If 0 is not found, label column Jk as 
necessary and delete cells (ik, Jk) and (i k + 1, Jk) from the sequence. If there 
are more cells, we are at 0* in cell (ik, Jk-1). Repeat Case B. If there are 
no more cells in the sequence, search row i 0 for a 0 that does not lie in a 
necessary column. If one is found, say in column j~, repeat Step 3 starting 
at cell (i0, J~). If one is not found, go to Step 4. 
The algorithm that we have developed is called the Hungarian method 
in honor of the mathematicians K6nig and Egerv~ry, on whose work it is 
based. The Hungarian method as we have described it assumes that the 
given assignment problem is a minimization problem. However, a maxi- 
mization problem can be modified in a way that will enable the Hungarian 
method to produce an optimal solution. Suppose we are given the problem 
Maximize 
subject to 
n 
E Xij 
j=l 
n 
E Xij 
i=l 
n 
n 
= E E cijxij 
i=lj=l 
= 1, 
i= 1,2,...,n 
=1, 
j = 1,2,...,n 
(5) 
xiy > 0 
integers. 

338 
Chapter 5 Special Types of Linear Programming Problems 
We can convert this problem to the minimization problem 
Minimize 
n 
n 
Z ~--- E E (--Cij)Xij 
i=lj=l 
subject to the constraints in (5). 
If some of the entries in the matrix [cij] were positive, we now have 
negative entries in [-cij], and the optimality criterion in Theorem 5.2 does 
not apply. However, we can use Theorem 5.3 and add to each entry in 
[-cij] the negative of the smallest (most negative) entry. This new matrix 
will have nonnegative entries, and the assignment problem for it can be 
solved using the Hungarian method. 
EXAMPLE 5. 
Suppose an assignment problem asks one to maximize the 
total value of the assignment for which the individual values are given by 
[c~j] = 
3 
7 
4 
6 
5 
2 
8 
5 
1 
3 
4 
7 
" 
6 
5 
2 
6 
The corresponding minimization problem is 
Minimize 
n 
n 
Z-- E E (--Cij)Xij 
i=lj=l 
subject to the constraints in (5). 
The smallest entry in [--Cij ] is --8; thus, we add 8 to each entry in [-cij] 
to obtain 
5 
1 
4 
2 
3 
6 
0 
3 
7 
5 
4 
1 
2 
3 
6 
2 
This matrix is now used as the cost matrix for the Hungarian method. 
A 
5.2 
EXERCISES 
In Exercises 1-6 solve the assignment problem with the given cost matrix. 
I4 
2 
3 
5-] 
3 
2 
5 
8 
9 
] 
6 
7 
4 
2 
3 
2 
3 
4 
6 
2. 
5 
3 
5 
4 
2 
3 
2 
5 
2 
4 
7 
3 
2 
4 
2 
5 
3 
4 
2 
6 
5 
5 
3 

5.2 The Assignment Problem 
339 
3 
4 
0 
2 
6 
7 
4 
6 
4 
5 
3 
6 
5 
7 
7 
8 
2 
8 
4. 
3. 
0 
8 
8 
4 
6 
4 
6 
4 
3 
7 
4 
9 
7 
5 
5 
0 
6 
7 
3 
5 
4 
2 
8 
1 
8 
3 
6 
6 
4 
3 
5. 
4 
4 
8 
8 
3 
5 
6. 
3 
8 
7 
4 
9 
7 
7 
9 
2 
3 
5 
7 
2 
7 
5 
8 
3 
2 
7 
4 
8 
5 
4 
3 
8 
5 
3 
7 
9 
1 
2 
4 
2 
6 
5 
7 
2 
8 
4 
6 
6 
9 
7 
4 
7 
3 
0 
8 
3 
5 
8 
6 
3 
2 
8 
9 
5 
6 
7 
0 
4 
2 
9 
5 
7 
3 
7. A company leases offices along one side of a hall in a large office building. 
Suppose that there are 15 offices all the same size leased by the company and 
that they are numbered 701 through 715. Because of a recent series of job 
reassignments the desks in some of the offices must be moved. Specifically, the 
desks in 702, 705, 708, 709, and 713 must be moved to 706, 707, 712, 714, and 
715, but it does not matter which desk goes to which office. Consequently, the 
facilities supervisor has decided to assign desks in such a way as to minimize the 
total distance over which they would need to be moved. What assignment should 
the supervisor use? 
5.2 
PROJECTS 
1. Consider the problem of scheduling the full-time registered nurses on a surgical 
floor of a hospital. Adequate patient care requires that there be four RNs 
during the day, two RNs in the evening, and one RN at night. Assume that the 
scheduling is done two weeks at a time. In the two-week planning horizon each 
nurse must work 10 shifts. 
(a) How many nurses are required? 
(b) A schedule for a nurse is a list indicating which of the 42 shifts (why 42?) 
are to be worked by that nurse. How many possible schedules are there? Which 
of these schedules would be less desirable? What guidelines might be used for 
making up acceptable schedules for the nurses? 
(c) Assume that the decisions about schedules for each nurse are made in the 
following manner: Each nurse is given a list of 10 possible schedules following 
the guidelines from part (b) and asked to rank them in order of desirability. 
Each nurse gets a different list of schedules. Each nurse's first choice is to be 
assigned a weight of 10, the second a weight of 9, and so on. How might ties be 
handled? The choice of one schedule for each nurse from among those pre- 
sented is determined by maximizing the sum of the weights of the chosen 
schedules. Set up a model for this situation. 
(d) Estimate the human time and the computer time involved in determining 
the schedule for staffing the floor in this way. (Computer time estimates can 
come from the size of the assignment problem.) 

340 
Chapter 5 Special Types of Linear Programming Problems 
2. Consider the following generalizations of the desk-moving problem (see Karp 
and Lee in Further Reading) in Exercise 7. 
(a) Assume that there are N offices from which desks must be moved. Let 
D = [dij] be the matrix of distances from office i to office j. Suppose that the 
current locations are s I < s 2 < .-- < sN, where "< " means ordered from near 
to far from the elevator along the hall. Suppose that the future locations are 
t I < t 2 < ... < t N. Show that an optimal assignment is s~, ~ t~,, k = 1, 2,..., N. 
(b) Suppose that, instead of being located along a hall, the offices are located 
around the entire perimeter of a floor of the office building. Suppose that the 
distance to walk completely around the floor is L and that D 
= 
[dij] is the 
matrix of shortest distances from location i to location j. What is the minimal- 
distance assignment? 
Further Reading 
Karp, R. M., and Li, S.-Y. "Two Special Cases of the Assignment Problem." Discrete Math. 13 
(1975), 129-142. 
Kuhn, H. W. "The Hungarian Method for the Assignment Problem." Naval Res. Logistics Q. 
2 (1955), 83-97. 
Kuhn, H. W. "Variants of the Hungarian Method for the Assignment Problem." Naval Res. 
Logistics Q. 3 (1956), 253-258. 
5.3 GRAPHS AND NETWORKS: BASIC DEFINITIONS 
A graph is a collection of points, called nodes (also called vertices), 
some of which (possibly all) are joined by lines, called arcs, edges, or 
branches. Every graph can be conveniently represented by a figure in the 
plane. Thus, Figure 5.1 shows a graph with seven nodes and nine arcs. We 
may note that the arc joining nodes 1 and 3 and the arc joining nodes 2 
and 4 intersect at a point that is not a node in this representation of the 
graph. The arc joining nodes i and j will be denoted by (i, j). 
An arc of the form (a, a) is called a loop. We will not allow loops in our 
graphs. If the nodes of a graph are numbered consecutively from 1 to n, 
FIGURE 5.1 

5.3 Graphs and Networks: Basic Definitions 
~1 
the graph can be represented by a matrix. The incidence matrix of a graph 
G is the matrix M = [mij], where 
1 
mij = 
0 
if node i is connected to node j 
otherwise. 
For an arbitrary graph, the incidence matrix is symmetric (M = MT). 
Why? The incidence matrix for the graph in Figure 5.1 is 
M 
~_ 
0 
1 
1 
1 
1 
0 
0 
1 
0 
1 
1 
0 
0 
0 
1 
1 
0 
0 
0 
0 
0 
1 
1 
0 
0 
1 
0 
1 
1 
0 
0 
1 
0 
1 
0 
0 
0 
0 
0 
1 
0 
0 
0 
0 
0 
1 
0 
0 
0 
A path between node i and node j of a graph is an ordered set of arcs 
(i, as), (a 1, a2), (a 2 , a3),... , (a r , j) 
joining i and j. Thus, a path between node 2 and node 6 in Figure 5.1 is 
(2, 4), (4, 5), (5, 6). 
Other paths between node 2 and node 6 are 
(2,1), 
(1,4), 
(4,5), 
(5, 6) 
(2,1), 
(1,5), 
(5, 6) 
and 
(2,3), 
(3,1), 
(1,4), 
(4,5), 
(5,6). 
(2,3), 
(3,1), 
(1,2) 
and 
(2,4), 
(4,5), 
(5,1), 
(1,2). 
A graph is said to be connected if there is a path joining any two nodes of 
the graph. The graph shown in Figure 5.1 is connected, whereas the graph 
shown in Figure 5.2 is not connected. 
An arc of a graph is called directed or oriented if there is a sense of 
direction so that one node is considered the point of origin and the other 
node is the point of termination. The directed arc from node i to node j 
will be denoted by (/-~). A graph in which every arc is directed is called a 
directed graph, a digraph, or an oriented graph. An example of a directed 
graph is shown in Figure 5.3. 
A cycle is a path joining a node to itself. Examples of cycles in Figure 
5.1 are 

342 
Chapter 5 Special Types of Linear Programming Problems 
FIGURE 5.2 
Q 
Q 
FIGURE 5.3 
An incidence matrix can also be used to represent a directed graph. In 
this case m ij = 1 if a directed arc connects i to j. Consequently, the 
incidence matrix of a digraph is usually not symmetric. The incidence 
matrix of the digraph in Figure 5.3 is 
M 
__. 
0 
1 
1 
0 
0 
0 
0 
0 
1 
1 
0 
0 
0 
0 
0 
0 
0 
1 
0 
0 
1 
0 
1 
0 " 
0 
0 
0 
0 
0 
1 
0 
0 
0 
0 
0 
0 
For certain applications of linear programming we are interested in the 
idea of a network. Intuitively, a network is an interconnection of several 
terminals by routes between certain pairs of these terminals. Each route 
has a capacity, and we are interested in studying the movement or flow of 

5.3 Graphs and Networks: Basic Definitions 
34~ 
material along these routes. We can study pipeline networks, highway 
networks, and electrical power distribution networks, as well as many other 
kinds. 
More formally, a network is a connected, directed graph for which a 
nonnegative number has been assigned to each ordered pair of nodes. This 
number is thought of as the capacity of the directed arc joining the two 
nodes. If node i is not connected to node j by an arc, the capacity c~j is set 
to zero. The capacities represent maximum amounts that may pass along 
arcs of networks; specifically, they may be tons of oil/hr in a transconti- 
nental oil pipeline network, cubic meters of water/min in a city water 
system, pulses/sec in a communications network, or number of vehicles/hr 
on a regional highway system. The nodes may represent shipping depots, 
relay stations, highway interchanges, or pumping stations. A network may 
be represented by its capacity matrix, which is a generalization of the 
incidence matrix of a graph and consists of the matrix of capacities of the 
arcs. The directed graph in Figure 5.3 becomes a network when we specify 
its capacity matrix as 
0 
7 
5 
0 
0 
0 
0 
0 
2 
3 
0 
0 
C= 
0 
0 
0 
0 
0 
8 
i 
0 
8 
0 
2 
0 " 
0 
0 
0 
0 
6 
0 
0 
0 
0 
0 
If there is no limit on the amount passing from node i to node j in a 
network, the capacity c ij is set equal to a very large number M. 
If flow is permitted in both directions between a pair of nodes (as in a 
two-way street system), the nodes are connected with two directed arcs, 
one going in each direction, each with its own capacity. That is, a two-way 
street is thought of as two one-way streets. 
A flow in a network is an assignment to each ordered pair of nodes (i, j) 
in the network of a nonnegative number xij that represents the amount of 
material moving in that directed arc. If node i is not connected to node j, 
then xij = 0. By definition, the flow may not exceed the capacity, so that 
we have for each i and j, i, j = 1, 2,..., n, 
0 <__ Xij ~ Cij. 
In many networks we single out two types of nodes for special consider- 
ation. The node S is called a source if every arc joining S to another node 
is oriented so that the flow is away from S. The node T is called a sink if 
every arc joining T to another node is directed so that the flow is toward 
T. That is, a flow is produced at a source and it is absorbed at a sink. 

344 
Chapter 5 Special Types of Linear Programming Problems 
We also specify that any flow cannot cause material to accumulate at 
any node other than a source or a sink. That is, with the exception of a 
source or a sink, the flow into a node is equal to the flow out of the node. 
If we have a network with exactly one source and exactly one sink, number 
the source as node 1 and the sink as node n. Then the flow must satisfy, 
for each node k, k = 2,..., n - 1, 
n 
n 
E x,,, 
= E x j. 
i=1 
j=l 
In the next few sections we will consider certain questions about flows in 
networks and certain situations that can be modeled with such a mathe- 
matical structure. 
1. For the graph 
5.3 
EXERCISES 
(a) find its incidence matrix; 
(b) find three paths joining nodes 1 and 4; 
(c) find two cycles from node 2. 
2. Follow the instructions in Exercise 1 for the following graph. 
3. Sketch the graph whose incidence matrix is 
0 
0 
1 
0 
1 
1 
0 
1 
1 
1 
0 
0 
0 
0 
1 
0 
1 
1 
0 
0 
1 
1 
1 
0 
0 
1 
1 
0 
(a) 
1 
0 
0 
1 
0 
(b) 
0 
1 
1 
0 
0 
0 
" 
1 
1 
1 
0 
1 
0 
1 
0 
1 
0 
~ 
0 
1 
0 
0 
1 
1 
0 
0 
1 
0 

5.4 The Maximal Flow Problem 
345 
4. Find the incidence matrix for the graph in Figure 5.2. 
5. For the network 
!0 
j 
9 
where the numbers on the edges are the capacities, 
(a) find the capacity matrix; 
(b) find the sources; 
(c) find the sinks. 
6. Follow the instructions for Exercise 5 for the following network. 
10 
8 
7 
7. Show that node i is a source in a network if and only if the ith column of the 
capacity matrix of the network is zero. 
8. Show that node j is a sink in a network if and only if the jth row of the capacity 
matrix of the network is zero. 
Further Reading 
Wilson, Robin J. Introduction to Graph Theory. Academic Press, New York, 1972. 
5.4 THE MAXIMAL FLOW PROBLEM 
Consider a network with n nodes that includes a single source and a 
single sink. For convenience we label the source node 1 and the sink node 
n. Let c 0 denote the capacity of arc (i, j). Consider a flow in this network, 
letting xij denote the amount of material flowing from node i to node j 
along arc (i, j). As we discussed in Section 5.3, the x~j must satisfy 
0 <_~Xij <___ Cij , 
i,j = 1,2,...,n 
(1) 
and 
Xik = ~ Xkj, 
k-2,3,...,n-1. 
(2) 
i=1 
j=l 

~ 
Chapter 5 Special Types of Linear Programming Problems 
We may also write (2) as 
n 
n 
~xik - 
Y]Xkj = O, 
k=2,3,...,n- 
1. 
i=1 
j=l 
The total flow starting from the source, which is to be maximized, is 
f= 
k=l 
The total flow into the sink is 
n 
E Xkn~ 
k=l 
(2a) 
(3) 
subject to 
n 
n 
E Xik -- E Xkj -- 0 
i=1 
j=l 
0 <xij<cij, 
i,j= 1,2,...,n. 
The mathematical formulation of the maximal flow problem shows that 
we have a linear programming problem, which could be solved by the 
simplex method. However, this approach is quite inefficient and in this 
section we present several better procedures. 
Before turning to the computational procedures, we note that the 
maximal flow problem occurs in many applications. As a typical applied 
problem, consider an electrical power distribution system represented by a 
network that has one source and many sinks. When a brownout appears 
imminent at a particular location (node), that location (node) is made a 
sink and the goal becomes to send as much electrical power as possible to 
this endangered location. Thus, we have a maximal flow problem. 
The following intuitive method appears to offer some hope for solving 
the maximal flow problem. We start at the source, and by proceeding along 
arcs with positive capacity we find a path from source to sink. (Why can we 
find this path?) The maximum amount fl of material that can be sent 
along this path is the minimum of the capacities of the arcs in the path. 
We now subtract fl from the capacity of each arc in the path just used. 
The capacity of at least one of the arcs in this path is now reduced to zero. 
We next return to the source and proceed along arcs with positive capacity 
n 
f= E Xlk 
k=l 
k = 2,3,..., n - 1 
(4) 
Maximize 
which by the conservation of flow is precisely the expression on the right 
side of (3) (verify). Thus, a mathematical formulation of the maximal flow 
problem is 

5. 4 The Maximal Flow Problem 
347 
to find another path to the sink. The maximum amount f2 of material that 
can be sent along this path is the minimum of the capacities of the arcs in 
the path. The capacity of at least one of the arcs in this path is now 
reduced to zero by subtracting f2 from each capacity in the path. We 
continue choosing possible paths from source to sink until there are no 
more paths all of whose arcs have positive capacity. The total flow f is the 
sum of the flows 
f=fl+f2+'"+fl,. 
To illustrate this intuitive method, consider the network with source 1 
and sink 6 (Figure 5.4). We first choose the path 
1 -o3--+2--+4--+6, 
, 
4 
9 
5 
8 
9 
which has flow fl = 2. Subtracting fl from the capacity of each arc in this 
path, we obtain the network in Figure 5.5. 
FIGURE 5.4 
2 
4 
3 
FIGURE 5.5 
Next, choose the path 
1~3-o5-o6, 
whose flow f2 = 5 yields the network in Figure 5.6. 

348 
Chapter 5 Special Types of Linear Programming Problems 
9 
2 
4 
3 
FIGURE 5.6 
Now choose the path 
1 ~2~5--->6 
with flow f3 = 1, obtaining the network in Figure 5.7. 
2 
9 
8 
3 
3 
2 
FIGURE 5.7 
For the next path we have 
1 --> 2 ~ 4 ---> 6 
with flow 
f4 = 2 
(Figure 5.8). 
0 
6 
~ 
1 
FIGURE 5.8 
We now choose the path 
1~3~6 
with flow 
f5=1 
(Figure 5.9). 

5.4 The Maximal Flow Problem 
349 
6 
0 
3 
! 
FIGURE 5.9 
Since we cannot find any other paths from source to sink, all of whose 
arcs have positive capacities, we are finished. The total flow is 
f=fl+f2+f3+fa+f5 =2+5+ 1 +2+ 
1=11. 
However, suppose that, instead of choosing the above sequence of paths 
from source to sink, we choose the sequence of paths indicated below: 
1 ---> 2 ---> 4 ~ 6, 
with flow 
fl = 4 
(Figure 5.10) 
0 
5 
i 
FIGURE 5.10 
1 ---> 3 ---> 5 ---> 6, 
with flow 
fe = 5 
(Figure 5.11) 
0 
FIGURE 5.11 

350 
Chapter 5 Special Types of Linear Programming Problems 
1 ~3---,6, 
with flow f3 =2 
(Figure 5.12) 
0 
FIGURE 5.12 
1 ~2---,5 ~6, 
with flow f4 = 1 
(Figure 5.13). 
4 
1 
FIGURE 5.13 
Since we cannot find any other paths from source to sink, all of whose 
arcs have positive capacities, we are finished. The total flow is 
f=f, +f2+f3+f4=4+5 
+2+ 
1 =12. 
Thus, the intuitive method does not always yield the maximal flow. How- 
ever, it can be modified to a correct algorithm by allowing fictitious flows 
in the opposite direction, so that an alternate path can be chosen. This 
principle is at the heart of the labeling procedure (sometimes called the 
augmenting path method) of Ford and Fulkerson. This procedure was 
developed in 1957 and was based on the earlier results of Kuhn and 
Egerv~ry (see Further Reading). In 1969 a substantial improvement in the 
augmenting path method was reported by Dinic, and many of the subse- 
quent improvements in the algorithms for solving the maximum flow 
problem have been based on his approach. There is now great interest in 
investigating the use of parallel processing to obtain further improvements. 
Some of the initial ideas in this direction are discussed in Goldberg and 
Tarjan. 
All these improved algorithms involved advanced ideas in combinatorial 
programming and complexity theory and consequently are beyond the 

5.4 The Maximal Flow Problem 
351 
scope of this book. The interested reader may see the books by Chvfital 
and by Papadimitriou and Steiglitz for more details. 
The Labeling Algorithm 
We now describe the labeling procedure of Ford and Fulkerson. It 
attempts to construct a feasible flow from source to sink and then to 
augment that flow to the capacity of the path, perhaps redirecting some of 
the flow and changing to a path of greater capacity. We continue to 
number the nodes so that node 1 is the source and node n is the sink. The 
directed arc joining node i to j has capacity c ij. If flow cannot take place 
from i to j, then cij = 0. For each arc we now define the excess capacity 
dij = cij - xij + xji. 
We begin with all flows set at value zero. Next label every directed arc with 
its excess capacity dij. 
Step 1. 
Starting at the source, we form the set N 1 of all nodes that are 
connected to the source by an arc with positive excess capacity. We use the 
index k for nodes in N 1. Now label each node in N 1 with the ordered pair 
of numbers (ek, Pk), where 
e k = dlk = excess capacity of the arc from the source to node k 
Pk = node that led to node k. 
Here Pk = 1 for each node in N 1, because we got to this node from the 
source. If we have labeled node n, the sink of the network, we then 
proceed directly to Step 5, where we increase the flow. 
Step 2. 
Choose the node in N 1 with smallest index; say it is node k. 
Let N 2 denote the set of all unlabeled nodes that are joined to node k by 
an arc with positive excess capacity. From now on we must assume that the 
source is a labeled node. If there are no such unlabeled nodes, we pick the 
node in N1 with the next smallest index and again form N2. We use the 
index m for each node in N2. Label each unlabeled node in N 2 with the 
ordered pair (em, Pm), where 
em -- min{dkm 
' e k} 
Pm = k. 
Observe that e m is the minimum of the excess capacities of the arcs from 
the source to node k and from node k to node m. Also, Pm denotes the 
node that led to node m. We repeat for each node in N1. 
Step 3. 
Repeat Step 2 with N r replacing N r_ 1. After a finite number of 
steps, we arrive at one of two possibilities. 
(i) The sink has not been labeled and no other nodes can be labeled. 
(ii) The sink has been labeled. 

~5~ 
Chapter 5 
Special Types of Linear Programming Problems 
Step 4. 
If we are in case (i), then the current flow can be shown to be 
maximal and we stop. 
Step 5. 
If we are in case (ii), then we have an augmenting path whose 
flow we increase as follows. Suppose that the sink has the label (er, Pr). 
The first number in the label, er, indicates the amount by which we can 
increase the flow. The second number in the label, Pr, gives the node that 
led to the sink, making it possible to move backwards along this path to the 
source. Let dst denote the excess capacities of the arcs in the path P. To 
increase the flow by e r we now calculate the excess capacities as 
d'st = d st - e r 
d'ts = dts -t- e r 
d'iy = dij 
for arcs not in P. 
Step 6. 
Return to Step 1. 
Assuming that a maximal flow exists, the algorithm terminates after a 
finite number of iterations. That is, after a finite number of iterations we 
reach case (i). 
We now calculate the net flows in each arc as follows. If cj~ = 0, so that 
flow cannot take place from node j to node i, then the flow in the arc from 
i to j is 
Xij -- Cij -- dij , 
where dij is the most recent excess capacity calculated as described in Step 
5. If both c~j and cji are positive, so that flow can take place from i to j as 
well as from j to i, observe that 
Cij 
Cji 
- 
dij 
= 
xij 
-- xji 
-- dji : 
xji - 
xij = 
-(cij 
- dij). 
dij and cji - dj~ cannot both be positive. We let 
Xij = Cij -- dij l 
Xy~ = 
0 
J 
if 
Cij -- dij ~ 0 
Xji -- Cji -- dji ~ 
x~j = 0 
] 
if 
Cji -- dji >_ O. 
Hence, Cij 
We illustrate the labeling method with the network considered in the 
intuitive method. Start with all flows set at value zero. Figure 5.14 shows 
the given network with each arc labeled with its excess capacity. 

d24 =4 
~,{'~ 
d42-0 
~ff.,. 
,,~/_ 1 \ 
~"<~,, 
FIGURE 5.14 
e 2 -- d12 = 9, 
e 3 = d13 = 8 
Step 1. 
Starting at the source, node 1, we find all nodes that are 
connected to it by an arc with positive excess capacity. These are nodes 2 
and 3. Thus, 
P2 = 1, P3 = 1. 
(9, 1) 
We now label nodes 2 and 3 with the respective ordered pairs (9, 1) and 
(8, 1), shown in Figure 5.15. 
5.4 The Maximal Flow Problem 
353 
(8, 1) 
FIGURE 5.15 
Step 2. 
Starting from node 2, we find all unlabeled nodes that are 
joined to node 2 by an arc with positive excess capacity. These are nodes 4 
and 5. The label on node 4 is (e4, P4), where 
e4 = min{d24 , e2} 
= min{4, 9} = 4 
p4=2. 

354 
Chapter 5 Special Types of Linear Programming Problems 
Similarly, the label on node 5 is (e 5, Ps), where 
e 5 = min{d25, e2} 
= min{4, 9} = 4 
p5 = 2. 
We proceed in the same manner from node 3. The only unlabeled node 
that can be reached from node 3 is node 6, the sink. This node is labeled 
(e6, P6), where 
e 6 
P6 
= rain{d36 , e3} 
= min{2, 8} = 2 
--3. 
The network in Figure 5.16 shows the labels obtained in Step 2. Observe 
that we have labeled the sink. 
(9, 1) 
(4, 2) 
d24 = 4 
a42=0 
"~.~ 
r 
r.,,, 
0 
e.r ,,,,, 
(2, 3) 
(8, 1) 
(4, 2) 
FIGURE 5.16 
Since we have labeled the sink, we proceed to Step 5. We can increase 
the flow from its initial value of 0 by e 6 = 2 units to total flow of 2 units. 
We now move backward to the source and calculate new excess capacities 
as follows. Since P6 = 3, we go back to node 3 and let 
d~6 = d36 - 
e 6 = 2 - 2 = 0 
d~3 = d63 + e 6 = 0 q- 2 = 2. 
Since P3 --- 1, we go back to node 1, the source, and let 
d~3 = d13 - 
e 6 = 8 - 2 = 6 
d~l = d31 -I- e 6 = 0 + 2 = 2. 
The network in Figure 5.17 shows the new excess capacities. We now 
return to Step 1 and relabel the nodes. In Figure 5.17 we have also 
indicated these new labels. 

5.4 The Maximal Flow Problem 
355 
(9 1) 
(4, 2) 
# ~ 
d24 =4 
/,~ 
,,4/, ~ 
~ 
~- ~N" 
_/~-'-~ 
~"~" ,, 
o",N_ 
~ 
\ 
d 
(6, 1) 
(4, 2) 
FIGURE 5.17 
(4,4) 
The sink has again been labeled. We can increase the flow by 4 units to 
a total flow of 6 units. We move backward along the path 
1~2~4~6 
and calculate new excess capacities, which are indicated in the network in 
Figure 5.18. In this network we have also indicated the newest labels on 
the nodes. 
(5 1) 
// 
~ 
d42 4 
~
~
 
(6, 1) 
(4, 2) 
FIGURE 5.18 
(4,5) 
The sink has been labeled and evidently we can increase the flow by 4 
units to a total flow of 10 units. We move backward along the path 
1~2~5~6 
and calculate new excess capacities, which are indicated in the network in 
Figure 5.19. In this network we have also indicated the newest labels on 
the nodes. 

356 
Chapter 5 Special Types of Linear Programming Problems 
(1 
"PJ \ ~  
(6 
1) 
"~ 
d24 =0 
(/~ 
', 
_ 
o" 
'1 
.~... 
~o',, 
~..-..~ 
~. 
~ ,.'xL 
o 
_/~,~ 
1) 
(5,3) 
FIGURE 5.19 
(2,5) 
We have labeled the sink and can now increase the flow by 2 units to a 
total of 12 units. We move backward along the path 
1--->3---,5---,6 
and calculate new excess capacities, which are indicated in the network in 
Figure 5.20. In this network we have also indicated the newest labels on 
the nodes. 
At this point, the sink has not been labeled and no other nodes can be 
labeled. The current flow of 12 units is maximal and we stop. The net flow 
in each arc is shown in the network in Figure 5.21. 
ZX 
We shall now show that if the above procedure yields case (i), then the 
current flow is optimal. We must start with another definition. A cut in a 
network is a set of directed arcs with the property that every path from the 
source to the sink contains at least one arc from the set. Since the number 
of directed arcs in a network is finite, the number of cuts is also finite. 
(I,i) 
% ,., 
,,'Xf \ 
(4. I1 
(3.31 
FIGURE 5.20 

5.4 The Maximal Flow Problem 
357 
8 
4 
FIGURE 5.21 
From this finite set of cuts we shall soon single out one cut. We define the 
capacity of a cut as the sum of the capacities of its directed arcs. 
EXAMPLE 1. 
Consider the network in Figure 5.22. The set of directed 
arcs 
is a cut with capacity 19. The set of directed arcs 
is not a cut because the path 
1~2~5~6 
contains no directed arc from the set. 
For a given path, the flow along this path cannot exceed the smallest of 
the capacities of its arcs. Hence, for any cut the flow along this path cannot 
exceed the capacity of whichever of its arcs belongs to the cut. Now a 
maximal flow consists of a sum of flows along various paths from the 
source to the sink. Therefore, for any cut a maximal flow cannot exceed 
1 
, 
4 
9 
5 
4 
2 
FIGURE 5.22 

~58 
Chapter 5 Special Types of Linear Programming Problems 
the sum of the capacities of the arcs of the various paths that belong to the 
cut. Hence, a maximal flow cannot exceed the capacity of the cut. 
A 
Suppose that we are now in case (i) of the labeling algorithm: the sink 
has not been labeled and no other nodes can be labeled. The set N of all 
nodes of the network can be partitioned into two disjoint subsets: N L, the 
labeled nodes, and N u, the unlabeled nodes. 
Let A be the set of directed arcs that join nodes in NL to nodes in N u. 
We first show that A is a cut in the network. Suppose it is not a cut, so 
that there is a path from source to sink that does not contain any directed 
arc from A. It then follows that all the nodes in this path belong either to 
ATE or to N u (why?). Since by definition the source is labeled and the sink 
is unlabeled, we have a contradiction to our assumption that A is not a 
cut. Therefore, it must be one. 
We next show that the capacity of cut A is equal to the maximal flow in 
the network. From Equation (2a) and the definition of the excess capaci- 
ties, we can write 
n 
E (Cij -- dij) = 0, 
i = 2,..., n - 1. 
(5) 
j=l 
When i = 1, we obtain from the definition of excess capacity 
n 
n 
E (Clj -- dlj) = ~-~ Xlj 
j=2 
j=2 
(6) 
since xjl = 0 for j = 1, 2,..., n (why?). Thus, the sum in (6) gives the total 
flow. We now combine (5) and (6) and obtain 
n 
E 
E (Cij -- dij) ---- ~ Xlj 
i~N L j= 1 
j=2 
= total flow 
(7) 
since the source, node 1, belongs to N L. Consider the left side of (7). If i 
and j both belong to N L, then cij- dij and cji- dji both occur on the 
left side of (7) and cancel each other out. Thus, only the terms for which j 
belongs to N U are left in the sum. If j is in N U, then dij = 0. Hence, the 
left side of (7) becomes 
E 
E 
Cij 
i~N L j~N U 
and thus (7) can be written as 
E 
E 
Cij : total flow. 
(8) 
i~NL j~Nu 
Now the left side of (8) is the capacity of the cut A, since A consisted of 
exactly those arcs joining nodes in NL to nodes in N u. Thus, for this 
particular cut, the total flow is exactly equal to its capacity. 

5.4 The Maximal Flow Problem 
359 
(! 
l) 
L 
0 
-- ~j\\ 
O 
(4. !) 
'r,, .,'N / 
b @ 
Cut line 
d53 = 2 
(3.3) 
FIGURE 5.23 
Since we have already shown that the maximal flow of a network cannot 
exceed the capacity of any cut, we conclude that for the cut A, just 
constructed, the flow is maximal. Thus, the labeling algorithm described 
earlier does yield a maximal flow. Moreover, similar reasoning shows that 
cut A has the minimum capacity among all cuts. The results established 
here can also be stated as the max flow-min cut theorem. 
THEOREM 5.5 (MAX FLOW-MIN 
CUT THEOREM). 
The maximum flow in 
a network is the minimum of the capacities of all cuts in the network. 
A 
EXAMPLE 2. 
The cut A defined by the network in Figure 5.20 is 
{(2, 4), (3, 6), (5,6)}. It takes the name cut from the graphical representa- 
tion in Figure 5.23. The cut line is drawn through precisely those arcs that 
belong to cut A. 
A 
When using the labeling algorithm it is possible that an augmented path 
will contain one or more arcs that must be traversed backward from their 
direction in the original network. The following example illustrates this 
situation. 
EXAMPLE 3. 
Consider the network in Figure 5.24. We wish to find the 
maximal flow. We apply the Labeling Algorithm and obtain the labels 
shown in Figure 5.25. 
9 
7 
5 
4 
7 
FIGURE 5.24 

360 
Chapter 5 Special Types of Linear Programming Problems 
(7,1) 
(7,2) 
~'~ 
d25 =9 
_ ~'~ 
d52=0 
- 
(9,1) 
(8,4) 
FIGURE 5.25 
Since the sink has been labeled, we can increase the flow by 5 units 
from its initial value of 0. We move backward along the path 
1---,2--,5--,8 
and calculate the new excess capacities. These are shown in Figure 5.26 
along with the new labels on the nodes. 
(2,1) 
(2,2) 
d25=4 
(2,6) 
~3~s 
d74=0 
(9,1) 
(8,4) 
FIGURE 5.26 
Again the sink has been labeled, and we can increase the flow by 2 units 
to a total of 7 units. We move backward along the path 
1---,2---,6--,8 
and calculate the new excess capacities. These are shown in Figure 5.27 
along with the new labels on the nodes. 

5.4 The Maximal Flow Problem 
361 
(4,6) 
~31"~0~~"~ 
d36:4 ~
~
 
~/~ 
(9,1) 
(8,4) 
FIGURE 5.27 
(4,6) 
Again the sink has been labeled, and we can increase the flow by an 
additional 4 units to a total of 11 units. We move backward along the path 
1--->3-->6--->8 
and calculate the new excess capacities. These are shown in Figure 5.28 
along with the new labels on the nodes. 
(8,7) 
d52= 5 
- 
II 
N ~ 
(9,1) 
(8,4) 
FIGURE 5.28 
(3,7) 
Also in this iteration of the Labeling Algorithm, the sink has been 
labeled. The augmenting path is 
1--->4-->7-->8, 
and we can increase the flow by 3 units to a total of 14 units. We calculate 
the new excess capacities as shown in Figure 5.29, along with the new 
labels on the nodes. 

362 
Chapter 5 Special Types of Linear Programming Problems 
(5,5) 
(5,7) 
\ +~'~~ 
<,,~:o ~
X
 
~+~/~ 
++,\+, 7 
(6,1) 
(5,4) 
FIGURE 5.29 
(2,6) 
Again the sink has been labeled, and we can increase the flow by 2 units 
to a total of 16 units. The augmenting path is 
1--->4--+7--->5--+2--+6--+8. 
Note that the arc 5--+ 2 in this path is the reverse of the arc 2--+ 5 
included in the original network. Thus, we must be careful with the indices 
in the excess capacity calculations. We have 
drl4 = d14 
-
-
 e 8 
d]l = d41 + e 8 
d]7 - d47 
-
-
 e 8 
d~/4 = d74 --I- e 8 
d~5 = d75 - e 8 
d~7 = d57 + e 8 
d~2 =d52-e 
8 
d~5 =d25 +e 8. 
Note that in calculating d~5 we have to increase the excess capacity of a 
forward arc. 
d~6 = d26 -- e 8 
d~i 2 = d62 -k- e 8 
d~8 = d68 -- e 8 
d86 = d86 + e 8 
The new capacities and new labels computed from them are shown in 
Figure 5.30. 
(3,5) 
(3,7) 
~'~ 
d25 =6 
~ 
d52=3 
~o" 
\'++'++'~ 
<,.,,+:o ~
~
 
.,J+l~ 
.+)u 
+ 
(4,1) 
(3,4) 
FIGURE 5.30 

5.4 7-he Maximal Flow Problem 
363 
In this case the sink is not labeled and no other nodes can be labeled. 
We have found a maximal flow, as shown in Figure 5.31. 
3 
7 
5 
FIGURE 5.31 
The minimum cut for this network can be computed by first partitioning 
the set of nodes N = {1, 2,..., 8} into the subsets NL and N v of labeled 
and unlabeled nodes, respectively, using the labels obtained in the last 
iteration of the Labeling Algorithm. We have N L = {1, 2, 3, 4, 5,7} and 
N U = {6, 8} Cut A is the set of directed arcs in the network leading from 
nodes in N L to nodes in N U. We have 
A = {(2, 6), (3, 6), (5, 8), (7, 8)}. 
The arcs in the cut set are shown in Figure 5.32 with gaps. Observe that 
the capacity of this cut is 
4+4+5 
+3 = 16=maximalflow. 
3 
7 
5 
8 
FIGURE 5.32 
A 
5.4 
EXERCISES 
In Exercises 1 and 2, a network that has been labeled with the labeling 
algorithm is given. In each case the sink has been labeled. Following the appropri- 
ate steps of the labeling algorithm, adjust the excess capacities and relabel the 
network. 

~4 
Chapter 5 Special Types of Linear Programming Problems 
(5, 1) 
(4 2) 
~',~ 
d24 =4 
- ~' ~er 
~."<~ ,, 
--~' -/ 
(7.1) 
(2,2) 
(8, 1 ) 
(8, 2) 
,,~l 
I 
N% 
_/~,> 
..y 
~ ,,.,~ 
(4, 1) 
(3, 1) 
In Exercises 3-6, find the maximal flow in the given network using the labeling 
algorithm. 
,. 
8 
-k, 
4 

5.4 The Maximal Flow Problem 
~ 
. 
7 
7~4 
9 
~
4
 
9 
5 
............... 
, D J  
/ 
8 
2 
9 
7 
8 
7. (a) Model the following situation as a maximal flow problem. There are an 
equal number, say n, of boys and girls at a school dance. It is also known for 
each boy-girl pair whether they are friends (this friendship relation could be 
given by an n x n matrix). Find the maximum number of friendly pairs that 
can be formed. 

~ 
Chapter 5 Spech~l Types of Linear Programming Problems 
(b) Solve the above problem for the friendship relation matrix 
1 
1 
0 
0 
0 
0 
0 
0 
1 
1 
1 
0 
1 
0 
0 
. 
0 
0 
1 
0 
1 
0 
1 
0 
1 
0 
(c) Solve the problem in part (a) for the friendship relation matrix 
0 
1 
0 
1 
0 
1 
0 
0 
1 
1 
0 
0 
1 
0 
1 . 
0 
0 
0 
1 
1 
0 
1 
0 
0 
0 
(d) Find a condition on the friendship relation matrix that will guarantee that 
each boy and girl is dancing with a friend. 
8. (a) Model the following situation as a maximal flow problem. Each day at the 
CPR Clothing Factory there are a large number of tasks to be assigned to 
the various machines. Assume that any task when assigned to a machine will 
occupy the machine for the entire day. Of course, only some tasks can be 
assigned to each machine--some machines are cutting machines; others are 
pleaters; and others can be set for basting, straight stitching, or top stitching. 
For a particular day suppose the list of tasks and possible machines is as 
given in the accompanying table. 
Machine 
Task 
1 
2 
3 
4 
5 
6 
1 
J 
2 
J 
3 
J 
d 
4 
J 
d 
5 
J 
J 
6 
J 
J 
7 
J 
J 
8 
J 
J 
j 
9 
j 
10 
j 
j 
(b) How many tasks can be completed? 
9. Show that the flow out of the source of a network is equal to the flow into the 
sink. 

5.4 The Maximal Flow Problem 
367 
5.4 
PROJECT 
There are many situations in which one is concerned about flow along one 
route. If there are no delays, the maximum flow is easily determined as the time 
available divided by time per unit flow. The interesting cases occur when there are 
delays at certain points and limited waiting facilities at these delay points. Exam- 
ples of such a situation are trains moving on a single track with sidings at certain 
points and a production line with certain slowly executed operations that cause 
bottlenecks. Of course, a railroad siding can hold only a certain number of trains. 
Likewise, we assume that the holding areas on the production line can contain only 
a limited number of items. 
These situations can be modeled as maximal flow problems. To construct the 
model, we must determine the network and the meanings of its nodes, arcs, and 
capacities. As a first step, let P be the duration of the period in which we want to 
maximize the flow. Divide P into k equal units. Then we may examine the state of 
the system at any time 0, 1 ..... k. For example, a train schedule would probably 
have P = 24 hr and k = 240, meaning that we know the status of each train every 
0.1 hr. 
Assume there are r delay points. Let n i (i = 1, 2 ..... r) be the capacity of the 
ith delay point. Let t o be the time needed to go from the beginning of the route to 
the first delay point. Let t i (i = 1, 2 ..... r- 
1) be the time needed to go from the 
ith delay point to the (i + 1)st delay point. Finally, let t r be the time needed to go 
from the rth delay point to the end of the route. 
(a) Assuming there are no delays, what is the maximal flow in the route during a 
period of length P? 
(b) What is the earliest arrival time at the end of the route, assuming that a unit 
of flow starts at the beginning of the time period? 
(c) What is the latest departure time from the beginning of the route that will 
allow a unit of flow to arrive at the end of the route before the time period is up? 
To construct the maximal flow model we will need a network that has a 
different set of nodes for each of the k + 1 specified times in the period under 
consideration. We will also need a decision function that governs whether a unit of 
flow can move from one delay point to the next. In the case of the train example, 
this function will be based in part on information about express train schedules and 
the need for a local train to be able to move from one siding to the next without 
being overtaken by an express. Let this function be 
_ [ 1 
8ij 
0 
if at time j the unit of flow can safely move 
from delay point i to delay point i + 1 
otherwise. 
(d) Formulate the maximal flow model of this situation. 
(e) Assume that 
k=13, 
r=3 
t o = 2, 
t I = 
3, 
t 2 = 1, 
t 3 = 2 
n 1 -- 2, 
n 2 = 1, 
n 3 = 3 

368 
Chapter 5 Special Types of Linear Programming Problems 
1 
1 
1 
0 
1 
1 
0 
0 
1 
0 
1 
1 
1] 
1 
0 
0 
1 
1 
0 
1 
1 
0 
1 
0 
1 
0]. 
1 
0 
1 
1 
0 
1 
1 
1 
0 
1 
0 
0 
1 
Find the maximal flow along this route for the given period of time. 
Further Reading 
Chv~tal, Va~ek. Linear Programming. Freeman, New York, 1980. 
Dinic, E. A. "Algorithm for Solution of a Problem of Maximum Flow in Networks with Power 
Estimation." Soviet Math. Doklady. 11 (1970), 1277-1280. 
Ford, L. R., Jr., and Fulkerson, D. R. Flows in Networks. Princeton Univ. Press, Princeton, NJ, 
1962. 
Goldberg, Andrew V., and Tarjan, Robert E. "A New Approach to the Maximum-Flow 
Problem." J. Assoc. Comput. Mach. 35 (1988), 921-940. 
Papadimitriou, Christos H., and Steiglitz, Kenneth. Combinatorial Optimization: Algorithms 
and Complexity. Prentice-Hall, Englewood Cliffs, NJ, 1982. 
5.5 THE SHORTEST ROUTE PROBLEM 
In many applied problems it is necessary to find the shortest path from 
a given node in a network to another node in the network. These include 
problems in the optimal distribution of goods (along highways or railroads), 
routing of communication signals, and transporting people in metropolitan 
areas. Another very interesting application of the shortest route problem is 
the problem of optimally replacing equipment that deteriorates with age. 
A number of these applications will be considered in the exercises. 
Many algorithms have been developed for the solution of the shortest 
route problem. The algorithm that we now present is not the most efficient 
one; however, it does have the advantage of being one of the simplest to 
describe and understand. Other algorithms are described in materials 
listed under Further Reading. 
Once we have designated our origin, the algorithm sweeps out to find 
the shortest route to each node in the network; in particular, it gives the 
shortest route from the origin to the designated destination node. The 
nodes are examined in order of distance from the origin. 
The set of nodes of the network is divided into two disjoint subsets, N r 
and N u, as follows: 
N r consists of the origin and all nodes that have been reached from the 
origin by a shortest route. 
N u consists of all nodes that have not yet been reached from the origin 
by a shortest route. 
Initially, N r contains only the origin; we transfer nodes from N u to N r in a 
systematic fashion to be described below. The algorithm terminates when 

5.5 The Shortest Route Problem 
369 
N, is empty. Of course, from a practical point of view, the algorithm can 
be stopped as soon as the destination node is in N r. 
To start the algorithm we find the node closest to the origin and place 
this node in N r. In case of ties we place all nodes that are closest to the 
origin in N r. This completes the first stage of the algorithm. 
At all subsequent stages of the algorithm we will need to find the node 
(or nodes) in N u that is (are) closest to the origin and move it (them) to 
Nr. We now describe how to find such a node. Let j be a node in N u and 
assume that j is at least as close to the origin as any other node in .IV,. We 
claim that j is connected to a node in N r by a directed arc (a path that 
does not go through any other nodes). Certainly j is connected to a node 
(the origin) in N r by a path (why?). If this path contains a node in N, 
other than j, then that node is closer to the origin, contradicting the 
choice of j. Thus, in this path the arc from j must go to a node in N~. 
Hence, the candidates for node j are all those nodes in N u that are 
connected to some node in N r by a directed arc. 
Thus, we select node j in N u as follows: 
For each node i in N r (including the origin) that is connected to a node 
k in N u by a directed arc, form s' = s + d, where s is the length of the 
shortest route from the origin to i (we keep track of this number for each 
node we add to N r) and d is the length of the directed arc from i to k. 
Select j as the node for which s' is minimal, put j in N r, and record its 
distance to the origin. In case of ties, choose all the tying nodes and place 
them in Nr. This completes the algorithm. This second step of selecting 
node j is repeated until the destination node is placed in Nr. 
We shall illustrate the algorithm and describe a way of keeping track of 
the necessary information by using the network in Figure 5.33. Its origin is 
node 1, and its destination is node 8. The number on each arc represents 
the length of the arc rather than the capacity as for a flow problem. 
Starting with the origin, we list each node across the top of the page. 
Under each node we list all the nodes that are connected to this node by a 
5 
2 
2 
7 
FIGURE 5.33 

370 
Chapter 5 Special Types of Linear Programming Problems 
directed arc that leaves this node. Next to each node in the list we indicate 
the length of the directed arc that joins the node at the top to that node in 
the list. For example, the list shows that node 3 is connected to nodes 4 
and 8 by directed arcs of lengths 6 and 9, respectively (Table 5.2). 
TABLE 5.2 
1 
2 
3 
4 
5 
6 
7 
8 
5-4 
5-2 
4-6 
8-2 
6-3 
7-5 
4-3 
2-5 
6-4 
8-9 
7-3 
3-5 
4-8 
8-4 
3-7 
7-7 
Initially, N r = {1}. We now find the node closest to the origin. The 
candidates are nodes 5, 2, and 3. The closest one is node 5. In the list, we 
circle the entry 5-4 under node 1 and mark the distance 4 next to node 5, 
indicating that the distance from the origin to node 5 is 4. We also cross 
out all node 5's in every column, since we have now found the shortest 
route to node 5, and the other directed arcs leading to node 5 will not be 
used. At this point, N r = {1, 5}, and our modified list is shown in Table 5.3. 
TABLE 5.3 
1 
2 
3 
4 
5-4 
6 
7 
8 
(• 
2-5 
6-4 
3-7 
7-7 
4-6 
8-2 
6-3 
7-5 
4-3 
8-9 
7-3 
3-5 
4-8 
8-4 
The nodes in N u that are connected to nodes in Nr by directed arcs can 
be found by reading the uncircled and not-crossed-off entries in the 
columns labeled by nodes in Nr. Thus, nodes 2, 3, and 6 are candidates for 
the next node to be added to N r. The distance from the origin to node 6 
via node 5 can be obtained by adding the number, 4, next to node 5 (the 
distance from the origin to node 5) to the length of the directed arc (5, 6), 
which is 3. The distances from the origin to nodes 2 and 3 come from the 
table. Since the smallest of these distances is 5, we choose node 2 to be put 
in N r. We circle 2-5 under 1 in the list, make the distance 5 next to node 
2, indicating that the shortest distance to node 2 is 5, and cross out all arcs 
leading into node 2 (there are none). At this point, N r = {1, 2, 5}, and our 
modified list is shown in Table 5.4. 

5.5 The Shortest Route Problem 
371 
TABLE 5.4 
1 
2-5 
3 
4 
5-4 
6 
7 
8 
(• 
(~ 
6-4 
3-7 
7-7 
4-6 
8-2 
6-3 
7-5 
4-3 
8-9 
7-3 
3-5 
4-8 
8-4 
Using the technique described before, we find that nodes 3, 6, and 7 are 
the next candidates for the node to be put in N r. The length of the routes 
associated with these nodes are given in Table 5.5. 
TABLE 5.5 
Route 
Length 
1~3 
7 
1--,2-,6 
5+4=9 
1--,2--,7 
5+7=12 
1--,5--,6 
4+3=7 
1--,5--,3 
4+5=9 
We select both nodes 3 and 6 to be reached along routes with length 7. 
We circle 3-7 under node 1 and 6-3 under node 5, mark the distance 7 
next to nodes 3 and 6 at the top of the columns, and cross out all node 3's 
and node 6's in every column. At this point, N r = {1,2,3,5,6}, and our 
modified list is shown in Table 5.6. Since the first and fifth columns have 
no other available directed arcs, we can ignore them, so we place a check 
mark over these columns. 
TABLE 5.6 
1 
2-5 
3-7 
4 
5-4 
6-7 
7 
8 
~) 
7-7 
4-6 
8-2 
6~) 
7-5 
4-3 
8-9 
7-3 
~ 
4-8 
8-4 
The next candidates for inclusion in N r are nodes 7, 4, and 8. The 
lengths of the routes associated with these nodes are given in Table 5.7. 

372 
Chapter 5 Special Types of Linear Programming Problems 
TABLE 5.7 
Route 
Length 
1 --> 2 --, 7 
5+7=12 
1--,3--,4 
7+6=13 
1--,3---,8 
7+9=16 
1--->5---,6--,7 
7+5=12 
1~5--,6~4 
7+8=15 
We must choose node 7, mark its distance as 12 (along two different 
paths), and circle 7-7 in column 2 and 7-5 in column 6. We also cross out 
all other occurrences of node 7 in any of the columns. Our list is now as 
shown in Table 5.8. 
TABLE 5.8 
1 
2-5 
3-7 
4 
5-4 
6-7 
7-12 
8 
@ 
9 
4-6 
8-2 
~) 
7~ 
4-3 
8-9 
~ 
~ 
4-8 
8-4 
After the next stage the list becomes as shown in Table 5.9 (verify). 
TABLE 5.9 
1 
2-5 
3-7 
4-13 
5-4 
6-7 
7-12 
@ 
9 
(3 
8-9 
~ 
~ 
~ 
8-4 
Verify that the final list is Table 5.10. 

5.5 The Shortest Route Problem 
373 
TABLE 5.10 
~/ 
~/ 
~/ 
~/ 
~/ 
~/ 
v ~ 
1 
2-5 
3-7 
4-13 
5-4 
6-7 
7-12 
8-15 
(• 
@ @ 
@ @ @ @ 
From this list we see that the shortest route to the destination (node 8) 
has length 15. The route can be found by looking in the circled numbers 
and working back from node 8. Among the circles find 8; it occurs in 
column 4. Thus, the last arc in the route is 4 ~ 8. Now find a 4 among the 
circles; it is in column 3. Thus, the route goes 3 ~ 4 ~ 8. After another 
step, we find the shortest route is 
1~3~4~8. 
The list also tells us that to get to node 6, for example, the shortest 
route has length 7. It is 
1-,5--,6. 
Equipment Replacement Problem 
Another situation that can be modeled as a shortest route problem is 
the question of when equipment that deteriorates with age should be 
replaced. Over a period of several years we would expect the price of the 
equipment to increase gradually and the cost of maintenance for the 
equipment to increase rapidly as it ages. Certainly this situation is familiar 
to every car owner. 
Consider a factory that must occasionally replace a piece of equipment 
that deteriorates with age. Assume that they are using a planning horizon 
of 5 years. At the beginning of Year 1 they will purchase a new piece of 
equipment that will be replaced after every j years. Their problem is to 
determine j so that the combined maintenance costs and purchase prices 
will be a minimum. The purchase prices and maintenance costs are given 
in Table 5.11. 
TABLE 5.11 
Purchase price 
Beginning of year 
1 
2 
3 
4 
5 
Price ($10,000) 
17 
19 
21 
25 
30 
Maintenance costs 
Age (years) 
0-1 
1-2 
2-3 
3-4 
4-5 
Cost ($10,000) 
3.8 
5.0 
9.7 
18.2 
30.4 

374 
Chapter 5 Special Types of Linear Programming Problems 
We now construct a network that will model the equipment replace- 
ment problem. The shortest path in the network will represent the optimal 
replacement scheme. Each node of the network will represent the begin- 
ning of a year in the planning horizon. We must also include a node to 
represent the end of the last year in the planning horizon. Each node is 
connected to every subsequent node by an arc. The arcs represent replace- 
ment strategies. For example, the arc connecting node 1 to node 4 
represents the strategy of purchasing a new machine at the beginning of 
Year 1 and then replacing it at the beginning of Year 4. The length of 
each arc represents the total cost (purchase price plus maintenance costs) 
of the corresponding replacement strategy. For example, the length of arc 
(1,4) is 17 + (3.8 + 5.0 + 9.7) = 20.5. Notice that this arc represents 
owning a piece of equipment for 3 years, so that there is a maintenance 
cost for each of the years. All the arcs for this network and the computa- 
tions for their lengths are given in Table 5.12. 
TABLE 5.12 
(1,2) 
17 
(1,3) 
17 
(1,4) 
17 
(1,5) 
17 
(1,6) 
17 
(2,3) 
19 
(2,4) 
19 
(2,5) 
19 
(2,6) 
19 
(3,4) 
21 
(3,5) 
21 
(3,6) 
21 
(4,5) 
25 
(4, 6) 
25 
(5, 6) 
30 
+ 3.8 
= 20.8 
+ (3.8 + 5.0) 
= 25.8 
+ (3.8 + 5.0 + 9.7) 
= 35.5 
+ (3.8 + 5.0 + 9.7 + 18.2) 
= 53.7 
+ (3.8 + 5.0 + 9.7 + 18.2 + 30.4) = 84.1 
+ 3.8 
= 22.8 
+ (3.8 + 5.0) 
= 27.8 
+ (3.8 + 5.0 + 9.7) 
= 37.5 
+ (3.8 + 5.0 + 9.7 + 18.2) 
= 55.7 
+ 3.8 
= 24.8 
+ (3.8 + 5.0) 
= 29.8 
+ (3.8 + 5.0 + 9.7) 
= 39.5 
+ 3.8 
= 28.8 
+ (3.8 + 5.0) 
= 33.8 
+ 3.8 
= 33.8 
The network that models this equipment replacement problem is shown in 
Figure 5.34. 
Any route through the network will represent a 5-year plan for equip- 
ment replacement. For example, the route 1 ~ 3 ~ 4 ~ 6 represents 
purchasing a new piece of equipment at the beginning of Year 1, of Year 
3, and of Year 4. The cost of this strategy is 25.8 + 24.8 + 33.8 = 84.4, or 
$844,000. The reader may use the shortest route algorithm to show that 
the optimal replacement strategy is to purchase a new piece of equipment 
at the beginning of Years 1 and 3. The cost of this strategy is $653,000. 
In general, if the planning horizon encompasses n time units, the 
network will have n + 1 nodes. The length of the arc joining node i to 

5.5 The Shortest Route Problem 
375 
39.5 
NN 
\ 
- 
5s.7 
"NNt 
33.8 
- _ 
FIGURE 5.34 
node j (i < j)will be the sum of the purchase price at the beginning of 
year i and the maintenance costs for the first j - i time periods. 
5.5 
EXERCISES 
In Exercises 1-4 find the shortest path between the indicated origin and the 
indicated destination. 
6 
L 
Origin 
5
~
4
 
1 
3 ~ ( ~ 5  
Destination 
4 _.,,,,.'~= ~ 
jr 
5 
7 
Origin ~ 
7 ] 
4 V 3  
16 
Destination 
6
~
 
2 

~ 
Chapter 5 Special Types of Linear Programming Problems 
. 
7 
4 
4 
4 
Origin 
3 
Destination 
4. 
Origin 
Destination 
In Exercises 5 and 6 we give tables of purchase prices and maintenance costs for 
pieces of equipment. Develop a replacement policy for the equipment. 
5. 
Year 
1 
2 
3 
4 
5 
6 
Purchase price 
($10,000) 
5 
6 
8 
9 
11 
12 
Age (years) 
0-1 
1-2 
2-3 
3-4 
4-5 
5-6 
Maintenance cost 
($10,000) 
3.0 
3.5 
5.5 
8.5 
12.0 
18.0 
Year 
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
Purchase price 
($10,000) 
Age (years) 
Maintenance cost 
($10,000) 
3.0 
3.5 
4.1 
4.9 
6.0 
6.5 
6.7 
6.7 
7.5 
8.0 
0-1 
1-2 
2-3 
3-4 
4-5 
5-6 
6-7 
7-8 
8-9 
9-10 
1.0 
1.5 
2.1 
2.8 
4.0 
7.0 
20.0 7.0 
8.0 
9.5 

5.5 The Shortest Route Problem 
377 
5.5 PROJECTS 
Charlie Anderson lives in the town of Hatboro in eastern Montgomery County. 
Frequently he travels to northeast Philadelphia to visit his parents. The network 
of roads connecting the two points is shown in Figure 5.35. Node 1 is Charlie's 
home and node 45 is his destination. This network is a combination of rural 
roads, urban streets, and expressways. The rural roads are in the area from 
Hatboro to the outskirts of Philadelphia (nodes 1 through 20). These roads have 
long stretches without intersections, few traffic signals and stop signs, and 
generally light traffic. The urban streets begin near the city limits of Philadel- 
phia (nodes 21 through 45). They are generally more congested and have many 
traffic signals. The expressway joins nodes 21, 44, and 45. 
Charlie has computed average times for traveling over each arc of the 
network. These times along with the length of each arc are given in Table 5.13. 
(a) Compute the route Charlie should use to travel the shortest distance when 
going from Hatboro to northeast Philadelphia. 
(b) Compute the route Charlie should use to get to northeast Philadelphia most 
quickly. 
(c) Compare your answers in parts (a) and (b)with the route 1 ~ 3 ~ 4 ~ 6 
10 ~ 17 ~ 19 ~ 23 ~ 29 ~ 34 ~ 38 ~ 41 ~ 42 ~ 43 ~ 45. Discuss 
Rte. 
Hatboro 
Start 
North 
Rte. 611 
Rte. 73 
28 
Rte. l 
~7 
'939 
43 4! 
"Rte. 13 
End 
NE Philadelphia 
FIGURE 5.35 
63 
95 

378 
Chapter 5 
Special Types of Linear Programming Problems 
TABLE 5.13 Distances and Times between Nodes 
Distance 
Time 
Nodes 
(miles) 
(sec) 
1-2 
1.5 
380 
1-3 
0.7 
190 
2-5 
3.1 
360 
3-2 
1.4 
210 
3-4 
0.7 
200 
4-6 
3.6 
380 
4-7 
1.1 
230 
5-6 
0.9 
120 
5-8 
1.6 
140 
6-9 
1.8 
225 
6-10 
1.8 
220 
7-11 
3.5 
450 
7-13 
3.1 
420 
8-9 
0.9 
150 
8-20 
2.0 
275 
9-18 
1.7 
255 
9-20 
1.2 
230 
10-12 
0.4 
85 
10-17 
0.8 
170 
11-12 
0.8 
110 
11-14 
1.8 
235 
12-15 
1.5 
220 
13-14 
1.5 
280 
13-28 
4.8 
750 
14-16 
0.6 
110 
15-16 
0.8 
140 
15-24 
2.2 
340 
16-25 
1.6 
230 
17-18 
0.2 
60 
17-19 
0.4 
95 
18-19 
0.3 
80 
18-22 
2.6 
400 
19-23 
1.6 
160 
20-21 
1.3 
260 
21-22 
1.6 
250 
21-44 
6.5 
480 
Distance 
Time 
Nodes 
(miles) 
(sec) 
22-33 
1.2 
220 
23-24 
0.8 
120 
23-29 
0.8 
140 
24-26 
1.6 
220 
24-30 
0.6 
175 
25-26 
0.6 
170 
25-27 
0.8 
170 
26-27 
0.6 
160 
26-31 
0.8 
190 
27-28 
0.3 
135 
28-32 
0.7 
180 
29-30 
0.6 
120 
29-33 
0.8 
150 
29-34 
0.6 
180 
30-31 
1.4 
225 
31-32 
0.9 
180 
31-35 
1.3 
320 
32-36 
1.3 
320 
33-34 
0.5 
100 
33-37 
1.0 
140 
34-35 
1.4 
240 
34-38 
1.3 
220 
35-36 
1.3 
230 
35-42 
1.5 
240 
36-43 
0.9 
190 
37-38 
1.4 
210 
37-39 
0.4 
80 
38-41 
1.0 
140 
39-40 
1.3 
240 
40-41 
1.2 
305 
40-44 
0.5 
55 
41-42 
0.2 
70 
42-43 
0.7 
240 
43-45 
0.5 
100 
44-45 
3.3 
520 
what penalties Charlie incurs by not using either the minimum distance or 
the minimum time routes. 
2. On the basis of published figures for new car prices and costs for maintenance, 
develop an optimal replacement policy for the time period 10 years ago until the 
present. 
Further Reading 
Djiskstra, E. W. "A Note on Two Problems in Connection with Graphs." Numerische Math. 1 
(1959), 269-271. 

5.6 The Critical Path Method 
379 
Dreyfus, S. E. "An Appraisal of Some Shortest-Path Algorithms," Perspectives on Optimiza- 
tion: A Collection of Expository Articles (A. M. Geoffrion, Ed.), pp. 197-238 Addison- 
Wesley, Reading, MA, 1972. 
5.6 THE CRITICAL PATH METHOD 
Scheduling models are an important area of study in operations re- 
search. We have examined some of these models in the examples and 
exercises in previous sections. The assignment problem gave one form of a 
scheduling model, as we showed in Project 1 of Section 5.2. This project 
discussed an idealized method of scheduling nurses on a floor of a hospital. 
The traveling salesman problem occurred as a scheduling model for a job 
shop. In this situation we were minimizing total setup time. The literature 
has extensive discussions of the airline flight crew problem. Some refer- 
ences to this problem and other scheduling problems are given under 
Further Reading. 
In this section we discuss yet another model for scheduling. This model, 
the critical path method, or CPM, was originally conceived by researchers 
at E. I. duPont de Nemours Company and Remington Rand in a collabora- 
tive effort. John W. Mauchly, James E. Kelley, and Morgan Walker had 
the lead roles in its development. It was tested in early 1958 on scheduling 
the construction of a $10 million chemical plant in Louisville, Kentucky. 
Its advantage in this situation was the small effort needed to incorporate 
design changes into the schedule. 
Modifications of the original model have been made, but the basic 
concepts remain the same. The model is based on a network that repre- 
sents the activities and completion times of a process. CPM is used in a 
variety of settings, including larger construction projects, complicated 
maintenance procedures, installation of computer systems, and production 
of motion pictures. A related planning process called PERT (Program 
Evaluation and Review Technique) is widely used for government activi- 
ties, especially in the Department of Defense. We will limit our discussion 
to the network model for CPM. 
It is typical of construction projects that many unrelated activities can 
be performed simultaneously with the only constraint being that certain 
activities must precede others. For example, the schedule for building a 
house cannot call for tarring the roof before the supporting beams have 
been erected and covered with sheeting. Aside from these mechanical 
constraints, the other important constraint is meeting the scheduled com- 
pletion date. CPM models the schedule of the activities of a project and 
indicates an order in which the individual activities should be performed. 
The nodes in the CPM network represent times, but not measured in 
days from the beginning of the project. Rather the times are completions 
of activities such as excavation completed or plumbing roughed in. In some 

~0 
Chapter 5 Special Types of Linear Programming Problems 
of the literature these times are called events. The arcs of the network 
represent activities such as install floor or paint walls. The source node is 
the beginning of the project and the sink node is the end of the project, 
and these can be the only source and sink, respectively, for the network. 
Instead of the usual specification giving a criterion for connecting two 
nodes with an arc, it is easier for CPM networks to give the dual 
specification: a criterion for having two arcs join at a node with one arc 
entering the node and one leaving. Two such arcs representing activities 
A i and A i are joined at a node with A i entering and Aj leaving if activity 
A i must be completed before activity A i can begin. For example, part of a 
CPM network might be 
"O 
O 
Erect roof 
Install sheeting 
support beams 
Tar roof 
In joining arcs one must be careful not to produce sequences in the 
network that do not fit with reality. If activities B and C must follow 
activity A, but are independent, they should be diagrammed as 
and not as 
B 
C 
A 0 
Oc 
A slightly more complicated situation occurs when there are four 
activities, A, B, C, and D, where A and B both precede C but only B 
precedes D. None of the following correctly diagrams this situation (why?). 
" 
0 
A 
C 
A 
C 
B 
D 
_ 
~' 
A 
B 
This dilemma is solved by introducing a dummy activity or logical re- 
straint. It is an arc of the network, usually shown as a dashed rather than a 

5.6 The Critical Path Method 
~81 
solid line, which represents no work. Using this device we can correctly 
represent the above situation as 
A 
- ~._.J 
c 
/ 
/ 
/ 
B 
D 
It is also important that no loops be introduced in a CPM network. A 
loop would mean that an activity must both precede and follow another--a 
logical impossibility. Loops are easily detected in a small network. In large 
networks with thousands of nodes, loops may be introduced by errors in 
the input data. Consequently, a first step in a CPM algorithm is to check 
the network for loops. 
Once a correct precedence network has been established for a problem, 
the key step in the CPM process must take place. An estimate for the time 
necessary to complete each activity must be given. The time estimates 
should be as good as possible. However, after the critical path is found, the 
planner can reexamine his or her estimates for the path to see if any are 
unrealistic. 
Typically the events of the projects are numbers but not necessarily in 
any particular order. In this way more events can be added to a network 
without having to renumber all previous events. The activities are specified 
by giving the pairs of nodes that are joined by their representing arcs. To 
ensure that each activity has a unique designation, dummy activities are 
inserted in case two activities connect the same two nodes. Thus, 
A 
B 
becomes 
A 
X\ 
j/ C/--, 
EXAMPLE 1. 
Consider the several-weekend project faced by a subur- 
ban homeowner who wants to replace his small metal storage shed with a 
larger, more solidly constructed one, building the newer one on the same 
site as the old shed. Since he has no other place to store tools, he plans to 
build the new shed around the old one and then to demolish the old shed. 

382 
Chapter 5 Special Types of Linear Programming Problems 
(;rad~ 
Install walkway 
Change ladder 
~., 
v 
4 
storage 
~ 
I 
Remove ~ 
lnstallncwdoor 
! 
w "~ 
i 
old ~ 
I 
/ 
/ 
door/ " 
- 
J i 
/ 
/ 
' 
Layout 
I Modify 
I 
Lay 
I 
Frame 
/Remove 
Install 
Removc 
i 
site 
/drainagcl 
found- 
I 
walls 
/ old 
new 
old 
floor 
.
.
.
.
 
9 
9 
4 
FIGURE 5.36 
He has drawn the precedence network shown in Figure 5.36. The number 
below each activity represents the time (hr) estimate for that activity. 
After the network representing the project activities has been deter- 
mined and checked for its accuracy, the critical path algorithm can be 
started. The first step of the algorithm is to compute the early event time 
for each node. The early event time TE(j) for node j represents the 
soonest that node j can be reached once all activities on all paths leading 
to the node have been completed. Mathematically it can be computed by 
modifying the shortest path algorithm to compute the longest path. 
In this example, we give the details of computing the early event times 
for the first six nodes. All the early event times are shown in square boxes 
in Figure 5.37. For the source node we have TE(1) = 0, representing the 
beginning of the project. Node 2 has only one arc leading into it, so that 
TE(2) = TE(1) + 1 = 1, that is, TE(2) is the sum of the activity time and 
the previous event time. Likewise, TE(3) = TE(2) + 1 = 2. 
Since node 4 has two arcs leading into it, we must choose the longer of 
the arcs to compute TE(4). Remember that TE(j) represents the earliest 
that the event can occur after all activities leading into it are completed. 
Thus, 
TE(4) = max{TE(2) + 2, TE(3) + 0} 
= max{3, 2} 
=3. 

5.6 The Critical Path Method 
383 
G 
~ 
1 
rade~ 
Instal walkway 
Change ladder ['~ 
/3 x..J 
~ 
4 
\ 
storage 
~ 
/ 
Remove ~ 
Install new door 
D 
D 
ID 
N/ 
89 
N 
N 
\\@ 
Lay out 
Modify I Lay 
I Frame / Remove Install 
Remove Install \ 
site 
drainage ~ found- [ walls / old 
new 
old 
floor ~, 
6 ~ 
t'~ati~ 
r 
-t'~r~176 _~r~176 ~walls 
~ 
_(~ 
@ 
l --k, / 
2--~ 
4-~,) 
5 =~,~ 
1 zk, fJ 
2 :~) 
3 ~ 
3 ~ 
) 
~ 
Trim 
/ 
~ 
//~ 
/ 
interfering 
\\ 
trees 
fl 
\ Install siding Ii/ 
\Finish interior/ 
4/ 
Run electric line 
/ 
4 
FIGURE 5.37 
Likewise, node 5 has two arcs coming into it. Therefore, 
TE(5) = max{TE(2) + 2, TE(4) + 4} 
= max{3, 7} 
=7. 
We also find TE(6) = TE(5) + 3 = 10. Thus, in general, 
TE(j) = m.ax {TE(i)+ length(/-~)}, 
l 
where the maximum is taken over all arcs leading into node j. 
The early event time computed for the node that represents project 
completion (the sink) is the total amount of time necessary to complete the 
project. If this time is unaceptable, the project manager can at this point 
investigate ways of speeding up some of the activities. He or she can 
choose the activities to concentrate on after the critical path (or several of 
them) is (are) known for the project. 
The second step of the algorithm is to compute the late event time for 
each node. The late event time TL(j) for node j is the latest time by which 
node j can be reached to still have the project completed in the shortest 
possible time. Again in this example, we give the details of computing the 
late event times for nodes 7 through 12. All late event times are shown in 
diamonds below the early event times in Figure 5.38. 
For the sink node we have TL(12) = 22, the total project time. For node 
11, since the activity of laying the floor takes 3 units, the start of floor 

384 
Chapter 5 Special Types of Linear Programming Problems 
B 
Grade site ~ 
Install walkway 
Change ladder ['2"] 
U~'x 
~ 
0"~ 
4 
\ 
storage 
~_ (,~ 
[ 
~ 
~ 
Install new door 
I 
I 
Y(3) 
/ 
R em o ve/1 k.~/~ 
2 
\\ 
I 
I 
" 
I 
old 
/ 
v 
\ 
l 
1 
]door/ 
\\ 
D 
ID 
Eft/ 
D/ 
D 
| 
N 
9 
Lay 
Frame 
Remove 
Install 
Remove 
Install 
out 
Modify 
I 
Lay ,t I drainage | 
found- /  wr:: e/R,:mov< 
new 
old 
fl~tall~t 
site _(~_ 
(,~ation (~ 
_ (,~roof _f-g,,xroof t,,i~walls ~ 
_ 
e 
e 
iTnrtimferin~g /~- 
~ 
// 
~(~ 
(~ 
/~ 
k\ trees 
/ 
~ Install siding t/ 
XFinish interior/ 
4/, 
~
,
 
Run electric line 
/ 
4 
FIGURE 5.38 
laying must occur at 19 units. That is, 
TL(ll) = TL(12)--3 =22--3 
= 19. 
There are two paths from node 10 to node 12. We must use the longer of 
the two to compute TL(10). We have 
TL(10) = min{TL(ll) -- 3, TL(12) -- 5} 
= min{ 16, 17} 
=16. 
For nodes 8 and 7, TL(9) = TL(10) - 2 = 14 and TL(8) = TL(12) -- 2 = 20, 
since only one arc leaves each of these nodes. For node 7, we find that 
TL(7) = min{Te(8) - 1, TL(9) - 1, TL(10) -- 4} 
= min{ 19, 13, 12} 
= 12. 
In general, we have 
TL< j) = m!n {TL<i) -- length(j-~)}, 
l 
where i runs through all nodes for which there is an arc from node j to 
node i. The fact that TL(1) must be 0 is a good check on the accuracy of 
the event time calculations. 
Recall that for any node j, TL(j) is the latest time by which all the 
activities leading from node j must be started to have the project finished 
on schedule. Also, TE(j) is the soonest time by which all the activities 

5. 6 The Critical Path Method 
385 
leading from node j can be started. These activities could not be started 
earlier because at least one activity leading to node j had not been 
completed. The difference TL(j) 
- TE(j) is called the float time or slack 
time. Any event that has zero float time is called a critical event. The next 
activity must be started without delay. A critical path is a path in the 
network that passes through only critical events and whose length is the 
value of T E at the project-completion node. There may be more than one 
critical path in a network. The activities along a critical path are the ones 
whose lengths are determining the total length of the project. 
In our example there is only one critical path, namely, 1 ~ 2 ~ 4 
5 ~ 7 ~ 10 ~ 11 ~ 12. This path is indicated by the heavy lines in 
Figure 5.38. 
/x 
5.6 
EXERCISES 
For the precedence networks given in Exercises 1-4, 
(a) find the early event times; 
(b) find the late event times; 
(c) find a critical path. 
. 
3 
8 
: 
4"~ 
3 ~  
1 
6 
~ 
4 
= 
5 
: 
3 
7 
7 
8 
5 
~ 
2 
r
~
~
 
6 
) 
7 
4 
6 
8 

~l]~ 
Chapter 5 Special Types of Linear Programming Problems 
~~ 
'6 
- 
7 
\ 
4 -~ 
8 
.G2 
6 
\ 
/ 
D 
G 
4 
_f~. 
2 
2 
9 
// 
5 
[ 
1 
2 _@ 
3 
. -\ 
v 
The networks given in Exercises 5 and 6 are to be models for certain projects. 
However, they are incorrectly constructed. Find and correct, if possible, the errors 
in the networks. 

5.6 The Critical Path Method 
3~7 
In Exercises 7 and 8 lists of activities and activity times are given. Also, we give 
the logical constraints on each activity. Construct a precedence network for each 
list of activities and find a critical path in the network. 
. Z 1 precedes A2, A3, A4, A 5, and A 6. 
A 10 follows A 2. 
A 11 follows A 3. 
A 7 follows A 5. 
A 8 follows Z 4 and precedes A lz. 
A 9 follows A 4 and A 7. 
A 6 and A 9 precede A13. 
A12 follows A10 and All. 
A12 and A13 terminate at the same time. 
Activity 
Time 
Activity 
Time 
A 1 
7 
A 2 
5 
A 3 
8 
A 4 
3 
A 5 
1 
A 6 
2 
A 7 
8 
A 8 
6 
A 9 
11 
A10 
2 
All 
5 
A12 
7 
A13 
9 
A1, A2, A3, and A17 start at the same time. 
A 1 precedes A 4 and A 5. 
A 6 follows A 2. 
A3 precedes A 7- 
A 10 follows As. 
A13 follows A 6, A7, and A10. 
A 4 precedes A s and A 9. 
A 12 follows As. 
A 9 precedes A15 and A16. 
All follows A17. 
A 14 follows A 12. 
A 11 precedes A 18. 
A 19 follows A15. 
A13 , A14 , A16 , A18 , and A19 terminate at the same time. 

388 
Chapter 5 
Special Types of Linear Programming Problems 
Activity 
Time 
Activity 
Time 
h 1 
6 
Z 2 
7 
Z 3 
5 
a 4 
4 
A 5 
8 
Z 6 
9 
h 7 
3 
A 8 
7 
Z 9 
6 
Zl0 
9 
All 
4 
A12 
3 
A13 
6 
A14 
8 
Z15 
7 
Z16 
5 
A17 
4 
A18 
6 
A19 
7 
Further Reading 
Antell, J. M., and Woodhead, R. W. Critical Path Methods in Construction Practice, 2nd ed. 
Wiley, New York, 1970. 
Moder, J. J., and Phillips, C. R. Project Management with CPM and PERT, 2nd ed. Van 
Nostrand, New York, 1970. 
Shaffer, L. R., Ritter, J. B., and Meyer, W. L. The Critical Path Method. McGraw-Hill, New 
York, 1965. 
5.7 COMPUTER ASPECTS (OPTIONAL) 
In this chapter we have described several linear programming problems 
that because of their specialized structure are more efficiently solved using 
their own algorithms rather than using the simplex algorithm. In fact, all 
these models can be framed in terms of networks, and some commercial 
simplex optimizers can recognize that the coefficient matrix of such a 
model is a network even if the user had not envisioned it as such. The 
algorithm that is most widely available for general network problems is the 
out-of-kilter algorithm, which solves the minimum cost flow problem in a 
capacitated network. 
The minimum cost flow problem can be mathematically formulated as 
follows. Consider a network with n nodes. With each node we associate a 
number bi, indicating the availability of a single commodity at node i. If 
b i > 0, there is b i of the commodity available at node i, and node i is 
called a source; if b i < 0, there is a demand for b i of the commodity at 
node i, and node i is called a sink; and if b i = 0, node i is called an 
intermediate or transshipment node. Let u ij and lij denote the upper and 
lower capacities of arc (/--~). Let c ij denote the cost of shipping one unit 
from node i to node j along arc (i, j). Since flow can be neither created 
nor destroyed at any node, we obtain the conservation of flow equations 
n 
~ 
E 
Xij -- 
Xki = b i, 
i = 1,2,..., n. 
j=l 
k=l 

5.7 Projects 
389 
Of course, we are interested in minimizing the total cost 
n 
~ E CijXij" 
i=lj=l 
Thus, a mathematical formulation of the minimum cost flow problem is 
Minimize 
n 
n 
E E CijXij 
i=1 j--1 
subject to 
n 
n 
E Xij -- E Xki-- bi, 
j=l 
k=l 
i= 1,2,...,n 
0 ~ lij <__ Xij <__ Uij, 
i= 1,2,...,n 
j = 1,2, 
,n. 
When this problem is represented by a network diagram, it is convenient 
to label each arc with the triple numbers [Cij, Uij, lij]. If an arc has no 
upper bound on its capacity, only the cost of shipping one unit of goods 
along the arc is given. 
The out-of-kilter algorithm finds, for a given amount of flow, the path 
that is cheapest. We examined a special case of this problem as we 
discussed the shortest route algorithm. There the amount of flow was one 
unit and each arc had an upper capacity of one unit and a lower capacity 
of zero units. 
All the models that we have discussed in this chapter can be trans- 
formed to minimum cost flow models. Thus, the out-of-kilter algorithm 
could be used to solve any of them. As an example of the types of network 
computer codes available, we discuss the features of a typical code for the 
out-of-kilter algorithm. However, a description of the algorithm itself is 
beyond the scope of this book. 
Input 
The network is specified by giving a list of arcs. Each arc is specified by 
giving the names of the nodes it joins in the order "from-to." For each arc 
the user must also specify the upper bound on the capacity (using a large 
number if the capacity is unbounded), the lower bound on the capacity, the 
cost, and the initial flow. In many cases the initial flow is chosen to be 
zero. The computer code may be designed to handle only integer flow 
problems, so that the cost, initial flow, and capacities must be restricted to 
integer variables. 

390 
Chapter 5 Special Types of Linear Programming Problems 
Since some codes do not have built-in routines for input verification, the 
user may have to check the inputs to make sure that: 
1. there are no dangling nodes in the network; 
2. there is only one source and one sink; 
3. the initial flow is conserved at each interior node, namely, at node j, 
EXpj "~ EXjq; 
P 
q 
4. the upper bound on each arc is greater than or equal to the lower 
bound for that arc. 
Output 
The code will have output options that allow the user to save the 
problem and restart it after making modifications to the network. The 
printed output will include a listing of the active arcs at an optimal 
solution along with the flow for each of these arcs. The output may also 
include a list of the zij - 
cij, which are marginal costs for increasing the 
flow one unit along the arcs (i, j). 
EXAMPLE 1. 
Consider the transportation problem (Example 1, Section 
5.1) with a cost matrix and demand and supply vectors as follows: 
[579 6] 
[120] 
100 
C= 
6 
7 
10 
5 , 
s= 
140 , 
and 
d= 
60 
7 
6 
8 
1 
100 
80 " 
120 
A minimum cost flow network that models this problem is given in Figure 
5.39. Note that each source is a node and each destination is a node. Each 
source node is connected to every destination node by an arc. The cost of 
each arc is that specified by the matrix C. Also included are two additional 
nodes, a supersouree and a supersink. These nodes serve to place the 
supply and demand constraints on the sources and destinations. The 
supersource is connected to each source by an arc that has cost zero, lower 
bound zero, and upper bound equaling the supply capacity of the source. A 
flow from the supersource to a source can be thought of as the process of 
manufacturing the goods at the source. The conservation of flow at each 
source guarantees that no more than the available supply may be shipped 
from that source. 
Likewise, each destination is connected to the supersink by an arc with 
upper bound equaling the demand at the destination, lower bound zero, 
and cost zero. 

5.7 Projects 
391 
5/ t3, 
@___ 
FIGURE 5.39 
We show in Table 5.14 the input necessary for solving this problem 
using the Unisys UKILT 1100 Out-of-Kilter Network Optimization System. 
There is one additional arc required for this system, connecting the 
supersink to the supersource. The system expects all nodes to have arcs 
leading both in and out of them. Such a network is called a circularization 
network. The cost of the new arc is zero, and the lower and upper bounds 
are equal to the total supply. This ensures that the total supply is used and 
that the total demand is met. The nodes are labeled by numbers and the 
arcs, by the pair of nodes that they join. The initial flows are all chosen to 
be zero. Table 5.15 shows the output of UKILT 1100 for this problem. 
Besides the out-of-kilter algorithm, there are implementations of the 
various specialized algorithms for particular settings. For example, the 
critical path method is typically used outside operations research settings, 
so that easily used codes for this algorithm have been developed. 
CPM Codes 
The CPM codes will produce charts and tables that can be used in the 
field by the project supervisors and foremen. Much of the effort in writing 
a CPM code goes into making this output meaningful to those who must 
use it. 
There are about 100 commercially available CPM codes. These are 
owned either by larger corporations that do many complicated scheduling 

392 
Chapter 5 Special Types of Linear Programming Problems 
TABLE 5.14 
Upper 
Lower 
From 
To 
Cost 
bound 
bound 
Initial 
flow 
1 
BEGIN 
2 
TRANSPORTATION 
3 
ARCS 
4 
Sl 
D1 
5 
i000 
0 
0 
5 
Sl 
D2 
7 
i000 
0 
0 
6 
S1 
D3 
9 
i000 
0 
0 
7 
S1 
D4 
6 
i000 
0 
0 
8 
S2 
D1 
6 
I000 
0 
0 
9 
S2 
D2 
7 
i000 
0 
0 
i0 
S2 
D3 
i0 
i000 
0 
0 
ii 
$2 
D4 
5 
i000 
0 
0 
12 
S3 
D1 
7 
i000 
0 
0 
13 
S3 
D2 
6 
i000 
0 
0 
14 
S3 
D3 
8 
i000 
0 
0 
15 
$3 
D4 
1 
i000 
0 
0 
16 
SSA 
Sl 
0 
120 
0 
0 
17 
SSA 
$2 
0 
140 
0 
0 
18 
SSA 
S3 
0 
i00 
0 
0 
19 
D1 
SSB 
0 
i00 
0 
0 
20 
D2 
SSB 
0 
60 
0 
0 
21 
D3 
SSB 
0 
80 
0 
0 
22 
D4 
SSB 
0 
120 
0 
0 
23 
SSB 
SSA 
0 
360 
360 
0 
24 
END 
25 
SOLVE 
26 
OUTPUT ii 
27 
REPORT ii 
28 
STOP 
tasks or by consulting firms. Experience has shown that networks repre- 
senting more than 250 activities should certainly be processed on a 
computer. In fact, the break-even point between hand and computer 
processing may drop to as low as 100 activities if the network is extremely 
complicated. The larger CPM codes can handle problems with as many as 
10,000 activities. The most popular CPM/project management codes are 
now run on PCs. They provide very easy to understand graphical displays 
and easy to use editing features. Some examples of such software are 
Super-Project, Time-Line, and Microsoft Project. 
Besides using the CPM algorithm on a given network, a typical code 
does a substantial amount of input verification. It will check for loops, 
dangling activities, and duplicate activities. Most codes also have provi- 
sions for saving the network in computer-readable form (e.g., on disk or 
tape) so that changes in the network can be easily made. 

5.7 Projects 
393 
TABLE 5.15 
TITLE 
TRANSPORTATION 
NUMBER 
OF NODES: 
9 
NUMBER 
OF ARCS: 
20 
TOTAL 
COST: 
1900 
*********************** 
* 
THE SOLUTION 
IS OPTIMAL 
* 
*********************** 
TRANS PORTAT I ON 
ARC NUMBER 
FROM NODE 
TO NODE 
COST 
MARG 
COST 
UPPER 
BOUND 
LOWER 
BOUND 
FLOW 
1 
S1 
D1 
5 
0 
i000 
0 
i00 
2 
Sl 
D2 
7 
1 
i000 
0 
0 
3 
Sl 
D3 
9 
0 
i000 
0 
20 
4 
S1 
D4 
6 
2 
i000 
0 
0 
5 
$2 
D1 
6 
0 
i000 
0 
0 
6 
$2 
D2 
7 
0 
i000 
0 
60 
7 
$2 
D3 
i0 
0 
i000 
0 
60 
8 
$2 
D4 
5 
0 
i000 
0 
20 
9 
S3 
D1 
7 
5 
I000 
0 
0 
I0 $3 
D2 
6 
4 
I000 
0 
0 
ii $3 
D3 
8 
2 
i000 
0 
0 
12 $3 
D4 
1 
0 
i000 
0 
i00 
13 SSA 
S1 
0 
-i 
120 
0 
120 
14 SSA 
$2 
0 
0 
140 
0 
140 
15 SSA 
$3 
0 
-4 
i00 
0 
i00 
16 D1 
SSB 
0 
-4 
i00 
0 
i00 
17 D2 
SSB 
0 
-3 
60 
0 
60 
18 D3 
SSB 
0 
0 
80 
0 
80 
19 D4 
SSB 
0 
-5 
120 
0 
120 
20 SSB 
SSA 
0 
i0 
360 
360 
360 
5.7 
EXERCISES 
1. Why is a supersink necessary in modeling a transportation problem as a 
minimum cost flow problem? What aspect of the problem could it represent? 
2. Model the assignment problem as a minimum cost flow problem. Assume that 
there are n persons and n jobs. 
3. Model the shortest route problem as a minimum cost flow problem. 
4. Model the critical path method as a minimum cost flow problem. 
5.7 
PROJECTS 
1. The transshipment problem. In many applications of the transportation problem 
we encounter a situation in which a node serves merely as a transshipment 

394 
Chapter 5 
Special Types of Linear Programming Problems 
-8 
-5 
+6 
0 
FIGURE 5.40 
point. That is, there is neither supply nor demand at the point, but goods are 
shipped through the point. Consider the network in Figure 5.40, where the 
numbers beside the nodes are the b i described in this Section. The nodes can be 
divided into five classes. 
Pure sourcemall arcs lead away from the node, b i > 0 
Pure sink--all arcs lead into the node, b i < 0 
Transshipment sourcemarcs leading into and out of the node, b i > 0 
Transhipment sinkmarcs leading into and out of the node, b i < 0 
Pure 
transshipment 
point--arcs 
leading 
into 
and 
out 
of the 
node, 
b i = 0 
(a) Classify the nodes in the network in Figure 5.40. 
Every transshipment problem can be formulated as a minimum cost flow 
problem. This formulation may require the introduction of a supersource or 
supersink. A supersource will be necessary when there is more than one pure 
source in the network. This supersource is connected to each pure source with 
an arc whose capacity has an upper bound equal to the supply at the pure 
source. Likewise, a supersink is necessary when there is more than one pure 
sink. A minimum cost flow problem specifies a particular value of flow that must 
be sent from the supersource to the supersink. Because there must be conserva- 
tion of flow at each node, the sum of the amounts available at the sources of a 
transshipment problem must equal the sum of the amounts required at the sinks 
before the problem can be formulated as a minimum cost flow problem. If these 
sums are not equal, an additional pure source or pure sink must be added as a 
dummy to the network to compensate for this difference. 
(b) Why does a dummy sink node have to be connected only to the source 
node and not to the transshipment nodes or other sink nodes? 
(c) Classify the nodes of the network in Figure 5.41. 
(d) Formulate the network in Figure 5.41 as a minimum cost flow problem. 

5.7 Projects 
395 
+5 
+3 
+6 
-2 
+2 
-8 
0 
-5 
FIGURE 5.41 
2. If your computer center has a network code, formulate and run the following. 
(a) Figure 5.4, Section 5.4 
(b) Example 2, Section 5.1 
(c) Example 1, Section 5.1 
(d) The shortest route example in Section 5.5 

APPENDIX 
.4 
Karmarkar's 
Algorithm 
I 
MPLEMENTATIONS OF THE simplex method, and later the revised 
simplex method, have been used as the primary techniques for 
solving linear programming problems since the simplex method was 
first discovered by Dantzig. The experience of solving a huge number of 
linear programming problems over a number of years led to the conjecture 
3 
that the number of iterations of the simplex method is proportional to ~n, 
where n is the number of variables in the problem. Furthermore, the 
number of operations per iteration is proportional to mn, where m is the 
number of constraints. Such an estimate indicates that the simplex algo- 
rithm is a polynomial time algorithm because its running time is propor- 
tional to a polynomial in n. However, in 1972 Klee and Minty provided a 
particular linear programming problem that interacted badly with the rules 
for choosing entering and departing variables and consequently had a 
running time proportional to 2"/2 _ 1. Thus the simplex algorithm is not a 
polynomial algorithm after all. Note that the problem used to disprove the 
conjecture was tailored to the particular pivoting strategy used by the 
simplex method. Jeroslow later showed that for any choice of pivoting 
397 

~98 
Appendix 
A: Karmarkar's Algorithm 
strategy, an equivalent problem could be constructed that had the same 
bad behavior. Although problems such as this have not arisen in applica- 
tions, there is no guarantee that such a problem will not occur. 
In the mid-1980s a new approach to solving linear programming prob- 
lems was proposed. It was based on work done in investigating algorithms 
to follow paths in the interior of the convex set of feasible solutions. 
Although these algorithms were first devised in the 1950s, they did not 
initially achieve the performance of the simplex method. In 1979 Khachiyan 
proved that by using the appropriate interior path algorithm, one can 
guarantee that a linear programming problem will be solved in polynomial 
time. This proof renewed interest in interior path methods. 
Instead of following the edges of the convex set from extreme point to 
extreme point, eventually reaching an optimal solution, the interior path 
methods burrow through the interior of the set to find an optimal solution. 
Because they are not constrained to the directions of the edges, nor their 
length, it is reasonable to assume that the interior path methods might be 
faster than the edge-following method. However, there has been no clear 
demonstration of the superiority of interior path methods. Most of the 
users of sophisticated linear programming software for frequent solutions 
of large problems still use software based on the edge-following simplex 
algorithm. 
The most successful of the interior path methods has been that due to 
Karmarkar. We will give a description of the main points of the algorithm 
without presenting the technical details. A full description of the algorithm 
is quite complex and involves geometric ideas that are beyond the scope of 
this book. A more complete description may be found in Nering and 
Tucker (Further Reading). 
To apply Karmarkar's method to a general linear programming prob- 
lem, we first convert the problem to a minimization one of the form: 
Minimize 
z -- cTx 
subject to 
Ax=b 
x>_O. 
Let x 0 be a feasible solution to this problem. For the Karmarkar 
method, x 0 will be chosen to be in the interior of the set of feasible 
solutions. That is, x 0 will be chosen to not lie on an edge or hyperplane 
forming the boundary of the set of feasible solutions. As contrasted to the 
simplex method in which most of the components of a basic feasible 
solution are 0, all the components of x 0 will be nonzero. We want to 
calculate a new feasible solution x 1 that is closer to the optimal solution of 
the problem. Let x 1 = x 0 + A x. We investigate how to choose A x to meet 
this criterion. The important aspects of the choice are the direction of A x 

Appendix A: Karmarkar's Algorithm 
399 
as a vector and its magnitude, or step size, to ensure that x~ remains within 
the set of feasible solutions. Note that the objective function at the new 
feasible solution x I is given by 
= 
C T 
C T 
cTx 1 
X0 + 
A X. 
Since this is a minimization problem, the quantity cTA x should be negative 
and as large as possible in absolute value, so that the value of the objective 
function at x 0, cTx0, is reduced as much as possible. Also 
Ax I = Ax 0 -[- AAx. 
If x 1 is to be feasible, we must have Ax 1 = b, so, by substitution, b = 
b + AAx or AAx = 0. This means that Ax must be chosen to lie in the 
null space of A (see Example 5, Section 0.4). Finally, x I must be nonnega- 
tive, which means that 
or 
0 __~ X 1 "-- X 0 -[- AX 
Ax > --x 0. 
We now develop a way to construct a vector in the null space of A from 
any given vector z. This process will allow us to choose A x so that e T A x 
has the proper value. We claim that for any vector z, the vector 
Pz = (I - AT(AAT)-~A)z = z - AT(AAT)- 1Az 
lies in the null space of A; that is, A(Pz) = 0 since 
A(Pz) = Az - AA T (AAT) - lax 
= Am - 
Az = 0. 
If we let z = -c and choose A x = --Pc then c TAx is as large negative as 
possible. 
We have now found the correct direction (-Pc). In computing the 
magnitude of the step, we want x I to lie in the interior of the set of 
feasible solutions so we must guarantee that we do not step as far as the 
boundary. This guarantee comes by choosing x I to be 
Pc 
x 0 - 0.98s 
IPcI' 
where s, the distance from x 0 to the boundary, can be computed from the 
inequality Ax >_ -x 0 and where -Pc/IPcl is a vector of length 1 in the 
direction of - Pc. 
The two features of this algorithm that we have overlooked are rescal- 
ing and a stopping condition. In practice, the general minimization prob- 
lem is transformed to a restricted form that has an obvious feasible 
solution for the initial value and whose objective function minimum is 

4{}0 
Appendix A: Karmarkar's Algorithm 
known to be zero. After each iteration of the algorithm, variables are 
rescaled to bring the current feasible solution to a standard form and to 
allow some freedom of movement near the boundary of the set of feasible 
solutions. We can stop the iterations when the objective function is within 
a preset distance from zero. We will then have to recover the solution to 
the linear programming problem as it was originally presented by undoing 
the various transformations that we performed on the given problem. 
Examples of Karmarkar's algorithm that clearly show its workings and 
its power in a simple setting are impossible to construct. The iterations of 
the algorithm require careful attention to numerical computations, some 
of which are quite complex. The following example constrains the set of 
feasible solutions to two dimensions, yet it highlights the important proper- 
ties of the algorithm. 
EXAMPLE 1. 
Consider the linear programming problem, 
Maximize 
z = 20x + 30y 
subject to 
-3x+y<l 
-x+y<3 
-x+2y<8 
y<6 
x + 2y < 18 
0.9x + 0.9y < 12 
x<9 
x-y<6 
x-2y<4 
x-3y<3 
x>0, 
y>0. 
By adding a slack variable to each constraint and converting the 
problem to a minimization problem, we can cast it in the form necessary 
for the Karmarkar algorithm. We obtain the problem, 
Minimize 
z = cTx 
subject to 
Ax=b 
x>__O, 

Appendix A: Karmarkar's Algorithm 
401 
where 
and 
A 
~. 
-3 
1 
1 
0 
0 
0 
0 
0 
0 
0 
0 
0 
-1 
1 
0 
1 
0 
0 
0 
0 
0 
0 
0 
0 
-1 
2 
0 
0 
1 
0 
0 
0 
0 
0 
0 
0 
0 
1 
0 
0 
0 
1 
0 
0 
0 
0 
0 
0 
1 
2 
0 
0 
0 
0 
1 
0 
0 
0 
0 
0 
0.9 
0.9 
0 
0 
0 
0 
0 
1 
0 
0 
0 
0 
1 
0 
0 
0 
0 
0 
0 
0 
1 
0 
0 
0 
1 
-1 
0 
0 
0 
0 
0 
0 
0 
1 
0 
0 
1 
-2 
0 
0 
0 
0 
0 
0 
0 
0 
1 
0 
1 
-3 
0 
0 
0 
0 
0 
0 
0 
0 
0 
1 
b T= [1 
3 
8 
6 
18 
12 
9 
6 
4 
3] 
c x= [-20 
-30 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0]. 
Since the problem has not been rescaled, an initial feasible solution for 
the Karmarkar algorithm looks rather unintuitive, but can be constructed 
from knowing that the point (1, 1) lies in the set of feasible solutions of the 
original maximization problem. We simply compute the values of the slack 
variables and find that 
x~=[1 
1 
3 
3 
7 
5 
15 
10.2 
8 
6 
5 
5] 
We will step in the direction -Pc from x 0. Using the definition of P given 
above and software that can perform numerical matrix manipulations, we 
find that the unit vector in the direction -Pc is 
U "- 
Pc 
IPcl 
0.2106943871859121 
0.1863525550415169 
0.4457306065162191 
0.0243418321443944 
-0.1620107228971102 
-0.1863525550415162 
-0.5833994972689248 
-0.3573422480046798 
-0.2106943871859076 
-0.0243418321443999 
0.1620107228971091 
0.3483632779386140 
We let Ax = su and solve the system of inequalities su > -x 0 for s. We 
find that s = 25.71136943075831. Thus, the new feasible solution for this 

402 
Appendix A: Karmarkar's Algorithm 
problem is 
x a=x o+0.98su= 
n 
6.308896401405732 
5.695551799297400 
14.231137404919790 
3.613344602108312 
2.917792802811221 
0.304448200702618 
0.300000000000000 
1.195996619367340 
2.691103598594382 
5.386655397891551 
9.082207197188751 
13.777758996485850 
These steps are repeated until we obtain a new feasible solution that is 
within a preset distance of the current solution. 
This example was solved using a version of Karmarkar's algorithm that 
included rescaling at each step. The interior path followed by this version 
is shown in Figure A.1. 
/x 
Optimal solution 
6 - - 
_ 940 
5- 
4- 
3- 
2- 
1- 
0 
I 
I 
, 
, 
I 
I 
I 
I 
I 
0 
1 
2 
3 
4 
5 
6 
7 
8 
9 
FIGURE A.1 
Further Reading 
Karmarkar, N. "A New Polynomial-Time Algorithm for Linear Programming." Combinator- 
ica 4 (1984), 373-395. 
Nering, Evar D., and Tucker, Albert W. Linear Programs and Related Problems. Academic 
Press, New York, 1993. 
Rockett, A. M., and Stevenson, J. C. "Karmarkar's Algorithm." Byte (September 1987), 156. 

APPENDIX 
Microcomputer 
Software 
S 
INCE THE FIRST edition of this book appeared, major develop- 
ments have taken place in the availability of software that can be 
effectively used in a linear programming course. As has already 
been noted in the text, one of the fortuitous events in the history of linear 
programming was the prodigious growth of computing power coupled with 
its precipitous cut in cost. Thus, as early as the 1960s powerful programs 
were developed to solve sizable linear programming problems on large 
computers. 
Nowadays, we are in the fortunate position of having a number of 
software packages for solving linear programming problems that are inex- 
pensive, are easy to use, and will run on a personal computer. In this 
appendix we provide some information on the following software packages: 
LINDO, MPSIII/pc, and the MATLAB optimization toolbox, programs 
that solve a problem presented to them in mathematical form; LINGO and 
PAM, programs that formulate a mathematical model of the problem and 
then proceed to solve it; and finally WHAT'S BEST!, a spreadsheet-based 
program that solves a mathematically presented program. These programs 
403 

404 
Appendix 
B: Microcomputer Software 
solve linear, integer, and mixed-integer programming problems and run on 
a variety of platforms, including PCs, Macintoshes, and various work 
stations. They are each available in a number of different versions, whose 
capabilities differ according to the platform. For example, standard LINDO 
will handle at most 200 variables, whereas extended LINDO will handle at 
most 100,000 variables. Keep in mind that to run the MATLAB optimiza- 
tion toolbox it is necessary to have MATLAB itself; similarly, to run 
WHAT'S BEST! it is necessary to have a spreadsheet that it supports, 
which includes the most widely used four or five spreadsheet programs. 
LINDO, LINGO, and WHAT'S BEST! are available from LINDO 
Systems, 1415 North Dayton Avenue, Chicago 60622; telephone, (800) 
441-2378. MATLAB and the MATLAB optimization toolbox are available 
from the The Math-Works, Inc., Cochituate Place, 24 Prime Park Way, 
Natick, Massachusetts 01760; telephone, (508) 653-2997. MPSIII/pc and 
PAM are available from Ketron Management Science, 2200 Wilson Boule- 
vard, No. 220, Arlington, Virginia 22201; telephone, (703) 558-8700. 
Further Reading 
Mor~, Jorge J., and Wright, Stephen J. Optimization Software Guide. SIAM, Philadelphia, 
1993. 

APPENDIX 
SMPX 
INTRODUCTION 
There are two main ways to use software to learn about linear program- 
ming and its extensions. One method uses a small version of a substantial 
linear programming code to find the solution to a problem and to interpret 
the solution in light of the scenario that was being modeled. This version 
should have good input and output capabilities but can hide all of the 
simplex algorithm. It is the answer that is most important. 
Alternatively, one can use a specially tailored version of the simplex 
algorithm as an experimental tool for understanding the subtleties of the 
algorithm, the sequence of steps taken as the problem is solved, and the 
consequences of the choices and decisions that are hidden in the 
problem-solving code. 
Using both types of software will contribute to understanding both the 
algorithmic process and its application to realistic problems. LINDO is an 
example of applications-oriented software; a copy can be obtained by 
mailing the order card that accompanies this book. SMPX has been 
created by Evar D. Nering to allow experimentation with the simplex 
405 

406 
Appendix C: SMPX 
algorithm: it is not recommended for use in solving applied problems. A 
disk with this program and some examples from the text is included with 
this book. A brief description of how to get started conducting experiments 
with SMPX follows. More information on the details of the software is 
available through the help system of SMPX. 
SMPX 
SMPX is a program for an IBM compatible personal computer and is 
designed for instructional purposes to support learning the principles of 
linear programming and the simplex algorithm. The program can handle 
only small tableaux, up to 10 by 10, that can be displayed on a computer 
screen. It also displays intermediate results that are useful to someone 
learning about the theory of linear programming but are unnecessary for 
using the simplex algorithm in an applied problem. 
The user interface of SMPX is similar in appearance to the user 
interface that characterizes Microsoft Windows. However, the program 
runs under DOS and the screen is a character-oriented display. Commands 
can be entered either with the keyboard and function keys or by using a 
mouse to activate buttons and menu items. The arrow keys and tab keys 
are used to navigate among the buttons and menus. A linear programming 
problem is described to SMPX by using its equivalent tableau and marking 
variables that are artificial. The software automatically provides slack 
variables, but it assumes that all constraints are written with < signs. 
Several files of examples and exercises taken from the book are in- 
cluded on the disk. These can be used as you explore the use of the 
software. You can add to these files or create other files of problems as 
you continue through the book and expand your investigations. The 
software _allows you to create new problems either by starting from a 
template with the correct number of variables and constraints or by 
modifying an existing problem. 
GETTING STARTED 
The simplest way to use SMPX is to create a directory on your hard 
drive called \LP. Then copy all the files from the disk to your hard drive. 
Start the program by giving its name as a command. Once the program is 
running you can explore the choices on the menu bar, including HELP, 
which starts the extensive context-sensitive help system. 
When you start SMPX, the first action you will usually take is to open a 
problem file. You can scan its contents in the preview window and then 
select a problem to solve in the work window. 
To stop work and exit from SMPX use the command Alt-X. 

Answers to 
Odd-Numbered 
Exercises 
CHAPTER 0 
Section 0.1, page 9 
1. a=4, 
b=2, 
3. (a) Not possible 
(c) 
3 
1 
12 
[911] 
5. All= 
10 
2 ' 
7. AC = BC = 
c=9, 
d= 
-1 
(b) Not possible 
[ 4 4 4 ]  
2 
44 
1 
11 
4 
(d) 
BA= 
-3 
-121 
8 
32 
9. 3x 1 - 2x 2 + 5x 3 + 4x 4 = 
1 
4x 1 + 2x 2 + 
x 3 
= - 3 
3x I + 4x 2 - 2x 3 + 
x 4 -" 
5 
-5 
12 
1 
16 
407 

408 
Answers to Odd-Numbered Exercises 
Section 0.2, page 20 
1 
0 
0 
11] 
1 
0 
0 
1 
0 
0 
3. 
0 
1 
0 
0 
1 
-4 
0 
0 
0 
0 
5. (a) 
(b) 
7. (a) 
(b) 
9. (a) 
(b) 
11. (a) 
(b) 
13. 
(i) 
(ii) 
(iii) 
~ 
2 
-~ 
2 
-1 
3 
0 
0 
0 
0 
0 
0 
x= 1, 
y=2, 
z= -2 
No solution 
No solution 
x=3, 
y= 
-2, 
z=0, 
w= 1 
No solution 
7 
7 
4 
8 
x= ~- 
~r, 
y= g + ~r, 
x=0, 
y--0, 
z=0 
x= -r, 
y=0, 
z=0, 
a= 
-3 
a = any real number except 3 or -3 
a=3 
z = r, where r is any real number. 
w = r, where r is any real number. 
Section 0.3, page 27 
[4 1] 
3 
2 
5 
3. (a) I 
1 
0 
0 
0 
0 
0 
0 
1 001 i 1000 
0 
1 
(b) 
0 
1 
0 
0 
1 
0 
0 
0 
2 
0 
0 
0 
0 
0 
0 
1 
5. (a) 
5 
4 3] 
-q 
1 
7. (a) [2] 
3 
1 
1 
1-'-0 
9. (a) Does not exist 
(b) 
I 
4 
-2 
13 
1 
7 
12 
1 
7 
(c) Does not exist 
(b) No inverse 
(c) 
(b) 
1 
1 
0 
-3 
7 
3 
5 
~" 
2 
4 
-1 
1 
1 
31 
--3 
9 
11 
I1 
0 
0 
0 
0 
1 
0 
0 
(c) 
0 
0 
1 
0 
0 
0 
-3 
1 
I 
14 
1 
1 1 
3 
2 
1 
0 
-~ 
5 
1 
1 
(c) No inverse 

Answers to Odd-Numbered Exercises 
409 
Section 0.4, page 32 
3. (b) and (c) 
5. (a) and (b) 
Section 0.5, page 41 
1. (a) and (b) 
3. (a) and (b) 
5. (c): 
1 
+ 
2 [1][3][5] 
2 
+ 
4 = 
7 
-1 
-2 
-1 
[i]+2[i] [i] 
7. (a) 
9. (a) 
[o] [31 [11 
11. (c): 
0 
- 
1 
-2 
2 
+3 
1 
1 
0 
1 
1 
13. (a) 
(b) 
2 
(c) 
1 
-2 
2 
19. 2 
[1] 
(d) 
2 
4 
CHAPTER 1 
Section 1.1, page 60 
1. Let x = amount of PEST (in kilograms) and 
y = amount of BUG (in kilograms). 
Minimize z = 3x + 2.5y 
subject to 
30x + 40y > 120 
40x+20y< 
80 
x>0, 
y>0. 
To change to standard form, change the objective function and the first 
constraint. 
3. Let x = number of Palium pills prescribed per day and 
y = number of Timade pills prescribed per day. 

~1 0 
Answers to Odd-Numbered Exercises 
Minimize z = 0.4x + 0.3y 
subject to 
4x + 
2y > 10 
0.5x + 0.5y < 
2 
x>_0, 
y>_0. 
To change to standard form, change the objective function and the first 
constraint. 
5. Let x = number of kilograms of Super and 
y = number of kilograms of Deluxe. 
Maximize z = 20x + 30y 
subject to 
0.5x + 0.25y < 120 
0.5x + 0.75y < 160 
x>0, 
y>0. 
This model is in standard form. 
7. Let x I = number of bags of Regular Lawn (in thousands) 
x 2 = number of bags of Super Lawn (in thousands), and 
x 3 = number of bags of Garden (in thousands). 
Maximize z = 300x I + 500x 2 + 400x 3 
subject to 
4x 1 + 4x 2 + 2x 3 < 80 
2x I + 3x 2 + 2x 3 < 50 
X 1 ~_~ 0, 
X 2 >_~ 0, 
X 3 >__ 0. 
This model is in standard form. 
9. Let x 1 = number of books in paperback binding, 
x 2 - number of books in bookclub binding, and 
x 3 - number of books in library binding. 
Maximize z = 0.5x 1 4- 0.8x 2 4- 1.2x 3 
subject to 
2x 1 + 2x 2 + 
3x 3 < 420 
4x 1 + 6x 2 4- 10x 3 < 600 
X 1 >__ 0, 
X 2 >__ 0, 
X 3 >__ 0. 
This model is in standard form. 
11. Let x~j - amount of the ith ingredient in the jth mixture (in kilograms), where 
Ingredient 1 = Sunflower seeds 
Ingredient 2 = Raisins 
Ingredient 3 = Peanuts 
Mixture 1 = Chewy 
Mixture 2 = Crunchy 
Mixture 3 = Nutty 

Answers to Odd-Numbered Exercises 
411 
3 
3 
3 
Maximize 
2 E Xil Jr 1.6 E xi2 + 1.2 E xi3 
i=1 
i=1 
i=1 
3 
3 
3 
- 
~ x~j - 1.5 ]~ x2j - 0.8 ~ x3j 
j=l 
j=l 
j=l 
Xij >__ O, 
subject to 
3 
Xlj ~__ 100 
j=l 
3 
E XEj <-- 80 
j=l 
3 
EX3j <__ 60 
j=l 
0.6Xll -- 0.4X21 + 0.6X31 _< 0 
--0.2Xll -- 0.2X21 + 0.8X31 _~< 0 
--0.4X12 + 0.6X22 + 0.6X32 _< 0 
0.8X13 -- 0.2X23 -- 0.2X33 __< 0 
0.6X13 + 0.6X23 -- 0.4X33 ~ 0 
i= 1,2,3; 
j= 
1,2,3 
Section 1.2, page 68 
1. Maximize z = [ 20 
subject to 
30,[yl 
0.4 
0.2 
1[1 [181 
0.3 
x 
< 
0.4 
Y 
- 
14 
[;1 o] 
3. Maximize z = 3x + 2y + 3v - 2w 
subject to 
2 
6 
2 
-4 
r..7 
7 
- 2 - 6 - 2  
4 
,.x.| 
-7 
3 
2 
5 
1 
[~] < 
8 
3 
2 
5 
1 
8 
6 
7 
2 
5 
4 
ixl Ii 
y 
> 
o 
u 
-- 
0 
w 
0 

412 
Answers to Odd-Numbered Exercises 
5. Maximize z = [-3 
-2 
0 
O] 
subject to 
2 
1 
1 
3 
-2 
0 
Iil 
O] 
= 
4 
1 
6 
7. Maximizez=[3 
2 
3 
-2 
O] 
subject to 
Ixl uy I~ o 
2 
3 
6 
x 
Y 
U 
w 
u 
6 
2 
-4 
2 
-5 
1 
7 
2 
5 
x 
o 
Y 
~ 
>__o 
w 
u 
0 
[~1 
LvJ 
subject to 
x o]y 
0 
v 
-- 
1 
w 
u 
iil { 1 
[o~ o~ 
1 o] 
_~o 
0.5 
0.75 
0 
1 
160 
I lx Ioil 
22 
]- [~3]-~ [15]; [2 
~a,[~ 
~][~ 
6 
~ 
]~_[o ] 
340 
1 
1 
0 j; 
z 
2 
2 
1 
8 
8 
1 
0 

Answers to Odd-Numbered Exercises 
413 
2 
2 
8 
8 
~, [~ ~1[~1-1 [~1 ~- [1~] 
2 
2 
2 
8 
8 
-2 
O] 
13.(a) x=2, 
y-3, 
u=3, 
v-4 
(b) Impossible 
Section 1.3, page 81 

~'14 
Answers to Odd-Numbered Exercises 

Answers to Odd-Numbered Exercises 
~'15 

~'1 ~ 
Answers to Odd-Numbered Exercises 

Answers to Odd-Numbered Exercises 
4"1 "~ 

~'1 ~ 
Answers to Odd-Numbered Exercises 

Answers to Odd-Numbered Exercises 
419 
Section 1.5, page 99 
1. 
(i) (a), (c), (e) 
(ii) (a), (c) 
(iii) (a), (b), (c) 
(iv) (a) basic variables are x 2, x4, x 5 
(c) basic variables are x 1, x 4, and one of x 2, x 3, x 5 
3. Let x 1 - number of glazed doughnuts per day and 
x 2 = number of powdered doughnuts per day. 
Maximize z - O.07x 1 + O.05x 2 
subject to 
X 1 "~" X 2 ___< 1400 
X 1 
__< 1000 
x 2 < 1200 
X 1 
~__ 
600 
x I > 0, 
x 2 > 0 
Extreme points: (600, 0), (1000, 0), (1000, 400) (600, 800) 
Optimal solution: (1000, 400); z = $90 
5. Let 
x 1 = number of glazed doughnuts per day 
x 2 = number of powdered doughtnuts per day, and 
x 3, x 4, x 5, x 6 = slack variables. 
Maximize z = 0.07x I + 0.05x 2 
subject to 
X 1 + X 2 + X 3 
= 1400 
X 1 
"+" X 4 
-- 1000 
+ X 2 
+ X 5 
= 1200 
X 1 
--X6"- 
600 
At the optimal solution, 
x 3 = 0 = additional number of doughnuts per day that could be baked; 
x 4 - 0 = additional number of doughnuts per day that could be glazed; 
x 5 - 800 = additional number of doughnuts per day that could be dipped; and 
x 6 = 400 = number of glazed doughnuts over the required number. 
The basic variables are Xl, x 2, xs, and x 6. 
7. (a) Basic if x2 and x4 are taken as nonbasic variables; basic if x I and x 2 are 
taken as nonbasic variables; and not basic if x l and x4 are taken as 
nonbasic variables 
(b) Not basic 
(c) Not basic 

420 
Answers to Odd-Numbered Exercises 
9. (a) Maximize z = 4x 1 + 2x 2 + 7x 3 
subject to 
2x 1 -- 
X 2 + 4X 3 + X 4 
"-- 18 
4x 1 + 2x 2 + 
5x 3 
+-x 5 -- 10 
xj>0, 
j= 
1,2 ..... 5 
(b) 
X 1 
X 2 
X 3 
X 4 
X 5 
Basic variables 
Optimal 
0 
0 
0 
18 
10 
x4, x 5 
No 
0 
0 
2 
10 
0 
x 3, x 4 
Yes 
0 
5 
0 
23 
0 
x 2, x 4 
No 
5 
0 
0 
13 
0 
x 1, x 4 
No 
CHAPTER 2 
Section 2.1, page 119 
1. 
x 
y 
u 
v 
u 
3 
5 
1 
0 
8 
v 
2 
7 
0 
1 
12 
2 
5 
0 
0 
0 
3. (a) x 2 
(b) x I 
(c) No finite optimal solution 
5. Using x 2 as the entering variable, 
X1 
X2 
X 3 
X4 
1 
1 
0 
1 
X 2 
~ 
~- 
1 
5 
X 3 
~ 
0 
1 
4 
2 
0 
0 
-2 
Using x 4 as the entering variable, 
Xl 
X 2 
X 3 
X4 
X 4 
1 
2 
0 
1 
3 
X 3 
1 
5_ 
1 
0 
9 
2 
2 
4 
4 
0 
0 
35 
2 

Answers to Odd-Numbered Exercises 
421 
X 1 
X2 
X3 
X4 
x 1 
1 
1 
5 
0 
4 
x 4 
0 
1 
7 
1 
10 
0 
3 
13 
0 
19 
9. (a) xl=20, 
x2--0, 
x3=0, 
u=6, 
v= 12, 
w=0 
Basic variables: Xl, u, v 
(b) 
X 1 
X 2 
X 3 
U 
V 
W 
x I 
1 
0 
8 
z5 
0 
13 
1 
0 
2 
x 2 
0 
1 
2 
5- 
v 
0 
0 
-5 
1 
1 
7 
5 
0 
0 
7 
7 
0 
7 
27 
11. 
13. 
15. 
17. 
19. 
21. 
(c) x1=5, 
x 2=3, 
x 3=0, 
u=0, 
v=6, 
w=0 
Basic variables: x 1, x 2, v 
[0]. 
O' 
z=O 
38 
y + 
12 
-- 
136 
x= 5g, 
= gs, 
y =0; 
z- 
23 
Make 0 kg Super blend and ~ 
kg Deluxe blend. 
Profit = $64.00 
Make 0 books in either paperback or library bindings and 100 books in 
book-club binding. 
Profit = $80.00 
No finite optimal solution 
[3 
0 
0 
0]T; 
Z= 15 
Section 2.2, page 130 
1. [ 0 
15 
75 
T]T; 
z = y 
The simplex algorithm examines the following extreme points: O, O, A, B. 

~ 
Answers to Odd-Numbered Exercises 

Answers to Odd-Numbered Exercises 
423 
Section 2.3, page 150 
1. (a) 
Yl 
Y2 
X1 
X2 
X3 
Yl 
Y2 
1 
2 
7 
1 
0 
4 
1 
3 
1 
0 
1 
5 
2 
5 
8 
0 
0 
-9 
(b) 
Yl 
Y2 
X1 
2M 
X2 
X3 
Yl 
Y2 
2 
7 
1 
0 
3 
1 
0 
1 
-5M 
3 
8M 
0 
0 
9M 
3. (a) 
Xl 
X2 
X3 
X4 
X5 
Yl 
Y2 
Yl 
1 
3 
2 
1 
0 
1 
0 
7 
Y2 
2 
1 
1 
0 
1 
0 
1 
4 
3 
4 
3 
1 
1 
0 
0 
11 
(b) 
Yl 
Y2 
X1 
X2 
X3 
X4 
X5 
Yl 
Y2 
3 
2 
1 
0 
1 
0 
1 
1 
0 
1 
0 
1 
3M 
2 
4M 
3M 
M 
M 
0 
0 
11M 
Xl 
X2 
X3 
X4 
X5 
YZ 
Y2 
1 
1 
3 
1 
3 
1 
X 2 
~ 
1 
0 
a 
X 
X 
4 
X3 
3 
0 
1 
1 
1 
1 
3 
3 
0 
0 
0 
0 
0 
1 
1 
0 

424 
Answers to Odd-Numbered Exercises 
7. (a) 
x 1 
x2 
x 3 
x4 
x 5 
3 
0 
1 
3 
X 2 
1 
1 
~o 
~ 
1 
0 
1 
1 
10 
7 
x4 
~ 
~ 
7 
1 
3 
3 
0 
~ 
0 
~ 
11. 
13. 
15. 
17. 
19. 
21. 
23. 
(b) No finite optimal solution 
An artificial variable Y l has a nonzero value at the end of Phase 1. There are 
no feasible solutions. 
Invest $70,000 in bond AAA and $30,000 in stock BB. Return = $7600 
No feasible solutions 
Make no PEST and 3 kg BUG. Profit = $7.50 
No feasible solutions 
Invest $40,000 in the utilities stock, $0 in the electronic stock, and $160,000 in 
the bond. Return = $11,600 
No finite optimal solution 
[0 
2 
0 
4 
0 
1]T; 
Z= 12 
CHAPTER 3 
Section 3.1, page 165 
1. Maximize z'= 
8w I + 12w 2 + 6w 3 
subject to 
w I + 2w 2 + 2w 3 < 3 
4w 1 + 3w 2 + 
w 3 < 4 
W 1 >~ O, W 2 >_~ O, W 3 >__ 0 
5. Minimize z' = 18w 1 + 12w2 
subject to 
3. Minimize z'= 
8W 1 -'[- 7w 2 + 12w 3 
subject to 
3w I + 5w 2 --b 4w 3 >_ 3 
2w 1 + 
w 2 
> 2 
w I + 2w 2 + 
w 3 >_ 5 
4w 2 -- 2w 3 >_ 7 
w I >_ O, w 3 >_ O, w 2 unrestricted 
3w I + 2w 2 >_ 3 
3w I + 2w 2 = 1 
w 1 + 4w 2 >_ 4 
w1 ~> O, 
w2 unrestricted 

Answers 
to Odd-Numbered 
Exercises 
425 
7. (a) Primal problem: 
Maximize z'= 
-2x 1 + 3x 2 -x 4 
subject to 
x I + 2x 2 + X 3 
X 1 + 4x 2 
x 2 
-- x 3 
x 1 >_~ O, 
x 2 >__ O, 
Dual problem: 
Minimize z" 
subject to 
= 7w 1 + 5w 2 - 3w 3 
< 
7 
-- 
X 4 -- 
5 
-- 5X4 __< --3 
X 3 >__ 0, 
X 4 >__ 0 
w I + 
w 2 
> - 2 
2w I + 4w 2 - 
w 3 > 
3 
w I 
- 
w 3 > 
0 
- 
w 2 - 
5w 3 > - 1 
Wl > 0, 
w3 > 0, 
w2 unrestricted 
9. Minimize z'= 
12Wl + 48w 2 + 360w 3 
subject to 
w I + 6w 2 + 36w 3 > 40 
w 1 + 6w 2 + 24w 3 > 30 
w 1 + 2w 2 + 18w 3 >__ 20 
w 1 > 0, 
w 2> 0, 
w 3 >0, 
where wl, w2, and w 3 denote fictitious prices representing the contributions to 
profit of one unit of land, capital, and labor, respectively. 
11. Minimize z' = bTw 
subject to 
Afw>__c 
BTw = d 
w>O 
Section 3.2, page 182 
1. The dual problem has an optimal feasible solution with objective function 
value 117.81. Moreover, the slack variables for the first four constraints of the 
dual problem must be zero at the optimal solution. 
3. The dual problem has either no feasible solutions or feasible solutions with 
unbounded objective function values. 
5. z=cTx= 
139 

426 
Answers to Odd-Numbered Exercises 
1 
7. [1 
~ 
0]T; 
Z-- 11 
Dual problem: 
Minimize z' = 2w 1 + 3w2 + 4w3 
subject to 
w I + 2w 2 + 3w 3 >__ 8 
w 1 + 3w 2 + 3w 3 >__ 9 
2w I + 4w 2 + 
W 3 >_~ 5 
W 1 > O, 
W 2 > O, 
W 3 ~__ 0 
Solution: 
[ 0 
1 
2 ]T; 
11. [0 
1 
1]; 
z' =20 
z' = 11 
Section 3.3, page 202 
CB 
2 
X 2 
5 
X 6 
0 
X 5 
1 
2 
6 
0 
0 
5 
X 1 
X2 
X3 
X4 
X5 
X6 
XB 
3 
1 
1 
2 
0 
0 
2 
2 
0 
0 
1 
0 
1 
3 
6 
0 
7 
6 
1 
0 
1 
5 
0 
4 
9 
0 
0 
19 
CB 
2 
X 1 
1 
x 4 
0 
x 6 
0 
Yl 
0 
Y2 
2 
3 
5 
1 
0 
0 
0 
0 
0 
Xl 
X2 
X3 
X4 
X5 
X6 
X7 
Yl 
Y2 
1 
1 
0 
1 
7 
1 
0 
1 
~. 
1 
0 
2 
2 
0 
3 
3 
0 
~- 
2 
0 
0 
4 
1 
0 
0 
1 
7 
0 
2 
XB 
1 
3 
7 
0 
0 
4 
0 
7. Any point on the line segment joining [5 
z=8 
9. No feasible solution 
11. [0 
2 
0 
4]T; 
Z -- 6 
I 
T 
0 
; 
0 
3 
z=5 
0] T and [0 
0 ]T; 

Answers to Odd-Numbered Exercises 
427 
13. Exercise 6 
1 
0 
0] 
First: 
B= 
0 
1 
0 , 
0 
0 
1 
3 
0 
0] 
Second: 
B= 
5 
1 
0 , 
1 
0 
1 1 0] 
Third: 
B= 
5 
3 
0 , 
1 
0 
1 
Final: 
B= 
5 
3 
1 , 
1 
0 
2 
Exercise 9 
1 
0 
O] 
B -1- 
0 
1 
0 
0 
0 
1 
i1 1 
0 
0 
5 
B -1 
--3 
1 
0 
1 
0 
1 
~ 
14 
B_I 
3 
0 
-- 
1--4" 
3 
z 
1 
14 
14 
2 
2 
1 1 
ff 
2-T 
B_ 1 
3 
4 
1 
-- 
7 
2--] 
1 
1 
2 
7 
21 
First: 
B = 
Second: 
B = 
Third: 
B = 
Final: 
B = 
I 
1 
1 
0 
0 
0 
0 
1 
0 
0 
0 
0 
1 
0 ' 
0 
0 
0 
1 
I 
1 
3 
0 
0 
0 
4 
0 
0 
0 
2 
1 
0 
0 
3 
0 
1 
I1 i 
3 
1 
01 
4 
0 
0 
2 
3 
0 ' 
3 
1 
1 
I 
1 
1 
1 
0 1 
0 
2 
0 
0 
0 
-1 
3 
0 ' 
0 
4 
1 
1 
I 
1 
0 
0 
01 
B_l= 
0 
1 
0 
0 
0 
0 
1 
0 
0 
0 
0 
1 
3 
0 
0 
1 
4 
1 
B_l= 
0 
a 
0 
0 
t 
1 
0 
0 
2 
3 
0 
1 
0 
4 
1 
7 
1 
0 
12 
3 
1 
0 
0 
B_I= 
0 
~- 
1 
1 
0 
6 
~ 
0 
0 
7 
1 
1 
12 
3 
1 
2 
1 
0 
3 
3 
1 
B_I= 
0 
~ 
0 
0 
1 
1 
0 
~ 
~ 
0 
0 
13 
1 
1 
6 
3 
15. Invest $70,000 in bond AAA and $30,000 in stock BB. Return = $7600 
17. Use only the large backhoe for 62 h. Cost = $266.67 
Section 3.4, page 214 
1. [4 
~ 
2 
0 
o]'r; 
z=40 
5 
5 
37 
3. [0 
~ 
2 
2 
~ 
0 
0]; 
z= 
5. No feasible solutions 

428 
Answers to Odd-Numbered Exercises 
Section 3.5, page 223 
Z 
1 
0 
0 
-~ 
2 
1. 
0 
1 
0 
0 
0 
1 
0 
1 
0 
0 
0 
[1 
0 
5 
1 
0 
3. 
2 
-2 
2 
-3 
5 
1 
28 
5.[~ 
~ 
~]T; z=~- 
7. Optimal solution is unbounded. 
9. [0 
0 
10 
0 
0 
0 
18 
0]T; 
z = 66 
Section 3.6, page 233 
7 
1. (a) 
- ~  
< 
AC 1 _~< ~ 
(b) 
7 < Ac 2 < oo 
-1 _< Ac 3 _< 5 
14 
--oo <Ac 
4 __< -V 
3. (a) 
- 2 < 
Ac 1 < 
oo 
(b) 
-oo < Ac2 _< 4 
-1 < Ac 3 < 
1 
--oo < AC4 _<< ~- 
-oo < Ac 5 _< 4 
-1 < Ac 6 < 
-4 _< Ab I < 17 
-12 _< Ab 2 _< 12 
3-< Ab3 < oo 
-4 _< Ab 1 < 12 
-9 _< Ab 2 < oo 
-12 _< Ab 3 _< 12 
5. (a) Plant 15 
3 
acres of corn, ~ acres of soybeans, and 3 acres of oats. 
Profit = $405 
(b) Plant seven acres of corn, three acres of oats, no soybeans. 
Profit = $340 
(c) Plant six acres of corn, six acres of oats, no soybeans. 
Profit-- $366 
(d) At least $10/acre 
CHAPTER 4 
Section 4.1, page 259 
1. Let 
x I -- number of type A and 
x2 = number of type B 
Minimize z = 22,000x 1 + 48,000x 2 
subject to 
100x I + 200x 2 >__ 600 
50x I + 140x 2 < 350 
x 1>0, 
x 2>0,integers. 

Answers to Odd-Numbered Exercises 
429 
1 
3. Let 
xi-- 
0 
{1 
a q = 
0 
otherwise 
10 
Minimize Z -- E 
CiXi 
i=l 
subject to 
if the i th CD is purchased 
otherwise 
if the j th song is on the i th CD 
10 
E 
aijxi ~ 1 
i=l 
j = 1,2,...,6 
5. Let 
w i = person-weeks necessary for project i 
c i -~ cost of project i (in thousands of dollars) 
v i = value of completion of project i 
1 
if project i is to be completed 
xi = 
0 
otherwise 
10 
M aximize Z = 
E 
ViXi 
I 
i=l 
subject to 
10 
E 
WiXi <-- 1000 
i=1 
10 
E 
CiXi <-- 1500. 
i=1 
Section 4.2, page 274 
1 
1 
7 
9 -- ~X3 "k- Ul -" 
8 
3. x=2, 
y=2; 
z=4 
5. x=l, 
y=0; 
z=4 
7. x=8, 
y=2; 
z=44 
13 
9. x=l, 
y=~; 
z= 5 
4 
28 
11. [0 
4 
g 
0]T; ~ Z-- 5- 

430 
Answers to Odd-Numbered Exercises 
Section 4.3, page 289 
1. 
Q 
9 
Q 
z = 4]- 1 
x=l 1 
y-3 
z-4~ 
Q 
z-4 
z=4 
x=l 
y=3 
x=l 
y=31 
z=5 
x=l 
y=4 
Q 
Optimal 
solution 
Optimal 
solution 
x=2 
y=2 
Optimal 
solution 
2 
z = 6g 
2 
x = 1-~ 
y=O 
~ 
[ ~ot~asi~,e ] 
z = 4 i 
x=l 
3 
Y=7 
z:4 
[ ~o, fea~i~'e 1 
x=l 
y=O 
Optimal 
solution 

Answers to Odd-Numbered Exercises 
431 
o 
z -- 46~110 
x •81 
4 
y=l~- 
2 
z = 34~- 2 
x =689 
y=l 
z-~ 
x=81 
y=2 
z-44~ 
I N~ 
] 
x=8 
y=21 
z 
= 3889 
x =61 
y=3 
z=44 
x=8 
y=2 
Optimal 
solution 
z=4 / 
I 
x = 15- 
v=3 
z=4~. 
z =4 
x=! 
x=2 
Y =3/ 
y=2 
Optimal 
solution 

432 
Answers to Odd-Numbered Exercises 
z= 91 
X 1 "-" X 4 -" 0 
x 2 = 4 
1 
X 3 = 
l~ 
Optimal 
solution 
11. Buy six machines of type A and no machines of type B. 
Cost = $132,000 
13. Buy two bottles each of cola and ginger ale, five bottles of root beer, and three 
bottles of grape. Cost = $7.28 
CHAPTER 5 
Section 5.1, page 324 
3. (a) 
50 
50 
70 
60 
90 
40 
30 
30 
(b) 
100 
40 
1 O0 
40 
10 
10 
90 
30 
(c) 
100 
40 
100 
40 
10 
10 
90 
30 

Answers to Odd-Numbered Exercises 
4~ 
~ 
20 
30 
60 
80 
30 
70 
z = $1730 
70 
100 
40 
100 
40 
10 
10 
90 
z = $950 
30 
. 
10 
10 
50 
70 
50 
20 
80 
or 
60 
10 
20 
50 
50 
20 
80 
z = $940 
11. 
z= 12 
13. 
20 
40 
20 
20 
60 
50 
z = $800 
40 
or 
20 
Dummy supply 
40 
60 
20 
40 
30 
40 
Dummy supply 

434 
Answers to Odd-Numbered Exercises 
Section 5.2, page 338 
I 
O 1 0 
01 
1 0 
0 
0 
0 
0 
0 
1 ; 
0 
0 
1 
0 
z=9 
0 
0 
1 
0 
0 
0 
0 
0 
0 
0 
0 
1 
0 
0 
0 
0 
1 
0 
1 
0 
0 
0 
0 
0 ; 
0 
1 
0 
0 
0 
0 
0 
0 
0 
1 
0 
0 
z= 12 
, 
0 
0 
0 
0 
0 
1 
1 
0 
0 
0 
0 
0 
0 
0 
1 
0 . 
i 
0 
0 
0 
0 
0 ' 
0 
0 
1 
0 
0 
0 
1 
0 
0 
0 
z=14 
7. One of the possible answers is 
702 --. 706 
705 ~ 707 
708 ~ 714 
709 ~ 712 
713 --. 715 
Section 5.3, page 344 
1. (a) 
0 
1 
0 
1 
0 
0 
0 
1 
1 
0 
1 
1 
0 
0 
1 
0 
1 
0 
0 
1 
0 
0 
1 
1 
0 
1 
1 0 
0 
1 0 
(b) 
(i) (1, 4) 
(ii) (1, 2), 
(2,4) 
(iii) (1,2), 
(2,6), 
(6,5), 
(5,3), 
(3,2), 
(2,4) 
(c) 
(i) (2,3), 
(3, 2) 
(ii) (2,1), 
(1, 4), 
(4, 2) 

3. (a) 
(b) 
d 
Answers to Odd-Numbered Exercises 
435 
0 
10 
0 
20 
0 
0 
0 
0 
0 
0 
5. (a) 
0 
9 
0 
0 
0 
0 
12 
0 
0 
5 
0 
0 
17 
0 
0 
(b) 1 
(c) 2 
Section 5.4, page 363 
(3, 1) 
(3, 2) 
,,~~~ ,i'::; -Q 
(7, 1) 
(3,3) 
4) 

4~ 
Answers to Odd-Numbered Exercises 
7// 
"" 
5 
Maximum flow = 12 
3 
/ 
8 
v 
Maximum flow -- 16 
7. (a) 
Boys 
Girls 
l 
1 
9 
1 
Source 
Sink 
Connect B i to Gj with an arc of capacity 1 if boy i and girl j are friends. 

Answers to Odd-Numbered Exercises 
437 
(b) 
(c) 
1 
(~ 
0 
0 
0 
0 
0 
0 
1 
(~ 
(~) 
0 
1 
0 
0 
0 
0 
(~ 
0 
1 
0 
1 
0 
(~) 
0 
0 
1 
0 
C) 
0 
(~) 
0 
0 
1 
1 
0 
0 
(~) 
0 
1 
0 
0 
0 
1 
(~) 
0 
(~) 
0 
0 
0 
(d) n lines are necessary to cover the Is. 
Section 5.5, page 375 
1. Path: 
1 ~5~7; 
Length=8 
3. Path: 
1 ~2~4~9; 
Length-- 14 
5. Replace the equipment at the beginning of the fourth year. 
Total equipment cost -- $380,000 
Section 5.6, page 385 
1. (a) and (b) 
Node 
Early event times 
Late event times 
1 
0 
0 
2 
3 
8 
3 
6 
6 
4 
8 
8 
5 
5 
10 
6 
9 
14 
7 
16 
20 
8 
9 
9 
9 
7 
12 
10 
18 
18 
11 
17 
22 
12 
25 
25 
(c) 1 ~4~8~ 
10~ 12 
or 
1~3~8~ 
10~ 12 

~ 
Answers to Odd-Numbered Exercises 
3. (a) and (b) 
Node 
Early event times 
Late event times 
1 
0 
0 
2 
3 
3 
3 
8 
8 
4 
7 
14 
5 
7 
21 
6 
22 
22 
7 
9 
23 
8 
14 
14 
9 
22 
22 
10 
15 
29 
11 
15 
29 
12 
31 
31 
(c) 1 ---> 2 ---> 3 ---> 8 ~ 6---, 9 ~ 
12 
5. Insert dummy activity between node 2 and node 5. 
7. 
s/ 
f,~s 
7 
.,'7"x, 
_ f 
3 
2 
D 
D 
Critical path: 
1 ~2~6~5--->8~9 

Answers to Odd-Numbered Exercises 
439 
Section 5.7, page 393 
3. 
| iOl,, ~ 
networkPr~ 
~ 
[0,1,1] _@ 
All remaining arcs should be labeled [c, 1, 0], where c is the length of the arc. 

Index 
Accounting price, 163 
Activity 
critical path method, 379-381 
dummy, 380 
Adjacent extreme points, 105, 109 
Algorithm, see also Method; Procedure 
cutting plane, 262-274, 276-277 
Karmarkar's, 397-402 
labeling, 350 
out-of-kilter, 388-391 
polynomial time, 397 
transportation, 308-309 
Arc, 340 
capacity, 343 
directed, 341 
oriented, 341 
Artificial variable, 131 
big M method, 147-150 
two-phase method, 135-147, 189-192 
Assigned column, 329 
Assigned row, 329 
Assignment problem, 251-252, 260, 326 
cost matrix, 328-338 
Hungarian method, 337-338 
incomplete assignments, 330-332 
scheduling problem as, 339, 379 
Augmented matrix, 5 
Augmenting path method, 350 
Basic solution, 95 
feasible, 96-98, 105-108 
feasible and degenerate, 123-126 
optimal, 97, 104 
Basic variable, 96 
departing, 109-111 
entering, 108-109 
Basis, 38, 40, 92 
reinverting, 243 
Big M method, 147 
Bland's Rule, for avoidance of cycling, 
128-129 
Boundary, of closed half-space, 73 
Boundedness 
convex set, 81, 267-268 
441 

442 
I.de~ 
Boundedness (continued) 
objective function, 172-173, 176 
solution set, 87, 89, 126, 172, 176-177 
Branch, 340 
dangling, 280 
Branch and bound method 
computer software, 280, 290-292 
Dakin's method, 280-289 
implicit enumeration, 277-279 
search method, 280 
solution tree, 278-280 
tableau tree, 288, 289 
Canonical form, 52-53 
duality, 157-159 
standard form, conversion from, 56, 65-68, 
97-98 
Capacity 
arc, 342 
cut, 356 
excess, 351 
Capacity matrix, 343 
Cell, tableau, 309 
Checking row sums, 243 
Circularization network, 391 
Closed half-space, 70 
hyperplane boundary, 73 
Codes, computer, see Computer software 
Coefficient matrix, 5 
Complementary slackness, 178-181 
Computer software packages 
CPLEX, 236 
Functional Mathematical Programming 
System, 236 
GAMS, 237 
LINDO, 237, 403, 404 
LINGO, 237, 403, 404 
Mathematical Programming System Ex- 
tended, 236 
Mathematical Programming System III, 
236 
MATLAB, 403, 404 
MIPIII, 291 
MPSIII/pc, 237, 403, 404 
Optimization Subroutine Library, 236 
PAM, 403, 404 
TRANCOL, 240 
UKILT 1100 Out-of-Kilter System, 391 
WHAT'S BEST!, 237, 403, 404 
Connected graph, 341 
Constraint, xx, 51 
additional, 204-205, 211-213, 263 
cutting plane, 263-264, 269-271 
dual, 178, 308-309, 312 
generalized upper bounding, 241-242 
geometry of, 70-75, 85 
nonnegative, 106, 114-118, 131-135 
scaling, 56 
Convex combination, 85, 86 
Convex function, 84 
Convex polyhedron, 85 
Convex set, 79-80 
bounded, 81, 267-268 
extreme point, 85 
Coordinate vector, 39-40 
Cost, see also Duality 
dual variable, 163 
equipment replacement, 373-375, 378 
minimum rule, 308 
penalty, 147 
Cost flow problem, minimum, 388-391 
Cost matrix, 328-338 
CPLEX software, 236 
CPM, see Critical path method 
Critical path method, 379 
activity diagram, 380-381 
computer code, 391-393 
critical path, 385 
dual specification, 380 
dummy activity, 380 
event, 380 
loops, avoidance of, 381 
time 
estimate, 381 
event, 382-383 
float, 385 
Cut, in a network 
capacity of, 356 
Max Flow-Min Cut Theorem, 358 
Cutting plane algorithm 
basic variable selection, 264, 276-277 
constraint derivation, 263-264, 269-271 
mixed integer programming, 268-274 
pure integer programming, 262-268 
Cycle, in a graph, 341 
Cycling, 122 
Bland's Rule, 128-129 
degeneracy, 126 
Dakin's method, 280-289 
Dangling branch, 280 
Dangling node, 281, 283, 287 
Decision variable, xx 

Index 
443 
Degenerate solution, 123 
cycling, 126 
Departing variable 
choice of, 312, 397 
simplex method tableaux, 109-111 
transportation algorithm, 309, 311-312 
Deterministic model, xx 
Diet problem, 46-47, 164-165 
Digraph, 341 
Dimension, of a subspace, 38 
Directed arc, 341 
Directed graph, 341 
Duality, see also Dual problem; Primal prob- 
lem 
constraints, 178, 308-309, 312 
economic interpretation, 156, 161-165, 181 
general problem, dual of, 197-201 
objective function, 172-173 
optimal solution, 174-182 
primal-dual solution pairs, 172-182 
profit, 163 
slack variable, 178-181 
Duality Theorem, 166, 174-176 
Weak, 172-173 
Dual problem, 156 
infeasible solution, 205-207 
optimal solution, 163, 193-201 
primal-dual solution pairs, 172-182 
primal problem, compared with, 160 
primal solution, 205-207 
transportation problem, 304-308 
Dual simplex method, 204-207 
procedure, 208-209 
reduction of infeasibility, 208-211 
restoration of feasibility, 205, 211-214 
Dual specification, in critical path method, 
380 
Dummy activity, in critical path method, 380 
Dummy destination, in transportation prob- 
lem, 324 
Early event time, 382 
Economics, see also Cost 
duality and, 156, 161-165, 181 
Edge, of a graph, 340 
Element, of a matrix, 2 
Elementary matrix, 23-24 
Elementary row operation, 12 
elementary matrix, 23-24 
inverse of a matrix, 25-26 
End point, 78 
Entering variable 
choice of, 397 
simplex method tableaux, 108-109 
Entry, 2 
leading, 11 
Equality 
converting to an inequality, 54-55 
converting an inequality to, 65-68 
Equipment replacement problem, 368, 
373-375, 378 
Error correction, 239, 243, 245 
Eta matrix, 218 
Eta vector, 218 
Event, in critical path method, 380, 385 
early event time, 382 
late event time, 383 
Excess capacity, 350 
Extreme point, 85 
adjacent, 105, 109 
Extreme Point Theorem, 85, 87, 97 
Feasibility criterion, in dual simplex method, 
208 
Feasible solution, see also Infeasible solu- 
tion, 64 
basic, 96-98, 105-108, 123-126 
bounded, 87, 89, 126, 172, 176 
degenerate, 123-126 
duality, 174-177, 205-207 
extreme point as, 86-87, 92-98 
geometry, 74-81, 85-90 
implicit enumeration, 277 
initial basic, 106-108, 135-150 
line segment joining any two, 79 
transportation problem, 298 
Fictitious flow, 349 
Fictitious price, 163, 165 
Float time, in critical path method, 385 
Flow 
fictitious, 349 
Max Flow-Min Cut Theorem, 358 
maximal, 345-363 
in a network, 343 
Flowchart, see also Procedure; Structure dia- 
gram 
dual simplex method, 209 
simplex method, 115 
two-phase method, 146 
Ford-Fulkerson method, 350 
Fractional part, 264 

444 
tndex 
Functional Mathematical Programming Sys- 
tem software, 236 
GAMS computer modeling language, 237 
Gauss-Jordan reduction, 11, 17-20 
Generalized upper bounding, 241-242 
General linear programming problem, 51 
dual of, 197-201 
Geometry 
of a constraint, 70-75, 85 
of a feasible solution, 74-81, 85-90 
of a linear programming problem, 70-81 
of an objective function, 75-78 
Gomory cutting plane method, see Cutting 
plane algorithm 
Graph, see also Network; Node; Path 
activity diagram, 380-381 
connected, 341 
directed, 341 
incidence matrix of, 340-341 
as a network, 342 
oriented, 341 
GUB, see Generalized upper bounding 
Half-space, see Closed half-space 
Homogeneous linear equations, 19-20 
Hungarian method, 337-338 
Hyperplane 
closed half-space intersection, 73 
constraints as, 73 
objective function as, 75-77 
Identity matrix, 4 
Implicitly enumerated set, 277-279 
Imputed value, 163, 176 
Incidence matrix, 340-341 
Index of summation, 4 
Inequality 
converting to an equality, 65-68 
converting an equality to, 54-55 
reversing of, 54 
Infeasible solution, see also Feasible solution 
restored to feasibility, by dual simplex 
method, 205, 211-214 
Initial basic feasible solution, 106-108, 
135-150 
Larson's method, 322-323, 324 
minimum cost rule, 308, 319 
Vogel's method, 319-323 
Initial basic variable, 194 
Integer programming 
computer software, 290-292 
linear programming problem, related 
branch and bound method, 277, 279-287 
cutting plane algorithm, 265, 267, 
268-269 
mixed, see Mixed integer programming 
model construction, 291-292 
network problem as, 292 
pure, see Pure integer programming 
zero-one, 251, 277-279 
Integer programming problems 
air filter manufacture, 290 
assignment, 251-252, 260 
either-or, 257 
equipment purchasing, 259 
fixed charge, 256-257 
future worth, 261-262 
knapsack, 250-251 
making change, 262 
mix of purchases, 259-260 
production, 259 
scheduling, 260 
stock cutting, 255-256 
transportation, 249-250 
traveling salesman, 252-254 
Integer programming solution 
by branch and bound method, 280 
by cutting plane algorithm, 262-274, 
276-277 
by Dakin's method, 280-289 
by dual simplex method, 265, 267, 274, 283 
by simplex method, 263 
Interior path method, 398-402 
Interior point, 78 
Intermediate node, 388 
Inverse, of a matrix, 22-27 
Invertible matrix, 22 
Karmarkar's algorithm, 397-398, 401-402 
rescaling, 399-400 
stopping condition, 399-400 
Knapsack problem, 250 
Labeling algorithm, 350-363 
Land-Doig Method, 280 
Larson's method, 322-323, 324 
Late event time, 383 
Lattice point, 267 

I,,de~ 
445 
LINDO software, 237, 403, 404 
Linear combination 
of tableau columns, 168 
of vectors, 33-34, 37 
Linear dependence, 35-37 
Linear independence, 35-40 
Linear programming 
canonical form, see Canonical form 
dual of canonical form, 157-159 
dual of general problem statement, 
197-201 
dual of noncanonical form, 159-161 
dual of standard form, 161-165 
fundamental theorem, 167; see also Dual- 
ity Theorem 
general problem statement, 51 
integer programming problem, related, 258 
standard form, see Standard form 
Linear programming problems 
activity analysis, 46 
advertising budget, 61-62 
agricultural crops, 57 
air filter manufacture, 235 
air pollution, 58 
blending, 49 
book binding, 59 
construction machinery, 62 
desk moving, 339 
diet, 46-47, 164-165 
disease treatment, 57 
either-or, 257 
equipment purchasing, 57 
equipment replacement, 368, 373-375, 378 
financial investment, 50-51, 58 
manufacturing, 156, 161 
maximal flow, 345-363 
minimum cost flow, 388-391 
mix 
coffee, 58 
feed, 60-61 
fertilizer, 58 
food, 60, 211-213 
pesticide, 57 
product, 46 
sweetener, 49-50 
refinery operation, 59-60 
sawmill, 46, 161-164, 205-207, 219-222, 
230-233 
scheduling, 339, 379-385 
shortest route, 368-375, 376-378 
train route, 366-367 
transportation, 47-49 
transshipment, 393-394 
Linear programming solution 
by dual simplex method, 204-214 
by interior path method, 398-402 
by Karmarkar's algorithm, 397-398, 
401-402 
by revised simplex method, 215-222 
by simplex method, 103-104 
Linear system of equations, 5 
condition for no solution, 19 
homogeneous, 19-20 
matrix representation, 5-6 
solved by Gauss Jordan reduction, 17-20 
Linear transformation, 84 
Line segment, 78 
LINGO software, 237, 403, 404 
Logical restraint, critical path method, 380 
Loop, 340 
critical path network, 381 
transportation tableau, 309-312 
LP codes, see Computer software 
LU factorization, 241 
Manufacturing problem, in standard form, 
156, 161 
Marginal value, 163-164 
Mathematical Programming System Ex- 
tended software, 236 
Mathematical Programming System III soft- 
ware, 236 
MATLAB software, 403, 404 
Matrice,~ tvnes of 
augmented, 5-6, 17-20 
capacity, 343 
coefficient, 5 
cost, 328-338 
elementary, 23-24 
identity, 4 
incidence, 340-341 
inverse, 22-27 
invertible, 22 
negative, 3 
noninvertible, 22 
nonsingular, 22-24 
partitioned, 8-9 
permutation, 327-328 
reduced row echelon, see Reduced row 
echelon form 
row equivalent, 13, 17-20 
singular, 22 
sparse, 241 
square, 2 

446 
I.de~ 
Matrices, types of (continued) 
submatrix, 7-8 
transpose, 7-9 
zer 0, 3 
Matrix, 1-2 
column of, 2 
element of, 2 
elementary row operations, 12-13, 23-26 
entry of, 2 
partitioning of, 8-9 
rank of, 41 
row of, 2 
Matrix addition, 2-3 
Matrix multiplication, 3, 4 
Matrix notation, for linear programming, 
63-68 
Matrix representation, of linear equations, 
5-6 
Max Flow-Min Cut Theorem, 358 
Maximal flow problem, 345 
augmenting path method, 350 
backward path, 358-363 
fictitious flows, 349 
labeling algorithm, 352-356 
Max Flow-Min Cut Theorem, 358 
Maximization problem 
conversion to minimization, 53-56, 133 
dual minimization problem, 156-161 
Method, see also Algorithm; Procedure 
augmenting path, 350 
big M, 147-150, 198, 201 
branch and bound, 277-289 
critical path, 382-385 
Dakin's, 280-281, 287-289 
dual simplex, 205-214 
Gomory, see Cutting plane algorithm 
Hungarian, 337-338 
interior path, 398-402 
labeling, 350-363 
Land-Doig, 280 
Larson's, 322-323, 324 
revised simplex, 215-222 
simplex, see Simplex method 
two-phase, see Two-phase method 
Vogel's, 319-323 
Microcomputer software, 236-237, 403-404 
Minimization problem 
conversion to maximization, 53-56, 133 
dual maximization problem, 156-161 
Minimum cost flow problem, 388-391 
Minimum cost rule, 308 
MIPIII software, 291 
Mixed integer programming, 256-258 
branch and bound method, 277-281, 
287-289 
cutting plane algorithm, 268-274 
Dakin's method, 280-281, 287-289 
Model 
deterministic, xx 
probabilistic, xx 
MPSIII/pc software, 237, 403, 404 
MPSX software, 236 
Necessary column, in cost matrix, 336 
Necessary row, in cost matrix, 336 
Negative of a matrix, 3 
Network, 342-343 
circularization, 391 
flow, 343 
maximal flow problem, 345-363 
shortest route problem, 368-375, 376-378 
Network cut 
capacity, 356 
Max Flow-Min Cut Theorem, 358 
Network problem, 292 
computer software, 388-393 
Node, 278, 340 
dangling, 281, 283, 287 
intermediate, 388 
sink, 343, 346-362, 388-391 
source, 343, 346-353, 388-391 
supersink, 390-391 
supersource, 390-391 
terminal, 279, 281, 292 
transshipment, 388 
Node, in solution tree, 278 
Dakin's method, 281 
generation of, 280-281 
selection of, 281 
terminal, 279, 281 
Nonbasic variable, 96 
Noninteger programming, see Linear pro- 
gramming; Mixed integer programming 
Noninvertible matrix, 22 
Nonlinear programming, 51 
Nonoptimal solution, see Optimal solution 
Nonsingular matrix, 22-24 
n-space, 29 
n-tuple, 29 
Null space, 32 
n-vector, 29 

,,,de~ 
447 
Objective function, 51 
boundedness of, 172-173, 176 
changes in, 226, 227-231 
computer modeling, 237-239 
cutting plane algorithm, 264 
dual, 172-173 
geometry of, 75-78 
Objective row, 106 
modification, in duality, 184-189 
optimality criterion, 108-109, 170 
Operations research, xvii 
Operations research study 
phases of, xviii 
Optimality criterion, 108-109 
dual simplex method, 208, 211-214 
Optimal solution, 64 
assignment problem, 329 
boundedness of, 177 
Optimization Subroutine Library software, 
236 
Optimizer, software module, 239 
OR, see Operations research 
Oriented arc, 341 
Oriented graph, 341 
OSL software, 236 
Out-of-kilter algorithm, 388-391 
PAM software, 403, 404 
Parameter, xx 
Parametric programming, 226 
Partition, of feasible solutions, 277-278 
Partitioned matrix, 8-9 
Path, 278, 340 
critical, 385 
interior, 398-402 
Penalty cost, 147 
Permutation matrix, 327-328 
Personal computer software, 236-237, 
403-404 
PERT, see Program Evaluation and Review 
Technique 
Pivot, in reduced row echelon form, 14-16 
Pivotal column, 110 
in Bland's Rule, 129 
Pivotal row, 110, 114 
in Bland's Rule, 129 
Pivoting, 112 
modification, in duality, 184-189 
simplex method, 110-116 
Polynomial time algorithm, 397 
Postprocessor, 239 
Preprocessor, 238-239 
Price, as a dual variable, 163 
Primal problem, 156 
dual-primal solution pairs, 172-182 
dual problem, compared with, 160 
dual solution, 193-201, 205-207 
nonoptimal solution, 205-207 
optimal solution, 207 
Probabilistic model, xx 
Problem statement, changes in, 225-233 
Procedure, see Algorithm; Flowchart; 
Method; Structure diagram 
Profit, see also Cost 
duality, 163 
maximization, 156 
Program Evaluation and Review Technique, 
379 
Pure integer programming, 258 
cutting plane algorithm, 262-268 
Dakin's method, 281-287 
Rank, of a matrix, 41 
Reduced row echelon form, 11-12 
augmented matrix, 17-20 
solution to linear equations, 17-20 
Reinverting the basis, 243 
Replacement value, 164 
Resource vector, changes in, 226, 231-233 
Restarting, 246 
Revised simplex method 
artificial variable, 216 
computer software, 241, 243 
eta matrix and vector, 218 
procedure, 217-219 
speed of computation, 216, 219 
Round-off error, 243 
Route, see Shortest route problem 
Row echelon form 
pivoting to, 112 
reduced, 11-20 
Row equivalent matrices, 131 
augmented, 17-20 
Sawmill problem, 46 
duality, 161-164 
dual simplex method, 205-207 
revised simplex method, 219-222 
sensitivity analysis, 230-233 

448 
~,~ 
Scalar multiplication, 6-7 
Scaling, 56 
computer software, 245-246 
Karmarkar's algorithm, 399-400 
Scheduling problem, 339, 379-385 
Search methods 
assignment problem, 332, 336-337 
solution tree, 280 
Sensitivity analysis, 225-233 
Shadow price, 163 
Shortest route problem, see also Traveling 
salesman problem 
distances and times between nodes, 
376-378 
equipment replacement, 373-375 
procedure, 368-373 
Sigma (E) notation, 4 
Simplex method, 103-104 
artificial variable, 135-147, 189-192 
basic feasible solution, 105-108 
cycling, 126-129 
degenerate solution, 122-126 
departing variable, 109-111, 397 
dual, see Dual simplex method 
entering variable, 108-109, 397 
inefficiency of, 250, 252, 346, 388 
integer programming, 263 
iterations, number of, 397 
modification, in duality, 184-189 
nonnegative constraints, 106, 114-118, 
131-135 
optimality criterion, 108-109 
pivoting, 110-116 
polynomial time algorithm, 397 
revised, see Revised simplex method 
running time, 397 
tableau construction, 106-114 
two-phase method, 135-147, 188-193 
Singular matrix, 22 
Sink node, 343 
maximal flow problem, 346-362 
minimum cost flow problem, 388-391 
Slackness, complementary, 178-181 
Slack time, 385 
Slack variable, 65 
primal and dual problems, 178-181 
simplex method, 104-106, 116-118 
standard to canonical conversion, 65-68 
Software, computer, see Computer software 
Solution tree, 278-280 
Source node, 343 
maximal flow problem, 346-353 
minimum cost flow problem, 388-391 
n-space, 29 
Spanning set, 34-35, 38 
Sparse matrix, 241 
Square matrix, 2 
Standard form, 51-53 
canonical form, conversion to, 56, 65-68, 
97-98 
defined, 51-53 
duality, 161-165 
general problem, conversion to, 54-56 
manufacturing problem, 156, 161 
simplex method, 104-118 
Starting procedure, see also Initial basic fea- 
sible solution 
restarting, 246 
transportation problem, 319-323 
Stock cutting problem, 255 
Structure diagram, see also Flowchart; Pro- 
cedure 
dual simplex method, 209 
simplex method, 116 
two-phase method, 147 
Submatrix, 7-8 
Subspace, 29-32 
dimension, 38 
null space, 32 
trivial, 30 
Summation notation, 4 
Supersink node, 390-391 
Supersource node, 390-391 
Supply and demand tableaux, 304-324 
Supply and demand vectors, 296-297 
Tableau 
cell, 309 
construction, 106-114 
cycling, 127-129 
degenerate solution, 122-126 
final, 193 
initial, 106-108 
linear combination of columns, 168 
loop, 309-312 
modification, in duality, 184-189 
pivoting, 110-116, 184-189 
transportation problem, 304-324 
tree, in branch and bound method, 288, 
289 
Terminal node, 279, 282 

~,,de~ 
449 
Theta (0) ratio, 110, 114 
tie for minimum, 122-126 
Time 
event, 382-383 
float, 385 
between nodes, scheduling problem, 
379-385 
between nodes, shortest route problem, 
376-378 
polynomial, 397 
slack, 385 
TRANCOL software, 240 
Transportation algorithm, 308-309 
minimum cost rule, 308 
Transportation problem, 47-49, 295 
assignment problem, as a, 327 
cost matrix, 296 
cost minimization, 299-324 
cycling, 317 
degeneracy, 317-318 
dual problem, 304-308 
dummy destination, 324 
initial basic feasible solution, 308, 319-323 
integer programming, 249-250 
loops, 309-312 
minimum cost rule, 308 
properties, 298 
routes, 300-304 
supply greater than demand extension, 324 
tableaux, 298, 304-324 
Transpose, of a matrix, 7-9 
Transshipment node, 388 
Traveling salesman problem, see also Short- 
est route problem, 252-254 
Tree of solutions, 278-280 
n-tuple, 29 
Two-phase method 
final optimal solution, 188-193 
initial basic solution, 135-147 
optimal dual solution, 198-201 
UKILT 1100 Out-of-Kilter software, 391 
Unboundedness, see Boundedness 
Unknown, see Decision variable 
Variable 
artificial, 135-150, 189-192 
basic, 96, 264, 276-277 
constraint, 55-56 
decision, xx 
departing, 109-111, 309, 311-312, 397 
dual, 163-165, 178-179, 181 
entering, 108-109, 397 
integer, see Integer programming 
nonbasic, 96 
slack, see Slack variable 
unconstrained, 55 
Vector 
basis, 38-40 
components of, 29 
coordinate, 39-40 
feasible solutions, 93-95 
linear combination, 33-34, 37 
linear independence, 35-40, 93-95 
n-vector, 29 
spanning set, 34-35, 38 
subspace, 30-32, 38 
supply and demand, 296-297 
Vertex, of a graph, 340 
Vogel's method, 319-323 
Weak Duality Theorem, 172-173 
WHAT'S BEST! software, 237, 403, 404 
Zero matrix, 3 
Zero-one programming, 251, 277-279 


Springer Tracts in Advanced Robotics
Volume 20
Editors: Bruno Siciliano · Oussama Khatib · Frans Groen

Springer Tracts in Advanced Robotics
Edited by B. Siciliano, O. Khatib, and F. Groen
Vol. 19: Lefebvre, T.; Bruyninckx, H.; De Schutter, J.
Nonlinear Kalman Filtering for Force-Controlled
Robot Tasks
280 p. 2005 [3-540-28023-5]
Vol. 18: Barbagli, F.; Prattichizzo, D.; Salisbury, K. (Eds.)
Multi-point Interaction with Real and Virtual Objects
281 p. 2005 [3-540-26036-6]
Vol. 17: Erdmann, M.; Hsu, D.; Overmars, M.;
van der Stappen, F.A (Eds.)
Algorithmic Foundations of Robotics VI
472 p. 2005 [3-540-25728-4]
Vol. 16: Cuesta, F.; Ollero, A.
Intelligent Mobile Robot Navigation
224 p. 2005 [3-540-23956-1]
Vol. 15: Dario, P.; Chatila R. (Eds.)
Robotics Research { The Eleventh International
Symposium
595 p. 2005 [3-540-23214-1]
Vol. 14: Prassler, E.; Lawitzky, G.; Stopp, A.;
Grunwald, G.; Hagele, M.; Dillmann, R.;
Iossiˇdis. I. (Eds.)
Advances in Human-Robot Interaction
414 p. 2005 [3-540-23211-7]
Vol. 13: Chung, W.
Nonholonomic Manipulators
115 p. 2004 [3-540-22108-5]
Vol. 12: Iagnemma K.; Dubowsky, S.
Mobile Robots in Rough Terrain {
Estimation, Motion Planning, and Control
with Application to Planetary Rovers
123 p. 2004 [3-540-21968-4]
Vol. 11: Kim, J.-H.; Kim, D.-H.; Kim, Y.-J.; Seow, K.-T.
Soccer Robotics
353 p. 2004 [3-540-21859-9]
Vol. 10: Siciliano, B.; De Luca, A.; Melchiorri, C.;
Casalino, G. (Eds.)
Advances in Control of Articulated and Mobile Robots
259 p. 2004 [3-540-20783-X]
Vol. 9: Yamane, K.
Simulating and Generating Motions of Human Figures
176 p. 2004 [3-540-20317-6]
Vol. 8: Baeten, J.; De Schutter, J.
Integrated Visual Servoing and Force Control
198 p. 2004 [3-540-40475-9]
Vol. 7: Boissonnat, J.-D.; Burdick, J.; Goldberg, K.;
Hutchinson, S. (Eds.)
Algorithmic Foundations of Robotics V
577 p. 2004 [3-540-40476-7]
Vol. 6: Jarvis, R.A.; Zelinsky, A. (Eds.)
Robotics Research { The Tenth International Symposium
580 p. 2003 [3-540-00550-1]
Vol. 5: Siciliano, B.; Dario, P. (Eds.)
Experimental Robotics VIII
685 p. 2003 [3-540-00305-3]
Vol. 4: Bicchi, A.; Christensen, H.I.;
Prattichizzo, D. (Eds.)
Control Problems in Robotics
296 p. 2003 [3-540-00251-0]
Vol. 3: Natale, C.
Interaction Control of Robot Manipulators {
Six-degrees-of-freedom Tasks
120 p. 2003 [3-540-00159-X]
Vol. 2: Antonelli, G.
Underwater Robots { Motion and Force Control of
Vehicle-Manipulator Systems
209 p. 2003 [3-540-00054-2]
Vol. 1: Caccavale, F.; Villani, L. (Eds.)
Fault Diagnosis and Fault Tolerance for Mechatronic
Systems { Recent Advances
191 p. 2002 [3-540-44159-X]

Yangsheng Xu  Yongsheng Ou
Control
of Single Wheel Robots
With 122 Figures and 34 Tables

Professor Bruno Siciliano, Dipartimento di Informatica e Sistemistica, Universit`a degli Studi di Napoli Federico
II, Via Claudio 21, 80125 Napoli, Italy, email: siciliano@unina.it
Professor Oussama Khatib, Robotics Laboratory, Department of Computer Science, Stanford University, St-
anford, CA 94305-9010, USA, email: khatib@cs.stanford.edu
Professor Frans Groen, Department of Computer Science, Universiteit van Amsterdam, Kruislaan 403, 1098
SJ Amsterdam, The Netherlands, email: groen@science.uva.nl
STAR (Springer Tracts in Advanced Robotics) has been promoted under the auspices of EURON (European
Robotics Research Network)
Authors
Yangsheng Xu
Yongsheng Ou
Chinese University of Hong Kong
Department of Automation and Computer-Aided Engineering
Shatin
Hong Kong SAR, P.R. China
ISSN print edition: 1610-7438
ISSN electronic edition: 1610-742X
ISBN-10 3-540-28184-3
Springer Berlin Heidelberg New York
ISBN-13 978-3-540-28184-9
Springer Berlin Heidelberg New York
Library of Congress Control Number: 2005930322
This work is subject to copyright. All rights are reserved, whether the whole or part of the material is concerned,
speciﬁcally the rights of translation, reprinting, reuse of illustrations, recitation, broadcasting, reproduction on
microﬁlm or in other ways, and storage in data banks. Duplication of this publication or parts thereof is permitted
only under the provisions of the German Copyright Law of September 9, 1965, in its current version, and
permission for use must always be obtained from Springer-Verlag. Violations are liable to prosecution under
German Copyright Law.
Springer is a part of Springer Science+Business Media
springeronline.com
© Springer-Verlag Berlin Heidelberg 2005
Printed in Germany
The use of general descriptive names, registered names, trademarks, etc. in this publication does not imply,
even in the absence of a speciﬁc statement, that such names are exempt from the relevant protective laws and
regulations and therefore free for general use.
Typesetting: Digital data supplied by editors.
Data-conversion and production: PTP-Berlin Protago-TEX-Production GmbH, Germany
Cover-Design: design & production GmbH, Heidelberg
Printed on acid-free paper
89/3141/Yu - 5 4 3 2 1 0

Editorial Advisory Board
EUROPE
Herman Bruyninckx, KU Leuven, Belgium
Raja Chatila, LAAS, France
Henrik Christensen, KTH, Sweden
Paolo Dario, Scuola Superiore Sant’Anna Pisa, Italy
R¨udiger Dillmann, Universit¨at Karlsruhe, Germany
AMERICA
Ken Goldberg, UC Berkeley, USA
John Hollerbach, University of Utah, USA
Lydia Kavraki, Rice University, USA
Tim Salcudean, University of British Columbia, Canada
Sebastian Thrun, Stanford University, USA
ASIA/OCEANIA
Peter Corke, CSIRO, Australia
Makoto Kaneko, Hiroshima University, Japan
Sukhan Lee, Sungkyunkwan University, Korea
Yangsheng Xu, Chinese University of Hong Kong, PRC
Shin’ichi Yuta, Tsukuba University, Japan

To our families

Foreword
At the dawn of the new millennium, robotics is undergoing a major trans-
formation in scope and dimension. From a largely dominant industrial focus,
robotics is rapidly expanding into the challenges of unstructured environ-
ments. Interacting with, assisting, serving, and exploring with humans, the
emerging robots will increasingly touch people and their lives.
The goal of the new series of Springer Tracts in Advanced Robotics (STAR)
is to bring, in a timely fashion, the latest advances and developments in
robotics on the basis of their signiﬁcance and quality. It is our hope that the
wider dissemination of research developments will stimulate more exchanges
and collaborations among the research community and contribute to further
advancement of this rapidly growing ﬁeld.
The monograph written by Yangsheng Xu and Yongsheng Ou is the cul-
mination of a considerable body of research by the ﬁrst author with the recent
support of the second author’s Ph.D. dissertation. The work builds upon a
novel concept in locomotion of nonholonomic underactuated robots, a ﬁeld
which has lately been attracting more and more scholars. Design, modelling
and control of a single-wheel, gyroscopically stabilized robot are explained in
detail, and its advantages over multiwheel vehicles are discussed. The volume
oﬀers a comprehensive treatment of the subject matter from the theoretical
development to experimental testing, while foreseeing a number of potential
applications of the new design in service robotics.
Certainly of interest to researchers in mobile robot dynamics and control,
this title constitutes a ﬁne addition to the series!
Naples, Italy,
Bruno Siciliano
April 2005
STAR Editor

Preface
The book is a collection of the research work on the control of a single wheel
gyroscopically stabilized robot. Although the robot was originally developed
at Carnegie Mellon University, most work shown in this book was carried
out in the Chinese University of Hong Kong. It focuses on the dynamics
and control aspects of the system, including dynamic modeling, model-based
control, learning-based control, and shared control with human operators.
The robot is a single wheel balanced by a spinning ﬂywheel attached
through a two-link manipulator at a drive motor. The spinning ﬂywheel acts
as a gyroscope to stabilize the robot, and at the same time it can be tilted
to achieve steering. It shows a completely diﬀerent picture from conventional
mobile robots, opening up the science of dynamically stable, but statically
unstable systems.
Conventional robots, either working in industry or service, are statically
stable, i.e., the motion is stable when the speed is low, and unstable or mal-
functioned when the speed is high. Dynamically stable robots, on the other
hand, are getting more and more stable when the speed is increased and tend
to be stable even in a rough terrain. The nature of the system is nonholonomic,
underactuated, and nonlinear, providing a rich array of research issues in dy-
namics, control, and sensing, which we have studied in this book to establish
a foundation of the science of dynamically stabilized robotics.
Potential applications for it are numerous. Because it can travel on both
land and water, it is of amphibious use on beaches or swampy areas, for trans-
portation, rescue, frontier inspections, mining detection, environment moni-
toring or recreation. As a surveillance robot, it could take advantage of its
slim proﬁle to pass through doorways and narrow passages, and its ability
of turning in place to maneuver in tight quarters. It also can be used as a
high-speed lunar vehicle, where the absence of aerodynamic disturbances and
low gravity would permit eﬃcient, high-speed mobility.
The road-map of this book is as follows.
In Chapter 1, a detailed description about the robot is given. We ﬁrstly
introduce the history of the robot’s development. Then, the hardware com-

XII
Preface
ponents and the robot’s concept are discussed. Finally, a summary of design
implementation for the robot is addressed.
In Chapter 2, according to the third prototype of the robot, we derive the
kinematics and dynamic model and study its dynamic properties, including
the stabilization and the tilt eﬀect of the ﬂywheel to the robot. This chapter
is based on the thesis work done by Mr. Samuel Au Kwok-Wai under the
supervision of the ﬁrst author.
In Chapter 3, based on the dynamic model of the robot, we study two
classes of nonholonomic constraints associated with the system. We then pro-
pose control laws for balance control, point-to-point control and line tracking
in Cartesian space. The experimental implementation for verifying the control
laws is provided.
Chapter 4 deals with a learning-based approach realized by learning from
human expert demonstration, as the model-based control for such a dynam-
ically stable system is too challenging. We then investigate the convergence
analysis for this class of learning-based controllers. Last, a method of including
new training samples without increasing computational costs is proposed.
Chapter 5 discusses on the input selection topic and the neural network
models for the motions of lateral balancing and tiltup implemented experi-
mentally. By combining the two motions into one, the robot is able to recover
from the fall position, and then to remain stable at the vertical position after
tiltup.
Since autonomous functions in a system may not work perfectly in some
unexpected situations, a level of intervention by the human operator is there-
fore necessary. In Chapter 6, the shared control with human operators, by us-
ing the aforementioned autonomous control approaches is investigated. Chap-
ters 5 and 6 are partially an extension of the thesis work done by Mr. Cedric
Kwok-Ho Law under the supervision of the ﬁrst author.
This book is appropriate for postgraduate students, research scientists and
engineers with interests in mobile robot dynamics and control. In particular,
the book will be a valuable reference for those interested in the topics of
mechanical and electrical design and implementation, dynamic modeling for
disc-liked mobile robots, and model-based control or learning based control in
the context of dynamically stable systems such as unicycle, bicycle, dicycle,
motorcycle and legged robots.
We would like to thank Mr. H. Ben Brown, Project Scientist in Robotics
Institute at Carnegie Mellon University, USA, for his original contribution to
the robot. Mr. Brown, while working with the ﬁrst author at Carnegie Mellon
University, designed and developed several generations of the robot. Although
most work shown in this book was carried out in the Chinese University of
Hong Kong, it would be impossible without the assistance of Mr. Brown for
building the excellent platform for real-time control and various experiments.
The ﬁrst author would also like to take this opportunity to thank Mr. Brown
for his long-term support, encouragement and friendship that made his time
in Carnegie Mellon more interesting, more exciting, and more meaningful.

Preface
XIII
Thanks also go to Mr. Samuel Au Kwok-Wai for his preliminary work
in the wireless communication and software programming which provides a
solid foundation for the control implementation. The authors also extend our
thanks to Mr. Huihuan Qian for proofreading the text. We would also like to
thank our colleagues for their valuable technical assistance in the ﬁnal stage
of preparing this monograph.
Finally, this book is supported in part by Hong Kong Research Grant
Council under the grants CUHK 4403/99E and CUHK 4228/01E.
The Chinese University of Hong Kong,
Yangsheng Xu and
Yongsheng Ou
Summer 2005

Contents
Foreword . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . IX
Preface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . XI
List of Figures 
  XIX
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
List of Tables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . XXIII
1 
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
1
1.1 Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
1
1.1.1 Brief History of Mobile Robots . . . . . . . . . . . . . . . . . . . . . . 
1
1.1.2 Problems of Stable Robots . . . . . . . . . . . . . . . . . . . . . . . . . 
3
1.1.3 Dynamically Stable Mobile Robots . . . . . . . . . . . . . . . . . . 
3
1.2 Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
5
1.2.1 Concept and Compromise . . . . . . . . . . . . . . . . . . . . . . . . . . 
5
1.2.2 Mechanism Design. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
7
1.2.3 Sensors and Onboard Computer . . . . . . . . . . . . . . . . . . . . . 
8
1.2.4 Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
2 
Kinematics and Dynamics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
2.1 Modeling in a Horizontal Plane. . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
2.1.1 Kinematic Constraints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
2.1.2 Equations of Motion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
2.1.3 Dynamic Properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
2.1.4 Simulation Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
2.2 Modeling on an Incline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
2.2.1 Motion on an Incline. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
2.2.2 Motion Planning on an Incline . . . . . . . . . . . . . . . . . . . . . . 25
2.2.3 Simulation Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29

XVI 
Contents
3 
Model-based Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
3.1 Linearized Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
3.1.1 Stabilization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
3.1.2 Path Following Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
3.1.3 Control Simulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
3.2 Nonlinear Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42
3.2.1 Balance Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47
3.2.2 Position Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50
3.2.3 Line Tracking Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54
3.2.4 Simulation Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56
3.3 Control Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63
3.3.1 Vertical Balance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68
3.3.2 Position Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68
3.3.3 Path Following . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71
4 
Learning-based Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73
4.1 Learning by CNN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73
4.1.1 Cascade Neural Network with Kalman Filtering . . . . . . . 74
4.1.2 Learning architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76
4.1.3 Model evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77
4.1.4 Training procedures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80
4.2 Learning by SVM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82
4.2.1 Support Vector Machines . . . . . . . . . . . . . . . . . . . . . . . . . . . 82
4.2.2 Learning Approach . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84
4.2.3 Convergence Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87
4.2.4 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96
4.3 Learning Control with Limited Training Data . . . . . . . . . . . . . . . 99
4.3.1 Eﬀect of Small Training Sample Size . . . . . . . . . . . . . . . . . 102
4.3.2 Resampling Approach . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109
4.3.3 Local Polynomial Fitting (LPF) . . . . . . . . . . . . . . . . . . . . . 110
4.3.4 Simulations and Experiments . . . . . . . . . . . . . . . . . . . . . . . 112
5 
Further Topics on Learning-based Control . . . . . . . . . . . . . . . . . 119
5.1 Input Selection for Learning Human Control Strategy . . . . . . . . 119
5.1.1 Sample Data Selection and Regrouping. . . . . . . . . . . . . . . 121
5.1.2 Signiﬁcance Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123
5.1.3 Dependency Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127
5.1.4 Experimental Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129
5.2 Implementation of Learning Control . . . . . . . . . . . . . . . . . . . . . . . 135
5.2.1 Validation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 136
5.2.2 Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 142
5.2.3 Discussions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 146

Contents
XVII
6 
Shared Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151
6.1 Control Diagram . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151
6.2 Schemes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153
6.2.1 Switch Mode. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 154
6.2.2 Distributed Mode . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 154
6.2.3 Combined Mode. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155
6.3 Shared Control of Gyrover . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155
6.4 How to Share . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 158
6.5 Experimental Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161
6.5.1 Heading Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 162
6.5.2 Straight Path . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 162
6.5.3 Circular Path . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 165
6.5.4 Point-to-point Navigation . . . . . . . . . . . . . . . . . . . . . . . . . . 165
6.6 Discussions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 166
7 
Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 175
7.1 Concluding Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 175
7.1.1 Concept and Implementations. . . . . . . . . . . . . . . . . . . . . . . 175
7.1.2 Kinematics and Dynamics . . . . . . . . . . . . . . . . . . . . . . . . . 175
7.1.3 Model-based Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 176
7.1.4 Learning-based Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . 176
7.2 Future Research Directions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 177
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 179
Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 187

List of Figures
1.1
The basic conﬁguration of the robot . . . . . . . . . . . . . . . . . . . . . . . .
6
1.2
Communication equipment: radio transmitter (left) and
laptops with wireless Modem (right). . . . . . . . . . . . . . . . . . . . . . . .
9
1.3
Hardware conﬁguration of the robot. . . . . . . . . . . . . . . . . . . . . . . .
9
1.4
The ﬁrst prototype of the robot . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
1.5
The second prototype of the robot . . . . . . . . . . . . . . . . . . . . . . . . . . 11
1.6
The third prototype of the robot . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
2.1
Deﬁnition of coordinate frames and system variables
. . . . . . . . . 13
2.2
The simulation results of a rolling disk without the ﬂywheel. . . . 22
2.3
The simulation results of the single wheel robot.. . . . . . . . . . . . . . 22
2.4
The simulation results of tilting the ﬂywheel of the robot with
˙βa = 73 deg/s . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
2.5
The experimental results of tilting the ﬂywheel of the robot
with ˙βa = 73 deg/s . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
2.6
Critical torque of a rolling disk v.s. a climbling angle . . . . . . . . . . 26
2.7
Critical torque of the robot v.s. a climbling angle . . . . . . . . . . . . . 26
2.8
Change orientation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
2.9
Disk rolls on a plane . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
2.10 Rolling up of a disk . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
2.11 Rolling up of a single wheel robot . . . . . . . . . . . . . . . . . . . . . . . . . . 31
2.12 Rolling down of a disk . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
2.13 Rolling down of a single wheel robot . . . . . . . . . . . . . . . . . . . . . . . . 32
3.1
The lateral description of Gyrover. . . . . . . . . . . . . . . . . . . . . . . . . . 34
3.2
Schematic of the control algorithms. . . . . . . . . . . . . . . . . . . . . . . . . 34
3.3
Principle of line following.(top view) . . . . . . . . . . . . . . . . . . . . . . . . 38
3.4
Schematic of the control algorithm for the Y-axis. . . . . . . . . . . . . 38
3.5
The simulation results (S1) for following the Y-axis. . . . . . . . . . . 41
3.6
The simulation results (S2) for following the Y-axis. . . . . . . . . . . 41
3.7
The simulation results (S3) for following the Y-axis. . . . . . . . . . . 41

XX 
List of Figures
3.8 The simulation results for following the Y-axis with
˙γ = 10 rad/s. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
3.9
The simulation results for following the Y-axis with
˙γ = 30 rad/s. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
3.10 The simulation results for following the Y-axis with σ = 20.. . . . 43
3.11 The simulation results for following the Y-axis with σ = 40.. . . . 44
3.12 System parameters of Gyrover’s simpliﬁed model. . . . . . . . . . . . . 45
3.13 Parameters in position control. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51
3.14 The robot’s path along connected corridors. . . . . . . . . . . . . . . . . . 54
3.15 The parameters in the line tracking problem. . . . . . . . . . . . . . . . . 55
3.16 Leaning angle β in balance control. . . . . . . . . . . . . . . . . . . . . . . . . . 58
3.17 Precession angle velocity ˙α in balance control. . . . . . . . . . . . . . . . 58
3.18 Driving speed ˙γ in balance control. . . . . . . . . . . . . . . . . . . . . . . . . . 59
3.19 Parameters of ˙α, β, ˙β, ˙γ in balance control. . . . . . . . . . . . . . . . . . . . 59
3.20 Leaning angle β in balance control II. . . . . . . . . . . . . . . . . . . . . . . . 60
3.21 Precession angle velocity ˙α in balance control II. . . . . . . . . . . . . . 60
3.22 Driving speed ˙γ in balance control II. . . . . . . . . . . . . . . . . . . . . . . . 61
3.23 Parameters of ˙α, β, ˙β, ˙γ in balance control II. . . . . . . . . . . . . . . . . 61
3.24 Displacement in X. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62
3.25 Displacement in Y. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63
3.26 X - Y of origin. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63
3.27 The joint-space variables in position control. . . . . . . . . . . . . . . . . . 64
3.28 Line tracking in X direction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64
3.29 Line tracking in Y direction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65
3.30 X - Y of in line tracking. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65
3.31 The joint-space variables in line tracking. . . . . . . . . . . . . . . . . . . . . 66
3.32 Function Tanh(.).. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66
3.33 Function Uanh(.). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66
3.34 Hardware conﬁguration of Gyrover. . . . . . . . . . . . . . . . . . . . . . . . . 67
3.35 Experiment in line following control. . . . . . . . . . . . . . . . . . . . . . . . 67
3.36 Camera pictures in balance control. . . . . . . . . . . . . . . . . . . . . . . . . . 68
3.37 Sensor data in balance control. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69
3.38 Trajectories in point-to-point control. . . . . . . . . . . . . . . . . . . . . . . . 70
3.39 Sensor data in point-to-point control. . . . . . . . . . . . . . . . . . . . . . . . 70
3.40 Trajectories in the straight path test. . . . . . . . . . . . . . . . . . . . . . . . 71
3.41 Sensor data in the straight path test. . . . . . . . . . . . . . . . . . . . . . . . 71
4.1
The cascade learning architecture. . . . . . . . . . . . . . . . . . . . . . . . . . . 76
4.2
Similarity measure between ¯
O1 and ¯
O2. . . . . . . . . . . . . . . . . . . . . . 79
4.3
Control data for diﬀerent motions. . . . . . . . . . . . . . . . . . . . . . . . . . . 80
4.4
Switchings in human control of the ﬂywheel. . . . . . . . . . . . . . . . . . 81
4.5
Similar inputs can be mapped to extreme diﬀerent outputs if
switching occurs. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82
4.6
The practical system and the human control from a dynamic
system. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88

List of Figures 
XXI 
4.7 A learning controller. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89
4.8 Gyrover: A single-wheel robot. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97
4.9 Deﬁnition of the Gyrover’s system parameters. . . . . . . . . . . . . . . . 97
4.10 The tilt angle βa Lean angle β of SVM learning control. . . . . . . 100
4.11 U1 comparison of the same Human control and SVM learner. . . 100
4.12 Curse of dimensionality. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101
4.13 Linear regression M=1. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108
4.14 Polynomial degree M=3. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108
4.15 Polynomial degree M=10. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108
4.16 The RMS error for both training and test sets. . . . . . . . . . . . . . . . 108
4.17 Examples of the unlabelled sample generation, when k = 3. . . . . 110
4.18 Local polynomial ﬁtting for lean angle β. . . . . . . . . . . . . . . . . . . . . 115
4.19 Comparison of U1 in a set of testing data. . . . . . . . . . . . . . . . . . . . 116
4.20 Human control. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116
4.21 The CNN-new model learning control. . . . . . . . . . . . . . . . . . . . . . . 117
5.1 Clustering the data into a small ball with radius r. . . . . . . . . . . . 122
5.2 The local sensitivity coeﬃcients of the ﬁrst four signiﬁcance
variables. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131
5.3 SVM learning control results. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134
5.4 Vertical balanced motion by human control, X(1,1). . . . . . . . . . . . 137
5.5 Control trajectories comparison for X(1,1). . . . . . . . . . . . . . . . . . . . 137
5.6 Vertical balanced motion by human control, X(1,2). . . . . . . . . . . . 138
5.7 Control trajectories comparison for X(1,2). . . . . . . . . . . . . . . . . . . . 138
5.8 Vertical balanced motion by human control, X(1,3). . . . . . . . . . . . 139
5.9 Control trajectories comparison for X(1,3). . . . . . . . . . . . . . . . . . . . 139
5.10 Tiltup motion by human control, X(2,1). . . . . . . . . . . . . . . . . . . . . 140
5.11 Control trajectories comparison for X(2,1). . . . . . . . . . . . . . . . . . . . 140
5.12 Tiltup motion by human control, X(2,2). . . . . . . . . . . . . . . . . . . . . 141
5.13 Control trajectories comparison for X(2,2). . . . . . . . . . . . . . . . . . . . 141
5.14 Tiltup motion by human control, X(2,3). . . . . . . . . . . . . . . . . . . . . 141
5.15 Control trajectories comparison for X(2,3). . . . . . . . . . . . . . . . . . . . 142
5.16 Vertical balancing by CNN model, trail #1. . . . . . . . . . . . . . . . . . 143
5.17 Vertical balancing by CNN model, trail #2. . . . . . . . . . . . . . . . . . 143
5.18 Vertical balancing by CNN model, trail #3. . . . . . . . . . . . . . . . . . 144
5.19 Vertical balancing by human operator. . . . . . . . . . . . . . . . . . . . . . . 145
5.20 Tiltup motion by CNN model, trail #1. . . . . . . . . . . . . . . . . . . . . . 146
5.21 Tiltup motion by CNN model, trail #2. . . . . . . . . . . . . . . . . . . . . . 147
5.22 Tiltup motion by human operator.. . . . . . . . . . . . . . . . . . . . . . . . . . 147
5.23 Combined motion. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148
5.24 Fluctuation in the lean angle made by the tiltup model. . . . . . . . 148
5.25 Tiltup and vertical balanced motion by CNN models. . . . . . . . . . 149
6.1 Switch mode. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 154
6.2 Distributed control mode. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155

XXII
List of Figures
6.3 Combined mode. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 156
6.4 A detailed structure of behavior connectivity in Gyrover control.156
6.5 Subsumption architecture of shared control. . . . . . . . . . . . . . . . . . 157
6.6 Sensor data acquired in the heading control test, A = 0.2. . . . . . 163
6.7 Sensor data acquired in the heading control test, A = 0.8. . . . . . 164
6.8 Experiment on tracking a straight path under shared control. . . 165
6.9 Experiment on tracking a curved path under shared control. . . . 165
6.10 Experiment on point-to-point navigation under shared control. . 167
6.11 Trajectory travelled in the straight path test. . . . . . . . . . . . . . . . . 168
6.12 Sensor data acquired in the straight path test. . . . . . . . . . . . . . . . 169
6.13 Gyrover trajectories in the curved path test. . . . . . . . . . . . . . . . . . 170
6.14 Sensor data acquired in the circular path test. . . . . . . . . . . . . . . . 171
6.15 Gyrover trajectories in the combined path test.. . . . . . . . . . . . . . . 172
6.16 Sensor data acquired in the combined path test. . . . . . . . . . . . . . . 173

List of Tables
1.1
Table of diﬀerent actuating mechanisms in Gyrover. . . . . . . . . . .
8
2.1
Variables deﬁnitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
2.2
Parameters used in simulation and experiments . . . . . . . . . . . . . . 21
2.3
System parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
3.1
The initial conditions for the simulations with diﬀerent initial
heading angles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
3.2
Physical parameters. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56
3.3
Initial parameters in balance control. . . . . . . . . . . . . . . . . . . . . . . . 57
3.4
Target and gain parameters in balance control I. . . . . . . . . . . . . . 57
3.5
Initial parameters in position control. . . . . . . . . . . . . . . . . . . . . . . . 62
3.6
Target and gain parameters in position control. . . . . . . . . . . . . . . 62
3.7
Initial parameters in line tracking. . . . . . . . . . . . . . . . . . . . . . . . . . . 62
3.8
Target and gain parameters in line tracking. . . . . . . . . . . . . . . . . . 63
4.1
Similarity measures between diﬀerent control trajectories. . . . . . 79
4.2
Sample human control data. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98
4.3
The training data sample. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113
4.4
The testing data sample . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113
4.5
The results of learning error with unlabelled training data. . . . . 113
4.6
Sample human control data. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114
5.1
Gyrover’s sensor data string . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130
5.2
Sample of human control data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130
5.3
The noise variance σ information for variables. . . . . . . . . . . . . . . . 130
5.4
The signiﬁcance order table . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132
5.5
The average “(linear) relative” coeﬃcients ¯ρ in full system
variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132
5.6
Part of ¯ρ in the 5 variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133
5.7
¯ρ in the 6 variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133

XXIV 
List of Tables
5.8 Part of ¯ρ in the 4 variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133
5.9
Similarity measures for vertical balanced control between
human and CNN model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 136
5.10 Similarity measures for tiltup control between human and
CNN model. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140
5.11 Performance measures for vertical balancing. . . . . . . . . . . . . . . . . . 142
5.12 Performance measures for tiltup motion. . . . . . . . . . . . . . . . . . . . . 144
5.13 Performance measures for combined motion. . . . . . . . . . . . . . . . . . 146
6.1
Decision making of A = 0.25. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160
6.2
Decision making of A = 0.50. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160
6.3
Decision making of A = 0.75. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161

1
Introduction
1.1 Background
1.1.1 Brief History of Mobile Robots
Land locomotion can be broadly characterized as quasi-static or dynamic.
Quasi-static equilibrium implies that inertial (acceleration-related) eﬀects are
small enough to ignore. That is, motions are slow enough that static equi-
librium can be assumed. Stability of quasi-static systems depends on keeping
the gravity vector through, the center of mass, within the vehicle’s polygon of
support determined by the ground-contact points of its wheels or feet. Energy
input is utilized predominantly in reacting against static forces. Such systems
typically have relatively rigid members, and can be controlled on the basis of
kinematic considerations.
In dynamic locomotion, inertial forces are signiﬁcant with respect to grav-
itational forces. Dynamic eﬀects gain relative importance when speed is high,
gravity is weak and dynamic disturbances (e.g. rough terrain) are high. Signif-
icant energy input is required in controlling system momentum, and in some
cases, in controlling elastic energy storage in the system. As performance lim-
its of mobile robots are pushed, dynamic eﬀects will increasingly come into
play. Further, robotic systems that behave dynamically may be able to ex-
ploit momentum to enhance mobility, as is clearly demonstrated by numerous
human-controlled systems: gymnasts, dancers and other athletes; stunt bicy-
cles and motorcycles; motorcycles on rough terrain; cars that vault obstacles
from ramps; etc.
It is paradoxical that those factors which produce static stability may con-
tradict dynamic stability. For example, a four-wheel vehicle that is very low
and wide has a broad polygon of support, is very stable statically, and can
tolerate large slopes without roll-over. However, when this vehicle passes over
bumps, dynamic disturbances at the wheels generate large torques, tending
to upset the vehicle about the roll, pitch and yaw axes. In eﬀect, the large
Y. Xu and Y. Ou: Control of Single Wheel Robots, STAR 20, pp. 1–12, 2005.
© Springer-Verlag Berlin Heidelberg 2005

2
1 Introduction
polygon of support required for static stability provides a leverage mecha-
nism for the generation of dynamic torque disturbances. Further, the support
points must comply with the surface, statically as well as dynamically, by con-
trol of support points (e.g. suspension) and/or by vehicle attitude changes.
Sophisticated vehicle suspensions have been developed to minimize dynamic
disturbances, but passive-spring suspensions decrease static stability by al-
lowing the center of mass to move toward the outside of the support polygon.
Active suspensions may overcome this problem, but require additional com-
plexity and energy expenditure.
As a second example, consider a bicycle or motorcycle which has two
wheels in the fore-aft (tandem) conﬁguration. Such a vehicle is statically un-
stable in the roll direction, but achieves dynamic stability at moderate speed
through appropriate steering geometry and gyroscopic action of the steered
front wheel. Steering stability generally increases with speed due to gyroscopic
eﬀects. Dynamic forces at the wheel-ground contact point act on or near the
vehicle center (sagital) plane, and thus produce minimal roll disturbances.
Additionally, the bicycle can remain upright when traveling on side slopes.
Thus, sacriﬁcing static roll stability enhances the dynamic roll stability and
permits the vehicle to automatically adjust to side slopes.
As a logical extension of this argument, consider a wheel rolling down an
incline. Under the inﬂuence of gravity, gyroscopic action causes the wheel to
precess (the axis of wheel rotation turns) about the vertical axis–rather than
simply falling sideways as it does when not rolling–and the wheel steers in the
direction it is leaning. The resulting curved path of motion of the wheel on the
ground produces radial (centrifugal) forces at the wheel-ground contact point,
tending to right the wheel. Dynamic disturbances due to surface irregularities
act through or near the wheel’s center of mass, producing minimal torques
in roll, pitch and yaw. The angular momentum of the wheel, in addition to
providing the natural gyroscopic steering mechanism, tends to stabilize the
wheel with respect to roll and yaw. In terms of attitude control, the wheel is
relatively insensitive to fore/aft and side slopes. The result is a highly stable
rolling motion with minimal attitude disturbances and tolerance to fore/aft
and vertical disturbances. One can readily observe this behavior by rolling an
automobile tire down a bumpy hillside.
There are precedents for single-wheel-like vehicles. In 1869, R.C. Hem-
mings [43] patented a Velocipede, a large wheel encircling the rider, powered
by hand cranks. Palmer [82] describes several single-wheel vehicles with an
operator riding inside. A 1935 publication [72] describes the Gyroauto, which
carried the riders between a pair of large, side-by-side wheels, and was claimed
capable of a speed of 116 mph (187 kph). Also in [72], there is a description
of the Dyno-Wheel, a concept having a bus-like chassis straddling a huge cen-
tral wheel. The relatively large diameter of a single-wheel vehicle enhances
its obstacle-crossing ability, smoothness of motion and rolling eﬃciency [12].
Further, a single-track vehicle can more easily ﬁnd obstacle-free paths on
the ground, and its narrow proﬁle can improve maneuverability. However,

1.1 Background
3
problems with steering, low-speed stability and aerodynamics, have kept such
vehicles from becoming commonplace.
1.1.2 Problems of Stable Robots
Traditional Research on mobile robotics has placed heavy emphasis on per-
ception, modeling of the environment and path planning. Consequently, ve-
hicles have been designed to be compatible with these planning limitations,
and the need for high speed has not been evident. Ultimately, as sensing and
computation capabilities improve, robots will be limited less by planning and
more by dynamic factors. Wheeled robots, capable of dynamic behavior - i.e.
high-speed motion on rough terrain - and of exploiting vehicle dynamics for
mobility, represent an exciting and largely unexplored area.
The purpose of our research is to exploit the natural steering behavior and
stability of the rolling wheel in the development of a highly dynamic, single
wheel mobile robot. We have built several prototypes of such a vehicle, and
demonstrated some of the potential capabilities.
1.1.3 Dynamically Stable Mobile Robots
Researchers have viewed mobile robots largely as quasi-static devices for
decades. Numerous robots with four, six or more wheels have been devel-
oped to maximize mobility on rough terrain. (See, for example, [13] [30] [45]
[52] [55].) Likewise, legged robots, which may have potentially greater mobil-
ity, have been built and demonstrated, as described in [60], [110]. Generally
these robots have featured low center-of-mass placement and broad base sup-
port, along with control and planning schemes designed to keep the center-
of-mass gravity vector within the support polygon (e.g. monitoring of slopes,
coordination of legs). Many designs have attempted to maximize mobility
with large wheels or legs, traction-enhancing tires or feet, multi-wheel driv-
ing, large body/ground clearance, articulated body conﬁgurations, etc. These
robots were often limited by motion-planning constraints and hence designed
for low-speed operation, typically 1 kph or less. Dynamic factors have little
inﬂuence on such systems, and consequently, have been largely ignored.
Traditional research on mobile robotics has placed heavy emphasis on per-
ception, modeling of the environment and path planning. Consequently, ve-
hicles have been designed to be compatible with these planning limitations,
and the need for high speed has not been evident. Ultimately, as sensing and
computation capabilities improve, robots will be limited less by planning and
more by dynamic factors. Wheeled robots, capable of dynamic behavior, i.e.
high-speed motion on rough terrain, and of exploiting vehicle dynamics for
mobility, represent an exciting and largely unexplored area.
A number of researchers have explored the possibilities of utilizing dynamic
behavior in various robot linkages, legged locomotors and other dynamic sys-
tems. Examples include Fukuda’s Brachiator [88] and Spong’s robot acrobat

4
1 Introduction
[100], both of which utilized dynamic swinging motions. Koditschek [22] and
Atkeson [92] and their students studied the dynamics of juggling. McGeer built
a robot that walked down slopes without additional power or control input,
utilizing the periodic swinging motion of the legs [70]. Raibert’s group built a
series of dynamically balanced, hopping and running robots that jumped ob-
stacles, climbed steps [44], and performed ﬂips. Arai [3] and Xu’s group [14]
developed an underactuated robot system by using the dynamic coupling be-
tween the passive and active joints. Dynamic motion in these systems allowed
the emergence of behaviors that would not have been possible quasi-statically.
In parallel with the work on linkage dynamics, other researchers have fo-
cused on the dynamics and balance of wheeled robots. Vos [109] developed
a self-contained unicycle that mimicked the behavior of a human cyclist in
maintaining roll and pitch stability. Koshiyama and Yamafuji [59] developed
a statically stable, single-wheel robot with an internal mechanism that could
could move fore and aft and turn in place; their work emphasized the control
of the (non-inverted) pendulum carried on the wheel, utilizing momentum
transfer in changing direction.
The invention of the wheel predates recorded history, but many interesting
wheeled vehicles have appeared within the pass few centuries. According to
the historian [79], the bicycle originated in the late 1700s as an unsteerable
vehicle known as the ”Hobby Horse”. In the early 1800s, steering was added
to the front wheel to facilitate changing vehicle direction; serendipitously, the
front-steering conﬁguration proved to be self-stabilizing. Prior to this develop-
ment, the thought of a vehicle traveling stably on two wheels would have been
considered ludicrous. In [82], Palmer describes several, single-wheel vehicles.
One, an 8-foot diameter, cage-like wheel was driven through foot treadles and
hand cranks by the driver inside. Presumably it was stabilized by gyroscopic
action, and steered by shifting the driver’s weight. A ”unicycle”, built around
1904, provided a seat for the driver inside a large, annular wheel, powered by
a gasoline engine.
The stabilizing eﬀect of gyroscopes has intrigued inventors for centuries,
and is central to a number of inventions and patents. Two interesting vehicles
appeared in a 1935 publication [47]. The ”Gyroauto” carried the driver and
a passenger between a pair of large, side-by-side wheels, and was claimed to
be capable of attaining a speed of 116 mph. The ”Dyno-Wheel” comprised a
bus-like chassis straddling a huge central wheel. Outrigger wheels apparently
were used to control steering, acceleration and braking. (It is doubtful that
the Dyno-Wheel was ever built.) In a 1968 patent [102], Summers proposed
gyroscopically stabilizing the roll axis of vehicles, enabling a narrow track for
passing through tight spaces (e.g. for logging equipment in forests). Several
inventions relate to placing stabilizing gyros on the front [42] or rear [79] wheel
of a bicycle. A 1933 patent [111] proposed a gyro stabilizer to reduce shimmy
on the front wheels of automobiles.
More recently, Koshiyama, et al and K. Yamafuji, et al [59] developed a
single-wheel robot with internal mechanism. The statically stable, spherical

1.2 Design
5
wheel can apparently turn in place through inertial eﬀects of the internal
mechanism, giving it good maneuverability. Much of the eﬀort is directed at
the control aspects for turning and generating forward motion. D.Vos and A.
Flotow [109] at MIT developed an autonomous control scheme for a unicycle
that is composed of a wheel, a frame, and turntable (inertia wheel). Two DC
motors drive the unicycle wheel for forward/reverse motion, and the turntable
for generation of yaw torques. The size and mass of the inertial drive mecha-
nism is large-2 to 3 times that of the wheel itself-limiting maneuverability and
the ability to recover from falls. Professor Yuta’s group [122] at University of
Tsukuba designed a unicycle that has an upper and lower part for steering,
and developed algorithms on navigation and control.
Pertinent to the issue of attitude disturbances on wheeled vehicles is the
work of Lindemann and Eisen at Jet Propulsion Laboratory [67]. They simu-
lated the dynamic behavior of conventional terrestrial construction equipment,
and pointed out the vulnerability of multi-wheeled vehicles to dynamic dis-
turbances. The related research work on multi-wheeled robots can be founded
extensively in papers published recently, for example, in [91] and [117].
1.2 Design
1.2.1 Concept and Compromise
Gyrover is a novel, single wheel gyroscopically stabilized robot, originally
developed at Carnegie Mellon University [21]. Figure 1 shows a schematic of
the mechanism design. Essentially, Gyrover is a sharp-edged wheel, with an
actuation mechanism ﬁtted inside the wheel. The actuation mechanism con-
sists of three separate actuators: (1)a spin motor, which spins a suspended
ﬂywheel at a high rate, imparting dynamic stability to the robot; (2)a title
motor, which controls the steering of Gyrover; and (3)a drive motor, which
causes forward and/or backward acceleration, by driving the single wheel di-
rectly.
The behavior of Gyrover is based on the principle of gyroscopic precession
as exhibited in the stability of a rolling wheel. Because of its angular momen-
tum, a spinning wheel tends to precess at right angles to an applied torque,
according to the fundamental equation of gyroscopic precession:
T = J × ω × Ω
(1.1)
where ω is the angular speed of the wheel, Ω is the wheel’s precession rate,
normal to the spin axis, J is the wheel polar moment of inertia about the
spin axis, and T is the applied torque, normal to the spin and precession
axes. Therefore, when a rolling wheel leans to one side, rather than just fall
over, the gravitationally induced torque causes the wheel to precess so that
it turns in the direction that it is leaning. Gyrover supplements this basic
concept with the addition of an internal gyroscope — the spinning ﬂywheel

6
1 Introduction
Tilt Motor
Rear  View
Side View
Forward
Axle
Tile Surface
Wheel
Structure
Main CPU
Board
and Gearing
Drive Motor
Gyro Rotor
Battery
Fig. 1.1. The basic conﬁguration of the robot
— nominally aligned with the wheel and spinning in the direction of forward
motion. The ﬂywheel’s angular momentum produces lateral stability when the
wheel is stopped or moving slowly.
Gyrover has a number of potential advantages over multi-wheeled vehicles:
1. The entire system can be enclosed within the wheel to provide mechanical
and environmental protection for equipment and mechanisms.
2. Gyrover is resistant to getting stuck on obstacles because it has no body
to hang up, no exposed appendages, and the entire exposed surface is live
(driven).
3. The tiltable ﬂywheel can be used to right the vehicle from its statically
stable, rest position (on its side). The wheel has no “backside” on which
to get stuck.
4. Without special steering mechanism, Gyrover can turn in place by simply
leaning and precessing in the desired direction for enhancing maneuver-
ability.
5. Single-point contact with the ground eliminates the need to accommodate
uneven surfaces and simpliﬁes control.
6. Full drive traction is available because all the weight is on the single drive
wheel.
7. A large pneumatic tire may have very low ground-contact pressure, result-
ing in minimal disturbance to the surface and minimum rolling resistance.
The tire may be suitable for traveling on soft soils, sand, snow or ice; riding
over brush or other vegetation; or, with adequate buoyancy, for traveling
on water.
Potential applications for Gyrover are numerous. Because it can travel on
both land and water, it may ﬁnd amphibious use on beaches or swampy areas,
for general transportation, exploration, rescue or recreation. Similarly, with
appropriate tread,it should travel well over soft snow with good traction and

1.2 Design
7
minimal rolling resistance. As a surveillance robot, Gyrover could use its slim
proﬁle to pass through doorways and narrow passages, and its ability to turn
in place to maneuver in tight quarters. Another potential application is as a
high-speed lunar vehicle, where the absence of aerodynamic disturbances and
low gravity would permit eﬃcient, high-speed mobility. As its development
progresses, we anticipate that other, more speciﬁc uses will become evident.
1.2.2 Mechanism Design
We have studied the feasibility through basic analysis and simple experiments,
and designed and built two, radio-controlled (RC) working models. These
have proven the concept workable, and have veriﬁed many of the expected
advantages.
Given a basic understanding of the gyroscopic principle and wheel dynamic
stability, our ﬁrst task was to ﬁnd a mechanism for steering the wheel along
a desired path. Turning (steering) of the wheel is the result of gyroscopic
precession about the yaw axis, caused by roll torques as explained above. We
considered two mechanisms for producing this torque: lateral shifting of weight
within the vehicle; and leaning of the wheel. With regard to the ﬁrst approach,
the need to provide adequate internal space for shifting large masses within
the vehicle is a signiﬁcant drawback. Moving the mass outside the wheel’s
envelope is unappealing because of the eﬀective broadening of the vehicle
and potential for the movable mass to contact the ground or other obstacles.
Allowing the entire wheel to lean employs the complete weight of the wheel
to shift laterally – not just a relatively small, movable mass – to generate the
needed roll torque. Leaning of the wheel, can be eﬀected by the use of internal
reaction wheels or masses, but these tend to acquire kinetic energy, become
velocity-saturated, and generate angular momentum that can corrupt vehicle
behavior. Another way is to react against an internal gyroscope as described
above; this mechanism has been implemented and operated successfully.
Several alternative conﬁgurations were considered. A spherical shape,
which can be statically stable, does not exhibit the natural steering behavior
resulting from the interaction of gravitational (overturning) torque and the
gyroscopic eﬀect. In fact, a narrow tire-contact area is desirable for steering
responsiveness (dependent on the gravitational torque for a given lean angle).
Two wheels, side-by-side provide static stability, but are sensitive to roll dis-
turbances, do not exhibit the same natural steering behavior, and will not roll
stably on two wheels above a critical speed. (Observe the behavior of a short
cylinder, such as a roll of tape, rolled along the ﬂoor.) Outboard wheels, either
on the sides or front/back, were considered for static stability and steering ef-
fects, as well as acceleration and braking enhancement; but in addition to
mechanical complexity, these defeat the basic elegance and control simplicity
of the concept. Actually, the robot has the potential to be statically stable to
provide a solid base of support for sensors, instruments or small manipulators,

8
1 Introduction
etc., simply by resting on its side. The present concept, totally enclosed in the
single wheel, provides a simple, reliable, rugged vehicle.
Experimental work to date includes several simple experiments to verify
the stability and steering principle, and three working vehicles.
1.2.3 Sensors and Onboard Computer
The latest model we are using currently is Gyrover III. It is built with a light-
weight bicycle tire and rim and a set of transparent domes. It includes a radio
system for remote control, on-board computer and a number of sensors to
permit data-logging and on-board control of the machine’s motion.
There are 3 actuating mechanisms in Gyrover: (i) Gyro tilt servo, (ii) Drive
motor, and (iii) DC gyro motor. Table 1.1 gives a detailed description of each
of them.
Actuator
Symbol
Descriptions
Gyro tilt servo
u0
The tilt servo controls the relative angle of the gyro spin
axis with respect to the wheel axis. In fact, by controlling
the tilt servo, we are able to controls the lean angle angle
of the robot indirectly.
Drive motor
u1
The robot forward/backward drive system uses a 2-stage,
tooth belt system to amplify the torque from the drive
motor.
DC gyro motor
u2
This motor cause the internal gyro to spin at a dersirable
operating speed, increase the angular momentum of the
gyro.
Table 1.1. Table of diﬀerent actuating mechanisms in Gyrover.
A number of on-board sensors have been installed on Gyrover to provide
information about the states of the machine to the on-board computer. The
information includes:
•
Gyro tilt angle, βa
•
The servo current
•
Drive motor current
•
Drive motor speed
•
Gyro speed, ˙γa
•
Angular rate (3-axes: Roll-Pitch-Yaw), ˙β, ˙γ and ˙α
•
Accleration (3-axes: Roll-Pitch-Yaw), ¨β, ¨γ and ¨α
•
Robot tilt angle (Roll), β
All these signals, plus the control inputs from the radio transmitter, can
be read by the computer. A custom-built circuit board contains the control
computer and ﬂashdisk, interface circuitry for the radio system and servos,

1.2 Design
9
Fig. 1.2. Communication equipment: radio transmitter (left) and
laptops with wireless Modem (right).
Gyro
Accelerometer
Sensors Assembly
Radio Receiver
I/O Board
Power Cable
Tilt Servo
Battery
Drive Motor and Encoder
Gyro Tilt Potentiometer
Speed Controller
Speed Controller
Fig. 1.3. Hardware conﬁguration of the robot.
components and logic to control power for the actuators, and an interface for
the on-board sensors. The on-board processing is performed by a 486 Cardio
PC.
In addition, several more sensors are planned to be incorporated with our
control algorithms in the near future. Visual processing capability or a Global
Positioning System (GPS) is a big issue for autonomous control, however, due
to the structural limitation of the robot, we have not equipped the robot with
this kind of device yet.
An on-board 100-MHZ 486 computer was installed in the robot to deal
with on-board sensing and control. A ﬂash PCMCIA card is used as the hard
disk of the computer. It communicates with a stationary PC via a pair of
wireless modems. Based on this communication system, we can download the
sensor data ﬁle from the on-board computer, send supervising commands to
the robot, and manually control the robot through the stationary PC. More-

10
1 Introduction
over, another radio transmitter is installed for human operators to remotely
control the robot via two joysticks of the transmitter(Fig 1.2). One uses the
transmitter to control the drive speed and tilt angle of the robot, hence, we
can record the operator’s driving data.
Numerous sensors are installed in the robot to measure the state vari-
ables(Fig 1.3). Two pulse encoders were installed to measure the spinning
rate of ﬂywheel and the wheel. Furthermore, we have two gyros and an ac-
celerometer to detect the angular velocity of yaw, pitch, roll, and acceleration
respectively. A 2-axis tilt sensor is developed and installed for direct measur-
ing the lean angle and pitch angle of the robot. A gyro tilt potentiometer is
used to calulate the tilt angle of the ﬂywheel and it’s rate change.
The onboard computer is run on an OS, called QNX, which is a real-
time microkernel OS developed by QNX Software System Limited. Because
of handling numerous sensors and communicating with the stationary PC, the
robot’s software system is divided into three main programs: (1)communica-
tion server, (2)sensor server and (3) controller. The communication server is
used to communicate between the onboard computer and the stationary lap-
top computer via RS232, while a sensor server is used to handle all the sensors
and actuators. The controller program implements the control algorithm and
communicates among these servers. All these programs are run independently
in order to allow real-time control of the robot.
1.2.4 Implementation
The ﬁrst vehicle, Gyrover I, shown in Figure 1.4, was assembled from RC
model airplane/car components, and quickly conﬁrmed the concept. The ve-
hicle has a diameter of 29 cm and mass of 2.0 kg. It can be easily driven and
Fig. 1.4. The ﬁrst prototype of the robot

1.2 Design
11
steered by remote control; has good high-speed stability on smooth or rough
terrain; and can be kept standing in place. This vehicle has traveled at over
10 kph, negotiated relatively rough terrain (a small gravel pile), and traversed
a 45-degree ramp 75% its height diameter. Recovery from falls (resting on
the round side of the wheel) has been achieved with a strategy using both the
wheel forward drive and gyro-tilt control. The main shortcomings of this robot
are its lack of resilience and vulnerability to wheel damage; excessive battery
drain due to drag on the gyro (bearing and aerodynamics); inadequate torque
in the tilt servo; and incomplete enclosure of the wheel.
Fig. 1.5. The second prototype of the robot
The second vehicle, Gyrover II, (Figure 1.5) was designed to address these
problems. It is slightly larger than Gyrover I (34 cm diameter, 2.0 kg), and
also utilizes many RC model parts. Tilt-servo torque and travel were both
approximately doubled. Gyrover II uses a gyro housed in a vacuum chamber
to cut power consumption by 80%, which increases battery life from about
10 minutes to 50 minutes. The entire robot is housed inside a specially de-
signed pneumatic tire which protects the mechanism from mechanical and
environmental abuse, and provides an enclosure that is resilient, although less
rugged than hoped. The robot contains a variety of sensors to monitor motor
currents, positions and speeds, tire and vacuum pressure, wheel/body orien-
tation, and gyro temperature. Gyrover II has been assembled and driven by
manual remote control on a smooth ﬂoor, and has shown the ability to ﬂoat
and be controllable on water.
The third version, Gyrover III, (Figure 1.6) was designed on a larger scale
to permit it to carry numerous inertial sensors and a computer (486 PC) for
data acquisition and/or control. This machine utilizes a lightweight, 40 cm
bicycle tire and rim, and a pair of transparent domes attached to the axle.
Overall weight is about 7 kg. Heavy-duty R/C motors and servo are used to
spin the gyro, drive the wheel forward and control gyro tilt. Gyrover III trav-

12
1 Introduction
Fig. 1.6. The third prototype of the robot
els up to 10mph, recovers from falls, and runs about 25 minutes per charge
of its NiCad batteries. It can carry a video camera that looks through the
transparent wheel, and transmit video data to a remote receiver/monitor.

2
Kinematics and Dynamics
2.1 Modeling in a Horizontal Plane
2.1.1 Kinematic Constraints
Previously, Gyrover was controlled only manually, using two joysticks to
control the drive and tilt motors through a radio link. A complete dynamic
model is necessary to develop automatic control of the system. In the following
sections, we will develop the nonholonomic kinematics constraints, as well as
a dynamic model using the constrained generalized Lagrangian formulation.
X
Y
Z
O
A
C
W
I
R
D
E
mg
l1
l2
θ
˙β
˙α
α
β
βa
˙γ
˙γa
yB
xB
zB
xa
ya
za
xc
yc
Xdirection
Fig. 2.1. Deﬁnition of coordinate frames and system variables
Y. Xu and Y. Ou: Control of Single Wheel Robots, STAR 20, pp. 13–32, 2005.
© Springer-Verlag Berlin Heidelberg 2005

14
2 Kinematics and Dynamics
Coordinate frame
In deriving the equations of motion of the robot, we assume that the wheel
is a rigid, homogeneous disk which rolls over a perfectly ﬂat surface without
slipping. We model the actuation mechanism, suspended from the wheel bear-
ing, as a two-link manipulator, with a spinning disk attached at the end of the
second link (Figure 2.1). The ﬁrst link of length l1 represents the vertical oﬀset
of the actuation mechanism from the axis of the Gyrover wheel. The second
link of length l2 represents the horizontal oﬀset of the spinning ﬂywheel and
is relatively smaller compared to the vertical oﬀset.
α, αa
Precession angles of the wheel and for the ﬂywheel,
respectively, measured about the vertical axis
β
Lean angles of the wheel
βa
Tilt angle between the link l1 and za-axis of the ﬂy-
wheel
γ, γa
Spin angles of the wheel and the ﬂywheel, respectively
θ
Angle between link l1 and xB-axis of the wheel
mw, mi, mf
Mass of the wheel, mass of the internal mechanism and
mass of the ﬂywheel respectively
m
Total mass of the robot
R, r
Radius of the wheel and the ﬂywheel respectively
Ixxw, Iyyw, Izzw Moment of inertia of the wheel about x, y and z axes
Ixxf, Iyyf, Izzf
Moment of inertia of the ﬂywheel about x, y and z
axes
µs, µg
Friction coeﬃcient in yaw and pitch directions, respec-
tively
u1, u2
Drive torque of the drive motor and tilt torque of the
tilt motor, respectively
Table 2.1. Variables deﬁnitions
Next, we assign four coordinates frames as follows: (1) the inertial frame
%
O, whose x −y plane is anchored to the ﬂat surface, (2) the body coor-
dinate frame %
B {xB, yB, zB}, whose origin is located at the center of the
single wheel, and whose z-axis represents the axis of rotation of the wheel,
(3) the coordinate frame of internal mechanism %
C {xc, yc, zc}, whose cen-
ter is located at point D, and whose z-axis is always parallel to zB, and (4)
the ﬂywheel coordinates frame %
E {xa, ya, za}, whose center is located at
the center of the Gyrover ﬂywheel, and whose z-axis represents the axis of
rotation of the ﬂywheel. Note that ya is always parallel to yc. The deﬁnition
and conﬁguration of system and variables are shown in Table 2.1 and Figure
2.1. Rolling without slipping is a typical example of a nonholonomic system,
since in most cases, some of the constrained equations for the system are non-
integrable. Gyrover is a similar type of nonholonomic system. Here we ﬁrst

2.1 Modeling in a Horizontal Plane
15
derive the constraints of the single wheel, and then derive the dynamic model
of Gyrover based on these constraints. We deﬁne (i, j, k) and (l, m, n) to be
the unit vectors of the coordinate system XY ZO(%
O) and xByBzBA(%
B),
respectively. Let Sx := sin(x) and Cx := cos(x). The transformation between
these two coordinate frames is given by


i
j
k

= RO
B


l
m
n


(2.1)
where RO
B is the rotation matrix from %
O to %
B.
RO
B =


−SαCβ −Cα −SαSβ
CαCβ −Sα CαSβ
−Sβ
0
Cβ


(2.2)
Let vA and ωB denote the velocity of the center of mass of the single wheel
and its angular velocity with respect to the inertia frame %
O. Then, we have
ωB = −˙αSβl + ˙βm + (˙γ + ˙αCβ)n
(2.3)
The constraints require that the disk rolls without slipping on the horizontal
plane, i.e. the velocity of the contact point on the disk is zero at any instant
vc = 0,
(2.4)
where vc is the velocity of contact point of the single wheel. Now, we can
express vA as
vA = ωB × rAC + vc
(2.5)
where rAC = −Rl representing the vector from the frame C to A in Figure 5.
Substituting Eqs. (2.3) and (2.4) in Eq. (2.5) gives
vA = ˙Xi + ˙Y j + ˙Zk,
(2.6)
where
˙X = R(˙γCα + ˙αCαCβ −˙βSαSβ)
(2.7)
˙Y = R(˙γSα + ˙αCβSα + ˙βCαSβ)
(2.8)
˙Z = R ˙βCβ
(2.9)
Eqs. (2.7) and (2.8) are nonintegrable and hence are nonholonomic while Eq.
(2.9) is integrable, i.e,
Z = RSβ.
(2.10)
Therefore, the robot can be represented by seven (e.g. X, Y, α, β, γ, βa, θ),
instead of eight, independent variables.

16
2 Kinematics and Dynamics
2.1.2 Equations of Motion
In this section, we study the equation of motion by calculating the La-
grangian L = T −P of the system, where T and P are the kinetic energy and
potential energy of the system respectively. We divide the system into three
parts: 1) single wheel, 2) internal mechanism, and 3) spinning ﬂywheel.
Single wheel
The kinetic energy of the single wheel is given by,
Tw = 1
2mw

˙X2 + ˙Y 2 + ˙Z2
+1
2
	
Ixxwω2
x + Iyywω2
y + Izzwω2
z

(2.11)
Substituting Eqs.(2.3) and (2.9) in Eq.(2.11) yields
Tw
= 1
2mw

˙X2 + ˙Y 2 + (R ˙βCβ)2
+ 1
2
	
Ixxw( ˙αSβ)2
+ Iyyw ˙β2 + Izzw( ˙αCβ + ˙γ)2
(2.12)
The potential energy of the single wheel is
Pw = mwgRSβ
(2.13)
Internal mechanism and spinning ﬂywheel
We need to compute the translational and rotational parts of kinetic
energy for the internal mechanism and ﬂywheel respectively. We assume that
l2 is very small compared with l1,
l2  0
(2.14)
Thus, the ﬂywheel’s center of mass (E) coincides with that of the internal
mechanism (D).
Let {xf, yf, zf} be the center of mass of the internal mechanism and the
ﬂywheel w.r.t. %
O. The transformation from the center of mass of single
wheel to the ﬂywheel can be described by:


xf
yf
zf

=


X
Y
Z

+ RO
B


l1Cθ
l1Sθ
0


(2.15)
Let T t
f denote the translational kinetic energy of the ﬂywheel and the internal
mechanism.
T t
f = 1
2(mi + mf)[ ˙x2
f + ˙y2
f + ˙z2
f]
(2.16)

2.1 Modeling in a Horizontal Plane
17
Diﬀerentiating Eq. (2.15) and substituting it in Eq. (2.16), we obtain T t
f. We
observed that the internal mechanism swings slowly, so it should not con-
tribute highly to the rotational kinetic energy. Let ωf be the angular velocity
of ﬂywheel w.r.t. %
O. We then have
ωf = RE
BωB +


0
˙βa
˙γa


(2.17)
where RE
B is the transformation from %
B to %
E.
RE
B =


CθSβa −SθSβa Cβa
Sθ
Cθ
0
CθCβa −CβaSθ Sβa


(2.18)
The rotational kinetic energy of the ﬂywheel is now given by,
T r
f = 1
2
	
(ωfx)2Ixxf + (ωfy)2Iyyf + (ωfz)2Izzf

(2.19)
The ﬂywheel is assumed to be a uniform disk and the principle moments of
inertia are Ixxf = Iyyf = 1
4mfr2, Izzf = 1
2mfr2. The potential energy of the
ﬂywheel and internal mechanism is
Pf = (mi + mf)(RSβ −l1CθSβ)
(2.20)
Lagrangian of the system
The Lagrangian of the system thus is
L =
	
Tw + (T t
f + T r
f )

−(Pw + Pf)
(2.21)
Substituting Eqs. (2.11), (2.16), (2.19), (2.13) and (2.20) in Eq. (2.21), we
may determine L. There are only two control torques available on the system.
One is drive torque (u1) and the other is the tilt torque (u2). Consequently,
using the constrained Lagrangian method, the dynamic equation of the entire
system is given by
M(q)¨q + N(q, ˙q) = AT λ + Bu
(2.22)
where M(q) ∈R7×7 and N(q, ˙q) ∈R7×1 are the inertia matrix and nonlinear
terms respectively.
A(q) =
! 1 0 −RCαCβ
RSαSβ
−RCα 0 0
0 1 −RCβSα −RCαSβ −RSα 0 0
"
(2.23)
q =
#
'
'
'
'
'
'
'
'
%
X
Y
α
β
γ
βa
θ
$
(
(
(
(
(
(
(
(
&
, λ =
! λ1
λ2
"
, B =
#
'
'
'
'
'
'
'
'
%
0 0
0 0
0 0
0 0
k1 0
0 1
k2 0
$
(
(
(
(
(
(
(
(
&
, u =
! u1
u2
"

18
2 Kinematics and Dynamics
The nonholonomic constraints can be written as,
A( ˙q) ˙q = 0.
(2.24)
It is noted that all elements of the last two columns of the matrix A are
zero, because the nonholonomic constraints only restrict the motion of the
single wheel, not the ﬂywheel. The last two columns represent the motion
variables of the ﬂywheel. Moreover, matrix B only has three rows with non-
zero elements since the input torques only drive the tilt angle of the ﬂywheel
(βa) and the rotating angle of the single wheel (γ), so that the ﬁfth and
the sixth rows of B are non-zero as they represent the tilting motion of the
ﬂywheel and the rotating motion of the single wheel respectively. Furthermore,
when the single wheel rotates, the pendulum motion of internal mechanism
is introduced, thus θ changes. Therefore, the drive torque of the single wheel
will also aﬀect the pendulum motion of the internal mechanism (θ), so that
the seventh row of matrix B is not zero.
Normal form of the system
In this section, we will eliminate the Lagrange multipliers so that a min-
imum set of diﬀerential equations is obtained by the similar way in [18]. We
ﬁrst partition the matrix A(q) into A1 and A2 where A = [A1 : A2]
A1 =
! 1 0
0 1
"
, A2 =
! −RCαCβ
RSαSβ
−RCα 0 0
−RCβSα −RCαSβ −RSα 0 0
"
Let
C(q) =

−A−1
1 A2
I3×3

(2.25)
Then consider the following relationship
˙q = C(q) ˙q2
(2.26)
where q1 = [X, Y ]T and q2 = [α, β, γ, βa, θ]T . Diﬀerentiating Eq. (2.26) yields
¨q = C(q)¨q2 + ˙C(q) ˙q2
(2.27)
Substituting Eq. (2.27) into Eq. (2.37) and premultiplying both sides by CT (q)
gives
CT (q)M(q)C(q)¨q2
= CT (q)

Bu −N(q, C(q) ˙q2) −M(q) ˙C(q) ˙q2

(2.28)
where CT (q)M(q)C(q) is 5×5 a symmetrical positive deﬁnite matrix function.
Note that Eq.(2.28) depends only on (α, β, γ, βa, θ). By numerical integration,
we can obtain q2 from ¨q2 in Eq. (2.28), and then obtain (X, Y ) by substituting
q2 and ˙q2 in Eq. (2.26).

2.1 Modeling in a Horizontal Plane
19
2.1.3 Dynamic Properties
Understanding the characteristics of the robot dynamics is of signiﬁcance in
the control of the system. To this end, we ﬁrst simplify the model. Practically,
we may assume Ixxw = Iyyw =
1
2mwR2, Izzw = mwR2, and l1 and l2 are
zero, thus the mass center of the ﬂywheel is coincident with the center of the
robot. For steady motion of the robot, the pendulum motion of the internal
mechanism is suﬃciently small to be ignored, thus θ is set to be zero. The
spinning rate of the ﬂywheel γa is set to be constant. Let Sβ,βa := sin(β +
βa), Cβ,βa := cos(β +βa), and S2ββa := sin[2(β +βa)]. Based on the previous
derivation, the normal form of the dynamics model is
M(q)¨q = F(q, ˙q) + Bu
(2.29)
˙X = R(˙γCα + ˙αCαCβ −˙βSαSβ)
(2.30)
˙Y = R(˙γSα + ˙αCβSα + ˙βCαSβ)
(2.31)
where q = [α, β, γ, βa]T ,
M =


M11
0
M13
0
0
Ixxf + Ixxw + mR2
0
Ixxf
M13
0
2Ixxw + mR2
0
0
Ixxf
0
Ixxf

,
F = [F1, F2, F3, F4]T ,
B =

0 0 1 0
0 0 0 1
T
, u =

u1
u2

M11 = Ixxf + Ixxw + IxxwC2
β + mR2C2
β + IxxfC2
β,βa
M13 = 2IxxwCβ + mR2Cβ
F1 = (Ixxw + mR2)S2β ˙α ˙β + IxxfS2ββa ˙α ˙β + IxxfS2ββa ˙α ˙βa
+2IxxwSβ ˙β ˙γ + 2IxxfSβ,βa ˙β ˙γa + 2IxfSβ,βa ˙βa ˙γa −µs ˙α
F2 = −gmRCβ −(Ixxw + mR2)CβSβ ˙α2 −IxxfCβ,βaSβ,βa ˙α2
−(2Ixxw + mR2)Sβ ˙α ˙γ −2IxxfSβ,βa ˙α ˙γa
F3 = 2(Ixxw + mR2)Sβ ˙α ˙β
F4 = −IxxfCβ,βaSβ,βa ˙α2 −2IxxfSβ,βa ˙α ˙γa
where (X, Y, Z) is the coordinates of the center of mass of the single wheel
with respect to the inertial frame as shown in Figure 2.1. M(q) ∈R4×4 and
N(q, ˙q) ∈R4×1 are the inertial matrix and nonlinear term respectively. Eqs.

20
2 Kinematics and Dynamics
constraints of the robot.
We further simplify the model by decoupling the tilting variable βa from
Eq. (2.29). Practically, βa is directly controlled by the tilt motor (position
control), assuming that the tilt actuator has an adequate torque to track the
desired βa(t) trajectory exactly. Therefore, βa can be decoupled from Eq.
(2.29). It is similar to the case of decoupling the steering variable from the
bicycle dynamics shown in [16],[35].
As we consider
˙βa as a new input uβa, the dynamics model Eq. (2.29)
becomes
˙βa = uβa
˜
M(˜q)¨˜q = ˜F(˜q, ˙˜q) + ˜B˜u.
(2.32)
with ˜q = [α, β, γ]T ,
˜
M =


M11
0
M13
0
Ixxf + Ixxw + mR2
0
M13
0
2Ixxw + mR2


˜F =

˜F1, ˜F2, ˜F3
T
˜B =

 0
0 1
˜B12 0 0
T
, ˜u =

u1
uβa

˜F1 = (Ixxw + mR2)S2β ˙α ˙β + IxxfS2ββa ˙α ˙β
−2IxxwSβ ˙β ˙γ + 2IxxfSβ,βa ˙β ˙γa −µs ˙α,
˜F2 = F2,
˜F3 = F3,
˜B12 = IxxfS2ββa ˙α + 2IxxfSβ,βa ˙γa,
Eq. (2.32) shows the reduced dynamic model of the single wheel robot af-
ter decoupling the tilting varible βa, with new matrices
˜
M(˜q) ∈R3×3 and
˜F(˜q, ˙˜q) ∈R3×1. The realistic geometric/mass parameters are shown in Table
2.2. It is noted that if the lean angle β is set to be 0o or 180o, meaning that
the single wheel robot falls on the ground, its inertial matrix ˜
M will become
singular because it violates the assumption of rolling without slipping.
The robot dynamic model is highly coupled, nonholonomic and underac-
tuacted. Because the major part of the robot is a rolling wheel, therefore it
processes the typical characteristics of a rolling disk. For a rolling disk, it
does not fall when it is rolling, because there is a gyroscopic torque, resulting
from the coupling motion between the roll and yaw motions, for balancing the
gravitational torque. However, its rolling rate must be high enough to pro-
vide a suﬃcient gyroscopic torque for balancing the disk. When we installed
(2.29) and (2.30), (2.31) form the dynamics model and nonholonomic velocity

2.1 Modeling in a Horizontal Plane
21
Single wheel parameters: m = 1.25kg, R = 17cm
Internal mechanism :
mi = 4.4kg,
Flywheel parameters :
mf = 2.4kg, r = 5cm
Friction coeﬃcients :
µs = 1Nm/(rad/s), µg = 0.01Nm/(rad/s)
Table 2.2. Parameters used in simulation and experiments
a ﬂywheel on the wheel, the robot’s gyroscopic torque is greater and depends
less on its rolling speed ˙γ, owing to the high spinning ﬂywheel, the lean angle
of the robot tends to remain unchanged. We can explain this characteristics
based on the equilibrium solution of Eq. (2.32). For the low yaw rate and
¨α = ¨β = ¨γ = 0, ˙β = 0, u1 = uβa = 0, Eq. (2.32) becomes
0 = gmRCβ + (2Ixxw + mR2)Sβ ˙α ˙γ + 2IxxfSβ,βa ˙α ˙γa,
(2.33)
From Eq. (2.33), the terms ˙γ ˙α and ˙γa ˙α are used to cancel the gravitational
torques gmRCβ, in order to stabilize the roll component of the system. Be-
cause the spinning rate ˙γa is very high, the term 2IxxfSβ,βa ˙α ˙γa is signiﬁcantly
large, in order to cancel the gravitational torque, even though the rolling speed
˙γ is low. Thus, it will achieve an equilibrium steering rate ˙αs,
˙αs =
−gmRCβs
2IxxfSβs,βa ˙γa + (2Ixxw + mR2)Sβs ˙γ
(2.34)
for a speciﬁc lean angle βs.
Figures 2.2 and 2.3 above show the simulation results of a rolling disk
without the ﬂywheel, and that of the single wheel robot, respectively, under
the same initial conditions



β = 70o, βa = 0o,
˙β = ˙α = ˙βa = 0 rad/s, ˙γ = 15 rad/s,
α = 0o.
Note that the lean angle β of a rolling disk without the ﬂywheel decreases
much rapidly than that of the single wheel robot as shown in Figures 2.2(b)
and 2.3(b). This veriﬁes the stabilizing eﬀect of the ﬂywheel on the single
wheel robot. In Figures 2.3(a),(c), under the inﬂuence of friction in the yaw
direction, the steering rate ˙α and the leaning rate ˙β will converge to a steady
state solution as shown in Eq. (2.34). Otherwise, an unwanted high frequency
oscillation will occur. Practically, it will not produce a signiﬁcant eﬀect on the
system because the frequency of the above oscillation is much greater than
the system response. If the rolling rate is reduced, the rolling disk will fall
much more quickly than in the previous case.
2.1.4 Simulation Study
Up to now, we only consider the case when the ﬂywheel’s orientation is ﬁxed
with respect to the single wheel. Here, we will focus on the tilting eﬀect of the

22
2 Kinematics and Dynamics
0
1
2
3
4
5
−1
0
1
2
3
4
5
(a)
0
1
2
3
4
5
35
40
45
50
55
60
65
70
(b)
0
1
2
3
4
5
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
(c)
−4
−2
0
2
−4
−3
−2
−1
0
(d)
˙β[rad/s]
β[deg]
˙α[rad/s]
x[m]
y[m]
t[sec]
t[sec]
t[sec]
Fig. 2.2.
The simulation results of a
rolling disk without the ﬂywheel.
0
1
2
3
4
5
−2
−1.5
−1
−0.5
0
(a)
0
1
2
3
4
5
69
69.2
69.4
69.6
69.8
70
(b)
0
1
2
3
4
5
−0.2
−0.15
−0.1
−0.05
0
0.05
0.1
0.15
(c)
−4
−2
0
2
4
−8
−6
−4
−2
0
(d)
˙β[rad/s]
β[deg]
˙α[rad/s]
x[m]
y[m]
t[sec]
t[sec]
t[sec]
Fig. 2.3.
The simulation results of the
single wheel robot.
ﬂywheel on the robot. Based on the conservation of angular momemtum, when
there is a change in the tilt angle of the ﬂywheel βa, the whole robot will rotate
in the opposite direction in order to maintain constant angular momentum. It
implies that we may control the lean angle of the robot for steering. Simulation
and experiment results are shown in Figure 2.4 and Figure 2.5, respectively,
under the same initial conditions



β = 90o, βa = α = 0o,
˙β = ˙α = ˙βa = 0 rad/s, ˙γ = 15 rad/s,
α = 0o.
Both Figures 2.4 and 2.5 show that if the tilt angle βa rotates in 73 deg/s
anticlockwise at t = 2.4 seconds, the lean angle β rotates in a clockwise di-
rection. In the experiment, the transient response of β is more critical than
the simulations. For 2.7 seconds, the tilt angle remains unchanged and then
β and steering rate ˙α converge to a steady position in both simulations and
experiments. In the experiment, we have not found any high frequency oscil-
lations perhaps because the sensor response and the sampling time are much
slower than the high frequency oscillations.
2.2 Modeling on an Incline
2.2.1 Motion on an Incline
Dynamics of a rolling disk
Consider a disk rolls without slipping along an inclined plane with an inclina-
tion angle ϕ. Assume that the disk is rigid with radius R. The dynamic model
can be developed using the constrained Lagrangian method. It is similar to

2.2 Modeling on an Incline
23
0
2
4
6
0
5
10
15
20
25
(a)
0
2
4
6
−150
−100
−50
0
50
100
(b)
0
2
4
6
−300
−200
−100
0
100
200
300
(c)
0
2
4
6
65
70
75
80
85
90
95
(d)
˙β[deg/s]
β[deg]
˙α[deg/s]
βa[deg]
t[sec]
t[sec]
t[sec]
t[sec]
Fig. 2.4. The simulation results of tilt-
ing the ﬂywheel of the robot with ˙βa =
73 deg/s
0
2
4
6
0
5
10
15
20
25
30
a
1
2
3
4
5
6
−200
−150
−100
−50
0
50
(b)
0
2
4
6
−150
−100
−50
0
50
(c)
0
2
4
6
60
70
80
90
100
110
120
(d)
˙β[deg/s]
β[deg]
˙α[deg/s]
βa[deg]
t[sec]
t[sec]
t[sec]
t[sec]
Fig. 2.5.
The experimental results of
tilting the ﬂywheel of the robot with ˙βa =
73 deg/s
the derivation in [115] & [87], except for some components due to the gravity
act on the system. Two nonholonomic velocity constraints are
˙X =
−R
!
˙γSα + ˙αSβCα −˙βCαSβ
"
(2.35)
˙Y
=
−R
!
˙γCα + ˙αCβCα + ˙βSαSβ
"
(2.36)
The rotational kinetic energy and the potential energy of the disk are given
below,
K = 1
2
#
m ˙X2 + m ˙Y 2 + mR2 ˙β2 cos2 β
$
+ 1
2Ix ˙α2 sin2 β
+1
2Ix ˙β2 + Ix
!
˙α cos β + ˙γ2
"
P = mg(R sin β cos ϕ + y sin ϕ)
Using the Lagrange undetermined multiplier λ method, the equations of
motion of a rolling disk can be determined below
m ¨
X = λ1
m ¨Y + mg sin ϕ = λ2
λ1Rsα + λ2Rcα = 2Ix
!
¨αcβ + ¨γ −˙α ˙γsβ
"
λ1Rsαcβ + λ2rcαcβ = Ix
!
¨α(1 + c2α) −2 ˙α ˙βsβcβ
+2¨γcβ −2 ˙β ˙γsβ
"
C −λ1Rcαsβ + λ2Rsαsβ =
!
mr2c2α + Ix
"¨β −mR2 ˙β2cβ
+mgRcβ + I ˙α2cβsβ + 2Ix ˙α ˙γsβ

24
2 Kinematics and Dynamics
where c = cos and s = sin. We eliminate the Lagrange multipliers by obtain-
ing ¨X, ¨Y . A normal form of the dynamic equation of a rolling disk is given
by
M(q)¨q + N(q, ˙q) = 0
where M(q) ∈R3×3 and N(q, ˙q) ∈R3×1 are the inertia matrix and nonlinear
terms respectively.
M =
#
%
Ix + (mR2 + Ix)c2β
0
(mR2 + 2Ix)cβ
0
mR2 + Ix
0
(mR2 + 2Ix)cβ
0
(mR2 + 2Ix)
$
&
N = [N1, N2, N3]T ,
q = [α, β, γ]T
N1 = −mgRcαcβsϕ −(Ix + mR2)s(2β) ˙α ˙β −2Ixsβ ˙β ˙γ
N2 = mgRcβcϕ −gmRsαsβsϕ + (mr2 + Ix)cβsβ ˙α2
+(mR2 + Ix)sβ ˙α ˙γ
N3 = −2(mR2 + Ix)sβ ˙α ˙β
Dynamics of a single wheel robot
The motion of a single wheel robot on an inclined plane diﬀers signiﬁcantly
from that on horizontal planes [21],[115]&[112]. On an incline, some compo-
nents of the gravitational forces act on the system. The nature of nonholo-
nomic constraints and the equation of motion can be derived as below. We
assume that the robot is a homogeneous, rigid disk which rolls over a suﬃ-
ciently rough slope without slipping. The high spinning ﬂywheel is anchored
on the centre of mass of the entire wheel. The dynamic equation of the entire
system is given by
M6(q)¨q + N6(q, ˙q) = AT λ + Bu
(2.37)
where M6(q) ∈R6×6 and N6(q, ˙q) ∈R6×1 are the inertia matrix and nonlinear
terms respectively.
A(q) =
! 1 0 −RSαCβ −RCαSβ −RSα 0
0 1 RCβCα −RSαSβ
RCα 0
"
B =
! 0 0 0 0 1 0
0 0 0 0 0 1
"T
, u = [u1, u2]T
q = [X, Y, α, β, γ, βa]T , λ = [λ1, λ2]T

2.2 Modeling on an Incline
25
The nonholonomic constraints of a rolling disk and a single wheel robot on
the inclined plane are identiﬁcal in Equations 2.35 & 2.36. The nonholonomic
constraints can be written as,
A(q) ˙q = 0
(2.38)
A minimum set of diﬀerential equations (Normal form) is obtained when the
Lagrange multipliers are eliminated. This model (2.39) is a nonholonomic and
underactuated model. The model of a single wheel robot on the ground [112],
assumed that the climbing angle ϕ = 0, is a subset of this model. The system
dynamics can be described as
!
%
%
#
m11
0
m13
0
0
m22
0
m24
m13
0
m33
0
0
m24
0
m44
"
&
&
$
!
%
%
#
¨α
¨β
¨γ
¨βa
"
&
&
$+N(q, ˙q) = Bu
(2.39)
2.2.2 Motion Planning on an Incline
Condition of rolling up
In this section, we determine the condition of rolling up of the robot on an inclined
plane. The system can be linearized around the position perpendicular to the surface
such as β = 90◦+ δβ, βa = δβa, ˙β = ˙δβ. After linearization, the linear acceleration
of the system along the plane and the angular acceleration of the rolling disk are
¨γ = u1 −mgRCαSϕ
mr2 + I
¨Y = u1 −mgRCαSϕ
mr2 + I
R
where I represents the moment of the whole robot along the Z axis, 2Ixw.
Consider a rolling disk, I is set to be the moment of inertia of the disk along
the Z axis. The condition of rolling without slipping holds, i.e., ¨y = R¨γ.
Initially ˙γ, v0 are set to be zero. The minimum value of the angular accel-
eration of the system is set to be ¨γmin. Therefore, the condition of rolling up
on an incline is
u1 ≥(mR2 + I)¨γmin + mgR sin ϕ
Let’s rearrange the above equation so that ϕ is represent as a function of the
minimum angular acceleration of the system ¨γmin, the moment of inertia I
and the applied torque u1.
sin ϕ = u1 −(mR2 + I)¨γmin
mgR
(2.40)

26
2 Kinematics and Dynamics
0
10
20
30
40
50
60
70
80
90
−4
−2
0
2
4
6
8
10
12
14
x 10
−4
φ (in deg)
τ (in Nm)
τcritical
τslip
Guarantee region 
Slipping region 
Fig. 2.6. Critical torque of a rolling disk v.s. a climbling angle
0
10
20
30
40
50
60
70
80
90
0
5
10
15
20
25
30
φ (climbling angle in deg)
τ (in Nm)
γ..
min=0
γ..
min=3
γ..
min=6
γ..
min=9
γ..
min=12
Fig. 2.7. Critical torque of the robot v.s. a climbling angle
Figures. 2.6 & 2.7 show the critical torques applied to the system for rolling
up an inclined plane with an inclination ϕ. Referring to Fig. 2.7, the condition
of rolling up for a single wheel robot is similar to a rolling disk in Fig.2.6. The
curve of a critical torque looks similar to that of a rolling disk, but it has
larger magnitude.
The dynamic balancing
We have proposed a linear state backstepping feedback[113]. Consider x1 =
δβ, x2 = ˙α, x3 = ˙δβ, the systems can be described in a diﬀerent form:
˙x1 = x3
(2.41)
˙x2 = f21x1 + f22x2 + f23x3 + g21uβa
˙x3 = f31x1 + f32x2 + g32
We deﬁne the following control law,

2.2 Modeling on an Incline
27
uβa =
−f21x1 −f22x2 −f23x3 + ˙α2 −k3(x2 −
η
f32 )
g21
˙α2 = −(1 + f31) ˙x1 −k1 ˙x3 −k2(x3 + k1x1)
f32
which can ensure the system to be stabilized around the position perpendic-
ular to the surface.
Planning rolling Up
In this section, we discuss how to remedy the failure of the condition of rolling
up by planning the robot motion in diﬀerent angles.
Method I : Changing of the orientation. Suppose we change the direction
of heading
sin
y 
ϕ
sin φ
= sin sin
=
ϕ 
α
 
2
2
x + y
From the previous result, the condition of rolling up is
2
C ≥(mR + I)¨
+ mgR sin
min
α sin
γ
ϕ
Now we consider the robot rolls on an inclined plane φ instead of ϕ.
1 ¨
Iα = 0 then 
˙α = ω , 
α = ω t + α
z
z
0
2
If we set
= ˙
ω
α = 0,
z
¨
¨
mX + mR cos α = 0
γ
¨
¨
mY 
mR sin
−
α = 0
γ
2
C 
mgR sin α sin
I 
mR
−
ϕ = ( +
)¨γ
The necessary conditions for rolling up are where C ≥mgR sin
. If the
α sin ϕ
acceleration of rolling of the robot is positive then the acceleration along the
ϕ
φ
α
ysinϕ
X
Y
Fig. 2.8. Change orientation

28
2 Kinematics and Dynamics
y
vo
v
s
l
var
g0d
gd
v0
Fig. 2.9. Disk rolls on a plane
X and Y directions will become positive. In this way, we are sure that, in the
direction of forward motion α0, the robot can roll up on an inclined plane φ.
Method II : Change the initial velocities. Consider that a disk rolls along
a straight line and then it hits an inclined surface. We assume that the robot
does not bounce at the moment of hitting and the robot can remain perpen-
dicular to the surface. We increase its angular velocity and linear velocity of
the disk’s center.
I¨γ = C −FR = C −maR,
for
F = ma
¨γ =
C
mR2 + I ,
for
a = R¨γ
The angular velocity and the velocity of the disk are

2Cs
IR+mR3 and

2CRs
I+mR2
respectively. After it collides with the surface, based on the dynamic equations
derived, the initial condition must be modiﬁed because ˙γ0, v0 are now non-zero
in this case. Before the hitting on the inclined surface, we have
¨y = RC −mR2g sin ϕ
mR2 + I
,
¨γ = C −mRg sin ϕ
mR2 + I
afterward, we have,
˙y = RC −mR2g sin ϕ
mR2 + I
t + v0 cos ϕ
¨γ = C −mRg sin ϕ
mR2 + I
t + ˙γ0
When ˙y = ˙ymin,

2.2 Modeling on an Incline
29
t = ( ˙ymin −v0 cos ϕ)(mRw + I)
RC −mR2g sin ϕ
y = v0 cos ϕt + 1
2
!RC −mR2g sin ϕ
mR2 + I
"
t2
l = (mR2 + I)( ˙y2
min −v0 cos ϕ)
2(RC −mR2g sin ϕ)
(v0 cos ϕ)2 = ˙y2
min −2l(RC −mR2g sin ϕ)
mR2 + I
If ˙ymin = 0, then
v2
0 = 2l(mR2g sin ϕ −RC)
cos ϕ2(mR2 + I)
l = v2
0 cos ϕ2(mR2 + I)
2(mR2g sin ϕ −RC)
2.2.3 Simulation Study
Based on [112], we found that the property of Gyrover is similar to a rolling
disk when its ﬂywheel does not spin. If Ixf is zero, the dynamics of the robot
is exactly the same as that of a rolling disk. The purpose of the high spinning
ﬂywheel is to provide a larger resistance to the rate of change of leaning,
stabilizing the robot to balance. Now, we investigate how the high spinning
ﬂywheel provides a balancing eﬀect for the robot when it climbs up on an
inclined plane. In short, the robot, when its ﬂywheel does not spin, cannot
roll up an inclined plane i.e. a rolling disk cannot climb up an inclined plane.
It is because the force components mgRcβcϕ and −gmTsαsβsϕ are dominant
on the roll dynamic. In the simulation, we assume that the single wheel robot
is a rolling disk with Ixw = Iyw = 1
2mR2 and Izw = mR2. We use the following
geometric/mass parameters from the real system throughout our simulations
in Table 2.3.
Robot wheel:
m = 15kg, R = 17cm, Ixw = 0.0289
Flywheel:
Ixf = 0.0063
Static Friction :
µs = 1Nm/(rad/s)
Sliding Friction :
µg = 0.1Nm/(rad/s)
An Inclination angle: ϕ = 10◦
Table 2.3. System parameters
The initial condition in the simulation study is
˙α = ˙β = 0 rad/s, ˙γ = −15 rad/s
α = γ = 0◦, β = 80◦, X = Y = 0

30
2 Kinematics and Dynamics
Case I: Rolling up case
•
Fig.2.10 shows the simulation of a rolling disk. It is noted that the lean
angle ˙β of the rolling disk decreases rapidly and the lean angle β becomes
zero. This means the disk falls over, or, its inertial matrix will become
singular, which violates the assumption of rolling without slipping. Besides,
the steering rate ˙α rises up and down, and the trajectory of the rolling disk
does not go up along the Y axis. The rolling disk fails to go up.
•
For a single wheel robot, we investigated the system when ˙γa = 1600 rpm =
0. It is clear that the spinning ﬂywheel provides a large angular momen-
tum. We stabilize the robot to the upright position β = 90◦, δβref = 0◦,
such that the resulting roll-up trajectory is a straight line. The feedback
gains are k1 = −30, k2 = −3 and k3 = 3 respectively. The simulation
results are shown in Figure 2.11. It shows that the lean angle β of the
robot exponentially converge to 90◦and the steering rate ˙α exponentially
converges to zero. The rolling speed of ˙γ becomes -35 rad/s and the trajec-
tory of the center of the robot oscillates at the beginning and then ﬁnally
is restricted to follow a straight line path.
Case II: Rolling down case
•
Fig.2.12 shows that a rolling disk falls down very rapidly from stationary.
It is because 5◦turns in the lean angle from the vertical direction makes
the disk fall down.
•
Fig.2.13 shows the single wheel robot when rolling down. We select the
feedback gains which are k1 = 30, k2 = −3 and k3 = −3 respectively. It
is similar to the rolling up case. This implies that the single wheel robot
is balanced along the vertical position by the tilting of the ﬂywheel.
0
0.2
0.4
0.6
0.8
0
5
10
15
20
25
30
35
t (s)
α (deg)
0
0.2
0.4
0.6
0.8
0
20
40
60
80
t (s)
β (deg)
0
0.2
0.4
0.6
0.8
0
0.5
1
1.5
2
t (s)
α. (rad/s)
0
0.2
0.4
0.6
0.8
−10
−8
−6
−4
−2
0
t (s)
β. (rad/s)
Fig. 2.10. Rolling up of a disk

2.2 Modeling on an Incline
31
0
0.02
0.04
0.06
0.08
0.1
0.12
0
0.2
0.4
0.6
0.8
1
1.2
Fig. 2.10. Continued
0
1
2
0
0.5
1
1.5
2
2.5
3
t
α (in deg)
0
1
2
78
80
82
84
86
88
90
92
t
β
0
1
2
−8
−6
−4
−2
0
2
t
βa
0
1
2
−10
−5
0
5
10
t
α.
0
1
2
−0.5
0
0.5
1
1.5
2
2.5
3
t
β.
0
1
2
−40
−35
−30
−25
−20
−15
t
γ.
−0.15
−0.1
−0.05
0
0.05
0
2
4
6
8
10
12
x
y
path of C.M. of robot on slope
0
0.5
1
1.5
2
−250
−200
−150
−100
−50
0
t
τ
angle= 10o
Fig. 2.11. Rolling up of a single wheel robot
0
0.2
0.4
0.6
0.8
0
2
4
6
8
10
0
0.2
0.4
0.6
0.8
80
100
120
140
160
180
Fig. 2.12. Rolling down of a disk

32
2 Kinematics and Dynamics
0
0.2
0.4
0.6
0.8
0
0.2
0.4
0.6
0.8
0
0.2
0.4
0.6
0.8
0
2
4
6
8
10
0
0.02
0.04
0.06
0.08
0.1
0.12
0.14
−0.45
−0.4
−0.35
−0.3
−0.25
−0.2
−0.15
−0.1
−0.05
0
0
1
2
0
0.5
1
1.5
2
t
α (in deg)
0
1
2
90
91
92
93
94
95
96
t
β
0
1
2
−1
0
1
2
3
4
t
βa
0
1
2
−6
−4
−2
0
2
4
6
t
α.
0
1
2
−2
−1.5
−1
−0.5
0
0.5
t
β.
0
1
2
0
5
10
15
20
t
γ.
−0.05
0
0.05
0.1
0.15
−7
−6
−5
−4
−3
−2
−1
0
x
y
path of C.M. of robot on slope
0
0.5
1
1.5
2
−50
0
50
100
150
200
t
τ
angle= 10o
Fig. 2.13. Rolling down of a single wheel robot
Fig. 2.12. Continued

3
Model-based Control
3.1 Linearized Model
3.1.1 Stabilization
Due to the inherent lateral instability, the ﬁrst step in controlling the robot
is stabilization. Fortunately, the lean angle of the robot can be controlled in-
directly by tilting the ﬂywheel. The single wheel robot steers by leaning at
diﬀerent angles. Therefore, it is necessary to design a controller that stabilizes
the robot at any desired lean angle, so as to control the steering velocity.
On the other hand, for a typical unicycle, it should be noted that the lon-
gitudinal and lateral motions are highly coupled to each other, and it is not
suitable to decompose the motions through a linearization method (Nakajima
et al. [75] and Sheng [96]). However, because of the stabilizing eﬀect of the
ﬂywheel, the eﬀect of coupling/cross terms between the longitudinal and lat-
eral motions of the robot become less signiﬁcant for the single wheel robot.
It is feasible to decouple them by linearizing the dynamic model, and then
by designing a linear state feedback law to control the lean angle and rolling
speed of the robot.
Linearized Model
We ﬁrst linearize the system Eq. (2.32) about the vertical position. The deﬁ-
nition of variables and the conﬁguration of the linearized model are shown in
Table 2.1 and Figure 3.1 respectively. In derivation of the linearized model,
we make the folllowing assumptions: (1) the spinning rate ˙γa is suﬃciently
large and remains constant, so that the terms ˙γa ˙β, ˙γa ˙α are suﬃciently larger
than the terms ˙β ˙γ, ˙β ˙α, ˙α2. (2) the terms ˙δβ, ˙δβa, ˙α, ˙θ are suﬃciently small, (3)
β = 90o + δβ, ˙γ = Ωo + Ω, βa = δβa. The linearized model can be represented
as
Y. Xu and Y. Ou: Control of Single Wheel Robots, STAR 20, pp. 33–71, 2005.
© Springer-Verlag Berlin Heidelberg 2005

34
3 Model-based Control
W
A
δβ
δβ
β
δβa
D
xB
za
βa
zB
tire
plastic dome
flywheel
Fig. 3.1. The lateral description of Gy-
rover.
 +
+
-
-
-
-
Eq.(3.9)
k1
k2
k3
uβa
u1
Ω
β
˙β
q
Robot
βref
˙αref
Fig. 3.2. Schematic of the control algo-
rithms.
(Ixxf + Ixxw)¨α = 2(IxxwΩo + Ixf ˙γa) ˙δβ −µs ˙α + 2Ixxf ˙γauβa
(3.1)
(Ixxw + mR2)¨δβ = gmRδβ −(2Ixxw + mR2)Ωo ˙α −2Ixxf ˙γa ˙α
(3.2)
(2Ixxw + mR2) ˙Ω= −µgΩ+ u1
(3.3)
Because Ωis independent of the roll and yaw dynamics (Eqs. (3.1) and (3.2)),
we can decompose the longitudinal motion Eq. (3.3), and establish closed-loop
control for controlling the angular velocity ˙γ to the nominal value Ω◦. The
remaining yaw and roll dynamics form a state equation,
˙x = Ax + Gu,
(3.4)
where x =

δβ, ˙α, ˙δβ
T
,
A =


0
0
1
0 a22 a23
a31 a32 0

, G =


0
b2
0


where a22, ..., a32 and b2 are derived from Eqs. (3.1) and (3.2).
Let C(A, G) be the controllability matrix of the system Eq. (3.4). Then
|C(A, G)| = |[ G | AG |A2G ]|
= a2
32b3
2
= ((2Ixxw + mR2)Ωo + 2Ixxf ˙γa
Ixxw + mR2
)2 × (
2Ixxf ˙γa
Ixxf + Ixxw
)3
(3.5)
From Eq. (3.5), the system is controllable if ˙γa = 0, for the reason that if the
spinning rate of the ﬂywheel is equal to zero, the ﬂywheel does not provide
an additional force for balancing the robot.

3.1 Linearized Model
35
If we consider the steering velocity ˙α as the output of Eq. (3.4), then the
transfer function becomes
H(s) = c(sI −A)−1b
=
b2(−a31 + s2)
s3 −a22s2 −(a31 + a23a32)s + a22a31
(3.6)
where c = [0, 1, 0] and b = G = [0, b2, 0]T . Note that there is a zero of H(s)
on the right-half plane, therefore it is a non-minimum phase. This means that
the robot will fall if we control the steering rate ˙α only.
Linear State Feedback
Based on the system Eq. (3.4), if we stabilize the robot to a desired lean angle
δβref such that
δβ = δβref,
˙α = ˙αs,
˙δβ = 0 rad/s, ¨δβ = ¨α = ˙Ω= 0 rad/s2,
(3.7)
then the linear state feedback can be designed as
uβa = −k1(δβ −δβref) −k2 ˙δβ −k3( ˙α −˙αs).
(3.8)
where k1, k2, and k3 are feedback gains and ˙αs is the equilibrium steering rate
under the conditions Eq. (3.7), that is
˙αs =
gmRδβref
(2Ixxw + mR2)Ωo + 2Ixxf ˙γa
(3.9)
In order to ensure an asymptotic stability of the system Eq. (3.4), the neces-
sary conditions of the feedback gains are
k1 < 0, k2 < 0, k3 > 0.
Asymptotic stability ensures that all eigenvalues of the closed loop system
have negative real parts. Note that for the given nominal rolling speed Ω◦,
the equilibrium steering rate ˙αs corresponds to one, and only one lean angle
δβref. Thus δβref and ˙αs can not be selected independently.
To allow the robot to track a desired path on the ground, we must be able
to control its steering and linear velocities. For tracking the desired steering
velocity ˙αref, we propose to track a roll angle δβref in which the equilib-
rium steering velocity ˙αs will be equal to the desired one ˙αref, according to
Eq. (3.9). The schematic diagram for controlling the steering rate is shown in
Figure 3.2.

36
3 Model-based Control
3.1.2 Path Following Control
The kinematic constraints of a typical mobile robot with a steering front wheel
can be written as


˙x
˙y
˙θ

=


cos θ 0
sin θ 0
0
1



 υ
ω

(3.10)
where (x, y) is the position of the center of the vehicles, θ is the orientation,
and (υ, ω) are the linear and steering velocities of the typical mobile robot
respectively. A number of trajectory tracking methods have been proposed
for the typical mobile robot [58], [87], [49], [89], [99]. However, because the
single wheel robot is not assumed to be in a vertical position, the constraints
described in Eq. (3.10) also depend on the lean angle and lean rate in this
case. Furthermore, for the typical mobile robot, the steering velocity ω can be
directly generated by turning the front wheel. The single wheel robot steers by
leaning itself to a predeﬁned angle. Therefore, the main diﬃculty in solving
the path following problem of the single wheel robot is that we must not
only control the position (x, y) and the orientation θ using two control inputs
(υ, ω), but also control the lean angle β within a stable region to prevent the
robot from falling.
Here, we propose an approach to the path following problem based on a
geometrical notion in controlling the path curvature. We redeﬁne the system
conﬁguration of the robot based on the geometrical notion and characteristics
of the nonholonomic motion.
Robot Conﬁguration
In the previous sections, we considered the center of mass (Xc, Yc) to be the
position of the robot. However, for path following where the robot needs to
track a desired path on the ground, it is better to use the point of contact a
on the ground to describe the position of the robot, instead of the center of
mass C (Figure 2.1). Let (xa, ya) be the coordinates of the contact point a on
the locus that coincides with a point of contact p of the robot. xa and ya can
be expressed as

xa
ya

=

Xc −RSαCβ
Yc + RCαCβ

(3.11)
Diﬀerentiating Eq. (3.11) with respect to time, velocity constraints at the
contact point a are found to be

˙xa
˙ya

=

υaCα
υaSα

(3.12)
where υa is the contact point velocity,
υa = R ˙γ
(3.13)

3.1 Linearized Model
37
Note that Eq. (3.12) is independent of the lean angle β and is identical to the
rolling constraints of a typical mobile robot, i.e. there is no drift term in Eq.
(3.12).
The status of the contact point of the robot can be described by an alter-
native set of conﬁgurations based upon [48],
q = (p, ϕ, κ) = ((xa, ya), ϕ, κ) ,
(3.14)
where (xa, ya), ϕ and κ are the position, the heading orientation and the path
curvature of the robot with respect to the inertial frame, respectively. The new
conﬁguration of the robot is shown in Figure 3.3. The heading orientation ϕ
is measured about the X-axis while the yaw angle α is measured about the
Y-axis. Then
ϕ = α −π
2 ,
˙ϕ = ˙α.
(3.15)
Normally, we control the kinematic system of the robot using the steering
and linear velocities to track a desired path. However, we can only indirectly
control the steering velocity of a robot by changing its lean angle. Because the
motion of the contact point a undergoes nonholonomic motion [48], its path
curvature κ can be expressed as,
κ(t) = dϕ(t)
ds
=
˙ϕ(t)
υa(t) = ˙α(t)
υa(t) =
1
r(t)
(3.16)
where r(t), s and ˙ϕ(t) are the radius of the curvature from the center of ro-
tation c to the contact point a, the path length, and the rotational velocity
of the robot, respectively (Figure 3.3). If the center c of the rotation is at
inﬁnity, the robot is moving in a straight line and the path curvature and
steering velocity are zero. From Eq. (3.16), for the given linear velocity υa, we
can control the path curvature κ by controlling the steering velocity ˙α.
Line Following
We designed a line following controller for the robot to track a desired straight
line. It is divided into two parts: (1) the velocity control law and (2) the torque
control law. Using the velocity control law, the velocity inputs of the robot
can be designed based on the error between the current conﬁgurations of the
robot and the given trajectory. Assuming that the linear velocity of the robot
is ﬁxed/controlled to a nominal value, we consider the steering velocity as the
only velocity input for the robot. Using the torque control law, we can design
the tilt torque of the ﬂywheel in order to lean the robot to a predeﬁned angle
which corresponds to the velocity input (steering velocity) obtained from the
velocity control law.

38
3 Model-based Control
desired
line
Robot
d
δd
ϕ1
a
X
Y
trajectory
−xa
ya
r(t)
ϕ
L
α
˙α
c
Fig. 3.3. Principle of line following.(top view)
Eq. (3.9)
δβref
κ
!
dκ
ds
dκ
ds = −aκ −bϕ −cxa
˙q
q
Dynamics
Kinematics
uβa = −k1(δβ −δβref )
−k2 ˙δβ −k3( ˙α −˙αref )
˙α = κυa
˙αref
uβa
xa, ya
Torque control law
Velocity control law
Fig. 3.4. Schematic of the control algorithm for the Y-axis.
Velocity control law
Unlike some typical control methods for the nonholonomic system, we used
the path curvature as a variable to describe the path tracking of the robot with
nonholonomic constraints. Therefore, based on [48], we consider the deriva-
tive of the path curvature as the velocity control law and then express the
derivative of the path curvature ( dκ
ds ) with respect to the path length s as :
dκ
ds = −aκ −b(ϕ −ϕ1) −cδd,
(3.17)
where a, b and c are positive constants, ϕ1 and δd are the direction of the
desired line and the perpendicular distance between the robot and the desired
line respectively (Figure 3.3). Eq. (3.17) is called a steering function. If (a, b, c)
are selected such that Eq. (3.17) is asymptotically stable, the continuity of

3.1 Linearized Model
39
the path curvature can be ensured, and dκ
ds →0 as the path length s increases,
i.e., δd →0, κ →0 and ϕ →ϕ1 as s increases. Hence, the robot converges to
the desired straight line asymptotically. In order to design the velocity input
(steering velocity), we ﬁrst ﬁnd the path curvative feedback κ by integrating
the Eq. (3.17) in each instant. Using the given linear velocity and the path
curvature feedback, we can obtain the corresponding velocity input for the
robot according to Eq. (3.16).
Convergence of the velocity control law
To achieve convergence of the control law Eq. (3.17), we must select a proper
set of coeﬃcients (a, b, c). We ﬁrst transform the coordinate system such that
the desired path becomes the Y-axis, i.e., δd = −xa, ϕ1 = 0. Then we introduce
a new time scale which is identical to the distance along the desired path ya
and represent variables in Eq. (3.17) (xa, ϕ, κ and dκ
ds ) in terms of the derivative
of xa with respect to ya. ( dnxa
dyn
a )
ϕ = tan−1(dxa
dya
)
(3.18)
κ = dϕ
ds =
dϕ
dya
ds
dya
=
d2xa
dy2
a
(1 + ( dxa
ya )2)
3
2
(3.19)
dκ
ds =
dκ
dya
ds
dya
,
= d3xa
dy3a
(1 + dxa
dya
2
)−2 −3dxa
dya
d2xa
dy2a
2
(1 + dxa
dya
2
)−3
(3.20)
Note that if xa →0, dxa
dya →0, d2xa
dy2
a →0 and d3xa
dy3
a →0, then ϕ →0, κ →0
and dκ
ds →0. Hence, the robot converges to the Y-axis. In order to track the
Y-axis, xa and its derivative with respect to ya must converge to zero. To this
end, we deﬁne a system of the ﬁrst order equation,
dξ
dya
= f(ξ),
(3.21)
where
ξ ≡


ξ1
ξ2
ξ3

≡


xa
dxa
dya
d2xa
dy2
a


(3.22)
If the system of Eq. (3.21) is asymptotically stable, xa and its derivative with
respect to ya converge to zero asymptotically. Using the Jacobian linearization,
the system of Eq. (3.21) becomes

40
3 Model-based Control
dξ
dya
= Aξ,
(3.23)
where
A =

df
dξ

ξ=0
=


0
1
0
0
0
1
−c −b −a


(3.24)
To ensure the asymptotic stability of the system Eq. (3.23), a, b and c must be
larger than zero while ab > c. The solution for critically damped conditions
is,
a = 3k,
b = 3k2,
c = k3.
(3.25)
where k is the gain of the velocity control law.
σ = 1
k
(3.26)
where σ is called the smoothness of the velocity control law.
Torque control law
By the velocity control law, the steering velocity input is deﬁned when the
linear velocity is given. Because of the absence of the front steering wheel, the
robot steers only by leaning itself to a predeﬁned angle. Therefore, we propose
to design the tilt input of the ﬂywheel uβa for tracking a lean angle trajectory
δβ(t) that is corresponding to the given steering velocity. Assuming that we
can control the lean angle β faster than the control of the steering velocity ˙α,
we may consider ˙α to be ﬁxed in the β time scale.
Based on Eq. (3.9), we can control the steering velocity ˙α to the desired
value where the robot must be stabilized to the corresponding lean angle δβref.
The linear state feedback for stabilizing the robot to the desired lean angle is
uβa = −k1(δβ −δβref(t)) −k2 ˙δβ
(3.27)
where δβ and δβref are the small pertubation of the lean angle from the ver-
tical position and the reference lean angle respectively. Note that the steering
velocity ˙α does not appear in Eq. (3.27) because if the robot stabilizes to
the desired lean angle δβref, its steering rate must converge to an equilibrium
value according to Eq. (3.9). Therefore, there is no need to include the steer-
ing velocity in the state feedback Eq. (3.27). The overall architecture of the
control algorithm has been shown in Figure 3.4.
3.1.3 Control Simulation
We ﬁrst show the simulation results of the robot to track a desired line y =
tan 30◦x, with

3.1 Linearized Model
41
ϕ1 = π
6 , δd = xa −tan 30◦ya
sec 30◦
.
(3.28)
The simulation results are shown in Figure 3.5. The initial conditions in this
simulation are shown in Table 3.1. The decoupled dynamic model [124] is
adopted through out the simulations.
Simulation Xa Ya α(0) β(0) κ(0) βa(0) ϕ(0)
S1
5[m] 0
70◦110◦
0
0
−20◦
S2
5[m] 0 110◦130◦
0
0
20◦
S3
5[m] 0 210◦110◦
0
0
120◦
Table 3.1. The initial conditions for the simulations with diﬀerent initial heading
angles
0
20
40
60
−1
0
1
2
3
4
5
 (a)
0
5
10
15
20
85
90
95
100
105
110
115
 (b)
0
5
10
15
20
−8
−6
−4
−2
0
2
4
x 10
−3
(c)
β[deg]
x[m]
y[m]
t
t
κ
Fig. 3.5. The simulation results (S1) for following the Y-axis.
0
20
40
60
−1
0
1
2
3
4
5
6
 (a)
0
5
10
15
20
60
70
80
90
100
110
120
130
 (b)
0
5
10
15
20
−20
−15
−10
−5
0
5
x 10
−3
(c)
β[deg]
x[m]
y[m]
t
t
κ
Fig. 3.6. The simulation results (S2) for following the Y-axis.
−3
17
37
57
−2
0
2
4
6
8
10
12
 (a)
0
5
10
15
20
20
40
60
80
100
120
 (b)
0
5
10
15
20
−0.15
−0.1
−0.05
0
0.05
(c)
β[deg]
x[m]
y[m]
t
t
κ
Fig. 3.7. The simulation results (S3) for following the Y-axis.
Figure 3.5 shows that the robot tracks the desired straight line without
falling down (β > 0◦).
Eﬀect of the initial heading angle
In this section, we show three simulation results (S1, S2 and S3) in which
the robot tracks the Y-axis, under diﬀerent initial conditions. The simulation

42
3 Model-based Control
results and the initial conditions are shown in Figures 3.5, 3.6, 3.7 and Ta-
ble 3.1, respectively. In these simulations, the rolling speed ˙γ of the robot is
controlled to a nominal rolling speed Ω◦= 30 rad/s, thus the contact point
velocity υa is also a constant. The smoothness is set to 30.
For simulation S1, the robot has a smaller path curvature than in sim-
ulation S2, because its initial heading angle ϕ(0) in simulation S1 is less
deviated from the Y-axis than in simulation S2. Furthermore, since the robot
has larger path curvature in simulation S2, it results in sharper turns. To this
end, the robot has to lean steeper to provide a suﬃcient steering velocity ˙α in
simulation S2. The situation becomes more serious in simulation S3. Figure
3.7 shows the lean angle β is saturated in 5 sec. Because the initial heading
angle ϕ(0) is more than 90◦, in order to ensure the curvature continuity while
following the Y-axis, it should have the largest path curvature among them
such that the lean angle β of the robot is the steepest, to provide a suﬃcient
steering velocity. As we have already set up a limit for the state feedback
controller, the lean angle can only be stabilized within β ∈(−60◦, 60◦). Oth-
erwise, the controller will become saturated and the lean angle will be ﬁxed
at the limiting value. When the robot gradually approaches the Y-axis, the
lean angle gradually increases until it reaches the vertical position (90◦).
Eﬀect of the rolling speed
In this section, we study the eﬀect of the rolling speed of the robot ˙γ on
the path following controller. The simulation results with diﬀerent rolling
speeds are shown in Figures 3.8 and 3.9, both with the same initial conditions
as in simulation S1. For ˙γ = 30 rad/s, the robot converges to the Y-axis
more rapidly than the other one, comparing Figures 3.8a and 3.9a. From
Figures 3.8b and 3.9b, the change of the lean angle β of the robot with a
lower rolling speed is more signiﬁcant than that of the robot with a higher
rolling speed. It is because, for the robot with a greater rolling speed, based
on Eq. (3.16), a lesser steering velocity is required for tracking the same path
curvature feedback κ. Thus, the change of the lean angle β decreases according
to Eq. (3.9).
3.2 Nonlinear Model
Recently, there has been growing interest in the design of feedback con-
trol laws for nonholonomic systems [57]. Due to Brockett’s theorem in [123], it
is well known that a nonholonomic control system cannot be asymptotically
stabilized to a resting conﬁguration by smooth time-invariant control laws
[18]. Despite this, several discontinuous or time-variant approaches have been
proposed for stabilizing such systems in [18], [46], [57], [63] and [104]. The
references above refer to systems with ﬁrst-order nonholonomic constraints,

3.2 Nonlinear Model
43
0
20
40
60
80
100
−2
−1
0
1
2
3
4
5
6
 (a)
0
5
10
15
20
40
60
80
100
120
140
 (b)
0
5
10
15
20
−0.03
−0.02
−0.01
0
0.01
(c)
β[deg]
x[m]
y[m]
t
t
κ
Fig. 3.8. The simulation results for following the Y-axis with ˙γ = 10 rad/s.
0
10
20
30
40
0
1
2
3
4
5
6
 (a)
0
5
10
15
20
80
90
100
110
120
130
140
t
 (b)
0
5
10
15
20
−20
−15
−10
−5
0
5
x 10
−3
t
(c)
β[deg]
x[m]
y[m]
κ
Fig. 3.9. The simulation results for following the Y-axis with ˙γ = 30 rad/s.
0
50
100
150
−2
−1
0
1
2
3
4
5
6
 (a)
0
5
10
15
20
20
40
60
80
100
120
140
 (b)
0
5
10
15
20
−0.03
−0.025
−0.02
−0.015
−0.01
−0.005
0
0.005
(c)
β[deg]
x[m]
y[m]
t
t
κ
Fig. 3.10. The simulation results for following the Y-axis with σ = 20.
which can usually be expressed in terms of nonintegrable linear velocity re-
lationships. However, there are another kind of mobile robot systems, which
possess both ﬁrst-order and second-order nonholonomic constraints, such as
our robot – Gyrover, bicycles and motorcycles. Mobile robots have their in-
herent nonholonomic features, which can be described as ﬁrst-order nonholo-
nomic constraints among joint velocities and Cartesian space velocities. These
constraints arise when robots roll on the ground without slipping. Because no
actuators can be used directly for stabilization in the lateral direction, these

44
3 Model-based Control
0
50
100
150
−1
0
1
2
3
4
5
6
7
 (a)
0
10
20
30
60
80
100
120
140
 (b)
0
10
20
30
−15
−10
−5
0
5
x 10
−3
(c)
β[deg]
x[m]
y[m]
t
t
κ
Fig. 3.11. The simulation results for following the Y-axis with σ = 40.
systems are underactuated nonlinear systems. This induces another nonholo-
nomic constraint of robots. Thus, to compare with the above research work,
our mobile robot systems – Gyrover, is more challenging to be controlled.
This work can provide some ideas for that class of problems. In this thesis,
we want to control an underactuated mobile robot system – Gyrover. There are
two control inputs: one is the steering torque (or steering rate) and the other
is the driving torque (or driving speed). However, we have four independent
generalized coordinates to control: (1) the lean angle, (2) the heading angle,
(3) the Cartesian space X axis, (4) the Cartesian space Y axis.
Papers [6] and [7] assumed that the robot remained around the vertical
position, which simplify this nonlinear system to a linear one. Therefore vali-
dation of the results can be limited. Some work had been about the tracking
of a rolling disk, such as in [87], which assumed three control inputs in the
direction of steering, leaning and rolling, where no unactuated joint and no
second-order constraint is presented. In [34], the author simpliﬁed the bicycle
dynamic model, and used velocities as control inputs to enable the lean angle
(called “roll-angle” in that paper) to track trajectories, which have continu-
ous diﬀerentials. However, the controller could not guarantee that the bicycle
would not topple over, i.e. the lean angle was out of range, before convergence.
In this chapter, we focus on three control problems that have not yet been
solved for Gyrover. The ﬁrst one is the balance of the robot while stand-
ing. The second one is concerned with point-to-point control. The third one
relates to following a straight line. These three problems are of signiﬁcance
in controlling a system with both ﬁrst-order and second-order nonholonomic
constraints.
Inertia Matrix and Nonholonomic Constraints
The kinematics and dynamics of Gyrover are diﬀerent from those of unicycles,
such as in [1]. The diﬀerence lies in the assumption that the unicycle always
remains vertical. On the contrary, Gyrover can be considered as a rolling disk
which is not constrained to the vertical position and is connected to a high

3.2 Nonlinear Model
45
speed spinning ﬂywheel. This model can serve well as a simpliﬁcation for the
study of a model of Gyrover.
Consider a disk rolling without slipping on a horizontal plane as shown in
Figure 4.9. Let %
o X, Y, Z and %
c x, y, z be the inertial frame whose x −y
plane is anchored to the ﬂat surface and the body coordinate frame whose
origin is located at the center of the rolling disk, respectively. Let (X, Y, Z) be
the Cartesian coordinates of the center of mass (c) with respect to the inertial
frame %
o. Let A denote the point of contact on the disk. The conﬁguration
of the disk can be described by six generalized coordinates (X, Y, Z, α, β, γ),
where α is the steering (precession) angle measured from X-axis to the contact
line, β ∈(0, π) is the lean (nutation) angle measured from Z-axis to the z, and
γ is the rolling angle. R is the radius of Gyrover. m, Ix, Iy and Iz represent
the total mass and the moment of inertia of Gyrover.
Z
Y
X
o
x
z
α.
β
β
.
γ.
α
L
τ
A
z
ν
y
c
Fig. 3.12. System parameters of Gyrover’s simpliﬁed model.
In the derivation of the model, we assume that the wheel rolls on the
ground without slipping. Based on the previous derivation in [112] and letting
Sx := sin (x), Cx := cos (x), the dynamic model is
M(q)¨q = N(q, ˙q) + Bu
(3.29)
 ˙X = R(˙γCα + ˙αCαCβ −˙βSαSβ)
˙Y = R(˙γSα + ˙αSαCβ + ˙βCαSβ)
(3.30)

46
3 Model-based Control
T
T
where q = [α, β, γ] , N = [N , N , N ]
1
2
3
M =


M11
0
M13
0
M22
0
M31
0
M33


B =

 1 0 0
0 0 1
T
, u =

 u1
u2

M11 = IxS2
β + (2Ix + mR2)C2
β
M13 = (2Ix + mR2)Cβ
M22 = Ix + mR2
M31 = M13
M33 = 2Ix + mR2
N1
= (Ix + mR2)S2β ˙α ˙β + 2IxSβ ˙β ˙γ
N2
= −mgRCβ −(Ix + mR2)CβSβ ˙α2 −(2Ix + mR2)Sβ ˙α ˙γ
N3
= 2(Ix + mR2)Sβ ˙α ˙β,
where (X, Y , Z) is the robot’s center of mass coordinate with respect to the
inertial frame as shown in Figure 4.9. M(q) ∈R3×3 and N(q, ˙q) ∈R3×1 are
the inertial matrix and nonlinear terms, respectively. Equations (3.29) and
(3.30) form the dynamic model and ﬁrst-order nonholonomic constraints in
the form of velocity.
In order to simplify the control design, we will transform the inertia matrix
of the model to a diagonal matrix and reduce the nonlinear terms of the
dynamic model. We ﬁrst intend to cancel some nonlinear terms by letting
u1 = −N1 + u3
u2 = −N3 + u4.
(3.31)
Substituting Equation (3.31) into Equation (3.29) yields



M11 ¨α + M13¨γ = u3
M22 ¨β
= −mgRCβ −(Ix + mR2)CβSβ ˙α2 −(2Ix + mR2)Sβ ˙α ˙γ
M31 ¨α + M33¨γ = u4
(3.32)
From the ﬁrst and third equations in Equation (3.32) solving with respect to
¨α and ¨γ one obtains
Mρ = M11M33 −M 2
13.
Due to its “shell” structure, even when Gyrover topples over, the lean angle
is not zero, so that S2
β > 0 and Mρ > 0 for all t. Then, we let
u5 = (M33/Mρ)u3 −(M13/Mρ)u4
u6 = −(M13/Mρ)u3 + (M11/Mρ)u4
and obtain

3.2 Nonlinear Model
47



¨α
= u5
M22 ¨β = −mgRCβ −(Ix + mR2)CβSβ ˙α2 −(2Ix + mR2)Sβ ˙α ˙γ
¨γ
= u6
(3.33)
Equation (3.30) is nonintegrable, which is deﬁned in [86]. Hence, it is the
ﬁrst-order nonholonomic constraint of the robot system. Moreover, we note
that no control input is available for actuating directly on lean angle β. This
forms a constraint in the form of accelerations. G. Oriolo in [80] proposed
some necessary conditions for the partial integrability of second-order non-
holonomic constraints, one of the conditions is that the gravitational term Gu
is constant. In Equation (3.33), the gravitational term varies along β, thus it
is a nonintegrable, second-order nonholonomic constraint.
3.2.1 Balance Control
Problem Statement
Our ﬁrst control problem is to enable the robot to stand vertically, i.e., to
stabilize the lean angle β to π/2 and ˙β, ¨β, ˙α and ˙γ to zero.
Based on the previous consideration, we are now able to specify more
clearly the aforementioned closed loop steering problem in the following gen-
eral terms.
Let the robot system deﬁned in Equation (3.33) be initially moving in
an undesired state and assume all the essential state variables are directly
measurable. Then ﬁnd a suitable (if any) state dependent control law [u5, u6]T ,
which guarantees the states [β −π/2, ˙β, ¨β, ˙α, ˙γ] to be asymptotically driven
to the null limiting point [0, 0, 0, 0, 0]T , while avoiding any attainment of the
conditions of β = 0 (or β = π) in any ﬁnite time.
Balance Control
First, we let



Gm = mgR/M22
Im = (Ix + mR2)/M22
Jm = (2Ix + mR2)/M22
(3.34)
where Gm, Im and Jm are positive scalar constant.
Then, the dynamic equations become



¨α = u5
¨β = −GmCβ −ImCβSβ ˙α2 −JmSβ ˙α ˙γ
¨γ = u6.
(3.35)
Letting x(i) be the ith time derivative of x, then from Equation (3.35), we
have
β(3) = h1(t) ˙β + h2(t)u5 + h3(t)u6
(3.36)

48
3 Model-based Control
where



h1(t) = GmSβ −ImC2β ˙α2 −JmCβ ˙α ˙γ
h2(t) = −ImS2β ˙α −JmSβ ˙γ
h3(t) = −JmSβ ˙α.
Set β(0) −π/2 = a, ˙β(0) = b, ¨β(0) = c, and a real number σ be
σ = |3a + 2b + c| /2 + |a + 2b + c| /2 + |a + b| /
√
2.
Proposition 1 Consider the system (3.35) with the feedback control laws
u5 and u6,
u5 =

−( ˙α −
4√k2V ) if ˙α(0) > 0
−( ˙α +
4√k2V ) otherwise
(3.37)
where V is deﬁned in Equation (3.39) and k2 is a positive number, which can
be designed, and
u6 = −(3(β −π/2) + 5 ˙β + 3¨β + h1(t) ˙β + h2(t)u5)/h3(t)
(3.38)
where, h1(t), h2(t), h3(t) are deﬁned as in Equation (3.36).
Any state ( ˙α, ˙γ, β, ˙β, ¨β) starting from the domain D, which is deﬁned as
D = {( ˙α(0), ˙γ(0), β(0), ˙β(0), ¨β(0))| ˙α(0) = 0,
0 < β(0) < π, σ < π/2, ˙α, ˙γ, β, ˙β, ¨β ∈R1}
converges to the limiting point [0, 0, π/2, 0, 0]T .
To present the proof, we need a lemma.
Lemma 1:
Consider a mechanical system with a constraint among a number of states
x1(t), x2(t), ..., xn(t) ∈R1. One of these state variables is uniquely determined
by the other states; i.e., xn = f(x1, x2, ...xn−1). Let the limit of xn exist as
t →∞and all the other states be asymptotically stabilizable to some real
values. If all of the states are continuous and bounded, for all of t, then, xn
is also asymptotically stabilized to its limit, which is decided by the other
states.
Proof:
First, since xn(t) is continuous and bounded, for all t, it is stable.
Second, because all of the other states are asymptotically stabilized to
some values, thus
lim
t→∞xi = ei exist i = 1, 2, ..., n −1.
Moreover, according to the property of limits, we have
lim
t→∞xn = lim
t→∞f(x1, x2, ...xn−1),
lim
t→∞xn = f( lim
t→∞x1, lim
t→∞x2, ..., lim
t→∞xn−1).

3.2 Nonlinear Model
49
Because xn is uniquely decided by the other states and has a limit as time
goes to inﬁnity, xn will converge to the limit decided by the other states.
Then, we address the proof for Proposition 1.
Proof:
First, to prove the subsystem β, ˙β, ¨β asymptotically stabilized, we consider
the following Lyapunov function candidate
V = (β −π/2)2/2 + ( ˙β + β −π/2)2/2 + (¨β + 2( ˙β + β −π/2))2/2.
(3.39)
We need to solve two more problems before we can prove the subsystem is
asymptotically stable. One is whether ˙α is not zero in any ﬁnite time; the
other is whether the controllers guarantee β is constrained in (0, π). We will
address the ﬁrst problem later. We prove the second problem by replacing u6
into Equation (3.35). We have
β(3) = −(3(β −π/2) + 5 ˙β + 3¨β).
(3.40)
By solving this linear diﬀerential equation, we obtain
β −π/2 = e−t((3a + 2b + c) +
√
2(a + b) sin(
√
2t) −(a + 2b + c) cos(
√
2t))/2.
From σ < π/2, we know that 0 < β < π for all t.
Then, from Equation (3.39) the time derivative of V is
˙V = (β −π/2) + ( ˙β + β −π/2)(¨β + ˙β) + (¨β + 2( ˙β + β −π/2))(β(3) + 2(¨β + ˙β)).
By substituting Equation (3.40) into it, we have
˙V = (β−π/2)+( ˙β+β−π/2)(¨β+ ˙β)−(¨β+2( ˙β+β−π/2))(3¨β+3 ˙β+3(β−π/2)).
Then,
˙V = −(β −π/2)2 −( ˙β + β −π/2)2 −(¨β + 2( ˙β + β −π/2))2.
(3.41)
We can use the following Lyapunov function to prove ˙α converging to zero.
Va =

k2V /4 + ˙α2/2.
From Equations (3.39) and (3.41), we have
˙V = −2V.
(3.42)
The time derivative of Vα is
˙Va =
√k2 ˙V
8
√
V
+ ˙αuα.
By substituting Equations (3.42) and (3.37) into it,

50
3 Model-based Control
˙Va = −

k2V /4 −( ˙α ∓
4
k2V ) ˙α.
Then,
˙Va = −(
4
k2V /2 ∓˙α)2.
(3.43)
Then we prove that the condition ˙α = 0 can not be approached in any
ﬁnite time. Without losing generality, we assume ˙α(0) > 0 and let k2 be 1.
If we let V (0) = V0, from Equation (3.42), and by solving the diﬀerential
equation, we obtain
V = e−2tV0.
By putting it into Equation (3.37), we have
¨α = −( ˙α −e−t/2 4
V0).
By solving this diﬀerential equation, we obtain
˙α = e−t ˙α(0) + 2(e−t/2 −e−t)
4
V0.
Thus, since ˙α(0) > 0, ˙α can not reach zero in any ﬁnite time.
Since sin β and ˙α are not zero in any ﬁnite time, then h3(t) does not
become zero for all t and from Equation (3.44), ˙γ is continuous. We will prove
that the limit of ˙γ exists and is zero, as time goes to inﬁnity.
From Lemma 1, if we asymptotically stabilize ¨β, β, ˙α and guarantee that
all of these states are continuous and bounded , ˙γ will asymptotically converge
to zero. Since ˙α will never be zero, there are two problems left. One is whether
β, ˙β, ¨β can be stabilized and the other is whether we can guarantee 0 < β < π
for all of t. From Equation (3.35), we have,
˙γ =
¨β + GmCβ + ImCβSβ ˙α2
−JmSβ ˙α
.
(3.44)
As V and ˙α reach very small values,
√
V is the higher order small of
4√
V .
For |β −π/2| ≤
√
V , |β −π/2| is the higher order small of ˙α, and so as to ˙β
and ¨β. Using a Taylor series expansion, cos(β) is a higher order small of ˙α.
Thus, from Lemma 1 and the above equation, ˙γ asymptotically converges to
zero.
3.2.2 Position Control
Here, we propose a controller that drives the robot to the Cartesian space ori-
gin to study the point to point control problem. This is extremely important,
for it serves as the basis for Cartesian space tracking.
Since the origin of the frame XY ZO(% o) in Figure 4.9 is ﬁxed on the
ground, for the purposes of tracking problems, it is more direct to use the point
of contact A on the ground to describe the position of the robot, instead of
the center of mass. Let (xa, ya) be the coordinates of the contact point A on

3.2 Nonlinear Model
51
the ground that coincides with a point of contact P of the robot in Figure 4.9.
xa and ya can be expressed as

 xa
ya

=

 X −RSαCβ
Y −RCαCβ

(3.45)
Diﬀerentiating Equation (3.45) with respect to time, we obtain,

˙xa = R ˙γCα
˙ya = R ˙γSα
(3.46)
There are two kinds of input control commands for Gyrover: one set of con-
trol commands are torques and the other set of commands are velocities. The
velocity commands uα and uγ control ˙α and ˙γ, respectively. Thus, Equations
(3.30) and (3.33) transform into











˙α = uα
¨β = −GmCβ −ImCβSβu2
α −JmSβuαuγ
˙γ
= uγ
˙xa = RuγCα
˙ya = RuγSα
(3.47)
where G 
I and J are the same as in Equation (3.34).
m, m
m
Problem Statement
Let us consider the robot with respect to the inertial frame XY ZO(% o),
as shown in Figure 3.13. By representing the Cartesian position of the robot
in terms of its polar coordinates, which involves the error distance e ≥0,
X
Y
xa
ya
α
O
θ
ψ
A
e
Fig. 3.13. Parameters in position control.

52
3 Model-based Control
measured from A to O (the origin of the frame), Gyrover’s orientation θ with
respect to the inertial frame, and deﬁning ψ = θ −α as the angle measured
between the vehicle principal axis and the distance vector e. When e = 0,
there is no deﬁnition for θ and ψ. Then the following equations are obtained
˙e = RuγCψ
˙ψ = −uα −Sψuγ/e
(3.48)
where Cψ := cos (ψ), Sψ := sin (ψ) and e = 0. Moreover, we deﬁne ψ = 0 and
˙ψ = −uα, if e = 0.
On the basis of the previous considerations, we are now ready to ad-
dress the aforementioned closed loop steering problem in the following general
terms.
Let the robot system be initially located at any non-zero distance from
the inertial frame and assume that all state variables required are directly
measurable. Then ﬁnd a suitable, if any, state feedback control law [uα, uγ]T
which guarantees the state [e, β −π/2, ˙β] to be asymptotically driven to the
null point [0, 0, 0]T , while avoiding any attainment of the conditions of β = 0
(or β = π) in a ﬁnite time.
Proposed Controller
Proposition 2 Consider the system (3.47) with the feedback control laws uα
and uγ,

uα = −k3Sgn(Cψ)Sgn(β −π/2 + ˙β)
uγ = −(k4e + uk)Sgn(Cψ)
(3.49)
where Sgn, K3 and k4 are deﬁned in Equations (3.50) and (3.51), k4 is a
positive scalar constant and k4 < k3 −1 , which can be designed.
Any state (e, β −π/2, ˙β) starting from the domain D deﬁned by
D = {(e(0), β(0) −π/2, ˙β(0))| e > 0, 0 < β < π,

(β −π/2)2 + (β −π/2 + ˙β)2/2 < π/2,
e, β, ˙β ∈R1}
converges to the point [0, 0, 0]T .
Proof: First, let Sgn(.) be a sign function described as follows:
Sgn(x) =

1,
if x ≥0
−1, if x < 0
(3.50)
Let k3 > 2 be a positive scalar constant, which can be designed and should
be less then ˙αmam. Let
f1 =
&&GmCβ + ImCβSβk2
3
&&
uk = (
&&&2(β −π/2 + ˙β)
&&& + f1)/(JmSβk3).
(3.51)

3.2 Nonlinear Model
53
Set
V = V1 + V2
V1 = ((β −π/2)2 + (β −π/2 + ˙β)2)/2
V2 = e2/2.
(3.52)
The time derivative ˙V is given by
˙V = ˙V1 + ˙V2
where
˙V1 = 2(β −π/2) ˙β + (β −π/2 + ˙β)( ˙β + ¨β)
Substituting Equations (3.51) and (3.49) into Equation (3.47), we obtain
(β −π/2 + ˙β)¨β = −(GmCβ + ICβSβk2
3)(β −π/2 + ˙β)
−
&&&(GmCβ + ImCβSβk2
3)(β −π/2 + ˙β)
&&&
−JmSβk3k4e
&&&β −π/2 + ˙β
&&& −2(β −π/2 + ˙β)2
≤−2(β −π/2 + ˙β)2
such that
˙V1 ≤−(β −π/2)2 −˙β2 −(β −π/2 + ˙β)2
Thus V1 is positive deﬁnite and ˙V1 is negative deﬁnite. The remaining prob-
lem concerns why β will not reach 0 or π during the entire process. Since
|β −π/2| ≤

V1(0) < π/2 and V1 is monotonically non increasing, they
guarantee 0 < β < π.
˙V2 = e˙e
= −eR |Cψ| (k4e + uk)
≤−k4e2R |Cψ|
Since R, k3, k4, Sβ and uk are positive, ˙V2 ≤0. In fact, ˙V2 is strictly negative,
except when
e
= 0
Cψ = 0
(3.53)
In these cases ˙V2 = 0. Equation (3.53) presents two possible solutions for the
system.
Moreover, Cψ = 0 is not a solution, which is evident from Equation (3.48).
By substituting uα and uγ into Equation (3.48), we have
˙ψ = Sgn(Cψ)(k3Sgn(β −π/2 + ˙β) + Sψ(k4e + uk)/e).
Because β →π/2, ˙β →0 and Cβ →0, uk will vanish. Thus we obtain
˙ψ = Sgn(Cψ)(k3Sgn(β −π/2 + ˙β) + k4Sψ).
Since k3 −1 > k4 > 0, during any sampling period, ˙ψ is not zero. Hence, any
system trajectory starting from a set in Cψ = 0 will not remain there. Thus,
e = 0 is the only solution, according to the LaSalle Proposition for nonsmooth
systems in [95]. Therefore, the proposition is proven.

54
3 Model-based Control
3.2.3 Line Tracking Control
For a mobile robot, such as Gyrover, most traveling tasks can be realized by
following connected segments of straight lines. For example, Figure 3.14 shows
a mobile robot’s travel path along a series of connected corridors.
How to realize the straight line following while keeping the robot verti-
cal is our control problem. As depicted in Figure 3.15, where (x2, y2) is the
coordinate of the second point, let us consider Gyrover initially positioned
at a neighborhood of the origin (i.e., the start point) and standing almost
vertically, about to take a straight line so as to approach the second point.
Robot
door
Lab
Fig. 3.14. The robot’s path along connected corridors.
Let us deﬁne r, e and d, which are nonnegative, as the distances from A
to the origin, the line and the second point, respectively. θ and φ are deﬁned
as in Figure 3.15. Then, we have the following equations







e˙e = rRuγSφ−αSφ−θ
d ˙d = pRuγ
p
= rCθ−α −

x2
2 + y2
2Cφ−α
= −dCϕ−α
where Sφ−α := sin (φ −α), Sφ−θ := sin (φ −θ), Cφ−θ := cos (φ −θ), and
Cϕ−α := cos (ϕ −α).
On the basis of previous considerations, we are now ready to address the
aforementioned closed loop line tracking problem in general terms.
Let the robot initially locate at some neighborhood of the origin and as-
sume that all state variables be directly measurable; then ﬁnd a suitable (if

3.2 Nonlinear Model
55
X
Y
x
y
α
e
O
( x
y )
θ
φ
2
2
r
B
P
ϕ
d
Fig. 3.15. The parameters in the line tracking problem.
any) feedback control law [uα, uγ]T that guarantees the state [β−π/2, ˙β, e, d]T
to be asymptotically driven to the point [0, 0, 0, 0]T , while avoiding any at-
tainment of the conditions of β = 0 (or β = π) in ﬁnite time.
Proposition 3 Consider the system (3.47) with the feedback control laws
uα and uγ,

uα = −k3Sgn(Sφ−αSφ−θ)Sgn(β −π/2 + ˙β)
uγ = −(f2 + uk)Sgn(Sφ−αSφ−θ)
(3.54)
where f2 is deﬁned in Equation (3.56).
Any state, starting from [β(0) −π/2, ˙β(0), e(0), d(0)] with 0 < β(0) < π
and e(0) > 0, converges to the point [0, 0, 0, 0]T .
Proof:
First, we introduce some deﬁnitions.
Sgn(.), f1 and uk are deﬁned as in Equations (3.50) and (3.51).
Let Θ(.) be a sign function described as follows:
Θ(x) =

1, if x ≥0
0, if x < 0.
(3.55)
We let
f2 = k5Θ(pSgn(Sφ−αSφ−θ))
(3.56)
where k5 is a positive scalar constant which can be designed and should be
less than ˙γmax.

56
3 Model-based Control
V = V1 + V2 + V3
V1 = ((β −π/2)2 + (β −π/2 + ˙β)2)/2
V2 = e2/2
V3 = d2/2
(3.57)
The time derivative ˙V is given by
˙V = ˙V1 + ˙V2 + ˙V3
where
˙V1 = (β −π/2) ˙β + (β −π/2 + ˙β)( ˙β + ¨β)
≤−(β −π/2)2 −(β −π/2 + ˙β)2
˙V2 = e˙e
= −rR |Sφ−αSφ−θ| (f2 + uk)
˙V3 = d ˙d
= pRuγ.
Since r, R, f2 and uk are nonnegative, ˙V2 ≤0. This means that the ﬁrst
term e is always non increasing in time and in the consequence. If d = 0 is
maintained, e is also zero. Moreover, from Figure 3.15, Sφ−θ = 0 and e = 0
can be deduced from each other. ˙α will never be zero, for ˙α = uα. Because φ
is a constant value, it is trivial to know that e will not stop decreasing until
e = 0.
V1 is positive deﬁnite and ˙V1 is negative deﬁnite. At the beginning β is
near vertical, so that during the entire process, β ≈π/2 is sustained. That
means cos β →0 and uk .= 0. By omitting them,
˙V3 = −k5pSgn(Sφ−αSφ−θ)Θ(pSgn(Sφ−αSφ−θ)).
thus, ˙V3 is negative semi-deﬁnite. As with the previous process, it is trivial
to prove that the only solution of the system, for V3 = 0 is d = 0. Thus, we
prove the proposition.
3.2.4 Simulation Study
The following simulation study is based on robot system parameters as shown
in Table 3.2.
m [kg] R [m] g [m/s2] Ix [kgm2]
Iy
Iz
8.05
0.17
9.8
0.116
0.116 0.232
Table 3.2. Physical parameters.
Set

3.2 Nonlinear Model
57
Balance Control
With regard to balance control, the initial condition seems to be too narrow
for the constraint, σ < π/2. If we make a small modiﬁcation to u6 as follows,
the controller can be used in a wider range of initial conditions.
u6 = −((2 + k1)(β −π/2) + (3 + 2k1) ˙β + (2 + k1)¨β + h1(t) ˙β + h2(t)u5)/h3(t)
where k1 is a positive number, which can be designed. To prove the subsys-
tem β, ˙β, ¨β is asymptotically stabilized, we consider the following Lyapunov
function candidate
V ∗= (β −π/2)2/2 + ( ˙β + β −π/2)2/2 + (¨β + (1 + k1)( ˙β + β −π/2))2/2.
Its derivative is
˙V ∗= −(β −π/2)2 −k1( ˙β + β −π/2)2 −(¨β + (1 + k1)( ˙β + β −π/2))2.
We completed two sets of simulations for balance control to compare the
diﬀerent eﬀects of k1. The initial parameters are shown in Table 3.3.
β0 [rad] ˙β0[rad/s] ˙α0
˙γ0
π/4
3
8 -8.660381
Table 3.3. Initial parameters in balance control.
In the ﬁrst simulation, k1 = 10, the target and other gain parameters are
shown in Table 3.4.
βd [rad] ˙βd [rad/s] ˙αd ˙γd k1 k2
π/2
0
0
0 10 1
Table 3.4. Target and gain parameters in balance control I.
The time span of the simulation is from 0 to 16.0s. The results are shown
in Figures 3.16, 3.17, 3.18 and 3.19 .
In the second simulation, except for k1 = 1, all other parameters are the
same as in the ﬁrst simulation.
The time span of the simulation is from 0 to 16.0s. The results are shown
in Figures 3.20, 3.21, 3.22 and 3.23 .
Even though it is possible that the states in simulation II will converge,
the control process in simulation I is faster and smoother.

58
3 Model-based Control
0
2
4
6
8
10
12
14
16
0.7
0.8
0.9
1
1.1
1.2
1.3
1.4
1.5
1.6
Time (s)
β
Fig. 3.16. Leaning angle β in balance control.
0
2
4
6
8
10
12
14
16
0
1
2
3
4
5
6
7
8
Time (s)
˙α
Fig. 3.17. Precession angle velocity ˙α in balance control.
Position Control
We have set the initial values of the simulation in Table 3.5.
The target parameters are shown in Table 3.6.
The time span of the simulation is from 0 to 16s. The results are shown
in Figures 3.24, 3.25, 3.26 and 3.27.
This simulation demonstrates that the proposed point to point controller
is correct.

3.2 Nonlinear Model
59
0
2
4
6
8
10
12
14
16
−9
−8
−7
−6
−5
−4
−3
−2
−1
0
Time (s)
˙γ
Fig. 3.18. Driving speed ˙γ in balance control.
0
2
4
6
8
10
12
14
16
−10
−8
−6
−4
−2
0
2
4
6
8
Time (s)
˙γ
˙β
˙α
β
Fig. 3.19. Parameters of ˙α, β, ˙β, ˙γ in balance control.
Line Tracking Control
The aim of the simulation is to track a straight line so as to demonstrate the
eﬃciency of the proposed line tracking controller. From the neighborhood of
an origin, the robot is driven along a straight line to reach the point(3,4).
The initial values of the simulation are shown in Table 3.7.
The target parameters are shown in Table 3.8.

60
3 Model-based Control
0
2
4
6
8
10
12
14
16
0.7
0.8
0.9
1
1.1
1.2
1.3
1.4
1.5
1.6
Time (s)
the
Fig. 3.20. Leaning angle β in balance control II.
0
2
4
6
8
10
12
14
16
0
1
2
3
4
5
6
7
8
Time (s)
˙α
Fig. 3.21. Precession angle velocity ˙α in balance control II.
The time span of the simulation is from 0 to 32s. The results are shown
in Figures 3.28, 3.29, 3.30 and 3.31.
Here, large values of k3 and k5 are better, however, they should be less
than the ˙αmax and ˙γmax, respectively.
Despite this, there are some drawbacks in the control process. The ﬁrst is
the system states exhibit highly oscillatory behavior and the second is that
control inputs sometimes need to switch too sharply and fast. Both will cause
diﬃculties for real time control and result in worse performance. To solve

3.2 Nonlinear Model
61
0
2
4
6
8
10
12
14
16
−9
−8
−7
−6
−5
−4
−3
−2
−1
Time (s)
˙γ
Fig. 3.22. Driving speed ˙γ in balance control II.
0
2
4
6
8
10
12
14
16
−10
−8
−6
−4
−2
0
2
4
6
8
Time (s)
˙γ
˙β
˙α
β
Fig. 3.23. Parameters of ˙α, β, ˙β, ˙γ in balance control II.
these problems, we propose to replace the sign functions in the controllers
with tanh functions.
Deﬁnition 3 Tanh(.) is a bipolar function described as follows:
Tanh(x) = 1 −e−k6x
1 + e−k6x
(3.58)
where k6 is a positive scalar constant, which can be designed. Tanh(.) is shown
as in Figure 3.32.

62
3 Model-based Control
β0 [rad] ˙β0 [rad/s] ˙α0
˙γ0
x0[m] y0
π/3
3
4 -3.660381
1.0
1.0
Table 3.5. Initial parameters in position control.
βe [rad] ˙βe [rad/s] xe[m] ye k3 k4
π/2
0
0
0 10 5
Table 3.6. Target and gain parameters in position control.
0
2
4
6
8
10
12
14
16
−0.2
0
0.2
0.4
0.6
0.8
1
1.2
Time (s)
x
Fig. 3.24. Displacement in X.
β0 [rad] ˙β0 [rad/s] ˙α0
˙γ0
x0[m] y0
π/2.1
0.1
4 -3.660381 0.05 0.05
Table 3.7. Initial parameters in line tracking.
Deﬁnition 4 Uanh(.) is a unipolar function described as follows:
Uanh(x) =
1
1 + e−k7x
(3.59)
where k7 is a positive scalar constant, which can be designed. Uanh(.) is shown
as in Figure 3.33.
We substitute Sgn(.) and Θ(.) with Tanh(.) and Uanh(.), respectively.
However, that they have diﬀerent values is a problem, if x = 0. In a real time
experiment, x = 0 very seldom appears and cannot be maintained, because
there is too much noise. Thus, it is safe to perform the suggested substitution
from this point of view.

3.3 Control Implementation
63
0
2
4
6
8
10
12
14
16
−0.2
0
0.2
0.4
0.6
0.8
1
1.2
Time (s)
y
Fig. 3.25. Displacement in Y.
−0.2
0
0.2
0.4
0.6
0.8
1
1.2
0
0.2
0.4
0.6
0.8
1
X (s)
Y
Fig. 3.26. X - Y of origin.
βe [rad] ˙βe [rad/s] xe[m] ye k3 k5
π/2
0
3
4 10 5
Table 3.8. Target and gain parameters in line tracking.
3.3 Control Implementation
An on-board 100-MHZ 486 computer was installed in Gyrover to deal with
on-board sensing and control. A ﬂash PCMCIA card is used as the com-

64
3 Model-based Control
0
2
4
6
8
10
12
14
16
−10
−8
−6
−4
−2
0
2
4
6
8
10
Time (s)
˙γ
˙β
˙α
β
Fig. 3.27. The joint-space variables in position control.
0
5
10
15
20
25
30
35
−0.5
0
0.5
1
1.5
2
2.5
3
Time (s)
x
Fig. 3.28. Line tracking in X direction.
puter’s hard disk and communicates with a stationary PC via a pair of wireless
modems. Based on this communication system, we can download the sensor
data ﬁle from the onboard computer, send supervising commands to Gyrover,
and manually control Gyrover through the stationary PC. Moreover, a radio
transmitter is installed for a human operator to remotely control Gyrover via
the transmitter’s two joysticks. One operator uses the transmitter to control
the drive speed and tilt angle of Gyrover. Hence, we can record the operator’s
driving data.

3.3 Control Implementation
65
0
5
10
15
20
25
30
35
−0.5
0
0.5
1
1.5
2
2.5
3
3.5
4
Time (s)
y
Fig. 3.29. Line tracking in Y direction.
−1 −0.5
0
0.5
1
1.5
2
2.5
3
3.5
4
0
0.5
1
1.5
2
2.5
3
3.5
X (s)
Y
Fig. 3.30. X - Y of in line tracking.
Numerous sensors are installed in Gyrover to measure state variables (Fig-
ure 3.34). Two pulse encoders are installed to measure the spinning rate of the
ﬂywheel and the wheel. Furthermore, we have two gyros and an accelerometer
to detect the angular velocity of yaw, pitch, roll, and acceleration respectively.
A 2-axis tilt sensor has been developed and installed for directly measuring
the lean angle and pitch angle of Gyrover. A gyro tilt potentiometer is used
to calculate the tilt angle of the ﬂywheel and its rate change.
The on-board computer runs on QNX, which is a real-time micro-kernel
OS developed by QNX Software System Limited. Gyrover’s software system

66
3 Model-based Control
0
5
10
15
20
25
30
35
−40
−30
−20
−10
0
10
20
30
40
Time (s)
˙γ
˙β
˙α
β
Fig. 3.31. The joint-space variables in line tracking.
−5
−4
−3
−2
−1
0
1
2
3
4
5
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
X
Y=Tanh(kx)
k=1
k=10
Fig. 3.32. Function Tanh(.).
−5
−4
−3
−2
−1
0
1
2
3
4
5
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
X
Y=Uanh(kx)
k=1
k=10
Fig. 3.33. Function Uanh(.).
is divided into three main programs: (1) communication server, (2) sensor
server, and (3) controller. The communication server is used to communicate
between the on-board computer and a stationary personal computer (PC)
via an RS232, while the sensor server is used to handle all the sensors and
actuators. The controller program implements the control algorithm and com-
municates among these servers. All programs run independently in order to
allow real-time control of Gyrover. To compensate for the friction at the joints,
we adopt the following approximate mathematical model:
F

f = µv ˙q + [µd + (µs −µd)Γ]sgn( ˙q)
(3.60)
where Γ = diag{e(−| ˙qi|/D)}, and

3.3 Control Implementation
67
Gyro
Accelerometer
Sensors Assembly
Radio Receiver
I/O Board
Power Cable
Tilt Servo
Battery
Drive Motor and Encoder
Tilt Potentiometer
Speed Controller
Speed Controller
Fig. 3.34. Hardware conﬁguration of Gyrover.
Fig. 3.35. Experiment in line following control.
µs ∈R3×3 is the static friction coeﬃcient;
We have µv = diag{0.17, 0.15, 0.09}, µd = diag{0.1, 0.1, 0.07}, µs =
diag{0.3, 0.25, 0.1}.
µv ∈R3×3 is the viscous friction coeﬃcient;
µd ∈R3×3 is the dynamic friction coeﬃcient;

68
3 Model-based Control
3.3.1 Vertical Balance
The purpose of this set of experiments is to keep Gyrover balanced. Some
experimental results are shown in Figure 3.36 and Figure 3.37.
Fig. 3.36. Camera pictures in balance control.
Despite several successful examples, control laws sometimes may fail. We
believe it is because the initial condition is set out of the domain D in Propo-
sition 1. With regard to balance control, the initial condition seems to be too
narrow for the constraint, σ < π/2. If we make a small modiﬁcation to u6 as
follows, the controller can be used in a wider range of initial conditions.
u6 = −((2 + k1)(β −π/2) + (3 + 2k1) ˙β + (2 + k1)¨β + h1(t) ˙β + h2(t)u5)/h3(t)
where k1 is a positive number, which can be designed. To prove the subsys-
tem β, ˙β, ¨β is asymptotically stabilized, we consider the following Lyapunov
function candidate
V ∗= (β −π/2)2/2 + ( ˙β + β −π/2)2/2 + (¨β + (1 + k1)( ˙β + β −π/2))2/2.
Its derivative is
˙V ∗= −(β −π/2)2 −k1( ˙β + β −π/2)2 −(¨β + (1 + k1)( ˙β + β −π/2))2.
3.3.2 Position Control
In this experiment, Gyrover is required to move from a Cartesian space point
(x0, y0) to the original point of Cartesian space, where x0 = 3 m and y0 = 4 m.

3.3 Control Implementation
69
0
20
40
60
80
40
60
80
100
120
140
Time (s)
β [deg]
0
20
40
60
80
−200
−100
0
100
200
Time (s)
0
20
40
60
80
−40
−20
0
20
40
Time (s)
0
20
40
60
80
−1
−0.5
0
0.5
1
Time (s)
˙β [deg/sec ]
˙α [deg/sec]
˙γ [deg/sec]
Fig. 3.37. Sensor data in balance control.
We mount a high resolution 2/3” ccd, format CANON COMMUNICATION
CAMERA VC-C1 on a tripod. The camera has been calibrated and we have
mapped the ﬁeld of vision to the Cartesian space coordinate which is anchored
on the ground. The camera has a pixel array of 768(H) × 576(V ). In order to
communicate the data, an interface board (digital I/O) is installed in a PC
and there are wireless modems to connect the PC and Gyrover.
The ﬁrst problem experienced is that the system states exhibited highly
oscillatory behavior and the second is that control inputs sometimes needed to
switch too sharply and fast. Both will cause diﬃculties for real time control
and result in worse performance. To solve these problems, we propose to
replace the sign functions in the controllers with tanh functions.
Let Tanh(.) be a bipolar function described as follows:
Tanh(x) = 1 −e−k6x
1 + e−k6x
(3.61)
where k6 is a positive scalar constant, which can be designed.
Let Uanh(.) be a unipolar function described as follows:
Uanh(x) =
1
1 + e−k7x
(3.62)
where k7 is a positive scalar constant, which can be designed.

70
3 Model-based Control
We substitute Sgn(.) and Θ(.) with Tanh(.) and Uanh(.), respectively.
However, that they have diﬀerent values is a problem, if x = 0. In a real time
experiment, x = 0 very seldom appears and cannot be maintained, because
there is too much noise. Thus, it is safe to perform the suggested substitution
from this point of view.
The trajectory that Gyrover traveled is shown in Figure 3.38. A number
of sensor reading results are shown in Figure 3.39.
0
0.5
1
1.5
2
2.5
3
3.5
4
0
0.5
1
1.5
2
2.5
3
3.5
4
•← start
← end
x (m)
y (m)
Fig. 3.38. Trajectories in point-to-point control.
0
5
10
15
40
60
80
100
120
140
Time (s)
β [deg]
0
5
10
15
−200
−100
0
100
200
Time (s)
0
5
10
15
−50
0
50
Time (s)
0
5
10
15
−20
0
20
40
60
80
Time (s)
˙β [deg/sec ]
˙α [deg/sec]
˙γ [deg/sec]
Fig. 3.39. Sensor data in point-to-point control.

3.3 Control Implementation
71
3.3.3 Path Following
In this experiment, Gyrover is required to travel a straight path which is
about 5 m long. The trajectory that Gyrover traveled is shown in Figure 6.11.
A number of sensor reading results are shown in Figure 6.16.
−3
−2
−1
0
1
2
3
0.5
1
1.5
2
2.5
3
3.5
4
4.5
•← start
← end
x (m)
y (m)
Fig. 3.40. Trajectories in the straight path test.
0
10
20
30
0
50
100
150
Time (s)
β [deg]
0
10
20
30
−200
−100
0
100
200
Time (s)
0
10
20
30
−100
−50
0
50
Time (s)
0
10
20
30
−50
0
50
100
150
Time (s)
˙β [deg/sec ]
˙α [deg/sec]
˙γ [deg/sec]
Fig. 3.41. Sensor data in the straight path test.

4
Learning-based Control
4.1 Learning by CNN
Due to the complexity of the system, it is diﬃcult for us to work out a “com-
plete” analytical model of it. Therefore, in this chapter, we propose using
a machine learning algorithm, Cascade Neural Network (CNN) with node-
decoupled extended Kalman Filtering (NDEKF), to model the robot’s behav-
iors from human control strategy (HAS).
Motivation
Gyrover is a single track mobile vehicle which is inherently unstable in the
lateral direction. With the lack of a wide polygon of support (single-point
contact with the ground), Gyrover has very bad static stability, even though
it is equipped with an internal gyroscope spinning at a high rate. The thin
pneumatic tire which is wrapped around the robot makes it diﬃcult to stand in
a stationary position for a very long time, it will fall on the ground. However,
by tilting the internal gyroscope into diﬀerent orientations, we can indirectly
control the lean angle of the robot, which implies that it is possible for us to
keep the robot to stay in its upright position with a proper control method.
Previous research of Gyrover has been focused on dynamics and control,
including the kinematic constraints and motion equations [74, 107, 115, 6, 5,
113, 114]. However, the robot concept brings a number of challenging prob-
lems in modeling and control because of the highly coupled dynamics, the
nonholonomic constraints and the non-minimum phase behavior of the sys-
tem. The proposed linear state feedback model in [6] only guarantees the local
stability of the system. Moreover, the dynamic model derived has been based
on many assumptions which may not be realistic.
In [5], a linear state feedback controller is developed for stabilizing the
robot to any desired angle, however, this model only applied for the case when
the robot reaches steady state. By consideration of the swinging motion of the
Y. Xu and Y. Ou: Control of Single Wheel Robots, STAR 20, pp. 73–117, 2005.
© Springer-Verlag Berlin Heidelberg 2005

74
4 Learning-based Control
internal mechanism, the model is modiﬁed in [113]. Unfortunately, the models
obtained above are based on the assumption of rolling without slipping, that
is, the robot must be rolling perfectly on the ground. Therefore, these models
are not applicable for the static situation. In the static situation, the coupling
between the wheel and the ﬂywheel becomes much more complicated, which
makes it diﬃcult to derive an analytical model by traditional control methods.
On the other hand, humans are capable of mastering complex and highly
nonlinear control systems. A typical example is car driving. For Gyrover con-
trol, humans are able to control the robot well if enough practice is undertaken.
Thus, we intuitively come up with the idea of machine learning, a model-free
approach to model this kind of human control strategy. This approach is suit-
able for Gyrover control for the following reasons:
•
Gyrover is a complex system, for which it is diﬃcult for us to develop
a complete dynamic model to represent the robot’s behaviors by using
traditional control methods.
•
From a practical point of view, it is equally diﬃcult to model the system
precisely due to some unmodeled factors, such as friction. Friction is an
important issue when we are dealing with the coupling between the wheel
and the spinning ﬂywheel.
•
Although Gyrover is a complex system, humans can control the robot
through a radio transmitter to perform various kinds of task. They do
not need to explicitly model a system in order to control it. Through
interaction with the system and observation of the behaviors of the system,
humans are able to “learn” how to control a system.
•
The learning process is in fact a direct input-output mapping between the
system sensory data and the actuation data. A controller is generated by
using the training data while a human “teacher” controls the system until
the synthesized controller can perform the same way as a human.
4.1.1 Cascade Neural Network with Kalman Filtering
The ﬁeld of intelligent control has emerged from the ﬁeld of classical control
theory to deal with applications which are too complex for classical control
approaches. In terms of complexity, human control strategy lies between low-
level feedback control and high-level reasoning, and encompasses a wide range
of useful physical tasks with a reasonably well-deﬁned numeric input/output
representation.
Here, we introduce a continuous learning architecture for modeling hu-
man control strategies based on a neural network. Since most neural networks
used today rely on rigid, ﬁxed architecture networks and/or with slow gra-
dient descent-based training algorithms, they may not be a suitable method
to model the complex, dynamic and nonlinear human control strategy. To
counter these problems, a new neural network learning architecture is pro-
posed in [78], which combines (1) ﬂexible cascade neural networks, which

4.1 Learning by CNN
75
dynamically adjust the size of the neural network as part of the learning pro-
cess, and (2) node-decoupled extended Kalman Filtering (NDEKF), a faster
converging alternative to exappropriation. This methodology has been proved
which can eﬃciently model human control skills [76], [116] and human sensa-
tion [62].
First of all, let’s discuss the architecture of cascade learning. In cascade
learning, the network topology is not ﬁxed prior to learning, hidden units are
added to an initially minimal network one at a time. This not only frees us
from a prior choice of network architecture, but also allows new hidden units
to assume variable activation functions. That is, each hidden unit’s activation
function no longer needs to be conﬁned to just a sigmoidal nonlinearity. A
priori assumption about the underlying functional form of the mapping we
wish to learn is thus minimized. The whole training process is described below:
1. Initially, no hidden unit exists in the network, only direct input-output
connections. These weights are trained ﬁrst, to obtain a linear relationship,
if any.
2. With no further signiﬁcant decrease in the RMS error (eRMS), a ﬁrst
hidden node will be introduced into the network from a pool of candidate
units. These candidate units are trained independently and in parallel
with diﬀerent random initial weights by using the quickprop algorithm.
3. The best candidate unit will be selected and installed into the network
if no more appreciable error reduction occurs, therefore, the ﬁrst hidden
node is produced.
4. Once the hidden unit is installed, all the input weights of the hidden
unit will be frozen, while the weights to the output unit(s) is/are going
to train again. This allows for a much faster convergence of the weights
during training than a standard multi-layer feedforward network.
5. This process (from step 2 - step 4) is repeated until the eRMS reduces
suﬃciently for the training set or the number of hidden units reaches a
predeﬁned maximum number.
Figure 4.1 illustrates, for example, how a two-input, single-output network
with a bias unit grows with increasing number of hidden nodes.
A cascade neural network with nin input units (including the bias unit), nh
hidden units, and nout, has nw connections (total number of weights) where,
nw = ninnout + nh(nin + nout) + (nh −1)nh/2
(4.1)
In fact, any multi-layer feedforward neural network with k hidden units
arranged in m layers, fully connected between consecutive layers, is a special
case of a cascade network with k hidden units with some of the weights equal
to zero. Thus, this architecture relaxes a prior assumptions about the func-
tional form of the model to be learnt by dynamically adjusting the network
size. We can further relax these assumptions by allowing new hidden units
to have diﬀerent activation functions. The kind of activation functions which

76
4 Learning-based Control
Bias
IN2
IN1
OUT
Bias
IN2
IN1
OUT
H1
H2
Bias
IN2
IN1
OUT
H1
established connection
new connection
new node
add 1st hidden
node
add 2nd hidden
node
and so on
. . . . . .
Fig. 4.1. The cascade learning architecture.
reduces eRMS most will be selected during the process, Sigmoid, Gaussian,
and sinusoidal functions of various frequency are some of the available types
of activation functions we can choose.
While quickprop is an improvement over the standard exappropriation al-
gorithm for adjusting the weights in the cascade network, it still requires many
iterations until satisfactory convergence is reached. When combining cascade
neural networks with node-decoupled extended Kalman ﬁltering (NDEKF),
[76] has shown that this methodology can solve the poor local minima prob-
lem, and that the resulting learning architecture substantially outperforms
other neural network training paradigms in learning speed and/or error con-
vergence for learning tasks important in control problems.
4.1.2 Learning architecture
Denote ωi
k as the input-side weight vector of length ni
w at iteration k, for
i ∈{0, 1, . . . , no}, and,
ni
w =

nin + nh −1
i = 0
nin + nh
i ∈{1, . . . , no}
(4.2)
The NDEKF weight-update recursion is given by, (starting from equa-
tion (3.6) to (3.9), {}’s, ()’s and []’s evaluate to scalars, vectors and matrices
respectively)
ωi
k+1 = ωi
k + {(ψi
k)T (Akξk)}φi
k
(4.3)
where ξk is the no-dimensional error vector for the current training pattern,
ψi
k is the no-dimensional vector of partial derivatives of the network’s output
unit signals with respect to the ith unit’s net input, and,

4.1 Learning by CNN
77
φi
k = P i
kζi
k
(4.4)
Ak =

I +
no
$
i=0
{(ζi
k)T φi
k}[ψi
k(ψi
k)T ]
−1
(4.5)
P i
k+1 = P i
k −{(ψi
k)T (Akψi
k)}[φi
k(φi
k)T ] + ηQI
(4.6)
P i
0 = (1/ηP )I
(4.7)
where ζi
k is the ni
w-dimensional input vector for the ith unit, and P i
k is the
ni
w × ni
w approximate conditional error covariance matrix for the ith unit.
The parameter ηQ is introduced in (3.9) to avoid the singularity problems for
error covariance matrices. Throughout the training, we use ηQ = 0.0001 and
ηP = 0.01.
The vector ψi
k can be computed in this way: let Oi be the value of the
ith output node, ΓO be its corresponding activation function, netOi be its net
activation, ΓH be the activation function for the current hidden unit being
trained, and netH be its net activation. We have,
∂Oi
∂netOj
= 0, ∀i = j
(4.8)
∂Oi
∂netOi
= Γ 
O(netOi), i ∈{1, . . . , no}
(4.9)
∂Oi
∂netH
= wHi · Γ 
O(netOi) · Γ 
H(netH)
(4.10)
where wHi is the weight connecting the current hidden node to the ith output
node.
4.1.3 Model evaluation
The main advantage of modeling a robot’s behaviors by learning, is that no
explicit physical model is required, however, this also presents its biggest
weakness. Since a model is trained by the input-output relationship only, the
lack of a scientiﬁc justiﬁcation degrades the conﬁdence that we can show in
these learnt models. This is especially true when the process we are going
to model is dynamic and stochastic in nature, which is the case in human
control strategy. For a dynamic process, errors may feed back into the model
to produce outputs which are not characteristics of the original process or
make the process unstable. For a stochastic process, a static error criterion
such as RMS error, based on the diﬀerence between the training data and the
predicted model outputs is inadequate to gauge the ﬁdelity of a learnt model
to the source process.
In general, for diﬀerent models, the similarity between a dynamic human
control trajectory and a model-generated one will vary continually, from com-
pletely dissimilar to nearly identical. Furthermore, one cannot expect exact
trajectories for the system and the learnt model, even if equivalent initial

78
4 Learning-based Control
conditions are given. To eﬀectively evaluate the learnt models, we introduce
a stochastic similarity measure proposed in [77]. This method is based on
Hidden Markov Model (HMM) analysis, which is a useful tool for comparing
stochastic, dynamic and multi-dimensional trajectories.
Hidden Markov Model is a trainable statistical model, which consists of a
set of n states, interconnected by probabilistic transitions, each of these states
has some output probability distributions associated with it. A discrete HMM
is completely deﬁned by,
λ = {A, B, π}
(4.11)
where A is the probabilistic ns × ns state transition matrix, B is the L × ns
output probability matrix with L discrete output symbols l ∈{1, 2, . . . , L},
and π is the n-length initial state probability distribution vector for HMM.
Two HMMs (λ1 and λ2) are said to be equivalent if and only if,
P(O|λ1) = P(O|λ2), ∀O
(4.12)
We prefer discrete HMMs rather than continuous or semi-continuous
HMMs, because they are relatively simple in computation and less sensitive
to initial random parameter settings. However, the human control trajectories
we are going to measure are continuous and real-valued functions. In order to
make use of the discrete HMMs, we must convert the data sets into sequences
of discrete symbols On by the following procedures:
1. Normalization
2. Spectral conversion
3. Power Spectral Density (PSD) estimation
4. Vector quantization
The purpose of steps (1) - (3) is to extract some meaningful feature vectors
V for the vector quantizer. In step (4), the feature vectors V are converted to
L discrete symbols, where L is the number of output observable in our HMMs.
In general, assume that we are going to compare the observation sequences
( ¯O1 and ¯O2) from two stochastic processes (Γ1 and Γ2). The probability of
the observation sequences ¯Oi given the HMM model λj, is given by [77],
Pij = P( ¯Oi|λj)1/ ¯Ti,
i, j ∈{1, 2}
(4.13)
where the above equation is normalized with respect to the total numbers of
symbols ¯Ti.
The similarity measure σ between ¯O1 and ¯O2 is,
σ( ¯O1, ¯O2) =
 
P12P21
P11P22
(4.14)
Figure 4.2 illustrates the overall approach to evaluate the similarity be-
tween two observation sequences. The HMMs are trained by each observation
sequence ﬁrst, then we cross-evaluate each observation sequence on the other

4.1 Learning by CNN
79
S1
S4
S3
S2
)
,
(
2
1 O
O
S1
S4
S3
S2
O1
O2
P11
P21
P12
P22
1
2
HMM1:
HMM2:
σ
λ
λ
Fig. 4.2. Similarity measure between ¯
O1 and ¯
O2.
HMM. Based on the four normalized probabilities, the similarity measure σ
can be obtained.
Here, we demonstrate an example of how this similarity measure works.
Figure 4.3 shows four Gyrover control trajectories. Figure 4.3(a) and 4.3(b)
correspond to the tiltup motion control, while Figure 4.3(c) and 4.3(d) cor-
respond to the lateral stabilization control of Gyrover. We applied the HMM
similarity measure across these four trajectories. We might expect that the
trajectories of the same motion should have a relatively high similarity and
that any two trajectories which were generated from diﬀerent kinds of motion
should have a low similarity value. We summarize the results in Table 4.1.
Tiltup #1 Tiltup #2 Vertical stab. #1 Vertical stab. #2
Tiltup #1
1.000
0.6306
0.0524
0.1234
Tiltup #2
0.6306
1.000
0.0615
0.0380
Vertical stab. #1
0.0524
0.0615
1.000
0.4994
Vertical stab. #2
0.1234
0.0380
0.4994
1.000
Table 4.1. Similarity measures between diﬀerent control trajectories.
From Table 4.1, it is clear that this similarity measure can accurately
classify dynamic control trajectories from the same type of motion, while
discriminating those from diﬀerent motions by giving a low similarity value.
This similarity measure can be applied towards validating a learned model’s
ﬁdelity to its training data, by comparing the model’s dynamic trajectories in
the feedback loop to the human’s dynamic control trajectories.

80
4 Learning-based Control
0
1
2
3
4
5
6
7
8
120
130
140
150
160
170
180
190
200
time
Tilt command
 Tiltup control trajectory #1 
(a) Tiltup 1
0
1
2
3
4
5
6
7
8
120
130
140
150
160
170
180
190
200
time
Tilt command
Tiltup control trajectory #2
(b) Tiltup 2
0
1
2
3
4
5
6
7
8
165
170
175
180
185
190
195
time
Tilt command
Vertical stabilization control trajectory #1
(c) Vertical balancing 1
0
1
2
3
4
5
6
7
8
165
170
175
180
185
190
195
time
Tilt command
Vertical stabilization control trajectory #2
(d) Vertical balancing 2
Fig. 4.3. Control data for diﬀerent motions.
4.1.4 Training procedures
Fist of all, we have made two assumptions for the training data provided for
the learning process:
1. Reliable training set. Since learning is a kind of high-level, model free
“teaching by showing” approach, the stability or robustness of the learnt
model heavily depends on the operating skills of a “human teacher”, in
order to provide reliable and stable control. Therefore, throughout the
teaching process, we assume that the operator is skillful and experienced
enough to master the robot. That is, the training data can fully reﬂect the
skills in a particular robot’s behavior. Besides the quality of the training
data, the quantity of the data points is equally important. If the training
set is in a larger scale, a more complete skill can be described.

4.1 Learning by CNN
81
2. Injective mapping. Another important issue is about the mappings be-
tween inputs and outputs in a static map. Figure 4.4 shows a human
control strategy for the lateral balancing behavior, it is not diﬃcult to ﬁg-
ure out that the control of the ﬂywheel is always switching (a very sharp
change). That is, at a short moment ago, the command is positive, but in
the next moment, the command will change to negative. Unfortunately,
the switching problem causes very similar inputs to be mapped to radi-
cally diﬀerent outputs, which is diﬃcult for the cascade neural network
to adapt to, Figure 4.5. To ensure that there will be a correct mapping,
enough time-delayed histories should be provided in the training data
set. In our cascade network training, we will provide at least 20 pieces of
history data (ni ≥20) to guarantee the infectiveness of the mapping.
0
5
10
15
20
25
30
35
40
−10
−8
−6
−4
−2
0
2
4
6
8
10
Change of control
time(sec)
Change of control to tilt motor (verticle stabilization)
Fig. 4.4. Switchings in human control of the ﬂywheel.
For each model, we process the training data as follows:
1. Removal of irrelevant data
Let [t, t+tm] denote an interval of time, in seconds, that a human operator
has given an inappropriate command during the experiment. Then, we
cut the data corresponding to the time interval [t −1, t + tm] from the
training data. In other words, we not only remove the irrelevant data from
the training set, but also the second data leading to the inappropriate

82
4 Learning-based Control
Input space
Output space
(
)
1
 
+
t
u
(
)
,1
,
 
-
t
t
Ξ
...
Γ
Fig. 4.5. Similar inputs can be mapped to extreme diﬀerent outputs if switching
occurs.
command time interval. This ensures that the cascade model does not
learn control behaviors that are potentially destabilizing.
2. Normalization
We normalize each input dimension of the training data, such that all the
input in the training data falls inside the interval [−1, 1].
3. Generate time-shifted data
As mentioned in the previous section, we need to provide enough time-
delayed values of each state and control variable such that the model is
able to build necessary derivative dependencies between the inputs and
outputs. In our cascade network training, we will provide 20 pieces of
history data.
4. Randomization
Finally, we randomize the input-output training vectors and select half
for training, while reserving the other half for testing.
The sampling rate of the training data is 40Hz and a typical training set
will consist of approximately 10,000 data points.
4.2 Learning by SVM
4.2.1 Support Vector Machines
Support Vector Machine
Originally, SVM was developed from classiﬁcation problems. It was then, ex-
tended to regression estimation problems, i.e., to problems related to ﬁnd the
function y = f(x) given by its measurements yi with noise at some (usually
random) vector x
(y1, x1), ..., (yl, xl).
(4.15)

4.2 Learning by SVM
83
where each of them is generated from an unknown probability distribution
P(x, y) containing the underlying dependency. (Here and below, bold face
characters denote vectors.)
In this paper, the term SVM will refer to both classiﬁcation and regression
methods, and the terms Support Vector Classiﬁcation (SVC) and Support
Vector Regression (SVR) will be used for speciﬁcation. In SVR, the basic idea
is to map the data X into a high-dimensional feature space F via a nonlinear
mapping Φ, and to do linear regression in this space [108].
f(x) = (ω · Φ(x)) + b
with Φ : Rn →F, ω ∈F,
(4.16)
where b is a threshold. Thus, linear regression in a high dimensional (fea-
ture) space corresponds to nonlinear regression in the low dimensional input
space Rn. Note that the dot product in Equation (4.16) between ω and Φ(x)
would have to be computed in this high dimensional space (which is usually
intractable). If we are not able to use the kernel that eventually leaves us
with dot products that can be implicitly expressed in the low dimensional
input space Rn. Since Φ is ﬁxed, we determine ω from the data by minimizing
the sum of the empirical risk Remp[f] and a complexity term  ω 2, which
enforces ﬂatness in feature space
Rreg[f] = Remp[f] + λ  ω 2=
l
$
i=1
C(f(xi) −yi) + λ  ω 2,
(4.17)
where l denotes the sample size (x1, ..., xl), C(.) is a loss function and λ is a
regularization constant. For a large set of loss functions, Equation (4.17) can
be minimized by solving a quadratic programming problem, which is uniquely
solvable [98]. It can be shown that the vector ω can be written in terms of the
data points
ω =
l
$
i=1
(αi −α∗)Φ(xi),
(4.18)
with αi, α∗
i being the solution of the aforementioned quadratic programming
problem [108]. The positive Lagrange multipliers αi and α∗
i are called support
values. They have an intuitive interpretation as forces pushing and pulling
the estimate f(xi) towards the measurements yi [27]. Taking Equation (4.18)
and Equation (4.16) into account, we are able to rewrite the whole problem
in terms of dot products in the low dimensional input space
f(x) = %l
l=1(αi −α∗
i )(Φ(xi) · Φ(x)) + b
= %l
l=1(αi −α∗
i )k(xi, x) + b.
(4.19)
In Equation (4.19), we introduce a kernel function k(xi, xj) = Φ(xi)·Φ(xj).
As explained in [15], any symmetric kernel function k satisfying Mercer’s
condition corresponds to a dot product in some feature space.
For a more detailed reference on the theory and computation of SVM,
readers are referred to [27].

84
4 Learning-based Control
4.2.2 Learning Approach
The skill that we are considering here is the control strategy demonstrated by
a human expert to obtain a certain control target. For example, in controlling a
robotic system, a human expert gives commands by way of a controller, such as
a joystick, and the robot executes the task. The desired trajectory of the robot
given by an expert through a joystick reﬂects the expert’s control strategy. The
goal of the human control strategy learning here is to model the expert control
strategy and according to current states select one command that represents
the most likely human expert strategy. Here, we consider the human expert
actions as the measurable stochastic process and the strategy behind it as the
mapping between the current states and commands. An SVR is employed to
represent human expert strategy for the given task, and the model parameters
are sought through an oﬀ-line learning process. This method allows human
experts to transfer their control strategy to robots. The procedure for SVM-
based control strategy learning can be summarized as follows:
1. Representing the control strategy by an SVM: Choosing a suitable kernel
and structure of an SVM for characterizing the control strategy.
2. Collecting the training data: Obtaining the data representing the control
strategy we want to model.
3. Training the model: Encoding the control strategy into an SVM.
4. Finding the best human performance: Learning/transferring the control
strategy.
For training an SVM learning controller, the system states will usually be
treated as the learning inputs and the control inputs/commands will be the
learning outputs.
Training Example Collection
An SVM does not require a large amount of training samples as do most
ANNs. Scholkopf [23] pointed out that the actual risk R(w) of the learning
machine is expressed as:
R(w) = 
 1
2fw(x) −ydP(x, y).
(4.20)
where P(x, y) is the same distribution deﬁned in (4.15). The problem is that
R(w) is unknown, since P is unknown.
The straightforward approach to minimize the empirical risk,
Remp(w) = 1
l
l
$
i=1
|fw(xi) −yi,
turns out not to guarantee a small actual risk R(w), if the number l of training
examples is limited. In other words: a small error in the training set does

4.2 Learning by SVM
85
not necessarily imply a high generalization ability (i.e., a small error on an
independent test set). This phenomenon is often referred to as overfitting. To
solve the problem, novel statistical techniques have been developed during the
last 30 years. For the learning problem, the Structure Risk Minimization
(SRM) principle is based on the fact that for any w ∈Λ and l > h, with a
probability of at least 1-η, the bound
R(w) ≤Remp(w) + Φ(h
l , log(η)
l
)
(4.21)
holds, where the confidence term Φ is deﬁned as
Φ(h
l , log(η)
l
) =

h(log 2l
h + 1) −log(η/4)
l
.
The parameter h is called the VC (V apnik-Chervonenkis) dimension of a
set of functions, which describes the capacity of a set of functions. Usually, to
decrease the Remp(w) to some bound, most ANNs with complex mathematical
structure have a very high value of h. It is noted that when n/h is small (for
example less than 20, the training sample is small in size), Φ has a large
value. When this occurs, performance poorly represents R(w) with Remp(w).
As a result, according to the SRM principle, a large training sample size is
required to acquire a satisfactory learning machine. However, by mapping the
inputs into a feature space using a relatively simple mathematical function,
such as a polynomial kernel, an SVM has a small value for h, and at the same
time maintains the Remp(w) in the same bound. To decrease R(w) with Φ,
therefore, requires small n, which is enough also for small h.
Training process
The ﬁrst problem, before beginning training, is to choose between SVR or
SVC; the choice being dependent on special control systems. For example, our
experimental system Gyrover has the control commands U0 and U1. Their
values are scaled to the tilt angle of the ﬂywheel and the driving speed of
Gyrover respectively. Thus, for this case, we will choose SVR. However, for
a smart wheelchair system, for instance, the control commands 1, 2, 3 and 4
correspond to “go ahead”, “turn left”, “turn right”, and “stop” respectively.
Since this is a classiﬁcation problem, it is better to choose SVC.
The second problem is to select a kernel. There are several kinds of kernel
appropriate for an SVM [27]. The main function of kernels in an SVM is to
map the input space to a high dimensional feature space, and at that space the
feature elements have a linear relation with the learning output. In most cases,
it is diﬃcult to set up this kind of linear relations. This is because usually we
do not know the mathematical relation between the input and output. Thus,
it is better to test more kernels and choose the best one.

86
4 Learning-based Control
The third problem is about “scaling”. Here, scaling refers to putting each
Column’s data into a range between −1 and 1. It is important to void the
outputs if they are seriously aﬀected by some states, for the “unit’s” sake.
Moreover, if some states are more important than others, we may enlarge
their range to emphasize their eﬀect.
Time Consideration
An SVM usually requires more time in obtaining the optimal weight matrix for
the same set of training samples than a general neural network learner, such
as Exappropriation neural networks (BPNN). An SVM determines the weight
matrix through an optimization seeking process, whereas an ANN determines
it by modifying the weights matrix backward from the diﬀerences between the
actual outputs and estimated outputs. [15] and Vapnik [108] show how training
an SVM for the pattern recognition problem (for a regression problem it is
similar but a little more complex) leads to the following quadratic optimization
problem (QP)OP1.
(OP1) minimize :
W(α) = −
l
$
i=1
αi + 1
2
l
$
i=1
l
$
j=1
yiyjαiαjk(xi, xj)
(4.22)
subject to :
l
$
i=1
yiαi = 0
∀i : 0 ≤αi ≤C
(4.23)
The number of training examples is denoted by l. α is a vector of l variables,
where each component αi corresponds to a training example (xi, yi). C is
the boundary of αi. The solution of OP1 is the vector ˆα for which (4.22) is
minimized and the constraints (4.23) are fulﬁlled. Deﬁning the matrix ˜Q as
( ˜Q)ij = yiyjk(xi, xj), this can be equivalently written as
minimize :
W(α) = −αT 1 + 1
2αT ˜Qα
(4.24)
subject to :
αT y = 0
, 0 ≤α ≤C1
(4.25)
Since the size of the matrix ˜Q is l2, the size of the optimization problem
depends on the number of training examples. However, most general ANNs,
(such as BPNN) have linear relationship with the sample size. Because of this,
SVMs usually need more training time than most other ANNs. Moreover, if
the size of the sample is large, this phenomenon is much more serious. For
example, for a size of 400 example set, an SVM needs about 10 minutes to
complete the training process and a BPNN requires about 1 minute will ﬁnish.
If the size of the sample reaches 10, 000, an SVM will need more than 100 hours
to complete, whereas, the BPNN just requires about 25 minutes.

4.2 Learning by SVM
87
Learning Precise Consideration
In practice applications, the control process of an SVM learner is much
smoother than general ANN learners. The reason for this is that many ANNs
and HMM methods exhibit the local minima problem, whereas for an SVM the
quadratic property guarantees the optimization functions are convex. Thus it
must be the global minima.
Moreover, for the class of dynamically stable, statically unstable robots
such as Gyrover, high learning precision is required and very important. This
is because the learning controller will be returned to control the robot and
form a new dynamic system. Larger errors in control inputs will cause sys-
tem instability and control process failure. During our experiments, we always
required several training sessions to produce a successful general ANN con-
troller. However, after training the SVM learning controller it always worked
well immediately.
4.2.3 Convergence Analysis
Here, we provide the convergence analysis of the learning controller, but not
the learning convergence of SVMs. From the practice point of view, we con-
sider the local asymptotical convergence instead of global or semi-global con-
vergence. This is because, in the training process, we can only collect data
in a region of systems variables, called the “working space”. It is a set of all
the points that the system states can reach. Furthermore, we deﬁne “region
of operation” D as a meaningful working region, a subset of “working space”.
Usually, region of operation D is a compact subset of the full space Rn with its
origin. Thus, with these local data, we almost can not obtain a highly precise
training result in the full space Rn.
In fact, for this class of learning controller, it can only drive the system
to converge into a bounded small neighborhood of the origin (or desirable
equilibrium). This is because of the SVM model learning error. For a local
asymptotical convergence, the region of attraction is a critical issue. The re-
gion of attraction may not be larger or equal to “region of operation” D, but
should be desirable large.
Problem Statement
˙x = f(x, u),
(4.26)
where
x ∈Rn
system states
u ∈Rm (m ≤n)
control inputs
˙x
time derivatives of the states x
f(·) : Rn+m →Rn unknown nonlinear function

88
4 Learning-based Control
x(0)
u(t)
x(t)
Plant
Human Control
Fig. 4.6. The practical system and the human control from a dynamic system.
The control objective can be described as: ﬁnd a continuous control law u =
u(x), such that all of the states of the above system (4.26) asymptotically
tend to a bounded small neighborhood of the origin.
Assume that a practical system has been well controlled by a human ex-
pert and the all states of the system have been put into in a small neighbor-
hood of origin. The practical system and the human control form a dynamic
system shown in Figure 4.6. Usually, a continuous-time control process is
approximately considered as a process of fast discrete-time sampling. This
continuous-time nonlinear control system is approximately described by the
diﬀerence equation of the form
x(t + 1) = fx(x(t), uh(t))
(4.27)
where x = [x1, x2, ..., xn]T ∈Rn is the state vector, uh ∈Rm is the human
control vector, fx = [f1, f2, ..., fn]T : Rn+m →Rn is a set of unknown nonlin-
ear functions. We assume throughout the paper that the state vector x of the
system can be measured.
There is a learning controller for the dynamic system (4.27), which is ob-
tained by oﬀ-line learning from a data table produced by the human expert
demonstration, shown in Figure 4.7. The diﬀerence Equation (4.28) approxi-
mately described the learning controller























ˆx1(t + 1) = ˆf1(x(t), u(t))
= f1(x(t), u(t)) + e1(x(t), u(t))
ˆx2(t + 1) = ˆf2(x(t), u(t))
= f2(x(t), u(t)) + e2(x(t), u(t))
...
ˆxn(t + 1) = ˆfn(x(t), u(t))
= fn(x(t), u(t)) + en(x(t), u(t))
u1(t + 1) = fn+1(x(t), u(t))
...
um(t + 1) = fn+m(x(t), u(t)),
(4.28)
where ˆx = [ˆx1, ˆx2, ..., ˆxn]T ∈Rn is the estimation for the state vector x
and ˆfx = [ ˆf1, ˆf2, ..., ˆfn]T
: Rn+m →Rn is an estimation for fx. e =
[e1, e2, ...en]T = fx −ˆfx is a model error. u = [u1, u2, ..., um]T ∈Rm is the

4.2 Learning by SVM
89
estimation for the human control vector uh and fu = [fn+1, fn+2, ..., fn+m]T :
Rn+m →Rm is the estimation for next time human control.
Neural networks
x(t)
u(t)
x(t+1)
u(t+1)
Fig. 4.7. A learning controller.
If we use the learning controller u to control the system, we have a
new closed-form continuous-time dynamic system and it is approximately de-
scribed by the diﬀerence equation of the form
x(t + 1) = fx(x(t), u(t))
u(t + 1) = fu(x(t), u(t)).
(4.29)
Furthermore, we let X = [xT uT ]T and f = [f T
x f T
u ]T , then
X(t + 1) = f(X(t))
(4.30)
and let ˆX = [ˆxT uT ]T and ˆf = [ ˆf T
x f T
u ]T , then
ˆX(t + 1) = ˆf( ˆX(t))
(4.31)
where (4.31) is an estimation for (4.30).
The aim of this section is twofold: to formulate conditions for the system
(4.30) to be stable and to determine the domain of attraction, if it is stable.
Lyapunov Theory
A very important tool in the stability analysis of discrete dynamic systems is
given by Lyapunov’s theory [65], [61].
Deﬁnition 1: A function V (X) is said to be positive deﬁnite in a region W
containing the origin if (i) V (0) = 0. (ii) V [X(t)] > 0 for all x ∈W, X = 0.
Deﬁnition 2: Let W be any set in Rn containing the origin and V : Rn →
R. We say that V is a Lyapunov function of Equation (4.30) on W if (i) V is
continuous on Rn. (ii) V is positive deﬁnite with respect to the origin in W.
(iii) ∆V (t) ≡V [X(t + 1)] −V [X(t)] ≤0 along the trajectory of (4.30) for all
X ∈W.

90
4 Learning-based Control
The existance of a Lyapunov function assures stability as given by the
following theorem.
Theorem 1: If V is a Lyapunov function of (4.30) in some neighborhood
of an equilibrium state X = 0, then X = 0 is a stable equilibrium.
If in addition −∆V is positive deﬁnite with respect to X = 0, then the
origin is asymptotically stable.
So far, the deﬁnition of stability and asymptotical stability are in terms
of perturbations of initial conditions. If the model error e is small, one hopes
that at least qualitatively, the behavior of the original system and that of the
perturbed one will be similar. For the exact relation, stable under perturba-
tionsneeds to be deﬁned [94].
Deﬁnition 3: Let X(X0, t) denote the solution of (4.30) with the initial
condition X0 = X(X0, 0). The origin X = 0 is said to be stable under pertur-
bations if for all - > 0 there exists δ1(-) and δ2(-) such that ||X0|| < δ1 and
||e(t, X)|| < δ2 for all k > 0 imply X(X0, t) < - for all t ≤0.
If in addition, for all - there is an r and a T(-) such that ||X0|| < r and
||e(t, X)|| < δ2(-) for all t > 0 imply ||X(X0, t)|| < - for all t > T(-), the
origin is said to be strongly stable under perturbations(SSUP).
Strongly stable under perturbations (SSUP) means that the equilibrium is
stable, and that states started in Br ⊂Ω actually converge to the error bound
at limited time. Ω is called a domain of attraction of the solution (while the
domain of attraction refers to the largest such region, i.e., to the set of all
points such that trajectories initialed at these points eventually converge to
the error bound.
With this in mind the following theorem [61], [94] can be stated:
Theorem 2: If f is Lipschitz continuous in a neighborhood of the equilib-
rium, then the system (4.30) is strongly stable under perturbations iﬀ it is
asymptotically stable.
In this paper, Support Vector Machine (SVM) will be considered as a
neural network structure to learn the human expert control process. In the
next section, a rough introduction to the SVM learner that we will use, will
be provided.
Convergence Analysis
There are many kernels that satisfy the Mercer’s condition as described in
[27]. In this paper, we take a simple polynomial kernel in Equation (4.19):
K(Xi, X) = ((Xi · X) + 1)d,
(4.32)
where d is user deﬁned (Taken from [108]).
After the oﬀ-line training process, we obtain the support values (α and
α∗) and the corresponding support vectors. Let Xi be sample data of X.
By expanding Equation (4.19) according to Equation (4.32), Let ˆf(X) =

4.2 Learning by SVM
91
[ ˆf1, ˆf2, ... ˆfm+n]T be a vector. ˆfi is a nonhomogeneous form of degree d in
X ∈Rn+m (containing all monomials of degree ≤d)
ˆfi =
$
0≤k1+k2+...+kn+m≤d
cjxk1
1 xk2
2 ...xkn
n ukn+1
1
...ukn+m
m
,
(4.33)
where k1, k2, ..., kn+m are nonnegative integers, and cj ∈R are weighting
coeﬃcients. j can be 1, 2, ..., M, where M = (n+m+d
n+m
). Then,
ˆf(X) = C + A
X + g(X),
(4.34)
where C = [c1, c2, ...cm+n]T is a constant vector, A
 ∈R(n+m)×(m+n) is a co-
eﬃcient matrix for the degree 1 in X and g(X) = [g1(X), g2(X), ...gn+m(X)]T
is a vector. gi(X) is a multinomial of degree ≥2 of X.
Assume that we had built m + n number of multiple-input-one-output
SVM models. According to [108], SVM can approximate to a model in any
accuracy, if the training data number is large enough, i.e. for any - > 0, if ¯n
is the sample data number and e is the model error, there exists a N > 0,
such that if ¯n > N, e < -. The following assumptions are made for the SVM
models.
Assumption 1: For system (4.30), in the region D, the number of sample
data is large enough.
Remark 3.1: Assumption 1 is usually required in control design with neural
networks for function approximation [31], [53]. Then from the above analysis,
we have assumption 2.
Assumption 2: For system (4.30), in the region D, the learning precision
is high.
Hence, if the model (4.31) is suﬃciently accurate, according to Equation
(4.34), the system (4.30) can be transformed to the Equation (4.35)
X(t + 1) = C + A
X + g(X),
(4.35)
Since the origin is an equilibrium point of the system, we have C =
[0, 0, ...0]T and then
X(t + 1) = A
X + g(X),
(4.36)
Thus, we can use the following theorem to judge the system (4.30) is strongly
stable under perturbations (SSUP) or not.
Theorem 3: For the system (4.30), with assumptions 1 and 2 being satisﬁed
in the region D, if −A = (I −A
) is a positive deﬁnite matrix and I is a
(n + m) × (n + m) identical matrix, then the closed-form system (4.30) is
strongly stable under perturbations (SSUP).
Proof: Let V = XT X. Then, V is positive deﬁnite. By diﬀerentiating V
along the trajectories of X one gets
∆V = ∂V
∂X ∆X = 2X∆X.
(4.37)

92
4 Learning-based Control
Substituting (4.36) and (4.37) into it, then
∆V = 2X(A
 −I)X + 2Xg(X) = 2XAX + 2Xg(X).
If we let µ be the smallest eigenvalue of the matrix −A, then we have µ|z|2 ≤
zT (−A)z,
∀z. The properties of g(X) imply the existence of a function
ρ(X) such that limX→0ρ(X) = 0 and |g(X)| ≤ρ(X)|X|. We can then get the
estimate
∆V = 2XAX + 2Xg(X) ≤−|X|2(µ −2ρ(X)).
The fact that ρ tends to zero as X tends to zero implies the existence of a
constant r > 0 such that ρ(X) < µ/2 whenever |X| ≤r. It follows that −∆V
is positive deﬁnite in |X| < r and the system (4.30) is asymptotically stable.
Moreover, since f is Lipschitz continuous and according to Theorem 2, we
know that the system is strongly stable under perturbations (SSUP).
Remark 3.2: The technique used in the proof can in principle give an ap-
proach in estimating the domain of attraction Ω. This value of r will however
often be quite conservative. The arrangement of this is a critical property to
evaluate the practical value of the SVM learning controller. In the next part
of this section, we will provide a better and more practical method to estimate
the domain of attraction Ω.
Computation of Stability Region
The problem of determining stability regions (Subsets of domain of attrac-
tion) or robust global stability [106], of nonlinear dynamical systems is of
fundamental importance in control engineering and control theory.
Let ∆X(t) = X(t + 1) −X(t), we transform the system (4.36) to
∆X = AX + g(X),
(4.38)
where we assumed that A is a negative deﬁnite matrix, i.e., all eigenvalues of A
have strictly negative real parts. Thus, the problem in this part is to estimate
the domain of attraction of X = 0. The main tool in achieving this goal is the
use of an appropriate Lyapunov function. In fact, there are almost an inﬁnite
kind of Lyapunov functions that can be applied for this aim. However, a large
number of experiments show that the type of quadratic Lyapunov functions,
which are chosen, usually can work out a desirable stability region if an SVM
learning controller has ﬁne performance in practical control experiments. In
the following we will address the quadratic Lyapunov function.
V (X) = XT PX,
(4.39)
where P is a positive deﬁnite symmetric (n + m) × (n + m) matrix. Assuming
that the matrix
Q = −(AT P + PA),
(4.40)

4.2 Learning by SVM
93
is positive deﬁnite, V (X) is a valid Lyapunov function for the linear system.
The set
Ωc = {X|V (X) ≤c}, c > 0,
(4.41)
is contained in the unknown domain of attraction if the inequality
∆V (X) = XT (AT P + PA)X + 2XT Pg(X) = −XT QX + 2XT Pg(X) < 0,
(4.42)
is valid for all
X ∈Ωc, X = 0
(4.43)
[66]. The problem is to maximize c, which is actually an optimization problem,
such that (4.42) and (4.43) are satisﬁed. The corresponding set Ωc is the
largest subset of the main attraction which can be guaranteed with the chosen
Lypunov function. So far, this goal can be divided into issues: to choose a
suitable quadratic Lyapunov function for maximizing c and to compute c.
For the ﬁrst issue, we need the following theorem [37].
Theorem 4: Let all eigenvalues of matrix A have strictly negative real parts.
Then, for every symmetric positive deﬁnite matrix Q, there is a symmetric
positive deﬁnite P such that
AT P + PA = −Q.
If Q is positive deﬁnite, then so is P. The proof is in [37]. At the same time, if
we choose a symmetric positive deﬁnite P, the unique Q may not be positive
deﬁnite. Thus, we transfer the ﬁrst issue to design a proper Q, to maximize
c. Since Q is a symmetric (n + m) × (n + m) matrix, we have
¯N = (n + m)(n + m + 1)
2
(4.44)
independent parameters that we can design. How to choose the ¯N parameters
is open problem and there is almost no research work about it that we have
found. We proposed a numerical approach to choose the ¯N parameters to
maximize c. We neglect detail of it here and go forward to the next issue
problem, but have left it in the experimental part to illustrate it with an
example.
The second problem can be described as if we have a Q, how to work out
c. There are a number of work about the second issue problem [33], [66] and
[106]. We follow the approach in [106]. If we have chosen a Q, then there is a
unique symmetric and positive deﬁnite P as,
P =





p1,1
p1,2
· · ·
p1,n+m
p1,2
p2,2
· · ·
p2,n+m
...
...
...
...
p1,n+m p2,n+m · · · pn+m,n+m.





(4.45)

94
4 Learning-based Control
P can be eﬃciently decomposed by means of the Cholesky Factorization into
a lower and upper triangular matrices with the following equation.
P = LT L.
(4.46)
Here the matrix L is an upper triangular matrix in the form
L =







l1,1 l1,2 l1,3 · · ·
l1,n+m
0 l2,2 l2,3 · · ·
l2,n+m
0
0 l3,3 · · ·
l3,n+m
...
...
... ...
...
0
0 · · ·
0 ln+m,n+m.







(4.47)
The elements of the matrix L are calculated as follows.
l1,1 = √p1,1,
l1,j = p1.,j
l1,1 ,
j = 2, ..., n + m
li,i =

(pi,i −%i−1
k=1 l2
k,j),
j = 2, ..., n + m
li,j =
1
li,i (pi,j −%i−1
k=1 lk,ilk,j), i = 2, ..., n + m −1, j = i + 1, ..., n + m.
(4.48)
We transform the system with
ˆX = LX,
(4.49)
where ˆX is the new state vector, into a diﬀerent system, whose state space
diﬀerence equations are given by the following equation
∆ˆX = LAL−1 ˆX + Lg(L−1 ˆX) = ˆA ˆX + ˆg( ˆX).
(4.50)
For the system, the state vector ˆX = 0 is also an equilibrium point. We can
now describe the same Lyapunov function according to the new state vector
ˆX,
V (X) = XT PX = XT LT LX = ˆXT ˆX = ˆV ( ˆX).
(4.51)
According to the Lyapunov stability theory we will investigate the asymptotic
stability of the system
∆ˆX = ˆA ˆX + ˆg( ˆX)
(4.52)
with the Lyapunov function
ˆV ( ˆX) = 2 ˆXT ˆX = 2 ˆXT ( ˆA ˆX + ˆg( ˆX)).
(4.53)
It must be strictly negative in the domain of attraction Ωc. We assume that
the degree of ˆV ( ˆX) is k. It is obvious that the polynomial
p( ˆX) = −∆ˆV ( ˆX)
(4.54)
must be strictly positive in Ωc. Furthermore, we transform the cartesian co-
ordinates into polar coordinates with the following replacements.

4.2 Learning by SVM
95
ˆX1
= r cos θ1 cos θ2 · · · cos θn+m−2 cos θn+m−1
ˆX2
= r cos θ1 cos θ2 · · · cos θn+m−2 sin θn+m−1
ˆX3
= r cos θ1 cos θ2 · · · cos θn+m−3 sin θn+m−2
...
...
...
ˆXi
= r cos θ1 cos θ2 · · · cos θn+m−i sin θn+m−i+1
...
...
...
ˆXn+m = r sin θ1,
(4.55)
where r is the radius and θ = [θ1, θ2, ..., θn+m−1]T are the angles. In this case
ˆV ( ˆX) = ˆV (r, θ) := ˆV (y) = r2,
(4.56)
where y is the vector [r, θ1, θ2, ..., θn+m−1]T . The function p( ˆX) can be repre-
sented with the equation (4.57).
p( ˆX) = p(r, θ) = p(y) = akrk + ak−1rk−1 + ... + a2r2,
(4.57)
where ai, i = 2, ..., k, is the function of the angles θ.
Next, we introduce the Theorem of Ehilich and Zeller and relative mate-
rials for working out c [106]. In the following, J = [a, b] denotes a nonempty
real interval with J ⊂R. We deﬁne the set of Chebychev points at interval J
for a given integer N > 0 by x(N, J) := {x1, x2, ..., xN}, where
xi := a + b
2
+ b −a
2
cos((2i −1)π
2N
),
i = 1, 2, ..., N.
(4.58)
Let pm be the set of polynomials p in one variable with deg p ≤m.
Then, we extend the deﬁnitions in one variable to several variables using
the following replacements. The interval J is replaced by
ˆJ = [a1, b1] × [a2, b2] × ... × [an+m, bn+m],
(4.59)
which represents a hyperrectangle . For the degree of p with respect to the
i-th variable xi we introduce the abbreviation mi and the set of Chebychev
points in ˆJ is given by
x( ˆN, ˆJ) := x(N1, [a1, b1]) × x(N2, [a2, b2]) × ... × x(Nn+m, [an+m, bn+m]),
(4.60)
where Ni is the number of Chebychev points in the interval [ai, bi].
Using the theorem of Ehilich and Zeller, we can ﬁnd out with the following
inequality whether the polynomial p(y) is strictly in a sphere with radius r.
(K + 1)py( ˆ
N,ˆj)
min
−(K −1)py( ˆ
N,ˆj)
max
> 0,
(4.61)
where the angles vary in the interval [0, 2π]; the radius varies in the interval
[0, r];

96
4 Learning-based Control
pI
min := minx∈Ip(x), pI
max := maxx∈Ip(x);
and
K =
n+m

i=1
C(mi
Ni
)
under the condition Ni > mi, i = 1, 2, ..., n + m and C(q) = [cos( q
2π)]−1 for
0 < q < 1. If the inequality (4.61) is valid, the following inequality are also
valid.
(K + 1)p(y[i]) −(K −1)p(y[j]) > 0,
i, j = 1, 2, ..., ˆN
(4.62)
with
py( ˆ
N,ˆj)
min
≤p(y[i]) ≤py( ˆ
N)
max ,
i = 1, 2, ..., j, ..., ˆN
(4.63)
where y[i], y[j] ∈y( ˆN, ˆj) are two Chebychev points. For ˆN Chebychev points
we have ˆN 2 inequalities of type (4.62) which are equivalent to (4.61). (4.62)
provide us the suﬃcient conditions for the strict positive of polynomial p(y).
Moreover,
p(r, θ[i]) > 0,
i = 1, 2, ..., ˆN
(4.64)
give us the necessary conditions for the strict positive of polynomial p(y).
If the inequalities (4.62) are numerically solved, an inner approximation r∗
in
to the maximum radius r∗is determined. The solution of inequalities (4.64)
give us an outer approximation r∗
out to r∗. In this case
r∗
in ≤r∗≤r∗
out
(4.65)
is valid. The maximum level of the surface c∗is equal to (r∗)2 and gives the
set Ωc∗. c∗lies
c∗
in ≤c∗≤c∗
out
(4.66)
and Ωc∗is the largest subset of the domain of attraction with a given Lya-
punov function. Now, we will provide the experimental work as an example
to illustrate the algorithm.
4.2.4 Experiments
Experimental System – Gyrover
The single-wheel gyroscopically-stabilized robot, Gyrover, takes advantage of
the dynamic stability of a single wheel. Figure 4.8 shows a photograph of the
third Gyrover prototype.
Gyrover is a sharp-edged wheel with an actuation mechanism ﬁtted inside
the rim. The actuation mechanism consists of three separate actuators: (1)
a spin motor, which spins a suspended ﬂywheel at a high rate and imparts
dynamic stability to the robot; (2) a tilt motor, which steers the Gyrover;

4.2 Learning by SVM
97
Fig. 4.8. Gyrover: A single-wheel
robot.
O
X
B
A
y
x
z
B
B
B
R
mg
α
α.
γ.
β
β
.
Z
βα
γα
.
α
Z
Y
θ
Fig. 4.9. Deﬁnition of the Gyrover’s system
parameters.
and (3) a drive motor, which causes forward and/or backward acceleration by
driving the single wheel directly.
The Gyrover is a single-wheel mobile robot that is dynamically stabiliz-
able but statically unstable. As a mobile robot, it has inherent nonholonomic
constraints. First-order nonholonomic constraints include constraints at joint
velocities and Cartesian space velocities. Because no actuator can be used di-
rectly for stabilization in the lateral direction, it is an underactuated nonlinear
system. This gives rise to a second-order nonholonomic constraint as a con-
sequence of dynamic constraints introduced by accelerative forces on passive
joints.
To represent the dynamics of the Gyrover, we need to deﬁne the coordinate
frames: three for position (X, Y, Z), and three for the single-wheel orientation
(α, β, γ). The Euler angles (α, β, γ) represent the precession, lean and rolling
angles of the wheel respectively. (βa, γa) represent the lean and rolling angles
of the ﬂywheel respectively. They are illustrated in Figure 4.9.
Task and Experimental Description
The aim of this experiment is twofold: to compare the ability of an SVM with
several general ANNs in learning human control skills and to train an SVM
learning controller, test it with Gyrover to illustrate the applications of the
previous asymptotically stability analysis and verify it.
The control problem consists of controlling Gyrover in keeping it balanced,
i.e., not falling down on the ground. We will build up an SVM learning con-
troller based on learning imparted from expert human demonstrations.
There are mainly two major control inputs: U0 controlling the rolling speed
of the single wheel ˙γ, and U1 controlling the angular position of the ﬂywheel

98
4 Learning-based Control
βa. For the manual-model (i.e., controlled by a human), U0 and U1 are input by
joysticks, and in the auto-model they are derived from the software controller.
During all experiments, we only used of U1. β and βa among the state variables
are used during the training process. In order to construct a controller for
balance, the trained model is adjusted in calibration with the above state
variables, and its output U1.
A human expert controlled Gyrover to keep it balanced and produced
around 2,400 training samples. Table 4.2 displays some raw sensor data from
the human expert control process.
Input
Output
β
βa
U1
5.5034 2.4023 179.0000
5.7185 2.3657 176.0000
5.6012 2.1313 170.0000
5.1271 2.1460 170.0000
5.9433 1.0425 143.0000
Table 4.2. Sample human control data.
After calibrating the data to the original points, we put the sample data
into two groups as training data table and testing data table to train an SVM
learning controller. Vapnik’s Polynomial kernel of order 2 in Equation (4.32)
is chosen and the input vector consists of current state variables (βa, β,).
The output consists of control input U1. Three-input-one-output SVM models
for each of the three variables were trained with the three current values as
inputs. For ˆβa(t + 1), ˆβ(t + 1), and u(t + 1) we have 977, 989 and 905 support
vectors, respectively with corresponding αi and α∗
i . By expanding the three
SVM models according to Equation (4.32) we have the following Equation
X(t + 1) = A
X + g(X),
(4.67)
where X(t + 1) = [ˆβa(t + 1) ˆβ(t + 1) u(t + 1)]T = [x1, x2, x3]t,
A
 =


0.8818
0.0074 −0.2339
−0.1808 0.8615 −0.2389
0.0154 −0.0007 1.0167


(4.68)
and
g(X) =
!
#
0.0004x2
1 −0.0013x1x2 + 0.0028x1x3 −0.0017x2x3 −0.0006x2
2 + 0.0027x2
3
0.0004x2
1 + 0.0002x1x2 + 0.0034x1x3 + 0.0017x2x3 + 0.0003x2
2 + 0.0049x2
3
0.0002x2
1 + 0.0002x1x2 −0.0002x1x3 −0.0001x2x3 −0.0001x2
2 −0.0007x2
3
"
$
(4.69)
Then, we have A = A
 −I , which is a negative deﬁnite matrix. Thus, the
system is strongly stable under perturbations (SSUP).

4.3 Learning Control with Limited Training Data
99
Next, we need to estimate the domain of attraction Ω. If we use V = XT X
and the Lyapunov function, i.e., Let P be an identical matrix, by application
of the algorithm in inequalities (4.62) and (4.64), we have r∗
i n = 1.991 and
r∗
out = 2.005, then r∗≈2. It is too small for a lean angle β in an interval of
[−2, 2] degrees.
Then, since m + n = 2 + 1 = 3, from Equation (4.44), for Q, we have 6
independent parameters that we can adjust. By some interval
qi = [ai, bi],
i = 1, 2, ..., 5
(4.70)
we ﬁnd out a suitable Q
Q =


0.1
0
0.1877
0
0.4
0.0329
0.1877 0.0329
1

.
(4.71)
From Equation (4.40) we have
P =


2.67141
−0.955202
6.05634
6.05634
1.3957
−0.955202
−0.542122 −0.542122
47.1831

.
(4.72)
If we use V = XT PX and the Lyapunov function, by application of the
algorithm in inequalities (4.62) and (4.64), we have r∗
i n = 21.9 and r∗
out =
22.0, then r∗≈22. The lean angle β is in a region of [−22, 22] degrees, which
is a suitable range that the lean angle can reach in a balanced experiment.
Experimental Results
By using the SVM learning controller to execute the balance experiment, the
experiment is successful. But by using a Backpropagation neural network and
the radial basis function(RBF) neural network to train the learning controller
with the same data table, the result failure. Figure 4.10 shows the tilt angle βa,
lean angle β of SVM learning control, and Figure 4.11 shows the comparison
of the same Human control and SVM learner.
4.3 Learning Control with Limited Training Data
Researchers have become increasingly interested in using artiﬁcial neural
networks (ANN) for learning control [4], [36], [37], [73] and [119]. A learning
controller, here, consists of a neural network model that has learned one or
more sets of system state and control input data from human expert demon-
strations and then, in the control process, uses the neural network model to
produce system control inputs according to the current system states. Learn-
ing control can be considered as a process of building a mapping between
system states and control inputs, i.e., a function regression problem.

100
4 Learning-based Control
0
500
1000
1500
2000
2500
−40
−20
0
20
40
60
βa [deg]
time(sec)
tilt angle of flywheel
0
500
1000
1500
2000
2500
40
60
80
100
120
140
160
β [deg]
time(sec)
lean angle of Gyrover
Fig. 4.10. The tilt angle βa Lean angle β of SVM learning control.
0
5
10
15
20
25
143
143.5
144
144.5
145
145.5
146
146.5
147
147.5
148
Time
U1
SVM
Human
Fig. 4.11. U1 comparison of the same Human control and SVM learner.
It is well known that a ﬁnite number of training samples causes practical
diﬃculties and constraints in realizing good learning [83]. Raudys and Jain [84]
point out that ANNs will also encounter similar diﬃculties and constraints
when the ratio of training sample size to input dimensionality is small. In

4.3 Learning Control with Limited Training Data
101
general, for applications with a large number of features and complex learn-
ing rules, the training sample size must be quite large. A large test sample
set is particularly essential to accurately evaluate a learner with a very low
error rate. In order to understand this important eﬀect, consider the following
very simple technique (not recommended in practice) for modelling non-linear
mapping from a set of input variables x to an output variable y on the basis
of a set of training data.
x
3
1
2
x
x
Fig. 4.12. Curse of dimensionality.
We begin by dividing each of the input variables into a number of intervals,
so that the value of a variable can be speciﬁed approximately by laying in
which number of boxes or cells as indicated in Figure 4.12. Each of the training
examples corresponds to a point in one of the cells, and carries an associated
value of the output variable y. If we are given a new point in the input space,
we can determine a corresponding value for y by ﬁnding which cell the point
falls in, and then returning the average value of y for all of the training points
which lie in that cell. By increasing the number of divisions along each axis we
could increase the precision with which the input variables can be speciﬁed.
There is, however, a major problem. If each input variable is divided into M
divisions, then the total number of cells is M d and grows exponentially with
the dimensionality of the input space. Since each cell must contain at least one
data point, this implies that the quantity of training data needed to specify
the mapping also grows exponentially. This phenomenon has been termed the
curse of dimensionality [11]. If we are forced to work with a limited quantity
of data, then increasing the dimensionality of the space rapidly leads to the
point where the data is very sparse, in which case it provides a very poor
representation of the mapping.
A large number of training samples can be very expensive and time con-
suming to acquire, especially, during an on-line learning control process. For

102
4 Learning-based Control
instance, we can only collect very limited training samples from a cycle of
human demonstration. It is well known that when the ratio of the number
of training samples to the VC (Vapnik-Chervonenkis) dimension of the func-
tion is small, the estimates of the regression function are not accurate and,
therefore, the learning control results may not be satisfactory. Meanwhile, the
real-time sensor data always have random noise. This has a bad eﬀect on the
learning control performance, as well. Thus, we need large sets of data to over-
come these problems. Moreover, sometimes we need to include some history
information of systems states and/or control inputs into the ANN inputs to
build a more stable model. These will cause the increasing of the dimension or
features of the neural network and increase the requirement for more training
data.
In this work, our main aim is to produce more new training samples (called
unlabelled sample, here) without increasing costs and to enforce the learning
eﬀect, so as to improve learning control.
The main problem in statistical pattern recognition is to design a classiﬁer.
A considerable amount of eﬀort has been devoted to designing a classiﬁer in
small training sample size situations [40], [41] and [83]. Many methods and
theoretical analysis have focused on nearest neighbor re-sampling or boot-
strap re-sampling. However, the major problem in learning control is function
approximation. There is limited research exploring function regression un-
der conditions of sparse data. Janet and Alice’s work in [18] examined three
re-sampling methods (cross validation, jackknife and bootstrap) for function
estimation.
In this chapter, we use the local polynomial ﬁtting approach to individually
rebuild the time-variant functions of system states. Then, through interpola-
tion in a smaller sampling time interval, we can rebuild any number of new
samples (or unlabelled samples).
4.3.1 Eﬀect of Small Training Sample Size
Our learning human control problem might be thought of as building a map
between the system states X and the control inputs Y , approximately. Both
X = (x1, x2, ...xm) and Y are continuous time-various vectors, where xi is one
of the system states and they are true values, but not random variables. In
fact, X may consist of a number of variables in previous and current system
states and previous control inputs. Y are current control inputs. Furthermore,
without the loss of generality, we restrict Y to be a scalar for the purposes of
simplifying the discussion.
Y = ˆF(X) + Err,
(4.73)
where ˆF is the estimation for true relation function F and Err ∈R is the total
error of the learning, which need to be reduced to lower than some desirable
value.

4.3 Learning Control with Limited Training Data
103
In this problem, we mainly face three types of error. The ﬁrst type of
error is measurement error, which is derived from the sensor reading process.
We call it observation error. It may exist in both X and Y . The second type
of error occurs due to the diﬀerence between the true mapping function and
some ﬁxed neural network structure. We call it structure error. As proven by
Cybenko [28] in 1989, most functions (including any continuous function with
bounded support) can be approximated by functions of a one-hidden-layer BP
neural network. Thus, we omit this type of error in our following discussion
and assume that we already have a good enough structure that allows us to
approximate any function, if we choose suitable parameters for that structure,
i.e. there exists θ∗, such that
Y = F(X) = f(X, θ∗),
(4.74)
where θ∗= (θ∗
1, θ∗
2, ..., θ∗
h)T , h is the number of the structure parameters. The
last type of error is the parameter error and it is the result of the estimating of
the best suitable parameters. The three types of error are closely related to the
estimation error. To compensate for the observation error, more training data
lead to better results. At the same time, the parameter error also requires more
training data to overcome the overﬁtting problem, which will be discussed in
more detail in next section. Thus, the estimation error can be reduced, but
not removed absolutely, because it is impossible to collect inﬁnite training
samples.
Next, we will address our problem by considering these errors, individually.
Let Y o be the observations for true Y . Let
Y o = Y + -,
where - has the Gaussian distribution with E(-) = 0 and variance σ2 > 0. If
Xo be the observation of X,
Xo = X + ε,
where ε ∈Rm. Here, we assume each system state xi has the similar obser-
vation error as Y . The distribution of ε has the form N(0, σ2Im). Here, we
have a data table, which includes the observation for both inputs and outputs
to train a neural network as a learning controller. The data table comes from
discrete time sampling from human expert control demonstrations. In fact,
from the data table, we have Y o and Xo, but not Y and X, thus we modify
Equation (4.74) to
Y o = f(X, θ∗) + -.
(4.75)
Usually, observation errors are much smaller then their true value. A Taylor
expansion to the ﬁrst order can be used to approximate f(Xo, θ) in terms of
f(X, θ)
f(Xo, θ) ≈f(X, θ) + (LXf)T · ε,
(4.76)
where LXf = ( ∂f(X,θ)
∂x1
, ∂f(X,θ)
∂x2
, ..., ∂f(X,θ)
∂xm
)T .

104
4 Learning-based Control
We focus our attention on the estimation of regression function f, here,
for θ∗. We show that for a fairly broad class of sampling scheme ti with a
ﬁxed and identical sampling rate ∆ > 0. Assume that ( ˆf) is an estimation of
Y from a random sample T of size n. The system is represented by Equation
(4.75). The least-squares estimate of θ∗is ˆθ, which is obtained by minimizing
the error function given in (4.77) (for neural networks, the error exappropri-
ation algorithm is a common method for minimizing the error function). The
predicted output from the model is ˆY o, as shown in (4.78)
S(θ) =
n
$
i=1
[Y o
i −f(Xo
i , θ)]2
(4.77)
ˆY o
i = f(Xo
i , ˆθ)
= f(Xi, ˆθ) + (LXf)T · ε.
(4.78)
If the model provides an accurate prediction of the actual system behavior,
then ˆθ is close to the true value of the set of parameters θ∗and a Taylor
expansion to the ﬁrst-order can be used to approximate f(Xi, ˆθ) in terms of
f(Xi, θ∗)
f(Xi, ˆθ) ≈f(Xi, θ∗) + (Lθf)T · (ˆθ −θ∗),
where Lθf = ( ∂f(X,θ∗)
∂θ∗
1
, ∂f(X,θ∗)
∂θ∗
2
, ..., ∂f(X,θ∗)
∂θ∗
h
)T . Then, Equation (4.78) turns
to be
ˆY o
i ≈f(Xi, θ∗) + (Lθf)T · (ˆθ −θ∗) + (LXf)T · ε.
(4.79)
The subscript value of o is given to denote the set of observation points
other than that used for the least-squares estimation of θ∗. By ignoring the
second-order small quantity and using Equations (4.75) and (4.79), Equation
(4.80) gives the diﬀerence between the new observation Y o
0 and the predicted
value ˆY o
0 , and Equation (4.81) gives the expected value of the diﬀerence.
Y o
0 −ˆY o
0 ≈-0 −(Lθf0)T (ˆθ −θ∗) −(LXf0)T · ε0.
(4.80)
E[Y o
0 −ˆY o
0 ] ≈E[-0] −(Lθf0)T E[(ˆθ −θ∗)] −(LXf0)T E[ε0] ≈0.
(4.81)
Because of the statistical independence among -0, ˆθ and ε0, the variance can
be expressed as
var[Y o
0 −ˆY o
0 ] ≈var[-0] + var[(Lθf0)T (ˆθ −θ∗)] + var[(LXf0)T ε0].
(4.82)
The distribution of (ˆθ −θ∗) can be approximated as having the distribu-
tion N(0, σ2
θ[F.(ˆθ)T F.(ˆθ)]−1) in [24]. The Jacobian matrix F.(ˆθ) has the form
shown in Equation (4.83), where the single period is placed to accord with the
notations used in [93] which denotes that the matrix has ﬁrst-order diﬀerential
terms

4.3 Learning Control with Limited Training Data
105
F.(θ∗) =


( ∂f(X1,θ∗)
∂θ∗
1
) ( ∂f(X1,θ∗)
∂θ∗
2
) · · · ( ∂f(X1,θ∗)
∂θ∗
h
)
( ∂f(X2,θ∗)
∂θ∗
1
)
· · ·
· · · ( ∂f(X2,θ∗)
∂θ∗
h
)
...
...
...
...
( ∂f(Xn,θ∗)
∂θ∗
1
) ( ∂f(Xn,θ∗)
∂θ∗
2
) · · · ( ∂f(Xn,θ∗)
∂θ∗
h
)


(4.83)
var[Y o
0 −ˆY o
0 ] ≈σ2+σ2
θ(Lθf0)T (FT
. F.)−1(Lθf0)+σ2(LXf0)T (LXf0)]. (4.84)
The matrix F has the dimensions n by h, where n is the number of samples
and h is the number of parameters θ, which composes ˆθ. To ﬁnd the eﬀects of
more sampling data on var[Y o
0 −ˆY o
0 ], we need the following useful result.
Lemma 1 Let A and D be nonsingular squared matrices of orders k.
Then, provided that the inverses exist,
(A + D)−1 = A−1 −A−1(D−1 + A−1)−1A−1.
(4.85)
Proof: A veriﬁcation is possible by showing that the product of (A+D)−1
and the right-hand side of Equation (4.85) is the identity matrix.
If we have more training samples ´n = n + n1, let
´F.(θ∗) =


( ∂f(X1,θ∗)
∂θ∗
1
) ( ∂f(X1,θ∗)
∂θ∗
2
) · · · ( ∂f(X1,θ∗)
∂θ∗
h
)
( ∂f(X2,θ∗)
∂θ∗
1
)
· · ·
· · · ( ∂f(X2,θ∗)
∂θ∗
h
)
...
...
...
...
( ∂f(X´n,θ∗)
∂θ∗
1
) ( ∂f(X´n,θ∗)
∂θ∗
2
) · · · ( ∂f(X´n,θ∗)
∂θ∗
h
).


(4.86)
The matrix ´F has the dimensions ´n by h. Then,
´FT ´F = FT F + FT
1 F1,
(4.87)
where F1 is similarly deﬁned as in Equation (4.83), but n is substituted with
n1. Let X be a h×1 vector. Since FT F and FT
1 F1 are positive deﬁnite matrices,
substituting A = FT F, and D = FT
1 F1, in Equation (4.85), we have A−1 and
A−1(D−1 + A−1)−1A−1 are positive deﬁnite, provided that they exist. From
Lemma 1,
XT (A + D)−1X ≤XT A−1X.
(4.88)
Then, from Equation (4.87)
XT (´FT ´F)−1X ≤XT (FT F)−1X.
(4.89)
Hence, when h is ﬁxed, the larger is n, the smaller is var[Y o
0 −ˆY o
0 ]. However,
for our problem, that n is not large enough, thus, the major errors come
from the second term of Equation (4.84). In this chapter, we propose to use
unlabelled training samples to reduce the variance of Y , i.e., to decrease the
learning error. The methods will be addressed in following sections in greater
detail. If we deﬁne the data in the data table (which has been collected from

106
4 Learning-based Control
human demonstrations) as labelled data, we then call the new data unlabelled
data, which are produced by interpolation from the labelled data. Let ( ´X, ´Y )
be one unlabelled data point.
If we assumed that the interpolation process is an unbiased estimation,
then let
´X = X + ´ε,
´Y = Y + ´-,
(4.90)
where the distributions of ´ε and ´- have the form N(0, ´σ2Im) and N(0, ´σ2).
Since the unlabelled data are produced from original observation data (Xo, Y o),
they will have larger errors than the original data, i.e., ´σ2 > σ2. Both the sen-
sor reading error and the independent interpolation error are correlative with
each other. Assume that totally ´n(> n) unlabelled data points are produced.
If the unlabelled data are used to train the neural network, we need to update
Equations (4.75) - (4.84) with Equation (4.90). Finally, we have
var[Y o
0 −ˆY o
0 ] ≈´σ2+σ2
θ(Lθf0)T (´FT
. ´F.)−1(Lθf0)+ ´σ2(LXf0)T (LXf0)], (4.91)
where ´F. is deﬁned in Equation (4.86) and ´n is the number of unlabelled sam-
ples used to obtain θ. Since ´n > n, we reduce the second term of var[Y o
0 −ˆY o
0 ].
However, the ﬁrst term and third term are not relative to the sample number
n, but the accuracy of the training sample. In fact, by using the unlabelled
training data we increased them. The ﬁnal result of var[Y o
0 −ˆY o
0 ] is dependent
on the competition of the three terms. In most cases, especially, when the orig-
inal training data is very sparse, the second term in Equation (4.91) dominates
the learning error. Thus, by using unlabelled data, we can reduce the learn-
ing error. However, when the number of unlabelled data continue to increase,
the eﬀect of the other two terms will be larger than the second term, ﬁnally.
Moreover, the interpolation error may also be larger, i.e., ´σ will increase. This
is because of the overﬁtting phenomenon. Thus, using too much unlabelled
data will not necessarily lead to better learning results. Later simulations will
also demonstrate this point.
The Overﬁtting Phenomenon
Here, we choose neural networks to realize the function regression. Many of
the important issues concerning the application of neural networks can be
introduced in the simpler context of polynomial curve ﬁtting. The problem is
to ﬁt a polynomial to a set of N data points by a technique of minimizing
error function. Consider the Mth-order polynomial given by
f(x) = w0 + w1x + ... + wMxM =
M
$
j=0
wjxj.
(4.92)
This can be regarded as a non-linear mapping which takes x as input and pro-
duces f(x) as output. The precise form of the function f(x) is determined by

4.3 Learning Control with Limited Training Data
107
the values of the parameters w0, ..., wM, which are analogous to the weights in
a neural network. It is convenient to denote the set of parameters (w0, ..., wM)
by the vector W. The polynomial can then be written as a functional mapping
in the form f = f(x, W).
We shall label the data with the index i = 1, ..., n, so that each data point
consists of a value of x, denoted by xi, and a corresponding desired value for
the output f, which we shall denote by yi.
Arti Scholkopf [15] pointed out that the actual risk R(W) of the learning
machine is expressed as:
R(W) =
 1
2f(x, W) −ydP(x, y).
(4.93)
The problem is that R(W) is unknown, since P(x, y) is unknown.
The straightforward approach to minimize the empirical risk,
Remp(W) = 1
n
l
$
i=1
1
2f(x, W)i −yi,
turns out not to guarantee a small actual risk R(w), if the number n of
training examples is limited. In other words: a small error on the training set
does not necessarily imply a high generalization ability (i.e., small error on
an independent test set). This phenomenon is often referred to as overfitting.
For the learning problem, the Structure Risk Minimization (SRM) principle
is based on the fact that for any W ∈Λ and n > h, with a probability of at
least 1-η, the bound
R(W) ≤Remp(W) + Φ(h
n, log(η)
n
)
(4.94)
holds, where the confidence term Φ is deﬁned as
Φ(h
n, log(η)
n
) =

h(log 2n
h + 1) −log(η/4)
n
.
The parameter h is called the VC (V apnik-Chervonenkis) dimension of a
set of functions, which describes the capacity of a set of functions. Usually, to
decrease the Remp(w) to some bound, most ANNs with complex mathematical
structures have a very high value of h. It is noted that when n/h is small (for
example less than 20, the training sample is small in size), Φ has a large value.
When this occurs, performance poorly represents R(W) with Remp(W). As a
result, according to the SRM principle, a large training sample size is required
to acquire a satisfactory learning machine.
We can illustrate this by a one-dimension polynomial curve ﬁtting problem.
Assume we generate training data from the function
h(x) = 0.5 + 0.4 sin(2πx) + -,
(4.95)

108
4 Learning-based Control
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
x
h(x)
Fig. 4.13. Linear regression M=1.
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
x
h(x)
Fig. 4.14. Polynomial degree M=3.
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
x
h(x)
Fig. 4.15. Polynomial degree M=10.
0
1
2
3
4
5
6
7
8
9
10
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
order of polynomial
RMS error
training
test
Fig. 4.16. The RMS error for both train-
ing and test sets.
by sampling the function h(x) at an equal interval of x and then adding ran-
dom noise with Gaussian distribution having a standard deviation σ = 0.05.
We will therefore be interested in seeing how close the polynomial f(x, W) is
to the function h(x). Figure 4.13 shows the 11 points from the training set,
as well as the function h(x), together with the result of ﬁtting a linear poly-
nomial, given by (4.92) with M = 1. As can be seen, this polynomial gives
a poor representation of h(x). We can obtain a better ﬁt by increasing the
order of the polynomial, since this increases the number of the free parameter
number in the function, which gives it greater ﬂexibility. Figure 4.14 shows
the result of ﬁtting a cubic polynomial (M = 3), which gives a much better
approximation to h(x). If, however, we increase the order of the polynomial
too far, then the proximation to the underlying function actually gets worse.
Figure 4.15 shows the result of ﬁtting a 10th-order polynomial (M = 10).
This is now able to achieve a perfect ﬁt to the training data. However, the
polynomial has ﬁtted the data by developing some dramatic oscillations. Such
functions are said to be over-ﬁtting to the data. As a consequence, this func-
tion gives a poor representation of h(x). Figure 4.16 shows a plot of Remp
for both the training data set and the test data set, as a function of order

4.3 Learning Control with Limited Training Data
109
M of the polynomial. We see that the training set error decreases steadily as
the order of the polynomial increases. The test set error, however, reaches a
minimum at M = 3, and thereafter increases as the order of the polynomial
is increased.
4.3.2 Resampling Approach
Our purpose is to improve the function estimation performance of ANN learn-
ing controllers by using the polynomial ﬁtting approach interpolation samples.
Let
xT
i = {xi(t1), xi(t2), ..., xi(tn)}
be an original training sample set of one system state, by the same sampling
rate ∆> 0 and Tj = (Xj, Yj), where Xj = {x1(tj), x2(tj), ..., xm(tj)} is an
original training sample point. An unlabelled sample set ¯T = { ¯T1, ¯T2, ... ¯
TN}
which has the size of ´n can be generated by the following.
Step 1 Using the original training sample set xT
1 to produce the segment
of local polynomial estimation ˆx1(t) of x1(t) with respect to time t ∈(t1, tn).
Let
x1(t) = ˆx1(t) + ´-,
where ˆx1, x1, t ∈R
Step 2 Repeat step 1 to produce the local polynomial estimation ˆxi(t)
(i = 2, 3, ..., m) and ˆY (t) of xi(t) (i = 2, 3, ..., m) and Y (t) with respect to
time t ∈(t1, tn).
Step 3 Divide the sampling rate ∆> 0 by k = ´n/n + 1 to produce new
sampling time interval ∆k = ∆/k. We produce new unlabelled samples ´T by
a combination of interpolating the polynomials ˆxi(t) (i = 1, 2, ..., m) and ˆY (t)
in the new sampling rate.
Figure 4.17 shows an example of one unlabelled training sample generation
process. In our method, unlabelled training samples can be generated in any
number.
Barron [8] has studied the way in which the residual sum-of-squares error
decreases as the number of parameters in a model is increased. For neural
networks he showed that this error falls as O(1/M 
), where M 
 is the number
of hidden nodes in a one hidden layer network. By contrast, the error only
decreases as O(1/M 2/d), where d is the dimensionality of the input space for
polynomials, or indeed any other series expansion in which it is the coeﬃ-
cients of linear combinations of ﬁxed functions that are adapted. However,
from the former analysis, the number of hidden nodes M 
 chosen cannot be
arbitrary large. For a practitioner, his theory provides the guidance to choose
the number of variables d, the number of network nodes M 
, and the sample
size n, such that both 1/M 
 and (M 
d/n) log n are small. In particular, with
M 
 ∼(n/(d log n))1/2, the bound on the mean squared error is a constant
multiple of (d log n/n)1/2.

110
4 Learning-based Control
t1
t
t
t
t
t
t
2
3
4
5
6
7
t22
Y(t)
x(t)
x(t)
1
2
Original training sample
Unlabelled training sample
Fig. 4.17. Examples of the unlabelled sample generation, when k = 3.
Unlabelled training sample data can be produced in any large number by
interpolation. Provided this condition, if we choose some suitable and large
number of nodes (M 
), we can obtain a very high learning precision. At this
time, the learning error is mainly derived from the polynomial curve ﬁtting,
instead of neural network learning. However, we have improved the learning
by these original limited training samples, for the learning problem has turned
the inputs from x ∈Rd to t ∈R1. By using this approach, we have turned
a multivariate function estimation to a univariate function estimation (i.e.
one-by-one mapping).
From Equation (4.91), the estimation accuracy of this interpolation process
is important, for it will aﬀect the ﬁrst and third terms in Equation (4.91)
and ﬁnally, the learning controller. In the next section, the local polynomial
ﬁtting algorithm for one dimensional polynomial curve ﬁtting is introduced
as a further improvement.
4.3.3 Local Polynomial Fitting (LPF)
Advantages of Local Polynomial Fitting
Compared with the traditional method of polynomial curve ﬁtting, there are
three main advantages in using the LPF method.
First, polynomial curve ﬁtting is a parametric regression method, based on
an important assumption that the estimation model is correct. Otherwise, a
large bias will inhibit precision. LPF as a nonparametric regression approach
removes this assumption. Thus, the approach can be feasible for any model
in unknown form and still retain a uniform convergence.

4.3 Learning Control with Limited Training Data
111
Second, LPF is a well-deﬁned approach for function estimation, which is
based on rigorous theoretical research. To obtain an ideal estimation perfor-
mance, more work on polynomial curve ﬁtting is required, such as the order
of polynomials chosen.
Third, LPF requires no boundary modiﬁcations. Boundary modiﬁcations
in polynomial curve ﬁtting are a very diﬃcult task.
Introduction to Local Polynomial Fitting
Let X and Y be two random variables whose relationship can be modelled as
Y = m(X) + σ(X)- E- = 0, var(-) = 1,
(4.96)
where X and - are independent. Of interest is to estimate the regression func-
tion m(x) = E(Y |X = x), based on (X1, Y1), ..., (Xn, Yn), a random sample
from (X, Y ). If x is not a random variable but y is, their relationship can be
modelled as
Y = m(X) + - E(-) = 0, var(-) = 1.
If the (p + 1)th derivative of m(x) at the point x0 exists, we can approxi-
mate m(x) locally by a polynomial of order p:
m(x) ≈m(x0) + m
(x0)(x −x0) + ... + m(p)(x0)(x −x0)p/p!,
(4.97)
for x in a neighborhood of x0, by using Taylor’s expansion. From a statistical
modelling viewpoint, Equation (4.79) models m(x) locally by a simple poly-
nomial model. This suggests using a locally weighted polynomial regression
Jh(x) =
n
$
i=1
{Yi −
p
$
j=0
βj(Xi −x0)j}2K(Xi −x0
h
),
(4.98)
where K(·) denotes a non-negative weight (kernel) function and h - bandwidth
- determines the size of the neighborhood of x0. If ˆβ = ( ˆβ0, ..., ˆβp) denotes the
solution to the above weighted least squares problem, then by Equation (4.97),
ˆβ estimates m(x).
It is more convenient to write the above least squares problem in matrix
notation. Denote by W the diagonal matrix with entries Wi = K((Xi−x0)/h).
Let X be the design matrix whose (l, j)th element is (Xl −x0)j−1 and put
y = (Y1, ...Yn)T . Then, the weighted least squares problem (4.98) can be
written in matrix form as
min
β (y −Xβ)T W(y −Xβ),
where β = (β0, ...βp)T . Ordinary least squares theory provides the solution
ˆβ = (XT WX)−1Wy,

112
4 Learning-based Control
whose conditional mean and variance are
E(ˆβ|X1, ...Xn) = (XT WX)−1Wm
= β + (XT WX)−1Wr,
var(ˆβ|X1, ...Xn) = (XT WX)−1(XT ΣX)(XT WX)−1,
(4.99)
where m = m(X1), ...m(Xn)T , r = m −Xβ, the residual of the local polyno-
mial approximation, and Σ = diag[K2{(Xi −x0)/h}σ2(Xi)].
At ﬁrst glance, the above regression approach looks similar to the tradi-
tional parametric approach in which the function is usually globally modelled
by a polynomial. In order to have a satisfactory modelling bias, the degree M
of the polynomial often has to be large. But this large degree M introduces
an over-parametrization, resulting in a large variability of the estimated pa-
rameters. As a consequence the estimated regression function is numerically
unstable. In marked contrast to this parametric approach, the technique is
local, and hence requires a small degree of the local polynomial, typically of
order p = 1 or occasionally p = 3.
4.3.4 Simulations and Experiments
To illustrate the behavior of using the unlabelled training data method for
solving the small sample size problem, which is proposed in the foregoing
section, we present here a numerical study, which shows the result that the
theory in this chapter works. All of the simulation tasks are completed in
MATLAB.
For the simulation study, we want to build a two-input and one-output
model. Their relations are as follows.
x1(t) = 0.5 + 0.4 sin(2πt) + -1
x2(t) = −0.6 + 0.3 cos(2πt) + -2
Y (t) = cos(x1) + sin(x2) + x1x2 + -3,
(4.100)
where, the distributions of -1, -2 and -3 have the same form N(0, σ2) and
σ = 0.01. The sampling rates are identical as ∆= 0.1s both for training data
and testing data. For training data, the time interval is t0 = 0 and tT = 1s,
such that total no = 11 data points in table 4.3 and for testing data, the time
interval is t0 = 0.05 and tT = 0.95s, such that total n1 = 10 data points in
table 4.4. Since the maxim frequency Ωh of the system in Equation (4.100) is
1Hz and the sampling frequency Ωs is 10Hz > 2Ωh, it satisﬁes the “Shannon”
Theorem.
We use a two-layer radial basis network structure. The ﬁrst layer has radial-
basis neurons. The second layer has pure-line neurons, and we calculate its
weighted input with dot-prod. Both layers have biases. We used MATLAB
function “newrbe” with “spread=0.1”, to train the neural network. Then, we
use the Equation (4.101) as evaluating function with the testing data for

4.3 Learning Control with Limited Training Data
113
x1
x2
Y
0.5104 -0.3080 0.4318
0.7315 -0.3574 0.1323
0.8938 -0.5146 -0.3163
0.8905 -0.7076 -0.6519
0.7373 -0.8340 -0.6232
0.4970 -0.9027 -0.3515
0.2674 -0.8584 -0.0114
0.1177 -0.6967 0.2528
0.1188 -0.5087 0.4345
0.2719 -0.3806 0.4863
0.4920 -0.3136 0.4233
Table 4.3. The training data sample.
x1
x2
Y
0.6054 -0.3213 0.3026
0.8347 -0.4217 -0.0964
0.8986 -0.6179 -0.5057
0.8347 -0.7902 -0.7121
0.6292 -0.8833 -0.5097
0.3812 -0.87067 -0.1597
0.1696 -0.7727 0.1501
0.1028 -0.6044 0.3709
0.1631 -0.4242 0.5190
0.3692 -0.3139 0.5111
Table 4.4. The testing data sample
testing the network performance. Let Yi be an actual test data output and ˆYi
be the corresponding neural network’s output,
err =
"
#
#
! 1
n1
n1
$
i=1
(Yi −ˆYi)2.
(4.101)
When the origin no = 11 training data for learning, the result of erro is 0.1607.
Then, we produced the unlabelled training data with MATLAB function
“spline” using the origin no = 11 training data with the scales k = 2, 3, 4, 5, 6, 7
and sampling rate ∆k = ∆/k. By removing the end tail much noisy data, we
produced six groups of unlabelled data, (nk = k(no −1)). Then, using the
unlabelled training data to train the neural networks individually, the results
of errk are shown in Table 4.5.
k nk
errk
2 20 0.0486
3 30 0.0461
4 40 0.0408
5 50 0.0316
6 60 0.0262
7 70 0.0267
9 90 0.1059
Table 4.5. The results of learning error with unlabelled training data.
As analysis in Section 4.3.1, with regard to Equation (4.91), when nk
increases, the mean-square error of learning decreases. However, after the nk
reached a speciﬁc lever, here nk = 70, the second term in Equation (4.91) did

114
4 Learning-based Control
dominate the total error, but not the ﬁrst and third terms. Thus, the error
increased.
Experimental Study
The aim of this experiment is to illustrate how to use the proposed local
polynomial ﬁtting interpolation re-sampling approach and verify it.
The control problem involves tracking Gyrover in a circular path with a
deﬁned radius. A human expert controls Gyrover to track a ﬁxed 1.5-meter
radius circular path and collect around 1,000 training samples with a sampling
rate of 0.062s (62ms). Table 4.6 displays some raw sensor data from the human
expert control process.
Input
Output
β
˙β
βa
˙α
U1
5.5034 0.9790 2.4023 0.8765 179.0000
5.7185 1.2744 2.3657 1.4478 176.0000
5.6012 -0.8374 2.1313 1.0767 170.0000
5.1271 0.6641 2.1460 0.6030 170.0000
5.9433 -0.4907 1.0425 1.3574 143.0000
Table 4.6. Sample human control data.
We build up an ANN learning controller based on learning imparted from
expert human demonstrations. Here, the ANN is a cascade neural network
architecture with node-decoupled extended Kalman ﬁltering learner (CNN),
which combines (1) ﬂexible cascade neural networks, which dynamically ad-
just the size of the hidden neurons as part of the learning process, and (2)
node-decoupled extended Kalman Filtering (NDEKF), a faster converging
alternative to quickprop algorithms. This methodology has been proven to
eﬃciently model human control skills [76].
Achieving this goal requires two major control inputs: U0 controlling the
rolling speed of the single wheel ˙γ, and U1 controlling the angular position
of the ﬂywheel βa. For the manual-mode (i.e., controlled by a human), U0
and U1 are input by joysticks, and in auto-mode they are derived from the
software controller. During all experiments, we ﬁx the value of U0.
Based on these original training sample data, the CNN learner exhibited
poor learning performance, and Gyrover always fell down soon after an initial
running. Using this original learning controller, Gyrover is unable to track
a circular path. Subsequently, the human expert controlled Gyrover to track
the circular path two times and produced around 17,000 and 18,000 training
samples respectively. By combining these two sets of data, the performance of
the CNN learner was more eﬀective and the experiment was successful. Thus,

4.3 Learning Control with Limited Training Data
115
the reason for the failure initially was due to the limited number of training
samples.
In this work, we used the approach outlined above to solve the small sam-
ple problem relying on the ﬁrst set of original training sample data. First, we
carried out the interpolation for lean angle β with regard to time t using a
local polynomial ﬁtting algorithm. In MATLAB, we used “lpolyreg” to collect
the polynomial coeﬃcients and the bandwidth and then used “lpolyregh” to
calculate the unlabelled training data, ´β. Figure 4.18 shows the local polyno-
mial ﬁtting for lean angle β.
0
2
4
6
50
100
150
200
β [deg]
time(sec)
Curve fitting of β [deg]
0
2
4
6
50
100
150
200
β [deg]
time(sec)
Original β [deg]
Fig. 4.18. Local polynomial ﬁtting for lean angle β.
Second, we performed the function estimation for ( ˙β, βa, ˙α, U1) as for
lean angle β. We produced the new unlabelled training sample by interpolation
with the new sampling rate of 0.0062s (6.2ms) and produced about 10,000
pieces of unlabelled training sample data.
Third, by putting these 10,000 pieces of unlabelled sample data into a new
CNN learning model, we developed a new CNN-new neural network model.
We can, therefore, compare it with the old CNN model, CNN-old, which is
produced by the original single-set training data. We use β and βa of another
set of human demonstration training data as inputs to compare the outputs
of these two neural network models. Figure 4.19 shows the comparison among
the human control, the CNN-new model learning control and the CNN-old
model learning control. As Figure 4.19 demonstrates, the CNN-new model
has smaller error variance than that of the CNN-old, i.e., it may have better
learning control performance. Practical experiments conﬁrm this.
Subsequently, we use the CNN-new model as a learning controller to let
Gyrover track a circle automatically in real-time. This experiment was also

116
4 Learning-based Control
0
0.02
0.04
0.06
0.08
0.1
0.12
120
130
140
150
160
170
180
190
200
210
220
Time (s)
U1
Human
CNN−new
CNN−old
Fig. 4.19. Comparison of U1 in a set of testing data.
successful. Figures 4.20 and 4.21 show the control state variables lean angle
β and control command U1 for a set of human and learning controls.
0
5
10
50
100
150
200
β [deg]
time(sec)
lean angle of Gyrover
0
5
10
120
140
160
180
200
220
Tilt Command
time(sec)
Control of flywheel
Fig. 4.20. Human control.

4.3 Learning Control with Limited Training Data
117
0
5
10
15
0
50
100
150
200
β [deg]
time(sec)
lean angle of Gyrover
0
5
10
15
130
140
150
160
170
180
190
Tilt Command
time(sec)
Control of flywheel
Fig. 4.21. The CNN-new model learning control.

5
Further Topics on Learning-based Control
5.1 Input Selection for Learning Human Control
Strategy
Modeling human control strategy (HCS) refers to a model of a human ex-
pert’s control action in response to system real-time feedback. That is, we aim
to develop a relationship between human control action (control commands)
and the system response (state variables).
The HCS model can be simply interpreted as a mapping function Φ such
that:
Φ : I −→O,
(5.1)
where I is the model input and O is the model output. Modeling HCS means
that the function Φ is derived by empirical input-output data. The major input
data are state variables and the output data are control variables. In order
to generate reliable models, we should not neglect some important factors,
such as previous control inputs. However, if we consider all of these as model
inputs, the dimension of the input space is too large and there is an excess
of highly redundant information. In fact, it is important that not all features
presented are taken into account. The following three types of undesirable
features should be discarded or neglected:
1. irrelevant features that do not aﬀect the output in any way;
2. redundant features that are linearly related to or dependent on other
features;
3. weakly relevant features with only trivial inﬂuences on the model output.
Here, the input selection for this class of regression problems involves the
removal of irrelevant features, marginally relevant features and redundant
features, and selecting a group of the smallest self-contained subset of features
from the full set of features with respect to the learning output. This feature
selection is a process independent of any special neural network architecture
and the training process. Moreover, we assume that the full features already
Y. Xu and Y. Ou: Control of Single Wheel Robots, STAR 20, pp. 119–149, 2005.
© Springer-Verlag Berlin Heidelberg 2005

120
5 Further Topics on Learning-based Control
include all the necessary information to determine the learning output. It
means that the input selection process will not produce any new feature.
When using random sampling, or experimental designs, several factors can
be varied independently. Factors can be varied one-at-a-time (OAT) and all
the other factors can be held constant, at the same time. Modeling human
control strategy is not an OAT process. All the eﬀective factors (system states
or parameters) are nonlinearly relevant to each other, i.e. if one of these factors
changes, then all the other key parameters will also change.
Diﬀerent feature selection methods have been analyzed in the past. Based
on information theory, Battiti [9] proposed the application of mutual informa-
tion critera to evaluate a set of candidate features and to select an informative
subset to be used as input features for a neural network classiﬁer. In [54], inter-
class and intraclass distances are used as the criterion for feature selection. A
method proposed by Thawonmas [105] performs an analysis of fuzzy regions.
All these approaches are suitable for classiﬁcation problems. Here, modeling
human control strategy is a problem of regression. A large set of methods for
input selection are based on the analysis of a trained multilayer feedforward
neural net work or other speciﬁc network architectures in [17], [25], [69] and
[101]. Even though, in these methods, the subset features are optimal for their
special network architectures, it is possible they are not suitable or optimal
for some other network architectures. Our method, while producing similar
results in test cases, is applied before learning starts and therefore does not de-
pend on the learning process. In [85] diﬀerent feature evaluation methods are
compared. In particular, the method based on principal component analysis
(PCA) evaluates features according to the projection of the largest eigenvector
of the correlation matrix on the initial dimensions. A major weakness of these
methods is that they are not invariant under a transformation of the variables.
Some nonlinear PCA approaches are examined in the literature [50]-[51]. In
fact, the process of building up a nonlinear PCA is similar to the training
process for a feedforward neural network. Another large class of methods are
called ﬁlter wrapper and a combination of them [19], [56] and [68]. These
,
methods are focused on a searching strategy for the best subset features. The
selection criteria are similar to the above approaches. The searching strat-
egy is very import for classiﬁcation problems with hundreds or thousands of
features. However, for regression problems there are usually fewer features.
Feature selection is one of the initial steps in the learning process. There-
fore, we propose to analyze through some relatively simple and eﬃcient pro-
cesses, the search for the smallest subset of full features. This process of input
selection ensures that learned models are not over-parameterized and are able
to be generalized beyond the narrow training data. Our approach can be di-
vided into three subtasks:
1. signiﬁcance analysis for the full features;
2. dependence analysis among a select set of features;
3. self-contained analysis between the select set of features and the learning
output.

5.1 Input Selection for Learning Human Control Strategy
121
5.1.1 Sample Data Selection and Regrouping
Our learning control problem can be considered as building a map Φ be-
tween the system states X and the control inputs Y . We assume both
X = [x1, x2, ..., xm] and Y may be time-variant vectors with continuous sec-
ond order derivatives. Furthermore, without the loss of generality, we restrict
Y to be scalar for the purposes of simplifying the discussion.
Φ : Y = f (X) = f (x1, x2, ..., xm).
(5.2)
Here, we focus our attention on the estimation of regression function f .
For a fairly broad class of sampling schemes ti, they have ﬁxed and identical
sampling time intervals δt > 0 (δt = ti+1 −ti). We have a sample data table,
which includes the observation for both inputs and outputs to train a neural
network as a learning controller. The data table is derived from a discrete time
sampling from human expert control demonstrations. A training data point
Tj = [Xo
j , Y o
j ] (j = 1, 2, ..., N) consists of a system state vector Xo
j and the
control input Y o
j . Let (Xo, Y o) be a data point from the sample data table
and Y o be the observations for true Y and
Y o = Y + -,
(5.3)
where - has the Gaussian distribution with E(-) = 0 and variance σ2 > 0. If
Xo is the observation of X,
Xo = X + ε,
(5.4)
where ε ∈Rm. Here, we assume each system state xi has a similar observation
error as Y . The distribution of ε has the form N(0, σ2Im). X form the full
feature set. We will select the key features from them with respect to the
learning output (or control input) Y . For the input selection, we need to
carefully select and collect the sample data into a number of new local groups.
The sample data table may be produced by one or a combination of several
human control processes. In the (m) dimension data space, according to the
distance D, we cluster the data into a small ball with radius r (D < r), as
shown in Figure 5.1
D = ||Xi −Xc|| =

(x1
i −x1c)2 + (x2
i −x2c)2 + ... + (xm
i −xm
c )2,
(5.5)
where Xc is the center data point of the ball (or data group).
In any given ball, even though the values of the data are very similar,
the ﬁrst-order time derivatives of them may diﬀer suﬃciently. This is because
the near data points may come from diﬀerent control processes or diﬀerent
paragraphs in the same control process. The selection criteria for a data point
Tj are as follows.

122
5 Further Topics on Learning-based Control
r
Fig. 5.1. Clustering the data into a small ball with radius r.
1. If we deﬁne η as the signal to noise ratio and for any select data point
Xj = [x1
j, x2
j, ...xm
j ],
δxi
j = xi(j + 1) −xi(j),
(5.6)
|δxi
j
δt | > ησ/δt,
∀i = 1, 2, ..., m,
(5.7)
where η = 1 for signiﬁcance analysis and η = 10 for dependent analysis
and σ is the variance of noise deﬁned in Equation (5.3).
2. For each data group (or ball), let ¯Nk be the number of the kth data group,
¯Nk ought to be larger than ¯N, where
¯N = max{20, 2m}.
(5.8)
3. The radius r of the balls may be deﬁned as
r =
√m
√ηξ ,
(5.9)
where
ξ = max{|
∂2f
∂xi, ∂xj
/ ∂f
∂xi
|}
∀i, j = 1, 2, ..., m.
Next, the regrouping strategy is as follows.
1. Remove the data points that do not satisfy condition 1 and reorder the
data points.
2. Select the ﬁrst data point T1 as the center of the ﬁrst data group.
3. Calculate the distance D1 between the next data point with the center
of the ﬁrst data group. If D1 ≤r, the data point is set to the ﬁrst data
group.
4. If D1 > r, calculate the distance D2 between this data point with the
center of the second data group. If D2 ≤r, the data point is set to the
second data group. If D2 > r, this data point will be tested with the third

5.1 Input Selection for Learning Human Control Strategy
123
group. If this data point can not be set into any group, it will be set as
the center of a new data group.
5. Repeat the last step, until all data points have been set into a group.
6. Remove the data groups where the number of data points is less than ¯N
and reorder the group number.
Here, we assume the data is large enough in the sense of this regrouping
process. Finally, we obtain k balls (or k groups) of sample data, which satisfy
the above conditions.
5.1.2 Signiﬁcance Analysis
We deﬁne I(xi) as the signiﬁcance value of xi in respect to Y , given that
i = 1, 2, ..., m and if I(xi) > I(xj), xi is more important than xj.
Here, for the signiﬁcance analysis, we mainly focus on signiﬁcance prob-
lems: i.e., what is the important order of the model parameters xi, i = 1, 2, ...m
in respect to Y .
Local Sensitivity Analysis
Local sensitivity analysis (Local SA) concentrates on the local impact of the
factors on the model. Local SA is usually carried out by computing partial
derivatives numerically. Local SA is especially suitable for human learning
control strategy, because the time interval of data sampling is the same for
all the variables. Our global signiﬁcant order is derived from the cumulation
eﬀect of the local sensitivity analysis process. The sensitivity coeﬃcient ∂Y
∂xi
is an evaluation of how parameter xi aﬀects the variable Y . In the followings
steps, we will address how to estimate this.
First, with regard to dealing with the local SA, since in each group the data
are very near to each other, we transform our nonlinear mapping Equation
(5.2) into an almost linear one through a ﬁrst-order Taylor series expansion:
dY = ∂f
∂x1
dx1 + ∂f
∂x2
dx2 + ... + ∂f
∂xm
dxm + o(||dX||).
(5.10)
X is not the same in each data group, but the diﬀerence is very small.
Let’s consider the eﬀect of this on the almost linear relation. Let ¯X be the
center data point of this data group. The ﬁrst-order Taylor series expansion
of the coeﬃcient of dxi is
∂f
∂xi =
∂f
∂xi
&&&
X= ¯
X +
∂2f
∂x1∂xi
&&&
X= ¯
X(x1 −¯x1) + ...
+
∂f
∂xm∂xi
&&&
X= ¯
X(xm −¯xm) + o(||(X −¯X||).
(5.11)
Set

124
5 Further Topics on Learning-based Control
A = [a1, a2, ..., am],
B = [b1, b2, ..., bm],
where
aj =
∂2f
∂xj∂xi ,
∀i = 1, 2, ..., m,
bj = xj −¯xj,
∀i = 1, 2, ..., m.
Then, Equation (5.11) evolves to be
∂f
∂xi
= ∂f
∂xi
&&&
X= ¯
X + ABT + o(||(X −¯X||).
(5.12)
ABT ≤

(a1 + a2 + ... + am)2(b1 + b2 + ... + bm)2
≤

m2(a2
1 + a2
2 + ... + a2m)(b2
1 + b2
2 + ... + b2m)
≤

mξ2r2( ∂f
∂xi )2.
Submitting Equation (5.9) into it we have
ABT ≤| ∂f
∂xi |/η.
Thus, ABT is a higher order smaller and the eﬀect on the linear relation is
small.
We assume that the sampling time interval is small enough. Then, we use
ﬁnite-diﬀerence to approximate the ﬁrst order diﬀerentials.
dY ≈δY, dxi ≈δxi, i = 1, ..., m,
where δY = Y (t + 1) −Y (t) and δxi = xi(t + 1) −xi(t).
Equation (5.10) will be
δY = ∂f
∂x1
&&&
X= ¯
Xδx1 + ∂f
∂x2
&&&
X= ¯
Xδx2 + ... + ∂f
∂xm
&&&
X= ¯
Xδxm + o(||δX||). (5.13)
In fact, the sample data are the observation for the true values of the
variables. From the sample data selection condition 1 in Equations (5.7), we
know that the sensor reading error is a higher order smaller. By combining
the approximation error and sensor reading error together, we have
δY = ∂f
∂x1
&&&
X= ¯
Xδx1 + ∂f
∂x2
&&&
X= ¯
Xδx2 + ... + ∂f
∂xm
&&&
X= ¯
Xδxm + -
,
(5.14)
where -
 partially comes from -, and assums independent Gaussian distribution
with E(-
) = 0 and V (-
) = σ
2.
Second, we use Least-squares theory to estimate ∂f
∂X in all k data groups.
If we let ∂f
∂X be β, Equation (5.14) becomes
δY = β1δx1 + β2δx2 + ... + βmδxm + -
,
(5.15)
where β = (β1, β2, ..., βm).

5.1 Input Selection for Learning Human Control Strategy
125
If we let ¯Ni be the number of sample data in group i, (i = 1, 2, ..., k). For
each group i, we form a ¯Ni × m data matrix X and a ¯Ni × 1 data vector Y.
Data matrices and vectors are written in roman letters and variables in italic.
The least squares estimator ˆβ of β is obtained by minimizing
LSE = (Y −Xβ)T (Y −Xβ).
(5.16)
If we take the derivative of Equation (5.16) with respect to β, it yields the
normal equation
(XT X)β = XT Y.
(5.17)
As addressed above, even though X are similar to each other, δX are
suﬃciently diﬀerent to each other. Although in most cases, (XT X)−1 exists,
here we do not discuss it, however we will discuss it later. Assuming that it
exists, the linear equation (5.17) has a unique solution. Then, pre-multiplying
Equation (5.17) by (XT X)−1 gives the least squares estimator of β, that is,
ˆβ = (XT X)−1XT Y.
(5.18)
If the above assumption holds, the least squares theory provides us with
the following well-known results [38]:
The k × 1 vector ˆβ has the following properties:
1.
E(ˆβ) = β;
that is, ˆβ is an unbiased estimator for β.
2. ˆβ is the best linear unbiased estimator (BLUE) for β, that is, among
the class of linear unbiased estimators, ˆβ has the smallest variance. The
variance of ˆβ is
V ar(ˆβ) = σ
2(XT X)−1.
As the above Equation shows, and we know ∂f
∂X
&&&
X= ¯
X = β, therefore ˆβ is
an ideal estimator for ∂f
∂X
&&&
X= ¯
X.
The sensitivity coeﬃcient ∂Y
∂xi is an estimate of the number of units that
change in the variable Y as a result of a unit change in the parameter xi. This
also means that the sensitivity result depends on its physical units of variables
and parameters, and is meaningful only when the units of the model are
known. In general cases, the variables and the parameters each have diﬀerent
physical units, and therefore the sensitivity coeﬃcients cannot be compared
with each other. Here, we assume that all data have been well calibrated. If
not, they need to be properly scaled according to the physical meaning.

126
5 Further Topics on Learning-based Control
Global Signiﬁcance Order
The number of noisy sample data pairs is limited in a local time piece and it
is not enough to depend on only one or several local normalized sensitivities
to determine the global signiﬁcance order. We need to combine all of the time
piece information to determine the signiﬁcance order.
The most direct way to deﬁne the signiﬁcance order is sorting the sum-
mary of the normalized sensitivity coeﬃcients. To avoid the normalized sen-
sitivity coeﬃcients canceling out with positive and negative values during the
summation process, we use their absolute values. Then, we deﬁne the Direct
Signiﬁcance Evaluation function as
I(xi) = 1
k
k
$
j=1
|Sj(xi)|,
(5.19)
where
Sj(xi) = ˆβj
i ,
i = 1, 2, ..., m,
j = 1, 2, ..., k.
Usually, this deﬁnition works well. However, in some cases where out-
liers are in evidence, some error data points of a parameter are much larger
than others and as such they will greatly aﬀect the average value. In these
conditions, we have to seek another deﬁnition for the signiﬁcance evaluation
function. According to probability theory, the most important parameter has
the largest possibility to be the ﬁrst important parameter. We can redeﬁne
the signiﬁcance evaluation function in the following steps;
1. In each local time piece, we sort the absolute values of normalized sensi-
tivity coeﬃcients.
2. For each parameter xi, we collect the times for each possible order. We
deﬁne nl(xi) as the number of times that |S(xi)| is the lth largest, where
l = 1, ..., m.
3. We deﬁne pl(xi) as the possibility that |S(xi)| is the lth largest, where
l = 1, ..., m.
pl(xi) = nl(xi)/k × 100%,
where nl(xi) ≥0 and
m
$
l=1
nl(xi) = k.
4. For each parameter, we deﬁne the Statistical Importance Evaluation func-
tion as
I(xi) = p1(xi) × 10m−1 + p2(xi) × 10m−2 + ...pm(xi).
(5.20)
Thus, the input variables selection can be performed after we work out
the signiﬁcance evaluation values for real learning problems.

5.1 Input Selection for Learning Human Control Strategy
127
5.1.3 Dependency Analysis
First, we will use an example to distinguish three terms, “relevant”, “depen-
dent” and “(linear) relative”, which will be used in this chapter. Suppose that
x1, x2, x3 and x4 have the following relations
x3 =
x1x2
x4 = 3x1 + 2x2.
Then the relation between x1 and x3 is “relevant”, but not “dependent” or
“(linear) relative”. The relation among x1, x2 and x3 is “dependent” and
“relevant”, but not “(linear) relative”. The relation between x1, x2 and x4 is
“(linear) relative”, “dependent” and “relevant”. The “dependent” relation is
important in the later two subtasks described in Section 5.1.3.
Let ´X = [´x1, ´x2, ..., ´x ¯m], be a subset of X and ¯m ≤m. Let’s consider the
relation among ´X. Supposed that the relationship of them is “dependent” and
´x ¯m = f 
(´x1, ´x2, ..., ´x ¯m−1).
(5.21)
We need to use the sample data table to verify this relation. After the data
selection and regrouping process, we know that each group of data are very
near to each other in the ¯m −1 dimension data space. Then, by a ﬁrst-order
Taylor series expansion, we transform our nonlinear mapping Equation (5.21)
into an almost linear one
d´x ¯m = ∂f 
∂´x1
d´x1 + ∂f 
∂´x2
d´x2 + ... +
∂f 
∂´x ¯m−1
d´x ¯m−1 + o(||d ´X||).
(5.22)
If there is a “dependent” relation among ´X, then they will be almost “(linear)
relative” among d ´X. As noted earlier, even though ´X are similar to each other,
d ´X are suﬃciently diﬀerent from each other.
As the similar analysis before we have
δ´x ¯m = ∂f 
∂x1
&&&
X= ¯
Xδ´x1 + ∂f 
∂x2
&&&
X= ¯
Xδ´x2 + ... +
∂f 
∂x ¯m−1
&&&
X= ¯
Xδ´x ¯m−1 + ¯-, (5.23)
where ¯- is similar to -
 which is a higher order smaller with respect to δ´x ¯m. If
we let ∂f 
∂X
&&&
X= ¯
X be ¯β, Equation (5.23) becomes
δ´x ¯m = ¯β1δ´x1 + ¯β2δ´x2 + ... + ¯β ¯m−1δ´x ¯m−1 + ¯-,
(5.24)
where ¯β = (¯β1, ¯β2, ..., ¯β ¯m−1).
We use Least-squares theory to estimate ¯β in all k data groups in the
following Equation.
ˆ¯β = (¯XT ¯X)−1 ¯XT ¯Y.
(5.25)
To eliminate the possibility that the terms in Equation (5.24), will cancel
each other, let

128
5 Further Topics on Learning-based Control
||δ´x ¯m|| = |¯β1δ´x1| + |¯β2δ´x2| + ... + |¯β ¯m−1δ´x ¯m−1|.
(5.26)
We deﬁne the “(linear) relative” coeﬃcient as
ρ =
¯-
||δ´x ¯m||.
(5.27)
For each data point in each data group, we will calculate the “(linear)
relative” coeﬃcient. For each group, if the group has ¯Nm data points, we
deﬁne the average “(linear) relative” coeﬃcient as,
¯ρ =
"
#
#
!
¯
Nm
$
i=1
ρ2
i / ¯Nm,
(5.28)
where ¯ρ ought to be a small number around 0.1 (≤0.2).
According to the information derived from these coeﬃcients, we can de-
termine the “(linear) relative” relation among the ﬁrst-order derivative of ´X
and, further, the nonlinear “dependent” relationship among them.
Furthermore, if the relation among ´X is not dependent, then we can use
the above analysis to verify the self-contained relation between Y and ´X. If
they have a self-contained relation, they will have a dependent relation and
their ﬁrst-order derivatives will exhibit a nearly “(linear) relative” relation. It
is possible that some other features will aﬀect Y . However, since the “(linear)
relative” coeﬃcient is small, the relevance of features should be weak, which
means they will be removed from the key feature set.
Thus, we can summarize our input selection approach by the following
steps.
1. Perform the signiﬁcance analysis between the full features and the learning
output. Reorder the features according to their signiﬁcance.
2. Select the ﬁrst important feature with learning output to carry out the
self-contained analysis. If the learning output is dependent on the ﬁrst
signiﬁcant feature, stop and report the result.
3. If not, combine the ﬁrst signiﬁcant feature with the second signiﬁcant
feature to perform a dependent analysis between them. If the second sig-
niﬁcant feature is dependent on the ﬁrst signiﬁcant feature, remove the
feature and go to the next step. If the relations are not dependent, execute
the self-contained analysis between these two features with the learning
output. If the learning output is dependent on these two features, stop
and report the result.
4. Combine the features in the former step with the next signiﬁcant feature to
perform a dependent analysis among them. If the next feature is dependent
on the former features, remove the feature and go back to the beginning
of this step, until there is no feature left, stop and report the result. If the
relations are not dependent, execute the self-contained analysis between
this new set of features with the learning output. If the learning output

5.1 Input Selection for Learning Human Control Strategy
129
is dependent on these new set of features, go to the next step. If not, go
back to the beginning of this step, until there is no feature left, stop and
report the result.
5. Remove the features one by one and perform the self-contained analysis
to eliminate any redundant features still remaining, until all features are
critical. Stop and report the result.
Since some features may be dependent on some later features or the com-
bination of some former features and later features, thus, the last step is
necessary and important.
5.1.4 Experimental Study
The aim of this experiment is to illustrate how to use the proposed criterion
to realize the input variables selection and validate the approach.
The control problem consists of tracking Gyrover in a circle within a de-
ﬁned radius. In the experimental system, for each sampling process, we can
collect 20 sensor readings. Their deﬁnitions and physical meanings are shown
in Table 5.1. Among them, there are 11 sensor readings, ‘ADC0’∼‘ADC10’,
corresponding to the system states variables. There are also two major sen-
sor readings, ‘PIR0’ (PIC-IN-READ 0) and ‘PIR1’(PIC-IN-READ 1), corre-
sponding to the two control inputs: U0 controlling the rolling speed of the
single wheel ˙γ, and U1 controlling the angular position of the ﬂywheel βa. For
the manual-model (i.e. controlled by a human), U0 and U1 are input by joy-
sticks, and in the auto-model, they are derived from the software controller.
During all experiments, we only focus on the value of U1 and ﬁx the value
of U0 to some suitable value. Since the capability of spinning motor for the
ﬂywheel is limited, after it drives the ﬂywheel to the working spinning speed
˙γa, the motor will try to maintain this speed. Thus, ‘PIR3’ (PIC-IN-READ 3)
can not be a control input and it is ﬁxed during the data producing process,
too.
A human expert controlled Gyrover to track a ﬁxed 3-meter radius circular
path and produced around N = 30744 training samples with identical time
intervals (δt = 0.025s). Table 5.2 displays some raw sensor data from the
human expert control process. In the learning control training process, the
system states variables are the learning model inputs and tilt command U1 is
the model output. If we put all of the 11 sensor readings, ‘ADC0’∼‘ADC10’,
into the learning process, the input dimension is too large to process a reliable
model due to the ‘curse of dimensionality’. Therefore we need to perform input
variables selection. Here, we use signiﬁcance analysis approach and dependence
analysis approach for it.
Signiﬁcance Analysis Study
It is the aim of this step to rank the 11 system variables (model inputs) in
signiﬁcance order with respect to the tilt command (model output).

130
5 Further Topics on Learning-based Control
Column Sensor name
Function descriptions
1
Counter read
Spinning angle of ﬂywheel
2
ADC0
Tilt servo position, i.e., tilt angle of ﬂywheel βa
3
ADC1
Tilt servo current
4
ADC2
Drive motor current
5
ADC3
Roll rate of Gyro, i.e., roll rate of single wheel ˙β
6
ADC4
Pitch rate of Gyro
7
ADC5
Yaw rate of Gyro, i.e., precession rate of single wheel ˙α
8
ADC6
X accelerometer
9
ADC7
Y accelerometer
10
ADC8
Z accelerometer
11
ADC9
X tilt sensor
12
ADC10
Y tilt sensor, i.e., lean angle of single wheel β
13
PIC-IN-READ 0
Drive command U0
14
PIC-IN-READ 1
Tilt command U1
15
PIC-IN-READ 3
Command for spinning speed of ﬂywheel
16
PIC-OUT-READ 0
Drive command
17
PIC-OUT-READ 1
Tilt command
18
PIC-OUT-READ 3
Command for spinning speed of ﬂywheel
19
instantaneous speed instantaneous driving speed of single wheel ˙γ
20
average speed
instantaneous driving speed of single wheel ¯˙γ in 5 sampling
intervals
Table 5.1. Gyrover’s sensor data string
Input
Output
ADC0 ADC1 ADC2 ADC3 ADC4 ADC5 ADC6 ADC7 ADC8 ADC9 ADC10
PIR1
2.614
0.029
-0.004
0.422
0.617
1.696
2.440
2.589
3.624
5.821
5.874
180
2.646
1.970
1.989
0.410
0.500
2.104
2.885
2.528
3.178
6.060
6.168
179
2.585
6.245
1.935
-0.300
0.659
2.717
2.724
2.393
3.221
6.021
6.407
178
2.419
3.308
3.475
0.505
0.478
0.310
2.172
2.489
3.214
6.075
5.400
178
2.382
3.352
3.636
-0.678
0.468
1.616
2.601
2.739
3.769
6.075
5.997
175
Table 5.2. Sample of human control data
First, we obtain the sensor reading error information as follows. If we ﬁx
Gyrover in some position and start it, all sensor readings should keep in some
values. In fact, they are changing around their true values for the noise sake.
Then we keep recording the sensor readings for a period of time and collect
hundreds of sensor readings for each sensor. From these data, we obtain the
noise variance σ for all variables, ‘ADC0’∼‘ADC10’ and ‘PIR1’ as shown in
Table 5.3.
ADC0 ADC1 ADC2 ADC3 ADC4 ADC5 ADC6 ADC7 ADC8 ADC9 ADC10 PIR1
0.007
0.003
0.006
0.006
0.004
0.004
0.006
0.006
0.007
0.002
0.003
0.019
Table 5.3. The noise variance σ information for variables
Second, we select and regroup data points Tj = [Xj, Yj](j = 1, 2, ..., 30744)
from the whole data table as in the following steps.

5.1 Input Selection for Learning Human Control Strategy
131
1. According to Equation (5.6) we calculate the ﬁrst derivatives for all data
points.
2. According to Equation (5.7) we select and place the data points into a
new data table with 14,634 data points by removing the data points that
their ﬁrst derivatives are less than the noise (η = 1).
3. According to Equations (5.8) and (5.9), we work out ¯N = 22 and r = 0.5.
4. According to r = 0.2, we cluster the 14634 data points into 6340 groups.
5. According to N = 22, we select 45 data groups from the 6340 group by
removing the groups in which the data numbers are less than 22.
Third, we perform the local sensitivity analysis (Local SA). According to
Equation (5.18), we work out the sensitivity coeﬃcients β = ∂f
∂X for all 45
data groups. The local sensitivity coeﬃcients of the ﬁrst four most signiﬁcant
variables are shown in Figure 5.2.
0
5
10
15
20
25
30
35
40
45
−15
−10
−5
0
5
10
15
Local sensitivity coefficents
groups
ADC0
ADC9
ADC10
ADC4
Fig. 5.2. The local sensitivity coeﬃcients of the ﬁrst four signiﬁcance variables.
Last, we rank all the system variables in the global signiﬁcance order.
According to the Direct Signiﬁcance Evaluation function in (5.19), we work
out the global signiﬁcance value I(xi) for each systems variable and order
them in Table 5.3.

132
5 Further Topics on Learning-based Control
Variables ADC0 ADC1 ADC2 ADC3 ADC4 ADC5 ADC6 ADC7 ADC8 ADC9 ADC10
I(xi)
331.53
3.07
5.23
25.48
73.99
51.71
15.29
24.91
11.26
173.14
97.89
rank
1
11
10
6
4
5
8
7
9
2
3
Table 5.4. The signiﬁcance order table
We performed the signiﬁcance analysis experiments several times with
diﬀerent data set combinations. The results are quite similar from time to
time.
Dependence Analysis Study
The aim of this step is to obtain the smallest set of system variables and the
control input is dependent on this set of features.
First, we execute the self-contained analysis between all of the m = 11
system variables with the tilt command.
1. We select and regroup data points Tj = [Xj, Yj](j = 1, 2, ..., 30744) from
the whole data table.
2. According to Equation (5.6), we calculate the ﬁrst derivatives for all data
points.
3. According to Equation (5.7) we select and place the data points into a
new data table with 1,085 data points by removing the data points that
their ﬁrst derivatives are less than ten times the noise variance σ (η = 10).
4. According to r, N, we cluster and select the 1,085 data points into 22
groups.
5. According to Equations (5.24)-(5.28), for each group, we calculate the
average ”(linear) relative” coeﬃcients that are in Table 5.5.
group
1
2
3
4
5
6
7
8
9
10
11
¯ρ
0.1011 0.1041 0.1103 0.1252 0.1280 0.1103 0.1232 0.1022 0.1235 0.0903 0.1250
group
12
13
14
15
16
17
18
19
20
21
22
¯ρ
0.1257 0.0983 0.1011 0.0865 0.1250 0.1078 0.0990 0.1030 0.1003 0.0316
0.099
Table 5.5. The average “(linear) relative” coeﬃcients ¯ρ in full system variables
Table 5.5 exhibits that the tilt command is dependent on these system vari-
ables. However, m = 11 is too high to train a learning controller.
Second, according to the signiﬁcance order in Table 5.4, we add the vari-
able one by one and execute the dependence analysis between the former fea-
tures with the new features and self-contained analysis between the selected
set of features with the tilt command. After the similar process with the
full variable self-contained analysis, we stop with a set variables of (‘ADC0’,
‘ADC9’, ‘ADC10’, ‘ADC4’, ‘ADC5’, ‘ADC3’). Table 5.6 shows part of the av-
erage “(linear) relative” coeﬃcients of the self-contained analysis between the

5.1 Input Selection for Learning Human Control Strategy
133
set variables of (‘ADC0’, ‘ADC9’, ‘ADC10’, ‘ADC4’, ‘ADC5’) with tilt com-
mand. Table 5.7 shows the average “(linear) relative” coeﬃcients of the self-
contained analysis between the set variables of (‘ADC0’, ‘ADC9’, ‘ADC10’,
‘ADC4’, ‘ADC5’,‘ADC3’) with the tilt command. From Table 5.6, the set fea-
tures (‘ADC0’, ‘ADC9’, ‘ADC10’, ‘ADC4’, ‘ADC5’) can not determine the
tilt command and from Table 5.7, the tilt command is almost dependent on
the variables of (‘ADC0’, ‘ADC9’, ‘ADC10’, ‘ADC4’, ‘ADC5’,‘ADC3’).
group
1
2
3
4
5
6
7
8
9
10
11
¯ρ
0.5851 0.7654 0.5944 0.6203 0.5336 0.5574 0.5112 0.5799 0.5863 0.6530 0.5334
Table 5.6. Part of ¯ρ in the 5 variables
group
1
2
3
4
¯ρ
0.1862 0.1654 0.1862 0.1247
Table 5.7. ¯ρ in the 6 variables
Last, we remove the set features of (‘ADC0’, ‘ADC9’, ‘ADC10’, ‘ADC4’,
‘ADC5’,‘ADC3’), one by one to obtain the smallest set of features. The ﬁnal
result is (‘ADC0’, ‘ADC10’, ‘ADC5’, ‘ADC3’). The self-contained analysis
between the set variables of (‘ADC0’, ‘ADC10’, ‘ADC5’, ‘ADC3’) with the
tilt command is shown in Table 5.8. The table shows that tilt command is
almost dependent on the variables of (‘ADC0’, ‘ADC10’, ‘ADC5’, ‘ADC3’).
group
1
2
3
4
5
6
7
8
9
10
11
¯ρ
0.1250 0.1198 0.1224 0.1219 0.1168 0.1219 0.1198 0.1250 0.1198 0.1168 0.1219
Table 5.8. Part of ¯ρ in the 4 variables
Learning Control Result
Based on these input variables, we built up an artiﬁcial neural network(ANN)
learning controller, based on learning imparted from expert human demon-
strations. Here, the ANN is a support vector machine learning architecture
(SVM), in [81], which can realize a very high learning accuracy with limited
data points. We trained an SVM learning controller to control Gyrover. The
learning controller works well and the model is very reliable. Figure 5.3 shows
the SVM learning control results.

134
5 Further Topics on Learning-based Control
0
200
400
−40
−20
0
20
βa [deg]
time(sec)
0
200
400
40
60
80
100
120
β [deg]
time(sec)
0
200
400
−4
−2
0
2
dba (deg/sec)
time(sec)
0
200
400
−200
−100
0
100
db [deg/sec]
time(sec)
0
200
400
−100
−50
0
50
da [deg/sec]
time(sec)
0
200
400
175
180
185
190
195
Tilt Command U1
time(sec)
Fig. 5.3. SVM learning control results.
Discussions and Remarks
The most critical and time-consuming part is the data selection and regroup-
ing. The data selection conditions need to be well controlled and parameters
need to be carefully selected.
If some of the sensor readings are too noisy to produce the data selec-
tion and collection, we may use smoothing technologies. There are many ap-
proaches in digital signal processing. An important practical point that needs
to be kept in mind is that the state variables in learning human control are
usually very low frequency signals.
As described in the signiﬁcance analysis section, if there are only limited
groups, and their (XT X)−1 do not exist, then remove these groups. If all
groups display the problem and it is because there exist two or more variables
with strong linear relations, remove one or some of them to avoid the linear
relation and perform the signiﬁcance analysis again.

5.2 Implementation of Learning Control
135
Moreover, the input variables X may miss some important variables
and/or add some almost irrelevant variables. We do not consider the missing
problem here, as mentioned in the introduction section: i.e., we assume that
we have already included all the important factors. For the latter problems,
we divide X into two parts:
X = ( ´X1 : ´X2),
where X ∈Rm, ´X1 ∈Rm1, ´X2 ∈Rm2 and m = m1 + m2 and ´X2 is not
relevant to Y .
Now suppose that the true model for Equation (5.15) becomes
δY = ¯β1δX1 + -
,
(5.29)
but instead we ﬁt
δY = ¯β1δX1 + ¯β2δX2 + -
,
(5.30)
to the data, where ¯β1 ∈Rm1 and ¯β2 ∈Rm2. We refer to Equation (5.29) and
Equation (5.30) as the reduced and full models respectively. Let ˆ¯β1F and ˆ¯β2F
be the least squares estimates of ¯β1 and ¯β2 in ﬁtting model (5.30) to the data.
Let ˆ¯β1R denote the least squares estimates when model (5.29) is ﬁtted to the
data. From [90], we have the following two conclusions:
1. ˆ¯β1F and ˆ¯β2F are unbiased estimates of ¯β1 and ¯β2, respectively. That is,
E(ˆ¯β1F ) = ¯β1 and E(ˆ¯β2F ) = 0.
2. E(ˆ¯σ2
F ) = σ2.
This means in the latter case that it is still an ideal estimate for the sensitivity
coeﬃcient ∂Y
∂xi .
How to choose features is another important aspect that will aﬀect the
learning accuracy. Thus, in the next chapter, we will handle this problem.
5.2 Implementation of Learning Control
In this chapter, we show the implementation results of the CNN models
trained in the previous chapter. First of all, we validate the CNN models
we obtained by applying a Hidden Markov Model based similarity measure.
Next, for the experimental implementations of the CNN models, we evaluate
the performance of these models by observing the lean angle of the robot and
the overall control on the ﬂywheel. Later, we combined the two motions into
a single motion. This combined motion ensures that the robot can be fully
recovered from the fall position back and balanced at its upright position.

136
5 Further Topics on Learning-based Control
5.2.1 Validation
In this subsection, we will evaluate each of the model generated by the cas-
cade learning algorithm for diﬀerent behaviors of the robot, including lateral
stabilization and tiltup motion. We apply the similarity measure mentioned
in Section 3.2.4 to quantify the level of similarity between the original human
control data and the model-generated trajectories through simulations. Since
we do not have a physical model for these kind of motion for Gyrover, our
simulations are done by feeding the current and history state variables and
control information into the cascade neural network, to see if it can generate
similar control output in each time instant.
Basically, we have two motions to learn: (1) Lateral balancing (i = 1), and
(2) Tiltup (i = 2). For each motion, we give three diﬀerent set of data for the
simulation. For notation convenience, let Xi,j, i ∈{1, 2}, j ∈{1, 2, 3}, denote
the run of diﬀerent motions i in trail #j.
Vertical Balancing
Figure 5.4, 5.6 and 5.8 show three diﬀerent vertical balanced motion by hu-
man control. The graph on the left of each ﬁgure is the plot of lean angle
data (β), while the right one plots the orientations of the ﬂywheel (βa). The
corresponding human control data and CNN model control data for X(1,1),
X(1,2) and X(1,3) are shown in Figure 5.5, 5.7 and 5.9 respectively. We perform
the similarity measure between the human control and CNN model control
trajectories for each motion, the results are summarized in Table 5.9. From
the performance of this vertical balancing CNN model, we can observe that
the model can generate similar control trajectories as human operator, with
an average similarity value of 0.5940.
similarity σ
X(1,1)
0.5885
X(1,2)
0.6235
X(1,3)
0.5700
average
0.5940
Table 5.9.
Similarity measures for vertical balanced control between human and
CNN model
Tilt-up Motion
Figure 5.10, 5.12 and 5.14 show three diﬀerent tiltup motion by human con-
trol. The corresponding human control data and CNN model control data for
X(2,1), X(2,2) and X(2,3) are shown in Figure 5.11, 5.13 and 5.15 respectively.

5.2 Implementation of Learning Control
137
0
20
40
60
80
−50
−40
−30
−20
−10
0
10
20
30
40
βa [deg]
time(sec)
tilt angle of flywheel
0
20
40
60
80
50
60
70
80
90
100
110
120
130
140
β [deg]
time(sec)
lean angle of Gyrover
Fig. 5.4. Vertical balanced motion by human control, X(1,1).
0
10
20
30
40
50
60
70
80
160
170
180
190
200
210
time
Tilt command
CNN model control trajectory
0
10
20
30
40
50
60
70
80
160
170
180
190
200
210
time
Tilt command
Human control trajectory
Fig. 5.5. Control trajectories comparison for X(1,1).
Again, we perform the similarity measure between the human control and
CNN model control trajectories for each motion, the results are summarized
in Table 5.10. The CNN model can also generate similar control trajectories
as human operator, with an average similarity value of 0.7437.

138
5 Further Topics on Learning-based Control
0
20
40
60
80
100
−30
−20
−10
0
10
20
30
40
βa [deg]
time(sec)
tilt angle of flywheel
0
20
40
60
80
100
50
60
70
80
90
100
110
120
130
β [deg]
time(sec)
lean angle of Gyrover
Fig. 5.6. Vertical balanced motion by human control, X(1,2).
0
10
20
30
40
50
60
70
80
90
100
160
170
180
190
200
time
Tilt command
 CNN model control trajectory
0
10
20
30
40
50
60
70
80
90
100
160
170
180
190
200
time
Tilt command
 Human control trajectory
Fig. 5.7. Control trajectories comparison for X(1,2).
Discussions
The simulations we have done in fact is the ﬁrst step to validate the CNN
models we obtained. By using the HMM similarity measure, we compare the
human control trajectory with the control trajectory generate from the CNN
model of a particular motion. If the similarity measure gives us a relatively

5.2 Implementation of Learning Control
139
0
20
40
60
−40
−20
0
20
40
60
βa [deg]
time(sec)
tilt angle of flywheel
0
20
40
60
50
60
70
80
90
100
110
120
130
β [deg]
time(sec)
lean angle of Gyrover
Fig. 5.8. Vertical balanced motion by human control, X(1,3).
0
10
20
30
40
50
60
70
150
160
170
180
190
200
time
Tilt command
 CNN model control trajectory
0
10
20
30
40
50
60
70
150
160
170
180
190
200
time
Tilt command
 Human control trajectory
Fig. 5.9. Control trajectories comparison for X(1,3).
high similarity value (σ ≥0 5), which implies the particular CNN model
.
can produce ’similar’ control output as human control. From the simulation
results of the lateral balancing and tiltup motion, we can verify that the CNN
models for both motions are able to model the human control strategy. Later
on, in the next chapter, we will further verify the models by experimental
implementation.

140
5 Further Topics on Learning-based Control
similarity σ
X(2,1)
0.7896
X(2,2)
0.7030
X(2,3)
0.7386
average
0.7437
Table 5.10. Similarity measures for tiltup control between human and CNN model
0
2
4
6
−40
−20
0
20
40
60
80
100
βa [deg]
time(sec)
tilt angle of flywheel
0
2
4
6
40
60
80
100
120
140
160
180
β [deg]
time(sec)
lean angle of Gyrover
Fig. 5.10. Tiltup motion by human control, X(2,1).
0
2
4
6
130
140
150
160
170
180
190
200
time
Tilt command
 CNN model control trajectory
0
2
4
6
130
140
150
160
170
180
190
200
time
Tilt command
 Human control trajectory
Fig. 5.11. Control trajectories comparison for X(2,1).

5.2 Implementation of Learning Control
141
0
2
4
6
−50
0
50
100
βa [deg]
time(sec)
tilt angle of flywheel
0
2
4
6
40
60
80
100
120
140
160
180
β [deg]
time(sec)
lean angle of Gyrover
Fig. 5.12. Tiltup motion by human control, X(2,2).
0
2
4
6
130
140
150
160
170
180
190
time
Tilt command
 CNN model control trajectory
0
2
4
6
130
140
150
160
170
180
190
time
Tilt command
 Human control trajectory
Fig. 5.13. Control trajectories comparison for X(2,2).
0
2
4
6
8
10
−100
−80
−60
−40
−20
0
20
40
60
βa [deg]
time(sec)
tilt angle of flywheel
0
2
4
6
8
10
0
50
100
150
β [deg]
time(sec)
lean angle of Gyrover
Fig. 5.14. Tiltup motion by human control, X(2,3).

142
5 Further Topics on Learning-based Control
0
2
4
6
8
160
170
180
190
200
210
220
230
240
time
Tilt command
 CNN model control trajectory
0
2
4
6
8
120
140
160
180
200
220
240
time
Tilt command
 Human control trajectory
Fig. 5.15. Control trajectories comparison for X(2,3).
5.2.2 Implementation
Vertical Balanced Motion
A number of experiments have been conducted to verify the CNN model
for vertical balancing, Figure 5.16, 5.17 and 5.18 shows the implementation
results. The human control strategy in balancing the robot at the vertical
position is given in Figure 5.19. As mentioned in the pervious chapter, we
evaluate the performance by the lean angle of the robot and the degree of
freedom remains for the ﬂywheel. We summarized the overall performance of
both CNN model and human operator for the vertical stabilized motion in
Table 5.11.
average lean angle ¯β DOFflywheel
CNN control #1
90.24o
0.9944
CNN control #2
88.11o
0.8756
CNN control #3
87.57o
0.8867
Human control
89.41o
0.9600
Table 5.11. Performance measures for vertical balancing.
When compared with human control, the CNN model we obtained for
vertical balancing behaves very similar to human. For the 3 diﬀerent trails,
the CNN model not only able to stablize the robot at around 90o, but also
reserved a high level of degree of freedom for the internal ﬂywheel to oppose
any motion that appears to make the robot to fall down.

5.2 Implementation of Learning Control
143
0
10
20
30
40
50
60
70
80
90
100
110
50
60
70
80
90
100
110
120
β [deg]
time(sec)
lean angle of Gyrover
0
10
20
30
40
50
60
70
80
90
100
110
−40
−20
0
20
40
60
80
βa [deg]
time(sec)
tilt angle of flywheel
Fig. 5.16. Vertical balancing by CNN model, trail #1.
0
10
20
30
40
50
60
50
60
70
80
90
100
110
120
β [deg]
time(sec)
lean angle of Gyrover
0
10
20
30
40
50
60
−40
−20
0
20
40
60
80
βa [deg]
time(sec)
tilt angle of flywheel
Fig. 5.17. Vertical balancing by CNN model, trail #2.
Tilt-up Motion
Next, we implement another CNN model which is trained by human tiltup
motion data, the results are shown in Figure 5.20 and 5.21 for CNN model

144
5 Further Topics on Learning-based Control
0
10
20
30
40
50
60
70
40
60
80
100
120
140
β [deg]
time(sec)
lean angle of Gyrover
0
10
20
30
40
50
60
70
−60
−40
−20
0
20
40
60
βa [deg]
time(sec)
tilt angle of flywheel
Fig. 5.18. Vertical balancing by CNN model, trail #3.
average lean angle ¯β DOF
DOFflywheel
CNN control #1
97 26
0 6774
97.
.
26o
0.6774
CNN control #2
95 60
0 4039
95.
.
60o
0.4039
Human control
87 31
0 7372
87.
.
31o
0.7372
Table 5.12. Performance measures for tiltup motion.
control, while the human control is shown in Figure 5.22. The performance of
these motions are summarized in Table 5.12.
Since a large portion of the ﬂywheel’s motion is contributed to tiltup the
robot, the overall degree of freedom of the ﬂywheel in tiltup motion is much
lower than that of lateral stabilization. For the CNN model control in Figure
5.20 and 5.21, the robot is lying on the ground initially, with β
o
≈150 , after
, after
a few seconds, the model tiltup the robot and brings the robot back to the
upright position.

5.2 Implementation of Learning Control
145
0
10
20
30
40
50
60
70
40
60
80
100
120
140
β [deg]
time(sec)
lean angle of Gyrover
0
10
20
30
40
50
60
70
−30
−20
−10
0
10
20
30
40
βa [deg]
time(sec)
tilt angle of flywheel
Fig. 5.19. Vertical balancing by human operator.
Combined Motion
We observed that the CNN models for lateral balancing and tiltup motion are
subjected to some initial condition, the problem can be solved by combining
the two motions to form a single motion.
Consider the case that the robot is in the fall position, that is, with β ≈
150. In Figure 5.20 and 5.21, although the CNN tiltup model is able to keep
the robot to stay around at 90o for a certain moment, the robot will fall back
to the ground eventually because the ﬂywheel has reached an ill-condition
(βa = ±90o). Moreover, the tiltup model is unable to let the robot to converge
to 90o sometimes, which causes a large ﬂuctuation in the lean angle about 90o,
Figure 5.24.
To deal with this problem, we combine the tiltup motion together with
the lateral balanced motion, Figure 5.23. Since the CNN model is unable to
keep the robot at the vertical position, after the robot has tiltup, we ask the
model to balance the robot at 90o.
The experimental result for the whole tiltup and stabilization process after
the combination is shown in Table 5.13 and Figure 5.25. Initially, the robot is
in a fall position, by executing the tiltup control of the CNN model, the robot
is recovered to the vertical position. Afterwards, the lateral stabilization is
controlled by another model which speciﬁcally trained for keeping the robot
into the vertical position. From the results, the combined motion can keep

146
5 Further Topics on Learning-based Control
0
5
10
15
40
60
80
100
120
140
160
180
β [deg]
time(sec)
lean angle of Gyrover
0
5
10
15
−20
0
20
40
60
80
100
βa [deg]
time(sec)
tilt angle of flywheel
Fig. 5.20. Tiltup motion by CNN model, trail #1.
average lean angle ¯β DOFflywheel
CNN control #1
88.40o
0.8998
Table 5.13. Performance measures for combined motion.
5.2.3 Discussions
In this chapter, the CNN models for lateral balancing and tiltup motion are
being veriﬁed by experimental implementations. By combining the two mo-
tions into a single motion, the robot is able to recover from the fall position,
and then to remain stable at the vertical position after tiltup. Therefore, we
have completed the low-level behavior module within the behavior-based ar-
chitecture shown in Figure 6.4. With this module completed, we are going to
develop a semi-autonomous control for Gyrover in the next chapter.
the robot at the vertical position well after tiltup from the ground for a much
more longer period of time.

5.2 Implementation of Learning Control
147
0
5
10
15
20
25
0
50
100
150
200
β [deg]
time(sec)
lean angle of Gyrover
0
5
10
15
20
25
−20
0
20
40
60
80
100
βa [deg]
time(sec)
tilt angle of flywheel
Fig. 5.21. Tiltup motion by CNN model, trail #2.
0
1
2
3
4
5
6
7
8
9
40
60
80
100
120
140
160
180
β [deg]
time(sec)
lean angle of Gyrover
0
1
2
3
4
5
6
7
8
9
−40
−20
0
20
40
60
80
100
βa [deg]
time(sec)
tilt angle of flywheel
Fig. 5.22. Tiltup motion by human operator.

148
5 Further Topics on Learning-based Control
Verticle position
Lateral stabilization
Tilting-up
Initial status
REAR VIEW OF GYROVER
flywheel
spinning axis of flywheel
motion of
flywheel
motion of robot
Fig. 5.23. Combined motion.
0
1
2
3
4
5
6
7
8
9
10
0
50
100
150
200
β [deg]
time(sec)
lean angle of Gyrover
0
1
2
3
4
5
6
7
8
9
10
−100
−50
0
50
100
βa [deg]
time(sec)
tilt angle of flywheel
Fig. 5.24. Fluctuation in the lean angle made by the tiltup model.

5.2 Implementation of Learning Control
149
0
10
20
30
40
50
60
70
80
90
100
40
60
80
100
120
140
160
180
β [deg]
time(sec)
lean angle of Gyrover
0
10
20
30
40
50
60
70
80
90
100
−50
0
50
100
βa [deg]
time(sec)
tilt angle of flywheel
Fig. 5.25. Tiltup and vertical balanced motion by CNN models.

6
Shared Control
Based on the successful implementations of the lateral balancing and tiltup
motion, in this chapter, we are going to develop a shared control framework
for Gyrover. In fact, any situation of a system using shared control will in-
volve human interactions. Under shared control, the human operator acts as
a supervisor for overall control, while the robot itself can handle some local
motions which in turn assist the human. In order to distribute the control
tasks systematically, we develop an expression to make such a decision. Ex-
perimental results will be given in order to verify our idea.
6.1 Control Diagram
In fact, shared control happens in many daily examples, especially for human-
animal interactions. First of all, let’s consider the horse riding case [103], it
is a fairly good example of a semi-autonomous system, or more speciﬁcally, a
shared control system.
For the horse which is being ridden by a human, it is usually able to take
care of all low-level tasks such as coordination of leg motion, stability, local
obstacle avoidance and provide enough power and speed for diﬀerent actions.
On the other hand, the rider provides global planning, interacts with the horse
to arrive at diﬀerent locations and achieve various goals. At the same time,
the rider can override any horse behavior by pulling the reins or hitting on
the horse’s body if necessary. Throughout the journey, the rider relies on the
horse motoric abilities and the horse’s behaviors become more intelligent by
receiving the rider’s commands. The interaction between the two individuals
happens in a natural and simple way.
Another example we want to illustrate is to ask a robotic arm to handle
a cup of tea [120]. The whole task can be decomposed into two subtasks:
(i) to handle the cup of tea safely without pouring the tea (local balancing),
and (ii) to reach the desired location (global navigation). In a teleoperated
environment, it may be diﬃcult for a human operator to perform both tasks
Y. Xu and Y. Ou: Control of Single Wheel Robots, STAR 20, pp. 151–173, 2005.
© Springer-Verlag Berlin Heidelberg 2005

152
6 Shared Control
simultaneously, or it could be mentally taxing. However, if an autonomous
module is introduced for the local stability of the cup, the operator in the
control loop is only responsible for the navigation task, which greatly reduces
the burden for the operators. Moreover, it is clear that the performance of
the system would be much better and stable than being controlled by a single
entity (human/machine).
Gyrover is a complex system not only in terms of the diﬃculties in deriving
its mathematical model, but also in terms of its control by human operator.
The robot can be controlled manually through a radio transmitter with two
independent joysticks, one of them is assigned to control the drive motor,
while the other one is assigned to control the tilt motor. Similar to a bicycle,
Gyrover is a single track vehicle which is inherently unstable in its lateral di-
rection. Therefore, diﬀerent from controlling a quasi-static mobile robot, the
human operator not only handles the global navigation for the robot, but also
needs to pay attention to govern the lean angle of the robot simultaneously.
Moreover, the highly coupling eﬀect between the wheel and the internal ﬂy-
wheel also complicates the control of Gyrover. To this end, for such a complex
system, instead of creating fully autonomous control, it is much more practical
to develop a control method which can “share” the workload of the human
operator.
Recently, shared control has been widely applied into many robotics man-
machine systems, from health care [10, 103, 26, 2, 97, 20] to telerobotics
[121, 64, 39, 120, 29]. For rehabilitation applications, a typical example is
robotic wheelchairs. Although the wheelchair itself can provide a level of au-
tonomy for the users, it is still desirable that the user can augment the con-
trol by the on-board joystick in some special situations (e.g. docking, passing
through a doorway). A telerobotic system usually consists of a human oper-
ator and several autonomous controllers. A human operator usually interacts
with the system in diﬀerent ways. One of the important issues is to develop
an eﬃcient method to combine a human and machine intelligences so that the
telerobotic system can perform tasks which cannot be done by either a hu-
man or autonomous controller alone [39]. In these shared control systems, the
autonomous modules exist in the system to assist the human operator during
navigation, in order to relieve the stress of the operators in a complex system.
Usually, the human operator is responsible for some high-level control (e.g.
global navigation), while the machine performs low-level control (e.g. local
obstacle avoidance).
In fact, the two behaviors we have mentioned in the previous chapters, (i)
Lateral balancing and (ii) Tiltup motion, are designed to tackle the robot’s
instability problem in the lateral direction. Since we have successfully modeled
and implemented the two behaviors by a machine learning approach and ver-
iﬁed them in experiments, the next step is to incorporate these motions with
human control in order to develop a shared control framework for Gyrover.
We prefer using a shared control scheme rather than a fully autonomous one
because of the following reasons:

6.2 Schemes
153
•
Sophisticated dynamic system. As mentioned before, it is diﬃcult for
us to obtain a complete mathematical model to govern the motions of
Gyrover, due to its complicated dynamic and nonholonomic nature. This
makes us encounter many diﬃculties in developing a fully autonomous
system for Gyrover at this stage.
•
Hardware limitations.
Due to the special physical structure of Gy-
rover, the current prototype of Gyrover we are using still does not have
any navigation devices equipped on-board (e.g. vision), which makes it
impossible for the robot to navigate itself.
•
Importance of human operators. Practically, some complicated tasks,
which may be trivial for humans, are often not performed well by robots.
Therefore, a human operator is essential to exist in the control loop in
order to monitor and operate the executive system.
•
Time and cost. Building a fully autonomous system which provides safe
and robust performance would be time consuming and costly, in terms of
computations and resources. In contrast, it is far more practical and much
cheaper to develop a semi-autonomous system.
•
Accuracy vs Reliablilty. Machines are excellent in performing repetitive
tasks quickly and accurately but their abilities to adapt to changes in the
environment is low. On the other hand, humans are usually reliable, with
tremendous perceptive ability and good decision making in unpredictable
situations, but their accuracy is relatively lower than machines. Shared
control can let them compensate each weakness which would result in
better control.
•
Teleoperations. Gyrover can be operated by humans through a radio
transmitter, which allows humans to participate in the control of the robot.
The main diﬃculty in developing a shared control for Gyrover is due to
the access to the tilt motor. Since the lean angle of the robot is controlled
by the tilt motor, not only the autonomous module will access the tilt motor
to achieve stability in the lateral direction, the human operator also needs to
access the tilt motor during navigation. At a particular time instant, these
commands may contradict each other. Therefore, it is a big issue to let the
system decide which command is going to be executed, and at the same time,
manage the contaminated commands in a reasonable way. To this end, we have
developed an expression for making this decision, which will be discussed in
the later part of this chapter. With better sharing between the machine and
human operator, the performance of the system can be enhanced, and the
range of tasks that can be performed by the system can also increase.
6.2 Schemes
In fact, there are many aspects of “sharing” in shared control, which vary from
application to application. Basically, semi-autonomous control can be catego-
rized into serial and parallel types [120]. In the serial type, manual control and

154
6 Shared Control
autonomous control cannot be executed simultaneously, only one of them will
be selected at a time. In parallel type, both manual and autonomous control
can be executed simultaneously.
In the following sections, we will brieﬂy discuss three operating modes
of shared control, namely: (1) Switch mode, (2) Distributed mode, and (3)
Combined mode.
6.2.1 Switch Mode
In switch mode, manual and autonomous control are switched in serial, as
shown in Figure 6.1. The condition to trigger the switch depends on applica-
tions, for example, if an operator acts as a supervisor of the control system,
the human control will only be activated whenever the system reaches an “ill
condition”. No matter which control module is switched, the robot will be
fully controlled by the selected one. If high cooperation between the machine
and operator is required, we must have a function (Π) which can “smartly”
switch between the two control modules.
Autonomous
module
Human
operator
ROBOT
Π
Fig. 6.1. Switch mode.
6.2.2 Distributed Mode
Figure 6.2 illustrates the architecture of distributed control. Diﬀerent from
switch mode, both manual and the autonomous control can be executed in
parallel in this mode. The control of various actuators (ui) in the entire system
will be distributed to either of the two modules.
Therefore, the two entities can exist in the system peacefully without
disturbing each other. However, this also shows the weakness of this mode

6.3 Shared Control of Gyrover
155
because there is no communication bewteen the two entities. The operator
cannot modify the commands from autonomous module even if the robot
performs or tends to perform undesirable motions.
Autonomous
module
Human
operator
ROBOT
u1
u4
u5
u3
u2
Fig. 6.2. Distributed control mode.
6.2.3 Combined Mode
Combined mode is in fact an extension of distributed mode, Figure 6.3. How-
ever, the input to a single actuator is a combination of the operator’s command
and the machine’s command. There are many ways to combine the output vec-
tors from the task modules: a simple summation, a simple average, weighted
sum and average, voting on angle and velocity, and some unusual variations.
In practice, the weighted average performs well since it is not computationally
expensive and its performance is predictable [29].
6.3 Shared Control of Gyrover
Analogous to the example of handling a cup of tea, in our approach, in order
to reduce the operator burden in controlling a statically unstable robot, it
is desired that Gyrover itself can maintain a degree of local balancing, while
the operator only responsible for the navigation task. In considering which
mode of sharing is suitable for Gyrover shared control, we found that the
commands from the automation module (lateral balancing and tiltup) always
contradicts the navigation commands. It is due to the special steering mech-
anism of Gyrover, which is entirely contributed by the tilting eﬀect of the
internal ﬂyhwheel.
As mentioned in section 3.2.1, when a disc is rolling, it will steer in the di-
rection in witch it is leaning. Since the autonomous module is designed to keep

156
6 Shared Control
Autonomous
module
Human
operator
ROBOT
u1
u4
u5
u3
u2
Σ
Fig. 6.3. Combined mode.
the lean angle in the vertical position, if we attempt to steer to the left/right
manually (i.e. lean to the left/right), the machine will generate commands to
stabilize the robot back to the vertical position, which will totally oppose the
changes we want to make. Therefore, the commands from the two modules
are impossible to combine into a single valid command during navigation.
Tiltup
Lateral balancing
reset
sensors
tilt servo
Gyrover
Heading
control
to
robot
from
robot
Path
tracking
Obstacles
avoidance
drive motor
to
robot
Map
Image Processor
Vision
Planner
Path
planning
activation
sensory and
behavior state infomation
Monitor
Video
High level behaviors
Mid-level behaviors
Low level behaviors
Fig. 6.4. A detailed structure of behavior connectivity in Gyrover control.

6.3 Shared Control of Gyrover
157
Referring to Figure 6.4, the mid and high-level behaviors are replaced
by a human operator in shared control. Due to the high ﬂexibility of the
subsumption architecure, we obtain the shared control architecture as shown
in Figure 6.5, without destorying the original control structure, which shows
the beauty of behavior-based control architecture. Since the navigation tasks
are entirely given to the human operator, the operator will solely control the
drive motor through a radio transmitter. On the other hand, because the
robot can maintain lateral stability when it stops rolling, or when a complete
fall is detected, it will automatically tiltup back to its upright position. Thus,
the tilt motor is jointly controlled by the operator and the machine.
Tiltup
Lateral balancing
reset
sensors
Gyrover
Navigations
to
robot
from
robot
to
robot
Human operator
Machine
Visual
information
Fig. 6.5. Subsumption architecture of shared control.
According to Figure 6.5, regarding the tilt motor, switch mode is used
since the operator and the machine cannot control the motor at the same
time; regarding the whole structure, the system is somewhat in a distributed
mode of sharing. As a result, the shared control of Gyrover combines the switch
mode and the distributed mode, which compensates each mode’s weakness.
drive motor
tilt servo

158
6 Shared Control
6.4 How to Share
Recalling the horse riding example, it is believed that the horse acknowledges
the rider’s commands if they exceed a certain threshold. This threshold may
depend on the horse’s training (reliability of the autonomous system), the skill
of rider, and on the situation at hand. If the rider wishes to correct or modify
the horse current behavior, he/she will increase the level of stimulus which is
acted on the horse (pulling the reins more or pushing harder on the saddle).
This continues until the horse changes its behavior as wished by the rider. A
poor communication or compromise between them can lead to undesirable or
even dangerous results. Therefore, in this section, we develop a function to
decide whether to follow or neglect the commands from the online operator.
First of all, let’s introduce the variables that constitute the function, which
are similar to those proposed in [103]:
1. Degree of Autonomy, A where 0 ≤A ≤1.
This is a parameter which can be adjusted by the online operator. If the
operator (a novice) wishes to rely much more on the autonomous module,
he/she should select a higher value of A at the beginning of an operation.
Otherwise, if an experienced operator is conﬁdent with his/her control
skills, a lower value of A can be selected. We will demonstrate the eﬀect
of this parameter later.
2. Strength of conﬂict, S where 0 ≤S ≤1.
This parameter measures the conﬂict between the operator and the current
status of the system, it will vary from time to time whenever the operator
is given a command to alter the system’s trajectory. A high value of S
indicates that the operator is making a control command which greatly
aﬀects the current status of the system, while a low value of S indicates
that only a small disturbance is generated. This value will pass to the func-
tion instantaneous to make a decision whether to execute the operator’s
command or not. The strength of conﬂict S can be deﬁned as:
Sβ =
∂β
∂βmax
or
Sout = |uoperator −umachine|
∂umax
(6.1)
where Sβ is measured in terms of the changes in the lean angle β of the
robot, Sout is in terms of the conﬂict between the command from the
operator and the machine.
3. Conﬁdence level, C where 0 ≤C ≤1.
Contradictory to the strength of conﬂict, C is a parameter to show the
conﬁdence of an operator in making the current control command. It is
obvious that the higher value of C, the more conﬁdent the operator is.
This is also a time varying parameter which will pass to the function to
let the system make a decision. The conﬁdence level C can be deﬁned as:

6.4 How to Share
159
C = |∂uoperator|
∂umax
(6.2)
Based on the above deﬁnition, at a particular time instant, the system
receives a command from the operator and the machine simultaneously, and
we obtain the following relationship between S and C:
rclif C > S, follow the operator’s command,
C ≤S, follow the machine’s command.
(6.3)
The above expressions imply that if the operator is conﬁdent enough to
modify the current system trajectory, his/her command will be executed. On
the other hand, if the system determines that the command of the operator
is potentially to let the robot fall down, his/her command will be neglected,
and the system will execute the balancing command from the autonomous
module. However, the threshold of the above expressions remains constant
and it is dependent on the system parameters. Practically, a system may be
potentially operated by diﬀerent operators, so it is desirable that the threshold
of the decision be dependent on the operator. To this end, we introduce the
parameter of Degree of Autonomy (A) into the above expressions,
rclif C · (1 −A) > S · A, follow the operator’s command,
C · (1 −A) ≤S · A, follow the machine’s command.
(6.4)
By rewriting equation (6.4), we have,
Π(A, S, C) = λ · C −S
(6.5)
where λ = (1 −A)/A for simplicity, and the decision ﬁnally becomes,
rclif Π(A, S, C) > 0, follow the operator’s command,
Π(A, S, C) ≤0, follow the machine’s command.
(6.6)
The function Π is called a decision function which allows a system to
decide whether to execute the command from an operator in a shared control
environment. To validate the decision function, we let A = 0, which implies
that the operator do not need any assistance from the autonomous module
and the system should respond to all the commands from the operator. From
equation (5.6),
Π(0, S, C) = +∞> 0
∀S, C
Π(0, S, C) is always positive so that the system always executes the commands
from the operator. Now, consider when A = 1,
Π(1, S, C) = −S ≤0
∀S, C

160
6 Shared Control
Π(1, S, C) is always negative or equal to zero, which implies the system will
totally follow the machine commands and disregard all the operator’s com-
mand.
To further validate the decision function Π in (6.5), we perform the fol-
lowing experiments to see how the system works with this function. Tables
6.1, 6.2 and 6.3 show the values obtained from the decision function Π by
using A = 0.25, A = 0.50 and A = 0.75 respectively. Based on the decision
criteria in (6.6), if Π(A, S, C) is greater than zero, the system will execute
the operator’s command at that particular moment, otherwise, the machine’s
command will be executed. In each table, a shaded value represents the system
has chosen the operator’s command.
Current lean angle of the robot β
∂u
40o
50o
60o
70o
80o
90o
100o
110o
120o
130o
140o
0
-0.56
-0.44
-0.33
-0.22
-0.11
0
-0.11
-0.22
-0.33
-0.44
-0.55
2
-0.36
-0.25
-0.14
-0.03
0.08
0.11
-0.01
-0.12
-0.23
-0.34
-0.45
4
-0.17
-0.06
0.06
0.17
0.28
0.21
0.10
-0.01
-0.12
-0.23
-0.34
6
0.03
0.14
0.25
0.36
0.43
0.32
0.21
0.09
-0.02
-0.13
-0.24
8
0.22
0.33
0.44
0.56
0.53
0.42
0.31
0.20
0.09
-0.02
-0.13
10
0.42
0.53
0.64
0.75
0.64
0.53
0.42
0.31
0.19
0.08
-0.03
15
0.09
1.01
1.13
1.01
0.90
0.79
0.68
0.57
0.46
0.35
0.24
20
1.39
1.50
1.39
1.28
1.17
1.06
0.94
0.83
0.72
0.61
0.50
30
2.14
2.03
1.92
1.81
1.69
1.58
1.47
1.36
1.25
1.14
1.03
40
2.67
2.56
2.44
2.33
2.22
2.11
2.00
1.89
1.78
1.67
1.56
Table 6.1. Decision making of A = 0.25.
Current lean angle of the robot β
∂u
40o
50o
60o
70o
80o
90o
100o 110o 120o 130o 140o
0
-0.56
-0.44
-0.33
-0.22
-0.11
0
-0.11 -0.22 -0.33 -0.44 -0.55
2
-0.46
-0.35
-0.24
-0.13
-0.02
0.01
-0.11 -0.22 -0.33 -0.44 -0.55
4
-0.37
-0.26
-0.14
-0.03
0.08
0.01
-0.01 -0.21 -0.32 -0.43 -0.54
6
-0.27
-0.16
-0.05
0.06
0.13
0.02
-0.09 -0.21 -0.32 -0.43 -0.54
8
-0.18
-0.07
0.04
0.16
0.13
0.02
-0.09 -0.20 -0.31 -0.42 -0.53
10
-0.08
0.03
0.14
0.25
0.14
0.03
-0.08 -0.19 -0.31 -0.42 -0.53
15
0.15
0.26
0.38
0.26
0.15
0.04
-0.07 -0.18 -0.29 -0.40 -0.51
20
0.39
0.50
0.39
0.28
0.17
0.06
-0.06 -0.17 -0.28 -0.39 -0.50
30
0.64
0.53
0.42
0.31
0.19
0.08
-0.03 -0.14 -0.25 -0.36 -0.47
40
0.67
0.56
0.44
0.33
0.22
0.11
0
-0.11 -0.22 -0.33 -0.44
Table 6.2. Decision making of A = 0.50.

6.5 Experimental Study
161
Current lean angle of the robot β
∂u
40o
50o
60o
70o
80o
90o
100o 110o 120o 130o 140o
0
-0.55
-0.44
-0.33
-0.22
-0.11
0
-0.11 -0.22 -0.33 -0.44 -0.55
2
-0.49
-0.38
-0.27
-0.16
-0.05 -0.03 -0.14 -0.25 -0.36 -0.47 -0.58
4
-0.43
-0.32
-0.21
-0.10
0.01
-0.06 -0.17 -0.28 -0.39 -0.50 -0.61
6
-0.37
-0.26
-0.15
-0.04
0.03
-0.08 -0.19 -0.31 -0.42 -0.53 -0.64
8
-0.31
-0.20
-0.09
0.02
0
-0.11 -0.22 -0.33 -0.44 -0.56 -0.67
10
-0.25
-0.14
-0.03
0.08
-0.03 -0.14 -0.25 -0.36 -0.47 -0.58 -0.69
15
-0.10
0.01
0.13
0.01
-0.10 -0.21 -0.32 -0.43 -0.54 -0.65 -0.76
20
0.06
0.17
0.06
-0.06
-0.17 -0.28 -0.39 -0.50 -0.61 -0.72 -0.83
30
0.14
0.03
-0.08
-0.19
-0.31 -0.42 -0.53 -0.64 -0.75 -0.86 -0.97
40
0
-0.11
-0.22
-0.33
-0.44 -0.56 -0.67 -0.78 -0.89 -1.00 -1.11
Table 6.3. Decision making of A = 0.75.
When A = 0.25, the system will be more likely to rely on the operator’s
command. In Table 6.1, most of the operator’s commands are chosen even
when the Conﬁdence level of his/her control is quite low (smaller ∂u). On
the other hand, for a higher value of A (Table 6.3), the system relies on the
machine’s commands more so that the frequency of accepting the operator’s
commands reduces signiﬁcantly. The above experiments simply illustrate that
the decision function Π can judge whether to execute a human operator’s
commands eﬀectively by taking the value A into accounts, which is very im-
portant in a shared control system.
In fact, the system neglects the operator’s commands only when the com-
mand is potentially dangerous to the robot. Since a positive change in the
tilt command will give a positive change in the lean angle of the robot, if the
lean angle is beyond 90o, a larger ∂u will make the lean angle grow bigger,
which potentially makes the robot falls down. Therefore, in this case, if the
operator is not conﬁdent enough to make this change, his/her command will
be neglected.
6.5 Experimental Study
In this section, we implement the shared control framework as shown in Figure
6.5, by applying the decision function we have mentioned in the last section.
We have designed several tasks for the robot to perform under the shared
control scheme, including (i) heading control (ii) straight path tracking, (iii)
circular path tracking, and (iv) point-to-point navigation.
Since the autonomous module now in hand is only responsible for the lat-
eral stabilization and tiltup motion when the robot is held in a stationary
location, the navigation task of the robot will be entirely given to the human
operator to control, which implies that the human cannot rely on the machine

162
6 Shared Control
throughout the navigation. Based on this limitation, we use a relatively high
level of autonomy (A ≈0.25) in Gyrover shared control. From the experi-
ments, we can observe that even the operator has shared a level of control
with the system, and the robot can still achieve some basic goals in mobile
teleoperations.
6.5.1 Heading Control
The purpose of this experiment is to illustrate the cooperation between the
human operator and the autonomous module in a shared control environment.
One special feature of Gyrover is the ability to turn into a desirable heading
direction at a stationary location, this motion can be achieved by controlling
the lean angle of the robot (left/right) until the desired heading direction is
reached.
When the robot is not rolling, the system will automatically execute the
lateral balancing module in order to maintain its lateral stability, by control-
ling the tilt motor. If the operator wishes to command the robot to turn into
a particular heading angle, he/she needs to make the robot lean at a certain
angle by controlling the tilt motor; also, in this case, the robot must stop the
autonomous module and execute the operator’s command. Therefore, if the
system cannot make the right decision, the operator can never control the
robot to turn into a desired heading direction.
The result of using A = 0.2 and A = 0.8 in the heading control test is
shown in Figure 6.6 and Figure 6.7 respectively. For A = 0.2, the operator
triggers the control of the tilt motor at 7.5 ≤t ≤9.5 and 14.5 ≤t ≤17,
in order to make the robot lean to a particular heading angle. It is clear
that the operator augments the control in these periods successfully, which is
expected when a low degree of autonomy is used. When there is no command
from the operator, the robot will execute the lateral balancing control from
the autonomous module in order to keep the robot around 90o. For A =
0.8, the control trajectory of the operator is completely diﬀerent from the
ﬁnal control output to the system. The operator wants to trigger the tilt
motor, but the system neglects most of his/her commands and continues to
execute the lateral balancing commands from the autonomous module. The
system will only execute those commands from the operator only when the
particular command greatly contributes to keeping the robot at 90o, or when
the conﬁdence level is high, for instance, at t ≈13 and t ≈17.
6.5.2 Straight Path
In the straight path test, the operator is asked to control the robot to travel
a straight path, approximately 44 ft long. The experimental setup is shown in
Figure 6.8. Three trails are given in this experiment, the trajectory that the
robot has travelled in each trail is shown in Figure 6.11. The sensor data of
the robot in trail #3 is plotted in Figure 6.12.

6.5 Experimental Study
163
0
2
4
6
8
10
12
14
16
18
20
22
0
50
100
150
200
β [deg]
time(sec)
lean angle of Gyrover
0
2
4
6
8
10
12
14
16
18
20
22
−100
−50
0
50
100
βa [deg]
time(sec)
tilt angle of flywheel
0
2
4
6
8
10
12
14
16
18
20
22
140
160
180
200
220
Tilt Command
time(sec)
Tilt command (Human)
0
2
4
6
8
10
12
14
16
18
20
22
160
180
200
220
240
Tilt Command
time(sec)
Tilt command (robot)
Fig. 6.6. Sensor data acquired in the heading control test, A = 0.2.
Under a shared control, although some of the control commands are ig-
nored by the system (ﬂattened peaks in the ﬁnal output of tilt motor com-
mand), the operator is still able to control the robot to travel a nearly straight
path, with an average 0.1736 ft oﬀset from the desired path. At t = 9, the
robot received no commands from the operator and started to execute the
lateral balancing module to balance the robot. As mentioned earlier, the con-
trol of the drive motor is entirely given to the operator, therefore, the system

164
6 Shared Control
0
2
4
6
8
10
12
14
16
18
20
0
50
100
150
β [deg]
time(sec)
lean angle of Gyrover
0
2
4
6
8
10
12
14
16
18
20
−100
−50
0
50
100
βa [deg]
time(sec)
tilt angle of flywheel
0
2
4
6
8
10
12
14
16
18
20
−20
−10
0
10
20
Tilt Command
time(sec)
Tilt command (Human)
0
2
4
6
8
10
12
14
16
18
20
−20
−10
0
10
20
30
Tilt Command
time(sec)
Tilt command (robot)
Fig. 6.7. Sensor data acquired in the heading control test, A = 0.8.
will not interfere with the drive motor command, which directly follows the
control of the operator.

6.5 Experimental Study
165
START
FINISH
44 feets
Fig. 6.8. Experiment on tracking a straight path under shared control.
6.5.3 Circular Path
Similar to the straight path test, the experimental setup is shown in Figure 6.9.
This time, the operator is required to control the robot travel a circular path.
In order to make the robot to turn in place, the operator needs to the tilt the
internal ﬂywheel to make a “lean steering” precisely. If the robot fails to follow
the right commands, it is unable to steer well. Figure 6.13 indicates the desired
path and the actual path travelled by the robot respectively. Figure 6.14 shows
the corresponding sensor data (trail #3) of the robot during travelling in a
circular path.
START
FINISH
9 feets
7 feets
Fig. 6.9. Experiment on tracking a curved path under shared control.
The average oﬀset in the circular path test is 0.51 ft. Although the robot
cannot track the circular path precisely, the operator can control the robot
to move back to the target location within 0.25 ft at nearly the end of the
experiments. Therefore, with a degree of shared control with the robot, the
operator is still able to control the robot to turn a tight corner.
6.5.4 Point-to-point Navigation
In this experiment, we require the robot to travel from one location to another
which are separated by a right-hand corner and they are far apart ( ≈60 ft),

166
6 Shared Control
Figure 6.10. The operator needs to control the robot to move from a starting
area to a speciﬁc destination, which is a 2 ft × 2 ft region (the dimension of
Gyrover is about 1.5 ft × 0.8 ft as viewed from the top). This experiment has
two main goals:
1. The robot must reach the destination within the speciﬁc area.
2. After the robot has reached the destination, it is required that the robot
can maintain its lateral balance even when the operator does not further
control it.
The experimental results are shown in Figure 6.15 and 6.16.
Although we are not concerned with whether the robot can accurately
track the path or not, the overall oﬀset from the path is 1.18 ft, which is an
acceptable value for a 60 ft long journey. Moreover, for the three trails in this
experiment, all the trajectories of the robot are converging to the destination
at the end of the path. From Figure 6.16, when t ≥14 (at the destination),
the operator did not command the robot anymore, however, the robot can
balance itself at around 90o. Therefore, under a shared control environment,
with the human operator responsible for the navigation task of the robot,
the robot is able to move from one location to another location which are
far apart, and to balance itself at the vertical position when the robot stops
moving (with no operator’s command).
6.6 Discussions
From the results we obtained from the previous experiments, we verify that our
proposed shared control algorithm can let the system choose between human
operator’s control commands or the commands from the autonomous module
systematically. Whenever the operator has chosen a high level of autonomy,
the system will execute the command from the the autonomous module unless
the operator has given a command which is ’conﬁdent’ enough to overcome
the conﬂict between the operator and the machine. On the other hand, if a low
degree of autnomy is chosen, the system will follow the operator’s command
unless a ’signiﬁcant’ error/conﬂict is measured. The proposed shared control
algorithm is able to allow two entities (human and machine) to exit in the
same system simultaneously.
Although Gyrover does not have an autonomous module to navigate it-
self to travel from one location to another, this can be done by sharing the
navigation task with the operator. Under shared control, the robot will main-
tain its lateral balance when the operator does not command it. On the other
hand, under a degree of sharing, the operator is still able to control the robot
to do some speciﬁc tasks (straight path tracking, point-to-point navigation,
etc). It is believed that if an autonomous navigation module exists in the sys-
tem, the operator can share more naviagtion control to the machine using the

6.6 Discussions
167
START
FINISH
9 ft
51 ft
2 X 2
sq. ft
Fig. 6.10. Experiment on point-to-point navigation under shared control.
proposed shared control algorithm, which can greatly reduce the duty of the
online human operator.

0
5
10
15
20
25
30
35
40
45
4
3
2
1
0
1
2
3
4
distance (ft)
path offset (ft)
Straight path test
#1
#2
#3
Fig. 6.11. Trajectory travelled in the straight path test.
168
6 Shared Control

6.6 Discussions
169
0
1
2
3
4
5
6
7
8
9
10
60
80
100
120
β [deg]
time(sec)
lean angle of Gyrover
0
1
2
3
4
5
6
7
8
9
10
−20
−10
0
10
20
βa [deg]
time(sec)
tilt angle of flywheel
0
1
2
3
4
5
6
7
8
9
10
180
185
190
195
200
Drive Command
time(sec)
Drive command (Human)
0
1
2
3
4
5
6
7
8
9
10
175
180
185
190
Tilt Command
time(sec)
Tilt command (Human)
0
1
2
3
4
5
6
7
8
9
10
175
180
185
190
Tilt Command
time(sec)
Tilt command (robot)
Fig. 6.12. Sensor data acquired in the straight path test.

170
6 Shared Control
0
2
4
6
8
10
12
14
0
2
4
6
8
10
12
14
x (ft)
y (ft)
Circular path test
#1
#2
#3
Fig. 6.13. Gyrover trajectories in the curved path test.

6.6 Discussions
171
0
1
2
3
4
5
6
7
8
9
40
60
80
100
120
140
β [deg]
time(sec)
lean angle of Gyrover
0
1
2
3
4
5
6
7
8
9
0
20
40
60
βa [deg]
time(sec)
tilt angle of flywheel
0
1
2
3
4
5
6
7
8
9
0
5
10
15
Drive Command
time(sec)
Drive command (Human)
0
1
2
3
4
5
6
7
8
9
−30
−20
−10
0
10
Tilt Command
time(sec)
Tilt command (Human)
0
1
2
3
4
5
6
7
8
9
−30
−20
−10
0
10
Tilt Command
time(sec)
Final tilt command (robot)
Fig. 6.14. Sensor data acquired in the circular path test.

172
6 Shared Control
0
2
4
6
8
10
12
14
0
5
10
15
20
25
30
35
40
45
50
55
x (ft)
y (ft)
Combined path test
#3
#2
#1
Fig. 6.15. Gyrover trajectories in the combined path test.

6.6 Discussions
173
0
2
4
6
8
10
12
14
16
18
20
22
40
60
80
100
120
140
β [deg]
time(sec)
lean angle of Gyrover
0
2
4
6
8
10
12
14
16
18
20
22
−40
−20
0
20
40
βa [deg]
time(sec)
tilt angle of flywheel
0
2
4
6
8
10
12
14
16
18
20
22
−5
0
5
10
15
20
Drive Command
time(sec)
Drive command (Human)
0
2
4
6
8
10
12
14
16
18
20
22
−20
−10
0
10
20
Tilt Command
time(sec)
Tilt command (Human)
0
2
4
6
8
10
12
14
16
18
20
22
−20
−10
0
10
20
Tilt Command
time(sec)
Final tilt command (robot)
Fig. 6.16. Sensor data acquired in the combined path test.

7
Conclusions
7.1 Concluding Remarks
7.1.1 Concept and Implementations
Gyrover is a novel concept for mobility that has distinct advantages over
conventional, statically stable vehicles. The concept has been veriﬁed with
three working models. Gyrover is particularly suited for operations at high
speed and rough terrain, and also shows promise as a marine/amphibious
vehicle. The mechanism can be completely enclosed, presenting a very clean,
protected envelope.
During the past several years, we have made signiﬁcant advances in the
technologies for a gyroscopically stabilized wheel. We have gained an un-
derstanding of the basics of gyro-stabilization, and have demonstrated the
feasibility of the Gyrover concept.
7.1.2 Kinematics and Dynamics
We have developed the kinematic and dynamic models of Gyrover in a horizon-
tal plan. In derivation of the model, we consider the robot as a combination of
three components: a rolling disk, an internal mechanism and a ﬂywheel. They
are linked by a two-link manipulator. We simpliﬁed the model by decoupling
the tilt angle of the ﬂywheel from the dynamics. We veriﬁed the model by
experiment and simulation. We demonstrated that the dynamics of the robot
is nonholonomic and underactuated. We demonstrated the dynamics coupling
between the wheel and the ﬂywheel, through the stabilization and tilting eﬀect
of the ﬂywheel on the robot.
Based on the above work, we have established the dynamics of a robot
rolling without slipping on an inclined plane. The pendulum swinging motion
is neglected and the vertical oﬀset of the actuation mechanism from the axis
of the whole wheel is reduced from the dynamics. If we let l1, l2 and θ be zero,
the dynamics are exactly the same as Gyrover on a horizontal plan.
Y. Xu and Y. Ou: Control of Single Wheel Robots, STAR 20, pp. 175–177, 2005.
© Springer-Verlag Berlin Heidelberg 2005

176
7 Conclusions
7.1.3 Model-based Control
First, using the linearization method, we developed a linear state feedback ap-
proach to stabilize the robot at any desired lean angle. This feedback provides
means for controlling the steering velocity of the robot. Finally, we developed
a line following controller for tracking any desired straight line while keeping
balance. The controller is composed of two parts: the velocity control law and
the torque control law. In the velocity control law, the velocity input (steering
velocity) is designed for ensuring the continuity of the path curvature. Then,
the robot can be stabilized for tracking a lean angle trajectory in which the
steering velocity is identical to the desired value. We addressed the eﬀects of
the initial heading angle, the rolling speed and the controller gains on the per-
formance of the controller. The work is of signiﬁcance in developing automatic
control for the dynamically stable but statically unstable robot.
Then, we studied control problems for a single wheel, gyroscopically stabi-
lized robot. We investigated the dynamics of the robot system, and analyzed
the two classes of nonholonomic constraints of the robot. We proposed three
control laws for balance, point-to-point control and line tracking. The three
problems considered are fundamental tasks for Gyrover control.
7.1.4 Learning-based Control
First, we proposed a novel method for human control strategy learning for
a dynamically stable system using an SVM. The SVM is a powerful tool
for function estimation and is feasible to characterize the stochastic process
in human control strategy learning. Moreover, we proposed approaches to
formulate conditions for the SVM learning control closed-form system to be
stable and to determine the domain of attraction, if it is stable.
Second, we proposed a new method of adding new training samples without
increasing costs. We use the local polynomial ﬁtting approach to individually
rebuild the time-variant functions of system states. Through interpolating, we
can eﬀectively produce any number of additional training samples. A number
of experiments based on a dynamically stable system – Gyrover – show that
by using additional unlabelled samples, the learning control performance can
be improved, and therefore the overﬁtting phenomenon can be mitigated.
Third, we examine a method of selecting input variables, through the eval-
uation of the signiﬁcance order function and dependent analysis to enhance the
learning control performance. It was observed that by executing the previous
input selection process in the modeling process, the ‘curse of dimensionality’
phenomenon might be mitigated. The mitigation of the ‘curse of dimensional-
ity’ phenomenon is important in a learning control process, especially that of
a dynamically stable system like Gyrover, for the following reasons. First, the
initial condition may diﬀer from case to case. Second, the controllers and the
system states are combined to form an integrated system and interact with

7.2 Future Research Directions
177
each other. Third, if the learning model does not eﬀectively set up the map-
ping between the system states and the control inputs, the learning controllers
from the model will be rigid and fail to make the control process converge to
the targets.
Last, we develop a shared control framework for Gyrover, based on the
behavior-based architecture. Since building a fully autonomous system is
costly and sometimes not practical, the main purpose of shared control is to re-
duce the operator’s control burden in a complex system. In order to distribute
the workload systematically in a shared control environment, we develop a
decision function to let the system judges whether to execute the operator’s
command or not, by considering the Degree of Autonomy, the Strength of
Conﬂict and the Conﬁdence level. Experiments show that this shared control
framework is able to share some of the control tasks from the operator without
decreasing the maneuverability of the robot.
7.2 Future Research Directions
While this book provides a great deal on mechanism design and control im-
plementation of Gyrover, there are a number of diﬀerent directions in which
the area in this work can be extended and applied. The followings are some
possible improvements and extension of this work.
We proposed the foundations of the shared control of Gyrover. It is only the
ﬁrst step towards full implementation. We hope that the learning controllers
are not only to be shared with a human operator, but also to be shared with
the model-based control.
We are seeking the possibility of applying these technologies on other dy-
namically stable systems, such as inverted pendulum systems, rolling disks,
bicycles, biped robots, hopping robots and so on. We believe that these tech-
nologies are suitable in overcoming the engineering challenges for the control
and design of similar systems as Gyrover.

References
1. M. Aicardi, G. Casalino, A. Balestrino, and A. Bicchi, “Closed loop smooth
steering of unicycle-like vehicles,” Proc. of the 33th IEEE Conf. on Decision
and Control, vol. 3, pp. 2455 -2458, 1994.
2. P. Aigner and B. McCarragher, “Shared control framework applied to a robotic
aid for the blind,” Proc. of the 1998 IEEE International Conference on Robotics
and Automation, vol. 1, pp. 717-722, 1998.
3. H. Arai and S. Tachi, “Position control of a manipulator with passive joints
using dynamic coupling”, IEEE Trans on Robotics and Automation, vol.7, no.4,
pp.528-534, 1991.
4. H. Asada and S. Liu, “Transfer of human skills to neural net robot controllers,”
Proc. of the 1999 IEEE International Conference on Robotics and Automation,
1991.
5. K. W. Au, “Dynamics and control of a single wheel, gyroscopically stabilized
robot,” M.Phil. Thesis, The Chinese University of Hong Kong, 1999.
6. K. W. Au and Y. S. Xu, “Decoupled dynamics and stabiliztion of single wheel
robot,” Proc. of the 1999 IEEE/RSJ International Conference on Intelligent
Robots and Systems , vol. 1, pp. 197-203, 1999.
7. K. W. Au and Y. S. Xu, “Path following of a single wheel robot,” Proc. of the
2000 IEEE International Conference on Robotics and Automation, vol. 3, pp.
2925-2930, 2000.
8. A. R. Barron, “Universal approximation bounds for superpositions of a sig-
moidal function,” IEEE Trans. on Information Theory,
vol. 39, no. 3, pp.
930-945, 1993.
9. R. Battiti, “Using mutual information for selecting features in supervised neural
net learning,” IEEE Trans. on Neural Networks, vol. 5, no. 4, pp. 537-550, 1994.
10. D. A. Bell, S. P. Levine, Y. Koren, L. A. Jaros, and J. Borenstein, “Design
criteria for obstacle avoidance in a shared-control system,” Whitaker Student
Scientiﬁc Paper Competition, RESNA’94 Annual Conference, June, 1994.
11. R. Bellman, Adaptive control processes: a guided tour. New Jersey: Princeton
University Press, 1961.
12. M. G. Bekker, Theory of land locomotion, University of Michigan Press, pp.213-
217, 1956.
13. M. G. Bekker, “Accomplishments and future tasks in oﬀ-road transportation”,
Journal of Terramechanics, vol.11, no.2, pp.11-30, Permagon Press, Ltd., 1974.

180
References
14. M. Bergerman and Y. Xu, “Dynamic coupling Index for underactuated manip-
ulators,” Journal of Robotic Systems, vol.12, no.10, pp.693-707, 1995.
15. S. Bernhard, C. J. C. Burges, and A. Smola, Advanced in kernel methods support
vector learning, Cambridge, MA, MIT Press, 1998.
16. A. V. Beznos, et. al., “Control of autonomous motion of two-wheel bicycle with
gyroscopic stabilisation,” Proc. IEEE Int. Conf. on Robotics and Automation,
vol. 3, pp. 2670-75, 1998.
17. C. Bishop, “Exact calulation of hessian matrix for the multilayer perceptron,”
Neural Computation, vol. 4, pp. 494-501, 1992.
18. A. M. Bloch, M. Reyhanoglu, and N. H. McClamroch, “Control and stabi-
lization of nonholonomic dynamic systems,” IEEE Transaction on Automation
Control, vol. 37, no. 11, pp. 1746-1756, 1992.
19. A. L. Blum and P. Largely, “Selection of relevant features and examples in
machine learning,” Artif. Intell., vol. 97, pp. 245-271, 1997.
20. G. Bourhis and Y. Agostini, “Man-machine cooperation for the control of an
intelligent powered wheelchair,” Journal of Intelligent and Robotics Systems,
vol. 22, no.3-4 pp. 269-287, 1998.
21. H. B. Brown and Y. Xu, “A single wheel gyroscopically stabilized robot,” Proc.
IEEE Int. Conf. on Robotics and Automation, vol. 4, pp. 3658-63, 1996.
22. M. Buhler, D. E. Koditschek, and P. J. Kindlmann, “A simple juggling robot:
theory and experimentation,” Experimental Robotics, edited by V. Hayward
and O. Khatib, pp.35-73, Springer-Verlag, 1989.
23. C. J. C. Burges and B. Scholkopf, “Improving the accuracy and speed of support
vector learning machines,” In M. Mozer, M. Jordan and T. Petsche Editors,
Advances in neural information processing Systems 9. Cambridge, MA, MIT
Press, pp. 375-381, 1997.
24. G. Chryssolouris, M. Lee, and A. Ramsey, “Conﬁdence interval prediction for
neural network models,” IEEE Trans. on Neural Networks, vol. 7, no. 1, pp.
229-232, 1996.
25. T. Cibas, F. F. Soulie, P. Gallinari and S. Raudys, “Variable selection with
neural networks,” Neural Computing, vol. 12, pp. 223-248, 1996.
26. R. A. Cooper, “Intelligent control of power wheelchairs,” IEEE Engineering in
Medicine and Biology Magazine, vol.14, issue: 4, pp. 423-431, 1995.
27. N. Cristianini and J. Shawe-Taylor, A Introduction to Support Vector Machines
and Other Kernel-based Learning Methods, Cambridge University Press, Cam-
bridge, 2000.
28. G. Cybenko, “Approximations by suppositions of s sigmoidal function,” Math-
ematics of Control, Signal and Systems, no. 2, pp. 871-875, 1989.
29. A. Douglas and Y. Xu, “Real-time shared control system for space telerobotics,”
Proc. of the 1993 IEEE/RSJ International Conference on Intelligent Robots and
Systems ’93, IROS ’93, vol. 3, pp. 2117-22, 1993.
30. D. R. Freitag, “History of wheels for oﬀ-road transport,” Journal of Terrame-
chanics, vol. 16, no. 2, pp. 49-68, Permagon Press, Ltd., 1979.
31. S. S. Ge, C. C. Huang, and T. Zhang, “Adaptive neural network control of
nonlinear systems by state and output feedback,” IEEE Trans. on Systems,
Man, and Cybernetics–Part B: Cybernetics, vol. 29, no. 6, pp. 818-828, 1999.
32. S. S. Ge, G. Y. Li, and T. H. Lee, “Adaptive NN control for a class of strict-
feedback discrete-time nonlinear systems,” Automatica, vol. 39, pp. 807-819,
2003.

References
181
33. R. Genesio, M. Tartaglia, and A. Vicino, “On the estimation of asymptotic
stability regions: stae of the art and new proposals,” IEEE Transactions on
Automatic Control, vol. 30, no. 8, pp. 747-755, 1985.
34. N. H. Getz, “Dynamic inversion of nonlinear maps with applications to non-
linear control and robotics,” Ph.D. thesis, University of California at Berkeley,
1995.
35. N. H. Getz, “Control of balance for a nonlinear nonholonomic non-minimum
phase model of a bicycle,” Proc. ACC, vol. 1, pp. 148-151.
36. C. G. Gingrich, D. R. Kuespert, and T. J. McAvoy, “Modeling human operators
using neural networks,” ISA Trans., vol. 31, no. 3, pp. 81-90, 1992.
37. T. Glad and L. Ljung, Eds., Comtrol Theory: Multivariable and Nonlinear
Methods, London and New York : Taylor & Francis, 2000.
38. F. A. Graybill, Theory and Application of Linear Model, North Scituate, MA:
Duxbury Press, 1976.
39. C. Guo, T. Tarn, N. Xi, and A.K. Bejczy, “Fusion of human and machine intelli-
gence for telerobotic systems,” Proc. of the 1995 IEEE International Conference
on Robotics and Automation, vol. 3, pp. 3310-15, 1995.
40. Y. Hamamoto, S. Uchimura, T. Kanaoka, and S. Tomita, “Evaluation of artiﬁ-
cial neural network classiﬁers in small sample size situations,” Proc. Int. Conf.
Neural Networks, pp. 1731-1735, 1993.
41. Y. Hamamoto, Y. Mitani, and S. Tomita, “On the eﬀect of the noise injection
in small training sample size situations,” Proc. Int. Conf. Neural Information
Processing, vol. 1, pp. 626-628, 1994.
42. C. Hautier, “Gyroscopic device for stabilization of laterally unstable vehicles,”
U.S. Patent, #3,787,066, 1974.
43. R. C. Hemmings, “Improvement in velocipede,” U.S. Patent, #92,528, 1869.
44. J. K. Hodgins and M. H. Raibert, “Adjusting step length for rough terrain loco-
motion,” IEEE Transactions on Robotics and Automation, vol.7 no.3, pp.289-
298, 1991.
45. I. C. Holm,“Articulated, wheeled oﬀ-road vehicles,” Journal of Terramechanics,
vol. 7, no. 1, pp. 19-54, Permagon Press, Ltd., 1970.
46. G. Indiveri, “Kinematic time-invariant control of a 2D nonholonomic vehicle,”
Proc. IEEE Conf. Decis. Control, vol. 3, pp. 2112-2117, 1999.
47. E. G. John, Modern Mechanix and Inventions, June, 1935.
48. Y. J. Kanayama and F. Fahroo, “A new line tracking method for nonholonomic
vehicles,” Proc. IEEE Int. Conf. on Robotics and Automation, vol. 4, pp. 2908-
11, 1997.
49. Y. J. Kanayama, Y. Kimura, F. Miyazaki, and T. Noguchi, “A stable track-
ing control method for a nonholonomic mobile robot,” Proc. IEEE/RSJ Int.
Workshop on Intelligent Robots and Systems, vol. 3, pp. 1236-41, 1991.
50. J. Karhunen and J. Joutsensalo, “Generalizations of principal component anal-
ysis, optimization problems, and neural networks,” Neural Computing, vol. 8,
pp. 529-562, 1995.
51. J. Karhunen, L. Wang and R. Vigario, “Nonlinear PCA type approaches for
source separation and independent component analysis,” Proc. 1995 Interna-
tional Conference on Artiﬁcial Neural Networks, vol. 2, pp. 995-1000, 1995.
52. A. Kemurdjian, et al, “Small Marsokhod conﬁguration,” Proc. IEEE Int. Conf.
on Robotics and Automation, vol. 1, pp. 165-68, 1992.

182
References
53. Y. H. Kim and F. L. Lewis, “Neural network output feedback control of robot
manipulators,” IEEE Transactions on Robotics and Automation, vol. 15, no. 2,
pp. 301-309, 1999.
54. K. Kira and L. A. Rendell, “The feature selection problem: traditional methods
and a new algorithm,” Proc. 10th National Conference on Artiﬁcial Intelligence,
pp. 129-134, 1992.
55. P. R. Klarer, “Recent developments in the robotics all terrain lunar explorer
rover (RATLER) program,” ASCE Specialty Conference on Robotics for Chal-
lenging Environments, Albuquerque, NM, 1994.
56. R. Kohavi and H. J. George, “Wrappers for feature subset selection,” Artif.
Intell., vol. 97, pp. 273-324, 1997.
57. I. Kolmanovsky and N. H. McClamroch, “Development in nonholonomic control
problems,” IEEE Contr. Syst. Mag., vol. 15, pp. 20-36, Jan., 1995.
58. I. Kolmanovsky and N. H. McClamroch, “Application of integrator backstep-
ping to nonholonomic control problems,” Proc IFAC Nonlinear Control Syst.
Des. Symp., pp. 747-752, 1995.
59. A. Kosushi and K. Yamafuji, “Development and motion control of the all di-
rection steering-type robot (1st report: a concept of spherical shaped robot,
roll and running control),” Proceedings of 9th Japanese Robotics Conference,
Japan, 1991.
60. E. P. Krotkov, R. G. Simmons, and W. L. Whittaker, “Ambler: Performance
of A Six-legged Planetary Rover,” Acta Astronautica, vol. 35, no. 1, 1995.
61. J. P. LaSalle, “Stability theory for diﬀerence equations,” Studies in Ordinary
Diﬀerential Equations, MAA Studies in Math., Amer. Math. Assoc., pp.1-31,
1977.
62. K. K. Lee and Y. Xu, “Human sensation modeling in virtual environments,”
Proceedings of 2000 IEEE/RSJ International Conference on Intelligent Robots
and Systems, vol.1, pp. 151-156, 2000.
63. T. C. Lee, K. T. Song, C. H. Lee, and C. C. Teng, “Tracking control of unicycle-
modeled mobile robots using a saturation feedback controller,” IEEE Transac-
tions on Control System Technology, vol. 9, no. 2, pp. 305-318, 2001.
64. S. Lee and H. S. Lee, “An advanced teleoperator control system: design and
evaluation,” Proc. of 1992 IEEE International Conference on Robotics and Au-
tomation, vol. 1, pp. 859-864, 1992.
65. A. U. Levin and K. S. Narendra, “Control of nonlinear dynamical systems
using neural networks: controllability and stabilization,” IEEE Transactions
on Neural Networks, vol. 4, no. 2, pp. 192-206, 1993.
66. A. U. Levin, “An analysis method for estimating the domain of attraction for
polynomial diﬀerential equations,” IEEE Transactions on Automatic Control,
vol. 36, pp. 2471-2475, 1994.
67. R. A. Lindemann and H. J. Eisen, “Mobility analysis, simulation and scale
model testing for the design of wheeled planetary rovers”, Missions, Technolo-
gies, and Design of Planetary Mobile Vehicles, 1992.
68. H. Liu and H. Motoda, Feature Selection for Knowledge Discovery and Data
Mining, Norwell, MA: Kluwer Academic Publishers, 1998.
69. H. A. Malki and A. Moghaddamjoo, “Using the Karhunen-Loe transformation
in the back-propagation traning algorithm,” IEEE Trans. on Neural Networks,
vol. 2, no. 1, pp. 162-165, 1991.
70. T. McGeer, “Passive dynamic walking”, International Journal of Robotics Re-
search, 1989.

References
183
71. W. T. Miller, R. S. Sutton, and P. I. Werbos, Eds., Neural Networks for Control,
Cambridge, MA: MIT press, 1990.
72. E. G. John , Modern Mechanix and Inventions, June, 1935.
73. J. F. Montgomery and G. A. Bekey, “Learning helicopter control through
“teaching by showing,” Proc. IEEE Int. Conf. on Decision and Control, vol. 4,
pp. 3647 -3652, 1998.
74. G. C. Nandy and Y. Xu, “Dynamic Model of A Gyroscopic Wheel,” Proc. of
the 1998 IEEE International Conference on Robotics and Automation , vol. 3,
pp. 2683-2688, 1998
75. R. Nakajima, T. Tsubouchi, S. Yuta, and E. Koyanagi, “A development of a
new mechanism of an autonomous unicycle,” Proc. IEEE/RSJ Int. Conf. on
Intelligent Robots and Systems, vol. 4, pp. 3658-3663, 1997.
76. M. Nechyba, “Learning and validation of human control strategy,” Ph.D. The-
sis, Carnegie Melon University, 1998.
77. M. Nechyba and Y. Xu, “Stochastic similarity for validating human control
strategy models”, IEEE Transactions on Robotics and Automation, vol. 14 ,
no. 3, pp. 437-451, June, 1998.
78. M. Nechyba and Y. Xu, “Cascade neural networks with node-decoupled ex-
tended Kalman ﬁltering,” Proc. IEEE Int. Symp. on Computational Intelligence
in Robotics and Automation, vol. 1, pp. 214-219, 1997.
79. S. H. Oliver and D. H. Berkebile, Wheels and Wheeling, Smithsonian Institution
Press, 1974.
80. G. Oriolo and Y. Nakamura, “Control of mechanical systems with second-
order nonholonomic constraints: underactuated manipulators,” Proc. of the
30th IEEE Conf. on Decision and Control, vol. 3 pp. 2398-2403, 1991.
81. Y. Ou and Y. Xu, “Balance control of a single wheel robot,” Proc. of the 2000
IEEE/RSJ International Conference on Intelligent Robots and Systems , vol.
2, pp. 1361-1366, 2002.
82. A. Palmer, Riding High: the Story of the Bicycle, Dutton, NY, 1956.
83. S. J. Raudys and A. K. Jain, “Small sample size eﬀects in statistical pattern
recognition: recommendations for practitioners,” IEEE Transactons PAMI-13,
no. 3, pp. 252-264, 1991.
84. S. J. Raudys and A. K. Jain, “Small sample size problems in designing artiﬁcial
neural networks,” in I. K. Sethi and A. K. Jain, Eds. Artiﬁcial Neural Networks
and Statistical Pattern Recognition Old and new Connections, Elsevier Science
Publishers, pp. 35-50, 1991.
85. M. F. Redondon and C. H. Espinosa, “Neural networks input selection by using
the training set,” International Joint Conference on Neural Networks, vol. 2,
pp. 1189 -1194, 1999.
86. M. Reyhanoglu, A.J. van der Schaft, N.H. McClamroch, and I. Kolmanovsky,
“Dynamics and control of a class of underactuated mechanical systems,” IEEE
Transactions on Automatic Control, vol. 44, no. 9, pp. 1663-1671, 1999.
87. C. Rui and N. H. McClamroch, “Stabilization and asymptotic path tracking
of a rolling disk,” Proc IEEE Int. Conf. on Decision and Control, vol 4, pp.
4294-4299, 1995.
88. F. Saito, T. Fukuda, and F. Arai, “Swing and locomotion control for two-link
brachiation robot,” IEEE Control Systems Magazine, vol.14, no.1, pp. 5-12,
Feb., 1994.
89. C. Samson, “Time-varying feedback stabilization of car like wheeled mobile
robot,” Int. Journal of Robotics Research, vol. 12, no. 1, pp. 55-64, 1993.

184
References
90. C. Samprit and S. H. Ali, Sensitivity Analysis in Linear Regression, New York:
John Wiley & Sons, 1988.
91. N. Sarkar, “Control of vehicles with two steerable wheels,” ICRA’97 Workshop
Innovative Design of Wheeled Mobile Robots, 1997.
92. S. Schael and C. G. Atkeson, “Robot juggling: implementation of memory-
based learning”, IEEE Control Systems Magazine, vol.14, no.1, pp.57-71, Feb.,
1994.
93. G. A. Seber and R. B. Schnabel, Nonlinear Regression. New York: Wiley, 1989.
94. P. Seibert, “Stability under perturbations in generalized dynamical systems,”
in J. P. LaSalle and S. Lefschetz Editors, Nonlinear Diﬀerencial Equations and
Nonlinear Mechanics, New York: Academic Press, pp.1-31, 1977.
95. D. Shevitz and B. Paden, “Lyapunov stability theory of nonsmooth systems,”
IEEE Transactions on Automatic Control, vol. 39, no. 9, pp. 1910-1914, 1994.
96. Z. Sheng and K. Yamafuji, “Postural stability of a human riding a unicycle and
its emulation by a robot,” IEEE Transactions on robotics and automation, vol.
13, no. 5, pp. 709-20, 1997.
97. R. C. Simpson and S. P. Levine, “Adaptive shared control of a smart wheelchair
operated by voice control,” Proceedings of the 1997 IEEE/RSJ International
Conference on Intelligent Robots and Systems, IROS’97, vol. 2, pp. 622-626,
1997.
98. A. Smola, “General cost function for support vector regression,” Proceedings of
the Ninth Australian Conf. on Neural Networks, pp. 79-83, 1998.
99. O. J. Sordalen C. and Canudas de Witt, “Exponential control law for a mobile
robot: extension to path following,” Proc. IEEE Int. Conf. on Robotics and
Automation, vol. 3, pp. 2158-2163, 1992.
100. M. W. Spong, “The swing up control problem for the acrobot,” IEEE Control
Systems Magazine, vol.15, no.1, pp.49-55, Feb., 1995.
101. J. M. Stepper, K. W. Baur Jr., and S. K. Rogers, “Integrated feature and
architecture selection,” IEEE Transactons on Neural Networks, vol. 7, no. 4,
pp. 1007-1014, 1996.
102. T. O. Summers, “Gyro stabilized vehicle,” U.S. Patent, #3,410,357, 1968.
103. K. A. Tahboub and H. H. Asada, “A semi-autonomous control architecture
applied to robotic wheelchairs,” Proceedings of the 1999 IEEE/RSJ Interna-
tional Conference on Intelligent Robots and Systems, 1999. IROS ’99., vol. 2,
pp. 906-911, 1999.
104. A. Tayebi and A. Rachid, “A uniﬁed discontinuous state feedback controller
for the path-following and the point-stabilization problems of a unicycle-like
mobile robot,” Proc. IEEE Int. Conf. on Control Applications, pp. 31-35, 1997.
105. R. Thawonmas and S. Abe, “Feature reduction based on analysis of fuzzy
regions,” Proc. 1995 International Conference on Artiﬁcial Neural Networks,
vol. 4, pp. 2130-2133, 1995.
106. B. Tibken and K. F. Dilaver, “Computation of subsets of the domain of attrac-
tion for polynomial systems,” Proc. IEEE Int. Conf. on Decision and Control,
pp. 2651-2656, 2002.
107. S. Tsai, E. D. Ferreira, and C. J. J. Paredis, “Control of the Gyrover,” Proc.
of the 1999 IEEE/RSJ International Conference on Intelligent Robots and Sys-
tems , pp. 179-184, 1999
108. V. N. Vapnik, The Nature of Statistical Learning Theory, Springer-Verlag, New
York, 1995.

References
185
109. D. W. Vos and A. H. Von Flotow, “Dynamics and nonlinear adaptive control
of an autonomous unicycle: theory and experiment,” Proceedings of the 29th
IEEE Conference on Decision and Control, 1990.
110. K. Waldron, The Adaptive Suspension Vehicle, MIT Press, 1989.
111. I. A. Weaver, “Gyroscopic vehicle steering stabilizer,” U.S. Patent, #1,945,874,
1934.
112. Y. Xu, H. B. Brown, and K. W. Au, “Dynamics mobility with single-wheel
conﬁguration,” Int. J. of Rob. Res., vol. 18, no. 7, pp. 728-738, 1999.
113. Y. S. Xu and L. W. Sun, “Stabilization of a gyroscopically stabilized robot on
an inclined plane,” Proceedings of the 2000 IEEE International Conference on
Robotics and Automation, vol. 4, pp. 3549-54, 2000.
114. Y. S. Xu and L. W. Sun, “Dynamics of a rolling disk and a single wheel
robot on an inclined plane,” Proceedings of the 2000 IEEE/RSJ International
Conference on Intelligent Robots and Systems, vol. 1, pp. 811-816, 2000.
115. Y. Xu, K. W. Au, G. C. Nandy, and H. B. Brown, ”Analysis of actuation and
dynamic balancing for a single wheel robot,” Proc. IEEE/ISJ Int. Conf. on
Intelligent Robots and Systems, vol.4, pp.3658-3663,1998.
116. Y. Xu, W. Yu, and K. Au, “Modeling human control strategy in a dynamically
stabilized robot,” Proc. of the 1999 IEEE/RSJ Int. Conf. on Intelligent Robots
and Systems, vol. 2, pp. 507-512, 1999.
117. X. Yun, “Rigid body motion analysis towards rotary vehicle”, ICRA’97 Work-
shop Innovative Design of Wheeled Mobile Robots, 1997.
118. J. Yang, Y. Xu, and C. S. Chen, “Human action learning via hidden Markov
model,” IEEE Transactons on System, Man, and Cybernetics, vol. 27, no. 1,
pp. 34-44, 1997.
119. J. Yang, Y. Xu, and C. S. Chen, “Hidden Markov model approach to skill
learning and its application to telerobotics,” IEEE Transactions on Robotic
and Automation, vol. 10, no. 5, pp. 621-631, 1994.
120. Y. Yokokohji, A. Ogawa, H. Hasunuma, and T. Yoshikawa, “Operation modes
for cooperating with autonomous functions in intelligent teleoperation sys-
tems,” Proc. of the 1993 IEEE International Conference on Robotics and Au-
tomation, vol. 3, pp. 510-515, 1993.
121. S. You, T. Wang, J. Wei, F. Yang, and Q. Zhang, “Share control in intelligent
arm/hand teleoperated system,” Proceedings of the 1999 IEEE International
Conference on Robotics and Automation, vol. 3, pp. 2489-94, 1999.
122. S. Yuta, “A novel mechanism of an autonomous unicycle,” ICRA’97 Workshop
Innovative Design of Wheeled Mobile Robots, 1997.
123. J. Zabczyk, “Some comments on stabilizability,” Applied Math. Optimization,
vol. 19, pp. 1-9, 1989.
124. K. W. Au and Y. Xu, “Decoupled dynamics and stabilization of single wheel
robot,” Proc. IEEE/RSJ Int. Workshop on Intelligent Robots and Systems, vol.
1, pp. 197-203, 1999.

Index
actual risk, 84
R(w), 84
Backpropagation neural networks, 86
BPNN, 86
backstepping, 26
Balance Control, 46–50
Cascade Neural Network, 74
CNN, 74
Curse of dimensionality, 101
Dependency Analysis, 125–127
(linear) relative, 125
dependent, 125
relevant, 125
desirable equilibrium, 87
empirical risk, 84
Remp(w), 84
ﬁrst-order nonholonomic constraint, 42,
45
Gyrover, 5
drive motor, 5
dynamic model, 13–21
Dynamic Properties, 19
Hardware
Accelerometer, 65
Drive Motor and Encoder, 65
Gyro, 65
Radio Receiver, 65
Speed Controller, 65
Tilt Potentiometer, 65
Tilt Servo, 65
spin motor, 5
title motor, 5
Variables deﬁnition, 14
α, αa, 14
β, βa, 14
γ, γa, 14
Drive torque, 14
Lean angles, 14
Spin angles, 14
Tilt angle, 14
Gyrover I, 10
Gyrover II, 10
Gyrover III, 8
HCS, 73
Injective mapping, 81
Input Selection, 116
kinematic constraints, 36
Lemma 1, 48
Linearized Model, 33
Local Polynomial Fitting, 110–112
LPF, 110
Local sensitivity analysis, 121
Local SA, 121
NDEKF, 75
Nonholonomic Constraints, 44–46
nonholonomic system, 14
Normal form of the system, 18

188
Index
overﬁtting, 103
Path Following Control, 36–40
Line Following, 37
Torque control law, 40
Velocity control law, 38
Position Control, 50–53
principal component analysis, 118
PCA, 118
PSD, 78
Quasi-static, 1
region of operation, 87
Reliable training set, 80
Sample Data Selection, 118
second-order nonholonomic constraint,
42, 46
Shared Control, 135
Combined Mode, 139
Conﬁdence level, 142
Degree of Autonomy, 141
Distributed Mode, 138
Strength of conﬂict, 141
Switch Mode, 138
spinning ﬂywheel, 6
Stabilization, 33
strongly stable under perturbations, 90
SSUP, 90
Structure Risk Minimization, 107
SRM, 107
Support Vector Classiﬁcation, 83
SVC, 83
Support Vector Machines, 82–84
SVM, 82
Support Vector Regression, 83
SVR, 83
VC dimension, 102
Wheeled robots, 3
working space, 87


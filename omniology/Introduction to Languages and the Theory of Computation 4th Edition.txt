
Introduction to Languages
and The Theory of
Computation
Fourth Edition
John C. Martin
North Dakota State University

INTRODUCTION TO LANGUAGES AND THE THEORY OF COMPUTATION, FOURTH EDITION
Published by McGraw-Hill, a business unit of The McGraw-Hill Companies, Inc., 1221 Avenue of the
Americas, New York, NY 10020. Copyright c⃝2011 by The McGraw-Hill Companies, Inc. All rights reserved.
Previous editions c⃝2003, 1997, and 1991. No part of this publication may be reproduced or distributed in any
form or by any means, or stored in a database or retrieval system, without the prior written consent of The
McGraw-Hill Companies, Inc., including, but not limited to, in any network or other electronic storage or
transmission, or broadcast for distance learning.
Some ancillaries, including electronic and print components, may not be available to customers outside the
United States.
This book is printed on acid-free paper.
1 2 3 4 5 6 7 8 9 0 DOC/DOC 1 0 9 8 7 6 5 4 3 2 1 0
ISBN 978–0–07–319146–1
MHID 0–07–319146–9
Vice President & Editor-in-Chief: Marty Lange
Vice President, EDP: Kimberly Meriwether David
Global Publisher: Raghothaman Srinivasan
Director of Development: Kristine Tibbetts
Senior Marketing Manager: Curt Reynolds
Senior Project Manager: Joyce Watters
Senior Production Supervisor: Laura Fuller
Senior Media Project Manager: Tammy Juran
Design Coordinator: Brenda A. Rolwes
Cover Designer: Studio Montage, St. Louis, Missouri
(USE) Cover Image: c⃝Getty Images
Compositor: Laserwords Private Limited
Typeface: 10/12 Times Roman
Printer: R. R. Donnelley
All credits appearing on page or at the end of the book are considered to be an extension of the copyright page.
Library of Congress Cataloging-in-Publication Data
Martin, John C.
Introduction to languages and the theory of computation / John C. Martin.—4th ed.
p. cm.
Includes bibliographical references and index.
ISBN 978-0-07-319146-1 (alk. paper)
1. Sequential machine theory. 2. Computable functions. I. Title.
QA267.5.S4M29 2010
511.3′5–dc22
2009040831
www.mhhe.com

To the memory of
Mary Helen Baldwin Martin, 1918–2008
D. Edna Brown, 1927–2007
and to
John C. Martin
Dennis S. Brown

iv
C O N T E N T S
Preface
vii
Introduction
x
C H A P T E R 1
Mathematical Tools and
Techniques
1
1.1
Logic and Proofs
1
1.2
Sets
8
1.3
Functions and Equivalence Relations
12
1.4
Languages
17
1.5
Recursive Deﬁnitions
21
1.6
Structural Induction
26
Exercises
34
C H A P T E R 2
Finite Automata and the
Languages They Accept
45
2.1
Finite Automata: Examples and
Deﬁnitions
45
2.2
Accepting the Union, Intersection, or
Difference of Two Languages
54
2.3
Distinguishing One String
from Another
58
2.4
The Pumping Lemma
63
2.5
How to Build a Simple Computer
Using Equivalence Classes
68
2.6
Minimizing the Number of States in
a Finite Automaton
73
Exercises
77
C H A P T E R 3
Regular Expressions,
Nondeterminism, and Kleene’s
Theorem
92
3.1
Regular Languages and Regular
Expressions
92
3.2
Nondeterministic Finite Automata
96
3.3
The Nondeterminism in an NFA Can
Be Eliminated
104
3.4
Kleene’s Theorem, Part 1
110
3.5
Kleene’s Theorem, Part 2
114
Exercises
117
C H A P T E R 4
Context-Free Languages
130
4.1
Using Grammar Rules to Deﬁne a
Language
130
4.2
Context-Free Grammars: Deﬁnitions
and More Examples
134
4.3
Regular Languages and Regular
Grammars
138
4.4
Derivation Trees and Ambiguity
141
4.5
Simpliﬁed Forms and Normal Forms
149
Exercises
154
C H A P T E R 5
Pushdown Automata
164
5.1
Deﬁnitions and Examples
164
5.2
Deterministic Pushdown Automata
172

Contents
v
5.3
A PDA from a Given CFG
176
5.4
A CFG from a Given PDA
184
5.5
Parsing
191
Exercises
196
C H A P T E R 6
Context-Free and
Non-Context-Free Languages
205
6.1
The Pumping Lemma for
Context-Free Languages
205
6.2
Intersections and Complements of
CFLs
214
6.3
Decision Problems Involving
Context-Free Languages
218
Exercises
220
C H A P T E R 7
Turing Machines
224
7.1
A General Model of Computation
224
7.2
Turing Machines as Language
Acceptors
229
7.3
Turing Machines That Compute
Partial Functions
234
7.4
Combining Turing Machines
238
7.5
Multitape Turing Machines
243
7.6
The Church-Turing Thesis
247
7.7
Nondeterministic Turing Machines
248
7.8
Universal Turing Machines
252
Exercises
257
C H A P T E R 8
Recursively Enumerable
Languages
265
8.1
Recursively Enumerable
and Recursive
265
8.2
Enumerating a Language
268
8.3
More General Grammars
271
8.4
Context-Sensitive Languages and the
Chomsky Hierarchy
277
8.5
Not Every Language Is Recursively
Enumerable
283
Exercises
290
C H A P T E R 9
Undecidable Problems
299
9.1
A Language That Can’t Be
Accepted, and a Problem That Can’t
Be Decided
299
9.2
Reductions and the Halting
Problem
304
9.3
More Decision Problems Involving
Turing Machines
308
9.4
Post’s Correspondence Problem
314
9.5
Undecidable Problems Involving
Context-Free Languages
321
Exercises
326
C H A P T E R 10
Computable Functions
331
10.1 Primitive Recursive Functions
331
10.2 Quantiﬁcation, Minimalization, and
μ-Recursive Functions
338
10.3 G¨odel Numbering
344
10.4 All Computable Functions Are
μ-Recursive
348
10.5 Other Approaches to Computability
352
Exercises
353
C H A P T E R 11
Introduction to Computational
Complexity
358
11.1 The Time Complexity of a Turing
Machine, and the Set P
358

vi
Contents
11.2 The Set NP and Polynomial
Veriﬁability
363
11.3 Polynomial-Time Reductions and
NP-Completeness
369
11.4 The Cook-Levin Theorem
373
11.5 Some Other NP-Complete Problems
378
Exercises
383
Solutions to Selected
Exercises
389
Selected Bibliography
425
Index of Notation
427
Index
428

vii
P R E F A C E
T
his book is an introduction to the theory of computation. After a chapter
presenting the mathematical tools that will be used, the book examines models
of computation and the associated languages, from the most elementary to the most
general: ﬁnite automata and regular languages; context-free languages and push-
down automata; and Turing machines and recursively enumerable and recursive
languages. There is a chapter on decision problems, reductions, and undecidabil-
ity, one on the Kleene approach to computability, and a ﬁnal one that introduces
complexity and NP-completeness.
Speciﬁc changes from the third edition are described below. Probably the most
noticeable difference is that this edition is shorter, with three fewer chapters and
fewer pages. Chapters have generally been rewritten and reorganized rather than
omitted. The reduction in length is a result not so much of leaving out topics as of
trying to write and organize more efﬁciently. My overall approach continues to be
to rely on the clarity and efﬁciency of appropriate mathematical language and to
add informal explanations to ease the way, not to substitute for the mathematical
language but to familiarize it and make it more accessible. Writing “more efﬁ-
ciently” has meant (among other things) limiting discussions and technical details
to what is necessary for the understanding of an idea, and reorganizing or replacing
examples so that each one contributes something not contributed by earlier ones.
In each chapter, there are several exercises or parts of exercises marked with
a (†). These are problems for which a careful solution is likely to be less routine
or to require a little more thought.
Previous editions of the text have been used at North Dakota State in a
two-semester sequence required of undergraduate computer science majors. A one-
semester course could cover a few essential topics from Chapter 1 and a substantial
portion of the material on ﬁnite automata and regular languages, context-free
languages and pushdown automata, and Turing machines. A course on Turing
machines, computability, and complexity could cover Chapters 7–11.
As I was beginning to work on this edition, reviewers provided a number of
thoughtful comments on both the third edition and a sample chapter of the new one.
I appreciated the suggestions, which helped me in reorganizing the ﬁrst few chapters
and the last chapter and provided a few general guidelines that I have tried to keep
in mind throughout. I believe the book is better as a result. Reviewers to whom I
am particularly grateful are Philip Bernhard, Florida Institute of Technology; Albert
M. K. Cheng, University of Houston; Vladimir Filkov, University of California-
Davis; Mukkai S. Krishnamoorthy, Rensselaer Polytechnic University; Gopalan
Nadathur, University of Minnesota; Prakash Panangaden, McGill University; Viera
K. Proulx, Northeastern University; Sing-Ho Sze, Texas A&M University; and
Shunichi Toida, Old Dominion University.

viii
Preface
I have greatly enjoyed working with Melinda Bilecki again, and Raghu Srini-
vasan at McGraw-Hill has been very helpful and understanding. Many thanks to
Michelle Gardner, of Laserwords Maine, for her attention to detail and her unfailing
cheerfulness. Finally, one more thank-you to my long-suffering wife, Pippa.
What’s New in This Edition
The text has been substantially rewritten, and only occasionally have passages from
the third edition been left unchanged. Speciﬁc organizational changes include the
following.
1.
One introductory chapter, “Mathematical Tools and Techniques,” replaces
Chapters 1 and 2 of the third edition. Topics in discrete mathematics in the
ﬁrst few sections have been limited to those that are used directly in
subsequent chapters. Chapter 2 in the third edition, on mathematical
induction and recursive deﬁnitions, has been shortened and turned into the
last two sections of Chapter 1. The discussion of induction emphasizes
“structural induction” and is tied more directly to recursive deﬁnitions of sets,
of which the deﬁnition of the set of natural numbers is a notable example. In
this way, the overall unity of the various approaches to induction is clariﬁed,
and the approach is more consistent with subsequent applications in the text.
2.
Three chapters on regular languages and ﬁnite automata have been shortened
to two. Finite automata are now discussed ﬁrst; the ﬁrst of the two chapters
begins with the model of computation and collects into one chapter the topics
that depend on the devices rather than on features of regular expressions.
Those features, along with the nondeterminism that simpliﬁes the proof of
Kleene’s theorem, make up the other chapter. Real-life examples of both
ﬁnite automata and regular expressions have been added to these chapters.
3.
In the chapter introducing Turing machines, there is slightly less attention to
the “programming” details of Turing machines and more emphasis on their
role as a general model of computation. One way that Chapters 8 and 9 were
shortened was to rely more on the Church-Turing thesis in the presentation of
an algorithm rather than to describe in detail the construction of a Turing
machine to carry it out.
4.
The two chapters on computational complexity in the third edition have
become one, the discussion focuses on time complexity, and the emphasis
has been placed on polynomial-time decidability, the sets P and NP, and
NP-completeness. A section has been added that characterizes NP in terms
of polynomial-time veriﬁability, and an introductory example has been added
to clarify the proof of the Cook-Levin theorem, in order to illustrate the idea
of the proof.
5.
In order to make the book more useful to students, a section has been added
at the end that contains solutions to selected exercises. In some cases these
are exercises representative of a general class of problems; in other cases the

Preface
ix
solutions may suggest approaches or techniques that have not been discussed
in the text. An exercise or part of an exercise for which a solution is
provided will have the exercise number highlighted in the chapter.
PowerPoint slides accompanying the book will be available on the McGraw-
Hill website at http://mhhe.com/martin, and solutions to most of the exercises will
be available to authorized instructors. In addition, the book will be available in
e-book format, as described in the paragraph below.
John C. Martin
Electronic Books
If you or your students are ready for an alternative version of the traditional text-
book, McGraw-Hill has partnered with CourseSmart to bring you an innovative
and inexpensive electronic textbook. Students can save up to 50% off the cost of
a print book, reduce their impact on the environment, and gain access to powerful
Web tools for learning, including full text search, notes and highlighting, and email
tools for sharing notes between classmates. eBooks from McGraw-Hill are smart,
interactive, searchable, and portable.
To review comp copies or to purchase an eBook, go to either www.
CourseSmart.com <http://www.coursesmart.com/>.
Tegrity
Tegrity Campus is a service that makes class time available all the time by automat-
ically capturing every lecture in a searchable format for students to review when
they study and complete assignments. With a simple one-click start and stop pro-
cess, you capture all computer screens and corresponding audio. Students replay
any part of any class with easy-to-use browser-based viewing on a PC or Mac.
Educators know that the more students can see, hear, and experience class
resources, the better they learn. With Tegrity Campus, students quickly recall key
moments by using Tegrity Campus’s unique search feature. This search helps stu-
dents efﬁciently ﬁnd what they need, when they need it, across an entire semester
of class recordings. Help turn all your students’ study time into learning moments
immediately supported by your lecture.
To learn more about Tegrity, watch a 2-minute Flash demo at http://
tegritycampus.mhhe.com

x
I N T R O D U C T I O N
C
omputers play such an important part in our lives that formulating a “theory
of computation” threatens to be a huge project. To narrow it down, we adopt
an approach that seems a little old-fashioned in its simplicity but still allows us
to think systematically about what computers do. Here is the way we will think
about a computer: It receives some input, in the form of a string of characters; it
performs some sort of “computation”; and it gives us some output.
In the ﬁrst part of this book, it’s even simpler than that, because the questions
we will be asking the computer can all be answered either yes or no. For example,
we might submit an input string and ask, “Is it a legal algebraic expression?” At
this point the computer is playing the role of a language acceptor. The language
accepted is the set of strings to which the computer answers yes—in our example,
the language of legal algebraic expressions. Accepting a language is approximately
the same as solving a decision problem, by receiving a string that represents an
instance of the problem and answering either yes or no. Many interesting compu-
tational problems can be formulated as decision problems, and we will continue
to study them even after we get to models of computation that are capable of
producing answers more complicated than yes or no.
If we restrict ourselves for the time being, then, to computations that are
supposed to solve decision problems, or to accept languages, then we can adjust
the level of complexity of our model in one of two ways. The ﬁrst is to vary the
problems we try to solve or the languages we try to accept, and to formulate a
model appropriate to the level of the problem. Accepting the language of legal
algebraic expressions turns out to be moderately difﬁcult; it can’t be done using
the ﬁrst model of computation we discuss, but we will get to it relatively early in
the book. The second approach is to look at the computations themselves: to say
at the outset how sophisticated the steps carried out by the computer are allowed
to be, and to see what sorts of languages can be accepted as a result. Our ﬁrst
model, a ﬁnite automaton, is characterized by its lack of any auxiliary memory,
and a language accepted by such a device can’t require the acceptor to remember
very much information during its computation.
A ﬁnite automaton proceeds by moving among a ﬁnite number of distinct states
in response to input symbols. Whenever it reaches an accepting state, we think of
it as giving a “yes” answer for the string of input symbols it has received so far.
Languages that can be accepted by ﬁnite automata are regular languages; they can
be described by either regular expressions or regular grammars, and generated
by combining one-element languages using certain simple operations. One step up
from a ﬁnite automaton is a pushdown automaton, and the languages these devices
accept can be generated by more general grammars called context-free grammars.
Context-free grammars can describe much of the syntax of high-level programming

Introduction
xi
languages, as well as related languages like legal algebraic expressions and bal-
anced strings of parentheses. The most general model of computation we will
study is the Turing machine, which can in principle carry out any algorithmic
procedure. It is as powerful as any computer. Turing machines accept recursively
enumerable languages, and one way of generating these is to use unrestricted
grammars.
Turing machines do not represent the only general model of computation,
and in Chapter 10 we consider Kleene’s alternative approach to computability.
The class of computable functions, which turn out to be the same as the Turing-
computable ones, can be described by specifying a set of “initial” functions and a
set of operations that can be applied to functions to produce new ones. In this way
the computable functions can be characterized in terms of the operations that can
actually be carried out algorithmically.
As powerful as the Turing machine model is potentially, it is not especially
user-friendly, and a Turing machine leaves something to be desired as an actual
computer. However, it can be used as a yardstick for comparing the inherent com-
plexity of one solvable problem to that of another. A simple criterion involving
the number of steps a Turing machine needs to solve a problem allows us to dis-
tinguish between problems that can be solved in a reasonable time and those that
can’t. At least, it allows us to distinguish between these two categories in principle;
in practice it can be very difﬁcult to determine which category a particular problem
is in. In the last chapter, we discuss a famous open question in this area, and look
at some of the ways the question has been approached.
The fact that these elements (abstract computing devices, languages, and var-
ious types of grammars) ﬁt together so nicely into a theory is reason enough to
study them—for people who enjoy theory. If you’re not one of those people, or
have not been up to now, here are several other reasons.
The algorithms that ﬁnite automata can execute, although simple by deﬁ-
nition, are ideally suited for some computational problems—they might be the
algorithms of choice, even if we have computers with lots of horsepower. We will
see examples of these algorithms and the problems they can solve, and some of
them are directly useful in computer science. Context-free grammars and push-
down automata are used in software form in compiler design and other eminently
practical areas.
A model of computation that is inherently simple, such as a ﬁnite automaton, is
one we can understand thoroughly and describe precisely, using appropriate math-
ematical notation. Having a ﬁrm grasp of the principles governing these devices
makes it easier to understand the notation, which we can then apply to more
complicated models of computation.
A Turing machine is simpler than any actual computer, because it is abstract.
We can study it, and follow its computation, without becoming bogged down by
hardware details or memory restrictions. A Turing machine is an implementation
of an algorithm. Studying one in detail is equivalent to studying an algorithm, and
studying them in general is a way of studying the algorithmic method. Having a
precise model makes it possible to identify certain types of computations that Turing

xii
Introduction
machines cannot carry out. We said earlier that Turing machines accept recursively
enumerable languages. These are not all languages, and Turing machines can’t
solve every problem. When we ﬁnd a problem a ﬁnite automaton can’t solve, we
can look for a more powerful type of computer, but when we ﬁnd a problem
that can’t be solved by a Turing machine (and we will discuss several examples
of such “undecidable” problems), we have found a limitation of the algorithmic
method.

1
C
H
A
P
T
E
R
1
Mathematical Tools
and Techniques
W
hen we discuss formal languages and models of computation, the deﬁnitions
will rely mostly on familiar mathematical objects (logical propositions and
operators, sets, functions, and equivalence relations) and the discussion will use
common mathematical techniques (elementary methods of proof, recursive deﬁ-
nitions, and two or three versions of mathematical induction). This chapter lays
out the tools we will be using, introduces notation and terminology, and presents
examples that suggest directions we will follow later.
The topics in this chapter are all included in a typical beginning course in
discrete mathematics, but you may be more familiar with some than with others.
Even if you have had a discrete math course, you will probably ﬁnd it helpful to
review the ﬁrst three sections. You may want to pay a little closer attention to the
last three, in which many of the approaches that characterize the subjects in this
course ﬁrst start to show up.
1.1 LOGIC AND PROOFS
In this ﬁrst section, we consider some of the ingredients used to construct logical
arguments. Logic involves propositions, which have truth values, either the value
true or the value false. The propositions “0 = 1” and “peanut butter is a source of
protein” have truth values false and true, respectively. When a simple proposition,
which has no variables and is not constructed from other simpler propositions, is
used in a logical argument, its truth value is the only information that is relevant.
A proposition involving a variable (a free variable, terminology we will explain
shortly) may be true or false, depending on the value of the variable. If the domain,
or set of possible values, is taken to be N, the set of nonnegative integers, the
proposition “x −1 is prime” is true for the value x = 8 and false when x = 10.

2
C H A P T E R 1
Mathematical Tools and Techniques
Compound propositions are constructed from simpler ones using logical con-
nectives. We will use ﬁve connectives, which are shown in the table below. In each
case, p and q are assumed to be propositions.
Connective
Symbol
Typical Use
English Translation
conjunction
∧
p ∧q
p and q
disjunction
∨
p ∨q
p or q
negation
¬
¬p
not p
conditional
→
p →q
if p then q
p only if q
biconditional
↔
p ↔q
p if and only if q
Each of these connectives is deﬁned by saying, for each possible combination
of truth values of the propositions to which it is applied, what the truth value of
the result is. The truth value of ¬p is the opposite of the truth value of p. For
the other four, the easiest way to present this information is to draw a truth table
showing the four possible combinations of truth values for p and q.
p
q
p∧q
p∨q
p→q
p↔q
T
T
T
T
T
T
T
F
F
T
F
F
F
T
F
T
T
F
F
F
F
F
T
T
Many of these entries don’t require much discussion. The proposition p ∧q
(“p and q”) is true when both p and q are true and false in every other case. “p
or q” is true if either or both of the two propositions p and q are true, and false
only when they are both false.
The conditional proposition p →q, “if p then q”, is deﬁned to be false when
p is true and q is false; one way to understand why it is deﬁned to be true in the
other cases is to consider a proposition like
x < 1 →x < 2
where the domain associated with the variable x is the set of natural numbers. It
sounds reasonable to say that this proposition ought to be true, no matter what
value is substituted for x, and you can see that there is no value of x that makes
x < 1 true and x < 2 false. When x = 0, both x < 1 and x < 2 are true; when
x = 1, x < 1 is false and x < 2 is true; and when x = 2, both x < 1 and x < 2
are false; therefore, the truth table we have drawn is the only possible one if we
want this compound proposition to be true in every case.
In English, the word order in a conditional statement can be changed without
changing the meaning. The proposition p →q can be read either “if p then q”
or “q if p”. In both cases, the “if” comes right before p. The other way to read
p →q, “p only if q”, may seem confusing until you realize that “only if” and
“if” mean different things. The English translation of the biconditional statement

1.1
Logic and Proofs
3
p ↔q is a combination of “p if q” and “p only if q”. The statement is true when
the truth values of p and q are the same and false when they are different.
Once we have the truth tables for the ﬁve connectives, ﬁnding the truth values
for an arbitrary compound proposition constructed using the ﬁve is a straightforward
operation. We illustrate the process for the proposition
(p ∨q) ∧¬(p →q)
We begin ﬁlling in the table below by entering the values for p and q in the two
leftmost columns; if we wished, we could copy one of these columns for each
occurrence of p or q in the expression. The order in which the remaining columns
are ﬁlled in (shown at the top of the table) corresponds to the order in which the
operations are carried out, which is determined to some extent by the way the
expression is parenthesized.
1
4
3
2
p
q (p ∨q) ∧
¬
(p →q)
T
T
T
F
F
T
T
F
T
T
T
F
F
T
T
F
F
T
F
F
F
F
F
T
The ﬁrst two columns to be computed are those corresponding to the subex-
pressions p ∨q and p →q. Column 3 is obtained by negating column 2, and the
ﬁnal result in column 4 is obtained by combining columns 1 and 3 using the ∧
operation.
A tautology is a compound proposition that is true for every possible combi-
nation of truth values of its constituent propositions—in other words, true in every
case. A contradiction is the opposite, a proposition that is false in every case. The
proposition p ∨¬p is a tautology, and p ∧¬p is a contradiction. The propositions
p and ¬p by themselves, of course, are neither.
According to the deﬁnition of the biconditional connective, p ↔q is true pre-
cisely when p and q have the same truth values. One type of tautology, therefore,
is a proposition of the form P ↔Q, where P and Q are compound propositions
that are logically equivalent—i.e., have the same truth value in every possible
case. Every proposition appearing in a formula can be replaced by any other logi-
cally equivalent proposition, because the truth value of the entire formula remains
unchanged. We write P ⇔Q to mean that the compound propositions P and Q
are logically equivalent. A related idea is logical implication. We write P ⇒Q
to mean that in every case where P is true, Q is also true, and we describe this
situation by saying that P logically implies Q.
The proposition P →Q and the assertion P ⇒Q look similar but are different
kinds of things. P →Q is a proposition, just like P and Q, and has a truth value
in each case. P ⇒Q is a “meta-statement”, an assertion about the relationship
between the two propositions P and Q. Because of the way we have deﬁned
the conditional, the similarity between them can be accounted for by observing

4
C H A P T E R 1
Mathematical Tools and Techniques
that P ⇒Q means P →Q is a tautology. In the same way, as we have already
observed, P ⇔Q means that P ↔Q is a tautology.
There is a long list of logical identities that can be used to simplify compound
propositions. We list just a few that are particularly useful; each can be veriﬁed by
observing that the truth tables for the two equivalent statements are the same.
The commutative laws:
p ∨q ⇔q ∨p
p ∧q ⇔q ∧p
The associative laws:
p ∨(q ∨r) ⇔(p ∨q) ∨r
p ∧(q ∧r) ⇔(p ∧q) ∧r
The distributive laws:
p ∨(q ∧r) ⇔(p ∨q) ∧(p ∨r)
p ∧(q ∨r) ⇔(p ∧q) ∨(p ∧r)
The De Morgan laws:
¬(p ∨q) ⇔¬p ∧¬q
¬(p ∧q) ⇔¬p ∨¬q
Here are three more involving the conditional and biconditional.
(p →q) ⇔(¬p ∨q)
(p →q) ⇔(¬q →¬p)
(p ↔q) ⇔((p →q) ∧(q →p))
The ﬁrst and third provide ways of expressing →and ↔in terms of the
three simpler connectives ∨, ∧, and ¬. The second asserts that the conditional
proposition p →q is equivalent to its contrapositive. The converse of p →q is
q →p, and these two propositions are not equivalent, as we suggested earlier in
discussing if and only if.
We interpret a proposition such as “x −1 is prime”, which we considered
earlier, as a statement about x, which may be true or false depending on the value
of x. There are two ways of attaching a logical quantiﬁer to the beginning of
the proposition; we can use the universal quantiﬁer “for every”, or the existential
quantiﬁer “for some”. We will write the resulting quantiﬁed statements as
∀x(x −1 is prime)
∃x(x −1 is prime)
In both cases, what we have is no longer a statement about x, which still appears
but could be given another name without changing the meaning, and it no longer
makes sense to substitute an arbitrary value for x. We say that x is no longer a
free variable, but is bound to the quantiﬁer. In effect, the statement has become
a statement about the domain from which possible values may be chosen for x.
If as before we take the domain to be the set N of nonnegative integers, the ﬁrst
statement is false, because “x −1 is prime” is not true for every x in the domain
(it is false when x = 10). The second statement, which is often read “there exists
x such that x −1 is prime”, is true; for example, 8 −1 is prime.
An easy way to remember the notation for the two quantiﬁers is to think
of ∀as an upside-down A, for “all”, and to think of ∃as a backward E, for
“exists”. Notation for quantiﬁed statements sometimes varies; we use parentheses

1.1
Logic and Proofs
5
in order to specify clearly the scope of the quantiﬁer, which in our example is
the statement “x −1 is prime”. If the quantiﬁed statement appears within a larger
formula, then an appearance of x outside the scope of this quantiﬁer means
something different.
We assume, unless explicitly stated otherwise, that in statements containing
two or more quantiﬁers, the same domain is associated with all of them. Being
able to understand statements of this sort requires paying particular attention to the
scope of each quantiﬁer. For example, the two statements
∀x(∃y((x < y))
∃y(∀x((x < y))
are superﬁcially similar (the same variables are bound to the same quantiﬁers, and
the inequalities are the same), but the statements do not express the same idea. The
ﬁrst says that for every x, there is a y that is larger. This is true if the domain in
both cases is N, for example. The second, on the other hand, says that there is a
single y such that no matter what x is, x is smaller than y. This statement is false,
for the domain N and every other domain of numbers, because if it were true, one
of the values of x that would have to be smaller than y is y itself. The best way to
explain the difference is to observe that in the ﬁrst case the statement ∃y(x < y) is
within the scope of ∀x, so that the correct interpretation is “there exists y, which
may depend on x”.
Manipulating quantiﬁed statements often requires negating them. If it is not
the case that for every x, P (x), then there must be some value of x for which P (x)
is not true. Similarly, if there does not exist an x such that P (x), then P (x) must
fail for every x. The general procedure for negating a quantifed statement is to
reverse the quantiﬁer (change ∀to ∃, and vice versa) and move the negation inside
the quantiﬁer. ¬(∀x(P (x))) is the same as ∃x(¬P (x)), and ¬(∃x(P (x))) is the
same as ∀x(¬P (x)). In order to negate a statement with several nested quantiﬁers,
such as
∀x(∃y(∀z(P (x, y, z))))
apply the general rule three times, moving from the outside in, so that the ﬁnal
result is
∃x(∀y(∃z(¬P (x, y, z))))
We have used “∃x(x −1 is prime)” as an example of a quantiﬁed statement.
To conclude our discussion of quantiﬁers, we consider how to express the statement
“x is prime” itself using quantiﬁers, where again the domain is the set N. A prime
is an integer greater than 1 whose only divisors are 1 and itself; the statement “x
is prime” can be formulated as “x > 1, and for every k, if k is a divisor of x, then
either k is 1 or k is x”. Finally, the statement “k is a divisor of x” means that there
is an integer m with x = m ∗k. Therefore, the statement we are looking for can
be written
(x > 1) ∧∀k((∃m(x = m ∗k)) →(k = 1 ∨k = x))

6
C H A P T E R 1
Mathematical Tools and Techniques
A typical step in a proof is to derive a statement from initial assumptions
and hypotheses, or from statements that have been derived previously, or from
other generally accepted facts, using principles of logical reasoning. The more
formal the proof, the stricter the criteria regarding what facts are “generally
accepted”, what principles of reasoning are allowed, and how carefully they are
elaborated.
You will not learn how to write proofs just by reading this section, because
it takes a lot of practice and experience, but we will illustrate a few basic proof
techniques in the simple proofs that follow.
We will usually be trying to prove a statement, perhaps with a quantiﬁer,
involving a conditional proposition p →q. The ﬁrst example is a direct proof, in
which we assume that p is true and derive q. We begin with the deﬁnitions of odd
integers, which appear in this example, and even integers, which will appear in
Example 1.3.
An integer n is odd if there exists an integer k so that n = 2k + 1.
An integer n is even if there exists an integer k so that n = 2k.
In Example 1.3, we will need the fact that every integer is either even or odd and
no integer can be both (see Exercise 1.51).
EXAMPLE 1.1
The Product of Two Odd Integers Is Odd
To Prove: For every two integers a and b, if a and b are odd, then ab is odd.
■Proof
The conditional statement can be restated as follows: If there exist integers i and j so
that a = 2i + 1 and b = 2j + 1, then there exists an integer k so that ab = 2k + 1. Our
proof will be constructive—not only will we show that there exists such an integer k,
but we will demonstrate how to construct it. Assuming that a = 2i + 1 and b = 2j + 1,
we have
ab = (2i + 1)(2j + 1)
= 4ij + 2i + 2j + 1
= 2(2ij + i + j) + 1
Therefore, if we let k = 2ij + i + j, we have the result we want, ab = 2k + 1.
An important point about this proof, or any proof of a statement that begins
“for every”, is that a “proof by example” is not sufﬁcient. An example can
constitute a proof of a statement that begins “there exists”, and an example can
disprove a statement beginning “for every”, by serving as a counterexample, but
the proof above makes no assumptions about a and b except that each is an odd
integer.
Next we present examples illustrating two types of indirect proofs, proof by
contrapositive and proof by contradiction.

1.1
Logic and Proofs
7
EXAMPLE 1.2
Proof by Contrapositive
To Prove: For every three positive integers i, j, and n, if ij = n, then i ≤√n or j ≤√n.
■Proof
The conditional statement p →q inside the quantiﬁer is logically equivalent to its contra-
positive, and so we start by assuming that there exist values of i, j, and n such that
not (i ≤√n or j ≤√n)
According to the De Morgan law, this implies
not (i ≤√n) and not (j ≤√n)
which in turn implies i > √n and j > √n. Therefore,
ij > √n√n = n
which implies that ij ̸= n. We have constructed a direct proof of the contrapositive statement,
which means that we have effectively proved the original statement.
For every proposition p, p is equivalent to the conditional proposition true
→p, whose contrapositive is ¬p →false. A proof of p by contradiction means
assuming that p is false and deriving a contradiction (i.e., deriving the statement
false). The example we use to illustrate proof by contradiction is more than two
thousand years old and was known to members of the Pythagorean school in Greece.
It involves positive rational numbers: numbers of the form m/n, where m and n
are positive integers.
EXAMPLE 1.3
Proof by Contradiction: The Square Root of 2 Is Irrational
To Prove: There are no positive integers m and n satisfying m/n =
√
2.
■Proof
Suppose for the sake of contradiction that there are positive integers m and n with m/n
=
√
2. Then by dividing both m and n by all the factors common to both, we obtain
p/q =
√
2, for some positive integers p and q with no common factors. If p/q =
√
2,
then p = q
√
2, and therefore p2 = 2q2. According to Example 1.1, since p2 is even, p
must be even; therefore, p = 2r for some positive integer r, and p2 = 4r2. This implies
that 2r2 = q2, and the same argument we have just used for p also implies that q is even.
Therefore, 2 is a common factor of p and q, and we have a contradiction of our previous
statement that p and q have no common factors.
It is often necessary to use more than one proof technique within a single
proof. Although the proof in the next example is not a proof by contradiction, that
technique is used twice within it. The statement to be proved involves the factorial

8
C H A P T E R 1
Mathematical Tools and Techniques
of a positive integer n, which is denoted by n! and is the product of all the positive
integers less than or equal to n.
EXAMPLE 1.4
There Must Be a Prime Between n and n!
To Prove: For every integer n > 2, there is a prime p satisfying n < p < n!.
■Proof
Because n > 2, the distinct integers n and 2 are two of the factors of n!. Therefore,
n! −1 ≥2n −1 = n + n −1 > n + 1 −1 = n
The number n! −1 has a prime factor p, which must satisfy p ≤n! −1 < n!. Therefore,
p < n!, which is one of the inequalities we need. To show the other one, suppose for the sake
of contradiction that p ≤n. Then by the deﬁnition of factorial, p must be one of the factors
of n!. However, p cannot be a factor of both n! and n! −1; if it were, it would be a factor of
1, their difference, and this is impossible because a prime must be bigger than 1. Therefore,
the assumption that p ≤n leads to a contradiction, and we may conclude that n < p < n!.
EXAMPLE 1.5
Proof by Cases
The last proof technique we will mention in this section is proof by cases. If P is a propo-
sition we want to prove, and P1 and P2 are propositions, at least one of which must be true,
then we can prove P by proving that P1 implies P and P2 implies P. This is sufﬁcient
because of the logical identities
(P1 →P) ∧(P2 →P) ⇔(P1 ∨P2) →P
⇔true →P
⇔P
which can be veriﬁed easily (saying that P1 or P2 must be true is the same as saying that
P1 ∨P2 is equivalent to true).
The principle is the same if there are more than two cases. If we want to show the ﬁrst
distributive law
p ∨(q ∧r) ⇔(p ∨q) ∧(p ∨r)
for example, then we must show that the truth values of the propositions on the left and
right are the same, and there are eight cases, corresponding to the eight combinations of
truth values for p, q, and r. An appropriate choice for P1 is “p, q, and r are all true”.
1.2 SETS
A ﬁnite set can be described, at least in principle, by listing its elements. The
formula
A = {1, 2, 4, 8}
says that A is the set whose elements are 1, 2, 4, and 8.

1.2
Sets
9
For inﬁnite sets, and even for ﬁnite sets if they have more than just a few
elements, ellipses (. . . ) are sometimes used to describe how the elements might be
listed:
B = {0, 3, 6, 9, . . . }
C = {13, 14, 15, . . . , 71}
A more reliable and often more informative way to describe sets like these is to
give the property that characterizes their elements. The sets B and C could be
described this way:
B = {x | x is a nonnegative integer multiple of 3}
C = {x | x is an integer and 13 ≤x ≤71}
We would read the ﬁrst formula “B is the set of all x such that x is a nonnegative
integer multiple of 3”. The expression before the vertical bar represents an arbitrary
element of the set, and the statement after the vertical bar contains the conditions,
or restrictions, that the expression must satisfy in order for it to represent a legal
element of the set.
In these two examples, the “expression” is simply a variable, which we have
arbitrarily named x. We often choose to include a little more information in the
expression; for example,
B = {3y | y is a nonnegative integer}
which we might read “B is the set of elements of the form 3y, where y is a
nonnegative integer”. Two more examples of this approach are
D = {{x} | x is an integer such that x ≥4}
E = {3i + 5j | i and j are nonnegative integers}
Here D is a set of sets; three of its elements are {4}, {5}, and {6}. We could describe
E using the formula
E = {0, 3, 5, 6, 8, 9, 10, . . .}
but the ﬁrst description of E is more informative, even if the other seems at ﬁrst
to be more straightforward.
For any set A, the statement that x is an element of A is written x ∈A, and
x /∈A means x is not an element of A. We write A ⊆B to mean A is a subset of
B, or that every element of A is an element of B; A ̸⊆B means that A is not a
subset of B (there is at least one element of A that is not an element of B). Finally,
the empty set, the set with no elements, is denoted by ∅.
A set is determined by its elements. For example, the sets {0, 1} and {1, 0}
are the same, because both contain the elements 0 and 1 and no others; the set
{0, 0, 1, 1, 1, 2} is the same as {0, 1, 2}, because they both contain 0, 1, and 2
and no other elements (no matter how many times each element is written, it’s the
same element); and there is only one empty set, because once you’ve said that a set

10
C H A P T E R 1
Mathematical Tools and Techniques
contains no elements, you’ve described it completely. To show that two sets A and
B are the same, we must show that A and B have exactly the same elements—i.e.,
that A ⊆B and B ⊆A.
A few sets will come up frequently. We have used N in Section 1.1 to denote
the set of natural numbers, or nonnegative integers; Z is the set of all integers, R
the set of all real numbers, and R+ the set of nonnegative real numbers. The sets
B and E above can be written more concisely as
B = {3y | y ∈N}
E = {3i + 5j | i, j ∈N}
We sometimes relax the { expression | conditions } format slightly when we
are describing a subset of another set, as in
C = {x ∈N | 13 ≤x ≤71}
which we would read “C is the set of all x in N such that . . . ”
For two sets A and B, we can deﬁne their union A ∪B, their intersection
A ∩B, and their difference A −B, as follows:
A ∪B = {x | x ∈A or x ∈B}
A ∩B = {x | x ∈A and x ∈B}
A −B = {x | x ∈A and x /∈B}
For example,
{1, 2, 3, 5} ∪{2, 4, 6} = {1, 2, 3, 4, 5, 6}
{1, 2, 3, 5} ∩{2, 4, 6} = {2}
{1, 2, 3, 5} −{2, 4, 6} = {1, 3, 5}
If we assume that A and B are both subsets of some “universal” set U, then we
can consider the special case U −A, which is written A′ and referred to as the
complement of A.
A′ = U −A = {x ∈U | x /∈A}
We think of A′ as “the set of everything that’s not in A”, but to be meaning-
ful this requires context. The complement of {1, 2} varies considerably, depending
on whether the universal set is chosen to be N, Z, R, or some other
set.
If the intersection of two sets is the empty set, which means that the
two sets have no elements in common, they are called disjoint sets. The sets
in a collection of sets are pairwise disjoint if, for every two distinct ones A
and B (“distinct” means not identical), A and B are disjoint. A partition of
a set S is a collection of pairwise disjoint subsets of S whose union is S;
we can think of a partition of S as a way of dividing S into non-overlapping
subsets.
There are a number of useful “set identities”, but they are closely analogous
to the logical identities we discussed in Section 1.1, and as the following example
demonstrates, they can be derived the same way.

1.2
Sets
11
EXAMPLE 1.6
The First De Morgan Law
There are two De Morgan laws for sets, just as there are for propositions; the ﬁrst asserts
that for every two sets A and B,
(A ∪B)′ = A′ ∩B′
We begin by noticing the resemblance between this formula and the logical identity
¬(p ∨q) ⇔¬p ∧¬q
The resemblance is not just superﬁcial. We deﬁned the logical connectives such as ∧and
∨by drawing truth tables, and we could deﬁne the set operations ∩and ∪by drawing
membership tables, where T denotes membership and F nonmembership:
A
B
A ∩B
A ∪B
T
T
T
T
T
F
F
T
F
T
F
T
F
F
F
F
As you can see, the truth values in the two tables are identical to the truth values in the
tables for ∧and ∨. We can therefore test a proposed set identity the same way we can test
a proposed logical identity, by constructing tables for the two expressions being compared.
When we do this for the expressions (A ∪B)′ and A′ ∩B′, or for the propositions ¬(p ∨q)
and ¬p ∧¬q, by considering the four cases, we obtain identical values in each case. We
may conclude that no matter what case x represents, x ∈(A ∪B)′ if and only if x ∈A′ ∩B′,
and the two sets are equal.
The associative law for unions, corresponding to the one for ∨, says that for
arbitrary sets A, B, and C,
A ∪(B ∪C) = (A ∪B) ∪C
so that we can write A ∪B ∪C without worrying about how to group the terms.
It is easy to see from the deﬁnition of union that
A ∪B ∪C = {x | x is an element of at least one of the sets A, B, and C}
For the same reasons, we can consider unions of any number of sets and adopt
notation to describe such unions. For example, if A0, A1, A2, . . . are sets,

{Ai | 0 ≤i ≤n} = {x | x ∈Ai for at least one i with 0 ≤i ≤n}

{Ai | i ≥0} = {x | x ∈Ai for at least one i with i ≥0}
In Chapter 3 we will encounter the set

{δ(p, σ) | p ∈δ∗(q, x)}

12
C H A P T E R 1
Mathematical Tools and Techniques
In all three of these formulas, we have a set S of sets, and we are describing the
union of all the sets in S. We do not need to know what the sets δ∗(q, x) and
δ(p, σ) are to understand that

{δ(p, σ) | p ∈δ∗(q, x)} = {x | x ∈δ(p, σ)
for at least one element p of δ∗(q, x)}
If δ∗(q, x) were {r, s, t}, for example, we would have

{δ(p, σ) | p ∈δ∗(q, x)} = δ(r, σ) ∪δ(s, σ) ∪δ(t, σ)
Sometimes the notation varies slightly. The two sets

{Ai | i ≥0}
and

{δ(p, σ) | p ∈δ∗(q, x)}
for example, might be written
∞

i=0
Ai
and

p∈δ∗(q,x)
δ(p, σ)
respectively.
Because there is also an associative law for intersections, exactly the same
notation can be used with ∩instead of ∪.
For a set A, the set of all subsets of A is called the power set of A and written
2A. The reason for the terminology and the notation is that if A is a ﬁnite set with
n elements, then 2A has exactly 2n elements (see Example 1.23). For example,
2{a,b,c} = {∅, {a}, {b}, {c}, {a, b}, {a, c}, {b, c}, {a, b, c}}
This example illustrates the fact that the empty set is a subset of every set, and
every set is a subset of itself.
One more set that can be constructed from two sets A and B is A × B, their
Cartesian product:
A × B = {(a, b) | a ∈A and b ∈B}
For example,
{0, 1} × {1, 2, 3} = {(0, 1), (0, 2), (0, 3), (1, 1), (1, 2), (1, 3)}
The elements of A × B are called ordered pairs, because (a, b) = (c, d) if and
only if a = c and b = d; in particular, (a, b) and (b, a) are different unless a and
b happen to be equal. More generally, A1 × A2 × · · · × Ak is the set of all “ordered
k-tuples” (a1, a2, . . . , ak), where ai is an element of Ai for each i.
1.3 FUNCTIONS AND EQUIVALENCE
RELATIONS
If A and B are two sets (possibly equal), a function f from A to B is a rule that
assigns to each element x of A an element f (x) of B. (Later in this section we
will mention a more precise deﬁnition, but for our purposes the informal “rule”

1.3
Functions and Equivalence Relations
13
deﬁnition will be sufﬁcient.) We write f : A →B to mean that f is a function
from A to B.
Here are four examples:
1.
The function f : N →R deﬁned by the formula f (x) = √x. (In other
words, for every x ∈N, f (x) = √x.)
2.
The function g : 2N →2N deﬁned by the formula g(A) = A ∪{0}.
3.
The function u : 2N × 2N →2N deﬁned by the formula u(S, T ) = S ∪T .
4.
The function i : N →Z deﬁned by
i(n) =
 n/2
if n is even
(−n −1)/2
if n is odd
For a function f from A to B, we call A the domain of f and B the codomain
of f . The domain of a function f is the set of values x for which f (x) is deﬁned.
We will say that two functions f and g are the same if and only if they have the
same domain, they have the same codomain, and f (x) = g(x) for every x in the
domain.
In some later chapters it will be convenient to refer to a partial function f
from A to B, one whose domain is a subset of A, so that f may be undeﬁned
at some elements of A. We will still write f : A →B, but we will be careful to
distinguish the set A from the domain of f , which may be a smaller set. When
we speak of a function from A to B, without any qualiﬁcation, we mean one with
domain A, and we might emphasize this by calling it a total function.
If f is a function from A to B, a third set involved in the description of f is
its range, which is the set
{f (x) | x ∈A}
(a subset of the codomain B). The range of f is the set of elements of the codomain
that are actually assigned by f to elements of the domain.
Deﬁnition 1.7
One-to-One and Onto Functions
A function f : A →B is one-to-one if f never assigns the same value
to two different elements of its domain. It is onto if its range is the entire
set B. A function from A to B that is both one-to-one and onto is called
a bijection from A to B.
Another way to say that a function f : A →B is one-to-one is to say that for
every y ∈B, y = f (x) for at most one x ∈A, and another way to say that f is onto
is to say that for every y ∈B, y = f (x) for at least one x ∈A. Therefore, saying
that f is a bijection from A to B means that every element y of the codomain B
is f (x) for exactly one x ∈A. This allows us to deﬁne another function f −1 from
B to A, by saying that for every y ∈B, f −1(y) is the element x ∈A for which

14
C H A P T E R 1
Mathematical Tools and Techniques
f (x) = y. It is easy to check that this “inverse function” is also a bijection and
satisﬁes these two properties: For every x ∈A, and every y ∈B,
f −1(f (x)) = x
f (f −1(y)) = y
Of the four functions deﬁned above, the function f from N to R is one-to-one
but not onto, because a real number is the square root of at most one natural number
and might not be the square root of any. The function g is not one-to-one, because
for every subset A of N that doesn’t contain 0, A and A ∪{0} are distinct and
g(A) = g(A ∪{0}). It is also not onto, because every element of the range of g is
a set containing 0 and not every subset of N does. The function u is onto, because
u(A, A) = A for every A ∈2N , but not one-to-one, because for every A ∈2N ,
u(A, ∅) is also A.
The formula for i seems more complicated, but looking at this partial tabulation
of its values
x
0
1
2
3
4
5
6
. . .
i(x)
0
−1
1
−2
2
−3
3
. . .
makes it easy to see that i is both one-to-one and onto. No integer appears more
than once in the list of values of i, and every integer appears once.
In the ﬁrst part of this book, we will usually not be concerned with whether
the functions we discuss are one-to-one or onto. The idea of a bijection between
two sets, such as our function i, will be important in Chapter 8, when we discuss
inﬁnite sets with different sizes.
An operation on a set A is a function that assigns to elements of A, or perhaps
to combinations of elements of A, other elements of A. We will be interested
particularly in binary operations (functions from A × A to A) and unary operations
(functions from A to A). The function u described above is an example of a binary
operation on the set 2N , and for every set S, both union and intersection are
binary operations on 2S. Familar binary operations on N, or on Z, include addition
and multiplication, and subtraction is a binary operation on Z. The complement
operation is a unary operation on 2S, for every set S, and negation is a unary
operation on the set Z. The notation adopted for some of these operations is
different from the usual functional notation; we write U ∪V rather than ∪(U, V ),
and a −b rather than −(a, b).
For a unary operation or a binary operation on a set A, we say that a subset
A1 of A is closed under the operation if the result of applying the operation to
elements of A1 is an element of A1. For example, if A = 2N , and A1 is the set
of all nonempty subsets of N, then A1 is closed under union (the union of two
nonempty subsets of N is a nonempty subset of N) but not under intersection. The
set of all subsets of N with fewer than 100 elements is closed under intersection
but not under union. If A = N, and A1 is the set of even natural numbers, then A1
is closed under both addition and multiplication; the set of odd natural numbers is
closed under multiplication but not under addition. We will return to this idea later
in this chapter, when we discuss recursive deﬁnitions of sets.

1.3
Functions and Equivalence Relations
15
We can think of a function f from a set A to a set B as establishing a
relationship between elements of A and elements of B; every element x ∈A is
“related” to exactly one element y ∈B, namely, y = f (x). A relation R from A
to B may be more general, in that an element x ∈A may be related to no elements
of B, to one element, or to more than one. We will use the notation aRb to mean
that a is related to b with respect to the relation R. For example, if A is the set of
people and B is the set of cities, we might consider the “has-lived-in” relation R
from A to B: If x ∈A and y ∈B, xRy means that x has lived in y. Some people
have never lived in a city, some have lived in one city all their lives, and some
have lived in several cities.
We’ve said that a function is a “rule”; exactly what is a relation?
Deﬁnition 1.8
A Relation from A to B, and a Relation on A
For two sets A and B, a relation from A to B is a subset of A × B. A
relation on the set A is a relation from A to A, or a subset of A × A.
The statement “a is related to b with respect to R” can be expressed by
either of the formulas aRb and (a, b) ∈R. As we have already pointed out, a
function f from A to B is simply a relation having the property that for every
x ∈A, there is exactly one y ∈B with (x, y) ∈f . Of course, in this special case,
a third way to write “x is related to y with respect to f ” is the most common:
y = f (x).
In the has-lived-in example above, the statement “Sally has lived in Atlanta”
seems easier to understand than the statement “(Sally, Atlanta)∈R”, but this is just
a question of notation. If we understand what R is, the two statements say the same
thing. In this book, we will be interested primarily in relations on a set, especially
ones that satisfy the three properties in the next deﬁnition.
Deﬁnition 1.9
Equivalence Relations
A relation R on a set A is an equivalence relation if it satisﬁes these
three properties.
1. R is reﬂexive: for every x ∈A, xRx.
2. R is symmetric: for every x and every y in A, if xRy, then yRx.
3. R is transitive: for every x, every y, and every z in A, if xRy and yRz,
then xRz.
If R is an equivalence relation on A, we often say “x is equivalent to y”
instead of “x is related to y”. Examples of relations that do not satisfy all three
properties can be found in the exercises. Here we present three simple examples
of equivalence relations.

16
C H A P T E R 1
Mathematical Tools and Techniques
EXAMPLE 1.10
The Equality Relation
We can consider the relation of equality on every set A, and the formula x = y expresses
the fact that (x, y) is an element of the relation. The properties of reﬂexivity, symmetry,
and transitivity are familiar properties of equality: Every element of A is equal to itself; for
every x and y in A, if x = y, then y = x; and for every x, y, and z, if x = y and y = z,
then x = z. This relation is the prototypical equivalence relation, and the three properties
are no more than what we would expect of any relation we described as one of equivalence.
EXAMPLE 1.11
The Relation on A Containing All Ordered Pairs
On every set A, we can also consider the relation R = A × A. Every possible ordered pair
of elements of A is in the relation—every element of A is related to every other element,
including itself. This relation is also clearly an equivalence relation; no statement of the
form “(under certain conditions) xRy” can possibly fail if xRy for every x and every y.
EXAMPLE 1.12
The Relation of Congruence Mod n on N
We consider the set N of natural numbers, and, for some positive integer n, the relation R
on N deﬁned as follows: for every x and y in N,
xRy if there is an integer k so that x −y = kn
In this case we write x ≡n y to mean xRy. Checking that the three properties are satisﬁed
requires a little more work this time, but not much. The relation is reﬂexive, because for every
x ∈N, x −x = 0 ∗n. It is symmetric, because for every x and every y in N, if x −y = kn,
then y −x = (−k)n. Finally, it is transitive, because if x −y = kn and y −z = jn, then
x −z = (x −y) + (y −z) = kn + jn = (k + j)n
One way to understand an equivalence relation R on a set A is to consider,
for each x ∈A, the subset [x]R of A containing all the elements equivalent to x.
Because an equivalence relation is reﬂexive, one of these elements is x itself, and
we can refer to the set [x]R as the equivalence class containing x.
Deﬁnition 1.13
The Equivalence Class Containing x
For an equivalence relation R on a set A, and an element x ∈A, the
equivalence class containing x is
[x]R = {y ∈A | yRx}
If there is no doubt about which equivalence relation we are using, we will
drop the subscript and write [x].

1.4
Languages
17
The phrase “the equivalence class containing x” is not misleading: For every
x ∈A, we have already seen that x ∈[x], and we can also check that x belongs
to only one equivalence class. Suppose that x, y ∈A and x ∈[y], so that xRy; we
show that [x] = [y]. Let z be an arbitrary element of [x], so that zRx. Because
zRx, xRy, and R is transitive, it follows that zRy; therefore, [x] ⊆[y]. For the
other inclusion we observe that if x ∈[y], then y ∈[x] because R is symmetric,
and the same argument with x and y switched shows that [y] ⊆[x].
These conclusions are summarized by Theorem 1.14.
Theorem 1.14
If R is an equivalence relation on a set A, the equivalence classes with
respect to R form a partition of A, and two elements of A are equivalent
if and only if they are elements of the same equivalence class.
Example 1.10 illustrates the extreme case in which every equivalence class
contains just one element, and Example 1.11 illustrates the other extreme, in which
the single equivalence class A contains all the elements. In the case of congruence
mod n for a number n > 1, some but not all of the elements of N other than x are
in [x]; the set [x] contains all natural numbers that differ from x by a multiple of n.
For an arbitrary equivalence relation R on a set A, knowing the partition
determined by R is enough to describe the relation completely. In fact, if we begin
with a partition of A, then the relation R on A that is deﬁned by the last statement
of Theorem 1.1 (two elements x and y are related if and only if x and y are
in the same subset of the partition) is an equivalence relation whose equivalence
classes are precisely the subsets of the partition. Specifying a subset of A × A and
specifying a partition on A are two ways of conveying the same information.
Finally, if R is an equivalence relation on A and S = [x], it follows from
Theorem 1.14 that every two elements of S are equivalent and no element of S is
equivalent to an element not in S. On the other hand, if S is a nonempty subset
of A, knowing that S satisﬁes these two properties allows us to say that S is an
equivalence class, even if we don’t start out with any particular x satisfying S = [x].
If x is an arbitrary element of S, every element of S belongs to [x], because it
is equivalent to x; and every element of [x] belongs to S, because otherwise the
element x of S would be equivalent to some element not in S. Therefore, for every
x ∈S, S = [x].
1.4 LANGUAGES
Familar languages include programming languages such as Java and natural lan-
guages like English, as well as unofﬁcial “dialects” with specialized vocabularies,
such as the language used in legal documents or the language of mathematics. In
this book we use the word “language” more generally, taking a language to be any
set of strings over an alphabet of symbols. In applying this deﬁnition to English,

18
C H A P T E R 1
Mathematical Tools and Techniques
we might take the individual strings to be English words, but it is more common to
consider English sentences, for which many grammar rules have been developed.
In the case of a language like Java, a string must satisfy certain rules in order to be
a legal statement, and a sequence of statements must satisfy certain rules in order
to be a legal program.
Many of the languages we study initially will be much simpler. They might
involve alphabets with just one or two symbols, and perhaps just one or two basic
patterns to which all the strings must conform. The main purpose of this section
is to present some notation and terminology involving strings and languages that
will be used throughout the book.
An alphabet is a ﬁnite set of symbols, such as {a, b} or {0, 1} or {A, B, C, . . . ,
Z}. We will usually use the Greek letter  to denote the alphabet. A string over 
is a ﬁnite sequence of symbols in . For a string x, |x| stands for the length (the
number of symbols) of x. In addition, for a string x over  and an element σ ∈,
nσ(x) = the number of occurrences of the symbol σ in the string x
The null string  is a string over , no matter what the alphabet  is. By deﬁnition,
|| = 0.
The set of all strings over  will be written ∗. For the alphabet {a, b}, we
have
{a, b}∗= {, a, b, aa, ab, ba, bb, aaa, aab, . . . }
Here we have listed the strings in canonical order, the order in which shorter strings
precede longer strings and strings of the same length appear alphabetically. Canon-
ical order is different from lexicographic, or strictly alphabetical order, in which
aa precedes b. An essential difference is that canonical order can be described by
making a single list of strings that includes every element of ∗exactly once. If
we wanted to describe an algorithm that did something with each string in {a, b}∗,
it would make sense to say, “Consider the strings in canonical order, and for each
one, . . . ” (see, for example, Section 8.2). If an algorithm were to “consider the
strings of {a, b}∗in lexicographic order”, it would have to start by considering ,
a, aa, aaa, . . . , and it would never get around to considering the string b.
A language over  is a subset of ∗. Here are a few examples of languages
over {a, b}:
1. The empty language ∅.
2. {, a, aab}, another ﬁnite language.
3. The language Pal of palindromes over {a, b} (strings such as aba or baab
that are unchanged when the order of the symbols is reversed).
4. {x ∈{a, b}∗| na(x) > nb(x)}.
5. {x ∈{a, b}∗| |x| ≥2 and x begins and ends with b}.
The null string  is always an element of ∗, but other languages over  may or
may not contain it; of these ﬁve examples, only the second and third do.
Here are a few real-world languages, in some cases involving larger alphabets.

1.4
Languages
19
6. The language of legal Java identiﬁers.
7. The language Expr of legal algebraic expressions involving the identiﬁer a,
the binary operations + and ∗, and parentheses. Some of the strings in the
language are a, a + a ∗a, and (a + a ∗(a + a)).
8. The language Balanced of balanced strings of parentheses (strings containing
the occurrences of parentheses in some legal algebraic expression). Some
elements are , ()(()), and ((((())))).
9. The language of numeric “literals” in Java, such as −41, 0.03, and 5.0E−3.
10. The language of legal Java programs. Here the alphabet would include
upper- and lowercase alphabetic symbols, numerical digits, blank spaces, and
punctuation and other special symbols.
The basic operation on strings is concatenation. If x and y are two strings
over an alphabet, the concatenation of x and y is written xy and consists of the
symbols of x followed by those of y. If x = ab and y = bab, for example, then
xy = abbab and yx = babab. When we concatenate the null string  with another
string, the result is just the other string (for every string x, x = x = x); and for
every x, if one of the formulas xy = x or yx = x is true for some string y, then
y = . In general, for two strings x and y, |xy| = |x| + |y|.
Concatenation is an associative operation; that is, (xy)z = x(yz), for all pos-
sible strings x, y, and z. This allows us to write xyz without specifying how the
factors are grouped.
If s is a string and s = tuv for three strings t, u, and v, then t is a preﬁx of s,
v is a sufﬁx of s, and u is a substring of s. Because one or both of t and u might
be , preﬁxes and sufﬁxes are special cases of substrings. The string  is a preﬁx
of every string, a sufﬁx of every string, and a substring of every string, and every
string is a preﬁx, a sufﬁx, and a substring of itself.
Languages are sets, and so one way of constructing new languages from exist-
ing ones is to use set operations. For two languages L1 and L2 over the alphabet
, L1 ∪L2, L1 ∩L2, and L1 −L2 are also languages over . If L ⊆∗, then by
the complement of L we will mean ∗−L. This is potentially confusing, because
if L is a language over , then L can be interpreted as a language over any larger
alphabet, but it will usually be clear what alphabet we are referring to.
We can also use the string operation of concatenation to construct new lan-
guages. If L1 and L2 are both languages over , the concatenation of L1 and L2
is the language
L1L2 = {xy | x ∈L1 and y ∈L2}
For example, {a, aa}{, b, ab} = {a, ab, aab, aa, aaab}. Because x = x for
every string x, we have
{}L = L{} = L
for every language L.
The language L = ∗, for example, satisﬁes the formula LL = L, and so
the formula LL1 = L does not always imply that L1 = {}. However, if L1 is

20
C H A P T E R 1
Mathematical Tools and Techniques
a language such that LL1 = L for every language L, or if L1L = L for every
language L, then L1 = {}.
At this point we can adopt “exponential” notation for the concatenation of k
copies of a single symbol a, a single string x, or a single language L. If k > 0,
then ak = aa . . . a, where there are k occurrences of a, and similarly for xk and
Lk. In the special case where L is simply the alphabet  (which can be interpreted
as a set of strings of length 1), k = {x ∈∗| |x| = k}.
We also want the exponential notation to make sense if k = 0, and the correct
deﬁnition requires a little care. It is desirable to have the formulas
aiaj = ai+j
xixj = xi+j
LiLj = Li+j
where a, x, and L are an alphabet symbol, a string, and a language, respectively.
In the case i = 0, the ﬁrst two formulas require that we deﬁne a0 and x0 to be ,
and the last formula requires that L0 be {}.
Finally, for a language L over an alphabet , we use the notation L∗to
denote the language of all strings that can be obtained by concatenating zero or
more strings in L. This operation on a language L is known as the Kleene star,
or Kleene closure, after the mathematician Stephen Kleene. The notation L∗is
consistent with the earlier notation ∗, which we can describe as the set of strings
obtainable by concatenating zero or more strings of length 1 over . L∗can be
deﬁned by the formula
L∗=

{Lk | k ∈N}
Because we have deﬁned L0 to be {}, “concatenating zero strings in L” produces
the null string, and  ∈L∗, no matter what the language L is.
When we describe languages using formulas that contain the union, con-
catenation, and Kleene L∗operations, we will use precedence rules similar to
the algebraic rules you are accustomed to. The formula L1 ∪L2L∗
3, for example,
means L1 ∪(L2(L∗
3)); of the three operations, the highest-precedence operation is
∗, next-highest is concatenation, and lowest is union. The expressions (L1 ∪L2)L∗
3,
L1 ∪(L2L3)∗, and (L1 ∪L2L3)∗all refer to different languages.
Strings, by deﬁnition, are ﬁnite (have only a ﬁnite number of symbols). Almost
all interesting languages are inﬁnite sets of strings, and in order to use the languages
we must be able to provide precise ﬁnite descriptions. There are at least two general
approaches to doing this, although there is not always a clear line separating them.
If we write
L1 = {ab, bab}∗∪{b}{ba}∗{ab}∗
we have described the language L1 by providing a formula showing the possible
ways of generating an element: either concatenating an arbitrary number of strings,
each of which is either ab or bab, or concatenating a single b with an arbitrary
number of copies of ba and then an arbitrary number of copies of ab. The fourth
example in our list above is the language
L2 = {x ∈{a, b}∗| na(x) > nb(x)}

1.5
Recursive Deﬁnitions
21
which we have described by giving a property that characterizes the elements. For
every string x ∈{a, b}∗, we can test whether x is in L2 by testing whether the
condition is satisﬁed.
In this book we will study notational schemes that make it easy to describe
how languages can be generated, and we will study various types of algorithms, of
increasing complexity, for recognizing, or accepting, strings in certain languages. In
the second approach, we will often identify an algorithm with an abstract machine
that can carry it out; a precise description of the algorithm or the machine will
effectively give us a precise way of specifying the language.
1.5 RECURSIVE DEFINITIONS
As you know, recursion is a technique that is often useful in writing computer
programs. In this section we will consider recursion as a tool for deﬁning sets:
primarily, sets of numbers, sets of strings, and sets of sets (of numbers or strings).
A recursive deﬁnition of a set begins with a basis statement that speciﬁes one
or more elements in the set. The recursive part of the deﬁnition involves one or
more operations that can be applied to elements already known to be in the set, so
as to produce new elements of the set.
As a way of deﬁning a set, this approach has a number of potential advantages:
Often it allows very concise deﬁnitions; because of the algorithmic nature of a
typical recursive deﬁnition, one can often see more easily how, or why, a particular
object is an element of the set being deﬁned; and it provides a natural way of
deﬁning functions on the set, as well as a natural way of proving that some condition
or property is satisﬁed by every element of the set.
EXAMPLE 1.15
The Set of Natural Numbers
The prototypical example of recursive deﬁnition is the axiomatic deﬁnition of the set N
of natural numbers. We assume that 0 is a natural number and that we have a “successor”
operation, which, for each natural number n, gives us another one that is the successor of n
and can be written n + 1. We might write the deﬁnition this way:
1.
0 ∈N.
2.
For every n ∈N, n + 1 ∈N.
3.
Every element of N can be obtained by using statement 1 or statement 2.
In order to obtain an element of N, we use statement 1 once and statement 2 a ﬁnite number
of times (zero or more). To obtain the natural number 7, for example, we use statement 1
to obtain 0; then statement 2 with n = 0 to obtain 1; then statement 2 with n = 1 to obtain
2; . . . ; and ﬁnally, statement 2 with n = 6 to obtain 7.
We can summarize the ﬁrst two statements by saying that N contains 0 and is closed
under the successor operation (the operation of adding 1).
There are other sets of numbers that contain 0 and are closed under the successor
operation: the set of all real numbers, for example, or the set of all fractions. The third

22
C H A P T E R 1
Mathematical Tools and Techniques
statement in the deﬁnition is supposed to make it clear that the set we are deﬁning is the one
containing only the numbers obtained by using statement 1 once and statement 2 a ﬁnite
number of times. In other words, N is the smallest set of numbers that contains 0 and is
closed under the successor operation: N is a subset of every other such set.
In the remaining examples in this section we will omit the statement corresponding to
statement 3 in this example, but whenever we deﬁne a set recursively, we will assume that
a statement like this one is in effect, whether or not it is stated explicitly.
Just as a recursive procedure in a computer program must have an “escape hatch”
to avoid calling itself forever, a recursive deﬁnition like the one above must have a basis
statement that provides us with at least one element of the set. The recursive statement, that
n + 1 ∈N for every n ∈N, works in combination with the basis statement to give us all
the remaining elements of the set.
EXAMPLE 1.16
Recursive Deﬁnitions of Other Subsets of N
If we use the deﬁnition in Example 1.15, but with a different value speciﬁed in the basis
statement:
1.
15 ∈A.
2.
For every n ∈A, n + 1 ∈A.
then the set A that has been deﬁned is the set of natural numbers greater than or equal to 15.
If we leave the basis statement the way it was in Example 1.15 but change the “suc-
cessor” operation by changing n + 1 to n + 7, we get a deﬁnition of the set of all natural
numbers that are multiples of 7.
Here is a deﬁnition of a subset B of N:
1.
1 ∈B.
2.
For every n ∈B, 2 ∗n ∈B.
3.
For every n ∈B, 5 ∗n ∈B.
The set B is the smallest set of numbers that contains 1 and is closed under multiplication
by 2 and 5. Starting with the number 1, we can obtain 2, 4, 8, . . . by repeated applications
of statement 2, and we can obtain 5, 25, 125, . . . by using statement 3. By using both
statements 2 and 3, we can obtain numbers such as 2 ∗5, 4 ∗5, and 2 ∗25. It is not hard to
convince yourself that B is the set
B = {2i ∗5j | i, j ∈N}
EXAMPLE 1.17
Recursive Deﬁnitions of {a,b}∗
Although we use  = {a, b} in this example, it will be easy to see how to modify the
deﬁnition so that it uses another alphabet. Our recursive deﬁnition of N started with the
natural number 0, and the recursive statement allowed us to take an arbitrary n and obtain a
natural number 1 bigger. An analogous recursive deﬁnition of {a, b}∗begins with the string
of length 0 and says how to take an arbitrary string x and obtain strings of length |x| + 1.

1.5
Recursive Deﬁnitions
23
1.
 ∈{a, b}∗.
2.
For every x ∈{a, b}∗, both xa and xb are in {a, b}∗.
To obtain a string z of length k, we start with  and obtain longer and longer preﬁxes
of z by using the second statement k times, each time concatenating the next symbol
onto the right end of the current preﬁx. A recursive deﬁnition that used ax and bx in
statement 2 instead of xa and xb would work just as well; in that case we would pro-
duce longer and longer sufﬁxes of z by adding each symbol to the left end of the current
sufﬁx.
EXAMPLE 1.18
Recursive Deﬁnitions of Two Other Languages over {a,b}
We let AnBn be the language
AnBn = {anbn | n ∈N}
and Pal the language introduced in Section 1.4 of all palindromes over {a, b}; a palindrome
is a string that is unchanged when the order of the symbols is reversed.
The shortest string in AnBn is , and if we have an element aibi of length 2i, the
way to get one of length 2i + 2 is to add a at the beginning and b at the end. Therefore, a
recursive deﬁnition of AnBn is:
1.
 ∈AnBn.
2.
For every x ∈AnBn, axb ∈AnBn.
It is only slightly harder to ﬁnd a recursive deﬁnition of Pal. The length of a palindrome
can be even or odd. The shortest one of even length is , and the two shortest ones of odd
length are a and b. For every palindrome x, a longer one can be obtained by adding the
same symbol at both the beginning and the end of x, and every palindrome of length at
least 2 can be obtained from a shorter one this way. The recursive deﬁnition is therefore
1.
, a, and b are elements of Pal.
2.
For every x ∈Pal, axa and bxb are in Pal.
Both AnBn and Pal will come up again, in part because they illustrate in a very simple way
some of the limitations of the ﬁrst type of abstract computing device we will consider.
EXAMPLE 1.19
Algebraic Expressions and Balanced Strings of Parentheses
As in Section 1.4, we let Expr stand for the language of legal algebraic expressions, where
for simplicity we restrict ourselves to two binary operators, + and ∗, a single identiﬁer a,
and left and right parentheses. Real-life expressions can be considerably more complicated
because they can have additional operators, multisymbol identiﬁers, and numeric literals
of various types; however, two operators are enough to illustrate the basic principles, and
the other features can easily be added by substituting more general subexpressions for the
identiﬁer a.
Expressions can be illegal for “local” reasons, such as illegal symbol-pairs, or because
of global problems involving mismatched parentheses. Explicitly prohibiting all the features

24
C H A P T E R 1
Mathematical Tools and Techniques
we want to consider illegal is possible but is tedious. A recursive deﬁnition, on the other
hand, makes things simple. The simplest algebraic expression consists of a single a, and any
other one is obtained by combining two subexpressions using + or ∗or by parenthesizing
a single subexpression.
1.
a ∈Expr.
2.
For every x and every y in Expr, x + y and x ∗y are in Expr.
3.
For every x ∈Expr, (x) ∈Expr.
The expression (a + a ∗(a + a)), for example, can be obtained as follows:
a ∈Expr, by statement 1.
a + a ∈Expr, by statement 2, where x and y are both a.
(a + a) ∈Expr, by statement 3, where x = a + a.
a ∗(a + a) ∈Expr, by statement 2, where x = a and y = (a + a).
a + a ∗(a + a) ∈Expr, by statement 2, where x = a and y = a ∗(a + a).
(a + a ∗(a + a)) ∈Expr, by statement 3, where x = a + a ∗(a + a).
It might have occurred to you that there is a shorter derivation of this string. In the fourth
line, because we have already obtained both a + a and (a + a), we could have said
a + a ∗(a + a) ∈Expr, by statement 2, where x = a + a and y = (a + a).
The longer derivation takes into account the normal rules of precedence, under which a +
a ∗(a + a) is interpreted as the sum of a and a ∗(a + a), rather than as the product of a + a
and (a + a). The recursive deﬁnition addresses only the strings that are in the language, not
what they mean or how they should be interpreted. We will discuss this issue in more detail
in Chapter 4.
Now we try to ﬁnd a recursive deﬁnition for Balanced, the language of balanced
strings of parentheses. We can think of balanced strings as the strings of parentheses that
can occur within strings in the language Expr. The string a has no parentheses; and the
two ways of forming new balanced strings from existing balanced strings are to concate-
nate two of them (because two strings in Expr can be concatenated, with either + or ∗
in between), or to parenthesize one of them (because a string in Expr can be parenthe-
sized).
1.
 ∈Balanced.
2.
For every x and every y in Balanced, xy ∈Balanced.
3.
For every x ∈Balanced, (x) ∈Balanced.
In order to use the “closed-under” terminology to paraphrase the recursive deﬁnitions of
Expr and Balanced, it helps to introduce a little notation. If we deﬁne operations ◦, •,
and ⋄by saying x ◦y = x + y, x • y = x ∗y, and ⋄(x) = (x), then we can say that Expr
is the smallest language that contains the string a and is closed under the operations ◦,
•, and ⋄. (This is confusing. We normally think of + and ∗as “operations”, but addi-
tion and multiplication are operations on sets of numbers, not sets of strings. In this
discussion + and ∗are simply alphabet symbols, and it would be incorrect to say that
Expr is closed under addition and multiplication.) Along the same line, if we describe the

1.5
Recursive Deﬁnitions
25
operation of enclosing a string within parentheses as “parenthesization”, we can say that
Balanced is the smallest language that contains  and is closed under concatenation and
parenthesization.
EXAMPLE 1.20
A Recursive Deﬁnition of a Set of Languages over {a,b}∗
We denote by F the subset of 2{a,b}∗(the set of languages over {a, b}) deﬁned as follows:
1.
∅, {}, {a}, and {b} are elements of F.
2.
For every L1 and every L2 in F, L1 ∪L2 ∈F.
3.
For every L1 and every L2 in F, L1L2 ∈F.
F is the smallest set of languages that contains the languages ∅, {}, {a}, and {b} and is
closed under the operations of union and concatenation.
Some elements of F, in addition to the four from statement 1, are {a, b}, {ab}, {a, b, ab},
{aba, abb, abab}, and {aa, ab, aab, ba, bb, bab}. The ﬁrst of these is the union of {a} and
{b}, the second is the concatenation of {a} and {b}, the third is the union of the ﬁrst
and second, the fourth is the concatenation of the second and third, and the ﬁfth is the
concatenation of the ﬁrst and third.
Can you think of any languages over {a, b} that are not in F? For every string x ∈
{a, b}∗, the language {x} can be obtained by concatenating |x| copies of {a} or {b}, and
every set {x1, x2, . . . , xk} of strings can be obtained by taking the union of the languages
{xi}. What could be missing?
This recursive deﬁnition is perhaps the ﬁrst one in which we must remember that
elements in the set we are deﬁning are obtained by using the basis statement and one
or more of the recursive statements a ﬁnite number of times. In the previous examples,
it wouldn’t have made sense to consider anything else, because natural numbers cannot
be inﬁnite, and in this book we never consider strings of inﬁnite length. It makes sense
to talk about inﬁnite languages over {a, b}, but none of them is in F. Statement 3 in
the deﬁnition of N in Example 1.15 says every element of N can be obtained by
using the ﬁrst two statements—can be obtained, for example, by someone with a pencil
and paper who is applying the ﬁrst two statements in the deﬁnition in real time. For a
language L to be in F, there must be a sequence of steps, each of which involves
statements in the deﬁnition, that this person could actually carry out to produce L:
There must be languages L0, L1, L2, . . . , Ln so that L0 is obtained from the basis
statement of the deﬁnition; for each i > 0, Li is either also obtained from the basis
statement or obtained from two earlier Lj’s using union or concatenation; and Ln = L.
The conclusion in this example is that the set F is the set of all ﬁnite languages over
{a, b}.
One ﬁnal observation about certain recursive deﬁnitions will be useful in Chap-
ter 4 and a few other places. Sometimes, although not in any of the examples so far
in this section, a ﬁnite set can be described most easily by a recursive deﬁnition.
In this case, we can take advantage of the algorithmic nature of these deﬁnitions
to formulate an algorithm for obtaining the set.

26
C H A P T E R 1
Mathematical Tools and Techniques
EXAMPLE 1.21
The Set of Cities Reachable from City s
Suppose that C is a ﬁnite set of cities, and the relation R is deﬁned on C by saying that for
cities c and d in C, cRd if there is a nonstop commercial ﬂight from c to d. For a particular
city s ∈C, we would like to determine the subset r(s) of C containing the cities that can
be reached from s, by taking zero or more nonstop ﬂights. Then it is easy to see that the
set r(s) can be described by the following recursive deﬁnition.
1.
s ∈r(s).
2.
For every c ∈r(s), and every d ∈C for which cRd, d ∈r(s).
Starting with s, by the time we have considered every sequence of steps in which the second
statement is used n times, we have obtained all the cities that can be reached from s by
taking n or fewer nonstop ﬂights. The set C is ﬁnite, and so the set r(s) is ﬁnite. If r(S) has
N elements, then it is easy to see that by using the second statement N −1 times we can
ﬁnd every element of r(s). However, we may not need that many steps. If after n steps we
have the set rn(s) of cities that can be reached from s in n or fewer steps, and rn+1(s) turns
out to be the same set (with no additional cities), then further iterations will not add any
more cities, and r(s) = rn(s). The conclusion is that we can obtain r(s) using the following
algorithm.
r0(s) = {s}
n = 0
repeat
n = n + 1
rn(s) = rn−1(s) ∪{d ∈C | cRd for some c ∈rn−1(s)}
until rn(s) = rn−1(s)
r(s) = rn(s)
In the same way, if we have a ﬁnite set C and a recursive deﬁnition of a subset S of C,
then even if we don’t know how many elements C has, we can translate our deﬁnition into
an algorithm that is guaranteed to terminate and to produce the set S.
In general, if R is a relation on an arbitrary set A, we can use a recursive deﬁnition
similar to the one above to obtain the transitive closure of R, which can be described as the
smallest transitive relation containing R.
1.6 STRUCTURAL INDUCTION
In the previous section we found a recursive deﬁnition for a language Expr of
simple algebraic expressions. Here it is again, with the operator notation we intro-
duced.
1.
a ∈Expr.
2.
For every x and every y in Expr, x ◦y and x • y are in Expr.
3.
For every x ∈Expr, ⋄(x) ∈Expr.

1.6
Structural Induction
27
(By deﬁnition, if x and y are elements of Expr, x ◦y = x + y, x • y = x ∗y, and
⋄(x) = (x).)
Suppose we want to prove that every string x in Expr satisﬁes the statement
P (x). (Two possibilities for P (x) are the statements “x has equal numbers of left
and right parentheses” and “x has an odd number of symbols”.) Suppose also that
the recursive deﬁnition of Expr provides all the information that we have about
the language. How can we do it?
The principle of structural induction says that in order to show that P (x) is
true for every x ∈Expr, it is sufﬁcient to show:
1.
P (a) is true.
2.
For every x and every y in Expr, if P (x) and P (y) are true, then P (x ◦y)
and P (x • y) are true.
3.
For every x ∈Expr, if P (x) is true, then P (⋄(x)) is true.
It’s not hard to believe that this principle is correct. If the element a of Expr
that we start with has the property we want, and if all the operations we can use to
get new elements preserve the property (that is, when they are applied to elements
having the property, they produce elements having the property), then there is no
way we can ever use the deﬁnition to produce an element of Expr that does not
have the property.
Another way to understand the principle is to use our paraphrase of the recur-
sive deﬁnition of Expr. Suppose we denote by LP the language of all strings
satisfying P . Then saying every string in Expr satisﬁes P is the same as saying
that Expr ⊆LP. If Expr is indeed the smallest language that contains a and is
closed under the operations ◦, •, and ⋄, then Expr is a subset of every language
that has these properties, and so it is enough to show that LP itself has them—i.e.,
LP contains a and is closed under the three operations. And this is just what the
principle of structural induction says.
The feature to notice in the statement of the principle is the close resemblance
of the statements 1–3 to the recursive deﬁnition of Expr. The outline of the proof is
provided by the structure of the deﬁnition. We illustrate the technique of structural
induction by taking P to be the second of the two properties mentioned above and
proving that every element of Expr satisﬁes it.
EXAMPLE 1.22
A Proof by Structural Induction That Every Element
of Expr Has Odd Length
To simplify things slightly, we will combine statements 2 and 3 of our ﬁrst deﬁnition into
a single statement, as follows:
1.
a ∈Expr.
2.
For every x and every y in Expr, x + y, x ∗y, and (x) are in Expr.

28
C H A P T E R 1
Mathematical Tools and Techniques
The corresponding statements that we will establish in our proof are these:
1.
|a| is odd.
2.
For every x and y in Expr, if |x| and |y| are odd, then |x + y|, |x ∗y|, and |(x)| are
odd.
The basis statement of the proof is the statement that |a| is odd, which corresponds to the
basis statement a ∈Expr in the recursive deﬁnition of Expr. When we prove the conditional
statement, in the induction step of the proof, we will assume that x and y are elements of
Expr and that |x| and |y| are odd. We refer to this assumption as the induction hypothesis.
We make no other assumptions about x and y; they are arbitrary elements of Expr. This is
confusing at ﬁrst, because it seems as though we are assuming what we’re trying to prove
(that for arbitrary elements x and y, |x| and |y| are odd). It’s important to say it carefully.
We are not assuming that |x| and |y| are odd for every x and y in Expr. Rather, we are
considering two arbitrary elements x and y and assuming that the lengths of those two
strings are odd, in order to prove the conditional statement
If |x| and |y| are odd, then |x ◦y|, |x • y| and | ⋄(x)| are odd.
Each time we present a proof by structural induction, we will be careful to state explic-
itly what we are trying to do in each step. We say ﬁrst what we are setting out to prove,
or what the ultimate objective is; second, what the statement is that needs to be proved
in the basis step, and why it is true; third, what the induction hypothesis is; fourth, what
we are trying to prove in the induction step; and ﬁnally, what the steps of that proof are.
Surprisingly often, if we are able to do the ﬁrst four things correctly and precisely, the proof
of the induction step turns out to be very easy.
Here is our proof.
To Prove: For every element x of Expr, |x| is odd.
Basis step. We wish to show that |a| is odd. This is true because |a| = 1.
Induction hypothesis. x and y are in Expr, and |x| and |y| are odd.
Statement to be proved in the induction step. |x + y|, |x ∗y|, and |(x)| are odd.
Proof of induction step. The numbers |x + y| and |x ∗y| are both |x| + |y| + 1,
because the symbols of x + y include those in x, those in y, and the additional
“operator” symbol. The number |(x)| is |x| + 2, because two parentheses have been
added to the symbols of x. The ﬁrst number is odd because the induction hypothesis
implies that it is the sum of two odd numbers plus 1, and the second number is odd
because the induction hypothesis implies that it is an odd number plus 2.
EXAMPLE 1.23
Mathematical Induction
Very often, the easiest way to prove a statement of the form “For every integer n ≥n0,
P(n)” is to apply the principle of structural induction, using the recursive deﬁnition given
in Example 1.11 of the subset {n ∈N | n ≥n0}. Such a proof is referred to as a proof
by mathematical induction, or simply a proof by induction. The statement P(n) might be a

1.6
Structural Induction
29
numerical fact or algebraic formula involving n, but in our subject it could just as easily be
a statement about a set with n elements, or a string of length n, or a sequence of n steps.
The basis step is to establish the statement P(n) for n0, the smallest number in the
set. The induction hypothesis is the assumption that k is a number in the set and that
P(n) is true when n = k, or that P(k) is true; and the induction step is to show using this
assumption that P(k + 1) is true. Here is an example, in which n0 = 0, so that the set is
simply N.
To prove: For every n ∈N, and every set A with n elements, 2A has exactly 2n elements.
Basis step. The statement to be proved is that for every set A with 0 elements, 2A
has 20 = 1 element. This is true because the only set with no elements is ∅, and
2∅= {∅}, which has one element.
Induction hypothesis. k ∈N and for every set A with k elements, 2A has 2k
elements.
Statement to be proved in the induction step. For every set A with k + 1 elements,
2A has 2k+1 elements.
Proof of induction step. If A has k + 1 elements, then because k ≥0, it must
have at least one. Let a be an element of A. Then A −{a} has k elements. By the
induction hypothesis, A −{k} has exactly 2k subsets, and so A has exactly 2k
subsets that do not contain a. Every subset B of A that contains a can be written
B = B1 ∪{a}, where B1 is a subset of A that doesn’t contain a, and for two
different subsets containing a, the corresponding subsets not containing a are also
different; therefore, there are precisely as many subsets of A that contain a as
there are subsets that do not. It follows that the total number of subsets is 2 ∗2k =
2k+1.
EXAMPLE 1.24
Strong Induction
We present another proof by mathematical induction, to show that every positive integer 2 or
larger can be factored into prime factors. The proof will illustrate a variant of mathematical
induction that is useful in situations where the ordinary induction hypothesis is not quite
sufﬁcient.
To prove: For every natural number n ≥2, n is either a prime or a product of two or more
primes.
For reasons that will be clear very soon, we modify the statement to be proved, in a
way that makes it seem like a stronger statement.
To prove: For every natural number n ≥2, every natural number m satisfying 2 ≤m ≤n
is either a prime or a product of two or more primes.
Basis step. When n = 2, the modiﬁed statement is that every number m satisfying
2 ≤m ≤2 is either a prime or a product of two or more primes. Of course the only
such number m is 2, and the statement is true because 2 is prime.

30
C H A P T E R 1
Mathematical Tools and Techniques
Induction hypothesis. k ≥2, and for every m satisfying 2 ≤m ≤k, m is either a
prime or a product of primes.
Statement to be proved in the induction step. For every m satisfying
2 ≤m ≤k + 1, m is either prime or a product of primes.
Proof of induction step. For every m with 2 ≤m ≤k, we already have the
conclusion we want, from the induction hypothesis. The only additional statement we
need to prove is that k + 1 is either prime or a product of primes.
If k + 1 is prime, then the statement we want is true. Otherwise, by the deﬁnition
of a prime, k + 1 = r ∗s, for some positive integers r and s, neither of which is 1 or
k + 1. It follows that 2 ≤r ≤k and 2 ≤s ≤k, and so the induction hypothesis
implies that each of the two is either prime or a product of primes. We may conclude
that their product k + 1 is the product of two or more primes.
As we observed in the proof, the basis step and the statement to be proved in the
induction step were not changed at all as a result of modifying the original statement. The
purpose of the modiﬁcation is simply to allow us to use the stronger induction hypothesis:
not only that the statement P(n) is true when n = k, but that it is true for every n satisfying
2 ≤n ≤k. This was not necessary in Example 1.23, but often you will ﬁnd when you
reach the proof of the induction step that the weaker hypothesis doesn’t provide enough
information. In this example, it tells us that k is a prime or a product of primes—but we
need to know that the numbers r and s have this property, and neither of these is k.
Now that we have ﬁnished this example, you don’t have to modify the statement to
be proved when you encounter another situation where the stronger induction hypothesis is
necessary; declaring that you are using strong induction allows you to assume it.
In Example 1.19 we gave a recursive deﬁnition of the language Balanced, the
set of balanced strings of parentheses. When you construct an algebraic expression
or an expression in a computer program, and you need to end up with a balanced
string of parentheses, you might check your work using an algorithm like the
following. Go through the string from left to right, and keep track of the number
of excess left parentheses; start at 0, add 1 each time you hit a left parenthesis,
and subtract 1 each time you hit a right parenthesis; if the number is 0 when you
reach the end and has never dropped below 0 along the way, the string is balanced.
We mentioned at the beginning of this section that strings in Expr or Balanced
have equal numbers of left and right parentheses; strengthening the condition by
requiring that no preﬁx have more right parentheses than left produces a condition
that characterizes balanced strings and explains why this algorithm works.
EXAMPLE 1.25
Another Characterization of Balanced Strings of Parentheses
The language Balanced was deﬁned as follows in Example 1.19.
1.
 ∈Balanced.
2.
For every x and every y in Balanced, both xy and (x) are elements of Balanced.

1.6
Structural Induction
31
We wish to show that a string x belongs to this language if and only if the statement B(x)
is true:
B(x): x contains equal numbers of left and right parentheses, and no preﬁx of x
contains more right than left.
For the ﬁrst part, showing that every string x in Balanced makes the condition
B(x) true, we can use structural induction, because we have a recursive deﬁnition of
Balanced. The basis step is to show that B() is true, and it is easy to see that it is.
The induction hypothesis is that x and y are two strings in Balanced for which B(x)
and B(y) are true, and the statement to be proved in the induction step is that B(xy)
and B((x)) are both true. We will show the ﬁrst statement, and the second is at least
as simple.
Because x and y both have equal numbers of left and right parentheses, the string xy
does also. If z is a preﬁx of xy, then either z is a preﬁx of x or z = xw for some preﬁx w
of y. In the ﬁrst case, the induction hypothesis tells us B(x) is true, which implies that z
cannot have more right parentheses than left. In the second case, the induction hypothesis
tells us that x has equal numbers of left and right parentheses and that w cannot have more
right than left; therefore, xw cannot have more right than left.
For the second part of the proof, we can’t use structural induction based on the recursive
deﬁnition of Balanced, because we’re trying to prove that every string of parentheses, not
just every string in Balanced, satisﬁes some property. There is not a lot to be gained by trying
to use structural induction based on a recursive deﬁnition of the set of strings of parentheses,
and instead we choose strong induction, rewriting the statement so that it involves an integer
explicitly:
To prove: For every n ∈N, if x is a string of parentheses so that |x| = n and B(x) is true,
then x ∈Balanced.
Basis step. The statement in the basis step is that if |x| = 0 and B(x), then x ∈
Balanced. We have more assumptions than we need; if |x| = 0, then x = , and so
x ∈Balanced because of statement 1 in the deﬁnition of Balanced.
Induction hypothesis. k ∈N, and for every string x of parentheses, if |x| ≤k
and B(x), then x ∈Balanced. (Writing “for every string x of parentheses, if |x| ≤k”
says the same thing as “for every m ≤k, and every string x of parentheses with
|x| = m” but involves one fewer variable and sounds a little simpler.)
Statement to be proved in induction step. For every string x of parentheses, if
|x| = k + 1 and B(x), then x ∈Balanced.
Proof of induction step. We suppose that x is a string of parentheses with
|x| = k + 1 and B(x). We must show that x ∈Balanced, which means that x can be
obtained from statement 1 or statement 2 in the deﬁnition. Since |x| > 0, statement 1
won’t help; we must show x can be obtained from statement 2. The trick here is to
look at the two cases in statement 2 and work backward.
If we want to show that x = yz for two shorter strings y and z in Balanced, the
way to do it is to show that x = yz for two shorter strings y and z satisfying B(y)
and B(z); for then the induction hypothesis will tell us that y and z are in Balanced.
However, this may not be possible, because the statements B(y) and B(z) require that

32
C H A P T E R 1
Mathematical Tools and Techniques
y and z both have equal numbers of left and right parentheses. The string ((())), for
example, cannot be expressed as a concatenation like this. We must show that these
other strings can be obtained from statement 2.
It seems reasonable, then, to consider two cases. Suppose ﬁrst that x = yz,
where y and z are both shorter than x and have equal numbers of left and right
parentheses. No preﬁx of y can have more right parentheses than left, because every
preﬁx of y is a preﬁx of x and B(x) is true. Because y has equal numbers of left and
right, and because no preﬁx of x can have more right than left, no preﬁx of z can
have more right than left. Therefore, both the statements B(y) and B(z) are true.
Since |y| ≤k and |z| ≤k, we can apply the induction hypothesis to both strings and
conclude that y and z are both elements of Balanced. It follows from statement 2 of
the deﬁnition that x = yz is also.
In the other case, we assume that x = (y) for some string y of parentheses and
that x cannot be written as a concatenation of shorter strings with equal numbers of
left and right parentheses. This second assumption is useful, because it tells us that no
preﬁx of y can have more right parentheses than left. (If some preﬁx did, then some
preﬁx y1 of y would have exactly one more right than left, which would mean that
the preﬁx (y1 of x had equal numbers; but this would contradict the assumption.) The
string y has equal numbers of left and right parentheses, because x does, and so the
statement B(y) is true. Therefore, by the induction hypothesis, y ∈Balanced, and it
follows from statement 2 that x ∈Balanced.
EXAMPLE 1.26
Sometimes Making a Statement Stronger Makes It Easier
to Prove
In this example we return to the recursive deﬁnition of Expr that we have already used
in Example 1.22 to prove a simple property of algebraic expressions. Suppose we want to
prove now that no string in Expr can contain the substring ++.
In the basis step we observe that a does not contain this substring. If we assume in
the induction hypothesis that neither x nor y contains it, then it is easy to conclude that
x ∗y and (x) also don’t. Trying to prove that x + y doesn’t, however, presents a problem.
Neither x nor y contains ++ as a substring, but if x ended with + or y started with +,
then ++ would occur in the concatenation. The solution is to prove the stronger statement
that for every x ∈Expr, x doesn’t begin or end with + and doesn’t contain the substring
++. In both the basis step and the induction step, there will be a few more things to prove,
but they are not difﬁcult and the induction hypothesis now contains all the information we
need to carry out the proof.
EXAMPLE 1.27
Deﬁning Functions on Sets Deﬁned Recursively
A recursive deﬁnition of a set suggests a way to prove things about elements of the set.
By the same principle, it offers a way of deﬁning a function at every element of the set. In
the case of the natural numbers, for example, if we deﬁne a function at 0, and if for every
natural number n we say what f (n + 1) is, assuming that we know what f (n) is, then

1.6
Structural Induction
33
we have effectively deﬁned the function at every element of N. There are many familiar
functions commonly deﬁned this way, such as the factorial function f :
f (0) = 1; for every n ∈N, f (n + 1) = (n + 1) ∗f (n)
and the function u : N →2A deﬁned by
u(0) = S0; for every n ∈N, u(n + 1) = u(n) ∪Sn+1
where S0, S1, . . . are assumed to be subsets of A. Writing nonrecursive deﬁnitions of these
functions is also common:
f (n) = n ∗(n −1) ∗(n −2) ∗· · · ∗2 ∗1
u(n) = S0 ∪S1 ∪S2 ∪· · · ∪Sn
In the second case, we could avoid “. . .” by writing
u(n) =
n
i=0
Si =

{Si | 0 ≤i ≤n}
although the recursive deﬁnition also provides concise deﬁnitions of both these notations.
It is easy to see that deﬁnitions like these are particularly well suited for induction proofs
of properties of the corresponding functions, and the exercises contain a few examples. We
consider a familiar function r : {a, b}∗→{a, b}∗that can be deﬁned recursively by referring
to the recursive deﬁnition of {a, b}∗in Example 1.17.
r() = ; for every x ∈{a, b}∗, r(xa) = ar(x) and r(xb) = br(x).
If it is not obvious what the function r is, you can see after using the deﬁnition to compute
r(aaba), for example,
r(aaba) = ar(aab) = abr(aa) = abar(a) = abar(a) = abaar() = abaa = abaa
that it is the function that reverses the order of the symbols of a string. We will often use
the notation xr instead of r(x), in this example as well as several places where this function
makes an appearance later.
To illustrate the close relationship between the recursive deﬁnition of {a, b}∗, the recur-
sive deﬁnition of r, and the principle of structural induction, we prove the following fact
about the reverse function.
For every x and every y in {a, b}∗, (xy)r = yrxr
In planning the proof, we are immediately faced with a potential problem, because the
statement has the form “for every x and every y, . . . ” rather than the simpler form “for
every x, . . . ” that we have considered before. The ﬁrst step in resolving this issue is to
realize that the quantiﬁers are nested; we can write the statement in the form ∀x(P(x)),
where P(x) is itself a quantiﬁed statement, ∀y(. . . ), and so we can attempt to use structural
induction on x.
In fact, although the principle here is reasonable, you will discover if you try this
approach that it doesn’t work. It will be easier to see why after we have completed the
proof using the approach that does work.
The phrase “for every x and every y” means the same thing as “for every y and every
x”, and now the corresponding formula looks like ∀y(. . . ). As we will see, this turns out
to be better because of the order in which x and y appear in the expression r(xy).

34
C H A P T E R 1
Mathematical Tools and Techniques
To prove: For every y in {a, b}∗, P(y) is true, where P(y) is the statement “for every
x ∈{a, b}∗, (xy)r = yrxr”.
Basis step. The statement to be proved in the basis step is this: For every x ∈{a, b}∗,
(x)r = rxr. This statement is true, because for every x, x = x; r is deﬁned to
be ; and xr = xr.
Induction hypothesis. y ∈{a, b}∗, and for every x ∈{a, b}∗, (xy)r = yrxr.
Statement to be proved in induction step. For every x ∈{a, b}∗,
(x(ya))r = (ya)rxr and (x(yb))r = (yb)rxr.
Proof of induction step. We will prove the ﬁrst part of the statement, and the proof in
the second part is almost identical. The tools that we have available are the recursive
deﬁnition of {a, b}∗, the recursive deﬁnition of r, and the induction hypothesis; all we
have to do is decide which one to use when, and avoid getting lost in the notation.
(x(ya))r = ((xy)a)r
(because concatenation is associative—i.e., x(ya) = (xy)a)
= a(xy)r
(by the second part of the deﬁnition of r, with xy instead of x)
= a(yrxr)
(by the induction hypothesis)
= (ayr)xr
(because concatenation is associative)
= (ya)rxr
(by the second part of the deﬁnition of r, with y instead of x)
Now you can see why the ﬁrst approach wouldn’t have worked. Using induction on
x, we would start out with ((xa)y)r. We can rewrite this as (x(ay))r, and the induction
hypothesis in this version would allow us to rewrite it again as (ay)rxr. But the a is at the
wrong end of the string ay, and the deﬁnition of r gives us no way to proceed further.
EXERCISES
1.1.
In each case below, construct a truth table for the statement and ﬁnd
another statement with at most one operator (∨, ∧, ¬, or →) that is
logically equivalent.
a. (p →q) ∧(p →¬q)
b. p ∨(p →q)
c. p ∧(p →q)
d. (p →q) ∧(¬p →q)
e. p ↔(p ↔q)
f. q ∧(p →q)
1.2.
A principle of classical logic is modus ponens, which asserts that the
proposition (p ∧(p →q)) →q is a tautology, or that p ∧(p ∧q)
logically implies q. Is there any way to deﬁne the conditional statement
p →q, other than the way we deﬁned it, that makes it false when p is
true and q is false and makes the modus ponens proposition a tautology?
Explain.
1.3.
Suppose m1 and m2 are integers representing months (1 ≤mi ≤12),
and d1 and d2 are integers representing days (di is at least 1 and no

Exercises
35
larger than the number of days in month mi). For each i, the pair (mi,
di) can be thought of as representing a date; for example, (9, 18)
represents September 18. We wish to write a logical proposition
involving the four integers that says (m1, d1) comes before (m2, d2) in the
calendar.
a. Find such a proposition that is a disjunction of two propositions (i.e.,
combines them using ∨).
b. Find such a proposition that is a conjunction of two propositions
(combines them using ∧).
1.4.
In each case below, say whether the statement is a tautology, a
contradiction, or neither.
a. p ∨¬(p →p)
b. p ∧¬(p →p)
c. p →¬p
d. (p →¬p) ∨(¬p →p)
e. (p →¬p) ∧(¬p →p)
f. (p ∧q) ∨(¬p) ∨(¬q)
1.5.
In the nine propositions p ∧q ∨r, p ∨q ∧r, ¬p ∧q, ¬p ∨q, ¬p →q,
p ∨q →r, p ∧q →r, p →q ∨r, and p →q ∧r, the standard
convention if no parentheses are used is to give ¬ the highest precedence,
∧the next-highest, ∨the next-highest after that, and →the lowest. For
example, ¬p ∨r would normally be interpreted (¬p) ∨r and p →q ∨r
would normally be interpreted p →(q ∨r). Are there any of the nine
whose truth value would be unchanged if the precedence of the two
operators involved were reversed? If so, which ones?
1.6.
Prove that every string of length 4 over the alphabet {a, b} contains the
substring xx, for some nonnull string x. One way is to consider all sixteen
cases, but try to reduce the number of cases as much as possible.
1.7.
Describe each of the following inﬁnite sets using the format
{
| n ∈N}, without using “. . . ” in the expression on the left side
of the vertical bar.
a. {0, −1, 2, −3, 4, −5, . . .}
b. {{0}, {1}, {2}, . . . }
c. {{0}, {0, 1}, {0, 1, 2}, {0, 1, 2, 3}, . . . }
d. {{0}, {0, 1}, {0, 1, 2, 3}, {0, 1, 2, 3, 4, 5, 6, 7}, {0, 1, . . ., 15}, {0, 1,
2, . . . , 31}, . . . }
1.8.
In each case below, ﬁnd an expression for the indicated set, involving A,
B, C, and any of the operations ∪, ∩, −, and ′.
a. {x|x ∈A or x ∈B but not both}
b. {x|x is an element of exactly one of the three sets A, B, and C}
c. {x|x is an element of at most one of the three sets A, B, and C}
d. {x|x is an element of exactly two of the three sets A, B, and C}

36
C H A P T E R 1
Mathematical Tools and Techniques
1.9.
For each integer n, denote by Cn the set of all real numbers less than n,
and for each positive number n let Dn be the set of all real numbers less
than 1/n. Express each of the following unions or intersections in a
simpler way. For example, the answer to (a) is C10. The answer is not
always one of the sets Ci or Di, but there is an equally simple answer in
each case. Since ∞is not a number, the expressions C∞and D∞do not
make sense and should not appear in your answers.
a. {Cn | 1 ≤n ≤10}
b. {Dn | 1 ≤n ≤10}
c. {Cn | 1 ≤n ≤10}
d. {Dn | 1 ≤n ≤10}
e. {Cn | 1 ≤n}
f. {Dn | 1 ≤n}
g. {Cn | 1 ≤n}
h. {Dn | 1 ≤n}
i.
{Cn | n ∈Z}
j.
{Cn | n ∈Z}
1.10.
List the elements of 22{0,1}, and number the items in your list.
1.11.
In each case below, say whether the given statement is true for the
universe (0, 1) = {x ∈R | 0 < x < 1}, and say whether it is true for the
universe [0, 1] = {x ∈R | 0 ≤x ≤1}. For each of the four cases, you
should therefore give two true-or-false answers.
a. ∀x(∃y(x > y))
b. ∀x(∃y(x ≥y))
c. ∃y(∀x(x > y))
d. ∃y(∀x(x ≥y))
1.12.
a. How many elements are there in the set
{∅, {∅}, {∅, {∅}}, {∅, {{∅, {∅}, {∅, {∅}}}}}}?
b. Describe precisely the algorithm you used to answer part (a).
1.13.
Simplify the given set as much as possible in each case below. Assume
that all the numbers involved are real numbers.
a. {{x | |x −a| < r} | r > 0}
b. {{x | |x −a| ≤r} | r > 0
1.14.
Suppose that A and B are nonempty sets and A × B ⊆B × A. Show that
A = B. Suggestion: show that A ⊆B and B ⊆A, using proof by
contradiction in each case.
1.15.
Suppose that A and B are subsets of a universal set U.
a. What is the relationship between 2A∪B and 2A ∪2B? (Under what
circumstances are they equal? If they are not equal, is one necessarily a
subset of the other, and if so, which one?) Give reasons for your
answers.

Exercises
37
b. Same question for 2A∩B and 2A ∩2B.
c. Same question for 2(A′) and (2A)′ (both subsets of 2U).
1.16.
Suppose A and B are ﬁnite sets, A has n elements, and f : A →B.
a. If f is one-to-one, what can you say about the number of elements
of B?
b. If f is onto, what can you say about the number of elements of B?
1.17.
In each case below, say whether the indicated function is one-to-one and
what its range is.
a. m : N →N deﬁned by m(x) = min(x, 2)
b. M : N →N deﬁned by M(x) = max(x, 2)
c. s : N →N deﬁned by s(x) = m(x) + M(x)
d. f : N −{0} →2N , where f (n) is the set of prime factors of n
e. (Here A is the set of all ﬁnite sets of primes and B is the set N −{0}.)
g : A →B, where g(S) is the product of the elements of S. (The
product of the elements of the empty set is 1.)
1.18.
Find a formula for a function from Z to N that is a bijection.
1.19.
In each case, say whether the function is one-to-one and whether it is onto.
a. f : Z × Z →Z × Z, deﬁned by f (a, b) = (a + b, a −b)
b. f : R × R →R × R, deﬁned by f (a, b) = (a + b, a −b)
1.20.
Suppose A and B are sets and f : A →B. For a subset S of A, we use
the notation f (S) to denote the set {f (x) | x ∈S}. Let S and T be subsets
of A.
a. Is the set f (S ∪T ) a subset of f (S) ∪f (T )? If so, give a proof; if
not, give a counterexample (i.e., say what the sets A, B, S, and T are
and what the function f is).
b. Is the set f (S) ∪f (T ) a subset of f (S ∪T )? Give either a proof or a
counterexample.
c. Repeat part (a) with intersection instead of union.
d. Repeat part (b) with intersection instead of union.
e. In each of the ﬁrst four parts where your answer is no, what extra
assumption on the function f would make the answer yes? Give
reasons for your answer.
1.21.
Let E be the set of even natural numbers, S the set of nonempty subsets
of E, T the set of nonempty subsets of N, and P the set of partitions of
N into two nonempty subsets.
a. Suppose f : T →P is deﬁned by the formula f (A) = {A, N −A} (in
other words, for a nonempty subset A of N, f (A) is the partition of N
consisting of the two subsets A and N −A). Is f a bijection from T
to P? Why or why not?
b. Suppose that g : S →P is deﬁned by g(A) = {A, N −A}. Is g a
bijection from S to P? Why or why not?

38
C H A P T E R 1
Mathematical Tools and Techniques
1.22.
Suppose U is a set, ◦is a binary operation on U, and S0 is a subset of U.
Deﬁne the subset A of U recursively as follows:
S0 ⊆A; for every x and y in A, x ◦y ∈A
(In other words, A is the smallest subset of A that contains the elements
of S0 and is closed under ◦.) Show that
A =

{S | S0 ⊆S ⊆U and S is closed under ◦}
1.23.
In each case below, a relation on the set {1, 2, 3} is given. Of the three
properties, reﬂexivity, symmetry, and transitivity, determine which ones
the relation has. Give reasons.
a. R = {(1, 3), (3, 1), (2, 2)}
b. R = {(1, 1), (2, 2), (3, 3), (1, 2)}
c. R = ∅
1.24.
For each of the eight lines of the table below, construct a relation on
{1, 2, 3} that ﬁts the description.
reﬂexive
symmetric
transitive
true
true
true
true
true
false
true
false
true
true
false
false
false
true
true
false
true
false
false
false
true
false
false
false
1.25.
Each case below gives a relation on the set of all nonempty subsets of N.
In each case, say whether the relation is reﬂexive, whether it is symmetric,
and whether it is transitive.
a. R is deﬁned by: ARB if and only if A ⊆B.
b. R is deﬁned by: ARB if and only if A ∩B ̸= ∅.
c. R is deﬁned by: ARB if and only if 1 ∈A ∩B.
1.26.
Let R be a relation on a set S. Write three quantiﬁed statements (the
domain being S in each case), which say, respectively, that R is not
reﬂexive, R is not symmetric, and R is not transitive.
1.27.
Suppose S is a nonempty set, A = 2S, and the relation R on A is deﬁned
as follows: For every X and every Y in A, XRY if and only if there is a
bijection from X to Y.
a. Show that R is an equivalence relation.
b. If S is a ﬁnite set with n elements, how many equivalence classes does
the equivalence relation R have?
c. Again assuming that S is ﬁnite, describe a function f : A →N so that
for every X and Y in A, XRY if and only if f (X) = f (Y).

Exercises
39
1.28.
Suppose A and B are sets, f : A →B is a function, and R is the relation
on A so that for x, y ∈A, xRy if and only if f (x) = f (y).
a. Show that R is an equivalence relation on A.
b. If A = {0, 1, 2, 3, 4, 5, 6, 7, 8}, B = N, and f (x) = (x −3)2 for every
x ∈A, how many equivalence classes are there, and what are the
elements of each one?
c. Suppose A has p elements and B has q elements. If the function f is
one-to-one (not necessarily onto), how many equivalence classes does
the equivalence relation R have? If the function f is onto (not
necessarily one-to-one), how many equivalence classes does R have?
1.29.
Show that for every set A and every equivalence relation R on A, there is
a set B and a function f : A →B such that R is the relation described in
Exercise 1.28.
1.30.
For a positive integer n, ﬁnd a function f : N →N so that the
equivalence relation ≡n on N can be described as in Exercise 1.28.
1.31.
Show that for every language L, LL∗= L∗if and only if  ∈L.
1.32.
For a ﬁnite language L, let |L| denote the number of elements of L. For
example, |{, a, ababb}| = 3. This notation has nothing to do with the
length |x| of a string x. The statement |L1L2| = |L1||L2| says that the
number of strings in the concatenation L1L2 is the same as the product of
the two numbers |L1| and |L2|. Is this always true? If so, give reasons,
and if not, ﬁnd two ﬁnite languages L1, L2 ⊆{a, b}∗such that
|L1L2| ̸= |L1||L2|.
1.33.
Let L1 and L2 be subsets of {a, b}∗.
a. Show that if L1 ⊆L2, then L∗
1 ⊆L∗
2.
b. Show that L∗
1 ∪L∗
2 ⊆(L1 ∪L2)∗.
c. Give an example of two languages L1 and L2 such that
L∗
1 ∪L∗
2 ̸= (L1 ∪L2)∗.
d.
†One way for the two languages L∗
1 ∪L∗
2 and (L1 ∪L2)∗to be equal
is for one of the two languages L1 and L2 to be a subset of the other,
or more generally, for one of the two languages L∗
1 and L∗
2 to be a
subset of the other. Find an example of languages L1 and L2 for
which neither of L∗
1, L∗
2 is a subset of the other, but L∗
1 ∪L∗
2 =
(L1 ∪L2)∗.
1.34.
†Suppose that x, y ∈{a, b}∗and neither is . Show that if xy = yx, then
for some string z and two integers i and j, x = zi and y = zj.
1.35.
†Consider the language L = {yy | y ∈{a, b}∗}. We know that
L = L{} = {}L, because every language L has this property. Is there
any other way to express L as the concatenation of two languages? Prove
your answer.
1.36.
a. Consider the language L of all strings of a’s and b’s that do not end
with b and do not contain the substring bb. Find a ﬁnite language S
such that L = S∗.

40
C H A P T E R 1
Mathematical Tools and Techniques
b. Show that there is no language S such that S∗is the language of all
strings of a’s and b’s that do not contain the substring bb.
1.37.
Let L1, L2, and L3 be languages over some alphabet . In each case
below, two languages are given. Say what the relationship is between
them. (Are they always equal? If not, is one always a subset of the
other?) Give reasons for your answers, including counterexamples if
appropriate.
a. L1(L2 ∩L3), L1L2 ∩L1L3
b. L∗
1 ∩L∗
2, (L1 ∩L2)∗
c. L∗
1L∗
2, (L1L2)∗
1.38.
In each case below, write a quantiﬁed statement, using the formal notation
discussed in the chapter, that expresses the given statement. In both cases
the set A is assumed to be a subset of the domain, not necessarily the
entire domain.
a. There are at least two distinct elements in the set A satisfying the
condition P (i.e., for which the proposition P (x) holds).
b. There is exactly one element x in the set A satisfying the condition P .
1.39.
Consider the following ‘proof’ that every symmetric, transitive relation R
on a set A must also be reﬂexive:
Let a be any element of A. Let b be any element of A for
which aRb. Then since R is symmetric, bRa. Now since R is
transitive, and since aRb and bRa, it follows that aRa.
Therefore R is reﬂexive.
Your answer to Exercise 1.24 shows that this proof cannot be correct.
What is the ﬁrst incorrect statement in the proof, and why is it
incorrect?
1.40.
†Suppose A is a set having n elements.
a. How many relations are there on A?
b. How many reﬂexive relations are there on A?
c. How many symmetric relations are there on A?
d. How many relations are there on A that are both reﬂexive and
symmetric?
1.41.
Suppose R is a relation on a nonempty set A.
a. Deﬁne Rs = R ∪{(x, y) | yRx}. Show that Rs is symmetric and is the
smallest symmetric relation on A containing R (i.e., for any symmetric
relation R1 with R ⊆R1, Rs ⊆R1).
b. Deﬁne Rt to be the intersection of all transitive relations on A
containing R. Show that Rt is transitive and is the smallest transitive
relation on A containing R.
c. Let Ru = R ∪{(x, y) | ∃z(xRz and zRy)}. Is Ru equal to the set Rt in
part (b)? Either prove that it is, or give an example in which it is not.

Exercises
41
The relations Rs and Rt are called the symmetric closure and transitive
closure of R, respectively.
1.42.
Suppose R is an equivalence relation on a set A. A subset S ⊆A is
pairwise inequivalent if no two distinct elements of S are equivalent. S is
a maximal pairwise inequivalent set if S is pairwise inequivalent and for
every element of A, there is an element of S equivalent to it. Show that a
set S is a maximal pairwise inequivalent set if and only if it contains
exactly one element of each equivalence class.
1.43.
Suppose R1 and R2 are equivalence relations on a set A. As discussed in
Section 1.3, the equivalence classes of R1 and R2 form partitions P1 and
P2, respectively, of A. Show that R1 ⊆R2 if and only if the partition P1 is
ﬁner than P2 (i.e., every subset in the partition P2 is the union of one or
more subsets in the partition P1).
1.44.
Each case below gives, a recursive deﬁnition of a subset L of {a, b}∗.
Give a simple nonrecursive deﬁnition of L in each case.
a. a ∈L; for any x ∈L, xa and xb are in L.
b. a ∈L; for any x ∈L, bx and xb are in L.
c. a ∈L; for any x ∈L, ax and xb are in L.
d. a ∈L; for any x ∈L, xb, xa, and bx are in L.
e. a ∈L; for any x ∈L, xb, ax, and bx are in L.
f. a ∈L; for any x ∈L, xb and xba are in L.
1.45.
Prove using mathematical induction that for every nonnegative integer n,
n

i=1
1
i(i + 1) =
n
n + 1
(If n = 0, the sum on the left is 0 by deﬁnition.)
1.46.
Suppose r is a real number other than 1. Prove using mathematical
induction that for every nonnegative integer n,
n

i=0
ri = 1 −rn+1
1 −r
1.47.
Prove using mathematical induction that for every nonnegative integer n,
1 +
n

i=1
i ∗i! = (n + 1)!
1.48.
Prove using mathematical induction that for every integer n ≥4, n! > 2n.
1.49.
Suppose x is any real number greater than −1. Prove using mathematical
induction that for every nonnegative integer n, (1 + x)n ≥1 + nx. (Be
sure you say in your proof exactly how you use the assumption that
x > −1.)

42
C H A P T E R 1
Mathematical Tools and Techniques
1.50.
Prove using mathematical induction that for every positive integer n,
n

i=1
i ∗2i = (n −1) ∗2n+1 + 2
1.51.
Prove using mathematical induction that for every nonnegative integer n,
n is either even or odd but not both. (By deﬁnition, an integer n is even if
there is an integer i so that n = 2 ∗i, and n is odd if there is an integer i
so that n = 2 ∗i + 1.)
1.52.
Prove that for every language L ⊆{a, b}∗, if L2 ⊆L, then LL∗⊆L.
1.53.
Suppose that  is an alphabet, and that f : ∗→∗has the property
that f (σ) = σ for every σ ∈ and f (xy) = f (x)f (y) for every
x, y ∈∗. Prove that for every x ∈∗, f (x) = x.
1.54.
Prove that for every positive integer n, there is a nonnegative integer i and
an odd integer j so that n = 2i ∗j.
1.55.
Show using mathematical induction that for every x ∈{a, b}∗such that x
begins with a and ends with b, x contains the substring ab.
1.56.
Show using mathematical induction that every nonempty subset A of N
has a smallest element. (Perhaps the hardest thing about this problem is
ﬁnding a way of formulating the statement so that it involves an integer n
and can therefore be proved by induction. Why is it not feasible to prove
that for every integer n ≥1, every subset A of N containing at least n
elements has a smallest element?)
1.57.
Some recursive deﬁnitions of functions on N don’t seem to be based
directly on the recursive deﬁnition of N in Example 1.15. The Fibonacci
function f is usually deﬁned as follows.
f (0) = 0; f (1) = 1; for every n > 1, f (n) = f (n −1) + f (n −2).
Here we need to give both the values f (0) and f (1) in the ﬁrst part of the
deﬁnition, and for each larger n, f (n) is deﬁned using both f (n −1) and
f (n −2). If there is any doubt in your mind, you can use strong induction
to verify that for every n ∈N, f (n) is actually deﬁned. Use strong
induction to show that for every n ∈N, f (n) ≤(5/3)n. (Note that in the
induction step, you can use the recursive formula only if n > 1; checking
the case n = 1 separately is comparable to performing a second basis step.)
1.58.
The numbers an, for n ≥0, are deﬁned recursively as follows.
a0 = −2; a1 = −2;
for n ≥2, an = 5an−1 −6an−2
Use strong induction to show that for every n ≥0, an = 2 ∗3n −4 ∗2n.
(Refer to Example 1.24.)
1.59.
Show that the set B in Example 1.11 is precisely the set
S = {2i ∗5j | i, j ∈N}.
1.60.
Suppose the language L ⊆{a, b}∗is deﬁned recursively as follows:
 ∈L;
for every x ∈L, both ax and axb are elements of L.

Exercises
43
Show that L = L0, where L0 = {aibj | i ≥j}. To show that L ⊆L0 you
can use structural induction, based on the recursive deﬁnition of L. In the
other direction, use strong induction on the length of a string in L0.
1.61.
Find a recursive deﬁnition for the language L = {aibj | i ≤j ≤2i}, and
show that it is correct (i.e., show that the language described by the
recursive deﬁnition is precisely L). In order to come up with a recursive
deﬁnition, it may be helpful to start with recursive deﬁnitions for each of
the languages {aibi | i ≥0} and {aib2i | i ≥0}.
1.62.
In each case below, ﬁnd a recursive deﬁnition for the language L and
show that it is correct.
a. L = {aibj | j ≥2i}
b. L = {aibj | j ≤2i}
1.63.
For a string x in the language Expr deﬁned in Example 1.19, na(x)
denotes the number of a’s in the string, and we will use nop(x) to stand
for the number of operators in x (the number of occurrences of + or ∗).
Show that for every x ∈Expr, na(x) = 1 + nop(x).
1.64.
For a string x in Expr, show that x does not start or end with + and does
not contain the substring ++. (In this case it would be feasible to prove,
ﬁrst, that strings in Expr do not start or end with +, and then that they
don’t contain the substring ++; as Example 1.26 suggests, however, it is
possible to prove both statements in the same induction proof.)
1.65.
Suppose L ⊆{a, b}∗is deﬁned as follows:
 ∈L;
for every x ∈L, both
xa and xba are in L.
Show that for every x ∈L, both of the following statements are true.
a. na(x) ≥nb(x).
b. x does not contain the substring bb.
1.66.
Suppose L ⊆{a, b}∗is deﬁned as follows:
 ∈L;
for every x and y in L, the strings axb, bxa, and xy are in L.
Show that L = AEqB, the language of all strings x in {a, b}∗satisfying
na(x) = nb(x).
1.67.
†Suppose L ⊆{a, b}∗is deﬁned as follows:
 ∈L;
for every x and y in L, the strings axby and bxay are in L.
Show that L = AEqB, the language of all strings x in {a, b}∗satisfying
na(x) = nb(x).
1.68.
†Suppose L ⊆{a, b}∗is deﬁned as follows:
a ∈L;
for every x and y in L, the strings
ax, bxy, xby, and xyb are in L.
Show that L = L0, the language of all strings x in {a, b}∗satisfying
na(x) > nb(x).

44
C H A P T E R 1
Mathematical Tools and Techniques
1.69.
For a relation R on a set S, the transitive closure of R is the relation Rt
deﬁned as follows: R ⊆Rt; for every x, every y, and every z in S, if
(x, y) ∈Rt and (y, z) ∈Rt, then (x, z) ∈Rt. (We can summarize the
deﬁnition by saying that Rt is the smallest transitive relation containing
R.) Show that if R1 and R2 are relations on S satisfying R1 ⊆R2, then
Rt
1 ⊆Rt
2.

45
C
H
A
P
T
E
R
2
Finite Automata and the
Languages They Accept
I
n this chapter, we introduce the ﬁrst of the models of computation we will study.
A ﬁnite automaton is a model of a particularly simple computing device, which
acts as a language acceptor. We will describe how one works and look at examples
of languages that can be accepted this way. Although the examples are simple,
they illustrate how ﬁnite automata can be useful, both in computer science and
more generally. We will also see how their limitations prevent them from being
general models of computation, and exactly what might keep a language from being
accepted by a ﬁnite automaton. The simplicity of the ﬁnite automaton model makes
it possible, not only to characterize in an elegant way the languages that can be
accepted, but also to formulate an algorithm for simplifying one of these devices
as much as possible.
2.1 FINITE AUTOMATA: EXAMPLES
AND DEFINITIONS
In this section we introduce a type of computer that is simple, partly because
the output it produces in response to an input string is limited to “yes” or
“no”, but mostly because of its primitive memory capabilities during a compu-
tation.
Any computer whose outputs are either “yes” or “no” acts as a language
acceptor; the language the computer accepts is the set of input strings that cause
it to produce the answer yes. In this chapter, instead of thinking of the computer
as receiving an input string and then producing an answer, it’s a little easier to
think of it as receiving individual input symbols, one at a time, and producing after
every one the answer for the current string of symbols that have been read so far.
Before the computer has received any input symbols, the current string is , and

46
C H A P T E R 2
Finite Automata and the Languages They Accept
there is an answer for that string too. If the current string is abbab, for example,
the computer might have produced the answers “yes, no, yes, yes, no, no” so far,
one for each of the six preﬁxes of abbab.
The very simplest device for accepting a language is one whose response
doesn’t even depend on the input symbols it receives. There are only two possibil-
ities: to announce at each step that the current string is accepted, and to announce
at each step that it is not accepted. These two language acceptors are easy to con-
struct, because they don’t have to remember anything about the input symbols they
have received, but of course they are not very useful. The only languages they can
accept are the entire set ∗and the empty language ∅.
Slightly more complicated is the case in which the answer depends on the
last input symbol received and not on any symbols before that. For example, if
 = {a, b}, a device might announce every time it receives the symbol a, and only
in that case, that the current string is accepted. In this case, the language it accepts
is the set of all strings that end with a.
These are examples of a type of language acceptor called a ﬁnite automaton
(FA), or ﬁnite-state machine. At each step, a ﬁnite automaton is in one of a ﬁnite
number of states (it is a ﬁnite automaton because its set of states is ﬁnite). Its
response depends only on the current state and the current symbol. A “response”
to being in a certain state and receiving a certain input symbol is simply to enter
a certain state, possibly the same one it was already in. The FA “announces”
acceptance or rejection in the sense that its current state is either an accepting
state or a nonaccepting state. In the two trivial examples where the response is
always the same, only one state is needed. For the language of strings ending with
a, two states are sufﬁcient, an accepting state for the strings ending with a and a
nonaccepting state for all the others.
Before a ﬁnite automaton has received any input, it is in its initial state, which
is an accepting state precisely if the null string is accepted. Once we know how
many states there are, which one is the initial state, and which ones are the accepting
states, the only other information we need in order to describe the operation of the
machine is the transition function, which speciﬁes for each combination of state and
input symbol the state the FA enters. The transition function can be described by
either a table of values or (the way we will use most often) a transition diagram. In
the diagram, states are represented by circles, transitions are represented by arrows
with input symbols as labels,
p
q
a
and accepting states are designated by double instead of single circles.

2.1
Finite Automata: Examples and Deﬁnitions
47
The initial state will have an arrow pointing to it that doesn’t come from another
state.
An FA can proceed through the input, automaton-like, by just remembering
at each step what state it’s in and changing states in response to input symbols in
accordance with the transition function. With the diagram, we can trace the compu-
tation for a particular input string by simply following the arrows that correspond
to the symbols of the string.
EXAMPLE 2.1
A Finite Automaton Accepting the Language of Strings
Ending in aa
In order to accept the language
L1 = {x ∈{a, b}∗| x ends with aa}
an FA can operate with three states, corresponding to the number of consecutive a’s that
must still be received next in order to produce a string in L1: two, because the current
string does not end with a; one, because the current string ends in a but not in aa; or none,
because the current string is already in L1. It is easy to see that a transition diagram can be
drawn as in Figure 2.2.
In state q0, the input symbol b doesn’t represent any progress toward obtaining a string
in L1, and it causes the ﬁnite automaton to stay in q0; input a allows it to go to q1. In q1, the
input b undoes whatever progress we had made and takes us back to q0, while an a gives
us a string in L1. In q2, the accepting state, input a allows us to stay in q2, because the last
two symbols of the current string are still both a, and b sends us back to the initial state q0.
b
a
b
b
a
a
q0
q2
q1
Figure 2.2
An FA accepting the strings ending
with aa.
EXAMPLE 2.3
An FA Accepting the Language of Strings Ending in b and Not
Containing the Substring aa
Let L2 be the language
L2 = {x ∈{a, b}∗| x ends with b and does not contain the substring aa}

48
C H A P T E R 2
Finite Automata and the Languages They Accept
a
b
a
b
b
a
q0
q1
a, b
q2
q3
Figure 2.4
An FA accepting the strings end-
ing with b and not containing aa.
In Example 2.1, no matter what the current string is, if the next two input symbols are both
a, the FA ends up in an accepting state. In accepting L2, if the next two input symbols
are a’s, not only do we want to end up in a nonaccepting state, but we want to make sure
that from that nonaccepting state we can never reach an accepting state. We can copy the
previous example by having three states q0, q1, and q2, in which the current strings don’t
end in a, end in exactly one a, and end in two a’s, respectively. This time all three are
nonaccepting states, and from q2 both transitions return to q2, so that once our FA reaches
this state it will never return to an accepting state.
The only other state we need is an accepting state q3. Once the FA reaches this state,
by receiving the symbol b in either q0 or q1, it stays there as long as it continues to receive
b’s and moves to q1 on input a. The transition diagram for this FA is shown in Figure 2.4.
EXAMPLE 2.5
An FA Illustrating a String Search Algorithm
Suppose we have a set of strings over {a, b} and we want to identify all the ones containing a
particular substring, say abbaab. A reasonable way to go about it is to build an FA accepting
the language
L3 = {x ∈{a, b}∗| x contains the substring abbaab}
and use it to test each string in the set. Once we have the FA, the number of steps required
to test each string is no more than the number of symbols in the string, so that we can be
conﬁdent this is an efﬁcient approach.
The idea behind the FA is the same as in Example 2.1, but using a string with both
a’s and b’s will make it easier to identify the underlying principle. We can start by drawing
this diagram:
q0
q1
a
q2
b
q3
b
q4
a
q5
a
b
q6
For each i, the FA will be in state qi whenever the current string ends with the preﬁx of
abbaab having length i and not with any longer preﬁx. Now we just have to try to add
transitions to complete the diagram. The transitions leaving q6 simply return to q6, because
this FA should accept the strings containing, not ending with, abbaab. For each i < 6, we
already have one transition from qi, and we have to decide where to send the other one.

2.1
Finite Automata: Examples and Deﬁnitions
49
q0
a
b
a
a
a
b
b
b
a
b
a, b
a
b
q1
q2
q3
q4
q5
q6
Figure 2.6
An FA accepting the strings containing the substring abbaab.
Consider i = 4. One string that causes the FA to be in state q4 is abba, and we must
decide what state corresponds to the string abbab. Because abbab ends with ab, and not
with any longer preﬁx of abbaab, the transition should go to q2. The other cases are similar,
and the resulting FA is shown in Figure 2.6.
If we want an FA accepting all the strings ending in abbaab, instead of all the strings
containing this substring, we can use the transition diagram in Figure 2.6 with different
transitions from the accepting state. The transition on input a should go from state q6 to
some earlier state corresponding to a preﬁx of abbaab. Which preﬁx? The answer is a, the
longest one that is a sufﬁx of abbaaba. In other words, we can proceed as if we were drawing
the FA accepting the set of strings containing abbaabb and after six symbols we received
input a instead of b. Similarly, the transition from q6 on input b should go to state q3.
EXAMPLE 2.7
An FA Accepting Binary Representations of Integers
Divisible by 3
We consider the language L of strings over the alphabet {0, 1} that are the binary represen-
tations of natural numbers divisible by 3. Another way of saying that n is divisible by 3 is
to say that n mod 3, the remainder when n is divided by 3, is 0. This seems to suggest that
the only information concerning the current string x that we really need to remember is the
remainder when the number represented by x is divided by 3.
The question is, if we know the remainder when the number represented by x is divided
by 3, is that enough to ﬁnd the remainders when the numbers represented by x0 and x1 are
divided by 3? And that raises the question: What are the numbers represented by x0 and x1?
Just as adding 0 to the end of a decimal representation corresponds to multiplying by
ten, adding 0 to the end of a binary representation corresponds to multiplying by 2. Just as
adding 1 to the end of a decimal representation corresponds to multiplying by ten and then
adding 1 (example: 39011 = 10 ∗3901 + 1), adding 1 to the end of a binary representation
corresponds to multiplying by 2 and then adding 1 (example: 1110 represents 14, and 11101
represents 29).
Now we are ready to answer the ﬁrst question: If x represents n, and n mod 3 is r,
then what are 2n mod 3 and (2n + 1) mod 3? It is almost correct that the answers are 2r
and 2r + 1; the only problem is that these numbers may be 3 or bigger, and in that case we
must do another mod 3 operation.
These facts are enough for us to construct our FA. We begin with states corresponding
to remainders 0, 1, and 2. The only one of these that is an accepting state is 0, because
remainder 0 means that the integer is divisible by 3, and the transitions from these states

50
C H A P T E R 2
Finite Automata and the Languages They Accept
1
0
0, 1
0
0
1
2
1
1
1
0
0
0, 1
Figure 2.8
An FA accepting binary representations of integers
divisible by 3.
follow the rules outlined above. These states do not include the initial state, because the
null string doesn’t qualify as a binary representation of a natural number, or the accepting
state corresponding to the string 0. We will disallow leading 0’s in binary representations,
except for the number 0 itself, and so we need one more state for the strings that start with
0 and have more than one digit. The resulting transition diagram is shown in Figure 2.8.
EXAMPLE 2.9
Lexical Analysis
Another real-world problem for which ﬁnite automata are ideally suited is lexical analysis,
the ﬁrst step in compiling a program written in a high-level language.
Before a C compiler can begin to determine whether a string such as
main(){ double b=41.3; b *= 4; ...
satisﬁes the many rules for the syntax of C, it must be able to break up the string into tokens,
which are the indecomposable units of the program. Tokens include reserved words (in this
example, “main” and “double”), punctuation symbols, identiﬁers, operators, various types
of parentheses and brackets, numeric literals such as “41.3” and “4”, and a few others.
Programming languages differ in their sets of reserved words, as well as in their rules
for other kinds of tokens. For example, “41.” is a legal token in C but not in Pascal, which
requires a numeric literal to have at least one digit on both sides of a decimal point.
In any particular language, the rules for constructing tokens are reasonably simple.
Testing a substring to see whether it represents a valid token can be done by a ﬁnite
automaton in software form; once this is done, the string of alphabet symbols can be replaced
by a sequence of tokens, each one represented in a form that is easier for the compiler to
use in its later processing.
We will illustrate a lexical-analysis FA for a C-like language in a greatly simpliﬁed case,
in which the only tokens are identiﬁers, semicolons, the assignment operator =, the reserved
word aa, and numeric literals consisting of one or more digits and possibly a decimal point.
An identiﬁer will start with a lowercase letter and will contain only lowercase letters and/or
digits. Saying that aa is reserved means that it cannot be an identiﬁer, although longer
identiﬁers might begin with aa or contain it as a substring. The job of the FA will be to
accept strings that consist of one or more consecutive tokens. (The FA will not be required

2.1
Finite Automata: Examples and Deﬁnitions
51
to determine whether a particular sequence of tokens makes sense; that job will have to be
performed at a later stage in the compilation.) The FA will be in an accepting state each
time it ﬁnishes reading another legal token, and the state will be one that is reserved for a
particular type of token; in this sense, the lexical analyzer will be able to classify the tokens.
Another way to simplify the transition diagram considerably is to make another assump-
tion: that two consecutive tokens are always separated by a blank space.
The crucial transitions of the FA are shown in Figure 2.10. The input alphabet is the
set containing the 26 lowercase letters, the 10 digits, a semicolon, an equals sign, a decimal
point, and the blank space . We have used a few abbreviations: D for any numerical digit,
L for any lowercase letter other than a, M for any numerical digit or letter other than a,
and N for any letter or digit. You can check that all possible transitions from the initial state
are shown. From every other state, transitions not shown go to a “reject” state, from which
all transitions return to that state; no attempt is made to continue the lexical analysis once
an error is detected.
The two portions of the diagram that require a little care are the ones involving tokens
with more than one symbol. State 3 corresponds to the identiﬁer a, state 4 to the reserved
word aa, and state 5 to any other identiﬁer. Transitions to state 5 are possible from state
3 with any letter or digit except a, from states 4 or 5 with any letter or digit, and from
the initial state with any letter other than a. State 6 corresponds to numeric literals without
decimal points and state 7 to those with decimal points. State 8 is not an accepting state,
because a numeric literal must have at least one digit.
This FA could be incorporated into lexical-analysis software as follows. Each time a
symbol causes the FA to make a transition out of the initial state, we mark that symbol in the
string; each time we are in one of the accepting states and receive a blank space, we mark
1
Δ
Δ
Δ
Δ
Δ
Δ
Δ
=
4
3
;
.
.
5
6
7
8
N
D
N
a
a
L
M
D
D
D
2
Figure 2.10
An FA illustrating a simpliﬁed version of lexical analysis.

52
C H A P T E R 2
Finite Automata and the Languages They Accept
the symbol just before the blank; and the token, whose type is identiﬁed by the accepting
state, is represented by the substring that starts with the ﬁrst of these two symbols and ends
with the second.
The restriction that tokens be separated by blanks makes the job of recognizing the
beginnings and ends of tokens very simple, but in practice there is no such rule, and we could
construct an FA without it. The transition diagram would be considerably more cluttered;
the FA would not be in the initial state at the beginning of each token, and many more
transitions between the other states would be needed.
Without a blank space to tell us where a token ends, we would normally adopt the
convention that each token extends as far as possible. A substring like “=2b3aa1” would
then be interpreted as containing two tokens, “=” and “2”, and at least the ﬁrst ﬁve symbols
of a third. There are substrings, such as “3...2”, that cannot be part of any legal string.
There are others, like “1.2..3”, that can but only if the extending-as-far-as-possible policy
is abandoned. Rejecting this particular string is probably acceptable anyway, because no way
of breaking it into tokens is compatible with the syntax of C.
See Example 3.5 for more discussion of tokens and lexical analysis.
Giving the following ofﬁcial deﬁnition of a ﬁnite automaton and developing
some related notation will make it easier to talk about these devices precisely.
Deﬁnition 2.11
A Finite Automaton
A ﬁnite automaton (FA) is a 5-tuple (Q, , q0, A, δ), where
Q is a ﬁnite set of states;
 is a ﬁnite input alphabet;
q0 ∈Q is the initial state;
A ⊆Q is the set of accepting states;
δ : Q ×  →Q is the transition function.
For any element q of Q and any symbol σ ∈, we interpret δ(q, σ)
as the state to which the FA moves, if it is in state q and receives the
input σ.
The ﬁrst line of the deﬁnition deserves a comment. What does it mean to say
that a simple computer is a 5-tuple? This is simply a formalism that allows us to
deﬁne an FA in a concise way. Describing a ﬁnite automaton precisely requires us
to specify ﬁve things, and it is easier to say
Let M = (Q, , q0, A, δ) be an FA
than it is to say
Let M be an FA with state set Q, input alphabet , initial state q0, set of
accepting states A, and transition function δ.

2.1
Finite Automata: Examples and Deﬁnitions
53
We write δ(q, σ) to mean the state an FA M goes to from q after receiving the
input symbol σ. The next step is to extend the notation to allow a corresponding
expression δ∗(q, x) that will represent the state the FA ends up in if it starts out
in state q and receives the string x of input symbols. In other words, we want
to deﬁne an “extended transition function” δ∗from Q × ∗to Q. The easiest
way to deﬁne it is recursively, using the recursive deﬁnition of ∗in Example
1.17. We begin by deﬁning δ∗(q, ), and since we don’t expect the state of M
to change as a result of getting the input string , we give the expression the
value q.
Deﬁnition 2.12
The Extended Transition Function δδδ∗
Let M = (Q, , q0, A, δ) be a ﬁnite automaton. We deﬁne the extended
transition function
δ∗: Q × ∗→Q
as follows:
1. For every q ∈Q, δ∗(q, ) = q
2. For every q ∈Q, every y ∈∗, and every σ ∈,
δ∗(q, yσ) = δ(δ∗(q, y), σ)
The recursive part of the deﬁnition says that we can evaluate δ∗(q, x) if we
know that x = yσ, for some string y and some symbol σ, and if we know what
state the FA is in after starting in q and processing the symbols of y. We do it by
just starting in that state and applying one last transition, the one corresponding to
the symbol σ. For example, if M contains the transitions
p
q
a
r
b
s
c
Figure 2.13
then
δ∗(p, abc) = δ(δ∗(p, ab), c)
= δ(δ(δ∗(p, a), b), c)
= δ(δ(δ∗(p, a), b), c)
= δ(δ(δ(δ∗(p, ), a), b), c)
= δ(δ(δ(p, a), b), c)
= δ(δ(q, b), c)
= δ(r, c)
= s
Looking at the diagram, of course, we can get the same answer by just follow-
ing the arrows. The point of this derivation is not that it’s always the simplest

54
C H A P T E R 2
Finite Automata and the Languages They Accept
way to ﬁnd the answer by hand, but that the recursive deﬁnition is a reasonable
way of deﬁning the extended transition function, and that the deﬁnition provides a
systematic algorithm.
Other properties you would expect δ∗to satisfy can be derived from our def-
inition. For example, a natural generalization of the recursive statement in the
deﬁnition is the formula
δ∗(q, xy) = δ∗(δ∗(q, x), y)
which is true for every state q and every two strings x and y in ∗. The proof is by
structural induction on y and is similar to the proof of the formula r(xy) = r(y)r(x)
in Example 1.27.
The extended transition function makes it possible to say concisely what it
means for an FA to accept a string or a language.
Deﬁnition 2.14
Acceptance by a Finite Automaton
Let M = (Q, , q0, A, δ) be an FA, and let x ∈∗. The string x is
accepted by M if
δ∗(q0, x) ∈A
and is rejected by M otherwise. The language accepted by M is the set
L(M) = {x ∈∗| x is accepted by M}
If L is a language over , L is accepted by M if and only if L = L(M).
Notice what the last statement in Deﬁnition 2.14 does not say. It doesn’t say
that L is accepted by M if every string in L is accepted by M. To take this as
the deﬁnition would not be useful (it’s easy to describe a one-state FA that accepts
every string in ∗). A ﬁnite automaton accepting a language L does its job by
distinguishing between strings in L and strings not in L: accepting the strings in
L and rejecting all the others.
2.2 ACCEPTING THE UNION,
INTERSECTION, OR DIFFERENCE
OF TWO LANGUAGES
Suppose L1 and L2 are both languages over . If x ∈∗, then knowing whether
x ∈L1 and whether x ∈L2 is enough to determine whether x ∈L1 ∪L2. This
means that if we have one algorithm to accept L1 and another to accept L2, we
can easily formulate an algorithm to accept L1 ∪L2. In this section we will show
that if we actually have ﬁnite automata accepting L1 and L2, then there is a ﬁnite
automaton accepting L1 ∪L2, and that the same approach also gives us FAs accept-
ing L1 ∩L2 and L1 −L2.

2.2
Accepting the Union, Intersection, or Difference of Two Languages
55
The idea is to construct an FA that effectively executes the two original ones
simultaneously, one whose current state records the current states of both. This
isn’t difﬁcult; we can simply use “states” that are ordered pairs (p, q), where p
and q are states in the two original FAs.
Theorem 2.15
Suppose M1 = (Q1, , q1, A1, δ1) and M2 = (Q2, , q2, A2, δ2) are ﬁnite
automata accepting L1 and L2, respectively. Let M be the FA (Q, , q0,
A, δ), where
Q = Q1 × Q2
q0 = (q1, q2)
and the transition function δ is deﬁned by the formula
δ((p, q), σ) = (δ1(p, σ), δ2(q, σ))
for every p ∈Q1, every q ∈Q2, and every σ ∈. Then
1. If A = {(p, q) | p ∈A1 or q ∈A2}, M accepts the language L1 ∪L2.
2. If A = {(p, q) | p ∈A1 and q ∈A2}, M accepts the language L1 ∩L2.
3. If A = {(p, q) | p ∈A1 and q /∈A2}, M accepts the language L1 −L2.
Proof
We consider statement 1, and the other two are similar. The way the
transition function δ is deﬁned allows us to say that at any point during
the operation of M, if (p, q) is the current state, then p and q are the
current states of M1 and M2, respectively. This will follow immediately
from the formula
δ∗(q0, x) = (δ∗
1(q1, x), δ∗
2(q2, x))
which is true for every x ∈∗. The proof is by structural induction on
x and is left to Exercise 2.12. For every string x, x is accepted by M
precisely if δ∗(q0, x) ∈A, and according to the deﬁnition of A in state-
ment 1 and the formula for δ∗, this is true precisely if δ∗
1(q1, x) ∈A1 or
δ∗
2(q2, x) ∈A2—i.e., precisely if x ∈L1 ∪L2.
As we will see in Example 2.16, we don’t always need to include every ordered
pair in the state set of the composite FA.
EXAMPLE 2.16
Constructing an FA Accepting L1 ∩L2
Let L1 and L2 be the languages
L1 = {x ∈{a, b}∗| aa is not a substring of x}
L2 = {x ∈{a, b}∗| x ends with ab}

56
C H A P T E R 2
Finite Automata and the Languages They Accept
Finite automata M1 and M2 accepting these languages are easy to obtain and are
shown in Figure 2.17a. The construction in Theorem 2.15 produces an FA with the nine
states shown in Figure 2.17b. Rather than drawing all eighteen transitions, we start at the
initial state (A, P), draw the two transitions to (B, Q) and (A, P) using the deﬁnition of
δ in the theorem, and continue in this way, at each step drawing transitions from a state
that has already been reached by some other transition. At some point, we have six states
such that every transition from one of these six goes to one of these six. Since none of the
remaining three states is reachable from any of the ﬁrst six, we can leave them out.
If we want our ﬁnite automaton to accept L1 ∪L2, then we designate as accepting
states the ordered pairs among the remaining six that involve at least one of the accepting
states A, B, and R. The result is shown in Figure 2.17c.
If instead we want to accept L1 ∩L2, then the only accepting state is (A, R), since
(B, R) was one of the three omitted. This allows us to simplify the FA even further. None
of the three states (C, P), (C, Q), and (C, R) is accepting, and every transition from one
of these three goes to one of these three; therefore, we can combine them all into a single
nonaccepting state. An FA accepting L1 ∩L2 is shown in Figure 2.17d. The FA we would get
for L1 −L2 is similar and also has just four states, but in that case two are accepting states.
A
B
C
a
b
b
a
a, b
b
a
P
Q
R
b
a
(a)
(d)
a
b
(c)
(b)
b
b
AP
BQ
a
b
a
a
a
b
a
b
b
a
CQ
CR
CP
b
a
b
a
a
b
a, b
AP
AQ
AR
BP
BQ
BR
CP
CQ
CR
b
b
b
a
a
a
a
a
b
b
a
b
AR
Figure 2.17
Constructing an FA to accept the intersection of two languages.

2.2
Accepting the Union, Intersection, or Difference of Two Languages
57
EXAMPLE 2.18
An FA Accepting Strings That Contain Either ab or bba
Figure 2.19a shows FAs M1 and M2 accepting L1 = {x ∈{a, b}∗| x contains the substring
ab} and L2 = {x ∈{a, b}∗| x contains the substring bba}, respectively. They are both
obtained by the technique described in Example 2.5. Using Theorem 2.15 to construct an
FA accepting L1 ∪L2 could result in one with twelve states, but Figure 2.19b illustrates an
approach that seems likely to require considerably fewer. If it works, the FA will need only
the states we’ve drawn, and the two paths to the accepting state will correspond to strings
in L1 and strings in L2, respectively.
This approach does work; the ﬁve states shown are sufﬁcient, and it is not difﬁcult
to complete the transitions from the three intermediate states. Instead, let’s see whether the
construction in the theorem gives us the same answer or one more complicated. Figure 2.19c
shows a partially completed diagram; to complete it, we must draw the transitions from (3, q)
and (2, s) and any additional states that may be required. So far we have six states, and you
can check that (3, p), (3, r), and (3, s) will also appear if we continue this way. Notice,
however, that because states 3 and s are accepting, and transitions from either state return
to that state, every state we add will be an ordered pair involving 3 or s or both, and every
transition from one of these accepting states will return to one. The conclusion is that we
can combine (3, q) and (2, s) and we don’t need any more states; the ﬁnal diagram is in
Figure 2.19d.
b
a
b
3
2
1
a
a, b
a, b
a, b
a
b
s
r
p
a
b
b
a
q
(a)
(b)
(c)
(d)
a
b
b
b
a
2, p
1, q
a
2, s
b
1, r
a
1, p
3, q
b
a
a
a
b
a
b
1, q
1, r
2, p
1, p
b
a, b
b
a
b
a
b
Figure 2.19
Constructing an FA to accept strings containing either ab or bba.

58
C H A P T E R 2
Finite Automata and the Languages They Accept
The construction in Theorem 2.15 will always work, but the FA it produces
may not be the simplest possible. Fortunately, if we need the simplest possible
one, we don’t need to rely on the somewhat unsystematic methods in these two
examples; we will see in Section 2.6 how to start with an arbitary FA and ﬁnd one
with the fewest possible states accepting the same language.
2.3 DISTINGUISHING ONE STRING FROM
ANOTHER
The ﬁnite automaton M in Example 2.1, accepting the language L of strings in
{a, b}∗ending with aa, had three states, corresponding to the three possible numbers
of a’s still needed to have a string in L. As simple as this sounds, it’s worth taking
a closer look. Could the FA be constructed with fewer than three states? And can
we be sure that three are enough? These are different questions; we will answer
the ﬁrst in this section and return to the second in Section 2.5.
Any FA with three states ignores, or forgets, almost all the information per-
taining to the current string. In the case of M, it makes no difference whether the
current string is aba or aabbabbabaaaba; the only relevant feature of these two
strings is that both end with a and neither ends with aa. However, it does make a
difference whether the current string is aba or ab, even though neither string is in
L. It makes a difference because of what input symbols might come next. If the
next input symbol is a, the current string at that point would be abaa in the ﬁrst
case and aba in the second; one string is in L and the other isn’t. The FA has to
be able to distinguish aba and ab now, so that in case the next input symbol is a it
will be able to distinguish the two corresponding longer strings. We will describe
the difference between aba and ab by saying that they are distinguishable with
respect to L: there is at least one string z such that of the two strings abaz and
abz, one is in L and the other isn’t.
Deﬁnition 2.20
Strings Distinguishable with Respect to L
If L is a language over the alphabet , and x and y are strings in ∗,
then x and y are distinguishable with respect to L, or L-distinguishable,
if there is a string z ∈∗such that either xz ∈L and yz /∈L, or xz /∈L
and yz ∈L. A string z having this property is said to distinguish x and
y with respect to L. An equivalent formulation is to say that x and y are
L-distinguishable if L/x ̸= L/y, where
L/x = {z ∈∗| xz ∈L}
The two strings x and y are L-indistinguishable if L/x = L/y, which
means that for every z ∈∗, xz ∈L if and only if yz ∈L.
The strings in a set S ⊆∗are pairwise L-distinguishable if for
every pair x, y of distinct strings in S, x and y are L-distinguishable.

2.3
Distinguishing One String from Another
59
The crucial fact about two L-distinguishable strings, or more generally about
a set of pairwise L-distinguishable strings, is given in Theorem 2.21, and it will
provide the answer to the ﬁrst question we asked in the ﬁrst paragraph.
Theorem 2.21
Suppose M = (Q, , q0, A, δ) is an FA accepting the language L ⊆∗. If
x and y are two strings in ∗that are L-distinguishable, then δ∗(q0, x) ̸=
δ∗(q0, y). For every n ≥2, if there is a set of n pairwise L-distinguishable
strings in ∗, then Q must contain at least n states.
Proof
If x and y are L-distinguishable, then for some string z, one of the strings
xz, yz is in L and the other isn’t. Because M accepts L, this means that
one of the states δ∗(q0, xz), δ∗(q0, yz) is an accepting state and the other
isn’t. In particular,
δ∗(q0, xz) ̸= δ∗(q0, yz)
According to Exercise 2.5, however,
δ∗(q0, xz) = δ∗(δ∗(q0, x), z)
δ∗(q0, yz) = δ∗(δ∗(q0, y), z)
Because the left sides are different, the right sides must be also, and so
δ∗(q0, x) ̸= δ∗(q0, y).
The second statement in the theorem follows from the ﬁrst: If M had
fewer than n states, then at least two of the n strings would cause M
to end up in the same state, but this is impossible if the two strings are
L-distinguishable.
Returning to our example, we can now say why there must be three states
in an FA accepting L, the language of strings ending with aa. We already have
an FA with three states accepting L. Three states are actually necessary if there
are three pairwise L-distinguishable strings, and we can ﬁnd three such strings by
choosing one corresponding to each state. We choose , a, and aa. The string a
distinguishes  and a, because a /∈L and aa ∈L; the string  distinguishes 
and aa; and it also distinguishes a and aa.
As the next example illustrates, the ﬁrst statement in Theorem 2.21 can be
useful in constructing a ﬁnite automaton to accept a language, because it can help
us decide at each step whether a transition should go to a state we already have or
whether we need to add another state.
EXAMPLE 2.22
Constructing an FA to Accept {aa, aab}∗{b}
Let L be the language {aa, aab}∗{b}. In Chapter 3 we will study a systematic way to con-
struct ﬁnite automata for languages like this one. It may not be obvious at this stage that

60
C H A P T E R 2
Finite Automata and the Languages They Accept
it will even be possible, but we will proceed by adding states as needed and hope that we
will eventually have enough.
The null string is not in L, and so the initial state should not be an accepting state.
The string b is in L, the string a is not, and the two strings  and a are L-distinguishable
because ab /∈L and aab ∈L. We have therefore determined that we need at least the
states in Figure 2.23a.
The language L contains b but no other strings beginning with b. It also contains no
strings beginning with ab. These two observations suggest that we introduce a state s to take
care of all strings that fail for either reason to be a preﬁx of an element of L (Fig.2.23b).
Notice that if two strings are L-distinguishable, at least one of them must be a preﬁx of an
element of L; therefore, two strings ending up in state s cannot be L-distinguishable. All
transitions from s will return to s.
Suppose the FA is in state p and receives the input a. It can’t stay in p, because the
strings a and aa are distinguished relative to L by the string ab. It can’t return to the initial
state, because  and aa are L-distinguishable. Therefore, we need a new state t. From t,
the input b must lead to an accepting state, because aab ∈L; this accepting state cannot
be r, because aab and a are L-distinguishable; call the new accepting state u. One of the
strings that gets the FA to state u is aab. If we receive another b in state u, the situation is
the same as for an initial b; aabb and b are both in L, but neither is a preﬁx of any longer
string in L. We can therefore let δ(u, b) = r.
a, b
a, b
b
p
s
a
b
b
b
a
a
a
t
r
u
q0
a, b
a, b
b
p
s
a
b
r
q0
p
a
b
r
q0
(a)
(b)
(c)
Figure 2.23
Constructing an FA to accept {aa, aab}∗{b}.

2.3
Distinguishing One String from Another
61
We have yet to deﬁne δ(t, a) and δ(u, a). States t and u can be thought of as representing
the end of one of the strings aa and aab. (The reason u is an accepting state is that one of
these two strings, aab, also happens to be the other one followed by b.) In either case, if
the next symbol is a, we should view it as the ﬁrst symbol in another occurrence of one
of these two strings. For this reason, we can deﬁne δ(t, a) = δ(u, a) = p, and we arrive at
the FA shown in Figure 2.23c.
It may be clear already that because we added each state only if necessary, the FA
we have constructed is the one with the fewest possible states. If we had not constructed
it ourselves, we could use Theorem 2.21 to show this. The FA has six states, and apply-
ing the second statement in the theorem seems to require that we produce six pairwise
L-distinguishable strings. Coming up with six strings is easy—we can choose one cor-
responding to each state—but verifying directly that they are pairwise L-distinguishable
requires looking at all 21 choices of two of them. A slightly easier approach, since there
are four nonaccepting states and two accepting states, is to show that the four strings that
are not in L are pairwise L-distinguishable and the two strings in L are L-distinguishable.
The argument in the proof of the theorem can easily be adapted to show that every FA
accepting L must then have at least four nonaccepting states and two accepting states. This
way we have to consider only seven sets of two strings and for each set ﬁnd a string that
distinguishes the two relative to L.
EXAMPLE 2.24
An FA Accepting Strings with a in the nth Symbol from the End
Suppose n is a positive integer, and Ln is the language of strings in {a, b}∗with at least n
symbols and an a in the nth position from the end.
The ﬁrst observation about accepting this language is that if a ﬁnite automaton “remem-
bers” the most recent n input symbols it has received, or remembers the entire current string
as long as its length is less than n, then it has enough information to continue making correct
decisions. Another way to say this is that no symbol that was received more than n symbols
ago should play any part in deciding whether the current string is accepted.
We can also see that if i < n and x is any string of length i, then the string bn−ix can
be treated the same as x by an FA accepting Ln. For example, suppose n = 5 and x = baa.
Neither of the strings bbbaa and baa is in L5, three more symbols are required in both cases
before an element of L5 will be obtained, and from that point on the two current strings
will always agree in the last ﬁve symbols. As a result, an FA accepting Ln will require no
more than 2n states, the number of strings of length n.
Finally, if we can show that the strings of length n are pairwise Ln-distinguishable,
Theorem 2.21 will tell us that we need this many states. Let x and y be distinct strings of
length n. They must differ in the ith symbol (from the left), for some i with 1 ≤i ≤n.
Every string z of length i −1 distinguishes these two relative to Ln, because xz and yz
differ in the ith symbol, which is the nth symbol from the right.
Figure 2.25 shows an FA with four states accepting L2. The labeling technique we have
used here also works for n > 2; if each state is identiﬁed with a string x of length n, and
x = σ1y where |y| = n −1, the transition function can be described by the formula
δ(σ1y, σ) = yσ

62
C H A P T E R 2
Finite Automata and the Languages They Accept
ab
aa
b
b
a
b
a
a
bb
ba
a
b
Figure 2.25
An FA accepting Ln in the case n = 2.
We can carry the second statement of Theorem 2.21 one step further, by consid-
ering a language L for which there are inﬁnitely many pairwise L-distinguishable
strings.
Theorem 2.26
For every language L ⊆∗, if there is an inﬁnite set S of pairwise L-
distinguishable strings, then L cannot be accepted by a ﬁnite automaton.
Proof
If S is inﬁnite, then for every n, S has a subset with n elements. If M
were a ﬁnite automaton accepting L, then Theorem 2.21 would say that
for every n, M would have at least n states. No ﬁnite automaton can have
this property!
It is not hard to ﬁnd languages with the property in Theorem 2.26. In
Example 2.27 we take L to be the language Pal from Example 1.18, the set of palin-
dromes over {a,b}. Not only is there an inﬁnite set of pairwise L-distinguishable
strings, but all the strings in {a,b}∗are pairwise L-distinguishable.
EXAMPLE 2.27
For Every Pair x, y of Distinct Strings in {a,b}∗, x and y Are
Distinguishable with Respect to Pal
First suppose that x ̸= y and |x| = |y|. Then xr, the reverse of x, distinguishes the two with
respect to Pal, because xxr ∈Pal and yxr /∈Pal. If |x| ̸= |y|, we assume x is shorter. If
x is not a preﬁx of y, then xxr ∈Pal and yxr /∈Pal. If x is a preﬁx of y, then y = xz
for some nonnull string z. If we choose the symbol σ (either a or b) so that zσ is not a
palindrome, then xσxr ∈Pal and yσxr = xzσxr /∈Pal.
An explanation for this property of Pal is easy to ﬁnd. If a computer is trying to accept
Pal, has read the string x, and starts to receive the symbols of another string z, it can’t
be expected to decide whether z is the reverse of x unless it can actually remember every
symbol of x. The only thing a ﬁnite automaton M can remember is what state it’s in, and
there are only a ﬁnite number of states. If x is a sufﬁciently long string, remembering every
symbol of x is too much to expect of M.

2.4
The Pumping Lemma
63
2.4 THE PUMPING LEMMA
A ﬁnite automaton accepting a language operates in a very simple way. Not sur-
prisingly, the languages that can be accepted in this way are “simple” languages,
but it is not yet clear exactly what this means. In this section, we will see one
property that every language accepted by an FA must have.
Suppose that M = (Q, , q0, A, δ) is an FA accepting L ⊆∗and that Q has
n elements. If x is a string in L with |x| = n −1, so that x has n distinct preﬁxes,
it is still conceivable that M is in a different state after processing every one. If
|x| ≥n, however, then by the time M has read the symbols of x, it must have
entered some state twice; there must be two different preﬁxes u and uv (saying
they are different is the same as saying that v ̸= ) such that
δ∗(q0, u) = δ∗(q0, uv)
This means that if x ∈L and w is the string satisfying x = uvw, then we
have the situation illustrated by Figure 2.28. In the course of reading the sym-
bols of x = uvw, M moves from the initial state to an accepting state by following
a path that contains a loop, corresponding to the symbols of v. There may be
more than one such loop, and more than one such way of breaking x into three
pieces u, v, and w; but at least one of the loops must have been completed by
the time M has read the ﬁrst n symbols of x. In other words, for at least one
of the choices of u, v, and w such that x = uvw and v corresponds to a loop,
|uv| ≤n.
The reason this is worth noticing is that it tells us there must be many more
strings besides x that are also accepted by M and are therefore in L: strings that
cause M to follow the same path but traverse the loop a different number of times.
The string obtained from x by omitting the substring v is in L, because M doesn’t
have to traverse the loop at all. For each i ≥2, the string uviw is in L, because
M can take the loop i times before proceeding to the accepting state.
The statement we have now proved is known as the Pumping Lemma for
Regular Languages. “Pumping” refers to the idea of pumping up the string x by
inserting additional copies of the string v (but remember that we also get one of
the new strings by leaving out v). “Regular” won’t be deﬁned until Chapter 3, but
we will see after we deﬁne regular languages that they turn out to be precisely the
ones that can be accepted by ﬁnite automata.
q0
v
u
w
Figure 2.28
What the three strings u, v, and w in
the pumping lemma might look like.

64
C H A P T E R 2
Finite Automata and the Languages They Accept
Theorem 2.29
The Pumping Lemma for Regular Languages
Suppose L is a language over the alphabet . If L is accepted by a ﬁnite
automaton M = (Q, , q0, A, δ), and if n is the number of states of M,
then for every x ∈L satisfying |x| ≥n, there are three strings u, v, and
w such that x = uvw and the following three conditions are true:
1. |uv| ≤n.
2. |v| > 0 (i.e., v ̸= ).
3. For every i ≥0, the string uviw also belongs to L.
Later in this section we will ﬁnd ways of applying this result for a language
L that is accepted by an FA. But the most common application is to show that
a language cannot be accepted by an FA, by showing that it doesn’t have the
property described in the pumping lemma.
A proof using the pumping lemma that L cannot be accepted by a ﬁnite
automaton is a proof by contradiction. We assume, for the sake of contradic-
tion, that L can be accepted by M, an FA with n states, and we try to select
a string in L with length at least n so that statements 1–3 lead to a contradic-
tion. There are a few places in the proof where it’s easy to go wrong, so before
looking at an example, we consider points at which we have to be particularly
careful.
Before we can think about applying statements 1–3, we must have a string x ∈
L with |x| ≥n. What is n? It’s the number of states in M, but we don’t know what
M is—the whole point of the proof is to show that it can’t exist! In other words,
our choice of x must involve n. We can’t say “let x = aababaabbab”, because
there’s no reason to expect that 11 ≥n. Instead, we might say “let x = anba2n”,
or “let x = bn+1anb”, or something comparable, depending on L.
The pumping lemma tells us some properties that every string in L satisﬁes,
as long as its length is at least n. It is very possible that for some choices of x, the
fact that x has these properties does not produce any contradiction. If we don’t get
a contradiction, we haven’t proved anything, and so we look for a string x that will
produce one. For example, if we are trying to show that the language of palindromes
over {a, b} cannot be accepted by an FA, there is no point in considering a string
x containing only a’s, because all the new strings that we will get by using the
pumping lemma will also contain only a’s, and they’re all palindromes too. No
contradiction!
Once we ﬁnd a string x that looks promising, the pumping lemma says that
there is some way of breaking x into shorter strings u, v, and w satisfying the
three conditions. It doesn’t tell us what these shorter strings are, only that they
satisfy conditions 1–3. If x = anbnan, we can’t say “let u = a10, v = an−10, and
w = bnan”. It’s not enough to show that some choices for u, v, and w produce a
contradiction—we have to show that we must get a contradiction, no matter what
u, v, and w are, as long as they satisfy conditions 1–3.
Let’s try an example.

2.4
The Pumping Lemma
65
EXAMPLE 2.30
The Language AnBn
Let L be the language AnBn introduced in Example 1.18:
L = {aibi | i ≥0}
It would be surprising if AnBn could be accepted by an FA; if the beginning input symbols
are a’s, a computer accepting L surely needs to remember how many of them there are,
because otherwise, once the input switches to b’s, it won’t be able to compare the two
numbers.
Suppose for the sake of contradiction that there is an FA M having n states and accepting
L. We choose x = anbn. Then x ∈L and |x| ≥n. Therefore, by the pumping lemma, there
are strings u, v, and w such that x = uvw and the conditions 1–3 in the theorem are true.
Because |uv| ≤n (by condition 1) and the ﬁrst n symbols of x are a’s (because of the
way we chose x), all the symbols in u and v must be a’s. Therefore, v = ak for some k > 0
(by condition 2). We can get a contradiction from statement 3 by using any number i other
than 1, because uviw will still have exactly n b’s but will no longer have exactly n a’s. The
string uv2w, for example, is an+kbn, obtained by inserting k additional a’s into the ﬁrst part
of x. This is a contradiction, because the pumping lemma says uv2w ∈L, but n + k ̸= n.
Not only does the string uv2w fail to be in L, but it also fails to be in the bigger
language AEqB containing all strings in {a, b}∗with the same number of a’s as b’s. Our
proof, therefore, is also a proof that AEqB cannot be accepted by an FA.
EXAMPLE 2.31
The Language {x ∈{a, b}∗| na(x) > nb(x)}
Let L be the language
L = {x ∈{a, b}∗| na(x) > nb(x)}
The ﬁrst sentence of a proof using the pumping lemma is always the same: Suppose for
the sake of contradiction that there is an FA M that accepts L and has n states. There are
more possibilities for x than in the previous example; we will suggest several choices, all
of which satisfy |x| ≥n but some of which work better than others in the proof.
First we try x = bna2n. Then certainly x ∈L and |x| ≥n. By the pumping lemma,
x = uvw for some strings u, v, and w satisfying conditions 1–3. Just as in Example 2.30,
it follows from conditions 1 and 2 that v = bk for some k > 0. We can get a contradiction
from condition 3 by considering uviw, where i is large enough that nb(uviw) ≥na(uviw).
Since |v| ≥1, i = n + 1 is guaranteed to be large enough. The string uvn+1w has at least n
more b’s than x does, and therefore at least 2n b’s, but it still has exactly 2n a’s.
Suppose that instead of bna2n we choose x = a2nbn. This time x = uvw, where v is
a string of one or more a’s and uviw ∈L for every i ≥0. The way to get a contradiction
now is to consider uv0w, which has fewer a’s than x does. Unfortunately, this produces a
contradiction only if |v| = n. Since we don’t know what |v| is, the proof will not work for
this choice of x.
The problem is not that x contains a’s before b’s; rather, it is that the original numbers
of a’s and b’s are too far apart to guarantee a contradiction. Getting a contradiction in this
case means making an inequality fail; if we start with a string in which the inequality is

66
C H A P T E R 2
Finite Automata and the Languages They Accept
just barely satisﬁed, then ideally any change in the right direction will cause it to fail. A
better choice, for example, is x = an+1bn. (If we had used x = bnan+1 instead of bna2n for
our ﬁrst choice, we could have used i = 2 instead of i = n to get a contradiction.)
Letting x = (ab)na is also a bad choice, but for a different reason. We know that
x = uvw for some strings u, v, and w satisfying conditions 1–3, but now we don’t have
enough information about the string v. It might be (ab)ka for some k, so that uv0w produces a
contradiction; it might be (ba)kb, so that uv2w produces a contradiction; or it might be either
(ab)k or (ba)k, so that changing the number of copies of v doesn’t change the relationship
between na and nb and doesn’t give us a contradiction.
EXAMPLE 2.32
The Language L = {ai 2 | i ≥0}
Whether a string of a’s is an element of L depends only on its length; in this sense, our
proof will be more about numbers than about strings.
Suppose L can be accepted by an FA M with n states. Let us choose x to be the
string an2. Then according to the pumping lemma, x = uvw for some strings u, v, and w
satisfying conditions 1–3. Conditions 1 and 2 tell us that 0 < |v| ≤n. Therefore,
n2 = |uvw| < |uv2w| = n2 + |v| ≤n2 + n < n2 + 2n + 1 = (n + 1)2
This is a contradiction, because condition 3 says that |uv2w| must be i2 for some integer i,
but there is no integer i whose square is strictly between n2 and (n + 1)2.
EXAMPLE 2.33
Languages Related to Programming Languages
Almost exactly the same pumping-lemma proof that we used in Example 2.30 to show AnBn
cannot be accepted by a ﬁnite automaton also works for several other languages, including
several that a compiler for a high-level programming language must be able to accept. These
include both the languages Balanced and Expr introduced in Example 1.19, because (m)n
is a balanced string, and (ma)n is a legal expression, if and only if m = n.
Another example is the set L of legal C programs. We don’t need to know much about
the syntax of C to show that this language can’t be accepted by an FA—only that the string
main(){{{ ... }}}
with m occurrences of “{” and n occurrences of “}”, is a legal C program precisely if m = n.
As usual, we start our proof by assuming that L is accepted by some FA with n states and
letting x be the string main() {n}n. If x = uvw, and these three strings satisfy conditions
1–3, then the easiest way to obtain a contradiction is to use i = 0 in condition 3. The string v
cannot contain any right brackets because of condition 1; if the shorter string uw is missing
any of the symbols in “main()”, then it doesn’t have the legal header necessary for a C
program, and if it is missing any of the left brackets, then the two numbers don’t match.
As the pumping lemma demonstrates, one way to answer questions about a
language is to examine a ﬁnite automaton that accepts it. In particular, for languages
that can be accepted by FAs, there are several decision problems (questions with

2.4
The Pumping Lemma
67
yes-or-no answers) we can answer this way, and some some of them have decision
algorithms that take advantage of the pumping lemma.
EXAMPLE 2.34
Decision Problems Involving Languages Accepted
by Finite Automata
The most fundamental question about a language L is which strings belong to it. The mem-
bership problem for a language L accepted by an FA M asks, for an arbitrary string x over
the input alphabet of M, whether x ∈L(M). We can think of the problem as speciﬁc to M,
so that an instance of the problem is a particular string x; we might also ask the question for
an arbitrary M and an arbitrary x, and consider an instance to be an ordered pair (M, x). In
either case, the way a ﬁnite automaton works makes it easy to ﬁnd a decision algorithm for
the problem. Knowing the string x and the speciﬁcations for M = (Q, , q0, A, δ) allows
us to compute the state δ∗(q0, x) and check to see whether it is an element of A.
The two problems below are examples of other questions we might ask about L(M):
1.
Given an FA M = (Q, , q0, A, δ), is L(M) nonempty?
2.
Given an FA M = (Q, , q0, A, δ), is L(M) inﬁnite?
One way for the language L(M) to be empty is for A to be empty, but this is not
the only way. The real question is not whether M has any accepting states, but whether it
has any that are reachable from q0. The same algorithm that we used in Example 1.21 to
ﬁnd the set of cities reachable from city S can be used here; another algorithm that is less
efﬁcient but easier to describe is the following.
■Decision Algorithm for Problem 1
Let n be the number of states of M. Try strings in order of length, to see whether any
are accepted by M; L(M) ̸= ∅if and only if there is a string of length less than n that is
accepted, where n is the number of states of M.
The reason this algorithm works is that according to the pumping lemma, for every
string x ∈L(M) with |x| ≥n, there is a shorter string in L(M), the one obtained by omitting
the middle portion v. Therefore, it is impossible for the shortest string accepted by M to
have length n or more.
It may be less obvious that an approach like this will work for problem 2, but again
we can use the pumping lemma. If n is the number of states of M, and x is a string in
L(M) with |x| ≥n, then there are inﬁnitely many longer strings in L(M). On the other
hand, if x ∈L(M) and |x| ≥2n, then x = uvw for some strings u, v, and w satisfying the
conditions in the pumping lemma, and uv0w = uw is a shorter string in L(M) whose length
is still n or more, because |v| ≤n. In other words, if there is a string in L with length at
least n, the shortest such string must have length less than 2n. It follows that the algorithm
below, which is undoubtedly inefﬁcient, is a decision algorithm for problem 2.
■Decision Algorithm for Problem 2
Try strings in order of length, starting with strings of length n, to see whether any are
accepted by M. L(M) is inﬁnite if and only if a string x is found with n ≤|x| < 2n.

68
C H A P T E R 2
Finite Automata and the Languages They Accept
2.5 HOW TO BUILD A SIMPLE COMPUTER
USING EQUIVALENCE CLASSES
In Section 2.3 we considered a three-state ﬁnite automaton M accepting L, the
set of strings over {a, b} that end with aa. We showed that three states were
necessary by ﬁnding three pairwise L-distinguishable strings, one for each state
of M. Now we are interested in why three states are enough. Of course, if M
really does accept L, then three are enough; we can check that this is the case
by showing that if x and y are two strings that cause M to end up in the
same state, then M doesn’t need to distinguish them, because they are not L-
distinguishable.
Each state of M corresponds to a set of strings, and we described the three sets
in Example 2.1: the strings that do not end with a, the strings that end with a but
not aa, and the strings in L. If x is a string in the second set, for example, then for
every string z, xz ∈L precisely if z = a or z itself ends in aa. We don’t need to
know what x is in order to say this, only that x ends with a but not aa. Therefore,
no two strings in this set are L-distinguishable. A similar argument works for each
of the other two sets.
We will use “L-indistinguishable” to mean not L-distinguishable. We can now
say that if S is any one of these three sets, then
1.
Any two strings in S are L-indistinguishable.
2.
No string of S is L-indistinguishable from a string not in S.
If the “L-indistinguishability” relation is an equivalence relation, then as we pointed
out at the end of Section 1.3, these two statements about a set S say precisely that
S is one of the equivalence classes. The relation is indeed an equivalence rela-
tion. Remember that x and y are L-indistinguishable if and only if L/x = L/y
(see Deﬁnition 2.20); this characterization makes it easy to see that the rela-
tion is reﬂexive, symmetric, and transitive, because the equality relation has these
properties.
Deﬁnition 2.35
The L-Indistinguishability Relation
For a language L ⊆{a, b}∗, we deﬁne the relation IL (an equivalence
relation) on ∗as follows: For x, y ∈∗,
xILy if and only if x and y are L-indistinguishable
In the case of the language L of strings ending with aa, we started with the
three-state FA and concluded that for each state, the corresponding set was one of
the equivalence classes of IL. What if we didn’t have an FA but had ﬁgured out
what the equivalence classes were, and that there were only three?

2.5
How to Build a Simple Computer Using Equivalence Classes
69
Strings Not
Ending in a
Strings Ending
in a but
Not in aa
Strings
Ending in aa
Then we would have at least the ﬁrst important ingredient of a ﬁnite automaton
accepting L: a ﬁnite set, whose elements we could call states. Calling a set of strings
a state is reasonable, because we already had in mind an association between a state
and the set of strings that led the FA to that state. But in case it seems questionable,
states don’t have to be anything special—they only have to be representable by
circles (or, as above, rectangles), with some coherent way of deﬁning an initial
state, a set of accepting states, and a transition function.
Once we start to describe this FA, the details fall into place. The initial state
should be the equivalence class containing the string , because  is one of the
strings corresponding to the initial state of any FA. Because we want the FA to
accept L, the accepting states (in this case, only one) should be the equivalence
classes containing elements of L.
Let us take one case to illustrate how the transitions can be deﬁned. The third
equivalence class is the set containing strings ending with aa. If we choose an
arbitrary element, say abaa, we have one string that we want to correspond to that
state. If the next input symbol is b, then the current string that results is abaab.
Now we simply have to determine which of the three sets this string belongs to,
and the answer is the ﬁrst. You can see in the other cases as well that what we end
up with is the diagram shown in Figure 2.2.
That’s almost all there is to the construction of the FA, although there are one
or two subtleties in the proof of Theorem 2.36. The other half of the theorem says
that there is an FA accepting L only if the set of equivalence classes of IL is ﬁnite.
The two parts therefore give us an if-and-only-if characterization of the languages
that can be accepted by ﬁnite automata. The pumping lemma in Section 2.4 does
not; we will see in Example 2.39 that there are languages L for which, although
L is not accepted by any FA, the pumping lemma does not allow us to prove this
and Theorem 2.36 does.
Theorem 2.36
If L ⊆∗can be accepted by a ﬁnite automaton, then the set QL of
equivalence classes of the relation IL on ∗is ﬁnite. Conversely, if the
set QL is ﬁnite, then the ﬁnite automaton ML = (QL, , q0, A, δ) accepts
L, where q0 = [], A = {q ∈QL | q ⊆L}, and for every x ∈∗and
every a ∈,
δ([x], a) = [xa]
Finally, ML has the fewest states of any FA accepting L.

70
C H A P T E R 2
Finite Automata and the Languages They Accept
Proof
If QL is inﬁnite, then a set S containing exactly one string from each
equivalence class is an inﬁnite set of pairwise L-distinguishable strings.
If there were an FA accepting L, it would have k states, for some k; but S
has k + 1 pairwise L-distinguishable strings, and it follows from Theorem
2.21 that every FA accepting L must have at least k + 1 states. Therefore,
there is no FA accepting L.
If QL is ﬁnite, on the other hand, we want to show that the FA ML
accepts L. First, however, we must consider the question of whether the
deﬁnition of ML even makes sense. The formula δ([x], a) = [xa] is sup-
posed to assign an equivalence class (exactly one) to the ordered pair
([x], a). The equivalence class [x] containing the string x might also con-
tain another string y, so that [x] = [y]. In order for the formula to be a
sensible deﬁnition of δ, it should be true that in this case
[xa] = δ([x], a) = δ([y], a) = [ya]
The question, then, is whether the statement [x] = [y] implies the state-
ment [xa] = [ya]. Fortunately, it does. If [x] = [y], then xILy, which
means that for every string z′, xz′ and yz′ are either both in L or both
not in L; therefore, for every z, xaz and yaz are either both in L or
both not in L (because of the previous statement with z′ = az), and so
xaILya.
The next step in the proof is to verify that with this deﬁnition of δ,
the formula
δ∗([x], y) = [xy]
holds for every two strings x, y ∈∗. This is a straightforward proof by
structural induction on y, which uses the deﬁnition of δ for this FA and
the deﬁnition of δ∗for any FA.
From this formula, it follows that
δ∗(q0, x) = δ∗([], x) = [x]
It follows from our deﬁnition of A that x is accepted by ML if and only
if [x] ⊆L. What we want is that x is accepted if and only if x ∈L, and
so we must show that [x] ⊆L if and only if x ∈L. If the ﬁrst statement
is true, then the second is, because x ∈[x]. On the other hand, if x ∈L,
and y is any element of [x], then yILx. Since x ∈L, and x and y are
L-indistinguishable, y = y ∈L. Therefore, [x] ⊆L.
A set containing one element from each equivalence class of IL is a set
of pairwise L-distinguishable strings. Therefore, by the second statement
of Theorem 2.21, every FA accepting L must have at least as many states
as there are equivalence classes. ML, with exactly this number of states,
has the fewest possible.

2.5
How to Build a Simple Computer Using Equivalence Classes
71
The statement that L can be accepted by an FA if and only if the set of
equivalence classes of IL is ﬁnite is known as the Myhill-Nerode Theorem.
As a practical matter, if we are trying to construct a ﬁnite automaton to accept
a language L, it is often easier to attack the problem directly, as in Example
2.22, than to determine the equivalence classes of IL. Theorem 2.36 is interesting
because it can be interpreted as an answer to the question of how much a computer
accepting a language L needs to remember about the current string x. It can forget
everything about x except the equivalence class it belongs to. The theorem provides
an elegant description of an “abstract” ﬁnite automaton accepting a language, and
we will see in the next section that it will help us if we already have an FA and
are trying to simplify it by eliminating all the unnecessary states.
If identifying equivalence classes is not always the easiest way to construct an
FA accepting a language L, identifying the equivalence classes from an existing
FA M = (Q, , q0, A, δ) is relatively straightforward. For each state q, we deﬁne
Lq = {x ∈∗| δ∗(q0, x) = q}
Every one of the sets Lq is a subset of some equivalence class of IL; other-
wise (if for some q, Lq contained strings in two different equivalence classes),
there would be two L-distinguishable strings that caused M to end up in the
same state, which would contradict Theorem 2.21. It follows that the number of
equivalence classes is no larger than the number of states. If M has the property
that strings corresponding to different states are L-distinguishable, then the two
numbers are the same, and the equivalence classes are precisely the sets Lq, just
as for the language of strings ending in aa discussed at the beginning of this
section.
It is possible that an FA M accepting L has more states than IL has equivalence
classes. This means that for at least one equivalence class S, there are two different
states q1 and q2 such that Lq1 and Lq2 are both subsets of S. We will see in
the next section that in this case M has more states than it needs, and we will
obtain an algorithm to simplify M by combining states wherever possible. The sets
Lq for the simpliﬁed FA are the equivalence classes of IL, as in the preceding
paragraph.
In the next example in this section, we return to the language AnBn and
calculate the equivalence classes. We have already used the pumping lemma to
show there is no FA accepting this language (Example 2.30), so we will not be
surprised to ﬁnd that there are inﬁnitely many equivalence classes.
EXAMPLE 2.37
The Equivalence Classes of IL, Where L = AnBn
As we observed in Example 2.30, accepting AnBn = {anbn | n ≥0} requires that we remem-
ber how many a’s we have read, so that we can compare that number to the number of b’s. A
precise way to say this is that two different strings of a’s are L-distinguishable: if i ̸= j, then
aibi ∈L and ajbi /∈L. Therefore, the equivalence classes [aj] are all distinct. If we were
interested only in showing that the set of equivalence classes is inﬁnite, we could stop here.

72
C H A P T E R 2
Finite Automata and the Languages They Accept
Exactly what are the elements of [aj]? Not only is the string aj L-distinguishable from
ai, but it is L-distinguishable from every other string x: A string that distinguishes the two
is abj+1, because ajabj+1 ∈L and xabj+1 /∈L. Therefore, there are no other strings in the
set [aj], and
[aj] = {aj}
Each of the strings ai is a preﬁx of an element of L. Other preﬁxes of elements of
L include elements of L themselves and strings of the form aibj where i > j. All other
strings in {a, b}∗are nonpreﬁxes of elements of L.
Two nonnull elements of L are L-indistinguishable, because if a string other than  is
appended to the end of either one, the result is not in L; and every nonnull string in L can
be distinguished from every string not in L by the string . Therefore, the set L −{} is
an equivalence class of IL.
The set of nonpreﬁxes of elements of L is another equivalence class: No two nonpreﬁxes
can be distinguished relative to L, and if xy ∈L, then the string y distinguishes x from
every nonpreﬁx.
We are left with the strings aibj with i > j > 0. Let’s consider an example, say x =
a7b3. The only string z with xz ∈L is b4. However, there are many other strings y that
share this property with x; every string ai+4bi with i > 0 does. The equivalence class
[a7b3] is the set {ai+4bi | i > 0} = {a5b, a6b2, a7b3, . . .}. Similarly, for every k > 0, the set
{ai+kbi | i > 0} is an equivalence class.
To summarize, L and the set of nonpreﬁxes of elements of L are two equivalence classes
that are inﬁnite sets. For each j ≥0, the set with the single element aj is an equivalence
class; and for every k > 0, the inﬁnite set {ak+ibi | i > 0} is an equivalence class. We have
now accounted for all the strings in {a, b}∗.
EXAMPLE 2.38
The Equivalence Classes of IL, Where L = {an2|n ∈N}
We show that if L is the language in Example 2.36 of strings in {a}∗with length a per-
fect square, then the elements of {a}∗are pairwise L-distinguishable. Suppose i and j are
two integers with 0 ≤i < j; we look for a positive integer k so that j + k is a perfect
square and i + k is not, so that ajak ∈L and aiak ̸∈L. Let k = (j + 1)2 −j = j 2 + j + 1.
Then j + k = (j + 1)2 but i + k = j 2 + i + j + 1 > j 2, so that i + k falls between j 2 and
(j + 1)2.
The language Pal ⊆{a, b}∗(Examples 1.18 and 2.31) was the ﬁrst one we found for
which no equivalence class of IL has more than one element. That example was perhaps
a little more dramatic than this one, in which the argument depends only on the length
of the string. Of course, palindromes over a one-symbol alphabet are not very interesting.
If we took L to be the set of all strings in {a, b}∗with length a perfect square, each
equivalence class would correspond to a natural number n and would contain all strings of
that length.
According to the pumping lemma, if L is accepted by an FA M, then there
is a number n, depending on M, such that we can draw some conclusions about

2.6
Minimizing the Number of States in a Finite Automaton
73
every string in L with length at least n. In the applications in Section 2.4, we didn’t
need to know where the number n came from, only that it existed. If there is no
such number (that is, if the assumption that there is leads to a contradiction), then
L cannot be accepted by an FA. If there is such a number—never mind where it
comes from—does it follow that there is an FA accepting L? The answer is no, as
the following slightly complicated example illustrates.
EXAMPLE 2.39
A Language Can Satisfy the Conclusions of the Pumping Lemma
and Still Not Be Accepted by a Finite Automaton
Consider the language
L = {aibjcj | i ≥1 and j ≥0} ∪{bjck | j ≥0 and k ≥0}
Strings in L contain a’s, then b’s, then c’s; if there is at least one a in the string, then the
number of b’s and the number of c’s have to be the same, and otherwise they don’t.
We will show that for the number n = 1, the statement in the pumping lemma is true
for L. Suppose x ∈L and |x| ≥1. If there is at least one a in the string x, and x = aibjcj,
we can deﬁne
u = 
v = a
w = ai−1bjcj
Every string of the form uviw is still of the form akbjcj and is therefore still an element
of L, whether k is 0 or not. If x is bicj, on the other hand, then again we deﬁne u to be
 and v to be the ﬁrst symbol in x, which is either b or c. It is also true in this case that
uviw ∈L for every i ≥0.
We can use Theorem 2.26 to show that there is no ﬁnite automaton accepting our
language L, because there is an inﬁnite set of pairwise L-distinguishable strings. If S is the
set {abn | n ≥0}, any two distinct elements abm and abn are distinguished by the string cm.
2.6 MINIMIZING THE NUMBER OF STATES
IN A FINITE AUTOMATON
Suppose we have a ﬁnite automaton M = (Q, , q0, A, δ) accepting L ⊆∗. For
a state q of M, we have introduced the notation Lq to denote the set of strings that
cause M to be in state q:
Lq = {x ∈∗| δ∗(q0, x) = q}
The ﬁrst step in reducing the number of states of M as much as possible is to
eliminate every state q for which Lq = ∅, along with transitions from these states.
None of these states is reachable from the initial state, and eliminating them does
not change the language accepted by M. For the remainder of this section, we
assume that all the states of M are reachable from q0.
We have deﬁned an equivalence relation on ∗, the L-indistinguishability
relation IL, and we have seen that for each state q in M, all the strings in Lq are
L-indistinguishable. In other words, the set Lq is a subset of one of the equivalence
classes of Lq.

74
C H A P T E R 2
Finite Automata and the Languages They Accept
The ﬁnite automaton we described in Theorem 2.36, with the fewest states of
any FA accepting L, is the one in which each state corresponds precisely to (accord-
ing to our deﬁnition, is) one of the equivalence classes of IL. For each state q in this
FA, Lq is as large as possible—it contains every string in some equivalence class
of IL. Every FA in which this statement doesn’t hold has more states than it needs.
There are states p and q such that the strings in Lp are L-indistinguishable from
those in Lq; if M doesn’t need to distinguish between these two types of strings,
then q can be eliminated, and the set Lp can be enlarged by adding the strings
in Lq.
The equivalence relation on ∗gives us an equivalence relation ≡on the set
Q of states of M. For p, q ∈Q,
p ≡q if and only if strings in Lp are L-indistinguishable from strings in Lq
This is the same as saying
p ≡q if and only if Lp and Lq are subsets of the same equivalence class of IL
Two strings x and y are L-distinguishable if for some string z, exactly one of
the two strings xz, yz is in L. Two states p and q fail to be equivalent if strings
x and y corresponding to p and q, respectively, are L-distinguishable, and this
means:
p ̸≡q if and only if, for some string z, exactly one of the states
δ∗(p, z), δ∗(q, z) is in A
In order to simplify M by eliminating unnecessary states, we just need to
identify the unordered pairs (p, q) for which the two states can be combined into
one. The deﬁnition of p ̸≡q makes it easier to identify the pairs (p, q) for which
p and q cannot be combined, the ones for which p ̸≡q. We look systematically
for a string z that might distinguish the states p and q (or distinguish a string in
Lp from one in Lq). With this in mind, we let SM be the set of such unordered
pairs.
SM is the set of unordered pairs (p, q) of distinct states satisfying p ̸≡q
A Recursive Deﬁnition of SM
The set SM can be deﬁned as follows:
1. For every pair (p, q) with p ̸= q, if exactly one of the two states is in A,
(p, q) ∈SM.
2. For every pair (r, s) of distinct states, if there is a symbol σ ∈ such that
the pair (δ(r, σ), δ(s, σ)) is in SM, then (r, s) ∈SM.
In the basis statement we get the pairs of states that can be distinguished by
. If the states δ(r, σ) and δ(s, σ) are distinguished by the string z, then the states
r and s are distinguished by the string σz; as a result, if the states r and s can be

2.6
Minimizing the Number of States in a Finite Automaton
75
distinguished by a string of length n, then the pair (r, s) can be added to the set
by using the recursive part of the deﬁnition n or fewer times.
Because the set SM is ﬁnite, this recursive deﬁnition provides an algorithm for
identifying the elements of SM.
Algorithm 2.40 Identifying the Pairs (p, q) with p ̸≡q List all unordered pairs
of distinct states (p, q). Make a sequence of passes through these pairs as follows.
On the ﬁrst pass, mark each pair (p, q) for which exactly one of the two states
p and q is in A. On each subsequent pass, examine each unmarked pair (r, s); if
there is a symbol σ ∈ such that δ(r, σ) = p, δ(s, σ) = q, and the pair (p, q)
has already been marked, then mark (r, s).
After a pass in which no new pairs are marked, stop. At that point, the marked
pairs (p, q) are precisely those for which p ̸≡q.
■
Algorithm 2.40 is the crucial ingredient in the algorithm to simplify the FA
by minimizing the number of states. When Algorithm 2.40 terminates, every pair
(p, q) that remains unmarked represents two states that can be combined into one,
because the corresponding sets Lp and Lq are subsets of the same equivalence
class. It may happen that every pair ends up marked; this means that for distinct
states p and q, strings in p are already L-distinguishable from strings in q, and M
already has the fewest states possible.
We ﬁnish up by making one ﬁnal pass through the states. The ﬁrst state to
be considered represents an equivalence class, or a state in our new minimal FA.
After that, a state q represents a new equivalence class, or a new state, only if
the pair (p, q) is marked for every state p considered previously. Each time we
consider a state q that does not produce a new state in the minimal FA, because it
is equivalent to a previous state p, we will add it to the set of original states that
are being combined with p.
Once we have the states in the resulting minimum-state FA, determining the
transitions is straightforward. Example 2.41 illustrates the algorithm.
EXAMPLE 2.41
Applying the Minimization Algorithm
Figure 2.42a shows a ﬁnite automaton with ten states, numbered 0–9, and Figure 2.42b shows
the unordered pairs (p, q) with p ̸= q. The pairs marked 1 are the ones marked on pass
1, in which exactly one state is an accepting state, and those marked 2 or 3 are the ones
marked on the second or third pass. In this example, the third pass is the last one on which
new pairs were marked.
How many passes are required and which pairs are marked on each one may depend
on the order in which the pairs are considered during each pass. The results in Figure 2.42b
are obtained by proceeding one vertical column at a time, and considering the pairs in each
column from top to bottom.
The pair (6, 3) is one of the pairs marked on the ﬁrst pass, since 3 is an accepting state
and 6 is not. When the pair (7, 2) is considered on the second pass, it is marked because
δ(7, a) = 6 and δ(2, a) = 3. When the pair (9, 3) is considered later on the second pass, it is

76
C H A P T E R 2
Finite Automata and the Languages They Accept
8
9
3
4
b
b
b
b
b
b
b
b
b
b
a
a
a
a
a
a
a
a
a
a
1
0
7
6
2
5
1
2
3
4
5
6
7
8
9
2
2
1
1
2
2
2
1
1
1
1
2
2
1
1
1
1
2
2
1
1
1
1
1
2
1
1
1
3
2
2
1
1
1
1
1
1
2
0
1
2
3
4
5
6
7
8
(a)
(b)
b
b
b
a
a
a
a
9
3,4,8
b
b
a
1,2,5
0
6,7
(c)
Figure 2.42
Minimizing the number of states in an FA.
also marked, because δ(9, a) = 7 and δ(3, a) = 2. The pair (7, 5) is marked on the second
pass. We have δ(9, a) = 7 and δ(4, a) = 5, but (9, 4) was considered earlier on the second
pass, and so it is not marked until the third pass.
With the information from Figure 2.42b, we can determine the states in the minimal
FA as follows. State 0 will be a new state. State 1 will be a new state, because the pair
(1, 0) is marked. State 2 will not, because (2, 1) is unmarked, which means we combine
states 2 and 1. State 3 will be a new state. State 4 will be combined with 3. State 5 will be
combined with states 1 and 2, because both (5, 1) and (5, 2) are unmarked. State 6 will be
a new state; state 7 is combined with state 6; state 8 is combined with 3 and 4; and state 9
is a new state. At this point, we have the ﬁve states shown in Figure 2.42c.
If we designate each state in the FA by the set of states in the original FA that were
combined to produce it, we can compute the transitions from the new state by considering

Exercises
77
any of the elements of that set. For example, one of the new states is {1, 2, 5}; in the original
FA, δ(1, a) = 8, which tells us that the a-transition from {1, 2, 5} goes to {3, 4, 8}. (If there
are any inconsistencies, such as δ(5, a) not being an element of {3, 4, 8}, then we’ve made
a mistake somewhere!)
EXERCISES
2.1.
In each part below, draw an FA accepting the indicated language over
{a, b}.
a. The language of all strings containing exactly two a’s.
b. The language of all strings containing at least two a’s.
c. The language of all strings that do not end with ab.
d. The language of all strings that begin or end with aa or bb.
e. The language of all strings not containing the substring aa.
f. The language of all strings in which the number of a’s is even.
g. The language of all strings in which both the number of a’s and the
number of b’s are even.
h. The language of all strings containing no more than one occurrence of
the string aa. (The string aaa contains two occurrences of aa.)
i.
The language of all strings in which every a (if there are any) is
followed immediately by bb.
j.
The language of all strings containing both bb and aba as substrings.
k. The language of all strings containing both aba and bab as substrings.
2.2.
For each of the FAs pictured in Fig. 2.43, give a simple verbal description
of the language it accepts.
2.3.
a. Draw a transition diagram for an FA that accepts the string abaa and
no other strings.
b. For a string x ∈{a, b}∗with |x| = n, how many states are required for
an FA accepting x and no other strings? For each of these states,
describe the strings that cause the FA to be in that state.
c. For a string x ∈{a, b}∗with |x| = n, how many states are required for
an FA accepting the language of all strings in {a, b}∗that begin with
x? For each of these states, describe the strings that cause the FA to be
in that state.
2.4.
Example 2.7 describes an FA accepting L3, the set of strings in {0, 1}∗that
are binary representations of integers divisible by 3. Draw a transition
diagram for an FA accepting L5.
2.5.
Suppose M = (Q, , q0, A, δ) is an FA, q is an element of Q, and x and
y are strings in ∗. Using structural induction on y, prove the formula
δ∗(q, xy) = δ∗(δ∗(q, x), y)

78
C H A P T E R 2
Finite Automata and the Languages They Accept
I
II
III
IV
V
(a)
a
b
b
a
a, b
a
b
I
II
III
IV
V
(b)
a
b
b
a
b
a
a
b
b
a
b
a
I
II
III
IV
V
(c)
a
b
a
a
a, b
VI
II
a
a
b
b
a, b
a, b
I
II
III
IV
(d)
b
b
a
a
b
b
I
a, b
b
a
b
a
a
II
III
IV
(e)
b
Figure 2.43
2.6.
Suppose M = (Q, , q0, A, δ) is an FA, q is an element of Q, and
δ(q, σ) = q for every σ ∈. Show using structural induction that for
every x ∈∗, δ∗(q, x) = q.
2.7.
Let M = (Q, , q0, A, δ) be an FA. Let M1 = (Q, , q0, R, δ), where R
is the set of states p in Q for which δ∗(p, z) ∈A for some string z. What

Exercises
79
is the relationship between the language accepted by M1 and the language
accepted by M? Prove your answer.
2.8.
Let M = (Q, , q0, A, δ) be an FA. Below are other conceivable methods
of deﬁning the extended transition function δ∗(see Deﬁnition 2.12). In
each case, determine whether it is in fact a valid deﬁnition of a function
on the set Q × ∗, and why. If it is, show using mathematical induction
that it deﬁnes the same function that Deﬁnition 2.12 does.
a. For every q ∈Q, δ∗(q, ) = q; for every y ∈∗, σ ∈, and q ∈Q,
δ∗(q, yσ) = δ∗(δ∗(q, y), σ).
b. For every q ∈Q, δ∗(q, ) = q; for every y ∈∗, σ ∈, and q ∈Q,
δ∗(q, σy) = δ∗(δ(q, σ), y).
c. For every q ∈Q, δ∗(q, ) = q; for every q ∈Q and every σ ∈,
δ∗(q, σ) = δ(q, σ); for every q ∈Q, and every x and y in ∗,
δ∗(q, xy) = δ∗(δ∗(q, x), y).
2.9.
In order to test a string for membership in a language like the one in
Example 2.1, we need to examine only the last few symbols. More
precisely, there is an integer n and a set S of strings of length n such that
for every string x of length n or greater, x is in the language if and only if
x = yz for some z ∈S.
a. Show that every language L having this property can be accepted by
an FA.
b. Show that every ﬁnite language has this property.
c. Give an example of an inﬁnite language that can be accepted by an FA
but does not have this property.
2.10.
Let M1 and M2 be the FAs pictured in Figure 2.44, accepting languages
L1 and L2, respectively.
Draw FAs accepting the following languages.
a. L1 ∪L2
b. L1 ∩L2
c. L1 −L2
2.11.
(For this problem, refer to the proof of Theorem 2.15.) Show that for
every x ∈∗and every (p, q) ∈Q, δ∗((p, q), x) = (δ∗
1(p, x), δ∗
2(q, x)).
a
b
b
b
a
a
A
B
(a)
C
M1
a, b
a
b
a
b
X
Y
Z
M2
(b)
Figure 2.44

80
C H A P T E R 2
Finite Automata and the Languages They Accept
2.12.
For each of the following languages, draw an FA accepting it.
a. {a, b}∗{a}
b. {bb, ba}∗
c. {a, b}∗{b, aa}{a, b}∗
d. {bbb, baa}∗{a}
e. {a} ∪{b}{a}∗∪{a}{b}∗{a}
f. {a, b}∗{ab, bba}
g. {b, bba}∗{a}
h. {aba, aa}∗{ba}∗
2.13.
For the FA pictured in Fig. 2.17d, show that there cannot be any other
FA with fewer states accepting the same language. (See Example 2.24,
in which the same result is established for the FA accepting the language
Ln.)
2.14.
Let z be a ﬁxed string of length n over the alphabet {a, b}. Using the
argument in Example 2.5, we can ﬁnd an FA with n + 1 states accepting
the language of all strings in {a, b}∗that end in z. The states correspond
to the n + 1 distinct preﬁxes of z. Show that there can be no FA with
fewer than n + 1 states accepting this language.
2.15.
Suppose L is a subset of {a, b}∗. If x0, x1, . . . is a sequence of distinct
strings in {a, b}∗such that for every n ≥0, xn and xn+1 are
L-distinguishable, does it follow that the strings x0, x1, . . . are pairwise
L-distinguishable? Either give a proof that it does follow, or ﬁnd an
example of a language L and strings x0, x1, . . . that represent a
counterexample.
2.16.
Let L ⊆{a, b}∗be an inﬁnite language, and for each n ≥0, let
Ln = {x ∈L | |x| = n}. Denote by s(n) the number of states an FA must
have in order to accept Ln. What is the smallest that s(n) can be if
Ln ̸= ∅? Give an example of an inﬁnite language L ⊆{a, b}∗such that for
every n satisfying Ln ̸= ∅, s(n) is this minimum number.
2.17.
Let L be the language AnBn = {anbn | n ≥0}.
a. Find two distinct strings x and y in {a, b}∗that are not
L-distinguishable.
b. Find an inﬁnite set of pairwise L-distinguishable strings.
2.18.
Let n be a positive integer and L = {x ∈{a, b}∗| |x| = n and
na(x) = nb(x)}. What is the minimum number of states in any FA that
accepts L? Give reasons for your answer.
2.19.
Let n be a positive integer, and let L be the set of all strings in Pal of
length 2n. In other words,
L = {xxr | x ∈{a, b}n}

Exercises
81
What is the minimum number of states in any FA that accepts L? Give
reasons for your answer.
2.20.
Suppose L and L1 are both languages over , and M is an FA with
alphabet . Let us say that M accepts L relative to L1 if M accepts every
string in the set L ∩L1 and rejects every string in the set L1 −L. Note
that this is not in general the same as saying that M accepts the language
L ∩L1.
Now suppose each of the languages L1, L2, . . . (subsets of ∗)
can be accepted by an FA, Li ⊆Li+1 for each i, and ∪∞
i=1Li = ∗. For
each i, let ni be the minimum number of states required to accept L
relative to Li. If there is no FA accepting L relative to Li, we say ni
is ∞.
a. Show that for each i, ni ≤ni+1.
b. Show that if the sequence ni is bounded (i.e., there is a constant C
such that ni ≤C for every i), then L can be accepted by an FA. Show
in particular that if there is some ﬁxed FA M that accepts L relative to
Li for every i, then M accepts L.
2.21.
For each of the following languages L ⊆{a, b}∗, show that the elements
of the inﬁnite set {an | n ≥0} are pairwise L-distinguishable.
a. L = {anba2n | n ≥0}
b. L = {aibjak | k > i + j}
c. L = {aibj | j = i or j = 2i}
d. L = {aibj | j is a multiple of i}
e. L = {x ∈{a, b}∗| na(x) < 2nb(x)}
f. L = {x ∈{a, b}∗| no preﬁx of x has more b’s than a’s}
g. L = {an3 | n ≥1}
h. L = {ww | w ∈{a, b}∗}
2.22.
For each of the languages in Exercise 2.21, use the pumping lemma to
show that it cannot be accepted by an FA.
2.23.
By ignoring some of the details in the statement of the pumping lemma,
we can easily get these two weaker statements.
I.
If L ⊆∗is an inﬁnite language that can be accepted by an FA, then
there are strings u, v, and w such that |v| > 0 and uviw ∈L for every
i ≥0.
II.
If L ⊆∗is an inﬁnite language that can be accepted by an FA, then
there are integers p and q such that q > 0 and for every i ≥0, L
contains a string of length p + iq.
For each language L in Exercise 2.21, decide whether statement II is
enough to show that L cannot be accepted by an FA, and explain your

82
C H A P T E R 2
Finite Automata and the Languages They Accept
answer. If statement II is not sufﬁcient, decide whether statement I is, and
explain your answer.
2.24.
Prove the following generalization of the pumping lemma, which can
sometimes make it unnecessary to break the proof into cases. If L can be
accepted by an FA, then there is an integer n such that for any x ∈L, and
any way of writing x as x = x1x2x3 with |x2| = n, there are strings u, v,
and w such that
a. x2 = uvw
b. |v| > 0
c. For every m ≥0, x1uvmwx3 ∈L
2.25.
Find a language L ⊆{a, b}∗such that, in order to prove that L cannot be
accepted by an FA, the pumping lemma is not sufﬁcient but the statement
in Exercise 2.24 is.
2.26.
The pumping lemma says that if M accepts a language L, and if n is the
number of states of M, then for every x ∈L satisfying |x| ≥n, . . .. Show
that the statement provides no information if L is ﬁnite: If M accepts a
ﬁnite language L, and n is the number of states of M, then L can contain
no strings of length n or greater.
2.27.
Describe decision algorithms to answer each of the following questions.
a. Given two FAs M1 and M2, are there any strings that are accepted by
neither?
b. Given an FA M = (Q, , q0, A, δ) and a state q ∈Q, is there an x
with |x| > 0 such that δ∗(q, x) = q?
c. Given an FA M accepting a language L, and given two strings x and
y, are x and y distinguishable with respect to L?
d. Given an FA M accepting a language L, and a string x, is x a preﬁx of
an element of L?
e. Given an FA M accepting a language L, and a string x, is x a sufﬁx of
an element of L?
f. Given an FA M accepting a language L, and a string x, is x a
substring of an element of L?
g. Given two FAs M1 and M2, is L(M1) a subset of L(M2)?
h. Given two FAs M1 and M2, is every element of L(M1) a preﬁx of an
element of L(M2)?
2.28.
Suppose L is a language over {a, b}, and there is a ﬁxed integer k such
that for every x ∈∗, xz ∈L for some string z with |z| ≤k. Does it
follow that there is an FA accepting L? Why or why not?
2.29.
For each statement below, decide whether it is true or false. If it is true,
prove it. If it is not true, give a counterexample. All parts refer to
languages over the alphabet {a, b}.
a. If L1 ⊆L2, and L1 cannot be accepted by an FA, then L2 cannot.
b. If L1 ⊆L2, and L2 cannot be accepted by an FA, then L1 cannot.

Exercises
83
c. If neither L1 nor L2 can be accepted by an FA, then L1 ∪L2 cannot.
d. If neither L1 nor L2 can be accepted by an FA, then L1 ∩L2 cannot.
e. If L cannot be accepted by an FA, then L′ cannot.
f. If L1 can be accepted by an FA and L2 cannot, then L1 ∪L2 cannot.
g. If L1 can be accepted by an FA, L2 cannot, and L1 ∩L2 can, then
L1 ∪L2 cannot.
h. If L1 can be accepted by an FA and neither L2 nor L1 ∩L2 can, then
L1 ∪L2 cannot.
i.
If each of the languages L1, L2, . . . can be accepted by an FA, then
∪∞
n=1Ln can.
j.
If none of the languages L1, L2, . . . can be accepted by an FA, and
Li ⊆Li+1 for each i, then ∪∞
n=1Ln cannot be accepted by an FA.
2.30.
†A set S of nonnegative integers is an arithmetic progression if for some
integers n and p,
S = {n + ip | i ≥0}
Let A be a subset of {a}∗, and let S = {|x| | x ∈A}.
a. Show that if S is an arithmetic progression, then A can be accepted by
an FA.
b. Show that if A can be accepted by an FA, then S is the union of a
ﬁnite number of arithmetic progressions.
2.31.
†This exercise involves languages of the form
L = {x ∈{a, b}∗| na(x) = f (nb(x))}
for some function f from the set of natural numbers to itself. Example
2.30 shows that if f is the function deﬁned by f (n) = n, then L cannot
be accepted by an FA. If f is any constant function (e.g., f (n) = 4), there
is an FA accepting L. One might ask whether this is still true when f is
not restricted quite so severely.
a. Show that if L can be accepted by an FA, the function f must be
bounded (for some integer B, f (n) ≤B for every n). (Suggestion:
suppose not, and apply the pumping lemma to strings of the form
af (n)bn.)
b. Show that if f (n) = n mod 2, then L can be accepted by an FA.
c. The function f in part (b) is an eventually periodic function; that is,
there are integers n0 and p, with p > 0, such that for every n ≥n0,
f (n) = f (n + p). Show that if f is any eventually periodic function,
L can be accepted by an FA.
d. Show that if L can be accepted by an FA, then f must be eventually
periodic. (Suggestion: as in part (a), ﬁnd a class of strings to which
you can apply the pumping lemma.)
2.32.
For which languages L ⊆{a, b}∗does the equivalence relation IL have
exactly one equivalence class?

84
C H A P T E R 2
Finite Automata and the Languages They Accept
2.33.
Let x be a string of length n in {a, b}∗, and let L = {x}. How many
equivalence classes does IL have? Describe them.
2.34.
Show that if L ⊆∗, and there is a string x ∈∗that is not a preﬁx of
an element of L, then the set of all strings that are not preﬁxes of
elements of L is an inﬁnite set that is one of the equivalence classes
of IL.
2.35.
Let L ⊆∗be any language. Show that if [] (the equivalence class of
IL containing ) is not {}, then it is inﬁnite.
2.36.
For a certain language L ⊆{a, b}∗, IL has exactly four equivalence
classes. They are [], [a], [ab], and [b]. It is also true that the three
strings a, aa, and abb are all equivalent, and that the two strings b and
aba are equivalent. Finally, ab ∈L, but  and a are not in L, and b is
not even a preﬁx of any element of L. Draw an FA accepting L.
2.37.
Suppose L ⊆{a, b}∗and IL has three equivalence classes. Suppose they
can be described as the three sets [a], [aa], and [aaa], and also as the
three sets [b], [bb], and [bbb]. How many possibilities are there for the
language L? For each one, draw a transition diagram for an FA
accepting it.
2.38.
In each part, ﬁnd every possible language L ⊆{a, b}∗for which the
equivalence classes of IL are the three given sets.
a. {a, b}∗{b}, {a, b}∗{ba}, {, a} ∪{a, b}∗{aa}
b. ({a, b}{a}∗{b})∗, ({a, b}{a}∗{b})∗{a}{a}∗, ({a, b}∗{a}∗{b})∗{b}{a}∗
c. {}, {a}({b} ∪{a}{a}∗{b})∗, {b}({a} ∪{b}{b}∗{a})∗
2.39.
In Example 2.37, if the language is changed to {anbn | n > 0}, so that it
does not contain , are there any changes in the partition of {a, b}∗
corresponding to IL? Explain.
2.40.
Consider the language L = AEqB = {x ∈{a, b}∗| na(x) = nb(x)}.
a. Show that if na(x) −nb(x) = na(y) −nb(y), then x IL y.
b. Show that if na(x) −nb(x) ̸= na(y) −nb(y), then x and y are
L-distinguishable.
c. Describe all the equivalence classes of IL.
2.41.
Let L ⊆∗be a language, and let L1 be the set of preﬁxes of elements of
L. What is the relationship, if any, between the two partitions of ∗
corresponding to the equivalence relations IL and IL1, respectively?
Explain.
2.42.
a. List all the subsets A of {a, b}∗having the property that for some
language L ⊆{a, b}∗for which IL has exactly two equivalence classes,
A = [].
b. For each set A that is one of your answers to (a), how many distinct
languages L are there such that IL has two equivalence classes and []
is A?
2.43.
Let L = {ww | w ∈{a, b}∗}. Describe all the equivalence classes of IL.

Exercises
85
2.44.
Let L be the language Balanced of balanced strings of parentheses.
Describe all the equivalence classes of IL.
2.45.
†Let L be the language of all fully parenthesized algebraic expressions
involving the operator + and the identiﬁer a. (L can be deﬁned
recursively by saying that a ∈L and (x + y) ∈L for every x and y in L.)
Describe all the equivalence classes of IL.
2.46.
†For a language L over , and two strings x and y in ∗that are
L-distinguishable, let
dL,x,y = min{|z| | z distinguishes x and y with respect to L}
a. For the language L = {x ∈{a, b}∗| x ends in aba}, ﬁnd the maximum
of the numbers dL,x,y over all possible pairs of L-distinguishable
strings x and y.
b. If L is the language of balanced strings of parentheses, and if x and y
are L-distinguishable strings with |x| = m and |y| = n, ﬁnd an upper
bound involving m and n on the numbers dL,x,y.
2.47.
For an arbitrary string x ∈{a, b}∗, denote by x∼the string obtained by
replacing all a’s by b’s and vice versa. For example, ∼=  and
(abb)∼= baa.
a. Deﬁne
L = {xx∼| x ∈{a, b}∗}
Determine the equivalence classes of IL.
b. Deﬁne
L1 = {xy | x ∈{a, b}∗and y is either x or x∼}
Determine the equivalence classes of IL1.
2.48.
†Let L = {x ∈{a, b}∗| nb(x) is an integer multiple of na(x)}. Determine
the equivalence classes of IL.
2.49.
Let L be a language over . We know that IL is a right-invariant
equivalence relation; i.e., for any x and y in ∗and any a ∈, if x IL y,
then xa IL ya. It follows from Theorem 2.36 that if the set of equivalence
classes of IL is ﬁnite, L can be accepted by an FA, and in this case L is
the union of some (zero or more) of these equivalence classes. Show that
if R is any right-invariant equivalence relation such that the set of
equivalence classes of R is ﬁnite and L is the union of some of the
equivalence classes of R, then L can be accepted by an FA.
2.50.
†If P is a partition of {a, b}∗(a collection of pairwise disjoint subsets
whose union is {a, b}∗), then there is an equivalence relation R on {a, b}∗
whose equivalence classes are precisely the subsets in P . Let us say that
P is right-invariant if the resulting equivalence relation is.
a. Show that for a subset S of {a, b}∗, S is one of the subsets of some
right-invariant partition (not necessarily a ﬁnite partition) of {a, b}∗if

86
C H A P T E R 2
Finite Automata and the Languages They Accept
and only if the following condition is satisﬁed: for every x, y ∈S, and
every z ∈{a, b}∗, xz and yz are either both in S or both not in S.
b. To what simpler condition does this one reduce in the case where S is
a ﬁnite set?
c. Show that if a ﬁnite set S satisﬁes this condition, then there is a ﬁnite
right-invariant partition having S as one of its subsets.
d. For an arbitrary set S satisfying the condition in part (a), there might
be no ﬁnite right-invariant partition having S as one of its subsets.
Characterize those sets S for which there is.
2.51.
For two languages L1 and L2 over , we deﬁne the quotient of L1 and
L2 to be the language
L1/L2 = {x | for some y ∈L2, xy ∈L1}
Show that if L1 can be accepted by an FA and L2 is any language, then
L1/L2 can be accepted by an FA.
2.52.
Suppose L is a language over , and x1, x2, . . . , xn are strings that are
pairwise L-distinguishable. How many distinct strings are necessary in
order to distinguish between the xi’s? In other words, what is the smallest
number k such that for some set {z1, z2, . . . , zk}, any two distinct xi’s are
distinguished, relative to L, by some zl? Prove your answer. (Here is a
way of thinking about the question that may make it easier. Think of the
xi’s as points on a piece of paper, and think of the zl’s as cans of paint,
each zl representing a different primary color. Saying that zl distinguishes
xi and xj means that one of those two points is colored with that primary
color and the other isn’t. We allow a single point to have more than one
primary color applied to it, and we assume that two distinct combinations
of primary colors produce different resulting colors. Then the question is,
how many different primary colors are needed in order to color the points
so that no two points end up the same color?)
2.53.
Suppose M = (Q, , q0, A, δ) is an FA accepting L. We know that if
p, q ∈Q and p ̸≡q, then there is a string z such that exactly one of the
two states δ∗(p, z) and δ∗(q, z) is in A. Show that there is an integer n
such that for every p and q with p ̸≡q, such a z can be found whose
length is no greater than n, and say what n is.
2.54.
Show that L can be accepted by an FA if and only if there is an integer n
such that, for every pair of L-distinguishable strings, the two strings can
be distinguished by a string of length ≤n. (Use the two previous
exercises.)
2.55.
For each of the FAs pictured in Fig. 2.45, use the minimization algorithm
described in Section 2.6 to ﬁnd a minimum-state FA recognizing the same
language. (It’s possible that the given FA may already be minimal.)
2.56.
Suppose that in applying the minimization algorithm in Section 2.6, we
establish some ﬁxed order in which to process the pairs, and we follow the
same order on each pass.

Exercises
87
(e)
(c)
(d)
( f )
(b)
a
b
b
b
1
4
6
2
5
3
a
a
a
b
b
a
(a)
b
a
b
b
a
a
3
4
5
1
2
a
a
a
b
b
b
a
b
b
b
1
3
6
7
4
5
2
a
a
a
a
a
a
b
b
b
b
a
b
1
3
7
6
5
4
2
a
b
a
b
a
a
b
b
a
b
a
b
a
b
1
3
7
6
5
4
2
a
b
a
a
a
b
b
b
a
b
b
a
a
b
1
3
7
6
5
4
a
b
a
b
a
a
b
b
a
b
a
b
2
Figure 2.45

88
C H A P T E R 2
Finite Automata and the Languages They Accept
2
9
6
4
3
b
b
b
b
b
a
a
a
a a
a
5
a
a
a
b
b
b
b
1
( g)
8
7
Figure 2.45
Continued
a. What is the maximum number of passes that might be required?
Describe an FA, and an ordering of the pairs, that would require this
number.
b. Is there always a ﬁxed order (depending on M) that would guarantee
that no pairs are marked after the ﬁrst pass, so that the algorithm
terminates after two passes?
2.57.
Each case below deﬁnes a language over {a, b}. In each case, decide
whether the language can be accepted by an FA, and prove that your
answer is correct.
a. The set of all strings x beginning with a nonnull string of the
form ww.
b. The set of all strings x containing some nonnull substring of the
form ww.
c. The set of all strings x having some nonnull substring of the form
www. (You may assume the following fact: there are arbitrarily long
strings in {a, b}∗that do not contain any nonnull substring of the form
www.)
d. The set of odd-length strings with middle symbol a.
e. The set of even-length strings of length at least 2 with the two middle
symbols equal.
f. The set of strings of the form xyx for some x with |x| ≥1.
g. The set of non-palindromes.
h. The set of strings in which the number of a’s is a perfect square.
i.
The set of strings having the property that in every preﬁx, the number
of a’s and the number of b’s differ by no more than 2.

Exercises
89
j.
The set of strings having the property that in some preﬁx, the number
of a’s is 3 more than the number of b’s.
k. The set of strings in which the number of a’s and the number of b’s
are both divisible by 5.
l.
The set of strings x for which there is an integer k > 1 (possibly
depending on x) such that the number of a’s in x and the number of
b’s in x are both divisible by k.
m. (Assuming that L can be accepted by an FA), Max(L) = {x ∈L |
there is no nonnull string y so that xy ∈L}.
n. (Assuming that L can be accepted by an FA), Min(L) = {x ∈L | no
preﬁx of x other than x itself is in L}.
2.58.
Find an example of a language L ⊆{a, b}∗such that L∗cannot be
accepted by an FA.
2.59.
Find an example of a language L over {a, b} such that L cannot be
accepted by an FA but L∗can.
2.60.
Find an example of a language L over {a, b} such that L cannot be
accepted by an FA but LL can.
2.61.
†Show that if L is any language over a one-symbol alphabet, then L∗can
be accepted by an FA.
2.62.
†Consider the two FAs in Fig. 2.46.
If you examine them closely you can see that they are really identical,
except that the states have different names: state p corresponds to state A,
q corresponds to B, and r corresponds to C. Let us describe this
correspondence by the “relabeling function” i; that is, i(p) = A,
i(q) = B, i(r) = C. What does it mean to say that under this
correspondence, the two FAs are “really identical”? It means several
things: First, the initial states correspond to each other; second, a state is
an accepting state if and only if the corresponding state is; and ﬁnally, the
transitions among the states of the ﬁrst FA are the same as those among
the corresponding states of the other. For example, if δ1 and δ2 are the
transition functions, then
δ1(p, a) = p and δ2(i(p), a) = i(p)
δ1(p, b) = q and δ2(i(p), b) = i(q)
p
a
q
r
a
b
a
b
b
a
A
C
AR
a
a
b
B
b
b
Figure 2.46

90
C H A P T E R 2
Finite Automata and the Languages They Accept
These formulas can be rewritten
δ2(i(p), a) = i(δ1(p, a)) and δ2(i(p), b) = i(δ1(p, b))
and these and all the other relevant formulas can be summarized by the
general formula
δ2(i(s), σ) = i(δ1(s, σ)) for every state s and alphabet symbol σ
In general, if M1 = (Q1, , q1, A1, δ1) and M2 = (Q2, , q2, A2, δ2) are
FAs, and i : Q1 →Q2 is a bijection (i.e., one-to-one and onto), we say
that i is an isomorphism from M1 to M2 if these conditions are satisﬁed:
i.
i(q1) = q2
ii.
for every q ∈Q1, i(q) ∈A2 if and only if q ∈A1
iii.
for every q ∈Q1 and every σ ∈, i(δ1(q, σ)) = δ2(i(q), σ)
and we say M1 is isomorphic to M2 if there is an isomorphism from M1
to M2. This is simply a precise way of saying that M1 and M2 are
“essentially the same”.
a. Show that the relation ∼on the set of FAs over , deﬁned by
M1 ∼M2 if M1 is isomorphic to M2, is an equivalence relation.
b. Show that if i is an isomorphism from M1 to M2 (notation as above),
then for every q ∈Q1 and x ∈∗,
i(δ∗
1(q, x)) = δ∗
2(i(q), x)
c. Show that two isomorphic FAs accept the same language.
d. How many one-state FAs over the alphabet {a, b} are there, no two of
which are isomorphic?
e. How many pairwise nonisomorphic two-state FAs over {a, b} are there,
in which both states are reachable from the initial state and at least one
state is accepting?
f. How many distinct languages are accepted by the FAs in the previous
part?
g. Show that the FAs described by these two transition tables are
isomorphic. The states are 1–6 in the ﬁrst, A–F in the second; the
initial states are 1 and A, respectively; the accepting states in the ﬁrst
FA are 5 and 6, and D and E in the second.
q
δ1(q, a)
δ1(q, b)
1
3
5
2
4
2
3
1
6
4
4
3
5
2
4
6
3
4
q
δ2(q, a)
δ2(q, b)
A
B
E
B
A
D
C
C
B
D
B
C
E
F
C
F
C
F

Exercises
91
2.63.
Suppose that M1 = (Q1, , q1, A1, δ1) and M2 = (Q2, , q2, A2, δ2) are
both FAs accepting the language L, and that both have as few states as
possible. Show that M1 and M2 are isomorphic (see Exercise 2.62). Note
that in both cases, the sets Lq forming the partition of ∗are precisely the
equivalence classes of IL. This tells you how to come up with a bijection
from Q1 to Q2. What you must do next is to show that the other
conditions of an isomorphism are satisﬁed.
2.64.
Use Exercise 2.63 to describe another decision algorithm to answer the
question “Given two FAs, do they accept the same language?”

92
C
H
A
P
T
E
R
3
Regular Expressions,
Nondeterminism, and Kleene’s
Theorem
A
simple way of describing a language is to describe a ﬁnite automaton that
accepts it. As with the models of computation we will study later, an alter-
native approach is to use some appropriate notation to describe how the strings of
the language can be generated. Languages that can be accepted by ﬁnite automata
are the same as regular languages, which can be represented by formulas called
regular expressions involving the operations of union, concatenation, and Kleene
star. In the case of ﬁnite automata, demonstrating this equivalence (by proving the
two parts of Kleene’s theorem) is simpliﬁed considerably by introducing nondeter-
minism, which will also play a part in the computational models we will study later.
Here, although allowing nondeterminism seems at ﬁrst to enhance the accepting
power of these devices, we will see that it can be eliminated.
3.1 REGULAR LANGUAGES AND REGULAR
EXPRESSIONS
Three of the languages over {a, b} that we considered in Chapter 2 are L1, the lan-
guage of strings ending in aa; L2, the language of strings containing either the sub-
string ab or the substring bba; and L3, the language {aa, aab}∗{b}. Like L3, both
L1 and L2 can be expressed by a formula involving the operations of union, con-
catenation, and Kleene ∗: L1 is {a, b}∗{aa} and L2 is {a, b}∗({ab} ∪{bba}){a, b}∗.
Languages that have formulas like these are called regular languages. In this sec-
tion we give a recursive deﬁnition of the set of regular languages over an alphabet
, and later in this chapter we show that these are precisely the languages that can
be accepted by a ﬁnite automaton.

3.1
Regular Languages and Regular Expressions
93
Deﬁnition 3.1
Regular Languages over an Alphabet 
If  is an alphabet, the set R of regular languages over  is deﬁned as
follows.
1. The language ∅is an element of R, and for every a ∈, the language {a} is
in R.
2. For any two languages L1 and L2 in R, the three languages
L1 ∪L2, L1L2,
and L∗
1
are elements of R.
The language {} is a regular language over , because ∅∗= {}. If  =
{a, b}, then L1 = {a, b}∗{aa} can be obtained from the deﬁnition by starting with
the two languages {a} and {b} and then using the recursive statement in the deﬁni-
tion four times: The language {a, b} is the union {a} ∪{b}; {aa} is the concatenation
{a}{a}; {a, b}∗is obtained by applying the Kleene star operation to {a, b}; and the
ﬁnal language is the concatenation of {a, b}∗and {aa}.
A regular language over  has an explicit formula. A regular expression for
the language is a slightly more user-friendly formula. The only differences are that
in a regular expression, parentheses replace {} and are omitted whenever the rules
of precedence allow it, and the union symbol ∪is replaced by +. Here are a few
examples (see Example 3.5 for a discussion of the last one):
Regular Language
Corresponding Regular Expression
∅
∅
{}

{a, b}∗
(a + b)∗
{aab}∗{a, ab}
(aab)∗(a + ab)
({aa, bb} ∪{ab, ba}{aa, bb}∗{ab, ba})∗
(aa + bb + (ab + ba)(aa + bb)∗(ab + ba))∗
When we write a regular expression like  or aab, which contains neither
+ nor ∗and corresponds to a one-element language, the regular expression looks
just like the string it represents. A more general regular expression involving one
or both of these operations can’t be mistaken for a string; we can think of it
as representing the general form of strings in the language. A regular expression
describes a regular language, and a regular language can be described by a regular
expression.
We say that two regular expressions are equal if the languages they describe
are equal. Some regular-expression identities are more obvious than others. The
formula
(a∗b∗)∗= (a + b)∗

94
C H A P T E R 3
Regular Expressions, Nondeterminism, and Kleene’s Theorem
is true because the language corresponding to a∗b∗contains both a and b. The
formula
(a + b)∗ab(a + b)∗+ b∗a∗= (a + b)∗
is true because the ﬁrst term on the left side corresponds to the strings in {a, b}∗
that contain the substring ab and the second term, b∗a∗, corresponds to the strings
that don’t.
EXAMPLE 3.2
The Language of Strings in {a, b}∗with an Odd Number of a’s
A string with an odd number of a’s has at least one a, and the additional a’s can be grouped
into pairs. There can be arbitrarily many b’s before the ﬁrst a, between any two consecutive
a’s, and after the last a. The expression
b∗ab∗(ab∗a)∗b∗
is not correct, because it doesn’t allow b’s between the second a in one of the repeating
pairs ab∗a and the ﬁrst a in the next pair. One correct regular expression describing the
language is
b∗ab∗(ab∗ab∗)∗
The expression
b∗a(b∗ab∗ab∗)∗
is also not correct, because it doesn’t allow strings with just one a to end with b, and the
expression
b∗a(b∗ab∗a)∗b∗
corrects the mistake. Another correct expression is
b∗a(b + ab∗a)∗
All of these could also be written with the single a on the right, as in
(b + ab∗a)∗ab∗
EXAMPLE 3.3
The Language of Strings in {a, b}∗Ending with b and Not
Containing aa
If a string does not contain the substring aa, then every a in the string either is followed
immediately by b or is the last symbol in the string. If the string ends with b, then every
a is followed immediately by b. Therefore, every string in the language L of strings that
end with b and do not contain aa matches the regular expression (b + ab)∗. This regular
expression does not describe L, however, because it allows the null string, which does not
end with b. At least one of the two strings b and ab must occur, and so a regular expression
for L is
(b + ab)∗(b + ab)

3.1
Regular Languages and Regular Expressions
95
EXAMPLE 3.4
Strings in {a,b}∗in Which Both the Number of a’s and the Number
of b’s Are Even
One of the regular expressions given in Example 3.2, b∗a(b + ab∗a)∗, describes the language
of strings with an odd number of a’s, and the ﬁnal portion of it, (b + ab∗a)∗, describes the
language of strings with an even number of a’s. We can interpret the two terms inside the
parentheses as representing the two possible ways of adding to the string without changing
the parity (the evenness or oddness) of the number of a’s: adding a string that has no a’s,
and adding a string that has two a’s. Every string x with an even number of a’s has a preﬁx
matching one of these two terms, and x can be decomposed into nonoverlapping substrings
that match one of these terms.
Let L be the subset of {a, b}∗containing the strings x for which both na(x) and nb(x)
are even. Every element of L has even length. We can use the same approach to ﬁnd a
regular expression for L, but this time it’s sufﬁcient to consider substrings of even length.
The easiest way to add a string of even length without changing the parity of the number
of a’s or the number of b’s is to add aa or bb. If a nonnull string x ∈L does not begin
with one of these, then it starts with either ab or ba, and the shortest substring following
this that restores the evenness of na and nb must also end with ab or ba, because its length
is even and strings of the form aa and bb don’t change the parity of na or nb.
The conclusion is that every nonnull string in L has a preﬁx that matches the regular
expression
aa + bb + (ab + ba)(aa + bb)∗(ab + ba)
and that a regular expression for L is
(aa + bb + (ab + ba)(aa + bb)∗(ab + ba))∗
EXAMPLE 3.5
Regular Expressions and Programming Languages
In Example 2.9 we built a ﬁnite automaton to carry out a very simple version of lexical
analysis: breaking up a part of a computer program into tokens, which are the basic building
blocks from which the expressions or statements are constructed. The last two sections of this
chapter are devoted to proving that ﬁnite automata can accept exactly the same languages
that regular expressions can describe, and in this example we construct regular expressions
for two classes of tokens.
An identiﬁer in the C programming language is a string of length 1 or more that
contains only letters, digits, and underscores (“ ”) and does not begin with a digit. If we
use the abbreviations l for “letter,” either uppercase or lowercase, and d for “digit,” then l
stands for the regular expression
a + b + c + . . . + z + A + B + . . . + Z
and d for the regular expression
0 + 1 + 2 + · · · + 9
(which has nothing to do with the integer 45), and a regular expression for the language of
C identiﬁers is
(l + )(l + d + )∗

96
C H A P T E R 3
Regular Expressions, Nondeterminism, and Kleene’s Theorem
Next we look for a regular expression to describe the language of numeric “literals,”
which typically includes strings such as 14, +1, −12, 14.3, −.99, 16., 3E14, −1.00E2,
4.1E−1, and .3E+2. Let us assume that such an expression may or may not begin with a
plus sign or a minus sign; it will contain one or more decimal digits, and possibly a decimal
point, and it may or may not end with a subexpression starting with E. If there is such a
subexpression, the portion after E may or may not begin with a sign and will contain one
or more decimal digits.
Our regular expression will involve the abbreviations d and l introduced above, and
we will use s to stand for “sign” (either  or a plus sign or a minus sign) and p for a
decimal point. It is not hard to convince yourself that a regular expression covering all the
possibilities is
s(dd∗( + pd∗) + pdd∗)( + Esdd∗)
In some programming languages, numeric literals are not allowed to contain a decimal
point unless there is at least one digit on both sides. A regular expression incorporating this
requirement is
sdd∗( + pdd∗)( + Esdd∗)
Other tokens in a high-level language can also be described by regular expressions,
in most cases even simpler than the ones in this example. Lexical analysis is the ﬁrst
phase in compiling a high-level-language program. There are programs called lexical-
analyzer generators; the input provided to such a program is a set of regular expres-
sions describing the structure of tokens, and the output produced by the program is a
software version of an FA that can be incorporated as a token-recognizing module in a
compiler. One of the most widely used lexical-analyzer generators is lex, a tool pro-
vided in the Unix operating system. It can be used in many situations that require the
processing of structured input, but it is often used together with yacc, another Unix
tool. The lexical analyzer produced by lex creates a string of tokens; and the parser
produced by yacc, on the basis of grammar rules provided as input, is able to deter-
mine the syntactic structure of the token string. (yacc stands for yet another compiler
compiler.)
Regular expressions come up in Unix in other ways as well. The Unix text editor
allows the user to specify a regular expression and searches for patterns in the text that
match it. Other commands such as grep (global regular expression print) and egrep
(extended global regular expression print) allow a user to search a ﬁle for strings that match
a speciﬁed regular expression.
3.2 NONDETERMINISTIC
FINITE AUTOMATA
The goal in the rest this chapter is to prove that regular languages, deﬁned in
Section 3.1, are precisely the languages accepted by ﬁnite automata. In order
to do this, we will introduce a more general “device,” a nondeterministic ﬁnite
automaton. The advantage of this approach is that it’s much easier to start with
an arbitrary regular expression and draw a transition diagram for something

3.2
Nondeterministic Finite Automata
97
that accepts the corresponding language and has an obvious connection to the
regular expression. The only problem is that the something might not be a
ﬁnite automaton, although it has a superﬁcial resemblance to one, and we have
to ﬁgure out how to interpret it in order to think of it as a physical device
at all.
EXAMPLE 3.6
Accepting the Language {aa,aab}∗{b})
There is a close resemblance between the diagram in Figure 3.7 and the regular expression
(aa + aab)∗b. The top loop corresponds to aa, the bottom one corresponds to aab, and
the remaining b-transition corresponds to the last b in the regular expression. To the extent
that we think of it as a transition diagram like that of an FA, its resemblance to the regular
expression suggests that the string aaaabaab, for example, should be accepted, because it
allows us to start at q0, take the top loop once, the bottom loop once, the top loop again,
and ﬁnish up with the transition to the accepting state.
This diagram, however, is not the transition diagram for an FA, because there are
three transitions from q0 and fewer than two from several other states. The input string
aaaabaab allows us to reach the accepting state, but it also allows us to follow, or at least
start to follow, other paths that don’t result in acceptance. We can imagine an idealized
“device” that would work by using the input symbols to follow paths shown in the diagram,
making arbitrary choices at certain points. It has to be nondeterministic, in the sense that
the path it follows is not determined by the input string. (If the ﬁrst input symbol is a, it
chooses whether to start up the top path or down the bottom one.) Even if there were a
way to build a physical device that acted like this, it wouldn’t accept this language in the
same way that an FA could. Suppose we watched it process a string x. If it ended up in
the accepting state at the end, we could say that x was in the language; if it didn’t, all
we could say for sure is that the moves it chose to make did not lead to the accepting
state.
b
a
a
a
b
a
q4
q1
q0
q2
q3
Figure 3.7
Using nondeterminism to accept
{aa,aab}∗{b}.

98
C H A P T E R 3
Regular Expressions, Nondeterminism, and Kleene’s Theorem
Allowing nondeterminism, by relaxing the rules for an FA, makes it easy to draw
diagrams corresponding to regular expressions. However, we should no longer think of
the diagram as representing an explicit algorithm for accepting the language, because an
algorithm refers to a sequence of steps that are determined by the input and would be the
same no matter how many times the algorithm was executed on the same input.
If the diagram doesn’t represent an explicit accepting algorithm, what good is it? One
way to answer this is to think of the diagram as describing a number of different sequences
of steps that might be followed. We can visualize these sequences for the input string
aaaabaab by drawing a computation tree, pictured in Figure 3.8.
A level of the tree corresponds to the input (the preﬁx of the entire input string)
read so far, and the states appearing on this level are those in which the device could be,
depending on the choices it has made so far. Two paths in the tree, such as the one that
q1
q0
a
a
a
a
q4
q2
q3
q1
q0
q0
a
a
a
b
b
a
q2
q3
q4
q1
q0
q0
a
a
a
b
q1
b
a
q2
q3
Figure 3.8
The computation tree for Figure 3.7 and
the input string aaaabaab.

3.2
Nondeterministic Finite Automata
99
starts by treating the initial a as the ﬁrst symbol of aab, terminate prematurely, because
the next input symbol does not allow a move from the current state. One path, which
corresponds to interpreting the input string as (aa)(aab)(aab), allows all the input to be
read and ends up in a nonaccepting state. The path in which the device makes the “correct”
choice at each step ends up at the accepting state when all the input symbols have been
read.
If we had the transition diagram in Figure 3.7 and were trying to use it to accept
the language, we could systematically keep track of the current sequence of steps, and
use a backtracking strategy whenever we couldn’t proceed any further or ﬁnished in a
nonaccepting state. The result would, in effect, be to search the computation tree using
a depth-ﬁrst search. In the next section we will see how to develop an ordinary ﬁnite
automaton that effectively executes a breadth-ﬁrst search of the tree, by keeping track after
each input symbol of all the possible states the various sequences of steps could have led
us to.
EXAMPLE 3.9
Accepting the Language {aab}∗{a,aba}∗
In this example we consider the regular expression (aab)∗(a + aba)∗. The techniques
of Example 3.6 don’t provide a simple way to draw a transition diagram related to the
regular expression, but Figure 3.10 illustrates another type of nondeterminism that
does.
The new feature is a “-transition,” which allows the device to change state with no
input. If the input a is received in state 0, there are three options: take the transition from
state 0 corresponding to the a in aab; move to state 3 and take the transition corresponding
to a; and move to state 3 and take the transition corresponding to the a in aba. The diagram
shows two a-transitions from state 3, but because of the -transition, we would have a
choice of moves even if there were only one.
Figure 3.11 shows a computation tree illustrating the possible sequences of moves for
the input string aababa. The -transition is drawn as a horizontal arrow, so that as in the
previous example, a new level of the tree corresponds to a new input symbol.
a
b
a
b
a
a
4
0
5
Λ
3
a
1
2
Figure 3.10
Using nondeterminism to accept
{aab}∗{a, aba}∗.

100
C H A P T E R 3
Regular Expressions, Nondeterminism, and Kleene’s Theorem
a
0
1
a
2
b
a
a
0
b
5
a
3
3
4
a
a
3
3
a
3
4
Λ
Λ
3
b
5
a
a
4
3
Figure 3.11
The computation tree for Figure 3.10 and
the input string aababa.
The string is accepted, because the device can choose to take the ﬁrst loop, execute the
-transition, and take the longer loop from state 3.
The transition diagrams in our ﬁrst two examples show four of the ﬁve ingre-
dients of an ordinary ﬁnite automaton. The one that must be handled differently is
the transition function δ. For a state q and an alphabet symbol σ, it is no longer
correct to say that δ(q, σ) is a state: There may be no transitions from state q on
input σ, or one, or more than one. There may also be -transitions. We can incor-
porate both of these features by making two changes: ﬁrst, enlarging the domain
of δ to include ordered pairs (q, ) as well as the pairs in Q × ; and second,
making the values of δ sets of states instead of individual states.
Deﬁnition 3.12
A Nondeterministic Finite Automaton
A nondeterministic ﬁnite automaton (NFA) is a 5-tuple (Q, , q0, A, δ),
where
Q is a ﬁnite set of states;
 is a ﬁnite input alphabet;

3.2
Nondeterministic Finite Automata
101
q0 ∈Q is the initial state;
A ⊆Q is the set of accepting states;
δ : Q × ( ∪{}) →2Q is the transition function.
For every element q of Q and every element σ of  ∪{}, we interpret
δ(q, σ) as the set of states to which the FA can move, if it is in state q and
receives the input σ, or, if σ = , the set of states other than q to which
the NFA can move from state q without receiving any input symbol.
In Example 3.9, for example, δ(0, a) = {1}, δ(0, ) = {3}, δ(0, b) = ∅, and
δ(0, a) = {3, 4}.
In the case of an NFA M = (Q, , q0, A, δ), we want δ∗(q, x) to tell us all the
states M can get to by starting at q and using the symbols in the string x. We can
still deﬁne the function δ∗recursively, but the mathematical notation required to
express this precisely is a little more involved, particularly if M has -transitions.
In order to deﬁne δ∗(q, xσ), where x ∈∗and σ ∈, we start by considering
δ∗(q, x), just as in the simple case of an ordinary FA. This is now a set of states,
and for each state p in this set, δ(p, σ) is itself a set. In order to include all the
possibilities, we need to consider

{δ(p, a) | p ∈δ∗(q, x)}
Finally, we must keep in mind that in the case of -transitions, “using all the
symbols in the string x” really means using all the symbols in x and perhaps -
transitions where they are possible. In the recursive step of the deﬁnition of δ∗,
once we have the union we have just described, we must consider all the additional
states we might be able to reach from elements of this union, using nothing but
-transitions.
You can probably see at this point how the following deﬁnition will be helpful
in our discussion.
Deﬁnition 3.13
The -Closure of a Set of States
Suppose M = (Q, , q0, A, δ) is an NFA, and S ⊆Q is a set of states.
The -closure of S is the set (S) that can be deﬁned recursively as
follows.
1. S ⊆(S).
2. For every q ∈(S), δ(q, ) ⊆(S).
In exactly the same way as in Example 1.21, we can convert the recursive
deﬁnition of (S) into an algorithm for evaluating it, as follows.

102
C H A P T E R 3
Regular Expressions, Nondeterminism, and Kleene’s Theorem
Algorithm to Calculate (S)
Initialize T to be S. Make a sequence of passes,
in each pass considering every q ∈T and adding to T every state in δ(q, ) that
is not already an element. Stop after the ﬁrst pass in which T is not changed. The
ﬁnal value of T is (S).
■
A state is in (S) if it is an element of S or can be reached from an element
of S using one or more -transitions.
With the help of Deﬁnition 3.13 we can now deﬁne the extended transition
function δ∗for a nondeterministic ﬁnite automaton.
Deﬁnition 3.14
The Extended Transition Function δδδ∗∗∗for an
NFA, and the Deﬁnition of Acceptance
Let M = (Q, , q0, A, δ) be an NFA. We deﬁne the extended transition
function
δ∗: Q × ∗→2Q
as follows:
1. For every q ∈Q, δ∗(q, ) = ({q}).
2. For every q ∈Q, every y ∈∗, and every σ ∈,
δ∗(q, yσ) = 

{δ(p, σ) | p ∈δ∗(q, y)}

A string x ∈∗is accepted by M if δ∗(q0, x) ∩A ̸= ∅. The language
L(M) accepted by M is the set of all strings accepted by M.
For the NFA in Example 3.9, which has only one -transition, it is easy
to evaluate δ∗(aababa) by looking at the computation tree in Figure 3.11. The
two states on the ﬁrst level of the diagram are 0 and 3, the elements of the
-closure of {0}. The states on the third level, for example, are 2, 3, and 4,
because δ∗(0, aa) = {2, 3, 4}. When we apply the recursive part of the deﬁnition
to evaluate δ∗(0, aab), we ﬁrst evaluate

{δ(p, b) | p ∈{2, 3, 4}} = δ(2, b) ∪δ(3, b) ∪δ(4, b) = {0} ∪∅∪{5}
= {0, 5}
and then we compute the -closure of this set, which contains the additional
element 3.
For an NFA M with no -transitions, both statements in the deﬁnition can be
simpliﬁed, because for every subset S of Q, (S) = S.
We illustrate the deﬁnition once more in a slightly more extended example.
EXAMPLE 3.15
Applying the Deﬁnitions of (S ) and δ∗
We start by evaluating the -closure of the set {v} in the NFA whose transition diagram is
shown in Figure 3.16. When we apply the algorithm derived from Deﬁnition 3.13, after one

3.2
Nondeterministic Finite Automata
103
w
b
Λ
q0
a
b
t
a
b
a
u
v
Λ
Λ
a
b
s
r
p
Λ
Λ
Figure 3.16
Evaluating the extended transition function when there are
-transitions.
pass T is {v, w}, after two passes it is {v, w, q0}, after three passes it is {v, w, q0, p, t}, and
during the next pass it remains unchanged. The set ({s}) is therefore {v, w, q0, p, t}.
If we want to apply the deﬁnition of δ∗to evaluate δ∗(q0, aba), the easiest way is to
begin with , the shortest preﬁx of aba, and work our way up one symbol at a time.
δ∗(q0, ) = ({q0})
= {q0, p, t}
δ∗(q0, a) = 

{δ(k, a) | k ∈δ∗(q0, )}

=  (δ(q0, a) ∪δ(p, a) ∪δ(t, a))
=  (∅∪{p} ∪{u})
= ({p, u})
= {p, u}
δ∗(q0, ab) = 

{δ(k, b) | k ∈{p, u}}

= (δ(p, b) ∪δ(u, b))
= ({r, v})
= {r, v, w, q0, p, t}
δ∗(q0, aba) = 

{δ(k, a) | k ∈{r, v, w, q0, p, t}}

= (δ(r, a) ∪δ(v, a) ∪δ(w, a) ∪δ(q0, a) ∪δ(p, a) ∪δ(t, a))
= ({s} ∪{v} ∪∅∪∅∪{p} ∪{u})
= ({s, v, p, u})
= {s, v, p, u, w, q0, t}

104
C H A P T E R 3
Regular Expressions, Nondeterminism, and Kleene’s Theorem
The evaluation of ({r, v}) is very similar to that of ({v}), since there are no -transitions
from r, and the evaluation of ({s, v, p, u}) is also similar. Because δ∗(q0, aba) contains
the accepting state w, the string aba is accepted.
A state r is an element of δ∗(q, x) if in the transition diagram there is a path from q to
r, in which there are transitions for every symbol in x and the next transition at each step
corresponds either to the next symbol in x or to . In simple examples, including this one,
you may feel that it’s easier to evaluate δ∗by looking at the diagram and determining by
inspection what states you can get to. One reason for having a precise recursive deﬁnition of
δ∗and a systematic algorithm for evaluating it is that otherwise it’s easy to overlook things.
3.3 THE NONDETERMINISM IN AN NFA CAN
BE ELIMINATED
We have observed nondeterminism in two slightly different forms in our discussion
of NFAs. It is most apparent if there is a state q and an alphabet symbol σ such
that several different transitions are possible in state q on input σ. A choice of
moves can also occur as a result of -transitions, because there may be states from
which the NFA can make either a transition on an input symbol or one on no input.
We will see in this section that both types of nondeterminism can be eliminated.
The idea in the second case is to introduce new transitions so that we no longer
need -transitions: In every case where there is no σ-transition from p to q but the
NFA can go from p to q by using one or more ’s as well as σ, we will introduce
the σ-transition. The resulting NFA may have even more nondeterminism of the
ﬁrst type than before, but it will be able to accept the same strings without using
-transitions.
The way we eliminate nondeterminism from an NFA having no -transitions
is simply to deﬁne it away, by ﬁnding an appropriate deﬁnition of state. We have
used this technique twice before, in Section 2.2 when we considered states that
were ordered pairs, and in Section 2.5 when we deﬁned a state to be a set of
strings. Here a similar approach is already suggested by the way we deﬁne the
transition function of an NFA, whose value is a set of states. If we say that for
an element p of a set S ⊆Q, the transition on input σ can possibly go to several
states, it sounds like nondeterminism; if we say that starting with an element of
the set S, the set of states to which we can go on input σ is

{δ(p, σ) | p ∈S}
and if both S and this set qualify as states in our new deﬁnition, then it sounds as
though we have eliminated the nondeterminism. The only question then is whether
the FA we obtain accepts the same strings as the NFA we started with.
Theorem 3.17
For every language L ⊆∗accepted by an NFA M = (Q, , q0, A, δ),
there is an NFA M1 with no -transitions that also accepts L.

3.3
The Nondeterminism in an NFA Can Be Eliminated
105
Proof
As we have already mentioned, we may need to add transitions in order
to guarantee that the same strings will be accepted even when the
-transitions are eliminated. In addition, if q0 /∈A but  ∈L, we will also
make q0 an accepting state of M1 in order to guarantee that M1 accepts .
We deﬁne
M1 = (Q, , q0, A1, δ1)
where for every q ∈Q, δ1(q, ) = ∅, and for every q ∈Q and every
σ ∈,
δ1(q, σ) = δ∗(q, σ)
Finally, we deﬁne
A1 =
 A ∪{q0}
if  ∈L
A
if not
For every state q and every x ∈∗, the way we have deﬁned the extended
transition function δ∗for the NFA M tells us that δ∗(q, x) is the set of
states M can reach by using the symbols of x together with -transitions.
The point of our deﬁnition of δ1 is that we want δ∗
1(q, x) to be the same
set, even though M1 has no -transitions. This may not be true for x = ,
because δ∗(q, ) = ({q}) and δ1(q, ) = {q}; this is the reason for the
deﬁnition of A1 above. We sketch the proof that for every q and every x
with |x| ≥1,
δ∗
1(q, x) = δ∗(q, x)
The proof is by structural induction on x. If x = a ∈, then by deﬁ-
nition of δ1, δ1(q, x) = δ∗(q, x), and because M1 has no -transitions,
δ1(q, x) = δ∗
1(q, x) (see Exercise 3.24).
Suppose that for some y with |y| ≥1, δ∗
1(q, y) = δ∗(q, y) for every
state q, and let σ be an arbitrary element of .
δ∗
1(q, yσ) =

{δ1(p, σ) | p ∈δ∗
1(q, y)}
=

{δ1(p, σ) | p ∈δ∗(q, y)} (by the induction hypothesis)
=

{δ∗(p, σ) | p ∈δ∗(q, y)} (by deﬁnition of δ1)
The last step in the induction proof is to check that this last expression is
indeed δ∗(q, yσ). This is a special case of the general formula
δ∗(q, yz) =

{δ∗(p, z) | p ∈δ∗(q, y)}
See Exercise 3.30 for the details.
Now we can verify that L(M1) = L(M) = L. If the string  is accep-
ted by M, then it is accepted by M1, because in this case q0 ∈A1 by def-
inition. If  /∈L(M), then A = A1; therefore, q0 /∈A1, and  /∈L(M1).

106
C H A P T E R 3
Regular Expressions, Nondeterminism, and Kleene’s Theorem
Suppose that |x| ≥1. If x ∈L(M), then δ∗(q0, x) contains an element
of A; therefore, since δ∗(q0, x) = δ∗
1(q0, x) and A ⊆A1, x ∈L(M1).
Now suppose |x| ≥1 and x ∈L(M1). Then δ∗
1(q0, x) contains an
element of A1. The state q0 is in A1 only if  ∈L; therefore, if δ∗
1(q0, x)
(which is the same as δ∗(q0, x)) contains q0, it also contains every element
of A in ({q0}). In any case, if x ∈L(M1), then δ∗
1(q0, x) must contain
an element of A, which implies that x ∈L(M).
Theorem 3.18
For every language L ⊆∗accepted by an NFA M = (Q, , q0, A, δ),
there is an FA M1 = (Q1, , q1, A1, δ1) that also accepts L.
Proof
Because of Theorem 3.17, it is sufﬁcient to prove the theorem in the case
when M has no -transitions. The formulas deﬁning δ∗are simpliﬁed
accordingly: δ∗(q, ) = {q} and δ∗(q, xσ) = ∪{δ(p, σ) | p ∈δ∗(q, x)}.
The ﬁnite automaton M1 can be deﬁned as follows, using the subset
construction: The states of M1 are sets of states of M, or
Q1 = 2Q
The initial state q1 of Q1 is {q0}. For every q ∈Q1 and every σ ∈,
δ1(q, σ) =

{δ(p, σ) | p ∈q}
and the accepting states of M1 are deﬁned by the formula
A1 = {q ∈Q1 | q ∩A ̸= ∅}
The last deﬁnition is the correct one, because a string x should be accepted
by M1 if, when the NFA M processes x, there is at least one state it might
end up in that is an element of A.
There is no doubt that M1 is an ordinary ﬁnite automaton. The expres-
sion δ∗
1(q1, x), however, is a set of states of M—not because M1 is
nondeterministic, but because we have deﬁned states of M1 to be sets
of states of M. The fact that the two devices accept the same language
follows from the fact that for every x ∈∗,
δ∗
1(q1, x) = δ∗(q0, x)
and we now prove this formula using structural induction on x. We must
keep in mind during the proof that δ∗
1 and δ∗are deﬁned in different ways,
because M1 is an FA and M is an NFA.
If x = , then
δ∗
1(q1, x) = δ∗
1(q1, )
= q1 (by the deﬁnition of δ∗
1)

3.3
The Nondeterminism in an NFA Can Be Eliminated
107
= {q0} (by the deﬁnition of q1)
= δ∗(q0, ) (by the deﬁnition of δ∗)
= δ∗(q0, x)
The induction hypothesis is that x is a string for which δ∗
1(q1, x) =
δ∗(q0, x), and we must show that for every σ ∈, δ∗
1(q1, xσ) = δ∗(q0, xσ).
δ∗
1(q1, xσ) = δ1(δ∗
1(q1, x), σ) (by the deﬁnition of δ∗
1)
= δ1(δ∗(q0, x), σ) (by the induction hypothesis)
=

{δ(p, σ) | p ∈δ∗(q0, x)} (by the deﬁnition of δ1)
= δ∗(q0, xσ) (by the deﬁnition of δ∗)
A string x is accepted by M1 precisely if δ∗
1(q1, x) ∈A1. We know now
that this is true if and only if δ∗(q0, x) ∈A1; and according to the deﬁ-
nition of A1, this is true if and only if δ∗(q0, x) ∩A ̸= ∅. Therefore, x is
accepted by M1 if and only if x is accepted by M.
We present three examples: one that illustrates the construction in Theorem
3.17, one that illustrates the subset construction in Theorem 3.18, and one in which
we use both to convert an NFA with -transitions to an ordinary FA.
EXAMPLE 3.19
Eliminating -Transitions from an NFA
Figure 3.20a shows the transition diagram for an NFA M with -transitions; it is not
hard to see that it accepts the language corresponding to the regular expression (a∗ab
(ba)∗)∗. We show in tabular form the values of the transition function δ, as well as the
values δ∗(q, a) and δ∗(q, b) that will give us the transition function δ1 in the resulting
NFA M1.
q
δδδ(q, a)
δδδ(q, b)
δδδ(q,)
δδδ∗∗∗(q, a)
δδδ∗∗∗(q, b)
1
∅
∅
{2}
{2, 3}
∅
2
{2, 3}
∅
∅
{2, 3}
∅
3
∅
{4}
∅
∅
{1, 2, 4}
4
∅
{5}
{1}
{2, 3}
{5}
5
{4}
∅
∅
{1, 2, 4}
∅
For example, the value δ∗(5, a) is the set {1, 2, 4}, because δ(5, a) = {4} and there are
-transitions from 4 to 1 and from 1 to 2.
Figure 3.20b shows the NFA M1, whose transition function has the values in the last
two columns of the table. In this example, the initial state of M is already an accepting
state, and so drawing the new transitions and eliminating the -transitions are the only
steps required to obtain M1.

108
C H A P T E R 3
Regular Expressions, Nondeterminism, and Kleene’s Theorem
1
a
a
5
b
b
a
2
3
4
Λ
Λ
1
a
a
b
5
b
a
b
a
a
a
a
a
b
2
3
4
a
(a)
(b)
Figure 3.20
Eliminating -transitions from an NFA.
EXAMPLE 3.21
Using the Subset Construction to Eliminate Nondeterminism
We consider the NFA M = (Q, {a, b}, 0, A, δ) in Example 3.6, shown in Figure 3.7. Instead
of labeling states as qi, here we will use only the subscript i. We will describe the FA
M1 = (2Q, {a, b}, {0}, A1, δ1) obtained from the construction in the proof of Theorem 3.18.
Because a set with n elements has 2n subsets, using this construction might require an
exponential increase in the number of states. As this example will illustrate, we can often
get by with fewer by considering only the states of M1 (subsets of Q) that are reachable
from {0}, the initial state of M1.
It is helpful, and in fact recommended, to use a transition table for δ in order to obtain
the values of δ1. The table is shown below.
q
δδδ(q, a)
δδδ(q, b)
0
{1, 2}
{4}
1
{0}
∅
2
{3}
∅
3
∅
{0}
4
∅
∅
The transition diagram for M1 is shown in Figure 3.22. For example, δ1({1, 2}, a) =
δ(1, a) ∪δ(2, a) = {0, 3}. If you compare Figure 3.22 to Figure 2.23c, you will see that
they are the same except for the way the states are labeled. The subset construction doesn’t
always produce the FA with the fewest possible states, but in this example it does.

3.3
The Nondeterminism in an NFA Can Be Eliminated
109
a, b
a, b
b
1, 2
a
b
b
b
a
a
a
0, 3
4
0, 4
0
Figure 3.22
Applying the subset construc-
tion to the NFA in Example
3.21.
EXAMPLE 3.23
Converting an NFA with -Transitions to an FA
For the NFA pictured in Figure 3.24a, we show the transition function in tabular form below,
as well as the transition function for the resulting NFA without -transitions. It is pictured
in Figure 3.24b.
q
δδδ( q, a)
δδδ( q, b)
δδδ( q,)
δδδ∗∗∗( q, a)
δδδ∗∗∗( q, b)
1
{1}
∅
{2, 4}
{1, 2, 3, 4, 5}
{4, 5}
2
{3}
{5}
∅
{3}
{5}
3
∅
{2}
∅
∅
{2}
4
{5}
{4}
∅
{5}
{4}
5
∅
∅
∅
∅
∅
b
1
a
a
3
Λ
5
4
Λ
b
b
2
b
1
a
a
3
5
4
b
b
2
(a)
a
a
a
a
a, b
a, b
(b)
Figure 3.24
Converting an NFA to an FA.

110
C H A P T E R 3
Regular Expressions, Nondeterminism, and Kleene’s Theorem
a
1
a
a
35
12345
245
3
2
5
4
45
a, b
a
a
a
a
a
a, b
b
b
b
b
b
b
b
b
(c)
f
Figure 3.24
Continued
The subset construction gives us a slightly greater variety of subsets this time, but
still considerably fewer than the total number of subsets of Q. The ﬁnal FA is shown in
Figure 3.24c.
3.4 KLEENE’S THEOREM, PART 1
If we are trying to construct a device that accepts a regular language L, we can
proceed one state at a time, as in Example 2.22, deciding at each step which
strings it is necessary to distinguish. Adding each additional state may get harder
as the number of states grows, but if we know somehow that there is an FA
accepting L, we can be sure that the procedure will eventually terminate and
produce one.
We have examples to show that for certain regular expressions, nondetermin-
ism simpliﬁes the problem of drawing an accepting device. In this section we will
use nondeterminism to show that we can do this for every regular expression. Fur-
thermore, we now have algorithms to convert the resulting NFA to an FA. The
conclusion will be that on the one hand, the state-by-state approach will always
work; and on the other hand, there is a systematic procedure that is also guaranteed
to work and may be more straightforward.
The general result is one half of Kleene’s theorem, which says that regular
languages are the languages that can be accepted by ﬁnite automata. We will discuss
the ﬁrst half in this section and the second in Section 3.5.

3.4
Kleene’s Theorem, Part 1
111
Theorem 3.25
Kleene’s Theorem, Part 1
For every alphabet , every regular language over  can be accepted by
a ﬁnite automaton.
Proof
Because of Theorems 3.17 and 3.18, it’s enough to show that every regular
language over  can be accepted by an NFA. The set of regular languages
over  is deﬁned recursively in Deﬁnition 3.1, and we will prove the
theorem by structural induction.
The languages ∅and {σ} (where σ ∈) can be accepted by the two
NFAs in Figure 3.26, respectively. The induction hypothesis is that L1
and L2 are both regular languages over  and that for both i = 1 and
i = 2, Li can be accepted by an NFA Mi = (Qi, , qi, Ai, δi). We can
assume, by renaming states if necessary, that Q1 and Q2 are disjoint. In
the induction step we must show that there are NFAs accepting the three
languages L(M1) ∪L(M2), L(M1)L(M2), and L(M1)∗.
In each case we will give an informal deﬁnition and a diagram show-
ing the idea of the construction. For simplicity, each diagram shows the
two NFAs M1 and M2 as having two accepting states, both distinct from
the initial state.
An NFA Mu accepting L(M1) ∪L(M2) is shown in Figure 3.27a.
Its states are those of M1 and M2 and one additional state qu that is the
initial state. The transitions include all the ones in M1 and M2 as well
as -transitions from qu to q1 and q2, the initial states of M1 and M2.
Finally, the accepting states are simply the states in A1 ∪A2.
If x ∈L(M1), for example, Mu can accept x by taking the -transition
from qu to q1 and then executing the moves that would allow M1 to accept
x. On the other hand, if x is any string accepted by Mu, there is a path
from qu to an element of A1 or A2. The ﬁrst transition in the path must
be a -transition, which takes Mu to q1 or q2. Because Q1 ∩Q2 = ∅, the
remainder of the path causes x to be accepted either by M1 or by M2.
An NFA Mc accepting L(M1)L(M2) is shown in Figure 3.27b. No
new states need to be added to those of M1 and M2. The initial state is q1,
and the accepting states are the elements of A2. The transitions include
all those of M1 and M2 and a new -transition from every element of
A1 to q2. If x is the string x1x2, where xi is accepted by Mi for each i,
then Mc can process x by moving from q1 to a state in A1 using ’s and
the symbols of x1, taking the -transition to q2, and moving to a state in
A2 using ’s and the symbols of x2. Conversely, if x is a string accepted
s
and
Figure 3.26

112
C H A P T E R 3
Regular Expressions, Nondeterminism, and Kleene’s Theorem
Λ
q1
qu
f1
'
q2
f1
'f2
f2
'f2
f2
qc = q1
f1
q2
f1
(a)
(b)
qk
f1
Λ
Λ
Λ
Λ
Λ
Λ
f1'
q1
(c)
'
Figure 3.27
Schematic diagram for Kleene’s theorem, Part 1.
by Mc, then at some point during the computation, Mc must execute the
-transition from an element of A1 to q2. If x1 is the preﬁx of x whose
symbols have been processed at that point, then x1 must be accepted by
M1; the remaining sufﬁx of x is accepted by M2, because it corresponds
to a path from q2 to an element of A2 that cannot involve any transitions
other than those of M2.
Finally, an NFA Mk accepting L(M1)∗is shown in Figure 3.27c.
Its states are the elements of Q1 and a new initial state qk that is also
the only accepting state. The transitions are those of M1, a -transition
from qk to q1, and a -transition from every element of A1 to qk. We
can see by structural induction that every element of L(M1)∗is accepted.
The null string is, because qk is an accepting state. Now suppose that
x ∈L(M1)∗is accepted and that y ∈L(M1). When M∗is in a state in
A1 after processing x, it can take a -transition to qk and another to q1,
process y so as to end up in an element of A1, and ﬁnish up by returning
to qk with a -transition. Therefore, xy is accepted by M∗.
We can argue in the opposite direction by using mathematical induc-
tion on the number of times M∗enters the state qk in the process of
accepting a string. If M∗visits qk only once in accepting x, then x = ,
which is an element of L(M1)∗. If we assume that n ≥1 and that every
string accepted by M∗that causes M∗to enter qk n or fewer times is in
L(M1)∗, then consider a string x that causes M∗to enter qk n + 1 times

3.4
Kleene’s Theorem, Part 1
113
and is accepted. Let x1 be the preﬁx of x that is accepted when M∗
enters qk the nth time, and let x2 be the remaining part of x. By the
induction hypothesis, x1 ∈L(M1)∗. In processing x2, M∗moves to q1 on
a -transition and then from q1 to an element of A1 using -transitions
in addition to the symbols of x2. Therefore, x2 ∈L(M1), and it follows
that x ∈L(M1)∗.
EXAMPLE 3.28
An NFA Corresponding to ((aa + b)∗(aba)∗bab)∗
The three portions of the induction step in the proof of Theorem 3.25 provide algorithms
for constructing an NFA corresponding to an arbitrary regular expression. These can be
combined into a general algorithm that could be used to automate the process.
The transition diagram in Figure 3.29a shows a literal application of the three algorithms
in the case of the regular expression ((aa + b)∗(aba)∗bab)∗. In this case there is no need
for all the -transitions that are called for by the algorithms, and a simpliﬁed NFA is
Λ
Λ
a
Λ
Λ
Λ
Λ
Λ
b
a
Λ
Λ
a
Λ
Λ
b
Λ
a
Λ
Λ
a
Λ
b
Λ
b
a
b
b
b
a
Λ
Λ
a
a
a
b
(a)
(b)
Figure 3.29
Constructing an NFA for the regular expression ((aa + b)∗(aba)∗bab)∗.

114
C H A P T E R 3
Regular Expressions, Nondeterminism, and Kleene’s Theorem
shown in Figure 3.29b. At least two -transitions are still helpful in order to preserve the
resemblance between the transition diagram and the regular expression. The algorithms can
often be shortened in examples, but for each step where one of them calls for an extra state
and/or a -transition, there are examples to show that dispensing with the extra state or the
transition doesn’t always work (see Exercises 3.45–3.48).
3.5 KLEENE’S THEOREM, PART 2
In this section we prove that if L is accepted by a ﬁnite automaton, then L is
regular. The proof will provide an algorithm for starting with an FA that accepts
L and ﬁnding a regular expression that describes L.
Theorem 3.30
Kleene’s Theorem, Part 2
For every ﬁnite automaton M = (Q, , q0, A, δ), the language L(M) is
regular.
Proof
For states p and q, we introduce the notation L(p, q) for the language
L(p, q) = {x ∈∗| δ∗(p, x) = q}
If we can show that for every p and q in Q, L(p, q) is regular, then it
will follow that L(M) is, because
L(M) =

{L(q0, q) | q ∈A}
and the union of a ﬁnite collection of regular languages is regular.
We will show that each language L(p, q) is regular by expressing it
in terms of simpler languages that are regular. Strings in L(p, q) cause
M to move from p to q in any manner whatsoever. One way to think
about simpler ways of moving from p to q is to think about the number
of transitions involved; the problem with this approach is that there is no
upper limit to this number, and so no obvious way to obtain a ﬁnal regular
expression. A similar approach that is more promising is to consider the
distinct states through which M passes as it moves from p to q. We can
start by considering how M can go from p to q without going through
any states, and at each step add one more state to the set through which
M is allowed to go. This procedure will terminate when we have enlarged
the set to include all possible states.
If x ∈L(p, q), we say x causes M to go from p to q through a state
r if there are nonnull strings x1 and x2 such that x = x1x2, δ∗(p, x1) = r,
and δ∗(r, x2) = q. In using a string of length 1 to go from p to q, M does
not go through any state. (If M loops from p back to p on the symbol a,
it does not go through p even though the string a causes it to leave p and
enter p.) In using a string of length n ≥2, it goes through a state n −1
times, but if n > 2 these states may not be distinct.

3.5
Kleene’s Theorem, Part 2
115
Now we assume that Q has n elements and that they are numbered
from 1 to n. For p, q ∈Q and j ≥0, we let L(p, q, j) be the set of
strings in L(p, q) that cause M to go from p to q without going through
any state numbered higher than j.
The set L(p, q, 0) is the set of strings that allow M to go from p to
q without going through any state at all. This includes the set of alphabet
symbols σ for which δ(p, σ) = q, and in the case when p = q it also
includes the string . In any case, L(p, q, 0) is a ﬁnite set of strings and
therefore regular.
Suppose that for some number k ≥0, L(p, q, k) is regular for every
p and every q in Q, and consider how a string can be in L(p, q, k + 1).
The easiest way is for it to be in L(p, q, k), because if M goes through no
state numbered higher than k, it certainly goes through no state numbered
higher than k + 1. The other strings in L(p, q, k + 1) are those that cause
M to go from p to q by going through state k + 1 and no higher-numbered
states. A path of this type goes from p to k + 1; it may return to k + 1
one or more times; and it ﬁnishes by going from k + 1 to q (see Figure
3.31). On each of these individual portions, the path starts or stops at state
k + 1 but doesn’t go through any state numbered higher than k.
Every string in L(p, q, k + 1) can be described in one of these two
ways, and every string that has one of these two forms is in L(p, q, k + 1).
The resulting formula is
L(p, q, k + 1) = L(p, q, k) ∪L(p, k + 1, k)L(k + 1, k + 1, k)∗
L(k + 1, q, k)
We have the ingredients, both for a proof by mathematical induction
that L(p, q) is regular and for an algorithm to obtain a regular expression
for this language. L(p, q, 0) can be described by a regular expression;
for each k < n, L(p, q, k + 1) is described by the formula above; and
L(p, q, n) = L(p, q), because the condition that the path go through no
state numbered higher than n is no restriction at all if there are no states
numbered higher than n. As we observed at the beginning of the proof, the
last step in obtaining a regular expression for L(M) is to use the + opera-
tion to combine the expressions for the languages L(q0, q), where q ∈A.
p
q
k +1
Figure 3.31
Going from p to q by going through k + 1.

116
C H A P T E R 3
Regular Expressions, Nondeterminism, and Kleene’s Theorem
EXAMPLE 3.32
Finding a Regular Expression Corresponding to an FA
Let M be the ﬁnite automaton pictured in Figure 3.33.
If we let r(i, j, k) denote a regular expression corresponding to the language L(i, j, k)
described in the proof of Theorem 3.30, then L(M) is described by the regular expression
r(M), where
r(M) = r(1, 1, 3) + r(1, 2, 3)
We might try calculating this expression from the top down, at least until we can see how
many of the terms r(i, j, k) we will need that involve smaller values of k. The recursive
formula in the proof of the theorem tells us that
r(1, 1, 3) = r(1, 1, 2) + r(1, 3, 2)r(3, 3, 2)∗r(3, 1, 2)
r(1, 2, 3) = r(1, 2, 2) + r(1, 3, 2)r(3, 3, 2)∗r(3, 2, 2)
Applying the formula to the expressions r(i, j, 2) that we apparently need, we obtain
r(1, 1, 2) = r(1, 1, 1) + r(1, 2, 1)r(2, 2, 1)∗r(2, 1, 1)
r(1, 3, 2) = r(1, 3, 1) + r(1, 2, 1)r(2, 2, 1)∗r(2, 3, 1)
r(3, 3, 2) = r(3, 3, 1) + r(3, 2, 1)r(2, 2, 1)∗r(2, 3, 1)
r(3, 1, 2) = r(3, 1, 1) + r(3, 2, 1)r(2, 2, 1)∗r(2, 1, 1)
r(1, 2, 2) = r(1, 2, 1) + r(1, 2, 1)r(2, 2, 1)∗r(2, 2, 1)
r(3, 2, 2) = r(3, 2, 1) + r(3, 2, 1)r(2, 2, 1)∗r(2, 2, 1)
At this point it is clear that we need every one of the expressions r(i, j, 1), and we
now start at the bottom and work our way up. The three tables below show the expressions
r(i, j, 0), r(i, j, 1), and r(i, j, 2) for all combinations of i and j. (Only six of the nine
entries in the last table are required.)
p
r( p, 1, 0)
r( p, 2, 0)
r( p, 3, 0)
1
a + 
b
∅
2
a

b
3
a
b

a
1
2
3
a
a
b
b
b
Figure 3.33
An
FA
for
which
we
want an equivalent regu-
lar expression.

Exercises
117
p
r(p, 1, 1)
r(p, 2, 1)
r(p, 3, 1)
1
a∗
a∗b
∅
2
aa∗
 + aa∗b
b
3
aa∗
a∗b

p
r(p, 1, 2)
r(p, 2, 2)
r(p, 3, 2)
1
a∗(baa∗)∗
a∗(baa∗)∗b
a∗(baa∗)∗bb
2
aa∗(baa∗)∗
(aa∗b)∗
(aa∗b)∗b
3
aa∗+ a∗baa∗(baa∗)∗
a∗b(aa∗b)∗
 + a∗b(aa∗b)∗b
For example,
r(2, 2, 1) = r(2, 2, 0) + r(2, 1, 0)r(1, 1, 0)∗r(1, 2, 0)
=  + (a)(a + )∗(b)
=  + aa∗b
r(3, 1, 2) = r(3, 1, 1) + r(3, 2, 1)r(2, 2, 1)∗r(2, 1, 1)
= aa∗+ (a∗b)( + aa∗b)∗(aa∗)
= aa∗+ a∗b(aa∗b)∗aa∗
= aa∗+ a∗baa∗(baa∗)∗
The terms required for the ﬁnal regular expression can now be obtained from the last
table. As you can see, these expressions get very involved, even though we have already
made some attempts to simplify them. There is no guarantee that the ﬁnal regular expression
is the simplest possible (it seems clear in this case that it is not), but at least we have a
systematic way of generating a regular expression corresponding to L(M).
EXERCISES
3.1.
In each case below, ﬁnd a string of minimum length in {a, b}∗not in the
language corresponding to the given regular expression.
a. b∗(ab)∗a∗
b. (a∗+ b∗)(a∗+ b∗)(a∗+ b∗)
c. a∗(baa∗)∗b∗
d. b∗(a + ba)∗b∗
3.2.
Consider the two regular expressions
r = a∗+ b∗
s = ab∗+ ba∗+ b∗a + (a∗b)∗
a. Find a string corresponding to r but not to s.
b. Find a string corresponding to s but not to r.
c. Find a string corresponding to both r and s.
d. Find a string in {a, b}∗corresponding to neither r nor s.

118
C H A P T E R 3
Regular Expressions, Nondeterminism, and Kleene’s Theorem
3.3.
Let r and s be arbitrary regular expressions over the alphabet . In each
case below, ﬁnd a simpler equivalent regular expression.
a. r(r∗r + r∗) + r∗
b. (r + )∗
c. (r + s)∗rs(r + s)∗+ s∗r∗
3.4.
It is not difﬁcult to show using mathematical induction that for every
integer n ≥2, there are nonnegative integers i and j such that
n = 2i + 3j. With this in mind, simplify the regular expression
(aa + aaa)(aa + aaa)∗.
3.5.
In each case below, give a simple description of the smallest set of
languages that contains all the “basic” languages ∅, {}, and {σ} (for
every σ ∈) and is closed under the speciﬁed operations.
a. union
b. concatenation
c. union and concatenation
3.6.
Suppose w and z are strings in {a, b}∗. Find regular expressions
corresponding to each of the languages deﬁned recursively below.
a.  ∈L; for every x ∈L, then wx and xz are elements of L.
b. a ∈L; for every x ∈L, wx, xw, and xz are elements of L.
c.  ∈L; a ∈L; for every x ∈L, wx and zx are in L.
3.7.
Find a regular expression corresponding to each of the following subsets
of {a, b}∗.
a.
The language of all strings containing exactly two a’s.
b.
The language of all strings containing at least two a’s.
c.
The language of all strings that do not end with ab.
d.
The language of all strings that begin or end with aa or bb.
e.
The language of all strings not containing the substring aa.
f.
The language of all strings in which the number of a’s is even.
g.
The language of all strings containing no more than one occurrence of
the string aa. (The string aaa should be viewed as containing two
occurrences of aa.)
h.
The language of all strings in which every a is followed immediately
by bb.
i.
The language of all strings containing both bb and aba as substrings.
j.
The language of all strings not containing the substring aaa.
k.
The language of all strings not containing the substring bba.
l.
The language of all strings containing both bab and aba as
substrings.
m. The language of all strings in which the number of a’s is even and the
number of b’s is odd.

Exercises
119
n.
The language of all strings in which both the number of a’s and the
number of b’s are odd.
3.8.
a. The regular expression (b + ab)∗(a + ab)∗describes the set of all
strings in {a, b}∗not containing the substring
x
for
any x. (Fill in the blanks appropriately.)
b. The regular expression (a + b)∗(aa∗bb∗aa∗+ bb∗aa∗bb∗)
(a + b)∗describes the set of all strings in {a, b}∗containing both
the substrings
and
. (Fill in the blanks
appropriately.)
3.9.
Show that every ﬁnite language is regular.
3.10.
a. If L is the language corresponding to the regular expression
(aab + bbaba)∗baba, ﬁnd a regular expression corresponding to
Lr = {xr | x ∈L}.
b. Using the example in part (a) as a model, give a recursive deﬁnition
(based on Deﬁnition 3.1) of the reverse er of a regular expression e.
c. Show that for every regular expression e, if the language L
corresponds to e, then Lr corresponds to er.
3.11.
The star height of a regular expression r over , denoted by sh(r), is
deﬁned as follows:
i.
sh(∅) = 0.
ii.
sh() = 0.
iii.
sh(σ) = 0 for every σ ∈.
iv.
sh((rs)) = sh((r + s)) = max(sh(r), sh(s)).
v.
sh((r∗)) = sh(r) + 1.
Find the star heights of the following regular expressions.
a. (a(a + a∗aa) + aaa)∗
b. (((a + a∗aa)aa)∗+ aaaaaa∗)∗
3.12.
For both the regular expressions in the previous exercise, ﬁnd an
equivalent regular expression of star height 1.
3.13.
Let c and d be regular expressions over .
a. Show that the formula r = c + rd, involving the variable r, is true if
the regular expression cd∗is substituted for r.
b. Show that if  is not in the language corresponding to d, then any
regular expression r satisfying r = c + rd corresponds to the same
language as cd∗.
3.14.
Describe precisely an algorithm that could be used to eliminate the symbol
∅from any regular expression that does not correspond to the empty
language.
3.15.
Describe an algorithm that could be used to eliminate the symbol  from
any regular expression whose corresponding language does not contain the
null string.

120
C H A P T E R 3
Regular Expressions, Nondeterminism, and Kleene’s Theorem
3.16.
The order of a regular language L is the smallest integer k for which
Lk = Lk+1, if there is one, and ∞otherwise.
a. Show that the order of L is ﬁnite if and only if there is an integer k
such that Lk = L∗, and that in this case the order of L is the smallest k
such that Lk = L∗.
b. What is the order of the regular language {} ∪{aa}{aaa}∗?
c. What is the order of the regular language {a} ∪{aa}{aaa}∗?
d. What is the order of the language corresponding to the regular
expression ( + b∗a)(b + ab∗ab∗a)∗?
3.17.
†A generalized regular expression is deﬁned the same way as an ordinary
regular expression, except that two additional operations, intersection and
complement, are allowed. So, for example, the generalized regular
expression abb∅′ ∩(∅′aaa∅′)′ represents the set of all strings in {a, b}∗
that start with abb and don’t contain the substring aaa.
a. Show that the subset {aba}∗of {a, b}∗can be described by a
generalized regular expression with no occurrences of ∗.
b. Can the subset {aaa}∗be described this way? Give reasons for your
answer.
3.18.
Figure 3.34, at the bottom of this page, shows a transition diagram for an
NFA. For each string below, say whether the NFA accepts it.
a. aba
b. abab
c. aaabbb
3.19.
Find a regular expression corresponding to the language accepted by the
NFA pictured in Figure 3.34. You should be able to do it without applying
Kleene’s theorem: First ﬁnd a regular expression describing the most
general way of reaching state 4 the ﬁrst time, and then ﬁnd a regular
expression describing the most general way, starting in state 4, of moving
to state 4 the next time.
3.20.
For each of the NFAs shown in Figure 3.35 on the next page, ﬁnd a
regular expression corresponding to the language it accepts.
3.21.
On the next page, after Figure 3.35, is the transition table for an NFA with
states 1–5 and input alphabet {a, b}. There are no -transitions.
3
2
1
4
b
b
a
a
a, b
Λ
a
Λ
5
Figure 3.34

Exercises
121
a
b
a
a
a
Λ
b
b
b
a
a
a
a
a
b
a
(a)
b
Λ
(b)
a
a
a
a
a
b
b
(c)
Λ
a
b
a
a
b
Λ
b
Λ
Figure 3.35
q
δδδ( q, a)
δδδ( q, b)
1
{1, 2}
{1}
2
{3}
{3}
3
{4}
{4}
4
{5}
∅
5
∅
{5}
a. Draw a transition diagram.
b. Calculate δ∗(1, ab).
c. Calculate δ∗(1, abaab).
3.22.
A transition table is given for an NFA with seven states.
q
δδδ( q, a)
δδδ( q, b)
δδδ( q,)
1
∅
∅
{2}
2
{3}
∅
{5}
3
∅
{4}
∅
4
{4}
∅
{1}
5
∅
{6, 7}
∅
6
{5}
∅
∅
7
∅
∅
{1}

122
C H A P T E R 3
Regular Expressions, Nondeterminism, and Kleene’s Theorem
Find:
a. ({2, 3})
b. ({1})
c. ({3, 4})
d. δ∗(1, ba)
e. δ∗(1, ab)
f. δ∗(1, ababa)
3.23.
A transition table is given for another NFA with seven states.
q
δδδ( q, a)
δδδ( q, b)
δδδ( q,)
1
{5}
∅
{4}
2
{1}
∅
∅
3
∅
{2}
∅
4
∅
{7}
{3}
5
∅
∅
{1}
6
∅
{5}
{4}
7
{6}
∅
∅
Calculate δ∗(1, ba).
3.24.
Let M = (Q, , q0, A, δ) be an NFA with no -transitions. Show that for
every q ∈Q and every σ ∈, δ∗(q, σ) = δ(q, σ).
3.25.
It is easy to see that if M = (Q, , q0, A, δ) is an FA accepting L, then
the FA M′ = (Q, , q0, Q −A, δ) accepts L′ (the FA obtained from
Theorem 2.15 by writing L′ = ∗−L is essentially M′). Does this still
work if M is an NFA? If so, prove it. If not, ﬁnd a counterexample.
3.26.
In Deﬁnition 3.14, δ∗is deﬁned recursively in an NFA by ﬁrst deﬁning
δ∗(q, ) and then deﬁning δ∗(q, yσ), where y ∈∗and σ ∈. Give an
acceptable recursive deﬁnition in which the recursive part of the deﬁnition
deﬁnes δ∗(q, σy) instead.
3.27.
Which of the following, if any, would be a correct substitute for the
second part of Deﬁnition 3.14? Give reasons for your answer.
a. δ∗(q, σy) = ({δ∗(r, y) | r ∈δ(q, σ)})
b. δ∗(q, σy) = {(δ∗(r, y)) | r ∈δ(q, σ)}
c. δ∗(q, σy) = {δ∗(r, y) | r ∈(δ(q, σ))}
d. δ∗(q, σy) = {(δ∗(r, y)) | r ∈(δ(q, σ))}
3.28.
Let M = (Q, , q0, A, δ) be an NFA. This exercise involves properties of
the -closure of a set S. Since (S) is deﬁned recursively, structural
induction can be used to show that (S) is a subset of some other set.
a. Show that if S and T are subsets of Q for which S ⊆T , then
(S) ⊆(T ).
b. Show that for any S ⊆Q, ((S)) = (S).

Exercises
123
c. Show that if S, T ⊆Q, then (S ∪T ) = (S) ∪(T ).
d. Show that if S ⊆Q, then (S) = {({p}) | p ∈S}.
e. Draw a transition diagram to illustrate the fact that (S ∩T ) and
(S) ∩(T ) are not always the same. Which is always a subset of the
other?
f. Draw a transition diagram illustrating the fact that (S′) and (S)′ are
not always the same. Which is always a subset of the other? Under
what circumstances are they equal?
3.29.
Let M = (Q, , q0, A, δ) be an NFA. A set S ⊆Q is called -closed if
(S) = S.
a. Show that the union of two -closed sets is -closed.
b. Show that the intersection of two -closed sets is -closed.
c. Show that for any subset S of Q, (S) is the smallest -closed set of
which S is a subset.
3.30.
†Let M = (Q, , q0, A, δ) be an NFA. Show that for every q ∈Q and
every x, y ∈∗,
δ∗(q, xy) =

{δ∗(r, y) | r ∈δ∗(q, x)}
3.31.
Let M = (Q, , q0, A, δ) be an FA, and let M1 = (Q, , q0, A, δ1) be the
NFA with no -transitions for which δ1(q, σ) = {δ(q, σ)} for every q ∈Q
and σ ∈. Show that for every q ∈Q and x ∈∗, δ∗
1(q, x) = {δ(q, x)}.
Recall that the two functions δ∗and δ∗
1 are deﬁned differently.
3.32.
Let M = (Q, , q0, A, δ) be an NFA accepting a language L. Assume that
there are no transitions to q0, that A has only one element, qf , and that
there are no transitions from qf .
a. Let M1 be obtained from M by adding -transitions from q0 to every
state that is reachable from q0 in M. (If p and q are states, q is
reachable from p if there is a string x ∈∗such that q ∈δ∗(p, x).)
Describe (in terms of L) the language accepted by M1.
b. Let M2 be obtained from M by adding -transitions to qf from every
state from which qf is reachable in M. Describe in terms of L the
language accepted by M2.
c. Let M3 be obtained from M by adding both the -transitions in (a)
and those in (b). Describe the language accepted by M3.
3.33.
Give an example of a regular language L containing  that cannot be
accepted by any NFA having only one accepting state and no
-transitions, and show that your answer is correct.
3.34.
Can every regular language not containing  be accepted by an NFA
having only one accepting state and no -transitions? Prove your answer.
3.35.
Let M = (Q, , q0, A, δ) be an NFA, let m be the maximum size of any
of the sets δ∗(q, σ) for q ∈Q and σ ∈, and let x be a string of length n
over the input alphabet.

124
C H A P T E R 3
Regular Expressions, Nondeterminism, and Kleene’s Theorem
(b)
1
4
5
a
a
b
b
b
Λ
Λ
Λ
3
2
2
(a)
1
2
3
4
a
a
b
1
5
3
b
1
3
2
b
a
a
a
Λ
Λ
Λ
Λ
Λ
Λ
(c)
4
4
b
a
a
a
a
(d)
(e)
1
4
2
3
5
6
b
a
b
b
a
Figure 3.36
a. What is the maximum number of distinct paths that there might be in
the computation tree corresponding to x?
b. In order to determine whether x is accepted by M, it is sufﬁcient to
replace the complete computation tree by one that is perhaps smaller,
obtained by “pruning” the original one so that no level of the tree
contains more nodes than the number of states in M (and no level
contains more nodes than there are at that level of the original tree).
Explain why this is possible, and how it might be done.
3.36.
Let M = (Q, , q0, A, δ) be an NFA. The NFA M1 obtained by
eliminating -transitions from M might have more accepting states than
M, because the initial state q0 is made an accepting state if
({q0}) ∩A ̸= ∅. Explain why it is not necessary to make all the states q
for which ({q}) ∩A ̸= ∅accepting states in M1.
3.37.
In each part of Figure 3.36 is pictured an NFA. Use the algorithm
described in the proof of Theorem 3.17 to draw an NFA with no
-transitions accepting the same language.
3.38.
Each part of Figure 3.37 pictures an NFA. Using the subset construction,
draw an FA accepting the same language. Label the ﬁnal picture so as to
make it clear how it was obtained from the subset construction.

Exercises
125
b
a
a, b
3
2
1
a, b
b
a
a, b
b
4
3
2
1
a
a, b
1
2
3
a, b
(b)
a
a
a
b
1
2
3
4
b
b
b
b
a
1
2
3
a
a
a
b
4
5
a
a
(d)
a
a
1
2
3
b
a
a
b
5
4
( f )
a
1
2
3
a
a
a
b
b
b
4
5
(g)
a
b
(a)
(c)
(e)
b
b
b
b
b
Figure 3.37
3.39.
Suppose L ⊆∗is a regular language. If every FA accepting L has at
least n states, then every NFA accepting L has at least
states. (Fill in
the blank, and explain your answer.)
3.40.
Each part of Figure 3.38 shows an NFA. Draw an FA accepting the same
language.
3.41.
For each of the following regular expressions, draw an NFA accepting the
corresponding language, so that there is a recognizable correspondence
between the regular expression and the transition diagram.
a. (b + bba)∗a
b. (a + b)∗(abb + ababa)(a + b)∗

126
C H A P T E R 3
Regular Expressions, Nondeterminism, and Kleene’s Theorem
a
1
Λ
Λ
Λ
2
3
4
5
6
7
a
a
b
b
1
2
3
a
b
4
5
6
a
b
b
a
b
b
Λ
(a)
(b)
b
a
Figure 3.38
c. (a + b)(ab)∗(abb)∗
d. (a + b)∗(abba∗+ (ab)∗ba)
e. (a∗bb)∗+ bb∗a∗
3.42.
For part (e) of Exercise 3.41, draw the NFA that is obtained by a literal
application of Kleene’s theorem, without any simpliﬁcations.
3.43.
Suppose M = (Q, , q0, A, δ) is an NFA accepting a language L. Let M1
be the NFA obtained from M by adding -transitions from each element
of A to q0. Describe (in terms of L) the language L(M1).
3.44.
Suppose M = (Q, , q0, A, δ) is an NFA accepting a language L.
a. Describe how to construct an NFA M1 with no transitions to its initial
state so that M1 also accepts L.
b. Describe how to construct an NFA M2 with exactly one accepting state
and no transitions from that state, so that M2 also accepts L.
3.45.
Suppose M is an NFA with exactly one accepting state qf that accepts the
language L ⊆{a, b}∗. In order to ﬁnd NFAs accepting the languages
{a}∗L and L{a}∗, we might try adding a-transitions from q0 to itself and
from qf to itself, respectively. Draw transition diagrams to show that
neither technique always works.
3.46.
In the construction of Mu in the proof of Theorem 3.25, consider this
alternative to the construction described: Instead of a new state qu and
-transitions from it to q1 and q2, make q1 the initial state of the new
NFA, and create a -transition from it to q2. Either prove that this works
in general, or give an example in which it fails.
3.47.
In the construction of Mc in the proof of Theorem 3.25, consider the
simpliﬁed case in which M1 has only one accepting state. Suppose that we
eliminate the -transition from the accepting state of M1 to q2, and merge
these two states into one. Either show that this would always work in this
case, or give an example in which it fails.
3.48.
In the construction of M∗in the proof of Theorem 3.25, suppose that
instead of adding a new state q0, with -transitions from it to q1 and to it

Exercises
127
a
a
a
a
b
b
b
b
(a)
b
b
(b)
b
a
a
a
Figure 3.39
from each accepting state of Q1, we make q1 both the initial state and the
accepting state, and create -transitions from each accepting state of M1
to q0. Either show that this works in general, or give an example in which
it fails.
3.49.
Figure 3.39 shows FAs M1 and M2 accepting languages L1 and L2,
respectively. Draw NFAs accepting each of the following languages, using
the constructions in the proof of Theorem 3.25.
a. L∗
2 ∪L1
b. L2L∗
1
c. L1L2 ∪(L2L1)∗
3.50.
Draw NFAs with no -transitions accepting L1L2 and L2L1, where L1
and L2 are as in Exercise 3.49. Do this by connecting the two given
diagrams directly, by arrows with appropriate labels.
3.51.
Use the algorithm of Theorem 3.30 to ﬁnd a regular expression
corresponding to each of the FAs shown in Figure 3.40. In each case, if
the FA has n states, construct tables showing L(p, q, j) for each j with
0 ≤j ≤n −1.
3.52.
Suppose M is an FA with the three states 1, 2, and 3, and 1 is both the
initial state and the only accepting state. The expressions r(p, q, 2)
corresponding to the languages L(p, q, 2) are shown in the table below.
Write a regular expression describing L(M).
p
r(p, 1, 2)
r(p, 2, 2)
r(p, 3, 2)
1

aa∗
b + aa∗b
2
∅
a∗
a∗b
3
a
aaa∗
 + b + ab + aaa∗b
3.53.
Suppose 1 and 2 are alphabets, and the function f : ∗
1 →∗
2 is a
homomorphism; i.e., f (xy) = f (x)f (y) for every x, y ∈∗
1.
a. Show that f () = .

128
C H A P T E R 3
Regular Expressions, Nondeterminism, and Kleene’s Theorem
1
b
b
a
a
b
b
a
a
a
2
3
1
2
3
b
a, b
2
3
1
4
b
a
a, b
a, b
a, b
2
3
1
4
a
b
b
b
b
a
a
(d)
(c)
(b)
(a)
a
Figure 3.40
b. Show that if L ⊆∗
1 is regular, then f (L) is regular. (f (L) is the set
{y ∈∗
2 | y = f (x) for some x ∈L}.)
c. Show that if L ⊆∗
2 is regular, then f −1(L) is regular. (f −1(L) is the
set {x ∈∗
1 | f (x) ∈L}.)
3.54.
Suppose M = (Q, , q0, A, δ) is an NFA. For two (not necessarily
distinct) states p and q, we deﬁne the regular expression e(p, q) as
follows: e(p, q) = l + r1 + r2 + · · · + rk, where l is either  (if δ(p, )
contains q) or ∅, and the ri’s are all the elements σ of  for which
δ(p, σ) contains q. It’s possible for e(p, q) to be ∅, if there are no
transitions from p to q; otherwise, e(p, q) represents the “most general”
transition from p to q.
If we generalize this by allowing e(p, q) to be an arbitrary regular
expression over , we get what is called an expression graph. If p and q
are two states in an expression graph G, and x ∈∗, we say that x allows
G to move from p to q if there are states p0, p1, . . . , pm, with p0 = p
and pm = q, such that x corresponds to the regular expression
e(p0, p1)e(p1, p2) . . . e(pn−1, pn). This allows us to say how G accepts a
string x (x allows G to move from the initial state to an accepting state),
and therefore to talk about the language accepted by G. It is easy to see
that in the special case where G is simply an NFA, the two deﬁnitions for
the language accepted by G coincide. It is also not hard to convince

Exercises
129
yourself, using Theorem 3.25, that for any expression graph G, the
language accepted by G can be accepted by an NFA.
We can use the idea of an expression graph to obtain an alternate
proof of Theorem 3.30, as follows. Starting with an FA M accepting L,
we may easily convert it to an NFA M1 accepting L, so that M1 has no
transitions to its initial state q0, exactly one accepting state qf (which is
different from q0), and no transitions from qf . The remainder of the proof
is to specify a reduction technique to reduce by one the number of states
other than q0 and qf , obtaining an equivalent expression graph at each
step, until q0 and qf are the only states remaining. The regular expression
e(q0, qf ) then describes the language accepted. If p is the state to be
eliminated, the reduction step involves redeﬁning e(q, r) for every pair of
states q and r other than p.
Describe in more detail how this reduction can be done. Then apply
this technique to the FAs in Figure 3.40 to obtain regular expressions
corresponding to their languages.

130
C
H
A
P
T
E
R
4
Context-Free Languages
R
egular languages and ﬁnite automata are too simple and too restrictive to be
able to handle languages that are at all complex. Using context-free grammars
allows us to generate more interesting languages; much of the syntax of a high-level
programming language, for example, can be described this way. In this chapter
we start with the deﬁnition of a context-free grammar and look at a number of
examples. A particularly simple type of context-free grammar provides another
way to describe the regular languages we discussed in Chapters 2 and 3. Later
in the chapter we study derivations in a context-free grammar, how a derivation
might be related to the structure of the string being derived, and the presence of
ambiguity in a grammar, which can complicate this relationship. We also consider a
few ways that a grammar might be simpliﬁed to make it easier to answer questions
about the strings in the corresponding language.
4.1 USING GRAMMAR RULES TO DEFINE A
LANGUAGE
The term grammar applied to a language like English refers to the rules for con-
structing phrases and sentences. For us a grammar is also a set of rules, simpler
than the rules of English, by which strings in a language can be generated. In the
ﬁrst few examples in this section, a grammar is another way to write the recursive
deﬁnitions that we used in Chapter 1 to deﬁne languages.
EXAMPLE 4.1
The language AnBn
In Example 1.18, we deﬁned the language AnBn = {anbn | n ≥0} using this recursive
deﬁnition:
1.
 ∈AnBn.
2.
for every S ∈AnBn, aSb ∈AnBn.

4.1
Using Grammar Rules to Deﬁne a Language
131
Let us think of S as a variable, representing an arbitrary string in AnBn. Rule 1,
which we rewrite as S →, says that the arbitrary string could simply be , obtained by
substituting  for the variable S. To obtain any other string, we must begin with rule 2,
which we write S →aSb. This rule says that a string in AnBn can have the form aSb
(or that S can be replaced by aSb), where the new occurrence of S represents some other
element of AnBn. Replacing S by aSb is the ﬁrst step in a derivation of the string, and the
remaining steps will be further applications of rules 1 and 2 that will give a value to the
new S. The derivation will continue as long as the string contains the variable S, so that in
this example the last step will always be to replace S by .
If α and β are strings, and α contains at least one occurrence of S, the notation
α ⇒β
will mean that β is obtained from α by using one of the two rules to replace a single
occurrence of S by either  or aSb. Using this notation, we would write
S ⇒aSb ⇒aaSbb ⇒aaaSbbb ⇒aaabbb
to describe the sequence of steps (three applications of rule 2, then one application of rule
1) we used to derive the string aaabbb ∈AnBn.
The notation is simpliﬁed further by writing rules 1 and 2 as
S → | aSb
and interpreting | as “or”. When we write the rules of a grammar this way, we give con-
catenation higher precedence than |, which means in our example that the two alternatives
in the formula S → | aSb are  and aSb, not  and a.
EXAMPLE 4.2
The Language Expr
In Example 1.19 we considered the language Expr of algebraic expressions involving the
binary operators + and ∗, left and right parentheses, and a single identiﬁer a. We used the
following recursive deﬁnition to describe Expr:
1.
a ∈Expr.
2.
For every x and y in Expr, x + y and x ∗y are in Expr.
3.
For every x ∈Expr, (x) ∈Expr.
Rule 2 involves two different letters x and y, because the two expressions being combined
with either + or ∗are not necessarily the same. However, they both represent elements
of Expr, and we still need only one variable in the grammar rules corresponding to the
recursive deﬁnition:
S →a | S + S | S ∗S | (S)
To derive the string a + (a ∗a), for example, we could use the sequence of steps
S ⇒S + S ⇒a + S ⇒a + (S) ⇒a + (S ∗S) ⇒a + (a ∗S) ⇒a + (a ∗a)
The string S + S that we obtain from the ﬁrst step of this derivation suggests that the ﬁnal
expression should be interpreted as the sum of two subexpressions. The subexpressions we

132
C H A P T E R 4
Context-Free Languages
end up with are obviously not the same, but they are both elements of Expr and can therefore
both be derived from S.
A derivation of an expression in Expr is related to the way we choose to interpret
the expression, and there may be several possible choices. The expression a + a ∗a, for
example, has at least these two derivations:
S ⇒S + S ⇒a + S ⇒a + S ∗S ⇒a + a ∗S ⇒a + a ∗a
and
S ⇒S ∗S ⇒S + S ∗S ⇒a + S ∗S ⇒a + a ∗S ⇒a + a ∗a
The ﬁrst steps in these two derivations yield the strings S + S and S ∗S, respectively. The
ﬁrst derivation suggests that the expression is the sum of two subexpressions, the second
that it is the product of two subexpressions.
The rules of precedence normally adopted for algebraic expressions say, among other
things, that multiplication has higher precedence than addition. If we adopt this precedence
rule, then when we evaluate the expression a + a ∗a, we ﬁrst evaluate the product a ∗a,
so that we interpret the expression as a sum, not a product. Because there is nothing in our
grammar rules to suggest that the ﬁrst derivation is preferable to the second, one possible
conclusion is that it might be better to use another grammar, in which every string has
essentially only one derivation. We will return to this question in Section 4.4, when we
discuss ambiguity in a grammar.
We have made the language Expr simple by restricting the expressions in several ways.
We could easily add grammar rules to allow other operations besides + and ∗; if we wanted
to allow other “atomic” expressions besides the identiﬁer a (more general identiﬁers, or
numeric literals such as 16 and 1.3E−2, or both), we could add another variable, A, to get
S →A | S + S | S ∗S | (S)
and then look for grammar rules beginning with A that would generate all the subexpres-
sions we wanted. The next example involves another language L for which grammar rules
generating L require more than one variable.
EXAMPLE 4.3
Palindromes and Nonpalindromes
We see from Example 1.18 that the language Pal of palindromes over the alphabet {a, b}
can be generated by the grammar rules
S → | a | b | aSa | bSb
What about its complement NonPal? The last two grammar rules in the deﬁnition of Pal
still seem to work: For every nonpalindrome x, both axa and bxb are also nonpalindromes.
But a recursive deﬁnition of NonPal cannot be as simple as the one for Pal, because there
is no ﬁnite set of strings comparable to {, a, b} that can serve as basis elements in the
deﬁnition (see Exercise 4.6).
To ﬁnd the crucial feature of a nonpalindrome, let’s look at one, say
x = abbbbaaba

4.1
Using Grammar Rules to Deﬁne a Language
133
The string x is abyba, where y = bbbaa. Working our way in from the ends, comparing the
symbol on the left with the one on the right, we wouldn’t know until we got to y whether
x was a palindrome or not, but once we saw that y looked like bza for some string z, it
would be clear, even without looking at any symbols of z. Here is a deﬁnition of NonPal:
1.
For every A ∈{a, b}∗, aAb and bAa are elements of NonPal;
2.
For every S in NonPal, aSa and bSb are in NonPal.
In order to obtain grammar rules generating NonPal, we can introduce A as a second
variable, representing an arbitrary element of {a, b}∗, and use grammar rules starting with
A that correspond to the recursive deﬁnition in Example 1.17. The complete set of rules for
NonPal is
S →aSa | bSb | aAb | bAa
A →Aa | Ab | 
A derivation of abbbbaaba in this grammar is
S ⇒aSa ⇒abSba ⇒abbAaba
⇒abbAaaba ⇒abbAbaaba ⇒abbAbbaaba ⇒abbbbaaba
In order to generate a language L using the kinds of grammars we are discussing, it is
often necessary to include several variables. The start variable is distinguished from the
others, and we will usually denote it by S. Each remaining variable can be thought of as
representing an arbitrary string in some auxiliary language involved in the deﬁnition of L
(the language of all strings that can be derived from that variable). We can still interpret the
grammar as a recursive deﬁnition of L, except that we must extend our notion of recursion
to include mutual recursion: rather than one object deﬁned recursively in terms of itself,
several objects deﬁned recursively in terms of each other.
EXAMPLE 4.4
English and Programming-Language Syntax
You can easily see how grammar rules can be used to describe simple English syntax. Many
useful sentences can be generated by the rule
<declarative sentence> →<subject phrase> <verb phrase> <object>
provided that reasonable rules are found for each of the three variables on the right. Three
examples are “haste makes waste”, “the ends justify the means”, and “we must extend our
notion” (from the last sentence in Example 4.3). You can also see how difﬁcult it would be
to ﬁnd grammars of a reasonable size that would allow more sophisticated sentences without
also allowing gibberish. (Try to formulate some rules to generate the preceding sentence,
“You can also see how . . . gibberish”. Unless your approach is to provide almost as many
rules as sentences, the chances are that the rules will also generate strings that aren’t really
English sentences.)
The syntax of programming languages is much simpler. Two types of statements in C
are if statements and for statements.
<statement> →. . . | <if −statement> | <for −statement>

134
C H A P T E R 4
Context-Free Languages
Assuming we are using “if statements” to include both those with else and those without,
we might write
<if −statement> →if ( <expr> ) <statement> |
if ( <expr> ) <statement> else <statement>
< for −statement> →for ( <expr>; <expr>; <expr> ) <statement>
where <expr> is another variable, for which the productions would need to be described.
The logic of a program often requires that a “statement” include several statements.
We can deﬁne a compound statement as follows:
<compound statement> →{ <statement-sequence> }
<statement-sequence> → | <statement> <statement-sequence>
A syntax diagram such as the one in Figure 4.5 accomplishes the same thing.
〈 statement 〉
Figure 4.5
A path through the diagram begins with {, ends with }, and traverses the loop zero or more
times.
4.2 CONTEXT-FREE GRAMMARS:
DEFINITIONS AND MORE EXAMPLES
Deﬁnition 4.6
Context-Free Grammars
A context-free grammar (CFG) is a 4-tuple G = (V, , S, P ), where V
and  are disjoint ﬁnite sets, S ∈V , and P is a ﬁnite set of formulas of
the form A →α, where A ∈V and α ∈(V ∪)∗.
Elements of  are called terminal symbols, or terminals, and elements
of V are variables, or nonterminals. S is the start variable, and elements
of P are grammar rules, or productions.
As in Section 4.1, we will reserve the symbol →for productions in a grammar,
and we will use ⇒for a step in a derivation. The notations
α ⇒n β
and
α ⇒∗β
refer to a sequence of n steps and a sequence of zero or more steps, respectively,
and we sometimes write
α ⇒G β
or
α ⇒n
G β
or
α ⇒∗
G β

4.2
Context-Free Grammars: Deﬁnitions and More Examples
135
to indicate explicitly that the steps involve productions in the grammar G. If G =
(V, , S, P ), the ﬁrst statement means that there are strings α1, α2, and γ in
(V ∪)∗and a production A →γ in P such that
α = α1Aα2
β = α1γ α2
In other words, β can be obtained from α in one step by applying the pro-
duction A →γ . Whenever there is no chance of confusion, we will drop the
subscript G.
In the situation we have just described, in which α = α1Aα2 and β = α1γ α2,
the formula α ⇒β represents a step in a derivation; if our deﬁnition of productions
allowed α →β to be a production, we might say that the variable A could be
replaced by γ , depending on its context—i.e., depending on the values of α1 and
α2. What makes a context-free grammar context-free is that the left side of a
production is a single variable and that we may apply the production to any string
containing that variable, independent of the context. In Chapter 8 we will consider
grammars, and productions, that are not context-free.
Deﬁnition 4.7
The Language Generated by a CFG
If G = (V, , S, P ) is a CFG, the language generated by G is
L(G) = {x ∈∗| S ⇒∗
G x}
A language L is a context-free language (CFL) if there is a CFG G with
L = L(G).
EXAMPLE 4.8
The Language AEqB
Exercises 1.65 and 4.16 both allow us to ﬁnd context-free grammars for the language
AEqB = {x ∈{a, b}∗| na(x) = nb(x)} that use only the variable S. In this example we
consider a grammar with three variables that is based on a deﬁnition involving mutual
recursion.
If x is a nonnull string in AEqB, then either x = ay, where y ∈Lb = {z | nb(z) =
na(z) + 1}, or x = by, where y ∈La = {z | na(z) = nb(z) + 1}. Let us use the variables A
and B to represent La and Lb, respectively, and try to ﬁnd a CFG for AEqB that involves
the three variables S, A, and B. So far, the appropriate productions are
S → | aB | bA
All we need are productions starting with A and B.
If a string x in La starts with a, then the remaining substring is an element of AEqB.
What if it starts with b? Then x = by, where y has two more a’s than b’s. The crucial obser-
vation here is that every string y having two more a’s than b’s must be the concatenation of
two strings y1 and y2, each with one more a. To see this, think about the relative numbers
of a’s and b’s in each preﬁx of y; speciﬁcally, for each preﬁx z, let d(z) = na(z) −nb(z).

136
C H A P T E R 4
Context-Free Languages
The shortest preﬁx of y is , and d() is obviously 0; the longest preﬁx is y itself, and
d(y) = 2 by assumption. Each time we add a single symbol to a preﬁx, the d-value changes
(either increases or decreases) by 1. If a quantity starts at 0, changes by 1 at each step, and
ends up at 2, it must be 1 at some point! Therefore, for some string y1 with d(y1) = 1,
and some other string y2, y = y1y2, and since d(y) = d(y1) + d(y2) = 2, d(y2) must also
be 1.
The argument is exactly the same for a string in Lb that starts with a. We conclude
that AEqB is generated by the CFG with productions
S → | aB | bA
A →aS | bAA
B →bS | aBB
One feature of this CFG is that if we call A the start variable instead of S, it also works as
a grammar generating the language La, and similarly for B and Lb.
We can obtain many more examples of context-free languages from the fol-
lowing theorem, which describes three ways of starting with CFLs and constructing
new ones.
Theorem 4.9
If L1 and L2 are context-free languages over an alphabet , then L1 ∪L2,
L1L2, and L∗
1 are also CFLs.
Proof
Suppose G1 = (V1, , S1, P1) generates L1 and G2 = (V2, , S2, P2) gen-
erates L2. We consider the three new languages one at a time, and in the
ﬁrst two cases we assume, by renaming variables if necessary, that G1
and G2 have no variables in common.
1. We construct a CFG Gu = (Vu, , Su, Pu) generating L1 ∪L2, as follows.
Su is a new variable not in either V1 or V2,
Vu = V1 ∪V2 ∪{Su}
and
Pu = P1 ∪P2 ∪{Su →S1 | S2}
For every x ∈L1 ∪L2, we can derive x in the grammar Gu by starting with
either Su →S1 or Su →S2 and continuing with the derivation in either G1
or G2. On the other hand, if Su ⇒∗
GU x, the ﬁrst step in any derivation must
be either Su ⇒S1 or Su ⇒S2, because those are the only productions with
left side Su. In the ﬁrst case, the remaining steps must involve productions in
G1, because no variables in V2 can appear, and so x ∈L1; similarly, in the
second case x ∈L2. Therefore, L(Gu) = L1 ∪L2.

4.2
Context-Free Grammars: Deﬁnitions and More Examples
137
2. To obtain Gc = (Vc, , Sc, Pc) generating L1L2, we add the single variable
Sc to the set V1 ∪V2, just as in the union, and we let
Pc = P1 ∪P2 ∪{Sc →S1S2}
For a string x = x1x2 ∈L1L2, where x1 ∈L1 and x2 ∈L2, a derivation of x
in Gc is
Sc ⇒S1S2 ⇒∗x1S2 ⇒∗x1x2
where the second step (actually a sequence of steps) is a derivation of x1 in
G1 and the third step is a derivation of x2 in G2. Conversely, since the ﬁrst
step in any derivation in Gc must be Sc ⇒S1S2, every string x derivable
from Sc must have the form x = x1x2, where for each i, xi is derivable from
Si in Gc. But because V1 ∩V2 = ∅, being derivable from Si in Gc means
being derivable from Si in Gi, so that x ∈L1L2.
3. We can deﬁne a CFG G∗= (V, , S, P) generating L∗
1 by letting
V = V1 ∪{S}, where S is a variable not in V1, and adding productions that
generate all possible strings Sk
1, where k ≥0. Let
P = P1 ∪{S →SS1 | }
Every string in L∗
1 is an element of L(G1)k for some k ≥0 and can
therefore be obtained from Sk
1; therefore, L∗
1 ⊆L(G∗). On the other hand,
every string x in L(G∗) must be derivable from Sk
1 for some k ≥0, and we
may conclude that x ∈L(G1)k ⊆L(G1)∗, because the only productions in P
starting with S1 are the ones in G1.
EXAMPLE 4.10
The Language {aibjck | j ̸= i + k}
Let L be the language {aibjck | j ̸= i + k} ⊆{a, b, c}∗. The form of each element of L
might suggest that we try expressing L as the concatenation of three languages L1, L2,
and L3, which contain strings of a’s, strings of b’s, and strings of c’s, respectively. This
approach doesn’t work. Both a2b4c3 and a3b4c2 are in L; if a2 were in L1, b4 were in L2,
and c2 were in L3, then a2b4c2, which isn’t an element of L, would be in L1L2L3.
Instead, we start by noticing that j ̸= i + k means that either j > i + k or j < i + k,
so that L is the union
L = L1 ∪L2 = {aibjck | j > i + k} ∪{aibjck | j < i + k}
There’s still no way to express L1 or L2 as a threefold concatenation using the approach we
tried originally (see Exercise 4.11), but L1 can be expressed as a concatenation of a slightly
different form. Observe that for any natural numbers i and k,
aibi+kck = (aibi)(bkck)
and a string in L1 differs from a string like this only in having at least one extra b in the
middle. In other words,
L1 = MNP = {aibi | i ≥0} {bm | m > 0} {bkck | k ≥0}
and it will be easy to ﬁnd CFGs for the three languages M, N, and P.

138
C H A P T E R 4
Context-Free Languages
L2 is slightly more complicated. For a string aibjck in L1, knowing that j > i + k tells
us in particular that j > i; for a string aibjck in L2, such that j < i + k, it is helpful to
know either how j is related to i or how it is related to k. Let us also split L2 into a union
of two languages:
L2 = L3 ∪L4 = {aibjck | j < i} ∪{aibjck | i ≤j < i + k}
Now we can use the same approach for L3 and L4 as we used for L1. We can write a string
in L3 as
aibjck = (ai−j) (ajbj) (ck)
where i −j > 0, j ≥0, and k ≥0 (and these inequalities are the only constraints on i −j,
j, and k), and a string in L4 as
aibjck = (aibi) (bj−icj−i) (ck−j+i)
where i, j −i, and k −j + i are natural numbers that are arbitrary except that i > 0,
j −i ≥0, and k −j + i > 0 (i.e., k + i > j). It follows that
L3 = QRT = {ai | i > 0} {bici | i ≥0} {ci | i ≥0}
L4 = UV W = {aibi | i > 0} {bici | i ≥0} {ci | i > 0}
We have now expressed L as the union of the three languages L1, L3, and L4, and each of
these three can be written as the concatenation of three languages. The context-free grammar
we are looking for will have productions
S →S1 | S3 | S4
S1 →SMSNSP
S3 →SQSRST
S4 →SUSV SW
as well as productions that start with the nine variables SM, SN, . . ., SW. We present pro-
ductions to take care of the ﬁrst three of these and leave the last six to you.
SM →aSMb | 
SN →bSN | b
SP →bSNc | 
4.3 REGULAR LANGUAGES AND REGULAR
GRAMMARS
The three operations in Theorem 4.9 are the ones involved in the recursive deﬁni-
tion of regular languages (Deﬁnition 3.1). The “basic” regular languages over an
alphabet  (the languages ∅and {σ}, for every σ ∈) are context-free languages.
These two statements provide the ingredients that would be necessary for a proof
using structural induction that every regular language is a CFL.
EXAMPLE 4.11
A CFG Corresponding to a Regular Expression
Let L ⊆{a, b}∗be the language corresponding to the regular expression
bba(ab)∗+ (ab + ba∗b)∗ba

4.3
Regular Languages and Regular Grammars
139
A literal application of the constructions in Theorem 4.9 would be tedious: the expres-
sion ab + ba∗b alone involves all three operations, including three uses of concatena-
tion. We don’t really need to introduce separate variables for the languages {a} and {b},
and there are other similar shortcuts to reduce the number of variables in the grammar.
(Keep in mind that just as when we constructed an NFA for a given regular expres-
sion in Chapter 3, sometimes a literal application of the constructions helps to avoid
errors.)
Here we might start by introducing the productions S →S1 | S2, since the language is
a union of two languages L1 and L2. The productions
S1 →S1ab | bba
are sufﬁcient to generate L1. L2 is complicated enough that we introduce another variable
T for the language corresponding to ab + ba∗b, and the productions
S2 →T S2 | ba
will then take care of L2. Finally, the productions
T →ab | bUb
U →aU | 
are sufﬁcient to generate the language represented by T . The complete CFG for L has ﬁve
variables and the productions
S →S1 | S2
S1 →S1ab | bba
S2 →T S2 | ba
T →ab | bUb
U →aU | 
Not only can every regular language be generated by a context-free grammar,
but it can be generated by a CFG of a particularly simple form. To introduce this
type of grammar, we consider the ﬁnite automaton in Figure 4.12, which accepts
the language {a, b}∗{ba}.
a
a
b
b
a
b
S
A
B
Figure 4.12
An FA accepting {a, b}∗{ba}.

140
C H A P T E R 4
Context-Free Languages
The idea behind the corresponding CFG is that states in the FA correspond to
variables in the grammar, and for each transition
s
T
U
the grammar will have a production T →σU. The six transitions in the ﬁgure
result in the six productions
S →aS | bA
A →bA | aB
B →bA | aS
The only other productions in the grammar will be -productions (i.e., the right
side will be ), and so the current string in an uncompleted derivation will consist
of a string of terminal symbols and a single variable at the end. The sequence of
transitions
S
b→A
b→A
a→B
a→S
b→A
a→B
corresponds to the sequence of steps
S ⇒bA ⇒bbA ⇒bbaB ⇒bbaaS ⇒bbaabA ⇒bbaabaB
We can see better now how the correspondence between states and variables makes
sense: a state is the way an FA “remembers” how to react to the next input symbol,
and the variable at the end of the current string can be thought of as the state
of the derivation—the way the derivation remembers, for each possible terminal
symbol, how to generate the appropriate little piece of the string that should come
next.
The way the string bbaaba is ﬁnally accepted by the FA is that the current
state B is an accepting state, and the way the corresponding derivation terminates
is that the variable B at the end of the current string will be replaced by .
Deﬁnition 4.13
Regular Grammars
A context-free grammar G = (V, , S, P ) is regular if every production
is of the form A →σB or A →, where A, B ∈V and σ ∈.
Theorem 4.14
For every language L ⊂∗, L is regular if and only if L = L(G) for
some regular grammar G.
Proof
If L is a regular language, then for some ﬁnite automaton M = (Q, , q0,
A, δ), L = L(M). As in the grammar above for the FA in Figure 4.12,
we deﬁne G = (V, , S, P ) by letting V be Q, letting S be the initial
state q0, and letting P be the set containing a production T →aU for

4.4
Derivation Trees and Ambiguity
141
every transition δ(T, a) = U in M, and a production T → for every
accepting state T of M. G is a regular grammar. It is easy to see, just
as in the example, that for every x = a1a2 . . . an, the transitions on these
symbols that start at q0 end at an accepting state if and only if there is a
derivation of x in G; in other words, L = L(G).
In the other direction, if G is a regular grammar with L = L(G), we
can reverse the construction to produce M = (Q, , q0, A, δ). Q is the set
of variables of G, q0 is the start variable, A is the set of states (variables)
for which there are -productions in G, and for every production T →
σU there is a transition T
σ→U in M. We cannot expect that M is an
ordinary ﬁnite automaton, because for some combinations of T and a,
there may be either more than one or fewer than one U for which T →σU
is a production. But M is an NFA, and the argument in the ﬁrst part of the
proof is still sufﬁcient; for every string x, there is a sequence of transitions
involving the symbols of x that starts at q0 and ends in an accepting state
if and only if there is a derivation of x in G. Therefore, L is regular,
because it is the language accepted by the NFA M.
The word regular is sometimes used to describe grammars that are slightly
different from the ones in Deﬁnition 4.13. Grammars in which every production has
one of the forms A →σB, A →σ, or A → are equivalent to ﬁnite automata in
the same way that our regular grammars are. Grammars with only the ﬁrst two of
these types of productions generate regular languages, and for every language L,
L −{} can be generated by such a grammar. Similarly, a language L is regular if
and only if the set of nonnull strings in L can be generated by a grammar in which
all productions have the form A →xB or A →x, where A and B are variables
and x is a nonnull string of terminals. Grammars of this last type are also called
linear. Some of these variations are discussed in the exercises.
4.4 DERIVATION TREES AND AMBIGUITY
In most of our examples so far, we have been interested in what strings a context-
free grammar generates. As Example 4.2 suggests, it is also useful to consider how
a string is generated by a CFG. A derivation may provide information about the
structure of the string, and if a string has several possible derivations, one may be
more appropriate than another.
Just as diagramming a sentence might help to exhibit its grammatical structure,
drawing a tree to represent a derivation of a string helps to visualize the steps of
the derivation and the corresponding structure of the string. In a derivation tree,
the root node represents the start variable S. For each interior node N, the portion
of the tree consisting of N and its children represents a production A →α used
in the derivation. N represents the variable A, and the children, from left to right,
represent the symbols of α. (If the production is A →, the node N has a single
child representing .) Each leaf node in the tree represents either a terminal symbol

142
C H A P T E R 4
Context-Free Languages
or , and the string being derived is the one obtained by reading the leaf nodes
from left to right.
A derivation of a string x in a CFG is a sequence of steps, and a single
step is described by specifying the current string, a variable appearing in the
string, a particular occurrence of that variable, and the production starting with
that variable. We will adopt the phrase variable-occurrence to mean a particular
occurrence of a particular variable. (For example, if S and A are variables, the
string S + A ∗S contains three variable-occurrences.) Using this terminology, we
may say that a step in a derivation is determined by the current string, a particular
variable-occurrence in the string, and the production to be applied to that variable-
occurrence.
It is easy to see that for each derivation in a CFG, there is exactly one deriva-
tion tree. The derivation begins with the start symbol S, which corresponds to
the root node of the tree, and the children of that node are determined by the
ﬁrst production in the derivation. At each subsequent step, a production is applied,
involving a variable-occurrence corresponding to a node N in the tree; that pro-
duction determines the portion of the tree consisting of N and its children. For
example, the derivation
S ⇒S + S ⇒a + S ⇒a + (S) ⇒a + (S ∗S) ⇒a + (a ∗S) ⇒a + (a ∗a)
in the CFG for Expr in Example 4.2 corresponds to the derivation tree in Fig-
ure 4.15.
S
S
S
+
a
(
)
*
S
S
S
a
a
Figure 4.15
A derivation
tree for
a + (a ∗a) in
Example 4.2.
There are other derivations that also correspond to this tree. Every derivation
of the string a + (a ∗a) must begin
S ⇒S + S
but now there are two possible ways to proceed, since the next step could involve
either of the two occurrences of S. If we chose the rightmost one, so as to obtain
S + (S), we would still have two choices for the step after that.
When we said above, “if a string has several possible derivations, one may
be more appropriate than another”, we were referring, not to derivations that dif-
fered in this way, but to derivations corresponding to different derivation trees.
Among all the derivations corresponding to the derivation tree in Figure 4.15 (see
Exercise 4.30), there are no essential differences, but only differences having to
do with which occurrence of S we choose for the next step. We could eliminate
these choices by agreeing, at every step where the current string has more than one
variable-occurrence, to use the leftmost one.
Deﬁnition 4.16
Leftmost and Rightmost Derivations
A derivation in a context-free grammar is a leftmost derivation (LMD) if,
at each step, a production is applied to the leftmost variable-occurrence
in the current string. A rightmost derivation (RMD) is deﬁned similarly.

4.4
Derivation Trees and Ambiguity
143
Theorem 4.17
If G is a context-free grammar, then for every x ∈L(G), these three
statements are equivalent:
1. x has more than one derivation tree.
2. x has more than one leftmost derivation.
3. x has more than one rightmost derivation.
Proof
We will show that x has more than one derivation tree if and only if x has
more than one LMD, and the equivalence involving rightmost derivations
will follow similarly.
If there are two different derivation trees for the string x, each of
them has a corresponding leftmost derivation. The two LMDs must be
different—otherwise that derivation would correspond to two different
derivation trees, and this is impossible for any derivation, leftmost or
otherwise.
If there are two different leftmost derivations of x, let the correspond-
ing derivation trees be T1 and T2. Suppose that in the ﬁrst step where the
two derivations differ, this step is
xAβ ⇒xα1β
in one derivation and
xAβ ⇒xα2β
in the other. Here x is a string of terminals, because the derivations are
leftmost; A is a variable; and α1 ̸= α2. In both T1 and T2 there is a node
corresponding to the variable-occurrence A, and the respective portions
of the two trees to the left of this node must be identical, because the
leftmost derivations have been the same up to this point. These two nodes
have different sets of children, and so T1 ̸= T2.
Deﬁnition 4.18
Ambiguity in a CFG
A context-free grammar G is ambiguous if for at least one x ∈L(G), x
has more than one derivation tree (or, equivalently, more than one leftmost
derivation).
We will return to the algebraic-expression grammar of Example 4.2 shortly,
but ﬁrst we consider an example of ambiguity arising from the deﬁnition of if-
statements in Example 4.4.

144
C H A P T E R 4
Context-Free Languages
EXAMPLE 4.19
The Dangling else
In the C programming language, an if-statement can be deﬁned by these grammar rules:
S →if ( E ) S |
if ( E ) S else S |
OS
(In our notation, E is short for <expression>, S for <statement>, and OS
for
<otherstatement>.) A statement in C that illustrates the ambiguity of these rules is
if (e1) if (e2) f(); else g();
The problem is that although in C the else should be associated with the second if, as in
if (e1) { if (e2) f(); else g(); }
there is nothing in these grammar rules to rule out the other interpretation:
if (e1) { if (e2) f(); } else g();
The two derivation trees in Figure 4.21 show the two possible interpretations of the statement;
the correct one is in Figure 4.21a.
There are equivalent grammar rules that allow only the correct interpretation. One
possibility is
S →S1 | S2
S1 →if ( E ) S1 else S1 | OS
S2 →if ( E ) S |
if ( E ) S1 else S2
These rules generate the same strings as the original ones and are unambiguous. We will not
prove either fact, but you can see how the second might be true. The variable S1 represents a
statement in which every if is matched by a corresponding else, and every statement derived
from S2 contains at least one unmatched if. The only variable appearing before else in these
rules is S1; because the else cannot match any of the if s in the statement derived from S1,
it must match the if that appeared at the same time it did.
EXAMPLE 4.20
Ambiguity in the CFG for Expr in Example 4.2
In the grammar G in Example 4.2, with productions
S →a | S + S | S ∗S | (S)
the string a + a ∗a has the two leftmost derivations
S ⇒S + S ⇒a + S ⇒a + S ∗S ⇒a + a ∗S ⇒a + a ∗a
S ⇒S ∗S ⇒S + S ∗S ⇒a + S ∗S ⇒a + a ∗S ⇒a + a ∗a
which correspond to the derivation trees in Figure 4.22.

4.4
Derivation Trees and Ambiguity
145
S
E
if
e1
e2
g ( ) ;
f (  ) ;
(
)
S
)
E
(
if
else
S
S
S
E
if
e1
e2
f (  ) ;
(
)
g ( ) ;
S
else
S
)
E
(
if
S
(a)
(b)
Figure 4.21
Two possible interpretations of a dangling else.
We observed in Example 4.2 that the ﬁrst of these two LMDs (or the ﬁrst of the two
derivation trees) matches the interpretation of a + a ∗a as a sum, rather than as a product;
one reason for the ambiguity is that the grammar allows either of the two operations +
and ∗to be given higher precedence. Just as adding braces to the C statements in Example
4.19 allowed only the correct interpretation, the addition of parentheses in this expression,
to obtain a + (a ∗a), has the same effect; the only leftmost derivation of this string is
S ⇒S + S ⇒a + S ⇒a + (S) ⇒a + (S ∗S) ⇒a + (a ∗S) ⇒a + (a ∗a)
S
S
S
*
S
a
a
S
+
a
(a)
S
S
S
+
a
a
a
*
S
S
(b)
Figure 4.22
Two derivation
trees for
a + a ∗a in
Example 4.2.
Another rule in algebra that is not enforced by this grammar is the rule that says
operations of the same precedence are performed left-to-right. For this reason, both the
expressions a + a + a and a ∗a ∗a also illustrate the ambiguity of the grammar. The ﬁrst
expression has the (correct) LMD
S ⇒S + S ⇒S + S + S ⇒a + S + S ⇒a + a + S ⇒a + a + a

146
C H A P T E R 4
Context-Free Languages
corresponding to the interpretation (a + a) + a, as well as the LMD
S ⇒S + S ⇒a + S ⇒a + S + S ⇒a + a + S ⇒a + a + a
corresponding to the other way of grouping the terms.
In order to ﬁnd an unambiguous CFG G1 generating this language, we look for pro-
ductions that do not allow any choice regarding either the precedence of the two operators
or the order in which operators of the same precedence are performed. To some extent, we
can ignore the parentheses at least temporarily. Let S1 be the start symbol of G1.
Saying that multiplication should have higher precedence than addition means that when
there is any doubt, an expression should be treated as a sum rather than a product. For this
reason, we concentrate ﬁrst on productions that will give us sums of terms. The problem with
S1 →S1 + S1 is that if we are attempting to derive a sum of three or more terms, there are
too many choices as to how many terms will come from the ﬁrst S1 and how many from the
second. Saying that additions are performed left-to-right, or that + “associates to the left”,
suggests that we think of a sum of n terms as another sum plus one additional term. We try
S1 →S1 + T | T
where T represents a term—that is, an expression that may be part of a sum but is not itself
a sum.
It is probably clear already that we would no longer want S1 →S1 ∗S1, even if it didn’t
cause the same problems as S1 →S1 + S1; we don’t want to say that an expression can be
either a sum or a product, because that was one of the sources of ambiguity. Instead, we
say that terms can be products. Products of what? Factors. This suggests
T →T ∗F | F
where, in the same way that S1 + T is preferable to T + S1, T ∗F is preferable to F ∗T .
We now have a hierarchy with three levels: expressions are sums of terms, and terms are
products of factors. Furthermore, we have incorporated the rule that each operator associates
to the left.
What remains is to deal with a and expressions with parentheses. The productions
S1 →T and T →F allow us to have an expression with a single term and a term with a
single factor; thus, although a by itself is a valid expression, it is best to call it a factor,
because it is neither a product nor a sum. Similarly, an expression in parentheses should
also be considered a factor. What is in the parentheses should be an expression, because
once we introduce a pair of parentheses we can start all over with what’s inside.
The CFG that we have now obtained is G1 = (V, , S1, P), where V = {S1, T, F},
 = {a, +, ∗, (, )}, and P contains the productions
S1 →S1 + T | T
T →T ∗F | F
F →a | (S1)
Both halves of the statement L(G) = L(G1) can be proved by mathematical induction
on the length of a string. The details, particularly for the statement L(G) ⊆L(G1), are
somewhat involved and are left to the exercises.

4.4
Derivation Trees and Ambiguity
147
The rest of this section is devoted to proving that G1 is unambiguous. In the proof,
for a string x ∈L(G1), it is helpful to talk about a symbol in x that is within parenthe-
ses, and we want this to mean that it lies between the left and right parentheses that are
introduced in a single step of a derivation of x. One might ask, however, whether there
can be two derivations of x, and a symbol that has this property with respect to one of
the two but not the other. The purpose of Deﬁnition 4.23 and Theorem 4.24 is to estab-
lish that this formulation is a satisfactory one and that “within parentheses” doesn’t depend
on which derivation we are using. In the proof of Theorem 4.24, we will use the result
obtained in Example 1.25, that balanced strings of parentheses are precisely the strings
with equal numbers of left and right parentheses and no preﬁx having more right than left
parentheses.
Deﬁnition 4.23
The Mate of a Left Parenthesis
in a Balanced String
The mate of a left parenthesis in a balanced string is the ﬁrst right paren-
thesis
following
it
for
which
the
string
starting
with
the
left
and ending with the right has equal numbers of left and right parentheses.
(Every
left
parenthesis
in
a
balanced
string
has
a
mate—see
Exercise 4.41.)
Theorem 4.24
For every x ∈L(G1), every derivation of x in G1, and every step of this
derivation in which two parentheses are introduced, the right parenthesis
is the mate of the left.
Proof
Suppose x = x1(0z)0x2, where (0 and )0 are two occurrences of parenthe-
ses that are introduced in the same step in some derivation of x. Then
F ⇒(0S1)0 ⇒∗(0z)0. It follows that z is a balanced string, and by the
deﬁnition of “mate”, the mate of (0 cannot appear after )0.
If it appears before )0, however, then z = z1)1z2, for some strings z1
and z2, where )1 is the mate of (0 and z1 has equal numbers of left and
right parentheses. This implies that the preﬁx z1)1 of z has more right
parentheses than left, which is impossible if z is balanced. Therefore, )0
is the mate of (0.
Deﬁnition 4.23 and Theorem 4.24 allow us to say that “between the two
parentheses produced in a single step of a derivation” is equivalent to “between
some left parenthesis and its mate”, so that either of these can be taken as the
deﬁnition of “within parentheses”.

148
C H A P T E R 4
Context-Free Languages
Theorem 4.25
The context-free grammar G1 with productions
S1 →S1 + T | T
T →T ∗F | F
F →a | (S1)
is unambiguous.
Proof
We prove the following statement, which implies the result: For every
x derivable from one of the variables S1, T , or F in G1, x has only
one leftmost derivation from that variable. The proof is by mathematical
induction on |x|. The basis step, for x = a, is easy, because for each of
the three variables, a has exactly one derivation from that variable.
The induction hypothesis is that k ≥1 and that for every y derivable
from S1, T , or F for which |y| ≤k, y has only one leftmost derivation
from that variable. We wish to show that the same is true for a string
x ∈L(G1) with |x| = k + 1.
We start with the case in which x contains at least one occurrence of
+ that is not within parentheses. Because the only occurrences of + in
strings derivable from T or F are within parentheses, every derivation of
x must begin S1 ⇒S1 + T , and the occurrence of + in this step is the
last one in x that is not within parentheses. Every leftmost derivation of
x from S1 must then have the form
S1 ⇒S1 + T ⇒∗y + T ⇒∗y + z
where the last two steps represent leftmost derivations of y from S1 and z
from T , respectively, and the + is still the last one not within parentheses.
According to the induction hypothesis, y has only one leftmost derivation
from S1, and z has only one from T ; therefore, x has only one LMD
from S1.
Next we consider the case in which every occurrence of + in x is
within parentheses but there is at least one occurrence of ∗not within
parentheses. Because x cannot be derived from F, every derivation of x
from S1 must begin S1 ⇒T ⇒T ∗F, and every derivation from T must
begin T ⇒T ∗F. In either case, this occurrence of ∗must be the last
one in x not within parentheses. As before, the subsequent steps of every
LMD must be
T ∗F ⇒∗y ∗F ⇒∗y ∗z
in which the derivations of y from T and z from F are both leftmost. The
induction hypothesis tells us that there is only one way for each of these
derivations to proceed, and that there is only one leftmost derivation of x
from S1 or T .

4.5
Simpliﬁed Forms and Normal Forms
149
Finally, suppose that every occurrence of + or ∗in x is within paren-
theses. Then x can be derived from any of the three variables, but every
derivation from S1 begins
S1 ⇒T ⇒F ⇒(S1)
and every derivation from T or F begins the same way but with the ﬁrst
one or two steps omitted. Therefore, x = (y), where S1 ⇒∗y. By the
induction hypothesis, y has only one LMD from S1, and it follows that x
has only one from each of the three variables.
4.5 SIMPLIFIED FORMS AND NORMAL
FORMS
Questions about the strings generated by a context-free grammar G are some-
times easier to answer if we know something about the form of the productions.
Sometimes this means knowing that certain types of productions never occur, and
sometimes it means knowing that every production has a certain simple form. For
example, suppose we want to know whether a string x is generated by G, and we
look for an answer by trying all derivations with one step, then all derivations with
two steps, and so on. If we don’t ﬁnd a derivation that produces x, how long do
we have to keep trying?
The number t of terminals in the current string of a derivation cannot decrease
at any step. If G has no -productions (of the form A →), then the length l
of the current string can’t decrease either, which means that the sum t + l can’t
decrease. If we also know that G has no “unit productions” (of the form A →B,
where A and B are variables), then the sum must increase, because every step
either adds a terminal or increases the number of variables. A derivation of x starts
with S, for which l + t = 1, and ends with x, for which l + t = 2|x|; therefore, no
derivation has more than 2|x| −1 steps. If we try all derivations with this many
steps or fewer and don’t ﬁnd one that generates x, we may conclude that x /∈L(G).
In this section we show how to modify an arbitrary CFG G so that the modiﬁed
grammar has no productions of either of these types but still generates L(G), except
possibly for the string . We conclude by showing how to modify G further
(eliminating both these types of productions is the ﬁrst step) so as to obtain a CFG
that is still essentially equivalent to G but is in Chomsky normal form, so that every
production has one of two very simple forms.
A simple example will suggest the idea of the algorithm to eliminate -
productions. Suppose one of the productions in G is
A →BCDCB
and that from both the variables B and C,  can be derived (in one or more steps),
as well as other nonnull strings of terminals. Once the algorithm has been applied,
the steps that replace B and C by  will no longer be possible, but we must still
be able to get all the nonnull strings from A that we could have gotten using these

150
C H A P T E R 4
Context-Free Languages
steps. This means that we should retain the production A →BCDCB, but we
should add A →CDCB (because we could have replaced the ﬁrst B by  and
the other occurrences of B and C by nonnull strings), A →DCB, A →CDB,
and so on—every one of the ﬁfteen productions that result from leaving out one
or more of the four occurrences of B and C in the right-hand string.
The necessary ﬁrst step, of course, is to identify the variables like B and C
from which  can be derived. We will refer to these as nullable variables. It is
easy to see that the set of nullable variables can be obtained using the following
recursive deﬁnition.
Deﬁnition 4.26
A Recursive Deﬁnition of the Set of
Nullable Variables of G
1. Every variable A for which there is a production A → is nullable.
2. If A1, A2, . . . , Ak are nullable variables (not necessarily distinct), and
B →A1A2 . . . Ak
is a production, then B is nullable.
This deﬁnition leads immediately to an algorithm for identifying the nullable
variables (see Example 1.21).
Theorem 4.27
For every context-free grammar G = (V, , S, P ), the following algo-
rithm produces a CFG G1 = (V, , S, P1) having no -productions and
satisfying L(G1) = L(G) −{}.
1. Identify the nullable variables in V , and initialize P1 to P.
2. For every production A →α in P, add to P1 every production obtained
from this one by deleting from α one or more variable-occurrences involving
a nullable variable.
3. Delete every -production from P1, as well as every production of the form
A →A.
Proof
It is obvious that G1 has no -productions. We show that for every A ∈V
and every nonnull x ∈∗, A ⇒∗
G x if and only if A ⇒∗
G1 x.
First we show, using mathematical induction on n, that for every
n ≥1, every A ∈V , and every x ∈∗with x ̸= , if A ⇒n
G x, then
A ⇒∗
G1 x. For the basis step, suppose A ⇒1
G x. Then A →x is a produc-
tion in P , and since x ̸= , this production is also in P1.
Suppose that k ≥1 and that for every n ≤k, every variable A, and
every nonnull x ∈∗for which A ⇒n
G x, A ⇒∗
G1 x. Now suppose that

4.5
Simpliﬁed Forms and Normal Forms
151
A ⇒k+1
G
x, for some x ∈∗−{}. Let the ﬁrst step in some (k + 1)-step
derivation of x from A in G be
A ⇒X1X2 . . . Xm
where each Xi is either a variable or a terminal. Then x = x1x2 . . . xm,
where for each i, either xi = Xi (a terminal), or xi is a string deriv-
able from the variable Xi in k or fewer steps. When all the (nullable)
variables Xi for which the corresponding xi is  are deleted from the
string X1X2 . . . Xm, there are still some Xi’s left, because x ̸= , and
so the resulting production is an element of P1. Furthermore, the induc-
tion hypothesis tells us that for each variable Xi remaining, Xi ⇒∗
G1 xi.
Therefore, A ⇒∗
G1 x.
For the converse, we show, using mathematical induction on n, that
for every n ≥1, every variable A, and every x ∈∗, if A ⇒n
G1 x, then
A ⇒∗
G x. If A ⇒1
G1 x, then A →x is a production in P1. It follows that
A →α is a production in P , where α is a string from which x can
be obtained by deleting zero or more occurrences of nullable variables.
Therefore, A ⇒∗
G x, because we can begin a derivation with the pro-
duction A →α and continue by deriving  from each of the nullable
variables that was deleted from α to obtain x.
Suppose that k ≥1, that for every n ≤k, every A, and every string
x with A ⇒n
G1 x, A ⇒∗
G x, and that A ⇒k+1
G1 x. Again, let the ﬁrst step
of some (k + 1)-step derivation of x from A in G1 be
A ⇒X1X2 . . . Xm
Then x = x1x2 . . . xm, where for each i, either xi = Xi or xi is a string
derivable from the variable Xi in the grammar G1 in k or fewer steps. By
the induction hypothesis, Xi ⇒∗
G xi for each i. By deﬁnition of G1, there
is a production A →α in P so that X1X2 . . . Xm can be obtained from α
by deleting some variable-occurrences involving nullable variables. This
implies that A ⇒∗
G X1X2 . . . Xm, and therefore that we can derive x from
A in G, by ﬁrst deriving X1 . . . Xm and then deriving each xi from the
corresponding Xi.
The procedure we use to eliminate unit productions from a CFG is rather
similar. We ﬁrst identify pairs of variables (A, B) for which A ⇒∗B (not only
those for which A →B is a production); then, for each such pair (A, B), and each
nonunit production B →α, we add the production A →α.
If we make the simplifying assumption that we have already eliminated -
productions from the grammar, then a sequence of steps by which A ⇒∗B involves
only unit productions. This allows us, for a variable A, to formulate the following
recursive deﬁnition of an “A-derivable” variable (a variable B different from A for
which A ⇒∗B):

152
C H A P T E R 4
Context-Free Languages
1.
If A →B is a production and B ̸= A, then B is A-derivable.
2.
If C is A-derivable, C →B is a production, and B ̸= A, then B is
A-derivable.
3.
No other variables are A-derivable.
Theorem 4.28
For every context-free grammar G = (V, , S, P ) without -productions,
the CFG G1 = (V, , S, P1) produced by the following algorithm gener-
ates the same language as G and has no unit productions.
1. Initialize P1 to be P, and for each A ∈V , identify the A-derivable variables.
2. For every pair (A, B) of variables for which B is A-derivable, and every
nonunit production B →α, add the production A →α to P1.
3. Delete all unit productions from P1.
Proof
The proof that L(G1) = L(G) is a straightforward induction proof and is
omitted.
Deﬁnition 4.29
Chomsky Normal Form
A context-free grammar is said to be in Chomsky normal form if every
production is of one of these two types:
A →BC
(where B and C are variables)
A →σ
(where σ is a terminal symbol)
Theorem 4.30
For every context-free grammar G, there is another CFG G1 in Chomsky
normal form such that L(G1) = L(G) −{}.
Proof
We describe brieﬂy the algorithm that can be used to construct the gram-
mar G1, and it is not hard to see that it generates the same language as
G, except possibly for the string .
The ﬁrst step is to apply the algorithms presented in Theorems 4.6
and 4.7 to eliminate -productions and unit productions. The second step
is to introduce for every terminal symbol σ a variable Xσ and a produc-
tion Xσ →σ, and in every production whose right side has at least two
symbols, to replace every occurrence of a terminal by the corresponding

4.5
Simpliﬁed Forms and Normal Forms
153
variable. At this point, every production looks like either A →σ or
A →B1B2 . . . Bk
where the Bi’s are variables and k ≥2.
The last step is to replace each production having more than two
variable-occurrences on the right by an equivalent set of productions, each
of which has exactly two variable-occurrences. This step is described best
by an example. The production
A →BACBDCBA
would be replaced by
A →BY1
Y1 →AY2
Y2 →CY3
Y3 →BY4
Y4 →DY5
Y5 →CY6
Y6 →BA
where the new variables Y1, . . . , Y6 are speciﬁc to this production and are
not used anywhere else.
EXAMPLE 4.31
Converting a CFG to Chomsky Normal Form
This example will illustrate all the algorithms described in this section: to identify nullable
variables, to eliminate -productions, to identify A-derivable variables for each A, to elim-
inate unit productions, and to convert to Chomsky normal form. Let G be the context-free
grammar with productions
S →T U | V
T →aT b | 
U →cU | 
V →aV c | W
W →bW | 
which can be seen to generate the language {aibjck | i = j or i = k}.
1.
(Identifying nullable variables) The variables T , U, and W are nullable because they
are involved in -productions; V is nullable because of the production V →W; and
S is also, either because of the production S →T U or because of S →V . So all the
variables are!
2.
(Eliminating -productions) Before the -productions are eliminated, the following
productions are added:
S →T
S →U
T →ab
U →c
V →ac
W →b
After eliminating -productions, we are left with
S →T U | T | U | V
T →aT b | ab
U →cU | c
V →aV c | ac | W
W →bW | b

154
C H A P T E R 4
Context-Free Languages
3.
(Identifying A-derivable variables, for each A) The S-derivable variables obviously
include T , U, and V , and they also include W because of the production V →W.
The V -derivable variable is W.
4.
(Eliminating unit productions) We add the productions
S →aT b | ab | cU | c | aV c | ac | bW | b
V →bW | b
before eliminating unit productions. At this stage, we have
S →T U | aT b | ab | cU | c | aV c | ac | bW | b
T →aT b | ab
U →cU | c
V →aV c | ac | bW | b
W →bW | b
5.
(Converting to Chomsky normal form) We replace a, b, and c by Xa, Xb, and Xc,
respectively, in productions whose right sides are not single terminals, obtaining
S →T U | XaT Xb | XaXb | XcU | c | XaV Xc | XaXc | XbW | b
T →XaT Xb | XaXb
U →XcU | c
V →XaV Xc | XaXc | XbW | b
W →XbW | b
This grammar fails to be in Chomsky normal form only because of the productions
S →XaT Xb, S →XaV Xc, T →XaT Xb, and V →XaV Xc. When we take care of
these as described above, we obtain the ﬁnal CFG G1 with productions
S →T U | XaY1 | XaXb | XcU | c | XaY2 | XaXc | XbW | b
Y1 →T Xb
Y2 →V Xc
T →XaY3 | XaXb
Y3 →T Xb
U →XcU | c
V →XaY4 | XaXc | XbW | b
Y4 →V Xc
W →XbW | b
(We obviously don’t need both Y1 and Y3, and we don’t need both Y2 and Y4, so we
could simplify G1 slightly.)
EXERCISES
4.1.
In each case below, say what language (a subset of {a, b}∗) is generated
by the context-free grammar with the indicated productions.

Exercises
155
a. S →aS | bS | 
b. S →SS | bS | a
c. S →SaS | b
d. S →SaS | b | 
e. S →T T
T →aT | T a | b
f. S →aSa | bSb | aAb | bAa
A →aAa | bAb | a | b | 
g. S →aT | bT | 
T →aS | bS
h. S →aT | bT
T →aS | bS | 
4.2.
Find a context-free grammar corresponding to the “syntax diagram” in
Figure 4.32.
4.3.
In each case below, ﬁnd a CFG generating the given language.
a. The set of odd-length strings in {a, b}∗with middle symbol a.
b. The set of even-length strings in {a, b}∗with the two middle symbols
equal.
c. The set of odd-length strings in {a, b}∗whose ﬁrst, middle, and last
symbols are all the same.
4.4.
In both parts below, the productions in a CFG G are given. In each part,
show ﬁrst that for every string x ∈L(G), na(x) = nb(x); then ﬁnd a string
x ∈{a, b}∗with na(x) = nb(x) that is not in L(G).
a. S →SabS | SbaS | 
b. S →aSb | bSa | abS | baS | Sab | Sba | 
S
a
b
c
d
S
e
A
A
f
g
j
i
h
Figure 4.32

156
C H A P T E R 4
Context-Free Languages
4.5.
Consider the CFG with productions
S →aSbScS | aScSbS | bSaScS | bScSaS | cSaSbS | cSbSaS | 
Does this generate the language {x ∈{a, b, c}∗| na(x) = nb(x) = nc(x)}?
Prove your answer.
4.6.
Show that the language of all nonpalindromes over {a, b} (see Example
4.3) cannot be generated by any CFG in which S →aSa | bSb are the
only productions with variables on the right side.
4.7.
Describe the language generated by the CFG with productions
S →ST | 
T →aS | bT | b
Give an induction proof that your answer is correct.
4.8.
What language over {a, b} does the CFG with productions
S →aaS | bbS | Saa | Sbb | abSab | abSba | baSba | baSab | 
generate? Prove your answer.
4.9.
Suppose that G1 = (V1, {a, b}, S1, P1) and G2 = (V2, {a, b}, S2, P2) are
CFGs and that V1 ∩V2 = ∅.
a. It is easy to see that no matter what G1 and G2 are, the CFG
Gu = (Vu, {a, b}, Su, Pu) deﬁned by Vu = V1 ∪V2, Su = S1, and
Pu = P1 ∪P2 ∪{S1 →S2} generates every string in L(G1) ∪L(G2).
Find grammars G1 and G2 (you can use V1 = {S1} and V2 = {S2}) and
a string x ∈L(Gu) such that x /∈L(G1) ∪L(G2).
b. As in part (a), the CFG Gc = (Vc, {a, b}, Sc, Pc) deﬁned by
Vc = V1 ∪V2, Sc = S1, and Pc = P1 ∪P2 ∪{S1 →S1S2} generates
every string in L(G1)L(G2). Find grammars G1 and G2 (again with
V1 = {S1} and V2 = {S2}) and a string x ∈L(Gc) such that
x /∈L(G1)L(G2).
c. The CFG G∗= (V, {a, b}, S, P ) deﬁned by V = V1, S = S1, and
P = P1 ∪{S1 →S1S1 | } generates every string in L(G1)∗. Find a
grammar G1 with V1 = {S1} and a string x ∈L(G∗) such that
x /∈L(G)∗.
4.10.
Find context-free grammars generating each of the languages below.
a. {aibj | i ≤j}
b. {aibj | i < j}
c. {aibj | j = 2i}
d. {aibj | i ≤j ≤2i}
e. {aibj | j ≤2i}
f. {aibj | j < 2i}
4.11.
a. Show that the language L = {aibjck | j > i + k} cannot be written in
the form L = L1L2L3, where L1, L2, and L3 are subsets of {a}∗, {b}∗,
and {c}∗, respectively.
b. Show the same thing for the language L = {aibjck | j < i + k}.

Exercises
157
4.12.
Find a context-free grammar generating the language {aibjck | i ̸= j + k}.
4.13.
†Find context-free grammars generating each of these languages, and
prove that your answers are correct.
a. {aibj | i ≤j ≤3i/2}
b. {aibj | i/2 ≤j ≤3i/2}
4.14.
Let L be the language generated by the CFG with productions
S →aSb | ab | SS
Show that no string in L begins with abb.
4.15.
Show using mathematical induction that every string produced by the
context-free grammar with productions
S →a | aS | bSS | SSb | SbS
has more a’s than b’s.
4.16.
Prove that the CFG with productions S →aSbS | aSbS |  generates the
language L = {x ∈{a, b}∗| na(x) = nb(x)}.
4.17.
†Show that the CFG with productions
S →aSaSbS | aSbSaS | bSaSaS | 
generates the language {x ∈{a, b}∗| na(x) = 2nb(x)}.
4.18.
†Show that the following CFG generates the language {x ∈{a, b}∗|
na(x) = 2nb(x)}.
S →SS | bT T | T bT | T T b | 
T →aS | SaS | Sa | a
4.19.
Let G be the CFG with productions S →a | aS | bSS | SSb | SbS. Show
that every x in {a, b}∗with na(x) > nb(x) is an element of L(G).
4.20.
Let G be the context-free grammar with productions S →SaT | 
T →T bS | . Show using mathematical induction that L(G) is the
language of all strings in {a, b}∗that don’t start with b. One direction is
easy. For the other direction, it might be easiest to prove two statements
simultaneously: (i) every string that doesn’t start with b can be derived
from S; (ii) every string that doesn’t start with a can be derived
from T .
4.21.
Deﬁnition 3.1 and Theorem 4.9 provide the ingredients for a structural-
induction proof that every regular language is a CFL. Give the proof.
4.22.
Show that if G is a context-free grammar in which every production has
one of the forms A →aB, A →a, and A → (where A and B are
variables and a is a terminal), then L(G) is regular. Suggestion: construct
an NFA accepting L(G), in which there is a state for each variable in G
and one additional state F, the only accepting state.
4.23.
Suppose L ⊆∗. Show that L is regular if and only if L = L(G) for
some context-free grammar G in which every production is either of the
form A →Ba or of the form A →, where A and B are variables and a

158
C H A P T E R 4
Context-Free Languages
is a terminal. (The grammars in Deﬁnition 4.13 are sometimes called
right-regular grammars, and the ones in this exercise are sometimes called
left-regular.)
4.24.
Let us call a context-free grammar G a grammar of type R if every
production has one of the three forms A →aB, A →Ba, or A →
(where A and B are variables and a is a terminal). Every regular language
can be generated by a grammar of type R. Is every language that is
generated by a grammar of type R regular? If so, give a proof; if not, ﬁnd
a counterexample.
4.25.
Show that for a language L ⊆∗, the following statements are equivalent.
a. L is regular.
b. L can be generated by a grammar in which all productions are either of
the form A →xB or of the form A → (where A and B are
variables and x ∈∗).
c. L can be generated by a grammar in which all productions are either of
the form A →Bx or of the form A → (where A and B are
variables and x ∈∗).
4.26.
In each part, draw an NFA (which might be an FA) accepting the language
generated by the CFG having the given productions.
a. S →aA | bC
A →aS | bB
B →aC | bA
C →aB | bS | 
b. S →bS | aA | 
A →aA | bB | b
B →bS
4.27.
Find a regular grammar generating the language L(M), where M is the
FA shown in Figure 4.33.
4.28.
Draw an NFA accepting the language generated by the grammar with
productions
S →abA | bB | aba
A →b | aB | bA
B →aB | aA
II
a
a
b
b
a, b
A
B
C
D
b
a
Figure 4.33

Exercises
159
4.29.
Each of the following grammars, though not regular, generates a regular
language. In each case, ﬁnd a regular grammar generating the language.
a. S →SSS | a | ab
b. S →AabB
A →aA | bA | 
B →Bab | Bb | ab | b
c. S →AAS | ab | aab
A →ab | ba | 
d. S →AB
A →aAa | bAb | a | b
B →aB | bB | 
e. S →AA | B
A →AAA | Ab | bA | a
B →bB | 
4.30.
a. Write the rightmost derivation of the string a + (a ∗a) corresponding
to the derivation tree in Figure 4.15.
b. How many distinct derivations (not necessarily leftmost or rightmost)
does the string a + (a ∗a) have in the CFG with productions
S →a | S + S | S ∗S | (S)?
4.31.
Again we consider the CFG in Example 4.2, with productions
S →a | S + S | S ∗S | (S).
a. How many distinct derivation trees does the string a + (a ∗a)/a −a
have in this grammar?
b. How many derivation trees are there for the string (a + (a + a))+
(a + a)?
4.32.
In the CFG in the previous exercise, suppose we deﬁne ni to be the
number of distinct derivation trees for the string a + a + . . . + a in which
there are i a’s. Then n1 = n2 = 1.
a. Find a recursive formula for ni, by ﬁrst observing that if i > 1 the root
of a derivation tree has two children labeled S, and then considering all
possibilities for the two subtrees.
b. How many derivation trees are there for the string a + a + a + a + a?
c. How many derivation trees are there for the string
a + a + a + a + a + a + a + a + a + a?
4.33.
Consider the C statements
x = 1; if (a > 2) if (a > 4) x = 2; else x = 3;
a. What is the resulting value of x if these statements are interpreted
according to the derivation tree in Figure 4.21a and a = 3?
b. Same question as in (a), but when a = 1.
c. What is the resulting value of x if these statements are interpreted
according to the derivation tree in Figure 4.21b and a = 3?
d. Same question as in (c), but when a = 1.
4.34.
Show that the CFG with productions
S →a | Sa | bSS | SSb | SbS
is ambiguous.

160
C H A P T E R 4
Context-Free Languages
4.35.
Consider the context-free grammar with productions
S →AB
A →aA | 
B →ab | bB | 
Every derivation of a string in this grammar must begin with the
production S →AB. Clearly, any string derivable from A has only one
derivation from A, and likewise for B. Therefore, the grammar is
unambiguous. True or false? Why?
4.36.
For each part of Exercise 4.1, decide whether the grammar is ambiguous
or not, and prove your answer.
4.37.
Show that the CFG in Example 4.8 is ambiguous.
4.38.
In each case below, show that the grammar is ambiguous, and ﬁnd an
equivalent unambiguous grammar.
a. S →SS | a | b
b. S →ABA
A →aA | 
B →bB | 
c. S →aSb | aaSb | 
d. S →aSb | abS | 
4.39.
Describe an algorithm for starting with a regular grammar and ﬁnding an
equivalent unambiguous grammar.
In the exercises that follow, take as the deﬁnition of “balanced string of paren-
theses” the criterion in Example 1.25: The string has an equal number of left and
right parentheses, and no preﬁx has more right than left.
4.40.
a. Show that for every string x of left and right parentheses, x is a preﬁx
of a balanced string if and only if no preﬁx of x has more right
parentheses than left.
b. Show that the language of preﬁxes of balanced strings of parentheses is
generated by the CFG with productions S →(S)S | (S | .
4.41.
Show that every left parenthesis in a balanced string has a mate.
4.42.
Show that if x is a balanced string of parentheses, then either x = (y) for
some balanced string y or x = x1x2 for two balanced strings x1 and x2
that are both shorter than x.
4.43.
Show that if (0 is an occurrence of a left parenthesis in a balanced string,
and )0 is its mate, then (0 is the rightmost left parentheses for which the
string consisting of it and )0 and everything in between is balanced.
4.44.
†Show that both of the CFGs below generate the language of balanced
strings of parentheses.
a. The CFG with productions S →S(S) | 
b. The CFG with productions S →(S)S | 
4.45.
Show that both of the CFGs in the previous exercise are unambiguous.

Exercises
161
4.46.
Let x be a string of left and right parentheses. A complete pairing of x is
a partition of the parentheses of x in to pairs such that (i) each pair
consists of one left parenthesis and one right parenthesis appearing
somewhere after it; and (ii) the parentheses between those in a pair are
themselves the union of pairs. Two parentheses in a pair are said to be
mates with respect to that pairing.
a. Show that there is at most one complete pairing of a string of
parentheses.
b. Show that a string of parentheses has a complete pairing if and only if
it is a balanced string, and in this case the two deﬁnitions of mates
coincide.
4.47.
†Let G be the CFG with productions
S →S + S | S ∗S | (S) | a
and G1 the CFG with productions
S1 →S1 + T | T
T →T ∗F | F
F →(S1) | a
(see Section 4.4).
a. Show that L(G1) ⊆L(G). One approach is to use induction and to
consider three cases in the induction step, corresponding to the three
possible ways a derivation of the string in L(G1) might begin:
S1 ⇒S1 + T
S1 ⇒T
⇒T ∗F
S1 ⇒T
⇒F ⇒(S1)
b. Show that L(G) ⊆L(G1). Again three cases are appropriate in the
induction step:
i.
x has a derivation in G beginning S ⇒(S)
ii.
x has a derivation beginning S ⇒S + S
iii.
Every derivation of x in G begins S ⇒S ∗S
In the second case it may be helpful to let x = x1 + x2 + . . . + xn,
where each xi ∈L(G1) and n is as large as possible. In the third case
it may be helpful to let x = x1 ∗x2 ∗. . . xn, where each xi ∈L(G) (not
L(G1)) and n is as large as possible.
4.48.
Show that the nullable variables deﬁned by Deﬁnition 4.7 are precisely
those variables A for which A ⇒∗.
4.49.
In each case below, ﬁnd a context-free grammar with no -productions
that generates the same language, except possibly for , as the given CFG.
a. S →AB | 
A →aASb | a
B →bS
b. S →AB | ABC
A →BA | BC |  | a
B →AC | CB |  | b
C →BC | AB | A | c

162
C H A P T E R 4
Context-Free Languages
4.50.
In each case, given the context-free grammar G, ﬁnd a CFG G′ with no
-productions and no unit productions that generates the language
L(G) −{}.
a. G has productions
S →ABA
A →aA | 
B →bB | 
b. G has productions
S →aSa | bSb | 
A →aBb | bBa
B →aB | bB | 
c. G has productions
S →A | B | C
A →aAa | B
B →bB | bb
C →aCaa | D
D →baD | abD | aa
4.51.
A variable A in a context-free grammar G = (V, , S, P ) is live if
A ⇒∗x for some x ∈∗. Give a recursive deﬁnition, and a corresponding
algorithm, for ﬁnding all live variables in G.
4.52.
A variable A in a context-free grammar G = (V, , S, P ) is reachable if
S ⇒∗αAβ for some α, β ∈( ∪V )∗. Give a recursive deﬁnition, and a
corresponding algorithm, for ﬁnding all reachable variables in G.
4.53.
†A variable A is a context-free grammar G = (V, , S, P ) is useful if for
some string x ∈∗, there is a derivation of x that takes the form
S ⇒∗αAβ ⇒∗x
A variable that is not useful is useless. Clearly if a variable is either not
live or not reachable (see the two preceding exercises), then it is useless.
The converse is not true, as the grammar with productions S →AB and
A →a illustrates. (The variable A is both live and reachable but still
useless.)
a. Let G be a CFG. Suppose G1 is obtained by eliminating all dead
variables from G and eliminating all productions in which dead
variables appear. Suppose G2 is then obtained from G1 by eliminating
all variables unreachable in G1, as well as productions in which such
variables appear. Show that G2 contains no useless variables, and
L(G2) = L(G).
b. Give an example to show that if the two steps are done in the opposite
order, the resulting grammar may still have useless variables.
c. In each case, given the context-free grammar G, ﬁnd an equivalent
CFG with no useless variables.
i.
G has productions
S →ABC | BaB
A →aA | BaC | aaa
B →bBb | a
C →CA | AC
ii.
G has productions
S →AB | AC
A →aAb | bAa | a
B →bbA | aaB | AB
C →abCa | aDb
D →bD | aC

Exercises
163
4.54.
In each case below, given the context-free grammar G, ﬁnd a CFG G1 in
Chomsky normal form generating L(G) −{}.
a. G has productions S →SS | (S) | 
b. G has productions S →S(S) | 
c. G has productions
S →AaA | CA | BaB
A →aaBa | CDA | aa | DC
B →bB | bAB | bb | aS
C →Ca | bC | D
D →bD | 
4.55.
For alphabets 1 and 2, a homomorphism from ∗
1 to ∗
2 is deﬁned in
Exercise 3.53. Show that if f : ∗
1 →∗
2 is a homomorphism and
L ⊆∗
1 is a context-free language, then f (L) ⊆∗
2 is also a CFG.
4.56.
†Let G be the context-free grammar with productions
S →aS | aSbS | c
and let G1 be the one with productions
S1 →T | U
T →aT bT | c
U →aS1 | aT bU
(G1 is a simpliﬁed version of the second grammar in Example 4.19.)
a. Show that G is ambiguous.
b. Show that G and G1 generate the same language.
c. Show that G1 is unambiguous.
4.57.
†Show that if a context-free grammar is unambiguous, then the grammar
obtained from it by the algorithm in Theorem 4.27 is also unambiguous.
4.58.
†Show that if a context-free grammar with no -productions is
unambiguous, then the one obtained from it by the algorithm in
Theorem 4.28 is also unambiguous.

164
C
H
A
P
T
E
R
5
Pushdown Automata
A
language can be generated by a context-free grammar precisely if it can be
accepted by a pushdown automaton, which is similar in some respects to a
ﬁnite automaton but has an auxiliary memory that operates according to the rules
of a stack. The default mode in a pushdown automaton (PDA) is to allow nondeter-
minism, and unlike the case of ﬁnite automata, the nondeterminism cannot always
be eliminated. We give the deﬁnition of a pushdown automaton and consider a few
simple examples, both with and without nondeterminism. We then describe two
ways of obtaining a PDA from a given context-free grammar, so that in each case
the moves made by the device as it accepts a string are closely related to a deriva-
tion of the string in the grammar. We also discuss the reverse construction, which
produces a context-free grammar corresponding to a given PDA. Both methods of
obtaining a PDA from a context-free grammar produce nondeterministic devices
in general, but we present two examples in which well-behaved grammars allow
us to eliminate the nondeterminism, so as to produce a parser for the language.
5.1 DEFINITIONS AND EXAMPLES
In this chapter we will describe how an abstract computing device called a push-
down automaton (PDA) accepts a language, and the languages that can be accepted
this way are precisely the context-free languages. To introduce these devices, we
will start with two simple CFLs that are not regular and extend the deﬁnition of a
ﬁnite automaton in a way that will allow the more general device, when restricted
like an FA to a single left-to-right scan of the input string, to remember enough
information to accept the language.
Consider the languages
AnBn = {anbn | n ≥0}
SimplePal = {xcxr | x ∈{a, b}∗}

5.1
Deﬁnitions and Examples
165
AnBn was the ﬁrst example of a context-free language presented in Chapter 4;
SimplePal, whose elements are palindromes of a particularly simple type, is just
as easy to describe by a context-free grammar. For this discussion, though, we
will not refer directly to grammars, just as we constructed examples of FAs at ﬁrst
without knowing how to obtain one from an arbitrary regular expression.
When a pushdown automaton reads an input symbol, it will be able to save it
(or perhaps save one or more other symbols) in its memory. In processing an input
string that might be in AnBn, all we need to remember is the number of a’s, but
saving the a’s themselves is about as easy a way as any to do that. For SimplePal, it
seems clear that we really do need to remember the individual symbols themselves.
Suppose a pushdown automaton is attempting to decide whether an input string
is in AnBn and that it has read and saved a number of a’s. If it now reads the
input symbol b, two things should happen. First, it should change states to register
the fact that from now on, the only legal input symbols are b’s. This is the way it
can remember that it has read at least one b. Second, it should delete one of the
a’s from its memory, because one fewer b is now required in order for the input
string to be accepted. Obviously, it doesn’t matter which a it deletes.
Now suppose a PDA is processing an input string that might be an element of
SimplePal and that it has read and saved a string of a’s and b’s. When it reads a c,
a change of state is appropriate, because now each symbol it reads will be handled
differently: Another c will be illegal, and an a or b will not be saved but used to
match a symbol in the memory. Which one? This time it does matter; the symbol
used should be the one that was saved most recently.
In both these examples, it will be sufﬁcient for the memory to operate like a
stack, which uses a last-in-ﬁrst-out rule. There is no limit to the number of symbols
that can be on the stack, but the PDA has immediate access only to the top symbol,
the one added most recently. We will adopt the terminology that is commonly used
with stacks, which explains where the term “pushdown automaton” comes from:
when a symbol is added to the memory, it is pushed onto the stack, so that it
becomes the new top symbol, and when a symbol is deleted it is popped off the
stack. By allowing a slightly more general type of operation, we will be able to
describe a move using a more uniform notation that will handle both these cases.
A PDA will be able in a single move to replace the symbol X currently on top of
the stack by a string α of stack symbols. The cases α = , α = X, and α = YX
(where Y is a single symbol) correspond to popping X off the stack, leaving the
stack unchanged, and pushing Y onto the stack, respectively (assuming in the last
case that the left end of the string YX corresponds to the top of the stack).
A single move of a pushdown automaton will depend on the current state, the
next input, and the symbol currently on top of the stack. In the move, the PDA
will be allowed to change states, as well as to modify the stack in the way we have
described. Before we give the precise deﬁnition of a pushdown automaton, there
are a few other things to mention. First, -transitions (moves in which no input
symbol is read) are allowed, so that “the next input” means either a symbol in the
input alphabet or . Second, a PDA is deﬁned in general to be nondeterministic
and at some stages of its operation may have a choice of more than one move.

166
C H A P T E R 5
Pushdown Automata
Finally, a PDA will be assumed to begin operation with an initial start symbol Z0
on its stack and will not be permitted to move unless the stack contains at least
one symbol; provided that Z0 is never removed and no additional copies of it are
pushed onto the stack, saying that this symbol is on top means that the stack is
effectively empty.
Deﬁnition 5.1
A Pushdown Automaton
A pushdown automaton (PDA) is a 7-tuple M = (Q, , , q0, Z0, A, δ),
where
Q is a ﬁnite set of states.
 and  are ﬁnite sets, the input and stack alphabets.
q0, the initial state, is an element of Q.
Z0, the initial stack symbol, is an element of .
A, the set of accepting states, is a subset of Q.
δ, the transition function, is a function from Q × ( ∪{}) ×  to
the set of ﬁnite subsets of Q × ∗.
Tracing the moves of a PDA on an input string is more complicated than
tracing a ﬁnite automaton, because of the stack. When we do this, we will keep
track of the current state, the portion of the input string that has not yet been read,
and the complete stack contents. The stack contents will be represented by a string
of stack symbols, and the leftmost symbol is assumed to be the one on top. A
conﬁguration of the PDA M = (Q, , , q0, Z0, A, δ) is a triple
(q, x, α)
where q ∈Q, x ∈∗, and α ∈∗. Although it is not required, the last symbol of
the string α will often be Z0, because in most cases it is not removed and remains
at the bottom of the stack. We write
(p, x, α) ⊢M (q, y, β)
to mean that one of the possible moves in the ﬁrst conﬁguration takes M to the
second. This can happen in two ways, depending on whether the move reads an
input symbol or is a -transition. In the ﬁrst case, x = σy for some σ ∈, and in
the second case x = y. We can summarize both cases by saying that x = σy for
some σ ∈ ∪{}. If α = Xγ for some X ∈ and some γ ∈∗, then β = ξγ
for some string ξ for which (q, ξ) ∈δ(p, σ, X).
More generally, we write
(p, x, α) ⊢n
M (q, y, β)
if there is a sequence of n moves taking from M from the ﬁrst conﬁguration to the
second, and
(p, x, α) ⊢∗
M (q, y, β)

5.1
Deﬁnitions and Examples
167
if there is a sequence of zero or more moves taking M from the ﬁrst conﬁguration
to the second. In the three notations ⊢M, ⊢n
M, and ⊢∗
M, if there is no confusion we
usually omit the subscript M.
Deﬁnition 5.2
Acceptance by a PDA
If M = (Q, , , q0, Z0, A, δ) and x ∈∗, the string x is accepted by
M if
(q0, x, Z0) ⊢∗
M (q, , α)
for some α ∈∗and some q ∈A. A language L ⊆∗is said to be
accepted by M if L is precisely the set of strings accepted by M; in
this case, we write L = L(M). Sometimes a string accepted by M, or a
language accepted by M, is said to be accepted by ﬁnal state.
According to this deﬁnition, whether or not a string x is accepted depends
only on the current state when x has been processed, not on the stack contents. An
accepting conﬁguration will be any conﬁguration in which the state is an accepting
state. Specifying an input string as part of a conﬁguration can be slightly confusing;
keep in mind that if x, y ∈∗, and
(q0, xy, Z0) ⊢∗
M (q, y, α)
and q ∈A, then although the original input is xy and the PDA reaches an accepting
conﬁguration, it is not xy that has been accepted, but only x. In order to be accepted,
a string must have been read in its entirety by the PDA.
EXAMPLE 5.3
PDAs Accepting the Languages AnBn and SimplePal
We have already described in general terms how pushdown automata can be constructed
to accept the languages AnBn and SimplePal deﬁned above. Now we present the PDAs in
more detail. In this chapter, for the most part, we will rely on transition tables rather than
transition diagrams, although for the PDA accepting SimplePal we will present both.
The PDA for AnBn is M1 = (Q, , , q0, Z0, A, δ), where Q = {q0, q1, q2, q3}, A =
{q0, q3}, and the transitions are those in Table 5.4.
M1 is in an accepting state initially, which allows  to be accepted. The only legal
input symbol in this state is a, and the ﬁrst a takes M1 to q1, the state in which additional
a’s are read and pushed onto the stack. Reading the ﬁrst b is the signal to move to q2, the
state in which b’s are read (these are the only legal input symbols when M1 is in the state
q2) and used to cancel a’s on the stack. The input string is accepted when a b cancels the
last a on the stack, so that the number of b’s and the number of a’s are equal. At that point,
M1 makes a -transition to the other accepting state, from which no moves are possible.
The reason a second accepting state is necessary is that without it the PDA could start all
over once it has processed a string in AnBn, and end up accepting strings such as aabbab.

168
C H A P T E R 5
Pushdown Automata
Table 5.4
Transition Table for a PDA Accepting AnBn
Move Number
State
Input
Stack Symbol
Move(s)
1
q0
a
Z0
(q1, aZ0)
2
q1
a
a
(q1, aa)
3
q1
b
a
(q2, )
4
q2
b
a
(q2, )
5
q2

Z0
(q3, Z0)
(all other combinations)
none
The sequence of moves causing aabb to be accepted is
(q0, aabb, Z0) ⊢(q1, abb, aZ0) ⊢(q1, bb, aaZ0)
⊢(q2, b, aZ0) ⊢(q2, , Z0) ⊢(q3, , Z0)
The language SimplePal does not contain , and as a result we can get by with one
fewer state in a pushdown automaton accepting it. As in the ﬁrst PDA, there is one state q0
for processing the ﬁrst half of the string, another, q1, for the second half, and an accepting
state q2 from which there are no moves. The PDA for this language is also similar to the
ﬁrst one in that in state q0, symbols in the ﬁrst half of the string are pushed onto the stack;
in state q1 there is only one legal input symbol at each step and it is used to cancel the top
stack symbol; and the PDA moves to the accepting state on a -transition when the stack
is empty except for Z0.
The moves by which the string abcba are accepted are shown below:
(q0, abcba, Z0) ⊢(q0, bcba, aZ0) ⊢(q0, cba, baZ0) ⊢(q1, ba, baZ0)
⊢(q1, a, aZ0) ⊢(q1, , Z0) ⊢(q2, , Z0)
A string can fail to be accepted either because it is never processed completely or because
the current state at the end of the processing is not an accepting state. These two scenarios
are illustrated by the strings acab and abc, respectively:
(q0, acab, Z0) ⊢(q0, cab, aZ0) ⊢(q1, ab, aZ0) ⊢(q1, b, Z0) ⊢(q2, b, Z0)
(q0, abc, Z0) ⊢(q0, bc, aZ0) ⊢(q0, c, baZ0) ⊢(q1, , baZ0)
In the ﬁrst case, although the sequence of moves ends in the accepting state, it is not acab
that has been accepted, but only the preﬁx aca.
Λ, Z0/Z0
c, Z0/Z0
b, Z0/bZ0
a, Z0/aZ0
a, b/ab
b, a/ba
b, b/bb
b, b/Λ
a, a/aa
a, a/Λ
c, a/a
c, b/b
q0
q1
q2
Figure 5.5
A transition diagram for a PDA accepting SimplePal.

5.1
Deﬁnitions and Examples
169
Table 5.6
Transition Table for a PDA Accepting SimplePal
Move Number
State
Input
Stack Symbol
Move(s)
1
q0
a
Z0
(q0, aZ0)
2
q0
b
Z0
(q0, bZ0)
3
q0
a
a
(q0, aa)
4
q0
b
a
(q0, ba)
5
q0
a
b
(q0, ab)
6
q0
b
b
(q0, bb)
7
q0
c
Z0
(q1, Z0)
8
q0
c
a
(q1, a)
9
q0
c
b
(q1, b)
10
q1
a
a
(q1, )
11
q1
b
b
(q1, )
12
q1

Z0
(q2, Z0)
(all other combinations)
none
Figure 5.5 shows a transition diagram for the PDA in Table 5.6. The labels on the
arrows are more complicated than in the case of a ﬁnite automaton, because each one
includes the input as well as the change made to the stack. In the second part of the
label, the top stack symbol comes before the slash and the string that replaces it comes
after. Even with the extra information, however, a diagram of this type does not capture
the machine’s behavior in the way an FA’s transition diagram does. Tracing the moves
means being able to remember information, perhaps the entire contents of the stack, at each
step.
Although the deﬁnition of a pushdown automaton allows nondeterminism,
neither of these two examples illustrates it—at least not if we take nondeterminism
to mean the possibility of a choice of moves. In the PDA in the next example,
there are some conﬁgurations in which several moves are possible, and we will see
later that the corresponding language L(M) can be accepted by a PDA only if this
feature is present. The existence of such languages is another signiﬁcant difference
between pushdown automata and ﬁnite automata.
EXAMPLE 5.7
A Pushdown Automaton Accepting Pal
Pal is the language of palindromes over {a, b} discussed in Examples 1.18 and 4.3. An odd-
length string in Pal is just like an element of SimplePal in Example 5.3 except that its
middle symbol is a or b, not c. Like the PDA above accepting SimplePal, this one has only
the three states q0, q1, and q2. q2 is the accepting state, and q0 and q1 are the states the
device is in when it is processing the ﬁrst half and the second half of the string, respectively.
Our strategy for accepting SimplePal depended strongly on the c in the middle, which
told us that we should stop pushing input symbols onto the stack and start using them to
cancel symbols currently on the stack. The question is, without this symbol marking the
middle of the string, how can a pushdown automaton know when it is time to change

170
C H A P T E R 5
Pushdown Automata
from the pushing-onto-the-stack state q0 to the popping-off-the-stack state q1? The answer
is that it can’t, and this is where the nondeterminism is necessary. Let’s begin, however, by
emphasizing that once the PDA makes this change of state, there is no more nondeterminism;
we proceed the way we did with SimplePal, so that we reach the accepting state q2 only if
the symbols we read from this point on match the ones currently on the stack. This is what
guarantees that even though we have many choices of moves, we never have a choice that
will cause a string to be accepted if it isn’t a palindrome.
A palindrome of odd length looks like xaxr or xbxr, and one of even length looks like
xxr. Suppose we have read the string x, have pushed all the symbols of x onto the stack,
and are still in the pushing-onto-the-stack state q0. Suppose also that the next input symbol
is a, so that we don’t need to worry about the string we’re working on being xbxr. What
we have to do is provide a choice of moves that will allow the string xaxr to be accepted;
another choice that will allow xxr to be accepted (if xr begins with a); and other choices
that will allow longer palindromes beginning with x to be accepted.
The moves that take care of these cases are: go to the state q1 by reading the symbol
a; go to q1 without reading a symbol; and read the a but push it onto the stack and stay in
q0. In either of the ﬁrst two cases, we will accept if and only if the symbols we read starting
now are those in xr.
Compare Table 5.8, for a PDA accepting Pal, to Table 5.6. In each of the ﬁrst six lines
of Table 5.8, the ﬁrst move shown is identical to the move in that line of Table 5.6, and the
last three lines of Table 5.8 are also the same as in Table 5.6. The tables differ only in the
moves that cause the PDA to go from q0 to q1. In the earlier PDA, the only way to do it
was to read a c. Here, the second move in each of the ﬁrst three lines does it by reading an
a; the second move in each of the next three lines does it by reading a b; and the moves in
lines 7–9 do it by means of a -transition.
The way the string abbba is accepted is for the machine to push the ﬁrst two symbols
onto the stack, so that the conﬁguration is (q0, bba, baZ0). At that point the input symbol b
is read and the state changes to q1, but the stack is unchanged; the PDA guesses that the b is
the middle symbol of an odd-length string and tests subsequent input symbols accordingly.
Table 5.8
Transition Table for a PDA Accepting Pal
Move Number
State
Input
Stack Symbol
Move(s)
1
q0
a
Z0
(q0, aZ0), (q1, Z0)
2
q0
a
a
(q0, aa), (q1, a)
3
q0
a
b
(q0, ab), (q1, b)
4
q0
b
Z0
(q0, bZ0), (q1, Z0)
5
q0
b
a
(q0, ba), (q1, a)
6
q0
b
b
(q0, bb), (q1, b)
7
q0

Z0
(q1, Z0)
8
q0

a
(q1, a)
9
q0

b
(q1, b)
10
q1
a
a
(q1, )
11
q1
b
b
(q1, )
12
q1

Z0
(q2, Z0)
(all other combinations)
none

5.1
Deﬁnitions and Examples
171
In other words, after reading two symbols of the input string, it chooses to test whether the
string is a palindrome of length 5 (if it is not, it will not be accepted). Once it makes this
choice, its moves are the only possible ones. The complete sequence is
(q0, abbba, Z0) ⊢(q0, bbba, aZ0) ⊢(q0, bba, baZ0)
⊢(q1, ba, baZ0) ⊢(q1, a, aZ0) ⊢(q1, , Z0) ⊢(q2, , Z0)
The presence of two moves in each of the ﬁrst six lines is an obvious indication of nondeter-
minism. Lines 7–9 constitute nondeterminism of a slightly less obvious type. For the string
abba, for example, the ﬁrst two moves are the same as before, resulting in the conﬁguration
(q0, ba, baZ0). The PDA then guesses, by making a -transition, that the input might be a
palindrome of length 4. The moves are
(q0, abba, Z0) ⊢(q0, bba, aZ0) ⊢(q0, ba, baZ0)
⊢(q1, ba, baZ0) ⊢(q1, a, aZ0) ⊢(q1, , Z0) ⊢(q2, , Z0)
Just as in Section 3.2, we can draw a computation tree for the PDA, showing the
conﬁguration and choice of moves at each step. Figure 5.9 shows the tree for the input
(q0, baab, Z0)
(q0, aab, bZ0)
(q2, aab, Z0)
(q2, baab, Z0)
(q1, baab, Z0)
(q1, aab, Z0)
(q0, ab, abZ0)
(q1, aab, bZ0)
(q1, ab, bZ0)
(q0, Λ, baabZ0)
(q1, b, aabZ0)
(q1, Λ, aabZ0)
(q1, b, bZ0)
(q1, Λ, Z0)
(q2, Λ, Z0)
(q1, Λ, baabZ0)
(q0, b, aabZ0)
(q1, ab, abZ0)
(q1, b, abZ0)
Figure 5.9
A computation tree for Table 5.8 and the input string baab.

172
C H A P T E R 5
Pushdown Automata
string baab. In each conﬁguration on the left, the state is q0. As long as there is at least
one unread input symbol, the PDA can choose from the three moves we have discussed.
The three branches, left to right, represent the move that stays in q0, the move that reads a
symbol and moves to q1, and the move to q1 on a -transition.
The path through the tree that leads to the string being accepted is the one that branches
to the right with a -transition after two symbols have been read. Paths that leave the vertical
path too soon terminate before the PDA has ﬁnished reading the input; it either stops because
it is unable to move or enters the accepting state prematurely, so that the string accepted is
a palindrome of length 0 or 1. Paths that follow the vertical path too long terminate with
input symbols left on the stack.
5.2 DETERMINISTIC PUSHDOWN
AUTOMATA
The pushdown automaton in Example 5.3 accepting SimplePal never has a choice
of moves. It is appropriate to call it deterministic if we are less strict than in Chapter
4 and allow a deterministic PDA to have no moves in some conﬁgurations. The one
in Example 5.7 accepting Pal illustrates both ways a PDA can be nondeterministic:
It can have more than one move for the same combination of state, input, and stack
symbol, and it can have a choice, for some combination of state and stack symbol,
between reading an input symbol and making a -transition without reading one.
Deﬁnition 5.10
A Deterministic Pushdown Automaton
A pushdown automaton M = (Q, , , q0, Z0, A, δ) is deterministic if it
satisﬁes both of the following conditions.
1. For every q ∈Q, every σ ∈ ∪{}, and every X ∈, the set δ(q, σ, X)
has at most one element.
2. For every q ∈Q, every σ ∈, and every X ∈, the two sets δ(q, σ, X)
and δ(q, , X) cannot both be nonempty.
A language L is a deterministic context-free language (DCFL) if there is
a deterministic PDA (DPDA) accepting L.
We have not yet shown that context-free languages are precisely the languages
that can be accepted by PDAs, and the last sentence in the deﬁnition anticipates the
results in Sections 5.3 and 5.4. Later in this section, we will show that not every
context-free language is a DCFL; it probably won’t be a surprise that Pal, for which
we constructed a PDA in Example 5.7, cannot be accepted by a deterministic PDA.
EXAMPLE 5.11
A DPDA Accepting Balanced
According to Example 1.25, a string of left and right parentheses is balanced if no preﬁx has
more right than left and there are equal numbers of left and right. We present a very simple

5.2
Deterministic Pushdown Automata
173
deterministic PDA M and argue that it accepts the language Balanced of balanced strings.
The only two states are q0 and q1, and the accepting state is q0. For clarity of notation, we
use [ and ] for our “parentheses.” The transition table is shown in Table 5.12 below.
Table 5.12
Transition Table for a DPDA Accepting Balanced
Move Number
State
Input
Stack Symbol
Move
1
q0
[
Z0
(q1, [Z0)
2
q1
[
[
(q1, [[)
3
q1
]
[
(q1, )
4
q1

Z0
(q0, Z0)
(all other combinations)
none
The PDA operates as follows. A left parenthesis in the input is always pushed onto the
stack; a right parenthesis in the input is canceled with a left parenthesis on the stack; and
once the machine enters state q1, it stays there until the stack is empty except for Z0, when
it returns to q0 with a -transition.
It is not hard to see that every string accepted by M must be a balanced string of
parentheses. Every preﬁx must have at least as many left parentheses as right, because for
every right parenthesis that is read, some previous left parenthesis must have been pushed
onto the stack and not yet canceled. And the total numbers of left and right parentheses must
be equal, because the stack must be empty except for Z0 by the time the string is accepted.
It is a little harder to show that every balanced string of parentheses is accepted by M.
First we establish a preliminary result: For every balanced string x, the statement P(x) is
true, where P(x) is
For every j ≥1, (q1, x, [jZ0) ⊢∗
M (q1, , [jZ0)
The proof is by mathematical induction on |x|. P() is true, because for every j, the
initial and ﬁnal conﬁgurations are identical. Suppose that k ≥0, that P(x) is true for every
balanced string x with |x| ≤k, and that x is a balanced string with |x| = k + 1.
According to the recursive deﬁnition of Balanced in Example 1.19, every nonnull
balanced string is either the concatenation of two shorter balanced strings or of the form
[y] for some balanced string y. We consider the two cases separately. If x = x1x2, where
x1 and x2 are both balanced strings shorter than x, then for every j, the induction hypothesis
used twice (once for x1, once for x2) implies that
(q1, x1x2, [jZ0) ⊢∗
M (q1, x2, [jZ0) ⊢∗
M (q1, , [jZ0)
In the other case, x = [y] for some balanced string y. In this case, for every j ≥1, the
induction hypothesis tells us that
(q1, y], [j+1Z0) ⊢∗(q1, ], [j+1Z0)
and it follows that for every j ≥1,
(q1, [y], [jZ0) ⊢(q1, y], [j+1Z0) ⊢∗(q1, ], [j+1Z0) ⊢(q1, , [jZ0)
This preliminary result allows us to show, again using mathematical induction on the
length of x, that every balanced string x is accepted by M. The basis step is easy:  is

174
C H A P T E R 5
Pushdown Automata
accepted because q0 is the accepting state. Suppose that k ≥0 and that every balanced string
of length k or less is accepted, and let x be a balanced string with |x| = k + 1. We consider
the same two cases as before. If x = x1x2, where x1 and x2 are shorter balanced strings,
then the induction hypothesis applied to x1 and x2 individually implies that
(q0, x1x2, Z0) ⊢∗(q0, x2, Z0) ⊢∗(q0, , Z0)
If x = [y], where y is balanced, then
(q0, [y], Z0) ⊢(q1, y], [Z0) ⊢∗(q1, ], [Z0) ⊢(q1, , Z0) ⊢(q0, , Z0)
Here the second statement follows from the preliminary result applied to the string y, and
the other statements involve moves of M that are shown in the transition table.
EXAMPLE 5.13
Two DPDAs Accepting AEqB
In order to ﬁnd a PDA accepting AEqB, the language {x ∈{a, b}∗| na(x) = nb(x)}, we can
adapt the idea of the PDA M in Example 5.11: using the stack to save copies of one symbol
if there is a temporary excess of that symbol, and canceling them with incoming occurrences
of the opposite symbol. The only difference now is that we allow preﬁxes of the string to
have either more a’s than b’s or more b’s than a’s, as long as the numbers are equal when
we’re done; before we ﬁnish processing the string we may have seen temporary excesses of
both symbols. We can continue to use the state q1 for the situation in which the string read
so far has more of one symbol than the other; as before, whenever equality is restored, the
PDA can return to the accepting state by a -transition.
Here are the moves made by the PDA in accepting abbaaabb.
(q0, abbaaabb,Z0) ⊢(q1, bbaaabb,aZ0) ⊢(q1, baaabb, Z0) ⊢(q0, baaabb, Z0)
⊢(q1, aaabb, bZ0) ⊢(q1, aabb, Z0) ⊢(q0, aabb, Z0) ⊢(q1, abb, aZ0)
⊢(q1, bb, aaZ0) ⊢(q1, b, aZ0) ⊢(q1, , Z0) ⊢(q0, , Z0)
According to Deﬁnition 5.10, -transitions are permissible in a DPDA, provided they
do not allow the device a choice between reading a symbol and making a -transition.
Both the PDA in Table 5.14 and the one in Table 5.12 are deterministic, because the only
-transition is in q1 with top stack symbol Z0, and with that combination of state and stack
symbol there are no other moves. If we wish, however, we can modify the DPDA in both
Table 5.14
Transition Table for a DPDA Accepting AEqB
Move Number
State
Input
Stack Symbol
Move
1
q0
a
Z0
(q1, aZ0)
2
q0
b
Z0
(q1, bZ0)
3
q1
a
a
(q1, aa)
4
q1
b
b
(q1, bb)
5
q1
a
b
(q1, )
6
q1
b
a
(q1, )
7
q1

Z0
(q0, Z0)
(all other combinations)
none

5.2
Deterministic Pushdown Automata
175
Table 5.15
A DPDA with No -transitions Accepting AEqB
Move Number
State
Input
Stack Symbol
Move
1
q0
a
Z0
(q1, AZ0)
2
q0
b
Z0
(q1, BZ0)
3
q1
a
A
(q1, aA)
4
q1
b
B
(q1, bB)
5
q1
a
a
(q1, aa)
6
q1
b
b
(q1, bb)
7
q1
a
b
(q1, )
8
q1
b
a
(q1, )
9
q1
a
B
(q0, )
10
q1
b
A
(q0, )
(all other combinations)
none
cases so that there are no -transitions at all. In order to do this for Table 5.14, we must
provide a way for the device to determine whether the symbol currently on the stack is the
only one other than Z0; an easy way to do this to use special symbols, say A and B, to
represent the ﬁrst extra a or extra b, respectively.
In this modiﬁed DPDA, shown in Table 5.15, the moves made by the PDA in accepting
abbaaabb are
(q0, abbaaabb,Z0) ⊢(q1, bbaaabb, AZ0) ⊢(q0, baaabb, Z0) ⊢(q1, aaabb, BZ0)
⊢(q0, aabb, Z0) ⊢(q1, abb, AZ0) ⊢(q1, bb, aAZ0) ⊢(q1, b, AZ0) ⊢(q0, , Z0)
As we mentioned at the beginning of this section, the language Pal can be
accepted by a simple PDA but cannot be accepted by a DPDA. It may seem
intuitively clear that the natural approach can’t be used by a deterministic device
(how can a PDA know, without guessing, that it has reached the middle of the
string?), but the proof that no DPDA can accept Pal requires a little care.
Theorem 5.16
The language Pal cannot be accepted by a deterministic pushdown automa-
ton.
Sketch of Proof
Suppose for the sake of contradiction that M is a
DPDA accepting Pal.
First, we can modify M if necessary (see Exercise 5.17) so that every
move is either one of the form
δ(p, σ, X) = {(q, )}
or one of the form
δ(p, σ, X) = {(q, αX)}
The effect is that M can still remove a symbol from the stack or place
another string on the stack, but it can’t do both in the same move. If

176
C H A P T E R 5
Pushdown Automata
the stack height does not decrease as a result of a move, then that move
cannot have removed any symbols from the stack.
Next, we observe that for every input string x ∈{a, b}∗, M must read
every symbol of x. This feature is guaranteed by the fact that x is a preﬁx
of the palindrome xxr, and a palindrome must be read completely by M
in order to be accepted. It follows in particular that no sequence of moves
can cause M to empty its stack.
The idea of the proof is to ﬁnd two different strings r and s such
that for every sufﬁx z, M treats both rz and sz the same way. This will
produce a contradiction, because according to Example 2.27, no matter
what r and s are, there is some z for which only one of the two strings
rz and sz will be a palindrome.
Here is one more deﬁnition: For an arbitrary x, there is a string yx
such that of all the possible strings xy, xyx is one whose processing
by M produces a conﬁguration with minimum stack height. The deﬁning
property of yx and the modiﬁcation of M mentioned at the beginning imply
that if αx represents the stack contents when xyx has been processed, no
symbols of αx are ever removed from the stack as a result of processing
subsequent input symbols.
Now we are ready to choose the strings r and s. There are inﬁnitely
many different strings of the form xyx. Therefore, because there are only
a ﬁnite number of combinations of state and stack symbol, there must
be two different strings r = uyu and s = vyv so that the conﬁgurations
resulting from M’s processing r and s have both the same state and the
same top stack symbol. It follows that although r and s are different, M
can’t tell the difference—the resulting states are the same, and the only
symbols on the stack that M will ever be able to examine after processing
the two strings are also the same. Therefore, for every z, the results of
processing rz and sz must be the same.
5.3 A PDA FROM A GIVEN CFG
The languages accepted by the pushdown automata we have studied so far have
been generated by simple context-free grammars, but we have not referred to gram-
mars in constructing the machines, relying instead on simple symmetry properties
of the strings themselves. In this section, we will consider two ways of constructing
a PDA from an arbitrary CFG.
In both cases, the device is nondeterministic. It attempts to simulate a derivation
in the grammar, using the stack to hold portions of the current string, and terminates
the computation if it determines at some point that the derivation-in-progress is
not consistent with the input string. It is possible in both cases to visualize the
simulation as an attempt to construct a derivation tree for the input string; the two
PDAs we end up with are called top-down and bottom-up because of the different

5.3
A PDA from a Given CFG
177
approaches they take to building the tree. In both cases, we will describe the PDA
obtained from a grammar G, indicate why the language accepted by the PDA is
precisely the same as the one generated by G, and look at an example to see
how the moves the PDA makes as it accepts a string correspond to the steps in a
derivation of x.
The top-down PDA begins by placing the start symbol of the grammar, which
is the symbol at the root of every derivation tree, on the stack. Starting at this point,
each step in the construction of a derivation tree consists of replacing a variable
that is currently on top of the stack by the right side of a grammar production
that begins with that variable. If there are several such productions, one is chosen
nondeterministically. This step corresponds to building the portion of the tree con-
taining that variable-node and its children. The intermediate moves of the PDA,
which prepare for the next step in the simulation, are to remove terminal symbols
from the stack as they are produced and match them with input symbols. To the
extent that they continue to match, the derivation being simulated is consistent with
the input string. Because the variable that is replaced in each step of the simulation
is the leftmost one in the current string, preceded only by terminal symbols that
have been removed from the stack already, the derivation being simulated is a
leftmost derivation.
Deﬁnition 5.17
The Nondeterministic Top-Down PDA NT(G)
Let G = (V, , S, P ) be a context-free grammar. The nondeterministic
top-down PDA corresponding to G is NT (G) = (Q, , , q0, Z0, A, δ),
deﬁned as follows:
Q = {q0, q1, q2}
A = {q2}
 = V ∪ ∪{Z0}
The initial move of NT (G) is the -transition
δ(q0, , Z0) = {(q1, SZ0)}
and the only move to the accepting state is the -transition
δ(q1, , Z0) = {(q2, Z0)}
The moves from q1 are the following:
For every A ∈V , δ(q1, , A) = {(q1, α) | A →α is a production in G}
For every σ ∈, δ(q1, σ, σ) = {(q1, )}
After the initial move and before the ﬁnal move to q2, the PDA stays in state
q1. In this state, the two types of moves are to replace a variable by the right side
of a production and to match a terminal symbol on the stack with an input symbol
and discard both. The nondeterminism in NT (G) comes from the choice of moves
when the top stack symbol is a variable for which there are several productions.

178
C H A P T E R 5
Pushdown Automata
Theorem 5.18
If G is a context-free grammar, then the nondeterministic top-down PDA
NT (G) accepts the language L(G).
Proof
Suppose ﬁrst that x ∈L(G), and consider a leftmost derivation of x in G.
We wish to show that there is a sequence of moves of NT (G) that will
simulate this derivation and cause x to be accepted.
For some m ≥0, the leftmost derivation of x has the form
S = x0A0α0 ⇒x0x1A1α1 ⇒x0x1x2A2α2 ⇒. . .
⇒x0x1 . . . xmAmαm ⇒x0x1 . . . xmxm+1 = x
where each string xi is a string of terminal symbols, each Ai is a variable,
and each αi is a string that may contain variables as well as terminals.
(The strings x0 and α0 are both .) For i < m, if the production applied to
the variable Ai in the derivation is Ai →βi, then the variable Ai+1 may
be either a variable in the string βi or a variable in αi that was already
present before the production was applied.
The ﬁrst move of NT (G) is to place S on top of the stack, and at that
point, the string x0 (i.e., ) of input symbols has already been read, and
the stack contains the string A0α0Z0. If we ignore Z0, the concatenation
of these two strings is the ﬁrst string in the derivation.
Now suppose that for some i < m, there is a sequence of moves, the
result of which is that the preﬁx x0x1 . . . xi of x has been read and the
string AiαiZ0 is on the stack. We observe that there are additional moves
NT (G) can make at that point that will simulate one more step of the
derivation, so that x0x1 . . . xixi+1 will have been read and Ai+1αi+1Z0 will
be on the stack. They are simply the moves that replace Ai by βi on the
stack, so that now the string on the stack has the preﬁx xi+1, and then
read the symbols in xi+1, using them to cancel the identical ones on the
stack.
The conclusion is that NT (G) can make moves that will eventually
cause x0x1 . . . xm to have been read and the stack to contain AmαmZ0.
At that point, replacing Am by βm on the stack will cause the stack to
contain βmαm, which is the same as xm+1, and NT (G) can then read those
symbols in the input string and match them with the identical symbols on
the stack. At that point, it has read x and the stack is empty except for
Z0, so that it can accept.
In the other direction, the only way a string x can be accepted by
NT (G) is for all the symbols of x to be matched with terminal symbols
on the stack; the only way this can happen, after the initial move placing S
on the stack, is for it to make a sequence of moves that replace variables
by strings on the stack, and between moves in this sequence to make
intermediate moves matching input symbols with terminals from the stack.

5.3
A PDA from a Given CFG
179
We let X0 be the string S, which is the ﬁrst variable-occurrence to show
up on top of the stack. After that, for each i ≥1, the ith time a variable-
occurrence appears on top of the stack, we let Xi be the string obtained
from the terminals already read and the current stack contents (excluding
Z0). Then the sequence of Xi’s, followed by x itself, constitutes a leftmost
derivation of x.
EXAMPLE 5.19
The Language Balanced
Let L be the language of balanced strings of parentheses (as in Example 5.11, we will use [
and ] instead of parentheses). Example 1.25 describes a balanced string by comparing the
number of left parentheses and right parentheses in each preﬁx, but balanced strings can also
be described as those strings of parentheses that appear within legal algebraic expressions.
The CFG for algebraic expressions in Example 4.2 suggests that the language of balanced
strings can be generated by the CFG G with productions
S →[S] | SS | 
The nondeterministic top-down PDA NT (G) corresponding to G is the one whose transitions
are described in Table 5.21 on the next page.
We consider the balanced string [[][]]. Figure 5.20 shows a derivation tree corre-
sponding to the leftmost derivation
S ⇒[S] ⇒[SS] ⇒[[S]S] ⇒[[]S] ⇒[[][S]] ⇒[[][]]
and we compare the moves made by NT (G) in accepting this string to the leftmost derivation
of the string in G.
S
[
]
S
S
[
]
S
S
Λ
[
]
S
S
Λ
Figure 5.20
A derivation tree for
the string [[][]], using
S →[S] | SS | .

180
C H A P T E R 5
Pushdown Automata
Table 5.21
Transition Table for the Top-Down PDA NT(G)
Move Number
State
Input
Stack Symbol
Move
1
q0

Z0
(q1, SZ0)
2
q1

S
(q1, [S]), (q1, SS), (q1, )
3
q1
[
[
(q1, )
4
q1
]
]
(q1, )
5
q1

Z0
(q2, Z0)
(all other combinations)
none
To the right of each move that replaces a variable on top of the stack, we show the
corresponding step in the leftmost derivation.
(q0, [[][]], Z0)
⊢(q1, [[][]], SZ0)
S
⊢(q1, [[][]], [S]Z0)
⇒[S]
⊢(q1, [][]], S]Z0)
⊢(q1, [][]], SS]Z0)
⇒[SS]
⊢(q1, [][]], [S]S]Z0)
⇒[[S]S]
⊢(q1, ][]], S]S]Z0)
⊢(q1, ][]], ]S]Z0)
⇒[[]S]
⊢(q1, []], S]Z0)
⊢(q1, []], [S]]Z0)
⇒[[][S]]
⊢(q1, ]], S]]Z0)
⊢(q1, ]], ]]Z0)
⇒[[][]]
⊢(q1, ], ]Z0)
⊢(q1, , Z0)
⇒[[]][]
⊢(q2, , Z0)
As the term bottom-up suggests, our second approach to accepting a string in
L(G) involves building a derivation tree from bottom to top, from leaf nodes to
the root. The top-down PDA places S (the top of the tree) on the stack in the very
ﬁrst step of its computation. In the bottom-up version, S doesn’t show up by itself
on the stack until the very end of the computation, by which time the PDA has
read all the input and can accept.
The way S ends up on the stack by itself is that it is put there to replace
the symbols on the right side of the production S →α that is the ﬁrst step in a
derivation of the string. We refer to a step like this as a reduction: a sequence of
PDA moves (often more than one, because only one symbol can be removed from
the stack in a single move) that remove from the stack the symbols representing the
right side of a production and replace them by the variable on the left side. We are
not really interested in the individual moves of a reduction, which are likely to be
done using states that are not used except in that speciﬁc sequence of moves, but
only in their combined effect. Each reduction “builds” the portion of a derivation
tree that consists of several children nodes and their parent. The other moves the

5.3
A PDA from a Given CFG
181
bottom-up PDA performs to prepare the way for a reduction are shift moves, which
simply read input symbols and push them onto the stack.
In going from top-down to bottom-up, there are several things that are reversed,
the most obvious being the “direction” in which the tree is constructed. Another is
the order of the steps of the derivation simulated by the PDA as it accepts a string;
the PDA simulates the reverse of a derivation, starting with the last move and ending
with the ﬁrst. Yet another reversal is the order in which we read the symbols on the
stack at the time a reduction is performed. For a simple example, suppose that a
string abc has the simple derivation S ⇒abc. The symbols a, b, and c get onto the
stack by three shift moves, which have the effect of reversing their order. In general,
a reduction corresponding to the production A →β is performed when the string
βr appears on top of the stack (the top symbol being the last symbol of β if β ̸= )
and is then replaced by A. Finally, for reasons that will be more clear shortly, the
derivation whose reverse the bottom-up PDA simulates is a rightmost derivation.
Deﬁnition 5.22
The Nondeterministic Bottom-Up PDA NB(G)
Let G = (V, , S, P ) be a context-free grammar. The nondeterministic
bottom-up PDA corresponding to G is NB(G) = (Q, , , q0, Z0, A, δ),
deﬁned as follows:
Q contains the initial state q0, the state q1, and the accepting state
q2, together with other states to be described shortly.
For every σ ∈ and every X ∈, δ(q0, σ, X) = {(q0, σX)}. This
is a shift move.
For every production B →α in G, and every nonnull string
β ∈∗, (q0, , αrβ) ⊢∗(q0, Bβ), where this reduction is a
sequence of one or more moves in which, if there is more than one,
the intermediate conﬁgurations involve other states that are speciﬁc
to this sequence and appear in no other moves of NB(G).
One of the elements of δ(q0, , S) is (q1, ), and δ(q1, , Z0) =
{(q2, Z0)}.
In the course of accepting a string x, the bottom-up PDA spends most of its
time in the state q0, leaving it if necessary within a reduction and in the last two
moves that lead to the accepting state q2 via q1.
Theorem 5.23
If G is a context-free grammar, then the nondeterministic bottom-up PDA
NB(G) accepts the language L(G).
Proof
As in the top-down case, we show that for a string x in L(G),
there is a sequence of moves NB(G) can make that will simulate

182
C H A P T E R 5
Pushdown Automata
a derivation of x in G (in reverse) and will result in x being
accepted.
Consider a rightmost derivation of x, which has the form
S = α0A0x0 ⇒α1A1x1x0 ⇒α2A2x2x1x0 ⇒. . .
⇒αmAmxmxm−1 . . . x1x0 ⇒xm+1xm . . . x1x0 = x
where, for each i, xi is a string of terminals, Ai is a variable, and αi is
a string of variables and/or terminals. As before, x0 and α0 are both .
If the production that is applied to the indicated occurrence of Ai in this
derivation is Ai →γi, then for i < m we may write
αi+1Ai+1xi+1xi . . . x0 = αiγixi . . . x0
(i.e., αi+1Ai+1xi+1 = αiγi, where the occurrence of Ai+1 is within either
αi or γi), and
xm+1xm . . . x0 = αmγmxm . . . x0
(i.e., xm+1 = αmγm).
From the initial conﬁguration, NB(G) can make a sequence of moves
that shift the symbols of xm+1 onto the stack, so that the stack contains
xr
m+1Z0 = γ r
mαr
mZ0. The string γ r
m is now on top of the stack, so that
NB(G) can execute a reduction that leads to the conﬁguration
(q0, xmxm−1 . . . x0, Amαr
mZ0)
Suppose in general that for some i > 0, there are moves leading to the
conﬁguration
(q0, xixi−1 . . . x0, Aiαr
i Z0)
NB(G) can then shift the symbols of xi onto the stack, resulting in the
conﬁguration
(q0, xi−1 . . . x0, xr
i Aiαr
i Z0) = (q0, xi−1 . . . x0, γ r
i−1αr
i−1Z0)
and reduce γi−1 to Ai−1, so that the conﬁguration becomes
(q0, xi−1 . . . x0, Ai−1αr
i−1Z0)
It follows that NB(G) has a sequence of moves that leads to the conﬁg-
uration
(q0, x0, A0αr
0Z0) = (q0, , S)
which allows it to accept x.
Now suppose that a string x is accepted by NB(G). This requires that
NB(G) be able to reach the conﬁguration
(q0, , SZ0)

5.3
A PDA from a Given CFG
183
using moves that are either shifts or reductions. Starting in the initial
conﬁguration (q0, x, Z0), if we denote by xm+1 the preﬁx of x that is
transferred to the stack before the ﬁrst reduction, and assume that in this
ﬁrst step γm is reduced to Am, then the conﬁguration after the reduction
looks like
(q0, y, Amαr
mZ0)
for some string αm, where y is the sufﬁx of x for which x = xm+1y.
Continuing in this way, if we let the other substrings of x that are shifted
onto the stack before each subsequent reduction be xm, xm−1, . . . , x0,
and assume that for each i > 0 the reduction occurring after xi has been
shifted reduces γi−1 to Ai−1, so that for some string αi−1 the conﬁguration
changes from
(q0, xi−1 . . . x0, γ r
i−1αr
i−1Z0)
to
(q0, xi−1 . . . x0, Ai−1αr
i−1Z0)
then the sequence of strings
S = α0A0x0, α1A1x1x0, α2A2x2x1x0, . . . ,
αmAmxmxm−1 . . . x1x0, xm+1xm . . . x1x0 = x
constitutes a rightmost derivation of x in the grammar G.
EXAMPLE 5.24
Simpliﬁed Algebraic Expressions
We consider the nondeterministic bottom-up PDA NB(G) for the context-free grammar G
with productions
S →S + T | T
T →T ∗a | a
This is essentially the grammar G1 in Example 4.20, but without parentheses. We will not
give an explicit transition table for the PDA. In addition to shift moves, it has the reductions
corresponding to the four productions in the grammar. In Table 5.26 we trace the operation
of NB(G) as it processes the string a + a ∗a, for which the derivation tree is shown in
Figure 5.25.
S
S
+
a
T
a
*
T
a
T
Figure 5.25
A derivation tree
for the string
a + a ∗a in
Example 5.24.
In Table 5.26, the ﬁve numbers in parentheses refer to the order in which the indicated
reduction is performed. In the table, we keep track at each step of the contents of the stack
and the unread input. To make it easier to follow, we show the symbols on the stack in
reverse order, which is their order when they appear in a current string of the derivation.
Each reduction, along with the corresponding step in the rightmost derivation, is shown on
the line with the conﬁguration that results from the reduction; in the preceding line, the
string that is replaced on the stack is underlined. Of course, as we have discussed, the steps
in the derivation go from the bottom to the top of the table. Notice that for each step of the
derivation, the current string is obtained by concatenating the reversed stack contents just
before the corresponding reduction (excluding Z0) with the string of unread input.

184
C H A P T E R 5
Pushdown Automata
Table 5.26
Bottom-Up Processing of a + a ∗a by NB (G)
Reduction
Stack (reversed)
Unread Input
Derivation Step
Z0
a + a ∗a
Z0 a
+ a ∗a
(1)
Z0 T
+ a ∗a
⇒a + a ∗a
(2)
Z0 S
+ a ∗a
⇒T + a ∗a
Z0 S +
a ∗a
Z0 S + a
∗a
(3)
Z0 S + T
∗a
⇒S + a ∗a
Z0 S + T ∗
a
Z0 S + T ∗a
(4)
Z0 S + T
⇒S + T ∗a
(5)
Z0 S
⇒S + T
(accept)
S
5.4 A CFG FROM A GIVEN PDA
We will demonstrate in this section that from every pushdown automaton M, a
context-free grammar can be constructed that accepts the language L(M). A pre-
liminary step that will simplify the proof is to show that starting with M, another
PDA can be obtained that accepts the same language by empty stack (rather than
the usual way, by ﬁnal state).
Deﬁnition 5.27
Acceptance by Empty Stack
If M is a PDA with input alphabet , initial state q1, and initial stack
symbol Z1, then M accepts a language L by empty stack if L = Le(M),
where
Le(M) = {x ∈∗| (q1, x, Z1) ⊢∗
M (q, , )
for some state q}
Theorem 5.28
If M = (Q, , , q0, Z0, A, δ) is a PDA, then there is another PDA M1
such that Le(M1) = L(M).
Sketch of Proof
The idea of the proof is to let M1 process an input
string the same way that M does, except that when M enters an accepting
state, and only when this happens, M1 empties its stack.
If we make M1 a duplicate of M but able to empty its stack auto-
matically whenever M enters an accepting state, then we obtain part
of what we want: Every time M accepts a string x, M1 will accept
x by empty stack. This is not quite what we want, however, because
M might terminate its computation in a nonaccepting state by emptying

5.4
A CFG from a Given PDA
185
its stack, and if M1 does the same thing, it will accept a string that
M doesn’t. To avoid this, we give M1 a different initial stack sym-
bol and let its ﬁrst move be to push M’s initial stack symbol on top
of its own; this allows it to avoid emptying its stack until it should
accept.
The way we allow M1 to empty its stack automatically when M
enters an accepting state is to provide it with a -transition from each
accepting state to a special “stack-emptying” state, from which there are
-transitions back to itself that pop every symbol off the stack until the
stack is empty.
Now we must discuss how to start with a PDA M that accepts a language by
empty stack, and ﬁnd a CFG G generating Le(M). In Section 5.3, starting with
a CFG, we constructed the nondeterministic top-down PDA so that in the process
of accepting a string in L(G), it simulated a leftmost derivation of the string. It
will be helpful in starting with the PDA to try to construct a grammar in a way
that will preserve as much as possible of this CFG-PDA correspondence—so that
a sequence of moves by which M accepts a string can be interpreted as simulating
the steps in a derivation of the string in the grammar.
When the top-down PDA corresponding to a grammar G makes a sequence
of moves to accept x, the conﬁguration of the PDA at each step is such that the
string of input symbols read so far, followed by the string on the stack, is the
current string in a derivation of x in G. One very simple way we might to try
to preserve this feature, if we are starting with a PDA and trying to construct a
grammar, is to ignore the states, let variables in the grammar be the stack symbols
of the PDA, let the start symbol of the grammar be the initial stack symbol Z0,
and for each move that reads a and replaces A on the stack by BC . . . D, introduce
the production
A →aBC . . . D
An initial move (q0, ab . . . , Z0) ⊢(q1, b . . . , ABZ0), which reads a and replaces
Z0 by ABZ0, would correspond to the ﬁrst step
Z0 ⇒aABZ0
in a derivation; a second move (q1, b . . . , ABZ0) ⊢(q2, . . . , CBZ0) that replaced
A by C on the stack would allow the second step
aABZ0 ⇒abCBZ0
and so forth. The states don’t show up in the derivation steps at all, but the
current string at each step is precisely the string of terminals read so far, fol-
lowed by the symbols (variables) on the stack. The fact that productions in the
grammar end in strings of variables would help to highlight the grammar-PDA
correspondence: In order to produce a string of terminal symbols from abCBZ0,
for example, we need eventually to eliminate the variables C, B, and Z0 from

186
C H A P T E R 5
Pushdown Automata
the string, and in order to accept the input string starting with ab by empty
stack, we need eventually to eliminate the stack symbols C, B, and Z0 from the
stack.
You may suspect that “ignore the states” sounds a little too simple to work in
general. It does allow the grammar to generate all the strings that the PDA accepts,
but it may also generate strings that are not accepted (see Exercise 5.35).
Although this approach must be modiﬁed, the essential idea can still be used.
Rather than using the stack symbols themselves as variables, we try things of the
form
[p, A, q]
where p and q are states. For the variable [p, A, q] to be replaced directly by σ
(either a terminal symbol or ), there must be a PDA move that reads σ, pops
A from the stack, and takes M from state p to state q. More general productions
involving [p, A, q] represent sequences of moves that take M from state p to q
and have the ultimate effect of removing A from the stack.
If the variable [p, A, q] appears in the current string of a derivation, then
completing the derivation requires that the variable be eliminated, perhaps replaced
by  or a terminal symbol. This will be possible if there is a move that takes M
from p to q and pops A from the stack. Suppose instead, however, that there is a
move from p to p1 that reads a and replaces A on the stack by B1B2 . . . Bm. It is
appropriate to introduce σ into our current string at this point, since we want the
preﬁx of terminals in our string to correspond to the input read so far. The new
string in the derivation should reﬂect the fact that the new symbols B1, . . . , Bm
have been added to the stack. The most direct way to eliminate these new symbols
is to start in p1 and make moves ending in a new state p2 that remove B1 from
the stack; then to make moves to p3 that remove B2; . . . ; to move from pm−1
to pm and remove Bm−1; and ﬁnally to move from pm to q and remove Bm. It
doesn’t matter what the states p2, p3, . . . , pm are, and we allow every string of
the form
σ[p1, B, p2][p2, B2, p3] . . . [pm, Bm, q]
to replace [p, A, q] in the current string. The actual moves of the PDA may
not accomplish these steps directly (one or more of the intermediate steps repre-
sented by [pi, Bi, pi+1] may need to be reﬁned further), but this is an appropriate
restatement at this point of what must eventually happen in order for the derivation
to be completed. Thus, we will introduce into our grammar the production
[p, A, q] →σ[p1, B, p2][p2, B2, p3] . . . [pm, Bm, q]
for every possible sequence of states p2, . . . , pm. Some such sequences will be
dead ends, because there will be no moves following the particular sequence
of states and having the ultimate effect represented by this sequence of vari-
ables. But no harm is done by introducing all these productions, because for
every derivation in which one of the dead-end sequences appears, there will be

5.4
A CFG from a Given PDA
187
at least one variable that cannot be eliminated from the string, and the derivation
will not produce a string of terminals. If we use S to denote the start symbol
of the grammar, the productions that we need at the beginning are those of the
form
S →[q0, Z0, q]
where q0 is the initial state. When we accept strings by empty stack, the ﬁnal
state is irrelevant, and we include a production of this type for every possible
state q.
Theorem 5.29
If M = (Q, , , q0, Z0, A, δ) is a pushdown automaton accepting L by
empty stack, then there is a context-free grammar G such that L = L(G).
Proof
We deﬁne G = (V, , S, P ) as follows. V contains S as well as all possi-
ble variables of the form [p, A, q], where A ∈ and p, q ∈Q. P contains
the following productions:
1. For every q ∈Q, the production S →[q0, Z0, q] is in P.
2. For every q, q1 ∈Q, every σ ∈ ∪{}, and every A ∈, if δ(q, σ, A)
contains (q1, ), then the production [q, A, q1] →σ is in P.
3. For every q, q1 ∈Q, every σ ∈ ∪{}, every A ∈, and every m ≥1, if
δ(q, σ, A) contains (q1, B1B2 . . . Bm) for some B1, B2, . . . , Bm in , then
for every choice of q2, q3, . . . , qm+1 in Q, the production
[q, A, qm+1] →σ[q1, B1, q2][q2, B2, q3] . . . [qm, Bm, qm+1]
is in P.
The idea of the proof is to characterize the strings of terminals that
can be derived from a variable [q, A, q′]—speciﬁcally, to show that for
every q, q′ ∈Q, every A ∈, and every x ∈∗,
(1)
[q, A, q′] ⇒∗
G x if and only if (q, x, A) ⊢∗
M (q′, , )
If x ∈Le(M), then formula (1) will imply that (q0, x, Z0) ⊢∗
M (q, , )
for some q ∈Q, because it implies that [q0, Z0, q] ⇒∗
G x for some q, and
we can start a derivation of x with a production of the ﬁrst type. On the
other hand, if x ∈L(G), then the ﬁrst step of every derivation of x must
be S ⇒[q0, Z0, q], for some q ∈Q; this means that [q0, Z0, q] ⇒∗
G x,
and formula (1) then implies that x ∈Le(M).
Both parts of formula (1) are proved using mathematical induction.
First we show that for every n ≥1,
(2)
If [q, A, q′] ⇒n
G x, then (q, x, A) ⊢∗(q′, , )

188
C H A P T E R 5
Pushdown Automata
In the basis step, the only production that allows this one-step deriva-
tion is one of the second type, and this is possible only if x is either
 or an element of  and δ(q, x, A) contains (q′, ). It follows that
(q, x, A) ⊢(q′, , ).
Suppose that k ≥1 and that for every n ≤k, whenever [q, A, q′] ⇒n
x, (q, x, A) ⊢∗(q′, , ). Now suppose that [q, A, q′] ⇒k+1 x. We wish
to show that (q, x, A) ⊢∗(q′, , ). Since k ≥1, the ﬁrst step in the
derivation of x must be
[q, A, q′] ⇒σ[q1, B1, q2][q2, B2, q3] . . . [qm, Bm, q′]
for some some m ≥1, some σ ∈ ∪{}, some sequence B1, B2, . . . ,
Bm in , and some sequence q1, q2, . . . , qm in Q, so that δ(q, σ, A)
contains (q1, B2 . . . Bm). The remainder of the derivation takes each of
the variables [qi, Bi, qi+1] to a string xi and the variable [qm, Bm, q′] to
a string xm. The strings x1, . . . , xm satisfy the formula σx1 . . . xm =
x, and each xi is derived from its respective variable in k or fewer
steps. The induction hypothesis implies that for each i with 1 ≤i ≤
m −1,
(qi, xi, Bi) ⊢∗(qi+1, , )
and that
(qm, xm, Bm) ⊢∗(q′, , )
If M is in the conﬁguration (q, x, A) = (q, σx1x2 . . . xm, A), then because
δ(q, σ, A) contains (q1, B1 . . . Bm), M can move in one step to the con-
ﬁguration
(q1, x1x2 . . . xm, B1B2 . . . Bm)
It can then move in a sequence of steps to
(q2, x2 . . . xm, B2 . . . Bm)
then to (q3, x3 . . . xm, B3 . . . Bm), and ultimately to (q′, , ). Statement
(2) then follows.
We complete the proof of statement (1) by showing that for every
n ≥1,
(3)
If (q, x, A) ⊢n (q′, , ), then [q, A, q′] ⇒∗x
In the case n = 1, a string x satisfying the hypothesis in statement
(3) must be of length 0 or 1, and δ(q, x, A) must contain (q′, ). We
may then derive x from [q, A, q′] using a production of the second
type.

5.4
A CFG from a Given PDA
189
Suppose that k ≥1 and that for every n ≤k and every combina-
tion of q, q′ ∈Q, x ∈∗, and A ∈, if (q, x, A) ⊢n (q′, , ), then
[q, A, q′] ⇒∗x. We wish to show that if (q, x, A) ⊢k+1 (q′, , ), then
[q, A, q′] ⇒∗x. We know that for some σ ∈ ∪{} and some y ∈∗,
x = σy and the ﬁrst of the k + 1 moves is
(q, x, A) = (q, σy, A) ⊢(q1, y, B1B2 . . . Bm)
Here m ≥1, since k ≥1, and the Bi’s are elements of . In other words,
δ(q, σ, A) contains (q1, B1 . . . Bm). The k subsequent moves end in the
conﬁguration (q′, , ); therefore, for each i with 1 ≤i ≤m there must
be intermediate points at which the stack contains precisely the string
BiBi+1 . . . Bm. For each such i, let qi be the state M is in the ﬁrst time
the stack contains Bi . . . Bm, and let xi be the portion of the input string
that is consumed in going from qi to qi+1 (or, if i = m, in going from qm
to the conﬁguration (q′, , )). Then
(qi, xi, Bi) ⊢∗(qi+1, , )
for each i with 1 ≤i ≤m −1, and
(qm, xm, Bm) ⊢∗(q′, , )
where each of the indicated sequences of moves has k or fewer. Therefore,
by the induction hypothesis,
[qi, Bi, qi+1] ⇒∗xi
for each i with 1 ≤i ≤m −1, and
[qm, Bm, q′] ⇒∗xm
Since δ(q, a, A) contains (q1, B1 . . . Bm), we know that
[q, A, q′] ⇒σ[q1, B1, q2][q2, B2, q3] . . . [qm, Bm, q′]
(this is a production of type 3), and we may conclude that
[q, A, q′] ⇒∗σx1x2 . . . xm = x
This completes the induction and the proof of the theorem.
EXAMPLE 5.30
A CFG from a PDA Accepting SimplePal
We return to the language SimplePal= {xcxr | x ∈{a, b}∗} from Example 5.3. The transition
table in Table 5.31 is modiﬁed in that the letters on the stack are uppercase and the PDA
accepts by empty stack.
In the grammar G = (V, , S, P) obtained from the construction in Theorem 5.29,
V contains S as well as every variable of the form [p, X, q], where X is a stack symbol

190
C H A P T E R 5
Pushdown Automata
Table 5.31
A PDA Accepting SimplePal by Empty Stack
Move Number
State
Input
Stack Symbol
Move(s)
1
q0
a
Z0
(q0, AZ0)
2
q0
b
Z0
(q0, BZ0)
3
q0
a
A
(q0, AA)
4
q0
b
A
(q0, BA)
5
q0
a
B
(q0, AB)
6
q0
b
B
(q0, BB)
7
q0
c
Z0
(q1, Z0)
8
q0
c
A
(q1, A)
9
q0
c
B
(q1, B)
10
q1
a
A
(q1, )
11
q1
b
B
(q1, )
12
q1

Z0
(q1, )
(all other combinations)
none
and p and q can each be either q0 or q1. Productions of the following types are contained
in P:
(0)
S
→
[q0, Z0, q]
(1)
[q0, Z0, q]
→
a[q0, A, p][p, Z0, q]
(2)
[q0, Z0, q]
→
b[q0, B, p][p, Z0, q]
(3)
[q0, A, q]
→
a[q0, A, p][p, A, q]
(4)
[q0, A, q]
→
b[q0, B, p][p, A, q]
(5)
[q0, B, q]
→
a[q0, A, p][p, B, q]
(6)
[q0, B, q]
→
b[q0, B, p][p, B, q]
(7)
[q0, Z0, q]
→
c[q1, Z0, q]
(8)
[q0, A, q]
→
c[q1, A, q]
(9)
[q0, B, q]
→
c[q1, B, q]
(10)
[q1, A, q1]
→
a
(11)
[q1, B, q1]
→
b
(12)
[q1, Z0, q1]
→

Allowing all combinations of p and q gives 35 productions in all.
The PDA accepts the string bacab by the sequence of moves
(q0, bacab, Z0) ⊢(q0, acab, BZ0)
⊢(q0, cab, ABZ0)
⊢(q1, ab, ABZ0)
⊢(q1, b, BZ0)
⊢(q1, , Z0)
⊢(q1, , )

5.5
Parsing
191
The corresponding leftmost derivation in the grammar is
S ⇒[q0, Z0, q1]
⇒b[q0, B, q1][q1, Z0, q1]
⇒ba[q0, A, q1][q1, B, q1][q1, Z0, q1]
⇒bac[q1, A, q1][q1, B, q1][q1, Z0, q1]
⇒baca[q1, B, q1][q1, Z0, q1]
⇒bacab[q1, Z0, q1]
⇒bacab
From the sequence of PDA moves, it may look as though there are several choices
of leftmost derivations. For example, we might start with the production S →[q0, Z0, q0].
Remember, however, that [q0, Z0, q] represents a sequence of moves from q0 to q that has
the ultimate effect of removing Z0 from the stack. Since the PDA ends up in state q1, it is
clear that q should be q1. Similarly, it may seem as if the second step could be
[q0, Z0, q1] ⇒b[q0, B, q0][q0, Z0, q1]
However, the sequence of PDA moves that starts in q0 and eliminates B from the stack ends
with the PDA in state q1, not q0. In fact, because every move to state q0 adds to the stack,
no string of terminals can ever be derived from the variable [q0, B, q0].
5.5 PARSING
To parse a sentence or an expression means to determine its syntax, or its gram-
matical structure. To parse a string relative to a context-free grammar G means to
determine an appropriate derivation for the string in G, or to determine that there
is none. Parsing a sentence is necessary to understand it; parsing an expression
makes it possible to evaluate it correctly; parsing a statement in a programming
language makes it possible to execute it correctly or translate it correctly into
machine language.
Section 5.3 described two general ways of obtaining a pushdown automaton
from a grammar so that a sequence of moves by which a string is accepted cor-
responds to a derivation of the string in the grammar. In this section we consider
two examples involving simple grammars. In the ﬁrst case, taking advantage of
the information available at each step from the input and the stack allows us to
eliminate the inherent nondeterminism from the top-down PDA, so as to arrive
at a rudimentary parser. In the second case, we can do the same thing with the
bottom-up PDA.
Because not every CFL can be accepted by a deterministic PDA, this goal is
not always achievable, and even for DCFLs the grammar that we start with may
be inappropriate and the process is not always straightforward. This section is not
intended to be a comprehensive survey of parsing techniques, but it may at least
serve as a starting point for a discussion of the development of efﬁcient parsers.

192
C H A P T E R 5
Pushdown Automata
EXAMPLE 5.32
A Top-Down Parser for Balanced
In Section 5.3 we considered the CFG with productions S →[S] | SS | . The way we
have formulated the top-down PDA involves inherent nondeterminism. Each time a variable
appears on top of the stack, the machine replaces it by the right side of a production. These
moves depend only on the variable; they are -transitions and are chosen nondeterministi-
cally. The most obvious approach to eliminating the nondeterminism is to consider whether
consulting the next input symbol as well would allow the PDA to choose the correct move.
The ﬁrst few moves made for the input string [[][]] are enough to show that in this
example, looking at the next input symbol is not enough.
1.
(q0, [[][]], Z0)
2.
⊢(q1, [[][]], SZ0)
S
3.
⊢(q1, [[][]], [S]Z0)
⇒[S]
4.
⊢(q1, [][]], S]Z0)
5.
⊢(q1, [][]], SS]Z0)
⇒[SS]
In both lines 2 and 4, S is on top of the stack and the next input symbol is a left parenthesis,
but S is replaced by different strings in these two places: [S] in line 3 and SS in line 5.
We might have guessed in advance that we would have problems like this, for at least
two reasons. First, the only variable in this grammar is S, and there are more productions
than there are terminal symbols for the right sides to start with. Second, the grammar is
ambiguous: Even if the stack and the next input symbol are enough to determine the next
move for a particular leftmost derivation, there may be more than one LMD.
Let’s try another grammar for this language. It can be shown (Exercises 4.44 and 4.45)
that the CFG G with productions
S →[S]S | 
is an unambiguous grammar that also generates the language Balanced. We begin by trying
the nondeterministic top-down PDA NT (G) on the input string []; we will see that there
is still a slight problem with eliminating the nondeterminism, but it will not be too serious.
1.
(q0, [], Z0)
2.
⊢(q1, [], SZ0)
S
3.
⊢(q1, [], [S]SZ0)
⇒[S]S
4.
⊢(q1, ], S]SZ0)
5.
⊢(q1, ], ]SZ0)
⇒[]S
6.
⊢(q1, , SZ0)
7.
⊢(q1, , Z0)
⇒[]
During these moves, there are three times when S is the top stack symbol. In the ﬁrst
two cases, the move is what we might have hoped: If the next input is [, the move is to
replace S by [S]S, and if it is ], the move is to replace S by . Although this string is
hardly enough of a test, longer strings will result in the same behavior (see the discussion
below). Unfortunately, S is also replaced by  at the end of the computation, when the
input string has been completely read. This is unfortunate because the PDA must make a
-transition in this case, and if it can choose between reading an input symbol and making
a -transition when S is on the stack, then it cannot be deterministic.

5.5
Parsing
193
Table 5.33
Transition Table for NT(G1)
Move Number
State
Input
Stack Symbol
Move(s)
1
q0

Z0
(q1, SZ0)
2
q1

S
(q1, S1$)
3
q1

S1
(q1, [S1]S1), (q1, )
4
q1
[
[
(q1, )
5
q1
]
]
(q1, )
6
q1
$
$
(q1, )
7
q1

Z0
(q2, Z0)
A similar problem arises for many CFGs. Even though at every step before the last
one, looking at the next input symbol is enough to decide which production to apply to
the variable on the stack, a -transition at the end of the computation seems to require
nondeterminism. We resolve the problem by introducing an end-marker $ into the alphabet
and changing the language slightly. Every string in the new language will be required to
end with $, and this symbol will be used only at the ends of strings in the language. Our
new grammar G1 has the productions
S →S1$
S1 →[S1]S1 | 
Table 5.33 is a transition table for the nondeterministic top-down PDA NT (G1).
Line 3 is the only part of the table that needs to be changed in order to eliminate the
nondeterminism. It is replaced by these lines:
State
Input
Stack Symbol
Move
q1
[
S1
(q1,[, [S1]S1)
q1,[

[
(q1, )
q1
]
S1
(q1,], )
q1,]

]
(q1, )
q1
$
S1
(q1,$, )
q1,$

$
(q1, )
In the cases when S1 is on the stack and the next input symbol is ] or $, replacing
S1 by  will be appropriate only if the symbol beneath it on the stack matches the input
symbol. The corresponding states q1,] and q1,$ allow the PDA to pop S1 and remember,
when the stack symbol below it comes to the top, which input symbol it is supposed to
match. In either state, the only correct move is to pop the appropriate symbol from the stack
and return to q1. For the sake of consistency, we use the state q1,[ for the case when the top
stack symbol is S1 and the next input is [. Although in this case S1 is replaced by [S1]S1,
the move from q1,[ is also to pop the [ and return to q1. The alternative, which would be
more efﬁcient, is to replace these two moves by a single one that doesn’t change state and
replaces S1 by S1]S1.
The modiﬁed PDA is clearly deterministic. To check that it is equivalent to the original,
it is enough to verify that our assumptions about the correct -transition made by NT (G1)

194
C H A P T E R 5
Pushdown Automata
in each case where S1 is on the stack are correct. If the next input symbol is either ] or $,
it cannot be correct for the PDA to replace S1 by [S1]S1, because the terminal symbol on
top of the stack would not match the input. If the next input symbol is [, replacing S1 by
 could be correct only if the symbol below S1 were either [ or S1. Here it is helpful to
remember that at each step M makes in the process of accepting a string x, the string of
input symbols already read, followed by the stack contents exclusive of Z0, is the current
string in a derivation of x. Since the current string in a leftmost derivation of x cannot
contain either the substring S1[ or S1S1, neither of these two situations can occur.
Not only does the deterministic PDA accept the same strings as NT (G1), but it pre-
serves the close correspondence between the moves made in accepting a string and the
leftmost derivation of the string. We trace the moves for the simple input string []$ and
show the corresponding derivation.
(q0, []$, Z0)
⊢(q1, []$, SZ0)
S
⊢(q1, []$, S1$Z0)
⇒S1$
⊢(q1,[, ]$, [S1]S1$Z0)
⇒[S1]S1$
⊢(q1, ]$, S1]S1$Z0)
⊢(q1,], $, ]S1$Z0)
⇒[]S1$
⊢(q1, $, S1$Z0)
⊢(q1,$, , $Z0)
⇒[]$
⊢(q1, , Z0)
⊢(q2, , Z0)
This example illustrates the fact that we have a parser for this CFG. In order
to obtain the leftmost derivation, or the derivation tree, for a string x ∈L(G1), all
we have to do is give x to the DPDA and watch the moves it makes.
G1 is an example of an LL(1) grammar, one in which the nondeterministic
top-down PDA corresponding to the grammar can be converted into a deterministic
top-down parser, simply by allowing the PDA to “look ahead” to the next input
symbol in order to decide what move to make. A grammar is LL(k) if looking
ahead k symbols in the input is always enough to choose the next move. Such
a grammar makes it possible to construct a deterministic top-down parser, and
there are systematic methods for determining whether a grammar is LL(k) and
for carrying out such a construction if it is. In some simple cases, a CFG that is
not LL(1) may be transformed using straightforward techniques into one that is
(Exercises 5.39-5.41).
EXAMPLE 5.34
A Bottom-Up Parser for SimpleExpr
In this example we return to the CFG in Example 5.24, with productions
S →S + T | T
T →T ∗a | a

5.5
Parsing
195
except that for the same reasons as in Example 5.11 we add the end-marker $ to the end of
every string in the language. G will be the modiﬁed grammar, with productions
S →S1$
S1 →S1 + T | T
T →T ∗a | a
In the nondeterministic bottom-up PDA NB (G), the two types of moves, other than the initial
move and the moves to accept, are shifts and reductions. A shift reads an input symbol and
pushes it onto the stack, and a reduction removes a string αr from the stack and replaces
it by the variable A in a production A →α. There are ﬁve possible reductions in the PDA
NB(G), two requiring only one move and three requiring two or more. Nondeterminism
arises in two ways: ﬁrst, in choosing whether to shift or to reduce, and second, in making
a choice of reductions if more than one is possible. There is only one correct move at each
step, and G turns out to be a grammar for which the combination of input symbol and stack
symbol will allow us to ﬁnd it.
One basic principle is that if there is a reduction that should be made, then it should be
made as soon as it is possible. For example, if a is on top of the stack, then a reduction, not
a shift, is the move to make. Any symbol that went onto the stack now would have to be
removed later in order to execute the reduction of a or T ∗a to T ; the only way to remove
it is to do another reduction; and because the derivation tree is built from the bottom up,
any subsequent reduction to construct the portion of the tree involving the a node needs its
parent, not the node itself.
Another principle that is helpful is that in the moves made by NB(G), the current string
in the derivation being simulated contains the reverse of the stack contents (not including
Z0) followed by the remaining input. We can use this to answer the question of which
reduction to execute if there seems to be a choice. If a ∗T (the reverse of T ∗a) were
on top of the stack, and we simply reduced a to T , then the current string in a rightmost
derivation would contain the substring T ∗T , which is never possible. Similarly, if T + S1,
the reverse of S1 + T , is on top, then reducing T to S1 can’t be correct. These two situations
can be summarized by saying that when a reduction is executed, the longest possible string
should be removed from the stack during the reduction.
Essentially the only remaining question is what to do when T is the top stack symbol:
shift or reduce? We can answer this by considering the possibilities for the next input sym-
bol. If it is +, then this + should eventually be part of the expression S1 + T (in reverse)
on the stack, so that S1 + T can be reduced to S1; therefore, the T should eventually be
reduced to S1, and so the time to do it is now. Similarly, if the next input is $, T should be
reduced to S1 so that S1$ can be reduced to S. The only other symbol that can follow T in
a rightmost derivation is ∗, and ∗cannot follow any other symbol; therefore, ∗should be
shifted onto the stack.
With these observations, we can formulate the rules the deterministic bottom-up PDA
should follow in choosing its move.
1.
If the top stack symbol is Z0, S1, +, or ∗, shift the next input symbol to the stack.
(None of these four symbols is the rightmost symbol in the right side of a production.)
2.
If the top stack symbol is $, reduce S1$ to S.
3.
If the top stack symbol is a, reduce T ∗a to T if possible; otherwise reduce a to T .
4.
If the top stack symbol is T , then reduce if the next input is + or $, and otherwise
shift.

196
C H A P T E R 5
Pushdown Automata
Table 5.35
Bottom-Up Processing of a + a ∗a$ by the DPDA
Rule
Stack (Reversed)
Unread Input
Derivation Step
Z0
a + a ∗a $
1
Z0 a
+ a ∗a $
3
Z0 T
+ a ∗a $
⇒a + a ∗a$
4 (red.)
Z0 S1 +
a ∗a $
⇒T + a ∗a$
1
Z0 S1 + a
∗a $
3
Z0 S1 + T
∗a $
⇒S1 + a ∗a$
4 (shift)
Z0 S1 + T ∗
a $
1
Z0 S1 + T ∗a
$
3
Z0 S1 + T
$
⇒S1 + T ∗a$
4 (red.)
Z0 S1 $
⇒S1 + T $
2
Z0 S
⇒S1$
5
S
5.
If the top stack symbol is S, then pop it from the stack; if Z0 is then the top symbol,
accept, and otherwise reject.
All these rules except the fourth one are easily incorporated into a transition table for
a deterministic PDA, but the fourth may require a little clariﬁcation. If the PDA sees the
combination (∗, T ) of next input symbol and stack symbol, then it shifts ∗onto the stack.
If it sees (+, T ) or ($, T ), then the moves it makes are ones that carry out the appropriate
reduction and then shift the input symbol onto the stack. The point is that “seeing” (+, T ),
for example, implies reading the +, and the PDA then uses auxiliary states to remember
that after it has performed a reduction, it should place + on the stack.
We now have the essential speciﬁcations for a deterministic PDA; like NB(G), the
moves it makes in accepting a string simulate in reverse the steps in a rightmost derivation
of that string, and in this sense our PDA serves as a bottom-up parser for G. The grammar
G is an example of a weak precedence grammar, which is perhaps the simplest type of
precedence grammar. The phrase refers to a precedence relation (a relation from  to —
i.e., a subset of  × ), where  is the set of terminals and  is the set containing terminals
and variables as well as Z0. In grammars of this type, a precedence relation can be used to
obtain a deterministic shift-reduce parser. In our example, the relation is simply the set of
pairs (a, X) ∈ ×  for which a reduction is appropriate.
In Table 5.26 we traced the moves of the nondeterministic bottom-up parser corre-
sponding to the original version of the grammar (without the end-marker), as it processed
the string a + a ∗a. The moves for our DPDA and the input string a + a ∗a$, shown in
Table 5.35 above, are similar; this time, for each conﬁguration, we show the rule (from the
set of ﬁve above) that the PDA used to get there, and, in the case of rule 4, whether the
move was a shift or a reduction.
EXERCISES
5.1.
a. For the PDA in Table 5.4, trace the sequence of moves made for the
input strings ab, aab, and abb.

Exercises
197
b. For the PDA in Table 5.6, trace the sequence of moves made for the
input strings bacab and baca.
5.2.
For the PDA in Table 5.8, trace every possible sequence of moves for
each of the two input strings aba and aab.
5.3.
For an input string x ∈{a, b}∗with |x| = n, how many possible complete
sequences of moves (sequences of moves that start in the initial
conﬁguration (q0, x, Z0) and terminate in a conﬁguration from which
no move is possible) can the PDA in Table 5.8 make? It is helpful to
remember that once the PDA reaches the state q1, there is no choice of
moves.
5.4.
Consider the PDA in Table 5.8, and for each of the following languages
over {a, b}, modify it to obtain a PDA accepting the language.
a. The language of even-length palindromes.
b. The language of odd-length palindromes.
5.5.
Give transition tables for PDAs accepting each of the following languages.
a. The language of all odd-length strings over {a, b} with middle symbol
a.
b. {anx | n ≥0, x ∈{a, b}∗and |x| ≤n}.
c. {aibjck | i, j, k ≥0 and j = i or j = k}.
5.6.
In both cases below, a transition table is given for a PDA with initial state
q0 and accepting state q2. Describe in each case the language that is
accepted.
Move Number
State
Input
Stack Symbol
Move(s)
1
q0
a
Z0
(q1, aZ0)
2
q0
b
Z0
(q1, bZ0)
3
q1
a
a
(q1, a), (q2, a)
4
q1
b
a
(q1, a)
5
q1
a
b
(q1, b)
6
q1
b
b
(q1, b), (q2, b)
(all other combinations)
none
Move Number
State
Input
Stack Symbol
Move(s)
1
q0
a
Z0
(q0, XZ0)
2
q0
b
Z0
(q0, XZ0)
3
q0
a
X
(q0, XX)
4
q0
b
X
(q0, XX)
5
q0
c
X
(q1, X)
6
q0
c
Z0
(q1, Z0)
7
q1
a
X
(q1, )
8
q1
b
X
(q1, )
9
q1

Z0
(q2, Z0)
(all other combinations)
none

198
C H A P T E R 5
Pushdown Automata
5.7.
What language (a subset of {a, b}∗) is accepted by the PDA whose
transition table is shown below, if the only accepting state is q3?
Move Number
State
Input
Stack Symbol
Move(s)
1
q0
a
Z0
(q0, xZ0), (q1, aZ0)
2
q0
b
Z0
(q0, xZ0), (q1, bZ0)
3
q0
a
x
(q0, xx), (q1, ax)
4
q0
b
x
(q0, xx)(q1, bx)
5
q1
a
a
(q1, a)
6
q1
b
b
(q1, b)
7
q1
a
b
(q1, b), (q2, )
8
q1
b
a
(q1, a), (q2, )
9
q2
a
x
(q2, )
10
q2
b
x
(q2, )
11
q2

Z0
(q3, Z0)
(all other combinations)
none
The PDA can stay in state q0 by pushing x onto the stack for each input
symbol read. From q0 it also has the choice of entering q1, by pushing
onto the stack the symbol it has just read. In state q1 there is always the
option of ignoring the input symbol that is read and leaving the stack
alone, but in order to reach the accepting state it must eventually be able
to move from q1 to q2.
5.8.
Give transition tables for PDAs accepting each of the following languages.
a. {aibj | i ≤j ≤2i}
b. {x ∈{a, b}∗| na(x) < nb(x) < 2na(x)}
5.9.
Table 5.6 shows the transitions for a PDA accepting SimplePal. Draw a
transition table for another PDA accepting this language and having only
two states, the nonaccepting state q0 and the accepting state q2. (Use
additional stack symbols.)
5.10.
Show that every regular language can be accepted by a deterministic PDA
M with only two states in which there are no -transitions and no
symbols are ever removed from the stack.
5.11.
Show that if L is accepted by a PDA, then L is accepted by a PDA in
which there are at most two stack symbols in addition to Z0.
5.12.
Show that if L is accepted by a PDA in which no symbols are ever
removed from the stack, then L is regular.
5.13.
Suppose L ⊆∗is accepted by a PDA M, and that for some ﬁxed k and
every x ∈∗, no sequence of moves made by M on input x causes the
stack to have more than k elements. Show that L is regular.
5.14.
Suppose L ⊆∗is accepted by a PDA M, and for some ﬁxed k, and
every x ∈∗, at least one choice of moves allows M to process x
completely so that the stack never contains more than k elements. Does it
follow that L is regular? Prove your answer.

Exercises
199
5.15.
Suppose L ⊆∗is accepted by a PDA M, and for some ﬁxed k, and
every x ∈L, at least one choice of moves allows M to accept x in such a
way that the stack never contains more than k elements. Does it follow
that L is regular? Prove your answer.
5.16.
Show that if L is accepted by a PDA, then L is accepted by a PDA that
never crashes (i.e., for which the stack never empties and no conﬁguration
is reached from which there is no move deﬁned).
5.17.
Show that if L is accepted by a PDA, then L is accepted by a PDA in
which every move either pops something from the stack (i.e., removes a
stack symbol without putting anything else on the stack); or pushes a
single symbol onto the stack on top of the symbol that was previously on
top; or leaves the stack unchanged.
5.18.
For each of the following languages, give a transition table for a
deterministic PDA that accepts that language.
a. {x ∈{a, b}∗| na(x) < nb(x)}
b. {x ∈{a, b}∗| na(x) ̸= nb(x)}
c. {x ∈{a, b}∗| na(x) = 2nb(x)}
d. {anbn+mam | n, m ≥0}
5.19.
Suppose M1 and M2 are PDAs accepting L1 and L2, respectively. For
both the languages L1L2 and L∗
1, describe a procedure for constructing a
PDA accepting the language. In each case, nondeterminism will be
necessary. Be sure to say precisely how the stack of the new machine
works; no relationship is assumed between the stack alphabets of M1
and M2.
5.20.
†Show that if L is accepted by a DPDA, then there is a DPDA accepting
the language {x#y | x ∈L and xy ∈L}. (The symbol # is assumed not to
occur in any of the strings of L.)
5.21.
Prove the converse of Theorem 5.28: If there is a PDA M = (Q, , ,
q0, Z0, A, δ) accepting L by empty stack (that is, x ∈L if and only if
(q0, x, Z0) ⊢∗
M (q, , ) for some state q), then there is a PDA M1
accepting L by ﬁnal state (i.e., the ordinary way).
5.22.
Show that in Exercise 5.21, if M is a deterministic PDA, then M1 can
also be taken to be deterministic.
5.23.
Show that if there are strings x and y in the language L such that x
is a preﬁx of y and x ̸= y, then no DPDA can accept L by empty
stack.
5.24.
Show that none of the following languages can be accepted by a DPDA.
(Determine exactly what properties of the language Pal are used in the
proof of Theorem 5.16, and show that these languages also have those
properties.)
a. The set of even-length palindromes over {a, b}
b. The set of odd-length palindromes over {a, b}

200
C H A P T E R 5
Pushdown Automata
c. {xx∼| x ∈{a, b}∗} (where x∼means the string obtained from x by
changing a’s to b’s and b’s to a’s)
d. {xy | x ∈{a, b}∗and y is either x or x∼}
5.25.
A counter automaton is a PDA with just two stack symbols, A and Z0,
for which the string on the stack is always of the form AnZ0 for some
n ≥0. (In other words, the only possible change in the stack contents is a
change in the number of A’s on the stack.) For some context-free
languages, such as AnBn, the obvious PDA to accept the language is in
fact a counter automaton. Construct a counter automaton to accept the
given language in each case below.
a. {x ∈{a, b}∗| na(x) = nb(x)}
b. {x ∈{0, 1}∗| na(x) = 2nb(x)}
5.26.
Suppose that M = (Q, , , q0, Z0, A, δ) is a deterministic PDA
accepting a language L. If x is a string in L, then by deﬁnition there
is a sequence of moves of M with input x in which all the symbols
of x are read. It is conceivable, however, that for some strings y /∈L,
no sequence of moves causes M to read all of y. This could happen
in two ways: M could either crash by not being able to move, or it could
enter a loop in which there were inﬁnitely many repeated
-transitions. Find an example of a DCFL L ⊆{a, b}∗, a string
y /∈L, and a DPDA M accepting L for which M crashes on y by
not being able to move. (Say what L is and what y is, and give a
transition table for M.) Note that once you have such an M, it can
easily be modiﬁed so that y causes it to enter an inﬁnite loop of
-transitions.
5.27.
a. Give a deﬁnition of “balanced string” involving two types of
brackets, say [] and {}, corresponding to the deﬁnition in Example
1.25.
b. Write a transition table for a DPDA accepting the language of balanced
strings of these two types of brackets.
5.28.
In each case below, you are given a CFG G and a string x that it
generates. For the top-down PDA NT(G), trace a sequence of moves by
which x is accepted, showing at each step the state, the unread input, and
the stack contents. Show at the same time the corresponding leftmost
derivation of x in the grammar. See Example 5.19 for a guide.
a. The grammar has productions
S →S + T | T
T
→T ∗F | F
F →[S] | a
and x = [a + a ∗a] ∗a.
b. The grammar has productions S →S + S | S ∗S | [S] | a, and
x = [a ∗a + a].
c. The grammar has productions S →[S]S | , and x = [][[][]].

Exercises
201
5.29.
Consider the CFG G with productions
S →aB | bA | 
A →aS | bAA
B →bS | aBB
generating AEqB, the nondeterministic bottom-up PDA NB(G), and
the input string aababb. After the ﬁrst few moves, the conﬁguration
of the PDA is (q0, abb, baaZ0). There are two possible remaining
sequences of moves that cause the string to be accepted. Write both of
them.
5.30.
For a certain CFG G, the moves shown below are those by which the
nondeterministic bottom-up PDA NB(G) accepts the input string aabbab.
Each occurrence of ⊢∗indicates a sequence of moves constituting a
reduction. Draw the derivation tree for aabbab that corresponds to this
sequence of moves.
(q0, aabbab, Z0) ⊢(q0, abbab, aZ0) ⊢(q0, bbab, aaZ0)
⊢(q0, bab, baaZ0) ⊢∗(q0, bab, SaZ0)
⊢(q0, ab, bSaZ0) ⊢∗(q0, ab, SZ0) ⊢(q0, b, aSZ0)
⊢(q0, , baSZ0) ⊢∗(q0, , SSZ0) ⊢∗(q0, , SZ0)
⊢(q1, , Z0) ⊢(q2, , Z0)
5.31.
Let G be the CFG with productions S →S + T | T
T →[S] | a.
Both parts of the question refer to the moves made by the
nondeterministic bottom-up PDA NB(G) in the process of accepting the
input string [a + [a]].
a. If the conﬁguration at some point is (q0, +[a]], S[Z0), what is the
conﬁguration one move later?
b. If the conﬁguration at some point is (q0, +[a]], T [Z0), what is the
conﬁguration one move later?
5.32.
Let M be the PDA in Example 5.7, except that move number 12 is
changed to (q2, ), so that M does in fact accept by empty stack. Let
x = ababa. Find a sequence of moves of M by which x is accepted, and
give the corresponding leftmost derivation in the CFG obtained from M as
in Theorem 5.29.
5.33.
Under what circumstances is the top-down PDA NT(G) deterministic?
(For what kind of grammar G, and what kind of language, could this
happen?) Can the bottom-up PDA NB(G) ever be deterministic?
Explain.
5.34.
In each case below, you are given a CFG G and a string x that it
generates. For the nondeterministic bottom-up PDA NB(G), trace a
sequence of moves by which x is accepted, showing at each step the stack
contents and the unread input. Show at the same time the corresponding
rightmost derivation of x (in reverse order) in the grammar. See Example
5.24 for a guide.

202
C H A P T E R 5
Pushdown Automata
a. The grammar has productions S →S[S] | , and x = [][[]].
b. The grammar has productions S →[S]S | , and x = [][[]].
5.35.
Let M be the PDA whose transition table is given in Table 5.31,
accepting SimplePal. Consider the simplistic preliminary approach to
obtaining a CFG described in the discussion preceding Theorem 5.29. The
states of M are ignored, the variables of the grammar are the stack
symbols of M, and for every move that reads σ and replaces A on the
stack by BC . . . D, we introduce the production A →σBC . . . D. Show
that although the string aa is not accepted by M, it is generated by the
resulting CFG.
5.36.
If the PDA in Theorem 5.29 is deterministic, what nice property does the
resulting grammar have? Can it have this property without the original
PDA being deterministic?
5.37.
Find the other useless variables in the CFG obtained in Example 5.30.
5.38.
In each case, the grammar with the given productions satisﬁes the LL(1)
property. For each one, give a transition table for the deterministic PDA
obtained as in Example 5.32.
a. S →S1$
S1 →AS1 | 
A →aA | b
b. S →S1$
S1 →aA
A →aA | bA | 
c. S →S1$
S1 →aAB | bBA
A →bS1 | a
B →aS1 | b
5.39.
If G is the CFG with productions S →T $ and T →T [T ] | , then you
can see by considering an input string like [][][]$, which has the
leftmost derivation
S ⇒T $ ⇒T [T ]$ ⇒T [T ][T ]$ ⇒T [T ][T ][T ]$
⇒∗[][][]$
that the combination of next input symbol and top stack symbol does
not determine the next move. The problem, referred to as left recursion,
is the production using the variable T whose right side starts with T .
In general, if a CFG has the productions T →T α | β, where β does
not begin with T , the left recursion can be eliminated by noticing
that the strings derivable from T using these productions are strings
of the form βαn, where n ≥0. The productions T →βU and U →
αU |  then generate the same strings with no left recursion. Use this
technique to ﬁnd an LL(1) grammar corresponding to the
grammar G.
5.40.
Another situation that obviously prevents a CFG from being LL(1)
is several productions involving the same variable whose right sides
begin with the same symbol. The problem can often be eliminated by
factoring: For example, the productions T →aα | aβ can be replaced
by T →aU and U →α | β. Use this technique (possibly more than

Exercises
203
once) to obtain an LL(1) grammar from the CFG G having
productions
S →T $
T →[T ] | []T | [T ]T | 
5.41.
In each case, the grammar with the given productions does not satisfy the
LL(1) property. Find an equivalent LL(1) grammar by factoring and
eliminating left recursion (see Exercises 5.39 and 5.40).
a. S →S1$
S1 →aaS1b | ab | bb
b. S →S1$
S1 →S1A | 
A →Aa | b
c. S →S1$
S1 →S1T | ab
T
→aT bb | ab
d. S →S1$
S1 →aAb | aAA | aB | bbA
A →aAb | ab
B →bBa | ba
5.42.
Show that for the CFG in part (c) of the previous exercise, if the last
production were T →a instead of T →ab, the grammar obtained by
factoring and eliminating left recursion would not be LL(1). (Find a string
that doesn’t work, and identify the point at which looking ahead one
symbol in the input isn’t enough to decide what move the PDA should
make.)
5.43.
Consider the CFG with productions
S →S1$
S1 →S1 + T | T
T →T ∗F | F
F →[S1] | a
a. Write the CFG obtained from this one by eliminating left recursion.
b. Give a transition table for a DPDA that acts as a top-down parser for
this language.
5.44.
Let G be the CFG with productions
S →S1$
S1 →[S1 + S1]|[S1 ∗S1]|a
such that L(G) is the language of all fully parenthesized algebraic
expressions involving the operators + and ∗and the identiﬁer a. Describe
a deterministic bottom-up parser obtained from this grammar as in
Example 5.34.
5.45.
Let G have productions
S →S1$
S1 →S1[S1]|S1[]|[S1]|[]
and let G1 have productions
S →S1$
S1 →[S1]S1|[S1]|[]S1|[]
a. Describe a deterministic bottom-up parser obtained from G as in
Example 5.34.
b. Show that G1 is not a weak precedence grammar.

204
C H A P T E R 5
Pushdown Automata
5.46.
a. Say exactly what the precedence relation is for the grammar in
Example 5.34. In other words, for which pairs (X, σ), where X is a
stack symbol and σ an input symbol, is it correct to reduce when X is
on top of the stack and σ is the next input?
b. Answer the same question for the larger grammar (also a weak
precedence grammar) with productions
S →S1$
S1 →S1 + T | S1 −T | T
T →T ∗F | T /F | F
F →[S1] | a

205
C
H
A
P
T
E
R
6
Context-Free and
Non-Context-Free Languages
N
ot all useful languages can be generated by context-free grammars, and a
pushdown automaton is limited signiﬁcantly by having to follow the rules of
a stack in accessing its memory. The pumping lemma for context-free languages,
like the pumping lemma for regular languages in Chapter 2, describes a property
that every context-free language must have, and as a result it allows us to show
that certain languages are not context-free. A slightly stronger result can be used
in some cases where the pumping lemma doesn’t apply. These results also suggest
algorithms to answer certain decision problems involving context-free languages.
Some of the languages that are shown not to be context-free illustrate differences
between regular languages and context-free languages having to do with closure
properties of certain set operations. In the second section of the chapter, some of
these differences are explored and some partial results obtained.
6.1 THE PUMPING LEMMA FOR
CONTEXT-FREE LANGUAGES
Once we deﬁne a ﬁnite automaton, it is not hard to think of languages that can’t be
accepted by one, even if proving it is a little harder. To accept AnBn = {anbn | n ≥
0}, for example, the ﬁniteness of the set of states makes it impossible after a while
to remember how many a’s have been read and therefore impossible to compare
that number to the number of b’s that follow.
We might argue in a similar way that neither
AnBnCn = {anbncn | n ≥0}
nor
XX = {xx | x ∈{a, b}∗}

206
C H A P T E R 6
Context-Free and Non-Context-Free Languages
can be accepted by a PDA. The way a PDA processes an input string aibjck allows
it to conﬁrm that i = j but not to remember that number long enough to compare
it to k. In the second case, even if we use nondeterminism to begin processing the
second half of the string, the symbol that we must compare to the next input is at
the bottom of the stack and therefore inaccessible.
One way to prove that AnBn is not regular is to use the pumping lemma
for regular languages. In this section we will establish a result for context-free
languages that is a little more complicated but very similar. The easiest way to
derive it is to look at context-free grammars rather than PDAs.
The principle behind the earlier pumping lemma is that if an input string is
long enough, a ﬁnite automaton processing it will have to enter some state a second
time. A corresponding principle for CFGs is that a sufﬁciently long derivation in a
grammar G will have to contain a self-embedded variable A; that is, the derivation
(or at least one with the same derivation tree) looks like
S ⇒∗vAz ⇒∗vwAyz ⇒∗vwxyz
so that the string derived from the ﬁrst occurrence of A is wAy and the string
derived from the second occurrence is x. (All ﬁve of the strings v, w, x, y, and z
are strings of terminals.) As a result,
S ⇒∗vAz ⇒∗vwAyz ⇒∗vw2Ay2z ⇒∗vw3Ay3z ⇒∗. . .
The string x can be derived from any one of these occurrences of A, so that each
of the strings vxz, vwxyz, vw2xy2z, . . . can be derived from S.
This observation will be useful if we can guarantee that the strings w and y
are not both null, and even more useful if we can impose some other restrictions on
the ﬁve strings of terminals. The easiest way to take care of both these objectives
is to modify the grammar so that it has no -productions or unit productions, and
for added convenience we will use an equivalent grammar in Chomsky normal
form.
Theorem 6.1
The Pumping Lemma for Context-Free Languages
Suppose L is a context-free language. Then there is an integer n so that
for every u ∈L with |u| ≥n, u can be written as u = vwxyz, for some
strings v, w, x, y, and z satisfying
1. |wy| > 0
2. |wxy| ≤n
3. for every m ≥0, vwmxymz ∈L
Proof
According to Theorem 4.31, we can ﬁnd a context-free grammar G so that
L(G) = L −{} and G is in Chomsky normal form, so that the right side
of every production is either a single terminal or a string of two variables.
Every derivation tree in this grammar is then a binary tree. By the height
of a derivation tree we will mean the number of edges (one less than the
number of nodes) in the longest path from the root to a leaf node.

6.1
The Pumping Lemma for Context-Free Languages
207
C1
S
b
a
C3
C4
C2
a
C5
b
B
C6
A
a
C9
C8
A
b
b
C7
u = (ab)(b)(ab)(b)(a)
   = v  w x y z
Figure 6.2
How the strings in the pumping lemma might
be obtained.
A binary tree of height h has no more than 2h leaf nodes. (This
fact can be proved using mathematical induction on h. See Exercise 6.1.)
Therefore, if u ∈L(G) and h is the height of a derivation tree for x, then
|u| ≤2h.
Let n be 2p+1, where p is the number of distinct variables in the
grammar G, and suppose that u is a string in L(G) of length at least n.
Then |u| > 2p, and it follows that every derivation tree for u must have
height greater than p. In other words, in a derivation tree for u, there must
be a path from the root to a leaf node with at least p + 1 interior nodes
(nodes other than the leaf node).
Consider the portion of the longest path consisting of the leaf node
and the p + 1 nodes just above it. Since every interior node corresponds
to a variable, and there are only p distinct variables in the grammar, this
portion of the path contains at least two occurrences of the same variable
A. Let x be the substring of u derived from the occurrence of A farthest
down on the path (closest to the leaf), and let w and y be the strings of
terminals such that the substring of u derived from the occurrence of A
farther from the leaf is wxy. Finally, let v and z be the preﬁx and the
sufﬁx of u that account for the remainder of the string, so that u = vwxyz.
(Figure 6.1 illustrates what all of this might look like.)
If N is the node corresponding to the occurrence of A farther from
the leaf node, the subtree with root node N has height p + 1 or less.
It follows that |wxy| ≤2p+1 = n. The leaf nodes corresponding to the

208
C H A P T E R 6
Context-Free and Non-Context-Free Languages
symbols of x are descendants of only one of the two children of N, and
because G is in Chomsky normal form, the other child also has descendant
nodes corresponding to terminal symbols. Therefore, w and y can’t both
be . Finally, we have
S ⇒∗vAz ⇒∗vwAyz ⇒∗vwxyz
where the two occurrences of A are the one farther from the leaf node
and the one closer to it, respectively. We have already seen how the third
conclusion of the theorem follows from this fact.
The comments we made in preparing to use the pumping lemma for regular
languages still apply here. If we are using Theorem 6.1 to prove by contradiction
that a language L is not a CFL, we start off assuming that it is, and we let n be the
integer whose existence is then guaranteed by the pumping lemma. The pumping
lemma makes an assertion about every string in L with length at least n, but the
only strings that will do us any good in the proof are those that will produce a
contradiction, and we try to select one of these to start with.
We can apply the pumping lemma to strings in L of length at least n, and all
we know about n is that it is “the integer in the pumping lemma.” For this reason,
the string u we select must be deﬁned in terms of n. Once we have chosen a string
u that looks promising, the pumping lemma says that some ways of breaking up
u into the ﬁve pieces v, w, x, y, z (not all ways) satisfy the three conditions.
It’s not sufﬁcient to look at one choice of v, . . . , z and show that if those ﬁve
strings satisfy the three conditions, they lead to a contradiction, because they may
not satisfy them. The only way to be sure of a contradiction is to show that for the
string u we have chosen, every choice of v, . . . , z satisfying the three conditions
leads to a contradiction.
EXAMPLE 6.3
Applying the Pumping Lemma to AnBnCn
Suppose for the sake of contradiction that AnBnCn is a context-free language, and let n
be the integer in the pumping lemma. Let u be the string anbncn. Then u ∈AnBnCn, and
|u| ≥n; therefore, according to the pumping lemma, u = vwxyz for some strings satisfying
the conditions 1–3.
Condition 1 implies that the string wxy contains at least one symbol, and condition
2 implies that it can contain no more than two distinct symbols. If σ1 is one of the three
symbols that occurs in wy, and σ2 is one of the three that doesn’t, then the string vw0xy0z
obtained from u by deleting w and y contains fewer than n occurrences of σ1 and exactly n
occurrences of σ2. This is a contradiction, because condition 3 implies that vw0xy0z must
have equal numbers of all three symbols. (We could also have obtained a contradiction by
considering m > 1 in condition 3.)
As you can see, we have shown that the string vxz = vw0xy0z is not only not an
element of AnBnCn, but also not an element of the larger language {t ∈{a, b, c}∗| na(t) =

6.1
The Pumping Lemma for Context-Free Languages
209
nb(t) = nc(t)}, and so our argument also works as a proof that this larger language is not
a CFL.
When we use the regular-language version of the pumping lemma, if n is the
integer, the portion of the string that is pumped occurs within the preﬁx of length
n. In Theorem 6.1, on the other hand, all we know about the location of w and y
in the string is that they occur within some substring of length n. As Example 6.4
illustrates, it is often necessary to consider several possible cases and to show that
in each case we can obtain a contradiction.
EXAMPLE 6.4
Applying the Pumping Lemma to XX
Suppose XX is a CFL, and let n be the integer in the pumping lemma. This time we will
choose u = anbnanbn, which is an element of XX whose length is at least n. Suppose
u = vwxyz and that these ﬁve strings satisfy conditions 1–3. As in Example 6.3, the sub-
string wxy contains at least one symbol and can overlap at most two of the four contiguous
groups of symbols.
We consider ﬁrst the case in which wy contains at least one a from the ﬁrst group of
a’s. This means that w does, or y does, or both do. Then neither w nor y can contain any
symbols from the second half of u. The string vw0xy0z then looks like
vw0xy0z = aibjanbn
for some i and j satisfying i < n and j ≤n. (j is less than n if wy also contained at least
one b.) If this string has odd length, it is certainly not an element of XX; if it has even length,
its midpoint is between two occurrences of a in the substring an, so that its ﬁrst half ends
with a. In either case, we have a contradiction, because vw0xy0z cannot be of the form xx.
In the second case, suppose that neither w nor y contains an a from the ﬁrst group of
a’s but that wy contains at least one b from the ﬁrst group of b’s. Then it may also contain
a’s from the second group, but it cannot contain b’s from the second group. This time,
vw0xy0z = anbiajbn
for some i with i < n and some j with j ≤n. If this string has even length, its midpoint
occurs somewhere within the substring biaj, and it is impossible for the ﬁrst half of the
string to end with bn. Again this contradicts condition 3.
It is sufﬁcient to consider two more cases, one in which wy contains no symbols from
the ﬁrst half of u but at least one a, and one in which wy contains only b’s from the second
group. In both these cases, we can obtain a contradiction by using m = 0 in statement 3,
and the reasoning is almost the same as in the ﬁrst two cases.
Just as in Example 6.3, this proof also shows that some other languages, such as
{aibiaibi | i ≥0}
and
{aibjaibj | i, j ≥0}
are not context-free.

210
C H A P T E R 6
Context-Free and Non-Context-Free Languages
Sometimes, as in the next example, different cases in the proof require different
choices of m in condition 3 to obtain a contradiction.
EXAMPLE 6.5
Applying the Pumping Lemma to {x ∈{a, b, c}∗| na(x) < nb(x) and
na(x) < nc(x)}
Let L = {x ∈{a, b, c}∗| na(x) < nb(x) and na(x) < nc(x)}, suppose that L is a CFL, and
let n be the integer in the pumping lemma. Let u = anbn+1cn+1, and let v, w, x, y, and z
be strings for which u = vwxyz and conditions 1–3 are satisﬁed.
If wy contains an a, then because of condition 2 it cannot contain a c. It follows that
vw2xy2z, obtained by adding one copy of w and y to u, has at least n + 1 a’s and exactly
n + 1 c’s. On the other hand, if wy does not contain an a, then it contains either a b or a
c; in this case, vw0xy0z contains exactly n a’s, and either no more than n b’s or no more
than n c’s. In both cases, we obtain a contradiction.
EXAMPLE 6.6
The Set of Legal C Programs Is Not a CFL
Although much of the syntax of programming languages like C can be described by context-
free grammars, there are some rules that depend on context, such as the rule that a variable
must be deﬁned before it is used. In the simplest case, checking that this rule is obeyed
involves testing whether a string has the form xyx, where x is a variable name and y is the
portion of the program between the variable declaration and its use. We know from Example
6.4 that the language XX is not a CFL, and it is not surprising that a language whose strings
must satisfy a similar restriction is not either.
Let L be the language of legal C programs, and suppose for the sake of contradiction
that L is a CFL. Let n be the integer in the pumping lemma. We won’t need to know much
about the C language, other than the fact that the string
main(){int aaa...a;aaa...a;aaa...a;}
in which each of the three strings of a’s has exactly n + 1 is an element of L. This will be
our choice of u. It consists of a rudimentary header, a declaration of an integer variable, and
two subsequent expressions, both consisting of just the variable name. The blank space after
“int” is necessary as a separator. If the program were executed, the expression aaa...a
would be “evaluated” twice, although the value would be meaningless in both cases because
the variable is not initialized.
According to the pumping lemma, u = vwxyz for some strings v, w, x, y, and z
satisfying conditions 1–3. We will show that in every possible case, condition 3 with m = 0
produces a contradiction.
If wy contains any of the ﬁrst six symbols of u, then vxz cannot be a program because
it doesn’t have a proper header. If it contains the left or right brace, then even if vxz has
a legal header, it doesn’t have the braces that must enclose the body of the program. If it
contains any of the four characters after the left brace, then there is not enough left in vxz
to qualify as a variable declaration. (Without the blank, “int” would be interpreted as part
of an identiﬁer.)

6.1
The Pumping Lemma for Context-Free Languages
211
The only remaining cases are those in which wxy is a substring of
aaa...a;aaa...a;aaa...a;
If wy contains the ﬁnal semicolon, then vxz is illegal, since there are still some of the a’s in
the last group remaining, and every statement (even if it is just an expression) must end with
a semicolon. If wy contains one of the two preceding semicolons, and possibly portions of
one or both of the identiﬁers on either side, then vxz has a variable declaration and another
identiﬁer; the two identiﬁers can’t match, because one has length n + 1 and the other is
longer. Finally, if wy contains only a portion of one of the identiﬁers (it can’t contain all
of it), then vxz contains a variable declaration and two subsequent expressions, but the
three identiﬁers are not all the same. In all three of these cases, the declaration-before-use
principle is violated.
The argument would almost work with u chosen to be the shorter program containing
only two occurrences of the identiﬁer. The case in which it fails is the one in which wy
contains the ﬁrst semicolon and nothing else. Deleting it would still leave a valid program
consisting of a single variable declaration, and extra copies could be interpreted as harmless
empty statements.
There are other examples of rules in C that cannot be tested by a PDA. For example, if
a program deﬁnes two functions f and g having n and m formal parameters, respectively,
and then makes calls on f and g, the numbers of parameters in the calls must agree with the
numbers in the corresponding deﬁnitions. Testing this condition is enough like recognizing
a string of the form anbmanbm (see Example 6.4) to suggest that it would also be enough
to keep L from being a CFL.
In the rest of this section, we present a result that is slightly stronger than the
pumping lemma and is referred to as Ogden’s lemma. Conditions 1 and 2 in the
pumping lemma provide some information about the strings w and y that are pumped,
but not much about their location in u. Ogden’s lemma allows us to designate cer-
tain positions of u as “distinguished” and to guarantee that some of the distin-
guished positions appear in the pumped portions. It is sometimes more convenient
than the pumping lemma and can occasionally be used when the pumping lemma fails.
Theorem 6.7
Ogden’s Lemma
Suppose L is a context-free language. Then there is an integer n so that for
every u ∈L with |u| ≥n, and every choice of n or more “distinguished”
positions in the string u, there are strings v, w, x, y, and z so that u =
vwxyz and the following conditions are satisﬁed.
1. The string wy contains at least one distinguished position.
2. The string wxy contains n or fewer distinguished positions.
3. For every m ≥0, vwmxymz ∈L.
Proof
As in the proof of the pumping lemma, we let G be a context-free grammar
in Chomsky normal form generating L −{}, and n = 2p+1, where p

212
C H A P T E R 6
Context-Free and Non-Context-Free Languages
is the number of variables in G. Suppose u ∈L, that n or more distinct
positions in u are designated “distinguished,” and that we have a derivation
tree for u in G. We describe a path from the root of this tree to a leaf
node that will give us the results we want.
The root node N0 is the ﬁrst node in the path. In general, if i ≥0,
the interior node Ni is the node most recently added to the path, and
Ni has two children, then Ni+1 is chosen to be the child with more dis-
tinguished descendants (leaf-node descendants of Ni+1 that correspond to
distinguished positions in u), or chosen arbitrarily if the two children have
the same number.
We will call an interior node in the path a branch point if it has two
children and both have at least one distinguished descendant. If we let
d(N) stand for the number of distinguished descendants of N, then the
way we have constructed the path implies that
d(Ni+1) = d(Ni) if Ni is not a branch point
d(Ni)/2 ≤d(Ni+1) < d(Ni) if Ni is a branch point
From these two statements, it follows that if Ni and Nj are consecutive
branch points on the path, with j > i, then
d(Ni)/2 ≤d(Nj) < d(Ni)
The induction argument required for the proof of Theorem 6.1 shows that
if a path from the root has h interior nodes, then the total number of
leaf-node descendants of the root node is no more than 2h. A very similar
argument shows that if our path has h branch points, then the total number
of distinguished descendants of the root node is no more than 2h.
As a result, our former argument can be used again, with distin-
guished descendants instead of descendants. Since there are more than 2p
distinguished leaf nodes, there must be more than p branch points in our
path. If as before we choose the p + 1 branch points on the path that
are closest to the leaf, then two of these must be labeled with the same
variable A, and we obtain the ﬁve strings v, w, x, y, and z exactly as
before. Condition 1 is true because the node labeled A that is higher up
on the path is a branch point.
As you can see, the pumping lemma is simply the special case of Ogden’s
lemma in which all the positions of u are distinguished.
EXAMPLE 6.8
Ogden’s Lemma Applied to {aibic j | j ̸= i}
Let L be the language {aibicj | j ̸= i}. Suppose L is a CFL, and let n be the integer in
Ogden’s lemma. For reasons that are not obvious now but will be shortly, we choose
u = anbncn+n!

6.1
The Pumping Lemma for Context-Free Languages
213
and we designate the ﬁrst n positions of u (the positions of the a’s) as distinguished. Suppose
u = vwxyz for some strings v, . . . , z satisfying conditions 1–3.
If either the string w or the string y contains two distinct symbols, then considering
m = 2 in condition 3 produces a contradiction, because the string vw2xy2z no longer matches
the regular expression a∗b∗c∗. Suppose neither w nor y contains two different symbols.
Because of condition 1, one of these two strings is ap for some p > 0.
We can easily take care of the following cases:
1.
w = ap and y = aq, where at least one of p and q is positive.
2.
w = ap and y = bq, where p > 0 and p ̸= q.
3.
w = ap and y = cq, where p > 0.
In each of these cases, the string vwmxymz will have different numbers of a’s and b’s as
long as m ̸= 1.
The only remaining case is the one in which w = ap and y = bp for some p > 0. In
this case we will not be able to get a contradiction by ﬁnding an m so that vwmxymz has
different numbers of a’s and b’s. But we can choose any value of m, and the one we choose
is k + 1, where k is the integer n!/p. The number of a’s in vwmxymz, which has m −1
more copies of w than the original string u, is
n + (m −1) ∗p = n + k ∗p = n + n!
and we have our contradiction, because the string has the same number of a’s as c’s.
EXAMPLE 6.9
Using Ogden’s Lemma when the Pumping Lemma Fails
You might suspect that the ordinary pumping lemma is not enough to take care of Example
6.8. Without knowing that some of the a’s are included in the pumped portion, there is no
way to rule out the case in which both w and y contain only c’s, and it doesn’t seem likely
that we can select m so that the number of c’s in vwmxymz is changed just enough to make
it equal to the number of a’s.
In this example we consider the language
L = {apbqcrds | p = 0 or q = r = s}
and we suppose that L is a CFL. This time we can prove that the pumping lemma doesn’t
allow us to obtain a contradiction, because the conclusions of the pumping lemma are all
true for this language. (This is the analogue for CFLs of Example 2.39.) To see this, let n
be any positive integer and u any element of L with |u| ≥n, say u = apbqcrds. If p = 0,
then we can take v to be , w to be the ﬁrst symbol of u, x and y to be , and z to be the
remaining sufﬁx of u; it is clear that conditions 1–3 hold. If p > 0, on the other hand, then
we can take w to be a, z to be ap−1bqcrds, and the other three strings to be ; no matter
what m is in condition 3, the string vwmxymz still has equal numbers of b’s, c’s, and d’s
and is therefore an element of L.
Instead, we let n be the integer in Ogden’s lemma, we choose u = abncndn, and we
specify that all the positions of u except the ﬁrst be distinguished. Suppose v, w, x, y, and
z are strings satisfying u = vwxyz and conditions 1–3. Then the string wy must contain
at least one b, one c, or one d, and it can’t contain all three symbols. Therefore, vw2xy2z

214
C H A P T E R 6
Context-Free and Non-Context-Free Languages
still contains at least one a, but it no longer has equal numbers of b’s, c’s, and d’s. The
conclusion is that this string cannot be in L, which is a contradiction.
6.2 INTERSECTIONS AND COMPLEMENTS
OF CFLs
The set of context-free languages, like the set of regular languages, is closed under
the operations of union, concatenation, and Kleene ∗. However, unlike the set of
regular languages, the set of CFLs is not closed under intersection or difference,
and we can demonstrate this by looking at the ﬁrst two examples in Section 6.1.
EXAMPLE 6.10
Two CFLs Whose Intersection is Not a CFL
Example 6.3 uses the pumping lemma for context-free languages to show that AnBnCn is
not a CFL. It is easy to write this language as the intersection of two others and not hard to
see that the two others are both context-free.
AnBnCn can be written
AnBnCn = {aibjck | i = j and j = k}
= {aibick | i, k ≥0} ∩{aibjcj | i, j ≥0}
The two languages involved in the intersection are similar to each other. The ﬁrst can be
written as the concatenation
{aibi | i ≥0}{ck | k ≥0} = L1L2
and the second as the concatenation
{ai | i ≥0}{bjcj | j ≥0} = L3L4
and we know how to ﬁnd a context-free grammar for the concatenation of two languages,
given a CFG for each one.
It would also not be difﬁcult to construct pushdown automata directly for the two
languages L1L2 and L3L4. In the ﬁrst case, for example, a PDA could save a’s on the
stack, match them with b’s, and remain in the accepting state while reading any number
of c’s.
EXAMPLE 6.11
A CFL Whose Complement Is Not a CFL
Surprisingly, although we showed in Example 6.4 that the language XX= {xx | x ∈{a, b}∗}
is not a context-free language, its complement is.
Let L be the complement {a, b}∗−XX. L contains every string in {a, b}∗of odd length.
If x ∈L and |x| = 2n for some n ≥1, then for some k with 1 ≤k ≤n, the kth and (n + k)th
symbols of x are different (say a and b, respectively). There are k −1 symbols before the
a, n −1 symbols between the two, and n −k symbols after the b. But instead of thinking
of the n −1 symbols between the two as n −k and then k −1 (the remaining symbols in
the ﬁrst half, followed by the symbols in the second half that precede the b),

6.2
Intersections and Complements of CFLs
215
a
b
we can think of them as k −1 and then n −k.
a
b
In other words, x is the concatenation of two odd-length strings, the ﬁrst with a in the
middle and k −1 symbols on either side, and the second with b in the middle and n −k
symbols on either side. Conversely, every concatenation of two such odd-length strings is
in L.
The conclusion is that L can be generated by the CFG with productions
S →A | B | AB | BA
A →EAE | a
B →EBE | b
E →a | b
The variables A and B generate odd-length strings with middle symbol a and b, respectively,
and together generate all odd-length strings. Therefore, L is a context-free language whose
complement is not a CFL.
EXAMPLE 6.12
Another Example with Intersections and Complements
Suppose L1, L2, and L3 are deﬁned as follows:
L1 = {aibjck | i ≤j}
L2 = {aibjck | j ≤k}
L3 = {aibjck | k ≤i}
For each i it is easy to ﬁnd both a CFG generating Li and a PDA accepting it. Therefore,
L1 ∩L2 ∩L3 represents another way of expressing the language AnBnCn as an intersection
of context-free languages.
A string can fail to be in L1 either because it fails to be in R, the regular language
{a}∗{b}∗{c}∗, or because it is aibjck for some i, j, and k with i > j. Using similar arguments
for L2 and L3, we obtain
L′
1 = R′ ∪{aibjck | i > j}
L′
2 = R′ ∪{aibjck | j > k}
L′
3 = R′ ∪{aibjck | k > i}
The language R′ is regular and therefore context-free, and the second language in each of
these unions is a CFL, just as the languages L1, L2, and L3 are. It follows that each of the
languages L′
i is a CFL and that their union is. Therefore, because of the formula
(L′
1 ∪L′
2 ∪L′
3)′ = L1 ∩L2 ∩L3
we conclude that L′
1 ∪L′
2 ∪L′
3 is another context-free language whose complement is not
a CFL.
We can derive a similar result without involving the language R by considering
A1 = {x ∈{a, b, c}∗| na(x) ≤nb(x)}
A2 = {x ∈{a, b, c}∗| nb(x) ≤nc(x)}
A3 = {x ∈{a, b, c}∗| nc(x) ≤na(x)}

216
C H A P T E R 6
Context-Free and Non-Context-Free Languages
The intersection of these three languages is {x ∈{a, b, c}∗| na(x) = nb(x) = nc(x)}, which
we saw in Example 6.3 is not a CFL. Therefore, A′
1 ∪A′
2 ∪A′
3 is a CFL whose complement
is not.
We showed in Section 2.2 that if M1 and M2 are both ﬁnite automata, we can
construct another FA that processes input strings so as to simulate the simultaneous
processing by M1 and M2; we can simply take states to be ordered pairs (p, q),
where p and q are states in M1 and M2, respectively. The construction works for
any of the three languages L(M1) ∪L(M2), L(M1) ∩L(M2), and L(M1) −L(M2),
as long as we choose accepting states appropriately.
It is unreasonable to expect this construction to work for PDAs, because the
current state of a PDA is only one aspect of the current conﬁguration. We could
deﬁne states (p, q) so that knowing the state of the new PDA would tell us the
states of the two original PDAs, but there is no general way to use one stack so as
to simulate two stacks.
There’s nothing to stop us from using the Cartesian-product construction with
just one stack, if it turns out that one stack is all we need. In the special case when
one of the PDAs doesn’t use its stack, because it’s actually an FA, the construction
works.
Theorem 6.13
If L1 is a context-free language and L2 is a regular language, then L1 ∩L2
is a CFL.
Sketch of Proof
Let M1 = (Q1, , , q1, Z0, A1, δ1) be a PDA accept-
ing L1 and M2 = (Q2, , q2, A2, δ2) an FA accepting L2. Then we deﬁne
the PDA M = (Q, , , q0, Z0, A, δ) as follows.
Q = Q1 × Q2
q0 = (q1, q2)
A = A1 × A2
For p ∈Q1, q ∈Q2, and Z ∈,
1. For every σ ∈, δ((p, q), σ, Z) is the set of pairs ((p′, q′), α) for which
(p′, α) ∈δ1(p, σ, Z) and δ2(q, σ) = q′.
2. δ((p, q), , Z) is the set of pairs ((p′, q), α) for which (p′, α) ∈δ1(p,
, Z).
This PDA’s computation simulates that of M1, because for each move M
consults the state of M1 (the ﬁrst component of the state-pair), the input,
and the stack; it also simulates the computation of M2, which requires
only the state of M2 and the input. M is nondeterministic if M1 is, but the
nondeterminism does not affect the second part of the state-pair. Similarly,
if M1 makes a -transition, then so does M, but the second component of
the state-pair is unchanged. The stack is used as if it were the stack of M1.
The conclusion of the theorem follows from the equivalence of
these two statements, for every two input strings y and z, every

6.2
Intersections and Complements of CFLs
217
state-pair (p, q), every string α of stack symbols, and every integer
n ≥0:
1. (q1, yz, Z1) ⊢n
M1 (p, z, α) and δ∗
2(q2, y) = q.
2. ((q1, q2), yz, Z1) ⊢n
M ((p, q), z, α).
Both directions can be proved by a straightforward induction argument.
We will show only the induction step in the proof that statement 1 implies
statement 2.
Suppose k ≥0 and that for every n ≤k, statement 1 implies statement
2. Now suppose that
(q1, yz, Z1) ⊢k+1
M1 (p, z, α) and δ∗
2(q2, y) = q
We want to show that
((q1, q2), yz, Z1) ⊢k+1
M
((p, q), z, α)
If the last move in the sequence of k + 1 moves of M1 is a -transition,
then
(q1, yz, Z1) ⊢k
M1 (p′, z, β) ⊢M1 (p, z, α)
for some p′ ∈Q1 and some β ∈∗. In this case, the induction hypothesis
implies that
((q1, q2), yz, Z1) ⊢k
M ((p′, q), z, β)
and from part 2 of the deﬁnition of δ we obtain
((p′, q), z, β) ⊢M ((p, q), z, α)
which implies the result. Otherwise, y = y′σ, where σ ∈, and
(q1, y′σz, Z1) ⊢k
M1 (p′, σz, β) ⊢M1 (p, z, α)
If we let q′ = δ∗
2(q2, y′), then the induction hypothesis implies that
((q1, q2), y′σz, Z1) ⊢k
M ((p′, q′), σz, β)
and part 1 of the deﬁnition of δ implies
((p′, q′), σz, β) ⊢M ((p, q), z, α)
which gives us the conclusion we want.
We have convinced ourselves (see Examples 6.10 and 6.11) that there is no
general way, given PDAs M1 and M2 that actually use their stacks, to construct
a PDA simulating both of them simultaneously. One might still ask, since the
construction for FAs worked equally well for unions and intersections, what makes
it possible for a PDA to accept the union of two arbitrary CFLs. Perhaps the
simplest answer is nondeterminism. If M1 and M2 are PDAs accepting L1 and L2,
respectively, we can construct a new PDA M as follows: its ﬁrst move, chosen

218
C H A P T E R 6
Context-Free and Non-Context-Free Languages
nondeterministically, is a -transition to the initial state of either M1 or M2; after
that, M’s moves are exactly those of the PDA it has chosen in the ﬁrst move. In
other words, M chooses nondeterministically whether to test for membership in the
ﬁrst language or the second. Testing for both is obviously harder, and that’s what
is necessary in order to accept the intersection.
Thinking about nondeterminism also helps us to understand how it might be
that no PDA can accept precisely the strings in L′, even if there is a PDA that
accepts precisely the string in L. For example, a PDA M might be able to choose
between two sequences of moves on an input string x, so that both choices read
all the symbols of x but only one causes M to end up in an accepting state. In
this case, the PDA obtained from M by reversing the accepting and nonaccepting
states will still have this property, and x will be accepted by both the original PDA
and the modiﬁed one.
Even if M is a deterministic PDA that accepts L, the presence of -transitions
might prevent the PDA M′ obtained from M by reversing the accepting and nonac-
cepting states from accepting L′, in at least two ways.
Table 5.14 shows the transitions of a deterministic PDA M that accepts the
language AEqB = {x ∈{a, b}∗| na(x) = nb(x)}. The string ab is accepted as a
result of these moves:
(q0, ab, Z0) ⊢(q1, b, aZ0) ⊢(q1, , Z0) ⊢(q2, , Z0)
The modiﬁed PDA M′ making the same sequence of moves would end in a nonac-
cepting state; unfortunately, M′ enters q2 by a -transition from q1, so that by the
time it enters the nonaccepting state q2, it’s too late—ab has already been accepted.
Another potential complication is that M could allow an inﬁnite sequence of
consecutive -transitions, which for a string x not in L(M) could prevent x from
being read completely. In this case, the PDA M′ would still not accept such a
string.
It is easy to see that for a DPDA M without -transitions, the DPDA M′
obtained this way accepts the complement of L(M). (You can check that for the
PDA M shown in Table 5.15, M′ accepts the language of all strings in {a, b}∗with
unequal numbers of a’s and b’s.) It is not as easy to see, but still true, that for
every language L accepted by a DPDA, L′ can also be accepted by a DPDA. This
result allows us to say in particular that if L is a context-free language for which
L′ is not a CFL, then L cannot be accepted by a DPDA. The language Pal, on
the other hand, is not accepted by a DPDA (Theorem 5.1), even though both the
language and its complement are CFLs (Example 4.3).
6.3 DECISION PROBLEMS INVOLVING
CONTEXT-FREE LANGUAGES
We have shown in Chapter 5 that starting with a context-free grammar G, there
are at least two simple ways of obtaining a pushdown automaton accepting L(G),
and that starting with a PDA M, there is a way to obtain a context-free grammar

6.3
Decision Problems Involving Context-Free Languages
219
G generating L(M). Because of these algorithms, questions about context-free
languages can be formulated by specifying either a CFG or a PDA.
One of the most basic questions about a context-free language is whether a
particular string is an element of the language. The membership problem for CFLs
is the decision problem
1.
Given a context-free grammar G and a string x, is x ∈L(G)?
For regular languages, it makes sense to ask the question by starting with a
ﬁnite automaton and a string, because the way the FA processes a string makes
the question easy to answer. We can trace the moves the FA makes on the input
string and see whether it ends up in an accepting state. If we started with an
NFA, we could still answer the question this way, once we applied the algo-
rithm that starts with an NFA and produces an equivalent FA. Trying to use
this approach with the membership problem for CFLs, however, would be more
complicated, because a PDA might have nondeterminism that cannot be elimi-
nated; we might have to consider a backtracking algorithm or some other way
of determining whether there is a sequence of moves that causes the string to be
accepted.
As we saw in Section 4.5, there is an algorithm to solve the membership
problem if we have a CFG G that generates L. If x = , deciding whether x ∈L
means deciding whether the start variable S is nullable (Deﬁnition 4.26); otherwise,
we have an algorithm to ﬁnd a CFG G1 with no -productions or unit productions
so that L(G1) = L(G) −{}, and we can decide whether x ∈L(G1) by trying all
possible derivations in G1 with 2|x| −1 or fewer steps.
This algorithm is a theoretical possibility but not likely to be useful in prac-
tice. Because context-free grammars play such a signiﬁcant role in programming
languages and other real-world languages, the membership problem is a practi-
cal problem that needs a practical solution. Fortunately, there are algorithms that
are considerably more efﬁcient. A well-known one is due to Cocke, Younger, and
Kasami and is described in the paper by Younger (1967). Another is an algorithm
by Earley (1970).
There are two decision problems involving regular languages for which we
found algorithms by using the pumping lemma for regular languages, and we
can use the CFL version of the pumping lemma to ﬁnd algorithms to answer the
corresponding questions about context-free languages.
2.
Given a context-free language L, is L nonempty?
3.
Given a context-free language L, is L inﬁnite?
The only signiﬁcant difference from the regular-language case is that there we
found the integer n in the statement of the pumping lemma by considering an
FA accepting the language (see the discussion preceding Theorem 2.29), so that
it was convenient to formulate the problem in terms of an FA; here we ﬁnd n by
considering a grammar generating the language (as in the proof of Theorem 6.1),

220
C H A P T E R 6
Context-Free and Non-Context-Free Languages
so that we would prefer to specify L in terms of a CFG. Once the integer n is
obtained, the algorithms are identical.
With solutions to the regular-language versions of problems 2 and 3, we were
able to solve other decision problems involving regular languages. The CFL ver-
sions of two of these are
4.
Given CFGs G1 and G2, is L(G1) ∩L(G2) nonempty?
5.
Given CFGs G1 and G2, is L(G1) ⊆L(G2)?
If we were able to take the same approach that we took with FAs, we would
construct a context-free grammar G with L(G) = L(G1) ∩L(G2), or with L(G) =
L(G1) −L(G2), and use the algorithm for problem 2 to determine whether L(G)
is nonempty. However, we know from the preceding section that in both cases
there may not be such a G. It seems natural to look for another algorithm. Later,
once we have described a more general model of computation, we will see that
there is none, and that these are both examples of undecidable problems. Studying
context-free grammars has already brought us to the point where we can easily
formulate decision problems for which algorithmic solutions may be impossible.
EXERCISES
6.1.
Show using mathematical induction that a binary tree of height h has no
more than 2h leaf nodes.
6.2.
In each case below, show using the pumping lemma that the given
language is not a CFL.
a. L = {aibjck | i < j < k}
b. L = {a2n | n ≥0}
c. L = {x ∈{a, b}∗| nb(x) = na(x)2
d. L = {anb2nan | n ≥0}
e. L = {x ∈{a, b, c}∗| na(x) = max {nb(x), nc(x)}}
f. L = {x ∈{a, b, c}∗| na(x) = min {nb(x), nc(x)}}
g. {anbmanbn+m | m, n ≥0}
6.3.
In the pumping-lemma proof in Example 6.4, give some examples of
choices of strings u ∈L with |u| ≥n that would not work.
6.4.
In the proof given in Example 6.4 using the pumping lemma, the
contradiction was obtained in each case by considering the string
vw0xy0z. Would it have been possible instead to use vw2xy2z in each
case? If so, give the proof in at least one case; if not, explain why not.
6.5.
For each case below, decide whether the given language is a CFL, and
prove your answer.
a. L = {anbmambn | m, n ≥0}
b. L = {xayb | x, y ∈{a, b}∗and |x| = |y|}

Exercises
221
c. L = {xcx | x ∈{a, b}∗}
d. L = {xyx | x, y ∈{a, b}∗and |x| ≥1}
e. L = {x ∈{a, b}∗| na(x) < nb(x) < 2na(x)}
f. L = {x ∈{a, b}∗| na(x) = 10nb(x)}
g. L = the set of non-balanced strings of parentheses
6.6.
If L is a CFL, does it follow that r(L) = {xr | x ∈L} is a CFL? Give
either a proof or a counterexample.
6.7.
State and prove theorems that generalize to context-free languages
statements I and II, respectively, in Exercise 2.23. Then give an example
to illustrate each of the following possibilities.
a. Theorem 6.1 can be used to show that the language is a CFL, but the
generalization of statement I cannot.
b. The generalization of statement I can be used to show the language is
not a CFL, but the generalization of statement II cannot.
c. The generalization of statement II can be used to show the language is
not a CFL.
6.8.
Show that if L is a DCFL and R is regular, then L ∩R is a DCFL.
6.9.
In each case below, show that the given language is a CFL but that its
complement is not.
a. {aibjck | i ≥j or i ≥k}
b. {aibjck | i ̸= j or i ̸= k}
6.10.
In Example 6.4, the pumping lemma proof began with the string
u = “main()int aaa...a;aaa...a;aaa...a;′′
Redo the proof using Ogden’s lemma and taking u to be
“main()int aaa...a;aaa...a;” (where both strings have n a’s).
6.11.
†Use Ogden’s lemma to show that the languages below are not CFLs.
a. {aibi+kak | k ̸= i}
b. {aibiajbj | j ̸= i}
c. {aibjai | j ̸= i}
6.12.
a. Show that if L is a CFL and F is ﬁnite, L −F is a CFL.
b. Show that if L is not a CFL and F is ﬁnite, then L −F is not a CFL.
c. Show that if L is not a CFL and F is ﬁnite, then L ∪F is not a CFL.
6.13.
For each part of Exercise 6.12, say whether the statement is true if “ﬁnite”
is replaced by “regular”, and give reasons.
6.14.
For each part of Exercise 6.12, say whether the statement is true if “CFL”
is replaced by “DCFL”, and give reasons.
6.15.
Verify that if M is a DPDA with no -transitions, accepting a language L,
then the DPDA obtained from M by reversing the accepting and
nonaccepting states accepts L′.

222
C H A P T E R 6
Context-Free and Non-Context-Free Languages
6.16.
†For each case below, decide whether the given language is a CFL, and
prove your answer.
a. L = {x ∈{a, b}∗| na(x) is a multiple of nb(x)}
b. Given a CFG L, the set of all preﬁxes of elements of L
c. Given a CFG L, the set of all sufﬁxes of elements of L
d. Given a CFG L, the set of all substrings of elements of L
e. {x ∈{a, b}∗| |x| is even and the ﬁrst half of x has more a’s than the
second}
f. {x ∈{a, b, c}∗| na(x), nb(x), and nc(x) have a common factor greater
than 1}
6.17.
†Prove the following variation of Theorem 6.1. If L is a CFL, then there is
an integer n so that for every u ∈L with |u| ≥n, and every choice of u1,
u2, and u3 satisfying u = u1u2u3 and |u2| ≥n, there are strings v, w, x, y,
and z satisfying the following conditions:
1. u = vwxyz
2. wy ̸= 
3. Either w or y is a nonempty substring of u2
4. For every m ≥0, vwixyiz ∈L
Hint: Suppose # is a symbol not appearing in strings in L, and let L1 be
the set of all strings that can be formed by inserting two occurrences of #
into an element of L. Show that L1 is a CFL, and apply Ogden’s lemma
to L1.
This result is taken from Floyd and Beigel (1994).
6.18.
Show that the result in Exercise 6.17 can be used in each part of Exercise
6.11.
6.19.
Show that the result in Exercise 6.17 can be used in Examples 6.8 and 6.9
to show that the language is not context-free.
6.20.
†The set of DCFLs is closed under the operation of complement, as
discussed in Section 6.2. Under which of the following other operations is
this set of languages closed? Give reasons for your answers.
a. Union
b. Intersection
c. Concatenation
d. Kleene ∗
e. Difference
6.21.
Use Exercises 5.20 and 6.8 to show that the following languages are not
DCFLs. This technique is used in Floyd and Beigel (1994), where the
language in Exercise 5.20 is referred to as Double-Duty(L).
a. Pal, the language of palindromes over {a, b} (Hint: Consider the
regular language corresponding to a∗b∗a∗#b∗a∗.)
b. {x ∈{a, b}∗| nb(x) = na(x) or nb(x) = 2na(x)}
c. {x ∈{a, b}∗| nb(x) < na(x) or nb(x) > 2na(x)}

Exercises
223
6.22.
Find an example of a CFL L so that {x#y | x ∈L and xy ∈L} is not a
CFL.
6.23.
†Show that if L ⊆{a}∗is a CFL, then L is regular.
6.24.
†Consider the language L = {x ∈{a, b}∗| na(x) = f (nb(x))}. In Exercise
2.31 you showed that L is regular if and only if f is ultimately periodic;
in other words, L is regular if and only if there is a positive integer p so
that for each r with 0 ≤r < p, f is eventually constant on the set Sp,r =
{jp + r | j ≥0}. Show that L is a CFL if and only if there is a positive
integer p so that for each r with 0 ≤r < p, f is eventually linear on the
set Sp,r. “Eventually linear” on Sp,r means that there are integers N, c,
and d so that for every j ≥N, f (jp + r) = cj + d. (Suggestion: for the
“if” direction, show how to construct a PDA accepting L; for the
converse, use the pumping lemma.)

224
C
H
A
P
T
E
R
7
Turing Machines
A
Turing machine is not just the next step beyond a pushdown automaton. It is,
according to the Church-Turing thesis, a general model of computation, poten-
tially able to execute any algorithm. This chapter presents the basic deﬁnitions as
well as examples that illustrate the various modes of a Turing machine: accepting
a language, computing a function, and carrying out other types of computations
as well. Variants such as multitape Turing machines are discussed brieﬂy. Nonde-
terministic Turing machines are introduced, as well as universal Turing machines,
which anticipate the modern stored-program computer.
We adopt the Turing machine as our way of formulating an algorithm, and
a few of the discussions in the chapter begin to suggest ideas about the limits of
computation that will play an important part in Chapters 8 and 9.
7.1 A GENERAL MODEL OF COMPUTATION
Both of the simple devices we studied earlier, ﬁnite automata and pushdown
automata, are models of computation. A machine of either type can receive an
input string and execute an algorithm to obtain an answer, following a set of rules
speciﬁc to the machine type. In both cases, the machine can execute only very spe-
cialized algorithms: in the case of an FA, algorithms in which there is an absolute
limit on the amount of information that can be remembered, and in the case of a
PDA, algorithms in which it is sufﬁcient to access information following the rules
imposed by a stack.
It is easy to ﬁnd examples of languages that cannot be accepted because of
these limitations. A ﬁnite automaton cannot accept
SimplePal = {xcxr | x ∈{a, b}∗
A device with only a stack can accept SimplePal but not
AnBnCn = {anbncn | n ∈N}
or
L = {xcx | x ∈{a, b}∗}

7.1
A General Model of Computation
225
It is not hard to see that a PDA-like machine with two stacks can accept AnBnCn,
and a ﬁnite automaton with a queue instead of a stack can accept L. Although in
both cases it might seem that the machine is being devised speciﬁcally to handle
one language, it turns out that both devices have substantially more computing
power than either an FA or a PDA, and both are reasonable candidates for a model
of general-purpose computation.
The abstract model we will study instead was designed in the 1930s by the
English mathematician Alan Turing and is referred to today as a Turing machine.
It was not obtained by adding data structures onto a ﬁnite automaton, and it pre-
dates the ﬁnite-automaton and pushdown-automaton models. Turing’s objective
was to demonstrate the inherent limitations of algorithmic methods. He began with
the idea of formulating a model capable in principle of carrying out any algo-
rithm whatsoever; this allowed him to assert that any deﬁciencies of the model
were in fact deﬁciencies of the algorithmic method. The Turing machine was
intended as a theoretical model, not as a practical way to carry out a computa-
tion. However, it anticipated in principle many of the features of modern electronic
computers.
Turing considered a human computer working with a pencil and paper. He
decided that without any loss of generality, the human computer could be assumed
to operate under simple rules: First, the only things written on the paper are
symbols from some ﬁxed ﬁnite alphabet; second, each step taken by the com-
puter depends only on the symbol he is currently examining and on his “state of
mind” at the time; third, although his state of mind may change as a result of his
examining different symbols, only a ﬁnite number of distinct states of mind are
possible.
With these principles in mind, Turing isolated what he took to be the primitive
steps that a human computer carries out during a computation:
1.
Examining an individual symbol on the paper;
2.
Erasing a symbol or replacing it by another;
3.
Transferring attention from one symbol to another nearby symbol.
Some of these elements are reminiscent of our simpler abstract models. A Turing
machine has a ﬁnite alphabet of symbols (actually two alphabets, an input alphabet
and a possibly larger one for use during the computation), and a ﬁnite set of states,
corresponding to the human computer’s states of mind. A sheet of paper is two-
dimensional but for simplicity Turing speciﬁed a linear tape, which has a left end
and is potentially inﬁnite to the right. The tape is marked off into squares, each
of which can hold one symbol; if a square has no symbol on it, we say that it
contains the blank symbol. For convenience, we sometimes think of the squares of
the tape as being numbered left-to-right, starting with 0, although this numbering
is not part of the ofﬁcial model and the numbers are not necessary in order to
describe the operation of the machine. We visualize the reading and writing as
being done by a tape head, which at any time is centered on one square of the
tape.

226
C H A P T E R 7
Turing Machines
In our version of a Turing machine, which is similar though not identical to the
one proposed by Turing, a single move is determined by the current state and the
current tape symbol, and consists of three parts:
1.
Changing from the current state to another, possibly different state;
2.
Replacing the symbol in the current square by another, possibly different
symbol;
3.
Leaving the tape head on the current square, or moving it one square to the
right, or moving it one square to the left if it is not already on the leftmost
square.
The tape serves as the input device (the input is the ﬁnite string of nonblank symbols
on the tape originally), the memory available for use during the computation, and
the output device. The output, if it is relevant, is the string of symbols left on the
tape at the end of the computation.
One crucial difference between a Turing machine and the two simpler machines
we have studied is that a Turing machine is not restricted to one left-to-right pass
through the input, performing its computation as it reads. The input string is present
initially, before the computation starts. Because the tape head will normally start
out at the beginning of the tape, we think of the machine as “reading” its input
from left to right, but it might move its tape head over the squares containing
the input symbols simply in order to go to another portion of the tape. A human
computer working in this format could examine part of the input, modify or erase
part of it, take time out to execute some computations in a different area of the
tape, return to re-examine the input, repeat any of these actions, and perhaps stop
the computation before he has looked at all, or even any, of the input; a Turing
machine can do these things as well.
Although it will be convenient to describe a computation of a Turing machine
in a way that is general enough to include a variety of “subprograms” within some
larger algorithm, the two primary objectives of a Turing machine that we will
focus on are accepting a language and computing a function. The ﬁrst, which we
will discuss in more detail in the next section, is like the computation of a ﬁnite
automaton or pushdown automaton, except that the languages can now be more
general; the second makes sense because of the enhanced output capability of the
Turing machine, and we will discuss it in Section 7.3.
A ﬁnite automaton requires more than one accepting state if for two strings x
and y that are accepted, the information that must be remembered is different; x
and y are both accepted, but the device must allow for the fact that there is more
input to come, and what it does with the subsequent symbols depends on whether it
started with x or with y. A Turing machine does not need to say whether each preﬁx

7.1
A General Model of Computation
227
of the input string is accepted or not. It has the entire string to start with, and once
the machine decides to accept or reject the input, the computation stops. For this
reason, a Turing machine never needs more than two halt states, one that denotes
acceptance and one that denotes rejection. If the computation is one of some other
type, in which acceptance is not the issue, a normal end to the computation usually
means halting in the accepting state, and the rejecting state usually signiﬁes some
sort of “crash,” or abnormal termination.
With Turing machines there is also a new issue, which did not arise with ﬁnite
automata and did not arise in any serious way with PDAs. If a Turing machine
decides to accept or reject the input string, it stops. But it might not decide to do
this, and so it might continue moving forever. This will turn out to be important!
Deﬁnition 7.1
Turing Machines
A Turing machine (TM) is a 5-tuple T = (Q, , , q0, δ), where
Q is a ﬁnite set of states. The two halt states ha and hr are not
elements of Q.
, the input alphabet, and , the tape alphabet, are both ﬁnite sets,
with  ⊆. The blank symbol  is not an element of .
q0, the initial state, is an element of Q.
δ is the transition function:
δ : Q × ( ∪{}) →(Q ∪{ha, hr}) × ( ∪{}) × {R, L, S}
For a state p ∈Q, a state q ∈Q ∪{ha, hr}, two symbols X, Y ∈ ∪{}, and
a “direction” D ∈{R, L, S}, we interpret the formula
δ(p, X) = (q, Y, D)
to mean that when T is in state p and the symbol on the current tape square is
X, the TM replaces X by Y on that square, changes to state q, and either moves
the tape head one square right, moves it one square left if the tape head is not
already on square 0, or leaves it stationary, depending on whether D is R, L, or
S, respectively. If the state q is either ha or hr, we say that this move causes T to
halt. Once it halts, it cannot move, since δ is deﬁned at (q, Y) only if q ∈Q.
In this chapter we will return to drawing transition diagrams, similar to but
more complicated than the diagrams for ﬁnite automata. The transition
δ(p, X) = (q, Y, D)
will be represented by the diagram
X/Y, D
p
q
If the TM attempts to execute the move δ(p, X) = (q, Y, L) when the tape
head is on square 0, we will say that it halts in the state hr, leaving the tape head

228
C H A P T E R 7
Turing Machines
in square 0 and leaving the symbol X unchanged. This is a way for the TM to halt
that is not reﬂected by the transition function.
Normally a TM begins with an input string starting in square 1 of its tape and
all other squares blank. We don’t insist on this, because in Section 7.4 we will
talk about a TM picking up where another one left off, so that the original tape
may already have been modiﬁed. We do always assume, however, that the set of
nonblank squares on the tape when a TM starts is ﬁnite, so that there must be only
a ﬁnite number of nonblank squares at any stage of its operation. We can describe
the current conﬁguration of a TM by a single string
xqy
where q is the current state, x is the string of symbols to the left of the current
square, y either is null or starts in the current square, and everything after xy on the
tape is blank. The string x may be null, and if not, the symbols in x may include
blanks as well as nonblanks. If the string y ends in a nonblank symbol, the strings
xqy, xqy, xqy, . . . all refer to the same conﬁguration; normally in this case
we will write the string with no blanks at the end. If all the symbols on and to the
right of the current square are blanks, we will normally write the conﬁguration as
xq
so as to identify the current symbol explicitly.
It is also useful sometimes to describe the tape (including the tape head position
as well as the tape contents) without mentioning the current state. We will do this
using the notation
xσy
where σ is the symbol in the current square, or
xy
where the string y, which may contain more than one symbol, begins in the current
square. In either case, x is the string to the left of the current square and all symbols
to the right of the string y are blank. When y = , instead of writing xy we will
usually write x.
We trace a sequence of TM moves by specifying the conﬁguration at each
step. For example, if q is a nonhalting state and r is any state, we write
xqy ⊢T zrw
or
xqy ⊢∗
T zrw
to mean that the TM T moves from the ﬁrst conﬁguration to the second in one
move or in zero or more moves, respectively. For example, if q is a nonhalting
state and the current conﬁguration of T is given by
aabqaa
and δ(q, a) = (r, , L), we could write
aabqaa ⊢T aarba
The notations ⊢T and ⊢∗
T are usually shortened to ⊢and ⊢∗if there is no ambiguity.

7.2
Turing Machines as Language Acceptors
229
Finally, if T has input alphabet  and x ∈∗, the initial conﬁguration corre-
sponding to input x is given by
q0x
In this conﬁguration T is in the initial state, the tape head is in square 0, that
square is blank, and x begins in square 1. A TM starting in this conﬁguration
most often begins by moving its tape head one square right to begin reading the
input.
It is widely believed that Turing was successful in formulating a completely
general model of computation—roughly speaking, that any algorithmic procedure
can be programmed on a TM. A statement referred to as Turing’s thesis, or the
Church-Turing thesis, makes this claim explicitly. Because at this stage we have not
even said how a TM accepts a string or computes a function, we will discuss the
Church-Turing thesis a little later, in Section 7.6. Before then, we will also consider
a number of examples of TMs, which may make the thesis seem reasonable when
it is ofﬁcially presented.
7.2 TURING MACHINES AS LANGUAGE
ACCEPTORS
Deﬁnition 7.2
Acceptance by a TM
If T = (Q, , , q0, δ) is a TM and x ∈∗, x is accepted by T if
q0x ⊢∗
T whay
for some strings w, y ∈( ∪{})∗(i.e., if, starting in the initial con-
ﬁguration corresponding to input x, T eventually halts in the accepting
state).
A language L ⊆∗is accepted by T if L = L(T ), where
L(T ) = {x ∈∗| x is accepted by T }
Saying that L is accepted by T means that for any x ∈∗, x ∈L if and only
if x is accepted by T . This does not imply that if x /∈L, then x is rejected by T .
The two possibilities for a string x not in L(T ) are that T rejects x and that T
never halts, or loops forever, on input x.
EXAMPLE 7.3
A TM Accepting a Regular Language
Figure 7.3a shows the transition diagram of a ﬁnite automaton accepting L = {a, b}∗{ab}
{a, b}∗∪{a, b}∗{ba}, the language of all strings in {a, b}∗that either contain the substring
ab or end with ba.
Using a TM instead of an FA to accept this language might seem like overkill, but
the transition diagram in Figure 7.4b illustrates the fact that every regular language can

230
C H A P T E R 7
Turing Machines
a
b
b
a
b
b
a
a, b
a
a/a, R
a/a, R
b/b, R
q
r
p
t
b/b, R
a/a, R
b/b, R
b/b, R
Δ /Δ, R
a/a, R
ha
Δ/Δ, R
Δ /Δ, R
a/a, R 
b/b, R
q0
s
(a)
(b)
Figure 7.4
A ﬁnite automaton, and a Turing machine accepting the same language.
be accepted by a Turing machine, and that every FA can easily be converted into a TM
accepting the same language.
Several comments about the TM are in order. First, it can process every input string
in the way a ﬁnite automaton is forced to, moving the tape head to the right on each move
and never modifying the symbols on the tape. This procedure will not work for languages
that are not regular.
Second, the diagram in Figure 7.4b does not show moves to the reject state. There is
one from each of the states p, q, and s (the nonhalting states that correspond to nonaccepting
states in the FA), and they all look like this:
Δ /Δ, R
hr
In each of these states, if the TM discovers by encountering a blank that it has reached
the end of the input string, it can reject the input. From now on, we will usually omit these
transitions in a diagram, so that very likely you will not see hr at all; it should be understood
that if for some combination of state and tape symbol, no move is shown explicitly, then the
move is to hr. What happens to the tape head and to the current symbol on the move is usually
unimportant, because the computation is over, but we will say that both stay unchanged.
Finally, the general algorithm illustrated by Figure 7.4b for converting an FA to a TM
requires the TM to read all the symbols of the input string, since there are no moves to
either halt state until a blank is encountered. In this example, a string could be accepted as
soon as an occurrence of ab is found, without reading the rest of the input. If we preferred
this approach, we could eliminate the state r altogether and send the b-transitions from both
q and t directly to ha.
EXAMPLE 7.5
A TM Accepting XX = {xx | x ∈{a, b}∗}
This example will demonstrate a little more of the computing power of Turing machines.
Let
L = {xx | x ∈{a, b}∗}

7.2
Turing Machines as Language Acceptors
231
and let us start by thinking about what an algorithm for testing input strings might look like.
For a string to be in L means ﬁrst that its length is even. If we were thinking about ﬁnite
automata, we could easily test this condition by using two states e (even) and o (odd), and
transitions back and forth:
b/b, R
a/a, R
b/b, R
a/a, R
e
o
This allows us to reject odd-length strings quickly but is ultimately not very helpful,
because if the length turns out to be even, we still need to ﬁnd the middle of the
string.
A human computer might do this by calculating the length of the string, dividing
that number by 2, and counting again to ﬁnd the spot that far in from the beginning.
A Turing machine could do this too, but it would be laborious. Furthermore, in this
problem arithmetic seems extraneous. A more alert human computer might position
his ﬁngers at both ends of the string and proceed to move both ﬁngers toward the
middle simultaneously, one symbol at a time. This is the approach we will take, and
this type of computation is common for a TM. Of course, a device with only one
tape head also has to do this rather laboriously, but this just reﬂects the fact that
when Turing conceived of these machines, he was not primarily concerned with their
efﬁciency.
As the TM works its way in from the ends, continuing to move its tape head back and
forth from one side to the other, it will replace input symbols by the corresponding uppercase
letters to keep track of its progress. Once it arrives at the middle of the string—and if it
realizes at this point that the length is odd, it can reject—it changes the symbols in the ﬁrst
half back to their original lowercase form.
The second half of the processing starts at the beginning again and, for each lowercase
symbol in the ﬁrst half of the string, compares the lowercase symbol to the corresponding
uppercase symbol in the second half. Here again we must keep track of our progress, and
we do this by changing lowercase symbols to uppercase and simultaneously erasing the
matching uppercase symbols.
The resulting TM is shown in Figure 7.6. For an even-length input string, the
ﬁrst phase of the computation is represented by the loop at the top of the diagram
that includes q1, q2, q3, and q4. At the end of this loop, the string has been converted
to uppercase and the current square contains the symbol that begins the second half of
the string. (An odd-length string is discovered in state q3 and rejected at that point.)
The transition from q1 to q5 starts moving the tape head to the left, and by the time
the state q6 is reached, the ﬁrst half of the string is again lowercase and the tape
head is back at the beginning of the string. In the comparison of the ﬁrst and second
halves, an a in the ﬁrst half that is not matched by the corresponding symbol in the
second half is discovered in state q8, and an unmatched b is discovered in q7. If the
comparison is successful, a blank is encountered in q6 and the string is accepted.

232
C H A P T E R 7
Turing Machines
b/b, L
q0
a/a, R
q0 Δ /Δ, R
b/b, R
A /A, R
B/B, R
a /a, L
a /A, L
b/B, L
B/B, L
A /A, L
Δ /Δ, L
a /A, R
b/B, R
q5
q6
q8
q9
q7
q1
q2
q3
q4
ha
B/B, L
A /A, L
B/b, L
A/a, L
Δ /Δ, R
b/B, R
Β/Δ, L
Α/Δ, L
a/a, R
b/b, R
Δ /Δ, R
a/A, R
Δ /Δ, S
Δ /Δ, S
B/B, R
A /A, R
a/a, R
b/b, R
Δ /Δ, R
a/a, L
b/b, L
Δ /Δ, L
Figure 7.5
A Turing machine accepting {xx | x ∈{a, b}∗}.
We trace the moves of the TM for three input strings: two that illustrate the two possible
ways a string can be rejected, and one that is in L.
q0aba
⊢q1aba
⊢Aq2ba
⊢∗Abaq2
⊢Abq3a
⊢Aq4bA
⊢q4AbA
⊢Aq1bA
⊢ABq2A
⊢Aq3BA
⊢AhrBA
(reject)
q0ab
⊢q1ab
⊢Aq2b
⊢Abq2
⊢Aq3b
⊢q4AB
⊢Aq1B
⊢q5AB
⊢q5aB
⊢q6aB
⊢Aq8B
⊢AhrB
(reject)
q0aa
⊢q1aa
⊢Aq2a
⊢Aaq2
⊢Aq3a
⊢q4AA
⊢Aq1A
⊢q5AA
⊢q5aA
⊢q6aA
⊢Aq8A
⊢q9A
⊢Aq6
⊢Aha
(accept)

7.2
Turing Machines as Language Acceptors
233
EXAMPLE 7.7
Accepting L = {aibaj | 0 ≤i < j }
If we were testing a string by hand for membership in L, there is at least one simple algo-
rithm we could use that would be very easy to carry out on a Turing machine. The TM
would be similar to the ﬁrst phase of the one in Example 7.5, but even simpler: it could work
its way in from the ends of the string to the b in the middle, in each iteration erasing an a
in both portions. (In the previous example, erasing wasn’t an option during the locating-the-
middle phase, because we needed to remember what the symbols were.) When either or both
portions had no a’s left, it would be easy to determine whether the string should be accepted.
The TM we present will not be this one and will not be quite as simple; we’ll say why
in a minute. Ours will begin by making a preliminary pass through the entire string to make
sure that it has the form a∗baa∗. If it doesn’t, it will be rejected; if it does, this TM also
carries out a sequence of iterations in which an a preceding the b and another one after the
b are erased, but the a it erases in each portion will be the leftmost a.
The TM T that follows this approach is shown in Figure 7.8. The top part, involving
states q0 through q4, is to handle the preliminary test described in the previous paragraph.
The subsequent passes begin in state q5. Because of the preliminary check, if the ﬁrst
symbol we see on some pass is b, and if we then ﬁnd an a somewhere to the right, perhaps
after moving through blanks where a’s have already been erased, we can simply accept.
Otherwise, if the symbol we see ﬁrst is a, we erase it, move the tape head to the right
through the remaining a’s until we hit the b, continue moving to the right through blanks
until we hit another a, erase it, move the tape head back past any blanks and past the b,
then past any remaining a’s until we hit a blank; at that point we reverse course and prepare
for another iteration. Each subsequent pass also starts with either the b (because we have
erased all the a’s that preceded it) or the leftmost unerased a preceding the b.
Here is a trace of the computation for the input string abaa:
q0abaa
⊢q1abaa
⊢aq1baa
⊢abq2aa
⊢abaq3a
⊢abaaq3
⊢abaq4a
⊢∗q4abaa
⊢q5abaa
⊢q6baa
⊢bq7aa
⊢q8ba
⊢q9ba
⊢q5ba
⊢bq10a
⊢bq10a
⊢bhaa
(accept)
Δ /Δ, R
b/b, R
a/a, R
q0
a/a, R
a/a, R
q1
q2
q3
Δ /Δ, L
a/a, L
b/b, L
q4
a/Δ, R
Δ/Δ, R
b/b, R
Δ /Δ, R
Δ /Δ, R
Δ /Δ, R
q5
q10
q6
a/a, S
ha
q7
a/Δ, L
Δ /Δ, L
q8
b/b, L
a/a, L
q9
a/a, R
b/b, R
Figure 7.8
A Turing machine accepting {aibaj | 0 ≤i < j}.

234
C H A P T E R 7
Turing Machines
It is not hard to see that every string in L will be accepted by T . Every string that
doesn’t match the regular expression a∗baa∗will be rejected. The remaining strings are
those of the form aibaj, where 0 < j ≤i. Let’s trace T on the simplest such string,
aba.
q0aba
⊢q1aba
⊢aq1ba
⊢abq2a
⊢abaq3
⊢abq4a
⊢∗q4aba
⊢q5aba
⊢q6ba
⊢bq7a
⊢q8b
⊢q9b
⊢q5b
⊢bq10
⊢bq10
⊢bq10
⊢. . .
The TM is in an inﬁnite loop—looking for an a that would allow it to accept, with no way
to determine that it will never ﬁnd one.
Obviously, given a choice, you’d pick the strategy we described at the beginning of the
example, not this one. But the point is that this TM works! Accepting a language requires
only that we accept every string in the language and not accept any string that isn’t in the
language.
This is perhaps at least a plausible example of a language L and an attempt to accept
it that results in a TM with inﬁnite loops for some input strings not in the language. In
this case, the TM can be replaced by another one that accepts the same language without
any danger of inﬁnite loops. A question that is harder to answer is whether this is always
possible. We will return to this issue brieﬂy in the next section and discuss it in considerably
more detail in Chapter 8.
7.3 TURING MACHINES THAT COMPUTE
PARTIAL FUNCTIONS
A computer program whose purpose is to produce an output string for every legal
input string can be interpreted as computing a function from one set of strings
to another. A Turing machine T with input alphabet  that does the same thing
computes a function whose domain D is a subset of ∗. Things will be a little
simpler if we say instead that T computes a partial function on ∗with domain D.
For every input string x in the domain of the function, T carries out a computation
that ends with the output string f (x) on the tape. The deﬁnition below considers a
partial function on (∗)k, a function of k string variables; when k > 1, the k strings
all appear initially on the tape, separated by blanks. The value of the function is
always assumed to be a single string, and most of the time we will consider only
k = 1.
The most signiﬁcant issue for a TM T computing a function f is what output
strings it produces for input strings in the domain of f . However, what T does
for other input strings is not completely insigniﬁcant, because we want to say
that T computes the partial function f , not some other partial function with a
larger domain. For this reason, we say that T should end up in the accepting state
for inputs in the domain of f and not for any other inputs. In other words, if
T computes a partial function with domain D, then in particular, T accepts D,
irrespective of the output it produces.

7.3
Turing Machines That Compute Partial Functions
235
Deﬁnition 7.9
A Turing Machine Computing a Function
Let T = (Q, , , q0, δ) be a Turing machine, k a natural number, and
f a partial function on (∗)k with values in ∗. We say that T computes
f if for every (x1, x2, . . . , xk) in the domain of f ,
q0x1x2 . . . xk ⊢∗
T haf (x1, x2, . . . , xk)
and no other input that is a k-tuple of strings is accepted by T .
A partial function f : (∗)k →∗is Turing-computable, or simply
computable, if there is a TM that computes f .
Ideally, as we have said, a TM should compute at most one function. This is
still not quite the case, partly because of technicalities involving functions that have
different codomains and are otherwise identical, but also because if T computes a
partial function f on (∗)2, then T can also be said to compute the partial function
f1 on ∗deﬁned by the formula f1(x) = f (x, ). We can at least say that once
we specify a natural number k and a set C ⊆∗for the codomain, a TM can’t
compute more than one partial function of k variables having codomain C.
We are free to talk about a TM computing a partial function whose domain
and codomain are sets of numbers, once we adopt a way to represent numbers
by strings. For our purposes it will be sufﬁcient to consider partial functions on
N k with values in N, and we will use unary notation to represent numbers: the
natural number n is represented by the string 1n = 11 . . . 1. The ofﬁcial deﬁnition
is essentially the same as Deﬁnition 7.9, except that the input alphabet is {1}, the
initial conﬁguration looks like
q01n11n2 . . . 1nk
and the ﬁnal conﬁguration that results from an input in the domain of the partial
function f is
ha1f (n1,n2,...,nk)
EXAMPLE 7.10
The Reverse of a String
Figure 7.11 shows a transition diagram for a TM computing the reverse function r : {a, b}∗→
{a, b}∗. The TM moves from the ends of the string to the middle, making a sequence of
swaps between symbols σ1 in the ﬁrst half and σ2 in the second, and using uppercase letters
to record progress. Each iteration starts in state q1 with the TM looking at the symbol σ1 in
the ﬁrst half. The tape head moves to the position of σ2 in the second half, remembering σ1
by using states q2 and q3 if it is a and q4 and q5 if it is b. When the tape head arrives at σ2
(the rightmost lowercase symbol), the TM deposits the (uppercase) symbol in that position,
but similarly follows different paths back to q1, depending on whether σ2 is a or b. The
last iteration is cut short in the case of an odd-length string: When the destination position
on the right is reached, there is already an uppercase symbol there. The last phase of the
computation is to move to the right end of the string and make a pass toward the beginning
of the tape, converting all the uppercase letters to their original form.

236
C H A P T E R 7
Turing Machines
q0
q1
Δ /Δ, R
Δ /Δ, L
Δ /Δ, L
A/A, L
B/B, L
Δ /Δ, L
A/A, L
B/B, L
a /A, L
a /B, L
b/B, L
b/A, L
Δ /Δ, L
Δ /Δ, S
a /A, R
A/A, R
B/B, R
A/A, R
B/A, R
A/A, R
B/A, R
A/A, R
B/A, R
A/A, R
B/B, R
A/B, R
B/B, R
a/a, R
b/b, R
a/a, R
b/b, R
a/a, L
b/b, L
b/B, R
q2
ha
q9
q4
q5
q8
q3
q6
q7
A/a, L
B/b, L
a/a, L
b/b, L
Figure 7.11
A Turing machine computing the reverse function.
We show the moves of the TM for the even-length string baba.
q0baba
⊢q1baba
⊢Bq4aba
⊢∗Babaq4
⊢Babq5a
⊢Baq6bB
⊢∗q6BabB
⊢Aq1abB
⊢AAq2bB
⊢AAbq2B
⊢AAq3bB
⊢Aq7AAB
⊢ABq1AB
⊢ABAq8B
⊢ABABq8
⊢ABAq9B
⊢∗q9abab
⊢haabab
EXAMPLE 7.12
The Quotient and Remainder Mod 2
The two transition diagrams in Figure 7.13 show TMs that compute the quotient and remain-
der, respectively, when a natural number is divided by 2. The one in Figure 7.13a is another
application of ﬁnding the middle of the string; This time, 1’s in the ﬁrst half are temporarily
replaced by x’s and 1’s in the second half are erased. As usual, exactly what happens in
the last iteration depends on whether the input n is even or odd. Here are traces illustrating
both cases.
q01111
⊢q11111
⊢xq2111
⊢x1q311
⊢∗x111q3
⊢x11q41
⊢x1q51
⊢xq511
⊢q5x11
⊢xq111
⊢xxq21
⊢xx1q3
⊢xxq41
⊢xq5x
⊢xxq1
⊢xq7x
⊢∗q711
⊢ha11
q0111
⊢q1111
⊢xq211
⊢x1q31
⊢x11q3
⊢x1q41
⊢xq51
⊢q5x1
⊢xq11
⊢xxq2
⊢xq6x
⊢q7x
⊢q71
⊢ha1

7.3
Turing Machines That Compute Partial Functions
237
Δ /Δ, R
Δ/Δ, L
1/1, R
x/x, R
1/x, R
1/Δ, L
1/1, L
1/1, R
q0
q1
x/1, L
q5
q4
q3
Δ/Δ, S
x/Δ, L
Δ /Δ, L
Δ /Δ, L
ha
q7
q6
q2
(a)
Δ /Δ, R
Δ/Δ, R
Δ /Δ, S
Δ /Δ, L
1/Δ, L
Δ /1, L
1/Δ, L
q0
ha
(b)
1/1, R
Figure 7.13
Computing the quotient and remainder mod 2.
The TM in Figure 7.13b moves the tape head to the end of the string, then makes a pass
from right to left in which the 1’s are counted and simultaneously erased. The ﬁnal output
is a single 1 if the input was odd and nothing otherwise.
EXAMPLE 7.14
The Characteristic Function of a Set
For a language L ⊆∗, the characteristic function of L is the function χL : ∗→{0, 1}
deﬁned by
χL(x) =

1
if x ∈L
0
if x /∈L
Here we think of 0 and 1 as alphabet symbols, rather than numbers, so that when we use a
TM to compute χL, unary notation is not involved.
Computing the function χL and accepting the language L are two approaches to the
question of whether an arbitrary string is in L, and Turing machines that carry out these
computations are similar in some respects. A TM computing χL indicates whether the input
string is in L by producing output 1 or output 0, and one accepting L indicates the same thing
by accepting or not accepting the input. If accepting a language L requires a complicated
algorithm, then computing χL will require an algorithm that is at least as complicated. We

238
C H A P T E R 7
Turing Machines
Δ /Δ, R
Δ/Δ, L
Δ/Δ, L
Δ/Δ, R
Δ/Δ, R
a/a, R
b/b, R
a/a, R
b/b, R
b/b, R
a/a, R
Δ/Δ, L
q0
q
a/a, R
r
a/a, R
b/b, R
a/Δ, L
b/Δ, L
a/Δ, L
b/Δ, L
Δ/1, L
t
Δ/0, L
ha
Δ/Δ, L
Δ/Δ, L
p
s
Figure 7.15
Computing χL, where L = {a, b}∗({ab}{a, b}∗∪{ba}).
can be a little more speciﬁc at this stage about the relationship between these two approaches,
although in Chapter 8 we will consider the question more thoroughly.
It is easy to see that if we have a TM T computing χL, then we can construct another
TM T1 accepting L by modifying T in one simple way: each move of T to the accepting
state is replaced by a sequence of moves that checks what output is on the tape, accepts if
it is 1, and rejects if it is 0.
If we want a TM T1 computing χL, however, it will have to accept every string, because
χL(x) is deﬁned for every x ∈∗. This means that if we are trying to obtain T1 from a TM
T that accepts L, it is important at the outset to know whether T halts for every input string.
If we know that it does, then modifying it so that the halting conﬁgurations are the correct
ones for computing χL can actually be done without too much trouble, as in the example
pictured in Figure 7.15. Otherwise, there is at least some serious doubt as to whether what
we want is even possible.
Figure 7.15 shows a transition diagram for a TM computing χL, where L is the reg-
ular language L = {a, b}∗{ab}{a, b}∗∪{a, b}∗{ba} discussed in Example 7.3. A transition
diagram for a TM accepting L is shown in Figure 7.4b.
7.4 COMBINING TURING MACHINES
A Turing machine represents an algorithm. Just as a typical large algorithm can be
described as a number of subalgorithms working in combination, we can combine
several Turing machines into a larger composite TM.
In the simplest case, if T1 and T2 are TMs, we can consider the composition
T1T2
whose computation can be described approximately by saying “ﬁrst execute T1,
then execute T2.” In order to make sense of this, we think of the three TMs T1,

7.4
Combining Turing Machines
239
T2, and T1T2 as sharing a common tape and tape head, and the tape contents
and tape head position when T2 starts are assumed to be the same as when T1
stops.
If T = (Q, , , q0, δ) is the composite TM T1T2, we can obtain the state
set Q (of nonhalting states) of T by taking the union of the state sets of T1
and T2, except that the states of T2 are relabeled if necessary so that the two
sets don’t overlap. The initial state of T is the initial state of T1. The transi-
tions of T include all those of T2, and all those of T1 that don’t go to ha. A
transition
X/Y, D
p
ha
in T1 becomes
X/Y, D
p
q2
in T, where q2 is the initial state of T2. In other words, T begins in the initial state
of T1 and executes the moves of T1 up to the point where T1 would halt. If T1 halts
in the reject state, then so does T , but if T1 halts in ha, then at that point T2 takes
over, in its initial state. The moves that cause T to accept are precisely those that
cause T2 to accept.
We can usually avoid being too explicit about how the input alphabets and
tape alphabets of T , T1, and T2 are related. It may be, for example, that T1’s
job is to create an input conﬁguration different from the one T2 would nor-
mally have started with; or it may be that T2 does not expect to be process-
ing input at all, but only making some modiﬁcation to the tape conﬁguration
in preparation for some other TM. It should be the case, at least, that any of
T1’s tape symbols that T2 is likely to encounter should be in T2’s input
alphabet.
It may now be a little clearer why in certain types of computations we are
careful to specify the ﬁnal tape conﬁguration. If T1 is to be followed by another
TM T2, T1 must ﬁnish up so that the tape looks like what T2 expects when it starts.
For example, if T1 and T2 compute the functions f and g, respectively, from N
to N, the output conﬁguration left by T1 is a legitimate input conﬁguration for T2,
so that the output from T1T2 resulting from input n is g(f (n)). In other words,
T1T2 computes the composite function g ◦f , deﬁned by the formula g ◦f (n) =
g(f (n)).
When we use a TM T as a component of a larger machine, then in order to
guarantee that it accomplishes what we want it to, we must be sure that the tape has
been prepared properly before T starts. For example, if T is designed to process an
input string starting in the normal input conﬁguration, then at the beginning of its
operation the tape should look like xy for some string x and some input string y.

240
C H A P T E R 7
Turing Machines
As long as T can process input y correctly starting with the conﬁguration q0y,
it will work correctly starting with xq0y, because it will never attempt to move
its tape head to the left of the blank and will therefore never see any symbols in
x. If the tape is not blank to the right of y, we can’t be sure T will complete its
processing properly.
In order to use the composite TM T1T2 in a larger context, and to draw tran-
sition diagrams without having to show the states of T1 and T2 explicitly, we can
write
T1 →T2
It is often helpful to use a mixed notation in which some but not all of the states
of a TM are shown. For example, we might use any of these notations
a
p
T
a/a, S
p
T
a/a, S
p
T
to mean “in state p, if the current symbol is a, then execute the TM T .” The
third notation indicates most explicitly that this can be viewed as a composite
TM formed by combining T with another very simple one. Similarly, the three
notations
a
T2
T1
a
T2
T1
a/a, S
T2
T1
stand for “execute T1, and if T1 halts in ha with current symbol a, then exe-
cute T2.” In all three, it is understood that if T1 halts in ha but the current
symbol is one other than a for which no transition is indicated, then the TM
rejects.
The TM pictured in Figure 7.16 should be interpreted as follows. If the current
tape symbol is a, execute the TM T ; if it is b, halt in the accepting state; and if it
is anything else, reject. In the ﬁrst case, if T halts in ha, then as long as the current
symbol is a, continue to execute T ; if at some point T halts in ha with current
symbol b, halt and accept. The TM might reject during one of the iterations of T ,
a
b/b, S
ha
q0
T
Figure 7.16

7.4
Combining Turing Machines
241
and it might loop forever, either because one of the iterations of T does or because
T accepts with current symbol a every time.
There are too many potentially useful TM components to try to list them. It
might not be worth trying to isolate the ﬁnd-the-middle-of-the-string operation that
we have used several times, because we almost always use some variation of it
that not only ﬁnds the middle of the string but simultaneously does something
else useful to one or both halves. Here are a few components that will come up
repeatedly.
EXAMPLE 7.17
Finding the Next Blank or the Previous Blank
We will use NB to denote the Turing machine that moves the tape head to the next blank
(the ﬁrst blank square to the right of the current head position), and PB for the TM that
moves to the previous blank (the rightmost blank square to the left of the current position).
Neither of these is useful by itself—in particular, PB rejects if it begins with no blanks to
the left of the current square—but both can often be used in composite TMs.
EXAMPLE 7.18
Copying a String
A Copy TM starts with tape x, where x is a string of nonblank symbols, and ends up with
xx. Figure 7.19 shows a transition diagram for such a TM if the alphabet is {a, b}, and all
that is needed in a more general situation is an “uppercase” version of each alphabet symbol.
b/b, R
q0
Δ /Δ, R
h
Δ /Δ, S
Δ /Δ, L
Δ /Δ, R
a/a, R
Δ /a, L
Δ /Δ, R
ha
Δ /Δ, L
B/b, L
A/a, L
B/B, R
A/A, R
a/A, R
b/B, R
b/b, R
a/a, R
b/b, L
a/a, L
b/b, L
a/a, L
b/b, R
a/a, R
b/b, R
a/a, R
Δ /b, L
Figure 7.19
A Turing machine to copy strings.

242
C H A P T E R 7
Turing Machines
EXAMPLE 7.20
Inserting and Deleting a Symbol
Deleting the current symbol means transforming the tape from xσy to xy, where σ is any
symbol, including , and y is a string of nonblank symbols. Inserting the symbol σ means
starting with xy and ending up with xσy, where again y contains no blanks. A Delete TM
for the alphabet {a, b} is shown in Figure 7.22. The states labeled qa and qb allow the TM
to remember a symbol between the time it is erased and the time it is copied in the next
square to the left.
Inserting the symbol σ is done virtually the same way, except that the single pass goes
from left to right, symbols are moved to the right instead of to the left, and the move that
starts things off writes σ instead of .
EXAMPLE 7.21
Erasing the Tape
There is no general “erase-the-tape” Turing machine! In particular, there is no TM E that
is capable of starting in an arbitrary conﬁguration, erasing all the nonblank symbols on or
to the right of the current square, and halting in the square it started in. Doing this would
require that it be able to ﬁnd the rightmost nonblank symbol, which is not possible without
special assumptions.
However, if T starts in the standard input conﬁguration, we can effectively have it erase
the portion of its tape to the right of its tape head after its operation. The trick is to have
T perform its computation with a few modiﬁcations that will make the erasing feasible. T
starts by placing an end-of-tape marker $ in the square following the input string; during its
computation, every time it moves its tape head as far right as the $, it moves the marker one
square further, to record the fact that one more square has been used in the computation.
This complicates the transition diagram for T , but in a straightforward way. Except for the
states involved in the preliminary moves, every state p is replaced by the transition shown
in Figure 7.23.
q0
b/a, L
q0
Δ /Δ, R
ha
b/Δ, R
b/b, R
a/b, L
a/Δ, R
a/a, R
Δ /Δ, L
qΔ
a/Δ, L
b/Δ, L
Δ /a, S
Δ /b, S
a/a, L
b/b, L
Δ /Δ, S
qa
qb
Figure 7.22
A Turing machine to delete a symbol.

7.5
Multitape Turing Machines
243
$/Δ, R
Δ / $, L
p
p'
Figure 7.23
Any additional transitions to or from p are unchanged. Once T has ﬁnished its computation,
it erases the tape as follows. It marks the square beyond which the tape is to be blank;
moves to the right until it hits the $, and erases it; then moves left back to the marked
square, erasing each symbol as it goes.
The conclusion is that although there is no such thing as a general erase-the-tape TM,
we can pretend that there is. We can include an erasing TM E in a larger composite machine,
and we will interpret it to mean that the components that have executed previously have
prepared the tape so that the end-of-tape marker is in place, and E will be able to use it and
erase it.
EXAMPLE 7.24
Comparing Two Strings
It is useful to be able to start with the tape xy and determine whether x = y. A compar-
ison operation like this can be adapted and repeated in order to ﬁnd out whether x occurs as
a substring of y. In the next example we use a TM Equal, which begins with xy (where
x, y ∈{a, b}∗), accepts if they are equal, and rejects otherwise.
The last example in this section illustrates how a few of these operations can
be used together.
EXAMPLE 7.25
Accepting the Language of Palindromes
In the TM shown below, NB and PB are the ones in Example 7.17, Copy is the TM in
Example 7.18, R is the TM in Example 7.10 computing the reverse function, and Equal is
the TM mentioned in the previous example.
Copy →NB →R →PB →Equal
The Turing machine accepts the language of palindromes over {a, b} by comparing the input
string to its reverse and accepting if and only if the two are equal.
7.5 MULTITAPE TURING MACHINES
Algorithms in which several different kinds of data are involved can be particularly
unwieldy to implement on a Turing machine, because of the bookkeeping necessary
to store and access the data. One feature that can simplify things is to have several
different tapes, with independent tape heads, and to be able to move all the tape
heads simultaneously in a single TM move.
This means that we are considering a different model of computation, a multi-
tape Turing machine. Just as we showed in Chapter 3 that allowing nondeterminism

244
C H A P T E R 7
Turing Machines
and -transitions did not increase the computing power of ﬁnite automata, we wish
to show now that allowing a Turing machine to have more than one tape does not
make it any more powerful than an ordinary TM.
If we are comparing an ordinary TM and a multitape TM with regard to
power (as opposed to convenience or efﬁciency), the question is whether the two
machines can solve the same problems and get the same answers. A TM of any
type produces an answer, ﬁrst by accepting or rejecting a string, and second by
producing an output string. What we must show, then, is that for every multitape
TM T , there is a single-tape TM that accepts exactly the same strings as T , rejects
the same strings as T , and produces exactly the same output as T for every input
string it accepts. To simplify the discussion we restrict ourselves to Turing machines
with two tapes, and it is easy to see that the principles are the same if there are
more than two.
A 2-tape TM can also be described by a 5-tuple T = (Q, , , q0, δ), where
this time
δ : Q × ( ∪{})2 →(Q ∪{ha, hr}) × ( ∪{})2 × {R,L,S}2
A single move can change the state, the symbols in the current squares on both
tapes, and the positions of the two tape heads. Let us describe a conﬁguration of a
2-tape TM by a 3-tuple
(q, x1a1y1, x2a2y2)
where q is the current state and xiaiyi represents the contents of tape i for each
value of i.
We will deﬁne the initial conﬁguration corresponding to input string x as
(q0, x, )
(that is, the input string is in the usual place on the ﬁrst tape and the second tape is
initially blank). Similarly, the output will appear on the ﬁrst tape, and the contents
of the second tape at the end of the computation will be irrelevant.
Theorem 7.26
For every 2-tape TM T = (Q, , , q0, δ), there is an ordinary 1-tape TM
T1 = (Q1, , 1, q1, δ1), with  ⊆1, such that
1. For every x ∈∗, T accepts x if and only if T1 accepts x, and T rejects x if
and only if T1 rejects x. (In particular, L(T ) = L(T1).)
2. For every x ∈∗, if
(q0, x, ) ⊢∗
T (ha, yaz, ubv)
for some strings y, z, u, v ∈( ∪{})∗and symbols a, b ∈ ∪{}, then
q1x ⊢∗
T1 yhaaz

7.5
Multitape Turing Machines
245
Proof
We describe how to construct an ordinary TM T1 that simulates the 2-tape
TM T and satisﬁes the properties in the statement of the theorem. There
are several possible ways an ordinary TM might simulate one with two
tapes; the way we will describe is for it to divide the tape into two parts,
as this diagram suggests.
$
1
2
1
2
1
In square 0 there is a marker to make it easy to locate the begin-
ning of the tape. The odd-numbered squares represent the ﬁrst tape and
the remaining even-numbered squares represent the second. To make the
operation easier to describe, we refer to the odd-numbered squares as
the ﬁrst track of the tape and the even-numbered squares beginning with
square 2 as the second track.
It will also be helpful to have a marker at the other end of the nonblank
portion of the tape, because if T accepts, then at the end of the simulation
T1 needs to delete all the symbols on the second track of the tape. Start-
ing in the initial conﬁguration q1x with input x = a1a2 . . . an, T1 places
the $ marker in square 0, inserts blanks between consecutive symbols of
x, and places the # marker after the last nonblank symbol, to produce
the tape
$a1a2 . . . an#
(so that the ﬁrst track of the tape has contents x and the second is blank).
From this point on, the # is moved if necessary to mark the farthest right
that T has moved on either of its tapes (see Example 7.21).
The only signiﬁcant complication in T1’s simulating the computation
of T is that it must be able to keep track of the locations of both tape
heads of T . We can handle this by including in T1’s tape alphabet an extra
copy σ ′ of every symbol σ (including ) that can appear on T ’s tape.
When T1 has ﬁnished simulating a move, if the tape heads of T are on
squares containing σ1 and σ2, respectively, then the two squares on T1’s
tape that represent those squares will have the symbols σ ′
1 and σ ′
2 instead.
At each step during the simulation, there will be one primed symbol on
each of the two tracks of the tape.
By using extra states, T1 can “remember” the current state of T ,
and it can also remember what the primed symbol on the ﬁrst track
is while it is looking for the primed symbol on the second track (or
vice-versa). The steps T1 makes in order to simulate a move of T are
these:

246
C H A P T E R 7
Turing Machines
1. Move the tape head left until the beginning-of-tape marker $ is encountered,
then right until a symbol of the form σ ′ is found on the ﬁrst track of the
tape. Remember σ and move back to the $.
2. Move right until a symbol τ ′ is found on the second track. If the move of T
that has now been determined is
δ(p, σ, τ) = (q, σ1, τ1, D1, D2)
then reject if q = hr, and otherwise change τ ′ to τ1 and move in direction
D2 to the appropriate square on the second track.
3. If in moving the tape head this way, the $ is encountered, reject, since T ’s
move would have attempted to move its tape head off the tape. Otherwise
(moving the # marker if necessary), change the symbol in the new square on
track 2 to the corresponding primed symbol, and move back to the $.
4. Locate σ ′ on the ﬁrst track again, change it to σ1, and move the tape head in
direction D1 to the appropriate square on the ﬁrst track.
5. After allowing for encountering either marker, as in step 3, change the new
symbol on the ﬁrst track to the corresponding primed symbol.
The three lines below illustrate one iteration in a simple case. (Vertical
lines are to make it easier to read.)
$

0
1′
1

0
0
0′
1
#

$

0
1′
1

0
0
1
1
′
#
$
′
0

1

0
0
1
1
′
#
Here we assume that σ and τ are 1 and 0, respectively, σ1 and τ1 are 
and 1, and D1 and D2 are L and R. The move being simulated is
δ(p, 1, 0) = (q, , 1, L, R)
The second line represents the situation after step 3: the # has been moved
one square to the right, the 0′ on the second track has been replaced by
1, and the symbol after this 1 on the second track is now ′. The third
line represents the situation after step 5.
As long as T has not halted, iterating these steps allows T1 to simulate
the moves of T correctly. If T ﬁnally accepts, then T1 must carry out these
additional steps in order to end up in the right conﬁguration.
6. Delete every square in the second track to the left of #.
7. Delete both end-of-tape markers, so that the remaining symbols begin in
square 0.
8. Move the tape head to the primed symbol, change it to the corresponding
unprimed symbol, and halt in ha with the tape head on that square.
Corollary 7.27 Every language that is accepted by a 2-tape TM can be accepted
by an ordinary 1-tape TM, and every function that is computed by a 2-tape TM
can be computed by an ordinary TM.
■

7.6
The Church-Turing Thesis
247
7.6 THE CHURCH-TURING THESIS
To say that the Turing machine is a general model of computation means that any
algorithmic procedure that can be carried out at all, by a human computer or a team
of humans or an electronic computer, can be carried out by a TM. This statement
was ﬁrst formulated by Alonzo Church in the 1930s and is usually referred to as
Church’s thesis, or the Church-Turing thesis. It is not a mathematically precise
statement that can be proved, because we do not have a precise deﬁnition of the
term algorithmic procedure. By now, however, there is enough evidence for the
thesis to have been generally accepted. Here is an informal summary of some of
the evidence.
1.
The nature of the model makes it seem likely that all the steps crucial to
human computation can be carried out by a TM. Humans normally work
with a two-dimensional sheet of paper, and a human computer may perhaps
be able to transfer his attention to a location that is not immediately adjacent
to the current one, but enhancements like these do not appear to change the
types of computation that are possible. A TM tape could be organized so as
to simulate two dimensions; one likely consequence would be that the TM
would require more moves to do what a human could do in one.
2.
Various enhancements of the TM model have been suggested in order to
make the operation more like that of a human computer, or more convenient,
or more efﬁcient. The multitape TM discussed brieﬂy in Section 7.5 is an
example. In each case, it has been shown that the computing power of the
device is unchanged.
3.
Other theoretical models of computation have been proposed. These include
abstract machines such as the ones mentioned in Section 7.1, with two stacks
or with a queue, as well as machines that are more like modern computers.
In addition, various notational systems (programming languages, grammars,
and other formal mathematical systems) have been suggested as ways of
describing or formulating computations. Again, in every case, the model has
been shown to be equivalent to the Turing machine.
4.
Since the introduction of the Turing machine, no one has suggested any type
of computation that ought to be included in the category of “algorithmic
procedure” and cannot be implemented on a TM.
So far in this chapter, we have considered several simple examples of Turing
machines. None of them is all that complex, although as we suggested in Sec-
tion 7.4, simple TMs can be combined to form complex ones. But you can already
see in Example 7.5 that in executing an algorithm, a TM depends on low-level
operations that can obviously be carried out in some form or other. What makes
an algorithm complex is not that the low-level details are more complicated, but
that there are more of them, and that they are combined in ways that involve
sophisticated logic or complex bookkeeping strategies. Increasing the complexity
of an algorithm poses challenges for someone trying to design a coherent and

248
C H A P T E R 7
Turing Machines
understandable implementation, but not challenges that require more computing
power than Turing machines provide.
We have said that the Church-Turing thesis is not mathematically precise
because the term “algorithmic procedure” is not mathematically precise. Once we
adopt the thesis, however, we are effectively giving a precise meaning to the term:
An algorithm is a procedure that can be carried out by a TM. Such a deﬁnition pro-
vides a starting point for a discussion of which problems have algorithmic solutions
and which don’t. This discussion begins in Chapter 8 and continues in Chapter 9.
Another way we will use the Church-Turing thesis in the rest of this book
is to describe algorithms in general terms, without giving all the details of Turing
machines that can execute them. Even in the discussion so far, on several occasions
we have sketched the operation of a TM without providing all the details; a full-
ﬂedged application of the Church-Turing thesis allows us to stop with a precise
description of an algorithm, without referring to a TM implementation at all.
7.7 NONDETERMINISTIC TURING
MACHINES
A nondeterministic Turing machine (NTM) T is deﬁned the same way as an ordi-
nary TM, except that for a state-symbol pair there might be more than one move.
This means that if T = (Q, , , q0, δ), and (q, a) ∈Q × ( ∪{}), then δ(q, a)
is a ﬁnite subset, not an element, of (Q ∪{ha, hr}) × ( ∪{}) × {R,L,S}.
We can use the same notation for conﬁgurations of an NTM as in Section 7.1;
if p is a nonhalting state, q is any state, a and b are tape symbols, and w, x, y, z
are strings,
wpax ⊢T yqbz
means that T has at least one move in the ﬁrst conﬁguration that leads to the
second, and
wpax ⊢∗
T yqbz
means that there is at least one sequence of zero or more moves that takes T from
the ﬁrst to the second. It is still correct to say that a string x is accepted by T if
q0x ⊢∗
T whay
for some strings w, y ∈( ∪{})∗.
The idea of an NTM that produces output will be useful, particularly one that
is a component in a larger machine. We will not consider the idea of computing
a function using a nondeterministic Turing machine, however, because a TM that
computes a function should produce no more than one output for a given input
string.
There are many types of languages for which nondeterminism makes it easy
to construct a TM accepting the language. We give two examples, and there are
others in the exercises.

7.7
Nondeterministic Turing Machines
249
EXAMPLE 7.28
The Set of Composite Natural Numbers
Let L be the subset of {1}∗of all strings whose length is a composite number (a nonprime
bigger than 2); since a prime is a natural number 2 or higher whose only positive divisors
are itself and 1, an element of L is a string of the form 1n, where n = p ∗q for some
integers p and q with p, q ≥2.
There are several possible strategies we could use to test an input x = 1n for member-
ship in L. One approach is to try combinations p ≥2 and q ≥2 to see if any combination
satisﬁes p ∗q = n (i.e., if 1p∗q = 1n).
Nondeterminism allows a Turing machine to guess, rather than trying all possible com-
binations. This means guessing values of p and q, multiplying them, and accepting if and
only if p ∗q = n. This sounds too simple to be correct, but it works. If n is nonprime,
then there are p ≥2 and q ≥2 with p ∗q = n, and there is a sequence of steps our NTM
could take that would cause the numbers generated nondeterministically to be precisely
p and q; in other words, there is a sequence of steps that would cause it to accept the
input. If x /∈L (which means that |x| is either prime or less than 2) then no matter what
moves the NTM makes to generate p ≥2 and q ≥2, p ∗q will not be n, and x will not
be accepted.
We can describe the operation of the NTM more precisely. The nondeterminism will
come from the component G2 shown in Figure 7.29, which does nothing but generate a
string of two or more 1’s on the tape.
The other components are the next-blank and previous-blank operations NB and PB
introduced in Example 7.17; a TM M that computes the multiplication function from N × N
to N; and the Equal TM described in Example 7.24. The complete NTM is shown below:
NB →G2 →NB →G2 →PB →M →PB →Equal
We illustrate one way this NTM might end up accepting the input string 115. The other
possible way is for G2 to generate 111 ﬁrst and then 11111.
111111111111111
(original tape)
111111111111111
(after NB)
11111111111111111111
(after G2)
11111111111111111111
(after NB)
11111111111111111111111
(after G2)
11111111111111111111111
(after PB)
111111111111111111111111111111
(after M)
111111111111111111111111111111
(after PB)
(accept)
(after Equal)
Δ /Δ, L
Δ /Δ,S
Δ/1, R
1/1, L
Δ /1, R
Δ /1, R
ha
Δ /Δ, R
Figure 7.29
An NTM to generate a string of two or more 1’s.

250
C H A P T E R 7
Turing Machines
EXAMPLE 7.30
The Language of Preﬁxes of Elements of L
In this example we start with a language L that we assume is accepted by some Turing
machine T , and we consider trying to ﬁnd a TM that accepts the language P(L) of preﬁxes
of elements of L. P(L) contains  as well as all the elements of L, and very likely
(depending on L) many other strings as well.
If the input alphabet of T is , a string x ∈∗is a preﬁx of an element of L if there is
a string y ∈∗such that xy ∈L. The set of candidates y we might have to test is inﬁnite.
(In the previous example there are easy ways to eliminate from consideration all but a ﬁnite
set of possible factorizations.) This doesn’t in itself rule out looking at the candidates one
at a time, because an algorithm to accept P(L) is allowed to loop forever on an input string
not in P(L).
There is another problem, however, that would also complicate things if we were
looking for a deterministic algorithm. The only way we have of testing for membership in
L is to use the Turing machine T , and T itself may loop forever on input strings not in L.
This means that in order to test an input string x for membership in P(L), simply executing
T on all the strings xy, one at a time, is not sufﬁcient: if T looped forever on xy1, and
there were another string y2 that we hadn’t tried yet for which xy2 ∈L, we would never
ﬁnd it.
This problem can also be resolved—see the proof of Theorem 8.9 for a similar argument.
But for now this is just another way in which nondeterminism makes it easier to describe a
solution. The NTM pictured below accepts P(L). It uses a component G, which is identical
to G2 in the previous example except for two features. First, it writes symbols in , not
just 1’s (so that it is nondeterministic in two ways—with respect to which symbol it writes
at each step, as well as when it stops); and second, it can generate strings of length 0 or 1
as well. Just as before, G is the only source of nondeterminism. The components NB and
PB are from Example 7.17, and Delete was discussed in Example 7.20.
NB →G →Delete →PB →T
If the input string x is in P(L), then there is a sequence of moves G can make that
will cause it to generate a string y for which xy ∈L. After the blank separating x and y is
deleted, and the tape head is moved to the blank in square 0, T is executed as if its input
were xy, and it will accept. On the other hand, if x /∈P(L), then no matter what string y
G generates, the string xy is not in L, and the NTM will not accept x because T will not
accept xy.
Both these examples illustrate how nondeterminism can make it easy to describe
an algorithm for accepting a language. But the algorithm being described is non-
deterministic. The main result in this section, Theorem 7.31, asserts that a nonde-
terministic algorithm is sufﬁcient, because once we have one, we can in prin-
ciple replace it by an ordinary deterministic one. Turing machines share this
feature with ﬁnite automata (where nondeterminism was helpful but not essential),
whereas in the case of pushdown automata the nondeterminism could often not be
eliminated.

7.7
Nondeterministic Turing Machines
251
Theorem 7.31
For every nondeterministic TM T = (Q, , , q0, δ), there is an ordinary
(deterministic) TM T1 = (Q1, , 1, q1, δ1) with L(T1) = L(T ).
Proof
The idea of the proof is simple: we will describe an algorithm that can
test, if necessary, every possible sequence of moves of T on an input
string x. If there is a sequence of moves that would cause T to accept
x, the algorithm will ﬁnd it. It’s only the details of the proof that are
complicated, and we will leave some of them out.
There is an upper limit on the number of choices T might have in
an arbitrary conﬁguration: the maximum possible size of the set δ(q, σ),
where q ∈Q and σ ∈ ∪{}. Let us make the simplifying assumption
that this upper bound is 2. Then we might as well assume that for every
combination of nonhalting state and tape symbol, there are exactly two
moves (which may be identical). We label these as move 0 and move 1,
and the order is arbitrary.
These assumptions allow us to represent a sequence of moves of T
on input x by a string of 0’s and 1’s. The string  represents the sequence
of no moves; assuming that the moves represented by the string s take us
to some nonhalting conﬁguration C, the moves represented by the string
s0 or the string s1 include the moves that take us to C, followed by move
0 or move 1, respectively, from that conﬁguration.
The order in which we test sequences of moves corresponds to canon-
ical order for the corresponding strings of 0’s and 1’s: We will test the
sequence of zero moves, then all sequences of one move, then all sequences
of two moves, and so forth. (This is likely to be a very inefﬁcient algo-
rithm, in the sense that testing a sequence of n + 1 moves requires us to
start by repeating a sequence of n moves that we tried earlier.)
The essential requirement of the algorithm is that if there is a sequence
of moves that allows T to accept x, the algorithm will ﬁnd it and accept;
and otherwise it will not accept. If x is never accepted and T can make
arbitrarily long sequences of moves on x, the algorithm will never ter-
minate. We will include a feature in the algorithm that will allow it to
terminate if for some n, no matter what choices T makes, it reaches the
reject state within n moves.
Our proof will not take full advantage of the Church-Turing thesis,
because it will sketch some of the techniques a TM might use to organize
the information it needs to carry out the algorithm. It will be a little easier
to visualize if we allow T1 to have four tapes:
1. The ﬁrst tape holds the original input string x, and its contents never change.
2. The second tape contains the binary string that represents the sequence of
moves we are currently testing.

252
C H A P T E R 7
Turing Machines
3. The third tape will act as T ’s working tape as it makes the sequence of
moves corresponding to the digits on tape 2.
4. We will describe the fourth tape’s contents shortly; this will be the
information that allows the algorithm to quit if it ﬁnds an n such that T
always rejects the input within n moves.
Testing all possible sequences of zero moves is easy. The string on
tape 2 is , and T1 doesn’t have to do anything.
Now let us assume that T1 has tested the sequence of moves corre-
sponding to the string on tape 2, and that those moves did not result in
x being accepted (if they did, then T1 can quit and accept). Here is a
summary of the steps T1 takes to carry out the next iteration.
1. T1 updates tape 2 so that it contains the next binary string in canonical order.
If the current string is not a string of 1’s, the next string will be the string, of
the same length as the current one, that follows it in alphabetical order; if the
current string is the string 1n, then the next string is 0n+1. There is one other
operation performed by T1 in this last case, which we will discuss shortly.
2. T1 places the marker $ in square 0 of tape 3, erases the rest of that tape,
places a blank in square 1, and copies the input string x from tape 1,
beginning in square 2. Tape 3 now looks like
$x
which if we ignore the $ is the same as the initial tape of T . The purpose of
the $ is to allow T1 to recover in case T would try to move its tape head off
the tape during its computation.
3. T1 then executes the moves of T corresponding to the binary digits on
tape 2. At each step, it decides what move to make by consulting the current
digit on tape 2 and the current symbol on tape 3. If at some point in this
process T accepts, then T1 accepts. If at some point T rejects, then T1 copies
all the binary digits on tape 2 onto tape 4, with a blank preceding them to
separate that string from the ones already on tape 4.
Now we can explain the role played by tape 4 in the algorithm. If
T1 tries the sequence of moves corresponding to the binary string 1n, the
last string of length n in canonical order, and that sequence or some part
of it causes T to reject, then T1 writes 1n on tape 4, and on the ﬁrst step
of the next iteration it examines tape 4 to determine whether all binary
sequences of length n appear. If they do, then T1 concludes that every
possible sequence of moves causes T to reject within n steps, and at that
point T1 can reject.
7.8 UNIVERSAL TURING MACHINES
The Turing machines we have studied so far have been special-purpose com-
puters, capable of executing a single algorithm. Just as a modern computer is

7.8
Universal Turing Machines
253
a stored-program computer, which can execute any program stored in its memory,
so a “universal” Turing machine, also anticipated by Alan Turing in a 1936 paper,
can execute any algorithm, provided it receives an input string that describes the
algorithm and any data it is to process.
Deﬁnition 7.32
Universal Turing Machines
A universal Turing machine is a Turing machine Tu that works as follows.
It is assumed to receive an input string of the form e(T )e(z), where T is
an arbitrary TM, z is a string over the input alphabet of T , and e is an
encoding function whose values are strings in {0, 1}∗. The computation
performed by Tu on this input string satisﬁes these two properties:
1. Tu accepts the string e(T )e(z) if and only if T accepts z.
2. If T accepts z and produces output y, then Tu produces output e(y).
In this section we will ﬁrst discuss a simple encoding function e, and then
sketch one approach to constructing a universal Turing machine.
The idea of an encoding function turns out to be useful in itself, and in Chapter
8 we will discuss some applications not directly related to universal TMs. The
crucial features of any encoding function e are these: First, it should be possible to
decide algorithmically, for an arbitrary string w ∈{0, 1}∗, whether w is a legitimate
value of e (i.e., the encoding of a TM or the encoding of a string); second, a string
w should represent at most one Turing machine, or at most one string; third, if w
is of the form e(T ) or e(z), there should be an algorithm for decoding w, to obtain
the Turing machine T or the string z that it represents. Any function that satisﬁes
these conditions is acceptable; the one we will use is not necessarily the best, but
it is easy to describe.
We will make the assumption that in specifying a Turing machine, the labels
attached to the states are irrelevant; in other words, we think of two TMs as being
identical if their transition diagrams are identical except for the names of the states.
This assumption allows us to number the states of an arbitrary TM and to base the
encoding on the numbering.
The encoding of a string will also involve the assignment of numbers to alpha-
bet symbols, but this is a little more complicated. As we have said, we want a string
w ∈{0, 1}∗to encode no more than one TM. If two TMs T1 and T2 are identical
except that one has tape alphabet {a, b} and the other {a, c}, and if we agree that
this difference is enough to make the two TMs different, then we can’t assign b
and c the same number. Similarly, we can’t ever assign the same number to two
different symbols that might show up in tape alphabets of TMs. This is the reason
for adopting the convention stated below.
Convention
We assume that there is an inﬁnite set S = {a1, a2, a3, . . .} of symbols, where
a1 = , such that the tape alphabet of every Turing machine T is a subset of S.

254
C H A P T E R 7
Turing Machines
The idea of the encoding we will use is simply that we represent a Turing
machine as a set of moves, and each move
δ(p, a) = (q, b, D)
is associated with a 5-tuple of numbers that represent the ﬁve components p, a, q,
b, and D of the move. Each number is represented by a string of that many 1’s,
and the 5-tuple is represented by the string containing all ﬁve of these strings, each
of the ﬁve followed by 0.
Deﬁnition 7.33
An Encoding Function
If T = (Q, , , q0, δ) is a TM and z is a string, we deﬁne the strings
e(T ) and e(z) as follows.
First we assign numbers to each state, tape symbol, and tape head
direction of T . Each tape symbol, including , is an element ai of S, and it
is assigned the number n(ai) = i. The accepting state ha, the rejecting state
hr, and the initial state q0 are assigned the numbers n(ha) = 1, n(hr) = 2,
and n(q0) = 3. The other elements q ∈Q are assigned distinct numbers
n(q), each at least 4. We don’t require the numbers to be consecutive,
and the order is not important. Finally, the three directions R, L, and S
are assigned the numbers n(R) = 1, n(L) = 2, and n(S) = 3.
For each move m of T of the form δ(p, σ) = (q, τ, D)
e(m) = 1n(p)01n(σ)01n(q)01n(τ)01n(D)0
We list the moves of T in some order as m1, . . . , mk, and we deﬁne
e(T ) = e(m1)0e(m2)0 . . . 0e(mk)0
If z = z1z2 . . . zj is a string, where each zi ∈S,
e(z) = 01n(z1)01n(z2)0 . . . 01n(zj )0
EXAMPLE 7.34
A Sample Encoding of a TM
Let T be the TM shown in Figure 7.35, which transforms an input string of a’s and b’s by
changing the leftmost a, if there is one, to b. We assume for simplicity that n(a) = 2 and
n(b) = 3. By deﬁnition, n(q0) = 3, and we let n(p) = 4 and n(r) = 5.
If m is the move determined by the formula δ(q0, ) = (p, , R), then
e(m) = 130101401010 = 111010111101010
and if we encode the moves in the order they appear in the diagram, left to right,
e(T ) = 111010111010100
111101110111101110100
1111011011111011101100
1111010111110101100
111110111011111011101100
11111010101011100

7.8
Universal Turing Machines
255
Δ /Δ, L
Δ /Δ,S
b/b, R
q0
b/b, L
q0 Δ /Δ, R
a/b, L
p
r
ha
Figure 7.35
Because the states of a TM can be numbered in different ways, and the moves
can be considered in any order, there are likely to be many strings of 0’s and 1’s
that represent the same TM. The important thing, however, is that a string of 0’s
and 1’s can’t represent more than one.
Starting with a string such as the one in Example 7.34, it is easy to reconstruct
the Turing machine it represents, because we can easily identify the individual
moves. The only remaining question is whether we can determine, for an arbitrary
string w of 0’s and 1’s, whether it actually represents a TM.
Every string of the form e(T ) must be a concatenation of one or more “5-
tuples,” each matching the regular expression (11∗0)50. There are several ways in
which a string having this form might still fail to be e(T ) for any Turing machine T .
It might contain a 5-tuple that shows a move from one of the halt states; or it might
contain more than one 5-tuple having the same ﬁrst two parts (that is, either the
same 5-tuple appears more than once, or there are 5-tuples representing more than
one move for the same state-symbol combination); or it might contain a 5-tuple in
which the last string of 1’s has more than three. Theorem 7.36 asserts, however,
that these are the only ways.
Theorem 7.36
Let E = {e(T ) | T is a Turing machine}. Then for every x ∈{0, 1}∗,
x ∈E if and only if all these conditions are satisﬁed:
1. x matches the regular expression (11∗0)50((11∗0)50)∗, so that it can be
viewed as a sequence of one or more 5-tuples.
2. No two substrings of x representing 5-tuples can have the same ﬁrst two
parts (no move can appear twice, and there can’t be two different moves for
a given combination of state and tape symbol).
3. None of the 5-tuples can have ﬁrst part 1 or 11 (there can be no moves from
a halting state).
4. The last part of each 5-tuple must be 1, 11, or 111 (it must represent a
direction).
These conditions do not guarantee that the string actually represents a TM that
carries out a meaningful computation. There might be no transitions from the initial
state, states that are unreachable for other reasons, and transitions to a state that
doesn’t show up in the encoding. (We can interpret missing transitions as rejecting

256
C H A P T E R 7
Turing Machines
moves, just as we do for transition diagrams.) But if these conditions are satisﬁed,
it is possible to draw a diagram that shows moves corresponding to each 5-tuple,
and so the string encodes a TM.
In any case, testing a string x ∈{0, 1}∗to determine whether it satisﬁes these
conditions is a straightforward process, and so we have veriﬁed that our encoding
function e satisﬁes the minimal requirements for such a function.
In the last part of this section, we sketch how a universal TM Tu might be
constructed. This time we will use three tapes. The ﬁrst tape is for input and output
and originally contains the string e(T )e(z), where T is a TM and z is a string over
the input alphabet of T . The second tape will correspond to the working tape of T ,
during the computation that simulates the computation of T on input z. The third
tape will have only the encoded form of T ’s current state.
There is no confusion about where e(T ) stops and e(z) starts on tape 1; in the
ﬁrst occurrence of 000, the ﬁrst two 0’s are the end of e(T ) and the third is the
beginning of e(z). Tu starts by transferring the string e(z), except for the initial 0,
from the end of tape 1 to tape 2, beginning in square 3. Since T begins with its left-
most square blank, Tu writes 10, the encoded form of , in squares 1 and 2. Square
0 is left blank, and the tape head begins on square 1. The second step is for Tu to
write 1110, the encoded form of the initial state, on tape 3, beginning in square 1.
As the simulation starts, the three tape heads are all on square 1. At each stage,
the next move is determined by T ’s state, represented by the string on tape 3, and
the current symbol on T ’s tape, whose encoding starts in the current position on
tape 2. To simulate this move, Tu has to search tape 1 for the 5-tuple whose ﬁrst
two parts match this state-input combination; assuming it is found, the last three
parts tell Tu how to carry out the move. To illustrate (returning to Example 7.34),
suppose that before the search, the three tapes look like this:
1110101110101001111011101111011101001111011011111011101100111101011.. .
10111011101101110110
1111
Tu searches for the 5-tuple that begins 11110110, which in this case is the
third 5-tuple on tape 1. (5-tuples end in 00.) It indicates that T ’s current symbol
should be changed from a (11) to b (111), that the state should change from p
to r (from 1111 to 11111), and that the tape head should be moved left. After Tu
simulates this move, the tapes look like
1110101110101001111011101111011101001111011011111011101100111101011.. .
101110111011101110110
11111
If T ’s computation on the input string z never terminates, then Tu will never
halt. T might reject, because of an explicit transition to hr, because no move is
speciﬁed, or because the move calls for it to move its tape head off the tape. Tu
can detect each of these situations (it detects the second by not ﬁnding on tape 1
the state-symbol combination it searches for), and in each case it rejects. Finally,

Exercises
257
T might accept, which Tu will discover when it sees that the third part of the
5-tuple it is processing is 1; in this case, after Tu has modiﬁed tape 2 as the 5-tuple
speciﬁes, it erases tape 1, copies tape 2 onto tape 1, and accepts.
EXERCISES
7.1.
Trace the TM in Figure 7.6, accepting the language {ss | s ∈{a, b}∗}, on
the string aaba. Show the conﬁguration at each step.
7.2.
Below is a transition table for a TM with input alphabet {a, b}.
q
σ
δ(q, σ)
q
σ
δ(q, σ)
q
σ
δ(q, σ)
q0

(q1, , R)
q2

(ha, , R)
q6
a
(q6, a, R)
q1
a
(q1, a, R)
q3

(q4, a, R)
q6
b
(q6, b, R)
q1
b
(q1, b, R)
q4
a
(q4, a, R)
q6

(q7, b, L)
q1

(q2, , L)
q4
b
(q4, b, R)
q7
a
(q7, a, L)
q2
a
(q3, , R)
q4

(q7, a, L)
q7
b
(q7, b, L)
q2
b
(q5, , R)
q5

(q6, b, R)
q7

(q2, , L)
What is the ﬁnal conﬁguration if the TM starts with input string x?
7.3.
Let T = (Q, , , q0, δ) be a TM, and let s and t be the sizes of the sets
Q and , respectively. How many distinct conﬁgurations of T could there
possibly be in which all tape squares past square n are blank and T ’s tape
head is on or to the left of square n? (The tape squares are numbered
beginning with 0.)
7.4.
For each of the following languages, draw a transition diagram for a
Turing machine that accepts that language.
a. AnBn = {anbn | n ≥0}
b. {aibj | i < j}
c. {aibj | i ≤j}
d. {aibj | i ̸= j}
7.5.
For each part below, draw a transition diagram for a TM that accepts
AEqB = {x ∈{a, b}∗| na(x) = nb(x)} by using the approach that is
described.
a. Search the string left-to-right for an a; as soon as one is found, replace
it by X, return to the left end, and search for b; replace it by X; return
to the left end and repeat these steps until one of the two searches is
unsuccessful.
b. Begin at the left and search for either an a or a b; when one is found,
replace it by X and continue to the right searching for the opposite
symbol; when it is found, replace it by X and move back to the left
end; repeat these steps until one of the two searches is unsuccessful.
7.6.
Draw a transition diagram for a TM accepting Pal, the language of
palindromes over {a, b}, using the following approach. Look at the

258
C H A P T E R 7
Turing Machines
leftmost symbol of the current string, erase it but remember it, move to the
rightmost symbol and see if it matches the one on the left; if so, erase it
and go back to the left end of the remaining string. Repeat these steps
until either the symbols are exhausted or the two symbols on the ends
don’t match.
7.7.
Draw a transition diagram for a TM accepting NonPal, the language of
nonpalindromes over {a, b}, using an approach similar to that in
Exercise 7.6.
7.8.
Refer to the transition diagram in Figure 7.8. Modify the diagram so that
on each pass in which there are a’s remaining to the right of the b, the
TM erases the rightmost a. The modiﬁed TM should accept the same
language but with no chance of an inﬁnite loop.
7.9.
Describe the language (a subset of {1}∗) accepted by the TM in
Figure 7.37.
7.10.
We do not deﬁne -transitions for a TM. Why not? What features of a
TM make it unnecessary or inappropriate to talk about -transitions?
7.11.
Given TMs T1 = (Q1, 1, 1, q1, δ1) and T2 = (Q2, 2, 2, q2, δ2), with
1 ⊆2, give a precise deﬁnition of the TM T1T2 = (Q, , , q0, δ). Say
precisely what Q, , , q0, and δ are.
7.12.
Suppose T is a TM accepting a language L. Describe how you would
modify T to obtain another TM accepting L that never halts in the reject
state hr.
7.13.
Suppose T is a TM that accepts every input. We might like to construct a
TM RT such that for every input string x, RT halts in the accepting state
h
x/1, L
1/1, L
1/1, R
1/1, R
Δ/Δ, R
Δ/Δ, S
Δ/Δ, L
ha
1/x, R
1/x, R
x/x, R
Δ/Δ, L
1/Δ, L
Δ/Δ, R
Figure 7.37

Exercises
259
with exactly the same tape contents as when T halts on input x, but with
the tape head positioned at the rightmost nonblank symbol on the tape.
Show that there is no ﬁxed TM T0 such that RT = T T0 for every T . (In
other words, there is no TM capable of executing the instruction “move
the tape head to the rightmost nonblank tape symbol” in every possible
situation.) Suggestion: Assume there is such a TM T0, and try to ﬁnd two
other TMs T1 and T2 such that if RT1 = T1T0 then RT2 cannot be T2T0.
7.14.
Draw the Insert(σ) TM, which changes the tape contents from yz to yσz.
Here y ∈( ∪{})∗, σ ∈ ∪{}, and z ∈∗. You may assume that
 = {a, b}.
7.15.
Draw a transition diagram for a TM Substring that begins with tape
xy, where x, y ∈{a, b}∗, and ends with the same strings on the tape
but with the tape head at the beginning of the ﬁrst occurrence in y of the
string x, if y contains an occurrence of x, and with the tape head on the
ﬁrst blank square following y otherwise.
7.16.
Does every TM compute a partial function? Explain.
7.17.
For each case below, draw a TM that computes the indicated function. In
the ﬁrst ﬁve parts, the function is from N to N. In each of these parts,
assume that the TM uses unary notation—i.e., the natural number n is
represented by the string 1n.
a. f (x) = x + 2
b. f (x) = 2x
c. f (x) = x2
d. f (x) = the smallest integer greater than or equal to log2(x + 1) (i.e.,
f (0) = 0, f (1) = 1, f (2) = f (3) = 2, f (4) = . . . = f (7) = 3, and so
on).
e. E: {a, b}∗× {a, b}∗→{0, 1} deﬁned by E(x, y) = 1 if x = y,
E(x, y) = 0 otherwise.
f. pl : {a, b}∗× {a, b}∗→{0, 1} deﬁned by pl(x, y) = 1 if x < y,
pl(x, y) = 0 otherwise. Here < means with respect to “lexicographic,”
or alphabetical, order. For example, a < aa, abab < abb, etc.
g. pc, the same function as in the previous part except this time < refers
to canonical order. That is, a shorter string precedes a longer one, and
the order of two strings of the same length is alphabetical.
7.18.
The TM shown in Figure 7.38 computes a function from {a, b}∗to {a, b}∗.
For any string x ∈{a, b}∗, describe the string f (x).
7.19.
Suppose TMs T1 and T2 compute the functions f1 and f2 from N to N,
respectively. Describe how to construct a TM to compute the function
f1 + f2.
7.20.
Draw a transition diagram for a TM with input alphabet {0, 1} that
interprets the input string as the binary representation of a nonnegative
integer and adds 1 to it.

260
C H A P T E R 7
Turing Machines
q0
Δ /Δ, R
Δ /a, L
Δ /Δ, S
Δ /Δ, R
a/a, S
b/b, L
Delete
b/b, R
b/b, R
a/a, L
b/b, L
a/a, R
b/b, R
a/a, L
b/b, L
a/a, R
Δ /a, L
Δ /Δ, L
Δ /Δ, S
ha
Figure 7.38
7.21.
Draw a TM that takes as input a string of 0’s and 1’s, interprets it as the
binary representation of a nonnegative integer, and leaves as output the
unary representation of that integer (i.e., a string of that many 1’s).
7.22.
Draw a TM that does the reverse of the previous problem: accepts a string
of n 1’s as input and leaves as output the binary representation of n.
7.23.
Draw a transition diagram for a three-tape TM that works as follows:
starting in the conﬁguration (q0, x, y, ), where x and y are strings of
0’s and 1’s of the same length, it halts in the conﬁguration
(ha, x, y, z), where z is the string obtained by interpreting x and y
as binary representations and adding them.
7.24.
In Example 7.5, a TM is given that accepts the language {ss | s ∈{a, b}∗}.
Draw a TM with tape alphabet {a, b} that accepts this language.
7.25.
We can consider a TM with a doubly inﬁnite tape, by allowing the numbers
of the tape squares to be negative as well as positive. In most respects the
rules for such a TM are the same as for an ordinary one, except that now
when we refer to the conﬁguration xqσy, including the initial
conﬁguration corresponding to some input string, there is no assumption
about exactly where on the tape the strings and the tape head are. Draw a
transition diagram for a TM with a doubly inﬁnite tape that does the
following: If it begins with the tape blank except for a single a somewhere
on it, it halts in the accepting state with the head on the square with
the a.

Exercises
261
7.26.
Let G be the nondeterministic TM described in Example 7.30, which
begins with a blank tape, writes an arbitrary string x on the tape, and halts
with tape x. Let NB, PB, Copy, and Equal be the TMs described in
Examples 7.17, 7.18, and 7.24. Consider the NTM
NB →G →Copy →NB →Delete →PB →PB →Equal
which is nondeterministic because G is. What language does it accept?
7.27.
Using the idea in Exercise 7.26, draw a transition diagram for an NTM
that accepts the language {1n | n = k2 for some k ≥0}.
7.28.
Suppose L is accepted by a TM T . For each of the following languages,
describe informally how to construct a nondeterministic TM that will
accept that language.
a. The set of all sufﬁxes of elements of L
b. The set of all substrings of elements of L
7.29.
Suppose L1 and L2 are subsets of ∗and T1 and T2 are TMs accepting L1
and L2, respectively. Describe how to construct a nondeterministic TM to
accept L1L2.
7.30.
Suppose T is a TM accepting a language L. Describe how to construct a
nondeterministic TM accepting L∗.
7.31.
Table 5.8 describes a PDA accepting the language Pal. Draw a TM that
accepts this language by simulating the PDA. You can make the TM
nondeterministic, and you can use a second tape to represent the stack.
7.32.
Describe informally how to construct a TM T that enumerates the set of
palindromes over {0, 1} in canonical order. In other words, T loops
forever, and for every positive integer n, there is some point at which the
initial portion of T ’s tape contains the string
010011000 . . . xn
where xn is the nth palindrome in canonical order, and this portion of the
tape is never subsequently changed.
7.33.
Suppose you are given a Turing machine T (you have the transition
diagram), and you are watching T processing an input string. At each step
you can see the conﬁguration of the TM: the state, the tape contents, and
the tape head position.
a. Suppose that for some n, the tape head does not move past square n
while you are watching. If the pattern continues, will you be able to
conclude at some point that the TM is in an inﬁnite loop? If so, what is
the longest you might need to watch in order to draw this conclusion?
b. Suppose that in each move you observe, the tape head moves right. If
the pattern continues, will you be able to conclude at some point that
the TM is in an inﬁnite loop? If so, what is the longest you might need
to watch in order to draw this conclusion?

262
C H A P T E R 7
Turing Machines
7.34.
In each of the following cases, show that the language accepted by the
TM T is regular.
a. There is an integer n such that no matter what the input string is, T
never moves its tape head to the right of square n.
b. For every n ≥0 and every input of length n, T begins by making
n + 1 moves in which the tape head is moved right each time, and
thereafter T does not move the tape head to the left of square n + 1.
7.35.
†Suppose T is a TM. For each integer i ≥0, denote by ni(T ) the number
of the rightmost square to which T has moved its tape head within the
ﬁrst i moves. (For example, if T moves its tape head right in the ﬁrst ﬁve
moves and left in the next three, then ni(T ) = i for i ≤5 and ni(T ) = 5
for 6 ≤i ≤10.) Suppose there is an integer k such that no matter what the
input string is, ni(T ) ≥i −k for every i ≥0. Does it follow that L(T ) is
regular? Give reasons for your answer.
7.36.
Suppose M1 is a two-tape TM, and M2 is the ordinary TM constructed in
Theorem 7.26 to simulate M1. If M1 requires n moves to process an input
string x, give an upper bound on the number of moves M2 requires in
order to simulate the processing of x. Note that the number of moves M1
has made places a limit on the position of its tape head. Try to make your
upper bound as sharp as possible.
7.37.
Show that if there is a TM T computing the function f : N →N, then
there is another one, T ′, whose tape alphabet is {1}. Suggestion: Suppose
T has tape alphabet  = {a1, a2, . . . , an}. Encode  and each of the ai’s
by a string of 1’s and ’s of length n + 1 (for example, encode  by
n + 1 blanks, and ai by 1in+1−i). Have T ′ simulate T , but using blocks
of n + 1 tape squares instead of single squares.
7.38.
Beginning with a nondeterministic Turing machine T1, the proof of
Theorem 7.31 shows how to construct an ordinary TM T2 that accepts the
same language. Suppose |x| = n, T1 never has more than two choices of
moves, and there is a sequence of nx moves by which T1 accepts x.
Estimate as precisely as possible the number of moves that might be
required for T2 to accept x.
7.39.
Formulate a precise deﬁnition of a two-stack automaton, which is like a
PDA except that it is deterministic and a move takes into account the
symbols on top of both stacks and can replace either or both of them.
Describe informally how you might construct a machine of this type
accepting {aibici | i ≥0}. Do it in a way that could be generalized to
{aibicidi | i ≥0}, {aibicidiei | i ≥0}, etc.
7.40.
Describe how a Turing machine can simulate a two-stack automaton;
speciﬁcally, show that any language that can be accepted by a two-stack
machine can be accepted by a TM.
7.41.
A Post machine is similar to a PDA, but with the following differences. It
is deterministic; it has an auxiliary queue instead of a stack; and the input

Exercises
263
is assumed to have been previously loaded onto the queue. For example, if
the input string is abb, then the symbol currently at the front of the queue
is a. Items can be added only to the rear of the queue, and deleted only
from the front. Assume that there is a marker Z0 initially on the queue
following the input string (so that in the case of null input Z0 is at the
front). The machine can be deﬁned as a 7-tuple M = (Q, , , q0, Z0, A,
δ), like a PDA. A single move depends on the state and the symbol
currently at the front of the queue; and the move has three components:
the resulting state, an indication of whether or not to remove the current
symbol from the front of the queue, and what to add to the rear of the
queue (a string, possibly null, of symbols from the queue alphabet).
Construct a Post machine to accept the language {anbncn | n ≥0}.
7.42.
We can specify a conﬁguration of a Post machine (see Exercise 7.41) by
specifying the state and the contents of the queue. If the original marker
Z0 is currently in the queue, so that the string in the queue is of the form
αZ0β, then the queue can be thought of as representing the tape of a
Turing machine, as follows. The marker Z0 is thought of, not as an actual
tape symbol, but as marking the right end of the string on the tape; the
string β is at the beginning of the tape, followed by the string α; and the
tape head is currently centered on the ﬁrst symbol of α—or, if α = , on
the ﬁrst blank square following the string β. In this way, the initial queue,
which contains the string αZ0, represents the initial tape of the Turing
machine with input string α, except that the blank in square 0 is missing
and the tape head scans the ﬁrst symbol of the input.
Using this representation, it is not difﬁcult to see how most of the moves
of a Turing machine can be simulated by the Post machine. Here is an
illustration. Suppose that the queue contains the string abbZ0ab, which
we take to represent the tape ababb. To simulate the Turing machine
move that replaces the a by c and moves to the right, we can do the
following:
a. remove a from the front and add c to the rear, producing bbZ0abc
b. add a marker, say $, to the rear, producing bbZ0abc$
c. begin a loop that simply removes items from the front and adds them
to the rear, continuing until the marker $ appears at the front. At this
point, the queue contains $bbZ0abc.
d. remove the marker, so that the ﬁnal queue represents the tape abcbb
The Turing machine move that is hardest to simulate is a move to the left.
Devise a way to do it. Then give an informal proof, based on the
simulation outlined in this discussion, that any language that can be
accepted by a Turing machine can be accepted by a Post machine.
7.43.
Show how a two-stack automaton can simulate a Post machine, using the
ﬁrst stack to represent the queue and using the second stack to help carry
out the various Post machine operations. The ﬁrst step in the simulation is

264
C H A P T E R 7
Turing Machines
to load the input string onto stack 1, using stack 2 ﬁrst in order to get the
symbols in the right order. Give an informal argument that any language
that can be accepted by a Post machine can be accepted by a two-stack
automaton. (The conclusion from this exercise and the preceding ones is
that the three types of machines—Turing machines, Post machines, and
two-stack automata—are equivalent with regard to the languages they can
accept.)

265
C
H
A
P
T
E
R
8
Recursively Enumerable
Languages
R
ecursively enumerable languages are the ones that can be accepted by Turing
machines, and in the ﬁrst section of the chapter we distinguish them from
recursive languages, those that can be decided by Turing machines. Only if a
language is recursive is there an algorithm guaranteed to determine whether an
arbitrary string is an element. In the second and third sections of the chapter we
examine two other ways of characterizing recursively enumerable languages, one
in terms of algorithms to list the elements and one using unrestricted grammars. In
the fourth section, we discuss the Chomsky hierarchy, which contains four classes
of languages, each having a corresponding model of computation and a correspond-
ing type of grammar. Two of the three classes other than recursively enumerable
languages are the ones discussed earlier in the book. The remaining one, the class
of context-sensitive languages, is also described brieﬂy. In the last section we use
a diagonal argument to demonstrate precisely that there are more languages than
there are Turing machines. As a result, there must be some languages that are not
recursively enumerable and others that are recursively enumerable but not recursive.
8.1 RECURSIVELY ENUMERABLE
AND RECURSIVE
Deﬁnition 8.1
Accepting a Language and Deciding
a Language
A Turing machine T with input alphabet  accepts a language L ⊆∗
if L(T ) = L. T decides L if T computes the characteristic function χL :
∗→{0, 1}. A language L is recursively enumerable if there is a TM
that accepts L, and L is recursive if there is a TM that decides L.

266
C H A P T E R 8
Recursively Enumerable Languages
Recursively enumerable languages are sometimes referred to as Turing-
acceptable, and recursive languages are sometimes called Turing-decidable, or
simply decidable.
As the discussion in Section 7.3 suggested, trying to accept L and trying to
decide L are two ways of approaching the membership problem for L:
Given a string x ∈∗, is x ∈L?
To decide L is to solve the problem conclusively, because for every x ∈∗, decid-
ing whether x ∈L is exactly what must be done in order to determine the value
of χL(x). Accepting L may be less conclusive. For a string x /∈L, an algorithm
accepting L works correctly as long as it doesn’t report that x ∈L; but not report-
ing that x ∈L is less informative than reporting that x /∈L. At this stage, however,
though the deﬁnitions are clearly different, it is not obvious whether there are really
languages satisfying one and not the other.
We will be able to resolve these questions before this chapter is over. In this
ﬁrst section, we establish several facts that will help us understand the relationships
between the two types of languages, starting in Theorem 8.2 with the fundamental
relationship that we ﬁgured out in Section 7.3.
In proving the statements in this section, as well as in later parts of the chapter,
we will take advantage of the Church-Turing thesis. For example, to show that a
language L is recursively enumerable, it will be sufﬁcient to describe an algorithm
(without describing exactly how a TM might execute the algorithm) that answers
yes if the input string is in L and either answers no or doesn’t answer if the input
string is not in L.
Theorem 8.2
Every recursive language is recursively enumerable.
Proof
Suppose T is a TM that decides L ⊆∗. An algorithm to accept L is the
following: Given x ∈∗, execute T on input x. T will halt and produce
an output; if the output is 1 (i.e., if χL(x) = 1), accept, and if the output
is 0, reject.
The reason the converse of Theorem 8.1 is not obviously true, and will turn
out to be false, is that if T accepts L, there may be input strings not in L that
cause T to loop forever. The closest we can come to the converse statement is
Theorem 8.3.
Theorem 8.3
If L ⊆∗is accepted by a TM T that halts on every input string, then
L is recursive.

8.1
Recursively Enumerable and Recursive
267
Proof
If T halts on every input string, then the following is an algorithm for
deciding L: Given x ∈∗, if T accepts x, return 1, and if T rejects x,
return 0.
A consequence of Theorem 8.3 is that if L is accepted by a nondeterministic
TM T , and if there is no input string on which T can possibly loop forever, then
L is recursive. The reason is that we can convert T to a deterministic TM T1 as
described in Section 7.7. The construction guarantees that if there are no inputs
allowing T to loop forever, then T1 halts on every input.
Theorem 8.4
If L1 and L2 are both recursively enumerable languages over , then
L1 ∪L2 and L1 ∩L2 are also recursively enumerable.
Proof
Suppose that T1 is a TM that accepts L1, and T2 is a TM that accepts
L2. The algorithms we describe to accept L1 ∪L2 and L1 ∩L2 will both
process an input string x by running T1 and T2 simultaneously on the
string x. We might implement the algorithm by building a two-tape TM
that literally simulates both TMs simultaneously, one on each tape, or
we might simply use transition diagrams for both TMs to trace one
move of each on the input string x, then two moves of each, and so
forth.
To accept L1 ∪L2, the algorithm is simply to wait until one of
the two TMs accepts x, and to accept only when that happens. If one
rejects, we abandon it and continue with the other. If both eventually
reject, we can reject and halt, though this is not required in order
to accept L1 ∪L2. If both TMs loop forever, then we never receive
an answer, but the only case in which this can occur is when x /∈
L1 ∪L2.
To accept L1 ∩L2, the algorithm is to wait until both TMs accept x,
and to accept only when that happens. This time, if one TM accepts, we
continue with the other; if either TM ever rejects, we can reject and halt,
though again this is not necessary.
Theorem 8.5
If L1 and L2 are both recursive languages over , then L1 ∪L2 and
L1 ∩L2 are also recursive.
Proof
See Exercise 8.1.

268
C H A P T E R 8
Recursively Enumerable Languages
Theorem 8.6
If L is a recursive language over , then its complement L′ is also
recursive.
Proof
If T is a TM that computes the characteristic function χL, the TM obtained
from T by interchanging the two outputs computes the characteristic func-
tion of L′.
Theorem 8.7
If L is a recursively enumerable language, and its complement L′ is also
recursively
enumerable,
then
L
is
recursive
(and
therefore,
by
Theorem 8.6, L′ is recursive).
Proof
If T is a TM accepting L, and T1 is another TM accepting L′, then here is
an algorithm to decide L: For a string x, execute T and T1 simultaneously
on input x, until one halts. (One will eventually accept, because either
x ∈L or x ∈L′.) If the one that halts ﬁrst is T , and it accepts, or the one
that halts ﬁrst is T1, and it rejects, return 1; otherwise return 0.
Theorem 8.7 implies that if there are any nonrecursive languages, then there
must be languages that are not recursively enumerable. (Otherwise, for every lan-
guage L, both L and L′ would be recursively enumerable and therefore recursive.)
Suppose we have a TM T that accepts a language L. The possibility that T
might loop forever on an input string x not in L might prevent us from using T to
decide L. (This is why the converse of Theorem 8.2 is not obviously true, as we
observed above.) The same possibility might also prevent us from using T in an
algorithm to accept L′ (this is why Theorem 8.6 is stated for recursive languages,
not recursively enumerable languages), because accepting L′ requires answering
yes for strings in L′, and these are the strings for which T might not return an
answer.
For the language L accepted by T , according to Theorems 8.6 and 8.7, the two
problems are equivalent: If we could ﬁnd another TM to accept L′, then we could
ﬁnd another TM to decide L, and conversely. We will see later in this chapter that
the potential difﬁculty cannot always be eliminated. There are languages that can
be accepted by TMs but not decided. These are the same languages that can be
accepted by TMs but whose complements cannot; they are languages that can be
accepted, but only by TMs that loop forever for some inputs not in the language.
8.2 ENUMERATING A LANGUAGE
To enumerate a set means to list the elements, and we begin by saying precisely
how a Turing machine enumerates a language L (or, informally, lists the elements

8.2
Enumerating a Language
269
of L). The easiest way to formulate the deﬁnition is to use a multitape TM with
one tape that operates exclusively as the output tape.
Deﬁnition 8.8
A TM Enumerating a Language
Let T be a k-tape Turing machine for some k ≥1, and let L ⊆∗. We
say T enumerates L if it operates such that the following conditions are
satisﬁed.
1. The tape head on the ﬁrst tape never moves to the left, and no nonblank
symbol printed on tape 1 is subsequently modiﬁed or erased.
2. For every x ∈L, there is some point during the operation of T when tape 1
has contents
x1#x2# . . . #xn#x#
for some n ≥0, where the strings x1, x2, . . . , xn are also elements of L and
x1, x2, . . . , xn, x are all distinct. If L is ﬁnite, then nothing is printed after
the # following the last element of L.
This idea leads to a characterization of recursively enumerable languages, and
with an appropriate modiﬁcation involving the order in which the strings are listed,
it can also be used to characterize recursive languages.
Theorem 8.9
For every language L ⊆∗, L is recursively enumerable if and only if
there is a TM enumerating L, and L is recursive if and only if there is a
TM that enumerates the strings in L in canonical order (see Section 1.4).
Proof
We have to prove, for an alphabet  and a language L ⊆∗, these four
things:
1. If there is a TM that accepts L, then there is a TM that enumerates L.
2. If there is a TM that enumerates L, then there is a TM that accepts L.
3. If there is a TM that decides L, then there is a TM that enumerates L in
canonical order.
4. If there is a TM that enumerates L in canonical order, then there is a TM
that decides L.
In all four of these proofs, just as in Section 8.1, we will take advan-
tage of the Church-Turing thesis.
We start with statement 3, which is easier than statement 1. If T
decides L, then for any string x, we can give x to T and wait for it to
tell us whether x ∈L. So here is an algorithm to list the elements of L
in canonical order: Consider the strings of L in canonical order; for each

270
C H A P T E R 8
Recursively Enumerable Languages
string x, give it to T and wait for the answer, and if the answer is yes,
include x in the list. Then the order in which the elements of L are listed
is the same as the order in which they are considered, which is canonical
order.
For statement 1, this argument doesn’t work without some modiﬁca-
tion, because if T is only assumed to accept L, “give it to T and wait for
the answer” might mean waiting forever. The modiﬁcation is that although
we start by considering the elements x0, x1, x2, . . . of ∗in canonical
order, we may not be able to decide whether to list a string xi the ﬁrst
time we consider it. Instead, we make repeated passes. On each pass, we
consider one additional string from the canonical-order list, and for each
string xi that we’re still unsure about, we execute T on xi for one more
step than we did in the previous pass.
For {a, b}∗= {, a, b, aa, . . .}, here is a description of the compu-
tations of T that we may have to perform during the ﬁrst four passes.
(Whenever we decide that a string x is in L, we eliminate it from the
canonical-order list, so that it is not included more than once in our enu-
meration.)
Pass 1: 1 step on input 
Pass 2: 2 steps on , 1 step on a
Pass 3: 3 steps on , 2 steps on a, 1 step on b
Pass 4: 4 steps on , 3 steps on a, 2 steps on b, 1 step on aa
The enumeration that is produced will contain only strings that are accepted
by T ; and every string x that is accepted by T in exactly k steps will
appear during the pass on which we ﬁrst execute T for k steps on input x.
The enumeration required in statement 1 does not need to be in canonical
order, and we can’t expect that it will be: There may be strings x and y
such that even though x precedes y in canonical order, y is included in
the enumeration on an earlier pass than x because T needs fewer steps to
accept it.
Statement 2 is easy. If T enumerates L, then an algorithm to accept
L is this: Given a string x, watch the computation of T , and accept x
precisely if T lists the string x. (This algorithm illustrates in an extreme
way the difference between accepting a language and deciding a language;
at least if L is inﬁnite, the only two possible outcomes for a string x are
that x will be accepted and that no answer will ever be returned.)
Statement 4 is almost as easy, except for a slight subtlety in the case
when L is ﬁnite, and allows us to use the same approach. This time,
because T is guaranteed to enumerate L in canonical order, we will be
able to decide whether a string x is in L as soon as one of these two things
happens: (i) x appears in the enumeration (which means that x ∈L); (ii) a
string y that follows x in canonical order appears in the enumeration, and
x has not yet appeared (which means that x /∈L). As long as L is inﬁnite,

8.3
More General Grammars
271
one of these two things will eventually happen. If L is ﬁnite, it may be that
neither happens, because our rules for enumerating a language allow T to
continue making moves forever after it has printed the last element of L.
However, we do not need the assumption in statement 4 to conclude that
a ﬁnite language L is recursive; in fact, every ﬁnite language is regular.
8.3 MORE GENERAL GRAMMARS
In this section we introduce a type of grammar more general than a context-
free grammar. These unrestricted grammars correspond to recursively enumerable
languages in the same way that CFGs correspond to languages accepted by PDAs
and regular grammars to those accepted by FAs.
The feature of context-free grammars that imposes such severe restrictions
on the corresponding languages (the feature that makes it possible to prove the
pumping lemma for CFLs) is precisely their context-freeness: every sufﬁciently
long derivation must contain a “self-embedded” variable A (one for which the
derivation looks like S ⇒∗vAz ⇒∗vwAyz), and any production having left side
A can be applied in any place that A appears.
The grammars we are about to introduce will therefore allow productions
involving a variable to depend on the context in which the variable appears. To
illustrate: Aa →ba allows A to be replaced by b, but only if it is followed imme-
diately by a. In fact, the easiest way to describe these more general productions
is to drop the association of a production with a speciﬁc variable, and to think
instead of a string being replaced by another string (Aa →b, for example). We
still want to say that a derivation terminates when the current string contains no
more variables; for this reason, we will require the left side of every production to
contain at least one variable. Context-free productions A →α will certainly still
be allowed, but in general they will not be the only ones.
Deﬁnition 8.10
Unrestricted Grammars
An unrestricted grammar is a 4-tuple G = (V, , S, P ), where V and 
are disjoint sets of variables and terminals, respectively. S is an element
of V called the start symbol, and P is a set of productions of the form
α →β
where α, β ∈(V ∪)∗and α contains at least one variable.
We can continue to use much of the notation that was developed for CFGs.
For example,
α ⇒∗
G β
means that β can be derived from α in G, in zero or more steps, and
L(G) = {x ∈∗| S ⇒∗
G x}

272
C H A P T E R 8
Recursively Enumerable Languages
One important difference is that because the productions are not necessarily context-
free, the assumption that S ⇒∗xAy ⇒∗z, where A ∈V and x, y, z ∈∗, no
longer implies that z = xwy for some string w.
EXAMPLE 8.11
A Grammar Generating {a2k | k ∈N }
Let L = {a2k | k ∈N}. L can be deﬁned recursively by saying that a ∈L and that for every
n ≥1, if an ∈L, then a2n ∈L. Using this idea to obtain a grammar means ﬁnding a way to
double the number of a’s in the string obtained so far. The idea is to use a variable D that
will act as a “doubling operator.” D replaces each a by two a’s, by means of the production
Da →aaD. At the beginning of each pass, D is introduced at the left end of the string, and
we think of each application of the production as allowing D to move past an a, doubling
it in the process. The complete grammar has the productions
S →LaR
L →LD
Da →aaD
DR →R
L →
R →
Beginning with the string LaR, the number of a’s will be doubled every time a copy of D is
produced and moves through the string. Both variables L and R can disappear at any time.
There is no danger of producing a string of a’s in which the action of one of the doubling
operators is cut short, because if R disappears when D is present, there is no way for D to
be eliminated. The string aaaa has the derivation
S ⇒LaR ⇒LDaR ⇒LaaDR ⇒LaaR ⇒LDaaR
⇒LaaDaR ⇒LaaaaDR ⇒LaaaaR ⇒aaaaR ⇒aaaa
EXAMPLE 8.12
A Grammar Generating {anbncn | n ≥1}
The previous example used the idea of a variable moving through the string and operating
on it. In this example we use a similar left-to-right movement, although there is no explicit
“operator” like the variable D, as well as another kind arising from the variables rearranging
themselves. Like the previous example, this one uses the variable L to denote the left end
of the string.
We begin with the productions
S →SABC | LABC
which allow us to obtain strings of the form L(ABC)n, where n ≥1. Next, productions that
allow the variables A, B, and C to arrange themselves in alphabetical order:
BA →AB
CB →BC
CA →AC
Finally, productions that allow the variables to be replaced by the corresponding terminals,
provided they are in alphabetical order:
LA →a
aA →aa
aB →ab
bB →bb
bC →bc
cC →cc
Although nothing forces the variables to arrange themselves in alphabetical order, doing so
is the only way they can ultimately be replaced by terminals.
None of the productions can rearrange existing terminal symbols. This means, for
example, that if an a in the string, which must come from an A, has a terminal right before

8.3
More General Grammars
273
it, that terminal must be an a, because no other terminal could have allowed the A to
become an a. The combination cb cannot occur for the same reason. The ﬁnal string has
equal numbers of a’s, b’s, and c’s, because originally there were equal numbers of the three
variables, and the terminal symbols must be in alphabetical order.
These examples illustrate the fact that unrestricted grammars can generate non-
context-free languages (see Exercises 6.2 and 6.3), although both these languages
still involve simple repetitive patterns that make it relatively easy to ﬁnd gram-
mars. As we are about to see, unrestricted grammars must be able to generate
more complicated languages, because every recursively enumerable language can
be generated this way. We begin with the converse result.
Theorem 8.13
For every unrestricted grammar G, there is a Turing machine T with
L(T ) = L(G).
Proof
We will describe a Turing machine T that accepts L(G), and we can
simplify the description considerably by making T nondeterministic. Its
tape alphabet will contain all the variables and terminals in G, and perhaps
other symbols, and it works as follows.
The ﬁrst phase of T ’s operation is simply to move the tape head to
the blank square following the input string x.
During the second phase of its operation, T treats this blank square as
if it were the beginning of its tape, and the input string x is undisturbed. In
this phase, T simulates a derivation in G nondeterministically, as follows.
First, the symbol S is written on the tape in the square following the
blank. Each subsequent step in the simulation can be accomplished by T
carrying out these actions:
1. Choosing a production α →β in the grammar G.
2. Selecting an occurrence of α, if there is one, in the string currently on the
tape.
3. Replacing this occurrence of α by β (which, if |α| ̸= |β|, means moving the
string immediately following this occurrence either to the left or to the right).
The nondeterminism shows up in both steps 1 and 2. This simulation phase
of T ’s operation may continue forever, if for each production chosen in
step 1 it is actually possible in step 2 to ﬁnd an occurrence of the left side
in the current string. Otherwise, step 2 will eventually fail because the
left side of the production chosen in step 1 does not appear in the string.
(One way this might happen, though not the only way, is that the string
has no more variables and is therefore actually an element of L(G).) In
this case, the tape head moves back past the blank and past the original
input string x to the blank at the beginning of the tape.

274
C H A P T E R 8
Recursively Enumerable Languages
The ﬁnal phase is to compare the two strings on the tape (x and the
string produced by the simulation), and to accept if and only if they are
equal.
If x ∈L(G), then there is a sequence of moves that causes the simu-
lation to produce x as the second string on the tape, and therefore causes
T to accept x. If x /∈L(G), then it is impossible for x to be accepted by
T ; even if the second phase terminates and the second string on the tape
contains only terminals, it is generated by productions in G, and so it will
not be x.
Theorem 8.14
For every Turing machine T with input alphabet , there is an unrestricted
grammar G generating the language L(T ) ⊆∗.
Proof
For simplicity, we assume that  = {a, b}. The grammar we construct
will have three types of productions:
1. Productions that generate, for every string x ∈{a, b}∗, a string containing
two identical copies of x, which we will refer to as x1 and x2. The string also
has additional variables that allow x2 to represent the initial conﬁguration of
T corresponding to input x, and that prevent the derivation from producing a
string of terminals before it has been determined whether T accepts x.
2. Productions that transform x2 (the portion representing a conﬁguration of T )
so as to simulate the moves of T on input x, while keeping x1 unchanged.
3. Productions that allow everything in the string except x1 (the unchanged
copy of x) to be erased, provided the computation of T that is being
simulated reaches the accept state.
The initial conﬁguration of T corresponding to input x includes the
symbols  and q0, as well as the symbols of x, and so these two symbols
will be used as variables in the productions of type 1. In addition, using
left and right parentheses as variables will make it easy to keep the two
copies x1 and x2 separate. The productions of type 1 are
S →S() | T
T →T (aa) | T (bb) | q0()
For example, a derivation might begin
S ⇒S() ⇒T () ⇒T (aa)()
⇒T (bb)(aa)() ⇒()q0(bb)(aa)()
where we think of the ﬁnal string as representing the conﬁguration
q0ba. The actual string has two copies of each blank as well as each
terminal symbol. The two copies of each blank are just to simplify things
slightly, but the two copies of each terminal are necessary: The ﬁrst copy

8.3
More General Grammars
275
belongs to x1 and the second to x2. The TM conﬁguration being repre-
sented can have any number of blanks at the end of the string, but this is
the stage of the derivation in which we need to produce enough symbols
to account for all the tape squares used by the TM as it processes the
input string. The conﬁguration we have shown that ends in one blank (so
that the string ends in a pair of blanks) would be appropriate if the TM
needs to move its tape head to the blank at the right of the input string,
but no farther, in the process of accepting it.
The productions of types 2 and 3 will be easy to understand if we con-
sider an example. Let’s not worry about what language the TM accepts, but
suppose that the transition diagram contains the transitions in Figure 8.1.
Then the computation that results in the string ba being accepted is
this:
q0ba ⊢q1ba ⊢bq1a ⊢q2b$ ⊢bq2$
⊢b$q3 ⊢b$ha
In the ﬁrst step, which involves the TM transition δ(q0, ) = (q1,
, R), the initial portion q0 of the conﬁguration is transformed to q1;
the grammar production that will simulate this step is
q0() →()q1
It is the second  in each of these two parenthetical pairs that is crucial,
because it is interpreted to be the symbol on the TM tape at that step
in the computation. (It may have started out as a different symbol, but
now it is .) The ﬁrst symbol in each pair remains unchanged during the
derivation. It is conceivable that for a different input string, the TM will
encounter a  in state q0 that was originally a or b, and for that reason
the grammar will also have the productions
q0(a) →(a)q1
and
q0(b) →(b)q1
In the computation we are considering, the second, fourth, and ﬁfth steps
also involve tape head moves to the right. In the fourth step, for example,
the portion q2b of the conﬁguration is transformed to bq2, and thus the
grammar will have all three productions
q2(b) →(b)q2
q2(ab) →(ab)q2
q2(bb) →(bb)q2
Δ /Δ, R
b/b, R
a/$, L
b/b, R
q0
q1
q2
$/$, R
Δ /Δ, S
q3
ha
Figure 8.15
Part of a TM transition diagram.

276
C H A P T E R 8
Recursively Enumerable Languages
although the derivation corresponding to this particular computation
requires only the last one.
The grammar productions corresponding to a tape head move to the
left are a little more complicated. In the third step of our computation, the
portion bq1a of the conﬁguration is transformed to q2b$; the corresponding
production is
(bb)q1(aa) →q2(bb)(a$)
and for the reasons we have just described, the grammar will contain every
possible production of the form
(σ1σ2)q1(σ3a) →q2(σ1σ2)(σ3$)
where σ1 and σ3 belong to the set {a, b, } and σ2 is one of these three
or any other tape symbol of the TM.
The last step of the computation above involves a move in which the
tape head is stationary, transforming q3 to ha, and the corresponding
productions look like
q3(σ) →ha(σ)
where σ is a, b, or .
The appearance of the variable ha in the derivation is what will make
it possible for everything in the string except the copy x1 of the original
input string to disappear. So far, the derivation has produced a string
containing pairs of the form (σ1σ2) and one occurrence of the variable ha.
The productions of type 3 will ﬁrst allow the ha to propagate until one
precedes each of the pairs, and then allow each pair to disappear unless
the ﬁrst symbol is a or b (i.e., a symbol in the unchanged copy x1 of the
string x). The “propagating” productions look like
(σ1σ2)ha →ha(σ1σ2)ha
and
ha(σ1σ2) →ha(σ1σ2)ha
and the “disappearing” productions look like
ha(σ1σ2) →σ1
if σ1 is either a or b, and
ha(σ2) →
This example already illustrates the sorts of productions that are
required for an arbitrary Turing machine. The productions of type 1 allow
two copies x1 and x2 of an arbitrary string x to be generated. The pro-
ductions corresponding to TM moves allow the derivation to proceed by
simulating the TM computation on x2, and if x is accepted the last type of
production permits the derivation to continue until only the string x1 = x
of terminals is left. On the other hand, if x is not accepted by the TM, the
variable ha will never make an appearance in the derivation, and there is
no other way for x to be obtained as the ﬁnal product.

8.4
Context-Sensitive Languages and the Chomsky Hierarchy
277
We conclude the argument by displaying the complete derivation of
the string ba in our example, along with the TM moves corresponding to
the middle phase of the derivation. At each step, we have underlined the
left side of the production to be used next.
S
⇒
S()
⇒
T ()
⇒
T (aa)()
⇒
T (bb)(aa)()
⇒
q0()(bb)(aa)()
q0ba
⇒
()q1(bb)(aa)()
⊢
q1ba
⇒
()(bb)q1(aa)()
⊢
bq1a
⇒
()q2(bb)(a$)()
⊢
q2b$
⇒
()(bb)q2(a$)()
⊢
bq2$
⇒
()(bb)(a$)q3()
⊢
b$q3
⇒
()(bb)(a$)ha()
⊢
b$ha
⇒
()(bb)ha(a$)ha()
⇒
()ha(bb)ha(a$)ha()
⇒
ha()ha(bb)ha(a$)ha()
⇒
ha(bb)ha(a$)ha()
⇒
bha(a$)ha()
⇒
baha()
⇒
ba
8.4 CONTEXT-SENSITIVE LANGUAGES
AND THE CHOMSKY HIERARCHY
Deﬁnition 8.16
Context-Sensitive Grammars
A context-sensitive grammar (CSG) is an unrestricted grammar in which
no production is length-decreasing. In other words, every production is of
the form α →β, where |β| ≥|α|.
A language is a context-sensitive language (CSL) if it can be gener-
ated by a context-sensitive grammar.
In this section we will discuss these grammars and these languages brieﬂy, and
we will describe a type of abstract computing device that corresponds to CSLs in
the same way that pushdown automata correspond to CFLs and Turing machines to

278
C H A P T E R 8
Recursively Enumerable Languages
recursively enumerable languages. See Exercise 8.32 for another characterization
of context-sensitive languages that may make it easier to understand why the phrase
“context-sensitive” is appropriate.
EXAMPLE 8.17
A CSG Generating L = {anbncn | n ≥1}
The unrestricted grammar we used in Example 8.12 for this language is not context-sensitive,
because of the production LA →a, but using the same principle we can easily ﬁnd a
grammar that is. Instead of using the variable A as well as a separate variable to mark the
beginning of the string, we introduce a new variable to serve both purposes. It is not hard
to verify that the CSG with productions
S →SABC | ABC
BA →AB
CA →AC
CB →BC
A →a
aA →aa
aB →ab
bB →bb
bC →bc
cC →cc
generates the language L.
Because a context-sensitive grammar cannot have -productions, no language
containing  can be a CSL; however, as Theorem 4.27 shows, every context-free
language is either a CSL or the union of {} and a CSL. It is appropriate to think
of context-sensitive languages as a generalization of context-free languages, and
as Example 8.17 suggests, many CSLs are not context-free. In fact, just about any
familiar language is context-sensitive; in particular, programming languages are—
the non-context-free features of the C language mentioned in Example 6.6 can be
handled by context-sensitive grammars. In any case, if we want to ﬁnd a model of
computation that corresponds exactly to CSLs, it will have to be potentially more
powerful than a PDA.
Theorem 8.13 describes a way of constructing a Turing machine correspond-
ing to a given unrestricted grammar G. One possible way of thinking about an
appropriate model of computation is to refer to this construction and see how the
TM’s operations are restricted if the grammar is actually context-sensitive.
The TM described in Theorem 8.13 simulates a derivation in G, using the
space on the tape to the right of the input string. The tape head never needs to
move farther to the right than the blank at the end of the current string. If G
is context-sensitive, the current string at any stage of the derivation is no longer
than the string of terminals being derived. Therefore, for an input string of length
n, the tape head never needs to move past square 2n (approximately) during the
computation. This restriction on the space used in processing a string is the crucial
feature of the following deﬁnition.
Deﬁnition 8.18
Linear-Bounded Automata
A linear-bounded automaton (LBA) is a 5-tuple M = (Q, , 
, q0, δ)
that is identical to a nondeterministic Turing machine, with the following

8.4
Context-Sensitive Languages and the Chomsky Hierarchy
279
exception. There are two extra tape symbols, [ and ], assumed not to
be elements of the tape alphabet 
. The initial conﬁguration of M corre-
sponding to input x is q0[x], with the symbol [ in the leftmost square and
the symbol ] in the ﬁrst square to the right of x. During its computation,
M is not permitted to replace either of these brackets or to move its tape
head to the left of the [ or to the right of the ].
Theorem 8.19
If L ⊆∗is a context-sensitive language, then there is a linear-bounded
automaton that accepts L.
Proof
We can follow the proof of Theorem 8.13, except that instead of being
able to use the space to the right of the input string on the tape, the LBA
must use the space between the two brackets. It can do this by converting
individual symbols into symbol-pairs so as to simulate two tape “tracks,”
the ﬁrst containing the input string and the second containing the necessary
computation space.
Suppose G = (V, , S, P ) is a context-sensitive grammar generating
L. In addition to other tape symbols, including elements of , the LBA
M that we construct will have in its tape alphabet symbols of the form
(a, b), where a and b are elements of  ∪V ∪{}. Its ﬁrst step will be
to convert the tape contents
[x1x2 . . . xn]
to
[(x1, )(x2, ) . . . (xn, )]
From this point on, the LBA follows the same procedure as the TM in
Theorem 8.8, using the second track of the tape as its working space. It
places S in the ﬁrst space on this track (replacing the ﬁrst ) and begins the
nondeterministic simulation of a derivation in G. As in the previous proof,
the simulation terminates if, having chosen a production α →β, the LBA
is unable to locate an occurrence of α in the current string; at that point,
the original input string is accepted if it matches the string currently in the
second track and rejected if it doesn’t. The new feature of this computation
is that the LBA also rejects the input if some iteration produces a string
that is too long to ﬁt into the n positions of the second track.
Because G is context-sensitive, no string appearing in the derivation
of x ∈L can be longer than x. This means that if the LBA begins with
input x, there is a sequence of moves that will cause it to accept. If x /∈L,
no simulated derivation will be able to produce the string x, and the input
will not be accepted.

280
C H A P T E R 8
Recursively Enumerable Languages
The use of two tape tracks in the proof of Theorem 8.19 allows the LBA to get
by with n squares on the tape instead of 2n. Using k tracks instead of two would
allow the LBA to simulate a computation that would ordinarily require kn squares.
This idea is the origin of the phrase linear-bounded: an LBA can simulate the
computation of a TM, provided the portion of the tape used by the TM is bounded
by some linear function of the input length.
Theorem 8.20
If L ⊆∗is accepted by a linear-bounded automaton M = (Q, , 
, q0, δ),
then there is a context-sensitive grammar G generating L −{}.
Proof
We give only a sketch of the proof, which is similar to the proof of The-
orem 8.14. The simple change described in Example 8.17 to the grammar
in Example 8.12 helps to understand how the unrestricted grammar of
Theorem 8.14 can be modiﬁed to make it context-sensitive.
In the grammar constructed in the proof of that theorem, the only
variables were S, T , left and right parentheses, tape symbols of the TM
that were not input symbols, and . Productions in that grammar such as
ha(b) →b and ha(b) → violate the context-sensitive condition; the
way to salvage the ﬁrst one is to interpret ha(b) as a single variable, and
the second one will be unnecessary because there are no blanks initially
between the end markers on the tape of M.
Because of the tape markers, G may also have variables such as
(a[), (a]), and (a[]), corresponding to situations during the simu-
lation when the current symbol , in a square originally containing a, is
the leftmost, rightmost, or only symbol between the markers, respectively.
Finally, because the tape head of M can move to a tape square contain-
ing [ or ], we will also have variables such as q(a[)L or q(a[])R.
The ﬁrst one signiﬁes that M is in state q, its tape head is on the square
containing [, and the next square contains  but originally contained a.
The ﬁrst portion of a derivation in G produces a string of the form
q0(σ1[σ1)L(σ2σ2) . . . (σn−1σn−1)(σnσn])
or
q0(σ[σ])L
and the productions needed to generate these strings are straightforward.
Because of the more complicated variables, the productions needed to
simulate the LBA’s moves require that we consider more combinations.
We will give the productions for the moves in which the tape head moves
right, and the others are comparable.
Corresponding to the move
δ(p, a) = (q, b, R)

8.4
Context-Sensitive Languages and the Chomsky Hierarchy
281
we have these productions:
p(σ1a)(σ2σ3) →(σ1b)q(σ2σ3)
p(σ1[a)(σ2σ3) →(σ1[b)q(σ2σ3)
p(σ1a)(σ2σ3]) →(σ1b)q(σ2σ3])
p(σ1[a)(σ2σ3) →(σ1[b)q(σ2σ3)
p(σ1a]) →q(σ1b])R
p(σ1[a]) →q(σ1[b])R
for every combination of σ1, σ2 ∈ and σ3 ∈
 ∪{}. In the ﬁrst four
lines both sides contain two variables, and in the last two lines both sides
contain only one variable.
Corresponding to the move
δ(p, [) = (q, [, R)
we have the two productions
p(σ1[σ2)L →q(σ1[σ2)
p(σ1[σ2])L →q(σ1[σ2])
for each σ1 ∈ and each σ2 ∈
 ∪{}.
The productions used in the last portion of the derivation, in which
copies of ha proliferate and then things disappear, are similar to the ones
in the proof of Theorem 8.14, except for the way we interpret them. A
production like
(a)ha(ba) →ha(a)ha(ba)
involves the two variables (a) and ha(ba) on the left side and two on
the right, and one like
ha(a) →a
involves only one variable on the left.
The four classes of languages that we have studied—regular, context-free,
context-sensitive, and recursively enumerable—make up what is often referred to
as the Chomsky hierarchy. Chomsky himself designated the four types as type 3,
type 2, type 1, and type 0, in order from the most restrictive to the most general.
Each level of the hierarchy, which is summarized in Table 8.21, can be characterized
by a class of grammars and by a speciﬁc model of computation.
The phrase type 0 grammar was originally used to describe a grammar in
which the left side of every production is a string of variables. It is not difﬁcult
to see that every unrestricted grammar is equivalent to one with this property
(Exercise 8.27).
The class of recursive languages does not show up explicitly in Table 8.21,
because there is no known way to characterize these languages using grammars.

282
C H A P T E R 8
Recursively Enumerable Languages
Table 8.21
The Chomsky Hierarchy
Languages
Form of Productions
Accepting
Type
(Grammars)
in Grammar
Device
3
Regular
A →aB, A →
Finite
(A, B ∈V , a ∈)
automaton
2
Context-free
A →α
Pushdown
(A ∈V , α ∈(V ∪)∗)
automaton
1
Context-sensitive
α →β
Linear-bounded
(α, β ∈(V ∪)∗, |β| ≥|α|,
automaton
α contains a variable)
0
Recursively
α →β
Turing machine
enumerable
(α, β ∈(V ∪)∗,
(unrestricted)
α contains a variable)
Theorem 8.22, however, allows us to locate this class with respect to the ones in
Table 8.21.
Theorem 8.22
Every context-sensitive language L is recursive.
Proof
Let G be a context-sensitive grammar generating L. Theorem 8.19 says
that there is an LBA M accepting L. According to the remark following
Theorem 8.3, it is sufﬁcient to show that there is a nondeterministic TM
T1 accepting L such that no input string can possibly cause T1 to loop
forever.
We may consider M to be a nondeterministic TM T , which begins
by inserting the markers [ and ] in the squares where they would be
already if we were thinking of T as an LBA. The TM T1 is constructed
as a modiﬁcation of T . T1 also begins by placing the markers on the tape,
although it will not be prohibited from moving its tape head past the right
marker, and it also transforms the tape between the two markers so that it
has two tracks. Just like T , T1 performs the ﬁrst iteration in a simulated
derivation in G by writing S in the ﬁrst position of the second track.
Before the second iteration, however, it moves to the blank portion of the
tape after the right marker and records the string S obtained in the ﬁrst
iteration. In each subsequent iteration, it executes the following steps, the
ﬁrst three of which are the same as those of T :
1. It selects a production α →β in G.
2. It attempts to select an occurrence of α in the current string of the
derivation; if it is unable to, it compares the current string to the input x,
accepts x if they are equal, and rejects x if they are not.

8.5
Not Every Language Is Recursively Enumerable
283
3. If it can select an occurrence of α, it attempts to replace it by β, and rejects
if this would result in a string longer than the input.
4. If it successfully replaces α by β, it compares the new current string with
the strings it has written in the portion of the tape to the right of ], and
rejects if the new one matches one of them; otherwise it writes the new
string after the most recent entry, so that the strings on the tape correspond
exactly to the steps of the derivation so far. Then it returns to the portion of
the tape between the markers for another iteration.
For every string x generated by G, and only for an x of this type,
there is a sequence of moves of T1 that causes x to be accepted. A string
may be rejected at several possible points during the operation of T1: in
step 2 of some iteration, or step 3, or step 4. If x is not accepted, then one
of these three outcomes must eventually occur, because if the iterations
continue to produce strings that are not compared to x and are no longer
than x, eventually one of them will appear a second time.
Returning to the Chomsky hierarchy, we have the inclusions
S3 ⊆S2 ⊆S1 ⊆R ⊆S0
where R is the set of recursive languages, Si is the set of languages of type i for
i = 0 and i = 1, and for i = 2 and i = 3 Si is the set of languages of type i that
don’t contain . (The last inclusion is Theorem 8.2.) We know already that the
ﬁrst two are strict inclusions: there are context-free languages that are not regular
and context-sensitive languages that are not context-free. The last two inclusions
are also strict: the third is discussed in Exercise 9.32, and in Chapter 9, Section
9.1, we will ﬁnd an example of a recursively enumerable language that is not
recursive.
Exercise 8.33 discusses the closure properties of the set of context-sensitive
languages with respect to operations such as union and concatenation. The question
of whether the complement of a CSL is a CSL is much harder and was not answered
until 1987. Szelepcs´enyi and Immerman showed independently in 1987 and 1988
that if L can be accepted by an LBA, then L′ can also be. There are still open
questions concerning context-sensitive languages, such as whether or not every
CSL can be accepted by a deterministic LBA.
8.5 NOT EVERY LANGUAGE IS
RECURSIVELY ENUMERABLE
In this section, we will consider languages over an alphabet , such as {0, 1}, and
Turing machines with input alphabet . We will show there are more languages
than there are Turing machines to accept them, so that there must be many languages
not accepted by any TMs.
Before we start, a word of explanation. In the next chapter, we will actually
produce an example of a language L ⊆{0, 1}∗that is not recursively enumerable.

284
C H A P T E R 8
Recursively Enumerable Languages
Why should we spend time now proving that such languages exist, if we will
soon have an example? There are three reasons: (i) the technique we use is a
remarkable one that caused considerable controversy in mathematical circles when
it was introduced in the nineteenth century; (ii) it is almost exactly the same as
the technique we will use in constructing our ﬁrst example, and it will help in
constructing others; and (iii) the example we ﬁnally obtain will not be an isolated
one, because our discussion in this section will show that most languages are not
recursively enumerable.
The ﬁrst step is to explain how it makes sense to say that one inﬁnite set, the
set of languages over {0, 1}, is larger than another inﬁnite set, the set of TMs with
input alphabet {0, 1}.
The best way to do that will be to formulate two deﬁnitions: what it means for
a set A to be the same size as a set B, and what it means for A to be larger than
B. The most familiar case is when the sets are ﬁnite. The set A = {a, b, c, d} is
the same size as B = {3, 5, 7, 9} and larger than C = {1, 15, 20}. Why? We might
say that A and B both have four elements and C has only three. A more helpful
answer, because it works with inﬁnite sets too, is that A is the same size as B
because the elements of A can be matched up with the elements of B in an exact
correspondence (in other words, there is a bijection f : A →B); and A is larger
than C, because A has a subset the same size as C but A itself is not the same
size as C.
These two approaches to comparing two ﬁnite sets are not really that different.
One of the simplest ways to say what “A has four elements” means is to say that
there is a bijection from A to the set {1, 2, 3, 4}. Counting the elements of A (“one,
two, three, four”) is shorthand for “this is the element that corresponds to 1, this is
the element that corresponds to 2, . . . , this is the element that corresponds to 4.”
In fact, the easiest way to say “A is ﬁnite” is to say that for some natural number
k, there is a bijection from A to the set {1, 2, 3, . . . , k}. If there are bijections
from A to {1, 2, 3, 4} and from B to {1, 2, 3, 4}, and if we just want to say that
A and B have the same size, without worrying about what that size is, we can
dispense with the set {1, 2, 3, 4} and match up the elements of A directly with
those of B.
Deﬁnition 8.23
A Set A of the Same Size as B
or Larger Than B
Two sets A and B, either ﬁnite or inﬁnite, are the same size if there is a
bijection f : A →B. A is larger than B if some subset of A is the same
size as B but A itself is not.
As you might expect, the “same size as” terminology can be used in ways that
are familiar intuitively, because the same-size relation is an equivalence relation
on sets (see Exercise 1.27). For example, “A is the same size as B” and “B is the

8.5
Not Every Language Is Recursively Enumerable
285
same size as A” are interchangeable, and we can speak of several sets as being the
same size. One would hope that the “larger-than” relation is transitive, and it is,
but this is harder to demonstrate; see Exercise 8.48.
The distinguishing feature of any inﬁnite set A is that there is an inexhaustible
supply of elements. There is at least one element a0; there is an element a1 different
from a0; there is another element a2 different from both a0 and a1; and so forth. For
every natural number n, the elements a0, a1, . . . , an do not exhaust the elements
of A, because it is inﬁnite, and so there is an element an+1 different from all
these. Every inﬁnite set A must have an inﬁnite subset A1 = {a0, a1, a2, . . .}, whose
elements can be arranged in a list (so that every element of A1 appears exactly
once in the list).
An inﬁnite set of this form, A1 = {a0, a1, a2, . . .}, is the same size as the set
N of natural numbers, because the function f : N →A1 deﬁned by the formula
f (i) = ai is a bijection. The preceding paragraph allows us to say that every
inﬁnite set is either the same size as, or larger than, N. In other words, N (or
any other set the same size) is the smallest possible inﬁnite set. We call such
a set countably inﬁnite, because as we have seen, starting to count the elements
means starting to list them: the ﬁrst, the second, and so on. We can at least start
counting. We can never ﬁnish, because the set is inﬁnite, but we can go as far as
we want. For every element x in the set, we can continue down the list until we
reach x.
Deﬁnition 8.24
Countably Inﬁnite and Countable Sets
A set A is countably inﬁnite (the same size as N) if there is a bijection
f : N →A, or a list a0, a1, . . . of elements of A such that every element
of A appears exactly once in the list. A is countable if A is either ﬁnite
or countably inﬁnite.
Theorem 8.25
Every inﬁnite set has a countably inﬁnite subset, and every subset of a
countable set is countable.
Proof
The ﬁrst statement was discussed above. If A is countable and B ⊆A, it
is sufﬁcient to consider the case when A and B are both inﬁnite. In this
case, A = {a0, a1, a2, . . .}, and the set I = {i ∈N | ai ∈B} is inﬁnite. If
we let j0 be the smallest number in I, j1 the next smallest, and so on,
then B = {bj0, bj1, . . .}, so that B is countable.
Next we present several examples of countable sets, and we will see in the
very ﬁrst one that inﬁnite sets can seem counter-intuitive or even paradoxical.

286
C H A P T E R 8
Recursively Enumerable Languages
EXAMPLE 8.26
The Set N × N Is Countable
We can describe the set by drawing a two-dimensional array:
(0,0)
(0,1)
(0,2)
(0,3)
. . .
(1,0)
(1,1)
(1,2)
(1,3)
. . .
(2,0)
(2,1)
(2,2)
(2,3)
. . .
(3,0)
(3,1)
(3,2)
(3,3)
. . .
. . .
The ordered pairs in the 0th row, the ones with ﬁrst coordinate 0, are in an inﬁnite list
and form a countably inﬁnite subset. So do the pairs in the ﬁrst row, and for every n ∈N,
so do the pairs in the nth row. There is a countably inﬁnite set of pairwise disjoint subsets
of N × N, each of which is countably inﬁnite. It probably seems obvious that N × N must
be bigger than N.
Obvious, maybe, but not true. As they appear in the array, the elements of N × N are
enough to ﬁll up inﬁnitely many inﬁnite lists. But they can also be arranged in a single list,
as Figure 8.27 is supposed to suggest. The path starts with the ordered pair (0, 0), which
is the only one in which the two coordinates add up to 0. The ﬁrst time the path spirals
back through the array, it hits the ordered pairs (0, 1) and (1, 0), the two in which the
coordinates add up to 1. The ordered pair (i, j) will be hit by the path during its nth pass
back through the array, where n = i + j. Therefore, N × N is countable—the same size
as, not bigger than, N. (When this result was published by Georg Cantor in the 1870s, it
seemed so counterintuitive that many people felt it could not be correct.)
For ﬁnite sets A and B, the assumption that A has a subset the same size as
B and at least one additional element is enough to imply that A is larger than B.
Example 8.26 illustrates why this assumption, or even one that seems much stronger,
is not enough in the case of inﬁnite sets.
(0, 0)
(1, 0)
(2, 0)
(3, 0)
(4, 0)
(0, 1)
(1, 1)
(2, 1)
(3, 1)
(4, 1)
(0, 2)
(1, 2)
(2, 2)
(3, 2)
(4, 2)
(0, 3)
(1, 3)
(2, 3)
(3, 3)
(4, 3)
(0, 4)
(1, 4)
(2, 4)
(3, 4)
(4, 4)
. . .
. . .
. . .
. . .
. . .
Figure 8.27
The set N × N is countable.

8.5
Not Every Language Is Recursively Enumerable
287
EXAMPLE 8.28
A Countable Union of Countable Sets Is Countable
If Si is a countable set for every i ∈N, then
S =
∞

i=0
Si
is countable. The result is a generalization of the previous example and also follows from the
idea underlying Figure 8.27. This time the ordered pair (i, j) in the ﬁgure stands for the jth
element of Si, so that the ith row of the two-dimensional array represents the elements of Si.
We must be a little more careful to specify the elements in the ﬁnal list S = {a0, a1, a2, . . .}
(the list determined by the spiral path), for two reasons: ﬁrst, the jth element of Si may not
exist, because Si is allowed to be ﬁnite; and second, there may be elements of S that belong
to Si for more than one i, and we don’t want any element to appear more than once in the
list. The path in the ﬁgure still works, if we say ﬁrst that a0 is the ﬁrst element of any Si hit
by the path (this makes sense provided not all the Si’s are empty, and of course if they are,
then S is empty); and second that for each n > 0, an is the ﬁrst element hit by the path that
is not already included among a0, a1, . . . , an−1. (Again, if there is no such element, then S
is ﬁnite and therefore countable.)
EXAMPLE 8.29
Languages Are Countable Sets
For a ﬁnite alphabet  (such as {a, b}), the set ∗of all strings over  is countable. This
follows from Example 8.28, because
∗=
∞

i=0
i
and each of the sets i is countable. In fact, each i is ﬁnite, and a way of listing the
elements of ∗that probably seems more natural than using the path in Figure 8.27 is to
list the strings in 0, then the ones in 1, then the ones in 2, and so forth. If strings in
i are listed alphabetically, then the resulting inﬁnite list is just what we get by listing all
the strings in canonical order.
EXAMPLE 8.30
The Set of Turing Machines Is Countable
Let T represent the set of Turing machines. A TM T can be represented by the string
e(T ) ∈{0, 1}∗, and a string can represent at most one TM. Therefore, the resulting function
e from T to {0, 1}∗is one-to-one, and we may think of it as a bijection from T to a subset
of {0, 1}∗. Because {0, 1}∗is countable, every subset is, and we can conclude that T is
countable.
Saying that the set of Turing machines with input alphabet  is countable is approx-
imately the same as saying that the set RE() of recursively enumerable languages over
 is countable. A language L ∈RE() can be accepted by a TM T with input alphabet
, and a TM can accept only one language over its input alphabet. Therefore, since T is
countable, the same argument we have just used shows that RE() is countable. Finally,
as long as we accept the idea that there are only countably many alphabets, we may say
because of Example 8.28 that the set of all recursively enumerable languages is countable.

288
C H A P T E R 8
Recursively Enumerable Languages
Example 8.30 provides us with a portion of the result we stated at the beginning
of this section. The set of TMs with input alphabet {0, 1} is countable, and therefore
a relatively small inﬁnite set. We have not yet shown, however, that there are any
larger ones. There are actually much larger ones (Exercise 8.45 shows that for every
set S, there is a larger set), but to complete the result we want, it will be enough to
show that the set of all languages over {0, 1} is not countable. In other words, the
set of subsets of {0, 1}∗is uncountable. Because {0, 1}∗and N are both countably
inﬁnite, and because two countably inﬁnite sets are identical, size-wise, except for
the notation used to describe the elements, the next example is all we need.
EXAMPLE 8.31
The Set 2N Is Uncountable
We wish to show that there can be no list of subsets of N containing every subset of N.
In other words, every list A0, A1, A2, . . . of subsets of N must leave out at least one.
The question is, without knowing anything about the subsets in the list, how can we
hope to show that there’s at least one subset missing? The answer is provided by an ingenious
argument of Cantor’s called a diagonal argument (we’ll see why shortly). Here is a subset,
constructed from the ones in the list, that cannot possibly be in the list:
A = {i ∈N | i /∈Ai}
The reason that A must be different from Ai for every i is that A and Ai differ because
of the number i, which is in one but not both of the two sets: if i ∈Ai, then by deﬁnition
of A, i does not satisfy the deﬁning condition of A, and so i /∈A; and if i /∈Ai, then (by
deﬁnition of A) i ∈A.
The formula for A, and the idea behind it, are a little cryptic and can seem bafﬂing at
ﬁrst. An illustration may be more helpful. Suppose that the ﬁrst few subsets in the list A0,
A1, . . . are described below.
A0 = {0, 2, 5, 9, . . .}
A1 = {1, 2, 3, 8, 12, . . .}
A2 = {0, 3, 6}
A3 = ∅
A4 = {4}
A5 = {2, 3, 5, 7, 11, . . .}
A6 = {8, 16, 24, . . .}
A7 = N
A8 = {1, 3, 5, 7, 9, . . .}
A9 = {n ∈N | n > 12}
. . .
In some cases the set Ai is not completely deﬁned. (For example, there is nothing in the
formula for A0 to suggest what the smallest element larger than 9 is.) However, we can ﬁnd
the ﬁrst few elements of A, because for each number from 0 through 9, we have enough
information to determine whether it satisﬁes the condition for A.

8.5
Not Every Language Is Recursively Enumerable
289
0 ∈A0, and so 0 /∈A.
1 ∈A1, and so 1 /∈A.
2 /∈A2, and so 2 ∈A.
3 /∈A3, and so 3 ∈A.
4 ∈A4, and so 4 /∈A.
5 ∈A5, and so 5 /∈A.
6 /∈A6, and so 6 ∈A.
7 ∈A7, and so 7 /∈A.
8 /∈A8, and so 8 ∈A.
9 /∈A9, and so 9 ∈A.
So far, we have found ﬁve elements of A: 2, 3, 6, 8, and 9. It’s very likely that there are
subsets in the list whose ﬁrst ﬁve elements are 2, 3, 6, 8, and 9. This might be true of A918,
for example. How can we be sure that A is different from A918? Because the construction
guarantees that A contains the number 918 precisely if A918 does not.
In order to understand what makes this a diagonal argument, let us consider a different
way of visualizing a subset of N. A subset S ⊆N can be described by saying, for each
i ∈N, whether i ∈S. If we use 1 to denote membership and 0 nonmembership, we can
think of S as an inﬁnite sequence of 0’s and 1’s, where the ith entry in the sequence is 1 if
i ∈S and 0 if i /∈S. This is like a Boolean array you might use to represent a subset in a
computer program, except that here the subset might be inﬁnite.
Using this representation, we can visualize the sequence A0, A1, . . . in our example
above as follows:
A0:
1
0
1
0
0
1
0
0
0
1
. . .
A1:
0
1
1
1
0
0
0
0
1
0
. . .
A2:
1
0
0
1
0
0
1
0
0
0
. . .
A3:
0
0
0
0
0
0
0
0
0
0
. . .
A4:
0
0
0
0
1
0
0
0
0
0
. . .
A5:
0
0
1
1
0
1
0
1
0
0
. . .
A6:
0
0
0
0
0
0
0
0
1
0
. . .
A7:
1
1
1
1
1
1
1
1
1
1
. . .
A8:
0
1
0
1
0
1
0
1
0
1
. . .
A9:
0
0
0
0
0
0
0
0
0
0
. . .
. . .
In the ﬁrst row, the 0th, 2nd, 5th, and 9th terms of the sequence are 1, because A0 contains
0, 2, 5, and 9. The underlined terms in this two-dimensional array are the diagonal terms,
and we obtain the sequence corresponding to the set A by reversing these: 0, 0, 1, 1, 0, 0,
1, 0, 1, 1, . . . Each entry we encounter as we make our way down the diagonal allows us
to distinguish the set A from one more of the sets Ai. As before, A = {2, 3, 6, 8, 9, . . .}.
In the ﬁrst paragraph of this section, we promised to show that “there must be
many languages over {0, 1} that cannot be accepted by TMs.” This is stated more
precisely in Theorem 8.32.

290
C H A P T E R 8
Recursively Enumerable Languages
Theorem 8.32
Not all languages are recursively enumerable. In fact, the set of languages
over {0, 1} that are not recursively enumerable is uncountable.
Proof
In Example 8.31 we showed that the set of subsets of N is uncount-
able, and we observed that because {0, 1}∗is the same size as N, it
follows that the set of languages over {0, 1} is uncountable. According to
Example 8.30, the set of recursively enumerable languages over {0, 1} is
countable. The theorem follows from the fact that if T is any countable
subset of an uncountable set S, S −T is uncountable (see Exercise 8.38).
Once we have an example of an uncountable set, there are simple ways of
getting other examples. If A is uncountable and A ⊆B, so that B is either the
same size as A or larger than A, then B is uncountable. Theorem 8.32 uses the
fact that if A is uncountable and C is countable, then A −C is uncountable.
Finally, if A is uncountable and there is a bijection from A to D, then D is
uncountable. However, a diagonal argument or something comparable is the only
method known for proving that a set is uncountable without having another set
that is already known to be. The exercises contain a few more illustrations of
diagonal arguments, and we have already said that we will use them again in the
next chapter.
Finally, one possible source of confusion in this section is worth calling to
your attention. We say informally that a set is uncountable if its elements “cannot
be listed.” Now from Theorem 8.32 we know there is a language L ⊆{0, 1}∗that,
according to Section 8.2, cannot be enumerated (or listed) by a TM. It is important
to understand that these are two very different ideas. The reason the elements of an
uncountable set cannot be listed has nothing to do with what they are—there are
just too many of them. The reason a language L fails to be recursively enumerable
has nothing to do with its size: There is an easy way to list all the elements of
{0, 1}∗, those of L and all the others. We can say that in some sense “there exists”
a list of the elements of L (at least the existence of such a list does not imply
any inherent contradiction), but there is no algorithm to tell us what strings are
in it.
EXERCISES
8.1.
Show that if L1 and L2 are recursive languages, then L1 ∪L2 and L1 ∩L2
are also.
8.2.
Consider modifying the proof of Theorem 8.4 by executing the two TMs
sequentially instead of simultaneously. Given TMs T1 and T2 accepting L1
and L2, respectively, and an input string x, we start by making a second
copy of x. We execute T1 on the second copy; if and when this
computation stops, the tape is erased except for the original input, and T2
is executed on it.

Exercises
291
a. Is this approach feasible for accepting L1 ∪L2, thereby showing that
the union of recursively enumerable languages is recursively
enumerable? Why or why not?
b. Is this approach feasible for accepting L1 ∩L2, thereby showing that
the intersection of recursively enumerable languages is recursively
enumerable? Why or why not?
8.3.
Is the following statement true or false? If L1, L2, . . . are any recursively
enumerable subsets of ∗, then ∞
i=1 Li is recursively enumerable. Give
reasons for your answer.
8.4.
Suppose L1, L2, . . . , Lk form a partition of ∗; in other words, their
union is ∗and any two of them are disjoint. Show that if each Li is
recursively enumerable, then each Li is recursive.
8.5.
Suppose that in Deﬁnition 8.8 we require that a TM enumerating a ﬁnite
language L eventually halt after printing the # following the last element
of L. This makes it a little simpler to prove the last of the four statements
listed in the proof of Theorem 8.9. Show how to resolve any
complications introduced in the proofs of the other three.
8.6.
Describe algorithms to enumerate these sets. (You do not need to discuss
the mechanics of constructing Turing machines to execute the algorithms.)
a. The set of all pairs (n, m) for which n and m are relatively prime
positive integers (“relatively prime” means having no common factor
bigger than 1)
b. The set of all strings over {0, 1} that contain a nonnull substring of the
form www
c. {n ∈N | for some positive integers x, y, and z, xn + yn = zn}
(Answer this question without using Fermat’s theorem, which says that
the only elements of the set are 1 and 2.)
8.7.
In Deﬁnition 8.8, the strings xi appearing on the output tape of T are
required to be distinct. Show that if L can be enumerated in the weaker
sense, in which this requirement is dropped, then L is recursively
enumerable.
8.8.
Suppose L is recursively enumerable but not recursive. Show that if T is a
TM accepting L, there must be inﬁnitely many input strings for which T
loops forever.
8.9.
Suppose L ⊆∗. Show that L is recursively enumerable if and only if
there is a computable partial function from ∗to ∗whose range is L.
8.10.
Suppose L ⊆∗. Show that L is recursively enumerable if and only if
there is a computable partial function from ∗to ∗whose domain is L.
8.11.
Let f : {0, 1}∗→{0, 1}∗be a partial function. Let g(f ), the graph of f ,
be the language {x#f (x) | x ∈{0, 1}∗}. Show that f can be computed by a
Turing machine if and only if the language g(f ) is recursively enumerable.
8.12.
Show that a set L ⊆∗is recursive if and only if there is an increasing
computable total function f : ∗→∗whose range is L. (“Increasing”

292
C H A P T E R 8
Recursively Enumerable Languages
means that if x precedes y with respect to canonical order, then f (x)
precedes f (y).)
8.13.
Show that if L ⊆∗and L is inﬁnite and recursively enumerable, then L
has an inﬁnite recursive subset.
8.14.
†Show that if L ⊆∗and L is inﬁnite and recursive, then L has an
inﬁnite subset that is not recursively enumerable and an inﬁnite subset that
is recursively enumerable but not recursive.
8.15.
Show that if L ⊆∗and L is inﬁnite and recursively enumerable, then L
has an inﬁnite subset that is not recursively enumerable and an inﬁnite
subset that is recursively enumerable but not recursive.
8.16.
Canonical order is a speciﬁc way of ordering the strings in ∗, and its use
in Theorem 8.9 is somewhat arbitrary. By an ordering of ∗, we mean
simply a bijection from the set of natural numbers to ∗. For any such
bijection f , and any language L ⊆∗, let us say that “L can be
enumerated in order f ” means that there is a TM T enumerating L and
for every i, the string f (i) is the ith string listed by T . For an arbitrary
ordering f of ∗, let E(f ) be the statement “For any L ⊆∗, L is
recursive if and only if it can be enumerated in order f .” For exactly
which types of orderings f is E(f ) true? Prove your answer.
8.17.
In each case below, describe the language generated by the unrestricted
grammar with the given productions. The symbols a, b, and c are
terminals, and all other symbols are variables.
a.
S →ABCS | ABC
AB →BA
AC →CA
BC →CB
BA →AB
CA →AC
CB →BC
A →a
B →b
C →c
b. S →LaR
L →LD | LT | 
Da →aaD
T a →aaaT
DR →R
T R →R
R →
c.
S →LaMR
L →LT | E
T a →aT
T M →aaMT
T R →aMR
Ea →aE
EM →E
ER →
8.18.
Consider the unrestricted grammar with the following productions.
S →T D1D2
T →ABCT | 
AB →BA
BA →AB
CA →AC
CB →BC
CD1 →D1C
CD2 →D2a
BD1 →D1b
A →a
D1 →
D2 →
a. Describe the language generated by this grammar.
b. Find a single production that could be substituted for BD1 →D1b so
that the resulting language would be
{xan | n ≥0, |x| = 2n, and na(x) = nb(x) = n}

Exercises
293
8.19.
For each of the following languages, ﬁnd an unrestricted grammar that
generates the language.
a. {anbnanbn | n ≥0}
b. {anxbn | n ≥0, x ∈{a, b}∗, |x| = n}
c. {sss | s ∈{a, b}∗}
d. {ssrs | s ∈{a, b}∗}
8.20.
For each of the following languages, ﬁnd an unrestricted grammar that
generates the language.
a. {x ∈{a, b, c}∗| na(x) < nb(x) and na(x) < nc(x)}
b.
†{x ∈{a, b, c}∗| na(x) < nb(x) < 2nc(x)}
c. {an | n = j(j + 1)/2 for some j ≥1} (Suggestion: if a string has j
groups of a’s, the ith group containing i a’s, then you can create j + 1
groups by adding an a to each of the j groups and adding a single
extra a at the beginning.)
8.21.
Suppose G is an unrestricted grammar with start symbol T that generates
the language L ⊆{a, b}∗. In each part below, another unrestricted
grammar is described. Say (in terms of L) what language it generates.
a. The grammar containing all the variables and all the productions of G,
two additional variables S (the start variable) and E, and the additional
productions
S →ET
E →
Ea →E
Eb →E
b. The grammar containing all the variables and all the productions of G,
four additional variables S (the start variable), F, R, and E, and the
additional productions
S →FT R
Fa →aF
Fb →bF
F →E
Ea →E
Eb →E
ER →
8.22.
Figure 7.6 shows the transition diagram for a TM accepting XX = {xx |
x ∈{a, b}∗}. In the grammar obtained from this TM as in the proof of
Theorem 8.14, give a derivation for the string abab.
8.23.
Find a context-sensitive grammar generating the language in Example 8.11.
8.24.
Find a context-sensitive grammar generating the language XX −{} =
{xx | x ∈{a, b}∗and x ̸= }.
8.25.
Example 8.17 provides a context-sensitive grammar that generates
{anbncn | n > 0}. Find another CSG generating this language that has only
three variables and six productions. Suggestion: omit the variables A and
A.
8.26.
Find CSGs equivalent to the grammars in Exercise 8.17, parts (b) and (c).
8.27.
Show that if L is any recursively enumerable language, then L can be
generated by a grammar in which the left side of every production is a
string of one or more variables.

294
C H A P T E R 8
Recursively Enumerable Languages
8.28.
Show by examples that the constructions in the proof of Theorem 4.9 do
not work to show that the class of recursively enumerable languages is
closed under concatenation and Kleene ∗, or that the class of CSLs is
closed under concatenation.
8.29.
Show that if for some positive integer k, there is a nondeterministic TM
accepting L such that for any input x, the tape head never moves past
square k|x|, then L −{} is a context-sensitive language.
8.30.
Show that if G is an unrestricted grammar generating L, and there is an
integer k such that for any x ∈L, every string appearing in a derivation of
x has length ≤k|x|, then L is recursive.
8.31.
In the proof of Theorem 8.20, the CSG productions corresponding to an
LBA move of the form δ(p, a) = (q, b, R) are given. Give the productions
corresponding to the move δ(p, a) = (q, b, L) and those corresponding to
the move δ(p, a) = (q, b, S).
8.32.
†Suppose G is a context-sensitive grammar. In other words, for every
production α →β of G, |β| ≥|α|. Show that there is a grammar G′, with
L(G′) = L(G), in which every production is of the form
αAβ →αXβ
where A is a variable and α, β, and X are strings of variables and/or
terminals, with X not null.
8.33.
Use the LBA characterization of context-sensitive languages to show that
the class of CSLs is closed under union, intersection, and concatenation,
and that if L is a CSL, then so is LL∗.
8.34.
†Suppose G1 and G2 are unrestricted grammars generating L1 and L2
respectively.
a. Using G1 and G2, describe an unrestricted grammar generating L1L2.
b. Using G1, describe an unrestricted grammar generating L∗
1.
8.35.
Show that every subset of a countable set is countable.
8.36.
By deﬁnition, a set S is ﬁnite if for some natural number n, there is a
bijection from S to {i ∈N | 1 ≤i ≤n}. An inﬁnite set is a set that is not
ﬁnite. Show that a set S is inﬁnite if and only if there is a bijection from S
to some proper subset of S. (Theorem 8.25 might be helpful.)
8.37.
Saying that a property is preserved under bijection means that if a set S
has the property and f : S →T is a bijection, then T also has the
property. Show that both countability and uncountability are preserved
under bijection.
8.38.
Show that if S is uncountable and T is countable, then S −T is
uncountable. Suggestion: proof by contradiction.
8.39.
Let Q be the set of all rational numbers, or fractions, negative as well as
nonnegative. Show that Q is countable by describing explicitly a bijection
from N to Q—in other words, a way of creating a list of rational numbers
that contains every rational number exactly once.

Exercises
295
8.40.
For each part below, use a diagonal argument to show that the given set is
uncountable.
a. The set S of inﬁnite sequences of 0’s and 1’s. This part is essentially
done for you, in Example 8.31, but write it out carefully.
b. The set I = {x ∈R | 0 ≤x < 1}. Do this by using the one-to-one
correspondence between elements of I and inﬁnite decimal expansions
0.a0a1 . . . that do not end with an inﬁnite sequence of consecutive 9’s.
Every element of I has exactly one such expansion, and every such
expansion represents exactly one element of I. For example, 0.5 could
be written using the inﬁnite decimal expansion 0.5000 . . . or the
expansion 0.49999 . . . , but only the ﬁrst is relevant in this problem,
because the second ends with an inﬁnite sequence of 9’s. In your
proof, you just have to make sure that the decimal expansion you
construct, in order to get a number not in the given list, doesn’t end
with an inﬁnite sequence of 9’s.
8.41.
For each case below, determine whether the given set is countable or
uncountable. Prove your answer.
a. The set of all three-element subsets of N
b. The set of all ﬁnite subsets of N
c. The set of all partitions of N into a ﬁnite number of subsets A1, A2,
. . . , Ak (such that any two are disjoint and k
i=1 Ai = N). The order
of the Ai’s is irrelevant—i.e., two partitions A1, . . . , Ak and B1, . . . ,
Bk are considered identical if each Ai is Bj for some j.
d. The set of all functions from N to {0, 1}
e. The set of all functions from {0, 1} to N
f. The set of all functions from N to N
g. The set of all nondecreasing functions from N to N
h. The set of all regular languages over {0, 1}
i.
The set of all context-free languages over {0, 1}
8.42.
We know that 2N is uncountable. Give an example of a set S ⊆2N such
that both S and 2N −S are uncountable.
8.43.
Show that the set of languages L over {0, 1} such that neither L nor L′ is
recursively enumerable is uncountable.
8.44.
This exercise is taken from Dowling (1989). It has to do with an actual
computer, which is assumed to use some ﬁxed operating system under
which all its programs run. A “program” can be thought of as a function
that takes one string as input and produces another string as output. On the
other hand, a program written in a speciﬁc language can be thought of as a
string itself.
By deﬁnition, a program P spreads a virus on input x if running P with
input x causes the operating system to be altered. It is safe on input x if
this doesn’t happen, and it is safe if it is safe on every input string. A
virus tester is a program IsSafe that when given the input P x, where P is

296
C H A P T E R 8
Recursively Enumerable Languages
a program and x is a string, produces the output “YES” if P is safe on
input x and “NO” otherwise. (We make the assumption that in a string of
the form P x, there is no ambiguity as to where the program P stops.)
Prove that if there is the actual possibility of a virus—i.e., there is a
program and an input that would cause the operating system to be
altered—then there can be no virus tester that is both safe and correct.
Hint: Suppose there is such a virus tester IsSafe. Then it is possible to
write a program D (for “diagonal”) that operates as follows when given a
program P as input. It evaluates IsSafe(PP); if the result is “NO,” it prints
“XXX”, and otherwise it alters the operating system. Now consider what
D does on input D.
8.45.
The two parts of this exercise show that for every set S (not necessarily
countable), 2S is larger than S.
a. For every S, describe a simple bijection from S to a subset of 2S.
b. Show that for every S, there is no bijection from S to 2S. (You can
copy the proof in Example 8.31, as long as you avoid trying to list the
elements of S or making any reference to the countability of S.)
8.46.
†For each part below, use a diagonal argument to show that the given set
is uncountable.
a. The set I in Exercise 8.40b, but this time using the one-to-one
correspondence between elements of I and inﬁnite binary expansions
0.a0a1 . . . (where each ai is 0 or 1) that do not end in an inﬁnite
sequence of 1’s. This is slightly harder than Exercise 8.40b, because if
x0, x1, . . . is a list of elements of I, and xi corresponds to the inﬁnite
binary expansion 0.ai,0ai,1 . . . , it may turn out that the binary
expansion 0.a0a1 . . . constructed using the diagonal argument does end
with an inﬁnite sequence of 1’s. You can get around this problem by
constructing a = 0.a0a1 . . . that is different from each xi because
a2i+1 ̸= ai,2i+1.
b. The set of bijections from N to N. If f0, f1, . . . is a list of bijections,
you can construct a bijection f that doesn’t appear in the list, by
making f (2n) different from fn(2n) and specifying both f (2n) and
f (2n + 1) so that f is guaranteed to be a bijection.
8.47.
In each case below, determine whether the given set is countable or
uncountable. Prove your answer.
a. The set of all real numbers that are roots of integer polynomials; in
other words, the set of real numbers x such that, for some nonnegative
integer n and some integers a0, a1, . . . , an, x is a solution to the
equation
a0 + a1x + a2x2 + . . . anxn = 0
b. The set of all nondecreasing functions from N to N.

Exercises
297
c. The set of all functions from N to N whose range is ﬁnite.
d. The set of all nondecreasing functions from N to N whose range is
ﬁnite (i.e, all “step” functions).
e. The set of all periodic functions from N to N. (A function f : N →
N is periodic if, for some positive integer Pf , f (x + Pf ) = f (x) for
every x.)
f. The set of all eventually constant functions from N to N. (A function
f : N →N is eventually constant if, for some C and for some N,
f (x) = C for every x ≥N.)
8.48.
†We have said that a set A is larger than a set B if there is a bijection
from B to a subset of A, but no bijection from B to A, and we have
proceeded to use this terminology much as we use the < relation on the
set of numbers. What we have not done is to show that this relation
satisﬁes the same essential properties that < does.
a. The Schr¨oder-Bernstein theorem asserts that if A and B are sets and
there are bijections f from A to a subset of B and g from B to a
subset of A, then there is a bijection from A to B. Prove this statement.
Here is a suggested approach. An ancestor of a ∈A is a point b ∈B
such that g(b) = a, or a point a1 ∈A such that g(f (a1)) = a, or a
point b1 ∈B such that g(f (g(b1))) = a, etc. In other words, an
ancestor of a ∈A is a point x in A or B such that by starting at x
and continuing to evaluate the two functions alternately, we arrive at
a. If g(f (g(b))) = a and b has no ancestors, for example, we will say
that a has the three ancestors f (g(b)), g(b), and b. Note that we
describe the number of ancestors as three, even in the case that
f (g(b)) = b or g(b) = a. In this way, A can be partitioned into three
sets A0, A1, and A∞; A0 is the set of elements of A having an even
(ﬁnite) number of ancestors, A1 is the set of elements having an odd
number, and A∞is the set of points having an inﬁnite number of
ancestors. Ancestors of elements of B are deﬁned the same way,
and we can partition B similarly into B0, B1, and B∞. Show that
there are bijections from A0 to B1, from A1 to B0, and from A∞
to B∞.
b. Show that the “larger than” relation on sets is transitive. In other
words, if there is a bijection from A to a subset of B but none from A
to B, and a bijection from B to a subset of C but none from B to C,
then there is a bijection from A to a subset of C but none from A
to C.
c. Show that the “larger-than” relation is asymmetric. In other words, if A
is larger than B, then B cannot be larger than A.
d. Show that countable sets are the smallest inﬁnite sets in both possible
senses: Not only are uncountable sets larger than countable sets, but no
inﬁnite set can be smaller than a countable set.

298
C H A P T E R 8
Recursively Enumerable Languages
8.49.
Let I be the unit interval [0, 1], the set of real numbers between 0 and 1.
Let S = I × I, the unit square. Use the Schr¨oder-Bernstein theorem (see
Exercise 8.48) to show that there is a bijection from I to S. One way is to
use inﬁnite decimal expansions as in Exercise 8.40b.
8.50.
Show that A is bigger than B if and only if there is a one-to-one function
from B to A but none from A to B. (One way is easy; for the other,
Exercise 8.48 will be helpful.)

299
C
H
A
P
T
E
R
9
Undecidable Problems
U
sing a diagonal argument that is logically almost identical to the one at the
end of Chapter 8, we can produce an example of a language not accepted
by any Turing machine. The way the language is deﬁned also makes it easy to
formulate a decision problem (a general question, each instance of which has a
yes-or-no answer) that is undecidable—that is, for which no algorithm can produce
the correct answer in every instance.
In general, we have a choice between considering whether a decision problem
is decidable and considering whether the language corresponding to it, the one
containing strings that represent “yes-instances” of the problem, is recursive. Using
either approach, once we have one example of an undecidable problem P , we
can obtain others by constructing reductions from P . In this chapter we describe
several other undecidable problems involving Turing machines, including the well-
known halting problem, as well as a general method for obtaining a large class of
unsolvable problems. In the last two sections of this chapter, we consider another
well-known undecidable problem, Post’s correspondence problem. Constructing
reductions from it is one of two ways we use to show that several simple-to-state
questions about context-free grammars are also undecidable.
9.1 A LANGUAGE THAT CAN’T BE
ACCEPTED, AND A PROBLEM
THAT CAN’T BE DECIDED
We know from Chapter 8, Section 8.5, that there are uncountably many languages,
too many for them all to be recursively enumerable. This conclusion by itself
doesn’t make it obvious that we will be able to identify and describe a non-
recursively-enumerable language. The set of descriptions, which are strings over
some alphabet , is countable, the same size as the set of recursively enumerable
languages; maybe whenever there is a precise ﬁnite description of a language L,
there is an algorithm to accept L.

300
C H A P T E R 9
Undecidable Problems
As it turns out, not only can we describe a language that is not recursively
enumerable, but we can do it by using the same diagonal argument that we used
for the uncountability result. We showed that for every list A0, A1, . . . of subsets
of N, there is a subset A of N that is not in the list. The fact that the sets Ai were
in a list was not crucial; here are the crucial steps.
1.
We started with a collection of subsets Ai of N, each one associated with a
speciﬁc element of N (namely, i).
2.
We deﬁned another subset A, containing the elements i of N that do not
belong to the subset Ai associated with i.
The conclusion was that for each i, A ̸= Ai, because i ∈A ⇔i /∈Ai.
Now we want to ﬁnd a language L ⊆{0, 1}∗that cannot be accepted by a
Turing machine—in other words, a language that is different from L(T ), for every
Turing machine T with input alphabet {0, 1}. The string e(T ) introduced in Section
7.8 is “associated with” L(T ), just as i is associated with Ai, and we can repeat
the argument as follows:
1.
We have a collection of subsets L(T ) of {0, 1}∗, each one associated with a
speciﬁc element of {0, 1}∗(namely, e(T )).
2.
We deﬁne another subset L, containing the elements e(T ) of {0, 1}∗that do
not belong to the subset L(T ) associated with e(T ).
The conclusion this time is that for each T , L ̸= L(T ), because e(T ) ∈L ⇔e(T )
/∈L(T ).
Deﬁnition 9.1
The Languages NSA and SA
Let
NSA = {e(T ) | T is a TM, and e(T ) /∈L(T )}
SA = {e(T ) | T is a TM, and e(T ) ∈L(T )}
(NSA and SA stand for “non-self-accepting” and “self-accepting.”)
Theorem 9.2
The language NSA is not recursively enumerable. The language SA is
recursively enumerable but not recursive.
Proof
The language NSA is the language L described above, and the conclusion
we obtained is that it is not recursively enumerable.
Let E be the language {e(T ) | T is a TM}. It follows from Theorem
7.36 that E is recursive. (This is one of the crucial features we identiﬁed

9.1
A Language That Can’t Be Accepted, and a Problem That Can’t Be Decided
301
in Section 7.8 for any encoding function e—there should be an algorithm
to determine, for any string z, whether z = e(T ) for a TM T .)
For every string z of 0’s and 1’s, exactly one of these three statements
is true: (i) z does not represent a TM; (ii) z represents a TM that accepts
z; (iii) z represents a TM that does not accept z. In other words,
{0, 1}∗= NSA ∪SA ∪E′
and the three sets on the right are disjoint. Therefore,
NSA = (SA ∪E′)′ = SA′ ∩E
If SA were recursive, then SA′ would be recursive, by Theorem 8.6, and
then NSA would be recursive, by Theorem 8.5. But NSA is not recursively
enumerable, and so by Theorem 8.2 it can’t be recursive.
To ﬁnish the proof, we need to show that SA is recursively enu-
merable. Here is an algorithm to accept SA. Given a string x ∈{0, 1}∗,
determine whether x ∈E. If not, then x /∈SA. If x ∈E, then x = e(T )
for some TM T , and as we observed in Section 7.8, we can reconstruct
T from the string e(T ). Then execute T on the input x, and accept x
precisely if T accepts x.
The statement of the theorem says that there is no algorithm to determine
whether a given string represents a TM that accepts its own encoding. Because we
can determine whether a string represents a TM, there is no algorithm to determine
whether a given TM accepts its own encoding. There is one aspect of this result
that deserves a little more discussion.
It may seem from the statement that when we are considering strings that a
TM T might or might not accept, the string e(T ) presents particular problems—
perhaps for most strings x we can easily decide whether T accepts x, but for
some reason, deciding whether T accepts e(T ) is more challenging. However,
you can see if you go back to the preliminary discussion that this is not the
right interpretation. All we needed for the diagonal argument was a string “asso-
ciated with” T . The string e(T ) was an obvious choice, but many other choices
would have worked just as well. For example, we could list all TMs by arrang-
ing the strings e(T ) in canonical order, and we could list all elements of {0, 1}∗
in canonical order, and we could associate the ith string with the ith Turing
machine.
The appropriate conclusion seems to be, not so much that it’s hard to decide
what a Turing machine T does with the string e(T ), but that it’s hard to answer
questions about Turing machines and the strings they accept. This is the way to
understand the signiﬁcance of Theorem 9.2. We can probably agree that for most
Turing machines T , whether T happens to accept its own encoding is not very
interesting, or at least not very important. The general question of whether an
arbitrary T accepts e(T ) is interesting, not because the answer is, but because even
though the question is easy to state, there is no algorithm to answer it; no algorithm

302
C H A P T E R 9
Undecidable Problems
can provide the right answer for every T . In the next section we will be able to
use this result to ﬁnd other, more useful problems that are also “undecidable”
in the same way. So, even though you may not feel that this particular question
provides an example of the following principle, we will have better examples
soon.
There are precise, easy-to-formulate questions for which it would be
useful to have algorithms to answer them, but for which there are no
such algorithms.
We can approach other decision problems the same way we approached this
one. For a decision problem, we use the term “yes-instances” to refer to instances of
the problem for which the answers are yes, and “no-instances” for the others. The
three languages SA, NSA, and E′ contain, respectively, the strings that represent
yes-instances, the strings that represent no-instances, and the strings that don’t
represent instances at all, of the decision problem
Self-Accepting: Given a TM T , does T accept the string e(T )?
In discussing a general decision problem P , we start the same way, by choosing
an encoding function e that allows us to represent instances I by strings e(I) over
some alphabet . We give the names Y(P ) and N(P ) to the sets of strings in
∗that represent yes-instances and no-instances, respectively, of P . If E(P ) =
Y(P ) ∪N(P ), then just as in our example, we have the third subset E(P )′ of
strings in ∗that don’t represent instances of P .
The requirements for a function to encode instances of P are the same as those
for a function to encode Turing machines. The function must be one-to-one, so that
a string can’t represent more than one instance; it must be possible to decode a
string e(I) and recover the instance I; and there must be an algorithm to decide
whether a given string in ∗represents an instance of P (in other words, the
language E(P ) should be recursive). We will refer to a function satisfying these
conditions as a reasonable encoding of instances of P .
Solving, or deciding, a decision problem P means starting with an arbitrary
instance I of P and deciding whether I is a yes-instance or a no-instance. The
way that a Turing machine must decide P , however, is to start with a string,
and decide whether the string represents a yes-instance of P , represents a no-
instance, or does not represent an instance. It sounds at ﬁrst as though T ’s job
is harder: before it can attempt to decide whether an instance is a yes-instance,
it must decide whether a string represents an instance at all. However, the extra
work simply reﬂects the fact that a TM must work with an input string. As long
as the encoding function is reasonable, there is an algorithm for deciding P if
and only if there is a Turing machine that decides Y(P ). Furthermore, whether
this statement is true does not depend on which encoding function we use to
deﬁne Y(P ); if one reasonable encoding makes the statement true, then every one
does.

9.1
A Language That Can’t Be Accepted, and a Problem That Can’t Be Decided
303
Deﬁnition 9.3
Decidable Problems
If P is a decision problem, and e is a reasonable encoding of instances of
P over the alphabet , we say that P is decidable if Y(P ) = {e(I) | I
is a yes-instance of P } is a recursive language.
Theorem 9.4
The decision problem Self-Accepting is undecidable.
Proof
Because SA = Y(Self-Accepting), the statement follows immediately from
Theorem 9.2.
A decision problem has the general form
Given
, is it true that
?
For every decision problem P , there is a complementary problem P ′, obtained
by changing “true” to “false” in the statement of the problem. For example, the
complementary problem to Self-Accepting is
Non-Self-Accepting: Given a TM T , does T fail to accept e(T )?
For every P , yes-instances of P are no-instances of P ′, and vice-versa. P and
P ′ are essentially just two ways of asking the same question, and the result in
Theorem 9.5 is not surprising.
Theorem 9.5
For every decision problem P , P is decidable if and only if the comple-
mentary problem P ′ is decidable.
Proof
For exactly the same reasons as in the proof of Theorem 9.2, we have
N(P ) = Y(P )′ ∩E(P )
E(P ) is assumed to be recursive. If Y(P ) is recursive, then so is Y(P )′,
and therefore so is N(P ) = Y(P ′). The other direction is similar.
Theorem 9.5 helps make it clear why Y(P ) is required to be recursive, not
just recursively enumerable, if we are to say that P is decidable. In our example,
although SA is recursively enumerable and NSA is not, Self-Accepting is no closer
to being decidable than its complementary problem is. An algorithm attempting to
solve either one would eventually hit an instance I that it couldn’t handle; the only
difference is that if it were trying to solve Self-Accepting, I would be a no-instance,
and if it were trying to solve Non-Self-Accepting, I would be a yes-instance.

304
C H A P T E R 9
Undecidable Problems
Finally, you should remember that even for an undecidable problem, there may
be many instances for which answers are easy to obtain. If T is a TM that rejects
every input on the ﬁrst move, then T obviously does not accept the string e(T );
and there are equally trivial TMs that obviously do accept their own encodings.
What makes Self-Accepting undecidable is that there is no algorithm guaranteed to
produce the right answer for every instance.
9.2 REDUCTIONS AND THE HALTING
PROBLEM
Very often in computer science, one problem can be solved by reducing it to
another, simpler one. Recursive algorithms usually rely on the idea of a reduction
in size: starting with an instance of size n and reducing it to a smaller instance of
the same problem. For example, in the binary search algorithm, a single comparison
reduces the number of elements to be searched from n to n/2.
In this section we consider reducing one decision problem to a different deci-
sion problem. Suppose we want to determine whether an NFA M accepts a string x.
The nondeterminism in M means that we can’t simply “run M on input x and see
what happens.” However, we have an algorithm (see the proofs of Theorems 3.17
and 3.18) that allows us to ﬁnd an ordinary FA M1 accepting the same strings as
M. The algorithm allows us to reduce the original problem to the simpler problem
of determining whether an FA accepts a given string.
Here’s another example. Let P1 be the problem: Given two FAs M1 and M2,
is L(M1) ⊆L(M2)? Once we realize that for two sets A and B, A ⊆B if and
only if A −B = ∅, we can use the algorithm described in the proof of Theorem
2.15 to reduce P1 to the problem P2: Given an FA M, is L(M) = ∅? We use the
algorithm to ﬁnd an FA M such that L(M) = L(M1) −L(M2), and the answer to
the original question (is L(M1) ⊆L(M2)?) is the same as the answer to the second
question (is L(M) = ∅?).
In both these examples, we started with an instance I of the original problem,
and we applied an algorithm to obtain an instance F(I) of the second problem
having the same answer. In the ﬁrst example, I is a pair (M, x), where M is an
NFA and x is a string; F(I) is a pair (M1, y), where M1 is an FA and y is a string.
M1 is chosen to be an FA accepting the same language as M, and y is chosen
simply to be x. In the second example, I is a pair (M1, M2) of FAs and F(I) is
the single FA M constructed such that L(M) = L(M1) −L(M2).
The two crucial features in a reduction are: First, for every instance I of the
ﬁrst problem that we start with, we must be able to obtain the instance F(I) of the
second problem algorithmically; and second, the answer to the second question for
the instance F(I) must be the same as the answer to the original question for I.
If the two problems P1 and P2 happen to be the membership problems for
languages L1 ⊆∗
1 and L2 ⊆∗
2, respectively, then an instance of P1 is a string
x ∈∗
1, and an instance of P2 is a string y ∈∗
2. In this case, a reduction from
P1 to P2 means an algorithm to compute for each x ∈∗
1 a string F(x) ∈∗
2 such
that for every x, x ∈L1 if and only if F(x) ∈L2. Here, because instances are

9.2
Reductions and the Halting Problem
305
strings, “algorithm” literally means “Turing machine”—the function F should be
computable. We refer to F as a reduction from L1 to L2; it is sometimes called a
many-one reduction.
Deﬁnition 9.6
Reducing One Decision Problem to Another,
and Reducing One Language to Another
Suppose P1 and P2 are decision problems. We say P1 is reducible to P2
(P1 ≤P2) if there is an algorithm that ﬁnds, for an arbitrary instance I of
P1, an instance F(I) of P2, such that for every I, the answers for the two
instances are the same, or I is a yes-instance of P1 if and only if F(I) is
a yes-instance of P2.
If L1 and L2 are languages over alphabets 1 and 2, respectively,
we say L1 is reducible to L2 (L1 ≤L2) if there is a Turing-computable
function f : ∗
1 →∗
2 such that for every x ∈∗
1,
x ∈L1 if and only if f (x) ∈L2
Theorem 9.7
Suppose L1 ⊆∗
1, L2 ⊆∗
2, and L1 ≤L2. If L2 is recursive, then L1 is
recursive.
Suppose P1 and P2 are decision problems, and P1 ≤P2. If P2 is
decidable, then P1 is decidable.
Proof
To prove the ﬁrst statement, let f : ∗
1 →∗
2 be a function that reduces
L1 to L2, and let Tf be a TM computing f . Let T2 be another TM that
decides L2. Let T be the composite TM Tf T2. On input x, T ﬁrst computes
f (x) and then decides, by returning output 1 or 0, whether f (x) ∈L2.
Because f is a reduction, this is the same as deciding whether x ∈L1.
Therefore, T decides the language L1.
Suppose now that F is an algorithm reducing P1 to P2, and that
we have encoding functions e1 and e2 for the instances of P1 and P2,
respectively. Our assumption is that Y(P2), the subset of ∗
2 containing
the strings that represent yes-instances of P2, is recursive, and we wish to
show that Y(P1) is also recursive.
For an arbitrary string x ∈∗
1, we can decide, assuming that e1 is a
reasonable encoding, whether x represents an instance of P1. If not, then
x /∈Y(P1). If x ∈E(P1), then x represents an instance I of P1, and we
have this sequence of equivalences:
x = e1(I) ∈Y(P1) ⇔I is a yes-instance of P1
⇔F(I) is a yes-instance of P2
⇔e2(F(I)) ∈Y(P2)

306
C H A P T E R 9
Undecidable Problems
The statement in the second line is true because F is a reduction from
P1 to P2. Because Y(P2) is recursive, we have an algorithm to decide
whether e2(F(I)) ∈Y(P2), and so we have an algorithm to decide whether
x ∈Y(P1).
In discussing the decidability of a problem P , we are identifying P with the
language Y(P ); deciding whether an instance I of P is a yes-instance is essentially
the same problem as deciding whether a string x represents a yes-instance of P .
It would seem reasonable that the statement P1 ≤P2 should be equivalent to the
statement Y(P1) ≤Y(P2). There are slight technical complications, because the
ﬁrst statement involves only instances of the two problems, whereas the second
involves all strings over the appropriate alphabets, whether or not they represent
instances of P1 or P2. However, with minor qualiﬁcations, this equivalence does
hold (see Exercises 9.16–9.18). When we discuss reductions, we will usually use
the slightly more informal deﬁnition involving “algorithms” unless instances of the
decision problems are actually strings.
The most obvious reason for discussing a reduction from P1 to P2 is that if we
have one, and if we can decide P2, then we have a way of deciding P1. However,
it is important to separate the idea of reducing P1 to P2 from the question of
whether either problem can be decided. The statement “if P2 is decidable, then
P1 is decidable” is equivalent to the statement “if P1 is not decidable, then P2 is
not decidable.” In this chapter, we will use Theorem 9.7 primarily to obtain more
examples of undecidable problems, or nonrecursive languages.
Let us consider two more decision problems: the general membership problem
for recursively enumerable languages, and a problem usually referred to as the
halting problem.
Accepts: Given a TM T and a string w, is w ∈L(T )?
Halts: Given a TM T and a string w, does T halt on input w?
In both cases, instances of the problem are ordered pairs (T, w), where T is a
TM and w is a string. Remember why these problems don’t both have trivial deci-
sion algorithms: We can’t just say “Execute T on input w and see what happens,”
because T might loop forever on input w.
Theorem 9.8
Both Accepts and Halts are undecidable.
Proof
By Theorems 9.4 and 9.7, in order to show Accepts is undecidable it is
sufﬁcient to show
Self-Accepting ≤Accepts
An instance of Self-Accepting is a TM T . A reduction to Accepts means
ﬁnding a pair F(T ) = (T1, y) such that T accepts e(T ) if and only if

9.2
Reductions and the Halting Problem
307
T1 accepts y. Letting T1 = T and y = e(T ) gives us such a pair, and
F(T ) = (T, e(T )) can be obtained algorithmically from T ; therefore, F
is a reduction from Self-Accepting to Accepts.
In order to prove that Halts is undecidable, we will show that
Accepts ≤Halts
Suppose we start with an arbitrary instance (T, x) of Accepts where T is
a TM and x is a string. We must describe the pair F(T, x) = (T1, y), an
instance of Halts such that the two answers are the same: T accepts x if
and only if T1 halts on input y.
How can we do it? It seems clear that T1 should somehow be deﬁned
in terms of T (and maybe in terms of x as well?), and y should be deﬁned
in terms of x (and maybe somehow depend on T as well?). We cannot
simply say “let T1 be T and let y = x,” because this choice doesn’t force
the two answers to be the same. (Obviously, if T accepts x, then T halts
on x, but T could also halt on x by rejecting x.)
Let us choose y to be x, since there don’t seem to be any natural
alternatives. Now what we want is a TM T1 such that for every x, T
accepts x if and only if T1 halts on x. A reformulation of this statement
is: (i) if T accepts x, then T1 halts on x, and (ii) if T doesn’t accept x,
then T1 doesn’t halt on x. Saying it this way may make it easier to see
what to do. Statement (i) will be true if T1 makes the same moves as T for
any input that is accepted. Statement (ii) will also be true for every string
on which T never halts, if T1 makes the same moves as T on every such
string. The only strings for which T1 needs to do something different from
T are the strings that cause T to reject; for these strings x, we want T1
not to halt on x. Therefore, we can construct T1 so that it makes the same
moves as T , except that every move of T to the reject state is replaced by
a sequence of moves causing T1 to loop forever. Replacing every move
of the form δ(p, a) = (hr, b, D) by the move δ(p, a) = (p, a, S) almost
accomplishes this, because if T1 ever arrives in the state p with the current
symbol a, then instead of rejecting, it will continue making moves that
leave the state, symbol, and tape head position unchanged.
The other way T might reject, however, is to try to move its tape head
left, starting in square 0. This case requires one additional modiﬁcation
to T . T1 will begin by inserting a new symbol $ in square 0, moving
everything else one square to the right, and then move to q0 with the tape
head in square 1. There will now be additional moves δ(q, $) = (q, $, S)
for every state q, such that if T ever tries to move its tape head off the
tape, T1 enters an inﬁnite loop with its tape head on square 0.
If T1 is the TM T with these modiﬁcations incorporated, then T1 will
accept every string T does, and T1 will loop forever on every string T
does not accept. Therefore, the function F deﬁned by F(T, x) = (T1, x)
is a reduction of Accepts to Halts.

308
C H A P T E R 9
Undecidable Problems
Often you can test for inﬁnite loops in a computer program by just verifying
that you haven’t made careless errors, such as initializing a variable incorrectly or
failing to increment a variable within a loop. In other cases, however, a simple test
is too much to expect. Here is an algorithm that would be easy to implement on a
Turing machine:
n = 4
while (n is the sum of two primes)
n = n+2
If you could decide whether this program continues forever, and prove your answer,
you would be famous! The statement that every even integer greater than 2 is the
sum of two primes is known as Goldbach’s conjecture, named after a mathematician
who made a similar conjecture in 1742; mathematicians have been interested in it
ever since, but in spite of a $1,000,000 prize offered in 2000 to anyone who could
prove or disprove it by 2002, no one has determined whether it is true or false. It
is clear, however, that it is true precisely if the program above loops forever.
The halting problem is a famous undecidable problem. As the above three-line
program might suggest, it is also a good example of a problem for which a decision
algorithm, though impossible, would be useful.
9.3 MORE DECISION PROBLEMS
INVOLVING TURING MACHINES
Of the three decision problems we have shown are undecidable, two, Accepts and
Halts have instances that are pairs (T, x), where T is a TM and x is a string.
The special case of Accepts in which an algorithm is still required to consider an
arbitrary TM but can restrict its attention to the string e(T ), is also undecidable;
this is Self-Accepting. If we restrict the problem in the opposite direction, by ﬁxing
the TM and only requiring an algorithm to consider arbitrary strings, there are TMs
for which the resulting problem is undecidable: for example, if Tu is a universal
TM, the decision problem
Given x, does Tu accept x?
is undecidable, because if we could answer the question for every string of the
form e(T )e(z), we would have a decision algorithm for Accepts.
In this section we extend the list of undecidable problems involving TMs.
We start in Theorem 9.9 with a list of ﬁve problems, for each of which we will
construct an appropriate reduction to show that the problem is undecidable.
Theorem 9.9
The following ﬁve decision problems are undecidable.
1. Accepts-: Given a TM T , is  ∈L(T )?
2. AcceptsEverything: Given a TM T with input alphabet , is L(T ) = ∗?
3. Subset: Given two TMs T1 and T2, is L(T1) ⊆L(T2)?

9.3
More Decision Problems Involving Turing Machines
309
4. Equivalent: Given two TMs T1 and T2, is L(T1) = L(T2)?
5. WritesSymbol: Given a TM T and a symbol a in the tape alphabet of T ,
does T ever write an a if it starts with an empty tape?
Proof
In each case, we will construct a reduction to the given problem from
another decision problem that we already know is undecidable.
1. Accepts ≤Accepts-
An instance of Accepts is a pair (T, x) containing a TM and a string.
We must show how to construct for every pair like this a TM F(T, x) = T1
(an instance of Accepts-) such that for every pair (T, x), T accepts x if
and only if T1 accepts .
We have to construct T1 such that even though we don’t know what
the outcome will be when T processes x, the outcome when T1 processes
 will be the same—or at least if the ﬁrst outcome is acceptance, then the
second will be, and if the ﬁrst is not, then the second is not. This seems
very confusing if you’re thinking about what a TM might “normally” do on
input , because it probably seems to have nothing to do with x! But keep
in mind that as long as  is the original input, then no matter what other
string T1 writes on the tape before starting a more “normal” computation,
it’s  that will be accepted or not accepted. If we want T1 to perform a
computation on  that has something to do with the computation of T on
x, then we can simply make the computations identical, except that ﬁrst
we have to put x on the tape before the rest of the computation proceeds.
That’s the key to the reduction. We let T1 = Write(x)T the composite
TM that ﬁrst writes x on the tape, beginning in square 1, and then executes
T . If T accepts x, then when T is called in the computation of T1, it will
process x as if it were the original input, and  will be accepted; otherwise
the second phase of T1’s computation will not end in acceptance, and 
will not be accepted.
It may not be clear what T1 is supposed to do if the original input is
not . It doesn’t matter, because whether or not the algorithm that comes
up with T1 is a correct reduction depends only on what T1 does with the
input .
2. Accepts- ≤AcceptsEverything
Both problems have instances that are TMs. Starting with an arbitrary
TM T , we must ﬁnd another TM F(T ) = T1 such that if T accepts  then
T1 accepts every string over its input alphabet, and if T doesn’t accept 
then there is at least one string not accepted by T1.
It may be a little easier after the ﬁrst reduction to see what to do here.
This time, if T1 starts with an input string x, it’s x that is accepted if T1
eventually reaches the accepting state, even if the computation resembles

310
C H A P T E R 9
Undecidable Problems
the one that T performs with input . We can deﬁne T1 to be the composite
Turing machine Erase T where Erase erases the input string and leaves
the tape head on square 0. For every input, whether T1 reaches ha depends
only on whether T accepts . If T accepts , T1 accepts everything, and
otherwise T1 accepts nothing.
3. AcceptsEverything ≤Subset
For an arbitrary instance T of AcceptsEverythingy, with input alphabet
, we must construct a pair of TMs (T1, T2) such that L(T ) = ∗if and
only if L(T1) ⊆L(T2).
We start by specifying in advance that T1 and T2 will both have input
alphabet  too, so that all the sets in the statements L(T ) = ∗and
L(T1) ⊆L(T2) are subsets of ∗. At this point, rather than starting to
think about what T1 and T2 should do, it’s appropriate to ask what subsets
of ∗we would like L(T1) and L(T2) to be, in order for the statement
L(T ) = ∗⇔L(T1) ⊆L(T2)
to be true. There’s an easy answer: Saying that L(T ) = ∗is the same
as saying that the subset L(T ) of ∗is big enough to contain all of ∗.
In other words,
L(T ) = ∗⇔∗⊆L(T )
Therefore, we can choose T1 such that L(T1) = ∗, and we can let T2 = T .
Any choice of T1 for which L(T1) = ∗will work; the easiest choice is to
make T1 enter the state ha on its ﬁrst move without looking at the input.
4. Subset ≤Equivalent
For this reduction, we must start with two arbitrary TMs T1 and T2,
and construct two other TMs T3 and T4 such that
L(T1) ⊆L(T2) ⇔L(T3) = L(T4)
Just as in the previous reduction, we can ﬁgure out what T3 and T4 should
be by just thinking about the languages we want them to accept. If A
and B are two sets, how can we express the statement A ⊆B by another
statement that says two sets are equal? One answer is
A ⊆B ⇔A ∩B = A
This will work. We can choose T3 to be a TM accepting L(T1) ∩L(T2)
(see Theorem 8.4), and we can choose T4 to be T1.
5. Accepts- ≤WritesSymbol
An instance of Accepts- is a TM T , and an instance of WritesSymbol
is a pair (T1, σ), where T1 is a TM and σ is a tape symbol of T1. For an
arbitrary T , we want a pair (T1, σ) such that T accepts  if and only if
T1 eventually writes the symbol σ, if it starts with an empty tape.

9.3
More Decision Problems Involving Turing Machines
311
In both cases (the TM T we start with, and the TM T1 we are trying
to ﬁnd), we are interested only in what the TM does when it starts with
a blank tape. We can make T1 do exactly what T does, except that if and
when T accepts, T1 writes the symbol σ, and T1 doesn’t do that unless T
accepts.
With that in mind, we can deﬁne (T1, σ) as follows. T1 will be similar
to T , with just two differences: it will have an additional tape symbol σ,
not in the tape alphabet of T , and any move of T that looks like δ(p, σ1) =
(ha, σ2, D) will be replaced by the move δ(p, σ1) = (ha, σ, D). Then T1
has the property that it accepts  precisely if T does, but more to the
point, this happens precisely if T1 writes the symbol σ.
You might be concerned about the special case in which a move of
T that looks like an accepting move is actually a rejecting move, because
it causes T to try to move the tape head off the tape. Because of the way
we have modiﬁed T to obtain T1, however, T1 does not actually write the
symbol σ in this case; if T tries to move its tape head left from square
0, then T1 will also, and our convention in this case is that the symbol
in square 0 before the move will remain unchanged (see the discussion
following Deﬁnition 7.1).
There are actually decision problems having to do with Turing machines that
are decidable! Here is a simple example: Given a TM T , does T make at least ten
moves on input ? Being “given” a TM allows us to trace as many of its moves
as we wish. Here is another example that is not quite as simple, and might surprise
you in the light of the ﬁfth problem in the list above.
WritesNonblank: Given a TM T , does T ever write a nonblank symbol on
input ?
Theorem 9.10
The decision problem WritesNonblank is decidable.
Proof
Suppose T is a TM with n nonhalting states. Then within n moves, either
it halts or it enters some nonhalting state q for the second time. We can
certainly decide whether T writes a nonblank symbol within the ﬁrst n
moves. If it reaches q the second time without having written a nonblank
symbol, the tape head is at least as far to the right as it was the ﬁrst time
q was reached, and the tape is still blank; in this case, T will continue
to repeat the ﬁnite sequence of moves that brought it from q back to q,
and the tape will remain blank forever. Therefore, an algorithm to decide
WritesNonblank is to trace T for n moves, or until it halts, whichever
comes ﬁrst; if by that time no nonblank symbol has been written, none
will ever be.

312
C H A P T E R 9
Undecidable Problems
In both of these last two cases, as well as in the ﬁrst two decision problems in
Theorem 9.9, the problem has the form: Given a TM T , . . . ? However, there is an
important difference between the two problems in the theorem and these last two:
The undecidable problems involve questions about the strings accepted by a TM,
and the two decidable problems ask questions about the way the TM operates, or
about the steps of a computation. The discouraging news, as we shall see, is that
the ﬁrst type of problem is almost always undecidable.
Deﬁnition 9.11
A Language Property of TMs
A property R of Turing machines is called a language property if, for
every Turing machine having property R, and every other TM T1 with
L(T1) = L(T ), T1 also has property R. A language property of TMs is
nontrivial if there is at least one TM that has the property and at least
one that doesn’t.
Some properties of TMs are clearly language properties: for example, “accepts
at least two strings” and “accepts .” (If T accepts at least two strings, and L(T1) =
L(T ), then T1 also accepts at least two strings.) Some properties are clearly not
language properties: for example, “writes a nonblank symbol when started on a
blank tape.”
In some cases you have to be a little more careful. For example, “accepts its
own encoding” might sound at ﬁrst like a language property, but it is not. Knowing
exactly what set of strings T accepts may not help you decide whether T accepts
e(T ) unless you also know something about the string e(T ) (see Exercise 9.23).
Theorem 9.12
Rice’s Theorem
If R is a nontrivial language property of TMs, then the decision problem
PR : Given a TM T , does T have property R?
is undecidable.
Proof
We will show that the undecidable problem Accepts- can be reduced,
either to PR or to its complementary problem Pnot−R. This is sufﬁcient
to prove the theorem, because by Theorem 9.5, if Pnot−R is undecidable,
then so is PR.
Because R is assumed to be a nontrivial language property of TMs,
there are two TMs TR and Tnot−R, the ﬁrst satisfying property R, the
second not satisfying it (i.e., satisfying property not-R). We may not even
need the second; let us describe the algorithm we will use to obtain our
reduction, and then consider exactly what it accomplishes.
We start with an arbitrary TM T , an instance of Accepts-, and
construct a TM T1 = F(T ) to work as follows. T1 begins by moving its
tape head to the ﬁrst blank after the input string and executing T as if its

9.3
More Decision Problems Involving Turing Machines
313
input was . If this computation causes T to reject, then T1 rejects; if it
causes T to accept, then T1 erases the portion of the tape following the
original input, moves its tape head back to square 0, and executes TR on
the original input.
If T accepts , then T1 accepts the language L(TR), and because TR
has the language property R, so does T1. If T doesn’t accept , either
because it rejects or because it loops forever, then T accepts the empty
language: the language L(T0), where T0 is a trivial TM that immediately
rejects every input. We don’t know whether T0 has property R or not,
but if it happens not to have it, our construction of T1 has given us a
reduction from Accepts- to PR.
Otherwise, if T0 has property R, we use exactly the same algorithm,
but substituting Tnot−R for TR. This allows us to say that if T accepts ,
T1 has property not-R, and otherwise T1 has property R, so that we have
a reduction from Accepts- to Pnot−R.
Now it should be more clear that our tentative conclusion in Section 9.1—
that it’s difﬁcult to answer questions about Turing machines and the strings they
accept—is correct. No matter what general question we ask about the language a
TM accepts, there is no algorithm that will always give us the answer. The only
exceptions to this rule are for trivial questions, to which the answer is always yes
or always no.
Here are a few more decision problems that Rice’s theorem implies are unde-
cidable. This list is certainly not exhaustive.
1.
(Assuming L is a recursively enumerable language): AcceptsL: Given a TM
T , is L(T ) = L?
2.
AcceptsSomething: Given a TM T , is there at least one string in L(T )?
3.
AcceptsTwoOrMore: Given a TM T , does L(T ) have at least two elements?
4.
AcceptsFinite: Given a TM T , is L(T ) ﬁnite?
5.
AcceptsRecursive: Given a TM T , is L(T ) (which by deﬁnition is recursively
enumerable) recursive?
In order to use Rice’s theorem directly to show that a decision problem P is
undecidable, P must be a problem involving the language accepted by one TM.
However, other problems might be proved undecidable by using Rice’s theorem
indirectly. For example, to show that the problem Equivalent mentioned in Theo-
rem 9.9 is undecidable, we could argue as follows. Let L = {}. Then by Rice’s
theorem, AcceptsL, problem 1 in the list just above, is undecidable. If T1 is a TM
with L(T1) = {}, then the function F(T ) = (T, T1) is a reduction from AcceptsL
to Equivalent, because for every TM T , L(T ) = {} if and only if L(T ) = L(T1).
Therefore, Equivalent is undecidable.
Decision problems involving the operation of a TM, as opposed to the language
it accepts, cannot be proved undecidable by using Rice’s theorem directly. As the

314
C H A P T E R 9
Undecidable Problems
examples WritesNonblank and WritesSymbol show, some of these are decidable
and some are not.
Finally, Rice’s theorem has nothing to say about trivial decision problems,
which are decidable. It may not always be easy to tell whether a problem is trivial.
The problem
Given a TM T , is L(T ) = NSA?
is trivial, because there is no TM that accepts NSA, and the answer is always no.
If we didn’t know NSA was not recursively enumerable, however, we might very
well guess that the problem was undecidable.
9.4 POST’S CORRESPONDENCE PROBLEM
The decision problem that we discuss in this section is a combinatorial problem
involving pairs of strings, and is, for a change, not obviously related to Turing
machines. Figure 9.13a shows a sample instance.
Think of each of the ﬁve rectangles in Figure 9.13a as a domino, and assume
that there is an unlimited supply of each of the ﬁve. The question is whether it
is possible to make a horizontal line of one or more dominoes, with duplicates
allowed, so that the string obtained by reading across the top halves matches the
one obtained by reading across the bottom.
In this example, the answer is yes. One way to do it is shown in Figure 9.13b.
10
101
01
100
0
10
100
0
1
010
(a)
10
101
1
010
01
100
0
10
100
0
100
0
0
10
100
0
(b)
Figure 9.13
Deﬁnition 9.14
Post’s Correspondence Problem
An instance of Post’s correspondence problem (PCP) is a set {(α1, β1),
(α2, β2), . . . , (αn, βn)} of pairs, where n ≥1 and the αi’s and βi’s are all
nonnull strings over an alphabet . The decision problem is this: Given
an instance of this type, do there exist a positive integer k and a sequence
of integers i1, i2, . . . , ik, with each ij satisfying 1 ≤ij ≤n, satisfying
αi1αi2 . . . αik = βi1βi2 . . . βik

9.4
Post’s Correspondence Problem
315
An instance of the modiﬁed Post correspondence problem (MPCP) looks
exactly like an instance of PCP, but now the sequence of integers is
required to start with 1. The question can be formulated this way: Do
there exist a positive integer k and a sequence i2, i3, . . . , ik such that
α1αi2 . . . αik = β1βi2 . . . βik
Instances of PCP and MPCP are called correspondence systems and mod-
iﬁed correspondence systems, respectively. For an instance of either type,
if it is a yes-instance we will say that there is a match for the instance,
or that the sequence of subscripts is a match, or that the string formed by
the αij ’s represents a match.
Just as in many of the other decision problems we have discussed, there are
simple trial-and-error algorithms which, for every yes-instance of PCP, can conﬁrm
that it is a yes-instance. For no-instances, however, algorithms like this don’t always
work, and there is no other algorithm that can do much better; in this section we
will show that
Accepts ≤MPCP ≤PCP
which implies that both problems are undecidable, and that neither is completely
unrelated to Turing machines after all.
Theorem 9.15
MPCP ≤PCP.
Proof
We will construct, for an arbitrary modiﬁed correspondence system
I = {(α1, β1), (α2, β2), . . . , (αn, βn)}
a correspondence system
J = {(α′
1, β′
1), (α′
2, β′
2), . . . , (α′
n, β′
n), (α′
n+1, β′
n+1)}
as follows. The symbols used in J will be those in I plus two additional
symbols # and $. For 1 ≤i ≤n, if
(αi, βi) = (a1a2 . . . ar, b1b2 . . . bs)
we let
(α′
i, β′
i) = (a1#a2# . . . ar#, #b1#b2# . . . #bs)
except that the string α′
1 has an additional # at the beginning. The last
pair in J is
(α′
n+1, β′
n+1) = ($, #$)

316
C H A P T E R 9
Undecidable Problems
The pairs of I correspond to the ﬁrst n pairs of J, and it is easy to check
that if α1αi2 . . . αik = β1βi2 . . . βik, then
α′
1α′
i2 . . . α′
ikα′
n+1 = β′
1β′
i2 . . . β′
ikβ′
n+1
On the other hand, because of the way the pairs of J are constructed, the
only possible match must start with pair 1; in fact, if
α′
i1α′
i2 . . . α′
ik = β′
i1β′
i2 . . . β′
ik
for some sequence i1, i2, . . . , ik, then i1 must be 1 and ik must be n + 1,
because the strings of the ﬁrst pair must begin with the same symbol
and those of the last pair must end with the same symbol. It is conceiv-
able that some earlier ij is also n + 1, but if im is the ﬁrst ij to equal
n + 1, then 1, i2, . . . , im is a match for J, and 1, i2, . . . , im−1 is a match
for I.
Theorem 9.16
Accepts ≤MPCP.
Proof
Let (T, w) be an arbitrary instance of Accepts, where T = (Q, , , q0, δ)
is a TM and w is a string over . We will construct a modiﬁed corre-
spondence system F(T, w) = (α1, β1), (α2, β2), . . . , (αn, βn) such that T
accepts w if and only if there is a match for F(T, w).
It will be convenient to assume that T never halts in the reject state,
and we can modify T if necessary so that it has this property and still
accepts the same strings (see the proof of Theorem 9.8). Some additional
terminology will be helpful: For an instance (α1, β1), (α2, β2), . . . , (αn, βn)
of MPCP, we call the sequence 1, i2, . . . , ij a partial match if α =
α1αi2 . . . αij is a preﬁx of β = β1βi2 . . . βij . We will also say in this case
that the string α obtained this way is a partial match, or that the two
strings α and β represent a partial match.
To simplify the notation, we will assume that w ̸= . This assumption
will not play an essential part in the proof, and we will indicate later how
to take care of the case w = .
Here is a rough outline of the proof. The symbols involved in our
pairs will be the ones that appear in conﬁgurations of T , together with the
additional symbol #. We will try to specify pairs (αi, βi) in the modiﬁed
correspondence system so that these four conditions are satisﬁed:
1. For every j, if q0	w, x1q1y1, . . . , xjqjyj are successive conﬁgurations
through which T moves as it processes w, then a partial match can be
obtained that looks like
α = #q0	w#x1q1y1#x2q2y2# . . . #xjqjyj

9.4
Post’s Correspondence Problem
317
2. Every partial match is a preﬁx of one like this—or at least starts deviating
from this form only after an occurrence of an accepting conﬁguration xhay
in the string.
3. If α = #q0	w#x1q1y1# . . . #xjhayj is a partial match (which will imply,
because of statement 2, that T accepts w), then there will be additional pairs
(αi, βi) that will allow the preﬁx α to “catch up with” β, so that there will
actually be a match for the modiﬁed correspondence system.
4. Statement 3 represents the only way that the modiﬁed correspondence
system will have a match.
If we can successfully do this, the conclusions we want will not be hard
to obtain.
Here is a slightly more detailed outline of how the pairs will be
constructed. The ﬁrst pair (α1, β1), with which every match must start, is
α1 = #
β1 = #q0	w#
Now suppose, for example, that δ(q0, 	) = (q1, a, R). Corresponding to
this move, we will have a pair
(q0	, aq1)
and we will make sure that this is the only choice for the next pair (αi2, βi2)
in a partial match. The resulting partial match will be
α = α1αi2 = #q0	
β = β1βi2 = #q0	w#aq1
At this point, the only correct way to add to α is to add the sym-
bols of w. Moreover, it is appropriate to add the same symbols to β,
because they appear in the conﬁguration that follows q0	w. Therefore,
we include in our modiﬁed correspondence system pairs (σ, σ) for every
σ ∈ ∪{	}, as well as the pair (#, #). These allow us to obtain the partial
match
α = #q0	w#
β = #q0	w#aq1w#
Now α has caught up to the original string β, and the new β has a
second portion representing the conﬁguration of T after the ﬁrst move.
We continue to extend the partial match in the same way. At each step,
as long as the accepting state ha has not appeared, the string β is one
step ahead of α, and the next portion to be added to α is determined. The
pairs (αi, βi) are such that every time αi’s are added to α to obtain the
conﬁguration of T required next, the corresponding βi’s that are added to
β specify the conﬁguration of T one move later. In this way, we guarantee
that every partial match is of the right form. Roughly speaking, the only

318
C H A P T E R 9
Undecidable Problems
thing left to do is to include pairs (αi, βi) that allow α to catch up to β
once the conﬁguration includes the state ha.
Here is a precise description of the pairs (αi, βi) included in the
modiﬁed correspondence system F(T, w). They are grouped into several
types; the order is not signiﬁcant, and no subscripts are speciﬁed except
for the ﬁrst pair (α1, β1) = (#, #q0	w#).
Pairs of type 1: (a, a) (for every a ∈ ∪{	}), and (#, #)
Pairs of type 2: For every choice of q ∈Q, p ∈Q ∪{ha}, and
a, b, c ∈ ∪{	},
(qa, pb) if δ(q, a) = (p, b, S)
(qa, bp) if δ(q, a) = (p, b, R)
(cqa, pcb) if δ(q, a) = (p, b, L)
(q#, pa#) if δ(q, 	) = (p, a, S)
(q#, ap#) if δ(q, 	) = (p, a, R)
(cq#, pca#) if δ(q, 	) = (p, a, L)
Pairs of type 3: For every choice of a, b ∈ ∪{	}, the pairs
(haa, ha), (aha, ha), (ahab, ha)
One pair of type 4: (ha##, #)
The proof of the theorem depends on this claim: If
α = γ #
β = γ #z#
is any partial match for the modiﬁed correspondence system, where z
represents a nonhalting conﬁguration of T , then we can extend it to a
partial match
α′ = γ #z#
β = γ #z#z′#
where z′ represents the conﬁguration of T one move later. Furthermore,
the string β′ shown is the only one that can correspond to this α′ in a
partial match.
We examine the claim in the case where
α = γ #
β = γ #a1 . . . akqak+1 . . . ak+m#
and m > 0 and δ(q, ak+1) = (p, b, R). The other cases are similar. We can
extend the partial match α by using ﬁrst the pairs (a1, a1), . . . , (ak, ak)
of type 1; then the pair (qak+1, bp) of type 2; then any remaining pairs

9.4
Post’s Correspondence Problem
319
(ak+2, ak+2), . . . , (ak+m, ak+m); and ﬁnally the pair (#, #). The partial
match we get is
α = γ #a1 . . . akqak+1 . . . ak+m#
β = γ #a1 . . . akqak+1 . . . ak+m#a1 . . . akbpak+2 . . . ak+m#
The substring of β between the last two #’s is the conﬁguration of T
resulting from the move indicated, and at no point in this sequence of
steps is there any choice as to which pair to use next. Thus the claim is
correct in this case.
Suppose that T accepts the string w. Then there is a sequence of
successive conﬁgurations
z0 = q0	w, z1, z2, . . . , zj
that ends with an accepting conﬁguration. The claim above implies that
there is a partial match
α = #z0#z1# . . . #zj−1#
β = #z0#z1# . . . #zj−1#zj#
to the modiﬁed correspondence system, where zj = uhav for two strings
u and v, either or both of which may be null. If at least one of these two
strings is nonnull, we can extend the partial match by using one pair of
type 3 and others of type 1, to obtain
α′ = αzj#
β′ = αzj#z′
j#
where z′
j still contains ha but has at least one fewer symbol than zj.
We can continue to extend the partial match, decreasing the lengths of
the strings between successive #’s, until we obtain a partial match of the
form
α′′#
α′′#ha#
at which point the pair of type 4 produces a match for the modiﬁed
correspondence system.
If T does not accept w, on the other hand, then our assumption is
that it loops forever. For every partial match of the form
α = #z0#z1# . . . #zk#
β = #z0#z1# . . . #zk#zk+1#
(where no string zi contains #), the second part of the claim implies that
the strings zi must represent consecutive conﬁgurations of T . It follows
that ha can never appear in any partial match, which means that the pair

320
C H A P T E R 9
Undecidable Problems
of type 4 is never used. Because α1 and β1 have different numbers of
#’s, and the pair of type 4 is the only other one with this property, the
modiﬁed correspondence system can have no match.
In the case w = , the only change needed in the proof is that the
initial pair is (#, #q0#) instead of (#, #q0	w#).
Theorem 9.17
Post’s correspondence problem is undecidable.
Proof
The theorem is an immediate consequence of Theorems 9.15 and 9.16.
EXAMPLE 9.18
A Modiﬁed Correspondence System for a TM
Let T be the TM shown in Figure 9.19 that accepts all strings in {a, b}∗ending with b.
q0
q1
q2
ha
a/a, R
b/b, R
Δ /Δ, R
Δ /Δ, L
b/Δ, S
a/a, R
Figure 9.19
In the modiﬁed correspondence system constructed as in the proof of Theorem 9.10,
the pairs of type 2 are these:
(q0	, 	q1)
(q0#, 	q1#)
(q1a, aq1)
(q1b, bq1)
(aq1	, q2a	)
(bq1	, q2b	)
(aq1#, q2a	#)
(bq1#, q2b	#)
(	q1	, q2		)
(	q1#, q2		#)
(q2a, aq1)
(q2b, ha	)
We consider three possible choices for pair 1, corresponding to two strings w that are
not accepted by T and one that is. For the input , pair 1 is (#, #q0#), and the partial match
below is the only one, except for smaller portions of it.
#
q0#
Δq1#
#q0#
Δq1#
q2ΔΔ#
The input string a causes T to loop forever. In this case, pair 1 is (#, #q0	a#). In
the partial match shown, the pair that appears last is appearing for the second time, and

9.5
Undecidable Problems Involving Context-Free Languages
321
longer partial matches would simply involve repetitions of the portion following the ﬁrst
occurrence.
#
#
#
#
q0Δ
q1a
aq1#
q2a
Δ
Δ
Δ
Δ
a
#q0Δa#
aq1Δ
#
#
#
Δq1
aq1
q2aΔ#
#
aq1
a
Δ
Δ
Δ
q2aΔ
q2a
aq1
#
Δ
Δ
Δ
Δ
Δ
Finally, for the input string b, which is accepted by T , pair 1 is (#, #q0	b#), and the
match is shown below.
#
#
#
#
q0Δ
q1b
bq1#
q2b
Δ
Δ
Δ
Δ
b
#q0Δb#
#
#
Δq1
bq1
q2bΔ#
#
haΔ
b
Δ
Δ
Δ
Δ
#
ΔhaΔ
ha
ha##
#
haΔ
ha
#
#
#
Δ
Δ
9.5 UNDECIDABLE PROBLEMS INVOLVING
CONTEXT-FREE LANGUAGES
In this section we will look at two approaches to obtaining undecidability results
involving context-free grammars and CFLs. The ﬁrst uses the fact that Post’s Cor-
respondence Problem is undecidable.
For an instance (α1, β1), (α2, β2), . . . , (αn, βn) of PCP, where the αi’s and
βi’s are strings over , we construct two context-free grammars Gα and Gβ whose
properties are related to those of the correspondence system. Let c1, c2, . . . , cn be
symbols that are not in . Gα will be the CFG with start symbol Sα and the 2n
productions
Sα →αiSαci | αici
(1 ≤i ≤n)
and Gβ is constructed the same way from the strings βi. Then for every string
c = ci1ci2 . . . cik, where k ≥1, there is exactly one string x in L(Gα) and exactly
one string y in L(Gβ) that is a string in ∗followed by c. The string x is
αik . . . αi2αi1ci1ci2 . . . cik, and the string y is βik . . . βi2βi1ci1ci2 . . . cik. Both x and y

322
C H A P T E R 9
Undecidable Problems
have unique derivations in their respective grammars, and both derivations are deter-
mined by c.
Theorem 9.20
These two problems are undecidable:
1. CFGNonemptyIntersection: Given two CFGs G1 and G2, is L(G1) ∩L(G2)
nonempty?
2. IsAmbiguous: Given a CFG G, is G ambiguous?
Proof
We will reduce PCP both to CFGNonemptyIntersection and to IsAmbigu-
ous, and the two reductions will be very similar.
Starting with an arbitrary instance I of PCP, involving αi’s and βi’s as
above, we construct the two context-free grammars Gα and Gβ as we have
described. For the ﬁrst reduction, we let F1(I) be the instance (Gα, Gβ) of
CFGNonemptyIntersection, and for the second we let F2(I) be the gram-
mar G constructed in the usual way to generate L(Gα) ∪L(Gβ): The start
symbol of G is S, and the productions include those of Gα, those of Gβ,
and the two additional ones S →Sα | Sβ.
If I is a yes-instance of PCP, then for some sequence of integers i1,
i2, . . . , ik,
αikαik−1 . . . αi1 = βikβik−1 . . . βi1
so that
αikαik−1 . . . αi1ci1 . . . cik = βikβik−1 . . . βi1ci1 . . . cik
It follows that this string is an element of the intersection L(Gα) ∩L(Gβ),
and that it has two derivations in G, one starting with S ⇒Sα and the
other with S ⇒Sβ. Therefore, both F1(I) and F2(I) are yes-instances of
their respective decision problems.
On the other hand, if either F1(I) or F2(I) is a yes-instance (the two
statements are equivalent), then for some x ∈∗and some sequence of
integers i1, . . . , ik, there is a string xci1ci2 . . . cik that is in the intersection
L(Gα) ∩L(Gβ) and can therefore be derived from either Sα or Sβ. As
we observed above, this implies that
x = αikαik−1 . . . αi1ci1 . . . cik = βikβik−1 . . . βi1ci1 . . . cik
which implies that I is a yes-instance of PCP.
The other approach in this section is to return to Turing machines and to con-
sider, for a TM T , some aspects of the computations of T that are represented by
context-free languages, so that being able to answer certain questions about CFLs
would allow us to answer questions about TMs.

9.5
Undecidable Problems Involving Context-Free Languages
323
Deﬁnition 9.21
Valid Computations of a TM
Let T = (Q, , , q0, δ) be a Turing machine. A valid computation of T
is a string of the form
z0#zr
1#z2#zr
3 . . . #zr
n−1#zn#
if n is even, or
z0#zr
1#z2#zr
3 . . . #zn−1#zr
n#
if n is odd, where in either case, # is a symbol not in , and the strings zi
represent successive conﬁgurations of T on some input string x, starting
with the initial conﬁguration z0 and ending with an accepting conﬁgura-
tion. The set of valid computations of T will be denoted by CT .
Reversing every other conﬁguration in these strings is the trick that allows us
to involve CFLs, because a substring of the form zi#zr
i+1# differs in only a few
positions from a palindrome.
Theorem 9.22
For a TM T = (Q, , , q0, δ), the set CT of valid computations of T is
the intersection of two context-free languages, and its complement C′
T is
a context-free language.
Proof
The two context-free languages that we will use to obtain CT can be
described in terms of these ﬁve languages:
L1 = {z#(z′)r# | z and z′ represent conﬁgurations of T for which z ⊢z′}
L2 = {zr#z′# | z and z′ represent conﬁgurations of T for which z ⊢z′}
I = {z# | z is an initial conﬁguration of T }
A = {z# | z is an accepting conﬁguration of T }
A1 = {zr# | z is an accepting conﬁguration of T }
First we will express CT as the intersection of two languages that can be
obtained from these ﬁve, and then we will show that the two languages
are actually CFLs.
A valid computation c of T in which the total number of conﬁgura-
tions is even can be written both as
c = z0#(zr
1#z2#) . . . (zr
n−2#zn−1#)zr
n#
where z0 is an initial conﬁguration and zn is an accepting conﬁguration,
and as
c = (z0#zr
1#)(z2#zr
3#) . . . (zn−1#zr
n#)

324
C H A P T E R 9
Undecidable Problems
In other words, c ∈IL∗
2A1 ∩L∗
1. Similarly, if the number of conﬁgurations
involved in the string c is odd, then c ∈IL∗
2 ∩L∗
1A. These two statements
imply that
CT ⊆L3 ∩L4
where L3 = IL∗
2(A1 ∪{}) and L4 = L∗
1(A ∪{}).
On the other hand, consider a string z that is either of the form
z = z0#zr
1#z2#zr
3 . . . #zr
n−1#zn#
or of the form
z0#zr
1#z2#zr
3 . . . #zn−1#zr
n#
where the zi’s represent conﬁgurations of T (not necessarily consecutive),
z0 represents an initial conﬁguration, and zn represents an accepting con-
ﬁguration. If z ∈L3, then for each odd i, zi ⊢zi+1, and if z ∈L4, the
same is true for each even i. It follows that
L3 ∩L4 ⊆CT
Therefore, we can complete the proof of the ﬁrst statement in the theorem
by showing that L3 and L4 are CFLs. Because of the way L3 and L4 are
deﬁned, it is sufﬁcient to show that the languages L1 and L2 are both CFLs.
We will prove that L1 is a CFL by describing a PDA M that accepts
it, and the proof for L2 is similar. M will have in its input alphabet both
states of T and tape symbols of T , including 	, and the two types of
symbols are assumed not to overlap.
One aspect of M’s computation, which can be accomplished without
the stack, is to check that the input string is of the form z#z′#, where both z
and z′ are elements of ∗Q∗. If the input is not of this form, M will reject.
The step that is crucial for the rest of the argument is to show that
M can operate so that if the ﬁrst portion z of the input is a conﬁguration
of T , then the stack contents when this portion has been processed is
simply (z′)rZ0, where z ⊢T z′ and Z0 is the initial stack symbol of M.
This feature will allow M to process the input after the ﬁrst # by simply
matching input symbols with stack symbols.
We consider the case in which
z = xapby
where x, y ∈( ∪{	})∗, p ∈Q, a, b ∈ ∪	, and
δ(p, b) = (q, c, L)
The other cases are similar. T moves from the conﬁguration xapby to
xqacy. M can operate by pushing input symbols onto its stack until it
sees a state of T ; when this happens, the stack contains axrZ0, and the
next input symbol is b. It pops the a, replaces it by caq, and continues to

9.5
Undecidable Problems Involving Context-Free Languages
325
read symbols and push them onto the stack until it encounters #. At this
point, z has been processed completely, the stack contents are
yrcaqxrZ0 = (xqacy)rZ0
and M can continue its operation through the remainder of the string in
the way we described.
For the second statement of the theorem, we must consider the com-
plement C′
T of CT in the set ( ∪{	} ∪Q ∪{ha} ∪{#})∗.
We can use the deﬁnition of a valid computation of T to show that
there are seven ways a string x over this alphabet can fail to be in CT .
The ﬁrst and simplest way is for x not to end in #. The remaining six
ways, for
x = z0#z1# . . . #zk#
where no zi contains #, are:
2. For some even i, zi does not represent a conﬁguration of T .
3. For some odd i, zr
i does not represent a conﬁguration of T .
4. The string z0 does not represent an initial conﬁguration of T .
5. zk is neither an accepting conﬁguration of T nor the reverse of one.
6. For some even i, zi and zr
i+1 represent conﬁgurations of T but zr
i+1 is not the
conﬁguration to which T moves from zi.
7. For some odd i, zr
i and zi+1 represent conﬁgurations of T but zi+1 is not the
conﬁguration to which T moves from zr
i .
Each of these seven conditions can be tested individually by a PDA,
and in some cases an FA would be enough. For the last two conditions,
nondeterminism can be used to select an i, and from that point on the
argument is similar to the one we used for the language L1 above.
The conclusion is that C′
T is the union of seven CFLs, which means
that it is a CFL itself.
Not only are there context-free grammars generating the languages L3 and L4
described in the proof of Theorem 9.22, but they can be constructed algorithmically
from the Turing machine T . This provides an alternative proof that the decision
problem CFGNonemptyIntersection is undecidable (see Exercise 9.26). The last
theorem in this section is another undecidability result that uses the second part of
Theorem 9.22.
Theorem 9.23
The decision problem
CFGGeneratesAll: Given a CFG G with terminal alphabet , is L(G)
= ∗?
is undecidable.

326
C H A P T E R 9
Undecidable Problems
Proof
If T is a TM, the last part of the proof of Theorem 9.22 refers to seven
CFLs whose union is C′
T ; we could formulate an algorithm to ﬁnd a
CFG for each one in terms of T , and therefore to ﬁnd a CFG F(T ) = G
generating C′
T . Let  be the terminal alphabet of G. Saying that T accepts
at least one string is equivalent to saying that CT is not empty, or that C′
T
is not ∗. As a result, the algorithm deﬁnes a reduction from the problem
AcceptsNothing: Given a TM T , is L(T ) = ∅?
to CFGGeneratesAll, and AcceptsNothing is undecidable by Rice’s theorem.
EXERCISES
9.1.
Show that the relation ≤on the set of languages (or on the set of decision
problems) is reﬂexive and transitive. Give an example to show that it is
not symmetric.
9.2.
Describe how a universal Turing machine could be used in the proof that
SA is recursively enumerable.
9.3.
Show that if L1 and L2 are languages over  and L2 is recursively
enumerable and L1 ≤L2, then L1 is recursively enumerable.
9.4.
Show that if L ⊆∗is neither empty nor all of ∗, then every recursive
language over  can be reduced to L.
9.5.
Fermat’s last theorem, until recently one of the most famous unproved
statements in mathematics, asserts that there are no integer solutions
(x, y, z, n) to the equation xn + yn = zn satisfying x, y > 0 and n > 2.
Ignoring the fact that the theorem has now been proved, explain how a
solution to the halting problem would allow you to determine the truth or
falsity of the statement.
9.6.
Show that every recursively enumerable language can be reduced to the
language Acc = {e(T )e(w) | T is a TM and T accepts input w}.
9.7.
As discussed at the beginning of Section 9.3, there is at least one TM T
such that the decision problem “Given w, does T accept w?” is unsolvable.
Show that every TM accepting a nonrecursive language has this property.
9.8.
Show that for every x ∈∗, the problem Accepts can be reduced to the
problem: Given a TM T , does T accept x? (This shows that, just as
Accepts- is unsolvable, so is Accepts-x, for every x.)
9.9.
Construct a reduction from Accepts- to the problem Accepts-{}: Given
a TM T , is L(T ) = {}?
9.10.
a.
Given two sets A and B, ﬁnd two sets C and D, deﬁned in terms of A
and B, such that A = B if and only if C ⊆D.
b.
Show that the problem Equivalent can be reduced to the problem
Subset.

Exercises
327
9.11.
Construct a reduction from the problem AcceptsEverything to the problem
Equivalent.
9.12.
For each decision problem below, determine whether it is decidable or
undecidable, and prove your answer.
a.
Given a TM T , does it ever reach a state other than its initial state if it
starts with a blank tape?
b.
Given a TM T and a nonhalting state q of T , does T ever enter state
q when it begins with a blank tape?
c.
Given a TM T and a nonhalting state q of T , is there an input string x
that would cause T eventually to enter state q?
d.
Given a TM T , does it accept the string  in an even number of
moves?
e.
Given a TM T , is there a string it accepts in an even number of
moves?
f.
Given a TM T and a string w, does T loop forever on input w?
g.
Given a TM T , are there any input strings on which T loops forever?
h.
Given a TM T and a string w, does T reject input w?
i.
Given a TM T , are there any input strings rejected by T ?
j.
Given a TM T , does T halt within ten moves on every string?
k.
Given a TM T , is there a string on which T halts within ten moves?
l.
†Given a TM T , does T eventually enter every one of its nonhalting
states if it begins with a blank tape?
m. Given a TM T , is there an input string that causes T to enter every
one of its nonhalting states?
9.13.
In this problem TMs are assumed to have input alphabet {0, 1}. For a
ﬁnite set S ⊆{0, 1}∗, PS denotes the decision problem: Given a TM T , is
S ⊆L(T )?
a.
Show that if x, y ∈{0, 1}∗, then P{x} ≤P{y}.
b.
Show that if x, y, z ∈{0, 1}∗, then P{x} ≤P{y,z}.
c.
Show that if x, y, z ∈{0, 1}∗, then P{x,y} ≤P{z}.
d.
Show that for every two ﬁnite subsets S and U of {0, 1}∗, PS ≤PU.
9.14.
†Repeat the previous problem, but this time letting PS denote the problem:
Given a TM T , is L(T ) = S?
9.15.
Let us make the informal assumption that Turing machines and computer
programs written in the C language are equally powerful, in the sense that
anything that can be programmed on one can be programmed on the other.
Give a convincing argument that both of the following decision problems
are undecidable:
a.
Given a C program, a statement s in the program, and a speciﬁc set I
of input data, is s ever executed when the program is run on input I?
b.
Given a C program and a statement s in the program, is there a set I of
input data such that s is executed when the program runs on input I?

328
C H A P T E R 9
Undecidable Problems
9.16.
Suppose P1 and P2 are decision problems, and Y(P1) ⊆∗
1 and
Y(P2) ⊆∗
2 are the corresponding languages (that is, the languages of
strings representing yes-instances of P1 and P2, respectively, with respect
to some reasonable encoding functions e1 and e2). Assume there is at least
one string in ∗
2 that is not an instance of P2. Suppose the function f
deﬁnes a reduction from P1 to P2; in other words, for every instance I of
P1, f (I) is an instance of P2 having the same answer. Show that
Y(P1) ≤Y(P2). Describe a function from ∗
1 to ∗
2 that gives a reduction.
9.17.
Suppose P1, P2, Y(P1), and Y(P2) are as in Exercise 9.16. Suppose also
that there is at least one no-instance of P2. Show that if there is a function
t : ∗→∗reducing Y(P1) to Y(P2), then there is another (computable)
function t′ reducing Y(P1) to Y(P2) and having the property that for every
x ∈∗that corresponds to an instance of P1, t′(x) corresponds to an
instance of P2.
9.18.
Let P1, P2, Y(P1), and Y(P2) be as in Exercise 9.16. Suppose t : ∗
1 →
∗
2 is a reduction of Y(P1) to Y(P2). According to Exercise 9.17, we may
assume that for every string x in ∗
1 representing an instance of P1, t(x)
represents an instance of P2. Show that P1 ≤P2. Describe a function f
that gives a reduction. (In other words, for an instance I of P1, say how to
calculate an instance f (I) of P2.)
9.19.
Show that the following decision problems involving unrestricted
grammars are undecidable.
a.
Given a grammar G and a string w, does G generate w?
b.
Given a grammar G, does it generate any strings?
c.
Given a grammar G with terminal alphabet , does it generate every
string in ∗?
d.
Given grammars G1 and G2, do they generate the same language?
9.20.
†This exercise presents an example of a language L such that neither L
nor L′ is recursively enumerable. Let Acc and AE be the languages over
{0, 1} deﬁned as follows.
Acc = {e(T )e(w) | T is a TM that accepts the input string w}
AE = {e(T ) | T is a TM accepting every string in its input alphabet}
(Acc and AE are the sets of strings representing yes-instances of the
problems Accepts and AcceptsEverything, respectively.) Acc′ and AE′
denote the complements of these two languages.
a.
Show that Acc ≤AE.
b.
Show that Acc′ ≤AE′.
c.
Show that AE′ is not recursively enumerable.
d.
Show that Acc′ ≤AE. (If x = e(T )e(z), let f (x) = e(ST,z), where
ST,z is a TM that works as follows. On input w, ST,z simulates the
computation performed by T on input z for up to |w| moves. If this
computation would cause T to accept within |w| moves, ST,z enters an

Exercises
329
inﬁnite loop; otherwise ST,z accepts. Show that if f (x) is deﬁned
appropriately for strings x not of the form e(T )e(z), then f deﬁnes a
reduction from Acc′ to AE.)
e.
Show that AE is not recursively enumerable.
9.21.
If AE is the language deﬁned in the previous exercise, show that if L is
any language whose complement is not recursively enumerable, then
L ≤AE.
9.22.
Find two undecidable decision problems, neither of which can be reduced
to the other, and prove it.
9.23.
Show that the property “accepts its own encoding” is not a language
property of TMs.
9.24.
In parts (a), (d), (e), (g), (i), (j), (k), (l), and (m) of Exercise 9.12, an
instance of the decision problem is a TM. Show in each part that the TM
property involved is not a language property.
9.25.
Rice’s theorem can be extended to decision problems whose instances are
pairs (T1, T2) of Turing machines, problems of the form: Given TMs T1
and T2, do T1 and T2 satisfy some property? Following the original version
of the theorem, we call a 2-TM property (i.e., a property of pairs of TMs)
a language property if, whenever T1 and T2 have the property and
L(T3) = L(T1) and L(T4) = L(T2), T3 and T4 also have the property; and
it is a nontrivial language property if there is a pair (T1, T2) of TMs
having the property and another pair (T3, T4) not having the property. By
adapting the proof of Rice’s theorem, prove that if R is a nontrivial
language property of two TMs, then the decision problem “Given TMs T1
and T2, do they have property R?” is undecidable.
9.26.
The decision problem NonemptyIntersection: “Given two TMs T1 and T2,
is L(T1) ∩L(T2) nonempty?” is undecidable, as a result of what was
proved in Exercise 9.25. Prove that this problem is undecidable without
using Rice’s theorem, by reducing another undecidable problem to it. Do
the same for the problem: Given two TMs T1 and T2, is L(T1) ∪L(T2)
empty?
9.27.
In each case below, either ﬁnd a match for the correspondence system or
show that none exists.
a.
110
1010
101
01
100
10
b.
0
101
001
0
01
101
1
10
9.28.
Show that the special case of PCP in which the alphabet has only two
symbols is still undecidable.
9.29.
Show that the special case of PCP in which the alphabet has only one
symbol is decidable.

330
C H A P T E R 9
Undecidable Problems
9.30.
Show that the problem CSLIsEmpty: given a linear-bounded automaton, is
the language it accepts empty? is undecidable. Suggestion: use the fact
that Post’s correspondence problem is undecidable, by starting with an
arbitrary correspondence system and constructing an LBA that accepts
precisely the strings α representing solutions to the correspondence system.
9.31.
This exercises establishes the fact that there is a recursive language over
{a, b} that is not context-sensitive. A diagonal argument is suggested. (At
this point, a diagonal argument or something comparable is the only
technique known for constructing languages that are not context-sensitive.)
a.
Describe a way to enumerate explicitly the set of context-sensitive
grammars generating languages over {a, b}. You may make the
assumption that for some set A = {A1, A2, . . . }, every such grammar
has start symbol A1 and only variables that are elements of A.
b.
If G1, G2, . . . is the enumeration in part (a), and x1, x2, . . . are the
nonnull elements of {a, b}∗listed in canonical order, let L = {xi | xi /∈
L(Gi)}. Show that L is recursive and not context-sensitive.
9.32.
Show that each of the following decision problems for CFGs is
undecidable.
a.
Given two CFGs G1 and G2, is L(G1) = L(G2)?
b.
Given two CFGs G1 and G2, is L(G1) ⊆L(G2)?
c.
Given a CFG G and a regular language R, is L(G) = R?
9.33.
†Is the decision problem. “Given a CFG G, and a string x, is
L(G) = {x}?” decidable or undecidable? Give reasons for your answer.
9.34.
Is the decision problem. “Given a CFG G and a regular language R, is
L(G) ⊆R?” decidable or undecidable? Give reasons for your answer.
9.35.
†Show that the problem. “Given a CFG G with terminal alphabet , is
L(G) ̸= ∗?” is undecidable by directly reducing PCP to it. Suggestion:
if Gα and Gβ are the CFGs constructed from an instance of PCP as in
Section 9.5, show that there is an algorithm to construct a CFG generating
(L(Gα) ∩L(Gβ))′.

331
C
H
A
P
T
E
R
10
Computable Functions
I
n the same way that most languages over an alphabet  are not decidable,
most partial functions on ∗are not computable. In this chapter we consider
an approach to computability due to Kleene, in which we try to say more explic-
itly what sorts of computations we (or a Turing machine) can actually carry out.
We start by considering only numeric functions, although this is not as much of
a restriction as it might seem. We discuss how to characterize the computable
functions by describing a set of “initial” functions and a set of operations that
preserve the property of computability. Using a numbering system introduced by
G¨odel to “arithmetize” a Turing machine, we can demonstrate that the resulting
“μ-recursive” functions are the same as the Turing-computable functions.
10.1 PRIMITIVE RECURSIVE FUNCTIONS
For the rest of this chapter, the functions we discuss will be partial functions from
N k to N, for some k ≥1. We will generally use a lowercase letter for a number
and an uppercase one for a vector (a k-tuple of natural numbers).
Deﬁnition 10.1
Initial Functions
The initial functions are the following:
1. Constant functions: For each k ≥1 and each a ≥0, the constant function
Ck
a : N k →N is deﬁned by the formula
Ck
a(X) = a
for every X ∈N k
2. The successor function s : N →N is deﬁned by the formula
s(x) = x + 1

332
C H A P T E R 10
Computable Functions
3. Projection functions: For each k ≥1 and each i with 1 ≤i ≤k, the
projection function pk
i : N k →N is deﬁned by the formula
pk
i (x1, x2, . . . , xi, . . . , xk) = xi
The
ﬁrst
operations
by
which
new
functions
will
be
obtained
are
composition, which in the simplest case gives us functions with formulas like
f ◦g(x) = f (g(x)), and an operation that produces a function deﬁned recursively
in terms of given functions. For the second one, a few preliminaries may be
helpful.
The simplest way to deﬁne a function f from N to N recursively is to deﬁne
f (0) ﬁrst, and then for every k ≥0 to deﬁne f (k + 1) in terms of f (k). A familiar
example is the factorial function:
0! = 1
(k + 1)! = (k + 1) ∗k!
In the recursive step, the expression for f (k + 1) involves both k and f (k). We
can generalize this by substituting any expression of the form h(k, f (k)), where
h is a function of two variables. In order to use this approach for a function f
of more than one variable, we simply restrict the recursion to the last coordinate.
In other words, we start by saying what f (x1, x2, . . . , xn, 0) is, for any choice
of (x1, . . . , xn). In the recursive step, we say what f (x1, x2, . . . , xn, k + 1) is, in
terms of f (x1, . . . , xn, k). If X represents the n-tuple (x1, . . . , xn), then in the most
general case, f (X, k + 1) may depend on X and k directly, in addition to f (X, k),
just as (k + 1)! depended on k as well as on k!. A reasonable way to formulate
this recursive step is to say that
f (X, k + 1) = h(X, k, f (X, k))
for some function h of n + 2 variables.
Deﬁnition 10.2
The Operations of Composition and
Primitive Recursion
1. Suppose f is a partial function from N k to N, and for each i with
1 ≤i ≤k, gi is a partial function from N m to N. The partial function
obtained from f and g1, g2, . . . , gk by composition is the partial function h
from N m to N deﬁned by the formula
h(X) = f (g1(X), g2(X), . . . , gk(X)) for every X ∈N m
2. Suppose n ≥0, and g and h are functions of n and n + 2 variables,
respectively. (By “a function of 0 variables,” we mean simply a constant.)
The function obtained from g and h by the operation of primitive recursion

10.1
Primitive Recursive Functions
333
is the function f : N n+1 →N deﬁned by the formulas
f (X, 0) = g(X)
f (X, k + 1) = h(X, k, f (X, k))
for every X ∈N n and every k ≥0.
A function obtained by either of these operations from total functions is a
total function. If f : N n+1 →N is obtained from g and h by primitive recursion,
and g(X) is undeﬁned for some X ∈N n, then f (X, 0) is undeﬁned, f (X, 1) =
h(X, 0, f (X, 0)) is undeﬁned, and similarly f (X, k) is undeﬁned for each k. For
the same reason, if f (X, k) is undeﬁned for some k, say k = k0, then f (X, k) is
undeﬁned for every k ≥k0. This is the same as saying that if f (X, k1) is deﬁned,
then f (X, k) is deﬁned for every k ≤k1.
Deﬁnition 10.3
Primitive Recursive Functions
The set PR of primitive recursive functions is deﬁned as follows.
1. All initial functions are elements of PR.
2. For every k ≥0 and m ≥0, if f : N k →N and g1, g2, . . . , gk : N m →N
are elements of PR, then the function f (g1, g2, . . . , gk) obtained from f and
g1, g2, . . . , gk by composition is an element of PR.
3. For every n ≥0, every function g : N n →N in PR, and every function
h : N n+2 →N in PR, the function f : N n+1 →N obtained from g and h
by primitive recursion is in PR.
In other words, the set PR is the smallest set of functions that contains
all the initial functions and is closed under the operations of composition
and primitive recursion.
It is almost obvious that the initial functions are computable, and the set
of computable functions is also closed under the operations of composition and
primitive recursion. It is possible to describe in detail Turing machines that can
compute the functions obtained by these operations, assuming that the functions we
start with are computable; the algorithms are straightforward, and we will simply
cite the Church-Turing thesis as the reason for Theorem 10.4.
Theorem 10.4
Every primitive recursive function is total and computable.

334
C H A P T E R 10
Computable Functions
Not all total computable functions are primitive recursive, and Exercise 10.28
discusses a way to show this using a diagonal argument. In the next section we will
consider other operations on functions that preserve the property of computability
but not that of primitive recursiveness.
EXAMPLE 10.5
Addition, Multiplication, and Subtraction
The functions Add and Mult, both functions from N × N to N, are deﬁned by the formulas
Add(x, y) = x + y
Mult(x, y) = x ∗y
In general, we can show that a function f is primitive recursive by constructing a primitive
recursive derivation: a sequence of functions f0, f1, . . . , fj such that fj = f and each fi
in the sequence is an initial function, or obtained from earlier functions in the sequence by
composition, or obtained from earlier functions by primitive recursion. For both Add and
Mult, we can obtain a derivation in reverse, by identifying simpler functions from which
the function we want can be obtained by primitive recursion.
Add(x, 0) = x = p1
1(x)
Add(x, k + 1) = (x + k) + 1 = s(Add(x, k)) = s(p3
3(x, k, Add(x, k)))
A primitive recursive derivation for Add, therefore, can consist of the three initial functions
p1
1, s, and p3
3, the function f3 obtained from s and p3
3 by composition, and Add, which is
obtained from p1
1 and f3 by primitive recursion.
Mult(x, 0) = 0 = C1
0(x)
Mult(x, k + 1) = x ∗k + x = Add(x, Mult(x, k))
The ﬁrst of the two arguments in the last expression is p3
1(x, k, Mult(x, k)) and the second is
p3
3(x, k, Mult(x, k)). Using this approach, we might begin our derivation with the derivation
of Add, continue with C1
0 and the function f obtained from Add, p3
1, and p3
3 by composition,
and ﬁnish with Mult, obtained from C1
0 and f by primitive recursion.
The “proper subtraction” function Sub is deﬁned by
Sub(x, y) =

x −y
if x ≥y
0
otherwise
Just as we showed that Add is primitive recursive by using the successor function, we show
Sub is by using a predecessor function Pred:
Pred(x) =

0
if x = 0
x −1
if x ≥1
Pred is primitive recursive because of the formulas
Pred(0) = 0
Pred(k + 1) = k
and now we can use the formula
Sub(x, 0) = x
Sub(x, k + 1) = Pred(Sub(x, k))

10.1
Primitive Recursive Functions
335
to show that Sub is primitive recursive. The proper subtraction operation is often written
.−,
with a dot above the minus sign, and another name for it is the monus operation.
The two functions Pred and Sub have both been deﬁned by considering two
cases. Sub(x,y), for example, is deﬁned one way if some condition P (x, y) is true
and another way if it is false. In general an n-place predicate P is a function, or
partial function, from N n to { true, false }, and the corresponding numeric function
is the characteristic function χP deﬁned by
χP (X) =
 1
if P (X) is true
0
if P (X) is false
It makes sense to say that the predicate P is primitive recursive, or that it is
computable, if the function χP is.
Theorem 10.6 asserts that the set of primitive recursive predicates includes the
common relational predicates, such as < and ̸=, and is closed under the logical
operations AND, OR, and NOT. Theorem 10.7 says that more general deﬁnitions by
cases, involving two or more primitive recursive predicates, also produce primitive
recursive functions.
Theorem 10.6
The two-place predicates LT, EQ, GT, LE, GE, and NE are primitive
recursive. (LT stands for “less than,” and the other ﬁve have similarly
intuitive abbreviations.) If P and Q are any primitive recursive n-place
predicates, then P ∧Q, P ∨Q, and ¬P are primitive recursive.
Proof
The second statement follows from the equations
χP1∧P2 = χP1 ∗χP2
χP1∨P2 = χP1 + χP2
.−χP1∧P2
χ(¬P1) = 1
.−χP1
For the ﬁrst statement, we introduce the function Sg: N →N deﬁned by
Sg(0) = 0
Sg(k + 1) = 1
This function takes the value 0 if x = 0 and the value 1 otherwise, and
its deﬁnition makes it clear that it is primitive recursive. We may write
χLT (x, y) = Sg(y
.−x)
which implies that χLT is obtained from primitive recursive functions
by composition. The result for the equality predicate follows from the
formula
χEQ(x, y) = 1
.−(Sg(x
.−y) + Sg(y
.−x))

336
C H A P T E R 10
Computable Functions
(If x < y or x > y, then one of the terms x
.−y and y
.−x is nonzero,
and the expression in parentheses is nonzero, causing the ﬁnal result to
be 0. If x = y, both terms in the parenthesized expression are 0, and the
ﬁnal result is 1.)
The other four relational predicates can be handled similarly, but
we can also use the second statement in the theorem along with the
formulas
LE = LT ∨EQ
GT = ¬LE
GE = ¬LT
NE = ¬EQ
If P is an n-place predicate and f1, f2, . . . , fn : N k →N, we can form
the k-place predicate Q = P (f1, . . . , fn), and the characteristic function χQ is
obtained from χP and f1, . . . , fn by composition. Using Theorem 10.6, we see that
arbitrarily complicated predicates constructed using relational and logical operators,
such as
(f1 = (3f2)2 ∧(f3 < f4 + f5)) ∨¬(P ∨Q)
are primitive recursive as long as the basic constituents (in this case, the functions
f1, . . . , f5 and the predicates P and Q) are.
Theorem 10.7
Suppose f1, f2, . . . , fk are primitive recursive functions from N m to N,
P1, P2, . . . , Pk are primitive recursive n-place predicates, and for every
X ∈N n, exactly one of the conditions P1(X), . . . , Pk(X) is true. Then
the function f : N n →N deﬁned by
f (X) =
⎧
⎪⎪⎨
⎪⎪⎩
f1(X)
if P1(X) is true
f2(X)
if P2(X) is true
. . .
fk(X)
if Pk(X) is true
is primitive recursive.
Proof
The last of the three assumptions guarantees that f is unambiguously
deﬁned and that
f = f1 ∗χP1 + f2 ∗χP2 + · · · + fk ∗χPk
The result follows from the fact that all the functions appearing on the
right side of this formula are primitive recursive.

10.1
Primitive Recursive Functions
337
EXAMPLE 10.8
The Mod and Div Functions
For natural numbers x and y with y > 0, we denote by Div(x, y) and Mod(x, y) the integer
quotient and remainder, respectively, when x is divided by y. For example, Div(8, 5) = 1,
Mod(8, 5) = 3, and Mod(12, 4) = 0. Unless we allow division by 0, these are not total func-
tions; let us say that for any x, Div(x, 0) = 0 and Mod(x, 0) = x. Then the usual formula
x = y ∗Div(x, y) + Mod(x, y)
still holds for every x and y, and
0 ≤Mod(x, y) < y
is true as long as y > 0.
We begin by showing that Mod is primitive recursive. The derivation involves recursion
in the ﬁrst variable, and for this reason we let
R(x, y) = Mod(y, x)
In order to show that Mod is primitive recursive, it is sufﬁcient to show that R is, because
Mod can be obtained from R, p2
2, and p2
1 by composition. The following formulas can be
veriﬁed easily.
R(x, 0) = Mod(0, x) = 0
R(x, k + 1) = Mod(k + 1, x)
=
⎧
⎪⎨
⎪⎩
R(x, k) + 1
if x ̸= 0 and R(x, k) + 1 < x
0
if x ̸= 0 and R(x, k) + 1 = x
k + 1
if x = 0
For example,
R(5, 6 + 1) = Mod(7, 5) = Mod(6, 5) + 1
since 5 ̸= 0 and Mod(6, 5) + 1 = 1 + 1 < 5, and
R(5, 9 + 1) = Mod(10, 5) = 0
since 5 ̸= 0 and Mod(9, 5) + 1 = 4 + 1 = 5. The function h deﬁned by
h(x1, x2, x3) =
⎧
⎪⎨
⎪⎩
x3 + 1
if x1 ̸= 0 and x3 + 1 < x1
0
if x1 ̸= 0 and x3 + 1 = x1
x2 + 1
if x1 = 0
is not a total function, since it is undeﬁned if x1 ̸= 0 and x3 + 1 > x1. However, the modi-
ﬁcation
h(x1, x2, x3) =
⎧
⎪⎨
⎪⎩
x3 + 1
if x1 ̸= 0 and x3 + 1 < x1
0
if x1 ̸= 0 and x3 + 1 ≥x1
x2 + 1
if x1 = 0
works just as well. The function R is obtained by primitive recursion from C1
0 and this modi-
ﬁed h, and Theorem 10.7 implies that h is primitive recursive. Therefore, so are R and Mod.
The function Div can now be handled in a similar way. If we deﬁne Q(x, y) to be
Div(y, x), then it is not hard to check that Q is obtained by primitive recursion from C1
0

338
C H A P T E R 10
Computable Functions
and the primitive recursive function h1 deﬁned by
h1(x1, x2, x3) =
⎧
⎪⎨
⎪⎩
x3
if x1 ̸= 0 and Mod(x2, x1) + 1 < x1
x3 + 1
if x1 ̸= 0 and Mod(x2, x1) + 1 = x1
0
if x1 = 0
(Note that for any choice of (x1, x2, x3), precisely one of the predicates appearing in this
deﬁnition is true.)
10.2 QUANTIFICATION, MINIMALIZATION,
AND μ-RECURSIVE FUNCTIONS
The operations that can be applied to predicates to produce new ones include
not only logical operations such as AND, but also universal and existential quan-
tiﬁers. A simple example is provided by the two-place predicate Sq deﬁned as
follows:
Sq(x, y) = (y2 = x)
Applying the quantiﬁer “there exists” to the second variable produces the one-place
predicate PerfectSquare, deﬁned by
PerfectSquare(x) = (there exists y with y2 = x)
For a second example, suppose that for x ∈N, sx denotes the xth element of
{0, 1}∗with respect to canonical order, and let Tu be a universal Turing machine.
We can consider the two-place predicate H(x, y) deﬁned by
H(x, y) = (Tu halts after exactly y moves on input sx)
and its existential quantiﬁcation
Halts(x) = (there exists y such that Tu halts after y moves on input sx)
The predicate Sq is primitive recursive, and we will see later in this chapter that
H is too. In any case, both are computable. The predicate Halts is certainly not com-
putable, because if we could decide for every x whether Halts (x) is true, we would
be able to decide the halting problem (see Section 9.2). Therefore, the operation
of quantiﬁcation does not preserve either computability or primitive recursiveness.
However, the two-place predicate EH deﬁned by
EH(x, k) = (there exists y ≤k such that Tu halts after y moves on input sx)
is computable, and we will see shortly that this restricted type of quantiﬁcation
also preserves primitive recursiveness. In a similar way, we let
ESq(x, k) = (there exists y ≤k such that y2 = x)
ESq is also primitive recursive.
There is an important difference between these two examples. For every natu-
ral number n, n ≤n2. If we determine that Sq(x, y) is false for every y with y ≤x,
then any y satisfying y2 = x would have to be larger than its square; therefore,

10.2
Quantiﬁcation, Minimalization, and μ-Recursive Functions
339
PerfectSquare(x) is false. In other words, the predicate PerfectSquare is exactly
the same as the primitive recursive predicate B deﬁned by B(x) = ESq(x, x). We
have already noticed that Halts is not computable. There is no k, or even a function
k(x), such that for every x, Tu halts on input sx if and only if it halts on input
sx within k moves. The predicate Halts illustrates the fact that if the simple algo-
rithm that comes with such a bound is not available, there may be no algorithm
at all.
Deﬁnition 10.9
Bounded Quantiﬁcations
Let P be an (n + 1)-place predicate. The bounded existential quantiﬁca-
tion of P is the (n + 1)-place predicate EP deﬁned by
EP (X, k) = (there exists y with 0 ≤y ≤k such that P (X, y) is true)
The bounded universal quantiﬁcation of P is the (n + 1)-place predicate
AP deﬁned by
AP (X, k) = (for every y satisfying 0 ≤y ≤k, P (X, y) is true)
Theorem 10.10
If P is a primitive recursive (n + 1)-place predicate, both the predicates
EP and AP are also primitive recursive.
Proof
Introducing two other “bounded operations” will help to simplify the
proof. If n ≥0 and g : N n+1 →N is primitive recursive, then we deﬁne
the functions f1, f2 : N n+1 →N, obtained from g by bounded sums and
bounded products, respectively, as follows. For every X ∈N n and k ≥0,
f1(X, k) =
k

i=0
g(X, i)
f2(X, k) =
k	
i=0
g(X, i)
The bounded product is a natural generalization of the factorial func-
tion, which is obtained by taking n = 0 and by letting the function g have
the value 1 when i = 0 and the value i when i > 0. We could also con-
sider more general sums and products that begin with the i = i0 term, for
any ﬁxed i0 (see Exercise 10.27).
We can write
f1(X, 0) = g(X, 0)
f1(X, k + 1) = f1(X, k) + g(X, k + 1)

340
C H A P T E R 10
Computable Functions
Therefore, f1 is obtained by primitive recursion from the two primitive
recursive functions g1 and h, where g1(X) = g(X, 0) and h(X, y, z) =
z + g(X, y + 1). A very similar argument shows that f2 is also primitive
recursive.
By deﬁnition of bounded universal quantiﬁcation, AP (X, k) is true
if and only if P (X, i) is true for every i with 0 ≤i ≤k. Therefore,
χAP (X, k) = 1 if and only if all the terms χP (X, i) are also 1. This equiv-
alence implies that
χAP (X, k) =
k	
i=0
χP (X, i)
and therefore that AP is primitive recursive.
Saying that there is an i such that 0 ≤i ≤k and P (X, i) is true is
the same as saying that P (X, i) is not always false for these i’s. In other
words,
EP (X, k) = ¬A¬P (X, k)
It follows from this formula that EP is also primitive recursive.
The bounded quantiﬁcations in this section preserve primitive recursiveness
and computability, and the unbounded versions don’t. In order to characterize the
computable functions as those obtained by starting with initial functions and apply-
ing certain operations, we need at least one operation that preserves computability
but not primitive recursiveness—because the initial functions are primitive recur-
sive, and not all computable functions are. The operation of minimalization turns
out to have this feature. It also has a bounded version as well as an unbounded
one.
For an (n + 1)-place predicate P , and a given X ∈N n, we may consider the
smallest value of y for which P (X, y) is true. To turn this operation into a bounded
one, we specify a value of k and ask for the smallest value of y that is less than or
equal to k and satisﬁes P (X, y). There may be no such y (whether or not we bound
the possible choices by k); therefore, because we want the bounded version of our
function to be total, we introduce an appropriate default value for the function in
this case.
Deﬁnition 10.11
Bounded Minimalization
For an (n + 1)-place predicate P , the bounded minimalization of P is the
function mP : N n+1 →N deﬁned by
mP (X, k) =
 min {y | 0 ≤y ≤k and P (X, y)}
if this set is not empty
k + 1
otherwise

10.2
Quantiﬁcation, Minimalization, and μ-Recursive Functions
341
The symbol μ is often used for the minimalization operator, and we some-
times write
mP(X, k) =
kμ y[P (X, y)]
An important special case is that in which P (X, y) is (f (X, y) = 0), for
some f : N n+1 →N. In this case mP is written mf and referred to as
the bounded minimalization of f .
Theorem 10.12
If P is a primitive recursive (n + 1)-place predicate, its bounded mini-
malization mP is a primitive recursive function.
Proof
We show that mP can be obtained from primitive recursive functions by
the operation of primitive recursion. For X ∈N n, mP(X, 0) is 0 if P (X, 0)
is true and 1 otherwise. In order to evaluate mP (X, k + 1), we consider
three cases. First, if there exists y ≤k for which P (X, y) is true, then
mP (X, k + 1) = mP(X, k). Second, if there is no such y but P (X, k + 1)
is true, then mP (X, k + 1) = k + 1. Third, if neither of these conditions
holds, then mP (X, k + 1) = k + 2. It follows that the function mP can be
obtained by primitive recursion from the functions g and h, where
g(X) =
 0
if P (X, 0) is true
1
otherwise
h(X, y, z) =
⎧
⎨
⎩
z
if EP (X, y) is true
y + 1
if ¬EP(X, y) ∧P (X, y + 1) is true
y + 2
if ¬EP(X, y) ∧¬P (X, y + 1) is true
Because P and EP are primitive recursive predicates, the functions g and
h are both primitive recursive.
EXAMPLE 10.13
The nth Prime Number
For n ≥0, let PrNo(n) be the nth prime number: PrNo(0) = 2, PrNo(1) = 3, PrNo(2) = 5,
and so on. Let us show that the function PrNo is primitive recursive.
First we observe that the one-place predicate Prime, deﬁned by
Prime(n) = (n ≥2) ∧¬(there exists y such that y ≥2 ∧y ≤n
.−1 ∧Mod(n, y) = 0)
is primitive recursive, and Prime(n) is true if and only if n is a prime.
For every k, PrNo(k + 1) is the smallest prime greater than PrNo(k). Therefore, if we
can just place a bound on the set of integers greater than PrNo(k) that may have to be tested
in order to ﬁnd a prime, then we can use the bounded minimalization operator to obtain

342
C H A P T E R 10
Computable Functions
PrNo by primitive recursion. The number-theoretic fact that makes this possible was proved
in Example 1.4: For every positive integer m, there is a prime greater than m and no larger
than m! + 1. (If we required m to be 3 or bigger, we could say there is a prime between m
and m!.)
With this in mind, let
P(x, y) = (y > x ∧Prime(y))
Then
PrNo(0) = 2
PrNo(k + 1) = mP(PrNo(k), PrNo(k)! + 1)
We have shown that PrNo can be obtained by primitive recursion from the two functions
C0
2 and h, where
h(x, y) = mP (y, y! + 1)
Therefore, PrNo is primitive recursive.
In deﬁning mP (X, k), the value of the bounded minimalization of a predicate
P at the point (X, k), we specify the default value k + 1 if there are no values of y
in the range 0 ≤y ≤k for which P (X, y) is true. Something like this is necessary
if P is a total function and we want mP to be total. If we want the minimalization to
be unbounded, and we want this operator to preserve computability, a default value
is no longer appropriate: If there is a value k such that P (X, k) is true, there is no
doubt that we can ﬁnd the smallest one, but it might be impossible to determine
that P (X, y) is false for every y and that the default value is therefore the right
one. Forcing the minimalization to be a total function might make it uncomputable;
allowing its domain to be smaller guarantees that it will be computable.
Deﬁnition 10.14
Unbounded Minimalization
If P is an (n + 1)-place predicate, the unbounded minimalization of P is
the partial function MP : N n →N deﬁned by
MP (X) = min {y | P (X, y) is true}
MP (X) is undeﬁned at any X ∈N n for which there is no y satisfying
P (X, y).
The notation μy[P (X, y)] is also used for MP (X). In the special case
in which P (X, y) = (f (X, y) = 0), we write MP = Mf and refer to this
function as the unbounded minimalization of f .
The fact that we want MP to be a computable partial function for any com-
putable predicate P also has another consequence. Suppose the algorithm we are
relying on for computing MP (X) is simply to evaluate P (X, y) for increasing val-
ues of y, and that P (X, y0) is undeﬁned. Although there might be a value y1 > y0

10.2
Quantiﬁcation, Minimalization, and μ-Recursive Functions
343
for which P (X, y1) is true, we will never get around to considering P (X, y1) if we
get stuck in an inﬁnite loop while trying to evaluate P (X, y0). In order to avoid
this problem, we stipulate that unbounded minimalization should be applied only
to total predicates or total functions.
Unbounded minimalization is the last of the operations we need in order to
characterize the computable functions. In the deﬁnition below, this operator is
applied only to predicates deﬁned by some numeric function being zero.
Deﬁnition 10.15
μμμ-Recursive Functions
The set M of μ-recursive, or simply recursive, partial functions is deﬁned
as follows.
1. Every initial function is an element of M.
2. Every function obtained from elements of M by composition or primitive
recursion is an element of M.
3. For every n ≥0 and every total function f : N n+1 →N in M, the function
Mf : N n →N deﬁned by
Mf (X) = μy[f (X, y) = 0]
is an element of M.
Just as in the case of primitive recursive functions, a function is in the set M
if and only if it has a ﬁnite, step-by-step derivation, where at each step either a
new initial function is introduced or one of the three operations is applied to initial
functions, to functions obtained earlier in the derivation, or to both. As long as
unbounded minimalization is not used, the function obtained at each step in such
a sequence is primitive recursive. Once unbounded minimization appears in the
sequence, the functions may cease to be primitive recursive or even total. If f
is obtained by composition or primitive recursion, it is possible for f to be total
even if not all the functions from which it is obtained are. Therefore, it is conceiv-
able that in the derivation of a μ-recursive function, unbounded minimalization
can be used more than once, even if its ﬁrst use produces a nontotal function.
However, in the proof of Theorem 10.20 we show that every μ-recursive func-
tion actually has a derivation in which unbounded minimalization is used only
once.
Theorem 10.16
All μ-recursive partial functions are computable.
Proof
For a proof using structural induction, it is sufﬁcient to show that if
f : N n+1 →N is a computable total function, then its unbounded mini-
malization Mf is a computable partial function.

344
C H A P T E R 10
Computable Functions
If Tf is a Turing machine computing f , a TM T computing Mf oper-
ates on an input X by computing f (X, 0), f (X, 1), . . . , until it discovers
an i for which f (X, i) = 0, and halting in ha with i as its output. If
there is no such i, the computation continues forever, which is acceptable
because Mf (X) is undeﬁned in that case.
10.3 G ¨ODEL NUMBERING
In the 1930s, the logician Kurt G¨odel developed a method of “arithmetizing” a
formal axiomatic system by assigning numbers to statements and formulas, so as
to be able to describe relationships between objects in the system using relationships
between the corresponding numbers. His ingenious use of these techniques led to
dramatic and unexpected results about logical systems; G¨odel’s incompleteness
theorem says, roughly speaking, that any formal system comprehensive enough to
include the laws of arithmetic must, if it is consistent, contain true statements that
cannot be proved within the system.
The numbering schemes introduced by G¨odel have proved to be useful in a
number of other settings. We will use this approach to arithmetize Turing
machines—to describe operations of TMs, and computations performed by TMs,
in terms of purely numeric operations. As a result, we will be able to show that
every Turing-computable function is μ-recursive.
Most G¨odel-numbering techniques depend on a familiar fact about positive
integers: Every positive integer can be factored as a product of primes (1 is the empty
product), and this factorization is unique except for differences in the order of the
factors. Because describing a Turing machine requires describing a sequence of tape
symbols, we start by assigning G¨odel numbers to sequences of natural numbers.
Deﬁnition 10.17
The G ¨odel Number of a Sequence of
Natural Numbers
For every n ≥1 and every ﬁnite sequence x0, x1, . . . , xn−1 of n natural
numbers, the G¨odel number of the sequence is the number
gn(x0, x1, . . . , xn−1) = 2x03x15x2 . . . (PrNo(n −1))xn−1
where PrNo(i) is the ith prime (Example 10.13).
The G¨odel number of every sequence is positive, and every positive integer is
the G¨odel number of a sequence. The function gn is not one-to-one; for example,
gn(0, 2, 1) = gn(0, 2, 1, 0, 0) = 203251
However, if gn(x0, x1, . . . , xm) =gn(y0, y1, . . . , ym, ym+1 . . . ym+k), then
m
	
i=0
PrNo(i)xi =
m
	
i=0
PrNo(i)yi
m+k
	
i=m+1
PrNo(i)yi

10.3
G¨odel Numbering
345
and because a number can have only one prime factorization, we must have yi = xi
for 0 ≤i ≤m and yi = 0 for i > m. Therefore, two sequences having the same
G¨odel number and ending with the same number of 0’s are identical. In particular,
for every n ≥1, every positive integer is the G¨odel number of at most one sequence
of n integers.
For every n, the G¨odel numbering we have deﬁned for sequences of length n
determines a function from N n to N. We will be imprecise and use the name gn
for any of these functions. All of them are primitive recursive.
If we start with a positive integer g, we can decode g to ﬁnd a sequence x0,
x1, . . . , xn whose G¨odel number is g by factoring g into primes. For each i, xi is
the number of times PrNo(i) appears as a factor of g. For example, the number
59895 has the prime factorization
59895 = 3251113 = 20325170113
and is therefore the G¨odel number of the sequence 0,2,1,0,3 or any other sequence
obtained from this by adding extra 0’s. The prime number 31 is the G¨odel number
of the sequence 0,0,0,0,0,0,0,0,0,0,1, since 31 =PrNo(10). This type of calculation
will be needed often enough that we introduce a function just for this purpose.
EXAMPLE 10.18
The Power to Which a Prime Is Raised in the Factorization of x
The function Exponent: N 2 →N is deﬁned as follows:
Exponent(i, x) =

the exponent of PrNo(i) in x’s prime factorization
if x > 0
0
if x = 0
For example, Exponent(4, 59895) = 3, because the fourth prime, 11, appears three times as
a factor of 59895. (Remember, 2 is the zeroth prime.)
We can show that Exponent is primitive recursive by expressing Exponent(i, x) in a way
that involves bounded minimalization. In order to make the formulas look less intimidating,
let us temporarily use the notation
M(x, i, y) = Mod(x, PrNo(i)y)
Saying that PrNo(i)y divides x evenly is the same as saying that M(x, i, y) = 0, and
this is true if and only if y ≤Exponent(i, x). Therefore, if y = Exponent(i, x) + 1, then
M(x, i, y) > 0, and this is the smallest value of y for which M(x, i, y) > 0. In other words,
Exponent(i, x) + 1 = μy[M(x, i, y) > 0]
or
Exponent(i, x) = μy[M(x, i, y) > 0]
.−1
However, if we want to compute Exponent(i, x) this way, by searching for the smallest y
satisfying M(x, i, y) > 0, we don’t have to consider any values bigger than x, because x is
already such a value: We know that
PrNo(i)x > x

346
C H A P T E R 10
Computable Functions
so that
M(x, i, y) = Mod(x, PrNo(i)y) = x > 0
Therefore,
Exponent(i, x) =
xμ y[Mod(x, PrNo(i)y) > 0]
.−1
All the operations involved in the right-hand expression preserve primitive recursiveness,
and it follows that Exponent is primitive recursive.
For many functions that are deﬁned recursively, the deﬁnition does not make
it obvious that the function is obtained by using primitive recursion. For example,
the right side of the formula
f (n + 1) = f (n) + f (n −1)
is not of the form h(n, f (n)), since it also depends on f (n −1). In general f (n +
1) might depend on several, or even all, of the terms f (0), f (1), . . . , f (n).
This type of recursion is known as course-of-values recursion, and it is related to
primitive recursion in the same way that strong induction (Example 1.24) is related
to ordinary mathematical induction.
G¨odel numbering provides us with a way to reformulate deﬁnitions like these.
Suppose f (n + 1) depends on some or all of the numbers f (0), . . . , f (n), and
possibly also directly on n. In order to obtain f by using primitive recursion, we
would like another function f1 for which
1.
Knowing f1(n) would allow us to calculate f (n).
2.
f1(n + 1) depends only on n and f1(n).
If we could relax the requirement that f1(n) be a number, we could consider the
entire sequence f1(n) = (f (0), f (1), . . . , f (n)). Condition 1 is satisﬁed, because
f (n) is simply the last term of the sequence f1(n). Since f (n + 1) can be expressed
in terms of n and f (0), . . . , f (n), the sequence (f (0), . . . , f (n), f (n + 1)) can be
said to depend only on n and the sequence (f (0), . . . , f (n)), so that we also have
condition 2. To make this intuitive idea work, all we need to do is to use the G¨odel
numbers of the sequences, instead of the sequences themselves. Rather than saying
that f (n + 1) depends on f (0), . . . , f (n), we say that f (n + 1) depends on the
single value gn(f (0), . . . , f (n)). The two versions are intuitively equivalent, since
there is enough information in gn(f (0), . . . , f (n)) to ﬁnd each number f (i).
Theorem 10.19
Suppose that g : N n →N and h : N n+2 →N are primitive recursive
functions, and f : N n+1 →N is obtained from g and h by course-of-
values recursion; that is,
f (X, 0) = g(X)
f (X, k + 1) = h(X, k, gn(f (X, 0), . . . , f (X, k)))

10.3
G¨odel Numbering
347
Then f is primitive recursive.
Proof
First we deﬁne f1 : N n+1 →N by the formula
f1(X, k) = gn(f (X, 0), f (X, 1), . . . , f (X, k))
Then f can be obtained from f1 by the formula
f (X, k) = Exponent(k, f1(X, k))
Therefore, it will be sufﬁcient to show that f1 is primitive recursive.
We have
f1(X, 0) = gn(f (X, 0)) = 2f (X,0) = 2g(X)
f1(X, k + 1) =
k+1
	
i=0
PrNo(i)f (X,i)
=
k	
i=0
PrNo(i)f (X,i) ∗PrNo(k + 1)f (X,k+1)
= f1(X, k) ∗PrNo(k + 1)h(X,k,f1(X,k))
= h1(X, k, f1(X, k))
where the function h1 is deﬁned by
h1(X, y, z) = z ∗PrNo(y + 1)h(X,y,z)
Because 2g and h1 are primitive recursive and f1 is obtained from these
two by primitive recursion, f1 is also primitive recursive.
Now we are ready to apply our G¨odel numbering techniques to Turing machines.
A TM move can be interpreted as a transformation of one TM conﬁguration to
another, and we need a way of characterizing a Turing machine conﬁguration by
a number.
We begin by assigning a number to each state. The halt states ha and hr are
assigned the numbers 0 and 1, and the elements of the state set Q will be assigned
the numbers 2, 3, . . . , s, with 2 representing the initial state. The tape head position
is simply the number of the current tape square, and we assign numbers to the tape
symbols by using 0 for the blank symbol , and 1, 2, . . . , t for the distinct
nonblank tape symbols. Now we can deﬁne the current tape number of the TM to
be the G¨odel number of the sequence of symbols currently on the tape. Because we
are identifying  with 0, the tape number does not depend on how many blanks
we include at the end of this sequence. The number of a blank tape is 1.
The conﬁguration of a TM is determined by the state, tape head position, and
current contents of the tape, and we deﬁne the conﬁguration number to be the
number
gn(q, P, tn)

348
C H A P T E R 10
Computable Functions
where q is the number of the current state, P is the current head position, and tn is
the current tape number. The most important feature of the conﬁguration number
is that from it we can reconstruct all the details of the conﬁguration; we will be
more explicit about this in the next section.
10.4 ALL COMPUTABLE FUNCTIONS ARE
μ-RECURSIVE
Suppose that f : N n →N is a partial function computed by the TM T . In order
to compute f (X) for an n-tuple X, T starts in the standard initial conﬁguration
corresponding to X, carries out a sequence of moves, and ends up in an accepting
conﬁguration, if X is in the domain of f , with a string representing f (X) on
the tape. At the end of the last section we introduced conﬁguration numbers for
T , which are G¨odel numbers of conﬁgurations; we can interpret each move of
the computation as transforming one conﬁguration number to another, and if the
computation halts, we can interpret the entire computation as transforming an initial
conﬁguration number c to the resulting halting-conﬁguration number fT (c).
We can therefore write
f (X) = ResultT (fT (InitConﬁg(n)(X)))
where fT is the function described in the ﬁrst paragraph; InitConﬁg(n)(X) is the
G¨odel number of the initial conﬁguration corresponding to the n-tuple X; and
ResultT : N →N assigns to j the number y if j represents an accepting conﬁg-
uration of T in which output corresponding to y is on the tape, and an appropriate
default value otherwise.
In order to show that f is μ-recursive, it will be sufﬁcient to show that each of
the three functions on the right side of this formula is. The two outer ones, ResultT
and InitConﬁg(n), are actually primitive recursive. That is too much to expect of fT ,
which is deﬁned in a way that involves unbounded minimalization and is deﬁned
only at conﬁguration numbers corresponding to n-tuples in the domain of f .
The list below itemizes these three, as well as several auxiliary functions that
will be useful in the description of fT .
1.
The function InitConﬁg(n) : N n →N does not actually depend on the Turing
machine, because of our assumption that the initial state of a TM is always
given the number 2. The G¨odel number of the initial conﬁguration depends
on the G¨odel number of the initial tape; in order to show that InitConﬁg(n) is
primitive recursive, it is sufﬁcient to show that t(n) : N n →N is, where
t(n)(x1, . . . , xn) is the tape number of the tape containing the string
1x11x2 . . . 1xn
The proof is by mathematical induction on n and is left to the exercises.
The function ResultT is one of several in this discussion whose value at
m will depend on whether m is the number of a conﬁguration of T . The next
step, therefore, is to consider two related predicates.

10.4
All Computable Functions are μ-Recursive
349
2.
IsConﬁgT is the one-place predicate deﬁned by
IsConﬁgT (n) = (n is a conﬁguration number for T )
and IsAcceptingT is deﬁned by
IsAcceptingT (m) =
 0
if IsConﬁgT (m) ∧Exponent(0, m) = 0
1
otherwise
IsAcceptingT (m) is 0 if and only if m is the number of an accepting
conﬁguration for T , and this predicate will be primitive recursive if
IsConﬁgT is.
Let sT be one more than the number of nonhalting states of T , which are
numbered starting with 2, and let tsT be the number of nonblank tape
symbols of T . A number m is a conﬁguration number for T if and only if
m = 2q3p5tn
where q ≤sT , p is arbitrary, and tn is the G¨odel number of a sequence of
natural numbers, each one between 0 and tsT .
The statement that m is of the general form 2a3b5c can be expressed by
saying that
(m ≥1) ∧(for every i, i ≤2 ∨Exponent(i, m) = 0)
For numbers m of this form, the conditions on q and tn are equivalent to the
statement
(Exponent(0, m) ≤sT ) ∧(Exponent(2, m) ≥1)
∧(for every i, Exponent(i, tn) ≤tsT )
In order to show that the conjunction of these two predicates is primitive
recursive, it is sufﬁcient to show that both of the universal quantiﬁcations can
be replaced by bounded universal quantiﬁcations. This is true because
Exponent(i, n) = 0 when i > n; the ﬁrst occurrence of “for every i” can be
replaced by “for every i ≤m” and the second by “for every i ≤tn.”
Therefore, IsConﬁgT is primitive recursive.
3.
Next, we check that the function ResultT : N →N is primitive recursive.
Because the tape number for the conﬁguration represented by n is
Exponent(2, n) and the prime factors of the tape number correspond to the
squares with nonblank symbols, we may write
ResultT (n) =
 HighestPrime(Exponent(2, n))
if IsConﬁgT (n)
0
otherwise
where for any positive k, HighestPrime(k) is the number of the largest prime
factor of k, and HighestPrime(0) = 0 (e.g., HighestPrime(2355192) = 7,
because 19 is PrNo(7)). It is not hard to see that the function HighestPrime
is primitive recursive (Exercise 10.22), and it follows that ResultT is.

350
C H A P T E R 10
Computable Functions
We are ready to consider fT , the numeric function that represents the process-
ing done by T . Without loss of generality, we can make the simplifying assumption
that T never attempts to move its tape head left from square 0.
4.
The current state, tape head position, tape symbol, and tape number can all
be calculated from the conﬁguration number. The formulas are
State(m) = Exponent(0, m)
Posn(m) = Exponent(1, m)
TapeNumber(m) = Exponent(2, m)
Symbol(m) = Exponent(Posn(m)), TapeNumber(m))
for every m that is a conﬁguration number for T , and 0 otherwise. Because
IsConﬁgT is a primitive recursive predicate, all four functions are primitive
recursive.
A single move is determined by considering cases, and may result in
new values for these four quantities. We have the corresponding functions
NewState, NewPosn, NewTapeNumber, and NewSymbol. NewState(m), for
example, is 0 if m is not a conﬁguration number; it is the same as State(m)
if m represents a conﬁguration from which T cannot move; and otherwise, it
has a possibly different value that is determined by the ordered pair
(State(m), Symbol(m)). All the cases can be described by primitive recursive
predicates, and the resulting function is therefore primitive recursive.
The same argument applies to NewSymbol, and to NewPosn as well
except that when there actually is a move, the new position may be obtained
from the old by adding or subtracting 1. The NewTapeNumber function is
slightly more complicated, partly because the new value depends on
Posn(m), NewSymbol(m), and Symbol(m) as well as the old
TapeNumber(m), and partly because the numeric operations required to
obtain it are more complicated. The details are left to Exercise 10.35, and the
conclusion is still that all four of these New functions are primitive recursive.
5.
The function MoveT : N →N is deﬁned by
MoveT (m) =
⎧
⎪⎪⎨
⎪⎪⎩
gn(NewState(m), NewPosn(m), NewTapeNum(m))
if IsConﬁgT (m)
0
otherwise
If m represents a conﬁguration of T from which T can move, then
MoveT (m) represents the conﬁguration after the next move; if m represents a
conﬁguration of T from which no move is possible, then MoveT (m) = m;
and if m does not represent a conﬁguration of T , MoveT (m) = 0. The
function MoveT is primitive recursive.

10.4
All Computable Functions are μ-Recursive
351
6.
We can go from a single move to a sequence of k moves by considering the
function MovesT : N →N deﬁned by
MovesT (m, 0) =
 m
if IsConﬁgT (m)
0
otherwise
MovesT (m, k + 1) =
 MoveT (Moves(m, k))
if IsConﬁgT (m)
0
otherwise
MovesT is obtained by primitive recursion from two primitive recursive
functions and is therefore primitive recursive. For a conﬁguration number m,
we may describe MovesT (m, k) as the conﬁguration number after k moves, if
T starts in conﬁguration m—or, if T is unable to make as many as k moves
from conﬁguration m, as the number of the last conﬁguration T reaches
starting from conﬁguration m.
7.
Finally, we deﬁne NumberOfMovesToAcceptT : N →N by the formula
NumberOfMovesToAcceptT (m) = μk[AcceptingT (MovesT (m, k)) = 0]
and fT : N →N by
fT (m) = MovesT (m, NumberOfMovesToAcceptT (m))
If m is a conﬁguration number for T and T eventually accepts when starting
from conﬁguration m, then NumberOfMovesToAcceptT (m) is the number of
moves from that point before T accepts, and fT (m) is the number of the
accepting conﬁguration that is eventually reached. For any other m, both
functions are undeﬁned. NumberOfMovesToAcceptT is μ-recursive because it
is obtained from a primitive recursive (total) function by unbounded
minimalization, and fT is μ-recursive because it is obtained by composition
from μ-recursive functions.
We have essentially proved Theorem 10.20.
Theorem 10.20
Every Turing-computable partial function from N n to N is μ-recursive.
The Rest of the Proof
Suppose the TM T computes f : N n →N. If
f (X) is deﬁned, then when T begins in the conﬁguration InitConﬁg(n)(X),
it eventually accepts. Therefore, fT (InitConﬁg(n)(X)) is the conﬁguration
number of the accepting conﬁguration (ha, 1f (X)), and when ResultT is
applied to this conﬁguration number it produces f (X). On the other hand,
suppose that f (X) is undeﬁned. Then T fails to accept input X, and this
means that fT (InitConﬁg(n)(X)) is undeﬁned. Therefore, f is identical to
the μ-recursive function
ResultT ◦fT ◦InitConﬁg(n)
Theorem 10.20 can be generalized. G¨odel numbering lets us extend the deﬁ-
nitions of primitive recursive and μ-recursive to functions involving strings, and it

352
C H A P T E R 10
Computable Functions
is relatively straightforward to derive analogues of Theorems 10.16 and 10.20 in
this more general setting.
10.5 OTHER APPROACHES TO
COMPUTABILITY
In the last section of this chapter we mention brieﬂy two other approaches to
computable functions that turn out to be equivalent to the two approaches we have
studied.
Unrestricted grammars provide a way of generating languages, and they can
also be used to compute functions. If G = (V, , S, P ) is a grammar, and f is a
partial function from ∗to ∗, G is said to compute f if there are variables A,
B, C, and D in the set V such that for every x and y in ∗,
f (x) = y if and only if AxB ⇒∗
G CyD
It can be shown, using arguments comparable to the ones in Section 8.3, that the
functions computable in this way are the same as the ones that can be computed
by Turing machines.
Computer programs can be viewed as computing functions from strings to
strings. Ignoring limitations imposed by a particular programming language or
implementation, and assuming that there is an unlimited amount of memory, no
limit to the size of integers, and so on, functions that can be computed this way
include all the Turing-computable functions.
We don’t need all the features present in modern high-level languages like
C to compute these functions. For numeric functions, we need to be able to per-
form certain basic operations: assignment statements; algebraic operations including
addition and subtraction; statements that transfer control, depending on the values
of certain variables; and, depending on the conventions we adopt regarding input
and output, perhaps “read” and “write” statements. Even with languages this simple,
it is possible to compute all Turing-computable functions. One approach to proving
this would be to write a program in such a language to simulate an arbitrary TM.
This might involve some sort of arithmetization of TMs similar to G¨odel number-
ing: One integer variable would represent the state, another the head position, a
third the tape contents, and so on. Another approach, comparable to Theorems 10.4
and 10.16, would be to show that the set of functions computable using the lan-
guage contains the initial functions and is closed under all the operations permitted
for μ-recursive functions.
The Church-Turing thesis asserts that every function that can be computed
using a high-level programming language is Turing-computable. A direct proof
might be carried out by building a TM that can simulate each feature of the language
and can therefore execute a program written in the language. Another approach
would be along the lines of Theorem 10.20. Program conﬁgurations comparable to
TM conﬁgurations can be described by specifying a statement (the next statement
to be executed) and the current values of all variables. Conﬁguration numbers
can be deﬁned, and each step in the execution of the program can be viewed as

Exercises
353
a transformation of one conﬁguration number to another, just as in the proof of
Theorem 10.20. As a result, every function computed by the program is μ-recursive.
EXERCISES
10.1.
Let F be the set of partial functions from N to N. Then F = C ∪U,
where the functions in C are computable and the ones in U are not.
Show that C is countable and U is not.
10.2.
The busy-beaver function b : N →N is deﬁned as follows. The value
b(0) is 0. For n > 0, there are only a ﬁnite number of Turing machines
having n nonhalting states q0, q1, . . . , qn−1 and tape alphabet {0, 1}. Let
T0, T1, . . . , Tm be the TMs of this type that eventually halt on input 1n,
and for each i, let ni be the number of 1’s that Ti leaves on its tape when
it halts after processing the input string 1n. The number b(n) is deﬁned to
be the maximum of the numbers n0, . . . , nm.
Show that the total function b : N →N is not computable.
Suggestion: Suppose for the sake of contradiction that Tb is a TM that
computes b. Then we can assume without loss of generality that Tb has
tape alphabet {0, 1}.
10.3.
Let f : N →N be the function deﬁned as follows: f (0) = 0, and for
n > 0, f (n) is the maximum number of moves a TM with n non-halting
states and tape alphabet {0, 1} can make if it starts with input 1n and
eventually halts. Show that f is not computable.
10.4.
Deﬁne f : N →N by letting f (0) be 0, and for n > 0 letting f (n) be
the maximum number of 1’s that a TM with n states and no more than n
tape symbols can leave on the tape, assuming that it starts with input 1n
and always halts. Show that f is not computable.
10.5.
Show that the uncomputability of the busy-beaver function (Exercise
10.2) implies that the halting problem is undecidable.
10.6.
†Suppose we deﬁne bb(0) to be 0, and for n > 0 we deﬁne bb(n) to be
the maximum number of 1’s that can be printed by a TM with n states
and tape alphabet {0, 1}, assuming that it starts with a blank tape and
eventually halts. Show that bb is not computable.
10.7.
Let b : N →N be the busy-beaver function in Exercise 10.2. Show that
b is eventually larger than every computable function; in other words, for
every computable total function g : N →N, there is an integer k such
that b(n) > g(n) for every n ≥k.
10.8.
Suppose we deﬁne b2(0) to be 0, and for n > 0 we deﬁne b2(n) to be the
largest number of 1’s that can be left on the tape of a TM with two states
and tape alphabet {0, 1}, if it starts with input 1n and eventually halts.
a. Give a convincing argument that b2 is computable.
b. Is the function bk (identical to b2 except that “two states” is replaced
by “k states”) computable for every k ≥2? Why or why not?

354
C H A P T E R 10
Computable Functions
10.9.
Show that if f : N →N is a total function, then f is computable if and
only if the decision problem. “Given natural numbers n and C, is
f (n) > C?” is solvable.
10.10.
Suppose that instead of including all constant functions in the set of
initial functions, C0
0 is the only constant function included. Describe what
the set PR obtained by Deﬁnition 10.3 would be.
10.11.
Suppose that in Deﬁnition 10.3 the operation of composition is allowed
but that of primitive recursion is not. What functions are obtained?
10.12.
If g(x) = x and h(x, y, z) = z + 2, what function is obtained from g and
h by primitive recursion?
10.13.
Here is a primitive recursive derivation. f0 = C0
1; f1 = C2
0; f2 is
obtained from f0 and f1 by primitive recursion; f3 = p2
2; f4 is obtained
from f2 and f3 by composition; f5 = C0
0; f6 is obtained from f5 and f4
by primitive recursion; f7 = p1
1; f8 = p3
3; f9 = s; f10 is obtained from
f9 and f8 by composition; f11 is obtained from f7 and f10 by primitive
recursion; f12 = p2
1; f12 is obtained from f6 and f12 by composition; f14
is obtained from f11, f12, and f3 by composition; and f15 is obtained
from f5 and f14 by primitive recursion.
Give simple formulas for f2, f6, f14, and f15.
10.14.
Find two functions g and h such that the function f deﬁned by f (x) =
x2 is obtained from g and h by primitive recursion.
10.15.
Give complete primitive recursive derivations for each of the following
functions.
a. f : N 2 →N deﬁned by f (x, y) = 2x + 3y
b. f : N →N deﬁned by f (n) = n!
c. f : N →N deﬁned by f (n) = 2n
d. f : N →N deﬁned by f (n) = n2
.−1
e. f : N 2 →N deﬁned by f (x, y) = |x −y|
10.16.
Show that for any n ≥1, the functions Addn and Multn from N n to N,
deﬁned by
Addn(x1, . . . , xn) = x1 + x2 + · · · + xn
Multn(x1, . . . , xn) = x1 ∗x2 ∗· · · ∗xn
respectively, are both primitive recursive.
10.17.
Show that if f : N →N is primitive recursive, A ⊆N is a ﬁnite set,
and g is a total function agreeing with f at every point not in A, then g
is primitive recursive.
10.18.
Show that if f : N →N is an eventually periodic total function, then f
is primitive recursive. “Eventually periodic” means that for some n0 and
some p > 0, f (x + p) = f (x) for every x ≥n0.
10.19.
Show that each of the following functions is primitive recursive.
a. f : N 2 →N deﬁned by f (x, y) = max{x, y}

Exercises
355
b. f : N 2 →N deﬁned by f (x, y) = min{x, y}
c. f : N →N deﬁned by f (x) = ⌊√x⌋(the largest natural number less
than or equal to √x)
d. f : N →N deﬁned by f (x) = ⌊log2(x + 1)⌋
10.20.
Suppose P is a primitive recursive (k + 1)-place predicate, and f and g
are primitive recursive functions of one variable. Show that the
predicates Af,gP and Ef,gP deﬁned by
Af,gP (X, k) = (for every i with f (k) ≤i ≤g(k), P (X, i))
Ef,gP (X, k) = (there exists i with f (k) ≤i ≤g(k) such that P (X, i))
are both primitive recursive.
10.21.
Show that if g : N 2 →N is primitive recursive, then f : N →N
deﬁned by
f (x) =
x

i=0
g(x, i)
is primitive recursive.
10.22.
Show that the function HighestPrime introduced in Section 10.4 is
primitive recursive.
10.23.
In addition to the bounded minimalization of a predicate, we might
deﬁne the bounded maximalization of a predicate P to be the function
mP deﬁned by
mP(X, k) =
 max{y ≤k | P (X, y) is true}
if this set is not empty
0
otherwise
a. Show mP is primitive recursive by ﬁnding two primitive recursive
functions from which it can be obtained by primitive recursion.
b. Show mP is primitive recursive by using bounded minimalization.
10.24.
†Consider the function f deﬁned recursively as follows:
f (0) = f (1) = 1; for x > 0, f (x) = 1 + f (⌊√x⌋)
Show that f is primitive recursive.
10.25.
a. Show that the function f : N 2 →N deﬁned by f (x, y) = (the
number of integer divisors of x less than or equal to y) is primitive
recursive. Use this to show that the one-place predicate Prime (see
Example 10.13) is primitive recursive.
b. Show that the function f : N 3 →N deﬁned by f (x, y, z) = (the
number of integers less than or equal to z that are divisors of both x
and y) is primitive recursive. Use this to show that the two-place
predicate P deﬁned by P (x, y) = (x and y are relatively prime) is
primitive recursive.
10.26.
†Show that the following functions from N to N are primitive recursive.

356
C H A P T E R 10
Computable Functions
a. f (n) = the leftmost digit in the decimal representation of 2x
b. f (n) = the nth digit of the inﬁnite decimal expansion of
√
2 = 1.414212 . . . (i.e., f (0) = 1, f (1) = 4, and so on)
10.27.
†Show that if g : N n+1 →N is primitive recursive, and l, m : N →N
are both primitive recursive, then the functions f1 and f2 from N n+1 to
N deﬁned by
f1(X, k) =
m(k)
	
i=l(k)
g(X, i)
f2(X, k) =
m(k)

i=l(k)
g(X, i)
are primitive recursive.
10.28.
Suppose  is an alphabet containing all the symbols necessary to
describe numeric functions, so that a primitive recursive derivation is a
string over . Suppose in addition that there is an algorithm capable of
deciding, for every string x over , whether x is a legitimate primitive
recursive derivation of a function of one variable. Then in principle we
can consider the strings in ∗in canonical order, and for each one that is
a primitive recursive derivation of a function f from N to N, we can
include f in a list of primitive recursive functions of one variable. The
resulting list f0, f1, . . . , will contain duplicates, because functions can
have more than one primitive recursive derivation, but it contains every
primitive recursive function of one variable.
a. With these assumptions, show that there is a computable total
function from N to N that is not primitive recursive.
b. Every μ-recursive function from N to N has a “μ-recursive
derivation.” What goes wrong when you try to adapt your argument
in part (a) to show that there is a computable function from N to N
that is not μ-recursive?
10.29.
Give an example to show that the unbounded universal quantiﬁcation of
a computable predicate need not be computable.
10.30.
Show that the unbounded minimalization of any predicate can be written
in the form μy[f (X, y) = 0], for some function f .
10.31.
The set of μ-recursive functions was deﬁned to be the smallest set that
contains the initial functions and is closed under the operations of
composition, primitive recursion, and unbounded minimalization (applied
to total functions). In the deﬁnition, no explicit mention is made of the
bounded operators (universal and existential quantiﬁcation, bounded
minimalization). Do bounded quantiﬁcations applied to μ-recursive
predicates always produce μ-recursive predicates? Does bounded
minimalization applied to μ-recursive predicates or functions always
produce μ-recursive functions? Explain.
10.32.
†Consider the following problem: Given a Turing machine T computing
some partial function f , is f a total function? Is this problem decidable?
Explain.

Exercises
357
10.33.
Suppose that f : N →N is a μ-recursive total function that is a
bijection from N to N. Show that its inverse f −1 is also μ-recursive.
10.34.
Show using mathematical induction that if t(n)(x1, . . . , xn) is the tape
number of the tape containing the string
1x11x2 . . . 1xn
then t(n) : N n →N is primitive recursive. Suggestion: In the induction
step, show that
tn(k+1)(X, xk+1) = tn(k)(X) ∗
xk+1
	
j=1
PrNo(k +
k

i=1
xi + j)
10.35.
Show that the function NewTapeNumber discussed in Section 10.4 is
primitive recursive. Suggestion: Determine the exponent e such that
TapeNumber(m) and NewTapeNumber(m) differ by the factor
PrNo(Posn(m))e, and use this to express NewTapeNumber(m) in terms
of TapeNumber(m).

358
C
H
A
P
T
E
R
11
Introduction to Computational
Complexity
A
decision problem is decidable if there is an algorithm that can answer it in
principle. In this chapter, we try to identify the problems for which there
are practical algorithms that can answer reasonable-size instances in a reasonable
amount of time. These aren’t necessarily the same thing. The satisﬁability problem
is decidable, but the known algorithms aren’t much of an improvement on the
brute-force approach, in which exponentially many cases are considered one at a
time.
The set P is the set of problems, or languages, that can be decided (by a Turing
machine, or by any comparable model of computation) in polynomial time, as a
function of the instance size. NP is deﬁned similarly, except that the restrictions
on the decision algorithms are relaxed so as to allow nondeterministic polynomial-
time algorithms. Most people assume that NP is a larger set—that being able to
guess a solution and verify it in polynomial time does not guarantee an ordinary
polynomial-time algorithm—but no one has been able to demonstrate that P ̸= NP.
Using the idea of a polynomial-time reduction, we discuss NP-complete prob-
lems, which are hardest problems in NP, and prove the Cook-Levin theorem, which
asserts that the satisﬁability problem is one of these. In the last section, we look at
a few of the many other decision problems that are now known to be NP-complete.
11.1 THE TIME COMPLEXITY OF A TURING
MACHINE, AND THE SET P
A Turing machine deciding a language L ⊆∗can be thought of as solving a
decision problem: Given x ∈∗, is x ∈L? A natural measure of the size of the
problem instance is the length of the input string. In the case of a decision problem
with other kinds of instances, we usually expect that any integer we might use to

11.1
The Time Complexity of a Turing Machine, and the Set P
359
measure the size of the instance will be closely related to the length of the string
that encodes that instance—although we will return to this issue later in this section
to pin down this relationship a little more carefully.
The ﬁrst step in describing and categorizing the complexity of computational
problems is to deﬁne the time complexity function of a Turing machine.
Deﬁnition 11.1
The Time Complexity of a Turing Machine
Suppose T is a Turing machine with input alphabet  that eventually
halts on every input string. The time complexity of T is the function
τT : N →N, where τT (n) is deﬁned by considering, for every input string
of length n in ∗, the number of moves T makes on that string before
halting, and letting τT (n) be the maximum of these numbers. When we
refer to a TM with a certain time complexity, it will be understood that
it halts on every input.
EXAMPLE 11.2
Calculating the Time Complexity of a Simple TM
Figure 11.3 shows the transition diagram also shown in Figure 7.6, for the TM T in Example
7.5 that accepts the language L = {xx | x ∈{a, b}∗}. We will derive a formula for τT (n) in
the case when n is even, and in the other case it is smaller.
The computation for a string of length n = 2k has three parts. First, T ﬁnds the middle,
changing the symbols to uppercase as it goes; second, it changes the ﬁrst half back to
lowercase as it moves the tape head back to the beginning; ﬁnally, it compares the two halves.
In the ﬁrst part, it takes one move to move the tape head to the ﬁrst input symbol,
2k + 1 moves to change the ﬁrst symbol and ﬁnd the rightmost symbol, and 2k more to
change the rightmost symbol and return the tape head to the leftmost lowercase symbol.
Each subsequent pass requires four fewer moves, and the total number of moves is therefore
1 +
k

i=0
(4i + 1) = 1 + (k + 1) + 4
k

i=0
i = k + 2 + 4k(k + 1)
2
= 2k2 + 3k + 2
The number of moves in the second part is k + 1 and also depends only on the length
of the input string. The length of the third part depends on the actual string, but the strings
requiring the most moves are the ones in L. The last part of the computation for a string
in L involves one pass for each of the k symbols in the ﬁrst half, and each pass contains k
moves to the right, k to the left, and one more to the right. The maximum number of moves
in this part is 2k2 + k, and we conclude that
τT (n) = τT (2k) = 4k2 + 5k + 4 = n2 + 5n/2 + 4
For each symbol of the input string, there are one or two moves that change the symbol to
the opposite case, and one move that compares it to another symbol. The formula for τT (n)
is quadratic because of the other moves, which involve repeated back-and-forth movements
from one end of the string to the other. It is possible to reduce the number of these by
adding more states. If we had the TM remember two symbols at a time, we could come

360
C H A P T E R 11
Introduction to Computational Complexity
b/b, L
q0
a/a, R
q0 Δ /Δ, R
b/b, R
A /A, R
B/B, R
a /a, L
a /A, L
b/B, L
B/B, L
A /A, L
Δ /Δ, L
a /A, R
b/B, R
q5
q6
q8
q9
q7
q1
q2
q3
q4
ha
B/B, L
A /A, L
B/b, L
A/a, L
Δ /Δ, R
b/B, R
Β/Δ, L
Α/Δ, L
a/a, R
b/b, R
Δ /Δ, R
a/A, R
Δ /Δ, S
Δ /Δ, S
B/B, R
A /A, R
a/a, R
b/b, R
Δ /Δ, R
a/a, L
b/b, L
Δ /Δ, L
Figure 11.3
A Turing machine accepting {xx | x ∈{a, b}∗}.
close to halving the number of back-and-forth passes, and going from two to a larger number
would improve the performance even more, at least for large values of n. Techniques like
these can, in fact, be used to reduce the time complexity of an arbitrary TM, by as large a
constant factor as we wish. However, the back-and-forth movement in this example can’t
be eliminated completely, and it is possible to show that every TM accepting this language
has time complexity that is at least quadratic.
In this example, the signiﬁcant feature of τT is the fact that the formula is
quadratic. The next deﬁnition makes it a little easier, when discussing the time
complexity of a TM, to focus on the general growth rate of the function rather than
the value at any speciﬁc point.

11.1
The Time Complexity of a Turing Machine, and the Set P
361
Deﬁnition 11.4
Big-Oh Notation
If f and g are partial functions from N to R+ (that is, both functions have
values that are nonnegative real numbers wherever they are deﬁned), we
say that f = O(g), or f (n) = O(g(n)) (which we read “f is big-oh of
g,” or “f (n) is big-oh of g(n)”), if for some positive numbers C and N,
f (n) ≤Cg(n) for every n ≥N
We can paraphrase the statement f = O(g) by saying that except perhaps for
a few values of n, f is no larger than g except for some constant factor. The
statement f (n) = O(g(n)) starts out looking like a statement about the number
f (n), but it is not. (The notation is standard but is confusing at ﬁrst.) It means the
same thing as f = O(g). Even though it tells us nothing about the values of f (n)
and g(n) for any speciﬁc n, it is a way of comparing the long-term growth rates
of the two functions.
If f and g are total functions with f = O(g), and g(n) is positive for every
n ∈N, then it is possible to say that for some positive C, f (n) ≤Cg(n) for every
n; the way the deﬁnition is stated allows us to write f = O(g) even if f or g is
undeﬁned in a few places, or if g(n) = 0 for a few values of n. For example,
n2 + 5n/2 + 4 = O(n2)
As a reason in this case, we could use the fact that for every n ≥1, n2 + 5n/2 + 4 ≤
n2 + 5n2/2 + 4n2 = 7.5n2. Or, if we preferred, we could say that n2 + 5n/2 + 4 ≤
2n2 for every n ≥4, because 5n/2 + 4 ≤n2 for those values of n.
Similarly, we can consider a polynomial f of degree k, with the formula
f (n) = aknk + ak−1nk−1 + . . . + a1n + a0
where the coefﬁcients ai are real numbers. As long as the leading coefﬁcient ak is
positive, f (n) will be positive for all sufﬁciently large values of n (and everywhere
it’s negative we can simply think of it as undeﬁned). An argument similar to the
one for the quadratic polynomial shows that f (n) = O(nk).
Although we haven’t gotten very far yet toward our goal of categorizing deci-
sion problems in terms of their complexity, we would probably say that the problem
of deciding the language in Example 11.2 is relatively simple. There is no shortage
of decision problems that appear to be much harder. An easy way to ﬁnd examples
is to consider combinatorial problems for which ﬁnding the answer might involve
considering a large number of cases. Here are two examples.
An instance of the satisﬁability problem is an expression involving Boolean
variables x1, x2, . . . , xn (whose values are true or false) and the logical connectives
∧, ∨, and ¬. The question is whether there is an assignment of truth values to the
variables that satisﬁes the expression, or makes its value true. There is no doubt that
this problem is decidable, because a brute-force algorithm will work: Try possible
assignments of truth values, until one has been found that satisﬁes the expression
or until all 2n assignments have been tried.

362
C H A P T E R 11
Introduction to Computational Complexity
The traveling salesman problem considers n cities that a salesman must visit,
with a distance speciﬁed for every pair of cities. It’s simplest to formulate this as
an optimization problem, and to ask in which order the salesman should visit the
cities in order to minimize the total distance traveled. We can turn the problem into
a decision problem by introducing another number k that represents a constraint
on the total distance: Is there an order in which the salesman can visit the cities so
that the total distance traveled is no more than k? There is a brute-force solution
here also, to consider possible orders one at a time. The decision problem might
be answered before all n! permutations have been considered, but the brute-force
approach for either formulation might require looking at all of them.
For every decision problem, if the number of steps required to decide an
instance of size n is proportional to n, computer hardware currently available
makes it possible to perform the computation for instances that are very large
indeed. The TM in Example 11.2 has time complexity that is roughly proportional
to n2, and that computation is still feasible for large values of n; many standard
algorithms involving matrices or graphs take time proportional to n2 or n3. On the
other hand, a decision problem for which the required time grows exponentially
with n is undecidable in practice except for small instances. If an instance of size
n required time 2n, and if we could just manage an instance of size 20, doubling
the computing speed would only allow us to go to n = 21, and a thousand-fold
increase would still not be quite sufﬁcient for n = 30. An instance of size 100
would be out of reach. It’s not hard to see that if the time is proportional to n!,
the situation is considerably worse.
Showing that a brute-force approach to solving a problem takes a long time
does not in itself mean that the problem is complex. The satisﬁability problem
and the traveling salesman problem are assumed to be hard, not because the brute-
force approach takes at least exponential time in both cases, but because no one has
found a way of solving either problem that doesn’t take at least exponential time.
These two are typical of a class of problems that will be important in this chapter,
because no one has been able to determine for sure just how complex they are,
and because an answer to that question would help to clarify many long-standing
open questions in complexity theory.
If we have determined that problems, or languages, requiring only quadratic
time to decide should be considered relatively easy, and those that require expo-
nential time or more should be considered very hard, what is a reasonable criterion
that can be used to distinguish the tractable problems, those for which reasonable-
size instances can be decided in a reasonable amount of time? The most common
answer, though it is not perfect, is that the tractable problems are those with a
polynomial-time solution; in particular, the problem of deciding a language L may
be considered tractable if L can be accepted by a Turing machine T for which
τT (n) = O(nk) for some k ∈N.
One reason this criterion is convenient is that the class of tractable problems
seems to be invariant among various models of computation. There is obviously
a difference between the length of a computation on a Turing machine and the
length of a corresponding computation on another model, whether it is a multitape

11.2
The Set NP and Polynomial Veriﬁability
363
TM or an electronic computer, but the difference is likely to be no more than a
polynomial factor. As a general rule, if a problem can be solved in polynomial
time on some computer, then the same is true on any other computer. Similarly,
using this criterion makes it unnecessary to distinguish between “the time required
to answer an instance of the problem,” as a function of the size of the instance, and
the number of steps a Turing machine needs to make on an input string representing
that instance. We must qualify this last statement only in the sense that the encoding
we use should be reasonable, and that another requirement for an encoding method
to be reasonable is that both encoding an instance and decoding a string should
take no more than polynomial time (in the ﬁrst case as a function of the instance
size, and in the second as a function of the string length).
Not only is the polynomial-time criterion convenient in these ways, but to a
large extent it seems to agree with people’s ideas of which problems are tractable.
Most interesting problems with polynomial-time solutions seem to require time
proportional to n2, or n3, or n4, rather than n50; and in the case of problems for
which no polynomial-time solutions are known, the only solutions that are known
typically require time proportional to 2n or worse.
Deﬁnition 11.5
The Set P
P is the set of languages L such that for some Turing machine T deciding
L and some k ∈N, τT (n) = O(nk).
Because of our comments above about decision problems and reasonable
encodings, we can speak informally about a decision problem being in P if for
some reasonable way of encoding instances, the resulting language of encoded
yes-instances is in P .
Most of the rest of this chapter deals with the question of which languages,
and which problems, are in P . We have seen a language that is, in Example 11.2.
We can construct “artiﬁcial” languages that are deﬁnitely not (see Exercise 11.19).
The satisﬁability problem and the traveling salesman problem seem to be good
candidates for real-life problems that are not in P . In the next section we will
introduce the set NP, which contains every problem in P but also many other
apparently difﬁcult problems such as these two. Most people believe that the sat-
isﬁability problem is not in P and therefore that P ̸= NP, but no one has proved
this, and the P ?= NP question remains very perplexing. See the review article by
Lance Fortnow in the September 2009 issue of Communications of the ACM for a
recent report on the status of this question.
11.2 THE SET NP AND POLYNOMIAL
VERIFIABILITY
The satisﬁability problem, which we discussed brieﬂy in Section 11.1, seems like
a hard problem because no one has found a decision algorithm that is signiﬁcantly

364
C H A P T E R 11
Introduction to Computational Complexity
better than testing possible assignments one at a time. Testing is not hard; for any
reasonable way of describing a Boolean expression with n distinct variables, we
can easily ﬁnd out whether a given truth assignment satisﬁes the expression. The
difﬁculty is that the number of possible assignments is exponential in n. Even if
we could test each one in constant time, independent of n, the total time required
would not be polynomial.
In Chapter 7, when we were interested only in whether a language was recur-
sively enumerable and not with how long it might take to test an input string, we
used nondeterminism in several examples to simplify the construction of a TM
(see Examples 7.28 and 7.30). We can consider a nondeterministic TM here as
well, which accepts Satisﬁable by guessing an assignment and then testing (deter-
ministically) whether it satisﬁes the expression. This approach doesn’t obviously
address the issue of complexity. The steps involved in guessing and testing an
assignment can easily be carried out in polynomial time, as we will see in more
detail below, but because the nondeterminism eliminates the time-consuming part
of the brute-force approach, the approach doesn’t seem to shed much light on how
hard the problem is.
Nevertheless, at least it provides a simple way of formulating the question
posed by problems like Sat: If we have a language L that can be accepted in
polynomial time by a nondeterministic TM (see Deﬁnition 11.6), is there any
reason to suspect that L is in P ?
We start by deﬁning the time complexity of an NTM, making the assumption
as we did in Deﬁnition 11.1 that no input string can cause it to loop forever. The
deﬁnition is almost the same as the earlier one, except that for each n we must
now consider the maximum number of moves the machine makes, not only over
all strings of length n but also over all possible sequences of moves it might make
on a particular input string of length n.
Deﬁnition 11.6
The Time Complexity of a
Nondeterministic Turing Machine
If T is an NTM with input alphabet  such that, for every x ∈∗, every
possible sequence of moves of T on input x eventually halts, the time com-
plexity τT : N →N is deﬁned by letting τT (n) be the maximum number
of moves T can possibly make on any input string of length n before halt-
ing. As before, if we speak of an NTM as having a time complexity, we
are assuming implicitly that no input string can cause it to loop forever.
Deﬁnition 11.7
The Set NP
NP is the set of languages L such that for some NTM T that cannot
loop forever on any input, and some integer k, T accepts L and τT (n) =
O(nk). We say that a language in NP can be accepted in nondeterministic
polynomial time.

11.2
The Set NP and Polynomial Veriﬁability
365
As before, we can speak of a decision problem being in NP if, for some
reasonable way of encoding instances, the corresponding language is.
Because an ordinary Turing machine can be considered an NTM, it is clear
that P ⊆NP. Before starting to discuss the opposite inclusion, we consider two
examples, including a more detailed discussion of the satisﬁability problem.
EXAMPLE 11.8
The Satisﬁability Problem
We let Sat denote the satisﬁability problem. An instance is an expression involving occur-
rences of the Boolean variables x1, x2, . . . , xν, for some ν ≥1. We use the term literal
to mean either an occurrence of a variable xi or an occurrence of xi, which represents the
negation ¬xi of xi. The expression is assumed to be in conjunctive normal form, or CNF:
It is the conjunction
C1 ∧C2 ∧. . . Cc
of clauses, or conjuncts, Ci, each of which is a disjunction (formed with ∨’s) of literals. For
an expression of this type, Sat asks whether the expression can be satisﬁed by some truth
assignment to the variables.
For example,
(x1 ∨x3 ∨x4) ∧(x1 ∨x3) ∧(x1 ∨x4 ∨x2) ∧x3 ∧(x2 ∨x4)
is an expression with ﬁve conjuncts that is satisﬁed by the truth assignment
x2 = x4 = true,
x1 = x3 = false
We will encode instances of Sat by omitting parentheses and ∨’s and using unary
notation for subscripts. For example, the expression
(x1 ∨x2) ∧(x2 ∨x3 ∨x1) ∧(x4 ∨x2)
will be represented by the string
∧x1x11 ∧x11x111x1 ∧x1111x11
Satisﬁable will be the language over  = {∧, x, x, 1} containing all the encoded yes-
instances.
If the size of a problem instance is taken to be the number k of literals (not necessarily
distinct), and n is the length of the string encoding the instance, then k ≤n, and it is easy to
see that n is bounded by a quadratic function of k. Therefore, if Satisﬁable ∈NP, it makes
sense to say that the decision problem Sat is in NP.
The ﬁrst step carried out by an NTM T accepting Satisﬁable is to verify that the input
string represents a valid CNF expression and that for some ν, the variables are x1, x2, . . . , xν.
If this is the case, T attempts to satisfy the expression, keeping track as it proceeds which
conjuncts have been satisﬁed so far and which variables within the unsatisﬁed conjuncts
have been assigned values. The iterative step consists of ﬁnding the ﬁrst conjunct not yet
satisﬁed; choosing a literal within that conjunct that has not been assigned a value (this is
the only occurrence of nondeterminism); giving the variable in that literal the value that
satisﬁes the conjunct; marking the conjunct as satisﬁed; and giving the same value to all

366
C H A P T E R 11
Introduction to Computational Complexity
subsequent occurrences of that variable in unsatisﬁed conjuncts, marking any conjuncts that
are satisﬁed as a result. The loop terminates in one of two ways. Either all conjuncts are
eventually satisﬁed, or the literals in the ﬁrst unsatisﬁed conjunct are all found to have
been falsiﬁed. In the ﬁrst case T accepts, and in the second it rejects. If the expression
is satisﬁable, some choice of moves causes T to guess a truth assignment that works, and
otherwise no choice of moves leads to acceptance.
The TM T can be constructed so that, except for a few steps that take time proportional
to n, all its actions are minor variations of the following operation: Begin with a string of
1’s in the input, delimited at both ends by some symbol other than 1, and locate some or
all of the other occurrences of this string that are similarly delimited. We leave it to you
to convince yourself that a single operation of this type can be done in polynomial time,
and that the number of such operations that must be performed is also no more than a
polynomial. Our conclusion is that Satisﬁable, and therefore Sat, are in NP.
EXAMPLE 11.9
Composites and Primes
A decision problem that is more familiar and almost obviously in NP is the problem to which
we will give the name Comp: Given a natural number n, is it a composite number—that
is, is there some smaller number other than 1 that divides it evenly? The way the problem
is stated makes it easy to apply the guess-and-test strategy, just as in Example 11.8. An
NTM can examine the input n, guess a positive integer p strictly between 1 and n, divide
n by p, and determine whether the remainder is 0. Or, taking even greater advantage of
nondeterminism, it could guess two posive integers p and q, both between 1 and n, and
multiply them to determine whether n = pq (see Example 7.28).
One subtlety related to any problem whose instances are numbers has to do with how
we measure the size of an instance. It might seem that an obvious measure of the size
of the instance n is n, which would be the length of an input string if we used unary
notation. However, people don’t do it this way: Using binary notation would allow an input
string with approximately log n digits, and it would be a little unreasonable to say that
an algorithm required polynomial time if that meant a polynomial function of n but an
exponential function of the input length.
It is easy to see that in the case of Comp, an NTM can carry out the steps described
above in polynomial time, even as a function of log n. Therefore, Comp is indeed in NP.
Without nondeterminism, the problem appears intimidating at ﬁrst. The most obvious
brute-force approach, testing natural numbers one at a time as possible divisors, certainly
does not produce a polynomial-time solution. We can improve on this, at least to the extent
that only primes less than n need to be considered as possible divisors, but the improvement
is not enough to make the time a polynomial in log n. However, this problem does not
seem to have quite the combinatorial complexity of the assignment problem, and there are
additional mathematical tools available to be applied, if only because properties of primes
and composites have been studied for centuries. In any case, the problem Comp is in a
different category than Sat, because it has actually been proven to be in P (see the paper
by Agrawal, Kayal, and Saxena).
This is slightly curious. The complementary problem to Comp is Prime: Given a natural
number n, is n prime? If Comp is in P, then Prime is also (see Exercise 11.9), but it’s

11.2
The Set NP and Polynomial Veriﬁability
367
not even clear at ﬁrst that Prime is in NP. Although the two problems ask essentially the
same question, a nondeterministic approach is available for one version (Does there exist
a number that is a factor of n?) and not obviously available for the other (Is p a non-
divisor of n for every p?). In fact, nondeterminism can be used for the problem Prime, as
suggested in the next paragraph, but the corresponding question is difﬁcult in general: It is
not known whether the complementary problem of every problem in NP is itself in NP (see
Exercise 11.10).
Although the proof that Comp or Prime is in P is complicated, here is a way to
approach the problem Prime nondeterministically. It involves a characterization of primes
due to Fermat: An integer n greater than 3 is prime if and only if there is an integer x, with
1 < x < n −1, satisfying
xn−1 ≡n 1, and for every m with 1 < m < n −1, xm ̸≡n 1
The fact that the characterization begins “there is an integer x” is what suggests some hope.
It’s still not apparent that this approach will be useful, because deciding that the statement
is true seems to require performing a test on every number between 1 and n −1. However,
this turns out not to be too serious an obstacle (Exercise 11.24).
Let us return to the satisﬁability problem in order to obtain a slightly different
characterization of NP, based on the guess-and-verify strategy we have mentioned
in all our examples. In the discussion, we consider a Boolean expression e, and an
assignment a of truth values to the variables in e; it may help to clarify things if
we identify e and a with the strings that represent them.
The NTM we described that accepts Satisﬁable takes the input string e and
guesses an assignment a. This is the nondeterministic part of its operation. The
remainder of the computation is deterministic and consists of checking whether
a satisﬁes e. If the expression is satisﬁable and the guesses in the ﬁrst part are
correct, the string a can be interpreted as a certiﬁcate for e, proving, or attesting
to the fact, that e is satisﬁable. By deﬁnition, every string e possessing a certiﬁcate
is in Satisﬁable. A certiﬁcate may be difﬁcult to obtain in the ﬁrst place, but the
process of verifying that it is a certiﬁcate for e is straightforward.
We can isolate the second portion of the computation by formulating the deﬁ-
nition of a veriﬁer for Satisﬁable: a deterministic algorithm that operates on a pair
(e, a) (we take this to mean a Turing machine operating on the input string e$a, for
some symbol $ not in the alphabet) and either accepts it or rejects it. We interpret
acceptance to mean that a is a certiﬁcate for e and that e is therefore in Satisﬁable;
rejection is interpreted to mean, not that e fails to be in Satisﬁable—there may be a
string different from a that is a certiﬁcate for e—but only that a is not a certiﬁcate
for e. An expression (string) e is an element of Satisﬁable if and only if there is a
string a such that the string e$a is accepted by the veriﬁer.
The issue of polynomial time arises when we consider how many steps are
required by a veriﬁer to accept or reject the string e$a. However, it is more appro-
priate to describe this number in terms of |e|, the size of the instance of the original
problem, than in terms of |e$a|, the length of the entire string.

368
C H A P T E R 11
Introduction to Computational Complexity
Deﬁnition 11.10
A Veriﬁer for a Language
If L ⊆∗, we say that a TM T is a veriﬁer for L if T accepts a language
L1 ⊆∗{$}∗, T halts on every input, and
L = {x ∈∗| for some a ∈∗, x$a ∈L1}
A veriﬁer T is a polynomial-time veriﬁer if there is a polynomial p such
that for every x and every a in ∗, the number of moves T makes on the
input string x$a is no more than p(|x|).
Theorem 11.11
For every language L ∈∗, L ∈NP if and only if L is polynomially
veriﬁable—i.e., there is a polynomial-time veriﬁer for L.
Proof
By deﬁnition, if L ∈NP, there is an NTM T1 accepting L in nondetermin-
istic polynomial time. This means that the function τT1 is bounded by some
polynomial q. We can obtain a veriﬁer T for L by considering strings m
representing sequences of moves of T1. The veriﬁcation algorithm takes
an input x$m, executes the moves of T1 corresponding to m on the input
x (or, if m represents a sequence of more than q(|x|) moves, executes
the ﬁrst q(|x|)), and accepts if and only if this sequence of moves causes
T1 to accept. It is clear that the veriﬁer operates in polynomial time as a
function of |x|.
On the other hand, if T is a polynomial-time veriﬁer for L, and
p is the associated polynomial, then we can construct an NTM T1 that
operates as follows: On input x, T1 places the symbol $ after the input
string, followed by a sequence of symbols, with length no more than
p(|x|), that it generates nondeterministically; it then executes the veriﬁer
T . T1 accepts the language L, because the strings in L are precisely those
that allow a sequence of moves leading to acceptance by the veriﬁer.
Furthermore, it is easy to see that the number of steps in the computation
of T1 is bounded by a polynomial function of |x|.
A veriﬁer for the language corresponding to Comp accepts strings x$p, where
x represents a natural number n and p a divisor of n; a certiﬁcate for x is simply a
number. In the case of the traveling salesman problem, mentioned in Section 11.1,
a certiﬁcate for an instance of the problem is a permutation of the cities so that
visiting them in that order is possible with the given constraint. We will see several
more problems later in the chapter for which there is a straightforward guess-and-
verify approach. According to Theorem 11.11, these examples are typical of all
the languages in NP. The proof of the theorem shows why this is not surprising:

11.3
Polynomial-Time Reductions and NP-Completeness
369
For every x in such a language, a sequence of moves by which the NTM accepts
x constitutes a certiﬁcate for the string.
11.3 POLYNOMIAL-TIME REDUCTIONS
AND NP-COMPLETENESS
Just as we can show that a problem is decidable by reducing it to another one that is,
we can show that languages are in P by reducing them to others that are. (If we have
languages we know are not in P , we can also use reductions to ﬁnd others that are
not.) In Chapter 9, reducing a language L1 to another language L2 required only that
we ﬁnd a computable function f such that deciding whether x ∈L1 is equivalent to
deciding whether f (x) ∈L2. In this case, we said that deciding the ﬁrst language
is no harder than deciding the second, because we considered only two degrees
of hardness, decidable and undecidable. To see whether x ∈L1, all we have to
do is compute f (x) and see whether it is in L2; and f is computable. Because
one decidable language can be harder (i.e., take longer) to decide than another,
a reduction should now satisfy some additional conditions. Not surprisingly, an
appropriate choice is to assume that the function be computable in polynomial
time.
Deﬁnition 11.12
Polynomial-Time Reductions
If L1 and L2 are languages over respective alphabets 1 and 2, a
polynomial-time reduction from L1 to L2 is a function f : ∗
1 →∗
2
satisfying two conditions: First, for every x ∈∗
1, x ∈L1 if and only if
f (x) ∈L2; and second, f can be computed in polynomial time—that is,
there is a TM with polynomial time complexity that computes f . If there
is a polynomial-time reduction from L1 to L2, we write L1 ≤p L2 and
say that L1 is polynomial-time reducible to L2.
If we interpret ≤p to mean “is no harder than,” then the statements in the
following theorem are not surprising.
Theorem 11.13
1. Polynomial-time reducibility is transitive: If L1 ≤p L2 and L2 ≤p L3, then
L1 ≤p L3.
2. If L1 ≤p L2 and L2 ∈P, then L1 ∈P.
Proof
1. Suppose the two reductions are f : ∗
1 →∗
2 and g : ∗
2 →∗
3. Then the
composite function g ◦f is a function from ∗
1 to ∗
3, and for every
x ∈∗
1, x ∈L1 if and only if g ◦f (x) ∈L3. To show g ◦f is a

370
C H A P T E R 11
Introduction to Computational Complexity
polynomial-time reduction from L1 to L3, it is therefore sufﬁcient to show
that it can be computed in polynomial time.
Suppose Tf and Tg compute f and g, respectively, in polynomial time; then
the composite TM Tf Tg computes g ◦f . From the fact that Tf computes f
in polynomial time, we can see that |f (x)| is itself bounded by a polynomial
function of |x|. From this fact and the inequality
τTf Tg(|x|) ≤τTf (|x|) + τTg(|f (x)|)
it then follows that the composite TM also operates in polynomial time.
2. Suppose T2 is a TM accepting L2 in polynomial time, and Tf is a TM
computing f in polynomial time, where f is the reduction from L1 to L2.
The composite TM Tf T2 accepts L1, because x ∈L1 if and only if
f (x) ∈L2. On an input string x, the number of steps in the computation of
Tf T2 is the number of steps required to compute f (x) plus the number of
steps required by T2 on the input f (x). Just as in statement 1, the length of
the string f (x) is bounded by a polynomial in |x|, and this implies that the
number of steps in T2’s computation must also be polynomially bounded as
a function of |x|.
Deﬁnition 11.12 and Theorem 11.13 extend to decision problems, provided
that we require reasonable encodings of instances. As a result, constructing an
appropriate polynomial-time reduction allows us to conclude that a problem is in
P or NP. Just as in Chapter 9, however, it is also a way of showing that one
problem is at least as hard as another. In our ﬁrst example we consider a decision
problem involving undirected graphs, and the reduction we construct will allow us
to say that it is at least as complex as Sat.
EXAMPLE 11.14
A Reduction from Sat to the Complete Subgraph Problem
We begin with some terminology. An undirected graph is a pair G = (V, E), where V is
a ﬁnite nonempty set of vertices and E is a set of edges. An edge is an unordered pair
(v1, v2) of vertices (“unordered” means that (v1, v2) and (v2, v1) are the same). If e is the
edge (v1, v2), then v1 and v2 are the end points of e, are said to be joined by e, and are said
to be adjacent. A complete subgraph of G is a subgraph (a graph whose vertex set and edge
set are subsets of the respective sets of G) in which every two vertices are adjacent. It is
clear that if a graph has a complete subgraph with k vertices, then for every n < k, it has a
complete subgraph with n vertices. Figure 11.15 shows a graph G with seven vertices and
ten edges. There is a complete subgraph with the vertices 3, 5, 6, and 7, and no subgraph
of G with more than 4 vertices is complete.
The complete subgraph problem is the problem: Given a graph G and a positive integer
k, does G have a complete subgraph with k vertices? We can represent the graph by a string
of edges followed by the integer k, where we represent the vertices by integers 1, 2, . . . ,
n and use unary notation for each vertex and for the number k. For example, the instance

11.3
Polynomial-Time Reductions and NP-Completeness
371
1
2
3
4
5
6
7
Figure 11.15
A graph with 7 vertices and
10 edges.
consisting of the graph in Figure 11.15 and the integer 4 might be represented by the
string
(1, 11)(11, 111)(111, 1111)(111, 11111) . . . (111111, 1111111)1111
A graph with n vertices has no more than n(n −1)/2 edges, and it is reasonable to take the
number n as the size of the instance.
The complete subgraph problem is in NP, because an NTM can take a string encoding
an instance (G, k), nondeterministically select k of the vertices, and examine the edges to
see whether every pair among those k is adjacent. We will construct a reduction from the
satisﬁability problem Sat to the complete subgraph problem. It is easiest to discuss the
reduction by considering the instances themselves, rather than the strings representing them.
Suppose x is the CNF expression
x =
c
i=1
di

j=1
ai,j
where each ai,j is a literal. We must describe an instance f (x) = (Gx, kx) of the complete
subgraph problem so that x is satisﬁable if and only if Gx has a complete subgraph with kx
vertices. We construct Gx = (Vx, Ex) so that vertices correspond to occurrences of literals
in x:
Vx = {(i, j) | 1 ≤i ≤c and 1 ≤j ≤di}
The edge set Ex is speciﬁed by saying that (i, j) is adjacent to (l, m) if and only if the
corresponding literals are in different conjuncts of x and there is a truth assignment that
makes them both true. In other words,
Ex = {((i, j), (l, m)) | i ̸= l and ai,j ̸= ¬al,m}
Finally, we deﬁne kx to to be c, the number of conjuncts in x.
If x is satisﬁable, then there is a truth assignment  such that for every i there is a
literal ai,ji that is given the value true by . The vertices
(1, j1), (2, j2), . . . , (k, jk)
determine a complete subgraph of Gx, because we have chosen the edges of Gx so that
every two of these vertices are adjacent. On the other hand, suppose there is a complete
subgraph of Gx with kx vertices. Because none of the corresponding literals is the negation
of another, some truth assignment makes them all true. Because these literals must be

372
C H A P T E R 11
Introduction to Computational Complexity
in distinct conjuncts, the assignment makes at least one literal in each conjunct true and
therefore satisﬁes x.
In order to construct the string representing (Gx, kx) from the string representing x,
a number of steps are necessary: associating the occurrences of literals in x with integers
from 1 through n; for each one, ﬁnding every other one occurring in a different conjunct
that is not the negation of that one; and constructing the string representing that edge. Each
of these steps can evidently be carried out in a number of steps that is no more than a
polynomial in |x|, and the overall time is still no more than polynomial.
The reduction in Example 11.14 allows us to compare the complexity of the
two decision problems, but not to say any more about the actual complexity of either
one. If a polynomial-time algorithm were to be found for the complete subgraph
problem, it would follow that there is also one for Sat as well; if it could be shown
that the complete subgraph problem had no polynomial algorithm, this reduction
would still leave the question for Sat unresolved. One approach to answering some
of these questions might be to try to identify a hardest problem in NP, or perhaps
several problems of similar difﬁculty that seem to be among the hardest, and then
to try to say something about how hard the problems actually are. At least the ﬁrst
of these two steps is potentially feasible. According to Theorem 11.13, it makes
sense to say a problem is a hardest problem in NP if every other problem in NP
is reducible to it.
Deﬁnition 11.16
NP-Hard and NP-Complete Languages
A language L is NP-hard if L1 ≤p L for every L ∈NP. L is NP-complete
if L ∈NP and L is NP-hard.
Theorem 11.17
1. If L and L1 are languages such that L is NP-hard and L ≤p L1, then L1 is
also NP-hard.
2. If L is any NP-complete language, then L ∈P if and only if P = NP.
Proof
Both parts of the theorem follow immediately from Theorem 11.13.
Deﬁnition 11.16 and Theorem 11.17 can be extended to decision problems; an
NP-complete problem is one for which the corresponding language is NP-complete.
Theorem 11.17 provides a way of obtaining more NP-complete problems, provided
that we can ﬁnd one to start with.
Remarkably, we can. It was shown in the early 1970s, independently by Cook
and Levin, that Sat is NP-complete (see Theorem 11.18). Example 11.14 provides
another example, and it turns out that there are many more, including problems
that involve graphs, networks, sets and partitions, scheduling, number theory, logic,
and other areas.

11.4
The Cook-Levin Theorem
373
Theorem 11.17 indicates both ways in which the idea of NP-completeness is
important. On the one hand, if someone were ever to ﬁnd an NP-complete problem
that could be solved by a polynomial-time algorithm, then the P
?= NP question
would be resolved; NP would disappear as a separate entity, and researchers could
concentrate on ﬁnding polynomial-time algorithms for all the problems now known
to be in NP. On the other hand, as long as the question remains open (or if someone
actually succeeds in proving that P ̸= NP), the difﬁculty of a problem can be
convincingly demonstrated by showing that it is NP-complete.
11.4 THE COOK-LEVIN THEOREM
An NP-complete problem is a problem in NP to which every other problem in NP
can be reduced. Theorem 11.18, proved independently by Stephen Cook and Leonid
Levin in 1971 and 1972, says that the satisﬁability problem has this property. Before
we state and sketch the proof of the result, we consider another problem that we
can reduce to Sat—not because the conclusion is interesting (the problem has only
a ﬁnite set of instances, which means that we don’t need to reduce it to anything
in order to solve it), but because the approach illustrates in a much simpler setting
the general principle that is used in the proof of the theorem and can be used to
reduce other problems to Sat.
In both this simple example and the proof of the theorem itself, it is convenient
to use a fact about logical propositions (see Exercise 11.13): For every proposition
p involving Boolean variables a1, a2, . . . , at and their negations, p is logically
equivalent to another proposition q involving these variables that is in conjunctive
normal form.
The problem is this: Given an undirected graph G = (V, E) with three vertices,
is G connected? (A graph with three vertices is connected if and only if at least
two of the three possible edges are actually present.) Reducing the problem to
Sat means constructing a function f from the set of 3-vertex graphs to the set of
Boolean expressions such that for every graph G, G is connected if and only if
f (G) is satisﬁable. For a ﬁnite problem like this, polynomial-time computability
is automatic.
We represent the vertices of G using the numbers 1, 2, and 3. We consider
three Boolean variables, labeled x1,2, x1,3, and x2,3, and we think of xi,j as being
associated with the statement “there is an edge from i to j.” With this interpretation
of the three variables, a formula “asserting” that the graph is connected is
S : (x1,2 ∧x1,3) ∨(x1,2 ∧x2,3) ∨(x1,3 ∧x2,3)
(S is not in conjunctive normal form—this is one of the places we use the fact
referred to above.) The statement we get from our interpretation of the variables
xi,j has nothing to do so far with the speciﬁc graph G but is simply a general
statement of what it means for a three-vertex graph to be connected. The question
for G is whether the statement can actually be true when the constraints are added,
which enumerate the edges that are missing from this particular instance of the
problem. For the graph with only the edge from 1 to 3, for example, the statement

374
C H A P T E R 11
Introduction to Computational Complexity
says “The edges from 1 to 2 and from 2 to 3 are missing, and at least two edges
are present.” The corresponding expression is
x1,2 ∧x2,3 ∧S
where as usual the bars over the x mean negation. This expression is not satisﬁed,
no matter what values are assigned to the three variables.
Our function f is obtained exactly this way. The expression f (G) is the
conjunction containing all terms xi,j corresponding to the edges not present, as
well as the CNF version of the expression S constructed above. In this simple
case, for every possible G, G is connected if and only if f (G) can be satisﬁed by
some truth assignment to the three Boolean variables.
This example, in addition to being simple, is perhaps atypical, because it is
clear that the more edges there are in the graph, the more likely it is to be connected.
As a result, asking whether the expression f (G) is satisﬁable boils down to asking
whether it is true when all the edges not explicitly listed as missing are actually
present. In general, we don’t expect that trying any particular single assignment
will be enough to test for satisﬁability.
Let us be a little more explicit as to why the connectedness of G is equivalent
to the satisﬁability of the expression f (G). We have said that “we think of xi,j as
being associated with the statement ‘there is an edge from i to j.’ ” How we think
of the variable xi,j isn’t really the issue. Rather, we have constructed the expression
S so that its logical structure mirrors exactly the structure of the statement “G is
connected” in this simple case; our interpretation of xi,j simply makes this logical
equivalence a little more evident. The additional portion of the expression f (G)
also mirrors exactly the constraints on the graph, arising from certain edges not
being present. Therefore, the question of whether we can specify the edges in a
way consistent with these constraints so that G is connected is exactly the same as
the question of whether we can assign truth values to the variables xi,j so that the
entire expression f (G) is satisﬁed.
Theorem 11.18
The Cook-Levin Theorem
The language Satisﬁable (or the decision problem Sat) is NP-complete.
Proof
We know from Example 11.8 that Satisﬁable is in NP, and now we must
show it is NP-hard, which means that for every language L ∈NP, there
is a polynomial-time reduction from L to Satisﬁable.
Suppose L is in NP. Then L must be L(T ) for some nondeterministic
Turing machine T = (Q, , , q0, δ) with polynomial time complexity.
Let p be a polynomial such that τT (n) ≤p(n) for every n. We make
additional assumptions about T and p, both with no loss of generality:
We assume that T never rejects by trying to move its tape head left from
square 0, and we assume that p(n) ≥n for every n. We denote by CNF
the set of CNF expressions, with no restrictions on the number of variables

11.4
The Cook-Levin Theorem
375
in the expression or on the names of the variables. To complete the proof,
it will be sufﬁcient to construct a function
g : ∗→CNF
satisfying these two conditions.
1. For every x ∈∗, x is accepted by T if and only if g(x) is satisﬁable.
2. The corresponding function f : ∗→{∧, x, x, 1}∗that reduces L(T ) to
Satisﬁable, obtained by relabeling variables in CNF expressions and
encoding the expressions appropriately, is computable in polynomial time.
The construction of g is the more complicated part of the proof, because
the expression g(x) will be a complicated expression. Fortunately, it is
relatively easy to verify that f is polynomial-time computable.
Two conﬁgurations C and C′ of T will be called consecutive in
either of two cases: Either C ⊢T C′, or C = C′ and T cannot move from
conﬁguration C. Then condition 1 above can be restated as follows: For
every x ∈∗, g(x) is satisﬁable if and only if there is a sequence of
consecutive conﬁgurations
C0, C1, . . . , Cp(|x|)
such that C0 is the initial conﬁguration corresponding to input x, and
Cp(|x|) is an accepting conﬁguration.
We begin by enumerating the states and tape symbols of T . Let
Q = {q0, q1, . . . , qt−2}, qt−1 = hr, and qt = ha
σ0 = 	 and  = {σ1, σ2, . . . , σs}
 = {σ0, . . . , σs, σs+1, . . . , σs′}
Next, let us ﬁx a string
x = σi1σi2 . . . σin ∈∗
and let N = p(|x|) = p(n). The statement that T accepts x involves only
the ﬁrst N + 1 conﬁgurations of T , and therefore only the tape squares
0, 1, . . . , N of T (because in N moves, the tape head can’t move past
square N). With this in mind, we can describe the Boolean variables we
will use in the expression g(x) (not surprisingly, many more than were
necessary in our connected-graph example above) and, for each one, say
how we will interpret it. The interpretations all have to do with details of
a conﬁguration of T .
Qi,j : after i moves, T is in state qj
Hi,k : after i moves, the tape head is on square k
Si,k,l : after i moves, the symbol in square k is σl
Here the numbers i and k vary from 0 to N, j varies from 0 to t, and l
varies from 0 to s′. There are enough variables here to describe (although

376
C H A P T E R 11
Introduction to Computational Complexity
of course it’s the interpretations, not the variables, that describe) every
detail of every one of the ﬁrst N + 1 conﬁgurations of T .
Here is the way we construct g(x), and it may help to keep in mind
the connected-graph illustration before the theorem:
g(x) = g1(x) ∧S(x)
The subexpression g1(x) corresponds to the statement that the initial con-
ﬁguration of T corresponds to the input string x. Although parts of the
remaining portion S(x) involve the number N, g1(x) is the only part
of g(x) that actually involves the symbols of x. The expression S(x)
describes all the relationships among the variables that must hold, in order
for the corresponding statement to say that the details of the conﬁgura-
tions are consistent with the operation of a Turing machine, and that the
ﬁnal conﬁguration is an accepting one.
It is easy to understand the formula g1(x):
g1(x) = Q0,0 ∧H0,0 ∧S0,0,0 ∧
n

k=1
S0,k,ik ∧
N

k=n+1
S0,k,0
The corresponding statement says that after 0 moves, the state is q0, the
tape head position is 0 and there is a blank in that position, the squares
1–n contain the symbols of x, and the remaining positions up to N are
blank.
The expression S(x) can be described as the conjunction of six subex-
pressions
S(x) =
7
i=2
gi(x)
We will describe only two of these in detail: g2(x), which is extremely
simple, and g3(x), which is the most complicated of the six.
The statement corresponding to g2(x) says that after N steps, T is in
the accepting state; the actual expression is
g2(x) = QN,t
The statement corresponding to g3(x) involves the transition function of
T ; it says that whenever T is in a conﬁguration from which it can move,
the state and current symbol one step later are those that result from one
of the legal moves of T . Suppose we denote by CM (for “can move”)
the set of pairs (j, l) for which δ(qj, σl) ̸= ∅. We want our statement to
say the following: For every i from 0 to N −1, for every tape square k
from 0 to N, and for every pair (j, l) ∈CM, if at time i the tape head is
at square k and the current state-symbol combination is (j, l), there is a
move in δ(qj, σl) such that at time i + 1, the state, the symbol at square
k, and the new tape head position are precisely those speciﬁed by that

11.4
The Cook-Levin Theorem
377
move. If there is only one move in the set δ(qj, σl), say (qj ′, σl′, D), the
appropriate expression for that choice of i, j, k, and l can be written
(Qi,j ∧Hi,k ∧Si,k,l) →(Qi+1,j ′ ∧Hi+1,k′ ∧Si+1,k,l′)
where
k′ =
⎧
⎨
⎩
k + 1
if D = R
k
if D = S
k −1
if D = L
Because, in general, δ(qj, σl) may have several elements, we need
(Qi,j ∧Hi,k ∧Si,k,l) →

m
(Qi+1,jm ∧Hi+1,km ∧Si+1,k,lm)
where m ranges over all moves in δ(qj, σl) and, for a given m, (jm, lm, km)
is the corresponding triple (j ′, l′, k′) for that move. Thus the complete
expression we want, except that it is not in conjunctive normal form,
looks like this:

i,j,k,l
((Qi,j ∧Hi,k ∧Si,k,l) →

m
(Qi+1,jm ∧Hi+1,km ∧Si+1,k,lm))
where i ranges from 0 to N −1, k ranges from 0 to N, and (j, l) varies
over all pairs in CM. Each of the conjuncts in this expression can be
converted to CNF, and the result is
g3(x) =

i,j,k,l
Fi,j,k,l
where Fi,j,k,l is in conjunctive normal form and the number of literals in
the expression is bounded independently of n.
The interpretations of the remaining pieces of g(x), the subexpres-
sions g4(x)-g7(x), are as follows:
4. For every i, if T cannot move at time i, then the conﬁguration at time i + 1
is unchanged.
5. T is in exactly one state at each step.
6. Each square of the tape contains exactly one symbol at each step.
7. The only changes in the tape are those that result from legal moves of T . In
other words, for each i and each k, if the tape head is not on square k at
time i, then the symbol in that square is unchanged at time i + 1.
It’s possible to convince yourself that the statement corresponding to the
expression S(x) is equivalent to the statement that T changes state, tape
symbol, and tape head position at each step in accordance with the rules
of a Turing machine and ends up at time N in the accepting state. There-
fore, the statement corresponding to g(x) = g1(x) ∧S(x) says that x is
accepted by T . If x is indeed accepted by T , then there is some sequence
of moves T can make on input x that leads to the accepting state within

378
C H A P T E R 11
Introduction to Computational Complexity
N moves; there are assignments to the Boolean variables representing
details of the conﬁgurations of T that are consistent with that sequence
of moves; and therefore, there are assignments that satisfy the expression
g(x) (perhaps several such assignments, as a result of the nondeterminism
of T ). On the other hand, if g(x) is satisﬁable, then an assignment that
satisﬁes it corresponds to a particular sequence of moves of T by which
x is accepted.
We will say a little about the number of steps required to compute
g(x) and f (x). A Turing machine Tg whose output is g(x) uses a number
of ingredients in its computation, among them the input string x, the
integers n = |x| and N = p(n), the integers t, s, and s′, and the transition
table for T , consisting of a set of 5-tuples. Tg starts by entering all this
information at the beginning of its tape, in a “reference” section. After this
reference section on the tape is space for all the active integer variables
i, j, k, and so on, used in the expression. The computation of p(n) can
be done in polynomial time, and the reference section can be set up in
polynomial time.
The essential principle that guarantees that g(x) can be computed in
polynomial time is this: The total number of literals in the expression
g(x) is bounded by a polynomial function of |x|; computing and writing
each one may require many individual operations, but the number of
such operations is bounded by a polynomial, and carrying out each one
(for example, searching a portion of the reference section of the tape to
compare two quantities) can be done in polynomial time. Therefore, Tg
takes no more than polynomial time.
In order to obtain f (x) from g(x), it is sufﬁcient to relabel the vari-
ables so that for some ν they are x1, . . . , xν. It is not hard to see that
this can be done within time bounded by a polynomial function of |g(x)|,
which is bounded in turn by a polynomial function of |x|.
11.5 SOME OTHER NP-COMPLETE
PROBLEMS
We know from the Cook-Levin theorem that the satisﬁability problem is NP-
complete, and we know from Example 11.14 that it can be reduced to the complete
subgraph problem. The conclusion, as a result of Theorem 11.17, is stated below.
Theorem 11.19
The complete subgraph problem is NP-complete.
Later in this section we will consider two other decision problems involv-
ing undirected graphs. Our next example is one of several possible variations

11.5
Some Other NP-Complete Problems
379
on the satisﬁability problem; see Papadimitriou, Computational Complexity, for
a discussion of some of the others. The problem 3-Sat is the same as Sat, except
that every conjunct in the CNF expression is assumed to be the disjunction of three
or fewer literals. We denote by 3-Satisﬁable the corresponding language, with yes-
instances encoded the same way. There is an obvious sense in which 3-Sat is no
harder than Sat. On the other hand, it is not signiﬁcantly easier.
Theorem 11.20
3-Sat is NP-complete.
Proof
3-Satisﬁable is clearly in NP, because Satisﬁable is. We will prove that
3-Satisﬁable is NP-hard by constructing a polynomial-time reduction of
Sat to it.
Let
x =
n

i=1
Ai = A1 ∧A2 ∧. . . ∧An
where each Ai is a disjunction of literals. We deﬁne f (x) by the formula
f (x) =
n

i=1
Bi
where Bi is simply Ai whenever Ai has three or fewer literals, and if
Ai = k
j=1 aj with k > 3, Bi is deﬁned by
Bi = (a1 ∨a2 ∨α1) ∧(a3 ∨α1 ∨α2) ∧(a4 ∨α2 ∨α3) ∧. . .
∧(ak−3 ∨αk−5 ∨αk−4) ∧(ak−2 ∨αk−4 ∨αk−3) ∧(ak−1 ∨ak ∨αk−3)
The assumption here is that the new variables α1, α2, . . . , αk−3 do not
appear either in the expression x or in any of the other Bj’s. In the simplest
case, k = 4, there are two clauses:
Bi = (a1 ∨a2 ∨α1) ∧(a3 ∨a4 ∨α1)
The formula for Bi is complicated, but here are the features that are
crucial:
1. Every conjunct after the ﬁrst one involves a literal of the form αr.
2. Every conjunct before the last one involves a literal of the form αs.
3. For every m with 2 < m < k −1, all the conjuncts before the one containing
am contain an αr for some r with 1 ≤r ≤m −2; and all the conjuncts after
this one contain an αr for some r with m −1 ≤r ≤k −3.
Suppose now that  is a truth assignment satisfying x, which means
that it satisﬁes each of the Ai’s. We want to show that for every i,

380
C H A P T E R 11
Introduction to Computational Complexity
with this assignment  to the aj’s, some choice of values for the addi-
tional variables α1, . . . , αk−3 makes Bi true. This is true if  makes
a1 or a2 true, because according to statement 1, assigning the value
false to every αr works. It is true if  makes either ak−1 or ak true,
because according to statement 2, assigning the value true to every αr
works in this case. Finally, if  makes am true for some m with 2 <
m < k −1, then according to statement 3, letting αr be true for 1 ≤
r ≤m −2 and false for the remaining r’s is enough to satisfy the for-
mula Bi.
Conversely, if  does not satisfy x, then  fails to satisfy some Ai
and therefore makes all the aj’s in Ai false. It is not hard to see that
with that assignment , no assignment of values to the extra variables
can make Bi true. For the ﬁrst conjunct of Bi to be true, α1 must be true;
for the second to be true, α2 must be true; . . .; for the next-to-last to be
true, αk−3 must be true; and now the last conjunct is forced to be false.
Therefore, if x is unsatisﬁable, so is f (x).
Now that we have the function f , we can easily obtain a function
f1 : ∗→∗, where  = {x, x, ∧, 1}, such that for every x ∈∗, x ∈
Satisﬁable if and only if f1(x) ∈3-Satisﬁable. To complete the proof it is
enough to show that f1 is computable in polynomial time, and this follows
almost immediately from the fact that the length of f1(x) is bounded by
a polynomial function of |x|.
In addition to the complete subgraph problem, many other important com-
binatorial problems can be formulated in terms of graphs. Here is a little more
terminology. A vertex cover for a graph G is a set C of vertices such that every
edge of G has an endpoint in C. For a positive integer k, we may think of the
integers 1, 2, . . . , k as distinct “colors,” and use them to color the vertices of a
graph. A k-coloring of G is an assignment to each vertex of one of the k colors
so that no two adjacent vertices are colored the same. In the graph G shown in
Figure 11.15, the set {1, 3, 5, 7} is a vertex cover for G, and it is easy to see that
there is no vertex cover with fewer than four vertices. Because there is a complete
subgraph with four vertices, G cannot be k-colored for any k < 4. Although the
absence of a complete subgraph with k + 1 vertices does not automatically imply
that the graph has a k-coloring, you can easily check that in this case there is a
4-coloring of G.
The vertex cover problem is this: Given a graph G and an integer k,
is there a vertex cover for G with k vertices? The k-colorability problem
is the problem: Given G and k, is there a k-coloring of G? Both problems are
in NP. In the second case, for example, colors between 1 and k can be
assigned nondeterministically to all the vertices, and then the edges can be
examined to determine whether the two endpoints of each one are colored dif-
ferently.

11.5
Some Other NP-Complete Problems
381
Theorem 11.21
The vertex cover problem is NP-complete.
Proof
We show that the problem is NP-hard by reducing the complete subgraph
problem to it. That is, we show that an instance I = (G, k) of the complete
subgraph problem can be transformed in polynomial time to an instance
f (I) = (G1, k1) of the vertex cover problem in such a way that G has a
complete subgraph with k vertices if and only if G1 has a vertex cover
with k1 vertices.
If G = (V, E), let G1 be the complement of G: G1 = (V, E1), where
E1 = {(i, j) | i, j ∈V and (i, j) /∈E}
and let k1 = |V | −k.
On the one hand, if vertices ν1, ν2, . . . , νk determine a complete
subgraph of G, then because no edge of G1 joins two of the νi’s, every
edge in G1 must have as an endpoint an element of V −{ν1, . . . , νk}, so
that this set of k1 vertices is a vertex cover for G1.
Conversely, suppose U = {u1, . . . , u|V |−k} is a vertex cover for G1.
Then the set V −U has k vertices; we will show that it determines a
complete subgraph of G. By the deﬁnition of complement, every two ver-
tices are joined either by an edge in G or by one in G1. However, two
vertices in V −U cannot be joined by an edge in G1, because by
deﬁnition of vertex cover, an edge in G1 must contain a vertex in U.
Therefore,
two
vertices
in
V −U
must
be
joined
by
an
edge
in G.
Because the transformation from G to G1 can be carried out in poly-
nomial time, the proof is complete.
Theorem 11.22
The k-colorability problem is NP-complete.
Proof
We show that 3-Sat is polynomial-time reducible to the k-colorability
problem. This means we must show how to construct, for each CNF
expression x in which all conjuncts have three or fewer literals, a graph
Gx and an integer kx such that x is satisﬁable if and only if there is a
kx-coloring of Gx.
The construction is very simple for the x’s with fewer than four
distinct variables and more complicated for the others. In the ﬁrst case,
we let I1 be a ﬁxed yes-instance of the k-colorability problem and I0 a
ﬁxed no-instance. We can simply answer the instance x, because there

382
C H A P T E R 11
Introduction to Computational Complexity
are no more than eight possible truth assignments to the variables, and let
the assigned value be I1 if x is a yes-instance and I0 otherwise.
Otherwise, let x = A1 ∧A2 ∧. . . ∧Ac be an instance of 3-Sat with
the n variables x1, x2, . . . , xn, where n ≥4. We construct the instance
(Gx, kx) by letting kx = n + 1 and deﬁning the graph Gx = (Vx, Ex) as
follows. There are 3n + c vertices:
Vx = {x1, x2, . . . , xn,
x1, x2, . . . , xn,
y1, y2, . . . , yn,
A1, A2, . . . , Ac}
There are six types of edges:
Ex = {(xi, xi) | 1 ≤i ≤n} ∪{(yi, yj) | i ̸= j} ∪{(yi, xj) | i ̸= j}
∪{(yi, xj) | i ̸= j} ∪{(xi, Aj) | xi /∈Aj} ∪{(xi, Aj) | xi /∈Aj}
(The notations xi /∈Aj and xi /∈Aj both mean that the literal does not
occur in the conjunct Aj.)
Suppose, on the one hand, that this graph is (n + 1)-colorable. No
two of the yi’s can be colored the same, because every two are adjacent;
therefore, n distinct colors are required for those vertices. Let us assume
that the yi’s are colored with the colors 1, 2, . . . , n. For any ﬁxed i,
consider the vertices xi and xi. Neither can be colored the same as yj
for any j ̸= i, because they are both adjacent to every such yj; and they
cannot be colored the same, because they are adjacent to each other; the
only possibility is that color i is used for one and color n + 1 for the other.
We have now accounted for the ﬁrst 3n vertices, and we consider
the remaining c. For each j with 1 ≤j ≤c, there must be some variable
xi such that neither xi nor xi appears in Aj, because Aj has no more
than three literals and there are at least four distinct variables. Since Aj is
adjacent to both xi and xi, and one of them is colored n + 1, Aj cannot
be. The vertex Aj must be colored with color i for some i with 1 ≤i ≤n.
One of the two vertices xi and xi is colored i; the one that is must appear
in the conjunct Aj, because otherwise it would be adjacent to the vertex
Aj. Let zj be a literal (either xi or xi) appearing in Aj that is colored
the same as Aj. Then no zj is the negation of any other zl, because if it
were, one of the two would be colored n + 1. This means that there is a
way of assigning truth values to the variables that makes every zj true.
The effect of this assignment is to satisfy each Aj, and it follows that x
is satisﬁable.
This argument is essentially reversible. We begin by coloring each yi
with color i. If x is satisﬁed by the assignment , then each conjunct Aj
contains a literal zj that is made true by . If zj is either xi or xi, we
color it with color i and its negation with color n + 1. Once we have done
this, if xl is a literal remaining uncolored, so is xl; we use color l to color
whichever one is made true by , and n + 1 to color the other. To see that
this is permissible, suppose for example that xl is colored with l. If xl /∈

Exercises
383
Aj, then Aj cannot be colored l because of xl (because xl /∈Aj), and Aj
cannot be colored l because of xl, because xl is not true for the assignment
. We conclude that if x is satisﬁable, then Gx is (n + 1)-colorable.
As in the previous theorem, it is easy to see that this construction can
be carried out in polynomial time, and it follows that 3-Sat is polynomial-
time reducible to the k-colorability problem.
We now have ﬁve problems that are NP-complete, and there are thousands
of other problems that are known to be. The more that are discovered, the more
potential ways there are to reduce NP-complete problems to other problems, and
the number of problems known to be NP-complete is growing constantly. The book
by Garey and Johnson, even though it was published some time ago, remains a
very good reference for a general discussion of the topic and contains a varied list
of NP-complete problems, grouped according to category (graphs, sets, and so on).
NP-completeness is still a somewhat mysterious property. Some decision prob-
lems are in P , while others that seem similar turn out to be NP-complete (Exercises
11.20 and 11.26). In the absence of either an answer to the P
?= NP question or
a deﬁnitive way of characterizing the problems that are tractable, people gener-
ally take a pragmatic approach. Many real-life decision problems require some
kind of solution. If a polynomial-time algorithm does not present itself, maybe the
problem can be shown to be NP-complete. In this case, it is probably not worth-
while spending a lot more time looking for a polynomial-time solution: Finding
one would almost certainly be difﬁcult, because if one were found, many other
problems would have polynomial-time solutions, and many people have already
spent a lot of time looking unsuccessfully for them. The next-best thing might be
to look for an algorithm that produces an approximate solution to the problem,
or one that provides a solution for a restricted set of instances. Both approaches
represent active areas of research.
EXERCISES
11.1.
Suppose f, g : N →N. Show that if f + g = O(max(f, g)).
11.2.
In Example 11.2, ﬁnd a formula for τT (n) when n is odd.
11.3.
Find the time complexity function for each of these TMs:
a. The TM in Example 7.10 that computes the reverse function from
{a, b}∗to {a, b}∗.
b. The Copy TM shown in Figure 7.19.
11.4.
Show that for any decidable decision problem, there is a way to encode
instances of the problem so that the corresponding language can be
accepted by a TM with linear time complexity.
11.5.
Suppose L1, L2 ⊆∗can be accepted by TMs with time complexity τ1
and τ2, respectively. Find appropriate functions g and h such that

384
C H A P T E R 11
Introduction to Computational Complexity
L1 ∪L2 and L1 ∩L2 can be accepted by TMs with time complexity in
O(g) and O(h), respectively.
11.6.
Describe in at least some detail a two-tape TM accepting the language of
Example 11.2 and having linear time complexity.
11.7.
Show that if L can be accepted by a multitape TM with time complexity
f , then L can be accepted by a one-tape machine with time complexity
O(f 2).
11.8.
The nondeterministic Turing machine we described that accepts
Satisﬁable repeats the following operation or minor variations of it:
starting with a string of 1’s in the input string, delimited at both ends by
a symbol other than 1, and locating some or all of the other occurrences
of this string that are similarly delimited. How long does an operation of
this type take on a one-tape TM? Use your answer to argue that the TM
accepting Satisﬁable has time complexity O(n3).
11.9.
a. Show that if L ∈P , then L′ ∈P . (L′ is the complement of L.)
b. Explain carefully why the fact that L ∈NP does not obviously imply
that L′ ∈NP.
11.10.
a. Let L1 and L2 be languages over 1 and 2, respectively. Show that
if L1 ≤p L2, then L′ ≤p L′
2.
b. Show that if there is an NP-complete language L whose complement
is in NP, then the complement of every language in NP is in NP.
11.11.
Show that if L1, L2 ⊆∗, L1 ∈P , and L2 is neither ∅nor ∗, then
L1 ≤p L2.
11.12.
a. If every instance of problem P1 is an instance of problem P2, and if
P2 is hard, then P1 is hard. True or false?
b. Show that 3-Sat ≤p Sat.
c. Generalize the result in part (b) in some appropriate way.
11.13.
Show that every Boolean expression F involving the variables a1, a2,
. . . , at is logically equivalent to an expression of the form
k
i=1
li
j=1
bi,j
where each bi,j is either an ar or an ar (i.e., an expression in conjunctive
normal form), as follows:
a. Show that if for every assignment  of truth values to the variables
a1, . . . , at, and every j with 1 ≤j ≤t, we deﬁne α,j by the
formula
α,j =
⎧
⎨
⎩
aj
if the assignment  makes aj true
aj
otherwise

Exercises
385
then the two expressions

∈S
t
j=1
α,j and ¬F
are logically equivalent, where S is the set of assignments that satisfy
¬F. This shows that ¬F is equivalent to an expression in disjunctive
normal form (a disjunction of subexpressions, each of which is a
conjunction of literals).
b. It follows that ¬F is equivalent to an expression in disjunctive
normal form. By generalizing the De Morgan laws (see Section 1.1),
show that F is equivalent to a CNF expression.
11.14.
In each case below, ﬁnd an equivalent expression that is in conjunctive
normal form.
a. a →(b ∧(c →(d ∨e)))
b. ∨n
i=1(ai ∧bi)
11.15.
Show that if k ≥4, the k-satisﬁability problem is NP-complete.
11.16.
Show that both of the following decision problems are in P .
a. DNF-Sat: Given a Boolean expression in disjunctive normal form (the
disjunction of clauses, each of which is a conjunction of literals), is it
satisﬁable?
b. CNF-Tautology: Given a Boolean expression in CNF, is it a tautology
(i.e., satisﬁed by every possible truth assignment)?
11.17.
Show that the general satisﬁability problem, Given an arbitrary Boolean
expression, not necessarily in conjunctive normal form, involving the
variables x1, x2, . . . , xν, is it satisﬁable? is NP-complete.
11.18.
Explain why it is appropriate to insist on binary notation when encoding
instances of the primality problem, but not necessary to do this when
encoding subscripts in instances of the satisﬁability problem.
11.19.
Consider the language L of all strings e(T )e(x)1n, where T is a
nondeterministic Turing machine, n ≥1, and T accepts x by some
sequence of no more than n moves. Show that the language L is
NP-complete.
11.20.
Show that the 2-colorability problem (Given a graph, is there a
2-coloring of the vertices?) is in P .
11.21.
Consider the following algorithm to solve the vertex cover problem.
First, we generate all subsets of the vertices containing exactly k vertices.
There are O(nk) such subsets. Then we check whether any of the
resulting subgraphs is complete. Why is this not a polynomial-time
algorithm (and thus a proof that P = NP)?
11.22.
Let f be a function in PF, the set of functions from ∗to ∗
computable in polynomial time. Let A (a language in ∗) be in P . Show

386
C H A P T E R 11
Introduction to Computational Complexity
that f −1(A) is in P , where by deﬁnition, f −1(A) = {z ∈∗|
f (z) ∈A}.
11.23.
In an undirected graph G, with vertex set V and edge set E, an
independent set of vertices is a set V1 ⊆V such that no two elements of
V ′ are joined by an edge in E. Let IS be the decision problem: Given a
graph G and an integer k, is there an independent set of vertices with at
least k elements? Denote by VC and CSG the vertex cover problem and
the complete subgraph problem, respectively. Construct a polynomial-
time reduction from each of the three problems IS, VC, and CSG to each
of the others. (Part of this problem has already been done, in the proof of
Theorem 11.21. In the remaining parts, you are not to use the
NP-completeness of any of these problems.)
11.24.
Show that if the positive integer n is not a prime, then for every x with
0 < x < n −1 that satisﬁes xn−1 ≡n 1, there is a prime factor p of n −1
satisfying
x(n−1)/p ≡n 1
This makes it a little easier to understand how it might be possible to test
a number n for primeness in nondeterministic polynomial time: In the
result of Fermat mentioned in Example 11.9, we don’t have to test that
xm ̸≡n 1 for every m satisfying 1 < m < n −1, but only for numbers m
of the form (n −1)/p, where p is a prime. Suggestion: Because of
Fermat’s result, if xn−1 ≡n 1 and n is not prime, there is an integer m
with 1 < m < n −1 such that xm ≡n 1; show that the smallest such m is
a divisor of n −1.
11.25.
For languages L1, L2 ⊆{a, b}∗, let
L1 ⊕L2 = L1{a} ∪L2{b}
a. Show that L1 ≤p L1 ⊕L2 and L2 ≤p L1 ⊕L2.
b. Show that for any languages L, L1, and L2 over {a, b}, with
L ̸= {a, b}∗, if L1 ≤p L and L2 ≤p L, then L1 ⊕L2 ≤p L.
11.26.
†Show that the 2-Satisﬁability problem is in P .
11.27.
†Show that both P and NP are closed under the operations of union,
intersection, concatenation, and Kleene ∗.
11.28.
†Show that the following decision problem is NP-complete: Given a
graph G in which every vertex has even degree, and an integer k, does G
have a vertex cover with k vertices? (The degree of a vertex is the
number of edges containing it.) Hint: given an arbitrary graph G, ﬁnd a
way to modify it by adding three vertices so that all the vertices of the
new graph have even degree.
11.29.
Give an alternate proof of the NP-completeness of the vertex cover
problem, by directly reducing the satisﬁability problem to it. Below is a
suggested way to proceed; show that it works.

Exercises
387
Starting with a CNF expression x = 	m
i=1 Ai, where Ai = ni
j=1 ai,j and
x involves the n variables v1, . . . , vn, construct an instance (G, k) of the
vertex cover problem as follows: ﬁrst, there are vertices vt
l and vf
l for
each variable vl, and these two are connected by an edge; next, for each
conjunct Ai there is a complete subgraph Gi with ni vertices, one for
each literal in Ai; ﬁnally, for a variable vl and a conjunct Ai, there is an
edge from the vertex in Gi corresponding to vl, either to vt
l (if vl appears
in Ai) or to vf
l (if vl doesn’t appear in Ai). Finally, let k be the integer
n + (n1 −1) + . . . + (nm −1).
11.30.
Show that the following decision problem is NP-complete. Given a ﬁnite
set A, a collection C of subsets of A, and an integer k, is there a subset
A1 of A having k or fewer elements such that A1 ∩S ̸= ∅for each S in
the collection C?
11.31.
†The exact cover problem is the following: given ﬁnite subsets S1, . . . ,
Sk of a set, with 
k
i=1 Si = A, is there a subset J of {1, 2, . . . , k} such
that for any two distinct elements i and j of J, Si ∩Sj = ∅and

i∈J Si = A?
Show that this problem is NP-complete by constructing a reduction from
the k-colorability problem.
11.32.
†The sum-of-subsets problem is the following: Given a sequence a1, a2,
. . . , an of integers, and an integer M, is there a subset J of {1, 2, . . . , n}
such that 
i∈J ai = M?
Show that this problem is NP-complete by constructing a reduction from
the exact cover problem.
11.33.
†The partition problem is the following: Given a sequence a1, a2, . . . , an
of integers, is there a subset J of {1, 2, . . . , n} such that 
i∈J ai =

i /∈J ai?
Show that this problem is NP-complete by constructing a reduction from
the sum-of-subsets problem.
11.34.
†The 0–1 knapsack problem is the following: Given two sequences w1,
. . . , wn and p1, . . . , pn of nonnegative numbers, and two numbers W
and P , is there a subset J of {1, 2, . . . , n} such that 
i∈J wi ≤W and

i∈J pi ≥P ? (The signiﬁcance of the name is that wi and pi are
viewed as the weight and the proﬁt of the ith item, respectively; we have
a “knapsack” that can hold no more than W pounds, and the problem is
asking whether it is possible to choose items to put into the knapsack
subject to that constraint, so that the total proﬁt is at least P .)
Show that the 0–1 knapsack problem is NP-complete by constructing a
reduction from the partition problem.

This page intentionally left blank 

389
S O L U T I O N S T O S E L E C T E D
E X E R C I S E S
CHAPTER 1
1.3(b). (m1 ≤m2) ∧((m1 < m2) ∨(d1 < d2)). The second clause in this state-
ment (the portion after the ∧) could also be written ((m1 = m2) →(d1 <
d2)).
1.8(d). Two correct expressions are
(A ∩B) ∪(A ∩C) ∪(B ∩C) ∩(A ∩B ∩C)′
and (A ∩B ∩C′) ∪(A ∩B′ ∩C) ∪(A′ ∩B ∩C).
1.12(b). One algorithm is described by the following pseudocode, which processes
the symbols left-to-right, starting with the leftmost bracket. The answer
is the ﬁnal value of e. This algorithm, however, is correct only if we can
assume that there are no duplicate elements in the set. The problem of
counting the distinct elements without this assumption is more compli-
cated, because duplicate elements such as {∅, {∅}} and {{∅}, ∅} might not
be written the same way.
read ch, the next nonblank character;
e = 0; (element count)
u = 1; (current number of unmatched left brackets)
while there are nonblank characters remaining
{ read ch, the next nonblank character;
if ch is a left bracket
u = u + 1;
else if ch is a right bracket
u = u - 1;
if u = 1 and ch is either ∅or a right bracket
e = e + 1;
}
1.15(a). If either A or B is a subset of the other, then the two sets are equal. If
not, then 2A ∪2B ⊆2A∪B, and if a ∈A −B and b ∈B −A, the subset
{a, b} is an element of 2A∪B that is not in 2A ∪2B.
1.21(a). No, because for every subset A ∈T , f (A) = f (N −A), so that f is not
one-to-one.

390
Solutions to Selected Exercises
1.24. We consider the case when the relation is to be reﬂexive, not symmet-
ric, and transitive. If R is a reﬂexive relation on the set, it must contain
the ordered pairs (1, 1), (2, 2), and (3, 3). If it is reﬂexive and not sym-
metric, it must contain at least one additional pair; suppose it contains
(1, 2). We can verify that the relation with only these four pairs is tran-
sitive. If x, y, and z are elements of {1, 2, 3} such that (x, y) and (y, z)
are in R, then at least one of the statements x = y and y = z must be
true, and it follows that (x, z) must be in R. The conclusion is that
R = {(1, 1), (2, 2), (3, 3), (1, 2)} is a reﬂexive, nonsymmetric, transitive
relation on {1, 2, 3} containing as few ordered pairs as possible.
1.34. Suppose that xy = yx. In order to ﬁnd a string α such that x = αp and
y = αq for some p and q, which implies that |α| is a divisor of both |x|
and |y|, we let d be the greatest common divisor of |x| and |y|. Then
we can write x = x1x2 . . . xp and y = y1y2 . . . yq, where all the xi’s and
yj’s are strings of length d and the numbers p and q have no common
factors.
Now we would like to show that the xi’s and yj’s are actually
all the same string α. The ﬁrst step is to observe that xqyp = ypxq,
because the fact that xy = yx allows us to rearrange the pieces of xqyp
so that the string doesn’t change but all the occurrences of y have been
moved to the beginning.
The string xqyp = ypxq has length 2pqd. The preﬁx of xqyp of
length pqd is xq, and the preﬁx of ypxq of the same length is yp. There-
fore, xq = yp.
In the string xq = (x1x2 . . . xp)q, the substring x1 occurs at positions
1, pd + 1, 2pd + 1, . . . , (q −1)pd + 1. In the string yp = (y1y2 . . . yq)p,
the strings that occur at these positions (which must therefore be equal)
are yr0, yr1, . . . , yrq−1, where for each i, ri ≡ip + 1 mod q. The numbers
r0, r1, . . . , rq−1 must be distinct, because the only way for ri and rj to
be the same if 0 ≤i < j ≤q −1 is for ip and jp to be congruent mod
q; this means that (j −i)p ≡0 mod q, or that (j −i)p is divisible by q,
and this is impossible if p and q have no common factors. The strings yr0,
. . . , yrq−1 are all equal, and because we now know that these subscripts
are distinct, we may conclude that all the yj’s are equal, say to the string
α. Virtually the same argument shows that all the xi’s are equal, and we
have the conclusion we want.
1.35. No. Suppose L = L1L2 and neither L1 nor L2 is {}. Because every
even-length string of a’s is in L, there are arbitrarily long strings of a’s
that must be in either L1 or L2. Similarly, there are arbitrarily long strings
of b’s that are in L1 or L2. It is not possible for L1 to have a nonnull string
of a’s and L2 to have a nonnull string of b’s, because the concatenation
could not be in L; similarly, there can’t be a string of b’s in L1 and a
string of a’s in L2. The only possibilities, therefore, are for all the strings
of a’s and all the strings of b’s to be in L1 or for all these strings to be in

Solutions to Selected Exercises
391
L2. Suppose, however, that they are all in L1. Let y be a nonnull string
in L2. Then y contains both a’s and b’s. L1 contains a string x of a’s
with |x| ≥|y|. But then xy, which is in L1L2, has b’s in the second half
and not in the ﬁrst, so that it can’t be in L. A similar argument shows
that L2 can’t contain all the strings of a’s and the strings of b’s. This
argument shows that our original assumption, that L = L1L2 and neither
L1 nor L2 is {}, cannot be true.
1.40(c). To make the solution easier to describe, take the set A to be {1, 2, . . . , n}.
If R is symmetric, specifying R is accomplished by specifying the pairs
(i, j) in R for which i ≤j, and two different such choices lead to two
different symmetric relations. Therefore, the number of symmetric rela-
tions on A is the number of subsets of the set {(i, j) | 1 ≤i ≤j ≤
n}. Because this set has n(n + 1)/2 elements, the number of subsets is
2n(n+1)/2.
1.44(f). We might try to either ﬁnd a formula for L or ﬁnd a simple property that
characterizes the elements of L. First we observe that every element of
L starts with a. If this doesn’t seem clear, you can construct a proof by
structural induction.
It may be helpful, in order to ﬁnd a formula for L, to write L = {a}L1
and to think about what the language L1 is. The statement a ∈L tells
us that  ∈L1, and the recursive part of the deﬁnition of L tells us
that if y ∈L1 (i.e., if ay ∈L), then yb and yba are both in L1. We
can recognize these statements about L1 as a recursive deﬁnition of the
language {b, ba}∗, which means that L = {a}{b, ba}∗.
Strings in L1 cannot start with a and cannot contain two a’s in a row.
(Again, this is a straightforward structural induction proof.) Furthermore,
every string that doesn’t start with a and doesn’t contain two consecutive
a’s is an element of L1. This can also be proved by mathematical induction
on the length of the string. Therefore, L1 is the set of all strings in {a, b}∗
that do not begin with a and do not contain two consecutive a’s, and it
follows that L is the set of all strings in {a, b}∗that begin with a and do
not contain two consecutive a’s.
1.56. You can construct a proof using mathematical induction that for every
n ∈N, every subset A of N that contains n has a smallest element. This
will be sufﬁcient, because it will imply that every nonempty subset of N
has a smallest element. (Saying that A is nonempty means that there is
some n such that A contains n.)
1.60. We show using mathematical induction that for every n ∈N, if y ∈L0
and |y| = n, then y ∈L. It will be appropriate to use strong induction.
The basis step is to show that if y ∈L0 and |y| = 0, then y ∈L.
This is true because if |y| = 0, then y = , and  ∈L because of the
ﬁrst statement in the deﬁnition of L.
The induction hypothesis is that k ∈N and that for every y ∈L0
with |y| ≤k, y ∈L.

392
Solutions to Selected Exercises
In the induction step, we will show that for every y ∈L0 with
|y| = k + 1, y ∈L. Suppose y is a string in L0 with |y| = k + 1. This
means that y = aibj for some i and j such that i ≥j and i + j = k + 1.
We must show using the deﬁnition of L that y ∈L. Because k ≥0, y
cannot be , and so in order to use the deﬁnition to show that y ∈L,
we must show that y = ax for some x ∈L or that y = axb for some
x ∈L. The statements y = aibj, i ≥j, and i + j > 0 imply that i > 0,
which means that y = ax for some string x; if j is also positive, then y is
also axb for some string x. The question in either case is whether x ∈L.
According to our induction hypothesis, strings in L0 that are shorter than
y are in L; so the real question in either of our two cases is whether
x ∈L0.
If j > 0, then y = axb, where x = ai−1bj−1, and i −1 ≥j −1
because i ≥j. Therefore, in this case x ∈L0. In the other case, when
j = 0, then y = ai = ax, where x = ai−1 = ai−1b0, and this string is
also in L0. In either case, the induction hypothesis tells us that x ∈L0,
and the deﬁnition of L then tells us that y ∈L.
Strong induction is necessary in the ﬁrst case, where y = axb and
|x| = k + 1 −2 = k −1, because we need to be able to say that a string
in L0 of length less than k is in L.
1.67. First we use structural induction to show that every x ∈L has
equal numbers of a’s and b’s. The basis step is to show that
na() = nb(), and this is true because both numbers are 0. The
induction hypothesis is that x and y are two strings in L and
that both x and y have equal numbers of a’s and b’s. In the
induction step, we must show that both axby and bxay also do.
In both cases, the number of a’s is na(x) + na(y) + 1 and the
number of b’s is nb(x) + nb(y) + 1. The induction hypothesis tells
us that na(x) = nb(x) and similarly for y, so that the conclusion
we want follows.
The converse is proved by strong induction on |x|. The basis step is
to show that  ∈L, and this is true by deﬁnition of L. The induction
hypothesis is that k ≥0 and that every string x with |x| ≤k having equal
numbers of a’s and b’s is in L. The statement to be shown in the induction
step is that if x has equal numbers of a’s and b’s and |x| = k + 1, then
x ∈L.
If x is either aby or bay, for some y with |y| = k −1, then y has
equal numbers of a’s and b’s, and is in L by the induction hypothesis.
Therefore, x, which is either aby or bay, is in L by deﬁnition of L.
The remaining case is when x starts with either aa or bb; we complete the
proof in the ﬁrst case. For a string z ∈{a, b}∗, let d(z) = na(z) −nb(z),
and consider d(z) for the preﬁxes z of x. Because d(aa) = 2 and d(x) =

Solutions to Selected Exercises
393
0, and adding a single symbol to a preﬁx changes the d value by 1,
there must exist a preﬁx z longer than aa for which d(z) = 1. Let z1
be the longest such preﬁx. Then x must be z1bz2 for some string z2—
otherwise, because d(z1a) = 2, there would be a preﬁx longer than z1 for
which d = 1. We now know that x = aay1bz2, and d(ay1) = d(aay1b) =
0. Therefore, d(z2) = 0 as well. By the induction hypothesis, both ay1
and z2 are in L. Therefore, x = a(ay1)bz2 is also in L, according to the
deﬁnition of L.
CHAPTER 2
2.1(h).
a
a
b
b
a
a
b
b
a, b
2.5. See the proof of the formula (xy)r = yrxr in Example 1.27, and use
the same approach. Interpret the statement to be “For every string y,
P (y) is true,” where P (y) is “For every state q and every string x,
δ∗(q, xy) = δ∗(δ∗(q, x), y).” Use structural induction on y, based on the
recursive deﬁnition of ∗in Example 1.17.
2.9(a). Suppose L has this property for some ﬁnite set S and some n. If L0 is
the set of elements of L with length less than n, then L = L0 ∪∗S.
The ﬁnite language L0 can be accepted by an FA. For each y ∈S, the
language of strings over  that end with y can be accepted by an FA,
and ∗S is the union of a ﬁnite number of languages of this form.
Therefore, L is the union of a ﬁnite number of languages that can be
accepted by an FA.
2.9(b). Suppose L is ﬁnite. Let n be an integer larger than the length of the
longest string in L, and let S be the empty set. The statement “For every
x with |x| ≥n, x ∈L if and only if x ends with an element of S” is true,
because both parts of the if-and-only-if statement are false.

394
Solutions to Selected Exercises
2.12(h).
a
b
a
a
b
a
a,b
b
b
a
b
2.21(b). Consider any two strings am and an, where m < n. These can be distin-
guished by the string bam+2, because ambam+2 ∈L and anbam+2
/∈L.
2.22(b). Suppose for the sake of contradiction that L can be accepted by a ﬁnite
automaton with n states. Let x = anban+2. Then x ∈L and |x| ≥n, and
according to the pumping lemma, x = uvw for some strings u, v, and w
such that |uv| ≤n, |v| > 0, and uviw ∈L for every i ≥0.
Because the ﬁrst n symbols of x are a’s, the string v must be aj
for some j > 0, and uv2w = an+jban+2. This is a contradiction, because
uv2w ∈L but n + 2 ̸> n + j + 1.
2.23(b). For the language L in Exercise 2.21(b), statement I is not sufﬁcient to
produce a contradiction, because the strings u = a, v = a, and w = 
satisfy the statement.
2.27(d). Let M = (Q, , q0, A, δ) and suppose that δ∗(q0, x) = q. Then x is a
preﬁx of an element of L if and only if the set of states reachable from
q (see Example 2.34) contains an element of A.
2.29(a). If a statement of this type is false, it is often possible to ﬁnd a very simple
counterexample. In (a), for example, take L1 to be a language over  that
cannot be accepted by an FA and L2 to be ∗.
2.30(b). Notice that this deﬁnition of arithmetic progression does not require the
integer p to be positive; if it did, every arithmetic progression would be
an inﬁnite set, and the result would not be true. Suppose A is accepted by
the FA M = (Q, {a}, q0, F, δ). Because Q is ﬁnite, there are integers m
and p such that p > 0 and δ∗(q0, am) = δ∗(q0, am+p). It follows that for
every n ≥m, δ∗(q0, an) = δ∗(q0, an+p). In particular, for every n satis-
fying m ≤n < m + p, if an ∈A, then an+ip ∈A for every i ≥0, and if
an /∈A, then an+ip /∈A for every i ≥0. If n1, n2, . . . , nr are the values

Solutions to Selected Exercises
395
of n between m and m + p −1 for which an ∈A, and P1, . . . , Pr are the
corresponding arithmetic progressions (that is, Pj = {nj + ip | i ≥0}),
then S is the union of the sets P1, . . . , Pr and the ﬁnite set of all k with
0 ≤k < m for which ak ∈A. Therefore, S is the union of a ﬁnite number
of arithmetic progressions.
2.38a. According to Theorem 2.36, the languages having this property can be
accepted by a 3-state FA in which the states are these three sets: the set S
of strings ending with b; the set T of strings ending with ba; and the set
U of strings that don’t end with either b or ba. The state U will be the
initial state, because the set U contains , and the transitions are shown
in this diagram.
b
a
a
a
b
b
U
S
T
It might seem as though each of the eight ways of designating the
accepting states from among these three produces an FA accepting one
of the languages. However, if L is one of these eight languages and L
can actually be accepted by an FA with fewer than three states, then
IL will have fewer than three equivalence classes. This is the case for
the languages ∅(accepted by the FA in which none of the three states is
accepting), {a, b}∗(the complement of ∅), S (accepted by the FA in which
S is the only accepting state), and T ∪U (the complement of S). The
remaining four ways of choosing accepting states give us the languages
T , S ∪U, S ∪T , and U, and these are the answer to the question.
2.42(a). If IL has two equivalence classes, then both states of the resulting FA
must be reachable from the initial state, and there must be exactly one
accepting state. There are three ways of drawing transitions from the
initial state, because the other state must be reachable, and there are four
ways of drawing transitions from the other state. Each of these twelve
combinations gives a different set A of strings corresponding to the initial
state, and each of these twelve is a possible answer to (a). For example,
the set ({a} ∪{b}{a}∗{b})∗corresponds to the transitions shown below.
a
b
b
a

396
Solutions to Selected Exercises
2.42(b). For each of these twelve sets A, there are two possible languages L for
which IL has two equivalence classes and A = []: A and the comple-
ment of A.
2.48. We sketch the proof that two strings x and y are equivalent if and only if
they have the same number of a’s and the same number of b’s. It is easy
to see that two strings like this are equivalent, or L-indistinguishable.
Suppose that x has i a’s and i + p b’s, and y has j a’s and j + q b’s,
and either i ̸= j or i + p ̸= j + q. One case in which this happens is
when p ̸= q; we consider this case, and the case in which p = q and
i ̸= j is similar.
We want to show that for some z, xz ∈L and yz /∈L. We can make
xz an element of L by picking z to contain N a’s and N −p b’s, for any
number N that is at least p, so that xz has i + N a’s and i + p + (N −
p) = i + N b’s. The string yz will have j + N a’s and j + (q −p) + N
b’s, and all we need to show in this case is that an N can be chosen such
that the second number is not a multiple of the ﬁrst. The ratio of the two
numbers is
j + N + (q −p)
j + N
= 1 + q −p
j + N
The second term in the second expression might be positive or negative,
but in order to guarantee that it is not an integer, it is sufﬁcient to pick
N such that j + N is larger than |q −p|.
2.55(f). See the ﬁgure below. As in Example 2.41, the square corresponding to a
pair (i, j) contains either nothing or a positive integer. In the ﬁrst case,
the pair is not marked during the execution of Algorithm 2.40, and in the
second case the integer represents the pass of the algorithm in which the
pair is marked.
2,6
1,3
5,7
4
a
b
b
a
a
2
3
4
5
6
7
1
2
1
1
1
1
2
2
1
1
2
1
1
1
1
2
3
4
5
6
b
2.60. Let L be the language {an | n > 0 and n is not prime}. Every sufﬁciently
large integer is the sum of two positive nonprimes, because an even
integer 8 or greater is the sum of two even integers 4 or greater, and an
odd number n is 1 + (n −1). Therefore, L2 contains all sufﬁciently long
strings of a’s, which means that L2 can be accepted by an FA. However,
L cannot.

Solutions to Selected Exercises
397
2.61. If L ⊆{a}∗, then the subset Lengths(L∗) = {|x| | x ∈L∗} is closed under
addition. It is possible to show that every subset of N closed under
addition is the union of a ﬁnite number of arithmetic progressions, so that
the result follows from Exercise 2.30(a).
CHAPTER 3
3.5(b). The set {∅} ∪{{x} | x ∈∗} of all zero- or one-element languages over .
3.7(h). (b + abb)∗.
3.7(k). Saying that a string doesn’t contain bba means that if bb appears, then
a cannot appear anywhere later in the string. Therefore, a string not con-
taining bba consists of a preﬁx not containing bb, possibly followed by
a string of b’s. By including in the string of b’s all the trailing b’s, we
may require that the preﬁx doesn’t end with b. Because (a + ba)∗corre-
sponds to strings not containing bb and not ending with b, one answer is
(a + ba)∗b∗.
3.13(b). Suppose r satisﬁes r = c + rd and  does not correspond to d, and let
x be a string corresponding to r. To show that x corresponds to cd∗,
assume for the sake of contradiction that x does not correspond to cdj
for any j. If we can show that for every n ≥0, x corresponds to the
regular expression rdn, we will have a contradiction (because strings
corresponding to d must be nonnull), and we may then conclude that
r = cd∗.
The proof is by induction. We know that x corresponds to r = rd0.
Suppose that k ≥0 and that x corresponds to rdk. The assumption is
that rdk = (c + rd)dk = cdk + rdk+1. If x does not correspond to cdk, it
must correspond to rdk+1.
3.14. Make a sequence of passes through the expression. In each pass, replace
any regular expression of the form ∅+ r or r + ∅by r, any regular
expression of the form ∅r or r∅by ∅, and any occurrence of ∅∗by .
Stop after any pass in which no changes are made. If ∅remains in the
expression, then the expression must actually be ∅, in which case the
corresponding language is empty.
3.17(b). {aaa}∗cannot be described this way. To simplify the discussion slightly,
we assume the alphabet is {a}; every subset of {a}∗describable by a
generalized regular expression not involving ∗has a property that this
language lacks.
For L ⊆{a}∗, we let e(L) be the set of all even integers k such
that k = |x| for some x ∈L, and we let e′(L) be the set of nonnegative
even integers not in e(L). Similarly, a positive odd number k is in
the set o(L) if k = |x| for some x ∈L, and k ∈o′(L) otherwise. We
will say that L is ﬁnitary if either e(L) or e′(L) is ﬁnite and either
o(L) or o′(L) is ﬁnite. In other words, L is ﬁnitary if, in the set of
all nonnegative even integers, the subset of those that are lengths of

398
Solutions to Selected Exercises
strings in L either is ﬁnite or has ﬁnite complement, and similarly for
the set of all positive odd integers.
It can be veriﬁed that if L1 and L2 are both ﬁnitary, then all the
languages L1 ∪L2, L1 ∩L2, L1L2, and L′
1 are also ﬁnitary. We will check
this in one case. Suppose, for example, that L1 and L2 are nonempty and
that e(L1) is ﬁnite, o(L1) is ﬁnite, e(L2) is ﬁnite, and o′(L2) is ﬁnite.
This means that L1 is actually ﬁnite, and L2 has only a ﬁnite number
of even-length strings but contains all the odd-length strings except for a
ﬁnite number.
Then L1 ∪L2 contains only a ﬁnite number of even-length strings,
and it contains all but a ﬁnite number of the odd-length strings, so that
e(L1 ∪L2) and o′(L1 ∪L2) are ﬁnite. Because L1 ∩L2 is ﬁnite, both
e(L1 ∩L2) and o(L1 ∩L2) are ﬁnite.
Consider the number e(L1L2). If L1 has no strings of odd length,
then the even-length strings in L1L2 are all obtained by concatenating
even-length strings in L1 with even-length strings in L2, which means
that e(L1L2) is ﬁnite; if there is an odd-length string in L1, however,
then because L2 contains almost all the odd-length strings, L1L2 contains
almost all the even-length strings—i.e., e′(L1L2) is ﬁnite. Similarly either
o(L1L2) is ﬁnite (if L1 contains no even-length strings), or o′(L1L2) is
ﬁnite (if L1 has a string of even length).
The number e′(L′
1) is ﬁnite, because every nonnegative even integer
that is not the length of an element of L′
1 must be the length of an
element of L1, and o′(L′
1) is also ﬁnite. Similarly, e′(L′
2) and o(L′
2) are
ﬁnite.
The language L = {aaa}∗is not ﬁnitary, because all four of the sets
e(L), e′(L), o(L), and o′(L) are inﬁnite. Therefore, L cannot be obtained
from the ﬁnitary languages ∅and {a} using only the operations of union,
intersection, concatenation, and complement.
3.28(a). We must show two things: ﬁrst, that for every s ∈S, s ∈(T ), and
second, that for every t ∈(T ), ({t}) ⊆(T ). The ﬁrst is true because
S ⊆T and (by deﬁnition) T ⊆(T ). The second is part of the deﬁnition
of (T ).
3.28(f). We always have S ⊆(S) and therefore (S)′ ⊆S′ ⊆(S′). If the ﬁrst
and third of these three sets are equal, then the equality of the ﬁrst two
implies that S = (S), and the equality of the second and third implies
that S′ = (S′). The converse is also clearly true: If S = (S) and S′ =
(S′), then (S)′ = (S′). Thus, the condition (S′) = (S)′ is true
precisely when there is neither a -transition from an element of S to
an element of S′ nor a -transition from an element of S′ to an element
of S. (A more concise way to say this is to say that S and S′ are both
-closed—see Exercise 3.29.)
3.30. The proof is by structural induction on y and will be very enjoyable for
those who like mathematical notation. The basis step is for y = . For

Solutions to Selected Exercises
399
every q ∈Q and every x ∈∗,
∪{δ∗(p, ) | p ∈δ∗(q, x)} = ∪{({p}) | p ∈δ∗(q, x)}
= (∪{{p} | p ∈δ∗(q, x)})
= (δ∗(q, x)) = δ∗(q, x) = δ∗(q, x)
In this formula, we are using parts of Exercise 3.28. The ﬁrst equality uses
the deﬁnition of δ∗. The second uses part (d) of Exercise 3.28. The third
is simply the deﬁnition of union. The fourth uses part (b) of Exercise 3.28
and the fact that according to the deﬁnition, δ∗(q, x) is (S) for some
set S.
Now assume that for the string y,
δ∗(q, xy) = ∪{δ∗(p, y) | p ∈δ∗(q, x)}
for every state q and every string x ∈∗. Then for every σ ∈,
δ∗(q, x(yσ)) = δ∗(q, (xy)σ)
= (∪{δ(p, σ)|p ∈δ∗(q, xy)})
= ({s|∃p(p ∈δ∗(q, xy) ∧s ∈δ(p, σ))})
= ({s|∃p(p ∈∪{δ∗(r, y)|r ∈δ∗(q, x)} ∧s ∈δ(p, σ))})
= ({s|∃r(r ∈δ∗(q, x) ∧∃p(p ∈δ∗(r, y) ∧s ∈δ(p, σ)))})
= ({s|∃r(r ∈δ∗(q, x) ∧s ∈∪{δ(p, σ)|p ∈δ∗(r, y)})})
= (∪{∪{δ(p, σ)|p ∈δ∗(r, y)}|r ∈δ∗(q, x)})
= ∪{(∪{δ(p, σ)|p ∈δ∗(r, y)})|r ∈δ∗(q, x)}
= ∪{δ∗(r, yσ)|r ∈δ∗(q, x)}
The second equality uses the deﬁnition of δ∗. The third is just a restate-
ment of the deﬁnition of union. The fourth uses the induction hypothesis.
The ﬁfth, sixth, and seventh are also just restatements involving the deﬁni-
tions of various unions. The eighth uses part (c) of Exercise 3.28 (actually,
the generalization of part (c) from the union of two sets to an arbitrary
union). The ninth is the deﬁnition of δ∗.
3.38(d).
1
2
1,2
Ø
3
1,4,5
1,2,5
a
b
b
a
b
b
b
b
a
a,b
a
a
a

400
Solutions to Selected Exercises
3.40(b).
1,3
a
a
a
a
b
b
b
b
b
b
a
b
a
a
2
3
3,5
1
5
6
4
a
a
a
a,b
b
Ø
1,3,4
1,2,
3,4
3.51(b). Simpliﬁed tables with the regular expressions r(p, q, k) for 0 ≤k ≤2 are
shown in Table 1 below.
The language corresponds to the regular expression r(1, 2, 2) + r(1,
3, 2)r(3, 3, 2)∗r(3, 2, 2), or
(a + b)a∗+ (a + b)a∗b((a + ba + bb)a∗b)∗(a + ba + bb)a∗
3.54. The reduction step in which p is eliminated is to redeﬁne e(q, r) for
every pair (q, r) where neither q nor r is p. The new value of e(q,
r) is
e(q, r) + e(q, p)e(p, p)∗e(p, r)
where the e(q, r) term in this expression represents the previous value.
The four-part ﬁgure on the next page illustrates this process for the FA
shown in Figure 3.40b. The ﬁrst picture shows the NFA obtained by
adding the states q0 and qf ; the subsequent pictures describe the machines
obtained by eliminating the states 1, 2, and 3, in that order. The regular
expression e(q0, qf ) in the last picture describes the language. It looks the
same as the regular expression obtained in the solution to Exercise 3.51(b),
Table 1
p r(p,1,0) r(p,2,0) r(p,3,0)
1

a + b
∅
2
∅
 + a
b
3
b
a

p r(p,1,1)
r(p,2,1)
r(p,3,1)
1

a + b
∅
2
∅
 + a
b
3
b
a + ba + bb

p r(p,1,2)
r(p,2,2)
r(p,3,2)
1

(a + b)a∗
(a + b)a∗b
2
∅
a∗
a∗b
3
b
(a + ba + bb)a∗ + (a + ba + bb)a∗b

Solutions to Selected Exercises
401
although the two algorithms do not always produce regular expressions
that look the same.
b
a + b
Λ
Λ
2
3
1
qo
qf
(a + b)a* + (a + b)a*b((a + ba + bb)a*b)*(a + ba + bb)a*
qo
qf
b
a + b
(a + b)a*
(a + b)a*b
(a + ba + bb)a*
a + ba + bb
Λ + (a + ba + bb)a*b
Λ
2
3
qo
qf
qf
a
b
a
a
3
qo
CHAPTER 4
4.1(d). The set of all strings in {a, b}∗not containing the substring bb.
4.6. If these two productions are the only ones with variables on the right
side, then no variable other than S can ever be involved in a derivation
from S, and the only other S-productions are S →x1 | x2 | . . . | xn,
where each xi is an element of {a, b}∗. In this case, no string of the form
ayb that is not one of the xi’s can be derived, which implies that not all
nonpalindromes can be generated.
4.13(b). S →aaSb | aaSbbb | aSb | . It is straightforward to check that every
string aibj obtained from this CFG satisﬁes the inequalities i/2 ≤j ≤
3i/2. We show the converse by strong induction on i. If i ≤2, the strings
aibj satisfying the inequalities are , ab, aab, aabb, and aabbb, and
all these strings can be derived from S. Suppose that k ≥2 and that for
every i ≤k and every j satisfying i/2 ≤j ≤3i/2, the string aibj can
be derived. We must show that for every j satisfying (k + 1)/2 ≤j ≤
3(k + 1)/2, the string ak+1bj can be obtained. We do this by considering
several cases.
If k is odd and j = (k + 1)/2, then ak+1bj = (aa)jbj, which can be
obtained by using the ﬁrst production j times.
If k is even and j = k/2 + 1, then ak+1bj = (aa)k/2abbk/2, and the
string can be obtained by using the ﬁrst production k/2 times and the
third production once.

402
Solutions to Selected Exercises
If k is odd and j = (k + 3)/2, then ak+1bj = (aa)(k−1)/2a2b2b(k−1)/2,
and the string can be obtained by using the ﬁrst production (k −1)/2
times and the third production twice.
If k is even and j = k/2 + 2, then ak+1bj = (aa)k/2−1a3b3bk/2−1,
and the string can be obtained by using the ﬁrst production k/2 −1 times
and the third production 3 times.
We have taken care of the cases in which (k + 1)/2 ≤j ≤(k + 4)/2.
In all the remaining cases, (k + 5)/2 ≤j ≤3(k + 1)/2 (and k is either
even or odd). It is not difﬁcult to check that (k −1)/2 ≤j −3 ≤3(k −
1)/2. It follows from the induction hypothesis that the string ak−1bj−3 can
be obtained from the CFG. Therefore, the string ak+1bj can be obtained
by using the same derivation except that one more step is added in which
the second production is used.
4.17. Call this context-free grammar G, and let L be the language of strings with
na(x) = 2nb(x). We sketch the proof by strong induction that L ⊆L(G).
The string  is in L(G). Suppose k ≥0 and every string in L of length
k or less is in L(G), and let x ∈L, with |x| = k + 1.
For a string z, let d(z) = na(z) −2nb(z). One of these six statements
must be true: (1) x begins with aab; (2) x begins with ab; (3) x begins
with aaa and ends with b; (4) x begins with aaa and ends with a; (5)
x begins with ba; (6) x begins with bb. We prove the induction step in
cases (2) and (4).
If x = aby, then d(ab) = −1 and d(x) = d(aby) = 0. Consider the
ﬁrst preﬁx of x for which d ≥0. Adding the last symbol of this preﬁx
causes d to increase, which means that the preﬁx ends with a. Therefore,
x = abwaz, where d(w) = d(z) = 0. The induction hypothesis implies
that w and z are both in L(G); we can then derive x by starting the
derivation with
S ⇒aSbSaS ⇒abSaS
and continuing to derive w from the ﬁrst S and z from the second.
In case (4), x = aaaya. Because d(aaa) > 0, we consider the short-
est nonnull preﬁx of x for which d ≤0. This preﬁx must end with b, so
that x = aaawbza. The possible values of d(aaaw) are 1 and 2. If it is
1, then d(aaw) = 0, so that x = a(aaw)bza. In this case, aaw and z are
in L(G) because of the induction hypothesis; we can derive x by starting
the derivation with
S ⇒aSbSaS ⇒aSbSa
and continuing so as to derive aaw from the ﬁrst S and z from the second.
If the value is 2, then d(aw) = 0, and x = aa(aw)b(za), so that d(za)
must be 0. The induction hypothesis tells us that aw and za are in L(G),
and we can then obtain x by a derivation that starts
S ⇒aSaSbS ⇒aaSbS

Solutions to Selected Exercises
403
4.32(a). In a derivation tree for the string with i occurrences of a, where i > 1,
the root node has two children, each of which is an S. From those two
S’s, i occurrences of a must be obtained. The possibilities are that 1 of
them comes from the left of the two and i −1 from the right, or two
come from the left and i −2 from the right, or . . . , or i −1 come from
the left and 1 comes from the right. The resulting formula is
ni =
i−1

j=1
njni−j
4.40(b). Using the criterion in part (a), we prove by strong induction that every
string x of parentheses in which no preﬁx has more right parentheses
than left can be derived from the grammar. The string  can be derived.
Suppose that k ≥0 and that for every string x of length ≤k in which
every preﬁx has at least as many left parentheses as right, x can be
derived.
Suppose that |x| = k + 1 and every preﬁx of x has at least as many
left parentheses as right. Then x = (y for some y. If every preﬁx of y
has at least as many left parentheses as right, then y can be derived
from the grammar, by the induction hypothesis; therefore, x = (y can,
because we can start the derivation S ⇒(S and continue by deriving y
from S.
If y has a preﬁx with an excess of right parentheses, let y1 be the
smallest such preﬁx. Then y1 must be y2), for some string y2. The string
y2 has equal numbers of left and right parentheses, and every preﬁx of y2
has at least as many left as right. We can write x = (y2)z for some string
z. Because every preﬁx of x has at least as many left as right, and because
(y2) has equal numbers, every preﬁx of z must have at least as many left
as right. Therefore, by the induction hypothesis, S ⇒∗y2 and S ⇒∗z. It
follows that S ⇒∗(y2)z, since we can start the derivation S ⇒(S)S and
continue by deriving y2 from the ﬁrst S and z from the second.
4.45. Consider the CFG in part (b). We prove by strong induction that for
every n ≥0, and every x such that S ⇒∗x and |x| = n, x has only one
leftmost derivation. This is true for n = 0. Suppose that k ≥0 and that
the statement is true for every n ≤k, and now suppose that S ⇒∗x and
|x| = k + 1. The ﬁrst step of a derivation of x is S ⇒S(S). It follows
that x = (y)z, where S ⇒∗y, S ⇒∗z, |y| ≤k, and |z| ≤k. Therefore, by
the induction hypothesis, both y and z have only one leftmost derivation.
In order to conclude that x does, we must eliminate the possibility that
x can be written in two different ways as x = (y)z, where S ⇒∗y and
S ⇒∗z. The reason this is impossible is that if x = (y)z and y and z are
both balanced strings of parentheses, then the right parenthesis shown is
the unique mate of the left one. This is shown in Theorem 4.25 for a
different grammar, and the argument for this CFG is essentially the same.

404
Solutions to Selected Exercises
4.56(b). In order to show that L(G1) ⊆L(G), one can prove that if T ⇒∗
G1 x or
U ⇒∗
G1 x, then S ⇒∗
G x. In the ﬁrst case, the statement is true because
both productions S →aSbS | c are present in G; the second can be
proved using strong induction on |x|.
In order to prove that L(G) ⊆L(G1), we can adapt the results of
Exercise 4.40. We can see from the productions in G that every string
generated from S must end with c, and therefore that every b in such a
string must be immediately preceded by c; moreover, every string gen-
erated from S has exactly one more c than b. These observations allow
us to say that strings in L(G) match the regular expression a∗c(ba∗c)∗,
and Exercise 4.40 suggests that L(G) is the set of strings matching this
expression in which every preﬁx has at least as many a’s as b’s. Although
we will not prove this characterization of L(G), we will use it in show-
ing that L(G) ⊆L(G1). The proof uses strong induction to prove the
following three statements simultaneously: (i) for every x ∈L(G) hav-
ing a derivation in G that involves only the productions S →aSbS | c
(which means that x ∈L(G) and x has equal numbers of a’s and b’s),
S1 ⇒T ⇒∗
G1 x; (ii) for every x ∈L(G) having a derivation in G that
begins with the production S →aS, S1 ⇒U ⇒∗
G1 x; and (iii) for every
x in L(G), if x has more a’s than b’s and every derivation of x in G
begins with the production S →aSbS, then S1 ⇒U ⇒∗
G1 aT bU ⇒∗
G1 x.
The basis step is straightforward. Suppose all three statements are true
for strings of length ≤k, and now suppose x ∈L(G) and |x| = k + 1.
If x has a derivation involving only S →aSbS and S →c, then either
x = c, or x = aybz for strings y and z in L(G) that also have derivations
involving only these productions. Because G1 contains the productions
T →aT bT | c, it follows from the induction hypothesis that S1 ⇒
T ⇒∗
G1 x. If x has a derivation beginning with S →aS, then x = ay for
some y ∈L(G), the induction hypothesis implies that y ∈L(G1), and
x ∈L(G1) because G1 contains the productions S1 →U and U →aS1.
Finally, suppose that x ∈L(G), x has more a’s than b’s, and every deriva-
tion of x in G begins with the production S →aSbS. Then the string x
matches the regular expression a∗c(ba∗c)∗, every preﬁx has at least as
many a’s as b’s, and there is at least one b. Let x = x1y1, where x1 is of
the form a∗cba∗c. If x started with aa, then x1 would be ax2 for some
x2 ∈L(G), and x would have a derivation beginning S ⇒aS; therefore,
x = acbz for some string z. The string z also matches the regular expres-
sion a∗c(ba∗c)∗, every preﬁx of z has at least as many a’s as b’s, and z
has more a’s than b’s. The induction hypothesis implies that U ⇒∗
G1 z,
and we may conclude that S1 ⇒U ⇒aT bU ⇒∗
G1 x.
4.56(c). It can be shown by strong induction on n that for every x with |x| ≥1,
if x can be derived from one of the three variables in n steps, then x has
only one leftmost derivation from that variable. We present the induction
step in the third case, and show that if x can be derived from U in k + 1
steps, then x has only one leftmost derivation from U. If x has a preﬁx

Solutions to Selected Exercises
405
of the form ayb, where y has equal numbers of a’s and b’s, then the ﬁrst
step in a derivation of x from U must be U ⇒aT bU (because if there
were a derivation starting U ⇒aS1, some string derivable from S1 would
have more b’s than a’s), and if no preﬁx of x has this form, then the ﬁrst
step in a derivation of x from U must be U ⇒aS1.
If the ﬁrst step in the derivation of x from U is U ⇒aS1, the induc-
tion hypothesis tells us that there is only one way to continue a leftmost
derivation, which means that x has only one leftmost derivation from U.
If the ﬁrst step is U ⇒aT bU, then x = aybz, where y and z can be
derived from T and U, respectively, in k or fewer steps. By the induction
hypothesis, y has only one leftmost derivation from T and z has only one
from U. Furthermore, there is only one choice for the string y, because
the preﬁx ayb of x must be the shortest preﬁx having equal numbers
of a’s and b’s (otherwise y would have a preﬁx with an excess of b’s).
Therefore, x has only one leftmost derivation from U.
CHAPTER 5
5.8(b). See Table 2.
Table 2
Move No.
State
Input
Stack Symbol
Move(s)
1
q0
a
Z0
(q1, aZ0)
2
q0
b
Z0
(q0, bZ0)
3
q0
a
b
(q1, )
4
q0
b
b
(q0, bb)
5
q1
a
Z0
(q2, aaZ0)
6
q1
b
Z0
(q1, bZ0)
7
q1
a
a
(q2, aaa)
8
q1
a
b
(q2, a)
9
q1
b
a
(q1, )
10
q1
b
b
(q1, bb)
11
q2
a
Z0
(q2, aZ0), (q2, aaZ0)
12
q2
a
a
(q2, aa), (q2, aaa)
13
q2
a
b
(q2, ), (q2, a)
14
q2
b
a
(q2, )
15
q2
b
b
(q2, bb)
16
q2

Z0
(q3, Z0)
(all other combinations)
none
The initial state is q0 and the accepting state q3. The ﬁrst a read
will be used to cancel one b, the second will be used to cancel two b’s,
and subsequent a’s can be used either way. (Note that every string in the
language has at least two a’s.)
The PDA stays in q0 as long as no a’s have been read. If and when
the ﬁrst a is read, it will be used to cancel a single b, either by removing b

406
Solutions to Selected Exercises
from the stack, or, if a is the ﬁrst input symbol, saving it on the stack for
a subsequent b to cancel. The state q1 means that a single a has been read;
if and when a second a is read, it will be used to cancel two b’s (either
by popping two b’s from the stack, or by popping one from the stack and
pushing an a to be canceled later, or by pushing two a’s), and the PDA
will go to state q2. In q2, each subsequent a will cancel either one or two
b’s, and the machine can enter q3 if the stack is empty except for Z0.
5.13. An NFA can be constructed to accept L, having as states pairs (q, α),
where q is a state in M and α is a string of k or fewer stack symbols,
representing the current contents of M’s stack. Such a pair provides a
complete description of M’s current status, and for every such pair and
every input (either  or an input symbol) it is possible using M’s deﬁ-
nition to specify the pairs that might result. Furthermore, there are only
ﬁnitely many such pairs. The initial state of the NFA is (q0, Z0), where
q0 is the initial state of M, and the pairs (q, α) that are designated as
accepting states are those for which q is an accepting state of M.
5.15. Yes. We can carry out the construction in the solution to Exercise 5.13,
with one addition. There is one additional state s to which any transi-
tion goes that would otherwise result in a pair (q, α) with |α| > k. All
transitions from s return to s. This guarantees that the NFA functions
correctly, because for every input string x in L, there is a sequence of
moves corresponding to x that never causes the NFA to enter s.
5.18(c). In the PDA shown in Table 3, q0 is the initial state and the only accepting
state. Let x be the current string. Because we wish to compare na(x) to
2nb(x), we will essentially treat every b we read as if it were two b’s. This
means that if we read a b when the top stack symbol is either b or Z0, we
push two b’s onto the stack and go to q1. If the top stack symbol is a when
we read a b, we pop it off and go to a temporary state; at that point, without
reading an input symbol, we pop a second a off if there is one, and push
a b on otherwise, and go to q1 in either case. Finally, top stack symbol
Table 3
Move No.
State
Input
Stack Symbol
Move(s)
1
q0
a
Z0
(q1, aZ0)
2
q0
b
Z0
(q1, bbZ0)
3
q1
a
a
(q1, aa)
4
q1
a
b
(q1, )
5
q1
b
b
(q1, bb)
6
q1
b
a
(qt, )
7
qt

a
(q1, )
8
qt

Z0
(q1, bZ0)
9
q1

Z0
(q0, Z0)
(all other combinations)
none

Solutions to Selected Exercises
407
Z0 in either q0 or q1 means that currently the number of a’s is twice the
number of b’s, so that if we are in q1 we make a -transition to q0.
5.19. To do this for L1L2, the basic idea is the same as for FAs: The compos-
ite machine acts like M1 until it reaches an accepting state, then takes
a -transition to the initial state of M2. However, we have to be more
careful because of the stack. For example, it’s possible for M1 to accept
a string and to have an empty stack when it’s done processing that string,
but if our composite machine ever has an empty stack, it can’t move.
The solution is to ﬁx it so that the composite machine can never empty
its stack. This can be done by inserting a new stack symbol Z under the
initial stack symbol of M1 and then returning to the initial state of M1.
From that point on, the moves are the same as those of M1, unless Z
appears on top of the stack or unless the state is an accepting state of M1.
If Z is on top of the stack and the state is not an accepting state in M1, the
machine crashes. If it reaches an accepting state of M1, then regardless
of what’s on top of the stack, it takes a -transition to the initial state of
M2 and pushes the initial stack symbol of M2 onto the stack. From then
on, it acts like M2, except that it never removes the initial stack symbol
of M2 from the stack. (If M2 ever removed its initial stack symbol, that
would be the last move it made, so our new machine can achieve the
same result as M2 without ever removing this symbol.)
5.20. Let M = (Q, , , q0, Z0, A, δ) be a DPDA accepting L. We construct
a DPDA M1 = (Q1, , , q′
0, Z0, A, δ1) accepting the new language as
follows. Q1 = Q ∪Q1, where Q1 is a set containing another copy q′
of every q ∈Q. (In particular, the initial state of M1 is the primed
version of q0.) For each q′ ∈Q′, and each a ∈ ∪{} and X ∈,
δ1(q′, a, X) = δ(q, a, X)′. In other words, for inputs that are  or ordi-
nary alphabet symbols, the new PDA behaves the same way with the
states q′ that M does with the original states. In addition, if q ∈A, then
for every stack symbol X, δ1(q′, #, X) = {(q, X)}. Finally, δ1 agrees with
δ for elements of Q and inputs other than #.
What this means is that M1 acts the same way as M up to the point
where the input # is encountered, except that the new states allow it to
remember that it has not yet seen #. Once this symbol is seen, if the
current state is q′ for some q ∈A (i.e., if M would have accepted the
current string), then the machine switches over to the original states for
the rest of the processing. It enters an accepting state subsequently if and
only if both the substring preceding the # and the entire string except for
the # would be accepted by M.
5.25(b). For x ∈{a, b}∗, let d(x) = 2nb(x) −na(x). We use the three states q0,
q+, and q−to indicate that the quantity d(x) is currently 0, positive,
or negative, respectively, and the number of ∗’s on the stack indicates
the current absolute value of d(x). q0 is the accepting state. The counter
automaton is shown in Table 4.

408
Solutions to Selected Exercises
Table 4
Move No.
State
Input
Stack Symbol
Move(s)
1
q0
a
Z0
(q−, ∗Z0)
2
q0
b
Z0
(q+, ∗∗Z0)
3
q−
a
∗
(q−, ∗∗)
4
q−
b
∗
(qt, )
5
qt

Z0
(q+, ∗Z0)
6
qt

∗
(q−, )
7
q−

Z0
(q0, Z0)
8
q+
b
∗
(q+, ∗∗∗)
9
q+
a
∗
(q+, )
10
q+

Z0
(q0, Z0)
(all other combinations)
none
Line 4 contains the move in which d(x) < 0 and we read the symbol
b. If there are at least two ∗’s on the stack, we pop two off; if there is
only one, we go to q+ and push one ∗onto the stack. The state qt is a
temporary state used in this procedure.
From both q−and q+, we can make a -transition to q0 when the
stack is empty except for Z0.
5.34(a). See Table 5.
Table 5
Stack (reversed)
Unread Input
Derivation Step
Z0
[][[]]
Z0 [
][[]]
Z0 [S
][[]]
⇒[][[]]
Z0 [S]
[[]]
Z0 [S][
[]]
Z0 [S][[
]]
Z0 [S][[S
]]
⇒[S][[]]
Z0 [S][[S]
]
Z0 [S][[S]S
]
⇒[S][[S]]
Z0 [S][S
]
⇒[S][[S]S]
Z0 [S][S]
Z0 [S][S]S
⇒[S][S]
Z0 [S]S
⇒[S][S]S
Z0 S
⇒[S]S
S
5.41(c).
S →S1$
S1 →abX
X →T X | 
T →aU
U →T bb | b

Solutions to Selected Exercises
409
CHAPTER 6
6.2(e). Suppose L is a CFL, and let n be the integer in the pumping lemma. Let
u = anbncn. Then u = vwxyz, where the usual three conditions on v, w,
x, y, and z hold.
If wy contains at least one a, then since |wxy| ≤n, wy can contain
no c’s. Therefore, vw0xy0z contains fewer than n a’s but exactly n c’s,
and so it is impossible for this string to be in L. If wy contains no a’s,
then vw2xy2z contains either more than n b’s or more than n c’s, but
exactly n a’s. In this case also, the string cannot be in L. Therefore, we
have a contradiction.
6.9(a). Call this language L. Then L is the union of the two languages {aibjck|i ≥
j} and {aibjck | i ≥k}, each of which is a CFL. The complement of L
in {a, b, c}∗is L′ = ({a}∗{b}∗{c}∗)′ ∪{aibjck | i < j and i < k}. From
the formula L′ ∩{a}∗{b}∗{c}∗= {aibjck | i < j and i < k} and Theorem
6.13, it follows that if L is a CFL, so is {aibjck | i < j and i < k}.
However, we can show using the pumping lemma that this is not the
case. Suppose {aibjck | i < j and i < k} is a CFL and let n be the
integer in the pumping lemma. Let u = anbn+1cn+1. Then u = vwxyz,
where the usual conditions hold. If wy contains a’s, it can contain no c’s,
and we obtain a contradiction by considering vw2xy2z. If wy contains no
a’s, we obtain a contradiction by considering vxz.
6.11(a). Suppose L = {aibi+kak | k ̸= i} is a CFL. Let n be the integer in Ogden’s
lemma, let u = anb2n+n!an+n!, and designate the ﬁrst n positions of u as
the distinguished positions. Then u = vwxyz, where wy contains at least
one from the ﬁrst group of a’s and vwixyiz ∈L for every i ≥0. If either
w or y contained both a’s and b’s, then clearly vw2xy2z would not be in
L. Also, if neither w nor y contained any b’s, then vw2xy2z would not
preserve the balance between a’s and b’s required for membership in L.
Therefore, w contains only a’s from the ﬁrst group, and y contains only
b’s. If the lengths of w and y were different, vw2xy2z could not be in L.
If the lengths are equal, say p, then let i = 1 + n!/p. Then
vwixyiz = an+(i−1)pb2n+n!+(i−1)pan+n! = an+n!b2n+2n!an+n!
which is not an element of L. This contradiction implies that L is not a
CFL.
6.16(b). Yes. Given a PDA M = (Q, , , q0, Z0, A, δ) accepting L, we can con-
struct another PDA M1 = (Q1, , , q1, Z0, A1, δ1) accepting the set of
preﬁxes of elements of L as follows. Q1 can be taken to be Q × {0, 1}—
in other words, a set containing two copies (q, 0) and (q, 1) of each
element of Q. The initial state q1 is (q0, 0), and the set A1 is A × {1}.
For each combination (q, 0) (where q ∈Q), a ∈, and X ∈, the set
δ1((q, 0), a, X) is simply {((p, 0), α) | (p, α) ∈δ(q, a, X)}; however,
δ1((q, 0), , X) contains not only the elements ((p, 0), α) for which

410
Solutions to Selected Exercises
(p, α) ∈δ(q, , X), but one additional element, ((q, 1), X). For each q ∈
Q, a ∈, and X ∈, δ1((q, 1), a, X) = ∅. Finally, for each q ∈Q and
each X ∈, δ1((q, 1), , X) is the union of the sets {((p, 1), α) | (p, α) ∈
δ(q, a, X)} over all a ∈ ∪{}.
The idea here is that M1 starts out in state (q0, 0) and acts on the
states (q, 0) exactly the same way M acts on the states q, until such time
as M1 takes a -transition from its current state (q, 0) to (q, 1). From
this point on, M1 can make the same moves on the states (q, 1) that
M can make on the states q, changing the stack in the same way, but
without reading any input. If xy ∈L, then for any sequence of moves M
makes corresponding to xy, ending in the state r, one possible sequence
of moves M1 can make on xy is to copy the moves of M while process-
ing x, reaching some state (qx, 0), then make a -transition to (qx, 1),
then continue simulating the sequence on the states (p, 1) but making
only -transitions, ending at the state (r, 1) having processed the string
x. Therefore, x is accepted by M1. Conversely, if x is accepted by M1,
there is a sequence of moves corresponding to x that ends at a state (p, 1),
where p ∈A. In this case, the moves of the sequence that involve states
(q, 0), ending at (qx, 0), correspond to moves M can make processing x,
ending at the state qx; and the remaining -transitions that end at (p, 1)
correspond to transitions that M can make using the symbols of some
string y. Therefore, if M1 accepts x, then there is a string y such that M
accepts xy.
6.16(f). No. Suppose L is a CFG, and let n be the integer in the pumping lemma.
Let u = apbpcp, where p is a prime larger than n. Then u = vwxyz,
where |wy| > 0, |wxy| ≤n, and vwixyiz ∈L for every i ≥0. These
conditions imply that wy contains no more than two distinct symbols.
Consider the string vw0xy0z = vxz, and let j = na(vxz), k = nb(vxz),
and m = nc(vxz). At least one of these is less than p, and all three are
positive. Because p is prime, it is not possible for all three to have a
nontrivial common factor. Therefore, L is not a CFG.
6.20(a). The languages L1 and L2 in Example 6.12 are both DCFLs. The language
(L1 ∪L2)′ ∩{a}∗{b}∗{c}∗is {aibjck | i > j > k}, which can be shown
by the pumping lemma not to be a CFL. It follows from Theorem 6.13
that (L1 ∪L2)′ is not a CFL, and it follows from the last paragraph of
Section 6.2 that L1 ∪L2 is not a DCFL.
6.20(c). Let L1 and L2 be as in part (a). Let L3 = {d}L1 ∪L2. Then L3 is a
DCFL, because in order to accept it, the presence or absence of an initial
d is all that needs to be checked before executing the appropriate DPDA
to accept either L1 or L2. The language {d}∗is also a DCFL. However,
{d}∗L3 is not a DCFL. The reason is that {d}∗L3 ∩{d}{a}∗{b}∗{c}∗=
{d}L1 ∪{d}L2. If {d}∗L3 were a DCFL, then this language would be
also (see Exercise 6.8), and it follows easily that L1 ∪L2 would be
as well.

Solutions to Selected Exercises
411
6.21(a). Let L1 = {x#y | x ∈pal and xy ∈pal}. Then if pal were a DCFL, L1
would be, and therefore, by Exercise 6.8, L1 ∩{a}∗{b}∗{a}∗{#}{b}∗{a}∗
would be also. However, this intersection is {aibjai#bjai | i, j ≥0}, and
it’s easy to show using the pumping lemma that this language is not even
a CFL.
6.23. To simplify things slightly, let D = {i | ai ∈L}. Then it will be sufﬁcient
to show that D is a ﬁnite union of (not necessarily inﬁnite) arithmetic
progressions of the form {m + ip | i ≥0}. (See Exercise 2.30.)
Let n be the integer in the pumping lemma applied to L. Then for
every m ∈D with m ≥n, there is an integer pm with 0 < pm ≤n for
which m + ipm ∈D for every i ≥0. There are only a ﬁnite number of
distinct pm’s; let p be the least common multiple of all of them. Then
for every m ∈D with m ≥n, m + ip ∈D for every i ≥0.
For each j with 0 ≤j < p, the set Sj = {r ∈D | r ≥n and r ≡
j mod p} is either empty or an inﬁnite arithmetic progression. (If it’s
not empty, it’s the arithmetic progression {mj + ip | i ≥0}, where mj
is the smallest element of Sj.) Furthermore, {r ∈D | r ≥n} is the union
of the sets Sj. Therefore, because the set {r ∈D | r ≤n} is the union
of one-element arithmetic progressions, D is a ﬁnite union of arithmetic
progressions.
CHAPTER 7
7.3. A conﬁguration is determined by specifying the state, the tape contents,
and the tape head position. There are s + 2 possibilities for the state,
including ha and hr; (t + 1)n+1 possibilities for the tape contents, since
there are n + 1 squares, each of which can have an element of  or a
blank; and n + 1 possibilities for the tape head position. The total number
of conﬁgurations is therefore (s + 2)(t + 1)n+1(n + 1).
7.4(b).
a/a, R
b/b, R
b/b, R
b/b, R
Δ/Δ, R
a/a, L
b/b, L
a/Δ, R
Δ/Δ, S
b/b, R
Δ/Δ, L
Δ/Δ, R
b/Δ, L
ha
7.10. A -transition in an NFA or a PDA makes it possible for the device to
make a move without processing an input symbol. Turing machines have
such moves already, because they are not restricted to a single left-to-
right pass through the input; they can make moves in which the tape head
remains stationary or moves to the left.

412
Solutions to Selected Exercises
7.13. Suppose there is such a TM T0, and consider a TM T1 that halts in the
conﬁguration ha1. Let n be the number of the highest-numbered square
read by T0 in the composite machine T1T0. Now let T2 be another TM
that halts in the conﬁguration ha1n1. Because the ﬁrst n tape squares
are identical in the ﬁnal conﬁgurations of T1 and T2, T0 will not read
the rightmost 1 left by T2, and so T2T0 will not halt with the tape head
positioned on the rightmost nonblank symbol left by T2.
7.17(f). The ﬁgure does not show the transitions for the last phase of the compu-
tation. In two places, “yes” stands for the moves that erase the tape and
halt in the accepting state with output 1; “no” stands for exactly the same
moves except that the output in that case is 0.
Δ/Δ, R
Δ/Δ, R
Δ/Δ, L
a/a, R
b/b, R
A/A, R
B/B, R
a/a, R
b/b, R
A/A, R
B/B, R
A/A, R
B/B, R
A/A, R
B/B, R
A/A, L
B/B, L
a/a, L
b/b, L
a/A, R
b/B, R
a/A, L
b/B, L
yes
Δ/Δ, R
Δ/Δ, R
b
no
a
yes
no
a,b
Δ
7.24. Here is a verbal description of one solution. Start by inserting an additional
blank at the beginning of the tape, to obtain x, where x is the input
string. Make a sequence of passes. The ﬁrst moves the ﬁrst symbol of
x one square to the left and the last symbol one square to the right;
the second moves the second symbol of x one square to the left and
the next-to-last symbol one square to the right; etc. If x has odd length,
reject. The tape now looks like x1x2, where x1 and x2 are the ﬁrst
and second halves of x. Now begin comparing symbols of x1 and x2,
but replacing symbols that have been compared by blanks, rather than
uppercase letters as in Example 7.5. In order for this to work, we check
before each pass to see whether the symbols that are about to be compared
are the last symbols in the respective halves; this avoids any attempt to
make another pass if there are no more nonblank symbols, which would
cause the TM to reject by attempting to move the tape head off the
tape.
7.33(a). If the tape head does not move past square n, the possible number of
nonhalting conﬁgurations is s(t + 1)n+1(n + 1), where s and t are the
sizes of Q and , respectively (see the solution to Exercise 7.3). Within

Solutions to Selected Exercises
413
that many moves, the TM will have either halted or repeated a nonhalting
conﬁguration, so that if it has not halted it is in an inﬁnite loop.
7.33(b). Let s be the size of Q. Suppose you watch the TM for s moves after it
has reached the ﬁrst square after the input string, and that each move is
to the right. Then the TM has encountered the combination (q, ) twice
for some state q, and it is in an inﬁnite loop.
7.35. Yes. The assumption on T implies that T can make no more than k
moves in which it does not move its tape head to the right. (Every such
move increases the difference between the number of moves made and the
current tape head position, and the assumption doesn’t allow the difference
to exceed k.) This observation allows us to build another TM T1 that
mimics T but always moves its tape head to the right, by allowing T1
enough states so that it can remember the contents of the k/2 squares
preceding the current one. By examining the transition table for T , we
can predict the contents of the current tape square and the state to which
T1 goes when it ﬁnally moves its tape head to the right. Therefore, there
is a TM that accepts the same language as T and always moves its tape
head to the right. This implies that L(T ) is regular (see Exercise 7.34(b)).
7.42. A Post machine simulates a move to the left on the tape by remembering
the last two symbols that have been removed from the front of the queue.
Suppose for example that the initial contents of the queue are abaab,
with the a at the front. The machine starts by placing a marker X at
the rear, to produce abaabX. It proceeds to remove symbols from the
front, remembering the last two, and placing the symbols at the rear but
lagging behind by one. After one such move, the contents are baabX
and the machine remembers that it has removed an a; after two steps the
contents are aabXa and the machine remembers that the last symbol was
b. When it removes X, it places X at the rear immediately, followed by
the b it remembers removing before X, to produce abaaXb. At this point
it begins moving symbols from the front to the rear and continues until it
has removed the X, not replacing it. The contents of the queue are now
babaa, and the move to the left has been effectively simulated.
CHAPTER 8
8.14. Suppose that L is inﬁnite and recursive. Then L is the range of some
computable increasing function from ∗to ∗. Because the function is
increasing, it represents a bijection f from ∗to L, and we may consider
its inverse f −1 : L →∗, which is also a bijection and also increasing.
Let x0, x1, . . . be an enumeration of ∗in canonical order. The function
f −1 is also computable, because we can evaluate f −1(y) by computing
f (x0), f (x1), . . . until we ﬁnd the string xi such that f (xi) = y.
A subset A of ∗is recursive if and only if f (A) = {f (x) | x ∈A} is
recursive. The reason is that determining whether a string x is an element

414
Solutions to Selected Exercises
of A is equivalent to determining whether f (x) is an element of f (A),
and because f and f −1 are computable, if we can do one of these, we
can do the other. For the same reason, A is recursively enumerable if and
only if f (A) is.
Now we can answer the question in the exercise. We can produce a
subset S of L that is not recursively enumerable by starting with a subset
A of ∗that is not and letting S be f (A). The same technique works to
ﬁnd an S that is recursively enumerable but not recursive.
8.20(b).
S →ABCS | ABBCS | AABBCS | BCS | BBCS | CS | BC
AB →BA BA →AB AC →CA CA →AC BC →CB CB →BC
A →a
B →b
C →c
Checking that every string generated by this grammar satisﬁes the
desired inequalities is a straightforward argument. The other direction
is more challenging. Let us denote by nA, nB, and nC the numbers of
A’s, B’s, and C’s, respectively, in a string obtained by the ﬁrst seven
productions. Let x = nA, y = nB −nA −1, and z = 2nC −nB −1, and
for each i ≤6 let ni be the number of times the ith production is used in
the derivation of the string. We may write the equations
x
=
n1
+
n2
+
2n3
y
=
n2
+
n4
+
2n5
z
=
n1
+
n4
+
2n6
It is sufﬁcient to show that for every choice of x, y, and z whose sum is
even, the equations are satisﬁed by some choice of nonnegative integer
values for all the xi’s. This can be veriﬁed by considering cases. When
x, y, and z are all even, for example, there is a solution in which n1 is
the minimum of x and z, and n2 = n4 = 0.
8.32. We will show that each production α →β, where |β| ≥|α|, can be
replaced by a set of productions of the desired form, so that the new
grammar generates the same language.
Suppose α = γ1A1γ2A2γ3 . . . γnAnγn+1, where the Ai’s are variables
and each γi is a string of terminals. The ﬁrst step is to introduce new
variables X1, . . . , Xn and productions of the desired type that allow the
string γ1X1γ2X2γ3 . . . γnXnγn+1 to be generated. These new variables can
appear only beginning with the string α and will be used only to obtain
the string β; the effect of this ﬁrst step is to guarantee that none of the
productions we are adding will cause strings not in the language to be
generated. The ﬁrst production is
γ1A1γ2A2γ3 . . . γnAnγn+1 →γ1X1γ2A2γ3 . . . γnAnγn+1
The next allows A2 to be replaced by X2, and so forth; the last allows
An to be replaced by Xn.

Solutions to Selected Exercises
415
The second step is to introduce additional variables, as well as pro-
ductions that allow us to generate a string Y1Y2 . . . Yk, where each Yi is
a variable and k = |α|. For example, if γ1 = abc, we can start with the
productions
cX1 →Y3X1
bY3 →Y2Y3
aY2 →Y1Y2
The variable Y4 will be the same as X1. Similarly, if γ2 = defg, we would
use X1d →X1Y5, Y5e →Y5Y6, and so forth. All these productions are of
the right form and they allow us to obtain the string Y1 . . . Yk. The Yi’s,
like the Xi’s, are reserved for this purpose and are used nowhere else in
the grammar.
The third step is to add still more productions that produce the string
β. Let
β = Z1Z2 . . . ZkZk+1 . . . Zm
where m ≥k and each Zi is either a variable or a terminal. The produc-
tions we need are
Y1Y2 →Z1Y2, Y2Y3 →Z2Y3,
. . . ,
Yk−1Yk →Zk−1Yk,
and Zk−1Yk →Zk−1ZkZk+1 . . . Zm
All the productions we have added are of the proper form; they permit
β to be derived from α; each one except the last one has a new variable
on the right side; and the left side of the last one can have been obtained
only starting from the string α. Therefore, only the strings derivable in
the original grammar can be derived in the new one.
8.34(a). Let S1 and S2 be the start symbols of G1 and G2, respectively. Let G
have new start symbol S as well as all the variables in G1 and G2 and
three new variables L, M, and R. G will have the productions in G1 and
those in G2 and the new productions
S →LS1MS2R
Lσ →σL
LM →L
LR →
(for every σ ∈). The idea is that the variables L and M prevent any
interaction between the productions of G1 and those of G2. As long as M
remains in place, no production can be applied whose left side involves
variables in G1 and terminal symbols produced by a production in G2;
and at the point when M is eliminated by the production LM →L,
there can be no variables in G1 remaining, because until L reaches M it
can move only by passing terminal symbols. Thereafter, as long as there
are variables remaining in the string, they are separated by L from any
terminal symbols that once preceded M. Therefore, no production can
be applied whose left side involves both terminals produced by G1 and
variables in G2.
8.41(c). Consider the sets P1, the set of nonempty subsets of N; P2, the set
of nonempty subsets of N −{0}; and P3, the set of two-subset partitions
of N.

416
Solutions to Selected Exercises
P1 is uncountable because 2N is. There is a bijection from P1 to P2,
because there is a bijection from N to N −{0}, and so P2 is uncountable.
Finally, we construct a bijection from P2 to P3, which will imply that P3
is uncountable. For a nonempty subset A of N −{0}, let f (A) be the
partition consisting of A and N −A. The function f is onto, because for
every two-set partition of N, one of the two sets is a nonempty set not
containing 0. It is also one-to-one, because if the two sets of a partition
are A and N −A, one of them fails to contain 0, so that the partition
cannot be both f (A) and f (N −A).
The set P3 is a subset of the set of all ﬁnite partitions of N, which
is therefore also uncountable.
8.41(g). For a subset S = {n0, n1, . . . } of N, where the ni’s are listed in increasing
order, we may consider the function f : N →N that takes the value 0
at every number i satisfying 0 ≤i < n0, the value 1 at every i with
n0 ≤i < n1, and so forth. This correspondence deﬁnes a bijection from
2N to a subset of the set of all nondecreasing functions from N to N,
and it follows that the set of such functions is uncountable.
8.46(b). Suppose f0, f1, . . . is a list of bijections from N to N. We will describe
a method for constructing a bijection f : N →N that is different from
fn for every n.
The idea of the construction is that for every n, two conditions will be
true. First, f (2n) ̸= fn(2n), which will guarantee that f ̸= fn; second, the
set {f (2n), f (2n + 1)} will be {2n, 2n + 1}, and the fact that this holds
for every n will guarantee that f is a bijection from N to N.
The second condition will be true because we will choose f (2n) to
be one of the two numbers 2n and 2n + 1, and f (2n + 1) to be the other
one. The ﬁrst will be true because at least one of these two numbers is
different from fn(2n), and that’s the one we choose for f (2n). (If both
numbers are different from fn(2n), we can choose either one for f (2n).)
8.48(a). If we deﬁne f0 : A0 →B by f0(x) = f (x), then f0 is one-to-one because
f is. For x ∈A0, f0(x) ∈B1, because the number of ancestors of f0(x)
is one more than the number of ancestors of x. Finally, for every y ∈B1,
y = f (x) for some x ∈A0. Therefore, f0 is a bijection from A0 to B1.
Similarly, g0 is a bijection from B0 to A1. It is not difﬁcult to see that the
function f∞from A∞to B∞deﬁned by f∞(x) = f (x) is a bijection.
We deﬁne F : A →B by letting F(x) = f (x) if x ∈A0 or x ∈A∞
and F(x) = g−1(x) if x ∈A1(x). It follows that F is a bijection from A
to B.
8.48(b). Let f : A →B and g : B →C be the two bijections. Then g ◦f is a
bijection from A to a subset of C. If there were a bijection h : A →C,
then h−1 would be a bijection from C to A; this would mean that h−1 ◦g
is a bijection from B to a subset of A. We already have a bijection from
A to a subset of B, and the Schr¨oder-Bernstein theorem would imply that
there was a bijection from A to B; but we are assuming this is not the case.

Solutions to Selected Exercises
417
CHAPTER 9
9.10. One answer to (a) is C = A ∪B and D = A ∩B. For the reduction in (b),
given Turing machines T1 and T2, we can construct T ′
1 and T ′
2 accepting
L(T1) ∪L(T2) and L(T1) ∩L(T2), respectively. Then L(T1) = L(T2) if
and only if L(T ′
1) ⊆L(T ′
2).
9.12(l). This problem is undecidable, because we can reduce Accepts- to it.
Given a TM T , an instance of Accepts-, we construct T1 as follows.
T1 has all of T ’s states as well as one additional state q; it has all
of T ’s tape symbols and an additional symbol $. The transitions of T1
are the same as those of T , with these additions and modiﬁcations. For
every accepting move δ(p, a) = (ha, b, D) of T , T1 has instead the move
δT1(p, a) = (q0, $, S), where q0 is the initial state of T . If the nonhalting
states of T are enumerated q0, q1, . . . , qn, then T1 has the initial tran-
sitions δT1(qi, $) = (qi+1, $, S) for 0 ≤i < n, δT1(qn, $) = (q, $, S), and
δT1(q, $) = (ha, $, S).
To summarize: For any move that would cause T to accept, T1 instead
moves to q0 and places $ on the tape; thereafter, the $ causes T1 to cycle
through its nonhalting states, q being the last, before accepting. If T
accepts , then some move causes T to accept, and therefore, T1 enters
all its nonhalting states when started with a blank tape. On the other hand,
if T doesn’t accept , then starting with a blank tape, T1 will never enter
the state q and does not enter all its nonhalting states.
9.14(a). Suppose x, y ∈{0, 1}∗, and let T be an instance of P{x}. Then we construct
T1 so that it begins by looking at its input; if the input is x, T1 replaces
it by y; if the input is y, T1 replaces it by x; and for any input other than
x or y, it makes no changes. From this point on, T1 simply executes T .
It follows that T accepts x and no other strings if and only if T1 accepts
y and no other strings, so that the construction is a reduction from P{x}
to P{y}.
9.14(b). If T is an instance of P{x}, we might try constructing T1 such that it
replaced either input y or input z by x before executing T . We would
then be able to say that T accepts x if and only if T1 accepts both y and
z. However, we could not be sure that T accepts no strings other than x
if and only if T1 accepts no strings other than y or z.
We describe how T1 might be constructed in the case when x pre-
cedes y and y precedes z in canonical order. Other cases can be handled
similarly. T1 begins by looking at its input and making these preliminary
steps: if the input is either y or z, T1 replaces it by x; If the input is
x, T1 replaces it by y; and if the input is any string w that follows z in
canonical order, T1 replaces it by its immediate predecessor in canonical
order. After these preliminary steps, T1 executes T on the string that is
now on its tape.
If T accepts x and nothing else, then T1 accepts both y and z. T1
does not accept x, because when T1 receives input x it simulates T on

418
Solutions to Selected Exercises
input y, and y is not one of the strings T accepts; T1 also does not accept
any string other than x, y, and z. On the other hand, if T1 accepts y and z
and no other strings, then T accepts x. It doesn’t accept any other string,
because for every w ̸= x, some input other than y or z would cause T1
to process w by running T on it.
Suppose T1 accepts y and z and nothing else. Because T1 processes
both y and z by executing T on the string x, T must accept x. T does not
accept y, because T1 processes input x by executing T on the string y,
and T1 does not accept input x. T does not accept z, because T1 processes
the input string that is the successor of z in canonical order by executing
T on z, and T1 does not accept this input string. And ﬁnally, T does not
accept any string other than x, y, or z, because for every such string, T1
executes T on it, as a result of receiving an input string—maybe the same
one, maybe a different one—that is also a string other than x, y, or z,
and T1 does not accept that input string.
It follows that this construction is a reduction of P{x} to P{y,z}.
9.20. The proof of Theorem 9.9 shows that the problem Accepts can be reduced
to the problem AcceptsEverything; showing the corresponding result for
the languages of yes-instances is straightforward. Furthermore, a function
that deﬁnes a reduction from Acc to AE also deﬁnes a reduction from
Acc′ to AE′.
It is easy to see that Acc is recursively enumerable. If Acc′ were
also recursively enumerable, then Acc would be recursive, which would
mean that Accepts would be decidable. Therefore, Acc′ is not recursively
enumerable, and it follows from Exercise 9.3 that AE′ is not either.
For part (d), let T0 be a TM that immediately accepts every input. To
complete the deﬁnition of f , we must say only what f (x) is for a string
x not of the form e(T )e(z). Because such a string is an element of Acc′,
we want f (x) to be an element of AE, and we deﬁne f (x) to be e(T0)
in this case.
Now if x = e(T )e(z), the string f (x) is e(ST,z). In the case where
T accepts z, say in n moves, the computation performed by ST,z on an
input string of length at least n causes it to enter an inﬁnite loop, because
by the time it ﬁnishes simulating n moves of T on input z it will discover
that T accepts z. Therefore, in this case, the TM ST,z does not accept
every string, and f (x) /∈AE. In the other case, where T does not accept
z, ST,z does accept every input, because no matter how long its input is,
simulating that number of moves of T on input z will not cause T to
accept. It follows that f deﬁnes a reduction from Acc′ to AE.
Finally, if AE were recursively enumerable, part (d) and Exercise 9.3
would imply that Acc′ would be recursively enumerable, but we know it
is not.
9.23. We will show that there are TMs T1 and T2 accepting the same language
such that T1 accepts the string e(T1) and T2 does not accept the string
e(T2).

Solutions to Selected Exercises
419
We recall that for a TM T having k moves, the string e(T ) that
encodes T is the concatenation of k shorter strings, each of which encodes
one of the moves of T . We know that there is an algorithm for determining
whether a string is of the form e(T ); and it is easy to tell for a string e(T )
how many moves are encoded in the string e(T ). Let L be the language
of all strings e(T ) that describe a Turing machine T and have an even
number of encoded moves. Then there is a TM T1 that accepts L. We can
construct another TM T2 that accepts the same language and is identical
to T1 except that it has one additional state, no moves to that state, and
one move from that state. Then one of the two TMs has an even number
of moves and the other an odd number; therefore, one accepts its own
encoding and the other doesn’t accept its own.
9.29. Suppose (α1, β1), (α2, β2), . . . , (αn, βn) is an instance of PCP in which
the αi’s and βi’s are all strings over the alphabet {a}, and let di = |αi| −
|βi|. If di = 0 for some i, the instance is a yes-instance. If di > 0 for every
i, or if di < 0 for every i, then the instance is a no-instance. And ﬁnally,
if di = p > 0 and dj = q < 0 for some i and j, then it is a yes-instance,
because αq
i αp
j = βq
i βp
j .
9.33. The problem is decidable, and the following is a decision algorithm.
First test x for membership in L(G). If x /∈L(G), then L(G) ̸= {x}.
If x ∈L(G), then L(G) = {x} if and only if L(G) ∩(∗−{x}) = ∅. We
can test this last condition as follows. Construct a PDA M accepting
L(G), using the techniques described in Section 5.3. Using the algorithm
described in the proof of Theorem 6.13, construct a PDA M1 accepting
L(M) ∩(∗−{x}), which is the intersection of L(M) and a regular lan-
guage. Construct a CFG G1 generating the language L(M1), using the
algorithms described in the proofs of Theorems 5.28 and 5.29. Finally,
use the algorithm in Section 6.3 to determine whether L(G1) = ∅.
CHAPTER 10
10.2. Suppose b is computable, and suppose Tb is a Turing machine that
computes it. Without loss of generality we can assume that Tb has tape
alphabet {0, 1}. Let T = TbT1, where T1 is a TM, also having tape alpha-
bet {0, 1}, that moves its tape head to the ﬁrst square to the right of its
starting position in which there is either a 0 or a blank, writes a 1 there,
and halts. Let m be the number of states of T . By deﬁnition of b, no
TM with m states and tape alphabet {0, 1} can end up with more than
b(m) 1’s on the tape if it halts on input 1m. However, T is a TM of this
type that halts with output 1b(m)+1. This is a contradiction.
10.6. For every n, we can construct a TM Tn that halts in conﬁguration ha1n
when it starts with a blank tape. Tn requires no tape symbols other than
1, and we can construct it so that for some number k independent of n,
Tn has k + n states. Now suppose f is a computable function, and let Tf

420
Solutions to Selected Exercises
be a TM with m states and tape alphabet {0, 1} that computes f . Let T ′
n
be the composite TM TnTf , so that starting with a blank tape, T ′
n halts
with output 1f (n). The number of states of T ′
n is k′ + n, for some constant
k′, and it follows from the deﬁnition of bb that f (n) ≤bb(k′ + n).
Suppose for the sake of contradiction that bb is computable. Then so
is the function c deﬁned by c(n) = bb(2n). Therefore, bb(2n) ≤bb(k′ +
n) for every n. But this is impossible, because for sufﬁciently large n,
there are TMs with 2n states and tape alphabet {0, 1} which, when started
with a blank tape, can write more 1’s before halting than any TM having
only n + k′ states and tape alphabet {0, 1}.
10.15(c). We let f0, . . . , f4 be the primitive recursive derivation of Add described
in Example 10.5. Let f5 be the constant (or function of zero variables)
1. Let f6 = p2
2. Let f7 be the function obtained from Add (i.e., f4), f6,
and f6 using composition. Then g(n) = 2n is obtained from f5 and f7
by primitive recursion, because of the formulas
g(0) = 1
g(k + 1) = 2k + 2k = Add(p2
2(k, g(k)), p2
2(k, g(k))) = f5(f7(k, g(k)))
10.22. The result follows from the formula
HighestPrime(k) = max{y ≤k | Exponent(y, k) > 0}
together with the next exercise.
10.23(a). mP (X, 0) = 0, and
mP (X, k + 1) =
 k + 1
if P (X, k + 1)
mP (X, k)
if ¬P (X, k + 1)
10.23(b).
mP(X, k) = min {y ≤k | for every z with y < z ≤k, ¬P (X, z)}
= min {y ≤k | for every z ≤k, z ≤y ∨¬P (X, z)}
10.26(b). The last digit in the decimal representation of a natural number n is
Mod(n, 10); it is therefore sufﬁcient to show that the function g is prim-
itive recursive, where g(0) = 1, g(1) = 14, g(2) = 141, and so forth.
g(i) is the largest integer less than or equal to the real number 10i√
2.
Equivalently, g(i) is the largest integer whose square does not exceed
(10i√
2)2 = 2 ∗102i. This means that g(i) + 1 is the smallest integer
whose square is greater than 2 ∗102i. Therefore, if we apply the min-
imization operator to the primitive recursive predicate P deﬁned by
P (x, y) = (y2 > 2 ∗102x), g(x) is obtained from subtracting 1 from
the result. The only remaining problem is to show how a bounded min-
imization can be used in this last step. We can do this by specifying a
value of y for which y2 is certain to be greater than 2 ∗102x. y = 2 ∗10x
is such a value, and so
f (x) = mP (x, 2 ∗10x)
.−1

Solutions to Selected Exercises
421
10.32. If there were a solution to this problem, then there would be a solution to
the problem: Given a TM computing some partial function, does it accept
every input? We know that the (apparently) more general problem: Given
an arbitrary TM, does it accept every input? is undecidable. It follows
from this, however, that the more restricted problem is undecidable,
because for every TM T there is another TM T ′ that computes a partial
function and accepts exactly the same strings as T . (T ′ can be constructed
as follows: First modify T such that instead of writing  it writes a
different symbol, say ′, and it makes the same moves on ′ that it
makes on ; then let T ′ = T T1, where T1 erases the tape and halts with
the tape head on square 0.) T ′ accepts input x if and only if T does, and
T ′ computes the partial function whose only value is . Therefore, the
given problem is undecidable.
10.33. If we let g(x, y) = |f (y) −x|, then f −1(x) = Mg(x) = μy[g(x, y) =
0].
CHAPTER 11
11.6. One approach is to have the TM copy the input onto a second tape and
return the tape heads to the beginning; then to have the ﬁrst tape head
move two squares to the right for every square moved by the second (and
to reject if the input length is odd), so that the second tape head is now at
the beginning of the second half of the string; to move the ﬁrst tape head
back to the beginning; and then to compare the ﬁrst half of tape 1 to the
second half of tape 2, one symbol at a time.
11.7. Suppose T is a k-tape TM with time complexity f (n). We present an
argument that is correct provided f (n) ≥n; this will be sufﬁcient as long
as T actually reads its input string.
We also simplify the argument slightly by assuming that k = 2, though
it is easy to adapt it to the general case. If T1 is the one-tape TM that
simulates T in the way described in Chapter 7, the ﬁrst portion of T1’s
operation is to create the “two-track” conﬁguration in which the input string
is on the ﬁrst track. This can be done by starting at the left end of the
tape, inserting  between each pair of consecutive input symbols, and then
inserting the # marker at the end. Inserting the ﬁrst  and returning the tape
head takes approximately 2n moves, and each subsequent insertion takes
less time than the previous one. The time to prepare the tape is therefore
O(n2), which by our assumption on f is O(f (n)2).
At any subsequent stage of the simulation of T , the total number of
tape squares to the left of the # marker is no more than 2f (n), because in
f (n) moves T cannot move past square f (n) on any of its tapes.
Simulating a single move of T requires three iterations of the following:
move the tape head to the current position on one of the tracks, make no
more than some ﬁxed number of moves, and move the tape head back to the

422
Solutions to Selected Exercises
beginning. Each of these three iterations can be done in time proportional
to the number of squares to the left of the #, which means time proportional
to f (n). Therefore, the total simulation, in which there are no more than
f (n) moves, can be accomplished in time O(f (n)2). Finally, eliminating
the second track of the tape and preparing the ﬁnal output can also be done
in time O(f (n)2), because the length of the output string is no more than
f (n).
11.19. The language L is in NP; a nondeterministic procedure to accept L is to
take the input string, and provided it is of the form e(T )e(x)1n, choose a
sequence of n moves of T on input x, accepting the input if the sequence
causes T to accept x.
To show that L is NP-hard, let L1 be a language in NP, and let T
be an NTM accepting L1 with nondeterministic time complexity bounded
by a polynomial p. Consider the function f : ∗→{0, 1}∗deﬁned by
f (x) = e(T )e(x)1p(|x|); for every string x, x ∈L1 if and only if f (x) ∈L,
and it is easy to check that f is polynomial-time computable.
11.20. Consider the following procedure for coloring the vertices in each “con-
nected component” of the graph. Choose one and color it white. Color all
the vertices adjacent to it black. For each of those, color all the vertices
white that are adjacent to that one and have not yet been colored. Continue
this process until there are no colored vertices with uncolored adjacent ver-
tices, and repeat this procedure for each component. This can be carried out
in polynomial time, and it can be determined in polynomial time whether
the resulting assignment of colors is in fact a 2-coloring of the graph. Fur-
thermore, the graph can be 2-colored if and only if this procedure can be
carried out so as to produce a 2-coloring.
11.24. If xn−1 ≡n 1 and n is not prime, then according to the result of Fermat
mentioned in Example 11.9, xm ≡n 1 for some m < n −1. The smallest
such m must be a divisor of n −1. The reason is that when we divide
n −1 by m, we get a quotient q and a remainder r, so that
n −1 = q ∗m + r and 0 ≤r < m
This means that xn−1 = xqm+r = (xm)q ∗xr, and because xn−1 and xm are
both congruent to 1 mod n, we must have (xm)q ≡n 1 and therefore xr ≡n 1.
It follows that r must be 0, because r < m and by deﬁnition m is the
smallest positive integer with xm ≡n 1. Therefore, n −1 is divisible by m.
Every proper divisor m of n −1 is of the form (n −1)/j for some
j > 1 that is a product of (one or more) prime factors of n −1. Therefore,
some multiple of m, say a ∗m, is (n −1)/p for a single prime p. Because
xa∗m = (xm)a ≡n 1, we may conclude that x(n−1)/p ≡n 1.
11.27. The least obvious of the three statements is that P is closed under the
Kleene ∗operation. Suppose L ∈P . We describe in general terms an algo-
rithm for accepting L∗, and it is not hard to see that it can be executed
in polynomial time on a TM. (There are more efﬁcient algorithms, whose
runtimes would be lower-degree polynomials.)

Solutions to Selected Exercises
423
For a string x = a1a2 . . . an of length n, and integers i and j with
1 ≤i ≤j ≤n + 1, denote by x[i, j] the substring aiai+1 . . . aj−1 of length
j −i. There are (n + 1)(n + 2)/2 ways of choosing i and j, although the
substrings x[i, j] are not all distinct. In the algorithm, we keep a table in
which every x[i, j] is marked with 1 if it has been found to be in L∗and 0
otherwise. We initialize the table by marking x[i, j] with 1 if it is in L and
0 if not. Because L ∈P , this initialization can be done in polynomial time.
Now we begin a sequence of iterations, in each of which we examine every
pair x[i, j], x[j, k] with 0 ≤i ≤j ≤k ≤n + 1; if x[i, j] and x[j, k] are
both marked with 1, then their concatenation x[i, k] is marked with 1. This
continues until either we have performed n iterations or we have executed
an iteration in which the table did not change. The string x is in L∗if and
only if x = x[1, n + 1] is marked with 1 at the termination of the algorithm.
11.31. Let G = (V, E) be a graph and k an integer. We construct an instance
(S1, S2, . . . , Sm) of the exact cover problem in terms of G and k.
The elements belonging to the subsets S1, . . . , Sm will be of two
types: elements of V and pairs of the form (e, i) with e ∈E and 1 ≤i ≤k.
Speciﬁcally, for each v ∈V and each i with 1 ≤i ≤k, let
Sv,i = {v} ∪{(e, i) | v is an end point of e}
In addition, for each pair (e, i), let Te,i = {(e, i)}. Then the union of all the
sets Sv,i and Te,i contains all the vertices of the graph, as well as all the
possible pairs (e, i).
Suppose there is a k-coloring of G in which the colors are 1, 2, . . . , k,
and that each vertex v is colored with the color iv. Then we can construct
an exact cover for our collection of sets as follows. We include every set
Sv,iv. The union of these contains all the vertices. We also include all the
Te,i’s for which the pair (e, i) has not already been included in some Sv,iv.
These sets form a cover—that is, their union contains all the elements in
the original union. To see that it is an exact cover, it is sufﬁcient to check
that no two sets Sv,iv and Sw,iw can intersect if v ̸= w. This is true if v and
w are not adjacent; if they are, it follows from the fact that the colors iv
and iw are different.
Suppose on the other hand that there is an exact cover for the collection
of Sv,i’s and Te,i’s. Then for every vertex v, v can appear in only one set
in the exact cover, and so there can be only one i, say iv, for which Sv,i
is included. Now we can check that if we color each vertex with the color
iv, we get a k-coloring of G. If not, then some edge e joins two vertices
v and w with iv = iw = i. This means, however, that both Sv,iv and Sw,iw
contain the pair (e, i), and this is impossible if the cover is exact.

This page intentionally left blank 

425
S E L E C T E D B I B L I O G R A P H Y
Agrawal M, Kayal N, Saxena N: PRIMES Is in P,
Annals of Mathematics 160(2): 781–793, 2004.
Bar-Hillel Y, Perles M, Shamir E: On Formal Properties
of Simple Phrase Structure Grammars, Zeitschrift
f¨ur Phonetik Sprachwissenschaft und
Kommunikations-forschung 14: 143–172, 1961.
Bovet DP, Crescenzi P: Introduction to the Theory of
Complexity. Englewood Cliffs, NJ: Prentice Hall,
1994.
Brown D, Levine JR, Mason T: lex & yacc, 2nd ed.
Sebastopol, CA: O’Reilly Media, Inc., 1995.
Cantor G: Contributions to the Founding of the Theory
of Transﬁnite Numbers. Mineola, NY: Dover, 1955.
Chomsky N: Three Models for the Description of
Language, IRE Transactions on Information Theory
2: 113–124, 1956.
Chomsky N: On Certain Formal Properties of Grammars,
Information and Control 2: 137–167, 1959.
Chomsky N: Context-Free Grammars and Pushdown
Storage, Quarterly Progress Report No. 65,
Cambridge, MA: Massachusetts Institute of
Technology Research Laboratory of Electronics,
1962, pp. 187–194.
Church A: An Unsolvable Problem of Elementary
Number Theory, American Journal of Mathematics
58: 345–363, 1936.
Cobham A: The Intrinsic Computational Difﬁculty of
Functions, Proceedings of the 1964 Congress for
Logic, Mathematics, and Philosophy of Science,
New York: North Holland, 1964, pp. 24–30.
Cook SA: The Complexity of Theorem Proving
Procedures, Proceedings of the Third Annual ACM
Symposium on the Theory of Computing, New York:
Association for Computing Machinery, 1971,
pp. 151–158.
Davis MD: Computability and Unsolvability. New York:
McGraw-Hill, 1958.
Davis, MD: The Undecidable. Hewlett, NY: Raven
Press, 1965.
Davis, MD, Sigal R, Weyuker EJ: Computability,
Complexity, and Languages: Fundamentals of
Theoretical Computer Science, 2nd ed. New York:
Academic Press, 1994.
Dowling WF: There Are No Safe Virus Tests, American
Mathematical Monthly 96: 835–836, 1989.
Earley J: An Efﬁcient Context-Free Parsing Algorithm,
Communications of the ACM 13(2): 94–102,
1970.
Floyd RW, Beigel R: The Language of Machines: An
Introduction to Computability and Formal
Languages. New York: Freeman, 1994.
Fortnow L: The Status of the P versus NP Problem,
Communications of the ACM 52(9): 78–86, 2009.
Garey MR, Johnson DS: Computers and Intractability: A
Guide to the Theory of NP-Completeness. New
York: Freeman, 1979.
Hartmanis J, Stearns RE: On the Computational
Complexity of Algorithms, Transactions of the
American Mathematical Society 117: 285–306,
1965.
Hopcroft JE., Motwani R, Ullman J: Introduction to
Automata Theory, Languages, and Computation,
2nd ed. Reading, MA: Addison-Wesley,
1979.
Immerman N: Nondeterministic Space Is Closed Under
Complementation, SIAM Journal of Computing 17:
935–938, 1988.
Karp RM: Reducibility Among Combinatorial Problems.
In Complexity of Computer Computations. New
York: Plenum Press, 1972, pp. 85–104.
Kleene SC: Introduction to Metamathematics. New
York: Van Nostrand, 1952.
Kleene SC: Representation of Events in Nerve Nets and
Finite Automata. In Shannon CE, McCarthy J

426
Selected Bibliography
(eds.), Automata Studies. Princeton, NJ: Princeton
University Press, 1956, pp. 3–42.
Knuth, DE: On the Translation of Languages from Left
to Right, Information and Control 8: 607–639,
1965.
Levin L: Universal Search Problems (in Russian),
Problemy Peredachi Informatsii 9(3): 265–266,
1973.
Lewis HR., Papadimitriou C: Elements of the Theory of
Computation, 2nd ed. Englewood Cliffs, NJ:
Prentice Hall, 1998.
Lewis PM II, Stearns RE: Syntax-Directed Transduction,
Journal of the ACM 15: 465–488, 1968.
McCulloch WS, Pitts W: A Logical Calculus of the
Ideas Immanent in Nervous Activity, Bulletin
of Mathematical Biophysics 5: 115–133, 1943.
Moore EF: Gedanken Experiments on Sequential
Machines. In Shannon CE, and McCarthy J (eds.),
Automata Studies. Princeton, NJ: Princeton
University Press, 1956, pp. 129–153.
Myhill J: Finite Automata and the Representation of
Events, WADD TR-57-624, Wright Patterson Air
Force Base, OH, 1957, pp. 112–137.
Nerode A: Linear Automaton Transformations,
Proceedings of the American Mathematical Society
9: 541–544, 1958.
Oettinger AG: Automatic Syntactic Analysis and the
Pushdown Store, Proceedings of the Symposia in
Applied Mathematics 12, Providence, RI: American
Mathematical Society 9, 1961 pp. 104–109.
Ogden O: A Helpful Result for Proving Inherent
Ambiguity, Mathematical Systems Theory 2:
191–194, 1968.
Papadimitriou CH: Computational Complexity. Reading,
MA: Addison-Wesley, 1994.
Post EL: A Variant of a Recursively Unsolvable
Problem, Bulletin of the American Mathematical
Society 52: 246–268, 1946.
Rabin MO: Real-Time Computation, Israel Journal of
Mathematics 1: 203–211, 1963.
Rabin MO, Scott D: Finite Automata and Their Decision
Problems, IBM Journal of Research and
Development 3: 115–125, 1959.
Rosenkrantz DJ, Stearns RE: Properties of Deterministic
Top-Down Grammars, Information and Control 17:
226–256, 1970.
Salomaa A: Jewels of Formal Language Theory.
Rockville, MD: Computer Science Press, 1981.
Sch¨utzenberger MP: On Context-Free Languages and
Pushdown Automata, Information and Control 6:
246–264, 1963.
Sipser M: Introduction to the Theory of Computation,
2nd ed. Florence, KY: Course Technology, 2005.
Szelepcs´eny R: The Method of Forcing for
Nondeterministic Automata, Bulletin of the EATCS
33: 96–100, 1987.
Turing AM: On Computable Numbers with an
Application to the Entscheidungsproblem,
Proceedings of the London Mathematical Society 2:
230–265, 1936.
van Leeuwen J (ed.): Handbook of Theoretical Computer
Science (Volume A, Algorithms and Complexity).
Amsterdam: MIT Press/Elsevier, 1990.
Younger DH: Recognition and Parsing of Context-Free
Languages in Time n3, Information and Control
10(2): 189–208, 1967.

427
I N D E X O F N O T A T I O N
∧,
2, 11
∨,
2, 11
¬,
2
→,
2
↔,
2
∀,
4
∃,
4
⊆,
9
,
9, 18
A ∪B,
10
A ∩B,
10
A′,
10
A–B,
10
∪,
11
∩,
11
A × B,
12
f : A →B,
13
∈,
13
f −1,
13
f (x),
13
x ≡n y,
16
x Ry,
16
,
18
nσ (x),
18
Exponential notation,
20
∗,
18, 234
L∗,
20
ℵ,
22
•,
22
◦,
22
♦,
22
□,
25
δ∗,
53
IL,
53
Lq,
73
SM,
74
i,
89
sh(r),
119
G,
128
S,
131, 132
S + S,
131
G,
134
α ⇒nβ,
134
α ⇒∗β,
134
α ⇒G β,
134
α ⇒n β,
134
α ⇒nG β,
134
α ⇒∗G β,
134
AEqB,
135
S,
142
$,
193
G1,
194
,
227
xqy,
228
xq,
228
xσy,
228
xy,
228
aabqa a,
228
q0x,
229
(∗)k,
234
T1T2,
238
g ◦f ,
239
P (L),
250
f : A →B,
250
NSA,
300
SA,
300
E′,
300
P ,
303
P ′,
303
F,
305
— ,
335
χP ,
335
EH,
338
ESq,
338
μ,
341
PrNo,
341–342
P ,
358
NP,
358
P ,
363
NP,
364
3-Sat,
379

428
I N D E X
−closure of set of states, 101
(S), 102
−transition, 99
A
A-derivable variable, 151
Acceptance
DPDA, 172–176
empty stack, 184
FA, 45–50, 54
language, 265
language acceptor, 45, 46
language of palindromes, 243
NFA, 97–100
PDA, 167–170
TM, 229–234
Accepting conﬁguration, 167
Add, 334
Agrawal, M., 425
Algebraic-expression grammar, 131–132, 143
Algorithm, 248
Algorithm procedure, 247
Alphabet, 18
Ambiguity, 143, 148
AnBn, 130–131, 164, 165
Ancestor, 297
Answers to selected exercises, 389–423
Arithmetic progression, 83
Associative law for unions, 11
Associative laws, 4
Automaton
counter, 200
ﬁnite. See Finite automaton (FA)
LBA, 278–279
NFA. See NFA
PDA. See Pushdown automaton (PDA)
two-stack, 263–264
B
Balanced, 30
Balanced
acceptance by DPDA, 172–174
balanced string of parentheses, 30–32
deﬁned, 19
language, as, 179
recursive deﬁnition, 24
smallest language, 25
top-down parser, 192
Balanced strings of parentheses, 23–25, 30–32
Bar-Hillel, Y., 425
Basis statement, 28
Beigel, R., 425
Bibliography, 425–426
Biconditional, 2
Big-Oh notation, 361
Bijection, 13, 14, 284
Binary operation, 14
Boolean array, 289
Bottom-up parser (SimpleExpr), 194–196
Bottom-up PDA, 180, 181
Bounded existential quantiﬁcation, 339
Bounded minimization, 340–341
Bounded products, 339
Bounded quantiﬁcations, 339
Bounded sums, 339
Bounded universal quantiﬁcation, 339
Bovet, D. P., 425
Brown, D., 425
Building computer with equivalence classes, 68–73
Busy-beaver function, 353
By ﬁnal state, 167
C
Canonical order, 18, 292
Cartesian product, 12
CFG, 134, 271
CFG corresponding to a regular expression, 138–139
CFGGenerateAll, 325–326
CFGNonemptyIntersection, 322, 325
CFL. See Context-free language (CFL)
Characteristic function, 237, 335
Chomsky, N., 425
Chomsky hierarchy, 281, 282
Chomsky normal form, 149, 152
Church, Alonzo, 247, 425
Church-Turing thesis, 247, 352
Church’s thesis, 247
Closed under the operation, 14

Index
429
CNF, 365
CNF-Tautology, 385
Cobham, A., 425
Codomain, 13
Combining Turing machines, 238–243
Commutative laws, 4
Comparing two strings, 243
Complement
CFL, 214–218
deﬁnition, 10
notation, 10
Complementary problem, 303
Complete subgraph, 370
Complete subgraph problem, 370–371, 378
Composite natural numbers, 249
Composites and primes, 366–367
Composition, 332
Compound propositions, 2
Compound statement, 134
Computable functions, 331–357
G¨odel numbering, 344–348
minimization, 340–343
μ-recursion, 348–352
μ-recursive function, 343–344
other approaches to computability, 352–353
primitive recursive functions, 331–338
quantiﬁcation, 338–340
Computation, 225
Computation tree, 98, 100, 171
Computational complexity, 358–387
Cook-Levin theorem, 373–378
NP-complete problems, 378
NP-completeness, 369–370
polynomial-time reductions, 369–372
polynomial veriﬁability, 363–369
set NP, 364
set P, 363
time complexity of TM, 359
Computational Complexity (Papadimitriou), 379
Concatenation, 19
Conditional, 2
Conﬁguration number, 347–348
Conjunction, 2
Conjunctive normal form (CNF), 365
Connectives, 2
Constant function, 331
Context-free grammar (CFG), 134, 271
Context-free language (CFL), 130–163
complement, 214–218
context-free grammar, 134–138
decision problems, 218–220
derivation trees and ambiguity, 141–149
grammar rules, 130–134
intersection, 214–218
pumping lemma, 205–214
regular language/regular grammar, 138–142
simpliﬁed forms/normal forms, 149–154
undecidable problems, 321–326
Context-sensitive grammar (CSG), 277
Context-sensitive language (CSL), 277
Contradiction, 3
Contrapositive, 4
Converse, 4
Converting an NFA with -transitions to an FA, 109
Converting CFG to Chomsky normal form, 153
Cook, Stephen, 373, 425
Cook-Levin theorem, 373–378
Copying a string, 241
Corescenzi, P., 425
Correspondence systems, 315
Countable set, 285
Countable union of countable sets, 287
Countably inﬁnite set, 285
Counter automaton, 200
Counting the elements, 284
Course-of-values recursion, 346
CSG, 277
CSL, 277
CSLIsEmpty, 330
D
δ∗, 53
Dangling else, 144, 145
Davis, M. D., 425
DCFL, 172
De Morgan laws, 4, 11
Decidable problems, 303
Deciding a language, 265
Decision problems, 66
CFL, 218–220
complementary problem, 303
decidability, 358
general form, 303
languages accepted by FAs, 66–67
reducing one to another, 305
TM, 308–314
undecidable. See Undecidable decision problems
yes-instances/no-instances, 302
Deﬁnition
acceptance by empty stack, 184
acceptance by FA, 54
acceptance by PDA, 167
acceptance by TM, 229
accepting a language, 265
ambiguity, 143
Big-Oh notation, 361
bounded minimization, 340–341
bounded quantiﬁcations, 339

430
Index
Deﬁnition—Cont.
Chomsky normal form, 152
composition, 332
context-free grammar, 134
countable set, 285
countably inﬁnite set, 285
CSG, 277
CSL, 277
decidable problems, 303
DPDA, 172
encoding function, 254
equivalence class containing x, 16
equivalence relation, 15
extended transition function (δ∗), 53
extended transition function for NFA, 102
ﬁnite automaton, 52
G¨odel number of sequence of natural numbers, 344
initial function, 331–332
−closure of set of states, 101
L-indistinguishability relation, 68
language generated by CFG, 135
language property of TM, 312
LBA, 278–279
LMD, 142
mate of left parentheses in balanced string, 147
μ-recursive function, 343
NB(G), 181
NFA, 100–101
NP-complete language, 372
NP-hard language, 372
NSA, 300
NT(G), 177
one-to-one function, 13
onto function, 13
PCP, 314
PDA, 166
polynomial-time reduction, 369
primitive recursion, 332–333
primitive recursive function, 333
recursive, 21–26
recursive deﬁnition of set of nullable variables, 150
reducing one decision problem to another, 305
reducing one language to another, 305
regular grammar, 140
regular languages over alphabet , 93
relation, 15
RMD, 142
SM, 74
SA, 300
set A of same size as B or larger than B, 284
set NP, 364
set P, 363
strings distinguishable with respect to L, 58
time complexity of NTM, 364
time complexity of TM, 359
TM computing a function, 235
TM enumerating a language, 269
Turing machine, 227
unbounded minimization, 342
universal TM, 253
unrestricted grammar, 271
valid computations of TM, 323
veriﬁer for language, 368
Deleting a symbol, 242
Derivation
LMD, 142
primitive recursive, 334
RMD, 142
strings, 141
Derivation tree, 141–148
Deterministic context-free language (DCFL), 172
Deterministic PDA (DPDA), 172–176
Diagonal argument, 288, 289
Difference, 10
Direct proof, 6
Disjoint set, 10
Disjunction negation, 2
Distributive laws, 4
Div, 337
DNF-Sat, 385
Domain, 13
Double-duty (L), 222
Doubly inﬁnite tape, 260
Dowling, W. F., 425
DPDA, 172–176
E
E′, 302
Earley, J., 425
egrep, 96
Eliminating -transitions from an NFA, 107, 108
else, 144
Empty set, 9
Empty stack, 184
Encoding function, 254
End-marker, 193
Enumerating a language, 268–271
Equality relation, 16
Equivalence class containing x, 16–17
Equivalence classes of IL, 71–72
Equivalence relation, 15–17
Erasing the tape, 242–243
Eventually linear, 223
Eventually periodic function, 83, 354
Exact cover problem, 387
Exercises, solutions, 389–423
Existential quantiﬁer, 4
Exponential notation, 20

Index
431
Expr, 19, 23, 24, 27, 131–132
Expression graph, 128
Extended transition function (δ∗), 53
F
FA. See Finite automaton (FA)
Factorial function, 332
Fermat’s last theorem, 326
Finding a regular expression corresponding to an
FA, 116
Finite automaton (FA), 45–91
acceptance/rejection, 46
accepting {aa, aab}*{b}, 59–61
accepting strings containing ab or bba, 57
accepting strings with a in nth symbol from
end, 61–62
building computer with equivalence classes, 68–73
converting NFA to, 109–110
decision problems, 67
deﬁned, 52
difference of two languages, 56
distinguishing one string from another, 58–59
language acceptor, as, 46–50
lexical analysis, 50–52
minimization algorithm, 73–77
model of computation, as, 224
NFA, 96–104. See also NFA
pumping lemma, 63–67
regular expression, 116–117
string search algorithm, 48–49
union, 55–56
Finite set, 8
Finite-state machine, 46
First De Morgan law, 11
Floyd, R. W., 425
for statement, 134
Formal deﬁnition. See Deﬁnition
Function
busy-beaver, 353
characteristic, 237–238, 335
computable. See Computable functions
constant, 331
deﬁned, 12
encoding, 254
eventually periodic, 83
factorial, 332
initial, 331–332
μ-recursive, 343
one-to-one, 13
onto, 13
primitive recursive, 331–338
projection, 332
relabeling, 89
relationships, 15
sets deﬁned recursively, 32–34
successor, 331
transition, 46
Functions and equivalence relations, 12–17
G
Garey, M. R., 425
Generalized regular expression, 120
Glossary. See Deﬁnition
G¨odel numbering, 344–348
Goldbach’s conjecture, 308
Grammar
algebraic-expression, 131–132, 143
CFG, 134, 271
Chomsky hierarchy, 281, 282
converting CGF to Chomsky normal form, 153
CSG, 277
left-regular, 158
linear, 141
LL(1), 194
LL(k), 194
regular, 140
right-regular, 158
type 0, 281
unrestricted, 271
weak precedence, 196
Grammar rules, 130–134
grep, 96
H
Halting problem, 306, 308
Halts, 338, 339
Hartmanis, J., 425
Homomorphism, 127, 163
Hopcroft, J. E., 425
Human computer, 225, 226
I
Identiﬁer (C programming language), 95
if statement, 134, 144
Ignore the states, 186
Immerman, N., 425
Incompleteness theorem, 344
Independent set of vertices, 386
Indirect proof, 6
Induction hypothesis, 28
Induction step, 28
Inﬁnite set, 285
InitConﬁg(n), 348
Initial function, 331–332
Inserting/deleting a symbol, 242
Intersection, 10
IsAcceptingT , 349
IsAmbiguous, 322, 325

432
Index
IsConﬁgT , 349
Isomorphic to M2, 90
Isomorphism from M1 to M2, 90
J
Johnson, D. S., 425
K
k-colorability problem, 380, 381–383
k-coloring, 380
Karp, R. M., 425
Kayal, N., 425
Kleene, S. C., 425
Kleene closure, 20
Kleene star, 20
Kleene’s theorem
Part I, 110–114
Part II, 114–117
Knuth, D. E., 425
L
L-distinguishable, 58
−closure of set of states, 101
(S), 102
−transition, 99
Language, 17–21
accepting, 265
CFG, 135
Chomsky hierarchy, 282
concatenation, 19
context-free. See Context-free language (CFL)
countable set, as, 287
CSL, 277
DCFL, 172
deciding, 265
enumerating, 268–271
grammar rules, 130–134
large alphabets, 18–19
NP-complete, 372
NP-hard, 372
NSA, 300
over, 18
programming language, and, 66
pumping lemma/accepted by FA, 73
recursive, 265, 266
recursively enumerable. See Recursively enumerable
language
reducing one to another, 305
regular, 92, 93
SA, 300
veriﬁer, 368
Language acceptor, 45, 46–50
Language over , 18
Language property, 312
LBA, 278–279
Left recursion, 202
Left-regular grammar, 158
Leftmost derivation (LMD), 142
Legal C programs, 210–211
Levin, Leonid, 373, 426
Levine, J. R., 425
Lewis, H. R., 426
Lewis, P. M., II, 426
lex, 96
Lexical analysis, 50–52, 96
Lexical analyzer, 96
Lexical-analyzer generator, 96
Lexicographic order, 18
Linear-bounded automaton (LBA), 278–279
Linear grammar, 141
Live variable, 162
LL(1) grammar, 194
LL(k) grammar, 194
LMD, 142
Logic and proofs, 1–8
Logical connectives, 2
Logical identities, 4
Logical implication, 3
Logical quantiﬁer, 4
Logically equivalent, 3
M
Many-one reduction, 305
Mason, T., 425
Match, 315
Mate, 161
Mathematical induction, 28–29
Mathematical tools and techniques, 1–44
functions and equivalence relations, 12–17
language. See Language
logic and proofs, 1–8
recursive deﬁnition, 21–26
set. See Set
structural induction, 26–34
McCulloch, W. S., 426
Membership problem, 67, 219
Membership table, 11
Meta-statement, 3
Minimization, 340–343
Minimization algorithm, 73–77
Mod, 337
Mod 2, 236–237
Modiﬁed correspondence systems, 315
Modiﬁed Post correspondence problem (MPCP), 315
Modus ponens, 34
Monus operation, 335
Moore, E. F., 426

Index
433
Motwani, R., 425
MoveT , 350
MPCP, 315
μ-recursion, 348–352
μ-recursive function, 343–344
Mult, 334
Multitape Turing machines, 243–246
Myhill, J., 426
N
n-place predicate, 335
Natural number, 10, 21
NB(G), 181
Nerode, A., 426
NewPosn, 350
NewState, 350
NewSymbol, 350
NewTapeNumber, 350
Next blank, 241
NFA, 96–104
accepting languages, 97–100
converting, to NFA, 109–110
corresponding to ((aa + b)*(aba)*bab)*, 113–114
eliminating nondeterminism, 104–110
formal deﬁnition, 100–101
model of computation, as, 224
No-instance, 302
Non-context-free language, 205. See also Context–free
language (CFL)
Non-self-accepting, 303
Nondeterminism
eliminating, from NFA, 104–110
NB(G), 181
NFA. See NFA
NT(G), 177
NTM, 248–252
Nondeterministic bottom-up PDA [NB(G)], 181
Nondeterministic ﬁnite automaton. See NFA
Nondeterministic polynomial algorithms, 358
Nondeterministic polynomial time, 364
Nondeterministic top-down PDA [NT(G)], 177
Nondeterministic Turing machine (NTM), 248–252, 364
Nonempty subset, 14
NonemptyIntersection, 329
NonPal, 132, 133
Nonpalindrome, 132
Nonterminal, 134
Nontrivial language property, 312
Normal forms, 149
NP-complete language, 372
NP-complete problems, 378
NP-completeness, 369–370
NP-hard language, 372
NSA, 300, 302
NT(G), 177
nth prime number, 341–342
NTM, 248–252, 364
Null string, 18
Nullable variables, 150
Numeric literal, 96
O
Oettinger, A. G., 426
Ogden, O., 426
Ogden’s lemma, 211–214
One-to-one function, 13
Onto function, 13
Operation on a set, 14
Ordered k-tuples, 12
Ordered pairs, 12
P
Pairwise disjoint, 10
Pairwise L-distinguishable, 58
Pal, 18, 23, 62
Palindrome, 18, 23, 132, 243
Papadimitriou, C. H., 426
Parenthesization, 25
Parser, 96
Parsing, 191–196
Partial match, 316
Partition, 10
Partition problem, 387
PCP, 314–321
PDA. See Pushdown automaton (PDA)
PerfectSquare, 338, 339
Perles, M., 425
Pitts, W., 426
Polynomial time, 358
Polynomial-time reductions, 369–372
Polynomial-time solution, 362
Polynomial-time veriﬁer, 368
Post, E. L., 426
Post machine, 262–264, 415
Post’s correspondence problem (PCP), 314–321
Power set, 12
Precedence grammar, 196
Pred, 334
Preﬁx, 19, 250
Preserved under bijection, 294
Previous blank, 241
Prime, 5, 366–367
Prime, 366–367
Primitive recursion, 332–333
Primitive recursive derivation, 334
Primitive recursive functions, 331–338
Production, 134
Program conﬁguration, 352
Programming-language syntax, 133–134

434
Index
Projection function, 332
Proof
direct, 6
indirect, 6
logic, and, 1–8
typical step, 6
Proof by cases, 8
Proof by contradiction, 7
Proof by contrapositive, 7
Proposition, 1
Pumping lemma, 63–67
{x ∈{a, b, c} ∗|na(x) < nb(x) and na(x) < nc(x)}, 210
AnBnCn, 208–209
context-free languages, 205–214
deﬁned, 63
not accepted by FA, 73
regular languages, 63–67
theorem, 64
XX, 209
Pushdown automaton (PDA), 164–204
acceptance, 167–170
bottom-up PDA, 180, 181
from CFG, 176–184
CFG from given PDA, 184
deﬁnitions/examples, 164–172
deterministic PDA, 172–176
formal deﬁnition, 166
parsing, 191–196
top-down PDA, 177
Pythagorean school, 7
Q
Quantiﬁcation, 338–340
Quantiﬁed statements, 4, 5
Quantiﬁer, 4, 5
Quotient and remainder Mod 2, 236–237
R
Rabin, M. O., 426
Reachable variable, 162
Really identical, 89
Recursive deﬁnition, 21–26
Recursive deﬁnition of SM, 74
Recursive language, 265, 266
Recursively enumerable language, 265–298
alternative name, 266
Chomsky hierarchy, 281, 282
CSG/CSL, 277
enumerating a language, 268–271
more general grammars, 271–277
recursive language, contrasted, 265, 266
theorem, 265–268
unrestricted grammar, 271
which languages are recursively enumerable, 283–290
Reducing one decision problem to another, 305
Reducing one language to another, 305
Reduction, 180, 181
Reductions and the halting problem, 304–308
References (bibliography), 425–426
Regular expression, 93, 95–96
Regular grammar, 140, 141
Regular language, 92, 93, 138
Relabeling function, 89
Relation, 15
Relation of congruence mod n on N, 16
Relation on A containing all ordered pairs, 16
ResultT , 348, 349
Reverse of a string, 235–236
Rice’s theorem, 312–314
Right-invariant, 85
Right-regular grammar, 158
Rightmost derivation (RMD), 142, 181
RMD, 142, 181
Rosenkrantz, D. J., 426
S
SM, 74
SA, 300, 302
Salomaa, A., 426
Satisﬁability problem, 361, 365–366
Satisﬁable, 367
Saxena, N., 425
Schr¨oder-Bernstein theorem, 297
Sch¨utzenberger, M. P., 426
Selected bibliography, 425–426
Selected exercises, solutions, 389–423
Self-accepting, 302
Self-embedded variable, 206, 271
Set, 8–12
countable, 285
countably inﬁnite, 285
disjoint, 10
empty, 9
ﬁnite, 8
inﬁnite, 285
natural numbers, 21–22
nonempty, 14
NP, 364
operation on, 14
P, 363
power, 12
subset, 9
Set identities, 10
Set of composite natural numbers, 249
Set of legal C programs, 210–211
Shamir, E., 425
Shift move, 181
Sigal, R., 425

Index
435
SimplePal, 164, 165
Simpliﬁed algebraic expressions, 183
Simpliﬁed norms, 149
Sipser, M., 426
Solutions to selected exercises, 389–423
Stack, 164
Stack-emptying state, 185
Star height, 119
Start variable, 134
State, 69, 104
Statement-sequence, 134
Stearns, R. E., 425, 426
String, 20
String over , 18
String search algorithm, 48–49
Strong induction, 29–30
Stronger statement, 32
Structural induction, 26–34
Sub, 334
Subset, 9
Subset construction, 106
Subset construction to eliminate nondeterminism, 108, 109
Substring, 19
Successor function, 331
Sufﬁx, 19
Sum-of-subsets problem, 387
Syntax diagram, 134
Syntax of programming languages, 133–134
Szelepcs´eny, R., 426
T
Tape head, 225
Tautology, 3
Techniques. See Mathematical tools and techniques
Terminal, 134
Terminal symbol, 134
Terminology. See Deﬁnition
3-Sat, 379
TM. See Turing machine
TM conﬁguration, 228, 347–348, 352
Token, 50
Tools and techniques. See Mathematical tools and techniques
Top-down parser (Balanced), 192
Top-down PDA, 177
Tractable problem, 362
Transition function, 46
Transition table
copying a string, 241
DPDA accepting AEzB, 174
DPDA accepting Balanced, 174
NFA with seven states, 121, 122
NT(G), 180
NT(G1), 193
PDA accepting AnBn, 168
PDA accepting Pal, 170
PDA accepting SimplePal, 169
Transitive closure, 26
Traveling salesman problem, 362
Triple, 166
Truth value, 1
Turing, Alan, 225, 253, 426
Turing-acceptable language, 266
Turing-decidable language, 266
Turing machine, 224–264
Church-Turing thesis, 247
combining, 238–243
computing a function, 235
conﬁguration, 228, 347–348
countable set, 287
current tape number, 347
decision problems, 308–314
deﬁned, 227
deleting a symbol, 242
doubly inﬁnite tape, 260
enumerating a language, 269
ﬁnite alphabet/ﬁnite set of states, 225
halt states, 227
language acceptor, as, 229–234
language property, 312
modiﬁed correspondence system, 320–321
multitape, 243–246
NTM, 248–252
partial functions, 234–238
Post machine, and, 264
simpler machines, contrasted, 226
time complexity, 359
two-stack automaton, and, 264
universal, 252–257
valid computations, 323
Turing’s thesis, 229. See also Church–Turing thesis
Two-stack automaton, 263–264
2-tape TM, 244
Type 0 grammar, 281
U
U −A, 10
Ullman, J., 425
Unary operation, 14
Unbounded minimization, 342
Undecidable decision problems, 299–330
CFL, 321–326
PCP, 314–321
reductions and the halting problem, 304–308
SA/NSA, 299–304
TM, 308–314
Undecidable problem, 220
Union, 10
Universal quantiﬁer, 4

436
Index
Universal Turing machine, 252–257
Unix, 96
Unrestricted grammar, 271, 352
Useful variable, 162
Useless, 162
V
Valid computations of TM, 323
van, Leewen, J., 426
Variable
A-derivable, 151
elements of V, 134
live, 162
nullable, 150
reachable, 162
self-embedded, 206, 271
start, 134
useful, 162
useless, 162
Variable-occurrence, 142
Veriﬁer for language, 368
Vertex cover, 380
Vertex cover problem, 380, 381
Virus tester, 296
W
Weak precedence grammar, 196
Weyuker, E. J., 425
Within parentheses, 147
X
yacc, 96
Yes-instance, 302
Younger, D. H., 426
Z
0–1 knapsack problem, 387


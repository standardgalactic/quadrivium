TLFeBOOK

AN INTRODUCTION TO
NUMERICAL ANALYSIS
FOR ELECTRICAL AND
COMPUTER ENGINEERS
Christopher J. Zarowski
University of Alberta, Canada
A JOHN WILEY & SONS, INC. PUBLICATION
TLFeBOOK

TLFeBOOK

AN INTRODUCTION TO
NUMERICAL ANALYSIS
FOR ELECTRICAL AND
COMPUTER ENGINEERS
TLFeBOOK

TLFeBOOK

AN INTRODUCTION TO
NUMERICAL ANALYSIS
FOR ELECTRICAL AND
COMPUTER ENGINEERS
Christopher J. Zarowski
University of Alberta, Canada
A JOHN WILEY & SONS, INC. PUBLICATION
TLFeBOOK

Copyright c⃝2004 by John Wiley & Sons, Inc. All rights reserved.
Published by John Wiley & Sons, Inc., Hoboken, New Jersey.
Published simultaneously in Canada.
No part of this publication may be reproduced, stored in a retrieval system, or transmitted in any form
or by any means, electronic, mechanical, photocopying, recording, scanning, or otherwise, except as
permitted under Section 107 or 108 of the 1976 United States Copyright Act, without either the prior
written permission of the Publisher, or authorization through payment of the appropriate per-copy fee
to the Copyright Clearance Center, Inc., 222 Rosewood Drive, Danvers, MA 01923, 978-750-8400,
fax 978-646-8600, or on the Web at www.copyright.com. Requests to the Publisher for permission
should be addressed to the Permissions Department, John Wiley & Sons, Inc., 111 River Street,
Hoboken, NJ 07030, (201) 748-6011, fax (201) 748-6008.
Limit of Liability/Disclaimer of Warranty: While the publisher and author have used their best efforts
in preparing this book, they make no representations or warranties with respect to the accuracy or
completeness of the contents of this book and speciﬁcally disclaim any implied warranties of
merchantability or ﬁtness for a particular purpose. No warranty may be created or extended by sales
representatives or written sales materials. The advice and strategies contained herein may not be
suitable for your situation. You should consult with a professional where appropriate. Neither the
publisher nor author shall be liable for any loss of proﬁt or any other commercial damages, including
but not limited to special, incidental, consequential, or other damages.
For general information on our other products and services, please contact our Customer Care
Department within the United States at 877-762-2974, outside the United States at 317-572-3993 or
fax 317-572-4002.
Wiley also publishes its books in a variety of electronic formats. Some content that appears in print,
however, may not be available in electronic format.
Library of Congress Cataloging-in-Publication Data:
Zarowski, Christopher J.
An introduction to numerical analysis for electrical and computer engineers / Christopher
J. Zarowski.
p. cm.
Includes bibliographical references and index.
ISBN 0-471-46737-5 (coth)
1. Electric engineering—Mathematics. 2. Computer science—Mathematics. 3. Numerical
analysis I. Title.
TK153.Z37 2004
621.3′01′518—dc22
2003063761
Printed in the United States of America.
10 9 8 7 6 5 4 3 2 1
TLFeBOOK

In memory of my mother
Lilian
and of my father
Walter
TLFeBOOK

TLFeBOOK

CONTENTS
Preface
xiii
1
Functional Analysis Ideas
1
1.1
Introduction
1
1.2
Some Sets
2
1.3
Some Special Mappings: Metrics, Norms, and Inner Products
4
1.3.1
Metrics and Metric Spaces
6
1.3.2
Norms and Normed Spaces
8
1.3.3
Inner Products and Inner Product Spaces
14
1.4
The Discrete Fourier Series (DFS)
25
Appendix 1.A
Complex Arithmetic
28
Appendix 1.B
Elementary Logic
31
References
32
Problems
33
2
Number Representations
38
2.1
Introduction
38
2.2
Fixed-Point Representations
38
2.3
Floating-Point Representations
42
2.4
Rounding Effects in Dot Product Computation
48
2.5
Machine Epsilon
53
Appendix 2.A
Review of Binary Number Codes
54
References
59
Problems
59
3
Sequences and Series
63
3.1
Introduction
63
3.2
Cauchy Sequences and Complete Spaces
63
3.3
Pointwise Convergence and Uniform Convergence
70
3.4
Fourier Series
73
3.5
Taylor Series
78
vii
TLFeBOOK

viii
CONTENTS
3.6
Asymptotic Series
97
3.7
More on the Dirichlet Kernel
103
3.8
Final Remarks
107
Appendix 3.A
COordinate Rotation DI gital Computing
(CORDIC)
107
3.A.1
Introduction
107
3.A.2
The Concept of a Discrete Basis
108
3.A.3
Rotating Vectors in the Plane
112
3.A.4
Computing Arctangents
114
3.A.5
Final Remarks
115
Appendix 3.B
Mathematical Induction
116
Appendix 3.C
Catastrophic Cancellation
117
References
119
Problems
120
4
Linear Systems of Equations
127
4.1
Introduction
127
4.2
Least-Squares Approximation and Linear Systems
127
4.3
Least-Squares Approximation and Ill-Conditioned Linear
Systems
132
4.4
Condition Numbers
135
4.5
LU Decomposition
148
4.6
Least-Squares Problems and QR Decomposition
161
4.7
Iterative Methods for Linear Systems
176
4.8
Final Remarks
186
Appendix 4.A
Hilbert Matrix Inverses
186
Appendix 4.B
SVD and Least Squares
191
References
193
Problems
194
5
Orthogonal Polynomials
207
5.1
Introduction
207
5.2
General Properties of Orthogonal Polynomials
207
5.3
Chebyshev Polynomials
218
5.4
Hermite Polynomials
225
5.5
Legendre Polynomials
229
5.6
An Example of Orthogonal Polynomial Least-Squares
Approximation
235
5.7
Uniform Approximation
238
TLFeBOOK

CONTENTS
ix
References
241
Problems
241
6
Interpolation
251
6.1
Introduction
251
6.2
Lagrange Interpolation
252
6.3
Newton Interpolation
257
6.4
Hermite Interpolation
266
6.5
Spline Interpolation
269
References
284
Problems
285
7
Nonlinear Systems of Equations
290
7.1
Introduction
290
7.2
Bisection Method
292
7.3
Fixed-Point Method
296
7.4
Newton–Raphson Method
305
7.4.1
The Method
305
7.4.2
Rate of Convergence Analysis
309
7.4.3
Breakdown Phenomena
311
7.5
Systems of Nonlinear Equations
312
7.5.1
Fixed-Point Method
312
7.5.2
Newton–Raphson Method
318
7.6
Chaotic Phenomena and a Cryptography Application
323
References
332
Problems
333
8
Unconstrained Optimization
341
8.1
Introduction
341
8.2
Problem Statement and Preliminaries
341
8.3
Line Searches
345
8.4
Newton’s Method
353
8.5
Equality Constraints and Lagrange Multipliers
357
Appendix 8.A
MATLAB Code for Golden Section Search
362
References
364
Problems
364
9
Numerical Integration and Differentiation
369
9.1
Introduction
369
TLFeBOOK

x
CONTENTS
9.2
Trapezoidal Rule
371
9.3
Simpson’s Rule
378
9.4
Gaussian Quadrature
385
9.5
Romberg Integration
393
9.6
Numerical Differentiation
401
References
406
Problems
406
10
Numerical Solution of Ordinary Differential Equations
415
10.1
Introduction
415
10.2
First-Order ODEs
421
10.3
Systems of First-Order ODEs
442
10.4
Multistep Methods for ODEs
455
10.4.1
Adams–Bashforth Methods
459
10.4.2
Adams–Moulton Methods
461
10.4.3
Comments on the Adams Families
462
10.5
Variable-Step-Size (Adaptive) Methods for ODEs
464
10.6
Stiff Systems
467
10.7
Final Remarks
469
Appendix 10.A
MATLAB Code for Example 10.8
469
Appendix 10.B
MATLAB Code for Example 10.13
470
References
472
Problems
473
11
Numerical Methods for Eigenproblems
480
11.1
Introduction
480
11.2
Review of Eigenvalues and Eigenvectors
480
11.3
The Matrix Exponential
488
11.4
The Power Methods
498
11.5
QR Iterations
508
References
518
Problems
519
12
Numerical Solution of Partial Differential Equations
525
12.1
Introduction
525
12.2
A Brief Overview of Partial Differential Equations
525
12.3
Applications of Hyperbolic PDEs
528
12.3.1
The Vibrating String
528
12.3.2
Plane Electromagnetic Waves
534
TLFeBOOK

CONTENTS
xi
12.4
The Finite-Difference (FD) Method
545
12.5
The Finite-Difference Time-Domain (FDTD) Method
550
Appendix 12.A
MATLAB Code for Example 12.5
557
References
560
Problems
561
13
An Introduction to MATLAB
565
13.1
Introduction
565
13.2
Startup
565
13.3
Some Basic Operators, Operations, and Functions
566
13.4
Working with Polynomials
571
13.5
Loops
572
13.6
Plotting and M-Files
573
References
577
Index
579
TLFeBOOK

TLFeBOOK

PREFACE
The subject of numerical analysis has a long history. In fact, it predates by cen-
turies the existence of the modern computer. Of course, the advent of the modern
computer in the middle of the twentieth century gave greatly added impetus to the
subject, and so it now plays a central role in a large part of engineering analysis,
simulation, and design. This is so true that no engineer can be deemed competent
without some knowledge and understanding of the subject. Because of the back-
ground of the author, this book tends to emphasize issues of particular interest to
electrical and computer engineers, but the subject (and the present book) is certainly
relevant to engineers from all other branches of engineering.
Given the importance level of the subject, a great number of books have already
been written about it, and are now being written. These books span a colossal
range of approaches, levels of technical difﬁculty, degree of specialization, breadth
versus depth, and so on. So, why should this book be added to the already huge,
and growing list of available books?
To begin, the present book is intended to be a part of the students’ ﬁrst exposure
to numerical analysis. As such, it is intended for use mainly in the second year
of a typical 4-year undergraduate engineering program. However, the book may
ﬁnd use in later years of such a program. Generally, the present book arises out of
the author’s objections to educational practice regarding numerical analysis. To be
more speciﬁc
1. Some books adopt a “grocery list” or “recipes” approach (i.e., “methods” at
the expense of “analysis”) wherein several methods are presented, but with
little serious discussion of issues such as how they are obtained and their
relative advantages and disadvantages. In this genre often little consideration
is given to error analysis, convergence properties, or stability issues. When
these issues are considered, it is sometimes in a manner that is too superﬁcial
for contemporary and future needs.
2. Some books fail to build on what the student is supposed to have learned
prior to taking a numerical analysis course. For example, it is common for
engineering students to take a ﬁrst-year course in matrix/linear algebra. Yet,
a number of books miss the opportunity to build on this material in a manner
that would provide a good bridge from ﬁrst year to more sophisticated uses
of matrix/linear algebra in later years (e.g., such as would be found in digital
signal processing or state variable control systems courses).
xiii
TLFeBOOK

xiv
PREFACE
3. Some books miss the opportunity to introduce students to the now quite vital
area of functional analysis ideas as applied to engineering problem solving.
Modern numerical analysis relies heavily on concepts such as function spaces,
orthogonality, norms, metrics, and inner products. Yet these concepts are
often considered in a very ad hoc way, if indeed they are considered at all.
4. Some books tie the subject matter of numerical analysis far too closely to
particular software tools and/or programming languages. But the highly tran-
sient nature of software tools and programming languages often blinds the
user to the timeless nature of the underlying principles of analysis. Further-
more, it is an erroneous belief that one can successfully employ numerical
methods solely through the use of “canned” software without any knowledge
or understanding of the technical details of the contents of the can. While
this does not imply the need to understand a software tool or program down
to the last line of code, it does rule out the “black box” methodology.
5. Some books avoid detailed analysis and derivations in the misguided belief
that this will make the subject more accessible to the student. But this denies
the student the opportunity to learn an important mode of thinking that is a
huge aid to practical problem solving. Furthermore, by cutting the student
off from the language associated with analysis the student is prevented from
learning those skills needed to read modern engineering literature, and to
extract from this literature those things that are useful for solving the problem
at hand.
The prospective user of the present book will likely notice that it contains material
that, in the past, was associated mainly with more advanced courses. However, the
history of numerical computing since the early 1980s or so has made its inclusion
in an introductory course unavoidable. There is nothing remarkable about this. For
example, the material of typical undergraduate signals and systems courses was,
not so long ago, considered to be suitable only for graduate-level courses. Indeed,
most (if not all) of the contents of any undergraduate program consists of material
that was once considered far too advanced for undergraduates, provided one goes
back far enough in time.
Therefore, with respect to the observations mentioned above, the following is a
summary of some of the features of the present book:
1. An axiomatic approach to function spaces is adopted within the ﬁrst chapter.
So the book immediately exposes the student to function space ideas, espe-
cially with respect to metrics, norms, inner products, and the concept of
orthogonality in a general setting. All of this is illustrated by several examples,
and the basic ideas from the ﬁrst chapter are reinforced by routine use
throughout the remaining chapters.
2. The present book is not closely tied to any particular software tool or pro-
gramming language, although a few MATLAB-oriented examples are pre-
sented. These may be understood without any understanding of MATLAB
TLFeBOOK

PREFACE
xv
(derived from the term matrix laboratory) on the part of the student, how-
ever. Additionally, a quick introduction to MATLAB is provided in Chapter
13. These examples are simply intended to illustrate that modern software
tools implement many of the theories presented in the book, and that the
numerical characteristics of algorithms implemented with such tools are not
materially different from algorithm implementations using older software
technologies (e.g., catastrophic convergence, and ill conditioning, continue
to be major implementation issues). Algorithms are often presented in a
Pascal-like pseudocode that is sufﬁciently transparent and general to allow
the user to implement the algorithm in the language of their choice.
3. Detailed proofs and/or derivations are often provided for many key results.
However, not all theorems or algorithms are proved or derived in detail
on those occasions where to do so would consume too much space, or not
provide much insight. Of course, the reader may dispute the present author’s
choices in this matter. But when a proof or derivation is omitted, a reference
is often cited where the details may be found.
4. Some modern applications examples are provided to illustrate the conse-
quences of various mathematical ideas. For example, chaotic cryptography,
the CORDIC (coordinate rotational digital computing) method, and least
squares for system identiﬁcation (in a biomedical application) are considered.
5. The sense in which series and iterative processes converge is given fairly
detailed treatment in this book as an understanding of these matters is now
so crucial in making good choices about which algorithm to use in an appli-
cation. Thus, for example, the difference between pointwise and uniform
convergence is considered. Kernel functions are introduced because of their
importance in error analysis for approximations based on orthogonal series.
Convergence rate analysis is also presented in the context of root-ﬁnding
algorithms.
6. Matrix analysis is considered in sufﬁcient depth and breadth to provide an
adequate introduction to those aspects of the subject particularly relevant to
modern areas in which it is applied. This would include (but not be limited
to) numerical methods for electromagnetics, stability of dynamic systems,
state variable control systems, digital signal processing, and digital commu-
nications.
7. The most important general properties of orthogonal polynomials are pre-
sented. The special cases of Chebyshev, Legendre, and Hermite polynomials
are considered in detail (i.e., detailed derivations of many basic properties
are given).
8. In treating the subject of the numerical solution of ordinary differential
equations, a few books fail to give adequate examples based on nonlin-
ear dynamic systems. But many examples in the present book are based on
nonlinear problems (e.g., the Dufﬁng equation). Furthermore, matrix methods
are introduced in the stability analysis of both explicit and implicit methods
for nth-order systems. This is illustrated with second-order examples.
TLFeBOOK

xvi
PREFACE
Analysis is often embedded in the main body of the text rather than being rele-
gated to appendixes, or to formalized statements of proof immediately following a
theorem statement. This is done to discourage attempts by the reader to “skip over
the math.” After all, skipping over the math defeats the purpose of the book.
Notwithstanding the remarks above, the present book lacks the rigor of a math-
ematically formal treatment of numerical analysis. For example, Lebesgue measure
theory is entirely avoided (although it is mentioned in passing). With respect to
functional analysis, previous authors (e.g., E. Kreyszig, Introductory Functional
Analysis with Applications) have demonstrated that it is very possible to do this
while maintaining adequate rigor for engineering purposes, and this approach is
followed here.
It is largely left to the judgment of the course instructor about what particular
portions of the book to cover in a course. Certainly there is more material here
than can be covered in a single term (or semester). However, it is recommended
that the ﬁrst four chapters be covered largely in their entirety (perhaps excepting
Sections 1.4, 3.6, 3.7, and the part of Section 4.6 regarding SVD). The material of
these chapters is simply too fundamental to be omitted, and is often drawn on in
later chapters.
Finally, some will say that topics such as function spaces, norms and inner
products, and uniform versus pointwise convergence, are too abstract for engineers.
Such individuals would do well to ask themselves in what way these ideas are
more abstract than Boolean algebra, convolution integrals, and Fourier or Laplace
transforms, all of which are standard fare in present-day electrical and computer
engineering curricula.
Engineering  past
Engineering  present
Engineering  future
Christopher Zarowski
TLFeBOOK

1
Functional Analysis Ideas
1.1
INTRODUCTION
Many engineering analysis and design problems are far too complex to be solved
without the aid of computers. However, the use of computers in problem solving
has made it increasingly necessary for users to be highly skilled in (practical)
mathematical analysis. There are a number of reasons for this. A few are as follows.
For one thing, computers represent data to ﬁnite precision. Irrational numbers
such as π or
√
2 do not have an exact representation on a digital computer (with the
possible exception of methods based on symbolic computing). Additionally, when
arithmetic is performed, errors occur as a result of rounding (e.g., the truncation of
the product of two n-bit numbers, which might be 2n bits long, back down to n
bits). Numbers have a limited dynamic range; we might get overﬂow or underﬂow
in a computation. These are examples of ﬁnite-precision arithmetic effects. Beyond
this, computational methods frequently have sources of error independent of these.
For example, an inﬁnite series must be truncated if it is to be evaluated on a com-
puter. The truncation error is something “additional” to errors from ﬁnite-precision
arithmetic effects. In all cases, the sources (and sizes) of error in a computation
must be known and understood in order to make sensible claims about the accuracy
of a computer-generated solution to a problem.
Many methods are “iterative.” Accuracy of the result depends on how many
iterations are performed. It is possible that a given method might be very slow,
requiring many iterations before achieving acceptable accuracy. This could involve
much computer runtime. The obvious solution of using a faster computer is usually
unacceptable. A better approach is to use mathematical analysis to understand why
a method is slow, and so to devise methods of speeding it up. Thus, an important
feature of analysis applied to computational methods is that of assessing how
much in the way of computing resources is needed by a given method. A given
computational method will make demands on computer memory, operations count
(the number of arithmetic operations, function evaluations, data transfers, etc.),
number of bits in a computer word, and so on.
A given problem almost always has many possible alternative solutions. Other
than accuracy and computer resource issues, ease of implementation is also rel-
evant. This is a human labor issue. Some methods may be easier to implement
on a given set of computing resources than others. This would have an impact
An Introduction to Numerical Analysis for Electrical and Computer Engineers, by C.J. Zarowski
ISBN 0-471-46737-5
c⃝2004 John Wiley & Sons, Inc.
1
TLFeBOOK

2
FUNCTIONAL ANALYSIS IDEAS
on software/hardware development time, and hence on system cost. Again, math-
ematical analysis is useful in deciding on the relative ease of implementation of
competing solution methods.
The subject of numerical computing is truly vast. Methods are required to handle
an immense range of problems, such as solution of differential equations (ordi-
nary or partial), integration, solution of equations and systems of equations (linear
or nonlinear), approximation of functions, and optimization. These problem types
appear to be radically different from each other. In some sense the differences
between them are true, but there are means to achieve some unity of approach in
understanding them.
The branch of mathematics that (perhaps) gives the greatest amount of unity
is sometimes called functional analysis. We shall employ ideas from this subject
throughout. However, our usage of these ideas is not truly rigorous; for example,
we completely avoid topology, and measure theory. Therefore, we tend to follow
simpliﬁed treatments of the subject such as Kreyszig [1], and then only those ideas
that are immediately relevant to us. The reader is assumed to be very comfortable
with elementary linear algebra, and calculus. The reader must also be comfortable
with complex number arithmetic (see Appendix 1.A now for a review if necessary).
Some knowledge of electric circuit analysis is presumed since this will provide
a source of applications examples later. (But application examples will also be
drawn from other sources.) Some knowledge of ordinary differential equations is
also assumed.
It is worth noting that an understanding of functional analysis is a tremendous
aid to understanding other subjects such as quantum physics, probability theory
and random processes, digital communications system analysis and design, digital
control systems analysis and design, digital signal processing, fuzzy systems, neural
networks, computer hardware design, and optimal design of systems. Many of the
ideas presented in this book are also intended to support these subjects.
1.2
SOME SETS
Variables in an engineering problem often take on values from sets of numbers.
In the present setting, the sets of greatest interest to us are (1) the set of integers
Z = {. . . −3, −2, −1, 0, 1, 2, 3 . . .}, (2) the set of real numbers R, and (3) the set of
complex numbers C = {x + jy|j =
√
−1, x, y ∈R}. The set of nonnegative inte-
gers is Z+ = {0, 1, 2, 3, . . . , } (so Z+ ⊂Z). Similarly, the set of nonnegative real
numbers is R+ = {x ∈R|x ≥0}. Other kinds of sets of numbers will be introduced
if and when they are needed.
If A and B are two sets, their Cartesian product is denoted by A × B =
{(a, b)|a ∈A, b ∈B}. The Cartesian product of n sets denoted A0, A1, . . . , An−1
is A0 × A1 × · · · × An−1 = {(a0, a1, . . . , an−1)|ak ∈Ak}.
Ideas from matrix/linear algebra are of great importance. We are therefore also
interested in sets of vectors. Thus, Rn shall denote the set of n-element vectors
with real-valued components, and similarly, Cn shall denote the set of n-element
TLFeBOOK

SOME SETS
3
vectors with complex-valued components. By default, we assume any vector x to
be a column vector:
x =


x0
x1
...
xn−2
xn−1


.
(1.1)
Naturally, row vectors are obtained by transposition. We will generally avoid using
bars over or under symbols to denote vectors. Whether a quantity is a vector will
be clear from the context of the discussion. However, bars will be used to denote
vectors when this cannot be easily avoided. The indexing of vector elements xk will
often begin with 0 as indicated in (1.1). Naturally, matrices are also important. Set
Rn×m denotes the set of matrices with n rows and m columns, and the elements are
real-valued. The notation Cn×m should now possess an obvious meaning. Matri-
ces will be denoted by uppercase symbols, again without bars. If A is an n × m
matrix, then
A = [ap,q]p=0,...,n−1, q=0,...,m−1.
(1.2)
Thus, the element in row p and column q of A is denoted ap,q. Indexing of rows
and columns again will typically begin at 0. The subscripts on the right bracket “]”
in (1.2) will often be omitted in the future. We may also write apq instead of ap,q
where no danger of confusion arises.
The elements of any vector may be regarded as the elements of a sequence of
ﬁnite length. However, we are also very interested in sequences of inﬁnite length.
An inﬁnite sequence may be denoted by x = (xk) = (x0, x1, x2, . . .), for which xk
could be either real-valued or complex-valued. It is possible for sequences to be
doubly inﬁnite, for instance, x = (xk) = (. . . , x−2, x−1, x0, x1, x2, . . .).
Relationships between variables are expressed as mathematical functions, that is,
mappings between sets. The notation f |A →B signiﬁes that function f associates
an element of set A with an element from set B. For example, f |R →R represents
a function deﬁned on the real-number line, and this function is also real-valued;
that is, it maps “points” in R to “points” in R. We are familiar with the idea
of “plotting” such a function on the xy plane if y = f (x) (i.e., x, y ∈R). It is
important to note that we may regard sequences as functions that are deﬁned on
either the set Z (the case of doubly inﬁnite sequences), or the set Z+ (the case
of singly inﬁnite sequences). To be more speciﬁc, if, for example, k ∈Z+, then
this number maps to some number xk that is either real-valued or complex-valued.
Since vectors are associated with sequences of ﬁnite length, they, too, may be
regarded as functions, but deﬁned on a ﬁnite subset of the integers. From (1.1) this
subset might be denoted by Zn = {0, 1, 2, . . . , n −2, n −1}.
Sets of functions are important. This is because in engineering we are often
interested in mappings between sets of functions. For example, in electric circuits
voltage and current waveforms (i.e., functions of time) are input to a circuit via volt-
age and current sources. Voltage drops across circuit elements, or currents through
TLFeBOOK

4
FUNCTIONAL ANALYSIS IDEAS
circuit elements are output functions of time. Thus, any circuit maps functions from
an input set to functions from some output set. Digital signal processing systems
do the same thing, except that here the functions are sequences. For example, a
simple digital signal processing system might accept as input the sequence (xn),
and produce as output the sequence (yn) according to
yn = xn + xn+1
2
(1.3)
for which n ∈Z+.
Some speciﬁc examples of sets of functions are as follows, and more will be
seen later. The set of real-valued functions deﬁned on the interval [a, b] ⊂R that
are n times continuously differentiable may be denoted by Cn[a, b]. This means
that all derivatives up to and including order n exist and are continuous. If n = 0
we often just write C[a, b], which is the set of continuous functions on the interval
[a, b]. We remark that the notation [a, b] implies inclusion of the endpoints of the
interval. Thus, (a, b) implies that the endpoints a and b are not to be included [i.e.,
if x ∈(a, b), then a < x < b].
A polynomial in the indeterminate x of degree n is
pn(x) =
n

k=0
pn,kxk.
(1.4)
Unless otherwise stated, we will always assume pn,k ∈R. The indeterminate x
is often considered to be either a real number or a complex number. But in
some circumstances the indeterminate x is merely regarded as a “placeholder,”
which means that x is not supposed to take on a value. In a situation like this
the polynomial coefﬁcients may also be regarded as elements of a vector (e.g.,
pn = [pn,0 pn,1 · · · pn,n]T ). This happens in digital signal processing when we
wish to convolve1 sequences of ﬁnite length, because the multiplication of polyno-
mials is mathematically equivalent to the operation of sequence convolution. We
will denote the set of all polynomials of degree n as Pn. If x is to be from the
interval [a, b] ⊂R, then the set of polynomials of degree n on [a, b] is denoted
by Pn[a, b]. If m < n we shall usually assume Pm[a, b] ⊂Pn[a, b].
1.3
SOME SPECIAL MAPPINGS: METRICS, NORMS,
AND INNER PRODUCTS
Sets of objects (vectors, sequences, polynomials, functions, etc.) often have cer-
tain special mappings deﬁned on them that turn these sets into what are commonly
called function spaces. Loosely speaking, functional analysis is about the properties
1These days it seems that the operation of convolution is ﬁrst given serious study in introductory signals
and systems courses. The operation of convolution is fundamental to all forms of signal processing,
either analog or digital.
TLFeBOOK

SOME SPECIAL MAPPINGS: METRICS, NORMS, AND INNER PRODUCTS
5
of function spaces. Generally speaking, numerical computation problems are best
handled by treating them in association with suitable mappings on well-chosen
function spaces. For our purposes, the three most important special types of map-
pings are (1) metrics, (2) norms, and (3) inner products. You are likely to be already
familiar with special cases of these really very general ideas.
The vector dot product is an example of an inner product on a vector space, while
the Euclidean norm (i.e., the square root of the sum of the squares of the elements in
a real-valued vector) is a norm on a vector space. The Euclidean distance between
two vectors (given by the Euclidean norm of the difference between the two vectors)
is a metric on a vector space. Again, loosely speaking, metrics give meaning to the
concept of “distance” between points in a function space, norms give a meaning
to the concept of the “size” of a vector, and inner products give meaning to the
concept of “direction” in a vector space.2
In Section 1.1 we expressed interest in the sizes of errors, and so naturally the
concept of a norm will be of interest. Later we shall see that inner products will
prove to be useful in devising means of overcoming problems due to certain sources
of error in a computation. In this section we shall consider various examples of
function spaces, some of which we will work with later on in the analysis of
certain computational problems. We shall see that there are many different kinds
of metric, norm, and inner product. Each kind has its own particular advantages
and disadvantages as will be discovered as we progress through the book.
Sometimes a quantity cannot be computed exactly. In this case we may try to
estimate bounds on the size of the quantity. For example, ﬁnding the exact error
in the truncation of a series may be impossible, but putting a bound on the error
might be relatively easy. In this respect the concepts of supremum and inﬁmum
can be important. These are deﬁned as follows.
Suppose we have E ⊂R. We say that E is bounded above if E has an upper
bound, that is, if there exists a B ∈R such that x ≤B for all x ∈E. If E ̸= ∅
(empty set; set containing no elements) there is a supremum of E [also called a
least upper bound (lub)], denoted
sup E.
For example, suppose E = [0, 1), then any B ≥1 is an upper bound for E, but
sup E = 1. More generally, sup E ≤B for every upper bound B of E. Thus, the
supremum is a “tight” upper bound. Similarly, E may be bounded below. If E has
a lower bound there is a b ∈R such that x ≥b for all x ∈E. If E ̸= ∅, then there
exists an inﬁmum [also called a greatest lower bound (glb)], denoted by
inf E.
For example, suppose now E = (0, 1]; then any b ≤0 is a lower bound for E,
but inf E = 0. More generally, inf E ≥b for every lower bound b of E. Thus, the
inﬁmum is a “tight” lower bound.
2The idea of “direction” is (often) considered with respect to the concept of an orthogonal basis in a
vector space. To deﬁne “orthogonality” requires the concept of an inner product. We shall consider this
in various ways later on.
TLFeBOOK

6
FUNCTIONAL ANALYSIS IDEAS
1.3.1
Metrics and Metric Spaces
In mathematics an axiomatic approach is often taken in the development of analysis
methods. This means that we deﬁne a set of objects, a set of operations to be
performed on the set of objects, and rules obeyed by the operations. This is typically
how mathematical systems are constructed. The reader (hopefully) has already seen
this approach in the application of Boolean algebra to the analysis and design of
digital electronic systems (i.e., digital logic). We adopt the same approach here.
We will begin with the following deﬁnition.
Deﬁnition 1.1: Metric Space, Metric
A metric space is a set X and a
function d|X × X →R+, which is called a metric or distance function on X.
If x, y, z ∈X then d satisﬁes the following axioms:
(M1)
d(x, y) = 0 if and only if (iff) x = y.
(M2)
d(x, y) = d(y, x) (symmetry property).
(M3)
d(x, y) ≤d(x, z) + d(z, y) (triangle inequality).
We emphasize that X by itself cannot be a metric space until we deﬁne d. Thus,
the metric space is often denoted by the pair (X, d). The phrase “if and only
if” probably needs some explanation. In (M1), if you were told that d(x, y) = 0,
then you must immediately conclude that x = y. Conversely, if you were told that
x = y, then you must immediately conclude that d(x, y) = 0. Instead of the words
“if and only if” it is also common to write
d(x, y) = 0 ⇔x = y.
The phrase “if and only if” is associated with elementary logic. This subject is
reviewed in Appendix 1.B. It is recommended that the reader study that appendix
before continuing with later chapters.
Some examples of metric spaces now follow.
Example 1.1
Set X = R, with
d(x, y) = |x −y|
(1.5)
forms a metric space. The metric (1.5) is what is commonly meant by the “distance
between two points on the real number line.” The metric (1.5) is quite useful in
discussing the sizes of errors due to rounding in digital computation. This is because
there is a norm on R that gives rise to the metric in (1.5) (see Section 1.3.2).
Example 1.2
The set of vectors Rn with
d(x, y) =
n−1

k=0
[xk −yk]2
	1/2
(1.6a)
TLFeBOOK

SOME SPECIAL MAPPINGS: METRICS, NORMS, AND INNER PRODUCTS
7
forms a (Euclidean) metric space. However, another valid metric on Rn is given by
d1(x, y) =
n−1

k=0
|xk −yk|.
(1.6b)
In other words, we can have the metric space (X, d), or (X, d1). These spaces are
different because their metrics differ.
Euclidean metrics, and their related norms and inner products, are useful in pos-
ing and solving least-squares approximation problems. Least-squares approximation
is a topic we shall consider in detail later.
Example 1.3
Consider the set of (singly) inﬁnite, complex-valued, and bounded
sequences
X = {x = (x0, x1, x2, . . .)|xk ∈C, |xk| ≤c(x)(all k)}.
(1.7a)
Here c(x) ≥0 is a bound that may depend on x, but not on k. This set forms a
metric space that may be denoted by l∞[0, ∞] if we employ the metric
d(x, y) = sup
k∈Z+ |xk −yk|.
(1.7b)
The notation [0, ∞] emphasizes that the sequences we are talking about are only
singly inﬁnite. We would use [−∞, ∞] to specify that we are talking about doubly
inﬁnite sequences.
Example 1.4
Deﬁne J = [a, b] ⊂R. The set C[a, b] will be a metric space if
d(x, y) = sup
t∈J
|x(t) −y(t)|.
(1.8)
In Example 1.1 the metric (1.5) gives the “distance” between points on the real-
number line. In Example 1.4 the “points” are real-valued, continuous functions of
t ∈[a, b]. In functional analysis it is essential to get used to the idea that functions
can be considered as points in a space.
Example 1.5
The set X in (1.7a), where we now allow c(x) →∞(in other
words, the sequence need not be bounded here), but with the metric
d(x, y) =
∞

k=0
1
2k+1
|xk −yk|
1 + |xk −yk|
(1.9)
is a metric space. (Sometimes this space is denoted s.)
TLFeBOOK

8
FUNCTIONAL ANALYSIS IDEAS
Example 1.6
Let p be a real-valued constant such that p ≥1. Consider the
set of complex-valued sequences
X =

x = (x0, x1, x2, . . .)|xk ∈C,
∞

k=0
|xk|p < ∞

.
(1.10a)
This set together with the metric
d(x, y) =
 ∞

k=0
|xk −yk|p
	1/p
(1.10b)
forms a metric space that we denote by lp[0, ∞].
Example 1.7
Consider the set of complex-valued functions on [a, b] ⊂R
X =

x(t)

 b
a
|x(t)|2 dt < ∞

(1.11a)
for which
d(x, y) =
 b
a
|x(t) −y(t)|2 dt
1/2
(1.11b)
is a metric. Pair (X, d) forms a metric space that is usually denoted by L2[a, b].
The metric space of Example 1.7 (along with certain variations) is very impor-
tant in the theory of orthogonal polynomials, and in least-squares approximation
problems. This is because it turns out to be an inner product space too (see
Section 1.3.3). Orthogonal polynomials have a major role to play in the solution
of least squares, and other types of approximation problem.
All of the metrics deﬁned in the examples above may be shown to satisfy the
axioms of Deﬁnition 1.1. Of course, at least in some cases, much effort might be
required to do this. In this book we largely avoid making this kind of effort.
1.3.2
Norms and Normed Spaces
So far our examples of function spaces have been metric spaces (Section 1.3.1).
Such spaces are not necessarily associated with the concept of a vector space.
However, normed spaces (i.e., spaces with norms deﬁned on them) are always
associated with vector spaces. So, before we can deﬁne a norm, we need to recall
the general deﬁnition of a vector space.
The following deﬁnition invokes the concept of a ﬁeld of numbers. This concept
arises in abstract algebra and number theory [e.g., 2, 3], a subject we wish to avoid
considering here.3 It is enough for the reader to know that R and C are ﬁelds under
3This avoidance is not to disparage abstract algebra. This subject is a necessary prerequisite to under-
standing concepts such as fast algorithms for digital signal processing (i.e., fast Fourier transforms, and
fast convolution algorithms; e.g., see Ref. 4), cryptography and data security, and error control codes
for digital communications.
TLFeBOOK

SOME SPECIAL MAPPINGS: METRICS, NORMS, AND INNER PRODUCTS
9
the usual real and complex arithmetic operations. These are really the only ﬁelds
that we shall work with. We remark, largely in passing, that rational numbers (set
denoted Q) are also a ﬁeld under the usual arithmetic operations.
Deﬁnition 1.2: Vector Space
A vector space (linear space) over a ﬁeld K is
a nonempty set X of elements x, y, z, . . . called vectors together with two algebraic
operations. These operations are vector addition, and the multiplication of vectors
by scalars that are elements of K. The following axioms must be satisﬁed:
(V1) If x, y ∈X, then x + y ∈X (additive closure).
(V2) If x, y, z ∈X, then (x + y) + z = x + (y + z) (associativity).
(V3) There exists a vector in X denoted 0 (zero vector) such that for all x ∈X,
we have x + 0 = 0 + x = x.
(V4) For all x ∈X, there is a vector −x ∈X such that −x + x = x +
(−x) = 0. We call −x the negative of a vector.
(V5) For all x, y ∈X we have x + y = y + x (commutativity).
(V6) If x ∈X and a ∈K, then the product of a and x is ax, and ax ∈X.
(V7) If x, y ∈X, and a ∈K, then a(x + y) = ax + ay.
(V8) If a, b ∈K, and x ∈X, then (a + b)x = ax + bx.
(V9) If a, b ∈K, and x ∈X, then ab(x) = a(bx).
(V10) If x ∈X, and 1 ∈K, then 1x = x multiplication of a vector by a unit
scalar; all ﬁelds contain a unit scalar (i.e., a number called “one”).
In this deﬁnition, as already noted, we generally work only with K = R, or K = C.
We represent the zero vector by 0 just as we also represent the scalar zero by 0.
Rarely is there danger of confusion.
The reader is already familiar with the special instances of this that relate to the
sets Rn and Cn. These sets are vector spaces under Deﬁnition 1.2, where vector
addition is deﬁned to be
x + y =


x0
x1
...
xn−1

+


y0
y1
...
yn−1

=


x0 + y0
x1 + y1
...
xn−1 + yn−1

,
(1.12a)
and multiplication by a ﬁeld element is deﬁned to be
ax =


ax0
ax1
...
axn−1

.
(1.12b)
The zero vector is 0 = [00 · · · 00]T , and −x = [−x0 −x1 · · · −xn−1]T . If X = Rn
then the elements of x and y are real-valued, and a ∈R, but if X = Cn then the
TLFeBOOK

10
FUNCTIONAL ANALYSIS IDEAS
elements of x and y are complex-valued, and a ∈C. The metric spaces in Exam-
ple 1.2 are therefore also vector spaces under the operations deﬁned in (1.12a,b).
Some further examples of vector spaces now follow.
Example 1.8
Metric space C[a, b] (Example 1.4) is a vector space under the
operations
(x + y)(t) = x(t) + y(t),
(αx)(t) = αx(t),
(1.13)
where α ∈R. The zero vector is the function that is identically zero on the interval
[a, b].
Example 1.9
Metric space l2[0, ∞] (Example 1.6) is a vector space under the
operations
x + y = (x0, x1, . . .) + (y0, y1, . . .) = (x0 + y0, x1 + y1, . . .),
αx = (αx0, αx1, . . .).
(1.14)
Here α ∈C.
If x, y ∈l2[0, ∞], then some effort is required to verify axiom (V1). This
requires the Minkowski inequality, which is
 ∞

k=0
|xk + yk|p
	1/p
≤
 ∞

k=0
|xk|p
	1/p
+
 ∞

k=0
|yk|p
	1/p
.
(1.15)
Refer back to Example 1.6; here we employ p = 2, but (1.15) is valid for p ≥1.
Proof of (1.15) is somewhat involved, and so is omitted here. The interested reader
can see Kreyszig [1, pp. 11–15].
We remark that the Minkowski inequality can be proved with the aid of the
H¨older inequality
∞

k=0
|xkyk| ≤
 ∞

k=0
|xk|p
	1/p  ∞

k=0
|yk|q
	1/q
(1.16)
for which here p > 1 and 1
p + 1
q = 1.
We are now ready to deﬁne a normed space.
Deﬁnition 1.3: Normed Space, Norm
A normed space X is a vector space
with a norm deﬁned on it. If x ∈X then the norm of x is denoted by
||x||
(read this as “norm of x”).
The norm must satisfy the following axioms:
(N1) ||x|| ≥0 (i.e., the norm is nonnegative).
TLFeBOOK

SOME SPECIAL MAPPINGS: METRICS, NORMS, AND INNER PRODUCTS
11
(N2) ||x|| = 0 ⇔x = 0.
(N3) ||αx|| = |α| ||x||. Here α is a scalar in the ﬁeld of X (i.e., α ∈K; see
Deﬁnition 3.2).
(N4) ||x + y|| ≤||x|| + ||y|| (triangle inequality).
The normed space is vector space X together with a norm, and so may be properly
denoted by the pair (X, || · ||). However, we may simply write X, and say “normed
space X,” so the norm that goes along with X is understood from the context of
the discussion.
It is important to note that all normed spaces are also metric spaces, where the
metric is given by
d(x, y) = ||x −y||
(x, y ∈X).
(1.17)
The metric in (1.17) is called the metric induced by the norm.
Various other properties of norms may be deduced. One of these is:
Example 1.10
Prove | ||y|| −||x|| | ≤||y −x||.
Proof
From (N3) and (N4)
||y|| = ||y −x + x|| ≤||y −x|| + ||x||, ||x|| = ||x −y + y|| ≤||y −x|| + ||y||.
Combining these, we obtain
||y|| −||x|| ≤||y −x||, ||y|| −||x|| ≥−||y −x||.
The claim follows immediately.
We may regard the norm as a mapping from X to set R: || · |||X →R. This
mapping can be shown to be continuous. However, this requires generalizing the
concept of continuity that you may know from elementary calculus. Here we deﬁne
continuity as follows.
Deﬁnition 1.4: Continuous Mapping
Suppose X = (X, d) and Y = (Y, d)
are two metric spaces. The mapping T |X →Y is said to be continuous at a point
x0 ∈X if for all ϵ > 0 there is a δ > 0 such that
d(T x, T x0) < ϵ
for all x satisfying
d(x, x0) < δ.
(1.18)
T is said to be continuous if it is continuous at every point of X.
Note that T x is just another way of writing T (x). (R, | · |) is a normed space; that
is, the set of real numbers with the usual arithmetic operations deﬁned on it is a
TLFeBOOK

12
FUNCTIONAL ANALYSIS IDEAS
vector space, and the absolute value of an element of R is the norm of that element.
If we identify Y in Deﬁnition 1.4 with metric space (R, | · |), then (1.18) becomes
d(T x, T x0) = d(||x||, ||x0||) = | ||x|| −||x0|| | < ϵ,
d(x, x0) = ||x −x0|| < δ.
To make these claims, we are using (1.17). In other words, X and Y are normed
spaces, and we employ the metrics induced by their respective norms. In addition,
we identify T with || · ||. Using Example 1.10, we obtain
| ||x|| −||x0|| | ≤||x −x0|| < δ.
Thus, the requirements of Deﬁnition 1.4 are met, and so we conclude that norms
are continuous mappings.
We now list some other normed spaces.
Example 1.11
The Euclidean space Rn and the unitary space Cn are both
normed spaces, where the norm is deﬁned to be
||x|| =
n−1

k=0
|xk|2
	1/2
.
(1.19)
For Rn the absolute value bars may be dropped.4 It is easy to see that d(x, y) =
||x −y|| gives the same metric as in (1.6a) for space Rn. We further remark that
for n = 1 we have ||x|| = |x|.
Example 1.12
The space lp[0, ∞] is a normed space if we deﬁne the norm
to be
||x|| =
 ∞

k=0
|xk|p
	1/p
(1.20)
for which d(x, y) = ||x −y|| coincides with the metric in (1.10b).
Example 1.13
The sequence space l∞[0, ∞] from Example 1.3 of Section 1.3.1
is a normed space, where the norm is deﬁned to be
||x|| = sup
k∈Z+ |xk|,
(1.21)
and this norm induces the metric of (1.7b).
4Suppose z = x + jy (j =
√
−1, x, y ∈R) is some arbitrary complex number. Recall that z2 ̸= |z|2
in general.
TLFeBOOK

SOME SPECIAL MAPPINGS: METRICS, NORMS, AND INNER PRODUCTS
13
Example 1.14
The space C[a, b] ﬁrst seen in Example 1.4 is a normed space,
where the norm is deﬁned by
||x|| = sup
t∈J
|x(t)|.
(1.22)
Naturally, this norm induces the metric of (1.8).
Example 1.15
The space L2[a, b] of Example 1.7 is a normed space for the
norm
||x|| =
 b
a
|x(t)|2 dt
1/2
.
(1.23)
This norm induces the metric in (1.11b).
The normed space of Example 1.15 is important in the following respect.
Observe that
||x||2 =
 b
a
|x(t)|2 dt.
(1.24)
Suppose we now consider a resistor with resistance R. If the voltage drop across its
terminals is v(t) and the current through it is i(t), we know that the instantaneous
power dissipated in the device is p(t) = v(t)i(t). If we assume that the resistor is
a linear device, then v(t) = Ri(t) via Ohm’s law. Thus
p(t) = v(t)i(t) = Ri2(t).
(1.25)
Consequently, the amount of energy delivered to the resistor over time interval
t ∈[a, b] is given by
E = R
 b
a
i2(t) dt.
(1.26)
If the voltage/current waveforms in our circuit containing R belong to the space
L2[a, b], then clearly E = R||i||2. We may therefore regard the square of the L2
norm [given by (1.24)] of a signal to be the energy of the signal, provided the
norm exists. This notion can be helpful in the optimal design of electric circuits
(e.g., electric ﬁlters), and also of optimal electronic circuits. In analogous fashion,
an element x of space l2[0, ∞] satisﬁes
||x||2 =
∞

k=0
|xk|2 < ∞
(1.27)
[see (1.10a) and Example 1.12]. We may consider ||x||2 to be the energy of the
single-sided sequence x. This notion is useful in the optimal design of digital ﬁlters.
TLFeBOOK

14
FUNCTIONAL ANALYSIS IDEAS
1.3.3
Inner Products and Inner Product Spaces
The concept of an inner product is necessary before one can talk about orthogonal
bases for vector spaces. Recall from elementary linear algebra that orthogonal
bases were important in representing vectors. From a computational standpoint,
as mentioned earlier, orthogonal bases can have a simplifying effect on certain
types of approximation problem (e.g., least-squares approximations), and represent
a means of controlling numerical errors due to so-called ill-conditioned problems.
Following our axiomatic approach, consider the following deﬁnition.
Deﬁnition 1.5: Inner Product Space, Inner Product
An inner product space
is a vector space X with an inner product deﬁned on it. The inner product is a
mapping ⟨·, ·⟩|X × X →K that satisﬁes the following axioms:
(I1) ⟨x + y, z⟩= ⟨x, z⟩+ ⟨y, z⟩.
(I2) ⟨αx, y⟩= α⟨x, y⟩.
(I3) ⟨x, y⟩= ⟨y, x⟩∗.
(I4) ⟨x, x⟩≥0, and ⟨x, x⟩= 0 ⇔x = 0.
Naturally, x, y, z ∈X, and α is a scalar from the ﬁeld K of vector space X. The
asterisk superscript on ⟨y, x⟩in (I3) denotes complex conjugation.5
If the ﬁeld of X is not C, then the operation of complex conjugation in (I3) is
redundant.
All inner product spaces are also normed spaces, and hence are also metric
spaces. This is because the inner product induces a norm on X
||x|| = [⟨x, x⟩]1/2
(1.28)
for all x ∈X. Following (1.17), the induced metric is
d(x, y) = ||x −y|| = [⟨x −y, x −y⟩]1/2.
(1.29)
Directly from the axioms of Deﬁnition 1.5, it is possible to deduce that (for
x, y, z ∈X and a, b ∈K)
⟨ax + by, z⟩= a⟨x, z⟩+ b⟨y, z⟩,
(1.30a)
⟨x, ay⟩= a∗⟨x, y⟩,
(1.30b)
and
⟨x, ay + bz⟩= a∗⟨x, y⟩+ b∗⟨x, z⟩.
(1.30c)
The reader should prove these as an exercise.
5If z = x + yj is a complex number, then its conjugate is z∗= x −yj.
TLFeBOOK

SOME SPECIAL MAPPINGS: METRICS, NORMS, AND INNER PRODUCTS
15
We caution the reader that not all normed spaces are inner product spaces. We
may construct an example with the aid of the following example.
Example 1.16
Let x, y be from an inner product space. If || · || is the norm
induced by the inner product, then ||x + y||2 + ||x −y||2 = 2(||x||2 + ||y||2). This
is the parallelogram equality.
Proof
Via (1.30a,c) we have
||x + y||2 = ⟨x + y, x + y⟩= ⟨x, x + y⟩+ ⟨y, x + y⟩
= ⟨x, x⟩+ ⟨x, y⟩+ ⟨y, x⟩+ ⟨y, y⟩,
and
||x −y||2 = ⟨x −y, x −y⟩= ⟨x, x −y⟩−⟨y, x −y⟩
= ⟨x, x⟩−⟨x, y⟩−⟨y, x⟩+ ⟨y, y⟩.
Adding these gives the stated result.
It turns out that the space lp[0, ∞] with p ̸= 2 is not an inner product space. The
parallelogram equality can be used to show this. Consider x = (1, 1, 0, 0, . . .), y =
(1, −1, 0, 0, . . .), which are certainly elements of lp[0, ∞] [see (1.10a)]. We see that
||x|| = ||y|| = 21/p, ||x + y|| = ||x −y|| = 2.
The parallelogram equality is not satisﬁed, which implies that our norm does not
come from an inner product. Thus, lp[0, ∞] with p ̸= 2 cannot be an inner product
space.
On the other hand, l2[0, ∞] is an inner product space, where the inner product
is deﬁned to be
⟨x, y⟩=
∞

k=0
xky∗
k .
(1.31)
Does this inﬁnite series converge? Yes, it does. To see this, we need the Cauchy–
Schwarz inequality.6 Recall the H¨older inequality of (1.16). Let p = 2, so that
q = 2. Then the Cauchy–Schwarz inequality is
∞

k=0
|xkyk| ≤
 ∞

k=0
|xk|2
	1/2  ∞

k=0
|yk|2
	1/2
.
(1.32)
6The inequality we consider here is related to the Schwarz inequality. We will consider the Schwarz
inequality later on. This inequality is of immense practical value to electrical and computer engineers.
It is used to derive the matched-ﬁlter receiver, which is employed in digital communications systems,
to derive the uncertainty principle in quantum mechanics and in signal processing, and to derive the
Cram´er–Rao lower bound on the variance of parameter estimators, to name only three applications.
TLFeBOOK

16
FUNCTIONAL ANALYSIS IDEAS
Now
|⟨x, y⟩| =

∞

k=0
xky∗
k
 ≤
∞

k=0
|xkyk|.
(1.33)
The inequality in (1.33) follows from the triangle inequality for | · |. (Recall that the
absolute value operation is a norm on R. It is also a norm on C; if z = x + jy ∈C,
then |z| =

x2 + y2.) The right-hand side of (1.32) is ﬁnite because x and y are
in l2[0, ∞]. Thus, from (1.33), ⟨x, y⟩is ﬁnite. Thus, the series (1.31) converges.
It turns out that C[a, b] is not an inner product space, either. But we will not
demonstrate the truth of this claim here.
Some further examples of inner product spaces are as follows.
Example 1.17
The Euclidean space Rn is an inner product space, where the
inner product is deﬁned to be
⟨x, y⟩=
n−1

k=0
xkyk.
(1.34)
The reader will recognize this as the vector dot product from elementary linear
algebra; that is, x · y = ⟨x, y⟩. It is well worth noting that
⟨x, y⟩= xT y.
(1.35)
Here the superscript T denotes transposition. So, xT is a row vector. The inner
product in (1.34) certainly induces the norm in (1.19).
Example 1.18
The unitary space Cn is an inner product space for the inner
product
⟨x, y⟩=
n−1

k=0
xky∗
k .
(1.36)
Again, the norm of (1.19) is induced by inner product (1.36). If H denotes the
operation of complex conjugation and transposition (this is called Hermitian trans-
position), then
yH = [y∗
0y∗
1 · · · y∗
n−1]
(row vector), and
⟨x, y⟩= yH x.
(1.37)
Example 1.19
The space L2[a, b] from Example 1.7 is an inner product space
if the inner product is deﬁned to be
⟨x, y⟩=
 b
a
x(t)y∗(t) dt.
(1.38)
TLFeBOOK

SOME SPECIAL MAPPINGS: METRICS, NORMS, AND INNER PRODUCTS
17
The norm induced by (1.38) is
||x|| =
 b
a
|x(t)|2 dt
1/2
.
(1.39)
This in turn induces the metric in (1.11b).
Now we consider the concept of orthogonality in a completely general manner.
Deﬁnition 1.6: Orthogonality
Let x, y be vectors from some inner product
space X. These vectors are orthogonal iff
⟨x, y⟩= 0.
The orthogonality of x and y is symbolized by writing x ⊥y. Similarly, for subsets
A, B ⊂X we write x ⊥A if x ⊥a for all a ∈A, and A ⊥B if a ⊥b for all a ∈A,
and b ∈B.
If we consider the inner product space R2, then it is easy to see that
⟨[1 0]T , [0 1]T ⟩= 0, so [0 1]T , and [1 0]T are orthogonal vectors. In fact, these
vectors form an orthogonal basis for R2, a concept we will consider more gen-
erally below. If we deﬁne the unit vectors e0 = [1 0]T , and e1 = [0 1]T , then we
recall that any x ∈R2 can be expressed as x = x0e0 + x1e1. (The extension of
this reasoning to Rn for n > 2 should be clear.) Another example of a pair of
orthogonal vectors would be x =
1
√
2[1 1]T , and y =
1
√
2[1 −1]T . These too form
an orthogonal basis for the space R2.
Deﬁne the functions
φ(x) =
 0,
x < 0 and x ≥1
1,
0 ≤x < 1
(1.40)
and
ψ(x) =



0,
x < 0 and x ≥1
1,
0 ≤x < 1
2
−1,
1
2 ≤x < 1
.
(1.41)
Function φ(x) is called the Haar scaling function, and function ψ(x) is called the
Haar wavelet [5]. The function φ(x) is also called an non-return-to-zero (NRZ)
pulse, and function ψ(x) is also called a Manchester pulse [6]. It is easy to con-
ﬁrm that these pulses are elements of L2(R) = L2(−∞, ∞), and that they are
orthogonal, that is, ⟨φ, ψ⟩= 0 under the inner product deﬁned in (1.38). This is
so because
⟨φ, ψ⟩=
 ∞
−∞
φ(x)ψ∗(x) dx =
 1
0
ψ(x) dx = 0.
TLFeBOOK

18
FUNCTIONAL ANALYSIS IDEAS
Thus, we consider φ and ψ to be elements in the inner product space L2(R), for
which the inner product is
⟨x, y⟩=
 ∞
−∞
x(t)y∗(t) dt.
It turns out that the Haar wavelet is the simplest example of the more general class
of Daubechies wavelets. The general theory of these wavelets ﬁrst appeared in
Daubechies [7]. Their development has revolutionized signal processing and many
other areas.7 The main reason for this is the fact that for any f (t) ∈L2(R)
f (t) =
∞

n=−∞
∞

k=−∞
⟨f, ψn,k⟩ψn,k(t),
(1.42)
where ψn,k(t) = 2n/2ψ(2nt −k). This doubly inﬁnite series is called a wavelet
series expansion for f . The coefﬁcients fn,k = ⟨f, ψn,k⟩have ﬁnite energy. In
effect, if we treat either k or n as a constant, then the resulting doubly inﬁnite
sequence is in the space l2[−∞, ∞]. In fact, it is also the case that
∞

n=−∞
∞

k=−∞
|fn,k|2 < ∞.
(1.43)
It is to be emphasized that the ψ used in (1.42) could be (1.41), or it could be
chosen from the more general class in Ref. 7. We shall not prove these things in
this book, as the technical arguments are quite hard.
The wavelet series is presently not as familiar to the broader electrical and
computer engineering community as is the Fourier series. A brief summary of the
Fourier series is as follows. Again, rigorous proofs of many of the following claims
will be avoided, though good introductory references to Fourier series are Tolstov
[8] or Kreyszig [9]. If f ∈L2(0, 2π), then
f (t) =
∞

n=−∞
fnejnt,
j =
√
−1,
(1.44)
where the Fourier (series) coefﬁcients are given by
fn = 1
2π
 2π
0
f (t)e−jnt dt.
(1.45)
We may deﬁne
en(t) = exp(jnt)
(t ∈(0, 2π), n ∈Z)
(1.46)
7For example, in digital communications the problem of designing good signaling pulses for data
transmission is best treated with respect to wavelet theory.
TLFeBOOK

SOME SPECIAL MAPPINGS: METRICS, NORMS, AND INNER PRODUCTS
19
so that we see
⟨f, en⟩= 1
2π
 2π
0
f (t)

ejnt∗
dt = fn.
(1.47)
The series (1.44) is the complex Fourier series expansion for f . Note that for
n, k ∈Z
exp[jn(t + 2πk)] = exp[jnt] exp[2πjnk] = exp[jnt].
(1.48)
Here we have used Euler’s identity
ejx = cos x + j sin x
(1.49)
and cos(2πk) = 1, sin(2πk) = 0. The function ejnt is therefore 2π-periodic; that
is, its period is 2π. It therefore follows that the series on the right-hand side of
(3.40) is a 2π-periodic function, too. The result (1.48) implies that, although f in
(1.44) is initially deﬁned only on (0, 2π), we are at liberty to “periodically extend”
f over the entire real-number line; that is, we can treat f as one period of the
periodic function
˜f (t) =

k∈Z
f (t + 2πk)
(1.50)
for which f (t) = ˜f (t) for t ∈(0, 2π). Thus, series (1.44) is a way to represent
periodic functions. Because f ∈L2(0, 2π), it turns out that
∞

n=−∞
|fn|2 < ∞
(1.51)
so that (fn) ∈l2[−∞, ∞].
Observe that in (1.47) we have “redeﬁned” the inner product on L2(0, 2π) to be
⟨x, y⟩= 1
2π
 2π
0
x(t)y∗(t) dt
(1.52)
which differs from (1.38) in that it has the factor
1
2π in front. This variation also
happens to be a valid inner product on the vector space deﬁned by the set in (1.11a).
Actually, it is a simple example of a weighted inner product.
Now consider, for n ̸= m
⟨en, em⟩= 1
2π
 2π
0
ejnte−jmt dt =
1
2πj(n −m)

ej(n−m)t2π
0
= e2πj(n−m) −1
2πj(n −m) =
1 −1
2πj(n −m) = 0.
(1.53)
Similarly
⟨en, en⟩= 1
2π
 2π
0
ejnte−jnt dt = 1
2π
 2π
0
dt = 1.
(1.54)
So, en and em (if n ̸= m) are orthogonal with respect to the inner product in (1.52).
TLFeBOOK

20
FUNCTIONAL ANALYSIS IDEAS
From basic electric circuit analysis, periodic signals have ﬁnite power. Therefore,
series (1.44) is a way to represent ﬁnite power signals.8 We might therefore consider
the space L2(0, 2π) to be the “space of ﬁnite power signals.” From considerations
involving the wavelet series representation of (1.42), we may consider L2(R) to
be the “space of ﬁnite energy signals.” Recall also the discussion at the end of
Section 1.3.2 (last paragraph).
An example of a Fourier series expansion is the following.
Example 1.20
Suppose that
f (t) =

1,
0 < t < π
−1,
π ≤t < 2π
.
(1.55)
A sketch of this function is one period of a 2π-periodic square wave. The Fourier
coefﬁcients are given by (for n ̸= 0)
fn = 1
2π
 2π
0
f (t)e−jnt dt = 1
2π
 π
0
e−jnt dt −
 2π
π
e−jnt dt
	
= 1
2π

−1
jn

e−jntπ
0 + 1
jn

e−jnt2π
π

= 1
2π
1 −e−jnπ −e−jnπ + 1
jn

= 1
π
1 −e−jnπ
jn
= 2
πne−jnπ/2 ejnπ/2 −e−jnπ/2
2j
= 2
πne−jnπ/2 sin
πn
2

,
(1.56)
where we have made use of
sin x = 1
2j [ejx −e−jx].
(1.57)
This is easily derived using the Euler identity in (1.49). For n = 0, it should be
clear that f0 = 0.
The coefﬁcients fn in (1.56) involve expressions containing j. Since f (t) is
real-valued, it therefore follows that we can rewrite the series expansion in such a
manner as to avoid complex arithmetic. It is almost a standard practice to do this.
We now demonstrate this process:
∞

n=−∞
fnejnt = 2
π
 ∞

n=1
1
ne−jnπ/2 sin
π
2 n

ejnt +
−1

n=−∞
1
ne−jnπ/2 sin
π
2 n

ejnt
	
= 2
π
 ∞

n=1
1
ne−jnπ/2 sin
π
2 n

ejnt +
∞

n=1
1
nejnπ/2 sin
π
2 n

e−jnt
	
8In fact, using phasor analysis and superposition, you can apply (1.44) to determine the steady-state
output of a circuit for any periodic input (including, and especially, nonsinusoidal periodic functions).
This makes the Fourier series very important in electrical/electronic circuit analysis.
TLFeBOOK

SOME SPECIAL MAPPINGS: METRICS, NORMS, AND INNER PRODUCTS
21
= 2
π
∞

n=1
1
n sin
π
2 n
 
ejnte−jπn/2 + e−jntejπn/2
= 4
π
∞

n=1
1
n cos

n

t −π
2

sin
π
2 n

Here we have used the fact that (see Appendix 1.A)
ejnte−jπn/2 + e−jntejπn/2 = 2 Re [ejnte−jπn/2] = 2 cos

n

t −π
2

.
This is so because if z = x + jy, then z + z∗= 2x = 2 Re [z]. Since
cos(α + β) = cos α cos β −sin α sin β,
we have
cos

n

t −π
2

= cos(nt) cos πn
2 + sin(nt) sin πn
2 .
However, if n is an even number, then sin(πn/2) = 0, and if n is an odd number,
then cos(πn/2) = 0. Therefore
4
π
∞

n=1
1
n cos

n

t −π
2

sin
π
2 n

= 4
π
∞

n=0
1
2n + 1 sin [(2n + 1)t] sin2 
(2n + 1)π
2

,
but sin2[(2n + 1) π
2 ] = 1, so ﬁnally we have
f (t) =
∞

n=−∞
fnejnt = 4
π
∞

n=0
1
2n + 1 sin[(2n + 1)t].
It is important to note that the wavelet series and Fourier series expansions have
something in common, in spite of the fact that they look quite different and indeed
are associated with quite different function spaces. The common feature is that both
representations involve the use of orthogonal basis functions. We are now ready to
consider this in a general manner.
Begin by recalling from elementary linear algebra that a basis for a vector space
such as X = Rn or X = Cn is a set of n vectors, say
B = {e0, e1, . . . , en−1}
(1.58)
such that the elements ek (basis vectors) are linearly independent. This means that
no vector in the set can be expressed as a linear combination of any of the others.
TLFeBOOK

22
FUNCTIONAL ANALYSIS IDEAS
In general, it is not necessary that ⟨ek, en⟩= 0 for n ̸= k. In other words, indepen-
dence does not require orthogonality. However, if set B is a basis (orthogonal or
otherwise) then for any x ∈X (vector space) there exists a set of coefﬁcients from
the ﬁeld of the vector space, say, b = {b0, b1 . . . , bn−1}, such that
x =
n−1

k=0
bkek.
(1.59)
We say that spaces Rn and Cn are of dimension n. This is a direct reference to the
number of basis vectors in B. This notion generalizes.
Now let us consider a sequence space (e.g., l2[0, ∞]). Suppose x =
(x0, x1, x2, . . .) ∈l2[0, ∞]. Deﬁne the following unit vector sequences:
e0 = (1, 0, 0, 0, . . .),
e1 = (0, 1, 0, 0, . . .),
e2 = (0, 0, 1, 0, . . .), etc. (1.60)
Clearly
x =
∞

k=0
xkek.
(1.61)
It is equally clear that no vector ek can be expressed as a linear combination of
any of the others. Thus, the countably inﬁnite set9 B = {e0, e1, e2, . . .} forms a
basis for l2[0, ∞]. The sequence space is therefore of inﬁnite dimension because
B has a countable inﬁnity of members. It is apparent as well that, under the inner
product deﬁned in (1.31), we have ⟨en, em⟩= δn−m. Sequence δ = (δn) is called
the Kr¨onecker delta sequence. It is deﬁned by
δn =
 1,
n = 0
0,
n ̸= 0 .
(1.62)
Therefore, the vectors in (1.60) are mutually orthogonal as well. So they happen to
form an orthogonal basis for l2[0, ∞]. Of course, this is not the only possible basis.
In general, given a countably inﬁnite set of vectors {ek|k ∈Z+} [no longer neces-
sarily those in (1.60)] that are linearly independent, and such that ek ∈l2[0, ∞],
for any x ∈l2[0, ∞] there will exist coefﬁcients ak ∈C such that
x =
∞

k=0
akek.
(1.63)
In view of the above, consider the following linearly independent set of vectors
from some inner product space X:
B = {ek|ek ∈X,
k ∈Z}.
(1.64)
9A set A is countably inﬁnite if its members can be put into one-to-one (1–1) correspondence with
the members of the set Z+. This is also equivalent to being able to place the elements of A into 1–1
correspondence with the elements of Z.
TLFeBOOK

SOME SPECIAL MAPPINGS: METRICS, NORMS, AND INNER PRODUCTS
23
Assume that this is a basis for X. In this case for any x ∈X, there are coefﬁcients
ak such that
x =

k∈Z
akek.
(1.65)
We deﬁne the set B to be orthogonal iff for all n, k ∈Z
⟨en, ek⟩= δn−k.
(1.66)
Assume that the elements of B in (1.64) satisfy (1.66). It is then easy to
see that
⟨x, en⟩=

k
akek, en

=

k
⟨akek, en⟩
(using (I1))
=

k
ak⟨ek, en⟩
(using (I2))
=

k
δk−nak
(using (1.66))
so ﬁnally we may say that
⟨x, en⟩= an.
(1.67)
In other words, if the basis B is orthogonal, then
x =

k∈Z
⟨x, ek⟩ek.
(1.68)
Previous examples (e.g., Fourier series expansion) are merely special cases of this
general idea. We see that one of the main features of an orthogonal basis is the
ease with which we can obtain the coefﬁcients ak. Nonorthogonal bases are harder
to work with in this respect. This is one of the reasons why orthogonal bases are
so universally popular.
A few comments on terminology are in order here. Some would say that the
condition (1.66) on B in (1.64) means that B is an orthonormal set, and we would
say that condition
⟨en, ek⟩= αnδn−k
is the condition for B to be an orthogonal set, where αn is not necessarily unity
(i.e., equal to one) for all n. However, in this book we often insist that orthogonal
basis vectors be “normalized” so condition (1.66) holds.
We conclude the present section by considering the following theorem. It was
mentioned in a footnote that the following Schwarz inequality (or variations of it)
is of very great value in electrical and computer engineering.
TLFeBOOK

24
FUNCTIONAL ANALYSIS IDEAS
Theorem 1.1: Schwarz Inequality
Let X be an inner product space, where
x, y ∈X. Then
|⟨x, y⟩| ≤||x|| ||y||.
(1.69)
Equality holds iff {x, y} is a linearly dependent set.
Proof
If y = 0 then ⟨x, 0⟩= 0, and (1.69) clearly holds in this special case.
Let y ̸= 0. For all scalars α in the ﬁeld of X we must have [via inner product
axioms and (1.30)]
0 ≤||x −αy||2 = ⟨x −αy, x −αy⟩
= ⟨x, x⟩−α∗⟨x, y⟩−α[⟨y, x⟩−α∗⟨y, y⟩].
If we select α∗= ⟨y, x⟩/⟨y, y⟩, then the quantity in the brackets [·] vanishes. Thus
0 ≤⟨x, x⟩−⟨y, x⟩
⟨y, y⟩⟨x, y⟩= ||x||2 −|⟨x, y⟩|2
||y||2
[using ⟨x, y⟩= ⟨y, x⟩∗, i.e., axiom (I3)]. Rearranging, this yields
|⟨x, y⟩|2 ≤||x||2||y||2,
and the result (1.69) follows (we must take positive square roots as ||x|| ≥0, and
|x| ≥0).
Equality holds iff y = 0, or else ||x −αy||2 = 0, hence x −αy = 0 [recall
(N2)], so x = αy, demonstrating linear dependence of x and y.
We may now see what Theorem 1.1 has to say when applied to the special case of
a vector dot product.
Example 1.21
Suppose that X is the inner product space of Example 1.17.
Since
|⟨x, y⟩| =

n−1

k=0
xkyk

and ||x|| =
n−1
k=0 x2
k
1/2
, we have from Theorem 1.1 that

n−1

k=0
xkyk
 ≤
n−1

k=0
x2
k
	1/2 n−1

k=0
y2
k
	1/2
.
(1.70)
If yk = αxk (α ∈R) for all k ∈Zn, then

n−1

k=0
xkyk
 = |α|
n−1

k=0
x2
k,
TLFeBOOK

THE DISCRETE FOURIER SERIES (DFS)
25
and
n−1
k=0 y2
k
1/2
= |α|
n−1
k=0 x2
k
1/2
, hence
n−1

k=0
x2
k
	1/2 n−1

k=0
y2
k
	1/2
= |α|
n−1

k=0
x2
k.
Thus, (1.70) does indeed hold with equality when y = αx.
1.4
THE DISCRETE FOURIER SERIES (DFS)
The subject of discrete Fourier series (DFS) and its relationship to the complex
Fourier series expansion of Section 1.3.3 is often deferred to later courses (e.g.,
signals and systems), but will be brieﬂy considered here as an additional example
of an orthogonal series expansion.
The complex Fourier series expansion of Section 1.3.3 was for 2π-periodic
functions deﬁned on the real-number line. A similar series expansion exists for
N-periodic sequences such as ˜x = (˜xn); that is, for N ∈{2, 3, 4, . . .} ⊂Z, consider
˜xn =

k∈Z
xn+kN
(1.71)
where x = (xn) is such that xn = 0 for n < 0, and for n ≥N as well. Thus, x is
just one period of ˜x. We observe that
˜xn+mN =
∞

k=−∞
xn+mN+kN =
∞

k=−∞
xn+(m+k)N =
∞

r=−∞
xn+rN = ˜xn
(r = m + k). This conﬁrms that ˜x is indeed N-periodic (i.e., periodic with period
N). We normally assume in a context such as this that xn ∈C. We also regard x
as a vector: x = [x0 x1 · · · xN−1]T ∈CN. An inner product may be deﬁned on
the space of N-periodic sequences according to
⟨˜x, ˜y⟩= ⟨x, y⟩= yH x
(1.72)
(recall Example 1.18), where y ∈CN is one period of ˜y. We assume, of course,
that ˜x and ˜y are bounded sequences so that (1.72) is well deﬁned.
Now deﬁne ek = [ek,0 ek,1 · · · ek,N−1]T ∈CN according to
ek,n = exp

j 2π
N kn

,
(1.73)
where n ∈ZN. The periodization of ek = (ek,n) is
˜ek,n =

m∈Z
ek,n+mN
(1.74)
TLFeBOOK

26
FUNCTIONAL ANALYSIS IDEAS
yielding ˜ek = (˜ek,n). That (1.73) is periodic with period N with respect to index n
is easily seen:
ek,n+mN = exp

j 2π
N k(n + mN)

= exp

j 2π
N kn

exp

j2πkm

= ek,n.
It can be shown (by exercise) that [using deﬁnition (1.72)]
⟨˜ek, ˜er⟩= ⟨ek, er⟩=
N−1

n=0
exp

−j 2π
N rn

exp

j 2π
N kn

=
N−1

n=0
exp

j 2π
N (k −r)n

=
 N,
k −r = 0
0,
otherwise
.
(1.75)
Thus, if we consider (ek,n), and (er,n) with k ̸= r we ﬁnd that these sequences are
orthogonal, and so form an orthogonal basis for the vector space CN. From (1.75)
we may write
⟨ek, er⟩= Nδk−r.
(1.76)
Thus, there must exist another vector X = [X0 X1 · · · XN−1]T ∈CN such that
xn = 1
N
N−1

k=0
Xk exp

j 2π
N kn

(1.77)
for n ∈ZN. In fact
⟨x, er⟩=
N−1

n=0
xne∗
r,n
= 1
N
N−1

n=0

N−1

k=0
Xk exp

j 2π
N kn

exp

−j 2π
N rn

= 1
N
N−1

k=0
Xk

N−1

n=0
exp

j 2π
N (k −r)n

= 1
N
N−1

k=0
Xk(Nδk−r) = Xr.
(1.78)
That is
Xk =
N−1

n=0
xn exp

−j 2π
N kn

(1.79)
for k ∈ZN.
TLFeBOOK

THE DISCRETE FOURIER SERIES (DFS)
27
In (1.77) we see xn+mN = xn for all m ∈Z. Thus, (xn) in (1.77) is N-periodic,
and so we have ˜xn = 1
N
N−1
k=0 Xk exp

j 2π
N kn

with Xk given by (1.79). Equation
(1.77) is the discrete Fourier series (DFS) expansion for an N-periodic complex-
valued sequence ˜x such as in (1.71). The DFS coefﬁcients are given by (1.79).
However, it is common practice to consider only ˜xn for n ∈ZN, which is equivalent
to only considering the vector x ∈CN. In this case the vector X ∈CN given by
(1.79) is now called the discrete Fourier transform (DFT) of the vector x, and the
expression in (1.77) is the inverse DFT (IDFT) of the vector X. We observe that
the DFT, and the IDFT can be concisely expressed in matrix form, where we deﬁne
the DFT matrix
F =

exp
 
−j 2π
N kn
!
k,n∈ZN
∈CN×N,
(1.80)
and we see from (1.77) that F −1 = 1
N F ∗(IDFT matrix). Thus, X = Fx. We remark
that the symmetry of F (i.e., F = F T ) means that either k or n in (1.80) may be
interpreted as row or column indices.
The DFT has a long history, and its invention is now attributed to Gauss
[10]. The DFT is of central importance to numerical computing generally, but
has particularly great signiﬁcance in digital signal processing as it represents a
numerical approximation to the Fourier transform, and it can also be used to
efﬁciently implement digital ﬁltering operations via so-called fast Fourier trans-
form (FFT) algorithms. The construction of FFT algorithms to efﬁciently compute
X = Fx (and x = F −1X) is rather involved, and not within the scope of the
present book. Simply note that the direct computation of the matrix-vector product
X = Fx needs N2 complex multiplications and N(N −1) complex additions. For
N = 2p (p ∈{1, 2, 3, . . .}), which is called the radix-2 case, the algorithm of Coo-
ley and Tukey [11] reduces the number of operations to something proportional to
N log2 N, which is a substantial savings compared to N2 operations with the direct
approach when N is large enough. Essentially, the method in Ref. 11 implicitly fac-
tors F according to F = FpFp−1 · · · F1, where the matrix factors Fk ∈CN×N are
sparse (i.e., contain many zero-valued entries). Note that multiplication by zero is
not implemented in either hardware or software and so does not represent a compu-
tational cost in the practical implementation of the FFT algorithm. It is noteworthy
that the algorithm of Ref. 11 also has a long history dating back to the work of
Gauss, as noted by Heideman et al. [10]. It is also important to mention that fast
algorithms exist for all possible N ̸= 2p [4]. The following example suggests one
of the important applications of the DFT/DFS.
Example 1.22
Suppose that xn = Aejθn with θ = 2π
N m for m = 1, 2, . . . ,
N
2 −1 (N is assumed to be even here). From (1.79) using (1.75)
Xk = ANδm−k.
(1.81)
TLFeBOOK

28
FUNCTIONAL ANALYSIS IDEAS
Now suppose instead that xn = Ae−jθn, so similarly
Xk = A
N−1

n=0
exp

−j 2π
N n(m + k)

= A
N−1

n=0
exp

j 2π
N n(N −m −k)

= ANδN−m−k.
(1.82)
Thus, if now xn = A cos(θn) = 1
2A[ejθn + e−jθn], then from (1.81) and (1.82), we
must have
Xk = 1
2AN[δm−k + δN−m−k].
(1.83)
We observe that Xk = 0 for all k ̸= m, N −m, but that
Xm = 1
2AN,
and
XN−m = 1
2AN.
Thus, Xk is nonzero only for indices k = m and k = N −m corresponding to
the frequency of (xn), which is θ = 2π
N m. The DFT/DFS is therefore quite use-
ful in detecting “sinusoids” (also sometimes called “tone detection”). This makes
the DFT/DFS useful in such applications as narrowband radar and sonar signal
detection.
Can you explain the necessity (or, at least, the desirability) of the second equality
in Eq. (1.82)?
APPENDIX 1.A
COMPLEX ARITHMETIC
Here we summarize the most important facts about arithmetic with complex num-
bers z ∈C (set of complex numbers). You shall ﬁnd this material very useful in
electric circuits, as well as in the present book.
Complex numbers may be represented in two ways: (1) Cartesian (rectangular)
form or (2) polar form. First we consider the Cartesian form.
In this case z ∈C has the form z = x + jy, where x, y ∈R (set of real num-
bers), and j =
√
−1. The complex conjugate of z is deﬁned to be z∗= x −jy (so
j∗= −j).
Suppose that z1 = x1 + jy1 and z2 = x2 + jy2 are two complex numbers. Addi-
tion and subtraction are deﬁned as
z1 ± z2 = (x1 ± x2) + j(y1 ± y2)
TLFeBOOK

COMPLEX ARITHMETIC
29
[e.g., (1 + 2j) + (3 −5j) = 4 −3j, and (1 + 2j) −(3 −5j) = −2 + 7j]. Using
j2 = −1, the product of z1 and z2 is
z1z2 = (x1 + jy1)(x2 + jy2)
= x1x2 + j2y1y2 + jy1x2 + jx1y2
= (x1x2 −y1y2) + j(x1y2 + x2y1).
We note that
zz∗= (x + jy)(x −jy) = x2 + y2 = |z|2,
so |z| =

x2 + y2 deﬁnes the magnitude of z. For example, (1 + 2j)(3 −5j) =
13 + j. The quotient of z1 and z2 is deﬁned to be
z1
z2
= z1z∗
2
z2z∗
2
= (x1 + jy1)(x2 −jy2)
x2
2 + y2
2
= (x1x2 + y1y2) + j(x2y1 −x1y2)
x2
2 + y2
2
= x1x2 + y1y2
x2
2 + y2
2
+ j x2y1 −x1y2
x2
2 + y2
2
,
where the last equality is z1/z2 in Cartesian form.
Now we may consider polar form representations. For z = x + jy, we may
regard x and y as the x and y coordinates (respectively) of a point in the Cartesian
plane (sometimes denoted R2).10 We may therefore express these coordinates in
polar form; thus, for any x and y we can write
x = r cos θ,
y = r sin θ,
where r ≥0, and θ ∈[0, 2π), or θ ∈(−π, π]. We observe that
x2 + y2 = r2(cos2 θ + sin2 θ) = r2,
so |z| = r.
Now recall the following Maclaurin series expansions (considered in greater
depth in Chapter 3):
sin x =
∞

n=1
(−1)n−1
x2n−1
(2n −1)!
cos x =
∞

n=1
(−1)n−1
x2n−2
(2n −2)!
ex =
∞

n=1
xn−1
(n −1)!
10This suggests that z may be equivalently represented by the column vector [xy]T . The vector inter-
pretation of complex numbers can be quite useful.
TLFeBOOK

30
FUNCTIONAL ANALYSIS IDEAS
These series converge for −∞< x < ∞. Observe the following:
ejx =
∞

n=1
(jx)n−1
(n −1)! =
∞

n=1

(jx)(2n−1)−1
[(2n −1) −1]! + (jx)(2n−1)
[2n −1]!
	
,
where we have split the summation into terms involving even n and odd n. Thus,
continuing
ejx =
∞

n=1
j2n−2x2n−2
(2n −2)! + j2n−1x2n−1
(2n −1)!

=
∞

n=1
j2n−2

x2n−2
(2n −2)! + j
x2n−1
(2n −1)!

(jj2n−2 = j2n−1)
=
∞

n=1
(−1)n−1
x2n−2
(2n −2)! + j
∞

n=1
(−1)n−1
x2n−1
(2n −1)!
(j2n−2 = (j2)n−1 = (−1)n−1)
= cos x + j sin x.
Thus, ejx = cos x + j sin x. This is justiﬁcation for Euler’s identity in (1.49). Addi-
tionally, since e−jx = cos x −j sin x, we have
ejx + e−jx = 2 cos x,
ejx −e−jx = 2j sin x.
These immediately imply that
sin x = ejx −e−jx
2j
,
cos x = ejx + e−jx
2
.
These identities allow for the conversion of expressions involving trig(onometric)
functions into expressions involving exponentials, and vice versa. The necessity to
do this is frequent. For this reason, they should be memorized, or else you should
remember how to derive them “on the spot” when necessary.
Now observe that
rejθ = r cos θ + jr sin θ,
so that if z = x + jy, then, because there exist r and θ such that x = r cos θ and
y = r sin θ, we may immediately write
z = rejθ.
TLFeBOOK

ELEMENTARY LOGIC
31
This is z in polar form. For example (assuming that θ is in radians)
1 + j =
√
2ejπ/4,
−1 + j =
√
2e3πj/4,
1 −j =
√
2e−jπ/4,
−1 −j =
√
2e−3πj/4.
It can sometimes be useful to observe that
j = ejπ/2,
−j = e−jπ/2,
and
−1 = e±jπ.
If z1 = r1ejθ1, and z2 = r2ejθ2, then
z1z2 = r1r2ej(θ1+θ2),
z1
z2
= r1
r2
ej(θ1−θ2).
In other words, multiplication and division of complex numbers is very easy when
they are expressed in polar form.
Finally, some terminology. For z = x + jy, we call x the real part of z, and we
call y the imaginary part of z. The notation is
x = Re [z],
y = Im [z].
That is, z = Re [z] + j Im [z].
APPENDIX 1.B
ELEMENTARY LOGIC
Here we summarize the basic language and ideas associated with elementary logic
as some of what is found here appears in later sections and chapters of this book.
The concepts found here appear often in mathematics and engineering literature.
Consider two mathematical statements represented as P and Q. Each statement
may be either true or false. Suppose that we know that if P is true, then Q is
certainly true (allowing the possibility that Q is true even if P is false). Then we
say that P implies Q, or Q is implied by P , or P is a sufﬁcient condition for Q,
or symbolically
P ⇒Q
or
Q ⇐P.
Suppose that if P is false, then Q is certainly false (allowing the possibility
that Q may be false even if P is true). Then we say that P is implied by Q, or Q
implies P , or P is a necessary condition for Q, or
P ⇐Q
or
Q ⇒P.
Now suppose that if P is true, then Q is certainly true, and if P is false, then
Q is certainly false. In other words, P and Q are either both true or both false.
Then we say that P implies and is implied by Q, or P is a necessary and sufﬁcient
TLFeBOOK

32
FUNCTIONAL ANALYSIS IDEAS
condition for Q, or P and Q are logically equivalent, or P if and only if Q, or
symbolically
P ⇔Q.
A common abbreviation for “if and only if” is iff.
The logical contrary of the statement P is called “not P .” It is often denoted by
either P or ∼P . This is the statement that is true if P is false, or false if P is true.
For example, if P is the statement “x > 1,” then ∼P is the statement “x ≤1.” If
P is the statement “f (x) ̸= 0 for all x ∈R,” then ∼P is the statement “there is
at least one x ∈R for which f (x) = 0.” We may write
x4 −5x2 + 4 = 0 ⇐x = 1
or
x = 2,
but the converse is not true because x4 −5x2 + 4 = 0 is a quartic equation pos-
sessing four possible solutions. We may write
x = 3 ⇒x2 = 3x,
but we cannot say x2 = 3x ⇒x = 3 because x = 0 is also possible.
Finally, we observe that
P ⇒Q is equivalent to ∼P ⇐∼Q,
P ⇐Q is equivalent to ∼P ⇒∼Q,
P ⇔Q is equivalent to ∼P ⇔∼Q;
that is, taking logical contraries reverses the directions of implication arrows.
REFERENCES
1. E.
Kreyszig,
Introductory Functional Analysis with Applications,
Wiley,
New
York, 1978.
2. A. P. Hillman and G. L. Alexanderson, A First Undergraduate Course in Abstract Alge-
bra, 3rd ed., Wadsworth, Belmont, CA, 1983.
3. R. B. J. T. Allenby, Rings, Fields and Groups: An Introduction to Abstract Algebra,
Edward Arnold, London, UK, 1983.
4. R. E. Blahut, Fast Algorithms for Digital Signal Processing, Addison-Wesley, Reading,
MA, 1985.
5. C. K. Chui, Wavelets: A Mathematical Tool for Signal Analysis. SIAM, Philadelphia,
PA, 1997.
6. R. E. Ziemer and W. H. Tranter, Principles of Communications: Systems, Modulation,
and Noise, 3rd ed., Houghton Mifﬂin, Boston, MA, 1990.
7. I. Daubechies, “Orthonormal Bases of Compactly Supported Wavelets,” Commun. Pure
Appl. Math. 41, 909–996 (1988).
TLFeBOOK

PROBLEMS
33
8. G. P. Tolstov, Fourier Series (transl. from Russian by R. A. Silverman), Dover Publi-
cations, New York, 1962.
9. E. Kreyszig, Advanced Engineering Mathematics, 4th ed., Wiley, New York, 1979.
10. M. T. Heideman, D. H. Johnson and C. S. Burrus, “Gauss and the History of the Fast
Fourier Transform,” IEEE ASSP Mag. 1, 14–21 (Oct. 1984).
11. J. W. Cooley and J. W. Tukey, “An Algorithm for the Machine Calculation of Complex
Fourier Series,” Math. Comput., 19, 297–301 (April 1965).
PROBLEMS
1.1. (a) Find a, b ∈R in
1 + 2j
−3 −j = a + bj.
(b) Find r, θ ∈R in
−3 + j = rejθ
(Of course, choose r > 0, and θ ∈(−π, π].)
1.2. Solve for x ∈C in the quadratic equation
x2 −2r cos θx + r2 = 0.
Here r ≥0, and θ ∈(−π, π]. Express your solution in polar form.
1.3. Let θ, and φ be arbitrary angles (so θ, φ ∈R). Show that
(cos θ + j sin θ)(cos φ + j sin φ) = cos(θ + φ) + j sin(θ + φ).
1.4. Prove the following theorem. Suppose z ∈C such that
z = r cos θ + jr sin θ
for which r = |z| > 0, and θ ∈(−π, π]. Let n ∈{1, 2, 3, . . .} (i.e., n is a
positive integer). The n different nth roots of z are given by
r1/n

cos
 θ + 2πk
n
!
+ j sin
 θ + 2πk
n
!
,
for k = 0, 1, 2, . . ., n −1.
1.5. State whether the following are true or false:
(a) |x| < 2 ⇒x < 2
(b) |x| < 3 ⇐0 < x < 3
TLFeBOOK

34
FUNCTIONAL ANALYSIS IDEAS
(c) x −y > 0 ⇒x > y > 0
(d) xy = 0 ⇒x = 0 and y = 0
(e) x = 10 ⇐x2 = 10x
Explain your answer in all cases.
1.6. Consider the function
f (x) =
 −x2 + 2x + 1,
0 ≤x < 1
x2 −2x + 3
2,
1 < x ≤2 .
Find
sup
x∈[0,2]
f (x),
inf
x∈[0,2] f (x).
1.7. Suppose that we have the following polynomials in the indeterminate x:
a(x) =
n

k=0
akxk,
b(x) =
m

j=0
bjxj.
Prove that
c(x) = a(x)b(x) =
n+m

l=0
clxl,
where
cl =
n

k=0
akbl−k.
[Comment: This is really asking us to prove that discrete convolution is
mathematically equivalent to polynomial multiplication. It explains why the
MATLAB routine for multiplying polynomials is called conv. Discrete con-
volution is a fundamental operation in digital signal processing, and is an
instance of something called ﬁnite impulse response (FIR) ﬁltering. You will
ﬁnd it useful to note that ak = 0 for k < 0, and k > n, and that bj = 0 for
j < 0, and j > m. Knowing this allows you to manipulate the summation
limits to achieve the desired result.]
1.8. Recall Example 1.5. Suppose that xk = 2k+1, and that yk = 1 for k ∈Z+.
Find the sum of the series d(x, y). (Hint: Recall the theory of geometric
series. For example, N
k=0 αk = 1−αN+1
1−α
if α ̸= 1.)
1.9. Prove that if x ̸= 1, then
Sn =
n

k=1
kxk−1
TLFeBOOK

PROBLEMS
35
is given by
Sn = 1 −(n + 1)xn + nxn+1
(1 −x)2
.
What is the formula for Sn when x = 1 ? (Hint: Begin by showing that
Sn −xSn = 1 + x + x2 + · · · + xn−1 −nxn.)
1.10. Recall Example 1.1. Prove that d(x, y) in (1.5) satisﬁes all the axioms for a
metric.
1.11. Recall Example 1.18. Prove that ⟨x, y⟩in (1.36) satisﬁes all the axioms for
an inner product.
1.12. By direct calculation, show that if x, y, z are elements from an inner product
space, then
||z −x||2 + ||z −y||2 = 1
2||x −y||2 + 2||z −1
2(x + y)||2
(Appolonius’ identity).
1.13. Suppose x, y ∈R3 (three-dimensional Euclidean space) such that
x = [1
1
1]T ,
y = [1
−1
1]T .
Find all vectors z ∈R3 such that ⟨x, z⟩= ⟨y, z⟩= 0.
1.14. The complex Fourier series expansion method as described is for f ∈
L2(0, 2π). Find the complex Fourier series expansion for f ∈L2(0, T ),
where 0 < T < ∞(i.e., the interval on which f is deﬁned is now of arbitrary
length).
1.15. Consider again the complex Fourier series expansion for f ∈L2(0, 2π).
Speciﬁcally, consider Eq. (1.44). If f (t) ∈R for all t ∈(0, 2π), then show
that fn = f ∗
−n. [The sequence (fn) is conjugate symmetric.] Use this to show
that for suitable an, bn ∈R (all n) we have
∞

n=−∞
fnejnt = a0 +
∞

n=1
[an cos(nt) + bn sin(nt)].
How are the coefﬁcients an and bn related to fn ? (Be very speciﬁc. There
is a simple formula.)
1.16. (a) Suppose that f ∈L2(0, 2π), and that speciﬁcally
f (t) =
 1,
0 < t < π
j,
π ≤t < 2π
.
Find fn in Eq. (1.44) using (1.45); that is, ﬁnd the complex Fourier series
expansion for f (t). Make sure that you appropriately simplify your series
expansion.
TLFeBOOK

36
FUNCTIONAL ANALYSIS IDEAS
(b) Show how to use the result in Example 1.20 to ﬁnd the complex Fourier
series expansion for f (t) in (a).
1.17. This problem is about ﬁnding the Fourier series expansion for the wave-
form at the output of a full-wave rectiﬁer circuit. This circuit is used in
AC/DC (alternating/direct-current) converters. Knowledge of the Fourier
series expansion gives information to aid in the design of such converters.
(a) Find the complex Fourier series expansion of
f (t) =
sin
 2π
T1
t
! ∈L2
 
0, T1
2
!
.
(b) Find the sequences (an), and (bn) in
f (t) = a0 +
∞

n=1

an cos
 2πn
T
t
!
+ bn sin
 2πn
T
t
!
for f (t) in (a). You need to consider how T is related to T1.
1.18. Recall the deﬁnitions of the Haar scaling function and Haar wavelet in
Eqs. (1.40) and (1.41), respectively. Deﬁne φk,n(t) = 2k/2φ(2kt −n), and
ψk,n(t) = 2k/2ψ(2kt −n). Recall that ⟨f (t), g(t)⟩=
" ∞
−∞f (t)g∗(t) dt is the
inner product for L2(R).
(a) Sketch φk,n(t), and ψk,n(t).
(b) Evaluate the integrals
 ∞
−∞
φ2
k,n(t) dt,
and
 ∞
−∞
ψ2
k,n(t) dt.
(c) Prove that
⟨φk,n(t), φk,m(t)⟩= δn−m.
1.19. Prove the following version of the Schwarz inequality. For all x, y ∈X (inner
product space)
| Re [⟨x, y⟩]| ≤||x|| ||y||
with equality iff y = βx, and β ∈R is a constant.
[Hint: The proof of this one is not quite like that of Theorem 1.1. Consider
⟨αx + y, αx + y⟩≥0 with α ∈R. The inner product is to be viewed as a
quadratic in α.]
1.20. The following result is associated with the proof of the uncertainty principle
for analog signals.
Prove that for f (t) ∈L2(R) such that |t|f (t) ∈L2(R) and f (1)(t) =
df (t)/dt ∈L2(R), we have the inequality
Re
 ∞
−∞
tf (t)[f (1)(t)]∗dt

2
≤
 ∞
−∞
|tf (t)|2 dt
  ∞
−∞
|f (1)(t)|2 dt

.
TLFeBOOK

PROBLEMS
37
1.21. Suppose ek = [ek,0 ek,1 · · · ek,N−2 ek,N−1]T ∈CN, where
ek,n = exp

j 2π
N kn

and k ∈ZN. If x, y ∈CN recall that ⟨x, y⟩= N−1
k=0 xky∗
k . Prove that
⟨ek, er⟩= Nδk−r. Thus, B = {ek|k ∈ZN} is an orthogonal basis for CN.
Set B is important in digital signal processing because it is used to deﬁne
the discrete Fourier transform.
TLFeBOOK

2
Number Representations
2.1
INTRODUCTION
In this chapter we consider how numbers are represented on a computer largely with
respect to the errors that occur when basic arithmetical operations are performed
on them. We are most interested here in so-called rounding errors (also called
roundoff errors). Floating-point computation is emphasized. This is due to the
fact that most numerical computation is performed with ﬂoating-point numbers,
especially when numerical methods are implemented in high-level programming
languages such as C, Pascal, FORTRAN, and C++. However, an understanding
of ﬂoating-point requires some understanding of ﬁxed-point schemes ﬁrst, and so
this case will be considered initially. In addition, ﬁxed-point schemes are used to
represent integer data (i.e., subsets of Z), and so the ﬁxed-point representation is
important in its own right. For example, the exponent in a ﬂoating-point number
is an integer.
The reader is assumed to be familiar with how integers are represented, and
how they are manipulated with digital hardware from a typical introductory dig-
ital electronics book or course. However, if this is not so, then some review of
this topic appears in Appendix 2.A. The reader should study this material now if
necessary.
Our main (historical) reference text for the material of this chapter is Wilkin-
son [1]. However, Golub and Van Loan [4, Section 2.4] is also a good refer-
ence. Golub and Van Loan [4] base their conventions and results in turn on
Forsythe et al. [5].
2.2
FIXED-POINT REPRESENTATIONS
We now consider ﬁxed-point fractions. We must do so because the mantissa in a
ﬂoating-point number is a ﬁxed-point fraction.
We assume that fractions are t + 1 digits long. If the number is in binary, then
we usually say “t + 1 bits” long instead. Suppose, then, that x is a (t + 1)-bit
fraction. We shall write it in the form
(x)2 = x0.x1x2 · · · xt−1xt
(xk ∈{0, 1}).
(2.1)
An Introduction to Numerical Analysis for Electrical and Computer Engineers, by C.J. Zarowski
ISBN 0-471-46737-5
c⃝2004 John Wiley & Sons, Inc.
38
TLFeBOOK

FIXED-POINT REPRESENTATIONS
39
The notation (x)2 means that x is in base-2 (binary) form. More generally, (x)r
means that x is expressed as a base-r number (e.g., if r = 10 this would be the
decimal representation). We use this notation to emphasize which base we are
working with when necessary (e.g., to avoid ambiguity). We shall assume that
(2.1) is a two’s complement fraction. Thus, bit x0 is the sign bit. If this bit is 1,
we interpret the fraction to be negative; otherwise, it is nonnegative. For example,
(1.1011)2 = (−0.3125)10. [To take the two’s complement of (1.1011)2, ﬁrst com-
plement every bit, and then add (0.0001)2. This gives (0.0101)2 = (0.3125)10.] In
general, for the case of a (t + 1)-bit two’s complement fraction, we obtain
−1 ≤x ≤1 −2−t.
(2.2)
In fact
(−1)10 = (1. 00 . . . 00
#
$%
&
t bits
)2,
(1 −2−t)10 = (0. 11 . . . 11
#
$%
&
t bits
)2.
(2.3)
We may regard (2.2) as specifying the dynamic range of the (t + 1)-bit two’s
complement fraction representation scheme. Numbers beyond this range are not
represented. Justiﬁcation of (2.2) [and (2.3)] would follow the argument for the
conversion of two’s complement integers into decimal integers that is considered
in Appendix 2.A.
Consider the set {x ∈R| −1 ≤x ≤1 −2−t}. In other words, x is a real number
within the limits imposed by (2.2), but it is not necessarily equal to a (t + 1)-bit
fraction. For example, x =
√
2 −1 is in the range (2.2), but it is an irrational
number, and so does not possess an exact (t + 1)-bit representation. We may choose
to approximate such a number with t + 1 bits. Denote the (t + 1)-bit approximation
of x as Q[x]. For example, Q[x] might be the approximation to x obtained by
selecting an element from set
B = {bn = −1 + 2−tn|n = 0, 1, . . . , 2t+1 −1} ⊂R
(2.4)
that is the closest to x, where distance is measured by the metric in Example 1.1.
Note that each number in B is representable as a (t + 1)-bit fraction. In fact, B is the
entire set of (t + 1)-bit two’s complement fractions. Formally, our approximation
is given by
Q[x] =
argmin
n∈{0,1,...,2t+1−1}
|x −bn|.
(2.5)
The notation “argmin” means “let Q[x] be the bn for the n in the set
{0, 1, . . . , 2t+1 −1} that minimizes |x −bn|.” In other words, we choose the
argument bn that minimizes the distance to x. Some reﬂection (and perhaps con-
sidering some simple examples for small t) will lead the reader to conclude that
the error in this approximation satisﬁes
|x −Q[x]| ≤2−(t+1).
(2.6)
TLFeBOOK

40
NUMBER REPRESENTATIONS
The error ϵ = x −Q[x] is called quantization error. Equation (2.6) is an upper
bound on the size (norm) of this error. In fact, in the notation of Chapter 1, if
||x|| = |x|, then ||ϵ|| = ||x −Q[x]|| ≤2−(t+1). We remark that our quantization
method is not unique. There are many other methods, and these will generally lead
to different bounds.
When we represent the numbers in a computational problem on a computer,
we see that errors due to quantization can arise even before we perform any oper-
ations on the numbers at all. However, errors will also arise in the course of
performing basic arithmetic operations on the numbers. We consider the sources
of these now.
If x, y are coded as in (2.1), then their sum might not be in the range speciﬁed
by (2.2). This can happen only if x and y are either both positive or both negative.
Such a condition is ﬁxed-point overﬂow. (A test for overﬂow in two’s complement
integer addition appears in Appendix 2.A, and it is easy to modify it for the problem
of overﬂow testing in the addition of fractions.) Similarly, overﬂow can occur when
a negative number is subtracted from a positive number, or if a positive number
is subtracted from a negative number. A test for this case is possible, too, but we
omit the details. Other than the problem of overﬂow, no errors can occur in the
addition or subtraction of fractions.
With respect to fractions, rounding error arises only when we perform multipli-
cation and division. We now consider errors in these operations.
We will deal with multiplication ﬁrst. Suppose that x and y are represented
according to (2.1). Suppose also that x0 = y0 = 0. It is easy to see that the product
of x and y is given by
p = xy =
' t
k=0
xk2−k
( ' t
n=0
yn2−n
(
= (x0 + x12−1 + · · · + xt2−t)(y0 + y12−1 + · · · + yt2−t)
= x0y0 + (x0y1 + x1y0)2−1 + · · · + xtyt2−2t.
(2.7)
This implies that the product is a (2t + 1)-bit number. If we allow x and y to be
either positive or negative, then the product will also be 2t + 1 bits long. Of course,
one of these bits is the sign bit. If we had to multiply several numbers together,
we see that the product wordsize would grow in some proportion to the number of
factors in the product. The growth is clearly very rapid, and no practical computer
could sustain this for very long. We are therefore forced in general to round off the
product p back down to a number that is only t + 1 bits long. Obviously, this will
introduce an error.
How should the rounding be done? There is more than one possibility (just as
there is more than one way to quantize). Wilkinson [1, p. 4] suggests the following.
Since the product p has the form
(p)2 = p0.p1p2 · · · pt−1ptpt+1 · · · p2t
(pk ∈{0, 1})
(2.8)
TLFeBOOK

FIXED-POINT REPRESENTATIONS
41
we may add 2−(t+1) to this product, and then simply discard the last t bits of the
resulting sum (i.e., the bits indexed t + 1 to 2t). For example, suppose t = 4, and
consider
0.00111111 = p
+0.00001000 = 2−5
0.01000111
Thus, the rounded product is (0.0100)2. The error involved in rounding in this
manner is not higher in magnitude than 1
22−t = 2−(t+1). Deﬁne the result of the
rounding operation to be f x[p] = f x[xy], so then
|p −f x[p]| ≤1
22−t.
(2.9)
[For the previous example, p = (0.00111111)2, and so f x[p] = (0.0100)2.] It is
natural to measure the sizes of errors in the same way as we measured the size of
quantization errors earlier. Thus, (2.9) is an upper bound on the size of the error
due to rounding a product. As with quantization, other rounding methods would
generally give other bounds. We remark that Wilkinson’s suggestion amounts to
“ordinary rounding.”
Finally, we consider ﬁxed-point division. Again, suppose that x and y are rep-
resented as in (2.1), and consider the quotient q = x/y. Obviously, we must avoid
y = 0. Also, the quotient will not be in the permitted range given by (2.2) unless
|y| ≥|x|. This implies that when ﬁxed-point division is implemented either the
dividend x or the divisor y need to be scaled to meet this restriction. Scaling is
multiplication by a power of 2, and so should be implemented to reduce rounding
error. We do not consider the speciﬁcs of how to achieve this. Another problem
is that x/y may require an inﬁnite number of bits to represent it. For example,
suppose
q = (0.0010)2
(0.0110)2
= (0.125)10
(0.375)10
=
 1
3
!
10
= (0.01)2.
The bar over 01 denotes the fact that this pattern repeats indeﬁnitely. Fortunately,
the same recipe for the rounding of products considered above may also be used
to round quotients. If f x[q] again denotes the result of applying this procedure to
q, then
|q −f x[q]| ≤1
22−t.
(2.10)
We see that the difﬁculties associated with division in ﬁxed-point representations
means that ﬁxed-point arithmetic should, if possible, not be used to implement
algorithms that require division. This forces us to either (1) employ ﬂoating-point
representations or (2) develop algorithms that solve the problem without the need
for division operations.
Both strategies are employed in practice. Usually choice 1 is easier.
TLFeBOOK

42
NUMBER REPRESENTATIONS
2.3
FLOATING-POINT REPRESENTATIONS
In the previous section we have seen that ﬁxed-point numbers are of very limited
dynamic range. This poses a major problem in employing them in engineering
computations since obviously we desire to work with numbers far beyond the
range in (2.2). Floating-point representations provide the deﬁnitive solution to this
problem. We remark (in passing) that the basic organization of a ﬂoating-point
arithmetic unit [i.e., digital hardware for ﬂoating-point addition and subtraction
appears in Ref. 2 (see pp. 295–306)]. There is a standard IEEE format for ﬂoating-
point numbers. We do not consider this standard here, but it is summarized in
Ref. 2 (see pp. 304–306). Some of the technical subtleties associated with the
IEEE standard are considered by Higham [6].
Following Golub and Van Loan [4, p. 61], the set F (subset of R) of ﬂoating-
point numbers consists of numbers of the form
x = x0.x1x2 · · · xt−1xt × re,
(2.11)
where x0 is a sign bit (which means that we can replace x0 by ±; this is done in
Ref. 4), and r is the base of the representation [typically r = 2 (binary), or r = 10
(decimal); we will emphasize r = 2]. Therefore, xk ∈{0, 1, . . . , r −2, r −1} for
1 ≤k ≤t. These are the digits (bits if r = 2) of the mantissa. We therefore see
that the mantissa is a fraction. 1 It is important to note that x1 ̸= 0, and this has
implications with regard to how operations are performed and the resulting rounding
errors. We call e the exponent. This is an integer quantity such that L ≤e ≤U.
For example, we might represent e as an n-bit two’s complement integer. We will
assume this unless otherwise speciﬁed in what follows. This would imply that
(e)2 = en−1en−2 · · · e1e0, and so
−2n−1 ≤e ≤2n−1 −1
(2.12)
(see Appendix A for justiﬁcation). For nonzero x ∈F, then
m ≤|x| ≤M,
(2.13a)
where
m = rL−1,
M = rU(1 −r−t).
(2.13b)
Equation (2.13) gives the dynamic range for the ﬂoating-point representation. With
r = 2 we see that the total wordsize for the ﬂoating-point number is t + n + 1 bits.
In the absence of rounding errors in a computation, our numbers may initially
be from the set
G = {x ∈R|m ≤|x| ≤M} ∪{0}.
(2.14)
1Including the sign bit the mantissa is (for r = 2) t + 1 bits long. Frequently in what follows we shall
refer to it as being only t bits long. This is because we are ignoring the sign bit, which is always
understood to be present.
TLFeBOOK

FLOATING-POINT REPRESENTATIONS
43
This set is analogous to the set {x ∈R| −1 ≤x ≤1 −2−t} that we saw in the
previous section in our study of ﬁxed-point quantization effects. Again following
Golub and Van Loan [4], we may deﬁne a mapping (operator) f l|G →F. Here
c = f l[x] (x ∈G) is obtained by choosing the closest c ∈F to x. As you might
expect, distance is measured using || · || = | · |, as we did in the previous section.
Golub and Van Loan call this rounded arithmetic [4], and it coincides with the
rounding procedure described by Wilkinson [1, pp. 7–11].
Suppose that x and y are two ﬂoating-point numbers (i.e., elements of F) and
that “op” denotes any of the four basic arithmetic operations (addition, subtrac-
tion, multiplication, or division). Suppose |x op y| /∈G. This implies that either
|x op y| > M (ﬂoating-point overﬂow), or 0 < |x op y| < m (ﬂoating-point under-
ﬂow) has occurred. Under normal circumstances an arithmetic fault such as over-
ﬂow will not happen unless an unstable procedure is being performed. The issue
of “numerical stability” will be considered later. Overﬂows typically cause runtime
error messages to appear. The underﬂow arithmetic fault occurs when a number
arises that is not zero, but is too small to represent in the set F. This usually poses
less of a problem than overﬂow. 2 However, as noted before, we are concerned
mainly with rounding errors here. If |x op y| ∈G, then we assume that the com-
puter implementation of x op y will be given by f l[x op y]. In other words, the
operator f l models rounding effects in ﬂoating-point arithmetic operations. We
remark that where ﬂoating-point arithmetic is concerned, rounding error arises in
all four arithmetic operations. This contrasts with ﬁxed-point arithmetic wherein
rounding errors arise only in multiplication and division.
It turns out that for the ﬂoating-point rounding procedure suggested above
f l[x op y] = (x op y)(1 + ϵ),
(2.15)
where
|ϵ| ≤1
2r1−t(= 2−t
if
r = 2).
(2.16)
We shall justify this only for the case r = 2. Our arguments will follow those of
Wilkinson [1, pp. 7–11].
Let us now consider the addition of the base-2 ﬂoating-point numbers
x = x0.x1 · · · xt
#
$%
&
=mx
×2ex
(2.17a)
and
y = y0.y1 · · · yt
#
$%
&
=my
×2ey,
(2.17b)
and we assume that |x| > |y|. (If instead |y| > |x|, then reverse the roles of x and
y.) If ex −ey > t, then
f l[x + y] = x.
(2.18)
2Underﬂows are simply set to zero on some machines.
TLFeBOOK

44
NUMBER REPRESENTATIONS
For example, if t = 4, and x = 0.1001 × 24, and y = 0.1110 × 2−1, then to add
these numbers, we must shift the bits in the mantissa of one of them so that both
have the same exponent. If we choose y (usually shifting is performed on the
smallest number), then y = 0.00000111 × 24. Therefore, x + y = 0.10010111 ×
24, but then f l[x + y] = 0.1001 × 24 = x.
Now if instead we have ex −ey ≤t, we divide y by 2ex−ey by shifting its man-
tissa ex −ey positions to the right. The sum x + 2ey−exy is then calculated exactly,
and requires ≤2t bits for its representation. The sum is multiplied by a power of 2,
using left or right shifts to ensure that the mantissa is properly normalized [recall
that for x in (2.11) we must have x1 ̸= 0]. Of course, the exponent must be modi-
ﬁed to account for the shift of the bits in the mantissa. The 2t-bit mantissa is then
rounded off to t bits using f l. Because we have |mx| + 2ey−ex|my| ≤1 + 1 = 2,
the largest possible right shift is by one bit position. However, a left shift of up to t
bit positions might be needed because of the cancellation of bits in the summation
process. Let us consider a few examples. We will assume that t = 4.
Example 2.1
Let x = 0.1001 × 24, and y = 0.1010 × 21. Thus
0.10010000 × 24
+0.00010100 × 24
0.10100100 × 24
and the sum is rounded to 0.1010 × 24 (computed sum).
Example 2.2
Let x = 0.1111 × 24, and y = 0.1010 × 22. Thus
0.11110000 × 24
+0.00101000 × 24
1.00011000 × 24
but 1.00011000 × 24 = 0.100011000 × 25, and this exact sum is rounded to 0.1001
× 25 (computed sum).
Example 2.3
Let x = 0.1111 × 2−4, and y = −.1110 × 2−4. Thus
0.11110000 × 2−4
−0.11100000 × 2−4
0.00010000 × 2−4
but 0.00010000 × 2−4 = 0.1000 × 2−7, and this exact sum is rounded to 0.1000 ×
2−7 (computed sum). Here there is much cancellation of the bits leading in turn
to a large shift of the mantissa of the exact sum to the left. Yet, the computed sum
is exact.
TLFeBOOK

FLOATING-POINT REPRESENTATIONS
45
We observe that the computed sum is obtained by computing the exact sum,
normalizing it so that the mantissa s0.s1 · · · st−1stst+1 · · · s2t satisﬁes s1 = 1 (i.e.,
s1 ̸= 0), and then we round it to t places (i.e., we apply f l). If the normalized
exact sum is s = ms × 2es(= x + y), then the rounding error ϵ′ is such that |ϵ′| ≤
1
22−t2es. Essentially, the error ϵ′ is due to rounding the mantissa (a ﬁxed-point
number) according to the method used in Section 2.2. Because of the form of ms,
1
22es ≤|s| < 2es, and so
f l[x + y] = (x + y)(1 + ϵ)
(2.19)
which is just a special case of (2.15). This expression requires further explanation,
however. Observe that
|s −f l[s]|
|s|
= |s −(s + ϵ′)|
|s|
= |ϵ′|
|s| ≤
1
22−t2es
|s|
which is the relative error3 due to rounding. Because we have 1
22es ≤|s| < 2es,
this error is biggest when |s| = 1
22es, so therefore we conclude that
|s −f l[s]|
|s|
≤2−t.
(2.20)
From (2.19) f l[s] = s + sϵ, so that |s −f l[s]| = |s||ϵ|, or |ϵ| = |s −f l[s]|/|s|.
Thus, |ϵ| ≤2−t, which is (2.16). In other words, |ϵ′| is the absolute error, and |ϵ|
is the relative error.
Finally, if x = 0 or y = 0 then no rounding error occurs: ϵ = 0. Subtraction
results do not differ from addition.
Now consider computing the product of x and y in (2.17). Since x = mx × 2ex,
and y = my × 2ey with x1 ̸= 0, and y1 ̸= 0 we must have
1
2
1
2 ≤|mxmy| < 1.
(2.21)
This implies that it may be necessary to normalize the mantissa of the product with
a shift to the left, and an appropriate adjustment of the exponent as well. The 2t-bit
mantissa of the product is rounded to give a t-bit mantissa. If x = 0, or y = 0 (or
both x and y are zero), then the product is zero.
3In general, if a is the exact value of some quantity and ˆa is some approximation to a, the absolute
error is ||a −ˆa||, while the relative error is
||a −ˆa||
||a||
(a ̸= 0).
The relative error is usually more meaningful in practice. This is because an error is really “big” or
“small” only in relation to the size of the quantity being approximated.
TLFeBOOK

46
NUMBER REPRESENTATIONS
We may consider a few examples. We will suppose t = 4. Begin with x =
0.1010 × 22, and y = 0.1111 × 21, so then
xy = 0.10010110 × 23,
and so f l[xy] = 0.1001 × 23 (computed product). If now x = 0.1000 × 24, y =
0.1000 × 2−1, then, before normalizing the mantissa, we have
xy = 0.01000000 × 23,
and after normalization we have
xy = 0.10000000 × 22
so that f l[xy] = 0.1000 × 22 (computed product). Finally, suppose that x =
0.1010 × 20, and y = 0.1010 × 20, so then the unnormalized product is
xy = 0.01100100 × 20
for which the normalized product is
xy = 0.11001000 × 2−1,
so ﬁnally f l[xy] = 0.1101 × 2−1 (computed product).
The application of f l to the normalized product will have exactly the same
effect as it did in the case of addition (or of subtraction). This may be under-
stood by recognizing that a 2t-bit mantissa will “look the same” to operator f l
regardless of how that mantissa was obtained. It therefore immediately follows
that
f l[xy] = (xy)(1 + ϵ),
(2.22)
which is another special case of (2.15), and |ϵ| ≤2−t, which is (2.16) again.
Now consider the quotient x/y, for x and y ̸= 0 in (2.17),
q = x
y = mx × 2ex
my × 2ey = mx
my
× 2ex−ey = mq × 2eq
(2.23)
(so mq = mx/my, and eq = ex −ey). The arithmetic unit in the machine has an
accumulator that we assume contains mx and which is “double length” in that it
is 2t bits long. Speciﬁcally, this accumulator initially stores x0.x1 · · · xt 0 · · · 0
# $% &
t bits
. If
|mx| > |my| the number in the accumulator is shifted one place to the right, and
so eq is increased by one (i.e., incremented). The number in the accumulator is
then divided by my in such a manner as to give a correctly rounded t-bit result.
This implies that the computed mantissa of the quotient, say, mq = q0.q1 · · · qt,
TLFeBOOK

FLOATING-POINT REPRESENTATIONS
47
satisﬁes the normalization condition q1 = 1, so that 1
2 ≤|mq| < 1. Once again we
must have
f l
x
y

= x
y (1 + ϵ)
(2.24)
such that |ϵ| ≤2−t. Therefore, (2.15) and (2.16) are now justiﬁed for all instances
of op.
We complete this section with a few examples. Suppose x = 0.1010 × 22, and
y = 0.1100 × 2−2, then
q = x
y = 0.1010 × 22
0.1100 × 2−2 = 0.10100000 × 22
0.1100 × 2−2
= 0.10100000
0.1100
× 24 = 0.11010101 × 24
so that f l[q] = 0.1101 × 24 (computed quotient). Now suppose that x = 0.1110 ×
23, and y = 0.1001 × 2−2, and so
q = x
y = 0.1110 × 23
0.1001 × 2−2 = 0.01110000 × 24
0.1001 × 2−2
= 0.01110000
0.1001
× 26 = 0.11000111 × 26
so that f l[q] = 0.1100 × 26 (computed quotient).
Thus far we have emphasized ordinary rounding, but an alternative implemen-
tation of f l is to use chopping. If x = ±
)∞
k=1 xk2−k*
× 2e, then, for chopping
operator f l, we have f l[x] = ±
)t
k=1 xk2−k*
× 2e (chopping x to t + 1 bits
including the sign bit). Thus, the absolute error is
|ϵ′| = |x −f l[x]| =


∞

k=t+1
xk2−k

2e ≤2e
∞

k=t+1
2−k
(as xk = 1 for all k > t), but since ∞
k=t+1 2−k = 2−t, we must have
|ϵ′| = |x −f l[x]| ≤2−t2e,
and so the relative error for chopping is
|ϵ| = |x −f l[x]|
|x|
≤2−tee
1
22e
= 2−t+1
(because we recall that |x| ≥1
22e). We see that the error in chopping is somewhat
bigger than the error in rounding, but chopping is somewhat easier to implement.
TLFeBOOK

48
NUMBER REPRESENTATIONS
2.4
ROUNDING EFFECTS IN DOT PRODUCT COMPUTATION
Suppose x, y ∈Rn. We recall from Chapter 1 (and from elementary linear algebra)
that the vector dot product is given by
⟨x, y⟩= xT y = yT x =
n−1

k=0
xkyk.
(2.25)
This operation occurs in matrix–vector product computation (e.g., y = Ax, where
A ∈Rn×n), digital ﬁlter implementation (i.e., computing discrete-time convolu-
tion), numerical integration, and other applications. In other words, it is so common
that it is important to understand how rounding errors can affect the accuracy of a
computed dot product.
We may regard dot product computation as a recursive process. Thus
sn−1 =
n−1

k=0
xkyk =
n−2

k=0
xkyk + xn−1yn−1 = sn−2 + xn−1yn−1.
So
sk = sk−1 + xkyk
(2.26)
for k = 0, 1, . . . , n −1, and s−1 = 0. Each arithmetic operation in (2.26) is a
separate ﬂoating-point operation and so introduces its own error into the over-
all calculation. We would like to obtain a general expression for this error. To
begin, we may model the computation process according to
ˆs0 = f l[x0y0]
ˆs1 = f l[ˆs0 + f l[x1y1]]
ˆs2 = f l[ˆs1 + f l[x2y2]]
...
ˆsn−2 = f l[ˆsn−3 + f l[xn−2yn−2]]
ˆsn−1 = f l[ˆsn−2 + f l[xn−1yn−1]].
(2.27)
From (2.15) we may write
ˆs0 = (x0y0)(1 + δ0)
ˆs1 = [ˆs0 + (x1y1)(1 + δ1)](1 + ϵ1)
ˆs2 = [ˆs1 + (x2y2)(1 + δ2)](1 + ϵ2)
...
TLFeBOOK

ROUNDING EFFECTS IN DOT PRODUCT COMPUTATION
49
ˆsn−2 = [ˆsn−3 + (xn−2yn−2)(1 + δn−2)](1 + ϵn−2)
ˆsn−1 = [ˆsn−2 + (xn−1yn−1)(1 + δn−1)](1 + ϵn−1),
(2.28)
where |δk| ≤2−t (for k = 0, 1, . . . , n −1), and |ϵk| ≤2−t (for k = 1, 2, . . . ,
n −1), via (2.16). It is possible to write4
ˆsn−1 =
n−1

k=0
xkyk(1 + γk) = sn−1 +
n−1

k=0
xkykγk,
(2.29)
where
1 + γk = (1 + δk)
n−1
/
j=k
(1 + ϵj)(ϵ0 = 0).
(2.30)
Note that the 
 notation means, for example
n
/
k=0
xk = x0x1x2 · · · xn−1xn,
(2.31)
where 
 is the symbol to compute the product of all xk for k = 0, 1, . . . , n. The
similarity to how we interpret  notation should therefore be clear.
The absolute value operator is a norm on R, so from the axioms for a norm
(recall Deﬁnition 1.3), we must have
|sn−1 −ˆsn−1| = |xT y −f l[xT y]| ≤
n−1

k=0
|xkyk||γk|.
(2.32)
In particular, obtaining this involves the repeated use of the triangle inequality.
Equation (2.32) thus represents an upper bound on the absolute error involved in
computing a vector dot product. Of course, the notation f l[xT y] symbolizes the
ﬂoating-point approximation to the exact quantity xT y. However, the bound in
(2.32) is incomplete because we need to appropriately bound the numbers γk.
To obtain the bound we wish involves using the following lemma.
Lemma 2.1: We have
1 + x ≤ex,
x ≥0
(2.33a)
ex ≤1 + 1.01x,
0 ≤x ≤.01.
(2.33b)
4Equation (2.29) is most easily arrived at by considering examples for small n, for instance
ˆs3 = x0y0(1 + δ0)(1 + ϵ0)(1 + ϵ1)(1 + ϵ2)(1 + ϵ3) + x1y1(1 + δ1)(1 + ϵ1)(1 + ϵ2)(1 + ϵ3)
+ x2y2(1 + δ2)(1 + ϵ2)(1 + ϵ3) + x3y3(1 + δ3)(1 + ϵ3),
and using such examples to “spot the pattern.”
TLFeBOOK

50
NUMBER REPRESENTATIONS
Proof
Begin with consideration of (2.33a). Recall that for −∞< x < ∞
ex =
∞

n=0
xn
n! .
(2.34)
Therefore
ex = 1 + x +
∞

n=2
xn
n!
so that
1 + x = ex −
∞

n=2
xn
n! ,
but the terms in the summation are all nonnegative, so (2.33a) follows immediately.
Now consider (2.33b), which is certainly valid for x = 0. The result will follow
if we prove
ex −1
x
≤1.01
(x ̸= 0).
From (2.34)
ex −1
x
=
∞

m=0
xm
(m + 1)! = 1 +
∞

m=1
xm
(m + 1)!
so we may also equivalently prove instead that
∞

m=1
xm
(m + 1)! ≤0.01
for 0 < x ≤0.01. Observe that
∞

m=1
xm
(m + 1)! = 1
2x + 1
6x2 + 1
24x3 + · · · ≤1
2x + x2 + x3 + x4 + · · ·
= 1
2x +
∞

k=2
xk = 1
2x +
∞

k=0
xk −1 −x
=
1
1 −x −1
2x −1 = 1
2x 1 + x
1 −x .
It is not hard to verify that
1
2x 1 + x
1 −x ≤0.01
for 0 < x ≤0.01. Thus, (2.33b) follows.
TLFeBOOK

ROUNDING EFFECTS IN DOT PRODUCT COMPUTATION
51
If n = 1, 2, 3, . . ., and if 0 ≤nu ≤0.01, then
(1 + u)n ≤(eu)n
[via (2.33a)]
≤1 + 1.01nu
[via (2.33b)].
(2.35)
Now if |δi| ≤u for i = 0, 1, . . . , n −1 then
n−1
/
i=0
(1 + δi) ≤
n−1
/
i=0
(1 + |δi|) ≤(1 + u)n
so via (2.35)
n−1
/
i=0
(1 + δi) ≤1 + 1.01nu,
(2.36)
where we must emphasize that 0 ≤nu ≤0.01. Certainly there is a δ such that
1 + δ =
n−1
/
i=0
(1 + δi),
(2.37)
and so from (2.36), |δ| ≤1.01nu. If we identify γk with δ in (2.33) for all k, then
|γk| ≤1.01nu
(2.38)
for which we consider u = 2−t [because in (2.30) both |ϵi| and |δi| ≤2−t]. Using
(2.38) in (2.32), we obtain
|xT y −f l[xT y]| ≤1.01nu
n−1

k=0
|xkyk|,
(2.39)
but n−1
k=0 |xkyk| = n−1
k=0 |xk||yk|, and this may be symbolized as |x|T |y| (so that
|x| = [|x0||x1| · · · |xn−1|]T ). Thus, we may rewrite (2.39) as
|xT y −f l[xT y]| ≤1.01nu|x|T |y|.
(2.40)
Observe that the relative error satisﬁes
|xT y −f l[xT y]|
|xT y|
≤1.01nu|x|T |y|
|xT y| .
(2.41)
The bound in (2.41) may be quite large if |x|T |y| ≫|xT y|. This suggests the
possibility of a large relative error. We remark that since u = 2−t, nu ≤0.01 will
hold in all practical cases unless n is very large (a typical value for t is t = 56).
The potentially large relative errors indicated by the analysis we have just made
are a consequence of the details of how the dot product was calculated. As noted
on p. 65 of Ref. 4, the use of a double-precision accumulator to compute the dot
TLFeBOOK

52
NUMBER REPRESENTATIONS
product can reduce the error dramatically. Essentially, if x and y are ﬂoating-
point vectors with t-bit mantissas, the “running sum” sk [of (2.26)] is built up in
an accumulator with a 2t-bit mantissa. Multiplication of two t-bit numbers can
be stored exactly in a double-precision variable. The large dynamic ﬂoating-point
range limits the likelihood of overﬂow/underﬂow. Only when ﬁnal sum sn−1 is
written to a single-precision memory location will there be a rounding error. It
therefore follows that when this alternative procedure is employed, we get
f l[xT y] = xT y(1 + δ)
(2.42)
for which |δ| ≈2−t (= u). Clearly, this is a big improvement.
The material of this section shows
1. The analysis required to obtain insightful bounds on errors can be quite
arduous.
2. Proper numerical technique can have a dramatic effect in reducing errors.
3. Proper technique can be revealed by analysis.
The following example illustrates how the bound on rounding error in dot prod-
uct computation may be employed.
Example 2.4
Assume the existence of a square root function such that
f l[√x] = √x(1 + ϵ) and |ϵ| ≤u. We use the algorithm that corresponds to the
bound of Eq. (2.40) to compute xT x (x ∈Rn), and then use this to give an algo-
rithm for ||x|| =
√
xT x. This can be expressed in the form of pseudocode:
s−1 := 0;
for k := 0 to n −1 do begin
sk := sk−1 + x2
k;
end;
||x|| := √sn−1;
We will now obtain a bound on the relative error due to rounding in the computation
of ||x||. We will use the fact that
√
1 + x ≤1 + x (for x ≥0).
Now
ϵ1 = f l[xT x] −xT x
xT x
⇒f l[xT x] = xT x(1 + ϵ1),
and via (2.41)
|ϵ1| ≤1.01nu|x|T |x|
|xT x|
= 1.01nu||x||2
||x||2 = 1.01nu
(|x|T |x| = n−1
k=0 |xk|2 = n−1
k=0 x2
k = ||x||2, and |||x||2| = ||x||2). So in “short-
hand” notation, f l[

f l[xT x]] ≡f l[||x||], and
f l[||x||] =

xT x

1 + ϵ1(1 + ϵ) = ||x||

1 + ϵ1(1 + ϵ),
TLFeBOOK

MACHINE EPSILON
53
and √1 + ϵ1 ≤1 + ϵ1, so
f l[||x||] ≤||x||(1 + ϵ1)(1 + ϵ).
Now (1 + ϵ1)(1 + ϵ) = 1 + ϵ1 + ϵ + ϵ1ϵ, implying that
||x||(1 + ϵ1)(1 + ϵ) = ||x|| + ||x||(ϵ1 + ϵ + ϵ1ϵ)
so therefore
f l[||x||] ≤||x|| + ||x||(ϵ1 + ϵ + ϵ1ϵ),
and thus

f l[||x||] −||x||
||x||
 ≤|ϵ1 + ϵ + ϵ1ϵ| ≤u + 1.01nu + 1.01nu2
= u[1 + 1.01n + 1.01nu].
Of course, we have used the fact that |ϵ| ≤u.
2.5
MACHINE EPSILON
In Section 2.3 upper bounds on the error involved in applying the operator f l were
derived. Speciﬁcally, we found that the relative error satisﬁes
|ϵ′| = |x −f l[x]|
|x|
≤
 2−t
(rounding)
2−t+1
(chopping) .
(2.43)
As suggested in Section 2.4, these bounds are often denoted by u; that is, u = 2−t
for rounding, and u = 2−t+1 for chopping. The bound u is often called the unit
roundoff [4, Section 2.4.2].
The details of how ﬂoating-point arithmetic is implemented on any given com-
puting machine may not be known or readily determined by the user. Thus, u
may not be known. However, an “experimental” approach is possible. One may
run a simple program to “estimate” u, and the estimate is the machine epsilon,
denoted ϵM. The machine epsilon is deﬁned to be the difference between 1.0 and
the next biggest ﬂoating-point number [6, Section 2.1]. Consequently, ϵM = 2−t+1.
A pseudocode to compute ϵM is as follows:
stop := 1;
eps := 1.0;
while stop == 1 do begin
eps := eps/2.0;
x := 1.0 + eps;
if x ≤1.0
begin
TLFeBOOK

54
NUMBER REPRESENTATIONS
stop := 0;
end;
end;
eps := 2.0 ∗eps;
This code may be readily implemented as a MATLAB routine. MATLAB stores
eps (= ϵM) as a built-in constant, and the reader may wish to test the code above
to see if the result agrees with MATLAB eps (as a programming exercise).
In this book we shall (unless otherwise stated) regard machine epsilon and unit
roundoff as practically interchangeable.
APPENDIX 2.A
REVIEW OF BINARY NUMBER CODES
This appendix summarizes typical methods used to represent integers in binary.
Extension of the results in this appendix to fractions is certainly possible. This
material is normally to be found in introductory digital electronics books. The
reader is here assumed to know Boolean algebra. This implies that the reader
knows that + can represent either algebraic addition, or the logical or operation.
Similarly, xy might mean the logical and of the Boolean variables x and y, or it
might mean the arithmetic product of the real variables x and y. The context must
be considered to ascertain which meaning applies.
Below we speak of “complements.” These are used to represent negative inte-
gers, and also to facilitate arithmetic with integers. We remark that the results of
this appendix are presented in a fairly general manner. Thus, the reader may wish,
for instance, to see numerical examples of arithmetic using two’s complement (2’s
comp.) codings. The reader can consult pp. 276–280 of Ref. 2 for such examples.
Almost any other books on digital logic will also provide a source of numerical
examples [3].
We may typically interpret a bit pattern in one of four ways, assuming that the
bit pattern is to represent a number (negative or nonnegative integer). An example
of this is as follows, and it provides a summary of common representations (e.g.,
for n = 3 bits):
Bit Pattern
Unsigned Integer
2’s Comp.
1’s Comp.
Sign Magnitude
0
0
0
0
0
0
0
0
0
1
1
1
1
1
0
1
0
2
2
2
2
0
1
1
3
3
3
3
1
0
0
4
−4
−3
−0
1
0
1
5
−3
−2
−1
1
1
0
6
−2
−1
−2
1
1
1
7
−1
−0
−3
TLFeBOOK

REVIEW OF BINARY NUMBER CODES
55
In the four coding schemes summarized in this table, the interpretation of the bit
pattern is always the same when the most signiﬁcant bit (MSB) is zero. A similar
table for n = 4 appears in Hamacher et al. [2, see p. 271].
Note that, philosophically speaking, the table above implies that a bit pattern
can have more than one meaning. It is up to the engineer to decide what meaning
it should have. Of course, this will be a function of purpose. Presently, our purpose
is that bit patterns should have meaning with respect to the problems of numerical
computing; that is, bit patterns must represent numerical information.
The relative merits of the three signed number coding schemes illustrated in the
table above may be summarized as follows:
Coding Scheme
Advantages
Disadvantages
2’s complement
Simple adder/subtracter
circuit
Only one code for 0
Circuit for ﬁnding the 2’s comp.
more complex than circuit for
ﬁnding the 1’s comp.
1’s complement
Easy to obtain the 1’s comp.
of a number
Circuit for addition and
subtraction more complex
than for the 2’s comp.
adder/subtracter
Two codes for 0
Sign magnitude
Intuitively obvious code
Has the most complex
adder/subtracter circuit
Two codes for 0
The following is a summary of some formulas associated with arithmetic (i.e.,
addition and subtraction) with r’s and (r −1)’s complements. In binary arithmetic
r = 2, while in decimal arithmetic r = 10. We emphasize the case r = 2.
Let A be an n-digit base-r number (integer)
A = An−1An−2 · · · A1A0
where Ak ∈{0, 1, . . . , r −2, r −1}. Digit An−1 is the most signiﬁcant digit
(MSD), while digit A0 is the least signiﬁcant digit (LSD). Provided that A is
not negative (i.e., is unsigned), we recognize that to convert A to a base-10 repre-
sentation (i.e., ordinary decimal number) requires us to compute
n−1

k=0
Akrk.
If A is allowed to be a negative integer, the usage of this summation needs modi-
ﬁcation. This is considered below.
The r’s complement of A is deﬁned to be
r’s complement of A = A∗=
 rn −A,
A ̸= 0
0,
A = 0
(2.A.1)
TLFeBOOK

56
NUMBER REPRESENTATIONS
The (r −1)’s complement of A is deﬁned to be
(r −1)’s complement of A = A = (rn −1) −A
(2.A.2)
It is important not to confuse the bar over the A in (2.A.2) with the Boolean not
operation, although for the special case of r = 2 the bar will denote complemen-
tation of each bit of A; that is, for r = 2
A = An−1An−2 · · · A1A0
where the bar now denotes the logical not operation. More generally, if A is a
base-r number
A = (r −1) −An−1
(r −1) −An−2
· · · (r −1) −A1
(r −1) −A0
Thus, to obtain A, each digit of A is subtracted from r −1. As a consequence,
comparing (2.A.1) and (2.A.2), we see that
A∗= A + 1
(2.A.3)
where the plus denotes algebraic addition (which takes place in base r).
Now we consider the three (previously noted) different methods for coding
integers when r = 2:
1. Sign-magnitude coding
2. One’s complement coding
3. Two’s complement coding
In all three of these coding schemes the most signiﬁcant bit (MSB) is the sign bit.
Speciﬁcally , if An−1 = 0, the number is nonnegative, and if An−1 = 1, the number
is negative. It can be shown that when the complement (either one’s or two’s) of
a binary number is taken, this is equivalent to placing a minus sign in front of the
number. As a consequence, when given a binary number A = An−1An−2 · · · A1A0
coded according to one of these three schemes, we may convert that number to a
base-10 integer according to the following formulas:
1. Sign-Magnitude Coding. The sign-magnitude binary number A = An−1An−2
· · · A1A0 (Ak ∈{0, 1}) has the base-10 equivalent
A =



n−2

i=0
Ai2i,
An−1 = 0
−
n−2

i=0
Ai2i,
An−1 = 1
(2.A.4)
TLFeBOOK

REVIEW OF BINARY NUMBER CODES
57
With this coding scheme there are two codings for zero:
(0)10 = (000 · · · 00
#
$%
&
n
)2 = (100 · · · 00
#
$%
&
n
)2
2. One’s Complement Coding. In this coding we represent −A as A. The one’s
complement binary number A = An−1An−2 · · · A1A0 (Ak ∈{0, 1}) has the
base-10 equivalent
A =



n−2

i=0
Ai2i
, An−1 = 0
−
n−2

i=0
Ai2i
, An−1 = 1
(2.A.5)
With this coding scheme there are also two codes for zero:
(0)10 = (000 · · · 00
#
$%
&
n
)2 = (111 · · · 11
#
$%
&
n
)2
3. Two’s Complement Coding. In this coding we represent −A as A∗(= A + 1).
The two’s complement binary number A = An−1An−2 · · · A1A0 (Ak ∈{0, 1})
has the base-10 equivalent
A = −2n−1An−1 +
n−2

i=0
Ai2i
(2.A.6)
The proof is as follows. If An−1 = 0, then A ≥0 and immediately the base-10
equivalent is A = n−2
i=0 Ai2i (via the procedure for converting a number in
base-2 to one in base-10), which is (2.A.6) for An−1 = 0. Now, if An−1 = 1,
then A < 0, and so if we take the two’s complement of A we must get |A|:
|A| = A + 1
= (1 −An−1)(1 −An−2) · · · (1 −A1)(1 −A0) + 00 · · · 01
#
$%
&
n
=
n−1

i=0
(1 −Ai)2i + 1
= 2n−1(1 −An−1) +
n−2

i=0
(1 −Ai)2i + 1
=
n−2

i=0
2i + 1 −
n−2

i=0
Ai2i(An−1 = 1)
TLFeBOOK

58
NUMBER REPRESENTATIONS
= 1 −2n−1
1 −2
+ 1 −
n−2

i=0
Ai2i
'
via
n

i=0
ai = 1 −an+1
1 −a
(
= 2n−1 −
n−2

i=0
Ai2i
and so A = −2n−1 + n−2
i=0 Ai2i, which is (2.A.6) for An−1 = 1.
In this coding scheme there is only one code for zero:
(0)10 = (000 · · · 00
#
$%
&
n
)2
When n-bit integers are added together, there is the possibility that the sum may
not ﬁt in n bits. This is overﬂow. The condition is easy to detect by monitoring
the signs of the operands and the sum. Suppose that x and y are n-bit two’s
complement coded integers, so that the sign bits of these operands are xn−1 and
yn−1. Suppose that the sum is denoted by s, implying that the sign bit is sn−1. The
Boolean function that tests for overﬂow of s = x + y (algebraic sum of x and y) is
T = xn−1yn−1sn−1 + xn−1yn−1sn−1.
The ﬁrst term will be logical 1 if the operands are negative while the sum is
positive. The second term will be logical 1 if the operands are positive but the sum
is negative. Either condition yields T = 1, thus indicating an overﬂow. A similar
test may be obtained for subtraction, but we omit this here.
The following is both the procedure and the justiﬁcation of the procedure for
adding two’s complement coded integers.
Theorem 2.A.1: Two’s Complement Addition
If A and B are n-bit two’s
complement coded numbers, then compute A + B (the sum of A and B) as though
they were unsigned numbers, discarding any carryout.
Proof
Suppose that A > 0, B > 0; then A + B will generate no carryout from
the bit position n −1 since An−1 = Bn−1 = 0 (i.e., the sign bits are zero-valued),
and the result will be correct if A + B < 2n−1. (If this inequality is not satisﬁed,
then the sign bit will be one, indicating a negative answer, which is wrong. This
amounts to an overﬂow.)
Suppose that A ≥B > 0; then
A + (−B) = A + B∗= A + 2n −B = 2n + A −B,
and if we discard the carryout, this is equivalent to subtracting 2n (because the
carryout has a weight of 2n). Doing this yields A + (−B) = A −B.
TLFeBOOK

PROBLEMS
59
Similarly
(−A) + B = A∗+ B = 2n −A + B = 2n + B −A,
and discarding the carry out yields (−A) + B = B −A.
Again, suppose that A ≥B > 0, then
(−A) + (−B) = A∗+ B∗= 2n −A + 2n −B = 2n + [2n −(A + B)]
= 2n + (A + B)∗
so discarding the carryout gives (−A) + (−B) = (A + B)∗, which is the desired
two’s complement representation of −(A + B), provided A + B ≤2n−1. (If this
latter inequality is not satisﬁed, then we have an overﬂow.)
The procedure for subtraction (and its justiﬁcation) follows similarly. We omit
these details.
REFERENCES
1. J. H. Wilkinson, Rounding Errors in Algebraic Processes, Prentice-Hall, Englewood
Cliffs, NJ, 1963.
2. V. C. Hamacher, Z
Vranesic, and S. G. Zaky, Computer Organization, 3rd ed.,
McGraw-Hill, New York, 1990.
3. J. F. Wakerly, Digital Design Principles and Practices, Prentice-Hall, Englewood Cliffs,
NJ, 1990.
4. G. H. Golub and C. F. Van Loan, Matrix Computations, 2nd ed., Johns Hopkins Univ.
Press, Baltimore, MD, 1989.
5. G. E. Forsythe, M. A. Malcolm, and C. B. Moler, Computer Methods for Mathematical
Computations, Prentice-Hall, Englewood Cliffs, NJ, 1977.
6. N. J. Higham, Accuracy and Stability of Numerical Algorithms. SIAM, Philadelphia, PA,
1996.
PROBLEMS
2.1. Let f x[x] denote the operation of reducing x (a ﬁxed-point binary fraction)
to t + 1 bits (including the sign bit) according to the Wilkinson rounding
(ordinary rounding) procedure in Section 2.2. Suppose that a = (0.1000)2,
b = (0.1001)2, and c = (0.0101)2, so t = 4 here. In arithmetic of unlimited
precision, we always have a(b + c) = ab + ac. Suppose that a practical com-
puting machine applies the operator f x[·] after every arithmetic operation.
(a) Find x = f x[f x[ab] + f x[ac]].
(b) Find y = f x[af x[b + c]].
Do you obtain x = y?
TLFeBOOK

60
NUMBER REPRESENTATIONS
This problem shows that the order of operations in an algorithm implemented
on a practical computer can affect the answer obtained.
2.2. Recall from Section 2.2 that
q =

1
3

10 = (0.01)2.
Find the absolute error in representing q as a (t + 1)-bit binary number. Find
the relative error. Assume both ordinary rounding and chopping (deﬁned at
the end of Section 2.3 with respect to ﬂoating-point arithmetic).
2.3. Recall that we deﬁne a ﬂoating-point number in base r to have the form
x = x0.x1x2 · · · xt−1xt
#
$%
&
=f
× re,
where x0 ∈{+, −} (sign digit), xk ∈{0, 1, . . . , r −1} for k = 1, 2, . . . , t, e
is the exponent (a signed integer), and x1 ̸= 0 (so r−1 ≤|f | < 1) if x ̸= 0.
Show that for x ̸= 0
m ≤|x| ≤M,
where for L ≤e ≤U, we have
m = rL−1,
M = rU(1 −r−t).
2.4. Suppose r = 10. We may consider the result of a decimal arithmetic operation
in the ﬂoating-point representation to be
x = ±
' ∞

k=1
xk10−k
(
× 10e.
(a) If f l[x] is the operator for chopping, then
f l[x] = (±.x1x2 · · · xt−1xt) × 10e,
thus, all digits xk for k > t are forced to zero.
(b) If f l[x] is the operator for rounding then it is deﬁned as follows. Add
0.00 · · · 01
#
$%
&
t+1 digits
to the mantissa if xt+1 ≥5, but if xt+1 < 5, the mantissa is
unchanged. Then all digits xk for k > t are forced to zero.
Show that the absolute error for chopping satisﬁes the upper bound
|x −f l[x]| ≤10−t10e,
and that the absolute error for rounding satisﬁes the upper bound
|x −f l[x]| ≤1
210−t10e.
TLFeBOOK

PROBLEMS
61
Show that the relative errors satisfy
|ϵ| = |x −f l[x]|
|x|
≤
 101−t
(chopping)
1
2101−t
(rounding)
.
2.5. Suppose that t = 4 and r = 2 (i.e., we are working with ﬂoating-point binary
numbers). Suppose that we have the operands
x = 0.1011 × 10−3,
y = −0.1101 × 102.
Find x + y, x −y, and xy. Clearly show the steps involved.
2.6. Suppose that A ∈Rn×n, x ∈Rn, and that f l[Ax] represents the result
of computing the product Ax on a ﬂoating-point computer. Deﬁne |A| =
[|ai,j|]i,j=0,1,...,n−1, and |x| = [|x0||x1| · · · |xn−1|]T . We have
f l[Ax] = Ax + e,
where e ∈Rn is the error vector. Of course, e models the rounding errors
involved in the actual computation of product Ax on the computer. Justify
the bound
|e| ≤1.01nu|A||x|.
2.7. Explain why a conditional test such as
if x ̸= y then begin
f := f/(x −y);
end;
is unreliable.
(Hint: Think about dynamic range limitations in ﬂoating-point arithmetic.)
2.8. Suppose
that
x = [x0x1 · · · xn−1]T
is
a
real-valued
vector,
||x||∞=
max0≤k≤n−1 |xk|, and that we wish to compute ||x||2 =
n−1
k=0 x2
k
1/2
.
Explain the advantages, and disadvantages of the following algorithm with
respect to computational efﬁciency (number of arithmetic operations, and
comparisons), and dynamic range limitations in ﬂoating-point arithmetic:
m := ||x||∞;
s := 0;
for k := 0 to n −1 do begin
s := s + (xk/m)2;
end;
||x||2 := m√s;
Comments regarding computational efﬁciency may be made with respect to
the pseudocode algorithm in Example 2.4.
TLFeBOOK

62
NUMBER REPRESENTATIONS
2.9. Recall that for x2 + bx + c = 0, the roots are
x1 = −b +
√
b2 −4c
2
,
x2 = −b −
√
b2 −4c
2
.
If b = −0.3001, c = 0.00006, then the “exact” roots for this set of parame-
ters are
x1 = 0.29989993,
x2 = 2.0006673 × 10−4.
Let us compute the roots using four-digit (i.e., t = 4) decimal (i.e., r = 10)
ﬂoating-point arithmetic, where, as a result of rounding quantization b, and
c are replaced with their approximations
b = −0.3001 = b,
c = 0.0001 ̸= c.
Compute x2, which is the approximation to x2 obtained using b and c in
place of b and c. Show that the relative error is

x2 −x2
x2
 ≈0.75
(i.e., the relative error is about 75%). (Comment: This is an example of
catastrophic cancellation.)
2.10. Suppose a, b ∈R, and x = a −b. Floating-point approximations to a and b
are ˆa = f l[a] = a(1 + ϵa) and ˆb = f l[b] = b(1 + ϵb), respectively. Hence
the ﬂoating-point approximation to x is ˆx = ˆa −ˆb. Show that the relative
error is of the form
|ϵ| =

x −ˆx
x
 ≤α |a| + |b|
|a −b| .
What is α? When is |ϵ| large?
2.11. For a ̸= 0, the quadratic equation ax2 + bx + c = 0 has roots given by
x1 = −b +
√
b2 −4ac
2a
,
x2 = −b −
√
b2 −4ac
2a
.
For c ̸= 0, quadratic equation cx2 + bx + a = 0 has roots given by
x′
1 = −b +
√
b2 −4ac
2c
, x′
2 = −b −
√
b2 −4ac
2c
.
(a) Show that x1x′
2 = 1 and x2x′
1 = 1.
(b) Using the result from Problem 2.10, explain accuracy problems that can
arise in computing either x1 or x2 when b2 ≫|4ac|. Can you use the
result in part (a) to alleviate the problem? Explain.
TLFeBOOK

3
Sequences and Series
3.1
INTRODUCTION
Sequences and series have a major role to play in computational methods. In this
chapter we consider various types of sequences and series, especially with respect
to their convergence behavior. A series might converge “mathematically,” and yet
it might not converge “numerically” (i.e., when implemented on a computer). Some
of the causes of difﬁculties such as this will be considered here, along with possible
remedies.
3.2
CAUCHY SEQUENCES AND COMPLETE SPACES
It was noted in the introduction to Chapter 1 that many computational processes
are “iterative” (the Newton–Raphson method for ﬁnding the roots of an equation,
iterative methods for linear system solution, etc.). The practical effect of this is to
produce sequences of elements from function spaces. The sequence produced by the
iterative computation is only useful if it converges. We must therefore investigate
what this means.
In Chapter 1 it was possible for sequences to be either singly or doubly inﬁnite.
Here we shall assume sequences are singly inﬁnite unless speciﬁcally stated to the
contrary.
We begin with the following (standard) deﬁnition taken from Kreyszig
[1, pp. 25–26]. Examples of applications of the deﬁnitions to follow will be con-
sidered later.
Deﬁnition 3.1: Convergence of a Sequence, Limit
A sequence (xn) in a
metric space X = (X, d) is said to converge, or to be convergent iff there is an
x ∈X such that
lim
n→∞d(xn, x) = 0.
(3.1)
The element x is called the limit of (xn) (i.e., limit of the sequence), and we may
state that
lim
n→∞xn = x.
(3.2)
An Introduction to Numerical Analysis for Electrical and Computer Engineers, by C.J. Zarowski
ISBN 0-471-46737-5
c⃝2004 John Wiley & Sons, Inc.
63
TLFeBOOK

64
SEQUENCES AND SERIES
We say that (xn) converges to x or has a limit x. If (xn) is not convergent, then
we say that it is a divergent sequence, or is simply divergent.
A shorthand expression for (3.2) is to write xn →x. We observe that sequence (xn)
is deﬁned to converge (or not) with respect to a particular metric here denoted d
(recall the axioms for a metric space from Chapter 1). We remark that it is possible
that, for some (xn) in some set X, the sequence might converge with respect to
one metric on the set, but might not converge with respect to another choice of
metric. It must be emphasized that the limit x must be an element of X in order
for the sequence to be convergent.
Suppose, for example, that X = (0, 1] ⊂R, and consider the sequence xn =
1
n+1(n ∈Z+). Suppose also that d(x, y) = |x −y|. The sequence (xn) does not
converge in X because the sequence “wants to go to 0.” But 0 is not in X. So the
sequence does not converge. (Of course, the sequence converges in X = R with
respect to our present choice of metric.)
It can be difﬁcult in practice to ascertain whether a particular sequence con-
verges according to Deﬁnition 3.1. This is because the limit x may not be known
in advance. In fact, this is almost always the case in computing applications of
sequences. Sometimes it is therefore easier to work with the following:
Deﬁnition 3.2: Cauchy Sequence, Complete Space
A sequence (xn) in a
metric space X = (X, d) is called a Cauchy sequence iff for all ϵ > 0 there is an
N(ϵ) ∈Z+ such that
d(xm, xn) < ϵ
(3.3)
for all m, n > N(ϵ). The space X is a complete space iff every Cauchy sequence
in X converges.
We often write N instead of N(ϵ), because N may depend on our choice of ϵ. It
is possible to prove that any convergent sequence is also Cauchy.
We remark that, if in fact the limit is known (or at least strongly suspected),
then applying Deﬁnition 3.1 may actually be easier than applying Deﬁnition 3.2.
We see that under Deﬁnition 3.2 the elements of a Cauchy sequence get closer to
each other as n and m increase. Establishing the “Cauchiness” of a sequence does
not require knowing the limit of the sequence. This, at least in principle, simpliﬁes
matters. However, a big problem with this deﬁnition is that there are metric spaces
X in which not all Cauchy sequences converge. In other words, there are incomplete
metric spaces. For example, the space X = (0, 1] with d(x, y) = |x −y| is not
complete. Recall that we considered xn = 1/(n + 1). This sequence is Cauchy,1
but the limit is 0, which is not in X. Thus, this sequence is a nonconvergent Cauchy
sequence. Thus, the space (X, | · |) is not complete.
1We see that
d(xm, xn) =

1
m + 1 −
1
n + 1
 ≤

n −m
nm
 =

1
m −1
n
 .
TLFeBOOK

CAUCHY SEQUENCES AND COMPLETE SPACES
65
A more subtle example of an incomplete metric space is the following. Recall
space C[a, b] from Example 1.4. Assume that a = 0 and b = 1, and now choose
the metric to be
d(x, y) =
 1
0
|x(t) −y(t)| dt
(3.4)
instead of Eq. (1.8). Space C[0, 1] with the metric (3.4) is not complete. This
may be shown by considering the sequence of continuous functions illustrated
in Fig. 3.1. The functions xm(t) in Fig. 3.1a form a Cauchy sequence. (Here we
assume m ≥1, and is an integer.) This is because d(xm, xn) is the area of the
triangle in Fig. 3.1b, and for any ϵ > 0, we have
d(xm, xn) < ϵ
whenever m, n > 1/(2ϵ). (Suppose n ≥m and consider that d(xm, xn) = 1
2( 1
m −
1
n) ≤
1
2m < ϵ.) We may see that this Cauchy sequence does not converge. Observe
that we have
xm(t) = 0
for
t ∈[0, 1
2],
xm(t) = 1
for
t ∈[am, 1],
1
1
am
xm
0
(a)
(b)
1
m
t
1
2
1
1
am
xn
xm
0
1
m
1
n
t
1
2
Figure 3.1
A Cauchy sequence of functions in C[0, 1].
For any ϵ > 0 we may ﬁnd N(ϵ) > 0 such that for n ≥m > N(ϵ)
1
m −1
n < ϵ.
If n < m, the roles of n and m may be reversed. The conditions of Deﬁnition 3.2 are met and so the
sequence is Cauchy.
TLFeBOOK

66
SEQUENCES AND SERIES
where am = 1
2 + 1
m. Therefore, for all x ∈C[0, 1],
d(xm, x) =
 1
0
|xm(t) −x(t)| dt
=
 1/2
0
|x(t)| dt +
 am
1/2
|xm(t) −x(t)| dt +
 1
am
|1 −x(t)| dt.
The integrands are all nonnegative, and so each of the integrals on the right-hand
side are nonnegative, too. Thus, to say that d(xm, x) →0 implies that each integral
approaches zero. Since x(t) is continuous, it must be the case that
x(t) = 0
for
t ∈[0, 1
2),
x(t) = 1
for
t ∈( 1
2, 1].
However, this is not possible for a continuous function. In other words, we have
a contradiction. Hence, (xn) does not converge (i.e., has no limit in X = C[0, 1]).
Again, we have a Cauchy sequence that does not converge, and so C[0, 1] with
the metric (3.4) is not complete.
This example also shows that a sequence of continuous functions may very well
possess a discontinuous limit. Actually, we have seen this phenomenon before.
Recall the example of the Fourier series in Chapter 1 (see Example 1.20). In this
case the series representation of the square wave was made up of terms that are all
continuous functions. Yet the series converges to a discontinuous limit. We shall
return to this issue again later.
So now, some metric spaces are not complete. This means that even though a
sequence is Cauchy, there is no guarantee of convergence. We are therefore faced
with the following questions:
1. What metric spaces are complete?
2. Can they be “completed” if they are not?
The answer to the second question is “Yes.” Given an incomplete metric space,
it is always possible to complete it. We have seen that a Cauchy sequence does
not converge when the sequence tends toward a limit that does not belong to the
space; thus, in a sense, the space “has holes in it.” Completion is the process of
ﬁlling in the holes. This amounts to adding the appropriate elements to the set that
made up the incomplete space. However, in general, this is a technically difﬁcult
process to implement in many cases, and so we will never do this. This is a job
normally left to mathematicians.
We will therefore content ourselves with answering the ﬁrst question. This will
be done simply by listing complete metric spaces that are useful to engineers:
1. Sets R and C with the metric d(x, y) = |x −y| are complete metric spaces.
TLFeBOOK

CAUCHY SEQUENCES AND COMPLETE SPACES
67
2. Recall Example 1.3. The space l∞[0, ∞] with the metric
d(x, y) = sup
k∈Z+ |xk −yk|
(3.5)
is a complete metric space. (A proof of this claim appears in Ref. 1, p. 34.)
3. The Euclidean space Rn and the unitary space Cn both with metric
d(x, y) =
n−1

k=0
|xk −yk|2
	1/2
(3.6)
are complete metric spaces. (Proof is on p. 33 of Ref. 1.)
4. Recall Example 1.6. Fixing p, the space lp[0, ∞] such that 1 ≤p < ∞is a
complete metric space. Here we recall that the metric is
d(x, y) =
 ∞

k=0
|xk −yk|p
	1/p
.
(3.7)
5. Recall Example 1.4. The set C[a, b] with the metric
d(x, y) = sup
t∈[a,b]
|x(t) −y(t)|
(3.8)
is a complete metric space. (Proof is on pp. 36–37 of Ref. 1.)
The last example is interesting because the special case C[0, 1] with metric (3.4)
was previously shown to be incomplete. Keeping the same set but changing the
metric from that in (3.4) to that in (3.8) changes the situation dramatically.
In Chapter 1 we remarked on the importance of the metric space L2[a, b] (recall
Example 1.7). The space is important as the “space of ﬁnite energy signals on the
interval [a, b].” (A “ﬁnite power” interpretation was also possible.) An important
special case of this was L2(R) = L2(−∞, ∞). Are these metric spaces complete?
Our notation implicitly assumes that the set (1.11a) (Chapter 1) contains the so-
called Lebesgue integrable functions on [a, b]. In this case the space L2[a, b] is
indeed complete with respect to the metric
d(x, y) =
 b
a
|x(t) −y(t)|2 dt
1/2
.
(3.9)
Lebesgue integrable functions2 have a complicated mathematical structure, and we
have promised to avoid any measure theory in this book. It is enough for the reader
2One of the “simplest” introductions to these is Rudin [2]. However, these functions appear in the
last chapter [2, Chapter 11]. Knowledge of much of the previous chapters is prerequisite to studying
Chapter 11. Thus, the effort required to learn measure theory is substantial.
TLFeBOOK

68
SEQUENCES AND SERIES
to assume that the functions in L2[a, b] are the familiar ones from elementary
calculus.3
The complete metric spaces considered in the two previous paragraphs also
happen to be normed spaces; recall Section 1.3.2. This is because the metrics are
all induced by suitable norms on the spaces. It therefore follows that these spaces
are complete normed spaces. Complete normed spaces are called Banach spaces.
Some of the complete normed spaces are also inner product spaces. Again, this
follows because in those cases an inner product is deﬁned that induced the norm.
Complete inner product spaces are called Hilbert spaces. To be more speciﬁc, the
following spaces are Hilbert spaces:
1. The Euclidean space Rn and the unitary space Cn along with the inner product
⟨x, y⟩=
n−1

k=0
xky∗
k
(3.10)
are both Hilbert spaces.
2. The space L2[a, b] with the inner product
⟨x, y⟩=
 b
a
x(t)y∗(t) dt
(3.11)
is a Hilbert space. [This includes the special case L2(R).]
3. The space l2[0, ∞] with the inner product
⟨x, y⟩=
∞

k=0
xky∗
k
(3.12)
is a Hilbert space.
We emphasize that (3.10) induces the metric (3.6), (3.11) induces the metric (3.9),
and (3.12) induces the metric (3.7) (but only for case p = 2; recall from Chapter 1
that lp[0, ∞] is not an inner product space when p ̸= 2). The three Hilbert spaces
listed above are particularly important because of the fact, in part, that elements in
these spaces have (as we have already noted) either ﬁnite energy or ﬁnite power
interpretations. Additionally, least-squares problems are best posed and solved
within these spaces. This will be considered later.
Deﬁne the set (of natural numbers) N = {1, 2, 3, . . .}. We have seen that sequences
of continuous functions may have a discontinuous limit. An extreme example of this
phenomenon is from p. 145 of Rudin [2].
3These “familiar” functions are called Riemann integrable functions. These functions form a proper
subset of the Lebesgue integrable functions.
TLFeBOOK

CAUCHY SEQUENCES AND COMPLETE SPACES
69
Example 3.1
For n ∈N deﬁne
xn(t) = lim
m→∞[cos(n!πt)]2m.
When n!t is an integer, then xn(t) = 1 (simply because cos(πk) = ±1 for k ∈Z).
For all other values of t, we must have xn(t) = 0 (simply because | cos t| < 1 when
t is not an integral multiple of π). Deﬁne
x(t) = lim
n→∞xn(t).
If t is irrational, then xn(t) = 0 for all n. Suppose that t is rational; that is, suppose
t = p/q for which p, q ∈Z. In this case n!t is an integer when n ≥q in which
case x(t) = 1. Consequently, we may conclude that
x(t) = lim
n→∞lim
m→∞[cos(n!πt)]2m =
 0,
t is irrational
1,
t is rational
.
(3.13)
We have mentioned (in footnote 3, above) that Riemann integrable functions are a
proper subset of the Lebesgue integrable functions. It turns out that x(t) in (3.13)
is Lebesgue integrable, but not Riemann integrable. In other words, you cannot use
elementary calculus to ﬁnd the integral of x(t) in (3.13). Of course, x(t) is a very
strange function. This is typical; that is, functions that are not Riemann integrable
are usually rather strange, and so are not commonly encountered (by the engineer).
It therefore follows that we do not need to worry much about the more general
class of Lebesgue integrable functions.
Limiting processes are potentially dangerous. This is illustrated by a very simple
example.
Example 3.2
Suppose n, m ∈N. Deﬁne
xm,n =
m
m + n.
(This is a double sequence. In Chapter 1 we saw that these arise routinely in wavelet
theory.) Treating n as a ﬁxed constant, we obtain
lim
m→∞xm,n = 1
so
lim
n→∞lim
m→∞xm,n = 1.
Now instead treat m as a ﬁxed constant so that
lim
n→∞xm,n = 0
TLFeBOOK

70
SEQUENCES AND SERIES
which in turn implies that
lim
m→∞lim
n→∞xm,n = 0.
Interchanging the order of the limits has given two completely different answers.
Interchanging the order of limits clearly must be done with great care.
The following example is simply another illustration of how to apply Deﬁni-
tion 3.2.
Example 3.3
Deﬁne
xn = 1 + (−1)n
n + 1
(n ∈Z+).
This is a sequence in the metric space (R, | · |). This space is complete, so we need
not know the limit of the sequence to determine whether it converges (although
we might guess that the limit is x = 1). We see that
d(xm, xn) =

(−1)m
m + 1 −(−1)n
n + 1
 ≤
1
m + 1 +
1
n + 1 ≤1
m + 1
n,
where the triangle inequality has been used. If we assume [without loss of generality
(commonly abbreviated w.l.o.g.)] that n ≥m > N(ϵ) then
1
m + 1
n ≤2
m < ϵ.
So, for a given ϵ > 0, we select n ≥m > 2/ϵ. The sequence is Cauchy, and so it
must converge.
We close this section with mention of Appendix 3.A. Think of the material in it
as being a very big applications example. This appendix presents an introduction
to coordinate rotation digital computing (CORDIC). This is an application of a
particular class of Cauchy sequence (called a discrete basis) to the problem of
performing certain elementary operations (e.g., vector rotation, computing sines
and cosines). The method is used in application-speciﬁc integrated circuits (ASICs),
gate arrays, and has been used in pocket calculators. Note that Appendix 3.A also
illustrates a useful series expansion which is expressed in terms of the discrete basis.
3.3
POINTWISE CONVERGENCE AND UNIFORM CONVERGENCE
The previous section informed us that sequences can converge in different ways,
assuming that they converge in any sense at all. We explore this issue further here.
TLFeBOOK

POINTWISE CONVERGENCE AND UNIFORM CONVERGENCE
71
Deﬁnition 3.3: Pointwise Convergence
Suppose that (xn(t)) (n ∈Z+) is a
sequence of functions for which t ∈S ⊂R. We say that the sequence converges
pointwise iff there is an x(t) (t ∈S) so that for all ϵ > 0 there is an N = N(ϵ, t)
such that
|xn(t) −x(t)| ≤ϵ
(3.14)
for n ≥N. We call x the limit of (xn) and write
x(t) = lim
n→∞xn(t)
(t ∈S).
(3.15)
We emphasize that under this deﬁnition N may depend on both ϵ and t. We may
contrast Deﬁnition 3.3 with the following deﬁnition.
Deﬁnition 3.4: Uniform Convergence
Suppose that (xn(t)) (n ∈Z+) is a
sequence of functions for which t ∈S ⊂R. We say that the sequence converges
uniformly iff there is an x(t) (t ∈S) so that for all ϵ > 0 there is an N = N(ϵ)
such that
|xn(t) −x(t)| ≤ϵ
(3.16)
for n ≥N. We call x the limit of (xn) and write
x(t) = lim
n→∞xn(t)
(t ∈S).
(3.17)
We emphasize that under this deﬁnition N never depends on t, although it may
depend on ϵ. It is apparent that a uniformly convergent sequence is also point-
wise convergent. However, the converse is not true; that is, a pointwise convergent
sequence is not necessarily uniformly convergent. This distinction is important in
understanding the convergence behavior of series as well as of sequences. In par-
ticular, it helps in understanding convergence phenomena in Fourier (and wavelet)
series expansions.
In contrast with the deﬁnitions of Section 3.2, under Deﬁnitions 3.3 and 3.4 the
elements of (xn) and the limit x need not reside in the same function space. In
fact, we do not ask what function spaces they belong to at all. In other words, the
deﬁnitions of this section represent a different approach to convergence analysis.
As with the Deﬁnition 3.1, direct application of Deﬁnitions 3.3 and 3.4 can be
quite difﬁcult since the limit, assuming it exists, is not often known in advance
(i.e., a priori) in practice. Therefore, we would hope for a convergence criterion
similar to the idea of Cauchy convergence in Section 3.2 (Deﬁnition 3.2). In fact,
we have the following theorem (from Rudin [2, pp. 147–148]).
Theorem 3.1: The sequence of functions (xn) deﬁned on S ⊂R converges
uniformly on S iff for all ϵ > 0 there is an N such that
|xm(t) −xn(t)| ≤ϵ
(3.18)
for all n, m ≥N.
TLFeBOOK

72
SEQUENCES AND SERIES
This is certainly analogous to the Cauchy criterion seen earlier. (We omit the
proof.)
Example 3.4
Suppose that (xn) is deﬁned according to
xn(t) =
1
nt + 1,
t ∈(0, 1) and n ∈N.
A sketch of xn(t) for various n appears in Fig. 3.2. We see that (“by inspection”)
xn →0. But consider for all ϵ > 0
|xn(t) −0| =
1
nt + 1 ≤ϵ
which implies that we must have
n ≥1
t
 1
ϵ −1
!
= N
so that N is a function of both t and ϵ. Convergence is therefore pointwise, and is
not uniform.
Other criteria for uniform convergence may be established. For example, there is
the following theorem (again from Rudin [2, p. 148]).
Theorem 3.2: Suppose that
lim
n→∞xn(t) = x(t)
(t ∈S).
Deﬁne
Mn = sup
t∈S
|xn(t) −x(t)|.
Then xn →x uniformly on S iff Mn →0 as n →∞.
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.2
0.4
0.6
0.8
1
t
xn (t)
n = 1
n = 2
n = 20
Figure 3.2
A plot of typical sequence elements for Example 3.4; here, t ∈[0.01, 0.99].
TLFeBOOK

FOURIER SERIES
73
−10
−8
−6
−4
−2
0
2
4
6
8
10
−0.5
0
0.5
t
n = 1
n = 2
n = 10
xn (t)
Figure 3.3
A plot of typical sequence elements for Example 3.5.
The proof is really an immediate consequence of Deﬁnition 3.4, and so is omitted
here.
Example 3.5
Suppose that
xn(t) =
t
1 + nt2 ,
t ∈R and n ∈N.
A sketch of xn(t) for various n appears in Fig. 3.3. We note that
dxn(t)
dt
= (1 + nt2) · 1 −t · (2nt)
[1 + nt2]2
=
1 −nt2
[1 + nt2]2 = 0
for t = ± 1
√n. We see that
xn
 
± 1
√n
!
= ±
1
2√n.
We also see that xn →0. So then
Mn = sup
t∈R
|xn(t)| =
1
2√n.
Clearly, Mn →0 as n →∞. Therefore, via Theorem 3.2, we immediately conclude
that xn →x uniformly on the real number line.
3.4
FOURIER SERIES
The Fourier series expansion was introduced brieﬂy in Chapter 1, where the behav-
ior of this series with respect to its convergence properties was not mentioned. In
this section we shall demonstrate the pointwise convergence of the Fourier series
TLFeBOOK

74
SEQUENCES AND SERIES
by the analysis of a particular example. Much of what follows is from Walter [17].
However, of necessity, the present treatment is not so rigorous.
Suppose that
g(t) = 1
π (π −t),
0 < t < 2π.
(3.19)
The reader is strongly invited to show that this has Fourier series expansion
g(t) = 2
π
∞

k=1
1
k sin(kt).
(3.20)
The procedure for doing this closely follows Example 1.20. In our analysis to
follow, it will be easier to work with
f (t) = π
2 g(t) =
∞

k=1
1
k sin(kt).
(3.21)
Deﬁne the sequence of partial sums
Sn(t) =
n

k=1
1
k sin(kt)
(n ∈N).
(3.22)
So we infer that
lim
n→∞Sn(t) = f (t),
(3.23)
but we do not know in what sense the partial sums tend to f (t). Is convergence
pointwise, or uniform?
We shall need the special function
Dn(t) = 1
π

1
2 +
n

k=1
cos(kt)
	
= 1
2π
sin(n + 1
2)t
sin( 1
2t)
.
(3.24)
This function is called the Dirichlet kernel. The second equality in (3.24) is not
obvious. We will prove it. Consider that
sin
 1
2t
!
(πDn(t)) = 1
2 sin
 1
2t
!
+
n

k=1
sin
 1
2t
!
cos(kt)
= 1
2 sin
 1
2t
!
+ 1
2
n

k=1
sin
 
k + 1
2
!
t + 1
2
n

k=1
sin
 1
2 −k
!
t
= 1
2 sin
 1
2t
!
+ 1
2
n

k=1
sin
 
k + 1
2
!
t −1
2
n

k=1
sin
 
k −1
2
!
t,
(3.25)
TLFeBOOK

FOURIER SERIES
75
where we have used the identity sin a cos b = 1
2 sin(a + b) + 1
2 sin(a −b). By
expanding the sums and looking for cancellations
n

k=1
sin
 
k + 1
2
!
t −
n

k=1
sin
 
k −1
2
!
t = sin
 
n + 1
2
!
t −sin
 1
2t
!
.
(3.26)
Applying (3.26) in (3.25), we obtain
sin
 1
2t
!
(πDn(t)) = 1
2 sin
 
n + 1
2
!
t
so immediately
Dn(t) = 1
2π
sin(n + 1
2)t
sin( 1
2t)
,
and this establishes (3.24). Using the identity sin(a + b) = sin a cos b + cos a sin b,
we may also write
Dn(t) = 1
2π

sin(nt) cos( 1
2t)
sin( 1
2t)
+ cos(nt)
	
.
(3.27)
For t > 0, using the form of the Dirichlet kernel in (3.27), we have
π
 t
0
Dn(x) dx =
 t
0

sin(nx) cos( 1
2x)
2 sin( 1
2x)
+ 1
2 cos(nx)
	
dx
=
 t
0
sin(nx)
x
dx +
 t
0
sin(nx)

1
2
cos( 1
2x)
sin( 1
2x)
−1
x
	
dx
+ 1
2
 t
0
cos(nx) dx.
(3.28)
We are interested in what happens when t is a small positive value, but n is large.
To begin with, it is not difﬁcult to see that
lim
n→∞
1
2
 t
0
cos(nx) dx = lim
n→∞
1
2n sin(nt) = 0.
(3.29)
Less clearly
lim
n→∞
 t
0
sin(nx)

1
2
cos( 1
2x)
sin( 1
2x)
−1
x
	
dx = 0
(3.30)
TLFeBOOK

76
SEQUENCES AND SERIES
(take this for granted). Through a simple change of variable
I (nt) =
 t
0
sin(nx)
x
dx =
 nt
0
sin x
x
dx.
(3.31)
In fact
 ∞
0
sin x
x
dx = π
2 .
(3.32)
This is not obvious, either. The result may be found in integral tables [18, p. 483].
In other words, even for very small t, I (nt) does not go to zero as n increases.
Consequently, using (3.29), (3.30), and (3.31) in (3.28), we have (for big n)
π
 t
0
Dn(x) dx ≈I (nt).
(3.33)
The results in the previous paragraph help in the following manner. Begin by
noting that
Sn(t) =
n

k=1
1
k sin(kt) =
n

k=1
 t
0
cos(kx) dx =
 t
0
 n

k=1
cos(kx)
	
dx
=
 t
0

1
2 +
n

k=1
cos(kx)
	
dx −1
2t = π
 t
0
Dn(x) dx −1
2t
(via (3.24)).
(3.34)
So from (3.33)
Sn(t) ≈I (nt) −1
2t.
(3.35)
Deﬁne the sequence tn = 1
nπ. Consequently
Sn(tn) ≈I (π) −1
2nπ.
(3.36)
As n →∞, tn →0, and Sn(tn) →I (π). We can say that for big n
Sn(0+) ≈I (π).
(3.37)
Now, f (0+) = π
2 , so for big n
Sn(0+)
f (0+) ≈2
π
 π
0
sin x
x
dx ≈1.18.
(3.38)
Numerical integration is needed to establish this. This topic is the subject of a later
chapter, however.
We see that the sequence of partial sums Sn(0+) converges to a value bigger
than f (0+) as n →∞. The approximation Sn(t) therefore tends to “overshoot”
TLFeBOOK

FOURIER SERIES
77
the true value of f (t) for small t. This is called the Gibbs phenomenon, or Gibbs
overshoot. We observe that t = 0 is the place where f (t) has a discontinuity. This
tendency of the Fourier series to overshoot near discontinuities is entirely typical.
We note that f (π) = Sn(π) = 0 for all n ≥1. Thus, for any ϵ > 0
|f (π) −Sn(π)| ≤ϵ
for all n ≥1. The previous analysis for t = 0+, and this one for t = π show that
N (in the deﬁnitions of convergence) depends on t. Convergence of the Fourier
series is therefore pointwise and not uniform. Generally, the Gibbs phenomenon is
a symptom of pointwise convergence.
We remark that the Gibbs phenomenon has an impact in the signal processing
applications of series expansions. Techniques for signal compression and signal
enhancement are often based on series expansions. The Gibbs phenomenon can
degrade the quality of decompressed or reconstructed signals. The phenomenon
is responsible for “ringing artifacts.” This is one reason why the convergence
properties of series expansions are important to engineers.
Figure 3.4 shows a plot of f (t), Sn(t) and the error
En(t) = Sn(t) −f (t).
(3.39)
The reader may conﬁrm (3.38) directly from the plot in Fig. 3.4a.
0
1
2
3
4
5
6
7
−2
−1
0
1
2
t
Amplitude
0
1
2
3
4
5
6
7
−2
−1.5
−1
−0.5
0
0.5
1
1.5
t
En (t) (error signal)
(a)
(b)
f(t)
Sn (t) for n = 30 
Figure 3.4
Plots of the Fourier series expansion for f (t) in (3.21), Sn(t) [of (3.22)] for
n = 30, and the error En(t) = Sn(t) −f (t).
TLFeBOOK

78
SEQUENCES AND SERIES
0
1
2
3
4
5
6
7
−2
−1.5
−1
−0.5
0
0.5
1
1.5
t
En (t) (error signals)
n = 10
n = 20
n = 30
Figure 3.5
En(t) of (3.39) for different values of n.
We conclude this section by remarking that
lim
n→∞
 2π
0
|En(t)|2 dt = 0,
(3.40)
that is, the energy of the error goes to zero in the limit as n goes to inﬁnity.
However, the amplitude of the error in the vicinity of a discontinuity remains
unchanged in the limit. This is more clearly seen in Fig. 3.5, where the error is
displayed for different values of n. Of course, this fact agrees with our analysis.
Equation (3.40) is really a consequence of the fact that (recalling Chapter 1) Sn(t)
and f (t) are both in the space L2(0, 2π). Rigorous proof of this is quite tough,
and so we omit the proof entirely.
3.5
TAYLOR SERIES
Assume that f (x) is real-valued and that x ∈R. One way to deﬁne the derivative
of f (x) at x = x0 is according to
f (1)(x) = df (x)
dx
|x=x0 = lim
x→x0
f (x) −f (x0)
x −x0
.
(3.41)
The notation is
f (n)(x) = dnf (x)
dxn
(3.42)
(so f (0)(x) = f (x)). From (3.41), we obtain
f (x) ≈f (x0) + f (1)(x0)(x −x0).
(3.43)
But how good is this approximation? Can we obtain a more accurate approximation
to f (x) if we know f (n)(x0) for n > 1? Again, what is the accuracy of the resulting
approximation? We consider these issues in this section.
TLFeBOOK

TAYLOR SERIES
79
Begin by recalling the following theorem.
Theorem 3.3: Mean-Value Theorem
If f (x) is continuous for x ∈[a, b]
with a continuous derivative for x ∈(a, b), then there is a number ξ ∈(a, b) such
that
f (b) −f (a)
b −a
= f (1)(ξ).
(3.44)
Therefore, if a = x0 and b = x, we must have
f (x) = f (x0) + f (1)(ξ)(x −x0).
(3.45)
This expression is “exact,” and so is in contrast with (3.43). Proof of Theo-
rem 3.3 may be found in, for example, Bers [19, p. 636]. Theorem 3.3 generalizes
to the following theorem.
Theorem 3.4: Generalized Mean-Value Theorem
Suppose that f (x) and
g(x) are continuous functions on x ∈[a, b]. Assume f (1)(x), and g(1)(x) exist and
are continuous, and g(1)(x) ̸= 0 for x ∈(a, b). There is a number ξ ∈(a, b) such
that
f (b) −f (a)
g(b) −g(a) = f (1)(ξ)
g(1)(ξ) .
(3.46)
Once again the proof is omitted, but may be found in Bers [19, p. 637].
The tangent to f (x) at x = x0 is given by t(x) = f (x0) + f (1)(x0)(x −x0)
(t(x0) = f (x0) and t(1)(x0) = f (1)(x0)). We wish to consider t(x) to be an approx-
imation to f (x), so the error is
f (x) −t(x) = e(x)
or
f (x) = f (x0) + f (1)(x0)(x −x0) + e(x).
(3.47)
Thus
e(x)
x −x0
= f (x) −f (x0)
x −x0
−f (1)(x0).
But
lim
x→x0
f (x) −f (x0)
x −x0
= f (1)(x0)
so immediately
lim
x→x0
e(x)
x −x0
= 0.
(3.48)
From (3.47) e(x0) = 0, and also from (3.47), we obtain
f (1)(x) = f (1)(x0) + e(1)(x),
(3.49)
TLFeBOOK

80
SEQUENCES AND SERIES
so e(1)(x0) = 0. From (3.49), f (2)(x) = e(2)(x) (so we now assume that f (x) has
a second derivative, and we will also assume that it is continuous). Theorem 3.4
has the following corollary.
Corollary 3.1
Suppose that f (a) = g(a) = 0, so for all b ̸= a there is a ξ ∈
(a, b) such that
f (b)
g(b) = f (1)(ξ)
g(1)(ξ) .
(3.50)
Now apply this corollary to f (x) = e(x), and g(x) = (x −x0)2, with a = x0,
b = x. Thus, from (3.50)
e(x)
(x −x0)2 =
e(1)(τ)
2(τ −x0)
(3.51)
(τ ∈(x0, x)). Apply the corollary once more to f (τ) = e(1)(τ), and g(τ) =
2(τ −x0):
e(1)(τ)
2(τ −x0) = 1
2e(2)(ξ) = 1
2f (2)(ξ)
(3.52)
(ξ ∈(x0, τ)). Apply (3.51) in (3.52)
e(x)
(x −x0)2 = 1
2f (2)(ξ)
or (for some ξ ∈(x0, x))
e(x) = 1
2f (2)(ξ)(x −x0)2.
(3.53)
If |f (2)(t)| ≤M2 for t ∈(x0, x), then
|e(x)| ≤
1
2 · 1M2|x −x0|2
(3.54)
and
f (x) = f (x0) + f (1)(x0)(x −x0) + e(x),
(3.55)
for which (3.54) is an upper bound on the size of the error involved in approxi-
mating f (x) using (3.43).
Example 3.6
Suppose that f (x) = √x; then
f (1)(x) =
1
2√x ,
f (2)(x) = −1
4
1
x√x .
Suppose that x0 = 1, and x = x0 + δx = 1 + δx. Thus, via (3.55)
√x =
√
1 + δx = 1 + f (1)(1)δx + e(x) = 1 + 1
2δx + e(x),
TLFeBOOK

TAYLOR SERIES
81
so if, for example, |δx| < 3
4, then |f (2)(x)| ≤2 (for x ∈( 1
4, 7
4)) so M2 = 2,
and so
|e(x)| ≤(δx)2
via (3.54). This bound may be compared to the following table of values:
δx
√
1 + δx
1 + 1
2δx
e(x)
(δx)2
−3
4
0.5000
0.6250
−0.1250
0.5625
−1
2
0.7071
0.7500
−0.0429
0.2500
0
1.0000
1.0000
0.0000
0.0000
1
2
1.2247
1.2500
−0.0253
0.2500
3
4
1.3229
1.3750
−0.0521
0.5625
It is easy to see that indeed |e(x)| ≤(δx)2.
We mention that Corollary 3.1 leads to l’Hˆopital’s rule. It therefore allows us
to determine
lim
x→a
f (x)
g(x)
when f (a) = g(a) = 0. We now digress brieﬂy to consider this subject. The rule
applies if f (x) and g(x) are continuous at x = a, if f (x) and g(x) have continu-
ous derivatives at x = a, and if g(1)(x) ̸= 0 near x = a, except perhaps at x = a.
l’Hˆopital’s rule is as follows. If limx→a f (x) = limx→a g(x) = 0 and limx→a
f (1)(x)
g(1)(x)
exists, then
lim
x→a
f (x)
g(x) = lim
x→a
f (1)(x)
g(1)(x) .
(3.56)
The rationale is that from Corollary 3.1 for all x ̸= a there is a ξ ∈(a, b) such that
f (x)
g(x) = f (1)(ξ)
g(1)(ξ) . So, if x is close to a then ξ must also be close to a, and f (1)(ξ)
g(1)(ξ)
is close to its limit. l’Hˆopital’s rule is also referred to as “the rule for evaluating
the indeterminate form 0
0.” If it happens that f (1)(a) = g(1)(a) = 0, then one may
attempt l’Hˆopital’s rule yet again; that is, if limx→a
f (2)(x)
g(2)(x) exists, then
lim
x→a
f (x)
g(x) = lim
x→a
f (2)(x)
g(2)(x) .
Example 3.7
Consider
lim
x→0
sin x −ex + 1
x2
= lim
x→0
d
dx [sin x −ex + 1]
d
dx [x2]
= lim
x→0
cos x −ex
2x
= lim
x→0
−sin x −ex
2
= −1
2
TLFeBOOK

82
SEQUENCES AND SERIES
for which the rule has been applied twice. Now consider instead
lim
x→0
1 −2x
2 + 4x = lim
x→0
d
dx [1 −2x]
d
dx [2 + 4x]
= lim
x→0
−2
4 = −1
2.
This is wrong ! l’Hˆopital’s rule does not apply here because f (0) = 1, and g(0) = 2
(i.e., we do not have f (0) = g(0) = 0 as needed by the theory).
The rule can be extended to cover other indeterminate forms (e.g., ∞
∞). For example,
consider
lim
x→0 x loge x = lim
x→0
loge x
1
x
= lim
x→0
1
x
−1
x2
= lim
x→0(−x) = 0.
An interesting case is that of ﬁnding
lim
x→∞
 
1 + 1
x
!x
.
This is an indeterminate of the form 1∞. Consider
lim
x→∞loge
 
1 + 1
x
!x
= lim
x→∞
loge
 
1 + 1
x
!
1
x
= lim
x→∞
1
1 + 1
x
 
−1
x2
!
−1
x2
= lim
x→∞
1
1 + 1
x
= 1.
The logarithm and exponential functions are continuous functions, so it happens to
be the case that
1 = lim
x→∞loge
 
1 + 1
x
!x
= loge

lim
x→∞
 
1 + 1
x
!x
,
TLFeBOOK

TAYLOR SERIES
83
that is, the limit and the logarithm can be interchanged. Thus
e1 = e
loge

lim
x→∞

1+ 1
x
x
so ﬁnally we have
lim
x→∞
 
1 + 1
x
!x
= e.
(3.57)
More generally, it can be shown that
lim
n→∞

1 + x
n
n
= ex.
(3.58)
This result has various applications, including some in probability theory relating to
Poisson and exponential random variables [20]. An alternative derivation of (3.57)
appears on pp. 64–65 of Rudin [2], but involves the use of the Maclaurin series
expansion for e. We revisit the Maclaurin series for ex later.
We have demonstrated that for suitable ξ ∈(x0, x)
f (x) = f (x0) + f (1)(x0)(x −x0) + 1
2f (2)(ξ)(x −x0)2
(recall (3.55)). Deﬁne
p(x) = f (x0) + f (1)(x0)(x −x0) + 1
2f (2)(x0)(x −x0)2
(3.59)
so this is some approximation to f (x) near x = x0. Equation (3.43) is a linear
approximation to f (x), and (3.59) is a quadratic approximation to f (x). Once
again, we wish to consider the error
f (x) −p(x) = e(x).
(3.60)
We note that
p(x0) = f (x0),
p(1)(x0) = f (1)(x0),
p(2)(x0) = f (2)(x0).
(3.61)
In other words, the approximation to f (x) in (3.59) matches the function and its
ﬁrst two derivatives at x = x0. Because of (3.61), via (3.60)
e(x0) = e(1)(x0) = e(2)(x0) = 0,
(3.62)
and so via (3.59) and (3.60)
e(3)(x) = f (3)(x)
(3.63)
TLFeBOOK

84
SEQUENCES AND SERIES
(because p(3)(x) = 0 since p(x) is a quadratic in x). As in the derivation of (3.53),
we may repeatedly apply Corollary 3.1:
e(x)
(x −x0)3 =
e(1)(t1)
3(t1 −x0)2
for t1 ∈(x0, x)
e(1)(t1)
3(t1 −x0)2 =
e(2)(t2)
3 · 2(t2 −x0)
for t2 ∈(x0, t1)
e(2)(t2)
3 · 2(t2 −x0) = e(3)(ξ)
3 · 2
for ξ ∈(x0, t2),
which together yield
e(x)
(x −x0)3 = f (3)(ξ)
3 · 2
for ξ ∈(x0, x)
or
e(x) =
1
3 · 2 · 1f (3)(ξ)(x −x0)3
(3.64)
for some ξ ∈(x0, x). Thus
f (x) = f (x0) + f (1)(x0)(x −x0) +
1
2 · 1f (2)(x0)(x −x0)2 + e(x).
(3.65)
Analogously to (3.54), if |f (3)(t)| ≤M3 for t ∈(x0, x), then we have the error
bound
|e(x)| ≤
1
3 · 2 · 1M3|x −x0|3.
(3.66)
We have gone from a linear approximation to f (x) to a quadratic approximation
to f (x). All of this suggests that we may generalize to a degree n polynomial
approximation to f (x). Therefore, we deﬁne
pn(x) =
n

k=0
pn,k(x −x0)k,
(3.67)
where
pn,k = 1
k!f (k)(x0).
(3.68)
Then
f (x) = pn(x) + en+1(x),
(3.69)
where the error term is
en+1(x) =
1
(n + 1)!f (n+1)(ξ)(x −x0)n+1
(3.70)
TLFeBOOK

TAYLOR SERIES
85
for suitable ξ ∈(x0, x). We call pn(x) the Taylor polynomial of degree n. This
polynomial is the approximation to f (x), and the error en+1(x) in (3.70) can be
formally obtained by the repeated application of Corollary 3.1. These details are
omitted. Expanding (3.69), we obtain
f (x) = f (x0) + f (1)(x0)(x −x0) + 1
2!f (2)(x0)(x −x0)2
+ · · · + 1
n!f (n)(x0)(x −x0)n +
1
(n + 1)!f (n+1)(ξ)(x −x0)n+1
(3.71)
which is the familiar Taylor formula for f (x). We remark that
f (k)(x0) = p(k)
n (x0)
(3.72)
for k = 0, 1, . . . , n −1, n. So we emphasize that the approximation pn(x) to f (x)
is based on forcing pn(x) to match the ﬁrst n derivatives of f (x), as well as
enforcing pn(x0) = f (x0). If |f (n+1)(t)| ≤Mn+1 for all t ∈I [interval I contains
(x0, x)], then
|en+1(x)| ≤
1
(n + 1)!Mn+1|x −x0|n+1.
(3.73)
If all derivatives of f (x) exist and are continuous, then we have the Taylor series
expansion of f (x), namely, the inﬁnite series
f (x) =
∞

k=0
1
k!f (k)(x0)(x −x0)k.
(3.74)
The Maclaurin series expansion is a special case of (3.74) for x0 = 0:
f (x) =
∞

k=0
1
k!f (k)(0)xk.
(3.75)
If we retain only terms k = 0 to k = n in the inﬁnite series (3.74) and (3.75), we
know that en+1(x) gives the error in the resulting approximation. This error may
be called the truncation error (since it arises from truncation of the inﬁnite series
to a ﬁnite number of terms). Now we consider some examples.
First recall the binomial theorem
(a + x)n =
n

k=0
 n
k
!
xkan−k,
(3.76)
where
 n
k
!
=
n!
k!(n −k)!.
(3.77)
TLFeBOOK

86
SEQUENCES AND SERIES
In (3.76) we emphasize that n ∈Z+. But we can use Taylor’s formula to obtain an
expression for (a + x)α when α ̸= 0, and α is not necessarily an element of Z+.
Let us consider the special case
f (x) = (1 + x)α
for which (if k ≥1)
f (k)(x) = α(α −1)(α −2) · · · (α −k + 1)(1 + x)α−k.
(3.78)
These derivatives are guaranteed to exist, provided x > −1. We will assume this
restriction always applies. So, in particular
f (k)(0) = α(α −1)(α −2) · · · (α −k + 1)
(3.79)
giving the Maclaurin expansion
(1 + x)α = 1 +
n

k=1
1
k![α(α −1) · · · (α −k + 1)]xk
+
1
(n + 1)![α(α −1) · · · (α −n)](1 + ξ)α−n−1xn+1
(3.80)
for some ξ ∈(x0, x). We may extend the deﬁnition (3.77), that is, deﬁne
 α
0
!
= 1,
 α
k
!
= 1
k!α(α −1) · · · (α −k + 1)(k ≥1)
(3.81)
so that (3.80) becomes
(1 + x)α =
n

k=0
 α
k
!
xk
#
$%
&
=pn(x)
+
 
α
n + 1
!
(1 + ξ)α−n−1xn+1
#
$%
&
=en+1(x)
,
(3.82)
for x > −1.
Example 3.8
We wish to compute [1.03]1/3 with n = 2 in (3.82), and to esti-
mate the error involved in doing so. We have x = 0.03, α = 1
3, and ξ ∈(0, .03).
Therefore from (3.82) [1 + x]1/3 is approximated by the Taylor polynomial
p2(x) = 1 +
'
1
3
1
(
x +
'
1
3
2
(
x2 = 1 + 1
3x −1
9x2
so
[1.03]1/3 ≈p2(0.03) = 1.009900000
TLFeBOOK

TAYLOR SERIES
87
but [1.03]1/3 = 1.009901634, so e3(x) = 1.634 × 10−6. From (3.82)
e3(x) =
'
1
3
3
(
(1 + ξ)
1
3 −3x3 = 5
34 (1 + ξ)−8/3x3,
and so e3(0.03) = 5
34
33
106 (1 + ξ)−8/3 = 5
3 × 10−6(1 + ξ)−8/3. Since 0 < ξ < 0.03,
we have
1.5403 × 10−6 < e3(.03) < 1.6667 × 10−6.
The actual error is certainly within this range.
If f (x) =
1
1+x , and if x0 = 0, then
1
1 + x =
n

k=0
(−1)kxk
#
$%
&
=pn(x)
+ (−1)n+1xn+1
1 + x
#
$%
&
=r(x)
.
(3.83)
This may be seen by recalling that
n

k=0
αk = 1 −αn+1
1 −α
(α ̸= 1).
(3.84)
So n
k=0(−1)kxk = 1−(−1)n+1xn+1
1+x
, and thus
n

k=0
(−1)kxk + (−1)n+1xn+1
1 + x
= 1 −(−1)n+1xn+1
1 + x
+ (−1)n+1xn+1
1 + x
=
1
1 + x .
This conﬁrms (3.83). We observe that the remainder term r(x) in (3.83) is not given
by en+1(x) in (3.82). We have obtained an exact expression for the remainder using
elementary methods.
Now, from (3.83), we have
1
1 + t = 1 −t + t2 −t3 + · · · + (−1)n−1tn−1 + (−1)ntn
1 + t
,
and we see immediately that
loge(1 + x) =
 x
0
dt
1 + t = x −1
2x2 + 1
3x3
+ · · · + (−1)n−1xn
n
+ (−1)n
 x
0
tn
1 + t dt
#
$%
&
=r(x)
.
(3.85)
TLFeBOOK

88
SEQUENCES AND SERIES
For x > 0, and 0 < t < x we have
1
1+t < 1, implying that
0 <
 x
0
tn
1 + t dt ≤
 x
0
tn dt = xn+1
n + 1
(x > 0).
For −1 < x < 0 with x < t < 0, we have
1
1 + t <
1
1 + x =
1
1 −|x|
so

 x
0
tn
1 + t dt
 ≤
1
1 −|x|

 x
0
tn dt
 =
1
1 −|x|

xn+1
n + 1
 =
|x|n+1
(1 −|x|)(n + 1).
Consequently, we may conclude that
|r(x)| ≤



1
n + 1xn+1,
x ≥0
|x|n+1
(1 −|x|)(n + 1),
−1 < x ≤0
.
(3.86)
Equation (3.85) gives us a means to compute logarithms, and (3.86) gives us a
bound on the error.
Now consider (3.83) with x replaced by x2:
1
1 + x2 =
n

k=0
(−1)kx2k + (−1)n+1x2n+2
1 + x2
.
(3.87)
Replacing n with n −1, replacing x with t, and expanding, this becomes
1
1 + t2 = 1 −t2 + t4 −· · · + (−1)n−1t2n−2 + (−1)nt2n
1 + t2 ,
where, on integrating, we obtain
tan−1 x =
 x
0
dt
1 + t2 = x −1
3x3 + 1
5x5 −1
7x7
+ · · · + (−1)n−1x2n−1
2n −1
+ (−1)n
 x
0
t2n
1 + t2 dt
#
$%
&
=r(x)
.
(3.88)
Because
1
1+t2 ≤1 for all t ∈R, it follows that
|r(x)| ≤|x|2n+1
2n + 1 .
(3.89)
TLFeBOOK

TAYLOR SERIES
89
We now have a method of computing π. Since π
4 = tan−1(1), we have
π
4 = 1 −1
3 + 1
5 −1
7 + · · · + (−1)n−1
2n −1 + r(1)
(3.90)
and
r(1) ≤
1
2n + 1.
(3.91)
Using (3.90) to compute π is not efﬁcient with respect to the number of arith-
metic operations needed (i.e., it is not computationally efﬁcient). This is because
to achieve an accuracy of about 1/n requires about n/2 terms in the series [which
follows from (3.91)]. However, if x is small (i.e., close to zero), then series (3.88)
converges relatively quickly. Observe that
tan−1 x + y
1 −xy = tan−1 x + tan−1 y.
(3.92)
Suppose that x = 1
2, and y = 1
3, then
x+y
1−xy = 1, so
π
4 = tan−1
 1
2
!
+ tan−1
 1
3
!
.
(3.93)
It is actually faster to compute tan−1( 1
2), and tan−1( 1
3) using (3.88), and for these
obtain π using (3.93) than to compute tan−1(1) directly. In fact, this approach (a
type of “divide and conquer” method) can be taken further by noting that
tan−1
 1
2
!
= tan−1
 1
3
!
+ tan−1
 1
7
!
, tan−1
 1
3
!
= tan−1
 1
5
!
+ tan−1
 1
8
!
implying that
π
4 = 2 tan−1
 1
5
!
+ tan−1
 1
7
!
+ 2 tan−1
 1
8
!
.
(3.94)
Now consider f (x) = ex. Since f (k)(x) = ex for all k ∈Z+ we have for x0 = 0
the Maclaurin series expansion
ex =
∞

k=0
xk
k! .
(3.95)
This is theoretically valid for −∞< x < ∞. We have employed this series before
in various ways. We now consider it as a computational tool for calculating ex.
Appendix 3.C is based on a famous example in Forsythe et al. [21, pp. 14–16].
This example shows that series expansions must be implemented on computers
TLFeBOOK

90
SEQUENCES AND SERIES
with rather great care. Speciﬁcally, Appendix 3.C shows what can happen when we
compute e−20 by the direct implementation of the series (3.95). Using MATLAB
as stated e−20 ≈4.1736 × 10−9, which is based on keeping terms k = 0 to 88
(inclusive) of (3.95). Using additional terms will have no effect on the ﬁnal answer
as they are too small. However, the correct value is actually e−20 = 2.0612 × 10−9,
as may be veriﬁed using the MATLAB exponential function, or using a typical
pocket calculator. Our series approximation has resulted in an answer possessing
no signiﬁcant digits at all. What went wrong? Many of the terms in the series
are orders of magnitude bigger than the ﬁnal result and typically possess rounding
errors about as big as the ﬁnal answer. The phenomenon is called catastrophic
cancellation (or catastrophic convergence). As Forsythe et al. [21] stated, “It is
important to realize that this great cancellation is not the cause of error in the
answer; it merely magniﬁes the error already present in the terms.” Catastrophic
cancellation can in principle be eliminated by carrying more signiﬁcant digits in the
computation. However, this is costly with respect to computing resources. In the
present problem a cheap and very simple solution is to compute e20 using (3.95),
and then take the reciprocal, i.e., use e−20 = 1/e20.
An important special function is the gamma function:
(z) =
 ∞
0
xz−1e−x dx.
(3.96)
Here, we assume z ∈R. This is an improper integral so we are left to wonder if
lim
M→∞
 M
0
xz−1e−x dx
exists. It turns out that the integral (3.96) converges for z > 0, but diverges for
z ≤0. The proof is slightly tedious, and so we will omit it [22, pp. 273–274]. If
z = n ∈N, then consider
(n) =
 ∞
0
xn−1e−x dx.
(3.97)
Now
(n + 1) =
 ∞
0
xne−x dx = lim
M→∞
 M
0
xne−x dx
= lim
M→∞

−xne−x|M
0 + n
 M
0
xn−1e−x dx

(via
"
u dv = uv −
"
v du, i.e., integration by parts). Therefore
(n + 1) = n
 ∞
0
xn−1e−x dx = n(n).
(3.98)
TLFeBOOK

TAYLOR SERIES
91
We see that (1) =
" ∞
0
e−x dx = [−e−x]∞
0 = 1. Thus, (n + 1) = n!. Using the
gamma function in combination with (3.95), we may obtain Stirling’s formula
n! ≈
√
2πnn+1/2e−n
(3.99)
which is a good approximation to n! if n is big. The details of a rigorous derivation
of this are tedious, so we give only an outline presentation. Begin by noting that
n! =
 ∞
0
xne−x dx =
 ∞
0
en ln x−x dx.
Let x = n + y, so
n! = e−n
 ∞
−n
en ln(n+y)−y dy.
Now, since n ln(n + y) = n ln

n
)
1 + y
n
*
= n ln n + n ln
)
1 + y
n
*
, we have
n! = e−n
 ∞
−n
en ln n+n ln(1+ y
n)−y dy
= e−nnn
 ∞
−n
en ln(1+ y
n)−y dy.
Using (3.85), that is
ln

1 + y
n

= y
n −y2
2n2 + y3
3n3 −· · · ,
we have
nln

1 + y
n

−y = −y2
2n + y3
3n2 −· · · ,
so
n! = nne−n
 ∞
−n
e−y2
2n + y3
3n2 −··· dy.
If now y = √nv, then dy = √ndv, and so
n! = nn+ 1
2 e−n
 ∞
−√n
e
−v2
2 + v3
3√n −··· dv.
So if n is big, then
n! ≈nn+ 1
2 e−n
 ∞
−∞
e−v2
2 dv.
TLFeBOOK

92
SEQUENCES AND SERIES
If we accept that
 ∞
−∞
e−x2/2 dx =
√
2π,
(3.100)
then immediately we have
n! ≈
√
2πnn+ 1
2 e−n,
and the formula is now established. Stirling’s formula is very useful in statistical
mechanics (e.g., deriving the Fermi–Dirac distribution of fermion particle energies,
and this in turn is important in understanding the operation of solid-state electronic
devices at a physical level).
Another important special function is
g(x) =
1
√
2πσ 2 exp

−(x −m)2
2σ 2

,
−∞< x < ∞
(3.101)
which is the Gaussian function (or Gaussian pulse). This function is of immense
importance in probability theory [20], and is also involved in the uncertainty princi-
ple in signal processing and quantum mechanics [23]. A sketch of g(x) for m = 0,
with σ 2 = 1, and σ 2 = 0.1 appears in Fig. 3.6. For m = 0 and σ 2 = 1, the standard
form pulse
f (x) =
1
√
2π
e−x2/2
(3.102)
is sometimes deﬁned [20]. In this case we observe that g(x) = 1
σ f
) x−m
σ
*
. We will
show that
 ∞
0
e−x2 dx = 1
2
√π,
(3.103)
which can be used to obtain (3.100) by a simple change of variable. From [22]
(p. 262) we have
−3
−2
−1
0
1
2
3
0
0.2
0.4
0.6
0.8
1
1.2
1.4
x
g(x)
s2 = 1
s2 = 0.1
Figure 3.6
Plots of two Gaussian pulses, where g(x) in (3.101) for m = 0, with σ 2 =
1, 0.1.
TLFeBOOK

TAYLOR SERIES
93
Theorem 3.5: Let limx→∞xpf (x) = A. Then
1.
" ∞
a
f (x) dx converges if p > 1 and −∞< A < ∞.
2.
" ∞
a
f (x) dx diverges if p ≤1 and A ̸= 0 (A may be inﬁnite).
We see that limx→∞x2e−x2 = 0 (perhaps via l’Hˆopital’s rule). So in The-
orem 3.5 f (x) = e−x2, and p = 2, with A = 0, and so
" ∞
0
e−x2 dx converges.
Deﬁne
IM =
 M
0
e−x2 dx =
 M
0
e−y2 dy
and let limM→∞IM = I. Then
I 2
M =
  M
0
e−x2 dx
!   M
0
e−y2 dy
!
=
 M
0
 M
0
e−(x2+y2) dx dy
=

RM

e−(x2+y2) dx dy
for which RM is the square OABC in Fig. 3.7. This square has sides of length M.
Since e−(x2+y2) > 0, we obtain

RL

e−(x2+y2) dx dy ≤I 2
M ≤

RU

e−(x2+y2) dx dy,
(3.104)
O
A
B
C
D
E
M
x
y
M
2
Figure 3.7
Regions used to establish (3.103).
TLFeBOOK

94
SEQUENCES AND SERIES
where RL is the region in the ﬁrst quadrant bounded by a circle of radius M.
Similarly, RU is the region in the ﬁrst quadrant bounded by a circle of radius
√
2M.
Using polar coordinates, r2 = x2 + y2 and dx dy = r dr dφ, so (3.104) becomes
 π/2
φ=0
 M
r=0
e−r2r dr dφ ≤I 2
M ≤
 π/2
φ=0
 √
2M
r=0
e−r2r dr dφ.
(3.105)
Since −1
2
d
dx e−x2 = xe−x2 we have
" M
0 re−r2 dr = −1
2[e−x2]M
0 = 1
2[1 −e−M2].
Thus, (3.105) reduces to
π
4 [1 −e−M2] ≤I 2
M ≤π
4 [1 −e−2M2].
(3.106)
If we now allow M →∞in (3.106), then I 2
M →π
4 , implying that I 2 = π
4 , or
I = 1
2
√π. This conﬁrms (3.103).
In probability theory it is quite important to be able to compute functions such
as the error function
erf(x) =
2
√π
 x
0
e−t2 dt.
(3.107)
This has wide application in digital communications system analysis, for example.
No closed-form4 expression for (3.107) exists. We may therefore try to compute
(3.107) using series expansions. In particular, we may try working with the Maclau-
rin series expansion for ex:
erf(x) =
2
√π
 x
0
 ∞

k=0
xk
k!
	
x=−t2
dt
=
2
√π
∞

k=0
(−1)k
k!
 x
0
t2k dt
=
2
√π
∞

k=0
(−1)kx2k+1
k!(2k + 1) .
(3.108)
However, to arrive at this expression, we had to integrate an inﬁnite series term
by term. It is not obvious that we can do this. When is this justiﬁed?
A power series is any series of the form
f (x) =
∞

k=0
ak(x −x0)k.
(3.109)
Clearly, Taylor and Maclaurin series are all examples of power series. We have the
following theorem.
4A closed-form expression is simply a “nice” formula typically involving more familiar functions such
as sines, cosines, tangents, polynomials, and exponential functions.
TLFeBOOK

TAYLOR SERIES
95
Theorem 3.6: Given the power series (3.109), there is an R ≥0 (which may
be R = +∞) such that the series is absolutely convergent for |x −x0| < R, and is
divergent for |x −x0| > R. At x −x0 = R and at x −x0 = −R, the series might
converge or diverge.
Series (3.109) is absolutely convergent if the series
h(x) =
∞

k=0
|ak(x −x0)k|
(3.110)
converges. We remark that absolutely convergent series are convergent. This means
that if (3.110) converges, then (3.109) also converges. (However, the converse is
not necessarily true.) We also have the following theorem.
Theorem 3.7: If
f (x) =
∞

k=0
ak(x −x0)k
for
|x −x0| < R,
where R > 0 is the radius of convergence of the power series, then f (x) is
continuous and differentiable in the interval of convergence x ∈(x0 −R, x0 + R),
and
f (1)(x) =
∞

k=1
kak(x −x0)k−1,
(3.111a)
 x
x0
f (t) dt =
∞

k=0
ak
k + 1(x −x0)k+1.
(3.111b)
This series [Eq. (3.111a,b)] also has a radius of convergence R.
As a consequence of Theorem 3.7, Eq. (3.108) is valid for −∞< x < ∞(i.e., the
radius of convergence is R = +∞). This is because the Maclaurin expansion for
ex had R = +∞.
Example 3.9
Here we will ﬁnd an expression for the error involved in trun-
cating the series for erf(x) in (3.108).
From (3.71) for some ξ ∈[0, x] (interval endpoints may be included because of
continuity of the function being approximated)
ex =
n

k=0
xk
k!
# $% &
=pn(x)
+en(x),
TLFeBOOK

96
SEQUENCES AND SERIES
where
en(x) =
1
(n + 1)!eξxn+1.
Thus, where x = −t2, so for some ξ such that −t2 ≤ξ ≤0
e−t2 = pn(−t2) + en(−t2),
and hence
erf(x) =
2
√π
 x
0
pn(−t2) dt
#
$%
&
=qn(x)
+
2
√π
 x
0
en(−t2) dt
#
$%
&
=ϵn(x)
,
where the degree n polynomial
qn(x) =
2
√π
 x
0
 n

k=0
(−1)kt2k
k!
	
dt =
2
√π
n

k=0
(−1)kx2k+1
k!(2k + 1)
is the approximation, and we are interested in the error
ϵn(x) = erf(x) −qn(x) =
2
√π
 x
0
en(−t2) dt.
Clearly
ϵn(x) =
2
√π
(−1)n+1
(n + 1)!
 x
0
t2n+2eξ dt,
where we recall that ξ depends on t in that −t2 ≤ξ ≤0. There is an integral
mean-value theorem, which states that for f (t), g(t) ∈C[a, b] (and g(t) does not
change sign on the interval [a, b]) there is a ζ ∈[a, b] such that
 b
a
g(t)f (t) dt = f (ζ)
 b
a
g(t) dt.
Thus, there is a ζ ∈[−x2, 0], giving
ϵn(x) =
2
√π eζ (−1)n+1
(n + 1)!
x2n+3
2n + 3.
Naturally the error expression in Example 3.9 can be used to estimate how many
terms one must keep in the series expansion (3.108) in order to compute erf(x) to
a desired accuracy.
TLFeBOOK

ASYMPTOTIC SERIES
97
3.6
ASYMPTOTIC SERIES
The Taylor series expansions of Section 3.5 might have a large radius of conver-
gence, but practically speaking, if x is sufﬁciently far from x0, then many many
terms may be needed in a computer implementation to converge to the correct
solution with adequate accuracy. This is highly inefﬁcient. Also, if many terms
are to be retained, then rounding errors might accumulate and destroy the result.
In other words, Taylor series approximations are really effective only for x suf-
ﬁciently close to x0 (i.e., “small x”). We therefore seek expansion methods that
give good approximations for large values of the argument x. These are called the
asymptotic expansions, or asymptotic series. This section is just a quick introduction
based mainly on Section 19.15 in Kreyszig [24]. Another source of information on
asymptotic expansions, although applied mainly to problems involving differential
equations, appears in Lakin and Sanchez [25].
Asymptotic expansions may take on different forms. That is, there are different
“varieties” of such expansions. (This is apparent in Ref. 25.) However, we will
focus on the following deﬁnition.
Deﬁnition 3.5: A series of the form
∞

k=0
ck
xk
(3.112)
for which ck ∈R (real-valued constants), and x ∈R is called an asymptotic expan-
sion, or asymptotic series, of a function f (x), which is deﬁned for all sufﬁciently
large x if, for every n ∈Z+

f (x) −
' n

k=0
ck
xk
(	
xn →0
as
x →∞,
(3.113)
and we shall then write
f (x) ∼
∞

k=0
ck
xk .
It is to be emphasized that the series (3.112) need not converge for any x. The
condition (3.113) suggests a possible method of ﬁnding sequence (ck). Speciﬁcally
f (x) −c0 →0
or
c0 = lim
x→∞f (x),

f (x) −c0 −c1
x

x →0
or
c1 = lim
x→∞[f (x) −c0]x,

f (x) −c0 −c1
x −c2
x2

x2 →0
or
c2 = lim
x→∞

f (x) −c0 −c1
x

x2,
TLFeBOOK

98
SEQUENCES AND SERIES
or in general
cn = lim
x→∞

f (x) −
n−1

k=0
ck
xk
	
xn
(3.114)
for n ≥1. However, this recursive procedure is seldom practical for generating
more than the ﬁrst few series coefﬁcients. Of course, in some cases this might
be all that is needed. We remark that Deﬁnition 3.5 can be usefully extended
according to
f (x) ∼g(x) + h(x)
 ∞

k=0
ck
xk
	
(3.115)
for which
f (x) −g(x)
h(x)
∼
∞

k=0
ck
xk .
(3.116)
The single most generally useful method for getting (ck) is probably to use “inte-
gration by parts.” This is illustrated with examples.
Example 3.10
Recall erf(x) from (3.107). We would like to evaluate this func-
tion for large x [whereas the series in (3.108) is better suited for small x; see the
error expression in Example 3.9]. In this regard it is preferable to work with the
complementary error function
erfc(x) = 1 −erf(x) =
2
√π
 ∞
x
e−t2 dt.
(3.117)
We observe that erf(∞) = 1 [via (3.103)]. Now let τ = t2, so that dt = 1
2τ −1/2dτ.
With this change of variable
erfc(x) =
1
√π
 ∞
x2 τ −1
2 e−τ dτ.
(3.118)
Now observe that via integration by parts, we have
 ∞
x2 τ −1
2 e−τ dτ = −τ −1/2e−τ|∞
x2 −1
2
 ∞
x2 τ −3
2 e−τ dτ
= 1
x e−x2 −1
2
 ∞
x2 τ −3
2 e−τ dτ,
 ∞
x2 τ −3
2 e−τ dτ = −τ −3/2e−τ|∞
x2 −3
2
 ∞
x2 τ −5
2 e−τ dτ
= 1
x3 e−x2 −3
2
 ∞
x2 τ −5
2 e−τ dτ,
TLFeBOOK

ASYMPTOTIC SERIES
99
and so on. We observe that this process of successive integration by parts has
generated integrals of the form
Fn(x) =
 ∞
x2 τ −(2n+1)/2e−τ dτ
(3.119)
for n ∈Z+, and we see that erfc(x) =
1
√π F0(x). So, if we apply integration by
parts to Fn(x), then
Fn(x) =
 ∞
x2 τ −(2n+1)/2e−τ dτ
= −τ −(2n+1)/2e−τ|∞
x2 −2n + 1
2
 ∞
x2 τ −(2n+3)/2e−τ dτ
= x−(2n+1)e−x2 −2n + 1
2
 ∞
x2 τ −(2n+3)/2e−τ dτ
so that we have the recursive expression
Fn(x) =
1
x2n+1 e−x2 −2n + 1
2
Fn+1(x)
(3.120)
which holds for n ∈Z+. This may be rewritten as
ex2Fn(x) =
1
x2n+1 −2n + 1
2
ex2Fn+1(x).
(3.121)
Repeated application of (3.121) yields
ex2F0(x) = 1
x −1
2ex2F1(x),
ex2F0(x) = 1
x −
1
2x3 + 1 · 3
22 ex2F2(x),
ex2F0(x) = 1
x −
1
2x3 + 1 · 3
22x5 −1 · 3 · 5
28
ex2F3(x),
and so ﬁnally
ex2F0(x) =
1
x −
1
2x3 + 1 · 3
22x5 −· · · + (−1)n−1 1 · 3 · · · (2n −3)
2n−1x2n−1

#
$%
&
=S2n−1(x)
+ (−1)n 1 · 3 · · · (2n −1)
2n
ex2Fn(x).
(3.122)
TLFeBOOK

100
SEQUENCES AND SERIES
From this it appears that our asymptotic expansion is
ex2F0(x) ∼1
x −
1
2x3 + 1 · 3
22x5 −· · · + (−1)n−1 1 · 3 · · · (2n −3)
2n−1x2n−1
+ · · ·
(3.123)
However, this requires conﬁrmation. Deﬁne Kn = (−2)−n[1 · 3 · · · (2n −1)]. From
(3.122), we have
[ex2F0(x) −S2n−1(x)]x2n−1 = Knex2x2n−1Fn(x).
(3.124)
We wish to show that for any ﬁxed n = 1, 2, 3, . . . the expression in (3.124) on
the right of the equality goes to zero as x →∞. In (3.119) we have
1
τ (2n+1)/2 ≤
1
x2n+1
for all τ ≥x2, which gives the bound
Fn(x) =
 ∞
x2
e−τ
τ (2n+1)/2 dτ ≤
1
x2n+1
 ∞
x2 e−τ dτ = e−x2
x2n+1 .
(3.125)
But this implies that
|Kn|ex2x2n−1Fn(x) ≤|Kn|ex2x2n−1 e−x2
x2n+1 = |Kn|
x2
and |Kn|
x2 →0 for x →∞. Thus, immediately, (3.123) is indeed the asymptotic
expansion for ex2F0(x). Hence
erfc(x) ∼
1
√π e−x2  1
x −
1
2x3 + 1 · 3
22x5 −· · · + (−1)n−1 1 · 3 · · · (2n −3)
2n−1x2n−1
+ · · ·

.
(3.126)
We recall from Section 3.4 that the integral
" x
0
sin t
t
dt was important in analyzing
the Gibbs phenomenon in Fourier series expansions. We now consider asymptotic
approximations to this integral.
Example 3.11
The sine integral is
Si(x) =
 x
0
sin t
t
dt,
(3.127)
and the complementary sine integral is
si(x) =
 ∞
x
sin t
t
dt.
(3.128)
TLFeBOOK

ASYMPTOTIC SERIES
101
It turns out that si(0) = π
2 , which is shown on p. 277 of Spiegel [22]. We wish to
ﬁnd an asymptotic series for Si(x). Since
π
2 =
 ∞
0
sin t
t
dt =
 x
0
sin t
t
dt +
 ∞
x
sin t
t
dt = Si(x) + si(x),
(3.129)
we will consider the expansion of si(x). If we integrate by parts in succession, then
 ∞
x
t−1 sin t dt = 1
x cos x −1 ·
 ∞
x
1
t2 cos t dt,
 ∞
x
t−2 cos t dt = −1
x2 sin x + 2 ·
 ∞
x
1
t3 sin t dt,
 ∞
x
t−3 sin t dt = 1
x3 cos x −3 ·
 ∞
x
1
t4 cos t dt,
 ∞
x
t−4 cos t dt = −1
x4 sin x + 4 ·
 ∞
x
1
t5 sin t dt,
and so on. If n ∈N, then we may deﬁne
sn(x) =
 ∞
x
t−n sin t dt,
cn(x) =
 ∞
x
t−n cos t dt.
(3.130)
Therefore, for odd n
sn(x) = 1
xn cos x −n cn+1(x),
(3.131a)
and for even n
cn(x) = −1
xn sin x + n sn+1(x).
(3.131b)
We observe that s1(x) = si(x). Repeated application of the recursions (3.131a,b)
results in
s1(x) = 1 · 1
x
cos x + 1 · 1
x2 sin x −1 · 2s3(x)
= 1 · 1
x
cos x + 1 · 1
x2 sin x −1 · 2
x3 cos x −1 · 2 · 3
x4
sin x + 1 · 2 · 3 · 4s5(x),
or in general
s1(x) = cos x
1 · 1
x
−1 · 2
x3 + · · · + (−1)n+1 (2n −2)!
x2n−1

+ sin x
1 · 1
x2 −1 · 2 · 3
x4
+ · · · + (−1)n+1 (2n −1)!
x2n

+ (−1)n(2n)!s2n+1(x)
(3.132)
TLFeBOOK

102
SEQUENCES AND SERIES
for n ∈N (with 0! = 1). From this, we obtain
[s1(x) −S2n(x)]x2n = (−1)n(2n)!x2ns2n+1(x)
(3.133)
for which S2n(x) is appropriately deﬁned as those terms in (3.132) involving sin x
and cos x. It is unclear whether
lim
x→∞x2ns2n+1(x) = 0
(3.134)
for any n ∈N. But if we accept (3.134), then
si(x) ∼S2n(x) = cos x
1 · 1
x
−1 · 2
x3 + · · · + (−1)n+1 (2n −2)!
x2n−1

+ sin x
1 · 1
x2 −1 · 2 · 3
x4
+ · · · + (−1)n+1 (2n −1)!
x2n

. (3.135)
Figure 3.8 shows plots of Si(x) and the asymptotic approximation π
2 −S2n(x) for
n = 1, 2, 3. We observe that the approximations are good for “large x,” but poor
for “small x,” as we would expect. Moreover, for small x, the approximations are
2
3
4
5
6
7
8
9
10
0
0.5
1
1.5
2
x
Amplitude
Si(x)
n = 1
n = 2
n = 3
1.6
1.8
2
2.2
2.4
2.6
2.8
3
−3
−2
−1
0
1
2
x
Amplitude
Si(x)
n = 1
n = 2
n = 3
(a)
(b)
Figure 3.8
Plot of Si(x) using MATLAB routine sinint, and plots of π
2 −S2n(x) for n =
1,2,3. The horizontal solid line is at height π/2.
TLFeBOOK

MORE ON THE DIRICHLET KERNEL
103
better for smaller n. This is also reasonable. Our plots therefore constitute informal
veriﬁcation of the correctness of (3.135), in spite of potential doubts about (3.134).
Another approach to ﬁnding (ck) is based on the fact that many special func-
tions are solutions to particular differential equations. If the originating differential
equation is known, it may be used to generate sequence (ck). However, this section
was intended to be relatively brief, and so this method is omitted. The interested
reader may see Kreyszig [24].
3.7
MORE ON THE DIRICHLET KERNEL
In Section 3.4 the Dirichlet kernel was introduced in order to analyze the manner
in which Fourier series converge in the vicinity of a discontinuity of a 2π-periodic
function. However, this was only done with respect to the special case of g(t) in
(3.19). In this section we consider the Dirichlet kernel Dn(t) in a more general
manner.
We begin by recalling the complex Fourier series expansion from Chapter 1
(Section 1.3.3). If f (t) ∈L2(0, 2π), then
f (t) =
∞

n=−∞
fnejnt,
(3.136)
where fn = ⟨f, en⟩, with en(t) = ejnt, and
⟨x, y⟩= 1
2π
 2π
0
x(t)y∗(t) dt.
(3.137)
An approximation to f (t) is the truncated Fourier series expansion
fL(t) =
L

n=−L
fnejnt ∈L2(0, 2π).
(3.138)
The approximation error is
ϵL(t) = f (t) −fL(t) ∈L2(0, 2π).
(3.139)
We seek a general expression for ϵL(t) that is hopefully more informative than
(3.139). The overall goal is to generalize the error analysis approach seen in
Section 3.4. Therefore, consider
ϵL(t) = f (t) −
L

n=−L
⟨f, en⟩en(t)
TLFeBOOK

104
SEQUENCES AND SERIES
[via fn = ⟨f, en⟩, and (3.138) into (3.139)]. Thus
ϵL(t) = f (t) −
L

n=−L

1
2π
 2π
0
f (x)e−jnx dx

en(t)
= f (t) −1
2π
 2π
0
f (x)

L

n=−L
ejn(t−x)

dx
(3.140)
Since
L

n=−L
ejn(t−x) = 1 + 2
L

n=1
cos[n(t −x)]
(3.141)
(show this as an exercise), via (3.24), we obtain
1 + 2
L

n=1
cos[n(t −x)] = sin[(L + 1
2)(t −x)]
sin[ 1
2(t −x)]
= 2πDL(t −x).
(3.142)
Immediately, we see that
ϵL(t) = f (t) −
 2π
0
f (x)DL(t −x) dx,
(3.143)
where also (recall (3.139))
fL(t) =
 2π
0
f (x)DL(t −x) dx.
(3.144)
Equation (3.144) is an alternative integral form of the approximation to f (t) origi-
nally speciﬁed in (3.138). The integral in (3.144) is really an example of something
called a convolution integral. The following example will demonstrate how we
might apply (3.143).
Example 3.12
Suppose that
f (t) =
 sin t,
0 < t < π
0,
π ≤t < 2π
.
Note that f (t) is continuous for all t, but that f (1)(t) = df (t)/dt is not continuous
everywhere. For example, f (1)(t) is not continuous at t = π. Plots of fL(t) for
various L (see Fig. 3.10) suggest that fL(t) converges most slowly to f (t) near
t = π. Can we say something about the rate of convergence?
TLFeBOOK

MORE ON THE DIRICHLET KERNEL
105
Therefore, consider
fL(π) = 1
2π
 π
0
sin x sin[(L + 1
2)(π −x)]
sin[ 1
2(π −x)]
dx.
(3.145)
Now
sin
1
2(π −x)

= sin
π
2

cos
 1
2x
!
−cos
π
2

sin
 1
2x
!
= cos
 1
2x
!
,
and since sin x = 2 sin( 1
2x) cos( 1
2x) so (3.145) reduces to
fL(π) = 1
π
 π
0
sin
 1
2x
!
sin
 
L + 1
2
!
(π −x)

dx
= 1
2π
 π
0

cos

(L + 1)x −
 
L + 1
2
!
π

−cos

Lx −
 
L + 1
2
!
π

dx
=
1
2π(L + 1)

sin

(L + 1)x −
 
L + 1
2
!
π
π
0
−
1
2πL

sin

Lx −
 
L + 1
2
!
π
π
0
=
1
2π(L + 1)

1 + sin
 
L + 1
2
!
π

−
1
2πL

−1 + sin
 
L + 1
2
!
π

= 1
2π
2L + 1 −(−1)L
L(L + 1)
.
(3.146)
We see that, as expected
lim
L→∞fL(π) = 0.
Also, (3.146) gives ϵL(π) = −fL(π), and this is the exact value for the approxi-
mation error at t = π for all L. Furthermore
|ϵL(π)| ∝1
L
for large L, and so we have a measure of the rate of convergence of the error, at
least at the point t = π. (Symbol “∝” means “proportional to.”)
We remark that f (t) in Example 3.12 may be regarded as the voltage drop
across the resistor R in Fig. 3.9. The circuit in Fig. 3.9 is a simple half-wave
TLFeBOOK

106
SEQUENCES AND SERIES
Ideal diode
R
f(t)
v(t)
+
+
−
−
Figure 3.9
An electronic circuit interpretation for Example 3.12; here, v(t) = sin(t) for
all t ∈R.
0
1
2
3
4
5
6
7
−0.2
0
0.2
0.4
0.6
0.8
1
1.2
t
Amplitude
f(t) = sin(t)
L = 10
L = 3
Figure 3.10
A plot of f (t) and Fourier series approximations to f (t) [i.e., fL(t) for
L = 3, 10].
rectiﬁer circuit. The reader ought to verify as an exercise that
fL(t) = 1
π + 1
2 sin t +
L

n=2
1 + (−1)n
π(1 −n2) cos nt.
(3.147)
Plots of fL(t) for L = 3, 10 versus the plot of f (t) appear in Fig. 3.10.
TLFeBOOK

COORDINATE ROTATION DI GITAL COMPUTING (CORDIC)
107
3.8
FINAL REMARKS
We have seen that sequences and series might converge “mathematically” yet not
“numerically.” Essentially, we have seen three categories of difﬁculty:
1. Pointwise convergence of series leading to irreducible errors in certain regions
of the approximation, such as the Gibbs phenomenon in Fourier expansions,
which arises in the vicinity of discontinuities in the function being approxi-
mated
2. The destructive effect of rounding errors as illustrated by the catastrophic
convergence of series
3. Slow convergence such as illustrated by the problem of computing π with
the Maclaurin expansion for tan−1 x
We have seen that some ingenuity may be needed to overcome obstacles such
as these. For example, a divide-and-conquer approach helped in the problem of
computing π. In the case of catastrophic convergence, the problem was solved by
changing the computational algorithm. The problem of overcoming Gibbs overshoot
is not considered here. However, it involves seeking uniformly convergent series
approximations.
APPENDIX 3.A
COORDINATE ROTATATION DI GITAL
COMPUTING (CORDIC)
3.A.1
Introduction
This appendix presents the basics of a method for computing “elementary func-
tions,” which includes the problem of rotating vectors in the plane, computing
tan−1 x, sin θ, and cos θ. The method to be used is called “coordinate rotation
digital computing” (CORDIC), and was invented by Jack Volder [3] in the late
1950s. However, in spite of the age of the method, it is still important. The method
is one of those great ideas that is able to survive despite technological changes. It
is a good example of how a clever mathematical idea, if anything, becomes less
obsolete with the passage of time.
The CORDIC method was, and is, desirable because it reduces the problem of
computing apparently complicated functions, such as trig functions, to a succession
of simple operations. Speciﬁcally, these simple operations are shifting and adding.
In the 1950s it was a major achievement just to build systems that could add two
numbers together because all that was available for use was vacuum tubes, and to a
lesser degree, discrete transistors. However, even with the enormous improvements
in computing technology that have occurred since then, it is still important to reduce
complicated operations to simple ones. Thus, the CORDIC method has survived
very well. For example, in 1980 [5] a special-purpose CORDIC VLSI (very large
scale integration) chip was presented. More recent references to the method will
TLFeBOOK

108
SEQUENCES AND SERIES
be given later. Nowadays, the CORDIC method is more likely to be implemented
with gate-array technology.
Since the CORDIC method involves the operations of shifting and adding only,
once the mathematics of the method is understood, it is easy to build CORDIC
computing hardware using the logic design methods considered in typical elemen-
tary digital logic courses or books.5 Consideration of CORDIC computing also
makes a connection between elementary mathematics courses (calculus and linear
algebra) and computer hardware systems design, as well as the subject of numerical
analysis.
3.A.2
The Concept of a Discrete Basis
The original paper of Volder [3] does not give a rigorous treatment of the mathe-
matics of the CORDIC method. However, in this section (based closely on Schelin
[6]), we will begin the process of deriving the CORDIC method in a mathemati-
cally rigorous manner. The central idea is to represent operands (e.g, the θ in sin θ)
in terms of a discrete basis. When the discrete basis representation is combined
with appropriate mathematical identities the CORDIC algorithm results.
Let R denote the set of real numbers. Everything we do here revolves around
the following theorem (From Schelin [6]).
Theorem 3.A.1: Suppose that θk ∈R for k ∈{0, 1, 2, 3, . . .} satisfy θ0 ≥θ1 ≥
θ2 ≥· · · ≥θn > 0, and that
θk ≤
n

j=k+1
θj + θn
for
0 ≤k ≤n,
(3.A.1a)
and suppose θ ∈R satisﬁes
|θ| ≤
n

j=0
θj.
(3.A.1b)
If θ(0) = 0, and θ(k+1) = θ(k) + δkθk for 0 ≤k ≤n, where
δk =

1,
if θ ≥θ(k)
−1,
if θ < θ(k)
(3.A.1c)
then
|θ −θ(k)| ≤
n

j=k
θj + θn
for
0 ≤k ≤n,
(3.A.1d)
and so in particular |θ −θ(n+1)| ≤θn.
5The method is also easy to implement in an assembly language (or other low-level) programming
environment.
TLFeBOOK

COORDINATE ROTATION DI GITAL COMPUTING (CORDIC)
109
Proof
We may use proof by mathematical induction6 on the index k. For k = 0
|θ −θ(0)| = |θ| ≤
n

j=0
θj <
n

j=0
θj + θn
via (3.A.1b).
Assume that |θ −θ(k)| ≤n
j=k θj + θn is true, and consider |θ −θ(k+1)|. Via
(3.A.1c), δk and θ −θ(k) have the same sign, and so
|θ −θ(k+1)| = |θ −θ(k) −δkθk| = ||θ −θ(k)| −θk|.
Now, via the inductive hypothesis (i.e., |θ −θ(k)| ≤n
j=k θj + θn), we have
|θ −θ(k)| −θk ≤
n

j=k
θj + θn −θk =
n

j=k+1
θj + θn.
(3.A.2a)
Via (3.A.1a), we obtain
−


n

j=k+1
θj + θn

≤−θk
so that
−


n

j=k+1
θj + θn

≤|θ −θ(k)| −θk.
(3.A.2b)
Combining (3.A.2a) with (3.A.2b) gives
|θ −θ(k+1)| = ||θ −θ(k)| −θk| ≤
n

j=k+1
θj + θn
so that (3.A.1d) holds for k replaced by k + 1, and so (3.A.1d) holds via induction.
We will call this result Schelin’s theorem. The set {θk} is called a discrete basis
if it satisﬁes the restrictions given in the theorem.
In what follows we will interpret θ as an angle. However, note that θ in this
theorem could be more general than this. Now suppose that we deﬁne
θk = tan−1 2−k
(3.A.3)
for k = 0, 1, . . . , n. Table 3.A.1 shows typical values for θk as given by (3.A.3).
We see that good approximations to θ can be obtained from relatively small n
(because from Schelin’s theorem |θ −θ(n+1)| ≤θn).
6A brief introduction to proof by mathematical induction appears in Appendix 3.B.
TLFeBOOK

110
SEQUENCES AND SERIES
TABLE 3.A.1
Values
for Some Elements of
the Discrete Basis given
by Eq. (3.A.3)
k
θk = tan−1 2−k
0
45◦
1
26.565◦
2
14.036◦
3
7.1250◦
4
3.5763◦
Clearly, these satisfy θ0 ≥θ1 ≥· · · ≥θn > 0, which is one of the restrictions in
Schelin’s theorem.
The mean-value theorem of elementary calculus says that there exists a ξ ∈
[x0, x0 + ] such that
df (x)
dx

x=ξ
= f (x0 + ) −f (x0)

so if f (x) = tan−1 x, then df (x)/dx = 1/(1 + x2), and thus
1
1 + x2

x∈[2−(k+1),2−k]
=
θk −θk+1
2−k −2−(k+1)
⇒
θk −θk+1
2−k −2−(k+1) ≤
1
1 + x2

x=2−(k+1)
because the slope of tan−1 x is largest at x = 2−(k+1), and this in turn implies
θk −θk+1 ≤2−k −2−(k+1)
1 + 2−2(k+1) = 2k+2 −2k+1
1 + 22(k+1) =
2k+1
1 + 22(k+1)
and
θk ≥
2k
1 + 22k
because tan−1 x ≥x d
dx tan−1 x =
x
1+x2 (x ≥0), for k = 0, 1, . . . , n (let x = 2−k).7
7For x ≥0
1
1 + x2 ≥d
dx
x
1 + x2 =
1 −x2
(1 + x2)2
(certainly 1 ≥1−x2
1+x2 for x ≥0)
⇒
 x
0
1
1 + t2 dt ≥
x
1 + x2 ⇒tan−1 x ≥
x
1 + x2
TLFeBOOK

COORDINATE ROTATION DI GITAL COMPUTING (CORDIC)
111
Now, as a consequence of these results
θk −θn = (θk −θk+1) + (θk+1 −θk+2) + · · · + (θn−2 −θn−1) + (θn−1 −θn)
=
n−1

j=k
(θj −θj+1)
≤
n−1

j=k
2j+1
1 + 22(j+1)
 
θj −θj+1 ≤
2j+1
1 + 22(j+1)
!
=
n

j=k+1
2j
1 + 22j
≤
n

j=k+1
θj
 
θj ≥
2j
1 + 22j
!
implying that
θk ≤
n

j=k+1
θj + θn
for k = 0, 1, . . . , n, and thus (3.A.1a) holds for {θk} in (3.A.3), and so (3.A.3) is
a concrete example of a discrete basis. If you have a pocket calculator handy then
it is easy to verify that
3

j=0
θj = 92.73◦> 90◦,
so we will work only with angles θ that satisfy |θ| ≤90◦. Thus, for such θ, there
exists a sequence {δk} such that δk ∈{−1, +1}, where
θ =
n

k=0
δkθk + ϵn+1 = θ(n+1) + ϵn+1,
(3.A.4)
where |ϵn+1| ≤θn = tan−1 2−n. Equation (3.A.1c) gives us a way to ﬁnd {δk}. We
can also write that any angle θ satisfying |θ| ≤π
2 (radians) can be represented
exactly as
θ =
∞

k=0
δkθk
for appropriately chosen coordinates {δk}.
In the next section we will begin to see how useful it is to be able to represent
angles in terms of the discrete basis given by (3.A.3).
TLFeBOOK

112
SEQUENCES AND SERIES
3.A.3
Rotating Vectors in the Plane
No one would disagree that a basic computational problem in electrical and com-
puter engineering is to ﬁnd
z′ = ejθz,
θ ∈R,
(3.A.5)
where j =
√
−1, z = x + jy, and z′ = x′ + jy′ (x, y, x′, y′ ∈R). Certainly, this
problem arises in the phasor analysis of circuits, in computer graphics (to rotate
objects, for example), and it also arises in digital signal processing (DSP) a lot.8
Thus, z and z′ are complex variables. Expressing them in terms of their real and
imaginary parts, we may rewrite (3.A.5) as
x′ + jy′ = (cos θ + j sin θ)(x + jy) = [x cos θ −y sin θ] + j[x sin θ + y cos θ],
and this can be further rewritten as the matrix–vector product
 x′
y′

=
 cos θ
−sin θ
sin θ
cos θ
  x
y

(3.A.6)
which we recognize as the formula for rotating the vector [xy]T in the plane to
[x′y′]T .
Recall the trigonometric identities
sin θ =
tan θ
√
1 + tan2 θ
,
cos θ =
1
√
1 + tan2 θ
,
(3.A.7)
and so for θ = θk in (3.A.3)
sin θk =
2−k
√
1 + 2−2k ,
cos θk =
1
√
1 + 2−2k .
(3.A.8)
Thus, to rotate xk + jyk by angle δkθk to xk+1 + jyk+1 is accomplished via
 xk+1
yk+1

=
1
√
1 + 2−2k

1
−δk2−k
δk2−k
1
  xk
yk

,
(3.A.9)
8In DSP it is often necessary to compute the discrete Fourier transform (DFT), which is an approximation
to the Fourier transform (FT) and is deﬁned by
Xn =
N−1

k=0
exp
 
−j 2πkn
N
xk
!
,
where {xk} is the samples of some analog signal, i.e., xk = x(kT ), where k is an integer and T is a
positive constant; x(t) for t ∈R is the analog signal. Note that additional information about the DFT
appeared in Section 1.4.
TLFeBOOK

COORDINATE ROTATION DI GITAL COMPUTING (CORDIC)
113
where we’ve used (3.A.6). From Schelin’s theorem, θ(n+1) ≈θ, so if we wish to
rotate x0 + jy0 = x + jy by θ(n+1) to xn+1 + jyn+1 (≈x′ + jy′) then via (3.A.9)
 xn+1
yn+1

=
' n
/
k=0
1
√
1 + 2−2k
( 
1
−δn2−n
δn2−n
1

· · ·

1
−δ12−1
δ12−1
1
  1
−δ0
δ0
1
  x0
y0

.
(3.A.10)
Deﬁne
Kn =
n
/
k=0
1
√
1 + 2−2k ,
(3.A.11)
where 0n
k=0 αk = αnαn−1αn−2 · · · α1α0.
Consider
 ˆxn+1
ˆyn+1

=

1
−δn2−n
δn2−n
1

· · ·

1
−δ12−1
δ12−1
1
  1
−δ0
δ0
1
  x0
y0

,
(3.A.12)
which is the same expression as (3.A.10) except that we have dropped the multi-
plication by Kn. We observe that to implement (3.A.12) requires only the simple
(to implement in digital hardware, or assembly language) operations of shifting
and adding. Implementing (3.A.12) rotates x + jy by approximately the desired
amount, but gives a solution vector that is a factor of 1/Kn longer than it should
be. Of course, this can be corrected by multiplying [ˆxn+1
ˆyn+1]T by Kn if desired.
Note that in some applications, this would not be necessary.
Note that
 cos θ
−sin θ
sin θ
cos θ

≈Kn

1
−δn2−n
δn2−n
1

· · ·

1
−δ12−1
δ12−1
1
  1
−δ0
δ0
1

,
(3.A.13)
and so the matrix product in (3.A.12) [or (3.A.10)] represents an efﬁcient approxi-
mate factorization of the rotation operator in (3.A.6). The approximation gets better
and better as n increases, and in the limit as n →∞becomes exact.
The computational complexity of the CORDIC rotation algorithm may be des-
cribed as follows. In Eq. (3.A.10) there are exactly 2n shifts (i.e., multiplications
by 2−k), and 2n + 2 additions, plus two scalings by factor Kn. As well, only n + 1
bits of storage are needed to save the sequence {δk}.
We conclude this section with a numerical example to show how to obtain the
sequence {δk} via (3.A.1c).
TLFeBOOK

114
SEQUENCES AND SERIES
Example 3.A.1
Suppose that we want to rotate a vector by an angle of θ =
20◦, and we decide that n = 4 gives sufﬁcient accuracy for the application at hand.
Via (3.A.1c) and Table 3.A.1, we have
θ(0) = 0◦
θ(1) = θ0 = 45◦as δ0 = +1 since θ ≥θ(0) = 0◦
θ(2) = θ(1) + δ1θ1
= 45◦−26.565◦as δ1 = −1 since θ < θ(1)
= 18.435◦
θ(3) = θ(2) + δ2θ2
= 18.435◦+ 14.036◦as δ2 = +1 since θ ≥θ(2)
= 32.471◦
θ(4) = θ(3) + δ3θ3
= 32.471◦−7.1250◦as δ3 = −1 since θ < θ(3)
= 25.346◦
θ(5) = θ(4) + δ4θ4
= 25.346◦−3.5763◦as δ4 = −1 since θ < θ(4)
= 21.770◦≈θ
Via (3.A.4) and |θ −θ(n+1)| ≤θn
|ϵ5| = 1.770◦< θ4 = 3.5763◦
so the error bound in Schelin’s theorem is actually somewhat conservative, at least
in this special case.
3.A.4
Computing Arctangents
The results in Section 3.A.3 can be modiﬁed to obtain a CORDIC algorithm for
computing an approximation to θ = tan−1(y/x). The idea is to ﬁnd the sequence
{δk|k = 0, 1, . . . , n −1, n} to rotate the vector [ x
y ]T = [ x0
y0 ]T to the
vector [xn
yn]T , where yn ≈0. More speciﬁcally, we would select δk so that
|yk+1| < |yk|.
Let ˆθ denote the approximation to θ. The desired algorithm to compute ˆθ may
be expressed as Pascal-like pseudocode:
ˆθ := 0;
x0 := x; y0 := y;
TLFeBOOK

COORDINATE ROTATION DI GITAL COMPUTING (CORDIC)
115
for k := 0 to n do begin
if yk ≥0 then begin
δk := −1;
end
else begin
δk := +1;
end ;
ˆθ := −δkθk + ˆθ;
xk+1 := xk −δk2−kyk;
yk+1 := δk2−kxk + yk;
end ;
In this pseudocode
 xk+1
yk+1

=

1
−δk2−k
δk2−k
1
  xk
yk

,
and we see that for the manner in which sequence {δk} is constructed by the
pseudocode, the inequality |yk+1| < |yk| is satisﬁed. We choose n to achieve the
desired accuracy of our estimate ˆθ of θ, speciﬁcally, |θ −ˆθ| ≤θn.
3.A.5
Final Remarks
As an exercise, the reader should modify the previous results to determine a
CORDIC method for computing cos θ, and sin θ. [Hint: Take a good look at
(3.A.13).]
The CORDIC philosophy can be extended to the computation of hyperbolic
trigonometric functions, logarithms9 and other functions [4, 7]. It can also perform
multiplication and division (see Table on p. 324 of Schelin [6]). As shown by Hu
and Naganathan [9], the rate of convergence of the CORDIC method can be accel-
erated by a method similar to the Booth algorithm (see pp. 287–289 of Hamacher
et al. [10]) for multiplication. However, this is at the expense of somewhat more
complicated hardware structures. A roundoff error analysis of the CORDIC method
has been performed by Hu [8]. We do not present these results in this book as they
are quite involved. Hu claims to have fairly tight bounds on the errors, however.
Fixed-point and ﬂoating-point schemes are both analyzed. A tutorial presentation
of CORDIC-based VLSI architectures for digital signal processing applications
appears in Hu [11]. Other papers on the CORDIC method are those by Timmer-
mann et al. [12] and Lee and Lang [13] (which appeared in the IEEE Transactions
on Computers, “Special Issue on Computer Arithmetic” of August 1992). An alter-
native summary of the CORDIC method may be found in Hwang [14]. Many of
the ideas in Hu’s paper [11] are applicable in a gate-array technology environ-
ment. Applications include the computation of discrete transforms (e.g., the DFT),
digital ﬁltering, adaptive ﬁltering, Kalman ﬁltering, the solution of special linear
9A clever alternative to the CORDIC approach for log calculations appears in Lo and Chen [15], and
a method of computing square roots without division appears in Mikami et al. [16].
TLFeBOOK

116
SEQUENCES AND SERIES
systems of equations (e.g., Toeplitz), deconvolution, and eigenvalue and singular
value decompositions.
APPENDIX 3.B
MATHEMATICAL INDUCTION
The basic idea of mathematical induction is as follows. Assume that we are given
a sequence of statements
S0, S1, . . . , Sn, . . .
and each Si is true or it is false. To prove that all of the statements are true (i.e.,
to prove that Sn is true for all n) by induction: (1) prove that Sn is true for n = 0,
and then (2) assume that Sn is true for any n = k and then show that Sn is true for
n = k + 1.
Example 3.B.1
Prove
Sn =
n

i=0
i2 = n(n + 1)(2n + 1)
6
,
n ≥0.
Proof
We will use induction, but note that there are other methods (e.g., via z
transforms). For n = 0, we obtain
S0 =
0

i=0
i2 = 0
and
n(n + 1)(2n + 1)
6
|n=0 = 0.
Thus, Sn is certainly true for n = 0.
Assume now that Sn is true for n = k so that
Sk =
k

i=0
i2 = k(k + 1)(2k + 1)
6
.
(3.B.1)
We have
Sk+1 =
k+1

i=0
i2 =
k

i=0
i2 + (k + 1)2 = Sk + (k + 1)2,
and so
Sk + (k + 1)2 = k(k + 1)(2k + 1)
6
+ (k + 1)2 = (k + 1)(k + 2)(2k + 3)
6
= n(n + 1)(2n + 1)
6
|n=k+1
where we have used (3.B.1).
TLFeBOOK

CATASTROPHIC CANCELLATION
117
Therefore, Sn is true for n = k + 1 if Sn is true for n = k. Therefore, Sn is true
for all n ≥0 by induction.
APPENDIX 3.C
CATASTROPHIC CANCELLATION
The phenomenon of catastrophic cancellation is illustrated in the following out-
put from a MATLAB implementation that ran on a Sun Microsystems Ultra 10
workstation using MATLAB version 6.0.0.88, release 12, in an attempt to compute
exp(−20) using the Maclaurin series for exp(x) directly:
----------------------------------
term k
x^k/ k !
----------------------------------
0
1.000000000000000
1
-20.000000000000000
2
200.000000000000000
3
-1333.333333333333258
4
6666.666666666666970
5
-26666.666666666667879
6
88888.888888888890506
7
-253968.253968253964558
8
634920.634920634911396
9
-1410934.744268077658489
10
2821869.488536155316979
11
-5130671.797338464297354
12
8551119.662230772897601
13
-13155568.711124267429113
14
18793669.587320379912853
15
-25058226.116427175700665
16
31322782.645533967763186
17
-36850332.524157606065273
18
40944813.915730677545071
19
-43099804.121821768581867
20
43099804.121821768581867
21
-41047432.496973112225533
22
37315847.724521011114120
23
-32448563.238713916391134
24
27040469.365594934672117
25
-21632375.492475949227810
26
16640288.840366113930941
27
-12326139.881752677261829
28
8804385.629823340103030
29
-6071990.089533339254558
30
4047993.393022226169705
31
-2611608.640659500379115
32
1632255.400412187911570
33
-989245.697219507652335
34
581909.233658534009010
35
-332519.562090590829030
36
184733.090050328260986
37
-99855.724351528784609
38
52555.644395541465201
TLFeBOOK

118
SEQUENCES AND SERIES
39
-26951.612510534087050
40
13475.806255267041706
41
-6573.564026959533294
42
3130.268584266443668
43
-1455.938876402997266
44
661.790398364998850
45
-294.129065939999407
46
127.882202582608457
47
-54.417958545790832
48
22.674149394079514
49
-9.254754854726333
50
3.701901941890533
51
-1.451726251721777
52
0.558356250662222
53
-0.210700471948008
54
0.078037211832596
55
-0.028377167939126
56
0.010134702835402
57
-0.003556036082597
58
0.001226219338827
59
-0.000415667572484
60
0.000138555857495
61
-0.000045428149998
62
0.000014654241935
63
-0.000004652140297
64
0.000001453793843
65
-0.000000447321182
66
0.000000135551873
67
-0.000000040463246
68
0.000000011900955
69
-0.000000003449552
70
0.000000000985586
71
-0.000000000277630
72
0.000000000077119
73
-0.000000000021129
74
0.000000000005710
75
-0.000000000001523
76
0.000000000000401
77
-0.000000000000104
78
0.000000000000027
79
-0.000000000000007
80
0.000000000000002
81
-0.000000000000000
82
0.000000000000000
83
-0.000000000000000
84
0.000000000000000
85
-0.000000000000000
86
0.000000000000000
87
-0.000000000000000
88
0.000000000000000
exp(-20) from sum of the above terms = 0.000000004173637
True value of exp(-20)
= 0.000000002061154
TLFeBOOK

REFERENCES
119
REFERENCES
1. E. Kreyszig, Introductory Functional Analysis with Applications, Wiley, New York,
1978.
2. W. Rudin, Principles of Mathematical Analysis, 3rd ed., McGraw-Hill, New York, 1976.
3. J. E. Volder, “The CORDIC Trigonometric Computing Technique,” IRE Trans. Electron.
Comput. EC-8, 330–334 (Sept. 1959).
4. J. S. Walther, “A Uniﬁed Algorithm for Elementary Functions,” AFIPS Conf. Proc.,
Vol. 38, 1971 Spring Joint Computer Conf., 379–385 May 18–20, 1971.
5. G. L. Haviland and A. A. Tuszynski, “A CORDIC Arithmetic Processor Chip,” IEEE
Trans. Comput. C-29, 68–79 (Feb. 1980).
6. C. W. Schelin, “Calculator Function Approximation,” Am. Math. Monthly 90, 317–325
(May 1983).
7. J.-M. Muller, “Discrete Basis and Computation of Elementary Functions,” IEEE Trans.
Comput. C-34, 857–862 (Sept. 1985).
8. Y. H. Hu, “The Quantization Effects of the CORDIC Algorithm,” IEEE Trans. Signal
Process. 40, 834–844 (April 1992).
9. Y. H. Hu and S. Naganathan, “An Angle Recoding Method for CORDIC Algorithm
Implementation,” IEEE Trans. Comput. 42, 99–102 (Jan. 1993).
10. V. C. Hamacher, Z. G. Vranesic, and S. G. Zaky, Computer Organization, 3rd ed.,
McGraw-Hill, New York, 1990.
11. Y. H. Hu, “CORDIC-Based VLSI Architectures for Digital Signal Processing,” IEEE
Signal Process. Mag. 9, 16–35 (July 1992).
12. D. Timmermann, H. Hahn, and B. J. Hosticka, “Low Latency Time CORDIC Algo-
rithms,” IEEE Trans. Comput. 41, 1010–1015 (Aug. 1992).
13. J. Lee and T. Lang, “Constant-Factor Redundant CORDIC for Angle Calculation and
Rotation,” IEEE Trans. Comput. 41, 1016–1025 (Aug. 1992).
14. K. Hwang, Computer Arithmetic: Principles, Architecture, and Design, Wiley, New
York, 1979.
15. H.-Y. Lo and J.-L. Chen, “A Hardwired Generalized Algorithm for Generating the Log-
arithm Base-k by Iteration,” IEEE Trans. Comput. C-36, 1363–1367 (Nov. 1987).
16. N. Mikami, M. Kobayashi, and Y. Yokoyama, “A New DSP-Oriented Algorithm for
Calculation of the Square Root Using a Nonlinear Digital Filter,” IEEE Trans. Signal
Process. 40, 1663–1669 (July 1992).
17. G. G. Walter, Wavelets and Other Orthogonal Systems with Applications, CRC Press,
Boca Raton, FL, 1994.
18. I. S. Gradshteyn and I. M. Ryzhik, in Table of Integrals, Series and Products, A. Jeffrey,
ed., 5th ed., Academic Press, San Diego, CA, 1994.
19. L. Bers, Calculus: Preliminary Edition, Vol. 2, Holt, Rinehart, Winston, New York, 1967.
20. A. Leon-Garcia, Probability and Random Processes for Electrical Engineering, 2nd ed.,
Addison-Wesley, Reading, MA, 1994.
21. G. E. Forsythe, M. A. Malcolm, and C. B. Moler, Computer Methods for Mathematical
Computations, Prentice-Hall, Englewood Cliffs, NJ, 1977.
22. M. R. Spiegel, Theory and Problems of Advanced Calculus (Schaum’s Outline Series).
Schaum (McGraw-Hill), New York, 1963.
TLFeBOOK

120
SEQUENCES AND SERIES
23. A. Papoulis, Signal Analysis, McGraw-Hill, New York, 1977.
24. E. Kreyszig, Advanced Engineering Mathematics, 4th ed., Wiley, New York, 1979.
25. W. D. Lakin and D. A. Sanchez, Topics in Ordinary Differential Equations, Dover Pub-
lications, New York, 1970.
PROBLEMS
3.1. Prove the following theorem: Every convergent sequence in a metric space
is a Cauchy sequence.
3.2. Let fn(x) = xn for n ∈{1, 2, 3, . . .} = N, and fn(x) ∈C[0, 1] for all n ∈N.
(a) What is f (x) = limn→∞fn(x) (for x ∈[0, 1])?
(b) Is f (x) ∈C[0, 1]?
3.3. Sequence (xn) is deﬁned to be xn = (n + 1)/(n + 2) for n ∈Z+. Clearly, if
X = [0, 1) ⊂R, then xn ∈X for all n ∈Z+. Assume the metric for metric
space X is d(x, y) = |x −y| (x, y ∈X).
(a) What is x = limn→∞xn?
(b) Is X a complete space?
(c) Prove that (xn) is Cauchy.
3.4. Recall Section 3.A.3 wherein the rotation operator was deﬁned [Eq. (3.A.6)].
(a) Find an expression for angle θ such that for y ̸= 0
 cos θ
−sin θ
sin θ
cos θ

#
$%
&
=G(θ)
 x
y

=
 x′
0

,
where x′ is some arbitrary nonzero constant.
(b) Prove that G−1(θ) = GT (θ) [i.e., the inverse of G(θ) is given by its
transpose].
(c) Consider the matrix
A =


4
2
0
1
4
1
0
2
4

.
Let 0n×m denote an array (matrix) of zeros with n rows, and m columns.
Find G(θ1), and G(θ2) so that

1
01×2
02×1
G(θ2)
  G(θ1)
02×1
01×2
1

#
$%
&
=QT
A = R,
where R is an upper triangular matrix (deﬁned in Section 4.5, if you do
not recall what this is).
TLFeBOOK

PROBLEMS
121
(d) Find Q−T (inverse of QT ).
(Comment: The procedure illustrated by this problem is important in various
applications such as solving least-squares approximations, and in ﬁnding the
eigenvalues and eigenvectors of matrices.)
3.5. Review Appendix 3.A. Suppose that we wish to rotate a vector [xy]T ∈R2
through an angle θ = 25◦± 1◦. Find n, and the required delta sequence (δk)
to achieve this accuracy.
3.6. Review Appendix 3.A. A certain CORDIC routine has the following pseu-
docode description:
Input x and z (|z| ≤1);
y0 := 0; z0 := z;
for k := 0 to n −1 do begin
if zk < 0 then begin
δk := −1;
end
else begin
δk := +1;
end;
yk+1 := yk + δkx2−k;
zk+1 := zk −δk2−k;
end;
The algorithm’s output is yn. What is yn?
3.7. Suppose that
xn(t) =
t
n + t
for n ∈Z+, and t ∈(0, 1) ⊂R. Show that (xn(t)) is uniformly convergent
on S = (0, 1).
3.8. Suppose that un > 0, and also that ∞
n=0 un converges. Prove that 0∞
n=0(1 +
un) converges.
[Hint: Recall Lemma 2.1 (of Chapter 2).]
3.9. Prove that limn→∞xn = 0 if |x| < 1.
3.10. Prove that for a, b, x ∈R
1
1 + |a −b| ≥
1
1 + |a −x|
1
1 + |b −x|.
(Comment: This inequality often appears in the context of convergence proofs
for certain series expansions.)
TLFeBOOK

122
SEQUENCES AND SERIES
3.11. Consider the function
KN(x) =
2π
N + 1
N

n=0
Dn(x)
[recall (3.24)]. Since Dn(x) is 2π-periodic, KN(x) is also 2π-periodic. We
may assume that x ∈[−π, π].
(a) Prove that
KN(x) =
1
N + 1
1 −cos(N + 1)x
1 −cos x
.
[Hint: Consider N
n=0 sin

(n + 1
2)x

= Im
N
n=0 ej(n+ 1
2 )x
.]
(b) Prove that
KN(x) ≥0.
(c) Prove that
1
2π
 π
−π
KN(x) dx = 1.
[Comment: The partial sums of the complex Fourier series expansion of the
2π-periodic function f (x) (again x ∈[−π, π]) are given by
fN(x) =
N

n=−N
fnejnx,
where fn =
1
2π
" π
−π f (x)e−jnx dx. Deﬁne
σN(x) =
1
N + 1
N

n=0
fn(x).
It can be shown that
σN(x) = 1
2π
 π
−π
f (x −t)KN(t) dt.
It is also possible to prove that σN(x) →f (x) uniformly on [−π, π] if f (x)
is continuous. This is often called Fej´er’s theorem.]
3.12. Repeat the analysis of Example 3.12 for fL(0).
3.13. If f (t) ∈L2(0, 2π) and f (t) = 
n∈Z fnejnt, show that
1
2π
 2π
0
|f (t)|2 dt =
∞

n=−∞
|fn|2.
TLFeBOOK

PROBLEMS
123
This relates the energy/power of f (t) to the energy/power of its Fourier
series coefﬁcients.
[Comment: For example, if f (t) is one period of a 2π-periodic signal, then
the power interpretation applies. In particular, suppose that f (t) = i(t) a
current waveform, and that this is the current into a resistor of resistance R;
then the average power delivered to R is
Pav = 1
2π
 2π
0
Ri2(t) dt.]
3.14. Use the result of Example 1.20 to prove that
∞

n=0
(−1)n
2n + 1 = π
4 .
3.15. Prove that
1
2π
 π
0
sin[(L + 1
2)t]
sin[ 1
2t]
dt = 1
2.
3.16. Use mathematical induction to prove that
(1.1)n ≥1 + 1
10n
for all n ∈N.
3.17. Use mathematical induction to prove that
(1 + h)n ≥1 + nh
for all n ∈N, with h ≥−1. (This is Bernoulli’s inequality.)
3.18. Use mathematical induction to prove that 4n + 2 is a multiple of 6 for all
n ∈N.
3.19. Conjecture a formula for (n ∈N)
Sn =
n

k=1
1
k(k + 1),
and prove it using mathematical induction.
3.20. Suppose that f (x) = tan x. Use (3.55) to approximate f (x) for all x ∈
(−1, 1). Find an upper bound on |e(x)|, where e(x) is the error of the
approximation.
TLFeBOOK

124
SEQUENCES AND SERIES
3.21. Given f (x) = x2 + 1, ﬁnd all ξ ∈(1, 2) such that
f (1)(ξ) = f (2) −f (1)
2 −1
.
3.22. Use (3.65) to ﬁnd an approximation to
√
1 + x for x ∈( 1
4, 7
4). Find an upper
bound on the magnitude of e(x).
3.23. Using a pocket calculator, compute (0.97)1/3 for n = 3 using (3.82). Find
upper and lower bounds on the error en+1(x).
3.24. Using a pocket calculator, compute [1.05]1/4 using (3.82). Choose n = 2 (i.e.,
quadratic approximation). Estimate the error involved in doing this using the
error bound expression. Compare the bounds to the actual error.
3.25. Show that
n

k=0
 n
k
!
= 2n.
3.26. Show that
n
j
 n −1
j −1
!
=
 n
j
!
for j = 1, 2, . . . , n −1, n.
3.27. Show that for n ≥2
n

k=1
k
 n
k
!
= n2n−1.
3.28. For
pn(k) =
 n
k
!
pk(1 −p)n−k, 0 ≤p ≤1,
where k = 0, 1, . . . , n, show that
pn(k + 1) =
(n −k)p
(k + 1)(1 −p)pn(k).
[Comment: This recursive approach for ﬁnding pn(k) extends the range of
n for which pn(k) may be computed before experiencing problems with
numerical errors.]
3.29. Identity (3.103) was conﬁrmed using an argument associated with Fig. 3.7. A
somewhat different approach is the following. Conﬁrm (3.103) by working
with

1
√
2π
 ∞
−∞
e−x2/2 dx
2
= 1
2π
 ∞
−∞
 ∞
−∞
e−(x2+y2)/2 dx dy.
TLFeBOOK

PROBLEMS
125
(Hint: Use the Cartesian to polar coordinate conversion x = r cos θ, y =
r sin θ.)
3.30. Find the Maclaurin (inﬁnite) series expansion for f (x) = sin−1 x. What is
the radius of convergence? [Hint: Theorem 3.7 and Eq. (3.82) are use-
ful.]
3.31. From Eq. (3.85) for x > −1
loge(1 + x) =
n

k=1
(−1)k−1xk
k
+ r(x),
(3.P.1)
where
|r(x)| ≤



1
n+1xn+1,
x ≥0
|x|n+1
(1−|x|)(n+1),
−1 < x ≤0
(3.P.2)
[Eq. (3.86)]. For what range of values of x does the series
loge(1 + x) =
∞

k=1
(−1)k−1xk
k
converge? Explain using (3.P.2).
3.32. The following problems are easily worked with a pocket calculator.
(a) It can be shown that
sin(x) =
n

k=0
(−1)k
(2k + 1)!x2k+1 + e2n+3(x),
(3.P.3)
where
|e2n+3(x)| ≤
1
(2n + 3)!|x|2n+3.
(3.P.4)
Use (3.P.4) to compute sin(x) to 3 decimal places of accuracy for x = 1.5
radians. How large does n need to be?
(b) Use the approximation
loge(1 + x) ≈
N

n=1
(−1)n−1xn
n
to compute loge(1.5) to three decimal places of accuracy. How large
should N be to achieve this level of accuracy?
TLFeBOOK

126
SEQUENCES AND SERIES
3.33. Assuming that

2
π
 π
0
sin x
x
dx −2
n

r=0
π2r(−1)r
(2r + 1)(2r + 1)!
 ≤
2π2n+1
(2n + 2)(2n + 2)!, (3.P.5)
show that for suitable n
2
π
 π
0
sin x
x
dx > 1.17.
What is the smallest n needed? Justify Eq. (3.P.5).
3.34. Using integration by parts, ﬁnd the asymptotic expansion of
c(x) =
 ∞
x
cos t2 dt.
3.35. Using integration by parts, ﬁnd the asymptotic expansion of
s(x) =
 ∞
x
sin t2 dt.
3.36. Use MATLAB to plot (on the same graph) function KN(x) in Problem 3.11
for N = 2, 4, 15.
TLFeBOOK

4
Linear Systems of Equations
4.1
INTRODUCTION
The necessity to solve linear systems of equations is commonplace in numerical
computing. This chapter considers a few examples of how such problems arise
(more examples will be seen in subsequent chapters) and the numerical problems
that are frequently associated with attempts to solve them. We are particularly inter-
ested in the phenomenon of ill conditioning. We will largely concentrate on how
the problem arises, what its effects are, and how to test for this problem.1 In addi-
tion to this, we will also consider methods of solving linear systems other than the
Gaussian elimination method that you most likely learned in an elementary linear
algebra course.2 More speciﬁcally, we consider LU and QR matrix factorization
methods, and iterative methods of linear system solution. The concept of a singular
value decomposition (SVD) is also introduced.
We will often employ the term “linear systems” instead of the longer phrase
“linear systems of equations.” However, the reader must be warned that the phrase
“linear systems” can have a different meaning from our present usage. In signals
and systems courses you will most likely see that a “linear system” is either a
continuous-time (i.e., analog) or discrete-time (i.e., digital) dynamic system whose
input/output (I/O) behavior satisﬁes superposition. However, such dynamic systems
can be described in terms of linear systems of equations.
4.2
LEAST-SQUARES APPROXIMATION AND LINEAR SYSTEMS
Suppose that f (x), g(x) ∈L2[0, 1], and that these functions are real-valued. Recall-
ing Chapter 1, their inner product is therefore given by
⟨f, g⟩=
 1
0
f (x)g(x) dx.
(4.1)
1Methods employed to avoid ill-conditioned linear systems of equations will be mainly considered in
a later chapter. These chieﬂy involve working with orthogonal basis sets.
2Review the Gaussian elimination procedure now if necessary. It is mandatory that you recall the basic
matrix and vector operations and properties [(AB)T = BT AT , etc.] from elementary linear algebra,
too.
An Introduction to Numerical Analysis for Electrical and Computer Engineers, by C.J. Zarowski
ISBN 0-471-46737-5
c⃝2004 John Wiley & Sons, Inc.
127
TLFeBOOK

128
LINEAR SYSTEMS OF EQUATIONS
Note that here we will assume that all members of L2[0, 1] are real-valued (for
simplicity). Now assume that {φn(x)|n ∈ZN} form a linearly independent set such
that φn(x) ∈L2[0, 1] for all n. We wish to ﬁnd the coefﬁcients an such that
f (x) ≈
N−1

n=0
anφn(x)
for
x ∈[0, 1].
(4.2)
A popular approach to ﬁnding an [1] is to choose them to minimize the functional
V (a) = ||f (x) −
N−1

n=0
anφn(x)||2,
(4.3)
where, for future convenience, we will treat an as belonging to the vector a =
[a0a1 · · · aN−2aN−1]T ∈RN. Of course, in (4.3) ||f ||2 = ⟨f, f ⟩via (4.1). We are
at liberty to think of
e(x) = f (x) −
N−1

n=0
anφn(x)
(4.4)
as the error between f (x) and its approximation 
n anφn(x). So, our goal is to
pick a to minimize ||e(x)||2, which, we have previously seen, may be interpreted
as the energy of the error e(x). This methodology of approximation is called least-
squares approximation. The version of this that we are now considering is only
one of a great many variations. We will see others later.
We may rewrite (4.3) as follows:
V (a) = ||f (x) −
N−1

n=0
anφn(x)||2
=

f −
N−1

n=0
anφn, f −
N−1

k=0
akφk

= ⟨f, f ⟩−

f,
N−1

k=0
akφk

−
N−1

n=0
anφn, f

+
N−1

n=0
anφn,
N−1

k=0
akφk

= ||f ||2 −
N−1

k=0
ak⟨f, φk⟩−
N−1

n=0
an⟨φn, f ⟩
+
N−1

n=0
N−1

k=0
akan⟨φk, φn⟩
TLFeBOOK

LEAST-SQUARES APPROXIMATION AND LINEAR SYSTEMS
129
= ||f ||2 −2
N−1

k=0
ak⟨f, φk⟩
+
N−1

n=0
N−1

k=0
akan⟨φk, φn⟩.
(4.5)
Naturally, we have made much use of the inner product properties of Chapter 1.
It is very useful to deﬁne
ρ = ||f ||2,
gk = ⟨f, φk⟩,
rn,k = ⟨φn, φk⟩,
(4.6)
along with the vector
g = [g0g1 · · · gN−1]T ∈RN,
(4.7a)
and the matrix
R = [rn,k] ∈RN×N.
(4.7b)
We immediately observe that R = RT (i.e., R is symmetric). This is by virtue of
the fact that rn,k = ⟨φn, φk⟩= ⟨φk, φn⟩= rk,n. Immediately we may rewrite (4.5)
in order to obtain the quadratic form3
V (a) = aT Ra −2aT g + ρ.
(4.8)
The quadratic form occurs very widely in optimization and approximation prob-
lems, and so warrants considerable study. An expanded view of R is
R =


 1
0
φ2
0(x) dx
 1
0
φ0(x)φ1(x) dx
· · ·
 1
0
φ0(x)φN−1(x) dx
 1
0
φ0(x)φ1(x) dx
 1
0
φ2
1(x) dx
· · ·
 1
1
φ1(x)φN−1(x) dx
...
...
...
 1
0
φ0(x)φN−1(x) dx
 1
0
φ1(x)φN−1(x) dx
· · ·
 1
0
φ2
N−1(x) dx


.
(4.9)
Note that the reader must get used to visualizing matrices and vectors in the general
manner now being employed. The practical use of linear/matrix algebra demands
this. Writing programs for anything involving matrix methods (which encompasses
a great deal) is almost impossible without this ability. Moreover, modern software
tools (e.g., MATLAB) assume that the user is skilled in this manner.
3The quadratic form is a generalization of the familiar quadratic ax2 + bx + c, for which x ∈C (if
we are interested in the roots of a quadratic equation; otherwise we usually consider x ∈R) , and also
a, b, c ∈R.
TLFeBOOK

130
LINEAR SYSTEMS OF EQUATIONS
It is essential to us that R−1 exist (i.e., we need R to be nonsingular). Fortu-
nately, this is always the case because {φn|n ∈ZN} is an independent set. We shall
prove this. If R is singular, then there is a set of coefﬁcients αi such that
N−1

i=0
αi⟨φi, φj⟩= 0
(4.10)
for j ∈ZN. This is equivalent to saying that the columns of R are linearly depen-
dent. Now consider the function
ˆf (x) =
N−1

i=0
αiφi(x)
so if (4.10) holds for all j ∈ZN, then
⟨ˆf , φj⟩=
N−1

i=0
αiφi, φj

=
N−1

i=0
αi⟨φi, φj⟩= 0
for all j ∈ZN. Thus
N−1

j=0
αj⟨ˆf , φj⟩=
N−1

j=0
αj
N−1

i=0
αi⟨φi, φj⟩
	
= 0,
implying that
N−1

i=0
αiφi,
N−1

j=0
αjφj

= 0,
or in other words, ⟨ˆf , ˆf ⟩= || ˆf ||2 = 0, and so
ˆf (x) = 0 for all x ∈[0, 1]. This
contradicts the assumption that {φn|n ∈ZN} is a linearly independent set. So R−1
must exist.
From (4.3) it is clear that (via basic norm properties) V (a) ≥0 for all a ∈RN.
If we now assume that f (x) = 0 (all x ∈[0, 1]), we have ρ = 0, and g = 0 too.
Thus, aT Ra ≥0 for all a.
Deﬁnition 4.1: Positive Semideﬁnite Matrix, Positive Deﬁnite Matrix
Sup-
pose that A = AT and that A ∈Rn×n. Suppose that x ∈Rn. We say that A is
positive semideﬁnite (psd) iff
xT Ax ≥0
for all x. We say that A is positive deﬁnite (pd) iff
xT Ax > 0
for all x ̸= 0.
TLFeBOOK

LEAST-SQUARES APPROXIMATION AND LINEAR SYSTEMS
131
If A is psd, we often symbolize this by writing A ≥0, and if A is pd, we often
symbolize this by writing A > 0. If a matrix is pd then it is clearly psd, but the
converse is not necessarily true.
So far it is clear that R ≥0. But in fact R > 0. This follows from the linear
independence of the columns of R. If the columns of R were linearly dependent,
then there would be an a ̸= 0 such that Ra = 0, but we have already shown that
R−1 exists, so it must be so that Ra = 0 iff a = 0. Immediately we conclude that
R is positive deﬁnite.
Why is R > 0 so important? Recall that we may solve ax2 + bx + c = 0 (x ∈C)
by completing the square:
ax2 + bx + c = a

x + b
2a
2
+ c −b2
4a .
(4.11)
Now if a > 0, then y(x) = ax2 + bx + c has a unique minimum. Since y(1)(x) =
2ax + b = 0 for x = −b
2a , this choice of x forces the ﬁrst term of (4.11) (right-hand
side of the equality) to zero, and we see that the minimum value of y(x) is
y
 
−b
2a
!
= c −b2
4a = 4ac −b2
4a
.
(4.12)
Thus, completing the square makes the location of the minimum (if it exists), and
the value of the minimum of a quadratic very obvious. For the same purpose we
may complete the square of (4.8):
V (a) = [a −R−1g]T R[a −R−1g] + ρ −gT R−1g.
(4.13)
It is quite easy to conﬁrm that (4.13) gives (4.8) (so these equations must be
equivalent):
[a −R−1g]T R[a −R−1g] + ρ −gT R−1g
= [aT −gT (R−1)T ][Ra −g] + ρ −gT R−1g
= aT Ra −gT R−1Ra −aT g + gT R−1g + ρ −gT R−1g
= aT Ra −gT a −aT g + ρ = aT Ra −2aT g + ρ.
We have used (R−1)T = (RT )−1 = R−1, and the fact that aT g = gT a. If vector
x = a −R−1g, then
[a −R−1g]T R[a −R−1g] = xT Rx.
So, because R > 0, it follows that xT Rx > 0 for all x ̸= 0. The last two terms of
(4.13) do not depend on a. So we can minimize V (a) only by minimizing the ﬁrst
term. R > 0 implies that this minimum must be for x = 0, implying a = ˆa, where
ˆa −R−1g = 0,
or
R ˆa = g.
(4.14)
TLFeBOOK

132
LINEAR SYSTEMS OF EQUATIONS
Thus
ˆa = argmin
a∈RN
V (a).
(4.15)
We see that to minimize ||e(x)||2 we must solve a linear system of equations,
namely, Eq. (4.14). We remark that for R > 0, the minimum of V (a) is at a unique
location ˆa ∈RN; that is, the minimum is unique.
In principle, solving least-squares approximation problems seems quite simple
because we have systematic (and numerically reliable) methods to solve (4.14)
(e.g., Gaussian elimination with partial pivoting). However, one apparent difﬁculty
is the need to determine various integrals:
gk =
 1
0
f (x)φk(x) dx, rn,k =
 1
0
φn(x)φk(x) dx.
Usually, the independent set {φk|k ∈ZN} is chosen to make ﬁnding rn,k rela-
tively straightforward. In fact, sometimes nice closed-form expressions exist. But
numerical integration is generally needed to ﬁnd gk. Practically, this could involve
applying series expansions such as considered in Chapter 3, or perhaps using
quadratures such as will be considered in a later chapter. Other than this, there
is a more serious problem. This is the problem that R might be ill-conditioned.
4.3
LEAST-SQUARES APPROXIMATION AND ILL-CONDITIONED
LINEAR SYSTEMS
A popular choice for an independent set {φk(x)} would be
φk(x) = xk
for
x ∈[0, 1], k ∈ZN.
(4.16)
Certainly, these functions belong to the inner product space L2[0, 1]. Thus, for
f (x) ∈L2[0, 1] an approximation to it is
ˆf (x) =
N−1

k=0
akxk ∈PN−1[0, 1],
(4.17)
and so we wish to ﬁt a degree N −1 polynomial to f (x). Consequently
gk =
 1
0
xkf (x) dx
(4.18a)
which is sometimes called the kth moment4 of f (x) on [0, 1], and also
rn,k =
 1
0
φn(x)φk(x) dx =
 1
0
xn+k dx =
1
n + k + 1
(4.18b)
for n, k ∈ZN.
4The concept of a moment is also central to probability theory.
TLFeBOOK

LEAST-SQUARES APPROXIMATION AND ILL-CONDITIONED LINEAR SYSTEMS
133
For example, suppose that N = 3 (i.e., a quadratic ﬁt); then
g =


 1
0
f (x) dx
#
$%
&
=g0
 1
0
xf (x) dx
#
$%
&
=g1
 1
0
x2f (x) dx
#
$%
&
=g2


T
,
(4.19a)
and
R =


1
1
2
1
3
1
2
1
3
1
4
1
3
1
4
1
5

=


r00
r01
r02
r10
r11
r12
r20
r21
r22

,
(4.19b)
and ˆa = [ˆa0 ˆa1 ˆa2]T , so we wish to solve


1
1
2
1
3
1
2
1
3
1
4
1
3
1
4
1
5




ˆa0
ˆa1
ˆa2

=


 1
0
f (x) dx
 1
0
xf (x) dx
 1
0
x2f (x) dx


.
(4.20)
We remark that R does not depend on the “data” f (x), only the elements of g
do. This is true in general, and it can be used to advantage. Speciﬁcally, if f (x)
changes frequently (i.e., we must work with different data), but the independent
set does not change, then we need to invert R only once.
Matrix R in (4.19b) is a special case of the famous Hilbert matrix [2–4]. The
general form of this matrix is (for any N ∈N)
R =


1
1
2
1
3
· · ·
1
N
1
2
1
3
1
4
· · ·
1
N + 1
1
3
1
4
1
5
· · ·
1
N + 2
...
...
...
...
1
N
1
N + 1
1
N + 2
· · ·
1
2N −1


∈RN×N.
(4.21)
Thus, (4.20) is a special case of a Hilbert linear system of equations. The matrix R
in (4.21) seems “harmless,” but it is actually a menace from a numerical computing
standpoint. We now demonstrate this concept.
TLFeBOOK

134
LINEAR SYSTEMS OF EQUATIONS
Suppose that our data are something very simple. Say that
f (x) = 1
for all
x ∈[0, 1].
In this case gk =
" 1
0 xk dx =
1
k+1. Therefore, for any N ∈N, we are compelled to
solve


1
1
2
1
3
· · ·
1
N
1
2
1
3
1
4
· · ·
1
N + 1
1
3
1
4
1
5
· · ·
1
N + 2
...
...
...
...
1
N
1
N + 1
1
N + 2
· · ·
1
2N −1




ˆa0
ˆa1
ˆa2
...
ˆaN−1


=


1
1
2
1
3
...
1
N


.
(4.22)
A moment of thought reveals that solving (4.22) is trivial because g is the ﬁrst
column of R. Immediately, we see that
ˆa = [100 · · · 00]T .
(4.23)
(No other solution is possible since R−1 exists, implying that R ˆa = g always
possesses a unique solution.)
MATLAB implements Gaussian elimination (with partial pivoting) using the
operator “\” to solve linear systems. For example, if we want x in Ax = y for
which A−1 exists, then x = A\y. MATLAB also computes R using function “hilb”;
that is, R = hilb(N) will result in R being set to a N × N Hilbert matrix. Using the
MATLAB “\” operator to solve for ˆa in (4.22) gives the expected answer (4.23) for
N ≤50 (at least). The computer-generated answers are correct to several decimal
places. (Note that it is somewhat unusual to want to ﬁt polynomials to data that
are of such large degree.) So far, so good.
Now consider the results in Appendix 4.A. The MATLAB function “inv” may
be used to compute the inverse of matrices. The appendix shows R−1 (computed
via inv) for N = 10, 11, 12, and the MATLAB computed product RR−1 for these
cases. Of course, RR−1 = I (identity matrix) is expected in all cases. For the
number of decimal places shown, we observe that RR−1 ̸= I. Not only that, but
the error E = RR−1 −I rapidly becomes large with an increase in N. For N = 12,
the error is substantial. In fact, the MATLAB function inv has built-in features to
warn of trouble, and it does so for case N = 12. Since RR−1 is not being computed
correctly, something has clearly gone wrong, and this has happened for rather small
values of N. This is in striking contrast with the previous problem, where we wanted
to compute ˆa in (4.22). In this case, apparently, nothing went wrong.
We may consider changing our data to f (x) = xN−1. In this case gk = 1/(N +
k) for k ∈ZN. The vector g in this case will be the last column of R. Thus,
TLFeBOOK

CONDITION NUMBERS
135
mathematically, ˆa = [00 · · · 001]T . If we use MATLAB “\” to compute ˆa for this
problem we obtain the computed solutions:
ˆˆa = [0.0000 0.0000 0.0000 0.0000 . . .
0.0002 −0.0006 0.0013 −0.00170.0014 −0.0007 1.0001]T
(N = 11),
ˆˆa = [0.0000 0.0000 0.0000 −0.0002 . . . 0.0015 −0.0067
0.0187 −0.0342 0.0403 −0.0297 0.0124 0.9978]T (N = 12).
The errors in the computed solutions ˆˆa here are much greater than those experienced
in computing ˆa in (4.23).
It turns out that the Hilbert matrix R is a classical example of an ill-conditioned
matrix (with respect to the problem of solving linear systems of equations). The
linear system in which it resides [i.e., Eq. (4.14)] is therefore an ill-conditioned
linear system. In such systems the ﬁnal answer (which is ˆa here) can be exquisitely
sensitive to very small perturbations (i.e., disturbances) in the inputs. The inputs
in this case are the elements of R and g. From Chapter 2 we remember that R
and g will not have an exact representation on the computer because of quantiza-
tion errors. Additionally, as the computation proceeds rounding errors will cause
further disturbances. The result is that in the end the ﬁnal computed solution can
deviate enormously from the correct mathematical solution. On the other hand, we
have also shown that it is possible for the computed solution to be very close to
the mathematically correct solution even in the presence of ill conditioning. Our
problem then is to be able to detect when ill conditioning arises, and hence might
pose a problem.
4.4
CONDITION NUMBERS
In the previous section there appears to be a problem involved in accurately com-
puting the inverse of R (Hilbert matrix). This was attributed to the so-called ill
conditioning of R. We begin here with some simpler lower-order examples that
illustrate how the solution to a linear system Ax = y can depend sensitively on A
and y. This will lead us to develop a theory of condition numbers that warn us that
the solution x might be inaccurately computed due to this sensitivity.
We will consider Ax = y on the assumption that A ∈Rn×n, and x, y ∈Rn.
Initially we will assume n = 2, so
 a00
a01
a10
a11
  x0
x1

=
 y0
y1

.
(4.24)
In practice, we may be uncertain about the accuracy of the entries of A and y.
Perhaps these entities originate from experimental data. So the entries may be
subject to experimental errors. Additionally, as previously mentioned, the elements
TLFeBOOK

136
LINEAR SYSTEMS OF EQUATIONS
of A and y cannot normally be exactly represented on a computer because of the
need to quantize their entries. Thus, we must consider the perturbed system



 a00
a01
a10
a11

+
 δa00
δa01
δa10
δa11

#
$%
&
=δA



 ˆx0
ˆx1

# $% &
=ˆx
=



 y0
y1

+
 δy0
δy1

#
$%
&
=δy



.
(4.25)
The perturbations are δA and δy. We will assume that these are “small.” As you
might expect, the practical deﬁnition of “small” will force us to deﬁne and work
with suitable norms. This is dealt with below. We further assume that the computing
machine we use to solve (4.25) is a “magical machine” that computes without
rounding errors. Thus, any errors in the computed solution, denoted ˆx here, can be
due only to the perturbations δA and δy. It is our hope that ˆx ≈x. Unfortunately,
this will not always be so, even for n = 2 with small perturbations.
Because n is small, we may obtain closed-form expressions for A−1, x, [A +
δA]−1, and ˆx. More speciﬁcally
A−1 =
1
a00a11 −a01a10

a11
−a01
−a10
a00

,
(4.26)
and
[A + δA]−1 =
1
(a00 + δa00)(a11 + δa11) −(a01 + δa01)(a10 + δa10)
×

a11 + δa11
−(a01 + δa01)
−(a10 + δa10)
(a00 + δa00)

.
(4.27)
The reader can conﬁrm these by multiplying A−1 as given in (4.26) by A. The
2 × 2 identity matrix should be obtained. Using these formulas, we may consider
the following example.
Example 4.1
Suppose that
A =
 1
−.01
2
.01

,
y =
 2
1

.
Nominally, the correct solution is
x =

1
−100

.
Let us consider different perturbation cases:
TLFeBOOK

CONDITION NUMBERS
137
1. Suppose that
δA =
 0
0
0
.005

,
δy = [0
0]T .
In this case
ˆx = [1.1429
−85.7143]T .
2. Suppose that
δA =
 0
0
0
−.03

,
δy = [0
0]T .
for which
A + δA =
 1
−.01
2
−.02

.
This matrix is mathematically singular, so it does not possess an inverse. If
MATLAB tries to compute ˆx using (4.27), then we obtain
ˆx = 1.0 × 1017 × [−0.0865
−8.6469]T
and MATLAB issues a warning that the answer may not be correct. Obvi-
ously, this is truly a nonsense answer.
3. Suppose that
δA =
 0
0
0
−.02

, δy = [0.10
−0.05]T .
In this case
ˆx = [−1.1500
−325.0000]T .
It is clear that small perturbations of A and y can lead to large errors in the
computed value for x. These errors are not a result of accumulated rounding errors
in the computational algorithm for solving the problem. For computations on a
“nonmagical” (i.e., “real”) computer, this should be at least intuitively plausible
since our formulas for ˆx are very simple in the sense of creating little opportunity
for rounding error to grow (there are very few arithmetical operations involved).
Thus, the errors x −ˆx must be due entirely (or nearly so) to uncertainties in the
original inputs. We conclude that the real problem is that the linear system we
are solving is too sensitive to perturbations in the inputs. This naturally raises the
question of how we may detect such sensitivity.
In view of this, we shall say that a matrix A is ill-conditioned if the solution x
(in Ax = y) is very sensitive to perturbations on A and y. Otherwise, the matrix
is said to be well-conditioned.
TLFeBOOK

138
LINEAR SYSTEMS OF EQUATIONS
We will need to introduce appropriate norms in order to objectively measure
the sizes of objects in our problem. However, before doing this we make a few
observations that give additional insight into the nature of the problem. In Example
4.1 we note that the ﬁrst column of A is big (in some sense), while the second
column is small. The smallness of the second column makes A close to being
singular. A similar observation may be made about Hilbert matrices. For a general
N × N Hilbert matrix, the last two columns are given by


1
N −1
1
N
1
N
1
N + 1
...
...
1
2N −2
1
2N −1


.
For very large N, it is apparent that these two columns are almost linearly depen-
dent; that is, one may be taken as close to being equal to the other. A simple
numerical example is that
1
100 ≈
1
101. Thus, at least at the outset, it seems that
ill-conditioned matrices are close to being singular, and that this is the root cause
of the sensitivity problem.
We now need to extend our treatment of the concept of norms from what we
have seen in earlier chapters. Our main source is Golub and Van Loan [5], but
similar information is to be found in Ref. 3 or 4 (or the references cited therein).
A fairly rigorous treatment of matrix and vector norms can be found in Horn and
Johnson [6].
Suppose again that x ∈Rn. The p-norm of x is deﬁned to be
||x||p =
n−1

k=0
|xk|p
	1/p
,
(4.28)
where p ≥1. The most important special cases are, respectively, the 1-norm, 2-
norm, and ∞-norm:
||x||1 =
n−1

k=0
|xk|,
(4.29a)
||x||2 = [xT x]1/2,
(4.29b)
||x||∞=
max
0≤k≤n−1 |xk|.
(4.29c)
The operation “max” means to select the biggest |xk|. A unit vector with respect
to norm || · || is a vector x such that ||x|| = 1. Note that if x is a unit vector with
TLFeBOOK

CONDITION NUMBERS
139
respect to one norm, then it is not necessarily a unit vector with respect to another
choice of norm. For example, suppose that x = [
√
3
2
1
2]T ; then
||x||2 = 1, ||x||1 =
√
3 + 1
2
, ||x||∞=
√
3
2 .
The vector x is a unit vector under the 2-norm, but is not a unit vector under the
1-norm or the ∞-norm.
Norms have various properties that we will list without proof. Assume that
x, y ∈Rn. For example, the H¨older inequality [recall (1.16) (in Chapter 1) for
comparison] is
|xT y| ≤||x||p||y||q
(4.30)
for which 1
p + 1
q = 1. A special case is the Cauchy–Schwarz inequality
|xT y| ≤||x||2||y||2,
(4.31)
which is a special instance of Theorem 1.1 (in Chapter 1). An important feature of
norms is that they are equivalent. This means that if || · ||α and || · ||β are norms
on Rn, then there are c1, c2 > 0 such that
c1||x||α ≤||x||β ≤c2||x||α
(4.32)
for all x ∈Rn. Some special instances of this are
||x||2 ≤||x||1 ≤√n||x||2,
(4.33a)
||x||∞≤||x||2 ≤√n||x||∞,
(4.33b)
||x||∞≤||x||1 ≤n||x||∞.
(4.33c)
Equivalence is signiﬁcant with respect to our problem in the following manner.
When we deﬁne condition numbers below, we shall see that the speciﬁc value of
the condition number depends in part on the choice of norm. However, equivalence
says that if a matrix is ill-conditioned with respect to one type of norm, then it
must be ill-conditioned with respect to any other type of norm. This can simplify
analysis in practice because it allows us to compute the condition number using
whatever norms are the easiest to work with. Equivalence can be useful in another
respect. If we have a sequence of vectors in the space Rn, then, if the sequence is
Cauchy with respect to some chosen norm, it must be Cauchy with respect to any
other choice of norm. This can simplify convergence analysis, again because we
may pick the norm that is easiest to work with.
In Chapter 2 we considered absolute and relative error in the execution of
ﬂoating-point operations. In this setting, operations were on scalars, and scalar
solutions were generated. Now we must redeﬁne absolute and relative error for
vector quantities using the norms deﬁned in the previous paragraph. Since ˆx ∈Rn
TLFeBOOK

140
LINEAR SYSTEMS OF EQUATIONS
is the computed (i.e., approximate) solution to x ∈Rn it is reasonable to deﬁne the
absolute error to be
ϵa = ||ˆx −x||,
(4.34a)
and the relative error is
ϵr = ||ˆx −x||
||x||
.
(4.34b)
Of course, x ̸= 0 is assumed here. The choice of norm is in principle arbitrary.
However, if we use the ∞-norm, then the concept of relative error with respect to
it can be made equivalent to a statement about the correct number of signiﬁcant
digits in ˆx:
||ˆx −x||∞
||x||∞
≈10−d.
(4.35)
In other words, the largest element of the computed solution ˆx is correct to approx-
imately d decimal digits. For example, suppose that x = [1.256 −2.554]T , and
ˆx = [1.251 −2.887]T ; then ˆx −x = [−0.005 −0.333]T , and so
||ˆx −x||∞= 0.333,
||x||∞= 2.554,
so therefore ϵr = 0.1304 ≈10−1. Thus, ˆx has a largest element that is accurate to
about one decimal digit, but the smallest element is observed to be correct to about
three signiﬁcant digits.
Matrices can have norms deﬁned on them. We have remarked that ill condi-
tioning seems to arise when a matrix is close to singular. Suitable matrix norms
can allow us to measure how close a matrix is to being singular, and thus gives
insight into its condition. Suppose that A, B ∈Rm×n (so A and B are not neces-
sarily square matrices). || · |||Rm×n →R is a matrix norm, provided the following
axioms hold:
(MN1)
||A|| ≥0 for all A, and ||A|| = 0 iff A = 0.
(MN2)
||A + B|| ≤||A|| + ||B||.
(MN3)
||αA|| = |α| ||A||. Constant α is from the same ﬁeld as the elements of
the matrix A.
In the present context we usually consider α ∈R. Extensions to complex-valued
matrices and vectors are possible. The axioms above are essentially the same as for
the norm in all other cases (see Deﬁnition 1.3 for comparison). The most common
matrix norms are the Frobenius norm
||A||F =
4
5
5
6
m−1

k=0
n−1

l=0
|ak,l|2
(4.36a)
TLFeBOOK

CONDITION NUMBERS
141
and the p-norms
||A||p = sup
x̸=0
||Ax||p
||x||p
.
(4.36b)
We see that in (4.36b) the matrix p-norm is dependent on the vector p-norm. Via
(4.36b), we have
||Ax||p ≤||A||p||x||p.
(4.36c)
We may regard A as an operator applied to x that yields output Ax. Equation
(4.36c) gives an upper bound on the size of the output, as we know the size of A
and the size of x as given by their respective p-norms. Also, since A ∈Rm×n it
must be the case that x ∈Rn, but y ∈Rm. We observe that
||A||p = sup
x̸=0

A
 
x
||x||p
!

p
= max
||x||p=1 ||Ax||p.
(4.37)
This is an alternative means to compute the matrix p-norm: Evaluate ||Ax||p at all
points on the unit sphere, which is the set of vectors {x|||x||p = 1}, and then pick
the largest value of ||Ax||p. Note that the term “sphere” is an extension of what
we normally mean by a sphere. For the 2-norm in n dimensions, the unit sphere is
clearly
||x||2 = [x2
0 + x2
1 + · · · + x2
n−1]1/2 = 1.
(4.38)
This represents our intuitive (i.e., Euclidean) notion of a sphere. But, say, for the
1-norm the unit sphere is
||x||1 = |x0| + |x1| + · · · + |xn−1| = 1.
(4.39)
Equations (4.38) and (4.39) specify very different looking surfaces in n-dimensional
space. A suggested exercise is to sketch these spheres for n = 2.
As with vector norms, matrix norms have various properties. One property pos-
sessed by the matrix p-norms is called the submultiplicative property:
||AB||p ≤||A||p||B||p
A ∈Rm×n,
B ∈Rn×q.
(4.40)
(The reader is warned that not all matrix norms possess this property; a coun-
terexample appears on p. 57 of Golub and Van Loan [5]). A miscellany of other
properties (including equivalences) is
||A||2 ≤||A||F ≤√n||A||2,
(4.41a)
max
i,j |ai,j| ≤||A||2 ≤√mn max
i,j |ai,j|,
(4.41b)
TLFeBOOK

142
LINEAR SYSTEMS OF EQUATIONS
||A||1 = max
j∈Zn
m−1

i=0
|ai,j|,
(4.41c)
||A||∞= max
i∈Zm
n−1

j=0
|ai,j|,
(4.41d)
1
√n||A||∞≤||A||2 ≤√m||A||∞,
(4.41e)
1
√m||A||1 ≤||A||2 ≤√n||A||1.
(4.41f)
The equivalences [e.g., (4.41a) and (4.41b)] have the same signiﬁcance for
matrices as the analogous equivalences for vectors seen in (4.32) and (4.33).
From (4.41c,d) we see that computing matrix 1-norms and ∞-norms is easy.
However, computing matrix 2-norms is not easy. Consider (4.37) with p = 2:
||A||2 = max
||x||2=1 ||Ax||2.
(4.42)
Let R = AT A ∈Rn×n (no, R is not a Hilbert matrix here; we have “recycled” the
symbol for another use), so then
||Ax||2
2 = xT AT Ax = xT Rx.
(4.43)
Now consider n = 2. Thus
xT Rx = [x0x1]
 r00
r01
r10
r11
  x0
x1

,
(4.44)
where r01 = r10 because R = RT . The vectors and matrix in (4.44) multiply out
to become
xT Rx = r00x2
0 + 2r01x0x1 + r11x2
1.
(4.45)
Since ||A||2
2 = max||x||2=1 ||Ax||2
2, we may ﬁnd ||A||2
2 by maximizing (4.45) subject
to the equality constraint ||x||2
2 = 1, i.e., xT x = x2
0 + x2
1 = 1. This problem may
be solved by using Lagrange multipliers (considered somewhat more formally in
Section 8.5). Thus, we must maximize
V (x) = xT Rx −λ[xT x −1],
(4.46)
where λ is the Lagrange multiplier. Since
V (x) = r00x2
0 + 2r01x0x1 + r11x2
1 −λ[x2
0 + x2
1 −1],
TLFeBOOK

CONDITION NUMBERS
143
we have
∂V (x)
∂x0
= 2r00x0 + 2r01x1 −2λx0 = 0,
∂V (x)
∂x1
= 2r01x0 + 2r11x1 −2λx1 = 0,
and these equations may be rewritten in matrix form as
 r00
r01
r10
r11
  x0
x1

= λ
 x0
x1

.
(4.47)
In other words, Rx = λx. Thus, the optimum choice of x is an eigenvector of
R = AT A. But which eigenvector is it?
First
note
that
A−1
exists
(by
assumption),
so
xT Rx = xT AT Ax =
(Ax)T (Ax) > 0 for all x ̸= 0. Therefore, R > 0. Additionally, R = RT , so all
of the eigenvalues of R are real numbers.5 Furthermore, because R > 0, all of its
eigenvalues are positive. This follows if we consider Rx = λx, and assume that
λ < 0. In this case xT Rx = λxT x = λ||x||2
2 < 0 for any x ̸= 0. [If λ = 0, then
Rx = 0 · x = 0 implies that x = 0 (as R−1 exists), so xT Rx = 0.] But this con-
tradicts the assumption that R > 0, and so all of the eigenvalues of R must be
positive. Now, since ||Ax||2
2 = xT AT Ax = xT Rx = xT (λx) = λ||x||2
2, and since
||x||2
2 = 1, it must be the case that ||Ax||2
2 is biggest for the eigenvector of R cor-
responding to the biggest eigenvalue of R. If the eigenvalues of R are denoted λ1
and λ0 with λ1 ≥λ0 > 0, then ﬁnally we must have
||A||2
2 = λ1.
(4.48)
This argument can be generalized for all n > 2. If R > 0, we assume that all
of its eigenvalues are distinct (this is not always true). If we denote them by
λ0, λ1, . . . , λn−1, then we may arrange them in decreasing order:
λn−1 > λn−2 > · · · > λ1 > λ0 > 0.
(4.49)
Therefore, for A ∈Rm×n
||A||2
2 = λn−1.
(4.50)
The problem of computing the eigenvalues and eigenvectors of a matrix has its own
special numerical difﬁculties. At this point we warn the reader that these problems
must never be treated lightly.
5If A is a real-valued symmetric square matrix, then we may prove this claim as follows. Suppose
that for eigenvector x of A, the eigenvalue is λ, that is, Ax = λx. Now ((Ax)∗)T = ((λx)∗)T , and so
(x∗)T AT = λ∗(x∗)T . Therefore, (x∗)T AT x = λ∗(x∗)T x. But (x∗)T AT x = (x∗)T Ax = λ(x∗)T x, so
ﬁnally λ∗(x∗)T x = λ(x∗)T x, so we must have λ = λ∗. This can be true only if λ ∈R.
TLFeBOOK

144
LINEAR SYSTEMS OF EQUATIONS
Example 4.2
Let det(A) denote the determinant of A. Suppose that R = AT A,
where
R =
 1
0.5
0.5
1

.
We will ﬁnd ||A||2. Consider
 1
0.5
0.5
1
  x0
x1

= λ
 x0
x1

.
We must solve det(λI −R) = 0 for λ. [Recall that det(λI −R) is the characteristic
polynomial of R.] Thus
det (λI −R) = det
  λ −1
−0.5
−0.5
λ −1
!
= (λ −1)2 −1
4 = 0,
and (λ −1)2 −1
4 = λ2 −2λ + 3
4 = 0, for
λ =
−(−2) ±
7
(−2)2 −4 · 1 · 3
4
2 · 1
= 2 ± 1
2
= 1
2, 3
2.
So, λ1 = 3
2, λ0 = 1
2. Thus, ||A||2
2 = λ1 = 3
2, and so ﬁnally
||A||2 =
8
3
2.
(We do not need the eigenvectors of R to compute the 2-norm of A.)
We see that the essence of computing the 2-norm of matrix A is to ﬁnd the zeros
of the characteristic polynomial of AT A. The problem of ﬁnding polynomial zeros
is the subject of a later chapter. Again, this problem has its own special numerical
difﬁculties that must never be treated lightly.
We now derive the condition number. Begin by assuming that A ∈Rn×n, and
that A−1 exists. The error between computed solution ˆx to Ax = y and x is
e = x −ˆx.
(4.51)
Ax = y, but Aˆx ̸= y in general. So we may deﬁne the residual
r = y −Aˆx.
(4.52)
We see that
Ae = Ax −Aˆx = y −Aˆx = r.
(4.53)
Thus, e = A−1r. We observe that if e = 0, then r = 0, but if r is small, then e
is not necessarily small because A−1 might be big, making A−1r big. In other
TLFeBOOK

CONDITION NUMBERS
145
words, a small residual r does not guarantee that ˆx is close to x. Sometimes r is
computed as a cursory check to see if ˆx is “reasonable.” The main advantage of
r is that it may always be computed, whereas x is not known in advance and so
e may never be computed exactly. Below it will be shown that considering r in
combination with a condition number is a more reliable method of assessing how
close ˆx is to x.
Now, since e = A−1r, we can say that ||e||p = ||A−1r||p ≤||A−1||p||r||p [via
(4.36c)]. Similarly, since r = Ae, we have ||r||p = ||Ae||p ≤||A||p||e||p. Thus
||r||p
||A||p
≤||e||p ≤||A−1||p||r||p.
(4.54)
Similarly, x = A−1y, so immediately
||y||p
||A||p
≤||x||p ≤||A−1||p||y||p.
(4.55)
If ||x||p ̸= 0, and ||y||p ̸= 0, then taking reciprocals in (4.55) yields
1
||A−1||p||y||p
≤
1
||x||p
≤||A||p
||y||p
.
(4.56)
We may multiply corresponding terms in (4.56) and (4.54) to obtain
1
||A−1||p||A||p
||r||p
||y||p
≤||e||p
||x||p
≤||A−1||p||A||p
||r||p
||y||p
.
(4.57)
We recall from (4.34b) that ϵr = ||x−ˆx||p
||x||p
= ||e||p
||x||p , so
1
||A−1||p||A||p
||r||p
||y||p
≤ϵr ≤||A−1||p||A||p
||r||p
||y||p
.
(4.58)
We call
||r||p
||y||p
= ||y −Aˆx||p
||y||p
(4.59)
the relative residual. We deﬁne
κp(A) = ||A||p||A−1||p
(4.60)
to be the condition number of A. It is immediately apparent that κp(A) ≥1 for
any A and valid p. We see that ϵr is between 1/κp(A) and κp(A) times the
relative residual. In particular, if κp(A) >> 1 (i.e., if the condition number is very
large), even if the relative residual is tiny, then ϵr might be large. On the other
hand, if κp(A) is close to unity, then ϵr will be small if the relative residual is
small. In conclusion, if κp(A) is large, it is a warning (not a certainty) that small
TLFeBOOK

146
LINEAR SYSTEMS OF EQUATIONS
perturbations in A and y may cause ˆx to differ greatly from x. Equivalently, if κp(A)
is large, then a small r does not imply that ˆx is close to x.
A rule of thumb in interpreting condition numbers is as follows [3, p. 229], and
is more or less true regardless of p in (4.60). If κp(A) ≈d × 10k, where d is a
decimal digit from one to nine, we can expect to lose (at worst) about k digits of
accuracy. The reason that p does not matter too much is because we recall that
matrix norms are equivalent. Therefore, for this rule of thumb to be useful, the
working precision of the computing machine/software package must be known.
For example, MATLAB computes to about 16 decimal digits of precision. Thus,
k ≥16 would give us concern that ˆx is not close to x.
Example 4.3
Suppose that
A =
 1
1 −ϵ
1
1

∈R2×2, |ϵ| << 1.
We will determine an estimate of κ1(A). Clearly
A−1 = 1
ϵ

1
−1 + ϵ
−1
1

=
 b00
b01
b10
b11

.
We have
1

i=0
|ai,0| = |a00| + |a10| = 2,
1

i=0
|ai,1| = |a01| + |a11| = |1 −ϵ| + 1,
so via (4.41c), ||A||1 = max{2, |1 −ϵ| + 1} ≈2. Similarly
1

i=0
|bi,0| = |b00| + |b10| = 2
|ϵ|,
1

i=0
|bi,1| = |b01| + |b11| = 1
|ϵ| +

−1 + ϵ
ϵ
 ,
so again via (4.41c) ||A−1||1 = max
9
2
|ϵ|,
1 −1
ϵ
 + 1
|ϵ|
:
≈
2
|ϵ|. Thus
κ1(A) ≈4
|ϵ|.
We observe that if ϵ = 0, then A−1 does not exist, so our approximation to κ1(A)
is a reasonable result because
lim
|ϵ|→0 κ1(A) = ∞.
We may wish to compute κ2(A) = ||A||2||A−1||2. We will suppose that A ∈
Rn×n and that A−1 exists. But we recall that computing matrix 2-norms involves
ﬁnding eigenvalues. More speciﬁcally, ||A||2
2 is the largest eigenvalue of R = AT A
TLFeBOOK

CONDITION NUMBERS
147
[recall (4.50)]. Suppose, as in (4.49), that λ0 is the smallest eigenvalue of R for
which the corresponding eigenvector is denoted by v, that is, Rv = λ0v. Then we
observe that R−1v = 1
λ0 v. In other words, 1/λ0 is an eigenvalue of R−1. By similar
reasoning, 1/λk for k ∈Zn must all be eigenvalues of R−1. Thus, 1/λ0 will be the
biggest eigenvalue of R−1. For present simplicity assume that A is a normal matrix.
This means that AAT = AT A = R. The reader is cautioned that not all matrices A
are normal. However, in this case we have R−1 = A−1A−T = A−T A−1. [Recall
that (A−1)T = (AT )−1 = A−T .] We have that ||A−1||2
2 is the largest eigenvalue of
A−T A−1, but R−1 = A−T A−1 since A is assumed normal. The largest eigenvalue
of R−1 has been established to be 1/λ0, so it must be the case that for a normal
matrix A (real-valued and invertible)
κ2(A) =
;
λn−1
λ0
,
(4.61)
that is, A is ill-conditioned if the ratio of the biggest to smallest eigenvalue of
AT A is large. In other words, a large eigenvalue spread is associated with matrix
ill conditioning. It turns out that this conclusion holds even if A is not normal; that
is, (4.61) is valid even if A is not normal. But we will not prove this. (The interested
reader can see pp. 312 and 340 of Horn and Johnson [6] for more information.)
An obvious difﬁculty with condition numbers is that their exact calculation often
seems to require knowledge of A−1. Clearly this is problematic since computing
A−1 accurately may not be easy or possible (because of ill conditioning). We seem
to have a “chicken and egg” problem. This problem is often dealt with by using
condition number estimators. This in turn generally involves placing bounds on
condition numbers. But the subject of condition number estimation is not within
the scope of this book. The interested reader might consult Higham [7] for further
information on this subject if desired. There is some information on this matter in
the treatise by Golub and Van Loan [5, pp. 128–130], which includes a pseudocode
algorithm for ∞-norm condition number estimation of an upper triangular nonsin-
gular matrix. We remark that ||A||2 is sometimes called the spectral norm of A,
and is actually best computed using entities called singular values [5, p. 72]. This
is because computing singular values avoids the necessity of computing A−1, and
can be done in a numerically reliable manner. Singular values will be discussed in
more detail later.
We conclude this section with a remark about the Hilbert matrix R of Section 4.3.
As discussed by Hill [3, p. 232], we have
κ2(R) ∝eαN
for some α > 0. (Recall that symbol ∝means “proportional to.”) Proving this is
tough, and we will not attempt it. Thus, the condition number of R grows very
rapidly with N and explains why the attempt to invert R in Appendix 4.A failed
for so small a value of N.
TLFeBOOK

148
LINEAR SYSTEMS OF EQUATIONS
4.5
LU DECOMPOSITION
In this section we will assume A ∈Rn×n, and that A−1 exists. Many algorithms
to solve Ax = y work by factoring the matrix A in various ways. In this section
we consider a Gaussian elimination approach to writing A as
A = LU,
(4.62)
where L is a nonsingular lower triangular matrix, and U is a nonsingular upper
triangular matrix. This is the LU decomposition (factorization) of A. Naturally,
L, U ∈Rn×n, and L = [li,j], U = [ui,j]. Since these matrices are lower and upper
triangular, respectively, it must be the case that
li,j = 0
for
j > i
and
ui,j = 0
for
j < i.
(4.63)
For example, the following are (respectively) lower and upper triangular matrices:
L =


1
0
0
1
1
0
1
1
1

,
U =


1
2
3
0
4
5
0
0
6

.
These matrices are clearly nonsingular since their determinants are 1 and 24, respec-
tively. In fact, L is nonsingular iff li,i ̸= 0 for all i, and U is nonsingular iff uj,j ̸= 0
for all j. We note that with A factored as in (4.62), the solution of Ax = y becomes
quite easy, but the details of this will be considered later. We now concentrate on
ﬁnding the factors L, U.
We begin by deﬁning a Gauss transformation matrix Gk such that
Gkx =


1
· · ·
0
0
· · ·
0
...
...
...
...
0
· · ·
1
0
· · ·
0
0
· · ·
−τ k
k
1
· · ·
0
...
...
...
...
0
· · ·
−τ k
n−1
0
· · ·
1




x0
...
xk−1
xk
...
xn−1


=


x0
...
xk−1
0
...
0


(4.64)
for
τ k
i =
xi
xk−1
, i = k, . . . , n −1.
(4.65)
The superscript k on τ k
j does not denote raising τj to a power; it is simply part of
the name of the symbol. This naming convention is needed to account for the fact
that there is a different set of τ values for every Gk. For this to work requires that
xk−1 ̸= 0. Equation (4.65) followed from considering the matrix–vector product
TLFeBOOK

LU DECOMPOSITION
149
in (4.64):
−τ k
k xk−1 + xk = 0
−τ k
k+1xk−1 + xk+1 = 0
...
−τ k
n−1xk−1 + xn−1 = 0.
We observe that Gk is “designed” to annihilate the last n −k elements of vector
x. We also see that Gk is lower triangular, and if it exists, always possesses an
inverse because the main diagonal elements are all equal to unity. A lower triangular
matrix where all of the main diagonal elements are equal to unity is called unit
lower triangular. Similar terminology applies to upper triangular matrices. Deﬁne
the kth Gauss vector
(τ k)T = [0 · · · 0
# $% &
k zeros
τ k
k τ k
k+1 · · · τ k
n−1].
(4.66)
The kth unit vector is
eT
k = [0 · · · 0
# $% &
k zeros
1
0 · · · 0
# $% &
n−k−1 zeros
].
(4.67)
If I is an n × n identity matrix, then
Gk = I −τ keT
k−1
(4.68)
for k = 1, 2, . . . , n −1. For example, if n = 4, we have
G1 =


1
0
0
0
−τ 1
1
1
0
0
−τ 1
2
0
1
0
−τ 1
3
0
0
1

,
G2 =


1
0
0
0
0
1
0
0
0
−τ 2
2
1
0
0
−τ 2
3
0
1

,
G3 =


1
0
0
0
0
1
0
0
0
0
1
0
0
0
−τ 3
3
1

.
(4.69)
The Gauss transformation matrices may be applied to A, yielding an upper trian-
gular matrix. This is illustrated by the following example.
Example 4.4
Suppose that
A =


1
2
3
4
−1
1
2
1
0
2
1
3
0
0
1
1

(= A0).
TLFeBOOK

150
LINEAR SYSTEMS OF EQUATIONS
We introduce matrices Ak, where Ak = GkAk−1 for k = 1, 2, . . . , n −1, and ﬁnally
U = An−1. Once again, Ak is not the kth power of A, but rather denotes the kth
matrix in a sequence of matrices. Now consider
G1A0 =


1
0
0
0
1
1
0
0
0
0
1
0
0
0
0
1




1
2
3
4
−1
1
2
1
0
2
1
3
0
0
1
1

=


1
2
3
4
0
3
5
5
0
2
1
3
0
0
1
1

= A1
for which the τ 1
i entries in the ﬁrst column of G1 depend on the ﬁrst column of
A0 (i.e., of A) according to (4.65). Similarly
G2A1 =


1
0
0
0
0
1
0
0
0
−2
3
1
0
0
0
0
1




1
2
3
4
0
3
5
5
0
2
1
3
0
0
1
1

=


1
2
3
4
0
3
5
5
0
0
−7
3
−1
3
0
0
1
1

= A2
for which the τ 2
i entries in the second column of G2 depend on the second column
of A1, and also
G3A2 =


1
0
0
0
0
1
0
0
0
0
1
0
0
0
3
7
1




1
2
3
4
0
3
5
5
0
0
−7
3
−1
3
0
0
1
1

=


1
2
3
4
0
3
5
5
0
0
−7
3
−1
3
0
0
0
6
7

= U
for which the τ 3
i entries in the third column of G3 depend on the third column of
A2. We see that U is indeed upper triangular, and it is also nonsingular. We also
see that
U = G3G2G1
#
$%
&
=L1
A.
Since the product of lower triangular matrices is a lower triangular matrix, it is the
case that L1 = G3G2G1 is lower triangular. Thus
A = L−1
1 U.
Since the inverse (if it exists) of a lower triangular matrix is also a lower triangular
matrix, we can deﬁne L = L−1
1 , and so A = LU. Thus
L = G−1
1 G−1
2 G−1
3 .
From this example it appears that we need to do much work in order to ﬁnd G−1
k .
However, this is not the case. It turns out that
G−1
k
= I + τ keT
k−1.
(4.70)
TLFeBOOK

LU DECOMPOSITION
151
This is easy to conﬁrm. From (4.68) and (4.70)
GkG−1
k
= [I −τ keT
k−1][I + τ keT
k−1] = I −τ keT
k−1 + τ keT
k−1 −τ keT
k−1τ keT
k−1
= I −τ keT
k−1τ keT
k−1.
But from (4.66) and (4.67) eT
k−1τ k = 0, so ﬁnally GkG−1
k
= I.
To obtain τ k
i from (4.65), we see that we must divide by xk−1. In our matrix
factorization application of the Gauss transformation, we have seen (in Example
4.4) that xk−1 will be an element of Ak. These elements are called pivots. It
is apparent that the factorization procedure cannot work if a pivot is zero. The
occurrence of zero-valued pivots is a common situation. A simple example of a
matrix that cannot be factored with our algorithm is
A =
 0
1
1
0

.
(4.71)
In this case
G1A =

1
0
−τ 1
1
1
  0
1
1
0

=
 0
1
1
−τ 1
1

,
(4.72)
and from (4.65)
τ 1
1 = x1
x0
= a10
a00
= 1
0 →∞.
(4.73)
This result implies that not all matrices possess an LU factorization. Let det(A)
denote the determinant of A. We may state a general condition for the existence of
the LU factorization:
Theorem 4.1: Since A = [ai,j]i,j=0,...,n−1 ∈Rn×n we deﬁne the kth leading
principle submatrix of A to be Ak = [ai,j]i,j=0,... ,k−1 ∈Rk×k for k = 1, 2, . . . , n
(so that A = An, and A1 = [a00] = a00). There exists a unit lower triangular matrix
L and an upper triangular matrix U such that A = LU, provided that det(Ak) ̸= 0
for all k = 1, 2 . . . , n. Furthermore, with U = [ui,j] ∈Rn×n we have det(Ak) =
0k−1
i=0 ui,i.
The proof is given in Golub and Van Loan [5]. It will not be considered here. For
A in (4.71), we see that A1 = [0] = 0, so det(A1) = 0. Thus, even though A−1
exists, it does not possess an LU decomposition. It is also easy to verify that for
A =


1
4
1
2
8
1
0
−1
1

,
although A−1 exists, again A does not possess an LU decomposition. In this case
we have det(A2) = det
  1
4
2
8
!
= 0. Theorem 4.1 leads to a test of positive
deﬁniteness according to the following theorem.
TLFeBOOK

152
LINEAR SYSTEMS OF EQUATIONS
Theorem 4.2: Suppose R ∈Rn×n with R = RT . Suppose that R = LDLT ,
where
L
is
unit
lower
triangular,
and
D
is
a
diagonal
matrix
(L =
[li,j]i,j=0,...,n−1, D = [di,j]i,j=0,...,n−1). If di,i > 0 for all i ∈Zn, then R > 0.
Proof
L is unit lower triangular, so for any y ∈Rn there will be a unique
x ∈Rn such that
y = LT x
(yT = xT L)
because L−1 exists. Thus, assuming D > 0
xT Rx = xT LDLT x = yT Dy =
n−1

i=0
y2
i di,i > 0
for all y ̸= 0, since di,i > 0 for all i ∈Zn. In fact, n−1
i=0 y2
i di,i = 0 iff yi = 0 for
all i ∈Zn. Consequently, xT Rx > 0 for all x ̸= 0, and so immediately R > 0.
We relate D in Theorem 4.2 to U in Theorem 4.1 according to U = DLT . If the
LDLT decomposition of a matrix R exists, then matrix D immediately tells us
whether R is pd just by viewing the signs of the diagonal elements.
We may deﬁne (as in Example 4.4) Ak = [ak
i,j], where k = 0, 1, . . . , n −1 and
A0 = A. Consequently
τ k
i =
xi
xk−1
=
ak−1
i,k−1
ak−1
k−1,k−1
(4.74)
for i = k, k + 1, . . . , n −1. This follows because Gk contains τ k
i , and as observed
in the example above, τ k
i depends on the column indexed k −1 in Ak−1. Thus, a
pseudocode program for ﬁnding U can therefore be stated as follows:
A0 := A;
for k := 1 to n −1 do begin
for i := k to n −1 do begin
τ k
i := ak−1
i,k−1/ak−1
k−1,k−1; {This loop computes τk}
end;
Ak := GkAk−1; { Gk contains τk
i via (4.64) }
end;
U := An−1;
We see that the pivots are ak−1
k−1,k−1 for k = 1, 2, . . . , n −1. Now
U = Gn−1Gn−2 · · · G2G1A.
so
A = G−1
1 G−1
2
· · · G−1
n−2G−1
n−1
#
$%
&
=L
U.
(4.75)
TLFeBOOK

LU DECOMPOSITION
153
Consequently, from (4.70), we obtain
L = (I + τ 1eT
0 )(I + τ 2eT
1 ) · · · (I + τ n−1eT
n−2) = I +
n−1

k=1
τ keT
k−1.
(4.76)
To conﬁrm the last equality of (4.76), consider deﬁning Lm = G−1
1
· · · G−1
m
for
m = 1, . . . , n −1. Assume that Lm = I + m
k=1 τ keT
k−1, which is true for m = 1
because L1 = G−1
1
= I + τ 1eT
0 . Consider Lm+1 = LmG−1
m+1, so
Lm+1 =
'
I +
m

k=1
τ keT
k−1
(
(I + τ m+1eT
m)
= I +
m

k=1
τ keT
k−1 + τ m+1eT
m +
m

k=1
τ keT
k−1τ m+1eT
m.
But eT
k−1τ m+1 = 0 for k = 1, . . . , m from (4.66) and (4.67). Thus
Lm+1 = I +
m

k=1
τ keT
k−1 + τ m+1eT
m = I +
m+1

k=1
τ keT
k−1.
Therefore, (4.76) is valid by mathematical induction. (A simpler example of a
proof by induction appears in Appendix 3.B.) Because of (4.76), the previous
pseudocode implicitly computes L as well as U. Thus, if no zero-valued pivots
are encountered, the algorithm will terminate, having provided us with both L
and U. [As an exercise, the reader should use (4.76) to ﬁnd L in Example 4.4
simply by looking at the appropriate entries of the matrices Gk; that is, do not
use L = G−1
1 G−1
2 G−1
3 . Having found L by this means, conﬁrm that LU = A.] We
remark that (4.76) shows that L is unit lower triangular.
It is worth mentioning that certain classes of matrix are guaranteed to possess
an LU decomposition. Suppose that A ∈Rn×n with A = AT and A > 0. Let v =
[v0 · · · vk−1
#
$%
&
=uT
0 · · · 0
# $% &
n−k zeros
]T ; then, if v ̸= 0, we have vT Av > 0, but if Ak is the kth
leading principle submatrix of A, then
vT Av = uT Aku > 0
which holds for all k = 1, 2, . . . , n. Consequently, Ak > 0 for all k, and so A−1
k
exists for all k. Since A−1
k
exists for all k, it follows that det(Ak) ̸= 0 for all k.
The conditions of Theorem 4.1 are met, and so A possesses an LU decomposi-
tion. That is, all real-valued, symmetric positive deﬁnite matrices possess an LU
decomposition.
TLFeBOOK

154
LINEAR SYSTEMS OF EQUATIONS
We recall that the class of positive deﬁnite matrices is an important one since
they have a direct association with least-squares approximation problems. This was
demonstrated in Section 4.2.
How many ﬂoating-point operations (ﬂops) are needed by the algorithm for
ﬁnding the LU decomposition of a matrix? Answering this question gives us an
indication of the computational complexity of the algorithm. Neglecting multipli-
cation by zero or by one, to compute Ak = GkAk−1 requires (n −k)(n −k + 1)
multiplications, and the same number of additions. This follows from considering
the product GkAk−1 with the factors partitioned into submatrices according to
Gk =

Ik
0
Tk
In−k
	
,
Ak−1 =

Ak−1
00
Ak−1
01
0
Ak−1
11
	
,
(4.77)
where Ik is a k × k identity matrix, Tk is (n −k) × k and is zero-valued except for
its last column, which contains −τ k [see (4.64)]. Similarly, Ak−1
00
is (k −1) ×
(k −1), Ak
01 is (k −1) × (n −k + 1), and Ak−1
11
is (n −k + 1) × (n −k + 1).
From the pseudocode, we see that we need n−1
k=1(n −k) division operations.
Operation Ak = GkAk−1 is executed for k = 1 to n −1, so the total number of
operations is:
n−1

k=1
(n −k)(n −k + 1)
multiplications
n−1

k=1
(n −k)(n −k + 1)
additions
n−1

k=1
(n −k)
divisions
We now recognize that
N

k=1
k = N(N + 1)
2
,
N

k=1
k2 = N(N + 1)(2N + 1)
6
,
(4.78)
where the second summation identity was proven in Appendix 3.B. The ﬁrst sum-
mation identity may be proved in a similar manner. Therefore
n−1

k=1
(n −k)(n −k + 1) =
n−1

k=1
[n2 + n −(2n + 1)k + k2]
= (n −1)(n2 + n) −(2n + 1)
n−1

k=1
k +
n−1

k=1
k2
TLFeBOOK

LU DECOMPOSITION
155
= 1
3n3 −1
3n,
(4.79a)
n−1

k=1
(n −k) = n(n −1) −
n−1

k=1
k = 1
2n2 −1
2n.
(4.79b)
So-called asymptotic complexity measures are deﬁned using
Deﬁnition 4.2: Big O
We say that f (n) = O(g(n)) if there is a 0 < c < ∞,
and an N ∈N (N < ∞) such that
f (n) ≤cg(n)
for all n > N.
Our algorithm needs a total of f (n) = 2( 1
3n3 −1
3n) + 1
2n2 −1
2n = 2
3n3 + 1
2n2 −
7
6n ﬂops. We may say that O(n3) operations (ﬂops) are needed (so here g(n) =
n3). We may read O(n3) as “order n-cubed,” so order n-cubed operations are
needed. If one operation takes one unit of time on a computing machine we say
the asymptotic time complexity of the algorithm is O(n3). Parameter n (matrix
order) is the size of the problem. We might also say that the time complexity
of the algorithm is cubic in the size of the problem since the number of oper-
ations f (n) is a cubic polynomial in n. But we caution the reader about ﬂop
counting:
“Flop counting is a necessarily crude approach to the measuring of program efﬁciency
since it ignores subscripting, memory trafﬁc, and the countless other overheads asso-
ciated with program execution. We must not infer too much from a comparison of
ﬂops counts. . . . Flop counting is just a ‘quick and dirty’ accounting method that
captures only one of several dimensions of the efﬁciency issue.”
—Golub and Van Loan [5, p. 20]
Asymptotic complexity measures allow us to talk about algorithmic resource
demands without getting bogged down in detailed expressions for computing time,
memory requirements, and other variables. However, the comment by Golub and
Van Loan above may clearly be extended to asymptotic measures.
Suppose that A is LU-factorable, and that we know L and U. Suppose that we
wish to solve Ax = y. Thus
LUx = y,
(4.80)
and deﬁne Ux = z, so we begin by considering
Lz = y.
(4.81)
TLFeBOOK

156
LINEAR SYSTEMS OF EQUATIONS
In expanded form this becomes


l00
0
0
· · ·
0
l10
l11
0
· · ·
0
l20
l21
l22
· · ·
0
...
...
...
...
ln−1,0
ln−1,1
ln−1,2
· · ·
ln−1,n−1




z0
z1
z2
...
zn−1


=


y0
y1
y2
...
yn−1


.
(4.82)
Since L−1 exists, solving (4.81) is easy using forward elimination (forward substi-
tution). Speciﬁcally, from (4.82)
z0 = y0
l0,0
z1 =
1
l1,1
[y1 −z0l1,0]
z2 =
1
l2,2
[y2 −z0l2,0 −z1l2,1]
...
zn−1 =
1
ln−1,n−1

yn−1 −
n−2

k=0
zkln−1,k
	
.
Thus, in general
zk =
1
lk,k

yk −
k−1

i=0
zilk,i
	
(4.83)
for k = 1, 2, . . . , n −1 with z0 = y0/l0,0. Since we now know z, we may solve
Ux = z by backward substitution. To see this, express the problem in expanded
form:


u0,0
u0,1
· · ·
u0,n−2
u0,n−1
0
u1,1
· · ·
u1,n−2
u1,n−1
...
...
...
...
0
0
· · ·
un−2,n−2
un−2,n−1
0
0
· · ·
0
un−1,n−1




x0
x1
...
xn−2
xn−1


=


z0
z1
...
zn−2
zn−1


.
(4.84)
From (4.84), we obtain
xn−1 =
zn−1
un−1,n−1
xn−2 =
1
un−2,n−2
[zn−2 −xn−1un−2,n−1]
TLFeBOOK

LU DECOMPOSITION
157
xn−3 =
1
un−3,n−3
[zn−3 −xn−1un−3,n−1 −xn−2un−3,n−2]
...
x0 =
1
u0,0

z0 −
n−1

k=1
xku0,k
	
.
In general
xk =
1
uk,k

zk −
n−1

i=k+1
xiuk,i


(4.85)
for k = n −2, . . . , 0 with xn−1 = zn−1/un−1,n−1. The forward-substitution and
backward-substitution algorithms that we have just derived have an asymptotic
time complexity of O(n2). The reader should conﬁrm this as an exercise. This
result suggests that most of the computational effort needed to solve for x in
Ax = y lies in the LU decomposition stage.
So far we have said nothing about the performance of our linear system solution
method with respect to ﬁnite precision arithmetic effects (i.e., rounding error).
Before considering this matter, we make a few remarks regarding the stability of
our method. We have noted that the LU decomposition algorithm will fail if a zero-
valued pivot is encountered. This can happen even if Ax = y has a solution and A
is well-conditioned. In other words, our algorithm is actually unstable since we can
input numerically well-posed problems that cause it to fail. This does not necessarily
mean that our algorithm should be totally rejected. For example, we have shown
that positive deﬁnite matrices will never result in a zero-valued pivot. Furthermore,
if A > 0, and it is well-conditioned, then it can be shown that an accurate answer
will be provided by the algorithm despite its faults. Nonetheless, the problem of
failure due to encountering a zero-valued pivot needs to be addressed. Also, what
happens if a pivot is not exactly zero, but is close to zero? We might expect that
this can result in a computed solution ˆx that differs greatly from the mathematically
exact solution x, especially where rounding error is involved, even if A is well-
conditioned.
Recall (2.15) from Chapter 2,
f l[x op y] = (x op y)(1 + ϵ)
(4.86)
for which |ϵ| ≤2−t. If we store A in a ﬂoating-point machine, then, because of
the necessity to quantize we are really storing the elements
[f l[A]]i,j = f l[ai,j] = ai,j(1 + ϵi,j)
(4.87)
TLFeBOOK

158
LINEAR SYSTEMS OF EQUATIONS
with |ϵi,j| ≤2−t. Suppose now that A, B ∈Rm×n; we then deﬁne6
|A| = [|ai,j|] ∈Rm×n,
(4.88)
and by B ≤A, we mean bi,j ≤ai,j for all i and j. So we may express (4.87) more
compactly as
|f l[A] −A| ≤u|A|,
(4.89)
where u = 2−t, since |ϵi,j| ≤2−t.
Forsythe and Moler [4, pp. 104–105] show that the computed solution ˆz to
Lz = y [recall (4.81)] as obtained by forward substitution is actually the exact
solution to a perturbed lower triangular system
(L + δL)ˆz = y,
(4.90)
where δL is a lower triangular perturbation matrix, and where
|δL| ≤1.01nu|L|.
(4.91)
A very similar bound exists for the problem of solving Ux = z by backward-
substitution. We will not derive these bounds, but will simply mention that the
derivation involves working with a bound similar to (2.39) in Chapter 2. From
(4.91) we have relative perturbations |δli,j |
|li,j | ≤1.01nu. It is apparent that since u
is typically quite tiny, unless n (matrix order) is quite huge, these relative pertur-
bations will not be signiﬁcant. In other words, forward substitution and backward
substitution are very stable procedures that are quite resistant to the effects of
rounding errors. Thus, any difﬁculties with our linear system solution procedure in
terms of a rounding error likely involve only the LU factorization stage.
The rounding error analysis for our Gaussian elimination algorithm is even more
involved than the effort required to obtain (4.91), so again we will content ourselves
with citing the main result without proof. We cite Theorem 3.3.1 in Golub and Van
Loan [5] as follows.
Theorem 4.3: Assume that A is an n × n matrix of ﬂoating-point numbers.
If no zero-valued pivots are encountered during the execution of the Gaussian
elimination algorithm for which A is the input, then the computed triangular factors
(here denoted ˆL and ˆU) satisfy
ˆL ˆU = A + δA
(4.92a)
such that
|δA| ≤3(n −1)u(|A| + | ˆL|| ˆU|) + O(u2).
(4.92b)
6There is some danger in confusing this with the determinant. That is, some people use |A| to denote
the determinant of A. We will avoid this here by sticking with det(A) as the notation for determinant
of A.
TLFeBOOK

LU DECOMPOSITION
159
In this theorem the term O(u2) denotes a part of the error term dependent on u2.
This is quite small as u2 = 2−2t (rounding assumed), and so may be practically
disregarded. The term arises in the work of Golub and Van Loan [5] because those
authors prefer to work with slightly looser bounding results than are to be found
in the volume by Forsythe and Moler [4]. The bound in (4.92b) gives us cause for
concern. The perturbation matrix δA may not be small. This is because | ˆL|| ˆU| can
be quite large. An example of this would be
A =


1
4
1
2
8.001
1
0
−1
1

,
for which
ˆL =


1
0
0
2
1
0
0
−1000
1

,
ˆU =


1
4
1
0
0.001
−1
0
0
−999

.
This has happened because
A1 =


1
4
1
0
0.001
−1
0
−1
1

,
which has a1
1,1 = 0.001. This is a small pivot and is ultimately responsible for
giving us “big” triangular factors. Clearly, the smaller the pivot the bigger the
potential problem. Golub and Van Loan’s [5] Theorem 3.3.2 (which we will not
repeat here) goes on to demonstrate that the errors in the computed triangular
factors can adversely affect the solution to Ax = LUx = y as obtained by forward
substitution and backward substitution. Thus, if we use the computed solutions ˆL
and ˆU in ˆL ˆU ˆx = y, then the computed solution ˆx may not be close to x.
How may our Gaussian elimination LU factorization algorithm be modiﬁed to
make it more stable? The standard solution is to employ partial pivoting. We do not
consider the method in detail here, but illustrate it with a simple example (Example
4.5). Essentially, before applying a Gauss transformation Gk, the rows of matrix
Ak−1 are permuted (i.e., exchanged) in such a manner as to make the pivot as
large as possible while simultaneously ensuring that Ak is as close to being upper
triangular as possible. Permutation operations have a matrix description, and such
matrices may be denoted by Pk. We remark that P −1
k
= P T
k .
Example 4.5
Suppose that
A =


1
4
1
2
8
1
0
−1
1

= A0.
TLFeBOOK

160
LINEAR SYSTEMS OF EQUATIONS
Thus
G1P1A0 =


1
0
0
−1
2
1
0
0
0
1




0
1
0
1
0
0
0
0
1




1
4
1
2
8
1
0
−1
1


=


1
0
0
−1
2
1
0
0
0
1




2
8
1
1
4
1
0
−1
1

=


2
8
1
0
0
1
2
0
−1
1

= A1,
and
G2P2A1 =


1
0
0
0
1
0
0
0
1




1
0
0
0
0
1
0
1
0




2
8
1
0
0
1
2
0
−1
1


=


1
0
0
0
1
0
0
0
1




2
8
1
0
−1
1
0
0
1
2

= A2
for which U = A2. We see that P2 interchanges rows 2 and 3 rather than 1 and
2 because to do otherwise would ruin the upper triangular structure we seek. It is
apparent that
G2P2G1P1A = U,
so that
A = P −1
1 G−1
1 P −1
2 G−1
2
#
$%
&
=L
U,
for which
L =


1
2
0
1
1
0
0
0
1
0

.
This matrix is manifestly not lower triangular. Thus, our use of partial pivoting to
achieve algorithmic stability has been purchased at the expense of some loss of
structure (although Theorem 3.4.1 in Ref. 5 shows how to recover much of what
is lost.7) Also, permutations involve moving data around in the computer, and this
is a potentially signiﬁcant cost. But these prices are usually worth paying.
7In general, the Gaussian elimination with partial pivoting algorithm generates
Gn−1Pn−1 · · · G2P2G1P1A = U,
and it turns out that
Pn−1 · · · P2P1A = LU
for which L is unit lower triangular, and U is upper triangular. The expression for L in terms of the
factors Gk is messy, and so we omit it. The interested reader can see pp. 112–113 of Ref. 5 for details.
TLFeBOOK

LEAST-SQUARES PROBLEMS AND QR DECOMPOSITION
161
It is worth mentioning that the need to trade off algorithm speed in favor of sta-
bility is common in numerical computing; that is, fast algorithms often have stability
problems. Much of numerical computing is about creating the fastest possible stable
algorithms. This is a notoriously challenging engineering problem.
A much more detailed account of Gaussian elimination with partial pivoting
appears in Golub and Van Loan [5, pp. 108–116]. This matter will not be discussed
further in this book.
4.6
LEAST-SQUARES PROBLEMS AND QR DECOMPOSITION
In this section we consider the QR decomposition of A ∈Rm×n for which m ≥n,
and A is of full rank [i.e., rank (A) = n]. Full rank in this sense means that the
columns of A are linearly independent. The QR decomposition of A is
A = QR,
(4.93)
where Q ∈Rm×m is an orthogonal matrix [i.e., QT Q = QQT = I (identity
matrix)], and R ∈Rm×n is upper triangular in the following sense:
R =


r0,0
r0,1
· · ·
r0,n−1
0
r1,1
· · ·
r1,n−1
...
...
...
0
0
· · ·
rn−1,n−1
0
0
· · ·
0
...
...
...
0
0
· · ·
0


=
 R
0

.
(4.94)
Here R ∈Rn×n is a square upper triangular matrix and is nonsingular because A
is full rank. The bottom block of zeros in R of (4.94) is (m −n) × n.
It should be immediately apparent that the existence of a QR decomposition
for A makes it quite easy to solve for x in Ax = y, if A−1 exists (which implies
that in this special case A is square). Thus, Ax = QRx = y, and so Rx = QT y.
The upper triangular linear system Rx = QT y may be readily solved by backward
substitution (recall the previous section).
The case where m > n is important because it arises in overdetermined least-
squares approximation problems.We illustrate with the following example based on
a real-world problem.8 Figure 4.1 is a plot of some simulated body core temperature
8This example is from the problem of estimating the circadian rhythm parameters of human patients
who have sustained head injuries. The estimates are obtained by the suitable processing of various
physiological data sets (e.g., body core temperature, heart rate, blood pressure). The nature of the injury
has made the patients’ rhythms deviate from the nominal 24-h cycle. Correct estimation of rhythm
parameters can lead to improved clinical treatment because of improved timing in the administering of
TLFeBOOK

162
LINEAR SYSTEMS OF EQUATIONS
0
10
20
30
40
50
60
70
80
90
36.8
36.9
37
37.1
37.2
 Time (hours) 
Patient temperature (Celcsius)
Illustration of least-squares fitting
Noisy data with trend 
Linear trend component 
Model
Figure 4.1
Simulated human patient temperature data to illustrate overdetermined least-
squares model parameter estimation. Here we have N = 1000 samples fn (the dots), for
Ts = 300 (seconds), T = 24 (hours), a = 2 × 10−7◦C/s, b = 37◦C, and c = 0.1◦C. The
solution to (4.103) is ˆa = 2.0582 × 10−7◦C/s, ˆb = 36.9999◦C, ˆc = 0.1012◦C.
measurements from a human patient (this is the noisy data with trend). The data
has three components:
1. A sinusoidal component
2. Random noise.
3. A linear trend.
Our problem is to estimate the parameters of the sinusoid (i.e., the amplitude,
period, and phase), which represents the patient’s circadian rhythm. In other words,
the noise and trend are undesirable and so are to be, in effect, removed from the
desired sinusoidal signal component. Here we will content ourselves with estimating
only the amplitude of the sinusoid. The problem of estimating the remaining param-
eters is tougher. Methods to estimate the remaining parameters will be considered
later. (This is a nonlinear optimization problem.)
We assume the model for the data in Fig. 4.1 is the analog signal
f (t) = at + b + c sin
 2π
T t
!
+ η(t).
(4.95)
Here the ﬁrst two terms model the trend (assumed to be a straight line), the third
term is the desired sinusoidal signal component, and η(t) is a random noise com-
ponent. We only possess samples of the signal fn = f (nTs) (i.e., t = nTs), for
n = 0, 1, . . . , N −1, where Ts is the sampling period of the data collection system.
medication. We emphasize that the model in (4.95) is grossly oversimpliﬁed. Indeed, a better model is
to replace term at + b with subharmonic, and harmonic terms of sin

2π
T t

. A harmonic term is one
of frequency 2π
T n, while a subharmonic has frequency 2π
T
1
n . Cosine terms should also be included in
the improved model.
TLFeBOOK

LEAST-SQUARES PROBLEMS AND QR DECOMPOSITION
163
We assume that we know T which is the period of the patient’s circadian rhythm.
Our model also implicitly assumes knowledge of the phase of the sinusoid, too.
These are very artiﬁcial assumptions since in practice these are the most important
parameters we are trying to estimate, and they are never known in advance. How-
ever, our present circumstances demand simpliﬁcation. Our estimate of fn may be
deﬁned by
ˆfn = aTsn + b + c sin
 2π
T nTs
!
.
(4.96)
This is a sampled version of the analog model, except the noise term has been
deleted.
We may estimate the unknown model parameters a, b, c by employing the same
basic strategy we used in Section 4.2, speciﬁcally, a least-squares approach. Thus,
deﬁning x = [a
b
c]T (vector of unknown parameters), we strive to minimize
V (x) =
N−1

n=0
e2
n =
N−1

n=0
[fn −ˆfn]2
(4.97)
with respect to x. Using matrix/vector notation was very helpful in Section 4.2,
and it remains so here. Deﬁne
vn =

Tsn
1
sin
 2π
T Tsn
!T
.
(4.98)
Thus
en = fn −vT
n x.
(4.99)
We may deﬁne the error vector e = [e0 e1
· · ·
eN−1]T , data vector f =
[f0 f1 · · · fN−1]T , and the matrix of basis vectors
A =


vT
0
vT
1
...
vT
N−1


∈RN×3.
(4.100)
Consequently, via (4.99)
e = f −Ax.
(4.101)
Obviously, we would like to have e = 0, which implies the desire to solve Ax = f .
If we have N = 3 and A−1 exists, then we may uniquely solve for x given any
f . However, in practice, N >> 3, so our linear system is overdetermined. Thus,
no unique solution is possible. We have no option but to select x to minimize e in
TLFeBOOK

164
LINEAR SYSTEMS OF EQUATIONS
some sense. Once again, previous experience from Section 4.2 says least-squares
is a viable choice. Thus, since ||e||2
2 = eT e = N−1
n=0 e2
n, we consider
V (x) = eT e = f T f
#$%&
=ρ
−2xT AT f
#$%&
=g
+ xT AT A
#$%&
=P
x
(4.102)
[which is a more compact version of (4.97)]. This is yet another quadratic form
[recall (4.8)]. We see that P ∈R3×3, and g ∈R3. In our problem A is full rank so
from the results in Section 4.2 we see that P > 0. Naturally, from the discussions
of Sections 4.3 and 4.4, the conditioning of P is a concern. Here it turns out that
because P is of low order (largely because we are interested only in estimating
three parameters) it typically has a low condition number. However, as the order
of P rises, the conditioning of P usually rapidly worsens; that is, ill conditioning
tends to be a severe problem when the number of parameters to be estimated rises.
From Section 4.2 we know that the optimum choice for x, denoted ˆx, is obtained
by solving the linear system
P ˆx = g.
(4.103)
The model curve of Fig. 4.1 (solid line) is the curve obtained using ˆx in (4.96).
Thus, since ˆx = [ˆa ˆb ˆc]T , we plot
ˆfn for ˆa, ˆb, ˆc in place of a, b, c in (4.96).
Equation (4.103) can be written as
AT Aˆx = AT f.
(4.104)
This is just the overdetermined linear system Aˆx = f multiplied on the left (i.e.,
premultiplied) by AT . The system (4.104) is often referred to in the literature as
the normal equations.
How is the previous applications example relevant to the problem of QR fac-
torizing A as in Eq. (4.93)? To answer this, we need to consider the condition
numbers of A, and of P = AT A, and to see how orthogonal matrices Q facili-
tate the solution of overdetermined least-squares problems. We will then move on
to the problem of how to practically compute the QR factorization of a full-rank
matrix. We will consider the issue of conditioning ﬁrst since this is a justiﬁcation
for considering QR factorization methods as opposed to the linear system solution
methods of the previous section.
Singular values were mentioned in Section 4.4 as being relevant to the problem
of computing spectral norms, and so of computing κ2(A). Now we need to consider
the consequences of
Theorem 4.4: Singular Value Decomposition (SVD)
Suppose A ∈Rm×n;
then there exist orthogonal matrices
U = [u0u1 · · · um−1] ∈Rm×m, V = [v0v1 · · · vn−1] ∈Rn×n
such that
 = UT AV = diag (σ0, σ1, . . . , σp−1) ∈Rm×n, p = min{m, n},
(4.105)
where σ0 ≥σ1 ≥. . . ≥σp−1 ≥0.
TLFeBOOK

LEAST-SQUARES PROBLEMS AND QR DECOMPOSITION
165
An outline proof appears in Ref. 5 (p. 71) and is omitted here. The nota-
tion diag(σ0, . . . , σp−1) means a diagonal matrix with main diagonal elements
σ0, . . . , σp−1. For example, if m = 3, n = 2, then p = 2, and
UT AV =


σ0
0
0
σ1
0
0

,
but if m = 2, n = 3, then again p = 2, but now
UT AV =
 σ0
0
0
0
σ1
0

.
The numbers σi are called singular values. Vector ui is the ith left singular vector,
and vi is the ith right singular vector. The following notation is helpful:
σi(A) = the ith singular value of A (i ∈Zp).
σmax(A) = the biggest singular value of A.
σmin(A) = the smallest singular value of A.
We observe that because AV = U, and AT U = V T we have, respectively
Avi = σiui, AT ui = σivi
(4.106)
for i ∈Zp. Singular values give matrix 2-norms; as noted in the following theorem.
Theorem 4.5:
||A||2 = σ0 = σmax(A).
Proof
Recall the result (4.37). From (4.105) A = UV T so
||Ax||2
2 = xT AT Ax,
and
AT A = V T V T =
p−1

i=0
σ 2
i vivT
i ∈Rn×n.
(4.107)
For any x ∈Rn there exist di such that
x =
n−1

i=0
divi
(4.108)
(because V is orthogonal so its column vectors form an orthogonal basis for Rn).
Thus
||x||2
2 = xT x =
n−1

i=0
n−1

j=0
didjvT
i vj =
n−1

i=0
d2
i
(4.109)
TLFeBOOK

166
LINEAR SYSTEMS OF EQUATIONS
(via vT
i vj = δi−j). Now
xT AT Ax =
p−1

i=0
σ 2
i (xT vi)(vT
i x) =
p−1

i=0
σ 2
i ⟨x, vi⟩2,
(4.110)
but
⟨x, vi⟩=

j
djvj, vi

=

j
dj⟨vi, vj⟩= di.
(4.111)
Using (4.111) in (4.110), we obtain
||Ax||2
2 =
n−1

i=0
σ 2
i d2
i
(4.112)
for which it is understood that σ 2
i = 0 for i > p −1. We maximize ||Ax||2
2 subject
to constraint ||x||2
2 = 1, which means employing Lagrange multipliers; that is, we
maximize
L(d) =
n−1

i=0
σ 2
i d2
i −λ
'n−1

i=0
d2
i −1
(
,
(4.113)
where d = [d0 d1 · · · dn−1]T . Thus
∂L(d)
∂dj
= 2σ 2
j dj −2λdj = 0,
or
σ 2
j dj = λdj.
(4.114)
From (4.114) into (4.112)
||Ax||2
2 = λ
n−1

i=0
d2
i = λ
(4.115)
for which we have used the fact that ||x||2
2 = 1 in (4.109). From (4.114) λ is the
eigenvalue of a diagonal matrix containing σ 2
i . Consequently, λ is maximized for
λ = σ 2
0 . Therefore, ||A||2 = σ0.
Suppose that
σ0 ≥· · · ≥σr−1 > σr = · · · = σp−1 = 0,
(4.116)
then
rank (A) = r.
(4.117)
TLFeBOOK

LEAST-SQUARES PROBLEMS AND QR DECOMPOSITION
167
Thus, the SVD of A can tell us the rank of A. In our overdetermined least-squares
problem we have m ≥n and A is assumed to be of full-rank. This implies that
r = n. Also, p = n. Thus, all singular values of a full-rank matrix are bigger than
zero. Now suppose that A−1 exists. From (4.105) A−1 = V −1UT . Immediately,
||A−1||2 = 1/σmin(A). Hence
κ2(A) = ||A||2||A−1||2 = σmax(A)
σmin(A) .
(4.118)
Thus, a large singular value spread is associated with matrix ill conditioning. [Recall
(4.61) and the related discussion.] As remarked on p. 223 of Ref. 5, Eq. (4.118)
can be extended to cover full-rank rectangular matrices with m ≥n:
A ∈Rm×n, rank (A) = n ⇒κ2(A) = σmax(A)
σmin(A) .
(4.119)
This also holds for the transpose of A because AT = V T UT , so AT has the
same singular values as A. Thus, κ2(AT ) = κ2(A). Golub and Van Loan [5, p. 225]
claim (without formal proof) that κ2(AT A) = [κ2(A)]2. In other words, if the linear
system Ax = f is ill-conditioned, then AT Aˆx = AT f is even more ill-conditioned.
The condition number of the latter system is the square of that of the former system.
More information on the conditioning of rectangular matrices is to be found in
Appendix 4.B. This includes justiﬁcation that κ2(AT A) = [κ2(A)]2.
A popular approach toward solving the normal equations AT Aˆx = AT f is based
on Cholesky decomposition
Theorem 4.6: Cholesky Decomposition
If R ∈Rn×n is symmetric and pos-
itive deﬁnite, then there exists a unique lower triangular matrix L ∈Rn×n with
positive diagonal entries such that R = LLT . This is the Cholesky decomposition
(factorization) of R.
Algorithms to ﬁnd this decomposition appear in Chapter 4 of Ref. 5. We do not
consider them except to note that if they are used, then the computed solution to
AT Aˆx = AT f , which we denote by ˆˆx, may satisfy
||ˆˆx −ˆx||2
||ˆx||2
≈u[κ2(A)]2,
(4.120)
where u is as in (4.89). Thus, this method of linear system solution is potentially
highly susceptible to errors due to ill-conditioned problems. On the other hand,
Cholesky approaches are computationally efﬁcient in that they require about n3/3
ﬂops (Floating-point operations). Clearly, Gaussian elimination may be employed
to solve the normal equations as well, but we recall that Gaussian elimination
needed about 2n3/3 ﬂops. Gaussian elimination is less efﬁcient because it does
not account for symmetry in matrix R. Note that these counts do not take into
consideration the number of ﬂops needed to determine AT A and AT f , and do
TLFeBOOK

168
LINEAR SYSTEMS OF EQUATIONS
not account for the number of ﬂops needed by the forward/backward substitution
steps. However, the comparison between Cholesky decomposition and Gaussian
elimination is reasonably fair because these other steps are essentially the same for
both approaches.
Recall that ||e||2
2 = ||Ax −f ||2
2. Thus, for orthogonal matrix Q
||QT e||2
2 = [QT e]T QT e = eT QQT e = eT e = ||e||2
2.
(4.121)
Thus, the 2-norm is invariant to orthogonal transformations. This is one of the
more important properties of 2-norms. Now consider
||e||2
2 = ||QT Ax −QT f ||2
2.
(4.122)
Suppose that
QT f =
 f u
f l

(4.123)
for which f u ∈Rn, and f l ∈Rm−n. Thus, from (4.94) and QT A = R, we obtain
QT Ax −QT f =

Rx −f u
−f l
	
,
(4.124)
implying that
||e||2
2 = ||Rx −f u||2
2 + ||f l||2
2.
(4.125)
Immediately, we see that
Rˆx = f u.
(4.126)
The least-squares optimal solution ˆx is therefore found by backward substitution.
Equally clearly, we see that
min
x ||e||2
2 = ||f l||2
2 = ρ2
LS.
(4.127)
This is the minimum error energy. Quantity ρ2
LS is also called the minimum sum
of squares, and e is called the residual [5]. It is easy to verify that κ2(Q) = 1
(Q is orthogonal). In other words, orthogonal matrices are perfectly conditioned.
This means that the operation QT A will not result in a matrix that is not as well
conditioned as A. This in turn suggests that solving our least-squares problem
using QR decomposition might be numerically more reliable than working with
the normal equations. As explained on p. 230 of Ref. 5, this is not necessarily
always true, but it is nevertheless a good reason to contemplate QR approaches to
solving least-squares problems.9
9If the residual is big and the problem is ill-conditioned, then neither QR nor normal equation methods
may give an accurate answer. However, QR approaches may be more accurate for small residuals in
ill-conditioned problems than normal equation approaches.
TLFeBOOK

LEAST-SQUARES PROBLEMS AND QR DECOMPOSITION
169
How may we compute Q? There are three major approaches:
1. Gram–Schmidt algorithms
2. Givens rotation algorithms
3. Householder transformation algorithms
We will consider only Householder transformations.
We begin by a review of how vectors are projected onto vectors. Recall the
law of cosines from trigonometry in reference to Fig. 4.2a. Assume that x, y ∈Rn.
Suppose that ||x −y||2 = a, ||x||2 = b, and that ||y||2 = c. Therefore, where θ is
the angle between x and y (0 ≤θ ≤π radians)
a2 = b2 + c2 −2bc cos θ,
(4.128)
or in terms of the vectors x and y, Eq. (4.128) becomes
||x −y||2
2 = ||x||2
2 + ||y||2
2 −2||x||2||y||2 cos θ.
(4.129)
In terms of inner products, this becomes
⟨x −y, x −y⟩= ⟨x, x⟩+ ⟨y, y⟩−2[⟨x, x⟩]1/2[⟨y, y⟩]1/2 cos θ,
which reduces to
⟨x, y⟩= [⟨x, x⟩]1/2[⟨y, y⟩]1/2 cos θ,
or
⟨x, y⟩= ||x||2||y||2 cos θ.
(4.130)
q
x − y
y
x
(a)
q
Pyx
y
z
x
(b)
Figure 4.2
Illustration of the law of cosines (a) and the projection of vector x onto vector
y (b).
TLFeBOOK

170
LINEAR SYSTEMS OF EQUATIONS
Now consider Fig. 4.2b. Vector Pyx is the projection of x onto y, where Py denotes
the projection operator that projects x onto y. It is immediately apparent that
||Pyx||2 = ||x||2 cos θ.
(4.131)
This is the Euclidean length of Pyx. The unit vector in the direction of y is y/||y||2.
Therefore
Pyx = ||x||2 cos θ
||y||2
y.
(4.132)
But from (4.130) this becomes
Pyx = ⟨x, y⟩
||y||2
2
y.
(4.133)
Since ⟨x, y⟩= xT y = yT x, we see that
Pyx = yT x
||y||2
2
y =

1
||y||2
2
yyT
	
x.
(4.134)
In (4.134) yyT ∈Rn×n, so the operator Py has the matrix representation
Py =
1
||y||2
2
yyT .
(4.135)
In Fig. 4.2b we see that z = x −Pyx, and that
z = (I −Py)x =

I −
1
||y||2
2
yyT
	
x,
(4.136)
which is the component of x that is orthogonal to y. We observe that
P 2
y =
1
||y||4
2
yyT yyT = y||y||2
2yT
||y||4
2
=
1
||y||2
2
yyT = Py.
(4.137)
If A2 = A, we say that matrix A is idempotent. Thus, projection operators are
idempotent. Also, P T
y = Py so projection operators are also symmetric.
In Fig. 4.3, x, y, z ∈Rn, and yT z = 0. Deﬁne the Householder transformation
matrix
H = I −2 yyT
||y||2
2
.
(4.138)
We see that H = I −2Py [via (4.135)]. Hence Hx is as shown in Fig. 4.3; that
is, the Householder transformation ﬁnds the reﬂection of vector x with respect to
vector z, and z ⊥y (Deﬁnition 1.6 of Chapter 1). Recall the unit vector ei ∈Rn
ei = [0 · · · 0
# $% &
i zeros
10 · · · 0]T ,
TLFeBOOK

LEAST-SQUARES PROBLEMS AND QR DECOMPOSITION
171
Hx
z
y
x
Pyx
Figure 4.3
Geometric interpretation of the Householder transformation operator H. Note
that zT y = 0.
so e0 = [10 · · · 0]T . Suppose that we want Hx = ae0 for some a ∈R with a ̸= 0;
that is, we wish to design H to annihilate all elements of x except for the top
element. Let y = x + αe0; then
yT x = (xT + αeT
0 )x = xT x + αx0
(4.139a)
(as x = [x0 x1 · · · xn−1]T ), and
||y||2
2 = (xT + αeT
0 )(x + αe0) = xT x + 2αx0 + α2.
(4.139b)
Therefore
Hx = x −2 yyT
||y||2
2
x = x −2 yT x
||y||2
2
y
so from (4.139), this becomes
Hx = x −2(xT x + αx0)(x + αe0)
||y||2
2
= x −2(xT x + αx0)x + α(yT x)e0
||y||2
2
=

1 −2
xT x + αx0
xT x + 2αx0 + α2

x −2α yT x
||y||2
2
e0.
(4.140)
To force the ﬁrst term to zero, we require
xT x + 2αx0 + α2 −2(xT x + αx0) = 0,
which implies that α2 = xT x, or in other words, we need
α = ±||x||2.
(4.141)
TLFeBOOK

172
LINEAR SYSTEMS OF EQUATIONS
Consequently, we select y = x ± ||x||2e0. In this case
Hx = −2α yT x
||y||2
2
e0 = −2α
xT x + αx0
xT x + 2αx0 + α2 e0
= −2α α2 + αx0
2α2 + 2αx0
e0 = −αe0 = ∓||x||2e0,
(4.142)
so Hx = ae0 for a = −α if y = x + αe0 with α = ±||x||2.
Example 4.6
Suppose x = [4
3
0]T , so ||x||2 = 5. Choose α = 5. Thus
y = [9
3
0]T , and
H = I −2yyT
yT y = 1
45


−36
−27
0
−27
36
0
0
0
45

.
We see that
Hx = 1
45


−36
−27
0
−27
36
0
0
0
45




4
3
0

= 1
45


−225
0
0

=


−5
0
0

,
so Hx = −αe0.
The Householder transformation is designed to annihilate elements of vectors. But
in contrast with the Gauss transformations of Section 4.5, Householder matrices
are orthogonal. To see this observe that
H T H =

I −2yyT
yT y
 
I −2yyT
yT y

= I −4yyT
yT y + 4yyT yyT
[yT y]2
= I −4yyT
yT y + 4yyT
yT y = I.
Thus, no matter how we select y, κ2(H) = 1. Householder matrices are therefore
perfectly conditioned.
To obtain R in (4.94), we deﬁne
˜Hk =
 Ik−1
0
0
Hk

∈Rm×m,
(4.143)
TLFeBOOK

LEAST-SQUARES PROBLEMS AND QR DECOMPOSITION
173
where k = 1, 2, . . . , n, Ik−1 is an order k −1 identity matrix, Hk is an order m −
k + 1 Householder transformation matrix. We design Hk to annihilate elements k
to m −1 of column k −1 in Ak−1, where A0 = A, and
Ak = ˜HkAk−1,
(4.144)
so An = R (in (4.94)). Much as in Section 4.5, we have Ak = [ak
i,j] ∈Rm×n, and
we assume m > n.
Example 4.7
Suppose
A = A0 =


a0
00
a0
01
a0
02
a0
10
a0
11
a0
12
a0
20
a0
21
a0
22
a0
30
a0
31
a0
32


∈R4×3,
and so therefore
A1 = ˜H1A0 =


×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×




a0
00
a0
01
a0
02
a0
10
a0
11
a0
12
a0
20
a0
21
a0
22
a0
30
a0
31
a0
32


=


a1
00
a1
01
a1
02
0
a1
11
a1
12
0
a1
21
a1
22
0
a1
31
a1
32


,
A2 = ˜H2A1 =


1
0
0
0
0
×
×
×
0
×
×
×
0
×
×
×




a1
00
a1
01
a1
02
0
a1
11
a1
12
0
a1
21
a1
22
0
a1
31
a1
32


=


a1
00
a1
01
a1
02
0
a2
11
a2
12
0
0
a2
22
0
0
a2
32


,
and
A3 = ˜H3A2 =


1
0
0
0
0
1
0
0
0
0
×
×
0
0
×
×




a1
00
a1
01
a1
02
0
a2
11
a2
12
0
0
a2
22
0
0
a2
32


=


a1
00
a1
01
a1
02
0
a2
11
a2
12
0
0
a3
22
0
0
0


= R.
TLFeBOOK

174
LINEAR SYSTEMS OF EQUATIONS
The × signs denote the Householder matrix elements that are not speciﬁed. This
example is intended only to show the general pattern of elements in the matrices.
Deﬁne
xk =

ak−1
k−1,k−1 ak−1
k,k−1 · · · ak−1
m−1,k−1
T
∈Rm−k+1,
(4.145)
so if xk =

xk
0 xk
1 · · · xk
m−k
T then xk
i = ak−1
i+k−1,k−1, and so
Hk = Im−k+1 −2yk(yk)T
(yk)T yk ,
(4.146)
where yk = xk ± ||xk||2ek
0, and ek
0 = [1 0 · · · 0]T ∈Rm−k+1. A pseudocode anal-
ogous to that for Gaussian elimination (recall Section 4.5) is as follows:
A0 := A;
for k := 1 to n do begin
for i := 0 to m −k do begin
xk
i := ak−1
i+k−1,k−1; {This loop makes xk}
end;
yk := xk + sign(xk
0)||xk||2ek
0;
Ak := ˜HkAk−1; { ˜Hk contains Hk via (4.146) }
end;
R := An;
From (4.143) ˜H T
k ˜Hk = Im because H T
k Hk = Im−k+1, and of course I T
k−1Ik−1 =
Ik−1; that is, ˜Hk is orthogonal for all k. Since
R = An = ˜Hn ˜Hn−1 · · · ˜H2 ˜H1A,
(4.147)
we have
A = ˜H T
1 ˜H T
2 · · · ˜H T
n
#
$%
&
=Q
R.
(4.148)
Thus, the pseudocode above implicitly computes Q because it creates the orthog-
onal factors ˜Hk.
In the pseudocode we see that
yk = xk + sign(xk
0) ||xk||2ek
0.
(4.149)
Recall from (4.141) that α = ±||x||2, so we must choose the sign of α. It is best that
α = sign(x0)||x||2, where sign(x0) = +1 for x0 ≥0, and sign(x0) = −1 if x0 < 0.
This turns out to ensure that H remains as close as possible to perfect orthogonality
in the face of rounding errors. Because ||x||2 might be very large or very small,
there is a risk of overﬂow or underﬂow in the computation of ||x||2. Thus, it
is often better to compute y from x/||x||∞. This works because scaling x does
TLFeBOOK

LEAST-SQUARES PROBLEMS AND QR DECOMPOSITION
175
not mathematically alter H (which may be conﬁrmed as an exercise). Typically,
m >> n (e.g., in the example of Fig. 4.1 we had m = N = 1000, while n = 3),
so, since ˜Hk ∈Rm×m, we rarely can accumulate and store the elements of ˜Hk for
all k as too much memory is needed for such a task. Instead, it is much better to
observe that (for example) if H ∈Rm×m, A ∈Rm×n then, as y ∈Rm, we have
HA =

I −2yyT
yT y

A = A −
2
yT y y(AT y)T .
(4.150)
From (4.150) AT y ∈Rn, which has jth element
[AT y]j =
m−1

k=0
ak,jyk
(4.151)
for j = 0, 1, . . . , n −1. If β = 2/yT y, then, from (4.150) and (4.151), we have
[HA]i,j = ai,j −βyi
m−1

k=0
ak,jyk
	
(4.152)
for i = 0, 1, . . . , m −1, and j = 0, 1, . . . , n −1. A pseudocode program that
implements this is as follows:
β := 2/yTy;
for j = 0 to n −1 do begin
s := m−1
k=0 ak,jyk;
s := βs;
for i := 0 to m −1 do begin
ai,j := ai,j −syi;
end;
end;
This program is written to overwrite matrix A with matrix HA. This reduces
computer system memory requirements. Recall (4.123), where we see that QT f
must be computed so that f u can be found. Knowledge of f u is essential to
compute ˆx via (4.126). As in the problem of computing ˜HkAk−1, we do not wish
to accumulate and save the factors ˜Hk in
QT f = ˜Hn ˜Hn−1 · · · ˜H1f.
(4.153)
Instead, QT f would be computed using an algorithm similar to that suggested by
(4.152).
All the suggestions in the previous paragraph are needed in a practical imple-
mentation of the Householder transformation matrix method for QR factorization.
As noted in Ref. 5, the rounding error performance of the practical Householder
QR factorization algorithm is quite good. It is stated [5] as well that the number of
ﬂops needed by the Householder method for ﬁnding ˆx is greater than that needed by
TLFeBOOK

176
LINEAR SYSTEMS OF EQUATIONS
Cholesky factorization. Somewhat simplistically, the Cholesky method is computa-
tionally more efﬁcient than the Householder method, but the Householder method
is less susceptible to ill conditioning and to rounding errors than is the Cholesky
method. More or less, there is therefore a tradeoff between speed and accuracy
involved in selecting between these competing methods for solving the overde-
termined least-squares problem. The Householder approach is also claimed [5] to
require more memory than the Cholesky approach.
4.7
ITERATIVE METHODS FOR LINEAR SYSTEMS
Matrix A ∈Rn×n is said to be sparse if most of its n2 elements are zero-valued.
Such matrices can arise in various applications, such as in the numerical solution
of partial differential equations (PDEs). Sections 4.5 and 4.6 have presented such
direct methods as the LU and QR decompositions (factorizations) of A in order to
solve Ax = b (assuming that A is nonsingular). However, these procedures do not
in themselves take advantage of any structure that may be possessed by A such
as sparsity. Thus, they are not necessarily computationally efﬁcient procedures.
Therefore, in the present section, we consider iterative methods to determine x ∈Rn
in Ax = b. In this section, whenever we consider Ax = b, we will always assume
that A−1 exists. Iterative methods work by creating a Cauchy sequence of vectors
(x(k)) that converges to x.10 Iterative methods may be particularly advantageous
when A is not only sparse, but is also large (i.e., large n). This is because direct
methods often require the considerable movement of data around the computing
machine memory system, and this can slow the computation down substantially. But
a properly conceived and implemented iterative method can alleviate this problem.
Our presentation of iterative methods here is based largely on the work of
Quarteroni et al. [8, Chapter 4]. We use much of the same notation as that in
Ref. 8. But it is a condensed presentation as this section is intended only to convey
the main ideas about iterative linear system solvers.
In Section 4.4 matrix and vector norms were considered in order to characterize
the sizes of errors in the numerical estimate of x in Ax = b due to perturbations
of A, and b. We will need to consider such norms here. As noted above, our goal
here is to derive a methodology to generate vector sequence (x(k))11 such that
lim
k→∞x(k) = x,
(4.154)
where x = [x0 x1 · · · xn−1]T ∈Rn satisﬁes Ax = b and x(k) = [x(k)
0
x(k)
1
· · ·
x(k)
n−1]T ∈Rn. The basic idea is to ﬁnd an operator T such that x(k+1) = T x(k)(=
T (x(k))), for k = 0, 1, 2, . . .. Because (x(k)) is designed to be Cauchy (recall
10As such, we will be revisiting ideas ﬁrst seen in Section 3.2.
11Note that the “(k)” in x(k) does not denote the raising of x to a power or the taking of the kth
derivative, but rather is part of the name of the vector. Similar notation applies to matrices. So, Ak is
the kth power of A, but A(k) is not.
TLFeBOOK

ITERATIVE METHODS FOR LINEAR SYSTEMS
177
Section 3.2) for any ϵ > 0, there will be an m ∈Z+ such that ||x(m) −x|| < ϵ
[recall that d(x(k), x) = ||x(k) −x||]. The operator T is deﬁned according to
x(k+1) = Bx(k) + f,
(4.155)
where x(0) ∈Rn is the starting value (initial guess about the solution x), B ∈Rn×n
is called the iteration matrix, and f ∈Rn is derived from A and b in Ax = b. Since
we want (4.154) to hold, from (4.155) we seek B and f such that x = Bx + f , or
A−1b = BA−1b + f (using Ax = b, implying x = A−1b), so
f = (I −B)A−1b.
(4.156)
The error vector at step k is deﬁned to be
e(k) = x(k) −x,
(4.157)
and naturally we want limk→∞e(k) = 0. Convergence would be in some suitably
selected norm.
As matters now stand, there is no guarantee that (4.154) will hold. We achieve
convergence only by the proper selection of B, and for matrices A possessing
suitable properties (considered below). Before we can consider these matters we
require certain basic results involving matrix norms.
Deﬁnition 4.3: Spectral Radius
Let s(A) denote the set of eigenvalues of
matrix A ∈Rn×n. The spectral radius of A is
ρ(A) = max
λ∈s(A) |λ|.
An important property possessed by ρ(A) is as follows.
Property 4.1
If A ∈Rn×n with ϵ > 0, then there is a norm denoted || · ||ϵ
(i.e., a norm perhaps dependent on ϵ) satisfying the consistency condition (4.36c),
and such that
||A||ϵ ≤ρ(A) + ϵ.
Proof
See Isaacson and Keller [9].
This is just a formal way of saying that there is always a matrix norm that is
arbitrarily close to the spectral radius of A
ρ(A) = inf
||·|| ||A||
(4.158)
with the inﬁmum (deﬁned in Section 1.3) taken over all possible norms that satisfy
(4.36c). We say that the sequence of matrices (A(k)) [with A(k) ∈Rn×n] converges
to A ∈Rn×n iff
lim
k→∞||A(k) −A|| = 0.
(4.159)
TLFeBOOK

178
LINEAR SYSTEMS OF EQUATIONS
The norm in (4.159) is arbitrary because of norm equivalence (recall discussion on
this idea in Section 4.4).
Theorem 4.7: Let A ∈Rn×n; then
lim
k→∞Ak = 0 ⇔ρ(A) < 1.
(4.160)
As well, the matrix geometric series ∞
k=0 Ak converges iff ρ(A) < 1. In this
instance
∞

k=0
Ak = (I −A)−1.
(4.161)
So, if ρ(A) < 1, then matrix I −A is invertible, and also
1
1 + ||A|| ≤||(I −A)−1|| ≤
1
1 −||A||,
(4.162)
where || · || here is an induced matrix norm (i.e., (4.36b) holds) such that ||A|| < 1.
Proof
We begin by showing (4.160) holds. Let ρ(A) < 1 so there must be an
ϵ > 0 such that ρ(A) < 1 −ϵ, and from Property 4.1 there is a consistent matrix
norm || · || such that
||A|| ≤ρ(A) + ϵ < 1.
Because [recall (4.40)] of ||Ak|| ≤||A||k < 1, and the deﬁnition of convergence,
as k →∞, we have Ak →0 ∈Rn×n. Conversely, assume that limk→∞Ak = 0,
and let λ be any eigenvalue of A. For eigenvector x (̸= 0) of A associated with
eigenvalue λ, we have Akx = λkx, and so limk→∞λk = 0. Thus, |λ| < 1, and
hence ρ(A) < 1. Now consider (4.161). If λ is an eigenvalue of A, then 1 −λ is
an eigenvalue of I −A. We observe that
(I −A)(I + A + A2 + · · · + An−1 + An) = I −An+1.
(4.163)
Since ρ(A) < 1, I −A has an inverse, and letting n →∞in (4.163) yields
(I −A)
∞

k=0
Ak = I
so that (4.161) holds.
Now, because matrix norm || · || satisﬁes (4.36b), we must have ||I|| = 1. Thus
1 = ||I|| ≤||I −A|| ||(I −A)−1|| ≤(1 + ||A||)||(I −A)−1||,
TLFeBOOK

ITERATIVE METHODS FOR LINEAR SYSTEMS
179
which gives the ﬁrst inequality in (4.162). Since I = (I −A) + A, we have
(I −A)−1 = I + A(I −A)−1
so that
||(I −A)−1|| ≤1 + ||A|| ||(I −A)−1||.
Condition ||A|| < 1 implies that this yields the second inequality in (4.162).
We mention that in Theorem 4.7 an induced matrix norm exists to give ||A|| <
1 because of Property 4.1 (recall that (A(k)) is convergent, giving ρ(A) < 1).
Theorem 4.7 now leads us to the following theorem.
Theorem 4.8: Suppose that f ∈Rn satisﬁes (4.156); then (x(k)) converges to
x satisfying Ax = b for any x(0) iff ρ(B) < 1.
Proof
From (4.155)–(4.157), we have
e(k+1) = x(k+1) −x = Bx(k) + f −x = Bx(k) + (I −B)A−1b −x
= Be(k) + Bx + (I −B)A−1b −x
= Be(k) + Bx + x −Bx −x
= Be(k).
Immediately, we see that
e(k) = Bke(0)
(4.164)
for k ∈Z+. From Theorem 4.7
lim
k→∞Bke(0) = 0
for all e(0) ∈Rn iff ρ(B) < 1.
On the other hand, suppose ρ(B) ≥1; then there is at least one eigenvalue λ of
B such that |λ| ≥1. Let e(0) be the eigenvector associated with λ, so Be(0) = λe(0),
implying that e(k) = λke(0). But this implies that e(k) ̸→0 as k →∞since |λ| ≥1.
This theorem gives a general condition on B so that iterative procedure (4.155)
converges. Theorem 4.9 (below) will say more. However, our problem now is to
ﬁnd B. From (4.158), and Theorem 4.7 a sufﬁcient condition for convergence is
that ||B|| < 1, for any matrix norm.
A general approach to constructing iterative methods is to use the additive
splitting of the matrix A according to
A = P −N,
(4.165)
TLFeBOOK

180
LINEAR SYSTEMS OF EQUATIONS
where P, N ∈Rn×n are suitable matrices, and P −1 exists. Matrix P is sometimes
called a preconditioning matrix, or preconditioner (for reasons we will not consider
here, but that are explained in Ref. 8). To be speciﬁc, we rewrite (4.155) as
x(k+1) = P −1Nx(k) + P −1b,
that is, for k ∈Z+
P x(k+1) = Nx(k) + b,
(4.166)
so that f = P −1b, and B = P −1N. Alternatively
x(k+1) = x(k) + P −1 [b −Ax(k)]
#
$%
&
=r(k)
,
(4.167)
where r(k) is the residual vector at step k. From (4.167) we see that to obtain
x(k+1) requires us to solve a linear system of equations involving P . Clearly, for
this approach to be worth the trouble, P must be nonsingular, and be easy to invert
as well in order to save on computations.
We will now make the additional assumption that the main diagonal elements of
A are nonzero (i.e., ai,i ̸= 0 for all i ∈Zn). All the iterative methods we consider in
this section will assume this. In this case we may express Ax = b in the equivalent
form
xi = 1
aii

bi −
n−1

j=0
j̸=i
aijxj


(4.168)
for i = 0, 1, . . . , n −1.
The expression (4.168) immediately leads to, for any initial guess x(0), the
Jacobi method, which is deﬁned by the iterations
x(k+1)
i
= 1
aii

bi −
n−1

j=0
j̸=i
aijx(k)
j


(4.169)
for i = 0, 1, . . . , n −1. It is easy to show that this algorithm implements the
splitting
P = D, N = D −A = L + U,
(4.170)
where D = diag(a0,0, a1,1, . . . , an−1,n−1) (i.e., diagonal matrix that is the main
diagonal elements of A), L is the lower triangular matrix such that lij = −aij
if i > j, and lij = 0 if i ≤j, and U is the upper triangular matrix such that
uij = −aij if j > i, and uij = 0 if j ≤i. Here the iteration matrix B is given by
B = BJ = P −1N = D−1(L + U) = I −D−1A.
(4.171)
TLFeBOOK

ITERATIVE METHODS FOR LINEAR SYSTEMS
181
The Jacobi method generalizes according to
x(k+1)
i
= ω
aii

bi −
n−1

j=0
j̸=i
aijx(k)
j

+ (1 −ω)x(k)
i
,
(4.172)
where i = 0, 1, . . . , n −1, and ω is the relaxation parameter. Relaxation parame-
ters are introduced into iterative procedures in order to control convergence rates.
The algorithm (4.172) is called the Jacobi overrelaxation (JOR) method. In this
algorithm the iteration matrix B takes on the form
B = BJ (ω) = ωBJ + (1 −ω)I,
(4.173)
and (4.172) can be expressed in the form (4.167) according to
x(k+1) = x(k) + ωD−1r(k).
(4.174)
The JOR method satisﬁes (4.156) provided that ω ̸= 0. The method is easily seen
to reduce to the Jacobi method when ω = 1.
An alternative to the Jacobi method is the Gauss–Seidel method. This is deﬁned
as
x(k+1)
i
= 1
aii

bi −
i−1

j=0
aijx(k+1)
j
−
n−1

j=i+1
aijx(k)
j

,
(4.175)
where i = 0, 1, . . . , n −1. In matrix form (4.175) can be expressed as
Dx(k+1) = b + Lx(k+1) + Ux(k),
(4.176)
where D, L, and U are the same matrices as those associated with the Jacobi
method. In the Gauss–Seidel method we implement the splitting
P = D −L,
N = U
(4.177)
with the iteration matrix
B = BGS = (D −L)−1U.
(4.178)
As there is an overrelaxation method for the Jacobi approach, the same idea applies
for the Gauss–Seidel case. The Gauss–Seidel successive overrelaxation (SOR)
method is deﬁned to be
x(k+1)
i
= ω
aii

bi −
i−1

j=0
aijx(k+1)
j
−
n−1

j=i+1
aijx(k)
j

+ (1 −ω)x(k)
i
,
(4.179)
TLFeBOOK

182
LINEAR SYSTEMS OF EQUATIONS
again for i = 0, 1, . . . , n −1. In matrix form this procedure can be expressed as
Dx(k+1) = ω[b + Lx(k+1) + Ux(k)] + (1 −ω)Dx(k)
or
[I −ωD−1L]x(k+1) = ωD−1b + [(1 −ω)I + ωD−1U]x(k),
(4.180)
for which the iteration matrix is now
B = BGS(ω) = [I −ωD−1L]−1[(1 −ω)I + ωD−1U].
(4.181)
We see from (4.180) (on multiplying both sides by D) that
[D −ωL]x(k+1) = ωb + [(1 −ω)D + ωU]x(k),
so from the fact that A = D −(L + U) [recall (4.170)], this may be rearranged as
x(k+1) = x(k) +
 1
ωD −L
−1
r(k)
(4.182)
(r(k) = b −Ax(k)), which is the form (4.167). Condition (4.156) holds if ω ̸= 0.
The case ω = 1 corresponds to the Gauss–Seidel method in (4.175). If ω ∈(0, 1),
the technique is often called an underrelaxation method, while for ω ∈(1, ∞) it
is an overrelaxation method.
We will now summarize, largely without proof, results concerning the conver-
gence of (x(k)) to x for sequences generated by the previous iterative algorithms.
We observe that every iteration in any of the proposed methods needs (in the worst
case, assuming that A is not sparse) O(n2) arithmetic operations. The total number
of iterations is m, and is needed to achieve desired accuracy ||x(m) −x|| < ϵ, and
so in turn the total number of arithmetic operations needed is O(mn2). Gaussian
elimination needs O(n3) operations to solve Ax = b, so the iterative methods are
worthwhile computationally only if m is sufﬁciently small. If m is about the same
size as n, then little advantage can be expected from iterative methods. On the
other hand, if A is sparse, perhaps possessing only O(n) nonzero elements, then
the iterative methods require only O(mn) operations to achieve ||x(m) −x|| < ϵ.
We need to give conditions on A so that x(k) →x, and also to say something about
the number of iterations needed to achieve convergence to desired accuracy.
Let us begin with the following deﬁnition.
Deﬁnition 4.4: Diagonal Dominance
Matrix A ∈Rn×n is diagonally domi-
nant if
|ai,i| >
n−1

j=0
j̸=i
|aij|
(4.183)
for i = 0, 1, . . . , n −1.
TLFeBOOK

ITERATIVE METHODS FOR LINEAR SYSTEMS
183
We mention here that Deﬁnition 4.4 is a bit different from Deﬁnition 6.2 (Chapter 6),
where diagonal dominance concepts appear in the context of spline interpolation
problems. It can be shown that if A in Ax = b is diagonally dominant according to
Deﬁnition 4.4, then the Jacobi and Gauss-Seidel methods both converge. Proof for
the Jacobi method appears in Theorem 4.2 of Ref. 8, while the Gauss–Seidel case is
proved by Axelsson [10].
If A = AT , and A > 0 both the Jacobi and Gauss–Seidel methods will converge.
A proof for the Gauss–Seidel case appears in Golub and Van Loan [5, Theo-
rem 10.1.2]. The Jacobi case is considered in Ref. 8. Convergence results exist for
the overrelaxation methods JOR and SOR. For example, if A = AT with A > 0,
the SOR method is convergent iff 0 < ω < 2 [8]. Naturally, we wish to select ω so
that convergence occurs as rapidly as possible (i.e., m in ||x(m) −x|| < ϵ is min-
imal). However, the problem of selecting the optimal value for ω is well beyond
the scope of this book.
We recall that our iterative procedures have the general form in (4.155), where
it is intended that x = Bx + f . We may regard y = T x = Bx + f as a mapping
T |Rn →Rn. On linear vector space Rn we may deﬁne the metric
d(x, y) = max
j∈Zn
|xj −yj|
(4.184)
(recall the properties of metrics from Chapter 1). Space (Rn, d) is a complete metric
space [11, p. 308]. From Kreyszig [11] we have the following theorem.
Theorem 4.9: If the linear system x = Bx + f is such that
n−1

j=0
|bij| < 1
for i = 0, 1, . . . , n −1 then solution x is unique. The solution can be obtained as
the limit of the vector sequence (x(k)) for k = 0, 1, 2, . . . (x(0) is arbitrary), where
x(k+1) = Bx(k) + f,
and where for α = maxi∈Zn
n−1
j=0 |bij|, we have the error bounds
d(x(m), x) ≤
α
1 −α d(x(m−1), x(m)) ≤
αm
1 −α d(x(0), x(1)).
(4.185)
Proof
We will give only an outline proof. This theorem is really just a special
instance of the contraction theorem, which appears and is proved in Chapter 7 (see
Theorem 7.3 and Corollary 7.1).
The essence of the proof is to consider the fact that
d(T x, T y) = max
i∈Zn

n−1

j=0
bij(xj −yj)

TLFeBOOK

184
LINEAR SYSTEMS OF EQUATIONS
≤max
j∈Zn
|xj −yj| max
i∈Zn
n−1

j=0
|bij|
= d(x, y) max
i∈Zn
n−1

j=0
|bij|,
so d(T x, T y) ≤αd(x, y), if we deﬁne
α = max
i∈Zn
n−1

j=0
|bij| = ||B||∞
[recall (4.41d)].
In this theorem we see that if α < 1, then x(k) →x. In this case d(T x, T y) <
d(x, y) for all x, y ∈Rn. Such a mapping T is called a contraction mapping (or
contractive mapping). We see that contraction mappings have the effect of moving
points in a space closer together. The error bounds stated in (4.185) give us an
idea about the number of iterations m needed to achieve ||x(m) −x|| < ϵ (ϵ >
0). We emphasize that condition α < 1 is sufﬁcient for convergence, so (x(k))
may converge to x even if this condition is violated. It is also noteworthy that
convergence will be fast if α is small, that is, if ||B||∞is small. The result in
Theorem 4.8 certainly suggests convergence ought to be fast if ρ(B) is small.
Example 4.8
We shall consider the application of SOR to the problem of
solving Ax = b, where
A =


4
1
0
0
1
4
1
0
0
1
4
1
0
0
1
4

,
b =


1
2
3
4

.
We shall assume that x(0) = [0000]T . Note that SOR is not the best way to solve
this problem. A better approach is to be found in Section 6.5 (Chapter 6). This
example is for illustration only. However, it is easy to conﬁrm that
x = [0.1627
0.3493
0.4402
0.8900]T .
Recall that the SOR iterations are speciﬁed by (4.179). However, we have not
discussed how to terminate the iterative process. A popular choice is to recall that
r(k) = b −Ax(k) [see (4.167)], and to stop the iterations when for k = m
||r(m)||
||r(0)|| ≤τ
(4.186)
TLFeBOOK

ITERATIVE METHODS FOR LINEAR SYSTEMS
185
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
0
50
100
150
w
m
 Number of iterations needed by SOR 
Figure 4.4
Plot of the number of iterations m needed by SOR as a function of ω for the
parameters of Example 4.8 in order to satisfy the stopping condition ||r(m)||∞/||r(0||∞≤τ.
for some τ > 0 (a small value). For our present purposes || · || shall be the norm
in (4.29c), which is compatible with the needs of Theorem 4.9. We shall choose
τ = 0.001.
We observe that A is diagonally dominant, so convergence is certainly expected
for ω = 1. In fact, A > 0 so convergence of the SOR method can be expected for
all ω ∈(0, 2). Figure 4.4 plots the m that achieves (4.186) versus ω, and we see
that there is an optimal choice for ω that is somewhat larger than ω = 1. In this
case though the optimal choice does not lead to much of an improvement over
choice ω = 1.
For our problem [recalling (4.170)], we have
D =


4
0
0
0
0
4
0
0
0
0
4
0
0
0
0
4

, L =


0
0
0
0
−1
0
0
0
0
−1
0
0
0
0
−1
0

,
U =


0
−1
0
0
0
0
−1
0
0
0
0
−1
0
0
0
0

.
From (4.178)
BGS =


0.0000
−0.2500
0.0000
0.0000
0.0000
0.0625
−0.2500
0.0000
0.0000
−0.0156
0.0625
−0.2500
0.0000
0.0039
−0.0156
0.0625

.
We therefore ﬁnd that ||BGS||∞= 0.3281. It is possible to show (preferably using
MATLAB or some other software tool that is good with eigenproblems) that
TLFeBOOK

186
LINEAR SYSTEMS OF EQUATIONS
ρ(BGS) = 0.1636. Given (4.185) in Theorem 4.9 we therefore expect fast con-
vergence for our problem since α is fairly small. In fact
||x(m) −x||∞≤
||BGS||m
∞
1 −||BGS||∞
||(D −L)−1b||∞
(4.187)
[using x(0) = 0, x(1) = (D −L)−1b]. For the stopping criterion of (4.186) we
obtained (recalling that ω = 1, and τ = 0.001) m = 5 with
x(5) = [0.1630
0.3490
0.4403
0.8899]T
so that
||x(5) −x||∞= 3.6455 × 10−4.
The right-hand side of (4.187) evaluates to
||BGS||m
∞
1 −||BGS||∞
||(D −L)−1b||∞= 4.7523 × 10−3.
Thus, (4.187) certainly holds true.
4.8
FINAL REMARKS
We have seen that inaccurate solutions to linear systems of equations can arise
when the linear system is ill-conditioned. Condition numbers warn us if this is
a potential problem. However, even if a problem is well-conditioned, an inac-
curate solution may arise if the algorithm applied to solve it is unstable. In the
case of problems arising out of algorithm instability, we naturally replace the
unstable algorithm with a stable one (e.g., Gaussian elimination may need to
be replaced by Gaussian elimination with partial pivoting). In the case of an
ill-conditioned problem, we may try to improve the accuracy of the solution by
either
1. Using an algorithm that does not worsen the conditioning of the underlying
problem (e.g., choosing QR factorization in preference to Cholesky factor-
ization)
2. Reformulating the problem so that it is better conditioned
We have not considered the second alternative in this chapter. This will be done
in Chapter 5.
APPENDIX 4.A
HILBERT MATRIX INVERSES
Consider the following MATLAB code:
R = hilb(10);
inv(R)
TLFeBOOK

HILBERT MATRIX INVERSES
187
ans =
1.0e+12 *
Columns 1 through 7
0.0000
-0.0000
0.0000
-0.0000
0.0000
-0.0000
0.0000
-0.0000
0.0000
-0.0000
0.0000
-0.0002
0.0005
-0.0008
0.0000
-0.0000
0.0001
-0.0010
0.0043
-0.0112
0.0178
-0.0000
0.0000
-0.0010
0.0082
-0.0379
0.1010
-0.1616
0.0000
-0.0002
0.0043
-0.0379
0.1767
-0.4772
0.7712
-0.0000
0.0005
-0.0112
0.1010
-0.4772
1.3014
-2.1208
0.0000
-0.0008
0.0178
-0.1616
0.7712
-2.1208
3.4803
-0.0000
0.0008
-0.0166
0.1529
-0.7358
2.0376
-3.3636
0.0000
-0.0004
0.0085
-0.0788
0.3820
-1.0643
1.7659
-0.0000
0.0001
-0.0018
0.0171
-0.0832
0.2330
-0.3883
Columns 8 through 10
-0.0000
0.0000
-0.0000
0.0008
-0.0004
0.0001
-0.0166
0.0085
-0.0018
0.1529
-0.0788
0.0171
-0.7358
0.3820
-0.0832
2.0376
-1.0643
0.2330
-3.3636
1.7659
-0.3883
3.2675
-1.7231
0.3804
-1.7231
0.9122
-0.2021
0.3804
-0.2021
0.0449
R*inv(R)
ans =
Columns 1 through 7
1.0000
0.0000
0.0000
-0.0000
0.0001
0.0001
-0.0001
-0.0000
1.0000
0.0000
-0.0000
0.0001
0.0001
-0.0002
-0.0000
0.0000
1.0000
-0.0000
0.0001
0.0000
-0.0001
-0.0000
0.0000
0.0000
1.0000
0.0000
0.0000
-0.0000
-0.0000
0.0000
0.0000
-0.0000
1.0000
-0.0000
-0.0000
-0.0000
0.0000
0.0000
-0.0000
0.0000
1.0000
-0.0000
-0.0000
0.0000
0.0000
-0.0000
0.0000
0.0000
0.9999
-0.0000
0.0000
0.0000
-0.0000
0.0000
0.0001
-0.0000
-0.0000
0.0000
0.0000
-0.0000
0.0000
0.0000
-0.0001
-0.0000
0.0000
0.0000
-0.0000
0.0000
0.0000
-0.0000
Columns 8 through 10
-0.0000
-0.0001
0.0000
0.0001
-0.0001
0.0000
0.0000
-0.0001
0.0000
-0.0000
-0.0000
0.0000
TLFeBOOK

188
LINEAR SYSTEMS OF EQUATIONS
0.0000
-0.0000
0.0000
-0.0000
-0.0000
0.0000
0.0000
-0.0000
0.0000
1.0000
-0.0000
0.0000
0.0000
1.0000
0.0000
-0.0000
-0.0000
1.0000
R = hilb(11);
inv(R)
ans =
1.0e+14 *
Columns 1 through 7
0.0000
-0.0000
0.0000
-0.0000
0.0000
-0.0000
0.0000
-0.0000
0.0000
-0.0000
0.0000
-0.0000
0.0000
-0.0000
0.0000
-0.0000
0.0000
-0.0000
0.0002
-0.0006
0.0012
-0.0000
0.0000
-0.0000
0.0003
-0.0019
0.0064
-0.0137
0.0000
-0.0000
0.0002
-0.0019
0.0110
-0.0381
0.0817
-0.0000
0.0000
-0.0006
0.0064
-0.0381
0.1329
-0.2877
0.0000
-0.0000
0.0012
-0.0137
0.0817
-0.2877
0.6270
-0.0000
0.0001
-0.0016
0.0183
-0.1101
0.3902
-0.8555
0.0000
-0.0000
0.0013
-0.0149
0.0905
-0.3227
0.7111
-0.0000
0.0000
-0.0006
0.0068
-0.0415
0.1487
-0.3292
0.0000
-0.0000
0.0001
-0.0013
0.0081
-0.0293
0.0651
Columns 8 through 11
-0.0000
0.0000
-0.0000
0.0000
0.0001
-0.0000
0.0000
-0.0000
-0.0016
0.0013
-0.0006
0.0001
0.0183
-0.0149
0.0068
-0.0013
-0.1101
0.0905
-0.0415
0.0081
0.3902
-0.3227
0.1487
-0.0293
-0.8555
0.7111
-0.3292
0.0651
1.1733
-0.9796
0.4553
-0.0903
-0.9796
0.8212
-0.3830
0.0762
0.4553
-0.3830
0.1792
-0.0357
-0.0903
0.0762
-0.0357
0.0071
R*inv(R)
ans =
Columns 1 through 7
0.9997
-0.0009
0.0022
0.0028
-0.0164
0.0558
-0.1229
-0.0002
0.9992
0.0020
0.0023
-0.0132
0.0454
-0.1029
-0.0002
-0.0007
1.0018
0.0019
-0.0112
0.0385
-0.0844
-0.0002
-0.0006
0.0016
1.0017
-0.0097
0.0331
-0.0736
-0.0002
-0.0006
0.0015
0.0015
0.9915
0.0285
-0.0638
-0.0002
-0.0005
0.0014
0.0013
-0.0076
1.0258
-0.0581
TLFeBOOK

HILBERT MATRIX INVERSES
189
-0.0002
-0.0005
0.0013
0.0012
-0.0070
0.0234
0.9468
-0.0001
-0.0005
0.0012
0.0011
-0.0063
0.0216
-0.0474
-0.0001
-0.0004
0.0011
0.0010
-0.0059
0.0201
-0.0448
-0.0001
-0.0004
0.0010
0.0009
-0.0053
0.0187
-0.0406
-0.0001
-0.0004
0.0010
0.0009
-0.0052
0.0179
-0.0395
Columns 8 through 11
0.1665
-0.1405
0.0652
-0.0091
0.1351
-0.1165
0.0530
-0.0071
0.1125
-0.0973
0.0452
-0.0058
0.0964
-0.0844
0.0385
-0.0047
0.0858
-0.0739
0.0341
-0.0041
0.0745
-0.0661
0.0300
-0.0037
0.0696
-0.0592
0.0280
-0.0033
1.0635
-0.0547
0.0251
-0.0029
0.0581
0.9495
0.0235
-0.0028
0.0536
-0.0458
1.0213
-0.0024
0.0527
-0.0445
0.0207
0.9976
R = hilb(12);
inv(R)
Warning: Matrix is close to singular or badly scaled.
Results may be inaccurate. RCOND = 2.632091e-17.
ans =
1.0e+15 *
Columns 1 through 7
0.0000
-0.0000
0.0000
-0.0000
0.0000
-0.0000
0.0000
-0.0000
0.0000
-0.0000
0.0000
-0.0000
0.0000
-0.0000
0.0000
-0.0000
0.0000
-0.0000
0.0001
-0.0002
0.0006
-0.0000
0.0000
-0.0000
0.0001
-0.0008
0.0032
-0.0086
0.0000
-0.0000
0.0001
-0.0008
0.0054
-0.0229
0.0624
-0.0000
0.0000
-0.0002
0.0032
-0.0229
0.0990
-0.2720
0.0000
-0.0000
0.0006
-0.0086
0.0624
-0.2720
0.7528
-0.0000
0.0000
-0.0011
0.0151
-0.1107
0.4863
-1.3545
0.0000
-0.0000
0.0013
-0.0173
0.1276
-0.5640
1.5794
-0.0000
0.0000
-0.0009
0.0124
-0.0920
0.4090
-1.1511
0.0000
-0.0000
0.0004
-0.0050
0.0377
-0.1686
0.4765
-0.0000
0.0000
-0.0001
0.0009
-0.0067
0.0301
-0.0855
Columns 8 through 12
-0.0000
0.0000
-0.0000
0.0000
-0.0000
0.0000
-0.0000
0.0000
-0.0000
0.0000
-0.0011
0.0013
-0.0009
0.0004
-0.0001
0.0151
-0.0173
0.0124
-0.0050
0.0009
-0.1107
0.1275
-0.0920
0.0377
-0.0067
0.4863
-0.5639
0.4090
-0.1686
0.0301
-1.3544
1.5793
-1.1510
0.4765
-0.0855
2.4505
-2.8712
2.1015
-0.8732
0.1572
TLFeBOOK

190
LINEAR SYSTEMS OF EQUATIONS
-2.8713
3.3786
-2.4821
1.0348
-0.1869
2.1016
-2.4822
1.8297
-0.7651
0.1385
-0.8732
1.0348
-0.7651
0.3208
-0.0582
0.1572
-0.1869
0.1386
-0.0582
0.0106
R*inv(R)
Warning: Matrix is close to singular or badly scaled.
Results may be inaccurate. RCOND = 2.632091e-17.
ans =
Columns 1 through 7
1.0126
-0.0066
-0.0401
0.0075
0.1532
-0.8140
2.2383
0.0113
0.9943
-0.0361
0.0100
0.1162
-0.6265
1.7168
0.0103
-0.0050
0.9673
0.0106
0.0952
-0.5300
1.4834
0.0094
-0.0045
-0.0299
1.0104
0.0797
-0.4573
1.2725
0.0087
-0.0041
-0.0275
0.0104
1.0703
-0.4038
1.0986
0.0081
-0.0037
-0.0255
0.0102
0.0621
0.6416
0.9971
0.0075
-0.0034
-0.0237
0.0099
0.0554
-0.3245
1.9062
0.0071
-0.0032
-0.0222
0.0095
0.0495
-0.2944
0.8232
0.0066
-0.0030
-0.0209
0.0093
0.0439
-0.2686
0.7246
0.0063
-0.0028
-0.0197
0.0087
0.0424
-0.2532
0.7002
0.0059
-0.0026
-0.0187
0.0087
0.0372
-0.2307
0.6309
0.0057
-0.0024
-0.0177
0.0081
0.0358
-0.2168
0.6064
Columns 8 through 12
-4.0762
4.7656
-3.5039
1.4385
-0.2183
-3.1582
3.7754
-2.7520
1.1123
-0.1649
-2.6250
3.1055
-2.3301
0.9219
-0.1390
-2.2676
2.6602
-1.9922
0.7905
-0.1163
-1.9863
2.4023
-1.7139
0.7104
-0.0992
-1.7969
2.1094
-1.5430
0.6289
-0.0897
-1.6133
1.9258
-1.4043
0.5581
-0.0779
-0.4658
1.7598
-1.2734
0.5146
-0.0715
-1.3047
2.5762
-1.1445
0.4629
-0.0651
-1.2793
1.5098
-0.1055
0.4424
-0.0619
-1.1387
1.3438
-0.9873
1.3955
-0.0529
-1.1025
1.2998
-0.9395
0.3809
0.9474
diary off
The MATLAB rcond function (which gave the number RCOND above) needs
some explanation. A useful reference on this is Hill [3, pp. 229–230]. It is based on
a condition number estimator in the old FORTRAN codes known as “LINPACK”.
It is based on 1-norms. rcond(A) will give the reciprocal of the 1-norm condition
number of A. If A is well-conditioned, then rcond(A) will be close to unity (i.e.,
close to one), and will be very tiny if A is ill-conditioned. The rule of thumb
involved in interpreting an rcond output is “if rcond(A) ≈d × 10−k, where d is
a digit from 1 to 9, then the elements of xcomp can usually be expected to have
k fewer signiﬁcant digits of accuracy than the elements of A” [3]. Here xcomp
TLFeBOOK

SVD AND LEAST SQUARES
191
is simply the computed solution to Ax = y; that is, in the notation of the present
set of notes, ˆx =xcomp. MATLAB does arithmetic with about 16 decimal digits
[3, p. 228], so in the preceding example of a Hilbert matrix inversion problem for
N = 12, since RCOND is about 10−17, we have lost about 17 digits in computing
R−1. Of course, this loss is catastrophic for our problem.
APPENDIX 4.B
SVD AND LEAST SQUARES
From Theorem 4.4, A = UV T , so this expands into the summation
A =
p−1

i=0
σiuivT
i .
(4.A.1)
But if r = rank (A), then (4.A.1) reduces to
A =
r−1

i=0
σiuivT
i .
(4.A.2)
In the following theorem ρ2
LS = ||f l||2
2 = ||Aˆx −f ||2
2 [see (4.127)], and ˆx is the
least-squares optimal solution to Ax = f .
Theorem 4.B.1: Let A be represented as in (4.A.2) with A ∈Rm×n and m ≥n.
If f ∈Rm then
ˆx =
r−1

i=0
'
uT
i f
σi
(
vi,
(4.A.3)
ρ2
LS =
m−1

i=r
(uT
i f )2.
(4.A.4)
Proof
For all x ∈Rn, using the invariance of the 2-norm to orthogonal trans-
formations, and the fact that V V T = In (n × n identity matrix)
||Ax −f ||2
2 = ||UT AV (V T x) −UT f ||2
2 = ||α −UT f ||2
2,
(4.A.5)
where α = V T x, so as α = [α0 · · · αn−1]T we have αi = vT
i x. Equation (4.A.5)
expands as
||Ax −f ||2
2 = αT T α −2αT T UT f + f T UUT f,
(4.A.6)
TLFeBOOK

192
LINEAR SYSTEMS OF EQUATIONS
which further expands as
||Ax −f ||2
2 =
r−1

i=0
σ 2
i α2
i −2
r−1

i=0
αiσiuT
i f +
m−1

i=0
[uT
i f ]2
=
r−1

i=0
[σ 2
i α2
i −2αiσiuT
i f + [uT
i f ]2] +
m−1

i=r
[uT
i f ]2
=
r−1

i=0
[σiαi −uT
i f ]2 +
m−1

i=r
[uT
i f ]2.
(4.A.7)
To minimize this we must have σiαi −uT
i f = 0, and so
αi = 1
σi
uT
i f
(4.A.8)
for i ∈Zr. As α = V T x, we have x = V α, so if we set αr = · · · = αn−1 = 0, then
from Eq. (4.A.8), we obtain
ˆx =
r−1

i=0
αivi =
r−1

i=0
uT
i f
σi
vi,
which is (4.A.3). For this choice of ˆx from (4.A.7)
||Aˆx −f ||2
2 =
m−1

i=r
[uT
i f ]2 = ρ2
LS,
which is (4.A.4).
Deﬁne A+ = V +UT (again A ∈Rm×n with m ≥n), where
+ = diag(σ −1
0 , . . . , σ −1
r−1, 0, . . . , 0) ∈Rn×m.
(4.A.9)
We observe that
A+f = V +UT f =
r−1

i=0
uT
i f
σi
vi = ˆx.
(4.A.10)
We call A+ the pseudoinverse of A. We have established that if rank (A) = n,
then AT Aˆx = AT f , so ˆx = (AT A)−1AT f , which implies that in this case A+ =
(AT A)−1AT . If A ∈Rn×n and A−1 exists, then A+ = A−1.
If A−1 exists (i.e., m = n and rank (A) = n), then we recall that κ2(A) =
||A||2||A−1||2. If A ∈Rm×n and m ≥n, then we extend this deﬁnition to
κ2(A) = ||A||2||A+||2.
(4.A.11)
TLFeBOOK

REFERENCES
193
We have established that ||A||2 = σ0 so since A+ = V +UT we have ||A+||2 =
1/σr−1. Consequently
κ2(A) =
σ0
σr−1
(4.A.12)
which provides a somewhat better justiﬁcation of (4.119), because if rank (A) =
n then (4.A.12) is κ2(A) = σ0/σn−1 = σmax(A)/σmin(A) [which is (4.119)].
From (4.A.11), κ2(AT A) = ||AT A||2||(AT A)+||2. With A = UV T , and AT =
V T UT we have
AT A = V T V T ,
and
(AT A)+ = V (T )+V T .
Thus, ||AT A||2 = σ 2
0 , and ||(AT A)+||2 = σ −2
n−1 (rank (A) = n). Thus
κ2(AT A) =
σ 2
0
σ 2
n−1
= [κ2(A)]2.
The condition number deﬁnition κp(A) in (4.60) was fully justiﬁed because of
(4.58). An analogous justiﬁcation exists for (4.A.11), but is much more difﬁcult to
derive, and this is why we do not consider it in this book.
REFERENCES
1. J. R. Rice, The Approximation of Functions, Vol. I: Linear Theory, Addison-Wesley,
Reading, MA, 1964.
2. M.-D. Choi, “Tricks or Treats with the Hilbert Matrix,” Am. Math. Monthly 90(5),
301–312 (May 1983).
3. D. R. Hill, Experiments in Computational Matrix Algebra (C. B. Moler, consulting ed.),
Random House, New York, 1988.
4. G. E. Forsythe and C. B. Moler, Computer Solution of Linear Algebraic Systems,
Prentice-Hall, Englewood Cliffs, NJ, 1967.
5. G. H. Golub and C. F. Van Loan, Matrix Computations, 2nd ed., Johns Hopkins Univ.
Press, Baltimore, MD, 1989.
6. R. A. Horn and C. R. Johnson, Matrix Analysis, Cambridge Univ. Press, Cambridge,
MA, 1985.
7. N. J. Higham, Accuracy and Stability of Numerical Algorithms, SIAM, Philadelphia, PA,
1996.
8. A. Quarteroni, R. Sacco, and F. Saleri, Numerical Mathematics (Texts in Applied Math-
ematics series, Vol. 37), Springer-Verlag, New York, 2000.
9. E. Isaacson and H. B. Keller, Analysis of Numerical Methods, Wiley, New York, 1966.
10. O. Axelsson, Iterative Solution Methods, Cambridge Univ. Press, New York, 1994.
11. E. Kreyszig, Introductory Functional Analysis with Applications, Wiley, New York, 1978.
TLFeBOOK

194
LINEAR SYSTEMS OF EQUATIONS
PROBLEMS
4.1. Function f (x) ∈L2[0, 1] is to be approximated according to
f (x) ≈a0x +
a1
x + c
using least squares, where c ∈R is some ﬁxed parameter. This involves
solving the linear system


1
3
1 −c loge

1 + 1
c

1 −c loge

1 + 1
c

1
c(c+1)


#
$%
&
=R
 ˆa0
ˆa1

# $% &
=ˆa
=
 " 1
0 xf (x) dx
" 1
0
f (x)
x+c dx
	
#
$%
&
=g
,
where ˆa is the vector from R2 that minimizes the energy V (a) [Eq. (4.8)].
(a) Suppose that f (x) = x +
1
x+1. Find ˆa for c = 1. For this special case
it is possible to know the answer in advance without solving the linear
system above. However, this problem requires you to solve the system.
[Hint: It helps to recall that
 a
b
c
d
−1
=
1
ad −bc

d
−b
−c
a

.
(b) Derive R [which is a special case of (4.9)].
4.2. Suppose that
A =

1
2
−1
−5

.
Find ||A||∞, ||A||1, and ||A||2.
4.3. Suppose that
A =
 1
0
2
ϵ

∈R2×2, ϵ ≥0,
and that A−1 exists. Find κ∞(A) if ϵ is small.
4.4. Suppose that
A =
 1
1
0
ϵ

∈R2×2,
and assume ϵ > 0 (so that A−1 always exists). Find κ2(A) = ||A||2||A−1||2.
What happens to condition number κ2(A) if ϵ →0?
4.5. Let A(ϵ), B(ϵ) ∈Rn×n. For example, A(ϵ) = [aij(ϵ)] so element aij(ϵ) of
A(ϵ) depends on the parameter ϵ ∈R.
TLFeBOOK

PROBLEMS
195
(a) Prove that
d
dϵ [A(ϵ)B(ϵ)] = A(ϵ)dB(ϵ)
dϵ
+ dA(ϵ)
dϵ
B(ϵ),
where dA(ϵ)/dt = [daij(ϵ)/dϵ], and dB(ϵ)/dt = [dbij(ϵ)/dϵ].
(b) Prove that
d
dϵ A−1(ϵ) = −A−1(ϵ)
dA(ϵ)
dϵ

A−1(ϵ).
[Hint: Consider
d
dϵ [A(ϵ)A−1(ϵ)] = d
dϵ I = 0, and use (a).]
4.6. This problem is an alternative derivation of κ(A). Suppose that ϵ ∈R, A, F ∈
Rn×n, and x(ϵ), y, f ∈Rn. Consider the perturbed linear system of equations
(A + ϵF)x(ϵ) = y + ϵf,
(4.P.1)
where Ax = y, so x(0) = x is the correct solution. Clearly, ϵF models the
errors in A, while ϵf models the errors in y. From (4.P.1), we obtain
x(ϵ) = [A + ϵF]−1(y + ϵf ).
(4.P.2)
The Taylor series expansion for x(ϵ) about ϵ = 0 is
x(ϵ) = x(0) + dx(0)
dϵ
ϵ + O(ϵ2),
(4.P.3)
where O(ϵ2) denotes terms in the expansion containing ϵk for k ≥2.
Use (4.P.3), results from the previous problem, and basic matrix–vector norm
properties (Section 4.4) to derive the bound in
||x(ϵ) −x||
||x||
≤ϵ ||A||||A−1||
#
$%
&
=κ(A)
||f ||
||y|| + ||F||
||A||

[Comment: The relative error in A is ρA = ϵ||F||/||A||, and the relative error
in y is ρy = ϵ||f ||/||y||, so more concisely ||x(ϵ) −x||/||x|| ≤κ(A)(ρA +
ρy).]
4.7. This problem follows the example relating to Eq. (4.95). An analog signal
f (t) is modeled according to
f (t) =
p−1

j=0
ajtj + η(t),
where aj ∈R for all j ∈Zp, and η(t) is some random noise term. We only
possess samples of the signal; that is, we only have the ﬁnite-length sequence
TLFeBOOK

196
LINEAR SYSTEMS OF EQUATIONS
(fn) deﬁned by fn = f (nTs) for n ∈ZN, where Ts > 0 is the sampling
period of the data acquisition system that gave us the samples. Our estimate
of fn is therefore
ˆfn =
p−1

j=0
ajT j
s nj.
With a = [a0 a1 · · · ap−2 ap−1]T ∈Rp, ﬁnd
V (a) =
N−1

n=0
[fn −ˆfn]2
in the form of Eq. (4.102). This implies that you must specify ρ, g, A, and P .
4.8. Sampled data fn (n = 0, 1, . . . , N −1) is modeled according to
ˆfn = a + bn + C sin(θn + φ).
Recall sin(A + B) = sin A cos B + cos A sin B. Also recall that
V (θ) =
N−1

n=0
[fn −ˆfn]2 = f T f −f T A(θ)[AT (θ)A(θ)]−1AT (θ)f,
where f = [f0f1 · · · fN−1]T ∈RN. Give a detailed expression for A(θ).
4.9. Prove that the product of two lower triangular matrices is a lower triangular
matrix.
4.10. Prove that the product of two upper triangular matrices is an upper triangular
matrix.
4.11. Find the LU decomposition of the matrix
A =


2
−1
0
−1
2
−1
0
−1
2


using Gauss transformations as recommended in Section 4.5. Use A = LU to
rewrite the factorization of A as A = LDLT , where L is unit lower triangular,
and D is a diagonal matrix. Is A > 0? Why?
4.12. Use Gaussian elimination to LU factorize the matrix
A =


4
−1
1
2
−1
4
−1
1
2
−1
4

.
Is A > 0? Why?
TLFeBOOK

PROBLEMS
197
4.13. (a) Consider A ∈Rn×n, and A = AT . Suppose that the leading principal
submatrices of A all have positive determinants. Prove that A > 0.
(b) Is


5
−3
0
−3
5
1
0
1
5

> 0?
Why?
4.14. The vector space Rn is an inner product space with inner product ⟨x, y⟩=
xT y for all x, y ∈Rn. Suppose that A ∈Rn×n, A = AT , and also that A > 0.
Prove that ⟨x, y⟩= xT Ay is also an inner product on the vector space Rn.
4.15. In the quadratic form
V (x) = f T f −2xT AT f + xT AT Ax,
we assume x ∈Rn, and AT A > 0. Prove that
V (ˆx) = f T f −f T A[AT A]−1AT f,
where ˆx is the vector that minimizes V (x).
4.16. Suppose that A ∈RN×M with M ≤N, and rank (A) = M. Suppose also that
⟨x, y⟩= xT y. Consider P = A[AT A]−1AT , P⊥= I −P (I is the N × N
identity matrix). Prove that for all x ∈RN we have ⟨P x, P⊥x⟩= 0. (Com-
ment: Matrices P and P⊥are examples of orthogonal projection operators.)
4.17. (a) Write a MATLAB function for forward substitution (solving Lz = y).
Write a MATLAB function for backward substitution (solving Ux = z).
Test your functions out on the following matrices and vectors:
L =


1
0
0
0
−1
1
0
0
0
2
3
1
0
0
0
−3
7
1

,
U =


1
2
3
4
0
3
5
5
0
0
−7
3
−1
3
0
0
0
6
7

,
y =
 1
1
−1
−1 T , z =

1
2
−7
3
−2
T .
(b) Write a MATLAB function to implement the LU decomposition algo-
rithm based on Gauss transformations considered in Section 4.5. Test
your function out on the following A matrices:
A =


1
2
3
4
−1
1
2
1
0
2
1
3
0
0
1
1

,
A =


2
−1
0
−1
2
−1
0
−1
2

.
TLFeBOOK

198
LINEAR SYSTEMS OF EQUATIONS
100 V
200 V
1 A
V6
V5
V4
V2
V3
V1
1000 Ω
10 Ω
100 Ω
100 Ω
100 Ω
50 Ω
8 Ω
20 Ω
+
−
+
−
Figure 4.P.1
The DC electric circuit of Problem 4.18.
4.18. Consider the DC electric circuit in Fig. 4.P.1.
Write the node equations for the node voltages V1, V2, . . . , V6 as shown.
These may be loaded into a vector
v = [V1V2V3V4V5V6]T
such that the node equations have the form
Gv = y.
(4.P.4)
Use the Gaussian elimination, forward substitution, and backward substitu-
tion MATLAB functions from the previous problem to solve the linear system
(4.P.4).
4.19. Let In ∈Rn×n be the order n identity matrix, and deﬁne
Tn =


a
b
0
· · ·
0
0
b
a
b
· · ·
0
0
0
b
a
· · ·
0
0
...
...
...
...
...
0
0
0
· · ·
a
b
0
0
0
· · ·
b
a


∈Rn×n.
TLFeBOOK

PROBLEMS
199
Matrix Tn is tridiagonal. It is also an example of a symmetric matrix that is
Toeplitz (deﬁned in a later problem). The characteristic polynomial of Tn is
pn(λ) = det(λIn −Tn). Show that
pn(λ) = (λ −a)pn−1(λ) −b2pn−2(λ)
(4.P.5)
for n = 3, 4, 5, . . .. Find p1(λ), and p2(λ) [initial conditions for the polyno-
mial recursion in (4.P.5)].
4.20. Consider the following If: T ∈Cn×n is Toeplitz if it has the form T =
[ti−j]i,j=0,1,...,n−1. Thus, for example, if n = 3 we have
T =


t0
t−1
t−2
t1
t0
t−1
t2
t1
t0

.
Observe that in a Toeplitz matrix all of the elements on any given
diagonal are equal to each other. A symmetric Toeplitz matrix has the
form T = [t|i−j|] since T = T T implies that t−i = ti for all i. Let xn =
[xn,0 xn,1 · · · xn,n−2 xn,n−1]T ∈Cn. Let Jn ∈Cn×n be the n × n exchange
matrix (also called the contra-identity matrix) which is deﬁned as the
matrix yielding ˆxn = Jnxn = [xn,n−1xn,n−2 · · · xn,1xn,0]T . We see that Jn
simply reverses the order of the elements of xn. An immediate con-
sequence of this is that J 2
n xn = xn (i.e., J 2
n = In). What is J3? (Write
this matrix out completely.) Suppose that Tn is a symmetric Toeplitz
matrix.
(a) Show that (noting that ˆτn = Jnτn)
Tn+1 =
 Tn
τn
τ T
n
t0

=
 t0
ˆτ T
n
ˆτn
Tn

(nesting property). What is τn?
(b) Show that
JnTnJn = Tn
(persymmetry property).
(Comment: Toeplitz matrices have an important role to play in digital signal
processing. For example, they appear in problems in spectral analysis, and
in voice compression algorithms.)
4.21. This problem is about a computationally efﬁcient method to solve the linear
system
Rnan = σ 2
ne0,
(4.P.6)
TLFeBOOK

200
LINEAR SYSTEMS OF EQUATIONS
where Rn ∈Rn×n is symmetric and Toeplitz (recall Problem 4.20). All
of the leading principle submatrices (recall the deﬁnition in Theo-
rem 4.1) of Rn are nonsingular. Also, e0 = [1 0 0 · · · 0 0]T ∈Rn, an =
[1 an,1
· · · an,n−2 an,n−1]T , and σ 2
n ∈R is unknown as well. Deﬁne
en = Jne0. Clearly, an,0 = 1 (all n).
(a) Prove that
Rn ˆan = σ 2
n ˆe0 = σ 2
nen,
(4.P.7)
where Jnan = ˆan.
(b) Observe that Rn[an
ˆan] = σ 2
n[e0
en]. Augmented matrix [an
ˆan] is
n × 2, and so [e0
en] is n × 2 as well. Prove that
Rn+1

an
0
0
ˆan
	
=

σ 2
ne0
ηn
ηn
σ 2
nen
	
.
(4.P.8)
What is ηn? (That is, ﬁnd a simple expression for it.)
(c) We wish to obtain
Rn+1[an+1
ˆan+1] = σ 2
n+1[e0
en+1]
(4.P.9)
from a manipulation of (4.P.8). To this end, ﬁnd a formula for parameter
Kn ∈R in
Rn

an
0
0
ˆan
	 
1
Kn
Kn
1
	
=

σ 2
ne0
ηn
ηn
σ 2
nen
	 
1
Kn
Kn
1

such that (4.P.9) is obtained. This implies that we obtain the vector
recursions
an+1 =
 an
0

+ Kn
 0
ˆan

and
ˆan+1 = Kn
 an
0

+
 0
ˆan

.
Find the initial condition a1 ∈R1×1. What is σ 2
1 ?
(d) Prove that
σ 2
n+1 = σ 2
n(1 −K2
n).
(e) Summarize the algorithm obtained in the previous steps in the form of
pseudocode. The resulting algorithm is often called the Levinson–Durbin
algorithm.
TLFeBOOK

PROBLEMS
201
(f) Count the number of arithmetic operations needed to implement the
Levinson–Durbin algorithm in (e). Compare this number to the num-
ber of arithmetic operations needed by the general LU decomposition
algorithm presented in Section 4.5.
(g) Write a MATLAB function to implement the Levinson-Durbin algorithm.
Test your algorithm out on the matrix
R =


2
1
0
1
2
1
0
1
2

.
(Hint: You will need the properties in Problem 4.20. The parameters Kn
are called reﬂection coefﬁcients, and are connected to certain problems in
electrical transmission line theory.)
4.22. This problem is about proving that solving (4.P.6) yields the LDLT decom-
position of R−1
n . (Of course, Rn is real-valued, symmetric, and Toeplitz.)
Observe that (via the nesting property for Toeplitz matrices)
Rn


1
0
· · ·
0
0
an,1
1
· · ·
0
0
an,2
an−1,1
· · ·
0
0
...
...
...
...
an,n−2
an−1,n−3
· · ·
1
0
an,n−1
an−1,n−2
· · ·
a2,1
1


#
$%
&
=Ln
=


σ 2
n
×
· · ·
×
×
0
σ 2
n−1
· · ·
×
×
0
0
· · ·
×
×
...
...
...
...
0
0
· · ·
σ 2
2
×
0
0
· · ·
0
σ 2
1


#
$%
&
= ˜Un
,
(4.P.10)
where × denotes “don’t care” entries; thus, the particular value of such an
entry is of no interest to us. Use (4.P.10) to prove that
LT
n RnLn = ˜Dn,
(4.P.11)
where ˜Dn = diag (σ 2
n, σ 2
n−1, . . . , σ 2
2 , σ 2
1 ) (diagonal matrix; see the comments
following Theorem 4.4). Use (4.P.11) to prove that
R−1
n
= LnDnLT
n .
(4.P.12)
What is Dn? [Hint: Using (4.P.10), and Rn = RT
n we note that LT
n RnLn can
be expressed in two distinct but equivalent ways. This observation is used
to establish (4.P.11).]
4.23. A matrix Tn is said to be strongly nonsingular (strongly regular) if all of its
leading principle submatrices are nonsingular (recall the deﬁnition in Theo-
rem 4.1). Suppose that xn = [xn,0 xn,1 · · · xn,n−2 xn,n−1]T ∈Cn, and deﬁne
TLFeBOOK

202
LINEAR SYSTEMS OF EQUATIONS
Zn ∈Cn×n according to
Znxn = [0 xn,0 xn,1 · · · xn,n−2]T ∈Cn.
Thus, Zn shifts the elements of any column vector down by one position.
The top position is ﬁlled in with a zero, while the last element xn,n−1 is lost.
Assume that Tn is real-valued, symmetric and Toeplitz. Consider
Tn −ZnTnZT
n = Xn.
(a) Find Xn.
(b) If Tn is strongly nonsingular then what is rank (Xn)? (Be careful. There
may be separate cases to consider.)
(c) Use δk (Kr¨onecker delta) to specify Zn; that is, Zn = [zi,j], so what is
zi,j in terms of the Kr¨onecker delta? (Hint: For example, the identity
matrix can be described as I = [δi−j].)
4.24. Suppose that R ∈RN×N is strongly nonsingular (see Problem 4.23 for the
deﬁnition of this term), and that R = RT . Thus, there exists the factorization
R = LNDNLT
N,
(4.P.13)
where LN is a unit lower triangular matrix, and DN is a diagonal matrix.
Let
LN = [l0 l1 · · · lN−2 lN−1],
so li is column i of LN. Let
DN = diag(d0, d1, . . . , dN−1).
Thus, via (4.P.13), we have
R =
N−1

k=0
dklklT
k .
(4.P.14)
Consider the following algorithm:
R0 := R;
for n := 0 to N −1 do begin
dn := eTnRnen;
ln := d−1
n Rnen;
Rn+1 := Rn −dnlnlTn ;
end;
TLFeBOOK

PROBLEMS
203
As usual, we have the unit vector
ei = [0 0 · · · 0
#
$%
&
i zeros
1 0 · · · 0]T ∈RN.
The algorithm above is the Jacobi procedure (algorithm) for computing the
Cholesky factorization (recall Theorem 4.6) of R.
Is R > 0 a necessary condition for the algorithm to work? Explain. Test the
Jacobi procedure out on
R =


2
1
0
1
2
1
0
1
2

.
Is R > 0? Justify your answer. How many arithmetic operations are needed to
implement the Jacobi procedure? How does this compare with the Gaussian
elimination method for general LU factorization considered in Section 4.5?
4.25. Suppose that A−1 exists, and that I + V T A−1U is also nonsingular. Of
course, I is the identity matrix.
(a) Prove the Sherman–Morrison–Woodbury formula
[A + UV T ]−1 = A−1 −A−1U[I + V T A−1U]−1V T A−1.
(b) Prove that if U = u ∈Cn and V = v ∈Cn, then
[A + uvT ]−1 = A−1 −A−1uvT A−1
1 + vT A−1u.
(Comment: These identities can be used to develop adaptive ﬁltering algo-
rithms, for example.)
4.26. Suppose that
A =
 a
b
0
c

.
Find conditions on a, b, and c to ensure that A > 0.
4.27. Suppose that A ∈Rn×n, and that A is not necessarily symmetric. We still
say that A > 0 iff xT Ax > 0 for all x ̸= 0 (x ∈Rn). Show that A > 0 iff
B = 1
2(A + AT ) > 0. Matrix B is often called the symmetric part of A. (Note:
In this book, unless stated to the contrary, a pd matrix is always assumed to
be symmetric.)
TLFeBOOK

204
LINEAR SYSTEMS OF EQUATIONS
4.28. Prove that for A ∈Rn×n
||A||∞=
max
0≤i≤n−1
n−1

j=0
|ai,j|.
4.29. Derive Eq. (4.128) (the law of cosines).
4.30. Consider An ∈Rn×n such that
An =


1
−1
−1
· · ·
−1
−1
0
1
−1
· · ·
−1
−1
0
0
1
· · ·
−1
−1
...
...
...
...
...
0
0
0
· · ·
1
−1
0
0
0
· · ·
0
1


,
A−1
n
=


1
1
2
· · ·
2n−3
2n−2
0
1
1
· · ·
2n−4
2n−3
0
0
1
· · ·
2n−5
2n−4
...
...
...
...
...
0
0
0
· · ·
1
1
0
0
0
· · ·
0
1


.
Show that κ∞(An) = n2n−1. Clearly, det(An) = 1. Consider
Dn = diag(10−1, 10−1, . . . , 10−1) ∈Rn.
What is κp(Dn)? Clearly, det(Dn) = 10−n. What do these two cases say
about the relationship between det(A), and κ(A) (A ∈Rn×n, and is nonsin-
gular) in general?
4.31. Recall Problem 1.7 (in Chapter 1). Suppose a = [a0a1 · · · an]T , b = [b0b1
· · · bm]T , and c = [c0c1 · · · cn+m]T , where
cl =
n

k=0
akbl−k.
Find matrix A such that c = Ab, and ﬁnd matrix B such that c = Ba. What
are the sizes of matrices A, and B? (Hint: A and B will be rectangular
Toeplitz matrices. This problem demonstrates the close association between
Toeplitz matrices and the convolution operation, and so partially explains the
central importance of Toeplitz matrices in digital signal processing.)
TLFeBOOK

PROBLEMS
205
4.32. Matrix P ∈Rn×n is a permutation matrix if it possesses exactly one one per
row and column, and zeros everywhere else. Such a matrix simply reorders
the elements in a vector. For example


0
1
0
0
0
0
1
0
1
0
0
0
0
0
0
1




x0
x1
x2
x3

=


x1
x2
x0
x3

.
Show that P −1 = P T (i.e., P is an orthogonal matrix).
4.33. Find c and s in
 c
−s
s
c
  x0
x1

=
 7
x2
0 + x2
1
0
	
,
where s2 + c2 = 1.
4.34. Consider
A =


2
1
0
0
1
2
1
0
0
1
2
1
0
0
1
2

.
Use Householder transformation matrices to ﬁnd the QR factorization of
matrix A.
4.35. Consider
A =


5
2
1
0
2
5
2
1
1
2
5
2
0
1
2
5

.
Using Householder transformation matrices, ﬁnd orthogonal matrices Q0,
and Q1 such that
Q1Q0A =


h00
h01
h02
h03
h10
h11
h12
h13
0
h21
h22
h23
0
0
h32
h33

= H.
[Comment: Matrix H is upper Hessenberg. This problem is an illustration of
a process that is important in ﬁnding matrix eigenvalues (Chapter 11).]
4.36. Write a MATLAB function to implement the Householder QR factorization
algorithm as given by the pseudocode in Section 4.6 [between Eqs. (4.146),
and (4.147)]. The function must output the separate factors ˜H T
k that make up
Q, in addition to the factors Q and R. Test your function out on the matrix
A in Problem 4.34.
TLFeBOOK

206
LINEAR SYSTEMS OF EQUATIONS
4.37. Prove Eq. (4.117), and establish that
rank (A) = rank (AT ).
4.38. If A ∈Cn×n, then the trace of A is given by
Tr(A) =
n−1

k=0
ak,k
(i.e., is the sum of the main diagonal elements of matrix A). Prove that
||A||2
F = Tr(AAH )
[recall (4.36a)].
4.39. Suppose that A, B ∈Rn×n, and Q ∈Rn×n is orthogonal; then, if A =
QBQT , prove that
Tr(A) = Tr(B).
4.40. Recall Theorem 4.4. Prove that for A ∈Rn×n
||A||2
F =
p−1

k=0
σ 2
k .
(Hint: Use the results from Problems 4.38 and 4.39.)
4.41. Consider Theorem 4.9. Is α < 1 a necessary condition for convergence?
Explain.
4.42. Suppose that A ∈Rn×n is pd and prove that the JOR method (Section 4.7)
converges- if
0 < ω <
2
ρ(D−1A).
4.43. Repeat the analysis made in Example 4.8, but instead use the matrix
A =


4
2
1
0
1
4
1
0
0
1
4
1
0
0
2
4

.
This will involve writing and running a suitable MATLAB function. Find the
optimum choice for ω to an accuracy of ±0.02.
TLFeBOOK

5
Orthogonal Polynomials
5.1
INTRODUCTION
Orthogonal polynomials arise in highly diverse settings. They can be solutions to
special classes of differential equations that arise in mathematical physics problems.
They are vital in the design of analog and digital ﬁlters. They arise in numerical
integration methods, and they have a considerable role to play in solving least-
squares and uniform approximation problems.
Therefore, in this chapter we begin by considering some of the properties shared
by all orthogonal polynomials. We then consider the special cases of Chebyshev,
Hermite, and Legendre polynomials. Additionally, we consider the application
of orthogonal polynomials to least-squares and uniform approximation problems.
However, we emphasize the case of least-squares approximation, which was ﬁrst
considered in some depth in Chapter 4. The approach to least-squares problems
taken here alleviates some of the concerns about ill-conditioning that were noted
in Chapter 4.
5.2
GENERAL PROPERTIES OF ORTHOGONAL POLYNOMIALS
We are interested here in the inner product space L2(D), where D is the domain
of deﬁnition of the functions in the space. Typically, D = [a, b], D = R, or D =
[0, ∞). We shall, as in Chapter 4, assume that all members of L2(D) are real-valued
to simplify matters. So far, our inner product has been
⟨f, g⟩=

D
f (x)g(x) dx,
(5.1)
but now we consider the weighted inner product
⟨f, g⟩=

D
w(x)f (x)g(x) dx,
(5.2)
where w(x) ≥0 (x ∈D) is the weighting function. This includes (5.1) as a special
case for which w(x) = 1 for all x ∈D.
An Introduction to Numerical Analysis for Electrical and Computer Engineers, by C.J. Zarowski
ISBN 0-471-46737-5
c⃝2004 John Wiley & Sons, Inc.
207
TLFeBOOK

208
ORTHOGONAL POLYNOMIALS
Our goal is to consider polynomials
φn(x) =
n

k=0
φn,kxk, x ∈D
(5.3)
of degree n such that
⟨φn, φm⟩= δn−m
(5.4)
for all n, m ∈Z+. The inner product is that of (5.2). Our polynomials {φn(x)|n ∈
Z+}. are orthogonal polynomials on D with respect to weighting function w(x).
Changing D and/or w(x) will generate very different polynomials, and we will
consider important special cases later. However, all orthogonal polynomials possess
certain features in common with each other regardless of the choice of D or w(x).
We shall consider a few of these in this section.
If p(x) is a polynomial of degree n, then we may write deg(p(x)) = n.
Theorem 5.1: Any three consecutive orthogonal polynomials are related by the
three-term recurrence formula (relation)
φn+1(x) = (Anx + Bn)φn(x) + Cnφn−1(x),
(5.5)
where
An = φn+1,n+1
φn,n
,
Bn = φn+1,n+1
φn,n
 φn+1,n
φn+1,n+1
−φn,n−1
φn,n
!
,
Cn = −φn+1,n+1φn−1,n−1
φ2n,n
.
(5.6)
Proof
Our proof is a somewhat expanded version of that in Isaacson and Keller
[1, pp. 204–205].
Observe that for An = φn+1,n+1/φn,n, we have
qn(x) = φn+1(x) −Anxφn(x) =
n+1

k=0
φn+1,kxk −An
n

k=0
φn,kxk+1
=
n+1

k=0
φn+1,kxk −An
n+1

j=1
φn,j−1xj
=
n

k=0
φn+1,kxk −An
n

j=1
φn,j−1xj + [φn+1,n+1 −Anφn,n]xn+1
=
n

k=0
φn+1,kxk −An
n

j=1
φn,j−1xj,
TLFeBOOK

GENERAL PROPERTIES OF ORTHOGONAL POLYNOMIALS
209
which is a polynomial of degree at most n, i.e., deg(qn(x)) ≤n. Thus, for suit-
able αk
qn(x) =
n

k=0
αkφk(x),
so because ⟨φk, φj⟩= δk−j, we have αj = ⟨qn, φj⟩. In addition
φn+1(x) −Anxφn(x) = αnφn(x) + αn−1φn−1(x) +
n−2

k=0
αkφk(x).
(5.7)
Now deg(xφj(x)) ≤n −1 for j ≤n −2. Thus, there are βk such that
xφj(x) =
n−1

k=0
βkφk(x)
so ⟨φr, xφj⟩= 0 if r > n −1, or
⟨φr, xφj⟩= 0
(5.8)
for j = 0, 1, . . . , n −2. From (5.7) via (5.8), we obtain
⟨φn+1 −Anxφn, φk⟩= ⟨φn+1, φk⟩−An⟨φn, xφk⟩= 0
for k = 0, 1, . . . , n −2. This is the inner product of φk(x) with the left-hand side
of (5.7). For the right-hand side of (5.7)
0 =

αnφn + αn−1φn−1 +
n−2

j=0
αjφj, φk

= αn⟨φn, φk⟩+ αn−1⟨φn−1, φk⟩+
n−2

j=0
αj⟨φj, φk⟩
=
n−2

j=0
αj⟨φj, φk⟩
again for k = 0, . . . , n −2. We can only have n−2
j=0 αj⟨φj, φk⟩= 0 if αj = 0 for
j = 0, 1, . . . , n −2. Thus, (5.7) reduces to
φn+1(x) −Anxφn(x) = αnφn(x) + αn−1φn−1(x)
or
φn+1(x) = (Anx + αn)φn(x) + αn−1φn−1(x),
(5.9)
TLFeBOOK

210
ORTHOGONAL POLYNOMIALS
which has the form of (5.5). We now need to verify that αn = Bn, αn−1 = Cn as
in (5.6). From (5.9)
φn(x) = An−1xφn−1(x) + αn−1φn−1(x) + αn−2φn−2(x)
or
xφn−1(x) =
1
An−1
φn(x) +

−
1
An−1
(αn−1φn−1(x) + αn−2φn−2(x))

#
$%
&
=pn−1(x)
(5.10)
for which deg(pn−1(x)) ≤n −1. Thus, from (5.9)
⟨φn+1, φn−1⟩= An⟨xφn, φn−1⟩+ αn⟨φn, φn−1⟩+ αn−1⟨φn−1, φn−1⟩
so that
αn−1 = −An⟨xφn, φn−1⟩= −An⟨φn, xφn−1⟩,
and via (5.10)
αn−1 = −An
<
φn,
1
An−1
φn + pn−1
=
,
which becomes
αn−1 = −An
An−1
⟨φn, φn⟩= −An
An−1
or
Cn = αn−1 = −φn+1,n+1
φn,n
φn−1,n−1
φn,n
,
which is the expression for Cn in (5.6). Expanding (5.5)
n+1

k=0
φn+1,kxk = An
n+1

k=1
φn,k−1xk + Bn
n

k=0
φn,kxk + Cn
n−1

k=0
φn−1,kxk,
where, on comparing the terms for k = n, we see that
φn+1,n = Anφn,n−1 + Bnφn,n
(since φn−1,n = 0). Therefore
Bn =
1
φn,n
[φn+1,n −Anφn,n−1] =
1
φn,n

φn+1,n −φn+1,n+1
φn,n
φn,n−1

= φn+1,n+1
φn,n
 φn+1,n
φn+1,n+1
−φn,n−1
φn,n

,
which is the expression for Bn in (5.6).
TLFeBOOK

GENERAL PROPERTIES OF ORTHOGONAL POLYNOMIALS
211
Theorem 5.2: Orthogonal polynomials satisfy the Christoffel–Darboux formula
(relation)
(x −y)
n

j=0
φj(x)φj(y) =
φn,n
φn+1,n+1
[φn+1(x)φn(y) −φn+1(y)φn(x)].
(5.11)
Proof
The proof is an expanded version of that from Isaacson and Keller [1,
p. 205].
From (5.5), we obtain
φn(y)φn+1(x) = (Anx + Bn)φn(x)φn(y) + Cnφn−1(x)φn(y),
(5.12)
so reversing the roles of x and y gives
φn(x)φn+1(y) = (Any + Bn)φn(y)φn(x) + Cnφn−1(y)φn(x).
(5.13)
Subtracting (5.13) from (5.12) yields
φn(y)φn+1(x) −φn(x)φn+1(y) = An(x −y)φn(x)φn(y)
+ Cn(φn−1(x)φn(y) −φn(x)φn−1(y)).
(5.14)
We note that
Cn
An
= −φn−1,n−1
φn,n
= −
1
An−1
,
(5.15)
so (5.14) may be rewritten as
(x −y)φn(x)φn(y) = A−1
n (φn+1(x)φn(y) −φn+1(y)φn(x))
−A−1
n−1(φn(x)φn−1(y) −φn−1(x)φn(y)).
Now consider
(x −y)
n

j=0
φj(x)φj(y) =
n

j=0
9
A−1
j [φj+1(x)φj(y) −φj+1(y)φj(x)]
−A−1
j−1[φj(x)φj−1(y) −φj−1(x)φj(y)]
:
.
(5.16)
Since Aj = φj+1,j+1/φj,j, we have A−1
−1 = φ−1,−1/φ0,0 = 0 because φ−1,−1 = 0.
Taking this into account, the summation on the right-hand side of (5.16) reduces
due to the cancellation of terms,1 and so (5.16) ﬁnally becomes
(x −y)
n

j=0
φj(x)φj(y) =
φn,n
φn+1,n+1
[φn+1(x)φn(y) −φn+1(y)φn(x)],
which is (5.11).
1The cancellation process seen here is similar to the one that occurred in the derivation of the Dirichlet
kernel identity (3.24). This mathematical technique seems to be a recurring theme in analysis.
TLFeBOOK

212
ORTHOGONAL POLYNOMIALS
The following corollary to Theorem 5.2 is from Hildebrand [2, p. 342]. However,
there appears to be no proof in Ref. 2, so one is provided here.
Corollary 5.1
With φ(1)
k (x) = dφk(x)/dx we have
n

j=0
φ2
j (x) =
φn,n
φn+1,n+1
[φ(1)
n+1(x)φn(x) −φ(1)
n (x)φn+1(x)].
(5.17)
Proof
Since
φn+1(x)φn(y) −φn+1(y)φn(x) = [φn+1(x) −φn+1(y)]φn(y)
−[φn(x) −φn(y)]φn+1(y)
via (5.11)
n

j=0
φj(x)φj(y)
=
φn,n
φn+1,n+1
φn+1(x) −φn+1(y)
x −y
φn(y) −φn(x) −φn(y)
x −y
φn+1(y)

. (5.18)
Letting y →x in (5.18) immediately yields (5.17). This is so from the deﬁnition of
the derivative, and the fact that all polynomials may be differentiated any number
of times.
The three-term recurrence relation is a vital practical method for orthogonal
polynomial generation. However, an alternative approach is the following.
If deg(qr−1(x)) = r −1 (qr−1(x) is an arbitrary degree r −1 polynomial), then
⟨φr, qr−1⟩=

D
w(x)φr(x)qr−1(x) dx = 0.
(5.19)
Deﬁne
w(x)φr(x) = drGr(x)
dxr
= G(r)
r (x)
(5.20)
so that (5.19) becomes

D
G(r)
r (x)qr−1(x) dx = 0.
(5.21)
If we repeatedly integrate by parts (using q(r)
r−1(x) = 0 for all x), we obtain

D
G(r)
r (x)qr−1(x) dx = G(r−1)
r
(x)qr−1(x)|D −

D
G(r−1)
r
(x)q(1)
r−1(x) dx
TLFeBOOK

GENERAL PROPERTIES OF ORTHOGONAL POLYNOMIALS
213
= G(r−1)
r
(x)qr−1(x)|D −G(r−2)
r
(x)q(1)
r−1(x)|D
+

D
G(r−2)
r
(x)q(2)
r−1(x) dx
= G(r−1)
r
(x)qr−1(x)|D −G(r−2)
r
(x)q(1)
r−1(x)|D
+ G(r−3)
r
(x)q(2)
r−1(x)|D +

D
G(r−3)
r
(x)q(3)
r−1(x) dx
and so on. Finally
[G(r−1)
r
qr−1 −G(r−2)
r
q(1)
r−1 + G(r−3)
r
q(2)
r−1 −· · · + (−1)r−1Grq(r−1)
r−1 ]D = 0,
(5.22a)
or alternatively
 r

k=1
(−1)k−1G(r−k)
r
q(k−1)
r−1
	
D
= 0.
(5.22b)
Since from (5.20)
φr(x) =
1
w(x)
drGr(x)
dxr
,
(5.23)
which is a polynomial of degree r, it must be the case that Gr(x) satisﬁes the
differential equation
dr+1
dxr+1

1
w(x)
drGr(x)
dxr

= 0
(5.24)
for x ∈D. Recalling D = [a, b] and allowing a →−∞, b →∞(so we may have
D = [a, ∞), or D = R), Eq. (5.22a) must be satisﬁed for any values of q(k)
r−1(a) and
q(k)
r−1(b) (k = 0, 1, . . . , r −1). This implies that we have the boundary conditions
G(k)
r (a) = 0, G(k)
r (b) = 0
(5.25)
for k = 0, 1, . . . , r −1. These restrict the solution to (5.24). It can be shown [6]
that (5.24) has a nontrivial2 solution for all r ∈Z+ assuming that w(x) ≥0 for
all x ∈D, and that the moments
"
D xkw(x) dx exist for all k ∈Z+. Proving this
is difﬁcult, and so will not be considered. The expression in (5.23) may be called
a Rodrigues formula for φr(x) (although it should be said that this terminology
usually arises only in the context of Legendre polynomials).
The Christoffel–Darboux formulas arise in various settings. For example, they
are relevant to the problem of designing Savitzky–Golay smoothing digital ﬁlters
[3], and they arise in numerical integration methods [2, 4]. Another way in which
the Christoffel–Darboux formula of (5.11) can make an appearance is as follows.
2The trivial solution for a differential equation is the identically zero function.
TLFeBOOK

214
ORTHOGONAL POLYNOMIALS
We may wish to approximate f (x) ∈L2(D) as
f (x) ≈
n

j=0
ajφj(x),
(5.26)
so the residual is
rn(x) = f (x) −
n

j=0
ajφj(x).
(5.27)
Adopting the now familiar least-squares approach, we select aj to minimize func-
tional V (a) = ||rn||2 = ⟨rn, rn⟩
V (a) =

D
w(x)r2
n(x) dx =

D
w(x)

f (x) −
n

j=0
ajφj(x)


2
dx,
(5.28)
where a = [a0 a1 · · · an]T ∈Rn+1. Of course, this expands to become
V (a) =

D
w(x)f 2(x) dx −2
n

j=0
aj

D
w(x)f (x)φj(x) dx
+
n

j=0
n

k=0
ajak

D
w(x)φj(x)φk(x) dx,
so if g = [g0 g1 · · · gn]T with gj =
"
D w(x)f (x)φj(x) dx, and R = [ri,j] ∈
R(n+1)×(n+1) with ri,j =
"
D w(x)φi(x)φj(x) dx, and ρ =
"
D w(x)f 2(x) dx, then
V (a) = ρ −2aT g + aT Ra.
(5.29)
But ri,j = ⟨φi, φj⟩= δi−j [via (5.4)], so we have R = I (identity matrix). Imme-
diately, one of the advantages of working with orthogonal polynomials is that R
is perfectly conditioned (contrast this with the Hilbert matrix of Chapter 4). This
in itself is a powerful incentive to consider working with orthogonal polynomial
expansions. Another nice consequence is that the optimal solution ˆa satisﬁes
ˆa = g,
(5.30)
that is
ˆaj =

D
w(x)f (x)φj(x) dx = ⟨f, φj⟩,
(5.31)
where j ∈Zn+1. If a = ˆa in (5.27) then we have the optimal residual
ˆrn(x) = f (x) −
n

j=0
ˆajφj(x).
(5.32)
TLFeBOOK

GENERAL PROPERTIES OF ORTHOGONAL POLYNOMIALS
215
We may substitute (5.31) into (5.32) to obtain
ˆrn(x) = f (x) −
n

j=0

D
w(y)f (y)φj(y) dy

φj(x)
= f (x) −

D
f (y)w(y)


n

j=0
φj(x)φj(y)

dy.
(5.33)
For convenience we deﬁne the kernel function3
Kn(x, y) =
n

j=0
φj(x)φj(y),
(5.34)
so that (5.33) becomes
ˆrn(x) = f (x) −

D
w(y)f (y)Kn(x, y) dy.
(5.35)
Clearly, Kn(x, y) in (5.34) has the alternative formula given by (5.11). Now con-
sider

D
f (x)w(y)Kn(x, y) dy = f (x)

D
w(y)
n

j=0
φj(x)φj(y) dy
= f (x)
n

j=0
φj(x)

D
w(y)φj(y) dy
= f (x)
n

j=0
φj(x)⟨1, φj⟩
= f (x)
n

j=0
φj(x)
<φ0(x)
φ0,0
, φj(x)
=
= f (x)φ0(x)
φ0,0
= f (x)
because φ0(x) = φ0,0 for x ∈D, and ⟨φ0, φj⟩= δj. Thus, (5.35) becomes
ˆrn(x) =

D
f (x)w(y)Kn(x, y) dy −

D
f (y)w(y)Kn(x, y) dy
=

D
w(y)Kn(x, y)[f (x) −f (y)] dy.
(5.36)
3Recall the Dirichlet kernel of Chapter 3, which we now see is really just a special instance of the
general idea considered here.
TLFeBOOK

216
ORTHOGONAL POLYNOMIALS
The optimal residual (optimal error) ˆrn(x) presumably gets smaller in some sense
as n →∞. Clearly, insight into how it behaves in the limit as n →∞can be
provided by (5.36). The behavior certainly depends on f (x), w(x), and the kernel
Kn(x, y). Intuitively, the summation expression for Kn(x, y) in (5.34) is likely
to be less convenient to work with than the alternative expression we obtain
from (5.11).
Some basic results on polynomial approximations are as follows.
Theorem 5.3: Weierstrass’ Approximation Theorem
Let f (x) be continu-
ous on the closed interval [a, b]. For any ϵ > 0 there exists an integer N = N(ϵ),
and a polynomial pN(x) ∈PN[a, b] (deg(pN(x)) ≤N) such that
|f (x) −pN(x)| ≤ϵ
for all x ∈[a, b].
Various proofs of this result exist in the literature (e.g., see Rice [5, p. 121] and
Isaacson and Keller [1, pp. 183–186]), but we omit them here. We see that the
convergence of pN(x) to f (x) is uniform as N →∞(recall Deﬁnition 3.4 in
Chapter 3). Theorem 5.3 states that any function continuous on an interval may be
approximated with a polynomial to arbitrary accuracy. Of course, a large degree
polynomial may be needed to achieve a particular level of accuracy depending upon
f (x). We also remark that Weierstrass’ theorem is an existence theorem. It claims
the existence of a polynomial that uniformly approximates a continuous function
on [a, b], but it does not tell us how to ﬁnd the polynomial. Some information
about the convergence behavior of a least-squares approximation is provided by
the following theorem.
Theorem 5.4: Let D = [a, b], w(x) = 1 for all x ∈D. Let f (x) be continuous
on D, and let
qn(x) =
n

j=0
ˆajφj(x)
(least-squares polynomial approximation to f (x) on D), so ˆaj = ⟨f, φj⟩=
" b
a f (x)φj(x) dx. Then
lim
n→∞V (ˆa) = lim
n→∞
 b
a
[f (x) −qn(x)]2 dx = 0,
and we have Parseval’s equality
 b
a
f 2(x) dx =
∞

j=0
ˆa2
j.
(5.37)
TLFeBOOK

GENERAL PROPERTIES OF ORTHOGONAL POLYNOMIALS
217
Proof
We use proof by contradiction. Assume that limn→∞V (ˆa) = δ > 0.
Pick any ϵ > 0 such that ϵ2 =
1
2(b−a)δ. By Theorem 5.3 (Weierstrass’ theorem)
there is a polynomial pm(x) such that |f (x) −pm(x)| ≤ϵ for x ∈D. Thus
 b
a
[f (x) −pm(x)]2 dx ≤ϵ2[b −a] = 1
2δ.
Now via (5.29) and (5.30)
V (ˆa) = ρ −ˆaT ˆa,
that is
V (ˆa) =
 b
a
f 2(x) dx −
n

j=0
ˆa2
j,
and V (a) ≥0 for all a ∈Rn+1. So we have Bessel’s inequality
V (ˆa) =
 b
a
f 2(x) dx −
n

j=0
ˆa2
j ≥0,
(5.38)
and V (ˆa) must be a nonincreasing function of n. Hence the least-squares approxi-
mation of degree m, say, qm(x), satisﬁes
1
2δ ≥
 b
a
[f (x) −qm(x)]2 dx ≥δ
which is a contradiction unless δ = 0. Since δ = 0, we must have (5.37).
We observe that Parseval’s equality of (5.37) relates the energy of f (x) to
the energy of the coefﬁcients ˆaj. The convergence behavior described by Theo-
rem 5.4 is sometimes referred to as convergence in the mean [1, pp. 197–198].
This theorem does not say what happens if f (x) is not continuous on [a, b]. The
result (5.36) is independent regardless of whether f (x) is continuous. It is a poten-
tially more powerful result for this reason. It turns out that the convergence of
the least-squares polynomial sequence (qn(x)) to f (x) is pointwise in general but,
depending on φj(x) and f (x), the convergence can sometimes be uniform. For uni-
form convergence, f (x) must be sufﬁciently smooth. The pointwise convergence
of the orthogonal polynomial series when f (x) has a discontinuity implies that the
Gibbs phenomenon can be expected. (Recall this phenomenon in the context of the
Fourier series expansion as seen in Section 3.4.)
We have seen polynomial approximation to functions in Chapter 3. There we saw
that the Taylor formula is a polynomial approximation to a function with a number
of derivatives equal to the degree of the Taylor polynomial. This approximation
technique is obviously limited to functions that are sufﬁciently differentiable. But
our present polynomial approximation methodology has no such limitation.
TLFeBOOK

218
ORTHOGONAL POLYNOMIALS
We remark that Theorem 5.4 suggests (but does not prove) that if f (x) ∈
L2[a, b], then
f (x) =
∞

j=0
⟨f, φj⟩φj(x),
(5.39)
where
⟨f, φj⟩=
 b
a
f (x)φj(x) dx.
(5.40)
This has the basic form of the Fourier series expansion that was ﬁrst seen in
Chapter 1. For this reason (5.39) is sometimes called a generalized Fourier series
expansion, although in the most general form of this idea the orthogonal functions
are not necessarily always polynomials. The idea of a generalized Fourier series
expansion can be extended to domains such as D = [0, ∞), or D = R, and to any
weighting functions that lead to solutions to (5.24).
The next three sections consider the Chebyshev, Hermite, and Legendre poly-
nomials as examples of how to apply the core theory of this section.
5.3
CHEBYSHEV POLYNOMIALS
Suppose that D = [a, b] and recall (5.28),
V (a) =
 b
a
w(x)r2
n(x) dx,
(5.41)
which is the (weighted) energy of the approximation error (residual) in (5.27):
rn(x) = f (x) −
n

j=0
ajφj(x).
(5.42)
The weighting function w(x) is often selected to give more or less weight to errors
in different places on the interval [a, b]. This is intended to achieve a degree of
control over error behavior. If w(x) = c > 0 for x ∈[a, b], then equal importance
is given to errors across the interval. This choice (with c = 1) gives rise to the
Legendre polynomials, and will be considered later. If we wish to give more weight
to errors at the ends of the interval, then a popular instance of this is for D = [−1, 1]
with weighting function
w(x) =
1
√
1 −x2 .
(5.43)
This choice leads to the famous Chebyshev polynomials of the ﬁrst kind. The reader
will most likely see these applied to problems in analog and digital ﬁlter design in
subsequent courses. For now, we concentrate on their basic theory.
TLFeBOOK

CHEBYSHEV POLYNOMIALS
219
The following lemma and ideas expressed in it are pivotal in understanding the
Chebyshev polynomials and how they are constructed.
Lemma 5.1:
If n ∈Z+, then
cos nθ =
n

k=0
βn,k cosk θ
(5.44)
for suitable βn,k ∈R.
Proof
First of all recall the trigonometric identities
cos(m + 1)θ = cos mθ cos θ −sin mθ sin θ
cos(m −1)θ = cos mθ cos θ + sin mθ sin θ
(which follow from the more basic identity cos(a + b) = cos a cos b −sin a sin b).
From the sum of these identities
cos(m + 1)θ = 2 cos mθ cos θ −cos(m −1)θ.
(5.45)
For n = 1 in (5.44), we have β1,0 = 0, β1,1 = 1, and for n = 0, we have β0,0 =
1. These will form initial conditions in a recursion that we now derive using
mathematical induction.
Assume that (5.44) is valid both for n = m and for n = m −1, and so
cos mθ =
m

k=0
βm,k cosk θ
cos(m −1)θ =
m−1

k=0
βm−1,k cosk θ,
and so via (5.45)
cos(m + 1)θ = 2 cos θ
m

k=0
βm,k cosk θ −
m−1

k=0
βm−1,k cosk θ
= 2
m

k=0
βm,k cosk+1 θ −
m−1

k=0
βm−1,k cosk θ
= 2
m+1

r=1
βm,r−1 cosr θ −
m−1

r=0
βm−1,r cosr θ
TLFeBOOK

220
ORTHOGONAL POLYNOMIALS
=
m+1

k=0
[2βm,k−1 −βm−1,k] cosk θ(βm,k = 0 for k < 0, and k > m)
=
m+1

k=0
βm+1,k cosk θ,
which is to say that
βm+1,k = 2βm,k−1 −βm−1,k.
(5.46)
This is the desired three-term recursion for the coefﬁcients in (5.44). As a conse-
quence of this result, Eq. (5.44) is valid for n = m + 1 and thus is valid for all
n ≥0 by mathematical induction.
This lemma states that cos nθ may be expressed as a polynomial of degree n in
cos θ. Equation (5.46) along with the initial conditions
β0,0 = 1,
β1,0 = 0,
β1,1 = 1
(5.47)
tells us how to ﬁnd the polynomial coefﬁcients. For example, from (5.46)
β2,k = 2β1,k−1 −β0,k
for k = 0, 1, 2. [In general, we evaluate (5.46) for k = 0, 1, . . . , m + 1.] Therefore
β2,0 = 2β1,−1 −β0,0 = −1,
β2,1 = 2β1,0 −β0,1 = 0,
β2,2 = 2β1,1 −β0,2 = 2
which implies that cos 2θ = −1 + 2 cos2 θ. This is certainly true as it is a well-
known trigonometric identity.
Lemma 5.1 possesses a converse. Any polynomial in cos θ of degree n can be
expressed as a linear combination of members from the set {cos kθ|k = 0, 1, . . . , n}.
We now have enough information to derive the Chebyshev polynomials. Recall-
ing (5.19) we need a polynomial φr(x) of degree r such that
 1
−1
1
√
1 −x2 φr(x)qr−1(x) dx = 0,
(5.48)
where qr−1(x) is an arbitrary polynomial such that deg(qr−1(x)) ≤r −1. Let us
change variables according to
x = cos θ.
(5.49)
TLFeBOOK

CHEBYSHEV POLYNOMIALS
221
Then dx = −sin θ dθ, and x ∈[−1, 1] maps to θ ∈[0, π]. Thus
 1
−1
1
√
1 −x2 φr(x)qr−1(x) dx = −
 0
π
1
√
1 −cos2 θ
φr(cos θ)qr−1(cos θ) sin θ dθ
=
 π
0
φr(cos θ)qr−1(cos θ) dθ = 0.
(5.50)
Because of Lemma 5.1 (and the above mentioned converse to it)
 π
0
φr(cos θ) cos kθ dθ = 0
(5.51)
for k = 0, 1, . . . , r −1. Consider φr(cos θ) = Cr cos rθ, then
Cr
 π
0
cos rθ cos kθ dθ = 1
2Cr
 π
0
[cos(r + k)θ + cos(r −k)θ] dθ
= 1
2Cr

1
r + k sin(r + k)θ +
1
r −k sin(r −k)θ
π
0
= 0
for k = 0, 1, . . . , r −1. Thus, we may indeed choose
φr(x) = Cr cos[r cos−1 x].
(5.52)
Constant Cr is selected to normalize the polynomial according to user requirements.
Perhaps the most common choice is simply to set Cr = 1 for all r ∈Z+. In this
case we set Tr(x) = φr(x):
Tr(x) = cos[r cos−1 x].
(5.53)
These are the Chebyshev polynomials of the ﬁrst kind.
By construction, if r ̸= k, then
⟨Tr, Tk⟩=
 1
−1
1
√
1 −x2 Tr(x)Tk(x) dx = 0.
Consider r = k
||Tr||2 =
 1
−1
1
√
1 −x2 T 2
r (x) dx
=
 1
−1
1
√
1 −x2 cos2[r cos−1 x] dx,
(5.54)
and apply (5.49). Thus
||Tr||2 =
 π
0
cos2 rθ dθ =
 π,
r = 0
1
2π,
r > 0 .
(5.55)
TLFeBOOK

222
ORTHOGONAL POLYNOMIALS
We have claimed φr(x) in (5.52), and hence Tr(x) in (5.53), are polynomials. This is
not immediately obvious, given that the expressions are in terms of trigonometric
functions. We will now conﬁrm that Tr(x) is indeed a polynomial in x for all
r ∈Z+. Our approach follows the proof of Lemma 5.1.
First, we observe that
T0(x) = cos(0 · cos−1 x) = 1,
T1(x) = cos(1 · cos−1 x) = x.
(5.56)
Clearly, these are polynomials in x. Once again (see the proof of Lemma 5.1)
Tn+1(x) = cos(n + 1)θ(θ = cos−1 x)
= cos nθ cos θ −(cos(n −1)θ −cos nθ cos θ)
= 2 cos nθ cos θ −cos(n −1)θ
= 2xTn(x) −Tn−1(x),
that is
Tn+1(x) = 2xTn(x) −Tn−1(x)
(5.57)
for n ∈N. The initial conditions are expressed in (5.56). We see that this is a
special case of (5.5) (Theorem 5.1). It is immediately clear from (5.57) and (5.56)
that Tn(x) is a polynomial in x for all n ∈Z+.
Some plots of Chebyshev polynomials of the ﬁrst kind appear in Fig. 5.1. We
remark that on the interval x ∈[−1, 1] the “ripples” of the polynomials are of
the same height. This fact makes this class of polynomials quite useful in certain
uniform approximation problems. It is one of the main properties that makes this
class of polynomials useful in ﬁlter design. Some indication of how the Chebyshev
polynomials relate to uniform approximation problems appears in Section 5.7.
Example 5.1
This example is about how to program the recursion speciﬁed
in (5.57). Deﬁne
Tn(x) =
n

j=0
Tn,jxj
so that Tn,j is the coefﬁcient of xj. This is the same kind of notation as in (5.3)
for φn(x). In this setting it is quite important to note that Tn,j = 0 for j < 0, and
for j > n. From (5.57), we obtain
n+1

j=0
Tn+1,jxj = 2x
n

j=0
Tn,jxj −
n−1

j=0
Tn−1,jxj,
and so
n+1

j=0
Tn+1,jxj = 2
n

j=0
Tn,jxj+1 −
n−1

j=0
Tn−1,jxj.
TLFeBOOK

CHEBYSHEV POLYNOMIALS
223
−1.5
−1
−0.5
0
0.5
1
1.5
−5
−4
−3
−2
−1
0
1
2
3
4
5
x
Tn(x)
n = 2 
n = 3 
n = 4 
n = 5 
Figure 5.1
Chebyshev polynomials of the ﬁrst kind of degrees 2, 3, 4, 5.
Changing the variable in the second summation according to k = j + 1 (so j =
k −1) yields
n+1

k=0
Tn+1,kxk = 2
n+1

k=1
Tn,k−1xk −
n−1

k=0
Tn−1,kxk,
and modifying the limits on the summations on the right-hand side yields
n+1

k=0
Tn+1,kxk = 2
n+1

k=0
Tn,k−1xk −
n+1

k=0
Tn−1,kxk.
We emphasize that this is permitted because we recall that Tn,−1 = Tn−1,n =
Tn−1,n+1 = 0 for all n ≥1. Comparing like powers of x gives us the recursion
Tn+1,k = 2Tn,k−1 −Tn−1,k
for k = 0, 1, . . . , n + 1 with n = 1, 2, 3, . . . . Since T0(x) = 1, we have T0,0 = 1,
and since T1(x) = x, we have T1,0 = 0, T1,1 = 1, which are the initial conditions
for the recursion. Therefore, a pseudocode program to compute Tn(x) is
T0,0 := 1;
T1,0 := 0; T1,1 := 1;
for n := 1 to N −1 do begin
for k := 0 to n + 1 do begin
TLFeBOOK

224
ORTHOGONAL POLYNOMIALS
Tn+1,k := 2Tn,k−1 −Tn−1,k;
end;
end;
This computes T2(x), . . . , TN(x) (so we need N ≥2).
We remark that the recursion in Example 5.1 may be implemented using integer
arithmetic, and so there will be no rounding or quantization errors involved in
computing Tn(x). However, there is the risk of computing machine overﬂow.
Example 5.2
This example is about changing representations. Speciﬁcally,
how might we express
pn(x) =
n

j=0
pn,jxj ∈Pn[−1, 1]
in terms of the Chebyshev polynomials of the ﬁrst kind? In other words, we wish
to determine the series coefﬁcients in
pn(x) =
n

j=0
αjTj(x).
We will consider this problem only for n = 4.
Therefore, we begin by noting that (via (5.56) and (5.57)) T0(x) = 1, T1(x) = x,
T2(x) = 2x2 −1, T3(x) = 4x3 −3x, and T4(x) = 8x4 −8x2 + 1. Consequently
4

j=0
αjTj(x) = α0 + α1x + 2α2x2 −α2
+ 4α3x3 −3α3x + 8α4x4 −8α4x2 + α4
= (α0 −α2 + α4) + (α1 −3α3)x + (2α2 −8α4)x2 + 4α3x3 + 8α4x4
implying that (on comparing like powers of x)
p4,0 = α0 −α2 + α4, p4,1 = α1 −3α3,
p4,2 = 2α2 −8α4, p4,3 = 4α3, p4,4 = 8α4.
This may be more conveniently expressed in matrix form:


1
0
−1
0
1
0
1
0
−3
0
0
0
2
0
−8
0
0
0
4
0
0
0
0
0
8


#
$%
&
=U


α0
α1
α2
α3
α4


#
$%
&
=α
=


p4,0
p4,1
p4,2
p4,3
p4,4


#
$%
&
=p
.
TLFeBOOK

HERMITE POLYNOMIALS
225
The upper triangular system Uα = p is certainly easy to solve using the backward
substitution algorithm presented in Chapter 4 (see Section 4.5). We note that the
elements of matrix U are the coefﬁcients {Tn,j} as might be obtained from the
algorithm in Example 5.1.
Can you guess, on the basis of Example 5.2, what matrix U will be for any n ?
5.4
HERMITE POLYNOMIALS
Now let us consider D = R with weighting function
w(x) = e−α2x2.
(5.58)
This is essentially the Gaussian pulse from Chapter 3. Recalling (5.23), we have
φr(x) = eα2x2 drGr(x)
dxr
,
(5.59)
where Gr(x) satisﬁes the differential equation
dr+1
dxr+1

eα2x2 drGr(x)
dxr

= 0
(5.60)
[recall (5.24)]. From (5.25) Gr(x) and G(k)
r (x) (for k = 1, 2, . . . , r −1) must tend
to zero as x →±∞. We may consider the trial solution
Gr(x) = Cre−α2x2.
(5.61)
The kth derivative of this is of the form
Cre−α2x2 × (polynomial of degree k),
so (5.61) satisﬁes both (5.60) and the required boundary conditions. Therefore
φr(x) = Creα2x2 dr
dxr (e−α2x2).
(5.62)
It is common practice to deﬁne the Hermite polynomials to be φr(x) for Cr =
(−1)r with either α2 = 1 or α2 = 1
2. We shall select α2 = 1, and so our Hermite
polynomials are
Hr(x) = (−1)rex2 dr
dxr (e−x2).
(5.63)
By construction, for k ̸= r
 ∞
−∞
e−α2x2φr(x)φk(x) dx = 0.
(5.64)
For the case where k = r, the following result is helpful.
TLFeBOOK

226
ORTHOGONAL POLYNOMIALS
It must be the case that
φr(x) =
r

k=0
φr,kxk
so that
||φr||2 =

D
w(x)φ2
r (x) dx =

D
w(x)φr(x)
 r

k=0
φr,kxk
	
dx,
but
"
D w(x)φr(x)xi dx = 0 for i = 0, 1, . . . , r −1 [special case of (5.19)], and so
||φr||2 = φr,r

D
w(x)φr(x)xr dx = φr,r

D
xrG(r)
r (x) dx
(5.65)
[via (5.20)]. We may integrate (5.65) by parts r times, and apply (5.25) to obtain
||φr||2 = (−1)rr!φr,r

D
Gr(x) dx.
(5.66)
So, for our present problem with k = r, we obtain
||φr||2 =
 ∞
−∞
e−α2x2φ2
r (x) dx = (−1)rr!φr,r
 ∞
−∞
Cre−α2x2 dx.
(5.67)
Now (if y = αx with α > 0)
 ∞
−∞
e−α2x2 dx = 2
 ∞
0
e−α2x2 dx = 2
α
 ∞
0
e−y2 dy = 1
α
√π
(5.68)
via (3.103) in Chapter 3. Consequently, (5.67) becomes
||φr||2 = (−1)rr!φr,rCr
√π
α .
(5.69)
With Cr = (−1)r and α = 1, we recall Hr(x) = φr(x), so (5.69) becomes
||Hr||2 = r!Hr,r
√π.
(5.70)
We need an expression for Hr,r.
We know that for suitable pn,k
dn
dxn e−x2 =
 n

k=0
pn,kxk
	
#
$%
&
=pn(x)
e−x2.
(5.71)
TLFeBOOK

HERMITE POLYNOMIALS
227
Now consider
dn+1
dxn+1 e−x2 = d
dx

 n

k=0
pn,kxk
	
e−x2

= −2x
 n

k=0
pn,kxk
	
e−x2 +
 n

k=1
kpn,kxk−1
	
e−x2
=

−2
n+1

k=0
pn,k−1 +
n+1

k=0
(k + 1)pn,k+1
	
xke−x2
=
n+1

k=0
[−2pn,k−1 + (k + 1)pn,k+1]
#
$%
&
=pn+1,k
xke−x2,
so we have the recurrence relation
pn+1,k = −2pn,k−1 + (k + 1)pn,k+1.
(5.72)
From (5.71) and (5.63) Hn(x) = (−1)npn(x). From (5.72)
pn+1,n+1 = −2pn,n + (n + 2)pn,n+2 = −2pn,n,
and so
Hn+1,n+1 = (−1)n+1pn+1,n+1 = −(−1)n(−2pn,n) = 2Hn,n.
(5.73)
Because H0(x) = 1 [via (5.63)], H0,0 = 1, and immediately we have Hn,n = 2n
[solution to the difference equation (5.73)]. Therefore
||Hr||2 = 2rr!√π,
(5.74)
thus,
" ∞
−∞e−x2H 2
r (x) dx = 2rr!√π.
A three-term recurrence relation for Hn(x) is needed. Deﬁne the generating
function
S(x, t) = exp[−t2 + 2xt] = exp[x2 −(t −x)2].
(5.75)
Observe that
∂n
∂tn S(x, t) = exp[x2] ∂n
∂tn exp[−(t −x)2] = (−1)nexp[x2] ∂n
∂xn exp[−(t −x)2].
(5.76)
Because of the second equality in (5.76), we have
S(n)(x, 0) = ∂n
∂tn S(x, 0) = (−1)nexp[x2] dn
dxn exp[−x2] = Hn(x).
(5.77)
TLFeBOOK

228
ORTHOGONAL POLYNOMIALS
The Maclaurin expansion of S(x, t) about t = 0 is [recall (3.75)]
S(x, t) =
∞

n=0
S(n)(x, 0)
n!
tn,
so via (5.77), this becomes
S(x, t) =
∞

n=0
Hn(x)
n!
tn.
(5.78)
Now, from (5.75) and (5.78), we have
∂S
∂x = 2te−t2+2tx =
∞

n=0
2tn+1
n!
Hn(x),
(5.79a)
and also
∂S
∂x =
∞

n=0
tn
n!
∂Hn(x)
∂x
=
∞

n=−1
tn+1
(n + 1)!
dHn+1(x)
dx
.
(5.79b)
Comparing the like powers of t in (5.79) yields
1
(n + 1)!
dHn+1(x)
dx
= 2
n!Hn(x),
which implies that
dHn+1(x)
dx
= 2(n + 1)Hn(x)
(5.80)
for n ∈Z+. We may also consider
∂S
∂t = (−2t + 2x)e−t2+2tx =
∞

n=0
(−2t + 2x)
n!
tnHn(x)
(5.81a)
and
∂S
∂t =
∞

n=1
tn−1
(n −1)!Hn(x) =
∞

n=0
tn
n!Hn+1(x).
(5.81b)
From (5.81a)
∞

n=0
(−2t + 2x)
n!
tnHn(x) = −2
∞

n=0
tn
(n −1)!Hn−1(x) + 2x
∞

n=0
tn
n!Hn(x).
(5.82)
TLFeBOOK

LEGENDRE POLYNOMIALS
229
−2.5
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
2.5
−150
−100
−50
0
50
100
150
x
Hn(x)
n = 2 
n = 3 
n = 4 
n = 5 
Figure 5.2
Hermite polynomials of degrees 2, 3, 4, 5.
Comparing like powers of t in (5.81b) and (5.82) yields
1
n!Hn+1(x) = −
2
(n −1)!Hn−1(x) + 2x 1
n!Hn(x)
or
Hn+1(x) = 2xHn(x) −2nHn−1(x).
(5.83)
This holds for all n ∈N. Equation (5.83) is another special case of (5.5).
The Hermite polynomials are relevant to quantum mechanics (quantum harmonic
oscillator problem), and they arise in signal processing as well (often in connection
with the uncertainty principle for signals). A few Hermite polynomials are plotted
in Fig. 5.2.
5.5
LEGENDRE POLYNOMIALS
We consider D = [−1, 1] with uniform weighting function
w(x) = 1
for all
x ∈D.
(5.84)
TLFeBOOK

230
ORTHOGONAL POLYNOMIALS
In this case (5.24) becomes
dr+1
dxr+1
drGr(x)
dxr

= 0
or
d2r+1
dx2r+1 Gr(x) = 0.
(5.85)
The boundary conditions of (5.25) become
G(k)
r (±1) = 0
(5.86)
for k ∈Zr. Consequently, the solution to (5.85) is given by
Gr(x) = Cr(x2 −1)r.
(5.87)
Thus, from (5.23)
φr(x) = Cr
dr
dxr (x2 −1)r.
(5.88)
The Legendre polynomials use Cr =
1
2rr!, and are denoted by
Pr(x) =
1
2rr!
dr
dxr (x2 −1)r,
(5.89)
which is the Rodrigues formula for Pr(x). By construction, for k ̸= r, we must
have
 1
−1
Pr(x)Pk(x) dx = 0.
(5.90)
From (5.66) and (5.87), we obtain
||Pr||2 = (−1)r
2r
Pr,r
 1
−1
(x2 −1)r dx.
(5.91)
Recalling the binomial theorem
(a + b)r =
r

k=0
 r
k
!
akbr−k,
(5.92)
TLFeBOOK

LEGENDRE POLYNOMIALS
231
we see that
dr
dxr (x2 −1)r = dr
dxr
r

k=0
 r
k
!
(−1)r−kx2k
= dr
dxr

x2r +
r−1

k=0
 r
k
!
(−1)r−kx2k
	
= 2r(2r −1) · · · (r + 1)xr + dr
dxr
r−1

k=0
 r
k
!
(−1)r−kx2k
	
,
implying that (recall (5.89))
Pr,r = 2r(2r −1) · · · (r + 1)
2rr!
= (2r)!
2r[r!]2 .
(5.93)
But we need to evaluate the integral in (5.91) as well. With the change of variable
x = sin θ, we have
Ir =
 1
−1
(x2 −1)r dx = (−1)r
 π/2
−π/2
cos2r+1 θ dθ.
(5.94)
We may integrate by parts
 π/2
−π/2
cos2r+1 θ dθ =
 π/2
−π/2
cos θ cos2r θ dθ
= cos2r θ sin θ|π/2
−π/2 + 2r
 π/2
−π/2
cos2r−1 θ sin2 θ dθ
(in
"
u dv = uv −
"
v du, we let u = cos2r θ, and dv = cos θ dθ), which becomes
(on using the identity sin2 θ = 1 −cos2 θ)
 π/2
−π/2
cos2r+1 θ dθ = 2r
 π/2
−π/2
cos2r−1 θ dθ −2r
 π/2
−π/2
cos2r+1 θ dθ
for r ≥1. Therefore
(2r + 1)
 π/2
−π/2
cos2r+1 θ dθ = 2r
 π/2
−π/2
cos2r−1 θ dθ.
(5.95)
Now, since [via (5.94)]
Ir−1 = −(−1)r
 π/2
−π/2
cos2r−1 θ dθ,
TLFeBOOK

232
ORTHOGONAL POLYNOMIALS
Eq. (5.95) becomes
(2r + 1)(−1)rIr = −2r(−1)rIr−1,
or more simply
Ir = −
2r
2r + 1Ir−1.
(5.96)
This holds for r ≥1 with initial condition I0 = 2. The solution to the difference
equation is
Ir = (−1)r 22r+1[r!]2
(2r + 1)! .
(5.97)
This can be conﬁrmed by direct substitution of (5.97) into (5.96). Consequently, if
we combine (5.91), (5.93), and (5.97), then
||Pr||2 = (−1)2
2r
· (2r)!
2r[r!]2 · (−1)r22r+1[r!]2
(2r + 1)!
,
which simpliﬁes to
||Pr||2 =
2
2r + 1.
(5.98)
A closed-form expression for Pn(x) is possible using (5.92) in (5.89). Speciﬁ-
cally, consider
Pn(x) =
1
2nn!
dn
dxn (x2 −1)n =
1
2nn!
dn
dxn
 M

k=0
 n
k
!
(−1)kx2n−2k
	
,
(5.99)
where M = n/2 (n even), or M = (n −1)/2 (n odd). We observe that
dn
dxn x2n−2k = (2n −2k)(2n −2k −1) · · · (n −2k + 1)xn−2k.
(5.100)
Now
(2n −2k)! = (2n −2k)(2n −2k −1) · · · (n −2k + 1) (n −2k) · · · 2 · 1
#
$%
&
=(n−2k)!
,
so (5.100) becomes
dn
dxn x2n−2k = (2n −2k)!
(n −2k)! xn−2k.
(5.101)
Thus, (5.99) reduces to
Pn(x) =
1
2nn!
M

k=0
 n
k
!
(−1)k (2n −2k)!
(n −2k)! xn−2k,
TLFeBOOK

LEGENDRE POLYNOMIALS
233
or alternatively
Pn(x) =
M

k=0
(−1)k
(2n −2k)!
2nk!(n −k)!(n −2k)!xn−2k.
(5.102)
Consider f (x) =
1
√
1−x so that f (k)(x) = (2k−1)(2k−3)···3·1
2k
(1 −x)−(2k+1)/2 (for
k ≥1). Deﬁne (2k −1)!! = (2k −1)(2k −3) · · · 3 · 1, and deﬁne (−1)!! = 1. As
usual, deﬁne 0! = 1. We note that (2n)! = 2nn!(2n −1)!! which may be seen by
considering
(2n)! = (2n)(2n −1)(2n −2)(2n −3) · · · 3 · 2 · 1
= [(2n)(2n −2) · · · 4 · 2][(2n −1)(2n −3) · · · 3 · 1] = 2nn!(2n −1)!!.
(5.103)
Consequently, the Maclaurin expansion for f (x) is given by
f (x) =
1
√
1 −x
=
∞

k=0
f (k)(0)
k!
xk =
∞

k=0
(2k)!
22k[k!]2 xk.
(5.104)
The ratio test [7, p. 709] conﬁrms that this series converges if |x| < 1. Using
(5.102) and (5.104), it is possible to show that
S(x, t) =
1
√
1 −2xt + t2 =
∞

n=0
Pn(x)tn,
(5.105)
so this is the generating function for the Legendre polynomials Pn(x). We observe
that
∂S
∂t =
x −t
[1 −2xt + t2]3/2 =
x −t
1 −2xt + t2 S.
(5.106)
Also
∂S
∂t =
∞

n=0
nPn(x)tn−1.
(5.107)
Equating (5.107) and (5.106), we have
x −t
1 −2xt + t2
∞

n=0
Pn(x)tn =
∞

n=0
nPn(x)tn−1,
which becomes
x
∞

n=0
Pn(x)tn −
∞

n=0
Pn(x)tn+1 =
∞

n=0
nPn(x)tn−1 −2x
∞

n=0
nPn(x)tn
+
∞

n=0
nPn(x)tn+1,
TLFeBOOK

234
ORTHOGONAL POLYNOMIALS
and if P−1(x) = 0, then this becomes
x
∞

n=0
Pn(x)tn −
∞

n=0
Pn−1(x)tn =
∞

n=0
(n + 1)Pn+1(x)tn −2x
∞

n=0
nPn(x)tn
+
∞

n=0
(n −1)Pn−1(x)tn,
so on comparing like powers of t in this expression, we obtain
xPn(x) −Pn−1(x) = (n + 1)Pn+1(x) −2xnPn(x) + (n −1)Pn−1(x),
which ﬁnally yields
(n + 1)Pn+1(x) = (2n + 1)xPn(x) −nPn−1(x)
or (for n ≥1)
Pn+1(x) = 2n + 1
n + 1 xPn(x) −
n
n + 1Pn−1(x),
(5.108)
which is the three-term recurrence relation for the Legendre polynomials, and hence
is yet another special case of (5.5).
−1.5
−1
−0.5
0
0.5
1
1.5
−4
−3
−2
−1
0
1
2
3
x
Pn(x)
n = 2
n = 3
n = 4
n = 5
Figure 5.3
Legendre polynomials of degrees 2, 3, 4, 5.
TLFeBOOK

ORTHOGONAL POLYNOMIAL LEAST-SQUARES APPROXIMATION
235
The Legendre polynomials arise in potential problems in electromagnetics. Mod-
eling the scattering of electromagnetic radiation by particles involves working with
these polynomials. Legendre polynomials appear in quantum mechanics as part
of the solution to Schr¨odinger’s equation for the hydrogen atom. Some Legendre
polynomials appear in Fig. 5.3.
5.6
AN EXAMPLE OF ORTHOGONAL POLYNOMIAL
LEAST-SQUARES APPROXIMATION
Chebyshev polynomials of the ﬁrst kind, and Legendre polynomials are both orthog-
onal on [−1, 1], but their weighting functions are different. We shall illustrate the
approximation behavior of these polynomials through an example wherein we wish
to approximate
f (x) =
 0,
−1 ≤x < 0
1,
0 ≤x ≤1
(5.109)
by both of these polynomial types. We will work with ﬁfth-degree least-squares
polynomial approximations in both cases.
We consider Legendre polynomial approximation ﬁrst. We must therefore nor-
malize the polynomials so that our basis functions have unit norm. Thus, our
approximation will be
ˆf (x) =
5

k=0
ˆakφk(x),
(5.110)
where
φk(x) =
1
||Pk||Pk(x),
and
ˆak = ⟨f, φk⟩=
 1
−1
1
||Pk||f (x)Pk(x) dx,
for which we see that
ˆf (x) =
5

k=0
 1
−1
f (x)Pk(x) dx
	
#
$%
&
=ˆαk
Pk(x)
||Pk||2 .
(5.111)
We have [e.g., via (5.102)]
P0(x) = 1, P1(x) = x, P2(x) = 1
2[3x2 −1],
P3(x) = 1
2[5x3 −3x], P4(x) = 1
8[35x4 −30x2 + 3],
P5(x) = 1
8[63x5 −70x3 + 15x].
TLFeBOOK

236
ORTHOGONAL POLYNOMIALS
The squared norms [via (5.98)] are
||P0||2 = 2,
||P1||2 = 2
3,
||P2||2 = 2
5,
||P3||2 = 2
7,
||P4||2 = 2
9,
||P5||2 = 2
11.
By direct calculation, ˆαk =
" 1
0 Pk(x) dx becomes
ˆα0 =
 1
0
1 · dx = 1, ˆα1 =
 1
0
x dx = 1
2,
ˆα2 = 1
2
 1
0
[3x2 −1] dx = 3
2
1
3x3
1
0
−1
2 [x]1
0 = 0,
ˆα3 = 1
2
 1
0
[5x3 −3x] dx = 5
2
1
4x4
1
0
−3
2
1
2x2
1
0
= −1
8,
ˆα4 = 1
8
 1
0
[35x4 −30x2 + 3] dx = 35
8
1
5x5
1
0
−30
8
1
3x3
1
0
+ 3
8 [x]1
0 = 0,
ˆα5 = 1
8
 1
0
[63x5 −70x3 + 15x] dx
= 63
8
1
6x6
1
0
−70
8
1
4x4
1
0
+ 15
8
1
2x2
1
0
= 1
16.
The substitution of these (and the squared norms ||Pk||2) into (5.111) yields the
least-squares Legendre polynomial approximation
ˆf (x) = 1
2P0(x) + 3
4P1(x) −7
16P3(x) + 11
32P5(x).
(5.112)
We observe that ˆf (0) = 1
2, ˆf (1) = 37
32 = 1.15625, and ˆf (−1) = −5
32 = −0.15625.
Now we consider the Chebyshev polynomial approximation. In this case
ˆf (x) =
5

k=0
ˆbkφk(x),
(5.113)
where
φk(x) =
1
||Tk||Tk(x),
and
ˆbk = ⟨f, φk⟩=
 1
−1
1
√
1 −x2
f (x)Tk(x)
||Tk||
dx,
TLFeBOOK

ORTHOGONAL POLYNOMIAL LEAST-SQUARES APPROXIMATION
237
from which we see that
ˆf (x) =
5

k=0
 1
−1
1
√
1 −x2 f (x)Tk(x) dx
	
#
$%
&
= ˆβk
1
||Tk||2 Tk(x).
(5.114)
We have [e.g., via (5.57)] the polynomials
T0(x) = 1,
T1(x) = x,
T2(x) = 2x2 −1,
T3(x) = 4x3 −3x,
T4(x) = 8x4 −8x2 + 1,
T5(x) = 16x5 −20x3 + 5x.
The squared norms [via (5.55)] are given by
||T0||2 = π, ||Tk||2 = π
2
(k ≥1).
By direct calculation
ˆβk =
" 1
0
1
√
1−x2 Tk(x) dx becomes (using x = cos θ, and
Tk(cos θ) = cos(kθ) [recall (5.53)] ˆβk =
" π/2
0
cos(kθ) dθ, and hence
ˆβ0 =
 π/2
0
1 · dθ = π
2 ,
ˆβ1 =
 π/2
0
cos θ dθ = [sin θ]π/2
0
= 1,
ˆβ2 =
 π/2
0
cos(2θ) dθ =
1
2 sin(2θ)
π/2
0
= 0,
ˆβ3 =
 π/2
0
cos(3θ) dθ =
1
3 sin(3θ)
π/2
0
= −1
3,
ˆβ4 =
 π/2
0
cos(4θ) dθ =
1
4 sin(4θ)
π/2
0
= 0,
ˆβ5 =
 π/2
0
cos(5θ) dθ =
1
5 sin(5θ)
π/2
0
= 1
5.
Substituting these (and the squared norms ||Tk||2) into (5.114) yields the least-
squares Chebyshev polynomial approximation
ˆf (x) = 1
π
π
2 T0(x) + 2T1(x) −2
3T3(x) + 2
5T5(x)

.
(5.115)
We observe that ˆf (0) = 1
2, ˆf (1) = 1.051737, and ˆf (−1) = −0.051737.
TLFeBOOK

238
ORTHOGONAL POLYNOMIALS
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
−0.2
0
0.2
0.4
0.6
0.8
1
1.2
x
 Amplitude 
f(x)
 Legendre approximation 
 Chebyshev approximation 
Figure 5.4
Plots of f (x) from (5.109), the Legendre approximation (5.112), and the
Chebyshev approximation (5.115). The least-squares approximations are ﬁfth-degree poly-
nomials in both cases.
Plots of f (x) from (5.109) and the approximations in (5.112) and (5.115) appear
in Fig. 5.4. Both polynomial approximations are ﬁfth-degree polynomials, and yet
they look fairly different. This is so because the weighting functions are different.
The Chebyshev approximation is better than the Legendre approximation near x =
±1 because greater weight is given to errors near the ends of the interval [−1, 1].
5.7
UNIFORM APPROXIMATION
The subject of uniform approximation is distinctly more complicated than that of
least-squares approximation. So, we will not devote too much space to it in this
book. We will concentrate on a relatively simple illustration of the main idea.
Recall the space C[a, b] from Chapter 1. We recall that it is a normed space
with the norm
||x|| = sup
t∈[a,b]
|x(t)|
(5.116)
for any x(t) ∈C[a, b]. In fact, further recalling Chapter 3, this space happens to
be a complete metric space for which the metric induced by (5.116) is
d(x, y) = sup
t∈[a,b]
|x(t) −y(t)|,
(5.117)
TLFeBOOK

UNIFORM APPROXIMATION
239
where x(t), y(t) ∈C[a, b]. In uniform approximation we (for example) may wish
to approximate f (t) ∈C[a, b] with x(t) ∈Pn[a, b] ⊂C[a, b] such that ||f −x||
is minimized. The norm (5.116) is sometimes called the Chebyshev norm, and so
our problem is sometimes called the Chebyshev approximation problem. The error
e(t) = f (t) −x(t) has the norm
||e|| = sup
t∈[a,b]
|f (t) −x(t)|
(5.118)
and is the maximum deviation between f (t) and x(t) on [a, b]. We wish to ﬁnd x(t)
to minimize this. Consequently, uniform approximation is sometimes also called
minimax approximation as we wish to minimize the maximum deviation between
f (t) and its approximation x(t). We remark that because f (t) is continuous (by
deﬁnition) it will have a well-deﬁned maximum on [a, b] (although this maximum
need not be at a unique location). We are therefore at liberty to replace “sup” by
“max” in (5.116), (5.117) and (5.118) if we wish.
Suppose that yj(t) ∈C[a, b] for j = 0, 1, . . . , n −1 and the set of functions
{yj(t)|j ∈Zn} are an independent set. This set generates an n-dimensional sub-
space4 of C[a, b] that may be denoted by Y =
9n−1
j=0 αjyj(t)|αj ∈R
:
. From
Kreyszig [8, p. 337] consider
Deﬁnition 5.1: Haar Condition
Subspace Y ⊂C[a, b] satisﬁes the Haar
condition if every y ∈Y (y ̸= 0) has at most n −1 zeros in [a, b], where n =
dim(Y) (dimension of the subspace Y).
We may select yj(t) = tj for j ∈Zn so any y(t) ∈Y has the form y(t) =
n−1
j=0 αjtj and thus is a polynomial of degree at most n −1. A degree n −1
polynomial has n −1 zeros, and so such a subspace Y satisﬁes the Haar condition.
Deﬁnition 5.2: Alternating Set
Let x ∈C[a, b], and y ∈Y, where Y is any
subspace of C[a, b]. A set of points t0, . . . , tk in [a, b] such that t0 < t1 < · · · < tk
is called an alternating set for x −y if x(tj) −y(tj) has alternately the values
+||x −y||, and −||x −y|| at consecutive points tj.
Thus, suppose x(tj) −y(tj) = +||x −y||, then x(tj+1) −y(tj+1) = −||x −y||,
but if instead x(tj) −y(tj) = −||x −y||, then x(tj+1) −y(tj+1) = +||x −y||. The
norm is, of course, that in (5.116).
Lemma 5.2: Best Approximation
Let Y be any subspace of C[a, b] that
satisﬁes the Haar condition. Given f ∈C[a, b], let y ∈Y be such that for f −y
there exists an alternating set of n + 1 points, where n = dim(Y), then y is the
best uniform approximation of f out of Y.
The proof is omitted, but may be found in Kreyszig [8, pp. 345–346].
4A subspace of a vector space X is a nonempty subset Y of X such that for any y1, y2 ∈Y, and all
scalars a, b from the ﬁeld of the vector space we have ay1 + by2 ∈Y.
TLFeBOOK

240
ORTHOGONAL POLYNOMIALS
Consider the particular case of C[−1, 1] with f (t) ∈C[−1, 1] such that for a
given n ∈N
f (t) = tn.
(5.119)
Now Y =
9n−1
j=0 αjtj|αj ∈R
:
, so yj(t) = tj for j ∈Zn. We wish to select αj
such that the error e = f −y
e(t) = f (t) −
n−1

j=0
αjtj
(5.120)
is minimized with respect to the Chebyshev norm (5.116); that is, select αj to
minimize ||e||. Clearly, dim(Y) = n. According to Lemma 5.2 ||e|| is minimized
if e(t) in (5.120) has an alternating set of n + 1 points.
Recall Lemma 5.1 (Section 5.3), which stated [see (5.44)] that
cos nθ =
n

k=0
βn,k cosk θ.
(5.121)
From (5.46) βn+1,n+1 = 2βn,n, and β0,0 = 1. Consequently, βn,n = 2n−1 (for n ≥
1). Thus, (5.121) can be rewritten as
cos nθ = 2n−1 cosn θ +
n−1

j=0
βn,j cosj θ
(5.122)
(n ≥1). Suppose that t = cos θ, so θ ∈[0, π] maps to t ∈[−1, 1] and from (5.122)
cos[n cos−1 t] = 2n−1tn +
n−1

j=0
βn,jtj.
(5.123)
We observe that cos nθ has an alternating set of n + 1 points θ0, θ1, . . . , θn on [0, π]
for which cos nθk = ±1 (clearly || cos nθ|| = 1). For example, if n = 1, then
θ0 = 0,
θ1 = π.
If n = 2, then
θ0 = 0,
θ1 = π/2,
θ2 = π,
and if n = 3, then
θ0 = 0,
θ1 = π
3 ,
θ2 = 2π
3 ,
θ3 = π.
In general, θk = k
nπ for k = 0, 1, . . . , n. Thus, if tk = cos θk then tk = cos
) k
nπ
*
.
We may rewrite (5.123) as
1
2n−1 cos[n cos−1 t] = tn +
1
2n−1
n−1

j=0
βn,jtj.
(5.124)
TLFeBOOK

PROBLEMS
241
This is identical to e(t) in (5.120) if we set βn,j = −2n−1αj. In other words, if we
choose
e(t) =
1
2n−1 cos[n cos−1 t],
(5.125)
then ||e|| is minimized since we know that e has an alternating set t = tk =
cos
) k
nπ
*
, k = 0, 1, . . . , n, and tk ∈[−1, 1]. We recall from Section 5.3 that Tk(t) =
cos[k cos−1 t] is the kth-degree Chebyshev polynomial of the ﬁrst kind [see (5.53)].
So, e(t) = Tn(t)/2n−1. Knowing this, we may readily determine the optimal coef-
ﬁcients αj in (5.120) if we so desire.
Thus, the Chebyshev polynomials of the ﬁrst kind determine the best degree
n −1 polynomial uniform approximation to f (t) = tn on the interval t ∈[−1, 1].
REFERENCES
1. E. Isaacson and H. B. Keller, Analysis of Numerical Methods, Wiley, New York, 1966.
2. F. B. Hildebrand, Introduction to Numerical Analysis, 2nd ed., McGraw-Hill, New York,
1974.
3. J. S. Lim and A. V. Oppenheim, eds., Advanced Topics in Signal Processing. Prentice-
Hall, Englewood Cliffs, NJ, 1988.
4. P. J. Davis and P. Rabinowitz, Numerical Integration, Blaisdell, Waltham, MA, 1967.
5. J. R. Rice, The Approximation of Functions, Vol. I: Linear Theory, Addison-Wesley,
Reading, MA, 1964.
6. G. Szeg¨o, Orthogonal Polynomials, 3rd ed., American Mathematical Society, 1967.
7. L. Bers, Calculus: Preliminary Edition, Vol. 2, Holt, Rinehart, Winston, New York, 1967.
8. E. Kreyszig, Introductory Functional Analysis with Applications, Wiley, New York,
1978.
PROBLEMS
5.1. Suppose that {φk(x)|k ∈ZN} are orthogonal polynomials on the interval D =
[a, b] ⊂R with respect to some weighting function w(x) ≥0. Show that
N−1

k=0
akφk(x) = 0
holds for all x ∈[a, b] iff ak = 0 for all k ∈ZN [i.e., prove that {φk(x)|k ∈
ZN} is an independent set].
5.2. Verify (5.11) by direct calculation for φk(x) = Tk(x)/||Tk|| with n = 2; that
is, ﬁnd φ0(x), φ1(x), φ2(x), and φ3(x), and verify that the left- and right-hand
sides of (5.11) are equal to each other.
TLFeBOOK

242
ORTHOGONAL POLYNOMIALS
5.3. Suppose that {φn(x)|n ∈Z+} are orthogonal polynomials on the interval
[a, b] with respect to some weighting function w(x) ≥0. Prove the fol-
lowing theorem: The roots xj (j = 1, 2, . . . , n) of φn(x) = 0 (n ∈N) are all
real-valued, simple, and a < xj < b for all j.
5.4. Recall that Tj(x) is the degree j Chebyshev polynomial of the ﬁrst kind.
Recall from Lemma 5.1 that cos(nθ) = n
k=0 βn,k cosk θ. For x ∈[−1, 1]
with k ∈Z+, we can ﬁnd coefﬁcients aj such that
xk =
k

j=0
ajTj(x).
Prove that
aj =
1
||Tj||2
j

r=0
βj,r
 π
0
cosk+r θ dθ.
Is this the best way to ﬁnd the coefﬁcients aj ? If not, specify an alternative
approach.
5.5. This problem is about the converse to Lemma 5.1. Suppose that we have
p(cos θ) = 3
k=0 a3,k cosk θ. We wish to ﬁnd coefﬁcients b3,j such that
p(cos θ) =
3

j=0
b3,j cos(jθ).
Use Lemma 5.1 to show that a = Ub, where
a = [a3,0 a3,1 a3,2 a3,3]T , b = [b3,0 b3,1 b3,2 b3,3]T
and U is an upper triangular matrix containing the coefﬁcients βm,k for
m = 0, 1, 2, 3, and k = 0, 1 . . . , m. Of course, we may use back-substitution
to solve Ub = a for b if this were desired.
5.6. Suppose that for n ∈Z+ we are given p(cos θ) = n
k=0 an,k cosk θ. Show
how to ﬁnd the coefﬁcients bn,j such that
p(cos θ) =
n

j=0
bn,j cos(jθ),
that is, generalize the previous problem from n = 3 to any n.
5.7. Recall Section 5.6, where
f (x) =
 0,
−1 ≤x < 0
1,
0 ≤x ≤1
TLFeBOOK

PROBLEMS
243
was approximated by
ˆf (x) =
n

k=0
ˆbkφk(x), φk(x) =
1
||Tk||Tk(x)
for n = 5. Find a general expression for ˆbk for all k ∈Z+. Use the resulting
expansion
f (x) =
∞

k=0
ˆbk
||Tk||Tk(x)
to prove that
π
4 =
∞

n=1
1
2n −1(−1)n−1.
5.8. Suppose that
f (x) =
 0,
−1 ≤x < 0
x,
0 ≤x ≤1
,
then ﬁnd ˆbk in
f (x) =
∞

k=0
ˆbk
1
||Tk||Tk(x).
5.9. Do the following:
(a) Solve the polynomial equation Tk(x) = 0 for all k > 0, and so ﬁnd all
the zeros of the polynomials Tk(x).
(b) Show that Tn(x) satisﬁes the differential equation
(1 −x2)T (2)
n (x) −xT (1)
n (x) + n2Tn(x) = 0.
Recall that T (r)
n (x) = drTn(x)/dxr.
5.10. The three-term recurrence relation for the Chebyshev polynomials of the sec-
ond kind is
Ur+1(x) = 2xUr(x) −Ur−1(x),
(5.P.1)
where r ≥1, and where the initial conditions are
U0(x) = 1, U1(x) = 2x.
(5.P.2)
We remark that (5.P.1) is identical to (5.57). Speciﬁcally, the recursion for
Chebyshev polynomials of both kinds is the same, except that the initial
TLFeBOOK

244
ORTHOGONAL POLYNOMIALS
conditions in (5.P.2) are not the same for both. [From (5.56) T0(x) = 1, but
T1(x) = x.] Since deg(Ur) = r, we have (for r ≥0)
Ur(x) =
r

j=0
Ur,jxj.
(5.P.3)
[Recall the notation for φr(x) in (5.3).] From the polynomial recursion in
(5.P.1) we may obtain
Ur+1,j = 2Ur,j−1 −Ur−1,j.
(5.P.4a)
This expression holds for r ≥1, and for j = 0, 1, . . . , r, r + 1. From (5.P.2)
the initial conditions for (5.P.4a) are
U0,0 = 1, U1,0 = 0, U1,1 = 2.
(5.P.4b)
Write a MATLAB function that uses (5.P.4) to generate the Chebyshev poly-
nomials of the second kind for r = 0, 1, . . . , N. Test your program out for
N = 8. Program output should be in the form of a table that is written to a
ﬁle. The tabular format should be something like
degree
0
1
2
3
4
5
6
7
8
0
1
0
0
0
0
0
0
0
0
1
0
2
0
0
0
0
0
0
0
2
-1
0
4
0
0
0
0
0
0
3
0
-4
0
8
0
0
0
0
0
etc.
5.11. The Chebyshev polynomials of the second kind use
D = [−1, 1], w(x) =

1 −x2.
(5.P.5)
Denote these polynomials as the set {φn(x)|n ∈Z+}. (This notation applies
if the polynomials are normalized to possess unity-valued norms.) Of course,
in our function space, we use the inner product
⟨f, g⟩=
 1
−1

1 −x2f (x)g(x) dx.
(5.P.6)
Derive the polynomials {φn(x)|n ∈Z+}. (Hint: The process is much the
same as the derivation of the Chebyshev polynomials of the ﬁrst kind pre-
sented in Section 5.3.) Therefore, begin by considering qr−1(x) which is
any polynomial of degree not more than r −1. Thus, for suitable ck ∈R, we
TLFeBOOK

PROBLEMS
245
must have qr−1(x) = r−1
j=0 cjφj(x), and ⟨φr, qr−1⟩= r−1
j=0 cj⟨φr, φj⟩= 0
because ⟨φr, φj⟩= 0 for j = 0, 1, . . . , r −1. In expanded form
⟨φr, qr−1⟩=
 1
−1

1 −x2φr(x)qr−1(x) dx = 0.
(5.P.7)
Use the change of variable x = cos θ (so dx = −sin θ dθ) to reduce (5.P.7) to
 π
0
sin2 θ cos(kθ)φr(cos θ) dθ = 0
(5.P.8)
for k = 0, 1, 2, . . . , r −1, where Lemma 5.1 and its converse have also been
employed. Next consider the candidate
φr(cos θ) = Cr
sin(r + 1)θ
sin θ
,
(5.P.9)
and verify that this satisﬁes (5.P.8). [Hence (5.P.9) satisﬁes (5.P.7).] Show
that (5.P.9) becomes
φr(x) = Cr
sin[(r + 1) cos−1 x]
sin cos−1 x
.
Prove that for Cr = 1 (all r ∈Z+)
φr+1(x) = 2xφr(x) −φr−1(x).
In this case we normally use the notation Ur(x) = φr(x). Verify that U0(x) =
1, and that U1(x) = 2x. Prove that ||Un||2 = π
2 for n ∈Z+. Finally, of
course, φn(x) = Un(x)/||Un||.
5.12. Write a MATLAB function to produce plots of Uk(x) for k = 2, 3, 4, 5 similar
to Fig. 5.1.
5.13. For φk(x) = Uk(x)/||Uk||, we have
φk(x) =
8
2
π Uk(x).
Since ⟨φk, φj⟩= δk−j, for any f (x) ∈L2[−1, 1], we have the series
expansion
f (x) =
∞

k=0
ˆakφk(x),
where
ˆak = ⟨f, φk⟩=
1
||Uk||
 1
−1

1 −x2f (x)Uk(x) dx.
TLFeBOOK

246
ORTHOGONAL POLYNOMIALS
Suppose that we work with the following function:
f (x) =
 0,
−1 ≤x < 0
1,
0 ≤x ≤1
.
(a) Find a nice general formula for the elements of the sequence (ˆak)
(k ∈Z+).
(b) Use MATLAB to plot the approximation
ˆf2(x) =
5

k=0
ˆakφk(x)
on the same graph as that of f (x) (i.e., create a plot similar to that of
Fig. 5.4). Suppose that ψk(x) = Tk(x)/||Tk||; then another approxima-
tion to f (x) is given by
ˆf1(x) =
5

k=0
ˆbkψk(x).
Plot ˆf1(x) on the same graph as ˆf2(x) and f (x).
(c) Compare the accuracy of the approximations ˆf1(x) and ˆf2(x) to f (x)
near the endpoints x = ±1. Which approximation is better near these
endpoints ? Explain why if you can.
5.14. Prove the following:
(a) Tn(x), and Tn−1(x) have no zeros in common.
(b) Between any two neighboring zeros of Tn(x), there is precisely one zero
of Tn−1(x). This is called the interleaving of zeros property.
(Comment: The interleaving of zeros property is possessed by all orthog-
onal polynomials. When this property is combined with ideas from later
chapters, it can be used to provide algorithms to ﬁnd the zeros of orthogonal
polynomials in general.)
5.15. (a) Show that we can write
Tn(x) = cos[n cos−1 x] = cosh[n cosh−1 x].
[Hint: Note that cos x = 1
2(ejx + e−jx), cosh x = 1
2(ex + e−x), so that
cos x = cosh(jx).]
(b) Prove that T2n(x) = Tn(2x2 −1). [Hint: cos(2x) = 2 cos2 x −1.]
5.16. Use Eq. (5.63) in the following problems:
(a) Show that
d
dx Hr(x) = 2rHr−1(x).
TLFeBOOK

PROBLEMS
247
(b) Show that
d
dx

e−x2 d
dx Hr(x)

= −2re−x2Hr(x).
(c) From the preceding conﬁrm that Hr(x) satisﬁes the Hermite differential
equation
H (2)
r
(x) −2xH (1)
r
(x) + 2rHr(x) = 0.
5.17. Find Hk(x) for k = 0, 1, 2, 3, 4, 5 (i.e., ﬁnd the ﬁrst six Hermite polynomials)
using (5.83).
5.18. Using (5.63) prove that
Hk(x) = n!
N

j=0
(−1)j
2n−2j
j!(n −2j)!xn−2j,
where N = n/2 (n is even), N = (n −1)/2 (n is odd).
5.19. Suppose that Pk(x) is the Legendre polynomial of degree k. Recall Eq.
(5.90). Find constants α and β such that for Qk(x) = Pk(αx + β) we have
(for k ̸= r)
 b
a
Qr(x)Qk(x) dx = 0.
[Comment: This linear transformation of variable allows us to least-squares
approximate f (x) using Legendre polynomial series on any interval [a, b].]
5.20. Recall Eq. (5.105):
1
√
1 −2xt + t2 =
∞

n=0
Pn(x)tn.
Verify the terms n = 0, 1, 2, 3.
[Hint: Recall Eq. (3.82) (From Chapter 3).]
5.21. The distance between two points A and B in R3 is r, while the distance from
A to the origin O is r1, and the distance from B to the origin O is r2. The
angle between the vector −→
OA, and vector −→
OB is θ.
(a) Show that
1
r =
1
7
r2
1 + r2
2 −2r1r2 cos θ
.
[Hint: Recall the law of cosines (Section 4.6).]
TLFeBOOK

248
ORTHOGONAL POLYNOMIALS
(b) Show that
1
r = 1
r2
∞

n=0
Pn(cos θ)
 r1
r2
!n
,
where, of course, Pn(x) is the Legendre polynomial of degree n.
(Comment: This result is important in electromagnetic potential theory.)
5.22. Recall Section 5.7. Write a MATLAB function to plot on the same graph
both f (t) = tn and e(t) [in (5.125)] for each of n = 2, 3, 4, 5. You must
generate four separate plots, one for each instance of n.
5.23. The set of points C = {ejθ|θ ∈R, j =
√
−1} is the unit circle of the complex
plane. If R(ejθ) ≥0 for all θ ∈R, then we may deﬁne an inner product on
a suitable space of functions that are deﬁned on C
⟨F, G⟩= 1
2π
 π
−π
R(ejθ)F ∗(ejθ)G(ejθ) dθ,
(5.P.10)
where, in general, F(ejθ), G(ejθ) ∈C. Since ejθ is 2π-periodic in θ, the
integration limits in (5.P.10) are −π to π, but another standard choice is
from 0 to 2π. The function R(ejθ) is a weighting function for the inner
product in (5.P.10). For R(ejθ), there will be a real-valued sequence (rk)
such that
R(ejθ) =
∞

k=−∞
rke−jkθ,
and we also have r−k = rk [i.e., (rk) is a symmetric sequence]. For F(ejθ)
and G(ejθ), we have real-valued sequences (fk), and (gk) such that
F(ejθ) =
∞

k=−∞
fke−jkθ, G(ejθ) =
∞

k=−∞
gke−jkθ.
(a) Show that ⟨e−jnθ, e−jmθ⟩= rn−m.
(b) Show that ⟨e−jnθF(ejθ), e−jnθG(ejθ)⟩= ⟨F(ejθ), G(ejθ)⟩.
(c) Show that ⟨F(ejθ), G(ejθ)⟩= ⟨1, F(e−jθ)G(ejθ)⟩.
[Comment: The unit circle C is of central importance in the theory of stability
of linear time-invariant (LTI) discrete-time systems, and so appears in the
subjects of digital control and digital signal processing.]
5.24. Recall Problem 4.21 from and the previous problem (5.23). Given an(z) =
n−1
k=0 an,kz−k (and z = ejθ), show that
⟨an(z), z−k⟩= σ 2
nδk
for k = 0, 1, . . . , n −1.
TLFeBOOK

PROBLEMS
249
[Comment: This result ultimately leads to an alternative derivation of the
Levinson–Durbin algorithm, and suggests that this algorithm actually gen-
erates a sequence of orthogonal polynomials on the unit circle C. These
polynomials are in the indeterminate z−1 (instead of x).]
5.25. It is possible to construct orthogonal polynomials on discrete sets. This prob-
lem is about a particular example of this. Suppose that
φr[n] =
r

j=0
φr,jnj,
where n ∈[−L, U] ⊂Z, U ≥L ≥0 and φr,r ̸= 0, so that deg(φr) = r, and
let us also assume that φr,j ∈R for all r, and j. We say that {φr[n]|n ∈Z+}
is an orthogonal set if ⟨φk[n], φm[n]⟩= ||φk||2δk−m, where the inner product
is deﬁned by
⟨f [n], g[n]⟩=
U

n=−L
w[n]f [n]g[n],
(5.P.11)
and w[n] ≥0 for all n ∈[−L, U] is a weighting sequence for our inner
product space. (Of course, ||f ||2 = ⟨f [n], f [n]⟩.) Suppose that L = U = M;
then, for w[n] = 1 (all n ∈[−M, M]), it can be shown (with much effort)
that the Gram polynomials are given by the three-term recurrence relation
pk+1[n] =
2(2k + 1)
(k + 1)(2M −k)npk[n] −
k
k + 1
2M + k + 1
2M −k
pk−1[n],
(5.P.12)
where p0[n] = 1, and p1[n] = n/M.
(a) Use (5.P.12) to ﬁnd pk[n] for k = 2, 3, 4, where M = 2.
(b) Use (5.P.12) to ﬁnd pk[n] for k = 2, 3, 4, where M = 3.
(c) Use (5.P.11) to ﬁnd ||pk||2 for k = 2, 3, 4, where M = 2.
(d) Use (5.P.11) to ﬁnd ||pk||2 for k = 2, 3, 4, where M = 3.
(Comment: The uniform weighting function w[n] = 1 makes the Gram poly-
nomials the discrete version of the Legendre polynomials. The Gram poly-
nomials were actually invented by Chebyshev.)
5.26. Integration by parts is clearly quite important in analysis (e.g., recall Section
3.6). To derive the Gram polynomials (previous problem) makes use of sum-
mation by parts.
Suppose that v[n], u[n], and f [n] are deﬁned on Z. Deﬁne the forward
difference operator  according to
f [n] = f [n + 1] −f [n]
TLFeBOOK

250
ORTHOGONAL POLYNOMIALS
[for any sequence (f [n])]. Prove the expression for summation by parts,
which is
U

n=−L
u[n]v[n] = u[n]v[n]
U+1
−L
−
U

n=−L
v[n + 1]u[n].
[Hint: Show that
u[n]v[n] = u[n]v[n] −v[n + 1]u[n],
and then consider using the identity
U

n=−L
f [n] =
U

n=−L
(f [n + 1] −f [n]) = f [n]
U+1
−L
.
Of course, f [n]
B
A = f [B] −f [A].]
TLFeBOOK

6
Interpolation
6.1
INTRODUCTION
Suppose that we have the data {(tk, x(tk))|k ∈Zn+1}, perhaps obtained experimen-
tally. An example of this appeared in Section 4.6 . In this case we assumed that
tk = kTs for which x(tk) = x(t)|t=kTs are the samples of some analog signal. In
this example these time samples were of simulated (and highly oversimpliﬁed)
physiological data for human patients (e.g., blood pressure, heart rate, body core
temperature). Our problem involved assuming a model for the data; thus, assum-
ing x(t) is explained by a particular mathematical function with certain unknown
parameters to be estimated on the basis of the model and the data. In other words,
we estimate x(t) with ˆx(t, α), where α is the vector of unknown parameters (the
model parameters to be estimated), and we chose α to minimize the error
e(tk) = x(tk) −ˆx(tk, α),
k ∈Zn+1
(6.1)
according to some criterion. We have emphasized choosing α to minimize
V (α) =
n

k=0
e2(tk).
(6.2)
This was the least-squares approach. However, the idea of choosing α to mini-
mize maxk∈Zn+1 |e(tk)| is an alternative suggested by Section 5.7. Other choices
are possible. However, no matter what choice we make, in all cases ˆx(tk, α) is
not necessarily exactly equal to x(tk) except perhaps by chance. The problem of
ﬁnding ˆx(t, α) to minimize e(t) in this manner is often called curve ﬁtting. It is to
be distinguished from interpolation, which may be deﬁned as follows.
Usually we assume t0 < t1 < · · · < tn−1 < tn with t0 = a, and tn = b so that
tk ∈[a, b] ⊂R. To interpolate the data {(tk, x(tk))|k ∈Zn+1}, we seek a function
p(t) such that t ∈[a, b], and
p(tk) = x(tk)
(6.3)
for all k ∈Zn+1. We might know something about the properties of x(t) for t ̸=
tk on interval [a, b], and so we might select p(t) to possess similar properties.
An Introduction to Numerical Analysis for Electrical and Computer Engineers, by C.J. Zarowski
ISBN 0-471-46737-5
c⃝2004 John Wiley & Sons, Inc.
251
TLFeBOOK

252
INTERPOLATION
However, we emphasize that the interpolating function p(t) exactly matches x(t)
at the given sample points tk, k ∈Zn+1.
Curve ﬁtting is used when the data are uncertain because of the corrupting effects
of measurement errors, random noise, or interference. Interpolation is appropriate
when the data are accurately or exactly known.
Interpolation is quite important in digital signal processing. For example, ban-
dlimited signals may need to be interpolated in order to change sampling rates.
Interpolation is vital in numerical integration methods, as will be seen later. In
this application the integrand is typically known or can be readily found at some
ﬁnite set of points with signiﬁcant accuracy. Interpolation at these points leads to
a function (usually a polynomial) that can be easily integrated, and so provides a
useful approximation to the given integral.
This chapter discusses interpolation with polynomials only. In principle it is
possible to interpolate using other functions (rational functions, trigonometric func-
tions, etc.) But these other approaches are usually more involved, and so will not
be considered in this book.
6.2
LAGRANGE INTERPOLATION
This chapter presents polynomial interpolation in three different forms. The ﬁrst
form, which might be called the direct form, involves obtaining the interpolat-
ing polynomial by the direct solution of a particular linear system of equations
(Vandermonde system). The second form is an alternative called Lagrange inter-
polation and is considered in this section along with the direct form. The third
form is called Newton interpolation, and is considered in the next section. All
three approaches give the same polynomial but expressed in different mathemati-
cal forms each possessing particular advantages and disadvantages. No one form
is useful in all applications, and this is why we must consider them all.
As in Section 6.1, we consider the data set {(tk, x(tk))|k ∈Zn+1}, and we will
let xk = x(tk). We wish, as already noted, that our interpolating function be a
polynomial of degree n:
pn(t) =
n

j=0
pn,jtj.
(6.4)
For example, if n = 1 (linear interpolation), then
xk =
1

j=0
p1,jtj
k ,
xk+1 =
1

j=0
p1,jtj
k+1,
TLFeBOOK

LAGRANGE INTERPOLATION
253
or
p1,0 + p1,1tk = xk,
p1,0 + p1,1tk+1 = xk+1,
or in matrix form this becomes
 1
tk
1
tk+1
  p1,0
p1,1

=

xk
xk+1

.
This has a unique solution, provided tk ̸= tk+1 because in this instance the matrix
has determinant det
  1
tk
1
tk+1
!
= tk+1 −tk. Polynomial p1(t) = p1,0 + p1,1t
linearly interpolates the points (tk, xk), and (tk+1, xk+1). As another example, if
n = 2 (quadratic interpolation), then we have
xk =
2

j=0
p2,jtj
k ,
xk+1 =
2

j=0
p2,jtj
k+1,
xk+2 =
2

j=0
p2,jtj
k+2,
or in matrix form


1
tk
t2
k
1
tk+1
t2
k+1
1
tk+2
t2
k+2




p2,0
p2,1
p2,2

=


xk
xk+1
xk+2

.
(6.5)
The matrix in (6.5) has the determinant (tk −tk+1)(tk+1 −tk+2)(tk+2 −tk), which
will not be zero if tk, tk+1 and tk+2 are all distinct. The polynomial p2(t) =
p2,0 + p2,1t + p2,2t2 interpolates the points (tk, xk), (tk+1, xk+1), and (tk+2, xk+2).
In general, for arbitrary n we have the linear system


1
tk
· · ·
tn−1
k
tn
k
1
tk+1
· · ·
tn−1
k+1
tn
k+1
...
...
...
...
1
tk+n−1
· · ·
tn−1
k+n−1
tn
k+n−1
1
tk+n
· · ·
tn−1
k+n
tn
k+n


#
$%
&
=A


pn,0
pn,1
...
pn,n−1
pn,n


#
$%
&
=p
=


xk
xk+1
...
xk+n−1
xk+n


#
$%
&
=x
.
(6.6)
TLFeBOOK

254
INTERPOLATION
Matrix A is called a Vandermonde matrix, and the linear system (6.6) is a Vander-
monde linear system of equations. The solution to (6.6) (if it exists) gives the direct
form of the interpolating polynomial stated in (6.4). For convenience we will let
k = 0. If we let t = t0, then
A = A(t) =


1
t
· · ·
tn−1
tn
1
t1
· · ·
tn−1
1
tn
1
...
...
...
...
1
tn−1
· · ·
tn−1
n−1
tn
n−1
1
tn
· · ·
tn−1
n
tn
n


.
(6.7)
Let D(t) = det(A(t)), and we see that D(t) is a polynomial in the indetermi-
nate t of degree n. Therefore, D(t) = 0 is an equation with exactly n roots (via
the fundamental theorem of algebra). But D(tk) = 0 for k = 1, . . . , n since the
rows of A(t) are dependent for any t = tk. So t1, t2, . . . , tn are the only possible
roots of D(t) = 0. Therefore, if t0, t1, . . . , tn are all distinct, then we must have
det(A(t0)) ̸= 0. Hence A in (6.6) will always possess an inverse if tk+i ̸= tk+j for
i ̸= j.
For small values of n (e.g., n = 1, or n = 2), the direct solution of (6.6) can be a
useful method of polynomial interpolation. However, it is known that Vandermonde
matrices can be very ill-conditioned even for relatively small n (e.g., n ≥10 or so).
This is particularly likely to happen for the common case of equispaced data, i.e.,
the case where tk = t0 + hk for k = 0, 1, . . . , n with h > 0, as mentioned in Hill [1,
p. 233]. Thus, we much prefer to avoid interpolating by the direct solution of (6.6)
when n ≫2. Also, direct solution of (6.6) is computationally inefﬁcient, unless one
contemplates the use of fast algorithms for Vandermonde system solution in Golub
and Van Loan [2]. These are signiﬁcantly faster than Gaussian elimination as they
possess asymptotic time complexities of only O(n2) versus the O(n3) complexity
of Gaussian elimination approaches.
We remark that so far we have proved the existence of a polynomial of degree n
that interpolates the data {(tk, xk)|k ∈Zn+1}, provided tk ̸= tj (j ̸= k). The poly-
nomial also happens to be unique, a fact readily apparent from the uniqueness of
the solution to (6.6) assuming the existence condition is met. So, if we can ﬁnd
(by any method) any polynomial of degree ≤n that interpolates the given data,
then this is the only possible interpolating polynomial for the data.
Since we disdain the idea of solving (6.6) directly we seek alternative methods
to obtain pn(t). In this regard better approach to polynomial interpolation is an
often Lagrange interpolation, which works as follows.
Again assume that we wish to interpolate the data set {(tk, xk)|k ∈Zn+1}. Sup-
pose that we possess polynomials (called Lagrange polynomials) Lj(t) with the
property
Lj(tk) =
 0,
j ̸= k
1,
j = k
= δj−k.
(6.8)
TLFeBOOK

LAGRANGE INTERPOLATION
255
Then the interpolating polynomial for the data set is
pn(t) = x0L0(t) + x1L1(t) + · · · + xnLn(t) =
n

j=0
xjLj(t).
(6.9)
We observe that
pn(tk) =
n

j=0
xjLj(tk) =
n

j=0
xjδj−k = xk
(6.10)
for k ∈Zn+1, so pn(t) in (6.9) does indeed interpolate the data set, and via unique-
ness, if we were to write pn(t) in the direct form pn(t) = n
j=0 pn,jtj, then the
polynomial coefﬁcients pn,j would satisfy (6.6). We may see that for j ∈Zn+1 the
Lagrange polynomials are given by
Lj(t) =
n
/
i=0
i̸=j
t −ti
tj −ti
.
(6.11)
Equation (6.9) is called the Lagrange form of the interpolating polynomial.
Example 6.1
Consider data set {(t0, x0), (t1, x1), (t2, x2)} so n = 2. Therefore
L0(t) = (t −t1)(t −t2)
(t0 −t1)(t0 −t2),
L1(t) = (t −t0)(t −t2)
(t1 −t0)(t1 −t2),
L2(t) = (t −t0)(t −t1)
(t2 −t0)(t2 −t1).
It is not difﬁcult to see that p2(t) = x0L0(t) + x1L1(t) + x2L2(t). For example,
p2(t0) = x0L0(t0) + x1L1(t0) + x2L2(t0) = x0.
Suppose that the data set has the speciﬁc values {(0, 1), (1, 2), (2, 3)}, and so
(6.6) becomes


1
0
0
1
1
1
1
2
4




p2,0
p2,1
p2,2

=


1
2
3

.
This has solution p2(t) = t + 1 (i.e., p2,2 = 0). We see that the interpolating poly-
nomial pn(t) need not have degree exactly equal to n, but can be of lower degree.
We also see that
L0(t) = (t −1)(t −2)
(0 −1)(0 −2) = 1
2(t −1)(t −2) = 1
2(t2 −3t + 2),
TLFeBOOK

256
INTERPOLATION
L1(t) = (t −0)(t −2)
(1 −0)(1 −2) = −t(t −2) = −(t2 −2t),
L2(t) = (t −0)(t −1)
(2 −0)(2 −1) = 1
2t(t −1) = 1
2(t2 −t).
Observe that
x0L0(t) + x1L1(t) + x2L2(t)
= 1 · 1
2(t2 −3t + 2) + 2 · (−1)(t2 −2t) + 3 · 1
2(t2 −t)
=
 1
2t2 −2t + 3
2t2
!
+
 
−3
2t + 4t −3
2t
!
+
 1
2 · 2
!
= t + 1,
which is p2(t). As expected, the Lagrange form of the interpolating polynomial,
and the solution to the Vandermonde system are the same polynomial.
We remark that once pn(t) is found, we often wish to evaluate pn(t) for t ̸= tk
(k ∈Zn+1). Suppose that pn(t) is known in direct form; then this should be done
using Horner’s rule:
pn(t) = pn,0 + t[pn,1 + t[pn,2 + t[pn,3 + · · · + t[pn,n−1 + tpn,n]] · · ·]]]. (6.12)
For example, if n = 3, then
p3(t) = p3,0 + t[p3,1 + t[p3,2 + tp3,3]].
(6.13)
To evaluate this requires only 6 ﬂops (3 ﬂoating-point multiplications, and 3 ﬂoating-
point additions). Evaluating p3(t) = p3,0 + p3,1t + p3,2t2 + p3,3t3 directly needs 9
ﬂops (6 ﬂoating-point multiplications and 3 ﬂoating-point additions). Thus, Horner’s
rule is more efﬁcient from a computational standpoint. Using Horner’s rule may be
described as evaluating the polynomial from the inside out.
While Lagrange interpolation represents a simple way to solve (6.6) the form
of the solution is (6.9), and so requires more effort to evaluate at t ̸= tk than the
direct form, the latter of which is easily evaluated using Horner’s rule as noted in
the previous paragraph. Another objection to Lagrange interpolation is that if we
were to add elements to our data set, then the calculations for the current data set
would have to be discarded, and this would force us to begin again. It is possible to
overcome this inefﬁciency using Newton’s divided-difference method which leads
to the Newton form of the interpolating polynomial. This may be seen in Hildebrand
[3, Chapter 2]. We will consider this methodology in the next section.
When we evaluate pn(t) for t ̸= tk, but with t ∈[a, b] = [t0, tn], then this is
interpolation. If we wish to evaluate pn(t) for t < a or t > b, then this is referred
to as extrapolation. To do this is highly risky. Indeed, even if we constrain t to
satisfy t ∈[a, b], the results can be poor. We illustrate with a famous example of
TLFeBOOK

NEWTON INTERPOLATION
257
Runge’s which is described in Forsythe et al. [4, pp. 69–70]. It is very possible
that for f (t) ∈C[a, b], we have
lim
n→∞sup
t∈[a,b]
|f (t) −pn(t)| = ∞.
Runge’s speciﬁc example is for t ∈[−5, 5] with
f (t) =
1
1 + t2 ,
where he showed that for any t satisfying 3.64 ≤|t| < 5, then
lim
n→∞sup |f (t) −pn(t)| = ∞.
This divergence with respect to the Chebyshev norm [recall Section 5.7 (of
Chapter 5) for the deﬁnition of this term] is called Runge’s phenomenon. A
fairly detailed account of Runge’s example appears in Isaacson and Keller [5,
pp. 275–279].
We close this section with mention of the approximation error f (t) −pn(t).
Suppose that f (t) ∈Cn+1[a, b], and it is understood that f (n+1)(t) is continuous
as well; that is, f (t) has continuous derivatives f (k)(t) for k = 1, 2, . . . , n + 1. It
can be shown that for suitable ξ = ξ(t)
en(t) = f (t) −pn(t) =
1
(n + 1)!f (n+1)(ξ)
n
/
i=0
(t −ti),
(6.14)
where ξ ∈[a, b]. If we know f (n+1)(t), then clearly (6.14) may yield useful bounds
on |en(t)|. Of course, if we know nothing about f (t), then (6.14) is useless.
Equation (6.14) is useless if f (t) is not sufﬁciently differentiable. A derivation
of (6.14) is given by Hildebrand [3, pp. 81–83], but we omit it here. However, it
will follow from results to be considered in the next section.
6.3
NEWTON INTERPOLATION
Deﬁne
x[t0, t1] = x(t1) −x(t0)
t1 −t0
= x1 −x0
t1 −t0
.
(6.15)
This is called the ﬁrst divided difference of x(t) relative to t1 and t0. We see that
x[t0, t1] = x[t1, t0]. We may linearly interpolate x(t) for t ∈[t0, t1] according to
x(t) ≈x(t0) + t −t0
t1 −t0
[x(t1) −x(t0)] = x(t0) + (t −t0)x[t0, t1].
(6.16)
TLFeBOOK

258
INTERPOLATION
It is convenient to deﬁne p0(t) = x(t0) and p1(t) = x(t0) + (t −t0)x[t0, t1]. This
is notation consistent with Section 6.2. In fact, p1(t) agrees with the solution to
 1
t0
1
t1
  p1,0
p1,1

=
 x0
x1

,
(6.17)
as we expect.
Unless x(t) is truly linear the secant slope x[t0, t1] will depend on the abscissas
t0 and t1. If x(t) is a second-degree polynomial, then x[t1, t] will itself be a linear
function of t for a given t1. Consequently, the ratio
x[t0, t1, t2] = x[t1, t2] −x[t0, t1]
t2 −t0
(6.18)
will be independent of t0, t1, and t2. [This ratio is the second divided difference of
x(t) with respect to t0, t1, and t2.] To see that this claim is true consider x(t) =
a0 + a1t + a2t2, so then
x[t1, t] = a1 + a2(t + t1).
(6.19)
Therefore
x[t1, t2] −x[t0, t1] = x[t1, t2] −x[t1, t0] = a2(t2 −t0)
so that x[t0, t1, t2] = a2. We also note from (6.18) that
x[t0, t1, t2] =
1
t2 −t0
x2 −x1
t2 −t1
−x1 −x0
t1 −t0

,
(6.20a)
x[t1, t0, t2] =
1
t2 −t1
x2 −x0
t2 −t0
−x0 −x1
t0 −t1

,
(6.20b)
for which we may rewrite these in symmetric form
x[t0, t1, t2] =
x0
(t0 −t1)(t0 −t2) +
x1
(t1 −t0)(t1 −t2) +
x2
(t2 −t0)(t2 −t1)
= x[t1, t0, t2],
(6.21)
that is, x[t0, t1, t2] = x[t1, t0, t2]. We see from (6.16) that
x(t) −x(t0)
t −t0
≈x[t0, t1],
that is
x[t0, t] ≈x[t0, t1],
(6.22)
TLFeBOOK

NEWTON INTERPOLATION
259
and from this we consider the difference
x[t0, t] −x[t0, t1] = x[t0, t] −x[t1, t0] = (t −t1)x[t0, t1, t]
(6.23)
via (6.18), and the symmetry property x[t0, t1, t] = x[t1, t0, t]. Since x(t) is assumed
to be quadratic, we may replace the approximation of (6.16) with the identity
x(t) = x(t0) + (t −t0)x[t0, t]
= x(t0) + (t −t0)x[t0, t1]
#
$%
&
=p1(t)
+(t −t0)(t −t1)x[t0, t1, t]
(6.24)
via (6.23). The ﬁrst equality of (6.24) may be veriﬁed by direct calculation using
x(t) = a0 + a1t + a2t2, and (6.19). We see that if p1(t) approximates x(t), then
the error involved is [from (6.24)]
e(t) = x(t) −p1(t) = (t −t0)(t −t1)x[t0, t1, t].
(6.25)
These results generalize to x(t) a polynomial of higher degree.
We may recursively deﬁne the divided differences of orders 0, 1, . . . , k −1, k
according to
x[t0] = x(t0) = x0,
x[t0, t1] = x(t1) −x(t0)
t1 −t0
= x1 −x0
t1 −t0
,
x[t0, t1, t2] = x[t1, t2] −x[t0, t1]
t2 −t0
,
...
x[t0, . . . , tk] = x[t1, . . . , tk] −x[t0, . . . , tk−1]
tk −t0
.
(6.26)
We have established the symmetry x[t0, t1] = x[t1, t0] (case k = 1), and also
x[t0, t1, t2] = x[t1, t0, t2] (case k = 2). For k = 2, symmetry of this kind can also
be deduced from the symmetric form (6.21). It seems reasonable [from (6.21)] that
in general
x[t0, . . . , tk]
=
x0
(t0 −t1) · · · (t0 −tk) +
x1
(t1 −t0) · · · (t1 −tk) +· · ·+
xk
(tk −t0) · · · (tk −tk−1)
=
k

j=0
1
0k
i=0
i̸=j
(tj −ti)
xj.
(6.27)
TLFeBOOK

260
INTERPOLATION
It is convenient to deﬁne the coefﬁcient of xj as
αk
j =
1
0k
i=0
i̸=j
(tj −ti)
(6.28)
for j = 0, 1, . . . , k. Thus, x[t0, . . . , tk] = k
j=0 αk
jxj. We may prove (6.27) for-
mally by mathematical induction. We outline the detailed approach as follows.
Suppose that it is true for k = r; that is, assume that
x[t0, . . . , tr] =
r

j=0
1
0r
i=0
i̸=j
(tj −ti)xj,
(6.29)
and consider [from deﬁnition (6.26)]
x[t0, . . . , tr+1] =
1
tr+1 −t0
{x[t1, . . . , tr+1] −x[t0, . . . , tr]}.
(6.30)
For (6.30)
x[t1, . . . , tr+1] =
x1
(t1 −t2) · · · (t1 −tr+1)
+
x2
(t2 −t1) · · · (t2 −tr+1) + · · · +
xr+1
(tr+1 −t1) · · · (tr+1 −tr),
(6.31a)
and
x[t0, . . . , tr] =
x0
(t0 −t1) · · · (t0 −tr)
+
x1
(t1 −t0) · · · (t1 −tr) + · · · +
xr
(tr −t0) · · · (tr −tr−1).
(6.31b)
If we substitute (6.31) into (6.30), we see that, for example, for the terms involving
only x1
1
tr+1 −t0

x1
(t1 −t2)(t1 −t3) · · · (t1 −tr)(t1 −tr+1)
−
x1
(t1 −t0)(t1 −t2) · · · (t1 −tr−1)(t1 −tr)

=
x1
tr+1 −t0
1
(t1 −t2) · · · (t1 −tr)

1
t1 −tr+1
−
1
t1 −t0

=
x1
(t1 −t0)(t1 −t2) · · · (t1 −tr)(t1 −tr+1) = αr+1
1
x1.
The same holds for all remaining terms in xj for j = 0, 1, . . . , r + 1. Hence (6.27)
is valid by induction for all k ≥1. Because of (6.27), the ordering of the arguments
TLFeBOOK

NEWTON INTERPOLATION
261
in x[t0, . . . , tk] is irrelevant. Consequently, x[t0, . . . , tk] can be expressed as the
difference between two divided differences of order k −1, having any k −1 of
their k arguments in common, divided by the difference between those arguments
that are not in common. For example
x[t0, t1, t2, t3] = x[t1, t2, t3] −x[t0, t1, t2]
t3 −t0
= x[t0, t2, t3] −x[t1, t2, t3]
t0 −t1
.
What happens if two arguments of a divided difference become equal? The
situation is reminiscent of Corollary 5.1 (in Chapter 5). For example, suppose t1 =
t + ϵ; then
x[t, t1] = x(t1) −x(t)
t1 −t
= x(t + ϵ) −x(t)
ϵ
,
implying that
x[t, t] = lim
ϵ→0
x(t + ϵ) −x(t)
ϵ
= dx(t)
dt
so that
x[t, t] = dx(t)
dt
.
(6.32)
This assumes that x(t) is differentiable. By similar reasoning
d
dt x[t0, . . . , tk, t] = x[t0, . . . , tk, t, t]
(6.33)
(assuming that t0, . . . , tk are constants). Suppose u1, . . . , un are differentiable func-
tions of t, then it turns out that
d
dt x[t0, . . . , tk, u1, . . . , un] =
n

j=1
x[t0, . . . , tk, u1, . . . , un, uj]duj
dt .
(6.34)
Therefore, if u1 = u2 = · · · = un = t, then, from (6.34)
d
dt x[t0, . . . , tk, t, . . . , t
# $% &
n
] = nx[t0, . . . , tk, t, . . . , t
# $% &
n+1
].
(6.35)
Using (6.33), and (6.35) it may be shown that
dr
dtr x[t0, . . . , tk, t] = r! x[t0, . . . , tk, t, . . . , t
# $% &
r+1
].
(6.36)
Of course, this assumes that x(t) is sufﬁciently differentiable.
TLFeBOOK

262
INTERPOLATION
Equation (6.24) is just a special case of something more general. We note that
if x(t) is not a quadratic, then (6.24) is only an approximation for t ̸∈{t0, t1, t2},
and so
x(t) ≈x(t0) + (t −t0)x[t0, t1] + (t −t0)(t −t1)x[t0, t1, t2] = p2(t).
(6.37)
It is easy to verify by direct evaluation that p2(ti) = x(ti) for i ∈{0, 1, 2}. Equation
(6.37) is the second-degree interpolation formula, while (6.16) is the ﬁrst-degree
interpolation formula. We may generalize (6.24) [and hence (6.37)] to higher
degrees by using (6.26); that is
x(t) = x[t0] + (t −t0)x[t0, t],
x[t0, t] = x[t0, t1] + (t −t1)x[t0, t1, t],
x[t0, t1, t] = x[t0, t1, t2] + (t −t2)x[t0, t1, t2, t],
...
x[t0, . . . , tn−1, t] = x[t0, . . . , tn] + (t −tn)x[t0, . . . , tn, t],
(6.38)
where the last equation follows from
x[t0, . . . , tn, t] = x[t1, . . . , tn, t] −x[t0, . . . , tn]
t −t0
= x[t0, . . . , tn−1, t] −x[t0, . . . , tn]
t −tn
(6.39)
(the second equality follows by exchanging t0 and tn). If the second relation of
(6.38) is substituted into the ﬁrst, we obtain
x(t) = x[t0] + (t −t0)x[t0, t1] + (t −t0)(t −t1)x[t0, t1, t],
(6.40)
which is just (6.24) again. If we substitute the third relation of (6.38) into (6.40),
we obtain
x(t) = x[t0] + (t −t0)x[t0, t1] + (t −t0)(t −t1)x[t0, t1, t2]
+ (t −t0)(t −t1)(t −t2)x[t0, t1, t2, t].
(6.41)
This leads to the third-degree interpolation formula
p3(t) = x[t0] + (t −t0)x[t0, t1] + (t −t0)(t −t1)x[t0, t1, t2]
+ (t −t0)(t −t1)(t −t2)x[t0, t1, t2, t3].
TLFeBOOK

NEWTON INTERPOLATION
263
Continuing in this fashion, we obtain
x(t) = x[t0] + (t −t0)x[t0, t1] + (t −t0)(t −t1)x[t0, t1, t2]
+ · · · + (t −t0)(t −t1) · · · (t −tn−1)x[t0, t1 . . . , tn] + e(t),
(6.42a)
where
e(t) = (t −t0)(t −t1) · · · (t −tn)x[t0, t1, . . . , tn, t],
(6.42b)
and we deﬁne
pn(t) = x[t0] + (t −t0)x[t0, t1]
+ (t −t0)(t −t1)x[t0, t1, t2] + · · · + (t −t0) · · · (t −tn−1)x[t0, . . . , tn],
(6.42c)
which is the nth-degree interpolating formula, and is clearly a polynomial of degree
n. So e(t) is the error involved in interpolating x(t) using polynomial pn(t). It is
the case that pn(tk) = x(tk) for k = 0, 1, . . . , n. Equation (6.42a) is the Newton
interpolating formula with divided differences. If x(t) is a polynomial of degree n
(or less), then e(t) = 0 (all t). This is more formally justiﬁed later.
Example 6.2
Consider e(t) for n = 2. This requires [via (6.27)]
x[t0, t1, t2, t] =
x0
(t0 −t1)(t0 −t2)(t0 −t) +
x1
(t1 −t0)(t1 −t2)(t1 −t)
+
x2
(t2 −t0)(t2 −t1)(t2 −t) +
x(t)
(t −t0)(t −t1)(t −t2)
Thus
e(t) = (t −t0)(t −t1)(t −t2)x[t0, t1, t2, t]
= −(t −t1)(t −t2)x0
(t0 −t1)(t0 −t2) −(t −t0)(t −t2)x1
(t1 −t0)(t1 −t2) −(t −t0)(t −t1)x2
(t2 −t0)(t2 −t1)
#
$%
&
=−p2(t)
+x(t).
(6.43)
The form of p2(t) in (6.43) is that of p2(t) in Example 6.1:
p2(t) = x0L0(t) + x1L1(t) + x2L2(t).
If x(t) = a0 + a1t + a2t2, then clearly e(t) = 0 for all t. In fact, p2,k = ak.
Example 6.3
Suppose that we wish to interpolate x(t) = et given that tk = kh
with h = 0.1, and k = 0, 1, 2, 3. We are told that
x0 = 1.000000,
x1 = 1.105171,
x2 = 1.221403,
x3 = 1.349859.
TLFeBOOK

264
INTERPOLATION
We will consider n = 2 (i.e., quadratic interpolation). The task is aided if we
construct the divided-difference table:
t0 = 0
x(t0) = 1.000000
x[t0, t1] = 1.05171
t1 = 0.1
x(t1) = 1.105171
x[t0, t1, t2] = 0.55305
x[t1, t2] = 1.16232
t2 = 0.2
x(t2) = 1.221403
x[t1, t2, t3] = 0.61120
x[t2, t3] = 1.28456
t3 = 0.3
x(t3) = 1.349859
For t ∈[0, 0.2] we consider [from (6.37)]
x(t) ≈x(t0) + (t −t0)x[t0, t1] + (t −t0)(t −t1)x[t0, t1, t2],
which for the data we are given becomes
x(t) ≈1.000000 + 1.051710t + 0.55305t(t −0.1) = pa
2(t).
(6.44a)
If t = 0.11, then pa
2(0.11) = 1.116296, while it turns out that x(0.11) = 1.116278,
so e(0.11) = x(0.11) −pa
2(0.11) = −0.000018. For t ∈[0.1, 0.3] we might con-
sider
x(t) ≈x(t1) + (t −t1)x[t1, t2] + (t −t1)(t −t2)x[t1, t2, t3],
which for the given data becomes
x(t) ≈1.105171 + 1.16232(t −0.1) + 0.61120(t −0.1)(t −0.2) = pb
2(t).
(6.44b)
We observe that to calculate pb
2(t) does not require discarding all the results needed
to determine pa
2(t). We do not need to begin again as with Lagrangian interpolation
since, for example
x[t0, t1, t2] = x[t1, t2] −x[t0, t1]
t2 −t0
,
x[t1, t2, t3] = x[t2, t3] −x[t1, t2]
t3 −t1
,
and both of these divided differences require x[t1, t2]. If we wanted to use cubic
interpolation then the table is very easily augmented to include x[t0, t1, t2, t3].
Thus, updating the interpolating polynomial due to the addition of more data to the
table, or of increasing n, may proceed more efﬁciently than if we were to employ
Lagrange interpolation.
We also observe that both pa
2(t) and pb
2(t) may be used to interpolate x(t) for
t ∈[0.1, 0.2]. Which polynomial should be chosen? Ideally, we would select the
one for which e(t) is the smallest. Practically, this means seeking bounds for |e(t)|
and choosing the interpolating polynomial with the best error bound.
TLFeBOOK

NEWTON INTERPOLATION
265
We have shown that if x(t) is approximated by pn(t), then the error has the form
e(t) = π(t)x[t0, . . . , tn, t]
(6.45a)
[recall (6.42b)], where
π(t) = (t −t0)(t −t1) · · · (t −tn),
(6.45b)
which is an degree n + 1 polynomial. The form of the error in (6.45a) can be useful
in analyzing the accuracy of numerical integration and numerical differentiation
procedures, but another form of the error can be found.
We note that e(t) = x(t) −pn(t) [recall (6.42)], so both x(t) −pn(t) and π(t)
vanish at t0, t1, . . . , tn. Consider the linear combination
X(t) = x(t) −pn(t) −κπ(t).
(6.46)
We wish to select κ so that X(t) = 0, where t ̸= tk for k ∈Zn+1. Such a κ
exists because π(t) vanishes only at t0, t1, . . . , tn. Let a = min{t0, . . . , tn, t}, b =
max{t0, . . . , tn, t}, and deﬁne the interval I = [a, b]. By construction, X(t) van-
ishes at least n + 2 times on I. Rolle’s theorem from calculus states that X(k)(t) =
dkX(t)/dtk vanishes at least n + 2 −k times inside I. Speciﬁcally, X(n+1)(t) van-
ishes at least once inside I. Let this point be called ξ. Therefore, from (6.46), we
obtain
x(n+1)(ξ) −p(n+1)
n
(ξ) −κπ(n+1)(ξ) = 0.
(6.47)
But pn(t) is a polynomial of degree n, so p(n+1)(t) = 0 for all t ∈I. From (6.45b)
π(n+1)(t) = (n + 1)!, so ﬁnally (6.47) reduces to
κ =
1
(n + 1)!x(n+1)(ξ).
(6.48)
From X(t) = 0 in (6.46), and using (6.48), we ﬁnd that
e(t) = x(t) −pn(t) =
1
(n + 1)!x(n+1)(ξ)π(t)
(6.49)
for some ξ ∈I. If we were to let t = tk for any k ∈Zn+1, then both sides of (6.49)
vanish even in this previously excluded case. This allows us to write
e(t) =
1
(n + 1)!x(n+1)(ξ(t))π(t)
(6.50)
for
some
ξ = ξ(t) ∈I
(I = [a, b]
with
a = min{t0, . . . , tn, t},
and
b =
max{t0, . . . , tn, t}). If x(n+1)(t) is continuous for t ∈I, then x(n+1)(t) is bounded
on I, so there is an Mn+1 > 0 such that
x(n+1)(ξ) ≤Mn+1,
(6.51)
TLFeBOOK

266
INTERPOLATION
and hence
|e(t)| ≤
Mn+1
(n + 1)!|π(t)|
(6.52)
for all t ∈I. It is to be emphasized that for this to hold x(n+1)(t) must exist,
and we normally require it to be continuous, too. Equations (6.50) and (6.45a) are
equivalent, and thus
π(t)x[t0, . . . , tn, t] =
1
(n + 1)!x(n+1)(ξ)π(t),
or in other words
x[t0, . . . , tn, t] =
1
(n + 1)!x(n+1)(ξ)
(6.53)
for some ξ ∈I, whenever x(n+1)(t) exists in I. In particular, if x(t) is a polynomial
of degree n or less, then (6.53) yields x[t0, . . . tn, t] = 0, hence e(t) = 0 for all t
(a fact mentioned earlier).
Example 6.4
Recall Example 6.3. We considered x(t) = et with t0 = 0, t1 =
0.1, t2 = 0.2, and we found that
pa
2(t) = 1.000000 + 1.051710t + 0.553050t(t −0.1)
for which pa
2(0.11) = 1.116296, and x(0.11) = 1.116278. The exact error is
e(0.11) = x(0.11) −pa
2(0.11) = −0.000018.
We will compare this with the bound we obtain from (6.52). Since n = 2, x(3)(t) =
et, and I = [0, 0.2], so
M3 = e0.2 = 1.221403
(which was given data), and π(t) = t(t −0.1)(t −0.2), so |π(0.11)| = 9.9 × 10−5.
Consequently, from (6.52), we have
|e(0.11)| ≤1.221403
3!
9.9 × 10−5 = 0.000020.
The actual error certainly agrees with this bound.
We end this section by observing that (6.14) immediately follows from (6.50).
6.4
HERMITE INTERPOLATION
In the previous sections polynomial interpolation methods matched the polynomial
only to the value of the function f (x) at various points x = xk ∈[a, b] ⊂R. In
TLFeBOOK

HERMITE INTERPOLATION
267
this section we consider Hermite interpolation where the interpolating polynomial
also matches the ﬁrst derivatives f (1)(x) at x = xk. This interpolation technique
is important in the development of higher order numerical integration methods as
will be seen in Chapter 9.
The following theorem is the main result, and is essentially Theorem 3.9 from
Burden and Faires [6].
Theorem 6.1: Hermite Interpolation
Suppose that f (x) ∈C1[a, b], and
that x0, x1, . . . , xn ∈[a, b] are distinct, then the unique polynomial of degree (at
most) 2n + 1 denoted by p2n+1(x), and such that
p2n+1(xj) = f (xj), p(1)
2n+1(xj) = f (1)(xj)
(6.54)
(j ∈Zn+1) is given by
p2n+1(x) =
n

k=0
hk(x)f (xk) +
n

k=0
ˆhk(x)f (1)(xk),
(6.55)
where
hk(x) = [1 −2L(1)
k (xk)(x −xk)][Lk(x)]2,
(6.56)
and
ˆhk(x) = (x −xk)[Lk(x)]2
(6.57)
such that [recall (6.11)]
Lk(x) =
n
/
i=0
i̸=k
x −xi
xk −xi
.
(6.58)
Proof
To show (6.54) for x0, x1, . . . , xn, we require that hk(x), and ˆhk(x) in
(6.56) and (6.57) satisfy the conditions
hk(xj) = δj−k,
h(1)
k (xj) = 0
(6.59a)
and
ˆhk(xj) = 0,
ˆh(1)
k (xj) = δj−k.
(6.59b)
Assuming that these conditions hold, we may conﬁrm (6.54) as follows. Via (6.55)
p2n+1(xj) =
n

k=0
hk(xj)f (xk) +
n

k=0
ˆhk(xj)f (1)(xk),
TLFeBOOK

268
INTERPOLATION
and via (6.59), this becomes
p2n+1(xj) =
n

k=0
δj−kf (xk) +
n

k=0
0 · f (1)(xk) = f (xj).
This conﬁrms the ﬁrst case in (6.54). Similarly, via (6.59)
p(1)
2n+1(xj) =
n

k=0
h(1)
k (xj)f (xk) +
n

k=0
ˆh(1)
k (xj)f (1)(xk)
becomes
p(1)
2n+1(xj) =
n

k=0
0 · f (xk) +
n

k=0
δj−kf (1)(xk) = f (1)(xj),
which conﬁrms the second case in (6.54).
Now we will conﬁrm that hk(x), and ˆhk(x) as deﬁned in (6.56), and (6.57)
satisfy the requirements given in (6.59). The conditions in (6.59b) imply that ˆhk(x)
must have a double root at x = xj for j ̸= k, and a single root at x = xk. A
polynomial of degree at most 2n + 1 that satisﬁes these requirements, and such
that ˆh(1)
k (xk) = 1 is
ˆhk(x) = (x −xk)
(x −x0)2 · · · (x −xk−1)2 · (x −xk+1)2 · · · (x −xn)2
(xk −x0)2 · · · (xk −xk−1)2 · (xk −xk+1)2 · · · (xk −xn)2
= (x −xk)L2
k(x).
Certainly ˆhk(xk) = 0. Moreover, ˆh(1)
k (x) = L2
k(x) + 2(x −xk)Lk(x)L(1)
k (x) so
ˆh(1)
k (xk) = L2
k(xk) = 1 [via (6.8)]. These verify (6.59b).
Now we consider (6.59a). These imply xj for j ̸= k is a double root of hk(x),
and we may consider (for suitable a and b to be found below)
hk(x) =
1
0n
i=0
i̸=k
(xi −xk)2 (x −x0)2 · · · (x −xk−1)2(x −xk+1)2
· · · (x −xn)2(ax + b)
which has degree at most 2n + 1. More concisely, this polynomial is
hk(x) = L2
k(x)(ax + b).
From (6.59a) we require
1 = hk(xk) = L2
k(xk)(axk + b) = axk + b,
(6.60)
TLFeBOOK

SPLINE INTERPOLATION
269
Also, h(1)
k (x) = aL2
k(x) + 2Lk(x)L(1)
k (x)(ax + b), and we also need [via (6.59a)]
h(1)
k (xk) = aL2
k(xk) + 2Lk(xk)L(1)
k (xk)(axk + b) = 0,
but again since Lk(xk) = 1, this expression reduces to
a + 2L(1)
k (xk) = 0,
where we have used (6.60). Hence
a = −2L(1)
k (xk),
b = 1 + 2L(1)
k (xk)xk.
Therefore, we ﬁnally have
hk(x) = [1 −2L(1)
k (xk)(x −xk)]L2
k(x)
which is (6.56). Since Lk(xj) = 0 for j ̸= k it is clear that hk(xj) = 0 for j ̸= k.
It is also easy to see that h(1)
k (xj) = 0 for all j ̸= k too. Thus, (6.59a) is conﬁrmed
for hk(x) as deﬁned in (6.56).
An error bound for Hermite interpolation is provided by the expression
f (x) = p2n+1(x) +
1
(2n + 2)!
n
/
i=0
(x −xi)2f (2n+2)(ξ)
(6.61)
for some ξ ∈(a, b), where f (x) ∈C2n+2[a, b]. We shall not derive (6.61) except
to note that the approach is similar to the derivation of (6.14). Equation (6.14) was
really derived in Section 6.3.
In its present form Hermite interpolation requires working with Lagrange poly-
nomials, and their derivatives. As noted by Burden and Faires [6], this is rather
tedious (i.e., not computationally efﬁcient). A procedure involving Newton inter-
polation (recall Section 6.3) may be employed to reduce the labor that would
otherwise be involved in Hermite interpolation. We do not consider this approach,
but instead refer the reader to Burden and Faires [6] for the details. We use Hermite
interpolation in Chapter 9 to develop numerical integration methods, and efﬁcient
Hermite interpolation is not needed for this purpose.
6.5
SPLINE INTERPOLATION
Spline (spliced line) interpolation is a particular kind of piecewise polynomial
interpolation. We may wish, for example, to approximate f (x) for x ∈[a, b] ⊂
R when given the sample points {(xk, f (xk))|k ∈Zn+1} by ﬁtting straight-line
segments in between (xk, f (xk)), and (xk+1, f (xk+1)) for k = 0, 1, . . . , n −1. An
TLFeBOOK

270
INTERPOLATION
0
0.5
1
1.5
2
2.5
3
3.5
4
−6
−4
−2
0
2
4
6
x
 y 
f(x)
Piecewise linear interpolant
Figure 6.1
The cubic polynomial f (x) = (x −1)(x −2)(x −3), and its piecewise linear
interpolant (dashed line) at the nodes xk = x0 + hk for which x0 = 0, and h = 1
2, where
k = 0, 1, . . . , n with n = 8.
example of this appears in Fig. 6.1. This has a number of disadvantages. Although
f (x) may be differentiable at x = xk, the piecewise linear approximation will not
be (in general). Also, the graph of the interpolant has visually displeasing “kinks”
in it. If interpolation is for a computer graphics application, or to deﬁne the physical
surface of an automobile body or airplane, then such kinks are seldom acceptable.
Splines are a means to deal with this problem. It is also worth noting that, more
recently, splines have found a role in the design of wavelet functions [7, 8], which
were brieﬂy mentioned in Chapter 1.
The following deﬁnition is taken from Epperson [9], and our exposition of spline
functions in this section follows that in [9] fairly closely. As always f (i)(x) =
dif (x)/dxi (i.e., this is the notation for the ith derivative of f (x)).
Deﬁnition 6.1: Spline
Suppose that we are given {(xk, f (xk))|k ∈Zn+1}.
The piecewise polynomial function pm(x) is called a spline if
(S1) pm(xk) = f (xk) for all k ∈Zn+1 (interpolation).
(S2) limx→x−
k p(i)
m (x) = limx→x+
k p(i)
m (x) for all i ∈ZN+1 (smoothness).
TLFeBOOK

SPLINE INTERPOLATION
271
(S3) pm(x) is a polynomial of degree no larger than m on every subinterval
[xk, xk+1] for k ∈Zn (interval of deﬁnition).
We say that m is the degree of approximation and N is the degree of smoothness
of the spline pm(x).
There is a relationship between m and N. As there are n subintervals [xk, xk+1],
and each of these is the domain of deﬁnition of a degree m polynomial, we see
that there are Df = n(m + 1) degrees of freedom. Each polynomial is speciﬁed by
m + 1 coefﬁcients, and there are n of these polynomials; hence Df is the number
of parameters to solve for in total. From Deﬁnition 6.1 there are n + 1 interpo-
lation conditions [axiom (S1)]. And there are n −1 junction points x1, . . . xn−1
(sometimes also called knots), with N + 1 continuity conditions being imposed on
each of them [axiom (S2)]. As a result, there are Dc = (n + 1) + (n −1)(N + 1)
constraints. Consider
Df −Dc = n(m + 1) −[(n + 1) + (n −1)(N + 1)] = n(m −N −1) + N.
(6.62)
It is a common practice to enforce the condition m −N −1 = 0; that is, we let
m = N + 1.
(6.63)
This relates the degree of approximation to the degree of smoothness in a simple
manner. Below we will focus our attention exclusively on the special case of the
cubic splines for which m = 3. From (6.63) we must therefore have N = 2. With
condition (6.63), then, from (6.62) we have
Df −Dc = N.
(6.64)
As a result, it is necessary to impose N further constraints on the design problem.
How this is done is considered in detail below. Since we will look only at m = 3
with N = 2, we must impose two additional constraints. This will be done by
imposing one constraint at each endpoint of the interval [a, b]. There is more than
one way to do this as will be seen later.
From Deﬁnition 6.1 it superﬁcially appears that we need to compute n different
polynomials. However, it is possible to recast our problem in terms of B-splines.
A B-spline acts as a prototype in the formation of a basis set of splines.
Aside from the assumption that m = 3, N = 2, let us further assume that
a = x0 < x1 < · · · < xn−1 < xn = b
(6.65)
with xk+1 −xk = h for k = 0, 1, . . . , n −1. This is the uniform grid assumption.
We will also need to account for boundary conditions, and this requires us to
introduce the additional grid points
x−3 = a −3h,
x−2 = a −2h,
x−1 = a −h
(6.66)
TLFeBOOK

272
INTERPOLATION
and
xn+3 = b + 3h,
xn+2 = b + 2h,
xn+1 = b + h.
(6.67)
Our prototype cubic B-spline will be the function
S(x) =



0,
x ≤−2
(x + 2)3,
−2 ≤x ≤−1
1 + 3(x + 1) + 3(x + 1)2 −3(x + 1)3,
−1 ≤x ≤0
1 + 3(1 −x) + 3(1 −x)2 −3(1 −x)3,
0 ≤x ≤1
(2 −x)3,
1 ≤x ≤2
0,
x ≥2
.
(6.68)
This function has nodes at x ∈{−2, −1, 0, 1, 2}. A plot of it appears in Fig. 6.2, and
we see that it has a bell shape similar to the Gaussian pulse we saw in Chapter 3.
We may verify that S(x) satisﬁes Deﬁnition 6.1 as follows. Plainly, it is piecewise
cubic (i.e., m = 3), so axiom (S3) holds. The ﬁrst and second derivatives are,
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
0
0.5
1
1.5
2
2.5
3
3.5
4
x
S(x)
Figure 6.2
A plot of the cubic B-spline deﬁned in Eq. (6.68).
TLFeBOOK

SPLINE INTERPOLATION
273
respectively
S(1)(x) =



0,
x ≤−2
3(x + 2)2,
−2 ≤x ≤−1
3 + 6(x + 1) −9(x + 1)2,
−1 ≤x ≤0
−3 −6(1 −x) + 9(1 −x)2,
0 ≤x ≤1
−3(2 −x)2,
1 ≤x ≤2
0,
x ≥2
,
(6.69)
and
S(2)(x) =



0,
x ≤−2
6(x + 2),
−2 ≤x ≤−1
6 −18(x + 1),
−1 ≤x ≤0
6 −18(1 −x),
0 ≤x ≤1
6(2 −x),
1 ≤x ≤2
0,
x ≥2
.
(6.70)
We note that from (6.68)–(6.70) that
S(0) = 4,
S(±1) = 1,
S(±2) = 0,
(6.71a)
S(1)(0) = 0,
S(1)(±1) = ∓3,
S(1)(±2) = 0,
(6.71b)
S(2)(0) = −12,
S(2)(±1) = 6,
S(2)(±2) = 0.
(6.71c)
So it is apparent that for i = 0, 1, 2 we have
lim
x→x−
k
S(i)(x) = lim
x→x+
k
S(i)(x)
for all xk ∈{−2, −1, 0, 1, 2}. Thus, the smoothness axiom (S2) is met for N = 2.
Now we need to consider how we may employ S(x) to approximate any f (x)
for x ∈[a, b] ⊂R when working with the grid speciﬁed in (6.65)–(6.67). To this
end we deﬁne
Si(x) = S
 x −xi
h
!
(6.72)
for
i = −1, 0, 1, . . ., n, n + 1.
Since
S(1)
i
(x) = 1
hS(1) ) x−xi
h
*
,
S(2)
i
(x) =
1
h2 S(2) ) x−xi
h
*
from (6.71), we have (e.g., xi±1 = xi ± h.)
Si(xi) = S(0) = 4,
Si(xi±1) = S(±1) = 1,
Si(xi±2) = S(±2) = 0,
(6.73a)
TLFeBOOK

274
INTERPOLATION
S(1)
i
(xi) = 0,
S(1)
i
(xi±1) = ∓3
h,
S(1)
i
(xi±2) = 0,
(6.73b)
S(2)
i
(xi) = −12
h2 ,
S(2)
i
(xi±1) = 6
h2 ,
S(2)
i
(xi±2) = 0.
(6.73c)
We construct a cubic B-spline interpolant for any f (x) by deﬁning spline p3(x)
to be a linear combination of Si(x) for i = −1, 0, 1, . . ., n, n + 1, i.e., for suitable
coefﬁcients ai we have
p3(x) =
n+1

i=−1
aiSi(x).
(6.74)
The series coefﬁcients ai are determined in order to satisfy axiom (S1) of Deﬁnition
6.1; thus, we have
f (xk) =
n+1

i=−1
aiSi(xk)
(6.75)
for k ∈Zn+1.
If we apply (6.73a) to (6.75), we observe that
f (x0)
=
n+1

i=−1
aiSi(x0)
=
a−1S−1(x0) + a0S0(x0) + a1S1(x0)
f (x1)
=
n+1

i=−1
aiSi(x1)
=
a0S0(x1) + a1S1(x1) + a2S2(x1)
...
f (xn)
=
n+1

i=−1
aiSi(xn)
=
an−1Sn−1(xn) + anSn(xn) + an+1Sn+1(xn)
.
(6.76)
For example, for f (x0) in (6.76), we note that Sk(x0) = 0 for k ≥2 since (xk =
x0 + kh)
Sk(x0) = S
 x0 −xk
h
!
= S
 x0 −(kh + x0)
h
!
= S(−k) = 0.
More generally
f (xk) = ak−1Sk−1(xk) + akSk(xk) + ak+1Sk+1(xk)
(6.77)
for which k ∈Zn+1. Again via (6.73a) we see that
Sk−1(xk) = S
 (x0 + kh) −(x0 + (k −1)h)
h
!
= S(1) = 1,
TLFeBOOK

SPLINE INTERPOLATION
275
Sk(xk) = S
 (x0 + kh) −(x0 + kh)
h
!
= S(0) = 4,
Sk+1(xk) = S
 (x0 + kh) −(x0 + (k + 1)h)
h
!
= S(−1) = 1.
Thus, (6.77) becomes
ak−1 + 4ak + ak+1 = f (xk)
(6.78)
again for k = 0, 1, . . . , n. In matrix form we have


1
4
1
0
· · ·
0
0
0
0
1
4
1
· · ·
0
0
0
...
...
...
...
...
...
...
0
0
0
0
· · ·
4
1
0
0
0
0
0
· · ·
1
4
1


#
$%
&
=A


a−1
a0
...
an
an+1


#
$%
&
=a
=


f (x0)
f (x1)
...
f (xn−1)
f (xn)


#
$%
&
=f
.
(6.79)
We note that A ∈R(n+1)×(n+3), so there are n + 1 equations in n + 3 unknowns
(a ∈Rn+3). The tridiagonal linear system in (6.79) cannot be solved in its present
form as we need two additional constraints. This is to be expected from our earlier
discussion surrounding (6.62)–(6.64). Recall that we have chosen m = 3, so N = 2,
and so, via (6.64), we have Df −Dc = N = 2, implying the need for two more
constraints. There are two common approaches to obtaining these constraints:
1. We enforce p(2)
3 (x0) = p(2)
3 (xn) = 0 (natural spline).
2. We enforce p(1)
3 (x0) = f (1)(x0), and p(1)
3 (xn) = f (1)(xn) (complete spline,
or clamped spline).
Of these two choices, the natural spline is a bit easier to work with but can lead
to larger approximation errors near the interval endpoints x0, and xn compared to
working with the complete spline. The complete spline can avoid the apparent need
to know the derivatives f (1)(x0), and f (1)(xn) by using numerical approximations
to the derivative (see Section 9.6).
We will ﬁrst consider the case of the natural spline. From (6.74), we obtain
p3(x0) = a−1S−1(x0) + a0S0(x0) + a1S1(x0),
so that
p(2)
3 (x0) = a−1S(2)
−1(x0) + a0S(2)
0 (x0) + a1S(2)
1 (x0).
(6.80)
TLFeBOOK

276
INTERPOLATION
Since S(2)
i
(x0) = 1
h2 S(2) ) x0−xi
h
*
, we have
S(2)
−1(x0) = 1
h2 S(2)
 x0 −(x0 −h)
h
!
= 1
h2 S(2)(1) = 6
h2 ,
(6.81a)
S(2)
0 (x0) = 1
h2 S(2)
 x0 −x0
h
!
= 1
h2 S(2)(0) = −12
h2 ,
(6.81b)
S(2)
1 (x0) = 1
h2 S(2)
 x0 −(x0 + h)
h
!
= 1
h2 S(2)(−1) = 6
h2 ,
(6.81c)
where (6.71c) was used. Thus, (6.80) reduces to
h−2[6a−1 −12a0 + 6a1] = 0
or
a−1 = 2a0 −a1.
(6.82)
Similarly
p3(xn) = an−1Sn−1(xn) + anSn(xn) + an+1Sn+1(xn),
so that
p(2)
3 (xn) = an−1S(2)
n−1(xn) + anS(2)
n (xn) + an+1S(2)
n+1(xn).
(6.83)
Since S(2)
i
(xn) = 1
h2 S(2) ) xn−xi
h
*
, we have
S(2)
n−1(xn) = 1
h2 S(2)
 (x0 + nh) −(x0 + (n −1)h)
h
!
= 1
h2 S(2)(1) = 6
h2 ,
(6.84a)
S(2)
n (xn) = 1
h2 S(2)
 (x0 + nh) −(x0 + nh)
h
!
= 1
h2 S(2)(0) = −12
h2 ,
(6.84b)
S(2)
n+1(xn) = 1
h2 S(2)
 (x0 + nh) −(x0 + (n + 1)h)
h
!
= 1
h2 S(2)(−1) = 6
h2 ,
(6.84c)
where (6.71c) was again employed. Thus, (6.83) reduces to
h−2[6an−1 −12an + 6an+1] = 0
or
an+1 = 2an −an−1.
(6.85)
Now since [from (6.79)]
a−1 + 4a0 + a1 = f (x0)
TLFeBOOK

SPLINE INTERPOLATION
277
using (6.82) we have
6a0 = f (x0),
(6.86a)
and similarly
an−1 + 4an + an+1 = f (xn),
so via (6.85) we have
6an = f (xn).
(6.86b)
Using (6.86) we may rewrite (6.79) as the linear system


4
1
0
· · ·
0
0
0
1
4
1
· · ·
0
0
0
...
...
...
...
...
...
0
0
0
· · ·
1
4
1
0
0
0
· · ·
0
1
4


#
$%
&
=A


a1
a2
...
an−2
an−1


#
$%
&
=a
=


f (x1) −1
6f (x0)
f (x2)
...
f (xn−2)
f (xn−1) −1
6f (xn)


#
$%
&
=f
.
(6.87)
Here we have A ∈R(n−1)×(n−1), so now the tridiagonal system Aa = f has a
unique solution, assuming that A−1 exists. The existence of A−1 will be justiﬁed
below.
Now we will consider the case of the complete spline. In the case of
p(1)
3 (x0) = a−1S(1)
−1(x0) + a0S(1)
0 (x0) + a1S(1)
1 (x0) = f (1)(x0),
(6.88)
since S(1)
i
(x0) = 1
hS(1) ) x0−xi
h
*
, we have, using (6.71b)
S(1)
−1(x0) = 1
hS(1)
 x0 −(x0 −h)
h
!
= 1
hS(1)(1) = −3
h,
S(1)
0 (x0) = 1
hS(1)
 x0 −x0
h
!
= 1
hS(1)(0) = 0,
S(1)
1 (x0) = 1
hS(1)
 x0 −(x0 + h)
h
!
= 1
hS(1)(−1) = 3
h,
so (6.88) becomes
p(1)
3 (x0) = 3h−1[−a−1 + a1] = f (1)(x0).
(6.89)
Similarly
p(1)
3 (xn) = an−1S(1)
n−1(xn) + anS(1)
n (xn) + an+1S(1)
n+1(xn) = f (1)(xn)
(6.90)
TLFeBOOK

278
INTERPOLATION
reduces to
p(1)
3 (xn) = 3h−1[−an−1 + an+1] = f (1)(xn).
(6.91)
From (6.89) and (6.91), we obtain
a−1 = a1 −1
3hf (1)(x0),
an+1 = an−1 + 1
3hf (1)(xn).
(6.92)
If we substitute (6.92) into (6.79), we obtain


4
2
0
· · ·
0
0
0
1
4
1
· · ·
0
0
0
...
...
...
...
...
...
0
0
0
· · ·
1
4
1
0
0
0
· · ·
0
2
4


#
$%
&
=A


a0
a1
...
an−1
an


#
$%
&
=a
=


f (x0) + 1
3hf (1)(x0)
f (x1)
...
f (xn−1)
f (xn) −1
3hf (1)(xn)


#
$%
&
=f
. (6.93)
Now we have A ∈R(n+1)×(n+1), and we see that Aa = f of (6.93) will have a
unique solution provided that A−1 exists.
Matrix A is tridiagonal, and so is quite sparse (i.e., it has many zero-valued
entries) because of the “locality” of the function S(x). This locality makes it pos-
sible to evaluate p3(x) in (6.74) efﬁciently. If we know that x ∈[xk, xk+1], then
p3(x) = ak−1Sk−1(x) + akSk(x) + ak+1Sk+1(x) + ak+2Sk+2(x).
(6.94)
We write supp g(x) = [a, b] to represent the fact that g(x) = 0 for all x < a, and
x > b, while g(x) might not be zero-valued for x ∈[a, b]. From (6.68) we may
therefore say that supp S(x) = [−2, 2], and so from (6.72), supp Si(x) = [xi −
2h, xi + 2h], so we see that Si(x) is not necessarily zero-valued for x ∈[xi −
2h, xi + 2h]. From this, (6.94), and the fact that xi = x0 + ih, we obtain
supp p3(x) = ∪k+2
i=k−1supp Si(x) = [x0 + (k −3)h, x0 + (k + 1)h] ∪
· · · ∪[x0 + kh, x0 + (k + 4)h]
(6.95)
)
if Ai are sets, then ∪m
i=n Ai = An ∪· · · ∪Am
*
,
which
“covers”
the
interval
[xk, xk+1] = [x0 + kh, x0 + (k + 1)h]. Because the sampling grid (6.65)–(6.67),
is uniform it is easy to establish that (via x ≥x0 + kh)
k =
>x −x0
h
?
,
(6.96)
where ⌊x⌋= the largest integer that is ≤x ∈R. Since supp S(x) is an interval of
ﬁnite length, we say that S(x) is compactly supported. This locality of support is
also a part of what makes splines useful in wavelet constructions.
TLFeBOOK

SPLINE INTERPOLATION
279
We now consider the solution of Aa = f in either of (6.87) or (6.93). Obviously,
Gaussian elimination is a possible method, but this is not efﬁcient since Gaussian
elimination is a general procedure that does not take advantage of any matrix
structure. The sparse tridiagonal structure of A can be exploited for a more efﬁcient
solution. We will consider a general method of tridiagonal linear system solution
here, but it is one that is based on modifying the general Gaussian elimination
method. We begin by deﬁning


a00
a01
0
· · ·
0
0
a10
a11
a12
· · ·
0
0
0
a21
a22
· · ·
0
0
...
...
...
...
...
0
0
0
· · ·
an−1,n−1
an−1,n
0
0
0
· · ·
an,n−1
an,n


#
$%
&
=A


x0
x1
x2
...
xn−1
xn


#
$%
&
=x
=


f0
f1
f2
...
fn−1
fn


#
$%
&
=f
. (6.97)
Clearly, A ∈R(n+1)×(n+1). There is some terminology associated with tridiagonal
matrices. The main diagonal consists of the elements ai,i, while the diagonal above
this consists of the elements ai,i+1, and is often called the superdiagonal. Similarly,
the diagonal below the main diagonal consists of the elements ai+1,i, and is often
called the subdiagonal.
Our approach to solving Ax = f in (6.97) will be to apply Gaussian elimination
to the augmented linear system [A|f ] in order to reduce A to upper triangular form,
and then backward substitution will be used to solve for x. This is much the same
procedure as considered in Section 4.5, except that the tridiagonal structure of A
makes matters easier. To see this, consider the special case of n = 3 as an example,
that is (A0 = A, f 0 = f )
[A0|f 0] =


a0
00
a0
01
0
0
|
f 0
0
a0
10
a0
11
a0
12
0
|
f 0
1
0
a0
21
a0
22
a0
23
|
f 0
2
0
0
a0
32
a0
33
|
f 0
3


.
(6.98)
We may apply elementary row operations to eliminate a0
10 in (6.98). Thus, (6.98)
becomes
[A1|f 1] =


a0
00
a0
01
0
0
|
f 0
0
0
a0
11 −a0
10
a0
00 a0
01
a0
12
0
|
f 0
1 −a0
10
a0
00 f 0
0
0
a0
21
a0
22
a0
23
|
f 0
2
0
0
a0
32
a0
33
|
f 0
3


TLFeBOOK

280
INTERPOLATION
=


a1
00
a1
01
0
0
|
f 1
0
0
a1
11
a1
12
0
|
f 1
1
0
a1
21
a1
22
a1
23
|
f 1
2
0
0
a1
32
a1
33
|
f 1
3


.
(6.99)
Now we apply elementary row operations to eliminate a1
21. Thus, (6.99) becomes
[A2|f 2] =


a1
00
a1
01
0
0
|
f 1
0
0
a1
11
a1
12
0
|
f 1
1
0
0
a1
22 −a1
21
a1
11 a1
12
a1
23
|
f 1
2 −a1
21
a1
11 f 1
1
0
0
a1
32
a1
33
|
f 1
3


=


a2
00
a2
01
0
0
|
f 2
0
0
a2
11
a2
12
0
|
f 2
1
0
0
a2
22
a2
23
|
f 2
2
0
0
a2
32
a2
33
|
f 2
3


.
(6.100)
Finally, we eliminate a2
32, in which case (6.100) becomes
[A3|f 3] =


a2
00
a2
01
0
0
|
f 2
0
0
a2
11
a2
12
0
|
f 2
1
0
0
a2
22
a2
23
|
f 2
2
0
0
0
a2
33 −a2
32
a2
22 a2
23
|
f 2
3 −a2
32
a2
22 f 2
2


=


a3
00
a3
01
0
0
|
f 3
0
0
a3
11
a3
12
0
|
f 3
1
0
0
a3
22
a3
23
|
f 3
2
0
0
0
a3
33
|
f 3
3


.
(6.101)
We have A3 = U, an upper triangular matrix. Thus, Ux = f 3 can be solved by
backward substitution. The reader should write a pseudocode program to implement
this approach for any n. Ideally, the code ought to be written such that only the
vector f , and the main, super-, and subdiagonals of A are stored.
TLFeBOOK

SPLINE INTERPOLATION
281
We observe that the algorithm we have just constructed will work only if ai
i,i ̸= 0
for all i = 0, 1, . . . , n. Thus, our approach may not be stable. We expect this
potential problem because our algorithm does not employ any pivoting. However,
our application here is to use the algorithm to solve either (6.87) or (6.93). For A in
either of these cases, we never get ai
i,i = 0. We may justify this claim as follows.
In Deﬁnition 6.2 it is understood that ai,j = 0 for i, j ̸∈Zn+1.
Deﬁnition 6.2: Diagonal
Dominance
The
tridiagonal
matrix
A =
[ai,j]i,j=0,...,n ∈R(n+1)×(n+1) is diagonally dominant if
ai,i > |ai,i−1| + |ai,i+1| > 0
for i = 0, 1, . . . , n.
It is clear that A in (6.87), or in (6.93) is diagonally dominant. For the algorithm
we have developed to solve Ax = f , in general we can say
ak+1
k+1,k+1 = ak
k+1,k+1 −
ak
k+1,k
ak
k,k
ak
k,k+1,
(6.102)
with ak+1
k+1,k = 0, and
ak
k,k+1 = ak,k+1
(6.103)
for k = 0, 1, . . . , n −1. Condition (6.103) states that the algorithm does not modify
the superdiagonal elements of A.
Theorem 6.2:
If tridiagonal matrix A is diagonally dominant, then the algo-
rithm for solving Ax = f will not yield ai
i,i = 0 for any i = 0, 1, . . . , n.
Proof
We give only an outline of the main idea. The complete proof needs
mathematical induction.
From Deﬁnition 6.2
a0
0,0 > |a0
0,−1| + |a0
0,1| = |a0
0,1| > 0,
so from (6.102)
a1
1,1 = a0
1,1 −
a0
0,1
a0
0,0
a0
1,0,
with a1
1,0 = 0. Thus, A1 can be obtained from A0 = A. Now again from Deﬁni-
tion 6.2
a0
1,1 > |a0
1,0| + |a0
1,2| > 0,
TLFeBOOK

282
INTERPOLATION
so that, because 0 < |a0
0,1/a0
0,0| < 1, we can say that
a1
1,1 ≥a0
1,1 −

a0
0,1
a0
0,0
 |a0
1,0|
≥|a0
1,0| + |a0
1,2| −

a0
0,1
a0
0,0
 |a0
1,0|
=
'
1 −

a0
0,1
a0
0,0

(
|a0
1,0| + |a0
1,2|
> |a0
1,2| = |a1
1,2| = |a1
1,0| + |a1
1,2| > 0,
and A1 is diagonally dominant. The diagonal dominance of A0 implies the diagonal
dominance of A1. In general, we see that if Ak is diagonally dominant, then Ak+1
will be diagonally dominant as well, and so Ak+1 can be formed from Ak for all
k = 0, 1, . . . , n −1. Thus, ai
i,i ̸= 0 for all i = 0, 1, . . . , n.
If A is diagonally dominant, it will be well-conditioned, too (not obvious), and
so our algorithm to solve Ax = f is actually quite stable in this case.
Before considering an example of spline interpolation, we specify a result con-
cerning the accuracy of approximation with cubic splines. This is a modiﬁed version
of Theorem 3.13 in Burden and Faires [6], or of Theorem 4.7 in Epperson [9].
Theorem 6.3:
If f (x) ∈C4[a, b] with maxx∈[a,b] |f (4)(x)| ≤M, and p3(x)
is the unique complete spline interpolant for f (x), then
max
x∈[a,b] |f (x) −p3(x)| ≤
5
384Mh4.
Proof
The proof is omitted, but Epperson [9] suggests referring to the article
by Hall [10].
We conclude this section with an illustration of the quality of approximation of the
cubic splines.
Example 6.5
Figure 6.3 shows the natural and complete cubic spline inter-
polants to the function f (x) = exp(−x2) for x ∈[−1, 1]. We have chosen the
nodes xk = −1 + 1
2k, for k = 0, 1, 2, 3, 4 (i.e., n = 4, and h = 1
2). Clearly,
f (1)(x) = −2x exp(−x2). Thus, at the nodes
f (±1) = 0.36787944,
f (± 1
2) = 0.60653066,
f (0) = 1.00000000,
and
f (1)(±1) = ∓0.73575888.
TLFeBOOK

SPLINE INTERPOLATION
283
−1.5
−1
−0.5
0
0.5
1
1.5
0.2
0.4
0.6
0.8
1
x
(a)
(b)
−1.5
−1
−0.5
0
0.5
1
1.5
x
y
y
0.2
0.4
0.6
0.8
1
Figure 6.3
Natural (a) and complete (b) spline interpolants (dashed lines) for the function
f (x) = e−x2 (solid lines) on the interval [−1, 1]. The circles correspond to node locations.
Here n = 4, with h = 1
2, and x0 = −1. The nodes are at xk = x0 + hk for k = 0, . . . , n.
Of course, the spline series for our example has the form
p3(x) =
5

k=−1
akSk(x),
so we need to determine the coefﬁcients ak from both (6.87), and (6.93).
Considering the natural spline interpolant ﬁrst, from (6.87), we have


4
1
0
1
4
1
0
1
4




a1
a2
a3

=


f (x1) −1
6f (x0)
f (x2)
f (x3) −1
6f (x4)

.
Additionally
a0 = 1
6f (x0) = 1
6f (−1),
a4 = 1
6f (x4) = 1
6f (1),
and
a−1 = f (x0) −4a0 −a1,
a5 = f (x4) −4a4 −a3.
TLFeBOOK

284
INTERPOLATION
The natural spline series coefﬁcients are therefore
k
ak
−1
−0.01094139
0
0.06131324
1
0.13356787
2
0.18321607
3
0.13356787
4
0.06131324
5
−0.01094139
Now, on considering the case of the complete spline, from (6.93) we have


4
2
0
0
0
1
4
1
0
0
0
1
4
1
0
0
0
1
4
1
0
0
0
2
4




a0
a1
a2
a3
a4


=


f (x0) + 1
3hf (1)(x0)
f (x1)
f (x2)
f (x3)
f (x4) −1
3hf (1)(x4)


.
Additionally
a−1 = a1 −1
3hf (1)(x0),
a5 = a3 + 1
3hf (1)(x4).
The complete spline series coefﬁcients are therefore
k
ak
−1
0.01276495
0
0.05493076
1
0.13539143
2
0.18230428
3
0.13539143
4
0.05493076
5
0.01276495
This example demonstrates what was suggested earlier, and that is that the com-
plete spline interpolant tends to be more accurate than the natural spline interpolant.
However, accuracy of the complete spline interpolant is contingent on accurate
estimation, or knowledge of the derivatives f (1)(x0) and f (1)(xn).
REFERENCES
1. D. R. Hill, Experiments in Computational Matrix Algebra (C. B. Moler, consulting ed.),
Random House, New York, 1988.
2. G. H. Golub and C. F. Van Loan, Matrix Computations, 2nd ed., Johns Hopkins Univ.
Press, Baltimore, MD, 1989.
TLFeBOOK

PROBLEMS
285
3. F. B. Hildebrand, Introduction to Numerical Analysis, 2nd ed., McGraw-Hill, New York,
1974.
4. G. E. Forsythe, M. A. Malcolm, and C. B. Moler, Computer Methods for Mathematical
Computations, Prentice-Hall, Englewood Cliffs, NJ, 1977.
5. E. Isaacson and H. B. Keller, Analysis of Numerical Methods, Wiley, New York,
1966.
6. R. L. Burden and J. D. Faires, Numerical Analysis, 4th ed., PWS-KENT Publi., Boston,
MA, 1989.
7. C. K. Chui, An Introduction to Wavelets, Academic Press, Boston, MA, 1992.
8. M. Unser and T. Blu, “Wavelet Theory Demystiﬁed,” IEEE Trans. Signal Process. 51,
470–483 (Feb. 2003).
9. J. F. Epperson, An Introduction to Numerical Methods and Analysis, Wiley, New York,
2002.
10. C. A. Hall, “On Error Bounds for Spline Interpolation,” J. Approx. Theory. 1, 209–218
(1968).
PROBLEMS
6.1. Find
the
Lagrange
interpolant
p2(t) = 2
j=0 xjLj(t)
for
data
set
{(−1, 1
2), (0, 1), (1, −1)}. Find p2,j in p2(t) = 2
j=0 p2,jtj.
6.2. Find
the
Lagrange
interpolant
p3(t) = 3
j=0 xjLj(t)
for
data
set
{(0, 1), ( 1
2, 2), (1, 3
2), (2, −1)}. Find p3,j in p3(t) = 3
j=0 p3,jtj.
6.3. We want to interpolate x(t) at t = t1, t2, and x(1)(t) = dx(t)/dt at t = t0, t3
using p3(t) = 3
j=0 p3,jtj. Let xj = x(tj) and x(1)
j
= x(1)(tj). Find the lin-
ear system of equations satisﬁed by (p3,j).
6.4. Find a general expression for L(1)
j (t) = dLj(t)/dt.
6.5. In Section 6.2 it was mentioned that fast algorithms exist to solve Vander-
monde linear systems of equations. The Vandermonde system Ap = x is
given in expanded form as


1
t0
t2
0
· · ·
tn−1
0
tn
0
1
t1
t2
1
· · ·
tn−1
1
tn
1
...
...
...
...
...
1
tn−1
t2
n−1
· · ·
tn−1
n−1
tn
n−1
1
tn
t2
n
· · ·
tn−1
n
tn
n




p0
p1
...
pn−1
pn


=


x0
x1
...
xn−1
xn


, (6.P.1)
and pn(t) = n
j=0 pjtj interpolates the points {(tj, xj)|k = 0, 1, 2, . . . , n}.
A fast algorithm to solve (6.P.1) is
TLFeBOOK

286
INTERPOLATION
for k := 0 to n −1 do begin
for i := n downto to k + 1 do begin
xi := (xi −xi−1)/(ti −ti−k−1);
end;
end;
for k := n −1 downto 0 do begin
for i := k to n −1 do begin
xi := xi −tkxi+1;
end;
end;
The algorithm overwrites vector x = [x0 x1 · · · xn]T with the vector p =
[p0 p1 · · · pn]T .
(a) Count the number of arithmetic operations needed by the fast algorithm.
What is the asymptotic time complexity of it, and how does this compare
with Gaussian elimination as a method to solve (6.P.1)?
(b) Test the fast algorithm out on the system


1
1
1
1
1
2
4
8
1
3
9
27
1
4
16
64




p0
p1
p2
p3

=


10
26
58
112

.
(c) The “top” k loop in the fast algorithm produces the Newton form (Section
6.3) of the representation for pn(t). For the system in (b), conﬁrm that
pn(t) =
n

k=0
xk
k−1
/
i=0
(t −ti),
where {xk|k ∈Zn+1} are the outputs from the top k loop. Since in (b)
n = 3, we must have for this particular special case
p3(t) = x0 + x1(t −t0) + x2(t −t0)(t −t1) + x3(t −t0)(t −t1)(t −t2).
(Comment: It has been noted by Bj¨orck and Pereyra that the fast algorithm
often yields accurate results even when A is ill-conditioned.)
6.6. Prove that for
An =


1
t0
t2
0
· · ·
tn−1
0
tn
0
1
t1
t2
1
· · ·
tn−1
1
tn
1
...
...
...
...
...
1
tn−1
t2
n−1
· · ·
tn−1
n−1
tn
n−1
1
tn
t2
n
· · ·
tn−1
n
tn
n


,
TLFeBOOK

PROBLEMS
287
we have
det(An) =
/
0≤i<j≤n
(ti −tj).
(Hint: Use mathematical induction.)
6.7. Write a MATLAB function to interpolate the data {(tj, xj)|j ∈Zn+1} with
polynomial pn(t) via Lagrange interpolation. The function must accept t as
input, and return pn(t).
6.8. Runge’s phenomenon was mentioned in Section 6.2 with respect to interpo-
lating f (t) = 1/(1 + t2) on t ∈[−5, 5]. Use polynomial pn(t) to interpolate
f (t) at the points tk = t0 + kh, k = 0, 1, . . . , n, where t0 = −5, and tn = 5
(so h = (tn −t0)/n). Do this for n = 5, 8, 10. Use the MATLAB function
from the previous problem. Plot f (t) and pn(t) on the same graph for all of
n = 5, 8, 10. Comment on the accuracy of interpolation as n increases.
6.9. Suppose that we wish to interpolate f (t) = sin t for t ∈[0, π/2] using poly-
nomial pn(t) = n
j=0 pn,jtj. The approximation error is en(t) = f (t) −
pn(t). Since |f (n)(t)| ≤1 for all t ∈R via (6.14), it follows that
|en(t)| ≤
1
(n + 1)!
n
/
i=0
|t −ti| = b(t).
(6.P.2)
Let the grid (sample) points be t0 = 0, tn = π/2, and tk = t0 + kh for k =
0, 1, . . . , n so that h = (tn −t0)/n = π
2n. Using MATLAB
(a) For n = 2, on the same graph plot f (t) −pn(t), and ±b(t).
(b) For n = 4, on the same graph plot f (t) −pn(t), and ±b(t).
Cases (a) and (b) can be separate plots (e.g., using MATLAB subplot). Does
the bound in (6.P.2) hold in both cases?
6.10. Consider f (t) = cosh t = 1
2[et + e−t] which is to be interpolated by p2(t) =
2
j=0 p2,jtj on t ∈[−1, 1] at the points t0 = −1, t0 = 0, t1 = 1. Use (6.14)
to ﬁnd an upper bound on the size of the approximation error en(t) for
t ∈[−1, 1], where the size of the approximation error is given by the norm
||en||∞= max
a≤t≤b |en(t)|,
where here a = −1, b = 1.
6.11. For
V =


1
1
· · ·
1
1
t0
t1
· · ·
tn−1
tn
...
...
...
...
tn
0
tn
1
· · ·
tn
n−1
tn
n

∈R(n+1)×(n+1),
TLFeBOOK

288
INTERPOLATION
it is claimed by Gautschi that
||V −1||∞≤max
0≤k≤n
n
/
i=0
i̸=k
1 + |ti|
|tk −ti|.
(6.P.3)
Find an upper bound on κ∞(V ) that uses (6.P.3). How useful is an upper
bound on the condition number? Would a lower bound on condition number
be more useful? Explain.
6.12. For each function listed below, use divided difference tables (recall Example
6.3) to construct the degree n Newton interpolating polynomial for the spec-
iﬁed points.
(a) f (t) = √t,
t0 = 0, t1 = 1, t2 = 3. Use n = 2.
(b) f (t) = cosh t,
t0 = −1, t1 = −1
2 , t2 = 1
2, t3 = 1. Use n = 3.
(c) f (t) = ln t,
t0 = 1, t1 = 2, t2 = 3. Use n = 2.
(d) f (t) = 1/(1 + t),
t0 = 0, t1 = 1
2, t2 = 1, t3 = 2. Use n = 3.
6.13. Prove Eq. (6.33).
6.14. Consider Theorem 6.1. For n = 2, ﬁnd hk(x) and ˆhk(x) as direct-form (i.e.,
a form such as (6.4)) polynomials in x. Of course, do this for all k = 0, 1, 2.
Find Hermite interpolation polynomial p2n+1(x) for n = 2 that interpolates
f (x) = √x at the points x0 = 1, x1 = 3/2, x2 = 2.
6.15. Find the Hermite interpolating polynomial p5(x) to interpolate f (x) = ex at
the points x0 = 0, x1 = 0.1, x2 = 0.2. Use MATLAB to compare the accu-
racy of approximation of p5(x) to that of pa
2(x) given in Example 6.3 (or
Example 6.4).
6.16. The following matrix is important in solving spline interpolation problems
[recall (6.87)]:
An =


4
1
0
· · ·
0
0
1
4
1
· · ·
0
0
0
1
4
· · ·
0
0
...
...
...
...
...
0
0
0
· · ·
4
1
0
0
0
· · ·
1
4


∈Rn×n.
Suppose that Dn = det(An).
(a) Find D1, D2, D3, and D4 by direct (hand) calculation using the basic
rules for computing determinants.
(b) Show that
Dn+2 −4Dn+1 + Dn = 0
TLFeBOOK

PROBLEMS
289
(i.e., the determinants we seek may be generated by a second-order dif-
ference equation).
(c) For suitable constants α, β ∈R, it can be shown that
Dn = α(2 +
√
3)n + β(2 −
√
3)n
(6.P.4)
(n ∈N). Find α and β.
[Hint: Set up two linear equations in the two unknowns α and β using
(6.P.4) for n = 1, 2 and the results from (a).]
(d) Prove that Dn > 0 for all n ∈N.
(e) Is An > 0 for all n ∈N? Justify your answer.
[Hint: Recall part (a) in Problem 4.13 (of Chapter 4).]
6.17. Repeat Example 6.5, except use
f (x) =
1
1 + x2 ,
and x ∈[−5, 5] with nodes xk = −5 + 5
2k for k = 0, 1, 2, 3, 4 (i.e., n = 4,
and h = 5
2). How do the results compare to the results in Problem 6.8 (assum-
ing that you have done Problem 6.8)? Of course, you should use MATLAB
to “do the dirty work.” You may use built-in MATLAB linear system solvers.
6.18. Repeat Example 6.5, except use
f (x) =
√
1 + x.
Use MATLAB to aid in the task.
6.19. Write a MATLAB function to solve tridiagonal linear systems of equations
based on the theory for doing so given in Section 6.5. Test your algorithm
out on the linear systems given in Example 6.5.
TLFeBOOK

7
Nonlinear Systems of Equations
7.1
INTRODUCTION
In this chapter we consider the problem of ﬁnding x to satisfy the equation
f (x) = 0
(7.1)
for arbitrary f , but such that f (x) ∈R. Usually we will restrict our discussion
to the case where x ∈R, but x ∈C can also be of signiﬁcant interest in practice
(especially if f is a polynomial). However, any x satisfying (7.1) is called a root
of the equation. Such an x is also called a zero of the function f . More generally,
we are also interested in solutions to systems of equations:
f0(x0, x1, . . . , xn−1) = 0,
f1(x0, x1, . . . , xn−1) = 0,
...
fn−1(x0, x1, . . . , xn−1) = 0.
(7.2)
Again, fi(x0, x1, . . . , xn−1) ∈R, and we will assume xk ∈R for all i, k ∈Zn. Var-
ious solution methods will be considered such the bisection method, ﬁxed-point
method, and the Newton–Raphson method. All of these methods are iterative in
that they generate a sequence of points (or more generally vectors) that converge
(we hope) to the desired solution. Consequently, ideas from Chapter 3 are rele-
vant here. The number of iterations needed to achieve a solution with a given
accuracy is considered. Iterative procedures can break down (i.e., fail) in various
ways. The sequence generated by a procedure may diverge, oscillate, or display
chaotic behavior (which in the past was sometimes described as “wandering behav-
ior”). Examples of breakdown phenomena will therefore also be considered. Some
attention will be given to chaotic phenomena, as these are of growing engineering
interest. Applications of chaos are now being considered for such areas as cryp-
tography and spread-spectrum digital communications. These proposed applications
are still controversial, but at the very least some knowledge of chaos gives a deeper
insight into the behavior of nonlinear dynamic systems.
An Introduction to Numerical Analysis for Electrical and Computer Engineers, by C.J. Zarowski
ISBN 0-471-46737-5
c⃝2004 John Wiley & Sons, Inc.
290
TLFeBOOK

INTRODUCTION
291
The equations considered in this chapter are nonlinear, and so are in contrast
with those of Chapter 4. Chapter 4 considered linear systems of equations only. The
reader is probably aware of the fact that a linear system either has a unique solution,
no solution, or an inﬁnite number of solutions. Which case applies depends on the
size and rank of the matrix in the linear system. Chapter 4 emphasized the handling
of square and invertible matrices for which the solution exists, and is unique. In
Chapter 4 we saw that well-deﬁned procedures exist (e.g., Gaussian elimination)
that give the solution in a ﬁnite number of steps.
The solution of nonlinear equations is signiﬁcantly more complicated than the
solution of linear systems. Existence and uniqueness problems typically have no
easy answers. For example, e2x + 1 = 0 has no real-valued solutions, but if we allow
x ∈C, then x = 1
2kπj for which k is an odd integer (since e2x + 1 = ejπk + 1 =
cos(kπ) + j sin(kπ) + 1 = cos(kπ) = −1 + 1 = 0 as k is odd). On the other hand
e−t −sin(t) = 0
also has an inﬁnite number of solutions, but for which t ∈R (see Fig. 7.1). How-
ever, the solutions are not speciﬁable with a nice formula. In Fig. 7.1 the solutions
correspond to the point where the two curves intersect each other.
Polynomial equations are of special interest.1 For example, x2 + 1 = 0 has only
complex solutions x = ±j. Multiple roots are also possible. For example
x3 −3x2 + 3x −1 = (x −1)3 = 0
has a real root of multiplicity 3 at x = 1. We remark that the methods of this
chapter are general and so (in principle) can be applied to ﬁnd the solution of
any nonlinear equation, polynomial or otherwise. But the special importance of
polynomial equations has caused the development (over the centuries) of algorithms
dedicated to polynomial equation solution. Thus, special algorithms exist to solve
pn(x) =
n

k=0
pn,kxk = 0,
(7.3)
1Why are polynomial equations of special interest? There are numerous answers to this. But the reader
already knows one reason why from basic electric circuits. For example, an unforced RLC (resistance ×
inductance × capacitance) circuit has a response due to energy initially stored in the energy storage
elements (the inductors and capacitors). If x(t) is the voltage drop across an element or the current
through an element, then
an
dnx(t)
dtn
+ an−1
dn−1x(t)
dtn−1
+ · · · + a1
dx(t)
dt
+ a0 = 0.
The coefﬁcients ak depend on the circuit elements. The solution to the differential equation depends on
the roots of the characteristic equation:
anλn + an−1λn−1 + · · · + a1λ + a0 = 0.
TLFeBOOK

292
NONLINEAR SYSTEMS OF EQUATIONS
−1
0
1
2
3
4
5
6
7
8
−1
−0.5
0
0.5
1
1.5
2
2.5
t
e−t
sin(t)
Figure 7.1
Plot of the individual terms in the equation f (t) = e−t −sin(t) = 0. The inﬁ-
nite number of solutions possible corresponds to the point where the plotted curves intersect.
and these algorithms do not apply to general nonlinear equations. Sometimes
these are based on the general methods we consider in this chapter. Other times
completely different methods are employed (e.g., replacing the problem of ﬁnd-
ing polynomial zeros by the equivalent problem of ﬁnding matrix eigenvalues as
suggested in Jenkins and Traub [22]). However, we do not consider these special
polynomial equation solvers here. We only mention that some interesting references
on this matter are Wilkinson [1] and Cohen [2]. These describe the concept of ill-
conditioned polynomials, and how to apply deﬂation procedures to produce more
accurate estimates of roots. The difﬁculties posed by multiple roots are considered
in Hull and Mathon [3]. Modern math-oriented software tools (e.g., MATLAB)
often take advantage of theories such as described in Refs. 1–3, (as well as in
other sources). In MATLAB polynomial zeros may be found using the roots and
mroots functions. Function mroots is a modern root ﬁnder that reliably determines
multiple roots.
7.2
BISECTION METHOD
The bisection method is a simple intuitive approach to solving
f (x) = 0.
(7.4)
It is assumed that f (x) ∈R, and that x ∈R. This method is based on the following
theorem.
Theorem 7.1: Intermediate Value Theorem
If f |[a, b] →R is continuous
on the closed, bounded interval [a, b], and y0 ∈R is such that f (a) ≤y0 ≤f (b),
then there is an x0 ∈[a, b] such that f (x0) = y0. In other words, a continuous
function on a closed and bounded interval takes on all values between f (a) and
f (b) at least once.
TLFeBOOK

BISECTION METHOD
293
The bisection method works as follows. Suppose that we have an initial interval
[a0, b0] such that
f (a0)f (b0) < 0,
(7.5)
which means that f (a0) and f (b0) have opposite signs (i.e., one is positive while
the other is negative). By Theorem 7.1 there must be a p ∈(a0, b0) so that f (p) =
0. We say that [a0, b0] brackets the root p. Suppose that
p0 = 1
2(a0 + b0).
(7.6)
This is the midpoint of the interval [a0, b0]. Consider the following cases:
1. If f (p0) = 0, then p = p0 and we have found a root. We may stop at this
point.
2. If f (a0)f (p0) < 0, then it must be the case that p ∈[a0, p0], so we deﬁne
the new interval [a1, b1] = [a0, p0]. This new interval brackets the root.
3. If f (p0)f (b0) < 0 then it must be the case that p ∈[p0, b0] so we deﬁne
the new interval [a1, b1] = [p0, b0]. This new interval brackets the root.
The process is repeated by considering the midpoint of the new interval, which is
p1 = 1
2(a1 + b1),
(7.7)
and considering the three cases again. In principle, the process terminates when
case 1 is encountered. In practice, case 1 is unlikely in part because of the effects
of rounding errors, and so we need a more practical criterion to stop the process.
This will be considered a little later on. For now, pseudocode describing the basic
algorithm may be stated as follows:
input [a0, b0] which brackets the root p;
p0 := (a0 + b0)/2;
k := 0;
while stopping criterion is not met do begin
if f(ak)f(pk) < 0 then begin
ak+1 := ak;
bk+1 := pk;
end;
else begin
ak+1 := pk;
bk+1 := bk;
end;
pk+1 := (ak+1 + bk+1)/2;
k := k + 1;
end;
When the algorithm terminates, the last value of pk+1 computed is an estimate
of p.
TLFeBOOK

294
NONLINEAR SYSTEMS OF EQUATIONS
We see that the bisection algorithm constructs sequence (pn) = (p0, p1, p2, . . .)
such that
lim
n→∞pn = p,
(7.8)
where pn is the midpoint of [an, bn], and f (p) = 0. Formal proof that this process
works (i.e., yields a unique p such that f (p) = 0) is due to the following theorem.
Theorem 7.2: Cantor’s Intersection Theorem
Suppose that ([ak, bk]) is a
sequence of closed and bounded intervals such that
[a0, b0] ⊃[a1, b1] ⊃· · · ⊃[an, bn] ⊃· · · ,
with limn→∞(bn −an) = 0. There is a unique point p ∈[an, bn] for all n ∈Z+:
∞
@
n=0
[an, bn] = {p}.
The bisection method produces (pn) such that an < pn < bn, and p ∈[an, bn] for
all n ∈Z+. Consequently, since pn = 1
2(an + bn)
|pn −p| ≤|bn −an| ≤b −a
2n
(7.9)
for n ∈Z+, so that limn→∞pn = p. Recall that f is assumed continuous on [a, b],
so limn→∞f (pn) = f (p). So now observe that
|pn −an| ≤1
2n |b −a|, |bn −pn| ≤1
2n |b −a|
(7.10)
so, via |x −y| = |(x −z) −(y −z)| ≤|x −z| + |y −z| (triangle inequality), we
have
|p −an| ≤|p −pn| + |pn −an| ≤1
2n (b −a) + 1
2n (b −a) =
1
2n−1 (b −a),
(7.11a)
and similarly
|p −bn| ≤|p −pn| + |pn −bn| ≤
1
2n−1 (b −a).
(7.11b)
Thus
lim
n→∞an = lim
n→∞bn = p.
(7.12)
At each step the root p is bracketed. This implies that there is a subsequence [of (pn)]
denoted (xn) converging to p so that f (xn) > 0 for all n ∈Z+. Similarly, there is a
subsequence (yn) converging to p such that f (yn) < 0 for all n ∈Z+. Thus
f (p) = lim
n→∞f (xn) ≥0, f (p) = lim
n→∞f (yn) ≤0,
TLFeBOOK

BISECTION METHOD
295
which implies that f (p) = 0. We must conclude that the bisection method produces
sequence (pn) converging to p such that f (p) = 0. Hence, the bisection method
always works.
Example 7.1
We want to ﬁnd 51/3 (cube root of ﬁve). This is equivalent to
solving the equation f (x) = x3 −5 = 0. We note that f (1) = −4 and f (2) = 3,
so we may use [a0, b0] = [a, b] = [1, 2] to initially bracket the root. We remark
that the “exact” value is 51/3 = 1.709976 (seven signiﬁcant ﬁgures). Consider the
following iterations of the bisection method:
[a0, b0] = [1, 2],
p0 = 1.500000,
f (p0) = −1.625000
[a1, b1] = [p0, b0],
p1 = 1.750000,
f (p1) = 0.359375
[a2, b2] = [a1, p1],
p2 = 1.625000,
f (p2) = −0.708984
[a3, b3] = [p2, b2],
p3 = 1.687500,
f (p3) = −0.194580
[a4, b4] = [p3, b3],
p4 = 1.718750,
f (p4) = 0.077362
[a5, b5] = [a4, p4],
p5 = 1.703125,
f (p5) = −0.059856
We see that |p5 −p| = |1.703125 −1.709976| = 0.006851. From (7.9)
|p5 −p| ≤b −a
25
= 2 −1
25
= 1
25 = 0.0312500.
The exact error certainly agrees with this bound.
When should we stop iterating? In other words, what stopping criterion should
be chosen? Some possibilities are (for ϵ > 0)
 1
2(bn −an)
 < ϵ,
(7.13a)
|pn −p| < ϵ,
(7.13b)
f (pn) < ϵ,
(7.13c)

pn −pn−1
pn
 < ϵ
(pn ̸= 0).
(7.13d)
We would stop iterating when the inequalities are satisﬁed. Usually (7.13d) is
recommended. Condition (7.13a) is not so good as termination depends on the size
of the nth interval, while it is the accuracy of the estimate of p that is of most
interest. Condition (7.13b) requires knowing p in advance, which is not reasonable
since it is p that we are trying to determine. Condition (7.13c) is based on f (pn),
and again we are more interested in how well pn approximates p. Thus, we are left
with (7.13d). This condition leads to termination when pn is relatively not much
different from pn−1.
TLFeBOOK

296
NONLINEAR SYSTEMS OF EQUATIONS
How may we characterize the computational efﬁciency of an iterative algorithm?
In Chapter 4 the algorithms terminated in a ﬁnite number of steps (with the excep-
tion of the iterative procedures suggested in Section 4.7 which do not terminate
unless a stopping condition is imposed), so ﬂop counting was a reasonable mea-
sure. Where iterative procedures such as the bisection method are concerned, we
prefer
Deﬁnition 7.1: Rate of Convergence
Suppose that (xn) converges to 0:
limn→∞xn = 0. Suppose that (pn) converges to p, i.e., limn→∞pn = p. If there
is a K ∈R, but K > 0, and N ∈Z+ such that
|pn −p| ≤K|xn|
for all n ≥N, then we say that (pn) converges to p with rate of convergence O(xn).
This is an alternative use of the “big O” notation that was ﬁrst seen in Chapter 4.
Recalling (7.9), we obtain
|pn −p| ≤(b −a) 1
2n ,
(7.14)
so that K = b −a, xn = 1
2n , and N = 0. Thus, the bisection method generates
sequence (pn) that converges to p (with f (p) = 0) at the rate O(1/2n).
From (7.14), if we want |pn −p| < ϵ, then we may choose n so that
|pn −p| ≤b −a
2n
< ϵ,
implying 2n > (b −a)/ϵ, or we may choose n so
n =
A
log2
 b −a
ϵ
!B
,
(7.15)
where ⌈x⌉= smallest integer greater than or equal to x. This can be used as an
alternative means to terminate the bisection algorithm. But the conservative nature
of (7.15) suggests that the algorithm that employs it may compute more iterations
than are really necessary for the desired accuracy.
7.3
FIXED-POINT METHOD
Here we consider the Banach ﬁxed-point theorem [4] as the theoretical basis for a
nonlinear equation solver. Suppose that X is a set, and T |X →X is a mapping of
X into itself. A ﬁxed point of T is an x ∈X such that
T x = x.
(7.16)
TLFeBOOK

FIXED-POINT METHOD
297
For example, suppose X = [0, 1] ⊂R, and
T x = 1
2x(1 −x).
(7.17)
We certainly have T x ∈[0, 1] for any x ∈X. The solution to
x = 1
2x(1 −x)
(i.e., to T x = x) is x = 0. So T has ﬁxed point x = 0. (We reject the solution
x = −1 since −1 ̸∈[0, 1].)
Deﬁnition 7.2: Contraction
Let X = (X, d) be a metric space. Mapping
T |X →X is called a contraction (or a contraction mapping, or a contractive
mapping) on X if there is an α ∈R such that 0 < α < 1, and for all x, y ∈X
d(T x, T y) ≤αd(x, y).
(7.18)
Applying T to “points” x and y brings them closer together if T is a contractive
mapping. If α = 1, the mapping is sometimes called nonexpansive.
Theorem 7.3: Banach Fixed-Point Theorem
Consider the metric space X =
(X, d), where X ̸∈∅. Suppose that X is complete, and T |X →X is a contraction
on X. Then T has a unique ﬁxed point.
Proof
We must construct (xn), and show that it is Cauchy so that (xn) con-
verges in X (recall Section 3.2). Then we prove that x is the only ﬁxed point of
T . Suppose that x0 ∈X is any point from X. Consider the sequence produced by
the repeated application of T to x0:
x0, x1 = T x0, x2 = T x1 = T 2x0, . . . , xn = T xn−1 = T nx0, . . . .
(7.19)
From (7.19) and (7.18), we obtain
d(xk+1, xk) = d(T xk, T xk−1)
≤αd(xk, xk−1)
= αd(T xk−1, T xk−2)
≤α2d(xk−1, xk−2)
...
≤αkd(x1, x0).
(7.20)
TLFeBOOK

298
NONLINEAR SYSTEMS OF EQUATIONS
Using the triangle inequality with n > k, from (7.20)
d(xk, xn) ≤d(xk, xk+1) + d(xk+1, xk+2) + · · · + d(xn−1, xn)
≤(αk + αk+1 · · · + αn−1)d(x0, x1)
= αk 1 −αn−k
1 −α
d(x0, x1)
(where the last equality follows by application of the formula for geometric series,
seen very frequently in previous chapters). Since 0 < α < 1, we have 1 −αn−k < 1.
Thus
d(xk, xn) ≤
αk
1 −α d(x0, x1)
(7.21)
(n > k). Since 0 < α < 1, 0 < 1 −α < 1, too. In addition, d(x0, x1) is ﬁxed (since
we have chosen x0). We may make the right-hand side of (7.21) arbitrarily small
by making k sufﬁciently big (keeping n > k). Consequently, (xn) is Cauchy. X is
assumed to be complete, so xn →x ∈X.
From the triangle inequality and (7.18)
d(T x, x) ≤d(x, xk) + d(xk, T x)
≤d(x, xk) + αd(xk−1, x)
(recall that T x = x)
< ϵ
(any ϵ > 0)
if k →∞, since xn →x. Consequently, d(x, T x) = 0 implies that T x = x (recall
(M1) in Deﬁnition 1.1). Immediately, x is a ﬁxed point of T .
Point x is unique. Let us assume that there is another ﬁxed point ˆx, i.e., T ˆx = ˆx.
But from (7.18)
d(x, ˆx) = d(T x, T ˆx) ≤αd(x, ˆx),
implying d(x, ˆx) = 0 because 0 < α < 1. Thus x = ˆx [(M1) From Deﬁnition 1.1
again].
Theorem 7.3 is also called the contraction theorem, a special instance of which
was seen in Section 4.7. The theorem applies to complete metric spaces, and so is
applicable to Banach and Hilbert spaces. (Recall that inner products induce norms,
and norms induce metrics. Hilbert and Banach spaces are complete. So they must
be complete metric spaces as well.)
Note that we deﬁne T 0x = x for any x ∈X. Thus T 0 is the identity mapping
(identity operator).
Corollary 7.1
Under the conditions of Theorem 7.3 sequence (xn) from xn =
T nx0 [i.e., the sequence in (7.19)] for any x0 ∈X converges to a unique x ∈X
such that T x = x. We have the following error estimates (i.e., bounds):
TLFeBOOK

FIXED-POINT METHOD
299
1. Prior estimate:
d(xk, x) ≤
αk
1 −α d(x0, x1)
(7.22a)
2. Posterior estimate:
d(xk, x) ≤
α
1 −α d(xk, xk−1).
(7.22b)
Proof
The prior estimate is an immediate consequence of Theorem 7.3 since
it lies within the proof of this theorem [let n →∞in (7.21)].
Now consider (7.22b). In (7.22a) let k = 1, y0 = x0, and y1 = x1. Thus,
d(x1, x) = d(y1, x), d(x0, x1) = d(y0, y1), and so
d(y1, x) ≤
α
1 −α d(y0, y1).
(7.23)
Let y0 = xk−1, so y1 = T y0 = T xk−1 = xk and (7.23) becomes
d(xk, x) ≤
α
1 −α d(xk, xk−1),
which is (7.22b).
The error bounds in Corollary 7.1 are useful in application of the contraction the-
orem to computational problems. For example, (7.22a) can estimate the number of
iteration steps needed to achieve a given accuracy in the estimate of the solution to
a nonlinear equation. The result would be analogous to Eq. (7.15) in the previous
section.
A difﬁculty with the theory so far is that T |X →X is not always contractive
over the entire metric space X, but only on a subset, say, Y ⊂X. However, a basic
result from functional analysis is that any closed subset of X is complete. Thus, for
any closed Y ⊂X, there is a ﬁxed point x ∈Y ⊂X, and xn →x (xn = T nx0 for
suitable x0 ∈Y). For this idea to work, we must choose x0 ∈Y so that xn ∈Y for
all n ∈Z+.
What does “closed subset” (formally) mean? A neighborhood of x ∈X (metric
space X) is the set Nϵ(x) = {y|d(x, y) < ϵ, ϵ > 0} ⊂X. Parameter ϵ is the radius
of the neighborhood. We say that x is a limit point of Y ⊂X if every neighborhood
of x contains y ̸= x such that y ∈Y. Y is closed if every limit point of Y belongs
to Y. These deﬁnitions are taken from Rudin [5, p. 32].
Example 7.2
Suppose that X = R, Y = (0, 1) ⊂X. We recall that X is a
complete metric space if d(x, y) = |x −y| (x, y ∈X). Is Y closed? Limit points
of Y are x = 0 and x = 1. [Any x ∈(0, 1) is also a limit point of Y.] But 0 ̸∈Y,
and 1 ̸∈Y. Therefore, Y is not closed. On the other hand, Y = [0, 1] is a closed
subset of X since the limit points are now in Y.
TLFeBOOK

300
NONLINEAR SYSTEMS OF EQUATIONS
Fixed-point theorems (and related ideas) such as we have considered so far have
a large application range. They can be used to prove the existence and uniqueness
of solutions to both integral and differential equations [4, pp. 314–326]. They can
also provide (sometimes) computational algorithms for their solution. Fixed-point
results can have applications in signal reconstruction and image processing [6],
digital ﬁlter design [7], the interpolation of bandlimited sequences [8], and the
solution to so-called convex feasibility problems in general [9]. However, we will
consider the application of ﬁxed-point theorems only to the problem of solving
nonlinear equations.
If we wish to ﬁnd p ∈R so that
f (p) = 0,
(7.24)
then we may deﬁne g(x) (g(x)|R →R) with ﬁxed point p such that
g(x) = x −f (x),
(7.25)
and we then see g(p) = p −f (p) = p ⇒f (p) = 0. Conversely, if there is a
function g(x) such that
g(p) = p,
(7.26)
then
f (x) = x −g(x)
(7.27)
will have a zero at x = p. So, if we wish to solve f (x) = 0, one approach would
be to ﬁnd a suitable g(x) as in (7.27) (or (7.25)), and ﬁnd ﬁxed points for it.
Theorem 7.3 informs us about the existence and uniqueness of ﬁxed points for
mappings on complete metric spaces (and ultimately on closed subsets of such
spaces). Furthermore, the theorem leads us to a well-deﬁned computational algo-
rithm to ﬁnd ﬁxed points. At the outset, space X = R with metric d(x, y) = |x −y|
is complete, and for us g and f are mappings on X. So if g and f are related
according to (7.27), then any ﬁxed point of g will be a zero of f , and can be
found by iterating as spelled out in Theorem 7.3. However, the discussion follow-
ing Corollary 7.1 warned us that g may not be contractive on X = R but only on
some subset Y ⊂X. In fact, g is only rarely contractive on all of R. We therefore
usually need to ﬁnd Y = [a, b] ⊂X = R such that g is contractive on Y, and then
we compute xn = gnx0 ∈Y for n ∈Z+. Then xn →x, and f (x) = 0. We again
emphasize that xn ∈Y is necessary for all n. If g is contractive on [a, b] ⊂X,
then, from Deﬁnition 7.2, this means that for any x, y ∈[a, b]
|g(x) −g(y)| ≤α|x −y|
(7.28)
for some real-valued α such that 0 < α < 1.
Example 7.3
Suppose that we want roots of f (x) = λx2 + (1 −λ)x = 0
(assume λ > 0). Of course, this is a quadratic equation and so may be easily
TLFeBOOK

FIXED-POINT METHOD
301
solved by the usual formula for the roots of such an equation (recall Section 4.2).
However, this example is an excellent illustration of the behavior of ﬁxed-point
schemes. It is quite easy to verify that
f (x) = x −λx(1 −x)
#
$%
&
=g(x)
.
(7.29)
We observe that g(x) is a quadratic in x, and also
g(1)(x) = (2x −1)λ,
so that g(x) has a maximum at x = 1
2 for which g( 1
2) = λ/4. Therefore, if we
allow only 0 < λ ≤4, then g(x) ∈[0, 1] for all x ∈[0, 1]. Certainly [0, 1] ⊂R. A
sketch of g(x) and of y = x appears in Fig. 7.2 for various λ. The intersection of
these two curves locates the ﬁxed points of g on [0, 1].
We will suppose Y = [0, 1]. Although g(x) ∈[0, 1] for all x ∈[0, 1] under the
stated conditions, g is not necessarily always contractive on the closed interval
[0, 1]. Suppose λ = 1
2; then, for all x ∈[0, 1], g(x) ∈[0, 1
8] (see the dotted line
in Fig. 7.2, and we can calculate this from knowledge of g). The mapping is
contractive for this case on all of [0, 1]. [This is justiﬁed below with respect to
Eq. (7.30).] Also, g(x) = x only for x = 0. If we select any x0 ∈[0, 1], then, for
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
x
g(x)
y = x
l = 0.5
l = 2.0
l = 2.8
l = 4.0
Figure 7.2
Plot of g(x) = λx(1 −x) for various λ, and a plot of y = x. The places where
y = x and g(x) intersect deﬁne the ﬁxed points of g(x).
TLFeBOOK

302
NONLINEAR SYSTEMS OF EQUATIONS
xn = gnx0, we can expect xn →0. For example, suppose x0 = 0.7500; then the
ﬁrst few iterates are
x0 = 0.7500
x1 = 0.5x0(1 −x0) = 0.0938
x2 = 0.5x1(1 −x1) = 0.0425
x3 = 0.5x2(1 −x2) = 0.0203
x4 = 0.5x3(1 −x3) = 0.0100
x5 = 0.5x4(1 −x4) = 0.0049
...
The process is converging to the unique ﬁxed point at x = 0.
Suppose now that λ = 2; then g(x) = 2x(1 −x), and now g(x) has two ﬁxed
points on [0, 1], which are x = 0, and x = 1
2. For all x ∈[0, 1] we have g(x) ∈
[0, 1
2], but g is not contractive on [0, 1]. For example, suppose x = 0.8, y = 0.9,
then g(.8) = 0.3200, and g(.9) = 0.1800. Thus, |x −y| = 0.1, but |g(x) −g(y)| =
0.14 > 0.1. On the other hand, suppose that x0 = 0.7500; then the ﬁrst few iter-
ates are
x0 = 0.7500
x1 = 2x0(1 −x0) = 0.3750
x2 = 2x1(1 −x1) = 0.4688
x3 = 2x2(1 −x2) = 0.4980
x4 = 2x3(1 −x3) = 0.5000
x5 = 2x4(1 −x4) = 0.5000
...
This process converges to one of the ﬁxed points of g even though g is not
contractive on [0, 1].
From (7.28)
α ≥

g(x) −g(y)
x −y

(x ̸= y),
from which, if we substitute g(x) = λx(1 −x), and g(y) = λy(1 −y), then
α = sup
x̸=y
λ|1 −x −y|.
(7.30)
If λ = 1
2, then α = 1
2. If λ = 2, we cannot have α, so that 0 < α < 1. If
g(x) = 2x(1 −x) (i.e., λ = 2 again), but now instead Y = [0.4, 0.6], then, for all
TLFeBOOK

FIXED-POINT METHOD
303
x ∈Y = [0.4, 0.6], we have g(x) ∈[0.48, 0.50] ⊂Y, implying g|Y →Y. From
(7.30) we have for this situation α = 0.4. The mapping g is contractive on Y.
Suppose x0 = 0.450000; then
x0 = 0.450000
x1 = 2x0(1 −x0) = 0.495000
x2 = 2x1(1 −x1) = 0.499950
x3 = 2x2(1 −x2) = 0.500000
x4 = 2x3(1 −x3) = 0.500000
...
The reader ought to compare the results of this example to the error bounds from
Corollary 7.1 as an exercise.
More generally, and again from (7.28), we have (for x, y ∈Y = [a, b] ⊂R)
α = sup
x̸=y

g(x) −g(y)
x −y
 .
(7.31)
Now recall the mean-value theorem (i.e., Theorem 3.3). If g(x) is continuous on
[a, b], and g(1)(x) is continuous on (a, b), then there is a ξ ∈(a, b) such that
g(1)(ξ) = g(b) −g(a)
b −a
.
Consequently, instead of (7.31) we may use
α =
sup
x∈(a,b)
|g(1)(x)|.
(7.32)
Example 7.4
We may use (7.32) to rework some of the results in Example 7.3.
Since g(1)(x) = λ(2x −1), if Y = [0, 1], and λ = 1
2, then
α =
sup
x∈(0,1)
x −1
2
 = 1
2.
If λ = 2, then α = 2. If now Y = [0.4, 0.6] with λ = 2, then
α =
sup
x∈(0.4,0.6)
|4x −2| = 0.4.
Now suppose that λ = 2.8, and consider Y = [0.61, 0.67], which contains a ﬁxed-
point of g (see Fig. 7.2, which contains a curve for this case). We have
α =
sup
x∈(0.61,0.67)
|5.6x −2.8| = 0.9520,
TLFeBOOK

304
NONLINEAR SYSTEMS OF EQUATIONS
and if x ∈Y, then g(x) ∈[0.619080, 0.666120] ⊂Y so that g|Y →Y, and so g is
contractive on Y. Thus, we consider the iterates
x0 = 0.650000
x1 = 2.8x0(1 −x0) = 0.637000
x2 = 2.8x1(1 −x1) = 0.647447
x3 = 2.8x2(1 −x2) = 0.639126
x4 = 2.8x3(1 −x3) = 0.645803
x5 = 2.8x4(1 −x4) = 0.640476
...
The true ﬁxed point (to 6 signiﬁcant ﬁgures) is x = 0.642857 (i.e., xn →x). We
may check these numbers against the bounds of Corollary 7.1. Therefore, we con-
sider the distances
d(x5, x) = 0.002381,
d(x0, x1) = 0.013000,
d(x5, x4) = 0.005327,
and from (7.22a)
d(x5, x) ≤
αk
1 −α d(x0, x1) = 0.211780,
and from (7.22b)
d(x5, x) ≤
α
1 −α d(x5, x4) = 0.105652.
These error bounds are very loose, but they are nevertheless consistent with the
true error d(x5, x) = 0.002381.
We have worked with g(x) = λx(1 −x) in the previous two examples
(Example 7.3 and Example 7.4). But this is not the only possible choice. It may
be better to make other choices.
Example 7.5
Again, assume f (x) = λx2 + (1 −λ)x = 0 as in the previous
two examples. Observe that
λx2 = (λ −1)x
implies that
x =
8
λ −1
λ
x1/2 = g(x).
(7.33)
TLFeBOOK

NEWTON–RAPHSON METHOD
305
If λ = 4, then f (x) = 0 for x = 0 and for x = 3
4. For g(x) in (7.29), g(1)(x) =
8x −4, and g(1)( 3
4) = 2. We cannot ﬁnd a closed interval Y containing x = 3
4 on
which g is contractive with g|Y →Y (the slope of the curve g(x) is too steep in
the vicinity of the ﬁxed point). But if we choose (7.33) instead, then
g(x) =
√
3
2 x1/2,
g(1)(x) =
√
3
4
1
x1/2 ,
and if Y = [0.7, 0.8], then for x ∈Y, we have g(x) ∈[0.7246, 0.7746] ⊂Y, and
α =
sup
x∈(0.7,0.8)
√
3
4
1
√x = 0.5175.
So, g in (7.33) is contractive on Y. We observe the iterates
x0 = 0.7800
x1 =
√
3
2 x1/2
0
= 0.7649
x2 =
√
3
2 x1/2
1
= 0.7574
x3 =
√
3
2 x1/2
2
= 0.7537
x4 =
√
3
2 x1/2
3
= 0.7518
x5 =
√
3
2 x1/2
4
= 0.7509,
...
which converge to x = 3
4 (i.e., xn →x).
7.4
NEWTON–RAPHSON METHOD
This method is yet another iterative approach to ﬁnding roots of nonlinear equations.
In fact, it is a version of the ﬁxed-point method that was considered in the previous
section. However, it is of sufﬁcient importance to warrant separate consideration
within its own section.
7.4.1
The Method
One way to derive the Newton–Raphson method is by a geometric approach. The
method attempts to solve
f (x) = 0
(7.34)
TLFeBOOK

306
NONLINEAR SYSTEMS OF EQUATIONS
y = f(x)
x
y
0
Line y − f(pn) = f (1)(pn)(x −pn)
[tangent to point (pn, f(pn)]
pn
pn+2
pn+1
f(pn)
Figure 7.3
Geometric interpretation of the Newton–Raphson method.
(x ∈R, and f (x) ∈R) by approximating the root p (i.e., f (p) = 0) by a succession
of x intercepts of tangent lines to the curve y = f (x) at x the current approximation
to p. Speciﬁcally, if pn is the current estimate of p, then the tangent line to the
point (pn, f (pn)) on the curve y = f (x) is
y = f (pn) + f (1)(pn)(x −pn)
(7.35)
(see Fig. 7.3). The next approximation to p is x = pn+1 such that y = 0, that is,
from (7.35)
0 = f (pn) + f (1)(pn)(pn+1 −pn),
so for n ∈Z+
pn+1 = pn −
f (pn)
f (1)(pn).
(7.36)
To start this process off, we need an initial guess at p, that is, we need to select p0.
Clearly this approach requires f (1)(pn) ̸= 0 for all n; otherwise the process will
terminate. Continuation will not be possible, except perhaps by choosing a new
starting point p0. This is one of the ways in which the method can break down.
It might be called premature termination. (Breakdown phenomena are discussed
further later in the text.)
Another derivation of (7.36) is as follows. Recall the theory of Taylor series
from Section 3.5. Suppose that f (x), f (1)(x), and f (2)(x) are all continuous on
[a, b]. Suppose that p is a root of f (x) = 0, and that pn approximates p. From
Taylor’s theorem [i.e., (3.71)]
f (x) = f (pn) + f (1)(pn)(x −pn) + 1
2f (2)(ξ)(x −pn)2,
(7.37)
TLFeBOOK

NEWTON–RAPHSON METHOD
307
where ξ = ξ(x) ∈(pn, x). Since f (p) = 0 from (7.37), we have
0 = f (p) = f (pn) + f (1)(pn)(p −pn) + 1
2f (2)(ξ)(p −pn)2.
(7.38)
If |p −pn| is small, we may neglect that last term of (7.38), and hence
0 ≈f (pn) + f (1)(pn)(p −pn),
implying
p ≈pn −
f (pn)
f (1)(pn).
(7.39)
We treat the right-hand side of (7.39) as pn+1, the next approximation to p. Thus,
we again arrive at (7.36). The assumption that (p −pn)2 is negligible is important.
If pn is not close enough to p, then the method may not converge. In particular,
the choice of starting point p0 is important.
As already mentioned, the Newton–Raphson method is a special instance of the
ﬁxed-point iteration method, where
g(x) = x −
f (x)
f (1)(x).
(7.40)
So
pn+1 = g(pn)
(7.41)
for n ∈Z+. Stopping criteria are (for ϵ > 0)
|pn −pn−1| < ϵ,
(7.42a)
|f (pn)| < ϵ,
(7.42b)

pn −pn−1
pn
 < ϵ,
pn ̸= 0.
(7.42c)
As with the bisection method, we prefer (7.42c).
Theorem 7.4: Convergence Theorem for the Newton–Raphson Method
Let f be continuous on [a, b]. Let f (1)(x), and f (2)(x) exist and be continuous for
all x ∈(a, b). If p ∈[a, b] with f (p) = 0, and f (1)(p) ̸= 0, then there is a δ > 0
such that (7.36) generates sequence (pn) with pn →p for any p0 ∈[p −δ, p + δ].
Proof
We have
pn+1 = g(pn),
n ∈Z+
with g(x) = x −
f (x)
f (1)(x). We need Y = [p −δ, p + δ] ⊂R with g|Y →Y, and g
is contractive on Y. (Then we may immediately apply the convergence results from
the previous section.)
TLFeBOOK

308
NONLINEAR SYSTEMS OF EQUATIONS
Since f (1)(p) ̸= 0, and since f (1)(x) is continuous at p, there will be a δ1 > 0
such that f (1)(x) ̸= 0 for all x ∈[p −δ1, p + δ1] ⊂[a, b], so g is deﬁned and
continuous for x ∈[p −δ1, p + δ1]. Also
g(1)(x) = 1 −[f (1)(x)]2 −f (x)f (2)(x)
[f (1)(x)]2
= f (x)f (2)(x)
[f (1)(x)]2
for x ∈[p −δ1, p + δ1]. In addition, f (2)(x) is continuous on (a, b), so g(1)(x) is
continuous on [p −δ1, p + δ1]. We assume f (p) = 0, so
g(1)(p) = f (p)f (2)(p)
[f (1)(p)]2
= 0.
We have g(1)(x) continuous at x = p, implying that limx→p g(1)(x) = g(1)(p) = 0,
so there is a δ > 0 with 0 < δ < δ1 such that
|g(1)(x)| ≤α < 1
(7.43)
for x ∈[p −δ, p + δ] = Y for some α such that 0 < α < 1. If x ∈Y, then by the
mean-value theorem, there is a ξ ∈(x, p) such that
|g(x) −p| = |g(x) −g(p)| = |g(1)(ξ)||x −p|
≤α|x −p| < |x −p| < δ,
so that we have g(x) ∈Y for all x ∈Y. That is, g|Y →Y. Because of (7.43)
α = sup
x∈Y
|g(1)(x)|,
and 0 < α < 1, so that g is contractive on Y. Immediately, sequence (pn) from
pn+1 = g(pn) for all p0 ∈Y converges to p (i.e., pn →p).
Essentially, the Newton–Raphson method is guaranteed to converge to a root if
p0 is close enough to it and f is sufﬁciently smooth. Theorem 7.4 is weak because
it does not specify how to select p0.
The Newton–Raphson method, if it converges, tends to do so quite quickly.
However, the method needs f (1)(pn) as well as f (pn). It might be the case that
f (1)(pn) requires much effort to evaluate. The secant method is a variation on the
Newton–Raphson method that replaces f (1)(pn) with an approximation. Speciﬁ-
cally, since
f (1)(pn) = lim
x→pn
f (x) −f (pn)
x −pn
,
TLFeBOOK

NEWTON–RAPHSON METHOD
309
if pn−1 ≈pn, then
f (1)(pn) ≈f (pn−1) −f (pn)
pn−1 −pn
.
(7.44)
This is the slope of the chord that connects the points (pn−1, f (pn−1)), and
(pn, f (pn)) on the graph of f (x). We may substitute (7.44) into (7.36) obtaining
(for n ∈N)
pn+1 = pn −(pn −pn−1)f (pn)
f (pn) −f (pn−1) .
(7.45)
We need p0 and p1 to initialize the iteration process in (7.45). Usually these are
chosen to bracket the root, but this does not guarantee convergence. If the method
does converge, it tends to do so more slowly than the Newton–Raphson method, but
this is the penalty to be paid for avoiding the computation of derivatives f (1)(x).
We remark that the method of false position is a modiﬁcation of the secant method
that is guaranteed to converge because successive approximations to p (i.e., pn−1
and pn) are chosen to always bracket the root. But it is possible that convergence
may be slow. We do not cover this method in this book. We merely mention that
it appears elsewhere in the literature [10–12].
7.4.2
Rate of Convergence Analysis
What can be said about the speed of convergence of ﬁxed-point schemes in gen-
eral, and of the Newton–Raphson method in particular? Consider the following
deﬁnition.
Deﬁnition 7.3: Suppose that (pn) is such that pn →p with pn ̸= p (n ∈Z+).
If there are λ > 0, and δ > 0 such that
lim
n→∞
|pn+1 −p|
|pn −p|δ = λ,
then we say that (pn) converges to p of order δ with asymptotic error constant λ.
Additionally
• If δ = 1, we say that (pn) is linearly convergent.
• If δ > 1, we have superlinear convergence.
• If δ = 2, we have quadratic convergence.
From this deﬁnition, if n is big enough, then
|pn+1 −p| ≈λ|pn −p|δ.
(7.46)
Thus, we would like δ to be large and λ to be small for fast convergence. Since
δ is in the exponent, this parameter is more important than λ for determining the
rate of convergence.
TLFeBOOK

310
NONLINEAR SYSTEMS OF EQUATIONS
Consider the ﬁxed-point iteration
pn+1 = g(pn),
where g satisﬁes the requirements of a contraction mapping (so the Banach ﬁxed-
point theorem applies). We can therefore say that
lim
n→∞
|pn+1 −p|
|pn −p|
= lim
n→∞
|g(pn) −g(p)|
|pn −p|
,
(7.47)
so from the mean-value theorem we have ξn between pn and p for which
g(pn) −g(p) = g(1)(ξn)(pn −p),
which, if used in (7.47), implies that
lim
n→∞
|pn+1 −p|
|pn −p|
= lim
n→∞|g(1)(ξn)|.
(7.48)
Because ξn is between pn and p and pn →p, we must have ξn →p as well. Also,
we will assume g(1)(x) is continuous at p, so (7.48) now becomes
lim
n→∞
|pn+1 −p|
|pn −p|
= |g(1)(p)|,
(7.49)
which is a constant. Applying Deﬁnition 7.3, we conclude that the ﬁxed-point
method is typically only linearly convergent because δ = 1, and the asymptotic
error constant is λ = |g(1)(p)|, provided g(1)(p) ̸= 0. However, if g(1)(p) = 0, we
expect faster convergence. It turns out that this is often the case for the Newton–
Raphson method, as will now be demonstrated.
The iterative scheme for the Newton–Raphson method is (7.36), which is a
particular case of ﬁxed-point iteration where now
g(x) = x −
f (x)
f (1)(x),
(7.50)
and for which
g(1)(x) = f (x)f (2)(x)
[f (1)(x)]2 ,
(7.51)
so that g(1)(p) = 0 because f (p) = 0. Thus, superlinear convergence is anticipated
for this particular ﬁxed-point scheme. Suppose that we have the Taylor expansion
g(x) = g(p) + g(1)(p)(x −p) + 1
2g(2)(ξ)(x −p)2
TLFeBOOK

NEWTON–RAPHSON METHOD
311
for which ξ is between p and x. Since g(p) = p and g(1)(p) = 0, this becomes
g(x) = p + 1
2g(2)(ξ)(x −p)2.
For x = pn, this in turn becomes
pn+1 = g(pn) = p + 1
2g(2)(ξn)(p −pn)2
(7.52)
for which ξn lies between p and pn. Equation (7.52) can be rearranged as
pn+1 −p = 1
2g(2)(ξn)(p −pn)2,
and so
lim
n→∞
|pn+1 −p|
|pn −p|2 = 1
2|g(2)(p)|,
(7.53)
since ξn →p, and we are also assuming that g(2)(x) is continuous at p. Imme-
diately, δ = 2, and the Newton–Raphson method is quadratically convergent. The
asymptotic error constant is plainly equal to 1
2|g(2)(p)|, provided g(2)(p) ̸= 0. If
g(2)(p) = 0, then an even higher order of convergence may be expected. It is
emphasized that these convergence results depend on g(x) being smooth enough.
Given this, since the convergence is at least quadratic, the number of accurate dec-
imal digits in the approximation to a root approximately doubles at every iteration.
7.4.3
Breakdown Phenomena
The Newton–Raphson method can fail to converge (i.e., break down) in vari-
ous ways. Premature termination was mentioned earlier. An example appears in
Fig. 7.4. This ﬁgure also illustrates divergence, which means that [if f (p) = 0]
lim
n→∞|pn −p| = ∞.
In other words, the sequence of iterates (pn) generated by the Newton–Raphson
method moves progressively farther and farther away from the desired root p.
Another failure mechanism called oscillation may be demonstrated as follows.
Suppose that
f (x) = x3 −2x + 2, f (1)(x) = 3x2 −2.
(7.54)
Newton’s method for this speciﬁc case is
pn+1 = pn −p3
n −2pn + 2
3p2n −2
= 2p3
n −2
3p2n −2.
(7.55)
If we were to select p0 = 0 as a starting point, then
p0 = 0,
p1 = 1,
p2 = 0,
p3 = 1,
p4 = 0,
p5 = 1, . . . .
TLFeBOOK

312
NONLINEAR SYSTEMS OF EQUATIONS
0
1
2
3
4
5
6
7
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
x
y
y = xe−x
tangent at x = 1
tangent at x = 2
tangent at x = 4
Figure 7.4
Illustration of breakdown phenomena in the Newton–Raphson method; here,
f (x) = xe−x. The ﬁgure shows premature termination if pn = x = 1 since f (1)(1) = 0.
Also shown is divergence for the case where p0 > 1. (See how the x intercepts of the
tangent lines for x > 1 become bigger as x increases.)
More succinctly, pn = 1
2(1 −(−1)n). The sequence of iterates is oscillating; it
does not diverge, and it does not converge, either. The sequence is periodic (it may
be said to be period 2), and in this case quite simple, but far more complicated
oscillations are possible with longer periods.
Premature termination, divergence, and oscillation are not the only possible
breakdown mechanisms. Another possibility is that the sequence (pn) can be
chaotic. Loosely speaking, chaos is a nonperiodic oscillation with a complicated
structure. Chaotic oscillations look a lot like random noise. This will be considered
in more detail in Section 7.6.
7.5
SYSTEMS OF NONLINEAR EQUATIONS
We now extend the ﬁxed-point and Newton–Raphson methods to solving nonlinear
systems of equations. We emphasize two equations in two unknowns (i.e., two-
dimensional problems). But much of what is said here applies to higher dimensions.
7.5.1
Fixed-Point Method
As remarked, it is easier to begin by ﬁrst considering the two-dimensional problem.
More speciﬁcally, we wish to solve
ˆf0(x0, x1) = 0,
ˆf1(x0, x1) = 0,
(7.56)
which we will assume may be rewritten in the form
x0 −f0(x0, x1) = 0,
x1 −f1(x0, x1) = 0,
(7.57)
and we see that solving these is equivalent to ﬁnding where the curves in (7.57)
intersect in R2 (i.e., [x0
x1]T ∈R2). A general picture is in Fig. 7.5. As in the
TLFeBOOK

SYSTEMS OF NONLINEAR EQUATIONS
313
0
x1
x0 −f0(x0, x1) = 0
x0
x1 −f1(x0, x1) = 0
Figure 7.5
Typical curves in R2 corresponding to the system of equations in (7.57).
case of one-dimensional problems, there will usually be more than one way to
rewrite (7.56) in the form of (7.57).
Example 7.6
Suppose
ˆf0(x0, x1) = x0 −x2
0 −1
4x2
1 = 0 ⇒f0(x0, x1) = x2
0 + 1
4x2
1,
ˆf1(x0, x1) = x1 −x2
0 + x2
1 = 0 ⇒f1(x0, x1) = x2
0 −x2
1.
We see that
x2
0 + 1
4x2
1 −x0 = 0 ⇒(x0 −1
2)2 + 1
4x2
1 = 1
4
(an ellipse in R2), and
x2
0 −x2
1 −x1 = 0 ⇒x2
0 −(x1 + 1
2)2 = −1
4
(a hyperbola in R2). These are plotted in Fig. 7.6. The solutions to the system
are the points where the ellipse and hyperbola intersect each other. Clearly, the
solution is not unique.
It is frequently the case that nonlinear systems of equations will have more than
one solution just as this was very possible and very common in the case of a single
equation.
Vector notation leads to compact descriptions. Speciﬁcally, we deﬁne x =
[x0x1]T ∈R2, f0(x) = f0(x0, x1), f1(x) = f1(x0, x1), and
F(x) =

f0(x0, x1)
f1(x0, x1)
	
=

f0(x)
f1(x)
	
.
(7.58)
TLFeBOOK

314
NONLINEAR SYSTEMS OF EQUATIONS
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
−2
−1.5
−1
−0.5
0
0.5
1
x0
x1
Figure 7.6
The curves of Example 7.6 (an ellipse and a hyperbola). The solutions to the
equations in Example 7.6 are the points where these curves intersect.
Then the nonlinear system in (7.57) becomes
x = F(x),
(7.59)
and the ﬁxed point p ∈R2 of F satisﬁes
p = F(p).
(7.60)
We recall that R2 is a normed space if
||x||2 =
1

i=0
x2
i = x2
0 + x2
1
(7.61)
(x ∈R2). We may consider a sequence of vectors (xn) (i.e., xn ∈R2 for n ∈Z+),
and it converges to x iff
lim
n→∞||xn −x|| = 0.
We recall from Chapter 3 that R2 with the norm in (7.61) is a complete space, so
every Cauchy sequence (xn) in it will converge.
As with the scalar case considered in Section 7.3, we consider the sequence of
iterates (pn) such that
pn+1 = F(pn),
n ∈Z+.
The previous statements (and the following theorem) apply if R2 is replaced by Rm
(m > 2); that is, the space can be of higher dimension to accommodate m equations
in m unknowns. Naturally, for Rm the norm in (7.61) must change according to
||x||2 = m−1
k=0 x2
k. The following theorem is really a special instance of the Banach
ﬁxed-point theorem seen earlier.
TLFeBOOK

SYSTEMS OF NONLINEAR EQUATIONS
315
Theorem 7.5: Suppose that R is a closed subset of R2, F|R →R, and F is
contractive on R; then
x = F(x)
has a unique solution p ∈R. The sequence (pn), where
pn+1 = F(pn),
p0 ∈R,
n ∈Z+
(7.62)
is such that
lim
n→∞||pn −p|| = 0
and
||pn −p|| ≤
αn
1 −α ||p1 −p0||,
(7.63)
where ||F(p1) −F(p2)|| ≤α||p1 −p2|| for any p1, p2 ∈R, and 0 < α < 1.
Proof
As noted, this theorem is really a special instance of the Banach ﬁxed-
point theorem (Theorem 7.3), so we only outline the proof.
It was mentioned in Section 7.3 that any closed subset of a complete metric
space is also complete. R2 with norm (7.61) is a complete metric space, and since
R ⊂R2 is closed, R must be complete. According to Theorem 7.3, F has a unique
ﬁxed point p ∈R [i.e., F(p) = p], and sequence (pn) from pn+1 = F(pn) (with
p0 ∈R, and n ∈Z+) converges to p. The error bound
||pn −p|| ≤
αn
1 −α ||p1 −p0||
is an immediate consequence of Corollary 7.1.
A typical choice for R would be the bounded and closed rectangular region
R = {[x0x1]T |a0 ≤x0 ≤b0, a1 ≤x1 ≤b1}.
(7.64)
The next theorem applies the Schwarz inequality of Chapter 1 to the estimation
of α. It must be admitted that applying the following theorem is often quite difﬁcult
in practice, and in the end it is often better to simply implement the iterative
method in (7.62) and experiment with it rather than go through the laborious task
of computing α according to the theorem’s dictates. However, exceptions cannot
be ruled out, and so knowledge of the theorem might be helpful.
Theorem 7.6: Suppose that R ⊂R2 is as deﬁned in (7.64); then, if
α = max
x∈R
 ∂f0
∂x0
!2
+
 ∂f0
∂x1
!2
+
 ∂f1
∂x0
!2
+
 ∂f1
∂x1
!2	1/2
,
(7.65)
TLFeBOOK

316
NONLINEAR SYSTEMS OF EQUATIONS
we have
||F(x1) −F(x2)|| ≤α||x1 −x2||
for all x1, x2 ∈R.
Proof
We use a two-dimensional version of the Taylor expansion theorem,
which we will not attempt to justify in this book.
Given x1 = [x1,0 x1,1]T , x2 = [x2,0 x2,1]T , there is a point ξ ∈R on the line
segment that joins x1 to x2 such that
F(x1) = F(x2) + F (1)(ξ)(x1 −x2)
= F(x2) +


∂f0(ξ)
∂x0
∂f0(ξ)
∂x1
∂f1(ξ)
∂x0
∂f1(ξ)
∂x1


 x1,0 −x2,0
x1,1 −x2,1

so
||F(x1) −F(x2)||2 =
∂f0(ξ)
∂x0
(x1,0 −x2,0) + ∂f0(ξ)
∂x1
(x1,1 −x2,1)
2
+
∂f1(ξ)
∂x0
(x1,0 −x2,0) + ∂f1(ξ)
∂x1
(x1,1 −x2,1)
2
≤
 ∂f0(ξ)
∂x0
!2
+
 ∂f0(ξ)
∂x1
!2	
||x1 −x2||2
+
 ∂f1(ξ)
∂x0
!2
+
 ∂f1(ξ)
∂x1
!2	
||x1 −x2||2
via the Schwarz inequality (Theorem 1.1). Consequently
||F(x1) −F(x2)||2 ≤
 ∂f0(ξ)
∂x0
!2
+
 ∂f0(ξ)
∂x1
!2
+
 ∂f1(ξ)
∂x0
!2
+
 ∂f1(ξ)
∂x1
!2	
||x1 −x2||2
≤

max
x∈R
 ∂f0(x)
∂x0
!2
+
 ∂f0(x)
∂x1
!2
+
 ∂f1(x)
∂x0
!2
+
 ∂f1(x)
∂x1
!2	
||x1 −x2||2
= α2||x1 −x2||2.
TLFeBOOK

SYSTEMS OF NONLINEAR EQUATIONS
317
As with the one-dimensional Taylor theorem from Chapter 3, our application of it
here assumes that F is sufﬁciently smooth.
To apply Theorem 7.5, we try to select F and R so that 0 < α < 1. As already
noted, this is often done experimentally. The iterative process (7.62) can give us
an algorithm only if we have a stopping criterion. A good choice is (for suitable
ϵ > 0) to stop iterating when
||pn −pn−1||
||pn||
< ϵ,
pn ̸= 0.
(7.66)
It is possible to use norms in (7.66) other than the one deﬁned by (7.61). (A
Chebyshev norm might be a good alternative.) Since our methodology is often to
make guesses about F and R, it is very possible that a given guess will be wrong.
In other words, convergence may never occur. Thus, a loop that implements the
algorithm should also be programmed to terminate when the number of iterations
exceeds some reasonable threshold. In this event, the program must also be written
to print a message saying that convergence has not occurred because the upper limit
on the allowed number of iterations was exceeded. This is an important example
of exception handling in numerical computing.
Example 7.7
If we consider f0(x0, x1) and f1(x0, x1) as in Example 7.6, then
the sequence of iterates for the starting vector p0 = [1
1]T is
p1,0 = 1.2500,
p1,1 = 0.0000
p2,0 = 1.5625,
p2,1 = 1.5625
p3,0 = 3.0518,
p3,1 = 0.0000
p4,0 = 9.3132,
p4,1 = 9.3132
...
This vector sequence is not converging to the root near [0.9
0.5]T (see Fig. 7.6).
Many experiments with different starting values do not lead to a solution. So we
need to change F.
We may rewrite x0 −x2
0 −1
4x2
1 = 0 as
x0 =
x2
0
x2
0 + 1
4x2
1
= f0(x0, x1),
and we may rewrite x1 −x2
0 + x2
1 = 0 as
x1 =
x2
0
1 + x1
= f1(x0, x1).
TLFeBOOK

318
NONLINEAR SYSTEMS OF EQUATIONS
These redeﬁne F [recall (7.58) for the general form of F]. For this choice of F
and again with p0 = [1
1]T , the sequence of vectors is
p1,0 = 0.8000,
p1,1 = 0.5000
p2,0 = 0.9110,
p2,1 = 0.4627
p3,0 = 0.9480,
p3,1 = 0.5818
p4,0 = 0.9140,
p4,1 = 0.5682
...
p14,0 = 0.9189,
p14,1 = 0.5461
In this case the vector sequence now converges to the root with four-decimal-place
accuracy by the 14th iteration.
7.5.2
Newton–Raphson Method
Consider yet again two equations in two unknowns
f0(x0, x1) = 0,
f1(x0, x1) = 0,
(7.67)
each of which deﬁnes a curve in the plane (again x = [x0
x1]T ∈R2). Solutions
to (7.67) are points of intersection of the two curves in R2. We will denote a point
of intersection by p = [p0
p1]T ∈R2, which is a root of the system (7.67).
Suppose that x0 = [x0,0
x0,1]T is an initial approximation to the root p. Assume
that f0 and f1 are smooth enough to possess a two-dimensional Taylor series expan-
sion,
f0(x0, x1) = f0(x0,0, x0,1) + ∂f0
∂x0
(x0,0, x0,1)(x0 −x0,0)
+ ∂f0
∂x1
(x0,0, x0,1)(x1 −x0,1) + 1
2!

∂2f0(x0,0, x0,1)
∂x2
0
(x0 −x0,0)2
+ 2∂2f0(x0,0, x0,1)
∂x0∂x1
(x0 −x0,0)(x1 −x0,1)
+ ∂2f0(x0,0, x0,1)
∂x2
1
(x1 −x0,1)2

+ · · ·
f1(x0, x1) = f1(x0,0, x0,1) + ∂f1
∂x0
(x0,0, x0,1)(x0 −x0,0)
+ ∂f1
∂x1
(x0,0, x0,1)(x1 −x0,1) + 1
2!

∂2f1(x0,0, x0,1)
∂x2
0
(x0 −x0,0)2
TLFeBOOK

SYSTEMS OF NONLINEAR EQUATIONS
319
+ 2∂2f1(x0,0, x0,1)
∂x0∂x1
(x0 −x0,0)(x1 −x0,1)
+ ∂2f1(x0,0, x0,1)
∂x2
1
(x1 −x0,1)2

+ · · ·
which have the somewhat more compact form using vector notation as
f0(x) = f0(x0) + ∂f0(x0)
∂x0
(x0 −x0,0) + ∂f0(x0)
∂x1
(x1 −x0,1)
+ 1
2!

∂2f0(x0)
∂x2
0
(x0 −x0,0)2 + 2∂2f0(x0)
∂x0∂x1
(x0 −x0,0)(x1 −x0,1)
+ ∂2f0(x0)
∂x2
1
(x1 −x0,1)2

+ · · ·
(7.68a)
f1(x) = f1(x0) + ∂f1(x0)
∂x0
(x0 −x0,0) + ∂f1(x0)
∂x1
(x1 −x0,1)
+ 1
2!

∂2f1(x0)
∂x2
0
(x0 −x0,0)2 + 2∂2f1(x0)
∂x0∂x1
(x0 −x0,0)(x1 −x0,1)
+ ∂2f1(x0)
∂x2
1
(x1 −x0,1)2

+ · · · .
(7.68b)
If x0 is close to p, then from (7.68), we have
0 = f0(p) ≈f0(x0) + ∂f0(x0)
∂x0
(p0 −x0,0) + ∂f0(x0)
∂x1
(p1 −x0,1)
+ 1
2!

∂2f0(x0)
∂x2
0
(p0 −x0,0)2 + 2∂2f0(x0)
∂x0∂x1
(p0 −x0,0)(p1 −x0,1)
+ ∂2f0(x0)
∂x2
1
(p1 −x0,1)2

+ · · ·
(7.69a)
0 = f1(p) ≈f1(x0) + ∂f1(x0)
∂x0
(p0 −x0,0) + ∂f1(x0)
∂x1
(p1 −x0,1)
+ 1
2!

∂2f1(x0)
∂x2
0
(p0 −x0,0)2 + 2∂2f1(x0)
∂x0∂x1
(p0 −x0,0)(p1 −x0,1)
+ ∂2f1(x0)
∂x2
1
(p1 −x0,1)2

+ · · ·
(7.69b)
TLFeBOOK

320
NONLINEAR SYSTEMS OF EQUATIONS
If we neglect the higher-order terms (second derivatives and higher-order deriva-
tives), then we obtain
∂f0(x0)
∂x0
(p0 −x0,0) + ∂f0(x0)
∂x1
(p1 −x0,1) ≈−f0(x0),
(7.70a)
∂f1(x0)
∂x0
(p0 −x0,0) + ∂f1(x0)
∂x1
(p1 −x0,1) ≈−f1(x0).
(7.70b)
As a shorthand notation, deﬁne
fi,j(x0) = ∂fi(x0)
∂xj
(7.71)
so (7.70) becomes
(p0 −x0,0)f0,0(x0) + (p1 −x0,1)f0,1(x0) ≈−f0(x0),
(7.72a)
(p0 −x0,0)f1,0(x0) + (p1 −x0,1)f1,1(x0) ≈−f1(x0).
(7.72b)
Multiply (7.72a) by f1,1(x0), and multiply (7.72b) by f0,1(x0). Subtracting the
second equation from the ﬁrst results in
(p0 −x0,0)[f0,0(x0)f1,1(x0) −f1,0(x0)f0,1(x0)]
≈−f0(x0)f1,1(x0) + f1(x0)f0,1(x0).
(7.73a)
Now multiply (7.72a) by f1,0(x0), and multiply (7.72b) by f0,0(x0). Subtracting
the second equation from the ﬁrst results in
(p1 −x0,1)[f0,1(x0)f1,0(x0) −f0,0(x0)f1,1(x0)]
≈−f0(x0)f1,0(x0) + f1(x0)f0,0(x0).
(7.73b)
From (7.73b), we obtain
p0 ≈x0,0 + −f0(x0)f1,1(x0) + f1(x0)f0,1(x0)
f0,0(x0)f1,1(x0) −f0,1(x0)f1,0(x0),
(7.74a)
p1 ≈x0,1 + −f1(x0)f0,0(x0) + f0(x0)f1,0(x0)
f0,0(x0)f1,1(x0) −f0,1(x0)f1,0(x0).
(7.74b)
We may assume that the right-hand side of (7.74) is the next approximation to p
x1,0 ≈x0,0 + −f0f1,1 + f1f0,1
f0,0f1,1 −f0,1f1,0

x0
,
(7.75a)
x1,1 ≈x0,1 + −f1f0,0 + f0f1,0
f0,0f1,1 −f0,1f1,0

x0
(7.75b)
TLFeBOOK

SYSTEMS OF NONLINEAR EQUATIONS
321
(x1 = [x1,0x1,1]T ), where the functions and derivatives are to be evaluated at x0.
We may continue this process to generate (xn) for n ∈Z+ (so in general xn =
[xn,0
xn,1]T ) according to
xn+1,0 = xn,0 + −f0(xn)f1,1(xn) + f1(xn)f0,1(xn)
f0,0(xn)f1,1(xn) −f0,1(xn)f1,0(xn),
(7.76a)
xn+1,1 = xn,1 + −f1(xn)f0,0(xn) + f0(xn)f1,0(xn)
f0,0(xn)f1,1(xn) −f0,1(xn)f1,0(xn).
(7.76b)
As in the previous subsection, we deﬁne
F(xn) =

f0(xn,0, xn,1)
f1(xn,0, xn,1)
	
=

f0(xn)
f1(xn)
	
.
(7.77)
Also
F (1)(xn) =

f0,0(xn)
f0,1(xn)
f1,0(xn)
f1,1(xn)
	
= JF (xn),
(7.78)
which is the Jacobian matrix JF evaluated at x = xn. We see that
[JF (xn)]−1 =
1
f0,0(xn)f1,1(xn) −f0,1(xn)f1,0(xn)

f1,1(xn)
−f0,1(xn)
−f1,0(xn)
f0,0(xn)
	
,
(7.79)
so in vector notation (7.76) becomes
xn+1 = xn −[JF (xn)]−1F(xn)
(7.80)
for n ∈Z+. If xn ∈Rm (i.e., if we consider m equations in m unknowns), then
JF (xn) =


f0,0(xn)
f0,1(xn)
· · ·
f0,m−1(xn)
f1,0(xn)
f1,1(xn)
· · ·
f1,m−1(xn)
...
...
...
fm−1,0(xn)
fm−1,1(xn)
· · ·
fm−1,m−1(xn)

,
(7.81a)
and
F(xn) =


f0(xn)
f1(xn)
...
fm−1(xn)

.
(7.81b)
Of course, xn = [xn,0 xn,1 · · · xn,m−1]T ∈Rm.
TLFeBOOK

322
NONLINEAR SYSTEMS OF EQUATIONS
Equation (7.80) reduces to (7.46) when we have only one equation in one
unknown. We see that the method will fail if JF (xn) is singular at xn. As in
the one-dimensional problem of Section 7.4, the success of the method depends
on picking a good starting point x0. If convergence occurs, then it is quadratic
as in the one-dimensional (i.e., scalar) case. It is sometimes possible to force the
method to converge even if the starting point is poorly selected, but this will not
be considered here. The computational complexity of the method is quite high. If
xn ∈Rm, then from (7.80) and (7.81), we require m2 + m function evaluations,
and we need to invert an m × m Jacobian matrix at every iteration. We know from
Chapter 4 that matrix inversion needs O(m3) operations. Ill conditioning of the
Jacobian is very much a potential problem as well.
Example 7.8
Refer to the examples in Section 7.5.1. In Example 7.6 there we
wanted to solve
f0(x0, x1) = x0 −x2
0 −1
4x2
1 = 0, f1(x0, x1) = x1 −x2
0 + x2
1 = 0.
Consequently
f0(xn) = xn,0 −x2
n,0 −1
4x2
n,1, f1(xn) = xn,1 −x2
n,0 + x2
n,1,
and the derivatives are
f0,0(xn) = 1 −2xn,0, f1,0(xn) = −2xn,0
f0,1(xn) = −1
2xn,1, f1,1(xn) = 1 + 2xn,1.
Via (7.76), the desired equations are
xn+1,0 = xn,0
+
−(xn,0 −x2
n,0 −1
4x2
n,1)(1 + 2xn,1) + (xn,1 −x2
n,0 + x2
n,1)(−1
2xn,1)
(1 −2xn,0)(1 + 2xn,1) −xn,0xn,1
,
(7.82a)
xn+1,1 = xn,1
+
−(xn,1 −x2
n,0 + x2
n,1)(1 −2xn,0) + (xn,0 −x2
n,0 −1
4x2
n,1)(−2xn,0)
(1 −2xn,0)(1 + 2xn,1) −xn,0xn,1
. (7.82b)
If we execute the iterative procedure in (7.82), we obtain
x0,0 = 0.8000,
x0,1 = 0.5000
x1,0 = 0.9391,
x1,1 = 0.5562
x2,0 = 0.9193,
x2,1 = 0.5463
x3,0 = 0.9189,
x3,1 = 0.5461
TLFeBOOK

CHAOTIC PHENOMENA AND A CRYPTOGRAPHY APPLICATION
323
We see that the answer is correct to four decimal places in only three iterations.
This is much faster than the ﬁxed-point method seen in Example 7.7.
7.6
CHAOTIC PHENOMENA AND A CRYPTOGRAPHY APPLICATION
Iterative processes such as xn+1 = g(xn) (which includes the Newton–Raphson
method) can converge to a ﬁxed point [i.e., to x such that g(x) = x], or they
can fail to do so in various ways. This was considered in Section 7.4.3. We are
interested here in the case where (xn) is a chaotic sequence, in which case g is
often said to be a chaotic map. Formal deﬁnitions exist for chaotic maps [13,
p. 50]. However, these are rather technical. They are also difﬁcult to apply except
in relatively simple cases. We shall therefore treat chaos in an intuitive/empirical
(i.e., experimental) manner for simplicity.
In Section 7.3 we considered examples based on the logistic map
g(x) = λx(1 −x)
(7.83)
(recall Examples 7.3–7.5). Suppose that λ = 4. Figure 7.7 shows two output
sequences from this map for two slightly different initial conditions. Plot (a) shows
for x0 = xa
0 = 0.745; while plot (b), x0 = xb
0 = 0.755. We see that |xa
0 −xb
0| =
0.01, yet after only a few iterations, the two sequences are very different from each
other. This is one of the distinguishing features of chaos: sensitive dependence
of the resulting sequence to minor changes in the initial conditions. For λ = 4,
we have g|[0, 1] →[0, 1], so divergence is impossible. Chaotic sequences do not
diverge. They remain bounded; that is, there is a M ∈R such that 0 < M < ∞
with |xn| ≤M for all n ∈Z+. But the sequence does not converge, and it is not
periodic, either. In fact, the plots in Fig. 7.7 show that the elements of sequence
(xn) seem to wander around rather aimlessly (i.e., apparently randomly). This wan-
dering behavior has been observed in the past [14, p. 167], but was not generally
recognized as being a chaotic phenomenon until more recently.
It has been known for a very long time that effective cryptographic systems
should exploit randomness [15]. Since chaotic sequences have apparently random
qualities, it is not surprising that they have been proposed as random-number gen-
erators (or as pseudo-random-number generators) for applications in cryptography
[16, 17]. However, it is presently a matter of legitimate controversy regarding just
how secure a cryptosystem based on chaos can be. One difﬁculty is as follows.
Nominally, a chaotic map g takes on values from the set of real numbers. But if
such a map is implemented on a digital computer, then, since all computers are
ﬁnite-state machines, any chaotic sequence will not be truly chaotic as it will even-
tually repeat. Short period sequences are cryptographically weak (i.e., not secure).
There is presently no effective procedure (beyond exhaustive searching) for deter-
mining when this difﬁculty will arise in a chaos-based system. This is not the only
problem (see p. 1507 of Ref. 17 for others).
Two speciﬁc chaos-based cryptosystems are presented by De Angeli et al. [18]
and Papadimitriou et al. [19]. (There have been many others proposed in recent
TLFeBOOK

324
NONLINEAR SYSTEMS OF EQUATIONS
0
10
20
30
40
50
60
70
80
90
100
0
0.2
0.4
0.6
0.8
1
n
xn
x0 = 0.745
0
10
20
30
40
50
60
70
80
90
100
0
0.2
0.4
0.6
0.8
1
n
xn
x0 = 0.755
(a)
(b)
Figure 7.7
Output sequence from the logistic map g(x) = 4x(1 −x) with initial conditions
x0 = 0.745 (a) and x0 = 0.755 (b). Observe that although these initial conditions are close
together, the resulting sequences become very different from each other after only a few
iterations.
years.) Key size is the number of different possible encryption keys available in the
system. Naturally, this number should be large enough to prevent a codebreaker
(eavesdropper, cryptanalyst) from guessing the correct key. However, it is also well
known that a large key size is most deﬁnitely not a guarantee that the cryptosystem
will be secure. Papadimitriou et al. [19] demonstrate that their system has a large
key size, but their security analysis [19] did not go beyond this. Their system [19]
seems difﬁcult to analyze, however. In what follows we shall present some analysis
of the system in De Angeli et al. [18], and see that if implemented on a digital
computer, it is not really very secure. We begin with a description of their system
[18]. Their method in [18] is based on the H´enon map (see Fig. 7.8), which is a
mapping deﬁned on R2 according to
x0,n+1 = 1 −αx2
0,n + x1,n
x1,n+1 = βx0,n
(7.84)
for n ∈Z+ (so [x0,nx1,n]T ∈R2), and which is known to be chaotic in some neigh-
borhood of
α = 1.4,
β = 0.3
(7.85)
TLFeBOOK

CHAOTIC PHENOMENA AND A CRYPTOGRAPHY APPLICATION
325
0
20
40
60
80
100
120
140
160
180
200
−1.5
−1
−0.5
0
0.5
1
1.5
n
x0,n
0
20
40
60
80
100
120
140
160
180
200
−0.3
−0.2
−0.1
0
0.1
0.2
0.3
0.4
n
x1,n
Figure 7.8
Typical state sequences from the H´enon map for α = 1.45 and β = 0.25, with
initial conditions x0,0 = −0.5 and x1,0 = 0.2.
so that constants α and β form the encryption key for the system. No choice in the
allowed neighborhood is a valid key. An immediate problem is that there seems
to be no detailed description of which points are allowed. A particular choice of
key should therefore be tested to see if the resulting sequence is chaotic. In what
follows the output sequence from the map is deﬁned to be
yn = x0,n.
(7.86)
The vector xn = [x0,n
x1,n]T is often called a state vector. The elements are state
variables.
The encryption algorithm works by mixing the chaotic sequence of (7.86) with
the message sequence, which we shall denote by (sn). The mixing (described below)
yields the cyphertext sequence, denoted (cn). A problem is that the receiver must
somehow recover the original message (sn) from the cyphertext (cn) using knowl-
edge of the encryption algorithm, and the key {α, β}.
Consider the following mapping:
ˆx0,n+1 = 1 −αy2
n + ˆx1,n,
ˆx1,n+1 = βyn.
(7.87)
Note that this mapping is of the same form as (7.84) except that x0,n is replaced
by yn, but from (7.86) these are the same (nominally). The mapping in (7.84)
TLFeBOOK

326
NONLINEAR SYSTEMS OF EQUATIONS
represents a physical system (or piece of software) at the transmitter, while (7.87)
is part of the receiver (hardware or software). Now deﬁne the error sequences
δxi,n = ˆxi,n −xi,n
(i ∈{0, 1})
(7.88)
for which it is possible to show [using (7.84), (7.86), and (7.87)] that
 δx0,n+1
δx1,n+1

=
 0
1
0
0

#
$%
&
=A
 δx0,n
δx1,n

.
(7.89)
We observe that A2 = 0. Matrix A is an example of a nilpotent matrix for this
reason. This immediately implies that the error sequences go to zero in at most
two iterations (i.e., two steps):
 δx0,n+2
δx1,n+2

=
 0
1
0
0
  0
1
0
0
  δx0,n
δx1,n

=
 0
0

.
This fact tells us that if (yn) is generated at the transmitter, and sent over the
communications channel to the receiver, then the receiver may perfectly recover2
the state sequence of the transmitter in not more than two steps. This is called dead-
beat synchronization. The system in (7.87) is a speciﬁc example of a nonlinear
observer for a nonlinear dynamic system. There is a well-developed theory of
observers for linear dynamic systems [20]. The notion of an observer is a control
systems concept, so we infer that control theory is central to the problem of applying
chaotic systems to cryptographic applications.
All of this suggests the following algorithm for encrypting a message. Assume
that we wish to encrypt a length N message sequence (sn); that is, we only have
the elements s0, s1, . . . , sN−2, sN−1.
1. The transmitter generates and transmits y0, y1 according to (7.84) and (7.86).
The transmitter also generates y2, y3, . . . , yN, yN+1, but these are not trans-
mitted.
2. The transmitter sends the cyphertext sequence
cn = yn+2
sn
(7.90)
for n = 0, 1, . . . , N −1. Of course, this assumes we never have sn = 0.
Equation (7.90) is not the only possible means of mixing the message and the
chaotic sequence together. The decryption algorithm at the receiver is as follows:
2This assumes a perfect communications channel (which is not realistic), and that rounding errors in
the computation are irrelevant (which will be true if the receiver implements arithmetic in the same
manner as the transmitter).
TLFeBOOK

CHAOTIC PHENOMENA AND A CRYPTOGRAPHY APPLICATION
327
1. The receiver regenerates the transmitter’s state sequence using (7.87), its
knowledge of the key, and the sequence elements y0, y1. Speciﬁcally, for
n = 0, 1 compute
ˆx0,n+1 = 1 −αy2
n + ˆx1,n,
ˆx1,n+1 = βyn
(7.91a)
while for n = 2, 3, . . . , N −1, N compute
ˆx0,n+1 = 1 −α ˆx2
0,n + ˆx1,n,
ˆx1,n+1 = β ˆx0,n.
(7.91b)
Recover yn for n = 2, 3, . . . , N, N + 1 according to
yn = ˆx0,n.
(7.92)
2. Recover the original message via
sn = yn+2
cn
,
(7.93)
where n = 0, 1, . . . , N −2, N −1.
The initial states at the transmitter and receiver are arbitrary; that is, any x0,0, x1,0
and ˆx1,0 may be selected in (7.84) and (7.91a). The elements y0, y1, and the cypher-
text are sent over the channel. If these are lost because of a corrupt channel, then the
receiver should request retransmission of this information. It is extremely impor-
tant that the transmitter resend the same synchronizing elements y0, y1 and not a
different pair. The reason will become clear below.
We remark that since we are assuming that the algorithms are implemented
on a digital computer, so each sequence element is a binary word of some form.
The synchronizing elements and cyphertext are thus a bitstream. Methods exist to
encode such data for transmission over imperfect channels such that the probability
of successful transmission can be made quite high. These are called error control
coding schemes.
In general, the receiver will fail to recover the message if (1) the way in which
arithmetic is performed by the receiver is not the same as at the transmitter, (2) the
channel corrupts the transmission, or (3) the receiver does not know the key {α, β}.
Item 1 is important since the failure to properly duplicate arithmetic operations at
the receiver will cause machine rounding errors to accumulate and prevent data
recovery. This is really a case of improper synchronization. The plots of Figs. 7.9–
7.11 illustrate some of these effects.
It is noteworthy that even though the receiver may not be a perfect match to the
transmitter, some of the samples (at the beginning of the message) are recovered.
TLFeBOOK

328
NONLINEAR SYSTEMS OF EQUATIONS
0
10
20
30
40
50
60
70
80
90
100
−1
−0.5
0
0.5
1
Time step
Amplitude
0
10
20
30
40
50
60
70
80
90
100
−1
−0.5
0
0.5
1
Time step
Amplitude
(a)
(b)
Figure 7.9
Transmitted (a) and reconstructed (b) message sequences. The sinusoidal mes-
sage sequence (sn) is perfectly reconstructed at the receiver; because the channel is perfect,
the receiver knows the key (here α = 1.4, β = 0.3), and arithmetic is performed in identical
fashion at both the transmitter and receiver.
This is actually an indication that the system is not really very secure. We now
consider security of the method in greater detail.
What is the key size? This question seems hard to answer accurately, but a
simple analysis is as follows. Suppose that the H´enon map is chaotic in the
rectangular region of the αβ plane deﬁned by α ∈[1.4 −α, 1.4 + α], and
β ∈[0.3 −β, 0.3 + β], with α, β > 0. We do not speciﬁcally know the
interval limits, and it is an ad hoc assumption that the chaotic neighborhood is
rectangular (this is a false assumption). Suppose that β is the smallest M-bit binary
fraction such that β ≥0.3 −β, and that β is the largest M-bit binary fraction such
that β ≤0.3 + β. In this case the number of M-bit fractions from 0.3 −β to
0.3 + β is about 2M(β −β) + 1. Thus
2M(β −β + 1) ≈2M[(0.3 + β) −(0.3 −β)] + 1 = 2M+1β + 1 = Kβ,
(7.94a)
and by similar reasoning for α
Kα = 2M+1α + 1,
(7.94b)
TLFeBOOK

CHAOTIC PHENOMENA AND A CRYPTOGRAPHY APPLICATION
329
0
10
20
30
40
50
60
70
80
90
100
−1
−0.5
0
0.5
1
Time step
Amplitude
0
10
20
30
40
50
60
70
80
90
100
−10
−5
0
5
10
15
20
Time step
Amplitude
(a)
(b)
Figure 7.10
Transmitted (a) and reconstructed (b) message sequences. Here, the conditions
of Fig. 7.9 hold except that the receiver uses a mismatched key α = 1.400001, β = 0.300001.
Thus, the message is eventually lost. (Note that the ﬁrst few message samples seem to be
recovered accurately.).
which implies that the key size is (very approximately)
K = KαKβ.
(7.95)
Even if α and β are small, and the structure of the chaotic neighborhood is
not rectangular, we can make M big enough (in principle) to generate a big key
space. Apparently, Ruelle [21, p. 19] has shown that the H´enon map is periodic
(i.e., not chaotic) for α = 1.3, β = 0.3, so the size of the chaotic region is not very
big. It is irregular and “full of holes” (the “holes” are key parameters that don’t
give chaotic outputs). In any case it seems that a large key size is possible. But as
cautioned earlier, this is no guarantee of security.
What does the transmitter send over the channel? This is the same as asking
what the eavesdropper knows. From the encryption algorithm the eavesdropper
knows the synchronizing elements y0, y1, and the cyphertext. The eavesdropper
also knows the algorithm, but not the key. Is this enough to ﬁnd the key? It is now
obvious to ask if knowledge of y0, y1 gives the key away. This question may be
easily (?) resolved as follows.
From (7.84) and (7.86) for n ∈N, we have
yn+1 = 1 −αy2
n + βyn−1.
(7.96)
TLFeBOOK

330
NONLINEAR SYSTEMS OF EQUATIONS
0
10
20
30
40
50
60
70
80
90
100
−1
−0.5
0
0.5
1
Time step
Amplitude
0
10
20
30
40
50
60
70
80
90
100
−2
−1
0
1
2
3
4
Time step
Amplitude
(a)
(b)
Figure 7.11
Transmitted (a) and reconstructed (b) message sequences. Here, the conditions
of Fig. 7.9 hold except the receiver and the transmitter do not perform arithmetic in identical
fashion. In this case the order of two operations was reversed at the receiver. Thus, the
message is eventually lost. (Note again that the ﬁrst few message samples seem to be
recovered accurately.).
The encryption algorithm generates y0, y1, . . . , yN+1, which, from (7.96), must
lead to the key satisfying the linear system of equations


y2
1
−y0
y2
2
−y1
...
...
y2
N−1
−yN−2
y2
N
−yN−1


#
$%
&
=Y
 α
β

# $% &
=a
=


1 −y2
1 −y3
...
1 −yN
1 −yN+1


#
$%
&
=y
.
(7.97)
Compactly, we have Ya = y. This is an overdetermined system. The eavesdropper
has only y0, y1, so from (7.97) the eavesdropper needs to solve

y2
1
−y0
y2
2
−y1
	  α
β

=
 1 −y2
1 −y3

.
(7.98)
TLFeBOOK

CHAOTIC PHENOMENA AND A CRYPTOGRAPHY APPLICATION
331
But the eavesdropper does not know y2, y3 as these were not transmitted. These
elements become mixed with the message, and so are not available to the eaves-
dropper. We immediately conclude that deadbeat synchronization is secure.3 There
is an alternative synchronization scheme [18] that may be proved to be insecure
by this method of analysis.
Does the cyphertext give key information away? This seems not to have a
complete answer either. However, we may demonstrate that the system of D’Angeli
et al. [18] (using deadbeat synchronization) is vulnerable to a known plaintext4
attack. Such a vulnerability is ordinarily sufﬁcient to preclude using a method in
a high-security application. The analysis assumes partial prior knowledge of the
message (sn). Let us speciﬁcally assume that
sn =
p

k=0
aknk,
(7.99)
but we do not know ak or p. That is, the structure of our message is a polyno-
mial sequence, but we do not know more than this. Combining (7.90) with (7.96)
gives us
cnsn = 1 −αc2
n−1s2
n−1 + βcn−2sn−2.
(7.100)
It is not difﬁcult to conﬁrm that
s2
n−1 =
2p

k=0




i+j=k
aiaj



#
$%
&
=bk
(n −1)k
(7.101)
so that (7.100) becomes
p

k=0
aknkcn +
2p

k=0
αbk(n −1)kc2
n−1 −
p

k=0
βak(n −2)kcn−2 = 1.
(7.102)
Remembering that the eavesdropper has cyphertext (cn) (obtained by eavesdrop-
ping), the eavesdropper can use (7.102) to set up a linear system of equations in
the key and message parameters ak. The code is therefore easily broken in this
case.
If the message has a more complex structure, (7.102) will generally be replaced
by some hard-to-solve nonlinear problem wherein the methods of previous sections
(e.g., Newton–Raphson) can be used to break the code. We conclude that, in spite
of technical problems from the eavesdropper’s point of view (ill conditioning,
incomplete cyphertext sequences, etc.), the scheme in Ref. 18 is not secure.
3This conclusion assumes there are no other distinct ways to work with the encryption algorithm
equations in such a manner as to give an equation that an eavesdropper can solve for the key knowing
only y0, y1, and the cyphertext.
4The message is also called plaintext.
TLFeBOOK

332
NONLINEAR SYSTEMS OF EQUATIONS
REFERENCES
1. J. H. Wilkinson, “The Perﬁdious Polynomial,” in Studies in Mathematics, G. H. Golub,
ed., Vol. 24, Mathematical Association of America, 1984.
2. A. M. Cohen, “Is the Polynomial so Perﬁdious?” Numerische Mathematik 68, 225–238
(1994).
3. T. E. Hull and R. Mathon, “The Mathematical Basis and Prototype Implementation of a
New Polynomial Root Finder with Quadratic Convergence,” ACM Trans. Math. Software
22, 261–280 (Sept. 1996).
4. E. Kreyszig, Introductory Functional Analysis with Applications, Wiley, New York,
1978.
5. W. Rudin, Principles of Mathematical Analysis, 3rd ed., McGraw-Hill, New York, 1976.
6. D. C. Youla and H. Webb, “Image Restoration by the Method of Convex Projections:
Part I—Theory,” IEEE Trans. Med. Imag. MI-1, 81–94 (Oct. 1982).
7. A. E. Cetin, O. N. Gerek and Y. Yardimci, “Equiripple FIR Filter Design by the FFT
Algorithm,” IEEE Signal Process. Mag. 14, 60–64 (March 1997).
8. K. Gr¨ochenig, “A Discrete Theory of Irregular Sampling,” Linear Algebra Appl. 193,
129–150 (1993).
9. H. H. Bauschke and J. M. Borwein, “On Projection Algorithms for Solving Convex
Feasibility Problems,” SIAM Rev. 38, 367–426 (Sept. 1996).
10. E. Kreyszig, Advanced Engineering Mathematics, 4th ed., Wiley, New York, 1979.
11. E. Isaacson and H. B. Keller, Analysis of Numerical Methods, Wiley, New York, 1966.
12. F. B. Hildebrand, Introduction to Numerical Analysis, 2nd ed., McGraw-Hill, New York,
1974.
13. R. L. Devaney, An Introduction to Chaotic Dynamical Systems, 2nd ed., Addison-
Wesley, Redwood City, CA, 1989.
14. G. E. Forsythe, M. A. Malcolm and C. B. Moler, Computer Methods for Mathematical
Computations, Prentice-Hall, Englewood Cliffs, NJ, 1977.
15. G. Brassard, Modern Cryptology: A Tutorial, Lecture Notes in Computer Science
(series), Vol. 325, G. Goos and J. Hartmanis, eds., Springer-Verlag, New York, 1988.
16. L. Kocarev, “Chaos-Based Cryptography: A Brief Overview,” IEEE Circuits Syst. Mag.
1(3), 6–21 (2001).
17. F. Dachselt and W. Schwarz, “Chaos and Cryptography,” IEEE Trans. Circuits Syst.
(Part I: Fundamental Theory and Applications) 48, 1498–1509 (Dec. 2001).
18. A. De Angeli, R. Genesio and A. Tesi, “Dead-Beat Chaos Synchronization in Discrete-
Time Systems,” IEEE Trans. Circuits Syst. (Part I: Fundamental Theory and Applica-
tions) 42, 54–56 (Jan. 1995).
19. S. Papadimitriou, A. Bezerianos and T. Bountis, “Secure Communications with Chaotic
Systems of Difference Equations,” IEEE Trans. Comput. 46, 27–38 (Jan. 1997).
20. M. S. Santina, A. R. Stubberud and G. H. Hostetter, Digital Control System Design,
2nd ed., Saunders College Publ., Fort Worth, TX, 1994.
21. D. Ruelle, Chaotic Evolution and Strange Attractors: The Statistical Analysis of Time
Series for Deterministic Nonlinear Systems, Cambridge Univ. Press, New York, 1989.
22. M. Jenkins and J. Traub, “A Three-Stage Variable Shift Algorithm for Polynomial Zeros
and Its Relation to Generalized Rayleigh Iteration,” Numer. Math. 14, 252–263 (1970).
TLFeBOOK

PROBLEMS
333
PROBLEMS
7.1. For the functions and starting intervals below solve f (x) = 0 using the
bisection method. Use stopping criterion (7.13d) with ϵ = 0.005. Do the
calculations with a pocket calculator.
(a) f (x) = loge x + 2x + 1, [a0, b0] = [0.2, 0.3].
(b) f (x) = x3 −cos x, [a0, b0] = [0.8, 1.0].
(c) f (x) = x −e−x/5, [a0, b0] = [ 3
4, 1].
(d) f (x) = x6 −x −1, [a0, b0] = [1, 3
2].
(e) f (x) = sin x
x
+ exp(−x), [a0, b0] = [3, 4].
(f) f (x) = sin x
x
−x + 1, [a0, b0] = [1, 2].
7.2. Consider f (x) = sin(x)/x. This function has a minimum value for some x ∈
[π, 2π]. Use the bisection method to ﬁnd this x. Use the stopping criterion
in (7.13d) with ϵ = 0.005. Use a pocket calculator to do the computations.
7.3. This problem introduces the variation on the bisection method called regula
falsi (or the method of false position). Suppose that [a0, b0] brackets the root
p [i.e., f (p) = 0]. Thus, f (a0)f (b0) < 0. The ﬁrst estimate of the root p,
denoted by p0, is where the line joining the points (a0, f (a0)) and (b0, f (b0))
crosses the x axis.
(a) Show that
p0 = a0 −
b0 −a0
f (b0) −f (a0)f (a0).
(b) Using stopping criterion (7.13d), write pseudocode for the method of
false position.
7.4. In certain signal detection problems (e.g., radar or sonar) the probability of
false alarm (FA) (i.e., of saying that a certain signal is present in the data
when it actually is not) is given by
PFA =
 ∞
η
1
(p/2)2p/2 x
p
2 −1e−x/2 dx,
(7.P.1)
where η is called the detection threshold. If p is an even number, it can be
shown that (7.P.1) reduces to the ﬁnite series
PFA = e−1
2 η
(p/2)−1

k=0
1
k!
η
2
k
.
(7.P.2)
The detection threshold η is a very important design parameter in signal
detectors. Often it is desired to specify an acceptable value for PFA (where
TLFeBOOK

334
NONLINEAR SYSTEMS OF EQUATIONS
0 < PFA < 1), and then it is necessary to solve nonlinear equation (7.P.2)
for η. Let p = 6. Use the bisection method to ﬁnd η for
(a) PFA = 0.001
(b) PFA = 0.01
(c) PFA = 0.1
7.5. We wish to solve
f (x) = x4 −5
2x3 + 5
2x −1 = 0
using a ﬁxed-point method. This requires ﬁnding g(x) such that
g(x) −x = f (x).
Find four different functions g(x).
7.6. Can the ﬁxed-point method be used to ﬁnd the solution to
f (x) = x6 −x −1 = 0
for the root located in the interval [1, 3
2]? Explain.
7.7. Consider the nonlinear equation
f (x) = x −e−x/5 = 0
(which has a solution on interval [ 3
4, 1]). Use (7.32) to estimate α. Recalling
that xn = gnx0 (in the ﬁxed-point method), if x0 = 1, then use (7.22a) to
estimate n so that d(xn, x) < 0.005 [x is the root of f (x) = 0]. Use a pocket
calculator to compute x1, . . . , xn.
7.8. Consider the nonlinear equation
f (x) = x −1 −1
2e−x = 0
(which has a solution on interval [1, 1.2]). Use (7.32) to estimate α. Recalling
that xn = gnx0 (in the ﬁxed-point method), if x0 = 1 then use (7.22a) to
estimate n so that d(xn, x) < 0.001 [x is the root of f (x) = 0]. Use a pocket
calculator to compute x1, . . . , xn.
7.9. Problem 5.14 (in Chapter 5) mentioned the fact that orthogonal polynomi-
als possess the “interleaving of zeros” property. Use this property and the
bisection method to derive an algorithm to ﬁnd the zeros of all Legendre poly-
nomials Pn(x) for n = 1, 2, . . . , N. Express the algorithm in pseudocode. Be
fairly detailed about this.
TLFeBOOK

PROBLEMS
335
7.10. We wish to ﬁnd all of the roots of
f (x) = x3 −3x2 + 4x −2 = 0.
There is one real-valued root, and two complex-valued roots. It is easy to
conﬁrm that f (1) = 0, but use
g(x) = x3 + 3x2 + x + 2
2x2 + 5
to estimate the real root p using ﬁxed-point iteration [i.e, pn+1 = g(pn)].
Using a pocket calculator, compute only p1, p2, p3, and p4, and use the
starting point p0 = 2. Also, use the Newton–Raphson method to estimate
the real root. Again choose p0 = 2, and compute only p1, p2, p3, and p4.
Once the real root is found, ﬁnding the complex-valued roots is easy. Find
the complex-valued roots by making use of the formula for the roots of a
quadratic equation.
7.11. Consider Eq. (7.36). Via Theorem 3.3, there is an αn between root p (f (p) =
0) and the iterate pn such that
f (p) −f (pn) = f (1)(αn)(p −pn).
(a) Show that
p −pn = (pn −pn−1)
'
f (pn)
f (pn−1)
f (1)(pn−1)
f (1)(αn)
(
#
$%
&
=An
.
[Hint: Use the identity 1 = f (pn−1)
f (pn−1)
f (1)(pn−1)
f (1)(pn−1).]
(b) Argue that if convergence is occurring, we have
lim
n→∞|An| = 1.

Hence limn→∞

p−pn
pn−pn−1
 = 1.

(c) An alternative stopping criterion for the Newton–Raphson method is to
stop iterating when
|f (pn)| + |pn −pn−1| < ϵ
for some suitably small ϵ > 0. Is this criterion preferable to (7.42)?
Explain.
TLFeBOOK

336
NONLINEAR SYSTEMS OF EQUATIONS
7.12. For the functions listed below, and for the stated starting value p0, use
the Newton–Raphson method to solve f (p) = 0. Use the stopping crite-
rion (7.42a) with ϵ = 0.001. Perform all calculations using only a pocket
calculator.
(a) f (x) = x + tan x, p0 = 2.
(b) f (x) = x6 −x −1, p0 = 1.5.
(c) f (x) = x3 −cos x, p0 = 1.
(d) f (x) = x −e−x/5, p0 = 1.
7.13. Use the Newton–Raphson method to ﬁnd the real-valued root of the polyno-
mial equation
f (x) = 1 + 1
2x + 1
6x2 + 1
24x3 = 0.
Choose starting point p0 = −2. Iterate 4 times. [Comment: Polynomial f (x)
arises in the stability analysis of a numerical method for solving ordinary
differential equations. This will be seen in Chapter 10, Eq. (10.83).]
7.14. Write a MATLAB function to solve Problem 7.4 using the Newton–Raphson
method. Use the stopping criterion (7.42a).
7.15. Prove the following theorem (Newton–Raphson error formula). Let f (x) ∈
C2[a, b], and f (p) = 0 for some p ∈[a, b]. For pn ∈[a, b] with
pn+1 = pn −
f (pn)
f (1)(pn),
there is a ξn between p and pn such that
p −pn+1 = −1
2(p −pn)2 f (2)(ξn)
f (1)(pn).
[Hint: Consider the Taylor series expansion of f (x) about the point x = pn
f (x) = f (pn) + (x −pn)f (1)(pn) + 1
2(x −pn)2f (2)(ξn),
and then set x = p.]
7.16. This problem is about two different methods to compute √x. To begin, recall
that if x is a binary ﬂoating-point number (Chapter 2), then it has the form
x = x0.x1 · · · xt × 2e,
where, since x ≥0, we have x0 = 0, and because of normalization x1 = 1.
Generally, xk ∈{0, 1}, and e is the exponent. If e = 2k (i.e., the exponent
TLFeBOOK

PROBLEMS
337
is even), we do not adjust x. But if e = 2k + 1 (i.e., the exponent is odd),
we shift the mantissa to the right by one bit position so that e becomes
e = 2k + 2. Thus, x now has the form
x = a × 2e,
where a ∈[ 1
4, 1) in general, and e is an even number. Immediately, √x =
√a × 2e/2. From this description we see that any square root algorithm need
work with arguments only on the interval [ 1
4, 1] without loss of generality
(w.l.o.g.).
(a) Finding the square root of x = a is the same problem as solving f (x) =
x2 −a = 0. Show that the Newton–Raphson method for doing so yields
the iterative algorithm
pn+1 = 1
2
 
pn + a
pn
!
,
(7.P.3)
where pn →√a. [Comment: It can be shown via an argument based
on the theorem in Problem 7.15 that to ensure convergence, we should
set p0 = 1
3(2a + 1). A simpler choice for the starting value is p0 =
1
2( 1
4 + 1) = 5
8 (since we know a ∈[ 1
4, 1]).]
(b) Mikami et al. (1992) suggest an alternative algorithm to ﬁnd √x. They
recommend that for a ∈[ 1
4, 1] the square root of a be obtained by the
algorithm
pn+1 = β(a −p2
n) + pn.
(7.P.4)
Deﬁne error sequence (en) as en = √a −pn. Show that
en+1 = βe2
n + (1 −2β√a)en.
[Comment: Mikami et al. recommend that p0 = 0.666667a + 0.354167.]
(c) What condition on β gives quadratic convergence for the algorithm in
(7.P.4)?
(d) Some microprocessors are intended for applications in high-speed digital
signal processing. As such, they tend to be ﬁxed-point machines with a
high-speed hardware multiplier, but no divider unit. Floating-point arith-
metic tends to be avoided in this application context. In view of this,
what advantage might (7.P.4) have over (7.P.3) as a means to compute
square roots?
7.17. Review Problem 7.10. Use x = x0 + jx1 (x0, x1 ∈R) to rewrite the equation
f (x) = x3 −3x2 + 4x −2 = 0 in the form
f0(x0, x1) = 0,
f1(x0, x1) = 0.
TLFeBOOK

338
NONLINEAR SYSTEMS OF EQUATIONS
Use the Newton–Raphson method (as implemented in MATLAB) to solve
this nonlinear system of equations for the complex roots of f (x) = 0. Use the
starting points x0 = [ x0,0
x0,1 ]T = [ 2
2 ]T and x0 = [ 2
−2 ]T .
Output six iterations in both cases.
7.18. Consider the nonlinear system of equations
f (x, y) = x2 + y2 −1 = 0,
g(x, y) = 1
4x2 + 4y2 −1 = 0.
(a) Sketch f and g on the (x, y) plane.
(b) Solve for the points of intersection of the two curves in (a) by hand
calculation.
(c) Write a MATLAB function that uses the Newton–Raphson method to
solve for the points of intersection. Use the starting vectors [x0y0]T =
[±1 ± 1]T . Output six iterations in all four cases.
7.19. If yn+1 = 1 −by2
n and xn =

1
4λ −1
2

yn + 1
2 for n ∈Z+, then, if b =
1
4λ2 −1
2λ, show that
xn+1 = λxn(1 −xn).
7.20. De Angeli, et al. [18] suggest an alternative synchronization scheme (i.e.,
alternative to deadbeat synchronization). This works as follows. Suppose
that at the transmitter
x0,n+1 = 1 −αx2
0,n + x1,n,
x1,n+1 = βx0,n,
yn = 1 −αx2
0,n.
The expression for yn here replaces that in (7.86). At the receiver
ˆx0,n+1 = ˆx1,n + yn
ˆx1,n+1 = β ˆx0,n.
(a) Error sequences are deﬁned as
δxi,n = ˆxi,n −xi,n
for i ∈{0, 1}. Find conditions on α and β, giving limn→∞||δxn|| =
0
(δxn = [δx0,n
δx1,n]T ).
(b) Prove that using yn = 1 −αx2
0,n to synchronize the receiver and trans-
mitter is not a secure synchronization method [i.e., an eavesdropper may
collect enough elements from (yn) to solve for the key {α, β}].
TLFeBOOK

PROBLEMS
339
7.21. The chaotic encryption scheme of De Angeli et al. [18], which employs
deadbeat synchronization, was shown to be vulnerable to a known plaintext
attack, assuming a polynomial message sequence (recall Section 7.6). Show
that it is vulnerable to a known plaintext attack when the message sequence
is given by
sn = a sin(ωn + φ).
Assuming that the eavesdropper already knows ω and φ, show that a, α and
β can be obtained by solving a third-order linear system of equations. How
many cyphertext elements cn are needed to solve the system?
7.22. Write, compile, and run a C program to implement the De Angeli et al.
chaotic encryption/decryption scheme in Section 7.6. (C is suggested here,
as I am not certain that MATLAB can do the job so easily.) Implement both
the encryption and decryption algorithms in the same program. Keep the
program structure simple and direct (i.e., avoid complicated data structures,
and difﬁcult pointer operations). The program input is plaintext from a ﬁle,
and the output is decrypted cyphertext (also known as “recovered plaintext”)
that is written to another ﬁle. The user is to input the encryption key {α, β},
and the decryption key {α1, β1} at the terminal. Test your program out on
some plaintext ﬁle of your own making. It should include keyboard charac-
ters other than letters of the alphabet and numbers. Of course, your program
must convert character data into a ﬂoating-point format. The ﬂoating-point
numbers are input to the encryption algorithm. Algorithm output is also a
sequence of ﬂoating-point numbers. These are decrypted using the decryp-
tion algorithm, and the resulting ﬂoating-point numbers are converted back
into characters. There is a complication involved in decryption. Recall that
nominally the plaintext is recovered according to
sn = yn+2
cn
for n = 0, 1, . . . , N −1. However, this is a ﬂoating-point operation that
incurs rounding error. It is therefore necessary to implement
sn = yn+2
cn
+ offset
for which offset is a small positive value (e.g., 0.0001). The rationale is
as follows. Suppose that nominally (i.e., in the absence of roundoff) sn =
yn+2/cn = 113.000000. The number 113 is the ASCII (American Standard
Code for Information Interchange) code for some text character. Rounding in
division may instead yield the value 112.999999. The operation of converting
this number to an integer type will give 112 instead of 113. Clearly, the offset
cures this problem. A rounding error that gives, for instance, 113.001000 is
harmless. We mention that if plaintext (sn) is from sampled voice or video,
TLFeBOOK

340
NONLINEAR SYSTEMS OF EQUATIONS
then these rounding issues are normally irrelevant. Try using the following
key sets:
α = 1.399, β = 0.305,
α1 = 1.399, β1 = 0.305,
and
α = 1.399, β = 0.305,
α1 = 1.389, β1 = 0.304.
Of course, perfect recovery is expected for the ﬁrst set, but not the second set.
[Comment: It is to be emphasized that in practice keys must be chosen that
cause the H´enon map to be chaotic to good approximation on the computer.
Keys must not lead to an unstable system, a system that converges to a
ﬁxed point, or to one with a short-period oscillation. Key choices leading to
instability will cause ﬂoating-point overﬂow (leading to a crash), while the
other undesirable choices likely generate security hazards. Ideally (and more
practically) the cyphertext array (ﬂoating-point numbers) should be written to
a ﬁle by an encryption program. A separate decryption program would then
read in the cyphertext from the ﬁle and decrypt it, producing the recovered
plaintext, which would be written to another ﬁle (i.e., three separate ﬁles:
original ﬁle of input plaintext, cyphertext ﬁle, and recovered plaintext ﬁle).]
TLFeBOOK

8
Unconstrained Optimization
8.1
INTRODUCTION
In engineering design it is frequently the case that an optimal design is sought
with respect to some performance criterion or criteria. Problems of this class are
generally referred to as optimal design problems, or mathematical programming
problems. Frequently nonlinearity is involved, and so the term nonlinear program-
ming also appears. There are many subcategories of problem types within this broad
category, and so space limitations will restrict us to an introductory presentation
mainly of so-called unconstrained problems. Even so, only a very few ideas from
within this category will be considered. However, some of the methods treated in
earlier chapters were actually examples of optimal design methods within this cate-
gory. This would include least-squares ideas, for example. In fact, an understanding
of least-squares ideas from the previous chapters helps a lot in understanding the
present one. Although the emphasis here is on unconstrained optimization problems,
some consideration of constrained problems appears in Section 8.5.
8.2
PROBLEM STATEMENT AND PRELIMINARIES
In this chapter we consider the problem
min
x∈Rn f (x)
(8.1)
for which f (x) ∈R. This notation means that we wish to ﬁnd a vector x that min-
imizes the function f (x). We shall follow previous notational practices, so here
x = [x0 x1 . . . xn−2 xn−1]T . In what follows we shall usually assume that f (x)
possesses all ﬁrst- and second-order partial derivatives (with respect to the elements
of x), and that these are continuous functions, too. In the present context the func-
tion f (x) is often called the objective function. For example, in least-squares prob-
lems (recall Chapter 4) we sought to minimize V (a) = ||f (x) −N−1
k=0 akφk(x)||2
with respect to a ∈RN. Thus, V (a) is an objective function. The least-squares
problem had sufﬁcient structure so that a simple solution was arrived at; speciﬁ-
cally, to ﬁnd the optimal vector a (denoted ˆa), all we had to do was solve a linear
system of equations.
An Introduction to Numerical Analysis for Electrical and Computer Engineers, by C.J. Zarowski
ISBN 0-471-46737-5
c⃝2004 John Wiley & Sons, Inc.
341
TLFeBOOK

342
UNCONSTRAINED OPTIMIZATION
Now we are interested in solving more general problems. For example, we might
wish to ﬁnd x = [x0x1]T to minimize Rosenbrock’s function
f (x) = 100(x1 −x2
0)2 + (1 −x0)2.
(8.2)
This is a famous standard test function (taken here from Fletcher [1]) often used
by those who design optimization algorithms to test out their theories. A contour
plot of this function appears in Fig. 8.1. It turns out that this function has a unique
minimum at x = ˆx = [1
1]T . As before, we have used a “hat” symbol to indicate
the optimal solution.
Some ideas from vector calculus are essential to understanding nonlinear opti-
mization problems. Of particular importance is the gradient operator:
∇=
 ∂
∂x0
∂
∂x1
· · ·
∂
∂xn−1
T
.
(8.3)
For example, if we apply this operator to Rosenbrock’s function, then
∇f (x) =


∂f (x)
∂x0
∂f (x)
∂x1

=
 −400x0(x1 −x2
0) −2(1 −x0)
200(x1 −x2
0)

.
(8.4)
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
−0.2
0
0.2
0.4
0.6
0.8
1
x0
x1
Figure 8.1
Contour plot of Rosenbrock’s function.
TLFeBOOK

PROBLEM STATEMENT AND PRELIMINARIES
343
We observe that ∇f (ˆx) = [0
0]T (recall that ˆx = [1
1]T ). In other words, the
gradient of the objective function at ˆx is zero. Intuitively we expect that if ˆx is
to minimize the objective function, then we should always have ∇f (ˆx) = 0. This
turns out to be a necessary but not sufﬁcient condition for a minimum. To see this,
consider g(x) = −x2(x ∈R) for which
∇g(x) = −2x.
Clearly, ∇g(0) = 0 yet x = 0 maximizes g(x) instead of minimizing it. Consider
h(x) = x3, so that
∇h(x) = 3x2
for which ∇h(0) = 0, but x = 0 neither minimizes nor maximizes h(x). In this
case x = 0 corresponds to a one-dimensional version of a saddle point.
Thus, ﬁnding x such that ∇f (x) = 0 generally gives us minima, maxima, or
saddle points. These are collectively referred to as stationary points. We need (if
possible) a condition that tells us whether a given stationary point corresponds to
a minimum. Before considering this matter, we note that another problem is that
a given objective function will often have several local minima. This is illustrated
in Fig. 8.2. The function is a quartic polynomial for which there are two values of
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
2.5
3
−5
0
5
10
15
20
25
30
35
40
x
f(x)
Figure 8.2
A function f (x) = x4 −3.1x3 + 2x + 1 with two local minima. The local
minimizers are near x = −0.5 and x = 2.25. The minimum near x = 2.25 is the unique
global minimum of f (x).
TLFeBOOK

344
UNCONSTRAINED OPTIMIZATION
x such that ∇f (x) = 0. We might denote these by ˆxa and ˆxb. Suppose that ˆxa is the
local minimizer near x = −0.5 and ˆxb is the local minimizer near x = 2.25. We see
that f (ˆxa + δ) > f (ˆxa) for all sufﬁciently small (but nonzero) δ ∈R. Similarly,
f (ˆxb + δ) > f (ˆxb) for all sufﬁciently small (but nonzero) δ ∈R. But we see that
f (ˆxb) < f (ˆxa), so ˆxb is the unique global minimizer for f (x). An algorithm for
minimizing f (x) should seek ˆxb; that is, in general, we seek the global minimizer
(assuming that it is unique). Except in special situations, an optimization algorithm
is usually not guaranteed to do better than ﬁnd a local minimizer, and is not
guaranteed to ﬁnd a global minimizer. Note that it is entirely possible for the
global minimizer not to be unique. A simple example is f (x) = sin(x) for which
the stationary points are x = (2k + 1)π/2 (k ∈Z). The minimizers that are a subset
of these points all give −1 for the value of sin(x).
To determine whether a stationary point is a local minimizer, it is useful to have
the Hessian matrix
∇2f (x) =
∂2f (x)
∂xi∂xj

i,j=0,1,...,n−1
∈Rn×n.
(8.5)
This matrix should not be confused with the Jacobian matrix (seen in Section 7.5.2).
The Jacobian and Hessian matrices are not the same. The veracity of this may be
seen by comparing their deﬁnitions. For the special case of n = 2, we have the
2 × 2 Hessian matrix
∇2f (x) =


∂2f (x)
∂x2
0
∂2f (x)
∂x0∂x1
∂2f (x)
∂x1∂x0
∂2f (x)
∂x2
1

.
(8.6)
If f (x) in (8.6) is Rosenbrock’s function, then (8.6) becomes
∇2f (x) =

1200x2
0 −400x1 + 2
−400x0
−400x0
200

.
(8.7)
The Hessian matrix for f (x) helps in the following manner. Suppose that ˆx is
a local minimizer for objective function f (x). We may Taylor-series-expand f (x)
around ˆx according to
f (ˆx + h) = f (ˆx) + hT ∇f (ˆx) + 1
2hT [∇2f (ˆx)]h + · · · ,
(8.8)
where h ∈Rn. (This will not be formally justiﬁed.) If f (x) is sufﬁciently smooth
and ||h|| is sufﬁciently small, then the terms in (8.8) that are not shown may
be entirely ignored (i.e., we neglect higher-order terms). In other words, in the
neighborhood of ˆx
f (ˆx + h) ≈f (ˆx) + hT ∇f (ˆx) + 1
2hT [∇2f (ˆx)]h
(8.9)
TLFeBOOK

LINE SEARCHES
345
is assumed. But this is the now familiar quadratic form. For convenience we will
(as did Fletcher [1]) deﬁne
G(x) = ∇2f (x),
(8.10)
so (8.9) may be rewritten as
f (ˆx + h) ≈f (ˆx) + hT ∇f (ˆx) + 1
2hT G(ˆx)h.
(8.11)
Sometimes we will write G instead of G(x) if there is no danger of confusion.
In Chapter 4 we proved that (8.11) has a unique minimum iff G > 0. In words,
f (x) looks like a positive deﬁnite quadratic form in the neighborhood of a local
minimizer. Therefore, the Hessian of f (x) at a local minimizer will be positive
deﬁnite and thus represents a way of testing a stationary point to see if it is a
minimizer. (Recall the second-derivative test for the minimum of a single-variable
function from elementary calculus, which is really just a special case of this more
general test.) More formally, we therefore have the following theorem.
Theorem 8.1: A sufﬁcient condition for a local minimizer ˆx is that ∇f (ˆx) = 0,
and G(ˆx) > 0.
This is a simpliﬁed statement of Fletcher’s Theorem 2.1.1 [1, p. 14]. The proof
is really just a more rigorous version of the Taylor series argument just given, and
will therefore be omitted. For convenience we will also deﬁne the gradient vector
g(x) = ∇f (x).
(8.12)
Sometimes we will write g for g(x) if there is no danger of confusion.
If we recall Rosenbrock’s function again, we may now test the claim made earlier
that ˆx = [1
1]T is a minimizer for f (x) in (8.2). For Rosenbrock’s function the
Hessian is in (8.7), and thus we have
G(ˆx) =

802
−400
−400
200

.
(8.13)
The eigenvalues of this matrix are λ = 0.3994, 1002. These are both bigger than
zero so G(ˆx) > 0. We have already remarked that g(ˆx) = 0, so immediately from
Theorem 8.1 we conclude that ˆx = [1
1]T is a local minimizer for Rosenbrock’s
function.
8.3
LINE SEARCHES
In general, for objective function f (x) we wish to allow x ∈Rn; that is, we seek the
minimum of f (x) by performing a search in an n-dimensional vector space. How-
ever, the one-dimensional problem is an important special case. In this section we
begin by considering n = 1, so x is a scalar. The problem of ﬁnding the minimum
TLFeBOOK

346
UNCONSTRAINED OPTIMIZATION
of f (x) [where f (x) ∈R for all x ∈R] is sometimes called the univariate search.
Various approaches exist for the solution of this problem, but we will consider only
the golden ratio search method (sometimes also called the golden section search).
We will then consider the backtracking line search [3] for the case where x ∈Rn
for any n ≥1.
Suppose that we know f (x) has a minimum over the interval [xj
l , xj
u]. Deﬁne
I j = xj
u −xj
l , which is the length of this interval. The index j represents the jth
iteration of the search algorithm, so it represents the current estimate of the interval
that contains the minimum. Select two points xj
a and xj
b (xj
a < xj
b) such that they
are symmetrically placed in the interval [xj
l , xj
u]. Speciﬁcally
xj
a −xj
l = xj
u −xj
b.
(8.14)
A new interval [xj+1
l
, xj+1
u
] is created according to the following procedure, such
that for all j
I j
I j+1 = τ > 1.
(8.15)
If f (xj
a) ≥f (xj
b) then the minimum lies in [xj
a, xj
u], and so I j+1 = xj
u −xj
a. Our
new points are given according to
xj+1
l
= xj
a,
xj+1
u
= xj
u,
xj+1
a
= xj
b
(8.16a)
and
xj+1
b
= xj
a + 1
τ I j+1.
(8.16b)
If f (xj
a) < f (xj
b), then the minimum lies in [xj
l , xj
b], and so I j+1 = xj
b −xj
l . Our
new points are given according to
xj+1
l
= xj
l ,
xj+1
u
= xj
b,
xj+1
b
= xj
a
(8.17a)
and
xj+1
a
= xj
b −1
τ I j+1.
(8.17b)
Since I 0 = x0
u −x0
l (j = 0 indicates the initial case), we must have x0
a = x0
u −1
τ I 0
and x0
b = x0
l + 1
τ I 0. Figure 8.3 illustrates the search procedure. This is for the
particular case (8.17).
Because of (8.15), the rate of convergence to the minimum can be estimated.
But we need to know τ; see the following theorem.
Theorem 8.2: The search interval lengths of the golden ratio search algorithm
are related according to
I j = I j+1 + I j+2
(8.18a)
TLFeBOOK

LINE SEARCHES
347
f(x)
x
0
x j
l
x j
a
x j
b
x j
u
x j
l
+1
x j
b
+1
x j
a
+1
x j
u
+1
Figure 8.3
Illustrating the golden ratio search procedure.
for which the golden ratio τ is given by
τ = 1
2(1 +
√
5) ≈1.62.
(8.18b)
Proof
There are four cases to consider in establishing (8.18a). First, suppose
that f (xj
a) ≥f (xj
b), so I j+1 = xj
u −xj
a with
xj+1
l
= xj
a,
xj+1
a
= xj
b,
xj+1
b
= xj
a + 1
τ I j+1,
xj+1
u
= xj
u.
If it happens that f (xj+1
a
) ≥f (xj+1
b
), then I j+2 = xj+1
u
−xj+1
a
= xj
u −xj
b =
xj
a −xj
l (via xj
a −xj
l = xj
u −xj
b). This implies that I j+1 + I j+2 = xj
u −xj
l = I j.
On the other hand, if it happens that f (xj+1
a
) < f (xj+1
b
), then I j+2 = xj+1
b
−
xj+1
l
= xj+1
u
−xj+1
a
= xj
a −xj
l (via xj+1
a
−xj+1
l
= xj+1
u
−xj+1
b
and xj
a −xj
l =
xj
u −xj
b). Again we have I j+1 + I j+2 = xj
u −xj
l = I j.
Now suppose that f (xj
a) < f (xj
b). Therefore, I j+1 = xj
b −xj
l with
xj+1
l
= xj
l ,
xj+1
a
= xj
b −1
τ I j+1,
xj+1
b
= xj
a,
xj+1
u
= xj
b.
If it happens that f (xj+1
a
) ≥f (xj+1
b
), then I j+2 = xj+1
u
−xj+1
a
= xj+1
b
−
xj+1
l
= xj
a −xj
l = xj
u −xj
b so that I j+1 + I j+2 = xj
u −xj
l = I j. Finally, suppose
that f (xj+1
a
) < f (xj+1
b
), so therefore I j+2 = xj+1
b
−xj+1
l
= xj
a −xj
l = xj
u −xj
b,
TLFeBOOK

348
UNCONSTRAINED OPTIMIZATION
so yet again we conclude that I j+1 + I j+2 = xj
u −xj
l = I j. Thus, (8.18a) is
now veriﬁed.
Since
I j
I j+1 = I j+1
I j+2 = τ
and
I j = I j+1 + I j+2,
we have I j+1 = 1
τ I j and I j+2 = 1
τ I j+1 = 1
τ 2 I j, so
I j = I j+1 + I j+2 = 1
τ I j + 1
τ 2 I j,
immediately implying that
τ 2 = τ + 1,
which yields (8.18b).
The golden ratio has a long and interesting history in art as well as science
and engineering, and this is considered in Schroeder [2]. For example, a famous
painting by Seurat contains ﬁgures that are proportioned according to this ratio
(Fig. 5.2 in Schroeder [2] includes a sketch).
The golden ratio search algorithm as presented so far assumes a user-provided
starting interval. The golden ratio search algorithm has the same drawback as the
bisection method for root ﬁnding (Chapter 7) in that the optimum solution must be
bracketed before the algorithm can be successfully run (in general). On the other
hand, an advantage of the golden ratio search method is that it does not need f (x)
to be differentiable. But the method can be slow to converge, in which case an
improved minimizer that also does not need derivatives can be found in Brent [4].
When setting up an optimization problem it is often advisable to look for ways
to reduce the dimensionality of the problem, if at all possible. We illustrate this
idea with an example that is similar in some ways to the least-squares problem
considered in the beginning of Section 4.6. Suppose that signal f (t) ∈R (t ∈R)
is modeled as
f (t) = a sin
 2π
T t + φ
!
+ η(t),
(8.19)
where η(t) is the term that accounts for noise, interference, or measurement errors
in the data f (t). In other words, our data are modeled as a sinusoid plus noise.
The amplitude a, phase φ, and period T are unknown parameters. We are assumed
only to possess samples of the signal; thus, we have only the sequence elements
fn = f (nTs) = a sin
 2π
T nTs + φ
!
+ η(nTs)
(8.20)
for n = 0, 1, . . . , N −1. The sampling period parameter Ts is assumed to be
known. As before, we may deﬁne an error sequence
en = fn −a sin
 2π
T nTs + φ
!
#
$%
&
= ˆfn
,
(8.21)
TLFeBOOK

LINE SEARCHES
349
where again n = 0, 1, . . . , N −1. The objective function for our problem (using a
least-squares criterion) would therefore be
V (a, T, φ) =
N−1

n=0

fn −a sin
 2π
T nTs + φ
!2
.
(8.22)
This function depends on the unknown model parameters a, T , and φ in a very non-
linear manner. We might formally deﬁne our parameter vector to be x = [a T φ]T ∈
R3. This would lead us to conclude that we must search (by some means) a three-
dimensional space to ﬁnd ˆx to minimize (8.22). But in this special problem it is
possible to reduce the dimensionality of the search space from three dimensions
down to only one dimension. Reducing the problem in this way makes it solvable
using the golden section search method that we have just considered.
Recall the trigonometric identity
sin(A + B) = sin A cos B + cos A sin B.
(8.23)
Using this, we may write
a sin
 2π
T nTs + φ
!
= a cos φ
# $% &
=x0
sin
 2π
T nTs
!
+ a sin φ
# $% &
=x1
cos
 2π
T nTs
!
.
(8.24)
Deﬁne x = [x0
x1]T and
vn =

sin
 2π
T nTs
!
cos
 2π
T nTs
!T
.
(8.25)
We may rewrite en in (8.21) using these vectors:
en = fn −vT
n x.
(8.26)
Note that the approach used here is the same as that used to obtain en in Eqn. (4.99).
Therefore, the reader will probably ﬁnd it useful to review this material now.
Continuing in this fashion, the error vector e = [e0e1 · · · eN−1]T , data vector f =
[f0f1 · · · fN−1]T , and matrix of basis vectors
A =


vT
0
vT
1...
vT
N−1


∈RN×2
all may be used to write
e = f −Ax,
(8.27)
TLFeBOOK

350
UNCONSTRAINED OPTIMIZATION
for which our objective function may be rewritten as
V (x) = eT e = f T f −2xT AT f + xT AT Ax,
(8.28)
which implicitly assumes that we already know the period T . If T were known
in advance, we could use the method of Chapter 4 to minimize (8.28); that is,
the optimum choice (least-squares sense) for x (denoted by ˆx) satisﬁes the linear
system
AT Aˆx = AT f.
(8.29)
Because of (8.24) (with ˆx = [ˆx0
ˆx1]T ), we obtain
ˆx0 = ˆa cos ˆφ,
ˆx1 = ˆa sin ˆφ.
(8.30)
Since we have ˆx from the solution to (8.29), we may use (8.30) to solve for ˆa
and ˆφ.
However, our original problem speciﬁed that we do not know a, φ, or T in
advance. So, how do we exploit these results to determine T as well as a and φ?
The approach is to change how we think about V (x) in (8.28). Instead of thinking
of V (x) as a function of x, consider it to be a function of T , but with x = ˆx as
given by (8.29). From (8.25) we see that vn depends on T , so A is also a function
of T . Thus, ˆx is a function of T , too, because of (8.29). In fact, we may emphasize
this by rewriting (8.29) as
[A(T )]T A(T )ˆx(T ) = [A(T )]T f.
(8.31)
In other words, A = A(T ), and ˆx = ˆx(T ). The objective function V (x) then becomes
V1(T ) = V (ˆx(T )) = f T f −2[ˆx(T )]T AT (T )f + [ˆx(T )]T AT (T )A(T )ˆx(T ).
(8.32)
However, we may substitute (8.31) into (8.32) and simplify with the result that
V1(T ) = f T f −f T A(T )[AT (T )A(T )]−1AT (T )f.
(8.33)
We have reduced the search space from three dimensions down to one, so we
may apply the golden section search algorithm (or some other univariate search
procedure) to objective function V1(T ). This would result in determining the opti-
mum period ˆT [which minimizes V1(T )]. We then compute A( ˆT ), and solve
for ˆx using (8.29) as before. Knowledge of ˆx allows us to determine ˆa and ˆφ
via (8.30).
Figure 8.4 illustrates a typical noisy sinusoid and the corresponding objective
function V1(T ). In this case the parameters chosen were T = 24 h, Ts = 5 min,
a = 1, φ = −π/10 radians, and N = 500. The noise component in the data was
created using MATLAB’s Gaussian random-number generator. The noise variance
is σ 2 = 0.5, and the mean value is zero. We observe that the minimum value of
V1(T ) certainly corresponds to a value of T that is at or close to 24 h.
TLFeBOOK

LINE SEARCHES
351
0
5
10
15
20
25
30
35
40
45
−3
−2
−1
0
1
2
3
4
Time (hours)
fn
16
18
20
22
24
26
28
30
32
250
300
350
400
450
500
Time (hours)
V1(T )
(a)
(b)
Figure 8.4
An example of a noisy sinusoid (a) and the corresponding objective function
V1(T ) (b).
Appendix 8.A contains a sample MATLAB program implementation of the
golden section search algorithm applied to the noisy sinusoid problem depicted
in Fig. 8.4. In golden.m Topt is ˆT , and eta η is the random noise sequence η(nTs).
We will now consider the backtracking line search algorithm. The exposition
will be similar to that in Boyd and Vandenberghe [5].
Now we assume that f (x) is our objective function with x ∈Rn. In a general
line search we seek the minimizing vector sequence (x(k)), k ∈Z+, and x(k) ∈Rn
(i.e., x(k) = [x(k)
0
x(k)
1
· · · x(k)
n−1]T ) constructed according to the iterative process
x(k+1) = x(k) + t(k)s(k),
(8.34)
where t(k) ∈R+, and t(k) > 0 except when x(k) is optimal [i.e., minimizes f (x)].
The vector s(k) is called the search direction, and scalar t(k) is called the step size.
Because line searches (8.34) are descent methods, we have
f (x(k+1)) < f (x(k))
(8.35)
except when x(k) is optimal. The “points” x(k+1) lie along a line in the direction
s(k) in n-dimensional space Rn, and since the minimum of f (x) must lie in the
TLFeBOOK

352
UNCONSTRAINED OPTIMIZATION
direction that satisﬁes (8.35), we must ensure that s(k) satisﬁes [recall (8.12)]
g(x(k))T s(k) = [∇f (x(k))]T s(k) < 0.
(8.36)
Geometrically, the negative-gradient vector −g(x(k)) (which “points down”) makes
an acute angle (i.e., one of magnitude <90◦) with the vector s(k). [Recall (4.130).]
If s(k) satisﬁes (8.36) for f (x(k)) (i.e., f (x) at x = x(k)), it is called a descent direc-
tion for f (x) at x(k). A general descent algorithm has the following pseudocode
description:
Specify starting point x(0) ∈Rn;
k := 0;
while stopping criterion is not met do begin
Find s(k); { determine descent direction }
Find t(k); { line search step }
Compute x(k+1) := x(k) + t(k)s(k);
{ update step }
k := k + 1;
end ;
Newton’s method with the backtracking line search (Section 8.4) is a speciﬁc
example of a descent method. There are others, but these will not be considered in
this book.
Now we need to say more about how to choose the step size t(k) on the assump-
tion that s(k) is known. How to determine the direction s(k) is the subject of
Section 8.4.
So far f |Rn →R. Subsequent considerations are simpliﬁed if we assume that
f (x) satisﬁes the following deﬁnition.
Deﬁnition 8.1: Convex Function
Function f |Rn →R is called a convex
function if for all θ ∈[0, 1], and for any x, y ∈Rn
f (θx + (1 −θ)y) ≤θf (x) + (1 −θ)f (y).
(8.37)
We emphasize that the domain of deﬁnition of f (x) is assumed to be Rn. It is
possible to modify the deﬁnition to accommodate f (x) where the domain of f (x)
is a proper subset of Rn. The geometric meaning of (8.37) is shown in Fig. 8.5
for the case where x ∈R (i.e., one-dimensional case). We see that when f (x)
is convex, the chord, which is the line segment joining (x, f (x)) to (y, f (y)),
always lies above the graph of f (x). Now further assume that f (x) is at least
twice continuously differentiable in all elements of the vector x. We observe that
for any x, y ∈Rn, if f (x) is convex, then
f (y) ≥f (x) + [∇f (x)]T (y −x).
(8.38)
From the considerations of Section 8.2 it is easy to believe that f (x) is convex iff
∇2f (x) ≥0
(8.39)
TLFeBOOK

NEWTON’S METHOD
353
x
f(x)
0
(x, f(x))
(y, f(y))
Figure 8.5
Graph of a convex function f (x) (x ∈R) and the chord that connects the points
(x, f (x)) and (y, f (y)).
for all x ∈Rn; that is, f (x) is convex iff its Hessian matrix is at least positive
semideﬁnite (recall Deﬁnition 4.1). The function f (x) is said to be strongly convex
iff ∇2f (x) > 0 for all x ∈Rn.
The backtracking line search attempts to approximately minimize f (x) along
the line {x + ts|t ≥0} for some given s (search direction at x). The pseudocode
for this algorithm is as follows:
Specify the search direction s;
t := 1;
while (f(x + ts) > f(x) + δt[∇f(x)]Ts)
t := αt;
end ;
In this algorithm 0 < δ < 1
2, and 0 < α < 1. Commonly, δ ∈[0.01, 0.30], and
α ∈[0.1, 0.5]. These parameter ranges will not be justiﬁed here. As suggested
earlier, how to choose s will be the subject of the next section. The method is
called “backtracking” as it begins with t = 1, and then reduces t by factor α until
f (x + ts) ≤f (x) + δt∇T f (x)s. [We have ∇T f (x) = [∇f (x)]T .] Recall that s
is a descent direction so that (8.36) holds, speciﬁcally, ∇T f (x)s < 0, and so if t
is small enough [recall (8.9)], then
f (x + ts) ≈f (x) + t∇T f (x)s < f (x) + δt∇T f (x)s,
which shows that the search must terminate eventually. We mention that the back-
tracking line search will terminate even if f (x) is only “locally convex”—convex
on some proper subset S of Rn. This will happen provided x ∈S in the algorithm.
8.4
NEWTON’S METHOD
Section 8.3 suggests attempting to reduce an n-dimensional search space to a
one-dimensional search space. Of course, this approach seldom works, which is
TLFeBOOK

354
UNCONSTRAINED OPTIMIZATION
why there is an elaborate body of methods on searching for minima in higher-
dimensional spaces. However, these methods are too involved to consider in detail
in this book, and so we will only partially elaborate on an idea from Section 8.2.
The quadratic model from Section 8.2 suggests an approach often called New-
ton’s method. Suppose that x(k) is the current estimate of the sought-after optimum
ˆx. Following (8.11), we have the Taylor approximation
f (x(k) + δ) ≈V (δ) = f (x(k)) + δT g(x(k)) + 1
2δT G(x(k))δ
(8.40)
for which δ ∈Rn since x(k) ∈Rn. Since x(k) is not necessarily the minimum ˆx,
usually g(x(k)) ̸= 0. Vector δ is selected to minimize V (δ), and since this is a
quadratic form, if G(x(k)) > 0, then
G(x(k))δ = −g(x(k)).
(8.41)
The next estimate of ˆx is given by
x(k+1) = x(k) + δ.
(8.42)
Pseudocode for Newton’s method (in its most basic form) is
Input starting point x(0);
k := 0;
While stopping criterion is not met do begin
G(x(k))δ := −g(x(k));
x(k+1) := x(k) + δ;
k := k + 1;
end;
The algorithm will terminate (if all goes well) with the last vector x(k+1) as a good
approximation to ˆx. However, the Hessian G(k) = G(x(k)) may not always be
positive deﬁnite, in which case this method can be expected to fail. Modiﬁcation
of the method is required to guarantee that at least it will converge to a local
minimum. Said modiﬁcations often involve changing the method to work with line
searches (i.e., Section 8.3 ideas). We will now say more about this.
As suggested in Ref. 5, we may combine the backtracking line search with the
basic form of Newton’s algorithm described above. A pseudocode description of
the result is
Input starting point x(0), and a tolerance ϵ > 0;
k := 0;
s(0) := −[G(x(0))]−1g(x(0)); { search direction at x(0)}
λ2 := −gT(x(0))s(0);
while λ2 > ϵ do begin
Use backtracking line search to find t(k) for x(k) and s(k);
x(k+1) := x(k) + t(k)s(k);
s(k+1) := −[G(x(k+1))]−1g(x(k+1));
λ2 := −gT(x(k+1))s(k+1);
k := k + 1;
end;
TLFeBOOK

NEWTON’S METHOD
355
The algorithm assumes that G(x(k)) > 0 for all k. In this case [G(x(k))]−1 > 0 as
well. If we deﬁne (for all x ∈Rn)
||x||2
G(y) = xT [G(y)]−1x,
(8.43)
then ||x||G(y) satisﬁes the norm axioms (recall Deﬁnition 1.3), and is in fact an
example of a weighted norm. But why do we consider λ2 as a stopping criterion in
Newton’s algorithm? If we recall the term 1
2δT G(x(k))δ in (8.40), since δ satisﬁes
(8.41), at step k we must have
1
2δT G(x(k))δ = 1
2gT (x(k))[G(x(k))]−1g(x(k)) = 1
2||g(x(k))||2
G(x(k)) = 1
2λ2. (8.44)
Estimate x(k) of ˆx is likely to be good if (8.44) in particular is small [as opposed
to merely considering squared unweighted norm g(x(k))T g(x(k))]. It is known that
Newton’s algorithm can converge rapidly (quadratic convergence). An analysis
showing this appears in Boyd and Vandenberghe [5] but is omitted here.
As it stands, Newton’s method is computationally expensive since we must solve
the linear system in (8.41) at every step k. This would normally involve applying
the Cholesky factorization algorithm that was ﬁrst mentioned (but not considered
in detail) in Section 4.6. We remark in passing that the Cholesky algorithm will
factorize G(k) according to
G(k) = LDLT ,
(8.45)
where L is unit lower triangular and D is a diagonal matrix. We also mention that
G(k) > 0 iff the elements of D are all positive, so the Cholesky algorithm provides
a built-in positive deﬁniteness test. The decomposition in (8.45) is a variation on
the LU decomposition of Chapter 4. Recall that we proved in Section 4.5 that
positive deﬁnite matrices always possess such a factorization (see Theorems 4.1
and 4.2). So Eq. (8.45) is consistent with this result. The necessity to solve a linear
system of equations at every step makes us wonder if sensitivity to ill-conditioned
matrices is a problem in Newton’s method. It turns out that the method is often
surprisingly resistant to ill conditioning (at least as reported in Ref. 5).
Example 8.1
A typical run of Newton’s algorithm with the backtracking line
search as applied to the problem of minimizing Rosenbrock’s function [Eq. (8.2)]
yields the following output:
k
t(k)
λ2
x(k)
0
x(k)
1
0
1.0000
800.00499
2.0000
2.0000
1
0.1250
1.98757
1.9975
3.9900
2
1.0000
0.41963
1.8730
3.4925
3
1.0000
0.49663
1.6602
2.7110
4
0.5000
0.38333
1.5945
2.5382
5
1.0000
0.21071
1.4349
2.0313
TLFeBOOK

356
UNCONSTRAINED OPTIMIZATION
k
t(k)
λ2
x(k)
0
x(k)
1
6
0.5000
0.14763
1.3683
1.8678
7
1.0000
0.07134
1.2707
1.6031
8
1.0000
0.03978
1.1898
1.4092
9
1.0000
0.01899
1.1076
1.2201
10
1.0000
0.00627
1.0619
1.1255
11
1.0000
0.00121
1.0183
1.0350
12
1.0000
0.00006
1.0050
1.0099
The search parameters selected in this example are α = 0.5, δ = 0.3, and ϵ =
.00001, and the ﬁnal estimate is x(13) = [1.0002
1.0003]T . For these same param-
eters if instead x(0) = [−1
1]T , then 18 iterations are needed, yielding x(18) =
[0.9999
0.9998]T . Figure 8.6 shows the sequence of points x(k) for the case
x(0) = [−1
1]T . The dashed line shows the path from starting point [−1
1]T to
the minimum at [1
1]T , and we see that the algorithm follows the “valley” to the
optimum solution quite well.
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
−0.2
0
0.2
0.4
0.6
0.8
1
x0
x1
Figure 8.6
The sequence of points (x(k)) generated by Newton’s method with the back-
tracking line search as applied to Rosenbrock’s function using the parameters α = 0.5,
δ = 0.3, and ϵ = 0.00001 with x(0) = [−1
1]T (see Example 8.1). The path followed is
shown by the dashed line.
TLFeBOOK

EQUALITY CONSTRAINTS AND LAGRANGE MULTIPLIERS
357
8.5
EQUALITY CONSTRAINTS AND LAGRANGE MULTIPLIERS
In this section we modify the original optimization problem in (8.1) according to
min
x∈Rn f (x)
subject tofi(x) = 0
for all
i = 0, 1, . . . , m −1
,
(8.46)
where f (x) is the objective function as before, and fi(x) = 0 for i = 0, 1, . . . ,
m −1 are the equality constraints. The functions fi|Rn →R are equality constraint
functions. The set F = {x|fi(x) = 0, i = 0, . . . , m −1} is called the feasible set.
We are interested in
ˆf = f (ˆx) = min
x∈F f (x).
(8.47)
There may be more than one x = ˆx ∈Rn satisfying (8.47); that is, the set
ˆX = {x|x ∈F,
f (ˆx) = ˆf }
(8.48)
may have more than one element in it. We assume that our problem yields ˆX with
at least one element in it (i.e., ˆX ̸= ∅).
Equation (8.47) is really a more compact statement of (8.46), and in words states
that any minimizer ˆx of f (x) must also satisfy the equality constraints. We recall
examples of this type of problem from Chapter 4 [e.g., the problem of deriving a
computable expression for κ2(A) and in the proof of Theorem 4.5]. More examples
will be seen later. Generally, in engineering, constrained optimization problems are
more common than unconstrained problems. However, it is important to understand
that algorithms for unconstrained problems form the core of algorithms for solving
constrained problems.
We now wish to make some general statements about how to solve (8.46), and
in so doing we introduce the concept of Lagrange multipliers. The arguments to
follow are somewhat heuristic, and they follow those of Section 9.1 in Fletcher [1].
Suppose that ˆx ∈Rn is at least a local minimizer for objective function f (x) ∈
R. Analogously to (8.9), we have
fi(ˆx + δ) ≈fi(ˆx) + gT
i (ˆx)δ + 1
2δT [∇2fi(ˆx)]δ,
(8.49)
where δ ∈Rn is some incremental step away from ˆx, and gi(x) = ∇fi(x) ∈Rn
(i = 0, 1, . . . , m −1) is the gradient of the ith constraint function at x. A feasible
incremental step δ must yield ˆx + δ ∈F, and so must satisfy
fi(ˆx + δ) = fi(ˆx) = 0
(8.50)
for all i. From (8.49) this implies the condition that δ must lie along feasible
direction s ∈Rn (at x = ˆx) such that
gT
i (ˆx)s = 0
(8.51)
TLFeBOOK

358
UNCONSTRAINED OPTIMIZATION
again for all i. [We shall suppose that the vectors gi(ˆx) = ∇f (ˆx) are linearly
independent for all i.] Recalling (8.36), if s were also a descent direction at ˆx, then
gT (ˆx)s < 0
(8.52)
would hold (g(x) = ∇f (x) ∈Rn). In this situation δ would reduce f (x), as δ is
along direction s. But this is impossible since we have assumed that ˆx is a local
minimizer for f (x). [For any s at ˆx, we expect to have gT (ˆx)s = 0.] Consequently,
no direction s can satisfy (8.51) and (8.52) simultaneously. This statement remains
true if g(ˆx) is a linear combination of gi(ˆx), that is, if, for suitable ˆλi ∈R we have
g(ˆx) =
m−1

i=0
ˆλigi(ˆx).1
(8.53)
Thus, a necessary condition for ˆx to be a local minimizer (or, more generally, a
stationary point) of f (x) is that [rewriting (8.53)]
g(ˆx) −
m−1

i=0
ˆλigi(ˆx) = 0
(8.54)
for suitable ˆλi ∈R which are called the Lagrange multipliers. We see that (8.54)
can be expressed as [with ∇as in (8.3)]
∇

f (ˆx) −
m−1

i=0
ˆλifi(ˆx)
	
= 0.
(8.55)
In other words, we replace the original problem (8.46) with the mathematically
equivalent problem of minimizing
L(x, λ) = f (x) −
m−1

i=0
λifi(x),
(8.56)
1Equation (8.53) may be more formally justiﬁed as follows. Note that the same argument will also
extend to make (8.53) a necessary condition for ˆx to be a local maximizer, or saddle point for f (x).
Thus, (8.53) is really a necessary condition for ˆx to be a stationary point of f (x). We employ proof by
contradiction. Suppose that
g(ˆx) = ˆGˆλ + h,
where ˆG = [g0(ˆx) · · · gm−1(ˆx)] ∈Rn×m, ˆλ = [ˆλ0 · · · ˆλm−1]T ∈Rm and h ̸= 0. Further assume that
h ∈Rn is the component of g(ˆx) that is orthogonal to all gi(ˆx). Thus, ˆGT h = 0. In this instance
s = −h will satisfy both (8.51) and (8.52) [i.e., gT (ˆx)s = −[ˆλT ˆGT + hT ]h = −hT h < 0]. Satisfaction
of (8.52) implies that a step δ in the direction s will reduce f (x) [i.e., f (ˆx + δ) < f (ˆx)]. But this cannot
be the case since ˆx is a local minimizer of f (x). Consequently, h ̸= 0 is impossible, which establishes
(8.53).
TLFeBOOK

EQUALITY CONSTRAINTS AND LAGRANGE MULTIPLIERS
359
called the Lagrangian function (or Lagrangian), where, of course, x ∈Rn and
λ ∈Rm. Since L|Rn × Rm →R, in order to satisfy (8.55), we must determine
ˆx, ˆλ such that
∇L(ˆx, ˆλ) = 0,
(8.57)
where now instead [of (8.3)] we have ∇=
 ∇x
∇λ

such that
∇x =
 ∂
∂x0
· · ·
∂
∂xn−1
T
,
∇λ =
 ∂
∂λ0
· · ·
∂
∂λm−1
T
.
(8.58)
Now we see that a necessary condition for a stationary point of f (x) subject to
our constraints is that ˆx and ˆλ form a stationary point of the Lagrangian function.
Of course, to resolve whether stationary point ˆx is a minimizer requires additional
information (e.g., the Hessian). Observe that ∇λL(x, λ) = [−f0(x) −f1(x) · · · −
fm−1(x)]T , so ∇λL(x, λ) = 0 implies that fi(x) = 0 for all i. This is why we take
derivatives of the Lagrangian with respect to all elements of λ; it is equivalent to
imposing the equality constraints as in the original problem (8.46).
We now consider a few examples of the application of the method of Lagrange
multipliers.
Example 8.2
This example is from Fletcher [1, pp. 196–198]. Suppose that
f (x) = x0 + x1,
f0(x) = x2
0 −x1 = 0,
so x = [x0
x1]T ∈R2, and the Lagrangian is
L(x, λ) = x0 + x1 −λ(x2
0 −x1).
Clearly, to obtain stationary points, we must solve
∂L
∂x0
= 1 −2λx0 = 0,
∂L
∂x1
= 1 + λ = 0,
∂L
∂λ = x2
0 −x1 = 0.
Immediately, ˆλ = −1, so that 1 −2ˆλˆx0 = 0 yields ˆx0 = −1
2, and ˆx2
0 −ˆx1 = 0
yields ˆx1 = 1
4. Thus
ˆx =

−1
2
1
4
T
.
Is ˆx really a minimizer, or is it a maximizer, or saddle point?
TLFeBOOK

360
UNCONSTRAINED OPTIMIZATION
An alternative means to solve our problem is to recognize that since x2
0 −x1 = 0,
we can actually minimize the new objective function
f ′(x0) = x0 + x1|x1=x2
0 = x0 + x2
0
with respect to x0 instead. Clearly, this is a positive deﬁnite quadratic with a well-
deﬁned and unique minimum at x0 = −1
2. Again we conclude ˆx = [−1
2
1
4]T , and
it must speciﬁcally be a minimizer.
In Example 8.2,
ˆf = f (ˆx) = ˆx0 + ˆx1 = −1
4 > −∞only because of the con-
straint, f0(x) = x2
0 −x1 = 0. Without such a constraint, we would have ˆf = −∞.
We have described the method of Lagrange multipliers as being applied largely to
minimization problems. But we have noted that this method applies to maximization
problems as well because (8.57) is the necessary condition for a stationary point,
and not just a minimizer. The next example is a simple maximization problem from
geometry.
Example 8.3
We wish to maximize
F(x, y) = 4xy
subject to the constraint
C(x, y) = 1
4x2 + y2 −1 = 0.
This problem may be interpreted as the problem of maximizing the area of a
rectangle of area 4xy such that the corners of the rectangle are on an ellipse centered
at the origin of the two-dimensional plane. The ellipse is the curve C(x, y) = 0.
(Drawing a sketch is a useful exercise.)
Following the Lagrange multiplier procedure, we construct the Lagrangian
function
G(x, y, λ) = 4xy + λ( 1
4x2 + y2 −1)
Taking the derivatives of G and setting them to zero yields
∂G
∂x = 4y + 1
2λx = 0
(8.59a)
∂G
∂y = 4x + 2λy = 0
(8.59b)
∂G
∂λ = 1
4x2 + y2 −1 = 0
(8.59c)
TLFeBOOK

EQUALITY CONSTRAINTS AND LAGRANGE MULTIPLIERS
361
From (8.59a,b) we have
λ = −8y
x ,
and
λ = −2x
y ,
which means that −2x/y = −8y/x, or x2 = 4y2. Thus, we may replace x2 by 4y2
in (8.59c), giving
2y2 −1 = 0
for which y2 = 1
2, and so x2 = 2. From these equations we easily obtain the loca-
tions of the corners of the rectangle on the ellipse. The area of the rectangle is also
seen to be four units.
Example 8.4
Recall from Chapter 4 that we sought a method to determine
(compute) the matrix 2-norm
||A||2 = max
||x||2=1 ||Ax||2.
Chapter 4 considered only the special case x ∈R2 (i.e., n = 2). Now we consider
the general case for which n ≥2.
Since ||Ax||2
2 = xT AT Ax = xT Rx with R = RT and R > 0 (if A is full rank),
our problem is to maximize xT Rx subject to the equality constraint ||x||2 = 1 (or
equivalently xT x = 1). The Lagrangian is
L(x, λ) = xT Rx −λ(xT x −1)
since f (x) = xT Rx and f0(x) = xT x −1. Now
f (x) = xT Rx =
n−1

i=0
n−1

j=0
xixjrij
=
n−1

i=0
x2
i rii +
n−1

i=0
n−1

j=0
i̸=j
xixjrij
so that (using rij = rji)
∂f
∂xk
= 2rkkxk +
n−1

j = 0
j ̸= k
xjrkj +
n−1

i = 0
i ̸= k
xirik
= 2rkkxk +
n−1

j = 0
j ̸= k
rkjxj +
n−1

j = 0
j ̸= k
rkjxj.
TLFeBOOK

362
UNCONSTRAINED OPTIMIZATION
This reduces to
∂f
∂xk
= 2rkkxk + 2
n−1

j = 0
j ̸= k
rkjxj = 2
n−1

j=0
rkjxj
for all k = 0, 1, . . . , n −1. Consequently, ∇xf (x) = 2Rx. Similarly, ∇xf0(x) =
2x. Also, ∇λL(x, λ) = −xT x + 1 = −f0(x), and so ∇L(x, λ) = 0 yields the
equations
2Rx −2λx = 0,
xT x −1 = 0.
The ﬁrst equation states that the maximizing solution (if it exists) must satisfy the
eigenproblem
Rx = λx
for which λ is an eigenvalue and x is the corresponding eigenvector. Consequently,
xT Rx = λxT x, so
λ = xT Rx
xT x
= ||Ax||2
2
||x||2
2
= ||Ax||2
2
must be chosen to be the biggest eigenvalue of R. Since R > 0, such an eigenvalue
will exist. As before (Chapter 4), we conclude that
||A||2
2 = λn−1
for which λn−1 is the largest of the eigenvalues λ0, . . . , λn−1 of R = AT A (λn−1 ≥
λn−2 ≥· · · ≥λ0 > 0).
APPENDIX 8.A
MATLAB CODE FOR GOLDEN SECTION SEARCH
%
%
SineFit1.m
%
% This routine computes the objective function V_1(T) for user input
% T and data vector f as required by the golden section search test
% procedure golden.m.
Note that Ts (sampling period) must be consistent
% with Ts in golden.m.
%
function
V1 = SineFit1(f,T);
N = length(f); % Number of samples collected
Ts = 5*60;
% 5 minute sampling period
TLFeBOOK

MATLAB CODE FOR GOLDEN SECTION SEARCH
363
% Compute the objective function V_1(T)
n = [0:N-1];
T = T*60*60;
A = [ sin(2*pi*Ts*n/T).’ cos(2*pi*Ts*n/T).’ ];
B = inv(A.’*A);
V1 = f.’*f - f.’*A*B*A.’*f;
%
%
golden.m
%
% This routine tests the golden section search procedure of Chapter 8
% on the noisy sinusoid problem depicted in Fig. 8.4.
%
% This routine creates a test signal and uses SineFit1.m to compute
% the corresponding objective function V_1(T) (given by Equation (8.33)
% in Chapter 8).
%
function Topt = golden
% Compute the test signal f
N = 500;
% Number of samples collected
Ts = 5*60;
% 5 minute sampling period
T = 24*60*60; % 24 hr period for the sinusoid
phi = -pi/10; % phase angle of sinusoid
a = 1.0;
% sinusoid amplitude
var = .5000;
% desired noise variance
std = sqrt(var);
eta = std*randn(1,N);
for n = 1:N
f(n) = a*sin(((2*pi*(n-1)*Ts)/T) + phi);
end;
f = f + eta;
f = f.’;
% Specify a starting interval and initial parameters
% (units of hours)
xl = 16;
xu = 32;
tau = (sqrt(5)+1)/2;
% The golden ratio
tol = .05;
% Accuracy of the location of the minimum
% Apply the golden section search procedure
I = xu - xl;
% length of the starting interval
xa = xu - I/tau;
xb = xl + I/tau;
TLFeBOOK

364
UNCONSTRAINED OPTIMIZATION
while I > tol
if SineFit1(f,xa) >= SineFit1(f,xb)
I = xu - xa;
xl = xa;
temp = xa;
xa = xb;
xb = temp + I/tau;
else
I = xb - xl;
xu = xb;
temp = xb;
xb = xa;
xa = temp - I/tau;
end
end
Topt = (xl + xu)/2;
% Estimate of optimum choice for T
REFERENCES
1. R. Fletcher, Practical Methods of Optimization, 2nd ed., Wiley, New York, 1987
(reprinted July 1993).
2. M. R. Schroeder, Number Theory in Science and Communication (with Applications
in Cryptography, Physics, Digital Information, Computing, and Self-Similarity), 2nd
(expanded) ed., Springer-Verlag, New York, 1986.
3. A. Quarteroni, R. Sacco, and F. Saleri, Numerical Mathematics (Texts in Applied Math-
ematics series, Vol. 37). Springer-Verlag, New York, 2000.
4. R. P. Brent, Algorithms for Minimization without Derivatives, Dover Publications, Mine-
ola, NY, 2002.
5. S. Boyd and L. Vandenberghe, Convex Optimization, preprint, Dec. 2001.
PROBLEMS
8.1. Suppose A ∈Rn×n, and that A is not symmetric in general. Prove that
∇(xT Ax) = (A + AT )x,
where ∇is the gradient operator [Eq. (8.3)].
8.2. This problem is about ideas from vector calculus useful in nonlinear opti-
mization methods.
(a) If s, x, x′ ∈Rn, then a line in Rn is deﬁned by
x = x(α) = x′ + αs,
α ∈R.
Vector s may be interpreted as determining the direction of the line in the
n-dimensional space Rn. The notation x(α) implies x(α) = [x0(α)x1(α)
· · · xn−1(α)]T . Prove that the slope df/dα of f (x(α)) ∈R along the line
TLFeBOOK

PROBLEMS
365
at any x(α) is given by
df
dα = sT ∇f,
where ∇is the gradient operator [see Eq. (8.3)]. (Hint: Use the chain
rule for derivatives.)
(b) Suppose that u(x), v(x) ∈Rn (again x ∈Rn). Prove that
∇(uT v) = (∇uT )v + (∇vT )u.
8.3. Use the golden ratio search method to ﬁnd the global minimizer of the
polynomial objective function in Fig. 8.2. Do the computations using a pocket
calculator, with starting interval [x0
l , x0
u] = [2, 2.5]. Iterate 5 times.
8.4. Review Problem 7.4 (in Chapter 7).
Use a MATLAB implementation of the golden ratio search method to ﬁnd
detection threshold η for PFA = 0.1, 0.01, and 0.001. The objective func-
tion is
f (η) =

PFA −e−(1/2)η
(p/2)−1

k=0
1
k!
η
2
k

.
Make reasonable choices about starting intervals.
8.5. Suppose x = [x0
x1]T ∈R2, and consider
f (x) = x4
0 + x0x1 + (1 + x1)2.
Find general expressions for the gradient and the Hessian of f (x). Is G(0) >
0? What does this signify? Use the Newton–Raphson method (Chapter 7)
to conﬁrm that ˆx = [0.6959
−1.3479]T is a stationary point for f (x).
Select x0 = [0.7000
−1.3]T as the starting point. Is G(ˆx) > 0? What does
this signify? In this problem do all necessary computations using a pocket
calculator.
8.6. If ˆx ∈Rn is a local minimizer for f (x), then we know that G(ˆx) > 0, that
is, G(ˆx) is positive deﬁnite (pd). On the other hand, if ˆx is a local maximizer
of f (x), then −G(ˆx) > 0. In this case we say that G(ˆx) is negative deﬁnite
(nd). Show that
f (x) = (x1 −x2
0)2 + x5
0
has only one stationary point, and that it is neither a minimizer nor a maxi-
mizer of f (x).
8.7. Take note of the criterion for a maximizer in the previous problem. For both
a = 6 and a = 8, ﬁnd all stationary points of the function
f (x) = 2x3
0 −3x2
0 −ax0x1(x0 −x1 −1).
Determine which are local minima, local maxima, or neither.
TLFeBOOK

366
UNCONSTRAINED OPTIMIZATION
8.8. Write a MATLAB function to ﬁnd the minimum of Rosenbrock’s function
using the Newton algorithm (basic form that does not employ a line search).
Separate functions must be written to implement computation of both the
gradient and the inverse of the Hessian. The function for computing the
inverse of the Hessian must return an integer that indicates whether the Hes-
sian is positive deﬁnite. The Newton algorithm must terminate if a Hessian
is encountered that is not positive deﬁnite. The I/O is to be at the terminal
only. The user must input the starting vector at the terminal, and the program
must report the estimated minimum, or it must print an error message if the
Hessian is not positive deﬁnite. Test your program out on the starting vectors
x(0) = [−3
−3]T
and
x(0) = [0
10]T .
8.9. Write and test your own MATLAB routine (or routines) to verify Example 8.1.
8.10. Find the points on the ellipse
x2
a2 + y2
b2 = 1
that are closest to, and farthest from, the origin (x, y) = (0, 0). Use the
method of Lagrange multipliers.
8.11. The theory of Lagrange multipliers in Section 8.5 is a bit oversimpliﬁed.
Consider the following theorem. Suppose that ˆx ∈Rn gives an extremum
(i.e., minimum or maximum) of f (x) ∈R among all x satisfying g(x) = 0.
If f, g ∈C1[D] for a domain D ⊂R containing ˆx, then either
g(ˆx) = 0
and
∇g(ˆx) = 0,
(8.P.1)
or there is a λ ∈R such that
g(ˆx) = 0
and
∇f (ˆx) −λ∇g(ˆx) = 0.
(8.P.2)
From this theorem, candidate points ˆx for extrema of f (x) satisfying g(x) =
0 therefore are
(a) Points where f and g fail to have continuous partial derivatives.
(b) Points satisfying (8.P.1).
(c) Points satisfying (8.P.2).
In view of the theorem above and its consequences, ﬁnd the minimum dis-
tance from x = [x0
x1
x2]T = [0
0
−1]T to the surface
g(x) = x2
0 + x2
1 −x5
2 = 0.
TLFeBOOK

PROBLEMS
367
8.12. A second-order ﬁnite-impulse response (FIR) digital ﬁlter has the frequency
response H(ejω) = 2
k=0 hke−jωk, where hk ∈R are the ﬁlter parameters.
Since H(ejω) is 2π−periodic we usually consider only ω ∈[−π, π]. The
DC response of the ﬁlter is H(1) = H(ej0) = 2
k=0 hk. Deﬁne the energy
of the ﬁlter in the band [−ωp, ωp] to be
E = 1
2π
 ωp
−ωp
|H(ejω)|2 dω.
Find the ﬁlter parameters h = [h0 h1 h2]T ∈R3 such that for ωp = π/2
energy E is minimized subject to the constraint that H(1) = 1 (i.e., the gain
of the ﬁlter is unity at DC). Plot |H(ejω)| for ω ∈[−π, π]. [Hint: E will
have the form E = hT Rh ∈R, where R ∈R3×3 is a symmetric Toeplitz
matrix (recall Problem 4.20). Note also that |H(ejω)|2 = H(ejω)H ∗(ejω).]
8.13. This problem introduces incremental condition estimation (ICE) and is based
on the paper C. H. Bischof, “Incremental Condition Estimation,” SIAM J.
Matrix Anal. Appl. 11, 312–322 (April 1990). ICE can be used to estimate
the condition number of a lower triangular matrix as it is generated one
row at a time. Many algorithms for linear system solution produce triangular
matrix factorizations one row or one column at a time. Thus, ICE may be
built into such algorithms to warn the user of possible inaccuracies in the
solution due to ill conditioning. Let An be an n × n matrix with singular
values σ1(An) ≥· · · ≥σn(An) ≥0. A condition number for An is
κ(An) = σ1(An)
σn(An).
Consider the order n lower triangular linear system
Lnxn = dn.
(8.P.3)
The minimum singular value, σn(Ln), of Ln satisﬁes
σn(Ln) ≤||dn||2
||xn||2
(||xn||2
2 = 
i x2
n,i). Thus, an estimate (upper bound) of this singular value is
ˆσn(Ln) = ||dn||2
||xn||2
.
We would like to make this upper bound as small as possible. So, Bischof
suggests ﬁnding xn to satisfy (8.P.3) such that ||xn||2 is maximized subject
to the constraint that ||dn||2 = 1. Given xn−1 such that Ln−1xn−1 = dn−1
TLFeBOOK

368
UNCONSTRAINED OPTIMIZATION
with ||dn−1||2 = 1 [which gives us ˆσn−1(Ln−1) = 1/||xn−1||2], ﬁnd sn and
cn such that ||xn||2 is maximized where
Lnxn =

Ln−1
0
vT
n
γn
	
xn =
 sndn−1
cn

= dn,
xn =

snxn−1
(cn −snαn)/γn
	
.
Find αn, cn, sn. Assume αn ̸= 0. (Comment: The indexing of the singular
values used here is different from that in Chapter 4. The present notation is
more convenient in the present context.)
8.14. Assume that A ∈Rm×n with m ≥n, but rank(A) < n is possible. As usual,
||x||2
2 = xT x. Solve the following problem:
min
x∈Rn ||Ax −b||2
2 + δ||x||2
2,
where δ > 0. This is often called the Tychonov regularization problem. It
is a simple ploy to alleviate problems with ill-conditioning in least-squares
applications. Since A is not necessarily of full rank, we have AT A ≥0, but
not necessarily AT A > 0. What is rank(AT A + δI)? Of course, I is the order
n identity matrix.
TLFeBOOK

9
Numerical Integration
and Differentiation
9.1
INTRODUCTION
We are interested in how to compute the integral
I =
 b
a
f (x) dx
(9.1)
for which f (x) ∈R (and, of course, x ∈R). Depending on f (x), and perhaps also
on [a, b], the reader knows that “nice” closed-form expressions for I rarely exist.
This forces us to consider numerical methods to approximate I. We have seen from
Chapter 3 that one approach is to ﬁnd a suitable series expansion for the integral
in (9.1). For example, recall that we wished to compute the error function
erf(x) =
2
√π
 x
0
e−t2 dt,
(9.2)
which has no antiderivative (i.e., “nice” formula). Recall that the error function is
crucial in solving various problems in applied probability that involve the Gaussian
probability density function [i.e., the function in (3.101) of Chapter 3]. The Taylor
series expansion of Eq. (3.108) was suggested as a means to approximately evaluate
erf(x), and is known to be practically effective if x is not too big. If x is large, then
the asymptotic expansion of Example 3.10 was suggested. The series expansion
methodology may seem to solve our problem, but there are integrals for which it
is not easy to ﬁnd series expansions of any kind.
A recursive approach may be attempted as an alternative. An example of this was
seen in Chapter 5, where ﬁnding the norm of a Legendre polynomial required solv-
ing a recursion involving variables that were certain integrals [recall Eq. (5.96)].
As another example of this approach, consider the following case from Forsythe
et al. [1]. Suppose that we wish to compute
En =
 1
0
xnex−1 dx
(9.3)
An Introduction to Numerical Analysis for Electrical and Computer Engineers, by C.J. Zarowski
ISBN 0-471-46737-5
c⃝2004 John Wiley & Sons, Inc.
369
TLFeBOOK

370
NUMERICAL INTEGRATION AND DIFFERENTIATION
for any n ∈N (natural numbers). Recalling integration by parts, we see that
 1
0
xnex−1 dx = xnex−1|1
0 −
 1
0
nxn−1ex−1 dx = 1 −nEn−1,
so
En = 1 −nEn−1
(9.4)
for n = 2, 3, 4, . . .. It is easy to conﬁrm that
E1 =
 1
0
xex−1 dx = 1
e .
This is the initial condition for recursion (9.4). We observe that En > 0 for all
n. But if, for example, MATLAB is used to compute E19, we obtain computed
solution ˆE19 = −5.1930, which is clearly wrong. Why has this happened?
Because of the need to quantize, E1 is actually stored in the computer as
ˆE1 = E1 + ϵ
for which ϵ is some quantization error. Assuming that the operations in (9.4) do
not lead to further errors (i.e., assuming no rounding errors), we may arrive at a
formula for ˆEn:
ˆE2 = 1 −2 ˆE1 = E2 + (−2)ϵ,
ˆE3 = 1 −3 ˆE2 = E3 + (−3)(−2)ϵ,
ˆE4 = 1 −4 ˆE3 = E4 + (−4)(−3)(−2)ϵ,
and so on. In general
ˆEn = En + (−1)n−11 · 2 · 3 · · · (n −1)nϵ,
(9.5)
so
ˆEn −En = (−1)n−1n!ϵ.
(9.6)
We see that even a very tiny quantization error ϵ will grow very rapidly during
the course of the computation even without any additional rounding errors at all !
Thus, (9.4) is a highly unstable numerical procedure, and so must be rejected as
a method to compute (9.3). However, it is possible to arrive at a stable procedure
by modifying (9.4). Now observe that
En =
 1
0
xnex−1 dx ≤
 1
0
xn dx =
1
n + 1,
(9.7)
TLFeBOOK

TRAPEZOIDAL RULE
371
implying that En →0 as n increases. Instead of (9.4), consider the recursion
[obtained by rearranging (9.4)]
En−1 = 1
n(1 −En).
(9.8)
If we wish to compute Em, we may assume En = 0 for some n signiﬁcantly bigger
than m and apply (9.8). From (9.7) the error involved in approximating En by zero
is not bigger than 1/(n + 1). Thus, an algorithm for Em is
Ek−1 = 1
k (1 −Ek)
for k = n, n −1, . . . , m + 2, m + 1, where En = 0. At each stage of this algorithm
the initial error is reduced by factor 1/k rather than being magniﬁed as it was in
the procedure of (9.4).
Other than potential numerical stability problems, which may or may not be
easy to solve, it is apparent that not all integration problems may be cast into a
recursive form. It is also possible that f (x) is not known for all x ∈R. We might
know f (x) only on a ﬁnite subset of R, or perhaps a countably inﬁnite subset of
R. This situation might arise in the context of obtaining f (x) experimentally. Such
a scenario would rule out the previous suggestions.
Thus, there is much room to consider alternative methodologies. In this chapter
we consider what may be collectively called quadrature methods. For the most part,
these are based on applying some of the interpolation ideas considered in Chapter 6.
But Gaussian quadrature (see Section 9.4) also employs orthogonal polynomials
(material from Chapter 5).
This Chapter is dedicated mainly to the subject of numerical integration by
quadratures. But the ﬁnal section considers numerical approximations to deriva-
tives (i.e., numerical differentiation). Numerical differentiation is relevant to the
numerical solution of differential equations (to be considered in later chapters),
and we have mentioned that it is relevant to spline interpolation (Section 6.5). In
fact, it can also ﬁnd a role in reﬁned methods of numerical integration (Section 9.5).
9.2
TRAPEZOIDAL RULE
A simple approach to numerical integration is the following.
In this book we implicitly assume all functions are Riemann integrable. From
elementary calculus such integrals are obtained by the limiting process
I =
 b
a
f (x) dx = lim
n→∞
b −a
n
n

k=1
f (x◦
k)
(9.9)
for which x0 = a, xn = b, and for which the value I is independent of the point x◦
k ∈
[xk−1, xk]. We remark that not all functions f (x) satisfy this requirement and so,
as was mentioned in Chapter 3, not all functions are Riemann integrable. However,
TLFeBOOK

372
NUMERICAL INTEGRATION AND DIFFERENTIATION
we will ignore this potential problem. We may approximate I according to
I ≈b −a
n
n

k=1
f (x◦
k).
(9.10)
Such an approximation is called the rectangular rule (or rectangle rule) for numer-
ical integration, and there are different variants depending on the choice for x◦
k.
Three possible choices are shown in Fig. 9.1. We mention that all variants involve
assuming f (x) is piecewise constant on [xk−1, xk], and so amount to the constant
interpolation of f (x) (i.e., ﬁtting a polynomial which is a constant to f (x) on
some interval). Deﬁne
h = b −a
n
.
From Fig. 9.1, the right-point rule uses
x◦
k = a + kh,
(9.11a)
while the left-point rule uses
x◦
k = a + (k −1)h,
(9.11b)
xk−1
xk
y = f(x)
0
x
a
b
(a)
xk−1
xk
y = f(x)
0
x
a
b
(b)
xk−1
xk
y = f(x)
0
x
a
b
(c)
Figure 9.1
Illustration of the different forms of the rectangular rule: (a) right-point rule;
(b) left-point rule; (c) midpoint rule. In all cases x0 = a and xn = b.
TLFeBOOK

TRAPEZOIDAL RULE
373
and the midpoint rule uses
x◦
k = a + (k −1
2)h,
(9.11c)
where in all cases k = 1, 2, . . . , n −1, n. The midpoint rule is often preferred
among the three as it is usually more accurate. However, the rectangular rule is often
(sometimes unfairly) regarded as too crude, and so the following (or something still
“better”) is chosen.
It is often better to approximate f (x) with trapezoids as shown in Fig. 9.2. This
results in the trapezoidal rule for numerical integration. From Fig. 9.2 we see that
this rule is based on the linear interpolation of the function f (x) on [xk−1, xk].
The approximation to
" xk
xk−1 f (x) dx is given by the area of the trapezoid: this is
 xk
xk−1
f (x) dx ≈1
2

f (xk) + f (xk−1)

(xk −xk−1) = 1
2

f (xk−1) + f (xk)

h
(9.12)
[We have h = xk −xk−1 = (b −a)/n.] It is intuitively plausible that this method
should be more accurate than the rectangular rule, and yet not require much, if
any, additional computational effort to implement it. Applying (9.12) for k = 1 to
k = n, we have
 b
a
f (x) dx ≈T (n) = h
2
n

k=1

f (xk−1) + f (xk)

,
(9.13)
and the summation expands out as
T (n) = h
2

f (x0) + 2f (x1) + 2f (x2) + · · · + 2f (xn−1) + f (xn)

,
(9.14)
where n ∈N (set of natural numbers).
f(xk)
f(xk−1)
xk−1
xk
y = f(x)
x
0
Figure 9.2
Illustration of the trapezoidal rule.
TLFeBOOK

374
NUMERICAL INTEGRATION AND DIFFERENTIATION
We may investigate the error behavior of the trapezoidal rule as follows. The
process of analysis begins by assuming that n = 1; that is, we linearly interpolate
f (x) on [a, b] using
p(x) = f (a)x −b
a −b + f (b)x −a
b −a ,
(9.15)
which is from the Lagrange interpolation formula [recall (6.9) for n = 1]. It must
be the case that for suitable error function e(x), we have
f (x) = p(x) + e(x)
(9.16)
with x ∈[a, b]. Consequently
 b
a
f (x) dx =
 b
a
p(x) dx +
 b
a
e(x) dx
= b −a
2
[f (a) + f (b)] +
 b
a
e(x) dx = T (1) + ET (1),
(9.17)
where
ET (1) =
 b
a
e(x) dx,
(9.18)
which is the error involved in using the trapezoidal rule. Of course, we would like
a suitable bound on this error. To obtain such a bound, we will assume that f (1)(x)
and f (2)(x) both exist and are continuous on [a, b]. Let x be ﬁxed at some value
such that a < x < b, and deﬁne
g(t) = f (t) −p(t) −[f (x) −p(x)] (t −a)(t −b)
(x −a)(x −b)
(9.19)
for t ∈[a, b]. It is not difﬁcult to conﬁrm that
g(a) = g(b) = g(x) = 0,
so g(t) vanishes at three different places on the interval [a, b]. Rolle’s theorem1
says that there are points ξ1 ∈(a, x), ξ2 ∈(x, b) such that
g(1)(ξ1) = g(1)(ξ2) = 0.
Thus, g(1)(t) vanishes at two different places on (a, b), so yet again by Rolle’s
theorem there is a ξ ∈(ξ1, ξ2) such that
g(2)(ξ) = 0.
1This was proved by Bers [2], but the proof is actually rather lengthy, and so we omit it.
TLFeBOOK

TRAPEZOIDAL RULE
375
We note that ξ = ξ(x), that is, point ξ depends on x. Therefore
g(2)(ξ) = f (2)(ξ) −p(2)(ξ) −[f (x) −p(x)]
2
(x −a)(x −b) = 0.
(9.20)
The polynomial p(x) is of the ﬁrst degree so p(2)(ξ) = 0. We may use this in
(9.20) and rearrange the result so that
f (x) = p(x) + 1
2f (2)(ξ(x))(x −a)(x −b)
(9.21)
for any x ∈(a, b). This expression also happens to be valid at x = a and at x = b,
so
e(x) = f (x) −p(x) = 1
2f (2)(ξ(x))(x −a)(x −b)
(9.22)
for x ∈[a, b]. But we need to evaluate ET (1) in (9.18). The second mean-value
theorem for integrals states that if f (x) is continuous and g(x) is integrable (Rie-
mann) on [a, b], and further that g(x) does not change sign on [a, b], then there
is a point p ∈(a, b) such that
 b
a
f (x)g(x) dx = f (p)
 b
a
g(x) dx.
(9.23)
The proof is omitted. We observe that (x −a)(x −b) does not change sign on
x ∈[a, b], so via this theorem we have
ET (1) = 1
2
 b
a
f (2)(ξ(x))(x −a)(x −b) dx = −1
12f (2)(p)(b −a)3,
(9.24)
where p ∈(a, b). We emphasize that this is the error for n = 1 in T (n). Naturally,
we want an error expression for n > 1, too. When n > 1, we may refer to the
integration rule as a compound or composite rule.
The error committed in numerically integrating over the kth subinterval [xk−1, xk]
must be [via (9.24)]
Ek = −1
12f (2)(ξk)(xk −xk−1)3 = −h3
12f (2)(ξk) = −h2
12
b −a
n
f (2)(ξk),
(9.25)
where ξk ∈[xk−1, xk] and k = 1, 2, . . . , n −1, n. Therefore, the total error com-
mitted is
ET (n) =
 b
a
f (x) dx −T (n) =
n

k=1
Ek,
(9.26)
which becomes [via (9.25)]
ET (n) = −h2
12
b −a
n
n

k=1
f (2)(ξk).
(9.27)
TLFeBOOK

376
NUMERICAL INTEGRATION AND DIFFERENTIATION
The average 1
n
n
k=1 f (2)(ξk) must lie between the largest and smallest values of
f (2)(x) on [a, b], so recalling that f (2)(x) is continuous on [a, b], the intermediate
value theorem (Theorem 7.1) yields that there is an ξ ∈(a, b) such that
f (2)(ξ) = 1
n
n

k=1
f (2)(ξk).
Therefore
ET (n) = −h2
12(b −a)f (2)(ξ),
(9.28)
where ξ ∈(a, b). If the maximum value of f (2)(x) on [a, b] is known, then this
may be used in (9.28) to provide an upper bound on the error. We remark that
ET (n) is often called truncation error.
We see that
T (n) = xT y
(9.29)
for which
x = h[ 1
21 · · · 1 1
2]T ∈Rn+1,
y = [f (x0)f (x1) · · · f (xn−1)f (xn)]T ∈Rn+1.
(9.30)
We know that rounding errors will be committed in the computation of (9.29). The
total rounding error might be denoted by ER. We recall from Chapter 2 [Eq. (2.40)]
that a bound on these errors is
|ER| = |xT y −f l[xT y]| ≤1.01(n + 1)u|x|T |y|,
(9.31)
where u is the unit roundoff, or else the machine epsilon. The cumulative effect of
rounding errors can be expected to grow as n increases. If we suppose that
M = max
x∈[a,b] |f (2)(x)|,
(9.32)
then, from (9.28), we obtain
|ET (n)| ≤1
12
1
n2 (b −a)3M.
(9.33)
Thus, (9.33) is an upper bound on the truncation error for the composite trapezoidal
rule. We see that the bound gets smaller as n increases. Thus, as we expect,
truncation error is reduced as the number of trapezoids used increases. Combining
(9.33) with (9.31) results in a bound on total error E:
|E| ≤1
12
1
n2 (b −a)3M + 1.01(n + 1)u|x|T |y|.
(9.34)
TLFeBOOK

TRAPEZOIDAL RULE
377
Usually n ≫1, so n + 1 ≈n. Thus, substituting this and (9.30) into (9.31) results in
|xT y −f l[xT y]| ≤1.01(b −a)u

|f (x0)| + |f (xn)|
2
+
n−1

k=1
|f (xk)|
	
,
(9.35)
and so (9.34) becomes
|E| ≤1
12
1
n2 (b −a)3M + 1.01(b −a)u

|f (x0)| + |f (xn)|
2
+
n−1

k=1
|f (xk)|
	
.
(9.36)
In general, the ﬁrst term in the bound of (9.36) becomes smaller as n increases,
while the second term becomes larger. Thus, there is a tradeoff involved in choosing
the number of trapezoids to approximate a given integral, and the best choice ought
to minimize the total error.
Example 9.1
We may apply the bound of (9.36) to the following problem. We
wish to compute
I =
 1
0
e−x dx.
Thus, [a, b] = [0, 1], and so b −a = 1. Of course, it is very easy to conﬁrm that
I = 1 −e−1. But this is what makes it a good example to test our theory out. We
also see that f (2)(x) = e−x, and so in (9.32) M = 1. Also
f (xk) = e−k/n
for k = 0, 1, . . . , n −1, n. It is therefore easy to see that
n−1

k=1
f (xk) = e−1/n −e−1
1 −e−1/n .
We might assume that the trapezoidal rule for this problem is implemented in the
C programming language using single-precision ﬂoating-point arithmetic, in which
case a typical value for u would be
u = 1.1921 × 10−7.
Therefore, from (9.36) the total error is bounded according to
|E| ≤
1
12n2 + 1.2040 × 10−7
e + 1
2e
+ e−1/n −e−1
1 −e−1/n

.
Figure 9.3 plots this bound versus n, and also shows the magnitude of the computed
(i.e., the true or actual) total error in the trapezoidal rule approximation, which
is |T (n) −I|. We see that the true error is always less than the bound, as we
would expect. However, the bound is rather pessimistic. Also, the bound predicts
TLFeBOOK

378
NUMERICAL INTEGRATION AND DIFFERENTIATION
10−8
103
104
105
106
107
10−6
10−4
10−2
100
Number of trapezoids used (n)
Magnitude of error
10−8
100
101
103
102
104
105
106
10−6
10−4
10−2
100
Number of trapezoids used (n)
Magnitude of error
Bound
Computed error
Bound
Computed error
Figure 9.3
Comparison of total error (computed) to bound on total error, illustrating the
tradeoff between rounding error and truncation error in numerical integration by the trape-
zoidal rule. The bound employed here is that of Eq. (9.36).
that the proper choice for n is much less than what the computed result predicts.
Speciﬁcally, the bound suggests that we choose n ≈100, while the computed result
suggests that we choose n ≈100, 000.
What is important is that the computed result and the bound both conﬁrm that
there is a tradeoff between minimizing the truncation error and minimizing the
rounding error. To minimize rounding error, we prefer a small n, but to minimize
the truncation error, we prefer a large n. The best solution minimizes the total error
from both sources.
In practice, attempting a detailed analysis to determine the true optimum choice
for n is usually not worth the effort. What is important is to understand the funda-
mental tradeoffs involved in the choice of n, and from this understanding select a
reasonable value for n.
9.3
SIMPSON’S RULE
The trapezoidal rule employed linear interpolation to approximate f (x) between
sample points xk on the x axis. We might consider quadratic interpolation in
TLFeBOOK

SIMPSON’S RULE
379
the hope of improving accuracy still further. Here “accuracy” is a reference to
truncation error.
Therefore, we wish to ﬁt a quadratic curve to the points (xk−1, f (xk−1)),
(xk, f (xk)) and (xk+1, f (xk+1)). We may deﬁne the quadratic to be
pk(x) = a(x −xk)2 + b(x −xk) + c.
(9.37)
Contrary to past practice, the subscript k now does not denote degree, but rather
denotes the “centerpoint” of the interval [xk−1, xk+1] on which we are ﬁtting the
quadratic. The situation is illustrated in Fig. 9.4 . For convenience, deﬁne yk =
f (xk). Therefore, from (9.37) we may set up three equations in the unknowns
a, b, c:
a(xk−1 −xk)2 + b(xk−1 −xk) + c = yk−1,
a(xk −xk)2 + b(xk −xk) + c = yk,
a(xk+1 −xk)2 + b(xk+1 −xk) + c = yk+1.
This is a linear system of equations, and we will assume that h = xk −xk−1 =
xk+1 −xk, so therefore
a = yk+1 −2yk + yk−1
2h2
,
(9.38a)
b = yk+1 −yk−1
2h
,
(9.38b)
c = yk.
(9.38c)
This leads to the approximation
 xk+1
xk−1
f (x) dx ≈
 xk+1
xk−1
pk(x) dx = h
3 [yk−1 + 4yk + yk+1].
(9.39)
x
y = f(x)
0
xk−1
xk+1
xk
Parabolic arc pk (x)
f(x)
Figure 9.4
Simpson’s rule for numerical integration.
TLFeBOOK

380
NUMERICAL INTEGRATION AND DIFFERENTIATION
Of course, some algebra has been omitted to arrive at the equality in (9.39). As
in Section 9.2, we wish to integrate f (x) on [a, b]. So, as before, a = x0, and
b = xn. If n is an even number, then the number of subdivisions of [a, b] is an
even number, and hence we have the approximation
I =
 b
a
f (x) dx ≈
 x2
x0
p1(x) dx +
 x4
x2
p3(x) dx + · · · +
 xn
xn−2
pn−1(x) dx
= h
3[y0 + 4y1 + 2y2 + 4y3 + 2y4 + · · · + 2yn−2 + 4yn−1 + yn].
(9.40)
The last equality follows from applying (9.39). We deﬁne the Simpson rule approx-
imation to I as
S(n) = h
3[y0 + 4y1 + 2y2 + 4y3 + 2y4 + · · · + 2yn−2 + 4yn−1 + yn]
(9.41)
for which n is even and n ≥2.
A truncation error analysis of Simpson’s rule is more involved than that of
the analysis of the trapezoidal rule seen in the previous section. Therefore, we
only outline the major steps and results. We begin by using only two subintervals
to approximate I =
" b
a f (x) dx, speciﬁcally, n = 2. Deﬁne c = (a + b)/2. Denote
the interpolating quadratic by p(x). For a suitable error function e(x), we must
have
f (x) = p(x) + e(x).
(9.42)
Immediately we see that
I =
 b
a
f (x) dx =
 b
a
p(x) dx +
 b
a
e(x) dx
= b −a
6

f (a) + 4f
 a + b
2
!
+ f (b)

+
 b
a
e(x) dx = S(2) + ES(2).
(9.43)
So, the truncation error in Simpson’s rule is thus
ES(2) =
 b
a
e(x) dx.
(9.44)
It is clear that Simpson’s rule is exact for f (x) a quadratic function. Less clear is
the fact that Simpson’s rule is exact if f (x) is a cubic polynomial. To demonstrate
the truth of this claim, we need an error result from Chapter 6. We assume that
f (k)(x) exists and is continuous for all k = 0, 1, 2, 3, 4 for all x ∈[a, b]. From
TLFeBOOK

SIMPSON’S RULE
381
Eq. (6.14) the error involved in interpolating f (x) with a quadratic polynomial is
given by
e(x) = 1
3!f (3)(ξ(x))(x −a)(x −b)(x −c)
(9.45)
for some ξ = ξ(x) ∈[a, b]. Hence
ES(2) =
 b
a
e(x) dx = 1
3!
 b
a
f (3)(ξ(x))(x −a)(x −b)(x −c) dx.
(9.46)
Unfortunately, polynomial (x −a)(x −b)(x −c) changes sign on the interval
[a, b], and so we are not able to apply the second mean-value theorem for integrals
as we did in Section 9.2. This is a major reason why the analysis of Simpson’s
rule is harder than the analysis of the trapezoidal rule. However, at this point we
may still consider (9.46) for the case where f (x) is a cubic polynomial. In this
case we must have f (3)(x) = K (some constant). Consequently, from (9.46)
ES(2) = K
3!
 b
a
(x −a)(x −b)(x −c) dx,
but if z = x −c, then, since c = 1
2(a + b), we must have
ES(2) = K
3!

1
2 (b−a)
−1
2 (b−a)
z

z + b −a
2
 
z −b −a
2

dz
= K
3!

1
2 (b−a)
−1
2 (b−a)
z

z2 −
 b −a
2
!2	
dz.
(9.47)
The integrand is an odd function of z, and the integration limits are symmetric
about the point z = 0. Immediately we conclude that ES(2) = 0 in this particular
case. Thus, we conclude that Simpson’s rule gives the exact result when f (x) is a
cubic polynomial.
Hermite interpolation (considered in a general way in Section 6.4) is polynomial
interpolation where not only does the interpolating polynomial match f (x) at the
sample points xk but the ﬁrst derivative of f (x) is matched as well. It is useful
to interpolate f (x) with a cubic polynomial that we will denote by r(x) at the
points (a, f (a)), (b, f (b)), and (c, f (c)), and also such that r(1)(c) = f (1)(c). A
cubic polynomial is speciﬁed by four coefﬁcients, so these constraints uniquely
determine r(x). In fact
r(x) = p(x) + α(x −a)(x −b)(x −c)
(9.48a)
for which
α = 4[p(1)(c) −f (1)(c)]
(b −a)2
.
(9.48b)
TLFeBOOK

382
NUMERICAL INTEGRATION AND DIFFERENTIATION
Analogously to (9.19), we may deﬁne
g(t) = f (t) −r(t) −[f (x) −r(x)] (t −a)(t −c)2(t −b)
(x −a)(x −c)2(x −b),
a ≤t ≤b.
(9.49)
It happens that g(k)(t) for k = 0, 1, 2, 3, 4 all exist and are continuous at all x ∈
[a, b]. Additionally, g(a) = g(b) = g(c) = g(1)(c) = g(x) = 0. The vanishing of
g(t) at four distinct points on [a, b], and g(1)(c) = 0 guarantees that g(4)(ξ) = 0
for some ξ ∈[a, b] by the repeated application of Rolle’s theorem. Consequently,
using (9.49), we obtain
g(4)(ξ) = f (4)(ξ) −r(4)(ξ) −[f (x) −r(x)]
4!
(x −a)(x −c)2(x −b) = 0. (9.50)
Since r(x) is cubic, r(4)(ξ) = 0, and so (9.50) can be used to say that
f (x) = r(x) + 1
4!f (4)(ξ(x))(x −a)(x −c)2(x −b)
(9.51)
for x ∈(a, b). This is valid at the endpoints of [a, b], so ﬁnally
e(x) = f (x) −r(x) = 1
4!f (4)(ξ(x))(x −a)(x −c)2(x −b)
(9.52)
for x ∈[a, b], and ξ(x) ∈[a, b]. Immediately, we see that
ES(2) = 1
4!
 b
a
f (4)(ξ(x))(x −a)(x −c)2(x −b) dx.
(9.53)
The polynomial in the integrand of (9.53) does not change sign on [a, b]. Thus, the
second mean-value theorem for integrals is applicable. Hence, for some ξ ∈(a, b),
we have
ES(2) = f (4)(ξ)
4!
 b
a
(x −a)(x −c)2(x −b) dx,
(9.54)
which reduces to
ES(2) = −h5
90f (4)(ξ)
(9.55)
again for some ξ ∈(a, b), where h = (b −a)/2.
We need an expression for ES(n), that is, an error expression for the compos-
ite Simpson rule. We will assume again that h = (b −a)/n, where n is an even
number. Consequently
ES(n) =
 b
a
f (x) dx −S(n) =
n/2

k=1
Ek,
(9.56)
TLFeBOOK

SIMPSON’S RULE
383
where Ek is the error committed in the approximation for the kth subinterval
[x2(k−1), x2k]. Thus, for ξk ∈[x2(k−1), x2k], with k = 1, 2, . . . , n/2, we have
Ek = −h5
90f (4)(ξk) = −h4
90
b −a
n
f (4)(ξk).
(9.57)
Therefore
ES(n) = −h4
180
b −a
n/2
n/2

k=1
f (4)(ξk).
(9.58)
Applying the intermediate-value theorem to the average
1
n/2
n/2
k=1 f (4)(ξk) con-
ﬁrms that there is a ξ ∈(a, b) such that
1
n/2
n/2

k=1
f (4)(ξk) = f (4)(ξ),
so therefore the truncation error expression for the composite Simpson rule becomes
ES(n) = −h4
180(b −a)f (4)(ξ)
(9.59)
for some ξ ∈(a, b). For convenience, we repeat the truncation error expression for
the composite trapezoidal rule:
ET (n) = −h2
12(b −a)f (2)(ξ).
(9.60)
It is not really obvious which rule, trapezoidal or Simpson’s, is better in general.
For a particular interval [a, b] and n, the two expressions depend on different
derivatives of f (x). It is possible that Simpson’s rule may not be an improvement
on the trapezoidal rule in particular cases for this reason. More speciﬁcally, a
function that is not too smooth can be expected to have “big” higher derivatives.
Simpson’s rule has a truncation error dependent on the fourth derivative, while the
trapezoidal rule has an error that depends only on the second derivative. Thus, a
nonsmooth function might be better approximated by the trapezoidal rule than by
Simpson’s. In fact, Davis and Rabinowitz [3, p. 26] state that
The more “reﬁned” a rule of approximate integration is, the more certain we must
be that it has been applied to a function which is sufﬁciently smooth. There may be
little or no advantage in using a ‘better’ rule for a function that is not smooth.
We repeat a famous example from Ref. 3 (originally due to Salzer and Levine).
TLFeBOOK

384
NUMERICAL INTEGRATION AND DIFFERENTIATION
Example 9.2
The following series deﬁnes a function due to Weierstrass that
happens to be continuous but is, surprisingly, not differentiable anywhere2
W(x) =
∞

n=1
1
2n cos(7nπx).
(9.61a)
If we assume that we may integrate this expression term by term, then
I (y) =
 y
0
W(x) dx = 1
π
∞

n=1
1
7n2n sin(7nπy).
(9.61b)
Of course, I =
" b
a W(x) dx = I (b) −I (a). The series (9.61b) gives the “exact”
value for I (y) and so may be compared to estimates produced by the trapezoidal
and Simpson rules. Assuming that n = 100, the following table of values is obtained
[MATLAB implementation of (9.61) and the numerical integration rules]:
Interval
Exact
Trapezoidal
Error
Simpson’s
Error
[a, b]
Value I
T (n)
I −T (n)
S(n)
I −S(n)
[0,.1]
0.01899291
0.01898760
0.00000531
0.01901426
−0.00002135
[.1, .2]
−0.04145650
−0.04143815
−0.00001834
−0.04146554
0.00000904
[.2, .3]
0.03084617
0.03084429
0.00000188
0.03086261
−0.00001645
[.3, .4]
0.00337701
0.00342534
−0.00004833
0.00341899
−0.00004198
[.4, .5]
−0.03298025
−0.03300674
0.00002649
−0.03303611
0.00005586
We see that the errors involved in both the trapezoidal and Simpson rules do
not differ greatly from each other. So Simpson’s rule has no advantage here.
It is commonplace for integration problems to involve integrals that possess
oscillatory integrands. For example, the Fourier transform of x(t) is deﬁned to be
X(ω) =
 ∞
−∞
x(t)e−jωt dt =
 ∞
−∞
x(t) cos(ωt) dt −j
 ∞
−∞
x(t) sin(ωt) dt.
(9.62)
You will likely see much of this integral in other books and associated courses
(e.g., signals and systems). Also, determination of the Fourier series coefﬁcients
required computing [recall Eq. (1.45)]
fn = 1
2π
 2π
0
f (x)e−jnx dx.
(9.63)
2Proof that W(x) is continuous but not differentiable is quite difﬁcult. There are many different
Weierstrass functions possessing this property of continuity without differentiability. Another example
complete with a proof appears on pp. 38–41 of K¨orner [4].
TLFeBOOK

GAUSSIAN QUADRATURE
385
An integrand is said to be rapidly oscillatory if there are numerous (i.e., of the
order of ≥10) local maxima and minima over the range of integration (i.e., here
assumed to be the ﬁnite interval [a, b]). Some care is often required to compute
these properly with the aid of the integration rules we have considered so far.
However, we will consider only a simple idea called integration between the zeros.
Davis and Rabinowitz have given a more detailed consideration of how to handle
oscillatory integrands [3, pp. 53–68].
Relevant to the computation of (9.63) is, for example, the integral
I =
 2π
0
f (x) sin(nx) dx.
(9.64)
It may be that f (x) oscillates very little or not at all on [0, 2π]. We may therefore
replace (9.64) with
I =
2n−1

k=0
 (k+1)π/n
kπ/n
f (x) sin(nx) dx.
(9.65)
The endpoints of the “subintegrals”
Ik =
 (k+1)π/n
kπ/n
f (x) sin(nx) dx
(9.66)
in (9.65) are the zeros of sin(nx) on [0, 2π]. Thus, we are truly proposing integra-
tion between the zeros. At this point it is easiest to approximate (9.66) with either
the trapezoidal or Simpson’s rules for all k. Since the endpoints of the integrands
in (9.66) are zero-valued, we can expect some savings in computation as a result
because some of the terms in the rules (9.14) and (9.41) will be zero-valued.
9.4
GAUSSIAN QUADRATURE
Gaussian quadrature is a numerical integration method that uses a higher order of
interpolation than do either the trapezoidal or Simpson rules. A detailed derivation
of the method is rather involved as it relies on Hermite interpolation, orthogonal
polynomial theory, and some aspects of the theory rely on issues relating to linear
system solution. Thus, only an outline presentation is given here. However, a
complete description may be found in Hildebrand [5, pp. 382–400]. Additional
information is presented in Davis and Rabinowitz [3].
It helps to recall ideas from Chapter 6 here. If we know f (x) for x = xj,
where j = 0, 1, . . . , n −1, n, then the Lagrange interpolating polynomial is (with
p(xj) = f (xj))
p(x) =
n

j=0
f (xj)Lj(x),
(9.67)
TLFeBOOK

386
NUMERICAL INTEGRATION AND DIFFERENTIATION
where
Lj(x) =
n
/
i=0
i̸=j
x −xi
xj −xi
.
(9.68)
Recalling (6.45b), we may similarly deﬁne
π(x) =
n
/
i=0
(x −xi).
(9.69)
Consequently
π(1)(x) =
n

j=0



n
/
k=0
k̸=j
(x −xk)



(9.70)
so that
π(1)(xi) =
n
/
k=0
k̸=i
(xi −xk),
(9.71)
which allows us to rewrite (9.68) as
Lj(x) =
π(x)
π(1)(xj)(x −xj)
(9.72)
for j = 0, 1, . . . , n. From (6.14)
e(x) = f (x) −p(x) =
1
(n + 1)!f (n+1)(ξ)π(x)
(9.73)
for some ξ ∈[a, b], and ξ = ξ(x).
We now summarize Hermite interpolation (recall Section 6.4 for more detail).
Suppose that we have knowledge of both f (x) and f (1)(x) at x = xj (again j =
0, 1, . . . , n). We may interpolate f (x) using a polynomial of degree 2n + 1 since
we must match the polynomial to both f (x) and f (1)(x) at x = xj. Thus, we need
the polynomial
p(x) =
n

k=0
hk(x)f (xk) +
n

k=0
ˆhk(x)f (1)(xk),
(9.74)
where hk(x) and ˆhk(x) are both polynomials of degree 2n + 1 that we must deter-
mine according to the constraints of our interpolation problem.
If
hi(xj) = δi−j, ˆhi(xj) = 0,
(9.75a)
TLFeBOOK

GAUSSIAN QUADRATURE
387
then p(xj) = f (xj), and if we have
h(1)
i (xj) = 0, ˆh(1)
i (xj) = δi−j,
(9.75b)
then p(1)(xj) = f (1)(xj) for all j = 0, 1, . . . , n. Using (9.75), it is possible to
arrive at the conclusion that
hi(x) = [1 −2L(1)
i (xi)(x −xi)][Li(x)]2
(9.76a)
and
ˆhi(x) = (x −xi)[Li(x)]2.
(9.76b)
Equation (9.74) along with (9.76) is Hermite’s interpolating formula. [Both parts
of Eq. (9.76) are derived on pp. 383–384 of Ref. 5, as well as in Theorem 6.1 of
Chapter 6 (below).] It is further possible to prove that for p(x) in (9.74) we have
the error function
e(x) = f (x) −p(x) =
1
(2n + 2)!f (2n+2)(ξ)[π(x)]2,
(9.77)
where ξ ∈[a, b] and ξ = ξ(x).
From (9.77) and (9.74), we obtain
f (x) =
n

k=0
hk(x)f (xk) +
n

k=0
ˆhk(x)f (1)(xk) +
1
(2n + 2)!f (2n+2)(ξ(x))[π(x)]2.
(9.78)
Suppose that w(x) ≥0 for x ∈[a, b]. Function w(x) is intended to be a weighting
function such as seen in Chapter 5. Consequently, from (9.78)
 b
a
w(x)f (x) dx =
n

k=0
 b
a
w(x)hk(x) dx

f (xk)
+
n

k=0
 b
a
w(x)ˆhk(x) dx

f (1)(xk)
+
1
(2n + 2)!
 b
a
f (2n+2)(ξ(x))w(x)[π(x)]2 dx
#
$%
&
=E
,
(9.79)
where a < ξ(x) < b, if a < xk < b. This can be rewritten as
 b
a
w(x)f (x) dx =
n

k=0
Hkf (xk) +
n

k=0
ˆHkf (1)(xk) + E,
(9.80)
TLFeBOOK

388
NUMERICAL INTEGRATION AND DIFFERENTIATION
where
Hk =
 b
a
w(x)hk(x) dx =
 b
a
w(x)[1 −2L(1)
k (xk)(x −xk)][Lk(x)]2 dx (9.81a)
and
ˆHk =
 b
a
w(x)ˆhk(x) dx =
 b
a
w(x)(x −xk)[Lk(x)]2 dx.
(9.81b)
If we neglect the term E in (9.80), then the resulting approximation to
" b
a w(x)
f (x) dx is called the Hermite quadrature formula. Since we are assuming that
w(x) ≥0, the second mean-value theorem for integrals allows us to claim that
E =
1
(2n + 2)!f (2n+2)(ξ)
 b
a
w(x)[π(x)]2 dx
(9.82)
for some ξ ∈[a, b].
Now, recalling (9.72), we see that (9.81b) can be rewritten as
ˆHk =
 b
a
w(x)(x −xk)

π(x)
π(1)(xk)(x −xk)
2
dx
=
1
π(1)(xk)
 b
a
w(x)π(x)

π(x)
π(1)(xk)(x −xk)

dx
=
1
π(1)(xk)
 b
a
w(x)π(x)Lk(x) dx.
(9.83)
We recall from Chapter 5 that an inner product on L2[a, b] is (f, g are real-valued)
⟨f, g⟩=
 b
a
w(x)f (x)g(x) dx.
(9.84)
Thus, ˆHk = 0 for k = 0, 1, . . . , n if π(x) is orthogonal to Lk(x) over [a, b] with
respect to the weighting function w(x). Since deg(Lk(x)) = n (all k), this will be
the case if π(x) is orthogonal to all polynomials of degree ≤n over [a, b] with
respect to the weighting function w(x). Note that deg(π(x)) = n + 1 [recall (9.69)].
In fact, the polynomial π(x) of degree n + 1 is orthogonal to all polynomials
of degree ≤n over [a, b] with respect to w(x), the Hermite quadrature formula
reduces to the simpler form
 b
a
w(x)f (x) dx =
n

k=0
Hkf (xk) + E,
(9.85)
where
E =
1
(2n + 2)!f (2n+2)(ξ)
 b
a
w(x)[π(x)]2 dx,
(9.86)
TLFeBOOK

GAUSSIAN QUADRATURE
389
and where x0, x1, . . . , xn are the zeros of π(x) (such that a < xk < b). A formula of
this type is called a Gaussian quadrature formula. The weights Hk are sometimes
called Christoffel numbers. We see that this numerical integration methodology
requires us to possess samples of f (x) at the zeros of π(x). Variations on this
theory can be used to remove this restriction [6], but we do not consider this
matter in this book. However, if f (x) is known at all x ∈[a, b], then this is not a
serious restriction.
In any case, to apply the approximation
 b
a
w(x)f (x) dx ≈
n

k=0
Hkf (xk),
(9.87)
it is clear that we need a method to determine the Christoffel numbers Hk. It is
possible to do this using the Christoffel–Darboux formula [Eq. (5.11); see also
Theorem 5.2]. From this it can be shown that
Hk = −
φn+2,n+2
φn+1,n+1φ(1)
n+1(xk)φn+2(xk)
,
(9.88)
where polynomial φr(x) is obtained, for instance, from (5.5) and where
φn+1(x) = φn+1,n+1π(x).
(9.89)
Thus, we identify the zeros of π(x) with the zeros of orthogonal polynomial
φn+1(x) [recalling that ⟨φi, φj⟩= δi−j with respect to inner product (9.84)].
Since there are an inﬁnite number of choices for orthogonal polynomials φk(x)
and there is a theory for creating them (Chapter 5), it is possible to choose π(x) in
an inﬁnite number of ways. We have implicitly assumed that [a, b] is a ﬁnite length
interval, but this assumption is actually entirely unnecessary. Inﬁnite or semiinﬁnite
intervals of integration are permitted. Thus, for example, π(x) may be associated
with the Hermite polynomials of Section 5.4, as well as with the Chebyshev or
Legendre polynomials.
Let us consider as an example the case of Chebyshev polynomials of the ﬁrst
kind (ﬁrst seen in Section 5.3). In this case
w(x) =
1
√
1 −x2
with [a, b] = [−1, 1], and we will obtain the Chebyshev–Gauss quadrature rule.
The Chebyshev polynomials of the ﬁrst kind are Tk(x) = cos[k cos−1 x], so, via
(5.55) for k > 0, we have
φk(x) =
8
2
π Tk(x).
(9.90)
TLFeBOOK

390
NUMERICAL INTEGRATION AND DIFFERENTIATION
From the recursion for Chebyshev polynomials of the ﬁrst kind [Eq. (5.57)], we
have (k > 0)
Tk,k = 2k−1
(9.91)

Tk(x) = k
j=0 Tk,jxj
. Thus
φk,k =
8
2
π 2k−1.
(9.92)
We have that Tk(x) = 0 for
x = xi = cos
2i + 1
2k
π

(9.93)
(i = 0, 1, . . . , k −1). Additionally
T (1)
k
(x) = k sin[k cos−1 x]
1
√
1 −x2 ,
(9.94)
so, if, for convenience, we deﬁne αi = 2i+1
2k π, then xi = cos αi, and therefore
T (1)
k
(xi) = k sin(kαi)
sin αi
= k
sin
2i + 1
2
π

sin αi
= k (−1)i
sin αi
.
(9.95)
Also
Tk+1(xi) = cos

(k + 1)
 2i + 1
2k
π
!
= cos
2i + 1
2
π + 2i + 1
2k
π

= cos
2i + 1
2
π

cos
2i + 1
2k
π

−sin
2i + 1
2
π

sin
2i + 1
2k
π

= −sin
2i + 1
2
π

sin αi = (−1)i+1 sin αi.
(9.96)
Therefore, (9.88) becomes
Hk = −
8
2
π 2n+1
'8
2
π 2n
( '8
2
π (n + 1) (−1)k
sin αk
( '8
2
π (−1)k+1 sin αk
( =
π
n + 1. (9.97)
Thus, the weights (Christoffel numbers) are all the same in this particular case. So,
(9.87) is now
 1
−1
f (x)
√
1 −x2 dx ≈
π
n + 1
n

k=0
f

cos
 2k + 1
2n + 2π
!
= C(n).
(9.98)
TLFeBOOK

GAUSSIAN QUADRATURE
391
The error expression E in (9.86) can also be reduced accordingly, but we will omit
this here (again, see Hildebrand [5]).
A simple example of the application of (9.98) is as follows.
Example 9.3
Suppose that f (x) =
√
1 −x2, in which case
 1
−1
f (x)
√
1 −x2 dx =
 1
−1
dx = 2.
For this case (9.98) becomes
 1
−1
dx ≈C(n) =
π
n + 1
n

k=0
sin
2k + 1
2n + 2π

.
For various n, we have the following table of values:
n
C(n)
1
2.2214
2
2.0944
5
2.0230
10
2.0068
20
2.0019
100
2.0001
Finally, we remark that the error expression in (9.82) suggests that the method
of this section is worth applying only if f (x) is sufﬁciently smooth. This is con-
sistent with comments made in Section 9.3 regarding how to choose between the
trapezoidal and Simpson’s rules. In the next example f (x) = e−x, which is a very
smooth function.
Example 9.4
Here we will consider
I =
 1
−1
e−x dx = e −1
e = 2.350402387
and compare the approximation to I obtained by applying Simpson’s rule and
Legendre–Gauss quadrature. We will assume n = 2 in both cases.
Let us ﬁrst consider application of Simpson’s rule. Since x0 = a = −1, x1 =
0, x1 = b = 1 (h = (b −a)/n = (1 −(−1))/2 = 1), we have via (9.41)
S(2) = 1
3[e+1 + 4e0 + e−1] = 2.362053757
TLFeBOOK

392
NUMERICAL INTEGRATION AND DIFFERENTIATION
for which the error is
ES(2) = I −S(2) = −0.011651.
Now let us consider the Legendre–Gauss quadrature for our problem. We recall
that for Legendre polynomials the weight function is w(x) = 1 for all x ∈[−1, 1]
(Section 5.5). From Section 5.6 we have
φ3(x) =
1
||P3||P3(x) =
8
7
2
1
2[5x3 −3x],
φ4(x) =
1
||P4||P4(x) =
8
9
2
1
8[35x4 −30x2 + 3].
Consequently, φ3,3 = 5
2
7
7
2, and the zeros of φ3(x) are at x = 0, ±
7
3
5, so now our
sample points (grid points, mesh points) are
x0 = −
8
3
5, x1 = 0, x2 = +
8
3
5.
Hence, since φ(1)
3 (x) =
7
7
2
1
2[15x2 −3], we have
φ(1)
3 (x0) = 3
8
7
2, φ(1)
3 (x1) = −3
2
8
7
2, φ(1)
3 (x2) = 3
8
7
2.
Also, φ4,4 = 35
8
7
9
2, and
φ4(x0) = −3
10
8
9
2, φ4(x1) = 3
8
8
9
2, φ4(x2) = −3
10
8
9
2.
Therefore, from (9.88) the Christoffel numbers are
H0 = 5
9, H1 = 8
9, H2 = 5
9.
From (9.87) the resulting quadrature is
 1
−1
e−x dx ≈H0e−x0 + H1e−x1 + H2e−x2 = L(2)
with
L(2) = 2.350336929,
TLFeBOOK

ROMBERG INTEGRATION
393
and the corresponding error is
EL(2) = I −L(2) = 6.5458 × 10−5.
Clearly, |EL(2)| ≪|ES(2)|. Thus, the Legendre-Gauss quadrature is much more
accurate than Simpson’s rule. Considering how small n is here, the accuracy of the
Legendre–Gauss quadrature is remarkably high.
9.5
ROMBERG INTEGRATION
Romberg integration is a recursive procedure that seeks to improve on the trape-
zoidal and Simpson rules. But before we consider this numerical integration
methodology, we will look at some more basic ideas.
Suppose that I =
" b
a f (x) dx and I (n) is a quadrature that approximates I. For
us, I (n) will be either the trapezoidal rule T (n) from (9.14), or else it will be
Simpson’s rule S(n) from (9.41). It could also be the corrected trapezoidal rule
TC(n), which is considered below [see either (9.104), or (9.107)].
It is possible to improve on the “basic” trapezoidal rule from Section 9.2. Begin
by recalling (9.27)
ET (n) = −1
12h3
n

k=1
f (2)(ξk),
(9.99)
where h = (b −a)/n, x0 = a, xn = b, and ξk ∈[xk−1, xk]. Of course, xk −xk−1 =
h (uniform sampling grid). Assuming (as usual) that f (k)(x) is Riemann integrable
for all k ≥0, then
lim
n→∞
n

k=1
hf (2)(ξk) = f (1)(b) −f (1)(a) =
 b
a
f (2)(x) dx.
(9.100)
Thus, we have the approximation
n

k=1
hf (2)(ξk) ≈f (1)(b) −f (1)(a).
(9.101)
Consequently,
ET (n) = −1
12h2
n

k=1
hf (2)(ξk) ≈−1
12h2[f (1)(b) −f (1)(a)]
(9.102)
or
I −T (n) ≈−1
12h2[f (1)(b) −f (1)(a)].
(9.103)
TLFeBOOK

394
NUMERICAL INTEGRATION AND DIFFERENTIATION
This immediately suggests that we can improve on the trapezoidal rule by replacing
T (n) with the new approximation
TC(n) = T (n) −1
12h2[f (1)(b) −f (1)(a)],
(9.104)
where TC(n) denotes the corrected trapezoidal rule approximation to I. Clearly,
once we have T (n), rather little extra effort is needed to obtain TC(n).
In fact, we do not necessarily need to know f (1)(x) exactly anywhere, much
less at the points x = a or x = b. In the next section we will argue that either
f (1)(x) = 1
2h[−3f (x) + 4f (x + h) −f (x + 2h)] + 1
3h2f (3)(ξ)
(9.105a)
or that
f (1)(x) = 1
2h[3f (x) −4f (x −h) + f (x −2h)] + 1
3h2f (3)(ξ).
(9.105b)
In (9.105a) ξ ∈[x, x + 2h], while in (9.105b) ξ ∈[x −2h, x]. Consequently, with
x0 = a for ξ0 ∈[a, a + 2h], we have [via (9.105a)]
f (1)(x0) = f (1)(a) = 1
2h[−3f (x0) + 4f (x1) −f (x2)] + 1
3h2f (3)(ξ0), (9.106a)
and with xn = b for ξn ∈[b −2h, b], we have [via (9.105b)]
f (1)(xn) = f (1)(b) = 1
2h[3f (xn) −4f (xn−1) + f (xn−2)] + 1
3h2f (3)(ξn).
(9.106b)
Thus, (9.104) becomes (approximate corrected trapezoidal rule)
TC(n) = T (n) −h
24[3f (xn) −4f (xn−1) + f (xn−2) + 3f (x0) −4f (x1) + f (x2)]
−h4
36[f (3)(ξn) −f (3)(ξ0)].
(9.107)
Of course, in evaluating (9.107) we would exclude the terms involving f (3)(ξ0),
and f (3)(ξn).
As noted in Epperson [7] for the trapezoidal and Simpson rules
I −I (n) ∝1
np ,
(9.108)
where p = 2 for I (n) = T (n) and p = 4 for I (n) = S(n). In other words, for a
given rule and a suitable constant C we must have I −I (n) ≈Cn−p. Now observe
TLFeBOOK

ROMBERG INTEGRATION
395
that we may deﬁne the ratio
r4n = I (n) −I (2n)
I (2n) −I (4n) ≈
(I −Cn−p) −(I −C(2n)−p)
(I −C(2n)−p) −(I −C(4n)−p)
=
(2n)−p −n−p
(4n)−p −(2n)−p =
2−p −1
4−p −2−p = 2p.
(9.109)
Immediately we conclude that
p ≈log2 r4n = log10 r4n
log10 2 .
(9.110)
This is useful as a check on program implementation of our quadratures. If (9.110)
is not approximately satisﬁed when we apply the trapezoidal or Simpson rules,
then (1) the integrand f (x) is not smooth enough for our theories to apply, (2)
there is a “bug” in the program, or (3) the error may not be decreasing quickly
with n because it is already tiny to begin with, as might happen when integrating
an oscillatory function using Simpson’s rule.
The following examples illustrate the previous principles.
Example 9.5
In this example we consider approximating
I =
 π/2
0
sin x dx = 1
using T (n) [via (9.14)], TC(n) [via (9.104)], and S(n) [via (9.41)]. Parameter p in
(9.110) is computed for each of these cases, and is displayed in the following table
(where “NaN” means “not a number”):
n
T (n)
p for T (n)
TC(n)
p for TC(n)
S(n)
p for S(n)
2 0.94805945
NaN
0.99946364
NaN
1.00227988
NaN
4 0.98711580
NaN
0.99996685
NaN
1.00013458
NaN
8 0.99678517
2.0141
0.99999793
4.0169
1.00000830
4.0864
16 0.99919668
2.0035
0.99999987
4.0042
1.00000052
4.0210
32 0.99979919
2.0009
0.99999999
4.0010
1.00000003
4.0052
64 0.99994980
2.0002
1.00000000
4.0003
1.00000000
4.0013
128 0.99998745
2.0001
1.00000000
4.0001
1.00000000
4.0003
256 0.99999686
2.0000
1.00000000
4.0000
1.00000000
4.0001
We see that since f (x) = sin x is a rather smooth function we obtain the values
for p that we expect to see.
Example 9.6
This example is in contrast with the previous one. Here we
approximate
I =
 1
0
x1/3 dx = 3
4
TLFeBOOK

396
NUMERICAL INTEGRATION AND DIFFERENTIATION
using T (n) [via (9.14)], TC(n) [via (9.107)], and S(n) [via (9.41)]. Again, p
[via (9.110)] is computed for each of these cases, and the results are tabulated
as follows:
n
T (n)
p for T (n)
TC(n)
p for TC(n)
S(n)
p for S(n)
2 0.64685026
NaN
0.69580035
NaN
0.69580035
NaN
4 0.70805534
NaN
0.72437494
NaN
0.72845703
NaN
8 0.73309996
1.2892
0.73980487
0.8890
0.74144817
1.3298
16 0.74322952
1.3059
0.74595297
1.3275
0.74660604
1.3327
32 0.74729720
1.3163
0.74839388
1.3327
0.74865310
1.3332
64 0.74892341
1.3227
0.74936261
1.3333
0.74946548
1.3333
128 0.74957176
1.3267
0.74974705
1.3333
0.74978788
1.3333
256 0.74982980
1.3292
0.74989962
1.3333
0.74991582
1.3333
We observe that f (x) = x1/3, but that f (1)(x) = 1
3x−2/3, f (2)(x) = −2
9x−5/3,
etc. Thus, the derivatives of f (x) are unbounded at x = 0, and so f (x) is not
smooth on the interval of integration. This explains why we obtain p ≈1.3333 in
all cases.
As a further step toward Romberg integration, consider the following. Since
I −I (2n) ≈C(2n)−p = C2−pn−p ≈2−p(I −I (n)), we obtain the approximate
equality
I ≈I (2n) −2−pI (n)
1 −2−p
,
or
I ≈2pI (2n) −I (n)
2p −1
= R(2n).
(9.111)
We call R(2n) Richardson’s extrapolated value (or Richardson’s extrapolation),
which is an improvement on I (2n). The estimated error in the extrapolation is
given by
ER(2n) = I (2n) −R(2n) = I (n) −I (2n)
2p −1
.
(9.112)
Of course, p in (9.111) and (9.112) must be the proper choice for the quadrature
I (n). We may “conﬁrm” that (9.111) works for T (n) (as an example) by considering
(9.28)
ET (n) = −h2
12(b −a)f (2)(ξ)
(9.113)
(for some ξ ∈[a, b]). Clearly
ET (2n) = −(h/2)2
12
(b −a)f (2)(ξ) ≈1
4ET (n),
TLFeBOOK

ROMBERG INTEGRATION
397
or in other words (ET (n) = I −T (n))
I −T (2n) ≈1
4[I −T (n)],
and hence for n ≥1
I ≈4T (2n) −T (n)
3
= RT (2n),
(9.114)
which is (9.111) for case p = 2. Equation (9.114) is called the Romberg integration
formula for the trapezoidal rule. Of course, a similar expression may be obtained
for Simpson’s rule [with p = 4 in (9.111)]; that is, for n even
I ≈16S(2n) −S(n)
15
= RS(2n).
(9.115)
In fact, it can also be shown that RT (2n) = S(2n):
S(2n) = 4T (2n) −T (n)
3
.
(9.116)
(Perhaps this is most easily seen in the special case where n = 1.) In other words,
the Romberg procedure applied to the trapezoidal rule yields the Simpson rule.
Now, Romberg integration is really the repeated application of the Richardson
extrapolation idea to the composite trapezoidal rule. A simple way to visualize the
process is with the Romberg table (Romberg array):
T (1)
T (2)
S(2)
T (4)
S(4)
RS(4)
T (8)
S(8)
RS(8)
•
T (16)
S(16)
RS(16)
•
•
...
...
...
...
...
In its present form the table consists of only three columns, but the recursive
process may be continued to produce a complete “triangular array.”
The complete Romberg integration procedure is often fully justiﬁed and devel-
oped with respect to the following theorem.
Theorem 9.1: Euler–Maclaurin Formula
Let f ∈C2k+2[a, b] for some
k ≥0, and let us approximate I =
" b
a f (x) dx by the composite trapezoidal rule
TLFeBOOK

398
NUMERICAL INTEGRATION AND DIFFERENTIATION
of (9.14). Letting hn = (b −a)/n for n ≥1, we have
T (n) = I +
k

i=1
B2i
(2i)!h2i
n [f (2i−1)(b) −f (2i−1)(a)]
+
B2k+2
(2k + 2)!h2k+2
n
(b −a)f (2k+2)(η),
(9.117)
where η ∈[a, b], and for j ≥1
B2j = (−1)j−1
 ∞

n=1
2
(2πn)2j
	
(2j)!
(9.118)
are the Bernoulli numbers.
Proof
This is Property 9.3 in Quarteroni et al. [8]. A proof appears in Ralston
[9]. Alternative descriptions of the Bernoulli numbers appear in Gradshteyn and
Ryzhik [10]. Although not apparent from (9.118), the Bernoulli numbers are all
rational numbers.
We will present the complete Romberg integration process in a more straight-
forward manner. Begin by considering the following theorem.
Theorem 9.2: Recursive Trapezoidal Rule
Suppose that h = (b −a)/(2n);
then, for n ≥1 (x0 = a, xn = b)
T (2n) = 1
2T (n) + h
n

k=1
f (x0 + (2k −1)h).
(9.119)
The ﬁrst column in the Romberg table is given by
T (2n) = 1
2T (2n−1) + xn −x0
2n
2n−1

k=1
f
 
x0 + (2k −1)(xn −x0)
2n
!
(9.120)
for all n ≥1.
Proof
Omitted, but clearly (9.120) immediately follows from (9.119).
We let R(k)
n
denote the row n, column k entry of the Romberg table, where k =
0, 1, . . . , N and n = k, . . . , N [i.e., we construct an (N + 1) × (N + 1) lower trian-
gular array, as suggested earlier]. Table entries are “blank” for n = 0, 1, . . . , k −1
in column k. The ﬁrst column of the table is certainly
R(0)
n
= T (2n)
(9.121)
TLFeBOOK

ROMBERG INTEGRATION
399
for n = 0, 1, . . . , N. From Theorem 9.2 we must have
R(0)
0
= b −a
2
[f (a) + f (b)],
(9.122a)
R(0)
n
= 1
2R(0)
n−1 + b −a
2n
2n−1

k=1
f
 
a + (2k −1)(b −a)
2n
!
,
(9.122b)
for n = 1, 2, . . . , N. Equations (9.122) are the algorithm for constructing the ﬁrst
column of the Romberg table. The second column is R(1)
n , and these numbers are
given by [via (9.114)]
R(1)
n
=
4R(0)
n
−R(0)
n−1
4 −1
(9.123)
for n = 1, 2, . . . , N. Similarly, the third column is R(2)
n , and these numbers are
given by [via (9.115)]
R(2)
n
=
42R(1)
n
−R(1)
n−1
42 −1
(9.124)
for n = 2, 3, . . . , N. The pattern suggested by (9.123) and (9.124) generalizes
according to
R(k)
n
=
4kR(k−1)
n
−R(k−1)
n−1
4k −1
(9.125)
for n = k, . . . , N, with k = 1, 2, . . . , N. Assuming that f (x) is sufﬁciently smooth,
we can estimate the error using the Richardson extrapolation method in this manner:
E(k)
n
=
R(k)
n−1 −R(k)
n
4k −1
.
(9.126)
This can be used to stop the recursive process of table construction when E(k)
n
is small enough. Recall that every entry R(k)
n
in the table is an estimate of I =
" b
a f (x) dx. In some sense R(N)
N
is the “ﬁnal estimate,” and will be the best one if
f (x) is smooth enough. Finally, the general appearance of the Romberg table is
R(0)
0
R(0)
1
R(1)
1
R(0)
2
R(1)
2
R(2)
2
...
...
...
...
R(0)
N−1
R(1)
N−1
R(2)
N−1
· · ·
R(N−1)
N−1
R(0)
N
R(1)
N
R(2)
N
· · ·
R(N−1)
N
R(N)
N
TLFeBOOK

400
NUMERICAL INTEGRATION AND DIFFERENTIATION
The Romberg integration procedure is efﬁcient. Function evaluation is conﬁned
to the construction of the ﬁrst column. The remaining columns are ﬁlled in with
just a ﬁxed (and small) number of arithmetic operations per entry as determined
by (9.125).
Example 9.7
Begin by considering the Romberg approximation to
I =
 1
0
ex dx = 1.718281828.
The Romberg table for this is (N = 3)
1.85914091
1.75393109
1.71886115
1.72722190
1.71831884
1.71828269
1.72051859
1.71828415
1.71828184
1.71828183
Table entry R(3)
3
= 1.71828183 is certainly the most accurate estimate of I. Now
contrast this example with the next one.
The zeroth-order modiﬁed Bessel function of the ﬁrst kind I0(y) ∈R(y ∈R)
is important in applied probability. For example, it appears in the problem of
computing bit error probabilities in amplitude shift keying (ASK) digital data com-
munications [11]. There is a series expansion expression for I0(y), but there is also
an integral form, which is
I0(y) = 1
2π
 2π
0
ey cos x dx.
(9.127)
For y = 1, we have (according to MATLAB’s besseli function; I0(y) =besseli(0,y))
I0(1) = 1.2660658778.
The Romberg table of estimates for this integral is (N = 4)
2.71828183
1.54308063
1.15134690
1.27154032
1.18102688
1.18300554
1.26606608
1.26424133
1.26978896
1.27116647
1.26606588
1.26606581
1.26618744
1.26613028
1.26611053
Plainly, table entry R(4)
4
= 1.26611053 is not as accurate as R(0)
4
= 1.26606588.
Apparently, the integrand of (9.127) is simply not smooth enough to beneﬁt from
the Romberg approach.
TLFeBOOK

NUMERICAL DIFFERENTIATION
401
9.6
NUMERICAL DIFFERENTIATION
A simple theory of numerical approximation to the derivative can be obtained via
Taylor series expansions (Chapter 3). Recall that [via (3.71)]
f (x + h) =
n

k=0
hk
k! f (k)(x) +
hn+1
(n + 1)!f (n+1)(ξ)
(9.128)
for suitable ξ ∈[x, x + h]. As usual, 0! = 1, and f (x) = f (0)(x). Since from ele-
mentary calculus
f (1)(x) = lim
h→0
f (x + h) −f (x)
h
⇒f (1)(x) ≈f (x + h) −f (x)
h
(9.129)
from (9.128), we have
f (1)(x) = f (x + h) −f (x)
h
−1
2!hf (2)(ξ).
(9.130)
This was obtained simply be rearranging
f (x + h) = f (x) + hf (1)(x) + 1
2!h2f (2)(ξ).
(9.131)
We may write
f (1)(x) = f (x + h) −f (x)
h
#
$%
&
= ˜f (1)
f (x)
+

−1
2!hf (2)(ξ)

#
$%
&
=ef (x)
.
(9.132)
Approximation ˜f (1)
f (x) is called the forward difference approximation to f (1)(x),
and the error ef (x) is seen to be approximately proportional to h. Now consider
(ξ1 ∈[x, x + h])
f (x + h) = f (x) + hf (1)(x) + 1
2!h2f (2)(x) + 1
3!h3f (3)(ξ1),
(9.133)
and clearly (ξ2 ∈[x −h, x])
f (x −h) = f (x) −hf (1)(x) + 1
2!h2f (2)(x) −1
3!h3f (3)(ξ2).
(9.134)
From (9.134)
f (1)(x) = f (x) −f (x −h)
h
#
$%
&
= ˜f (1)
b
(x)
+
 1
2!hf (2)(ξ)

#
$%
&
=eb(x)
.
(9.135)
TLFeBOOK

402
NUMERICAL INTEGRATION AND DIFFERENTIATION
Here the approximation ˜f (1)
b (x) is called the backward difference approximation to
f (1)(x), and has error eb(x) that is also approximately proportional to h. However,
an improvement is possible, and this is obtained by subtracting (9.134) from (9.133)
f (x + h) −f (x −h) = 2hf (1)(x) + 1
3!h3[f (3)(ξ1) + f (3)(ξ2)],
or on rearranging this, we have
f (1)(x) = f (x + h) −f (x −h)
2h
#
$%
&
= ˜f (1)
c
(x)
+

−h2
6
f (3)(ξ1) + f (3)(ξ2)
2
	
#
$%
&
=ec(x)
.
(9.136)
Recalling the derivation of (9.28), there is a ξ ∈[x −h, x + h] such that
f (3)(ξ) = 1
2[f (3)(ξ1) + f (3)(ξ2)]
(9.137)
(ξ1 ∈[x, x + h], ξ2 ∈[x −h, h]). Hence, (9.136) can be rewritten as (for some
ξ ∈[x −h, x + h])
f (1)(x) = f (x + h) −f (x −h)
2h
#
$%
&
= ˜f (1)
c
(x)
+

−h2
6 f (3)(ξ)

#
$%
&
=ec(x)
.
(9.138)
Clearly, the error ec(x), which is the error of the central difference approximation
˜f (1)
c
(x) to f (1)(x), is proportional to h2. Thus, if f (x) is smooth enough, the
central difference approximation is more accurate than the forward or backward
difference approximations.
The errors ef (x), eb(x), and ec(x) are truncation errors in the approximations.
Of course, when implementing any of these approximations on a computer there
will be rounding errors, too. Each approximation can be expressed in the form
˜f (1)(x) = 1
hf T y,
(9.139)
where for the forward difference approximation
f = [f (x)
f (x + h)]T , y = [−1
1]T ,
(9.140a)
for the backward difference approximation
f = [f (x −h)
f (x)]T , y = [−1
1]T ,
(9.140b)
TLFeBOOK

NUMERICAL DIFFERENTIATION
403
and for the central difference approximation
f = [f (x −h)
f (x + h)]T , y = 1
2[−1
1]T .
(9.140c)
Thus, the approximations are all Euclidean inner products of samples of f (x) with
a vector of constants y, followed by division by h. This is structurally much the
same kind of computation as numerical integration. In fact, an upper bound on the
size of the error due to rounding is given by the following theorem.
Theorem 9.3: Since f l[ ˜f (1)(x)] = f l

f l[f T y]
h

, we have (f, y ∈Rm)
|f l[ ˜f (1)(x)] −˜f (1)(x)| ≤u
h[1.01m + 1]||f ||2||y||2.
(9.141)
Proof
Our analysis here is rather similar to Example 2.4 in Chapter 2. Thus,
we exploit yet again the results from Chapter 2 on rounding errors in dot product
computation.
Via (2.41)
f l[f T y] = f T y(1 + ϵ1)
for which
|ϵ1| ≤1.01mu|f |T |y|
|f T y| .
In addition
f l[ ˜f (1)(x)] = f T y
h (1 + ϵ1)(1 + ϵ),
where |ϵ| ≤u. Thus, since
f l[ ˜f (1)(x)] = f T y
h
+ f T y
h (ϵ1 + ϵ + ϵ1ϵ),
we have
|f l[ ˜f (1)(x)] −˜f (1)(x)| ≤|f T y|
h
(|ϵ1| + |ϵ| + |ϵ1||ϵ|) = |f T y|
h
×
 
1.01mu|f |T |y|
|f T y| + u + 1.01mu2 |f |T |y|
|f T y|
!
,
and since u2 ≪u, we may neglect the last term, yielding
|f l[ ˜f (1)(x)] −˜f (1)(x)| ≤1
h

1.01mu|f |T |y| + u|f T y|

.
TLFeBOOK

404
NUMERICAL INTEGRATION AND DIFFERENTIATION
But |f |T |y| =< |f |, |y| >≤||f ||2||y||2, and |f T y| = |⟨f, y⟩| ≤||f ||2||y||2 via
Theorem 1.1, so ﬁnally we have
|f l[ ˜f (1)(x)] −˜f (1)(x)| ≤u
h[1.01m + 1]||f ||2||y||2,
which is the theorem statement.
The bound in (9.141) suggests that, since we treat u, m, ||f ||2, and ||y||2 are
ﬁxed, as h becomes smaller, the rounding errors will grow in size. On the other
hand, as h becomes smaller, the truncation errors diminish in size. Thus, much as
with numerical integration (recall Fig. 9.3), there is a tradeoff between rounding
and truncation errors leading in the present case to the existence of some optimal
value for the choice of h. In most practical circumstances the truncation errors will
dominate, however.
We recall that interpolation theory from Chapter 6 was useful in developing
theories on numerical integration in earlier sections of the present chapter. We
therefore reasonably expect that interpolation ideas from Chapter 6 ought to be
helpful in developing approximations to the derivative.
Recall that pn(x) = n
k=0 pn,kxk interpolates f (x) for x ∈[a, b], i.e., pn(xk) =
f (xk) with x0 = a, xn = b, and xk ∈[a, b] for all k, but such that xk ̸= xj for
k ̸= j. If f (x) ∈Cn+1[a, b], then, from (6.14)
f (x) = pn(x) +
1
(n + 1)!f (n+1)(ξ)
n
/
i=0
(x −xi)
(9.142)
(ξ = ξ(x) ∈[a, b]). For convenience we also deﬁned π(x) = 0n
i=0(x −xi). Thus,
from (9.142)
f (1)(x) = p(1)
n (x) +
1
(n + 1)!

π(1)(x)f (n+1)(ξ) + π(x) d
dx f (n+1)(ξ)

. (9.143)
Since ξ = ξ(x) is a function of x that is seldom known, it is not at all clear
what df (n+1)(ξ)/dx is in general, and to evaluate this also assumes that dξ(x)/dx
exists.3 We may sidestep this problem by evaluating (9.143) only for x = xk, and
since π(xk) = 0 for all k, Eq. (9.143) reduces to
f (1)(xk) = p(1)
n (xk) +
1
(n + 1)!π(1)(xk)f (n+1)(ξ(xk)).
(9.144)
For simplicity we will now assume that xk = x0 + hk, h = (b −a)/n (i.e.,
uniform sampling grid). We will also suppose that n = 2, in which case (9.144)
becomes
f (1)(xk) = p(1)
2 (xk) + 1
6π(1)(xk)f (3)(ξ(xk)).
(9.145)
3This turns out to be true, although it is not easy to prove. Fortunately, we do not need this result.
TLFeBOOK

NUMERICAL DIFFERENTIATION
405
Since π(x) = (x −x0)(x −x1)(x −x2), we have π(1)(x) = (x −x1)(x −x2) +
(x −x0)(x −x1) + (x −x0)(x −x2), and therefore
π(1)(x0) = (x0 −x1)(x0 −x2)
= (−h)(−2h)
= 2h2,
π(1)(x1) = (x1 −x0)(x1 −x2)
= h(−h)
= −h2,
π(1)(x2) = (x2 −x0)(x2 −x1)
= (2h)h
= 2h2.
(9.146)
From (6.9), we obtain
p2(x) = f (x0)L0(x) + f (x1)L1(x) + f (x2)L2(x),
(9.147)
where, from (6.11)
Lj(x) =
2
/
i=0
i̸=j
x −xi
xj −xi
,
(9.148)
and hence the approximation to f (1)(xk) is (k ∈{0, 1, 2})
p(1)
n (xk) = f (x0)L(1)
0 (xk) + f (x1)L(1)
1 (xk) + f (x2)L(1)
2 (xk).
(9.149)
From (9.148)
L0(x) =
(x−x1)(x−x2)
(x0−x1)(x0−x2),
L(1)
0 (x) = (x−x1)+(x−x2)
(x0−x1)(x0−x2),
L1(x) =
(x−x0)(x−x2)
(x1−x0)(x1−x2),
L(1)
1 (x) = (x−x0)+(x−x2)
(x1−x0)(x1−x2),
L2(x) =
(x−x0)(x−x1)
(x2−x0)(x2−x1),
L(1)
2 (x) = (x−x0)+(x−x1)
(x2−x0)(x2−x1).
(9.150)
Hence
L(1)
0 (x0) = −3
2h, L(1)
0 (x1) = −1
2h, L(1)
0 (x2) = + 1
2h,
L(1)
1 (x0) = + 2
h, L(1)
1 (x1) = 0, L(1)
1 (x2) = −2
h,
(9.151)
L(1)
2 (x0) = −1
2h, L(1)
2 (x1) = + 1
2h, L(1)
2 (x2) = + 3
2h.
We therefore have the following expressions for f (1)(xk):
f (1)(x0) = 1
2h[−3f (x0) + 4f (x1) −f (x2)] + 1
3h2f (3)(ξ(x0)),
(9.152a)
f (1)(x1) = 1
2h[−f (x0) + f (x2)] −1
6h2f (3)(ξ(x1)),
(9.152b)
f (1)(x2) = 1
2h[f (x0) −4f (x1) + 3f (x2)] + 1
3h2f (3)(ξ(x2)).
(9.152c)
TLFeBOOK

406
NUMERICAL INTEGRATION AND DIFFERENTIATION
We recognize that the case (9.152b) contains the central difference approximation
[recall (9.138)], since we may let x0 = x −h, x2 = x + h (and x1 = x). If we let
x0 = x, x1 = x + h and x2 = x + 2h, then (9.152a) yields
f (1)(x) ≈1
2h[−3f (x) + 4f (x + h) −f (x + 2h)],
(9.153)
and if we let x2 = x, x1 = x −h, and x0 = x −2h, then (9.152c) yields
f (1)(x) ≈1
2h[f (x −2h) −4f (x −h) + 3f (x)].
(9.154)
Note that (9.153) and (9.154) were employed in obtaining TC(n) in (9.107).
REFERENCES
1. G. E. Forsythe, M. A. Malcolm, and C. B. Moler, Computer Methods for Mathematical
Computations, Prentice-Hall, Englewood Cliffs, NJ, 1977.
2. L. Bers, Calculus: Preliminary Edition, Vol. 2, Holt, Rinehart, Winston, New York,
1967.
3. P. J. Davis and P. Rabinowitz, Numerical Integration, Blaisdell, Waltham, MA, 1967.
4. T. W. K¨orner, Fourier Analysis, Cambridge Univ. Press, New York, 1988.
5. F. B. Hildebrand, Introduction to Numerical Analysis, 2nd ed., McGraw-Hill, New York,
1974.
6. W. Sweldens and R. Piessens, “Quadrature Formulae and Asymptotic Error Expansions
for Wavelet Approximations of Smooth Functions,” SIAM J. Numer. Anal. 31, 1240–
1264 (Aug. 1994).
7. J. F. Epperson, An Introduction to Numerical Methods and Analysis, Wiley, New York,
2002.
8. A. Quarteroni, R. Sacco, and F. Saleri, Numerical Mathematics (Texts in Applied Math-
ematics series, Vol. 37), Springer-Verlag, New York, 2000.
9. A. Ralston, A First Course in Numerical Analysis, McGraw-Hill, New York, 1965.
10. I. S. Gradshteyn and I. M. Ryzhik, in Table of Integrals, Series and Products, 5th ed.,
A. Jeffrey, ed., Academic Press, San Diego, CA, 1994.
11. R. E. Ziemer and W. H. Tranter, Principles of Communications: Systems, Modulation,
and Noise, Houghton Mifﬂin, Boston, MA, 1976.
PROBLEMS
9.1. This problem is based on an assignment problem due to I. Leonard. Consider
In =
 1
0
xn
x + a dx,
TLFeBOOK

PROBLEMS
407
where a ≥1 and n ∈Z+. It is easy to see that for 0 < x < 1, we have
xn+1 < xn, and 0 < In+1 < In for all n ∈Z+. For 0 < x < 1, we have
xn
1 + a <
xn
x + a < xn
a ,
implying that
 1
0
xn
1 + a dx < In <
 1
0
xn
a dx
or
1
(n + 1)(1 + a) < In <
1
(n + 1)a ,
so immediately limn→∞In = 0. Also, we have the difference equation
In =
 1
0
xn−1[x + a −a]
x + a
dx = 1
n −aIn−1
(9.P.1)
for n ∈N, where I0 =
" 1
0
1
x+a dx =

loge(x + a)
1
0 = loge

1+a
a

.
(a) Assume that ˆI0 = I0 + ϵ is the computed value of I0. Assume that no
other errors arise in computing In for n ≥1 using (9.P.1). Then
ˆIn = 1
n −a ˆIn−1.
Deﬁne the error en = ˆIn −In, and ﬁnd a difference equation for en.
(b) Solve for en, and show that for large enough a we have limn→∞
|en| = ∞.
(c) Find a stable algorithm to compute In for n ∈Z+.
9.2. (a) Find an upper bound on the magnitude of the rounding error involved in
applying the trapezoidal rule to
 a
0
x2 dx.
(b) Find an upper bound on the magnitude of the truncation error in applying
the trapezoidal rule to the integral in (a) above.
9.3. Consider the integral
I (ϵ) =
 a
ϵ
√x dx,
TLFeBOOK

408
NUMERICAL INTEGRATION AND DIFFERENTIATION
where a > ϵ > 0. Write a MATLAB routine to ﬁll in the following table for
a = 1, and n = 100:
ϵ
|I(ϵ) −T (n)|
BT (n)
|I(ϵ) −S(n)|
BS(n)
0.1000
0.0100
0.0010
0.0001
In this table T (n) is from (9.14), S(n) is from (9.41), and
|ET (n)| ≤BT (n),
|ES(n)| ≤BS(n),
where the upper bounds BT (n) and BS(n) are obtained using (9.33) and (9.59),
respectively.
9.4. Consider the integral
I =
 1
0
dx
1 + x2 = π
4 .
(a) Use the trapezoidal rule to estimate I, assuming that h = 1
4.
(b) Use Simpson’s rule to estimate I, assuming that h = 1
4.
(c) Use the corrected trapezoidal rule to estimate I, assuming that h = 1
4.
Perform all computations using only a pocket calculator.
9.5. Consider the integral
I =
 1/2
−1/2
dx
1 −x2 = ln 3.
(a) Use the trapezoidal rule to estimate I, assuming that h = 1
4.
(b) Use Simpson’s rule to estimate I, assuming that h = 1
4.
(c) Use the corrected trapezoidal rule to estimate I, assuming that h = 1
4.
Perform all computations using only a pocket calculator.
9.6. Consider the integral
I =
 π/2
0
sin(3x)
sin x
dx = π
2 .
(a) Use the trapezoidal rule to estimate I, assuming that h = π/12.
(b) Use Simpson’s Rule to estimate I, assuming that h = π/12.
(c) Use the corrected trapezoidal rule to estimate I, assuming that h = π/12.
Perform all computations using only a pocket calculator.
TLFeBOOK

PROBLEMS
409
9.7. The length of the curve y = f (x) for a ≤x ≤b is given by
L =
 b
a
7
1 + [f (1)(x)]2 dx.
Suppose f (x) = cos x. Compute L for a = −π/2 and b = π/2. Use the
trapezoidal rule, selecting n so that |ET (n)| ≤0.001. Hence, using (9.33),
select n such that
1
12
1
n2 (b −a)3M ≤0.001.
Do the computations using a suitable MATLAB routine.
9.8. Recall Example 6.5. Estimate the integral
I =
 1
−1
e−x2 dx
by
(a) Integrating the natural spline interpolant
(b) Integrating the complete spline interpolant
9.9. Find the constants α and β in x = αt + β, and ﬁnd f in terms of g such
that
 b
a
g(t) dt =
 1
−1
f (x)
√
1 −x2 dx.
[Comment: This transformation will permit you to apply the Chebyshev–
Gauss quadrature rule from Eq. (9.98) to general integrands.]
9.10. Consider the midpoint rule (Section 9.2). If we approximate I =
" b
a f (x) dx
by one rectangle, then the rule is
R(1) = (b −a)f
 a + b
2
!
,
so the truncation error involved in this approximation is
ER(1) =
 b
a
f (x) dx −R(1).
Use Taylor expansion error analysis to ﬁnd an approximation to ER(1). [Hint:
With x = (a + b)/2 and h = b −a, we obtain
f (x) = f (x) + (x −x)f (1)(x) + 1
2!(x −x)2f (2)(x) + · · · .
Consider f (a), f (b), and
" b
a f (x) dx using this series expansion.]
TLFeBOOK

410
NUMERICAL INTEGRATION AND DIFFERENTIATION
9.11. Use both the trapezoidal rule [Eq. (9.14)] and the Chebyshev–Gauss quadra-
ture rule [Eq. (9.98)] to approximate I = 2
π
" π
0
sin x
x
dx for n = 6. Assuming
that I = 1.1790, which rule gives an answer closer to this value for I ? Use
a pocket calculator to do the computations.
9.12. Consider the integral
I =
 π
0
cos2 x dx = π
2 .
Write a MATLAB routine to approximate I using the trapezoidal rule, and
Richardson’s extrapolation. The program must allow you to ﬁll in the fol-
lowing table:
n
T (n)
|I −T (n)|
R(n)
|I −R(n)|
2
4
8
16
32
64
128
256
512
1024
Of course, the extrapolated values are obtained from (9.111), where I (n) =
T (n). Does extrapolation improve on the accuracy? Comment on this.
9.13. Write a MATLAB routine that allows you to make a table similar to that in
Example 9.6, but for the integral
I =
 1
0
x1/4 dx.
9.14. The complete elliptic integral of the ﬁrst kind is
K(k) =
 π/2
0
dθ

1 −k2 sin2 θ
, 0 < k < 1,
and the complete elliptic integral of the second kind is
E(k) =
 π/2
0

1 −k2 sin2 θ dθ, 0 < k < 1.
(a) Find a series expansion for K(k). [Hint: Recall (3.80).]
TLFeBOOK

PROBLEMS
411
(b) Construct a Romberg table for N = 4 for the integral K( 1
2). Use the
series expansion from (a) to ﬁnd the “exact” value of K( 1
2) and compare.
(c) Find a series expansion for E(k). [Hint: Recall (3.80).]
(d) Construct a Romberg table for N = 4 for the integral E( 1
2). Use the series
expansion from (c) to ﬁnd the “exact” value of E( 1
2) and compare.
Use MATLAB to do all of the calculations. [Comment: Elliptic integrals are
important in electromagnetic potential problems (e.g., ﬁnding the magnetic
vector potential of a circular current loop), and are important in analog and
digital ﬁlter design (e.g., elliptic ﬁlters).]
9.15. This problem is about an alternative approach to the derivation of Gauss-type
quadrature rules. The problem statement is long, but the solution is short
because so much information has been provided. Suppose that w(x) ≥0 for
x ∈[a, b], so w(x) is some weighting function. We wish to ﬁnd weights wk
and sample points xk for k = 0, 1, . . . , n −1 such that
 b
a
w(x)f (x) dx =
n−1

k=0
wkf (xk)
(9.P.2)
for all f (x) = xj, where j = 0, 1, . . . , 2n −2, 2n −1. This task is greatly
aided by deﬁning the moments
mj =
 b
a
w(x)xj dx,
where mj is called the jth moment of w(x). In everything that follows it is
important to realize that because of (9.P.2)
mj =
 b
a
xjw(x) dx =
n−1

k=0
wkxj
k .
The method proposed here implicitly assumes that it is easy to ﬁnd the
moments mj. Once the weights and sample points are found, expression
(9.P.2) forms a quadrature rule according to
 b
a
w(x)f (x) dx ≈
n−1

k=0
wkf (xk) = G(n −1),
where now f (x) is essentially arbitrary. Deﬁne the vectors
w = [w0w1 · · · wn−2wn−1]T ∈Rn,
m = [m0m1 · · · m2n−2m2n−1]T ∈R2n.
TLFeBOOK

412
NUMERICAL INTEGRATION AND DIFFERENTIATION
Find matrix A ∈Rn×2n such that AT w = m. Matrix A turns out to be a
rectangular Vandermonde matrix. If we knew the sample points xk, then it
would be possible to use AT w = m to solve for the weights in w. (This is
so even though the linear system is overdetermined.) Deﬁne the polynomial
pn(x) =
n−1
/
j=0
(x −xj) =
n

j=0
pn,jxj
for which we see that pn,n = 1. We observe that the zeros of pn(x) happen
to be the sample points xk that we are looking for. The following suggestion
makes it possible to ﬁnd the sample points xk by ﬁrst ﬁnding the polynomial
pn(x). Using (in principle) Chapter 7 ideas, we can then ﬁnd the roots of
pn(x) = 0, and so ﬁnd the sample points. Consider the expression
mj+rpn,j =
n−1

k=0
wkxj+r
k
pn,j,
(9.P.3)
where r = 0, 1, . . . , n −2, n −1. Consider the sum of (9.P.3) over j =
0, 1, . . . , n. Use this sum to show that
n−1

j=0
mj+rpn,j = −mn+r
(9.P.4)
for r = 0, 1, . . . , n −1. This can be expressed in matrix form as Mp = q,
where
M = [mi+j]i,j=0,1,...,n−1 ∈Rn×n
which is called a Hankel matrix, and where
p = [pn,0pn,1 · · · pn,n−1]T ∈Rn
and
−q = [mnmn+1 · · · m2n−2m2n−1]T .
Formal proof that M−1 always exists is possible, but is omitted from con-
sideration. [Comment: The reader is warned that the approach to Gaussian
quadrature suggested here is not really practical due to the ill-conditioning
of the matrices involved (unless n is small).]
9.16. In the previous problem let a = −1, b = 1, and w(x) =
√
1 −x2. Develop a
numerically reliable algorithm to compute the moments mj =
" b
a w(x)xj dx.
TLFeBOOK

PROBLEMS
413
9.17. Derive Eq. (9.88).
9.18. Repeat Example 9.4, except that the integral is now
I =
 1
−1
e−x2 dx.
9.19. This problem is a preview of certain aspects of probability theory. It is an
example of an application for numerical integration. An experiment produces
a measurable quantity denoted x ∈R. The experiment is random in that the
value of x is different from one experiment to the next, but the probability
of x lying within a particular range of values is known to be
P = P [a ≤x ≤b] =
 b
a
fX(x) dx,
where
fX(x) =
1
√
2πσ 2 exp
 
−x2
2σ 2
!
,
which is an instance of the Gaussian function mentioned in Chapter 3. This
fact may be interpreted as follows. Suppose that we perform the experiment R
times, where R is a “large” number. Then on average we expect a ≤x ≤b
a total of PR times. The function fX(x) is called a Gaussian probability
density function (pdf) with a mean of zero, and variance σ 2. Recall Fig.
3.6, which shows the effect of changing σ 2. In Monte Carlo simulations of
digital communications systems, or for that matter any other system where
randomness is an important factor, it is necessary to write programs that gen-
erate simulated random variables such as x. The MATLAB randn function
will generate zero mean Gaussian random variables with variance σ 2 = 1.
For example, x = randn(1, N) will load N Gaussian random variables into
the row vector x. Write a MATLAB routine to generate N = 1000 simu-
lated Gaussian random variables (also called Gaussian variates) using randn.
Count the number of times x satisﬁes −1 ≤x ≤1. Let this count be denoted
ˆC. Your routine must also use the trapezoidal rule to estimate the probability
P = P [−1 ≤x ≤1] using erf(x) [deﬁned in Eq. (3.107)]. The magnitude of
the error in computing P must be <0.0001. This will involve using the trun-
cation error bound (9.33) in Chapter 9 to estimate the number of trapezoids
n that you need to do this job. You are to neglect rounding error effects here.
Compute C = P N. Your program must print ˆC and C to a ﬁle. Of course,
we expect ˆC ≈C.
9.20. Develop a MATLAB routine to ﬁll in the following table, which uses the
central difference approximation to the ﬁrst derivative of a function [i.e.,
TLFeBOOK

414
NUMERICAL INTEGRATION AND DIFFERENTIATION
˜f (1)
c
(x)] to estimate f (1)(x), where here
f (x) = loge x.
x
h = 10−4 h = 10−5 h = 10−6 h = 10−7 h = 10−8
f (1)(x)
1.0
2.0
3.0
4.0
5.0
6.0
7.0
8.0
9.0
10.0
Explain the results you get.
9.21. Suppose that f (x) = e−x2, and recall (9.132).
(a) Sketch f (2)(x).
(b) Let h = 1/10, and compute
˜f (1)
f (1). Find an upper bound on |ef (1)|.
Compute |f (1)(1) −˜f (1)
f (1)| = |ef (1)|, and compare to the bound.
(c) Let h = 1
10, and compute ˜f (1)
f (1/
√
2). Find an upper bound on |ef (1/
√
2)|.
Compute |f (1)(1/
√
2) −˜f (1)
f (1/
√
2)| = |ef (1/
√
2)|, and compare to the
bound.
9.22. Show that another approximation to f (1)(x) is given by
f (1)(x) ≈
1
12h[8f (x + h) −8f (x −h) −f (x + 2h) + f (x −2h)].
Give an expression for the error involved in using this approximation.
TLFeBOOK

10
Numerical Solution of Ordinary
Differential Equations
10.1
INTRODUCTION
In this chapter we consider numerical methods for the solution of ordinary differ-
ential equations (ODEs). We recall that in such differential equations the function
that we wish to solve for is in one independent variable. By contrast partial differ-
ential equations (PDEs) involve solving for functions in two or more independent
variables. The numerical solution of PDEs is a subject for a later chapter.
With respect to the level of importance of the subject the reader knows that all
dynamic systems with physical variables that change continuously over time (or
space, or both) are described in terms of differential equations, and so form the
basis for a substantial portion of engineering systems analysis, and design across
all branches of engineering. The reader is also well aware of the fact that it is quite
easy to arrive at differential equations that completely defy attempts at an analytical
solution. This remains so in spite of the existence of quite advanced methods for
analytical solution (e.g., symmetry methods that use esoteric ideas from Lie group
theory [1]), and so the need for this chapter is not hard to justify.
Where ODEs are concerned, differential equations arise within two broad cate-
gories of problems:
1. Initial-value problems (IVPs)
2. Boundary-value problems (BVPs)
In this chapter we shall restrict consideration to initial value problems. However,
this is quite sufﬁcient to accommodate much of electric/electronic circuit modeling,
modeling the orbital dynamics of satellites around planetary bodies, and many other
problems besides.
A simple example of an ODE for which no general analytical theory of solution
is known is the Dufﬁng equation
md2x(t)
dt2
+ k dx(t)
dt
+ αx(t) + δx3(t) = F cos(ωt),
(10.1)
An Introduction to Numerical Analysis for Electrical and Computer Engineers, by C.J. Zarowski
ISBN 0-471-46737-5
c⃝2004 John Wiley & Sons, Inc.
415
TLFeBOOK

416
NUMERICAL SOLUTION OF ORDINARY DIFFERENTIAL EQUATIONS
where t ≥0. Since we are concerned with initial-value problems we would need to
know, at least implicitly, x(0), and dx(0)
dt
which are the initial conditions. If it were
the case that δ = 0 then the solution of (10.1) is straightforward because it is a
particular case of a second-order linear ODE with constant coefﬁcients. Perhaps the
best method for solving (10.1) in this case would be the Laplace transform method.
However, the case where δ ̸= 0 immediately precludes a straightforward analytical
solution of this kind. It is worth noting that the Dufﬁng equation models a forced
nonlinear mechanical spring, where the restoring force of the spring is accounted
for by the terms αx(t) + δx3(t). Function x(t), which we wish to solve for, is the
displacement at time t of some point on the spring (e.g., the point mass m at the
free end) with respect to a suitable reference frame. Term k dx(t)
dt
is the opposing
friction, while F cos(ωt) is the periodic forcing function that drives the system. An
example of a recent application for (10.1) is in the modeling of micromechanical
ﬁlters/resonators [2].1
At the outset we consider only ﬁrst-order problems, speciﬁcally, how to solve
(numerically)
dx
dt = f (x, t),
x0 = x(0)
(10.2)
for t ≥0. [From now on we shall often write x instead of x(t), and dx/dt instead of
dx(t)/dt for brevity.] However, the example of (10.1) is a second-order problem.
But it is possible to replace it with a system of equivalent ﬁrst-order problems.
There are many ways to do this in principle. One way is to deﬁne
y = dx
dt .
(10.3)
The functions x(t) and y(t) are examples of state variables. Since we are interpret-
ing (10.1) as the model for a mechanical system wherein x(t) is displacement, it
therefore follows that we may interpret y(t) as velocity. From the deﬁnition (10.3)
we may use (10.1) to write
dy
dt = −α
mx −k
my −δ
mx3 + F
m cos(ωt)
(10.4a)
and
dx
dt = y.
(10.4b)
1The clock circuit in many present-day digital systems is built around a quartz crystal. Such crystals
do not integrate onto chips. Micromechanical resonators are intended to replace the crystal since such
resonators can be integrated onto chips. This is in furtherance of the goal of more compact electronic
systems. This applications example is a good illustration of the rapidly growing trend to integrate
nonelectrical/nonelectronic systems onto chips. The implication of this is that it is now very necessary
for the average electrical and/or computer engineer to become very knowledgeable about most other
branches of engineering, and to possess a much broader and deeper knowledge of science (physics,
chemistry, biology, etc.) and mathematics.
TLFeBOOK

INTRODUCTION
417
Equations (10.4) have the forms
dx
dt = f (x, y, t)
(10.5a)
dy
dt = g(x, y, t)
(10.5b)
for the appropriate choices of f and g. The initial conditions for our example
are x(0) and y(0) (initial position and initial velocity, respectively). These rep-
resent a coupled system of ﬁrst-order ODEs. Methods applicable to the solution
of (10.2) are extendable to the larger problem of solving systems of ﬁrst-order
ODEs, and so in this way higher-order ODEs may be solved. Thus, we shall also
consider the numerical solution of initial-value problems in systems of ﬁrst-order
ODEs.
The next two examples illustrate how to arrive at coupled systems of ﬁrst-order
ODEs for electrical and electronic circuits.
Example 10.1
Consider the linear electric circuit shown in Fig. 10.1. The input
to the circuit is the voltage source vs(t), while we may regard the output as the
voltage drop across capacitor C, denoted vC(t). The differential equation relating
the input voltage vs(t) and the output voltage vC(t) is thus
L1C d3vC(t)
dt3
+ R1C d2vC(t)
dt2
+
 L1
L2
+ 1
! dvC(t)
dt
+ R1
L2
vC(t) = dvs(t)
dt
. (10.6)
This third-order ODE may be obtained by mesh analysis of the circuit. The reader
ought to attempt this derivation as an exercise. One way to replace (10.6) with
a coupled system of ﬁrst-order ODEs is to deﬁne the state variables xk(t) (k ∈
{0, 1, 2}) according to
x0(t) = vC(t),
x1(t) = dvC(t)
dt
,
x2(t) = d2vC(t)
dt2
.
(10.7)
Substituting (10.7) into (10.6) yields
L1C dx2(t)
dt
+ R1Cx2(t) +
 L1
L2
+ 1
!
x1(t) + R1
L2
x0(t) = dvs(t)
dt
.
iL1(t)
iL2(t)
R1
L1
L2
C
vC(t)
vs(t)
+
−
+
−
Figure 10.1
The linear electric circuit for Example 10.1.
TLFeBOOK

418
NUMERICAL SOLUTION OF ORDINARY DIFFERENTIAL EQUATIONS
If we recognize that
x1(t) = dx0(t)
dt
,
x2(t) = dx1(t)
dt
then the complete system of ﬁrst order ODEs is
dx0(t)
dt
= x1(t),
dx1(t)
dt
= x2(t),
(10.8)
dx2(t)
dt
=
1
L1C
dvs(t)
dt
−R1
L1
x2(t) −
1
L1C
 L1
L2
+ 1
!
x1(t) −
R
L1L2C x0(t).
In many ways this is not the best description for the circuit dynamics.
Instead, we may ﬁnd the matrix A ∈R3×3 and column vector b ∈R3 such that


dvC(t)
dt
diL1(t)
dt
diL2(t)
dt


= A


vC(t)
iL1(t)
iL2(t)

+ bvs(t).
(10.9)
This deﬁnes a new set of state equations in terms of the new state variables vC(t),
iL1(t), and iL2(t). The matrix A and vector b contain constants that depend only
on the circuit parameters R1, L1, L2, and C.
Equation (10.9) is often a better representation than (10.8) because
1. There is no derivative of the forcing function vs(t) in (10.9) as there is in
(10.8).
2. There is a general (linear) theory of solution to (10.9) that is in practice easy
to apply, and it is based on state-space methods.
3. Inductor currents [i.e., iL1(t), iL2(t)] and capacitor voltages [i.e., vC(t)] can
be readily measured in a laboratory setting while derivatives of these are
not as easily measured. Thus, it is relatively easy to compare theoretical and
numerical solutions to (10.9) with laboratory experimental results.
Since
iC(t) = C dvC(t)
dt
,
vL1(t) = L1
diL1(t)
dt
,
vL2(t) = L2
diL2(t)
dt
on applying Kirchoff’s Voltage law (KVL) and Kirchoff’s Current law (KCL), we
arrive at the relevant state equations as follows.
TLFeBOOK

INTRODUCTION
419
First
vs(t) = R1iL1(t) + L1
diL1(t)
dt
+ L2
diL2(t)
dt
.
(10.10)
We see that vC(t) = vL2(t), and so
vC(t) = L2
iL2(t)
dt
,
giving
diL2(t)
dt
= 1
L2
vC(t),
(10.11)
which is one of the required state equations. Since
iL1(t) = iC(t) + iL2(t),
and so
iL1(t) = C dvC(t)
dt
+ iL2(t),
we also have
dvC(t)
dt
= 1
C iL1(t) −1
C iL2(t).
(10.12)
This is another of the required state equations. Substituting (10.11) into (10.10)
gives the ﬁnal state equation
diL1(t)
dt
= −R1
L1
iL1(t) −1
L1
vC(t) + 1
L1
vs(t).
(10.13)
The state equations may be collected together in matrix form as required:


dvC(t)
dt
diL1(t)
dt
diL2(t)
dt


=


0
1
C
−1
C
−1
L1
−R1
L1
0
1
L2
0
0


#
$%
&
=A


vC(t)
iL1(t)
iL2(t)

+


0
1
L1
0


#
$%
&
=b
vs(t).
Example 10.2
Now let us consider a more complicated third-order nonlinear
electronic circuit called the Colpitts oscillator [9]. The electronic circuit, and its
electric circuit equivalent (model) appears in Fig. 10.2. This circuit is a popular
analog signal generator with a long history (it used to be built using vacuum
tubes). The device Q is a three-terminal device called an NPN-type bipolar junction
transistor (BJT). The detailed theory of operation of BJTs is beyond the scope of
TLFeBOOK

420
NUMERICAL SOLUTION OF ORDINARY DIFFERENTIAL EQUATIONS
C2
C1
C2
C1
L
Q
R
(a)
(b)
vCC(t)
vCC(t)
RL
L
RL
iL(t)
iL(t)
vCE(t)
vBE(t)
iC(t)
iB(t)
iC(t)
iB(t)
vBE(t)
REE
REE
VEE
VEE
+
+
−
+
−
vCE(t)
+
−
−
Figure 10.2
The BJT Colpitts oscillator (a) and its electric circuit equivalent (b).
this book, but may be found in basic electronics texts [10]. For present purposes
it is enough to know that Q may be represented with a nonlinear resistor R (the
resistor enclosed in the box in Fig. 10.2b), and a current-controlled current source
(CCCS), where
iC(t) = βF iB(t)
(10.14)
and
iB(t) =



0,
vBE(t) ≤VTH
(vBE(t) −VTH )
RON
,
vBE(t) > VTH
.
(10.15)
The current iB(t) is called the base current of Q, and ﬂows into the base terminal
of the transistor as shown. From (10.15) we observe that if the base–emitter voltage
vBE(t) is below a threshold voltage VTH , then there is no base current into Q (i.e.,
the device is cut off). The relationship between vBE(t) −VTH , and iB(t) obeys
Ohm’s law only when vBE(t) is above threshold, in which case Q is active. In
either case (10.14) says the collector current iC(t) is directly proportional to iB(t),
and the constant of proportionality βF is called the forward current gain of Q.
Voltage vCE(t) is the collector–emitter voltage of Q. Typically, VTH ≈0.75 V, βF
is about 100 (order of magnitude), and RON (on resistance of Q) is seldom more
than hundreds of ohms in size.
TLFeBOOK

FIRST-ORDER ODEs
421
From (10.15) iB(t) is a nonlinear function of vBE(t) that we may compactly write
as iB(t) = fR(vBE(t)). There are power supply voltages vCC(t), and VEE. Voltage
VEE < 0 is a constant, with a typical value VEE = −5 V. Here we treat vCC (t)
as time-varying, but it is usually the case that (approximately) vCC (t) = VCC u(t),
where
u(t) =
 1,
t ≥0
0,
t < 0 .
(10.16)
Function u(t) is the unit step function. To say that vCC (t) = VCC u(t) is to say that
the circuit is turned on at time t = 0. Typically, VCC = +5 V.
The reader may verify (again as a circuit analysis review exercise) that state
equations for the Colpitts oscillator are:
C1
dvCE(t)
dt
= iL(t) −βF fR(vBE(t)),
(10.17a)
C2
dvBE(t)
dt
= −vBE(t) + VEE
REE
−fR(vBE(t)) −iL(t),
(10.17b)
LdiL(t)
dt
= vCC (t) −vCE(t) + vBE(t) −RLiL(t).
(10.17c)
Thus, the state variables are vBE(t), vCE(t), and iL(t). As previously, this circuit
description is not unique, but it is convenient.
Since numerical methods only provide approximate solutions to ODEs, we are
naturally concerned about the accuracy of these approximations. There are also
issues about the stability of proposed methods, and so this matter as well will be
considered in this chapter.
10.2
FIRST-ORDER ODEs
Strictly speaking, before applying a numerical method to the solution of an ODE,
we must be certain that a solution exists. We are also interested in whether the
solution is unique. It is worth stating that in many cases, since ODEs are often
derived from problems in the physical world, existence and uniqueness are often
“obvious” for physical reasons. Notwithstanding this, a mathematical statement
about existence and uniqueness is worthwhile.
The following deﬁnition is needed by the succeeding theorem regarding the
existence and uniqueness of solutions to ﬁrst order ODE initial value problems.
Deﬁnition 10.1: The Lipschitz Condition
The function f (x, t) ∈R satisﬁes
a Lipschitz condition in x for S ⊂R2 iff there is an α > 0 such that
|f (x, t) −f (y, t)| ≤α|x −y|
when (x, t), (y, t) ∈S. The constant α is called a Lipschitz constant for f (x, t).
TLFeBOOK

422
NUMERICAL SOLUTION OF ORDINARY DIFFERENTIAL EQUATIONS
It is apparent that if f (x, t) satisﬁes a Lipschitz condition, then it is smooth in
some sense. The following theorem is about the existence and uniqueness of the
solution to
dx
dt = f (x, t)
(10.18)
for 0 ≤t ≤tf , with the initial condition x0 = x(0). Time t = 0 is the initial time, or
starting time. We call constant tf the ﬁnal time. Essentially, we are only interested
in the solution over a ﬁnite time interval. This constraint on the theory is not
unreasonable since a computer can run for only a ﬁnite amount of time anyway.
We also remark that interpreting the independent variable t as time is common
practice, but not mandatory in general.
Theorem 10.1: Picard’s Theorem
Suppose that S = {(x, t) ∈R2|0 ≤t ≤
tf , −∞< x < ∞}, and that f (x, t) is continuous on S. If f satisﬁes a Lipschitz
condition on set S in the variable x, then the initial-value problem (10.18) has a
unique solution x = x(t) for all 0 ≤t ≤tf .
Proof
Omitted. We simply mention that it is based on the Banach ﬁxed-point
theorem (recall Theorem 7.3).
We also mention that a proof of a somewhat different version of this theorem
appears in Kreyszig [3, pp. 315–317]. It involves working with a contractive map-
ping on a certain closed subspace of C(J), where J = [t0 −β, t0 + β] ⊂R and
C(J) is the metric space of continuous functions on J, where the metric is that
of (1.8) in Chapter 1. It was remarked in Chapter 3 [see Eq. (3.8)] that this space
is complete. Thus, any closed subspace of it is complete as well (a fact that was
mentioned in Chapter 7 following Corollary 7.1).
We may now consider speciﬁc numerical techniques. Deﬁne xn = x(tn) for
n ∈Z+. Usually we assume that t0 = 0, and that
tn+1 = tn + h,
(10.19)
where h > 0, and we call h the step size. From (10.18)
x(1)
n
= dxn
dt = dx(t)
dt
|t=tn = f (xn, tn).
(10.20)
We may expand solution x(t) in a Taylor series about t = tn. Therefore, since
x(tn+1) = x(tn + h) = xn+1, and with x(k)
n
= x(k)(tn)
xn+1 = xn + hx(1)
n
+ 1
2!h2x(2)
n
+ 1
3!h3x(3)
n
+ · · · .
(10.21)
If we drop terms in x(k)
n
for k > 1, then (10.21) and (10.20) imply
xn+1 = xn + hx(1)
n
= xn + hf (xn, tn).
TLFeBOOK

FIRST-ORDER ODEs
423
Since x0 = x(t0) = x(0) we may ﬁnd (xn) via
xn+1 = xn + hf (xn, tn).
(10.22)
This is often called the Euler method (or Euler’s method).2 A more accurate descrip-
tion would be to call it the explicit form of Euler’s method in order to distinguish
it from the implicit form to be considered a little later on. The distinction matters
in practice because implicit methods tend to be stable, whereas explicit methods
are often prone to instability.
A few general words about stability and accuracy are now appropriate. In what
follows we will assume (unless otherwise noted) that the solution to a differen-
tial equation remains bounded; that is, |x(t)| < M < ∞for all t ≥0. However,
approximations to this solution [e.g., (xn) from (10.22)] will not necessarily remain
bounded in the limit as n →∞; that is, our numerical methods might not always be
stable. Of course, in a situation like this the numerical solution will deviate greatly
from the correct solution, and this is simply unacceptable. It therefore follows that
we must ﬁnd methods to test the stability of a proposed numerical solution. Some
informal deﬁnitions relating to stability are
Stable method: The numerical solution does not grow without bound (i.e., “blow
up”) with any choice of parameters such as step size.
Unstable method: The numerical solution blows up with any choices of param-
eters (such as step size).
Conditionally stable method: For certain choices of parameters the numerical
solution remains bounded.
We mention that even if the Euler method is stable, its accuracy is low because
only the ﬁrst two terms in the Taylor series are retained. More speciﬁcally, we
say that it is a ﬁrst-order method because only the ﬁrst power of h is retained in
the Taylor approximation that gave rise to it. The omission of higher-order terms
causes truncation errors. Since h2 (and higher power) terms are omitted we also
say that the truncation error per step (sometimes called the order of accuracy) is
of order h2. This is often written as O(h2). (Here we follow the terminology in
Kreyszig [4, pp. 793–794].) In summary, we prefer methods that are both stable,
and accurate. It is important to emphasize that accuracy and stability are distinct
concepts, and so must never be confused.
2Strictly speaking, in truncating the series in (10.21) we should write ˆxn+1 = xn + hf (xn, tn) so that
Euler’s method is
ˆxn+1 = ˆxn + hf (ˆxn, tn)
with ˆx0 = x0. This is to emphasize that the method only generates approximations to xn = x(tn).
However, this kind of notation is seldom applied. It is assumed that the reader knows that the numerical
method only approximates x(tn) even though the notation does not necessarily explicitly distinguish
the exact value from the approximate.
TLFeBOOK

424
NUMERICAL SOLUTION OF ORDINARY DIFFERENTIAL EQUATIONS
We can say more about the accuracy of the Euler method:
Theorem 10.2: For dx(t)/dt = f (x(t), t) let f (x(t), t) be Lipschitz continu-
ous with constant α (Deﬁnition 10.1), and assume that x(t) ∈C2[t0, tf ] (tf > t0).
If ˆxn ≈x(tn) = xn, where (tn = t0 + nh, and tn ≤tf )
ˆxn+1 = ˆxn + hf (ˆxn, tn), (ˆx0 ≈x(t0))
then
|x(tn) −ˆxn| ≤eα(tn−t0)|x(t0) −ˆx0| + 1
2hM eα(tn−t0) −1
α
,
(10.23)
where M = maxt∈[t0,tf ] |x(2)(t)|.
Proof
Euler’s method is
ˆxn+1 = ˆxn + hf (ˆxn, tn)
and from Taylor’s theorem
x(tn+1) = x(tn) + hx(1)(tn) + 1
2h2x(2)(ξn)
for some ξn ∈[tn, tn+1]. Thus
x(tn+1) −ˆxn+1 = x(tn) −ˆxn + h[x(1)(tn) −f (ˆxn, tn)] + 1
2h2x(2)(ξn)
= x(tn) −ˆxn + h[f (x(tn), tn) −f (ˆxn, tn)] + 1
2h2x(2)(ξn)
so that
|x(tn+1) −ˆxn+1| ≤|x(tn) −ˆxn| + αh|x(tn) −ˆxn| + 1
2h2|x(2)(ξn)|.
For convenience we will let en = |x(tn) −ˆxn|, λ = 1 + αh, and rn = 1
2h2x(2)(ξn),
so that
en+1 ≤λen + rn.
It is easy to see that3
e1
≤
λe0 + r0
e2
≤
λe1 + r1 = λ2e0 + λr0 + r1
e3
≤
λe2 + r2 = λ3e0 + λ2r0 + λr1 + r2
...
en
≤
λne0 +
n−1

j=0
λjrn−1−j
.
3More formally, we may use mathematical induction.
TLFeBOOK

FIRST-ORDER ODEs
425
If M = maxt∈[t0,tf ] |x(2)(t)| then rn−1−j ≤1
2h2M, and hence
en ≤λne0 + 1
2h2M
n−1

j=0
λj,
and since n−1
j=0 λj = λn−1
λ−1 , and for x ≥−1 we have (1 + x)n ≤enx, thus
n−1

j=0
λj = λn −1
λ −1 ≤enαh −1
αh
= eα(tn−t0) −1
αh
.
Consequently,
en ≤eα(tn−t0)e0 + 1
2hM eα(tn−t0) −1
α
which immediately yields the theorem statement.
We remark that e0 = |x(t0) −ˆx0| = 0 only if ˆx0 = x(t0) exactly. Where quan-
tization errors (recall Chapter 2) are concerned, this will seldom be the case. The
second term in the bound of (10.23) may be large even if h is tiny. In other words,
Euler’s method is not necessarily very accurate. Certainly, from (10.23) we can
say that en ∝h.
As a brief digression, we also note that Theorem 10.2 needed the bound
(1 + x)n ≤enx(x ≥−1).
(10.24)
We may easily establish (10.24) as follows. From the Maclaurin expansion
(Chapter 3)
ex = 1 + x + 1
2x2eξ
(for some ξ ∈[0, x]) so that
0 ≤1 + x ≤1 + x + 1
2x2eξ = ex,
and because 1 + x ≥0 (i.e., x ≥−1)
0 ≤(1 + x)n ≤enx,
thus establishing (10.24).
TLFeBOOK

426
NUMERICAL SOLUTION OF ORDINARY DIFFERENTIAL EQUATIONS
The stability of any method may be analyzed in the following manner. First
recall the Taylor series expansion of f (x, t) about the point (x0, t0)
f (x, t) = f (x0, t0) + (t −t0)∂f (x0, t0)
∂t
+ (x −x0)∂f (x0, t0)
∂x
+ 1
2!

(t −t0)2 ∂2f (x0, t0)
∂t2
+ 2(t −t0)(x −x0)∂2f (x0, t0)
∂t∂x
+(x −x0)2 ∂2f (x0, t0)
∂x2

+ · · ·
(10.25)
If we retain only the linear terms of (10.25) and substitute these into (10.18), then
we obtain
x(1)(t) = dx
dt = f (x0, t0) + (t −t0)∂f (x0, t0)
∂t
+ (x −x0)∂f (x0, t0)
∂x
= ∂f (x0, t0)
∂x
#
$%
&
=λ
x+ ∂f (x0, t0)
∂t
#
$%
&
=λ1
t+

f (x0, t0) −t0
∂f (x0, t0)
∂t
−x0
∂f (x0, t0)
∂x

#
$%
&
=λ2
,
(10.26)
so this has the general form (with λ, λ1, and λ2 as constants)
dx
dt = λx + λ1t + λ2.
(10.27)
This linearized approximation to the original problem in (10.18) allows us to inves-
tigate the behavior of the solution in close proximity to (x0, t0). Equation (10.27)
is often simpliﬁed still further by considering what is called the model problem
dx
dt = λx.
(10.28)
Thus, here we assume that λ1t + λ2 in (10.27) can also be neglected. However,
we do remark that (10.27) has the form
dx
dt + P (t)x = Q(t)xn,
(10.29)
where n = 0, P (t) = −λ, and Q(t) = λ1t + λ2. Thus, (10.27) is an instance of
Bernoulli’s differential equation [5, p. 62] for which a general method of solution
exists. But for the purpose of stability analysis it turns out to be enough (usually,
but not always) to consider only (10.28). Equation (10.28) is certainly simple in
that its solution is
x(t) = x(0)eλt.
(10.30)
TLFeBOOK

FIRST-ORDER ODEs
427
If Euler’s method is applied to (10.28), then
xn+1 = xn + hλxn = (1 + hλ)xn.
(10.31)
Clearly, for n ∈Z+
xn = (1 + hλ)nx0,
(10.32)
and we may avoid limn→∞|xn| = ∞if
|1 + hλ| ≤1.
The model problem (10.28) with the solution (10.30) is stable4 only if λ < 0.
Hence Euler’s method is conditionally stable for
λ < 0
and
h ≤2
|λ|,
(10.33)
and is unstable if
|1 + hλ| > 1.
(10.34)
We see that depending on λ and h, the explicit Euler method might be unstable.
Now we consider the alternative implicit form of Euler’s method. This method
is also called the backward Euler method. Instead of (10.22) we use
xn+1 = xn + hf (xn+1, tn+1).
(10.35)
It can be seen that a drawback of this method is the necessity to solve (10.35) for
xn+1. This is generally a nonlinear problem requiring the techniques of Chapter 7.
However, a strength of the implicit method is enhanced stability. This may be
easily seen as follows. Apply (10.35) to the model problem (10.28), yielding
xn+1 = xn + λhxn+1
or
xn+1 =
1
1 −λhxn.
(10.36)
Clearly
xn =

1
1 −λh
n
x0.
(10.37)
Since we must assume as before that λ < 0, the backward Euler method is stable
for all h > 0. In this sense we may say that the backward Euler method is uncon-
ditionally stable. Thus, the implicit Euler method (10.35) is certainly more stable
4For stability we usually insist that λ < 0 as opposed to allowing λ = 0. This is to accommodate a
concept called bounded-input, bounded-output (BIBO) stability. However, we do not consider the details
of this matter here.
TLFeBOOK

428
NUMERICAL SOLUTION OF ORDINARY DIFFERENTIAL EQUATIONS
than the previous explicit form (10.22). However, the implicit and explicit forms
have the same accuracy as both are ﬁrst-order methods.
Example 10.3
We wish to apply the implicit and explicit forms of the Euler
method to
dx
dt + x = 0
for x(0) = 1. Of course, since this is a simple linear problem, we immediately
know that
x(t) = x(0)e−t = e−t
for all t ≥0. Since we have f (x, t) = −x, we obtain
∂f (x, t)
∂x
= −1,
implying that λ = −1. Thus, the explicit Euler method (10.22) gives
xn+1 = (1 −h)xn
(10.38a)
for which 0 < h ≤2 via (10.33). Similarly, (10.35) gives for the implicit method
xn+1 = xn −hxn+1
or
xn+1 =
1
1 + hxn
(10.38b)
for which h > 0. In both (10.38a) and (10.38b) we have x0 = 1.
Some typical simulation results for (10.38) appear in Fig. 10.3. Note the insta-
bility of the explicit method for the case where h > 2.
It is to be noted that a small step size h is desirable to achieve good accuracy.
Yet a larger h is desirable to minimize the amount of computation involved in
simulating the differential equation over the desired time interval.
Example 10.4
Now consider the ODE
dx
dt + 2tx = te−t2x3
[5, pp. 62–63]. The exact solution to this differential equation is
x2(t) =
3
e−t2 + ce2t2
(10.39)
TLFeBOOK

FIRST-ORDER ODEs
429
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
0
0.2
0.4
0.6
0.8
1
Time (t)
Time (t)
Amplitude
0
2
4
6
8
10
12
14
16
18
20
−2
−1
0
1
2
3
Amplitude
(a)
(b)
x(t)
Implicit Euler (h = 0.1)
Explicit Euler (h = 0.1)
x(t)
Implicit Euler (h = 2.1)
Explicit Euler (h = 2.1)
Figure 10.3
Illustration of the implicit and explicit forms of the Euler method for the
differential equation in Example 10.3. In plot (a), h is small enough that the explicit method
is stable. Here the implicit and explicit methods display similar accuracies. In plot (b), h
is too big for the explicit method to be stable. Instability is indicated by the oscillatory
behavior of the method and the growing amplitude of the oscillations with time. However,
the implicit method remains stable, but because h is quite large, the accuracy is not very
good.
for t ≥0, where c = 3
x2
0 −1, and we assume that c > 0. Thus
f (x, t) = te−t2x3 −2tx
so
∂f (x, t)
∂x
= 3te−t2x2 −2t.
Consequently
λ = ∂f (x0, t0)
∂x
= ∂f (x0, 0)
∂x
= 0.
Via (10.33) we conclude that h > 0 is possible for both forms of the Euler method.
Since stability is therefore not a problem here, we choose to simulate the differential
TLFeBOOK

430
NUMERICAL SOLUTION OF ORDINARY DIFFERENTIAL EQUATIONS
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
0
0.2
0.4
0.6
0.8
1
Time (t)
(a)
(b)
Amplitude
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
0
0.2
0.4
0.6
0.8
1
Time (t)
Amplitude
x(t)
Explicit Euler (h = 0.02)
x(t)
Explicit Euler (h = 0.20)
Figure 10.4
Illustration of the explicit Euler method for the differential equation in
Example 10.4. Clearly, although stability is not a problem here, the accuracy of the method
is better for smaller h.
equation using the explicit Euler method as this is much simpler to implement.
Thus, from (10.22), we obtain
xn+1 = xn + h[tne−t2nx3
n −2tnxn].
(10.40)
We shall assume x0 = 1 [initial condition x(0)]. Of course, tn = hn for n =
0, 1, 2, . . ..
Figure 10.4 illustrates the exact solution from (10.39), and the simulated solution
via (10.40) for h = 0.02 and h = 0.20. As expected, the result for h = 0.02 is more
accurate.
The next example involves an ODE whose solution does not remain bounded
over time. Nevertheless, our methods are applicable since we terminate the simu-
lation after a ﬁnite time.
Example 10.5
Consider the ODE
dx
dt = t2 −2x
t
TLFeBOOK

FIRST-ORDER ODEs
431
for t ≥t0 > 0 [5, pp. 60–61]. The exact solution is given by
x(t) = t3
5 + c
5t2 .
(10.41)
The initial condition is x(t0) = x0 with t0 > 0, and so
x0 = t3
0
5 + c
5t2
0
,
implying that
c = 5t2
0x0 −t5
0.
Since f (x, t) = t2 −2x
t , we have
λ = ∂f (x0, t0)
∂x
= −2
t0
,
so via (10.33) for the explicit Euler method
0 < h ≤t0.
However, this result is misleading here because x(t) is not bounded with time. In
other words, it does not really apply here. From (10.22)
xn+1 = xn + h

t2
n −2xn
tn

,
(10.42a)
where
tn = t0 + nh
for n ∈Z+. If we consider the implicit method, then via (10.35)
xn+1 = xn + h

t2
n+1 −2xn+1
tn+1

,
or
xn+1 =
xn + ht2
n+1
1 + 2h
tn+1
,
(10.42b)
where tn+1 = t0 + (n + 1)h for n ∈Z+.
Figure 10.5 illustrates the exact solution x(t) from (10.41) along with the simu-
lated solutions from (10.42). This is for x0 = 1 and t0 = 0.05 with h = 0.025 (a),
and h = 5 (b). It can be seen that the implicit method is more accurate for t close
to t0. Of course, this could be very signiﬁcant since startup transients are often of
TLFeBOOK

432
NUMERICAL SOLUTION OF ORDINARY DIFFERENTIAL EQUATIONS
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
0
0.5
1
1.5
2
Time (t)
Amplitude
Implicit Euler (h = 0.025)
0
50
100
150
200
250
−5
0
5
10
15
20 × 105
Time (t)
Amplitude
x(t)
Explicit Euler (h = 0.025)
Implicit Euler (h = 5)
x(t)
Explicit Euler (h = 5)
(a)
(b)
Figure 10.5
Illustration of the explicit and implicit forms of the Euler method for the ODE
in Example 10.5. In plot (a), note that the implicit form tracks the true solution x(t) better
near t0 = 0.05. In plot (b), note that both forms display a similar accuracy even though h
is huge, provided t ≫t0.
interest in simulations of dynamic systems. It is noteworthy that both implicit and
explicit forms simulate the true solution with similar accuracy for t much bigger
than t0 even when h is large.
This example is of a “stiff system” (see Section 10.6).
Example 10.5 illustrates that stability and accuracy issues with respect to the
numerical solution of ODE initial-value problems can be more subtle than our
previous analysis would suggest. The reader is therefore duly cautioned about
these matters.
Recalling (3.71) from Chapter 3 (or recalling Theorem 10.2), the Taylor formula
for x(t) about t = tn is [recall xn = x(tn) for all n]
xn+1 = xn + hf (xn, tn)
#
$%
&
=ˆxn+1
+1
2h2x(2)(ξ)
(10.43)
for some ξ ∈[tn, tn+1]. Thus, the truncation error per step in the Euler method is
deﬁned to be
en+1 = xn+1 −ˆxn+1 = 1
2h2x(2)(ξ).
(10.44)
TLFeBOOK

FIRST-ORDER ODEs
433
We may therefore state, as suggested earlier, that the truncation error per step is of
order O(h2) because of this. The usefulness of (10.44) is somewhat limited in that
it depends on the solution x(t) [or rather on the second derivative x(2)(t)], which
is, of course, something we seldom know in practice.
How may we obtain more accurate methods? More speciﬁcally, this means
ﬁnding methods for which the truncation error per step is of order O(hm) with
m > 2.
One way to obtain improved accuracy is to try to improve the Euler method.
More than one possibility for improvement exists. However, a popular approach is
Heun’s method. It is based on the following observation. A drawback of the Euler
method in (10.22) is that f (xn, tn) is the derivative x(1)(t) at the beginning of the
interval [tn, tn+1], and yet x(1)(t) varies over [tn, tn+1]. The implicit form of the
Euler method works with f (xn+1, tn+1), namely, the derivative at t = tn+1, and so
has a similar defect. Therefore, intuitively, we may believe that we can improve
the algorithm by replacing f (xn, tn) with the average derivative
1
2

f (xn, tn) + f (xn + hf (xn, tn), tn + h)

.
(10.45)
This is approximately the average of x(1)(t) at the endpoints of interval [tn, tn+1].
The approximation is due to the fact that
f (xn+1, tn+1) ≈f (xn + hf (xn, tn), tn + h).
(10.46)
We see in (10.46) that we have employed (10.22) to approximate xn+1 according
to xn+1 = xn + hf (xn, tn) (explicit Euler method). Of course, tn+1 = tn + h does
not involve any approximation. Thus, Heun’s method is deﬁned by
xn+1 = xn + h
2

f (xn, tn) + f (xn + hf (xn, tn), tn + h)

.
(10.47)
This is intended to replace (10.22) and (10.35).
However, (10.47) is an explicit method, and so we may wonder about its sta-
bility. If we apply the model problem to (10.47), we obtain
xn+1 =

1 + λh + 1
2h2λ2
xn
(10.48)
for which
xn =

1 + λh + 1
2h2λ2n
x0.
(10.49)
For stability we must select h such that we avoid limn→∞|xn| = ∞. For conve-
nience, deﬁne
σ = 1 + λh + 1
2h2λ2,
(10.50)
so this requirement implies that we must have |σ| ≤1. A plot of (10.50) in terms
of hλ appears in Fig. 10.6. This makes it easy to see that we must have
−2 ≤hλ ≤0.
(10.51)
TLFeBOOK

434
NUMERICAL SOLUTION OF ORDINARY DIFFERENTIAL EQUATIONS
−2.5
−2
−1.5
−1
−0.5
0
0.5
−1
−0.5
0
0.5
1
1.5
2
hl
s
Figure 10.6
A plot of σ in terms of hλ [see Eq. (10.50)].
Since λ < 0 is assumed [again because we assume that x(t) is bounded] (10.51),
implies that Heun’s method is conditionally stable for the same conditions as in
(10.33). Thus, the stability characteristics of the method are identical to those of
the explicit Euler method, which is perhaps not such a surprize.
What about the accuracy of Heun’s method? Here we may see that there is an
improvement. Again via (3.71) from Chapter 3
xn+1 = xn + hx(1)(tn) + 1
2h2x(2)(tn) + 1
6h3x(3)(ξ)
(10.52)
for some ξ ∈[tn, tn+1]. We may approximate x(2)(tn) using a forward difference
operation
x(2)(tn) ≈x(1)(tn+1) −x(1)(tn)
h
(10.53)
so that (10.52) becomes [using x(1)(tn+1) = f (xn+1, tn+1), and x(1)(tn) =
f (xn, tn)]
xn+1 = xn + hf (xn, tn) + 1
2h2

x(1)(tn+1) −x(1)(tn)
h
	
+ 1
6h3x(3)(ξ),
(10.54)
or upon simplifying this, we have
xn+1 = xn + h
2

f (xn, tn) + f (xn+1, tn+1)

+ 1
6h3x(3)(ξ).
(10.55)
Replacing f (xn+1, tn+1) in (10.55) with the approximation (10.46), and dropping
the error term, we see that what remains is identical to (10.47), namely, Heun’s
method. Various approximations were made to arrive at this conclusion, but they
are certainly reasonable, and so we claim that the truncation error per step for
Heun’s method is
en+1 = 1
6h3x(3)(ξ),
(10.56)
TLFeBOOK

FIRST-ORDER ODEs
435
where again ξ ∈[tn, tn+1], and so this error is of the order O(h3). In other words,
although Heun’s method is based on modifying the explicit Euler method, the
modiﬁcation has lead to a method with improved accuracy.
Example 10.6
Here we repeat Example 10.5 by applying Heun’s method under
the same conditions as for Fig. 10.5a. Thus, the differential equation is again
dx
dt = t2 −2x
t
and again we choose x0 = 1.0, t0 = 0.05, with h = 0.025. The simulation result
appears in Fig. 10.7.
It is very clear that Heun’s method is distinctly more accurate than the Euler
method, especially near t = t0.
Heun’s method may be viewed in a different light by considering the following.
We may formally integrate (10.18) to arrive at x(t) according to
x(t) −x(tn) =
 t
tn
f (x, τ) dτ.
(10.57)
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
Time (t)
Amplitude
x(t)
Implicit Euler (h = 0.025)
Heun (h = 0.025)
Figure 10.7
Comparison
of
the
implicit
Euler
method
with
Heun’s
method
for
Example 10.6. We see that Heun’s method is more accurate, especially for t near t0 = 0.05.
TLFeBOOK

436
NUMERICAL SOLUTION OF ORDINARY DIFFERENTIAL EQUATIONS
So if t = tn+1, then, from (10.57), we obtain
x(tn+1) = x(tn) +
 tn+1
tn
f (x, τ) dτ
or
xn+1 = xn +
 tn+1
tn
f (x, τ) dτ.
(10.58)
According to the trapezoidal rule for numerical integration (see Chapter 9,
Section 9.2), we have
 tn+1
tn
f (x, τ) dτ ≈h
2 [f (xn, tn) + f (xn+1, tn+1)]
(since h = tn+1 −tn), and hence (10.58) becomes
xn+1 = xn + h
2 [f (xn, tn) + f (xn+1, tn+1)],
(10.59)
which is just the ﬁrst step in the derivation of Heun’s method. We may certainly
call (10.59) the trapezoidal method. Clearly, it is an implicit method since we must
solve (10.59) for xn+1. Equally clearly, Eq. (10.59) appears in (10.55), and so we
immediately conclude that the trapezoidal method is a second-order method with
a truncation error per step of order O(h3). Thus, we may regard Heun’s method
as the explicit form of the trapezoidal method. Or, equivalently, the trapezoidal
method can be regarded as the implicit form of Heun’s method. We mention that
the trapezoidal method is unconditionally stable, but will not prove this here.
The following example illustrates some more subtle issues relating to the stability
of numerical solutions to ODE initial-value problems. It is an applications example
from population dynamics, but the issues it raises are more broadly applicable. The
example is taken from Beltrami [6].
Example 10.7
Suppose that x(t) is the total size of a population (people,
insects, bacteria, etc.). The members of the population exist in a habitat that can
realistically support not more than N individuals. This is the carrying capacity
for the system. The population may grow at some rate that diminishes to zero
as x(t) approaches N. But if the population size x(t) is much smaller than the
carrying capacity, the rate of growth might be considered proportional to the present
population size. Consequently, a model for population growth might be
dx(t)
dt
= rx(t)

1 −x(t)
N

.
(10.60)
This is called the logistic equation. By separation of variables this equation has
solution
x(t) =
N
1 + ce−rt
(10.61)
TLFeBOOK

FIRST-ORDER ODEs
437
for t ≥0. As usual, c depends on the initial condition (initial population size) x(0).
The exact solution in (10.61) is clearly “well behaved.” Therefore, any numerical
solution to (10.60) must also be well behaved.
Suppose that we attempt to simulate (10.60) numerically using the explicit Euler
method. In this case we obtain [via (10.22)]
xn+1 = (1 + hr)xn −hr
N x2
n.
(10.62)
Suppose that we transform variables according to xn = αyn in which case (10.62)
can be rewritten as
yn+1 = (1 + hr)yn

1 −
hrα
N(1 + hr)yn

.
(10.63)
If we select
α = N(1 + hr)
hr
,
then (10.63) becomes
yn+1 = λyn(1 −yn),
(10.64)
where λ = 1 + hr. We recognize this as the logistic map from Chapter 7 [see
Examples 7.3–7.5 and Eq. (7.83)]. From Section 7.6 in particular we recall that
this map can become chaotic for certain choices of λ. In other words, chaotic
instability is another possible failure mode for a numerical method that purports to
solve ODEs.
The explicit form of the Euler method is actually an example of a ﬁrst-order
Runge–Kutta method. Similarly, Heun’s method is an example of a second-order
Runge–Kutta method. It is second-order essentially because the approximation
involved retains the term in h2 in the Taylor series expansion. We mention that
methods of still higher order can be obtained simply by retaining more terms in the
Taylor series expansion of (10.21). This is seldom done because to do so requires
working with derivatives of increasing order, and this requires much computational
effort. But this effort can be completely avoided by developing Runge–Kutta meth-
ods of higher order. We now outline a general approach for doing this. It is based
on material from Rao [7].
All Runge–Kutta methods have a particular form that may be stated as
xn+1 = xn + hα(xn, tn, h),
(10.65)
where α(xn, tn, h) is called the increment function. The increment function is
selected to represent the average slope on the interval t ∈[tn, tn+1]. In particular,
the increment function has the form
α(xn, tn, h) =
m

j=1
cjkj,
(10.66)
TLFeBOOK

438
NUMERICAL SOLUTION OF ORDINARY DIFFERENTIAL EQUATIONS
where m is called the order of the Runge–Kutta method, cj are constants, and
coefﬁcients kj are obtained recursively according to
k1 = f (xn, tn)
k2 = f (xn + a2,1hk1, tn + p2h)
k3 = f (xn + a3,1hk1 + a3,2hk2, tn + p3h)
...
km = f

xn +
m−1

j=1
am,jhkj, tn + pmh

.
(10.67)
A more compact description of the Runge–Kutta methods is
xn+1 = xn + h
m

j=1
cjkj,
(10.68a)
where
kj = f

xn + h
j−1

l=1
aj,lkl, tn + pjh

.
(10.68b)
To specify a particular method requires selecting a variety of coefﬁcients (cj, aj,l,
etc.). How is this to be done?
We illustrate with examples. Suppose that m = 1. In this case
xn+1 = xn + hc1k1 = xn + hc1f (xn, tn)
(10.69)
which gives (10.22) when c1 = 1. Thus, we are justiﬁed in calling the explicit
Euler method a ﬁrst-order Runge–Kutta method.
Suppose that m = 2. In this case
xn+1 = xn + hc1f (xn, tn) + hc2f (xn + a2,1hf (xn, tn), tn + p2h).
(10.70)
We observe that if we choose
c2 = c1 = 1
2, a2,1 = 1, p2 = 1,
(10.71)
then (10.70) reduces to
xn+1 = xn + 1
2h[f (xn, tn) + f (xn + hf (xn, tn), tn + h)],
which is Heun’s method [compare this with (10.47)]. Thus, we are justiﬁed in call-
ing Heun’s method a second-order Runge–Kutta method. However, the coefﬁcient
TLFeBOOK

FIRST-ORDER ODEs
439
choices in (10.71) are not unique. Other choices will lead to other second-order
Runge–Kutta methods. We may arrive at a systematic approach for creating alter-
natives as follows.
For convenience as in (10.21), deﬁne x(k)
n
= x(k)(tn). Since m = 2, we will
consider the Taylor expansion
xn+1 = xn + hx(1)
n
+ 1
2h2x(2)
n
+ O(h3)
(10.72)
[recall (10.52)] for which the term O(h3) simply denotes the higher-order terms.
We recall that x(1)(t) = f (x, t), so x(1)
n
= f (xn, tn), and via the chain rule
x(2)(t) = ∂f
∂t + ∂f
∂x
dx
dt = ∂f
∂t + ∂f
∂x f (x, t),
(10.73)
so (10.72) may be rewritten as
xn+1 = xn + hf (xn, tn) + 1
2h2 ∂f (xn, tn)
∂t
+ 1
2h2 ∂f (xn, tn)
∂x
f (xn, tn) + O(h3).
(10.74)
Once again, the Runge–Kutta method for m = 2 is
xn+1 = xn + hc1f (xn, tn) + hc2f (xn + a2,1hk1, tn + p2h).
(10.75)
Recalling (10.26), the Taylor expansion of f (xn + a2,1hk1, tn + p2h) is given by
f (xn + a2,1hk1, tn + p2h) = f (xn, tn) + a2,1hf (xn, tn)∂f (xn, tn)
∂x
+ p2h∂f (xn, tn)
∂t
+ O(h2).
(10.76)
Now we substitute (10.76) into (10.75) to obtain
xn+1 = xn + (c1 + c2)hf (xn, tn) + p2c2h2 ∂f (xn, tn)
∂t
+ a2,1c2h2 ∂f (xn, tn)
∂x
f (xn, tn) + O(h3).
(10.77)
We may now compare like terms of (10.77) with those in (10.74) to conclude that
the coefﬁcients we seek satisfy the nonlinear system of equations
c1 + c2 = 1, p2c2 = 1
2, a2,1c2 = 1
2.
(10.78)
To generate second-order Runge–Kutta methods, we are at liberty to choose the
coefﬁcients c1, c2, p2, and a2,1 in any way we wish so long as the choice sat-
isﬁes (10.78). Clearly, Heun’s method is only one choice among many possible
choices. We observe from (10.78) that we have four unknowns, but possess three
TLFeBOOK

440
NUMERICAL SOLUTION OF ORDINARY DIFFERENTIAL EQUATIONS
equations. Thus, we may select one parameter “arbitrarily” that will then determine
the remaining ones. For example, we may select c2, so then, from (10.78)
c1 = 1 −c2,
p2 =
1
2c2
,
and
a2,1 =
1
2c2
.
(10.79)
Since one parameter is freely chosen, thus constraining all the rest, we say that
second-order Runge–Kutta methods possess one degree of freedom.
It should be clear that the previous procedure may be extended to systematically
generate Runge–Kutta methods of higher order (i.e., m > 2). However, this requires
a more complete description of the Taylor expansion for a function in two variables.
This is stated as follows.
Theorem 10.3: Taylor’s Theorem
Suppose that f (x, t), and all of its partial
derivatives of order n + 1 or less are deﬁned and continuous on D = {(x, t)|a ≤
t ≤b, c ≤x ≤d}. Let (x0, t0) ∈D; then, for all (x, t) ∈D, there is a point (η, ξ) ∈
D such that
f (x, t) =
n

r=0

1
r!
r

k=0
 r
k
!
(t −t0)r−k(x −x0)k ∂rf (x0, t0)
∂tr−k∂xk

+
1
(n + 1)!
n+1

k=0
 n + 1
k
!
(t −t0)n+1−k(x −x0)k ∂n+1f (η, ξ)
∂tn+1−k∂xk
for which (η, ξ) is on the line segment that joins the points (x0, t0), and (x, t).
Proof
Omitted.
The reader can now easily imagine that any attempt to apply this approach for
m > 2 will be quite tedious. Thus, we shall not do this here. We will restrict our-
selves to stating a few facts. Applying the method to m = 3 (i.e., the generation
of third-order Runge–Kutta methods) leads to algorithm coefﬁcients satisfying six
equations with eight unknowns. There will be 2 degrees of freedom as a conse-
quence.
Fourth-order Runge–Kutta methods (i.e., m = 4) also possess two degrees of
freedom, and also have a truncation error per step of O(h5). One such method
(attributed to Runge) in common use is
xn+1 = xn + h
6 [k1 + 2k2 + 2k3 + k4] ,
(10.80)
where
k1 = f (xn, tn)
k2 = f

xn + 1
2hk1, tn + 1
2h

k3 = f

xn + 1
2hk2, tn + 1
2h

k4 = f (xn + hk3, tn + h).
(10.81)
Of course, an inﬁnite number of other fourth-order methods are possible.
TLFeBOOK

FIRST-ORDER ODEs
441
We mention that Runge–Kutta methods are explicit methods, and so in principle
carry some risk of instability. However, it turns out that the higher the order of
the method, the lower the risk of stability problems. In particular, users of fourth-
order methods typically experience few stability problems in practice. In fact, it
can be shown that on applying (10.80) and (10.81) to the model problem (10.28),
we obtain
xn =

1 + hλ + 1
2h2λ2 + 1
6h3λ3 + 1
24h4λ4
n
x0.
(10.82)
A plot of
σ = 1 + hλ + 1
2h2λ2 + 1
6h3λ3 + 1
24h4λ4
(10.83)
in terms of hλ appears in Fig. 10.8. To avoid limn→∞|xn| = ∞, we must have
|σ| ≤1, and so it turns out that
−2.785 ≤hλ ≤0
(see Table 9.11 on p. 685 of Rao [7]). This is in agreement with Fig. 10.8. Thus
λ < 0,
h ≤2.785
|λ| .
(10.84)
This represents an improvement over (10.33).
Example 10.8
Once again we repeat Example 10.5 for which
dx
dt = t2 −2x
t
with x0 = 1.0 for t0 = .05, but here instead our step size is now h = 0.05. Addition-
ally, our comparison is between Heun’s method and the fourth-order Runge–Kutta
method deﬁned by Eqs. (10.80) and (10.81).
−3.5
−3
−2.5
−2
−1.5
−1
−0.5
0
0.5
0
0.5
1
1.5
2
hl
s
Figure 10.8
A plot of σ in terms of hλ [see Eq. (10.83)].
TLFeBOOK

442
NUMERICAL SOLUTION OF ORDINARY DIFFERENTIAL EQUATIONS
0
0.5
1
1.5
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Time (t)
Amplitude
x(t)
4th-order Runge−Kutta (h = 0.05)
Heun (h = 0.05)
Figure 10.9
Comparison of Heun’s method (a second-order Runge–Kutta method) with
a fourth-order Runge–Kutta method [Eqs. (10.80) and (10.81)]. This is for the differential
equation in Example 10.8.
The simulated solutions based on these methods appear in Fig. 10.9. As expected,
the fourth-order method is much more accurate than Heun’s method. The plot
in Fig. 10.9 was generated using MATLAB, and the code for this appears in
Appendix 10.A as an example. (Of course, previous plots were also produced by
MATLAB codes similar to that in Appendix 10.A.)
10.3
SYSTEMS OF FIRST-ORDER ODEs
The methods of Section 10.2 may be extended to handle systems of ﬁrst-order
ODEs where the number of ODEs in the system is arbitrary (but ﬁnite). However,
we will consider only systems of two ﬁrst-order ODEs here. Speciﬁcally, we wish
to solve (numerically)
dx
dt = f (x, y, t),
(10.85a)
dy
dt = g(x, y, t),
(10.85b)
where the initial condition is x0 = x(t0) and y0 = y(t0). This is sufﬁcient, for
example, to simulate the Dufﬁng equation mentioned in Section 10.1.
TLFeBOOK

SYSTEMS OF FIRST-ORDER ODEs
443
As in Section 10.2, we will begin with Euler methods. Therefore, following
(10.21) we may Taylor-expand x(t) and y(t) about the sampling time t = tn. As
before, for convenience we may deﬁne x(k)
n
= x(k)(tn), y(k)
n
= y(k)(tn). The relevant
expansions are given by
xn+1 = xn + hx(1)
n
+ 1
2h2x(2)
n
+ · · · ,
(10.86a)
yn+1 = yn + hy(1)
n
+ 1
2h2y(2)
n
+ · · · .
(10.86b)
The explicit Euler method follows by retaining the ﬁrst two terms in each expansion
in (10.86). Thus, the Euler method in this case is
xn+1 = xn + hf (xn, yn, tn),
(10.87a)
yn+1 = yn + hg(xn, yn, tn),
(10.87b)
where we have used the fact that x(1)
n
= f (xn, yn, tn) and y(1)
n
= g(xn, yn, tn). As
we might expect, the implicit form of the Euler method is
xn+1 = xn + hf (xn+1, yn+1, tn+1),
(10.88a)
yn+1 = yn + hg(xn+1, yn+1, tn+1).
(10.88b)
Of course, to employ (10.88) will generally involve solving a nonlinear system of
equations for xn+1 and yn+1, necessitating the use of Chapter 7 techniques. As
before, we refer to parameter h as the step size.
The accuracy of the explicit and implicit Euler methods for systems is the same
as for individual equations; speciﬁcally, it is O(h2). However, stability analysis is
more involved. Matrix methods simply cannot be avoided. This is demonstrated as
follows.
The model problem for a single ﬁrst-order ODE was Eq. (10.28). For a coupled
system of two ﬁrst-order ODEs as in (10.85), the model problem is now
dx
dt = a00x + a01y,
(10.89a)
dy
dt = a10x + a11y.
(10.89b)
Here ai,j are real-valued constants. We remark that this may be written in more
compact matrix form
dx
dt = Ax,
(10.90)
where x = x(t) = [x(t)y(t)]T and dx/dt = [dx(t)/dt
dy(t)/dt]T and, of course
A =
 a00
a01
a10
a11

.
(10.91)
TLFeBOOK

444
NUMERICAL SOLUTION OF ORDINARY DIFFERENTIAL EQUATIONS
Powerful claims are possible using matrix methods. For example, it can be argued
that if x(t) ∈RN (so that A ∈RN×N), then dx(t)/dt = Ax(t) has solution
x(t) = eAtx(0)
(10.92)
for t ≥0; that is, (10.92) for N = 2 is the general solution to (10.90) [and hence
to (10.89)].5
The constants in A are related to f (x, y, t) and g(x, y, t) in the following
manner. Recall (10.25). If we retain only the linear terms in the Taylor expansions
of f and g around the point (x0, y0, t0), then
f (x, y, t) ≈f (x0, y0, t0) + (x −x0)∂f (x0, y0, t0)
∂x
+ (y −y0)∂f (x0, y0, t0)
∂y
+ (t −t0)∂f (x0, y0, t0)
∂t
,
(10.93a)
g(x, y, t) ≈g(x0, y0, t0) + (x −x0)∂g(x0, y0, t0)
∂x
+ (y −y0)∂g(x0, y0, t0)
∂y
+ (t −t0)∂g(x0, y0, t0)
∂t
.
(10.93b)
As a consequence
A =


∂f (x0, y0, t0)
∂x
∂f (x0, y0, t0)
∂y
∂g(x0, y0, t0)
∂x
∂g(x0, y0, t0)
∂y

.
(10.94)
At this point we may apply the explicit Euler method (10.87) to the model problem
(10.89), which results in
 xn+1
yn+1

=
 1 + ha00
ha01
ha10
1 + ha11
  xn
yn

.
(10.95)
5Yes, as surprising as it seems, although At is a matrix exp(At) makes sense as an operation. In fact,
for example, with x(t) = [x0(t) · · · xn−1(t)]T , the system of ﬁrst-order ODEs
dx(t)
dt
= Ax(t) + by(t)
(A ∈Rn×n, b ∈Rn, and y(t) ∈R) has the general solution
x(t) = eAtx(0−) +
 t
0−eA(t−τ)by(τ) dτ.
The integral in this solution is an example of a convolution integral.
TLFeBOOK

SYSTEMS OF FIRST-ORDER ODEs
445
An alternative form for this is
xn+1 = (I + hA)xn.
(10.96)
Here I is a 2 × 2 identity matrix and xn = [xn
yn]T . With x0 = [x0
y0]T , we
may immediately claim that
xn = (I + hA)nx0,
(10.97)
where n ∈Z+. We observe that this includes (10.32) as a special case. Naturally,
we must select step size h to avoid instability; that is, we are forced to select h
to prevent limn→∞||xn|| = ∞. In principle, the choice of norm is arbitrary, but
2-norms are often chosen. We recall that there is a nonsingular matrix T (matrix
of eigenvectors) such that
T −1[I + hA]T = ,
(10.98)
where  is the matrix of eigenvalues. We will assume that
 =
 λ0
0
0
λ1

.
(10.99)
In other words, we assume I + hA is diagonalizable. This is not necessarily always
the case, but is an acceptable assumption for present purposes. Since from (10.98)
we have I + hA = T T −1, (10.96) becomes
xn+1 = T T −1xn,
or
T −1xn+1 = T −1xn.
(10.100)
Let yn = T −1xn, so therefore (10.100) becomes
yn+1 = yn.
(10.101)
In any norm limn→∞||yn|| ̸= ∞, provided |λk| ≤1 for all k = 0, 1, . . . , N −1
( ∈RN×N). Consequently, limn→∞||xn|| ̸= ∞too (because ||xn|| = ||T yn|| ≤
||T || ||yn|| and ||T || is ﬁnite). We conclude that h is an acceptable step size,
provided the eigenvalues of I + hA do not possess a magnitude greater than unity.
Note that in practice we normally insist that h result in |λk| < 1 for all k.
We may apply the previous stability analysis to the implicit Euler method.
Speciﬁcally, apply (10.88) to model problem (10.89), giving
xn+1 = xn + h[a00xn+1 + a01yn+1]
yn+1 = yn + h[a10xn+1 + a11yn+1],
TLFeBOOK

446
NUMERICAL SOLUTION OF ORDINARY DIFFERENTIAL EQUATIONS
which in matrix form becomes
 xn+1
yn+1

=
 xn
yn

+ h
 a00
a01
a10
a11
  xn+1
yn+1

,
or more compactly as
xn+1 = xn + hAxn+1.
(10.102)
Consequently, for n ∈Z+
xn = ([I −hA]−1)nx0.
(10.103)
For convenience we can deﬁne B = [I −hA]−1. Superﬁcially, (10.103) seems to
have the same form as (10.97). We might be lead therefore to believe (falsely)
that the implicit method can be unstable, too. However, we may assume that there
exists a nonsingular matrix V such that (if A ∈RN×N)
V −1AV = ,
(10.104)
where  = diag(γ0, γ1, . . . , γN−1), which is the diagonal matrix of the eigenvalues
of A. (Once again, it is not necessarily the case that A is always diagonalizable,
but the assumption is reasonable for our present purposes.) Immediately
[I −hA]−1 = [I −hV V −1]−1 = (V [V −1V −h]V −1)−1
so that
[I −hA]−1 = V [I −h]−1V −1.
(10.105)
Consequently xn+1 = [I −hA]−1xn becomes
xn+1 = V [I −h]−1V −1xn.
(10.106)
Deﬁne yn = V −1xn, and so (10.106) becomes
yn+1 = [I −h]−1yn.
(10.107)
Because I −h is a diagonal matrix, a typical main diagonal element of [I −
h]−1 is σk = 1/(1 −hγk). It is a fact (which we will not prove here) that the
model problem in the general case is stable provided the eigenvalues of A all pos-
sess negative-valued real parts.6 Thus, provided Re(γk) < 0 for all k, we are assured
that |σk| < 1 for all k, and hence limn→∞||yn|| = 0. Thus, limn→∞||xn|| = 0, too,
and so we conclude that the implicit form of the Euler method is unconditionally
stable. Thus, if the model problem is stable, we may select any h > 0.
6The eigenvalues of A may be complex-valued, and so it is the real parts of these that truly determine
system stability.
TLFeBOOK

SYSTEMS OF FIRST-ORDER ODEs
447
The following example is of a linear system for which a mathematically exact
solution can be found. [In fact, the solution is given by (10.92).]
Example 10.9
Consider the ODE system
dx
dt = −2x + 1
4y,
(10.108a)
dy
dt = −3x.
(10.108b)
The initial condition is x0 = x(0) = 1, y0 = y(0) = −1. From (10.94) we see that
A =

−2
1
4
−3
0
	
.
The eigenvalues of A are γ0 = −1
2, and γ1 = −3
2. These eigenvalues are both
negative, and so the solution to (10.108) happens to be stable. In fact, the exact
solution can be shown to be
x(t) = −3
4e−t/2 + 7
4e−3t/2,
(10.109a)
y(t) = −9
2e−t/2 + 7
2e−3t/2
(10.109b)
for t ≥0. Note that the eigenvalues of A appear in the exponents of the exponentials
in (10.109). This is not a coincidence. The explicit Euler method has the iterations
xn+1 = xn + h

−2xn + 1
4yn

,
(10.110a)
yn+1 = yn −3hxn.
(10.110b)
Simulation results are shown in Figs. 10.10 and 10.11 for h = 0.1 and h =
1.4, respectively. This involves comparing (10.110a,b) with the exact solution
(10.109a,b). Figure 10.10b shows a plot of the eigenvalues of I + hA for various
step sizes. We see from this that choosing h = 1.4 must result in an unstable simu-
lation. This is conﬁrmed by the result in Fig. 10.11. For comparison purposes, the
eigenvalues of I + hA and of [I −hA]−1 are plotted in Fig. 10.12. This shows
that, at least in this particular case, the implicit method is more stable than the
explicit method.
Example 10.10
Recall the Dufﬁng equation of Section 10.1. Also, recall the
fact that this ODE can be rewritten in the form of (10.85a,b), and this was done in
Eq. (10.4a,b).
Figures 10.13 and 10.14 show the result of simulating the Dufﬁng equation
using the explicit Euler method for the model parameters
F = 0.5,
ω = 1,
m = 1,
α = 1,
δ = 0.1,
k = 0.05.
TLFeBOOK

448
NUMERICAL SOLUTION OF ORDINARY DIFFERENTIAL EQUATIONS
0
1
2
3
4
5
6
7
8
9
10
−2.5
−2
−1.5
−1
−0.5
0
0.5
1
Time (t)
Amplitude
0
0.5
1
1.5
−1.5
−1
−0.5
0
0.5
1
Step size (h)
Amplitude
x(t)
y(t)
Euler method (h = 0.1)
Euler method (h = 0.1)
l0
l1
(a)
(b)
Figure 10.10
Simulation results for h = 0.1. Plot (b) shows the eigenvalues of I + hA.
Both plots were obtained by applying the explicit form of the Euler method to the ODE
system of Example 10.9. Clearly, the simulation is stable for h = 0.1.
Thus (10.1) is now
d2x
dt2 = 0.5 cos(t) −0.05dx
dt −[x + 0.1x3].
We use initial condition x(0) = y(0) = 0. The driving function (applied force)
0.5 cos(t) is being opposed by the restoring force of the spring (terms in square
brackets) and friction (ﬁrst derivative term). Therefore, on physical grounds, we do
not expect the solution x(t) to grow without bound as t →∞. Thus, the simulated
solution to this problem must be stable, too.
We mention that an analytical solution to the differential equation that we are
simulating is not presently known.
From (10.94) for our Dufﬁng system example we have
A =


0
1
−α
m −3δ
m x2
0
−k
m

.
Example 10.11
According to Hydon [1, p. 61], the second-order ODE
d2x
dt2 =
 dx
dt
!2 1
x +
 
x −1
x
! dx
dt
(10.111)
TLFeBOOK

SYSTEMS OF FIRST-ORDER ODEs
449
0
1
2
3
4
5
6
7
8
9
10
−8
−6
−4
−2
0
2
4
6
8
Time (t)
Amplitude
x(t)
y(t)
Euler method (h = 1.4)
Euler method (h = 1.4)
Figure 10.11
This is the result of applying the explicit form of the Euler method to the
ODE system of Example 10.9. Clearly, the simulation is not stable for h = 1.4. This is
predicted by the eigenvalue plot in Fig. 10.10b, which shows that one of the eigenvalues of
I + hA has a magnitude exceeding unity for this choice of h.
0
0.5
1
1.5
−1.5
−1
−0.5
0
0.5
1
Step size (h)
(a)
(b)
0
0.5
1
1.5
Step size (h)
Amplitude
Amplitude
l0
l1
l0
l1
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
l0
l1
Figure 10.12
Plots of the eigenvalues of I + hA (a), which determine the stability of the
explicit Euler method and the eigenvalues of [I −hA]−1 (b), which determine the stability
of the implicit Euler method. This applies for the ODE system of Example 10.9.
TLFeBOOK

450
NUMERICAL SOLUTION OF ORDINARY DIFFERENTIAL EQUATIONS
0
10
20
30
40
50
60
70
80
90
−3
−2
−1
0
1
2
3
4
Time (t)
Amplitude
Amplitude
xn (h = .020)
yn (h = .020)
0
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.9996
0.9998
1
1.0002
1.0004
1.0006
Step size (h)
(b)
(a)
|l0|
|l1|
Figure 10.13
(a) Explicit Euler method simulation of the Dufﬁng equation; (b) magnitude
of the eigenvalues of I + hA. Both plots were obtained by applying the explicit form of the
Euler method to the ODE system of Example 10.10, which is the Dufﬁng equation expressed
as a coupled system of ﬁrst-order ODEs. The simulation is apparently stable for h = 0.02.
This is in agreement with the prediction based on the eigenvalues of I + hA [plot (b)],
which have a magnitude of less than unity for this choice of h.
has the exact solution
x(t) =



c1 −
7
c2
1 −1 tanh(
7
c2
1 −1(t + c2)),
c2
1 > 1
c1 −(t + c2)−1,
c2
1 = 1
c1 +
7
1 −c2
1 tanh(
7
1 −c2
1(t + c2)),
c2
1 < 1
.
(10.112)
The ODE in (10.111) can be rewritten as the system of ﬁrst-order ODEs
dx
dt = y,
(10.113a)
dy
dt = y2
x +
 
x −1
x
!
y.
(10.113b)
TLFeBOOK

SYSTEMS OF FIRST-ORDER ODEs
451
0
10
20
30
40
50
60
−20
−10
0
10
20
Time (t)
Amplitude
0
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.9996
0.9998
1
1.0002
1.0004
1.0006
Step size (h)
Amplitude
xn (h = 0.055)
yn (h = 0.055)
(b)
(a)
|l0|
|l1|
Figure 10.14
(a) Explicit Euler method simulation of the Dufﬁng equation; (b) magnitude
of the eigenvalues of I + hA. Both plots were obtained by applying the explicit form of the
Euler method to the ODE system of Example 10.10, which is the Dufﬁng equation expressed
as a coupled system of ﬁrst-order ODEs. The simulation is not stable for h = 0.055. This is
in agreement with the prediction based on the eigenvalues of I + hA [plot (b)], which have
a magnitude exceeding unity for this choice of h.
The initial condition is x0 = x(0), and y(0) = dx(t)
dt |t=0. From (10.94) we have
A =


0
1
−y2
0
x2
0
+ y0
'
1 + 1
x2
0
(
2y0
x0
+
 
x0 −1
x0
!

.
(10.114)
Using (10.113a), we may obtain y(t) from (10.112). For example, let us consider
simulating the case c2
1 = 1. Thus, in this case
y(t) =
1
(t + c2)2 .
(10.115)
For the choice c2
1 = 1, we have
x0 = c1 −1
c2
,
(10.116a)
y0 = 1
c2
2
.
(10.116b)
TLFeBOOK

452
NUMERICAL SOLUTION OF ORDINARY DIFFERENTIAL EQUATIONS
0
0.5
1
1.5
2
2.5
3
3.5
−1
0
1
2
3
4
Time (t)
(a)
(b)
Amplitude
Amplitude
x(t)
y(t)
xn (h = 0.020)
yn (h = 0.020)
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0
0.5
1
1.5
2
Step size (h)
|l0|
|l1|
Figure 10.15
Magnitude of the eigenvalues of I + hA is shown in plot (b). Both plots
show the simulation results of applying the explicit Euler method to the ODE system in
Example 10.11. The simulation is (as expected) stable for h = 0.02. Clearly, the simulated
result agrees well with the exact solution.
If we select c1 = 1, then
x0 = 1 −1
c2
,
y0 = (1 −x0)2.
(10.117)
Let us assume x0 = −1, and so y0 = 4. The result of applying the explicit Euler
method to system (10.113) with these conditions is shown in Figs. 10.15 and 10.16.
The magnitudes of the eigenvalues of I + hA are displayed in plots (b) of both
ﬁgures. We see that for Fig. 10.15, h = 0.02 and a stable simulation is the result,
while for Fig. 10.16, we have h = 0.35, for which the simulation is unstable. This
certainly agrees with the stability predictions based on ﬁnding the eigenvalues of
matrix I + hA.
Examples 10.10 and 10.11 illustrate just how easy it is to arrive at differential
equations that are not so simple to simulate in a stable manner with low-order
explicit methods. It is possible to select an h that is “small” in some sense, yet
not small enough for stability. The cubic nonlinearity in the Dufﬁng model makes
the implementation of the implicit form of Euler’s method in this problem quite
TLFeBOOK

SYSTEMS OF FIRST-ORDER ODEs
453
0
1
2
3
4
5
6
2
1
0
1
2
3
4
5
Time (t)
Amplitude
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0
0.5
1
1.5
2
Step size (h)
Amplitude
x(t)
y(t)
xn (h = 0.35)
yn (h = 0.35)
|λ0|
|λ1|
(a)
(b)
Figure 10.16
Plot (b) shows magnitude of eigenvalues of I + hA. Both plots show the sim-
ulation results of applying the explicit Euler method to the ODE system in Example 10.11.
The simulation is (as expected) not stable for h = 0.35. Instability is conﬁrmed by the fact
that the simulated result deviates greatly from the exact solution when t is sufﬁciently large.
unattractive. So, a better approach to simulating the Dufﬁng equation is with a
higher-order explicit method.
For example, Heun’s method for (10.85) may be stated as
xn+1 = xn + 1
2h[f (xn, yn, tn) + f (xn + k1, yn + l1, tn + h)],
(10.118a)
yn+1 = yn + 1
2h[g(xn, yn, tn) + g(xn + k1, yn + l1, tn + h)],
(10.118b)
where
k1 = hf (xn, yn, tn), l1 = hg(xn, yn, tn).
(10.119)
Also, for example, Chapter 36 of Bronson [8] contains a summary of higher-order
methods that may be applied to (10.85).
Example 10.12
Recall the Dufﬁng equation simulation in Example 10.10.
Figure 10.17 illustrates the simulation of the Dufﬁng equation using both the
explicit Euler method and Heun’s method for a small h (i.e., h = 0.005 in both
cases).
TLFeBOOK

454
NUMERICAL SOLUTION OF ORDINARY DIFFERENTIAL EQUATIONS
0
10
20
30
40
50
60
70
80
−3
−2
−1
0
1
2
3
Time (t)
Amplitude
0
10
20
30
40
50
60
70
80
−3
−2
−1
0
1
2
3
Time (t)
Amplitude
xn (h = 0.005)
yn (h = 0.005)
xn (h = 0.005)
yn (h = 0.005)
(a)
(b)
Figure 10.17
Comparison of the explicit Euler (a) and Heun (b) method simulations of
the ODE in Example 10.12, which is the Dufﬁng equation. Here the step size h is small
enough that the two methods give similar results.
At this point we note that there are other ways to display the results of numerical
solutions to ODEs that can lead to further insights into the behavior of the dynamic
system that is modeled by those ODEs. Figure 10.18 illustrates the phase portrait
of the Dufﬁng system. This is obtained by plotting the points (xn, yn) on the
Cartesian plane, yielding an approximate plot of (x(t), y(t)). The resulting curve
is the trajectory, or orbit for the system. Periodicity of the system’s response is
indicated by curves that encircle a point of equilibrium, which in this case would
be the center of the Cartesian plane [i.e., point (0, 0)]. The trajectory is tending to
an approximately ellipse-shaped closed curve indicative of approximately simple
harmonic motion.
The results in Fig. 10.18 are based on the parameters given in Example 10.10.
However, Fig. 10.19 shows what happens when the system parameters become
F = 0.3,
ω = 1,
m = 1,
α = −1,
δ = 1,
k = 0.22.
(10.120)
The phase portrait displays a more complicated periodicity than what appears in
Fig. 10.18. The ﬁgure is similar to Fig. 2.2.5 in Guckenheimer and Holmes [11].
TLFeBOOK

MULTISTEP METHODS FOR ODEs
455
0
20
40
60
80
100
120
140
160
−3
−2
−1
0
1
2
3
Time (t)
Amplitude
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
x(t)
y(t)
xn (h = 0.005)
yn (h = 0.005)
(a)
(b)
Figure 10.18
(a) The result of applying Heun’s method to obtain the numerical solution
of the Dufﬁng system speciﬁed in Example 10.10; (b) the phase portrait for the system
obtained by plotting the points (xn, yn) [from plot (a)] on the Cartesian plane, thus yielding
an approximate plot of (x(t), y(t)).
As in the cases of the explicit and implicit Euler methods, we may obtain a
theory of stability for Heun’s method. As before, the approach is to apply the
model problem (10.90) to (10.118) and (10.119). As an exercise, the reader should
show that this yields
xn+1 =

I + hA + 1
2h2A2
xn,
(10.121)
where I is the 2 × 2 identity matrix, A is obtained by using (10.94), and, of course,
xn = [xnyn]T . [The similarity between (10.121) and (10.48) is no coincidence.]
Criteria for the selection of step size h leading to a stable simulation can be
obtained by analysis of (10.121). But the details are not considered here.
10.4
MULTISTEP METHODS FOR ODEs
The numerical ODE solvers we have considered so far were either implicit methods
or explicit methods. But in all cases they were examples of so-called single-step
TLFeBOOK

456
NUMERICAL SOLUTION OF ORDINARY DIFFERENTIAL EQUATIONS
0
20
40
60
80
100
120
140
160
−1.5
−1
−0.5
0
0.5
1
1.5
Time (t)
Amplitude
−1.5
−1
−0.5
0
0.5
1
1.5
−1
−0.5
0
0.5
1
x(t)
y(t)
xn (h = 0.005)
yn (h = 0.005)
(a)
(b)
Figure 10.19
(a) The result of applying Heun’s method to obtain the numerical solu-
tion of the Dufﬁng system speciﬁed in Example 10.12 [i.e., using the parameter values in
Eq. (10.120)]; (b) the phase portrait for the system obtained by plotting the points (xn, yn)
[from plot (a)] on the Cartesian plane, thus yielding an approximate plot of (x(t), y(t)).
methods; that is, xn+1 was ultimately only a function of xn. A disadvantage of
single-step methods is that to achieve good accuracy often requires the use of
higher-order methods (e.g., fourth- or ﬁfth-order Runge–Kutta). But higher-order
methods need many function evaluations per step, and so are computationally
expensive.
Implicit single-step methods, although inherently stable, are not more accurate
than explicit methods, although they can track fast changes in the solution x(t)
better than can explicit methods (recall Example 10.5). However, implicit methods
may require nonlinear system solvers (i.e., Chapter 7 methods) as part of their
implementation. This is a complication that is also not necessarily very efﬁcient
computationally. Furthermore, the methods in Chapter 7 possess their own stability
problems. Therefore, in this section we introduce multistep predictor–corrector
methods that overcome some of the deﬁciencies of the methods we have considered
so far.
In this section we return to consideration of a single ﬁrst-order ODE IVP
x(1)(t) = dx(t)
dt
= f (x(t), t), x0 = x(0).
(10.122)
TLFeBOOK

MULTISTEP METHODS FOR ODEs
457
In reality, we have already seen a single-step predictor–corrector method in
Section 10.2. Suppose that we have the following method:
˜xn+1 = xn + hf (xn, tn)
(predictor step)
(10.123a)
xn+1 = xn + 1
2h[f (˜xn+1, tn+1) + f (xn, tn)]
(corrector step).
(10.123b)
If we substitute (10.123a) into (10.123b), we again arrive at Heun’s method
[Eq. (10.47)], which overcame the necessity to solve for xn+1 in the implicit method
of Eq. (10.59) (trapezoidal method). Generally, predictor–corrector methods replace
implicit methods in this manner, and we will see more examples further on in
this section. When a higher-order implicit method is “converted” to a predictor–
corrector method, the need to solve nonlinear equations is eliminated and accuracy
is preserved, but the stability characteristics of the implicit method will be lost, at
least to some extent. Of course, a suitable stability theory will still allow the user
to select reasonable values for the step size parameter h.
We may now consider a few simple examples of multistep methods. Perhaps
the simplest multistep methods derive from the numerical differentiation ideas from
Section 9.6 (of Chapter 9). Recall (9.138), for which
x(1)(t) = 1
2h[x(t + h) −x(t −h)] −1
6h2x(3)(ξ)
(10.124)
for some ξ ∈[t −h, t + h]. The explicit Euler method (10.22) can be replaced
with the midpoint method derived from [using t = tn in (10.124)]
f (x(tn), tn) ≈1
2h[x(tn+1) −x(tn−1)],
so this method is
xn+1 = xn−1 + 2hf (xn, tn).
(10.125)
This is an explicit method, but xn+1 depends on xn−1 as well as xn. We may call
it a two-step method. Similarly, via (9.153)
f (x(tn), tn) ≈1
2h[−3x(tn) + 4x(tn+1) −x(tn+2)],
so we have the method
xn+2 = 4xn+1 −3xn −2hf (xn, tn)
which can be rewritten as
xn+1 = 4xn −3xn−1 −2hf (xn−1, tn−1).
(10.126)
TLFeBOOK

458
NUMERICAL SOLUTION OF ORDINARY DIFFERENTIAL EQUATIONS
Finally, via (9.154), we obtain
f (x(tn), tn) ≈1
2h[x(tn−2) −4x(tn−1) + 3x(tn)],
yielding the method
xn+1 = 4
3xn −1
3xn−1 + 2
3hf (xn+1, tn+1).
(10.127)
Method (10.126) is a two-step method that is explicit, but (10.127) is a two-step
implicit method since we need to solve for xn+1.
A problem with multistep methods is that the IVP (10.122) provides only one
initial condition x0. But for n = 1 in any of (10.125), (10.126), or (10.127), we
need to know x1; that is, for two-step methods we need two initial conditions, or
starting values. A simple way out of this dilemma is to use single-step methods to
provide any missing starting values. In fact, predictor–corrector methods derived
from single-step concepts (e.g., Runge–Kutta methods) are often used to provide
the starting values for multistep methods.
What about multistep method accuracy? Let us consider the midpoint method
again. From (10.124) for some ξn ∈[tn−1, tn+1]
x(tn+1) = x(tn−1) + 2hf (x(tn), tn) + 1
3h3x(3)(ξn),
(10.128)
so the method (10.125) has a truncation error per step that is O(h3). The midpoint
method is therefore more accurate than the explicit Euler method [recall (10.43)
and (10.44)]. Yet we see that both methods need only one function evaluation
per step. Thus, the midpoint method is more efﬁcient than the Euler method. We
recall that Heun’s method (a Runge–Kutta method) has a truncation error per
step that is O(h3), too [recall (10.56)], and so Heun’s method may be used to
initialize (i.e., provide starting values for) the midpoint method (10.125). This
speciﬁc situation holds up in general. Thus, a multistep method can often achieve
comparable accuracy to single-step methods, and yet use fewer function calls,
leading to reduced computational effort.
What about stability considerations? Let us continue with our midpoint method
example. If we apply the model problem (10.28) to (10.125), we have the difference
equation
xn+1 = xn−1 + 2hλxn,
(10.129)
which is a second-order difference equation. This has characteristic equation7
z2 −2hλz −1 = 0.
(10.130)
7We may rewrite (10.129) as
xn+2 −2hλxn+1 −xn = 0,
which has the z-transform
(z2 −2hλz −1)X(z) = 0.
TLFeBOOK

MULTISTEP METHODS FOR ODEs
459
For convenience, let ρ = hλ, in which case the roots of (10.130) are easily seen
to be
z1 = ρ +
7
ρ2 + 1,
z2 = ρ −
7
ρ2 + 1, .
(10.131)
A general solution to (10.129) will have the form
xn = c1zn
1 + c2zn
2
(10.132)
for n ∈Z+. Knowledge of x0 and x1 allows us to solve for the constants c1 and c2 in
(10.132), if this is desired. However, more importantly, we recall that we assume
λ < 0, and we seek step size h > 0 so that limn→∞|xn| ̸= ∞. But in this case
ρ = hλ < 0, and hence from (10.131), |z2| > 1 for all h > 0. If c2 ̸= 0 in (10.132)
(which is practically always the case), then we will have limn→∞|xn| = ∞! Thus,
the midpoint method is inherently unstable under all realistic conditions ! Term c1zn
1
in (10.132) is “harmless” since |z1| < 1 for suitable h. But the term c2zn
2, often
called a parasitic term, will eventually “blow up” with increasing n, thus fatally
corrupting the approximation to x(t).
Unfortunately, parasitic terms are inherent in multistep methods. However, there
are more advanced methods with stability theories designed to minimize the effects
of the parasitics. We now consider a few of these improved multistep methods.
10.4.1
Adams–Bashforth Methods
Here we look at the Adams–Bashforth (AB) family of multistep ODE IVP solvers.
Section 10.4.2 will look at the Adams–Moulton (AM) family. Our approach follows
Epperson [12, Section 6.6]. Both families are derived using Lagrange interpolation
[recall Section 6.2 from Chapter 6 (above)].
Recall (10.122) which we may integrate to obtain (tn = t0 + nh)
x(tn+1) = x(tn) +
 tn+1
tn
f (x(t), t) dt.
(10.133)
Now suppose that we had the samples x(tn−k) for k = 0, 1, . . . , m (i.e., m + 1 sam-
ples of the exact solution x(t)). Via Lagrange interpolation theory, we may interpo-
late F(t) = f (x(t), t) [the integrand of (10.133)] for t ∈[tn−m, tn+1]8 according to
pm(t) =
m

k=0
Lk(t)f (x(tn−k), tn−k),
(10.134)
A solution to (10.129) exists only if z2 −2hλz −1 = 0. If the reader has not had a signals and systems
course (or equivalent) then this reasoning must be accepted “on faith.” But it may help to observe that
the reasoning is similar to the theory of solution for linear ODEs in constant coefﬁcients.
8The upper limit on the interval [tn−m, tn] has been extended from tn to tn+1 here. This is allowed
under interpolation theory, and actually poses no great problem in either method development or error
analysis. We are using the Lagrange interpolant to extrapolate from t = tn to tn+1.
TLFeBOOK

460
NUMERICAL SOLUTION OF ORDINARY DIFFERENTIAL EQUATIONS
where
Lk(t) =
m
/
i=0
i̸=k
t −tn−i
tn−k −tn−i
.
(10.135)
From (6.14) for some ξt ∈[tn−m, tn+1]
F(t) = pm(t) +
1
(m + 1)!F (m+1)(ξt)
m
/
i=0
(t −tn−i).
(10.136)
However, F(t) = f (x(t), t) = x(1)(t) so (10.136) becomes
F(t) = pm(t) +
1
(m + 1)!x(m+2)(ξt)
m
/
i=0
(t −tn−i).
(10.137)
Thus, if we now substitute (10.137) into (10.133), we obtain
x(tn+1) = x(tn) +
m

k=0
f (x(tn−k), tn−k)
 tn+1
tn
Lk(t) dt + Rm(tn+1),
(10.138)
where
Rm(tn+1) =
 tn+1
tn
1
(m + 1)!x(m+2)(ξt)
m
/
i=0
(t −tn−i)
#
$%
&
=π(t)
dt.
(10.139)
Polynomial π(t) does not change sign for t ∈[tn, tn+1] (which is the interval of
integration). Thus, we can say that there is a ξn ∈[tn, tn+1] such that
Rm(tn+1) =
1
(m + 1)!x(m+2)(ξn)
 tn+1
tn
π(t) dt.
(10.140)
For convenience, deﬁne
ρm =
1
(m + 1)!
 tn+1
tn
π(t) dt =
1
(m + 1)!
 tn+1
tn
(t −tn)(t −tn−1) · · · (t −tn−m) dt
(10.141)
and
λk =
 tn+1
tn
Lk(t) dt.
(10.142)
Thus, (10.138) reduces to [with Rm(tn+1) = ρmx(m+2)(ξn)]
x(tn+1) = x(tn) +
m

k=0
λkf (x(tn−k), tn−k) + ρmx(m+2)(ξn)
(10.143)
TLFeBOOK

MULTISTEP METHODS FOR ODEs
461
TABLE 10.1
Adams–Bashforth Method Parameters
m
λ0
λ1
λ2
λ3
Rm(tn+1)
0
h
1
2h2x(2)(ξn)
1
3
2h
−1
2h
5
12h3x(3)(ξn)
2
23
12h
−16
12h
5
12h
3
8h4x(4)(ξn)
3
55
24h
−59
24h
37
24h
−9
24h
251
720h5x(5)(ξn)
for some ξn ∈[tn, tn+1]. The order m + 1 Adams–Bashforth method is therefore
deﬁned to be
xn+1 = xn +
m

k=0
λkf (xn−k, tn−k).
(10.144)
It is an explicit method involving m + 1 steps. Table 10.1 summarizes the method
parameters for various m, and is essentially Table 6.6 from Ref. 12.
10.4.2
Adams–Moulton Methods
The Adams–Moulton methods are a modiﬁcation of the Adams–Bashforth meth-
ods. The Adams–Bashforth methods interpolate using the nodes tn, tn−1, . . . , tn−m.
On the other hand, the Adams–Moulton methods interpolate using the nodes
tn+1, tn, . . . , tn−m+1. Note that the number of nodes is the same in both methods.
Consequently, (10.138) becomes
x(tn+1) = x(tn) +
m−1

k=−1
f (x(tn−k), tn−k)
 tn+1
tn
Lk(t) dt + Rm(tn+1),
(10.145)
where now
Lk(t) =
m−1
/
i=−1
i̸=k
t −tn−i
tn−k −tn−i
,
(10.146)
and Rm(tn+1) = ρmx(m+2)(ξn) with
ρm =
1
(m + 1)!
 tn+1
tn
(t −tn+1)(t −tn) · · · (t −tn−m+1) dt.
(10.147)
TLFeBOOK

462
NUMERICAL SOLUTION OF ORDINARY DIFFERENTIAL EQUATIONS
Thus, the order m + 1 Adams–Moulton method is deﬁned to be
xn+1 = xn +
m−1

k=−1
λkf (xn−k, tn−k),
(10.148)
where
λk =
 tn+1
tn
Lk(t) dt
(10.149)
[same as (10.142) except k = −1, 0, 1, . . ., m −1, and Lk(t) is now (10.146)].
Method (10.148) is an implicit method since it is necessary to solve for xn+1.
It also requires m + 1 steps. Table 10.2 summarizes the method parameters for
various m, and is essentially Table 6.7 from Ref. 12.
10.4.3
Comments on the Adams Families
For small values of m in Tables 10.1 and 10.2, we see that the Adams families
(AB family and AM family) correspond to methods seen earlier. To be speciﬁc:
1. For m = 0 in Table 10.1, λ0 = h, so (10.144) yields the explicit Euler method
(10.22).
2. For m = 0 in Table 10.2, λ−1 = h, so (10.148) yields the implicit Euler
method (10.35).
3. For m = 1 in Table 10.2, λ−1 = λ0 = 1
2h, so (10.148) yields the trapezoidal
method (10.59).
Stability analysis for members of the Adams families is performed in the usual
manner. For example, when m = 1 in Table 10.1 (i.e., consider the second-order
AB method), Eq. (10.144) becomes
xn+1 = xn + 1
2h[3f (xn, tn) −f (xn−1, tn−1)].
(10.150)
TABLE 10.2
Adams–Moulton Method Parameters
m
λ−1
λ0
λ1
λ2
Rm(tn+1)
0
h
−1
2h2x(2)(ξn)
1
1
2h
1
2h
−1
12h3x(3)(ξn)
2
5
12h
8
12h
−1
12h
−1
24h4x(4)(ξn)
3
9
24h
19
24h
−5
24h
1
24h
−19
720h5x(5)(ξn)
TLFeBOOK

MULTISTEP METHODS FOR ODEs
463
Application of the model problem (10.28) to (10.150) yields
xn+1 =
 
1 + 3
2hλ
!
xn −1
2hλxn−1
or
xn+2 −
 
1 + 3
2hλ
!
xn+1 + 1
2hλxn = 0.
This has characteristic equation (with ρ = hλ)
z2 −
 
1 + 3
2ρ
!
z + 1
2ρ = 0.
(10.151)
This equation has roots
z1 = 1
2
 
1 + 3
2ρ
!
+
8
1 + ρ + 9
4ρ2
	
,
z2 = 1
2
 
1 + 3
2ρ
!
−
8
1 + ρ + 9
4ρ2
	
.
(10.152)
We need to know what range of h > 0 yields |z1|, |z2| < 1. We consider only ρ < 0
since λ < 0. Figure 10.20 plots |z1| and |z2| versus ρ, and suggests that we may
select h such that
−1 < hλ < 0.
(10.153)
Plots of stability regions for the other Adams families members may be seen in
Figs. 6.7 and 6.8 of Epperson [12]. Note that stability regions occupy the complex
plane as it is assumed in such a context that λ ∈C. However, we have restricted
−2
−1.8
−1.6
−1.4
−1.2
−1
−0.8
−0.6
−0.4
−0.2
0
0
0.5
1
1.5
2
2.5
r
|zk|
|z1|
|z2|
Figure 10.20
Magnitudes of the roots in (10.152) as a function of ρ = hλ.
TLFeBOOK

464
NUMERICAL SOLUTION OF ORDINARY DIFFERENTIAL EQUATIONS
our attention to a ﬁrst-order ODE IVP here, and so it is actually enough to assume
that λ ∈R.
Finally, observe that AB and AM methods can be combined to yield predictor–
correctors. An mth-order AB method can act as a predictor for an mth-order
AM method that is the corrector. A Runge–Kutta method can initialize the
procedure.
10.5
VARIABLE-STEP-SIZE (ADAPTIVE) METHODS FOR ODEs
Accuracy in the numerical solution of ODEs requires either increasing the order
of the method applied to the problem or decreasing the step-size parameter h.
However, high-order methods (e.g., Runge–Kutta methods of order exceeding 5)
are not very attractive at least in part because of the computational effort involved.
To preserve accuracy while reducing computational requirements suggests that we
should adaptively vary step size h.
Recall Example 10.5, where we saw that low order methods were not accurate
near t = t0. For a method of a given order, we would like in Example 10.5 to
have a small h for t near t0, but a larger h for t away from t0. This would reduce
the overall number of function evaluations needed to estimate x(t) for t ∈[t0, tf ].
The idea of adaptively varying h from step to step requires monitoring the error
in the solution somehow; that is, ideally, we need to infer en = |x(tn) −xn| [xn
is the estimate of x(t) at t = tn from some method] at step n. If en is small
enough, h may be increased in size at the next step, but if en is too big, we
decrease h.
Of course, we do not know x(tn), so we do not have direct access to the error en.
However, one idea that is implemented in modern software tools (e.g., MATLAB
routines ode23 and ode45) is to compute xn for a given h using two methods,
each of a different order. The method of higher order is of greater accuracy, so
if xn does not differ much between the methods, we are lead to believe that h is
small enough, and so may be increased in the next step. On the other hand, if the
xn values given by the different methods signiﬁcantly vary, we are then lead to
believe that h is too big, and so should be reduced.
In this section we give only a basic outline of the main ideas of this process.
Our emphasis is on the Runge–Kutta–Fehlberg (RKF) methods, of which MAT-
LAB routines ode23 and ode45 are particular implementations. Routine ode23
implements second- and third-order Runge–Kutta methods, while ode45 imple-
ments fourth- and ﬁfth-order Runge–Kutta methods. Computational efﬁciency is
maintained by sharing intermediate results that are common to both second- and
third-order methods and common to both fourth- and ﬁfth-order methods. More
speciﬁcally, Runge–Kutta methods of consecutive orders have constants such as kj
[recall (10.67)] in common with each other and so need not be computed twice.
We mention that ode45 implements a method based on Dormand and Prince [14],
and a more detailed account of this appears in Epperson [12]. An analysis of the
RKF methods also appears in Burden and Faires [17]. The details of all of this are
TLFeBOOK

VARIABLE-STEP-SIZE (ADAPTIVE) METHODS FOR ODEs
465
quite tedious, and so are not presented here. It is also worth noting that an account
of MATLAB ODE solvers is given by Shampine and Reichelt [13], who present
some improvements to the older MATLAB codes that make them better at solving
stiff systems (next section).
A pseudocode for something like ode45 is as follows, and is based on Algo-
rithm 6.5 in Epperson [12]:
Input t0, x0; { initial condition and starting time }
Input tolerance ϵ > 0;
Input the initial step size h > 0, and final time tf > t0;
n := 0;
while tn ≤tf do begin
X1 := RKF4(xn, tn, h); { 4th order RKF estimate of xn+1 }
X2 := RKF5(xn, tn, h); { 5th order RKF estimate of xn+1 }
E := |X1 −X2|;
if 1
4 hϵ ≤E ≤hϵ then begin { h is OK }
xn+1 := X2;
tn+1 := tn + h;
n := n + 1;
else if E > hϵ then { h is too big }
h := h/2; { reduce h and repeat }
else { h is too small }
h := 2h;
xn+1 := X2;
tn+1 := tn + h;
n := n + 1;
end;
end;
Of course, variations on the “theme” expressed in this pseudocode are possible. As
noted in Epperson [12], a drawback of this algorithm is that it will tend to oscillate
between small and large step size values. We emphasize that the method is based on
considering the local error in going from time step tn to tn+1. However, this does
not in itself guarantee that the global error |x(tn) −xn| is small. It turns out that if
adequate smoothness prevails [e.g., if f (x, t) is Lipschitz as per Deﬁnition 10.1],
then small local errors do imply small global errors (see Theorem 6.6 or 6.7 in
Ref. 12).
Example 10.13
This example illustrates a typical application of MATLAB
routine ode23 to the problem of simulating the Colpitts oscillator circuit of Example
10.2.
Figure 10.21 shows a typical plot of vCE(t), and the phase portrait for parameter
values
VTH = 0.75 V (volts),
VCC = 5 (V),
VEE = −5 (V),
REE = 400  (ohms),
RL = 35 (),
L = 98.5 × 10−6 H (henries),
βF = 200,
RON = 100 (),
C1 = C2 = 54 × 10−9 F (farads).
(10.154)
TLFeBOOK

466
NUMERICAL SOLUTION OF ORDINARY DIFFERENTIAL EQUATIONS
0
(a)
(b)
1
2
3
4
5
6
−2.5
−2
−1.5
−1
−0.5
0
0.5
1
VBE (t) (volts)
VCE (t) (volts)
VCE (t) (volts)
2.3
2.4
2.5
2.6
2.7
2.8
2.9
3
3.1
× 10−3
1
2
3
4
5
6
t (seconds)
Figure 10.21
Chaotic regime (a) phase portrait of Colpitts oscillator and (b) collector–
emitter voltage, showing typical results of applying MATLAB routine ode23 to the simula-
tion of the Colpitts oscillator circuit. Equation (10.154) speciﬁes the circuit parameters for
the results shown here.
These circuit parameters were used in Kennedy [9], and the phase portrait in
Fig. 10.21 is essentially that in Fig. 5 of that article [9]. The MATLAB code that
generates Fig. 10.21 appears in Appendix 10.B.
For the parameters in (10.154) the circuit simulation phase portrait in Fig. 10.21
is that of a strange attractor [11, 16], and so is strongly indicative (although not
conclusive) of chaotic dynamics in the circuit.
We note that under “normal” circumstances the Colpitts oscillator is intended to
generate sinusoidal waveforms, and so the chaotic regime traditionally represents
a failure mode, or abnormal operating condition for the circuit. However, Kennedy
[9] suggests that the chaotic mode of operation may be useful in chaos-based data
communications (e.g., chaotic–carrier communications).
The following circuit parameters lead to approximately sinusoidal circuit out-
puts:
VTH = 0.75 V,
VCC = 5 V,
VEE = −5 V,
REE = 100 ,
RL = 200 ,
L = 100 × 10−6 H,
βF = 80,
RON = 115 ,
C1 = 45 × 10−9 F,
C2 = 58 × 10−9 F.
(10.155)
TLFeBOOK

STIFF SYSTEMS
467
−6
−5
−4
−3
−2
−1
0
1
0
0.2
0.4
0.6
0.8
1
1.2
1.4
2.88
2.9
2.92
2.94
2.96
2.98
3
3.02
−4
−3.5
−3
−2.5
−2
−1.5
−1
(a)
(b)
VBE (t) (volts)
VCE (t) (volts)
VCE (t) (volts)
× 10−3
t (seconds)
Figure 10.22
Sinusoidal
operations
(a) phase
portrait
of
Colpitts
operator
and
(b) collector–emitter voltage, showing typical results of applying MATLAB routine ode23
to the simulation of the Colpitts oscillator circuit. Equation (10.155) speciﬁes the circuit
parameters for the results shown here.
Figure 10.22 shows the phase portrait for the oscillator using these parameter val-
ues. We see that vCE(t) is much more sinusoidal than in Fig. 10.21. The trajectory
in the phase portrait of Fig. 10.22 is tending to an elliptical closed curve indicative
of simple harmonic (i.e., sinusoidal) oscillation.
10.6
STIFF SYSTEMS
Consider the general system of coupled ﬁrst-order ODEs
dx0(t)
dt
=
f0(x0, x1, . . . , xm−1, t),
dx1(t)
dt
=
f1(x0, x1, . . . , xm−1, t),
...
dxm−1(t)
dt
=
fm−1(x0, x1, . . . , xm−1, t),
(10.156)
TLFeBOOK

468
NUMERICAL SOLUTION OF ORDINARY DIFFERENTIAL EQUATIONS
which we wish to solve for t ≥0 given x(0), where x(t) = [x0(t) x1(t)
· · · xm−1(t)]T . If we also deﬁne f (x(t), t) = [f0(x(t), t) f1(x(t), t) · · · fm−1
(x(t), t)]T , then we may express (10.156) in compact vector form as
dx(t)
dt
= f (x(t), t).
(10.157)
We have so far described a general order m ODE IVP.
If we now wish to consider the stability of a numerical method applied to the
solution of (10.156) [or (10.157)], then we need to consider the model problem
dx(t)
dt
= Ax(t),
(10.158)
where
A =


∂f0(x(0), 0)
∂x0
∂f0(x(0), 0)
∂x1
· · ·
∂f0(x(0), 0)
∂xm−1
∂f1(x(0), 0)
∂x0
∂f1(x(0), 0)
∂x1
· · ·
∂f1(x(0), 0)
∂xm−1
...
...
...
∂fm−1(x(0), 0)
∂x0
∂fm−1(x(0), 0)
∂x1
· · ·
∂fm−1(x(0), 0)
∂xm−1


(10.159)
[which generalizes A ∈R2×2 in (10.94)]. The solution to (10.158) is given by
x(t) = eAtx(0)
(10.160)
[recall (10.92)].9 Ensuring the stability of the order m linear ODE system in
(10.158) requires all the eigenvalues λk of A to have negative real parts (i.e.,
Re[λk] < 0 for all k = 0, 1, . . . , m −1, where λk is the kth eigenvalue of A).
Of course, if m = 1, then with x(t) = x0(t), (10.158) reduces to
dx(t)
dt
= λx(t),
(10.161)
which is the model problem (10.28) again. Recall once again Example 10.5, for
which we found that
λ = ∂f (x0, t0)
∂x
= −2
t0
,
9Note that if we know x(t0) (any t0 ∈R), then we may slightly generalize our linear problem (10.158)
to determining x(t) for all t ≥t0, in which case
x(t) = eA(t−t0)x(t0)
replaces (10.160). However, little is lost by assuming t0 = 0.
TLFeBOOK

MATLAB CODE FOR EXAMPLE 10.8
469
as (10.41) is the solution to dx/dt = t2 −2x/t for t ≥t0 > 0. If t0 is small, then
|λ| is large, and we saw that numerical methods, especially low-order explicit
ones, had difﬁculty in estimating x(t) accurately when t was near t0. If we recall
(10.33) (which stated that h ≤2/|λ|) as an example, we see that large negative
values for λ force us to select small step sizes h to ensure stability of the explicit
Euler method. Since λ is an eigenvalue of A = [λ] in (10.161), we might expect
that this generalizes. In other words, a numerical method can be expected to have
accuracy problems if A in (10.159) has eigenvalues with large negative real parts.
In a situation like this x(t) in (10.156) has (it seems) a solution that changes so
rapidly for some time intervals (e.g., fast startup transients) that accurate numerical
solutions are hard to achieve. Such systems are called stiff systems.
So far our deﬁnition of a stiff system has not been at all rigorous. Indeed,
a rigorous deﬁnition is hard to come by. Higham and Trefethen [15] argue that
looking at the eigenvalues of A alone is not enough to decide on the stiffness of
(10.156) in a completely reliable manner. It is possible, for example, that A may
have favorable eigenvalues and yet (10.156) may still be stiff.
Stiff systems will not be discussed further here except to note that implicit
methods, or higher-order predictor–corrector methods, should be used for their
solution. The paper by Higham and Trefethen [15] is highly recommended reading
for those readers seriously interested in the problems posed by stiff systems.
10.7
FINAL REMARKS
In the numerical solution (i.e., simulation) of ordinary differential equations (ODEs),
two issues are of primary importance: accuracy and stability. The successful sim-
ulation of any system requires proper attention to both of these issues.
Computational efﬁciency is also an issue. Generally, we prefer to use the largest
possible step size consistent with required accuracy, and as such to avoid any
instability in the simulation.
APPENDIX 10.A
MATLAB CODE FOR EXAMPLE 10.8
%
%
f23.m
%
% This defines function f(x,t) in the differential equation for Example 10.8
% (in Section 10.2).
%
function y = f23(x,t)
y = t*t - (2*x/t);
%
%
Runge.m
%
% This routine simulates the Heun’s, and 4th order Runge-Kutta methods as
TLFeBOOK

470
NUMERICAL SOLUTION OF ORDINARY DIFFERENTIAL EQUATIONS
% applied to the differential equation in Example 10.8 (Sect. 10.2), so this
% routine requires function f23.m.
It therefore generates Fig. 10.9.
%
function Runge
t0 = .05;
% initial time (starting time)
x0 = 1.0;
% initial condition (x(t0))
% Exact solution x(t)
c = (5*t0*t0*x0) - (t0^5);
te = [t0:.02:1.5];
for k = 1:length(te)
xe(k) = (te(k)*te(k)*te(k))/5 + c/(5*te(k)*te(k));
end;
h = .05;
% Heun’s method simulation
xh(1) = x0;
th(1) = t0;
for n = 1:25
fn = th(n)*th(n) - (2*xh(n)/th(n));
% f(x_n,t_n)
th(n+1) = th(n) + h;
xn1 = xh(n) + h*fn;
tn1 = th(n+1);
fn1 = tn1*tn1 - (2*xn1/tn1);
% f(x_{n+1},t_{n+1}) (approx.)
xh(n+1) = xh(n) + (h/2)*(fn + fn1);
end;
% 4th order Runge-Kutta simulation
xr(1) = x0;
tr(1) = t0;
for n = 1:25
t = tr(n);
x = xr(n);
k1 = f23(x,t);
k2 = f23(x + .5*h*k1,t + .5*h);
k3 = f23(x + .5*h*k2,t + .5*h);
k4 = f23(x + h*k3,t + h);
xr(n+1) = xr(n) + (h/6)*(k1 + 2*k2 + 2*k3 + k4);
tr(n+1) = tr(n) + h;
end;
plot(te,xe,’-’,tr,xr,’--o’,th,xh,’--+’), grid
legend(’x(t)’,’4th Order Runge-Kutta (h = .05)’,’Heun (h = .05)’,1);
xlabel(’ Time (t) ’)
ylabel(’ Amplitude ’)
APPENDIX 10.B
MATLAB CODE FOR EXAMPLE 10.13
%
%
fR.m
%
TLFeBOOK

MATLAB CODE FOR EXAMPLE 10.13
471
% This is Equation (10.15) of Chapter 10 pertaining to Example 10.2.
%
function i = fR(v)
VTH = 0.75; % Threshold voltage in volts
RON = 100;
% On resistance of NPN BJT Q in Ohms
if v <= VTH
i = 0;
else
i = (v-VTH)/RON;
end;
%
%
vCC.m
%
% Supply voltage function v_CC(t) for Example 10.2 of Chapter 10.
% Here v_CC(t) = V_CC u(t) (i.e., oscillator switches on at t = 0).
%
function v = vCC(t)
VCC = 5;
if t < 0
v = 0;
else
v = VCC;
end;
%
%
Colpitts.m
%
% Computes the right-hand side of the state equations in Equation (10.17a,b,c)
% pertaining to Example 10.2 of Chapter 10.
%
function y = Colpitts(t,x)
C1 = 54e-9;
C2 = 54e-9;
REE = 400;
VEE = -5;
betaF = 200;
RL = 35;
L = 98.5e-6;
y(1) = ( x(3) - betaF*fR(x(2)) )/C1;
y(2) = ( -(x(2)+VEE)/REE - fR(x(2)) - x(3) )/C2;
y(3) = ( vCC(t) - x(1) + x(2) - RL*x(3))/L;
y = y.’;
%
%
SimulateColpitts.m
%
% This routine uses vCC.m, fR.m and Colpitts.m to simulate the Colpitts
% oscillator circuit of Example 10.2 in Chapter 10.
It produces
% Figure 10.21 in Chapter 10.
%
% The state vector x(:,:) is as follows:
%
x(:,1) = v_CE(t)
TLFeBOOK

472
NUMERICAL SOLUTION OF ORDINARY DIFFERENTIAL EQUATIONS
%
x(:,2) = v_BE(t)
%
x(:,3) = i_L(t)
%
function SimulateColpitts
[t,x] = ode23(@Colpitts, [ 0 0.003 ], [ 0 0 0 ]);
% [ 0 0.003 ] ---> Simulate 0 to 3 milliseconds
% [ 0 0 0 ]
---> Initial state vector
clf
L = length(t);
subplot(211), plot(x(:,1),x(:,2)), grid
xlabel(’ v_{CE} (t) (volts) ’)
ylabel(’ v_{BE} (t) (volts) ’)
title(’ Phase Portrait of the Colpitts Oscillator (Chaotic Regime) ’)
subplot(212), plot(t(L-1999:L),x(L-1999:L,1),’-’), grid
xlabel(’ t (seconds) ’)
ylabel(’ v_{CE} (t) (volts) ’)
title(’ Collector-Emitter Voltage (Chaotic Regime) ’)
REFERENCES
1. P. E. Hydon, Symmetry Methods for Differential Equations: A Beginner’s Guide, Cam-
bridge Univ. Press, Cambridge, UK, 2000.
2. C. T.-C. Nguyen and R. T. Howe, “An Integrated CMOS Micromechanical Resonator
High-Q Oscillator,” IEEE J. Solid-State Circuits 34, 450–455 (April 1999).
3. E. Kreyszig, Introductory Functional Analysis with Applications, Wiley, New York,
1978.
4. E. Kreyszig, Advanced Engineering Mathematics, 4th ed., Wiley, New York, 1979.
5. L. M. Kells, Differential Equations: A Brief Course with Applications, McGraw-Hill,
New York, 1968.
6. E. Beltrami, Mathematics for Dynamic Modeling, Academic Press, Boston, MA, 1987.
7. S. S. Rao, Applied Numerical Methods for Engineers and Scientists, Prentice-Hall,
Upper Saddle River, NJ, 2002.
8. R. Bronson, Modern Introductory Differential Equations (Schaum’s Outline Series),
McGraw-Hill, New York, 1973.
9. M. P. Kennedy, “Chaos in the Colpitts Oscillator,” IEEE Trans. Circuits Syst. (Part I:
Fundamental Theory and Applications) 41, 771–774 (Nov. 1994).
10. A. S. Sedra and K. C. Smith, Microelectronic Circuits, 3rd ed., Saunders College Publ.,
Philadelphia, PA, 1989.
11. J. Guckenheimer and P. Holmes, Nonlinear Oscillations, Dynamical Systems, and Bifur-
cations of Vector Fields, Springer-Verlag, New York, 1983.
12. J. F. Epperson, An Introduction to Numerical Methods and Analysis, Wiley, New York,
2002.
13. L. F. Shampine and M. W. Reichelt, “The MATLAB ODE Suite,” SIAM J. Sci. Comput.
18, 1–22 (Jan. 1997).
TLFeBOOK

PROBLEMS
473
14. J. R. Dormand and P. J. Prince, “A Family of Embedded Runge-Kutta Formulae,” J.
Comput. Appl. Math. 6, 19–26 (1980).
15. D. J. Higham and L. N. Trefethen, “Stiffness of ODEs,” BIT 33, 285–303 (1993).
16. P. G. Drazin, Nonlinear Systems, Cambridge Univ. Press, Cambridge, UK, 1992.
17. R. L. Burden and J. D. Faires, Numerical Analysis, 4th ed., PWS-KENT Publ., Boston,
MA, 1989.
PROBLEMS
10.1. Consider the electric circuit depicted in Fig. 10.P.1. Find matrix A ∈R2×2
and vector b ∈R2 such that


diL1(t)
dt
diL2(t)
dt

= A

iL1(t)
iL2(t)
	
+ bvs(t),
where iLk(t) is the current through inductor Lk (k ∈{1, 2, 3}).
(Comment: Although the number of energy storage elements in the circuit is
3, there are only two state variables needed to describe the circuit dynamics.)
10.2. The circuit in Fig. 10.P.2 is a simpliﬁed model for a parametric ampliﬁer.
The ampliﬁer contains a reverse-biased varactor diode that is modeled by
the parallel interconnection of linear time-invariant capacitor C0 and linear
time-varying capacitor C(t). You may assume that C(t) = 2C1 cos(ωpt),
where C1 is constant, and ωp is the pumping frequency. Note that
iC(t) = d
dt [C(t)v(t)].
The input to the ampliﬁer is the ideal cosinusoidal current source is(t) =
2Is cos(ω0t), and the load is the resistor R, so the output is the current i(t)
vs(t)
R1
R2
L2
L1
L3
iL1(t)
iL2(t)
iL3(t)
+
−
Figure 10.P.1
The linear electric circuit for Problem 10.1.
TLFeBOOK

474
NUMERICAL SOLUTION OF ORDINARY DIFFERENTIAL EQUATIONS
L
C(t)
C0
v(t)
iC0(t)
iC(t)
Load
R
iL(t)
is(t)
i(t)
Varactor  diode  model
+
−
Figure 10.P.2
A model for a parametric ampliﬁer (Problem 10.2).
into R. Write the state equations for the circuit, assuming that the state vari-
ables are v(t), and iL(t). Write these equations in matrix form. (Comment:
This problem is based on the example of a parametric ampliﬁer as consid-
ered in C. A. Desoer and E. S. Kuh, Basic Circuit Theory, McGraw-Hill,
New York, 1969.)
10.3. Give a detailed derivation of Eqs. (10.17).
10.4. The general linear ﬁrst-order ODE is
dx(t)
dt
= a(t)x(t) + b(t), x(t0) = x0.
Use the trapezoidal method to ﬁnd an expression for xn+1 in terms of xn,
tn, and tn+1.
10.5. Prove that the trapezoidal method for ODEs is unconditionally stable.
10.6. Consider Theorem 10.2. Assume that ˆx0 = x(t0). Use (10.23) to ﬁnd an
upper bound on |x(tn) −ˆxn|/M for the following ODE IVPs:
(a) dx
dt = 1 −2x,
x(0) = 1.
(b) dx
dt = 2 cos x,
x(0) = 0.
10.7. Consider the ODE IVP
dx(t)
dt
= 1 −2x,
x(0) = 1.
(a) Approximate the solution to this problem using the explicit Euler
method with h = 0.1 for n = 0, 1, . . . , 10. Do the computations with a
pocket calculator.
(b) Find the exact solution x(t).
TLFeBOOK

PROBLEMS
475
10.8. Consider the ODE IVP
dx(t)
dt
= 2 cos x,
x(0) = 0.
(a) Approximate the solution to this problem using the explicit Euler
method with h = 0.1 for n = 0, 1, . . . , 10. Do the computations with a
pocket calculator.
(b) Find the exact solution x(t).
10.9. Write a MATLAB routine to simulate the ODE
dx
dt =
x
x + t
for the initial condition x(0) = 1. Use both the implicit and explicit Euler
methods. The program must accept as input the step size h, and the number
of iterations N that are desired. Parameters h and N are the same for both
methods. The program output is be written to a ﬁle in the form of a table
something such as (e.g., for h = 0.05, and N = 5) the following:
time step
explicit x_n
implicit x_n
.0000
value
value
.0500
value
value
.1000
value
value
.1500
value
value
.2000
value
value
.2500
value
value
Test your program out on
h = 0.01,
N = 100
and
h = 0.10,
N = 10.
10.10. Consider
dx(t)
dt
= α(1 −2βt2)e−βt2.
(10.P.1)
(a) Verify that for t ≥0, with x(0) = 0, we have the solution
x(t) = αte−βt2.
(10.P.2)
(b) For what range of step sizes h is the explicit Euler method a stable
means of solving (10.P.1)?
TLFeBOOK

476
NUMERICAL SOLUTION OF ORDINARY DIFFERENTIAL EQUATIONS
(c) Write a MATLAB routine to simulate (10.P.1) for x(0) = 0 using both
the explicit and implicit Euler methods. Assume that α = 10 and β = 1.
Test your program out on
h = 0.01,
N = 400
(10.P.3a)
and
h = 0.10,
N = 40.
(10.P.3b)
The program must produce plots of {xn|n = 0, 1, . . . , N} (for both
explicit and implicit methods), and x(t) (from (10.P.2)) on the same
graph. This will lead to two separate plots, one for each of (10.P.3a)
and (10.P.3b).
10.11. Consider
dx(t)
dt
= 2tx + 1,
x(0) = 0.
(a) Find {xn|n = 0, 1, . . . , 10} for h = 0.1 using the explicit Euler method.
Do the calculations with a pocket calculator.
(b) Verify that
x(t) = et2  t
0
e−s2 ds.
(c) Does the stability condition (10.33) apply here? Explain.
10.12. Prove that Runge–Kutta methods of order one have no degrees of freedom
(i.e., we are forced to select c1 in only one possible way).
10.13. A fourth-order Runge-Kutta method is
xn+1 = xn + 1
6h[k1 + 2k2 + 2k3 + k4]
for which
k1 = f (xn, tn),
k2 = f

xn + 1
2hk1, tn + 1
2h

,
k3 = f

xn + 1
2hk2, tn + 1
2h

, k4 = f (xn + hk3, tn + h).
When applied to the model problem, we get xn = σ nx0 for n ∈Z+. Derive
the expression for σ.
10.14. A third-order Runge–Kutta method is
xn+1 = xn + 1
6[k1 + 4k2 + k3]
TLFeBOOK

PROBLEMS
477
for which
k1 = hf (xn, tn),
k2 = hf (xn + 1
2h, tn + 1
2k1),
k3 = hf (xn + h, tn −k1 + 2k2).
(a) When applied to the model problem, we get xn = σ nx0 for n ∈Z+.
Derive the expression for σ.
(b) Find the allowable range of step sizes h that ensure stability of the
method.
10.15. Consider the fourth-order Runge–Kutta method in Eqs. (10.80) and (10.81).
Show that if f (x, t) = f (t), then the method reduces to Simpson’s rule for
numerical integration over the interval [tn, tn+1].
10.16. Recall Eq. (10.98). Suppose that A ∈R2×2 has distinct eigenvalues γk such
that Re[γk] > 0 for at least one of the eigenvalues. Show that I + hA will
have at least one eigenvalue λk such that |λk| > 1.
10.17. Consider the coupled ﬁrst-order ODEs
dx
dt = −y
7
x2 + y2,
(10.P.4a)
dy
dt = x
7
x2 + y2,
(10.P.4b)
where (x0, y0) = (x(0), y(0)) are the initial conditions.
(a) Prove that for suitable constants r0 and θ0, we have
x(t) = r0 cos(r0t + θ0),
y(t) = r0 sin(r0t + θ0).
(10.P.5)
(b) Write a MATLAB routine to simulate the system represented by (10.P.4)
using the explicit Euler method [which will produce xn = [xn
yn]T
such that xn ≈x(tn) and yn ≈y(tn)]. Assume that h = 0.05 and the
initial condition is x0 = [1
0]T . Plot xn and (x(t), y(t)) (via (10.P.5))
on the (x, y) plane.
(c) Write a MATLAB routine to simulate the system represented by (10.P.4)
using Heun’s method [which will produce xn = [xn
yn]T such that
xn ≈x(tn) and yn ≈y(tn)]. Assume that h = 0.05 and the initial con-
dition is x0 = [1
0]T . Plot xn, and (x(t), y(t)) [via (10.P.5)] on the
(x, y) plane.
Make reasonable choices about the number of time steps in the simulation.
10.18. In the previous problem the step size is h = 0.05.
(a) Determine whether the simulation using the explicit Euler method is
stable for this choice of step size. (Hint: Recall that one must consider
the eigenvalues of I + hA.)
TLFeBOOK

478
NUMERICAL SOLUTION OF ORDINARY DIFFERENTIAL EQUATIONS
(b) Determine whether the simulation using Heun’s method is stable for
this choice of step size. [Hint: Consider the implications of (10.121).]
Use the MATLAB eig function to assist you in your calculations.
10.19. A curve in R2 is speciﬁed parametrically according to
x(t) = A cos(ωt),
y(t) = αA cos(ωt −φ),
(10.P.6)
where α, A > 0, and t ∈R is the “parameter.” When the points (x(t), y(t))
are plotted, the result is what electrical engineers often call a Lissajous ﬁgure
(or curve), which is really just an alternative name for a phase portrait.
(a) Find an implicit function expression for the curve, that is, ﬁnd a descrip-
tion of the form
f (x, y) = 0
(10.P.7)
[i.e., via algebra and trigonometry eliminate t from (10.9.6) to obtain
(10.9.7)].
(b) On the (x, y) plane sketch the Lissajous ﬁgures for cases φ = 0, φ =
± π
2 and φ = ± π
4 .
(c) An interpretation of (10.9.6) is that x(t) may be the input voltage from
a source in an electric circuit, while y(t) may be the output voltage
drop across a load element in the circuit (in the steady-state condition
of course). Find a simple expression for sin φ in terms of B such that
f (0, B) = 0 [i.e., point(s) B on the y axis where the curve cuts the
y axis], and in terms of α and A. (Comment: On analog oscilloscopes
of olden days, it was possible to display a Lissajous ﬁgure, and so use
this ﬁgure to estimate the phase angle φ on the lab bench.)
10.20. Verify the values for λk in Table 10.1 for m = 2.
10.21. Verify the values for λk in Table 10.2 for m = 2.
10.22. For the ODE IVP
dx(t)
dt
= f (x, t),
x(t0) = x0,
write pseudocode for a numerical method that approximates the solution to
it using an AB method for m = 2 as a predictor, an AM method for m = 2
as a corrector, and the third-order Runge–Kutta (RK) method from Problem
10.14 to perform the initialization.
10.23. From Forsythe, Malcolm, and Moler (see Ref. 5 in Chapter 2)
dx
dt = 998x + 1998y
(10.P.8a)
dy
dt = −999x −1999y
(10.P.8b)
TLFeBOOK

PROBLEMS
479
has the solution
x(t) = 4e−t −3e−1000t,
(10.P.9a)
y(t) = −2e−t + 3e−1000t,
(10.P.9b)
where x(0) = y(0) = 1. Recall A as given by (10.94).
(a) By direct substitution verify that (10.P.9) is the solution to (10.P.8).
(b) If the eigenvalues of I + hA are λ0 and λ1, plot |λ0| and |λ1| versus h
(using MATLAB).
(c) If the eigenvalues of I + hA + 1
2h2A2 [recall (10.121)] are λ0 and λ1,
plot |λ0| and |λ1| versus h (using MATLAB).
In parts (b) and (c), what can be said about the range of values for h leading
to a stable simulation of the system (10.P.8)?
10.24. Consider the coupled system of ﬁrst-order ODEs
dx(t)
dt
= Ax(t) + y(t),
(10.P.10)
where A ∈Rn×n and x(t), y(t) ∈Rn for all t ∈R+. Suppose that the eigen-
values of A are λ0, λ1, . . . , λn−1 such that Re[λk] < 0 for all k ∈Zn.
Suppose that
σ ≤Re[λk] ≤τ < 0
for all k. A stiffness quotient is deﬁned to be
r = σ
τ .
The system (10.P.10) is said to be stiff if r ≫1 (again, we assume Re[λk] <
0 for all k).
For the previous problem, does A correspond to a stiff system? (Comment:
As Higham and Trefethen [15] warned, the present stiffness deﬁnition is
not entirely reliable.)
TLFeBOOK

11
Numerical Methods
for Eigenproblems
11.1
INTRODUCTION
In previous chapters we have seen that eigenvalues and eigenvectors are important
(e.g., recall condition numbers from Chapter 4, and the stability analysis of numer-
ical methods for ODEs in Chapter 10). In this chapter we treat the eigenproblem
somewhat more formally than previously. We shall deﬁne and review the basic
problem in Section 11.2, and in Section 11.3 we shall apply this understanding to
the problem of computing the matrix exponential [i.e., exp(At), where A ∈Rn×n
and t ∈R] since this is of central importance in many areas of electrical and com-
puter engineering (signal processing, stability of dynamic systems, control systems,
circuit simulation, etc.). In subsequent sections we will consider numerical methods
to determine the eigenvalues and eigenvectors of matrices.
11.2
REVIEW OF EIGENVALUES AND EIGENVECTORS
In this section we review some basic facts relating to the determination of eigen-
values and eigenvectors of matrices. Our emphasis, with a few exceptions, is on
matrices that are diagonalizable.
Deﬁnition 11.1: Eigenproblem
Let A ∈Cn×n. The eigenproblem for A is
to ﬁnd solutions to the matrix equation
Ax = λx,
(11.1)
where λ ∈C and x ∈Cn such that x ̸= 0. A solution (λ, x) to (11.1) is called an
eigenpair, λ is an eigenvalue, and x is its corresponding eigenvector.
Even if A ∈Rn×n (the situation we will emphasize most) it is very possible to
have λ ∈C and x ∈Cn. We must also emphasize that x = 0 is never permitted to
be an eigenvector for A.
We may rewrite (11.1) as (I is the n × n identity matrix)
(A −λI)x = 0,
(11.2a)
An Introduction to Numerical Analysis for Electrical and Computer Engineers, by C.J. Zarowski
ISBN 0-471-46737-5
c⃝2004 John Wiley & Sons, Inc.
480
TLFeBOOK

REVIEW OF EIGENVALUES AND EIGENVECTORS
481
or equivalently as
(λI −A)x = 0.
(11.2b)
Equations (11.2) are homogeneous linear systems of n equations in n unknowns.
Since x = 0 is never an eigenvector, an eigenvector x must be a nontrivial solution
to (11.2). An n × n homogeneous linear system has a nonzero (i.e., nontrivial)
solution iff the coefﬁcient matrix is singular. Immediately, eigenvalue λ satisﬁes
det(A −λI) = 0,
(11.3a)
or equivalently
det(λI −A) = 0.
(11.3b)
Of course, p(λ) = det(λI −A) is a polynomial of degree n. In principle, we
may ﬁnd eigenpairs by ﬁnding p(λ) (the characteristic polynomial of A), then
ﬁnding the zeros of p(λ), and then substituting these into (11.2) to ﬁnd x. In
practice this approach really works only for small analytical examples. Properly
conceived numerical methods are needed to determine eigenpairs reliably for larger
matrices A.
To set the stage for what follows, consider the following examples.
Example 11.1
From Hill [1], consider the example
A =


1
0
0
2
1
0
0
0
3

.
Here p(λ) = det(λI −A) = (λ −1)2(λ −3) so A has a double eigenvalue (eigen-
value of multiplicity 2) at λ = 1, and a simple eigenvalue at λ = 3. The eigenvalues
may be individually denoted as λ0 = 1, λ1 = 1, and λ2 = 3.
For λ = 3, (3I −A)x = 0 becomes


2
0
0
−2
2
0
0
0
0




x0
x1
x2

=


0
0
0

,
and from the application of elementary row operations this reduces to


1
0
0
0
1
0
0
0
0




x0
x1
x2

=


0
0
0

.
Immediately, x0 = x1 = 0, and x2 is arbitrary (except, of course, it is not allowed
to be zero) so that the general form of the eigenvector corresponding to λ = λ2 is
v(2) = [ 0
0
x2 ]T ∈C3.
TLFeBOOK

482
NUMERICAL METHODS FOR EIGENPROBLEMS
On the other hand, now let us consider λ = λ0 = λ1. In this case (I −A)x = 0
becomes


0
0
0
−2
0
0
0
0
−2




x0
x1
x2

=


0
0
0

,
which reduces to


1
0
0
0
0
1
0
0
0




x0
x1
x2

=


0
0
0

.
Immediately, x0 = x2 = 0, and x1 is arbitrary, so an eigenvector corresponding to
λ = λ0 = λ1 is of the general form
v(0) = [ 0
x1
0 ]T ∈C3.
Even though λ = 1 is a double eigenvalue, we are able to ﬁnd only one eigenvector
for this case. In effect, one eigenvector seems to be “missing.”
Example 11.2
Now consider (again from Hill [1])
A =


0
−2
1
1
3
−1
0
0
1

.
Here p(λ) = det(λI −A) = (λ −1)2(λ −2). The eigenvalues of A are thus λ0 =
1, λ1 = 1, and λ2 = 2.
For λ = 2, (2I −A)x = 0 becomes


2
2
−1
−1
−1
1
0
0
1




x0
x1
x2

=


0
0
0

,
which reduces to


1
1
0
0
0
1
0
0
0




x0
x1
x2

=


0
0
0

,
so the general form of the eigenvector for λ = λ2 is
v(2) = x(2) = [ −x0
x0
0 ]T ∈C3.
Now, if we consider λ = λ0 = λ1, (I −A)x = 0 becomes


1
2
−1
−1
−2
1
0
0
0




x0
x1
x2

=


0
0
0

,
TLFeBOOK

REVIEW OF EIGENVALUES AND EIGENVECTORS
483
which reduces to


1
2
−1
0
0
0
0
0
0




x0
x1
x2

=


0
0
0

.
Since x0 + 2x1 −x2 = 0, we may choose any two of x0, x1, or x2 as free para-
meters giving a general form of an eigenvector for λ = λ0 = λ1 as
v(0) = x0


1
0
1


#
$%
&
=x(0)
+ x1


0
1
2


#
$%
&
=x(1)
∈C3.
Thus, we have eigenpairs (λ0, x(0)), (λ1, x(1)), (λ2, x(2)).
We continue to emphasize that in all cases any free parameters are arbitrary,
except that they must never be selected to give a zero-valued eigenvector. In
Example 11.2 λ = 1 is an eigenvalue of multiplicity 2, and v(0) is a vector in
a two-dimensional vector subspace of C3. On the other hand, in Example 11.1
λ = 1 is also of multiplicity 2, and yet v(0) is only a vector in a one-dimensional
vector subspace of C3.
Deﬁnition 11.2: Defective Matrix
For any A ∈Cn×n, if the multiplicity
of any eigenvalue λ ∈C is not equal to the dimension of the solution space
(eigenspace) of (λI −A)x = 0, then A is defective.
From this deﬁnition, A in Example 11.1 is a defective matrix, while A in
Example 11.2 is not defective (i.e., is nondefective). In a sense soon to be made
precise, defective matrices cannot be diagonalized. However, all matrices, diago-
nalizable or not, can be placed into Jordan canonical form, as follows.
Theorem 11.1: Jordan Decomposition
If A ∈Cn×n, then there exists a non-
singular matrix T ∈Cn×n such that
T −1AT = diag(J0, J1, . . . , Jk−1),
where
Ji =


λi
1
0
· · ·
0
0
0
λi
1
· · ·
0
0
...
...
...
...
...
0
0
0
· · ·
λi
1
0
0
0
· · ·
0
λi


∈Cmi×mi,
and k−1
i=0 mi = n.
TLFeBOOK

484
NUMERICAL METHODS FOR EIGENPROBLEMS
Proof
See Halmos [2] or Horn and Johnson [3].
Of course, λi in the theorem is an eigenvalue of A. The submatrices Ji are called
Jordan blocks. The number of blocks k and their dimensions mi are unique, but
their order is not unique. Note that if an eigenvalue has a multiplicity of unity (i.e.,
if it is simple), then the Jordan block in this case is the 1 × 1 matrix consisting
of that eigenvalue. From the theorem statement the characteristic polynomial of
A ∈Cn×n is given by
p(λ) = det(λI −A) =
k−1
/
i=0
(λ −λi)mi.
(11.4)
But if A ∈Rn×n, and if λi ∈C for some i, then λ∗
i must also be an eigenvalue of
A, that is, a zero of p(λ). This follows from the fundamental theorem of algebra,
which states that complex-valued roots of polynomials with real-valued coefﬁcients
must always occur in complex–conjugate pairs.
Example 11.3
Consider (with θ ̸= kπ, k ∈Z)
A =
 cos θ
−sin θ
sin θ
cos θ

∈R2×2
(2 × 2 rotation operator from Appendix 3.A). We have the characteristic equation
p(λ) = det(λI −A) = det
'
λ −cos θ
sin θ
−sin θ
λ −cos θ
	(
= λ2 −2 cos θλ + 1 = 0
for which the roots (eigenvalues of A) are therefore λ = e±jθ. Deﬁne λ0 = ejθ,
λ1 = e−jθ. Clearly λ1 = λ∗
0 (i.e., the two simple eigenvalues of A are a conju-
gate pair).
For λ = λ0, (λ0I −A)x = 0 is
sin θ

j
1
−1
j
	  x0
x1

=
 0
0

,
which reduces to

1
−j
0
0
	  x0
x1

=
 0
0

.
The eigenvector for λ = λ0 is therefore of the form
x(0) = a

1
−j

, a ∈C.
TLFeBOOK

REVIEW OF EIGENVALUES AND EIGENVECTORS
485
Similarly, for λ = λ1, the homogeneous linear system (λ1I −A)x = 0 is
sin θ
 −j
1
−1
−j
  x0
x1

=
 0
0

,
which reduces to
 1
j
0
0
  x0
x1

=
 0
0

.
The eigenvector for λ = λ1 is therefore of the form
x(1) = b
 1
j

,
b ∈C.
Of course, free parameters a, and b are never allowed to be zero.
Computing the Jordan canonical form when mi > 1 is numerically rather dif-
ﬁcult (as noted in Golub and Van Loan [4] and Horn and Johnson [3]), and so
is often avoided. However, there are important exceptions often involving state-
variable (state-space) systems analysis and design (e.g., see Fairman [5]). Within
the theory of Jordan forms it is possible to ﬁnd supposedly “missing” eigenvectors,
resulting in a theory of generalized eigenvectors. We will not consider this here as
it is rather involved. Some of the references at the end of this chapter cover the
relevant theory [3].
We will now consider a series of theorems leading to a sufﬁcient condition for
A to be nondefective. Our presentation largely follows Hill [1].
Theorem 11.2: If A ∈Cn×n, then the eigenvectors corresponding to two dis-
tinct eigenvalues of A are linearly independent.
Proof
We employ proof by contradiction.
Suppose that (α, x) and (β, y) are two eigenpairs for A, and α ̸= β. Assume
y = ax for some a ̸= 0 (a ∈C). Thus
βy = Ay = aAx = aαx
and also
βy = aβx.
Hence
aβx = aαx,
implying that
a(β −α)x = 0.
TLFeBOOK

486
NUMERICAL METHODS FOR EIGENPROBLEMS
But x ̸= 0 as it is an eigenvector of A, and also a ̸= 0. Immediately, α = β,
contradicting our assumption that these eigenvalues are distinct. Thus, y = ax is
impossible; that is, we have proved that y is independent of x.
Theorem 11.2 leads us to the next theorem.
Theorem 11.3: If A ∈Cn×n has n distinct eigenvalues, then A has n linearly
independent eigenvectors.
Proof
Uses mathematical induction (e.g., Stewart [6]).
We have already seen the following theorem.
Theorem 11.4: If A ∈Rn×n and A = AT , then all eigenvalues of A are real-
valued.
Proof
See Hill [1], or see the appropriate footnote in Chapter 4.
In addition to this theorem, we also have the following one.
Theorem 11.5: If A ∈Rn×n, and if A = AT , then eigenvectors corresponding
to distinct eigenvalues of A are orthogonal.
Proof
Suppose that (α, x) and (β, y) are eigenpairs of A with α ̸= β. We wish
to show that xT y = yT x = 0 (recall Deﬁnition 1.6). Now
αx = Ax = AT x
so that
αyT x = yT AT x = (Ay)T x = βyT x,
implying that
(α −β)yT x = 0.
But α ̸= β so that yT x = 0, that is, x ⊥y.
Theorem 11.5 states that eigenspaces corresponding to distinct eigenvalues of a
symmetric, real-valued matrix form mutually orthogonal vector subspaces of Rn.
Any vector from one eigenspace must therefore be orthogonal to any eigenvector
from another eigenspace. If we recall Deﬁnition 11.2, it is apparent that all sym-
metric, real-valued matrices are nondefective, if their eigenvalues are all distinct.1
In fact, even if A ∈Cn×n and is not symmetric, then, as long as the eigenvalues
are distinct, A will be nondefective (Theorem 11.3).
1It is possible to go even further and prove that any real-valued, symmetric matrix is nondefective,
even if it possesses multiple eigenvalues. Thus, any real-valued, symmetric matrix is diagonalizable.
TLFeBOOK

REVIEW OF EIGENVALUES AND EIGENVECTORS
487
Deﬁnition 11.3: Similarity Transformation
If A, B ∈Cn×n, and there is a
nonsingular matrix P ∈Cn×n such that
B = P −1AP,
we say that B is similar to A, and that P is a similarity transformation.
If A ∈Cn×n, and A has n distinct eigenvalues forming n distinct eigenpairs
{(λk, x(k))|k = 0, 1, . . . , n −1}, then
Ax(k) = λkx(k)
allows us to write
A [x(0)x(1) · · · x(n−1)]
#
$%
&
=P
= [x(0)x(1) · · · x(n−1)]
#
$%
&
=P


λ0
0
· · ·
0
0
0
λ1
· · ·
0
0
...
...
...
...
0
0
· · ·
0
λn−1


#
$%
&
=
,
(11.5)
that is, AP = P, where  = diag(λ0, λ1, . . . , λn−1) ∈Cn×n is the diagonal matrix
of eigenvalues of A. Thus
P −1AP = ,
(11.6)
and the matrix of eigenvectors P ∈Cn×n of A deﬁnes the similarity transformation
that diagonalizes matrix A. More generally, we have the following theorem.
Theorem 11.6: If A, B ∈Cn×n, and A and B are similar matrices, then A and
B have the same eigenvalues.
Proof
Since A and B are similar, there exists a nonsingular matrix P ∈Cn×n
such that B = P −1AP. Therefore
det(λI −B) = det(λI −P −1AP)
= det(P −1(PP−1λ −A)P )
= det(P −1) det(λI −A) det(P )
= det(λI −A).
Thus, A and B possess the same characteristic polynomial, and so possess identical
eigenvalues.
In other words, similarity transformations preserve eigenvalues.2 Note that The-
orem 11.6 holds regardless of whether A and B are defective. In developing (11.6)
2This makes such transformations highly valuable in state-space control systems design, in addition to
a number of other application areas.
TLFeBOOK

488
NUMERICAL METHODS FOR EIGENPROBLEMS
we have seen that if A ∈Cn×n has n distinct eigenvalues, it is diagonalizable. We
emphasize that this is only a sufﬁcient condition. Example 11.2 conﬁrms that a
matrix can have eigenvalues with multiplicity greater than one, and yet still be
diagonalizable.
11.3
THE MATRIX EXPONENTIAL
In Chapter 10 the problem of computing eAt (A ∈Rn×n, and t ∈R) was associated
with the stability analysis of numerical methods for systems of ODEs. It is also
noteworthy that to solve
dx(t)
dt
= Ax(t) + by(t)
(11.7)
[x(t) = [x0(t)x1(t) · · · xn−1(t)]T ∈Rn, A ∈Rn×n, and b ∈Rn with y(t) ∈R for
all t] required us to compute eAt [recall Example 10.1, which involved an example
of (11.7) from electric circuit analysis; see Eq. (10.9)]. Thus, we see that computing
the matrix exponential is an important problem in analysis. In this section we shall
gain more familiarity with the matrix exponential because of its signiﬁcance.
Moler and Van Loan [7] caution that computing the matrix exponential is a
numerically difﬁcult problem. Stable, reliable, accurate, and computationally efﬁ-
cient algorithms are not so easy to come by. Their paper [7], as its title states,
considers 19 methods, and none of them are entirely satisfactory. Indeed, this
paper [7] appeared in 1978, and to this day the problem of successfully computing
eAt for any A ∈Cn×n has not been fully resolved. We shall say something about
why this is a difﬁcult problem later.
Before considering this matter, we shall consider a general analytic (i.e., hand
calculation) method for obtaining eAt for any A ∈Rn×n, including when A is
defective. In principle, this would involve working with Jordan decompositions
and generalized eigenvectors, but we will avoid this by adopting the approach
suggested in Leonard [8].
The matrix exponential eAt may be deﬁned in the expected manner as
(t) = eAt =
∞

k=0
1
k!Aktk,
(11.8)
so, for example, the kth derivative of the matrix exponential is
(k)(t) = AkeAt = eAtAk
(11.9)
for k ∈Z+ ((0)(t) = (t)). To see how this works, consider the following special
case k = 1:
(1)(t) = d
dt

 ∞

k=0
1
k!Aktk

= d
dt

I + 1
1!At + 1
2!A2t2 + · · · + 1
k!Aktk + · · ·

TLFeBOOK

THE MATRIX EXPONENTIAL
489
= 1
1!A + 2
2!A2t + · · · + k
k!Aktk−1 + · · ·
= A

I + 1
1!At + · · · +
1
(k −1)!Ak−1tk−1 + · · ·

= AeAt = eAtA.
It is possible to formally verify that the series in (11.8) converges to a matrix
function of t ∈R by working with the Jordan decomposition of A. However,
we will avoid this level of detail. But we will consider the situation where A
is diagonalizable later on.
There is some additional background material needed to more fully appreciate
[8], and we will now consider this. The main result is the Cayley–Hamilton theorem
(Theorem 11.8, below).
Deﬁnition 11.4: Minors and Cofactors
Let A ∈Cn×n. The minor mij is the
determinant of the (n −1) × (n −1) submatrix of A derived from it by deleting
row i and column j. The cofactor cij associated with mij is cij = (−1)i+jmij for
all i, j ∈Zn.
A formula for the inverse of A (assuming this exists) is given by Theorem 11.7.
Theorem 11.7: If A ∈Cn×n is nonsingular, then
A−1 =
1
det(A)adj(A),
(11.10)
where adj(A) (adjoint matrix of A) is the transpose of the matrix of cofactors of
A. Thus, if C = [cij] ∈Cn×n is the matrix of cofactors of A, then adj(A) = CT .
Proof
See Noble and Daniel [9].
Of course, the method suggested by Theorem 11.7 is useful only for the hand
calculation of low-order (small n) problems. Practical matrix inversion must use
ideas from Chapter 4. But Theorem 11.7 is a very useful result for theoretical
purposes, such as obtaining the following theorem.
Theorem 11.8: Cayley–Hamilton Theorem
Any matrix A ∈Cn×n satisﬁes
its own characteristic equation.
Proof
The characteristic polynomial for A is p(λ) = det(λI −A), and can be
written as
p(λ) = λn + a1λn−1 + · · · + an−1λ + an
for suitable constants ak ∈C. The theorem claims that
An + a1An−1 + · · · + an−1A + anI = 0,
(11.11)
TLFeBOOK

490
NUMERICAL METHODS FOR EIGENPROBLEMS
where I is the order n identity matrix. To show (11.11), we consider adj(µI −A)
whose elements are polynomials in µ of a degree that is not greater than n −1,
where µ is not an eigenvalue of A. Hence
adj(µI −A) = M0µn−1 + M1µn−2 + · · · + Mn−2µ + Mn−1
for suitable constant matrices Mk ∈Cn×n. Via Theorem 11.7
(µI −A) adj(µI −A) = det(µI −A)I,
or in expanded form, this becomes
(µI −A)(M0µn−1 + M1µn−2 + · · · + Mn−2µ + Mn−1)
= (µn + a1µn−1 + · · · + an)I.
If we now equate like powers of µ on both sides of this equation, we obtain
M0 = I,
M1 −AM0 = a1I,
M2 −AM1 = a2I,
...
(11.12)
Mn−1 −AMn−2 = an−1I,
−AMn−1 = anI.
Premultiplying3 the jth equation in (11.12) by An−j (j = 0, 1, . . . , n), and then
adding all the equations that result from this, yields
AnM0 + An−1(M1 −AM0) + An−2(M2 −AM1) + · · · + A(Mn−1 −AMn−2)
−AMn−1 = An + a1An−1 + a2An−2 + · · · + an−1An−1 + anI.
But the left-hand side of this is seen to be zero because of cancellation of all the
terms, and (11.11) immediately results.
As an exercise the reader should verify that the matrices in Examples 11.1–11.3,
all satisfy their own characteristic equations.
We will now consider the approach in Leonard [8], who, however, assumes that
the reader is familiar with the theory of solution of nth-order homogeneous linear
ODEs in constant coefﬁcients
x(n)(t) + cn−1x(n−1)(t) + · · · + c1x(1)(t) + c0x(t) = 0,
(11.13)
3This means that we must multiply on the left.
TLFeBOOK

THE MATRIX EXPONENTIAL
491
where the initial conditions are known. In particular, the reader must know that if
λ is a root of the characteristic equation
λn + cn−1λn−1 + · · · + c1λ + c0 = 0,
(11.14)
then if λ has multiplicity m, its contribution to the solution of the initial-value
problem (IVP) (11.13) is of the general form
(a0 + a1t + · · · + am−1tm−1)eλt.
(11.15)
These matters are considered by Derrick and Grossman [10] and Reid [11]. We
shall be combining these facts with the results of Theorem 11.10 (below).
Leonard [8] presents two theorems that relate the solution of (11.13) to the
computation of (t) = eAt.
Theorem 11.9: Leonard I
Let A ∈Rn×n be a constant matrix with charac-
teristic polynomial
p(λ) = det(λI −A) = λn + cn−1λn−1 + · · · + c1λ + c0.
(t) = eAt is the unique solution to the nth-order matrix differential equation
(n)(t) + cn−1(n−1)(t) + · · · + c1(1)(t) + c0(t) = 0
(11.16)
with initial conditions
(0) = I, (1)(0) = A, . . . , (n−2)(0) = An−2, (n−1)(0) = An−1.
(11.17)
Proof
We will demonstrate uniqueness of the solution ﬁrst of all.
Suppose that 1(t) and 2(t) are two solutions to (11.16) for the initial condi-
tions stated in (11.17). Let (t) = 1(t) −2(t) for present purposes, in which
case (t) satisﬁes (11.16) with the initial conditions
(0) = (1)(0) = · · · = (n−2)(0) = (n−1)(0) = 0.
Consequently, each entry of the matrix (t) satisﬁes a scalar IVP of the form
x(n)(t) + cn−1x(n−1)(t) + · · · + c1x(1)(t) + c0x(t) = 0,
x(0) = x(1)(0) = · · · = x(n−2)(0) = x(n−1)(0) = 0,
where the solution is x(t) = 0 for all t, so that (t) = 0 for all t ∈R+. Thus,
1(t) = 2(t), and so the solution must be unique (if it exists).
TLFeBOOK

492
NUMERICAL METHODS FOR EIGENPROBLEMS
Now we conﬁrm that the solution is (t) = eAt (i.e., we conﬁrm existence in
a constructive manner). Let A be a constant matrix of order n with characteristic
polynomial p(λ) as in the theorem statement. If now (t) = eAt, then we recall that
(k)(t) = AkeAt, k = 1, 2, . . . , n
(11.18)
[see (11.9)] so that
(n)(t) + cn−1(n−1)(t) + · · · + c1(1)(t) + c0(t)
= [An + cn−1An−1 + · · · + c1A + c0I]eAt
= p(A)eAt = 0
via Theorem 11.8 (Cayley–Hamilton). From (11.18), we obtain
(0)(0) = I, (1)(0) = A, . . . , (n−2)(0) = An−2, (n−1)(0) = An−1,
and so (t) = eAt is the unique solution to the IVP in the theorem statement.
Theorem 11.10: Leonard II
Let A ∈Rn×n be a constant matrix with char-
acteristic polynomial
p(λ) = λn + cn−1λn−1 + · · · + c1λ + c0,
then
eAt = x0(t)I + x1(t)A + x2(t)A2 + · · · + xn−1(t)An−1,
where xk(t), k ∈Zn are the solutions to the nth-order scalar ODEs
x(n)(t) + cn−1x(n−1)(t) + · · · + c1x(1)(t) + c0x(t) = 0,
satisfying the initial conditions
x(j)
k (0) = δj−k
for j, k ∈Zn (x(0)
k (t) = xk(t)).
Proof
Let constant matrix A have characteristic polynomial p(λ) as in the
theorem statement. Deﬁne
(t) = x0(t)I + x1(t)A + x2(t)A2 + · · · + xn−1(t)An−1,
where xk(t), k ∈Zn are unique solutions to the nth-order scalar ODEs
x(n)(t) + cn−1x(n−1)(t) + · · · + c1x(1)(t) + c0x(t) = 0,
TLFeBOOK

THE MATRIX EXPONENTIAL
493
satisfying the initial conditions stated in the theorem. Thus, for all t ∈R+
(n)(t) + cn−1(n−1)(t) + · · · + c1(1)(t) + c0(t)
=
n−1

k=0
9
x(n)
k (t) + cn−1x(n−1)
k
(t) + · · · + c1x(1)
k (t) + c0xk(t)
:
Ak
= 0 · I + 0 · A + · · · + 0 · An−1 = 0.
As well we see that
(0)
=
x0(0)I + x1(0)A + · · · + xn−1(0)An−1
= I,
(1)(0)
=
x(1)
0 (0)I + x(1)
1 (0)A + · · · + x(1)
n−1(0)An−1
= A,
...
(n−1)(0) = x(n−1)
0
(0)I + x(n−1)
1
(0)A + · · · + x(n−1)
n−1 (0)An−1 = An−1.
Therefore
(t) = x0(t)I + x1(t)A + · · · + xn−1(t)An−1
satisﬁes the IVP
(n)(t) + cn−1(n−1)(t) + · · · + c1(1)(t) + c0(t) = 0
possessing the initial conditions
(k)(0) = Ak
(k ∈Zn). The solution is unique, and so we must conclude that eAt = n−1
k=0 xk(t)Ak
for all t ∈R+, which is the central claim of the theorem.
An example of how to apply the result of Theorem 11.10 is as follows.
Example 11.4
Suppose that
A =
 α
γ
0
β

∈R2×2,
which clearly has the eigenvalues λ = α, β. Begin by assuming distinct eigenvalues
for A, speciﬁcally, that α ̸= β.
The general solution to the second-order homogeneous ODE
x(2)(t) + c1x(1)(t) + c0x(t) = 0
with characteristic roots α, β (eigenvalues of A) is [recall (11.15)]
x(t) = a0eαt + a1eβt.
We have x(1)(t) = a0αeαt + a1βeβt.
TLFeBOOK

494
NUMERICAL METHODS FOR EIGENPROBLEMS
For the initial conditions x(0) = 1, x(1)(0) = 0, we have the linear system
of equations
a0 + a1 = 1,
αa0 + βa1 = 0,
which solve to yield
a0 =
β
β −α , a1 = −
α
β −α .
Thus, the solution in this case is
x0(t) =
1
β −α [βeαt −αeβt].
Now, if instead the initial conditions are x(0) = 0, x(1)(0) = 1, we have the linear
system of equations
a0 + a1 = 0,
αa0 + βa1 = 1,
which solve to yield
a0 = −
1
β −α , a1 =
1
β −α .
Thus, the solution in this case is
x1(t) =
1
β −α [−eαt + eβt].
Via Leonard II we must have
eAt = x0(t)I + x1(t)A
=
1
β −α
 βeαt −αeβt
0
0
βeαt −αeβt

+
1
β −α
 −αeαt + αeβt
−γ eαt + γ eβt
0
−βeαt + βeβt

=
1
β −α
 (β −α)eαt
−γ eαt + γ eβt
0
(β −α)eβt

=
 eαt
γ
β−α(eβt −eαt)
0
eβt

.
(11.19)
Now assume that α = β.
The general solution to
x(2)(t) + c1x(1)(t) + c0x(t) = 0
TLFeBOOK

THE MATRIX EXPONENTIAL
495
with characteristic roots α, α (eigenvalues of A) is [again, recall (11.15)]
x(t) = (a0 + a1t)eαt.
We have x(1)(t) = (a1 + a0α + a1αt)eαt.
For the initial conditions x(0) = 1, x(1)(0) = 0 we have the linear system
of equations
a0 = 1,
a1 + a0α = 0,
which solves to yield a1 = −α, so that
x0(t) = (1 −αt)eαt.
Now if instead the initial conditions are x(0) = 0, x(1)(0) = 1, we have the linear
system of equations
a0 = 0,
a1 + a0α = 1,
which solves to yield a1 = 1 so that
x1(t) = teαt.
If we again apply Leonard II, then we have
eAt = x0(t)I + x1(t)A
=
 eαt
γ teαt
0
eαt

.
(11.20)
A good exercise for the reader is to verify that x(t) = eAtx(0) solves dx(t)/dt =
Ax(t) [of course, here x(t) = [x0(t)x1(t)]T is a state vector] in both of the cases
considered in Example 11.4. Do this by direct substitution.
Example 11.4 is considered in Moler and Van Loan [7] as it illustrates problems
in computing eAt when the eigenvalues of A are nearly multiple. If we consider
(11.19) when β −α is small, and yet is not negligible, the “divided difference”
eβt −eαt
β −α
,
(11.21)
when computed directly, may result in a large relative error. In (11.19) the ratio
(11.21) is multiplied by γ , so the ﬁnal answer may be very inaccurate, indeed.
Matrix A in Example 11.4 is of low order (i.e., n = 2) and is triangular. This type
of problem is very difﬁcult to detect and correct when A is larger and not triangular.
TLFeBOOK

496
NUMERICAL METHODS FOR EIGENPROBLEMS
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
0
5
10
15
20
t
|| eAt ||2
s/m
s
Figure 11.1
An illustration of the hump phenomenon in computing eAt.
Another difﬁculty noted in Moler and Van Loan [7] is sometimes called the hump
phenomenon. It is illustrated in Fig. 11.1 for Eq. (11.19) using the parameters
α = −1.01,
β = −1.00,
γ = 50.
(11.22)
Figure 11.1 is a plot of matrix 2-norm [spectral norm; recall Equation (4.37) with
p = 2 in Chapter 4] ||eAt||2 versus t. (It is a version of Fig. 1 in Ref. 7.) The
problem with this arises from the fact that one way or another some algorithms for
the computation of eAt make use of the identity
eAt = (eAt/m)m.
(11.23)
When s/m is under the hump while s lies beyond it (e.g., in Fig. 11.1 s = 4
with m = 8), we can have
||eAs||2 ≪||eAs/m||m
2 .
(11.24)
Unfortunately, rounding errors in the mth power of eAs/m are usually small only
relative to ||eAs/m||2, rather than ||eAs||2. Thus, rounding errors may be a problem
in using (11.23) to compute eAt.
The Taylor series expansion in (11.8) is not a good method for computing eAt.
The reader should recall the example of catastrophic convergence in the compu-
tation of ex (x ∈R) from Chapter 3 (Appendix 3.C). It is not difﬁcult to imagine
that the problem of catastrophic convergence in (11.8) is likely to be much worse,
and much harder to contain. Indeed this is the case as shown by an example in
Moler and Van Loan [7].
It was suggested earlier in this section that the series in (11.8) can be shown to
converge by considering the diagonalization of A (assuming that A is nondefective).
Suppose that A ∈Cn×n, and that A possesses eigenvalues that are all distinct, and
so we may apply (11.6). Since
Ak = [P P −1]k
= [P P −1][P P −1] · · · [P P −1][P P −1]
#
$%
&
k factors
= P kP −1,
(11.25)
TLFeBOOK

THE MATRIX EXPONENTIAL
497
we have
eAt =
∞

k=0
1
k!Aktk =
∞

k=0
1
k!P kP −1tk = P
 ∞

k=0
1
k!ktk
	
P −1
= P diag
' ∞

k=0
1
k!λk
0tk,
∞

k=0
1
k!λk
1tk, . . . ,
∞

k=0
1
k!λk
n−1tk
(
P −1
= P diag(eλ0t, eλ1t, . . . , eλn−1t)P −1.
If we deﬁne
et = diag(eλ0t, eλ1t, . . . , eλn−1t),
(11.26)
then clearly we can say that
eAt =
∞

k=0
1
k!Aktk = P etP −1.
(11.27)
We know from the theory of Maclaurin series that ex = ∞
k=0
1
k!xk converges for
all x ∈R. Thus, et converges for all t ∈R and is well deﬁned, and hence the
series in (11.8) converges, and so eAt is well deﬁned. Of course, all of this suggests
that eAt may be numerically computed using (11.27). From Chapter 3 we infer that
accurate, reliable means to compute ex (x is a scalar) do exist. Also, reliable
methods exist to ﬁnd the elements of , and this will be considered later. But P
may be close to singular, that is, the condition number κ(P ) (recall Chapter 4) may
be large, and so accurate determination of P −1, which is required by (11.27), may
be difﬁcult. Additionally, the approach (11.27) lacks generality since it won’t work
unless A is nondefective (i.e., can be diagonalized). Matrix factorization methods
to compute eAt (including those in the defective case) are considered in greater
detail in Ref. 7, and this matter will not be mentioned further here.
In Chapter 4 a condition number κ(A) was deﬁned that informed us about the
sensitivity of the solution to Ax = b due to perturbations in A and b. It is possible
to develop a similar notion for the problem of computing eAt. From Golub and
Van Loan [4], the matrix exponential condition number is
ν(A, t) = max
||E||2≤1


 t
0
eA(t−s)EeAs ds


2
||A||2
||eAt||2
.
(11.28)
(The theory behind this originally appeared in Van Loan [12].) In this expression
E ∈Rn×n is a perturbation matrix. The condition number (11.28) measures the
sensitivity of mapping A →eAt for a given t ∈R. For a given t, there is a matrix
E such that
||e(A+E)t −eAt||2
||eAt||2
≈ν(A, t)||E||2
||A||2
.
(11.29)
TLFeBOOK

498
NUMERICAL METHODS FOR EIGENPROBLEMS
We see from this that if ν(A, t) is large, then a small change in A (modeled by the
perturbation matrix E) can cause a large change in eAt. In general, it is not easy
to specify A leading to large values for ν(A, t). However, it is known that
ν(A, t) ≥t||A||2
(11.30)
for t ∈R+ with equality iff A is normal. (Any A ∈Cn×n is normal iff AH A =
AAH .) Thus, it appears that normal matrices are generally the least troublesome
with respect to computing eAt. From the deﬁnition of a normal matrix we see that
real-valued, symmetric matrices are an important special case.
Of the less dubious means to compute eAt, Golub and Van Loan’s algorithm
11.3.1 [4] is suggested. It is based on Pad´e approximation, which is the use of
rational functions to approximate other functions. However, we will only refer the
reader to Ref. 4 (or Ref. 7) for the relevant details. A version of Algorithm 11.3.1
[4] is implemented in the MATLAB expm function, and MATLAB provides other
algorithm implementations for computing eAt.
11.4
THE POWER METHODS
In this section we consider a simple approach to determine the eigenvalues and
eigenvectors of A ∈Rn×n. The approach is iterative. The main result is as follows.
Theorem 11.11: Let A ∈Rn×n be such that
(a) A has n linearly independent eigenvectors x(k), corresponding to the eigen-
pairs {(λk, x(k))|k ∈Zn}.
(b) The eigenvalues satisfy λn−1 ∈R with
|λn−1| > |λn−2| ≥|λn−3| ≥· · · |λ1| ≥|λ0|
(λn−1 is the dominant eigenvalue).
If y0 ∈Rn is a starting vector such that
y0 =
n−1

j=0
ajx(j)
with an−1 ̸= 0, then for yk+1 = Ayk with k ∈Z+
lim
k→∞
Aky0
λk
n−1
= cx(n−1)
(11.31)
for some c ̸= 0, and (recalling that ⟨x, y⟩= xT y = yT x for any x, y ∈Rn)
lim
k→∞
⟨y0, Aky0⟩
⟨y0, Ak−1y0⟩= λn−1.
(11.32)
TLFeBOOK

THE POWER METHODS
499
Proof
We observe that yk = Aky0, and since Akx(j) = λk
jx(j), we have
Aky0 =
n−2

j=0
ajλk
jx(j) + an−1λk
n−1x(n−1),
implying that
Aky0
λk
n−1
= an−1x(n−1) +
n−2

j=0
aj
 λj
λn−1
!k
x(j).
(11.33)
Since |λj| < |λn−1| for j = 0, 1, . . . , n −2, we have limk→∞

 λj
λn−1
k = 0,
and hence
lim
k→∞
Aky0
λk
n−1
= an−1x(n−1).
Now Aky0 = n−1
j=0 ajλk
jx(j), so that
⟨y0, Aky0⟩=
n−1

j=0
ajλk
j⟨y0, x(j)⟩,
and if ηj = ⟨y0, x(j)⟩, then
⟨y0, Aky0⟩
⟨y0, Ak−1y0⟩=
an−1λk
n−1ηn−1 + n−2
j=0 ajλk
jηj
an−1λk−1
n−1ηn−1 + n−2
j=0 ajλk−1
j
ηj
= λn−1


an−1ηn−1 + n−2
j=0 ajηj
 λj
λn−1
!k
an−1ηn−1 + n−2
j=0 ajηj
 λj
λn−1
!k−1

.
Again, since limk→∞

 λj
λn−1
k = 0 (for j = 0, 1, . . . , n −2), we have
lim
k→∞
⟨y0, Aky0⟩
⟨y0, Ak−1y0⟩= λn−1.
We note that
1 >

λn−2
λn−1
 ≥

λn−3
λn−1
 ≥· · · ≥

λ0
λn−1
 ,
so the rate of convergence of Aky0/λk
n−1 to an−1x(n−1) is, according to (11.33),
dominated by the term containing λn−2/λn−1. This is sometimes expressed
TLFeBOOK

500
NUMERICAL METHODS FOR EIGENPROBLEMS
by writing


Aky0
λk
n−1
−an−1x(n−1)

 = O
' λn−2
λn−1
!k(
.
(11.34)
The choice of norm in (11.34) is arbitrary. Of course, a small value for |λn−2/λn−1|
implies faster convergence.
The theory of Theorem 11.11 assumes that A is nondefective. If the algorithm
suggested by this theorem is applied to a defective A, it will attempt to converge.
In effect, for any defective matrix A there is a nondefective matrix close to it, and
so the limiting values in (11.31) and (11.32) will be for a “nearby” nondefective
matrix. However, convergence can be very slow, particularly if the dependent
eigenvectors of A correspond to λn−1, and λn−2. In this situation the results may
not be meaningful.
If y0 is chosen such that an−1 = 0, rounding errors in computing yk+1 = Ayk
will usually give a yk with a component in the direction of the eigenvector x(n−1).
Thus, convergence ultimately is the result. To ensure that this happens, it is best
to select y0 with noninteger components.
From (11.31), if k is big enough, then
yk = Aky0 ≈cλk
n−1x(n−1),
and so yk is an approximation to x(n−1) (to within some scale factor). However,
if |λn−1| > 1, we see that ||yk|| →∞with increasing k, while if |λn−1| < 1, we
see that ||yk|| →0 with increasing k. Either way, serious numerical problems will
certainly result (overﬂow in the former case and rounding errors or underﬂow in the
latter case). This difﬁculty may be eliminated by the proper scaling of the iterates,
and leads to what is sometimes called the scaled power algorithm:
Input N; { upper limit on the number of iterations }
Input y0; { starting vector }
y0 := y0/||y0||2; { normalize y0 to unit norm }
k := 0;
while k < N do begin
zk+1 := Ayk;
yk+1 := zk+1/||zk+1||2;
λ(k+1) := yT
k+1Ayk+1;
k := k + 1;
end;
In this algorithm yk+1 is the (k + 1)th estimate of x(n+1), while λ(k+1) is the
corresponding estimate of eigenvalue λn−1. From the pseudocode above we may
easily see that
yk =
Aky0
||Aky0||2
(11.35)
TLFeBOOK

THE POWER METHODS
501
for all k ≥1. With y0 = n−1
j=0 ajx(j) (aj ∈R such that ||y0||2 = 1)
Aky0 =
n−2

j=0
ajAkx(j) + an−1Akx(n−1) = an−1λk
n−1x(n−1) +
n−2

j=0
ajλk
jx(j)
= an−1λk
n−1


x(n−1) +
n−2

j=0
aj
an−1
 λj
λn−1
!k
x(j)
#
$%
&
=u(k)


,
and as in Theorem 11.11 we see limk→∞||u(k)||2 = 0, and (11.35) becomes
yk =
an−1λk
n−1[x(n−1) + u(k)]
||an−1λk
n−1[x(n−1) + u(k)]||2
= µk
x(n−1) + u(k)
||x(n−1) + u(k)||2
,
(11.36)
where µk is the sign of an−1λk
n−1 (i.e., µk ∈{+1, −1}). Clearly, as k →∞, vector
yk in (11.36) becomes a better and better approximation to eigenvector x(n−1). To
conﬁrm that λ(k+1) estimates λn−1, consider that from (11.36) we have (for k
sufﬁciently large)
λ(k+1) = yT
k+1Ayk+1 ≈(x(n−1))T Ax(n−1)
||x(n−1)||2
2
= λn−1
[recall that Ax(n−1) = λn−1x(n−1)].
If |λn−1| = |λn−2| > |λj| for j = 0, 1, . . . , n −3, we have two dominant eigen-
values. In this situation, as noted in Quarteroni et al. [13], convergence may or may
not occur. If, for example, λn−1 = λn−2, then vector sequence (yk) converges to
a vector in the subspace of Rn spanned by x(n−1), and x(n−2). In this case, since
A ∈Rn×n, we must have λn−1, λn−2 ∈R, and hence for k ≥1, we must have
Aky0 =
n−3

j=0
ajλk
jx(j) + an−2λk
n−2x(n−2) + an−1λk
n−1x(n−1),
implying that
Aky0
λk
n−1
= an−1x(n−1) + an−2x(n−2) +
n−3

j=0
aj
 λj
λn−1
!k
x(j)
so that
lim
k→∞
Aky0
λk
n−1
= an−1x(n−1) + an−2x(n−2),
TLFeBOOK

502
NUMERICAL METHODS FOR EIGENPROBLEMS
which is a vector in a two-dimensional subspace of Rn. On the other hand,
recall Example 11.3, where n = 2 so that λ0 = ejθ, λ1 = e−jθ = λ∗
0. From (11.35)
||yk||2 = 1, so, because A is a rotation operator, yk = Aky0/||Aky0||2 will always
be a point on the unit circle {(x, y)|x2 + y2 = 1}. Convergence does not occur
since it is generally not the same point from one iteration to the next (e.g., consider
θ = π/2 radians).
Example 11.5
Consider an example based on application of the scaled power
algorithm to the matrix
A =


4
1
0
1
4
1
0
1
4

.
This matrix turns out to have the eigenvalues
λ0 = 2.58578644,
λ1 = 4.00000000,
λ2 = 5.41421356
(as may be determined in MATLAB using the eig function). If we deﬁne yk =
[yk,0 yk,1 yk,2]T ∈R3, then, from the algorithm, we obtain the iterates:
k
yk,0
yk,1
yk,2
λ(k)
0
0.57735027
0.57735027
0.57735027
—
1
0.53916387
0.64699664
0.53916387
5.39534884
2
0.51916999
0.67891460
0.51916999
5.40988836
3
0.50925630
0.69376945
0.50925630
5.41322584
4
0.50444312
0.70076692
0.50444312
5.41398821
5
0.50212703
0.70408585
0.50212703
5.41416216
6
0.50101700
0.70566560
0.50101700
5.41420184
7
0.50048597
0.70641885
0.50048597
5.41421089
8
0.50023215
0.70677831
0.50023215
5.41421295
9
0.50011089
0.70694993
0.50011089
5.41421342
10
0.50005296
0.70703187
0.50005296
5.41421353
11
0.50002530
0.70707101
0.50002530
5.41421356
In only 11 iterations the power algorithm has obtained the dominant eigenvalue
to an accuracy of eight decimal places.
Continue to assume that A ∈Rn×n is nondefective. Now deﬁne Aµ = A −µI
(where I is the n × n identity matrix, as usual), and µ ∈R is called the shift
(or shift parameter).4 We will assume that µ always results in the existence of
A−1
µ . Because A is not defective, there will be a nonsingular matrix P such that
P −1AP =  = diag(λ0, λ1, . . . , λn−1) (recall the basic facts from Section 11.2
that justify this). Consequently
Aµ = P P −1 −µI ⇒P −1AµP =  −µI,
(11.37)
4The reason for introducing the shift parameter µ will be made clear a bit later.
TLFeBOOK

THE POWER METHODS
503
and so A−1
µ
has the eigenvalues
γk =
1
λk −µ, k ∈Zn.
(11.38)
P is a similarity transformation that diagonalizes Aµ, giving  −µI, so the eigen-
values of ( −µI)−1 must be the eigenvalues of A−1
µ as these are similar matrices.
A modiﬁcation of the previous scaled power algorithm is the following shifted
inverse power algorithm:
Input N; { upper limit on the number of iterations }
Input y0; { starting vector }
y0 := y0/||y0||2; { normalize y0 to unit norm }
k := 0;
while k < N do begin
Aµzk+1 := yk;
yk+1 := zk+1/||zk+1||2;
γ (k+1) := yT
k+1Ayk+1;
k := k + 1;
end;
Assume that the eigenvalues of A satisfy
|λn−1| ≥|λn−2| ≥· · · ≥|λ1| > |λ0|,
(11.39)
and also that µ = 0, so then |λk| = 1/|γk|, and (11.39) yields
|γ0| > |γ1| ≥|γ2| ≥· · · ≥|γn−2| ≥|γn−1|.
(11.40)
We observe that in the shifted inverse power algorithm Aµzk+1 = yk is equivalent
to zk+1 = A−1
µ yk, and so A−1
µ effectively replaces A in the statement zk+1 := Ayk in
the scaled power algorithm. This implies that the shifted inverse power algorithm
produces vector sequence (yk) that converges to the eigenvector of A−1
µ = A−1
corresponding to the eigenvalue γ0 (= 1/λ0). Since Ax(0) = λ0x(0) implies that
A−1x(0) = 1
λ0 x(0) = γ0x(0), then for a sufﬁciently large k, the vector yk will approx-
imate x(0). The argument to verify this follows the proof of Theorem 11.11.
Therefore, consider the starting vector
y0 = a0x(0) +
n−1

j=1
ajx(j), a0 ̸= 0
(11.41)
(such that ||y0||2 = 1). We see that we must have
A−ky0 = a0
1
λk
0
x(0) +
n−1

j=1
aj
1
λk
j
x(j)
= a0γ k
0 x(0) +
n−1

j=1
ajγ k
j x(j),
(11.42)
TLFeBOOK

504
NUMERICAL METHODS FOR EIGENPROBLEMS
and thus
A−ky0
γ k
0
= a0x(0) +
n−1

j=1
aj
 γj
γ0
!k
x(j).
Immediately, because of (11.40), we must have
lim
k→∞
A−ky0
γ k
0
= a0x(0).
From (11.42), we obtain
A−ky0 = a0γ k
0


x(0) +
n−1

j=1
aj
a0
 γj
γ0
!k
x(j)
#
$%
&
=v(k)


.
(11.43)
In much the same way as we arrived at (11.35), for the shifted inverse power
algorithm, we must have for all k ≥1
yk =
A−k
µ y0
||A−k
µ y0||2
.
(11.44)
From (11.43) for µ = 0, this equation becomes
yk =
a0γ k
0 [x(0) + v(k)]
||a0γ k
0 [x(0) + v(k)]||2
= νk
x(0) + v(k)
||x(0) + v(k)||2
,
(11.45)
where νk ∈{+1, −1}. From the pseudocode for the shifted inverse power algorithm,
if k is large enough, via (11.45), we obtain
γ (k+1) = yT
k+1Ayk+1 ≈(x(0))T Ax(0)
||x(0)||2
2
= λ0.
(11.46)
Thus, γ (k+1) is an approximation to λ0.
In summary, for a nondefective A ∈Rn×n such that (11.39) holds, the shifted
inverse power algorithm will generate a sequence of increasingly better approxi-
mations to the eigenpair (λ0, x(0)), when we set µ = 0.
We note that the scaled power algorithm needs O(n2) ﬂops (recall the deﬁnition
of ﬂops from Chapter 4) at every iteration. This is due mainly to the matrix–vector
product step zk+1 = Ayk. To solve the linear system Aµzk+1 = yk requires O(n3)
ﬂops in general. To save on computations in the implementation of the shifted
inverse power algorithm, it is often best to LU -decompose Aµ (recall Section 4.5)
only once: Aµ = LU. At each iteration LUzk+1 = yk may be solved using forward
TLFeBOOK

THE POWER METHODS
505
and backward substitution, which is efﬁcient since this needs only O(n2) ﬂops at
every iteration. Even so, because of the need to compute the LU decomposition of
Aµ, the shifted inverse power algorithm still needs O(n3) ﬂops overall. It is thus
intrinsically more computation-intensive than is the scaled power algorithm.
However, it is the concept of a shift that makes the shifted inverse power algo-
rithm attractive, at least in some circumstances. But before we consider the reason
for introducing the shift parameter µ, the reader should view the following example.
Example 11.6
Let us reconsider matrix A from Example 11.5. If we apply
the shifted inverse power algorithm using µ = 0 to this matrix, we obtain the
following iterates:
k
yk,0
yk,1
yk,2
γ (k)
0
0.57735027
0.57735027
0.57735027
—
1
0.63960215
0.42640143
0.63960215
5.09090909
2
0.70014004
0.14002801
0.70014004
4.39215686
3
0.69011108
−0.21792981
0.69011108
3.39841689
4
0.62357865
−0.47148630
0.62357865
2.82396484
5
0.56650154
−0.59845803
0.56650154
2.64389042
6
0.53330904
−0.65662999
0.53330904
2.59925317
7
0.51623508
−0.68337595
0.51623508
2.58886945
8
0.50782504
−0.69586455
0.50782504
2.58649025
9
0.50375306
−0.70175901
0.50375306
2.58594700
10
0.50179601
−0.70455768
0.50179601
2.58582306
11
0.50085857
−0.70589049
0.50085857
2.58579479
12
0.50041023
−0.70652615
0.50041023
2.58578834
13
0.50019597
−0.70682953
0.50019597
2.58578687
14
0.50009360
−0.70697438
0.50009360
2.58578654
15
0.50004471
−0.70704355
0.50004471
2.58578646
16
0.50002135
−0.70707658
0.50002135
2.58578644
We see that the method converges to an estimate of λ0 (smallest eigenvalue of
A) that is accurate to eight decimal places in only 16 iterations.
As an exercise, the reader should conﬁrm that the vector yk in the bottom row
of the table above is an estimate of the eigenvector for λ0. The reader should do
the same for Example 11.5.
Recall again that A−1
µ
is assumed to exist (so that µ is not an eigenvalue of A).
Observe that Ax(j) = λjx(j), so that (A −µI)x(j) = (λj −µ)x(j), and therefore
A−1
µ x(j) = γjx(j). Suppose that there is an m ∈Zn such that
|λm −µ| < |λj −µ|
(11.47)
for all j ∈Zn, but that j ̸= m; that is, λm is closest to µ of all the eigenvalues of
A. This really says that λm has a multiplicity of one (i.e., is simple). Now consider
TLFeBOOK

506
NUMERICAL METHODS FOR EIGENPROBLEMS
the starting vector
y0 =
n−1

j=0
j̸=m
ajx(j) + amx(m)
(11.48)
with am ̸= 0, and ||y0||2 = 1. Clearly
A−k
µ y0 =
n−1

j=0
j̸=m
ajγ k
j x(j) + amγ k
mx(m),
implying that
A−k
µ y0
γ km
= amx(m) +
n−1

j=0
j̸=m
aj
 γj
γm
!k
x(j).
(11.49)
Now via (11.47)

γj
γm
 =

λm −µ
λj −µ
 < 1.
Therefore, via (11.49)
lim
k→∞
A−k
µ y0
γ km
= amx(m).
This implies that the vector sequence (yk) in the shifted inverse power algorithm
converges to x(m). Put simply, by the proper selection of the shift parameter µ, we
can extract just about any eigenpair of A that we wish to (as long as λm is simple).
Thus, in this sense, the shifted inverse power algorithm is more general than the
scaled power algorithm. The following example illustrates another important point.
Example 11.7
Once again we apply the shifted inverse power algorithm to
matrix A from Example 11.5. However, now we select µ = 2. The resulting
sequence of iterates for this case is as follows:
k
yk,0
yk,1
yk,2
γ (k)
0
0.57735027
0.57735027
0.57735027
—
1
0.70710678
0.00000000
0.70710678
4.00000000
2
0.57735027
−0.57735027
0.57735027
2.66666667
3
0.51449576
−0.68599434
0.51449576
2.58823529
4
0.50251891
−0.70352647
0.50251891
2.58585859
5
0.50043309
−0.70649377
0.50043309
2.58578856
6
0.50007433
−0.70700164
0.50007433
2.58578650
7
0.50001275
−0.70708874
0.50001275
2.58578644
TLFeBOOK

THE POWER METHODS
507
We see that convergence to the smallest eigenvalue of A has now occurred in
only seven iterations, which is faster than the case considered in Example 11.6 (for
which we used µ = 0).
We see that this example illustrates the fact that a properly chosen shift param-
eter can greatly accelerate the convergence of iterative eigenproblem solvers. This
notion of shifting to improve convergence rates is also important in practical imple-
mentations of QR iteration methods for solving eigenproblems (next section).
So far our methods extract only one eigenvalue from A at a time. One may
apply a method called deﬂation to extract all the eigenvalues of A under certain
conditions. Begin by noting the following elementary result.
Lemma 11.1: Suppose that B ∈R(n−1)×(n−1), and that B−1 exists, and that
r ∈Rn−1, then

1
rT
0
B
−1
=
 1
−rT B−1
0
B−1

.
(11.50)
Proof
Exercise.
The deﬂation procedure is based on the following theorem.
Theorem 11.12: Deﬂation
Suppose that An ∈Rn×n, that eigenvalue λi ∈R
for all i ∈Zn, and that all the eigenvalues of An are distinct. The dominant eigenpair
of An is (λn−1, x(n−1)), and we assume that ||x(n−1)||2 = 1. Suppose that Qn ∈
Rn×n is an orthogonal matrix such that Qnx(n−1) = [1 0 · · · 0 0]T = e0; then
QnAnQT
n =

λn−1
aT
n−1
0
An−1
	
.
(11.51)
Proof
Qn exists because it can be a Householder transformation matrix (recall
Section 4.6). Any eigenvector x(k) of An can always be normalized so that
||x(k)||2 = 1.
Following (11.5), we have
An [x(n−1)x(n−2) · · · x(1)x(0)]
#
$%
&
=Tn
= [x(n−1)x(n−2) · · · x(1)x(0)]
#
$%
&
=Tn


λn−1
0
· · ·
0
0
0
λn−2
· · ·
0
0
...
...
...
...
0
0
· · ·
λ1
0
0
0
· · ·
0
λ0


#
$%
&
=Dn
,
TLFeBOOK

508
NUMERICAL METHODS FOR EIGENPROBLEMS
that is, AnTn = TnDn. Thus, (QT
n = Q−1
n
via orthogonality)
QnAnQT
n = QnTnDnT −1
n QT
n = (QnTn)Dn(QnTn)−1.
(11.52)
Now
QnTn = [e0
Qnx(n−2) · · · Qnx(1)Qnx(0)] =

1
bT
n−1
0
Bn−1
	
,
and via Lemma 11.1, we have

1
bT
n−1
0
Bn−1
	−1
=

1
−bT
n−1B−1
n−1
0
B−1
n−1
	
.
Thus, (11.52) becomes
QnAnQT
n =

1
bT
n−1
0
Bn−1
	 
λn−1
0
0
Dn−1
	 
1
−bT
n−1B−1
n−1
0
B−1
n−1
	
=

λn−1
bT
n−1(Dn−1 −λn−1In−1)B−1
n−1
0
Bn−1Dn−1B−1
n−1
	
,
which has the form given in (11.51).
From Theorem 11.6, QnAnQT
n and An are similar matrices, and so have the
same eigenvalues. Via (11.51), An−1 has the same eigenvalues as An, except
for λn−1. Clearly, the scaled power method could be used to ﬁnd the eigenpair
(λn−1, x(n−1)). The Householder procedure from Section 4.6 gives Qn. From The-
orem 11.12 we obtain An−1, and the deﬂation procedure may be repeated to ﬁnd
all the remaining eigenvalues of A = An.
It is important to note that the deﬂation procedure may be improved with respect
to computational efﬁciency by employing instead the Rayleigh quotient iteration
method. This replaces the power methods we have considered so far. This approach
is suggested and considered in detail in Golub and Van Loan [4] and Epperson [14];
we omit the details here.
11.5
QR ITERATIONS
The power methods of Section 11.4 and variations thereof such as Rayleigh quotient
iterations are deﬁcient in that they are not computationally efﬁcient methods for
computing all possible eigenpairs. The power methods are really at their best when
we seek only a few eigenpairs (usually corresponding to either the smallest or
the largest eigenvalues). In Section 11.4 the power methods were applied only to
TLFeBOOK

QR ITERATIONS
509
computing real-valued eigenvalues, but it is noteworthy that power methods can
be adapted to ﬁnding complex–conjugate eigenvalue pairs [19].
The QR iterations algorithms are, according to Watkins [15], due originally to
Francis [16] and Kublanovskaya [17]. The methodology involved in QR iterations
is based, in turn, on earlier work of H. Rutishauser performed in the 1950s. The
detailed theory and rationale for the QR iterations are not by any means straight-
forward, and even the geometric arguments in Ref. 15 (based, in turn, on the work
of Parlett and Poole [18]) are not easy to follow. However, for matrices A ∈Cn×n
that are dense (i.e., nonsparse; recall Section 4.7), that are not too large, and that
are nondefective, the QR iterations are the best approach presently known for ﬁnd-
ing all possible eigenpairs of A. Indeed, the MATLAB eig function implements a
modern version of the QR iteration methodology.5
Because of the highly involved nature of the QR iteration theory, we will only
present a few of the main ideas here. Other than the references cited so far, the
reader is referred to the literature [4,6,13,19] for more thorough discussions. Of
course, these are not the only references available on this subject.
Eigenvalue computations such as the QR iterations reduce large problems into
smaller problems. Golub and Van Loan [4] present two lemmas that are involved
in this reduction approach. Recall from Section 4.7 that s(A) denotes the set of all
the eigenvalues of matrix A (and is also called the spectrum of A).
Lemma 11.2: If A ∈Cn×n is of the form
A =
 A00
A01
0
A11

,
where A00 ∈Cp×p, A01 ∈Cp×q, A11 ∈Cq×q (q + p = n), then s(A) = s(A00) ∪
s(A11).
Proof
Consider
Ax =
 A00
A01
0
A11
  x1
x2

= λ
 x1
x2

(x1 ∈Cp and x2 ∈Cq). If x2 ̸= 0, then A11x2 = λx2, and so we conclude that
λ ∈s(A11). On the other hand, if x2 = 0, then A00x1 = λx1, so we must have
λ ∈s(A00). Thus, s(A) ∈s(A00) ∪s(A11). Sets s(A) and s(A00) ∪s(A11) have
the same cardinality (i.e., the same number of elements), and so s(A) = s(A00) ∪
s(A11).
5If A ∈Cn×n, then [V, D] = eig(A) such that
A = V DV −1,
where D ∈Cn×n is the diagonal matrix of eigenvalues of A and V ∈Cn×n is the matrix whose columns
are the corresponding eigenvectors. The eigenvectors in V are “normalized” so that each eigenvector
has a 2-norm of unity.
TLFeBOOK

510
NUMERICAL METHODS FOR EIGENPROBLEMS
Essentially, Lemma 11.2 states that if A is block upper triangular, the eigen-
values lie within the diagonal blocks.
Lemma 11.3: If A ∈Cn×n, B ∈Cp×p, X ∈Cn×p (with p ≤n) satisfy
AX = XB, rank(X) = p,
(11.53)
then there is a unitary Q ∈Cn×n (so Q−1 = QH ) such that
QHAQ = T =

T00
T01
0
T11
	
,
(11.54)
where
T00 ∈Cp×p,
T01 ∈Cp×(n−p),
T11 ∈C(n−p)×(n−p),
and
s(T00) =
s(A) ∩s(B).
Proof
The QR decomposition idea from Section 4.6 generalizes to any X ∈
Cn×p with p ≤n and rank(X) = p; that is, complex-valued Householder matrices
are available. Thus, there is a unitary matrix Q ∈Cn×n such that
X = Q
 R
0

,
where R ∈Cp×p. Substituting this into (11.53) yields

T00
T01
T10
T11
	  R
0

=
 R
0

B,
(11.55)
where
QHAQ =

T00
T01
T10
T11
	
.
From (11.55) T10R = 0, implying that T10 = 0 [yielding (11.54)], and also T00R =
RB, implying that B = R−1T00R (R−1 exists because X is full-rank). T00 and
B are similar matrices so s(B) = s(T00). From Lemma 11.2 we have s(A) =
s(T00) ∪s(T11). Thus, s(A) = s(B) ∪s(T11). From basic properties regarding sets
(distributive laws)
s(T00) ∩s(A) = s(T00) ∩[s(B) ∪s(T11)]
= [s(T00) ∩s(B)] ∪[s(T00) ∩s(T11)]
= s(T00) ∩∅,
implying that s(T00) = s(T00) ∩s(A) = s(A) ∩s(B). This statement [s(T00) =
s(A) ∩s(B)] really says that the eigenvalues of B are a subset of those of A.
TLFeBOOK

QR ITERATIONS
511
Recall that a subspace of vector space Cn is a subset of Cn that is also a vector
space. Suppose that we have the vectors x0, . . . , xm−1 ∈Cn; then we may deﬁne
the spanning set as
span(x0, . . . , xm−1) =



m−1

j=0
ajxj|aj ∈C


.
(11.56)
In particular, if S = span(x), where x is an eigenvector of A, then
y ∈S ⇒Ay ∈S,
and so S is invariant for A, or invariant to the action of A. It is a subspace
(eigenspace) of Cn that is invariant to A. Lemmas 11.2 and 11.3 can be used to
establish the following important decomposition theorem (Theorem 7.4.1 in Ref.
4). We emphasize that it is for real-valued A only.
Theorem 11.13: Real Schur Decomposition
If A ∈Rn×n, then there is an
orthogonal matrix Q ∈Rn×n such that
QT AQ =


R00
R01
· · ·
R0,m−1
0
R11
· · ·
R1,m−1
...
...
...
0
0
· · ·
Rm−1,m−1

= R,
(11.57)
where each Ri,i is either 1 × 1 or 2 × 2. In the latter case Ri,i will have a complex–
conjugate pair of eigenvalues.
Proof
The matrix A ∈Rn×n, so det(λI −A) has real-valued coefﬁcients, and
so complex eigenvalues of A always occur in conjugate pairs (recall Section 11.2).
Let k be the number of complex–conjugate eigenvalue pairs in s(A). We will
employ mathematical induction on k.
The theorem certainly holds for k = 0 via Lemmas 11.2 and 11.3 since real-
valued matrices are only a special case. Now we assume that k ≥1 (i.e., A possesses
at least one conjugate pair of eigenvalues). Suppose that an eigenvalue is λ =
α + jβ ∈s(A) with β ̸= 0. There must be vectors x, y ∈Rn (with y ̸= 0) such that
A(x + jy) = (α + jβ)(x + jy),
or equivalently
A[xy] = [xy]

α
β
−β
α

.
(11.58)
Since β ̸= 0, vectors x and y span a two-dimensional subspace of Rn that is
invariant to the action of A because of (11.58). From Lemma 11.3 there is an
TLFeBOOK

512
NUMERICAL METHODS FOR EIGENPROBLEMS
orthogonal matrix U1 ∈Rn×n such that
UT
1 AU1 =

T00
T01
0
T11
	
,
where T00 ∈R2×2, T01 ∈R2×(n−2), T11 ∈R(n−2)×(n−2), and s(T00) = {λ, λ∗}. By
induction there is another orthogonal matrix U2 such that UT
2 T11U2 has the neces-
sary structure. Equation (11.57) then follows by letting
Q = U1
 I2
0
0
U2

,
where I2 is the 2 × 2 identity matrix. Of course, this process may be repeated as
often as needed.
A method that reliably gives us the blocks Ri,i for all i in (11.57) therefore gives
us all the eigenvalues of A since Ri,i is only 1 × 1, or 2 × 2, making its eigenvalues
easy to ﬁnd in any case. The elements in the ﬁrst subdiagonal of QT AQ of (11.57)
are not necessarily zero-valued (again because Ri,i might be 2 × 2), so we say that
QT AQ is upper quasi-triangular.
Deﬁnition 11.5: Hessenberg Form
Matrix A ∈Cn×n is in Hessenberg form
if ai,j = 0 for all i, j such that i −j > 1.
Technically, A in this deﬁnition is upper Hessenberg. Matrix A in Example 4.4
is Hessenberg. All upper triangular matrices are Hessenberg. The quasi-triangular
matrix QT AQ in Theorem 11.13 is Hessenberg.
A pseudocode for the basic QR iterations algorithm is
Input N; { Upper limit on the number of iterations }
Input A ∈Rn×n; { Matrix we want to eigendecompose }
H0 := QT
0AQ0; { Reduce A to Hessenberg form }
k := 1;
while k ≤N do begin
Hk−1 := QkRk; { QR-decomposition step }
Hk := RkQk;
k := k + 1;
end;
In this algorithm we emphasize that A is assumed to be real-valued. Generalization
to the complex case is possible but omitted. The statement H0 = QT
0 AQ0 generally
involves applying orthogonal transformation Q0 ∈Rn×n to reduce A to Hessenberg
matrix H0, although in principle this is not necessary. However, there are major
advantages (discussed below) to reducing A to Hessenberg form as a ﬁrst step.
The basis for this initial reduction step is the following theorem, which proves that
such a step is always possible.
TLFeBOOK

QR ITERATIONS
513
Theorem 11.14: Hessenberg Reduction
If A ∈Rn×n, there is an orthogonal
matrix Q ∈Rn×n such that QT AQ = H is Hessenberg.
Proof
From Section 4.6 in general there is an orthogonal matrix P such that
P x = ||x||2e0 (e.g., P is a Householder matrix), where e0 = [1 0 · · · 0 0]T ∈Rn
if x ∈Rn.
Partition A according to
A(0) = A =

a(0)
00
bT
0
a0
A(0)
11
	
,
where a(0)
00 ∈R and a0, b0 ∈Rn−1, A(0)
11 ∈R(n−1)×(n−1). Let P1 be orthogonal such
that P1a0 = ||a0||2e0 (e0 ∈Rn−1). Deﬁne
Q1 =
 1
0
0
P1

,
and clearly Q−1
1
= QT
1 (i.e., Q1 is also orthogonal). Thus
A(1) = Q1A(0)QT
1 =

a(0)
00
bT
0 P T
1
||a0||2e0
P1A(0)
11 P T
1
	
.
The ﬁrst column of A(1) satisﬁes the Hessenberg condition since it is
[a(0)
00 ||a0||2
0 · · · 0
# $% &
n−2 zeros
]T . The process may be repeated again by partitioning A(1)
according to
A(1) =

A(1)
00
bT
1
[0
a1]
A(1)
11
	
,
where A(1)
00 ∈R2×2, and [0
a1], b1 ∈R(n−2)×2, A(1)
11 ∈R(n−2)×(n−2). Let P2 be
orthogonal such that P2a1 = ||a1||2e0 (e0 ∈Rn−2). Deﬁne
Q2 =
 I2
0
0
P2

,
where I2 is the 2 × 2 identity matrix. Thus
A(2) = Q2A(1)QT
2 = Q2Q1A(0)QT
1 QT
2
=

A(1)
00
bT
1 P T
2
[0
||a1||2e0]
P2A(1)
11 P T
2
	
,
TLFeBOOK

514
NUMERICAL METHODS FOR EIGENPROBLEMS
and the ﬁrst two columns of A(2) satisfy the Hessenberg condition. Of course, we
may continue in this fashion, ﬁnally yielding
A(n−2) = Qn−2 · · · Q2Q1AQT
1 QT
2 · · · QT
n−2,
which is Hessenberg. We may deﬁne QT = Qn−2 · · · Q2Q1 and H = A(n−2),
which is the claim made in the theorem statement.
Thus, Theorem 11.14 contains a prescription for ﬁnding H0 = QT
0 A0Q0 as well
as a simple proof of existence of the decomposition. Hessenberg reduction is done
to facilitate reducing the amount of computation per iteration. Clearly, A and H0
are similar matrices and so possess the same eigenvalues.
From the pseudocode for k = 1, 2, . . . , N, we obtain
Hk−1 = QkRk,
Hk = RkQk,
which yields
HN = QT
N · · · QT
2 QT
1 H0Q1Q2 · · · QN,
(11.59)
so therefore
HN = QT
N · · · QT
2 QT
1 QT
0 AQ0Q1Q2 · · · QN.
(11.60)
Matrices HN and A are similar, and so have the same eigenvalues for any N. It is
important to note that if Qk is constructed properly then Hk is Hessenberg for all
k. As explained in Golub and Van Loan [4, Section 7.4.2], the use of orthogonal
matrices Qk based on the 2 × 2 rotation operator (matrix A from Example 11.3)
is recommended. These orthogonal matrices are called Givens matrices, or Givens
rotations. The result is an algorithm that needs only O(n2) ﬂops per iteration
instead of O(n3) ﬂops. Overall computational complexity is still O(n3) ﬂops, due
to the initial Hessenberg reduction step. It is to be noted that the rounding error
performance of the suggested algorithm is quite good [19].
We have already noted the desirability of the real Schur decomposition of A
into R according to QT AQ = R in (11.57). In fact, with proper attention to details
(many of which cannot be considered here), the QR iterations method is an excellent
means to ﬁnd Q and R as in any valid matrix norm
lim
N→∞HN = R
(11.61)
and
∞
/
i=0
Qi = Q
(11.62)
TLFeBOOK

QR ITERATIONS
515
(of course, 0N
i=0 Qi = Q0Q1 · · · QN; the ordering of factors in the product is
important since Qi is a matrix for all i). The formal proof of this is rather difﬁcult,
and so it is omitted.
Suppose that we have
A =


0
0
0
· · ·
0
−an
1
0
0
· · ·
0
−an−1
0
1
0
· · ·
0
−an−2
...
...
...
...
...
0
0
0
· · ·
0
−a2
0
0
0
· · ·
1
−a1


∈Rn×n.
(11.63)
It can be shown that
p(λ) = det(λI −A) = λn + a1λn−1 + a2λn−2 + · · · + an−1λ + an.
(11.64)
Matrix A is called a companion matrix. We see that it is easy to obtain (11.64) from
(11.63), or vice versa. We also see that A is Hessenberg. Because of (11.61), we
may conceivably input A of (11.63) into the basic QR iterations algorithm (omitting
the initial Hessenberg reduction step), and so determine the roots of p(λ) = 0 as
these are the eigenvalues of A. Since p(λ) is essentially arbitrary, except that it
should not yield a defective A, we have an algorithm to solve the polynomial zero-
ﬁnding problem that was mentioned in Chapter 7. Unfortunately, it has been noted
[4,19] that this is not necessarily a stable method for ﬁnding polynomial zeros.
Example 11.8
Suppose that
p(λ) = (λ2 −2λ + 2)(λ2 −
√
2λ + 1)
= λ4 −(2 +
√
2)λ3 + (2
√
2 + 3)λ2 −2(1 +
√
2)λ + 2,
which has zeros for
λ ∈

1 ± j, 1
√
2
± 1
√
2
j

.
After 50 iterations of the basic QR iterations algorithm, we obtain
H50 =


0.5000
−6.0355
0.9239
5.3848
0.2071
1.5000
−0.3827
−2.2304
0.0000
0.0000
0.7071
−0.7071
0.0000
0.0000
0.7071
0.7071

=

R0,0
R0,1
0
R1,1
	
,
where Ri,j ∈R2×2. The reader may wish to conﬁrm that R0,0 has the eigenvalues
1 ± j, and that R1,1 has the eigenvalues
1
√
2(1 ± j).
TLFeBOOK

516
NUMERICAL METHODS FOR EIGENPROBLEMS
On the other hand, for
p(λ) = (λ + 1)(λ2 −2λ + 2)(λ2 −
√
2λ + 1),
which is a slight modiﬁcation of the previous example, the basic QR iterations
algorithm will fail to converge.
The following point is also important. In (11.60), deﬁne Q = Q0Q1 · · · QN,
so that HN = QT AQ. Now suppose that A = AT . Clearly, H T
N = QT AT Q =
QT AQ = HN. This implies that HN will be tridiagonal (deﬁned in Section 6.5)
for a real and symmetric A. Because of (11.61), we must now have
lim
N→∞HN = diag(R0,0, R1,1, . . . , Rm−1,m−1) = D,
where each Ri,i is 1 × 1. Thus, D is the diagonal matrix of eigenvalues of A. Also,
we must have 0∞
i=0 Qi as the corresponding matrix of eigenvectors of A.
Example 11.9
Suppose that A is the matrix from Example 11.5:
A =


4
1
0
1
4
1
0
1
4

.
We see that A is in Hessenberg form already. After 36 iterations of the basic QR
iteration algorithm, we obtain
H36 =


5.4142
0.0000
0.0000
0.0000
4.0000
0.0000
0.0000
0.0000
2.5858

,
so the matrix is diagonal and reveals all eigenvalues of A. Additionally, we have
36
/
i=0
Qi =


0.5000
−0.7071
0.5000
0.7071
0.0000
−0.7071
0.5000
0.7071
0.5000

,
which is a good approximation to the eigenvectors of A.
We noted in Section 11.4 that shifting can be used to accelerate convergence in
power methods. Similarly, shifting can be employed in QR iterations to achieve
the same result. Indeed, all modern implementations of QR iterations incorporate
some form of shifting for this reason. The previous basic QR iteration algorithm
may be modiﬁed to incorporate the shift parameter µ ∈R. The overall structure of
the result is described by the following pseudocode:
TLFeBOOK

QR ITERATIONS
517
Input N; { Upper limit on the number of iterations }
Input A ∈Rn×n; { Matrix we want to eigendecompose }
H0 := QT
0AQ0; { Reduce A to Hessenberg form }
k := 1;
while k ≤N do begin
Determine the shift parameter µ ∈R;
Hk−1 −µI := QkRk; { QR-decomposition step }
Hk := RkQk + µI;
k := k + 1;
end;
The reader may readily conﬁrm that we still have
HN = QT
N · · · QT
1 QT
0 AQ0Q1 · · · QN
just as we had in (11.60) for the basic QR iteration algorithm. Thus, we again ﬁnd
that Hk is similar to A for all k. Perhaps the simplest means to generate µ is the
single-shift QR iterations algorithm:
Input N; { Upper limit on the number of iterations }
Input A ∈Rn×n; { Matrix we want to eigendecompose }
H0 := QT
0AQ0; { Reduce A to Hessenberg form }
k := 1;
while k ≤N do begin
µk := Hk−1(n −1, n −1); { µk is the lower right corner element of Hk−1 }
Hk−1 −µkI := QkRk; { QR-decomposition step }
Hk := RkQk + µkI;
k := k + 1;
end;
We note that µ is not ﬁxed in general from one iteration to the next. Basically, µ
varies from iteration to iteration in order to account for new information about s(A)
as the subdiagonal entries of Hk converge to zero. We will avoid the technicalities
involved in a full justiﬁcation of this approach except to mention that it is ﬂawed,
and that more sophisticated shifting methods are needed for an acceptable algorithm
(e.g., the double shift [4,19]). However, the following example shows that shifting
in this way does speed convergence.
Example 11.10
If we apply the single-shift QR iterations algorithm to A in
Example 11.9, we obtain the following matrix in only one iteration:
H1 =


4.0000
−1.4142
0.0000
−1.4142
4.0000
0.0000
0.0000
0.0000
4.0000

.
This matrix certainly does not have the structure of H36 in Example 11.9, but the
eigenvalues of the submatrix

4.0000
−1.4142
−1.4142
4.0000

are 5.4142, 2.5858.
TLFeBOOK

518
NUMERICAL METHODS FOR EIGENPROBLEMS
Finally, we mention that our pseudocodes assume a user-speciﬁed number of
iterations N. This is not convenient, and is inefﬁcient in practice. Criteria to auto-
matically terminate the QR iterations without user intervention are available, but a
discussion of this matter is beyond our scope.
REFERENCES
1. D. R. Hill, Experiments in Computational Matrix Algebra (C. B. Moler, consulting ed.),
Random House, New York, 1988.
2. P. Halmos, Finite Dimensional Vector Spaces, Van Nostrand, New York, 1958.
3. R. A. Horn and C. R. Johnson, Matrix Analysis, Cambridge Univ. Press, Cambridge,
UK, 1985.
4. G. H. Golub and C. F. Van Loan, Matrix Computations, 2nd ed., Johns Hopkins Univ.
Press, Baltimore, MD, 1989.
5. F. W. Fairman, “On Using Singular Value Decomposition to Obtain Irreducible Jordan
Realizations,” in Linear Circuits, Systems and Signal Processing: Theory and Appli-
cation, C. I. Byrnes, C. F. Martin, and R. E. Saeks, eds., North-Holland, Amsterdam,
1988, pp. 35–40.
6. G. Stewart, Introduction to Matrix Computations, Academic Press, New York, 1973.
7. C. Moler and C. Van Loan, “Nineteen Dubious Ways to Compute the Exponential of a
Matrix,” SIAM Rev. 20, 801–836 (Oct. 1978).
8. I. E. Leonard, “The Matrix Exponential,” SIAM Rev. 38, 507–512 (Sept. 1996).
9. B. Noble and J. W. Daniel, Applied Linear Algebra, Prentice-Hall, Englewood Cliffs,
NJ, 1977.
10. W. R. Derrick and S. I. Grossman, Elementary Differential Equations with Applications,
2nd ed., Addison-Wesley, Reading, MA, 1981.
11. W. T. Reid, Ordinary Differential Equations, Wiley, New York, 1971.
12. C. Van Loan, “The Sensitivity of the Matrix Exponential,” SIAM J. Numer. Anal. 14,
971–981 (Dec. 1977).
13. A. Quarteroni, R. Sacco, and F. Saleri, Numerical Mathematics (Texts in Applied Math-
ematics series, Vol. 37). Springer-Verlag, New York, 2000.
14. J. F. Epperson, An Introduction to Numerical Methods and Analysis, Wiley, New
York, 2002.
15. D. S. Watkins, “Understanding the QR Algorithm,” SIAM Rev. 24, 427–440 (Oct.
1982).
16. J. G. F. Francis, “The QR Transformation: A Unitary Analogue to the LR Transforma-
tions, Parts I and II,” Comput. J. 4, 265–272, 332–345 (1961).
17. V. N. Kublanovskaya, “On Some Algorithms for the Solution of the Complete Eigen-
value Problem,” USSR Comput. Math. Phys. 3, 637–657, (1961).
18. B. N. Parlett and W. G. Poole, Jr., “A Geometric Theory for the QR, LU, and Power
Iterations,” SIAM J. Numer. Anal. 10, 389–412 (1973).
19. J. H. Wilkinson, The Algebraic Eigenvalue Problem, Clarendon Press, Oxford, UK,
1965.
TLFeBOOK

PROBLEMS
519
PROBLEMS
11.1. Aided with at most a pocket calculator, ﬁnd all the eigenvalues and eigen-
vectors of the following matrices:
(a) A =
 4
1
1
4

.
(b) B =


0
0
−2
1
0
1
0
1
2

.
(c) C =


0
1
1
4
0
0
−1
4
1
0
1

.
(d) D =

0
1 −j
−j
2

.
11.2. A conic section in R2 is described in general by
αx2
0 + 2βx0x1 + γ x2
1 + δx0 + ϵx1 + ρ = 0.
(11.P.1)
(a) Show that (11.P.1) can be rewritten as a quadratic form:
xT Ax + gT x + ρ = 0,
(11.P.2)
where x = [x0
x1]T , and A = AT .
(b) For a conic section in standard form A is diagonal. Suppose that A is
diagonal. State the conditions on the diagonal elements that result in
(11.P.2) describing an ellipse, a parabola, and a hyperbola.
(c) Suppose that (11.P.2) is not in standard form (i.e., A is not a diagonal
matrix). Explain how similarity transformations might be used to place
(11.P.2) in standard form.
11.3. Consider the companion matrix
C =


0
0
0
· · ·
0
−cn
1
0
0
· · ·
0
−cn−1
0
1
0
· · ·
0
−cn−2
...
...
...
...
...
0
0
0
· · ·
0
−c2
0
0
0
· · ·
1
−c1


∈Cn×n.
TLFeBOOK

520
NUMERICAL METHODS FOR EIGENPROBLEMS
Prove that
pn(λ) = det(λI −C) = λn + c1λn−1 + c2λn−2 + · · · + cn−1λ + cn.
11.4. Suppose that the eigenvalues of A ∈Cn×n are λ0, λ1, . . . , λn−1. Find the
eigenvalues of A + αI, where I is the order n identity matrix and α ∈C
is a constant.
11.5. Suppose that A ∈Rn×n is orthogonal (i.e., A−1 = AT ); then, if λ is an
eigenvalue of A, show that we must have |λ| = 1.
11.6. Find all the eigenvalues of
A =


cos θ
0
−sin θ
0
0
cos φ
0
−sin φ
sin θ
0
cos θ
0
0
sin φ
0
cos φ

.
(Hint: The problem is simpliﬁed by using permutation matrices.)
11.7. Consider the following deﬁnition: A, B ∈Cn×n are simultaneously diago-
nalizable if there is a similarity matrix S ∈Cn×n such that S−1AS, and
S−1BS are both diagonal matrices. Show that if A, B ∈Cn×n are simulta-
neously diagonalizable, then they commute (i.e., AB = BA).
11.8. Prove the following theorem. Let A, B ∈Cn×n be diagonalizable. There-
fore, A and B commute iff they are simultaneously diagonalizable.
11.9. Matrix A ∈Cn×n is a square root of B ∈Cn×n if A2 = B. Show that every
diagonalizable matrix in Cn×n has a square root.
11.10. Prove the following (Bauer–Fike) theorem (which says something about
how perturbations of matrices affect their eigenvalues). If γ is an eigenvalue
of A + E ∈Rn×n, and T −1AT = D = diag(λ0, λ1, . . . , λn−1), then
min
λ∈s(A) |λ −γ | ≤κ2(T )||E||2.
Recall that s(A) denotes the set of eigenvalues of A (i.e., the spectrum of
A). [Hint: If γ ∈s(A), the result is certainly true, so we need consider
only the situation where γ /∈s(A). Conﬁrm that if T −1(A + E −γ I)T
is singular, then so is I + (D −γ I)−1(T −1ET ). Note that if for some
B ∈Rn×n the matrix I + B is singular, then (I + B)x = 0 for some x ∈Rn
that is nonzero, so ||x||2 = ||Bx||2, and so ||B||2 ≥1. Consider upper and
lower bounds on the norm ||(D −γ I)−1(T −1ET )||2.]
11.11. The Daubechies 4-tap scaling function φ(t) satisﬁes the two-scale difference
equation
φ(t) = p0φ(2t) + p1φ(2t −1) + p2φ(2t −2) + p3φ(2t −3),
(11.P.3)
TLFeBOOK

PROBLEMS
521
where suppφ(t) = [0, 3] ⊂R [i.e., φ(t) is nonzero only on the interval
[0, 3]], and where
p0 = 1
4(1 +
√
3),
p1 = 1
4(3 +
√
3),
p2 = 1
4(3 −
√
3),
p3 = 1
4(1 −
√
3).
Note that the solution to (11.P.3) is continuous (an important fact).
(a) Find the matrix M such that
Mφ = φ,
(11.P.4)
where φ = [φ(1)φ(2)]T ∈R2.
(b) Find the eigenvalues of M. Find the solution φ to (11.P.4).
(c) Take φ from (b) and multiply it by constant α such that α 2
k=1 φ(k) =
1 (i.e., replace φ by the normalized form αφ).
(d) Using the normalized vector from (c) (i.e., αφ), ﬁnd φ(k/2) for all
k ∈Z.
[Comment: Computation of the Daubechies 4-tap scaling function is the
ﬁrst major step in computing the Daubechies 4-tap wavelet. The process
suggested in (d) may be continued to compute φ(k/2J ) for any k ∈Z, and
for any positive integer J. The algorithm suggested by this is often called
the interpolatory graphical display algorithm (IGDA).]
11.12. Prove the following theorem. Suppose A ∈Rn×n and A = AT . Then A > 0
iff A = P T P for some nonsingular matrix P ∈Rn×n.
11.13. Let A ∈Rn×n be symmetric with eigenvalues
λ0 ≤λ1 ≤· · · ≤λn−2 ≤λn−1.
Show that for all x ∈Rn
λ0xT x ≤xT Ax ≤λn−1xT x.
[Hint: Use the fact that there is an orthogonal matrix P such that P T AP =
 (diagonal matrix of eigenvalues of A). Partition P in terms of its row
vectors.]
11.14. Section 11.3 presented a method of computing eAt “by hand.” Use this
method to
(a) Derive (10.109) (in Example 10.9).
(b) Derive (10.P.9) in Problem 10.23.
TLFeBOOK

522
NUMERICAL METHODS FOR EIGENPROBLEMS
(c) Find a closed-form expression for eAt, where
A =


λ
1
0
0
λ
1
0
0
λ

.
11.15. This exercise conﬁrms that eigenvalues and singular values are deﬁnitely
not the same thing. Consider the matrix
A =
 0
2
1
1

.
Use the MATLAB eig and svd functions to ﬁnd the eigenvalues and singular
values of A.
11.16. Show that e(A+B)t = eAteBt for all t ∈R if AB = BA. Does e(A+B)t =
eAteBt always hold for all t ∈R when AB ̸= BA ? Justify your answer.
11.17. This problem is an introduction to Floquet theory. Consider a linear system
with state vector x(t) ∈Rn for all t ∈R such that
dx(t)
dt
= A(t)x(t)
(11.P.5)
for some A(t) ∈Rn×n (all t ∈R), and such that A(t + T ) = A(t) for some
T > 0 [so that A(t) is periodic with period T ]. Let (t) be the fundamental
matrix of the system such that
d(t)
dt
= A(t)(t), (0) = I.
(11.P.6)
(a) Let (t) = (t + T ), and show that
d(t)
dt
= A(t)(t).
(11.P.7)
(b) Show that (t + T ) = (t)C, where C = (T ). [Hint: Equations
(11.P.6) and (11.P.7) differ only in their initial conditions [i.e., (0) =
what ?].]
(c) Assume that C−1 exists for some C ∈Rn×n, and that there exists some
R ∈Rn×n such that C = eT R. Deﬁne P (t) = (t)e−tR, and show that
P (t + T ) = P (t). [Thus, (t) = P (t)etR, which is the general form of
the solution to (11.P.6).]
(Comment: Further details of the theory of solution of (11.P.5) based on
working with (11.P.6) may be found in E. A. Coddington and N. Levinson,
TLFeBOOK

PROBLEMS
523
Theory of Ordinary Differential Equations, McGraw-Hill, New York, 1955.
The main thing for the student to notice is that the theory involves matrix
exponentials.)
11.18. Consider the matrix
A =


4
2
0
0
1
4
1
0
0
1
4
1
0
0
2
4

.
(a) Create a MATLAB routine that implements the scaled power algorithm,
and use your routine to ﬁnd the largest eigenvalue of A.
(b) Create a MATLAB routine that implements the shifted inverse power
algorithm, and use your routine to ﬁnd the smallest eigenvalue of A.
11.19. For A in the previous problem, ﬁnd κ2(A) using the MATLAB routines that
you created to solve the problem.
11.20. If A ∈Rn×n, and A = AT , then, for some x ∈Rn such that x ̸= 0, we
deﬁne the Rayleigh quotient of A and x to be the ratio xT Ax/xT x (=
⟨x, Ax⟩/⟨x, x⟩). The Rayleigh quotient iterative algorithm is described as
k := 0;
while k < N do begin
µk := zT
k Azk/zT
k zk;
(A −µkI)yk := zk;
zk+1 := yk/||yk||2;
k := k + 1;
end;
The user inputs z0, which is the initial guess about the eigenvector. Note
that the shift µk is changed (i.e., updated) at every iteration. This has the
effect of accelerating convergence (i.e., of reducing the number of iterations
needed to achieve an accurate solution). However, Aµk = A −µkI needs
to be factored anew with every iteration as a result. Prove the following
theorem. Let A ∈Rn×n be symmetric and (λ, x) be an eigenpair for A. If
y ≈x, µ = yT Ay/yT y with ||x||2 = ||y||2 = 1, then
|λ −µ| ≤||A −λI||2||x −y||2
2.
[Comment: The norm ||A −λI||2 is an A-dependent constant, while ||x −
y||2
2 = ||e||2
2 is the square of the size of the error between eigenvector x
and the estimate y of it. So the size of the error between λ and the estimate
µ (i.e., |λ −µ|) are proportional to ||e||2
2 at the worst. This explains the
fast convergence of the method (i.e., only a relatively small N is usually
needed). Note that the proof uses (4.31).]
TLFeBOOK

524
NUMERICAL METHODS FOR EIGENPROBLEMS
11.21. Write a MATLAB routine that implements the basic QR iteration algorithm.
You may use the MATLAB function qr to perform QR factorizations. Test
your program out on the following matrices:
(a)
A =


4
2
0
0
1
4
1
0
0
1
4
1
0
0
2
4

.
(b)
B =


0
0
0
−1
2
1
0
0
1 +
1
√
2
0
1
0
−3
2 −
√
2
0
0
1
1 +
√
2


.
Use other built-in MATLAB functions (e.g., roots or eig) to verify your
answers. Iterate enough to obtain entries for HN that are accurate to four
decimal places.
11.22. Repeat the previous problem using your own MATLAB implementation of
the single-shift QR iteration algorithm. Compare the number of iterations
needed to obtain four decimal places of accuracy with the result from the
previous problem.
11.23. Suppose that X, Y ∈Rn×n, and we deﬁne the matrices
A = X + jY, B =
 X
−Y
Y
X

.
Show that if λ ∈s(A) is real-valued, then λ ∈s(B). Find a relationship
between the corresponding eigenvectors.
TLFeBOOK

12
Numerical Solution of Partial
Differential Equations
12.1
INTRODUCTION
The subject of partial differential equations (PDEs) with respect to the matter of
their numerical solution is impossibly large to properly cover within a single chapter
(or, for that matter, even within a single textbook). Furthermore, the development of
numerical methods for PDEs is a highly active area of research, and so it continues
to be a challenge to decide what is truly “fundamental” material to cover at an
introductory level. In this chapter we shall place emphasis on wave propagation
problems modeled by hyperbolic PDEs (deﬁned in Section 12.2). We will consider
especially the ﬁnite-difference time-domain (FDTD) method [8], as this appears to
be gaining importance in such application areas as modeling of the scattering of
electromagnetic waves from particles and objects and modeling of optoelectronic
systems. We will only illustrate the method with respect to planar electromagnetic
wave propagation problems at normal incidence. However, prior to this we shall
give an overview of PDEs, including how they are classiﬁed into elliptic, parabolic,
and hyperbolic types.
12.2
A BRIEF OVERVIEW OF PARTIAL DIFFERENTIAL EQUATIONS
In this section we deﬁne some notation and terminology that is used throughout
the chapter. We explain how second-order PDEs are classiﬁed. We also summarize
some problems that will not be covered within this book, simply citing references
where the interested reader can ﬁnd out more.
We will consider only two-dimensional functions u(x, t) ∈R (or u(x, y) ∈R),
where the independent variable x is interpreted as a space variable, and independent
variable t is interpreted as time. The order of a PDE is the order of the highest
derivative. For our purposes, we will never consider PDEs of an order greater than
2. Common shorthand notation for partial derivatives includes
ux = ∂u
∂x ,
ut = ∂u
∂t ,
uxt = ∂2u
∂t∂x ,
uxx = ∂2u
∂x2 .
An Introduction to Numerical Analysis for Electrical and Computer Engineers, by C.J. Zarowski
ISBN 0-471-46737-5
c⃝2004 John Wiley & Sons, Inc.
525
TLFeBOOK

526
NUMERICAL SOLUTION OF PARTIAL DIFFERENTIAL EQUATIONS
If the PDE has solution u(x, y), where x and y are spatial variables, then often
we are interested only in approximating the solution on a bounded region (i.e., a
bounded subset of R2). However, we will consider mainly a PDE with solution
u(x, t), where, as already noted, t is time, and so we consider u(x, t) only for
x ∈[a, b] = [x0, xf ] and t ∈[t0, tf ]. Commonly, t0 = 0, x0 = 0, and xf = L, with
tf = T . We wish to approximate the PDE solution u(x, t) at grid points (mesh
points) much as we did in the problem of numerically solving ODEs as considered
in Chapter 10. Thus, we wish to approximate u(xk, tn), where
xk = x0 + hk,
tn = t0 + τn,
(12.1)
such that
h = 1
M (xf −x0),
τ = 1
N (tf −t0),
(12.2)
and so k = 0, 1, . . . , M, and n = 0, 1, . . . , N. This implies that we assume sam-
pling on a uniform two-dimensional grid deﬁned on the xt plane [(x, t) plane].
Commonly, the numerical approximation to u(xk, tn) is denoted by uk,n. The index
k is then a space index, and n is a time index.
There is a classiﬁcation scheme for second-order linear PDEs. According to
Kreyszig [1], Myint-U and Debnath [2], and Courant and Hilbert [9], a PDE of
the form
Auxx + 2Buxy + Cuyy = F(x, y, u, ux, uy)
(12.3)
is elliptic if AC −B2 > 0, parabolic if AC −B2 = 0, and is hyperbolic if AC −
B2 < 0.1 It is possible for A, B, and C to be functions of x and y, in which case
(12.3) may be of a type (elliptic, parabolic, hyperbolic) that varies with x and y. For
example, (12.3) might be hyperbolic in one region of R2 but parabolic in another.
Of course, the terminology as to type remains the same when space variable y is
replaced by time variable t.
An example of an elliptic PDE is the Poisson equation from electrostatics [10]
Vxx + Vyy = −1
ϵ ρ(x, y),
(12.4)
where the solution V (x, y) is the electrical potential (e.g., in units of volts) at the
spatial location (x, y) in R2, constant ϵ is the permittivity of the medium (e.g.,
in units of farads per meter), and ρ(x, y) is the charge density (e.g., in units of
coulombs per square meter) at the spatial point (x, y). We have assumed that
the permittivity is a constant, but it can vary spatially as well. Certainly, (12.4)
1This classiﬁcation scheme is related to the classiﬁcation of conic sections on the Cartesian plane. The
general equation for such a conic on R2 is
Ax2 + Bxy + Cy2 + Dx + Ey + F = 0.
The conic is hyperbolic if B2 −4AC > 0, parabolic if B2 −4AC = 0, and is elliptic if B2 −4AC < 0.
TLFeBOOK

A BRIEF OVERVIEW OF PARTIAL DIFFERENTIAL EQUATIONS
527
has the form of (12.3), where for u(x, y) = V (x, y) we have B = 0, A = C =
1, and F(x, y, u, ux, uy) = −ρ(x, y)/ϵ. Therefore, AC −B2 = 1 > 0, conﬁrming
that (12.4) is elliptic.
To develop an approximate method of solving (12.4), one may employ ﬁnite
differences. For example, from Taylor series theory
∂2V (xk, yn)
∂x2
= V (xk+1, yn) −2V (xk, yn) + V (xk−1, yn)
h2
−h2
12
∂4V (ξk, yn)
∂x4
(12.5a)
for some ξk ∈[xk−1, xk+1], and
∂2V (xk, yn)
∂y2
= V (xk, yn+1) −2V (xk, yn) + V (xk, yn−1)
τ 2
−τ 2
12
∂4V (xk, ηn)
∂y4
(12.5b)
for some ηn ∈[yn−1, yn+1], where xk = x0 + hk, yn = y0 + τn [recall (12.1), and
(12.2)]. The ﬁnite-difference approximation to (12.4) is thus
Vk+1,n −2Vk,n + Vk−1,n
h2
+ Vk,n+1 −2Vk,n + Vk,n−1
τ 2
= −ρ(xk, yn)
ϵ
.
(12.6)
Here k = 0, 1, . . . , M, and n = 0, 1, . . . , N. Depending on ρ(x, y) and boundary
conditions, it is possible to rewrite (12.6) as a linear system of equations in the
unknown (approximate) potentials Vk,n. In practice, N and M may be large, and so
the linear system of equations will be of high order consisting of O(NM) unknowns
to solve for. Because of the structure of (12.6), the linear system is a sparse one,
too. It has therefore been pointed out [3,7,11] that iterative solution methods are
preferred, such as the Jacobi or Gauss-Seidel methods (recall Section 4.7). This
avoids the problems inherent in storing and manipulating large dense matrices.
Epperson [11] notes that in recent years conjugate gradient methods have begun to
displace Gauss–Seidel/Jacobi approaches to solving large and sparse linear systems
such as are generated from (12.6). In part this is due to the difﬁculties inherent
in obtaining the optimal value for the relaxation parameter ω (recall the deﬁnition
from Section 4.7).
An example of a parabolic PDE is sometimes called the heat equation, or diffu-
sion equation [2–4, 11] since it models one-dimensional diffusion processes such
as the ﬂow of heat through a metal bar. The general form of the basic parabolic
PDE is
ut = α2uxx
(12.7)
for x ∈[0, L], t ∈R+. Here u(x, t) could be the temperature of some material at
(x, t). It could also be the concentration of some chemical substance that is diffusing
out from some source. (Of course, other physical interpretations are possible.)
Typical boundary conditions are
u(0, t) = 0,
u(L, t) = 0 for t > 0,
(12.8a)
TLFeBOOK

528
NUMERICAL SOLUTION OF PARTIAL DIFFERENTIAL EQUATIONS
and the initial condition is
u(x, 0) = f (x).
(12.8b)
The initial condition might be an initial temperature distribution, or chemical con-
centration. If u(x, t) is interpreted as temperature, then the boundary conditions
(12.8a) state that the ends of the one-dimensional medium are held at a constant
temperature of 0 (e.g., degrees Celsius).
Equation (12.7) has the form of (12.3) with B = C = 0, and hence AC −B2 =
0, which is the criterion for a parabolic PDE. Finite-difference schemes analogous
to the case for elliptic PDEs may be developed. Classically, perhaps the most
popular choice is the Crank–Nicolson method summarized by Burden and Faires
[3], but given a more detailed treatment by Epperson [11].
Another popular numerical solution technique for PDEs is the ﬁnite-element
(FEL) method. It applies to a broad class of PDEs, and there are many commercially
available software packages that implement this approach for various applications
such as structural vibration analysis, or electromagnetics. However, we will not
consider the FEL method as it deserves its own textbook. The interested reader can
see Strang and Fix [5] or Brenner and Scott [6] for details. A brief introduction
appears in Burden and Faires [3].
As stated earlier, the emphasis in this book will be on wave propagation prob-
lems as modeled by hyperbolic PDEs. We now turn our attention to this class
of PDEs.
12.3
APPLICATIONS OF HYPERBOLIC PDEs
In this section we summarize two problems that illustrate how hyperbolic PDEs
arise in practice. In later sections we will see that although both involve the mod-
eling of waves propagating in physical systems, the numerical methods for their
solution are different in the two cases, and yet they have in common the application
of ﬁnite-difference schemes.
12.3.1
The Vibrating String
Consider an elastic string with its ends ﬁxed at the points x = 0 and x = L (so
that the string is of length L unstretched). If the string is plucked at position
x = xP (xP ∈(0, L)) at time t = 0 such as shown in Fig. 12.1, then it will vibrate
for t > 0. The PDE describing u(x, t), which is the displacement of the string at
position x and time t, is given by
utt = c2uxx.
(12.9)
The system of Fig. 12.1 is also characterized by the boundary conditions
u(0, t) = 0,
u(L, t) = 0 for all t ∈R+,
(12.10)
TLFeBOOK

APPLICATIONS OF HYPERBOLIC PDEs
529
0
L
x
u(x,t)
P
xP
u(xP,0)
Figure 12.1
An elastic string plucked at time t = 0 at point P, which is located at x = xP .
which specify that the string’s ends are ﬁxed, and we have the initial conditions
u(x, 0) = f (x),
∂u(x, t)
∂t

t=0
= g(x),
(12.11)
which describes the initial displacement, and velocity of the string, respectively.
As explained, for example, in Kreyszig [1] or in Elmore and Heald [12], the PDE
(12.9) is derived from elementary Newtonian mechanics based on the following
assumptions:
1. The mass of the string per unit of length is a constant.
2. The string is perfectly elastic, offers no resistance to bending, and there is
no friction.
3. The tension in stretching the string before ﬁxing its ends is large enough to
neglect the action of gravity.
4. The motion of the string is purely a vibration in the vertical plane (i.e., the
y direction), and the deﬂection and slope are small in absolute value.
We will omit the details of the derivation of (12.9), as this would carry us too far
off course. However, note that constant c2 in (12.9) is
c2 = T
ρ ,
(12.12)
where T is the tension in the string (e.g., units of newtons) and ρ is the density of
the string (e.g., units of kilograms per meter). A dimensional analysis of (12.12)
quickly reveals that c has the units of speed (e.g, meters per second). It speciﬁes
the speed at which waves propagate on the string.
TLFeBOOK

530
NUMERICAL SOLUTION OF PARTIAL DIFFERENTIAL EQUATIONS
It is easy to conﬁrm that (12.9) is a hyperbolic PDE since, on comparing (12.9)
with (12.3), we have A = c2, B = 0, and C = −1. Thus, AC −B2 = −c2 < 0,
which meets the deﬁnition of a hyperbolic PDE.
At this point we summarize a standard method for obtaining series-based analyt-
ical solutions to PDEs. This is the method of separation of variables (also called the
product method). We shall also ﬁnd that Fourier series expansions (recall Chapters 1
and 3) have an important role to play in the solution method. The solutions we
obtain yield test cases that we can use to gauge the accuracy of numerical methods
that we consider later on.
Assume that the solution2 to (12.9) can be rewritten in the form
u(x, t) = X(x)T (t).
(12.13)
Clearly
uxx = XxxT,
utt = XTtt,
(12.14)
which may be substituted into (12.9), yielding
XTtt = c2T Xxx,
(12.15)
or equivalently
Ttt
c2T = Xxx
X .
(12.16)
The expression on the left-hand side is a function of t only, while that on the
right-hand side is a function only of x. Thus, both sides must equal some constant,
say, κ:
Ttt
c2T = Xxx
X
= κ.
(12.17)
From (12.17) we obtain two second-order linear ODEs in constant coefﬁcients
Xxx −κX = 0
(12.18)
and
Ttt −c2κT = 0.
(12.19)
We will now ascertain the general form of the solutions to (12.18) and (12.19),
based on the conditions (12.10) and (12.11). From (12.10) substituted into (12.13),
we obtain
u(0, t) = X(0)T (t) = 0,
u(L, t) = X(L)T (t) = 0.
2Theories about the existence and uniqueness of solutions to PDEs are often highly involved, and so
we completely ignore this matter here. The reader is advised to consult books dedicated to PDEs and
their solution for such information.
TLFeBOOK

APPLICATIONS OF HYPERBOLIC PDEs
531
If T (t) = 0 (all t), then u(x, t) = 0 for all x and t. This is the trivial solution, and
we reject it. Thus, we must have
X(0) = X(L) = 0.
(12.20)
For κ = 0, Eq. (12.18) is Xxx = 0, which has the general solution X(x) = ax + b,
but from (12.20) we conclude that a = b = 0, and so X(x) = 0 for all x. This is the
trivial solution and so is rejected. If κ = µ2 > 0, we have ODE Xxx −µ2X = 0,
which has a characteristic equation possessing roots at ±µ. Consequently, X(x) =
aeµx + be−µx. If we apply (12.20) to this, we conclude that a = b = 0, once
again giving the trivial solution X(x) = 0 for all x. Now ﬁnally suppose that
κ = −β2 < 0, in which case (12.18) becomes
Xxx + β2X = 0,
(12.21)
which has the characteristic equation s2 + β2 = 0. Thus, the general solution to
(12.21) is of the form
X(x) = a cos(βx) + b sin(βx).
(12.22)
Applying (12.20) yields
X(0) = a = 0,
X(L) = b sin(βL) = 0.
(12.23)
Clearly, to avoid encountering the trivial solution, we must assume that b ̸= 0.
Thus, we must have
sin(βL) = 0,
implying that we have
β = nπ
L ,
n ∈Z.
(12.24)
However, we avoid β = 0 (for n = 0) to prevent X(x) = 0 for all x; and we
consider only n ∈{1, 2, 3, . . .} = N because sin(−x) = −sin x, and the minus sign
can be absorbed into the constant b. Thus, in general
X(x) = Xn(x) = bn sin
nπ
L x

(12.25)
for n ∈N, and where x ∈[0, L]. So now we have found that
κ = −β2 = −
nπ
L
2
,
in which case (12.19) takes on the form
Ttt + λ2
nT = 0,
where
λn = nπc
L .
(12.26)
TLFeBOOK

532
NUMERICAL SOLUTION OF PARTIAL DIFFERENTIAL EQUATIONS
This has a general solution of the form
Tn(t) = An cos(λnt) + Bn sin(λnt)
(12.27)
again for n ∈N. Consequently, un(x, t) = Xn(x)Tn(t) is a solution to (12.9) for
all n ∈N, and
un(x, t) = [An cos(λnt) + Bn sin(λnt)] sin
nπ
L x

.
(12.28)
The functions (12.28) are eigenfunctions with eigenvalues λn = nπc/L for the PDE
in (12.9). The set {λn|n ∈N} is the spectrum. Each un(x, t) represents harmonic
motion of the string with frequency λn/(2π) cycles per unit of time, and is also
called the nth normal mode for the string. The ﬁrst mode for n = 1 is called the
fundamental mode, and the others (for n > 1) are called overtones, or harmonics.
It is clear that un(x, t) in (12.28) satisﬁes PDE (12.9), and the boundary conditions
(12.10). However, un(x, t) by itself will not satisfy (12.9), (12.10) and (12.11)
all simultaneously. In general, the complete solution is [using superposition as the
PDE (12.9) is linear]
u(x, t) =
∞

n=1
un(x, t) =
∞

n=1
[An cos(λnt) + Bn sin(λnt)] sin
nπ
L x

,
(12.29)
where the initial conditions (12.11) are employed to ﬁnd the series coefﬁcients An
and Bn for all n. We will now consider how this is done in general.
From (12.11) we obtain
u(x, 0) =
∞

n=1
An sin
nπ
L x

= f (x)
(12.30)
and
∂u(x, t)
∂t
|t=0 =
 ∞

n=1
[−Anλn sin(λnt) + Bnλn cos(λnt)] sin
nπ
L x
	
t=0
= g(x)
or
∞

n=1
Bnλn sin
nπ
L x

= g(x).
(12.31)
The orthogonality properties of sinusoids can be used to determine An and Bn
for all n. Note that (12.30) and (12.31) are particular instances of Fourier series
expansions. In particular, observe that for k, n ∈N
 L
0
sin
nπ
L x

sin
 kπ
L x
!
dx =
 0,
n ̸= k
L/2,
n = k
.
(12.32)
TLFeBOOK

APPLICATIONS OF HYPERBOLIC PDEs
533
Plainly, set {sin(nπx/L)|n ∈N} is an orthogonal set. Thus, from (12.30)
2
L
 L
0
sin
 kπ
L x
! 
 ∞

n=1
An sin
nπ
L x

dx = 2
L
 L
0
f (x) sin
 kπ
L x
!
dx,
and via (12.32) this reduces to
Ak = 2
L
 L
0
f (x) sin
 kπ
L x
!
dx,
(12.33)
and similarly from (12.31), and (12.32)
Bk =
2
λkL
 L
0
g(x) sin
 kπ
L x
!
dx.
(12.34)
Example 12.1
Suppose that g(x) = 0 for all x. Thus, the initial velocity of the
string is zero. Let the initial position (deﬂection) of the plucked string be triangular
such that
f (x) =



2H
L x,
0 < x ≤L
2
2H
L (L −x),
L
2 < x < L
.
(12.35)
In Fig. 12.1 this corresponds to xP = L/2 and u(xP , 0) = H. Since g(x) = 0 for
all x via (12.34), we must have Bk = 0 for all k. From (12.33) we have
Ak = 4H
L2
 L/2
0
x sin
 kπ
L x
!
dx +
 L
L/2
(L −x) sin
 kπ
L x
!
dx
	
.
Since

sin(ax) dx = −1
a cos(ax) + C
and

x sin(ax) dx = −1
a x cos(ax) + 1
a2 sin(ax) + C
(C is a constant of integration), on simpliﬁcation we have
Ak = 8H
k2π2 sin
 kπ
2
!
.
(12.36)
TLFeBOOK

534
NUMERICAL SOLUTION OF PARTIAL DIFFERENTIAL EQUATIONS
0
0.2
0.4
0.6
0.8
1
0
5
10
15
20
−0.1
−0.05
0
0.05
0.1
Position (x )
Time (t )
Displacement [u (x,t )]
Figure 12.2
Fourier series solution to the vibrating string problem. A mesh plot of u(x, t)
as given by Eq. (12.37) for the parameters L = 1, c/L = 1
8, and H = 1
10. The plot employed
the ﬁrst 100 terms of the series expansion.
Thus, substituting (12.36) into (12.29) yields the general solution for our example,
namely
u(x, t) = 8H
π2
∞

k=1
1
k2 sin
 kπ
2
!
cos
 kπc
L t
!
sin
 kπ
L x
!
,
but sin(kπ/2) = 0 for even k, and so this expression reduces to
u(x, t) = 8H
π2
∞

n=1
(−1)n−1
(2n −1)2 cos
 (2n −1)πc
L
t
!
sin
 (2n −1)π
L
x
!
.
(12.37)
Figure 12.2 shows a typical plot of the function u(x, t) as given by (12.37) (for
the parameters stated in the ﬁgure caption).
The reader is encouraged to think about Fig. 12.2, and to ask if the picture is
a reasonable one on the basis of his/her intuitive understanding of how a plucked
string (say, that on a stringed musical instrument) behaves. Of course, this question
should be considered with respect to the modeling assumptions that lead to (12.9),
and that were listed earlier.
12.3.2
Plane Electromagnetic Waves
An electromagnetic wave (e.g., radio wave or light) in three-dimensional space R3
within some material is described by the vector magnetic ﬁeld intensity H(x, y, z, t)
TLFeBOOK

APPLICATIONS OF HYPERBOLIC PDEs
535
[e.g., in units of amperes per meter, (A/m)] and the vector electric ﬁeld intensity
E(x, y, z, t) [e.g., in units of volts per meter (V/m)] such that
H(x, y, z, t) = Hx(x, y, z, t)ˆx + Hy(x, y, z, t) ˆy + Hz(x, y, z, t)ˆz,
E(x, y, z, t) = Ex(x, y, z, t)ˆx + Ey(x, y, z, t) ˆy + Ez(x, y, z, t)ˆz,
where ˆx, ˆy, and ˆz are the unit vectors in the x, y, and z directions of R3, respec-
tively. The dynamic equations that H and E both satisfy are Maxwell’s equations:
∇× E = −∂B
∂t
(Faraday’s law),
(12.38)
∇× H = ∂D
∂t
(Ampere’s law).
(12.39)
Here the material in which the wave propagates contains no charges or cur-
rent sources. The magnetic ﬂux density B(x, y, z, t), and the electric ﬂux density
D(x, y, z, t) are assumed to satisfy
D = ϵE,
B = µH.
(12.40)
These relations assume that the material is linear, isotropic [i.e., the same in all
directions, and homogeneous i.e., the parameters ϵ and µ do not vary with (x, y, z)].
Constant ϵ is the material’s permittivity [units of farads per meter (F/m)], and
constant µ is the material’s permeability [units of henries per meter (H/m)]. The
permittivity and permeability of free space (i.e., a vacuum) are often denoted by
ϵ0 and µ0, respectively, and
ϵ = ϵrϵ0,
µ = µrµ0,
(12.41)
where ϵr is the relative permittivity and µr is the relative permeability of the
material. Note that
ϵ0 = 8.854185 × 10−12 F/m,
µ0 = 400π × 10−9 H/m.
(12.42)
If the material is air, then ϵr ≈1 and µr ≈1 to very good approximation, and
so air is not practically distinguished (usually) from free space. For commonly
occurring dielectric materials (i.e., insulators), we have µr ≈1, also to excellent
approximation. On the other hand, for magnetic materials (e.g., iron, cobalt, nickel,
various alloys and mixtures), µr will be very different from unity, and in fact
the relationship B = µH must often be replaced by sometimes quite complicated
nonlinear relationships, often involving the phenomenon of hysteresis. But we will
completely avoid this situation here.
TLFeBOOK

536
NUMERICAL SOLUTION OF PARTIAL DIFFERENTIAL EQUATIONS
In general, for the vector ﬁeld A = Ax ˆx + Ay ˆy + Azˆz, the curl is the determinant
∇× A =

ˆx
ˆy
ˆz
∂
∂x
∂
∂y
∂
∂z
Ax
Ay
Az

,
(12.43)
so, expanding this expression with A = E and A = H gives (respectively)
∇× E = ˆx
 ∂Ez
∂y −∂Ey
∂z
!
+ ˆy
 ∂Ex
∂z −∂Ez
∂x
!
+ ˆz
 ∂Ey
∂x −∂Ex
∂y
!
,
(12.44)
and
∇× H = ˆx
 ∂Hz
∂y −∂Hy
∂z
!
+ ˆy
 ∂Hx
∂z −∂Hz
∂x
!
+ ˆz
 ∂Hy
∂x −∂Hx
∂y
!
.
(12.45)
We will consider only transverse electromagnetic (TEM) waves (i.e., plane
waves). If such a wave propagates in the x direction, then we may assume that
Ex = Ez = 0, and so Hx = Hy = 0. Note that the electric and magnetic ﬁeld com-
ponents Ey and Hz are orthogonal to each other. They lie within the (y, z) plane,
which itself is orthogonal to the direction of travel of the plane wave. From (12.44)
and (12.45), Maxwell’s equations (12.38) and (12.39) reduce to
∂Ey
∂x = −µ∂Hz
∂t ,
∂Hz
∂x = −ϵ ∂Ey
∂t ,
(12.46)
where we have used (12.40). Combining the two equations in (12.46), we obtain
either
∂2Hz
∂x2 = µϵ ∂2Hz
∂t2 ,
(12.47)
which is the wave equation for the magnetic ﬁeld, or
∂2Ey
∂x2 = µϵ ∂2Ey
∂t2 ,
(12.48)
which is the wave equation for the electric ﬁeld. If we deﬁne
v =
1
√µϵ ,
(12.49)
then the general solution to (12.48) (for example) can be expressed in the form
Ey(x, t) = Eyr(x −vt) + Eyl(x + vt),
(12.50)
TLFeBOOK

APPLICATIONS OF HYPERBOLIC PDEs
537
where the ﬁrst term is a wave propagating in the +x direction (i.e., to the right)
with speed v and the second term is a wave propagating in the −x direction (i.e.,
to the left) with speed v.3 Equation (12.50) is the classical D’Alembert solution to
the scalar wave equation (12.48). Clearly, similar reasoning applies to (12.47). Of
course, using (12.49) in (12.48), we can write
∂2Ey
∂t2
= v2 ∂2Ey
∂x2
(12.51)
which has the same form as (12.9). In short, the mathematics describing the vibra-
tions of mechanical systems is much the same as that describing electromagnetic
systems, only the physical interpretations differ. Of course, (12.51) clearly implies
that (12.47) and (12.48) are hyperbolic PDEs.
Example 12.2
It is easy to conﬁrm that (12.37) can be rewritten in the form
of (12.50). Via the identity
cos A sin B = 1
2[sin(A + B) −sin(A −B)]
(12.52)
we see that
cos
(2n −1)πc
L
t

sin
(2n −1)π
L
x

= 1
2 sin
(2n −1)π
L
(x + ct)

+1
2 sin
(2n −1)π
L
(x −ct)

.
Thus, (12.37) may immediately be rewritten as
u(x, t) = 4H
π2
∞

n=1
(−1)n−1
(2n −1)2 sin
(2n −1)π
L
(x −ct)

#
$%
&
=u1(x−ct)
+ 4H
π2
∞

n=1
(−1)n−1
(2n −1)2 sin
(2n −1)π
L
(x + ct)

#
$%
&
=u1(x+ct)
.
We note that when ϵr = µr = 1, we have v = c, where
c =
1
√µ0ϵ0
.
(12.53)
3Readers are invited to draw a simple sketch and convince themselves that this interpretation is cor-
rect. This interpretation is vital in understanding the propagation of electromagnetic waves in layered
materials, such as thin optical ﬁlms.
TLFeBOOK

538
NUMERICAL SOLUTION OF PARTIAL DIFFERENTIAL EQUATIONS
This is the speed of light in a vacuum. Since for real materials µr > 1 and ϵr > 1,
we have v < c, so an electromagnetic wave cannot travel at a speed exceeding that
of light in a vacuum.
Now we will assume sinusoidal solutions to the wave equations such as would
originate from sinusoidal sources.4 Speciﬁcally, let us assume that
Ey(x, t) = E0 sin(ωt −βx),
(12.54)
where β = ω√µϵ = 2π/λ, and λ is the wavelength (e.g., in units of meters). The
frequency of the source is ω, a ﬁxed constant, and so the wavelength will vary
depending on the medium. If the free-space wavelength is denoted λ0, then
2π
λ0
= ω
c ,
(12.55)
where c is from (12.53). If the free-space wave then propagates into a denser
material, then
2π
λ = ω
v
(12.56)
for v given by (12.49). From (12.55) and (12.56), we obtain
λ = v
c λ0.
(12.57)
Since v ≤c, we always have λ ≤λ0; that is, the wavelength will shorten. This
observation is useful in checking numerical methods that model the propagation
of sinusoidal waves across interfaces between different materials (e.g., layered
structures such as thin optical ﬁlms).
From (12.54) we have
∂Ey
∂x = −βE0 cos(ωt −βx)
so that from the ﬁrst equation in (12.46) we have
Hz =
 βE0
µ
cos(ωt −βx) dt = βE0
µω sin(ωt −βx).
(12.58)
The characteristic impedance of the medium in which the sinusoidal electromag-
netic wave travels is deﬁned to be
Z = Ey
Hz
= µω
β
=
8µ
ϵ =
8µr
ϵr
Z0,
(12.59)
4For example, the Colpitts oscillator from Chapter 10 could operate as a sinusoidal signal generator to
drive an antenna, thus producing a sinusoidal electromagnetic wave (radio wave) in space.
TLFeBOOK

APPLICATIONS OF HYPERBOLIC PDEs
539
where Z0 = √µ0/ϵ0 is the characteristic impedance of free space. The units of
Z are in ohms (). We see that Z is analogous to the concept of impedance that
arises in electric circuit analysis.
The analogy between our present problem and phasor analysis in basic electric
circuit theory can be exploited. For suitable E(x) ∈R
Ey = Ey(x, t) = E(x)ejωt
(12.60)
so that
∂2Ey
∂x2 = d2E(x)
dx2
ejωt,
∂2Ey
∂t2
= −ω2E(x)ejωt.
(12.61)
Substituting (12.61) into (12.48) yields
d2E(x)
dx2
ejωt = −µϵω2E(x)ejωt,
which reduces to the second-order linear ODE
d2E(x)
dx2
+ µϵω2E(x) = 0.
(12.62)
For convenience, we deﬁne the propagation constant
γ = jω√µϵ = jβ,
(12.63)
so −γ 2 = µϵω2, and (12.62) is now
d2E
dx2 −γ 2E = 0
(12.64)
(E = E(x)). This ODE has a general solution of the form
E(x) = E0e−γ x + E1eγ x,
(12.65)
where E0 and E1 are constants. Recalling (12.60), it follows that
Ey(x, t) = E0ejωt−γ x + E1ejωt+γ x
= E0e−j(βx−ωt) + E1ej(βx+ωt).
(12.66)
Of course, the ﬁrst term is a wave propagating to the right, and the second term is
a wave propagating to the left.
So far we have assumed wave propagation in lossless materials since this is
the easiest case to consider at the outset. We shall now consider the effects of
lossy materials on propagation. This will be important in that it is a more realistic
TLFeBOOK

540
NUMERICAL SOLUTION OF PARTIAL DIFFERENTIAL EQUATIONS
assumption in practice, and it is important in designing perfectly matched layers
(PMLs) in the ﬁnite-difference time-domain (FDTD) method, as will be seen later.
We may deﬁne electrical conductivity σ [units of amperes per volt-meter
[A/(V·m)] or mhos/meter], and magnetic conductivity σ ∗[units of volts per
ampere-meter [V/(A·m)] or ohms/meter]. In this case (12.38) and (12.39) take
on the more general forms.
∇× E = −µ∂H
∂t −σ ∗H,
(12.67)
∇× H = ϵ ∂E
∂t + σE.
(12.68)
Note that σ ∗is not the complex conjugate of σ. In fact, σ, σ ∗∈R with σ > 0
for a lossy material (i.e., an imperfect insulator, or a conductor), and σ ∗≥0. As
before we will assume E possesses only a y component, and H possesses only a z
component. Since we again have propagation only in the x direction, via (12.44),
(12.45) in (12.67), and (12.68), we have
∂Ey
∂x = −µ∂Hz
∂t
−σ ∗Hz,
(12.69)
−∂Hz
∂x = ϵ ∂Ey
∂t
+ σEy.
(12.70)
If Ey = Ey(x, t) possesses a term that propagates only to the right, then (using
phasors again)
E = Ey(x, t) ˆy = E0ejωte−γ x ˆy
(12.71)
for some suitable propagation constant γ . For suitable characteristic impedance Z,
we must have
H = Hz(x, t)ˆz = 1
Z E0ejωte−γ x ˆz.
(12.72)
We may use (12.69) and (12.70) to determine γ and Z. Substituting (12.71) and
(12.72) into (12.69) and (12.70) and solving for γ and Z yields
Z2 = jωµ + σ ∗
jωϵ + σ ,
γ 2 = (jωϵ + σ)(jωµ + σ ∗).
(12.73)
How to handle the complex square roots needed to obtain Z and γ will be dealt
with below. Observe that the equations in (12.73) reduce to the previous cases
(12.59), and (12.63) when σ = σ ∗= 0. It is noteworthy that when we have the
condition
σ ∗
µ = σ
ϵ ,
(12.74)
then we have
Z2 =
jωµ + µ
ϵ σ
jωϵ + σ
= µ
ϵ
jω + σ
ϵ
jω + σ
ϵ
= µ
ϵ .
(12.75)
TLFeBOOK

APPLICATIONS OF HYPERBOLIC PDEs
541
Condition (12.74) is what makes the creation of a PML possible, as will be con-
sidered later in this section, and will be demonstrated in Section 12.5.
Now we must investigate what happens to waves when they encounter a sudden
change in the material properties, speciﬁcally, an interface between layers. This
situation is depicted in Fig. 12.3. Assume that medium 1 has physical parameters
ϵ1, µ1, σ1, and σ ∗
1 , while medium 2 has physical parameters ϵ2, µ2, σ2, and σ ∗
2 .
The corresponding characteristic impedance and propagation constant for medium
1 is thus [via (12.73)]
Z2
1 = jωµ1 + σ ∗
1
jωϵ1 + σ1
,
γ 2
1 = (jωϵ1 + σ1)(jωµ1 + σ ∗
1 ),
(12.76)
while for medium 2 we have
Z2
2 = jωµ2 + σ ∗
2
jωϵ2 + σ2
,
γ 2
2 = (jωϵ2 + σ2)(jωµ2 + σ ∗
2 ).
(12.77)
In Fig. 12.3 for some constants E and H we have for the incident ﬁeld
Ei = Ei ˆy = Eejωte−γ1x ˆy,
H i = Hi ˆz = Hejωte−γ1x ˆz
x
y
z
(Out of the page)
Reflected
wave
Transmitted
wave
Incident
wave
Hi
Et
Ht
Er
Hr
Ei
Interface (boundary) between
two different media
Medium 1
Medium 2
Figure 12.3
A plane wave normally incident on an interface (boundary) between two
different media. The magnetic ﬁeld components are directed orthogonal to the page. The
interface is at x = 0 and is the yz−plane.
TLFeBOOK

542
NUMERICAL SOLUTION OF PARTIAL DIFFERENTIAL EQUATIONS
so that
∂Hi
∂t
= jωHi,
∂Ei
∂x = −γ1Ei,
and via (12.69)
−γ1Ei = −jωµ1Hi −σ ∗
1 Hi,
implying that
Ei
Hi
= jωµ1 + σ ∗
1
γ1
= Z1.
(12.78)
Similarly, for the transmitted ﬁeld, we must have
Et
Ht
= jωµ2 + σ ∗
2
γ2
= Z2.
(12.79)
But, for the reﬂected ﬁeld, again for suitable constants E′, and H ′ we have
Er = Er ˆy = E′ejωteγ1x ˆy,
H r = Hr ˆz = H ′ejωteγ1x ˆz
so that
∂Hr
∂t
= jωHr,
∂Er
∂x = γ1Er,
and via (12.69)
γ1Er = −jωµ1Hr −σ ∗
1 Hr,
implying
Er
Hr
= −jωµ1 + σ ∗
1
γ1
= −Z1.
(12.80)
The electric and magnetic ﬁeld components are tangential to the interface, and so
must be continuous across it. This implies that at x = 0, and for all t (in Fig. 12.3)
Hi + Hr = Ht,
Ei + Er = Et.
(12.81)
If we substitute (12.78)–(12.80) into (12.81), then after a bit of algebra, we have
Et
Ei
=
2Z2
Z2 + Z1
= τ
(12.82)
and
Er
Ei
= Z2 −Z1
Z2 + Z1
= ρ.
(12.83)
It is easy to conﬁrm that
τ = 1 + ρ.
(12.84)
TLFeBOOK

APPLICATIONS OF HYPERBOLIC PDEs
543
We call τ the transmission coefﬁcient from medium 1 into medium 2, and ρ is the
reﬂection coefﬁcient from medium 1 into medium 2. The coefﬁcients τ and ρ are
often called Fresnel coefﬁcients, especially in the ﬁeld of optics.
If σ ∗= 0 then from (12.73) the propagation constant for a sinusoidal wave in
a conductor is obtained from
γ 2 = −µϵω2 + jωµσ,
(12.85)
where µϵω2 > 0 and ωµσ ≥0. We may express γ 2 in polar form: γ 2 = r1ejθ1.
But γ = rejθ = [r1ejθ1]1/2. In general, if z = rejθ, then
z1/2 = r1/2ejθ/2,
or
z1/2 = r1/2ej(θ/2+π).
(12.86)
From (12.85)
r1 = |γ 2| = ωµ

σ 2 + ϵ2ω2,
θ1 = π
2 + tan−1  ϵ
σ ω

.
(12.87)
Consequently
γ = ±[ωµ

σ 2 + ϵ2ω2]1/2ej[ π
4 + 1
2 tan−1(ϵω/σ)].
(12.88)
A special case is the perfect insulator (perfect dielectric) for which σ = 0. Since
tan−1(∞) = π/2, Eq. (12.88) reduces to γ = ±j√µϵω = ±jβ. More generally
(σ not necessarily zero) we have γ = ±(α + jβ), where
α = [ωµ

σ 2 + ϵ2ω2]1/2 cos
π
4 + 1
2 tan−1  ϵ
σ ω

,
(12.89a)
β = [ωµ

σ 2 + ϵ2ω2]1/2 sin
π
4 + 1
2 tan−1  ϵ
σ ω

.
(12.89b)
Example 12.3
Assume that µ = µ0, ϵ = ϵ0, and that σ = 0.0001 mhos/meter.
For ω = 2πf [f is the sinusoid’s frequency in Hertz (Hz)] from (12.89), we obtain
the following table:
f (Hz)
α
β
1 × 106
.015236
0.025911
1 × 107
.018762
0.210423
1 × 108
.018836
2.095929
1 × 109
.018837
20.958455
Keeping the parameters the same, except that now σ = 0.01 mhos/meter, we
have the following table:
TLFeBOOK

544
NUMERICAL SOLUTION OF PARTIAL DIFFERENTIAL EQUATIONS
f (Hz)
α
β
1 × 106
.198140
0.199245
1 × 107
.611091
0.646032
1 × 108
1.523602
2.591125
1 × 109
1.876150
21.042254
In general, from (12.89a) we have α ≥0. If in Fig. 12.3 medium 1 is free space
while medium 2 is a conductor with 0 < σ < ∞, then Et has the form [using
(12.82)] for x ≥0
Et = τEejωte−γ2x ˆy = τEejωte−α2xe−jβ2x ˆy.
(12.90)
Of course, we also have Ei = Eejωte−jβ1x ˆy for x ≤0 since γ1 = jβ1 (i.e., α1 = 0
in free space), and Er = ρEejωtejβ1x ˆy for x ≤0 [using (12.83)]. Since α2 > 0, the
factor e−α2x will go to zero as x →∞. The amplitude of the wave must decay as
it progresses from free space (medium 1) into the conductive medium (medium 2).
The rate of decay certainly depends on the size of α2.
Now suppose that medium 1 is again free space, but that medium 2 has both
σ2 > 0 and σ ∗
2 > 0 such that condition (12.74) holds with µ2 = µ0, and ϵ2 = ϵ0,
speciﬁcally
σ ∗
2
µ0
= σ2
ϵ0
(12.91)
which implies [via (12.75)] that Z2 = √µ0/ϵ0. Since medium 1 is free space Z1 =
√µ0/ϵ0, too. The reﬂection coefﬁcient from medium 1 into medium 2 is [via
(12.83)]
ρ = Z2 −Z1
Z2 + Z1
= Z0 −Z0
2Z0
= 0.
When wave Ei in medium 1 encounters the interface (at x = 0 in Fig. 12.3), there
will be no reﬂected component, that is, we will have Er = 0. From (12.73) we
obtain
γ 2
2 = (σ2σ ∗
2 −ω2µ0ϵ0) + jω(σ2µ0 + σ ∗
2 ϵ0),
(12.92)
and we select the medium 2 parameters so that for γ2 = α2 + jβ2 we obtain α2 > 0,
and α2 is large enough so that the wave is rapidly attenuated in that e−α2x is small
for relatively small x. In this case we may deﬁne medium 2 to be a perfectly
matched layer (PML). It is perfectly matched in the sense that its characteristic
impedance is the same as that of medium 1, thus eliminating reﬂections at the
interface. Because it is lossy, it absorbs radiation incident on it. The layer dissipates
energy without reﬂection. It thus simulates the walls of an anechoic chamber. In
other words, an anechoic chamber has walls that approximately realize condition
(12.74). The necessity to simulate the walls of an anechoic chamber will become
clearer in Section 12.5 when we look at the FDTD method.
TLFeBOOK

THE FINITE-DIFFERENCE (FD) METHOD
545
Finally, we remark on the similarities between the vibrating string problem
and the problem considered here. The analytical solution method employed in
Section 12.3.1 was separation of variables, and we have employed the same ap-
proach here since all of our electromagnetic ﬁeld solutions are of the form u(x, t) =
X(x)T (t). The main difference is that in the vibrating string problem we have
boundary conditions deﬁned by the ends of the string being tied down somewhere,
while in the electromagnetic wave propagation problem as we have considered it
here there are no boundaries, or rather, the boundaries are at x = ±∞.
12.4
THE FINITE-DIFFERENCE (FD) METHOD
We now consider a classical approach to the numerical solution of (12.9) that we
call the ﬁnite-difference (FD) method. Note that the method to follow is by no means
the only approach. Indeed, the FDTD method to be considered in Section 12.5 is
an alternative, and there are still others.
Following (12.5)
∂2u(xk, tn)
∂t2
= u(xk, tn+1) −2u(xk, tn) + u(xk, tn−1)
τ 2
−τ 2
12
∂4u(xk, ηn)
∂t4
(12.93)
for some ηn ∈[tn−1, tn+1], and
∂2u(xk, tn)
∂x2
= u(xk+1, tn) −2u(xk, tn) + u(xk−1, tn)
h2
−h2
12
∂4u(ξk, tn)
∂x4
(12.94)
for some ξk ∈[xk−1, xk+1], where
xk = kh,
tn = nτ
(12.95)
for k = 0, 1, . . . , M, and n ∈Z+. On substitution of (12.93) and (12.94) into (12.9),
we have
u(xk, tn+1) −2u(xk, tn) + u(xk, tn−1)
τ 2
−c2 u(xk+1, tn) −2u(xk, tn) + u(xk−1, tn)
h2
= 1
12

τ 2 ∂4u(xk, ηn)
∂t4
−c2h2 ∂4u(ξk, tn)
∂x4

#
$%
&
=ek,n
,
(12.96)
where ek,n is the local truncation error. Since uk,n ≈u(xk, tn) from (12.96), we
obtain the difference equation
uk,n+1 −2uk,n + uk,n−1 −e2uk+1,n + 2e2uk,n −e2uk−1,n = 0,
(12.97)
where
e = τ
hc
(12.98)
TLFeBOOK

546
NUMERICAL SOLUTION OF PARTIAL DIFFERENTIAL EQUATIONS
is sometimes called the Courant parameter. It has a crucial role to play in deter-
mining the stability of the FD method (and of the FDTD method, too). If we solve
(12.97) for uk,n+1 we obtain
uk,n+1 = 2(1 −e2)uk,n + e2(uk+1,n + uk−1,n) −uk,n−1,
(12.99)
where k = 1, 2, . . . , M −1, and n = 1, 2, 3, . . .. Equation (12.99) is the main recur-
sion in the FD algorithm. However, we need to account for the initial and boundary
conditions in order to initialize this recursion. Before we consider this matter note
that, in the language of Section 10.2, the FD algorithm has a truncation error of
O(τ 2 + h2) per step [via ek,n in (12.96)].
Immediately on applying (12.10), since L = Mh, we have
u0,n = uM,n = 0
for
n ∈Z+.
(12.100)
Since from (12.11) u(x, 0) = f (x), we also have
uk,0 = f (xk)
(12.101)
for k = 0, 1, . . . , M. From (12.101) we have uk,0, but we also need uk,1 [consider
(12.99) for n = 1]. To obtain a suitable expression ﬁrst observe that from (12.9)
∂2u(x, t)
∂t2
= c2 ∂2u(x, t)
∂x2
⇒∂2u(x, 0)
∂t2
= c2 ∂2u(x, 0)
∂x2
= c2f (2)(x).
(12.102)
Now, on applying the Taylor series expansion, we see that for some µ ∈[0, t1] =
[0, τ] (and µ may depend on x)
u(x, t1) = u(x, 0) + τ ∂u(x, 0)
∂t
+ 1
2τ 2 ∂2u(x, 0)
∂t2
+ 1
6τ 3 ∂3u(x, µ)
∂t3
,
and on applying (12.11) and (12.102), this becomes
u(x, t1) = u(x, 0) + τg(x) + 1
2τ 2c2f (2)(x) + 1
6τ 3 ∂3u(x, µ)
∂t3
.
(12.103)
In particular, for x = xk, this yields
u(xk, t1) = u(xk, 0) + τg(xk) + 1
2τ 2c2f (2)(xk) + 1
6τ 3 ∂3u(xk, µk)
∂t3
.
(12.104)
If f (x) ∈C4[0, L], then, for some ζk ∈[xk−1, xk+1], we have
f (2)(xk) = f (xk+1) −2f (xk) + f (xk−1)
h2
−h2
12f (4)(ζk).
(12.105)
TLFeBOOK

THE FINITE-DIFFERENCE (FD) METHOD
547
Since u(xk, 0) = f (xk) [recall (12.11) again], and if we substitute (12.105) into
(12.104), we obtain
u(xk, t1) = f (xk) + τg(xk) + 1
2
c2τ 2
h2 [f (xk+1) −2f (xk) + f (xk−1)]
+ O(τ 3 + τ 2h2).
(12.106)
Via (12.98) this yields the required approximation
uk,1 = f (xk) + τg(xk) + 1
2e2[f (xk+1) −2f (xk) + f (xk−1)]
or
uk,1 = (1 −e2)f (xk) + 1
2e2[f (xk+1) + f (xk−1)] + τg(xk).
(12.107)
Taking account of the boundary conditions (12.100), Eq. (12.99) can be expressed
in matrix form as


u1,n+1
u2,n+1
...
uM−2,n+1
uM−1,n+1


=


2(1 −e2)
e2
0
· · ·
0
0
e2
2(1 −e2)
e2
· · ·
0
0
...
...
...
...
...
0
0
0
· · ·
2(1 −e2)
e2
0
0
0
· · ·
e2
2(1 −e2)


×


u1,n
u2,n
...
uM−2,n
uM−1,n


−


u1,n−1
u2,n−1
...
uM−2,n−1
uM−1,n−1


.
(12.108)
This matrix recursion is run for n = 1, 2, 3, . . ., and the initial conditions are pro-
vided by (12.101) and (12.107).
We recall from Chapter 10 that numerical methods for the solution of ODE IVPs
can be unstable. The same problem can arise in the numerical solution of PDEs.
In particular, as the FD method is effectively an explicit method, it can certainly
become unstable if h and τ are inappropriately selected.
As noted by others [2, 13], we will have
lim
h,τ→0 uk,n = u(hk, τn)
provided that 0 < e ≤1. This is the famous Courant–Friedrichs–Lewy (CFL) con-
dition for the stability of the FD method, and is originally due to Courant et al.
[23]. The special case where e = 1 is interesting and easy to analyze. In this case
(12.99) reduces to
uk,n+1 = uk+1,n −uk,n−1 + uk−1,n.
(12.109)
TLFeBOOK

548
NUMERICAL SOLUTION OF PARTIAL DIFFERENTIAL EQUATIONS
We recall that u(x, t) has the form
u(x, t) = v(x −ct) + w(x + ct)
[see (12.50)]. Hence u(xk, tn) = v(xk −ctn) + w(xk + ctn). Observe that, since
h = cτ (as e = 1), we have
u(xk+1, tn) −u(xk, tn−1) + u(xk−1, tn) = v(xk + h −ctn) + w(xk + h + ctn)
−v(xk −ctn + cτ) −w(xk + ctn −cτ)
+ v(xk −h −ctn) + w(xk −h + ctn)
= v(xk −h −ctn) + w(xk + h + ctn)
= v(xk −ctn −cτ) + w(xk + ctn + cτ)
= v(xk −ctn+1) + w(xk + ctn+1)
= u(xk, tn+1),
or
u(xk, tn+1) = u(xk+1, tn) −u(xk, tn−1) + u(xk−1, tn).
(12.110)
Equation (12.110) has a form that is identical to that of (12.109). In other words,
the algorithm (12.109) gives the exact solution to (12.9), but only at x = hk and
t = τn with h = cτ (which is a rather restrictive situation).
A more general approach to error analysis that conﬁrms the CFL condition is
sometimes called von Neumann stability analysis [2]. We outline the approach as
follows. We begin by deﬁning the global truncation error
ϵk,n = u(xk, tn) −uk,n.
(12.111)
Via (12.96)
u(xk, tn+1) −2u(xk, tn) + u(xk, tn−1) −e2[u(xk+1, tn) −2u(xk, tn) + u(xk−1, tn)]
= τ 2ek,n.
(12.112)
If we subtract (12.97) from (12.112) and simplify the result using (12.111), we
obtain
ϵk,n+1 = 2(1 −e2)ϵk,n + e2[ϵk+1,n + ϵk−1,n] −ϵk,n−1 + τ 2ek,n
(12.113)
for k = 0, 1, . . . , M (L = Mh), and n = 1, 2, 3, . . .. Equation (12.113) is a two-
dimensional difference equation for the global error sequence (ϵk,n). The term
τ 2ek,n is a forcing term, and if u(x, t) is smooth enough, the forcing term will
be bounded for all k and n. Basically, we can show that the CFL condition 0 <
e ≤1 prevents limn→∞|ϵk,n| = ∞for all k = 0, 1, . . . , M. Analogously to our
TLFeBOOK

THE FINITE-DIFFERENCE (FD) METHOD
549
stability analysis approach for ODE IVPs from Chapter 10, we may consider the
homogeneous problem
ϵk,n+1 = 2(1 −e2)ϵk,n + e2[ϵk+1,n + ϵk−1,n] −ϵk,n−1,
(12.114)
which is just (12.113) with the forcing term made identically zero for all k and
n. In Section 12.3.1 we learned that separation of variables was a useful means
to solve (12.9). We therefore believe that a discrete version of this approach is
helpful at solving (12.114). To this end we postulate a typical solution of (12.114)
of the form
ϵk,n = exp[jαkh + βnτ]
(12.115)
for suitable constants α ∈R and β ∈C. We note that (12.115) has similarities to
(12.28) and is really a term in a discrete form of Fourier series expansion. We also
see that
|ϵk,n| = | exp(βnτ)| = |sn|.
Thus, if |s| ≤1, we will not have unbounded growth of the error sequence (ϵk,n)
as n increases. If we now substitute (12.115) into (12.114), we obtain (after sim-
pliﬁcation) the characteristic equation
s2 −[2(1 −e2) + 2 cos(αh)e2]
#
$%
&
=2b
s + 1 = 0.
(12.116)
Using the identity 2 sin2 x = 1 −cos(2x), we obtain
b = 1 −2e2 sin2
 αh
2
!
.
(12.117)
It is easy to conﬁrm that |b| ≤1 for all e such that 0 ≤e ≤1 because 0 ≤
sin2 (αh/2) ≤1 for all all αh ∈R. We note that s2 −2bs + 1 = 0 for s = s1, s2,
where
s1 = b +

b2 −1,
s2 = b −

b2 −1.
(12.118)
If |b| > 1, then |sk| > 1 for some k ∈{1, 2}, which can happen if we permit e > 1.
Naturally we reject this choice as it yields unbounded growth in the size of ϵk,n
as n →∞. If |b| ≤1, then clearly |sk| = 1 for all k. (To see this, consider the
product s1s2 = s1s∗
1 = |s1|2.) This prevents unbounded growth of ϵk,n. Thus, we
have validated the CFL condition for the selection of Courant parameter e (i.e., we
must always choose e to satisfy 0 < e ≤1).
Example 12.4
Figure 12.4 illustrates the application of the recursion (12.108)
to the vibrating string problem of Example 12.1. The simulation parameters are
stated in the ﬁgure caption. The reader should compare the approximate solution
TLFeBOOK

550
NUMERICAL SOLUTION OF PARTIAL DIFFERENTIAL EQUATIONS
0
0.2
0.4
0.6
0.8
1
0
5
10
15
20
−0.1
−0.05
0
0.05
0.1
Position (kh)
Time (nt)
Estimated displacement (uk,n)
Figure 12.4
FD method approximate solution to the vibrating string problem. A mesh
plot of uk,n as given by Eq. (12.108) for the parameters L = 1, c/L = 1
8, and H = 1
10.
Additionally, h = 0.05 and τ = 0.1, which meets the CFL criterion for stability of the
simulation.
of Fig. 12.4 to the exact solution of Fig. 12.2. The apparent loss of accuracy as the
number of time steps increases (i.e., with increasing nτ) is due to the phenomenon
of numerical dispersion [22], a topic considered in the next section in the context
of the FDTD method. Of course, simulation accuracy improves as h, τ →0 for
ﬁxed T = Nτ, and L = Mh.
12.5
THE FINITE-DIFFERENCE TIME-DOMAIN (FDTD) METHOD
The FDTD method is often attributed to Yee [14]. It is a ﬁnite-difference scheme
just as the FD method of Section 12.4 is a ﬁnite-difference scheme. However, it is
of such a nature as to be particularly useful in solving hyperbolic PDEs where the
boundary conditions are at inﬁnity (i.e., wave propagation problems of the kind
considered in Section 12.3.2).
The FDTD method considers approximations to Hz(x, t) and Ey(x, t) given by
applying the central difference and forward difference approximations to the ﬁrst
derivatives in the PDEs (12.69) and (12.70). We will use the following notation
for the sampling of continuous functions such as f (x, t):
fk,n ≈f (kx, nt),
fk+ 1
2, n + 1
2
≈f ((k + 1
2)x, (n + 1
2)t)
(12.119)
(so x replaces h, and t replaces τ here, where h and τ were the grid spacings
used in previous sections). For convenience, let E = Ey and H = Hz (i.e., we drop
TLFeBOOK

THE FINITE-DIFFERENCE TIME-DOMAIN (FDTD) METHOD
551
the subscripts on the ﬁeld components). We approximate the derivatives in (12.69)
and (12.70) speciﬁcally according to
∂H
∂t ≈1
t [Hk+ 1
2 ,n+ 1
2 −Hk+ 1
2 ,n−1
2 ],
(12.120a)
∂E
∂t ≈1
t [Ek,n+1 −Ek,n],
(12.120b)
∂H
∂x ≈
1
x [Hk+ 1
2 ,n+ 1
2 −Hk−1
2 ,n+ 1
2 ],
(12.120c)
∂E
∂x ≈1
t [Ek+1,n −Ek,n].
(12.120d)
Deﬁne ϵk = ϵ(kx), µk = µ((k + 1
2)x), σk = σ(kx), and σ ∗
k = σ ∗((k + 1
2)
x), which assumes the general situation where the material parameters vary with
x ∈[0, L] (computational region). Substituting these discretized material parame-
ters, and (12.120) into (12.69) and (12.70), we obtain the following algorithm:
Hk+ 1
2 ,n+ 1
2 =

1 −σ ∗
k
µk
t

Hk+ 1
2 ,n−1
2 −1
µk
t
x [Ek+1,n −Ek,n],
(12.121a)
Ek,n+1 =

1 −σk
ϵk
t

Ek,n −1
ϵk
t
x [Hk+ 1
2 ,n+ 1
2 −Hk−1
2 ,n+ 1
2 ].
(12.121b)
This is sometimes called the leapfrog algorithm. The dependencies between the esti-
mated ﬁeld components in (12.121) are illustrated in Fig. 12.5. If we assume that
H(−1
2x, t) = H((M + 1
2)x, t) = 0
(12.122)
n −1
2
1
2
x = 0
x = L
0
M
1
n +
n
n + 1
x
M − 1
Computational  region
Time (t)
Figure 12.5
An illustration of the dependencies between the approximate ﬁeld components
given by (12.121a,b); the lines with arrows denote the “ﬂow” of these dependencies [◦
electric ﬁeld component (Ey); □magnetic ﬁeld component (Hz)].
TLFeBOOK

552
NUMERICAL SOLUTION OF PARTIAL DIFFERENTIAL EQUATIONS
for all t ∈R+, then a more detailed pseudocode description of the FDTD algo-
rithm is
Hk+ 1
2 , −1
2
:= 0 for k = 0, 1, . . . , M −1;
Ek,0 := 0 for k = 0, 1, . . . , M;
for n := 0 to N −1 do begin
Em,n := E0 sin(ωnt); {0 < m < M}
for k := 0 to M −1 do begin
Hk+ 1
2, n + 1
2
:=

1 −
σ∗
k
µk t

Hk+ 1
2, n −1
2
−1
µk
t
x [Ek+1,n −Ek,n];
end;
for k := 0 to M do begin
Ek,n+1 :=

1 −σk
ϵk t

Ek,n −1
ϵk
t
x [Hk+ 1
2, n + 1
2
−Hk−1
2 , n + 1
2
];
end;
end;
The statement Em,n := E0 sin(ωnt) simulates an antenna that broadcasts a
sinusoidal electromagnetic wave from the location x = mx ∈(0, L). Of course,
the antenna must be located in free space.
The FDTD algorithm is an explicit difference scheme, and so it may have
stability problems. However, it can be shown that the algorithm is stable provided
we have
e = c t
x ≤1
 
or t ≤x
c
!
,
(12.123)
Thus, the CFL condition of Section 12.4 applies to the FDTD algorithm as well.
A justiﬁcation of this claim appears in Taﬂove [8]. A MATLAB implementation
of the FDTD algorithm may be found in Appendix 12.A (see routine FDTD.m). In
this implementation we have introduced the parameters sx and st (0 < sx, st < 1)
such that
x = sxλ0,
t = st
x
c .
(12.124)
Clearly, ct/x = st, and so the CFL condition is met. Also, spatial sampling is
determined by x = sxλ0, which is some fraction of a free-space wavelength λ0
[recall (12.55)]. Note that the algorithm simulates the ﬁeld for all t ∈[0, T ], where
T = Nt. If the wave is propagating only through free space, then the wave will
travel a distance
D = cT = Nstsxλ0,
(12.125)
that is, the distance traveled is Nstsx free-space wavelengths. Since L = Msxλ0
(i.e., the computational region spans Msx free-space wavelengths), this allows us
to make a reasonable choice for N.
A problem with the FDTD algorithm is that even if the computational region is
only free-space, a wave launched from location x = mx ∈(0, L) will eventually
strike the boundaries at x = 0 and/or x = L, and so will be reﬂected back toward
the source. These reﬂections will cause very large errors in the estimates of H, and
TLFeBOOK

THE FINITE-DIFFERENCE TIME-DOMAIN (FDTD) METHOD
553
E. But we know from Section 12.3.2 that we may design absorbing layers called
perfectly matched layers (PMLs) that suppress these reﬂections.
Suppose that the PML has physical parameters µ, ϵ, σ, and σ ∗, then, from
(12.73), the PML will have a propagation constant given by
γ 2 = (σσ ∗−ω2µϵ) + jω(σµ + σ ∗ϵ).
(12.126)
If we enforce the condition (12.74), namely
σ ∗
µ = σ
ϵ ,
(12.127)
then (12.126) becomes
γ 2 = µ
ϵ (σ 2 −ω2ϵ2) + 2jωσµ.
(12.128)
If we enforce σ 2 −ω2ϵ2 ≤0, then γ = α + jβ, where
α =
8µ
ϵ [σ 2 + ω2ϵ2]1/2 cos
π
4 + 1
2Tan−1
 ω2ϵ2 −σ 2
2ωσϵ
!
.
(12.129)
Equation (12.129) is obtained by the same arguments that yielded (12.89). As
α > 0, then a wave on entering the PML will be attenuated by a factor e−αx,
where x is the depth of penetration of the wave into the PML. A particularly
simple choice for α is to let σ 2 = ω2ϵ2, in which case
α = ω√µϵ.
(12.130)
Since ω = 2π
λ0 c and c =
1
√µ0ϵ0 , with µ = µrµ0 and ϵ = ϵrϵ0, we can rewrite
(12.130) as
α = √µrϵr
2π
λ0
.
(12.131)
If we are matching the PML to free space, then µr = ϵr = 1, and so α = 2π
λ0 , in
which case
e−αx = e−2π
λ0 x.
(12.132)
If the PML is of thickness x = 2λ0, then, from (12.132) we have e−αx = e−4π ≈
3.5 × 10−6. A PML that is two free-space wavelengths thick will therefore absorb
very nearly all of the radiation incident on it at the wavelength λ0. Since we have
chosen σ 2 = ω2ϵ2, it is easy to conﬁrm that
σ = 2π
λ0
ϵr
Z0
,
σ ∗= 2π
λ0
µrZ0
(12.133)
TLFeBOOK

554
NUMERICAL SOLUTION OF PARTIAL DIFFERENTIAL EQUATIONS
0
1
2
3
4
5
6
7
8
9
10
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
Distance (x l0)
Ey (volts per meter)
Free space
PML 1
Dielectric
PML 2
Antenna
Figure 12.6
Typical output from FDTD.m (see Appendix 12.A) for the system described in
Example 12.5. The antenna broadcasts a sinusoid of wavelength λ0 = 500 nm (nanometers)
from the location x = 3λ0. The transmitted ﬁeld strength is E0 = 1 V/m.
[via (12.127)], where we recall that Z0 = √µ0/ϵ0. Routine FDTD.m in Appen-
dix 12.A implements PMLs according to this approach.
Example 12.5
Figure 12.6 illustrates a typical output from FDTD.m (Appen-
dix 12.A). The system shown in the ﬁgure occupies a computational region of
length 10λ0 (i.e., x ∈[0, 10λ0]). Somewhat arbitrarily we have λ0 = 500 nm (nano-
meters). The antenna (which, given the wavelength, could be a laser) is located
at index m = 150 (i.e., is at x = mx = msxλ0 = 3λ0, since sx = .02). The free-
space region is for x ∈(2λ0, 6λ0). The lossless dielectric occupies x ∈[6λ0, 8λ0],
and has a relative permittivity of ϵr = 4. The entire computational region is non-
magnetic, and so we have µ = µ0 everywhere. Clearly, PML 1 is matched to free
space, while PML 2 is matched to the dielectric.
Since ϵr = 4, according to (12.82), the transmission coefﬁcient from free space
into the dielectric is
τ =
2
8 µ0
ϵrϵ0
8 µ0
ϵrϵ0
+
8µ0
ϵ0
=
2
1 + √ϵr
= 2
3.
TLFeBOOK

THE FINITE-DIFFERENCE TIME-DOMAIN (FDTD) METHOD
555
Since E0 = 1 V/m, the amplitude of the electric ﬁeld within the dielectric must be
τE0 = 2
3 V/m. From Fig. 12.6 the reader can see that the electric ﬁeld within the
dielectric does indeed have an amplitude of about 2
3 V/m to a good approximation.
From (12.57) the wavelength within the dielectric material is
λ =
1
√ϵr
λ0 = 1
2λ0.
Again from Fig. 12.6 we see that the wavelength of the transmitted ﬁeld is indeed
close to 1
2λ0 within the dielectric.
We observe that the PMLs in Example 12.5 do not perfectly suppress reﬂections
at their boundaries. For example, the wave crest closest to the interface between
PML 2 and the dielectric, and that lies within the dielectric, is somewhat higher
than it should be. It is the discretization of a continuous space that has lead to these
residual reﬂections.
The theory of PMLs presented here does not easily extend from electromagnetic
wave propagation problems in one spatial dimension into propagation problems in
two or three spatial dimensions. It appears that the ﬁrst truly successful extension of
PML theory to higher spatial dimensions is due to B´erenger [15,16]. Wu and Fang
[17] claim to have improved the theory still further by improving the suppression
of the residual reﬂections noted above.
The problem of numerical dispersion was mentioned in Example 12.4 in the
application of the FD method to the simulation of a vibrating string. We conclude
this chapter with an account of the problem based mainly on the work of Trefethen
[22]. We will assume lossless propagation, so σ = σ ∗= 0 in (12.121). We will also
assume that the computational region is free space, so µ = µ0, and ϵ = ϵ0 every-
where. If we now substitute E(x, t) = E0 sin(ωt −βx) and H(x, t) = H0 sin(ωt −
βx) into either of (12.121a) or (12.121b), apply the appropriate trigonometric iden-
tities, and then cancel out common factors, we obtain the identity
sin
 ωt
2
!
= e sin
 βx
2
!
.
(12.134)
We may use (12.134) and (12.123) to obtain
vp = ω
β = c
e
2
βx sin−1

e sin
 βx
2
!
(12.135)
which is the phase speed of the wave of wavelength λ0 (recall β = 2π/λ0) in the
FDTD method. For the continuous wave E(x, t) [or, for that matter, H(x, t)], recall
from Section 12.3.2 that ω/β = c, so without spatial or temporal discretization
effects, a sinusoid will propagate through free space at the speed c regardless of its
wavelength (or, equivalently, its frequency). However, (12.135) suggests that the
speed of an FDTD-simulated sinusoidal wave will vary with the wavelength. As
TLFeBOOK

556
NUMERICAL SOLUTION OF PARTIAL DIFFERENTIAL EQUATIONS
explained in Ref. 22 (or see Ref. 12), the group speed
vg = dω
dβ = d
dβ (βvp) = c
cos

βx
2

8
1 −e2 sin2 
βx
2

(12.136)
is more relevant to assessing how propagation speed varies with the wavelength.
Again, in free space the continuous wave propagates at the group speed dω
dβ =
d
dβ (βc) = c. A plot of vg/c versus λ0/x [with vg/c given by (12.136)] appears
in Fig. 2 of Represa et al. [19]. It shows that short-wavelength sinusoids travel at
slower speeds than do long-wavelength sinusoids when simulated using the FDTD
method.
We have seen that nonsinusoidal waveforms (e.g., the triangle wave of Exam-
ple 12.1) are a superposition of sinusoids of varying frequency. Thus, if we use
the FDTD method, the FD method, or indeed any numerical method to simulate
wave propagation, we will see the effects of numerical dispersion. In other words,
the various frequency components in the wave will travel at different speeds, and
so the original shape of the wave will become lost as the simulation progresses
in time (i.e., as nt increases). Figure 12.7 illustrates this for the case of two
0
1
2
3
4
5
6
7
8
−2
−1
0
1
2
Distance (x l0)
Ey (volts per meter) 
0
1
2
3
4
5
6
7
8
−2
−1
0
1
2
Distance (x l0)
Ey (volts per meter)
(a)
(b)
numerical dispersion
Errors due to
n = 1500 steps
n = 500 steps
Figure 12.7
Numerical dispersion in the FDTD method as illustrated by the propagation
of two Gaussian pulses. The medium is free space.
TLFeBOOK

MATLAB CODE FOR EXAMPLE 12.5
557
Gaussian pulses traveling in opposite directions. The two pulses originally appeared
at x = 4λ0, and the medium is free space. For N = 1500 time steps, there is a very
noticeable error due to the “breakup” of the pulses as their constituent frequency
components separate out as a result of the numerical dispersion.
In closing, note that more examples of numerical dispersion may be found in
Luebbers et al. [18]. Shin and Nevels [21] explain how to work with Gaussian
test pulses to reduce numerical dispersion. We mention that Represa et al. [19]
use absorbing boundary conditions based on the theory in Mur [20], which is a
different method from the PML approach we have used in this book.
APPENDIX 12.A
MATLAB CODE FOR EXAMPLE 12.5
%
%
permittivity.m
%
% This routine specifies the permittivity profile of the
% computational region [0,L], and is needed by FDTD.m
%
function epsilon = permittivity(k,sx,lambda0,M)
epsilon0 = 8.854185*1e-12; % free-space permittivity
er1 = 4;
% relative permittivity of the dielectric
Dx = sx*lambda0;
% this is Delta x
x = k*Dx;
% position at which we determine epsilon
L = M*Dx;
% location of right end of computational
% region
if ((x >= 0) & (x < (L-4*lambda0)))
epsilon = epsilon0;
else
epsilon = er1*epsilon0;
end;
%
%
permeability.m
%
% This routine specifies the permeability profile of the
% computational region [0,L], and is needed by FDTD.m
%
function
mu = permeability(k,sx,lambda0,M)
mu0 = 400*pi*1e-9;
% free-space permeability
Dx = sx*lambda0;
% this is Delta x
x = k*Dx;
% position at which we determine mu
L = M*Dx;
% location of right end of computational
% region
mu = mu0;
%
%
econductivity.m
%
% This routine specifies the electrical conductivity profile
TLFeBOOK

558
NUMERICAL SOLUTION OF PARTIAL DIFFERENTIAL EQUATIONS
% of the computational region [0,L], and is needed by FDTD.m
%
function sigma = econductivity(k,sx,lambda0,M)
epsilon0 = 8.854185*1e-12; % free-space permittivity
mu0 = 400*pi*1e-9;
% free-space permeability
er1 = 4;
% dielectric relative permittivity
epsilon1 = er1*epsilon0;
% dielectric permittivity
Z0 = sqrt(mu0/epsilon0);
% free-space impedance
Dx = sx*lambda0;
% this is Delta x
x = k*Dx;
% position at which we determine sigma
L = M*Dx;
% location of right end of computational
% region
star1 = (2*pi/lambda0)*(1/Z0); % conductivity of PML 1 (at x = 0 end)
star2 = er1*star1;
% conductivity of PML 2 (at x = L end)
if ((x > 2*lambda0) & (x < L-2*lambda0))
sigma = 0;
elseif (x <= 2*lambda0)
sigma = star1;
elseif (x >= (L - 2*lambda0))
sigma = star2;
end;
%
%
mconductivity.m
%
% This routine specifies the magnetic conductivity profile of the
% computational region [0,L], and is needed by FDTD.m
%
function sigmastar = mconductivity(k,sx,lambda0,M)
epsilon0 = 8.854185*1e-12; % free-space permittivity
mu0 = 400*pi*1e-9;
% free-space permeability
Z0 = sqrt(mu0/epsilon0);
% free-space impedance
Dx = sx*lambda0;
% this is Delta x
x = (k+.5)*Dx;
% position at which we determine sigmastar
L = M*Dx;
% location of right end of computational
% region
star = (2*pi/lambda0)*Z0;
if ((x > 2*lambda0) & (x < L-2*lambda0))
sigmastar = 0;
else
sigmastar = star;
end;
%
%
FDTD.m
%
% This routine produces the plot in Fig. 12.6
which is associated with
% Example 12.5. Thus, it illustrates the FDTD method.
%
% The routine returns the total electric field component Ey to the
% caller.
%
TLFeBOOK

MATLAB CODE FOR EXAMPLE 12.5
559
function Ey = FDTD
mu0 = 400*pi*1e-9;
% free-space permeability
epsilon0 = 8.854185*1e-12;
% free-space permittivity
c = 1/sqrt(mu0*epsilon0);
% speed of light in free-space
lambda0 = 500;
% free-space wavelength of the source
% in nanometers
lambda0 = lambda0*1e-9;
% free-space wavelength of the source
% in meters
beta0 = (2*pi)/lambda0;
% free-space beta (wavenumber)
sx = .02;
% fraction of a free-space wavelength
% used to determine Delta x
st = .10;
% scale factor used to determine time-step
% size Delta t
Dx = sx*lambda0;
% Delta x
Dt = (st/c)*Dx;
% application of the CFL condition
% to determine time-step Delta t
E0 = 1;
% amplitude of the electric field (V/m)
% generated by the source
m = 150;
% source (antenna) location index
omega = beta0*c;
% source frequency (radians/second)
M = 500;
% number of spatial grid points is (M+1)
N = 4000;
% the number of time steps in the simulation
E = zeros(1,M+1);
% initial electric field
H = zeros(1,M+2);
% initial magnetic field
% Specify the material properties in the computational region
% (which is x in [0,M*Dx])
for k = 0:M
epsilon(k+1) = permittivity(k,sx,lambda0,M);
ce(k+1) = 1 - Dt*econductivity(k,sx,lambda0,M)/epsilon(k+1);
h(k+1) = 1/epsilon(k+1);
end;
h = h*Dt/Dx;
for k = 0:(M-1)
mu(k+1) = permeability(k,sx,lambda0,M);
ch(k+1) = 1 - Dt*mconductivity(k,sx,lambda0,M)/mu(k+1);
e(k+1) = 1/mu(k+1);
end;
e = e*Dt/Dx;
% Run the simulation for N time steps
for n = 1:N
E(m+1) = E0*sin(omega*(n-1)*Dt);
% Antenna is at index m
H(2:M+1) = ch.*H(2:M+1) - e.*(E(2:M+1) - E(1:M));
E(1:M+1) = ce.*E(1:M+1) - h.*(H(2:M+2) - H(1:M+1));
E(m+1) = E0*sin(omega*n*Dt);
Ey(n,:) = E;
% Save the total electric field at time step n
end;
TLFeBOOK

560
NUMERICAL SOLUTION OF PARTIAL DIFFERENTIAL EQUATIONS
REFERENCES
1. E. Kreyszig, Advanced Engineering Mathematics, 4th ed., Wiley, New York, 1979.
2. T. Myint-U and L. Debnath, Partial Differential Equations for Scientists and Engineers,
3rd ed., North-Holland, New York, 1987.
3. R. L. Burden and J. D. Faires, Numerical Analysis, 4th ed., PWS-KENT Publ., Boston,
MA, 1989.
4. A. Quarteroni, R. Sacco and F. Saleri, Numerical Mathematics (Texts in Applied Math-
ematics series, Vol. 37), Springer-Verlag, New York, 2000.
5. G. Strang and G. Fix, An Analysis of the Finite Element Method, Prentice-Hall, Engle-
wood Cliffs, NJ, 1973.
6. S. Brenner and R. Scott, The Mathematical Theory of Finite Element Methods, Springer-
Verlag, New York, 1994.
7. C. T. Kelley, Iterative Methods for Linear and Nonlinear Equations, SIAM, Philadel-
phia, PA, 1995.
8. A. Taﬂove,
Computational Electrodynamics: The
Finite-Difference Time-Domain
Method, Artech House, Norwood, MA, 1995.
9. R. Courant and D. Hilbert, Methods of Mathematical Physics, Vol. II. Partial Differen-
tial Equations, Wiley, New York, 1962.
10. J. D. Kraus and K. R. Carver, Electromagnetics, 2nd ed., McGraw-Hill, New York,
1973.
11. J. F. Epperson, An Introduction to Numerical Methods and Analysis, Wiley, New York,
2002.
12. W. C. Elmore and M. A. Heald, Physics of Waves, Dover Publ., New York, 1969.
13. E. Isaacson and H. B. Keller, Analysis of Numerical Methods, Wiley, New York,
1966.
14. K. S. Yee, “Numerical Solution of Initial Boundary Value Problems Involving
Maxwell’s Equations in Isotropic Media,” IEEE Trans. Antennas Propag. AP-14, 302–
307 (May 1966).
15. J.-P. B´erenger, “A Perfectly Matched Layer for the Absorption of Electromagnetic
Waves,” J. Comput. Phys. 114, 185–200 (1994).
16. J.-P. B´erenger, “Perfectly Matched Layer for the FDTD Solution of Wave-Structure
Interaction Problems,” IEEE Trans. Antennas Propag. 44, 110–117 (Jan. 1996).
17. Z. Wu and J. Fang, “High-Performance PML Algorithms,” IEEE Microwave Guided
Wave Lett. 6, 335–337 (Sept. 1996).
18. R. J. Luebbers, K. S. Kunz and K. A. Chamberlin, “An Interactive Demonstration of
Electromagnetic Wave Propagation Using Time-Domain Finite Differences,” IEEE
Trans. Educ. 33, 60–68 (Feb. 1990).
19. J. Represa, C. Pereira, M. Panizo and F. Tadeo, “A Simple Demonstration of Numerical
Dispersion under FDTD,” IEEE Trans. Educ. 40, 98–102 (Feb. 1997).
20. G. Mur, “Absorbing Boundary Conditions for the Finite-Difference Approximation of
the Time-Domain Electromagnetic-Field Equations,” IEEE Trans. Electromagn. Compat.
EMC-23, 377–382 (Nov. 1981).
21. C.-S. Shin and R. Nevels, “Optimizing the Gaussian Excitation Function in the Finite
Difference Time Domain Method,” IEEE Trans. Educ. 45, 15–18 (Feb. 2002).
TLFeBOOK

PROBLEMS
561
22. L. N. Trefethen, “Group Velocity in Finite Difference Schemes,” SIAM Rev. 24, 113–136
(April 1982).
23. R. Courant, K. Friedrichs and H. Lewy, “ ¨Uber die Partiellen Differenzengleichungen
der Mathematischen Physik,” Math. Ann. 100, 32–74 (1928).
PROBLEMS
12.1. Classify the following PDEs into elliptic, parabolic, and hyperbolic types
(or a combination of types).
(a) 3uxx + 5uxy + uyy = x + y
(b) uxx −uxy + 2uyy = ux + u
(c) yuxx + uyy = 0
(d) y2uxx −2xyuxy + x2uyy = 0
(e) 4x2uxx + uyy = u
12.2. Derive (12.5) (both equations).
12.3. In (12.6) let h = τ, and let N = M = 4, and for convenience let fk,n =
ρ(xk, yn)/ϵ. Deﬁne the vectors
V = [V1,1 V1,2 V1,3 V2,1 V2,2 V2,3 V3,1 V3,2 V3,3],
f = [f1,1 f1,2 f1,3 f2,1 f2,2 f2,3 f3,1 f3,2 f3,3].
Find the matrix A such that AV = h2f . Assume that
V0,n = Vk,0 = Vk,4 = V4,n = 0
for all k, and n.
12.4. In (12.6) let h = τ = 1
4, and N = M = 4, and assume that ρ(xk, yn) = 0
for all k, and n. Let x0 = y0 = 0. Suppose that
V (0, y) = V (x, 0) = 0, V (x, 1) = x, V (1, y) = y.
Find the linear system of equations for Vk,n with 1 ≤k, n ≤3, and put it
in matrix form.
12.5. Recall the previous problem.
(a) Write a MATLAB routine to implement the Gauss–Seidel method (recall
Section 4.7). Use your routine to solve the linear system of equations
in the previous problem.
(b) Find the exact solution to the PDE
Vxx + Vyy = 0
for the boundary conditions stated in the previous problem.
TLFeBOOK

562
NUMERICAL SOLUTION OF PARTIAL DIFFERENTIAL EQUATIONS
(c) Use the solution in (b) to ﬁnd V (k/4, n/4) at 1 ≤k, n ≤3, and compare
to the results obtained from part (a). They should be the same. Explain
why. [Hint: Consider the error terms in (12.5).]
12.6. The previous two problems suggest that the linear systems that arise in the
numerical solution of elliptic PDEs are sparse, and so it is worth considering
their solution using the iterative methods from Section 4.7. Recall that the
iterative methods of Section 4.7 have the general form
x(k+1) = Bx(k) + f
[from (4.155)]. Show that
||x(k+1) −x(k)||∞
||x(k) −x(k−1)||∞
≤||B||∞.
[Comment: Recalling (4.36c), it can be shown that ρ(A) ≤||A||p [which is
really another way of expressing (4.158)]. For example, this result can be
used to estimate the spectral radius of BJ [Eq. (4.171)].]
12.7. Consider Eq. (12.9), with the initial and boundary conditions in (12.10) and
(12.11), respectively.
(a) Consider the change of variables
ξ = x + ct,
η = x −ct
and φ(ξ, η) replaces u(x, t) according to
φ(ξ, η) = u
 1
2(ξ + η), 1
2c(ξ −η)
!
.
(12.P.1)
Verify the derivative operator equivalences
∂
∂x ≡∂
∂ξ + ∂
∂η, 1
c
∂
∂t ≡∂
∂ξ −∂
∂η.
(12.P.2)
[Hint: ∂φ
∂ξ = ∂u
∂x
∂x
∂ξ + ∂u
∂t
∂t
∂ξ , ∂φ
∂η = ∂u
∂x
∂x
∂η + ∂u
∂t
∂t
∂η.]
(b) Show that (12.9) is replaceable with
∂2φ
∂ξ∂η = 0.
(12.P.3)
(c) Show that the solution to (12.P.3) is of the form
φ(ξ, η) = P (ξ) + Q(η),
and hence
u(x, t) = P (x + ct) + Q(x −ct),
where P and Q are arbitrary twice continuously differentiable functions.
TLFeBOOK

PROBLEMS
563
(d) Show that
P (x) + Q(x) = f (x),
P (1)(x) −Q(1)(x) = 1
c g(x).
(e) Use the facts from (d) to show that
u(x, t) = 1
2[f (x + ct) + f (x −ct)] + 1
2c
 x+ct
x−ct
g(s) ds.
12.8. In (12.11) suppose that
f (x) =



−H
d
 
x −L
2 −d
!
,
L
2 ≤x ≤L
2 + d
H
d
 
x −L
2 + d
!
,
L
2 −d ≤x ≤L
2
0,
elsewhere
,
where 0 < d ≤L/2. Assume that g(x) = 0 for all x.
(a) Sketch f (x).
(b) Write a MATLAB routine to implement the FD algorithm (12.108) for
computing uk,n. Write the routine in such a way that it is easy to change
the parameters c, H, h, τ, N, M, and d. The routine must produce a mesh
plot similar to Fig. 12.4 and a plot similar to Fig. 12.7 on the same page
(i.e., make use of the subplot). The latter plot is to be of uk,N versus k. Try
out your routine using the parameters c = 1
8, h = 0.05, τ = 0.025, H =
0.1, d = L/10, M = 200, and N = 1100 (recalling that L = Mh). Do
you ﬁnd numerical dispersion effects?
12.9. Repeat Example 12.1 for f (x) and g(x) in the previous problem.
12.10. Example 12.1 is about the “plucked string.” Repeat Example 12.1 assuming
that f (x) = 0 and
g(x) =



2V
L x,
0 ≤x ≤L
2
2V
L (L −x),
L
2 ≤x ≤L
This describes a “struck string.”
12.11. The MATLAB routines in Appendix 12.A implement the FDTD method,
and generate information for the plot in Fig. 12.6. However, the reﬂected
ﬁeld component for 2λ0 ≤x ≤6λ0 is not computed or displayed.
Modify the code(s) in Appendix 12.A to compute the reﬂected ﬁeld com-
ponent in the free-space region and to plot it. Verify that at the interface
TLFeBOOK

564
NUMERICAL SOLUTION OF PARTIAL DIFFERENTIAL EQUATIONS
between the free-space region and the dielectric that |ρ| = 1
3 (magnitude of
the reﬂection coefﬁcient). Of course, you will need to read the amplitude
of the reﬂected component from your plot to do this.
12.12. Derive Eq. (12.134).
12.13. Modify the MATLAB code(s) in Appendix 12.A to generate a plot similar
to Fig. 12.7. [Hint: Ek,0 = E0 exp

−

k−m
iw
2
for k = 0, 1, . . . , M is the
initial electric ﬁeld. Set the initial magnetic ﬁeld to zero for all k.]
12.14. Plot vg/c versus λ0/x for vg given by (12.136). Choose e = 0.1, 0.5, and
0.8, and plot all curves on the same graph. Use these curves to explain why
the errors due to numerical dispersion (see Fig. 12.7) are worse on the side
of the pulse opposite to its direction of travel.
TLFeBOOK

13
An Introduction to MATLAB
13.1
INTRODUCTION
MATLAB is short for “matrix laboratory,” and is an extremely powerful software
tool1 for the development and testing of algorithms over a wide range of ﬁelds
including, but not limited to, control systems, signal processing, optimization, image
processing, wavelet methods, probability and statistics, and symbolic computing.
These various applications are generally divided up into toolboxes that typically
must be licensed separately from the core package.
Many books have already been written that cover MATLAB in varying degrees
of detail. Some, such as Nakamura [3], Quarteroni et al. [4], and Recktenwald
[5], emphasize MATLAB with respect to numerical analysis and methods, but
are otherwise fairly general. Other books emphasize MATLAB with respect to
particular areas such as matrix analysis and methods (e.g., see Golub and Van
Loan [1] or Hill [2]). Some books implicitly assume that the reader already knows
MATLAB [1,4]. Others assume little or no previous knowledge on the part of the
reader [2,3,5].
This chapter is certainly not a comprehensive treatment of the MATLAB tool,
and is nothing more than a quick introduction to it. Thus, the reader will have to
obtain other books on the subject, or consult the appropriate manuals for further
information. MATLAB’s online help facility is quite useful, too.
13.2
STARTUP
Once properly installed, MATLAB is often invoked (e.g., on a UNIX workstation
with a cmdtool window open) by typing matlab, and hitting return. A window
under which MATLAB runs will appear. The MATLAB prompt also appears:
>>
MATLAB commands may then be entered and executed interactively.
If you wish to work with commands in M-ﬁles (discussed further below), then
having two cmdtool windows open to the same working directory is usually desir-
able. One window would be used to run MATLAB, and the other would be used
1MATLAB is written in C, but is effectively a language on its own.
An Introduction to Numerical Analysis for Electrical and Computer Engineers, by C.J. Zarowski
ISBN 0-471-46737-5
c⃝2004 John Wiley & Sons, Inc.
565
TLFeBOOK

566
AN INTRODUCTION TO MATLAB
to edit the M-ﬁles as needed (to either develop the algorithm in the ﬁle or to
debug it).
13.3
SOME BASIC OPERATORS, OPERATIONS, AND FUNCTIONS
The MATLAB command
>> diary filename
will create the ﬁle ﬁlename, and every command you type in and run, and every
result of this, will be stored in ﬁlename. This is useful for making a permanent
record of a MATLAB session which can help in documentation and sometimes in
debugging. When writing MATLAB M-ﬁles, always make sure to document your
programs. The examples in Section 13.6 illustrate this.
MATLAB tends to work in the more or less intuitive way where matrix and/or
vector operations are concerned. Of course, it is in the nature of this software tool
to assume the user is already familiar with matrix analysis and methods before
attempting to use it.
When a MATLAB command creates a vector as the output from some operation,
it may be in the form of a column or a row vector, depending on the command. A
typical MATLAB row vector is
>>x = [ 1 1 1];
>>
The semicolon at the end of a line prevents the printout of the result of the command
at that line (this is useful in preventing display clutter). If you wish to turn it into
a column vector, then type:
>>x = x.’
x =
1
1
1
Making this conversion is sometimes necessary as the inputs to some routines need
the vectors in either row or column format, and some routines do not care. Routines
that do not care whether the input vector is row or column make the conversion to
a consistent form internally. For example, the following command sequence (which
can be stored in a ﬁle called an M-ﬁle) converts vector x into a row vector if it is
not one already:
>> [N,M] = size(x);
>> if N ~= 1
>>
x = x.’;
>>
end;
>>
TLFeBOOK

SOME BASIC OPERATORS, OPERATIONS, AND FUNCTIONS
567
In this routine N is the number of rows and M is the number of columns in x. (The
size command also accepts matrices.) The related command length(x) will return
the length of vector x. This can be very useful in for loops (below).
The addition of vectors works in the obvious way:
>> x = [ 1 1 1];
>> y = [ 2 -1 2 ];
>> x + y
ans =
3
0
3
>>
In this routine, the answer might be saved in vector z by typing >> z = x + y;.
Clearly, to add vectors without error means that they must be of the same size.
MATLAB will generate an error message if matrix and vector objects are not
dimensioned properly when operations are performed on them. The mismatching
of array sizes is a very common error in MATLAB programming.
Matrices can be entered as follows:
>> A = [ 1 1 ; 1 2 ]
A =
1
1
1
2
>>
Again, addition or subtraction would occur in the expected manner. We can invert
a matrix as follows:
>> inv(A)
ans =
2
-1
-1
1
>>
Operation det(A) will give the determinant of A, and [L, U] = lu(A) will give the
LU factorization of A (if it exists). Of course, there are many other routines for
common matrix operations, and decompositions (QR decomposition, singular value
decomposition, eigendecompositions, etc.). Compute y = Ax + b:
>> x = [ 1 -1 ].’;
>> b = [ 2
3 ].’;
>> y = A*x + b
TLFeBOOK

568
AN INTRODUCTION TO MATLAB
y =
2
2
>>
The colon operator can extract parts of matrices and vectors. For example, to
place the elements in rows j to k of column n of matrix B into vector x, use
>> x = B(j : k, n);. To extract the element from row k and column n of matrix A,
use >> x = A(k, n);. To raise something (including matrices) to a speciﬁc power,
use >> C = A∧p, for which p is the desired power. (This computes C = Ap.)
(Note: MATLAB indexes vectors and matrices beginning with 1.)
Unless the user overrides the defaults, variables i and j denote the square root
of −1:
>> sqrt(-1)
ans =
0 + 1.0000i
>>
Here, i and j are built-in constants. So, to enter a complex number, say, z = 3 −2j,
type
>> z = 3 - 2*i;
Observe
>> x = [ 1
1+i ];
>> x’
ans =
1.0000
1.0000 - 1.0000i
>>
So the transposition operator without the period gives the complex–conjugate
transpose (Hermitian transpose). Note that besides i and j, another useful built-in
constant is pi (= π).
Floating-point numbers are entered as, for instance, 1.5e −3 (which is 1.5 ×
10−3). MATLAB agrees with IEEE ﬂoating-point conventions, and so 0/0 will
result in NaN (“not a number”) to more clearly indicate an undeﬁned operation.
An operation like 1/0 will result in Inf as an output.
TLFeBOOK

SOME BASIC OPERATORS, OPERATIONS, AND FUNCTIONS
569
We may summarize a few important operators, functions, and other terms:
Relational Operators
<
less than
<=
less than or equal to
>
greater than
>=
greater than or equal to
==
equal to
∼=
not equal to
Logical Operators
&
and
|
or
∼
not
Trigonometric Functions
sin
sine
cos
cosine
tan
tangent
asin
arcsine
acos
arccosine
atan
arctangent
atan2
four quadrant arctangent
sinh
hyperbolic sine
cosh
hyperbolic cosine
tanh
hyperbolic tangent
asinh
hyperbolic arcsine
acosh
hyperbolic arccosine
atanh
hyperbolic arctangent
Elementary Mathematical Functions
abs
absolute value
angle
phase angle (argument of a complex number)
sqrt
square root
TLFeBOOK

570
AN INTRODUCTION TO MATLAB
Elementary Mathematical Functions
real
real part of a complex number
imag
imaginary part of a complex number
conj
complex conjugate
rem
remainder or modulus
exp
exponential to base e
log
natural logarithm
log10
base-10 logarithm
round
round to nearest integer
ﬁx
round toward zero
ﬂoor
round toward −∞
ceil
round toward +∞
In setting up the time axis for plotting things (discussed below), a useful com-
mand is illustrated by
>> y = [0:.2:1]
y =
0
0.2000
0.4000
0.6000
0.8000
1.0000
>>
Thus, [x:y:z] creates a row vector whose ﬁrst element is x and whose last element
is z (depending on step size y), where the elements in between are of the form x
+ ky (where k is a positive integer).
It can also be useful to create vectors of zeros:
>> zeros(size([1:4]))
ans =
0
0
0
0
>>
Or, alternatively, a simpler way is
>> zeros(1,4)
ans =
0
0
0
0
>>
TLFeBOOK

WORKING WITH POLYNOMIALS
571
Using “zeros(n,m)” will result in an n × m matrix of zeros. Similarly, a vector
(or matrix) containing only ones would be obtained using the MATLAB function
called “ones.”
13.4
WORKING WITH POLYNOMIALS
We have seen on many occasions that polynomials are vital to numerical analysis
and methods. MATLAB has nice tools for dealing with these objects.
Suppose that we have polynomials P1(s) = s + 2 and P2(s) = 3s + 4 and wish
to multiply them. In this case type the following command sequence:
>> P1 = [ 1 2 ];
>> P2 = [ 3 4 ];
>> conv(P1,P2)
ans =
3
10
8
>>
This is the correct answer since P1(s)P2(s) = 3s2 + 10s + 8. From this we see
polynomials are represented as vectors of the polynomial coefﬁcients, where the
highest-degree coefﬁcient is the ﬁrst element in the vector. This rule is followed
pretty consistently. (Note: “conv” is the MATLAB convolution function, so if you
don’t already know this, convolution is mathematically essentially the same as
polynomial multiplication.)
Suppose P (s) = s2 + s −2, and we want the roots. In this case you may type
>> P = [ 1
1
-2 ];
>> roots(P)
ans =
-2
1
>>
MATLAB (version 5 and later) has “mroots,” which is a root ﬁnder that does a
better job of computing multiple roots.
The MATLAB function “polyval” is used to evaluate polynomials. For example,
suppose P (s) = s2 + 3s + 5, and we wanted to compute P (−3). The command
sequence is
>> P = [ 1
3
5 ];
>> polyval(P,-3)
TLFeBOOK

572
AN INTRODUCTION TO MATLAB
ans =
5
>>
13.5
LOOPS
We may illustrate the simplest loop construct in MATLAB as follows:
>> t = [0:.1:1];
>> for k = 1:length(t)
>>
x(k) = 5*sin( (pi/3) * t(k) ) + 2;
>>
end;
>>
This command sequence computes x(t) = 5 sin( π
3 t) + 2 for t = 0.1k, where k =
0, 1, . . . , 10. The result is saved in the (row) vector x. However, an alternative
approach is to vectorize the calculation according to
>> t = [0:.1:1];
>> x = 5*sin( pi*t/3 ) + 2*ones(1,length(t));
>>
This yields the same result. Vectorizing calculations leads to faster code (in terms
of runtime).
A potentially useful method to add (append) elements to a vector is
>> x = [];
>> for k = 1:2:6
>>
x = [ x k ];
>>
end;
>> x
x =
1
3
5
>>
where x = [ ] deﬁnes x to be initially empty, while the for loop appends 1, 3, and
5 to the vector one element at a time.
The format of numerical outputs can be controlled using MATLAB fprintf
(which has many similarities to the ANSI C fprintf function). For example
>> for k = 0:9
fprintf(’%12.8f\n’,sqrt(k));
end;
0.00000000
TLFeBOOK

PLOTTING AND M-FILES
573
1.00000000
1.41421356
1.73205081
2.00000000
2.23606798
2.44948974
2.64575131
2.82842712
3.00000000
>>
The use of a ﬁle identiﬁer can force the result to be printed to a speciﬁc ﬁle instead
of to the terminal (which is the result in this example). MATLAB also has save
and load commands that can save variables and arrays to memory, and read them
back, respectively.
Certainly, for loops may be nested in the expected manner. Of course, MATLAB
also supports a “while” statement. For information on conditional statements (i.e.,
“if” statements), use “>> help if.”
13.6
PLOTTING AND M-FILES
Let’s illustrate plotting and the use of M-ﬁles with an example. Note that M-ﬁles
are also called script ﬁles (use script as the keyword when using help for more
information on this feature).
As an exercise the reader may wish to create a ﬁle called “stepH.m” (open and
edit it in the manner you are accustomed to). In this ﬁle place the following lines:
%
%
stepH.m
%
% This routine computes the unit-step response of the
% LTI system with system function H(s) given by
%
%
K
%
H(s) =
-------------
%
s^2 + 3s + K
%
% for user input parameter K.
The result is plotted.
%
function stepH(K)
b = [ 0 0 K ];
a = [ 1 3 K ];
clf
% Clear any existing plots from the screen
step(b,a);
% Compute the step response and plot it
grid
% plot the grid
TLFeBOOK

574
AN INTRODUCTION TO MATLAB
This M-ﬁle becomes a MATLAB command, and for K = 0.1 may be executed
using
>> stepH(.1);
>>
This will result in another window opening where the plot of the step response will
appear. To save this ﬁle for printing, use
>> print -dps filename.ps
which will save the plot as a postscript ﬁle called “ﬁlename.ps.” Other printing for-
mats are available. As usual, the details are available from online help. Figure 13.1
is the plot produced by stepH.m for the speciﬁed value of K.
Another example of an M-ﬁle that computes the frequency response (both mag-
nitude and phase) of the linear time-invariant (LTI) system with Laplace transfer
function is
H(s) =
1
s2 + 1
2s + 1
.
Time (seconds)
Amplitude
0
30
60
90
120
150
180
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Figure 13.1
Step response: typical output from stepH.m.
TLFeBOOK

PLOTTING AND M-FILES
575
If you are not familiar with Laplace transforms, recall phasor analysis from basic
electric circuit theory. You may, for instance, interpret H(jω) as the ratio of two
phasor voltages (frequency ω), such as
H(jω) = V2(jω)
V1(jω).
The numerator phasor V2(jω) is the output of the system, and the denominator
phasor V1(jω) is the input to the system (a sinusoidal voltage source).
%
%
freqresp.m
%
% This routine plots the frequency response of the Laplace
% transfer function
%
%
1
%
H(s) =
----------------
%
s^2 + .5s
+
1
%
% The magnitude response (in dB) and the phase response (in
% degrees) are returned in the vectors mag, and pha,
% respectively.
The places on the frequency axis where the
% response is computed are returned in vector f.
%
function [mag,pha,f] = freqresp
b = [ 0 0 1];
a = [ 1 .5 1 ];
w = logspace(-2,1,50); % Compute the frequency response for 10^(-2) to 10^1
% radians per second at 50 points in this range
h = freqs(b,a,w);
% h is the frequency response
mag = abs(h);
% magnitude response
pha = angle(h);
% phase response
f = w/(2*pi);
% setup frequency axis in Hz
pha = pha*180/pi;
% phase now in degrees
mag = 20*log10(mag);
% magnitude response now in dB
clf
subplot(211), semilogx(f,mag,’-’), grid
xlabel(’ Frequency (Hz) ’)
ylabel(’ Amplitude (dB) ’)
title(’ Magnitude Response ’)
subplot(212), semilogx(f,pha,’-’), grid
xlabel(’ Frequency (Hz) ’)
ylabel(’ Phase Angle (Degrees) ’)
title(’ Phase Response ’)
Executing the command
>> [mag,phase,f] = freqresp;
>>
TLFeBOOK

576
AN INTRODUCTION TO MATLAB
10−3
10−2
10−1
100
101
−40
−30
−20
−10
0
10
Frequency (Hz)
Amplitude (decibels)
10−3
10−2
10−1
100
101
−200
−150
−100
−50
0
Frequency (Hz)
Phase angle (degrees)
(a)
(b)
Figure 13.2
The output from freqresp.m: (a) magnitude response; (b) phase response.
will result in the plot of Fig. 13.2, and will also give the vectors mag, phase,
and f. Vector mag contains the magnitude response (in decibels), and vector phase
contains the phase response (in degrees) at the sample values in the vector f, which
deﬁnes the frequency axis for the plots (in hertz). In other words, “freqresp.m” is a
Bode plotting routine. Note that the MATLAB command “bode” does Bode plots
as well.
Additional labeling may be applied to plots using the MATLAB command text
(or via a mouse using “gtext”; see online help). As well, the legend statement is
useful in producing labels for a plot with different curves on the same graph. For
example
function ShowLegend
ul = 2.5*pi;
ll = -pi/4;
N = 200;
dt = (ul - ll)/N;
for k = 0:N-1
t(k+1) = ll + dt*k;
x(k+1) = exp(-t(k+1));
y(k+1) = sin(t(k+1));
end;
TLFeBOOK

REFERENCES
577
−1
0
1
2
3
4
5
6
7
8
−1
−0.5
0
0.5
1
1.5
2
2.5
t
e−t
sin(t)
Figure 13.3
Illustration of the MATLAB legend statement.
subplot(211), plot(t,x,’-’,t,y,’--’), grid
xlabel(’ t ’)
legend(’ e^{-t} ’, ’ sin(t) ’)
When this code is run, it gives Fig. 13.3. Note that the label syntax is similar to
that in LaTeX [6].
Other sample MATLAB codes have appeared as appendixes in earlier chapters,
and the reader may wish to view these as additional examples.
REFERENCES
1. G. H. Golub and C. F. Van Loan, Matrix Computations, 2nd ed. Johns Hopkins Univ.
Press, Baltimore, MD, 1989.
2. D. R. Hill, Experiments in Computational Matrix Algebra (C. B. Moler, consulting ed.),
Random House, New York, 1988.
3. S. Nakamura, Numerical Analysis and Graphic Visualization with MATLAB, 2nd ed.
Prentice-Hall, Upper Saddle River, NJ, 2002.
4. A. Quarteroni, R. Sacco, and F. Saleri, Numerical Mathematics (Texts in Applied Math-
ematics series, Vol. 37). Springer-Verlag, New York, 2000.
5. G. Recktenwald, Numerical Methods with MATLAB: Implementation and Application,
Prentice-Hall, Upper Saddle River, NJ, 2000.
6. M. Goossens, F. Mittelbach, and A. Samarin, The LaTeX Companion, Addison-Wesley,
Reading, MA, 1994.
TLFeBOOK

TLFeBOOK

INDEX
Absolute convergence, 95
Additive splitting, 179
Adjoint matrix, 489
Alternating set, 239
Ampere’s law, 535
Anechoic chamber, 544
Appolonius’ identity, 35
Argmin, 39
ASCII, 339
Asymptotic expansion, 97
Asymptotic series, 97–103
Asymptotic time complexity, 155
Backtracking line search, 346,353
Backward difference
approximation, 402
Backward Euler method
see Euler’s method of solution
(implicit form)
Backward substitution, 156
Banach ﬁxed-point theorem, 297,422
see also Contraction theorem
Banach space, 68
Basic QR iterations algorithm, 512
Bauer-Fike theorem, 520
Bernoulli’s differential equation, 426
Bernoulli’s inequality, 123
Bernoulli numbers, 398
Bessel’s inequality, 217
“Big O” notation, 155,296
Binary number codes
One’s complement, 57
Sign-magnitude, 56
An Introduction to Numerical Analysis for Electrical and Computer Engineers, by C.J. Zarowski
ISBN 0-471-46737-5
c⃝2004 John Wiley & Sons, Inc.
Two’s complement, 57
Binomial theorem, 85
Bipolar junction transistor (BJT)
Base current, 420
Base-emitter voltage, 420
Collector current, 420
Collector-emitter voltage, 420
Forward current gain, 420
On resistance, 420
Bisection method, 292–296
Block upper triangular matrix, 510
Boundary conditions, 527
Boundary value problem (BVP), 445
C, 38,339,565
C++, 38
Cancellation, 44
Cantor’s intersection theorem, 294
Cartesian product, 2
Catastrophic cancellation, 62,117
Catastrophic convergence, 90
Cauchy sequence, 64
Cauchy-Schwarz inequality, 139
Cayley-Hamilton theorem, 489
Central difference approximation, 402
Chaotic sequence, 323
Characteristic equation, 458,549
Characteristic impedance, 538
Characteristic impedance of free
space, 539
Characteristic polynomial, 144,481
Chebyshev approximation, 239
Chebyshev norm, 239
579
TLFeBOOK

580
INDEX
Chebyshev polynomials of the ﬁrst kind,
218–225
Chebyshev polynomials
of the second kind, 243–245
Cholesky decomposition, 167,355
Christoffel-Darboux formula, 211,389
Christoffel numbers, 389
Closed subset, 299
Cofactor, 489
Colpitts oscillator, 419,465,538
Compactly supported, 278
Companion matrix, 515,519
Complementary error function, 98
Complementary sine integral, 100
Complete solution, 532
Complete space, 64
Complete spline, 275
Completing the square, 131
Complex numbers
Cartesian form, 28
Imaginary part, 31
Magnitude, 29
Polar form, 28
Real part, 31
Computational complexity, 154
Computational efﬁciency, 89
Computational region, 551
Condition number of a matrix,
135–147
Conic section, 519,526
Conjugate gradient methods, 527
Conjugate symmetry, 35
Continuous mapping, 11
Contraction (contractive)
mapping, 184,297
Contraction theorem, 183,298
Contra-identity matrix, 199
Convergence in the mean, 217
Convergence of a sequence, 63
Convex function, 352
Convolution, 204
Convolution integral, 104,444
Coordinate rotation digital computing
(CORDIC) method, 70,107–116
Countably inﬁnite set, 22
Courant-Friedrichs-Lewy (CFL)
condition, 547
Courant parameter, 546
Crank-Nicolson method, 528
Cubic B-spline, 272
Current-controlled current source
(CCCS), 420
Curve ﬁtting, 252
Cyphertext sequence, 325
D’Alembert solution, 537
Daubechies 4-tap scaling function, 520
Daubechies wavelets, 18
Deadbeat synchronization, 326
Defective matrix, 483
Deﬂation theorem, 507
Degree of freedom, 440
Descent algorithm, 352
Detection threshold, 333
Diagonal dominance, 182,281
Diagonalizable matrix, 445
Diffusion equation
see Heat equation
Dimension, 22
Dirichlet kernel, 74,103
Discrete basis, 70,108
Discrete convolution, 34
Discrete Fourier series, 25
Discrete Fourier transform
(DFT), 27,37,112
Divergent sequence, 64
Divided differences, 257–259
Divided-difference table, 264
Dominant eigenvalue, 498
Double sequence, 69
Dufﬁng equation,
415,447–448,453–454
Eavesdropper, 331
Eigenfunction, 532
Eigenpair, 480
Eigenproblem, 480
Eigenspace, 483
TLFeBOOK

INDEX
581
Eigenvalue
Matrix, 143,480
Partial differential equation
(PDE), 532
Eigenvector, 480
Electric ﬂux density, 535
Electrical conductivity, 540
Elementary logic
Logical contrary, 32
Logically equivalent, 32
Necessary condition, 31
Sufﬁcient condition, 31
Elliptic integrals, 410–411
Elliptic partial differential
equation, 526
Equality constraint, 357
Error control coding, 327
Encryption key, 325
Energy
Of a signal (analog), 13
Of a sequence, 13
Error
Absolute, 45,60,140
Relative, 45,60,140,195
Euler-Maclaurin formula, 397
Error function, 94
Euclidean metric, 6
Euclidean space, 12,16
Euler’s identity, 19,30
Exchange matrix
see Contra-identity matrix
Faraday’s law, 535
Fast Fourier transform (FFT), 27
Feasible direction, 357
Feasible incremental step, 357
Feasible set, 357
Fejer’s theorem, 122
Finite difference (FD) method, 545–550
Finite-difference time-domain (FDTD)
method, 525,540,550–557
Finite-element method (FEL), 528
Finite impulse response (FIR) ﬁltering,
34,367
Finite-precision arithmetic effects, 1
Fixed-point dynamic range, 39
Fixed-point number
representations, 38–41
Fixed-point overﬂow, 40
Fixed-point method, 296–305,312–318
Fixed-point rounding, 41
Floating-point chopping, 47
Floating-point dynamic range, 42
Floating-point exponent, 42
Floating-point mantissa, 42
Floating-point normalization, 45
Floating-point number
representations, 42–47
Floating-point overﬂow, 43
Floating-point rounding, 43
Floating-point underﬂow, 43
Flop, 154
Floquet theory, 522
Forcing term, 548
FORTRAN, 38
Forward difference approximation, 401
Forward elimination, 156
Forward substitution
see Forward elimination
Fourier series, 18,73
Free space, 535
Fresnel coefﬁcients, 543
Frobenius norm, 140
Full rank matrix, 161
Function space, 4
Fundamental mode, 532
Fundamental theorem
of algebra, 254,484
Gamma function, 90
Gaussian function
see Gaussian pulse
Gaussian probability
density function, 413
Gaussian pulse, 92,556
Gauss-Seidel method, 181,527
Gauss-Seidel successive overrelaxation
(SOR) method, 181
TLFeBOOK

582
INDEX
Gauss transformation matrix, 148
Gauss vector, 149
Generalized eigenvectors, 485
Generalized Fourier series
expansion, 218
Generalized mean-value theorem, 79
Generating functions
Hermite polynomials, 227
Legendre polynomials, 233
Geometric series, 34
Gibbs phenomenon, 77
Givens matrix, 514
Global truncation error, 548
Golden ratio, 347
Golden ratio search method, 346
Gradient operator, 342
Greatest lower bound
see Inﬁmum
Group speed, 556
Haar condition, 239
Haar scaling function, 17,36
Haar wavelet, 17,36
Hankel matrix, 412
Harmonics, 162,532
Heat equation, 527
Henon map, 324
Hermite’s interpolation formula,
267–269,387
Hermite polynomials, 225–229
Hessenberg matrix, 205,512
Hessenberg reduction, 513
Hessian matrix, 344
Hilbert matrix, 133
Hilbert space, 68
Holder inequality, 139
Homogeneous material, 535
Householder transformation, 169–170
Hump phenomenon, 496
Hyperbolic partial differential
equation, 526
Hysteresis, 535
Ideal diode, 106
Idempotent matrix, 170
IEEE ﬂoating-point standard, 42
If and only if (iff), 32
Ill-conditioned matrix, 132
Increment function, 437
Incremental condition estimation
(ICE), 367
Inequality
Cauchy-Schwarz, 15
Holder, 10
Minkowski, 10
Schwarz, 24,36
Inﬁmum, 5
Initial conditions, 528–529
Initial value problem (IVP)
Adams-Bashforth (AB) methods
of solution, 459–461
Adams-Moulton (AM) methods
of solution, 461–462
Chaotic instability, 437
Deﬁnition, 415–416
Euler’s method of solution (explicit
form), 423,443
Euler method of solution (implicit
form), 427,443
Global error, 465
Heun’s method of solution, 433,453
Midpoint method of solution,
457–458
Model problem, 426,443
Multistep predictor-corrector
methods of solution, 456–457
Parasitic term, 459
Runge-Kutta methods of solution,
437–441
Runge-Kutta-Fehlberg (RKF)
methods of solution, 464–466
Single-step methods of solution, 455
Stability, 421,423,427,433,436,
445–447,458–459,462–463
Stability regions, 463
Stiff systems, 432,467–469
Trapezoidal method of solution, 436
Truncation error per step, 432–434
Inner product, 14
Inner product space, 14
TLFeBOOK

INDEX
583
Interface between media, 541
Integral mean-value theorem, 96
Integration between zeros, 385
Integration by parts, 90
Intermediate value theorem, 292,383
Interpolation
Hermite, 266–269,381
Lagrange, 252–257
Newton, 257–266
Polynomial, 251
Spline, 269–284
Interpolatory graphical display
algorithm (IGDA), 521
Invariance (2-norm), 168
Invariant subspace, 511
Inverse discrete Fourier transformi
(IDFT), 27
Inverse power algorithm, 503
Isotropic material, 535
Jacobi algorithm, 203
Jacobi method, 181,527
Jacobi overrelaxation
(JOR) method, 181
Jacobian matrix, 321
Jordan blocks, 484
Jordan canonical form, 483
Kernel function, 215
Key size, 324,329
Kirchoff’s laws, 418
Known plaintext attack, 331,339
Kronecker delta sequence, 22
L’Hopital’s rule, 81
Lagrange multipliers, 142,166,357–358
Lagrange polynomials, 254
Lagrangian function, 359
Laplace transform, 416
Law of cosines, 169
Leading principle submatrix, 151
Leapfrog algorithm, 551
Least upper bound
see Supremum
Least-squares, 127–128,132,161
Least-squares approximation using
orthogonal polynomials, 235
Lebesgue integrable functions, 67
Legendre polynomials, 229–235
Levinson-Durbin algorithm, 200
Limit of a sequence, 63
Line searches, 345–353
Linearly independent, 21
Lipschitz condition, 421
Lipschitz constant, 421
Lissajous ﬁgure, 478
Local minima, 343
Local minimizer, 365
Local truncation error, 545
Logistic equation, 436
Logistic map, 323,437
Lossless material, 539
Lower triangular matrix, 148
LU decomposition, 148
Machine epsilon, 53
Maclaurin series, 85
Magnetic conductivity, 540
Magnetic ﬂux density, 535
Manchester pulse, 17
Mathematical induction, 116
MATLAB, 34,54,90,102,117–118,126,
129,134–135,137,146,185,
186–191, 197–198,201,205–206,
244–246,248,287–289,292,336,
338–339,351,362–366,409–411,
413,442,465–467,469–472,
475–479,498,509,522–524,552,
557–559,561,563–564,565–577
Matrix exponential, 444,488–498
Matrix exponential condition
number, 497
Matrix norm, 140
Matrix norm equivalence, 141
Matrix operator, 176
Matrix spectrum, 509
TLFeBOOK

584
INDEX
Maxwell’s equations, 535,540
Mean value theorem, 79,303
Message sequence, 325
Method of false position, 333
Method of separation of variables, 530
Metric, 6
Metric induced by the norm, 11
Metric space, 6
Micromechanical resonator, 416
Minor, 489
Moments, 140,411
Natural spline, 275
Negative deﬁnite matrix, 365
Nesting property, 199
Newton’s method, 353–356
Newton’s method with backtracking
line search, 354–356
Newton-Raphson method breakdown
phenomena, 311–312
Newton-Raphson method, 305–312,
318–323
Newton-Raphson method rate of con-
vergence, 309–311
Nondefective matrix, 483
Nonexpansive mapping, 297
Nonlinear observer, 326
Non-return-to-zero (NRZ) pulse, 17
Norm, 10
Normal equations, 164
Normal matrix, 147,498
Normal mode, 532
Normed space, 10
Numerical dispersion, 550,555–556
Numerical integration
Chebyshev-Gauss quadrature, 389
Composite Simpson’s rule, 380
Composite trapezoidal rule, 376
Corrected trapezoidal rule, 394
Gaussian quadrature, 389
Hermite quadrature, 388
Left-point rule, 372
Legendre-Gauss quadrature, 391
Midpoint rule, 373,409
Rectangular rule, 372
Recursive trapezoidal rule, 398
Richardson’s extrapolation, 396
Right-point rule, 372
Romberg integration formula, 397
Romberg table, 397,399
Simpson’s rule, 378–385
Simpson’s rule truncation error,
380,383
Trapezoidal rule, 371–378
Trapezoidal rule truncation error for-
mula, 375–376
Objective function, 341
Orbit
see Trajectory
Order of a partial differential equation,
525
Ordinary differential equation
(ODE), 415
Orthogonality, 17
Orthogonal matrix, 161
Orthogonal polynomial, 208
Orthonormal set, 23
Overdetermined least-squares
approximation, 161
Overrelaxation method, 182
Overshoot, 76–77
Overtones
see Harmonics
Parabolic partial differential
equation, 526
Parallelogram equality, 15
Parametric ampliﬁer, 473
Parseval’s equality, 216
Partial differential equation (PDE), 525
PASCAL, 38
Perfectly matched layer (PML),
540,544,553
Permeability, 535
Permittivity, 535
Permutation matrix, 205
TLFeBOOK

INDEX
585
Persymmetry property, 199
Phase portrait, 454
Phase speed, 555
Phasor analysis, 539
Picard’s theorem, 422
Pivot, 151
Plane electromagnetic waves, 534–535
Plane waves, 536
Pointwise convergence, 71
Poisson equation, 526
Polynomial, 4
Positive deﬁnite matrix, 130
Positive semideﬁnite matrix, 130
Power method, 498–500
Power series, 94
Probability of false alarm, 333
Product method
see Method of separation of variables
Projection operator, 170,197
Propagation constant, 539
Pseudoinverse, 192
Pumping frequency, 473
QR decomposition, 161
QR factorization
see QR decomposition
QR iterations, 508–518
Quadratic form, 129,164
Quantization, 39
Quantization error, 40
Quartz crystal, 416
Radius of convergence, 95
Rate of convergence, 105,296
Ratio test, 233
Rayleigh quotient iteration, 508,523
Real Schur decomposition, 511
Reﬂection coefﬁcient, 543
Reﬂection coefﬁcients of a Toeplitz
matrix, 201
Regula falsi
see Method of false position
Relative permeability, 535
Relative permittivity, 535
Relative perturbations, 158
Residual reﬂections, 555
Residual vector, 144–145,168,180
Riemann integrable functions, 68–69
Ringing artifacts, 77
Rodrigues formula, 213,230
Rolle’s theorem, 265
Rosenbrock’s function, 342
Rotation operator, 112–113,120,484
Rounding errors, 38
Roundoff errors
see Rounding errors
Runge’s phenomenon, 257
Saddle point, 343
Scaled power algorithm, 580
Schelin’s theorem, 109
Second-order linear partial differential
equation, 526
Sequence, 3
Sequence of partial sums, 74,122
Sherman-Morrison-Woodbury
formula, 203
Shift parameter, 502,516
Similarity transformation, 487
Simultaneous diagonalizability, 520
Sine integral, 100
Single-shift QR iterations
algorithm, 517
Singular values, 147,165
Singular value decomposition
(SVD), 164
Singular vectors, 165
Stability, 157
State variables, 416
State vector, 325
Stationary point, 343
Stiffness quotient, 479
Stirling’s formula, 91
Strange attractor, 466
Strongly nonsingular matrix
see Strongly regular matrix
Strongly regular matrix, 201
TLFeBOOK

586
INDEX
Subharmonics, 162
Submultiplicative property of matrix
norms, 141
Summation by parts, 249
Spectral norm, 147
Spectral radius, 177
Spectrum, 532
Speed of light, 538
Supremum, 5
Symmetric part of a matrix, 203
Taylor series, 78–96
Taylor polynomial, 85
Taylor’s theorem, 440
Three-term recurrence formula (relation)
Deﬁnition, 208
Chebyshev polynomials of the ﬁrst
kind, 222
Chebyshev polynomials of the second
kind, 243
Gram polynomials, 249
Hermite polynomials, 229
Legendre polynomials, 234
Toeplitz matrix, 199
Trace of a matrix, 206
Trajectory, 454
Transmission coefﬁcient, 543
Transverse electromagnetic (TEM)
waves, 586
Tridiagonal matrix, 199,275,516
Truncation error, 85
Truncation error per step, 546
Two-dimensional Taylor series,
318–319
Tychonov regularization, 368
Uncertainty principle, 36
Underrelaxation method, 182
Uniform approximation, 222,238
Uniform convergence, 71
Unique global minimizer, 344
Unitary space, 12,16
Unit circle, 248,502
Unit roundoff, 53
Unit sphere, 141
Unit step function, 421
Unit vector, 17,170
UNIX, 565
Upper quasi-triangular matrix, 512
Upper triangular matrix, 148
Vandermonde matrix, 254,285
Varactor diode, 473
Vector electric ﬁeld intensity, 534
Vector magnetic ﬁeld intensity, 535
Vector, 9
Vector dot product, 16,48
Vector Hermitian transpose, 16
Vector norm, 138
Vector norm equivalence, 139
Vector space, 9
Vector subspace, 239
Vector transpose, 16
Vibrating string, 528–534
Von Neumann stability analysis, 548
Wavelength, 538
Wavelet series, 18
Wave equation
Electric ﬁeld, 536
Magnetic ﬁeld, 536
Weierstrass’ approximation
theorem, 216
Weierstrass function, 384
Weighted inner product, 207
Weighting function, 207
Well-conditioned matrix, 137
Zeroth order modiﬁed Bessel function
of the ﬁrst kind, 400
TLFeBOOK


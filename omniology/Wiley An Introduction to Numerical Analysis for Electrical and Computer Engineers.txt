TLFeBOOK

AN INTRODUCTION TO
NUMERICAL ANALYSIS
FOR ELECTRICAL AND
COMPUTER ENGINEERS
Christopher J. Zarowski
University of Alberta, Canada
A JOHN WILEY & SONS, INC. PUBLICATION
TLFeBOOK

TLFeBOOK

AN INTRODUCTION TO
NUMERICAL ANALYSIS
FOR ELECTRICAL AND
COMPUTER ENGINEERS
TLFeBOOK

TLFeBOOK

AN INTRODUCTION TO
NUMERICAL ANALYSIS
FOR ELECTRICAL AND
COMPUTER ENGINEERS
Christopher J. Zarowski
University of Alberta, Canada
A JOHN WILEY & SONS, INC. PUBLICATION
TLFeBOOK

Copyright c‚Éù2004 by John Wiley & Sons, Inc. All rights reserved.
Published by John Wiley & Sons, Inc., Hoboken, New Jersey.
Published simultaneously in Canada.
No part of this publication may be reproduced, stored in a retrieval system, or transmitted in any form
or by any means, electronic, mechanical, photocopying, recording, scanning, or otherwise, except as
permitted under Section 107 or 108 of the 1976 United States Copyright Act, without either the prior
written permission of the Publisher, or authorization through payment of the appropriate per-copy fee
to the Copyright Clearance Center, Inc., 222 Rosewood Drive, Danvers, MA 01923, 978-750-8400,
fax 978-646-8600, or on the Web at www.copyright.com. Requests to the Publisher for permission
should be addressed to the Permissions Department, John Wiley & Sons, Inc., 111 River Street,
Hoboken, NJ 07030, (201) 748-6011, fax (201) 748-6008.
Limit of Liability/Disclaimer of Warranty: While the publisher and author have used their best efforts
in preparing this book, they make no representations or warranties with respect to the accuracy or
completeness of the contents of this book and speciÔ¨Åcally disclaim any implied warranties of
merchantability or Ô¨Åtness for a particular purpose. No warranty may be created or extended by sales
representatives or written sales materials. The advice and strategies contained herein may not be
suitable for your situation. You should consult with a professional where appropriate. Neither the
publisher nor author shall be liable for any loss of proÔ¨Åt or any other commercial damages, including
but not limited to special, incidental, consequential, or other damages.
For general information on our other products and services, please contact our Customer Care
Department within the United States at 877-762-2974, outside the United States at 317-572-3993 or
fax 317-572-4002.
Wiley also publishes its books in a variety of electronic formats. Some content that appears in print,
however, may not be available in electronic format.
Library of Congress Cataloging-in-Publication Data:
Zarowski, Christopher J.
An introduction to numerical analysis for electrical and computer engineers / Christopher
J. Zarowski.
p. cm.
Includes bibliographical references and index.
ISBN 0-471-46737-5 (coth)
1. Electric engineering‚ÄîMathematics. 2. Computer science‚ÄîMathematics. 3. Numerical
analysis I. Title.
TK153.Z37 2004
621.3‚Ä≤01‚Ä≤518‚Äîdc22
2003063761
Printed in the United States of America.
10 9 8 7 6 5 4 3 2 1
TLFeBOOK

In memory of my mother
Lilian
and of my father
Walter
TLFeBOOK

TLFeBOOK

CONTENTS
Preface
xiii
1
Functional Analysis Ideas
1
1.1
Introduction
1
1.2
Some Sets
2
1.3
Some Special Mappings: Metrics, Norms, and Inner Products
4
1.3.1
Metrics and Metric Spaces
6
1.3.2
Norms and Normed Spaces
8
1.3.3
Inner Products and Inner Product Spaces
14
1.4
The Discrete Fourier Series (DFS)
25
Appendix 1.A
Complex Arithmetic
28
Appendix 1.B
Elementary Logic
31
References
32
Problems
33
2
Number Representations
38
2.1
Introduction
38
2.2
Fixed-Point Representations
38
2.3
Floating-Point Representations
42
2.4
Rounding Effects in Dot Product Computation
48
2.5
Machine Epsilon
53
Appendix 2.A
Review of Binary Number Codes
54
References
59
Problems
59
3
Sequences and Series
63
3.1
Introduction
63
3.2
Cauchy Sequences and Complete Spaces
63
3.3
Pointwise Convergence and Uniform Convergence
70
3.4
Fourier Series
73
3.5
Taylor Series
78
vii
TLFeBOOK

viii
CONTENTS
3.6
Asymptotic Series
97
3.7
More on the Dirichlet Kernel
103
3.8
Final Remarks
107
Appendix 3.A
COordinate Rotation DI gital Computing
(CORDIC)
107
3.A.1
Introduction
107
3.A.2
The Concept of a Discrete Basis
108
3.A.3
Rotating Vectors in the Plane
112
3.A.4
Computing Arctangents
114
3.A.5
Final Remarks
115
Appendix 3.B
Mathematical Induction
116
Appendix 3.C
Catastrophic Cancellation
117
References
119
Problems
120
4
Linear Systems of Equations
127
4.1
Introduction
127
4.2
Least-Squares Approximation and Linear Systems
127
4.3
Least-Squares Approximation and Ill-Conditioned Linear
Systems
132
4.4
Condition Numbers
135
4.5
LU Decomposition
148
4.6
Least-Squares Problems and QR Decomposition
161
4.7
Iterative Methods for Linear Systems
176
4.8
Final Remarks
186
Appendix 4.A
Hilbert Matrix Inverses
186
Appendix 4.B
SVD and Least Squares
191
References
193
Problems
194
5
Orthogonal Polynomials
207
5.1
Introduction
207
5.2
General Properties of Orthogonal Polynomials
207
5.3
Chebyshev Polynomials
218
5.4
Hermite Polynomials
225
5.5
Legendre Polynomials
229
5.6
An Example of Orthogonal Polynomial Least-Squares
Approximation
235
5.7
Uniform Approximation
238
TLFeBOOK

CONTENTS
ix
References
241
Problems
241
6
Interpolation
251
6.1
Introduction
251
6.2
Lagrange Interpolation
252
6.3
Newton Interpolation
257
6.4
Hermite Interpolation
266
6.5
Spline Interpolation
269
References
284
Problems
285
7
Nonlinear Systems of Equations
290
7.1
Introduction
290
7.2
Bisection Method
292
7.3
Fixed-Point Method
296
7.4
Newton‚ÄìRaphson Method
305
7.4.1
The Method
305
7.4.2
Rate of Convergence Analysis
309
7.4.3
Breakdown Phenomena
311
7.5
Systems of Nonlinear Equations
312
7.5.1
Fixed-Point Method
312
7.5.2
Newton‚ÄìRaphson Method
318
7.6
Chaotic Phenomena and a Cryptography Application
323
References
332
Problems
333
8
Unconstrained Optimization
341
8.1
Introduction
341
8.2
Problem Statement and Preliminaries
341
8.3
Line Searches
345
8.4
Newton‚Äôs Method
353
8.5
Equality Constraints and Lagrange Multipliers
357
Appendix 8.A
MATLAB Code for Golden Section Search
362
References
364
Problems
364
9
Numerical Integration and Differentiation
369
9.1
Introduction
369
TLFeBOOK

x
CONTENTS
9.2
Trapezoidal Rule
371
9.3
Simpson‚Äôs Rule
378
9.4
Gaussian Quadrature
385
9.5
Romberg Integration
393
9.6
Numerical Differentiation
401
References
406
Problems
406
10
Numerical Solution of Ordinary Differential Equations
415
10.1
Introduction
415
10.2
First-Order ODEs
421
10.3
Systems of First-Order ODEs
442
10.4
Multistep Methods for ODEs
455
10.4.1
Adams‚ÄìBashforth Methods
459
10.4.2
Adams‚ÄìMoulton Methods
461
10.4.3
Comments on the Adams Families
462
10.5
Variable-Step-Size (Adaptive) Methods for ODEs
464
10.6
Stiff Systems
467
10.7
Final Remarks
469
Appendix 10.A
MATLAB Code for Example 10.8
469
Appendix 10.B
MATLAB Code for Example 10.13
470
References
472
Problems
473
11
Numerical Methods for Eigenproblems
480
11.1
Introduction
480
11.2
Review of Eigenvalues and Eigenvectors
480
11.3
The Matrix Exponential
488
11.4
The Power Methods
498
11.5
QR Iterations
508
References
518
Problems
519
12
Numerical Solution of Partial Differential Equations
525
12.1
Introduction
525
12.2
A Brief Overview of Partial Differential Equations
525
12.3
Applications of Hyperbolic PDEs
528
12.3.1
The Vibrating String
528
12.3.2
Plane Electromagnetic Waves
534
TLFeBOOK

CONTENTS
xi
12.4
The Finite-Difference (FD) Method
545
12.5
The Finite-Difference Time-Domain (FDTD) Method
550
Appendix 12.A
MATLAB Code for Example 12.5
557
References
560
Problems
561
13
An Introduction to MATLAB
565
13.1
Introduction
565
13.2
Startup
565
13.3
Some Basic Operators, Operations, and Functions
566
13.4
Working with Polynomials
571
13.5
Loops
572
13.6
Plotting and M-Files
573
References
577
Index
579
TLFeBOOK

TLFeBOOK

PREFACE
The subject of numerical analysis has a long history. In fact, it predates by cen-
turies the existence of the modern computer. Of course, the advent of the modern
computer in the middle of the twentieth century gave greatly added impetus to the
subject, and so it now plays a central role in a large part of engineering analysis,
simulation, and design. This is so true that no engineer can be deemed competent
without some knowledge and understanding of the subject. Because of the back-
ground of the author, this book tends to emphasize issues of particular interest to
electrical and computer engineers, but the subject (and the present book) is certainly
relevant to engineers from all other branches of engineering.
Given the importance level of the subject, a great number of books have already
been written about it, and are now being written. These books span a colossal
range of approaches, levels of technical difÔ¨Åculty, degree of specialization, breadth
versus depth, and so on. So, why should this book be added to the already huge,
and growing list of available books?
To begin, the present book is intended to be a part of the students‚Äô Ô¨Årst exposure
to numerical analysis. As such, it is intended for use mainly in the second year
of a typical 4-year undergraduate engineering program. However, the book may
Ô¨Ånd use in later years of such a program. Generally, the present book arises out of
the author‚Äôs objections to educational practice regarding numerical analysis. To be
more speciÔ¨Åc
1. Some books adopt a ‚Äúgrocery list‚Äù or ‚Äúrecipes‚Äù approach (i.e., ‚Äúmethods‚Äù at
the expense of ‚Äúanalysis‚Äù) wherein several methods are presented, but with
little serious discussion of issues such as how they are obtained and their
relative advantages and disadvantages. In this genre often little consideration
is given to error analysis, convergence properties, or stability issues. When
these issues are considered, it is sometimes in a manner that is too superÔ¨Åcial
for contemporary and future needs.
2. Some books fail to build on what the student is supposed to have learned
prior to taking a numerical analysis course. For example, it is common for
engineering students to take a Ô¨Årst-year course in matrix/linear algebra. Yet,
a number of books miss the opportunity to build on this material in a manner
that would provide a good bridge from Ô¨Årst year to more sophisticated uses
of matrix/linear algebra in later years (e.g., such as would be found in digital
signal processing or state variable control systems courses).
xiii
TLFeBOOK

xiv
PREFACE
3. Some books miss the opportunity to introduce students to the now quite vital
area of functional analysis ideas as applied to engineering problem solving.
Modern numerical analysis relies heavily on concepts such as function spaces,
orthogonality, norms, metrics, and inner products. Yet these concepts are
often considered in a very ad hoc way, if indeed they are considered at all.
4. Some books tie the subject matter of numerical analysis far too closely to
particular software tools and/or programming languages. But the highly tran-
sient nature of software tools and programming languages often blinds the
user to the timeless nature of the underlying principles of analysis. Further-
more, it is an erroneous belief that one can successfully employ numerical
methods solely through the use of ‚Äúcanned‚Äù software without any knowledge
or understanding of the technical details of the contents of the can. While
this does not imply the need to understand a software tool or program down
to the last line of code, it does rule out the ‚Äúblack box‚Äù methodology.
5. Some books avoid detailed analysis and derivations in the misguided belief
that this will make the subject more accessible to the student. But this denies
the student the opportunity to learn an important mode of thinking that is a
huge aid to practical problem solving. Furthermore, by cutting the student
off from the language associated with analysis the student is prevented from
learning those skills needed to read modern engineering literature, and to
extract from this literature those things that are useful for solving the problem
at hand.
The prospective user of the present book will likely notice that it contains material
that, in the past, was associated mainly with more advanced courses. However, the
history of numerical computing since the early 1980s or so has made its inclusion
in an introductory course unavoidable. There is nothing remarkable about this. For
example, the material of typical undergraduate signals and systems courses was,
not so long ago, considered to be suitable only for graduate-level courses. Indeed,
most (if not all) of the contents of any undergraduate program consists of material
that was once considered far too advanced for undergraduates, provided one goes
back far enough in time.
Therefore, with respect to the observations mentioned above, the following is a
summary of some of the features of the present book:
1. An axiomatic approach to function spaces is adopted within the Ô¨Årst chapter.
So the book immediately exposes the student to function space ideas, espe-
cially with respect to metrics, norms, inner products, and the concept of
orthogonality in a general setting. All of this is illustrated by several examples,
and the basic ideas from the Ô¨Årst chapter are reinforced by routine use
throughout the remaining chapters.
2. The present book is not closely tied to any particular software tool or pro-
gramming language, although a few MATLAB-oriented examples are pre-
sented. These may be understood without any understanding of MATLAB
TLFeBOOK

PREFACE
xv
(derived from the term matrix laboratory) on the part of the student, how-
ever. Additionally, a quick introduction to MATLAB is provided in Chapter
13. These examples are simply intended to illustrate that modern software
tools implement many of the theories presented in the book, and that the
numerical characteristics of algorithms implemented with such tools are not
materially different from algorithm implementations using older software
technologies (e.g., catastrophic convergence, and ill conditioning, continue
to be major implementation issues). Algorithms are often presented in a
Pascal-like pseudocode that is sufÔ¨Åciently transparent and general to allow
the user to implement the algorithm in the language of their choice.
3. Detailed proofs and/or derivations are often provided for many key results.
However, not all theorems or algorithms are proved or derived in detail
on those occasions where to do so would consume too much space, or not
provide much insight. Of course, the reader may dispute the present author‚Äôs
choices in this matter. But when a proof or derivation is omitted, a reference
is often cited where the details may be found.
4. Some modern applications examples are provided to illustrate the conse-
quences of various mathematical ideas. For example, chaotic cryptography,
the CORDIC (coordinate rotational digital computing) method, and least
squares for system identiÔ¨Åcation (in a biomedical application) are considered.
5. The sense in which series and iterative processes converge is given fairly
detailed treatment in this book as an understanding of these matters is now
so crucial in making good choices about which algorithm to use in an appli-
cation. Thus, for example, the difference between pointwise and uniform
convergence is considered. Kernel functions are introduced because of their
importance in error analysis for approximations based on orthogonal series.
Convergence rate analysis is also presented in the context of root-Ô¨Ånding
algorithms.
6. Matrix analysis is considered in sufÔ¨Åcient depth and breadth to provide an
adequate introduction to those aspects of the subject particularly relevant to
modern areas in which it is applied. This would include (but not be limited
to) numerical methods for electromagnetics, stability of dynamic systems,
state variable control systems, digital signal processing, and digital commu-
nications.
7. The most important general properties of orthogonal polynomials are pre-
sented. The special cases of Chebyshev, Legendre, and Hermite polynomials
are considered in detail (i.e., detailed derivations of many basic properties
are given).
8. In treating the subject of the numerical solution of ordinary differential
equations, a few books fail to give adequate examples based on nonlin-
ear dynamic systems. But many examples in the present book are based on
nonlinear problems (e.g., the DufÔ¨Ång equation). Furthermore, matrix methods
are introduced in the stability analysis of both explicit and implicit methods
for nth-order systems. This is illustrated with second-order examples.
TLFeBOOK

xvi
PREFACE
Analysis is often embedded in the main body of the text rather than being rele-
gated to appendixes, or to formalized statements of proof immediately following a
theorem statement. This is done to discourage attempts by the reader to ‚Äúskip over
the math.‚Äù After all, skipping over the math defeats the purpose of the book.
Notwithstanding the remarks above, the present book lacks the rigor of a math-
ematically formal treatment of numerical analysis. For example, Lebesgue measure
theory is entirely avoided (although it is mentioned in passing). With respect to
functional analysis, previous authors (e.g., E. Kreyszig, Introductory Functional
Analysis with Applications) have demonstrated that it is very possible to do this
while maintaining adequate rigor for engineering purposes, and this approach is
followed here.
It is largely left to the judgment of the course instructor about what particular
portions of the book to cover in a course. Certainly there is more material here
than can be covered in a single term (or semester). However, it is recommended
that the Ô¨Årst four chapters be covered largely in their entirety (perhaps excepting
Sections 1.4, 3.6, 3.7, and the part of Section 4.6 regarding SVD). The material of
these chapters is simply too fundamental to be omitted, and is often drawn on in
later chapters.
Finally, some will say that topics such as function spaces, norms and inner
products, and uniform versus pointwise convergence, are too abstract for engineers.
Such individuals would do well to ask themselves in what way these ideas are
more abstract than Boolean algebra, convolution integrals, and Fourier or Laplace
transforms, all of which are standard fare in present-day electrical and computer
engineering curricula.
Engineering  past
Engineering  present
Engineering  future
Christopher Zarowski
TLFeBOOK

1
Functional Analysis Ideas
1.1
INTRODUCTION
Many engineering analysis and design problems are far too complex to be solved
without the aid of computers. However, the use of computers in problem solving
has made it increasingly necessary for users to be highly skilled in (practical)
mathematical analysis. There are a number of reasons for this. A few are as follows.
For one thing, computers represent data to Ô¨Ånite precision. Irrational numbers
such as œÄ or
‚àö
2 do not have an exact representation on a digital computer (with the
possible exception of methods based on symbolic computing). Additionally, when
arithmetic is performed, errors occur as a result of rounding (e.g., the truncation of
the product of two n-bit numbers, which might be 2n bits long, back down to n
bits). Numbers have a limited dynamic range; we might get overÔ¨Çow or underÔ¨Çow
in a computation. These are examples of Ô¨Ånite-precision arithmetic effects. Beyond
this, computational methods frequently have sources of error independent of these.
For example, an inÔ¨Ånite series must be truncated if it is to be evaluated on a com-
puter. The truncation error is something ‚Äúadditional‚Äù to errors from Ô¨Ånite-precision
arithmetic effects. In all cases, the sources (and sizes) of error in a computation
must be known and understood in order to make sensible claims about the accuracy
of a computer-generated solution to a problem.
Many methods are ‚Äúiterative.‚Äù Accuracy of the result depends on how many
iterations are performed. It is possible that a given method might be very slow,
requiring many iterations before achieving acceptable accuracy. This could involve
much computer runtime. The obvious solution of using a faster computer is usually
unacceptable. A better approach is to use mathematical analysis to understand why
a method is slow, and so to devise methods of speeding it up. Thus, an important
feature of analysis applied to computational methods is that of assessing how
much in the way of computing resources is needed by a given method. A given
computational method will make demands on computer memory, operations count
(the number of arithmetic operations, function evaluations, data transfers, etc.),
number of bits in a computer word, and so on.
A given problem almost always has many possible alternative solutions. Other
than accuracy and computer resource issues, ease of implementation is also rel-
evant. This is a human labor issue. Some methods may be easier to implement
on a given set of computing resources than others. This would have an impact
An Introduction to Numerical Analysis for Electrical and Computer Engineers, by C.J. Zarowski
ISBN 0-471-46737-5
c‚Éù2004 John Wiley & Sons, Inc.
1
TLFeBOOK

2
FUNCTIONAL ANALYSIS IDEAS
on software/hardware development time, and hence on system cost. Again, math-
ematical analysis is useful in deciding on the relative ease of implementation of
competing solution methods.
The subject of numerical computing is truly vast. Methods are required to handle
an immense range of problems, such as solution of differential equations (ordi-
nary or partial), integration, solution of equations and systems of equations (linear
or nonlinear), approximation of functions, and optimization. These problem types
appear to be radically different from each other. In some sense the differences
between them are true, but there are means to achieve some unity of approach in
understanding them.
The branch of mathematics that (perhaps) gives the greatest amount of unity
is sometimes called functional analysis. We shall employ ideas from this subject
throughout. However, our usage of these ideas is not truly rigorous; for example,
we completely avoid topology, and measure theory. Therefore, we tend to follow
simpliÔ¨Åed treatments of the subject such as Kreyszig [1], and then only those ideas
that are immediately relevant to us. The reader is assumed to be very comfortable
with elementary linear algebra, and calculus. The reader must also be comfortable
with complex number arithmetic (see Appendix 1.A now for a review if necessary).
Some knowledge of electric circuit analysis is presumed since this will provide
a source of applications examples later. (But application examples will also be
drawn from other sources.) Some knowledge of ordinary differential equations is
also assumed.
It is worth noting that an understanding of functional analysis is a tremendous
aid to understanding other subjects such as quantum physics, probability theory
and random processes, digital communications system analysis and design, digital
control systems analysis and design, digital signal processing, fuzzy systems, neural
networks, computer hardware design, and optimal design of systems. Many of the
ideas presented in this book are also intended to support these subjects.
1.2
SOME SETS
Variables in an engineering problem often take on values from sets of numbers.
In the present setting, the sets of greatest interest to us are (1) the set of integers
Z = {. . . ‚àí3, ‚àí2, ‚àí1, 0, 1, 2, 3 . . .}, (2) the set of real numbers R, and (3) the set of
complex numbers C = {x + jy|j =
‚àö
‚àí1, x, y ‚ààR}. The set of nonnegative inte-
gers is Z+ = {0, 1, 2, 3, . . . , } (so Z+ ‚äÇZ). Similarly, the set of nonnegative real
numbers is R+ = {x ‚ààR|x ‚â•0}. Other kinds of sets of numbers will be introduced
if and when they are needed.
If A and B are two sets, their Cartesian product is denoted by A √ó B =
{(a, b)|a ‚ààA, b ‚ààB}. The Cartesian product of n sets denoted A0, A1, . . . , An‚àí1
is A0 √ó A1 √ó ¬∑ ¬∑ ¬∑ √ó An‚àí1 = {(a0, a1, . . . , an‚àí1)|ak ‚ààAk}.
Ideas from matrix/linear algebra are of great importance. We are therefore also
interested in sets of vectors. Thus, Rn shall denote the set of n-element vectors
with real-valued components, and similarly, Cn shall denote the set of n-element
TLFeBOOK

SOME SETS
3
vectors with complex-valued components. By default, we assume any vector x to
be a column vector:
x =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
x0
x1
...
xn‚àí2
xn‚àí1
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
(1.1)
Naturally, row vectors are obtained by transposition. We will generally avoid using
bars over or under symbols to denote vectors. Whether a quantity is a vector will
be clear from the context of the discussion. However, bars will be used to denote
vectors when this cannot be easily avoided. The indexing of vector elements xk will
often begin with 0 as indicated in (1.1). Naturally, matrices are also important. Set
Rn√óm denotes the set of matrices with n rows and m columns, and the elements are
real-valued. The notation Cn√óm should now possess an obvious meaning. Matri-
ces will be denoted by uppercase symbols, again without bars. If A is an n √ó m
matrix, then
A = [ap,q]p=0,...,n‚àí1, q=0,...,m‚àí1.
(1.2)
Thus, the element in row p and column q of A is denoted ap,q. Indexing of rows
and columns again will typically begin at 0. The subscripts on the right bracket ‚Äú]‚Äù
in (1.2) will often be omitted in the future. We may also write apq instead of ap,q
where no danger of confusion arises.
The elements of any vector may be regarded as the elements of a sequence of
Ô¨Ånite length. However, we are also very interested in sequences of inÔ¨Ånite length.
An inÔ¨Ånite sequence may be denoted by x = (xk) = (x0, x1, x2, . . .), for which xk
could be either real-valued or complex-valued. It is possible for sequences to be
doubly inÔ¨Ånite, for instance, x = (xk) = (. . . , x‚àí2, x‚àí1, x0, x1, x2, . . .).
Relationships between variables are expressed as mathematical functions, that is,
mappings between sets. The notation f |A ‚ÜíB signiÔ¨Åes that function f associates
an element of set A with an element from set B. For example, f |R ‚ÜíR represents
a function deÔ¨Åned on the real-number line, and this function is also real-valued;
that is, it maps ‚Äúpoints‚Äù in R to ‚Äúpoints‚Äù in R. We are familiar with the idea
of ‚Äúplotting‚Äù such a function on the xy plane if y = f (x) (i.e., x, y ‚ààR). It is
important to note that we may regard sequences as functions that are deÔ¨Åned on
either the set Z (the case of doubly inÔ¨Ånite sequences), or the set Z+ (the case
of singly inÔ¨Ånite sequences). To be more speciÔ¨Åc, if, for example, k ‚ààZ+, then
this number maps to some number xk that is either real-valued or complex-valued.
Since vectors are associated with sequences of Ô¨Ånite length, they, too, may be
regarded as functions, but deÔ¨Åned on a Ô¨Ånite subset of the integers. From (1.1) this
subset might be denoted by Zn = {0, 1, 2, . . . , n ‚àí2, n ‚àí1}.
Sets of functions are important. This is because in engineering we are often
interested in mappings between sets of functions. For example, in electric circuits
voltage and current waveforms (i.e., functions of time) are input to a circuit via volt-
age and current sources. Voltage drops across circuit elements, or currents through
TLFeBOOK

4
FUNCTIONAL ANALYSIS IDEAS
circuit elements are output functions of time. Thus, any circuit maps functions from
an input set to functions from some output set. Digital signal processing systems
do the same thing, except that here the functions are sequences. For example, a
simple digital signal processing system might accept as input the sequence (xn),
and produce as output the sequence (yn) according to
yn = xn + xn+1
2
(1.3)
for which n ‚ààZ+.
Some speciÔ¨Åc examples of sets of functions are as follows, and more will be
seen later. The set of real-valued functions deÔ¨Åned on the interval [a, b] ‚äÇR that
are n times continuously differentiable may be denoted by Cn[a, b]. This means
that all derivatives up to and including order n exist and are continuous. If n = 0
we often just write C[a, b], which is the set of continuous functions on the interval
[a, b]. We remark that the notation [a, b] implies inclusion of the endpoints of the
interval. Thus, (a, b) implies that the endpoints a and b are not to be included [i.e.,
if x ‚àà(a, b), then a < x < b].
A polynomial in the indeterminate x of degree n is
pn(x) =
n

k=0
pn,kxk.
(1.4)
Unless otherwise stated, we will always assume pn,k ‚ààR. The indeterminate x
is often considered to be either a real number or a complex number. But in
some circumstances the indeterminate x is merely regarded as a ‚Äúplaceholder,‚Äù
which means that x is not supposed to take on a value. In a situation like this
the polynomial coefÔ¨Åcients may also be regarded as elements of a vector (e.g.,
pn = [pn,0 pn,1 ¬∑ ¬∑ ¬∑ pn,n]T ). This happens in digital signal processing when we
wish to convolve1 sequences of Ô¨Ånite length, because the multiplication of polyno-
mials is mathematically equivalent to the operation of sequence convolution. We
will denote the set of all polynomials of degree n as Pn. If x is to be from the
interval [a, b] ‚äÇR, then the set of polynomials of degree n on [a, b] is denoted
by Pn[a, b]. If m < n we shall usually assume Pm[a, b] ‚äÇPn[a, b].
1.3
SOME SPECIAL MAPPINGS: METRICS, NORMS,
AND INNER PRODUCTS
Sets of objects (vectors, sequences, polynomials, functions, etc.) often have cer-
tain special mappings deÔ¨Åned on them that turn these sets into what are commonly
called function spaces. Loosely speaking, functional analysis is about the properties
1These days it seems that the operation of convolution is Ô¨Årst given serious study in introductory signals
and systems courses. The operation of convolution is fundamental to all forms of signal processing,
either analog or digital.
TLFeBOOK

SOME SPECIAL MAPPINGS: METRICS, NORMS, AND INNER PRODUCTS
5
of function spaces. Generally speaking, numerical computation problems are best
handled by treating them in association with suitable mappings on well-chosen
function spaces. For our purposes, the three most important special types of map-
pings are (1) metrics, (2) norms, and (3) inner products. You are likely to be already
familiar with special cases of these really very general ideas.
The vector dot product is an example of an inner product on a vector space, while
the Euclidean norm (i.e., the square root of the sum of the squares of the elements in
a real-valued vector) is a norm on a vector space. The Euclidean distance between
two vectors (given by the Euclidean norm of the difference between the two vectors)
is a metric on a vector space. Again, loosely speaking, metrics give meaning to the
concept of ‚Äúdistance‚Äù between points in a function space, norms give a meaning
to the concept of the ‚Äúsize‚Äù of a vector, and inner products give meaning to the
concept of ‚Äúdirection‚Äù in a vector space.2
In Section 1.1 we expressed interest in the sizes of errors, and so naturally the
concept of a norm will be of interest. Later we shall see that inner products will
prove to be useful in devising means of overcoming problems due to certain sources
of error in a computation. In this section we shall consider various examples of
function spaces, some of which we will work with later on in the analysis of
certain computational problems. We shall see that there are many different kinds
of metric, norm, and inner product. Each kind has its own particular advantages
and disadvantages as will be discovered as we progress through the book.
Sometimes a quantity cannot be computed exactly. In this case we may try to
estimate bounds on the size of the quantity. For example, Ô¨Ånding the exact error
in the truncation of a series may be impossible, but putting a bound on the error
might be relatively easy. In this respect the concepts of supremum and inÔ¨Åmum
can be important. These are deÔ¨Åned as follows.
Suppose we have E ‚äÇR. We say that E is bounded above if E has an upper
bound, that is, if there exists a B ‚ààR such that x ‚â§B for all x ‚ààE. If E Ã∏= ‚àÖ
(empty set; set containing no elements) there is a supremum of E [also called a
least upper bound (lub)], denoted
sup E.
For example, suppose E = [0, 1), then any B ‚â•1 is an upper bound for E, but
sup E = 1. More generally, sup E ‚â§B for every upper bound B of E. Thus, the
supremum is a ‚Äútight‚Äù upper bound. Similarly, E may be bounded below. If E has
a lower bound there is a b ‚ààR such that x ‚â•b for all x ‚ààE. If E Ã∏= ‚àÖ, then there
exists an inÔ¨Åmum [also called a greatest lower bound (glb)], denoted by
inf E.
For example, suppose now E = (0, 1]; then any b ‚â§0 is a lower bound for E,
but inf E = 0. More generally, inf E ‚â•b for every lower bound b of E. Thus, the
inÔ¨Åmum is a ‚Äútight‚Äù lower bound.
2The idea of ‚Äúdirection‚Äù is (often) considered with respect to the concept of an orthogonal basis in a
vector space. To deÔ¨Åne ‚Äúorthogonality‚Äù requires the concept of an inner product. We shall consider this
in various ways later on.
TLFeBOOK

6
FUNCTIONAL ANALYSIS IDEAS
1.3.1
Metrics and Metric Spaces
In mathematics an axiomatic approach is often taken in the development of analysis
methods. This means that we deÔ¨Åne a set of objects, a set of operations to be
performed on the set of objects, and rules obeyed by the operations. This is typically
how mathematical systems are constructed. The reader (hopefully) has already seen
this approach in the application of Boolean algebra to the analysis and design of
digital electronic systems (i.e., digital logic). We adopt the same approach here.
We will begin with the following deÔ¨Ånition.
DeÔ¨Ånition 1.1: Metric Space, Metric
A metric space is a set X and a
function d|X √ó X ‚ÜíR+, which is called a metric or distance function on X.
If x, y, z ‚ààX then d satisÔ¨Åes the following axioms:
(M1)
d(x, y) = 0 if and only if (iff) x = y.
(M2)
d(x, y) = d(y, x) (symmetry property).
(M3)
d(x, y) ‚â§d(x, z) + d(z, y) (triangle inequality).
We emphasize that X by itself cannot be a metric space until we deÔ¨Åne d. Thus,
the metric space is often denoted by the pair (X, d). The phrase ‚Äúif and only
if‚Äù probably needs some explanation. In (M1), if you were told that d(x, y) = 0,
then you must immediately conclude that x = y. Conversely, if you were told that
x = y, then you must immediately conclude that d(x, y) = 0. Instead of the words
‚Äúif and only if‚Äù it is also common to write
d(x, y) = 0 ‚áîx = y.
The phrase ‚Äúif and only if‚Äù is associated with elementary logic. This subject is
reviewed in Appendix 1.B. It is recommended that the reader study that appendix
before continuing with later chapters.
Some examples of metric spaces now follow.
Example 1.1
Set X = R, with
d(x, y) = |x ‚àíy|
(1.5)
forms a metric space. The metric (1.5) is what is commonly meant by the ‚Äúdistance
between two points on the real number line.‚Äù The metric (1.5) is quite useful in
discussing the sizes of errors due to rounding in digital computation. This is because
there is a norm on R that gives rise to the metric in (1.5) (see Section 1.3.2).
Example 1.2
The set of vectors Rn with
d(x, y) =
n‚àí1

k=0
[xk ‚àíyk]2
	1/2
(1.6a)
TLFeBOOK

SOME SPECIAL MAPPINGS: METRICS, NORMS, AND INNER PRODUCTS
7
forms a (Euclidean) metric space. However, another valid metric on Rn is given by
d1(x, y) =
n‚àí1

k=0
|xk ‚àíyk|.
(1.6b)
In other words, we can have the metric space (X, d), or (X, d1). These spaces are
different because their metrics differ.
Euclidean metrics, and their related norms and inner products, are useful in pos-
ing and solving least-squares approximation problems. Least-squares approximation
is a topic we shall consider in detail later.
Example 1.3
Consider the set of (singly) inÔ¨Ånite, complex-valued, and bounded
sequences
X = {x = (x0, x1, x2, . . .)|xk ‚ààC, |xk| ‚â§c(x)(all k)}.
(1.7a)
Here c(x) ‚â•0 is a bound that may depend on x, but not on k. This set forms a
metric space that may be denoted by l‚àû[0, ‚àû] if we employ the metric
d(x, y) = sup
k‚ààZ+ |xk ‚àíyk|.
(1.7b)
The notation [0, ‚àû] emphasizes that the sequences we are talking about are only
singly inÔ¨Ånite. We would use [‚àí‚àû, ‚àû] to specify that we are talking about doubly
inÔ¨Ånite sequences.
Example 1.4
DeÔ¨Åne J = [a, b] ‚äÇR. The set C[a, b] will be a metric space if
d(x, y) = sup
t‚ààJ
|x(t) ‚àíy(t)|.
(1.8)
In Example 1.1 the metric (1.5) gives the ‚Äúdistance‚Äù between points on the real-
number line. In Example 1.4 the ‚Äúpoints‚Äù are real-valued, continuous functions of
t ‚àà[a, b]. In functional analysis it is essential to get used to the idea that functions
can be considered as points in a space.
Example 1.5
The set X in (1.7a), where we now allow c(x) ‚Üí‚àû(in other
words, the sequence need not be bounded here), but with the metric
d(x, y) =
‚àû

k=0
1
2k+1
|xk ‚àíyk|
1 + |xk ‚àíyk|
(1.9)
is a metric space. (Sometimes this space is denoted s.)
TLFeBOOK

8
FUNCTIONAL ANALYSIS IDEAS
Example 1.6
Let p be a real-valued constant such that p ‚â•1. Consider the
set of complex-valued sequences
X =

x = (x0, x1, x2, . . .)|xk ‚ààC,
‚àû

k=0
|xk|p < ‚àû

.
(1.10a)
This set together with the metric
d(x, y) =
 ‚àû

k=0
|xk ‚àíyk|p
	1/p
(1.10b)
forms a metric space that we denote by lp[0, ‚àû].
Example 1.7
Consider the set of complex-valued functions on [a, b] ‚äÇR
X =

x(t)

 b
a
|x(t)|2 dt < ‚àû

(1.11a)
for which
d(x, y) =
 b
a
|x(t) ‚àíy(t)|2 dt
1/2
(1.11b)
is a metric. Pair (X, d) forms a metric space that is usually denoted by L2[a, b].
The metric space of Example 1.7 (along with certain variations) is very impor-
tant in the theory of orthogonal polynomials, and in least-squares approximation
problems. This is because it turns out to be an inner product space too (see
Section 1.3.3). Orthogonal polynomials have a major role to play in the solution
of least squares, and other types of approximation problem.
All of the metrics deÔ¨Åned in the examples above may be shown to satisfy the
axioms of DeÔ¨Ånition 1.1. Of course, at least in some cases, much effort might be
required to do this. In this book we largely avoid making this kind of effort.
1.3.2
Norms and Normed Spaces
So far our examples of function spaces have been metric spaces (Section 1.3.1).
Such spaces are not necessarily associated with the concept of a vector space.
However, normed spaces (i.e., spaces with norms deÔ¨Åned on them) are always
associated with vector spaces. So, before we can deÔ¨Åne a norm, we need to recall
the general deÔ¨Ånition of a vector space.
The following deÔ¨Ånition invokes the concept of a Ô¨Åeld of numbers. This concept
arises in abstract algebra and number theory [e.g., 2, 3], a subject we wish to avoid
considering here.3 It is enough for the reader to know that R and C are Ô¨Åelds under
3This avoidance is not to disparage abstract algebra. This subject is a necessary prerequisite to under-
standing concepts such as fast algorithms for digital signal processing (i.e., fast Fourier transforms, and
fast convolution algorithms; e.g., see Ref. 4), cryptography and data security, and error control codes
for digital communications.
TLFeBOOK

SOME SPECIAL MAPPINGS: METRICS, NORMS, AND INNER PRODUCTS
9
the usual real and complex arithmetic operations. These are really the only Ô¨Åelds
that we shall work with. We remark, largely in passing, that rational numbers (set
denoted Q) are also a Ô¨Åeld under the usual arithmetic operations.
DeÔ¨Ånition 1.2: Vector Space
A vector space (linear space) over a Ô¨Åeld K is
a nonempty set X of elements x, y, z, . . . called vectors together with two algebraic
operations. These operations are vector addition, and the multiplication of vectors
by scalars that are elements of K. The following axioms must be satisÔ¨Åed:
(V1) If x, y ‚ààX, then x + y ‚ààX (additive closure).
(V2) If x, y, z ‚ààX, then (x + y) + z = x + (y + z) (associativity).
(V3) There exists a vector in X denoted 0 (zero vector) such that for all x ‚ààX,
we have x + 0 = 0 + x = x.
(V4) For all x ‚ààX, there is a vector ‚àíx ‚ààX such that ‚àíx + x = x +
(‚àíx) = 0. We call ‚àíx the negative of a vector.
(V5) For all x, y ‚ààX we have x + y = y + x (commutativity).
(V6) If x ‚ààX and a ‚ààK, then the product of a and x is ax, and ax ‚ààX.
(V7) If x, y ‚ààX, and a ‚ààK, then a(x + y) = ax + ay.
(V8) If a, b ‚ààK, and x ‚ààX, then (a + b)x = ax + bx.
(V9) If a, b ‚ààK, and x ‚ààX, then ab(x) = a(bx).
(V10) If x ‚ààX, and 1 ‚ààK, then 1x = x multiplication of a vector by a unit
scalar; all Ô¨Åelds contain a unit scalar (i.e., a number called ‚Äúone‚Äù).
In this deÔ¨Ånition, as already noted, we generally work only with K = R, or K = C.
We represent the zero vector by 0 just as we also represent the scalar zero by 0.
Rarely is there danger of confusion.
The reader is already familiar with the special instances of this that relate to the
sets Rn and Cn. These sets are vector spaces under DeÔ¨Ånition 1.2, where vector
addition is deÔ¨Åned to be
x + y =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
x0
x1
...
xn‚àí1
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª+
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
y0
y1
...
yn‚àí1
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª=
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
x0 + y0
x1 + y1
...
xn‚àí1 + yn‚àí1
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª,
(1.12a)
and multiplication by a Ô¨Åeld element is deÔ¨Åned to be
ax =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
ax0
ax1
...
axn‚àí1
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª.
(1.12b)
The zero vector is 0 = [00 ¬∑ ¬∑ ¬∑ 00]T , and ‚àíx = [‚àíx0 ‚àíx1 ¬∑ ¬∑ ¬∑ ‚àíxn‚àí1]T . If X = Rn
then the elements of x and y are real-valued, and a ‚ààR, but if X = Cn then the
TLFeBOOK

10
FUNCTIONAL ANALYSIS IDEAS
elements of x and y are complex-valued, and a ‚ààC. The metric spaces in Exam-
ple 1.2 are therefore also vector spaces under the operations deÔ¨Åned in (1.12a,b).
Some further examples of vector spaces now follow.
Example 1.8
Metric space C[a, b] (Example 1.4) is a vector space under the
operations
(x + y)(t) = x(t) + y(t),
(Œ±x)(t) = Œ±x(t),
(1.13)
where Œ± ‚ààR. The zero vector is the function that is identically zero on the interval
[a, b].
Example 1.9
Metric space l2[0, ‚àû] (Example 1.6) is a vector space under the
operations
x + y = (x0, x1, . . .) + (y0, y1, . . .) = (x0 + y0, x1 + y1, . . .),
Œ±x = (Œ±x0, Œ±x1, . . .).
(1.14)
Here Œ± ‚ààC.
If x, y ‚ààl2[0, ‚àû], then some effort is required to verify axiom (V1). This
requires the Minkowski inequality, which is
 ‚àû

k=0
|xk + yk|p
	1/p
‚â§
 ‚àû

k=0
|xk|p
	1/p
+
 ‚àû

k=0
|yk|p
	1/p
.
(1.15)
Refer back to Example 1.6; here we employ p = 2, but (1.15) is valid for p ‚â•1.
Proof of (1.15) is somewhat involved, and so is omitted here. The interested reader
can see Kreyszig [1, pp. 11‚Äì15].
We remark that the Minkowski inequality can be proved with the aid of the
H¬®older inequality
‚àû

k=0
|xkyk| ‚â§
 ‚àû

k=0
|xk|p
	1/p  ‚àû

k=0
|yk|q
	1/q
(1.16)
for which here p > 1 and 1
p + 1
q = 1.
We are now ready to deÔ¨Åne a normed space.
DeÔ¨Ånition 1.3: Normed Space, Norm
A normed space X is a vector space
with a norm deÔ¨Åned on it. If x ‚ààX then the norm of x is denoted by
||x||
(read this as ‚Äúnorm of x‚Äù).
The norm must satisfy the following axioms:
(N1) ||x|| ‚â•0 (i.e., the norm is nonnegative).
TLFeBOOK

SOME SPECIAL MAPPINGS: METRICS, NORMS, AND INNER PRODUCTS
11
(N2) ||x|| = 0 ‚áîx = 0.
(N3) ||Œ±x|| = |Œ±| ||x||. Here Œ± is a scalar in the Ô¨Åeld of X (i.e., Œ± ‚ààK; see
DeÔ¨Ånition 3.2).
(N4) ||x + y|| ‚â§||x|| + ||y|| (triangle inequality).
The normed space is vector space X together with a norm, and so may be properly
denoted by the pair (X, || ¬∑ ||). However, we may simply write X, and say ‚Äúnormed
space X,‚Äù so the norm that goes along with X is understood from the context of
the discussion.
It is important to note that all normed spaces are also metric spaces, where the
metric is given by
d(x, y) = ||x ‚àíy||
(x, y ‚ààX).
(1.17)
The metric in (1.17) is called the metric induced by the norm.
Various other properties of norms may be deduced. One of these is:
Example 1.10
Prove | ||y|| ‚àí||x|| | ‚â§||y ‚àíx||.
Proof
From (N3) and (N4)
||y|| = ||y ‚àíx + x|| ‚â§||y ‚àíx|| + ||x||, ||x|| = ||x ‚àíy + y|| ‚â§||y ‚àíx|| + ||y||.
Combining these, we obtain
||y|| ‚àí||x|| ‚â§||y ‚àíx||, ||y|| ‚àí||x|| ‚â•‚àí||y ‚àíx||.
The claim follows immediately.
We may regard the norm as a mapping from X to set R: || ¬∑ |||X ‚ÜíR. This
mapping can be shown to be continuous. However, this requires generalizing the
concept of continuity that you may know from elementary calculus. Here we deÔ¨Åne
continuity as follows.
DeÔ¨Ånition 1.4: Continuous Mapping
Suppose X = (X, d) and Y = (Y, d)
are two metric spaces. The mapping T |X ‚ÜíY is said to be continuous at a point
x0 ‚ààX if for all œµ > 0 there is a Œ¥ > 0 such that
d(T x, T x0) < œµ
for all x satisfying
d(x, x0) < Œ¥.
(1.18)
T is said to be continuous if it is continuous at every point of X.
Note that T x is just another way of writing T (x). (R, | ¬∑ |) is a normed space; that
is, the set of real numbers with the usual arithmetic operations deÔ¨Åned on it is a
TLFeBOOK

12
FUNCTIONAL ANALYSIS IDEAS
vector space, and the absolute value of an element of R is the norm of that element.
If we identify Y in DeÔ¨Ånition 1.4 with metric space (R, | ¬∑ |), then (1.18) becomes
d(T x, T x0) = d(||x||, ||x0||) = | ||x|| ‚àí||x0|| | < œµ,
d(x, x0) = ||x ‚àíx0|| < Œ¥.
To make these claims, we are using (1.17). In other words, X and Y are normed
spaces, and we employ the metrics induced by their respective norms. In addition,
we identify T with || ¬∑ ||. Using Example 1.10, we obtain
| ||x|| ‚àí||x0|| | ‚â§||x ‚àíx0|| < Œ¥.
Thus, the requirements of DeÔ¨Ånition 1.4 are met, and so we conclude that norms
are continuous mappings.
We now list some other normed spaces.
Example 1.11
The Euclidean space Rn and the unitary space Cn are both
normed spaces, where the norm is deÔ¨Åned to be
||x|| =
n‚àí1

k=0
|xk|2
	1/2
.
(1.19)
For Rn the absolute value bars may be dropped.4 It is easy to see that d(x, y) =
||x ‚àíy|| gives the same metric as in (1.6a) for space Rn. We further remark that
for n = 1 we have ||x|| = |x|.
Example 1.12
The space lp[0, ‚àû] is a normed space if we deÔ¨Åne the norm
to be
||x|| =
 ‚àû

k=0
|xk|p
	1/p
(1.20)
for which d(x, y) = ||x ‚àíy|| coincides with the metric in (1.10b).
Example 1.13
The sequence space l‚àû[0, ‚àû] from Example 1.3 of Section 1.3.1
is a normed space, where the norm is deÔ¨Åned to be
||x|| = sup
k‚ààZ+ |xk|,
(1.21)
and this norm induces the metric of (1.7b).
4Suppose z = x + jy (j =
‚àö
‚àí1, x, y ‚ààR) is some arbitrary complex number. Recall that z2 Ã∏= |z|2
in general.
TLFeBOOK

SOME SPECIAL MAPPINGS: METRICS, NORMS, AND INNER PRODUCTS
13
Example 1.14
The space C[a, b] Ô¨Årst seen in Example 1.4 is a normed space,
where the norm is deÔ¨Åned by
||x|| = sup
t‚ààJ
|x(t)|.
(1.22)
Naturally, this norm induces the metric of (1.8).
Example 1.15
The space L2[a, b] of Example 1.7 is a normed space for the
norm
||x|| =
 b
a
|x(t)|2 dt
1/2
.
(1.23)
This norm induces the metric in (1.11b).
The normed space of Example 1.15 is important in the following respect.
Observe that
||x||2 =
 b
a
|x(t)|2 dt.
(1.24)
Suppose we now consider a resistor with resistance R. If the voltage drop across its
terminals is v(t) and the current through it is i(t), we know that the instantaneous
power dissipated in the device is p(t) = v(t)i(t). If we assume that the resistor is
a linear device, then v(t) = Ri(t) via Ohm‚Äôs law. Thus
p(t) = v(t)i(t) = Ri2(t).
(1.25)
Consequently, the amount of energy delivered to the resistor over time interval
t ‚àà[a, b] is given by
E = R
 b
a
i2(t) dt.
(1.26)
If the voltage/current waveforms in our circuit containing R belong to the space
L2[a, b], then clearly E = R||i||2. We may therefore regard the square of the L2
norm [given by (1.24)] of a signal to be the energy of the signal, provided the
norm exists. This notion can be helpful in the optimal design of electric circuits
(e.g., electric Ô¨Ålters), and also of optimal electronic circuits. In analogous fashion,
an element x of space l2[0, ‚àû] satisÔ¨Åes
||x||2 =
‚àû

k=0
|xk|2 < ‚àû
(1.27)
[see (1.10a) and Example 1.12]. We may consider ||x||2 to be the energy of the
single-sided sequence x. This notion is useful in the optimal design of digital Ô¨Ålters.
TLFeBOOK

14
FUNCTIONAL ANALYSIS IDEAS
1.3.3
Inner Products and Inner Product Spaces
The concept of an inner product is necessary before one can talk about orthogonal
bases for vector spaces. Recall from elementary linear algebra that orthogonal
bases were important in representing vectors. From a computational standpoint,
as mentioned earlier, orthogonal bases can have a simplifying effect on certain
types of approximation problem (e.g., least-squares approximations), and represent
a means of controlling numerical errors due to so-called ill-conditioned problems.
Following our axiomatic approach, consider the following deÔ¨Ånition.
DeÔ¨Ånition 1.5: Inner Product Space, Inner Product
An inner product space
is a vector space X with an inner product deÔ¨Åned on it. The inner product is a
mapping ‚ü®¬∑, ¬∑‚ü©|X √ó X ‚ÜíK that satisÔ¨Åes the following axioms:
(I1) ‚ü®x + y, z‚ü©= ‚ü®x, z‚ü©+ ‚ü®y, z‚ü©.
(I2) ‚ü®Œ±x, y‚ü©= Œ±‚ü®x, y‚ü©.
(I3) ‚ü®x, y‚ü©= ‚ü®y, x‚ü©‚àó.
(I4) ‚ü®x, x‚ü©‚â•0, and ‚ü®x, x‚ü©= 0 ‚áîx = 0.
Naturally, x, y, z ‚ààX, and Œ± is a scalar from the Ô¨Åeld K of vector space X. The
asterisk superscript on ‚ü®y, x‚ü©in (I3) denotes complex conjugation.5
If the Ô¨Åeld of X is not C, then the operation of complex conjugation in (I3) is
redundant.
All inner product spaces are also normed spaces, and hence are also metric
spaces. This is because the inner product induces a norm on X
||x|| = [‚ü®x, x‚ü©]1/2
(1.28)
for all x ‚ààX. Following (1.17), the induced metric is
d(x, y) = ||x ‚àíy|| = [‚ü®x ‚àíy, x ‚àíy‚ü©]1/2.
(1.29)
Directly from the axioms of DeÔ¨Ånition 1.5, it is possible to deduce that (for
x, y, z ‚ààX and a, b ‚ààK)
‚ü®ax + by, z‚ü©= a‚ü®x, z‚ü©+ b‚ü®y, z‚ü©,
(1.30a)
‚ü®x, ay‚ü©= a‚àó‚ü®x, y‚ü©,
(1.30b)
and
‚ü®x, ay + bz‚ü©= a‚àó‚ü®x, y‚ü©+ b‚àó‚ü®x, z‚ü©.
(1.30c)
The reader should prove these as an exercise.
5If z = x + yj is a complex number, then its conjugate is z‚àó= x ‚àíyj.
TLFeBOOK

SOME SPECIAL MAPPINGS: METRICS, NORMS, AND INNER PRODUCTS
15
We caution the reader that not all normed spaces are inner product spaces. We
may construct an example with the aid of the following example.
Example 1.16
Let x, y be from an inner product space. If || ¬∑ || is the norm
induced by the inner product, then ||x + y||2 + ||x ‚àíy||2 = 2(||x||2 + ||y||2). This
is the parallelogram equality.
Proof
Via (1.30a,c) we have
||x + y||2 = ‚ü®x + y, x + y‚ü©= ‚ü®x, x + y‚ü©+ ‚ü®y, x + y‚ü©
= ‚ü®x, x‚ü©+ ‚ü®x, y‚ü©+ ‚ü®y, x‚ü©+ ‚ü®y, y‚ü©,
and
||x ‚àíy||2 = ‚ü®x ‚àíy, x ‚àíy‚ü©= ‚ü®x, x ‚àíy‚ü©‚àí‚ü®y, x ‚àíy‚ü©
= ‚ü®x, x‚ü©‚àí‚ü®x, y‚ü©‚àí‚ü®y, x‚ü©+ ‚ü®y, y‚ü©.
Adding these gives the stated result.
It turns out that the space lp[0, ‚àû] with p Ã∏= 2 is not an inner product space. The
parallelogram equality can be used to show this. Consider x = (1, 1, 0, 0, . . .), y =
(1, ‚àí1, 0, 0, . . .), which are certainly elements of lp[0, ‚àû] [see (1.10a)]. We see that
||x|| = ||y|| = 21/p, ||x + y|| = ||x ‚àíy|| = 2.
The parallelogram equality is not satisÔ¨Åed, which implies that our norm does not
come from an inner product. Thus, lp[0, ‚àû] with p Ã∏= 2 cannot be an inner product
space.
On the other hand, l2[0, ‚àû] is an inner product space, where the inner product
is deÔ¨Åned to be
‚ü®x, y‚ü©=
‚àû

k=0
xky‚àó
k .
(1.31)
Does this inÔ¨Ånite series converge? Yes, it does. To see this, we need the Cauchy‚Äì
Schwarz inequality.6 Recall the H¬®older inequality of (1.16). Let p = 2, so that
q = 2. Then the Cauchy‚ÄìSchwarz inequality is
‚àû

k=0
|xkyk| ‚â§
 ‚àû

k=0
|xk|2
	1/2  ‚àû

k=0
|yk|2
	1/2
.
(1.32)
6The inequality we consider here is related to the Schwarz inequality. We will consider the Schwarz
inequality later on. This inequality is of immense practical value to electrical and computer engineers.
It is used to derive the matched-Ô¨Ålter receiver, which is employed in digital communications systems,
to derive the uncertainty principle in quantum mechanics and in signal processing, and to derive the
Cram¬¥er‚ÄìRao lower bound on the variance of parameter estimators, to name only three applications.
TLFeBOOK

16
FUNCTIONAL ANALYSIS IDEAS
Now
|‚ü®x, y‚ü©| =

‚àû

k=0
xky‚àó
k
 ‚â§
‚àû

k=0
|xkyk|.
(1.33)
The inequality in (1.33) follows from the triangle inequality for | ¬∑ |. (Recall that the
absolute value operation is a norm on R. It is also a norm on C; if z = x + jy ‚ààC,
then |z| =

x2 + y2.) The right-hand side of (1.32) is Ô¨Ånite because x and y are
in l2[0, ‚àû]. Thus, from (1.33), ‚ü®x, y‚ü©is Ô¨Ånite. Thus, the series (1.31) converges.
It turns out that C[a, b] is not an inner product space, either. But we will not
demonstrate the truth of this claim here.
Some further examples of inner product spaces are as follows.
Example 1.17
The Euclidean space Rn is an inner product space, where the
inner product is deÔ¨Åned to be
‚ü®x, y‚ü©=
n‚àí1

k=0
xkyk.
(1.34)
The reader will recognize this as the vector dot product from elementary linear
algebra; that is, x ¬∑ y = ‚ü®x, y‚ü©. It is well worth noting that
‚ü®x, y‚ü©= xT y.
(1.35)
Here the superscript T denotes transposition. So, xT is a row vector. The inner
product in (1.34) certainly induces the norm in (1.19).
Example 1.18
The unitary space Cn is an inner product space for the inner
product
‚ü®x, y‚ü©=
n‚àí1

k=0
xky‚àó
k .
(1.36)
Again, the norm of (1.19) is induced by inner product (1.36). If H denotes the
operation of complex conjugation and transposition (this is called Hermitian trans-
position), then
yH = [y‚àó
0y‚àó
1 ¬∑ ¬∑ ¬∑ y‚àó
n‚àí1]
(row vector), and
‚ü®x, y‚ü©= yH x.
(1.37)
Example 1.19
The space L2[a, b] from Example 1.7 is an inner product space
if the inner product is deÔ¨Åned to be
‚ü®x, y‚ü©=
 b
a
x(t)y‚àó(t) dt.
(1.38)
TLFeBOOK

SOME SPECIAL MAPPINGS: METRICS, NORMS, AND INNER PRODUCTS
17
The norm induced by (1.38) is
||x|| =
 b
a
|x(t)|2 dt
1/2
.
(1.39)
This in turn induces the metric in (1.11b).
Now we consider the concept of orthogonality in a completely general manner.
DeÔ¨Ånition 1.6: Orthogonality
Let x, y be vectors from some inner product
space X. These vectors are orthogonal iff
‚ü®x, y‚ü©= 0.
The orthogonality of x and y is symbolized by writing x ‚ä•y. Similarly, for subsets
A, B ‚äÇX we write x ‚ä•A if x ‚ä•a for all a ‚ààA, and A ‚ä•B if a ‚ä•b for all a ‚ààA,
and b ‚ààB.
If we consider the inner product space R2, then it is easy to see that
‚ü®[1 0]T , [0 1]T ‚ü©= 0, so [0 1]T , and [1 0]T are orthogonal vectors. In fact, these
vectors form an orthogonal basis for R2, a concept we will consider more gen-
erally below. If we deÔ¨Åne the unit vectors e0 = [1 0]T , and e1 = [0 1]T , then we
recall that any x ‚ààR2 can be expressed as x = x0e0 + x1e1. (The extension of
this reasoning to Rn for n > 2 should be clear.) Another example of a pair of
orthogonal vectors would be x =
1
‚àö
2[1 1]T , and y =
1
‚àö
2[1 ‚àí1]T . These too form
an orthogonal basis for the space R2.
DeÔ¨Åne the functions
œÜ(x) =
 0,
x < 0 and x ‚â•1
1,
0 ‚â§x < 1
(1.40)
and
œà(x) =
Ô£±
Ô£¥Ô£≤
Ô£¥Ô£≥
0,
x < 0 and x ‚â•1
1,
0 ‚â§x < 1
2
‚àí1,
1
2 ‚â§x < 1
.
(1.41)
Function œÜ(x) is called the Haar scaling function, and function œà(x) is called the
Haar wavelet [5]. The function œÜ(x) is also called an non-return-to-zero (NRZ)
pulse, and function œà(x) is also called a Manchester pulse [6]. It is easy to con-
Ô¨Årm that these pulses are elements of L2(R) = L2(‚àí‚àû, ‚àû), and that they are
orthogonal, that is, ‚ü®œÜ, œà‚ü©= 0 under the inner product deÔ¨Åned in (1.38). This is
so because
‚ü®œÜ, œà‚ü©=
 ‚àû
‚àí‚àû
œÜ(x)œà‚àó(x) dx =
 1
0
œà(x) dx = 0.
TLFeBOOK

18
FUNCTIONAL ANALYSIS IDEAS
Thus, we consider œÜ and œà to be elements in the inner product space L2(R), for
which the inner product is
‚ü®x, y‚ü©=
 ‚àû
‚àí‚àû
x(t)y‚àó(t) dt.
It turns out that the Haar wavelet is the simplest example of the more general class
of Daubechies wavelets. The general theory of these wavelets Ô¨Årst appeared in
Daubechies [7]. Their development has revolutionized signal processing and many
other areas.7 The main reason for this is the fact that for any f (t) ‚ààL2(R)
f (t) =
‚àû

n=‚àí‚àû
‚àû

k=‚àí‚àû
‚ü®f, œàn,k‚ü©œàn,k(t),
(1.42)
where œàn,k(t) = 2n/2œà(2nt ‚àík). This doubly inÔ¨Ånite series is called a wavelet
series expansion for f . The coefÔ¨Åcients fn,k = ‚ü®f, œàn,k‚ü©have Ô¨Ånite energy. In
effect, if we treat either k or n as a constant, then the resulting doubly inÔ¨Ånite
sequence is in the space l2[‚àí‚àû, ‚àû]. In fact, it is also the case that
‚àû

n=‚àí‚àû
‚àû

k=‚àí‚àû
|fn,k|2 < ‚àû.
(1.43)
It is to be emphasized that the œà used in (1.42) could be (1.41), or it could be
chosen from the more general class in Ref. 7. We shall not prove these things in
this book, as the technical arguments are quite hard.
The wavelet series is presently not as familiar to the broader electrical and
computer engineering community as is the Fourier series. A brief summary of the
Fourier series is as follows. Again, rigorous proofs of many of the following claims
will be avoided, though good introductory references to Fourier series are Tolstov
[8] or Kreyszig [9]. If f ‚ààL2(0, 2œÄ), then
f (t) =
‚àû

n=‚àí‚àû
fnejnt,
j =
‚àö
‚àí1,
(1.44)
where the Fourier (series) coefÔ¨Åcients are given by
fn = 1
2œÄ
 2œÄ
0
f (t)e‚àíjnt dt.
(1.45)
We may deÔ¨Åne
en(t) = exp(jnt)
(t ‚àà(0, 2œÄ), n ‚ààZ)
(1.46)
7For example, in digital communications the problem of designing good signaling pulses for data
transmission is best treated with respect to wavelet theory.
TLFeBOOK

SOME SPECIAL MAPPINGS: METRICS, NORMS, AND INNER PRODUCTS
19
so that we see
‚ü®f, en‚ü©= 1
2œÄ
 2œÄ
0
f (t)

ejnt‚àó
dt = fn.
(1.47)
The series (1.44) is the complex Fourier series expansion for f . Note that for
n, k ‚ààZ
exp[jn(t + 2œÄk)] = exp[jnt] exp[2œÄjnk] = exp[jnt].
(1.48)
Here we have used Euler‚Äôs identity
ejx = cos x + j sin x
(1.49)
and cos(2œÄk) = 1, sin(2œÄk) = 0. The function ejnt is therefore 2œÄ-periodic; that
is, its period is 2œÄ. It therefore follows that the series on the right-hand side of
(3.40) is a 2œÄ-periodic function, too. The result (1.48) implies that, although f in
(1.44) is initially deÔ¨Åned only on (0, 2œÄ), we are at liberty to ‚Äúperiodically extend‚Äù
f over the entire real-number line; that is, we can treat f as one period of the
periodic function
Àúf (t) =

k‚ààZ
f (t + 2œÄk)
(1.50)
for which f (t) = Àúf (t) for t ‚àà(0, 2œÄ). Thus, series (1.44) is a way to represent
periodic functions. Because f ‚ààL2(0, 2œÄ), it turns out that
‚àû

n=‚àí‚àû
|fn|2 < ‚àû
(1.51)
so that (fn) ‚ààl2[‚àí‚àû, ‚àû].
Observe that in (1.47) we have ‚ÄúredeÔ¨Åned‚Äù the inner product on L2(0, 2œÄ) to be
‚ü®x, y‚ü©= 1
2œÄ
 2œÄ
0
x(t)y‚àó(t) dt
(1.52)
which differs from (1.38) in that it has the factor
1
2œÄ in front. This variation also
happens to be a valid inner product on the vector space deÔ¨Åned by the set in (1.11a).
Actually, it is a simple example of a weighted inner product.
Now consider, for n Ã∏= m
‚ü®en, em‚ü©= 1
2œÄ
 2œÄ
0
ejnte‚àíjmt dt =
1
2œÄj(n ‚àím)

ej(n‚àím)t2œÄ
0
= e2œÄj(n‚àím) ‚àí1
2œÄj(n ‚àím) =
1 ‚àí1
2œÄj(n ‚àím) = 0.
(1.53)
Similarly
‚ü®en, en‚ü©= 1
2œÄ
 2œÄ
0
ejnte‚àíjnt dt = 1
2œÄ
 2œÄ
0
dt = 1.
(1.54)
So, en and em (if n Ã∏= m) are orthogonal with respect to the inner product in (1.52).
TLFeBOOK

20
FUNCTIONAL ANALYSIS IDEAS
From basic electric circuit analysis, periodic signals have Ô¨Ånite power. Therefore,
series (1.44) is a way to represent Ô¨Ånite power signals.8 We might therefore consider
the space L2(0, 2œÄ) to be the ‚Äúspace of Ô¨Ånite power signals.‚Äù From considerations
involving the wavelet series representation of (1.42), we may consider L2(R) to
be the ‚Äúspace of Ô¨Ånite energy signals.‚Äù Recall also the discussion at the end of
Section 1.3.2 (last paragraph).
An example of a Fourier series expansion is the following.
Example 1.20
Suppose that
f (t) =

1,
0 < t < œÄ
‚àí1,
œÄ ‚â§t < 2œÄ
.
(1.55)
A sketch of this function is one period of a 2œÄ-periodic square wave. The Fourier
coefÔ¨Åcients are given by (for n Ã∏= 0)
fn = 1
2œÄ
 2œÄ
0
f (t)e‚àíjnt dt = 1
2œÄ
 œÄ
0
e‚àíjnt dt ‚àí
 2œÄ
œÄ
e‚àíjnt dt
	
= 1
2œÄ

‚àí1
jn

e‚àíjntœÄ
0 + 1
jn

e‚àíjnt2œÄ
œÄ

= 1
2œÄ
1 ‚àíe‚àíjnœÄ ‚àíe‚àíjnœÄ + 1
jn

= 1
œÄ
1 ‚àíe‚àíjnœÄ
jn
= 2
œÄne‚àíjnœÄ/2 ejnœÄ/2 ‚àíe‚àíjnœÄ/2
2j
= 2
œÄne‚àíjnœÄ/2 sin
œÄn
2

,
(1.56)
where we have made use of
sin x = 1
2j [ejx ‚àíe‚àíjx].
(1.57)
This is easily derived using the Euler identity in (1.49). For n = 0, it should be
clear that f0 = 0.
The coefÔ¨Åcients fn in (1.56) involve expressions containing j. Since f (t) is
real-valued, it therefore follows that we can rewrite the series expansion in such a
manner as to avoid complex arithmetic. It is almost a standard practice to do this.
We now demonstrate this process:
‚àû

n=‚àí‚àû
fnejnt = 2
œÄ
 ‚àû

n=1
1
ne‚àíjnœÄ/2 sin
œÄ
2 n

ejnt +
‚àí1

n=‚àí‚àû
1
ne‚àíjnœÄ/2 sin
œÄ
2 n

ejnt
	
= 2
œÄ
 ‚àû

n=1
1
ne‚àíjnœÄ/2 sin
œÄ
2 n

ejnt +
‚àû

n=1
1
nejnœÄ/2 sin
œÄ
2 n

e‚àíjnt
	
8In fact, using phasor analysis and superposition, you can apply (1.44) to determine the steady-state
output of a circuit for any periodic input (including, and especially, nonsinusoidal periodic functions).
This makes the Fourier series very important in electrical/electronic circuit analysis.
TLFeBOOK

SOME SPECIAL MAPPINGS: METRICS, NORMS, AND INNER PRODUCTS
21
= 2
œÄ
‚àû

n=1
1
n sin
œÄ
2 n
 
ejnte‚àíjœÄn/2 + e‚àíjntejœÄn/2
= 4
œÄ
‚àû

n=1
1
n cos

n

t ‚àíœÄ
2

sin
œÄ
2 n

Here we have used the fact that (see Appendix 1.A)
ejnte‚àíjœÄn/2 + e‚àíjntejœÄn/2 = 2 Re [ejnte‚àíjœÄn/2] = 2 cos

n

t ‚àíœÄ
2

.
This is so because if z = x + jy, then z + z‚àó= 2x = 2 Re [z]. Since
cos(Œ± + Œ≤) = cos Œ± cos Œ≤ ‚àísin Œ± sin Œ≤,
we have
cos

n

t ‚àíœÄ
2

= cos(nt) cos œÄn
2 + sin(nt) sin œÄn
2 .
However, if n is an even number, then sin(œÄn/2) = 0, and if n is an odd number,
then cos(œÄn/2) = 0. Therefore
4
œÄ
‚àû

n=1
1
n cos

n

t ‚àíœÄ
2

sin
œÄ
2 n

= 4
œÄ
‚àû

n=0
1
2n + 1 sin [(2n + 1)t] sin2 
(2n + 1)œÄ
2

,
but sin2[(2n + 1) œÄ
2 ] = 1, so Ô¨Ånally we have
f (t) =
‚àû

n=‚àí‚àû
fnejnt = 4
œÄ
‚àû

n=0
1
2n + 1 sin[(2n + 1)t].
It is important to note that the wavelet series and Fourier series expansions have
something in common, in spite of the fact that they look quite different and indeed
are associated with quite different function spaces. The common feature is that both
representations involve the use of orthogonal basis functions. We are now ready to
consider this in a general manner.
Begin by recalling from elementary linear algebra that a basis for a vector space
such as X = Rn or X = Cn is a set of n vectors, say
B = {e0, e1, . . . , en‚àí1}
(1.58)
such that the elements ek (basis vectors) are linearly independent. This means that
no vector in the set can be expressed as a linear combination of any of the others.
TLFeBOOK

22
FUNCTIONAL ANALYSIS IDEAS
In general, it is not necessary that ‚ü®ek, en‚ü©= 0 for n Ã∏= k. In other words, indepen-
dence does not require orthogonality. However, if set B is a basis (orthogonal or
otherwise) then for any x ‚ààX (vector space) there exists a set of coefÔ¨Åcients from
the Ô¨Åeld of the vector space, say, b = {b0, b1 . . . , bn‚àí1}, such that
x =
n‚àí1

k=0
bkek.
(1.59)
We say that spaces Rn and Cn are of dimension n. This is a direct reference to the
number of basis vectors in B. This notion generalizes.
Now let us consider a sequence space (e.g., l2[0, ‚àû]). Suppose x =
(x0, x1, x2, . . .) ‚ààl2[0, ‚àû]. DeÔ¨Åne the following unit vector sequences:
e0 = (1, 0, 0, 0, . . .),
e1 = (0, 1, 0, 0, . . .),
e2 = (0, 0, 1, 0, . . .), etc. (1.60)
Clearly
x =
‚àû

k=0
xkek.
(1.61)
It is equally clear that no vector ek can be expressed as a linear combination of
any of the others. Thus, the countably inÔ¨Ånite set9 B = {e0, e1, e2, . . .} forms a
basis for l2[0, ‚àû]. The sequence space is therefore of inÔ¨Ånite dimension because
B has a countable inÔ¨Ånity of members. It is apparent as well that, under the inner
product deÔ¨Åned in (1.31), we have ‚ü®en, em‚ü©= Œ¥n‚àím. Sequence Œ¥ = (Œ¥n) is called
the Kr¬®onecker delta sequence. It is deÔ¨Åned by
Œ¥n =
 1,
n = 0
0,
n Ã∏= 0 .
(1.62)
Therefore, the vectors in (1.60) are mutually orthogonal as well. So they happen to
form an orthogonal basis for l2[0, ‚àû]. Of course, this is not the only possible basis.
In general, given a countably inÔ¨Ånite set of vectors {ek|k ‚ààZ+} [no longer neces-
sarily those in (1.60)] that are linearly independent, and such that ek ‚ààl2[0, ‚àû],
for any x ‚ààl2[0, ‚àû] there will exist coefÔ¨Åcients ak ‚ààC such that
x =
‚àû

k=0
akek.
(1.63)
In view of the above, consider the following linearly independent set of vectors
from some inner product space X:
B = {ek|ek ‚ààX,
k ‚ààZ}.
(1.64)
9A set A is countably inÔ¨Ånite if its members can be put into one-to-one (1‚Äì1) correspondence with
the members of the set Z+. This is also equivalent to being able to place the elements of A into 1‚Äì1
correspondence with the elements of Z.
TLFeBOOK

SOME SPECIAL MAPPINGS: METRICS, NORMS, AND INNER PRODUCTS
23
Assume that this is a basis for X. In this case for any x ‚ààX, there are coefÔ¨Åcients
ak such that
x =

k‚ààZ
akek.
(1.65)
We deÔ¨Åne the set B to be orthogonal iff for all n, k ‚ààZ
‚ü®en, ek‚ü©= Œ¥n‚àík.
(1.66)
Assume that the elements of B in (1.64) satisfy (1.66). It is then easy to
see that
‚ü®x, en‚ü©=

k
akek, en

=

k
‚ü®akek, en‚ü©
(using (I1))
=

k
ak‚ü®ek, en‚ü©
(using (I2))
=

k
Œ¥k‚àínak
(using (1.66))
so Ô¨Ånally we may say that
‚ü®x, en‚ü©= an.
(1.67)
In other words, if the basis B is orthogonal, then
x =

k‚ààZ
‚ü®x, ek‚ü©ek.
(1.68)
Previous examples (e.g., Fourier series expansion) are merely special cases of this
general idea. We see that one of the main features of an orthogonal basis is the
ease with which we can obtain the coefÔ¨Åcients ak. Nonorthogonal bases are harder
to work with in this respect. This is one of the reasons why orthogonal bases are
so universally popular.
A few comments on terminology are in order here. Some would say that the
condition (1.66) on B in (1.64) means that B is an orthonormal set, and we would
say that condition
‚ü®en, ek‚ü©= Œ±nŒ¥n‚àík
is the condition for B to be an orthogonal set, where Œ±n is not necessarily unity
(i.e., equal to one) for all n. However, in this book we often insist that orthogonal
basis vectors be ‚Äúnormalized‚Äù so condition (1.66) holds.
We conclude the present section by considering the following theorem. It was
mentioned in a footnote that the following Schwarz inequality (or variations of it)
is of very great value in electrical and computer engineering.
TLFeBOOK

24
FUNCTIONAL ANALYSIS IDEAS
Theorem 1.1: Schwarz Inequality
Let X be an inner product space, where
x, y ‚ààX. Then
|‚ü®x, y‚ü©| ‚â§||x|| ||y||.
(1.69)
Equality holds iff {x, y} is a linearly dependent set.
Proof
If y = 0 then ‚ü®x, 0‚ü©= 0, and (1.69) clearly holds in this special case.
Let y Ã∏= 0. For all scalars Œ± in the Ô¨Åeld of X we must have [via inner product
axioms and (1.30)]
0 ‚â§||x ‚àíŒ±y||2 = ‚ü®x ‚àíŒ±y, x ‚àíŒ±y‚ü©
= ‚ü®x, x‚ü©‚àíŒ±‚àó‚ü®x, y‚ü©‚àíŒ±[‚ü®y, x‚ü©‚àíŒ±‚àó‚ü®y, y‚ü©].
If we select Œ±‚àó= ‚ü®y, x‚ü©/‚ü®y, y‚ü©, then the quantity in the brackets [¬∑] vanishes. Thus
0 ‚â§‚ü®x, x‚ü©‚àí‚ü®y, x‚ü©
‚ü®y, y‚ü©‚ü®x, y‚ü©= ||x||2 ‚àí|‚ü®x, y‚ü©|2
||y||2
[using ‚ü®x, y‚ü©= ‚ü®y, x‚ü©‚àó, i.e., axiom (I3)]. Rearranging, this yields
|‚ü®x, y‚ü©|2 ‚â§||x||2||y||2,
and the result (1.69) follows (we must take positive square roots as ||x|| ‚â•0, and
|x| ‚â•0).
Equality holds iff y = 0, or else ||x ‚àíŒ±y||2 = 0, hence x ‚àíŒ±y = 0 [recall
(N2)], so x = Œ±y, demonstrating linear dependence of x and y.
We may now see what Theorem 1.1 has to say when applied to the special case of
a vector dot product.
Example 1.21
Suppose that X is the inner product space of Example 1.17.
Since
|‚ü®x, y‚ü©| =

n‚àí1

k=0
xkyk

and ||x|| =
n‚àí1
k=0 x2
k
1/2
, we have from Theorem 1.1 that

n‚àí1

k=0
xkyk
 ‚â§
n‚àí1

k=0
x2
k
	1/2 n‚àí1

k=0
y2
k
	1/2
.
(1.70)
If yk = Œ±xk (Œ± ‚ààR) for all k ‚ààZn, then

n‚àí1

k=0
xkyk
 = |Œ±|
n‚àí1

k=0
x2
k,
TLFeBOOK

THE DISCRETE FOURIER SERIES (DFS)
25
and
n‚àí1
k=0 y2
k
1/2
= |Œ±|
n‚àí1
k=0 x2
k
1/2
, hence
n‚àí1

k=0
x2
k
	1/2 n‚àí1

k=0
y2
k
	1/2
= |Œ±|
n‚àí1

k=0
x2
k.
Thus, (1.70) does indeed hold with equality when y = Œ±x.
1.4
THE DISCRETE FOURIER SERIES (DFS)
The subject of discrete Fourier series (DFS) and its relationship to the complex
Fourier series expansion of Section 1.3.3 is often deferred to later courses (e.g.,
signals and systems), but will be brieÔ¨Çy considered here as an additional example
of an orthogonal series expansion.
The complex Fourier series expansion of Section 1.3.3 was for 2œÄ-periodic
functions deÔ¨Åned on the real-number line. A similar series expansion exists for
N-periodic sequences such as Àúx = (Àúxn); that is, for N ‚àà{2, 3, 4, . . .} ‚äÇZ, consider
Àúxn =

k‚ààZ
xn+kN
(1.71)
where x = (xn) is such that xn = 0 for n < 0, and for n ‚â•N as well. Thus, x is
just one period of Àúx. We observe that
Àúxn+mN =
‚àû

k=‚àí‚àû
xn+mN+kN =
‚àû

k=‚àí‚àû
xn+(m+k)N =
‚àû

r=‚àí‚àû
xn+rN = Àúxn
(r = m + k). This conÔ¨Årms that Àúx is indeed N-periodic (i.e., periodic with period
N). We normally assume in a context such as this that xn ‚ààC. We also regard x
as a vector: x = [x0 x1 ¬∑ ¬∑ ¬∑ xN‚àí1]T ‚ààCN. An inner product may be deÔ¨Åned on
the space of N-periodic sequences according to
‚ü®Àúx, Àúy‚ü©= ‚ü®x, y‚ü©= yH x
(1.72)
(recall Example 1.18), where y ‚ààCN is one period of Àúy. We assume, of course,
that Àúx and Àúy are bounded sequences so that (1.72) is well deÔ¨Åned.
Now deÔ¨Åne ek = [ek,0 ek,1 ¬∑ ¬∑ ¬∑ ek,N‚àí1]T ‚ààCN according to
ek,n = exp

j 2œÄ
N kn

,
(1.73)
where n ‚ààZN. The periodization of ek = (ek,n) is
Àúek,n =

m‚ààZ
ek,n+mN
(1.74)
TLFeBOOK

26
FUNCTIONAL ANALYSIS IDEAS
yielding Àúek = (Àúek,n). That (1.73) is periodic with period N with respect to index n
is easily seen:
ek,n+mN = exp

j 2œÄ
N k(n + mN)

= exp

j 2œÄ
N kn

exp

j2œÄkm

= ek,n.
It can be shown (by exercise) that [using deÔ¨Ånition (1.72)]
‚ü®Àúek, Àúer‚ü©= ‚ü®ek, er‚ü©=
N‚àí1

n=0
exp

‚àíj 2œÄ
N rn

exp

j 2œÄ
N kn

=
N‚àí1

n=0
exp

j 2œÄ
N (k ‚àír)n

=
 N,
k ‚àír = 0
0,
otherwise
.
(1.75)
Thus, if we consider (ek,n), and (er,n) with k Ã∏= r we Ô¨Ånd that these sequences are
orthogonal, and so form an orthogonal basis for the vector space CN. From (1.75)
we may write
‚ü®ek, er‚ü©= NŒ¥k‚àír.
(1.76)
Thus, there must exist another vector X = [X0 X1 ¬∑ ¬∑ ¬∑ XN‚àí1]T ‚ààCN such that
xn = 1
N
N‚àí1

k=0
Xk exp

j 2œÄ
N kn

(1.77)
for n ‚ààZN. In fact
‚ü®x, er‚ü©=
N‚àí1

n=0
xne‚àó
r,n
= 1
N
N‚àí1

n=0

N‚àí1

k=0
Xk exp

j 2œÄ
N kn

exp

‚àíj 2œÄ
N rn

= 1
N
N‚àí1

k=0
Xk

N‚àí1

n=0
exp

j 2œÄ
N (k ‚àír)n

= 1
N
N‚àí1

k=0
Xk(NŒ¥k‚àír) = Xr.
(1.78)
That is
Xk =
N‚àí1

n=0
xn exp

‚àíj 2œÄ
N kn

(1.79)
for k ‚ààZN.
TLFeBOOK

THE DISCRETE FOURIER SERIES (DFS)
27
In (1.77) we see xn+mN = xn for all m ‚ààZ. Thus, (xn) in (1.77) is N-periodic,
and so we have Àúxn = 1
N
N‚àí1
k=0 Xk exp

j 2œÄ
N kn

with Xk given by (1.79). Equation
(1.77) is the discrete Fourier series (DFS) expansion for an N-periodic complex-
valued sequence Àúx such as in (1.71). The DFS coefÔ¨Åcients are given by (1.79).
However, it is common practice to consider only Àúxn for n ‚ààZN, which is equivalent
to only considering the vector x ‚ààCN. In this case the vector X ‚ààCN given by
(1.79) is now called the discrete Fourier transform (DFT) of the vector x, and the
expression in (1.77) is the inverse DFT (IDFT) of the vector X. We observe that
the DFT, and the IDFT can be concisely expressed in matrix form, where we deÔ¨Åne
the DFT matrix
F =

exp
 
‚àíj 2œÄ
N kn
!
k,n‚ààZN
‚ààCN√óN,
(1.80)
and we see from (1.77) that F ‚àí1 = 1
N F ‚àó(IDFT matrix). Thus, X = Fx. We remark
that the symmetry of F (i.e., F = F T ) means that either k or n in (1.80) may be
interpreted as row or column indices.
The DFT has a long history, and its invention is now attributed to Gauss
[10]. The DFT is of central importance to numerical computing generally, but
has particularly great signiÔ¨Åcance in digital signal processing as it represents a
numerical approximation to the Fourier transform, and it can also be used to
efÔ¨Åciently implement digital Ô¨Åltering operations via so-called fast Fourier trans-
form (FFT) algorithms. The construction of FFT algorithms to efÔ¨Åciently compute
X = Fx (and x = F ‚àí1X) is rather involved, and not within the scope of the
present book. Simply note that the direct computation of the matrix-vector product
X = Fx needs N2 complex multiplications and N(N ‚àí1) complex additions. For
N = 2p (p ‚àà{1, 2, 3, . . .}), which is called the radix-2 case, the algorithm of Coo-
ley and Tukey [11] reduces the number of operations to something proportional to
N log2 N, which is a substantial savings compared to N2 operations with the direct
approach when N is large enough. Essentially, the method in Ref. 11 implicitly fac-
tors F according to F = FpFp‚àí1 ¬∑ ¬∑ ¬∑ F1, where the matrix factors Fk ‚ààCN√óN are
sparse (i.e., contain many zero-valued entries). Note that multiplication by zero is
not implemented in either hardware or software and so does not represent a compu-
tational cost in the practical implementation of the FFT algorithm. It is noteworthy
that the algorithm of Ref. 11 also has a long history dating back to the work of
Gauss, as noted by Heideman et al. [10]. It is also important to mention that fast
algorithms exist for all possible N Ã∏= 2p [4]. The following example suggests one
of the important applications of the DFT/DFS.
Example 1.22
Suppose that xn = AejŒ∏n with Œ∏ = 2œÄ
N m for m = 1, 2, . . . ,
N
2 ‚àí1 (N is assumed to be even here). From (1.79) using (1.75)
Xk = ANŒ¥m‚àík.
(1.81)
TLFeBOOK

28
FUNCTIONAL ANALYSIS IDEAS
Now suppose instead that xn = Ae‚àíjŒ∏n, so similarly
Xk = A
N‚àí1

n=0
exp

‚àíj 2œÄ
N n(m + k)

= A
N‚àí1

n=0
exp

j 2œÄ
N n(N ‚àím ‚àík)

= ANŒ¥N‚àím‚àík.
(1.82)
Thus, if now xn = A cos(Œ∏n) = 1
2A[ejŒ∏n + e‚àíjŒ∏n], then from (1.81) and (1.82), we
must have
Xk = 1
2AN[Œ¥m‚àík + Œ¥N‚àím‚àík].
(1.83)
We observe that Xk = 0 for all k Ã∏= m, N ‚àím, but that
Xm = 1
2AN,
and
XN‚àím = 1
2AN.
Thus, Xk is nonzero only for indices k = m and k = N ‚àím corresponding to
the frequency of (xn), which is Œ∏ = 2œÄ
N m. The DFT/DFS is therefore quite use-
ful in detecting ‚Äúsinusoids‚Äù (also sometimes called ‚Äútone detection‚Äù). This makes
the DFT/DFS useful in such applications as narrowband radar and sonar signal
detection.
Can you explain the necessity (or, at least, the desirability) of the second equality
in Eq. (1.82)?
APPENDIX 1.A
COMPLEX ARITHMETIC
Here we summarize the most important facts about arithmetic with complex num-
bers z ‚ààC (set of complex numbers). You shall Ô¨Ånd this material very useful in
electric circuits, as well as in the present book.
Complex numbers may be represented in two ways: (1) Cartesian (rectangular)
form or (2) polar form. First we consider the Cartesian form.
In this case z ‚ààC has the form z = x + jy, where x, y ‚ààR (set of real num-
bers), and j =
‚àö
‚àí1. The complex conjugate of z is deÔ¨Åned to be z‚àó= x ‚àíjy (so
j‚àó= ‚àíj).
Suppose that z1 = x1 + jy1 and z2 = x2 + jy2 are two complex numbers. Addi-
tion and subtraction are deÔ¨Åned as
z1 ¬± z2 = (x1 ¬± x2) + j(y1 ¬± y2)
TLFeBOOK

COMPLEX ARITHMETIC
29
[e.g., (1 + 2j) + (3 ‚àí5j) = 4 ‚àí3j, and (1 + 2j) ‚àí(3 ‚àí5j) = ‚àí2 + 7j]. Using
j2 = ‚àí1, the product of z1 and z2 is
z1z2 = (x1 + jy1)(x2 + jy2)
= x1x2 + j2y1y2 + jy1x2 + jx1y2
= (x1x2 ‚àíy1y2) + j(x1y2 + x2y1).
We note that
zz‚àó= (x + jy)(x ‚àíjy) = x2 + y2 = |z|2,
so |z| =

x2 + y2 deÔ¨Ånes the magnitude of z. For example, (1 + 2j)(3 ‚àí5j) =
13 + j. The quotient of z1 and z2 is deÔ¨Åned to be
z1
z2
= z1z‚àó
2
z2z‚àó
2
= (x1 + jy1)(x2 ‚àíjy2)
x2
2 + y2
2
= (x1x2 + y1y2) + j(x2y1 ‚àíx1y2)
x2
2 + y2
2
= x1x2 + y1y2
x2
2 + y2
2
+ j x2y1 ‚àíx1y2
x2
2 + y2
2
,
where the last equality is z1/z2 in Cartesian form.
Now we may consider polar form representations. For z = x + jy, we may
regard x and y as the x and y coordinates (respectively) of a point in the Cartesian
plane (sometimes denoted R2).10 We may therefore express these coordinates in
polar form; thus, for any x and y we can write
x = r cos Œ∏,
y = r sin Œ∏,
where r ‚â•0, and Œ∏ ‚àà[0, 2œÄ), or Œ∏ ‚àà(‚àíœÄ, œÄ]. We observe that
x2 + y2 = r2(cos2 Œ∏ + sin2 Œ∏) = r2,
so |z| = r.
Now recall the following Maclaurin series expansions (considered in greater
depth in Chapter 3):
sin x =
‚àû

n=1
(‚àí1)n‚àí1
x2n‚àí1
(2n ‚àí1)!
cos x =
‚àû

n=1
(‚àí1)n‚àí1
x2n‚àí2
(2n ‚àí2)!
ex =
‚àû

n=1
xn‚àí1
(n ‚àí1)!
10This suggests that z may be equivalently represented by the column vector [xy]T . The vector inter-
pretation of complex numbers can be quite useful.
TLFeBOOK

30
FUNCTIONAL ANALYSIS IDEAS
These series converge for ‚àí‚àû< x < ‚àû. Observe the following:
ejx =
‚àû

n=1
(jx)n‚àí1
(n ‚àí1)! =
‚àû

n=1

(jx)(2n‚àí1)‚àí1
[(2n ‚àí1) ‚àí1]! + (jx)(2n‚àí1)
[2n ‚àí1]!
	
,
where we have split the summation into terms involving even n and odd n. Thus,
continuing
ejx =
‚àû

n=1
j2n‚àí2x2n‚àí2
(2n ‚àí2)! + j2n‚àí1x2n‚àí1
(2n ‚àí1)!

=
‚àû

n=1
j2n‚àí2

x2n‚àí2
(2n ‚àí2)! + j
x2n‚àí1
(2n ‚àí1)!

(jj2n‚àí2 = j2n‚àí1)
=
‚àû

n=1
(‚àí1)n‚àí1
x2n‚àí2
(2n ‚àí2)! + j
‚àû

n=1
(‚àí1)n‚àí1
x2n‚àí1
(2n ‚àí1)!
(j2n‚àí2 = (j2)n‚àí1 = (‚àí1)n‚àí1)
= cos x + j sin x.
Thus, ejx = cos x + j sin x. This is justiÔ¨Åcation for Euler‚Äôs identity in (1.49). Addi-
tionally, since e‚àíjx = cos x ‚àíj sin x, we have
ejx + e‚àíjx = 2 cos x,
ejx ‚àíe‚àíjx = 2j sin x.
These immediately imply that
sin x = ejx ‚àíe‚àíjx
2j
,
cos x = ejx + e‚àíjx
2
.
These identities allow for the conversion of expressions involving trig(onometric)
functions into expressions involving exponentials, and vice versa. The necessity to
do this is frequent. For this reason, they should be memorized, or else you should
remember how to derive them ‚Äúon the spot‚Äù when necessary.
Now observe that
rejŒ∏ = r cos Œ∏ + jr sin Œ∏,
so that if z = x + jy, then, because there exist r and Œ∏ such that x = r cos Œ∏ and
y = r sin Œ∏, we may immediately write
z = rejŒ∏.
TLFeBOOK

ELEMENTARY LOGIC
31
This is z in polar form. For example (assuming that Œ∏ is in radians)
1 + j =
‚àö
2ejœÄ/4,
‚àí1 + j =
‚àö
2e3œÄj/4,
1 ‚àíj =
‚àö
2e‚àíjœÄ/4,
‚àí1 ‚àíj =
‚àö
2e‚àí3œÄj/4.
It can sometimes be useful to observe that
j = ejœÄ/2,
‚àíj = e‚àíjœÄ/2,
and
‚àí1 = e¬±jœÄ.
If z1 = r1ejŒ∏1, and z2 = r2ejŒ∏2, then
z1z2 = r1r2ej(Œ∏1+Œ∏2),
z1
z2
= r1
r2
ej(Œ∏1‚àíŒ∏2).
In other words, multiplication and division of complex numbers is very easy when
they are expressed in polar form.
Finally, some terminology. For z = x + jy, we call x the real part of z, and we
call y the imaginary part of z. The notation is
x = Re [z],
y = Im [z].
That is, z = Re [z] + j Im [z].
APPENDIX 1.B
ELEMENTARY LOGIC
Here we summarize the basic language and ideas associated with elementary logic
as some of what is found here appears in later sections and chapters of this book.
The concepts found here appear often in mathematics and engineering literature.
Consider two mathematical statements represented as P and Q. Each statement
may be either true or false. Suppose that we know that if P is true, then Q is
certainly true (allowing the possibility that Q is true even if P is false). Then we
say that P implies Q, or Q is implied by P , or P is a sufÔ¨Åcient condition for Q,
or symbolically
P ‚áíQ
or
Q ‚áêP.
Suppose that if P is false, then Q is certainly false (allowing the possibility
that Q may be false even if P is true). Then we say that P is implied by Q, or Q
implies P , or P is a necessary condition for Q, or
P ‚áêQ
or
Q ‚áíP.
Now suppose that if P is true, then Q is certainly true, and if P is false, then
Q is certainly false. In other words, P and Q are either both true or both false.
Then we say that P implies and is implied by Q, or P is a necessary and sufÔ¨Åcient
TLFeBOOK

32
FUNCTIONAL ANALYSIS IDEAS
condition for Q, or P and Q are logically equivalent, or P if and only if Q, or
symbolically
P ‚áîQ.
A common abbreviation for ‚Äúif and only if‚Äù is iff.
The logical contrary of the statement P is called ‚Äúnot P .‚Äù It is often denoted by
either P or ‚àºP . This is the statement that is true if P is false, or false if P is true.
For example, if P is the statement ‚Äúx > 1,‚Äù then ‚àºP is the statement ‚Äúx ‚â§1.‚Äù If
P is the statement ‚Äúf (x) Ã∏= 0 for all x ‚ààR,‚Äù then ‚àºP is the statement ‚Äúthere is
at least one x ‚ààR for which f (x) = 0.‚Äù We may write
x4 ‚àí5x2 + 4 = 0 ‚áêx = 1
or
x = 2,
but the converse is not true because x4 ‚àí5x2 + 4 = 0 is a quartic equation pos-
sessing four possible solutions. We may write
x = 3 ‚áíx2 = 3x,
but we cannot say x2 = 3x ‚áíx = 3 because x = 0 is also possible.
Finally, we observe that
P ‚áíQ is equivalent to ‚àºP ‚áê‚àºQ,
P ‚áêQ is equivalent to ‚àºP ‚áí‚àºQ,
P ‚áîQ is equivalent to ‚àºP ‚áî‚àºQ;
that is, taking logical contraries reverses the directions of implication arrows.
REFERENCES
1. E.
Kreyszig,
Introductory Functional Analysis with Applications,
Wiley,
New
York, 1978.
2. A. P. Hillman and G. L. Alexanderson, A First Undergraduate Course in Abstract Alge-
bra, 3rd ed., Wadsworth, Belmont, CA, 1983.
3. R. B. J. T. Allenby, Rings, Fields and Groups: An Introduction to Abstract Algebra,
Edward Arnold, London, UK, 1983.
4. R. E. Blahut, Fast Algorithms for Digital Signal Processing, Addison-Wesley, Reading,
MA, 1985.
5. C. K. Chui, Wavelets: A Mathematical Tool for Signal Analysis. SIAM, Philadelphia,
PA, 1997.
6. R. E. Ziemer and W. H. Tranter, Principles of Communications: Systems, Modulation,
and Noise, 3rd ed., Houghton MifÔ¨Çin, Boston, MA, 1990.
7. I. Daubechies, ‚ÄúOrthonormal Bases of Compactly Supported Wavelets,‚Äù Commun. Pure
Appl. Math. 41, 909‚Äì996 (1988).
TLFeBOOK

PROBLEMS
33
8. G. P. Tolstov, Fourier Series (transl. from Russian by R. A. Silverman), Dover Publi-
cations, New York, 1962.
9. E. Kreyszig, Advanced Engineering Mathematics, 4th ed., Wiley, New York, 1979.
10. M. T. Heideman, D. H. Johnson and C. S. Burrus, ‚ÄúGauss and the History of the Fast
Fourier Transform,‚Äù IEEE ASSP Mag. 1, 14‚Äì21 (Oct. 1984).
11. J. W. Cooley and J. W. Tukey, ‚ÄúAn Algorithm for the Machine Calculation of Complex
Fourier Series,‚Äù Math. Comput., 19, 297‚Äì301 (April 1965).
PROBLEMS
1.1. (a) Find a, b ‚ààR in
1 + 2j
‚àí3 ‚àíj = a + bj.
(b) Find r, Œ∏ ‚ààR in
‚àí3 + j = rejŒ∏
(Of course, choose r > 0, and Œ∏ ‚àà(‚àíœÄ, œÄ].)
1.2. Solve for x ‚ààC in the quadratic equation
x2 ‚àí2r cos Œ∏x + r2 = 0.
Here r ‚â•0, and Œ∏ ‚àà(‚àíœÄ, œÄ]. Express your solution in polar form.
1.3. Let Œ∏, and œÜ be arbitrary angles (so Œ∏, œÜ ‚ààR). Show that
(cos Œ∏ + j sin Œ∏)(cos œÜ + j sin œÜ) = cos(Œ∏ + œÜ) + j sin(Œ∏ + œÜ).
1.4. Prove the following theorem. Suppose z ‚ààC such that
z = r cos Œ∏ + jr sin Œ∏
for which r = |z| > 0, and Œ∏ ‚àà(‚àíœÄ, œÄ]. Let n ‚àà{1, 2, 3, . . .} (i.e., n is a
positive integer). The n different nth roots of z are given by
r1/n

cos
 Œ∏ + 2œÄk
n
!
+ j sin
 Œ∏ + 2œÄk
n
!
,
for k = 0, 1, 2, . . ., n ‚àí1.
1.5. State whether the following are true or false:
(a) |x| < 2 ‚áíx < 2
(b) |x| < 3 ‚áê0 < x < 3
TLFeBOOK

34
FUNCTIONAL ANALYSIS IDEAS
(c) x ‚àíy > 0 ‚áíx > y > 0
(d) xy = 0 ‚áíx = 0 and y = 0
(e) x = 10 ‚áêx2 = 10x
Explain your answer in all cases.
1.6. Consider the function
f (x) =
 ‚àíx2 + 2x + 1,
0 ‚â§x < 1
x2 ‚àí2x + 3
2,
1 < x ‚â§2 .
Find
sup
x‚àà[0,2]
f (x),
inf
x‚àà[0,2] f (x).
1.7. Suppose that we have the following polynomials in the indeterminate x:
a(x) =
n

k=0
akxk,
b(x) =
m

j=0
bjxj.
Prove that
c(x) = a(x)b(x) =
n+m

l=0
clxl,
where
cl =
n

k=0
akbl‚àík.
[Comment: This is really asking us to prove that discrete convolution is
mathematically equivalent to polynomial multiplication. It explains why the
MATLAB routine for multiplying polynomials is called conv. Discrete con-
volution is a fundamental operation in digital signal processing, and is an
instance of something called Ô¨Ånite impulse response (FIR) Ô¨Åltering. You will
Ô¨Ånd it useful to note that ak = 0 for k < 0, and k > n, and that bj = 0 for
j < 0, and j > m. Knowing this allows you to manipulate the summation
limits to achieve the desired result.]
1.8. Recall Example 1.5. Suppose that xk = 2k+1, and that yk = 1 for k ‚ààZ+.
Find the sum of the series d(x, y). (Hint: Recall the theory of geometric
series. For example, N
k=0 Œ±k = 1‚àíŒ±N+1
1‚àíŒ±
if Œ± Ã∏= 1.)
1.9. Prove that if x Ã∏= 1, then
Sn =
n

k=1
kxk‚àí1
TLFeBOOK

PROBLEMS
35
is given by
Sn = 1 ‚àí(n + 1)xn + nxn+1
(1 ‚àíx)2
.
What is the formula for Sn when x = 1 ? (Hint: Begin by showing that
Sn ‚àíxSn = 1 + x + x2 + ¬∑ ¬∑ ¬∑ + xn‚àí1 ‚àínxn.)
1.10. Recall Example 1.1. Prove that d(x, y) in (1.5) satisÔ¨Åes all the axioms for a
metric.
1.11. Recall Example 1.18. Prove that ‚ü®x, y‚ü©in (1.36) satisÔ¨Åes all the axioms for
an inner product.
1.12. By direct calculation, show that if x, y, z are elements from an inner product
space, then
||z ‚àíx||2 + ||z ‚àíy||2 = 1
2||x ‚àíy||2 + 2||z ‚àí1
2(x + y)||2
(Appolonius‚Äô identity).
1.13. Suppose x, y ‚ààR3 (three-dimensional Euclidean space) such that
x = [1
1
1]T ,
y = [1
‚àí1
1]T .
Find all vectors z ‚ààR3 such that ‚ü®x, z‚ü©= ‚ü®y, z‚ü©= 0.
1.14. The complex Fourier series expansion method as described is for f ‚àà
L2(0, 2œÄ). Find the complex Fourier series expansion for f ‚ààL2(0, T ),
where 0 < T < ‚àû(i.e., the interval on which f is deÔ¨Åned is now of arbitrary
length).
1.15. Consider again the complex Fourier series expansion for f ‚ààL2(0, 2œÄ).
SpeciÔ¨Åcally, consider Eq. (1.44). If f (t) ‚ààR for all t ‚àà(0, 2œÄ), then show
that fn = f ‚àó
‚àín. [The sequence (fn) is conjugate symmetric.] Use this to show
that for suitable an, bn ‚ààR (all n) we have
‚àû

n=‚àí‚àû
fnejnt = a0 +
‚àû

n=1
[an cos(nt) + bn sin(nt)].
How are the coefÔ¨Åcients an and bn related to fn ? (Be very speciÔ¨Åc. There
is a simple formula.)
1.16. (a) Suppose that f ‚ààL2(0, 2œÄ), and that speciÔ¨Åcally
f (t) =
 1,
0 < t < œÄ
j,
œÄ ‚â§t < 2œÄ
.
Find fn in Eq. (1.44) using (1.45); that is, Ô¨Ånd the complex Fourier series
expansion for f (t). Make sure that you appropriately simplify your series
expansion.
TLFeBOOK

36
FUNCTIONAL ANALYSIS IDEAS
(b) Show how to use the result in Example 1.20 to Ô¨Ånd the complex Fourier
series expansion for f (t) in (a).
1.17. This problem is about Ô¨Ånding the Fourier series expansion for the wave-
form at the output of a full-wave rectiÔ¨Åer circuit. This circuit is used in
AC/DC (alternating/direct-current) converters. Knowledge of the Fourier
series expansion gives information to aid in the design of such converters.
(a) Find the complex Fourier series expansion of
f (t) =
sin
 2œÄ
T1
t
! ‚ààL2
 
0, T1
2
!
.
(b) Find the sequences (an), and (bn) in
f (t) = a0 +
‚àû

n=1

an cos
 2œÄn
T
t
!
+ bn sin
 2œÄn
T
t
!
for f (t) in (a). You need to consider how T is related to T1.
1.18. Recall the deÔ¨Ånitions of the Haar scaling function and Haar wavelet in
Eqs. (1.40) and (1.41), respectively. DeÔ¨Åne œÜk,n(t) = 2k/2œÜ(2kt ‚àín), and
œàk,n(t) = 2k/2œà(2kt ‚àín). Recall that ‚ü®f (t), g(t)‚ü©=
" ‚àû
‚àí‚àûf (t)g‚àó(t) dt is the
inner product for L2(R).
(a) Sketch œÜk,n(t), and œàk,n(t).
(b) Evaluate the integrals
 ‚àû
‚àí‚àû
œÜ2
k,n(t) dt,
and
 ‚àû
‚àí‚àû
œà2
k,n(t) dt.
(c) Prove that
‚ü®œÜk,n(t), œÜk,m(t)‚ü©= Œ¥n‚àím.
1.19. Prove the following version of the Schwarz inequality. For all x, y ‚ààX (inner
product space)
| Re [‚ü®x, y‚ü©]| ‚â§||x|| ||y||
with equality iff y = Œ≤x, and Œ≤ ‚ààR is a constant.
[Hint: The proof of this one is not quite like that of Theorem 1.1. Consider
‚ü®Œ±x + y, Œ±x + y‚ü©‚â•0 with Œ± ‚ààR. The inner product is to be viewed as a
quadratic in Œ±.]
1.20. The following result is associated with the proof of the uncertainty principle
for analog signals.
Prove that for f (t) ‚ààL2(R) such that |t|f (t) ‚ààL2(R) and f (1)(t) =
df (t)/dt ‚ààL2(R), we have the inequality
Re
 ‚àû
‚àí‚àû
tf (t)[f (1)(t)]‚àódt

2
‚â§
 ‚àû
‚àí‚àû
|tf (t)|2 dt
  ‚àû
‚àí‚àû
|f (1)(t)|2 dt

.
TLFeBOOK

PROBLEMS
37
1.21. Suppose ek = [ek,0 ek,1 ¬∑ ¬∑ ¬∑ ek,N‚àí2 ek,N‚àí1]T ‚ààCN, where
ek,n = exp

j 2œÄ
N kn

and k ‚ààZN. If x, y ‚ààCN recall that ‚ü®x, y‚ü©= N‚àí1
k=0 xky‚àó
k . Prove that
‚ü®ek, er‚ü©= NŒ¥k‚àír. Thus, B = {ek|k ‚ààZN} is an orthogonal basis for CN.
Set B is important in digital signal processing because it is used to deÔ¨Åne
the discrete Fourier transform.
TLFeBOOK

2
Number Representations
2.1
INTRODUCTION
In this chapter we consider how numbers are represented on a computer largely with
respect to the errors that occur when basic arithmetical operations are performed
on them. We are most interested here in so-called rounding errors (also called
roundoff errors). Floating-point computation is emphasized. This is due to the
fact that most numerical computation is performed with Ô¨Çoating-point numbers,
especially when numerical methods are implemented in high-level programming
languages such as C, Pascal, FORTRAN, and C++. However, an understanding
of Ô¨Çoating-point requires some understanding of Ô¨Åxed-point schemes Ô¨Årst, and so
this case will be considered initially. In addition, Ô¨Åxed-point schemes are used to
represent integer data (i.e., subsets of Z), and so the Ô¨Åxed-point representation is
important in its own right. For example, the exponent in a Ô¨Çoating-point number
is an integer.
The reader is assumed to be familiar with how integers are represented, and
how they are manipulated with digital hardware from a typical introductory dig-
ital electronics book or course. However, if this is not so, then some review of
this topic appears in Appendix 2.A. The reader should study this material now if
necessary.
Our main (historical) reference text for the material of this chapter is Wilkin-
son [1]. However, Golub and Van Loan [4, Section 2.4] is also a good refer-
ence. Golub and Van Loan [4] base their conventions and results in turn on
Forsythe et al. [5].
2.2
FIXED-POINT REPRESENTATIONS
We now consider Ô¨Åxed-point fractions. We must do so because the mantissa in a
Ô¨Çoating-point number is a Ô¨Åxed-point fraction.
We assume that fractions are t + 1 digits long. If the number is in binary, then
we usually say ‚Äút + 1 bits‚Äù long instead. Suppose, then, that x is a (t + 1)-bit
fraction. We shall write it in the form
(x)2 = x0.x1x2 ¬∑ ¬∑ ¬∑ xt‚àí1xt
(xk ‚àà{0, 1}).
(2.1)
An Introduction to Numerical Analysis for Electrical and Computer Engineers, by C.J. Zarowski
ISBN 0-471-46737-5
c‚Éù2004 John Wiley & Sons, Inc.
38
TLFeBOOK

FIXED-POINT REPRESENTATIONS
39
The notation (x)2 means that x is in base-2 (binary) form. More generally, (x)r
means that x is expressed as a base-r number (e.g., if r = 10 this would be the
decimal representation). We use this notation to emphasize which base we are
working with when necessary (e.g., to avoid ambiguity). We shall assume that
(2.1) is a two‚Äôs complement fraction. Thus, bit x0 is the sign bit. If this bit is 1,
we interpret the fraction to be negative; otherwise, it is nonnegative. For example,
(1.1011)2 = (‚àí0.3125)10. [To take the two‚Äôs complement of (1.1011)2, Ô¨Årst com-
plement every bit, and then add (0.0001)2. This gives (0.0101)2 = (0.3125)10.] In
general, for the case of a (t + 1)-bit two‚Äôs complement fraction, we obtain
‚àí1 ‚â§x ‚â§1 ‚àí2‚àít.
(2.2)
In fact
(‚àí1)10 = (1. 00 . . . 00
#
$%
&
t bits
)2,
(1 ‚àí2‚àít)10 = (0. 11 . . . 11
#
$%
&
t bits
)2.
(2.3)
We may regard (2.2) as specifying the dynamic range of the (t + 1)-bit two‚Äôs
complement fraction representation scheme. Numbers beyond this range are not
represented. JustiÔ¨Åcation of (2.2) [and (2.3)] would follow the argument for the
conversion of two‚Äôs complement integers into decimal integers that is considered
in Appendix 2.A.
Consider the set {x ‚ààR| ‚àí1 ‚â§x ‚â§1 ‚àí2‚àít}. In other words, x is a real number
within the limits imposed by (2.2), but it is not necessarily equal to a (t + 1)-bit
fraction. For example, x =
‚àö
2 ‚àí1 is in the range (2.2), but it is an irrational
number, and so does not possess an exact (t + 1)-bit representation. We may choose
to approximate such a number with t + 1 bits. Denote the (t + 1)-bit approximation
of x as Q[x]. For example, Q[x] might be the approximation to x obtained by
selecting an element from set
B = {bn = ‚àí1 + 2‚àítn|n = 0, 1, . . . , 2t+1 ‚àí1} ‚äÇR
(2.4)
that is the closest to x, where distance is measured by the metric in Example 1.1.
Note that each number in B is representable as a (t + 1)-bit fraction. In fact, B is the
entire set of (t + 1)-bit two‚Äôs complement fractions. Formally, our approximation
is given by
Q[x] =
argmin
n‚àà{0,1,...,2t+1‚àí1}
|x ‚àíbn|.
(2.5)
The notation ‚Äúargmin‚Äù means ‚Äúlet Q[x] be the bn for the n in the set
{0, 1, . . . , 2t+1 ‚àí1} that minimizes |x ‚àíbn|.‚Äù In other words, we choose the
argument bn that minimizes the distance to x. Some reÔ¨Çection (and perhaps con-
sidering some simple examples for small t) will lead the reader to conclude that
the error in this approximation satisÔ¨Åes
|x ‚àíQ[x]| ‚â§2‚àí(t+1).
(2.6)
TLFeBOOK

40
NUMBER REPRESENTATIONS
The error œµ = x ‚àíQ[x] is called quantization error. Equation (2.6) is an upper
bound on the size (norm) of this error. In fact, in the notation of Chapter 1, if
||x|| = |x|, then ||œµ|| = ||x ‚àíQ[x]|| ‚â§2‚àí(t+1). We remark that our quantization
method is not unique. There are many other methods, and these will generally lead
to different bounds.
When we represent the numbers in a computational problem on a computer,
we see that errors due to quantization can arise even before we perform any oper-
ations on the numbers at all. However, errors will also arise in the course of
performing basic arithmetic operations on the numbers. We consider the sources
of these now.
If x, y are coded as in (2.1), then their sum might not be in the range speciÔ¨Åed
by (2.2). This can happen only if x and y are either both positive or both negative.
Such a condition is Ô¨Åxed-point overÔ¨Çow. (A test for overÔ¨Çow in two‚Äôs complement
integer addition appears in Appendix 2.A, and it is easy to modify it for the problem
of overÔ¨Çow testing in the addition of fractions.) Similarly, overÔ¨Çow can occur when
a negative number is subtracted from a positive number, or if a positive number
is subtracted from a negative number. A test for this case is possible, too, but we
omit the details. Other than the problem of overÔ¨Çow, no errors can occur in the
addition or subtraction of fractions.
With respect to fractions, rounding error arises only when we perform multipli-
cation and division. We now consider errors in these operations.
We will deal with multiplication Ô¨Årst. Suppose that x and y are represented
according to (2.1). Suppose also that x0 = y0 = 0. It is easy to see that the product
of x and y is given by
p = xy =
' t
k=0
xk2‚àík
( ' t
n=0
yn2‚àín
(
= (x0 + x12‚àí1 + ¬∑ ¬∑ ¬∑ + xt2‚àít)(y0 + y12‚àí1 + ¬∑ ¬∑ ¬∑ + yt2‚àít)
= x0y0 + (x0y1 + x1y0)2‚àí1 + ¬∑ ¬∑ ¬∑ + xtyt2‚àí2t.
(2.7)
This implies that the product is a (2t + 1)-bit number. If we allow x and y to be
either positive or negative, then the product will also be 2t + 1 bits long. Of course,
one of these bits is the sign bit. If we had to multiply several numbers together,
we see that the product wordsize would grow in some proportion to the number of
factors in the product. The growth is clearly very rapid, and no practical computer
could sustain this for very long. We are therefore forced in general to round off the
product p back down to a number that is only t + 1 bits long. Obviously, this will
introduce an error.
How should the rounding be done? There is more than one possibility (just as
there is more than one way to quantize). Wilkinson [1, p. 4] suggests the following.
Since the product p has the form
(p)2 = p0.p1p2 ¬∑ ¬∑ ¬∑ pt‚àí1ptpt+1 ¬∑ ¬∑ ¬∑ p2t
(pk ‚àà{0, 1})
(2.8)
TLFeBOOK

FIXED-POINT REPRESENTATIONS
41
we may add 2‚àí(t+1) to this product, and then simply discard the last t bits of the
resulting sum (i.e., the bits indexed t + 1 to 2t). For example, suppose t = 4, and
consider
0.00111111 = p
+0.00001000 = 2‚àí5
0.01000111
Thus, the rounded product is (0.0100)2. The error involved in rounding in this
manner is not higher in magnitude than 1
22‚àít = 2‚àí(t+1). DeÔ¨Åne the result of the
rounding operation to be f x[p] = f x[xy], so then
|p ‚àíf x[p]| ‚â§1
22‚àít.
(2.9)
[For the previous example, p = (0.00111111)2, and so f x[p] = (0.0100)2.] It is
natural to measure the sizes of errors in the same way as we measured the size of
quantization errors earlier. Thus, (2.9) is an upper bound on the size of the error
due to rounding a product. As with quantization, other rounding methods would
generally give other bounds. We remark that Wilkinson‚Äôs suggestion amounts to
‚Äúordinary rounding.‚Äù
Finally, we consider Ô¨Åxed-point division. Again, suppose that x and y are rep-
resented as in (2.1), and consider the quotient q = x/y. Obviously, we must avoid
y = 0. Also, the quotient will not be in the permitted range given by (2.2) unless
|y| ‚â•|x|. This implies that when Ô¨Åxed-point division is implemented either the
dividend x or the divisor y need to be scaled to meet this restriction. Scaling is
multiplication by a power of 2, and so should be implemented to reduce rounding
error. We do not consider the speciÔ¨Åcs of how to achieve this. Another problem
is that x/y may require an inÔ¨Ånite number of bits to represent it. For example,
suppose
q = (0.0010)2
(0.0110)2
= (0.125)10
(0.375)10
=
 1
3
!
10
= (0.01)2.
The bar over 01 denotes the fact that this pattern repeats indeÔ¨Ånitely. Fortunately,
the same recipe for the rounding of products considered above may also be used
to round quotients. If f x[q] again denotes the result of applying this procedure to
q, then
|q ‚àíf x[q]| ‚â§1
22‚àít.
(2.10)
We see that the difÔ¨Åculties associated with division in Ô¨Åxed-point representations
means that Ô¨Åxed-point arithmetic should, if possible, not be used to implement
algorithms that require division. This forces us to either (1) employ Ô¨Çoating-point
representations or (2) develop algorithms that solve the problem without the need
for division operations.
Both strategies are employed in practice. Usually choice 1 is easier.
TLFeBOOK

42
NUMBER REPRESENTATIONS
2.3
FLOATING-POINT REPRESENTATIONS
In the previous section we have seen that Ô¨Åxed-point numbers are of very limited
dynamic range. This poses a major problem in employing them in engineering
computations since obviously we desire to work with numbers far beyond the
range in (2.2). Floating-point representations provide the deÔ¨Ånitive solution to this
problem. We remark (in passing) that the basic organization of a Ô¨Çoating-point
arithmetic unit [i.e., digital hardware for Ô¨Çoating-point addition and subtraction
appears in Ref. 2 (see pp. 295‚Äì306)]. There is a standard IEEE format for Ô¨Çoating-
point numbers. We do not consider this standard here, but it is summarized in
Ref. 2 (see pp. 304‚Äì306). Some of the technical subtleties associated with the
IEEE standard are considered by Higham [6].
Following Golub and Van Loan [4, p. 61], the set F (subset of R) of Ô¨Çoating-
point numbers consists of numbers of the form
x = x0.x1x2 ¬∑ ¬∑ ¬∑ xt‚àí1xt √ó re,
(2.11)
where x0 is a sign bit (which means that we can replace x0 by ¬±; this is done in
Ref. 4), and r is the base of the representation [typically r = 2 (binary), or r = 10
(decimal); we will emphasize r = 2]. Therefore, xk ‚àà{0, 1, . . . , r ‚àí2, r ‚àí1} for
1 ‚â§k ‚â§t. These are the digits (bits if r = 2) of the mantissa. We therefore see
that the mantissa is a fraction. 1 It is important to note that x1 Ã∏= 0, and this has
implications with regard to how operations are performed and the resulting rounding
errors. We call e the exponent. This is an integer quantity such that L ‚â§e ‚â§U.
For example, we might represent e as an n-bit two‚Äôs complement integer. We will
assume this unless otherwise speciÔ¨Åed in what follows. This would imply that
(e)2 = en‚àí1en‚àí2 ¬∑ ¬∑ ¬∑ e1e0, and so
‚àí2n‚àí1 ‚â§e ‚â§2n‚àí1 ‚àí1
(2.12)
(see Appendix A for justiÔ¨Åcation). For nonzero x ‚ààF, then
m ‚â§|x| ‚â§M,
(2.13a)
where
m = rL‚àí1,
M = rU(1 ‚àír‚àít).
(2.13b)
Equation (2.13) gives the dynamic range for the Ô¨Çoating-point representation. With
r = 2 we see that the total wordsize for the Ô¨Çoating-point number is t + n + 1 bits.
In the absence of rounding errors in a computation, our numbers may initially
be from the set
G = {x ‚ààR|m ‚â§|x| ‚â§M} ‚à™{0}.
(2.14)
1Including the sign bit the mantissa is (for r = 2) t + 1 bits long. Frequently in what follows we shall
refer to it as being only t bits long. This is because we are ignoring the sign bit, which is always
understood to be present.
TLFeBOOK

FLOATING-POINT REPRESENTATIONS
43
This set is analogous to the set {x ‚ààR| ‚àí1 ‚â§x ‚â§1 ‚àí2‚àít} that we saw in the
previous section in our study of Ô¨Åxed-point quantization effects. Again following
Golub and Van Loan [4], we may deÔ¨Åne a mapping (operator) f l|G ‚ÜíF. Here
c = f l[x] (x ‚ààG) is obtained by choosing the closest c ‚ààF to x. As you might
expect, distance is measured using || ¬∑ || = | ¬∑ |, as we did in the previous section.
Golub and Van Loan call this rounded arithmetic [4], and it coincides with the
rounding procedure described by Wilkinson [1, pp. 7‚Äì11].
Suppose that x and y are two Ô¨Çoating-point numbers (i.e., elements of F) and
that ‚Äúop‚Äù denotes any of the four basic arithmetic operations (addition, subtrac-
tion, multiplication, or division). Suppose |x op y| /‚ààG. This implies that either
|x op y| > M (Ô¨Çoating-point overÔ¨Çow), or 0 < |x op y| < m (Ô¨Çoating-point under-
Ô¨Çow) has occurred. Under normal circumstances an arithmetic fault such as over-
Ô¨Çow will not happen unless an unstable procedure is being performed. The issue
of ‚Äúnumerical stability‚Äù will be considered later. OverÔ¨Çows typically cause runtime
error messages to appear. The underÔ¨Çow arithmetic fault occurs when a number
arises that is not zero, but is too small to represent in the set F. This usually poses
less of a problem than overÔ¨Çow. 2 However, as noted before, we are concerned
mainly with rounding errors here. If |x op y| ‚ààG, then we assume that the com-
puter implementation of x op y will be given by f l[x op y]. In other words, the
operator f l models rounding effects in Ô¨Çoating-point arithmetic operations. We
remark that where Ô¨Çoating-point arithmetic is concerned, rounding error arises in
all four arithmetic operations. This contrasts with Ô¨Åxed-point arithmetic wherein
rounding errors arise only in multiplication and division.
It turns out that for the Ô¨Çoating-point rounding procedure suggested above
f l[x op y] = (x op y)(1 + œµ),
(2.15)
where
|œµ| ‚â§1
2r1‚àít(= 2‚àít
if
r = 2).
(2.16)
We shall justify this only for the case r = 2. Our arguments will follow those of
Wilkinson [1, pp. 7‚Äì11].
Let us now consider the addition of the base-2 Ô¨Çoating-point numbers
x = x0.x1 ¬∑ ¬∑ ¬∑ xt
#
$%
&
=mx
√ó2ex
(2.17a)
and
y = y0.y1 ¬∑ ¬∑ ¬∑ yt
#
$%
&
=my
√ó2ey,
(2.17b)
and we assume that |x| > |y|. (If instead |y| > |x|, then reverse the roles of x and
y.) If ex ‚àíey > t, then
f l[x + y] = x.
(2.18)
2UnderÔ¨Çows are simply set to zero on some machines.
TLFeBOOK

44
NUMBER REPRESENTATIONS
For example, if t = 4, and x = 0.1001 √ó 24, and y = 0.1110 √ó 2‚àí1, then to add
these numbers, we must shift the bits in the mantissa of one of them so that both
have the same exponent. If we choose y (usually shifting is performed on the
smallest number), then y = 0.00000111 √ó 24. Therefore, x + y = 0.10010111 √ó
24, but then f l[x + y] = 0.1001 √ó 24 = x.
Now if instead we have ex ‚àíey ‚â§t, we divide y by 2ex‚àíey by shifting its man-
tissa ex ‚àíey positions to the right. The sum x + 2ey‚àíexy is then calculated exactly,
and requires ‚â§2t bits for its representation. The sum is multiplied by a power of 2,
using left or right shifts to ensure that the mantissa is properly normalized [recall
that for x in (2.11) we must have x1 Ã∏= 0]. Of course, the exponent must be modi-
Ô¨Åed to account for the shift of the bits in the mantissa. The 2t-bit mantissa is then
rounded off to t bits using f l. Because we have |mx| + 2ey‚àíex|my| ‚â§1 + 1 = 2,
the largest possible right shift is by one bit position. However, a left shift of up to t
bit positions might be needed because of the cancellation of bits in the summation
process. Let us consider a few examples. We will assume that t = 4.
Example 2.1
Let x = 0.1001 √ó 24, and y = 0.1010 √ó 21. Thus
0.10010000 √ó 24
+0.00010100 √ó 24
0.10100100 √ó 24
and the sum is rounded to 0.1010 √ó 24 (computed sum).
Example 2.2
Let x = 0.1111 √ó 24, and y = 0.1010 √ó 22. Thus
0.11110000 √ó 24
+0.00101000 √ó 24
1.00011000 √ó 24
but 1.00011000 √ó 24 = 0.100011000 √ó 25, and this exact sum is rounded to 0.1001
√ó 25 (computed sum).
Example 2.3
Let x = 0.1111 √ó 2‚àí4, and y = ‚àí.1110 √ó 2‚àí4. Thus
0.11110000 √ó 2‚àí4
‚àí0.11100000 √ó 2‚àí4
0.00010000 √ó 2‚àí4
but 0.00010000 √ó 2‚àí4 = 0.1000 √ó 2‚àí7, and this exact sum is rounded to 0.1000 √ó
2‚àí7 (computed sum). Here there is much cancellation of the bits leading in turn
to a large shift of the mantissa of the exact sum to the left. Yet, the computed sum
is exact.
TLFeBOOK

FLOATING-POINT REPRESENTATIONS
45
We observe that the computed sum is obtained by computing the exact sum,
normalizing it so that the mantissa s0.s1 ¬∑ ¬∑ ¬∑ st‚àí1stst+1 ¬∑ ¬∑ ¬∑ s2t satisÔ¨Åes s1 = 1 (i.e.,
s1 Ã∏= 0), and then we round it to t places (i.e., we apply f l). If the normalized
exact sum is s = ms √ó 2es(= x + y), then the rounding error œµ‚Ä≤ is such that |œµ‚Ä≤| ‚â§
1
22‚àít2es. Essentially, the error œµ‚Ä≤ is due to rounding the mantissa (a Ô¨Åxed-point
number) according to the method used in Section 2.2. Because of the form of ms,
1
22es ‚â§|s| < 2es, and so
f l[x + y] = (x + y)(1 + œµ)
(2.19)
which is just a special case of (2.15). This expression requires further explanation,
however. Observe that
|s ‚àíf l[s]|
|s|
= |s ‚àí(s + œµ‚Ä≤)|
|s|
= |œµ‚Ä≤|
|s| ‚â§
1
22‚àít2es
|s|
which is the relative error3 due to rounding. Because we have 1
22es ‚â§|s| < 2es,
this error is biggest when |s| = 1
22es, so therefore we conclude that
|s ‚àíf l[s]|
|s|
‚â§2‚àít.
(2.20)
From (2.19) f l[s] = s + sœµ, so that |s ‚àíf l[s]| = |s||œµ|, or |œµ| = |s ‚àíf l[s]|/|s|.
Thus, |œµ| ‚â§2‚àít, which is (2.16). In other words, |œµ‚Ä≤| is the absolute error, and |œµ|
is the relative error.
Finally, if x = 0 or y = 0 then no rounding error occurs: œµ = 0. Subtraction
results do not differ from addition.
Now consider computing the product of x and y in (2.17). Since x = mx √ó 2ex,
and y = my √ó 2ey with x1 Ã∏= 0, and y1 Ã∏= 0 we must have
1
2
1
2 ‚â§|mxmy| < 1.
(2.21)
This implies that it may be necessary to normalize the mantissa of the product with
a shift to the left, and an appropriate adjustment of the exponent as well. The 2t-bit
mantissa of the product is rounded to give a t-bit mantissa. If x = 0, or y = 0 (or
both x and y are zero), then the product is zero.
3In general, if a is the exact value of some quantity and ÀÜa is some approximation to a, the absolute
error is ||a ‚àíÀÜa||, while the relative error is
||a ‚àíÀÜa||
||a||
(a Ã∏= 0).
The relative error is usually more meaningful in practice. This is because an error is really ‚Äúbig‚Äù or
‚Äúsmall‚Äù only in relation to the size of the quantity being approximated.
TLFeBOOK

46
NUMBER REPRESENTATIONS
We may consider a few examples. We will suppose t = 4. Begin with x =
0.1010 √ó 22, and y = 0.1111 √ó 21, so then
xy = 0.10010110 √ó 23,
and so f l[xy] = 0.1001 √ó 23 (computed product). If now x = 0.1000 √ó 24, y =
0.1000 √ó 2‚àí1, then, before normalizing the mantissa, we have
xy = 0.01000000 √ó 23,
and after normalization we have
xy = 0.10000000 √ó 22
so that f l[xy] = 0.1000 √ó 22 (computed product). Finally, suppose that x =
0.1010 √ó 20, and y = 0.1010 √ó 20, so then the unnormalized product is
xy = 0.01100100 √ó 20
for which the normalized product is
xy = 0.11001000 √ó 2‚àí1,
so Ô¨Ånally f l[xy] = 0.1101 √ó 2‚àí1 (computed product).
The application of f l to the normalized product will have exactly the same
effect as it did in the case of addition (or of subtraction). This may be under-
stood by recognizing that a 2t-bit mantissa will ‚Äúlook the same‚Äù to operator f l
regardless of how that mantissa was obtained. It therefore immediately follows
that
f l[xy] = (xy)(1 + œµ),
(2.22)
which is another special case of (2.15), and |œµ| ‚â§2‚àít, which is (2.16) again.
Now consider the quotient x/y, for x and y Ã∏= 0 in (2.17),
q = x
y = mx √ó 2ex
my √ó 2ey = mx
my
√ó 2ex‚àíey = mq √ó 2eq
(2.23)
(so mq = mx/my, and eq = ex ‚àíey). The arithmetic unit in the machine has an
accumulator that we assume contains mx and which is ‚Äúdouble length‚Äù in that it
is 2t bits long. SpeciÔ¨Åcally, this accumulator initially stores x0.x1 ¬∑ ¬∑ ¬∑ xt 0 ¬∑ ¬∑ ¬∑ 0
# $% &
t bits
. If
|mx| > |my| the number in the accumulator is shifted one place to the right, and
so eq is increased by one (i.e., incremented). The number in the accumulator is
then divided by my in such a manner as to give a correctly rounded t-bit result.
This implies that the computed mantissa of the quotient, say, mq = q0.q1 ¬∑ ¬∑ ¬∑ qt,
TLFeBOOK

FLOATING-POINT REPRESENTATIONS
47
satisÔ¨Åes the normalization condition q1 = 1, so that 1
2 ‚â§|mq| < 1. Once again we
must have
f l
x
y

= x
y (1 + œµ)
(2.24)
such that |œµ| ‚â§2‚àít. Therefore, (2.15) and (2.16) are now justiÔ¨Åed for all instances
of op.
We complete this section with a few examples. Suppose x = 0.1010 √ó 22, and
y = 0.1100 √ó 2‚àí2, then
q = x
y = 0.1010 √ó 22
0.1100 √ó 2‚àí2 = 0.10100000 √ó 22
0.1100 √ó 2‚àí2
= 0.10100000
0.1100
√ó 24 = 0.11010101 √ó 24
so that f l[q] = 0.1101 √ó 24 (computed quotient). Now suppose that x = 0.1110 √ó
23, and y = 0.1001 √ó 2‚àí2, and so
q = x
y = 0.1110 √ó 23
0.1001 √ó 2‚àí2 = 0.01110000 √ó 24
0.1001 √ó 2‚àí2
= 0.01110000
0.1001
√ó 26 = 0.11000111 √ó 26
so that f l[q] = 0.1100 √ó 26 (computed quotient).
Thus far we have emphasized ordinary rounding, but an alternative implemen-
tation of f l is to use chopping. If x = ¬±
)‚àû
k=1 xk2‚àík*
√ó 2e, then, for chopping
operator f l, we have f l[x] = ¬±
)t
k=1 xk2‚àík*
√ó 2e (chopping x to t + 1 bits
including the sign bit). Thus, the absolute error is
|œµ‚Ä≤| = |x ‚àíf l[x]| =
Ô£´
Ô£≠
‚àû

k=t+1
xk2‚àík
Ô£∂
Ô£∏2e ‚â§2e
‚àû

k=t+1
2‚àík
(as xk = 1 for all k > t), but since ‚àû
k=t+1 2‚àík = 2‚àít, we must have
|œµ‚Ä≤| = |x ‚àíf l[x]| ‚â§2‚àít2e,
and so the relative error for chopping is
|œµ| = |x ‚àíf l[x]|
|x|
‚â§2‚àítee
1
22e
= 2‚àít+1
(because we recall that |x| ‚â•1
22e). We see that the error in chopping is somewhat
bigger than the error in rounding, but chopping is somewhat easier to implement.
TLFeBOOK

48
NUMBER REPRESENTATIONS
2.4
ROUNDING EFFECTS IN DOT PRODUCT COMPUTATION
Suppose x, y ‚ààRn. We recall from Chapter 1 (and from elementary linear algebra)
that the vector dot product is given by
‚ü®x, y‚ü©= xT y = yT x =
n‚àí1

k=0
xkyk.
(2.25)
This operation occurs in matrix‚Äìvector product computation (e.g., y = Ax, where
A ‚ààRn√ón), digital Ô¨Ålter implementation (i.e., computing discrete-time convolu-
tion), numerical integration, and other applications. In other words, it is so common
that it is important to understand how rounding errors can affect the accuracy of a
computed dot product.
We may regard dot product computation as a recursive process. Thus
sn‚àí1 =
n‚àí1

k=0
xkyk =
n‚àí2

k=0
xkyk + xn‚àí1yn‚àí1 = sn‚àí2 + xn‚àí1yn‚àí1.
So
sk = sk‚àí1 + xkyk
(2.26)
for k = 0, 1, . . . , n ‚àí1, and s‚àí1 = 0. Each arithmetic operation in (2.26) is a
separate Ô¨Çoating-point operation and so introduces its own error into the over-
all calculation. We would like to obtain a general expression for this error. To
begin, we may model the computation process according to
ÀÜs0 = f l[x0y0]
ÀÜs1 = f l[ÀÜs0 + f l[x1y1]]
ÀÜs2 = f l[ÀÜs1 + f l[x2y2]]
...
ÀÜsn‚àí2 = f l[ÀÜsn‚àí3 + f l[xn‚àí2yn‚àí2]]
ÀÜsn‚àí1 = f l[ÀÜsn‚àí2 + f l[xn‚àí1yn‚àí1]].
(2.27)
From (2.15) we may write
ÀÜs0 = (x0y0)(1 + Œ¥0)
ÀÜs1 = [ÀÜs0 + (x1y1)(1 + Œ¥1)](1 + œµ1)
ÀÜs2 = [ÀÜs1 + (x2y2)(1 + Œ¥2)](1 + œµ2)
...
TLFeBOOK

ROUNDING EFFECTS IN DOT PRODUCT COMPUTATION
49
ÀÜsn‚àí2 = [ÀÜsn‚àí3 + (xn‚àí2yn‚àí2)(1 + Œ¥n‚àí2)](1 + œµn‚àí2)
ÀÜsn‚àí1 = [ÀÜsn‚àí2 + (xn‚àí1yn‚àí1)(1 + Œ¥n‚àí1)](1 + œµn‚àí1),
(2.28)
where |Œ¥k| ‚â§2‚àít (for k = 0, 1, . . . , n ‚àí1), and |œµk| ‚â§2‚àít (for k = 1, 2, . . . ,
n ‚àí1), via (2.16). It is possible to write4
ÀÜsn‚àí1 =
n‚àí1

k=0
xkyk(1 + Œ≥k) = sn‚àí1 +
n‚àí1

k=0
xkykŒ≥k,
(2.29)
where
1 + Œ≥k = (1 + Œ¥k)
n‚àí1
/
j=k
(1 + œµj)(œµ0 = 0).
(2.30)
Note that the 
 notation means, for example
n
/
k=0
xk = x0x1x2 ¬∑ ¬∑ ¬∑ xn‚àí1xn,
(2.31)
where 
 is the symbol to compute the product of all xk for k = 0, 1, . . . , n. The
similarity to how we interpret  notation should therefore be clear.
The absolute value operator is a norm on R, so from the axioms for a norm
(recall DeÔ¨Ånition 1.3), we must have
|sn‚àí1 ‚àíÀÜsn‚àí1| = |xT y ‚àíf l[xT y]| ‚â§
n‚àí1

k=0
|xkyk||Œ≥k|.
(2.32)
In particular, obtaining this involves the repeated use of the triangle inequality.
Equation (2.32) thus represents an upper bound on the absolute error involved in
computing a vector dot product. Of course, the notation f l[xT y] symbolizes the
Ô¨Çoating-point approximation to the exact quantity xT y. However, the bound in
(2.32) is incomplete because we need to appropriately bound the numbers Œ≥k.
To obtain the bound we wish involves using the following lemma.
Lemma 2.1: We have
1 + x ‚â§ex,
x ‚â•0
(2.33a)
ex ‚â§1 + 1.01x,
0 ‚â§x ‚â§.01.
(2.33b)
4Equation (2.29) is most easily arrived at by considering examples for small n, for instance
ÀÜs3 = x0y0(1 + Œ¥0)(1 + œµ0)(1 + œµ1)(1 + œµ2)(1 + œµ3) + x1y1(1 + Œ¥1)(1 + œµ1)(1 + œµ2)(1 + œµ3)
+ x2y2(1 + Œ¥2)(1 + œµ2)(1 + œµ3) + x3y3(1 + Œ¥3)(1 + œµ3),
and using such examples to ‚Äúspot the pattern.‚Äù
TLFeBOOK

50
NUMBER REPRESENTATIONS
Proof
Begin with consideration of (2.33a). Recall that for ‚àí‚àû< x < ‚àû
ex =
‚àû

n=0
xn
n! .
(2.34)
Therefore
ex = 1 + x +
‚àû

n=2
xn
n!
so that
1 + x = ex ‚àí
‚àû

n=2
xn
n! ,
but the terms in the summation are all nonnegative, so (2.33a) follows immediately.
Now consider (2.33b), which is certainly valid for x = 0. The result will follow
if we prove
ex ‚àí1
x
‚â§1.01
(x Ã∏= 0).
From (2.34)
ex ‚àí1
x
=
‚àû

m=0
xm
(m + 1)! = 1 +
‚àû

m=1
xm
(m + 1)!
so we may also equivalently prove instead that
‚àû

m=1
xm
(m + 1)! ‚â§0.01
for 0 < x ‚â§0.01. Observe that
‚àû

m=1
xm
(m + 1)! = 1
2x + 1
6x2 + 1
24x3 + ¬∑ ¬∑ ¬∑ ‚â§1
2x + x2 + x3 + x4 + ¬∑ ¬∑ ¬∑
= 1
2x +
‚àû

k=2
xk = 1
2x +
‚àû

k=0
xk ‚àí1 ‚àíx
=
1
1 ‚àíx ‚àí1
2x ‚àí1 = 1
2x 1 + x
1 ‚àíx .
It is not hard to verify that
1
2x 1 + x
1 ‚àíx ‚â§0.01
for 0 < x ‚â§0.01. Thus, (2.33b) follows.
TLFeBOOK

ROUNDING EFFECTS IN DOT PRODUCT COMPUTATION
51
If n = 1, 2, 3, . . ., and if 0 ‚â§nu ‚â§0.01, then
(1 + u)n ‚â§(eu)n
[via (2.33a)]
‚â§1 + 1.01nu
[via (2.33b)].
(2.35)
Now if |Œ¥i| ‚â§u for i = 0, 1, . . . , n ‚àí1 then
n‚àí1
/
i=0
(1 + Œ¥i) ‚â§
n‚àí1
/
i=0
(1 + |Œ¥i|) ‚â§(1 + u)n
so via (2.35)
n‚àí1
/
i=0
(1 + Œ¥i) ‚â§1 + 1.01nu,
(2.36)
where we must emphasize that 0 ‚â§nu ‚â§0.01. Certainly there is a Œ¥ such that
1 + Œ¥ =
n‚àí1
/
i=0
(1 + Œ¥i),
(2.37)
and so from (2.36), |Œ¥| ‚â§1.01nu. If we identify Œ≥k with Œ¥ in (2.33) for all k, then
|Œ≥k| ‚â§1.01nu
(2.38)
for which we consider u = 2‚àít [because in (2.30) both |œµi| and |Œ¥i| ‚â§2‚àít]. Using
(2.38) in (2.32), we obtain
|xT y ‚àíf l[xT y]| ‚â§1.01nu
n‚àí1

k=0
|xkyk|,
(2.39)
but n‚àí1
k=0 |xkyk| = n‚àí1
k=0 |xk||yk|, and this may be symbolized as |x|T |y| (so that
|x| = [|x0||x1| ¬∑ ¬∑ ¬∑ |xn‚àí1|]T ). Thus, we may rewrite (2.39) as
|xT y ‚àíf l[xT y]| ‚â§1.01nu|x|T |y|.
(2.40)
Observe that the relative error satisÔ¨Åes
|xT y ‚àíf l[xT y]|
|xT y|
‚â§1.01nu|x|T |y|
|xT y| .
(2.41)
The bound in (2.41) may be quite large if |x|T |y| ‚â´|xT y|. This suggests the
possibility of a large relative error. We remark that since u = 2‚àít, nu ‚â§0.01 will
hold in all practical cases unless n is very large (a typical value for t is t = 56).
The potentially large relative errors indicated by the analysis we have just made
are a consequence of the details of how the dot product was calculated. As noted
on p. 65 of Ref. 4, the use of a double-precision accumulator to compute the dot
TLFeBOOK

52
NUMBER REPRESENTATIONS
product can reduce the error dramatically. Essentially, if x and y are Ô¨Çoating-
point vectors with t-bit mantissas, the ‚Äúrunning sum‚Äù sk [of (2.26)] is built up in
an accumulator with a 2t-bit mantissa. Multiplication of two t-bit numbers can
be stored exactly in a double-precision variable. The large dynamic Ô¨Çoating-point
range limits the likelihood of overÔ¨Çow/underÔ¨Çow. Only when Ô¨Ånal sum sn‚àí1 is
written to a single-precision memory location will there be a rounding error. It
therefore follows that when this alternative procedure is employed, we get
f l[xT y] = xT y(1 + Œ¥)
(2.42)
for which |Œ¥| ‚âà2‚àít (= u). Clearly, this is a big improvement.
The material of this section shows
1. The analysis required to obtain insightful bounds on errors can be quite
arduous.
2. Proper numerical technique can have a dramatic effect in reducing errors.
3. Proper technique can be revealed by analysis.
The following example illustrates how the bound on rounding error in dot prod-
uct computation may be employed.
Example 2.4
Assume the existence of a square root function such that
f l[‚àöx] = ‚àöx(1 + œµ) and |œµ| ‚â§u. We use the algorithm that corresponds to the
bound of Eq. (2.40) to compute xT x (x ‚ààRn), and then use this to give an algo-
rithm for ||x|| =
‚àö
xT x. This can be expressed in the form of pseudocode:
s‚àí1 := 0;
for k := 0 to n ‚àí1 do begin
sk := sk‚àí1 + x2
k;
end;
||x|| := ‚àösn‚àí1;
We will now obtain a bound on the relative error due to rounding in the computation
of ||x||. We will use the fact that
‚àö
1 + x ‚â§1 + x (for x ‚â•0).
Now
œµ1 = f l[xT x] ‚àíxT x
xT x
‚áíf l[xT x] = xT x(1 + œµ1),
and via (2.41)
|œµ1| ‚â§1.01nu|x|T |x|
|xT x|
= 1.01nu||x||2
||x||2 = 1.01nu
(|x|T |x| = n‚àí1
k=0 |xk|2 = n‚àí1
k=0 x2
k = ||x||2, and |||x||2| = ||x||2). So in ‚Äúshort-
hand‚Äù notation, f l[

f l[xT x]] ‚â°f l[||x||], and
f l[||x||] =

xT x

1 + œµ1(1 + œµ) = ||x||

1 + œµ1(1 + œµ),
TLFeBOOK

MACHINE EPSILON
53
and ‚àö1 + œµ1 ‚â§1 + œµ1, so
f l[||x||] ‚â§||x||(1 + œµ1)(1 + œµ).
Now (1 + œµ1)(1 + œµ) = 1 + œµ1 + œµ + œµ1œµ, implying that
||x||(1 + œµ1)(1 + œµ) = ||x|| + ||x||(œµ1 + œµ + œµ1œµ)
so therefore
f l[||x||] ‚â§||x|| + ||x||(œµ1 + œµ + œµ1œµ),
and thus

f l[||x||] ‚àí||x||
||x||
 ‚â§|œµ1 + œµ + œµ1œµ| ‚â§u + 1.01nu + 1.01nu2
= u[1 + 1.01n + 1.01nu].
Of course, we have used the fact that |œµ| ‚â§u.
2.5
MACHINE EPSILON
In Section 2.3 upper bounds on the error involved in applying the operator f l were
derived. SpeciÔ¨Åcally, we found that the relative error satisÔ¨Åes
|œµ‚Ä≤| = |x ‚àíf l[x]|
|x|
‚â§
 2‚àít
(rounding)
2‚àít+1
(chopping) .
(2.43)
As suggested in Section 2.4, these bounds are often denoted by u; that is, u = 2‚àít
for rounding, and u = 2‚àít+1 for chopping. The bound u is often called the unit
roundoff [4, Section 2.4.2].
The details of how Ô¨Çoating-point arithmetic is implemented on any given com-
puting machine may not be known or readily determined by the user. Thus, u
may not be known. However, an ‚Äúexperimental‚Äù approach is possible. One may
run a simple program to ‚Äúestimate‚Äù u, and the estimate is the machine epsilon,
denoted œµM. The machine epsilon is deÔ¨Åned to be the difference between 1.0 and
the next biggest Ô¨Çoating-point number [6, Section 2.1]. Consequently, œµM = 2‚àít+1.
A pseudocode to compute œµM is as follows:
stop := 1;
eps := 1.0;
while stop == 1 do begin
eps := eps/2.0;
x := 1.0 + eps;
if x ‚â§1.0
begin
TLFeBOOK

54
NUMBER REPRESENTATIONS
stop := 0;
end;
end;
eps := 2.0 ‚àóeps;
This code may be readily implemented as a MATLAB routine. MATLAB stores
eps (= œµM) as a built-in constant, and the reader may wish to test the code above
to see if the result agrees with MATLAB eps (as a programming exercise).
In this book we shall (unless otherwise stated) regard machine epsilon and unit
roundoff as practically interchangeable.
APPENDIX 2.A
REVIEW OF BINARY NUMBER CODES
This appendix summarizes typical methods used to represent integers in binary.
Extension of the results in this appendix to fractions is certainly possible. This
material is normally to be found in introductory digital electronics books. The
reader is here assumed to know Boolean algebra. This implies that the reader
knows that + can represent either algebraic addition, or the logical or operation.
Similarly, xy might mean the logical and of the Boolean variables x and y, or it
might mean the arithmetic product of the real variables x and y. The context must
be considered to ascertain which meaning applies.
Below we speak of ‚Äúcomplements.‚Äù These are used to represent negative inte-
gers, and also to facilitate arithmetic with integers. We remark that the results of
this appendix are presented in a fairly general manner. Thus, the reader may wish,
for instance, to see numerical examples of arithmetic using two‚Äôs complement (2‚Äôs
comp.) codings. The reader can consult pp. 276‚Äì280 of Ref. 2 for such examples.
Almost any other books on digital logic will also provide a source of numerical
examples [3].
We may typically interpret a bit pattern in one of four ways, assuming that the
bit pattern is to represent a number (negative or nonnegative integer). An example
of this is as follows, and it provides a summary of common representations (e.g.,
for n = 3 bits):
Bit Pattern
Unsigned Integer
2‚Äôs Comp.
1‚Äôs Comp.
Sign Magnitude
0
0
0
0
0
0
0
0
0
1
1
1
1
1
0
1
0
2
2
2
2
0
1
1
3
3
3
3
1
0
0
4
‚àí4
‚àí3
‚àí0
1
0
1
5
‚àí3
‚àí2
‚àí1
1
1
0
6
‚àí2
‚àí1
‚àí2
1
1
1
7
‚àí1
‚àí0
‚àí3
TLFeBOOK

REVIEW OF BINARY NUMBER CODES
55
In the four coding schemes summarized in this table, the interpretation of the bit
pattern is always the same when the most signiÔ¨Åcant bit (MSB) is zero. A similar
table for n = 4 appears in Hamacher et al. [2, see p. 271].
Note that, philosophically speaking, the table above implies that a bit pattern
can have more than one meaning. It is up to the engineer to decide what meaning
it should have. Of course, this will be a function of purpose. Presently, our purpose
is that bit patterns should have meaning with respect to the problems of numerical
computing; that is, bit patterns must represent numerical information.
The relative merits of the three signed number coding schemes illustrated in the
table above may be summarized as follows:
Coding Scheme
Advantages
Disadvantages
2‚Äôs complement
Simple adder/subtracter
circuit
Only one code for 0
Circuit for Ô¨Ånding the 2‚Äôs comp.
more complex than circuit for
Ô¨Ånding the 1‚Äôs comp.
1‚Äôs complement
Easy to obtain the 1‚Äôs comp.
of a number
Circuit for addition and
subtraction more complex
than for the 2‚Äôs comp.
adder/subtracter
Two codes for 0
Sign magnitude
Intuitively obvious code
Has the most complex
adder/subtracter circuit
Two codes for 0
The following is a summary of some formulas associated with arithmetic (i.e.,
addition and subtraction) with r‚Äôs and (r ‚àí1)‚Äôs complements. In binary arithmetic
r = 2, while in decimal arithmetic r = 10. We emphasize the case r = 2.
Let A be an n-digit base-r number (integer)
A = An‚àí1An‚àí2 ¬∑ ¬∑ ¬∑ A1A0
where Ak ‚àà{0, 1, . . . , r ‚àí2, r ‚àí1}. Digit An‚àí1 is the most signiÔ¨Åcant digit
(MSD), while digit A0 is the least signiÔ¨Åcant digit (LSD). Provided that A is
not negative (i.e., is unsigned), we recognize that to convert A to a base-10 repre-
sentation (i.e., ordinary decimal number) requires us to compute
n‚àí1

k=0
Akrk.
If A is allowed to be a negative integer, the usage of this summation needs modi-
Ô¨Åcation. This is considered below.
The r‚Äôs complement of A is deÔ¨Åned to be
r‚Äôs complement of A = A‚àó=
 rn ‚àíA,
A Ã∏= 0
0,
A = 0
(2.A.1)
TLFeBOOK

56
NUMBER REPRESENTATIONS
The (r ‚àí1)‚Äôs complement of A is deÔ¨Åned to be
(r ‚àí1)‚Äôs complement of A = A = (rn ‚àí1) ‚àíA
(2.A.2)
It is important not to confuse the bar over the A in (2.A.2) with the Boolean not
operation, although for the special case of r = 2 the bar will denote complemen-
tation of each bit of A; that is, for r = 2
A = An‚àí1An‚àí2 ¬∑ ¬∑ ¬∑ A1A0
where the bar now denotes the logical not operation. More generally, if A is a
base-r number
A = (r ‚àí1) ‚àíAn‚àí1
(r ‚àí1) ‚àíAn‚àí2
¬∑ ¬∑ ¬∑ (r ‚àí1) ‚àíA1
(r ‚àí1) ‚àíA0
Thus, to obtain A, each digit of A is subtracted from r ‚àí1. As a consequence,
comparing (2.A.1) and (2.A.2), we see that
A‚àó= A + 1
(2.A.3)
where the plus denotes algebraic addition (which takes place in base r).
Now we consider the three (previously noted) different methods for coding
integers when r = 2:
1. Sign-magnitude coding
2. One‚Äôs complement coding
3. Two‚Äôs complement coding
In all three of these coding schemes the most signiÔ¨Åcant bit (MSB) is the sign bit.
SpeciÔ¨Åcally , if An‚àí1 = 0, the number is nonnegative, and if An‚àí1 = 1, the number
is negative. It can be shown that when the complement (either one‚Äôs or two‚Äôs) of
a binary number is taken, this is equivalent to placing a minus sign in front of the
number. As a consequence, when given a binary number A = An‚àí1An‚àí2 ¬∑ ¬∑ ¬∑ A1A0
coded according to one of these three schemes, we may convert that number to a
base-10 integer according to the following formulas:
1. Sign-Magnitude Coding. The sign-magnitude binary number A = An‚àí1An‚àí2
¬∑ ¬∑ ¬∑ A1A0 (Ak ‚àà{0, 1}) has the base-10 equivalent
A =
Ô£±
Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£≤
Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£≥
n‚àí2

i=0
Ai2i,
An‚àí1 = 0
‚àí
n‚àí2

i=0
Ai2i,
An‚àí1 = 1
(2.A.4)
TLFeBOOK

REVIEW OF BINARY NUMBER CODES
57
With this coding scheme there are two codings for zero:
(0)10 = (000 ¬∑ ¬∑ ¬∑ 00
#
$%
&
n
)2 = (100 ¬∑ ¬∑ ¬∑ 00
#
$%
&
n
)2
2. One‚Äôs Complement Coding. In this coding we represent ‚àíA as A. The one‚Äôs
complement binary number A = An‚àí1An‚àí2 ¬∑ ¬∑ ¬∑ A1A0 (Ak ‚àà{0, 1}) has the
base-10 equivalent
A =
Ô£±
Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£≤
Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£≥
n‚àí2

i=0
Ai2i
, An‚àí1 = 0
‚àí
n‚àí2

i=0
Ai2i
, An‚àí1 = 1
(2.A.5)
With this coding scheme there are also two codes for zero:
(0)10 = (000 ¬∑ ¬∑ ¬∑ 00
#
$%
&
n
)2 = (111 ¬∑ ¬∑ ¬∑ 11
#
$%
&
n
)2
3. Two‚Äôs Complement Coding. In this coding we represent ‚àíA as A‚àó(= A + 1).
The two‚Äôs complement binary number A = An‚àí1An‚àí2 ¬∑ ¬∑ ¬∑ A1A0 (Ak ‚àà{0, 1})
has the base-10 equivalent
A = ‚àí2n‚àí1An‚àí1 +
n‚àí2

i=0
Ai2i
(2.A.6)
The proof is as follows. If An‚àí1 = 0, then A ‚â•0 and immediately the base-10
equivalent is A = n‚àí2
i=0 Ai2i (via the procedure for converting a number in
base-2 to one in base-10), which is (2.A.6) for An‚àí1 = 0. Now, if An‚àí1 = 1,
then A < 0, and so if we take the two‚Äôs complement of A we must get |A|:
|A| = A + 1
= (1 ‚àíAn‚àí1)(1 ‚àíAn‚àí2) ¬∑ ¬∑ ¬∑ (1 ‚àíA1)(1 ‚àíA0) + 00 ¬∑ ¬∑ ¬∑ 01
#
$%
&
n
=
n‚àí1

i=0
(1 ‚àíAi)2i + 1
= 2n‚àí1(1 ‚àíAn‚àí1) +
n‚àí2

i=0
(1 ‚àíAi)2i + 1
=
n‚àí2

i=0
2i + 1 ‚àí
n‚àí2

i=0
Ai2i(An‚àí1 = 1)
TLFeBOOK

58
NUMBER REPRESENTATIONS
= 1 ‚àí2n‚àí1
1 ‚àí2
+ 1 ‚àí
n‚àí2

i=0
Ai2i
'
via
n

i=0
ai = 1 ‚àían+1
1 ‚àía
(
= 2n‚àí1 ‚àí
n‚àí2

i=0
Ai2i
and so A = ‚àí2n‚àí1 + n‚àí2
i=0 Ai2i, which is (2.A.6) for An‚àí1 = 1.
In this coding scheme there is only one code for zero:
(0)10 = (000 ¬∑ ¬∑ ¬∑ 00
#
$%
&
n
)2
When n-bit integers are added together, there is the possibility that the sum may
not Ô¨Åt in n bits. This is overÔ¨Çow. The condition is easy to detect by monitoring
the signs of the operands and the sum. Suppose that x and y are n-bit two‚Äôs
complement coded integers, so that the sign bits of these operands are xn‚àí1 and
yn‚àí1. Suppose that the sum is denoted by s, implying that the sign bit is sn‚àí1. The
Boolean function that tests for overÔ¨Çow of s = x + y (algebraic sum of x and y) is
T = xn‚àí1yn‚àí1sn‚àí1 + xn‚àí1yn‚àí1sn‚àí1.
The Ô¨Årst term will be logical 1 if the operands are negative while the sum is
positive. The second term will be logical 1 if the operands are positive but the sum
is negative. Either condition yields T = 1, thus indicating an overÔ¨Çow. A similar
test may be obtained for subtraction, but we omit this here.
The following is both the procedure and the justiÔ¨Åcation of the procedure for
adding two‚Äôs complement coded integers.
Theorem 2.A.1: Two‚Äôs Complement Addition
If A and B are n-bit two‚Äôs
complement coded numbers, then compute A + B (the sum of A and B) as though
they were unsigned numbers, discarding any carryout.
Proof
Suppose that A > 0, B > 0; then A + B will generate no carryout from
the bit position n ‚àí1 since An‚àí1 = Bn‚àí1 = 0 (i.e., the sign bits are zero-valued),
and the result will be correct if A + B < 2n‚àí1. (If this inequality is not satisÔ¨Åed,
then the sign bit will be one, indicating a negative answer, which is wrong. This
amounts to an overÔ¨Çow.)
Suppose that A ‚â•B > 0; then
A + (‚àíB) = A + B‚àó= A + 2n ‚àíB = 2n + A ‚àíB,
and if we discard the carryout, this is equivalent to subtracting 2n (because the
carryout has a weight of 2n). Doing this yields A + (‚àíB) = A ‚àíB.
TLFeBOOK

PROBLEMS
59
Similarly
(‚àíA) + B = A‚àó+ B = 2n ‚àíA + B = 2n + B ‚àíA,
and discarding the carry out yields (‚àíA) + B = B ‚àíA.
Again, suppose that A ‚â•B > 0, then
(‚àíA) + (‚àíB) = A‚àó+ B‚àó= 2n ‚àíA + 2n ‚àíB = 2n + [2n ‚àí(A + B)]
= 2n + (A + B)‚àó
so discarding the carryout gives (‚àíA) + (‚àíB) = (A + B)‚àó, which is the desired
two‚Äôs complement representation of ‚àí(A + B), provided A + B ‚â§2n‚àí1. (If this
latter inequality is not satisÔ¨Åed, then we have an overÔ¨Çow.)
The procedure for subtraction (and its justiÔ¨Åcation) follows similarly. We omit
these details.
REFERENCES
1. J. H. Wilkinson, Rounding Errors in Algebraic Processes, Prentice-Hall, Englewood
Cliffs, NJ, 1963.
2. V. C. Hamacher, Z
Vranesic, and S. G. Zaky, Computer Organization, 3rd ed.,
McGraw-Hill, New York, 1990.
3. J. F. Wakerly, Digital Design Principles and Practices, Prentice-Hall, Englewood Cliffs,
NJ, 1990.
4. G. H. Golub and C. F. Van Loan, Matrix Computations, 2nd ed., Johns Hopkins Univ.
Press, Baltimore, MD, 1989.
5. G. E. Forsythe, M. A. Malcolm, and C. B. Moler, Computer Methods for Mathematical
Computations, Prentice-Hall, Englewood Cliffs, NJ, 1977.
6. N. J. Higham, Accuracy and Stability of Numerical Algorithms. SIAM, Philadelphia, PA,
1996.
PROBLEMS
2.1. Let f x[x] denote the operation of reducing x (a Ô¨Åxed-point binary fraction)
to t + 1 bits (including the sign bit) according to the Wilkinson rounding
(ordinary rounding) procedure in Section 2.2. Suppose that a = (0.1000)2,
b = (0.1001)2, and c = (0.0101)2, so t = 4 here. In arithmetic of unlimited
precision, we always have a(b + c) = ab + ac. Suppose that a practical com-
puting machine applies the operator f x[¬∑] after every arithmetic operation.
(a) Find x = f x[f x[ab] + f x[ac]].
(b) Find y = f x[af x[b + c]].
Do you obtain x = y?
TLFeBOOK

60
NUMBER REPRESENTATIONS
This problem shows that the order of operations in an algorithm implemented
on a practical computer can affect the answer obtained.
2.2. Recall from Section 2.2 that
q =

1
3

10 = (0.01)2.
Find the absolute error in representing q as a (t + 1)-bit binary number. Find
the relative error. Assume both ordinary rounding and chopping (deÔ¨Åned at
the end of Section 2.3 with respect to Ô¨Çoating-point arithmetic).
2.3. Recall that we deÔ¨Åne a Ô¨Çoating-point number in base r to have the form
x = x0.x1x2 ¬∑ ¬∑ ¬∑ xt‚àí1xt
#
$%
&
=f
√ó re,
where x0 ‚àà{+, ‚àí} (sign digit), xk ‚àà{0, 1, . . . , r ‚àí1} for k = 1, 2, . . . , t, e
is the exponent (a signed integer), and x1 Ã∏= 0 (so r‚àí1 ‚â§|f | < 1) if x Ã∏= 0.
Show that for x Ã∏= 0
m ‚â§|x| ‚â§M,
where for L ‚â§e ‚â§U, we have
m = rL‚àí1,
M = rU(1 ‚àír‚àít).
2.4. Suppose r = 10. We may consider the result of a decimal arithmetic operation
in the Ô¨Çoating-point representation to be
x = ¬±
' ‚àû

k=1
xk10‚àík
(
√ó 10e.
(a) If f l[x] is the operator for chopping, then
f l[x] = (¬±.x1x2 ¬∑ ¬∑ ¬∑ xt‚àí1xt) √ó 10e,
thus, all digits xk for k > t are forced to zero.
(b) If f l[x] is the operator for rounding then it is deÔ¨Åned as follows. Add
0.00 ¬∑ ¬∑ ¬∑ 01
#
$%
&
t+1 digits
to the mantissa if xt+1 ‚â•5, but if xt+1 < 5, the mantissa is
unchanged. Then all digits xk for k > t are forced to zero.
Show that the absolute error for chopping satisÔ¨Åes the upper bound
|x ‚àíf l[x]| ‚â§10‚àít10e,
and that the absolute error for rounding satisÔ¨Åes the upper bound
|x ‚àíf l[x]| ‚â§1
210‚àít10e.
TLFeBOOK

PROBLEMS
61
Show that the relative errors satisfy
|œµ| = |x ‚àíf l[x]|
|x|
‚â§
 101‚àít
(chopping)
1
2101‚àít
(rounding)
.
2.5. Suppose that t = 4 and r = 2 (i.e., we are working with Ô¨Çoating-point binary
numbers). Suppose that we have the operands
x = 0.1011 √ó 10‚àí3,
y = ‚àí0.1101 √ó 102.
Find x + y, x ‚àíy, and xy. Clearly show the steps involved.
2.6. Suppose that A ‚ààRn√ón, x ‚ààRn, and that f l[Ax] represents the result
of computing the product Ax on a Ô¨Çoating-point computer. DeÔ¨Åne |A| =
[|ai,j|]i,j=0,1,...,n‚àí1, and |x| = [|x0||x1| ¬∑ ¬∑ ¬∑ |xn‚àí1|]T . We have
f l[Ax] = Ax + e,
where e ‚ààRn is the error vector. Of course, e models the rounding errors
involved in the actual computation of product Ax on the computer. Justify
the bound
|e| ‚â§1.01nu|A||x|.
2.7. Explain why a conditional test such as
if x Ã∏= y then begin
f := f/(x ‚àíy);
end;
is unreliable.
(Hint: Think about dynamic range limitations in Ô¨Çoating-point arithmetic.)
2.8. Suppose
that
x = [x0x1 ¬∑ ¬∑ ¬∑ xn‚àí1]T
is
a
real-valued
vector,
||x||‚àû=
max0‚â§k‚â§n‚àí1 |xk|, and that we wish to compute ||x||2 =
n‚àí1
k=0 x2
k
1/2
.
Explain the advantages, and disadvantages of the following algorithm with
respect to computational efÔ¨Åciency (number of arithmetic operations, and
comparisons), and dynamic range limitations in Ô¨Çoating-point arithmetic:
m := ||x||‚àû;
s := 0;
for k := 0 to n ‚àí1 do begin
s := s + (xk/m)2;
end;
||x||2 := m‚àös;
Comments regarding computational efÔ¨Åciency may be made with respect to
the pseudocode algorithm in Example 2.4.
TLFeBOOK

62
NUMBER REPRESENTATIONS
2.9. Recall that for x2 + bx + c = 0, the roots are
x1 = ‚àíb +
‚àö
b2 ‚àí4c
2
,
x2 = ‚àíb ‚àí
‚àö
b2 ‚àí4c
2
.
If b = ‚àí0.3001, c = 0.00006, then the ‚Äúexact‚Äù roots for this set of parame-
ters are
x1 = 0.29989993,
x2 = 2.0006673 √ó 10‚àí4.
Let us compute the roots using four-digit (i.e., t = 4) decimal (i.e., r = 10)
Ô¨Çoating-point arithmetic, where, as a result of rounding quantization b, and
c are replaced with their approximations
b = ‚àí0.3001 = b,
c = 0.0001 Ã∏= c.
Compute x2, which is the approximation to x2 obtained using b and c in
place of b and c. Show that the relative error is

x2 ‚àíx2
x2
 ‚âà0.75
(i.e., the relative error is about 75%). (Comment: This is an example of
catastrophic cancellation.)
2.10. Suppose a, b ‚ààR, and x = a ‚àíb. Floating-point approximations to a and b
are ÀÜa = f l[a] = a(1 + œµa) and ÀÜb = f l[b] = b(1 + œµb), respectively. Hence
the Ô¨Çoating-point approximation to x is ÀÜx = ÀÜa ‚àíÀÜb. Show that the relative
error is of the form
|œµ| =

x ‚àíÀÜx
x
 ‚â§Œ± |a| + |b|
|a ‚àíb| .
What is Œ±? When is |œµ| large?
2.11. For a Ã∏= 0, the quadratic equation ax2 + bx + c = 0 has roots given by
x1 = ‚àíb +
‚àö
b2 ‚àí4ac
2a
,
x2 = ‚àíb ‚àí
‚àö
b2 ‚àí4ac
2a
.
For c Ã∏= 0, quadratic equation cx2 + bx + a = 0 has roots given by
x‚Ä≤
1 = ‚àíb +
‚àö
b2 ‚àí4ac
2c
, x‚Ä≤
2 = ‚àíb ‚àí
‚àö
b2 ‚àí4ac
2c
.
(a) Show that x1x‚Ä≤
2 = 1 and x2x‚Ä≤
1 = 1.
(b) Using the result from Problem 2.10, explain accuracy problems that can
arise in computing either x1 or x2 when b2 ‚â´|4ac|. Can you use the
result in part (a) to alleviate the problem? Explain.
TLFeBOOK

3
Sequences and Series
3.1
INTRODUCTION
Sequences and series have a major role to play in computational methods. In this
chapter we consider various types of sequences and series, especially with respect
to their convergence behavior. A series might converge ‚Äúmathematically,‚Äù and yet
it might not converge ‚Äúnumerically‚Äù (i.e., when implemented on a computer). Some
of the causes of difÔ¨Åculties such as this will be considered here, along with possible
remedies.
3.2
CAUCHY SEQUENCES AND COMPLETE SPACES
It was noted in the introduction to Chapter 1 that many computational processes
are ‚Äúiterative‚Äù (the Newton‚ÄìRaphson method for Ô¨Ånding the roots of an equation,
iterative methods for linear system solution, etc.). The practical effect of this is to
produce sequences of elements from function spaces. The sequence produced by the
iterative computation is only useful if it converges. We must therefore investigate
what this means.
In Chapter 1 it was possible for sequences to be either singly or doubly inÔ¨Ånite.
Here we shall assume sequences are singly inÔ¨Ånite unless speciÔ¨Åcally stated to the
contrary.
We begin with the following (standard) deÔ¨Ånition taken from Kreyszig
[1, pp. 25‚Äì26]. Examples of applications of the deÔ¨Ånitions to follow will be con-
sidered later.
DeÔ¨Ånition 3.1: Convergence of a Sequence, Limit
A sequence (xn) in a
metric space X = (X, d) is said to converge, or to be convergent iff there is an
x ‚ààX such that
lim
n‚Üí‚àûd(xn, x) = 0.
(3.1)
The element x is called the limit of (xn) (i.e., limit of the sequence), and we may
state that
lim
n‚Üí‚àûxn = x.
(3.2)
An Introduction to Numerical Analysis for Electrical and Computer Engineers, by C.J. Zarowski
ISBN 0-471-46737-5
c‚Éù2004 John Wiley & Sons, Inc.
63
TLFeBOOK

64
SEQUENCES AND SERIES
We say that (xn) converges to x or has a limit x. If (xn) is not convergent, then
we say that it is a divergent sequence, or is simply divergent.
A shorthand expression for (3.2) is to write xn ‚Üíx. We observe that sequence (xn)
is deÔ¨Åned to converge (or not) with respect to a particular metric here denoted d
(recall the axioms for a metric space from Chapter 1). We remark that it is possible
that, for some (xn) in some set X, the sequence might converge with respect to
one metric on the set, but might not converge with respect to another choice of
metric. It must be emphasized that the limit x must be an element of X in order
for the sequence to be convergent.
Suppose, for example, that X = (0, 1] ‚äÇR, and consider the sequence xn =
1
n+1(n ‚ààZ+). Suppose also that d(x, y) = |x ‚àíy|. The sequence (xn) does not
converge in X because the sequence ‚Äúwants to go to 0.‚Äù But 0 is not in X. So the
sequence does not converge. (Of course, the sequence converges in X = R with
respect to our present choice of metric.)
It can be difÔ¨Åcult in practice to ascertain whether a particular sequence con-
verges according to DeÔ¨Ånition 3.1. This is because the limit x may not be known
in advance. In fact, this is almost always the case in computing applications of
sequences. Sometimes it is therefore easier to work with the following:
DeÔ¨Ånition 3.2: Cauchy Sequence, Complete Space
A sequence (xn) in a
metric space X = (X, d) is called a Cauchy sequence iff for all œµ > 0 there is an
N(œµ) ‚ààZ+ such that
d(xm, xn) < œµ
(3.3)
for all m, n > N(œµ). The space X is a complete space iff every Cauchy sequence
in X converges.
We often write N instead of N(œµ), because N may depend on our choice of œµ. It
is possible to prove that any convergent sequence is also Cauchy.
We remark that, if in fact the limit is known (or at least strongly suspected),
then applying DeÔ¨Ånition 3.1 may actually be easier than applying DeÔ¨Ånition 3.2.
We see that under DeÔ¨Ånition 3.2 the elements of a Cauchy sequence get closer to
each other as n and m increase. Establishing the ‚ÄúCauchiness‚Äù of a sequence does
not require knowing the limit of the sequence. This, at least in principle, simpliÔ¨Åes
matters. However, a big problem with this deÔ¨Ånition is that there are metric spaces
X in which not all Cauchy sequences converge. In other words, there are incomplete
metric spaces. For example, the space X = (0, 1] with d(x, y) = |x ‚àíy| is not
complete. Recall that we considered xn = 1/(n + 1). This sequence is Cauchy,1
but the limit is 0, which is not in X. Thus, this sequence is a nonconvergent Cauchy
sequence. Thus, the space (X, | ¬∑ |) is not complete.
1We see that
d(xm, xn) =

1
m + 1 ‚àí
1
n + 1
 ‚â§

n ‚àím
nm
 =

1
m ‚àí1
n
 .
TLFeBOOK

CAUCHY SEQUENCES AND COMPLETE SPACES
65
A more subtle example of an incomplete metric space is the following. Recall
space C[a, b] from Example 1.4. Assume that a = 0 and b = 1, and now choose
the metric to be
d(x, y) =
 1
0
|x(t) ‚àíy(t)| dt
(3.4)
instead of Eq. (1.8). Space C[0, 1] with the metric (3.4) is not complete. This
may be shown by considering the sequence of continuous functions illustrated
in Fig. 3.1. The functions xm(t) in Fig. 3.1a form a Cauchy sequence. (Here we
assume m ‚â•1, and is an integer.) This is because d(xm, xn) is the area of the
triangle in Fig. 3.1b, and for any œµ > 0, we have
d(xm, xn) < œµ
whenever m, n > 1/(2œµ). (Suppose n ‚â•m and consider that d(xm, xn) = 1
2( 1
m ‚àí
1
n) ‚â§
1
2m < œµ.) We may see that this Cauchy sequence does not converge. Observe
that we have
xm(t) = 0
for
t ‚àà[0, 1
2],
xm(t) = 1
for
t ‚àà[am, 1],
1
1
am
xm
0
(a)
(b)
1
m
t
1
2
1
1
am
xn
xm
0
1
m
1
n
t
1
2
Figure 3.1
A Cauchy sequence of functions in C[0, 1].
For any œµ > 0 we may Ô¨Ånd N(œµ) > 0 such that for n ‚â•m > N(œµ)
1
m ‚àí1
n < œµ.
If n < m, the roles of n and m may be reversed. The conditions of DeÔ¨Ånition 3.2 are met and so the
sequence is Cauchy.
TLFeBOOK

66
SEQUENCES AND SERIES
where am = 1
2 + 1
m. Therefore, for all x ‚ààC[0, 1],
d(xm, x) =
 1
0
|xm(t) ‚àíx(t)| dt
=
 1/2
0
|x(t)| dt +
 am
1/2
|xm(t) ‚àíx(t)| dt +
 1
am
|1 ‚àíx(t)| dt.
The integrands are all nonnegative, and so each of the integrals on the right-hand
side are nonnegative, too. Thus, to say that d(xm, x) ‚Üí0 implies that each integral
approaches zero. Since x(t) is continuous, it must be the case that
x(t) = 0
for
t ‚àà[0, 1
2),
x(t) = 1
for
t ‚àà( 1
2, 1].
However, this is not possible for a continuous function. In other words, we have
a contradiction. Hence, (xn) does not converge (i.e., has no limit in X = C[0, 1]).
Again, we have a Cauchy sequence that does not converge, and so C[0, 1] with
the metric (3.4) is not complete.
This example also shows that a sequence of continuous functions may very well
possess a discontinuous limit. Actually, we have seen this phenomenon before.
Recall the example of the Fourier series in Chapter 1 (see Example 1.20). In this
case the series representation of the square wave was made up of terms that are all
continuous functions. Yet the series converges to a discontinuous limit. We shall
return to this issue again later.
So now, some metric spaces are not complete. This means that even though a
sequence is Cauchy, there is no guarantee of convergence. We are therefore faced
with the following questions:
1. What metric spaces are complete?
2. Can they be ‚Äúcompleted‚Äù if they are not?
The answer to the second question is ‚ÄúYes.‚Äù Given an incomplete metric space,
it is always possible to complete it. We have seen that a Cauchy sequence does
not converge when the sequence tends toward a limit that does not belong to the
space; thus, in a sense, the space ‚Äúhas holes in it.‚Äù Completion is the process of
Ô¨Ålling in the holes. This amounts to adding the appropriate elements to the set that
made up the incomplete space. However, in general, this is a technically difÔ¨Åcult
process to implement in many cases, and so we will never do this. This is a job
normally left to mathematicians.
We will therefore content ourselves with answering the Ô¨Årst question. This will
be done simply by listing complete metric spaces that are useful to engineers:
1. Sets R and C with the metric d(x, y) = |x ‚àíy| are complete metric spaces.
TLFeBOOK

CAUCHY SEQUENCES AND COMPLETE SPACES
67
2. Recall Example 1.3. The space l‚àû[0, ‚àû] with the metric
d(x, y) = sup
k‚ààZ+ |xk ‚àíyk|
(3.5)
is a complete metric space. (A proof of this claim appears in Ref. 1, p. 34.)
3. The Euclidean space Rn and the unitary space Cn both with metric
d(x, y) =
n‚àí1

k=0
|xk ‚àíyk|2
	1/2
(3.6)
are complete metric spaces. (Proof is on p. 33 of Ref. 1.)
4. Recall Example 1.6. Fixing p, the space lp[0, ‚àû] such that 1 ‚â§p < ‚àûis a
complete metric space. Here we recall that the metric is
d(x, y) =
 ‚àû

k=0
|xk ‚àíyk|p
	1/p
.
(3.7)
5. Recall Example 1.4. The set C[a, b] with the metric
d(x, y) = sup
t‚àà[a,b]
|x(t) ‚àíy(t)|
(3.8)
is a complete metric space. (Proof is on pp. 36‚Äì37 of Ref. 1.)
The last example is interesting because the special case C[0, 1] with metric (3.4)
was previously shown to be incomplete. Keeping the same set but changing the
metric from that in (3.4) to that in (3.8) changes the situation dramatically.
In Chapter 1 we remarked on the importance of the metric space L2[a, b] (recall
Example 1.7). The space is important as the ‚Äúspace of Ô¨Ånite energy signals on the
interval [a, b].‚Äù (A ‚ÄúÔ¨Ånite power‚Äù interpretation was also possible.) An important
special case of this was L2(R) = L2(‚àí‚àû, ‚àû). Are these metric spaces complete?
Our notation implicitly assumes that the set (1.11a) (Chapter 1) contains the so-
called Lebesgue integrable functions on [a, b]. In this case the space L2[a, b] is
indeed complete with respect to the metric
d(x, y) =
 b
a
|x(t) ‚àíy(t)|2 dt
1/2
.
(3.9)
Lebesgue integrable functions2 have a complicated mathematical structure, and we
have promised to avoid any measure theory in this book. It is enough for the reader
2One of the ‚Äúsimplest‚Äù introductions to these is Rudin [2]. However, these functions appear in the
last chapter [2, Chapter 11]. Knowledge of much of the previous chapters is prerequisite to studying
Chapter 11. Thus, the effort required to learn measure theory is substantial.
TLFeBOOK

68
SEQUENCES AND SERIES
to assume that the functions in L2[a, b] are the familiar ones from elementary
calculus.3
The complete metric spaces considered in the two previous paragraphs also
happen to be normed spaces; recall Section 1.3.2. This is because the metrics are
all induced by suitable norms on the spaces. It therefore follows that these spaces
are complete normed spaces. Complete normed spaces are called Banach spaces.
Some of the complete normed spaces are also inner product spaces. Again, this
follows because in those cases an inner product is deÔ¨Åned that induced the norm.
Complete inner product spaces are called Hilbert spaces. To be more speciÔ¨Åc, the
following spaces are Hilbert spaces:
1. The Euclidean space Rn and the unitary space Cn along with the inner product
‚ü®x, y‚ü©=
n‚àí1

k=0
xky‚àó
k
(3.10)
are both Hilbert spaces.
2. The space L2[a, b] with the inner product
‚ü®x, y‚ü©=
 b
a
x(t)y‚àó(t) dt
(3.11)
is a Hilbert space. [This includes the special case L2(R).]
3. The space l2[0, ‚àû] with the inner product
‚ü®x, y‚ü©=
‚àû

k=0
xky‚àó
k
(3.12)
is a Hilbert space.
We emphasize that (3.10) induces the metric (3.6), (3.11) induces the metric (3.9),
and (3.12) induces the metric (3.7) (but only for case p = 2; recall from Chapter 1
that lp[0, ‚àû] is not an inner product space when p Ã∏= 2). The three Hilbert spaces
listed above are particularly important because of the fact, in part, that elements in
these spaces have (as we have already noted) either Ô¨Ånite energy or Ô¨Ånite power
interpretations. Additionally, least-squares problems are best posed and solved
within these spaces. This will be considered later.
DeÔ¨Åne the set (of natural numbers) N = {1, 2, 3, . . .}. We have seen that sequences
of continuous functions may have a discontinuous limit. An extreme example of this
phenomenon is from p. 145 of Rudin [2].
3These ‚Äúfamiliar‚Äù functions are called Riemann integrable functions. These functions form a proper
subset of the Lebesgue integrable functions.
TLFeBOOK

CAUCHY SEQUENCES AND COMPLETE SPACES
69
Example 3.1
For n ‚ààN deÔ¨Åne
xn(t) = lim
m‚Üí‚àû[cos(n!œÄt)]2m.
When n!t is an integer, then xn(t) = 1 (simply because cos(œÄk) = ¬±1 for k ‚ààZ).
For all other values of t, we must have xn(t) = 0 (simply because | cos t| < 1 when
t is not an integral multiple of œÄ). DeÔ¨Åne
x(t) = lim
n‚Üí‚àûxn(t).
If t is irrational, then xn(t) = 0 for all n. Suppose that t is rational; that is, suppose
t = p/q for which p, q ‚ààZ. In this case n!t is an integer when n ‚â•q in which
case x(t) = 1. Consequently, we may conclude that
x(t) = lim
n‚Üí‚àûlim
m‚Üí‚àû[cos(n!œÄt)]2m =
 0,
t is irrational
1,
t is rational
.
(3.13)
We have mentioned (in footnote 3, above) that Riemann integrable functions are a
proper subset of the Lebesgue integrable functions. It turns out that x(t) in (3.13)
is Lebesgue integrable, but not Riemann integrable. In other words, you cannot use
elementary calculus to Ô¨Ånd the integral of x(t) in (3.13). Of course, x(t) is a very
strange function. This is typical; that is, functions that are not Riemann integrable
are usually rather strange, and so are not commonly encountered (by the engineer).
It therefore follows that we do not need to worry much about the more general
class of Lebesgue integrable functions.
Limiting processes are potentially dangerous. This is illustrated by a very simple
example.
Example 3.2
Suppose n, m ‚ààN. DeÔ¨Åne
xm,n =
m
m + n.
(This is a double sequence. In Chapter 1 we saw that these arise routinely in wavelet
theory.) Treating n as a Ô¨Åxed constant, we obtain
lim
m‚Üí‚àûxm,n = 1
so
lim
n‚Üí‚àûlim
m‚Üí‚àûxm,n = 1.
Now instead treat m as a Ô¨Åxed constant so that
lim
n‚Üí‚àûxm,n = 0
TLFeBOOK

70
SEQUENCES AND SERIES
which in turn implies that
lim
m‚Üí‚àûlim
n‚Üí‚àûxm,n = 0.
Interchanging the order of the limits has given two completely different answers.
Interchanging the order of limits clearly must be done with great care.
The following example is simply another illustration of how to apply DeÔ¨Åni-
tion 3.2.
Example 3.3
DeÔ¨Åne
xn = 1 + (‚àí1)n
n + 1
(n ‚ààZ+).
This is a sequence in the metric space (R, | ¬∑ |). This space is complete, so we need
not know the limit of the sequence to determine whether it converges (although
we might guess that the limit is x = 1). We see that
d(xm, xn) =

(‚àí1)m
m + 1 ‚àí(‚àí1)n
n + 1
 ‚â§
1
m + 1 +
1
n + 1 ‚â§1
m + 1
n,
where the triangle inequality has been used. If we assume [without loss of generality
(commonly abbreviated w.l.o.g.)] that n ‚â•m > N(œµ) then
1
m + 1
n ‚â§2
m < œµ.
So, for a given œµ > 0, we select n ‚â•m > 2/œµ. The sequence is Cauchy, and so it
must converge.
We close this section with mention of Appendix 3.A. Think of the material in it
as being a very big applications example. This appendix presents an introduction
to coordinate rotation digital computing (CORDIC). This is an application of a
particular class of Cauchy sequence (called a discrete basis) to the problem of
performing certain elementary operations (e.g., vector rotation, computing sines
and cosines). The method is used in application-speciÔ¨Åc integrated circuits (ASICs),
gate arrays, and has been used in pocket calculators. Note that Appendix 3.A also
illustrates a useful series expansion which is expressed in terms of the discrete basis.
3.3
POINTWISE CONVERGENCE AND UNIFORM CONVERGENCE
The previous section informed us that sequences can converge in different ways,
assuming that they converge in any sense at all. We explore this issue further here.
TLFeBOOK

POINTWISE CONVERGENCE AND UNIFORM CONVERGENCE
71
DeÔ¨Ånition 3.3: Pointwise Convergence
Suppose that (xn(t)) (n ‚ààZ+) is a
sequence of functions for which t ‚ààS ‚äÇR. We say that the sequence converges
pointwise iff there is an x(t) (t ‚ààS) so that for all œµ > 0 there is an N = N(œµ, t)
such that
|xn(t) ‚àíx(t)| ‚â§œµ
(3.14)
for n ‚â•N. We call x the limit of (xn) and write
x(t) = lim
n‚Üí‚àûxn(t)
(t ‚ààS).
(3.15)
We emphasize that under this deÔ¨Ånition N may depend on both œµ and t. We may
contrast DeÔ¨Ånition 3.3 with the following deÔ¨Ånition.
DeÔ¨Ånition 3.4: Uniform Convergence
Suppose that (xn(t)) (n ‚ààZ+) is a
sequence of functions for which t ‚ààS ‚äÇR. We say that the sequence converges
uniformly iff there is an x(t) (t ‚ààS) so that for all œµ > 0 there is an N = N(œµ)
such that
|xn(t) ‚àíx(t)| ‚â§œµ
(3.16)
for n ‚â•N. We call x the limit of (xn) and write
x(t) = lim
n‚Üí‚àûxn(t)
(t ‚ààS).
(3.17)
We emphasize that under this deÔ¨Ånition N never depends on t, although it may
depend on œµ. It is apparent that a uniformly convergent sequence is also point-
wise convergent. However, the converse is not true; that is, a pointwise convergent
sequence is not necessarily uniformly convergent. This distinction is important in
understanding the convergence behavior of series as well as of sequences. In par-
ticular, it helps in understanding convergence phenomena in Fourier (and wavelet)
series expansions.
In contrast with the deÔ¨Ånitions of Section 3.2, under DeÔ¨Ånitions 3.3 and 3.4 the
elements of (xn) and the limit x need not reside in the same function space. In
fact, we do not ask what function spaces they belong to at all. In other words, the
deÔ¨Ånitions of this section represent a different approach to convergence analysis.
As with the DeÔ¨Ånition 3.1, direct application of DeÔ¨Ånitions 3.3 and 3.4 can be
quite difÔ¨Åcult since the limit, assuming it exists, is not often known in advance
(i.e., a priori) in practice. Therefore, we would hope for a convergence criterion
similar to the idea of Cauchy convergence in Section 3.2 (DeÔ¨Ånition 3.2). In fact,
we have the following theorem (from Rudin [2, pp. 147‚Äì148]).
Theorem 3.1: The sequence of functions (xn) deÔ¨Åned on S ‚äÇR converges
uniformly on S iff for all œµ > 0 there is an N such that
|xm(t) ‚àíxn(t)| ‚â§œµ
(3.18)
for all n, m ‚â•N.
TLFeBOOK

72
SEQUENCES AND SERIES
This is certainly analogous to the Cauchy criterion seen earlier. (We omit the
proof.)
Example 3.4
Suppose that (xn) is deÔ¨Åned according to
xn(t) =
1
nt + 1,
t ‚àà(0, 1) and n ‚ààN.
A sketch of xn(t) for various n appears in Fig. 3.2. We see that (‚Äúby inspection‚Äù)
xn ‚Üí0. But consider for all œµ > 0
|xn(t) ‚àí0| =
1
nt + 1 ‚â§œµ
which implies that we must have
n ‚â•1
t
 1
œµ ‚àí1
!
= N
so that N is a function of both t and œµ. Convergence is therefore pointwise, and is
not uniform.
Other criteria for uniform convergence may be established. For example, there is
the following theorem (again from Rudin [2, p. 148]).
Theorem 3.2: Suppose that
lim
n‚Üí‚àûxn(t) = x(t)
(t ‚ààS).
DeÔ¨Åne
Mn = sup
t‚ààS
|xn(t) ‚àíx(t)|.
Then xn ‚Üíx uniformly on S iff Mn ‚Üí0 as n ‚Üí‚àû.
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.2
0.4
0.6
0.8
1
t
xn (t)
n = 1
n = 2
n = 20
Figure 3.2
A plot of typical sequence elements for Example 3.4; here, t ‚àà[0.01, 0.99].
TLFeBOOK

FOURIER SERIES
73
‚àí10
‚àí8
‚àí6
‚àí4
‚àí2
0
2
4
6
8
10
‚àí0.5
0
0.5
t
n = 1
n = 2
n = 10
xn (t)
Figure 3.3
A plot of typical sequence elements for Example 3.5.
The proof is really an immediate consequence of DeÔ¨Ånition 3.4, and so is omitted
here.
Example 3.5
Suppose that
xn(t) =
t
1 + nt2 ,
t ‚ààR and n ‚ààN.
A sketch of xn(t) for various n appears in Fig. 3.3. We note that
dxn(t)
dt
= (1 + nt2) ¬∑ 1 ‚àít ¬∑ (2nt)
[1 + nt2]2
=
1 ‚àínt2
[1 + nt2]2 = 0
for t = ¬± 1
‚àön. We see that
xn
 
¬± 1
‚àön
!
= ¬±
1
2‚àön.
We also see that xn ‚Üí0. So then
Mn = sup
t‚ààR
|xn(t)| =
1
2‚àön.
Clearly, Mn ‚Üí0 as n ‚Üí‚àû. Therefore, via Theorem 3.2, we immediately conclude
that xn ‚Üíx uniformly on the real number line.
3.4
FOURIER SERIES
The Fourier series expansion was introduced brieÔ¨Çy in Chapter 1, where the behav-
ior of this series with respect to its convergence properties was not mentioned. In
this section we shall demonstrate the pointwise convergence of the Fourier series
TLFeBOOK

74
SEQUENCES AND SERIES
by the analysis of a particular example. Much of what follows is from Walter [17].
However, of necessity, the present treatment is not so rigorous.
Suppose that
g(t) = 1
œÄ (œÄ ‚àít),
0 < t < 2œÄ.
(3.19)
The reader is strongly invited to show that this has Fourier series expansion
g(t) = 2
œÄ
‚àû

k=1
1
k sin(kt).
(3.20)
The procedure for doing this closely follows Example 1.20. In our analysis to
follow, it will be easier to work with
f (t) = œÄ
2 g(t) =
‚àû

k=1
1
k sin(kt).
(3.21)
DeÔ¨Åne the sequence of partial sums
Sn(t) =
n

k=1
1
k sin(kt)
(n ‚ààN).
(3.22)
So we infer that
lim
n‚Üí‚àûSn(t) = f (t),
(3.23)
but we do not know in what sense the partial sums tend to f (t). Is convergence
pointwise, or uniform?
We shall need the special function
Dn(t) = 1
œÄ

1
2 +
n

k=1
cos(kt)
	
= 1
2œÄ
sin(n + 1
2)t
sin( 1
2t)
.
(3.24)
This function is called the Dirichlet kernel. The second equality in (3.24) is not
obvious. We will prove it. Consider that
sin
 1
2t
!
(œÄDn(t)) = 1
2 sin
 1
2t
!
+
n

k=1
sin
 1
2t
!
cos(kt)
= 1
2 sin
 1
2t
!
+ 1
2
n

k=1
sin
 
k + 1
2
!
t + 1
2
n

k=1
sin
 1
2 ‚àík
!
t
= 1
2 sin
 1
2t
!
+ 1
2
n

k=1
sin
 
k + 1
2
!
t ‚àí1
2
n

k=1
sin
 
k ‚àí1
2
!
t,
(3.25)
TLFeBOOK

FOURIER SERIES
75
where we have used the identity sin a cos b = 1
2 sin(a + b) + 1
2 sin(a ‚àíb). By
expanding the sums and looking for cancellations
n

k=1
sin
 
k + 1
2
!
t ‚àí
n

k=1
sin
 
k ‚àí1
2
!
t = sin
 
n + 1
2
!
t ‚àísin
 1
2t
!
.
(3.26)
Applying (3.26) in (3.25), we obtain
sin
 1
2t
!
(œÄDn(t)) = 1
2 sin
 
n + 1
2
!
t
so immediately
Dn(t) = 1
2œÄ
sin(n + 1
2)t
sin( 1
2t)
,
and this establishes (3.24). Using the identity sin(a + b) = sin a cos b + cos a sin b,
we may also write
Dn(t) = 1
2œÄ

sin(nt) cos( 1
2t)
sin( 1
2t)
+ cos(nt)
	
.
(3.27)
For t > 0, using the form of the Dirichlet kernel in (3.27), we have
œÄ
 t
0
Dn(x) dx =
 t
0

sin(nx) cos( 1
2x)
2 sin( 1
2x)
+ 1
2 cos(nx)
	
dx
=
 t
0
sin(nx)
x
dx +
 t
0
sin(nx)

1
2
cos( 1
2x)
sin( 1
2x)
‚àí1
x
	
dx
+ 1
2
 t
0
cos(nx) dx.
(3.28)
We are interested in what happens when t is a small positive value, but n is large.
To begin with, it is not difÔ¨Åcult to see that
lim
n‚Üí‚àû
1
2
 t
0
cos(nx) dx = lim
n‚Üí‚àû
1
2n sin(nt) = 0.
(3.29)
Less clearly
lim
n‚Üí‚àû
 t
0
sin(nx)

1
2
cos( 1
2x)
sin( 1
2x)
‚àí1
x
	
dx = 0
(3.30)
TLFeBOOK

76
SEQUENCES AND SERIES
(take this for granted). Through a simple change of variable
I (nt) =
 t
0
sin(nx)
x
dx =
 nt
0
sin x
x
dx.
(3.31)
In fact
 ‚àû
0
sin x
x
dx = œÄ
2 .
(3.32)
This is not obvious, either. The result may be found in integral tables [18, p. 483].
In other words, even for very small t, I (nt) does not go to zero as n increases.
Consequently, using (3.29), (3.30), and (3.31) in (3.28), we have (for big n)
œÄ
 t
0
Dn(x) dx ‚âàI (nt).
(3.33)
The results in the previous paragraph help in the following manner. Begin by
noting that
Sn(t) =
n

k=1
1
k sin(kt) =
n

k=1
 t
0
cos(kx) dx =
 t
0
 n

k=1
cos(kx)
	
dx
=
 t
0

1
2 +
n

k=1
cos(kx)
	
dx ‚àí1
2t = œÄ
 t
0
Dn(x) dx ‚àí1
2t
(via (3.24)).
(3.34)
So from (3.33)
Sn(t) ‚âàI (nt) ‚àí1
2t.
(3.35)
DeÔ¨Åne the sequence tn = 1
nœÄ. Consequently
Sn(tn) ‚âàI (œÄ) ‚àí1
2nœÄ.
(3.36)
As n ‚Üí‚àû, tn ‚Üí0, and Sn(tn) ‚ÜíI (œÄ). We can say that for big n
Sn(0+) ‚âàI (œÄ).
(3.37)
Now, f (0+) = œÄ
2 , so for big n
Sn(0+)
f (0+) ‚âà2
œÄ
 œÄ
0
sin x
x
dx ‚âà1.18.
(3.38)
Numerical integration is needed to establish this. This topic is the subject of a later
chapter, however.
We see that the sequence of partial sums Sn(0+) converges to a value bigger
than f (0+) as n ‚Üí‚àû. The approximation Sn(t) therefore tends to ‚Äúovershoot‚Äù
TLFeBOOK

FOURIER SERIES
77
the true value of f (t) for small t. This is called the Gibbs phenomenon, or Gibbs
overshoot. We observe that t = 0 is the place where f (t) has a discontinuity. This
tendency of the Fourier series to overshoot near discontinuities is entirely typical.
We note that f (œÄ) = Sn(œÄ) = 0 for all n ‚â•1. Thus, for any œµ > 0
|f (œÄ) ‚àíSn(œÄ)| ‚â§œµ
for all n ‚â•1. The previous analysis for t = 0+, and this one for t = œÄ show that
N (in the deÔ¨Ånitions of convergence) depends on t. Convergence of the Fourier
series is therefore pointwise and not uniform. Generally, the Gibbs phenomenon is
a symptom of pointwise convergence.
We remark that the Gibbs phenomenon has an impact in the signal processing
applications of series expansions. Techniques for signal compression and signal
enhancement are often based on series expansions. The Gibbs phenomenon can
degrade the quality of decompressed or reconstructed signals. The phenomenon
is responsible for ‚Äúringing artifacts.‚Äù This is one reason why the convergence
properties of series expansions are important to engineers.
Figure 3.4 shows a plot of f (t), Sn(t) and the error
En(t) = Sn(t) ‚àíf (t).
(3.39)
The reader may conÔ¨Årm (3.38) directly from the plot in Fig. 3.4a.
0
1
2
3
4
5
6
7
‚àí2
‚àí1
0
1
2
t
Amplitude
0
1
2
3
4
5
6
7
‚àí2
‚àí1.5
‚àí1
‚àí0.5
0
0.5
1
1.5
t
En (t) (error signal)
(a)
(b)
f(t)
Sn (t) for n = 30 
Figure 3.4
Plots of the Fourier series expansion for f (t) in (3.21), Sn(t) [of (3.22)] for
n = 30, and the error En(t) = Sn(t) ‚àíf (t).
TLFeBOOK

78
SEQUENCES AND SERIES
0
1
2
3
4
5
6
7
‚àí2
‚àí1.5
‚àí1
‚àí0.5
0
0.5
1
1.5
t
En (t) (error signals)
n = 10
n = 20
n = 30
Figure 3.5
En(t) of (3.39) for different values of n.
We conclude this section by remarking that
lim
n‚Üí‚àû
 2œÄ
0
|En(t)|2 dt = 0,
(3.40)
that is, the energy of the error goes to zero in the limit as n goes to inÔ¨Ånity.
However, the amplitude of the error in the vicinity of a discontinuity remains
unchanged in the limit. This is more clearly seen in Fig. 3.5, where the error is
displayed for different values of n. Of course, this fact agrees with our analysis.
Equation (3.40) is really a consequence of the fact that (recalling Chapter 1) Sn(t)
and f (t) are both in the space L2(0, 2œÄ). Rigorous proof of this is quite tough,
and so we omit the proof entirely.
3.5
TAYLOR SERIES
Assume that f (x) is real-valued and that x ‚ààR. One way to deÔ¨Åne the derivative
of f (x) at x = x0 is according to
f (1)(x) = df (x)
dx
|x=x0 = lim
x‚Üíx0
f (x) ‚àíf (x0)
x ‚àíx0
.
(3.41)
The notation is
f (n)(x) = dnf (x)
dxn
(3.42)
(so f (0)(x) = f (x)). From (3.41), we obtain
f (x) ‚âàf (x0) + f (1)(x0)(x ‚àíx0).
(3.43)
But how good is this approximation? Can we obtain a more accurate approximation
to f (x) if we know f (n)(x0) for n > 1? Again, what is the accuracy of the resulting
approximation? We consider these issues in this section.
TLFeBOOK

TAYLOR SERIES
79
Begin by recalling the following theorem.
Theorem 3.3: Mean-Value Theorem
If f (x) is continuous for x ‚àà[a, b]
with a continuous derivative for x ‚àà(a, b), then there is a number Œæ ‚àà(a, b) such
that
f (b) ‚àíf (a)
b ‚àía
= f (1)(Œæ).
(3.44)
Therefore, if a = x0 and b = x, we must have
f (x) = f (x0) + f (1)(Œæ)(x ‚àíx0).
(3.45)
This expression is ‚Äúexact,‚Äù and so is in contrast with (3.43). Proof of Theo-
rem 3.3 may be found in, for example, Bers [19, p. 636]. Theorem 3.3 generalizes
to the following theorem.
Theorem 3.4: Generalized Mean-Value Theorem
Suppose that f (x) and
g(x) are continuous functions on x ‚àà[a, b]. Assume f (1)(x), and g(1)(x) exist and
are continuous, and g(1)(x) Ã∏= 0 for x ‚àà(a, b). There is a number Œæ ‚àà(a, b) such
that
f (b) ‚àíf (a)
g(b) ‚àíg(a) = f (1)(Œæ)
g(1)(Œæ) .
(3.46)
Once again the proof is omitted, but may be found in Bers [19, p. 637].
The tangent to f (x) at x = x0 is given by t(x) = f (x0) + f (1)(x0)(x ‚àíx0)
(t(x0) = f (x0) and t(1)(x0) = f (1)(x0)). We wish to consider t(x) to be an approx-
imation to f (x), so the error is
f (x) ‚àít(x) = e(x)
or
f (x) = f (x0) + f (1)(x0)(x ‚àíx0) + e(x).
(3.47)
Thus
e(x)
x ‚àíx0
= f (x) ‚àíf (x0)
x ‚àíx0
‚àíf (1)(x0).
But
lim
x‚Üíx0
f (x) ‚àíf (x0)
x ‚àíx0
= f (1)(x0)
so immediately
lim
x‚Üíx0
e(x)
x ‚àíx0
= 0.
(3.48)
From (3.47) e(x0) = 0, and also from (3.47), we obtain
f (1)(x) = f (1)(x0) + e(1)(x),
(3.49)
TLFeBOOK

80
SEQUENCES AND SERIES
so e(1)(x0) = 0. From (3.49), f (2)(x) = e(2)(x) (so we now assume that f (x) has
a second derivative, and we will also assume that it is continuous). Theorem 3.4
has the following corollary.
Corollary 3.1
Suppose that f (a) = g(a) = 0, so for all b Ã∏= a there is a Œæ ‚àà
(a, b) such that
f (b)
g(b) = f (1)(Œæ)
g(1)(Œæ) .
(3.50)
Now apply this corollary to f (x) = e(x), and g(x) = (x ‚àíx0)2, with a = x0,
b = x. Thus, from (3.50)
e(x)
(x ‚àíx0)2 =
e(1)(œÑ)
2(œÑ ‚àíx0)
(3.51)
(œÑ ‚àà(x0, x)). Apply the corollary once more to f (œÑ) = e(1)(œÑ), and g(œÑ) =
2(œÑ ‚àíx0):
e(1)(œÑ)
2(œÑ ‚àíx0) = 1
2e(2)(Œæ) = 1
2f (2)(Œæ)
(3.52)
(Œæ ‚àà(x0, œÑ)). Apply (3.51) in (3.52)
e(x)
(x ‚àíx0)2 = 1
2f (2)(Œæ)
or (for some Œæ ‚àà(x0, x))
e(x) = 1
2f (2)(Œæ)(x ‚àíx0)2.
(3.53)
If |f (2)(t)| ‚â§M2 for t ‚àà(x0, x), then
|e(x)| ‚â§
1
2 ¬∑ 1M2|x ‚àíx0|2
(3.54)
and
f (x) = f (x0) + f (1)(x0)(x ‚àíx0) + e(x),
(3.55)
for which (3.54) is an upper bound on the size of the error involved in approxi-
mating f (x) using (3.43).
Example 3.6
Suppose that f (x) = ‚àöx; then
f (1)(x) =
1
2‚àöx ,
f (2)(x) = ‚àí1
4
1
x‚àöx .
Suppose that x0 = 1, and x = x0 + Œ¥x = 1 + Œ¥x. Thus, via (3.55)
‚àöx =
‚àö
1 + Œ¥x = 1 + f (1)(1)Œ¥x + e(x) = 1 + 1
2Œ¥x + e(x),
TLFeBOOK

TAYLOR SERIES
81
so if, for example, |Œ¥x| < 3
4, then |f (2)(x)| ‚â§2 (for x ‚àà( 1
4, 7
4)) so M2 = 2,
and so
|e(x)| ‚â§(Œ¥x)2
via (3.54). This bound may be compared to the following table of values:
Œ¥x
‚àö
1 + Œ¥x
1 + 1
2Œ¥x
e(x)
(Œ¥x)2
‚àí3
4
0.5000
0.6250
‚àí0.1250
0.5625
‚àí1
2
0.7071
0.7500
‚àí0.0429
0.2500
0
1.0000
1.0000
0.0000
0.0000
1
2
1.2247
1.2500
‚àí0.0253
0.2500
3
4
1.3229
1.3750
‚àí0.0521
0.5625
It is easy to see that indeed |e(x)| ‚â§(Œ¥x)2.
We mention that Corollary 3.1 leads to l‚ÄôHÀÜopital‚Äôs rule. It therefore allows us
to determine
lim
x‚Üía
f (x)
g(x)
when f (a) = g(a) = 0. We now digress brieÔ¨Çy to consider this subject. The rule
applies if f (x) and g(x) are continuous at x = a, if f (x) and g(x) have continu-
ous derivatives at x = a, and if g(1)(x) Ã∏= 0 near x = a, except perhaps at x = a.
l‚ÄôHÀÜopital‚Äôs rule is as follows. If limx‚Üía f (x) = limx‚Üía g(x) = 0 and limx‚Üía
f (1)(x)
g(1)(x)
exists, then
lim
x‚Üía
f (x)
g(x) = lim
x‚Üía
f (1)(x)
g(1)(x) .
(3.56)
The rationale is that from Corollary 3.1 for all x Ã∏= a there is a Œæ ‚àà(a, b) such that
f (x)
g(x) = f (1)(Œæ)
g(1)(Œæ) . So, if x is close to a then Œæ must also be close to a, and f (1)(Œæ)
g(1)(Œæ)
is close to its limit. l‚ÄôHÀÜopital‚Äôs rule is also referred to as ‚Äúthe rule for evaluating
the indeterminate form 0
0.‚Äù If it happens that f (1)(a) = g(1)(a) = 0, then one may
attempt l‚ÄôHÀÜopital‚Äôs rule yet again; that is, if limx‚Üía
f (2)(x)
g(2)(x) exists, then
lim
x‚Üía
f (x)
g(x) = lim
x‚Üía
f (2)(x)
g(2)(x) .
Example 3.7
Consider
lim
x‚Üí0
sin x ‚àíex + 1
x2
= lim
x‚Üí0
d
dx [sin x ‚àíex + 1]
d
dx [x2]
= lim
x‚Üí0
cos x ‚àíex
2x
= lim
x‚Üí0
‚àísin x ‚àíex
2
= ‚àí1
2
TLFeBOOK

82
SEQUENCES AND SERIES
for which the rule has been applied twice. Now consider instead
lim
x‚Üí0
1 ‚àí2x
2 + 4x = lim
x‚Üí0
d
dx [1 ‚àí2x]
d
dx [2 + 4x]
= lim
x‚Üí0
‚àí2
4 = ‚àí1
2.
This is wrong ! l‚ÄôHÀÜopital‚Äôs rule does not apply here because f (0) = 1, and g(0) = 2
(i.e., we do not have f (0) = g(0) = 0 as needed by the theory).
The rule can be extended to cover other indeterminate forms (e.g., ‚àû
‚àû). For example,
consider
lim
x‚Üí0 x loge x = lim
x‚Üí0
loge x
1
x
= lim
x‚Üí0
1
x
‚àí1
x2
= lim
x‚Üí0(‚àíx) = 0.
An interesting case is that of Ô¨Ånding
lim
x‚Üí‚àû
 
1 + 1
x
!x
.
This is an indeterminate of the form 1‚àû. Consider
lim
x‚Üí‚àûloge
 
1 + 1
x
!x
= lim
x‚Üí‚àû
loge
 
1 + 1
x
!
1
x
= lim
x‚Üí‚àû
1
1 + 1
x
 
‚àí1
x2
!
‚àí1
x2
= lim
x‚Üí‚àû
1
1 + 1
x
= 1.
The logarithm and exponential functions are continuous functions, so it happens to
be the case that
1 = lim
x‚Üí‚àûloge
 
1 + 1
x
!x
= loge

lim
x‚Üí‚àû
 
1 + 1
x
!x
,
TLFeBOOK

TAYLOR SERIES
83
that is, the limit and the logarithm can be interchanged. Thus
e1 = e
loge

lim
x‚Üí‚àû

1+ 1
x
x
so Ô¨Ånally we have
lim
x‚Üí‚àû
 
1 + 1
x
!x
= e.
(3.57)
More generally, it can be shown that
lim
n‚Üí‚àû

1 + x
n
n
= ex.
(3.58)
This result has various applications, including some in probability theory relating to
Poisson and exponential random variables [20]. An alternative derivation of (3.57)
appears on pp. 64‚Äì65 of Rudin [2], but involves the use of the Maclaurin series
expansion for e. We revisit the Maclaurin series for ex later.
We have demonstrated that for suitable Œæ ‚àà(x0, x)
f (x) = f (x0) + f (1)(x0)(x ‚àíx0) + 1
2f (2)(Œæ)(x ‚àíx0)2
(recall (3.55)). DeÔ¨Åne
p(x) = f (x0) + f (1)(x0)(x ‚àíx0) + 1
2f (2)(x0)(x ‚àíx0)2
(3.59)
so this is some approximation to f (x) near x = x0. Equation (3.43) is a linear
approximation to f (x), and (3.59) is a quadratic approximation to f (x). Once
again, we wish to consider the error
f (x) ‚àíp(x) = e(x).
(3.60)
We note that
p(x0) = f (x0),
p(1)(x0) = f (1)(x0),
p(2)(x0) = f (2)(x0).
(3.61)
In other words, the approximation to f (x) in (3.59) matches the function and its
Ô¨Årst two derivatives at x = x0. Because of (3.61), via (3.60)
e(x0) = e(1)(x0) = e(2)(x0) = 0,
(3.62)
and so via (3.59) and (3.60)
e(3)(x) = f (3)(x)
(3.63)
TLFeBOOK

84
SEQUENCES AND SERIES
(because p(3)(x) = 0 since p(x) is a quadratic in x). As in the derivation of (3.53),
we may repeatedly apply Corollary 3.1:
e(x)
(x ‚àíx0)3 =
e(1)(t1)
3(t1 ‚àíx0)2
for t1 ‚àà(x0, x)
e(1)(t1)
3(t1 ‚àíx0)2 =
e(2)(t2)
3 ¬∑ 2(t2 ‚àíx0)
for t2 ‚àà(x0, t1)
e(2)(t2)
3 ¬∑ 2(t2 ‚àíx0) = e(3)(Œæ)
3 ¬∑ 2
for Œæ ‚àà(x0, t2),
which together yield
e(x)
(x ‚àíx0)3 = f (3)(Œæ)
3 ¬∑ 2
for Œæ ‚àà(x0, x)
or
e(x) =
1
3 ¬∑ 2 ¬∑ 1f (3)(Œæ)(x ‚àíx0)3
(3.64)
for some Œæ ‚àà(x0, x). Thus
f (x) = f (x0) + f (1)(x0)(x ‚àíx0) +
1
2 ¬∑ 1f (2)(x0)(x ‚àíx0)2 + e(x).
(3.65)
Analogously to (3.54), if |f (3)(t)| ‚â§M3 for t ‚àà(x0, x), then we have the error
bound
|e(x)| ‚â§
1
3 ¬∑ 2 ¬∑ 1M3|x ‚àíx0|3.
(3.66)
We have gone from a linear approximation to f (x) to a quadratic approximation
to f (x). All of this suggests that we may generalize to a degree n polynomial
approximation to f (x). Therefore, we deÔ¨Åne
pn(x) =
n

k=0
pn,k(x ‚àíx0)k,
(3.67)
where
pn,k = 1
k!f (k)(x0).
(3.68)
Then
f (x) = pn(x) + en+1(x),
(3.69)
where the error term is
en+1(x) =
1
(n + 1)!f (n+1)(Œæ)(x ‚àíx0)n+1
(3.70)
TLFeBOOK

TAYLOR SERIES
85
for suitable Œæ ‚àà(x0, x). We call pn(x) the Taylor polynomial of degree n. This
polynomial is the approximation to f (x), and the error en+1(x) in (3.70) can be
formally obtained by the repeated application of Corollary 3.1. These details are
omitted. Expanding (3.69), we obtain
f (x) = f (x0) + f (1)(x0)(x ‚àíx0) + 1
2!f (2)(x0)(x ‚àíx0)2
+ ¬∑ ¬∑ ¬∑ + 1
n!f (n)(x0)(x ‚àíx0)n +
1
(n + 1)!f (n+1)(Œæ)(x ‚àíx0)n+1
(3.71)
which is the familiar Taylor formula for f (x). We remark that
f (k)(x0) = p(k)
n (x0)
(3.72)
for k = 0, 1, . . . , n ‚àí1, n. So we emphasize that the approximation pn(x) to f (x)
is based on forcing pn(x) to match the Ô¨Årst n derivatives of f (x), as well as
enforcing pn(x0) = f (x0). If |f (n+1)(t)| ‚â§Mn+1 for all t ‚ààI [interval I contains
(x0, x)], then
|en+1(x)| ‚â§
1
(n + 1)!Mn+1|x ‚àíx0|n+1.
(3.73)
If all derivatives of f (x) exist and are continuous, then we have the Taylor series
expansion of f (x), namely, the inÔ¨Ånite series
f (x) =
‚àû

k=0
1
k!f (k)(x0)(x ‚àíx0)k.
(3.74)
The Maclaurin series expansion is a special case of (3.74) for x0 = 0:
f (x) =
‚àû

k=0
1
k!f (k)(0)xk.
(3.75)
If we retain only terms k = 0 to k = n in the inÔ¨Ånite series (3.74) and (3.75), we
know that en+1(x) gives the error in the resulting approximation. This error may
be called the truncation error (since it arises from truncation of the inÔ¨Ånite series
to a Ô¨Ånite number of terms). Now we consider some examples.
First recall the binomial theorem
(a + x)n =
n

k=0
 n
k
!
xkan‚àík,
(3.76)
where
 n
k
!
=
n!
k!(n ‚àík)!.
(3.77)
TLFeBOOK

86
SEQUENCES AND SERIES
In (3.76) we emphasize that n ‚ààZ+. But we can use Taylor‚Äôs formula to obtain an
expression for (a + x)Œ± when Œ± Ã∏= 0, and Œ± is not necessarily an element of Z+.
Let us consider the special case
f (x) = (1 + x)Œ±
for which (if k ‚â•1)
f (k)(x) = Œ±(Œ± ‚àí1)(Œ± ‚àí2) ¬∑ ¬∑ ¬∑ (Œ± ‚àík + 1)(1 + x)Œ±‚àík.
(3.78)
These derivatives are guaranteed to exist, provided x > ‚àí1. We will assume this
restriction always applies. So, in particular
f (k)(0) = Œ±(Œ± ‚àí1)(Œ± ‚àí2) ¬∑ ¬∑ ¬∑ (Œ± ‚àík + 1)
(3.79)
giving the Maclaurin expansion
(1 + x)Œ± = 1 +
n

k=1
1
k![Œ±(Œ± ‚àí1) ¬∑ ¬∑ ¬∑ (Œ± ‚àík + 1)]xk
+
1
(n + 1)![Œ±(Œ± ‚àí1) ¬∑ ¬∑ ¬∑ (Œ± ‚àín)](1 + Œæ)Œ±‚àín‚àí1xn+1
(3.80)
for some Œæ ‚àà(x0, x). We may extend the deÔ¨Ånition (3.77), that is, deÔ¨Åne
 Œ±
0
!
= 1,
 Œ±
k
!
= 1
k!Œ±(Œ± ‚àí1) ¬∑ ¬∑ ¬∑ (Œ± ‚àík + 1)(k ‚â•1)
(3.81)
so that (3.80) becomes
(1 + x)Œ± =
n

k=0
 Œ±
k
!
xk
#
$%
&
=pn(x)
+
 
Œ±
n + 1
!
(1 + Œæ)Œ±‚àín‚àí1xn+1
#
$%
&
=en+1(x)
,
(3.82)
for x > ‚àí1.
Example 3.8
We wish to compute [1.03]1/3 with n = 2 in (3.82), and to esti-
mate the error involved in doing so. We have x = 0.03, Œ± = 1
3, and Œæ ‚àà(0, .03).
Therefore from (3.82) [1 + x]1/3 is approximated by the Taylor polynomial
p2(x) = 1 +
'
1
3
1
(
x +
'
1
3
2
(
x2 = 1 + 1
3x ‚àí1
9x2
so
[1.03]1/3 ‚âàp2(0.03) = 1.009900000
TLFeBOOK

TAYLOR SERIES
87
but [1.03]1/3 = 1.009901634, so e3(x) = 1.634 √ó 10‚àí6. From (3.82)
e3(x) =
'
1
3
3
(
(1 + Œæ)
1
3 ‚àí3x3 = 5
34 (1 + Œæ)‚àí8/3x3,
and so e3(0.03) = 5
34
33
106 (1 + Œæ)‚àí8/3 = 5
3 √ó 10‚àí6(1 + Œæ)‚àí8/3. Since 0 < Œæ < 0.03,
we have
1.5403 √ó 10‚àí6 < e3(.03) < 1.6667 √ó 10‚àí6.
The actual error is certainly within this range.
If f (x) =
1
1+x , and if x0 = 0, then
1
1 + x =
n

k=0
(‚àí1)kxk
#
$%
&
=pn(x)
+ (‚àí1)n+1xn+1
1 + x
#
$%
&
=r(x)
.
(3.83)
This may be seen by recalling that
n

k=0
Œ±k = 1 ‚àíŒ±n+1
1 ‚àíŒ±
(Œ± Ã∏= 1).
(3.84)
So n
k=0(‚àí1)kxk = 1‚àí(‚àí1)n+1xn+1
1+x
, and thus
n

k=0
(‚àí1)kxk + (‚àí1)n+1xn+1
1 + x
= 1 ‚àí(‚àí1)n+1xn+1
1 + x
+ (‚àí1)n+1xn+1
1 + x
=
1
1 + x .
This conÔ¨Årms (3.83). We observe that the remainder term r(x) in (3.83) is not given
by en+1(x) in (3.82). We have obtained an exact expression for the remainder using
elementary methods.
Now, from (3.83), we have
1
1 + t = 1 ‚àít + t2 ‚àít3 + ¬∑ ¬∑ ¬∑ + (‚àí1)n‚àí1tn‚àí1 + (‚àí1)ntn
1 + t
,
and we see immediately that
loge(1 + x) =
 x
0
dt
1 + t = x ‚àí1
2x2 + 1
3x3
+ ¬∑ ¬∑ ¬∑ + (‚àí1)n‚àí1xn
n
+ (‚àí1)n
 x
0
tn
1 + t dt
#
$%
&
=r(x)
.
(3.85)
TLFeBOOK

88
SEQUENCES AND SERIES
For x > 0, and 0 < t < x we have
1
1+t < 1, implying that
0 <
 x
0
tn
1 + t dt ‚â§
 x
0
tn dt = xn+1
n + 1
(x > 0).
For ‚àí1 < x < 0 with x < t < 0, we have
1
1 + t <
1
1 + x =
1
1 ‚àí|x|
so

 x
0
tn
1 + t dt
 ‚â§
1
1 ‚àí|x|

 x
0
tn dt
 =
1
1 ‚àí|x|

xn+1
n + 1
 =
|x|n+1
(1 ‚àí|x|)(n + 1).
Consequently, we may conclude that
|r(x)| ‚â§
Ô£±
Ô£¥Ô£¥Ô£≤
Ô£¥Ô£¥Ô£≥
1
n + 1xn+1,
x ‚â•0
|x|n+1
(1 ‚àí|x|)(n + 1),
‚àí1 < x ‚â§0
.
(3.86)
Equation (3.85) gives us a means to compute logarithms, and (3.86) gives us a
bound on the error.
Now consider (3.83) with x replaced by x2:
1
1 + x2 =
n

k=0
(‚àí1)kx2k + (‚àí1)n+1x2n+2
1 + x2
.
(3.87)
Replacing n with n ‚àí1, replacing x with t, and expanding, this becomes
1
1 + t2 = 1 ‚àít2 + t4 ‚àí¬∑ ¬∑ ¬∑ + (‚àí1)n‚àí1t2n‚àí2 + (‚àí1)nt2n
1 + t2 ,
where, on integrating, we obtain
tan‚àí1 x =
 x
0
dt
1 + t2 = x ‚àí1
3x3 + 1
5x5 ‚àí1
7x7
+ ¬∑ ¬∑ ¬∑ + (‚àí1)n‚àí1x2n‚àí1
2n ‚àí1
+ (‚àí1)n
 x
0
t2n
1 + t2 dt
#
$%
&
=r(x)
.
(3.88)
Because
1
1+t2 ‚â§1 for all t ‚ààR, it follows that
|r(x)| ‚â§|x|2n+1
2n + 1 .
(3.89)
TLFeBOOK

TAYLOR SERIES
89
We now have a method of computing œÄ. Since œÄ
4 = tan‚àí1(1), we have
œÄ
4 = 1 ‚àí1
3 + 1
5 ‚àí1
7 + ¬∑ ¬∑ ¬∑ + (‚àí1)n‚àí1
2n ‚àí1 + r(1)
(3.90)
and
r(1) ‚â§
1
2n + 1.
(3.91)
Using (3.90) to compute œÄ is not efÔ¨Åcient with respect to the number of arith-
metic operations needed (i.e., it is not computationally efÔ¨Åcient). This is because
to achieve an accuracy of about 1/n requires about n/2 terms in the series [which
follows from (3.91)]. However, if x is small (i.e., close to zero), then series (3.88)
converges relatively quickly. Observe that
tan‚àí1 x + y
1 ‚àíxy = tan‚àí1 x + tan‚àí1 y.
(3.92)
Suppose that x = 1
2, and y = 1
3, then
x+y
1‚àíxy = 1, so
œÄ
4 = tan‚àí1
 1
2
!
+ tan‚àí1
 1
3
!
.
(3.93)
It is actually faster to compute tan‚àí1( 1
2), and tan‚àí1( 1
3) using (3.88), and for these
obtain œÄ using (3.93) than to compute tan‚àí1(1) directly. In fact, this approach (a
type of ‚Äúdivide and conquer‚Äù method) can be taken further by noting that
tan‚àí1
 1
2
!
= tan‚àí1
 1
3
!
+ tan‚àí1
 1
7
!
, tan‚àí1
 1
3
!
= tan‚àí1
 1
5
!
+ tan‚àí1
 1
8
!
implying that
œÄ
4 = 2 tan‚àí1
 1
5
!
+ tan‚àí1
 1
7
!
+ 2 tan‚àí1
 1
8
!
.
(3.94)
Now consider f (x) = ex. Since f (k)(x) = ex for all k ‚ààZ+ we have for x0 = 0
the Maclaurin series expansion
ex =
‚àû

k=0
xk
k! .
(3.95)
This is theoretically valid for ‚àí‚àû< x < ‚àû. We have employed this series before
in various ways. We now consider it as a computational tool for calculating ex.
Appendix 3.C is based on a famous example in Forsythe et al. [21, pp. 14‚Äì16].
This example shows that series expansions must be implemented on computers
TLFeBOOK

90
SEQUENCES AND SERIES
with rather great care. SpeciÔ¨Åcally, Appendix 3.C shows what can happen when we
compute e‚àí20 by the direct implementation of the series (3.95). Using MATLAB
as stated e‚àí20 ‚âà4.1736 √ó 10‚àí9, which is based on keeping terms k = 0 to 88
(inclusive) of (3.95). Using additional terms will have no effect on the Ô¨Ånal answer
as they are too small. However, the correct value is actually e‚àí20 = 2.0612 √ó 10‚àí9,
as may be veriÔ¨Åed using the MATLAB exponential function, or using a typical
pocket calculator. Our series approximation has resulted in an answer possessing
no signiÔ¨Åcant digits at all. What went wrong? Many of the terms in the series
are orders of magnitude bigger than the Ô¨Ånal result and typically possess rounding
errors about as big as the Ô¨Ånal answer. The phenomenon is called catastrophic
cancellation (or catastrophic convergence). As Forsythe et al. [21] stated, ‚ÄúIt is
important to realize that this great cancellation is not the cause of error in the
answer; it merely magniÔ¨Åes the error already present in the terms.‚Äù Catastrophic
cancellation can in principle be eliminated by carrying more signiÔ¨Åcant digits in the
computation. However, this is costly with respect to computing resources. In the
present problem a cheap and very simple solution is to compute e20 using (3.95),
and then take the reciprocal, i.e., use e‚àí20 = 1/e20.
An important special function is the gamma function:
(z) =
 ‚àû
0
xz‚àí1e‚àíx dx.
(3.96)
Here, we assume z ‚ààR. This is an improper integral so we are left to wonder if
lim
M‚Üí‚àû
 M
0
xz‚àí1e‚àíx dx
exists. It turns out that the integral (3.96) converges for z > 0, but diverges for
z ‚â§0. The proof is slightly tedious, and so we will omit it [22, pp. 273‚Äì274]. If
z = n ‚ààN, then consider
(n) =
 ‚àû
0
xn‚àí1e‚àíx dx.
(3.97)
Now
(n + 1) =
 ‚àû
0
xne‚àíx dx = lim
M‚Üí‚àû
 M
0
xne‚àíx dx
= lim
M‚Üí‚àû

‚àíxne‚àíx|M
0 + n
 M
0
xn‚àí1e‚àíx dx

(via
"
u dv = uv ‚àí
"
v du, i.e., integration by parts). Therefore
(n + 1) = n
 ‚àû
0
xn‚àí1e‚àíx dx = n(n).
(3.98)
TLFeBOOK

TAYLOR SERIES
91
We see that (1) =
" ‚àû
0
e‚àíx dx = [‚àíe‚àíx]‚àû
0 = 1. Thus, (n + 1) = n!. Using the
gamma function in combination with (3.95), we may obtain Stirling‚Äôs formula
n! ‚âà
‚àö
2œÄnn+1/2e‚àín
(3.99)
which is a good approximation to n! if n is big. The details of a rigorous derivation
of this are tedious, so we give only an outline presentation. Begin by noting that
n! =
 ‚àû
0
xne‚àíx dx =
 ‚àû
0
en ln x‚àíx dx.
Let x = n + y, so
n! = e‚àín
 ‚àû
‚àín
en ln(n+y)‚àíy dy.
Now, since n ln(n + y) = n ln

n
)
1 + y
n
*
= n ln n + n ln
)
1 + y
n
*
, we have
n! = e‚àín
 ‚àû
‚àín
en ln n+n ln(1+ y
n)‚àíy dy
= e‚àínnn
 ‚àû
‚àín
en ln(1+ y
n)‚àíy dy.
Using (3.85), that is
ln

1 + y
n

= y
n ‚àíy2
2n2 + y3
3n3 ‚àí¬∑ ¬∑ ¬∑ ,
we have
nln

1 + y
n

‚àíy = ‚àíy2
2n + y3
3n2 ‚àí¬∑ ¬∑ ¬∑ ,
so
n! = nne‚àín
 ‚àû
‚àín
e‚àíy2
2n + y3
3n2 ‚àí¬∑¬∑¬∑ dy.
If now y = ‚àönv, then dy = ‚àöndv, and so
n! = nn+ 1
2 e‚àín
 ‚àû
‚àí‚àön
e
‚àív2
2 + v3
3‚àön ‚àí¬∑¬∑¬∑ dv.
So if n is big, then
n! ‚âànn+ 1
2 e‚àín
 ‚àû
‚àí‚àû
e‚àív2
2 dv.
TLFeBOOK

92
SEQUENCES AND SERIES
If we accept that
 ‚àû
‚àí‚àû
e‚àíx2/2 dx =
‚àö
2œÄ,
(3.100)
then immediately we have
n! ‚âà
‚àö
2œÄnn+ 1
2 e‚àín,
and the formula is now established. Stirling‚Äôs formula is very useful in statistical
mechanics (e.g., deriving the Fermi‚ÄìDirac distribution of fermion particle energies,
and this in turn is important in understanding the operation of solid-state electronic
devices at a physical level).
Another important special function is
g(x) =
1
‚àö
2œÄœÉ 2 exp

‚àí(x ‚àím)2
2œÉ 2

,
‚àí‚àû< x < ‚àû
(3.101)
which is the Gaussian function (or Gaussian pulse). This function is of immense
importance in probability theory [20], and is also involved in the uncertainty princi-
ple in signal processing and quantum mechanics [23]. A sketch of g(x) for m = 0,
with œÉ 2 = 1, and œÉ 2 = 0.1 appears in Fig. 3.6. For m = 0 and œÉ 2 = 1, the standard
form pulse
f (x) =
1
‚àö
2œÄ
e‚àíx2/2
(3.102)
is sometimes deÔ¨Åned [20]. In this case we observe that g(x) = 1
œÉ f
) x‚àím
œÉ
*
. We will
show that
 ‚àû
0
e‚àíx2 dx = 1
2
‚àöœÄ,
(3.103)
which can be used to obtain (3.100) by a simple change of variable. From [22]
(p. 262) we have
‚àí3
‚àí2
‚àí1
0
1
2
3
0
0.2
0.4
0.6
0.8
1
1.2
1.4
x
g(x)
s2 = 1
s2 = 0.1
Figure 3.6
Plots of two Gaussian pulses, where g(x) in (3.101) for m = 0, with œÉ 2 =
1, 0.1.
TLFeBOOK

TAYLOR SERIES
93
Theorem 3.5: Let limx‚Üí‚àûxpf (x) = A. Then
1.
" ‚àû
a
f (x) dx converges if p > 1 and ‚àí‚àû< A < ‚àû.
2.
" ‚àû
a
f (x) dx diverges if p ‚â§1 and A Ã∏= 0 (A may be inÔ¨Ånite).
We see that limx‚Üí‚àûx2e‚àíx2 = 0 (perhaps via l‚ÄôHÀÜopital‚Äôs rule). So in The-
orem 3.5 f (x) = e‚àíx2, and p = 2, with A = 0, and so
" ‚àû
0
e‚àíx2 dx converges.
DeÔ¨Åne
IM =
 M
0
e‚àíx2 dx =
 M
0
e‚àíy2 dy
and let limM‚Üí‚àûIM = I. Then
I 2
M =
  M
0
e‚àíx2 dx
!   M
0
e‚àíy2 dy
!
=
 M
0
 M
0
e‚àí(x2+y2) dx dy
=

RM

e‚àí(x2+y2) dx dy
for which RM is the square OABC in Fig. 3.7. This square has sides of length M.
Since e‚àí(x2+y2) > 0, we obtain

RL

e‚àí(x2+y2) dx dy ‚â§I 2
M ‚â§

RU

e‚àí(x2+y2) dx dy,
(3.104)
O
A
B
C
D
E
M
x
y
M
2
Figure 3.7
Regions used to establish (3.103).
TLFeBOOK

94
SEQUENCES AND SERIES
where RL is the region in the Ô¨Årst quadrant bounded by a circle of radius M.
Similarly, RU is the region in the Ô¨Årst quadrant bounded by a circle of radius
‚àö
2M.
Using polar coordinates, r2 = x2 + y2 and dx dy = r dr dœÜ, so (3.104) becomes
 œÄ/2
œÜ=0
 M
r=0
e‚àír2r dr dœÜ ‚â§I 2
M ‚â§
 œÄ/2
œÜ=0
 ‚àö
2M
r=0
e‚àír2r dr dœÜ.
(3.105)
Since ‚àí1
2
d
dx e‚àíx2 = xe‚àíx2 we have
" M
0 re‚àír2 dr = ‚àí1
2[e‚àíx2]M
0 = 1
2[1 ‚àíe‚àíM2].
Thus, (3.105) reduces to
œÄ
4 [1 ‚àíe‚àíM2] ‚â§I 2
M ‚â§œÄ
4 [1 ‚àíe‚àí2M2].
(3.106)
If we now allow M ‚Üí‚àûin (3.106), then I 2
M ‚ÜíœÄ
4 , implying that I 2 = œÄ
4 , or
I = 1
2
‚àöœÄ. This conÔ¨Årms (3.103).
In probability theory it is quite important to be able to compute functions such
as the error function
erf(x) =
2
‚àöœÄ
 x
0
e‚àít2 dt.
(3.107)
This has wide application in digital communications system analysis, for example.
No closed-form4 expression for (3.107) exists. We may therefore try to compute
(3.107) using series expansions. In particular, we may try working with the Maclau-
rin series expansion for ex:
erf(x) =
2
‚àöœÄ
 x
0
 ‚àû

k=0
xk
k!
	
x=‚àít2
dt
=
2
‚àöœÄ
‚àû

k=0
(‚àí1)k
k!
 x
0
t2k dt
=
2
‚àöœÄ
‚àû

k=0
(‚àí1)kx2k+1
k!(2k + 1) .
(3.108)
However, to arrive at this expression, we had to integrate an inÔ¨Ånite series term
by term. It is not obvious that we can do this. When is this justiÔ¨Åed?
A power series is any series of the form
f (x) =
‚àû

k=0
ak(x ‚àíx0)k.
(3.109)
Clearly, Taylor and Maclaurin series are all examples of power series. We have the
following theorem.
4A closed-form expression is simply a ‚Äúnice‚Äù formula typically involving more familiar functions such
as sines, cosines, tangents, polynomials, and exponential functions.
TLFeBOOK

TAYLOR SERIES
95
Theorem 3.6: Given the power series (3.109), there is an R ‚â•0 (which may
be R = +‚àû) such that the series is absolutely convergent for |x ‚àíx0| < R, and is
divergent for |x ‚àíx0| > R. At x ‚àíx0 = R and at x ‚àíx0 = ‚àíR, the series might
converge or diverge.
Series (3.109) is absolutely convergent if the series
h(x) =
‚àû

k=0
|ak(x ‚àíx0)k|
(3.110)
converges. We remark that absolutely convergent series are convergent. This means
that if (3.110) converges, then (3.109) also converges. (However, the converse is
not necessarily true.) We also have the following theorem.
Theorem 3.7: If
f (x) =
‚àû

k=0
ak(x ‚àíx0)k
for
|x ‚àíx0| < R,
where R > 0 is the radius of convergence of the power series, then f (x) is
continuous and differentiable in the interval of convergence x ‚àà(x0 ‚àíR, x0 + R),
and
f (1)(x) =
‚àû

k=1
kak(x ‚àíx0)k‚àí1,
(3.111a)
 x
x0
f (t) dt =
‚àû

k=0
ak
k + 1(x ‚àíx0)k+1.
(3.111b)
This series [Eq. (3.111a,b)] also has a radius of convergence R.
As a consequence of Theorem 3.7, Eq. (3.108) is valid for ‚àí‚àû< x < ‚àû(i.e., the
radius of convergence is R = +‚àû). This is because the Maclaurin expansion for
ex had R = +‚àû.
Example 3.9
Here we will Ô¨Ånd an expression for the error involved in trun-
cating the series for erf(x) in (3.108).
From (3.71) for some Œæ ‚àà[0, x] (interval endpoints may be included because of
continuity of the function being approximated)
ex =
n

k=0
xk
k!
# $% &
=pn(x)
+en(x),
TLFeBOOK

96
SEQUENCES AND SERIES
where
en(x) =
1
(n + 1)!eŒæxn+1.
Thus, where x = ‚àít2, so for some Œæ such that ‚àít2 ‚â§Œæ ‚â§0
e‚àít2 = pn(‚àít2) + en(‚àít2),
and hence
erf(x) =
2
‚àöœÄ
 x
0
pn(‚àít2) dt
#
$%
&
=qn(x)
+
2
‚àöœÄ
 x
0
en(‚àít2) dt
#
$%
&
=œµn(x)
,
where the degree n polynomial
qn(x) =
2
‚àöœÄ
 x
0
 n

k=0
(‚àí1)kt2k
k!
	
dt =
2
‚àöœÄ
n

k=0
(‚àí1)kx2k+1
k!(2k + 1)
is the approximation, and we are interested in the error
œµn(x) = erf(x) ‚àíqn(x) =
2
‚àöœÄ
 x
0
en(‚àít2) dt.
Clearly
œµn(x) =
2
‚àöœÄ
(‚àí1)n+1
(n + 1)!
 x
0
t2n+2eŒæ dt,
where we recall that Œæ depends on t in that ‚àít2 ‚â§Œæ ‚â§0. There is an integral
mean-value theorem, which states that for f (t), g(t) ‚ààC[a, b] (and g(t) does not
change sign on the interval [a, b]) there is a Œ∂ ‚àà[a, b] such that
 b
a
g(t)f (t) dt = f (Œ∂)
 b
a
g(t) dt.
Thus, there is a Œ∂ ‚àà[‚àíx2, 0], giving
œµn(x) =
2
‚àöœÄ eŒ∂ (‚àí1)n+1
(n + 1)!
x2n+3
2n + 3.
Naturally the error expression in Example 3.9 can be used to estimate how many
terms one must keep in the series expansion (3.108) in order to compute erf(x) to
a desired accuracy.
TLFeBOOK

ASYMPTOTIC SERIES
97
3.6
ASYMPTOTIC SERIES
The Taylor series expansions of Section 3.5 might have a large radius of conver-
gence, but practically speaking, if x is sufÔ¨Åciently far from x0, then many many
terms may be needed in a computer implementation to converge to the correct
solution with adequate accuracy. This is highly inefÔ¨Åcient. Also, if many terms
are to be retained, then rounding errors might accumulate and destroy the result.
In other words, Taylor series approximations are really effective only for x suf-
Ô¨Åciently close to x0 (i.e., ‚Äúsmall x‚Äù). We therefore seek expansion methods that
give good approximations for large values of the argument x. These are called the
asymptotic expansions, or asymptotic series. This section is just a quick introduction
based mainly on Section 19.15 in Kreyszig [24]. Another source of information on
asymptotic expansions, although applied mainly to problems involving differential
equations, appears in Lakin and Sanchez [25].
Asymptotic expansions may take on different forms. That is, there are different
‚Äúvarieties‚Äù of such expansions. (This is apparent in Ref. 25.) However, we will
focus on the following deÔ¨Ånition.
DeÔ¨Ånition 3.5: A series of the form
‚àû

k=0
ck
xk
(3.112)
for which ck ‚ààR (real-valued constants), and x ‚ààR is called an asymptotic expan-
sion, or asymptotic series, of a function f (x), which is deÔ¨Åned for all sufÔ¨Åciently
large x if, for every n ‚ààZ+

f (x) ‚àí
' n

k=0
ck
xk
(	
xn ‚Üí0
as
x ‚Üí‚àû,
(3.113)
and we shall then write
f (x) ‚àº
‚àû

k=0
ck
xk .
It is to be emphasized that the series (3.112) need not converge for any x. The
condition (3.113) suggests a possible method of Ô¨Ånding sequence (ck). SpeciÔ¨Åcally
f (x) ‚àíc0 ‚Üí0
or
c0 = lim
x‚Üí‚àûf (x),

f (x) ‚àíc0 ‚àíc1
x

x ‚Üí0
or
c1 = lim
x‚Üí‚àû[f (x) ‚àíc0]x,

f (x) ‚àíc0 ‚àíc1
x ‚àíc2
x2

x2 ‚Üí0
or
c2 = lim
x‚Üí‚àû

f (x) ‚àíc0 ‚àíc1
x

x2,
TLFeBOOK

98
SEQUENCES AND SERIES
or in general
cn = lim
x‚Üí‚àû

f (x) ‚àí
n‚àí1

k=0
ck
xk
	
xn
(3.114)
for n ‚â•1. However, this recursive procedure is seldom practical for generating
more than the Ô¨Årst few series coefÔ¨Åcients. Of course, in some cases this might
be all that is needed. We remark that DeÔ¨Ånition 3.5 can be usefully extended
according to
f (x) ‚àºg(x) + h(x)
 ‚àû

k=0
ck
xk
	
(3.115)
for which
f (x) ‚àíg(x)
h(x)
‚àº
‚àû

k=0
ck
xk .
(3.116)
The single most generally useful method for getting (ck) is probably to use ‚Äúinte-
gration by parts.‚Äù This is illustrated with examples.
Example 3.10
Recall erf(x) from (3.107). We would like to evaluate this func-
tion for large x [whereas the series in (3.108) is better suited for small x; see the
error expression in Example 3.9]. In this regard it is preferable to work with the
complementary error function
erfc(x) = 1 ‚àíerf(x) =
2
‚àöœÄ
 ‚àû
x
e‚àít2 dt.
(3.117)
We observe that erf(‚àû) = 1 [via (3.103)]. Now let œÑ = t2, so that dt = 1
2œÑ ‚àí1/2dœÑ.
With this change of variable
erfc(x) =
1
‚àöœÄ
 ‚àû
x2 œÑ ‚àí1
2 e‚àíœÑ dœÑ.
(3.118)
Now observe that via integration by parts, we have
 ‚àû
x2 œÑ ‚àí1
2 e‚àíœÑ dœÑ = ‚àíœÑ ‚àí1/2e‚àíœÑ|‚àû
x2 ‚àí1
2
 ‚àû
x2 œÑ ‚àí3
2 e‚àíœÑ dœÑ
= 1
x e‚àíx2 ‚àí1
2
 ‚àû
x2 œÑ ‚àí3
2 e‚àíœÑ dœÑ,
 ‚àû
x2 œÑ ‚àí3
2 e‚àíœÑ dœÑ = ‚àíœÑ ‚àí3/2e‚àíœÑ|‚àû
x2 ‚àí3
2
 ‚àû
x2 œÑ ‚àí5
2 e‚àíœÑ dœÑ
= 1
x3 e‚àíx2 ‚àí3
2
 ‚àû
x2 œÑ ‚àí5
2 e‚àíœÑ dœÑ,
TLFeBOOK

ASYMPTOTIC SERIES
99
and so on. We observe that this process of successive integration by parts has
generated integrals of the form
Fn(x) =
 ‚àû
x2 œÑ ‚àí(2n+1)/2e‚àíœÑ dœÑ
(3.119)
for n ‚ààZ+, and we see that erfc(x) =
1
‚àöœÄ F0(x). So, if we apply integration by
parts to Fn(x), then
Fn(x) =
 ‚àû
x2 œÑ ‚àí(2n+1)/2e‚àíœÑ dœÑ
= ‚àíœÑ ‚àí(2n+1)/2e‚àíœÑ|‚àû
x2 ‚àí2n + 1
2
 ‚àû
x2 œÑ ‚àí(2n+3)/2e‚àíœÑ dœÑ
= x‚àí(2n+1)e‚àíx2 ‚àí2n + 1
2
 ‚àû
x2 œÑ ‚àí(2n+3)/2e‚àíœÑ dœÑ
so that we have the recursive expression
Fn(x) =
1
x2n+1 e‚àíx2 ‚àí2n + 1
2
Fn+1(x)
(3.120)
which holds for n ‚ààZ+. This may be rewritten as
ex2Fn(x) =
1
x2n+1 ‚àí2n + 1
2
ex2Fn+1(x).
(3.121)
Repeated application of (3.121) yields
ex2F0(x) = 1
x ‚àí1
2ex2F1(x),
ex2F0(x) = 1
x ‚àí
1
2x3 + 1 ¬∑ 3
22 ex2F2(x),
ex2F0(x) = 1
x ‚àí
1
2x3 + 1 ¬∑ 3
22x5 ‚àí1 ¬∑ 3 ¬∑ 5
28
ex2F3(x),
and so Ô¨Ånally
ex2F0(x) =
1
x ‚àí
1
2x3 + 1 ¬∑ 3
22x5 ‚àí¬∑ ¬∑ ¬∑ + (‚àí1)n‚àí1 1 ¬∑ 3 ¬∑ ¬∑ ¬∑ (2n ‚àí3)
2n‚àí1x2n‚àí1

#
$%
&
=S2n‚àí1(x)
+ (‚àí1)n 1 ¬∑ 3 ¬∑ ¬∑ ¬∑ (2n ‚àí1)
2n
ex2Fn(x).
(3.122)
TLFeBOOK

100
SEQUENCES AND SERIES
From this it appears that our asymptotic expansion is
ex2F0(x) ‚àº1
x ‚àí
1
2x3 + 1 ¬∑ 3
22x5 ‚àí¬∑ ¬∑ ¬∑ + (‚àí1)n‚àí1 1 ¬∑ 3 ¬∑ ¬∑ ¬∑ (2n ‚àí3)
2n‚àí1x2n‚àí1
+ ¬∑ ¬∑ ¬∑
(3.123)
However, this requires conÔ¨Årmation. DeÔ¨Åne Kn = (‚àí2)‚àín[1 ¬∑ 3 ¬∑ ¬∑ ¬∑ (2n ‚àí1)]. From
(3.122), we have
[ex2F0(x) ‚àíS2n‚àí1(x)]x2n‚àí1 = Knex2x2n‚àí1Fn(x).
(3.124)
We wish to show that for any Ô¨Åxed n = 1, 2, 3, . . . the expression in (3.124) on
the right of the equality goes to zero as x ‚Üí‚àû. In (3.119) we have
1
œÑ (2n+1)/2 ‚â§
1
x2n+1
for all œÑ ‚â•x2, which gives the bound
Fn(x) =
 ‚àû
x2
e‚àíœÑ
œÑ (2n+1)/2 dœÑ ‚â§
1
x2n+1
 ‚àû
x2 e‚àíœÑ dœÑ = e‚àíx2
x2n+1 .
(3.125)
But this implies that
|Kn|ex2x2n‚àí1Fn(x) ‚â§|Kn|ex2x2n‚àí1 e‚àíx2
x2n+1 = |Kn|
x2
and |Kn|
x2 ‚Üí0 for x ‚Üí‚àû. Thus, immediately, (3.123) is indeed the asymptotic
expansion for ex2F0(x). Hence
erfc(x) ‚àº
1
‚àöœÄ e‚àíx2  1
x ‚àí
1
2x3 + 1 ¬∑ 3
22x5 ‚àí¬∑ ¬∑ ¬∑ + (‚àí1)n‚àí1 1 ¬∑ 3 ¬∑ ¬∑ ¬∑ (2n ‚àí3)
2n‚àí1x2n‚àí1
+ ¬∑ ¬∑ ¬∑

.
(3.126)
We recall from Section 3.4 that the integral
" x
0
sin t
t
dt was important in analyzing
the Gibbs phenomenon in Fourier series expansions. We now consider asymptotic
approximations to this integral.
Example 3.11
The sine integral is
Si(x) =
 x
0
sin t
t
dt,
(3.127)
and the complementary sine integral is
si(x) =
 ‚àû
x
sin t
t
dt.
(3.128)
TLFeBOOK

ASYMPTOTIC SERIES
101
It turns out that si(0) = œÄ
2 , which is shown on p. 277 of Spiegel [22]. We wish to
Ô¨Ånd an asymptotic series for Si(x). Since
œÄ
2 =
 ‚àû
0
sin t
t
dt =
 x
0
sin t
t
dt +
 ‚àû
x
sin t
t
dt = Si(x) + si(x),
(3.129)
we will consider the expansion of si(x). If we integrate by parts in succession, then
 ‚àû
x
t‚àí1 sin t dt = 1
x cos x ‚àí1 ¬∑
 ‚àû
x
1
t2 cos t dt,
 ‚àû
x
t‚àí2 cos t dt = ‚àí1
x2 sin x + 2 ¬∑
 ‚àû
x
1
t3 sin t dt,
 ‚àû
x
t‚àí3 sin t dt = 1
x3 cos x ‚àí3 ¬∑
 ‚àû
x
1
t4 cos t dt,
 ‚àû
x
t‚àí4 cos t dt = ‚àí1
x4 sin x + 4 ¬∑
 ‚àû
x
1
t5 sin t dt,
and so on. If n ‚ààN, then we may deÔ¨Åne
sn(x) =
 ‚àû
x
t‚àín sin t dt,
cn(x) =
 ‚àû
x
t‚àín cos t dt.
(3.130)
Therefore, for odd n
sn(x) = 1
xn cos x ‚àín cn+1(x),
(3.131a)
and for even n
cn(x) = ‚àí1
xn sin x + n sn+1(x).
(3.131b)
We observe that s1(x) = si(x). Repeated application of the recursions (3.131a,b)
results in
s1(x) = 1 ¬∑ 1
x
cos x + 1 ¬∑ 1
x2 sin x ‚àí1 ¬∑ 2s3(x)
= 1 ¬∑ 1
x
cos x + 1 ¬∑ 1
x2 sin x ‚àí1 ¬∑ 2
x3 cos x ‚àí1 ¬∑ 2 ¬∑ 3
x4
sin x + 1 ¬∑ 2 ¬∑ 3 ¬∑ 4s5(x),
or in general
s1(x) = cos x
1 ¬∑ 1
x
‚àí1 ¬∑ 2
x3 + ¬∑ ¬∑ ¬∑ + (‚àí1)n+1 (2n ‚àí2)!
x2n‚àí1

+ sin x
1 ¬∑ 1
x2 ‚àí1 ¬∑ 2 ¬∑ 3
x4
+ ¬∑ ¬∑ ¬∑ + (‚àí1)n+1 (2n ‚àí1)!
x2n

+ (‚àí1)n(2n)!s2n+1(x)
(3.132)
TLFeBOOK

102
SEQUENCES AND SERIES
for n ‚ààN (with 0! = 1). From this, we obtain
[s1(x) ‚àíS2n(x)]x2n = (‚àí1)n(2n)!x2ns2n+1(x)
(3.133)
for which S2n(x) is appropriately deÔ¨Åned as those terms in (3.132) involving sin x
and cos x. It is unclear whether
lim
x‚Üí‚àûx2ns2n+1(x) = 0
(3.134)
for any n ‚ààN. But if we accept (3.134), then
si(x) ‚àºS2n(x) = cos x
1 ¬∑ 1
x
‚àí1 ¬∑ 2
x3 + ¬∑ ¬∑ ¬∑ + (‚àí1)n+1 (2n ‚àí2)!
x2n‚àí1

+ sin x
1 ¬∑ 1
x2 ‚àí1 ¬∑ 2 ¬∑ 3
x4
+ ¬∑ ¬∑ ¬∑ + (‚àí1)n+1 (2n ‚àí1)!
x2n

. (3.135)
Figure 3.8 shows plots of Si(x) and the asymptotic approximation œÄ
2 ‚àíS2n(x) for
n = 1, 2, 3. We observe that the approximations are good for ‚Äúlarge x,‚Äù but poor
for ‚Äúsmall x,‚Äù as we would expect. Moreover, for small x, the approximations are
2
3
4
5
6
7
8
9
10
0
0.5
1
1.5
2
x
Amplitude
Si(x)
n = 1
n = 2
n = 3
1.6
1.8
2
2.2
2.4
2.6
2.8
3
‚àí3
‚àí2
‚àí1
0
1
2
x
Amplitude
Si(x)
n = 1
n = 2
n = 3
(a)
(b)
Figure 3.8
Plot of Si(x) using MATLAB routine sinint, and plots of œÄ
2 ‚àíS2n(x) for n =
1,2,3. The horizontal solid line is at height œÄ/2.
TLFeBOOK

MORE ON THE DIRICHLET KERNEL
103
better for smaller n. This is also reasonable. Our plots therefore constitute informal
veriÔ¨Åcation of the correctness of (3.135), in spite of potential doubts about (3.134).
Another approach to Ô¨Ånding (ck) is based on the fact that many special func-
tions are solutions to particular differential equations. If the originating differential
equation is known, it may be used to generate sequence (ck). However, this section
was intended to be relatively brief, and so this method is omitted. The interested
reader may see Kreyszig [24].
3.7
MORE ON THE DIRICHLET KERNEL
In Section 3.4 the Dirichlet kernel was introduced in order to analyze the manner
in which Fourier series converge in the vicinity of a discontinuity of a 2œÄ-periodic
function. However, this was only done with respect to the special case of g(t) in
(3.19). In this section we consider the Dirichlet kernel Dn(t) in a more general
manner.
We begin by recalling the complex Fourier series expansion from Chapter 1
(Section 1.3.3). If f (t) ‚ààL2(0, 2œÄ), then
f (t) =
‚àû

n=‚àí‚àû
fnejnt,
(3.136)
where fn = ‚ü®f, en‚ü©, with en(t) = ejnt, and
‚ü®x, y‚ü©= 1
2œÄ
 2œÄ
0
x(t)y‚àó(t) dt.
(3.137)
An approximation to f (t) is the truncated Fourier series expansion
fL(t) =
L

n=‚àíL
fnejnt ‚ààL2(0, 2œÄ).
(3.138)
The approximation error is
œµL(t) = f (t) ‚àífL(t) ‚ààL2(0, 2œÄ).
(3.139)
We seek a general expression for œµL(t) that is hopefully more informative than
(3.139). The overall goal is to generalize the error analysis approach seen in
Section 3.4. Therefore, consider
œµL(t) = f (t) ‚àí
L

n=‚àíL
‚ü®f, en‚ü©en(t)
TLFeBOOK

104
SEQUENCES AND SERIES
[via fn = ‚ü®f, en‚ü©, and (3.138) into (3.139)]. Thus
œµL(t) = f (t) ‚àí
L

n=‚àíL

1
2œÄ
 2œÄ
0
f (x)e‚àíjnx dx

en(t)
= f (t) ‚àí1
2œÄ
 2œÄ
0
f (x)

L

n=‚àíL
ejn(t‚àíx)

dx
(3.140)
Since
L

n=‚àíL
ejn(t‚àíx) = 1 + 2
L

n=1
cos[n(t ‚àíx)]
(3.141)
(show this as an exercise), via (3.24), we obtain
1 + 2
L

n=1
cos[n(t ‚àíx)] = sin[(L + 1
2)(t ‚àíx)]
sin[ 1
2(t ‚àíx)]
= 2œÄDL(t ‚àíx).
(3.142)
Immediately, we see that
œµL(t) = f (t) ‚àí
 2œÄ
0
f (x)DL(t ‚àíx) dx,
(3.143)
where also (recall (3.139))
fL(t) =
 2œÄ
0
f (x)DL(t ‚àíx) dx.
(3.144)
Equation (3.144) is an alternative integral form of the approximation to f (t) origi-
nally speciÔ¨Åed in (3.138). The integral in (3.144) is really an example of something
called a convolution integral. The following example will demonstrate how we
might apply (3.143).
Example 3.12
Suppose that
f (t) =
 sin t,
0 < t < œÄ
0,
œÄ ‚â§t < 2œÄ
.
Note that f (t) is continuous for all t, but that f (1)(t) = df (t)/dt is not continuous
everywhere. For example, f (1)(t) is not continuous at t = œÄ. Plots of fL(t) for
various L (see Fig. 3.10) suggest that fL(t) converges most slowly to f (t) near
t = œÄ. Can we say something about the rate of convergence?
TLFeBOOK

MORE ON THE DIRICHLET KERNEL
105
Therefore, consider
fL(œÄ) = 1
2œÄ
 œÄ
0
sin x sin[(L + 1
2)(œÄ ‚àíx)]
sin[ 1
2(œÄ ‚àíx)]
dx.
(3.145)
Now
sin
1
2(œÄ ‚àíx)

= sin
œÄ
2

cos
 1
2x
!
‚àícos
œÄ
2

sin
 1
2x
!
= cos
 1
2x
!
,
and since sin x = 2 sin( 1
2x) cos( 1
2x) so (3.145) reduces to
fL(œÄ) = 1
œÄ
 œÄ
0
sin
 1
2x
!
sin
 
L + 1
2
!
(œÄ ‚àíx)

dx
= 1
2œÄ
 œÄ
0

cos

(L + 1)x ‚àí
 
L + 1
2
!
œÄ

‚àícos

Lx ‚àí
 
L + 1
2
!
œÄ

dx
=
1
2œÄ(L + 1)

sin

(L + 1)x ‚àí
 
L + 1
2
!
œÄ
œÄ
0
‚àí
1
2œÄL

sin

Lx ‚àí
 
L + 1
2
!
œÄ
œÄ
0
=
1
2œÄ(L + 1)

1 + sin
 
L + 1
2
!
œÄ

‚àí
1
2œÄL

‚àí1 + sin
 
L + 1
2
!
œÄ

= 1
2œÄ
2L + 1 ‚àí(‚àí1)L
L(L + 1)
.
(3.146)
We see that, as expected
lim
L‚Üí‚àûfL(œÄ) = 0.
Also, (3.146) gives œµL(œÄ) = ‚àífL(œÄ), and this is the exact value for the approxi-
mation error at t = œÄ for all L. Furthermore
|œµL(œÄ)| ‚àù1
L
for large L, and so we have a measure of the rate of convergence of the error, at
least at the point t = œÄ. (Symbol ‚Äú‚àù‚Äù means ‚Äúproportional to.‚Äù)
We remark that f (t) in Example 3.12 may be regarded as the voltage drop
across the resistor R in Fig. 3.9. The circuit in Fig. 3.9 is a simple half-wave
TLFeBOOK

106
SEQUENCES AND SERIES
Ideal diode
R
f(t)
v(t)
+
+
‚àí
‚àí
Figure 3.9
An electronic circuit interpretation for Example 3.12; here, v(t) = sin(t) for
all t ‚ààR.
0
1
2
3
4
5
6
7
‚àí0.2
0
0.2
0.4
0.6
0.8
1
1.2
t
Amplitude
f(t) = sin(t)
L = 10
L = 3
Figure 3.10
A plot of f (t) and Fourier series approximations to f (t) [i.e., fL(t) for
L = 3, 10].
rectiÔ¨Åer circuit. The reader ought to verify as an exercise that
fL(t) = 1
œÄ + 1
2 sin t +
L

n=2
1 + (‚àí1)n
œÄ(1 ‚àín2) cos nt.
(3.147)
Plots of fL(t) for L = 3, 10 versus the plot of f (t) appear in Fig. 3.10.
TLFeBOOK

COORDINATE ROTATION DI GITAL COMPUTING (CORDIC)
107
3.8
FINAL REMARKS
We have seen that sequences and series might converge ‚Äúmathematically‚Äù yet not
‚Äúnumerically.‚Äù Essentially, we have seen three categories of difÔ¨Åculty:
1. Pointwise convergence of series leading to irreducible errors in certain regions
of the approximation, such as the Gibbs phenomenon in Fourier expansions,
which arises in the vicinity of discontinuities in the function being approxi-
mated
2. The destructive effect of rounding errors as illustrated by the catastrophic
convergence of series
3. Slow convergence such as illustrated by the problem of computing œÄ with
the Maclaurin expansion for tan‚àí1 x
We have seen that some ingenuity may be needed to overcome obstacles such
as these. For example, a divide-and-conquer approach helped in the problem of
computing œÄ. In the case of catastrophic convergence, the problem was solved by
changing the computational algorithm. The problem of overcoming Gibbs overshoot
is not considered here. However, it involves seeking uniformly convergent series
approximations.
APPENDIX 3.A
COORDINATE ROTATATION DI GITAL
COMPUTING (CORDIC)
3.A.1
Introduction
This appendix presents the basics of a method for computing ‚Äúelementary func-
tions,‚Äù which includes the problem of rotating vectors in the plane, computing
tan‚àí1 x, sin Œ∏, and cos Œ∏. The method to be used is called ‚Äúcoordinate rotation
digital computing‚Äù (CORDIC), and was invented by Jack Volder [3] in the late
1950s. However, in spite of the age of the method, it is still important. The method
is one of those great ideas that is able to survive despite technological changes. It
is a good example of how a clever mathematical idea, if anything, becomes less
obsolete with the passage of time.
The CORDIC method was, and is, desirable because it reduces the problem of
computing apparently complicated functions, such as trig functions, to a succession
of simple operations. SpeciÔ¨Åcally, these simple operations are shifting and adding.
In the 1950s it was a major achievement just to build systems that could add two
numbers together because all that was available for use was vacuum tubes, and to a
lesser degree, discrete transistors. However, even with the enormous improvements
in computing technology that have occurred since then, it is still important to reduce
complicated operations to simple ones. Thus, the CORDIC method has survived
very well. For example, in 1980 [5] a special-purpose CORDIC VLSI (very large
scale integration) chip was presented. More recent references to the method will
TLFeBOOK

108
SEQUENCES AND SERIES
be given later. Nowadays, the CORDIC method is more likely to be implemented
with gate-array technology.
Since the CORDIC method involves the operations of shifting and adding only,
once the mathematics of the method is understood, it is easy to build CORDIC
computing hardware using the logic design methods considered in typical elemen-
tary digital logic courses or books.5 Consideration of CORDIC computing also
makes a connection between elementary mathematics courses (calculus and linear
algebra) and computer hardware systems design, as well as the subject of numerical
analysis.
3.A.2
The Concept of a Discrete Basis
The original paper of Volder [3] does not give a rigorous treatment of the mathe-
matics of the CORDIC method. However, in this section (based closely on Schelin
[6]), we will begin the process of deriving the CORDIC method in a mathemati-
cally rigorous manner. The central idea is to represent operands (e.g, the Œ∏ in sin Œ∏)
in terms of a discrete basis. When the discrete basis representation is combined
with appropriate mathematical identities the CORDIC algorithm results.
Let R denote the set of real numbers. Everything we do here revolves around
the following theorem (From Schelin [6]).
Theorem 3.A.1: Suppose that Œ∏k ‚ààR for k ‚àà{0, 1, 2, 3, . . .} satisfy Œ∏0 ‚â•Œ∏1 ‚â•
Œ∏2 ‚â•¬∑ ¬∑ ¬∑ ‚â•Œ∏n > 0, and that
Œ∏k ‚â§
n

j=k+1
Œ∏j + Œ∏n
for
0 ‚â§k ‚â§n,
(3.A.1a)
and suppose Œ∏ ‚ààR satisÔ¨Åes
|Œ∏| ‚â§
n

j=0
Œ∏j.
(3.A.1b)
If Œ∏(0) = 0, and Œ∏(k+1) = Œ∏(k) + Œ¥kŒ∏k for 0 ‚â§k ‚â§n, where
Œ¥k =

1,
if Œ∏ ‚â•Œ∏(k)
‚àí1,
if Œ∏ < Œ∏(k)
(3.A.1c)
then
|Œ∏ ‚àíŒ∏(k)| ‚â§
n

j=k
Œ∏j + Œ∏n
for
0 ‚â§k ‚â§n,
(3.A.1d)
and so in particular |Œ∏ ‚àíŒ∏(n+1)| ‚â§Œ∏n.
5The method is also easy to implement in an assembly language (or other low-level) programming
environment.
TLFeBOOK

COORDINATE ROTATION DI GITAL COMPUTING (CORDIC)
109
Proof
We may use proof by mathematical induction6 on the index k. For k = 0
|Œ∏ ‚àíŒ∏(0)| = |Œ∏| ‚â§
n

j=0
Œ∏j <
n

j=0
Œ∏j + Œ∏n
via (3.A.1b).
Assume that |Œ∏ ‚àíŒ∏(k)| ‚â§n
j=k Œ∏j + Œ∏n is true, and consider |Œ∏ ‚àíŒ∏(k+1)|. Via
(3.A.1c), Œ¥k and Œ∏ ‚àíŒ∏(k) have the same sign, and so
|Œ∏ ‚àíŒ∏(k+1)| = |Œ∏ ‚àíŒ∏(k) ‚àíŒ¥kŒ∏k| = ||Œ∏ ‚àíŒ∏(k)| ‚àíŒ∏k|.
Now, via the inductive hypothesis (i.e., |Œ∏ ‚àíŒ∏(k)| ‚â§n
j=k Œ∏j + Œ∏n), we have
|Œ∏ ‚àíŒ∏(k)| ‚àíŒ∏k ‚â§
n

j=k
Œ∏j + Œ∏n ‚àíŒ∏k =
n

j=k+1
Œ∏j + Œ∏n.
(3.A.2a)
Via (3.A.1a), we obtain
‚àí
Ô£Æ
Ô£∞
n

j=k+1
Œ∏j + Œ∏n
Ô£π
Ô£ª‚â§‚àíŒ∏k
so that
‚àí
Ô£Æ
Ô£∞
n

j=k+1
Œ∏j + Œ∏n
Ô£π
Ô£ª‚â§|Œ∏ ‚àíŒ∏(k)| ‚àíŒ∏k.
(3.A.2b)
Combining (3.A.2a) with (3.A.2b) gives
|Œ∏ ‚àíŒ∏(k+1)| = ||Œ∏ ‚àíŒ∏(k)| ‚àíŒ∏k| ‚â§
n

j=k+1
Œ∏j + Œ∏n
so that (3.A.1d) holds for k replaced by k + 1, and so (3.A.1d) holds via induction.
We will call this result Schelin‚Äôs theorem. The set {Œ∏k} is called a discrete basis
if it satisÔ¨Åes the restrictions given in the theorem.
In what follows we will interpret Œ∏ as an angle. However, note that Œ∏ in this
theorem could be more general than this. Now suppose that we deÔ¨Åne
Œ∏k = tan‚àí1 2‚àík
(3.A.3)
for k = 0, 1, . . . , n. Table 3.A.1 shows typical values for Œ∏k as given by (3.A.3).
We see that good approximations to Œ∏ can be obtained from relatively small n
(because from Schelin‚Äôs theorem |Œ∏ ‚àíŒ∏(n+1)| ‚â§Œ∏n).
6A brief introduction to proof by mathematical induction appears in Appendix 3.B.
TLFeBOOK

110
SEQUENCES AND SERIES
TABLE 3.A.1
Values
for Some Elements of
the Discrete Basis given
by Eq. (3.A.3)
k
Œ∏k = tan‚àí1 2‚àík
0
45‚ó¶
1
26.565‚ó¶
2
14.036‚ó¶
3
7.1250‚ó¶
4
3.5763‚ó¶
Clearly, these satisfy Œ∏0 ‚â•Œ∏1 ‚â•¬∑ ¬∑ ¬∑ ‚â•Œ∏n > 0, which is one of the restrictions in
Schelin‚Äôs theorem.
The mean-value theorem of elementary calculus says that there exists a Œæ ‚àà
[x0, x0 + ] such that
df (x)
dx

x=Œæ
= f (x0 + ) ‚àíf (x0)

so if f (x) = tan‚àí1 x, then df (x)/dx = 1/(1 + x2), and thus
1
1 + x2

x‚àà[2‚àí(k+1),2‚àík]
=
Œ∏k ‚àíŒ∏k+1
2‚àík ‚àí2‚àí(k+1)
‚áí
Œ∏k ‚àíŒ∏k+1
2‚àík ‚àí2‚àí(k+1) ‚â§
1
1 + x2

x=2‚àí(k+1)
because the slope of tan‚àí1 x is largest at x = 2‚àí(k+1), and this in turn implies
Œ∏k ‚àíŒ∏k+1 ‚â§2‚àík ‚àí2‚àí(k+1)
1 + 2‚àí2(k+1) = 2k+2 ‚àí2k+1
1 + 22(k+1) =
2k+1
1 + 22(k+1)
and
Œ∏k ‚â•
2k
1 + 22k
because tan‚àí1 x ‚â•x d
dx tan‚àí1 x =
x
1+x2 (x ‚â•0), for k = 0, 1, . . . , n (let x = 2‚àík).7
7For x ‚â•0
1
1 + x2 ‚â•d
dx
x
1 + x2 =
1 ‚àíx2
(1 + x2)2
(certainly 1 ‚â•1‚àíx2
1+x2 for x ‚â•0)
‚áí
 x
0
1
1 + t2 dt ‚â•
x
1 + x2 ‚áítan‚àí1 x ‚â•
x
1 + x2
TLFeBOOK

COORDINATE ROTATION DI GITAL COMPUTING (CORDIC)
111
Now, as a consequence of these results
Œ∏k ‚àíŒ∏n = (Œ∏k ‚àíŒ∏k+1) + (Œ∏k+1 ‚àíŒ∏k+2) + ¬∑ ¬∑ ¬∑ + (Œ∏n‚àí2 ‚àíŒ∏n‚àí1) + (Œ∏n‚àí1 ‚àíŒ∏n)
=
n‚àí1

j=k
(Œ∏j ‚àíŒ∏j+1)
‚â§
n‚àí1

j=k
2j+1
1 + 22(j+1)
 
Œ∏j ‚àíŒ∏j+1 ‚â§
2j+1
1 + 22(j+1)
!
=
n

j=k+1
2j
1 + 22j
‚â§
n

j=k+1
Œ∏j
 
Œ∏j ‚â•
2j
1 + 22j
!
implying that
Œ∏k ‚â§
n

j=k+1
Œ∏j + Œ∏n
for k = 0, 1, . . . , n, and thus (3.A.1a) holds for {Œ∏k} in (3.A.3), and so (3.A.3) is
a concrete example of a discrete basis. If you have a pocket calculator handy then
it is easy to verify that
3

j=0
Œ∏j = 92.73‚ó¶> 90‚ó¶,
so we will work only with angles Œ∏ that satisfy |Œ∏| ‚â§90‚ó¶. Thus, for such Œ∏, there
exists a sequence {Œ¥k} such that Œ¥k ‚àà{‚àí1, +1}, where
Œ∏ =
n

k=0
Œ¥kŒ∏k + œµn+1 = Œ∏(n+1) + œµn+1,
(3.A.4)
where |œµn+1| ‚â§Œ∏n = tan‚àí1 2‚àín. Equation (3.A.1c) gives us a way to Ô¨Ånd {Œ¥k}. We
can also write that any angle Œ∏ satisfying |Œ∏| ‚â§œÄ
2 (radians) can be represented
exactly as
Œ∏ =
‚àû

k=0
Œ¥kŒ∏k
for appropriately chosen coordinates {Œ¥k}.
In the next section we will begin to see how useful it is to be able to represent
angles in terms of the discrete basis given by (3.A.3).
TLFeBOOK

112
SEQUENCES AND SERIES
3.A.3
Rotating Vectors in the Plane
No one would disagree that a basic computational problem in electrical and com-
puter engineering is to Ô¨Ånd
z‚Ä≤ = ejŒ∏z,
Œ∏ ‚ààR,
(3.A.5)
where j =
‚àö
‚àí1, z = x + jy, and z‚Ä≤ = x‚Ä≤ + jy‚Ä≤ (x, y, x‚Ä≤, y‚Ä≤ ‚ààR). Certainly, this
problem arises in the phasor analysis of circuits, in computer graphics (to rotate
objects, for example), and it also arises in digital signal processing (DSP) a lot.8
Thus, z and z‚Ä≤ are complex variables. Expressing them in terms of their real and
imaginary parts, we may rewrite (3.A.5) as
x‚Ä≤ + jy‚Ä≤ = (cos Œ∏ + j sin Œ∏)(x + jy) = [x cos Œ∏ ‚àíy sin Œ∏] + j[x sin Œ∏ + y cos Œ∏],
and this can be further rewritten as the matrix‚Äìvector product
 x‚Ä≤
y‚Ä≤

=
 cos Œ∏
‚àísin Œ∏
sin Œ∏
cos Œ∏
  x
y

(3.A.6)
which we recognize as the formula for rotating the vector [xy]T in the plane to
[x‚Ä≤y‚Ä≤]T .
Recall the trigonometric identities
sin Œ∏ =
tan Œ∏
‚àö
1 + tan2 Œ∏
,
cos Œ∏ =
1
‚àö
1 + tan2 Œ∏
,
(3.A.7)
and so for Œ∏ = Œ∏k in (3.A.3)
sin Œ∏k =
2‚àík
‚àö
1 + 2‚àí2k ,
cos Œ∏k =
1
‚àö
1 + 2‚àí2k .
(3.A.8)
Thus, to rotate xk + jyk by angle Œ¥kŒ∏k to xk+1 + jyk+1 is accomplished via
 xk+1
yk+1

=
1
‚àö
1 + 2‚àí2k

1
‚àíŒ¥k2‚àík
Œ¥k2‚àík
1
  xk
yk

,
(3.A.9)
8In DSP it is often necessary to compute the discrete Fourier transform (DFT), which is an approximation
to the Fourier transform (FT) and is deÔ¨Åned by
Xn =
N‚àí1

k=0
exp
 
‚àíj 2œÄkn
N
xk
!
,
where {xk} is the samples of some analog signal, i.e., xk = x(kT ), where k is an integer and T is a
positive constant; x(t) for t ‚ààR is the analog signal. Note that additional information about the DFT
appeared in Section 1.4.
TLFeBOOK

COORDINATE ROTATION DI GITAL COMPUTING (CORDIC)
113
where we‚Äôve used (3.A.6). From Schelin‚Äôs theorem, Œ∏(n+1) ‚âàŒ∏, so if we wish to
rotate x0 + jy0 = x + jy by Œ∏(n+1) to xn+1 + jyn+1 (‚âàx‚Ä≤ + jy‚Ä≤) then via (3.A.9)
 xn+1
yn+1

=
' n
/
k=0
1
‚àö
1 + 2‚àí2k
( 
1
‚àíŒ¥n2‚àín
Œ¥n2‚àín
1

¬∑ ¬∑ ¬∑

1
‚àíŒ¥12‚àí1
Œ¥12‚àí1
1
  1
‚àíŒ¥0
Œ¥0
1
  x0
y0

.
(3.A.10)
DeÔ¨Åne
Kn =
n
/
k=0
1
‚àö
1 + 2‚àí2k ,
(3.A.11)
where 0n
k=0 Œ±k = Œ±nŒ±n‚àí1Œ±n‚àí2 ¬∑ ¬∑ ¬∑ Œ±1Œ±0.
Consider
 ÀÜxn+1
ÀÜyn+1

=

1
‚àíŒ¥n2‚àín
Œ¥n2‚àín
1

¬∑ ¬∑ ¬∑

1
‚àíŒ¥12‚àí1
Œ¥12‚àí1
1
  1
‚àíŒ¥0
Œ¥0
1
  x0
y0

,
(3.A.12)
which is the same expression as (3.A.10) except that we have dropped the multi-
plication by Kn. We observe that to implement (3.A.12) requires only the simple
(to implement in digital hardware, or assembly language) operations of shifting
and adding. Implementing (3.A.12) rotates x + jy by approximately the desired
amount, but gives a solution vector that is a factor of 1/Kn longer than it should
be. Of course, this can be corrected by multiplying [ÀÜxn+1
ÀÜyn+1]T by Kn if desired.
Note that in some applications, this would not be necessary.
Note that
 cos Œ∏
‚àísin Œ∏
sin Œ∏
cos Œ∏

‚âàKn

1
‚àíŒ¥n2‚àín
Œ¥n2‚àín
1

¬∑ ¬∑ ¬∑

1
‚àíŒ¥12‚àí1
Œ¥12‚àí1
1
  1
‚àíŒ¥0
Œ¥0
1

,
(3.A.13)
and so the matrix product in (3.A.12) [or (3.A.10)] represents an efÔ¨Åcient approxi-
mate factorization of the rotation operator in (3.A.6). The approximation gets better
and better as n increases, and in the limit as n ‚Üí‚àûbecomes exact.
The computational complexity of the CORDIC rotation algorithm may be des-
cribed as follows. In Eq. (3.A.10) there are exactly 2n shifts (i.e., multiplications
by 2‚àík), and 2n + 2 additions, plus two scalings by factor Kn. As well, only n + 1
bits of storage are needed to save the sequence {Œ¥k}.
We conclude this section with a numerical example to show how to obtain the
sequence {Œ¥k} via (3.A.1c).
TLFeBOOK

114
SEQUENCES AND SERIES
Example 3.A.1
Suppose that we want to rotate a vector by an angle of Œ∏ =
20‚ó¶, and we decide that n = 4 gives sufÔ¨Åcient accuracy for the application at hand.
Via (3.A.1c) and Table 3.A.1, we have
Œ∏(0) = 0‚ó¶
Œ∏(1) = Œ∏0 = 45‚ó¶as Œ¥0 = +1 since Œ∏ ‚â•Œ∏(0) = 0‚ó¶
Œ∏(2) = Œ∏(1) + Œ¥1Œ∏1
= 45‚ó¶‚àí26.565‚ó¶as Œ¥1 = ‚àí1 since Œ∏ < Œ∏(1)
= 18.435‚ó¶
Œ∏(3) = Œ∏(2) + Œ¥2Œ∏2
= 18.435‚ó¶+ 14.036‚ó¶as Œ¥2 = +1 since Œ∏ ‚â•Œ∏(2)
= 32.471‚ó¶
Œ∏(4) = Œ∏(3) + Œ¥3Œ∏3
= 32.471‚ó¶‚àí7.1250‚ó¶as Œ¥3 = ‚àí1 since Œ∏ < Œ∏(3)
= 25.346‚ó¶
Œ∏(5) = Œ∏(4) + Œ¥4Œ∏4
= 25.346‚ó¶‚àí3.5763‚ó¶as Œ¥4 = ‚àí1 since Œ∏ < Œ∏(4)
= 21.770‚ó¶‚âàŒ∏
Via (3.A.4) and |Œ∏ ‚àíŒ∏(n+1)| ‚â§Œ∏n
|œµ5| = 1.770‚ó¶< Œ∏4 = 3.5763‚ó¶
so the error bound in Schelin‚Äôs theorem is actually somewhat conservative, at least
in this special case.
3.A.4
Computing Arctangents
The results in Section 3.A.3 can be modiÔ¨Åed to obtain a CORDIC algorithm for
computing an approximation to Œ∏ = tan‚àí1(y/x). The idea is to Ô¨Ånd the sequence
{Œ¥k|k = 0, 1, . . . , n ‚àí1, n} to rotate the vector [ x
y ]T = [ x0
y0 ]T to the
vector [xn
yn]T , where yn ‚âà0. More speciÔ¨Åcally, we would select Œ¥k so that
|yk+1| < |yk|.
Let ÀÜŒ∏ denote the approximation to Œ∏. The desired algorithm to compute ÀÜŒ∏ may
be expressed as Pascal-like pseudocode:
ÀÜŒ∏ := 0;
x0 := x; y0 := y;
TLFeBOOK

COORDINATE ROTATION DI GITAL COMPUTING (CORDIC)
115
for k := 0 to n do begin
if yk ‚â•0 then begin
Œ¥k := ‚àí1;
end
else begin
Œ¥k := +1;
end ;
ÀÜŒ∏ := ‚àíŒ¥kŒ∏k + ÀÜŒ∏;
xk+1 := xk ‚àíŒ¥k2‚àíkyk;
yk+1 := Œ¥k2‚àíkxk + yk;
end ;
In this pseudocode
 xk+1
yk+1

=

1
‚àíŒ¥k2‚àík
Œ¥k2‚àík
1
  xk
yk

,
and we see that for the manner in which sequence {Œ¥k} is constructed by the
pseudocode, the inequality |yk+1| < |yk| is satisÔ¨Åed. We choose n to achieve the
desired accuracy of our estimate ÀÜŒ∏ of Œ∏, speciÔ¨Åcally, |Œ∏ ‚àíÀÜŒ∏| ‚â§Œ∏n.
3.A.5
Final Remarks
As an exercise, the reader should modify the previous results to determine a
CORDIC method for computing cos Œ∏, and sin Œ∏. [Hint: Take a good look at
(3.A.13).]
The CORDIC philosophy can be extended to the computation of hyperbolic
trigonometric functions, logarithms9 and other functions [4, 7]. It can also perform
multiplication and division (see Table on p. 324 of Schelin [6]). As shown by Hu
and Naganathan [9], the rate of convergence of the CORDIC method can be accel-
erated by a method similar to the Booth algorithm (see pp. 287‚Äì289 of Hamacher
et al. [10]) for multiplication. However, this is at the expense of somewhat more
complicated hardware structures. A roundoff error analysis of the CORDIC method
has been performed by Hu [8]. We do not present these results in this book as they
are quite involved. Hu claims to have fairly tight bounds on the errors, however.
Fixed-point and Ô¨Çoating-point schemes are both analyzed. A tutorial presentation
of CORDIC-based VLSI architectures for digital signal processing applications
appears in Hu [11]. Other papers on the CORDIC method are those by Timmer-
mann et al. [12] and Lee and Lang [13] (which appeared in the IEEE Transactions
on Computers, ‚ÄúSpecial Issue on Computer Arithmetic‚Äù of August 1992). An alter-
native summary of the CORDIC method may be found in Hwang [14]. Many of
the ideas in Hu‚Äôs paper [11] are applicable in a gate-array technology environ-
ment. Applications include the computation of discrete transforms (e.g., the DFT),
digital Ô¨Åltering, adaptive Ô¨Åltering, Kalman Ô¨Åltering, the solution of special linear
9A clever alternative to the CORDIC approach for log calculations appears in Lo and Chen [15], and
a method of computing square roots without division appears in Mikami et al. [16].
TLFeBOOK

116
SEQUENCES AND SERIES
systems of equations (e.g., Toeplitz), deconvolution, and eigenvalue and singular
value decompositions.
APPENDIX 3.B
MATHEMATICAL INDUCTION
The basic idea of mathematical induction is as follows. Assume that we are given
a sequence of statements
S0, S1, . . . , Sn, . . .
and each Si is true or it is false. To prove that all of the statements are true (i.e.,
to prove that Sn is true for all n) by induction: (1) prove that Sn is true for n = 0,
and then (2) assume that Sn is true for any n = k and then show that Sn is true for
n = k + 1.
Example 3.B.1
Prove
Sn =
n

i=0
i2 = n(n + 1)(2n + 1)
6
,
n ‚â•0.
Proof
We will use induction, but note that there are other methods (e.g., via z
transforms). For n = 0, we obtain
S0 =
0

i=0
i2 = 0
and
n(n + 1)(2n + 1)
6
|n=0 = 0.
Thus, Sn is certainly true for n = 0.
Assume now that Sn is true for n = k so that
Sk =
k

i=0
i2 = k(k + 1)(2k + 1)
6
.
(3.B.1)
We have
Sk+1 =
k+1

i=0
i2 =
k

i=0
i2 + (k + 1)2 = Sk + (k + 1)2,
and so
Sk + (k + 1)2 = k(k + 1)(2k + 1)
6
+ (k + 1)2 = (k + 1)(k + 2)(2k + 3)
6
= n(n + 1)(2n + 1)
6
|n=k+1
where we have used (3.B.1).
TLFeBOOK

CATASTROPHIC CANCELLATION
117
Therefore, Sn is true for n = k + 1 if Sn is true for n = k. Therefore, Sn is true
for all n ‚â•0 by induction.
APPENDIX 3.C
CATASTROPHIC CANCELLATION
The phenomenon of catastrophic cancellation is illustrated in the following out-
put from a MATLAB implementation that ran on a Sun Microsystems Ultra 10
workstation using MATLAB version 6.0.0.88, release 12, in an attempt to compute
exp(‚àí20) using the Maclaurin series for exp(x) directly:
----------------------------------
term k
x^k/ k !
----------------------------------
0
1.000000000000000
1
-20.000000000000000
2
200.000000000000000
3
-1333.333333333333258
4
6666.666666666666970
5
-26666.666666666667879
6
88888.888888888890506
7
-253968.253968253964558
8
634920.634920634911396
9
-1410934.744268077658489
10
2821869.488536155316979
11
-5130671.797338464297354
12
8551119.662230772897601
13
-13155568.711124267429113
14
18793669.587320379912853
15
-25058226.116427175700665
16
31322782.645533967763186
17
-36850332.524157606065273
18
40944813.915730677545071
19
-43099804.121821768581867
20
43099804.121821768581867
21
-41047432.496973112225533
22
37315847.724521011114120
23
-32448563.238713916391134
24
27040469.365594934672117
25
-21632375.492475949227810
26
16640288.840366113930941
27
-12326139.881752677261829
28
8804385.629823340103030
29
-6071990.089533339254558
30
4047993.393022226169705
31
-2611608.640659500379115
32
1632255.400412187911570
33
-989245.697219507652335
34
581909.233658534009010
35
-332519.562090590829030
36
184733.090050328260986
37
-99855.724351528784609
38
52555.644395541465201
TLFeBOOK

118
SEQUENCES AND SERIES
39
-26951.612510534087050
40
13475.806255267041706
41
-6573.564026959533294
42
3130.268584266443668
43
-1455.938876402997266
44
661.790398364998850
45
-294.129065939999407
46
127.882202582608457
47
-54.417958545790832
48
22.674149394079514
49
-9.254754854726333
50
3.701901941890533
51
-1.451726251721777
52
0.558356250662222
53
-0.210700471948008
54
0.078037211832596
55
-0.028377167939126
56
0.010134702835402
57
-0.003556036082597
58
0.001226219338827
59
-0.000415667572484
60
0.000138555857495
61
-0.000045428149998
62
0.000014654241935
63
-0.000004652140297
64
0.000001453793843
65
-0.000000447321182
66
0.000000135551873
67
-0.000000040463246
68
0.000000011900955
69
-0.000000003449552
70
0.000000000985586
71
-0.000000000277630
72
0.000000000077119
73
-0.000000000021129
74
0.000000000005710
75
-0.000000000001523
76
0.000000000000401
77
-0.000000000000104
78
0.000000000000027
79
-0.000000000000007
80
0.000000000000002
81
-0.000000000000000
82
0.000000000000000
83
-0.000000000000000
84
0.000000000000000
85
-0.000000000000000
86
0.000000000000000
87
-0.000000000000000
88
0.000000000000000
exp(-20) from sum of the above terms = 0.000000004173637
True value of exp(-20)
= 0.000000002061154
TLFeBOOK

REFERENCES
119
REFERENCES
1. E. Kreyszig, Introductory Functional Analysis with Applications, Wiley, New York,
1978.
2. W. Rudin, Principles of Mathematical Analysis, 3rd ed., McGraw-Hill, New York, 1976.
3. J. E. Volder, ‚ÄúThe CORDIC Trigonometric Computing Technique,‚Äù IRE Trans. Electron.
Comput. EC-8, 330‚Äì334 (Sept. 1959).
4. J. S. Walther, ‚ÄúA UniÔ¨Åed Algorithm for Elementary Functions,‚Äù AFIPS Conf. Proc.,
Vol. 38, 1971 Spring Joint Computer Conf., 379‚Äì385 May 18‚Äì20, 1971.
5. G. L. Haviland and A. A. Tuszynski, ‚ÄúA CORDIC Arithmetic Processor Chip,‚Äù IEEE
Trans. Comput. C-29, 68‚Äì79 (Feb. 1980).
6. C. W. Schelin, ‚ÄúCalculator Function Approximation,‚Äù Am. Math. Monthly 90, 317‚Äì325
(May 1983).
7. J.-M. Muller, ‚ÄúDiscrete Basis and Computation of Elementary Functions,‚Äù IEEE Trans.
Comput. C-34, 857‚Äì862 (Sept. 1985).
8. Y. H. Hu, ‚ÄúThe Quantization Effects of the CORDIC Algorithm,‚Äù IEEE Trans. Signal
Process. 40, 834‚Äì844 (April 1992).
9. Y. H. Hu and S. Naganathan, ‚ÄúAn Angle Recoding Method for CORDIC Algorithm
Implementation,‚Äù IEEE Trans. Comput. 42, 99‚Äì102 (Jan. 1993).
10. V. C. Hamacher, Z. G. Vranesic, and S. G. Zaky, Computer Organization, 3rd ed.,
McGraw-Hill, New York, 1990.
11. Y. H. Hu, ‚ÄúCORDIC-Based VLSI Architectures for Digital Signal Processing,‚Äù IEEE
Signal Process. Mag. 9, 16‚Äì35 (July 1992).
12. D. Timmermann, H. Hahn, and B. J. Hosticka, ‚ÄúLow Latency Time CORDIC Algo-
rithms,‚Äù IEEE Trans. Comput. 41, 1010‚Äì1015 (Aug. 1992).
13. J. Lee and T. Lang, ‚ÄúConstant-Factor Redundant CORDIC for Angle Calculation and
Rotation,‚Äù IEEE Trans. Comput. 41, 1016‚Äì1025 (Aug. 1992).
14. K. Hwang, Computer Arithmetic: Principles, Architecture, and Design, Wiley, New
York, 1979.
15. H.-Y. Lo and J.-L. Chen, ‚ÄúA Hardwired Generalized Algorithm for Generating the Log-
arithm Base-k by Iteration,‚Äù IEEE Trans. Comput. C-36, 1363‚Äì1367 (Nov. 1987).
16. N. Mikami, M. Kobayashi, and Y. Yokoyama, ‚ÄúA New DSP-Oriented Algorithm for
Calculation of the Square Root Using a Nonlinear Digital Filter,‚Äù IEEE Trans. Signal
Process. 40, 1663‚Äì1669 (July 1992).
17. G. G. Walter, Wavelets and Other Orthogonal Systems with Applications, CRC Press,
Boca Raton, FL, 1994.
18. I. S. Gradshteyn and I. M. Ryzhik, in Table of Integrals, Series and Products, A. Jeffrey,
ed., 5th ed., Academic Press, San Diego, CA, 1994.
19. L. Bers, Calculus: Preliminary Edition, Vol. 2, Holt, Rinehart, Winston, New York, 1967.
20. A. Leon-Garcia, Probability and Random Processes for Electrical Engineering, 2nd ed.,
Addison-Wesley, Reading, MA, 1994.
21. G. E. Forsythe, M. A. Malcolm, and C. B. Moler, Computer Methods for Mathematical
Computations, Prentice-Hall, Englewood Cliffs, NJ, 1977.
22. M. R. Spiegel, Theory and Problems of Advanced Calculus (Schaum‚Äôs Outline Series).
Schaum (McGraw-Hill), New York, 1963.
TLFeBOOK

120
SEQUENCES AND SERIES
23. A. Papoulis, Signal Analysis, McGraw-Hill, New York, 1977.
24. E. Kreyszig, Advanced Engineering Mathematics, 4th ed., Wiley, New York, 1979.
25. W. D. Lakin and D. A. Sanchez, Topics in Ordinary Differential Equations, Dover Pub-
lications, New York, 1970.
PROBLEMS
3.1. Prove the following theorem: Every convergent sequence in a metric space
is a Cauchy sequence.
3.2. Let fn(x) = xn for n ‚àà{1, 2, 3, . . .} = N, and fn(x) ‚ààC[0, 1] for all n ‚ààN.
(a) What is f (x) = limn‚Üí‚àûfn(x) (for x ‚àà[0, 1])?
(b) Is f (x) ‚ààC[0, 1]?
3.3. Sequence (xn) is deÔ¨Åned to be xn = (n + 1)/(n + 2) for n ‚ààZ+. Clearly, if
X = [0, 1) ‚äÇR, then xn ‚ààX for all n ‚ààZ+. Assume the metric for metric
space X is d(x, y) = |x ‚àíy| (x, y ‚ààX).
(a) What is x = limn‚Üí‚àûxn?
(b) Is X a complete space?
(c) Prove that (xn) is Cauchy.
3.4. Recall Section 3.A.3 wherein the rotation operator was deÔ¨Åned [Eq. (3.A.6)].
(a) Find an expression for angle Œ∏ such that for y Ã∏= 0
 cos Œ∏
‚àísin Œ∏
sin Œ∏
cos Œ∏

#
$%
&
=G(Œ∏)
 x
y

=
 x‚Ä≤
0

,
where x‚Ä≤ is some arbitrary nonzero constant.
(b) Prove that G‚àí1(Œ∏) = GT (Œ∏) [i.e., the inverse of G(Œ∏) is given by its
transpose].
(c) Consider the matrix
A =
Ô£Æ
Ô£∞
4
2
0
1
4
1
0
2
4
Ô£π
Ô£ª.
Let 0n√óm denote an array (matrix) of zeros with n rows, and m columns.
Find G(Œ∏1), and G(Œ∏2) so that

1
01√ó2
02√ó1
G(Œ∏2)
  G(Œ∏1)
02√ó1
01√ó2
1

#
$%
&
=QT
A = R,
where R is an upper triangular matrix (deÔ¨Åned in Section 4.5, if you do
not recall what this is).
TLFeBOOK

PROBLEMS
121
(d) Find Q‚àíT (inverse of QT ).
(Comment: The procedure illustrated by this problem is important in various
applications such as solving least-squares approximations, and in Ô¨Ånding the
eigenvalues and eigenvectors of matrices.)
3.5. Review Appendix 3.A. Suppose that we wish to rotate a vector [xy]T ‚ààR2
through an angle Œ∏ = 25‚ó¶¬± 1‚ó¶. Find n, and the required delta sequence (Œ¥k)
to achieve this accuracy.
3.6. Review Appendix 3.A. A certain CORDIC routine has the following pseu-
docode description:
Input x and z (|z| ‚â§1);
y0 := 0; z0 := z;
for k := 0 to n ‚àí1 do begin
if zk < 0 then begin
Œ¥k := ‚àí1;
end
else begin
Œ¥k := +1;
end;
yk+1 := yk + Œ¥kx2‚àík;
zk+1 := zk ‚àíŒ¥k2‚àík;
end;
The algorithm‚Äôs output is yn. What is yn?
3.7. Suppose that
xn(t) =
t
n + t
for n ‚ààZ+, and t ‚àà(0, 1) ‚äÇR. Show that (xn(t)) is uniformly convergent
on S = (0, 1).
3.8. Suppose that un > 0, and also that ‚àû
n=0 un converges. Prove that 0‚àû
n=0(1 +
un) converges.
[Hint: Recall Lemma 2.1 (of Chapter 2).]
3.9. Prove that limn‚Üí‚àûxn = 0 if |x| < 1.
3.10. Prove that for a, b, x ‚ààR
1
1 + |a ‚àíb| ‚â•
1
1 + |a ‚àíx|
1
1 + |b ‚àíx|.
(Comment: This inequality often appears in the context of convergence proofs
for certain series expansions.)
TLFeBOOK

122
SEQUENCES AND SERIES
3.11. Consider the function
KN(x) =
2œÄ
N + 1
N

n=0
Dn(x)
[recall (3.24)]. Since Dn(x) is 2œÄ-periodic, KN(x) is also 2œÄ-periodic. We
may assume that x ‚àà[‚àíœÄ, œÄ].
(a) Prove that
KN(x) =
1
N + 1
1 ‚àícos(N + 1)x
1 ‚àícos x
.
[Hint: Consider N
n=0 sin

(n + 1
2)x

= Im
N
n=0 ej(n+ 1
2 )x
.]
(b) Prove that
KN(x) ‚â•0.
(c) Prove that
1
2œÄ
 œÄ
‚àíœÄ
KN(x) dx = 1.
[Comment: The partial sums of the complex Fourier series expansion of the
2œÄ-periodic function f (x) (again x ‚àà[‚àíœÄ, œÄ]) are given by
fN(x) =
N

n=‚àíN
fnejnx,
where fn =
1
2œÄ
" œÄ
‚àíœÄ f (x)e‚àíjnx dx. DeÔ¨Åne
œÉN(x) =
1
N + 1
N

n=0
fn(x).
It can be shown that
œÉN(x) = 1
2œÄ
 œÄ
‚àíœÄ
f (x ‚àít)KN(t) dt.
It is also possible to prove that œÉN(x) ‚Üíf (x) uniformly on [‚àíœÄ, œÄ] if f (x)
is continuous. This is often called Fej¬¥er‚Äôs theorem.]
3.12. Repeat the analysis of Example 3.12 for fL(0).
3.13. If f (t) ‚ààL2(0, 2œÄ) and f (t) = 
n‚ààZ fnejnt, show that
1
2œÄ
 2œÄ
0
|f (t)|2 dt =
‚àû

n=‚àí‚àû
|fn|2.
TLFeBOOK

PROBLEMS
123
This relates the energy/power of f (t) to the energy/power of its Fourier
series coefÔ¨Åcients.
[Comment: For example, if f (t) is one period of a 2œÄ-periodic signal, then
the power interpretation applies. In particular, suppose that f (t) = i(t) a
current waveform, and that this is the current into a resistor of resistance R;
then the average power delivered to R is
Pav = 1
2œÄ
 2œÄ
0
Ri2(t) dt.]
3.14. Use the result of Example 1.20 to prove that
‚àû

n=0
(‚àí1)n
2n + 1 = œÄ
4 .
3.15. Prove that
1
2œÄ
 œÄ
0
sin[(L + 1
2)t]
sin[ 1
2t]
dt = 1
2.
3.16. Use mathematical induction to prove that
(1.1)n ‚â•1 + 1
10n
for all n ‚ààN.
3.17. Use mathematical induction to prove that
(1 + h)n ‚â•1 + nh
for all n ‚ààN, with h ‚â•‚àí1. (This is Bernoulli‚Äôs inequality.)
3.18. Use mathematical induction to prove that 4n + 2 is a multiple of 6 for all
n ‚ààN.
3.19. Conjecture a formula for (n ‚ààN)
Sn =
n

k=1
1
k(k + 1),
and prove it using mathematical induction.
3.20. Suppose that f (x) = tan x. Use (3.55) to approximate f (x) for all x ‚àà
(‚àí1, 1). Find an upper bound on |e(x)|, where e(x) is the error of the
approximation.
TLFeBOOK

124
SEQUENCES AND SERIES
3.21. Given f (x) = x2 + 1, Ô¨Ånd all Œæ ‚àà(1, 2) such that
f (1)(Œæ) = f (2) ‚àíf (1)
2 ‚àí1
.
3.22. Use (3.65) to Ô¨Ånd an approximation to
‚àö
1 + x for x ‚àà( 1
4, 7
4). Find an upper
bound on the magnitude of e(x).
3.23. Using a pocket calculator, compute (0.97)1/3 for n = 3 using (3.82). Find
upper and lower bounds on the error en+1(x).
3.24. Using a pocket calculator, compute [1.05]1/4 using (3.82). Choose n = 2 (i.e.,
quadratic approximation). Estimate the error involved in doing this using the
error bound expression. Compare the bounds to the actual error.
3.25. Show that
n

k=0
 n
k
!
= 2n.
3.26. Show that
n
j
 n ‚àí1
j ‚àí1
!
=
 n
j
!
for j = 1, 2, . . . , n ‚àí1, n.
3.27. Show that for n ‚â•2
n

k=1
k
 n
k
!
= n2n‚àí1.
3.28. For
pn(k) =
 n
k
!
pk(1 ‚àíp)n‚àík, 0 ‚â§p ‚â§1,
where k = 0, 1, . . . , n, show that
pn(k + 1) =
(n ‚àík)p
(k + 1)(1 ‚àíp)pn(k).
[Comment: This recursive approach for Ô¨Ånding pn(k) extends the range of
n for which pn(k) may be computed before experiencing problems with
numerical errors.]
3.29. Identity (3.103) was conÔ¨Årmed using an argument associated with Fig. 3.7. A
somewhat different approach is the following. ConÔ¨Årm (3.103) by working
with

1
‚àö
2œÄ
 ‚àû
‚àí‚àû
e‚àíx2/2 dx
2
= 1
2œÄ
 ‚àû
‚àí‚àû
 ‚àû
‚àí‚àû
e‚àí(x2+y2)/2 dx dy.
TLFeBOOK

PROBLEMS
125
(Hint: Use the Cartesian to polar coordinate conversion x = r cos Œ∏, y =
r sin Œ∏.)
3.30. Find the Maclaurin (inÔ¨Ånite) series expansion for f (x) = sin‚àí1 x. What is
the radius of convergence? [Hint: Theorem 3.7 and Eq. (3.82) are use-
ful.]
3.31. From Eq. (3.85) for x > ‚àí1
loge(1 + x) =
n

k=1
(‚àí1)k‚àí1xk
k
+ r(x),
(3.P.1)
where
|r(x)| ‚â§
Ô£±
Ô£≤
Ô£≥
1
n+1xn+1,
x ‚â•0
|x|n+1
(1‚àí|x|)(n+1),
‚àí1 < x ‚â§0
(3.P.2)
[Eq. (3.86)]. For what range of values of x does the series
loge(1 + x) =
‚àû

k=1
(‚àí1)k‚àí1xk
k
converge? Explain using (3.P.2).
3.32. The following problems are easily worked with a pocket calculator.
(a) It can be shown that
sin(x) =
n

k=0
(‚àí1)k
(2k + 1)!x2k+1 + e2n+3(x),
(3.P.3)
where
|e2n+3(x)| ‚â§
1
(2n + 3)!|x|2n+3.
(3.P.4)
Use (3.P.4) to compute sin(x) to 3 decimal places of accuracy for x = 1.5
radians. How large does n need to be?
(b) Use the approximation
loge(1 + x) ‚âà
N

n=1
(‚àí1)n‚àí1xn
n
to compute loge(1.5) to three decimal places of accuracy. How large
should N be to achieve this level of accuracy?
TLFeBOOK

126
SEQUENCES AND SERIES
3.33. Assuming that

2
œÄ
 œÄ
0
sin x
x
dx ‚àí2
n

r=0
œÄ2r(‚àí1)r
(2r + 1)(2r + 1)!
 ‚â§
2œÄ2n+1
(2n + 2)(2n + 2)!, (3.P.5)
show that for suitable n
2
œÄ
 œÄ
0
sin x
x
dx > 1.17.
What is the smallest n needed? Justify Eq. (3.P.5).
3.34. Using integration by parts, Ô¨Ånd the asymptotic expansion of
c(x) =
 ‚àû
x
cos t2 dt.
3.35. Using integration by parts, Ô¨Ånd the asymptotic expansion of
s(x) =
 ‚àû
x
sin t2 dt.
3.36. Use MATLAB to plot (on the same graph) function KN(x) in Problem 3.11
for N = 2, 4, 15.
TLFeBOOK

4
Linear Systems of Equations
4.1
INTRODUCTION
The necessity to solve linear systems of equations is commonplace in numerical
computing. This chapter considers a few examples of how such problems arise
(more examples will be seen in subsequent chapters) and the numerical problems
that are frequently associated with attempts to solve them. We are particularly inter-
ested in the phenomenon of ill conditioning. We will largely concentrate on how
the problem arises, what its effects are, and how to test for this problem.1 In addi-
tion to this, we will also consider methods of solving linear systems other than the
Gaussian elimination method that you most likely learned in an elementary linear
algebra course.2 More speciÔ¨Åcally, we consider LU and QR matrix factorization
methods, and iterative methods of linear system solution. The concept of a singular
value decomposition (SVD) is also introduced.
We will often employ the term ‚Äúlinear systems‚Äù instead of the longer phrase
‚Äúlinear systems of equations.‚Äù However, the reader must be warned that the phrase
‚Äúlinear systems‚Äù can have a different meaning from our present usage. In signals
and systems courses you will most likely see that a ‚Äúlinear system‚Äù is either a
continuous-time (i.e., analog) or discrete-time (i.e., digital) dynamic system whose
input/output (I/O) behavior satisÔ¨Åes superposition. However, such dynamic systems
can be described in terms of linear systems of equations.
4.2
LEAST-SQUARES APPROXIMATION AND LINEAR SYSTEMS
Suppose that f (x), g(x) ‚ààL2[0, 1], and that these functions are real-valued. Recall-
ing Chapter 1, their inner product is therefore given by
‚ü®f, g‚ü©=
 1
0
f (x)g(x) dx.
(4.1)
1Methods employed to avoid ill-conditioned linear systems of equations will be mainly considered in
a later chapter. These chieÔ¨Çy involve working with orthogonal basis sets.
2Review the Gaussian elimination procedure now if necessary. It is mandatory that you recall the basic
matrix and vector operations and properties [(AB)T = BT AT , etc.] from elementary linear algebra,
too.
An Introduction to Numerical Analysis for Electrical and Computer Engineers, by C.J. Zarowski
ISBN 0-471-46737-5
c‚Éù2004 John Wiley & Sons, Inc.
127
TLFeBOOK

128
LINEAR SYSTEMS OF EQUATIONS
Note that here we will assume that all members of L2[0, 1] are real-valued (for
simplicity). Now assume that {œÜn(x)|n ‚ààZN} form a linearly independent set such
that œÜn(x) ‚ààL2[0, 1] for all n. We wish to Ô¨Ånd the coefÔ¨Åcients an such that
f (x) ‚âà
N‚àí1

n=0
anœÜn(x)
for
x ‚àà[0, 1].
(4.2)
A popular approach to Ô¨Ånding an [1] is to choose them to minimize the functional
V (a) = ||f (x) ‚àí
N‚àí1

n=0
anœÜn(x)||2,
(4.3)
where, for future convenience, we will treat an as belonging to the vector a =
[a0a1 ¬∑ ¬∑ ¬∑ aN‚àí2aN‚àí1]T ‚ààRN. Of course, in (4.3) ||f ||2 = ‚ü®f, f ‚ü©via (4.1). We are
at liberty to think of
e(x) = f (x) ‚àí
N‚àí1

n=0
anœÜn(x)
(4.4)
as the error between f (x) and its approximation 
n anœÜn(x). So, our goal is to
pick a to minimize ||e(x)||2, which, we have previously seen, may be interpreted
as the energy of the error e(x). This methodology of approximation is called least-
squares approximation. The version of this that we are now considering is only
one of a great many variations. We will see others later.
We may rewrite (4.3) as follows:
V (a) = ||f (x) ‚àí
N‚àí1

n=0
anœÜn(x)||2
=

f ‚àí
N‚àí1

n=0
anœÜn, f ‚àí
N‚àí1

k=0
akœÜk

= ‚ü®f, f ‚ü©‚àí

f,
N‚àí1

k=0
akœÜk

‚àí
N‚àí1

n=0
anœÜn, f

+
N‚àí1

n=0
anœÜn,
N‚àí1

k=0
akœÜk

= ||f ||2 ‚àí
N‚àí1

k=0
ak‚ü®f, œÜk‚ü©‚àí
N‚àí1

n=0
an‚ü®œÜn, f ‚ü©
+
N‚àí1

n=0
N‚àí1

k=0
akan‚ü®œÜk, œÜn‚ü©
TLFeBOOK

LEAST-SQUARES APPROXIMATION AND LINEAR SYSTEMS
129
= ||f ||2 ‚àí2
N‚àí1

k=0
ak‚ü®f, œÜk‚ü©
+
N‚àí1

n=0
N‚àí1

k=0
akan‚ü®œÜk, œÜn‚ü©.
(4.5)
Naturally, we have made much use of the inner product properties of Chapter 1.
It is very useful to deÔ¨Åne
œÅ = ||f ||2,
gk = ‚ü®f, œÜk‚ü©,
rn,k = ‚ü®œÜn, œÜk‚ü©,
(4.6)
along with the vector
g = [g0g1 ¬∑ ¬∑ ¬∑ gN‚àí1]T ‚ààRN,
(4.7a)
and the matrix
R = [rn,k] ‚ààRN√óN.
(4.7b)
We immediately observe that R = RT (i.e., R is symmetric). This is by virtue of
the fact that rn,k = ‚ü®œÜn, œÜk‚ü©= ‚ü®œÜk, œÜn‚ü©= rk,n. Immediately we may rewrite (4.5)
in order to obtain the quadratic form3
V (a) = aT Ra ‚àí2aT g + œÅ.
(4.8)
The quadratic form occurs very widely in optimization and approximation prob-
lems, and so warrants considerable study. An expanded view of R is
R =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
 1
0
œÜ2
0(x) dx
 1
0
œÜ0(x)œÜ1(x) dx
¬∑ ¬∑ ¬∑
 1
0
œÜ0(x)œÜN‚àí1(x) dx
 1
0
œÜ0(x)œÜ1(x) dx
 1
0
œÜ2
1(x) dx
¬∑ ¬∑ ¬∑
 1
1
œÜ1(x)œÜN‚àí1(x) dx
...
...
...
 1
0
œÜ0(x)œÜN‚àí1(x) dx
 1
0
œÜ1(x)œÜN‚àí1(x) dx
¬∑ ¬∑ ¬∑
 1
0
œÜ2
N‚àí1(x) dx
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
(4.9)
Note that the reader must get used to visualizing matrices and vectors in the general
manner now being employed. The practical use of linear/matrix algebra demands
this. Writing programs for anything involving matrix methods (which encompasses
a great deal) is almost impossible without this ability. Moreover, modern software
tools (e.g., MATLAB) assume that the user is skilled in this manner.
3The quadratic form is a generalization of the familiar quadratic ax2 + bx + c, for which x ‚ààC (if
we are interested in the roots of a quadratic equation; otherwise we usually consider x ‚ààR) , and also
a, b, c ‚ààR.
TLFeBOOK

130
LINEAR SYSTEMS OF EQUATIONS
It is essential to us that R‚àí1 exist (i.e., we need R to be nonsingular). Fortu-
nately, this is always the case because {œÜn|n ‚ààZN} is an independent set. We shall
prove this. If R is singular, then there is a set of coefÔ¨Åcients Œ±i such that
N‚àí1

i=0
Œ±i‚ü®œÜi, œÜj‚ü©= 0
(4.10)
for j ‚ààZN. This is equivalent to saying that the columns of R are linearly depen-
dent. Now consider the function
ÀÜf (x) =
N‚àí1

i=0
Œ±iœÜi(x)
so if (4.10) holds for all j ‚ààZN, then
‚ü®ÀÜf , œÜj‚ü©=
N‚àí1

i=0
Œ±iœÜi, œÜj

=
N‚àí1

i=0
Œ±i‚ü®œÜi, œÜj‚ü©= 0
for all j ‚ààZN. Thus
N‚àí1

j=0
Œ±j‚ü®ÀÜf , œÜj‚ü©=
N‚àí1

j=0
Œ±j
N‚àí1

i=0
Œ±i‚ü®œÜi, œÜj‚ü©
	
= 0,
implying that
N‚àí1

i=0
Œ±iœÜi,
N‚àí1

j=0
Œ±jœÜj

= 0,
or in other words, ‚ü®ÀÜf , ÀÜf ‚ü©= || ÀÜf ||2 = 0, and so
ÀÜf (x) = 0 for all x ‚àà[0, 1]. This
contradicts the assumption that {œÜn|n ‚ààZN} is a linearly independent set. So R‚àí1
must exist.
From (4.3) it is clear that (via basic norm properties) V (a) ‚â•0 for all a ‚ààRN.
If we now assume that f (x) = 0 (all x ‚àà[0, 1]), we have œÅ = 0, and g = 0 too.
Thus, aT Ra ‚â•0 for all a.
DeÔ¨Ånition 4.1: Positive SemideÔ¨Ånite Matrix, Positive DeÔ¨Ånite Matrix
Sup-
pose that A = AT and that A ‚ààRn√ón. Suppose that x ‚ààRn. We say that A is
positive semideÔ¨Ånite (psd) iff
xT Ax ‚â•0
for all x. We say that A is positive deÔ¨Ånite (pd) iff
xT Ax > 0
for all x Ã∏= 0.
TLFeBOOK

LEAST-SQUARES APPROXIMATION AND LINEAR SYSTEMS
131
If A is psd, we often symbolize this by writing A ‚â•0, and if A is pd, we often
symbolize this by writing A > 0. If a matrix is pd then it is clearly psd, but the
converse is not necessarily true.
So far it is clear that R ‚â•0. But in fact R > 0. This follows from the linear
independence of the columns of R. If the columns of R were linearly dependent,
then there would be an a Ã∏= 0 such that Ra = 0, but we have already shown that
R‚àí1 exists, so it must be so that Ra = 0 iff a = 0. Immediately we conclude that
R is positive deÔ¨Ånite.
Why is R > 0 so important? Recall that we may solve ax2 + bx + c = 0 (x ‚ààC)
by completing the square:
ax2 + bx + c = a

x + b
2a
2
+ c ‚àíb2
4a .
(4.11)
Now if a > 0, then y(x) = ax2 + bx + c has a unique minimum. Since y(1)(x) =
2ax + b = 0 for x = ‚àíb
2a , this choice of x forces the Ô¨Årst term of (4.11) (right-hand
side of the equality) to zero, and we see that the minimum value of y(x) is
y
 
‚àíb
2a
!
= c ‚àíb2
4a = 4ac ‚àíb2
4a
.
(4.12)
Thus, completing the square makes the location of the minimum (if it exists), and
the value of the minimum of a quadratic very obvious. For the same purpose we
may complete the square of (4.8):
V (a) = [a ‚àíR‚àí1g]T R[a ‚àíR‚àí1g] + œÅ ‚àígT R‚àí1g.
(4.13)
It is quite easy to conÔ¨Årm that (4.13) gives (4.8) (so these equations must be
equivalent):
[a ‚àíR‚àí1g]T R[a ‚àíR‚àí1g] + œÅ ‚àígT R‚àí1g
= [aT ‚àígT (R‚àí1)T ][Ra ‚àíg] + œÅ ‚àígT R‚àí1g
= aT Ra ‚àígT R‚àí1Ra ‚àíaT g + gT R‚àí1g + œÅ ‚àígT R‚àí1g
= aT Ra ‚àígT a ‚àíaT g + œÅ = aT Ra ‚àí2aT g + œÅ.
We have used (R‚àí1)T = (RT )‚àí1 = R‚àí1, and the fact that aT g = gT a. If vector
x = a ‚àíR‚àí1g, then
[a ‚àíR‚àí1g]T R[a ‚àíR‚àí1g] = xT Rx.
So, because R > 0, it follows that xT Rx > 0 for all x Ã∏= 0. The last two terms of
(4.13) do not depend on a. So we can minimize V (a) only by minimizing the Ô¨Årst
term. R > 0 implies that this minimum must be for x = 0, implying a = ÀÜa, where
ÀÜa ‚àíR‚àí1g = 0,
or
R ÀÜa = g.
(4.14)
TLFeBOOK

132
LINEAR SYSTEMS OF EQUATIONS
Thus
ÀÜa = argmin
a‚ààRN
V (a).
(4.15)
We see that to minimize ||e(x)||2 we must solve a linear system of equations,
namely, Eq. (4.14). We remark that for R > 0, the minimum of V (a) is at a unique
location ÀÜa ‚ààRN; that is, the minimum is unique.
In principle, solving least-squares approximation problems seems quite simple
because we have systematic (and numerically reliable) methods to solve (4.14)
(e.g., Gaussian elimination with partial pivoting). However, one apparent difÔ¨Åculty
is the need to determine various integrals:
gk =
 1
0
f (x)œÜk(x) dx, rn,k =
 1
0
œÜn(x)œÜk(x) dx.
Usually, the independent set {œÜk|k ‚ààZN} is chosen to make Ô¨Ånding rn,k rela-
tively straightforward. In fact, sometimes nice closed-form expressions exist. But
numerical integration is generally needed to Ô¨Ånd gk. Practically, this could involve
applying series expansions such as considered in Chapter 3, or perhaps using
quadratures such as will be considered in a later chapter. Other than this, there
is a more serious problem. This is the problem that R might be ill-conditioned.
4.3
LEAST-SQUARES APPROXIMATION AND ILL-CONDITIONED
LINEAR SYSTEMS
A popular choice for an independent set {œÜk(x)} would be
œÜk(x) = xk
for
x ‚àà[0, 1], k ‚ààZN.
(4.16)
Certainly, these functions belong to the inner product space L2[0, 1]. Thus, for
f (x) ‚ààL2[0, 1] an approximation to it is
ÀÜf (x) =
N‚àí1

k=0
akxk ‚ààPN‚àí1[0, 1],
(4.17)
and so we wish to Ô¨Åt a degree N ‚àí1 polynomial to f (x). Consequently
gk =
 1
0
xkf (x) dx
(4.18a)
which is sometimes called the kth moment4 of f (x) on [0, 1], and also
rn,k =
 1
0
œÜn(x)œÜk(x) dx =
 1
0
xn+k dx =
1
n + k + 1
(4.18b)
for n, k ‚ààZN.
4The concept of a moment is also central to probability theory.
TLFeBOOK

LEAST-SQUARES APPROXIMATION AND ILL-CONDITIONED LINEAR SYSTEMS
133
For example, suppose that N = 3 (i.e., a quadratic Ô¨Åt); then
g =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
 1
0
f (x) dx
#
$%
&
=g0
 1
0
xf (x) dx
#
$%
&
=g1
 1
0
x2f (x) dx
#
$%
&
=g2
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª
T
,
(4.19a)
and
R =
Ô£Æ
Ô£ØÔ£ØÔ£∞
1
1
2
1
3
1
2
1
3
1
4
1
3
1
4
1
5
Ô£π
Ô£∫Ô£∫Ô£ª=
Ô£Æ
Ô£∞
r00
r01
r02
r10
r11
r12
r20
r21
r22
Ô£π
Ô£ª,
(4.19b)
and ÀÜa = [ÀÜa0 ÀÜa1 ÀÜa2]T , so we wish to solve
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
1
1
2
1
3
1
2
1
3
1
4
1
3
1
4
1
5
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
Ô£Æ
Ô£∞
ÀÜa0
ÀÜa1
ÀÜa2
Ô£π
Ô£ª=
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
 1
0
f (x) dx
 1
0
xf (x) dx
 1
0
x2f (x) dx
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
(4.20)
We remark that R does not depend on the ‚Äúdata‚Äù f (x), only the elements of g
do. This is true in general, and it can be used to advantage. SpeciÔ¨Åcally, if f (x)
changes frequently (i.e., we must work with different data), but the independent
set does not change, then we need to invert R only once.
Matrix R in (4.19b) is a special case of the famous Hilbert matrix [2‚Äì4]. The
general form of this matrix is (for any N ‚ààN)
R =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
1
1
2
1
3
¬∑ ¬∑ ¬∑
1
N
1
2
1
3
1
4
¬∑ ¬∑ ¬∑
1
N + 1
1
3
1
4
1
5
¬∑ ¬∑ ¬∑
1
N + 2
...
...
...
...
1
N
1
N + 1
1
N + 2
¬∑ ¬∑ ¬∑
1
2N ‚àí1
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
‚ààRN√óN.
(4.21)
Thus, (4.20) is a special case of a Hilbert linear system of equations. The matrix R
in (4.21) seems ‚Äúharmless,‚Äù but it is actually a menace from a numerical computing
standpoint. We now demonstrate this concept.
TLFeBOOK

134
LINEAR SYSTEMS OF EQUATIONS
Suppose that our data are something very simple. Say that
f (x) = 1
for all
x ‚àà[0, 1].
In this case gk =
" 1
0 xk dx =
1
k+1. Therefore, for any N ‚ààN, we are compelled to
solve
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
1
1
2
1
3
¬∑ ¬∑ ¬∑
1
N
1
2
1
3
1
4
¬∑ ¬∑ ¬∑
1
N + 1
1
3
1
4
1
5
¬∑ ¬∑ ¬∑
1
N + 2
...
...
...
...
1
N
1
N + 1
1
N + 2
¬∑ ¬∑ ¬∑
1
2N ‚àí1
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
ÀÜa0
ÀÜa1
ÀÜa2
...
ÀÜaN‚àí1
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
=
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
1
1
2
1
3
...
1
N
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
(4.22)
A moment of thought reveals that solving (4.22) is trivial because g is the Ô¨Årst
column of R. Immediately, we see that
ÀÜa = [100 ¬∑ ¬∑ ¬∑ 00]T .
(4.23)
(No other solution is possible since R‚àí1 exists, implying that R ÀÜa = g always
possesses a unique solution.)
MATLAB implements Gaussian elimination (with partial pivoting) using the
operator ‚Äú\‚Äù to solve linear systems. For example, if we want x in Ax = y for
which A‚àí1 exists, then x = A\y. MATLAB also computes R using function ‚Äúhilb‚Äù;
that is, R = hilb(N) will result in R being set to a N √ó N Hilbert matrix. Using the
MATLAB ‚Äú\‚Äù operator to solve for ÀÜa in (4.22) gives the expected answer (4.23) for
N ‚â§50 (at least). The computer-generated answers are correct to several decimal
places. (Note that it is somewhat unusual to want to Ô¨Åt polynomials to data that
are of such large degree.) So far, so good.
Now consider the results in Appendix 4.A. The MATLAB function ‚Äúinv‚Äù may
be used to compute the inverse of matrices. The appendix shows R‚àí1 (computed
via inv) for N = 10, 11, 12, and the MATLAB computed product RR‚àí1 for these
cases. Of course, RR‚àí1 = I (identity matrix) is expected in all cases. For the
number of decimal places shown, we observe that RR‚àí1 Ã∏= I. Not only that, but
the error E = RR‚àí1 ‚àíI rapidly becomes large with an increase in N. For N = 12,
the error is substantial. In fact, the MATLAB function inv has built-in features to
warn of trouble, and it does so for case N = 12. Since RR‚àí1 is not being computed
correctly, something has clearly gone wrong, and this has happened for rather small
values of N. This is in striking contrast with the previous problem, where we wanted
to compute ÀÜa in (4.22). In this case, apparently, nothing went wrong.
We may consider changing our data to f (x) = xN‚àí1. In this case gk = 1/(N +
k) for k ‚ààZN. The vector g in this case will be the last column of R. Thus,
TLFeBOOK

CONDITION NUMBERS
135
mathematically, ÀÜa = [00 ¬∑ ¬∑ ¬∑ 001]T . If we use MATLAB ‚Äú\‚Äù to compute ÀÜa for this
problem we obtain the computed solutions:
ÀÜÀÜa = [0.0000 0.0000 0.0000 0.0000 . . .
0.0002 ‚àí0.0006 0.0013 ‚àí0.00170.0014 ‚àí0.0007 1.0001]T
(N = 11),
ÀÜÀÜa = [0.0000 0.0000 0.0000 ‚àí0.0002 . . . 0.0015 ‚àí0.0067
0.0187 ‚àí0.0342 0.0403 ‚àí0.0297 0.0124 0.9978]T (N = 12).
The errors in the computed solutions ÀÜÀÜa here are much greater than those experienced
in computing ÀÜa in (4.23).
It turns out that the Hilbert matrix R is a classical example of an ill-conditioned
matrix (with respect to the problem of solving linear systems of equations). The
linear system in which it resides [i.e., Eq. (4.14)] is therefore an ill-conditioned
linear system. In such systems the Ô¨Ånal answer (which is ÀÜa here) can be exquisitely
sensitive to very small perturbations (i.e., disturbances) in the inputs. The inputs
in this case are the elements of R and g. From Chapter 2 we remember that R
and g will not have an exact representation on the computer because of quantiza-
tion errors. Additionally, as the computation proceeds rounding errors will cause
further disturbances. The result is that in the end the Ô¨Ånal computed solution can
deviate enormously from the correct mathematical solution. On the other hand, we
have also shown that it is possible for the computed solution to be very close to
the mathematically correct solution even in the presence of ill conditioning. Our
problem then is to be able to detect when ill conditioning arises, and hence might
pose a problem.
4.4
CONDITION NUMBERS
In the previous section there appears to be a problem involved in accurately com-
puting the inverse of R (Hilbert matrix). This was attributed to the so-called ill
conditioning of R. We begin here with some simpler lower-order examples that
illustrate how the solution to a linear system Ax = y can depend sensitively on A
and y. This will lead us to develop a theory of condition numbers that warn us that
the solution x might be inaccurately computed due to this sensitivity.
We will consider Ax = y on the assumption that A ‚ààRn√ón, and x, y ‚ààRn.
Initially we will assume n = 2, so
 a00
a01
a10
a11
  x0
x1

=
 y0
y1

.
(4.24)
In practice, we may be uncertain about the accuracy of the entries of A and y.
Perhaps these entities originate from experimental data. So the entries may be
subject to experimental errors. Additionally, as previously mentioned, the elements
TLFeBOOK

136
LINEAR SYSTEMS OF EQUATIONS
of A and y cannot normally be exactly represented on a computer because of the
need to quantize their entries. Thus, we must consider the perturbed system
Ô£±
Ô£¥Ô£¥Ô£¥Ô£≤
Ô£¥Ô£¥Ô£¥Ô£≥
 a00
a01
a10
a11

+
 Œ¥a00
Œ¥a01
Œ¥a10
Œ¥a11

#
$%
&
=Œ¥A
Ô£º
Ô£¥Ô£¥Ô£¥Ô£Ω
Ô£¥Ô£¥Ô£¥Ô£æ
 ÀÜx0
ÀÜx1

# $% &
=ÀÜx
=
Ô£±
Ô£¥Ô£¥Ô£¥Ô£≤
Ô£¥Ô£¥Ô£¥Ô£≥
 y0
y1

+
 Œ¥y0
Œ¥y1

#
$%
&
=Œ¥y
Ô£º
Ô£¥Ô£¥Ô£¥Ô£Ω
Ô£¥Ô£¥Ô£¥Ô£æ
.
(4.25)
The perturbations are Œ¥A and Œ¥y. We will assume that these are ‚Äúsmall.‚Äù As you
might expect, the practical deÔ¨Ånition of ‚Äúsmall‚Äù will force us to deÔ¨Åne and work
with suitable norms. This is dealt with below. We further assume that the computing
machine we use to solve (4.25) is a ‚Äúmagical machine‚Äù that computes without
rounding errors. Thus, any errors in the computed solution, denoted ÀÜx here, can be
due only to the perturbations Œ¥A and Œ¥y. It is our hope that ÀÜx ‚âàx. Unfortunately,
this will not always be so, even for n = 2 with small perturbations.
Because n is small, we may obtain closed-form expressions for A‚àí1, x, [A +
Œ¥A]‚àí1, and ÀÜx. More speciÔ¨Åcally
A‚àí1 =
1
a00a11 ‚àía01a10

a11
‚àía01
‚àía10
a00

,
(4.26)
and
[A + Œ¥A]‚àí1 =
1
(a00 + Œ¥a00)(a11 + Œ¥a11) ‚àí(a01 + Œ¥a01)(a10 + Œ¥a10)
√ó

a11 + Œ¥a11
‚àí(a01 + Œ¥a01)
‚àí(a10 + Œ¥a10)
(a00 + Œ¥a00)

.
(4.27)
The reader can conÔ¨Årm these by multiplying A‚àí1 as given in (4.26) by A. The
2 √ó 2 identity matrix should be obtained. Using these formulas, we may consider
the following example.
Example 4.1
Suppose that
A =
 1
‚àí.01
2
.01

,
y =
 2
1

.
Nominally, the correct solution is
x =

1
‚àí100

.
Let us consider different perturbation cases:
TLFeBOOK

CONDITION NUMBERS
137
1. Suppose that
Œ¥A =
 0
0
0
.005

,
Œ¥y = [0
0]T .
In this case
ÀÜx = [1.1429
‚àí85.7143]T .
2. Suppose that
Œ¥A =
 0
0
0
‚àí.03

,
Œ¥y = [0
0]T .
for which
A + Œ¥A =
 1
‚àí.01
2
‚àí.02

.
This matrix is mathematically singular, so it does not possess an inverse. If
MATLAB tries to compute ÀÜx using (4.27), then we obtain
ÀÜx = 1.0 √ó 1017 √ó [‚àí0.0865
‚àí8.6469]T
and MATLAB issues a warning that the answer may not be correct. Obvi-
ously, this is truly a nonsense answer.
3. Suppose that
Œ¥A =
 0
0
0
‚àí.02

, Œ¥y = [0.10
‚àí0.05]T .
In this case
ÀÜx = [‚àí1.1500
‚àí325.0000]T .
It is clear that small perturbations of A and y can lead to large errors in the
computed value for x. These errors are not a result of accumulated rounding errors
in the computational algorithm for solving the problem. For computations on a
‚Äúnonmagical‚Äù (i.e., ‚Äúreal‚Äù) computer, this should be at least intuitively plausible
since our formulas for ÀÜx are very simple in the sense of creating little opportunity
for rounding error to grow (there are very few arithmetical operations involved).
Thus, the errors x ‚àíÀÜx must be due entirely (or nearly so) to uncertainties in the
original inputs. We conclude that the real problem is that the linear system we
are solving is too sensitive to perturbations in the inputs. This naturally raises the
question of how we may detect such sensitivity.
In view of this, we shall say that a matrix A is ill-conditioned if the solution x
(in Ax = y) is very sensitive to perturbations on A and y. Otherwise, the matrix
is said to be well-conditioned.
TLFeBOOK

138
LINEAR SYSTEMS OF EQUATIONS
We will need to introduce appropriate norms in order to objectively measure
the sizes of objects in our problem. However, before doing this we make a few
observations that give additional insight into the nature of the problem. In Example
4.1 we note that the Ô¨Årst column of A is big (in some sense), while the second
column is small. The smallness of the second column makes A close to being
singular. A similar observation may be made about Hilbert matrices. For a general
N √ó N Hilbert matrix, the last two columns are given by
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
1
N ‚àí1
1
N
1
N
1
N + 1
...
...
1
2N ‚àí2
1
2N ‚àí1
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
For very large N, it is apparent that these two columns are almost linearly depen-
dent; that is, one may be taken as close to being equal to the other. A simple
numerical example is that
1
100 ‚âà
1
101. Thus, at least at the outset, it seems that
ill-conditioned matrices are close to being singular, and that this is the root cause
of the sensitivity problem.
We now need to extend our treatment of the concept of norms from what we
have seen in earlier chapters. Our main source is Golub and Van Loan [5], but
similar information is to be found in Ref. 3 or 4 (or the references cited therein).
A fairly rigorous treatment of matrix and vector norms can be found in Horn and
Johnson [6].
Suppose again that x ‚ààRn. The p-norm of x is deÔ¨Åned to be
||x||p =
n‚àí1

k=0
|xk|p
	1/p
,
(4.28)
where p ‚â•1. The most important special cases are, respectively, the 1-norm, 2-
norm, and ‚àû-norm:
||x||1 =
n‚àí1

k=0
|xk|,
(4.29a)
||x||2 = [xT x]1/2,
(4.29b)
||x||‚àû=
max
0‚â§k‚â§n‚àí1 |xk|.
(4.29c)
The operation ‚Äúmax‚Äù means to select the biggest |xk|. A unit vector with respect
to norm || ¬∑ || is a vector x such that ||x|| = 1. Note that if x is a unit vector with
TLFeBOOK

CONDITION NUMBERS
139
respect to one norm, then it is not necessarily a unit vector with respect to another
choice of norm. For example, suppose that x = [
‚àö
3
2
1
2]T ; then
||x||2 = 1, ||x||1 =
‚àö
3 + 1
2
, ||x||‚àû=
‚àö
3
2 .
The vector x is a unit vector under the 2-norm, but is not a unit vector under the
1-norm or the ‚àû-norm.
Norms have various properties that we will list without proof. Assume that
x, y ‚ààRn. For example, the H¬®older inequality [recall (1.16) (in Chapter 1) for
comparison] is
|xT y| ‚â§||x||p||y||q
(4.30)
for which 1
p + 1
q = 1. A special case is the Cauchy‚ÄìSchwarz inequality
|xT y| ‚â§||x||2||y||2,
(4.31)
which is a special instance of Theorem 1.1 (in Chapter 1). An important feature of
norms is that they are equivalent. This means that if || ¬∑ ||Œ± and || ¬∑ ||Œ≤ are norms
on Rn, then there are c1, c2 > 0 such that
c1||x||Œ± ‚â§||x||Œ≤ ‚â§c2||x||Œ±
(4.32)
for all x ‚ààRn. Some special instances of this are
||x||2 ‚â§||x||1 ‚â§‚àön||x||2,
(4.33a)
||x||‚àû‚â§||x||2 ‚â§‚àön||x||‚àû,
(4.33b)
||x||‚àû‚â§||x||1 ‚â§n||x||‚àû.
(4.33c)
Equivalence is signiÔ¨Åcant with respect to our problem in the following manner.
When we deÔ¨Åne condition numbers below, we shall see that the speciÔ¨Åc value of
the condition number depends in part on the choice of norm. However, equivalence
says that if a matrix is ill-conditioned with respect to one type of norm, then it
must be ill-conditioned with respect to any other type of norm. This can simplify
analysis in practice because it allows us to compute the condition number using
whatever norms are the easiest to work with. Equivalence can be useful in another
respect. If we have a sequence of vectors in the space Rn, then, if the sequence is
Cauchy with respect to some chosen norm, it must be Cauchy with respect to any
other choice of norm. This can simplify convergence analysis, again because we
may pick the norm that is easiest to work with.
In Chapter 2 we considered absolute and relative error in the execution of
Ô¨Çoating-point operations. In this setting, operations were on scalars, and scalar
solutions were generated. Now we must redeÔ¨Åne absolute and relative error for
vector quantities using the norms deÔ¨Åned in the previous paragraph. Since ÀÜx ‚ààRn
TLFeBOOK

140
LINEAR SYSTEMS OF EQUATIONS
is the computed (i.e., approximate) solution to x ‚ààRn it is reasonable to deÔ¨Åne the
absolute error to be
œµa = ||ÀÜx ‚àíx||,
(4.34a)
and the relative error is
œµr = ||ÀÜx ‚àíx||
||x||
.
(4.34b)
Of course, x Ã∏= 0 is assumed here. The choice of norm is in principle arbitrary.
However, if we use the ‚àû-norm, then the concept of relative error with respect to
it can be made equivalent to a statement about the correct number of signiÔ¨Åcant
digits in ÀÜx:
||ÀÜx ‚àíx||‚àû
||x||‚àû
‚âà10‚àíd.
(4.35)
In other words, the largest element of the computed solution ÀÜx is correct to approx-
imately d decimal digits. For example, suppose that x = [1.256 ‚àí2.554]T , and
ÀÜx = [1.251 ‚àí2.887]T ; then ÀÜx ‚àíx = [‚àí0.005 ‚àí0.333]T , and so
||ÀÜx ‚àíx||‚àû= 0.333,
||x||‚àû= 2.554,
so therefore œµr = 0.1304 ‚âà10‚àí1. Thus, ÀÜx has a largest element that is accurate to
about one decimal digit, but the smallest element is observed to be correct to about
three signiÔ¨Åcant digits.
Matrices can have norms deÔ¨Åned on them. We have remarked that ill condi-
tioning seems to arise when a matrix is close to singular. Suitable matrix norms
can allow us to measure how close a matrix is to being singular, and thus gives
insight into its condition. Suppose that A, B ‚ààRm√ón (so A and B are not neces-
sarily square matrices). || ¬∑ |||Rm√ón ‚ÜíR is a matrix norm, provided the following
axioms hold:
(MN1)
||A|| ‚â•0 for all A, and ||A|| = 0 iff A = 0.
(MN2)
||A + B|| ‚â§||A|| + ||B||.
(MN3)
||Œ±A|| = |Œ±| ||A||. Constant Œ± is from the same Ô¨Åeld as the elements of
the matrix A.
In the present context we usually consider Œ± ‚ààR. Extensions to complex-valued
matrices and vectors are possible. The axioms above are essentially the same as for
the norm in all other cases (see DeÔ¨Ånition 1.3 for comparison). The most common
matrix norms are the Frobenius norm
||A||F =
4
5
5
6
m‚àí1

k=0
n‚àí1

l=0
|ak,l|2
(4.36a)
TLFeBOOK

CONDITION NUMBERS
141
and the p-norms
||A||p = sup
xÃ∏=0
||Ax||p
||x||p
.
(4.36b)
We see that in (4.36b) the matrix p-norm is dependent on the vector p-norm. Via
(4.36b), we have
||Ax||p ‚â§||A||p||x||p.
(4.36c)
We may regard A as an operator applied to x that yields output Ax. Equation
(4.36c) gives an upper bound on the size of the output, as we know the size of A
and the size of x as given by their respective p-norms. Also, since A ‚ààRm√ón it
must be the case that x ‚ààRn, but y ‚ààRm. We observe that
||A||p = sup
xÃ∏=0

A
 
x
||x||p
!

p
= max
||x||p=1 ||Ax||p.
(4.37)
This is an alternative means to compute the matrix p-norm: Evaluate ||Ax||p at all
points on the unit sphere, which is the set of vectors {x|||x||p = 1}, and then pick
the largest value of ||Ax||p. Note that the term ‚Äúsphere‚Äù is an extension of what
we normally mean by a sphere. For the 2-norm in n dimensions, the unit sphere is
clearly
||x||2 = [x2
0 + x2
1 + ¬∑ ¬∑ ¬∑ + x2
n‚àí1]1/2 = 1.
(4.38)
This represents our intuitive (i.e., Euclidean) notion of a sphere. But, say, for the
1-norm the unit sphere is
||x||1 = |x0| + |x1| + ¬∑ ¬∑ ¬∑ + |xn‚àí1| = 1.
(4.39)
Equations (4.38) and (4.39) specify very different looking surfaces in n-dimensional
space. A suggested exercise is to sketch these spheres for n = 2.
As with vector norms, matrix norms have various properties. One property pos-
sessed by the matrix p-norms is called the submultiplicative property:
||AB||p ‚â§||A||p||B||p
A ‚ààRm√ón,
B ‚ààRn√óq.
(4.40)
(The reader is warned that not all matrix norms possess this property; a coun-
terexample appears on p. 57 of Golub and Van Loan [5]). A miscellany of other
properties (including equivalences) is
||A||2 ‚â§||A||F ‚â§‚àön||A||2,
(4.41a)
max
i,j |ai,j| ‚â§||A||2 ‚â§‚àömn max
i,j |ai,j|,
(4.41b)
TLFeBOOK

142
LINEAR SYSTEMS OF EQUATIONS
||A||1 = max
j‚ààZn
m‚àí1

i=0
|ai,j|,
(4.41c)
||A||‚àû= max
i‚ààZm
n‚àí1

j=0
|ai,j|,
(4.41d)
1
‚àön||A||‚àû‚â§||A||2 ‚â§‚àöm||A||‚àû,
(4.41e)
1
‚àöm||A||1 ‚â§||A||2 ‚â§‚àön||A||1.
(4.41f)
The equivalences [e.g., (4.41a) and (4.41b)] have the same signiÔ¨Åcance for
matrices as the analogous equivalences for vectors seen in (4.32) and (4.33).
From (4.41c,d) we see that computing matrix 1-norms and ‚àû-norms is easy.
However, computing matrix 2-norms is not easy. Consider (4.37) with p = 2:
||A||2 = max
||x||2=1 ||Ax||2.
(4.42)
Let R = AT A ‚ààRn√ón (no, R is not a Hilbert matrix here; we have ‚Äúrecycled‚Äù the
symbol for another use), so then
||Ax||2
2 = xT AT Ax = xT Rx.
(4.43)
Now consider n = 2. Thus
xT Rx = [x0x1]
 r00
r01
r10
r11
  x0
x1

,
(4.44)
where r01 = r10 because R = RT . The vectors and matrix in (4.44) multiply out
to become
xT Rx = r00x2
0 + 2r01x0x1 + r11x2
1.
(4.45)
Since ||A||2
2 = max||x||2=1 ||Ax||2
2, we may Ô¨Ånd ||A||2
2 by maximizing (4.45) subject
to the equality constraint ||x||2
2 = 1, i.e., xT x = x2
0 + x2
1 = 1. This problem may
be solved by using Lagrange multipliers (considered somewhat more formally in
Section 8.5). Thus, we must maximize
V (x) = xT Rx ‚àíŒª[xT x ‚àí1],
(4.46)
where Œª is the Lagrange multiplier. Since
V (x) = r00x2
0 + 2r01x0x1 + r11x2
1 ‚àíŒª[x2
0 + x2
1 ‚àí1],
TLFeBOOK

CONDITION NUMBERS
143
we have
‚àÇV (x)
‚àÇx0
= 2r00x0 + 2r01x1 ‚àí2Œªx0 = 0,
‚àÇV (x)
‚àÇx1
= 2r01x0 + 2r11x1 ‚àí2Œªx1 = 0,
and these equations may be rewritten in matrix form as
 r00
r01
r10
r11
  x0
x1

= Œª
 x0
x1

.
(4.47)
In other words, Rx = Œªx. Thus, the optimum choice of x is an eigenvector of
R = AT A. But which eigenvector is it?
First
note
that
A‚àí1
exists
(by
assumption),
so
xT Rx = xT AT Ax =
(Ax)T (Ax) > 0 for all x Ã∏= 0. Therefore, R > 0. Additionally, R = RT , so all
of the eigenvalues of R are real numbers.5 Furthermore, because R > 0, all of its
eigenvalues are positive. This follows if we consider Rx = Œªx, and assume that
Œª < 0. In this case xT Rx = ŒªxT x = Œª||x||2
2 < 0 for any x Ã∏= 0. [If Œª = 0, then
Rx = 0 ¬∑ x = 0 implies that x = 0 (as R‚àí1 exists), so xT Rx = 0.] But this con-
tradicts the assumption that R > 0, and so all of the eigenvalues of R must be
positive. Now, since ||Ax||2
2 = xT AT Ax = xT Rx = xT (Œªx) = Œª||x||2
2, and since
||x||2
2 = 1, it must be the case that ||Ax||2
2 is biggest for the eigenvector of R cor-
responding to the biggest eigenvalue of R. If the eigenvalues of R are denoted Œª1
and Œª0 with Œª1 ‚â•Œª0 > 0, then Ô¨Ånally we must have
||A||2
2 = Œª1.
(4.48)
This argument can be generalized for all n > 2. If R > 0, we assume that all
of its eigenvalues are distinct (this is not always true). If we denote them by
Œª0, Œª1, . . . , Œªn‚àí1, then we may arrange them in decreasing order:
Œªn‚àí1 > Œªn‚àí2 > ¬∑ ¬∑ ¬∑ > Œª1 > Œª0 > 0.
(4.49)
Therefore, for A ‚ààRm√ón
||A||2
2 = Œªn‚àí1.
(4.50)
The problem of computing the eigenvalues and eigenvectors of a matrix has its own
special numerical difÔ¨Åculties. At this point we warn the reader that these problems
must never be treated lightly.
5If A is a real-valued symmetric square matrix, then we may prove this claim as follows. Suppose
that for eigenvector x of A, the eigenvalue is Œª, that is, Ax = Œªx. Now ((Ax)‚àó)T = ((Œªx)‚àó)T , and so
(x‚àó)T AT = Œª‚àó(x‚àó)T . Therefore, (x‚àó)T AT x = Œª‚àó(x‚àó)T x. But (x‚àó)T AT x = (x‚àó)T Ax = Œª(x‚àó)T x, so
Ô¨Ånally Œª‚àó(x‚àó)T x = Œª(x‚àó)T x, so we must have Œª = Œª‚àó. This can be true only if Œª ‚ààR.
TLFeBOOK

144
LINEAR SYSTEMS OF EQUATIONS
Example 4.2
Let det(A) denote the determinant of A. Suppose that R = AT A,
where
R =
 1
0.5
0.5
1

.
We will Ô¨Ånd ||A||2. Consider
 1
0.5
0.5
1
  x0
x1

= Œª
 x0
x1

.
We must solve det(ŒªI ‚àíR) = 0 for Œª. [Recall that det(ŒªI ‚àíR) is the characteristic
polynomial of R.] Thus
det (ŒªI ‚àíR) = det
  Œª ‚àí1
‚àí0.5
‚àí0.5
Œª ‚àí1
!
= (Œª ‚àí1)2 ‚àí1
4 = 0,
and (Œª ‚àí1)2 ‚àí1
4 = Œª2 ‚àí2Œª + 3
4 = 0, for
Œª =
‚àí(‚àí2) ¬±
7
(‚àí2)2 ‚àí4 ¬∑ 1 ¬∑ 3
4
2 ¬∑ 1
= 2 ¬± 1
2
= 1
2, 3
2.
So, Œª1 = 3
2, Œª0 = 1
2. Thus, ||A||2
2 = Œª1 = 3
2, and so Ô¨Ånally
||A||2 =
8
3
2.
(We do not need the eigenvectors of R to compute the 2-norm of A.)
We see that the essence of computing the 2-norm of matrix A is to Ô¨Ånd the zeros
of the characteristic polynomial of AT A. The problem of Ô¨Ånding polynomial zeros
is the subject of a later chapter. Again, this problem has its own special numerical
difÔ¨Åculties that must never be treated lightly.
We now derive the condition number. Begin by assuming that A ‚ààRn√ón, and
that A‚àí1 exists. The error between computed solution ÀÜx to Ax = y and x is
e = x ‚àíÀÜx.
(4.51)
Ax = y, but AÀÜx Ã∏= y in general. So we may deÔ¨Åne the residual
r = y ‚àíAÀÜx.
(4.52)
We see that
Ae = Ax ‚àíAÀÜx = y ‚àíAÀÜx = r.
(4.53)
Thus, e = A‚àí1r. We observe that if e = 0, then r = 0, but if r is small, then e
is not necessarily small because A‚àí1 might be big, making A‚àí1r big. In other
TLFeBOOK

CONDITION NUMBERS
145
words, a small residual r does not guarantee that ÀÜx is close to x. Sometimes r is
computed as a cursory check to see if ÀÜx is ‚Äúreasonable.‚Äù The main advantage of
r is that it may always be computed, whereas x is not known in advance and so
e may never be computed exactly. Below it will be shown that considering r in
combination with a condition number is a more reliable method of assessing how
close ÀÜx is to x.
Now, since e = A‚àí1r, we can say that ||e||p = ||A‚àí1r||p ‚â§||A‚àí1||p||r||p [via
(4.36c)]. Similarly, since r = Ae, we have ||r||p = ||Ae||p ‚â§||A||p||e||p. Thus
||r||p
||A||p
‚â§||e||p ‚â§||A‚àí1||p||r||p.
(4.54)
Similarly, x = A‚àí1y, so immediately
||y||p
||A||p
‚â§||x||p ‚â§||A‚àí1||p||y||p.
(4.55)
If ||x||p Ã∏= 0, and ||y||p Ã∏= 0, then taking reciprocals in (4.55) yields
1
||A‚àí1||p||y||p
‚â§
1
||x||p
‚â§||A||p
||y||p
.
(4.56)
We may multiply corresponding terms in (4.56) and (4.54) to obtain
1
||A‚àí1||p||A||p
||r||p
||y||p
‚â§||e||p
||x||p
‚â§||A‚àí1||p||A||p
||r||p
||y||p
.
(4.57)
We recall from (4.34b) that œµr = ||x‚àíÀÜx||p
||x||p
= ||e||p
||x||p , so
1
||A‚àí1||p||A||p
||r||p
||y||p
‚â§œµr ‚â§||A‚àí1||p||A||p
||r||p
||y||p
.
(4.58)
We call
||r||p
||y||p
= ||y ‚àíAÀÜx||p
||y||p
(4.59)
the relative residual. We deÔ¨Åne
Œ∫p(A) = ||A||p||A‚àí1||p
(4.60)
to be the condition number of A. It is immediately apparent that Œ∫p(A) ‚â•1 for
any A and valid p. We see that œµr is between 1/Œ∫p(A) and Œ∫p(A) times the
relative residual. In particular, if Œ∫p(A) >> 1 (i.e., if the condition number is very
large), even if the relative residual is tiny, then œµr might be large. On the other
hand, if Œ∫p(A) is close to unity, then œµr will be small if the relative residual is
small. In conclusion, if Œ∫p(A) is large, it is a warning (not a certainty) that small
TLFeBOOK

146
LINEAR SYSTEMS OF EQUATIONS
perturbations in A and y may cause ÀÜx to differ greatly from x. Equivalently, if Œ∫p(A)
is large, then a small r does not imply that ÀÜx is close to x.
A rule of thumb in interpreting condition numbers is as follows [3, p. 229], and
is more or less true regardless of p in (4.60). If Œ∫p(A) ‚âàd √ó 10k, where d is a
decimal digit from one to nine, we can expect to lose (at worst) about k digits of
accuracy. The reason that p does not matter too much is because we recall that
matrix norms are equivalent. Therefore, for this rule of thumb to be useful, the
working precision of the computing machine/software package must be known.
For example, MATLAB computes to about 16 decimal digits of precision. Thus,
k ‚â•16 would give us concern that ÀÜx is not close to x.
Example 4.3
Suppose that
A =
 1
1 ‚àíœµ
1
1

‚ààR2√ó2, |œµ| << 1.
We will determine an estimate of Œ∫1(A). Clearly
A‚àí1 = 1
œµ

1
‚àí1 + œµ
‚àí1
1

=
 b00
b01
b10
b11

.
We have
1

i=0
|ai,0| = |a00| + |a10| = 2,
1

i=0
|ai,1| = |a01| + |a11| = |1 ‚àíœµ| + 1,
so via (4.41c), ||A||1 = max{2, |1 ‚àíœµ| + 1} ‚âà2. Similarly
1

i=0
|bi,0| = |b00| + |b10| = 2
|œµ|,
1

i=0
|bi,1| = |b01| + |b11| = 1
|œµ| +

‚àí1 + œµ
œµ
 ,
so again via (4.41c) ||A‚àí1||1 = max
9
2
|œµ|,
1 ‚àí1
œµ
 + 1
|œµ|
:
‚âà
2
|œµ|. Thus
Œ∫1(A) ‚âà4
|œµ|.
We observe that if œµ = 0, then A‚àí1 does not exist, so our approximation to Œ∫1(A)
is a reasonable result because
lim
|œµ|‚Üí0 Œ∫1(A) = ‚àû.
We may wish to compute Œ∫2(A) = ||A||2||A‚àí1||2. We will suppose that A ‚àà
Rn√ón and that A‚àí1 exists. But we recall that computing matrix 2-norms involves
Ô¨Ånding eigenvalues. More speciÔ¨Åcally, ||A||2
2 is the largest eigenvalue of R = AT A
TLFeBOOK

CONDITION NUMBERS
147
[recall (4.50)]. Suppose, as in (4.49), that Œª0 is the smallest eigenvalue of R for
which the corresponding eigenvector is denoted by v, that is, Rv = Œª0v. Then we
observe that R‚àí1v = 1
Œª0 v. In other words, 1/Œª0 is an eigenvalue of R‚àí1. By similar
reasoning, 1/Œªk for k ‚ààZn must all be eigenvalues of R‚àí1. Thus, 1/Œª0 will be the
biggest eigenvalue of R‚àí1. For present simplicity assume that A is a normal matrix.
This means that AAT = AT A = R. The reader is cautioned that not all matrices A
are normal. However, in this case we have R‚àí1 = A‚àí1A‚àíT = A‚àíT A‚àí1. [Recall
that (A‚àí1)T = (AT )‚àí1 = A‚àíT .] We have that ||A‚àí1||2
2 is the largest eigenvalue of
A‚àíT A‚àí1, but R‚àí1 = A‚àíT A‚àí1 since A is assumed normal. The largest eigenvalue
of R‚àí1 has been established to be 1/Œª0, so it must be the case that for a normal
matrix A (real-valued and invertible)
Œ∫2(A) =
;
Œªn‚àí1
Œª0
,
(4.61)
that is, A is ill-conditioned if the ratio of the biggest to smallest eigenvalue of
AT A is large. In other words, a large eigenvalue spread is associated with matrix
ill conditioning. It turns out that this conclusion holds even if A is not normal; that
is, (4.61) is valid even if A is not normal. But we will not prove this. (The interested
reader can see pp. 312 and 340 of Horn and Johnson [6] for more information.)
An obvious difÔ¨Åculty with condition numbers is that their exact calculation often
seems to require knowledge of A‚àí1. Clearly this is problematic since computing
A‚àí1 accurately may not be easy or possible (because of ill conditioning). We seem
to have a ‚Äúchicken and egg‚Äù problem. This problem is often dealt with by using
condition number estimators. This in turn generally involves placing bounds on
condition numbers. But the subject of condition number estimation is not within
the scope of this book. The interested reader might consult Higham [7] for further
information on this subject if desired. There is some information on this matter in
the treatise by Golub and Van Loan [5, pp. 128‚Äì130], which includes a pseudocode
algorithm for ‚àû-norm condition number estimation of an upper triangular nonsin-
gular matrix. We remark that ||A||2 is sometimes called the spectral norm of A,
and is actually best computed using entities called singular values [5, p. 72]. This
is because computing singular values avoids the necessity of computing A‚àí1, and
can be done in a numerically reliable manner. Singular values will be discussed in
more detail later.
We conclude this section with a remark about the Hilbert matrix R of Section 4.3.
As discussed by Hill [3, p. 232], we have
Œ∫2(R) ‚àùeŒ±N
for some Œ± > 0. (Recall that symbol ‚àùmeans ‚Äúproportional to.‚Äù) Proving this is
tough, and we will not attempt it. Thus, the condition number of R grows very
rapidly with N and explains why the attempt to invert R in Appendix 4.A failed
for so small a value of N.
TLFeBOOK

148
LINEAR SYSTEMS OF EQUATIONS
4.5
LU DECOMPOSITION
In this section we will assume A ‚ààRn√ón, and that A‚àí1 exists. Many algorithms
to solve Ax = y work by factoring the matrix A in various ways. In this section
we consider a Gaussian elimination approach to writing A as
A = LU,
(4.62)
where L is a nonsingular lower triangular matrix, and U is a nonsingular upper
triangular matrix. This is the LU decomposition (factorization) of A. Naturally,
L, U ‚ààRn√ón, and L = [li,j], U = [ui,j]. Since these matrices are lower and upper
triangular, respectively, it must be the case that
li,j = 0
for
j > i
and
ui,j = 0
for
j < i.
(4.63)
For example, the following are (respectively) lower and upper triangular matrices:
L =
Ô£Æ
Ô£∞
1
0
0
1
1
0
1
1
1
Ô£π
Ô£ª,
U =
Ô£Æ
Ô£∞
1
2
3
0
4
5
0
0
6
Ô£π
Ô£ª.
These matrices are clearly nonsingular since their determinants are 1 and 24, respec-
tively. In fact, L is nonsingular iff li,i Ã∏= 0 for all i, and U is nonsingular iff uj,j Ã∏= 0
for all j. We note that with A factored as in (4.62), the solution of Ax = y becomes
quite easy, but the details of this will be considered later. We now concentrate on
Ô¨Ånding the factors L, U.
We begin by deÔ¨Åning a Gauss transformation matrix Gk such that
Gkx =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
1
¬∑ ¬∑ ¬∑
0
0
¬∑ ¬∑ ¬∑
0
...
...
...
...
0
¬∑ ¬∑ ¬∑
1
0
¬∑ ¬∑ ¬∑
0
0
¬∑ ¬∑ ¬∑
‚àíœÑ k
k
1
¬∑ ¬∑ ¬∑
0
...
...
...
...
0
¬∑ ¬∑ ¬∑
‚àíœÑ k
n‚àí1
0
¬∑ ¬∑ ¬∑
1
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
x0
...
xk‚àí1
xk
...
xn‚àí1
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
=
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
x0
...
xk‚àí1
0
...
0
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
(4.64)
for
œÑ k
i =
xi
xk‚àí1
, i = k, . . . , n ‚àí1.
(4.65)
The superscript k on œÑ k
j does not denote raising œÑj to a power; it is simply part of
the name of the symbol. This naming convention is needed to account for the fact
that there is a different set of œÑ values for every Gk. For this to work requires that
xk‚àí1 Ã∏= 0. Equation (4.65) followed from considering the matrix‚Äìvector product
TLFeBOOK

LU DECOMPOSITION
149
in (4.64):
‚àíœÑ k
k xk‚àí1 + xk = 0
‚àíœÑ k
k+1xk‚àí1 + xk+1 = 0
...
‚àíœÑ k
n‚àí1xk‚àí1 + xn‚àí1 = 0.
We observe that Gk is ‚Äúdesigned‚Äù to annihilate the last n ‚àík elements of vector
x. We also see that Gk is lower triangular, and if it exists, always possesses an
inverse because the main diagonal elements are all equal to unity. A lower triangular
matrix where all of the main diagonal elements are equal to unity is called unit
lower triangular. Similar terminology applies to upper triangular matrices. DeÔ¨Åne
the kth Gauss vector
(œÑ k)T = [0 ¬∑ ¬∑ ¬∑ 0
# $% &
k zeros
œÑ k
k œÑ k
k+1 ¬∑ ¬∑ ¬∑ œÑ k
n‚àí1].
(4.66)
The kth unit vector is
eT
k = [0 ¬∑ ¬∑ ¬∑ 0
# $% &
k zeros
1
0 ¬∑ ¬∑ ¬∑ 0
# $% &
n‚àík‚àí1 zeros
].
(4.67)
If I is an n √ó n identity matrix, then
Gk = I ‚àíœÑ keT
k‚àí1
(4.68)
for k = 1, 2, . . . , n ‚àí1. For example, if n = 4, we have
G1 =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
1
0
0
0
‚àíœÑ 1
1
1
0
0
‚àíœÑ 1
2
0
1
0
‚àíœÑ 1
3
0
0
1
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª,
G2 =
Ô£Æ
Ô£ØÔ£ØÔ£∞
1
0
0
0
0
1
0
0
0
‚àíœÑ 2
2
1
0
0
‚àíœÑ 2
3
0
1
Ô£π
Ô£∫Ô£∫Ô£ª,
G3 =
Ô£Æ
Ô£ØÔ£ØÔ£∞
1
0
0
0
0
1
0
0
0
0
1
0
0
0
‚àíœÑ 3
3
1
Ô£π
Ô£∫Ô£∫Ô£ª.
(4.69)
The Gauss transformation matrices may be applied to A, yielding an upper trian-
gular matrix. This is illustrated by the following example.
Example 4.4
Suppose that
A =
Ô£Æ
Ô£ØÔ£ØÔ£∞
1
2
3
4
‚àí1
1
2
1
0
2
1
3
0
0
1
1
Ô£π
Ô£∫Ô£∫Ô£ª(= A0).
TLFeBOOK

150
LINEAR SYSTEMS OF EQUATIONS
We introduce matrices Ak, where Ak = GkAk‚àí1 for k = 1, 2, . . . , n ‚àí1, and Ô¨Ånally
U = An‚àí1. Once again, Ak is not the kth power of A, but rather denotes the kth
matrix in a sequence of matrices. Now consider
G1A0 =
Ô£Æ
Ô£ØÔ£ØÔ£∞
1
0
0
0
1
1
0
0
0
0
1
0
0
0
0
1
Ô£π
Ô£∫Ô£∫Ô£ª
Ô£Æ
Ô£ØÔ£ØÔ£∞
1
2
3
4
‚àí1
1
2
1
0
2
1
3
0
0
1
1
Ô£π
Ô£∫Ô£∫Ô£ª=
Ô£Æ
Ô£ØÔ£ØÔ£∞
1
2
3
4
0
3
5
5
0
2
1
3
0
0
1
1
Ô£π
Ô£∫Ô£∫Ô£ª= A1
for which the œÑ 1
i entries in the Ô¨Årst column of G1 depend on the Ô¨Årst column of
A0 (i.e., of A) according to (4.65). Similarly
G2A1 =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
1
0
0
0
0
1
0
0
0
‚àí2
3
1
0
0
0
0
1
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª
Ô£Æ
Ô£ØÔ£ØÔ£∞
1
2
3
4
0
3
5
5
0
2
1
3
0
0
1
1
Ô£π
Ô£∫Ô£∫Ô£ª=
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
1
2
3
4
0
3
5
5
0
0
‚àí7
3
‚àí1
3
0
0
1
1
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª= A2
for which the œÑ 2
i entries in the second column of G2 depend on the second column
of A1, and also
G3A2 =
Ô£Æ
Ô£ØÔ£ØÔ£∞
1
0
0
0
0
1
0
0
0
0
1
0
0
0
3
7
1
Ô£π
Ô£∫Ô£∫Ô£ª
Ô£Æ
Ô£ØÔ£ØÔ£∞
1
2
3
4
0
3
5
5
0
0
‚àí7
3
‚àí1
3
0
0
1
1
Ô£π
Ô£∫Ô£∫Ô£ª=
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
1
2
3
4
0
3
5
5
0
0
‚àí7
3
‚àí1
3
0
0
0
6
7
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª= U
for which the œÑ 3
i entries in the third column of G3 depend on the third column of
A2. We see that U is indeed upper triangular, and it is also nonsingular. We also
see that
U = G3G2G1
#
$%
&
=L1
A.
Since the product of lower triangular matrices is a lower triangular matrix, it is the
case that L1 = G3G2G1 is lower triangular. Thus
A = L‚àí1
1 U.
Since the inverse (if it exists) of a lower triangular matrix is also a lower triangular
matrix, we can deÔ¨Åne L = L‚àí1
1 , and so A = LU. Thus
L = G‚àí1
1 G‚àí1
2 G‚àí1
3 .
From this example it appears that we need to do much work in order to Ô¨Ånd G‚àí1
k .
However, this is not the case. It turns out that
G‚àí1
k
= I + œÑ keT
k‚àí1.
(4.70)
TLFeBOOK

LU DECOMPOSITION
151
This is easy to conÔ¨Årm. From (4.68) and (4.70)
GkG‚àí1
k
= [I ‚àíœÑ keT
k‚àí1][I + œÑ keT
k‚àí1] = I ‚àíœÑ keT
k‚àí1 + œÑ keT
k‚àí1 ‚àíœÑ keT
k‚àí1œÑ keT
k‚àí1
= I ‚àíœÑ keT
k‚àí1œÑ keT
k‚àí1.
But from (4.66) and (4.67) eT
k‚àí1œÑ k = 0, so Ô¨Ånally GkG‚àí1
k
= I.
To obtain œÑ k
i from (4.65), we see that we must divide by xk‚àí1. In our matrix
factorization application of the Gauss transformation, we have seen (in Example
4.4) that xk‚àí1 will be an element of Ak. These elements are called pivots. It
is apparent that the factorization procedure cannot work if a pivot is zero. The
occurrence of zero-valued pivots is a common situation. A simple example of a
matrix that cannot be factored with our algorithm is
A =
 0
1
1
0

.
(4.71)
In this case
G1A =

1
0
‚àíœÑ 1
1
1
  0
1
1
0

=
 0
1
1
‚àíœÑ 1
1

,
(4.72)
and from (4.65)
œÑ 1
1 = x1
x0
= a10
a00
= 1
0 ‚Üí‚àû.
(4.73)
This result implies that not all matrices possess an LU factorization. Let det(A)
denote the determinant of A. We may state a general condition for the existence of
the LU factorization:
Theorem 4.1: Since A = [ai,j]i,j=0,...,n‚àí1 ‚ààRn√ón we deÔ¨Åne the kth leading
principle submatrix of A to be Ak = [ai,j]i,j=0,... ,k‚àí1 ‚ààRk√ók for k = 1, 2, . . . , n
(so that A = An, and A1 = [a00] = a00). There exists a unit lower triangular matrix
L and an upper triangular matrix U such that A = LU, provided that det(Ak) Ã∏= 0
for all k = 1, 2 . . . , n. Furthermore, with U = [ui,j] ‚ààRn√ón we have det(Ak) =
0k‚àí1
i=0 ui,i.
The proof is given in Golub and Van Loan [5]. It will not be considered here. For
A in (4.71), we see that A1 = [0] = 0, so det(A1) = 0. Thus, even though A‚àí1
exists, it does not possess an LU decomposition. It is also easy to verify that for
A =
Ô£Æ
Ô£∞
1
4
1
2
8
1
0
‚àí1
1
Ô£π
Ô£ª,
although A‚àí1 exists, again A does not possess an LU decomposition. In this case
we have det(A2) = det
  1
4
2
8
!
= 0. Theorem 4.1 leads to a test of positive
deÔ¨Åniteness according to the following theorem.
TLFeBOOK

152
LINEAR SYSTEMS OF EQUATIONS
Theorem 4.2: Suppose R ‚ààRn√ón with R = RT . Suppose that R = LDLT ,
where
L
is
unit
lower
triangular,
and
D
is
a
diagonal
matrix
(L =
[li,j]i,j=0,...,n‚àí1, D = [di,j]i,j=0,...,n‚àí1). If di,i > 0 for all i ‚ààZn, then R > 0.
Proof
L is unit lower triangular, so for any y ‚ààRn there will be a unique
x ‚ààRn such that
y = LT x
(yT = xT L)
because L‚àí1 exists. Thus, assuming D > 0
xT Rx = xT LDLT x = yT Dy =
n‚àí1

i=0
y2
i di,i > 0
for all y Ã∏= 0, since di,i > 0 for all i ‚ààZn. In fact, n‚àí1
i=0 y2
i di,i = 0 iff yi = 0 for
all i ‚ààZn. Consequently, xT Rx > 0 for all x Ã∏= 0, and so immediately R > 0.
We relate D in Theorem 4.2 to U in Theorem 4.1 according to U = DLT . If the
LDLT decomposition of a matrix R exists, then matrix D immediately tells us
whether R is pd just by viewing the signs of the diagonal elements.
We may deÔ¨Åne (as in Example 4.4) Ak = [ak
i,j], where k = 0, 1, . . . , n ‚àí1 and
A0 = A. Consequently
œÑ k
i =
xi
xk‚àí1
=
ak‚àí1
i,k‚àí1
ak‚àí1
k‚àí1,k‚àí1
(4.74)
for i = k, k + 1, . . . , n ‚àí1. This follows because Gk contains œÑ k
i , and as observed
in the example above, œÑ k
i depends on the column indexed k ‚àí1 in Ak‚àí1. Thus, a
pseudocode program for Ô¨Ånding U can therefore be stated as follows:
A0 := A;
for k := 1 to n ‚àí1 do begin
for i := k to n ‚àí1 do begin
œÑ k
i := ak‚àí1
i,k‚àí1/ak‚àí1
k‚àí1,k‚àí1; {This loop computes œÑk}
end;
Ak := GkAk‚àí1; { Gk contains œÑk
i via (4.64) }
end;
U := An‚àí1;
We see that the pivots are ak‚àí1
k‚àí1,k‚àí1 for k = 1, 2, . . . , n ‚àí1. Now
U = Gn‚àí1Gn‚àí2 ¬∑ ¬∑ ¬∑ G2G1A.
so
A = G‚àí1
1 G‚àí1
2
¬∑ ¬∑ ¬∑ G‚àí1
n‚àí2G‚àí1
n‚àí1
#
$%
&
=L
U.
(4.75)
TLFeBOOK

LU DECOMPOSITION
153
Consequently, from (4.70), we obtain
L = (I + œÑ 1eT
0 )(I + œÑ 2eT
1 ) ¬∑ ¬∑ ¬∑ (I + œÑ n‚àí1eT
n‚àí2) = I +
n‚àí1

k=1
œÑ keT
k‚àí1.
(4.76)
To conÔ¨Årm the last equality of (4.76), consider deÔ¨Åning Lm = G‚àí1
1
¬∑ ¬∑ ¬∑ G‚àí1
m
for
m = 1, . . . , n ‚àí1. Assume that Lm = I + m
k=1 œÑ keT
k‚àí1, which is true for m = 1
because L1 = G‚àí1
1
= I + œÑ 1eT
0 . Consider Lm+1 = LmG‚àí1
m+1, so
Lm+1 =
'
I +
m

k=1
œÑ keT
k‚àí1
(
(I + œÑ m+1eT
m)
= I +
m

k=1
œÑ keT
k‚àí1 + œÑ m+1eT
m +
m

k=1
œÑ keT
k‚àí1œÑ m+1eT
m.
But eT
k‚àí1œÑ m+1 = 0 for k = 1, . . . , m from (4.66) and (4.67). Thus
Lm+1 = I +
m

k=1
œÑ keT
k‚àí1 + œÑ m+1eT
m = I +
m+1

k=1
œÑ keT
k‚àí1.
Therefore, (4.76) is valid by mathematical induction. (A simpler example of a
proof by induction appears in Appendix 3.B.) Because of (4.76), the previous
pseudocode implicitly computes L as well as U. Thus, if no zero-valued pivots
are encountered, the algorithm will terminate, having provided us with both L
and U. [As an exercise, the reader should use (4.76) to Ô¨Ånd L in Example 4.4
simply by looking at the appropriate entries of the matrices Gk; that is, do not
use L = G‚àí1
1 G‚àí1
2 G‚àí1
3 . Having found L by this means, conÔ¨Årm that LU = A.] We
remark that (4.76) shows that L is unit lower triangular.
It is worth mentioning that certain classes of matrix are guaranteed to possess
an LU decomposition. Suppose that A ‚ààRn√ón with A = AT and A > 0. Let v =
[v0 ¬∑ ¬∑ ¬∑ vk‚àí1
#
$%
&
=uT
0 ¬∑ ¬∑ ¬∑ 0
# $% &
n‚àík zeros
]T ; then, if v Ã∏= 0, we have vT Av > 0, but if Ak is the kth
leading principle submatrix of A, then
vT Av = uT Aku > 0
which holds for all k = 1, 2, . . . , n. Consequently, Ak > 0 for all k, and so A‚àí1
k
exists for all k. Since A‚àí1
k
exists for all k, it follows that det(Ak) Ã∏= 0 for all k.
The conditions of Theorem 4.1 are met, and so A possesses an LU decomposi-
tion. That is, all real-valued, symmetric positive deÔ¨Ånite matrices possess an LU
decomposition.
TLFeBOOK

154
LINEAR SYSTEMS OF EQUATIONS
We recall that the class of positive deÔ¨Ånite matrices is an important one since
they have a direct association with least-squares approximation problems. This was
demonstrated in Section 4.2.
How many Ô¨Çoating-point operations (Ô¨Çops) are needed by the algorithm for
Ô¨Ånding the LU decomposition of a matrix? Answering this question gives us an
indication of the computational complexity of the algorithm. Neglecting multipli-
cation by zero or by one, to compute Ak = GkAk‚àí1 requires (n ‚àík)(n ‚àík + 1)
multiplications, and the same number of additions. This follows from considering
the product GkAk‚àí1 with the factors partitioned into submatrices according to
Gk =

Ik
0
Tk
In‚àík
	
,
Ak‚àí1 =

Ak‚àí1
00
Ak‚àí1
01
0
Ak‚àí1
11
	
,
(4.77)
where Ik is a k √ó k identity matrix, Tk is (n ‚àík) √ó k and is zero-valued except for
its last column, which contains ‚àíœÑ k [see (4.64)]. Similarly, Ak‚àí1
00
is (k ‚àí1) √ó
(k ‚àí1), Ak
01 is (k ‚àí1) √ó (n ‚àík + 1), and Ak‚àí1
11
is (n ‚àík + 1) √ó (n ‚àík + 1).
From the pseudocode, we see that we need n‚àí1
k=1(n ‚àík) division operations.
Operation Ak = GkAk‚àí1 is executed for k = 1 to n ‚àí1, so the total number of
operations is:
n‚àí1

k=1
(n ‚àík)(n ‚àík + 1)
multiplications
n‚àí1

k=1
(n ‚àík)(n ‚àík + 1)
additions
n‚àí1

k=1
(n ‚àík)
divisions
We now recognize that
N

k=1
k = N(N + 1)
2
,
N

k=1
k2 = N(N + 1)(2N + 1)
6
,
(4.78)
where the second summation identity was proven in Appendix 3.B. The Ô¨Årst sum-
mation identity may be proved in a similar manner. Therefore
n‚àí1

k=1
(n ‚àík)(n ‚àík + 1) =
n‚àí1

k=1
[n2 + n ‚àí(2n + 1)k + k2]
= (n ‚àí1)(n2 + n) ‚àí(2n + 1)
n‚àí1

k=1
k +
n‚àí1

k=1
k2
TLFeBOOK

LU DECOMPOSITION
155
= 1
3n3 ‚àí1
3n,
(4.79a)
n‚àí1

k=1
(n ‚àík) = n(n ‚àí1) ‚àí
n‚àí1

k=1
k = 1
2n2 ‚àí1
2n.
(4.79b)
So-called asymptotic complexity measures are deÔ¨Åned using
DeÔ¨Ånition 4.2: Big O
We say that f (n) = O(g(n)) if there is a 0 < c < ‚àû,
and an N ‚ààN (N < ‚àû) such that
f (n) ‚â§cg(n)
for all n > N.
Our algorithm needs a total of f (n) = 2( 1
3n3 ‚àí1
3n) + 1
2n2 ‚àí1
2n = 2
3n3 + 1
2n2 ‚àí
7
6n Ô¨Çops. We may say that O(n3) operations (Ô¨Çops) are needed (so here g(n) =
n3). We may read O(n3) as ‚Äúorder n-cubed,‚Äù so order n-cubed operations are
needed. If one operation takes one unit of time on a computing machine we say
the asymptotic time complexity of the algorithm is O(n3). Parameter n (matrix
order) is the size of the problem. We might also say that the time complexity
of the algorithm is cubic in the size of the problem since the number of oper-
ations f (n) is a cubic polynomial in n. But we caution the reader about Ô¨Çop
counting:
‚ÄúFlop counting is a necessarily crude approach to the measuring of program efÔ¨Åciency
since it ignores subscripting, memory trafÔ¨Åc, and the countless other overheads asso-
ciated with program execution. We must not infer too much from a comparison of
Ô¨Çops counts. . . . Flop counting is just a ‚Äòquick and dirty‚Äô accounting method that
captures only one of several dimensions of the efÔ¨Åciency issue.‚Äù
‚ÄîGolub and Van Loan [5, p. 20]
Asymptotic complexity measures allow us to talk about algorithmic resource
demands without getting bogged down in detailed expressions for computing time,
memory requirements, and other variables. However, the comment by Golub and
Van Loan above may clearly be extended to asymptotic measures.
Suppose that A is LU-factorable, and that we know L and U. Suppose that we
wish to solve Ax = y. Thus
LUx = y,
(4.80)
and deÔ¨Åne Ux = z, so we begin by considering
Lz = y.
(4.81)
TLFeBOOK

156
LINEAR SYSTEMS OF EQUATIONS
In expanded form this becomes
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
l00
0
0
¬∑ ¬∑ ¬∑
0
l10
l11
0
¬∑ ¬∑ ¬∑
0
l20
l21
l22
¬∑ ¬∑ ¬∑
0
...
...
...
...
ln‚àí1,0
ln‚àí1,1
ln‚àí1,2
¬∑ ¬∑ ¬∑
ln‚àí1,n‚àí1
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
z0
z1
z2
...
zn‚àí1
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
=
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
y0
y1
y2
...
yn‚àí1
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
(4.82)
Since L‚àí1 exists, solving (4.81) is easy using forward elimination (forward substi-
tution). SpeciÔ¨Åcally, from (4.82)
z0 = y0
l0,0
z1 =
1
l1,1
[y1 ‚àíz0l1,0]
z2 =
1
l2,2
[y2 ‚àíz0l2,0 ‚àíz1l2,1]
...
zn‚àí1 =
1
ln‚àí1,n‚àí1

yn‚àí1 ‚àí
n‚àí2

k=0
zkln‚àí1,k
	
.
Thus, in general
zk =
1
lk,k

yk ‚àí
k‚àí1

i=0
zilk,i
	
(4.83)
for k = 1, 2, . . . , n ‚àí1 with z0 = y0/l0,0. Since we now know z, we may solve
Ux = z by backward substitution. To see this, express the problem in expanded
form:
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
u0,0
u0,1
¬∑ ¬∑ ¬∑
u0,n‚àí2
u0,n‚àí1
0
u1,1
¬∑ ¬∑ ¬∑
u1,n‚àí2
u1,n‚àí1
...
...
...
...
0
0
¬∑ ¬∑ ¬∑
un‚àí2,n‚àí2
un‚àí2,n‚àí1
0
0
¬∑ ¬∑ ¬∑
0
un‚àí1,n‚àí1
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
x0
x1
...
xn‚àí2
xn‚àí1
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
=
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
z0
z1
...
zn‚àí2
zn‚àí1
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
(4.84)
From (4.84), we obtain
xn‚àí1 =
zn‚àí1
un‚àí1,n‚àí1
xn‚àí2 =
1
un‚àí2,n‚àí2
[zn‚àí2 ‚àíxn‚àí1un‚àí2,n‚àí1]
TLFeBOOK

LU DECOMPOSITION
157
xn‚àí3 =
1
un‚àí3,n‚àí3
[zn‚àí3 ‚àíxn‚àí1un‚àí3,n‚àí1 ‚àíxn‚àí2un‚àí3,n‚àí2]
...
x0 =
1
u0,0

z0 ‚àí
n‚àí1

k=1
xku0,k
	
.
In general
xk =
1
uk,k
Ô£Æ
Ô£∞zk ‚àí
n‚àí1

i=k+1
xiuk,i
Ô£π
Ô£ª
(4.85)
for k = n ‚àí2, . . . , 0 with xn‚àí1 = zn‚àí1/un‚àí1,n‚àí1. The forward-substitution and
backward-substitution algorithms that we have just derived have an asymptotic
time complexity of O(n2). The reader should conÔ¨Årm this as an exercise. This
result suggests that most of the computational effort needed to solve for x in
Ax = y lies in the LU decomposition stage.
So far we have said nothing about the performance of our linear system solution
method with respect to Ô¨Ånite precision arithmetic effects (i.e., rounding error).
Before considering this matter, we make a few remarks regarding the stability of
our method. We have noted that the LU decomposition algorithm will fail if a zero-
valued pivot is encountered. This can happen even if Ax = y has a solution and A
is well-conditioned. In other words, our algorithm is actually unstable since we can
input numerically well-posed problems that cause it to fail. This does not necessarily
mean that our algorithm should be totally rejected. For example, we have shown
that positive deÔ¨Ånite matrices will never result in a zero-valued pivot. Furthermore,
if A > 0, and it is well-conditioned, then it can be shown that an accurate answer
will be provided by the algorithm despite its faults. Nonetheless, the problem of
failure due to encountering a zero-valued pivot needs to be addressed. Also, what
happens if a pivot is not exactly zero, but is close to zero? We might expect that
this can result in a computed solution ÀÜx that differs greatly from the mathematically
exact solution x, especially where rounding error is involved, even if A is well-
conditioned.
Recall (2.15) from Chapter 2,
f l[x op y] = (x op y)(1 + œµ)
(4.86)
for which |œµ| ‚â§2‚àít. If we store A in a Ô¨Çoating-point machine, then, because of
the necessity to quantize we are really storing the elements
[f l[A]]i,j = f l[ai,j] = ai,j(1 + œµi,j)
(4.87)
TLFeBOOK

158
LINEAR SYSTEMS OF EQUATIONS
with |œµi,j| ‚â§2‚àít. Suppose now that A, B ‚ààRm√ón; we then deÔ¨Åne6
|A| = [|ai,j|] ‚ààRm√ón,
(4.88)
and by B ‚â§A, we mean bi,j ‚â§ai,j for all i and j. So we may express (4.87) more
compactly as
|f l[A] ‚àíA| ‚â§u|A|,
(4.89)
where u = 2‚àít, since |œµi,j| ‚â§2‚àít.
Forsythe and Moler [4, pp. 104‚Äì105] show that the computed solution ÀÜz to
Lz = y [recall (4.81)] as obtained by forward substitution is actually the exact
solution to a perturbed lower triangular system
(L + Œ¥L)ÀÜz = y,
(4.90)
where Œ¥L is a lower triangular perturbation matrix, and where
|Œ¥L| ‚â§1.01nu|L|.
(4.91)
A very similar bound exists for the problem of solving Ux = z by backward-
substitution. We will not derive these bounds, but will simply mention that the
derivation involves working with a bound similar to (2.39) in Chapter 2. From
(4.91) we have relative perturbations |Œ¥li,j |
|li,j | ‚â§1.01nu. It is apparent that since u
is typically quite tiny, unless n (matrix order) is quite huge, these relative pertur-
bations will not be signiÔ¨Åcant. In other words, forward substitution and backward
substitution are very stable procedures that are quite resistant to the effects of
rounding errors. Thus, any difÔ¨Åculties with our linear system solution procedure in
terms of a rounding error likely involve only the LU factorization stage.
The rounding error analysis for our Gaussian elimination algorithm is even more
involved than the effort required to obtain (4.91), so again we will content ourselves
with citing the main result without proof. We cite Theorem 3.3.1 in Golub and Van
Loan [5] as follows.
Theorem 4.3: Assume that A is an n √ó n matrix of Ô¨Çoating-point numbers.
If no zero-valued pivots are encountered during the execution of the Gaussian
elimination algorithm for which A is the input, then the computed triangular factors
(here denoted ÀÜL and ÀÜU) satisfy
ÀÜL ÀÜU = A + Œ¥A
(4.92a)
such that
|Œ¥A| ‚â§3(n ‚àí1)u(|A| + | ÀÜL|| ÀÜU|) + O(u2).
(4.92b)
6There is some danger in confusing this with the determinant. That is, some people use |A| to denote
the determinant of A. We will avoid this here by sticking with det(A) as the notation for determinant
of A.
TLFeBOOK

LU DECOMPOSITION
159
In this theorem the term O(u2) denotes a part of the error term dependent on u2.
This is quite small as u2 = 2‚àí2t (rounding assumed), and so may be practically
disregarded. The term arises in the work of Golub and Van Loan [5] because those
authors prefer to work with slightly looser bounding results than are to be found
in the volume by Forsythe and Moler [4]. The bound in (4.92b) gives us cause for
concern. The perturbation matrix Œ¥A may not be small. This is because | ÀÜL|| ÀÜU| can
be quite large. An example of this would be
A =
Ô£Æ
Ô£∞
1
4
1
2
8.001
1
0
‚àí1
1
Ô£π
Ô£ª,
for which
ÀÜL =
Ô£Æ
Ô£∞
1
0
0
2
1
0
0
‚àí1000
1
Ô£π
Ô£ª,
ÀÜU =
Ô£Æ
Ô£∞
1
4
1
0
0.001
‚àí1
0
0
‚àí999
Ô£π
Ô£ª.
This has happened because
A1 =
Ô£Æ
Ô£∞
1
4
1
0
0.001
‚àí1
0
‚àí1
1
Ô£π
Ô£ª,
which has a1
1,1 = 0.001. This is a small pivot and is ultimately responsible for
giving us ‚Äúbig‚Äù triangular factors. Clearly, the smaller the pivot the bigger the
potential problem. Golub and Van Loan‚Äôs [5] Theorem 3.3.2 (which we will not
repeat here) goes on to demonstrate that the errors in the computed triangular
factors can adversely affect the solution to Ax = LUx = y as obtained by forward
substitution and backward substitution. Thus, if we use the computed solutions ÀÜL
and ÀÜU in ÀÜL ÀÜU ÀÜx = y, then the computed solution ÀÜx may not be close to x.
How may our Gaussian elimination LU factorization algorithm be modiÔ¨Åed to
make it more stable? The standard solution is to employ partial pivoting. We do not
consider the method in detail here, but illustrate it with a simple example (Example
4.5). Essentially, before applying a Gauss transformation Gk, the rows of matrix
Ak‚àí1 are permuted (i.e., exchanged) in such a manner as to make the pivot as
large as possible while simultaneously ensuring that Ak is as close to being upper
triangular as possible. Permutation operations have a matrix description, and such
matrices may be denoted by Pk. We remark that P ‚àí1
k
= P T
k .
Example 4.5
Suppose that
A =
Ô£Æ
Ô£∞
1
4
1
2
8
1
0
‚àí1
1
Ô£π
Ô£ª= A0.
TLFeBOOK

160
LINEAR SYSTEMS OF EQUATIONS
Thus
G1P1A0 =
Ô£Æ
Ô£∞
1
0
0
‚àí1
2
1
0
0
0
1
Ô£π
Ô£ª
Ô£Æ
Ô£∞
0
1
0
1
0
0
0
0
1
Ô£π
Ô£ª
Ô£Æ
Ô£∞
1
4
1
2
8
1
0
‚àí1
1
Ô£π
Ô£ª
=
Ô£Æ
Ô£∞
1
0
0
‚àí1
2
1
0
0
0
1
Ô£π
Ô£ª
Ô£Æ
Ô£∞
2
8
1
1
4
1
0
‚àí1
1
Ô£π
Ô£ª=
Ô£Æ
Ô£∞
2
8
1
0
0
1
2
0
‚àí1
1
Ô£π
Ô£ª= A1,
and
G2P2A1 =
Ô£Æ
Ô£∞
1
0
0
0
1
0
0
0
1
Ô£π
Ô£ª
Ô£Æ
Ô£∞
1
0
0
0
0
1
0
1
0
Ô£π
Ô£ª
Ô£Æ
Ô£∞
2
8
1
0
0
1
2
0
‚àí1
1
Ô£π
Ô£ª
=
Ô£Æ
Ô£∞
1
0
0
0
1
0
0
0
1
Ô£π
Ô£ª
Ô£Æ
Ô£∞
2
8
1
0
‚àí1
1
0
0
1
2
Ô£π
Ô£ª= A2
for which U = A2. We see that P2 interchanges rows 2 and 3 rather than 1 and
2 because to do otherwise would ruin the upper triangular structure we seek. It is
apparent that
G2P2G1P1A = U,
so that
A = P ‚àí1
1 G‚àí1
1 P ‚àí1
2 G‚àí1
2
#
$%
&
=L
U,
for which
L =
Ô£Æ
Ô£∞
1
2
0
1
1
0
0
0
1
0
Ô£π
Ô£ª.
This matrix is manifestly not lower triangular. Thus, our use of partial pivoting to
achieve algorithmic stability has been purchased at the expense of some loss of
structure (although Theorem 3.4.1 in Ref. 5 shows how to recover much of what
is lost.7) Also, permutations involve moving data around in the computer, and this
is a potentially signiÔ¨Åcant cost. But these prices are usually worth paying.
7In general, the Gaussian elimination with partial pivoting algorithm generates
Gn‚àí1Pn‚àí1 ¬∑ ¬∑ ¬∑ G2P2G1P1A = U,
and it turns out that
Pn‚àí1 ¬∑ ¬∑ ¬∑ P2P1A = LU
for which L is unit lower triangular, and U is upper triangular. The expression for L in terms of the
factors Gk is messy, and so we omit it. The interested reader can see pp. 112‚Äì113 of Ref. 5 for details.
TLFeBOOK

LEAST-SQUARES PROBLEMS AND QR DECOMPOSITION
161
It is worth mentioning that the need to trade off algorithm speed in favor of sta-
bility is common in numerical computing; that is, fast algorithms often have stability
problems. Much of numerical computing is about creating the fastest possible stable
algorithms. This is a notoriously challenging engineering problem.
A much more detailed account of Gaussian elimination with partial pivoting
appears in Golub and Van Loan [5, pp. 108‚Äì116]. This matter will not be discussed
further in this book.
4.6
LEAST-SQUARES PROBLEMS AND QR DECOMPOSITION
In this section we consider the QR decomposition of A ‚ààRm√ón for which m ‚â•n,
and A is of full rank [i.e., rank (A) = n]. Full rank in this sense means that the
columns of A are linearly independent. The QR decomposition of A is
A = QR,
(4.93)
where Q ‚ààRm√óm is an orthogonal matrix [i.e., QT Q = QQT = I (identity
matrix)], and R ‚ààRm√ón is upper triangular in the following sense:
R =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
r0,0
r0,1
¬∑ ¬∑ ¬∑
r0,n‚àí1
0
r1,1
¬∑ ¬∑ ¬∑
r1,n‚àí1
...
...
...
0
0
¬∑ ¬∑ ¬∑
rn‚àí1,n‚àí1
0
0
¬∑ ¬∑ ¬∑
0
...
...
...
0
0
¬∑ ¬∑ ¬∑
0
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
=
 R
0

.
(4.94)
Here R ‚ààRn√ón is a square upper triangular matrix and is nonsingular because A
is full rank. The bottom block of zeros in R of (4.94) is (m ‚àín) √ó n.
It should be immediately apparent that the existence of a QR decomposition
for A makes it quite easy to solve for x in Ax = y, if A‚àí1 exists (which implies
that in this special case A is square). Thus, Ax = QRx = y, and so Rx = QT y.
The upper triangular linear system Rx = QT y may be readily solved by backward
substitution (recall the previous section).
The case where m > n is important because it arises in overdetermined least-
squares approximation problems.We illustrate with the following example based on
a real-world problem.8 Figure 4.1 is a plot of some simulated body core temperature
8This example is from the problem of estimating the circadian rhythm parameters of human patients
who have sustained head injuries. The estimates are obtained by the suitable processing of various
physiological data sets (e.g., body core temperature, heart rate, blood pressure). The nature of the injury
has made the patients‚Äô rhythms deviate from the nominal 24-h cycle. Correct estimation of rhythm
parameters can lead to improved clinical treatment because of improved timing in the administering of
TLFeBOOK

162
LINEAR SYSTEMS OF EQUATIONS
0
10
20
30
40
50
60
70
80
90
36.8
36.9
37
37.1
37.2
 Time (hours) 
Patient temperature (Celcsius)
Illustration of least-squares fitting
Noisy data with trend 
Linear trend component 
Model
Figure 4.1
Simulated human patient temperature data to illustrate overdetermined least-
squares model parameter estimation. Here we have N = 1000 samples fn (the dots), for
Ts = 300 (seconds), T = 24 (hours), a = 2 √ó 10‚àí7‚ó¶C/s, b = 37‚ó¶C, and c = 0.1‚ó¶C. The
solution to (4.103) is ÀÜa = 2.0582 √ó 10‚àí7‚ó¶C/s, ÀÜb = 36.9999‚ó¶C, ÀÜc = 0.1012‚ó¶C.
measurements from a human patient (this is the noisy data with trend). The data
has three components:
1. A sinusoidal component
2. Random noise.
3. A linear trend.
Our problem is to estimate the parameters of the sinusoid (i.e., the amplitude,
period, and phase), which represents the patient‚Äôs circadian rhythm. In other words,
the noise and trend are undesirable and so are to be, in effect, removed from the
desired sinusoidal signal component. Here we will content ourselves with estimating
only the amplitude of the sinusoid. The problem of estimating the remaining param-
eters is tougher. Methods to estimate the remaining parameters will be considered
later. (This is a nonlinear optimization problem.)
We assume the model for the data in Fig. 4.1 is the analog signal
f (t) = at + b + c sin
 2œÄ
T t
!
+ Œ∑(t).
(4.95)
Here the Ô¨Årst two terms model the trend (assumed to be a straight line), the third
term is the desired sinusoidal signal component, and Œ∑(t) is a random noise com-
ponent. We only possess samples of the signal fn = f (nTs) (i.e., t = nTs), for
n = 0, 1, . . . , N ‚àí1, where Ts is the sampling period of the data collection system.
medication. We emphasize that the model in (4.95) is grossly oversimpliÔ¨Åed. Indeed, a better model is
to replace term at + b with subharmonic, and harmonic terms of sin

2œÄ
T t

. A harmonic term is one
of frequency 2œÄ
T n, while a subharmonic has frequency 2œÄ
T
1
n . Cosine terms should also be included in
the improved model.
TLFeBOOK

LEAST-SQUARES PROBLEMS AND QR DECOMPOSITION
163
We assume that we know T which is the period of the patient‚Äôs circadian rhythm.
Our model also implicitly assumes knowledge of the phase of the sinusoid, too.
These are very artiÔ¨Åcial assumptions since in practice these are the most important
parameters we are trying to estimate, and they are never known in advance. How-
ever, our present circumstances demand simpliÔ¨Åcation. Our estimate of fn may be
deÔ¨Åned by
ÀÜfn = aTsn + b + c sin
 2œÄ
T nTs
!
.
(4.96)
This is a sampled version of the analog model, except the noise term has been
deleted.
We may estimate the unknown model parameters a, b, c by employing the same
basic strategy we used in Section 4.2, speciÔ¨Åcally, a least-squares approach. Thus,
deÔ¨Åning x = [a
b
c]T (vector of unknown parameters), we strive to minimize
V (x) =
N‚àí1

n=0
e2
n =
N‚àí1

n=0
[fn ‚àíÀÜfn]2
(4.97)
with respect to x. Using matrix/vector notation was very helpful in Section 4.2,
and it remains so here. DeÔ¨Åne
vn =

Tsn
1
sin
 2œÄ
T Tsn
!T
.
(4.98)
Thus
en = fn ‚àívT
n x.
(4.99)
We may deÔ¨Åne the error vector e = [e0 e1
¬∑ ¬∑ ¬∑
eN‚àí1]T , data vector f =
[f0 f1 ¬∑ ¬∑ ¬∑ fN‚àí1]T , and the matrix of basis vectors
A =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
vT
0
vT
1
...
vT
N‚àí1
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
‚ààRN√ó3.
(4.100)
Consequently, via (4.99)
e = f ‚àíAx.
(4.101)
Obviously, we would like to have e = 0, which implies the desire to solve Ax = f .
If we have N = 3 and A‚àí1 exists, then we may uniquely solve for x given any
f . However, in practice, N >> 3, so our linear system is overdetermined. Thus,
no unique solution is possible. We have no option but to select x to minimize e in
TLFeBOOK

164
LINEAR SYSTEMS OF EQUATIONS
some sense. Once again, previous experience from Section 4.2 says least-squares
is a viable choice. Thus, since ||e||2
2 = eT e = N‚àí1
n=0 e2
n, we consider
V (x) = eT e = f T f
#$%&
=œÅ
‚àí2xT AT f
#$%&
=g
+ xT AT A
#$%&
=P
x
(4.102)
[which is a more compact version of (4.97)]. This is yet another quadratic form
[recall (4.8)]. We see that P ‚ààR3√ó3, and g ‚ààR3. In our problem A is full rank so
from the results in Section 4.2 we see that P > 0. Naturally, from the discussions
of Sections 4.3 and 4.4, the conditioning of P is a concern. Here it turns out that
because P is of low order (largely because we are interested only in estimating
three parameters) it typically has a low condition number. However, as the order
of P rises, the conditioning of P usually rapidly worsens; that is, ill conditioning
tends to be a severe problem when the number of parameters to be estimated rises.
From Section 4.2 we know that the optimum choice for x, denoted ÀÜx, is obtained
by solving the linear system
P ÀÜx = g.
(4.103)
The model curve of Fig. 4.1 (solid line) is the curve obtained using ÀÜx in (4.96).
Thus, since ÀÜx = [ÀÜa ÀÜb ÀÜc]T , we plot
ÀÜfn for ÀÜa, ÀÜb, ÀÜc in place of a, b, c in (4.96).
Equation (4.103) can be written as
AT AÀÜx = AT f.
(4.104)
This is just the overdetermined linear system AÀÜx = f multiplied on the left (i.e.,
premultiplied) by AT . The system (4.104) is often referred to in the literature as
the normal equations.
How is the previous applications example relevant to the problem of QR fac-
torizing A as in Eq. (4.93)? To answer this, we need to consider the condition
numbers of A, and of P = AT A, and to see how orthogonal matrices Q facili-
tate the solution of overdetermined least-squares problems. We will then move on
to the problem of how to practically compute the QR factorization of a full-rank
matrix. We will consider the issue of conditioning Ô¨Årst since this is a justiÔ¨Åcation
for considering QR factorization methods as opposed to the linear system solution
methods of the previous section.
Singular values were mentioned in Section 4.4 as being relevant to the problem
of computing spectral norms, and so of computing Œ∫2(A). Now we need to consider
the consequences of
Theorem 4.4: Singular Value Decomposition (SVD)
Suppose A ‚ààRm√ón;
then there exist orthogonal matrices
U = [u0u1 ¬∑ ¬∑ ¬∑ um‚àí1] ‚ààRm√óm, V = [v0v1 ¬∑ ¬∑ ¬∑ vn‚àí1] ‚ààRn√ón
such that
 = UT AV = diag (œÉ0, œÉ1, . . . , œÉp‚àí1) ‚ààRm√ón, p = min{m, n},
(4.105)
where œÉ0 ‚â•œÉ1 ‚â•. . . ‚â•œÉp‚àí1 ‚â•0.
TLFeBOOK

LEAST-SQUARES PROBLEMS AND QR DECOMPOSITION
165
An outline proof appears in Ref. 5 (p. 71) and is omitted here. The nota-
tion diag(œÉ0, . . . , œÉp‚àí1) means a diagonal matrix with main diagonal elements
œÉ0, . . . , œÉp‚àí1. For example, if m = 3, n = 2, then p = 2, and
UT AV =
Ô£Æ
Ô£∞
œÉ0
0
0
œÉ1
0
0
Ô£π
Ô£ª,
but if m = 2, n = 3, then again p = 2, but now
UT AV =
 œÉ0
0
0
0
œÉ1
0

.
The numbers œÉi are called singular values. Vector ui is the ith left singular vector,
and vi is the ith right singular vector. The following notation is helpful:
œÉi(A) = the ith singular value of A (i ‚ààZp).
œÉmax(A) = the biggest singular value of A.
œÉmin(A) = the smallest singular value of A.
We observe that because AV = U, and AT U = V T we have, respectively
Avi = œÉiui, AT ui = œÉivi
(4.106)
for i ‚ààZp. Singular values give matrix 2-norms; as noted in the following theorem.
Theorem 4.5:
||A||2 = œÉ0 = œÉmax(A).
Proof
Recall the result (4.37). From (4.105) A = UV T so
||Ax||2
2 = xT AT Ax,
and
AT A = V T V T =
p‚àí1

i=0
œÉ 2
i vivT
i ‚ààRn√ón.
(4.107)
For any x ‚ààRn there exist di such that
x =
n‚àí1

i=0
divi
(4.108)
(because V is orthogonal so its column vectors form an orthogonal basis for Rn).
Thus
||x||2
2 = xT x =
n‚àí1

i=0
n‚àí1

j=0
didjvT
i vj =
n‚àí1

i=0
d2
i
(4.109)
TLFeBOOK

166
LINEAR SYSTEMS OF EQUATIONS
(via vT
i vj = Œ¥i‚àíj). Now
xT AT Ax =
p‚àí1

i=0
œÉ 2
i (xT vi)(vT
i x) =
p‚àí1

i=0
œÉ 2
i ‚ü®x, vi‚ü©2,
(4.110)
but
‚ü®x, vi‚ü©=

j
djvj, vi

=

j
dj‚ü®vi, vj‚ü©= di.
(4.111)
Using (4.111) in (4.110), we obtain
||Ax||2
2 =
n‚àí1

i=0
œÉ 2
i d2
i
(4.112)
for which it is understood that œÉ 2
i = 0 for i > p ‚àí1. We maximize ||Ax||2
2 subject
to constraint ||x||2
2 = 1, which means employing Lagrange multipliers; that is, we
maximize
L(d) =
n‚àí1

i=0
œÉ 2
i d2
i ‚àíŒª
'n‚àí1

i=0
d2
i ‚àí1
(
,
(4.113)
where d = [d0 d1 ¬∑ ¬∑ ¬∑ dn‚àí1]T . Thus
‚àÇL(d)
‚àÇdj
= 2œÉ 2
j dj ‚àí2Œªdj = 0,
or
œÉ 2
j dj = Œªdj.
(4.114)
From (4.114) into (4.112)
||Ax||2
2 = Œª
n‚àí1

i=0
d2
i = Œª
(4.115)
for which we have used the fact that ||x||2
2 = 1 in (4.109). From (4.114) Œª is the
eigenvalue of a diagonal matrix containing œÉ 2
i . Consequently, Œª is maximized for
Œª = œÉ 2
0 . Therefore, ||A||2 = œÉ0.
Suppose that
œÉ0 ‚â•¬∑ ¬∑ ¬∑ ‚â•œÉr‚àí1 > œÉr = ¬∑ ¬∑ ¬∑ = œÉp‚àí1 = 0,
(4.116)
then
rank (A) = r.
(4.117)
TLFeBOOK

LEAST-SQUARES PROBLEMS AND QR DECOMPOSITION
167
Thus, the SVD of A can tell us the rank of A. In our overdetermined least-squares
problem we have m ‚â•n and A is assumed to be of full-rank. This implies that
r = n. Also, p = n. Thus, all singular values of a full-rank matrix are bigger than
zero. Now suppose that A‚àí1 exists. From (4.105) A‚àí1 = V ‚àí1UT . Immediately,
||A‚àí1||2 = 1/œÉmin(A). Hence
Œ∫2(A) = ||A||2||A‚àí1||2 = œÉmax(A)
œÉmin(A) .
(4.118)
Thus, a large singular value spread is associated with matrix ill conditioning. [Recall
(4.61) and the related discussion.] As remarked on p. 223 of Ref. 5, Eq. (4.118)
can be extended to cover full-rank rectangular matrices with m ‚â•n:
A ‚ààRm√ón, rank (A) = n ‚áíŒ∫2(A) = œÉmax(A)
œÉmin(A) .
(4.119)
This also holds for the transpose of A because AT = V T UT , so AT has the
same singular values as A. Thus, Œ∫2(AT ) = Œ∫2(A). Golub and Van Loan [5, p. 225]
claim (without formal proof) that Œ∫2(AT A) = [Œ∫2(A)]2. In other words, if the linear
system Ax = f is ill-conditioned, then AT AÀÜx = AT f is even more ill-conditioned.
The condition number of the latter system is the square of that of the former system.
More information on the conditioning of rectangular matrices is to be found in
Appendix 4.B. This includes justiÔ¨Åcation that Œ∫2(AT A) = [Œ∫2(A)]2.
A popular approach toward solving the normal equations AT AÀÜx = AT f is based
on Cholesky decomposition
Theorem 4.6: Cholesky Decomposition
If R ‚ààRn√ón is symmetric and pos-
itive deÔ¨Ånite, then there exists a unique lower triangular matrix L ‚ààRn√ón with
positive diagonal entries such that R = LLT . This is the Cholesky decomposition
(factorization) of R.
Algorithms to Ô¨Ånd this decomposition appear in Chapter 4 of Ref. 5. We do not
consider them except to note that if they are used, then the computed solution to
AT AÀÜx = AT f , which we denote by ÀÜÀÜx, may satisfy
||ÀÜÀÜx ‚àíÀÜx||2
||ÀÜx||2
‚âàu[Œ∫2(A)]2,
(4.120)
where u is as in (4.89). Thus, this method of linear system solution is potentially
highly susceptible to errors due to ill-conditioned problems. On the other hand,
Cholesky approaches are computationally efÔ¨Åcient in that they require about n3/3
Ô¨Çops (Floating-point operations). Clearly, Gaussian elimination may be employed
to solve the normal equations as well, but we recall that Gaussian elimination
needed about 2n3/3 Ô¨Çops. Gaussian elimination is less efÔ¨Åcient because it does
not account for symmetry in matrix R. Note that these counts do not take into
consideration the number of Ô¨Çops needed to determine AT A and AT f , and do
TLFeBOOK

168
LINEAR SYSTEMS OF EQUATIONS
not account for the number of Ô¨Çops needed by the forward/backward substitution
steps. However, the comparison between Cholesky decomposition and Gaussian
elimination is reasonably fair because these other steps are essentially the same for
both approaches.
Recall that ||e||2
2 = ||Ax ‚àíf ||2
2. Thus, for orthogonal matrix Q
||QT e||2
2 = [QT e]T QT e = eT QQT e = eT e = ||e||2
2.
(4.121)
Thus, the 2-norm is invariant to orthogonal transformations. This is one of the
more important properties of 2-norms. Now consider
||e||2
2 = ||QT Ax ‚àíQT f ||2
2.
(4.122)
Suppose that
QT f =
 f u
f l

(4.123)
for which f u ‚ààRn, and f l ‚ààRm‚àín. Thus, from (4.94) and QT A = R, we obtain
QT Ax ‚àíQT f =

Rx ‚àíf u
‚àíf l
	
,
(4.124)
implying that
||e||2
2 = ||Rx ‚àíf u||2
2 + ||f l||2
2.
(4.125)
Immediately, we see that
RÀÜx = f u.
(4.126)
The least-squares optimal solution ÀÜx is therefore found by backward substitution.
Equally clearly, we see that
min
x ||e||2
2 = ||f l||2
2 = œÅ2
LS.
(4.127)
This is the minimum error energy. Quantity œÅ2
LS is also called the minimum sum
of squares, and e is called the residual [5]. It is easy to verify that Œ∫2(Q) = 1
(Q is orthogonal). In other words, orthogonal matrices are perfectly conditioned.
This means that the operation QT A will not result in a matrix that is not as well
conditioned as A. This in turn suggests that solving our least-squares problem
using QR decomposition might be numerically more reliable than working with
the normal equations. As explained on p. 230 of Ref. 5, this is not necessarily
always true, but it is nevertheless a good reason to contemplate QR approaches to
solving least-squares problems.9
9If the residual is big and the problem is ill-conditioned, then neither QR nor normal equation methods
may give an accurate answer. However, QR approaches may be more accurate for small residuals in
ill-conditioned problems than normal equation approaches.
TLFeBOOK

LEAST-SQUARES PROBLEMS AND QR DECOMPOSITION
169
How may we compute Q? There are three major approaches:
1. Gram‚ÄìSchmidt algorithms
2. Givens rotation algorithms
3. Householder transformation algorithms
We will consider only Householder transformations.
We begin by a review of how vectors are projected onto vectors. Recall the
law of cosines from trigonometry in reference to Fig. 4.2a. Assume that x, y ‚ààRn.
Suppose that ||x ‚àíy||2 = a, ||x||2 = b, and that ||y||2 = c. Therefore, where Œ∏ is
the angle between x and y (0 ‚â§Œ∏ ‚â§œÄ radians)
a2 = b2 + c2 ‚àí2bc cos Œ∏,
(4.128)
or in terms of the vectors x and y, Eq. (4.128) becomes
||x ‚àíy||2
2 = ||x||2
2 + ||y||2
2 ‚àí2||x||2||y||2 cos Œ∏.
(4.129)
In terms of inner products, this becomes
‚ü®x ‚àíy, x ‚àíy‚ü©= ‚ü®x, x‚ü©+ ‚ü®y, y‚ü©‚àí2[‚ü®x, x‚ü©]1/2[‚ü®y, y‚ü©]1/2 cos Œ∏,
which reduces to
‚ü®x, y‚ü©= [‚ü®x, x‚ü©]1/2[‚ü®y, y‚ü©]1/2 cos Œ∏,
or
‚ü®x, y‚ü©= ||x||2||y||2 cos Œ∏.
(4.130)
q
x ‚àí y
y
x
(a)
q
Pyx
y
z
x
(b)
Figure 4.2
Illustration of the law of cosines (a) and the projection of vector x onto vector
y (b).
TLFeBOOK

170
LINEAR SYSTEMS OF EQUATIONS
Now consider Fig. 4.2b. Vector Pyx is the projection of x onto y, where Py denotes
the projection operator that projects x onto y. It is immediately apparent that
||Pyx||2 = ||x||2 cos Œ∏.
(4.131)
This is the Euclidean length of Pyx. The unit vector in the direction of y is y/||y||2.
Therefore
Pyx = ||x||2 cos Œ∏
||y||2
y.
(4.132)
But from (4.130) this becomes
Pyx = ‚ü®x, y‚ü©
||y||2
2
y.
(4.133)
Since ‚ü®x, y‚ü©= xT y = yT x, we see that
Pyx = yT x
||y||2
2
y =

1
||y||2
2
yyT
	
x.
(4.134)
In (4.134) yyT ‚ààRn√ón, so the operator Py has the matrix representation
Py =
1
||y||2
2
yyT .
(4.135)
In Fig. 4.2b we see that z = x ‚àíPyx, and that
z = (I ‚àíPy)x =

I ‚àí
1
||y||2
2
yyT
	
x,
(4.136)
which is the component of x that is orthogonal to y. We observe that
P 2
y =
1
||y||4
2
yyT yyT = y||y||2
2yT
||y||4
2
=
1
||y||2
2
yyT = Py.
(4.137)
If A2 = A, we say that matrix A is idempotent. Thus, projection operators are
idempotent. Also, P T
y = Py so projection operators are also symmetric.
In Fig. 4.3, x, y, z ‚ààRn, and yT z = 0. DeÔ¨Åne the Householder transformation
matrix
H = I ‚àí2 yyT
||y||2
2
.
(4.138)
We see that H = I ‚àí2Py [via (4.135)]. Hence Hx is as shown in Fig. 4.3; that
is, the Householder transformation Ô¨Ånds the reÔ¨Çection of vector x with respect to
vector z, and z ‚ä•y (DeÔ¨Ånition 1.6 of Chapter 1). Recall the unit vector ei ‚ààRn
ei = [0 ¬∑ ¬∑ ¬∑ 0
# $% &
i zeros
10 ¬∑ ¬∑ ¬∑ 0]T ,
TLFeBOOK

LEAST-SQUARES PROBLEMS AND QR DECOMPOSITION
171
Hx
z
y
x
Pyx
Figure 4.3
Geometric interpretation of the Householder transformation operator H. Note
that zT y = 0.
so e0 = [10 ¬∑ ¬∑ ¬∑ 0]T . Suppose that we want Hx = ae0 for some a ‚ààR with a Ã∏= 0;
that is, we wish to design H to annihilate all elements of x except for the top
element. Let y = x + Œ±e0; then
yT x = (xT + Œ±eT
0 )x = xT x + Œ±x0
(4.139a)
(as x = [x0 x1 ¬∑ ¬∑ ¬∑ xn‚àí1]T ), and
||y||2
2 = (xT + Œ±eT
0 )(x + Œ±e0) = xT x + 2Œ±x0 + Œ±2.
(4.139b)
Therefore
Hx = x ‚àí2 yyT
||y||2
2
x = x ‚àí2 yT x
||y||2
2
y
so from (4.139), this becomes
Hx = x ‚àí2(xT x + Œ±x0)(x + Œ±e0)
||y||2
2
= x ‚àí2(xT x + Œ±x0)x + Œ±(yT x)e0
||y||2
2
=

1 ‚àí2
xT x + Œ±x0
xT x + 2Œ±x0 + Œ±2

x ‚àí2Œ± yT x
||y||2
2
e0.
(4.140)
To force the Ô¨Årst term to zero, we require
xT x + 2Œ±x0 + Œ±2 ‚àí2(xT x + Œ±x0) = 0,
which implies that Œ±2 = xT x, or in other words, we need
Œ± = ¬±||x||2.
(4.141)
TLFeBOOK

172
LINEAR SYSTEMS OF EQUATIONS
Consequently, we select y = x ¬± ||x||2e0. In this case
Hx = ‚àí2Œ± yT x
||y||2
2
e0 = ‚àí2Œ±
xT x + Œ±x0
xT x + 2Œ±x0 + Œ±2 e0
= ‚àí2Œ± Œ±2 + Œ±x0
2Œ±2 + 2Œ±x0
e0 = ‚àíŒ±e0 = ‚àì||x||2e0,
(4.142)
so Hx = ae0 for a = ‚àíŒ± if y = x + Œ±e0 with Œ± = ¬±||x||2.
Example 4.6
Suppose x = [4
3
0]T , so ||x||2 = 5. Choose Œ± = 5. Thus
y = [9
3
0]T , and
H = I ‚àí2yyT
yT y = 1
45
Ô£Æ
Ô£∞
‚àí36
‚àí27
0
‚àí27
36
0
0
0
45
Ô£π
Ô£ª.
We see that
Hx = 1
45
Ô£Æ
Ô£∞
‚àí36
‚àí27
0
‚àí27
36
0
0
0
45
Ô£π
Ô£ª
Ô£Æ
Ô£∞
4
3
0
Ô£π
Ô£ª= 1
45
Ô£Æ
Ô£∞
‚àí225
0
0
Ô£π
Ô£ª=
Ô£Æ
Ô£∞
‚àí5
0
0
Ô£π
Ô£ª,
so Hx = ‚àíŒ±e0.
The Householder transformation is designed to annihilate elements of vectors. But
in contrast with the Gauss transformations of Section 4.5, Householder matrices
are orthogonal. To see this observe that
H T H =

I ‚àí2yyT
yT y
 
I ‚àí2yyT
yT y

= I ‚àí4yyT
yT y + 4yyT yyT
[yT y]2
= I ‚àí4yyT
yT y + 4yyT
yT y = I.
Thus, no matter how we select y, Œ∫2(H) = 1. Householder matrices are therefore
perfectly conditioned.
To obtain R in (4.94), we deÔ¨Åne
ÀúHk =
 Ik‚àí1
0
0
Hk

‚ààRm√óm,
(4.143)
TLFeBOOK

LEAST-SQUARES PROBLEMS AND QR DECOMPOSITION
173
where k = 1, 2, . . . , n, Ik‚àí1 is an order k ‚àí1 identity matrix, Hk is an order m ‚àí
k + 1 Householder transformation matrix. We design Hk to annihilate elements k
to m ‚àí1 of column k ‚àí1 in Ak‚àí1, where A0 = A, and
Ak = ÀúHkAk‚àí1,
(4.144)
so An = R (in (4.94)). Much as in Section 4.5, we have Ak = [ak
i,j] ‚ààRm√ón, and
we assume m > n.
Example 4.7
Suppose
A = A0 =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
a0
00
a0
01
a0
02
a0
10
a0
11
a0
12
a0
20
a0
21
a0
22
a0
30
a0
31
a0
32
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
‚ààR4√ó3,
and so therefore
A1 = ÀúH1A0 =
Ô£Æ
Ô£ØÔ£ØÔ£∞
√ó
√ó
√ó
√ó
√ó
√ó
√ó
√ó
√ó
√ó
√ó
√ó
√ó
√ó
√ó
√ó
Ô£π
Ô£∫Ô£∫Ô£ª
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
a0
00
a0
01
a0
02
a0
10
a0
11
a0
12
a0
20
a0
21
a0
22
a0
30
a0
31
a0
32
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
=
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
a1
00
a1
01
a1
02
0
a1
11
a1
12
0
a1
21
a1
22
0
a1
31
a1
32
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
,
A2 = ÀúH2A1 =
Ô£Æ
Ô£ØÔ£ØÔ£∞
1
0
0
0
0
√ó
√ó
√ó
0
√ó
√ó
√ó
0
√ó
√ó
√ó
Ô£π
Ô£∫Ô£∫Ô£ª
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
a1
00
a1
01
a1
02
0
a1
11
a1
12
0
a1
21
a1
22
0
a1
31
a1
32
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
=
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
a1
00
a1
01
a1
02
0
a2
11
a2
12
0
0
a2
22
0
0
a2
32
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
,
and
A3 = ÀúH3A2 =
Ô£Æ
Ô£ØÔ£ØÔ£∞
1
0
0
0
0
1
0
0
0
0
√ó
√ó
0
0
√ó
√ó
Ô£π
Ô£∫Ô£∫Ô£ª
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
a1
00
a1
01
a1
02
0
a2
11
a2
12
0
0
a2
22
0
0
a2
32
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
=
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
a1
00
a1
01
a1
02
0
a2
11
a2
12
0
0
a3
22
0
0
0
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
= R.
TLFeBOOK

174
LINEAR SYSTEMS OF EQUATIONS
The √ó signs denote the Householder matrix elements that are not speciÔ¨Åed. This
example is intended only to show the general pattern of elements in the matrices.
DeÔ¨Åne
xk =

ak‚àí1
k‚àí1,k‚àí1 ak‚àí1
k,k‚àí1 ¬∑ ¬∑ ¬∑ ak‚àí1
m‚àí1,k‚àí1
T
‚ààRm‚àík+1,
(4.145)
so if xk =

xk
0 xk
1 ¬∑ ¬∑ ¬∑ xk
m‚àík
T then xk
i = ak‚àí1
i+k‚àí1,k‚àí1, and so
Hk = Im‚àík+1 ‚àí2yk(yk)T
(yk)T yk ,
(4.146)
where yk = xk ¬± ||xk||2ek
0, and ek
0 = [1 0 ¬∑ ¬∑ ¬∑ 0]T ‚ààRm‚àík+1. A pseudocode anal-
ogous to that for Gaussian elimination (recall Section 4.5) is as follows:
A0 := A;
for k := 1 to n do begin
for i := 0 to m ‚àík do begin
xk
i := ak‚àí1
i+k‚àí1,k‚àí1; {This loop makes xk}
end;
yk := xk + sign(xk
0)||xk||2ek
0;
Ak := ÀúHkAk‚àí1; { ÀúHk contains Hk via (4.146) }
end;
R := An;
From (4.143) ÀúH T
k ÀúHk = Im because H T
k Hk = Im‚àík+1, and of course I T
k‚àí1Ik‚àí1 =
Ik‚àí1; that is, ÀúHk is orthogonal for all k. Since
R = An = ÀúHn ÀúHn‚àí1 ¬∑ ¬∑ ¬∑ ÀúH2 ÀúH1A,
(4.147)
we have
A = ÀúH T
1 ÀúH T
2 ¬∑ ¬∑ ¬∑ ÀúH T
n
#
$%
&
=Q
R.
(4.148)
Thus, the pseudocode above implicitly computes Q because it creates the orthog-
onal factors ÀúHk.
In the pseudocode we see that
yk = xk + sign(xk
0) ||xk||2ek
0.
(4.149)
Recall from (4.141) that Œ± = ¬±||x||2, so we must choose the sign of Œ±. It is best that
Œ± = sign(x0)||x||2, where sign(x0) = +1 for x0 ‚â•0, and sign(x0) = ‚àí1 if x0 < 0.
This turns out to ensure that H remains as close as possible to perfect orthogonality
in the face of rounding errors. Because ||x||2 might be very large or very small,
there is a risk of overÔ¨Çow or underÔ¨Çow in the computation of ||x||2. Thus, it
is often better to compute y from x/||x||‚àû. This works because scaling x does
TLFeBOOK

LEAST-SQUARES PROBLEMS AND QR DECOMPOSITION
175
not mathematically alter H (which may be conÔ¨Årmed as an exercise). Typically,
m >> n (e.g., in the example of Fig. 4.1 we had m = N = 1000, while n = 3),
so, since ÀúHk ‚ààRm√óm, we rarely can accumulate and store the elements of ÀúHk for
all k as too much memory is needed for such a task. Instead, it is much better to
observe that (for example) if H ‚ààRm√óm, A ‚ààRm√ón then, as y ‚ààRm, we have
HA =

I ‚àí2yyT
yT y

A = A ‚àí
2
yT y y(AT y)T .
(4.150)
From (4.150) AT y ‚ààRn, which has jth element
[AT y]j =
m‚àí1

k=0
ak,jyk
(4.151)
for j = 0, 1, . . . , n ‚àí1. If Œ≤ = 2/yT y, then, from (4.150) and (4.151), we have
[HA]i,j = ai,j ‚àíŒ≤yi
m‚àí1

k=0
ak,jyk
	
(4.152)
for i = 0, 1, . . . , m ‚àí1, and j = 0, 1, . . . , n ‚àí1. A pseudocode program that
implements this is as follows:
Œ≤ := 2/yTy;
for j = 0 to n ‚àí1 do begin
s := m‚àí1
k=0 ak,jyk;
s := Œ≤s;
for i := 0 to m ‚àí1 do begin
ai,j := ai,j ‚àísyi;
end;
end;
This program is written to overwrite matrix A with matrix HA. This reduces
computer system memory requirements. Recall (4.123), where we see that QT f
must be computed so that f u can be found. Knowledge of f u is essential to
compute ÀÜx via (4.126). As in the problem of computing ÀúHkAk‚àí1, we do not wish
to accumulate and save the factors ÀúHk in
QT f = ÀúHn ÀúHn‚àí1 ¬∑ ¬∑ ¬∑ ÀúH1f.
(4.153)
Instead, QT f would be computed using an algorithm similar to that suggested by
(4.152).
All the suggestions in the previous paragraph are needed in a practical imple-
mentation of the Householder transformation matrix method for QR factorization.
As noted in Ref. 5, the rounding error performance of the practical Householder
QR factorization algorithm is quite good. It is stated [5] as well that the number of
Ô¨Çops needed by the Householder method for Ô¨Ånding ÀÜx is greater than that needed by
TLFeBOOK

176
LINEAR SYSTEMS OF EQUATIONS
Cholesky factorization. Somewhat simplistically, the Cholesky method is computa-
tionally more efÔ¨Åcient than the Householder method, but the Householder method
is less susceptible to ill conditioning and to rounding errors than is the Cholesky
method. More or less, there is therefore a tradeoff between speed and accuracy
involved in selecting between these competing methods for solving the overde-
termined least-squares problem. The Householder approach is also claimed [5] to
require more memory than the Cholesky approach.
4.7
ITERATIVE METHODS FOR LINEAR SYSTEMS
Matrix A ‚ààRn√ón is said to be sparse if most of its n2 elements are zero-valued.
Such matrices can arise in various applications, such as in the numerical solution
of partial differential equations (PDEs). Sections 4.5 and 4.6 have presented such
direct methods as the LU and QR decompositions (factorizations) of A in order to
solve Ax = b (assuming that A is nonsingular). However, these procedures do not
in themselves take advantage of any structure that may be possessed by A such
as sparsity. Thus, they are not necessarily computationally efÔ¨Åcient procedures.
Therefore, in the present section, we consider iterative methods to determine x ‚ààRn
in Ax = b. In this section, whenever we consider Ax = b, we will always assume
that A‚àí1 exists. Iterative methods work by creating a Cauchy sequence of vectors
(x(k)) that converges to x.10 Iterative methods may be particularly advantageous
when A is not only sparse, but is also large (i.e., large n). This is because direct
methods often require the considerable movement of data around the computing
machine memory system, and this can slow the computation down substantially. But
a properly conceived and implemented iterative method can alleviate this problem.
Our presentation of iterative methods here is based largely on the work of
Quarteroni et al. [8, Chapter 4]. We use much of the same notation as that in
Ref. 8. But it is a condensed presentation as this section is intended only to convey
the main ideas about iterative linear system solvers.
In Section 4.4 matrix and vector norms were considered in order to characterize
the sizes of errors in the numerical estimate of x in Ax = b due to perturbations
of A, and b. We will need to consider such norms here. As noted above, our goal
here is to derive a methodology to generate vector sequence (x(k))11 such that
lim
k‚Üí‚àûx(k) = x,
(4.154)
where x = [x0 x1 ¬∑ ¬∑ ¬∑ xn‚àí1]T ‚ààRn satisÔ¨Åes Ax = b and x(k) = [x(k)
0
x(k)
1
¬∑ ¬∑ ¬∑
x(k)
n‚àí1]T ‚ààRn. The basic idea is to Ô¨Ånd an operator T such that x(k+1) = T x(k)(=
T (x(k))), for k = 0, 1, 2, . . .. Because (x(k)) is designed to be Cauchy (recall
10As such, we will be revisiting ideas Ô¨Årst seen in Section 3.2.
11Note that the ‚Äú(k)‚Äù in x(k) does not denote the raising of x to a power or the taking of the kth
derivative, but rather is part of the name of the vector. Similar notation applies to matrices. So, Ak is
the kth power of A, but A(k) is not.
TLFeBOOK

ITERATIVE METHODS FOR LINEAR SYSTEMS
177
Section 3.2) for any œµ > 0, there will be an m ‚ààZ+ such that ||x(m) ‚àíx|| < œµ
[recall that d(x(k), x) = ||x(k) ‚àíx||]. The operator T is deÔ¨Åned according to
x(k+1) = Bx(k) + f,
(4.155)
where x(0) ‚ààRn is the starting value (initial guess about the solution x), B ‚ààRn√ón
is called the iteration matrix, and f ‚ààRn is derived from A and b in Ax = b. Since
we want (4.154) to hold, from (4.155) we seek B and f such that x = Bx + f , or
A‚àí1b = BA‚àí1b + f (using Ax = b, implying x = A‚àí1b), so
f = (I ‚àíB)A‚àí1b.
(4.156)
The error vector at step k is deÔ¨Åned to be
e(k) = x(k) ‚àíx,
(4.157)
and naturally we want limk‚Üí‚àûe(k) = 0. Convergence would be in some suitably
selected norm.
As matters now stand, there is no guarantee that (4.154) will hold. We achieve
convergence only by the proper selection of B, and for matrices A possessing
suitable properties (considered below). Before we can consider these matters we
require certain basic results involving matrix norms.
DeÔ¨Ånition 4.3: Spectral Radius
Let s(A) denote the set of eigenvalues of
matrix A ‚ààRn√ón. The spectral radius of A is
œÅ(A) = max
Œª‚ààs(A) |Œª|.
An important property possessed by œÅ(A) is as follows.
Property 4.1
If A ‚ààRn√ón with œµ > 0, then there is a norm denoted || ¬∑ ||œµ
(i.e., a norm perhaps dependent on œµ) satisfying the consistency condition (4.36c),
and such that
||A||œµ ‚â§œÅ(A) + œµ.
Proof
See Isaacson and Keller [9].
This is just a formal way of saying that there is always a matrix norm that is
arbitrarily close to the spectral radius of A
œÅ(A) = inf
||¬∑|| ||A||
(4.158)
with the inÔ¨Åmum (deÔ¨Åned in Section 1.3) taken over all possible norms that satisfy
(4.36c). We say that the sequence of matrices (A(k)) [with A(k) ‚ààRn√ón] converges
to A ‚ààRn√ón iff
lim
k‚Üí‚àû||A(k) ‚àíA|| = 0.
(4.159)
TLFeBOOK

178
LINEAR SYSTEMS OF EQUATIONS
The norm in (4.159) is arbitrary because of norm equivalence (recall discussion on
this idea in Section 4.4).
Theorem 4.7: Let A ‚ààRn√ón; then
lim
k‚Üí‚àûAk = 0 ‚áîœÅ(A) < 1.
(4.160)
As well, the matrix geometric series ‚àû
k=0 Ak converges iff œÅ(A) < 1. In this
instance
‚àû

k=0
Ak = (I ‚àíA)‚àí1.
(4.161)
So, if œÅ(A) < 1, then matrix I ‚àíA is invertible, and also
1
1 + ||A|| ‚â§||(I ‚àíA)‚àí1|| ‚â§
1
1 ‚àí||A||,
(4.162)
where || ¬∑ || here is an induced matrix norm (i.e., (4.36b) holds) such that ||A|| < 1.
Proof
We begin by showing (4.160) holds. Let œÅ(A) < 1 so there must be an
œµ > 0 such that œÅ(A) < 1 ‚àíœµ, and from Property 4.1 there is a consistent matrix
norm || ¬∑ || such that
||A|| ‚â§œÅ(A) + œµ < 1.
Because [recall (4.40)] of ||Ak|| ‚â§||A||k < 1, and the deÔ¨Ånition of convergence,
as k ‚Üí‚àû, we have Ak ‚Üí0 ‚ààRn√ón. Conversely, assume that limk‚Üí‚àûAk = 0,
and let Œª be any eigenvalue of A. For eigenvector x (Ã∏= 0) of A associated with
eigenvalue Œª, we have Akx = Œªkx, and so limk‚Üí‚àûŒªk = 0. Thus, |Œª| < 1, and
hence œÅ(A) < 1. Now consider (4.161). If Œª is an eigenvalue of A, then 1 ‚àíŒª is
an eigenvalue of I ‚àíA. We observe that
(I ‚àíA)(I + A + A2 + ¬∑ ¬∑ ¬∑ + An‚àí1 + An) = I ‚àíAn+1.
(4.163)
Since œÅ(A) < 1, I ‚àíA has an inverse, and letting n ‚Üí‚àûin (4.163) yields
(I ‚àíA)
‚àû

k=0
Ak = I
so that (4.161) holds.
Now, because matrix norm || ¬∑ || satisÔ¨Åes (4.36b), we must have ||I|| = 1. Thus
1 = ||I|| ‚â§||I ‚àíA|| ||(I ‚àíA)‚àí1|| ‚â§(1 + ||A||)||(I ‚àíA)‚àí1||,
TLFeBOOK

ITERATIVE METHODS FOR LINEAR SYSTEMS
179
which gives the Ô¨Årst inequality in (4.162). Since I = (I ‚àíA) + A, we have
(I ‚àíA)‚àí1 = I + A(I ‚àíA)‚àí1
so that
||(I ‚àíA)‚àí1|| ‚â§1 + ||A|| ||(I ‚àíA)‚àí1||.
Condition ||A|| < 1 implies that this yields the second inequality in (4.162).
We mention that in Theorem 4.7 an induced matrix norm exists to give ||A|| <
1 because of Property 4.1 (recall that (A(k)) is convergent, giving œÅ(A) < 1).
Theorem 4.7 now leads us to the following theorem.
Theorem 4.8: Suppose that f ‚ààRn satisÔ¨Åes (4.156); then (x(k)) converges to
x satisfying Ax = b for any x(0) iff œÅ(B) < 1.
Proof
From (4.155)‚Äì(4.157), we have
e(k+1) = x(k+1) ‚àíx = Bx(k) + f ‚àíx = Bx(k) + (I ‚àíB)A‚àí1b ‚àíx
= Be(k) + Bx + (I ‚àíB)A‚àí1b ‚àíx
= Be(k) + Bx + x ‚àíBx ‚àíx
= Be(k).
Immediately, we see that
e(k) = Bke(0)
(4.164)
for k ‚ààZ+. From Theorem 4.7
lim
k‚Üí‚àûBke(0) = 0
for all e(0) ‚ààRn iff œÅ(B) < 1.
On the other hand, suppose œÅ(B) ‚â•1; then there is at least one eigenvalue Œª of
B such that |Œª| ‚â•1. Let e(0) be the eigenvector associated with Œª, so Be(0) = Œªe(0),
implying that e(k) = Œªke(0). But this implies that e(k) Ã∏‚Üí0 as k ‚Üí‚àûsince |Œª| ‚â•1.
This theorem gives a general condition on B so that iterative procedure (4.155)
converges. Theorem 4.9 (below) will say more. However, our problem now is to
Ô¨Ånd B. From (4.158), and Theorem 4.7 a sufÔ¨Åcient condition for convergence is
that ||B|| < 1, for any matrix norm.
A general approach to constructing iterative methods is to use the additive
splitting of the matrix A according to
A = P ‚àíN,
(4.165)
TLFeBOOK

180
LINEAR SYSTEMS OF EQUATIONS
where P, N ‚ààRn√ón are suitable matrices, and P ‚àí1 exists. Matrix P is sometimes
called a preconditioning matrix, or preconditioner (for reasons we will not consider
here, but that are explained in Ref. 8). To be speciÔ¨Åc, we rewrite (4.155) as
x(k+1) = P ‚àí1Nx(k) + P ‚àí1b,
that is, for k ‚ààZ+
P x(k+1) = Nx(k) + b,
(4.166)
so that f = P ‚àí1b, and B = P ‚àí1N. Alternatively
x(k+1) = x(k) + P ‚àí1 [b ‚àíAx(k)]
#
$%
&
=r(k)
,
(4.167)
where r(k) is the residual vector at step k. From (4.167) we see that to obtain
x(k+1) requires us to solve a linear system of equations involving P . Clearly, for
this approach to be worth the trouble, P must be nonsingular, and be easy to invert
as well in order to save on computations.
We will now make the additional assumption that the main diagonal elements of
A are nonzero (i.e., ai,i Ã∏= 0 for all i ‚ààZn). All the iterative methods we consider in
this section will assume this. In this case we may express Ax = b in the equivalent
form
xi = 1
aii
Ô£Æ
Ô£ØÔ£ØÔ£∞bi ‚àí
n‚àí1

j=0
jÃ∏=i
aijxj
Ô£π
Ô£∫Ô£∫Ô£ª
(4.168)
for i = 0, 1, . . . , n ‚àí1.
The expression (4.168) immediately leads to, for any initial guess x(0), the
Jacobi method, which is deÔ¨Åned by the iterations
x(k+1)
i
= 1
aii
Ô£Æ
Ô£ØÔ£ØÔ£∞bi ‚àí
n‚àí1

j=0
jÃ∏=i
aijx(k)
j
Ô£π
Ô£∫Ô£∫Ô£ª
(4.169)
for i = 0, 1, . . . , n ‚àí1. It is easy to show that this algorithm implements the
splitting
P = D, N = D ‚àíA = L + U,
(4.170)
where D = diag(a0,0, a1,1, . . . , an‚àí1,n‚àí1) (i.e., diagonal matrix that is the main
diagonal elements of A), L is the lower triangular matrix such that lij = ‚àíaij
if i > j, and lij = 0 if i ‚â§j, and U is the upper triangular matrix such that
uij = ‚àíaij if j > i, and uij = 0 if j ‚â§i. Here the iteration matrix B is given by
B = BJ = P ‚àí1N = D‚àí1(L + U) = I ‚àíD‚àí1A.
(4.171)
TLFeBOOK

ITERATIVE METHODS FOR LINEAR SYSTEMS
181
The Jacobi method generalizes according to
x(k+1)
i
= œâ
aii
Ô£Æ
Ô£ØÔ£ØÔ£∞bi ‚àí
n‚àí1

j=0
jÃ∏=i
aijx(k)
j
Ô£π
Ô£∫Ô£∫Ô£ª+ (1 ‚àíœâ)x(k)
i
,
(4.172)
where i = 0, 1, . . . , n ‚àí1, and œâ is the relaxation parameter. Relaxation parame-
ters are introduced into iterative procedures in order to control convergence rates.
The algorithm (4.172) is called the Jacobi overrelaxation (JOR) method. In this
algorithm the iteration matrix B takes on the form
B = BJ (œâ) = œâBJ + (1 ‚àíœâ)I,
(4.173)
and (4.172) can be expressed in the form (4.167) according to
x(k+1) = x(k) + œâD‚àí1r(k).
(4.174)
The JOR method satisÔ¨Åes (4.156) provided that œâ Ã∏= 0. The method is easily seen
to reduce to the Jacobi method when œâ = 1.
An alternative to the Jacobi method is the Gauss‚ÄìSeidel method. This is deÔ¨Åned
as
x(k+1)
i
= 1
aii
Ô£Æ
Ô£∞bi ‚àí
i‚àí1

j=0
aijx(k+1)
j
‚àí
n‚àí1

j=i+1
aijx(k)
j
Ô£π
Ô£ª,
(4.175)
where i = 0, 1, . . . , n ‚àí1. In matrix form (4.175) can be expressed as
Dx(k+1) = b + Lx(k+1) + Ux(k),
(4.176)
where D, L, and U are the same matrices as those associated with the Jacobi
method. In the Gauss‚ÄìSeidel method we implement the splitting
P = D ‚àíL,
N = U
(4.177)
with the iteration matrix
B = BGS = (D ‚àíL)‚àí1U.
(4.178)
As there is an overrelaxation method for the Jacobi approach, the same idea applies
for the Gauss‚ÄìSeidel case. The Gauss‚ÄìSeidel successive overrelaxation (SOR)
method is deÔ¨Åned to be
x(k+1)
i
= œâ
aii
Ô£Æ
Ô£∞bi ‚àí
i‚àí1

j=0
aijx(k+1)
j
‚àí
n‚àí1

j=i+1
aijx(k)
j
Ô£π
Ô£ª+ (1 ‚àíœâ)x(k)
i
,
(4.179)
TLFeBOOK

182
LINEAR SYSTEMS OF EQUATIONS
again for i = 0, 1, . . . , n ‚àí1. In matrix form this procedure can be expressed as
Dx(k+1) = œâ[b + Lx(k+1) + Ux(k)] + (1 ‚àíœâ)Dx(k)
or
[I ‚àíœâD‚àí1L]x(k+1) = œâD‚àí1b + [(1 ‚àíœâ)I + œâD‚àí1U]x(k),
(4.180)
for which the iteration matrix is now
B = BGS(œâ) = [I ‚àíœâD‚àí1L]‚àí1[(1 ‚àíœâ)I + œâD‚àí1U].
(4.181)
We see from (4.180) (on multiplying both sides by D) that
[D ‚àíœâL]x(k+1) = œâb + [(1 ‚àíœâ)D + œâU]x(k),
so from the fact that A = D ‚àí(L + U) [recall (4.170)], this may be rearranged as
x(k+1) = x(k) +
 1
œâD ‚àíL
‚àí1
r(k)
(4.182)
(r(k) = b ‚àíAx(k)), which is the form (4.167). Condition (4.156) holds if œâ Ã∏= 0.
The case œâ = 1 corresponds to the Gauss‚ÄìSeidel method in (4.175). If œâ ‚àà(0, 1),
the technique is often called an underrelaxation method, while for œâ ‚àà(1, ‚àû) it
is an overrelaxation method.
We will now summarize, largely without proof, results concerning the conver-
gence of (x(k)) to x for sequences generated by the previous iterative algorithms.
We observe that every iteration in any of the proposed methods needs (in the worst
case, assuming that A is not sparse) O(n2) arithmetic operations. The total number
of iterations is m, and is needed to achieve desired accuracy ||x(m) ‚àíx|| < œµ, and
so in turn the total number of arithmetic operations needed is O(mn2). Gaussian
elimination needs O(n3) operations to solve Ax = b, so the iterative methods are
worthwhile computationally only if m is sufÔ¨Åciently small. If m is about the same
size as n, then little advantage can be expected from iterative methods. On the
other hand, if A is sparse, perhaps possessing only O(n) nonzero elements, then
the iterative methods require only O(mn) operations to achieve ||x(m) ‚àíx|| < œµ.
We need to give conditions on A so that x(k) ‚Üíx, and also to say something about
the number of iterations needed to achieve convergence to desired accuracy.
Let us begin with the following deÔ¨Ånition.
DeÔ¨Ånition 4.4: Diagonal Dominance
Matrix A ‚ààRn√ón is diagonally domi-
nant if
|ai,i| >
n‚àí1

j=0
jÃ∏=i
|aij|
(4.183)
for i = 0, 1, . . . , n ‚àí1.
TLFeBOOK

ITERATIVE METHODS FOR LINEAR SYSTEMS
183
We mention here that DeÔ¨Ånition 4.4 is a bit different from DeÔ¨Ånition 6.2 (Chapter 6),
where diagonal dominance concepts appear in the context of spline interpolation
problems. It can be shown that if A in Ax = b is diagonally dominant according to
DeÔ¨Ånition 4.4, then the Jacobi and Gauss-Seidel methods both converge. Proof for
the Jacobi method appears in Theorem 4.2 of Ref. 8, while the Gauss‚ÄìSeidel case is
proved by Axelsson [10].
If A = AT , and A > 0 both the Jacobi and Gauss‚ÄìSeidel methods will converge.
A proof for the Gauss‚ÄìSeidel case appears in Golub and Van Loan [5, Theo-
rem 10.1.2]. The Jacobi case is considered in Ref. 8. Convergence results exist for
the overrelaxation methods JOR and SOR. For example, if A = AT with A > 0,
the SOR method is convergent iff 0 < œâ < 2 [8]. Naturally, we wish to select œâ so
that convergence occurs as rapidly as possible (i.e., m in ||x(m) ‚àíx|| < œµ is min-
imal). However, the problem of selecting the optimal value for œâ is well beyond
the scope of this book.
We recall that our iterative procedures have the general form in (4.155), where
it is intended that x = Bx + f . We may regard y = T x = Bx + f as a mapping
T |Rn ‚ÜíRn. On linear vector space Rn we may deÔ¨Åne the metric
d(x, y) = max
j‚ààZn
|xj ‚àíyj|
(4.184)
(recall the properties of metrics from Chapter 1). Space (Rn, d) is a complete metric
space [11, p. 308]. From Kreyszig [11] we have the following theorem.
Theorem 4.9: If the linear system x = Bx + f is such that
n‚àí1

j=0
|bij| < 1
for i = 0, 1, . . . , n ‚àí1 then solution x is unique. The solution can be obtained as
the limit of the vector sequence (x(k)) for k = 0, 1, 2, . . . (x(0) is arbitrary), where
x(k+1) = Bx(k) + f,
and where for Œ± = maxi‚ààZn
n‚àí1
j=0 |bij|, we have the error bounds
d(x(m), x) ‚â§
Œ±
1 ‚àíŒ± d(x(m‚àí1), x(m)) ‚â§
Œ±m
1 ‚àíŒ± d(x(0), x(1)).
(4.185)
Proof
We will give only an outline proof. This theorem is really just a special
instance of the contraction theorem, which appears and is proved in Chapter 7 (see
Theorem 7.3 and Corollary 7.1).
The essence of the proof is to consider the fact that
d(T x, T y) = max
i‚ààZn

n‚àí1

j=0
bij(xj ‚àíyj)

TLFeBOOK

184
LINEAR SYSTEMS OF EQUATIONS
‚â§max
j‚ààZn
|xj ‚àíyj| max
i‚ààZn
n‚àí1

j=0
|bij|
= d(x, y) max
i‚ààZn
n‚àí1

j=0
|bij|,
so d(T x, T y) ‚â§Œ±d(x, y), if we deÔ¨Åne
Œ± = max
i‚ààZn
n‚àí1

j=0
|bij| = ||B||‚àû
[recall (4.41d)].
In this theorem we see that if Œ± < 1, then x(k) ‚Üíx. In this case d(T x, T y) <
d(x, y) for all x, y ‚ààRn. Such a mapping T is called a contraction mapping (or
contractive mapping). We see that contraction mappings have the effect of moving
points in a space closer together. The error bounds stated in (4.185) give us an
idea about the number of iterations m needed to achieve ||x(m) ‚àíx|| < œµ (œµ >
0). We emphasize that condition Œ± < 1 is sufÔ¨Åcient for convergence, so (x(k))
may converge to x even if this condition is violated. It is also noteworthy that
convergence will be fast if Œ± is small, that is, if ||B||‚àûis small. The result in
Theorem 4.8 certainly suggests convergence ought to be fast if œÅ(B) is small.
Example 4.8
We shall consider the application of SOR to the problem of
solving Ax = b, where
A =
Ô£Æ
Ô£ØÔ£ØÔ£∞
4
1
0
0
1
4
1
0
0
1
4
1
0
0
1
4
Ô£π
Ô£∫Ô£∫Ô£ª,
b =
Ô£Æ
Ô£ØÔ£ØÔ£∞
1
2
3
4
Ô£π
Ô£∫Ô£∫Ô£ª.
We shall assume that x(0) = [0000]T . Note that SOR is not the best way to solve
this problem. A better approach is to be found in Section 6.5 (Chapter 6). This
example is for illustration only. However, it is easy to conÔ¨Årm that
x = [0.1627
0.3493
0.4402
0.8900]T .
Recall that the SOR iterations are speciÔ¨Åed by (4.179). However, we have not
discussed how to terminate the iterative process. A popular choice is to recall that
r(k) = b ‚àíAx(k) [see (4.167)], and to stop the iterations when for k = m
||r(m)||
||r(0)|| ‚â§œÑ
(4.186)
TLFeBOOK

ITERATIVE METHODS FOR LINEAR SYSTEMS
185
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
0
50
100
150
w
m
 Number of iterations needed by SOR 
Figure 4.4
Plot of the number of iterations m needed by SOR as a function of œâ for the
parameters of Example 4.8 in order to satisfy the stopping condition ||r(m)||‚àû/||r(0||‚àû‚â§œÑ.
for some œÑ > 0 (a small value). For our present purposes || ¬∑ || shall be the norm
in (4.29c), which is compatible with the needs of Theorem 4.9. We shall choose
œÑ = 0.001.
We observe that A is diagonally dominant, so convergence is certainly expected
for œâ = 1. In fact, A > 0 so convergence of the SOR method can be expected for
all œâ ‚àà(0, 2). Figure 4.4 plots the m that achieves (4.186) versus œâ, and we see
that there is an optimal choice for œâ that is somewhat larger than œâ = 1. In this
case though the optimal choice does not lead to much of an improvement over
choice œâ = 1.
For our problem [recalling (4.170)], we have
D =
Ô£Æ
Ô£ØÔ£ØÔ£∞
4
0
0
0
0
4
0
0
0
0
4
0
0
0
0
4
Ô£π
Ô£∫Ô£∫Ô£ª, L =
Ô£Æ
Ô£ØÔ£ØÔ£∞
0
0
0
0
‚àí1
0
0
0
0
‚àí1
0
0
0
0
‚àí1
0
Ô£π
Ô£∫Ô£∫Ô£ª,
U =
Ô£Æ
Ô£ØÔ£ØÔ£∞
0
‚àí1
0
0
0
0
‚àí1
0
0
0
0
‚àí1
0
0
0
0
Ô£π
Ô£∫Ô£∫Ô£ª.
From (4.178)
BGS =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
0.0000
‚àí0.2500
0.0000
0.0000
0.0000
0.0625
‚àí0.2500
0.0000
0.0000
‚àí0.0156
0.0625
‚àí0.2500
0.0000
0.0039
‚àí0.0156
0.0625
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª.
We therefore Ô¨Ånd that ||BGS||‚àû= 0.3281. It is possible to show (preferably using
MATLAB or some other software tool that is good with eigenproblems) that
TLFeBOOK

186
LINEAR SYSTEMS OF EQUATIONS
œÅ(BGS) = 0.1636. Given (4.185) in Theorem 4.9 we therefore expect fast con-
vergence for our problem since Œ± is fairly small. In fact
||x(m) ‚àíx||‚àû‚â§
||BGS||m
‚àû
1 ‚àí||BGS||‚àû
||(D ‚àíL)‚àí1b||‚àû
(4.187)
[using x(0) = 0, x(1) = (D ‚àíL)‚àí1b]. For the stopping criterion of (4.186) we
obtained (recalling that œâ = 1, and œÑ = 0.001) m = 5 with
x(5) = [0.1630
0.3490
0.4403
0.8899]T
so that
||x(5) ‚àíx||‚àû= 3.6455 √ó 10‚àí4.
The right-hand side of (4.187) evaluates to
||BGS||m
‚àû
1 ‚àí||BGS||‚àû
||(D ‚àíL)‚àí1b||‚àû= 4.7523 √ó 10‚àí3.
Thus, (4.187) certainly holds true.
4.8
FINAL REMARKS
We have seen that inaccurate solutions to linear systems of equations can arise
when the linear system is ill-conditioned. Condition numbers warn us if this is
a potential problem. However, even if a problem is well-conditioned, an inac-
curate solution may arise if the algorithm applied to solve it is unstable. In the
case of problems arising out of algorithm instability, we naturally replace the
unstable algorithm with a stable one (e.g., Gaussian elimination may need to
be replaced by Gaussian elimination with partial pivoting). In the case of an
ill-conditioned problem, we may try to improve the accuracy of the solution by
either
1. Using an algorithm that does not worsen the conditioning of the underlying
problem (e.g., choosing QR factorization in preference to Cholesky factor-
ization)
2. Reformulating the problem so that it is better conditioned
We have not considered the second alternative in this chapter. This will be done
in Chapter 5.
APPENDIX 4.A
HILBERT MATRIX INVERSES
Consider the following MATLAB code:
R = hilb(10);
inv(R)
TLFeBOOK

HILBERT MATRIX INVERSES
187
ans =
1.0e+12 *
Columns 1 through 7
0.0000
-0.0000
0.0000
-0.0000
0.0000
-0.0000
0.0000
-0.0000
0.0000
-0.0000
0.0000
-0.0002
0.0005
-0.0008
0.0000
-0.0000
0.0001
-0.0010
0.0043
-0.0112
0.0178
-0.0000
0.0000
-0.0010
0.0082
-0.0379
0.1010
-0.1616
0.0000
-0.0002
0.0043
-0.0379
0.1767
-0.4772
0.7712
-0.0000
0.0005
-0.0112
0.1010
-0.4772
1.3014
-2.1208
0.0000
-0.0008
0.0178
-0.1616
0.7712
-2.1208
3.4803
-0.0000
0.0008
-0.0166
0.1529
-0.7358
2.0376
-3.3636
0.0000
-0.0004
0.0085
-0.0788
0.3820
-1.0643
1.7659
-0.0000
0.0001
-0.0018
0.0171
-0.0832
0.2330
-0.3883
Columns 8 through 10
-0.0000
0.0000
-0.0000
0.0008
-0.0004
0.0001
-0.0166
0.0085
-0.0018
0.1529
-0.0788
0.0171
-0.7358
0.3820
-0.0832
2.0376
-1.0643
0.2330
-3.3636
1.7659
-0.3883
3.2675
-1.7231
0.3804
-1.7231
0.9122
-0.2021
0.3804
-0.2021
0.0449
R*inv(R)
ans =
Columns 1 through 7
1.0000
0.0000
0.0000
-0.0000
0.0001
0.0001
-0.0001
-0.0000
1.0000
0.0000
-0.0000
0.0001
0.0001
-0.0002
-0.0000
0.0000
1.0000
-0.0000
0.0001
0.0000
-0.0001
-0.0000
0.0000
0.0000
1.0000
0.0000
0.0000
-0.0000
-0.0000
0.0000
0.0000
-0.0000
1.0000
-0.0000
-0.0000
-0.0000
0.0000
0.0000
-0.0000
0.0000
1.0000
-0.0000
-0.0000
0.0000
0.0000
-0.0000
0.0000
0.0000
0.9999
-0.0000
0.0000
0.0000
-0.0000
0.0000
0.0001
-0.0000
-0.0000
0.0000
0.0000
-0.0000
0.0000
0.0000
-0.0001
-0.0000
0.0000
0.0000
-0.0000
0.0000
0.0000
-0.0000
Columns 8 through 10
-0.0000
-0.0001
0.0000
0.0001
-0.0001
0.0000
0.0000
-0.0001
0.0000
-0.0000
-0.0000
0.0000
TLFeBOOK

188
LINEAR SYSTEMS OF EQUATIONS
0.0000
-0.0000
0.0000
-0.0000
-0.0000
0.0000
0.0000
-0.0000
0.0000
1.0000
-0.0000
0.0000
0.0000
1.0000
0.0000
-0.0000
-0.0000
1.0000
R = hilb(11);
inv(R)
ans =
1.0e+14 *
Columns 1 through 7
0.0000
-0.0000
0.0000
-0.0000
0.0000
-0.0000
0.0000
-0.0000
0.0000
-0.0000
0.0000
-0.0000
0.0000
-0.0000
0.0000
-0.0000
0.0000
-0.0000
0.0002
-0.0006
0.0012
-0.0000
0.0000
-0.0000
0.0003
-0.0019
0.0064
-0.0137
0.0000
-0.0000
0.0002
-0.0019
0.0110
-0.0381
0.0817
-0.0000
0.0000
-0.0006
0.0064
-0.0381
0.1329
-0.2877
0.0000
-0.0000
0.0012
-0.0137
0.0817
-0.2877
0.6270
-0.0000
0.0001
-0.0016
0.0183
-0.1101
0.3902
-0.8555
0.0000
-0.0000
0.0013
-0.0149
0.0905
-0.3227
0.7111
-0.0000
0.0000
-0.0006
0.0068
-0.0415
0.1487
-0.3292
0.0000
-0.0000
0.0001
-0.0013
0.0081
-0.0293
0.0651
Columns 8 through 11
-0.0000
0.0000
-0.0000
0.0000
0.0001
-0.0000
0.0000
-0.0000
-0.0016
0.0013
-0.0006
0.0001
0.0183
-0.0149
0.0068
-0.0013
-0.1101
0.0905
-0.0415
0.0081
0.3902
-0.3227
0.1487
-0.0293
-0.8555
0.7111
-0.3292
0.0651
1.1733
-0.9796
0.4553
-0.0903
-0.9796
0.8212
-0.3830
0.0762
0.4553
-0.3830
0.1792
-0.0357
-0.0903
0.0762
-0.0357
0.0071
R*inv(R)
ans =
Columns 1 through 7
0.9997
-0.0009
0.0022
0.0028
-0.0164
0.0558
-0.1229
-0.0002
0.9992
0.0020
0.0023
-0.0132
0.0454
-0.1029
-0.0002
-0.0007
1.0018
0.0019
-0.0112
0.0385
-0.0844
-0.0002
-0.0006
0.0016
1.0017
-0.0097
0.0331
-0.0736
-0.0002
-0.0006
0.0015
0.0015
0.9915
0.0285
-0.0638
-0.0002
-0.0005
0.0014
0.0013
-0.0076
1.0258
-0.0581
TLFeBOOK

HILBERT MATRIX INVERSES
189
-0.0002
-0.0005
0.0013
0.0012
-0.0070
0.0234
0.9468
-0.0001
-0.0005
0.0012
0.0011
-0.0063
0.0216
-0.0474
-0.0001
-0.0004
0.0011
0.0010
-0.0059
0.0201
-0.0448
-0.0001
-0.0004
0.0010
0.0009
-0.0053
0.0187
-0.0406
-0.0001
-0.0004
0.0010
0.0009
-0.0052
0.0179
-0.0395
Columns 8 through 11
0.1665
-0.1405
0.0652
-0.0091
0.1351
-0.1165
0.0530
-0.0071
0.1125
-0.0973
0.0452
-0.0058
0.0964
-0.0844
0.0385
-0.0047
0.0858
-0.0739
0.0341
-0.0041
0.0745
-0.0661
0.0300
-0.0037
0.0696
-0.0592
0.0280
-0.0033
1.0635
-0.0547
0.0251
-0.0029
0.0581
0.9495
0.0235
-0.0028
0.0536
-0.0458
1.0213
-0.0024
0.0527
-0.0445
0.0207
0.9976
R = hilb(12);
inv(R)
Warning: Matrix is close to singular or badly scaled.
Results may be inaccurate. RCOND = 2.632091e-17.
ans =
1.0e+15 *
Columns 1 through 7
0.0000
-0.0000
0.0000
-0.0000
0.0000
-0.0000
0.0000
-0.0000
0.0000
-0.0000
0.0000
-0.0000
0.0000
-0.0000
0.0000
-0.0000
0.0000
-0.0000
0.0001
-0.0002
0.0006
-0.0000
0.0000
-0.0000
0.0001
-0.0008
0.0032
-0.0086
0.0000
-0.0000
0.0001
-0.0008
0.0054
-0.0229
0.0624
-0.0000
0.0000
-0.0002
0.0032
-0.0229
0.0990
-0.2720
0.0000
-0.0000
0.0006
-0.0086
0.0624
-0.2720
0.7528
-0.0000
0.0000
-0.0011
0.0151
-0.1107
0.4863
-1.3545
0.0000
-0.0000
0.0013
-0.0173
0.1276
-0.5640
1.5794
-0.0000
0.0000
-0.0009
0.0124
-0.0920
0.4090
-1.1511
0.0000
-0.0000
0.0004
-0.0050
0.0377
-0.1686
0.4765
-0.0000
0.0000
-0.0001
0.0009
-0.0067
0.0301
-0.0855
Columns 8 through 12
-0.0000
0.0000
-0.0000
0.0000
-0.0000
0.0000
-0.0000
0.0000
-0.0000
0.0000
-0.0011
0.0013
-0.0009
0.0004
-0.0001
0.0151
-0.0173
0.0124
-0.0050
0.0009
-0.1107
0.1275
-0.0920
0.0377
-0.0067
0.4863
-0.5639
0.4090
-0.1686
0.0301
-1.3544
1.5793
-1.1510
0.4765
-0.0855
2.4505
-2.8712
2.1015
-0.8732
0.1572
TLFeBOOK

190
LINEAR SYSTEMS OF EQUATIONS
-2.8713
3.3786
-2.4821
1.0348
-0.1869
2.1016
-2.4822
1.8297
-0.7651
0.1385
-0.8732
1.0348
-0.7651
0.3208
-0.0582
0.1572
-0.1869
0.1386
-0.0582
0.0106
R*inv(R)
Warning: Matrix is close to singular or badly scaled.
Results may be inaccurate. RCOND = 2.632091e-17.
ans =
Columns 1 through 7
1.0126
-0.0066
-0.0401
0.0075
0.1532
-0.8140
2.2383
0.0113
0.9943
-0.0361
0.0100
0.1162
-0.6265
1.7168
0.0103
-0.0050
0.9673
0.0106
0.0952
-0.5300
1.4834
0.0094
-0.0045
-0.0299
1.0104
0.0797
-0.4573
1.2725
0.0087
-0.0041
-0.0275
0.0104
1.0703
-0.4038
1.0986
0.0081
-0.0037
-0.0255
0.0102
0.0621
0.6416
0.9971
0.0075
-0.0034
-0.0237
0.0099
0.0554
-0.3245
1.9062
0.0071
-0.0032
-0.0222
0.0095
0.0495
-0.2944
0.8232
0.0066
-0.0030
-0.0209
0.0093
0.0439
-0.2686
0.7246
0.0063
-0.0028
-0.0197
0.0087
0.0424
-0.2532
0.7002
0.0059
-0.0026
-0.0187
0.0087
0.0372
-0.2307
0.6309
0.0057
-0.0024
-0.0177
0.0081
0.0358
-0.2168
0.6064
Columns 8 through 12
-4.0762
4.7656
-3.5039
1.4385
-0.2183
-3.1582
3.7754
-2.7520
1.1123
-0.1649
-2.6250
3.1055
-2.3301
0.9219
-0.1390
-2.2676
2.6602
-1.9922
0.7905
-0.1163
-1.9863
2.4023
-1.7139
0.7104
-0.0992
-1.7969
2.1094
-1.5430
0.6289
-0.0897
-1.6133
1.9258
-1.4043
0.5581
-0.0779
-0.4658
1.7598
-1.2734
0.5146
-0.0715
-1.3047
2.5762
-1.1445
0.4629
-0.0651
-1.2793
1.5098
-0.1055
0.4424
-0.0619
-1.1387
1.3438
-0.9873
1.3955
-0.0529
-1.1025
1.2998
-0.9395
0.3809
0.9474
diary off
The MATLAB rcond function (which gave the number RCOND above) needs
some explanation. A useful reference on this is Hill [3, pp. 229‚Äì230]. It is based on
a condition number estimator in the old FORTRAN codes known as ‚ÄúLINPACK‚Äù.
It is based on 1-norms. rcond(A) will give the reciprocal of the 1-norm condition
number of A. If A is well-conditioned, then rcond(A) will be close to unity (i.e.,
close to one), and will be very tiny if A is ill-conditioned. The rule of thumb
involved in interpreting an rcond output is ‚Äúif rcond(A) ‚âàd √ó 10‚àík, where d is
a digit from 1 to 9, then the elements of xcomp can usually be expected to have
k fewer signiÔ¨Åcant digits of accuracy than the elements of A‚Äù [3]. Here xcomp
TLFeBOOK

SVD AND LEAST SQUARES
191
is simply the computed solution to Ax = y; that is, in the notation of the present
set of notes, ÀÜx =xcomp. MATLAB does arithmetic with about 16 decimal digits
[3, p. 228], so in the preceding example of a Hilbert matrix inversion problem for
N = 12, since RCOND is about 10‚àí17, we have lost about 17 digits in computing
R‚àí1. Of course, this loss is catastrophic for our problem.
APPENDIX 4.B
SVD AND LEAST SQUARES
From Theorem 4.4, A = UV T , so this expands into the summation
A =
p‚àí1

i=0
œÉiuivT
i .
(4.A.1)
But if r = rank (A), then (4.A.1) reduces to
A =
r‚àí1

i=0
œÉiuivT
i .
(4.A.2)
In the following theorem œÅ2
LS = ||f l||2
2 = ||AÀÜx ‚àíf ||2
2 [see (4.127)], and ÀÜx is the
least-squares optimal solution to Ax = f .
Theorem 4.B.1: Let A be represented as in (4.A.2) with A ‚ààRm√ón and m ‚â•n.
If f ‚ààRm then
ÀÜx =
r‚àí1

i=0
'
uT
i f
œÉi
(
vi,
(4.A.3)
œÅ2
LS =
m‚àí1

i=r
(uT
i f )2.
(4.A.4)
Proof
For all x ‚ààRn, using the invariance of the 2-norm to orthogonal trans-
formations, and the fact that V V T = In (n √ó n identity matrix)
||Ax ‚àíf ||2
2 = ||UT AV (V T x) ‚àíUT f ||2
2 = ||Œ± ‚àíUT f ||2
2,
(4.A.5)
where Œ± = V T x, so as Œ± = [Œ±0 ¬∑ ¬∑ ¬∑ Œ±n‚àí1]T we have Œ±i = vT
i x. Equation (4.A.5)
expands as
||Ax ‚àíf ||2
2 = Œ±T T Œ± ‚àí2Œ±T T UT f + f T UUT f,
(4.A.6)
TLFeBOOK

192
LINEAR SYSTEMS OF EQUATIONS
which further expands as
||Ax ‚àíf ||2
2 =
r‚àí1

i=0
œÉ 2
i Œ±2
i ‚àí2
r‚àí1

i=0
Œ±iœÉiuT
i f +
m‚àí1

i=0
[uT
i f ]2
=
r‚àí1

i=0
[œÉ 2
i Œ±2
i ‚àí2Œ±iœÉiuT
i f + [uT
i f ]2] +
m‚àí1

i=r
[uT
i f ]2
=
r‚àí1

i=0
[œÉiŒ±i ‚àíuT
i f ]2 +
m‚àí1

i=r
[uT
i f ]2.
(4.A.7)
To minimize this we must have œÉiŒ±i ‚àíuT
i f = 0, and so
Œ±i = 1
œÉi
uT
i f
(4.A.8)
for i ‚ààZr. As Œ± = V T x, we have x = V Œ±, so if we set Œ±r = ¬∑ ¬∑ ¬∑ = Œ±n‚àí1 = 0, then
from Eq. (4.A.8), we obtain
ÀÜx =
r‚àí1

i=0
Œ±ivi =
r‚àí1

i=0
uT
i f
œÉi
vi,
which is (4.A.3). For this choice of ÀÜx from (4.A.7)
||AÀÜx ‚àíf ||2
2 =
m‚àí1

i=r
[uT
i f ]2 = œÅ2
LS,
which is (4.A.4).
DeÔ¨Åne A+ = V +UT (again A ‚ààRm√ón with m ‚â•n), where
+ = diag(œÉ ‚àí1
0 , . . . , œÉ ‚àí1
r‚àí1, 0, . . . , 0) ‚ààRn√óm.
(4.A.9)
We observe that
A+f = V +UT f =
r‚àí1

i=0
uT
i f
œÉi
vi = ÀÜx.
(4.A.10)
We call A+ the pseudoinverse of A. We have established that if rank (A) = n,
then AT AÀÜx = AT f , so ÀÜx = (AT A)‚àí1AT f , which implies that in this case A+ =
(AT A)‚àí1AT . If A ‚ààRn√ón and A‚àí1 exists, then A+ = A‚àí1.
If A‚àí1 exists (i.e., m = n and rank (A) = n), then we recall that Œ∫2(A) =
||A||2||A‚àí1||2. If A ‚ààRm√ón and m ‚â•n, then we extend this deÔ¨Ånition to
Œ∫2(A) = ||A||2||A+||2.
(4.A.11)
TLFeBOOK

REFERENCES
193
We have established that ||A||2 = œÉ0 so since A+ = V +UT we have ||A+||2 =
1/œÉr‚àí1. Consequently
Œ∫2(A) =
œÉ0
œÉr‚àí1
(4.A.12)
which provides a somewhat better justiÔ¨Åcation of (4.119), because if rank (A) =
n then (4.A.12) is Œ∫2(A) = œÉ0/œÉn‚àí1 = œÉmax(A)/œÉmin(A) [which is (4.119)].
From (4.A.11), Œ∫2(AT A) = ||AT A||2||(AT A)+||2. With A = UV T , and AT =
V T UT we have
AT A = V T V T ,
and
(AT A)+ = V (T )+V T .
Thus, ||AT A||2 = œÉ 2
0 , and ||(AT A)+||2 = œÉ ‚àí2
n‚àí1 (rank (A) = n). Thus
Œ∫2(AT A) =
œÉ 2
0
œÉ 2
n‚àí1
= [Œ∫2(A)]2.
The condition number deÔ¨Ånition Œ∫p(A) in (4.60) was fully justiÔ¨Åed because of
(4.58). An analogous justiÔ¨Åcation exists for (4.A.11), but is much more difÔ¨Åcult to
derive, and this is why we do not consider it in this book.
REFERENCES
1. J. R. Rice, The Approximation of Functions, Vol. I: Linear Theory, Addison-Wesley,
Reading, MA, 1964.
2. M.-D. Choi, ‚ÄúTricks or Treats with the Hilbert Matrix,‚Äù Am. Math. Monthly 90(5),
301‚Äì312 (May 1983).
3. D. R. Hill, Experiments in Computational Matrix Algebra (C. B. Moler, consulting ed.),
Random House, New York, 1988.
4. G. E. Forsythe and C. B. Moler, Computer Solution of Linear Algebraic Systems,
Prentice-Hall, Englewood Cliffs, NJ, 1967.
5. G. H. Golub and C. F. Van Loan, Matrix Computations, 2nd ed., Johns Hopkins Univ.
Press, Baltimore, MD, 1989.
6. R. A. Horn and C. R. Johnson, Matrix Analysis, Cambridge Univ. Press, Cambridge,
MA, 1985.
7. N. J. Higham, Accuracy and Stability of Numerical Algorithms, SIAM, Philadelphia, PA,
1996.
8. A. Quarteroni, R. Sacco, and F. Saleri, Numerical Mathematics (Texts in Applied Math-
ematics series, Vol. 37), Springer-Verlag, New York, 2000.
9. E. Isaacson and H. B. Keller, Analysis of Numerical Methods, Wiley, New York, 1966.
10. O. Axelsson, Iterative Solution Methods, Cambridge Univ. Press, New York, 1994.
11. E. Kreyszig, Introductory Functional Analysis with Applications, Wiley, New York, 1978.
TLFeBOOK

194
LINEAR SYSTEMS OF EQUATIONS
PROBLEMS
4.1. Function f (x) ‚ààL2[0, 1] is to be approximated according to
f (x) ‚âàa0x +
a1
x + c
using least squares, where c ‚ààR is some Ô¨Åxed parameter. This involves
solving the linear system
Ô£Æ
Ô£∞
1
3
1 ‚àíc loge

1 + 1
c

1 ‚àíc loge

1 + 1
c

1
c(c+1)
Ô£π
Ô£ª
#
$%
&
=R
 ÀÜa0
ÀÜa1

# $% &
=ÀÜa
=
 " 1
0 xf (x) dx
" 1
0
f (x)
x+c dx
	
#
$%
&
=g
,
where ÀÜa is the vector from R2 that minimizes the energy V (a) [Eq. (4.8)].
(a) Suppose that f (x) = x +
1
x+1. Find ÀÜa for c = 1. For this special case
it is possible to know the answer in advance without solving the linear
system above. However, this problem requires you to solve the system.
[Hint: It helps to recall that
 a
b
c
d
‚àí1
=
1
ad ‚àíbc

d
‚àíb
‚àíc
a

.
(b) Derive R [which is a special case of (4.9)].
4.2. Suppose that
A =

1
2
‚àí1
‚àí5

.
Find ||A||‚àû, ||A||1, and ||A||2.
4.3. Suppose that
A =
 1
0
2
œµ

‚ààR2√ó2, œµ ‚â•0,
and that A‚àí1 exists. Find Œ∫‚àû(A) if œµ is small.
4.4. Suppose that
A =
 1
1
0
œµ

‚ààR2√ó2,
and assume œµ > 0 (so that A‚àí1 always exists). Find Œ∫2(A) = ||A||2||A‚àí1||2.
What happens to condition number Œ∫2(A) if œµ ‚Üí0?
4.5. Let A(œµ), B(œµ) ‚ààRn√ón. For example, A(œµ) = [aij(œµ)] so element aij(œµ) of
A(œµ) depends on the parameter œµ ‚ààR.
TLFeBOOK

PROBLEMS
195
(a) Prove that
d
dœµ [A(œµ)B(œµ)] = A(œµ)dB(œµ)
dœµ
+ dA(œµ)
dœµ
B(œµ),
where dA(œµ)/dt = [daij(œµ)/dœµ], and dB(œµ)/dt = [dbij(œµ)/dœµ].
(b) Prove that
d
dœµ A‚àí1(œµ) = ‚àíA‚àí1(œµ)
dA(œµ)
dœµ

A‚àí1(œµ).
[Hint: Consider
d
dœµ [A(œµ)A‚àí1(œµ)] = d
dœµ I = 0, and use (a).]
4.6. This problem is an alternative derivation of Œ∫(A). Suppose that œµ ‚ààR, A, F ‚àà
Rn√ón, and x(œµ), y, f ‚ààRn. Consider the perturbed linear system of equations
(A + œµF)x(œµ) = y + œµf,
(4.P.1)
where Ax = y, so x(0) = x is the correct solution. Clearly, œµF models the
errors in A, while œµf models the errors in y. From (4.P.1), we obtain
x(œµ) = [A + œµF]‚àí1(y + œµf ).
(4.P.2)
The Taylor series expansion for x(œµ) about œµ = 0 is
x(œµ) = x(0) + dx(0)
dœµ
œµ + O(œµ2),
(4.P.3)
where O(œµ2) denotes terms in the expansion containing œµk for k ‚â•2.
Use (4.P.3), results from the previous problem, and basic matrix‚Äìvector norm
properties (Section 4.4) to derive the bound in
||x(œµ) ‚àíx||
||x||
‚â§œµ ||A||||A‚àí1||
#
$%
&
=Œ∫(A)
||f ||
||y|| + ||F||
||A||

[Comment: The relative error in A is œÅA = œµ||F||/||A||, and the relative error
in y is œÅy = œµ||f ||/||y||, so more concisely ||x(œµ) ‚àíx||/||x|| ‚â§Œ∫(A)(œÅA +
œÅy).]
4.7. This problem follows the example relating to Eq. (4.95). An analog signal
f (t) is modeled according to
f (t) =
p‚àí1

j=0
ajtj + Œ∑(t),
where aj ‚ààR for all j ‚ààZp, and Œ∑(t) is some random noise term. We only
possess samples of the signal; that is, we only have the Ô¨Ånite-length sequence
TLFeBOOK

196
LINEAR SYSTEMS OF EQUATIONS
(fn) deÔ¨Åned by fn = f (nTs) for n ‚ààZN, where Ts > 0 is the sampling
period of the data acquisition system that gave us the samples. Our estimate
of fn is therefore
ÀÜfn =
p‚àí1

j=0
ajT j
s nj.
With a = [a0 a1 ¬∑ ¬∑ ¬∑ ap‚àí2 ap‚àí1]T ‚ààRp, Ô¨Ånd
V (a) =
N‚àí1

n=0
[fn ‚àíÀÜfn]2
in the form of Eq. (4.102). This implies that you must specify œÅ, g, A, and P .
4.8. Sampled data fn (n = 0, 1, . . . , N ‚àí1) is modeled according to
ÀÜfn = a + bn + C sin(Œ∏n + œÜ).
Recall sin(A + B) = sin A cos B + cos A sin B. Also recall that
V (Œ∏) =
N‚àí1

n=0
[fn ‚àíÀÜfn]2 = f T f ‚àíf T A(Œ∏)[AT (Œ∏)A(Œ∏)]‚àí1AT (Œ∏)f,
where f = [f0f1 ¬∑ ¬∑ ¬∑ fN‚àí1]T ‚ààRN. Give a detailed expression for A(Œ∏).
4.9. Prove that the product of two lower triangular matrices is a lower triangular
matrix.
4.10. Prove that the product of two upper triangular matrices is an upper triangular
matrix.
4.11. Find the LU decomposition of the matrix
A =
Ô£Æ
Ô£∞
2
‚àí1
0
‚àí1
2
‚àí1
0
‚àí1
2
Ô£π
Ô£ª
using Gauss transformations as recommended in Section 4.5. Use A = LU to
rewrite the factorization of A as A = LDLT , where L is unit lower triangular,
and D is a diagonal matrix. Is A > 0? Why?
4.12. Use Gaussian elimination to LU factorize the matrix
A =
Ô£Æ
Ô£∞
4
‚àí1
1
2
‚àí1
4
‚àí1
1
2
‚àí1
4
Ô£π
Ô£ª.
Is A > 0? Why?
TLFeBOOK

PROBLEMS
197
4.13. (a) Consider A ‚ààRn√ón, and A = AT . Suppose that the leading principal
submatrices of A all have positive determinants. Prove that A > 0.
(b) Is
Ô£Æ
Ô£∞
5
‚àí3
0
‚àí3
5
1
0
1
5
Ô£π
Ô£ª> 0?
Why?
4.14. The vector space Rn is an inner product space with inner product ‚ü®x, y‚ü©=
xT y for all x, y ‚ààRn. Suppose that A ‚ààRn√ón, A = AT , and also that A > 0.
Prove that ‚ü®x, y‚ü©= xT Ay is also an inner product on the vector space Rn.
4.15. In the quadratic form
V (x) = f T f ‚àí2xT AT f + xT AT Ax,
we assume x ‚ààRn, and AT A > 0. Prove that
V (ÀÜx) = f T f ‚àíf T A[AT A]‚àí1AT f,
where ÀÜx is the vector that minimizes V (x).
4.16. Suppose that A ‚ààRN√óM with M ‚â§N, and rank (A) = M. Suppose also that
‚ü®x, y‚ü©= xT y. Consider P = A[AT A]‚àí1AT , P‚ä•= I ‚àíP (I is the N √ó N
identity matrix). Prove that for all x ‚ààRN we have ‚ü®P x, P‚ä•x‚ü©= 0. (Com-
ment: Matrices P and P‚ä•are examples of orthogonal projection operators.)
4.17. (a) Write a MATLAB function for forward substitution (solving Lz = y).
Write a MATLAB function for backward substitution (solving Ux = z).
Test your functions out on the following matrices and vectors:
L =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
1
0
0
0
‚àí1
1
0
0
0
2
3
1
0
0
0
‚àí3
7
1
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª,
U =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
1
2
3
4
0
3
5
5
0
0
‚àí7
3
‚àí1
3
0
0
0
6
7
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª,
y =
 1
1
‚àí1
‚àí1 T , z =

1
2
‚àí7
3
‚àí2
T .
(b) Write a MATLAB function to implement the LU decomposition algo-
rithm based on Gauss transformations considered in Section 4.5. Test
your function out on the following A matrices:
A =
Ô£Æ
Ô£ØÔ£ØÔ£∞
1
2
3
4
‚àí1
1
2
1
0
2
1
3
0
0
1
1
Ô£π
Ô£∫Ô£∫Ô£ª,
A =
Ô£Æ
Ô£∞
2
‚àí1
0
‚àí1
2
‚àí1
0
‚àí1
2
Ô£π
Ô£ª.
TLFeBOOK

198
LINEAR SYSTEMS OF EQUATIONS
100 V
200 V
1 A
V6
V5
V4
V2
V3
V1
1000 ‚Ñ¶
10 ‚Ñ¶
100 ‚Ñ¶
100 ‚Ñ¶
100 ‚Ñ¶
50 ‚Ñ¶
8 ‚Ñ¶
20 ‚Ñ¶
+
‚àí
+
‚àí
Figure 4.P.1
The DC electric circuit of Problem 4.18.
4.18. Consider the DC electric circuit in Fig. 4.P.1.
Write the node equations for the node voltages V1, V2, . . . , V6 as shown.
These may be loaded into a vector
v = [V1V2V3V4V5V6]T
such that the node equations have the form
Gv = y.
(4.P.4)
Use the Gaussian elimination, forward substitution, and backward substitu-
tion MATLAB functions from the previous problem to solve the linear system
(4.P.4).
4.19. Let In ‚ààRn√ón be the order n identity matrix, and deÔ¨Åne
Tn =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
a
b
0
¬∑ ¬∑ ¬∑
0
0
b
a
b
¬∑ ¬∑ ¬∑
0
0
0
b
a
¬∑ ¬∑ ¬∑
0
0
...
...
...
...
...
0
0
0
¬∑ ¬∑ ¬∑
a
b
0
0
0
¬∑ ¬∑ ¬∑
b
a
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
‚ààRn√ón.
TLFeBOOK

PROBLEMS
199
Matrix Tn is tridiagonal. It is also an example of a symmetric matrix that is
Toeplitz (deÔ¨Åned in a later problem). The characteristic polynomial of Tn is
pn(Œª) = det(ŒªIn ‚àíTn). Show that
pn(Œª) = (Œª ‚àía)pn‚àí1(Œª) ‚àíb2pn‚àí2(Œª)
(4.P.5)
for n = 3, 4, 5, . . .. Find p1(Œª), and p2(Œª) [initial conditions for the polyno-
mial recursion in (4.P.5)].
4.20. Consider the following If: T ‚ààCn√ón is Toeplitz if it has the form T =
[ti‚àíj]i,j=0,1,...,n‚àí1. Thus, for example, if n = 3 we have
T =
Ô£Æ
Ô£∞
t0
t‚àí1
t‚àí2
t1
t0
t‚àí1
t2
t1
t0
Ô£π
Ô£ª.
Observe that in a Toeplitz matrix all of the elements on any given
diagonal are equal to each other. A symmetric Toeplitz matrix has the
form T = [t|i‚àíj|] since T = T T implies that t‚àíi = ti for all i. Let xn =
[xn,0 xn,1 ¬∑ ¬∑ ¬∑ xn,n‚àí2 xn,n‚àí1]T ‚ààCn. Let Jn ‚ààCn√ón be the n √ó n exchange
matrix (also called the contra-identity matrix) which is deÔ¨Åned as the
matrix yielding ÀÜxn = Jnxn = [xn,n‚àí1xn,n‚àí2 ¬∑ ¬∑ ¬∑ xn,1xn,0]T . We see that Jn
simply reverses the order of the elements of xn. An immediate con-
sequence of this is that J 2
n xn = xn (i.e., J 2
n = In). What is J3? (Write
this matrix out completely.) Suppose that Tn is a symmetric Toeplitz
matrix.
(a) Show that (noting that ÀÜœÑn = JnœÑn)
Tn+1 =
 Tn
œÑn
œÑ T
n
t0

=
 t0
ÀÜœÑ T
n
ÀÜœÑn
Tn

(nesting property). What is œÑn?
(b) Show that
JnTnJn = Tn
(persymmetry property).
(Comment: Toeplitz matrices have an important role to play in digital signal
processing. For example, they appear in problems in spectral analysis, and
in voice compression algorithms.)
4.21. This problem is about a computationally efÔ¨Åcient method to solve the linear
system
Rnan = œÉ 2
ne0,
(4.P.6)
TLFeBOOK

200
LINEAR SYSTEMS OF EQUATIONS
where Rn ‚ààRn√ón is symmetric and Toeplitz (recall Problem 4.20). All
of the leading principle submatrices (recall the deÔ¨Ånition in Theo-
rem 4.1) of Rn are nonsingular. Also, e0 = [1 0 0 ¬∑ ¬∑ ¬∑ 0 0]T ‚ààRn, an =
[1 an,1
¬∑ ¬∑ ¬∑ an,n‚àí2 an,n‚àí1]T , and œÉ 2
n ‚ààR is unknown as well. DeÔ¨Åne
en = Jne0. Clearly, an,0 = 1 (all n).
(a) Prove that
Rn ÀÜan = œÉ 2
n ÀÜe0 = œÉ 2
nen,
(4.P.7)
where Jnan = ÀÜan.
(b) Observe that Rn[an
ÀÜan] = œÉ 2
n[e0
en]. Augmented matrix [an
ÀÜan] is
n √ó 2, and so [e0
en] is n √ó 2 as well. Prove that
Rn+1

an
0
0
ÀÜan
	
=

œÉ 2
ne0
Œ∑n
Œ∑n
œÉ 2
nen
	
.
(4.P.8)
What is Œ∑n? (That is, Ô¨Ånd a simple expression for it.)
(c) We wish to obtain
Rn+1[an+1
ÀÜan+1] = œÉ 2
n+1[e0
en+1]
(4.P.9)
from a manipulation of (4.P.8). To this end, Ô¨Ånd a formula for parameter
Kn ‚ààR in
Rn

an
0
0
ÀÜan
	 
1
Kn
Kn
1
	
=

œÉ 2
ne0
Œ∑n
Œ∑n
œÉ 2
nen
	 
1
Kn
Kn
1

such that (4.P.9) is obtained. This implies that we obtain the vector
recursions
an+1 =
 an
0

+ Kn
 0
ÀÜan

and
ÀÜan+1 = Kn
 an
0

+
 0
ÀÜan

.
Find the initial condition a1 ‚ààR1√ó1. What is œÉ 2
1 ?
(d) Prove that
œÉ 2
n+1 = œÉ 2
n(1 ‚àíK2
n).
(e) Summarize the algorithm obtained in the previous steps in the form of
pseudocode. The resulting algorithm is often called the Levinson‚ÄìDurbin
algorithm.
TLFeBOOK

PROBLEMS
201
(f) Count the number of arithmetic operations needed to implement the
Levinson‚ÄìDurbin algorithm in (e). Compare this number to the num-
ber of arithmetic operations needed by the general LU decomposition
algorithm presented in Section 4.5.
(g) Write a MATLAB function to implement the Levinson-Durbin algorithm.
Test your algorithm out on the matrix
R =
Ô£Æ
Ô£∞
2
1
0
1
2
1
0
1
2
Ô£π
Ô£ª.
(Hint: You will need the properties in Problem 4.20. The parameters Kn
are called reÔ¨Çection coefÔ¨Åcients, and are connected to certain problems in
electrical transmission line theory.)
4.22. This problem is about proving that solving (4.P.6) yields the LDLT decom-
position of R‚àí1
n . (Of course, Rn is real-valued, symmetric, and Toeplitz.)
Observe that (via the nesting property for Toeplitz matrices)
Rn
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
1
0
¬∑ ¬∑ ¬∑
0
0
an,1
1
¬∑ ¬∑ ¬∑
0
0
an,2
an‚àí1,1
¬∑ ¬∑ ¬∑
0
0
...
...
...
...
an,n‚àí2
an‚àí1,n‚àí3
¬∑ ¬∑ ¬∑
1
0
an,n‚àí1
an‚àí1,n‚àí2
¬∑ ¬∑ ¬∑
a2,1
1
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
#
$%
&
=Ln
=
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
œÉ 2
n
√ó
¬∑ ¬∑ ¬∑
√ó
√ó
0
œÉ 2
n‚àí1
¬∑ ¬∑ ¬∑
√ó
√ó
0
0
¬∑ ¬∑ ¬∑
√ó
√ó
...
...
...
...
0
0
¬∑ ¬∑ ¬∑
œÉ 2
2
√ó
0
0
¬∑ ¬∑ ¬∑
0
œÉ 2
1
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
#
$%
&
= ÀúUn
,
(4.P.10)
where √ó denotes ‚Äúdon‚Äôt care‚Äù entries; thus, the particular value of such an
entry is of no interest to us. Use (4.P.10) to prove that
LT
n RnLn = ÀúDn,
(4.P.11)
where ÀúDn = diag (œÉ 2
n, œÉ 2
n‚àí1, . . . , œÉ 2
2 , œÉ 2
1 ) (diagonal matrix; see the comments
following Theorem 4.4). Use (4.P.11) to prove that
R‚àí1
n
= LnDnLT
n .
(4.P.12)
What is Dn? [Hint: Using (4.P.10), and Rn = RT
n we note that LT
n RnLn can
be expressed in two distinct but equivalent ways. This observation is used
to establish (4.P.11).]
4.23. A matrix Tn is said to be strongly nonsingular (strongly regular) if all of its
leading principle submatrices are nonsingular (recall the deÔ¨Ånition in Theo-
rem 4.1). Suppose that xn = [xn,0 xn,1 ¬∑ ¬∑ ¬∑ xn,n‚àí2 xn,n‚àí1]T ‚ààCn, and deÔ¨Åne
TLFeBOOK

202
LINEAR SYSTEMS OF EQUATIONS
Zn ‚ààCn√ón according to
Znxn = [0 xn,0 xn,1 ¬∑ ¬∑ ¬∑ xn,n‚àí2]T ‚ààCn.
Thus, Zn shifts the elements of any column vector down by one position.
The top position is Ô¨Ålled in with a zero, while the last element xn,n‚àí1 is lost.
Assume that Tn is real-valued, symmetric and Toeplitz. Consider
Tn ‚àíZnTnZT
n = Xn.
(a) Find Xn.
(b) If Tn is strongly nonsingular then what is rank (Xn)? (Be careful. There
may be separate cases to consider.)
(c) Use Œ¥k (Kr¬®onecker delta) to specify Zn; that is, Zn = [zi,j], so what is
zi,j in terms of the Kr¬®onecker delta? (Hint: For example, the identity
matrix can be described as I = [Œ¥i‚àíj].)
4.24. Suppose that R ‚ààRN√óN is strongly nonsingular (see Problem 4.23 for the
deÔ¨Ånition of this term), and that R = RT . Thus, there exists the factorization
R = LNDNLT
N,
(4.P.13)
where LN is a unit lower triangular matrix, and DN is a diagonal matrix.
Let
LN = [l0 l1 ¬∑ ¬∑ ¬∑ lN‚àí2 lN‚àí1],
so li is column i of LN. Let
DN = diag(d0, d1, . . . , dN‚àí1).
Thus, via (4.P.13), we have
R =
N‚àí1

k=0
dklklT
k .
(4.P.14)
Consider the following algorithm:
R0 := R;
for n := 0 to N ‚àí1 do begin
dn := eTnRnen;
ln := d‚àí1
n Rnen;
Rn+1 := Rn ‚àídnlnlTn ;
end;
TLFeBOOK

PROBLEMS
203
As usual, we have the unit vector
ei = [0 0 ¬∑ ¬∑ ¬∑ 0
#
$%
&
i zeros
1 0 ¬∑ ¬∑ ¬∑ 0]T ‚ààRN.
The algorithm above is the Jacobi procedure (algorithm) for computing the
Cholesky factorization (recall Theorem 4.6) of R.
Is R > 0 a necessary condition for the algorithm to work? Explain. Test the
Jacobi procedure out on
R =
Ô£Æ
Ô£∞
2
1
0
1
2
1
0
1
2
Ô£π
Ô£ª.
Is R > 0? Justify your answer. How many arithmetic operations are needed to
implement the Jacobi procedure? How does this compare with the Gaussian
elimination method for general LU factorization considered in Section 4.5?
4.25. Suppose that A‚àí1 exists, and that I + V T A‚àí1U is also nonsingular. Of
course, I is the identity matrix.
(a) Prove the Sherman‚ÄìMorrison‚ÄìWoodbury formula
[A + UV T ]‚àí1 = A‚àí1 ‚àíA‚àí1U[I + V T A‚àí1U]‚àí1V T A‚àí1.
(b) Prove that if U = u ‚ààCn and V = v ‚ààCn, then
[A + uvT ]‚àí1 = A‚àí1 ‚àíA‚àí1uvT A‚àí1
1 + vT A‚àí1u.
(Comment: These identities can be used to develop adaptive Ô¨Åltering algo-
rithms, for example.)
4.26. Suppose that
A =
 a
b
0
c

.
Find conditions on a, b, and c to ensure that A > 0.
4.27. Suppose that A ‚ààRn√ón, and that A is not necessarily symmetric. We still
say that A > 0 iff xT Ax > 0 for all x Ã∏= 0 (x ‚ààRn). Show that A > 0 iff
B = 1
2(A + AT ) > 0. Matrix B is often called the symmetric part of A. (Note:
In this book, unless stated to the contrary, a pd matrix is always assumed to
be symmetric.)
TLFeBOOK

204
LINEAR SYSTEMS OF EQUATIONS
4.28. Prove that for A ‚ààRn√ón
||A||‚àû=
max
0‚â§i‚â§n‚àí1
n‚àí1

j=0
|ai,j|.
4.29. Derive Eq. (4.128) (the law of cosines).
4.30. Consider An ‚ààRn√ón such that
An =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
1
‚àí1
‚àí1
¬∑ ¬∑ ¬∑
‚àí1
‚àí1
0
1
‚àí1
¬∑ ¬∑ ¬∑
‚àí1
‚àí1
0
0
1
¬∑ ¬∑ ¬∑
‚àí1
‚àí1
...
...
...
...
...
0
0
0
¬∑ ¬∑ ¬∑
1
‚àí1
0
0
0
¬∑ ¬∑ ¬∑
0
1
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
,
A‚àí1
n
=
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
1
1
2
¬∑ ¬∑ ¬∑
2n‚àí3
2n‚àí2
0
1
1
¬∑ ¬∑ ¬∑
2n‚àí4
2n‚àí3
0
0
1
¬∑ ¬∑ ¬∑
2n‚àí5
2n‚àí4
...
...
...
...
...
0
0
0
¬∑ ¬∑ ¬∑
1
1
0
0
0
¬∑ ¬∑ ¬∑
0
1
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
Show that Œ∫‚àû(An) = n2n‚àí1. Clearly, det(An) = 1. Consider
Dn = diag(10‚àí1, 10‚àí1, . . . , 10‚àí1) ‚ààRn.
What is Œ∫p(Dn)? Clearly, det(Dn) = 10‚àín. What do these two cases say
about the relationship between det(A), and Œ∫(A) (A ‚ààRn√ón, and is nonsin-
gular) in general?
4.31. Recall Problem 1.7 (in Chapter 1). Suppose a = [a0a1 ¬∑ ¬∑ ¬∑ an]T , b = [b0b1
¬∑ ¬∑ ¬∑ bm]T , and c = [c0c1 ¬∑ ¬∑ ¬∑ cn+m]T , where
cl =
n

k=0
akbl‚àík.
Find matrix A such that c = Ab, and Ô¨Ånd matrix B such that c = Ba. What
are the sizes of matrices A, and B? (Hint: A and B will be rectangular
Toeplitz matrices. This problem demonstrates the close association between
Toeplitz matrices and the convolution operation, and so partially explains the
central importance of Toeplitz matrices in digital signal processing.)
TLFeBOOK

PROBLEMS
205
4.32. Matrix P ‚ààRn√ón is a permutation matrix if it possesses exactly one one per
row and column, and zeros everywhere else. Such a matrix simply reorders
the elements in a vector. For example
Ô£Æ
Ô£ØÔ£ØÔ£∞
0
1
0
0
0
0
1
0
1
0
0
0
0
0
0
1
Ô£π
Ô£∫Ô£∫Ô£ª
Ô£Æ
Ô£ØÔ£ØÔ£∞
x0
x1
x2
x3
Ô£π
Ô£∫Ô£∫Ô£ª=
Ô£Æ
Ô£ØÔ£ØÔ£∞
x1
x2
x0
x3
Ô£π
Ô£∫Ô£∫Ô£ª.
Show that P ‚àí1 = P T (i.e., P is an orthogonal matrix).
4.33. Find c and s in
 c
‚àís
s
c
  x0
x1

=
 7
x2
0 + x2
1
0
	
,
where s2 + c2 = 1.
4.34. Consider
A =
Ô£Æ
Ô£ØÔ£ØÔ£∞
2
1
0
0
1
2
1
0
0
1
2
1
0
0
1
2
Ô£π
Ô£∫Ô£∫Ô£ª.
Use Householder transformation matrices to Ô¨Ånd the QR factorization of
matrix A.
4.35. Consider
A =
Ô£Æ
Ô£ØÔ£ØÔ£∞
5
2
1
0
2
5
2
1
1
2
5
2
0
1
2
5
Ô£π
Ô£∫Ô£∫Ô£ª.
Using Householder transformation matrices, Ô¨Ånd orthogonal matrices Q0,
and Q1 such that
Q1Q0A =
Ô£Æ
Ô£ØÔ£ØÔ£∞
h00
h01
h02
h03
h10
h11
h12
h13
0
h21
h22
h23
0
0
h32
h33
Ô£π
Ô£∫Ô£∫Ô£ª= H.
[Comment: Matrix H is upper Hessenberg. This problem is an illustration of
a process that is important in Ô¨Ånding matrix eigenvalues (Chapter 11).]
4.36. Write a MATLAB function to implement the Householder QR factorization
algorithm as given by the pseudocode in Section 4.6 [between Eqs. (4.146),
and (4.147)]. The function must output the separate factors ÀúH T
k that make up
Q, in addition to the factors Q and R. Test your function out on the matrix
A in Problem 4.34.
TLFeBOOK

206
LINEAR SYSTEMS OF EQUATIONS
4.37. Prove Eq. (4.117), and establish that
rank (A) = rank (AT ).
4.38. If A ‚ààCn√ón, then the trace of A is given by
Tr(A) =
n‚àí1

k=0
ak,k
(i.e., is the sum of the main diagonal elements of matrix A). Prove that
||A||2
F = Tr(AAH )
[recall (4.36a)].
4.39. Suppose that A, B ‚ààRn√ón, and Q ‚ààRn√ón is orthogonal; then, if A =
QBQT , prove that
Tr(A) = Tr(B).
4.40. Recall Theorem 4.4. Prove that for A ‚ààRn√ón
||A||2
F =
p‚àí1

k=0
œÉ 2
k .
(Hint: Use the results from Problems 4.38 and 4.39.)
4.41. Consider Theorem 4.9. Is Œ± < 1 a necessary condition for convergence?
Explain.
4.42. Suppose that A ‚ààRn√ón is pd and prove that the JOR method (Section 4.7)
converges- if
0 < œâ <
2
œÅ(D‚àí1A).
4.43. Repeat the analysis made in Example 4.8, but instead use the matrix
A =
Ô£Æ
Ô£ØÔ£ØÔ£∞
4
2
1
0
1
4
1
0
0
1
4
1
0
0
2
4
Ô£π
Ô£∫Ô£∫Ô£ª.
This will involve writing and running a suitable MATLAB function. Find the
optimum choice for œâ to an accuracy of ¬±0.02.
TLFeBOOK

5
Orthogonal Polynomials
5.1
INTRODUCTION
Orthogonal polynomials arise in highly diverse settings. They can be solutions to
special classes of differential equations that arise in mathematical physics problems.
They are vital in the design of analog and digital Ô¨Ålters. They arise in numerical
integration methods, and they have a considerable role to play in solving least-
squares and uniform approximation problems.
Therefore, in this chapter we begin by considering some of the properties shared
by all orthogonal polynomials. We then consider the special cases of Chebyshev,
Hermite, and Legendre polynomials. Additionally, we consider the application
of orthogonal polynomials to least-squares and uniform approximation problems.
However, we emphasize the case of least-squares approximation, which was Ô¨Årst
considered in some depth in Chapter 4. The approach to least-squares problems
taken here alleviates some of the concerns about ill-conditioning that were noted
in Chapter 4.
5.2
GENERAL PROPERTIES OF ORTHOGONAL POLYNOMIALS
We are interested here in the inner product space L2(D), where D is the domain
of deÔ¨Ånition of the functions in the space. Typically, D = [a, b], D = R, or D =
[0, ‚àû). We shall, as in Chapter 4, assume that all members of L2(D) are real-valued
to simplify matters. So far, our inner product has been
‚ü®f, g‚ü©=

D
f (x)g(x) dx,
(5.1)
but now we consider the weighted inner product
‚ü®f, g‚ü©=

D
w(x)f (x)g(x) dx,
(5.2)
where w(x) ‚â•0 (x ‚ààD) is the weighting function. This includes (5.1) as a special
case for which w(x) = 1 for all x ‚ààD.
An Introduction to Numerical Analysis for Electrical and Computer Engineers, by C.J. Zarowski
ISBN 0-471-46737-5
c‚Éù2004 John Wiley & Sons, Inc.
207
TLFeBOOK

208
ORTHOGONAL POLYNOMIALS
Our goal is to consider polynomials
œÜn(x) =
n

k=0
œÜn,kxk, x ‚ààD
(5.3)
of degree n such that
‚ü®œÜn, œÜm‚ü©= Œ¥n‚àím
(5.4)
for all n, m ‚ààZ+. The inner product is that of (5.2). Our polynomials {œÜn(x)|n ‚àà
Z+}. are orthogonal polynomials on D with respect to weighting function w(x).
Changing D and/or w(x) will generate very different polynomials, and we will
consider important special cases later. However, all orthogonal polynomials possess
certain features in common with each other regardless of the choice of D or w(x).
We shall consider a few of these in this section.
If p(x) is a polynomial of degree n, then we may write deg(p(x)) = n.
Theorem 5.1: Any three consecutive orthogonal polynomials are related by the
three-term recurrence formula (relation)
œÜn+1(x) = (Anx + Bn)œÜn(x) + CnœÜn‚àí1(x),
(5.5)
where
An = œÜn+1,n+1
œÜn,n
,
Bn = œÜn+1,n+1
œÜn,n
 œÜn+1,n
œÜn+1,n+1
‚àíœÜn,n‚àí1
œÜn,n
!
,
Cn = ‚àíœÜn+1,n+1œÜn‚àí1,n‚àí1
œÜ2n,n
.
(5.6)
Proof
Our proof is a somewhat expanded version of that in Isaacson and Keller
[1, pp. 204‚Äì205].
Observe that for An = œÜn+1,n+1/œÜn,n, we have
qn(x) = œÜn+1(x) ‚àíAnxœÜn(x) =
n+1

k=0
œÜn+1,kxk ‚àíAn
n

k=0
œÜn,kxk+1
=
n+1

k=0
œÜn+1,kxk ‚àíAn
n+1

j=1
œÜn,j‚àí1xj
=
n

k=0
œÜn+1,kxk ‚àíAn
n

j=1
œÜn,j‚àí1xj + [œÜn+1,n+1 ‚àíAnœÜn,n]xn+1
=
n

k=0
œÜn+1,kxk ‚àíAn
n

j=1
œÜn,j‚àí1xj,
TLFeBOOK

GENERAL PROPERTIES OF ORTHOGONAL POLYNOMIALS
209
which is a polynomial of degree at most n, i.e., deg(qn(x)) ‚â§n. Thus, for suit-
able Œ±k
qn(x) =
n

k=0
Œ±kœÜk(x),
so because ‚ü®œÜk, œÜj‚ü©= Œ¥k‚àíj, we have Œ±j = ‚ü®qn, œÜj‚ü©. In addition
œÜn+1(x) ‚àíAnxœÜn(x) = Œ±nœÜn(x) + Œ±n‚àí1œÜn‚àí1(x) +
n‚àí2

k=0
Œ±kœÜk(x).
(5.7)
Now deg(xœÜj(x)) ‚â§n ‚àí1 for j ‚â§n ‚àí2. Thus, there are Œ≤k such that
xœÜj(x) =
n‚àí1

k=0
Œ≤kœÜk(x)
so ‚ü®œÜr, xœÜj‚ü©= 0 if r > n ‚àí1, or
‚ü®œÜr, xœÜj‚ü©= 0
(5.8)
for j = 0, 1, . . . , n ‚àí2. From (5.7) via (5.8), we obtain
‚ü®œÜn+1 ‚àíAnxœÜn, œÜk‚ü©= ‚ü®œÜn+1, œÜk‚ü©‚àíAn‚ü®œÜn, xœÜk‚ü©= 0
for k = 0, 1, . . . , n ‚àí2. This is the inner product of œÜk(x) with the left-hand side
of (5.7). For the right-hand side of (5.7)
0 =

Œ±nœÜn + Œ±n‚àí1œÜn‚àí1 +
n‚àí2

j=0
Œ±jœÜj, œÜk

= Œ±n‚ü®œÜn, œÜk‚ü©+ Œ±n‚àí1‚ü®œÜn‚àí1, œÜk‚ü©+
n‚àí2

j=0
Œ±j‚ü®œÜj, œÜk‚ü©
=
n‚àí2

j=0
Œ±j‚ü®œÜj, œÜk‚ü©
again for k = 0, . . . , n ‚àí2. We can only have n‚àí2
j=0 Œ±j‚ü®œÜj, œÜk‚ü©= 0 if Œ±j = 0 for
j = 0, 1, . . . , n ‚àí2. Thus, (5.7) reduces to
œÜn+1(x) ‚àíAnxœÜn(x) = Œ±nœÜn(x) + Œ±n‚àí1œÜn‚àí1(x)
or
œÜn+1(x) = (Anx + Œ±n)œÜn(x) + Œ±n‚àí1œÜn‚àí1(x),
(5.9)
TLFeBOOK

210
ORTHOGONAL POLYNOMIALS
which has the form of (5.5). We now need to verify that Œ±n = Bn, Œ±n‚àí1 = Cn as
in (5.6). From (5.9)
œÜn(x) = An‚àí1xœÜn‚àí1(x) + Œ±n‚àí1œÜn‚àí1(x) + Œ±n‚àí2œÜn‚àí2(x)
or
xœÜn‚àí1(x) =
1
An‚àí1
œÜn(x) +

‚àí
1
An‚àí1
(Œ±n‚àí1œÜn‚àí1(x) + Œ±n‚àí2œÜn‚àí2(x))

#
$%
&
=pn‚àí1(x)
(5.10)
for which deg(pn‚àí1(x)) ‚â§n ‚àí1. Thus, from (5.9)
‚ü®œÜn+1, œÜn‚àí1‚ü©= An‚ü®xœÜn, œÜn‚àí1‚ü©+ Œ±n‚ü®œÜn, œÜn‚àí1‚ü©+ Œ±n‚àí1‚ü®œÜn‚àí1, œÜn‚àí1‚ü©
so that
Œ±n‚àí1 = ‚àíAn‚ü®xœÜn, œÜn‚àí1‚ü©= ‚àíAn‚ü®œÜn, xœÜn‚àí1‚ü©,
and via (5.10)
Œ±n‚àí1 = ‚àíAn
<
œÜn,
1
An‚àí1
œÜn + pn‚àí1
=
,
which becomes
Œ±n‚àí1 = ‚àíAn
An‚àí1
‚ü®œÜn, œÜn‚ü©= ‚àíAn
An‚àí1
or
Cn = Œ±n‚àí1 = ‚àíœÜn+1,n+1
œÜn,n
œÜn‚àí1,n‚àí1
œÜn,n
,
which is the expression for Cn in (5.6). Expanding (5.5)
n+1

k=0
œÜn+1,kxk = An
n+1

k=1
œÜn,k‚àí1xk + Bn
n

k=0
œÜn,kxk + Cn
n‚àí1

k=0
œÜn‚àí1,kxk,
where, on comparing the terms for k = n, we see that
œÜn+1,n = AnœÜn,n‚àí1 + BnœÜn,n
(since œÜn‚àí1,n = 0). Therefore
Bn =
1
œÜn,n
[œÜn+1,n ‚àíAnœÜn,n‚àí1] =
1
œÜn,n

œÜn+1,n ‚àíœÜn+1,n+1
œÜn,n
œÜn,n‚àí1

= œÜn+1,n+1
œÜn,n
 œÜn+1,n
œÜn+1,n+1
‚àíœÜn,n‚àí1
œÜn,n

,
which is the expression for Bn in (5.6).
TLFeBOOK

GENERAL PROPERTIES OF ORTHOGONAL POLYNOMIALS
211
Theorem 5.2: Orthogonal polynomials satisfy the Christoffel‚ÄìDarboux formula
(relation)
(x ‚àíy)
n

j=0
œÜj(x)œÜj(y) =
œÜn,n
œÜn+1,n+1
[œÜn+1(x)œÜn(y) ‚àíœÜn+1(y)œÜn(x)].
(5.11)
Proof
The proof is an expanded version of that from Isaacson and Keller [1,
p. 205].
From (5.5), we obtain
œÜn(y)œÜn+1(x) = (Anx + Bn)œÜn(x)œÜn(y) + CnœÜn‚àí1(x)œÜn(y),
(5.12)
so reversing the roles of x and y gives
œÜn(x)œÜn+1(y) = (Any + Bn)œÜn(y)œÜn(x) + CnœÜn‚àí1(y)œÜn(x).
(5.13)
Subtracting (5.13) from (5.12) yields
œÜn(y)œÜn+1(x) ‚àíœÜn(x)œÜn+1(y) = An(x ‚àíy)œÜn(x)œÜn(y)
+ Cn(œÜn‚àí1(x)œÜn(y) ‚àíœÜn(x)œÜn‚àí1(y)).
(5.14)
We note that
Cn
An
= ‚àíœÜn‚àí1,n‚àí1
œÜn,n
= ‚àí
1
An‚àí1
,
(5.15)
so (5.14) may be rewritten as
(x ‚àíy)œÜn(x)œÜn(y) = A‚àí1
n (œÜn+1(x)œÜn(y) ‚àíœÜn+1(y)œÜn(x))
‚àíA‚àí1
n‚àí1(œÜn(x)œÜn‚àí1(y) ‚àíœÜn‚àí1(x)œÜn(y)).
Now consider
(x ‚àíy)
n

j=0
œÜj(x)œÜj(y) =
n

j=0
9
A‚àí1
j [œÜj+1(x)œÜj(y) ‚àíœÜj+1(y)œÜj(x)]
‚àíA‚àí1
j‚àí1[œÜj(x)œÜj‚àí1(y) ‚àíœÜj‚àí1(x)œÜj(y)]
:
.
(5.16)
Since Aj = œÜj+1,j+1/œÜj,j, we have A‚àí1
‚àí1 = œÜ‚àí1,‚àí1/œÜ0,0 = 0 because œÜ‚àí1,‚àí1 = 0.
Taking this into account, the summation on the right-hand side of (5.16) reduces
due to the cancellation of terms,1 and so (5.16) Ô¨Ånally becomes
(x ‚àíy)
n

j=0
œÜj(x)œÜj(y) =
œÜn,n
œÜn+1,n+1
[œÜn+1(x)œÜn(y) ‚àíœÜn+1(y)œÜn(x)],
which is (5.11).
1The cancellation process seen here is similar to the one that occurred in the derivation of the Dirichlet
kernel identity (3.24). This mathematical technique seems to be a recurring theme in analysis.
TLFeBOOK

212
ORTHOGONAL POLYNOMIALS
The following corollary to Theorem 5.2 is from Hildebrand [2, p. 342]. However,
there appears to be no proof in Ref. 2, so one is provided here.
Corollary 5.1
With œÜ(1)
k (x) = dœÜk(x)/dx we have
n

j=0
œÜ2
j (x) =
œÜn,n
œÜn+1,n+1
[œÜ(1)
n+1(x)œÜn(x) ‚àíœÜ(1)
n (x)œÜn+1(x)].
(5.17)
Proof
Since
œÜn+1(x)œÜn(y) ‚àíœÜn+1(y)œÜn(x) = [œÜn+1(x) ‚àíœÜn+1(y)]œÜn(y)
‚àí[œÜn(x) ‚àíœÜn(y)]œÜn+1(y)
via (5.11)
n

j=0
œÜj(x)œÜj(y)
=
œÜn,n
œÜn+1,n+1
œÜn+1(x) ‚àíœÜn+1(y)
x ‚àíy
œÜn(y) ‚àíœÜn(x) ‚àíœÜn(y)
x ‚àíy
œÜn+1(y)

. (5.18)
Letting y ‚Üíx in (5.18) immediately yields (5.17). This is so from the deÔ¨Ånition of
the derivative, and the fact that all polynomials may be differentiated any number
of times.
The three-term recurrence relation is a vital practical method for orthogonal
polynomial generation. However, an alternative approach is the following.
If deg(qr‚àí1(x)) = r ‚àí1 (qr‚àí1(x) is an arbitrary degree r ‚àí1 polynomial), then
‚ü®œÜr, qr‚àí1‚ü©=

D
w(x)œÜr(x)qr‚àí1(x) dx = 0.
(5.19)
DeÔ¨Åne
w(x)œÜr(x) = drGr(x)
dxr
= G(r)
r (x)
(5.20)
so that (5.19) becomes

D
G(r)
r (x)qr‚àí1(x) dx = 0.
(5.21)
If we repeatedly integrate by parts (using q(r)
r‚àí1(x) = 0 for all x), we obtain

D
G(r)
r (x)qr‚àí1(x) dx = G(r‚àí1)
r
(x)qr‚àí1(x)|D ‚àí

D
G(r‚àí1)
r
(x)q(1)
r‚àí1(x) dx
TLFeBOOK

GENERAL PROPERTIES OF ORTHOGONAL POLYNOMIALS
213
= G(r‚àí1)
r
(x)qr‚àí1(x)|D ‚àíG(r‚àí2)
r
(x)q(1)
r‚àí1(x)|D
+

D
G(r‚àí2)
r
(x)q(2)
r‚àí1(x) dx
= G(r‚àí1)
r
(x)qr‚àí1(x)|D ‚àíG(r‚àí2)
r
(x)q(1)
r‚àí1(x)|D
+ G(r‚àí3)
r
(x)q(2)
r‚àí1(x)|D +

D
G(r‚àí3)
r
(x)q(3)
r‚àí1(x) dx
and so on. Finally
[G(r‚àí1)
r
qr‚àí1 ‚àíG(r‚àí2)
r
q(1)
r‚àí1 + G(r‚àí3)
r
q(2)
r‚àí1 ‚àí¬∑ ¬∑ ¬∑ + (‚àí1)r‚àí1Grq(r‚àí1)
r‚àí1 ]D = 0,
(5.22a)
or alternatively
 r

k=1
(‚àí1)k‚àí1G(r‚àík)
r
q(k‚àí1)
r‚àí1
	
D
= 0.
(5.22b)
Since from (5.20)
œÜr(x) =
1
w(x)
drGr(x)
dxr
,
(5.23)
which is a polynomial of degree r, it must be the case that Gr(x) satisÔ¨Åes the
differential equation
dr+1
dxr+1

1
w(x)
drGr(x)
dxr

= 0
(5.24)
for x ‚ààD. Recalling D = [a, b] and allowing a ‚Üí‚àí‚àû, b ‚Üí‚àû(so we may have
D = [a, ‚àû), or D = R), Eq. (5.22a) must be satisÔ¨Åed for any values of q(k)
r‚àí1(a) and
q(k)
r‚àí1(b) (k = 0, 1, . . . , r ‚àí1). This implies that we have the boundary conditions
G(k)
r (a) = 0, G(k)
r (b) = 0
(5.25)
for k = 0, 1, . . . , r ‚àí1. These restrict the solution to (5.24). It can be shown [6]
that (5.24) has a nontrivial2 solution for all r ‚ààZ+ assuming that w(x) ‚â•0 for
all x ‚ààD, and that the moments
"
D xkw(x) dx exist for all k ‚ààZ+. Proving this
is difÔ¨Åcult, and so will not be considered. The expression in (5.23) may be called
a Rodrigues formula for œÜr(x) (although it should be said that this terminology
usually arises only in the context of Legendre polynomials).
The Christoffel‚ÄìDarboux formulas arise in various settings. For example, they
are relevant to the problem of designing Savitzky‚ÄìGolay smoothing digital Ô¨Ålters
[3], and they arise in numerical integration methods [2, 4]. Another way in which
the Christoffel‚ÄìDarboux formula of (5.11) can make an appearance is as follows.
2The trivial solution for a differential equation is the identically zero function.
TLFeBOOK

214
ORTHOGONAL POLYNOMIALS
We may wish to approximate f (x) ‚ààL2(D) as
f (x) ‚âà
n

j=0
ajœÜj(x),
(5.26)
so the residual is
rn(x) = f (x) ‚àí
n

j=0
ajœÜj(x).
(5.27)
Adopting the now familiar least-squares approach, we select aj to minimize func-
tional V (a) = ||rn||2 = ‚ü®rn, rn‚ü©
V (a) =

D
w(x)r2
n(x) dx =

D
w(x)
Ô£Æ
Ô£∞f (x) ‚àí
n

j=0
ajœÜj(x)
Ô£π
Ô£ª
2
dx,
(5.28)
where a = [a0 a1 ¬∑ ¬∑ ¬∑ an]T ‚ààRn+1. Of course, this expands to become
V (a) =

D
w(x)f 2(x) dx ‚àí2
n

j=0
aj

D
w(x)f (x)œÜj(x) dx
+
n

j=0
n

k=0
ajak

D
w(x)œÜj(x)œÜk(x) dx,
so if g = [g0 g1 ¬∑ ¬∑ ¬∑ gn]T with gj =
"
D w(x)f (x)œÜj(x) dx, and R = [ri,j] ‚àà
R(n+1)√ó(n+1) with ri,j =
"
D w(x)œÜi(x)œÜj(x) dx, and œÅ =
"
D w(x)f 2(x) dx, then
V (a) = œÅ ‚àí2aT g + aT Ra.
(5.29)
But ri,j = ‚ü®œÜi, œÜj‚ü©= Œ¥i‚àíj [via (5.4)], so we have R = I (identity matrix). Imme-
diately, one of the advantages of working with orthogonal polynomials is that R
is perfectly conditioned (contrast this with the Hilbert matrix of Chapter 4). This
in itself is a powerful incentive to consider working with orthogonal polynomial
expansions. Another nice consequence is that the optimal solution ÀÜa satisÔ¨Åes
ÀÜa = g,
(5.30)
that is
ÀÜaj =

D
w(x)f (x)œÜj(x) dx = ‚ü®f, œÜj‚ü©,
(5.31)
where j ‚ààZn+1. If a = ÀÜa in (5.27) then we have the optimal residual
ÀÜrn(x) = f (x) ‚àí
n

j=0
ÀÜajœÜj(x).
(5.32)
TLFeBOOK

GENERAL PROPERTIES OF ORTHOGONAL POLYNOMIALS
215
We may substitute (5.31) into (5.32) to obtain
ÀÜrn(x) = f (x) ‚àí
n

j=0

D
w(y)f (y)œÜj(y) dy

œÜj(x)
= f (x) ‚àí

D
f (y)w(y)
Ô£Æ
Ô£∞
n

j=0
œÜj(x)œÜj(y)
Ô£π
Ô£ªdy.
(5.33)
For convenience we deÔ¨Åne the kernel function3
Kn(x, y) =
n

j=0
œÜj(x)œÜj(y),
(5.34)
so that (5.33) becomes
ÀÜrn(x) = f (x) ‚àí

D
w(y)f (y)Kn(x, y) dy.
(5.35)
Clearly, Kn(x, y) in (5.34) has the alternative formula given by (5.11). Now con-
sider

D
f (x)w(y)Kn(x, y) dy = f (x)

D
w(y)
n

j=0
œÜj(x)œÜj(y) dy
= f (x)
n

j=0
œÜj(x)

D
w(y)œÜj(y) dy
= f (x)
n

j=0
œÜj(x)‚ü®1, œÜj‚ü©
= f (x)
n

j=0
œÜj(x)
<œÜ0(x)
œÜ0,0
, œÜj(x)
=
= f (x)œÜ0(x)
œÜ0,0
= f (x)
because œÜ0(x) = œÜ0,0 for x ‚ààD, and ‚ü®œÜ0, œÜj‚ü©= Œ¥j. Thus, (5.35) becomes
ÀÜrn(x) =

D
f (x)w(y)Kn(x, y) dy ‚àí

D
f (y)w(y)Kn(x, y) dy
=

D
w(y)Kn(x, y)[f (x) ‚àíf (y)] dy.
(5.36)
3Recall the Dirichlet kernel of Chapter 3, which we now see is really just a special instance of the
general idea considered here.
TLFeBOOK

216
ORTHOGONAL POLYNOMIALS
The optimal residual (optimal error) ÀÜrn(x) presumably gets smaller in some sense
as n ‚Üí‚àû. Clearly, insight into how it behaves in the limit as n ‚Üí‚àûcan be
provided by (5.36). The behavior certainly depends on f (x), w(x), and the kernel
Kn(x, y). Intuitively, the summation expression for Kn(x, y) in (5.34) is likely
to be less convenient to work with than the alternative expression we obtain
from (5.11).
Some basic results on polynomial approximations are as follows.
Theorem 5.3: Weierstrass‚Äô Approximation Theorem
Let f (x) be continu-
ous on the closed interval [a, b]. For any œµ > 0 there exists an integer N = N(œµ),
and a polynomial pN(x) ‚ààPN[a, b] (deg(pN(x)) ‚â§N) such that
|f (x) ‚àípN(x)| ‚â§œµ
for all x ‚àà[a, b].
Various proofs of this result exist in the literature (e.g., see Rice [5, p. 121] and
Isaacson and Keller [1, pp. 183‚Äì186]), but we omit them here. We see that the
convergence of pN(x) to f (x) is uniform as N ‚Üí‚àû(recall DeÔ¨Ånition 3.4 in
Chapter 3). Theorem 5.3 states that any function continuous on an interval may be
approximated with a polynomial to arbitrary accuracy. Of course, a large degree
polynomial may be needed to achieve a particular level of accuracy depending upon
f (x). We also remark that Weierstrass‚Äô theorem is an existence theorem. It claims
the existence of a polynomial that uniformly approximates a continuous function
on [a, b], but it does not tell us how to Ô¨Ånd the polynomial. Some information
about the convergence behavior of a least-squares approximation is provided by
the following theorem.
Theorem 5.4: Let D = [a, b], w(x) = 1 for all x ‚ààD. Let f (x) be continuous
on D, and let
qn(x) =
n

j=0
ÀÜajœÜj(x)
(least-squares polynomial approximation to f (x) on D), so ÀÜaj = ‚ü®f, œÜj‚ü©=
" b
a f (x)œÜj(x) dx. Then
lim
n‚Üí‚àûV (ÀÜa) = lim
n‚Üí‚àû
 b
a
[f (x) ‚àíqn(x)]2 dx = 0,
and we have Parseval‚Äôs equality
 b
a
f 2(x) dx =
‚àû

j=0
ÀÜa2
j.
(5.37)
TLFeBOOK

GENERAL PROPERTIES OF ORTHOGONAL POLYNOMIALS
217
Proof
We use proof by contradiction. Assume that limn‚Üí‚àûV (ÀÜa) = Œ¥ > 0.
Pick any œµ > 0 such that œµ2 =
1
2(b‚àía)Œ¥. By Theorem 5.3 (Weierstrass‚Äô theorem)
there is a polynomial pm(x) such that |f (x) ‚àípm(x)| ‚â§œµ for x ‚ààD. Thus
 b
a
[f (x) ‚àípm(x)]2 dx ‚â§œµ2[b ‚àía] = 1
2Œ¥.
Now via (5.29) and (5.30)
V (ÀÜa) = œÅ ‚àíÀÜaT ÀÜa,
that is
V (ÀÜa) =
 b
a
f 2(x) dx ‚àí
n

j=0
ÀÜa2
j,
and V (a) ‚â•0 for all a ‚ààRn+1. So we have Bessel‚Äôs inequality
V (ÀÜa) =
 b
a
f 2(x) dx ‚àí
n

j=0
ÀÜa2
j ‚â•0,
(5.38)
and V (ÀÜa) must be a nonincreasing function of n. Hence the least-squares approxi-
mation of degree m, say, qm(x), satisÔ¨Åes
1
2Œ¥ ‚â•
 b
a
[f (x) ‚àíqm(x)]2 dx ‚â•Œ¥
which is a contradiction unless Œ¥ = 0. Since Œ¥ = 0, we must have (5.37).
We observe that Parseval‚Äôs equality of (5.37) relates the energy of f (x) to
the energy of the coefÔ¨Åcients ÀÜaj. The convergence behavior described by Theo-
rem 5.4 is sometimes referred to as convergence in the mean [1, pp. 197‚Äì198].
This theorem does not say what happens if f (x) is not continuous on [a, b]. The
result (5.36) is independent regardless of whether f (x) is continuous. It is a poten-
tially more powerful result for this reason. It turns out that the convergence of
the least-squares polynomial sequence (qn(x)) to f (x) is pointwise in general but,
depending on œÜj(x) and f (x), the convergence can sometimes be uniform. For uni-
form convergence, f (x) must be sufÔ¨Åciently smooth. The pointwise convergence
of the orthogonal polynomial series when f (x) has a discontinuity implies that the
Gibbs phenomenon can be expected. (Recall this phenomenon in the context of the
Fourier series expansion as seen in Section 3.4.)
We have seen polynomial approximation to functions in Chapter 3. There we saw
that the Taylor formula is a polynomial approximation to a function with a number
of derivatives equal to the degree of the Taylor polynomial. This approximation
technique is obviously limited to functions that are sufÔ¨Åciently differentiable. But
our present polynomial approximation methodology has no such limitation.
TLFeBOOK

218
ORTHOGONAL POLYNOMIALS
We remark that Theorem 5.4 suggests (but does not prove) that if f (x) ‚àà
L2[a, b], then
f (x) =
‚àû

j=0
‚ü®f, œÜj‚ü©œÜj(x),
(5.39)
where
‚ü®f, œÜj‚ü©=
 b
a
f (x)œÜj(x) dx.
(5.40)
This has the basic form of the Fourier series expansion that was Ô¨Årst seen in
Chapter 1. For this reason (5.39) is sometimes called a generalized Fourier series
expansion, although in the most general form of this idea the orthogonal functions
are not necessarily always polynomials. The idea of a generalized Fourier series
expansion can be extended to domains such as D = [0, ‚àû), or D = R, and to any
weighting functions that lead to solutions to (5.24).
The next three sections consider the Chebyshev, Hermite, and Legendre poly-
nomials as examples of how to apply the core theory of this section.
5.3
CHEBYSHEV POLYNOMIALS
Suppose that D = [a, b] and recall (5.28),
V (a) =
 b
a
w(x)r2
n(x) dx,
(5.41)
which is the (weighted) energy of the approximation error (residual) in (5.27):
rn(x) = f (x) ‚àí
n

j=0
ajœÜj(x).
(5.42)
The weighting function w(x) is often selected to give more or less weight to errors
in different places on the interval [a, b]. This is intended to achieve a degree of
control over error behavior. If w(x) = c > 0 for x ‚àà[a, b], then equal importance
is given to errors across the interval. This choice (with c = 1) gives rise to the
Legendre polynomials, and will be considered later. If we wish to give more weight
to errors at the ends of the interval, then a popular instance of this is for D = [‚àí1, 1]
with weighting function
w(x) =
1
‚àö
1 ‚àíx2 .
(5.43)
This choice leads to the famous Chebyshev polynomials of the Ô¨Årst kind. The reader
will most likely see these applied to problems in analog and digital Ô¨Ålter design in
subsequent courses. For now, we concentrate on their basic theory.
TLFeBOOK

CHEBYSHEV POLYNOMIALS
219
The following lemma and ideas expressed in it are pivotal in understanding the
Chebyshev polynomials and how they are constructed.
Lemma 5.1:
If n ‚ààZ+, then
cos nŒ∏ =
n

k=0
Œ≤n,k cosk Œ∏
(5.44)
for suitable Œ≤n,k ‚ààR.
Proof
First of all recall the trigonometric identities
cos(m + 1)Œ∏ = cos mŒ∏ cos Œ∏ ‚àísin mŒ∏ sin Œ∏
cos(m ‚àí1)Œ∏ = cos mŒ∏ cos Œ∏ + sin mŒ∏ sin Œ∏
(which follow from the more basic identity cos(a + b) = cos a cos b ‚àísin a sin b).
From the sum of these identities
cos(m + 1)Œ∏ = 2 cos mŒ∏ cos Œ∏ ‚àícos(m ‚àí1)Œ∏.
(5.45)
For n = 1 in (5.44), we have Œ≤1,0 = 0, Œ≤1,1 = 1, and for n = 0, we have Œ≤0,0 =
1. These will form initial conditions in a recursion that we now derive using
mathematical induction.
Assume that (5.44) is valid both for n = m and for n = m ‚àí1, and so
cos mŒ∏ =
m

k=0
Œ≤m,k cosk Œ∏
cos(m ‚àí1)Œ∏ =
m‚àí1

k=0
Œ≤m‚àí1,k cosk Œ∏,
and so via (5.45)
cos(m + 1)Œ∏ = 2 cos Œ∏
m

k=0
Œ≤m,k cosk Œ∏ ‚àí
m‚àí1

k=0
Œ≤m‚àí1,k cosk Œ∏
= 2
m

k=0
Œ≤m,k cosk+1 Œ∏ ‚àí
m‚àí1

k=0
Œ≤m‚àí1,k cosk Œ∏
= 2
m+1

r=1
Œ≤m,r‚àí1 cosr Œ∏ ‚àí
m‚àí1

r=0
Œ≤m‚àí1,r cosr Œ∏
TLFeBOOK

220
ORTHOGONAL POLYNOMIALS
=
m+1

k=0
[2Œ≤m,k‚àí1 ‚àíŒ≤m‚àí1,k] cosk Œ∏(Œ≤m,k = 0 for k < 0, and k > m)
=
m+1

k=0
Œ≤m+1,k cosk Œ∏,
which is to say that
Œ≤m+1,k = 2Œ≤m,k‚àí1 ‚àíŒ≤m‚àí1,k.
(5.46)
This is the desired three-term recursion for the coefÔ¨Åcients in (5.44). As a conse-
quence of this result, Eq. (5.44) is valid for n = m + 1 and thus is valid for all
n ‚â•0 by mathematical induction.
This lemma states that cos nŒ∏ may be expressed as a polynomial of degree n in
cos Œ∏. Equation (5.46) along with the initial conditions
Œ≤0,0 = 1,
Œ≤1,0 = 0,
Œ≤1,1 = 1
(5.47)
tells us how to Ô¨Ånd the polynomial coefÔ¨Åcients. For example, from (5.46)
Œ≤2,k = 2Œ≤1,k‚àí1 ‚àíŒ≤0,k
for k = 0, 1, 2. [In general, we evaluate (5.46) for k = 0, 1, . . . , m + 1.] Therefore
Œ≤2,0 = 2Œ≤1,‚àí1 ‚àíŒ≤0,0 = ‚àí1,
Œ≤2,1 = 2Œ≤1,0 ‚àíŒ≤0,1 = 0,
Œ≤2,2 = 2Œ≤1,1 ‚àíŒ≤0,2 = 2
which implies that cos 2Œ∏ = ‚àí1 + 2 cos2 Œ∏. This is certainly true as it is a well-
known trigonometric identity.
Lemma 5.1 possesses a converse. Any polynomial in cos Œ∏ of degree n can be
expressed as a linear combination of members from the set {cos kŒ∏|k = 0, 1, . . . , n}.
We now have enough information to derive the Chebyshev polynomials. Recall-
ing (5.19) we need a polynomial œÜr(x) of degree r such that
 1
‚àí1
1
‚àö
1 ‚àíx2 œÜr(x)qr‚àí1(x) dx = 0,
(5.48)
where qr‚àí1(x) is an arbitrary polynomial such that deg(qr‚àí1(x)) ‚â§r ‚àí1. Let us
change variables according to
x = cos Œ∏.
(5.49)
TLFeBOOK

CHEBYSHEV POLYNOMIALS
221
Then dx = ‚àísin Œ∏ dŒ∏, and x ‚àà[‚àí1, 1] maps to Œ∏ ‚àà[0, œÄ]. Thus
 1
‚àí1
1
‚àö
1 ‚àíx2 œÜr(x)qr‚àí1(x) dx = ‚àí
 0
œÄ
1
‚àö
1 ‚àícos2 Œ∏
œÜr(cos Œ∏)qr‚àí1(cos Œ∏) sin Œ∏ dŒ∏
=
 œÄ
0
œÜr(cos Œ∏)qr‚àí1(cos Œ∏) dŒ∏ = 0.
(5.50)
Because of Lemma 5.1 (and the above mentioned converse to it)
 œÄ
0
œÜr(cos Œ∏) cos kŒ∏ dŒ∏ = 0
(5.51)
for k = 0, 1, . . . , r ‚àí1. Consider œÜr(cos Œ∏) = Cr cos rŒ∏, then
Cr
 œÄ
0
cos rŒ∏ cos kŒ∏ dŒ∏ = 1
2Cr
 œÄ
0
[cos(r + k)Œ∏ + cos(r ‚àík)Œ∏] dŒ∏
= 1
2Cr

1
r + k sin(r + k)Œ∏ +
1
r ‚àík sin(r ‚àík)Œ∏
œÄ
0
= 0
for k = 0, 1, . . . , r ‚àí1. Thus, we may indeed choose
œÜr(x) = Cr cos[r cos‚àí1 x].
(5.52)
Constant Cr is selected to normalize the polynomial according to user requirements.
Perhaps the most common choice is simply to set Cr = 1 for all r ‚ààZ+. In this
case we set Tr(x) = œÜr(x):
Tr(x) = cos[r cos‚àí1 x].
(5.53)
These are the Chebyshev polynomials of the Ô¨Årst kind.
By construction, if r Ã∏= k, then
‚ü®Tr, Tk‚ü©=
 1
‚àí1
1
‚àö
1 ‚àíx2 Tr(x)Tk(x) dx = 0.
Consider r = k
||Tr||2 =
 1
‚àí1
1
‚àö
1 ‚àíx2 T 2
r (x) dx
=
 1
‚àí1
1
‚àö
1 ‚àíx2 cos2[r cos‚àí1 x] dx,
(5.54)
and apply (5.49). Thus
||Tr||2 =
 œÄ
0
cos2 rŒ∏ dŒ∏ =
 œÄ,
r = 0
1
2œÄ,
r > 0 .
(5.55)
TLFeBOOK

222
ORTHOGONAL POLYNOMIALS
We have claimed œÜr(x) in (5.52), and hence Tr(x) in (5.53), are polynomials. This is
not immediately obvious, given that the expressions are in terms of trigonometric
functions. We will now conÔ¨Årm that Tr(x) is indeed a polynomial in x for all
r ‚ààZ+. Our approach follows the proof of Lemma 5.1.
First, we observe that
T0(x) = cos(0 ¬∑ cos‚àí1 x) = 1,
T1(x) = cos(1 ¬∑ cos‚àí1 x) = x.
(5.56)
Clearly, these are polynomials in x. Once again (see the proof of Lemma 5.1)
Tn+1(x) = cos(n + 1)Œ∏(Œ∏ = cos‚àí1 x)
= cos nŒ∏ cos Œ∏ ‚àí(cos(n ‚àí1)Œ∏ ‚àícos nŒ∏ cos Œ∏)
= 2 cos nŒ∏ cos Œ∏ ‚àícos(n ‚àí1)Œ∏
= 2xTn(x) ‚àíTn‚àí1(x),
that is
Tn+1(x) = 2xTn(x) ‚àíTn‚àí1(x)
(5.57)
for n ‚ààN. The initial conditions are expressed in (5.56). We see that this is a
special case of (5.5) (Theorem 5.1). It is immediately clear from (5.57) and (5.56)
that Tn(x) is a polynomial in x for all n ‚ààZ+.
Some plots of Chebyshev polynomials of the Ô¨Årst kind appear in Fig. 5.1. We
remark that on the interval x ‚àà[‚àí1, 1] the ‚Äúripples‚Äù of the polynomials are of
the same height. This fact makes this class of polynomials quite useful in certain
uniform approximation problems. It is one of the main properties that makes this
class of polynomials useful in Ô¨Ålter design. Some indication of how the Chebyshev
polynomials relate to uniform approximation problems appears in Section 5.7.
Example 5.1
This example is about how to program the recursion speciÔ¨Åed
in (5.57). DeÔ¨Åne
Tn(x) =
n

j=0
Tn,jxj
so that Tn,j is the coefÔ¨Åcient of xj. This is the same kind of notation as in (5.3)
for œÜn(x). In this setting it is quite important to note that Tn,j = 0 for j < 0, and
for j > n. From (5.57), we obtain
n+1

j=0
Tn+1,jxj = 2x
n

j=0
Tn,jxj ‚àí
n‚àí1

j=0
Tn‚àí1,jxj,
and so
n+1

j=0
Tn+1,jxj = 2
n

j=0
Tn,jxj+1 ‚àí
n‚àí1

j=0
Tn‚àí1,jxj.
TLFeBOOK

CHEBYSHEV POLYNOMIALS
223
‚àí1.5
‚àí1
‚àí0.5
0
0.5
1
1.5
‚àí5
‚àí4
‚àí3
‚àí2
‚àí1
0
1
2
3
4
5
x
Tn(x)
n = 2 
n = 3 
n = 4 
n = 5 
Figure 5.1
Chebyshev polynomials of the Ô¨Årst kind of degrees 2, 3, 4, 5.
Changing the variable in the second summation according to k = j + 1 (so j =
k ‚àí1) yields
n+1

k=0
Tn+1,kxk = 2
n+1

k=1
Tn,k‚àí1xk ‚àí
n‚àí1

k=0
Tn‚àí1,kxk,
and modifying the limits on the summations on the right-hand side yields
n+1

k=0
Tn+1,kxk = 2
n+1

k=0
Tn,k‚àí1xk ‚àí
n+1

k=0
Tn‚àí1,kxk.
We emphasize that this is permitted because we recall that Tn,‚àí1 = Tn‚àí1,n =
Tn‚àí1,n+1 = 0 for all n ‚â•1. Comparing like powers of x gives us the recursion
Tn+1,k = 2Tn,k‚àí1 ‚àíTn‚àí1,k
for k = 0, 1, . . . , n + 1 with n = 1, 2, 3, . . . . Since T0(x) = 1, we have T0,0 = 1,
and since T1(x) = x, we have T1,0 = 0, T1,1 = 1, which are the initial conditions
for the recursion. Therefore, a pseudocode program to compute Tn(x) is
T0,0 := 1;
T1,0 := 0; T1,1 := 1;
for n := 1 to N ‚àí1 do begin
for k := 0 to n + 1 do begin
TLFeBOOK

224
ORTHOGONAL POLYNOMIALS
Tn+1,k := 2Tn,k‚àí1 ‚àíTn‚àí1,k;
end;
end;
This computes T2(x), . . . , TN(x) (so we need N ‚â•2).
We remark that the recursion in Example 5.1 may be implemented using integer
arithmetic, and so there will be no rounding or quantization errors involved in
computing Tn(x). However, there is the risk of computing machine overÔ¨Çow.
Example 5.2
This example is about changing representations. SpeciÔ¨Åcally,
how might we express
pn(x) =
n

j=0
pn,jxj ‚ààPn[‚àí1, 1]
in terms of the Chebyshev polynomials of the Ô¨Årst kind? In other words, we wish
to determine the series coefÔ¨Åcients in
pn(x) =
n

j=0
Œ±jTj(x).
We will consider this problem only for n = 4.
Therefore, we begin by noting that (via (5.56) and (5.57)) T0(x) = 1, T1(x) = x,
T2(x) = 2x2 ‚àí1, T3(x) = 4x3 ‚àí3x, and T4(x) = 8x4 ‚àí8x2 + 1. Consequently
4

j=0
Œ±jTj(x) = Œ±0 + Œ±1x + 2Œ±2x2 ‚àíŒ±2
+ 4Œ±3x3 ‚àí3Œ±3x + 8Œ±4x4 ‚àí8Œ±4x2 + Œ±4
= (Œ±0 ‚àíŒ±2 + Œ±4) + (Œ±1 ‚àí3Œ±3)x + (2Œ±2 ‚àí8Œ±4)x2 + 4Œ±3x3 + 8Œ±4x4
implying that (on comparing like powers of x)
p4,0 = Œ±0 ‚àíŒ±2 + Œ±4, p4,1 = Œ±1 ‚àí3Œ±3,
p4,2 = 2Œ±2 ‚àí8Œ±4, p4,3 = 4Œ±3, p4,4 = 8Œ±4.
This may be more conveniently expressed in matrix form:
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£∞
1
0
‚àí1
0
1
0
1
0
‚àí3
0
0
0
2
0
‚àí8
0
0
0
4
0
0
0
0
0
8
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
#
$%
&
=U
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£∞
Œ±0
Œ±1
Œ±2
Œ±3
Œ±4
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
#
$%
&
=Œ±
=
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£∞
p4,0
p4,1
p4,2
p4,3
p4,4
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
#
$%
&
=p
.
TLFeBOOK

HERMITE POLYNOMIALS
225
The upper triangular system UŒ± = p is certainly easy to solve using the backward
substitution algorithm presented in Chapter 4 (see Section 4.5). We note that the
elements of matrix U are the coefÔ¨Åcients {Tn,j} as might be obtained from the
algorithm in Example 5.1.
Can you guess, on the basis of Example 5.2, what matrix U will be for any n ?
5.4
HERMITE POLYNOMIALS
Now let us consider D = R with weighting function
w(x) = e‚àíŒ±2x2.
(5.58)
This is essentially the Gaussian pulse from Chapter 3. Recalling (5.23), we have
œÜr(x) = eŒ±2x2 drGr(x)
dxr
,
(5.59)
where Gr(x) satisÔ¨Åes the differential equation
dr+1
dxr+1

eŒ±2x2 drGr(x)
dxr

= 0
(5.60)
[recall (5.24)]. From (5.25) Gr(x) and G(k)
r (x) (for k = 1, 2, . . . , r ‚àí1) must tend
to zero as x ‚Üí¬±‚àû. We may consider the trial solution
Gr(x) = Cre‚àíŒ±2x2.
(5.61)
The kth derivative of this is of the form
Cre‚àíŒ±2x2 √ó (polynomial of degree k),
so (5.61) satisÔ¨Åes both (5.60) and the required boundary conditions. Therefore
œÜr(x) = CreŒ±2x2 dr
dxr (e‚àíŒ±2x2).
(5.62)
It is common practice to deÔ¨Åne the Hermite polynomials to be œÜr(x) for Cr =
(‚àí1)r with either Œ±2 = 1 or Œ±2 = 1
2. We shall select Œ±2 = 1, and so our Hermite
polynomials are
Hr(x) = (‚àí1)rex2 dr
dxr (e‚àíx2).
(5.63)
By construction, for k Ã∏= r
 ‚àû
‚àí‚àû
e‚àíŒ±2x2œÜr(x)œÜk(x) dx = 0.
(5.64)
For the case where k = r, the following result is helpful.
TLFeBOOK

226
ORTHOGONAL POLYNOMIALS
It must be the case that
œÜr(x) =
r

k=0
œÜr,kxk
so that
||œÜr||2 =

D
w(x)œÜ2
r (x) dx =

D
w(x)œÜr(x)
 r

k=0
œÜr,kxk
	
dx,
but
"
D w(x)œÜr(x)xi dx = 0 for i = 0, 1, . . . , r ‚àí1 [special case of (5.19)], and so
||œÜr||2 = œÜr,r

D
w(x)œÜr(x)xr dx = œÜr,r

D
xrG(r)
r (x) dx
(5.65)
[via (5.20)]. We may integrate (5.65) by parts r times, and apply (5.25) to obtain
||œÜr||2 = (‚àí1)rr!œÜr,r

D
Gr(x) dx.
(5.66)
So, for our present problem with k = r, we obtain
||œÜr||2 =
 ‚àû
‚àí‚àû
e‚àíŒ±2x2œÜ2
r (x) dx = (‚àí1)rr!œÜr,r
 ‚àû
‚àí‚àû
Cre‚àíŒ±2x2 dx.
(5.67)
Now (if y = Œ±x with Œ± > 0)
 ‚àû
‚àí‚àû
e‚àíŒ±2x2 dx = 2
 ‚àû
0
e‚àíŒ±2x2 dx = 2
Œ±
 ‚àû
0
e‚àíy2 dy = 1
Œ±
‚àöœÄ
(5.68)
via (3.103) in Chapter 3. Consequently, (5.67) becomes
||œÜr||2 = (‚àí1)rr!œÜr,rCr
‚àöœÄ
Œ± .
(5.69)
With Cr = (‚àí1)r and Œ± = 1, we recall Hr(x) = œÜr(x), so (5.69) becomes
||Hr||2 = r!Hr,r
‚àöœÄ.
(5.70)
We need an expression for Hr,r.
We know that for suitable pn,k
dn
dxn e‚àíx2 =
 n

k=0
pn,kxk
	
#
$%
&
=pn(x)
e‚àíx2.
(5.71)
TLFeBOOK

HERMITE POLYNOMIALS
227
Now consider
dn+1
dxn+1 e‚àíx2 = d
dx

 n

k=0
pn,kxk
	
e‚àíx2

= ‚àí2x
 n

k=0
pn,kxk
	
e‚àíx2 +
 n

k=1
kpn,kxk‚àí1
	
e‚àíx2
=

‚àí2
n+1

k=0
pn,k‚àí1 +
n+1

k=0
(k + 1)pn,k+1
	
xke‚àíx2
=
n+1

k=0
[‚àí2pn,k‚àí1 + (k + 1)pn,k+1]
#
$%
&
=pn+1,k
xke‚àíx2,
so we have the recurrence relation
pn+1,k = ‚àí2pn,k‚àí1 + (k + 1)pn,k+1.
(5.72)
From (5.71) and (5.63) Hn(x) = (‚àí1)npn(x). From (5.72)
pn+1,n+1 = ‚àí2pn,n + (n + 2)pn,n+2 = ‚àí2pn,n,
and so
Hn+1,n+1 = (‚àí1)n+1pn+1,n+1 = ‚àí(‚àí1)n(‚àí2pn,n) = 2Hn,n.
(5.73)
Because H0(x) = 1 [via (5.63)], H0,0 = 1, and immediately we have Hn,n = 2n
[solution to the difference equation (5.73)]. Therefore
||Hr||2 = 2rr!‚àöœÄ,
(5.74)
thus,
" ‚àû
‚àí‚àûe‚àíx2H 2
r (x) dx = 2rr!‚àöœÄ.
A three-term recurrence relation for Hn(x) is needed. DeÔ¨Åne the generating
function
S(x, t) = exp[‚àít2 + 2xt] = exp[x2 ‚àí(t ‚àíx)2].
(5.75)
Observe that
‚àÇn
‚àÇtn S(x, t) = exp[x2] ‚àÇn
‚àÇtn exp[‚àí(t ‚àíx)2] = (‚àí1)nexp[x2] ‚àÇn
‚àÇxn exp[‚àí(t ‚àíx)2].
(5.76)
Because of the second equality in (5.76), we have
S(n)(x, 0) = ‚àÇn
‚àÇtn S(x, 0) = (‚àí1)nexp[x2] dn
dxn exp[‚àíx2] = Hn(x).
(5.77)
TLFeBOOK

228
ORTHOGONAL POLYNOMIALS
The Maclaurin expansion of S(x, t) about t = 0 is [recall (3.75)]
S(x, t) =
‚àû

n=0
S(n)(x, 0)
n!
tn,
so via (5.77), this becomes
S(x, t) =
‚àû

n=0
Hn(x)
n!
tn.
(5.78)
Now, from (5.75) and (5.78), we have
‚àÇS
‚àÇx = 2te‚àít2+2tx =
‚àû

n=0
2tn+1
n!
Hn(x),
(5.79a)
and also
‚àÇS
‚àÇx =
‚àû

n=0
tn
n!
‚àÇHn(x)
‚àÇx
=
‚àû

n=‚àí1
tn+1
(n + 1)!
dHn+1(x)
dx
.
(5.79b)
Comparing the like powers of t in (5.79) yields
1
(n + 1)!
dHn+1(x)
dx
= 2
n!Hn(x),
which implies that
dHn+1(x)
dx
= 2(n + 1)Hn(x)
(5.80)
for n ‚ààZ+. We may also consider
‚àÇS
‚àÇt = (‚àí2t + 2x)e‚àít2+2tx =
‚àû

n=0
(‚àí2t + 2x)
n!
tnHn(x)
(5.81a)
and
‚àÇS
‚àÇt =
‚àû

n=1
tn‚àí1
(n ‚àí1)!Hn(x) =
‚àû

n=0
tn
n!Hn+1(x).
(5.81b)
From (5.81a)
‚àû

n=0
(‚àí2t + 2x)
n!
tnHn(x) = ‚àí2
‚àû

n=0
tn
(n ‚àí1)!Hn‚àí1(x) + 2x
‚àû

n=0
tn
n!Hn(x).
(5.82)
TLFeBOOK

LEGENDRE POLYNOMIALS
229
‚àí2.5
‚àí2
‚àí1.5
‚àí1
‚àí0.5
0
0.5
1
1.5
2
2.5
‚àí150
‚àí100
‚àí50
0
50
100
150
x
Hn(x)
n = 2 
n = 3 
n = 4 
n = 5 
Figure 5.2
Hermite polynomials of degrees 2, 3, 4, 5.
Comparing like powers of t in (5.81b) and (5.82) yields
1
n!Hn+1(x) = ‚àí
2
(n ‚àí1)!Hn‚àí1(x) + 2x 1
n!Hn(x)
or
Hn+1(x) = 2xHn(x) ‚àí2nHn‚àí1(x).
(5.83)
This holds for all n ‚ààN. Equation (5.83) is another special case of (5.5).
The Hermite polynomials are relevant to quantum mechanics (quantum harmonic
oscillator problem), and they arise in signal processing as well (often in connection
with the uncertainty principle for signals). A few Hermite polynomials are plotted
in Fig. 5.2.
5.5
LEGENDRE POLYNOMIALS
We consider D = [‚àí1, 1] with uniform weighting function
w(x) = 1
for all
x ‚ààD.
(5.84)
TLFeBOOK

230
ORTHOGONAL POLYNOMIALS
In this case (5.24) becomes
dr+1
dxr+1
drGr(x)
dxr

= 0
or
d2r+1
dx2r+1 Gr(x) = 0.
(5.85)
The boundary conditions of (5.25) become
G(k)
r (¬±1) = 0
(5.86)
for k ‚ààZr. Consequently, the solution to (5.85) is given by
Gr(x) = Cr(x2 ‚àí1)r.
(5.87)
Thus, from (5.23)
œÜr(x) = Cr
dr
dxr (x2 ‚àí1)r.
(5.88)
The Legendre polynomials use Cr =
1
2rr!, and are denoted by
Pr(x) =
1
2rr!
dr
dxr (x2 ‚àí1)r,
(5.89)
which is the Rodrigues formula for Pr(x). By construction, for k Ã∏= r, we must
have
 1
‚àí1
Pr(x)Pk(x) dx = 0.
(5.90)
From (5.66) and (5.87), we obtain
||Pr||2 = (‚àí1)r
2r
Pr,r
 1
‚àí1
(x2 ‚àí1)r dx.
(5.91)
Recalling the binomial theorem
(a + b)r =
r

k=0
 r
k
!
akbr‚àík,
(5.92)
TLFeBOOK

LEGENDRE POLYNOMIALS
231
we see that
dr
dxr (x2 ‚àí1)r = dr
dxr
r

k=0
 r
k
!
(‚àí1)r‚àíkx2k
= dr
dxr

x2r +
r‚àí1

k=0
 r
k
!
(‚àí1)r‚àíkx2k
	
= 2r(2r ‚àí1) ¬∑ ¬∑ ¬∑ (r + 1)xr + dr
dxr
r‚àí1

k=0
 r
k
!
(‚àí1)r‚àíkx2k
	
,
implying that (recall (5.89))
Pr,r = 2r(2r ‚àí1) ¬∑ ¬∑ ¬∑ (r + 1)
2rr!
= (2r)!
2r[r!]2 .
(5.93)
But we need to evaluate the integral in (5.91) as well. With the change of variable
x = sin Œ∏, we have
Ir =
 1
‚àí1
(x2 ‚àí1)r dx = (‚àí1)r
 œÄ/2
‚àíœÄ/2
cos2r+1 Œ∏ dŒ∏.
(5.94)
We may integrate by parts
 œÄ/2
‚àíœÄ/2
cos2r+1 Œ∏ dŒ∏ =
 œÄ/2
‚àíœÄ/2
cos Œ∏ cos2r Œ∏ dŒ∏
= cos2r Œ∏ sin Œ∏|œÄ/2
‚àíœÄ/2 + 2r
 œÄ/2
‚àíœÄ/2
cos2r‚àí1 Œ∏ sin2 Œ∏ dŒ∏
(in
"
u dv = uv ‚àí
"
v du, we let u = cos2r Œ∏, and dv = cos Œ∏ dŒ∏), which becomes
(on using the identity sin2 Œ∏ = 1 ‚àícos2 Œ∏)
 œÄ/2
‚àíœÄ/2
cos2r+1 Œ∏ dŒ∏ = 2r
 œÄ/2
‚àíœÄ/2
cos2r‚àí1 Œ∏ dŒ∏ ‚àí2r
 œÄ/2
‚àíœÄ/2
cos2r+1 Œ∏ dŒ∏
for r ‚â•1. Therefore
(2r + 1)
 œÄ/2
‚àíœÄ/2
cos2r+1 Œ∏ dŒ∏ = 2r
 œÄ/2
‚àíœÄ/2
cos2r‚àí1 Œ∏ dŒ∏.
(5.95)
Now, since [via (5.94)]
Ir‚àí1 = ‚àí(‚àí1)r
 œÄ/2
‚àíœÄ/2
cos2r‚àí1 Œ∏ dŒ∏,
TLFeBOOK

232
ORTHOGONAL POLYNOMIALS
Eq. (5.95) becomes
(2r + 1)(‚àí1)rIr = ‚àí2r(‚àí1)rIr‚àí1,
or more simply
Ir = ‚àí
2r
2r + 1Ir‚àí1.
(5.96)
This holds for r ‚â•1 with initial condition I0 = 2. The solution to the difference
equation is
Ir = (‚àí1)r 22r+1[r!]2
(2r + 1)! .
(5.97)
This can be conÔ¨Årmed by direct substitution of (5.97) into (5.96). Consequently, if
we combine (5.91), (5.93), and (5.97), then
||Pr||2 = (‚àí1)2
2r
¬∑ (2r)!
2r[r!]2 ¬∑ (‚àí1)r22r+1[r!]2
(2r + 1)!
,
which simpliÔ¨Åes to
||Pr||2 =
2
2r + 1.
(5.98)
A closed-form expression for Pn(x) is possible using (5.92) in (5.89). SpeciÔ¨Å-
cally, consider
Pn(x) =
1
2nn!
dn
dxn (x2 ‚àí1)n =
1
2nn!
dn
dxn
 M

k=0
 n
k
!
(‚àí1)kx2n‚àí2k
	
,
(5.99)
where M = n/2 (n even), or M = (n ‚àí1)/2 (n odd). We observe that
dn
dxn x2n‚àí2k = (2n ‚àí2k)(2n ‚àí2k ‚àí1) ¬∑ ¬∑ ¬∑ (n ‚àí2k + 1)xn‚àí2k.
(5.100)
Now
(2n ‚àí2k)! = (2n ‚àí2k)(2n ‚àí2k ‚àí1) ¬∑ ¬∑ ¬∑ (n ‚àí2k + 1) (n ‚àí2k) ¬∑ ¬∑ ¬∑ 2 ¬∑ 1
#
$%
&
=(n‚àí2k)!
,
so (5.100) becomes
dn
dxn x2n‚àí2k = (2n ‚àí2k)!
(n ‚àí2k)! xn‚àí2k.
(5.101)
Thus, (5.99) reduces to
Pn(x) =
1
2nn!
M

k=0
 n
k
!
(‚àí1)k (2n ‚àí2k)!
(n ‚àí2k)! xn‚àí2k,
TLFeBOOK

LEGENDRE POLYNOMIALS
233
or alternatively
Pn(x) =
M

k=0
(‚àí1)k
(2n ‚àí2k)!
2nk!(n ‚àík)!(n ‚àí2k)!xn‚àí2k.
(5.102)
Consider f (x) =
1
‚àö
1‚àíx so that f (k)(x) = (2k‚àí1)(2k‚àí3)¬∑¬∑¬∑3¬∑1
2k
(1 ‚àíx)‚àí(2k+1)/2 (for
k ‚â•1). DeÔ¨Åne (2k ‚àí1)!! = (2k ‚àí1)(2k ‚àí3) ¬∑ ¬∑ ¬∑ 3 ¬∑ 1, and deÔ¨Åne (‚àí1)!! = 1. As
usual, deÔ¨Åne 0! = 1. We note that (2n)! = 2nn!(2n ‚àí1)!! which may be seen by
considering
(2n)! = (2n)(2n ‚àí1)(2n ‚àí2)(2n ‚àí3) ¬∑ ¬∑ ¬∑ 3 ¬∑ 2 ¬∑ 1
= [(2n)(2n ‚àí2) ¬∑ ¬∑ ¬∑ 4 ¬∑ 2][(2n ‚àí1)(2n ‚àí3) ¬∑ ¬∑ ¬∑ 3 ¬∑ 1] = 2nn!(2n ‚àí1)!!.
(5.103)
Consequently, the Maclaurin expansion for f (x) is given by
f (x) =
1
‚àö
1 ‚àíx
=
‚àû

k=0
f (k)(0)
k!
xk =
‚àû

k=0
(2k)!
22k[k!]2 xk.
(5.104)
The ratio test [7, p. 709] conÔ¨Årms that this series converges if |x| < 1. Using
(5.102) and (5.104), it is possible to show that
S(x, t) =
1
‚àö
1 ‚àí2xt + t2 =
‚àû

n=0
Pn(x)tn,
(5.105)
so this is the generating function for the Legendre polynomials Pn(x). We observe
that
‚àÇS
‚àÇt =
x ‚àít
[1 ‚àí2xt + t2]3/2 =
x ‚àít
1 ‚àí2xt + t2 S.
(5.106)
Also
‚àÇS
‚àÇt =
‚àû

n=0
nPn(x)tn‚àí1.
(5.107)
Equating (5.107) and (5.106), we have
x ‚àít
1 ‚àí2xt + t2
‚àû

n=0
Pn(x)tn =
‚àû

n=0
nPn(x)tn‚àí1,
which becomes
x
‚àû

n=0
Pn(x)tn ‚àí
‚àû

n=0
Pn(x)tn+1 =
‚àû

n=0
nPn(x)tn‚àí1 ‚àí2x
‚àû

n=0
nPn(x)tn
+
‚àû

n=0
nPn(x)tn+1,
TLFeBOOK

234
ORTHOGONAL POLYNOMIALS
and if P‚àí1(x) = 0, then this becomes
x
‚àû

n=0
Pn(x)tn ‚àí
‚àû

n=0
Pn‚àí1(x)tn =
‚àû

n=0
(n + 1)Pn+1(x)tn ‚àí2x
‚àû

n=0
nPn(x)tn
+
‚àû

n=0
(n ‚àí1)Pn‚àí1(x)tn,
so on comparing like powers of t in this expression, we obtain
xPn(x) ‚àíPn‚àí1(x) = (n + 1)Pn+1(x) ‚àí2xnPn(x) + (n ‚àí1)Pn‚àí1(x),
which Ô¨Ånally yields
(n + 1)Pn+1(x) = (2n + 1)xPn(x) ‚àínPn‚àí1(x)
or (for n ‚â•1)
Pn+1(x) = 2n + 1
n + 1 xPn(x) ‚àí
n
n + 1Pn‚àí1(x),
(5.108)
which is the three-term recurrence relation for the Legendre polynomials, and hence
is yet another special case of (5.5).
‚àí1.5
‚àí1
‚àí0.5
0
0.5
1
1.5
‚àí4
‚àí3
‚àí2
‚àí1
0
1
2
3
x
Pn(x)
n = 2
n = 3
n = 4
n = 5
Figure 5.3
Legendre polynomials of degrees 2, 3, 4, 5.
TLFeBOOK

ORTHOGONAL POLYNOMIAL LEAST-SQUARES APPROXIMATION
235
The Legendre polynomials arise in potential problems in electromagnetics. Mod-
eling the scattering of electromagnetic radiation by particles involves working with
these polynomials. Legendre polynomials appear in quantum mechanics as part
of the solution to Schr¬®odinger‚Äôs equation for the hydrogen atom. Some Legendre
polynomials appear in Fig. 5.3.
5.6
AN EXAMPLE OF ORTHOGONAL POLYNOMIAL
LEAST-SQUARES APPROXIMATION
Chebyshev polynomials of the Ô¨Årst kind, and Legendre polynomials are both orthog-
onal on [‚àí1, 1], but their weighting functions are different. We shall illustrate the
approximation behavior of these polynomials through an example wherein we wish
to approximate
f (x) =
 0,
‚àí1 ‚â§x < 0
1,
0 ‚â§x ‚â§1
(5.109)
by both of these polynomial types. We will work with Ô¨Åfth-degree least-squares
polynomial approximations in both cases.
We consider Legendre polynomial approximation Ô¨Årst. We must therefore nor-
malize the polynomials so that our basis functions have unit norm. Thus, our
approximation will be
ÀÜf (x) =
5

k=0
ÀÜakœÜk(x),
(5.110)
where
œÜk(x) =
1
||Pk||Pk(x),
and
ÀÜak = ‚ü®f, œÜk‚ü©=
 1
‚àí1
1
||Pk||f (x)Pk(x) dx,
for which we see that
ÀÜf (x) =
5

k=0
 1
‚àí1
f (x)Pk(x) dx
	
#
$%
&
=ÀÜŒ±k
Pk(x)
||Pk||2 .
(5.111)
We have [e.g., via (5.102)]
P0(x) = 1, P1(x) = x, P2(x) = 1
2[3x2 ‚àí1],
P3(x) = 1
2[5x3 ‚àí3x], P4(x) = 1
8[35x4 ‚àí30x2 + 3],
P5(x) = 1
8[63x5 ‚àí70x3 + 15x].
TLFeBOOK

236
ORTHOGONAL POLYNOMIALS
The squared norms [via (5.98)] are
||P0||2 = 2,
||P1||2 = 2
3,
||P2||2 = 2
5,
||P3||2 = 2
7,
||P4||2 = 2
9,
||P5||2 = 2
11.
By direct calculation, ÀÜŒ±k =
" 1
0 Pk(x) dx becomes
ÀÜŒ±0 =
 1
0
1 ¬∑ dx = 1, ÀÜŒ±1 =
 1
0
x dx = 1
2,
ÀÜŒ±2 = 1
2
 1
0
[3x2 ‚àí1] dx = 3
2
1
3x3
1
0
‚àí1
2 [x]1
0 = 0,
ÀÜŒ±3 = 1
2
 1
0
[5x3 ‚àí3x] dx = 5
2
1
4x4
1
0
‚àí3
2
1
2x2
1
0
= ‚àí1
8,
ÀÜŒ±4 = 1
8
 1
0
[35x4 ‚àí30x2 + 3] dx = 35
8
1
5x5
1
0
‚àí30
8
1
3x3
1
0
+ 3
8 [x]1
0 = 0,
ÀÜŒ±5 = 1
8
 1
0
[63x5 ‚àí70x3 + 15x] dx
= 63
8
1
6x6
1
0
‚àí70
8
1
4x4
1
0
+ 15
8
1
2x2
1
0
= 1
16.
The substitution of these (and the squared norms ||Pk||2) into (5.111) yields the
least-squares Legendre polynomial approximation
ÀÜf (x) = 1
2P0(x) + 3
4P1(x) ‚àí7
16P3(x) + 11
32P5(x).
(5.112)
We observe that ÀÜf (0) = 1
2, ÀÜf (1) = 37
32 = 1.15625, and ÀÜf (‚àí1) = ‚àí5
32 = ‚àí0.15625.
Now we consider the Chebyshev polynomial approximation. In this case
ÀÜf (x) =
5

k=0
ÀÜbkœÜk(x),
(5.113)
where
œÜk(x) =
1
||Tk||Tk(x),
and
ÀÜbk = ‚ü®f, œÜk‚ü©=
 1
‚àí1
1
‚àö
1 ‚àíx2
f (x)Tk(x)
||Tk||
dx,
TLFeBOOK

ORTHOGONAL POLYNOMIAL LEAST-SQUARES APPROXIMATION
237
from which we see that
ÀÜf (x) =
5

k=0
 1
‚àí1
1
‚àö
1 ‚àíx2 f (x)Tk(x) dx
	
#
$%
&
= ÀÜŒ≤k
1
||Tk||2 Tk(x).
(5.114)
We have [e.g., via (5.57)] the polynomials
T0(x) = 1,
T1(x) = x,
T2(x) = 2x2 ‚àí1,
T3(x) = 4x3 ‚àí3x,
T4(x) = 8x4 ‚àí8x2 + 1,
T5(x) = 16x5 ‚àí20x3 + 5x.
The squared norms [via (5.55)] are given by
||T0||2 = œÄ, ||Tk||2 = œÄ
2
(k ‚â•1).
By direct calculation
ÀÜŒ≤k =
" 1
0
1
‚àö
1‚àíx2 Tk(x) dx becomes (using x = cos Œ∏, and
Tk(cos Œ∏) = cos(kŒ∏) [recall (5.53)] ÀÜŒ≤k =
" œÄ/2
0
cos(kŒ∏) dŒ∏, and hence
ÀÜŒ≤0 =
 œÄ/2
0
1 ¬∑ dŒ∏ = œÄ
2 ,
ÀÜŒ≤1 =
 œÄ/2
0
cos Œ∏ dŒ∏ = [sin Œ∏]œÄ/2
0
= 1,
ÀÜŒ≤2 =
 œÄ/2
0
cos(2Œ∏) dŒ∏ =
1
2 sin(2Œ∏)
œÄ/2
0
= 0,
ÀÜŒ≤3 =
 œÄ/2
0
cos(3Œ∏) dŒ∏ =
1
3 sin(3Œ∏)
œÄ/2
0
= ‚àí1
3,
ÀÜŒ≤4 =
 œÄ/2
0
cos(4Œ∏) dŒ∏ =
1
4 sin(4Œ∏)
œÄ/2
0
= 0,
ÀÜŒ≤5 =
 œÄ/2
0
cos(5Œ∏) dŒ∏ =
1
5 sin(5Œ∏)
œÄ/2
0
= 1
5.
Substituting these (and the squared norms ||Tk||2) into (5.114) yields the least-
squares Chebyshev polynomial approximation
ÀÜf (x) = 1
œÄ
œÄ
2 T0(x) + 2T1(x) ‚àí2
3T3(x) + 2
5T5(x)

.
(5.115)
We observe that ÀÜf (0) = 1
2, ÀÜf (1) = 1.051737, and ÀÜf (‚àí1) = ‚àí0.051737.
TLFeBOOK

238
ORTHOGONAL POLYNOMIALS
‚àí1
‚àí0.8
‚àí0.6
‚àí0.4
‚àí0.2
0
0.2
0.4
0.6
0.8
1
‚àí0.2
0
0.2
0.4
0.6
0.8
1
1.2
x
 Amplitude 
f(x)
 Legendre approximation 
 Chebyshev approximation 
Figure 5.4
Plots of f (x) from (5.109), the Legendre approximation (5.112), and the
Chebyshev approximation (5.115). The least-squares approximations are Ô¨Åfth-degree poly-
nomials in both cases.
Plots of f (x) from (5.109) and the approximations in (5.112) and (5.115) appear
in Fig. 5.4. Both polynomial approximations are Ô¨Åfth-degree polynomials, and yet
they look fairly different. This is so because the weighting functions are different.
The Chebyshev approximation is better than the Legendre approximation near x =
¬±1 because greater weight is given to errors near the ends of the interval [‚àí1, 1].
5.7
UNIFORM APPROXIMATION
The subject of uniform approximation is distinctly more complicated than that of
least-squares approximation. So, we will not devote too much space to it in this
book. We will concentrate on a relatively simple illustration of the main idea.
Recall the space C[a, b] from Chapter 1. We recall that it is a normed space
with the norm
||x|| = sup
t‚àà[a,b]
|x(t)|
(5.116)
for any x(t) ‚ààC[a, b]. In fact, further recalling Chapter 3, this space happens to
be a complete metric space for which the metric induced by (5.116) is
d(x, y) = sup
t‚àà[a,b]
|x(t) ‚àíy(t)|,
(5.117)
TLFeBOOK

UNIFORM APPROXIMATION
239
where x(t), y(t) ‚ààC[a, b]. In uniform approximation we (for example) may wish
to approximate f (t) ‚ààC[a, b] with x(t) ‚ààPn[a, b] ‚äÇC[a, b] such that ||f ‚àíx||
is minimized. The norm (5.116) is sometimes called the Chebyshev norm, and so
our problem is sometimes called the Chebyshev approximation problem. The error
e(t) = f (t) ‚àíx(t) has the norm
||e|| = sup
t‚àà[a,b]
|f (t) ‚àíx(t)|
(5.118)
and is the maximum deviation between f (t) and x(t) on [a, b]. We wish to Ô¨Ånd x(t)
to minimize this. Consequently, uniform approximation is sometimes also called
minimax approximation as we wish to minimize the maximum deviation between
f (t) and its approximation x(t). We remark that because f (t) is continuous (by
deÔ¨Ånition) it will have a well-deÔ¨Åned maximum on [a, b] (although this maximum
need not be at a unique location). We are therefore at liberty to replace ‚Äúsup‚Äù by
‚Äúmax‚Äù in (5.116), (5.117) and (5.118) if we wish.
Suppose that yj(t) ‚ààC[a, b] for j = 0, 1, . . . , n ‚àí1 and the set of functions
{yj(t)|j ‚ààZn} are an independent set. This set generates an n-dimensional sub-
space4 of C[a, b] that may be denoted by Y =
9n‚àí1
j=0 Œ±jyj(t)|Œ±j ‚ààR
:
. From
Kreyszig [8, p. 337] consider
DeÔ¨Ånition 5.1: Haar Condition
Subspace Y ‚äÇC[a, b] satisÔ¨Åes the Haar
condition if every y ‚ààY (y Ã∏= 0) has at most n ‚àí1 zeros in [a, b], where n =
dim(Y) (dimension of the subspace Y).
We may select yj(t) = tj for j ‚ààZn so any y(t) ‚ààY has the form y(t) =
n‚àí1
j=0 Œ±jtj and thus is a polynomial of degree at most n ‚àí1. A degree n ‚àí1
polynomial has n ‚àí1 zeros, and so such a subspace Y satisÔ¨Åes the Haar condition.
DeÔ¨Ånition 5.2: Alternating Set
Let x ‚ààC[a, b], and y ‚ààY, where Y is any
subspace of C[a, b]. A set of points t0, . . . , tk in [a, b] such that t0 < t1 < ¬∑ ¬∑ ¬∑ < tk
is called an alternating set for x ‚àíy if x(tj) ‚àíy(tj) has alternately the values
+||x ‚àíy||, and ‚àí||x ‚àíy|| at consecutive points tj.
Thus, suppose x(tj) ‚àíy(tj) = +||x ‚àíy||, then x(tj+1) ‚àíy(tj+1) = ‚àí||x ‚àíy||,
but if instead x(tj) ‚àíy(tj) = ‚àí||x ‚àíy||, then x(tj+1) ‚àíy(tj+1) = +||x ‚àíy||. The
norm is, of course, that in (5.116).
Lemma 5.2: Best Approximation
Let Y be any subspace of C[a, b] that
satisÔ¨Åes the Haar condition. Given f ‚ààC[a, b], let y ‚ààY be such that for f ‚àíy
there exists an alternating set of n + 1 points, where n = dim(Y), then y is the
best uniform approximation of f out of Y.
The proof is omitted, but may be found in Kreyszig [8, pp. 345‚Äì346].
4A subspace of a vector space X is a nonempty subset Y of X such that for any y1, y2 ‚ààY, and all
scalars a, b from the Ô¨Åeld of the vector space we have ay1 + by2 ‚ààY.
TLFeBOOK

240
ORTHOGONAL POLYNOMIALS
Consider the particular case of C[‚àí1, 1] with f (t) ‚ààC[‚àí1, 1] such that for a
given n ‚ààN
f (t) = tn.
(5.119)
Now Y =
9n‚àí1
j=0 Œ±jtj|Œ±j ‚ààR
:
, so yj(t) = tj for j ‚ààZn. We wish to select Œ±j
such that the error e = f ‚àíy
e(t) = f (t) ‚àí
n‚àí1

j=0
Œ±jtj
(5.120)
is minimized with respect to the Chebyshev norm (5.116); that is, select Œ±j to
minimize ||e||. Clearly, dim(Y) = n. According to Lemma 5.2 ||e|| is minimized
if e(t) in (5.120) has an alternating set of n + 1 points.
Recall Lemma 5.1 (Section 5.3), which stated [see (5.44)] that
cos nŒ∏ =
n

k=0
Œ≤n,k cosk Œ∏.
(5.121)
From (5.46) Œ≤n+1,n+1 = 2Œ≤n,n, and Œ≤0,0 = 1. Consequently, Œ≤n,n = 2n‚àí1 (for n ‚â•
1). Thus, (5.121) can be rewritten as
cos nŒ∏ = 2n‚àí1 cosn Œ∏ +
n‚àí1

j=0
Œ≤n,j cosj Œ∏
(5.122)
(n ‚â•1). Suppose that t = cos Œ∏, so Œ∏ ‚àà[0, œÄ] maps to t ‚àà[‚àí1, 1] and from (5.122)
cos[n cos‚àí1 t] = 2n‚àí1tn +
n‚àí1

j=0
Œ≤n,jtj.
(5.123)
We observe that cos nŒ∏ has an alternating set of n + 1 points Œ∏0, Œ∏1, . . . , Œ∏n on [0, œÄ]
for which cos nŒ∏k = ¬±1 (clearly || cos nŒ∏|| = 1). For example, if n = 1, then
Œ∏0 = 0,
Œ∏1 = œÄ.
If n = 2, then
Œ∏0 = 0,
Œ∏1 = œÄ/2,
Œ∏2 = œÄ,
and if n = 3, then
Œ∏0 = 0,
Œ∏1 = œÄ
3 ,
Œ∏2 = 2œÄ
3 ,
Œ∏3 = œÄ.
In general, Œ∏k = k
nœÄ for k = 0, 1, . . . , n. Thus, if tk = cos Œ∏k then tk = cos
) k
nœÄ
*
.
We may rewrite (5.123) as
1
2n‚àí1 cos[n cos‚àí1 t] = tn +
1
2n‚àí1
n‚àí1

j=0
Œ≤n,jtj.
(5.124)
TLFeBOOK

PROBLEMS
241
This is identical to e(t) in (5.120) if we set Œ≤n,j = ‚àí2n‚àí1Œ±j. In other words, if we
choose
e(t) =
1
2n‚àí1 cos[n cos‚àí1 t],
(5.125)
then ||e|| is minimized since we know that e has an alternating set t = tk =
cos
) k
nœÄ
*
, k = 0, 1, . . . , n, and tk ‚àà[‚àí1, 1]. We recall from Section 5.3 that Tk(t) =
cos[k cos‚àí1 t] is the kth-degree Chebyshev polynomial of the Ô¨Årst kind [see (5.53)].
So, e(t) = Tn(t)/2n‚àí1. Knowing this, we may readily determine the optimal coef-
Ô¨Åcients Œ±j in (5.120) if we so desire.
Thus, the Chebyshev polynomials of the Ô¨Årst kind determine the best degree
n ‚àí1 polynomial uniform approximation to f (t) = tn on the interval t ‚àà[‚àí1, 1].
REFERENCES
1. E. Isaacson and H. B. Keller, Analysis of Numerical Methods, Wiley, New York, 1966.
2. F. B. Hildebrand, Introduction to Numerical Analysis, 2nd ed., McGraw-Hill, New York,
1974.
3. J. S. Lim and A. V. Oppenheim, eds., Advanced Topics in Signal Processing. Prentice-
Hall, Englewood Cliffs, NJ, 1988.
4. P. J. Davis and P. Rabinowitz, Numerical Integration, Blaisdell, Waltham, MA, 1967.
5. J. R. Rice, The Approximation of Functions, Vol. I: Linear Theory, Addison-Wesley,
Reading, MA, 1964.
6. G. Szeg¬®o, Orthogonal Polynomials, 3rd ed., American Mathematical Society, 1967.
7. L. Bers, Calculus: Preliminary Edition, Vol. 2, Holt, Rinehart, Winston, New York, 1967.
8. E. Kreyszig, Introductory Functional Analysis with Applications, Wiley, New York,
1978.
PROBLEMS
5.1. Suppose that {œÜk(x)|k ‚ààZN} are orthogonal polynomials on the interval D =
[a, b] ‚äÇR with respect to some weighting function w(x) ‚â•0. Show that
N‚àí1

k=0
akœÜk(x) = 0
holds for all x ‚àà[a, b] iff ak = 0 for all k ‚ààZN [i.e., prove that {œÜk(x)|k ‚àà
ZN} is an independent set].
5.2. Verify (5.11) by direct calculation for œÜk(x) = Tk(x)/||Tk|| with n = 2; that
is, Ô¨Ånd œÜ0(x), œÜ1(x), œÜ2(x), and œÜ3(x), and verify that the left- and right-hand
sides of (5.11) are equal to each other.
TLFeBOOK

242
ORTHOGONAL POLYNOMIALS
5.3. Suppose that {œÜn(x)|n ‚ààZ+} are orthogonal polynomials on the interval
[a, b] with respect to some weighting function w(x) ‚â•0. Prove the fol-
lowing theorem: The roots xj (j = 1, 2, . . . , n) of œÜn(x) = 0 (n ‚ààN) are all
real-valued, simple, and a < xj < b for all j.
5.4. Recall that Tj(x) is the degree j Chebyshev polynomial of the Ô¨Årst kind.
Recall from Lemma 5.1 that cos(nŒ∏) = n
k=0 Œ≤n,k cosk Œ∏. For x ‚àà[‚àí1, 1]
with k ‚ààZ+, we can Ô¨Ånd coefÔ¨Åcients aj such that
xk =
k

j=0
ajTj(x).
Prove that
aj =
1
||Tj||2
j

r=0
Œ≤j,r
 œÄ
0
cosk+r Œ∏ dŒ∏.
Is this the best way to Ô¨Ånd the coefÔ¨Åcients aj ? If not, specify an alternative
approach.
5.5. This problem is about the converse to Lemma 5.1. Suppose that we have
p(cos Œ∏) = 3
k=0 a3,k cosk Œ∏. We wish to Ô¨Ånd coefÔ¨Åcients b3,j such that
p(cos Œ∏) =
3

j=0
b3,j cos(jŒ∏).
Use Lemma 5.1 to show that a = Ub, where
a = [a3,0 a3,1 a3,2 a3,3]T , b = [b3,0 b3,1 b3,2 b3,3]T
and U is an upper triangular matrix containing the coefÔ¨Åcients Œ≤m,k for
m = 0, 1, 2, 3, and k = 0, 1 . . . , m. Of course, we may use back-substitution
to solve Ub = a for b if this were desired.
5.6. Suppose that for n ‚ààZ+ we are given p(cos Œ∏) = n
k=0 an,k cosk Œ∏. Show
how to Ô¨Ånd the coefÔ¨Åcients bn,j such that
p(cos Œ∏) =
n

j=0
bn,j cos(jŒ∏),
that is, generalize the previous problem from n = 3 to any n.
5.7. Recall Section 5.6, where
f (x) =
 0,
‚àí1 ‚â§x < 0
1,
0 ‚â§x ‚â§1
TLFeBOOK

PROBLEMS
243
was approximated by
ÀÜf (x) =
n

k=0
ÀÜbkœÜk(x), œÜk(x) =
1
||Tk||Tk(x)
for n = 5. Find a general expression for ÀÜbk for all k ‚ààZ+. Use the resulting
expansion
f (x) =
‚àû

k=0
ÀÜbk
||Tk||Tk(x)
to prove that
œÄ
4 =
‚àû

n=1
1
2n ‚àí1(‚àí1)n‚àí1.
5.8. Suppose that
f (x) =
 0,
‚àí1 ‚â§x < 0
x,
0 ‚â§x ‚â§1
,
then Ô¨Ånd ÀÜbk in
f (x) =
‚àû

k=0
ÀÜbk
1
||Tk||Tk(x).
5.9. Do the following:
(a) Solve the polynomial equation Tk(x) = 0 for all k > 0, and so Ô¨Ånd all
the zeros of the polynomials Tk(x).
(b) Show that Tn(x) satisÔ¨Åes the differential equation
(1 ‚àíx2)T (2)
n (x) ‚àíxT (1)
n (x) + n2Tn(x) = 0.
Recall that T (r)
n (x) = drTn(x)/dxr.
5.10. The three-term recurrence relation for the Chebyshev polynomials of the sec-
ond kind is
Ur+1(x) = 2xUr(x) ‚àíUr‚àí1(x),
(5.P.1)
where r ‚â•1, and where the initial conditions are
U0(x) = 1, U1(x) = 2x.
(5.P.2)
We remark that (5.P.1) is identical to (5.57). SpeciÔ¨Åcally, the recursion for
Chebyshev polynomials of both kinds is the same, except that the initial
TLFeBOOK

244
ORTHOGONAL POLYNOMIALS
conditions in (5.P.2) are not the same for both. [From (5.56) T0(x) = 1, but
T1(x) = x.] Since deg(Ur) = r, we have (for r ‚â•0)
Ur(x) =
r

j=0
Ur,jxj.
(5.P.3)
[Recall the notation for œÜr(x) in (5.3).] From the polynomial recursion in
(5.P.1) we may obtain
Ur+1,j = 2Ur,j‚àí1 ‚àíUr‚àí1,j.
(5.P.4a)
This expression holds for r ‚â•1, and for j = 0, 1, . . . , r, r + 1. From (5.P.2)
the initial conditions for (5.P.4a) are
U0,0 = 1, U1,0 = 0, U1,1 = 2.
(5.P.4b)
Write a MATLAB function that uses (5.P.4) to generate the Chebyshev poly-
nomials of the second kind for r = 0, 1, . . . , N. Test your program out for
N = 8. Program output should be in the form of a table that is written to a
Ô¨Åle. The tabular format should be something like
degree
0
1
2
3
4
5
6
7
8
0
1
0
0
0
0
0
0
0
0
1
0
2
0
0
0
0
0
0
0
2
-1
0
4
0
0
0
0
0
0
3
0
-4
0
8
0
0
0
0
0
etc.
5.11. The Chebyshev polynomials of the second kind use
D = [‚àí1, 1], w(x) =

1 ‚àíx2.
(5.P.5)
Denote these polynomials as the set {œÜn(x)|n ‚ààZ+}. (This notation applies
if the polynomials are normalized to possess unity-valued norms.) Of course,
in our function space, we use the inner product
‚ü®f, g‚ü©=
 1
‚àí1

1 ‚àíx2f (x)g(x) dx.
(5.P.6)
Derive the polynomials {œÜn(x)|n ‚ààZ+}. (Hint: The process is much the
same as the derivation of the Chebyshev polynomials of the Ô¨Årst kind pre-
sented in Section 5.3.) Therefore, begin by considering qr‚àí1(x) which is
any polynomial of degree not more than r ‚àí1. Thus, for suitable ck ‚ààR, we
TLFeBOOK

PROBLEMS
245
must have qr‚àí1(x) = r‚àí1
j=0 cjœÜj(x), and ‚ü®œÜr, qr‚àí1‚ü©= r‚àí1
j=0 cj‚ü®œÜr, œÜj‚ü©= 0
because ‚ü®œÜr, œÜj‚ü©= 0 for j = 0, 1, . . . , r ‚àí1. In expanded form
‚ü®œÜr, qr‚àí1‚ü©=
 1
‚àí1

1 ‚àíx2œÜr(x)qr‚àí1(x) dx = 0.
(5.P.7)
Use the change of variable x = cos Œ∏ (so dx = ‚àísin Œ∏ dŒ∏) to reduce (5.P.7) to
 œÄ
0
sin2 Œ∏ cos(kŒ∏)œÜr(cos Œ∏) dŒ∏ = 0
(5.P.8)
for k = 0, 1, 2, . . . , r ‚àí1, where Lemma 5.1 and its converse have also been
employed. Next consider the candidate
œÜr(cos Œ∏) = Cr
sin(r + 1)Œ∏
sin Œ∏
,
(5.P.9)
and verify that this satisÔ¨Åes (5.P.8). [Hence (5.P.9) satisÔ¨Åes (5.P.7).] Show
that (5.P.9) becomes
œÜr(x) = Cr
sin[(r + 1) cos‚àí1 x]
sin cos‚àí1 x
.
Prove that for Cr = 1 (all r ‚ààZ+)
œÜr+1(x) = 2xœÜr(x) ‚àíœÜr‚àí1(x).
In this case we normally use the notation Ur(x) = œÜr(x). Verify that U0(x) =
1, and that U1(x) = 2x. Prove that ||Un||2 = œÄ
2 for n ‚ààZ+. Finally, of
course, œÜn(x) = Un(x)/||Un||.
5.12. Write a MATLAB function to produce plots of Uk(x) for k = 2, 3, 4, 5 similar
to Fig. 5.1.
5.13. For œÜk(x) = Uk(x)/||Uk||, we have
œÜk(x) =
8
2
œÄ Uk(x).
Since ‚ü®œÜk, œÜj‚ü©= Œ¥k‚àíj, for any f (x) ‚ààL2[‚àí1, 1], we have the series
expansion
f (x) =
‚àû

k=0
ÀÜakœÜk(x),
where
ÀÜak = ‚ü®f, œÜk‚ü©=
1
||Uk||
 1
‚àí1

1 ‚àíx2f (x)Uk(x) dx.
TLFeBOOK

246
ORTHOGONAL POLYNOMIALS
Suppose that we work with the following function:
f (x) =
 0,
‚àí1 ‚â§x < 0
1,
0 ‚â§x ‚â§1
.
(a) Find a nice general formula for the elements of the sequence (ÀÜak)
(k ‚ààZ+).
(b) Use MATLAB to plot the approximation
ÀÜf2(x) =
5

k=0
ÀÜakœÜk(x)
on the same graph as that of f (x) (i.e., create a plot similar to that of
Fig. 5.4). Suppose that œàk(x) = Tk(x)/||Tk||; then another approxima-
tion to f (x) is given by
ÀÜf1(x) =
5

k=0
ÀÜbkœàk(x).
Plot ÀÜf1(x) on the same graph as ÀÜf2(x) and f (x).
(c) Compare the accuracy of the approximations ÀÜf1(x) and ÀÜf2(x) to f (x)
near the endpoints x = ¬±1. Which approximation is better near these
endpoints ? Explain why if you can.
5.14. Prove the following:
(a) Tn(x), and Tn‚àí1(x) have no zeros in common.
(b) Between any two neighboring zeros of Tn(x), there is precisely one zero
of Tn‚àí1(x). This is called the interleaving of zeros property.
(Comment: The interleaving of zeros property is possessed by all orthog-
onal polynomials. When this property is combined with ideas from later
chapters, it can be used to provide algorithms to Ô¨Ånd the zeros of orthogonal
polynomials in general.)
5.15. (a) Show that we can write
Tn(x) = cos[n cos‚àí1 x] = cosh[n cosh‚àí1 x].
[Hint: Note that cos x = 1
2(ejx + e‚àíjx), cosh x = 1
2(ex + e‚àíx), so that
cos x = cosh(jx).]
(b) Prove that T2n(x) = Tn(2x2 ‚àí1). [Hint: cos(2x) = 2 cos2 x ‚àí1.]
5.16. Use Eq. (5.63) in the following problems:
(a) Show that
d
dx Hr(x) = 2rHr‚àí1(x).
TLFeBOOK

PROBLEMS
247
(b) Show that
d
dx

e‚àíx2 d
dx Hr(x)

= ‚àí2re‚àíx2Hr(x).
(c) From the preceding conÔ¨Årm that Hr(x) satisÔ¨Åes the Hermite differential
equation
H (2)
r
(x) ‚àí2xH (1)
r
(x) + 2rHr(x) = 0.
5.17. Find Hk(x) for k = 0, 1, 2, 3, 4, 5 (i.e., Ô¨Ånd the Ô¨Årst six Hermite polynomials)
using (5.83).
5.18. Using (5.63) prove that
Hk(x) = n!
N

j=0
(‚àí1)j
2n‚àí2j
j!(n ‚àí2j)!xn‚àí2j,
where N = n/2 (n is even), N = (n ‚àí1)/2 (n is odd).
5.19. Suppose that Pk(x) is the Legendre polynomial of degree k. Recall Eq.
(5.90). Find constants Œ± and Œ≤ such that for Qk(x) = Pk(Œ±x + Œ≤) we have
(for k Ã∏= r)
 b
a
Qr(x)Qk(x) dx = 0.
[Comment: This linear transformation of variable allows us to least-squares
approximate f (x) using Legendre polynomial series on any interval [a, b].]
5.20. Recall Eq. (5.105):
1
‚àö
1 ‚àí2xt + t2 =
‚àû

n=0
Pn(x)tn.
Verify the terms n = 0, 1, 2, 3.
[Hint: Recall Eq. (3.82) (From Chapter 3).]
5.21. The distance between two points A and B in R3 is r, while the distance from
A to the origin O is r1, and the distance from B to the origin O is r2. The
angle between the vector ‚àí‚Üí
OA, and vector ‚àí‚Üí
OB is Œ∏.
(a) Show that
1
r =
1
7
r2
1 + r2
2 ‚àí2r1r2 cos Œ∏
.
[Hint: Recall the law of cosines (Section 4.6).]
TLFeBOOK

248
ORTHOGONAL POLYNOMIALS
(b) Show that
1
r = 1
r2
‚àû

n=0
Pn(cos Œ∏)
 r1
r2
!n
,
where, of course, Pn(x) is the Legendre polynomial of degree n.
(Comment: This result is important in electromagnetic potential theory.)
5.22. Recall Section 5.7. Write a MATLAB function to plot on the same graph
both f (t) = tn and e(t) [in (5.125)] for each of n = 2, 3, 4, 5. You must
generate four separate plots, one for each instance of n.
5.23. The set of points C = {ejŒ∏|Œ∏ ‚ààR, j =
‚àö
‚àí1} is the unit circle of the complex
plane. If R(ejŒ∏) ‚â•0 for all Œ∏ ‚ààR, then we may deÔ¨Åne an inner product on
a suitable space of functions that are deÔ¨Åned on C
‚ü®F, G‚ü©= 1
2œÄ
 œÄ
‚àíœÄ
R(ejŒ∏)F ‚àó(ejŒ∏)G(ejŒ∏) dŒ∏,
(5.P.10)
where, in general, F(ejŒ∏), G(ejŒ∏) ‚ààC. Since ejŒ∏ is 2œÄ-periodic in Œ∏, the
integration limits in (5.P.10) are ‚àíœÄ to œÄ, but another standard choice is
from 0 to 2œÄ. The function R(ejŒ∏) is a weighting function for the inner
product in (5.P.10). For R(ejŒ∏), there will be a real-valued sequence (rk)
such that
R(ejŒ∏) =
‚àû

k=‚àí‚àû
rke‚àíjkŒ∏,
and we also have r‚àík = rk [i.e., (rk) is a symmetric sequence]. For F(ejŒ∏)
and G(ejŒ∏), we have real-valued sequences (fk), and (gk) such that
F(ejŒ∏) =
‚àû

k=‚àí‚àû
fke‚àíjkŒ∏, G(ejŒ∏) =
‚àû

k=‚àí‚àû
gke‚àíjkŒ∏.
(a) Show that ‚ü®e‚àíjnŒ∏, e‚àíjmŒ∏‚ü©= rn‚àím.
(b) Show that ‚ü®e‚àíjnŒ∏F(ejŒ∏), e‚àíjnŒ∏G(ejŒ∏)‚ü©= ‚ü®F(ejŒ∏), G(ejŒ∏)‚ü©.
(c) Show that ‚ü®F(ejŒ∏), G(ejŒ∏)‚ü©= ‚ü®1, F(e‚àíjŒ∏)G(ejŒ∏)‚ü©.
[Comment: The unit circle C is of central importance in the theory of stability
of linear time-invariant (LTI) discrete-time systems, and so appears in the
subjects of digital control and digital signal processing.]
5.24. Recall Problem 4.21 from and the previous problem (5.23). Given an(z) =
n‚àí1
k=0 an,kz‚àík (and z = ejŒ∏), show that
‚ü®an(z), z‚àík‚ü©= œÉ 2
nŒ¥k
for k = 0, 1, . . . , n ‚àí1.
TLFeBOOK

PROBLEMS
249
[Comment: This result ultimately leads to an alternative derivation of the
Levinson‚ÄìDurbin algorithm, and suggests that this algorithm actually gen-
erates a sequence of orthogonal polynomials on the unit circle C. These
polynomials are in the indeterminate z‚àí1 (instead of x).]
5.25. It is possible to construct orthogonal polynomials on discrete sets. This prob-
lem is about a particular example of this. Suppose that
œÜr[n] =
r

j=0
œÜr,jnj,
where n ‚àà[‚àíL, U] ‚äÇZ, U ‚â•L ‚â•0 and œÜr,r Ã∏= 0, so that deg(œÜr) = r, and
let us also assume that œÜr,j ‚ààR for all r, and j. We say that {œÜr[n]|n ‚ààZ+}
is an orthogonal set if ‚ü®œÜk[n], œÜm[n]‚ü©= ||œÜk||2Œ¥k‚àím, where the inner product
is deÔ¨Åned by
‚ü®f [n], g[n]‚ü©=
U

n=‚àíL
w[n]f [n]g[n],
(5.P.11)
and w[n] ‚â•0 for all n ‚àà[‚àíL, U] is a weighting sequence for our inner
product space. (Of course, ||f ||2 = ‚ü®f [n], f [n]‚ü©.) Suppose that L = U = M;
then, for w[n] = 1 (all n ‚àà[‚àíM, M]), it can be shown (with much effort)
that the Gram polynomials are given by the three-term recurrence relation
pk+1[n] =
2(2k + 1)
(k + 1)(2M ‚àík)npk[n] ‚àí
k
k + 1
2M + k + 1
2M ‚àík
pk‚àí1[n],
(5.P.12)
where p0[n] = 1, and p1[n] = n/M.
(a) Use (5.P.12) to Ô¨Ånd pk[n] for k = 2, 3, 4, where M = 2.
(b) Use (5.P.12) to Ô¨Ånd pk[n] for k = 2, 3, 4, where M = 3.
(c) Use (5.P.11) to Ô¨Ånd ||pk||2 for k = 2, 3, 4, where M = 2.
(d) Use (5.P.11) to Ô¨Ånd ||pk||2 for k = 2, 3, 4, where M = 3.
(Comment: The uniform weighting function w[n] = 1 makes the Gram poly-
nomials the discrete version of the Legendre polynomials. The Gram poly-
nomials were actually invented by Chebyshev.)
5.26. Integration by parts is clearly quite important in analysis (e.g., recall Section
3.6). To derive the Gram polynomials (previous problem) makes use of sum-
mation by parts.
Suppose that v[n], u[n], and f [n] are deÔ¨Åned on Z. DeÔ¨Åne the forward
difference operator  according to
f [n] = f [n + 1] ‚àíf [n]
TLFeBOOK

250
ORTHOGONAL POLYNOMIALS
[for any sequence (f [n])]. Prove the expression for summation by parts,
which is
U

n=‚àíL
u[n]v[n] = u[n]v[n]
U+1
‚àíL
‚àí
U

n=‚àíL
v[n + 1]u[n].
[Hint: Show that
u[n]v[n] = u[n]v[n] ‚àív[n + 1]u[n],
and then consider using the identity
U

n=‚àíL
f [n] =
U

n=‚àíL
(f [n + 1] ‚àíf [n]) = f [n]
U+1
‚àíL
.
Of course, f [n]
B
A = f [B] ‚àíf [A].]
TLFeBOOK

6
Interpolation
6.1
INTRODUCTION
Suppose that we have the data {(tk, x(tk))|k ‚ààZn+1}, perhaps obtained experimen-
tally. An example of this appeared in Section 4.6 . In this case we assumed that
tk = kTs for which x(tk) = x(t)|t=kTs are the samples of some analog signal. In
this example these time samples were of simulated (and highly oversimpliÔ¨Åed)
physiological data for human patients (e.g., blood pressure, heart rate, body core
temperature). Our problem involved assuming a model for the data; thus, assum-
ing x(t) is explained by a particular mathematical function with certain unknown
parameters to be estimated on the basis of the model and the data. In other words,
we estimate x(t) with ÀÜx(t, Œ±), where Œ± is the vector of unknown parameters (the
model parameters to be estimated), and we chose Œ± to minimize the error
e(tk) = x(tk) ‚àíÀÜx(tk, Œ±),
k ‚ààZn+1
(6.1)
according to some criterion. We have emphasized choosing Œ± to minimize
V (Œ±) =
n

k=0
e2(tk).
(6.2)
This was the least-squares approach. However, the idea of choosing Œ± to mini-
mize maxk‚ààZn+1 |e(tk)| is an alternative suggested by Section 5.7. Other choices
are possible. However, no matter what choice we make, in all cases ÀÜx(tk, Œ±) is
not necessarily exactly equal to x(tk) except perhaps by chance. The problem of
Ô¨Ånding ÀÜx(t, Œ±) to minimize e(t) in this manner is often called curve Ô¨Åtting. It is to
be distinguished from interpolation, which may be deÔ¨Åned as follows.
Usually we assume t0 < t1 < ¬∑ ¬∑ ¬∑ < tn‚àí1 < tn with t0 = a, and tn = b so that
tk ‚àà[a, b] ‚äÇR. To interpolate the data {(tk, x(tk))|k ‚ààZn+1}, we seek a function
p(t) such that t ‚àà[a, b], and
p(tk) = x(tk)
(6.3)
for all k ‚ààZn+1. We might know something about the properties of x(t) for t Ã∏=
tk on interval [a, b], and so we might select p(t) to possess similar properties.
An Introduction to Numerical Analysis for Electrical and Computer Engineers, by C.J. Zarowski
ISBN 0-471-46737-5
c‚Éù2004 John Wiley & Sons, Inc.
251
TLFeBOOK

252
INTERPOLATION
However, we emphasize that the interpolating function p(t) exactly matches x(t)
at the given sample points tk, k ‚ààZn+1.
Curve Ô¨Åtting is used when the data are uncertain because of the corrupting effects
of measurement errors, random noise, or interference. Interpolation is appropriate
when the data are accurately or exactly known.
Interpolation is quite important in digital signal processing. For example, ban-
dlimited signals may need to be interpolated in order to change sampling rates.
Interpolation is vital in numerical integration methods, as will be seen later. In
this application the integrand is typically known or can be readily found at some
Ô¨Ånite set of points with signiÔ¨Åcant accuracy. Interpolation at these points leads to
a function (usually a polynomial) that can be easily integrated, and so provides a
useful approximation to the given integral.
This chapter discusses interpolation with polynomials only. In principle it is
possible to interpolate using other functions (rational functions, trigonometric func-
tions, etc.) But these other approaches are usually more involved, and so will not
be considered in this book.
6.2
LAGRANGE INTERPOLATION
This chapter presents polynomial interpolation in three different forms. The Ô¨Årst
form, which might be called the direct form, involves obtaining the interpolat-
ing polynomial by the direct solution of a particular linear system of equations
(Vandermonde system). The second form is an alternative called Lagrange inter-
polation and is considered in this section along with the direct form. The third
form is called Newton interpolation, and is considered in the next section. All
three approaches give the same polynomial but expressed in different mathemati-
cal forms each possessing particular advantages and disadvantages. No one form
is useful in all applications, and this is why we must consider them all.
As in Section 6.1, we consider the data set {(tk, x(tk))|k ‚ààZn+1}, and we will
let xk = x(tk). We wish, as already noted, that our interpolating function be a
polynomial of degree n:
pn(t) =
n

j=0
pn,jtj.
(6.4)
For example, if n = 1 (linear interpolation), then
xk =
1

j=0
p1,jtj
k ,
xk+1 =
1

j=0
p1,jtj
k+1,
TLFeBOOK

LAGRANGE INTERPOLATION
253
or
p1,0 + p1,1tk = xk,
p1,0 + p1,1tk+1 = xk+1,
or in matrix form this becomes
 1
tk
1
tk+1
  p1,0
p1,1

=

xk
xk+1

.
This has a unique solution, provided tk Ã∏= tk+1 because in this instance the matrix
has determinant det
  1
tk
1
tk+1
!
= tk+1 ‚àítk. Polynomial p1(t) = p1,0 + p1,1t
linearly interpolates the points (tk, xk), and (tk+1, xk+1). As another example, if
n = 2 (quadratic interpolation), then we have
xk =
2

j=0
p2,jtj
k ,
xk+1 =
2

j=0
p2,jtj
k+1,
xk+2 =
2

j=0
p2,jtj
k+2,
or in matrix form
Ô£Æ
Ô£∞
1
tk
t2
k
1
tk+1
t2
k+1
1
tk+2
t2
k+2
Ô£π
Ô£ª
Ô£Æ
Ô£∞
p2,0
p2,1
p2,2
Ô£π
Ô£ª=
Ô£Æ
Ô£∞
xk
xk+1
xk+2
Ô£π
Ô£ª.
(6.5)
The matrix in (6.5) has the determinant (tk ‚àítk+1)(tk+1 ‚àítk+2)(tk+2 ‚àítk), which
will not be zero if tk, tk+1 and tk+2 are all distinct. The polynomial p2(t) =
p2,0 + p2,1t + p2,2t2 interpolates the points (tk, xk), (tk+1, xk+1), and (tk+2, xk+2).
In general, for arbitrary n we have the linear system
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
1
tk
¬∑ ¬∑ ¬∑
tn‚àí1
k
tn
k
1
tk+1
¬∑ ¬∑ ¬∑
tn‚àí1
k+1
tn
k+1
...
...
...
...
1
tk+n‚àí1
¬∑ ¬∑ ¬∑
tn‚àí1
k+n‚àí1
tn
k+n‚àí1
1
tk+n
¬∑ ¬∑ ¬∑
tn‚àí1
k+n
tn
k+n
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
#
$%
&
=A
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
pn,0
pn,1
...
pn,n‚àí1
pn,n
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
#
$%
&
=p
=
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
xk
xk+1
...
xk+n‚àí1
xk+n
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
#
$%
&
=x
.
(6.6)
TLFeBOOK

254
INTERPOLATION
Matrix A is called a Vandermonde matrix, and the linear system (6.6) is a Vander-
monde linear system of equations. The solution to (6.6) (if it exists) gives the direct
form of the interpolating polynomial stated in (6.4). For convenience we will let
k = 0. If we let t = t0, then
A = A(t) =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
1
t
¬∑ ¬∑ ¬∑
tn‚àí1
tn
1
t1
¬∑ ¬∑ ¬∑
tn‚àí1
1
tn
1
...
...
...
...
1
tn‚àí1
¬∑ ¬∑ ¬∑
tn‚àí1
n‚àí1
tn
n‚àí1
1
tn
¬∑ ¬∑ ¬∑
tn‚àí1
n
tn
n
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
(6.7)
Let D(t) = det(A(t)), and we see that D(t) is a polynomial in the indetermi-
nate t of degree n. Therefore, D(t) = 0 is an equation with exactly n roots (via
the fundamental theorem of algebra). But D(tk) = 0 for k = 1, . . . , n since the
rows of A(t) are dependent for any t = tk. So t1, t2, . . . , tn are the only possible
roots of D(t) = 0. Therefore, if t0, t1, . . . , tn are all distinct, then we must have
det(A(t0)) Ã∏= 0. Hence A in (6.6) will always possess an inverse if tk+i Ã∏= tk+j for
i Ã∏= j.
For small values of n (e.g., n = 1, or n = 2), the direct solution of (6.6) can be a
useful method of polynomial interpolation. However, it is known that Vandermonde
matrices can be very ill-conditioned even for relatively small n (e.g., n ‚â•10 or so).
This is particularly likely to happen for the common case of equispaced data, i.e.,
the case where tk = t0 + hk for k = 0, 1, . . . , n with h > 0, as mentioned in Hill [1,
p. 233]. Thus, we much prefer to avoid interpolating by the direct solution of (6.6)
when n ‚â´2. Also, direct solution of (6.6) is computationally inefÔ¨Åcient, unless one
contemplates the use of fast algorithms for Vandermonde system solution in Golub
and Van Loan [2]. These are signiÔ¨Åcantly faster than Gaussian elimination as they
possess asymptotic time complexities of only O(n2) versus the O(n3) complexity
of Gaussian elimination approaches.
We remark that so far we have proved the existence of a polynomial of degree n
that interpolates the data {(tk, xk)|k ‚ààZn+1}, provided tk Ã∏= tj (j Ã∏= k). The poly-
nomial also happens to be unique, a fact readily apparent from the uniqueness of
the solution to (6.6) assuming the existence condition is met. So, if we can Ô¨Ånd
(by any method) any polynomial of degree ‚â§n that interpolates the given data,
then this is the only possible interpolating polynomial for the data.
Since we disdain the idea of solving (6.6) directly we seek alternative methods
to obtain pn(t). In this regard better approach to polynomial interpolation is an
often Lagrange interpolation, which works as follows.
Again assume that we wish to interpolate the data set {(tk, xk)|k ‚ààZn+1}. Sup-
pose that we possess polynomials (called Lagrange polynomials) Lj(t) with the
property
Lj(tk) =
 0,
j Ã∏= k
1,
j = k
= Œ¥j‚àík.
(6.8)
TLFeBOOK

LAGRANGE INTERPOLATION
255
Then the interpolating polynomial for the data set is
pn(t) = x0L0(t) + x1L1(t) + ¬∑ ¬∑ ¬∑ + xnLn(t) =
n

j=0
xjLj(t).
(6.9)
We observe that
pn(tk) =
n

j=0
xjLj(tk) =
n

j=0
xjŒ¥j‚àík = xk
(6.10)
for k ‚ààZn+1, so pn(t) in (6.9) does indeed interpolate the data set, and via unique-
ness, if we were to write pn(t) in the direct form pn(t) = n
j=0 pn,jtj, then the
polynomial coefÔ¨Åcients pn,j would satisfy (6.6). We may see that for j ‚ààZn+1 the
Lagrange polynomials are given by
Lj(t) =
n
/
i=0
iÃ∏=j
t ‚àíti
tj ‚àíti
.
(6.11)
Equation (6.9) is called the Lagrange form of the interpolating polynomial.
Example 6.1
Consider data set {(t0, x0), (t1, x1), (t2, x2)} so n = 2. Therefore
L0(t) = (t ‚àít1)(t ‚àít2)
(t0 ‚àít1)(t0 ‚àít2),
L1(t) = (t ‚àít0)(t ‚àít2)
(t1 ‚àít0)(t1 ‚àít2),
L2(t) = (t ‚àít0)(t ‚àít1)
(t2 ‚àít0)(t2 ‚àít1).
It is not difÔ¨Åcult to see that p2(t) = x0L0(t) + x1L1(t) + x2L2(t). For example,
p2(t0) = x0L0(t0) + x1L1(t0) + x2L2(t0) = x0.
Suppose that the data set has the speciÔ¨Åc values {(0, 1), (1, 2), (2, 3)}, and so
(6.6) becomes
Ô£Æ
Ô£∞
1
0
0
1
1
1
1
2
4
Ô£π
Ô£ª
Ô£Æ
Ô£∞
p2,0
p2,1
p2,2
Ô£π
Ô£ª=
Ô£Æ
Ô£∞
1
2
3
Ô£π
Ô£ª.
This has solution p2(t) = t + 1 (i.e., p2,2 = 0). We see that the interpolating poly-
nomial pn(t) need not have degree exactly equal to n, but can be of lower degree.
We also see that
L0(t) = (t ‚àí1)(t ‚àí2)
(0 ‚àí1)(0 ‚àí2) = 1
2(t ‚àí1)(t ‚àí2) = 1
2(t2 ‚àí3t + 2),
TLFeBOOK

256
INTERPOLATION
L1(t) = (t ‚àí0)(t ‚àí2)
(1 ‚àí0)(1 ‚àí2) = ‚àít(t ‚àí2) = ‚àí(t2 ‚àí2t),
L2(t) = (t ‚àí0)(t ‚àí1)
(2 ‚àí0)(2 ‚àí1) = 1
2t(t ‚àí1) = 1
2(t2 ‚àít).
Observe that
x0L0(t) + x1L1(t) + x2L2(t)
= 1 ¬∑ 1
2(t2 ‚àí3t + 2) + 2 ¬∑ (‚àí1)(t2 ‚àí2t) + 3 ¬∑ 1
2(t2 ‚àít)
=
 1
2t2 ‚àí2t + 3
2t2
!
+
 
‚àí3
2t + 4t ‚àí3
2t
!
+
 1
2 ¬∑ 2
!
= t + 1,
which is p2(t). As expected, the Lagrange form of the interpolating polynomial,
and the solution to the Vandermonde system are the same polynomial.
We remark that once pn(t) is found, we often wish to evaluate pn(t) for t Ã∏= tk
(k ‚ààZn+1). Suppose that pn(t) is known in direct form; then this should be done
using Horner‚Äôs rule:
pn(t) = pn,0 + t[pn,1 + t[pn,2 + t[pn,3 + ¬∑ ¬∑ ¬∑ + t[pn,n‚àí1 + tpn,n]] ¬∑ ¬∑ ¬∑]]]. (6.12)
For example, if n = 3, then
p3(t) = p3,0 + t[p3,1 + t[p3,2 + tp3,3]].
(6.13)
To evaluate this requires only 6 Ô¨Çops (3 Ô¨Çoating-point multiplications, and 3 Ô¨Çoating-
point additions). Evaluating p3(t) = p3,0 + p3,1t + p3,2t2 + p3,3t3 directly needs 9
Ô¨Çops (6 Ô¨Çoating-point multiplications and 3 Ô¨Çoating-point additions). Thus, Horner‚Äôs
rule is more efÔ¨Åcient from a computational standpoint. Using Horner‚Äôs rule may be
described as evaluating the polynomial from the inside out.
While Lagrange interpolation represents a simple way to solve (6.6) the form
of the solution is (6.9), and so requires more effort to evaluate at t Ã∏= tk than the
direct form, the latter of which is easily evaluated using Horner‚Äôs rule as noted in
the previous paragraph. Another objection to Lagrange interpolation is that if we
were to add elements to our data set, then the calculations for the current data set
would have to be discarded, and this would force us to begin again. It is possible to
overcome this inefÔ¨Åciency using Newton‚Äôs divided-difference method which leads
to the Newton form of the interpolating polynomial. This may be seen in Hildebrand
[3, Chapter 2]. We will consider this methodology in the next section.
When we evaluate pn(t) for t Ã∏= tk, but with t ‚àà[a, b] = [t0, tn], then this is
interpolation. If we wish to evaluate pn(t) for t < a or t > b, then this is referred
to as extrapolation. To do this is highly risky. Indeed, even if we constrain t to
satisfy t ‚àà[a, b], the results can be poor. We illustrate with a famous example of
TLFeBOOK

NEWTON INTERPOLATION
257
Runge‚Äôs which is described in Forsythe et al. [4, pp. 69‚Äì70]. It is very possible
that for f (t) ‚ààC[a, b], we have
lim
n‚Üí‚àûsup
t‚àà[a,b]
|f (t) ‚àípn(t)| = ‚àû.
Runge‚Äôs speciÔ¨Åc example is for t ‚àà[‚àí5, 5] with
f (t) =
1
1 + t2 ,
where he showed that for any t satisfying 3.64 ‚â§|t| < 5, then
lim
n‚Üí‚àûsup |f (t) ‚àípn(t)| = ‚àû.
This divergence with respect to the Chebyshev norm [recall Section 5.7 (of
Chapter 5) for the deÔ¨Ånition of this term] is called Runge‚Äôs phenomenon. A
fairly detailed account of Runge‚Äôs example appears in Isaacson and Keller [5,
pp. 275‚Äì279].
We close this section with mention of the approximation error f (t) ‚àípn(t).
Suppose that f (t) ‚ààCn+1[a, b], and it is understood that f (n+1)(t) is continuous
as well; that is, f (t) has continuous derivatives f (k)(t) for k = 1, 2, . . . , n + 1. It
can be shown that for suitable Œæ = Œæ(t)
en(t) = f (t) ‚àípn(t) =
1
(n + 1)!f (n+1)(Œæ)
n
/
i=0
(t ‚àíti),
(6.14)
where Œæ ‚àà[a, b]. If we know f (n+1)(t), then clearly (6.14) may yield useful bounds
on |en(t)|. Of course, if we know nothing about f (t), then (6.14) is useless.
Equation (6.14) is useless if f (t) is not sufÔ¨Åciently differentiable. A derivation
of (6.14) is given by Hildebrand [3, pp. 81‚Äì83], but we omit it here. However, it
will follow from results to be considered in the next section.
6.3
NEWTON INTERPOLATION
DeÔ¨Åne
x[t0, t1] = x(t1) ‚àíx(t0)
t1 ‚àít0
= x1 ‚àíx0
t1 ‚àít0
.
(6.15)
This is called the Ô¨Årst divided difference of x(t) relative to t1 and t0. We see that
x[t0, t1] = x[t1, t0]. We may linearly interpolate x(t) for t ‚àà[t0, t1] according to
x(t) ‚âàx(t0) + t ‚àít0
t1 ‚àít0
[x(t1) ‚àíx(t0)] = x(t0) + (t ‚àít0)x[t0, t1].
(6.16)
TLFeBOOK

258
INTERPOLATION
It is convenient to deÔ¨Åne p0(t) = x(t0) and p1(t) = x(t0) + (t ‚àít0)x[t0, t1]. This
is notation consistent with Section 6.2. In fact, p1(t) agrees with the solution to
 1
t0
1
t1
  p1,0
p1,1

=
 x0
x1

,
(6.17)
as we expect.
Unless x(t) is truly linear the secant slope x[t0, t1] will depend on the abscissas
t0 and t1. If x(t) is a second-degree polynomial, then x[t1, t] will itself be a linear
function of t for a given t1. Consequently, the ratio
x[t0, t1, t2] = x[t1, t2] ‚àíx[t0, t1]
t2 ‚àít0
(6.18)
will be independent of t0, t1, and t2. [This ratio is the second divided difference of
x(t) with respect to t0, t1, and t2.] To see that this claim is true consider x(t) =
a0 + a1t + a2t2, so then
x[t1, t] = a1 + a2(t + t1).
(6.19)
Therefore
x[t1, t2] ‚àíx[t0, t1] = x[t1, t2] ‚àíx[t1, t0] = a2(t2 ‚àít0)
so that x[t0, t1, t2] = a2. We also note from (6.18) that
x[t0, t1, t2] =
1
t2 ‚àít0
x2 ‚àíx1
t2 ‚àít1
‚àíx1 ‚àíx0
t1 ‚àít0

,
(6.20a)
x[t1, t0, t2] =
1
t2 ‚àít1
x2 ‚àíx0
t2 ‚àít0
‚àíx0 ‚àíx1
t0 ‚àít1

,
(6.20b)
for which we may rewrite these in symmetric form
x[t0, t1, t2] =
x0
(t0 ‚àít1)(t0 ‚àít2) +
x1
(t1 ‚àít0)(t1 ‚àít2) +
x2
(t2 ‚àít0)(t2 ‚àít1)
= x[t1, t0, t2],
(6.21)
that is, x[t0, t1, t2] = x[t1, t0, t2]. We see from (6.16) that
x(t) ‚àíx(t0)
t ‚àít0
‚âàx[t0, t1],
that is
x[t0, t] ‚âàx[t0, t1],
(6.22)
TLFeBOOK

NEWTON INTERPOLATION
259
and from this we consider the difference
x[t0, t] ‚àíx[t0, t1] = x[t0, t] ‚àíx[t1, t0] = (t ‚àít1)x[t0, t1, t]
(6.23)
via (6.18), and the symmetry property x[t0, t1, t] = x[t1, t0, t]. Since x(t) is assumed
to be quadratic, we may replace the approximation of (6.16) with the identity
x(t) = x(t0) + (t ‚àít0)x[t0, t]
= x(t0) + (t ‚àít0)x[t0, t1]
#
$%
&
=p1(t)
+(t ‚àít0)(t ‚àít1)x[t0, t1, t]
(6.24)
via (6.23). The Ô¨Årst equality of (6.24) may be veriÔ¨Åed by direct calculation using
x(t) = a0 + a1t + a2t2, and (6.19). We see that if p1(t) approximates x(t), then
the error involved is [from (6.24)]
e(t) = x(t) ‚àíp1(t) = (t ‚àít0)(t ‚àít1)x[t0, t1, t].
(6.25)
These results generalize to x(t) a polynomial of higher degree.
We may recursively deÔ¨Åne the divided differences of orders 0, 1, . . . , k ‚àí1, k
according to
x[t0] = x(t0) = x0,
x[t0, t1] = x(t1) ‚àíx(t0)
t1 ‚àít0
= x1 ‚àíx0
t1 ‚àít0
,
x[t0, t1, t2] = x[t1, t2] ‚àíx[t0, t1]
t2 ‚àít0
,
...
x[t0, . . . , tk] = x[t1, . . . , tk] ‚àíx[t0, . . . , tk‚àí1]
tk ‚àít0
.
(6.26)
We have established the symmetry x[t0, t1] = x[t1, t0] (case k = 1), and also
x[t0, t1, t2] = x[t1, t0, t2] (case k = 2). For k = 2, symmetry of this kind can also
be deduced from the symmetric form (6.21). It seems reasonable [from (6.21)] that
in general
x[t0, . . . , tk]
=
x0
(t0 ‚àít1) ¬∑ ¬∑ ¬∑ (t0 ‚àítk) +
x1
(t1 ‚àít0) ¬∑ ¬∑ ¬∑ (t1 ‚àítk) +¬∑ ¬∑ ¬∑+
xk
(tk ‚àít0) ¬∑ ¬∑ ¬∑ (tk ‚àítk‚àí1)
=
k

j=0
1
0k
i=0
iÃ∏=j
(tj ‚àíti)
xj.
(6.27)
TLFeBOOK

260
INTERPOLATION
It is convenient to deÔ¨Åne the coefÔ¨Åcient of xj as
Œ±k
j =
1
0k
i=0
iÃ∏=j
(tj ‚àíti)
(6.28)
for j = 0, 1, . . . , k. Thus, x[t0, . . . , tk] = k
j=0 Œ±k
jxj. We may prove (6.27) for-
mally by mathematical induction. We outline the detailed approach as follows.
Suppose that it is true for k = r; that is, assume that
x[t0, . . . , tr] =
r

j=0
1
0r
i=0
iÃ∏=j
(tj ‚àíti)xj,
(6.29)
and consider [from deÔ¨Ånition (6.26)]
x[t0, . . . , tr+1] =
1
tr+1 ‚àít0
{x[t1, . . . , tr+1] ‚àíx[t0, . . . , tr]}.
(6.30)
For (6.30)
x[t1, . . . , tr+1] =
x1
(t1 ‚àít2) ¬∑ ¬∑ ¬∑ (t1 ‚àítr+1)
+
x2
(t2 ‚àít1) ¬∑ ¬∑ ¬∑ (t2 ‚àítr+1) + ¬∑ ¬∑ ¬∑ +
xr+1
(tr+1 ‚àít1) ¬∑ ¬∑ ¬∑ (tr+1 ‚àítr),
(6.31a)
and
x[t0, . . . , tr] =
x0
(t0 ‚àít1) ¬∑ ¬∑ ¬∑ (t0 ‚àítr)
+
x1
(t1 ‚àít0) ¬∑ ¬∑ ¬∑ (t1 ‚àítr) + ¬∑ ¬∑ ¬∑ +
xr
(tr ‚àít0) ¬∑ ¬∑ ¬∑ (tr ‚àítr‚àí1).
(6.31b)
If we substitute (6.31) into (6.30), we see that, for example, for the terms involving
only x1
1
tr+1 ‚àít0

x1
(t1 ‚àít2)(t1 ‚àít3) ¬∑ ¬∑ ¬∑ (t1 ‚àítr)(t1 ‚àítr+1)
‚àí
x1
(t1 ‚àít0)(t1 ‚àít2) ¬∑ ¬∑ ¬∑ (t1 ‚àítr‚àí1)(t1 ‚àítr)

=
x1
tr+1 ‚àít0
1
(t1 ‚àít2) ¬∑ ¬∑ ¬∑ (t1 ‚àítr)

1
t1 ‚àítr+1
‚àí
1
t1 ‚àít0

=
x1
(t1 ‚àít0)(t1 ‚àít2) ¬∑ ¬∑ ¬∑ (t1 ‚àítr)(t1 ‚àítr+1) = Œ±r+1
1
x1.
The same holds for all remaining terms in xj for j = 0, 1, . . . , r + 1. Hence (6.27)
is valid by induction for all k ‚â•1. Because of (6.27), the ordering of the arguments
TLFeBOOK

NEWTON INTERPOLATION
261
in x[t0, . . . , tk] is irrelevant. Consequently, x[t0, . . . , tk] can be expressed as the
difference between two divided differences of order k ‚àí1, having any k ‚àí1 of
their k arguments in common, divided by the difference between those arguments
that are not in common. For example
x[t0, t1, t2, t3] = x[t1, t2, t3] ‚àíx[t0, t1, t2]
t3 ‚àít0
= x[t0, t2, t3] ‚àíx[t1, t2, t3]
t0 ‚àít1
.
What happens if two arguments of a divided difference become equal? The
situation is reminiscent of Corollary 5.1 (in Chapter 5). For example, suppose t1 =
t + œµ; then
x[t, t1] = x(t1) ‚àíx(t)
t1 ‚àít
= x(t + œµ) ‚àíx(t)
œµ
,
implying that
x[t, t] = lim
œµ‚Üí0
x(t + œµ) ‚àíx(t)
œµ
= dx(t)
dt
so that
x[t, t] = dx(t)
dt
.
(6.32)
This assumes that x(t) is differentiable. By similar reasoning
d
dt x[t0, . . . , tk, t] = x[t0, . . . , tk, t, t]
(6.33)
(assuming that t0, . . . , tk are constants). Suppose u1, . . . , un are differentiable func-
tions of t, then it turns out that
d
dt x[t0, . . . , tk, u1, . . . , un] =
n

j=1
x[t0, . . . , tk, u1, . . . , un, uj]duj
dt .
(6.34)
Therefore, if u1 = u2 = ¬∑ ¬∑ ¬∑ = un = t, then, from (6.34)
d
dt x[t0, . . . , tk, t, . . . , t
# $% &
n
] = nx[t0, . . . , tk, t, . . . , t
# $% &
n+1
].
(6.35)
Using (6.33), and (6.35) it may be shown that
dr
dtr x[t0, . . . , tk, t] = r! x[t0, . . . , tk, t, . . . , t
# $% &
r+1
].
(6.36)
Of course, this assumes that x(t) is sufÔ¨Åciently differentiable.
TLFeBOOK

262
INTERPOLATION
Equation (6.24) is just a special case of something more general. We note that
if x(t) is not a quadratic, then (6.24) is only an approximation for t Ã∏‚àà{t0, t1, t2},
and so
x(t) ‚âàx(t0) + (t ‚àít0)x[t0, t1] + (t ‚àít0)(t ‚àít1)x[t0, t1, t2] = p2(t).
(6.37)
It is easy to verify by direct evaluation that p2(ti) = x(ti) for i ‚àà{0, 1, 2}. Equation
(6.37) is the second-degree interpolation formula, while (6.16) is the Ô¨Årst-degree
interpolation formula. We may generalize (6.24) [and hence (6.37)] to higher
degrees by using (6.26); that is
x(t) = x[t0] + (t ‚àít0)x[t0, t],
x[t0, t] = x[t0, t1] + (t ‚àít1)x[t0, t1, t],
x[t0, t1, t] = x[t0, t1, t2] + (t ‚àít2)x[t0, t1, t2, t],
...
x[t0, . . . , tn‚àí1, t] = x[t0, . . . , tn] + (t ‚àítn)x[t0, . . . , tn, t],
(6.38)
where the last equation follows from
x[t0, . . . , tn, t] = x[t1, . . . , tn, t] ‚àíx[t0, . . . , tn]
t ‚àít0
= x[t0, . . . , tn‚àí1, t] ‚àíx[t0, . . . , tn]
t ‚àítn
(6.39)
(the second equality follows by exchanging t0 and tn). If the second relation of
(6.38) is substituted into the Ô¨Årst, we obtain
x(t) = x[t0] + (t ‚àít0)x[t0, t1] + (t ‚àít0)(t ‚àít1)x[t0, t1, t],
(6.40)
which is just (6.24) again. If we substitute the third relation of (6.38) into (6.40),
we obtain
x(t) = x[t0] + (t ‚àít0)x[t0, t1] + (t ‚àít0)(t ‚àít1)x[t0, t1, t2]
+ (t ‚àít0)(t ‚àít1)(t ‚àít2)x[t0, t1, t2, t].
(6.41)
This leads to the third-degree interpolation formula
p3(t) = x[t0] + (t ‚àít0)x[t0, t1] + (t ‚àít0)(t ‚àít1)x[t0, t1, t2]
+ (t ‚àít0)(t ‚àít1)(t ‚àít2)x[t0, t1, t2, t3].
TLFeBOOK

NEWTON INTERPOLATION
263
Continuing in this fashion, we obtain
x(t) = x[t0] + (t ‚àít0)x[t0, t1] + (t ‚àít0)(t ‚àít1)x[t0, t1, t2]
+ ¬∑ ¬∑ ¬∑ + (t ‚àít0)(t ‚àít1) ¬∑ ¬∑ ¬∑ (t ‚àítn‚àí1)x[t0, t1 . . . , tn] + e(t),
(6.42a)
where
e(t) = (t ‚àít0)(t ‚àít1) ¬∑ ¬∑ ¬∑ (t ‚àítn)x[t0, t1, . . . , tn, t],
(6.42b)
and we deÔ¨Åne
pn(t) = x[t0] + (t ‚àít0)x[t0, t1]
+ (t ‚àít0)(t ‚àít1)x[t0, t1, t2] + ¬∑ ¬∑ ¬∑ + (t ‚àít0) ¬∑ ¬∑ ¬∑ (t ‚àítn‚àí1)x[t0, . . . , tn],
(6.42c)
which is the nth-degree interpolating formula, and is clearly a polynomial of degree
n. So e(t) is the error involved in interpolating x(t) using polynomial pn(t). It is
the case that pn(tk) = x(tk) for k = 0, 1, . . . , n. Equation (6.42a) is the Newton
interpolating formula with divided differences. If x(t) is a polynomial of degree n
(or less), then e(t) = 0 (all t). This is more formally justiÔ¨Åed later.
Example 6.2
Consider e(t) for n = 2. This requires [via (6.27)]
x[t0, t1, t2, t] =
x0
(t0 ‚àít1)(t0 ‚àít2)(t0 ‚àít) +
x1
(t1 ‚àít0)(t1 ‚àít2)(t1 ‚àít)
+
x2
(t2 ‚àít0)(t2 ‚àít1)(t2 ‚àít) +
x(t)
(t ‚àít0)(t ‚àít1)(t ‚àít2)
Thus
e(t) = (t ‚àít0)(t ‚àít1)(t ‚àít2)x[t0, t1, t2, t]
= ‚àí(t ‚àít1)(t ‚àít2)x0
(t0 ‚àít1)(t0 ‚àít2) ‚àí(t ‚àít0)(t ‚àít2)x1
(t1 ‚àít0)(t1 ‚àít2) ‚àí(t ‚àít0)(t ‚àít1)x2
(t2 ‚àít0)(t2 ‚àít1)
#
$%
&
=‚àíp2(t)
+x(t).
(6.43)
The form of p2(t) in (6.43) is that of p2(t) in Example 6.1:
p2(t) = x0L0(t) + x1L1(t) + x2L2(t).
If x(t) = a0 + a1t + a2t2, then clearly e(t) = 0 for all t. In fact, p2,k = ak.
Example 6.3
Suppose that we wish to interpolate x(t) = et given that tk = kh
with h = 0.1, and k = 0, 1, 2, 3. We are told that
x0 = 1.000000,
x1 = 1.105171,
x2 = 1.221403,
x3 = 1.349859.
TLFeBOOK

264
INTERPOLATION
We will consider n = 2 (i.e., quadratic interpolation). The task is aided if we
construct the divided-difference table:
t0 = 0
x(t0) = 1.000000
x[t0, t1] = 1.05171
t1 = 0.1
x(t1) = 1.105171
x[t0, t1, t2] = 0.55305
x[t1, t2] = 1.16232
t2 = 0.2
x(t2) = 1.221403
x[t1, t2, t3] = 0.61120
x[t2, t3] = 1.28456
t3 = 0.3
x(t3) = 1.349859
For t ‚àà[0, 0.2] we consider [from (6.37)]
x(t) ‚âàx(t0) + (t ‚àít0)x[t0, t1] + (t ‚àít0)(t ‚àít1)x[t0, t1, t2],
which for the data we are given becomes
x(t) ‚âà1.000000 + 1.051710t + 0.55305t(t ‚àí0.1) = pa
2(t).
(6.44a)
If t = 0.11, then pa
2(0.11) = 1.116296, while it turns out that x(0.11) = 1.116278,
so e(0.11) = x(0.11) ‚àípa
2(0.11) = ‚àí0.000018. For t ‚àà[0.1, 0.3] we might con-
sider
x(t) ‚âàx(t1) + (t ‚àít1)x[t1, t2] + (t ‚àít1)(t ‚àít2)x[t1, t2, t3],
which for the given data becomes
x(t) ‚âà1.105171 + 1.16232(t ‚àí0.1) + 0.61120(t ‚àí0.1)(t ‚àí0.2) = pb
2(t).
(6.44b)
We observe that to calculate pb
2(t) does not require discarding all the results needed
to determine pa
2(t). We do not need to begin again as with Lagrangian interpolation
since, for example
x[t0, t1, t2] = x[t1, t2] ‚àíx[t0, t1]
t2 ‚àít0
,
x[t1, t2, t3] = x[t2, t3] ‚àíx[t1, t2]
t3 ‚àít1
,
and both of these divided differences require x[t1, t2]. If we wanted to use cubic
interpolation then the table is very easily augmented to include x[t0, t1, t2, t3].
Thus, updating the interpolating polynomial due to the addition of more data to the
table, or of increasing n, may proceed more efÔ¨Åciently than if we were to employ
Lagrange interpolation.
We also observe that both pa
2(t) and pb
2(t) may be used to interpolate x(t) for
t ‚àà[0.1, 0.2]. Which polynomial should be chosen? Ideally, we would select the
one for which e(t) is the smallest. Practically, this means seeking bounds for |e(t)|
and choosing the interpolating polynomial with the best error bound.
TLFeBOOK

NEWTON INTERPOLATION
265
We have shown that if x(t) is approximated by pn(t), then the error has the form
e(t) = œÄ(t)x[t0, . . . , tn, t]
(6.45a)
[recall (6.42b)], where
œÄ(t) = (t ‚àít0)(t ‚àít1) ¬∑ ¬∑ ¬∑ (t ‚àítn),
(6.45b)
which is an degree n + 1 polynomial. The form of the error in (6.45a) can be useful
in analyzing the accuracy of numerical integration and numerical differentiation
procedures, but another form of the error can be found.
We note that e(t) = x(t) ‚àípn(t) [recall (6.42)], so both x(t) ‚àípn(t) and œÄ(t)
vanish at t0, t1, . . . , tn. Consider the linear combination
X(t) = x(t) ‚àípn(t) ‚àíŒ∫œÄ(t).
(6.46)
We wish to select Œ∫ so that X(t) = 0, where t Ã∏= tk for k ‚ààZn+1. Such a Œ∫
exists because œÄ(t) vanishes only at t0, t1, . . . , tn. Let a = min{t0, . . . , tn, t}, b =
max{t0, . . . , tn, t}, and deÔ¨Åne the interval I = [a, b]. By construction, X(t) van-
ishes at least n + 2 times on I. Rolle‚Äôs theorem from calculus states that X(k)(t) =
dkX(t)/dtk vanishes at least n + 2 ‚àík times inside I. SpeciÔ¨Åcally, X(n+1)(t) van-
ishes at least once inside I. Let this point be called Œæ. Therefore, from (6.46), we
obtain
x(n+1)(Œæ) ‚àíp(n+1)
n
(Œæ) ‚àíŒ∫œÄ(n+1)(Œæ) = 0.
(6.47)
But pn(t) is a polynomial of degree n, so p(n+1)(t) = 0 for all t ‚ààI. From (6.45b)
œÄ(n+1)(t) = (n + 1)!, so Ô¨Ånally (6.47) reduces to
Œ∫ =
1
(n + 1)!x(n+1)(Œæ).
(6.48)
From X(t) = 0 in (6.46), and using (6.48), we Ô¨Ånd that
e(t) = x(t) ‚àípn(t) =
1
(n + 1)!x(n+1)(Œæ)œÄ(t)
(6.49)
for some Œæ ‚ààI. If we were to let t = tk for any k ‚ààZn+1, then both sides of (6.49)
vanish even in this previously excluded case. This allows us to write
e(t) =
1
(n + 1)!x(n+1)(Œæ(t))œÄ(t)
(6.50)
for
some
Œæ = Œæ(t) ‚ààI
(I = [a, b]
with
a = min{t0, . . . , tn, t},
and
b =
max{t0, . . . , tn, t}). If x(n+1)(t) is continuous for t ‚ààI, then x(n+1)(t) is bounded
on I, so there is an Mn+1 > 0 such that
x(n+1)(Œæ) ‚â§Mn+1,
(6.51)
TLFeBOOK

266
INTERPOLATION
and hence
|e(t)| ‚â§
Mn+1
(n + 1)!|œÄ(t)|
(6.52)
for all t ‚ààI. It is to be emphasized that for this to hold x(n+1)(t) must exist,
and we normally require it to be continuous, too. Equations (6.50) and (6.45a) are
equivalent, and thus
œÄ(t)x[t0, . . . , tn, t] =
1
(n + 1)!x(n+1)(Œæ)œÄ(t),
or in other words
x[t0, . . . , tn, t] =
1
(n + 1)!x(n+1)(Œæ)
(6.53)
for some Œæ ‚ààI, whenever x(n+1)(t) exists in I. In particular, if x(t) is a polynomial
of degree n or less, then (6.53) yields x[t0, . . . tn, t] = 0, hence e(t) = 0 for all t
(a fact mentioned earlier).
Example 6.4
Recall Example 6.3. We considered x(t) = et with t0 = 0, t1 =
0.1, t2 = 0.2, and we found that
pa
2(t) = 1.000000 + 1.051710t + 0.553050t(t ‚àí0.1)
for which pa
2(0.11) = 1.116296, and x(0.11) = 1.116278. The exact error is
e(0.11) = x(0.11) ‚àípa
2(0.11) = ‚àí0.000018.
We will compare this with the bound we obtain from (6.52). Since n = 2, x(3)(t) =
et, and I = [0, 0.2], so
M3 = e0.2 = 1.221403
(which was given data), and œÄ(t) = t(t ‚àí0.1)(t ‚àí0.2), so |œÄ(0.11)| = 9.9 √ó 10‚àí5.
Consequently, from (6.52), we have
|e(0.11)| ‚â§1.221403
3!
9.9 √ó 10‚àí5 = 0.000020.
The actual error certainly agrees with this bound.
We end this section by observing that (6.14) immediately follows from (6.50).
6.4
HERMITE INTERPOLATION
In the previous sections polynomial interpolation methods matched the polynomial
only to the value of the function f (x) at various points x = xk ‚àà[a, b] ‚äÇR. In
TLFeBOOK

HERMITE INTERPOLATION
267
this section we consider Hermite interpolation where the interpolating polynomial
also matches the Ô¨Årst derivatives f (1)(x) at x = xk. This interpolation technique
is important in the development of higher order numerical integration methods as
will be seen in Chapter 9.
The following theorem is the main result, and is essentially Theorem 3.9 from
Burden and Faires [6].
Theorem 6.1: Hermite Interpolation
Suppose that f (x) ‚ààC1[a, b], and
that x0, x1, . . . , xn ‚àà[a, b] are distinct, then the unique polynomial of degree (at
most) 2n + 1 denoted by p2n+1(x), and such that
p2n+1(xj) = f (xj), p(1)
2n+1(xj) = f (1)(xj)
(6.54)
(j ‚ààZn+1) is given by
p2n+1(x) =
n

k=0
hk(x)f (xk) +
n

k=0
ÀÜhk(x)f (1)(xk),
(6.55)
where
hk(x) = [1 ‚àí2L(1)
k (xk)(x ‚àíxk)][Lk(x)]2,
(6.56)
and
ÀÜhk(x) = (x ‚àíxk)[Lk(x)]2
(6.57)
such that [recall (6.11)]
Lk(x) =
n
/
i=0
iÃ∏=k
x ‚àíxi
xk ‚àíxi
.
(6.58)
Proof
To show (6.54) for x0, x1, . . . , xn, we require that hk(x), and ÀÜhk(x) in
(6.56) and (6.57) satisfy the conditions
hk(xj) = Œ¥j‚àík,
h(1)
k (xj) = 0
(6.59a)
and
ÀÜhk(xj) = 0,
ÀÜh(1)
k (xj) = Œ¥j‚àík.
(6.59b)
Assuming that these conditions hold, we may conÔ¨Årm (6.54) as follows. Via (6.55)
p2n+1(xj) =
n

k=0
hk(xj)f (xk) +
n

k=0
ÀÜhk(xj)f (1)(xk),
TLFeBOOK

268
INTERPOLATION
and via (6.59), this becomes
p2n+1(xj) =
n

k=0
Œ¥j‚àíkf (xk) +
n

k=0
0 ¬∑ f (1)(xk) = f (xj).
This conÔ¨Årms the Ô¨Årst case in (6.54). Similarly, via (6.59)
p(1)
2n+1(xj) =
n

k=0
h(1)
k (xj)f (xk) +
n

k=0
ÀÜh(1)
k (xj)f (1)(xk)
becomes
p(1)
2n+1(xj) =
n

k=0
0 ¬∑ f (xk) +
n

k=0
Œ¥j‚àíkf (1)(xk) = f (1)(xj),
which conÔ¨Årms the second case in (6.54).
Now we will conÔ¨Årm that hk(x), and ÀÜhk(x) as deÔ¨Åned in (6.56), and (6.57)
satisfy the requirements given in (6.59). The conditions in (6.59b) imply that ÀÜhk(x)
must have a double root at x = xj for j Ã∏= k, and a single root at x = xk. A
polynomial of degree at most 2n + 1 that satisÔ¨Åes these requirements, and such
that ÀÜh(1)
k (xk) = 1 is
ÀÜhk(x) = (x ‚àíxk)
(x ‚àíx0)2 ¬∑ ¬∑ ¬∑ (x ‚àíxk‚àí1)2 ¬∑ (x ‚àíxk+1)2 ¬∑ ¬∑ ¬∑ (x ‚àíxn)2
(xk ‚àíx0)2 ¬∑ ¬∑ ¬∑ (xk ‚àíxk‚àí1)2 ¬∑ (xk ‚àíxk+1)2 ¬∑ ¬∑ ¬∑ (xk ‚àíxn)2
= (x ‚àíxk)L2
k(x).
Certainly ÀÜhk(xk) = 0. Moreover, ÀÜh(1)
k (x) = L2
k(x) + 2(x ‚àíxk)Lk(x)L(1)
k (x) so
ÀÜh(1)
k (xk) = L2
k(xk) = 1 [via (6.8)]. These verify (6.59b).
Now we consider (6.59a). These imply xj for j Ã∏= k is a double root of hk(x),
and we may consider (for suitable a and b to be found below)
hk(x) =
1
0n
i=0
iÃ∏=k
(xi ‚àíxk)2 (x ‚àíx0)2 ¬∑ ¬∑ ¬∑ (x ‚àíxk‚àí1)2(x ‚àíxk+1)2
¬∑ ¬∑ ¬∑ (x ‚àíxn)2(ax + b)
which has degree at most 2n + 1. More concisely, this polynomial is
hk(x) = L2
k(x)(ax + b).
From (6.59a) we require
1 = hk(xk) = L2
k(xk)(axk + b) = axk + b,
(6.60)
TLFeBOOK

SPLINE INTERPOLATION
269
Also, h(1)
k (x) = aL2
k(x) + 2Lk(x)L(1)
k (x)(ax + b), and we also need [via (6.59a)]
h(1)
k (xk) = aL2
k(xk) + 2Lk(xk)L(1)
k (xk)(axk + b) = 0,
but again since Lk(xk) = 1, this expression reduces to
a + 2L(1)
k (xk) = 0,
where we have used (6.60). Hence
a = ‚àí2L(1)
k (xk),
b = 1 + 2L(1)
k (xk)xk.
Therefore, we Ô¨Ånally have
hk(x) = [1 ‚àí2L(1)
k (xk)(x ‚àíxk)]L2
k(x)
which is (6.56). Since Lk(xj) = 0 for j Ã∏= k it is clear that hk(xj) = 0 for j Ã∏= k.
It is also easy to see that h(1)
k (xj) = 0 for all j Ã∏= k too. Thus, (6.59a) is conÔ¨Årmed
for hk(x) as deÔ¨Åned in (6.56).
An error bound for Hermite interpolation is provided by the expression
f (x) = p2n+1(x) +
1
(2n + 2)!
n
/
i=0
(x ‚àíxi)2f (2n+2)(Œæ)
(6.61)
for some Œæ ‚àà(a, b), where f (x) ‚ààC2n+2[a, b]. We shall not derive (6.61) except
to note that the approach is similar to the derivation of (6.14). Equation (6.14) was
really derived in Section 6.3.
In its present form Hermite interpolation requires working with Lagrange poly-
nomials, and their derivatives. As noted by Burden and Faires [6], this is rather
tedious (i.e., not computationally efÔ¨Åcient). A procedure involving Newton inter-
polation (recall Section 6.3) may be employed to reduce the labor that would
otherwise be involved in Hermite interpolation. We do not consider this approach,
but instead refer the reader to Burden and Faires [6] for the details. We use Hermite
interpolation in Chapter 9 to develop numerical integration methods, and efÔ¨Åcient
Hermite interpolation is not needed for this purpose.
6.5
SPLINE INTERPOLATION
Spline (spliced line) interpolation is a particular kind of piecewise polynomial
interpolation. We may wish, for example, to approximate f (x) for x ‚àà[a, b] ‚äÇ
R when given the sample points {(xk, f (xk))|k ‚ààZn+1} by Ô¨Åtting straight-line
segments in between (xk, f (xk)), and (xk+1, f (xk+1)) for k = 0, 1, . . . , n ‚àí1. An
TLFeBOOK

270
INTERPOLATION
0
0.5
1
1.5
2
2.5
3
3.5
4
‚àí6
‚àí4
‚àí2
0
2
4
6
x
 y 
f(x)
Piecewise linear interpolant
Figure 6.1
The cubic polynomial f (x) = (x ‚àí1)(x ‚àí2)(x ‚àí3), and its piecewise linear
interpolant (dashed line) at the nodes xk = x0 + hk for which x0 = 0, and h = 1
2, where
k = 0, 1, . . . , n with n = 8.
example of this appears in Fig. 6.1. This has a number of disadvantages. Although
f (x) may be differentiable at x = xk, the piecewise linear approximation will not
be (in general). Also, the graph of the interpolant has visually displeasing ‚Äúkinks‚Äù
in it. If interpolation is for a computer graphics application, or to deÔ¨Åne the physical
surface of an automobile body or airplane, then such kinks are seldom acceptable.
Splines are a means to deal with this problem. It is also worth noting that, more
recently, splines have found a role in the design of wavelet functions [7, 8], which
were brieÔ¨Çy mentioned in Chapter 1.
The following deÔ¨Ånition is taken from Epperson [9], and our exposition of spline
functions in this section follows that in [9] fairly closely. As always f (i)(x) =
dif (x)/dxi (i.e., this is the notation for the ith derivative of f (x)).
DeÔ¨Ånition 6.1: Spline
Suppose that we are given {(xk, f (xk))|k ‚ààZn+1}.
The piecewise polynomial function pm(x) is called a spline if
(S1) pm(xk) = f (xk) for all k ‚ààZn+1 (interpolation).
(S2) limx‚Üíx‚àí
k p(i)
m (x) = limx‚Üíx+
k p(i)
m (x) for all i ‚ààZN+1 (smoothness).
TLFeBOOK

SPLINE INTERPOLATION
271
(S3) pm(x) is a polynomial of degree no larger than m on every subinterval
[xk, xk+1] for k ‚ààZn (interval of deÔ¨Ånition).
We say that m is the degree of approximation and N is the degree of smoothness
of the spline pm(x).
There is a relationship between m and N. As there are n subintervals [xk, xk+1],
and each of these is the domain of deÔ¨Ånition of a degree m polynomial, we see
that there are Df = n(m + 1) degrees of freedom. Each polynomial is speciÔ¨Åed by
m + 1 coefÔ¨Åcients, and there are n of these polynomials; hence Df is the number
of parameters to solve for in total. From DeÔ¨Ånition 6.1 there are n + 1 interpo-
lation conditions [axiom (S1)]. And there are n ‚àí1 junction points x1, . . . xn‚àí1
(sometimes also called knots), with N + 1 continuity conditions being imposed on
each of them [axiom (S2)]. As a result, there are Dc = (n + 1) + (n ‚àí1)(N + 1)
constraints. Consider
Df ‚àíDc = n(m + 1) ‚àí[(n + 1) + (n ‚àí1)(N + 1)] = n(m ‚àíN ‚àí1) + N.
(6.62)
It is a common practice to enforce the condition m ‚àíN ‚àí1 = 0; that is, we let
m = N + 1.
(6.63)
This relates the degree of approximation to the degree of smoothness in a simple
manner. Below we will focus our attention exclusively on the special case of the
cubic splines for which m = 3. From (6.63) we must therefore have N = 2. With
condition (6.63), then, from (6.62) we have
Df ‚àíDc = N.
(6.64)
As a result, it is necessary to impose N further constraints on the design problem.
How this is done is considered in detail below. Since we will look only at m = 3
with N = 2, we must impose two additional constraints. This will be done by
imposing one constraint at each endpoint of the interval [a, b]. There is more than
one way to do this as will be seen later.
From DeÔ¨Ånition 6.1 it superÔ¨Åcially appears that we need to compute n different
polynomials. However, it is possible to recast our problem in terms of B-splines.
A B-spline acts as a prototype in the formation of a basis set of splines.
Aside from the assumption that m = 3, N = 2, let us further assume that
a = x0 < x1 < ¬∑ ¬∑ ¬∑ < xn‚àí1 < xn = b
(6.65)
with xk+1 ‚àíxk = h for k = 0, 1, . . . , n ‚àí1. This is the uniform grid assumption.
We will also need to account for boundary conditions, and this requires us to
introduce the additional grid points
x‚àí3 = a ‚àí3h,
x‚àí2 = a ‚àí2h,
x‚àí1 = a ‚àíh
(6.66)
TLFeBOOK

272
INTERPOLATION
and
xn+3 = b + 3h,
xn+2 = b + 2h,
xn+1 = b + h.
(6.67)
Our prototype cubic B-spline will be the function
S(x) =
Ô£±
Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£≤
Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£≥
0,
x ‚â§‚àí2
(x + 2)3,
‚àí2 ‚â§x ‚â§‚àí1
1 + 3(x + 1) + 3(x + 1)2 ‚àí3(x + 1)3,
‚àí1 ‚â§x ‚â§0
1 + 3(1 ‚àíx) + 3(1 ‚àíx)2 ‚àí3(1 ‚àíx)3,
0 ‚â§x ‚â§1
(2 ‚àíx)3,
1 ‚â§x ‚â§2
0,
x ‚â•2
.
(6.68)
This function has nodes at x ‚àà{‚àí2, ‚àí1, 0, 1, 2}. A plot of it appears in Fig. 6.2, and
we see that it has a bell shape similar to the Gaussian pulse we saw in Chapter 3.
We may verify that S(x) satisÔ¨Åes DeÔ¨Ånition 6.1 as follows. Plainly, it is piecewise
cubic (i.e., m = 3), so axiom (S3) holds. The Ô¨Årst and second derivatives are,
‚àí2
‚àí1.5
‚àí1
‚àí0.5
0
0.5
1
1.5
2
0
0.5
1
1.5
2
2.5
3
3.5
4
x
S(x)
Figure 6.2
A plot of the cubic B-spline deÔ¨Åned in Eq. (6.68).
TLFeBOOK

SPLINE INTERPOLATION
273
respectively
S(1)(x) =
Ô£±
Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£≤
Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£≥
0,
x ‚â§‚àí2
3(x + 2)2,
‚àí2 ‚â§x ‚â§‚àí1
3 + 6(x + 1) ‚àí9(x + 1)2,
‚àí1 ‚â§x ‚â§0
‚àí3 ‚àí6(1 ‚àíx) + 9(1 ‚àíx)2,
0 ‚â§x ‚â§1
‚àí3(2 ‚àíx)2,
1 ‚â§x ‚â§2
0,
x ‚â•2
,
(6.69)
and
S(2)(x) =
Ô£±
Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£≤
Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£≥
0,
x ‚â§‚àí2
6(x + 2),
‚àí2 ‚â§x ‚â§‚àí1
6 ‚àí18(x + 1),
‚àí1 ‚â§x ‚â§0
6 ‚àí18(1 ‚àíx),
0 ‚â§x ‚â§1
6(2 ‚àíx),
1 ‚â§x ‚â§2
0,
x ‚â•2
.
(6.70)
We note that from (6.68)‚Äì(6.70) that
S(0) = 4,
S(¬±1) = 1,
S(¬±2) = 0,
(6.71a)
S(1)(0) = 0,
S(1)(¬±1) = ‚àì3,
S(1)(¬±2) = 0,
(6.71b)
S(2)(0) = ‚àí12,
S(2)(¬±1) = 6,
S(2)(¬±2) = 0.
(6.71c)
So it is apparent that for i = 0, 1, 2 we have
lim
x‚Üíx‚àí
k
S(i)(x) = lim
x‚Üíx+
k
S(i)(x)
for all xk ‚àà{‚àí2, ‚àí1, 0, 1, 2}. Thus, the smoothness axiom (S2) is met for N = 2.
Now we need to consider how we may employ S(x) to approximate any f (x)
for x ‚àà[a, b] ‚äÇR when working with the grid speciÔ¨Åed in (6.65)‚Äì(6.67). To this
end we deÔ¨Åne
Si(x) = S
 x ‚àíxi
h
!
(6.72)
for
i = ‚àí1, 0, 1, . . ., n, n + 1.
Since
S(1)
i
(x) = 1
hS(1) ) x‚àíxi
h
*
,
S(2)
i
(x) =
1
h2 S(2) ) x‚àíxi
h
*
from (6.71), we have (e.g., xi¬±1 = xi ¬± h.)
Si(xi) = S(0) = 4,
Si(xi¬±1) = S(¬±1) = 1,
Si(xi¬±2) = S(¬±2) = 0,
(6.73a)
TLFeBOOK

274
INTERPOLATION
S(1)
i
(xi) = 0,
S(1)
i
(xi¬±1) = ‚àì3
h,
S(1)
i
(xi¬±2) = 0,
(6.73b)
S(2)
i
(xi) = ‚àí12
h2 ,
S(2)
i
(xi¬±1) = 6
h2 ,
S(2)
i
(xi¬±2) = 0.
(6.73c)
We construct a cubic B-spline interpolant for any f (x) by deÔ¨Åning spline p3(x)
to be a linear combination of Si(x) for i = ‚àí1, 0, 1, . . ., n, n + 1, i.e., for suitable
coefÔ¨Åcients ai we have
p3(x) =
n+1

i=‚àí1
aiSi(x).
(6.74)
The series coefÔ¨Åcients ai are determined in order to satisfy axiom (S1) of DeÔ¨Ånition
6.1; thus, we have
f (xk) =
n+1

i=‚àí1
aiSi(xk)
(6.75)
for k ‚ààZn+1.
If we apply (6.73a) to (6.75), we observe that
f (x0)
=
n+1

i=‚àí1
aiSi(x0)
=
a‚àí1S‚àí1(x0) + a0S0(x0) + a1S1(x0)
f (x1)
=
n+1

i=‚àí1
aiSi(x1)
=
a0S0(x1) + a1S1(x1) + a2S2(x1)
...
f (xn)
=
n+1

i=‚àí1
aiSi(xn)
=
an‚àí1Sn‚àí1(xn) + anSn(xn) + an+1Sn+1(xn)
.
(6.76)
For example, for f (x0) in (6.76), we note that Sk(x0) = 0 for k ‚â•2 since (xk =
x0 + kh)
Sk(x0) = S
 x0 ‚àíxk
h
!
= S
 x0 ‚àí(kh + x0)
h
!
= S(‚àík) = 0.
More generally
f (xk) = ak‚àí1Sk‚àí1(xk) + akSk(xk) + ak+1Sk+1(xk)
(6.77)
for which k ‚ààZn+1. Again via (6.73a) we see that
Sk‚àí1(xk) = S
 (x0 + kh) ‚àí(x0 + (k ‚àí1)h)
h
!
= S(1) = 1,
TLFeBOOK

SPLINE INTERPOLATION
275
Sk(xk) = S
 (x0 + kh) ‚àí(x0 + kh)
h
!
= S(0) = 4,
Sk+1(xk) = S
 (x0 + kh) ‚àí(x0 + (k + 1)h)
h
!
= S(‚àí1) = 1.
Thus, (6.77) becomes
ak‚àí1 + 4ak + ak+1 = f (xk)
(6.78)
again for k = 0, 1, . . . , n. In matrix form we have
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
1
4
1
0
¬∑ ¬∑ ¬∑
0
0
0
0
1
4
1
¬∑ ¬∑ ¬∑
0
0
0
...
...
...
...
...
...
...
0
0
0
0
¬∑ ¬∑ ¬∑
4
1
0
0
0
0
0
¬∑ ¬∑ ¬∑
1
4
1
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
#
$%
&
=A
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
a‚àí1
a0
...
an
an+1
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
#
$%
&
=a
=
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
f (x0)
f (x1)
...
f (xn‚àí1)
f (xn)
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
#
$%
&
=f
.
(6.79)
We note that A ‚ààR(n+1)√ó(n+3), so there are n + 1 equations in n + 3 unknowns
(a ‚ààRn+3). The tridiagonal linear system in (6.79) cannot be solved in its present
form as we need two additional constraints. This is to be expected from our earlier
discussion surrounding (6.62)‚Äì(6.64). Recall that we have chosen m = 3, so N = 2,
and so, via (6.64), we have Df ‚àíDc = N = 2, implying the need for two more
constraints. There are two common approaches to obtaining these constraints:
1. We enforce p(2)
3 (x0) = p(2)
3 (xn) = 0 (natural spline).
2. We enforce p(1)
3 (x0) = f (1)(x0), and p(1)
3 (xn) = f (1)(xn) (complete spline,
or clamped spline).
Of these two choices, the natural spline is a bit easier to work with but can lead
to larger approximation errors near the interval endpoints x0, and xn compared to
working with the complete spline. The complete spline can avoid the apparent need
to know the derivatives f (1)(x0), and f (1)(xn) by using numerical approximations
to the derivative (see Section 9.6).
We will Ô¨Årst consider the case of the natural spline. From (6.74), we obtain
p3(x0) = a‚àí1S‚àí1(x0) + a0S0(x0) + a1S1(x0),
so that
p(2)
3 (x0) = a‚àí1S(2)
‚àí1(x0) + a0S(2)
0 (x0) + a1S(2)
1 (x0).
(6.80)
TLFeBOOK

276
INTERPOLATION
Since S(2)
i
(x0) = 1
h2 S(2) ) x0‚àíxi
h
*
, we have
S(2)
‚àí1(x0) = 1
h2 S(2)
 x0 ‚àí(x0 ‚àíh)
h
!
= 1
h2 S(2)(1) = 6
h2 ,
(6.81a)
S(2)
0 (x0) = 1
h2 S(2)
 x0 ‚àíx0
h
!
= 1
h2 S(2)(0) = ‚àí12
h2 ,
(6.81b)
S(2)
1 (x0) = 1
h2 S(2)
 x0 ‚àí(x0 + h)
h
!
= 1
h2 S(2)(‚àí1) = 6
h2 ,
(6.81c)
where (6.71c) was used. Thus, (6.80) reduces to
h‚àí2[6a‚àí1 ‚àí12a0 + 6a1] = 0
or
a‚àí1 = 2a0 ‚àía1.
(6.82)
Similarly
p3(xn) = an‚àí1Sn‚àí1(xn) + anSn(xn) + an+1Sn+1(xn),
so that
p(2)
3 (xn) = an‚àí1S(2)
n‚àí1(xn) + anS(2)
n (xn) + an+1S(2)
n+1(xn).
(6.83)
Since S(2)
i
(xn) = 1
h2 S(2) ) xn‚àíxi
h
*
, we have
S(2)
n‚àí1(xn) = 1
h2 S(2)
 (x0 + nh) ‚àí(x0 + (n ‚àí1)h)
h
!
= 1
h2 S(2)(1) = 6
h2 ,
(6.84a)
S(2)
n (xn) = 1
h2 S(2)
 (x0 + nh) ‚àí(x0 + nh)
h
!
= 1
h2 S(2)(0) = ‚àí12
h2 ,
(6.84b)
S(2)
n+1(xn) = 1
h2 S(2)
 (x0 + nh) ‚àí(x0 + (n + 1)h)
h
!
= 1
h2 S(2)(‚àí1) = 6
h2 ,
(6.84c)
where (6.71c) was again employed. Thus, (6.83) reduces to
h‚àí2[6an‚àí1 ‚àí12an + 6an+1] = 0
or
an+1 = 2an ‚àían‚àí1.
(6.85)
Now since [from (6.79)]
a‚àí1 + 4a0 + a1 = f (x0)
TLFeBOOK

SPLINE INTERPOLATION
277
using (6.82) we have
6a0 = f (x0),
(6.86a)
and similarly
an‚àí1 + 4an + an+1 = f (xn),
so via (6.85) we have
6an = f (xn).
(6.86b)
Using (6.86) we may rewrite (6.79) as the linear system
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
4
1
0
¬∑ ¬∑ ¬∑
0
0
0
1
4
1
¬∑ ¬∑ ¬∑
0
0
0
...
...
...
...
...
...
0
0
0
¬∑ ¬∑ ¬∑
1
4
1
0
0
0
¬∑ ¬∑ ¬∑
0
1
4
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
#
$%
&
=A
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
a1
a2
...
an‚àí2
an‚àí1
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
#
$%
&
=a
=
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
f (x1) ‚àí1
6f (x0)
f (x2)
...
f (xn‚àí2)
f (xn‚àí1) ‚àí1
6f (xn)
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
#
$%
&
=f
.
(6.87)
Here we have A ‚ààR(n‚àí1)√ó(n‚àí1), so now the tridiagonal system Aa = f has a
unique solution, assuming that A‚àí1 exists. The existence of A‚àí1 will be justiÔ¨Åed
below.
Now we will consider the case of the complete spline. In the case of
p(1)
3 (x0) = a‚àí1S(1)
‚àí1(x0) + a0S(1)
0 (x0) + a1S(1)
1 (x0) = f (1)(x0),
(6.88)
since S(1)
i
(x0) = 1
hS(1) ) x0‚àíxi
h
*
, we have, using (6.71b)
S(1)
‚àí1(x0) = 1
hS(1)
 x0 ‚àí(x0 ‚àíh)
h
!
= 1
hS(1)(1) = ‚àí3
h,
S(1)
0 (x0) = 1
hS(1)
 x0 ‚àíx0
h
!
= 1
hS(1)(0) = 0,
S(1)
1 (x0) = 1
hS(1)
 x0 ‚àí(x0 + h)
h
!
= 1
hS(1)(‚àí1) = 3
h,
so (6.88) becomes
p(1)
3 (x0) = 3h‚àí1[‚àía‚àí1 + a1] = f (1)(x0).
(6.89)
Similarly
p(1)
3 (xn) = an‚àí1S(1)
n‚àí1(xn) + anS(1)
n (xn) + an+1S(1)
n+1(xn) = f (1)(xn)
(6.90)
TLFeBOOK

278
INTERPOLATION
reduces to
p(1)
3 (xn) = 3h‚àí1[‚àían‚àí1 + an+1] = f (1)(xn).
(6.91)
From (6.89) and (6.91), we obtain
a‚àí1 = a1 ‚àí1
3hf (1)(x0),
an+1 = an‚àí1 + 1
3hf (1)(xn).
(6.92)
If we substitute (6.92) into (6.79), we obtain
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
4
2
0
¬∑ ¬∑ ¬∑
0
0
0
1
4
1
¬∑ ¬∑ ¬∑
0
0
0
...
...
...
...
...
...
0
0
0
¬∑ ¬∑ ¬∑
1
4
1
0
0
0
¬∑ ¬∑ ¬∑
0
2
4
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
#
$%
&
=A
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
a0
a1
...
an‚àí1
an
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
#
$%
&
=a
=
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
f (x0) + 1
3hf (1)(x0)
f (x1)
...
f (xn‚àí1)
f (xn) ‚àí1
3hf (1)(xn)
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
#
$%
&
=f
. (6.93)
Now we have A ‚ààR(n+1)√ó(n+1), and we see that Aa = f of (6.93) will have a
unique solution provided that A‚àí1 exists.
Matrix A is tridiagonal, and so is quite sparse (i.e., it has many zero-valued
entries) because of the ‚Äúlocality‚Äù of the function S(x). This locality makes it pos-
sible to evaluate p3(x) in (6.74) efÔ¨Åciently. If we know that x ‚àà[xk, xk+1], then
p3(x) = ak‚àí1Sk‚àí1(x) + akSk(x) + ak+1Sk+1(x) + ak+2Sk+2(x).
(6.94)
We write supp g(x) = [a, b] to represent the fact that g(x) = 0 for all x < a, and
x > b, while g(x) might not be zero-valued for x ‚àà[a, b]. From (6.68) we may
therefore say that supp S(x) = [‚àí2, 2], and so from (6.72), supp Si(x) = [xi ‚àí
2h, xi + 2h], so we see that Si(x) is not necessarily zero-valued for x ‚àà[xi ‚àí
2h, xi + 2h]. From this, (6.94), and the fact that xi = x0 + ih, we obtain
supp p3(x) = ‚à™k+2
i=k‚àí1supp Si(x) = [x0 + (k ‚àí3)h, x0 + (k + 1)h] ‚à™
¬∑ ¬∑ ¬∑ ‚à™[x0 + kh, x0 + (k + 4)h]
(6.95)
)
if Ai are sets, then ‚à™m
i=n Ai = An ‚à™¬∑ ¬∑ ¬∑ ‚à™Am
*
,
which
‚Äúcovers‚Äù
the
interval
[xk, xk+1] = [x0 + kh, x0 + (k + 1)h]. Because the sampling grid (6.65)‚Äì(6.67),
is uniform it is easy to establish that (via x ‚â•x0 + kh)
k =
>x ‚àíx0
h
?
,
(6.96)
where ‚åäx‚åã= the largest integer that is ‚â§x ‚ààR. Since supp S(x) is an interval of
Ô¨Ånite length, we say that S(x) is compactly supported. This locality of support is
also a part of what makes splines useful in wavelet constructions.
TLFeBOOK

SPLINE INTERPOLATION
279
We now consider the solution of Aa = f in either of (6.87) or (6.93). Obviously,
Gaussian elimination is a possible method, but this is not efÔ¨Åcient since Gaussian
elimination is a general procedure that does not take advantage of any matrix
structure. The sparse tridiagonal structure of A can be exploited for a more efÔ¨Åcient
solution. We will consider a general method of tridiagonal linear system solution
here, but it is one that is based on modifying the general Gaussian elimination
method. We begin by deÔ¨Åning
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
a00
a01
0
¬∑ ¬∑ ¬∑
0
0
a10
a11
a12
¬∑ ¬∑ ¬∑
0
0
0
a21
a22
¬∑ ¬∑ ¬∑
0
0
...
...
...
...
...
0
0
0
¬∑ ¬∑ ¬∑
an‚àí1,n‚àí1
an‚àí1,n
0
0
0
¬∑ ¬∑ ¬∑
an,n‚àí1
an,n
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
#
$%
&
=A
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
x0
x1
x2
...
xn‚àí1
xn
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
#
$%
&
=x
=
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
f0
f1
f2
...
fn‚àí1
fn
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
#
$%
&
=f
. (6.97)
Clearly, A ‚ààR(n+1)√ó(n+1). There is some terminology associated with tridiagonal
matrices. The main diagonal consists of the elements ai,i, while the diagonal above
this consists of the elements ai,i+1, and is often called the superdiagonal. Similarly,
the diagonal below the main diagonal consists of the elements ai+1,i, and is often
called the subdiagonal.
Our approach to solving Ax = f in (6.97) will be to apply Gaussian elimination
to the augmented linear system [A|f ] in order to reduce A to upper triangular form,
and then backward substitution will be used to solve for x. This is much the same
procedure as considered in Section 4.5, except that the tridiagonal structure of A
makes matters easier. To see this, consider the special case of n = 3 as an example,
that is (A0 = A, f 0 = f )
[A0|f 0] =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£∞
a0
00
a0
01
0
0
|
f 0
0
a0
10
a0
11
a0
12
0
|
f 0
1
0
a0
21
a0
22
a0
23
|
f 0
2
0
0
a0
32
a0
33
|
f 0
3
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
(6.98)
We may apply elementary row operations to eliminate a0
10 in (6.98). Thus, (6.98)
becomes
[A1|f 1] =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
a0
00
a0
01
0
0
|
f 0
0
0
a0
11 ‚àía0
10
a0
00 a0
01
a0
12
0
|
f 0
1 ‚àía0
10
a0
00 f 0
0
0
a0
21
a0
22
a0
23
|
f 0
2
0
0
a0
32
a0
33
|
f 0
3
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
TLFeBOOK

280
INTERPOLATION
=
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£∞
a1
00
a1
01
0
0
|
f 1
0
0
a1
11
a1
12
0
|
f 1
1
0
a1
21
a1
22
a1
23
|
f 1
2
0
0
a1
32
a1
33
|
f 1
3
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
(6.99)
Now we apply elementary row operations to eliminate a1
21. Thus, (6.99) becomes
[A2|f 2] =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
a1
00
a1
01
0
0
|
f 1
0
0
a1
11
a1
12
0
|
f 1
1
0
0
a1
22 ‚àía1
21
a1
11 a1
12
a1
23
|
f 1
2 ‚àía1
21
a1
11 f 1
1
0
0
a1
32
a1
33
|
f 1
3
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
=
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£∞
a2
00
a2
01
0
0
|
f 2
0
0
a2
11
a2
12
0
|
f 2
1
0
0
a2
22
a2
23
|
f 2
2
0
0
a2
32
a2
33
|
f 2
3
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
(6.100)
Finally, we eliminate a2
32, in which case (6.100) becomes
[A3|f 3] =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
a2
00
a2
01
0
0
|
f 2
0
0
a2
11
a2
12
0
|
f 2
1
0
0
a2
22
a2
23
|
f 2
2
0
0
0
a2
33 ‚àía2
32
a2
22 a2
23
|
f 2
3 ‚àía2
32
a2
22 f 2
2
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
=
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
a3
00
a3
01
0
0
|
f 3
0
0
a3
11
a3
12
0
|
f 3
1
0
0
a3
22
a3
23
|
f 3
2
0
0
0
a3
33
|
f 3
3
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
(6.101)
We have A3 = U, an upper triangular matrix. Thus, Ux = f 3 can be solved by
backward substitution. The reader should write a pseudocode program to implement
this approach for any n. Ideally, the code ought to be written such that only the
vector f , and the main, super-, and subdiagonals of A are stored.
TLFeBOOK

SPLINE INTERPOLATION
281
We observe that the algorithm we have just constructed will work only if ai
i,i Ã∏= 0
for all i = 0, 1, . . . , n. Thus, our approach may not be stable. We expect this
potential problem because our algorithm does not employ any pivoting. However,
our application here is to use the algorithm to solve either (6.87) or (6.93). For A in
either of these cases, we never get ai
i,i = 0. We may justify this claim as follows.
In DeÔ¨Ånition 6.2 it is understood that ai,j = 0 for i, j Ã∏‚ààZn+1.
DeÔ¨Ånition 6.2: Diagonal
Dominance
The
tridiagonal
matrix
A =
[ai,j]i,j=0,...,n ‚ààR(n+1)√ó(n+1) is diagonally dominant if
ai,i > |ai,i‚àí1| + |ai,i+1| > 0
for i = 0, 1, . . . , n.
It is clear that A in (6.87), or in (6.93) is diagonally dominant. For the algorithm
we have developed to solve Ax = f , in general we can say
ak+1
k+1,k+1 = ak
k+1,k+1 ‚àí
ak
k+1,k
ak
k,k
ak
k,k+1,
(6.102)
with ak+1
k+1,k = 0, and
ak
k,k+1 = ak,k+1
(6.103)
for k = 0, 1, . . . , n ‚àí1. Condition (6.103) states that the algorithm does not modify
the superdiagonal elements of A.
Theorem 6.2:
If tridiagonal matrix A is diagonally dominant, then the algo-
rithm for solving Ax = f will not yield ai
i,i = 0 for any i = 0, 1, . . . , n.
Proof
We give only an outline of the main idea. The complete proof needs
mathematical induction.
From DeÔ¨Ånition 6.2
a0
0,0 > |a0
0,‚àí1| + |a0
0,1| = |a0
0,1| > 0,
so from (6.102)
a1
1,1 = a0
1,1 ‚àí
a0
0,1
a0
0,0
a0
1,0,
with a1
1,0 = 0. Thus, A1 can be obtained from A0 = A. Now again from DeÔ¨Åni-
tion 6.2
a0
1,1 > |a0
1,0| + |a0
1,2| > 0,
TLFeBOOK

282
INTERPOLATION
so that, because 0 < |a0
0,1/a0
0,0| < 1, we can say that
a1
1,1 ‚â•a0
1,1 ‚àí

a0
0,1
a0
0,0
 |a0
1,0|
‚â•|a0
1,0| + |a0
1,2| ‚àí

a0
0,1
a0
0,0
 |a0
1,0|
=
'
1 ‚àí

a0
0,1
a0
0,0

(
|a0
1,0| + |a0
1,2|
> |a0
1,2| = |a1
1,2| = |a1
1,0| + |a1
1,2| > 0,
and A1 is diagonally dominant. The diagonal dominance of A0 implies the diagonal
dominance of A1. In general, we see that if Ak is diagonally dominant, then Ak+1
will be diagonally dominant as well, and so Ak+1 can be formed from Ak for all
k = 0, 1, . . . , n ‚àí1. Thus, ai
i,i Ã∏= 0 for all i = 0, 1, . . . , n.
If A is diagonally dominant, it will be well-conditioned, too (not obvious), and
so our algorithm to solve Ax = f is actually quite stable in this case.
Before considering an example of spline interpolation, we specify a result con-
cerning the accuracy of approximation with cubic splines. This is a modiÔ¨Åed version
of Theorem 3.13 in Burden and Faires [6], or of Theorem 4.7 in Epperson [9].
Theorem 6.3:
If f (x) ‚ààC4[a, b] with maxx‚àà[a,b] |f (4)(x)| ‚â§M, and p3(x)
is the unique complete spline interpolant for f (x), then
max
x‚àà[a,b] |f (x) ‚àíp3(x)| ‚â§
5
384Mh4.
Proof
The proof is omitted, but Epperson [9] suggests referring to the article
by Hall [10].
We conclude this section with an illustration of the quality of approximation of the
cubic splines.
Example 6.5
Figure 6.3 shows the natural and complete cubic spline inter-
polants to the function f (x) = exp(‚àíx2) for x ‚àà[‚àí1, 1]. We have chosen the
nodes xk = ‚àí1 + 1
2k, for k = 0, 1, 2, 3, 4 (i.e., n = 4, and h = 1
2). Clearly,
f (1)(x) = ‚àí2x exp(‚àíx2). Thus, at the nodes
f (¬±1) = 0.36787944,
f (¬± 1
2) = 0.60653066,
f (0) = 1.00000000,
and
f (1)(¬±1) = ‚àì0.73575888.
TLFeBOOK

SPLINE INTERPOLATION
283
‚àí1.5
‚àí1
‚àí0.5
0
0.5
1
1.5
0.2
0.4
0.6
0.8
1
x
(a)
(b)
‚àí1.5
‚àí1
‚àí0.5
0
0.5
1
1.5
x
y
y
0.2
0.4
0.6
0.8
1
Figure 6.3
Natural (a) and complete (b) spline interpolants (dashed lines) for the function
f (x) = e‚àíx2 (solid lines) on the interval [‚àí1, 1]. The circles correspond to node locations.
Here n = 4, with h = 1
2, and x0 = ‚àí1. The nodes are at xk = x0 + hk for k = 0, . . . , n.
Of course, the spline series for our example has the form
p3(x) =
5

k=‚àí1
akSk(x),
so we need to determine the coefÔ¨Åcients ak from both (6.87), and (6.93).
Considering the natural spline interpolant Ô¨Årst, from (6.87), we have
Ô£Æ
Ô£∞
4
1
0
1
4
1
0
1
4
Ô£π
Ô£ª
Ô£Æ
Ô£∞
a1
a2
a3
Ô£π
Ô£ª=
Ô£Æ
Ô£ØÔ£∞
f (x1) ‚àí1
6f (x0)
f (x2)
f (x3) ‚àí1
6f (x4)
Ô£π
Ô£∫Ô£ª.
Additionally
a0 = 1
6f (x0) = 1
6f (‚àí1),
a4 = 1
6f (x4) = 1
6f (1),
and
a‚àí1 = f (x0) ‚àí4a0 ‚àía1,
a5 = f (x4) ‚àí4a4 ‚àía3.
TLFeBOOK

284
INTERPOLATION
The natural spline series coefÔ¨Åcients are therefore
k
ak
‚àí1
‚àí0.01094139
0
0.06131324
1
0.13356787
2
0.18321607
3
0.13356787
4
0.06131324
5
‚àí0.01094139
Now, on considering the case of the complete spline, from (6.93) we have
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£∞
4
2
0
0
0
1
4
1
0
0
0
1
4
1
0
0
0
1
4
1
0
0
0
2
4
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£∞
a0
a1
a2
a3
a4
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
=
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
f (x0) + 1
3hf (1)(x0)
f (x1)
f (x2)
f (x3)
f (x4) ‚àí1
3hf (1)(x4)
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
Additionally
a‚àí1 = a1 ‚àí1
3hf (1)(x0),
a5 = a3 + 1
3hf (1)(x4).
The complete spline series coefÔ¨Åcients are therefore
k
ak
‚àí1
0.01276495
0
0.05493076
1
0.13539143
2
0.18230428
3
0.13539143
4
0.05493076
5
0.01276495
This example demonstrates what was suggested earlier, and that is that the com-
plete spline interpolant tends to be more accurate than the natural spline interpolant.
However, accuracy of the complete spline interpolant is contingent on accurate
estimation, or knowledge of the derivatives f (1)(x0) and f (1)(xn).
REFERENCES
1. D. R. Hill, Experiments in Computational Matrix Algebra (C. B. Moler, consulting ed.),
Random House, New York, 1988.
2. G. H. Golub and C. F. Van Loan, Matrix Computations, 2nd ed., Johns Hopkins Univ.
Press, Baltimore, MD, 1989.
TLFeBOOK

PROBLEMS
285
3. F. B. Hildebrand, Introduction to Numerical Analysis, 2nd ed., McGraw-Hill, New York,
1974.
4. G. E. Forsythe, M. A. Malcolm, and C. B. Moler, Computer Methods for Mathematical
Computations, Prentice-Hall, Englewood Cliffs, NJ, 1977.
5. E. Isaacson and H. B. Keller, Analysis of Numerical Methods, Wiley, New York,
1966.
6. R. L. Burden and J. D. Faires, Numerical Analysis, 4th ed., PWS-KENT Publi., Boston,
MA, 1989.
7. C. K. Chui, An Introduction to Wavelets, Academic Press, Boston, MA, 1992.
8. M. Unser and T. Blu, ‚ÄúWavelet Theory DemystiÔ¨Åed,‚Äù IEEE Trans. Signal Process. 51,
470‚Äì483 (Feb. 2003).
9. J. F. Epperson, An Introduction to Numerical Methods and Analysis, Wiley, New York,
2002.
10. C. A. Hall, ‚ÄúOn Error Bounds for Spline Interpolation,‚Äù J. Approx. Theory. 1, 209‚Äì218
(1968).
PROBLEMS
6.1. Find
the
Lagrange
interpolant
p2(t) = 2
j=0 xjLj(t)
for
data
set
{(‚àí1, 1
2), (0, 1), (1, ‚àí1)}. Find p2,j in p2(t) = 2
j=0 p2,jtj.
6.2. Find
the
Lagrange
interpolant
p3(t) = 3
j=0 xjLj(t)
for
data
set
{(0, 1), ( 1
2, 2), (1, 3
2), (2, ‚àí1)}. Find p3,j in p3(t) = 3
j=0 p3,jtj.
6.3. We want to interpolate x(t) at t = t1, t2, and x(1)(t) = dx(t)/dt at t = t0, t3
using p3(t) = 3
j=0 p3,jtj. Let xj = x(tj) and x(1)
j
= x(1)(tj). Find the lin-
ear system of equations satisÔ¨Åed by (p3,j).
6.4. Find a general expression for L(1)
j (t) = dLj(t)/dt.
6.5. In Section 6.2 it was mentioned that fast algorithms exist to solve Vander-
monde linear systems of equations. The Vandermonde system Ap = x is
given in expanded form as
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
1
t0
t2
0
¬∑ ¬∑ ¬∑
tn‚àí1
0
tn
0
1
t1
t2
1
¬∑ ¬∑ ¬∑
tn‚àí1
1
tn
1
...
...
...
...
...
1
tn‚àí1
t2
n‚àí1
¬∑ ¬∑ ¬∑
tn‚àí1
n‚àí1
tn
n‚àí1
1
tn
t2
n
¬∑ ¬∑ ¬∑
tn‚àí1
n
tn
n
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
p0
p1
...
pn‚àí1
pn
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
=
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
x0
x1
...
xn‚àí1
xn
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
, (6.P.1)
and pn(t) = n
j=0 pjtj interpolates the points {(tj, xj)|k = 0, 1, 2, . . . , n}.
A fast algorithm to solve (6.P.1) is
TLFeBOOK

286
INTERPOLATION
for k := 0 to n ‚àí1 do begin
for i := n downto to k + 1 do begin
xi := (xi ‚àíxi‚àí1)/(ti ‚àíti‚àík‚àí1);
end;
end;
for k := n ‚àí1 downto 0 do begin
for i := k to n ‚àí1 do begin
xi := xi ‚àítkxi+1;
end;
end;
The algorithm overwrites vector x = [x0 x1 ¬∑ ¬∑ ¬∑ xn]T with the vector p =
[p0 p1 ¬∑ ¬∑ ¬∑ pn]T .
(a) Count the number of arithmetic operations needed by the fast algorithm.
What is the asymptotic time complexity of it, and how does this compare
with Gaussian elimination as a method to solve (6.P.1)?
(b) Test the fast algorithm out on the system
Ô£Æ
Ô£ØÔ£ØÔ£∞
1
1
1
1
1
2
4
8
1
3
9
27
1
4
16
64
Ô£π
Ô£∫Ô£∫Ô£ª
Ô£Æ
Ô£ØÔ£ØÔ£∞
p0
p1
p2
p3
Ô£π
Ô£∫Ô£∫Ô£ª=
Ô£Æ
Ô£ØÔ£ØÔ£∞
10
26
58
112
Ô£π
Ô£∫Ô£∫Ô£ª.
(c) The ‚Äútop‚Äù k loop in the fast algorithm produces the Newton form (Section
6.3) of the representation for pn(t). For the system in (b), conÔ¨Årm that
pn(t) =
n

k=0
xk
k‚àí1
/
i=0
(t ‚àíti),
where {xk|k ‚ààZn+1} are the outputs from the top k loop. Since in (b)
n = 3, we must have for this particular special case
p3(t) = x0 + x1(t ‚àít0) + x2(t ‚àít0)(t ‚àít1) + x3(t ‚àít0)(t ‚àít1)(t ‚àít2).
(Comment: It has been noted by Bj¬®orck and Pereyra that the fast algorithm
often yields accurate results even when A is ill-conditioned.)
6.6. Prove that for
An =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
1
t0
t2
0
¬∑ ¬∑ ¬∑
tn‚àí1
0
tn
0
1
t1
t2
1
¬∑ ¬∑ ¬∑
tn‚àí1
1
tn
1
...
...
...
...
...
1
tn‚àí1
t2
n‚àí1
¬∑ ¬∑ ¬∑
tn‚àí1
n‚àí1
tn
n‚àí1
1
tn
t2
n
¬∑ ¬∑ ¬∑
tn‚àí1
n
tn
n
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
,
TLFeBOOK

PROBLEMS
287
we have
det(An) =
/
0‚â§i<j‚â§n
(ti ‚àítj).
(Hint: Use mathematical induction.)
6.7. Write a MATLAB function to interpolate the data {(tj, xj)|j ‚ààZn+1} with
polynomial pn(t) via Lagrange interpolation. The function must accept t as
input, and return pn(t).
6.8. Runge‚Äôs phenomenon was mentioned in Section 6.2 with respect to interpo-
lating f (t) = 1/(1 + t2) on t ‚àà[‚àí5, 5]. Use polynomial pn(t) to interpolate
f (t) at the points tk = t0 + kh, k = 0, 1, . . . , n, where t0 = ‚àí5, and tn = 5
(so h = (tn ‚àít0)/n). Do this for n = 5, 8, 10. Use the MATLAB function
from the previous problem. Plot f (t) and pn(t) on the same graph for all of
n = 5, 8, 10. Comment on the accuracy of interpolation as n increases.
6.9. Suppose that we wish to interpolate f (t) = sin t for t ‚àà[0, œÄ/2] using poly-
nomial pn(t) = n
j=0 pn,jtj. The approximation error is en(t) = f (t) ‚àí
pn(t). Since |f (n)(t)| ‚â§1 for all t ‚ààR via (6.14), it follows that
|en(t)| ‚â§
1
(n + 1)!
n
/
i=0
|t ‚àíti| = b(t).
(6.P.2)
Let the grid (sample) points be t0 = 0, tn = œÄ/2, and tk = t0 + kh for k =
0, 1, . . . , n so that h = (tn ‚àít0)/n = œÄ
2n. Using MATLAB
(a) For n = 2, on the same graph plot f (t) ‚àípn(t), and ¬±b(t).
(b) For n = 4, on the same graph plot f (t) ‚àípn(t), and ¬±b(t).
Cases (a) and (b) can be separate plots (e.g., using MATLAB subplot). Does
the bound in (6.P.2) hold in both cases?
6.10. Consider f (t) = cosh t = 1
2[et + e‚àít] which is to be interpolated by p2(t) =
2
j=0 p2,jtj on t ‚àà[‚àí1, 1] at the points t0 = ‚àí1, t0 = 0, t1 = 1. Use (6.14)
to Ô¨Ånd an upper bound on the size of the approximation error en(t) for
t ‚àà[‚àí1, 1], where the size of the approximation error is given by the norm
||en||‚àû= max
a‚â§t‚â§b |en(t)|,
where here a = ‚àí1, b = 1.
6.11. For
V =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
1
1
¬∑ ¬∑ ¬∑
1
1
t0
t1
¬∑ ¬∑ ¬∑
tn‚àí1
tn
...
...
...
...
tn
0
tn
1
¬∑ ¬∑ ¬∑
tn
n‚àí1
tn
n
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª‚ààR(n+1)√ó(n+1),
TLFeBOOK

288
INTERPOLATION
it is claimed by Gautschi that
||V ‚àí1||‚àû‚â§max
0‚â§k‚â§n
n
/
i=0
iÃ∏=k
1 + |ti|
|tk ‚àíti|.
(6.P.3)
Find an upper bound on Œ∫‚àû(V ) that uses (6.P.3). How useful is an upper
bound on the condition number? Would a lower bound on condition number
be more useful? Explain.
6.12. For each function listed below, use divided difference tables (recall Example
6.3) to construct the degree n Newton interpolating polynomial for the spec-
iÔ¨Åed points.
(a) f (t) = ‚àöt,
t0 = 0, t1 = 1, t2 = 3. Use n = 2.
(b) f (t) = cosh t,
t0 = ‚àí1, t1 = ‚àí1
2 , t2 = 1
2, t3 = 1. Use n = 3.
(c) f (t) = ln t,
t0 = 1, t1 = 2, t2 = 3. Use n = 2.
(d) f (t) = 1/(1 + t),
t0 = 0, t1 = 1
2, t2 = 1, t3 = 2. Use n = 3.
6.13. Prove Eq. (6.33).
6.14. Consider Theorem 6.1. For n = 2, Ô¨Ånd hk(x) and ÀÜhk(x) as direct-form (i.e.,
a form such as (6.4)) polynomials in x. Of course, do this for all k = 0, 1, 2.
Find Hermite interpolation polynomial p2n+1(x) for n = 2 that interpolates
f (x) = ‚àöx at the points x0 = 1, x1 = 3/2, x2 = 2.
6.15. Find the Hermite interpolating polynomial p5(x) to interpolate f (x) = ex at
the points x0 = 0, x1 = 0.1, x2 = 0.2. Use MATLAB to compare the accu-
racy of approximation of p5(x) to that of pa
2(x) given in Example 6.3 (or
Example 6.4).
6.16. The following matrix is important in solving spline interpolation problems
[recall (6.87)]:
An =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
4
1
0
¬∑ ¬∑ ¬∑
0
0
1
4
1
¬∑ ¬∑ ¬∑
0
0
0
1
4
¬∑ ¬∑ ¬∑
0
0
...
...
...
...
...
0
0
0
¬∑ ¬∑ ¬∑
4
1
0
0
0
¬∑ ¬∑ ¬∑
1
4
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
‚ààRn√ón.
Suppose that Dn = det(An).
(a) Find D1, D2, D3, and D4 by direct (hand) calculation using the basic
rules for computing determinants.
(b) Show that
Dn+2 ‚àí4Dn+1 + Dn = 0
TLFeBOOK

PROBLEMS
289
(i.e., the determinants we seek may be generated by a second-order dif-
ference equation).
(c) For suitable constants Œ±, Œ≤ ‚ààR, it can be shown that
Dn = Œ±(2 +
‚àö
3)n + Œ≤(2 ‚àí
‚àö
3)n
(6.P.4)
(n ‚ààN). Find Œ± and Œ≤.
[Hint: Set up two linear equations in the two unknowns Œ± and Œ≤ using
(6.P.4) for n = 1, 2 and the results from (a).]
(d) Prove that Dn > 0 for all n ‚ààN.
(e) Is An > 0 for all n ‚ààN? Justify your answer.
[Hint: Recall part (a) in Problem 4.13 (of Chapter 4).]
6.17. Repeat Example 6.5, except use
f (x) =
1
1 + x2 ,
and x ‚àà[‚àí5, 5] with nodes xk = ‚àí5 + 5
2k for k = 0, 1, 2, 3, 4 (i.e., n = 4,
and h = 5
2). How do the results compare to the results in Problem 6.8 (assum-
ing that you have done Problem 6.8)? Of course, you should use MATLAB
to ‚Äúdo the dirty work.‚Äù You may use built-in MATLAB linear system solvers.
6.18. Repeat Example 6.5, except use
f (x) =
‚àö
1 + x.
Use MATLAB to aid in the task.
6.19. Write a MATLAB function to solve tridiagonal linear systems of equations
based on the theory for doing so given in Section 6.5. Test your algorithm
out on the linear systems given in Example 6.5.
TLFeBOOK

7
Nonlinear Systems of Equations
7.1
INTRODUCTION
In this chapter we consider the problem of Ô¨Ånding x to satisfy the equation
f (x) = 0
(7.1)
for arbitrary f , but such that f (x) ‚ààR. Usually we will restrict our discussion
to the case where x ‚ààR, but x ‚ààC can also be of signiÔ¨Åcant interest in practice
(especially if f is a polynomial). However, any x satisfying (7.1) is called a root
of the equation. Such an x is also called a zero of the function f . More generally,
we are also interested in solutions to systems of equations:
f0(x0, x1, . . . , xn‚àí1) = 0,
f1(x0, x1, . . . , xn‚àí1) = 0,
...
fn‚àí1(x0, x1, . . . , xn‚àí1) = 0.
(7.2)
Again, fi(x0, x1, . . . , xn‚àí1) ‚ààR, and we will assume xk ‚ààR for all i, k ‚ààZn. Var-
ious solution methods will be considered such the bisection method, Ô¨Åxed-point
method, and the Newton‚ÄìRaphson method. All of these methods are iterative in
that they generate a sequence of points (or more generally vectors) that converge
(we hope) to the desired solution. Consequently, ideas from Chapter 3 are rele-
vant here. The number of iterations needed to achieve a solution with a given
accuracy is considered. Iterative procedures can break down (i.e., fail) in various
ways. The sequence generated by a procedure may diverge, oscillate, or display
chaotic behavior (which in the past was sometimes described as ‚Äúwandering behav-
ior‚Äù). Examples of breakdown phenomena will therefore also be considered. Some
attention will be given to chaotic phenomena, as these are of growing engineering
interest. Applications of chaos are now being considered for such areas as cryp-
tography and spread-spectrum digital communications. These proposed applications
are still controversial, but at the very least some knowledge of chaos gives a deeper
insight into the behavior of nonlinear dynamic systems.
An Introduction to Numerical Analysis for Electrical and Computer Engineers, by C.J. Zarowski
ISBN 0-471-46737-5
c‚Éù2004 John Wiley & Sons, Inc.
290
TLFeBOOK

INTRODUCTION
291
The equations considered in this chapter are nonlinear, and so are in contrast
with those of Chapter 4. Chapter 4 considered linear systems of equations only. The
reader is probably aware of the fact that a linear system either has a unique solution,
no solution, or an inÔ¨Ånite number of solutions. Which case applies depends on the
size and rank of the matrix in the linear system. Chapter 4 emphasized the handling
of square and invertible matrices for which the solution exists, and is unique. In
Chapter 4 we saw that well-deÔ¨Åned procedures exist (e.g., Gaussian elimination)
that give the solution in a Ô¨Ånite number of steps.
The solution of nonlinear equations is signiÔ¨Åcantly more complicated than the
solution of linear systems. Existence and uniqueness problems typically have no
easy answers. For example, e2x + 1 = 0 has no real-valued solutions, but if we allow
x ‚ààC, then x = 1
2kœÄj for which k is an odd integer (since e2x + 1 = ejœÄk + 1 =
cos(kœÄ) + j sin(kœÄ) + 1 = cos(kœÄ) = ‚àí1 + 1 = 0 as k is odd). On the other hand
e‚àít ‚àísin(t) = 0
also has an inÔ¨Ånite number of solutions, but for which t ‚ààR (see Fig. 7.1). How-
ever, the solutions are not speciÔ¨Åable with a nice formula. In Fig. 7.1 the solutions
correspond to the point where the two curves intersect each other.
Polynomial equations are of special interest.1 For example, x2 + 1 = 0 has only
complex solutions x = ¬±j. Multiple roots are also possible. For example
x3 ‚àí3x2 + 3x ‚àí1 = (x ‚àí1)3 = 0
has a real root of multiplicity 3 at x = 1. We remark that the methods of this
chapter are general and so (in principle) can be applied to Ô¨Ånd the solution of
any nonlinear equation, polynomial or otherwise. But the special importance of
polynomial equations has caused the development (over the centuries) of algorithms
dedicated to polynomial equation solution. Thus, special algorithms exist to solve
pn(x) =
n

k=0
pn,kxk = 0,
(7.3)
1Why are polynomial equations of special interest? There are numerous answers to this. But the reader
already knows one reason why from basic electric circuits. For example, an unforced RLC (resistance √ó
inductance √ó capacitance) circuit has a response due to energy initially stored in the energy storage
elements (the inductors and capacitors). If x(t) is the voltage drop across an element or the current
through an element, then
an
dnx(t)
dtn
+ an‚àí1
dn‚àí1x(t)
dtn‚àí1
+ ¬∑ ¬∑ ¬∑ + a1
dx(t)
dt
+ a0 = 0.
The coefÔ¨Åcients ak depend on the circuit elements. The solution to the differential equation depends on
the roots of the characteristic equation:
anŒªn + an‚àí1Œªn‚àí1 + ¬∑ ¬∑ ¬∑ + a1Œª + a0 = 0.
TLFeBOOK

292
NONLINEAR SYSTEMS OF EQUATIONS
‚àí1
0
1
2
3
4
5
6
7
8
‚àí1
‚àí0.5
0
0.5
1
1.5
2
2.5
t
e‚àít
sin(t)
Figure 7.1
Plot of the individual terms in the equation f (t) = e‚àít ‚àísin(t) = 0. The inÔ¨Å-
nite number of solutions possible corresponds to the point where the plotted curves intersect.
and these algorithms do not apply to general nonlinear equations. Sometimes
these are based on the general methods we consider in this chapter. Other times
completely different methods are employed (e.g., replacing the problem of Ô¨Ånd-
ing polynomial zeros by the equivalent problem of Ô¨Ånding matrix eigenvalues as
suggested in Jenkins and Traub [22]). However, we do not consider these special
polynomial equation solvers here. We only mention that some interesting references
on this matter are Wilkinson [1] and Cohen [2]. These describe the concept of ill-
conditioned polynomials, and how to apply deÔ¨Çation procedures to produce more
accurate estimates of roots. The difÔ¨Åculties posed by multiple roots are considered
in Hull and Mathon [3]. Modern math-oriented software tools (e.g., MATLAB)
often take advantage of theories such as described in Refs. 1‚Äì3, (as well as in
other sources). In MATLAB polynomial zeros may be found using the roots and
mroots functions. Function mroots is a modern root Ô¨Ånder that reliably determines
multiple roots.
7.2
BISECTION METHOD
The bisection method is a simple intuitive approach to solving
f (x) = 0.
(7.4)
It is assumed that f (x) ‚ààR, and that x ‚ààR. This method is based on the following
theorem.
Theorem 7.1: Intermediate Value Theorem
If f |[a, b] ‚ÜíR is continuous
on the closed, bounded interval [a, b], and y0 ‚ààR is such that f (a) ‚â§y0 ‚â§f (b),
then there is an x0 ‚àà[a, b] such that f (x0) = y0. In other words, a continuous
function on a closed and bounded interval takes on all values between f (a) and
f (b) at least once.
TLFeBOOK

BISECTION METHOD
293
The bisection method works as follows. Suppose that we have an initial interval
[a0, b0] such that
f (a0)f (b0) < 0,
(7.5)
which means that f (a0) and f (b0) have opposite signs (i.e., one is positive while
the other is negative). By Theorem 7.1 there must be a p ‚àà(a0, b0) so that f (p) =
0. We say that [a0, b0] brackets the root p. Suppose that
p0 = 1
2(a0 + b0).
(7.6)
This is the midpoint of the interval [a0, b0]. Consider the following cases:
1. If f (p0) = 0, then p = p0 and we have found a root. We may stop at this
point.
2. If f (a0)f (p0) < 0, then it must be the case that p ‚àà[a0, p0], so we deÔ¨Åne
the new interval [a1, b1] = [a0, p0]. This new interval brackets the root.
3. If f (p0)f (b0) < 0 then it must be the case that p ‚àà[p0, b0] so we deÔ¨Åne
the new interval [a1, b1] = [p0, b0]. This new interval brackets the root.
The process is repeated by considering the midpoint of the new interval, which is
p1 = 1
2(a1 + b1),
(7.7)
and considering the three cases again. In principle, the process terminates when
case 1 is encountered. In practice, case 1 is unlikely in part because of the effects
of rounding errors, and so we need a more practical criterion to stop the process.
This will be considered a little later on. For now, pseudocode describing the basic
algorithm may be stated as follows:
input [a0, b0] which brackets the root p;
p0 := (a0 + b0)/2;
k := 0;
while stopping criterion is not met do begin
if f(ak)f(pk) < 0 then begin
ak+1 := ak;
bk+1 := pk;
end;
else begin
ak+1 := pk;
bk+1 := bk;
end;
pk+1 := (ak+1 + bk+1)/2;
k := k + 1;
end;
When the algorithm terminates, the last value of pk+1 computed is an estimate
of p.
TLFeBOOK

294
NONLINEAR SYSTEMS OF EQUATIONS
We see that the bisection algorithm constructs sequence (pn) = (p0, p1, p2, . . .)
such that
lim
n‚Üí‚àûpn = p,
(7.8)
where pn is the midpoint of [an, bn], and f (p) = 0. Formal proof that this process
works (i.e., yields a unique p such that f (p) = 0) is due to the following theorem.
Theorem 7.2: Cantor‚Äôs Intersection Theorem
Suppose that ([ak, bk]) is a
sequence of closed and bounded intervals such that
[a0, b0] ‚äÉ[a1, b1] ‚äÉ¬∑ ¬∑ ¬∑ ‚äÉ[an, bn] ‚äÉ¬∑ ¬∑ ¬∑ ,
with limn‚Üí‚àû(bn ‚àían) = 0. There is a unique point p ‚àà[an, bn] for all n ‚ààZ+:
‚àû
@
n=0
[an, bn] = {p}.
The bisection method produces (pn) such that an < pn < bn, and p ‚àà[an, bn] for
all n ‚ààZ+. Consequently, since pn = 1
2(an + bn)
|pn ‚àíp| ‚â§|bn ‚àían| ‚â§b ‚àía
2n
(7.9)
for n ‚ààZ+, so that limn‚Üí‚àûpn = p. Recall that f is assumed continuous on [a, b],
so limn‚Üí‚àûf (pn) = f (p). So now observe that
|pn ‚àían| ‚â§1
2n |b ‚àía|, |bn ‚àípn| ‚â§1
2n |b ‚àía|
(7.10)
so, via |x ‚àíy| = |(x ‚àíz) ‚àí(y ‚àíz)| ‚â§|x ‚àíz| + |y ‚àíz| (triangle inequality), we
have
|p ‚àían| ‚â§|p ‚àípn| + |pn ‚àían| ‚â§1
2n (b ‚àía) + 1
2n (b ‚àía) =
1
2n‚àí1 (b ‚àía),
(7.11a)
and similarly
|p ‚àíbn| ‚â§|p ‚àípn| + |pn ‚àíbn| ‚â§
1
2n‚àí1 (b ‚àía).
(7.11b)
Thus
lim
n‚Üí‚àûan = lim
n‚Üí‚àûbn = p.
(7.12)
At each step the root p is bracketed. This implies that there is a subsequence [of (pn)]
denoted (xn) converging to p so that f (xn) > 0 for all n ‚ààZ+. Similarly, there is a
subsequence (yn) converging to p such that f (yn) < 0 for all n ‚ààZ+. Thus
f (p) = lim
n‚Üí‚àûf (xn) ‚â•0, f (p) = lim
n‚Üí‚àûf (yn) ‚â§0,
TLFeBOOK

BISECTION METHOD
295
which implies that f (p) = 0. We must conclude that the bisection method produces
sequence (pn) converging to p such that f (p) = 0. Hence, the bisection method
always works.
Example 7.1
We want to Ô¨Ånd 51/3 (cube root of Ô¨Åve). This is equivalent to
solving the equation f (x) = x3 ‚àí5 = 0. We note that f (1) = ‚àí4 and f (2) = 3,
so we may use [a0, b0] = [a, b] = [1, 2] to initially bracket the root. We remark
that the ‚Äúexact‚Äù value is 51/3 = 1.709976 (seven signiÔ¨Åcant Ô¨Ågures). Consider the
following iterations of the bisection method:
[a0, b0] = [1, 2],
p0 = 1.500000,
f (p0) = ‚àí1.625000
[a1, b1] = [p0, b0],
p1 = 1.750000,
f (p1) = 0.359375
[a2, b2] = [a1, p1],
p2 = 1.625000,
f (p2) = ‚àí0.708984
[a3, b3] = [p2, b2],
p3 = 1.687500,
f (p3) = ‚àí0.194580
[a4, b4] = [p3, b3],
p4 = 1.718750,
f (p4) = 0.077362
[a5, b5] = [a4, p4],
p5 = 1.703125,
f (p5) = ‚àí0.059856
We see that |p5 ‚àíp| = |1.703125 ‚àí1.709976| = 0.006851. From (7.9)
|p5 ‚àíp| ‚â§b ‚àía
25
= 2 ‚àí1
25
= 1
25 = 0.0312500.
The exact error certainly agrees with this bound.
When should we stop iterating? In other words, what stopping criterion should
be chosen? Some possibilities are (for œµ > 0)
 1
2(bn ‚àían)
 < œµ,
(7.13a)
|pn ‚àíp| < œµ,
(7.13b)
f (pn) < œµ,
(7.13c)

pn ‚àípn‚àí1
pn
 < œµ
(pn Ã∏= 0).
(7.13d)
We would stop iterating when the inequalities are satisÔ¨Åed. Usually (7.13d) is
recommended. Condition (7.13a) is not so good as termination depends on the size
of the nth interval, while it is the accuracy of the estimate of p that is of most
interest. Condition (7.13b) requires knowing p in advance, which is not reasonable
since it is p that we are trying to determine. Condition (7.13c) is based on f (pn),
and again we are more interested in how well pn approximates p. Thus, we are left
with (7.13d). This condition leads to termination when pn is relatively not much
different from pn‚àí1.
TLFeBOOK

296
NONLINEAR SYSTEMS OF EQUATIONS
How may we characterize the computational efÔ¨Åciency of an iterative algorithm?
In Chapter 4 the algorithms terminated in a Ô¨Ånite number of steps (with the excep-
tion of the iterative procedures suggested in Section 4.7 which do not terminate
unless a stopping condition is imposed), so Ô¨Çop counting was a reasonable mea-
sure. Where iterative procedures such as the bisection method are concerned, we
prefer
DeÔ¨Ånition 7.1: Rate of Convergence
Suppose that (xn) converges to 0:
limn‚Üí‚àûxn = 0. Suppose that (pn) converges to p, i.e., limn‚Üí‚àûpn = p. If there
is a K ‚ààR, but K > 0, and N ‚ààZ+ such that
|pn ‚àíp| ‚â§K|xn|
for all n ‚â•N, then we say that (pn) converges to p with rate of convergence O(xn).
This is an alternative use of the ‚Äúbig O‚Äù notation that was Ô¨Årst seen in Chapter 4.
Recalling (7.9), we obtain
|pn ‚àíp| ‚â§(b ‚àía) 1
2n ,
(7.14)
so that K = b ‚àía, xn = 1
2n , and N = 0. Thus, the bisection method generates
sequence (pn) that converges to p (with f (p) = 0) at the rate O(1/2n).
From (7.14), if we want |pn ‚àíp| < œµ, then we may choose n so that
|pn ‚àíp| ‚â§b ‚àía
2n
< œµ,
implying 2n > (b ‚àía)/œµ, or we may choose n so
n =
A
log2
 b ‚àía
œµ
!B
,
(7.15)
where ‚åàx‚åâ= smallest integer greater than or equal to x. This can be used as an
alternative means to terminate the bisection algorithm. But the conservative nature
of (7.15) suggests that the algorithm that employs it may compute more iterations
than are really necessary for the desired accuracy.
7.3
FIXED-POINT METHOD
Here we consider the Banach Ô¨Åxed-point theorem [4] as the theoretical basis for a
nonlinear equation solver. Suppose that X is a set, and T |X ‚ÜíX is a mapping of
X into itself. A Ô¨Åxed point of T is an x ‚ààX such that
T x = x.
(7.16)
TLFeBOOK

FIXED-POINT METHOD
297
For example, suppose X = [0, 1] ‚äÇR, and
T x = 1
2x(1 ‚àíx).
(7.17)
We certainly have T x ‚àà[0, 1] for any x ‚ààX. The solution to
x = 1
2x(1 ‚àíx)
(i.e., to T x = x) is x = 0. So T has Ô¨Åxed point x = 0. (We reject the solution
x = ‚àí1 since ‚àí1 Ã∏‚àà[0, 1].)
DeÔ¨Ånition 7.2: Contraction
Let X = (X, d) be a metric space. Mapping
T |X ‚ÜíX is called a contraction (or a contraction mapping, or a contractive
mapping) on X if there is an Œ± ‚ààR such that 0 < Œ± < 1, and for all x, y ‚ààX
d(T x, T y) ‚â§Œ±d(x, y).
(7.18)
Applying T to ‚Äúpoints‚Äù x and y brings them closer together if T is a contractive
mapping. If Œ± = 1, the mapping is sometimes called nonexpansive.
Theorem 7.3: Banach Fixed-Point Theorem
Consider the metric space X =
(X, d), where X Ã∏‚àà‚àÖ. Suppose that X is complete, and T |X ‚ÜíX is a contraction
on X. Then T has a unique Ô¨Åxed point.
Proof
We must construct (xn), and show that it is Cauchy so that (xn) con-
verges in X (recall Section 3.2). Then we prove that x is the only Ô¨Åxed point of
T . Suppose that x0 ‚ààX is any point from X. Consider the sequence produced by
the repeated application of T to x0:
x0, x1 = T x0, x2 = T x1 = T 2x0, . . . , xn = T xn‚àí1 = T nx0, . . . .
(7.19)
From (7.19) and (7.18), we obtain
d(xk+1, xk) = d(T xk, T xk‚àí1)
‚â§Œ±d(xk, xk‚àí1)
= Œ±d(T xk‚àí1, T xk‚àí2)
‚â§Œ±2d(xk‚àí1, xk‚àí2)
...
‚â§Œ±kd(x1, x0).
(7.20)
TLFeBOOK

298
NONLINEAR SYSTEMS OF EQUATIONS
Using the triangle inequality with n > k, from (7.20)
d(xk, xn) ‚â§d(xk, xk+1) + d(xk+1, xk+2) + ¬∑ ¬∑ ¬∑ + d(xn‚àí1, xn)
‚â§(Œ±k + Œ±k+1 ¬∑ ¬∑ ¬∑ + Œ±n‚àí1)d(x0, x1)
= Œ±k 1 ‚àíŒ±n‚àík
1 ‚àíŒ±
d(x0, x1)
(where the last equality follows by application of the formula for geometric series,
seen very frequently in previous chapters). Since 0 < Œ± < 1, we have 1 ‚àíŒ±n‚àík < 1.
Thus
d(xk, xn) ‚â§
Œ±k
1 ‚àíŒ± d(x0, x1)
(7.21)
(n > k). Since 0 < Œ± < 1, 0 < 1 ‚àíŒ± < 1, too. In addition, d(x0, x1) is Ô¨Åxed (since
we have chosen x0). We may make the right-hand side of (7.21) arbitrarily small
by making k sufÔ¨Åciently big (keeping n > k). Consequently, (xn) is Cauchy. X is
assumed to be complete, so xn ‚Üíx ‚ààX.
From the triangle inequality and (7.18)
d(T x, x) ‚â§d(x, xk) + d(xk, T x)
‚â§d(x, xk) + Œ±d(xk‚àí1, x)
(recall that T x = x)
< œµ
(any œµ > 0)
if k ‚Üí‚àû, since xn ‚Üíx. Consequently, d(x, T x) = 0 implies that T x = x (recall
(M1) in DeÔ¨Ånition 1.1). Immediately, x is a Ô¨Åxed point of T .
Point x is unique. Let us assume that there is another Ô¨Åxed point ÀÜx, i.e., T ÀÜx = ÀÜx.
But from (7.18)
d(x, ÀÜx) = d(T x, T ÀÜx) ‚â§Œ±d(x, ÀÜx),
implying d(x, ÀÜx) = 0 because 0 < Œ± < 1. Thus x = ÀÜx [(M1) From DeÔ¨Ånition 1.1
again].
Theorem 7.3 is also called the contraction theorem, a special instance of which
was seen in Section 4.7. The theorem applies to complete metric spaces, and so is
applicable to Banach and Hilbert spaces. (Recall that inner products induce norms,
and norms induce metrics. Hilbert and Banach spaces are complete. So they must
be complete metric spaces as well.)
Note that we deÔ¨Åne T 0x = x for any x ‚ààX. Thus T 0 is the identity mapping
(identity operator).
Corollary 7.1
Under the conditions of Theorem 7.3 sequence (xn) from xn =
T nx0 [i.e., the sequence in (7.19)] for any x0 ‚ààX converges to a unique x ‚ààX
such that T x = x. We have the following error estimates (i.e., bounds):
TLFeBOOK

FIXED-POINT METHOD
299
1. Prior estimate:
d(xk, x) ‚â§
Œ±k
1 ‚àíŒ± d(x0, x1)
(7.22a)
2. Posterior estimate:
d(xk, x) ‚â§
Œ±
1 ‚àíŒ± d(xk, xk‚àí1).
(7.22b)
Proof
The prior estimate is an immediate consequence of Theorem 7.3 since
it lies within the proof of this theorem [let n ‚Üí‚àûin (7.21)].
Now consider (7.22b). In (7.22a) let k = 1, y0 = x0, and y1 = x1. Thus,
d(x1, x) = d(y1, x), d(x0, x1) = d(y0, y1), and so
d(y1, x) ‚â§
Œ±
1 ‚àíŒ± d(y0, y1).
(7.23)
Let y0 = xk‚àí1, so y1 = T y0 = T xk‚àí1 = xk and (7.23) becomes
d(xk, x) ‚â§
Œ±
1 ‚àíŒ± d(xk, xk‚àí1),
which is (7.22b).
The error bounds in Corollary 7.1 are useful in application of the contraction the-
orem to computational problems. For example, (7.22a) can estimate the number of
iteration steps needed to achieve a given accuracy in the estimate of the solution to
a nonlinear equation. The result would be analogous to Eq. (7.15) in the previous
section.
A difÔ¨Åculty with the theory so far is that T |X ‚ÜíX is not always contractive
over the entire metric space X, but only on a subset, say, Y ‚äÇX. However, a basic
result from functional analysis is that any closed subset of X is complete. Thus, for
any closed Y ‚äÇX, there is a Ô¨Åxed point x ‚ààY ‚äÇX, and xn ‚Üíx (xn = T nx0 for
suitable x0 ‚ààY). For this idea to work, we must choose x0 ‚ààY so that xn ‚ààY for
all n ‚ààZ+.
What does ‚Äúclosed subset‚Äù (formally) mean? A neighborhood of x ‚ààX (metric
space X) is the set Nœµ(x) = {y|d(x, y) < œµ, œµ > 0} ‚äÇX. Parameter œµ is the radius
of the neighborhood. We say that x is a limit point of Y ‚äÇX if every neighborhood
of x contains y Ã∏= x such that y ‚ààY. Y is closed if every limit point of Y belongs
to Y. These deÔ¨Ånitions are taken from Rudin [5, p. 32].
Example 7.2
Suppose that X = R, Y = (0, 1) ‚äÇX. We recall that X is a
complete metric space if d(x, y) = |x ‚àíy| (x, y ‚ààX). Is Y closed? Limit points
of Y are x = 0 and x = 1. [Any x ‚àà(0, 1) is also a limit point of Y.] But 0 Ã∏‚ààY,
and 1 Ã∏‚ààY. Therefore, Y is not closed. On the other hand, Y = [0, 1] is a closed
subset of X since the limit points are now in Y.
TLFeBOOK

300
NONLINEAR SYSTEMS OF EQUATIONS
Fixed-point theorems (and related ideas) such as we have considered so far have
a large application range. They can be used to prove the existence and uniqueness
of solutions to both integral and differential equations [4, pp. 314‚Äì326]. They can
also provide (sometimes) computational algorithms for their solution. Fixed-point
results can have applications in signal reconstruction and image processing [6],
digital Ô¨Ålter design [7], the interpolation of bandlimited sequences [8], and the
solution to so-called convex feasibility problems in general [9]. However, we will
consider the application of Ô¨Åxed-point theorems only to the problem of solving
nonlinear equations.
If we wish to Ô¨Ånd p ‚ààR so that
f (p) = 0,
(7.24)
then we may deÔ¨Åne g(x) (g(x)|R ‚ÜíR) with Ô¨Åxed point p such that
g(x) = x ‚àíf (x),
(7.25)
and we then see g(p) = p ‚àíf (p) = p ‚áíf (p) = 0. Conversely, if there is a
function g(x) such that
g(p) = p,
(7.26)
then
f (x) = x ‚àíg(x)
(7.27)
will have a zero at x = p. So, if we wish to solve f (x) = 0, one approach would
be to Ô¨Ånd a suitable g(x) as in (7.27) (or (7.25)), and Ô¨Ånd Ô¨Åxed points for it.
Theorem 7.3 informs us about the existence and uniqueness of Ô¨Åxed points for
mappings on complete metric spaces (and ultimately on closed subsets of such
spaces). Furthermore, the theorem leads us to a well-deÔ¨Åned computational algo-
rithm to Ô¨Ånd Ô¨Åxed points. At the outset, space X = R with metric d(x, y) = |x ‚àíy|
is complete, and for us g and f are mappings on X. So if g and f are related
according to (7.27), then any Ô¨Åxed point of g will be a zero of f , and can be
found by iterating as spelled out in Theorem 7.3. However, the discussion follow-
ing Corollary 7.1 warned us that g may not be contractive on X = R but only on
some subset Y ‚äÇX. In fact, g is only rarely contractive on all of R. We therefore
usually need to Ô¨Ånd Y = [a, b] ‚äÇX = R such that g is contractive on Y, and then
we compute xn = gnx0 ‚ààY for n ‚ààZ+. Then xn ‚Üíx, and f (x) = 0. We again
emphasize that xn ‚ààY is necessary for all n. If g is contractive on [a, b] ‚äÇX,
then, from DeÔ¨Ånition 7.2, this means that for any x, y ‚àà[a, b]
|g(x) ‚àíg(y)| ‚â§Œ±|x ‚àíy|
(7.28)
for some real-valued Œ± such that 0 < Œ± < 1.
Example 7.3
Suppose that we want roots of f (x) = Œªx2 + (1 ‚àíŒª)x = 0
(assume Œª > 0). Of course, this is a quadratic equation and so may be easily
TLFeBOOK

FIXED-POINT METHOD
301
solved by the usual formula for the roots of such an equation (recall Section 4.2).
However, this example is an excellent illustration of the behavior of Ô¨Åxed-point
schemes. It is quite easy to verify that
f (x) = x ‚àíŒªx(1 ‚àíx)
#
$%
&
=g(x)
.
(7.29)
We observe that g(x) is a quadratic in x, and also
g(1)(x) = (2x ‚àí1)Œª,
so that g(x) has a maximum at x = 1
2 for which g( 1
2) = Œª/4. Therefore, if we
allow only 0 < Œª ‚â§4, then g(x) ‚àà[0, 1] for all x ‚àà[0, 1]. Certainly [0, 1] ‚äÇR. A
sketch of g(x) and of y = x appears in Fig. 7.2 for various Œª. The intersection of
these two curves locates the Ô¨Åxed points of g on [0, 1].
We will suppose Y = [0, 1]. Although g(x) ‚àà[0, 1] for all x ‚àà[0, 1] under the
stated conditions, g is not necessarily always contractive on the closed interval
[0, 1]. Suppose Œª = 1
2; then, for all x ‚àà[0, 1], g(x) ‚àà[0, 1
8] (see the dotted line
in Fig. 7.2, and we can calculate this from knowledge of g). The mapping is
contractive for this case on all of [0, 1]. [This is justiÔ¨Åed below with respect to
Eq. (7.30).] Also, g(x) = x only for x = 0. If we select any x0 ‚àà[0, 1], then, for
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
x
g(x)
y = x
l = 0.5
l = 2.0
l = 2.8
l = 4.0
Figure 7.2
Plot of g(x) = Œªx(1 ‚àíx) for various Œª, and a plot of y = x. The places where
y = x and g(x) intersect deÔ¨Åne the Ô¨Åxed points of g(x).
TLFeBOOK

302
NONLINEAR SYSTEMS OF EQUATIONS
xn = gnx0, we can expect xn ‚Üí0. For example, suppose x0 = 0.7500; then the
Ô¨Årst few iterates are
x0 = 0.7500
x1 = 0.5x0(1 ‚àíx0) = 0.0938
x2 = 0.5x1(1 ‚àíx1) = 0.0425
x3 = 0.5x2(1 ‚àíx2) = 0.0203
x4 = 0.5x3(1 ‚àíx3) = 0.0100
x5 = 0.5x4(1 ‚àíx4) = 0.0049
...
The process is converging to the unique Ô¨Åxed point at x = 0.
Suppose now that Œª = 2; then g(x) = 2x(1 ‚àíx), and now g(x) has two Ô¨Åxed
points on [0, 1], which are x = 0, and x = 1
2. For all x ‚àà[0, 1] we have g(x) ‚àà
[0, 1
2], but g is not contractive on [0, 1]. For example, suppose x = 0.8, y = 0.9,
then g(.8) = 0.3200, and g(.9) = 0.1800. Thus, |x ‚àíy| = 0.1, but |g(x) ‚àíg(y)| =
0.14 > 0.1. On the other hand, suppose that x0 = 0.7500; then the Ô¨Årst few iter-
ates are
x0 = 0.7500
x1 = 2x0(1 ‚àíx0) = 0.3750
x2 = 2x1(1 ‚àíx1) = 0.4688
x3 = 2x2(1 ‚àíx2) = 0.4980
x4 = 2x3(1 ‚àíx3) = 0.5000
x5 = 2x4(1 ‚àíx4) = 0.5000
...
This process converges to one of the Ô¨Åxed points of g even though g is not
contractive on [0, 1].
From (7.28)
Œ± ‚â•

g(x) ‚àíg(y)
x ‚àíy

(x Ã∏= y),
from which, if we substitute g(x) = Œªx(1 ‚àíx), and g(y) = Œªy(1 ‚àíy), then
Œ± = sup
xÃ∏=y
Œª|1 ‚àíx ‚àíy|.
(7.30)
If Œª = 1
2, then Œ± = 1
2. If Œª = 2, we cannot have Œ±, so that 0 < Œ± < 1. If
g(x) = 2x(1 ‚àíx) (i.e., Œª = 2 again), but now instead Y = [0.4, 0.6], then, for all
TLFeBOOK

FIXED-POINT METHOD
303
x ‚ààY = [0.4, 0.6], we have g(x) ‚àà[0.48, 0.50] ‚äÇY, implying g|Y ‚ÜíY. From
(7.30) we have for this situation Œ± = 0.4. The mapping g is contractive on Y.
Suppose x0 = 0.450000; then
x0 = 0.450000
x1 = 2x0(1 ‚àíx0) = 0.495000
x2 = 2x1(1 ‚àíx1) = 0.499950
x3 = 2x2(1 ‚àíx2) = 0.500000
x4 = 2x3(1 ‚àíx3) = 0.500000
...
The reader ought to compare the results of this example to the error bounds from
Corollary 7.1 as an exercise.
More generally, and again from (7.28), we have (for x, y ‚ààY = [a, b] ‚äÇR)
Œ± = sup
xÃ∏=y

g(x) ‚àíg(y)
x ‚àíy
 .
(7.31)
Now recall the mean-value theorem (i.e., Theorem 3.3). If g(x) is continuous on
[a, b], and g(1)(x) is continuous on (a, b), then there is a Œæ ‚àà(a, b) such that
g(1)(Œæ) = g(b) ‚àíg(a)
b ‚àía
.
Consequently, instead of (7.31) we may use
Œ± =
sup
x‚àà(a,b)
|g(1)(x)|.
(7.32)
Example 7.4
We may use (7.32) to rework some of the results in Example 7.3.
Since g(1)(x) = Œª(2x ‚àí1), if Y = [0, 1], and Œª = 1
2, then
Œ± =
sup
x‚àà(0,1)
x ‚àí1
2
 = 1
2.
If Œª = 2, then Œ± = 2. If now Y = [0.4, 0.6] with Œª = 2, then
Œ± =
sup
x‚àà(0.4,0.6)
|4x ‚àí2| = 0.4.
Now suppose that Œª = 2.8, and consider Y = [0.61, 0.67], which contains a Ô¨Åxed-
point of g (see Fig. 7.2, which contains a curve for this case). We have
Œ± =
sup
x‚àà(0.61,0.67)
|5.6x ‚àí2.8| = 0.9520,
TLFeBOOK

304
NONLINEAR SYSTEMS OF EQUATIONS
and if x ‚ààY, then g(x) ‚àà[0.619080, 0.666120] ‚äÇY so that g|Y ‚ÜíY, and so g is
contractive on Y. Thus, we consider the iterates
x0 = 0.650000
x1 = 2.8x0(1 ‚àíx0) = 0.637000
x2 = 2.8x1(1 ‚àíx1) = 0.647447
x3 = 2.8x2(1 ‚àíx2) = 0.639126
x4 = 2.8x3(1 ‚àíx3) = 0.645803
x5 = 2.8x4(1 ‚àíx4) = 0.640476
...
The true Ô¨Åxed point (to 6 signiÔ¨Åcant Ô¨Ågures) is x = 0.642857 (i.e., xn ‚Üíx). We
may check these numbers against the bounds of Corollary 7.1. Therefore, we con-
sider the distances
d(x5, x) = 0.002381,
d(x0, x1) = 0.013000,
d(x5, x4) = 0.005327,
and from (7.22a)
d(x5, x) ‚â§
Œ±k
1 ‚àíŒ± d(x0, x1) = 0.211780,
and from (7.22b)
d(x5, x) ‚â§
Œ±
1 ‚àíŒ± d(x5, x4) = 0.105652.
These error bounds are very loose, but they are nevertheless consistent with the
true error d(x5, x) = 0.002381.
We have worked with g(x) = Œªx(1 ‚àíx) in the previous two examples
(Example 7.3 and Example 7.4). But this is not the only possible choice. It may
be better to make other choices.
Example 7.5
Again, assume f (x) = Œªx2 + (1 ‚àíŒª)x = 0 as in the previous
two examples. Observe that
Œªx2 = (Œª ‚àí1)x
implies that
x =
8
Œª ‚àí1
Œª
x1/2 = g(x).
(7.33)
TLFeBOOK

NEWTON‚ÄìRAPHSON METHOD
305
If Œª = 4, then f (x) = 0 for x = 0 and for x = 3
4. For g(x) in (7.29), g(1)(x) =
8x ‚àí4, and g(1)( 3
4) = 2. We cannot Ô¨Ånd a closed interval Y containing x = 3
4 on
which g is contractive with g|Y ‚ÜíY (the slope of the curve g(x) is too steep in
the vicinity of the Ô¨Åxed point). But if we choose (7.33) instead, then
g(x) =
‚àö
3
2 x1/2,
g(1)(x) =
‚àö
3
4
1
x1/2 ,
and if Y = [0.7, 0.8], then for x ‚ààY, we have g(x) ‚àà[0.7246, 0.7746] ‚äÇY, and
Œ± =
sup
x‚àà(0.7,0.8)
‚àö
3
4
1
‚àöx = 0.5175.
So, g in (7.33) is contractive on Y. We observe the iterates
x0 = 0.7800
x1 =
‚àö
3
2 x1/2
0
= 0.7649
x2 =
‚àö
3
2 x1/2
1
= 0.7574
x3 =
‚àö
3
2 x1/2
2
= 0.7537
x4 =
‚àö
3
2 x1/2
3
= 0.7518
x5 =
‚àö
3
2 x1/2
4
= 0.7509,
...
which converge to x = 3
4 (i.e., xn ‚Üíx).
7.4
NEWTON‚ÄìRAPHSON METHOD
This method is yet another iterative approach to Ô¨Ånding roots of nonlinear equations.
In fact, it is a version of the Ô¨Åxed-point method that was considered in the previous
section. However, it is of sufÔ¨Åcient importance to warrant separate consideration
within its own section.
7.4.1
The Method
One way to derive the Newton‚ÄìRaphson method is by a geometric approach. The
method attempts to solve
f (x) = 0
(7.34)
TLFeBOOK

306
NONLINEAR SYSTEMS OF EQUATIONS
y = f(x)
x
y
0
Line y ‚àí f(pn) = f (1)(pn)(x ‚àípn)
[tangent to point (pn, f(pn)]
pn
pn+2
pn+1
f(pn)
Figure 7.3
Geometric interpretation of the Newton‚ÄìRaphson method.
(x ‚ààR, and f (x) ‚ààR) by approximating the root p (i.e., f (p) = 0) by a succession
of x intercepts of tangent lines to the curve y = f (x) at x the current approximation
to p. SpeciÔ¨Åcally, if pn is the current estimate of p, then the tangent line to the
point (pn, f (pn)) on the curve y = f (x) is
y = f (pn) + f (1)(pn)(x ‚àípn)
(7.35)
(see Fig. 7.3). The next approximation to p is x = pn+1 such that y = 0, that is,
from (7.35)
0 = f (pn) + f (1)(pn)(pn+1 ‚àípn),
so for n ‚ààZ+
pn+1 = pn ‚àí
f (pn)
f (1)(pn).
(7.36)
To start this process off, we need an initial guess at p, that is, we need to select p0.
Clearly this approach requires f (1)(pn) Ã∏= 0 for all n; otherwise the process will
terminate. Continuation will not be possible, except perhaps by choosing a new
starting point p0. This is one of the ways in which the method can break down.
It might be called premature termination. (Breakdown phenomena are discussed
further later in the text.)
Another derivation of (7.36) is as follows. Recall the theory of Taylor series
from Section 3.5. Suppose that f (x), f (1)(x), and f (2)(x) are all continuous on
[a, b]. Suppose that p is a root of f (x) = 0, and that pn approximates p. From
Taylor‚Äôs theorem [i.e., (3.71)]
f (x) = f (pn) + f (1)(pn)(x ‚àípn) + 1
2f (2)(Œæ)(x ‚àípn)2,
(7.37)
TLFeBOOK

NEWTON‚ÄìRAPHSON METHOD
307
where Œæ = Œæ(x) ‚àà(pn, x). Since f (p) = 0 from (7.37), we have
0 = f (p) = f (pn) + f (1)(pn)(p ‚àípn) + 1
2f (2)(Œæ)(p ‚àípn)2.
(7.38)
If |p ‚àípn| is small, we may neglect that last term of (7.38), and hence
0 ‚âàf (pn) + f (1)(pn)(p ‚àípn),
implying
p ‚âàpn ‚àí
f (pn)
f (1)(pn).
(7.39)
We treat the right-hand side of (7.39) as pn+1, the next approximation to p. Thus,
we again arrive at (7.36). The assumption that (p ‚àípn)2 is negligible is important.
If pn is not close enough to p, then the method may not converge. In particular,
the choice of starting point p0 is important.
As already mentioned, the Newton‚ÄìRaphson method is a special instance of the
Ô¨Åxed-point iteration method, where
g(x) = x ‚àí
f (x)
f (1)(x).
(7.40)
So
pn+1 = g(pn)
(7.41)
for n ‚ààZ+. Stopping criteria are (for œµ > 0)
|pn ‚àípn‚àí1| < œµ,
(7.42a)
|f (pn)| < œµ,
(7.42b)

pn ‚àípn‚àí1
pn
 < œµ,
pn Ã∏= 0.
(7.42c)
As with the bisection method, we prefer (7.42c).
Theorem 7.4: Convergence Theorem for the Newton‚ÄìRaphson Method
Let f be continuous on [a, b]. Let f (1)(x), and f (2)(x) exist and be continuous for
all x ‚àà(a, b). If p ‚àà[a, b] with f (p) = 0, and f (1)(p) Ã∏= 0, then there is a Œ¥ > 0
such that (7.36) generates sequence (pn) with pn ‚Üíp for any p0 ‚àà[p ‚àíŒ¥, p + Œ¥].
Proof
We have
pn+1 = g(pn),
n ‚ààZ+
with g(x) = x ‚àí
f (x)
f (1)(x). We need Y = [p ‚àíŒ¥, p + Œ¥] ‚äÇR with g|Y ‚ÜíY, and g
is contractive on Y. (Then we may immediately apply the convergence results from
the previous section.)
TLFeBOOK

308
NONLINEAR SYSTEMS OF EQUATIONS
Since f (1)(p) Ã∏= 0, and since f (1)(x) is continuous at p, there will be a Œ¥1 > 0
such that f (1)(x) Ã∏= 0 for all x ‚àà[p ‚àíŒ¥1, p + Œ¥1] ‚äÇ[a, b], so g is deÔ¨Åned and
continuous for x ‚àà[p ‚àíŒ¥1, p + Œ¥1]. Also
g(1)(x) = 1 ‚àí[f (1)(x)]2 ‚àíf (x)f (2)(x)
[f (1)(x)]2
= f (x)f (2)(x)
[f (1)(x)]2
for x ‚àà[p ‚àíŒ¥1, p + Œ¥1]. In addition, f (2)(x) is continuous on (a, b), so g(1)(x) is
continuous on [p ‚àíŒ¥1, p + Œ¥1]. We assume f (p) = 0, so
g(1)(p) = f (p)f (2)(p)
[f (1)(p)]2
= 0.
We have g(1)(x) continuous at x = p, implying that limx‚Üíp g(1)(x) = g(1)(p) = 0,
so there is a Œ¥ > 0 with 0 < Œ¥ < Œ¥1 such that
|g(1)(x)| ‚â§Œ± < 1
(7.43)
for x ‚àà[p ‚àíŒ¥, p + Œ¥] = Y for some Œ± such that 0 < Œ± < 1. If x ‚ààY, then by the
mean-value theorem, there is a Œæ ‚àà(x, p) such that
|g(x) ‚àíp| = |g(x) ‚àíg(p)| = |g(1)(Œæ)||x ‚àíp|
‚â§Œ±|x ‚àíp| < |x ‚àíp| < Œ¥,
so that we have g(x) ‚ààY for all x ‚ààY. That is, g|Y ‚ÜíY. Because of (7.43)
Œ± = sup
x‚ààY
|g(1)(x)|,
and 0 < Œ± < 1, so that g is contractive on Y. Immediately, sequence (pn) from
pn+1 = g(pn) for all p0 ‚ààY converges to p (i.e., pn ‚Üíp).
Essentially, the Newton‚ÄìRaphson method is guaranteed to converge to a root if
p0 is close enough to it and f is sufÔ¨Åciently smooth. Theorem 7.4 is weak because
it does not specify how to select p0.
The Newton‚ÄìRaphson method, if it converges, tends to do so quite quickly.
However, the method needs f (1)(pn) as well as f (pn). It might be the case that
f (1)(pn) requires much effort to evaluate. The secant method is a variation on the
Newton‚ÄìRaphson method that replaces f (1)(pn) with an approximation. SpeciÔ¨Å-
cally, since
f (1)(pn) = lim
x‚Üípn
f (x) ‚àíf (pn)
x ‚àípn
,
TLFeBOOK

NEWTON‚ÄìRAPHSON METHOD
309
if pn‚àí1 ‚âàpn, then
f (1)(pn) ‚âàf (pn‚àí1) ‚àíf (pn)
pn‚àí1 ‚àípn
.
(7.44)
This is the slope of the chord that connects the points (pn‚àí1, f (pn‚àí1)), and
(pn, f (pn)) on the graph of f (x). We may substitute (7.44) into (7.36) obtaining
(for n ‚ààN)
pn+1 = pn ‚àí(pn ‚àípn‚àí1)f (pn)
f (pn) ‚àíf (pn‚àí1) .
(7.45)
We need p0 and p1 to initialize the iteration process in (7.45). Usually these are
chosen to bracket the root, but this does not guarantee convergence. If the method
does converge, it tends to do so more slowly than the Newton‚ÄìRaphson method, but
this is the penalty to be paid for avoiding the computation of derivatives f (1)(x).
We remark that the method of false position is a modiÔ¨Åcation of the secant method
that is guaranteed to converge because successive approximations to p (i.e., pn‚àí1
and pn) are chosen to always bracket the root. But it is possible that convergence
may be slow. We do not cover this method in this book. We merely mention that
it appears elsewhere in the literature [10‚Äì12].
7.4.2
Rate of Convergence Analysis
What can be said about the speed of convergence of Ô¨Åxed-point schemes in gen-
eral, and of the Newton‚ÄìRaphson method in particular? Consider the following
deÔ¨Ånition.
DeÔ¨Ånition 7.3: Suppose that (pn) is such that pn ‚Üíp with pn Ã∏= p (n ‚ààZ+).
If there are Œª > 0, and Œ¥ > 0 such that
lim
n‚Üí‚àû
|pn+1 ‚àíp|
|pn ‚àíp|Œ¥ = Œª,
then we say that (pn) converges to p of order Œ¥ with asymptotic error constant Œª.
Additionally
‚Ä¢ If Œ¥ = 1, we say that (pn) is linearly convergent.
‚Ä¢ If Œ¥ > 1, we have superlinear convergence.
‚Ä¢ If Œ¥ = 2, we have quadratic convergence.
From this deÔ¨Ånition, if n is big enough, then
|pn+1 ‚àíp| ‚âàŒª|pn ‚àíp|Œ¥.
(7.46)
Thus, we would like Œ¥ to be large and Œª to be small for fast convergence. Since
Œ¥ is in the exponent, this parameter is more important than Œª for determining the
rate of convergence.
TLFeBOOK

310
NONLINEAR SYSTEMS OF EQUATIONS
Consider the Ô¨Åxed-point iteration
pn+1 = g(pn),
where g satisÔ¨Åes the requirements of a contraction mapping (so the Banach Ô¨Åxed-
point theorem applies). We can therefore say that
lim
n‚Üí‚àû
|pn+1 ‚àíp|
|pn ‚àíp|
= lim
n‚Üí‚àû
|g(pn) ‚àíg(p)|
|pn ‚àíp|
,
(7.47)
so from the mean-value theorem we have Œæn between pn and p for which
g(pn) ‚àíg(p) = g(1)(Œæn)(pn ‚àíp),
which, if used in (7.47), implies that
lim
n‚Üí‚àû
|pn+1 ‚àíp|
|pn ‚àíp|
= lim
n‚Üí‚àû|g(1)(Œæn)|.
(7.48)
Because Œæn is between pn and p and pn ‚Üíp, we must have Œæn ‚Üíp as well. Also,
we will assume g(1)(x) is continuous at p, so (7.48) now becomes
lim
n‚Üí‚àû
|pn+1 ‚àíp|
|pn ‚àíp|
= |g(1)(p)|,
(7.49)
which is a constant. Applying DeÔ¨Ånition 7.3, we conclude that the Ô¨Åxed-point
method is typically only linearly convergent because Œ¥ = 1, and the asymptotic
error constant is Œª = |g(1)(p)|, provided g(1)(p) Ã∏= 0. However, if g(1)(p) = 0, we
expect faster convergence. It turns out that this is often the case for the Newton‚Äì
Raphson method, as will now be demonstrated.
The iterative scheme for the Newton‚ÄìRaphson method is (7.36), which is a
particular case of Ô¨Åxed-point iteration where now
g(x) = x ‚àí
f (x)
f (1)(x),
(7.50)
and for which
g(1)(x) = f (x)f (2)(x)
[f (1)(x)]2 ,
(7.51)
so that g(1)(p) = 0 because f (p) = 0. Thus, superlinear convergence is anticipated
for this particular Ô¨Åxed-point scheme. Suppose that we have the Taylor expansion
g(x) = g(p) + g(1)(p)(x ‚àíp) + 1
2g(2)(Œæ)(x ‚àíp)2
TLFeBOOK

NEWTON‚ÄìRAPHSON METHOD
311
for which Œæ is between p and x. Since g(p) = p and g(1)(p) = 0, this becomes
g(x) = p + 1
2g(2)(Œæ)(x ‚àíp)2.
For x = pn, this in turn becomes
pn+1 = g(pn) = p + 1
2g(2)(Œæn)(p ‚àípn)2
(7.52)
for which Œæn lies between p and pn. Equation (7.52) can be rearranged as
pn+1 ‚àíp = 1
2g(2)(Œæn)(p ‚àípn)2,
and so
lim
n‚Üí‚àû
|pn+1 ‚àíp|
|pn ‚àíp|2 = 1
2|g(2)(p)|,
(7.53)
since Œæn ‚Üíp, and we are also assuming that g(2)(x) is continuous at p. Imme-
diately, Œ¥ = 2, and the Newton‚ÄìRaphson method is quadratically convergent. The
asymptotic error constant is plainly equal to 1
2|g(2)(p)|, provided g(2)(p) Ã∏= 0. If
g(2)(p) = 0, then an even higher order of convergence may be expected. It is
emphasized that these convergence results depend on g(x) being smooth enough.
Given this, since the convergence is at least quadratic, the number of accurate dec-
imal digits in the approximation to a root approximately doubles at every iteration.
7.4.3
Breakdown Phenomena
The Newton‚ÄìRaphson method can fail to converge (i.e., break down) in vari-
ous ways. Premature termination was mentioned earlier. An example appears in
Fig. 7.4. This Ô¨Ågure also illustrates divergence, which means that [if f (p) = 0]
lim
n‚Üí‚àû|pn ‚àíp| = ‚àû.
In other words, the sequence of iterates (pn) generated by the Newton‚ÄìRaphson
method moves progressively farther and farther away from the desired root p.
Another failure mechanism called oscillation may be demonstrated as follows.
Suppose that
f (x) = x3 ‚àí2x + 2, f (1)(x) = 3x2 ‚àí2.
(7.54)
Newton‚Äôs method for this speciÔ¨Åc case is
pn+1 = pn ‚àíp3
n ‚àí2pn + 2
3p2n ‚àí2
= 2p3
n ‚àí2
3p2n ‚àí2.
(7.55)
If we were to select p0 = 0 as a starting point, then
p0 = 0,
p1 = 1,
p2 = 0,
p3 = 1,
p4 = 0,
p5 = 1, . . . .
TLFeBOOK

312
NONLINEAR SYSTEMS OF EQUATIONS
0
1
2
3
4
5
6
7
‚àí0.8
‚àí0.6
‚àí0.4
‚àí0.2
0
0.2
0.4
0.6
x
y
y = xe‚àíx
tangent at x = 1
tangent at x = 2
tangent at x = 4
Figure 7.4
Illustration of breakdown phenomena in the Newton‚ÄìRaphson method; here,
f (x) = xe‚àíx. The Ô¨Ågure shows premature termination if pn = x = 1 since f (1)(1) = 0.
Also shown is divergence for the case where p0 > 1. (See how the x intercepts of the
tangent lines for x > 1 become bigger as x increases.)
More succinctly, pn = 1
2(1 ‚àí(‚àí1)n). The sequence of iterates is oscillating; it
does not diverge, and it does not converge, either. The sequence is periodic (it may
be said to be period 2), and in this case quite simple, but far more complicated
oscillations are possible with longer periods.
Premature termination, divergence, and oscillation are not the only possible
breakdown mechanisms. Another possibility is that the sequence (pn) can be
chaotic. Loosely speaking, chaos is a nonperiodic oscillation with a complicated
structure. Chaotic oscillations look a lot like random noise. This will be considered
in more detail in Section 7.6.
7.5
SYSTEMS OF NONLINEAR EQUATIONS
We now extend the Ô¨Åxed-point and Newton‚ÄìRaphson methods to solving nonlinear
systems of equations. We emphasize two equations in two unknowns (i.e., two-
dimensional problems). But much of what is said here applies to higher dimensions.
7.5.1
Fixed-Point Method
As remarked, it is easier to begin by Ô¨Årst considering the two-dimensional problem.
More speciÔ¨Åcally, we wish to solve
ÀÜf0(x0, x1) = 0,
ÀÜf1(x0, x1) = 0,
(7.56)
which we will assume may be rewritten in the form
x0 ‚àíf0(x0, x1) = 0,
x1 ‚àíf1(x0, x1) = 0,
(7.57)
and we see that solving these is equivalent to Ô¨Ånding where the curves in (7.57)
intersect in R2 (i.e., [x0
x1]T ‚ààR2). A general picture is in Fig. 7.5. As in the
TLFeBOOK

SYSTEMS OF NONLINEAR EQUATIONS
313
0
x1
x0 ‚àíf0(x0, x1) = 0
x0
x1 ‚àíf1(x0, x1) = 0
Figure 7.5
Typical curves in R2 corresponding to the system of equations in (7.57).
case of one-dimensional problems, there will usually be more than one way to
rewrite (7.56) in the form of (7.57).
Example 7.6
Suppose
ÀÜf0(x0, x1) = x0 ‚àíx2
0 ‚àí1
4x2
1 = 0 ‚áíf0(x0, x1) = x2
0 + 1
4x2
1,
ÀÜf1(x0, x1) = x1 ‚àíx2
0 + x2
1 = 0 ‚áíf1(x0, x1) = x2
0 ‚àíx2
1.
We see that
x2
0 + 1
4x2
1 ‚àíx0 = 0 ‚áí(x0 ‚àí1
2)2 + 1
4x2
1 = 1
4
(an ellipse in R2), and
x2
0 ‚àíx2
1 ‚àíx1 = 0 ‚áíx2
0 ‚àí(x1 + 1
2)2 = ‚àí1
4
(a hyperbola in R2). These are plotted in Fig. 7.6. The solutions to the system
are the points where the ellipse and hyperbola intersect each other. Clearly, the
solution is not unique.
It is frequently the case that nonlinear systems of equations will have more than
one solution just as this was very possible and very common in the case of a single
equation.
Vector notation leads to compact descriptions. SpeciÔ¨Åcally, we deÔ¨Åne x =
[x0x1]T ‚ààR2, f0(x) = f0(x0, x1), f1(x) = f1(x0, x1), and
F(x) =

f0(x0, x1)
f1(x0, x1)
	
=

f0(x)
f1(x)
	
.
(7.58)
TLFeBOOK

314
NONLINEAR SYSTEMS OF EQUATIONS
‚àí1
‚àí0.8
‚àí0.6
‚àí0.4
‚àí0.2
0
0.2
0.4
0.6
0.8
1
‚àí2
‚àí1.5
‚àí1
‚àí0.5
0
0.5
1
x0
x1
Figure 7.6
The curves of Example 7.6 (an ellipse and a hyperbola). The solutions to the
equations in Example 7.6 are the points where these curves intersect.
Then the nonlinear system in (7.57) becomes
x = F(x),
(7.59)
and the Ô¨Åxed point p ‚ààR2 of F satisÔ¨Åes
p = F(p).
(7.60)
We recall that R2 is a normed space if
||x||2 =
1

i=0
x2
i = x2
0 + x2
1
(7.61)
(x ‚ààR2). We may consider a sequence of vectors (xn) (i.e., xn ‚ààR2 for n ‚ààZ+),
and it converges to x iff
lim
n‚Üí‚àû||xn ‚àíx|| = 0.
We recall from Chapter 3 that R2 with the norm in (7.61) is a complete space, so
every Cauchy sequence (xn) in it will converge.
As with the scalar case considered in Section 7.3, we consider the sequence of
iterates (pn) such that
pn+1 = F(pn),
n ‚ààZ+.
The previous statements (and the following theorem) apply if R2 is replaced by Rm
(m > 2); that is, the space can be of higher dimension to accommodate m equations
in m unknowns. Naturally, for Rm the norm in (7.61) must change according to
||x||2 = m‚àí1
k=0 x2
k. The following theorem is really a special instance of the Banach
Ô¨Åxed-point theorem seen earlier.
TLFeBOOK

SYSTEMS OF NONLINEAR EQUATIONS
315
Theorem 7.5: Suppose that R is a closed subset of R2, F|R ‚ÜíR, and F is
contractive on R; then
x = F(x)
has a unique solution p ‚ààR. The sequence (pn), where
pn+1 = F(pn),
p0 ‚ààR,
n ‚ààZ+
(7.62)
is such that
lim
n‚Üí‚àû||pn ‚àíp|| = 0
and
||pn ‚àíp|| ‚â§
Œ±n
1 ‚àíŒ± ||p1 ‚àíp0||,
(7.63)
where ||F(p1) ‚àíF(p2)|| ‚â§Œ±||p1 ‚àíp2|| for any p1, p2 ‚ààR, and 0 < Œ± < 1.
Proof
As noted, this theorem is really a special instance of the Banach Ô¨Åxed-
point theorem (Theorem 7.3), so we only outline the proof.
It was mentioned in Section 7.3 that any closed subset of a complete metric
space is also complete. R2 with norm (7.61) is a complete metric space, and since
R ‚äÇR2 is closed, R must be complete. According to Theorem 7.3, F has a unique
Ô¨Åxed point p ‚ààR [i.e., F(p) = p], and sequence (pn) from pn+1 = F(pn) (with
p0 ‚ààR, and n ‚ààZ+) converges to p. The error bound
||pn ‚àíp|| ‚â§
Œ±n
1 ‚àíŒ± ||p1 ‚àíp0||
is an immediate consequence of Corollary 7.1.
A typical choice for R would be the bounded and closed rectangular region
R = {[x0x1]T |a0 ‚â§x0 ‚â§b0, a1 ‚â§x1 ‚â§b1}.
(7.64)
The next theorem applies the Schwarz inequality of Chapter 1 to the estimation
of Œ±. It must be admitted that applying the following theorem is often quite difÔ¨Åcult
in practice, and in the end it is often better to simply implement the iterative
method in (7.62) and experiment with it rather than go through the laborious task
of computing Œ± according to the theorem‚Äôs dictates. However, exceptions cannot
be ruled out, and so knowledge of the theorem might be helpful.
Theorem 7.6: Suppose that R ‚äÇR2 is as deÔ¨Åned in (7.64); then, if
Œ± = max
x‚ààR
 ‚àÇf0
‚àÇx0
!2
+
 ‚àÇf0
‚àÇx1
!2
+
 ‚àÇf1
‚àÇx0
!2
+
 ‚àÇf1
‚àÇx1
!2	1/2
,
(7.65)
TLFeBOOK

316
NONLINEAR SYSTEMS OF EQUATIONS
we have
||F(x1) ‚àíF(x2)|| ‚â§Œ±||x1 ‚àíx2||
for all x1, x2 ‚ààR.
Proof
We use a two-dimensional version of the Taylor expansion theorem,
which we will not attempt to justify in this book.
Given x1 = [x1,0 x1,1]T , x2 = [x2,0 x2,1]T , there is a point Œæ ‚ààR on the line
segment that joins x1 to x2 such that
F(x1) = F(x2) + F (1)(Œæ)(x1 ‚àíx2)
= F(x2) +
Ô£Æ
Ô£ØÔ£ØÔ£∞
‚àÇf0(Œæ)
‚àÇx0
‚àÇf0(Œæ)
‚àÇx1
‚àÇf1(Œæ)
‚àÇx0
‚àÇf1(Œæ)
‚àÇx1
Ô£π
Ô£∫Ô£∫Ô£ª
 x1,0 ‚àíx2,0
x1,1 ‚àíx2,1

so
||F(x1) ‚àíF(x2)||2 =
‚àÇf0(Œæ)
‚àÇx0
(x1,0 ‚àíx2,0) + ‚àÇf0(Œæ)
‚àÇx1
(x1,1 ‚àíx2,1)
2
+
‚àÇf1(Œæ)
‚àÇx0
(x1,0 ‚àíx2,0) + ‚àÇf1(Œæ)
‚àÇx1
(x1,1 ‚àíx2,1)
2
‚â§
 ‚àÇf0(Œæ)
‚àÇx0
!2
+
 ‚àÇf0(Œæ)
‚àÇx1
!2	
||x1 ‚àíx2||2
+
 ‚àÇf1(Œæ)
‚àÇx0
!2
+
 ‚àÇf1(Œæ)
‚àÇx1
!2	
||x1 ‚àíx2||2
via the Schwarz inequality (Theorem 1.1). Consequently
||F(x1) ‚àíF(x2)||2 ‚â§
 ‚àÇf0(Œæ)
‚àÇx0
!2
+
 ‚àÇf0(Œæ)
‚àÇx1
!2
+
 ‚àÇf1(Œæ)
‚àÇx0
!2
+
 ‚àÇf1(Œæ)
‚àÇx1
!2	
||x1 ‚àíx2||2
‚â§

max
x‚ààR
 ‚àÇf0(x)
‚àÇx0
!2
+
 ‚àÇf0(x)
‚àÇx1
!2
+
 ‚àÇf1(x)
‚àÇx0
!2
+
 ‚àÇf1(x)
‚àÇx1
!2	
||x1 ‚àíx2||2
= Œ±2||x1 ‚àíx2||2.
TLFeBOOK

SYSTEMS OF NONLINEAR EQUATIONS
317
As with the one-dimensional Taylor theorem from Chapter 3, our application of it
here assumes that F is sufÔ¨Åciently smooth.
To apply Theorem 7.5, we try to select F and R so that 0 < Œ± < 1. As already
noted, this is often done experimentally. The iterative process (7.62) can give us
an algorithm only if we have a stopping criterion. A good choice is (for suitable
œµ > 0) to stop iterating when
||pn ‚àípn‚àí1||
||pn||
< œµ,
pn Ã∏= 0.
(7.66)
It is possible to use norms in (7.66) other than the one deÔ¨Åned by (7.61). (A
Chebyshev norm might be a good alternative.) Since our methodology is often to
make guesses about F and R, it is very possible that a given guess will be wrong.
In other words, convergence may never occur. Thus, a loop that implements the
algorithm should also be programmed to terminate when the number of iterations
exceeds some reasonable threshold. In this event, the program must also be written
to print a message saying that convergence has not occurred because the upper limit
on the allowed number of iterations was exceeded. This is an important example
of exception handling in numerical computing.
Example 7.7
If we consider f0(x0, x1) and f1(x0, x1) as in Example 7.6, then
the sequence of iterates for the starting vector p0 = [1
1]T is
p1,0 = 1.2500,
p1,1 = 0.0000
p2,0 = 1.5625,
p2,1 = 1.5625
p3,0 = 3.0518,
p3,1 = 0.0000
p4,0 = 9.3132,
p4,1 = 9.3132
...
This vector sequence is not converging to the root near [0.9
0.5]T (see Fig. 7.6).
Many experiments with different starting values do not lead to a solution. So we
need to change F.
We may rewrite x0 ‚àíx2
0 ‚àí1
4x2
1 = 0 as
x0 =
x2
0
x2
0 + 1
4x2
1
= f0(x0, x1),
and we may rewrite x1 ‚àíx2
0 + x2
1 = 0 as
x1 =
x2
0
1 + x1
= f1(x0, x1).
TLFeBOOK

318
NONLINEAR SYSTEMS OF EQUATIONS
These redeÔ¨Åne F [recall (7.58) for the general form of F]. For this choice of F
and again with p0 = [1
1]T , the sequence of vectors is
p1,0 = 0.8000,
p1,1 = 0.5000
p2,0 = 0.9110,
p2,1 = 0.4627
p3,0 = 0.9480,
p3,1 = 0.5818
p4,0 = 0.9140,
p4,1 = 0.5682
...
p14,0 = 0.9189,
p14,1 = 0.5461
In this case the vector sequence now converges to the root with four-decimal-place
accuracy by the 14th iteration.
7.5.2
Newton‚ÄìRaphson Method
Consider yet again two equations in two unknowns
f0(x0, x1) = 0,
f1(x0, x1) = 0,
(7.67)
each of which deÔ¨Ånes a curve in the plane (again x = [x0
x1]T ‚ààR2). Solutions
to (7.67) are points of intersection of the two curves in R2. We will denote a point
of intersection by p = [p0
p1]T ‚ààR2, which is a root of the system (7.67).
Suppose that x0 = [x0,0
x0,1]T is an initial approximation to the root p. Assume
that f0 and f1 are smooth enough to possess a two-dimensional Taylor series expan-
sion,
f0(x0, x1) = f0(x0,0, x0,1) + ‚àÇf0
‚àÇx0
(x0,0, x0,1)(x0 ‚àíx0,0)
+ ‚àÇf0
‚àÇx1
(x0,0, x0,1)(x1 ‚àíx0,1) + 1
2!

‚àÇ2f0(x0,0, x0,1)
‚àÇx2
0
(x0 ‚àíx0,0)2
+ 2‚àÇ2f0(x0,0, x0,1)
‚àÇx0‚àÇx1
(x0 ‚àíx0,0)(x1 ‚àíx0,1)
+ ‚àÇ2f0(x0,0, x0,1)
‚àÇx2
1
(x1 ‚àíx0,1)2

+ ¬∑ ¬∑ ¬∑
f1(x0, x1) = f1(x0,0, x0,1) + ‚àÇf1
‚àÇx0
(x0,0, x0,1)(x0 ‚àíx0,0)
+ ‚àÇf1
‚àÇx1
(x0,0, x0,1)(x1 ‚àíx0,1) + 1
2!

‚àÇ2f1(x0,0, x0,1)
‚àÇx2
0
(x0 ‚àíx0,0)2
TLFeBOOK

SYSTEMS OF NONLINEAR EQUATIONS
319
+ 2‚àÇ2f1(x0,0, x0,1)
‚àÇx0‚àÇx1
(x0 ‚àíx0,0)(x1 ‚àíx0,1)
+ ‚àÇ2f1(x0,0, x0,1)
‚àÇx2
1
(x1 ‚àíx0,1)2

+ ¬∑ ¬∑ ¬∑
which have the somewhat more compact form using vector notation as
f0(x) = f0(x0) + ‚àÇf0(x0)
‚àÇx0
(x0 ‚àíx0,0) + ‚àÇf0(x0)
‚àÇx1
(x1 ‚àíx0,1)
+ 1
2!

‚àÇ2f0(x0)
‚àÇx2
0
(x0 ‚àíx0,0)2 + 2‚àÇ2f0(x0)
‚àÇx0‚àÇx1
(x0 ‚àíx0,0)(x1 ‚àíx0,1)
+ ‚àÇ2f0(x0)
‚àÇx2
1
(x1 ‚àíx0,1)2

+ ¬∑ ¬∑ ¬∑
(7.68a)
f1(x) = f1(x0) + ‚àÇf1(x0)
‚àÇx0
(x0 ‚àíx0,0) + ‚àÇf1(x0)
‚àÇx1
(x1 ‚àíx0,1)
+ 1
2!

‚àÇ2f1(x0)
‚àÇx2
0
(x0 ‚àíx0,0)2 + 2‚àÇ2f1(x0)
‚àÇx0‚àÇx1
(x0 ‚àíx0,0)(x1 ‚àíx0,1)
+ ‚àÇ2f1(x0)
‚àÇx2
1
(x1 ‚àíx0,1)2

+ ¬∑ ¬∑ ¬∑ .
(7.68b)
If x0 is close to p, then from (7.68), we have
0 = f0(p) ‚âàf0(x0) + ‚àÇf0(x0)
‚àÇx0
(p0 ‚àíx0,0) + ‚àÇf0(x0)
‚àÇx1
(p1 ‚àíx0,1)
+ 1
2!

‚àÇ2f0(x0)
‚àÇx2
0
(p0 ‚àíx0,0)2 + 2‚àÇ2f0(x0)
‚àÇx0‚àÇx1
(p0 ‚àíx0,0)(p1 ‚àíx0,1)
+ ‚àÇ2f0(x0)
‚àÇx2
1
(p1 ‚àíx0,1)2

+ ¬∑ ¬∑ ¬∑
(7.69a)
0 = f1(p) ‚âàf1(x0) + ‚àÇf1(x0)
‚àÇx0
(p0 ‚àíx0,0) + ‚àÇf1(x0)
‚àÇx1
(p1 ‚àíx0,1)
+ 1
2!

‚àÇ2f1(x0)
‚àÇx2
0
(p0 ‚àíx0,0)2 + 2‚àÇ2f1(x0)
‚àÇx0‚àÇx1
(p0 ‚àíx0,0)(p1 ‚àíx0,1)
+ ‚àÇ2f1(x0)
‚àÇx2
1
(p1 ‚àíx0,1)2

+ ¬∑ ¬∑ ¬∑
(7.69b)
TLFeBOOK

320
NONLINEAR SYSTEMS OF EQUATIONS
If we neglect the higher-order terms (second derivatives and higher-order deriva-
tives), then we obtain
‚àÇf0(x0)
‚àÇx0
(p0 ‚àíx0,0) + ‚àÇf0(x0)
‚àÇx1
(p1 ‚àíx0,1) ‚âà‚àíf0(x0),
(7.70a)
‚àÇf1(x0)
‚àÇx0
(p0 ‚àíx0,0) + ‚àÇf1(x0)
‚àÇx1
(p1 ‚àíx0,1) ‚âà‚àíf1(x0).
(7.70b)
As a shorthand notation, deÔ¨Åne
fi,j(x0) = ‚àÇfi(x0)
‚àÇxj
(7.71)
so (7.70) becomes
(p0 ‚àíx0,0)f0,0(x0) + (p1 ‚àíx0,1)f0,1(x0) ‚âà‚àíf0(x0),
(7.72a)
(p0 ‚àíx0,0)f1,0(x0) + (p1 ‚àíx0,1)f1,1(x0) ‚âà‚àíf1(x0).
(7.72b)
Multiply (7.72a) by f1,1(x0), and multiply (7.72b) by f0,1(x0). Subtracting the
second equation from the Ô¨Årst results in
(p0 ‚àíx0,0)[f0,0(x0)f1,1(x0) ‚àíf1,0(x0)f0,1(x0)]
‚âà‚àíf0(x0)f1,1(x0) + f1(x0)f0,1(x0).
(7.73a)
Now multiply (7.72a) by f1,0(x0), and multiply (7.72b) by f0,0(x0). Subtracting
the second equation from the Ô¨Årst results in
(p1 ‚àíx0,1)[f0,1(x0)f1,0(x0) ‚àíf0,0(x0)f1,1(x0)]
‚âà‚àíf0(x0)f1,0(x0) + f1(x0)f0,0(x0).
(7.73b)
From (7.73b), we obtain
p0 ‚âàx0,0 + ‚àíf0(x0)f1,1(x0) + f1(x0)f0,1(x0)
f0,0(x0)f1,1(x0) ‚àíf0,1(x0)f1,0(x0),
(7.74a)
p1 ‚âàx0,1 + ‚àíf1(x0)f0,0(x0) + f0(x0)f1,0(x0)
f0,0(x0)f1,1(x0) ‚àíf0,1(x0)f1,0(x0).
(7.74b)
We may assume that the right-hand side of (7.74) is the next approximation to p
x1,0 ‚âàx0,0 + ‚àíf0f1,1 + f1f0,1
f0,0f1,1 ‚àíf0,1f1,0

x0
,
(7.75a)
x1,1 ‚âàx0,1 + ‚àíf1f0,0 + f0f1,0
f0,0f1,1 ‚àíf0,1f1,0

x0
(7.75b)
TLFeBOOK

SYSTEMS OF NONLINEAR EQUATIONS
321
(x1 = [x1,0x1,1]T ), where the functions and derivatives are to be evaluated at x0.
We may continue this process to generate (xn) for n ‚ààZ+ (so in general xn =
[xn,0
xn,1]T ) according to
xn+1,0 = xn,0 + ‚àíf0(xn)f1,1(xn) + f1(xn)f0,1(xn)
f0,0(xn)f1,1(xn) ‚àíf0,1(xn)f1,0(xn),
(7.76a)
xn+1,1 = xn,1 + ‚àíf1(xn)f0,0(xn) + f0(xn)f1,0(xn)
f0,0(xn)f1,1(xn) ‚àíf0,1(xn)f1,0(xn).
(7.76b)
As in the previous subsection, we deÔ¨Åne
F(xn) =

f0(xn,0, xn,1)
f1(xn,0, xn,1)
	
=

f0(xn)
f1(xn)
	
.
(7.77)
Also
F (1)(xn) =

f0,0(xn)
f0,1(xn)
f1,0(xn)
f1,1(xn)
	
= JF (xn),
(7.78)
which is the Jacobian matrix JF evaluated at x = xn. We see that
[JF (xn)]‚àí1 =
1
f0,0(xn)f1,1(xn) ‚àíf0,1(xn)f1,0(xn)

f1,1(xn)
‚àíf0,1(xn)
‚àíf1,0(xn)
f0,0(xn)
	
,
(7.79)
so in vector notation (7.76) becomes
xn+1 = xn ‚àí[JF (xn)]‚àí1F(xn)
(7.80)
for n ‚ààZ+. If xn ‚ààRm (i.e., if we consider m equations in m unknowns), then
JF (xn) =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
f0,0(xn)
f0,1(xn)
¬∑ ¬∑ ¬∑
f0,m‚àí1(xn)
f1,0(xn)
f1,1(xn)
¬∑ ¬∑ ¬∑
f1,m‚àí1(xn)
...
...
...
fm‚àí1,0(xn)
fm‚àí1,1(xn)
¬∑ ¬∑ ¬∑
fm‚àí1,m‚àí1(xn)
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª,
(7.81a)
and
F(xn) =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
f0(xn)
f1(xn)
...
fm‚àí1(xn)
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª.
(7.81b)
Of course, xn = [xn,0 xn,1 ¬∑ ¬∑ ¬∑ xn,m‚àí1]T ‚ààRm.
TLFeBOOK

322
NONLINEAR SYSTEMS OF EQUATIONS
Equation (7.80) reduces to (7.46) when we have only one equation in one
unknown. We see that the method will fail if JF (xn) is singular at xn. As in
the one-dimensional problem of Section 7.4, the success of the method depends
on picking a good starting point x0. If convergence occurs, then it is quadratic
as in the one-dimensional (i.e., scalar) case. It is sometimes possible to force the
method to converge even if the starting point is poorly selected, but this will not
be considered here. The computational complexity of the method is quite high. If
xn ‚ààRm, then from (7.80) and (7.81), we require m2 + m function evaluations,
and we need to invert an m √ó m Jacobian matrix at every iteration. We know from
Chapter 4 that matrix inversion needs O(m3) operations. Ill conditioning of the
Jacobian is very much a potential problem as well.
Example 7.8
Refer to the examples in Section 7.5.1. In Example 7.6 there we
wanted to solve
f0(x0, x1) = x0 ‚àíx2
0 ‚àí1
4x2
1 = 0, f1(x0, x1) = x1 ‚àíx2
0 + x2
1 = 0.
Consequently
f0(xn) = xn,0 ‚àíx2
n,0 ‚àí1
4x2
n,1, f1(xn) = xn,1 ‚àíx2
n,0 + x2
n,1,
and the derivatives are
f0,0(xn) = 1 ‚àí2xn,0, f1,0(xn) = ‚àí2xn,0
f0,1(xn) = ‚àí1
2xn,1, f1,1(xn) = 1 + 2xn,1.
Via (7.76), the desired equations are
xn+1,0 = xn,0
+
‚àí(xn,0 ‚àíx2
n,0 ‚àí1
4x2
n,1)(1 + 2xn,1) + (xn,1 ‚àíx2
n,0 + x2
n,1)(‚àí1
2xn,1)
(1 ‚àí2xn,0)(1 + 2xn,1) ‚àíxn,0xn,1
,
(7.82a)
xn+1,1 = xn,1
+
‚àí(xn,1 ‚àíx2
n,0 + x2
n,1)(1 ‚àí2xn,0) + (xn,0 ‚àíx2
n,0 ‚àí1
4x2
n,1)(‚àí2xn,0)
(1 ‚àí2xn,0)(1 + 2xn,1) ‚àíxn,0xn,1
. (7.82b)
If we execute the iterative procedure in (7.82), we obtain
x0,0 = 0.8000,
x0,1 = 0.5000
x1,0 = 0.9391,
x1,1 = 0.5562
x2,0 = 0.9193,
x2,1 = 0.5463
x3,0 = 0.9189,
x3,1 = 0.5461
TLFeBOOK

CHAOTIC PHENOMENA AND A CRYPTOGRAPHY APPLICATION
323
We see that the answer is correct to four decimal places in only three iterations.
This is much faster than the Ô¨Åxed-point method seen in Example 7.7.
7.6
CHAOTIC PHENOMENA AND A CRYPTOGRAPHY APPLICATION
Iterative processes such as xn+1 = g(xn) (which includes the Newton‚ÄìRaphson
method) can converge to a Ô¨Åxed point [i.e., to x such that g(x) = x], or they
can fail to do so in various ways. This was considered in Section 7.4.3. We are
interested here in the case where (xn) is a chaotic sequence, in which case g is
often said to be a chaotic map. Formal deÔ¨Ånitions exist for chaotic maps [13,
p. 50]. However, these are rather technical. They are also difÔ¨Åcult to apply except
in relatively simple cases. We shall therefore treat chaos in an intuitive/empirical
(i.e., experimental) manner for simplicity.
In Section 7.3 we considered examples based on the logistic map
g(x) = Œªx(1 ‚àíx)
(7.83)
(recall Examples 7.3‚Äì7.5). Suppose that Œª = 4. Figure 7.7 shows two output
sequences from this map for two slightly different initial conditions. Plot (a) shows
for x0 = xa
0 = 0.745; while plot (b), x0 = xb
0 = 0.755. We see that |xa
0 ‚àíxb
0| =
0.01, yet after only a few iterations, the two sequences are very different from each
other. This is one of the distinguishing features of chaos: sensitive dependence
of the resulting sequence to minor changes in the initial conditions. For Œª = 4,
we have g|[0, 1] ‚Üí[0, 1], so divergence is impossible. Chaotic sequences do not
diverge. They remain bounded; that is, there is a M ‚ààR such that 0 < M < ‚àû
with |xn| ‚â§M for all n ‚ààZ+. But the sequence does not converge, and it is not
periodic, either. In fact, the plots in Fig. 7.7 show that the elements of sequence
(xn) seem to wander around rather aimlessly (i.e., apparently randomly). This wan-
dering behavior has been observed in the past [14, p. 167], but was not generally
recognized as being a chaotic phenomenon until more recently.
It has been known for a very long time that effective cryptographic systems
should exploit randomness [15]. Since chaotic sequences have apparently random
qualities, it is not surprising that they have been proposed as random-number gen-
erators (or as pseudo-random-number generators) for applications in cryptography
[16, 17]. However, it is presently a matter of legitimate controversy regarding just
how secure a cryptosystem based on chaos can be. One difÔ¨Åculty is as follows.
Nominally, a chaotic map g takes on values from the set of real numbers. But if
such a map is implemented on a digital computer, then, since all computers are
Ô¨Ånite-state machines, any chaotic sequence will not be truly chaotic as it will even-
tually repeat. Short period sequences are cryptographically weak (i.e., not secure).
There is presently no effective procedure (beyond exhaustive searching) for deter-
mining when this difÔ¨Åculty will arise in a chaos-based system. This is not the only
problem (see p. 1507 of Ref. 17 for others).
Two speciÔ¨Åc chaos-based cryptosystems are presented by De Angeli et al. [18]
and Papadimitriou et al. [19]. (There have been many others proposed in recent
TLFeBOOK

324
NONLINEAR SYSTEMS OF EQUATIONS
0
10
20
30
40
50
60
70
80
90
100
0
0.2
0.4
0.6
0.8
1
n
xn
x0 = 0.745
0
10
20
30
40
50
60
70
80
90
100
0
0.2
0.4
0.6
0.8
1
n
xn
x0 = 0.755
(a)
(b)
Figure 7.7
Output sequence from the logistic map g(x) = 4x(1 ‚àíx) with initial conditions
x0 = 0.745 (a) and x0 = 0.755 (b). Observe that although these initial conditions are close
together, the resulting sequences become very different from each other after only a few
iterations.
years.) Key size is the number of different possible encryption keys available in the
system. Naturally, this number should be large enough to prevent a codebreaker
(eavesdropper, cryptanalyst) from guessing the correct key. However, it is also well
known that a large key size is most deÔ¨Ånitely not a guarantee that the cryptosystem
will be secure. Papadimitriou et al. [19] demonstrate that their system has a large
key size, but their security analysis [19] did not go beyond this. Their system [19]
seems difÔ¨Åcult to analyze, however. In what follows we shall present some analysis
of the system in De Angeli et al. [18], and see that if implemented on a digital
computer, it is not really very secure. We begin with a description of their system
[18]. Their method in [18] is based on the H¬¥enon map (see Fig. 7.8), which is a
mapping deÔ¨Åned on R2 according to
x0,n+1 = 1 ‚àíŒ±x2
0,n + x1,n
x1,n+1 = Œ≤x0,n
(7.84)
for n ‚ààZ+ (so [x0,nx1,n]T ‚ààR2), and which is known to be chaotic in some neigh-
borhood of
Œ± = 1.4,
Œ≤ = 0.3
(7.85)
TLFeBOOK

CHAOTIC PHENOMENA AND A CRYPTOGRAPHY APPLICATION
325
0
20
40
60
80
100
120
140
160
180
200
‚àí1.5
‚àí1
‚àí0.5
0
0.5
1
1.5
n
x0,n
0
20
40
60
80
100
120
140
160
180
200
‚àí0.3
‚àí0.2
‚àí0.1
0
0.1
0.2
0.3
0.4
n
x1,n
Figure 7.8
Typical state sequences from the H¬¥enon map for Œ± = 1.45 and Œ≤ = 0.25, with
initial conditions x0,0 = ‚àí0.5 and x1,0 = 0.2.
so that constants Œ± and Œ≤ form the encryption key for the system. No choice in the
allowed neighborhood is a valid key. An immediate problem is that there seems
to be no detailed description of which points are allowed. A particular choice of
key should therefore be tested to see if the resulting sequence is chaotic. In what
follows the output sequence from the map is deÔ¨Åned to be
yn = x0,n.
(7.86)
The vector xn = [x0,n
x1,n]T is often called a state vector. The elements are state
variables.
The encryption algorithm works by mixing the chaotic sequence of (7.86) with
the message sequence, which we shall denote by (sn). The mixing (described below)
yields the cyphertext sequence, denoted (cn). A problem is that the receiver must
somehow recover the original message (sn) from the cyphertext (cn) using knowl-
edge of the encryption algorithm, and the key {Œ±, Œ≤}.
Consider the following mapping:
ÀÜx0,n+1 = 1 ‚àíŒ±y2
n + ÀÜx1,n,
ÀÜx1,n+1 = Œ≤yn.
(7.87)
Note that this mapping is of the same form as (7.84) except that x0,n is replaced
by yn, but from (7.86) these are the same (nominally). The mapping in (7.84)
TLFeBOOK

326
NONLINEAR SYSTEMS OF EQUATIONS
represents a physical system (or piece of software) at the transmitter, while (7.87)
is part of the receiver (hardware or software). Now deÔ¨Åne the error sequences
Œ¥xi,n = ÀÜxi,n ‚àíxi,n
(i ‚àà{0, 1})
(7.88)
for which it is possible to show [using (7.84), (7.86), and (7.87)] that
 Œ¥x0,n+1
Œ¥x1,n+1

=
 0
1
0
0

#
$%
&
=A
 Œ¥x0,n
Œ¥x1,n

.
(7.89)
We observe that A2 = 0. Matrix A is an example of a nilpotent matrix for this
reason. This immediately implies that the error sequences go to zero in at most
two iterations (i.e., two steps):
 Œ¥x0,n+2
Œ¥x1,n+2

=
 0
1
0
0
  0
1
0
0
  Œ¥x0,n
Œ¥x1,n

=
 0
0

.
This fact tells us that if (yn) is generated at the transmitter, and sent over the
communications channel to the receiver, then the receiver may perfectly recover2
the state sequence of the transmitter in not more than two steps. This is called dead-
beat synchronization. The system in (7.87) is a speciÔ¨Åc example of a nonlinear
observer for a nonlinear dynamic system. There is a well-developed theory of
observers for linear dynamic systems [20]. The notion of an observer is a control
systems concept, so we infer that control theory is central to the problem of applying
chaotic systems to cryptographic applications.
All of this suggests the following algorithm for encrypting a message. Assume
that we wish to encrypt a length N message sequence (sn); that is, we only have
the elements s0, s1, . . . , sN‚àí2, sN‚àí1.
1. The transmitter generates and transmits y0, y1 according to (7.84) and (7.86).
The transmitter also generates y2, y3, . . . , yN, yN+1, but these are not trans-
mitted.
2. The transmitter sends the cyphertext sequence
cn = yn+2
sn
(7.90)
for n = 0, 1, . . . , N ‚àí1. Of course, this assumes we never have sn = 0.
Equation (7.90) is not the only possible means of mixing the message and the
chaotic sequence together. The decryption algorithm at the receiver is as follows:
2This assumes a perfect communications channel (which is not realistic), and that rounding errors in
the computation are irrelevant (which will be true if the receiver implements arithmetic in the same
manner as the transmitter).
TLFeBOOK

CHAOTIC PHENOMENA AND A CRYPTOGRAPHY APPLICATION
327
1. The receiver regenerates the transmitter‚Äôs state sequence using (7.87), its
knowledge of the key, and the sequence elements y0, y1. SpeciÔ¨Åcally, for
n = 0, 1 compute
ÀÜx0,n+1 = 1 ‚àíŒ±y2
n + ÀÜx1,n,
ÀÜx1,n+1 = Œ≤yn
(7.91a)
while for n = 2, 3, . . . , N ‚àí1, N compute
ÀÜx0,n+1 = 1 ‚àíŒ± ÀÜx2
0,n + ÀÜx1,n,
ÀÜx1,n+1 = Œ≤ ÀÜx0,n.
(7.91b)
Recover yn for n = 2, 3, . . . , N, N + 1 according to
yn = ÀÜx0,n.
(7.92)
2. Recover the original message via
sn = yn+2
cn
,
(7.93)
where n = 0, 1, . . . , N ‚àí2, N ‚àí1.
The initial states at the transmitter and receiver are arbitrary; that is, any x0,0, x1,0
and ÀÜx1,0 may be selected in (7.84) and (7.91a). The elements y0, y1, and the cypher-
text are sent over the channel. If these are lost because of a corrupt channel, then the
receiver should request retransmission of this information. It is extremely impor-
tant that the transmitter resend the same synchronizing elements y0, y1 and not a
different pair. The reason will become clear below.
We remark that since we are assuming that the algorithms are implemented
on a digital computer, so each sequence element is a binary word of some form.
The synchronizing elements and cyphertext are thus a bitstream. Methods exist to
encode such data for transmission over imperfect channels such that the probability
of successful transmission can be made quite high. These are called error control
coding schemes.
In general, the receiver will fail to recover the message if (1) the way in which
arithmetic is performed by the receiver is not the same as at the transmitter, (2) the
channel corrupts the transmission, or (3) the receiver does not know the key {Œ±, Œ≤}.
Item 1 is important since the failure to properly duplicate arithmetic operations at
the receiver will cause machine rounding errors to accumulate and prevent data
recovery. This is really a case of improper synchronization. The plots of Figs. 7.9‚Äì
7.11 illustrate some of these effects.
It is noteworthy that even though the receiver may not be a perfect match to the
transmitter, some of the samples (at the beginning of the message) are recovered.
TLFeBOOK

328
NONLINEAR SYSTEMS OF EQUATIONS
0
10
20
30
40
50
60
70
80
90
100
‚àí1
‚àí0.5
0
0.5
1
Time step
Amplitude
0
10
20
30
40
50
60
70
80
90
100
‚àí1
‚àí0.5
0
0.5
1
Time step
Amplitude
(a)
(b)
Figure 7.9
Transmitted (a) and reconstructed (b) message sequences. The sinusoidal mes-
sage sequence (sn) is perfectly reconstructed at the receiver; because the channel is perfect,
the receiver knows the key (here Œ± = 1.4, Œ≤ = 0.3), and arithmetic is performed in identical
fashion at both the transmitter and receiver.
This is actually an indication that the system is not really very secure. We now
consider security of the method in greater detail.
What is the key size? This question seems hard to answer accurately, but a
simple analysis is as follows. Suppose that the H¬¥enon map is chaotic in the
rectangular region of the Œ±Œ≤ plane deÔ¨Åned by Œ± ‚àà[1.4 ‚àíŒ±, 1.4 + Œ±], and
Œ≤ ‚àà[0.3 ‚àíŒ≤, 0.3 + Œ≤], with Œ±, Œ≤ > 0. We do not speciÔ¨Åcally know the
interval limits, and it is an ad hoc assumption that the chaotic neighborhood is
rectangular (this is a false assumption). Suppose that Œ≤ is the smallest M-bit binary
fraction such that Œ≤ ‚â•0.3 ‚àíŒ≤, and that Œ≤ is the largest M-bit binary fraction such
that Œ≤ ‚â§0.3 + Œ≤. In this case the number of M-bit fractions from 0.3 ‚àíŒ≤ to
0.3 + Œ≤ is about 2M(Œ≤ ‚àíŒ≤) + 1. Thus
2M(Œ≤ ‚àíŒ≤ + 1) ‚âà2M[(0.3 + Œ≤) ‚àí(0.3 ‚àíŒ≤)] + 1 = 2M+1Œ≤ + 1 = KŒ≤,
(7.94a)
and by similar reasoning for Œ±
KŒ± = 2M+1Œ± + 1,
(7.94b)
TLFeBOOK

CHAOTIC PHENOMENA AND A CRYPTOGRAPHY APPLICATION
329
0
10
20
30
40
50
60
70
80
90
100
‚àí1
‚àí0.5
0
0.5
1
Time step
Amplitude
0
10
20
30
40
50
60
70
80
90
100
‚àí10
‚àí5
0
5
10
15
20
Time step
Amplitude
(a)
(b)
Figure 7.10
Transmitted (a) and reconstructed (b) message sequences. Here, the conditions
of Fig. 7.9 hold except that the receiver uses a mismatched key Œ± = 1.400001, Œ≤ = 0.300001.
Thus, the message is eventually lost. (Note that the Ô¨Årst few message samples seem to be
recovered accurately.).
which implies that the key size is (very approximately)
K = KŒ±KŒ≤.
(7.95)
Even if Œ± and Œ≤ are small, and the structure of the chaotic neighborhood is
not rectangular, we can make M big enough (in principle) to generate a big key
space. Apparently, Ruelle [21, p. 19] has shown that the H¬¥enon map is periodic
(i.e., not chaotic) for Œ± = 1.3, Œ≤ = 0.3, so the size of the chaotic region is not very
big. It is irregular and ‚Äúfull of holes‚Äù (the ‚Äúholes‚Äù are key parameters that don‚Äôt
give chaotic outputs). In any case it seems that a large key size is possible. But as
cautioned earlier, this is no guarantee of security.
What does the transmitter send over the channel? This is the same as asking
what the eavesdropper knows. From the encryption algorithm the eavesdropper
knows the synchronizing elements y0, y1, and the cyphertext. The eavesdropper
also knows the algorithm, but not the key. Is this enough to Ô¨Ånd the key? It is now
obvious to ask if knowledge of y0, y1 gives the key away. This question may be
easily (?) resolved as follows.
From (7.84) and (7.86) for n ‚ààN, we have
yn+1 = 1 ‚àíŒ±y2
n + Œ≤yn‚àí1.
(7.96)
TLFeBOOK

330
NONLINEAR SYSTEMS OF EQUATIONS
0
10
20
30
40
50
60
70
80
90
100
‚àí1
‚àí0.5
0
0.5
1
Time step
Amplitude
0
10
20
30
40
50
60
70
80
90
100
‚àí2
‚àí1
0
1
2
3
4
Time step
Amplitude
(a)
(b)
Figure 7.11
Transmitted (a) and reconstructed (b) message sequences. Here, the conditions
of Fig. 7.9 hold except the receiver and the transmitter do not perform arithmetic in identical
fashion. In this case the order of two operations was reversed at the receiver. Thus, the
message is eventually lost. (Note again that the Ô¨Årst few message samples seem to be
recovered accurately.).
The encryption algorithm generates y0, y1, . . . , yN+1, which, from (7.96), must
lead to the key satisfying the linear system of equations
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
y2
1
‚àíy0
y2
2
‚àíy1
...
...
y2
N‚àí1
‚àíyN‚àí2
y2
N
‚àíyN‚àí1
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
#
$%
&
=Y
 Œ±
Œ≤

# $% &
=a
=
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
1 ‚àíy2
1 ‚àíy3
...
1 ‚àíyN
1 ‚àíyN+1
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
#
$%
&
=y
.
(7.97)
Compactly, we have Ya = y. This is an overdetermined system. The eavesdropper
has only y0, y1, so from (7.97) the eavesdropper needs to solve

y2
1
‚àíy0
y2
2
‚àíy1
	  Œ±
Œ≤

=
 1 ‚àíy2
1 ‚àíy3

.
(7.98)
TLFeBOOK

CHAOTIC PHENOMENA AND A CRYPTOGRAPHY APPLICATION
331
But the eavesdropper does not know y2, y3 as these were not transmitted. These
elements become mixed with the message, and so are not available to the eaves-
dropper. We immediately conclude that deadbeat synchronization is secure.3 There
is an alternative synchronization scheme [18] that may be proved to be insecure
by this method of analysis.
Does the cyphertext give key information away? This seems not to have a
complete answer either. However, we may demonstrate that the system of D‚ÄôAngeli
et al. [18] (using deadbeat synchronization) is vulnerable to a known plaintext4
attack. Such a vulnerability is ordinarily sufÔ¨Åcient to preclude using a method in
a high-security application. The analysis assumes partial prior knowledge of the
message (sn). Let us speciÔ¨Åcally assume that
sn =
p

k=0
aknk,
(7.99)
but we do not know ak or p. That is, the structure of our message is a polyno-
mial sequence, but we do not know more than this. Combining (7.90) with (7.96)
gives us
cnsn = 1 ‚àíŒ±c2
n‚àí1s2
n‚àí1 + Œ≤cn‚àí2sn‚àí2.
(7.100)
It is not difÔ¨Åcult to conÔ¨Årm that
s2
n‚àí1 =
2p

k=0
Ô£±
Ô£≤
Ô£≥

i+j=k
aiaj
Ô£º
Ô£Ω
Ô£æ
#
$%
&
=bk
(n ‚àí1)k
(7.101)
so that (7.100) becomes
p

k=0
aknkcn +
2p

k=0
Œ±bk(n ‚àí1)kc2
n‚àí1 ‚àí
p

k=0
Œ≤ak(n ‚àí2)kcn‚àí2 = 1.
(7.102)
Remembering that the eavesdropper has cyphertext (cn) (obtained by eavesdrop-
ping), the eavesdropper can use (7.102) to set up a linear system of equations in
the key and message parameters ak. The code is therefore easily broken in this
case.
If the message has a more complex structure, (7.102) will generally be replaced
by some hard-to-solve nonlinear problem wherein the methods of previous sections
(e.g., Newton‚ÄìRaphson) can be used to break the code. We conclude that, in spite
of technical problems from the eavesdropper‚Äôs point of view (ill conditioning,
incomplete cyphertext sequences, etc.), the scheme in Ref. 18 is not secure.
3This conclusion assumes there are no other distinct ways to work with the encryption algorithm
equations in such a manner as to give an equation that an eavesdropper can solve for the key knowing
only y0, y1, and the cyphertext.
4The message is also called plaintext.
TLFeBOOK

332
NONLINEAR SYSTEMS OF EQUATIONS
REFERENCES
1. J. H. Wilkinson, ‚ÄúThe PerÔ¨Ådious Polynomial,‚Äù in Studies in Mathematics, G. H. Golub,
ed., Vol. 24, Mathematical Association of America, 1984.
2. A. M. Cohen, ‚ÄúIs the Polynomial so PerÔ¨Ådious?‚Äù Numerische Mathematik 68, 225‚Äì238
(1994).
3. T. E. Hull and R. Mathon, ‚ÄúThe Mathematical Basis and Prototype Implementation of a
New Polynomial Root Finder with Quadratic Convergence,‚Äù ACM Trans. Math. Software
22, 261‚Äì280 (Sept. 1996).
4. E. Kreyszig, Introductory Functional Analysis with Applications, Wiley, New York,
1978.
5. W. Rudin, Principles of Mathematical Analysis, 3rd ed., McGraw-Hill, New York, 1976.
6. D. C. Youla and H. Webb, ‚ÄúImage Restoration by the Method of Convex Projections:
Part I‚ÄîTheory,‚Äù IEEE Trans. Med. Imag. MI-1, 81‚Äì94 (Oct. 1982).
7. A. E. Cetin, O. N. Gerek and Y. Yardimci, ‚ÄúEquiripple FIR Filter Design by the FFT
Algorithm,‚Äù IEEE Signal Process. Mag. 14, 60‚Äì64 (March 1997).
8. K. Gr¬®ochenig, ‚ÄúA Discrete Theory of Irregular Sampling,‚Äù Linear Algebra Appl. 193,
129‚Äì150 (1993).
9. H. H. Bauschke and J. M. Borwein, ‚ÄúOn Projection Algorithms for Solving Convex
Feasibility Problems,‚Äù SIAM Rev. 38, 367‚Äì426 (Sept. 1996).
10. E. Kreyszig, Advanced Engineering Mathematics, 4th ed., Wiley, New York, 1979.
11. E. Isaacson and H. B. Keller, Analysis of Numerical Methods, Wiley, New York, 1966.
12. F. B. Hildebrand, Introduction to Numerical Analysis, 2nd ed., McGraw-Hill, New York,
1974.
13. R. L. Devaney, An Introduction to Chaotic Dynamical Systems, 2nd ed., Addison-
Wesley, Redwood City, CA, 1989.
14. G. E. Forsythe, M. A. Malcolm and C. B. Moler, Computer Methods for Mathematical
Computations, Prentice-Hall, Englewood Cliffs, NJ, 1977.
15. G. Brassard, Modern Cryptology: A Tutorial, Lecture Notes in Computer Science
(series), Vol. 325, G. Goos and J. Hartmanis, eds., Springer-Verlag, New York, 1988.
16. L. Kocarev, ‚ÄúChaos-Based Cryptography: A Brief Overview,‚Äù IEEE Circuits Syst. Mag.
1(3), 6‚Äì21 (2001).
17. F. Dachselt and W. Schwarz, ‚ÄúChaos and Cryptography,‚Äù IEEE Trans. Circuits Syst.
(Part I: Fundamental Theory and Applications) 48, 1498‚Äì1509 (Dec. 2001).
18. A. De Angeli, R. Genesio and A. Tesi, ‚ÄúDead-Beat Chaos Synchronization in Discrete-
Time Systems,‚Äù IEEE Trans. Circuits Syst. (Part I: Fundamental Theory and Applica-
tions) 42, 54‚Äì56 (Jan. 1995).
19. S. Papadimitriou, A. Bezerianos and T. Bountis, ‚ÄúSecure Communications with Chaotic
Systems of Difference Equations,‚Äù IEEE Trans. Comput. 46, 27‚Äì38 (Jan. 1997).
20. M. S. Santina, A. R. Stubberud and G. H. Hostetter, Digital Control System Design,
2nd ed., Saunders College Publ., Fort Worth, TX, 1994.
21. D. Ruelle, Chaotic Evolution and Strange Attractors: The Statistical Analysis of Time
Series for Deterministic Nonlinear Systems, Cambridge Univ. Press, New York, 1989.
22. M. Jenkins and J. Traub, ‚ÄúA Three-Stage Variable Shift Algorithm for Polynomial Zeros
and Its Relation to Generalized Rayleigh Iteration,‚Äù Numer. Math. 14, 252‚Äì263 (1970).
TLFeBOOK

PROBLEMS
333
PROBLEMS
7.1. For the functions and starting intervals below solve f (x) = 0 using the
bisection method. Use stopping criterion (7.13d) with œµ = 0.005. Do the
calculations with a pocket calculator.
(a) f (x) = loge x + 2x + 1, [a0, b0] = [0.2, 0.3].
(b) f (x) = x3 ‚àícos x, [a0, b0] = [0.8, 1.0].
(c) f (x) = x ‚àíe‚àíx/5, [a0, b0] = [ 3
4, 1].
(d) f (x) = x6 ‚àíx ‚àí1, [a0, b0] = [1, 3
2].
(e) f (x) = sin x
x
+ exp(‚àíx), [a0, b0] = [3, 4].
(f) f (x) = sin x
x
‚àíx + 1, [a0, b0] = [1, 2].
7.2. Consider f (x) = sin(x)/x. This function has a minimum value for some x ‚àà
[œÄ, 2œÄ]. Use the bisection method to Ô¨Ånd this x. Use the stopping criterion
in (7.13d) with œµ = 0.005. Use a pocket calculator to do the computations.
7.3. This problem introduces the variation on the bisection method called regula
falsi (or the method of false position). Suppose that [a0, b0] brackets the root
p [i.e., f (p) = 0]. Thus, f (a0)f (b0) < 0. The Ô¨Årst estimate of the root p,
denoted by p0, is where the line joining the points (a0, f (a0)) and (b0, f (b0))
crosses the x axis.
(a) Show that
p0 = a0 ‚àí
b0 ‚àía0
f (b0) ‚àíf (a0)f (a0).
(b) Using stopping criterion (7.13d), write pseudocode for the method of
false position.
7.4. In certain signal detection problems (e.g., radar or sonar) the probability of
false alarm (FA) (i.e., of saying that a certain signal is present in the data
when it actually is not) is given by
PFA =
 ‚àû
Œ∑
1
(p/2)2p/2 x
p
2 ‚àí1e‚àíx/2 dx,
(7.P.1)
where Œ∑ is called the detection threshold. If p is an even number, it can be
shown that (7.P.1) reduces to the Ô¨Ånite series
PFA = e‚àí1
2 Œ∑
(p/2)‚àí1

k=0
1
k!
Œ∑
2
k
.
(7.P.2)
The detection threshold Œ∑ is a very important design parameter in signal
detectors. Often it is desired to specify an acceptable value for PFA (where
TLFeBOOK

334
NONLINEAR SYSTEMS OF EQUATIONS
0 < PFA < 1), and then it is necessary to solve nonlinear equation (7.P.2)
for Œ∑. Let p = 6. Use the bisection method to Ô¨Ånd Œ∑ for
(a) PFA = 0.001
(b) PFA = 0.01
(c) PFA = 0.1
7.5. We wish to solve
f (x) = x4 ‚àí5
2x3 + 5
2x ‚àí1 = 0
using a Ô¨Åxed-point method. This requires Ô¨Ånding g(x) such that
g(x) ‚àíx = f (x).
Find four different functions g(x).
7.6. Can the Ô¨Åxed-point method be used to Ô¨Ånd the solution to
f (x) = x6 ‚àíx ‚àí1 = 0
for the root located in the interval [1, 3
2]? Explain.
7.7. Consider the nonlinear equation
f (x) = x ‚àíe‚àíx/5 = 0
(which has a solution on interval [ 3
4, 1]). Use (7.32) to estimate Œ±. Recalling
that xn = gnx0 (in the Ô¨Åxed-point method), if x0 = 1, then use (7.22a) to
estimate n so that d(xn, x) < 0.005 [x is the root of f (x) = 0]. Use a pocket
calculator to compute x1, . . . , xn.
7.8. Consider the nonlinear equation
f (x) = x ‚àí1 ‚àí1
2e‚àíx = 0
(which has a solution on interval [1, 1.2]). Use (7.32) to estimate Œ±. Recalling
that xn = gnx0 (in the Ô¨Åxed-point method), if x0 = 1 then use (7.22a) to
estimate n so that d(xn, x) < 0.001 [x is the root of f (x) = 0]. Use a pocket
calculator to compute x1, . . . , xn.
7.9. Problem 5.14 (in Chapter 5) mentioned the fact that orthogonal polynomi-
als possess the ‚Äúinterleaving of zeros‚Äù property. Use this property and the
bisection method to derive an algorithm to Ô¨Ånd the zeros of all Legendre poly-
nomials Pn(x) for n = 1, 2, . . . , N. Express the algorithm in pseudocode. Be
fairly detailed about this.
TLFeBOOK

PROBLEMS
335
7.10. We wish to Ô¨Ånd all of the roots of
f (x) = x3 ‚àí3x2 + 4x ‚àí2 = 0.
There is one real-valued root, and two complex-valued roots. It is easy to
conÔ¨Årm that f (1) = 0, but use
g(x) = x3 + 3x2 + x + 2
2x2 + 5
to estimate the real root p using Ô¨Åxed-point iteration [i.e, pn+1 = g(pn)].
Using a pocket calculator, compute only p1, p2, p3, and p4, and use the
starting point p0 = 2. Also, use the Newton‚ÄìRaphson method to estimate
the real root. Again choose p0 = 2, and compute only p1, p2, p3, and p4.
Once the real root is found, Ô¨Ånding the complex-valued roots is easy. Find
the complex-valued roots by making use of the formula for the roots of a
quadratic equation.
7.11. Consider Eq. (7.36). Via Theorem 3.3, there is an Œ±n between root p (f (p) =
0) and the iterate pn such that
f (p) ‚àíf (pn) = f (1)(Œ±n)(p ‚àípn).
(a) Show that
p ‚àípn = (pn ‚àípn‚àí1)
'
f (pn)
f (pn‚àí1)
f (1)(pn‚àí1)
f (1)(Œ±n)
(
#
$%
&
=An
.
[Hint: Use the identity 1 = f (pn‚àí1)
f (pn‚àí1)
f (1)(pn‚àí1)
f (1)(pn‚àí1).]
(b) Argue that if convergence is occurring, we have
lim
n‚Üí‚àû|An| = 1.

Hence limn‚Üí‚àû

p‚àípn
pn‚àípn‚àí1
 = 1.

(c) An alternative stopping criterion for the Newton‚ÄìRaphson method is to
stop iterating when
|f (pn)| + |pn ‚àípn‚àí1| < œµ
for some suitably small œµ > 0. Is this criterion preferable to (7.42)?
Explain.
TLFeBOOK

336
NONLINEAR SYSTEMS OF EQUATIONS
7.12. For the functions listed below, and for the stated starting value p0, use
the Newton‚ÄìRaphson method to solve f (p) = 0. Use the stopping crite-
rion (7.42a) with œµ = 0.001. Perform all calculations using only a pocket
calculator.
(a) f (x) = x + tan x, p0 = 2.
(b) f (x) = x6 ‚àíx ‚àí1, p0 = 1.5.
(c) f (x) = x3 ‚àícos x, p0 = 1.
(d) f (x) = x ‚àíe‚àíx/5, p0 = 1.
7.13. Use the Newton‚ÄìRaphson method to Ô¨Ånd the real-valued root of the polyno-
mial equation
f (x) = 1 + 1
2x + 1
6x2 + 1
24x3 = 0.
Choose starting point p0 = ‚àí2. Iterate 4 times. [Comment: Polynomial f (x)
arises in the stability analysis of a numerical method for solving ordinary
differential equations. This will be seen in Chapter 10, Eq. (10.83).]
7.14. Write a MATLAB function to solve Problem 7.4 using the Newton‚ÄìRaphson
method. Use the stopping criterion (7.42a).
7.15. Prove the following theorem (Newton‚ÄìRaphson error formula). Let f (x) ‚àà
C2[a, b], and f (p) = 0 for some p ‚àà[a, b]. For pn ‚àà[a, b] with
pn+1 = pn ‚àí
f (pn)
f (1)(pn),
there is a Œæn between p and pn such that
p ‚àípn+1 = ‚àí1
2(p ‚àípn)2 f (2)(Œæn)
f (1)(pn).
[Hint: Consider the Taylor series expansion of f (x) about the point x = pn
f (x) = f (pn) + (x ‚àípn)f (1)(pn) + 1
2(x ‚àípn)2f (2)(Œæn),
and then set x = p.]
7.16. This problem is about two different methods to compute ‚àöx. To begin, recall
that if x is a binary Ô¨Çoating-point number (Chapter 2), then it has the form
x = x0.x1 ¬∑ ¬∑ ¬∑ xt √ó 2e,
where, since x ‚â•0, we have x0 = 0, and because of normalization x1 = 1.
Generally, xk ‚àà{0, 1}, and e is the exponent. If e = 2k (i.e., the exponent
TLFeBOOK

PROBLEMS
337
is even), we do not adjust x. But if e = 2k + 1 (i.e., the exponent is odd),
we shift the mantissa to the right by one bit position so that e becomes
e = 2k + 2. Thus, x now has the form
x = a √ó 2e,
where a ‚àà[ 1
4, 1) in general, and e is an even number. Immediately, ‚àöx =
‚àöa √ó 2e/2. From this description we see that any square root algorithm need
work with arguments only on the interval [ 1
4, 1] without loss of generality
(w.l.o.g.).
(a) Finding the square root of x = a is the same problem as solving f (x) =
x2 ‚àía = 0. Show that the Newton‚ÄìRaphson method for doing so yields
the iterative algorithm
pn+1 = 1
2
 
pn + a
pn
!
,
(7.P.3)
where pn ‚Üí‚àöa. [Comment: It can be shown via an argument based
on the theorem in Problem 7.15 that to ensure convergence, we should
set p0 = 1
3(2a + 1). A simpler choice for the starting value is p0 =
1
2( 1
4 + 1) = 5
8 (since we know a ‚àà[ 1
4, 1]).]
(b) Mikami et al. (1992) suggest an alternative algorithm to Ô¨Ånd ‚àöx. They
recommend that for a ‚àà[ 1
4, 1] the square root of a be obtained by the
algorithm
pn+1 = Œ≤(a ‚àíp2
n) + pn.
(7.P.4)
DeÔ¨Åne error sequence (en) as en = ‚àöa ‚àípn. Show that
en+1 = Œ≤e2
n + (1 ‚àí2Œ≤‚àöa)en.
[Comment: Mikami et al. recommend that p0 = 0.666667a + 0.354167.]
(c) What condition on Œ≤ gives quadratic convergence for the algorithm in
(7.P.4)?
(d) Some microprocessors are intended for applications in high-speed digital
signal processing. As such, they tend to be Ô¨Åxed-point machines with a
high-speed hardware multiplier, but no divider unit. Floating-point arith-
metic tends to be avoided in this application context. In view of this,
what advantage might (7.P.4) have over (7.P.3) as a means to compute
square roots?
7.17. Review Problem 7.10. Use x = x0 + jx1 (x0, x1 ‚ààR) to rewrite the equation
f (x) = x3 ‚àí3x2 + 4x ‚àí2 = 0 in the form
f0(x0, x1) = 0,
f1(x0, x1) = 0.
TLFeBOOK

338
NONLINEAR SYSTEMS OF EQUATIONS
Use the Newton‚ÄìRaphson method (as implemented in MATLAB) to solve
this nonlinear system of equations for the complex roots of f (x) = 0. Use the
starting points x0 = [ x0,0
x0,1 ]T = [ 2
2 ]T and x0 = [ 2
‚àí2 ]T .
Output six iterations in both cases.
7.18. Consider the nonlinear system of equations
f (x, y) = x2 + y2 ‚àí1 = 0,
g(x, y) = 1
4x2 + 4y2 ‚àí1 = 0.
(a) Sketch f and g on the (x, y) plane.
(b) Solve for the points of intersection of the two curves in (a) by hand
calculation.
(c) Write a MATLAB function that uses the Newton‚ÄìRaphson method to
solve for the points of intersection. Use the starting vectors [x0y0]T =
[¬±1 ¬± 1]T . Output six iterations in all four cases.
7.19. If yn+1 = 1 ‚àíby2
n and xn =

1
4Œª ‚àí1
2

yn + 1
2 for n ‚ààZ+, then, if b =
1
4Œª2 ‚àí1
2Œª, show that
xn+1 = Œªxn(1 ‚àíxn).
7.20. De Angeli, et al. [18] suggest an alternative synchronization scheme (i.e.,
alternative to deadbeat synchronization). This works as follows. Suppose
that at the transmitter
x0,n+1 = 1 ‚àíŒ±x2
0,n + x1,n,
x1,n+1 = Œ≤x0,n,
yn = 1 ‚àíŒ±x2
0,n.
The expression for yn here replaces that in (7.86). At the receiver
ÀÜx0,n+1 = ÀÜx1,n + yn
ÀÜx1,n+1 = Œ≤ ÀÜx0,n.
(a) Error sequences are deÔ¨Åned as
Œ¥xi,n = ÀÜxi,n ‚àíxi,n
for i ‚àà{0, 1}. Find conditions on Œ± and Œ≤, giving limn‚Üí‚àû||Œ¥xn|| =
0
(Œ¥xn = [Œ¥x0,n
Œ¥x1,n]T ).
(b) Prove that using yn = 1 ‚àíŒ±x2
0,n to synchronize the receiver and trans-
mitter is not a secure synchronization method [i.e., an eavesdropper may
collect enough elements from (yn) to solve for the key {Œ±, Œ≤}].
TLFeBOOK

PROBLEMS
339
7.21. The chaotic encryption scheme of De Angeli et al. [18], which employs
deadbeat synchronization, was shown to be vulnerable to a known plaintext
attack, assuming a polynomial message sequence (recall Section 7.6). Show
that it is vulnerable to a known plaintext attack when the message sequence
is given by
sn = a sin(œân + œÜ).
Assuming that the eavesdropper already knows œâ and œÜ, show that a, Œ± and
Œ≤ can be obtained by solving a third-order linear system of equations. How
many cyphertext elements cn are needed to solve the system?
7.22. Write, compile, and run a C program to implement the De Angeli et al.
chaotic encryption/decryption scheme in Section 7.6. (C is suggested here,
as I am not certain that MATLAB can do the job so easily.) Implement both
the encryption and decryption algorithms in the same program. Keep the
program structure simple and direct (i.e., avoid complicated data structures,
and difÔ¨Åcult pointer operations). The program input is plaintext from a Ô¨Åle,
and the output is decrypted cyphertext (also known as ‚Äúrecovered plaintext‚Äù)
that is written to another Ô¨Åle. The user is to input the encryption key {Œ±, Œ≤},
and the decryption key {Œ±1, Œ≤1} at the terminal. Test your program out on
some plaintext Ô¨Åle of your own making. It should include keyboard charac-
ters other than letters of the alphabet and numbers. Of course, your program
must convert character data into a Ô¨Çoating-point format. The Ô¨Çoating-point
numbers are input to the encryption algorithm. Algorithm output is also a
sequence of Ô¨Çoating-point numbers. These are decrypted using the decryp-
tion algorithm, and the resulting Ô¨Çoating-point numbers are converted back
into characters. There is a complication involved in decryption. Recall that
nominally the plaintext is recovered according to
sn = yn+2
cn
for n = 0, 1, . . . , N ‚àí1. However, this is a Ô¨Çoating-point operation that
incurs rounding error. It is therefore necessary to implement
sn = yn+2
cn
+ offset
for which offset is a small positive value (e.g., 0.0001). The rationale is
as follows. Suppose that nominally (i.e., in the absence of roundoff) sn =
yn+2/cn = 113.000000. The number 113 is the ASCII (American Standard
Code for Information Interchange) code for some text character. Rounding in
division may instead yield the value 112.999999. The operation of converting
this number to an integer type will give 112 instead of 113. Clearly, the offset
cures this problem. A rounding error that gives, for instance, 113.001000 is
harmless. We mention that if plaintext (sn) is from sampled voice or video,
TLFeBOOK

340
NONLINEAR SYSTEMS OF EQUATIONS
then these rounding issues are normally irrelevant. Try using the following
key sets:
Œ± = 1.399, Œ≤ = 0.305,
Œ±1 = 1.399, Œ≤1 = 0.305,
and
Œ± = 1.399, Œ≤ = 0.305,
Œ±1 = 1.389, Œ≤1 = 0.304.
Of course, perfect recovery is expected for the Ô¨Årst set, but not the second set.
[Comment: It is to be emphasized that in practice keys must be chosen that
cause the H¬¥enon map to be chaotic to good approximation on the computer.
Keys must not lead to an unstable system, a system that converges to a
Ô¨Åxed point, or to one with a short-period oscillation. Key choices leading to
instability will cause Ô¨Çoating-point overÔ¨Çow (leading to a crash), while the
other undesirable choices likely generate security hazards. Ideally (and more
practically) the cyphertext array (Ô¨Çoating-point numbers) should be written to
a Ô¨Åle by an encryption program. A separate decryption program would then
read in the cyphertext from the Ô¨Åle and decrypt it, producing the recovered
plaintext, which would be written to another Ô¨Åle (i.e., three separate Ô¨Åles:
original Ô¨Åle of input plaintext, cyphertext Ô¨Åle, and recovered plaintext Ô¨Åle).]
TLFeBOOK

8
Unconstrained Optimization
8.1
INTRODUCTION
In engineering design it is frequently the case that an optimal design is sought
with respect to some performance criterion or criteria. Problems of this class are
generally referred to as optimal design problems, or mathematical programming
problems. Frequently nonlinearity is involved, and so the term nonlinear program-
ming also appears. There are many subcategories of problem types within this broad
category, and so space limitations will restrict us to an introductory presentation
mainly of so-called unconstrained problems. Even so, only a very few ideas from
within this category will be considered. However, some of the methods treated in
earlier chapters were actually examples of optimal design methods within this cate-
gory. This would include least-squares ideas, for example. In fact, an understanding
of least-squares ideas from the previous chapters helps a lot in understanding the
present one. Although the emphasis here is on unconstrained optimization problems,
some consideration of constrained problems appears in Section 8.5.
8.2
PROBLEM STATEMENT AND PRELIMINARIES
In this chapter we consider the problem
min
x‚ààRn f (x)
(8.1)
for which f (x) ‚ààR. This notation means that we wish to Ô¨Ånd a vector x that min-
imizes the function f (x). We shall follow previous notational practices, so here
x = [x0 x1 . . . xn‚àí2 xn‚àí1]T . In what follows we shall usually assume that f (x)
possesses all Ô¨Årst- and second-order partial derivatives (with respect to the elements
of x), and that these are continuous functions, too. In the present context the func-
tion f (x) is often called the objective function. For example, in least-squares prob-
lems (recall Chapter 4) we sought to minimize V (a) = ||f (x) ‚àíN‚àí1
k=0 akœÜk(x)||2
with respect to a ‚ààRN. Thus, V (a) is an objective function. The least-squares
problem had sufÔ¨Åcient structure so that a simple solution was arrived at; speciÔ¨Å-
cally, to Ô¨Ånd the optimal vector a (denoted ÀÜa), all we had to do was solve a linear
system of equations.
An Introduction to Numerical Analysis for Electrical and Computer Engineers, by C.J. Zarowski
ISBN 0-471-46737-5
c‚Éù2004 John Wiley & Sons, Inc.
341
TLFeBOOK

342
UNCONSTRAINED OPTIMIZATION
Now we are interested in solving more general problems. For example, we might
wish to Ô¨Ånd x = [x0x1]T to minimize Rosenbrock‚Äôs function
f (x) = 100(x1 ‚àíx2
0)2 + (1 ‚àíx0)2.
(8.2)
This is a famous standard test function (taken here from Fletcher [1]) often used
by those who design optimization algorithms to test out their theories. A contour
plot of this function appears in Fig. 8.1. It turns out that this function has a unique
minimum at x = ÀÜx = [1
1]T . As before, we have used a ‚Äúhat‚Äù symbol to indicate
the optimal solution.
Some ideas from vector calculus are essential to understanding nonlinear opti-
mization problems. Of particular importance is the gradient operator:
‚àá=
 ‚àÇ
‚àÇx0
‚àÇ
‚àÇx1
¬∑ ¬∑ ¬∑
‚àÇ
‚àÇxn‚àí1
T
.
(8.3)
For example, if we apply this operator to Rosenbrock‚Äôs function, then
‚àáf (x) =
Ô£Æ
Ô£ØÔ£ØÔ£∞
‚àÇf (x)
‚àÇx0
‚àÇf (x)
‚àÇx1
Ô£π
Ô£∫Ô£∫Ô£ª=
 ‚àí400x0(x1 ‚àíx2
0) ‚àí2(1 ‚àíx0)
200(x1 ‚àíx2
0)

.
(8.4)
‚àí1
‚àí0.8
‚àí0.6
‚àí0.4
‚àí0.2
0
0.2
0.4
0.6
0.8
1
‚àí0.2
0
0.2
0.4
0.6
0.8
1
x0
x1
Figure 8.1
Contour plot of Rosenbrock‚Äôs function.
TLFeBOOK

PROBLEM STATEMENT AND PRELIMINARIES
343
We observe that ‚àáf (ÀÜx) = [0
0]T (recall that ÀÜx = [1
1]T ). In other words, the
gradient of the objective function at ÀÜx is zero. Intuitively we expect that if ÀÜx is
to minimize the objective function, then we should always have ‚àáf (ÀÜx) = 0. This
turns out to be a necessary but not sufÔ¨Åcient condition for a minimum. To see this,
consider g(x) = ‚àíx2(x ‚ààR) for which
‚àág(x) = ‚àí2x.
Clearly, ‚àág(0) = 0 yet x = 0 maximizes g(x) instead of minimizing it. Consider
h(x) = x3, so that
‚àáh(x) = 3x2
for which ‚àáh(0) = 0, but x = 0 neither minimizes nor maximizes h(x). In this
case x = 0 corresponds to a one-dimensional version of a saddle point.
Thus, Ô¨Ånding x such that ‚àáf (x) = 0 generally gives us minima, maxima, or
saddle points. These are collectively referred to as stationary points. We need (if
possible) a condition that tells us whether a given stationary point corresponds to
a minimum. Before considering this matter, we note that another problem is that
a given objective function will often have several local minima. This is illustrated
in Fig. 8.2. The function is a quartic polynomial for which there are two values of
‚àí2
‚àí1.5
‚àí1
‚àí0.5
0
0.5
1
1.5
2
2.5
3
‚àí5
0
5
10
15
20
25
30
35
40
x
f(x)
Figure 8.2
A function f (x) = x4 ‚àí3.1x3 + 2x + 1 with two local minima. The local
minimizers are near x = ‚àí0.5 and x = 2.25. The minimum near x = 2.25 is the unique
global minimum of f (x).
TLFeBOOK

344
UNCONSTRAINED OPTIMIZATION
x such that ‚àáf (x) = 0. We might denote these by ÀÜxa and ÀÜxb. Suppose that ÀÜxa is the
local minimizer near x = ‚àí0.5 and ÀÜxb is the local minimizer near x = 2.25. We see
that f (ÀÜxa + Œ¥) > f (ÀÜxa) for all sufÔ¨Åciently small (but nonzero) Œ¥ ‚ààR. Similarly,
f (ÀÜxb + Œ¥) > f (ÀÜxb) for all sufÔ¨Åciently small (but nonzero) Œ¥ ‚ààR. But we see that
f (ÀÜxb) < f (ÀÜxa), so ÀÜxb is the unique global minimizer for f (x). An algorithm for
minimizing f (x) should seek ÀÜxb; that is, in general, we seek the global minimizer
(assuming that it is unique). Except in special situations, an optimization algorithm
is usually not guaranteed to do better than Ô¨Ånd a local minimizer, and is not
guaranteed to Ô¨Ånd a global minimizer. Note that it is entirely possible for the
global minimizer not to be unique. A simple example is f (x) = sin(x) for which
the stationary points are x = (2k + 1)œÄ/2 (k ‚ààZ). The minimizers that are a subset
of these points all give ‚àí1 for the value of sin(x).
To determine whether a stationary point is a local minimizer, it is useful to have
the Hessian matrix
‚àá2f (x) =
‚àÇ2f (x)
‚àÇxi‚àÇxj

i,j=0,1,...,n‚àí1
‚ààRn√ón.
(8.5)
This matrix should not be confused with the Jacobian matrix (seen in Section 7.5.2).
The Jacobian and Hessian matrices are not the same. The veracity of this may be
seen by comparing their deÔ¨Ånitions. For the special case of n = 2, we have the
2 √ó 2 Hessian matrix
‚àá2f (x) =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
‚àÇ2f (x)
‚àÇx2
0
‚àÇ2f (x)
‚àÇx0‚àÇx1
‚àÇ2f (x)
‚àÇx1‚àÇx0
‚àÇ2f (x)
‚àÇx2
1
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª.
(8.6)
If f (x) in (8.6) is Rosenbrock‚Äôs function, then (8.6) becomes
‚àá2f (x) =

1200x2
0 ‚àí400x1 + 2
‚àí400x0
‚àí400x0
200

.
(8.7)
The Hessian matrix for f (x) helps in the following manner. Suppose that ÀÜx is
a local minimizer for objective function f (x). We may Taylor-series-expand f (x)
around ÀÜx according to
f (ÀÜx + h) = f (ÀÜx) + hT ‚àáf (ÀÜx) + 1
2hT [‚àá2f (ÀÜx)]h + ¬∑ ¬∑ ¬∑ ,
(8.8)
where h ‚ààRn. (This will not be formally justiÔ¨Åed.) If f (x) is sufÔ¨Åciently smooth
and ||h|| is sufÔ¨Åciently small, then the terms in (8.8) that are not shown may
be entirely ignored (i.e., we neglect higher-order terms). In other words, in the
neighborhood of ÀÜx
f (ÀÜx + h) ‚âàf (ÀÜx) + hT ‚àáf (ÀÜx) + 1
2hT [‚àá2f (ÀÜx)]h
(8.9)
TLFeBOOK

LINE SEARCHES
345
is assumed. But this is the now familiar quadratic form. For convenience we will
(as did Fletcher [1]) deÔ¨Åne
G(x) = ‚àá2f (x),
(8.10)
so (8.9) may be rewritten as
f (ÀÜx + h) ‚âàf (ÀÜx) + hT ‚àáf (ÀÜx) + 1
2hT G(ÀÜx)h.
(8.11)
Sometimes we will write G instead of G(x) if there is no danger of confusion.
In Chapter 4 we proved that (8.11) has a unique minimum iff G > 0. In words,
f (x) looks like a positive deÔ¨Ånite quadratic form in the neighborhood of a local
minimizer. Therefore, the Hessian of f (x) at a local minimizer will be positive
deÔ¨Ånite and thus represents a way of testing a stationary point to see if it is a
minimizer. (Recall the second-derivative test for the minimum of a single-variable
function from elementary calculus, which is really just a special case of this more
general test.) More formally, we therefore have the following theorem.
Theorem 8.1: A sufÔ¨Åcient condition for a local minimizer ÀÜx is that ‚àáf (ÀÜx) = 0,
and G(ÀÜx) > 0.
This is a simpliÔ¨Åed statement of Fletcher‚Äôs Theorem 2.1.1 [1, p. 14]. The proof
is really just a more rigorous version of the Taylor series argument just given, and
will therefore be omitted. For convenience we will also deÔ¨Åne the gradient vector
g(x) = ‚àáf (x).
(8.12)
Sometimes we will write g for g(x) if there is no danger of confusion.
If we recall Rosenbrock‚Äôs function again, we may now test the claim made earlier
that ÀÜx = [1
1]T is a minimizer for f (x) in (8.2). For Rosenbrock‚Äôs function the
Hessian is in (8.7), and thus we have
G(ÀÜx) =

802
‚àí400
‚àí400
200

.
(8.13)
The eigenvalues of this matrix are Œª = 0.3994, 1002. These are both bigger than
zero so G(ÀÜx) > 0. We have already remarked that g(ÀÜx) = 0, so immediately from
Theorem 8.1 we conclude that ÀÜx = [1
1]T is a local minimizer for Rosenbrock‚Äôs
function.
8.3
LINE SEARCHES
In general, for objective function f (x) we wish to allow x ‚ààRn; that is, we seek the
minimum of f (x) by performing a search in an n-dimensional vector space. How-
ever, the one-dimensional problem is an important special case. In this section we
begin by considering n = 1, so x is a scalar. The problem of Ô¨Ånding the minimum
TLFeBOOK

346
UNCONSTRAINED OPTIMIZATION
of f (x) [where f (x) ‚ààR for all x ‚ààR] is sometimes called the univariate search.
Various approaches exist for the solution of this problem, but we will consider only
the golden ratio search method (sometimes also called the golden section search).
We will then consider the backtracking line search [3] for the case where x ‚ààRn
for any n ‚â•1.
Suppose that we know f (x) has a minimum over the interval [xj
l , xj
u]. DeÔ¨Åne
I j = xj
u ‚àíxj
l , which is the length of this interval. The index j represents the jth
iteration of the search algorithm, so it represents the current estimate of the interval
that contains the minimum. Select two points xj
a and xj
b (xj
a < xj
b) such that they
are symmetrically placed in the interval [xj
l , xj
u]. SpeciÔ¨Åcally
xj
a ‚àíxj
l = xj
u ‚àíxj
b.
(8.14)
A new interval [xj+1
l
, xj+1
u
] is created according to the following procedure, such
that for all j
I j
I j+1 = œÑ > 1.
(8.15)
If f (xj
a) ‚â•f (xj
b) then the minimum lies in [xj
a, xj
u], and so I j+1 = xj
u ‚àíxj
a. Our
new points are given according to
xj+1
l
= xj
a,
xj+1
u
= xj
u,
xj+1
a
= xj
b
(8.16a)
and
xj+1
b
= xj
a + 1
œÑ I j+1.
(8.16b)
If f (xj
a) < f (xj
b), then the minimum lies in [xj
l , xj
b], and so I j+1 = xj
b ‚àíxj
l . Our
new points are given according to
xj+1
l
= xj
l ,
xj+1
u
= xj
b,
xj+1
b
= xj
a
(8.17a)
and
xj+1
a
= xj
b ‚àí1
œÑ I j+1.
(8.17b)
Since I 0 = x0
u ‚àíx0
l (j = 0 indicates the initial case), we must have x0
a = x0
u ‚àí1
œÑ I 0
and x0
b = x0
l + 1
œÑ I 0. Figure 8.3 illustrates the search procedure. This is for the
particular case (8.17).
Because of (8.15), the rate of convergence to the minimum can be estimated.
But we need to know œÑ; see the following theorem.
Theorem 8.2: The search interval lengths of the golden ratio search algorithm
are related according to
I j = I j+1 + I j+2
(8.18a)
TLFeBOOK

LINE SEARCHES
347
f(x)
x
0
x j
l
x j
a
x j
b
x j
u
x j
l
+1
x j
b
+1
x j
a
+1
x j
u
+1
Figure 8.3
Illustrating the golden ratio search procedure.
for which the golden ratio œÑ is given by
œÑ = 1
2(1 +
‚àö
5) ‚âà1.62.
(8.18b)
Proof
There are four cases to consider in establishing (8.18a). First, suppose
that f (xj
a) ‚â•f (xj
b), so I j+1 = xj
u ‚àíxj
a with
xj+1
l
= xj
a,
xj+1
a
= xj
b,
xj+1
b
= xj
a + 1
œÑ I j+1,
xj+1
u
= xj
u.
If it happens that f (xj+1
a
) ‚â•f (xj+1
b
), then I j+2 = xj+1
u
‚àíxj+1
a
= xj
u ‚àíxj
b =
xj
a ‚àíxj
l (via xj
a ‚àíxj
l = xj
u ‚àíxj
b). This implies that I j+1 + I j+2 = xj
u ‚àíxj
l = I j.
On the other hand, if it happens that f (xj+1
a
) < f (xj+1
b
), then I j+2 = xj+1
b
‚àí
xj+1
l
= xj+1
u
‚àíxj+1
a
= xj
a ‚àíxj
l (via xj+1
a
‚àíxj+1
l
= xj+1
u
‚àíxj+1
b
and xj
a ‚àíxj
l =
xj
u ‚àíxj
b). Again we have I j+1 + I j+2 = xj
u ‚àíxj
l = I j.
Now suppose that f (xj
a) < f (xj
b). Therefore, I j+1 = xj
b ‚àíxj
l with
xj+1
l
= xj
l ,
xj+1
a
= xj
b ‚àí1
œÑ I j+1,
xj+1
b
= xj
a,
xj+1
u
= xj
b.
If it happens that f (xj+1
a
) ‚â•f (xj+1
b
), then I j+2 = xj+1
u
‚àíxj+1
a
= xj+1
b
‚àí
xj+1
l
= xj
a ‚àíxj
l = xj
u ‚àíxj
b so that I j+1 + I j+2 = xj
u ‚àíxj
l = I j. Finally, suppose
that f (xj+1
a
) < f (xj+1
b
), so therefore I j+2 = xj+1
b
‚àíxj+1
l
= xj
a ‚àíxj
l = xj
u ‚àíxj
b,
TLFeBOOK

348
UNCONSTRAINED OPTIMIZATION
so yet again we conclude that I j+1 + I j+2 = xj
u ‚àíxj
l = I j. Thus, (8.18a) is
now veriÔ¨Åed.
Since
I j
I j+1 = I j+1
I j+2 = œÑ
and
I j = I j+1 + I j+2,
we have I j+1 = 1
œÑ I j and I j+2 = 1
œÑ I j+1 = 1
œÑ 2 I j, so
I j = I j+1 + I j+2 = 1
œÑ I j + 1
œÑ 2 I j,
immediately implying that
œÑ 2 = œÑ + 1,
which yields (8.18b).
The golden ratio has a long and interesting history in art as well as science
and engineering, and this is considered in Schroeder [2]. For example, a famous
painting by Seurat contains Ô¨Ågures that are proportioned according to this ratio
(Fig. 5.2 in Schroeder [2] includes a sketch).
The golden ratio search algorithm as presented so far assumes a user-provided
starting interval. The golden ratio search algorithm has the same drawback as the
bisection method for root Ô¨Ånding (Chapter 7) in that the optimum solution must be
bracketed before the algorithm can be successfully run (in general). On the other
hand, an advantage of the golden ratio search method is that it does not need f (x)
to be differentiable. But the method can be slow to converge, in which case an
improved minimizer that also does not need derivatives can be found in Brent [4].
When setting up an optimization problem it is often advisable to look for ways
to reduce the dimensionality of the problem, if at all possible. We illustrate this
idea with an example that is similar in some ways to the least-squares problem
considered in the beginning of Section 4.6. Suppose that signal f (t) ‚ààR (t ‚ààR)
is modeled as
f (t) = a sin
 2œÄ
T t + œÜ
!
+ Œ∑(t),
(8.19)
where Œ∑(t) is the term that accounts for noise, interference, or measurement errors
in the data f (t). In other words, our data are modeled as a sinusoid plus noise.
The amplitude a, phase œÜ, and period T are unknown parameters. We are assumed
only to possess samples of the signal; thus, we have only the sequence elements
fn = f (nTs) = a sin
 2œÄ
T nTs + œÜ
!
+ Œ∑(nTs)
(8.20)
for n = 0, 1, . . . , N ‚àí1. The sampling period parameter Ts is assumed to be
known. As before, we may deÔ¨Åne an error sequence
en = fn ‚àía sin
 2œÄ
T nTs + œÜ
!
#
$%
&
= ÀÜfn
,
(8.21)
TLFeBOOK

LINE SEARCHES
349
where again n = 0, 1, . . . , N ‚àí1. The objective function for our problem (using a
least-squares criterion) would therefore be
V (a, T, œÜ) =
N‚àí1

n=0

fn ‚àía sin
 2œÄ
T nTs + œÜ
!2
.
(8.22)
This function depends on the unknown model parameters a, T , and œÜ in a very non-
linear manner. We might formally deÔ¨Åne our parameter vector to be x = [a T œÜ]T ‚àà
R3. This would lead us to conclude that we must search (by some means) a three-
dimensional space to Ô¨Ånd ÀÜx to minimize (8.22). But in this special problem it is
possible to reduce the dimensionality of the search space from three dimensions
down to only one dimension. Reducing the problem in this way makes it solvable
using the golden section search method that we have just considered.
Recall the trigonometric identity
sin(A + B) = sin A cos B + cos A sin B.
(8.23)
Using this, we may write
a sin
 2œÄ
T nTs + œÜ
!
= a cos œÜ
# $% &
=x0
sin
 2œÄ
T nTs
!
+ a sin œÜ
# $% &
=x1
cos
 2œÄ
T nTs
!
.
(8.24)
DeÔ¨Åne x = [x0
x1]T and
vn =

sin
 2œÄ
T nTs
!
cos
 2œÄ
T nTs
!T
.
(8.25)
We may rewrite en in (8.21) using these vectors:
en = fn ‚àívT
n x.
(8.26)
Note that the approach used here is the same as that used to obtain en in Eqn. (4.99).
Therefore, the reader will probably Ô¨Ånd it useful to review this material now.
Continuing in this fashion, the error vector e = [e0e1 ¬∑ ¬∑ ¬∑ eN‚àí1]T , data vector f =
[f0f1 ¬∑ ¬∑ ¬∑ fN‚àí1]T , and matrix of basis vectors
A =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£∞
vT
0
vT
1...
vT
N‚àí1
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
‚ààRN√ó2
all may be used to write
e = f ‚àíAx,
(8.27)
TLFeBOOK

350
UNCONSTRAINED OPTIMIZATION
for which our objective function may be rewritten as
V (x) = eT e = f T f ‚àí2xT AT f + xT AT Ax,
(8.28)
which implicitly assumes that we already know the period T . If T were known
in advance, we could use the method of Chapter 4 to minimize (8.28); that is,
the optimum choice (least-squares sense) for x (denoted by ÀÜx) satisÔ¨Åes the linear
system
AT AÀÜx = AT f.
(8.29)
Because of (8.24) (with ÀÜx = [ÀÜx0
ÀÜx1]T ), we obtain
ÀÜx0 = ÀÜa cos ÀÜœÜ,
ÀÜx1 = ÀÜa sin ÀÜœÜ.
(8.30)
Since we have ÀÜx from the solution to (8.29), we may use (8.30) to solve for ÀÜa
and ÀÜœÜ.
However, our original problem speciÔ¨Åed that we do not know a, œÜ, or T in
advance. So, how do we exploit these results to determine T as well as a and œÜ?
The approach is to change how we think about V (x) in (8.28). Instead of thinking
of V (x) as a function of x, consider it to be a function of T , but with x = ÀÜx as
given by (8.29). From (8.25) we see that vn depends on T , so A is also a function
of T . Thus, ÀÜx is a function of T , too, because of (8.29). In fact, we may emphasize
this by rewriting (8.29) as
[A(T )]T A(T )ÀÜx(T ) = [A(T )]T f.
(8.31)
In other words, A = A(T ), and ÀÜx = ÀÜx(T ). The objective function V (x) then becomes
V1(T ) = V (ÀÜx(T )) = f T f ‚àí2[ÀÜx(T )]T AT (T )f + [ÀÜx(T )]T AT (T )A(T )ÀÜx(T ).
(8.32)
However, we may substitute (8.31) into (8.32) and simplify with the result that
V1(T ) = f T f ‚àíf T A(T )[AT (T )A(T )]‚àí1AT (T )f.
(8.33)
We have reduced the search space from three dimensions down to one, so we
may apply the golden section search algorithm (or some other univariate search
procedure) to objective function V1(T ). This would result in determining the opti-
mum period ÀÜT [which minimizes V1(T )]. We then compute A( ÀÜT ), and solve
for ÀÜx using (8.29) as before. Knowledge of ÀÜx allows us to determine ÀÜa and ÀÜœÜ
via (8.30).
Figure 8.4 illustrates a typical noisy sinusoid and the corresponding objective
function V1(T ). In this case the parameters chosen were T = 24 h, Ts = 5 min,
a = 1, œÜ = ‚àíœÄ/10 radians, and N = 500. The noise component in the data was
created using MATLAB‚Äôs Gaussian random-number generator. The noise variance
is œÉ 2 = 0.5, and the mean value is zero. We observe that the minimum value of
V1(T ) certainly corresponds to a value of T that is at or close to 24 h.
TLFeBOOK

LINE SEARCHES
351
0
5
10
15
20
25
30
35
40
45
‚àí3
‚àí2
‚àí1
0
1
2
3
4
Time (hours)
fn
16
18
20
22
24
26
28
30
32
250
300
350
400
450
500
Time (hours)
V1(T )
(a)
(b)
Figure 8.4
An example of a noisy sinusoid (a) and the corresponding objective function
V1(T ) (b).
Appendix 8.A contains a sample MATLAB program implementation of the
golden section search algorithm applied to the noisy sinusoid problem depicted
in Fig. 8.4. In golden.m Topt is ÀÜT , and eta Œ∑ is the random noise sequence Œ∑(nTs).
We will now consider the backtracking line search algorithm. The exposition
will be similar to that in Boyd and Vandenberghe [5].
Now we assume that f (x) is our objective function with x ‚ààRn. In a general
line search we seek the minimizing vector sequence (x(k)), k ‚ààZ+, and x(k) ‚ààRn
(i.e., x(k) = [x(k)
0
x(k)
1
¬∑ ¬∑ ¬∑ x(k)
n‚àí1]T ) constructed according to the iterative process
x(k+1) = x(k) + t(k)s(k),
(8.34)
where t(k) ‚ààR+, and t(k) > 0 except when x(k) is optimal [i.e., minimizes f (x)].
The vector s(k) is called the search direction, and scalar t(k) is called the step size.
Because line searches (8.34) are descent methods, we have
f (x(k+1)) < f (x(k))
(8.35)
except when x(k) is optimal. The ‚Äúpoints‚Äù x(k+1) lie along a line in the direction
s(k) in n-dimensional space Rn, and since the minimum of f (x) must lie in the
TLFeBOOK

352
UNCONSTRAINED OPTIMIZATION
direction that satisÔ¨Åes (8.35), we must ensure that s(k) satisÔ¨Åes [recall (8.12)]
g(x(k))T s(k) = [‚àáf (x(k))]T s(k) < 0.
(8.36)
Geometrically, the negative-gradient vector ‚àíg(x(k)) (which ‚Äúpoints down‚Äù) makes
an acute angle (i.e., one of magnitude <90‚ó¶) with the vector s(k). [Recall (4.130).]
If s(k) satisÔ¨Åes (8.36) for f (x(k)) (i.e., f (x) at x = x(k)), it is called a descent direc-
tion for f (x) at x(k). A general descent algorithm has the following pseudocode
description:
Specify starting point x(0) ‚ààRn;
k := 0;
while stopping criterion is not met do begin
Find s(k); { determine descent direction }
Find t(k); { line search step }
Compute x(k+1) := x(k) + t(k)s(k);
{ update step }
k := k + 1;
end ;
Newton‚Äôs method with the backtracking line search (Section 8.4) is a speciÔ¨Åc
example of a descent method. There are others, but these will not be considered in
this book.
Now we need to say more about how to choose the step size t(k) on the assump-
tion that s(k) is known. How to determine the direction s(k) is the subject of
Section 8.4.
So far f |Rn ‚ÜíR. Subsequent considerations are simpliÔ¨Åed if we assume that
f (x) satisÔ¨Åes the following deÔ¨Ånition.
DeÔ¨Ånition 8.1: Convex Function
Function f |Rn ‚ÜíR is called a convex
function if for all Œ∏ ‚àà[0, 1], and for any x, y ‚ààRn
f (Œ∏x + (1 ‚àíŒ∏)y) ‚â§Œ∏f (x) + (1 ‚àíŒ∏)f (y).
(8.37)
We emphasize that the domain of deÔ¨Ånition of f (x) is assumed to be Rn. It is
possible to modify the deÔ¨Ånition to accommodate f (x) where the domain of f (x)
is a proper subset of Rn. The geometric meaning of (8.37) is shown in Fig. 8.5
for the case where x ‚ààR (i.e., one-dimensional case). We see that when f (x)
is convex, the chord, which is the line segment joining (x, f (x)) to (y, f (y)),
always lies above the graph of f (x). Now further assume that f (x) is at least
twice continuously differentiable in all elements of the vector x. We observe that
for any x, y ‚ààRn, if f (x) is convex, then
f (y) ‚â•f (x) + [‚àáf (x)]T (y ‚àíx).
(8.38)
From the considerations of Section 8.2 it is easy to believe that f (x) is convex iff
‚àá2f (x) ‚â•0
(8.39)
TLFeBOOK

NEWTON‚ÄôS METHOD
353
x
f(x)
0
(x, f(x))
(y, f(y))
Figure 8.5
Graph of a convex function f (x) (x ‚ààR) and the chord that connects the points
(x, f (x)) and (y, f (y)).
for all x ‚ààRn; that is, f (x) is convex iff its Hessian matrix is at least positive
semideÔ¨Ånite (recall DeÔ¨Ånition 4.1). The function f (x) is said to be strongly convex
iff ‚àá2f (x) > 0 for all x ‚ààRn.
The backtracking line search attempts to approximately minimize f (x) along
the line {x + ts|t ‚â•0} for some given s (search direction at x). The pseudocode
for this algorithm is as follows:
Specify the search direction s;
t := 1;
while (f(x + ts) > f(x) + Œ¥t[‚àáf(x)]Ts)
t := Œ±t;
end ;
In this algorithm 0 < Œ¥ < 1
2, and 0 < Œ± < 1. Commonly, Œ¥ ‚àà[0.01, 0.30], and
Œ± ‚àà[0.1, 0.5]. These parameter ranges will not be justiÔ¨Åed here. As suggested
earlier, how to choose s will be the subject of the next section. The method is
called ‚Äúbacktracking‚Äù as it begins with t = 1, and then reduces t by factor Œ± until
f (x + ts) ‚â§f (x) + Œ¥t‚àáT f (x)s. [We have ‚àáT f (x) = [‚àáf (x)]T .] Recall that s
is a descent direction so that (8.36) holds, speciÔ¨Åcally, ‚àáT f (x)s < 0, and so if t
is small enough [recall (8.9)], then
f (x + ts) ‚âàf (x) + t‚àáT f (x)s < f (x) + Œ¥t‚àáT f (x)s,
which shows that the search must terminate eventually. We mention that the back-
tracking line search will terminate even if f (x) is only ‚Äúlocally convex‚Äù‚Äîconvex
on some proper subset S of Rn. This will happen provided x ‚ààS in the algorithm.
8.4
NEWTON‚ÄôS METHOD
Section 8.3 suggests attempting to reduce an n-dimensional search space to a
one-dimensional search space. Of course, this approach seldom works, which is
TLFeBOOK

354
UNCONSTRAINED OPTIMIZATION
why there is an elaborate body of methods on searching for minima in higher-
dimensional spaces. However, these methods are too involved to consider in detail
in this book, and so we will only partially elaborate on an idea from Section 8.2.
The quadratic model from Section 8.2 suggests an approach often called New-
ton‚Äôs method. Suppose that x(k) is the current estimate of the sought-after optimum
ÀÜx. Following (8.11), we have the Taylor approximation
f (x(k) + Œ¥) ‚âàV (Œ¥) = f (x(k)) + Œ¥T g(x(k)) + 1
2Œ¥T G(x(k))Œ¥
(8.40)
for which Œ¥ ‚ààRn since x(k) ‚ààRn. Since x(k) is not necessarily the minimum ÀÜx,
usually g(x(k)) Ã∏= 0. Vector Œ¥ is selected to minimize V (Œ¥), and since this is a
quadratic form, if G(x(k)) > 0, then
G(x(k))Œ¥ = ‚àíg(x(k)).
(8.41)
The next estimate of ÀÜx is given by
x(k+1) = x(k) + Œ¥.
(8.42)
Pseudocode for Newton‚Äôs method (in its most basic form) is
Input starting point x(0);
k := 0;
While stopping criterion is not met do begin
G(x(k))Œ¥ := ‚àíg(x(k));
x(k+1) := x(k) + Œ¥;
k := k + 1;
end;
The algorithm will terminate (if all goes well) with the last vector x(k+1) as a good
approximation to ÀÜx. However, the Hessian G(k) = G(x(k)) may not always be
positive deÔ¨Ånite, in which case this method can be expected to fail. ModiÔ¨Åcation
of the method is required to guarantee that at least it will converge to a local
minimum. Said modiÔ¨Åcations often involve changing the method to work with line
searches (i.e., Section 8.3 ideas). We will now say more about this.
As suggested in Ref. 5, we may combine the backtracking line search with the
basic form of Newton‚Äôs algorithm described above. A pseudocode description of
the result is
Input starting point x(0), and a tolerance œµ > 0;
k := 0;
s(0) := ‚àí[G(x(0))]‚àí1g(x(0)); { search direction at x(0)}
Œª2 := ‚àígT(x(0))s(0);
while Œª2 > œµ do begin
Use backtracking line search to find t(k) for x(k) and s(k);
x(k+1) := x(k) + t(k)s(k);
s(k+1) := ‚àí[G(x(k+1))]‚àí1g(x(k+1));
Œª2 := ‚àígT(x(k+1))s(k+1);
k := k + 1;
end;
TLFeBOOK

NEWTON‚ÄôS METHOD
355
The algorithm assumes that G(x(k)) > 0 for all k. In this case [G(x(k))]‚àí1 > 0 as
well. If we deÔ¨Åne (for all x ‚ààRn)
||x||2
G(y) = xT [G(y)]‚àí1x,
(8.43)
then ||x||G(y) satisÔ¨Åes the norm axioms (recall DeÔ¨Ånition 1.3), and is in fact an
example of a weighted norm. But why do we consider Œª2 as a stopping criterion in
Newton‚Äôs algorithm? If we recall the term 1
2Œ¥T G(x(k))Œ¥ in (8.40), since Œ¥ satisÔ¨Åes
(8.41), at step k we must have
1
2Œ¥T G(x(k))Œ¥ = 1
2gT (x(k))[G(x(k))]‚àí1g(x(k)) = 1
2||g(x(k))||2
G(x(k)) = 1
2Œª2. (8.44)
Estimate x(k) of ÀÜx is likely to be good if (8.44) in particular is small [as opposed
to merely considering squared unweighted norm g(x(k))T g(x(k))]. It is known that
Newton‚Äôs algorithm can converge rapidly (quadratic convergence). An analysis
showing this appears in Boyd and Vandenberghe [5] but is omitted here.
As it stands, Newton‚Äôs method is computationally expensive since we must solve
the linear system in (8.41) at every step k. This would normally involve applying
the Cholesky factorization algorithm that was Ô¨Årst mentioned (but not considered
in detail) in Section 4.6. We remark in passing that the Cholesky algorithm will
factorize G(k) according to
G(k) = LDLT ,
(8.45)
where L is unit lower triangular and D is a diagonal matrix. We also mention that
G(k) > 0 iff the elements of D are all positive, so the Cholesky algorithm provides
a built-in positive deÔ¨Åniteness test. The decomposition in (8.45) is a variation on
the LU decomposition of Chapter 4. Recall that we proved in Section 4.5 that
positive deÔ¨Ånite matrices always possess such a factorization (see Theorems 4.1
and 4.2). So Eq. (8.45) is consistent with this result. The necessity to solve a linear
system of equations at every step makes us wonder if sensitivity to ill-conditioned
matrices is a problem in Newton‚Äôs method. It turns out that the method is often
surprisingly resistant to ill conditioning (at least as reported in Ref. 5).
Example 8.1
A typical run of Newton‚Äôs algorithm with the backtracking line
search as applied to the problem of minimizing Rosenbrock‚Äôs function [Eq. (8.2)]
yields the following output:
k
t(k)
Œª2
x(k)
0
x(k)
1
0
1.0000
800.00499
2.0000
2.0000
1
0.1250
1.98757
1.9975
3.9900
2
1.0000
0.41963
1.8730
3.4925
3
1.0000
0.49663
1.6602
2.7110
4
0.5000
0.38333
1.5945
2.5382
5
1.0000
0.21071
1.4349
2.0313
TLFeBOOK

356
UNCONSTRAINED OPTIMIZATION
k
t(k)
Œª2
x(k)
0
x(k)
1
6
0.5000
0.14763
1.3683
1.8678
7
1.0000
0.07134
1.2707
1.6031
8
1.0000
0.03978
1.1898
1.4092
9
1.0000
0.01899
1.1076
1.2201
10
1.0000
0.00627
1.0619
1.1255
11
1.0000
0.00121
1.0183
1.0350
12
1.0000
0.00006
1.0050
1.0099
The search parameters selected in this example are Œ± = 0.5, Œ¥ = 0.3, and œµ =
.00001, and the Ô¨Ånal estimate is x(13) = [1.0002
1.0003]T . For these same param-
eters if instead x(0) = [‚àí1
1]T , then 18 iterations are needed, yielding x(18) =
[0.9999
0.9998]T . Figure 8.6 shows the sequence of points x(k) for the case
x(0) = [‚àí1
1]T . The dashed line shows the path from starting point [‚àí1
1]T to
the minimum at [1
1]T , and we see that the algorithm follows the ‚Äúvalley‚Äù to the
optimum solution quite well.
‚àí1
‚àí0.8
‚àí0.6
‚àí0.4
‚àí0.2
0
0.2
0.4
0.6
0.8
1
‚àí0.2
0
0.2
0.4
0.6
0.8
1
x0
x1
Figure 8.6
The sequence of points (x(k)) generated by Newton‚Äôs method with the back-
tracking line search as applied to Rosenbrock‚Äôs function using the parameters Œ± = 0.5,
Œ¥ = 0.3, and œµ = 0.00001 with x(0) = [‚àí1
1]T (see Example 8.1). The path followed is
shown by the dashed line.
TLFeBOOK

EQUALITY CONSTRAINTS AND LAGRANGE MULTIPLIERS
357
8.5
EQUALITY CONSTRAINTS AND LAGRANGE MULTIPLIERS
In this section we modify the original optimization problem in (8.1) according to
min
x‚ààRn f (x)
subject tofi(x) = 0
for all
i = 0, 1, . . . , m ‚àí1
,
(8.46)
where f (x) is the objective function as before, and fi(x) = 0 for i = 0, 1, . . . ,
m ‚àí1 are the equality constraints. The functions fi|Rn ‚ÜíR are equality constraint
functions. The set F = {x|fi(x) = 0, i = 0, . . . , m ‚àí1} is called the feasible set.
We are interested in
ÀÜf = f (ÀÜx) = min
x‚ààF f (x).
(8.47)
There may be more than one x = ÀÜx ‚ààRn satisfying (8.47); that is, the set
ÀÜX = {x|x ‚ààF,
f (ÀÜx) = ÀÜf }
(8.48)
may have more than one element in it. We assume that our problem yields ÀÜX with
at least one element in it (i.e., ÀÜX Ã∏= ‚àÖ).
Equation (8.47) is really a more compact statement of (8.46), and in words states
that any minimizer ÀÜx of f (x) must also satisfy the equality constraints. We recall
examples of this type of problem from Chapter 4 [e.g., the problem of deriving a
computable expression for Œ∫2(A) and in the proof of Theorem 4.5]. More examples
will be seen later. Generally, in engineering, constrained optimization problems are
more common than unconstrained problems. However, it is important to understand
that algorithms for unconstrained problems form the core of algorithms for solving
constrained problems.
We now wish to make some general statements about how to solve (8.46), and
in so doing we introduce the concept of Lagrange multipliers. The arguments to
follow are somewhat heuristic, and they follow those of Section 9.1 in Fletcher [1].
Suppose that ÀÜx ‚ààRn is at least a local minimizer for objective function f (x) ‚àà
R. Analogously to (8.9), we have
fi(ÀÜx + Œ¥) ‚âàfi(ÀÜx) + gT
i (ÀÜx)Œ¥ + 1
2Œ¥T [‚àá2fi(ÀÜx)]Œ¥,
(8.49)
where Œ¥ ‚ààRn is some incremental step away from ÀÜx, and gi(x) = ‚àáfi(x) ‚ààRn
(i = 0, 1, . . . , m ‚àí1) is the gradient of the ith constraint function at x. A feasible
incremental step Œ¥ must yield ÀÜx + Œ¥ ‚ààF, and so must satisfy
fi(ÀÜx + Œ¥) = fi(ÀÜx) = 0
(8.50)
for all i. From (8.49) this implies the condition that Œ¥ must lie along feasible
direction s ‚ààRn (at x = ÀÜx) such that
gT
i (ÀÜx)s = 0
(8.51)
TLFeBOOK

358
UNCONSTRAINED OPTIMIZATION
again for all i. [We shall suppose that the vectors gi(ÀÜx) = ‚àáf (ÀÜx) are linearly
independent for all i.] Recalling (8.36), if s were also a descent direction at ÀÜx, then
gT (ÀÜx)s < 0
(8.52)
would hold (g(x) = ‚àáf (x) ‚ààRn). In this situation Œ¥ would reduce f (x), as Œ¥ is
along direction s. But this is impossible since we have assumed that ÀÜx is a local
minimizer for f (x). [For any s at ÀÜx, we expect to have gT (ÀÜx)s = 0.] Consequently,
no direction s can satisfy (8.51) and (8.52) simultaneously. This statement remains
true if g(ÀÜx) is a linear combination of gi(ÀÜx), that is, if, for suitable ÀÜŒªi ‚ààR we have
g(ÀÜx) =
m‚àí1

i=0
ÀÜŒªigi(ÀÜx).1
(8.53)
Thus, a necessary condition for ÀÜx to be a local minimizer (or, more generally, a
stationary point) of f (x) is that [rewriting (8.53)]
g(ÀÜx) ‚àí
m‚àí1

i=0
ÀÜŒªigi(ÀÜx) = 0
(8.54)
for suitable ÀÜŒªi ‚ààR which are called the Lagrange multipliers. We see that (8.54)
can be expressed as [with ‚àáas in (8.3)]
‚àá

f (ÀÜx) ‚àí
m‚àí1

i=0
ÀÜŒªifi(ÀÜx)
	
= 0.
(8.55)
In other words, we replace the original problem (8.46) with the mathematically
equivalent problem of minimizing
L(x, Œª) = f (x) ‚àí
m‚àí1

i=0
Œªifi(x),
(8.56)
1Equation (8.53) may be more formally justiÔ¨Åed as follows. Note that the same argument will also
extend to make (8.53) a necessary condition for ÀÜx to be a local maximizer, or saddle point for f (x).
Thus, (8.53) is really a necessary condition for ÀÜx to be a stationary point of f (x). We employ proof by
contradiction. Suppose that
g(ÀÜx) = ÀÜGÀÜŒª + h,
where ÀÜG = [g0(ÀÜx) ¬∑ ¬∑ ¬∑ gm‚àí1(ÀÜx)] ‚ààRn√óm, ÀÜŒª = [ÀÜŒª0 ¬∑ ¬∑ ¬∑ ÀÜŒªm‚àí1]T ‚ààRm and h Ã∏= 0. Further assume that
h ‚ààRn is the component of g(ÀÜx) that is orthogonal to all gi(ÀÜx). Thus, ÀÜGT h = 0. In this instance
s = ‚àíh will satisfy both (8.51) and (8.52) [i.e., gT (ÀÜx)s = ‚àí[ÀÜŒªT ÀÜGT + hT ]h = ‚àíhT h < 0]. Satisfaction
of (8.52) implies that a step Œ¥ in the direction s will reduce f (x) [i.e., f (ÀÜx + Œ¥) < f (ÀÜx)]. But this cannot
be the case since ÀÜx is a local minimizer of f (x). Consequently, h Ã∏= 0 is impossible, which establishes
(8.53).
TLFeBOOK

EQUALITY CONSTRAINTS AND LAGRANGE MULTIPLIERS
359
called the Lagrangian function (or Lagrangian), where, of course, x ‚ààRn and
Œª ‚ààRm. Since L|Rn √ó Rm ‚ÜíR, in order to satisfy (8.55), we must determine
ÀÜx, ÀÜŒª such that
‚àáL(ÀÜx, ÀÜŒª) = 0,
(8.57)
where now instead [of (8.3)] we have ‚àá=
 ‚àáx
‚àáŒª

such that
‚àáx =
 ‚àÇ
‚àÇx0
¬∑ ¬∑ ¬∑
‚àÇ
‚àÇxn‚àí1
T
,
‚àáŒª =
 ‚àÇ
‚àÇŒª0
¬∑ ¬∑ ¬∑
‚àÇ
‚àÇŒªm‚àí1
T
.
(8.58)
Now we see that a necessary condition for a stationary point of f (x) subject to
our constraints is that ÀÜx and ÀÜŒª form a stationary point of the Lagrangian function.
Of course, to resolve whether stationary point ÀÜx is a minimizer requires additional
information (e.g., the Hessian). Observe that ‚àáŒªL(x, Œª) = [‚àíf0(x) ‚àíf1(x) ¬∑ ¬∑ ¬∑ ‚àí
fm‚àí1(x)]T , so ‚àáŒªL(x, Œª) = 0 implies that fi(x) = 0 for all i. This is why we take
derivatives of the Lagrangian with respect to all elements of Œª; it is equivalent to
imposing the equality constraints as in the original problem (8.46).
We now consider a few examples of the application of the method of Lagrange
multipliers.
Example 8.2
This example is from Fletcher [1, pp. 196‚Äì198]. Suppose that
f (x) = x0 + x1,
f0(x) = x2
0 ‚àíx1 = 0,
so x = [x0
x1]T ‚ààR2, and the Lagrangian is
L(x, Œª) = x0 + x1 ‚àíŒª(x2
0 ‚àíx1).
Clearly, to obtain stationary points, we must solve
‚àÇL
‚àÇx0
= 1 ‚àí2Œªx0 = 0,
‚àÇL
‚àÇx1
= 1 + Œª = 0,
‚àÇL
‚àÇŒª = x2
0 ‚àíx1 = 0.
Immediately, ÀÜŒª = ‚àí1, so that 1 ‚àí2ÀÜŒªÀÜx0 = 0 yields ÀÜx0 = ‚àí1
2, and ÀÜx2
0 ‚àíÀÜx1 = 0
yields ÀÜx1 = 1
4. Thus
ÀÜx =

‚àí1
2
1
4
T
.
Is ÀÜx really a minimizer, or is it a maximizer, or saddle point?
TLFeBOOK

360
UNCONSTRAINED OPTIMIZATION
An alternative means to solve our problem is to recognize that since x2
0 ‚àíx1 = 0,
we can actually minimize the new objective function
f ‚Ä≤(x0) = x0 + x1|x1=x2
0 = x0 + x2
0
with respect to x0 instead. Clearly, this is a positive deÔ¨Ånite quadratic with a well-
deÔ¨Åned and unique minimum at x0 = ‚àí1
2. Again we conclude ÀÜx = [‚àí1
2
1
4]T , and
it must speciÔ¨Åcally be a minimizer.
In Example 8.2,
ÀÜf = f (ÀÜx) = ÀÜx0 + ÀÜx1 = ‚àí1
4 > ‚àí‚àûonly because of the con-
straint, f0(x) = x2
0 ‚àíx1 = 0. Without such a constraint, we would have ÀÜf = ‚àí‚àû.
We have described the method of Lagrange multipliers as being applied largely to
minimization problems. But we have noted that this method applies to maximization
problems as well because (8.57) is the necessary condition for a stationary point,
and not just a minimizer. The next example is a simple maximization problem from
geometry.
Example 8.3
We wish to maximize
F(x, y) = 4xy
subject to the constraint
C(x, y) = 1
4x2 + y2 ‚àí1 = 0.
This problem may be interpreted as the problem of maximizing the area of a
rectangle of area 4xy such that the corners of the rectangle are on an ellipse centered
at the origin of the two-dimensional plane. The ellipse is the curve C(x, y) = 0.
(Drawing a sketch is a useful exercise.)
Following the Lagrange multiplier procedure, we construct the Lagrangian
function
G(x, y, Œª) = 4xy + Œª( 1
4x2 + y2 ‚àí1)
Taking the derivatives of G and setting them to zero yields
‚àÇG
‚àÇx = 4y + 1
2Œªx = 0
(8.59a)
‚àÇG
‚àÇy = 4x + 2Œªy = 0
(8.59b)
‚àÇG
‚àÇŒª = 1
4x2 + y2 ‚àí1 = 0
(8.59c)
TLFeBOOK

EQUALITY CONSTRAINTS AND LAGRANGE MULTIPLIERS
361
From (8.59a,b) we have
Œª = ‚àí8y
x ,
and
Œª = ‚àí2x
y ,
which means that ‚àí2x/y = ‚àí8y/x, or x2 = 4y2. Thus, we may replace x2 by 4y2
in (8.59c), giving
2y2 ‚àí1 = 0
for which y2 = 1
2, and so x2 = 2. From these equations we easily obtain the loca-
tions of the corners of the rectangle on the ellipse. The area of the rectangle is also
seen to be four units.
Example 8.4
Recall from Chapter 4 that we sought a method to determine
(compute) the matrix 2-norm
||A||2 = max
||x||2=1 ||Ax||2.
Chapter 4 considered only the special case x ‚ààR2 (i.e., n = 2). Now we consider
the general case for which n ‚â•2.
Since ||Ax||2
2 = xT AT Ax = xT Rx with R = RT and R > 0 (if A is full rank),
our problem is to maximize xT Rx subject to the equality constraint ||x||2 = 1 (or
equivalently xT x = 1). The Lagrangian is
L(x, Œª) = xT Rx ‚àíŒª(xT x ‚àí1)
since f (x) = xT Rx and f0(x) = xT x ‚àí1. Now
f (x) = xT Rx =
n‚àí1

i=0
n‚àí1

j=0
xixjrij
=
n‚àí1

i=0
x2
i rii +
n‚àí1

i=0
n‚àí1

j=0
iÃ∏=j
xixjrij
so that (using rij = rji)
‚àÇf
‚àÇxk
= 2rkkxk +
n‚àí1

j = 0
j Ã∏= k
xjrkj +
n‚àí1

i = 0
i Ã∏= k
xirik
= 2rkkxk +
n‚àí1

j = 0
j Ã∏= k
rkjxj +
n‚àí1

j = 0
j Ã∏= k
rkjxj.
TLFeBOOK

362
UNCONSTRAINED OPTIMIZATION
This reduces to
‚àÇf
‚àÇxk
= 2rkkxk + 2
n‚àí1

j = 0
j Ã∏= k
rkjxj = 2
n‚àí1

j=0
rkjxj
for all k = 0, 1, . . . , n ‚àí1. Consequently, ‚àáxf (x) = 2Rx. Similarly, ‚àáxf0(x) =
2x. Also, ‚àáŒªL(x, Œª) = ‚àíxT x + 1 = ‚àíf0(x), and so ‚àáL(x, Œª) = 0 yields the
equations
2Rx ‚àí2Œªx = 0,
xT x ‚àí1 = 0.
The Ô¨Årst equation states that the maximizing solution (if it exists) must satisfy the
eigenproblem
Rx = Œªx
for which Œª is an eigenvalue and x is the corresponding eigenvector. Consequently,
xT Rx = ŒªxT x, so
Œª = xT Rx
xT x
= ||Ax||2
2
||x||2
2
= ||Ax||2
2
must be chosen to be the biggest eigenvalue of R. Since R > 0, such an eigenvalue
will exist. As before (Chapter 4), we conclude that
||A||2
2 = Œªn‚àí1
for which Œªn‚àí1 is the largest of the eigenvalues Œª0, . . . , Œªn‚àí1 of R = AT A (Œªn‚àí1 ‚â•
Œªn‚àí2 ‚â•¬∑ ¬∑ ¬∑ ‚â•Œª0 > 0).
APPENDIX 8.A
MATLAB CODE FOR GOLDEN SECTION SEARCH
%
%
SineFit1.m
%
% This routine computes the objective function V_1(T) for user input
% T and data vector f as required by the golden section search test
% procedure golden.m.
Note that Ts (sampling period) must be consistent
% with Ts in golden.m.
%
function
V1 = SineFit1(f,T);
N = length(f); % Number of samples collected
Ts = 5*60;
% 5 minute sampling period
TLFeBOOK

MATLAB CODE FOR GOLDEN SECTION SEARCH
363
% Compute the objective function V_1(T)
n = [0:N-1];
T = T*60*60;
A = [ sin(2*pi*Ts*n/T).‚Äô cos(2*pi*Ts*n/T).‚Äô ];
B = inv(A.‚Äô*A);
V1 = f.‚Äô*f - f.‚Äô*A*B*A.‚Äô*f;
%
%
golden.m
%
% This routine tests the golden section search procedure of Chapter 8
% on the noisy sinusoid problem depicted in Fig. 8.4.
%
% This routine creates a test signal and uses SineFit1.m to compute
% the corresponding objective function V_1(T) (given by Equation (8.33)
% in Chapter 8).
%
function Topt = golden
% Compute the test signal f
N = 500;
% Number of samples collected
Ts = 5*60;
% 5 minute sampling period
T = 24*60*60; % 24 hr period for the sinusoid
phi = -pi/10; % phase angle of sinusoid
a = 1.0;
% sinusoid amplitude
var = .5000;
% desired noise variance
std = sqrt(var);
eta = std*randn(1,N);
for n = 1:N
f(n) = a*sin(((2*pi*(n-1)*Ts)/T) + phi);
end;
f = f + eta;
f = f.‚Äô;
% Specify a starting interval and initial parameters
% (units of hours)
xl = 16;
xu = 32;
tau = (sqrt(5)+1)/2;
% The golden ratio
tol = .05;
% Accuracy of the location of the minimum
% Apply the golden section search procedure
I = xu - xl;
% length of the starting interval
xa = xu - I/tau;
xb = xl + I/tau;
TLFeBOOK

364
UNCONSTRAINED OPTIMIZATION
while I > tol
if SineFit1(f,xa) >= SineFit1(f,xb)
I = xu - xa;
xl = xa;
temp = xa;
xa = xb;
xb = temp + I/tau;
else
I = xb - xl;
xu = xb;
temp = xb;
xb = xa;
xa = temp - I/tau;
end
end
Topt = (xl + xu)/2;
% Estimate of optimum choice for T
REFERENCES
1. R. Fletcher, Practical Methods of Optimization, 2nd ed., Wiley, New York, 1987
(reprinted July 1993).
2. M. R. Schroeder, Number Theory in Science and Communication (with Applications
in Cryptography, Physics, Digital Information, Computing, and Self-Similarity), 2nd
(expanded) ed., Springer-Verlag, New York, 1986.
3. A. Quarteroni, R. Sacco, and F. Saleri, Numerical Mathematics (Texts in Applied Math-
ematics series, Vol. 37). Springer-Verlag, New York, 2000.
4. R. P. Brent, Algorithms for Minimization without Derivatives, Dover Publications, Mine-
ola, NY, 2002.
5. S. Boyd and L. Vandenberghe, Convex Optimization, preprint, Dec. 2001.
PROBLEMS
8.1. Suppose A ‚ààRn√ón, and that A is not symmetric in general. Prove that
‚àá(xT Ax) = (A + AT )x,
where ‚àáis the gradient operator [Eq. (8.3)].
8.2. This problem is about ideas from vector calculus useful in nonlinear opti-
mization methods.
(a) If s, x, x‚Ä≤ ‚ààRn, then a line in Rn is deÔ¨Åned by
x = x(Œ±) = x‚Ä≤ + Œ±s,
Œ± ‚ààR.
Vector s may be interpreted as determining the direction of the line in the
n-dimensional space Rn. The notation x(Œ±) implies x(Œ±) = [x0(Œ±)x1(Œ±)
¬∑ ¬∑ ¬∑ xn‚àí1(Œ±)]T . Prove that the slope df/dŒ± of f (x(Œ±)) ‚ààR along the line
TLFeBOOK

PROBLEMS
365
at any x(Œ±) is given by
df
dŒ± = sT ‚àáf,
where ‚àáis the gradient operator [see Eq. (8.3)]. (Hint: Use the chain
rule for derivatives.)
(b) Suppose that u(x), v(x) ‚ààRn (again x ‚ààRn). Prove that
‚àá(uT v) = (‚àáuT )v + (‚àávT )u.
8.3. Use the golden ratio search method to Ô¨Ånd the global minimizer of the
polynomial objective function in Fig. 8.2. Do the computations using a pocket
calculator, with starting interval [x0
l , x0
u] = [2, 2.5]. Iterate 5 times.
8.4. Review Problem 7.4 (in Chapter 7).
Use a MATLAB implementation of the golden ratio search method to Ô¨Ånd
detection threshold Œ∑ for PFA = 0.1, 0.01, and 0.001. The objective func-
tion is
f (Œ∑) =

PFA ‚àíe‚àí(1/2)Œ∑
(p/2)‚àí1

k=0
1
k!
Œ∑
2
k

.
Make reasonable choices about starting intervals.
8.5. Suppose x = [x0
x1]T ‚ààR2, and consider
f (x) = x4
0 + x0x1 + (1 + x1)2.
Find general expressions for the gradient and the Hessian of f (x). Is G(0) >
0? What does this signify? Use the Newton‚ÄìRaphson method (Chapter 7)
to conÔ¨Årm that ÀÜx = [0.6959
‚àí1.3479]T is a stationary point for f (x).
Select x0 = [0.7000
‚àí1.3]T as the starting point. Is G(ÀÜx) > 0? What does
this signify? In this problem do all necessary computations using a pocket
calculator.
8.6. If ÀÜx ‚ààRn is a local minimizer for f (x), then we know that G(ÀÜx) > 0, that
is, G(ÀÜx) is positive deÔ¨Ånite (pd). On the other hand, if ÀÜx is a local maximizer
of f (x), then ‚àíG(ÀÜx) > 0. In this case we say that G(ÀÜx) is negative deÔ¨Ånite
(nd). Show that
f (x) = (x1 ‚àíx2
0)2 + x5
0
has only one stationary point, and that it is neither a minimizer nor a maxi-
mizer of f (x).
8.7. Take note of the criterion for a maximizer in the previous problem. For both
a = 6 and a = 8, Ô¨Ånd all stationary points of the function
f (x) = 2x3
0 ‚àí3x2
0 ‚àíax0x1(x0 ‚àíx1 ‚àí1).
Determine which are local minima, local maxima, or neither.
TLFeBOOK

366
UNCONSTRAINED OPTIMIZATION
8.8. Write a MATLAB function to Ô¨Ånd the minimum of Rosenbrock‚Äôs function
using the Newton algorithm (basic form that does not employ a line search).
Separate functions must be written to implement computation of both the
gradient and the inverse of the Hessian. The function for computing the
inverse of the Hessian must return an integer that indicates whether the Hes-
sian is positive deÔ¨Ånite. The Newton algorithm must terminate if a Hessian
is encountered that is not positive deÔ¨Ånite. The I/O is to be at the terminal
only. The user must input the starting vector at the terminal, and the program
must report the estimated minimum, or it must print an error message if the
Hessian is not positive deÔ¨Ånite. Test your program out on the starting vectors
x(0) = [‚àí3
‚àí3]T
and
x(0) = [0
10]T .
8.9. Write and test your own MATLAB routine (or routines) to verify Example 8.1.
8.10. Find the points on the ellipse
x2
a2 + y2
b2 = 1
that are closest to, and farthest from, the origin (x, y) = (0, 0). Use the
method of Lagrange multipliers.
8.11. The theory of Lagrange multipliers in Section 8.5 is a bit oversimpliÔ¨Åed.
Consider the following theorem. Suppose that ÀÜx ‚ààRn gives an extremum
(i.e., minimum or maximum) of f (x) ‚ààR among all x satisfying g(x) = 0.
If f, g ‚ààC1[D] for a domain D ‚äÇR containing ÀÜx, then either
g(ÀÜx) = 0
and
‚àág(ÀÜx) = 0,
(8.P.1)
or there is a Œª ‚ààR such that
g(ÀÜx) = 0
and
‚àáf (ÀÜx) ‚àíŒª‚àág(ÀÜx) = 0.
(8.P.2)
From this theorem, candidate points ÀÜx for extrema of f (x) satisfying g(x) =
0 therefore are
(a) Points where f and g fail to have continuous partial derivatives.
(b) Points satisfying (8.P.1).
(c) Points satisfying (8.P.2).
In view of the theorem above and its consequences, Ô¨Ånd the minimum dis-
tance from x = [x0
x1
x2]T = [0
0
‚àí1]T to the surface
g(x) = x2
0 + x2
1 ‚àíx5
2 = 0.
TLFeBOOK

PROBLEMS
367
8.12. A second-order Ô¨Ånite-impulse response (FIR) digital Ô¨Ålter has the frequency
response H(ejœâ) = 2
k=0 hke‚àíjœâk, where hk ‚ààR are the Ô¨Ålter parameters.
Since H(ejœâ) is 2œÄ‚àíperiodic we usually consider only œâ ‚àà[‚àíœÄ, œÄ]. The
DC response of the Ô¨Ålter is H(1) = H(ej0) = 2
k=0 hk. DeÔ¨Åne the energy
of the Ô¨Ålter in the band [‚àíœâp, œâp] to be
E = 1
2œÄ
 œâp
‚àíœâp
|H(ejœâ)|2 dœâ.
Find the Ô¨Ålter parameters h = [h0 h1 h2]T ‚ààR3 such that for œâp = œÄ/2
energy E is minimized subject to the constraint that H(1) = 1 (i.e., the gain
of the Ô¨Ålter is unity at DC). Plot |H(ejœâ)| for œâ ‚àà[‚àíœÄ, œÄ]. [Hint: E will
have the form E = hT Rh ‚ààR, where R ‚ààR3√ó3 is a symmetric Toeplitz
matrix (recall Problem 4.20). Note also that |H(ejœâ)|2 = H(ejœâ)H ‚àó(ejœâ).]
8.13. This problem introduces incremental condition estimation (ICE) and is based
on the paper C. H. Bischof, ‚ÄúIncremental Condition Estimation,‚Äù SIAM J.
Matrix Anal. Appl. 11, 312‚Äì322 (April 1990). ICE can be used to estimate
the condition number of a lower triangular matrix as it is generated one
row at a time. Many algorithms for linear system solution produce triangular
matrix factorizations one row or one column at a time. Thus, ICE may be
built into such algorithms to warn the user of possible inaccuracies in the
solution due to ill conditioning. Let An be an n √ó n matrix with singular
values œÉ1(An) ‚â•¬∑ ¬∑ ¬∑ ‚â•œÉn(An) ‚â•0. A condition number for An is
Œ∫(An) = œÉ1(An)
œÉn(An).
Consider the order n lower triangular linear system
Lnxn = dn.
(8.P.3)
The minimum singular value, œÉn(Ln), of Ln satisÔ¨Åes
œÉn(Ln) ‚â§||dn||2
||xn||2
(||xn||2
2 = 
i x2
n,i). Thus, an estimate (upper bound) of this singular value is
ÀÜœÉn(Ln) = ||dn||2
||xn||2
.
We would like to make this upper bound as small as possible. So, Bischof
suggests Ô¨Ånding xn to satisfy (8.P.3) such that ||xn||2 is maximized subject
to the constraint that ||dn||2 = 1. Given xn‚àí1 such that Ln‚àí1xn‚àí1 = dn‚àí1
TLFeBOOK

368
UNCONSTRAINED OPTIMIZATION
with ||dn‚àí1||2 = 1 [which gives us ÀÜœÉn‚àí1(Ln‚àí1) = 1/||xn‚àí1||2], Ô¨Ånd sn and
cn such that ||xn||2 is maximized where
Lnxn =

Ln‚àí1
0
vT
n
Œ≥n
	
xn =
 sndn‚àí1
cn

= dn,
xn =

snxn‚àí1
(cn ‚àísnŒ±n)/Œ≥n
	
.
Find Œ±n, cn, sn. Assume Œ±n Ã∏= 0. (Comment: The indexing of the singular
values used here is different from that in Chapter 4. The present notation is
more convenient in the present context.)
8.14. Assume that A ‚ààRm√ón with m ‚â•n, but rank(A) < n is possible. As usual,
||x||2
2 = xT x. Solve the following problem:
min
x‚ààRn ||Ax ‚àíb||2
2 + Œ¥||x||2
2,
where Œ¥ > 0. This is often called the Tychonov regularization problem. It
is a simple ploy to alleviate problems with ill-conditioning in least-squares
applications. Since A is not necessarily of full rank, we have AT A ‚â•0, but
not necessarily AT A > 0. What is rank(AT A + Œ¥I)? Of course, I is the order
n identity matrix.
TLFeBOOK

9
Numerical Integration
and Differentiation
9.1
INTRODUCTION
We are interested in how to compute the integral
I =
 b
a
f (x) dx
(9.1)
for which f (x) ‚ààR (and, of course, x ‚ààR). Depending on f (x), and perhaps also
on [a, b], the reader knows that ‚Äúnice‚Äù closed-form expressions for I rarely exist.
This forces us to consider numerical methods to approximate I. We have seen from
Chapter 3 that one approach is to Ô¨Ånd a suitable series expansion for the integral
in (9.1). For example, recall that we wished to compute the error function
erf(x) =
2
‚àöœÄ
 x
0
e‚àít2 dt,
(9.2)
which has no antiderivative (i.e., ‚Äúnice‚Äù formula). Recall that the error function is
crucial in solving various problems in applied probability that involve the Gaussian
probability density function [i.e., the function in (3.101) of Chapter 3]. The Taylor
series expansion of Eq. (3.108) was suggested as a means to approximately evaluate
erf(x), and is known to be practically effective if x is not too big. If x is large, then
the asymptotic expansion of Example 3.10 was suggested. The series expansion
methodology may seem to solve our problem, but there are integrals for which it
is not easy to Ô¨Ånd series expansions of any kind.
A recursive approach may be attempted as an alternative. An example of this was
seen in Chapter 5, where Ô¨Ånding the norm of a Legendre polynomial required solv-
ing a recursion involving variables that were certain integrals [recall Eq. (5.96)].
As another example of this approach, consider the following case from Forsythe
et al. [1]. Suppose that we wish to compute
En =
 1
0
xnex‚àí1 dx
(9.3)
An Introduction to Numerical Analysis for Electrical and Computer Engineers, by C.J. Zarowski
ISBN 0-471-46737-5
c‚Éù2004 John Wiley & Sons, Inc.
369
TLFeBOOK

370
NUMERICAL INTEGRATION AND DIFFERENTIATION
for any n ‚ààN (natural numbers). Recalling integration by parts, we see that
 1
0
xnex‚àí1 dx = xnex‚àí1|1
0 ‚àí
 1
0
nxn‚àí1ex‚àí1 dx = 1 ‚àínEn‚àí1,
so
En = 1 ‚àínEn‚àí1
(9.4)
for n = 2, 3, 4, . . .. It is easy to conÔ¨Årm that
E1 =
 1
0
xex‚àí1 dx = 1
e .
This is the initial condition for recursion (9.4). We observe that En > 0 for all
n. But if, for example, MATLAB is used to compute E19, we obtain computed
solution ÀÜE19 = ‚àí5.1930, which is clearly wrong. Why has this happened?
Because of the need to quantize, E1 is actually stored in the computer as
ÀÜE1 = E1 + œµ
for which œµ is some quantization error. Assuming that the operations in (9.4) do
not lead to further errors (i.e., assuming no rounding errors), we may arrive at a
formula for ÀÜEn:
ÀÜE2 = 1 ‚àí2 ÀÜE1 = E2 + (‚àí2)œµ,
ÀÜE3 = 1 ‚àí3 ÀÜE2 = E3 + (‚àí3)(‚àí2)œµ,
ÀÜE4 = 1 ‚àí4 ÀÜE3 = E4 + (‚àí4)(‚àí3)(‚àí2)œµ,
and so on. In general
ÀÜEn = En + (‚àí1)n‚àí11 ¬∑ 2 ¬∑ 3 ¬∑ ¬∑ ¬∑ (n ‚àí1)nœµ,
(9.5)
so
ÀÜEn ‚àíEn = (‚àí1)n‚àí1n!œµ.
(9.6)
We see that even a very tiny quantization error œµ will grow very rapidly during
the course of the computation even without any additional rounding errors at all !
Thus, (9.4) is a highly unstable numerical procedure, and so must be rejected as
a method to compute (9.3). However, it is possible to arrive at a stable procedure
by modifying (9.4). Now observe that
En =
 1
0
xnex‚àí1 dx ‚â§
 1
0
xn dx =
1
n + 1,
(9.7)
TLFeBOOK

TRAPEZOIDAL RULE
371
implying that En ‚Üí0 as n increases. Instead of (9.4), consider the recursion
[obtained by rearranging (9.4)]
En‚àí1 = 1
n(1 ‚àíEn).
(9.8)
If we wish to compute Em, we may assume En = 0 for some n signiÔ¨Åcantly bigger
than m and apply (9.8). From (9.7) the error involved in approximating En by zero
is not bigger than 1/(n + 1). Thus, an algorithm for Em is
Ek‚àí1 = 1
k (1 ‚àíEk)
for k = n, n ‚àí1, . . . , m + 2, m + 1, where En = 0. At each stage of this algorithm
the initial error is reduced by factor 1/k rather than being magniÔ¨Åed as it was in
the procedure of (9.4).
Other than potential numerical stability problems, which may or may not be
easy to solve, it is apparent that not all integration problems may be cast into a
recursive form. It is also possible that f (x) is not known for all x ‚ààR. We might
know f (x) only on a Ô¨Ånite subset of R, or perhaps a countably inÔ¨Ånite subset of
R. This situation might arise in the context of obtaining f (x) experimentally. Such
a scenario would rule out the previous suggestions.
Thus, there is much room to consider alternative methodologies. In this chapter
we consider what may be collectively called quadrature methods. For the most part,
these are based on applying some of the interpolation ideas considered in Chapter 6.
But Gaussian quadrature (see Section 9.4) also employs orthogonal polynomials
(material from Chapter 5).
This Chapter is dedicated mainly to the subject of numerical integration by
quadratures. But the Ô¨Ånal section considers numerical approximations to deriva-
tives (i.e., numerical differentiation). Numerical differentiation is relevant to the
numerical solution of differential equations (to be considered in later chapters),
and we have mentioned that it is relevant to spline interpolation (Section 6.5). In
fact, it can also Ô¨Ånd a role in reÔ¨Åned methods of numerical integration (Section 9.5).
9.2
TRAPEZOIDAL RULE
A simple approach to numerical integration is the following.
In this book we implicitly assume all functions are Riemann integrable. From
elementary calculus such integrals are obtained by the limiting process
I =
 b
a
f (x) dx = lim
n‚Üí‚àû
b ‚àía
n
n

k=1
f (x‚ó¶
k)
(9.9)
for which x0 = a, xn = b, and for which the value I is independent of the point x‚ó¶
k ‚àà
[xk‚àí1, xk]. We remark that not all functions f (x) satisfy this requirement and so,
as was mentioned in Chapter 3, not all functions are Riemann integrable. However,
TLFeBOOK

372
NUMERICAL INTEGRATION AND DIFFERENTIATION
we will ignore this potential problem. We may approximate I according to
I ‚âàb ‚àía
n
n

k=1
f (x‚ó¶
k).
(9.10)
Such an approximation is called the rectangular rule (or rectangle rule) for numer-
ical integration, and there are different variants depending on the choice for x‚ó¶
k.
Three possible choices are shown in Fig. 9.1. We mention that all variants involve
assuming f (x) is piecewise constant on [xk‚àí1, xk], and so amount to the constant
interpolation of f (x) (i.e., Ô¨Åtting a polynomial which is a constant to f (x) on
some interval). DeÔ¨Åne
h = b ‚àía
n
.
From Fig. 9.1, the right-point rule uses
x‚ó¶
k = a + kh,
(9.11a)
while the left-point rule uses
x‚ó¶
k = a + (k ‚àí1)h,
(9.11b)
xk‚àí1
xk
y = f(x)
0
x
a
b
(a)
xk‚àí1
xk
y = f(x)
0
x
a
b
(b)
xk‚àí1
xk
y = f(x)
0
x
a
b
(c)
Figure 9.1
Illustration of the different forms of the rectangular rule: (a) right-point rule;
(b) left-point rule; (c) midpoint rule. In all cases x0 = a and xn = b.
TLFeBOOK

TRAPEZOIDAL RULE
373
and the midpoint rule uses
x‚ó¶
k = a + (k ‚àí1
2)h,
(9.11c)
where in all cases k = 1, 2, . . . , n ‚àí1, n. The midpoint rule is often preferred
among the three as it is usually more accurate. However, the rectangular rule is often
(sometimes unfairly) regarded as too crude, and so the following (or something still
‚Äúbetter‚Äù) is chosen.
It is often better to approximate f (x) with trapezoids as shown in Fig. 9.2. This
results in the trapezoidal rule for numerical integration. From Fig. 9.2 we see that
this rule is based on the linear interpolation of the function f (x) on [xk‚àí1, xk].
The approximation to
" xk
xk‚àí1 f (x) dx is given by the area of the trapezoid: this is
 xk
xk‚àí1
f (x) dx ‚âà1
2

f (xk) + f (xk‚àí1)

(xk ‚àíxk‚àí1) = 1
2

f (xk‚àí1) + f (xk)

h
(9.12)
[We have h = xk ‚àíxk‚àí1 = (b ‚àía)/n.] It is intuitively plausible that this method
should be more accurate than the rectangular rule, and yet not require much, if
any, additional computational effort to implement it. Applying (9.12) for k = 1 to
k = n, we have
 b
a
f (x) dx ‚âàT (n) = h
2
n

k=1

f (xk‚àí1) + f (xk)

,
(9.13)
and the summation expands out as
T (n) = h
2

f (x0) + 2f (x1) + 2f (x2) + ¬∑ ¬∑ ¬∑ + 2f (xn‚àí1) + f (xn)

,
(9.14)
where n ‚ààN (set of natural numbers).
f(xk)
f(xk‚àí1)
xk‚àí1
xk
y = f(x)
x
0
Figure 9.2
Illustration of the trapezoidal rule.
TLFeBOOK

374
NUMERICAL INTEGRATION AND DIFFERENTIATION
We may investigate the error behavior of the trapezoidal rule as follows. The
process of analysis begins by assuming that n = 1; that is, we linearly interpolate
f (x) on [a, b] using
p(x) = f (a)x ‚àíb
a ‚àíb + f (b)x ‚àía
b ‚àía ,
(9.15)
which is from the Lagrange interpolation formula [recall (6.9) for n = 1]. It must
be the case that for suitable error function e(x), we have
f (x) = p(x) + e(x)
(9.16)
with x ‚àà[a, b]. Consequently
 b
a
f (x) dx =
 b
a
p(x) dx +
 b
a
e(x) dx
= b ‚àía
2
[f (a) + f (b)] +
 b
a
e(x) dx = T (1) + ET (1),
(9.17)
where
ET (1) =
 b
a
e(x) dx,
(9.18)
which is the error involved in using the trapezoidal rule. Of course, we would like
a suitable bound on this error. To obtain such a bound, we will assume that f (1)(x)
and f (2)(x) both exist and are continuous on [a, b]. Let x be Ô¨Åxed at some value
such that a < x < b, and deÔ¨Åne
g(t) = f (t) ‚àíp(t) ‚àí[f (x) ‚àíp(x)] (t ‚àía)(t ‚àíb)
(x ‚àía)(x ‚àíb)
(9.19)
for t ‚àà[a, b]. It is not difÔ¨Åcult to conÔ¨Årm that
g(a) = g(b) = g(x) = 0,
so g(t) vanishes at three different places on the interval [a, b]. Rolle‚Äôs theorem1
says that there are points Œæ1 ‚àà(a, x), Œæ2 ‚àà(x, b) such that
g(1)(Œæ1) = g(1)(Œæ2) = 0.
Thus, g(1)(t) vanishes at two different places on (a, b), so yet again by Rolle‚Äôs
theorem there is a Œæ ‚àà(Œæ1, Œæ2) such that
g(2)(Œæ) = 0.
1This was proved by Bers [2], but the proof is actually rather lengthy, and so we omit it.
TLFeBOOK

TRAPEZOIDAL RULE
375
We note that Œæ = Œæ(x), that is, point Œæ depends on x. Therefore
g(2)(Œæ) = f (2)(Œæ) ‚àíp(2)(Œæ) ‚àí[f (x) ‚àíp(x)]
2
(x ‚àía)(x ‚àíb) = 0.
(9.20)
The polynomial p(x) is of the Ô¨Årst degree so p(2)(Œæ) = 0. We may use this in
(9.20) and rearrange the result so that
f (x) = p(x) + 1
2f (2)(Œæ(x))(x ‚àía)(x ‚àíb)
(9.21)
for any x ‚àà(a, b). This expression also happens to be valid at x = a and at x = b,
so
e(x) = f (x) ‚àíp(x) = 1
2f (2)(Œæ(x))(x ‚àía)(x ‚àíb)
(9.22)
for x ‚àà[a, b]. But we need to evaluate ET (1) in (9.18). The second mean-value
theorem for integrals states that if f (x) is continuous and g(x) is integrable (Rie-
mann) on [a, b], and further that g(x) does not change sign on [a, b], then there
is a point p ‚àà(a, b) such that
 b
a
f (x)g(x) dx = f (p)
 b
a
g(x) dx.
(9.23)
The proof is omitted. We observe that (x ‚àía)(x ‚àíb) does not change sign on
x ‚àà[a, b], so via this theorem we have
ET (1) = 1
2
 b
a
f (2)(Œæ(x))(x ‚àía)(x ‚àíb) dx = ‚àí1
12f (2)(p)(b ‚àía)3,
(9.24)
where p ‚àà(a, b). We emphasize that this is the error for n = 1 in T (n). Naturally,
we want an error expression for n > 1, too. When n > 1, we may refer to the
integration rule as a compound or composite rule.
The error committed in numerically integrating over the kth subinterval [xk‚àí1, xk]
must be [via (9.24)]
Ek = ‚àí1
12f (2)(Œæk)(xk ‚àíxk‚àí1)3 = ‚àíh3
12f (2)(Œæk) = ‚àíh2
12
b ‚àía
n
f (2)(Œæk),
(9.25)
where Œæk ‚àà[xk‚àí1, xk] and k = 1, 2, . . . , n ‚àí1, n. Therefore, the total error com-
mitted is
ET (n) =
 b
a
f (x) dx ‚àíT (n) =
n

k=1
Ek,
(9.26)
which becomes [via (9.25)]
ET (n) = ‚àíh2
12
b ‚àía
n
n

k=1
f (2)(Œæk).
(9.27)
TLFeBOOK

376
NUMERICAL INTEGRATION AND DIFFERENTIATION
The average 1
n
n
k=1 f (2)(Œæk) must lie between the largest and smallest values of
f (2)(x) on [a, b], so recalling that f (2)(x) is continuous on [a, b], the intermediate
value theorem (Theorem 7.1) yields that there is an Œæ ‚àà(a, b) such that
f (2)(Œæ) = 1
n
n

k=1
f (2)(Œæk).
Therefore
ET (n) = ‚àíh2
12(b ‚àía)f (2)(Œæ),
(9.28)
where Œæ ‚àà(a, b). If the maximum value of f (2)(x) on [a, b] is known, then this
may be used in (9.28) to provide an upper bound on the error. We remark that
ET (n) is often called truncation error.
We see that
T (n) = xT y
(9.29)
for which
x = h[ 1
21 ¬∑ ¬∑ ¬∑ 1 1
2]T ‚ààRn+1,
y = [f (x0)f (x1) ¬∑ ¬∑ ¬∑ f (xn‚àí1)f (xn)]T ‚ààRn+1.
(9.30)
We know that rounding errors will be committed in the computation of (9.29). The
total rounding error might be denoted by ER. We recall from Chapter 2 [Eq. (2.40)]
that a bound on these errors is
|ER| = |xT y ‚àíf l[xT y]| ‚â§1.01(n + 1)u|x|T |y|,
(9.31)
where u is the unit roundoff, or else the machine epsilon. The cumulative effect of
rounding errors can be expected to grow as n increases. If we suppose that
M = max
x‚àà[a,b] |f (2)(x)|,
(9.32)
then, from (9.28), we obtain
|ET (n)| ‚â§1
12
1
n2 (b ‚àía)3M.
(9.33)
Thus, (9.33) is an upper bound on the truncation error for the composite trapezoidal
rule. We see that the bound gets smaller as n increases. Thus, as we expect,
truncation error is reduced as the number of trapezoids used increases. Combining
(9.33) with (9.31) results in a bound on total error E:
|E| ‚â§1
12
1
n2 (b ‚àía)3M + 1.01(n + 1)u|x|T |y|.
(9.34)
TLFeBOOK

TRAPEZOIDAL RULE
377
Usually n ‚â´1, so n + 1 ‚âàn. Thus, substituting this and (9.30) into (9.31) results in
|xT y ‚àíf l[xT y]| ‚â§1.01(b ‚àía)u

|f (x0)| + |f (xn)|
2
+
n‚àí1

k=1
|f (xk)|
	
,
(9.35)
and so (9.34) becomes
|E| ‚â§1
12
1
n2 (b ‚àía)3M + 1.01(b ‚àía)u

|f (x0)| + |f (xn)|
2
+
n‚àí1

k=1
|f (xk)|
	
.
(9.36)
In general, the Ô¨Årst term in the bound of (9.36) becomes smaller as n increases,
while the second term becomes larger. Thus, there is a tradeoff involved in choosing
the number of trapezoids to approximate a given integral, and the best choice ought
to minimize the total error.
Example 9.1
We may apply the bound of (9.36) to the following problem. We
wish to compute
I =
 1
0
e‚àíx dx.
Thus, [a, b] = [0, 1], and so b ‚àía = 1. Of course, it is very easy to conÔ¨Årm that
I = 1 ‚àíe‚àí1. But this is what makes it a good example to test our theory out. We
also see that f (2)(x) = e‚àíx, and so in (9.32) M = 1. Also
f (xk) = e‚àík/n
for k = 0, 1, . . . , n ‚àí1, n. It is therefore easy to see that
n‚àí1

k=1
f (xk) = e‚àí1/n ‚àíe‚àí1
1 ‚àíe‚àí1/n .
We might assume that the trapezoidal rule for this problem is implemented in the
C programming language using single-precision Ô¨Çoating-point arithmetic, in which
case a typical value for u would be
u = 1.1921 √ó 10‚àí7.
Therefore, from (9.36) the total error is bounded according to
|E| ‚â§
1
12n2 + 1.2040 √ó 10‚àí7
e + 1
2e
+ e‚àí1/n ‚àíe‚àí1
1 ‚àíe‚àí1/n

.
Figure 9.3 plots this bound versus n, and also shows the magnitude of the computed
(i.e., the true or actual) total error in the trapezoidal rule approximation, which
is |T (n) ‚àíI|. We see that the true error is always less than the bound, as we
would expect. However, the bound is rather pessimistic. Also, the bound predicts
TLFeBOOK

378
NUMERICAL INTEGRATION AND DIFFERENTIATION
10‚àí8
103
104
105
106
107
10‚àí6
10‚àí4
10‚àí2
100
Number of trapezoids used (n)
Magnitude of error
10‚àí8
100
101
103
102
104
105
106
10‚àí6
10‚àí4
10‚àí2
100
Number of trapezoids used (n)
Magnitude of error
Bound
Computed error
Bound
Computed error
Figure 9.3
Comparison of total error (computed) to bound on total error, illustrating the
tradeoff between rounding error and truncation error in numerical integration by the trape-
zoidal rule. The bound employed here is that of Eq. (9.36).
that the proper choice for n is much less than what the computed result predicts.
SpeciÔ¨Åcally, the bound suggests that we choose n ‚âà100, while the computed result
suggests that we choose n ‚âà100, 000.
What is important is that the computed result and the bound both conÔ¨Årm that
there is a tradeoff between minimizing the truncation error and minimizing the
rounding error. To minimize rounding error, we prefer a small n, but to minimize
the truncation error, we prefer a large n. The best solution minimizes the total error
from both sources.
In practice, attempting a detailed analysis to determine the true optimum choice
for n is usually not worth the effort. What is important is to understand the funda-
mental tradeoffs involved in the choice of n, and from this understanding select a
reasonable value for n.
9.3
SIMPSON‚ÄôS RULE
The trapezoidal rule employed linear interpolation to approximate f (x) between
sample points xk on the x axis. We might consider quadratic interpolation in
TLFeBOOK

SIMPSON‚ÄôS RULE
379
the hope of improving accuracy still further. Here ‚Äúaccuracy‚Äù is a reference to
truncation error.
Therefore, we wish to Ô¨Åt a quadratic curve to the points (xk‚àí1, f (xk‚àí1)),
(xk, f (xk)) and (xk+1, f (xk+1)). We may deÔ¨Åne the quadratic to be
pk(x) = a(x ‚àíxk)2 + b(x ‚àíxk) + c.
(9.37)
Contrary to past practice, the subscript k now does not denote degree, but rather
denotes the ‚Äúcenterpoint‚Äù of the interval [xk‚àí1, xk+1] on which we are Ô¨Åtting the
quadratic. The situation is illustrated in Fig. 9.4 . For convenience, deÔ¨Åne yk =
f (xk). Therefore, from (9.37) we may set up three equations in the unknowns
a, b, c:
a(xk‚àí1 ‚àíxk)2 + b(xk‚àí1 ‚àíxk) + c = yk‚àí1,
a(xk ‚àíxk)2 + b(xk ‚àíxk) + c = yk,
a(xk+1 ‚àíxk)2 + b(xk+1 ‚àíxk) + c = yk+1.
This is a linear system of equations, and we will assume that h = xk ‚àíxk‚àí1 =
xk+1 ‚àíxk, so therefore
a = yk+1 ‚àí2yk + yk‚àí1
2h2
,
(9.38a)
b = yk+1 ‚àíyk‚àí1
2h
,
(9.38b)
c = yk.
(9.38c)
This leads to the approximation
 xk+1
xk‚àí1
f (x) dx ‚âà
 xk+1
xk‚àí1
pk(x) dx = h
3 [yk‚àí1 + 4yk + yk+1].
(9.39)
x
y = f(x)
0
xk‚àí1
xk+1
xk
Parabolic arc pk (x)
f(x)
Figure 9.4
Simpson‚Äôs rule for numerical integration.
TLFeBOOK

380
NUMERICAL INTEGRATION AND DIFFERENTIATION
Of course, some algebra has been omitted to arrive at the equality in (9.39). As
in Section 9.2, we wish to integrate f (x) on [a, b]. So, as before, a = x0, and
b = xn. If n is an even number, then the number of subdivisions of [a, b] is an
even number, and hence we have the approximation
I =
 b
a
f (x) dx ‚âà
 x2
x0
p1(x) dx +
 x4
x2
p3(x) dx + ¬∑ ¬∑ ¬∑ +
 xn
xn‚àí2
pn‚àí1(x) dx
= h
3[y0 + 4y1 + 2y2 + 4y3 + 2y4 + ¬∑ ¬∑ ¬∑ + 2yn‚àí2 + 4yn‚àí1 + yn].
(9.40)
The last equality follows from applying (9.39). We deÔ¨Åne the Simpson rule approx-
imation to I as
S(n) = h
3[y0 + 4y1 + 2y2 + 4y3 + 2y4 + ¬∑ ¬∑ ¬∑ + 2yn‚àí2 + 4yn‚àí1 + yn]
(9.41)
for which n is even and n ‚â•2.
A truncation error analysis of Simpson‚Äôs rule is more involved than that of
the analysis of the trapezoidal rule seen in the previous section. Therefore, we
only outline the major steps and results. We begin by using only two subintervals
to approximate I =
" b
a f (x) dx, speciÔ¨Åcally, n = 2. DeÔ¨Åne c = (a + b)/2. Denote
the interpolating quadratic by p(x). For a suitable error function e(x), we must
have
f (x) = p(x) + e(x).
(9.42)
Immediately we see that
I =
 b
a
f (x) dx =
 b
a
p(x) dx +
 b
a
e(x) dx
= b ‚àía
6

f (a) + 4f
 a + b
2
!
+ f (b)

+
 b
a
e(x) dx = S(2) + ES(2).
(9.43)
So, the truncation error in Simpson‚Äôs rule is thus
ES(2) =
 b
a
e(x) dx.
(9.44)
It is clear that Simpson‚Äôs rule is exact for f (x) a quadratic function. Less clear is
the fact that Simpson‚Äôs rule is exact if f (x) is a cubic polynomial. To demonstrate
the truth of this claim, we need an error result from Chapter 6. We assume that
f (k)(x) exists and is continuous for all k = 0, 1, 2, 3, 4 for all x ‚àà[a, b]. From
TLFeBOOK

SIMPSON‚ÄôS RULE
381
Eq. (6.14) the error involved in interpolating f (x) with a quadratic polynomial is
given by
e(x) = 1
3!f (3)(Œæ(x))(x ‚àía)(x ‚àíb)(x ‚àíc)
(9.45)
for some Œæ = Œæ(x) ‚àà[a, b]. Hence
ES(2) =
 b
a
e(x) dx = 1
3!
 b
a
f (3)(Œæ(x))(x ‚àía)(x ‚àíb)(x ‚àíc) dx.
(9.46)
Unfortunately, polynomial (x ‚àía)(x ‚àíb)(x ‚àíc) changes sign on the interval
[a, b], and so we are not able to apply the second mean-value theorem for integrals
as we did in Section 9.2. This is a major reason why the analysis of Simpson‚Äôs
rule is harder than the analysis of the trapezoidal rule. However, at this point we
may still consider (9.46) for the case where f (x) is a cubic polynomial. In this
case we must have f (3)(x) = K (some constant). Consequently, from (9.46)
ES(2) = K
3!
 b
a
(x ‚àía)(x ‚àíb)(x ‚àíc) dx,
but if z = x ‚àíc, then, since c = 1
2(a + b), we must have
ES(2) = K
3!

1
2 (b‚àía)
‚àí1
2 (b‚àía)
z

z + b ‚àía
2
 
z ‚àíb ‚àía
2

dz
= K
3!

1
2 (b‚àía)
‚àí1
2 (b‚àía)
z

z2 ‚àí
 b ‚àía
2
!2	
dz.
(9.47)
The integrand is an odd function of z, and the integration limits are symmetric
about the point z = 0. Immediately we conclude that ES(2) = 0 in this particular
case. Thus, we conclude that Simpson‚Äôs rule gives the exact result when f (x) is a
cubic polynomial.
Hermite interpolation (considered in a general way in Section 6.4) is polynomial
interpolation where not only does the interpolating polynomial match f (x) at the
sample points xk but the Ô¨Årst derivative of f (x) is matched as well. It is useful
to interpolate f (x) with a cubic polynomial that we will denote by r(x) at the
points (a, f (a)), (b, f (b)), and (c, f (c)), and also such that r(1)(c) = f (1)(c). A
cubic polynomial is speciÔ¨Åed by four coefÔ¨Åcients, so these constraints uniquely
determine r(x). In fact
r(x) = p(x) + Œ±(x ‚àía)(x ‚àíb)(x ‚àíc)
(9.48a)
for which
Œ± = 4[p(1)(c) ‚àíf (1)(c)]
(b ‚àía)2
.
(9.48b)
TLFeBOOK

382
NUMERICAL INTEGRATION AND DIFFERENTIATION
Analogously to (9.19), we may deÔ¨Åne
g(t) = f (t) ‚àír(t) ‚àí[f (x) ‚àír(x)] (t ‚àía)(t ‚àíc)2(t ‚àíb)
(x ‚àía)(x ‚àíc)2(x ‚àíb),
a ‚â§t ‚â§b.
(9.49)
It happens that g(k)(t) for k = 0, 1, 2, 3, 4 all exist and are continuous at all x ‚àà
[a, b]. Additionally, g(a) = g(b) = g(c) = g(1)(c) = g(x) = 0. The vanishing of
g(t) at four distinct points on [a, b], and g(1)(c) = 0 guarantees that g(4)(Œæ) = 0
for some Œæ ‚àà[a, b] by the repeated application of Rolle‚Äôs theorem. Consequently,
using (9.49), we obtain
g(4)(Œæ) = f (4)(Œæ) ‚àír(4)(Œæ) ‚àí[f (x) ‚àír(x)]
4!
(x ‚àía)(x ‚àíc)2(x ‚àíb) = 0. (9.50)
Since r(x) is cubic, r(4)(Œæ) = 0, and so (9.50) can be used to say that
f (x) = r(x) + 1
4!f (4)(Œæ(x))(x ‚àía)(x ‚àíc)2(x ‚àíb)
(9.51)
for x ‚àà(a, b). This is valid at the endpoints of [a, b], so Ô¨Ånally
e(x) = f (x) ‚àír(x) = 1
4!f (4)(Œæ(x))(x ‚àía)(x ‚àíc)2(x ‚àíb)
(9.52)
for x ‚àà[a, b], and Œæ(x) ‚àà[a, b]. Immediately, we see that
ES(2) = 1
4!
 b
a
f (4)(Œæ(x))(x ‚àía)(x ‚àíc)2(x ‚àíb) dx.
(9.53)
The polynomial in the integrand of (9.53) does not change sign on [a, b]. Thus, the
second mean-value theorem for integrals is applicable. Hence, for some Œæ ‚àà(a, b),
we have
ES(2) = f (4)(Œæ)
4!
 b
a
(x ‚àía)(x ‚àíc)2(x ‚àíb) dx,
(9.54)
which reduces to
ES(2) = ‚àíh5
90f (4)(Œæ)
(9.55)
again for some Œæ ‚àà(a, b), where h = (b ‚àía)/2.
We need an expression for ES(n), that is, an error expression for the compos-
ite Simpson rule. We will assume again that h = (b ‚àía)/n, where n is an even
number. Consequently
ES(n) =
 b
a
f (x) dx ‚àíS(n) =
n/2

k=1
Ek,
(9.56)
TLFeBOOK

SIMPSON‚ÄôS RULE
383
where Ek is the error committed in the approximation for the kth subinterval
[x2(k‚àí1), x2k]. Thus, for Œæk ‚àà[x2(k‚àí1), x2k], with k = 1, 2, . . . , n/2, we have
Ek = ‚àíh5
90f (4)(Œæk) = ‚àíh4
90
b ‚àía
n
f (4)(Œæk).
(9.57)
Therefore
ES(n) = ‚àíh4
180
b ‚àía
n/2
n/2

k=1
f (4)(Œæk).
(9.58)
Applying the intermediate-value theorem to the average
1
n/2
n/2
k=1 f (4)(Œæk) con-
Ô¨Årms that there is a Œæ ‚àà(a, b) such that
1
n/2
n/2

k=1
f (4)(Œæk) = f (4)(Œæ),
so therefore the truncation error expression for the composite Simpson rule becomes
ES(n) = ‚àíh4
180(b ‚àía)f (4)(Œæ)
(9.59)
for some Œæ ‚àà(a, b). For convenience, we repeat the truncation error expression for
the composite trapezoidal rule:
ET (n) = ‚àíh2
12(b ‚àía)f (2)(Œæ).
(9.60)
It is not really obvious which rule, trapezoidal or Simpson‚Äôs, is better in general.
For a particular interval [a, b] and n, the two expressions depend on different
derivatives of f (x). It is possible that Simpson‚Äôs rule may not be an improvement
on the trapezoidal rule in particular cases for this reason. More speciÔ¨Åcally, a
function that is not too smooth can be expected to have ‚Äúbig‚Äù higher derivatives.
Simpson‚Äôs rule has a truncation error dependent on the fourth derivative, while the
trapezoidal rule has an error that depends only on the second derivative. Thus, a
nonsmooth function might be better approximated by the trapezoidal rule than by
Simpson‚Äôs. In fact, Davis and Rabinowitz [3, p. 26] state that
The more ‚ÄúreÔ¨Åned‚Äù a rule of approximate integration is, the more certain we must
be that it has been applied to a function which is sufÔ¨Åciently smooth. There may be
little or no advantage in using a ‚Äòbetter‚Äô rule for a function that is not smooth.
We repeat a famous example from Ref. 3 (originally due to Salzer and Levine).
TLFeBOOK

384
NUMERICAL INTEGRATION AND DIFFERENTIATION
Example 9.2
The following series deÔ¨Ånes a function due to Weierstrass that
happens to be continuous but is, surprisingly, not differentiable anywhere2
W(x) =
‚àû

n=1
1
2n cos(7nœÄx).
(9.61a)
If we assume that we may integrate this expression term by term, then
I (y) =
 y
0
W(x) dx = 1
œÄ
‚àû

n=1
1
7n2n sin(7nœÄy).
(9.61b)
Of course, I =
" b
a W(x) dx = I (b) ‚àíI (a). The series (9.61b) gives the ‚Äúexact‚Äù
value for I (y) and so may be compared to estimates produced by the trapezoidal
and Simpson rules. Assuming that n = 100, the following table of values is obtained
[MATLAB implementation of (9.61) and the numerical integration rules]:
Interval
Exact
Trapezoidal
Error
Simpson‚Äôs
Error
[a, b]
Value I
T (n)
I ‚àíT (n)
S(n)
I ‚àíS(n)
[0,.1]
0.01899291
0.01898760
0.00000531
0.01901426
‚àí0.00002135
[.1, .2]
‚àí0.04145650
‚àí0.04143815
‚àí0.00001834
‚àí0.04146554
0.00000904
[.2, .3]
0.03084617
0.03084429
0.00000188
0.03086261
‚àí0.00001645
[.3, .4]
0.00337701
0.00342534
‚àí0.00004833
0.00341899
‚àí0.00004198
[.4, .5]
‚àí0.03298025
‚àí0.03300674
0.00002649
‚àí0.03303611
0.00005586
We see that the errors involved in both the trapezoidal and Simpson rules do
not differ greatly from each other. So Simpson‚Äôs rule has no advantage here.
It is commonplace for integration problems to involve integrals that possess
oscillatory integrands. For example, the Fourier transform of x(t) is deÔ¨Åned to be
X(œâ) =
 ‚àû
‚àí‚àû
x(t)e‚àíjœât dt =
 ‚àû
‚àí‚àû
x(t) cos(œât) dt ‚àíj
 ‚àû
‚àí‚àû
x(t) sin(œât) dt.
(9.62)
You will likely see much of this integral in other books and associated courses
(e.g., signals and systems). Also, determination of the Fourier series coefÔ¨Åcients
required computing [recall Eq. (1.45)]
fn = 1
2œÄ
 2œÄ
0
f (x)e‚àíjnx dx.
(9.63)
2Proof that W(x) is continuous but not differentiable is quite difÔ¨Åcult. There are many different
Weierstrass functions possessing this property of continuity without differentiability. Another example
complete with a proof appears on pp. 38‚Äì41 of K¬®orner [4].
TLFeBOOK

GAUSSIAN QUADRATURE
385
An integrand is said to be rapidly oscillatory if there are numerous (i.e., of the
order of ‚â•10) local maxima and minima over the range of integration (i.e., here
assumed to be the Ô¨Ånite interval [a, b]). Some care is often required to compute
these properly with the aid of the integration rules we have considered so far.
However, we will consider only a simple idea called integration between the zeros.
Davis and Rabinowitz have given a more detailed consideration of how to handle
oscillatory integrands [3, pp. 53‚Äì68].
Relevant to the computation of (9.63) is, for example, the integral
I =
 2œÄ
0
f (x) sin(nx) dx.
(9.64)
It may be that f (x) oscillates very little or not at all on [0, 2œÄ]. We may therefore
replace (9.64) with
I =
2n‚àí1

k=0
 (k+1)œÄ/n
kœÄ/n
f (x) sin(nx) dx.
(9.65)
The endpoints of the ‚Äúsubintegrals‚Äù
Ik =
 (k+1)œÄ/n
kœÄ/n
f (x) sin(nx) dx
(9.66)
in (9.65) are the zeros of sin(nx) on [0, 2œÄ]. Thus, we are truly proposing integra-
tion between the zeros. At this point it is easiest to approximate (9.66) with either
the trapezoidal or Simpson‚Äôs rules for all k. Since the endpoints of the integrands
in (9.66) are zero-valued, we can expect some savings in computation as a result
because some of the terms in the rules (9.14) and (9.41) will be zero-valued.
9.4
GAUSSIAN QUADRATURE
Gaussian quadrature is a numerical integration method that uses a higher order of
interpolation than do either the trapezoidal or Simpson rules. A detailed derivation
of the method is rather involved as it relies on Hermite interpolation, orthogonal
polynomial theory, and some aspects of the theory rely on issues relating to linear
system solution. Thus, only an outline presentation is given here. However, a
complete description may be found in Hildebrand [5, pp. 382‚Äì400]. Additional
information is presented in Davis and Rabinowitz [3].
It helps to recall ideas from Chapter 6 here. If we know f (x) for x = xj,
where j = 0, 1, . . . , n ‚àí1, n, then the Lagrange interpolating polynomial is (with
p(xj) = f (xj))
p(x) =
n

j=0
f (xj)Lj(x),
(9.67)
TLFeBOOK

386
NUMERICAL INTEGRATION AND DIFFERENTIATION
where
Lj(x) =
n
/
i=0
iÃ∏=j
x ‚àíxi
xj ‚àíxi
.
(9.68)
Recalling (6.45b), we may similarly deÔ¨Åne
œÄ(x) =
n
/
i=0
(x ‚àíxi).
(9.69)
Consequently
œÄ(1)(x) =
n

j=0
Ô£±
Ô£¥Ô£¥Ô£≤
Ô£¥Ô£¥Ô£≥
n
/
k=0
kÃ∏=j
(x ‚àíxk)
Ô£º
Ô£¥Ô£¥Ô£Ω
Ô£¥Ô£¥Ô£æ
(9.70)
so that
œÄ(1)(xi) =
n
/
k=0
kÃ∏=i
(xi ‚àíxk),
(9.71)
which allows us to rewrite (9.68) as
Lj(x) =
œÄ(x)
œÄ(1)(xj)(x ‚àíxj)
(9.72)
for j = 0, 1, . . . , n. From (6.14)
e(x) = f (x) ‚àíp(x) =
1
(n + 1)!f (n+1)(Œæ)œÄ(x)
(9.73)
for some Œæ ‚àà[a, b], and Œæ = Œæ(x).
We now summarize Hermite interpolation (recall Section 6.4 for more detail).
Suppose that we have knowledge of both f (x) and f (1)(x) at x = xj (again j =
0, 1, . . . , n). We may interpolate f (x) using a polynomial of degree 2n + 1 since
we must match the polynomial to both f (x) and f (1)(x) at x = xj. Thus, we need
the polynomial
p(x) =
n

k=0
hk(x)f (xk) +
n

k=0
ÀÜhk(x)f (1)(xk),
(9.74)
where hk(x) and ÀÜhk(x) are both polynomials of degree 2n + 1 that we must deter-
mine according to the constraints of our interpolation problem.
If
hi(xj) = Œ¥i‚àíj, ÀÜhi(xj) = 0,
(9.75a)
TLFeBOOK

GAUSSIAN QUADRATURE
387
then p(xj) = f (xj), and if we have
h(1)
i (xj) = 0, ÀÜh(1)
i (xj) = Œ¥i‚àíj,
(9.75b)
then p(1)(xj) = f (1)(xj) for all j = 0, 1, . . . , n. Using (9.75), it is possible to
arrive at the conclusion that
hi(x) = [1 ‚àí2L(1)
i (xi)(x ‚àíxi)][Li(x)]2
(9.76a)
and
ÀÜhi(x) = (x ‚àíxi)[Li(x)]2.
(9.76b)
Equation (9.74) along with (9.76) is Hermite‚Äôs interpolating formula. [Both parts
of Eq. (9.76) are derived on pp. 383‚Äì384 of Ref. 5, as well as in Theorem 6.1 of
Chapter 6 (below).] It is further possible to prove that for p(x) in (9.74) we have
the error function
e(x) = f (x) ‚àíp(x) =
1
(2n + 2)!f (2n+2)(Œæ)[œÄ(x)]2,
(9.77)
where Œæ ‚àà[a, b] and Œæ = Œæ(x).
From (9.77) and (9.74), we obtain
f (x) =
n

k=0
hk(x)f (xk) +
n

k=0
ÀÜhk(x)f (1)(xk) +
1
(2n + 2)!f (2n+2)(Œæ(x))[œÄ(x)]2.
(9.78)
Suppose that w(x) ‚â•0 for x ‚àà[a, b]. Function w(x) is intended to be a weighting
function such as seen in Chapter 5. Consequently, from (9.78)
 b
a
w(x)f (x) dx =
n

k=0
 b
a
w(x)hk(x) dx

f (xk)
+
n

k=0
 b
a
w(x)ÀÜhk(x) dx

f (1)(xk)
+
1
(2n + 2)!
 b
a
f (2n+2)(Œæ(x))w(x)[œÄ(x)]2 dx
#
$%
&
=E
,
(9.79)
where a < Œæ(x) < b, if a < xk < b. This can be rewritten as
 b
a
w(x)f (x) dx =
n

k=0
Hkf (xk) +
n

k=0
ÀÜHkf (1)(xk) + E,
(9.80)
TLFeBOOK

388
NUMERICAL INTEGRATION AND DIFFERENTIATION
where
Hk =
 b
a
w(x)hk(x) dx =
 b
a
w(x)[1 ‚àí2L(1)
k (xk)(x ‚àíxk)][Lk(x)]2 dx (9.81a)
and
ÀÜHk =
 b
a
w(x)ÀÜhk(x) dx =
 b
a
w(x)(x ‚àíxk)[Lk(x)]2 dx.
(9.81b)
If we neglect the term E in (9.80), then the resulting approximation to
" b
a w(x)
f (x) dx is called the Hermite quadrature formula. Since we are assuming that
w(x) ‚â•0, the second mean-value theorem for integrals allows us to claim that
E =
1
(2n + 2)!f (2n+2)(Œæ)
 b
a
w(x)[œÄ(x)]2 dx
(9.82)
for some Œæ ‚àà[a, b].
Now, recalling (9.72), we see that (9.81b) can be rewritten as
ÀÜHk =
 b
a
w(x)(x ‚àíxk)

œÄ(x)
œÄ(1)(xk)(x ‚àíxk)
2
dx
=
1
œÄ(1)(xk)
 b
a
w(x)œÄ(x)

œÄ(x)
œÄ(1)(xk)(x ‚àíxk)

dx
=
1
œÄ(1)(xk)
 b
a
w(x)œÄ(x)Lk(x) dx.
(9.83)
We recall from Chapter 5 that an inner product on L2[a, b] is (f, g are real-valued)
‚ü®f, g‚ü©=
 b
a
w(x)f (x)g(x) dx.
(9.84)
Thus, ÀÜHk = 0 for k = 0, 1, . . . , n if œÄ(x) is orthogonal to Lk(x) over [a, b] with
respect to the weighting function w(x). Since deg(Lk(x)) = n (all k), this will be
the case if œÄ(x) is orthogonal to all polynomials of degree ‚â§n over [a, b] with
respect to the weighting function w(x). Note that deg(œÄ(x)) = n + 1 [recall (9.69)].
In fact, the polynomial œÄ(x) of degree n + 1 is orthogonal to all polynomials
of degree ‚â§n over [a, b] with respect to w(x), the Hermite quadrature formula
reduces to the simpler form
 b
a
w(x)f (x) dx =
n

k=0
Hkf (xk) + E,
(9.85)
where
E =
1
(2n + 2)!f (2n+2)(Œæ)
 b
a
w(x)[œÄ(x)]2 dx,
(9.86)
TLFeBOOK

GAUSSIAN QUADRATURE
389
and where x0, x1, . . . , xn are the zeros of œÄ(x) (such that a < xk < b). A formula of
this type is called a Gaussian quadrature formula. The weights Hk are sometimes
called Christoffel numbers. We see that this numerical integration methodology
requires us to possess samples of f (x) at the zeros of œÄ(x). Variations on this
theory can be used to remove this restriction [6], but we do not consider this
matter in this book. However, if f (x) is known at all x ‚àà[a, b], then this is not a
serious restriction.
In any case, to apply the approximation
 b
a
w(x)f (x) dx ‚âà
n

k=0
Hkf (xk),
(9.87)
it is clear that we need a method to determine the Christoffel numbers Hk. It is
possible to do this using the Christoffel‚ÄìDarboux formula [Eq. (5.11); see also
Theorem 5.2]. From this it can be shown that
Hk = ‚àí
œÜn+2,n+2
œÜn+1,n+1œÜ(1)
n+1(xk)œÜn+2(xk)
,
(9.88)
where polynomial œÜr(x) is obtained, for instance, from (5.5) and where
œÜn+1(x) = œÜn+1,n+1œÄ(x).
(9.89)
Thus, we identify the zeros of œÄ(x) with the zeros of orthogonal polynomial
œÜn+1(x) [recalling that ‚ü®œÜi, œÜj‚ü©= Œ¥i‚àíj with respect to inner product (9.84)].
Since there are an inÔ¨Ånite number of choices for orthogonal polynomials œÜk(x)
and there is a theory for creating them (Chapter 5), it is possible to choose œÄ(x) in
an inÔ¨Ånite number of ways. We have implicitly assumed that [a, b] is a Ô¨Ånite length
interval, but this assumption is actually entirely unnecessary. InÔ¨Ånite or semiinÔ¨Ånite
intervals of integration are permitted. Thus, for example, œÄ(x) may be associated
with the Hermite polynomials of Section 5.4, as well as with the Chebyshev or
Legendre polynomials.
Let us consider as an example the case of Chebyshev polynomials of the Ô¨Årst
kind (Ô¨Årst seen in Section 5.3). In this case
w(x) =
1
‚àö
1 ‚àíx2
with [a, b] = [‚àí1, 1], and we will obtain the Chebyshev‚ÄìGauss quadrature rule.
The Chebyshev polynomials of the Ô¨Årst kind are Tk(x) = cos[k cos‚àí1 x], so, via
(5.55) for k > 0, we have
œÜk(x) =
8
2
œÄ Tk(x).
(9.90)
TLFeBOOK

390
NUMERICAL INTEGRATION AND DIFFERENTIATION
From the recursion for Chebyshev polynomials of the Ô¨Årst kind [Eq. (5.57)], we
have (k > 0)
Tk,k = 2k‚àí1
(9.91)

Tk(x) = k
j=0 Tk,jxj
. Thus
œÜk,k =
8
2
œÄ 2k‚àí1.
(9.92)
We have that Tk(x) = 0 for
x = xi = cos
2i + 1
2k
œÄ

(9.93)
(i = 0, 1, . . . , k ‚àí1). Additionally
T (1)
k
(x) = k sin[k cos‚àí1 x]
1
‚àö
1 ‚àíx2 ,
(9.94)
so, if, for convenience, we deÔ¨Åne Œ±i = 2i+1
2k œÄ, then xi = cos Œ±i, and therefore
T (1)
k
(xi) = k sin(kŒ±i)
sin Œ±i
= k
sin
2i + 1
2
œÄ

sin Œ±i
= k (‚àí1)i
sin Œ±i
.
(9.95)
Also
Tk+1(xi) = cos

(k + 1)
 2i + 1
2k
œÄ
!
= cos
2i + 1
2
œÄ + 2i + 1
2k
œÄ

= cos
2i + 1
2
œÄ

cos
2i + 1
2k
œÄ

‚àísin
2i + 1
2
œÄ

sin
2i + 1
2k
œÄ

= ‚àísin
2i + 1
2
œÄ

sin Œ±i = (‚àí1)i+1 sin Œ±i.
(9.96)
Therefore, (9.88) becomes
Hk = ‚àí
8
2
œÄ 2n+1
'8
2
œÄ 2n
( '8
2
œÄ (n + 1) (‚àí1)k
sin Œ±k
( '8
2
œÄ (‚àí1)k+1 sin Œ±k
( =
œÄ
n + 1. (9.97)
Thus, the weights (Christoffel numbers) are all the same in this particular case. So,
(9.87) is now
 1
‚àí1
f (x)
‚àö
1 ‚àíx2 dx ‚âà
œÄ
n + 1
n

k=0
f

cos
 2k + 1
2n + 2œÄ
!
= C(n).
(9.98)
TLFeBOOK

GAUSSIAN QUADRATURE
391
The error expression E in (9.86) can also be reduced accordingly, but we will omit
this here (again, see Hildebrand [5]).
A simple example of the application of (9.98) is as follows.
Example 9.3
Suppose that f (x) =
‚àö
1 ‚àíx2, in which case
 1
‚àí1
f (x)
‚àö
1 ‚àíx2 dx =
 1
‚àí1
dx = 2.
For this case (9.98) becomes
 1
‚àí1
dx ‚âàC(n) =
œÄ
n + 1
n

k=0
sin
2k + 1
2n + 2œÄ

.
For various n, we have the following table of values:
n
C(n)
1
2.2214
2
2.0944
5
2.0230
10
2.0068
20
2.0019
100
2.0001
Finally, we remark that the error expression in (9.82) suggests that the method
of this section is worth applying only if f (x) is sufÔ¨Åciently smooth. This is con-
sistent with comments made in Section 9.3 regarding how to choose between the
trapezoidal and Simpson‚Äôs rules. In the next example f (x) = e‚àíx, which is a very
smooth function.
Example 9.4
Here we will consider
I =
 1
‚àí1
e‚àíx dx = e ‚àí1
e = 2.350402387
and compare the approximation to I obtained by applying Simpson‚Äôs rule and
Legendre‚ÄìGauss quadrature. We will assume n = 2 in both cases.
Let us Ô¨Årst consider application of Simpson‚Äôs rule. Since x0 = a = ‚àí1, x1 =
0, x1 = b = 1 (h = (b ‚àía)/n = (1 ‚àí(‚àí1))/2 = 1), we have via (9.41)
S(2) = 1
3[e+1 + 4e0 + e‚àí1] = 2.362053757
TLFeBOOK

392
NUMERICAL INTEGRATION AND DIFFERENTIATION
for which the error is
ES(2) = I ‚àíS(2) = ‚àí0.011651.
Now let us consider the Legendre‚ÄìGauss quadrature for our problem. We recall
that for Legendre polynomials the weight function is w(x) = 1 for all x ‚àà[‚àí1, 1]
(Section 5.5). From Section 5.6 we have
œÜ3(x) =
1
||P3||P3(x) =
8
7
2
1
2[5x3 ‚àí3x],
œÜ4(x) =
1
||P4||P4(x) =
8
9
2
1
8[35x4 ‚àí30x2 + 3].
Consequently, œÜ3,3 = 5
2
7
7
2, and the zeros of œÜ3(x) are at x = 0, ¬±
7
3
5, so now our
sample points (grid points, mesh points) are
x0 = ‚àí
8
3
5, x1 = 0, x2 = +
8
3
5.
Hence, since œÜ(1)
3 (x) =
7
7
2
1
2[15x2 ‚àí3], we have
œÜ(1)
3 (x0) = 3
8
7
2, œÜ(1)
3 (x1) = ‚àí3
2
8
7
2, œÜ(1)
3 (x2) = 3
8
7
2.
Also, œÜ4,4 = 35
8
7
9
2, and
œÜ4(x0) = ‚àí3
10
8
9
2, œÜ4(x1) = 3
8
8
9
2, œÜ4(x2) = ‚àí3
10
8
9
2.
Therefore, from (9.88) the Christoffel numbers are
H0 = 5
9, H1 = 8
9, H2 = 5
9.
From (9.87) the resulting quadrature is
 1
‚àí1
e‚àíx dx ‚âàH0e‚àíx0 + H1e‚àíx1 + H2e‚àíx2 = L(2)
with
L(2) = 2.350336929,
TLFeBOOK

ROMBERG INTEGRATION
393
and the corresponding error is
EL(2) = I ‚àíL(2) = 6.5458 √ó 10‚àí5.
Clearly, |EL(2)| ‚â™|ES(2)|. Thus, the Legendre-Gauss quadrature is much more
accurate than Simpson‚Äôs rule. Considering how small n is here, the accuracy of the
Legendre‚ÄìGauss quadrature is remarkably high.
9.5
ROMBERG INTEGRATION
Romberg integration is a recursive procedure that seeks to improve on the trape-
zoidal and Simpson rules. But before we consider this numerical integration
methodology, we will look at some more basic ideas.
Suppose that I =
" b
a f (x) dx and I (n) is a quadrature that approximates I. For
us, I (n) will be either the trapezoidal rule T (n) from (9.14), or else it will be
Simpson‚Äôs rule S(n) from (9.41). It could also be the corrected trapezoidal rule
TC(n), which is considered below [see either (9.104), or (9.107)].
It is possible to improve on the ‚Äúbasic‚Äù trapezoidal rule from Section 9.2. Begin
by recalling (9.27)
ET (n) = ‚àí1
12h3
n

k=1
f (2)(Œæk),
(9.99)
where h = (b ‚àía)/n, x0 = a, xn = b, and Œæk ‚àà[xk‚àí1, xk]. Of course, xk ‚àíxk‚àí1 =
h (uniform sampling grid). Assuming (as usual) that f (k)(x) is Riemann integrable
for all k ‚â•0, then
lim
n‚Üí‚àû
n

k=1
hf (2)(Œæk) = f (1)(b) ‚àíf (1)(a) =
 b
a
f (2)(x) dx.
(9.100)
Thus, we have the approximation
n

k=1
hf (2)(Œæk) ‚âàf (1)(b) ‚àíf (1)(a).
(9.101)
Consequently,
ET (n) = ‚àí1
12h2
n

k=1
hf (2)(Œæk) ‚âà‚àí1
12h2[f (1)(b) ‚àíf (1)(a)]
(9.102)
or
I ‚àíT (n) ‚âà‚àí1
12h2[f (1)(b) ‚àíf (1)(a)].
(9.103)
TLFeBOOK

394
NUMERICAL INTEGRATION AND DIFFERENTIATION
This immediately suggests that we can improve on the trapezoidal rule by replacing
T (n) with the new approximation
TC(n) = T (n) ‚àí1
12h2[f (1)(b) ‚àíf (1)(a)],
(9.104)
where TC(n) denotes the corrected trapezoidal rule approximation to I. Clearly,
once we have T (n), rather little extra effort is needed to obtain TC(n).
In fact, we do not necessarily need to know f (1)(x) exactly anywhere, much
less at the points x = a or x = b. In the next section we will argue that either
f (1)(x) = 1
2h[‚àí3f (x) + 4f (x + h) ‚àíf (x + 2h)] + 1
3h2f (3)(Œæ)
(9.105a)
or that
f (1)(x) = 1
2h[3f (x) ‚àí4f (x ‚àíh) + f (x ‚àí2h)] + 1
3h2f (3)(Œæ).
(9.105b)
In (9.105a) Œæ ‚àà[x, x + 2h], while in (9.105b) Œæ ‚àà[x ‚àí2h, x]. Consequently, with
x0 = a for Œæ0 ‚àà[a, a + 2h], we have [via (9.105a)]
f (1)(x0) = f (1)(a) = 1
2h[‚àí3f (x0) + 4f (x1) ‚àíf (x2)] + 1
3h2f (3)(Œæ0), (9.106a)
and with xn = b for Œæn ‚àà[b ‚àí2h, b], we have [via (9.105b)]
f (1)(xn) = f (1)(b) = 1
2h[3f (xn) ‚àí4f (xn‚àí1) + f (xn‚àí2)] + 1
3h2f (3)(Œæn).
(9.106b)
Thus, (9.104) becomes (approximate corrected trapezoidal rule)
TC(n) = T (n) ‚àíh
24[3f (xn) ‚àí4f (xn‚àí1) + f (xn‚àí2) + 3f (x0) ‚àí4f (x1) + f (x2)]
‚àíh4
36[f (3)(Œæn) ‚àíf (3)(Œæ0)].
(9.107)
Of course, in evaluating (9.107) we would exclude the terms involving f (3)(Œæ0),
and f (3)(Œæn).
As noted in Epperson [7] for the trapezoidal and Simpson rules
I ‚àíI (n) ‚àù1
np ,
(9.108)
where p = 2 for I (n) = T (n) and p = 4 for I (n) = S(n). In other words, for a
given rule and a suitable constant C we must have I ‚àíI (n) ‚âàCn‚àíp. Now observe
TLFeBOOK

ROMBERG INTEGRATION
395
that we may deÔ¨Åne the ratio
r4n = I (n) ‚àíI (2n)
I (2n) ‚àíI (4n) ‚âà
(I ‚àíCn‚àíp) ‚àí(I ‚àíC(2n)‚àíp)
(I ‚àíC(2n)‚àíp) ‚àí(I ‚àíC(4n)‚àíp)
=
(2n)‚àíp ‚àín‚àíp
(4n)‚àíp ‚àí(2n)‚àíp =
2‚àíp ‚àí1
4‚àíp ‚àí2‚àíp = 2p.
(9.109)
Immediately we conclude that
p ‚âàlog2 r4n = log10 r4n
log10 2 .
(9.110)
This is useful as a check on program implementation of our quadratures. If (9.110)
is not approximately satisÔ¨Åed when we apply the trapezoidal or Simpson rules,
then (1) the integrand f (x) is not smooth enough for our theories to apply, (2)
there is a ‚Äúbug‚Äù in the program, or (3) the error may not be decreasing quickly
with n because it is already tiny to begin with, as might happen when integrating
an oscillatory function using Simpson‚Äôs rule.
The following examples illustrate the previous principles.
Example 9.5
In this example we consider approximating
I =
 œÄ/2
0
sin x dx = 1
using T (n) [via (9.14)], TC(n) [via (9.104)], and S(n) [via (9.41)]. Parameter p in
(9.110) is computed for each of these cases, and is displayed in the following table
(where ‚ÄúNaN‚Äù means ‚Äúnot a number‚Äù):
n
T (n)
p for T (n)
TC(n)
p for TC(n)
S(n)
p for S(n)
2 0.94805945
NaN
0.99946364
NaN
1.00227988
NaN
4 0.98711580
NaN
0.99996685
NaN
1.00013458
NaN
8 0.99678517
2.0141
0.99999793
4.0169
1.00000830
4.0864
16 0.99919668
2.0035
0.99999987
4.0042
1.00000052
4.0210
32 0.99979919
2.0009
0.99999999
4.0010
1.00000003
4.0052
64 0.99994980
2.0002
1.00000000
4.0003
1.00000000
4.0013
128 0.99998745
2.0001
1.00000000
4.0001
1.00000000
4.0003
256 0.99999686
2.0000
1.00000000
4.0000
1.00000000
4.0001
We see that since f (x) = sin x is a rather smooth function we obtain the values
for p that we expect to see.
Example 9.6
This example is in contrast with the previous one. Here we
approximate
I =
 1
0
x1/3 dx = 3
4
TLFeBOOK

396
NUMERICAL INTEGRATION AND DIFFERENTIATION
using T (n) [via (9.14)], TC(n) [via (9.107)], and S(n) [via (9.41)]. Again, p
[via (9.110)] is computed for each of these cases, and the results are tabulated
as follows:
n
T (n)
p for T (n)
TC(n)
p for TC(n)
S(n)
p for S(n)
2 0.64685026
NaN
0.69580035
NaN
0.69580035
NaN
4 0.70805534
NaN
0.72437494
NaN
0.72845703
NaN
8 0.73309996
1.2892
0.73980487
0.8890
0.74144817
1.3298
16 0.74322952
1.3059
0.74595297
1.3275
0.74660604
1.3327
32 0.74729720
1.3163
0.74839388
1.3327
0.74865310
1.3332
64 0.74892341
1.3227
0.74936261
1.3333
0.74946548
1.3333
128 0.74957176
1.3267
0.74974705
1.3333
0.74978788
1.3333
256 0.74982980
1.3292
0.74989962
1.3333
0.74991582
1.3333
We observe that f (x) = x1/3, but that f (1)(x) = 1
3x‚àí2/3, f (2)(x) = ‚àí2
9x‚àí5/3,
etc. Thus, the derivatives of f (x) are unbounded at x = 0, and so f (x) is not
smooth on the interval of integration. This explains why we obtain p ‚âà1.3333 in
all cases.
As a further step toward Romberg integration, consider the following. Since
I ‚àíI (2n) ‚âàC(2n)‚àíp = C2‚àípn‚àíp ‚âà2‚àíp(I ‚àíI (n)), we obtain the approximate
equality
I ‚âàI (2n) ‚àí2‚àípI (n)
1 ‚àí2‚àíp
,
or
I ‚âà2pI (2n) ‚àíI (n)
2p ‚àí1
= R(2n).
(9.111)
We call R(2n) Richardson‚Äôs extrapolated value (or Richardson‚Äôs extrapolation),
which is an improvement on I (2n). The estimated error in the extrapolation is
given by
ER(2n) = I (2n) ‚àíR(2n) = I (n) ‚àíI (2n)
2p ‚àí1
.
(9.112)
Of course, p in (9.111) and (9.112) must be the proper choice for the quadrature
I (n). We may ‚ÄúconÔ¨Årm‚Äù that (9.111) works for T (n) (as an example) by considering
(9.28)
ET (n) = ‚àíh2
12(b ‚àía)f (2)(Œæ)
(9.113)
(for some Œæ ‚àà[a, b]). Clearly
ET (2n) = ‚àí(h/2)2
12
(b ‚àía)f (2)(Œæ) ‚âà1
4ET (n),
TLFeBOOK

ROMBERG INTEGRATION
397
or in other words (ET (n) = I ‚àíT (n))
I ‚àíT (2n) ‚âà1
4[I ‚àíT (n)],
and hence for n ‚â•1
I ‚âà4T (2n) ‚àíT (n)
3
= RT (2n),
(9.114)
which is (9.111) for case p = 2. Equation (9.114) is called the Romberg integration
formula for the trapezoidal rule. Of course, a similar expression may be obtained
for Simpson‚Äôs rule [with p = 4 in (9.111)]; that is, for n even
I ‚âà16S(2n) ‚àíS(n)
15
= RS(2n).
(9.115)
In fact, it can also be shown that RT (2n) = S(2n):
S(2n) = 4T (2n) ‚àíT (n)
3
.
(9.116)
(Perhaps this is most easily seen in the special case where n = 1.) In other words,
the Romberg procedure applied to the trapezoidal rule yields the Simpson rule.
Now, Romberg integration is really the repeated application of the Richardson
extrapolation idea to the composite trapezoidal rule. A simple way to visualize the
process is with the Romberg table (Romberg array):
T (1)
T (2)
S(2)
T (4)
S(4)
RS(4)
T (8)
S(8)
RS(8)
‚Ä¢
T (16)
S(16)
RS(16)
‚Ä¢
‚Ä¢
...
...
...
...
...
In its present form the table consists of only three columns, but the recursive
process may be continued to produce a complete ‚Äútriangular array.‚Äù
The complete Romberg integration procedure is often fully justiÔ¨Åed and devel-
oped with respect to the following theorem.
Theorem 9.1: Euler‚ÄìMaclaurin Formula
Let f ‚ààC2k+2[a, b] for some
k ‚â•0, and let us approximate I =
" b
a f (x) dx by the composite trapezoidal rule
TLFeBOOK

398
NUMERICAL INTEGRATION AND DIFFERENTIATION
of (9.14). Letting hn = (b ‚àía)/n for n ‚â•1, we have
T (n) = I +
k

i=1
B2i
(2i)!h2i
n [f (2i‚àí1)(b) ‚àíf (2i‚àí1)(a)]
+
B2k+2
(2k + 2)!h2k+2
n
(b ‚àía)f (2k+2)(Œ∑),
(9.117)
where Œ∑ ‚àà[a, b], and for j ‚â•1
B2j = (‚àí1)j‚àí1
 ‚àû

n=1
2
(2œÄn)2j
	
(2j)!
(9.118)
are the Bernoulli numbers.
Proof
This is Property 9.3 in Quarteroni et al. [8]. A proof appears in Ralston
[9]. Alternative descriptions of the Bernoulli numbers appear in Gradshteyn and
Ryzhik [10]. Although not apparent from (9.118), the Bernoulli numbers are all
rational numbers.
We will present the complete Romberg integration process in a more straight-
forward manner. Begin by considering the following theorem.
Theorem 9.2: Recursive Trapezoidal Rule
Suppose that h = (b ‚àía)/(2n);
then, for n ‚â•1 (x0 = a, xn = b)
T (2n) = 1
2T (n) + h
n

k=1
f (x0 + (2k ‚àí1)h).
(9.119)
The Ô¨Årst column in the Romberg table is given by
T (2n) = 1
2T (2n‚àí1) + xn ‚àíx0
2n
2n‚àí1

k=1
f
 
x0 + (2k ‚àí1)(xn ‚àíx0)
2n
!
(9.120)
for all n ‚â•1.
Proof
Omitted, but clearly (9.120) immediately follows from (9.119).
We let R(k)
n
denote the row n, column k entry of the Romberg table, where k =
0, 1, . . . , N and n = k, . . . , N [i.e., we construct an (N + 1) √ó (N + 1) lower trian-
gular array, as suggested earlier]. Table entries are ‚Äúblank‚Äù for n = 0, 1, . . . , k ‚àí1
in column k. The Ô¨Årst column of the table is certainly
R(0)
n
= T (2n)
(9.121)
TLFeBOOK

ROMBERG INTEGRATION
399
for n = 0, 1, . . . , N. From Theorem 9.2 we must have
R(0)
0
= b ‚àía
2
[f (a) + f (b)],
(9.122a)
R(0)
n
= 1
2R(0)
n‚àí1 + b ‚àía
2n
2n‚àí1

k=1
f
 
a + (2k ‚àí1)(b ‚àía)
2n
!
,
(9.122b)
for n = 1, 2, . . . , N. Equations (9.122) are the algorithm for constructing the Ô¨Årst
column of the Romberg table. The second column is R(1)
n , and these numbers are
given by [via (9.114)]
R(1)
n
=
4R(0)
n
‚àíR(0)
n‚àí1
4 ‚àí1
(9.123)
for n = 1, 2, . . . , N. Similarly, the third column is R(2)
n , and these numbers are
given by [via (9.115)]
R(2)
n
=
42R(1)
n
‚àíR(1)
n‚àí1
42 ‚àí1
(9.124)
for n = 2, 3, . . . , N. The pattern suggested by (9.123) and (9.124) generalizes
according to
R(k)
n
=
4kR(k‚àí1)
n
‚àíR(k‚àí1)
n‚àí1
4k ‚àí1
(9.125)
for n = k, . . . , N, with k = 1, 2, . . . , N. Assuming that f (x) is sufÔ¨Åciently smooth,
we can estimate the error using the Richardson extrapolation method in this manner:
E(k)
n
=
R(k)
n‚àí1 ‚àíR(k)
n
4k ‚àí1
.
(9.126)
This can be used to stop the recursive process of table construction when E(k)
n
is small enough. Recall that every entry R(k)
n
in the table is an estimate of I =
" b
a f (x) dx. In some sense R(N)
N
is the ‚ÄúÔ¨Ånal estimate,‚Äù and will be the best one if
f (x) is smooth enough. Finally, the general appearance of the Romberg table is
R(0)
0
R(0)
1
R(1)
1
R(0)
2
R(1)
2
R(2)
2
...
...
...
...
R(0)
N‚àí1
R(1)
N‚àí1
R(2)
N‚àí1
¬∑ ¬∑ ¬∑
R(N‚àí1)
N‚àí1
R(0)
N
R(1)
N
R(2)
N
¬∑ ¬∑ ¬∑
R(N‚àí1)
N
R(N)
N
TLFeBOOK

400
NUMERICAL INTEGRATION AND DIFFERENTIATION
The Romberg integration procedure is efÔ¨Åcient. Function evaluation is conÔ¨Åned
to the construction of the Ô¨Årst column. The remaining columns are Ô¨Ålled in with
just a Ô¨Åxed (and small) number of arithmetic operations per entry as determined
by (9.125).
Example 9.7
Begin by considering the Romberg approximation to
I =
 1
0
ex dx = 1.718281828.
The Romberg table for this is (N = 3)
1.85914091
1.75393109
1.71886115
1.72722190
1.71831884
1.71828269
1.72051859
1.71828415
1.71828184
1.71828183
Table entry R(3)
3
= 1.71828183 is certainly the most accurate estimate of I. Now
contrast this example with the next one.
The zeroth-order modiÔ¨Åed Bessel function of the Ô¨Årst kind I0(y) ‚ààR(y ‚ààR)
is important in applied probability. For example, it appears in the problem of
computing bit error probabilities in amplitude shift keying (ASK) digital data com-
munications [11]. There is a series expansion expression for I0(y), but there is also
an integral form, which is
I0(y) = 1
2œÄ
 2œÄ
0
ey cos x dx.
(9.127)
For y = 1, we have (according to MATLAB‚Äôs besseli function; I0(y) =besseli(0,y))
I0(1) = 1.2660658778.
The Romberg table of estimates for this integral is (N = 4)
2.71828183
1.54308063
1.15134690
1.27154032
1.18102688
1.18300554
1.26606608
1.26424133
1.26978896
1.27116647
1.26606588
1.26606581
1.26618744
1.26613028
1.26611053
Plainly, table entry R(4)
4
= 1.26611053 is not as accurate as R(0)
4
= 1.26606588.
Apparently, the integrand of (9.127) is simply not smooth enough to beneÔ¨Åt from
the Romberg approach.
TLFeBOOK

NUMERICAL DIFFERENTIATION
401
9.6
NUMERICAL DIFFERENTIATION
A simple theory of numerical approximation to the derivative can be obtained via
Taylor series expansions (Chapter 3). Recall that [via (3.71)]
f (x + h) =
n

k=0
hk
k! f (k)(x) +
hn+1
(n + 1)!f (n+1)(Œæ)
(9.128)
for suitable Œæ ‚àà[x, x + h]. As usual, 0! = 1, and f (x) = f (0)(x). Since from ele-
mentary calculus
f (1)(x) = lim
h‚Üí0
f (x + h) ‚àíf (x)
h
‚áíf (1)(x) ‚âàf (x + h) ‚àíf (x)
h
(9.129)
from (9.128), we have
f (1)(x) = f (x + h) ‚àíf (x)
h
‚àí1
2!hf (2)(Œæ).
(9.130)
This was obtained simply be rearranging
f (x + h) = f (x) + hf (1)(x) + 1
2!h2f (2)(Œæ).
(9.131)
We may write
f (1)(x) = f (x + h) ‚àíf (x)
h
#
$%
&
= Àúf (1)
f (x)
+

‚àí1
2!hf (2)(Œæ)

#
$%
&
=ef (x)
.
(9.132)
Approximation Àúf (1)
f (x) is called the forward difference approximation to f (1)(x),
and the error ef (x) is seen to be approximately proportional to h. Now consider
(Œæ1 ‚àà[x, x + h])
f (x + h) = f (x) + hf (1)(x) + 1
2!h2f (2)(x) + 1
3!h3f (3)(Œæ1),
(9.133)
and clearly (Œæ2 ‚àà[x ‚àíh, x])
f (x ‚àíh) = f (x) ‚àíhf (1)(x) + 1
2!h2f (2)(x) ‚àí1
3!h3f (3)(Œæ2).
(9.134)
From (9.134)
f (1)(x) = f (x) ‚àíf (x ‚àíh)
h
#
$%
&
= Àúf (1)
b
(x)
+
 1
2!hf (2)(Œæ)

#
$%
&
=eb(x)
.
(9.135)
TLFeBOOK

402
NUMERICAL INTEGRATION AND DIFFERENTIATION
Here the approximation Àúf (1)
b (x) is called the backward difference approximation to
f (1)(x), and has error eb(x) that is also approximately proportional to h. However,
an improvement is possible, and this is obtained by subtracting (9.134) from (9.133)
f (x + h) ‚àíf (x ‚àíh) = 2hf (1)(x) + 1
3!h3[f (3)(Œæ1) + f (3)(Œæ2)],
or on rearranging this, we have
f (1)(x) = f (x + h) ‚àíf (x ‚àíh)
2h
#
$%
&
= Àúf (1)
c
(x)
+

‚àíh2
6
f (3)(Œæ1) + f (3)(Œæ2)
2
	
#
$%
&
=ec(x)
.
(9.136)
Recalling the derivation of (9.28), there is a Œæ ‚àà[x ‚àíh, x + h] such that
f (3)(Œæ) = 1
2[f (3)(Œæ1) + f (3)(Œæ2)]
(9.137)
(Œæ1 ‚àà[x, x + h], Œæ2 ‚àà[x ‚àíh, h]). Hence, (9.136) can be rewritten as (for some
Œæ ‚àà[x ‚àíh, x + h])
f (1)(x) = f (x + h) ‚àíf (x ‚àíh)
2h
#
$%
&
= Àúf (1)
c
(x)
+

‚àíh2
6 f (3)(Œæ)

#
$%
&
=ec(x)
.
(9.138)
Clearly, the error ec(x), which is the error of the central difference approximation
Àúf (1)
c
(x) to f (1)(x), is proportional to h2. Thus, if f (x) is smooth enough, the
central difference approximation is more accurate than the forward or backward
difference approximations.
The errors ef (x), eb(x), and ec(x) are truncation errors in the approximations.
Of course, when implementing any of these approximations on a computer there
will be rounding errors, too. Each approximation can be expressed in the form
Àúf (1)(x) = 1
hf T y,
(9.139)
where for the forward difference approximation
f = [f (x)
f (x + h)]T , y = [‚àí1
1]T ,
(9.140a)
for the backward difference approximation
f = [f (x ‚àíh)
f (x)]T , y = [‚àí1
1]T ,
(9.140b)
TLFeBOOK

NUMERICAL DIFFERENTIATION
403
and for the central difference approximation
f = [f (x ‚àíh)
f (x + h)]T , y = 1
2[‚àí1
1]T .
(9.140c)
Thus, the approximations are all Euclidean inner products of samples of f (x) with
a vector of constants y, followed by division by h. This is structurally much the
same kind of computation as numerical integration. In fact, an upper bound on the
size of the error due to rounding is given by the following theorem.
Theorem 9.3: Since f l[ Àúf (1)(x)] = f l

f l[f T y]
h

, we have (f, y ‚ààRm)
|f l[ Àúf (1)(x)] ‚àíÀúf (1)(x)| ‚â§u
h[1.01m + 1]||f ||2||y||2.
(9.141)
Proof
Our analysis here is rather similar to Example 2.4 in Chapter 2. Thus,
we exploit yet again the results from Chapter 2 on rounding errors in dot product
computation.
Via (2.41)
f l[f T y] = f T y(1 + œµ1)
for which
|œµ1| ‚â§1.01mu|f |T |y|
|f T y| .
In addition
f l[ Àúf (1)(x)] = f T y
h (1 + œµ1)(1 + œµ),
where |œµ| ‚â§u. Thus, since
f l[ Àúf (1)(x)] = f T y
h
+ f T y
h (œµ1 + œµ + œµ1œµ),
we have
|f l[ Àúf (1)(x)] ‚àíÀúf (1)(x)| ‚â§|f T y|
h
(|œµ1| + |œµ| + |œµ1||œµ|) = |f T y|
h
√ó
 
1.01mu|f |T |y|
|f T y| + u + 1.01mu2 |f |T |y|
|f T y|
!
,
and since u2 ‚â™u, we may neglect the last term, yielding
|f l[ Àúf (1)(x)] ‚àíÀúf (1)(x)| ‚â§1
h

1.01mu|f |T |y| + u|f T y|

.
TLFeBOOK

404
NUMERICAL INTEGRATION AND DIFFERENTIATION
But |f |T |y| =< |f |, |y| >‚â§||f ||2||y||2, and |f T y| = |‚ü®f, y‚ü©| ‚â§||f ||2||y||2 via
Theorem 1.1, so Ô¨Ånally we have
|f l[ Àúf (1)(x)] ‚àíÀúf (1)(x)| ‚â§u
h[1.01m + 1]||f ||2||y||2,
which is the theorem statement.
The bound in (9.141) suggests that, since we treat u, m, ||f ||2, and ||y||2 are
Ô¨Åxed, as h becomes smaller, the rounding errors will grow in size. On the other
hand, as h becomes smaller, the truncation errors diminish in size. Thus, much as
with numerical integration (recall Fig. 9.3), there is a tradeoff between rounding
and truncation errors leading in the present case to the existence of some optimal
value for the choice of h. In most practical circumstances the truncation errors will
dominate, however.
We recall that interpolation theory from Chapter 6 was useful in developing
theories on numerical integration in earlier sections of the present chapter. We
therefore reasonably expect that interpolation ideas from Chapter 6 ought to be
helpful in developing approximations to the derivative.
Recall that pn(x) = n
k=0 pn,kxk interpolates f (x) for x ‚àà[a, b], i.e., pn(xk) =
f (xk) with x0 = a, xn = b, and xk ‚àà[a, b] for all k, but such that xk Ã∏= xj for
k Ã∏= j. If f (x) ‚ààCn+1[a, b], then, from (6.14)
f (x) = pn(x) +
1
(n + 1)!f (n+1)(Œæ)
n
/
i=0
(x ‚àíxi)
(9.142)
(Œæ = Œæ(x) ‚àà[a, b]). For convenience we also deÔ¨Åned œÄ(x) = 0n
i=0(x ‚àíxi). Thus,
from (9.142)
f (1)(x) = p(1)
n (x) +
1
(n + 1)!

œÄ(1)(x)f (n+1)(Œæ) + œÄ(x) d
dx f (n+1)(Œæ)

. (9.143)
Since Œæ = Œæ(x) is a function of x that is seldom known, it is not at all clear
what df (n+1)(Œæ)/dx is in general, and to evaluate this also assumes that dŒæ(x)/dx
exists.3 We may sidestep this problem by evaluating (9.143) only for x = xk, and
since œÄ(xk) = 0 for all k, Eq. (9.143) reduces to
f (1)(xk) = p(1)
n (xk) +
1
(n + 1)!œÄ(1)(xk)f (n+1)(Œæ(xk)).
(9.144)
For simplicity we will now assume that xk = x0 + hk, h = (b ‚àía)/n (i.e.,
uniform sampling grid). We will also suppose that n = 2, in which case (9.144)
becomes
f (1)(xk) = p(1)
2 (xk) + 1
6œÄ(1)(xk)f (3)(Œæ(xk)).
(9.145)
3This turns out to be true, although it is not easy to prove. Fortunately, we do not need this result.
TLFeBOOK

NUMERICAL DIFFERENTIATION
405
Since œÄ(x) = (x ‚àíx0)(x ‚àíx1)(x ‚àíx2), we have œÄ(1)(x) = (x ‚àíx1)(x ‚àíx2) +
(x ‚àíx0)(x ‚àíx1) + (x ‚àíx0)(x ‚àíx2), and therefore
œÄ(1)(x0) = (x0 ‚àíx1)(x0 ‚àíx2)
= (‚àíh)(‚àí2h)
= 2h2,
œÄ(1)(x1) = (x1 ‚àíx0)(x1 ‚àíx2)
= h(‚àíh)
= ‚àíh2,
œÄ(1)(x2) = (x2 ‚àíx0)(x2 ‚àíx1)
= (2h)h
= 2h2.
(9.146)
From (6.9), we obtain
p2(x) = f (x0)L0(x) + f (x1)L1(x) + f (x2)L2(x),
(9.147)
where, from (6.11)
Lj(x) =
2
/
i=0
iÃ∏=j
x ‚àíxi
xj ‚àíxi
,
(9.148)
and hence the approximation to f (1)(xk) is (k ‚àà{0, 1, 2})
p(1)
n (xk) = f (x0)L(1)
0 (xk) + f (x1)L(1)
1 (xk) + f (x2)L(1)
2 (xk).
(9.149)
From (9.148)
L0(x) =
(x‚àíx1)(x‚àíx2)
(x0‚àíx1)(x0‚àíx2),
L(1)
0 (x) = (x‚àíx1)+(x‚àíx2)
(x0‚àíx1)(x0‚àíx2),
L1(x) =
(x‚àíx0)(x‚àíx2)
(x1‚àíx0)(x1‚àíx2),
L(1)
1 (x) = (x‚àíx0)+(x‚àíx2)
(x1‚àíx0)(x1‚àíx2),
L2(x) =
(x‚àíx0)(x‚àíx1)
(x2‚àíx0)(x2‚àíx1),
L(1)
2 (x) = (x‚àíx0)+(x‚àíx1)
(x2‚àíx0)(x2‚àíx1).
(9.150)
Hence
L(1)
0 (x0) = ‚àí3
2h, L(1)
0 (x1) = ‚àí1
2h, L(1)
0 (x2) = + 1
2h,
L(1)
1 (x0) = + 2
h, L(1)
1 (x1) = 0, L(1)
1 (x2) = ‚àí2
h,
(9.151)
L(1)
2 (x0) = ‚àí1
2h, L(1)
2 (x1) = + 1
2h, L(1)
2 (x2) = + 3
2h.
We therefore have the following expressions for f (1)(xk):
f (1)(x0) = 1
2h[‚àí3f (x0) + 4f (x1) ‚àíf (x2)] + 1
3h2f (3)(Œæ(x0)),
(9.152a)
f (1)(x1) = 1
2h[‚àíf (x0) + f (x2)] ‚àí1
6h2f (3)(Œæ(x1)),
(9.152b)
f (1)(x2) = 1
2h[f (x0) ‚àí4f (x1) + 3f (x2)] + 1
3h2f (3)(Œæ(x2)).
(9.152c)
TLFeBOOK

406
NUMERICAL INTEGRATION AND DIFFERENTIATION
We recognize that the case (9.152b) contains the central difference approximation
[recall (9.138)], since we may let x0 = x ‚àíh, x2 = x + h (and x1 = x). If we let
x0 = x, x1 = x + h and x2 = x + 2h, then (9.152a) yields
f (1)(x) ‚âà1
2h[‚àí3f (x) + 4f (x + h) ‚àíf (x + 2h)],
(9.153)
and if we let x2 = x, x1 = x ‚àíh, and x0 = x ‚àí2h, then (9.152c) yields
f (1)(x) ‚âà1
2h[f (x ‚àí2h) ‚àí4f (x ‚àíh) + 3f (x)].
(9.154)
Note that (9.153) and (9.154) were employed in obtaining TC(n) in (9.107).
REFERENCES
1. G. E. Forsythe, M. A. Malcolm, and C. B. Moler, Computer Methods for Mathematical
Computations, Prentice-Hall, Englewood Cliffs, NJ, 1977.
2. L. Bers, Calculus: Preliminary Edition, Vol. 2, Holt, Rinehart, Winston, New York,
1967.
3. P. J. Davis and P. Rabinowitz, Numerical Integration, Blaisdell, Waltham, MA, 1967.
4. T. W. K¬®orner, Fourier Analysis, Cambridge Univ. Press, New York, 1988.
5. F. B. Hildebrand, Introduction to Numerical Analysis, 2nd ed., McGraw-Hill, New York,
1974.
6. W. Sweldens and R. Piessens, ‚ÄúQuadrature Formulae and Asymptotic Error Expansions
for Wavelet Approximations of Smooth Functions,‚Äù SIAM J. Numer. Anal. 31, 1240‚Äì
1264 (Aug. 1994).
7. J. F. Epperson, An Introduction to Numerical Methods and Analysis, Wiley, New York,
2002.
8. A. Quarteroni, R. Sacco, and F. Saleri, Numerical Mathematics (Texts in Applied Math-
ematics series, Vol. 37), Springer-Verlag, New York, 2000.
9. A. Ralston, A First Course in Numerical Analysis, McGraw-Hill, New York, 1965.
10. I. S. Gradshteyn and I. M. Ryzhik, in Table of Integrals, Series and Products, 5th ed.,
A. Jeffrey, ed., Academic Press, San Diego, CA, 1994.
11. R. E. Ziemer and W. H. Tranter, Principles of Communications: Systems, Modulation,
and Noise, Houghton MifÔ¨Çin, Boston, MA, 1976.
PROBLEMS
9.1. This problem is based on an assignment problem due to I. Leonard. Consider
In =
 1
0
xn
x + a dx,
TLFeBOOK

PROBLEMS
407
where a ‚â•1 and n ‚ààZ+. It is easy to see that for 0 < x < 1, we have
xn+1 < xn, and 0 < In+1 < In for all n ‚ààZ+. For 0 < x < 1, we have
xn
1 + a <
xn
x + a < xn
a ,
implying that
 1
0
xn
1 + a dx < In <
 1
0
xn
a dx
or
1
(n + 1)(1 + a) < In <
1
(n + 1)a ,
so immediately limn‚Üí‚àûIn = 0. Also, we have the difference equation
In =
 1
0
xn‚àí1[x + a ‚àía]
x + a
dx = 1
n ‚àíaIn‚àí1
(9.P.1)
for n ‚ààN, where I0 =
" 1
0
1
x+a dx =

loge(x + a)
1
0 = loge

1+a
a

.
(a) Assume that ÀÜI0 = I0 + œµ is the computed value of I0. Assume that no
other errors arise in computing In for n ‚â•1 using (9.P.1). Then
ÀÜIn = 1
n ‚àía ÀÜIn‚àí1.
DeÔ¨Åne the error en = ÀÜIn ‚àíIn, and Ô¨Ånd a difference equation for en.
(b) Solve for en, and show that for large enough a we have limn‚Üí‚àû
|en| = ‚àû.
(c) Find a stable algorithm to compute In for n ‚ààZ+.
9.2. (a) Find an upper bound on the magnitude of the rounding error involved in
applying the trapezoidal rule to
 a
0
x2 dx.
(b) Find an upper bound on the magnitude of the truncation error in applying
the trapezoidal rule to the integral in (a) above.
9.3. Consider the integral
I (œµ) =
 a
œµ
‚àöx dx,
TLFeBOOK

408
NUMERICAL INTEGRATION AND DIFFERENTIATION
where a > œµ > 0. Write a MATLAB routine to Ô¨Åll in the following table for
a = 1, and n = 100:
œµ
|I(œµ) ‚àíT (n)|
BT (n)
|I(œµ) ‚àíS(n)|
BS(n)
0.1000
0.0100
0.0010
0.0001
In this table T (n) is from (9.14), S(n) is from (9.41), and
|ET (n)| ‚â§BT (n),
|ES(n)| ‚â§BS(n),
where the upper bounds BT (n) and BS(n) are obtained using (9.33) and (9.59),
respectively.
9.4. Consider the integral
I =
 1
0
dx
1 + x2 = œÄ
4 .
(a) Use the trapezoidal rule to estimate I, assuming that h = 1
4.
(b) Use Simpson‚Äôs rule to estimate I, assuming that h = 1
4.
(c) Use the corrected trapezoidal rule to estimate I, assuming that h = 1
4.
Perform all computations using only a pocket calculator.
9.5. Consider the integral
I =
 1/2
‚àí1/2
dx
1 ‚àíx2 = ln 3.
(a) Use the trapezoidal rule to estimate I, assuming that h = 1
4.
(b) Use Simpson‚Äôs rule to estimate I, assuming that h = 1
4.
(c) Use the corrected trapezoidal rule to estimate I, assuming that h = 1
4.
Perform all computations using only a pocket calculator.
9.6. Consider the integral
I =
 œÄ/2
0
sin(3x)
sin x
dx = œÄ
2 .
(a) Use the trapezoidal rule to estimate I, assuming that h = œÄ/12.
(b) Use Simpson‚Äôs Rule to estimate I, assuming that h = œÄ/12.
(c) Use the corrected trapezoidal rule to estimate I, assuming that h = œÄ/12.
Perform all computations using only a pocket calculator.
TLFeBOOK

PROBLEMS
409
9.7. The length of the curve y = f (x) for a ‚â§x ‚â§b is given by
L =
 b
a
7
1 + [f (1)(x)]2 dx.
Suppose f (x) = cos x. Compute L for a = ‚àíœÄ/2 and b = œÄ/2. Use the
trapezoidal rule, selecting n so that |ET (n)| ‚â§0.001. Hence, using (9.33),
select n such that
1
12
1
n2 (b ‚àía)3M ‚â§0.001.
Do the computations using a suitable MATLAB routine.
9.8. Recall Example 6.5. Estimate the integral
I =
 1
‚àí1
e‚àíx2 dx
by
(a) Integrating the natural spline interpolant
(b) Integrating the complete spline interpolant
9.9. Find the constants Œ± and Œ≤ in x = Œ±t + Œ≤, and Ô¨Ånd f in terms of g such
that
 b
a
g(t) dt =
 1
‚àí1
f (x)
‚àö
1 ‚àíx2 dx.
[Comment: This transformation will permit you to apply the Chebyshev‚Äì
Gauss quadrature rule from Eq. (9.98) to general integrands.]
9.10. Consider the midpoint rule (Section 9.2). If we approximate I =
" b
a f (x) dx
by one rectangle, then the rule is
R(1) = (b ‚àía)f
 a + b
2
!
,
so the truncation error involved in this approximation is
ER(1) =
 b
a
f (x) dx ‚àíR(1).
Use Taylor expansion error analysis to Ô¨Ånd an approximation to ER(1). [Hint:
With x = (a + b)/2 and h = b ‚àía, we obtain
f (x) = f (x) + (x ‚àíx)f (1)(x) + 1
2!(x ‚àíx)2f (2)(x) + ¬∑ ¬∑ ¬∑ .
Consider f (a), f (b), and
" b
a f (x) dx using this series expansion.]
TLFeBOOK

410
NUMERICAL INTEGRATION AND DIFFERENTIATION
9.11. Use both the trapezoidal rule [Eq. (9.14)] and the Chebyshev‚ÄìGauss quadra-
ture rule [Eq. (9.98)] to approximate I = 2
œÄ
" œÄ
0
sin x
x
dx for n = 6. Assuming
that I = 1.1790, which rule gives an answer closer to this value for I ? Use
a pocket calculator to do the computations.
9.12. Consider the integral
I =
 œÄ
0
cos2 x dx = œÄ
2 .
Write a MATLAB routine to approximate I using the trapezoidal rule, and
Richardson‚Äôs extrapolation. The program must allow you to Ô¨Åll in the fol-
lowing table:
n
T (n)
|I ‚àíT (n)|
R(n)
|I ‚àíR(n)|
2
4
8
16
32
64
128
256
512
1024
Of course, the extrapolated values are obtained from (9.111), where I (n) =
T (n). Does extrapolation improve on the accuracy? Comment on this.
9.13. Write a MATLAB routine that allows you to make a table similar to that in
Example 9.6, but for the integral
I =
 1
0
x1/4 dx.
9.14. The complete elliptic integral of the Ô¨Årst kind is
K(k) =
 œÄ/2
0
dŒ∏

1 ‚àík2 sin2 Œ∏
, 0 < k < 1,
and the complete elliptic integral of the second kind is
E(k) =
 œÄ/2
0

1 ‚àík2 sin2 Œ∏ dŒ∏, 0 < k < 1.
(a) Find a series expansion for K(k). [Hint: Recall (3.80).]
TLFeBOOK

PROBLEMS
411
(b) Construct a Romberg table for N = 4 for the integral K( 1
2). Use the
series expansion from (a) to Ô¨Ånd the ‚Äúexact‚Äù value of K( 1
2) and compare.
(c) Find a series expansion for E(k). [Hint: Recall (3.80).]
(d) Construct a Romberg table for N = 4 for the integral E( 1
2). Use the series
expansion from (c) to Ô¨Ånd the ‚Äúexact‚Äù value of E( 1
2) and compare.
Use MATLAB to do all of the calculations. [Comment: Elliptic integrals are
important in electromagnetic potential problems (e.g., Ô¨Ånding the magnetic
vector potential of a circular current loop), and are important in analog and
digital Ô¨Ålter design (e.g., elliptic Ô¨Ålters).]
9.15. This problem is about an alternative approach to the derivation of Gauss-type
quadrature rules. The problem statement is long, but the solution is short
because so much information has been provided. Suppose that w(x) ‚â•0 for
x ‚àà[a, b], so w(x) is some weighting function. We wish to Ô¨Ånd weights wk
and sample points xk for k = 0, 1, . . . , n ‚àí1 such that
 b
a
w(x)f (x) dx =
n‚àí1

k=0
wkf (xk)
(9.P.2)
for all f (x) = xj, where j = 0, 1, . . . , 2n ‚àí2, 2n ‚àí1. This task is greatly
aided by deÔ¨Åning the moments
mj =
 b
a
w(x)xj dx,
where mj is called the jth moment of w(x). In everything that follows it is
important to realize that because of (9.P.2)
mj =
 b
a
xjw(x) dx =
n‚àí1

k=0
wkxj
k .
The method proposed here implicitly assumes that it is easy to Ô¨Ånd the
moments mj. Once the weights and sample points are found, expression
(9.P.2) forms a quadrature rule according to
 b
a
w(x)f (x) dx ‚âà
n‚àí1

k=0
wkf (xk) = G(n ‚àí1),
where now f (x) is essentially arbitrary. DeÔ¨Åne the vectors
w = [w0w1 ¬∑ ¬∑ ¬∑ wn‚àí2wn‚àí1]T ‚ààRn,
m = [m0m1 ¬∑ ¬∑ ¬∑ m2n‚àí2m2n‚àí1]T ‚ààR2n.
TLFeBOOK

412
NUMERICAL INTEGRATION AND DIFFERENTIATION
Find matrix A ‚ààRn√ó2n such that AT w = m. Matrix A turns out to be a
rectangular Vandermonde matrix. If we knew the sample points xk, then it
would be possible to use AT w = m to solve for the weights in w. (This is
so even though the linear system is overdetermined.) DeÔ¨Åne the polynomial
pn(x) =
n‚àí1
/
j=0
(x ‚àíxj) =
n

j=0
pn,jxj
for which we see that pn,n = 1. We observe that the zeros of pn(x) happen
to be the sample points xk that we are looking for. The following suggestion
makes it possible to Ô¨Ånd the sample points xk by Ô¨Årst Ô¨Ånding the polynomial
pn(x). Using (in principle) Chapter 7 ideas, we can then Ô¨Ånd the roots of
pn(x) = 0, and so Ô¨Ånd the sample points. Consider the expression
mj+rpn,j =
n‚àí1

k=0
wkxj+r
k
pn,j,
(9.P.3)
where r = 0, 1, . . . , n ‚àí2, n ‚àí1. Consider the sum of (9.P.3) over j =
0, 1, . . . , n. Use this sum to show that
n‚àí1

j=0
mj+rpn,j = ‚àímn+r
(9.P.4)
for r = 0, 1, . . . , n ‚àí1. This can be expressed in matrix form as Mp = q,
where
M = [mi+j]i,j=0,1,...,n‚àí1 ‚ààRn√ón
which is called a Hankel matrix, and where
p = [pn,0pn,1 ¬∑ ¬∑ ¬∑ pn,n‚àí1]T ‚ààRn
and
‚àíq = [mnmn+1 ¬∑ ¬∑ ¬∑ m2n‚àí2m2n‚àí1]T .
Formal proof that M‚àí1 always exists is possible, but is omitted from con-
sideration. [Comment: The reader is warned that the approach to Gaussian
quadrature suggested here is not really practical due to the ill-conditioning
of the matrices involved (unless n is small).]
9.16. In the previous problem let a = ‚àí1, b = 1, and w(x) =
‚àö
1 ‚àíx2. Develop a
numerically reliable algorithm to compute the moments mj =
" b
a w(x)xj dx.
TLFeBOOK

PROBLEMS
413
9.17. Derive Eq. (9.88).
9.18. Repeat Example 9.4, except that the integral is now
I =
 1
‚àí1
e‚àíx2 dx.
9.19. This problem is a preview of certain aspects of probability theory. It is an
example of an application for numerical integration. An experiment produces
a measurable quantity denoted x ‚ààR. The experiment is random in that the
value of x is different from one experiment to the next, but the probability
of x lying within a particular range of values is known to be
P = P [a ‚â§x ‚â§b] =
 b
a
fX(x) dx,
where
fX(x) =
1
‚àö
2œÄœÉ 2 exp
 
‚àíx2
2œÉ 2
!
,
which is an instance of the Gaussian function mentioned in Chapter 3. This
fact may be interpreted as follows. Suppose that we perform the experiment R
times, where R is a ‚Äúlarge‚Äù number. Then on average we expect a ‚â§x ‚â§b
a total of PR times. The function fX(x) is called a Gaussian probability
density function (pdf) with a mean of zero, and variance œÉ 2. Recall Fig.
3.6, which shows the effect of changing œÉ 2. In Monte Carlo simulations of
digital communications systems, or for that matter any other system where
randomness is an important factor, it is necessary to write programs that gen-
erate simulated random variables such as x. The MATLAB randn function
will generate zero mean Gaussian random variables with variance œÉ 2 = 1.
For example, x = randn(1, N) will load N Gaussian random variables into
the row vector x. Write a MATLAB routine to generate N = 1000 simu-
lated Gaussian random variables (also called Gaussian variates) using randn.
Count the number of times x satisÔ¨Åes ‚àí1 ‚â§x ‚â§1. Let this count be denoted
ÀÜC. Your routine must also use the trapezoidal rule to estimate the probability
P = P [‚àí1 ‚â§x ‚â§1] using erf(x) [deÔ¨Åned in Eq. (3.107)]. The magnitude of
the error in computing P must be <0.0001. This will involve using the trun-
cation error bound (9.33) in Chapter 9 to estimate the number of trapezoids
n that you need to do this job. You are to neglect rounding error effects here.
Compute C = P N. Your program must print ÀÜC and C to a Ô¨Åle. Of course,
we expect ÀÜC ‚âàC.
9.20. Develop a MATLAB routine to Ô¨Åll in the following table, which uses the
central difference approximation to the Ô¨Årst derivative of a function [i.e.,
TLFeBOOK

414
NUMERICAL INTEGRATION AND DIFFERENTIATION
Àúf (1)
c
(x)] to estimate f (1)(x), where here
f (x) = loge x.
x
h = 10‚àí4 h = 10‚àí5 h = 10‚àí6 h = 10‚àí7 h = 10‚àí8
f (1)(x)
1.0
2.0
3.0
4.0
5.0
6.0
7.0
8.0
9.0
10.0
Explain the results you get.
9.21. Suppose that f (x) = e‚àíx2, and recall (9.132).
(a) Sketch f (2)(x).
(b) Let h = 1/10, and compute
Àúf (1)
f (1). Find an upper bound on |ef (1)|.
Compute |f (1)(1) ‚àíÀúf (1)
f (1)| = |ef (1)|, and compare to the bound.
(c) Let h = 1
10, and compute Àúf (1)
f (1/
‚àö
2). Find an upper bound on |ef (1/
‚àö
2)|.
Compute |f (1)(1/
‚àö
2) ‚àíÀúf (1)
f (1/
‚àö
2)| = |ef (1/
‚àö
2)|, and compare to the
bound.
9.22. Show that another approximation to f (1)(x) is given by
f (1)(x) ‚âà
1
12h[8f (x + h) ‚àí8f (x ‚àíh) ‚àíf (x + 2h) + f (x ‚àí2h)].
Give an expression for the error involved in using this approximation.
TLFeBOOK

10
Numerical Solution of Ordinary
Differential Equations
10.1
INTRODUCTION
In this chapter we consider numerical methods for the solution of ordinary differ-
ential equations (ODEs). We recall that in such differential equations the function
that we wish to solve for is in one independent variable. By contrast partial differ-
ential equations (PDEs) involve solving for functions in two or more independent
variables. The numerical solution of PDEs is a subject for a later chapter.
With respect to the level of importance of the subject the reader knows that all
dynamic systems with physical variables that change continuously over time (or
space, or both) are described in terms of differential equations, and so form the
basis for a substantial portion of engineering systems analysis, and design across
all branches of engineering. The reader is also well aware of the fact that it is quite
easy to arrive at differential equations that completely defy attempts at an analytical
solution. This remains so in spite of the existence of quite advanced methods for
analytical solution (e.g., symmetry methods that use esoteric ideas from Lie group
theory [1]), and so the need for this chapter is not hard to justify.
Where ODEs are concerned, differential equations arise within two broad cate-
gories of problems:
1. Initial-value problems (IVPs)
2. Boundary-value problems (BVPs)
In this chapter we shall restrict consideration to initial value problems. However,
this is quite sufÔ¨Åcient to accommodate much of electric/electronic circuit modeling,
modeling the orbital dynamics of satellites around planetary bodies, and many other
problems besides.
A simple example of an ODE for which no general analytical theory of solution
is known is the DufÔ¨Ång equation
md2x(t)
dt2
+ k dx(t)
dt
+ Œ±x(t) + Œ¥x3(t) = F cos(œât),
(10.1)
An Introduction to Numerical Analysis for Electrical and Computer Engineers, by C.J. Zarowski
ISBN 0-471-46737-5
c‚Éù2004 John Wiley & Sons, Inc.
415
TLFeBOOK

416
NUMERICAL SOLUTION OF ORDINARY DIFFERENTIAL EQUATIONS
where t ‚â•0. Since we are concerned with initial-value problems we would need to
know, at least implicitly, x(0), and dx(0)
dt
which are the initial conditions. If it were
the case that Œ¥ = 0 then the solution of (10.1) is straightforward because it is a
particular case of a second-order linear ODE with constant coefÔ¨Åcients. Perhaps the
best method for solving (10.1) in this case would be the Laplace transform method.
However, the case where Œ¥ Ã∏= 0 immediately precludes a straightforward analytical
solution of this kind. It is worth noting that the DufÔ¨Ång equation models a forced
nonlinear mechanical spring, where the restoring force of the spring is accounted
for by the terms Œ±x(t) + Œ¥x3(t). Function x(t), which we wish to solve for, is the
displacement at time t of some point on the spring (e.g., the point mass m at the
free end) with respect to a suitable reference frame. Term k dx(t)
dt
is the opposing
friction, while F cos(œât) is the periodic forcing function that drives the system. An
example of a recent application for (10.1) is in the modeling of micromechanical
Ô¨Ålters/resonators [2].1
At the outset we consider only Ô¨Årst-order problems, speciÔ¨Åcally, how to solve
(numerically)
dx
dt = f (x, t),
x0 = x(0)
(10.2)
for t ‚â•0. [From now on we shall often write x instead of x(t), and dx/dt instead of
dx(t)/dt for brevity.] However, the example of (10.1) is a second-order problem.
But it is possible to replace it with a system of equivalent Ô¨Årst-order problems.
There are many ways to do this in principle. One way is to deÔ¨Åne
y = dx
dt .
(10.3)
The functions x(t) and y(t) are examples of state variables. Since we are interpret-
ing (10.1) as the model for a mechanical system wherein x(t) is displacement, it
therefore follows that we may interpret y(t) as velocity. From the deÔ¨Ånition (10.3)
we may use (10.1) to write
dy
dt = ‚àíŒ±
mx ‚àík
my ‚àíŒ¥
mx3 + F
m cos(œât)
(10.4a)
and
dx
dt = y.
(10.4b)
1The clock circuit in many present-day digital systems is built around a quartz crystal. Such crystals
do not integrate onto chips. Micromechanical resonators are intended to replace the crystal since such
resonators can be integrated onto chips. This is in furtherance of the goal of more compact electronic
systems. This applications example is a good illustration of the rapidly growing trend to integrate
nonelectrical/nonelectronic systems onto chips. The implication of this is that it is now very necessary
for the average electrical and/or computer engineer to become very knowledgeable about most other
branches of engineering, and to possess a much broader and deeper knowledge of science (physics,
chemistry, biology, etc.) and mathematics.
TLFeBOOK

INTRODUCTION
417
Equations (10.4) have the forms
dx
dt = f (x, y, t)
(10.5a)
dy
dt = g(x, y, t)
(10.5b)
for the appropriate choices of f and g. The initial conditions for our example
are x(0) and y(0) (initial position and initial velocity, respectively). These rep-
resent a coupled system of Ô¨Årst-order ODEs. Methods applicable to the solution
of (10.2) are extendable to the larger problem of solving systems of Ô¨Årst-order
ODEs, and so in this way higher-order ODEs may be solved. Thus, we shall also
consider the numerical solution of initial-value problems in systems of Ô¨Årst-order
ODEs.
The next two examples illustrate how to arrive at coupled systems of Ô¨Årst-order
ODEs for electrical and electronic circuits.
Example 10.1
Consider the linear electric circuit shown in Fig. 10.1. The input
to the circuit is the voltage source vs(t), while we may regard the output as the
voltage drop across capacitor C, denoted vC(t). The differential equation relating
the input voltage vs(t) and the output voltage vC(t) is thus
L1C d3vC(t)
dt3
+ R1C d2vC(t)
dt2
+
 L1
L2
+ 1
! dvC(t)
dt
+ R1
L2
vC(t) = dvs(t)
dt
. (10.6)
This third-order ODE may be obtained by mesh analysis of the circuit. The reader
ought to attempt this derivation as an exercise. One way to replace (10.6) with
a coupled system of Ô¨Årst-order ODEs is to deÔ¨Åne the state variables xk(t) (k ‚àà
{0, 1, 2}) according to
x0(t) = vC(t),
x1(t) = dvC(t)
dt
,
x2(t) = d2vC(t)
dt2
.
(10.7)
Substituting (10.7) into (10.6) yields
L1C dx2(t)
dt
+ R1Cx2(t) +
 L1
L2
+ 1
!
x1(t) + R1
L2
x0(t) = dvs(t)
dt
.
iL1(t)
iL2(t)
R1
L1
L2
C
vC(t)
vs(t)
+
‚àí
+
‚àí
Figure 10.1
The linear electric circuit for Example 10.1.
TLFeBOOK

418
NUMERICAL SOLUTION OF ORDINARY DIFFERENTIAL EQUATIONS
If we recognize that
x1(t) = dx0(t)
dt
,
x2(t) = dx1(t)
dt
then the complete system of Ô¨Årst order ODEs is
dx0(t)
dt
= x1(t),
dx1(t)
dt
= x2(t),
(10.8)
dx2(t)
dt
=
1
L1C
dvs(t)
dt
‚àíR1
L1
x2(t) ‚àí
1
L1C
 L1
L2
+ 1
!
x1(t) ‚àí
R
L1L2C x0(t).
In many ways this is not the best description for the circuit dynamics.
Instead, we may Ô¨Ånd the matrix A ‚ààR3√ó3 and column vector b ‚ààR3 such that
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
dvC(t)
dt
diL1(t)
dt
diL2(t)
dt
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
= A
Ô£Æ
Ô£ØÔ£ØÔ£∞
vC(t)
iL1(t)
iL2(t)
Ô£π
Ô£∫Ô£∫Ô£ª+ bvs(t).
(10.9)
This deÔ¨Ånes a new set of state equations in terms of the new state variables vC(t),
iL1(t), and iL2(t). The matrix A and vector b contain constants that depend only
on the circuit parameters R1, L1, L2, and C.
Equation (10.9) is often a better representation than (10.8) because
1. There is no derivative of the forcing function vs(t) in (10.9) as there is in
(10.8).
2. There is a general (linear) theory of solution to (10.9) that is in practice easy
to apply, and it is based on state-space methods.
3. Inductor currents [i.e., iL1(t), iL2(t)] and capacitor voltages [i.e., vC(t)] can
be readily measured in a laboratory setting while derivatives of these are
not as easily measured. Thus, it is relatively easy to compare theoretical and
numerical solutions to (10.9) with laboratory experimental results.
Since
iC(t) = C dvC(t)
dt
,
vL1(t) = L1
diL1(t)
dt
,
vL2(t) = L2
diL2(t)
dt
on applying Kirchoff‚Äôs Voltage law (KVL) and Kirchoff‚Äôs Current law (KCL), we
arrive at the relevant state equations as follows.
TLFeBOOK

INTRODUCTION
419
First
vs(t) = R1iL1(t) + L1
diL1(t)
dt
+ L2
diL2(t)
dt
.
(10.10)
We see that vC(t) = vL2(t), and so
vC(t) = L2
iL2(t)
dt
,
giving
diL2(t)
dt
= 1
L2
vC(t),
(10.11)
which is one of the required state equations. Since
iL1(t) = iC(t) + iL2(t),
and so
iL1(t) = C dvC(t)
dt
+ iL2(t),
we also have
dvC(t)
dt
= 1
C iL1(t) ‚àí1
C iL2(t).
(10.12)
This is another of the required state equations. Substituting (10.11) into (10.10)
gives the Ô¨Ånal state equation
diL1(t)
dt
= ‚àíR1
L1
iL1(t) ‚àí1
L1
vC(t) + 1
L1
vs(t).
(10.13)
The state equations may be collected together in matrix form as required:
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
dvC(t)
dt
diL1(t)
dt
diL2(t)
dt
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
=
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
0
1
C
‚àí1
C
‚àí1
L1
‚àíR1
L1
0
1
L2
0
0
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
#
$%
&
=A
Ô£Æ
Ô£ØÔ£ØÔ£∞
vC(t)
iL1(t)
iL2(t)
Ô£π
Ô£∫Ô£∫Ô£ª+
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
0
1
L1
0
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª
#
$%
&
=b
vs(t).
Example 10.2
Now let us consider a more complicated third-order nonlinear
electronic circuit called the Colpitts oscillator [9]. The electronic circuit, and its
electric circuit equivalent (model) appears in Fig. 10.2. This circuit is a popular
analog signal generator with a long history (it used to be built using vacuum
tubes). The device Q is a three-terminal device called an NPN-type bipolar junction
transistor (BJT). The detailed theory of operation of BJTs is beyond the scope of
TLFeBOOK

420
NUMERICAL SOLUTION OF ORDINARY DIFFERENTIAL EQUATIONS
C2
C1
C2
C1
L
Q
R
(a)
(b)
vCC(t)
vCC(t)
RL
L
RL
iL(t)
iL(t)
vCE(t)
vBE(t)
iC(t)
iB(t)
iC(t)
iB(t)
vBE(t)
REE
REE
VEE
VEE
+
+
‚àí
+
‚àí
vCE(t)
+
‚àí
‚àí
Figure 10.2
The BJT Colpitts oscillator (a) and its electric circuit equivalent (b).
this book, but may be found in basic electronics texts [10]. For present purposes
it is enough to know that Q may be represented with a nonlinear resistor R (the
resistor enclosed in the box in Fig. 10.2b), and a current-controlled current source
(CCCS), where
iC(t) = Œ≤F iB(t)
(10.14)
and
iB(t) =
Ô£±
Ô£¥Ô£≤
Ô£¥Ô£≥
0,
vBE(t) ‚â§VTH
(vBE(t) ‚àíVTH )
RON
,
vBE(t) > VTH
.
(10.15)
The current iB(t) is called the base current of Q, and Ô¨Çows into the base terminal
of the transistor as shown. From (10.15) we observe that if the base‚Äìemitter voltage
vBE(t) is below a threshold voltage VTH , then there is no base current into Q (i.e.,
the device is cut off). The relationship between vBE(t) ‚àíVTH , and iB(t) obeys
Ohm‚Äôs law only when vBE(t) is above threshold, in which case Q is active. In
either case (10.14) says the collector current iC(t) is directly proportional to iB(t),
and the constant of proportionality Œ≤F is called the forward current gain of Q.
Voltage vCE(t) is the collector‚Äìemitter voltage of Q. Typically, VTH ‚âà0.75 V, Œ≤F
is about 100 (order of magnitude), and RON (on resistance of Q) is seldom more
than hundreds of ohms in size.
TLFeBOOK

FIRST-ORDER ODEs
421
From (10.15) iB(t) is a nonlinear function of vBE(t) that we may compactly write
as iB(t) = fR(vBE(t)). There are power supply voltages vCC(t), and VEE. Voltage
VEE < 0 is a constant, with a typical value VEE = ‚àí5 V. Here we treat vCC (t)
as time-varying, but it is usually the case that (approximately) vCC (t) = VCC u(t),
where
u(t) =
 1,
t ‚â•0
0,
t < 0 .
(10.16)
Function u(t) is the unit step function. To say that vCC (t) = VCC u(t) is to say that
the circuit is turned on at time t = 0. Typically, VCC = +5 V.
The reader may verify (again as a circuit analysis review exercise) that state
equations for the Colpitts oscillator are:
C1
dvCE(t)
dt
= iL(t) ‚àíŒ≤F fR(vBE(t)),
(10.17a)
C2
dvBE(t)
dt
= ‚àívBE(t) + VEE
REE
‚àífR(vBE(t)) ‚àíiL(t),
(10.17b)
LdiL(t)
dt
= vCC (t) ‚àívCE(t) + vBE(t) ‚àíRLiL(t).
(10.17c)
Thus, the state variables are vBE(t), vCE(t), and iL(t). As previously, this circuit
description is not unique, but it is convenient.
Since numerical methods only provide approximate solutions to ODEs, we are
naturally concerned about the accuracy of these approximations. There are also
issues about the stability of proposed methods, and so this matter as well will be
considered in this chapter.
10.2
FIRST-ORDER ODEs
Strictly speaking, before applying a numerical method to the solution of an ODE,
we must be certain that a solution exists. We are also interested in whether the
solution is unique. It is worth stating that in many cases, since ODEs are often
derived from problems in the physical world, existence and uniqueness are often
‚Äúobvious‚Äù for physical reasons. Notwithstanding this, a mathematical statement
about existence and uniqueness is worthwhile.
The following deÔ¨Ånition is needed by the succeeding theorem regarding the
existence and uniqueness of solutions to Ô¨Årst order ODE initial value problems.
DeÔ¨Ånition 10.1: The Lipschitz Condition
The function f (x, t) ‚ààR satisÔ¨Åes
a Lipschitz condition in x for S ‚äÇR2 iff there is an Œ± > 0 such that
|f (x, t) ‚àíf (y, t)| ‚â§Œ±|x ‚àíy|
when (x, t), (y, t) ‚ààS. The constant Œ± is called a Lipschitz constant for f (x, t).
TLFeBOOK

422
NUMERICAL SOLUTION OF ORDINARY DIFFERENTIAL EQUATIONS
It is apparent that if f (x, t) satisÔ¨Åes a Lipschitz condition, then it is smooth in
some sense. The following theorem is about the existence and uniqueness of the
solution to
dx
dt = f (x, t)
(10.18)
for 0 ‚â§t ‚â§tf , with the initial condition x0 = x(0). Time t = 0 is the initial time, or
starting time. We call constant tf the Ô¨Ånal time. Essentially, we are only interested
in the solution over a Ô¨Ånite time interval. This constraint on the theory is not
unreasonable since a computer can run for only a Ô¨Ånite amount of time anyway.
We also remark that interpreting the independent variable t as time is common
practice, but not mandatory in general.
Theorem 10.1: Picard‚Äôs Theorem
Suppose that S = {(x, t) ‚ààR2|0 ‚â§t ‚â§
tf , ‚àí‚àû< x < ‚àû}, and that f (x, t) is continuous on S. If f satisÔ¨Åes a Lipschitz
condition on set S in the variable x, then the initial-value problem (10.18) has a
unique solution x = x(t) for all 0 ‚â§t ‚â§tf .
Proof
Omitted. We simply mention that it is based on the Banach Ô¨Åxed-point
theorem (recall Theorem 7.3).
We also mention that a proof of a somewhat different version of this theorem
appears in Kreyszig [3, pp. 315‚Äì317]. It involves working with a contractive map-
ping on a certain closed subspace of C(J), where J = [t0 ‚àíŒ≤, t0 + Œ≤] ‚äÇR and
C(J) is the metric space of continuous functions on J, where the metric is that
of (1.8) in Chapter 1. It was remarked in Chapter 3 [see Eq. (3.8)] that this space
is complete. Thus, any closed subspace of it is complete as well (a fact that was
mentioned in Chapter 7 following Corollary 7.1).
We may now consider speciÔ¨Åc numerical techniques. DeÔ¨Åne xn = x(tn) for
n ‚ààZ+. Usually we assume that t0 = 0, and that
tn+1 = tn + h,
(10.19)
where h > 0, and we call h the step size. From (10.18)
x(1)
n
= dxn
dt = dx(t)
dt
|t=tn = f (xn, tn).
(10.20)
We may expand solution x(t) in a Taylor series about t = tn. Therefore, since
x(tn+1) = x(tn + h) = xn+1, and with x(k)
n
= x(k)(tn)
xn+1 = xn + hx(1)
n
+ 1
2!h2x(2)
n
+ 1
3!h3x(3)
n
+ ¬∑ ¬∑ ¬∑ .
(10.21)
If we drop terms in x(k)
n
for k > 1, then (10.21) and (10.20) imply
xn+1 = xn + hx(1)
n
= xn + hf (xn, tn).
TLFeBOOK

FIRST-ORDER ODEs
423
Since x0 = x(t0) = x(0) we may Ô¨Ånd (xn) via
xn+1 = xn + hf (xn, tn).
(10.22)
This is often called the Euler method (or Euler‚Äôs method).2 A more accurate descrip-
tion would be to call it the explicit form of Euler‚Äôs method in order to distinguish
it from the implicit form to be considered a little later on. The distinction matters
in practice because implicit methods tend to be stable, whereas explicit methods
are often prone to instability.
A few general words about stability and accuracy are now appropriate. In what
follows we will assume (unless otherwise noted) that the solution to a differen-
tial equation remains bounded; that is, |x(t)| < M < ‚àûfor all t ‚â•0. However,
approximations to this solution [e.g., (xn) from (10.22)] will not necessarily remain
bounded in the limit as n ‚Üí‚àû; that is, our numerical methods might not always be
stable. Of course, in a situation like this the numerical solution will deviate greatly
from the correct solution, and this is simply unacceptable. It therefore follows that
we must Ô¨Ånd methods to test the stability of a proposed numerical solution. Some
informal deÔ¨Ånitions relating to stability are
Stable method: The numerical solution does not grow without bound (i.e., ‚Äúblow
up‚Äù) with any choice of parameters such as step size.
Unstable method: The numerical solution blows up with any choices of param-
eters (such as step size).
Conditionally stable method: For certain choices of parameters the numerical
solution remains bounded.
We mention that even if the Euler method is stable, its accuracy is low because
only the Ô¨Årst two terms in the Taylor series are retained. More speciÔ¨Åcally, we
say that it is a Ô¨Årst-order method because only the Ô¨Årst power of h is retained in
the Taylor approximation that gave rise to it. The omission of higher-order terms
causes truncation errors. Since h2 (and higher power) terms are omitted we also
say that the truncation error per step (sometimes called the order of accuracy) is
of order h2. This is often written as O(h2). (Here we follow the terminology in
Kreyszig [4, pp. 793‚Äì794].) In summary, we prefer methods that are both stable,
and accurate. It is important to emphasize that accuracy and stability are distinct
concepts, and so must never be confused.
2Strictly speaking, in truncating the series in (10.21) we should write ÀÜxn+1 = xn + hf (xn, tn) so that
Euler‚Äôs method is
ÀÜxn+1 = ÀÜxn + hf (ÀÜxn, tn)
with ÀÜx0 = x0. This is to emphasize that the method only generates approximations to xn = x(tn).
However, this kind of notation is seldom applied. It is assumed that the reader knows that the numerical
method only approximates x(tn) even though the notation does not necessarily explicitly distinguish
the exact value from the approximate.
TLFeBOOK

424
NUMERICAL SOLUTION OF ORDINARY DIFFERENTIAL EQUATIONS
We can say more about the accuracy of the Euler method:
Theorem 10.2: For dx(t)/dt = f (x(t), t) let f (x(t), t) be Lipschitz continu-
ous with constant Œ± (DeÔ¨Ånition 10.1), and assume that x(t) ‚ààC2[t0, tf ] (tf > t0).
If ÀÜxn ‚âàx(tn) = xn, where (tn = t0 + nh, and tn ‚â§tf )
ÀÜxn+1 = ÀÜxn + hf (ÀÜxn, tn), (ÀÜx0 ‚âàx(t0))
then
|x(tn) ‚àíÀÜxn| ‚â§eŒ±(tn‚àít0)|x(t0) ‚àíÀÜx0| + 1
2hM eŒ±(tn‚àít0) ‚àí1
Œ±
,
(10.23)
where M = maxt‚àà[t0,tf ] |x(2)(t)|.
Proof
Euler‚Äôs method is
ÀÜxn+1 = ÀÜxn + hf (ÀÜxn, tn)
and from Taylor‚Äôs theorem
x(tn+1) = x(tn) + hx(1)(tn) + 1
2h2x(2)(Œæn)
for some Œæn ‚àà[tn, tn+1]. Thus
x(tn+1) ‚àíÀÜxn+1 = x(tn) ‚àíÀÜxn + h[x(1)(tn) ‚àíf (ÀÜxn, tn)] + 1
2h2x(2)(Œæn)
= x(tn) ‚àíÀÜxn + h[f (x(tn), tn) ‚àíf (ÀÜxn, tn)] + 1
2h2x(2)(Œæn)
so that
|x(tn+1) ‚àíÀÜxn+1| ‚â§|x(tn) ‚àíÀÜxn| + Œ±h|x(tn) ‚àíÀÜxn| + 1
2h2|x(2)(Œæn)|.
For convenience we will let en = |x(tn) ‚àíÀÜxn|, Œª = 1 + Œ±h, and rn = 1
2h2x(2)(Œæn),
so that
en+1 ‚â§Œªen + rn.
It is easy to see that3
e1
‚â§
Œªe0 + r0
e2
‚â§
Œªe1 + r1 = Œª2e0 + Œªr0 + r1
e3
‚â§
Œªe2 + r2 = Œª3e0 + Œª2r0 + Œªr1 + r2
...
en
‚â§
Œªne0 +
n‚àí1

j=0
Œªjrn‚àí1‚àíj
.
3More formally, we may use mathematical induction.
TLFeBOOK

FIRST-ORDER ODEs
425
If M = maxt‚àà[t0,tf ] |x(2)(t)| then rn‚àí1‚àíj ‚â§1
2h2M, and hence
en ‚â§Œªne0 + 1
2h2M
n‚àí1

j=0
Œªj,
and since n‚àí1
j=0 Œªj = Œªn‚àí1
Œª‚àí1 , and for x ‚â•‚àí1 we have (1 + x)n ‚â§enx, thus
n‚àí1

j=0
Œªj = Œªn ‚àí1
Œª ‚àí1 ‚â§enŒ±h ‚àí1
Œ±h
= eŒ±(tn‚àít0) ‚àí1
Œ±h
.
Consequently,
en ‚â§eŒ±(tn‚àít0)e0 + 1
2hM eŒ±(tn‚àít0) ‚àí1
Œ±
which immediately yields the theorem statement.
We remark that e0 = |x(t0) ‚àíÀÜx0| = 0 only if ÀÜx0 = x(t0) exactly. Where quan-
tization errors (recall Chapter 2) are concerned, this will seldom be the case. The
second term in the bound of (10.23) may be large even if h is tiny. In other words,
Euler‚Äôs method is not necessarily very accurate. Certainly, from (10.23) we can
say that en ‚àùh.
As a brief digression, we also note that Theorem 10.2 needed the bound
(1 + x)n ‚â§enx(x ‚â•‚àí1).
(10.24)
We may easily establish (10.24) as follows. From the Maclaurin expansion
(Chapter 3)
ex = 1 + x + 1
2x2eŒæ
(for some Œæ ‚àà[0, x]) so that
0 ‚â§1 + x ‚â§1 + x + 1
2x2eŒæ = ex,
and because 1 + x ‚â•0 (i.e., x ‚â•‚àí1)
0 ‚â§(1 + x)n ‚â§enx,
thus establishing (10.24).
TLFeBOOK

426
NUMERICAL SOLUTION OF ORDINARY DIFFERENTIAL EQUATIONS
The stability of any method may be analyzed in the following manner. First
recall the Taylor series expansion of f (x, t) about the point (x0, t0)
f (x, t) = f (x0, t0) + (t ‚àít0)‚àÇf (x0, t0)
‚àÇt
+ (x ‚àíx0)‚àÇf (x0, t0)
‚àÇx
+ 1
2!

(t ‚àít0)2 ‚àÇ2f (x0, t0)
‚àÇt2
+ 2(t ‚àít0)(x ‚àíx0)‚àÇ2f (x0, t0)
‚àÇt‚àÇx
+(x ‚àíx0)2 ‚àÇ2f (x0, t0)
‚àÇx2

+ ¬∑ ¬∑ ¬∑
(10.25)
If we retain only the linear terms of (10.25) and substitute these into (10.18), then
we obtain
x(1)(t) = dx
dt = f (x0, t0) + (t ‚àít0)‚àÇf (x0, t0)
‚àÇt
+ (x ‚àíx0)‚àÇf (x0, t0)
‚àÇx
= ‚àÇf (x0, t0)
‚àÇx
#
$%
&
=Œª
x+ ‚àÇf (x0, t0)
‚àÇt
#
$%
&
=Œª1
t+

f (x0, t0) ‚àít0
‚àÇf (x0, t0)
‚àÇt
‚àíx0
‚àÇf (x0, t0)
‚àÇx

#
$%
&
=Œª2
,
(10.26)
so this has the general form (with Œª, Œª1, and Œª2 as constants)
dx
dt = Œªx + Œª1t + Œª2.
(10.27)
This linearized approximation to the original problem in (10.18) allows us to inves-
tigate the behavior of the solution in close proximity to (x0, t0). Equation (10.27)
is often simpliÔ¨Åed still further by considering what is called the model problem
dx
dt = Œªx.
(10.28)
Thus, here we assume that Œª1t + Œª2 in (10.27) can also be neglected. However,
we do remark that (10.27) has the form
dx
dt + P (t)x = Q(t)xn,
(10.29)
where n = 0, P (t) = ‚àíŒª, and Q(t) = Œª1t + Œª2. Thus, (10.27) is an instance of
Bernoulli‚Äôs differential equation [5, p. 62] for which a general method of solution
exists. But for the purpose of stability analysis it turns out to be enough (usually,
but not always) to consider only (10.28). Equation (10.28) is certainly simple in
that its solution is
x(t) = x(0)eŒªt.
(10.30)
TLFeBOOK

FIRST-ORDER ODEs
427
If Euler‚Äôs method is applied to (10.28), then
xn+1 = xn + hŒªxn = (1 + hŒª)xn.
(10.31)
Clearly, for n ‚ààZ+
xn = (1 + hŒª)nx0,
(10.32)
and we may avoid limn‚Üí‚àû|xn| = ‚àûif
|1 + hŒª| ‚â§1.
The model problem (10.28) with the solution (10.30) is stable4 only if Œª < 0.
Hence Euler‚Äôs method is conditionally stable for
Œª < 0
and
h ‚â§2
|Œª|,
(10.33)
and is unstable if
|1 + hŒª| > 1.
(10.34)
We see that depending on Œª and h, the explicit Euler method might be unstable.
Now we consider the alternative implicit form of Euler‚Äôs method. This method
is also called the backward Euler method. Instead of (10.22) we use
xn+1 = xn + hf (xn+1, tn+1).
(10.35)
It can be seen that a drawback of this method is the necessity to solve (10.35) for
xn+1. This is generally a nonlinear problem requiring the techniques of Chapter 7.
However, a strength of the implicit method is enhanced stability. This may be
easily seen as follows. Apply (10.35) to the model problem (10.28), yielding
xn+1 = xn + Œªhxn+1
or
xn+1 =
1
1 ‚àíŒªhxn.
(10.36)
Clearly
xn =

1
1 ‚àíŒªh
n
x0.
(10.37)
Since we must assume as before that Œª < 0, the backward Euler method is stable
for all h > 0. In this sense we may say that the backward Euler method is uncon-
ditionally stable. Thus, the implicit Euler method (10.35) is certainly more stable
4For stability we usually insist that Œª < 0 as opposed to allowing Œª = 0. This is to accommodate a
concept called bounded-input, bounded-output (BIBO) stability. However, we do not consider the details
of this matter here.
TLFeBOOK

428
NUMERICAL SOLUTION OF ORDINARY DIFFERENTIAL EQUATIONS
than the previous explicit form (10.22). However, the implicit and explicit forms
have the same accuracy as both are Ô¨Årst-order methods.
Example 10.3
We wish to apply the implicit and explicit forms of the Euler
method to
dx
dt + x = 0
for x(0) = 1. Of course, since this is a simple linear problem, we immediately
know that
x(t) = x(0)e‚àít = e‚àít
for all t ‚â•0. Since we have f (x, t) = ‚àíx, we obtain
‚àÇf (x, t)
‚àÇx
= ‚àí1,
implying that Œª = ‚àí1. Thus, the explicit Euler method (10.22) gives
xn+1 = (1 ‚àíh)xn
(10.38a)
for which 0 < h ‚â§2 via (10.33). Similarly, (10.35) gives for the implicit method
xn+1 = xn ‚àíhxn+1
or
xn+1 =
1
1 + hxn
(10.38b)
for which h > 0. In both (10.38a) and (10.38b) we have x0 = 1.
Some typical simulation results for (10.38) appear in Fig. 10.3. Note the insta-
bility of the explicit method for the case where h > 2.
It is to be noted that a small step size h is desirable to achieve good accuracy.
Yet a larger h is desirable to minimize the amount of computation involved in
simulating the differential equation over the desired time interval.
Example 10.4
Now consider the ODE
dx
dt + 2tx = te‚àít2x3
[5, pp. 62‚Äì63]. The exact solution to this differential equation is
x2(t) =
3
e‚àít2 + ce2t2
(10.39)
TLFeBOOK

FIRST-ORDER ODEs
429
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
0
0.2
0.4
0.6
0.8
1
Time (t)
Time (t)
Amplitude
0
2
4
6
8
10
12
14
16
18
20
‚àí2
‚àí1
0
1
2
3
Amplitude
(a)
(b)
x(t)
Implicit Euler (h = 0.1)
Explicit Euler (h = 0.1)
x(t)
Implicit Euler (h = 2.1)
Explicit Euler (h = 2.1)
Figure 10.3
Illustration of the implicit and explicit forms of the Euler method for the
differential equation in Example 10.3. In plot (a), h is small enough that the explicit method
is stable. Here the implicit and explicit methods display similar accuracies. In plot (b), h
is too big for the explicit method to be stable. Instability is indicated by the oscillatory
behavior of the method and the growing amplitude of the oscillations with time. However,
the implicit method remains stable, but because h is quite large, the accuracy is not very
good.
for t ‚â•0, where c = 3
x2
0 ‚àí1, and we assume that c > 0. Thus
f (x, t) = te‚àít2x3 ‚àí2tx
so
‚àÇf (x, t)
‚àÇx
= 3te‚àít2x2 ‚àí2t.
Consequently
Œª = ‚àÇf (x0, t0)
‚àÇx
= ‚àÇf (x0, 0)
‚àÇx
= 0.
Via (10.33) we conclude that h > 0 is possible for both forms of the Euler method.
Since stability is therefore not a problem here, we choose to simulate the differential
TLFeBOOK

430
NUMERICAL SOLUTION OF ORDINARY DIFFERENTIAL EQUATIONS
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
0
0.2
0.4
0.6
0.8
1
Time (t)
(a)
(b)
Amplitude
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
0
0.2
0.4
0.6
0.8
1
Time (t)
Amplitude
x(t)
Explicit Euler (h = 0.02)
x(t)
Explicit Euler (h = 0.20)
Figure 10.4
Illustration of the explicit Euler method for the differential equation in
Example 10.4. Clearly, although stability is not a problem here, the accuracy of the method
is better for smaller h.
equation using the explicit Euler method as this is much simpler to implement.
Thus, from (10.22), we obtain
xn+1 = xn + h[tne‚àít2nx3
n ‚àí2tnxn].
(10.40)
We shall assume x0 = 1 [initial condition x(0)]. Of course, tn = hn for n =
0, 1, 2, . . ..
Figure 10.4 illustrates the exact solution from (10.39), and the simulated solution
via (10.40) for h = 0.02 and h = 0.20. As expected, the result for h = 0.02 is more
accurate.
The next example involves an ODE whose solution does not remain bounded
over time. Nevertheless, our methods are applicable since we terminate the simu-
lation after a Ô¨Ånite time.
Example 10.5
Consider the ODE
dx
dt = t2 ‚àí2x
t
TLFeBOOK

FIRST-ORDER ODEs
431
for t ‚â•t0 > 0 [5, pp. 60‚Äì61]. The exact solution is given by
x(t) = t3
5 + c
5t2 .
(10.41)
The initial condition is x(t0) = x0 with t0 > 0, and so
x0 = t3
0
5 + c
5t2
0
,
implying that
c = 5t2
0x0 ‚àít5
0.
Since f (x, t) = t2 ‚àí2x
t , we have
Œª = ‚àÇf (x0, t0)
‚àÇx
= ‚àí2
t0
,
so via (10.33) for the explicit Euler method
0 < h ‚â§t0.
However, this result is misleading here because x(t) is not bounded with time. In
other words, it does not really apply here. From (10.22)
xn+1 = xn + h

t2
n ‚àí2xn
tn

,
(10.42a)
where
tn = t0 + nh
for n ‚ààZ+. If we consider the implicit method, then via (10.35)
xn+1 = xn + h

t2
n+1 ‚àí2xn+1
tn+1

,
or
xn+1 =
xn + ht2
n+1
1 + 2h
tn+1
,
(10.42b)
where tn+1 = t0 + (n + 1)h for n ‚ààZ+.
Figure 10.5 illustrates the exact solution x(t) from (10.41) along with the simu-
lated solutions from (10.42). This is for x0 = 1 and t0 = 0.05 with h = 0.025 (a),
and h = 5 (b). It can be seen that the implicit method is more accurate for t close
to t0. Of course, this could be very signiÔ¨Åcant since startup transients are often of
TLFeBOOK

432
NUMERICAL SOLUTION OF ORDINARY DIFFERENTIAL EQUATIONS
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
0
0.5
1
1.5
2
Time (t)
Amplitude
Implicit Euler (h = 0.025)
0
50
100
150
200
250
‚àí5
0
5
10
15
20 √ó 105
Time (t)
Amplitude
x(t)
Explicit Euler (h = 0.025)
Implicit Euler (h = 5)
x(t)
Explicit Euler (h = 5)
(a)
(b)
Figure 10.5
Illustration of the explicit and implicit forms of the Euler method for the ODE
in Example 10.5. In plot (a), note that the implicit form tracks the true solution x(t) better
near t0 = 0.05. In plot (b), note that both forms display a similar accuracy even though h
is huge, provided t ‚â´t0.
interest in simulations of dynamic systems. It is noteworthy that both implicit and
explicit forms simulate the true solution with similar accuracy for t much bigger
than t0 even when h is large.
This example is of a ‚Äústiff system‚Äù (see Section 10.6).
Example 10.5 illustrates that stability and accuracy issues with respect to the
numerical solution of ODE initial-value problems can be more subtle than our
previous analysis would suggest. The reader is therefore duly cautioned about
these matters.
Recalling (3.71) from Chapter 3 (or recalling Theorem 10.2), the Taylor formula
for x(t) about t = tn is [recall xn = x(tn) for all n]
xn+1 = xn + hf (xn, tn)
#
$%
&
=ÀÜxn+1
+1
2h2x(2)(Œæ)
(10.43)
for some Œæ ‚àà[tn, tn+1]. Thus, the truncation error per step in the Euler method is
deÔ¨Åned to be
en+1 = xn+1 ‚àíÀÜxn+1 = 1
2h2x(2)(Œæ).
(10.44)
TLFeBOOK

FIRST-ORDER ODEs
433
We may therefore state, as suggested earlier, that the truncation error per step is of
order O(h2) because of this. The usefulness of (10.44) is somewhat limited in that
it depends on the solution x(t) [or rather on the second derivative x(2)(t)], which
is, of course, something we seldom know in practice.
How may we obtain more accurate methods? More speciÔ¨Åcally, this means
Ô¨Ånding methods for which the truncation error per step is of order O(hm) with
m > 2.
One way to obtain improved accuracy is to try to improve the Euler method.
More than one possibility for improvement exists. However, a popular approach is
Heun‚Äôs method. It is based on the following observation. A drawback of the Euler
method in (10.22) is that f (xn, tn) is the derivative x(1)(t) at the beginning of the
interval [tn, tn+1], and yet x(1)(t) varies over [tn, tn+1]. The implicit form of the
Euler method works with f (xn+1, tn+1), namely, the derivative at t = tn+1, and so
has a similar defect. Therefore, intuitively, we may believe that we can improve
the algorithm by replacing f (xn, tn) with the average derivative
1
2

f (xn, tn) + f (xn + hf (xn, tn), tn + h)

.
(10.45)
This is approximately the average of x(1)(t) at the endpoints of interval [tn, tn+1].
The approximation is due to the fact that
f (xn+1, tn+1) ‚âàf (xn + hf (xn, tn), tn + h).
(10.46)
We see in (10.46) that we have employed (10.22) to approximate xn+1 according
to xn+1 = xn + hf (xn, tn) (explicit Euler method). Of course, tn+1 = tn + h does
not involve any approximation. Thus, Heun‚Äôs method is deÔ¨Åned by
xn+1 = xn + h
2

f (xn, tn) + f (xn + hf (xn, tn), tn + h)

.
(10.47)
This is intended to replace (10.22) and (10.35).
However, (10.47) is an explicit method, and so we may wonder about its sta-
bility. If we apply the model problem to (10.47), we obtain
xn+1 =

1 + Œªh + 1
2h2Œª2
xn
(10.48)
for which
xn =

1 + Œªh + 1
2h2Œª2n
x0.
(10.49)
For stability we must select h such that we avoid limn‚Üí‚àû|xn| = ‚àû. For conve-
nience, deÔ¨Åne
œÉ = 1 + Œªh + 1
2h2Œª2,
(10.50)
so this requirement implies that we must have |œÉ| ‚â§1. A plot of (10.50) in terms
of hŒª appears in Fig. 10.6. This makes it easy to see that we must have
‚àí2 ‚â§hŒª ‚â§0.
(10.51)
TLFeBOOK

434
NUMERICAL SOLUTION OF ORDINARY DIFFERENTIAL EQUATIONS
‚àí2.5
‚àí2
‚àí1.5
‚àí1
‚àí0.5
0
0.5
‚àí1
‚àí0.5
0
0.5
1
1.5
2
hl
s
Figure 10.6
A plot of œÉ in terms of hŒª [see Eq. (10.50)].
Since Œª < 0 is assumed [again because we assume that x(t) is bounded] (10.51),
implies that Heun‚Äôs method is conditionally stable for the same conditions as in
(10.33). Thus, the stability characteristics of the method are identical to those of
the explicit Euler method, which is perhaps not such a surprize.
What about the accuracy of Heun‚Äôs method? Here we may see that there is an
improvement. Again via (3.71) from Chapter 3
xn+1 = xn + hx(1)(tn) + 1
2h2x(2)(tn) + 1
6h3x(3)(Œæ)
(10.52)
for some Œæ ‚àà[tn, tn+1]. We may approximate x(2)(tn) using a forward difference
operation
x(2)(tn) ‚âàx(1)(tn+1) ‚àíx(1)(tn)
h
(10.53)
so that (10.52) becomes [using x(1)(tn+1) = f (xn+1, tn+1), and x(1)(tn) =
f (xn, tn)]
xn+1 = xn + hf (xn, tn) + 1
2h2

x(1)(tn+1) ‚àíx(1)(tn)
h
	
+ 1
6h3x(3)(Œæ),
(10.54)
or upon simplifying this, we have
xn+1 = xn + h
2

f (xn, tn) + f (xn+1, tn+1)

+ 1
6h3x(3)(Œæ).
(10.55)
Replacing f (xn+1, tn+1) in (10.55) with the approximation (10.46), and dropping
the error term, we see that what remains is identical to (10.47), namely, Heun‚Äôs
method. Various approximations were made to arrive at this conclusion, but they
are certainly reasonable, and so we claim that the truncation error per step for
Heun‚Äôs method is
en+1 = 1
6h3x(3)(Œæ),
(10.56)
TLFeBOOK

FIRST-ORDER ODEs
435
where again Œæ ‚àà[tn, tn+1], and so this error is of the order O(h3). In other words,
although Heun‚Äôs method is based on modifying the explicit Euler method, the
modiÔ¨Åcation has lead to a method with improved accuracy.
Example 10.6
Here we repeat Example 10.5 by applying Heun‚Äôs method under
the same conditions as for Fig. 10.5a. Thus, the differential equation is again
dx
dt = t2 ‚àí2x
t
and again we choose x0 = 1.0, t0 = 0.05, with h = 0.025. The simulation result
appears in Fig. 10.7.
It is very clear that Heun‚Äôs method is distinctly more accurate than the Euler
method, especially near t = t0.
Heun‚Äôs method may be viewed in a different light by considering the following.
We may formally integrate (10.18) to arrive at x(t) according to
x(t) ‚àíx(tn) =
 t
tn
f (x, œÑ) dœÑ.
(10.57)
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
Time (t)
Amplitude
x(t)
Implicit Euler (h = 0.025)
Heun (h = 0.025)
Figure 10.7
Comparison
of
the
implicit
Euler
method
with
Heun‚Äôs
method
for
Example 10.6. We see that Heun‚Äôs method is more accurate, especially for t near t0 = 0.05.
TLFeBOOK

436
NUMERICAL SOLUTION OF ORDINARY DIFFERENTIAL EQUATIONS
So if t = tn+1, then, from (10.57), we obtain
x(tn+1) = x(tn) +
 tn+1
tn
f (x, œÑ) dœÑ
or
xn+1 = xn +
 tn+1
tn
f (x, œÑ) dœÑ.
(10.58)
According to the trapezoidal rule for numerical integration (see Chapter 9,
Section 9.2), we have
 tn+1
tn
f (x, œÑ) dœÑ ‚âàh
2 [f (xn, tn) + f (xn+1, tn+1)]
(since h = tn+1 ‚àítn), and hence (10.58) becomes
xn+1 = xn + h
2 [f (xn, tn) + f (xn+1, tn+1)],
(10.59)
which is just the Ô¨Årst step in the derivation of Heun‚Äôs method. We may certainly
call (10.59) the trapezoidal method. Clearly, it is an implicit method since we must
solve (10.59) for xn+1. Equally clearly, Eq. (10.59) appears in (10.55), and so we
immediately conclude that the trapezoidal method is a second-order method with
a truncation error per step of order O(h3). Thus, we may regard Heun‚Äôs method
as the explicit form of the trapezoidal method. Or, equivalently, the trapezoidal
method can be regarded as the implicit form of Heun‚Äôs method. We mention that
the trapezoidal method is unconditionally stable, but will not prove this here.
The following example illustrates some more subtle issues relating to the stability
of numerical solutions to ODE initial-value problems. It is an applications example
from population dynamics, but the issues it raises are more broadly applicable. The
example is taken from Beltrami [6].
Example 10.7
Suppose that x(t) is the total size of a population (people,
insects, bacteria, etc.). The members of the population exist in a habitat that can
realistically support not more than N individuals. This is the carrying capacity
for the system. The population may grow at some rate that diminishes to zero
as x(t) approaches N. But if the population size x(t) is much smaller than the
carrying capacity, the rate of growth might be considered proportional to the present
population size. Consequently, a model for population growth might be
dx(t)
dt
= rx(t)

1 ‚àíx(t)
N

.
(10.60)
This is called the logistic equation. By separation of variables this equation has
solution
x(t) =
N
1 + ce‚àírt
(10.61)
TLFeBOOK

FIRST-ORDER ODEs
437
for t ‚â•0. As usual, c depends on the initial condition (initial population size) x(0).
The exact solution in (10.61) is clearly ‚Äúwell behaved.‚Äù Therefore, any numerical
solution to (10.60) must also be well behaved.
Suppose that we attempt to simulate (10.60) numerically using the explicit Euler
method. In this case we obtain [via (10.22)]
xn+1 = (1 + hr)xn ‚àíhr
N x2
n.
(10.62)
Suppose that we transform variables according to xn = Œ±yn in which case (10.62)
can be rewritten as
yn+1 = (1 + hr)yn

1 ‚àí
hrŒ±
N(1 + hr)yn

.
(10.63)
If we select
Œ± = N(1 + hr)
hr
,
then (10.63) becomes
yn+1 = Œªyn(1 ‚àíyn),
(10.64)
where Œª = 1 + hr. We recognize this as the logistic map from Chapter 7 [see
Examples 7.3‚Äì7.5 and Eq. (7.83)]. From Section 7.6 in particular we recall that
this map can become chaotic for certain choices of Œª. In other words, chaotic
instability is another possible failure mode for a numerical method that purports to
solve ODEs.
The explicit form of the Euler method is actually an example of a Ô¨Årst-order
Runge‚ÄìKutta method. Similarly, Heun‚Äôs method is an example of a second-order
Runge‚ÄìKutta method. It is second-order essentially because the approximation
involved retains the term in h2 in the Taylor series expansion. We mention that
methods of still higher order can be obtained simply by retaining more terms in the
Taylor series expansion of (10.21). This is seldom done because to do so requires
working with derivatives of increasing order, and this requires much computational
effort. But this effort can be completely avoided by developing Runge‚ÄìKutta meth-
ods of higher order. We now outline a general approach for doing this. It is based
on material from Rao [7].
All Runge‚ÄìKutta methods have a particular form that may be stated as
xn+1 = xn + hŒ±(xn, tn, h),
(10.65)
where Œ±(xn, tn, h) is called the increment function. The increment function is
selected to represent the average slope on the interval t ‚àà[tn, tn+1]. In particular,
the increment function has the form
Œ±(xn, tn, h) =
m

j=1
cjkj,
(10.66)
TLFeBOOK

438
NUMERICAL SOLUTION OF ORDINARY DIFFERENTIAL EQUATIONS
where m is called the order of the Runge‚ÄìKutta method, cj are constants, and
coefÔ¨Åcients kj are obtained recursively according to
k1 = f (xn, tn)
k2 = f (xn + a2,1hk1, tn + p2h)
k3 = f (xn + a3,1hk1 + a3,2hk2, tn + p3h)
...
km = f
Ô£´
Ô£≠xn +
m‚àí1

j=1
am,jhkj, tn + pmh
Ô£∂
Ô£∏.
(10.67)
A more compact description of the Runge‚ÄìKutta methods is
xn+1 = xn + h
m

j=1
cjkj,
(10.68a)
where
kj = f
Ô£´
Ô£≠xn + h
j‚àí1

l=1
aj,lkl, tn + pjh
Ô£∂
Ô£∏.
(10.68b)
To specify a particular method requires selecting a variety of coefÔ¨Åcients (cj, aj,l,
etc.). How is this to be done?
We illustrate with examples. Suppose that m = 1. In this case
xn+1 = xn + hc1k1 = xn + hc1f (xn, tn)
(10.69)
which gives (10.22) when c1 = 1. Thus, we are justiÔ¨Åed in calling the explicit
Euler method a Ô¨Årst-order Runge‚ÄìKutta method.
Suppose that m = 2. In this case
xn+1 = xn + hc1f (xn, tn) + hc2f (xn + a2,1hf (xn, tn), tn + p2h).
(10.70)
We observe that if we choose
c2 = c1 = 1
2, a2,1 = 1, p2 = 1,
(10.71)
then (10.70) reduces to
xn+1 = xn + 1
2h[f (xn, tn) + f (xn + hf (xn, tn), tn + h)],
which is Heun‚Äôs method [compare this with (10.47)]. Thus, we are justiÔ¨Åed in call-
ing Heun‚Äôs method a second-order Runge‚ÄìKutta method. However, the coefÔ¨Åcient
TLFeBOOK

FIRST-ORDER ODEs
439
choices in (10.71) are not unique. Other choices will lead to other second-order
Runge‚ÄìKutta methods. We may arrive at a systematic approach for creating alter-
natives as follows.
For convenience as in (10.21), deÔ¨Åne x(k)
n
= x(k)(tn). Since m = 2, we will
consider the Taylor expansion
xn+1 = xn + hx(1)
n
+ 1
2h2x(2)
n
+ O(h3)
(10.72)
[recall (10.52)] for which the term O(h3) simply denotes the higher-order terms.
We recall that x(1)(t) = f (x, t), so x(1)
n
= f (xn, tn), and via the chain rule
x(2)(t) = ‚àÇf
‚àÇt + ‚àÇf
‚àÇx
dx
dt = ‚àÇf
‚àÇt + ‚àÇf
‚àÇx f (x, t),
(10.73)
so (10.72) may be rewritten as
xn+1 = xn + hf (xn, tn) + 1
2h2 ‚àÇf (xn, tn)
‚àÇt
+ 1
2h2 ‚àÇf (xn, tn)
‚àÇx
f (xn, tn) + O(h3).
(10.74)
Once again, the Runge‚ÄìKutta method for m = 2 is
xn+1 = xn + hc1f (xn, tn) + hc2f (xn + a2,1hk1, tn + p2h).
(10.75)
Recalling (10.26), the Taylor expansion of f (xn + a2,1hk1, tn + p2h) is given by
f (xn + a2,1hk1, tn + p2h) = f (xn, tn) + a2,1hf (xn, tn)‚àÇf (xn, tn)
‚àÇx
+ p2h‚àÇf (xn, tn)
‚àÇt
+ O(h2).
(10.76)
Now we substitute (10.76) into (10.75) to obtain
xn+1 = xn + (c1 + c2)hf (xn, tn) + p2c2h2 ‚àÇf (xn, tn)
‚àÇt
+ a2,1c2h2 ‚àÇf (xn, tn)
‚àÇx
f (xn, tn) + O(h3).
(10.77)
We may now compare like terms of (10.77) with those in (10.74) to conclude that
the coefÔ¨Åcients we seek satisfy the nonlinear system of equations
c1 + c2 = 1, p2c2 = 1
2, a2,1c2 = 1
2.
(10.78)
To generate second-order Runge‚ÄìKutta methods, we are at liberty to choose the
coefÔ¨Åcients c1, c2, p2, and a2,1 in any way we wish so long as the choice sat-
isÔ¨Åes (10.78). Clearly, Heun‚Äôs method is only one choice among many possible
choices. We observe from (10.78) that we have four unknowns, but possess three
TLFeBOOK

440
NUMERICAL SOLUTION OF ORDINARY DIFFERENTIAL EQUATIONS
equations. Thus, we may select one parameter ‚Äúarbitrarily‚Äù that will then determine
the remaining ones. For example, we may select c2, so then, from (10.78)
c1 = 1 ‚àíc2,
p2 =
1
2c2
,
and
a2,1 =
1
2c2
.
(10.79)
Since one parameter is freely chosen, thus constraining all the rest, we say that
second-order Runge‚ÄìKutta methods possess one degree of freedom.
It should be clear that the previous procedure may be extended to systematically
generate Runge‚ÄìKutta methods of higher order (i.e., m > 2). However, this requires
a more complete description of the Taylor expansion for a function in two variables.
This is stated as follows.
Theorem 10.3: Taylor‚Äôs Theorem
Suppose that f (x, t), and all of its partial
derivatives of order n + 1 or less are deÔ¨Åned and continuous on D = {(x, t)|a ‚â§
t ‚â§b, c ‚â§x ‚â§d}. Let (x0, t0) ‚ààD; then, for all (x, t) ‚ààD, there is a point (Œ∑, Œæ) ‚àà
D such that
f (x, t) =
n

r=0

1
r!
r

k=0
 r
k
!
(t ‚àít0)r‚àík(x ‚àíx0)k ‚àÇrf (x0, t0)
‚àÇtr‚àík‚àÇxk

+
1
(n + 1)!
n+1

k=0
 n + 1
k
!
(t ‚àít0)n+1‚àík(x ‚àíx0)k ‚àÇn+1f (Œ∑, Œæ)
‚àÇtn+1‚àík‚àÇxk
for which (Œ∑, Œæ) is on the line segment that joins the points (x0, t0), and (x, t).
Proof
Omitted.
The reader can now easily imagine that any attempt to apply this approach for
m > 2 will be quite tedious. Thus, we shall not do this here. We will restrict our-
selves to stating a few facts. Applying the method to m = 3 (i.e., the generation
of third-order Runge‚ÄìKutta methods) leads to algorithm coefÔ¨Åcients satisfying six
equations with eight unknowns. There will be 2 degrees of freedom as a conse-
quence.
Fourth-order Runge‚ÄìKutta methods (i.e., m = 4) also possess two degrees of
freedom, and also have a truncation error per step of O(h5). One such method
(attributed to Runge) in common use is
xn+1 = xn + h
6 [k1 + 2k2 + 2k3 + k4] ,
(10.80)
where
k1 = f (xn, tn)
k2 = f

xn + 1
2hk1, tn + 1
2h

k3 = f

xn + 1
2hk2, tn + 1
2h

k4 = f (xn + hk3, tn + h).
(10.81)
Of course, an inÔ¨Ånite number of other fourth-order methods are possible.
TLFeBOOK

FIRST-ORDER ODEs
441
We mention that Runge‚ÄìKutta methods are explicit methods, and so in principle
carry some risk of instability. However, it turns out that the higher the order of
the method, the lower the risk of stability problems. In particular, users of fourth-
order methods typically experience few stability problems in practice. In fact, it
can be shown that on applying (10.80) and (10.81) to the model problem (10.28),
we obtain
xn =

1 + hŒª + 1
2h2Œª2 + 1
6h3Œª3 + 1
24h4Œª4
n
x0.
(10.82)
A plot of
œÉ = 1 + hŒª + 1
2h2Œª2 + 1
6h3Œª3 + 1
24h4Œª4
(10.83)
in terms of hŒª appears in Fig. 10.8. To avoid limn‚Üí‚àû|xn| = ‚àû, we must have
|œÉ| ‚â§1, and so it turns out that
‚àí2.785 ‚â§hŒª ‚â§0
(see Table 9.11 on p. 685 of Rao [7]). This is in agreement with Fig. 10.8. Thus
Œª < 0,
h ‚â§2.785
|Œª| .
(10.84)
This represents an improvement over (10.33).
Example 10.8
Once again we repeat Example 10.5 for which
dx
dt = t2 ‚àí2x
t
with x0 = 1.0 for t0 = .05, but here instead our step size is now h = 0.05. Addition-
ally, our comparison is between Heun‚Äôs method and the fourth-order Runge‚ÄìKutta
method deÔ¨Åned by Eqs. (10.80) and (10.81).
‚àí3.5
‚àí3
‚àí2.5
‚àí2
‚àí1.5
‚àí1
‚àí0.5
0
0.5
0
0.5
1
1.5
2
hl
s
Figure 10.8
A plot of œÉ in terms of hŒª [see Eq. (10.83)].
TLFeBOOK

442
NUMERICAL SOLUTION OF ORDINARY DIFFERENTIAL EQUATIONS
0
0.5
1
1.5
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Time (t)
Amplitude
x(t)
4th-order Runge‚àíKutta (h = 0.05)
Heun (h = 0.05)
Figure 10.9
Comparison of Heun‚Äôs method (a second-order Runge‚ÄìKutta method) with
a fourth-order Runge‚ÄìKutta method [Eqs. (10.80) and (10.81)]. This is for the differential
equation in Example 10.8.
The simulated solutions based on these methods appear in Fig. 10.9. As expected,
the fourth-order method is much more accurate than Heun‚Äôs method. The plot
in Fig. 10.9 was generated using MATLAB, and the code for this appears in
Appendix 10.A as an example. (Of course, previous plots were also produced by
MATLAB codes similar to that in Appendix 10.A.)
10.3
SYSTEMS OF FIRST-ORDER ODEs
The methods of Section 10.2 may be extended to handle systems of Ô¨Årst-order
ODEs where the number of ODEs in the system is arbitrary (but Ô¨Ånite). However,
we will consider only systems of two Ô¨Årst-order ODEs here. SpeciÔ¨Åcally, we wish
to solve (numerically)
dx
dt = f (x, y, t),
(10.85a)
dy
dt = g(x, y, t),
(10.85b)
where the initial condition is x0 = x(t0) and y0 = y(t0). This is sufÔ¨Åcient, for
example, to simulate the DufÔ¨Ång equation mentioned in Section 10.1.
TLFeBOOK

SYSTEMS OF FIRST-ORDER ODEs
443
As in Section 10.2, we will begin with Euler methods. Therefore, following
(10.21) we may Taylor-expand x(t) and y(t) about the sampling time t = tn. As
before, for convenience we may deÔ¨Åne x(k)
n
= x(k)(tn), y(k)
n
= y(k)(tn). The relevant
expansions are given by
xn+1 = xn + hx(1)
n
+ 1
2h2x(2)
n
+ ¬∑ ¬∑ ¬∑ ,
(10.86a)
yn+1 = yn + hy(1)
n
+ 1
2h2y(2)
n
+ ¬∑ ¬∑ ¬∑ .
(10.86b)
The explicit Euler method follows by retaining the Ô¨Årst two terms in each expansion
in (10.86). Thus, the Euler method in this case is
xn+1 = xn + hf (xn, yn, tn),
(10.87a)
yn+1 = yn + hg(xn, yn, tn),
(10.87b)
where we have used the fact that x(1)
n
= f (xn, yn, tn) and y(1)
n
= g(xn, yn, tn). As
we might expect, the implicit form of the Euler method is
xn+1 = xn + hf (xn+1, yn+1, tn+1),
(10.88a)
yn+1 = yn + hg(xn+1, yn+1, tn+1).
(10.88b)
Of course, to employ (10.88) will generally involve solving a nonlinear system of
equations for xn+1 and yn+1, necessitating the use of Chapter 7 techniques. As
before, we refer to parameter h as the step size.
The accuracy of the explicit and implicit Euler methods for systems is the same
as for individual equations; speciÔ¨Åcally, it is O(h2). However, stability analysis is
more involved. Matrix methods simply cannot be avoided. This is demonstrated as
follows.
The model problem for a single Ô¨Årst-order ODE was Eq. (10.28). For a coupled
system of two Ô¨Årst-order ODEs as in (10.85), the model problem is now
dx
dt = a00x + a01y,
(10.89a)
dy
dt = a10x + a11y.
(10.89b)
Here ai,j are real-valued constants. We remark that this may be written in more
compact matrix form
dx
dt = Ax,
(10.90)
where x = x(t) = [x(t)y(t)]T and dx/dt = [dx(t)/dt
dy(t)/dt]T and, of course
A =
 a00
a01
a10
a11

.
(10.91)
TLFeBOOK

444
NUMERICAL SOLUTION OF ORDINARY DIFFERENTIAL EQUATIONS
Powerful claims are possible using matrix methods. For example, it can be argued
that if x(t) ‚ààRN (so that A ‚ààRN√óN), then dx(t)/dt = Ax(t) has solution
x(t) = eAtx(0)
(10.92)
for t ‚â•0; that is, (10.92) for N = 2 is the general solution to (10.90) [and hence
to (10.89)].5
The constants in A are related to f (x, y, t) and g(x, y, t) in the following
manner. Recall (10.25). If we retain only the linear terms in the Taylor expansions
of f and g around the point (x0, y0, t0), then
f (x, y, t) ‚âàf (x0, y0, t0) + (x ‚àíx0)‚àÇf (x0, y0, t0)
‚àÇx
+ (y ‚àíy0)‚àÇf (x0, y0, t0)
‚àÇy
+ (t ‚àít0)‚àÇf (x0, y0, t0)
‚àÇt
,
(10.93a)
g(x, y, t) ‚âàg(x0, y0, t0) + (x ‚àíx0)‚àÇg(x0, y0, t0)
‚àÇx
+ (y ‚àíy0)‚àÇg(x0, y0, t0)
‚àÇy
+ (t ‚àít0)‚àÇg(x0, y0, t0)
‚àÇt
.
(10.93b)
As a consequence
A =
Ô£Æ
Ô£ØÔ£ØÔ£∞
‚àÇf (x0, y0, t0)
‚àÇx
‚àÇf (x0, y0, t0)
‚àÇy
‚àÇg(x0, y0, t0)
‚àÇx
‚àÇg(x0, y0, t0)
‚àÇy
Ô£π
Ô£∫Ô£∫Ô£ª.
(10.94)
At this point we may apply the explicit Euler method (10.87) to the model problem
(10.89), which results in
 xn+1
yn+1

=
 1 + ha00
ha01
ha10
1 + ha11
  xn
yn

.
(10.95)
5Yes, as surprising as it seems, although At is a matrix exp(At) makes sense as an operation. In fact,
for example, with x(t) = [x0(t) ¬∑ ¬∑ ¬∑ xn‚àí1(t)]T , the system of Ô¨Årst-order ODEs
dx(t)
dt
= Ax(t) + by(t)
(A ‚ààRn√ón, b ‚ààRn, and y(t) ‚ààR) has the general solution
x(t) = eAtx(0‚àí) +
 t
0‚àíeA(t‚àíœÑ)by(œÑ) dœÑ.
The integral in this solution is an example of a convolution integral.
TLFeBOOK

SYSTEMS OF FIRST-ORDER ODEs
445
An alternative form for this is
xn+1 = (I + hA)xn.
(10.96)
Here I is a 2 √ó 2 identity matrix and xn = [xn
yn]T . With x0 = [x0
y0]T , we
may immediately claim that
xn = (I + hA)nx0,
(10.97)
where n ‚ààZ+. We observe that this includes (10.32) as a special case. Naturally,
we must select step size h to avoid instability; that is, we are forced to select h
to prevent limn‚Üí‚àû||xn|| = ‚àû. In principle, the choice of norm is arbitrary, but
2-norms are often chosen. We recall that there is a nonsingular matrix T (matrix
of eigenvectors) such that
T ‚àí1[I + hA]T = ,
(10.98)
where  is the matrix of eigenvalues. We will assume that
 =
 Œª0
0
0
Œª1

.
(10.99)
In other words, we assume I + hA is diagonalizable. This is not necessarily always
the case, but is an acceptable assumption for present purposes. Since from (10.98)
we have I + hA = T T ‚àí1, (10.96) becomes
xn+1 = T T ‚àí1xn,
or
T ‚àí1xn+1 = T ‚àí1xn.
(10.100)
Let yn = T ‚àí1xn, so therefore (10.100) becomes
yn+1 = yn.
(10.101)
In any norm limn‚Üí‚àû||yn|| Ã∏= ‚àû, provided |Œªk| ‚â§1 for all k = 0, 1, . . . , N ‚àí1
( ‚ààRN√óN). Consequently, limn‚Üí‚àû||xn|| Ã∏= ‚àûtoo (because ||xn|| = ||T yn|| ‚â§
||T || ||yn|| and ||T || is Ô¨Ånite). We conclude that h is an acceptable step size,
provided the eigenvalues of I + hA do not possess a magnitude greater than unity.
Note that in practice we normally insist that h result in |Œªk| < 1 for all k.
We may apply the previous stability analysis to the implicit Euler method.
SpeciÔ¨Åcally, apply (10.88) to model problem (10.89), giving
xn+1 = xn + h[a00xn+1 + a01yn+1]
yn+1 = yn + h[a10xn+1 + a11yn+1],
TLFeBOOK

446
NUMERICAL SOLUTION OF ORDINARY DIFFERENTIAL EQUATIONS
which in matrix form becomes
 xn+1
yn+1

=
 xn
yn

+ h
 a00
a01
a10
a11
  xn+1
yn+1

,
or more compactly as
xn+1 = xn + hAxn+1.
(10.102)
Consequently, for n ‚ààZ+
xn = ([I ‚àíhA]‚àí1)nx0.
(10.103)
For convenience we can deÔ¨Åne B = [I ‚àíhA]‚àí1. SuperÔ¨Åcially, (10.103) seems to
have the same form as (10.97). We might be lead therefore to believe (falsely)
that the implicit method can be unstable, too. However, we may assume that there
exists a nonsingular matrix V such that (if A ‚ààRN√óN)
V ‚àí1AV = ,
(10.104)
where  = diag(Œ≥0, Œ≥1, . . . , Œ≥N‚àí1), which is the diagonal matrix of the eigenvalues
of A. (Once again, it is not necessarily the case that A is always diagonalizable,
but the assumption is reasonable for our present purposes.) Immediately
[I ‚àíhA]‚àí1 = [I ‚àíhV V ‚àí1]‚àí1 = (V [V ‚àí1V ‚àíh]V ‚àí1)‚àí1
so that
[I ‚àíhA]‚àí1 = V [I ‚àíh]‚àí1V ‚àí1.
(10.105)
Consequently xn+1 = [I ‚àíhA]‚àí1xn becomes
xn+1 = V [I ‚àíh]‚àí1V ‚àí1xn.
(10.106)
DeÔ¨Åne yn = V ‚àí1xn, and so (10.106) becomes
yn+1 = [I ‚àíh]‚àí1yn.
(10.107)
Because I ‚àíh is a diagonal matrix, a typical main diagonal element of [I ‚àí
h]‚àí1 is œÉk = 1/(1 ‚àíhŒ≥k). It is a fact (which we will not prove here) that the
model problem in the general case is stable provided the eigenvalues of A all pos-
sess negative-valued real parts.6 Thus, provided Re(Œ≥k) < 0 for all k, we are assured
that |œÉk| < 1 for all k, and hence limn‚Üí‚àû||yn|| = 0. Thus, limn‚Üí‚àû||xn|| = 0, too,
and so we conclude that the implicit form of the Euler method is unconditionally
stable. Thus, if the model problem is stable, we may select any h > 0.
6The eigenvalues of A may be complex-valued, and so it is the real parts of these that truly determine
system stability.
TLFeBOOK

SYSTEMS OF FIRST-ORDER ODEs
447
The following example is of a linear system for which a mathematically exact
solution can be found. [In fact, the solution is given by (10.92).]
Example 10.9
Consider the ODE system
dx
dt = ‚àí2x + 1
4y,
(10.108a)
dy
dt = ‚àí3x.
(10.108b)
The initial condition is x0 = x(0) = 1, y0 = y(0) = ‚àí1. From (10.94) we see that
A =

‚àí2
1
4
‚àí3
0
	
.
The eigenvalues of A are Œ≥0 = ‚àí1
2, and Œ≥1 = ‚àí3
2. These eigenvalues are both
negative, and so the solution to (10.108) happens to be stable. In fact, the exact
solution can be shown to be
x(t) = ‚àí3
4e‚àít/2 + 7
4e‚àí3t/2,
(10.109a)
y(t) = ‚àí9
2e‚àít/2 + 7
2e‚àí3t/2
(10.109b)
for t ‚â•0. Note that the eigenvalues of A appear in the exponents of the exponentials
in (10.109). This is not a coincidence. The explicit Euler method has the iterations
xn+1 = xn + h

‚àí2xn + 1
4yn

,
(10.110a)
yn+1 = yn ‚àí3hxn.
(10.110b)
Simulation results are shown in Figs. 10.10 and 10.11 for h = 0.1 and h =
1.4, respectively. This involves comparing (10.110a,b) with the exact solution
(10.109a,b). Figure 10.10b shows a plot of the eigenvalues of I + hA for various
step sizes. We see from this that choosing h = 1.4 must result in an unstable simu-
lation. This is conÔ¨Årmed by the result in Fig. 10.11. For comparison purposes, the
eigenvalues of I + hA and of [I ‚àíhA]‚àí1 are plotted in Fig. 10.12. This shows
that, at least in this particular case, the implicit method is more stable than the
explicit method.
Example 10.10
Recall the DufÔ¨Ång equation of Section 10.1. Also, recall the
fact that this ODE can be rewritten in the form of (10.85a,b), and this was done in
Eq. (10.4a,b).
Figures 10.13 and 10.14 show the result of simulating the DufÔ¨Ång equation
using the explicit Euler method for the model parameters
F = 0.5,
œâ = 1,
m = 1,
Œ± = 1,
Œ¥ = 0.1,
k = 0.05.
TLFeBOOK

448
NUMERICAL SOLUTION OF ORDINARY DIFFERENTIAL EQUATIONS
0
1
2
3
4
5
6
7
8
9
10
‚àí2.5
‚àí2
‚àí1.5
‚àí1
‚àí0.5
0
0.5
1
Time (t)
Amplitude
0
0.5
1
1.5
‚àí1.5
‚àí1
‚àí0.5
0
0.5
1
Step size (h)
Amplitude
x(t)
y(t)
Euler method (h = 0.1)
Euler method (h = 0.1)
l0
l1
(a)
(b)
Figure 10.10
Simulation results for h = 0.1. Plot (b) shows the eigenvalues of I + hA.
Both plots were obtained by applying the explicit form of the Euler method to the ODE
system of Example 10.9. Clearly, the simulation is stable for h = 0.1.
Thus (10.1) is now
d2x
dt2 = 0.5 cos(t) ‚àí0.05dx
dt ‚àí[x + 0.1x3].
We use initial condition x(0) = y(0) = 0. The driving function (applied force)
0.5 cos(t) is being opposed by the restoring force of the spring (terms in square
brackets) and friction (Ô¨Årst derivative term). Therefore, on physical grounds, we do
not expect the solution x(t) to grow without bound as t ‚Üí‚àû. Thus, the simulated
solution to this problem must be stable, too.
We mention that an analytical solution to the differential equation that we are
simulating is not presently known.
From (10.94) for our DufÔ¨Ång system example we have
A =
Ô£Æ
Ô£∞
0
1
‚àíŒ±
m ‚àí3Œ¥
m x2
0
‚àík
m
Ô£π
Ô£ª.
Example 10.11
According to Hydon [1, p. 61], the second-order ODE
d2x
dt2 =
 dx
dt
!2 1
x +
 
x ‚àí1
x
! dx
dt
(10.111)
TLFeBOOK

SYSTEMS OF FIRST-ORDER ODEs
449
0
1
2
3
4
5
6
7
8
9
10
‚àí8
‚àí6
‚àí4
‚àí2
0
2
4
6
8
Time (t)
Amplitude
x(t)
y(t)
Euler method (h = 1.4)
Euler method (h = 1.4)
Figure 10.11
This is the result of applying the explicit form of the Euler method to the
ODE system of Example 10.9. Clearly, the simulation is not stable for h = 1.4. This is
predicted by the eigenvalue plot in Fig. 10.10b, which shows that one of the eigenvalues of
I + hA has a magnitude exceeding unity for this choice of h.
0
0.5
1
1.5
‚àí1.5
‚àí1
‚àí0.5
0
0.5
1
Step size (h)
(a)
(b)
0
0.5
1
1.5
Step size (h)
Amplitude
Amplitude
l0
l1
l0
l1
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
l0
l1
Figure 10.12
Plots of the eigenvalues of I + hA (a), which determine the stability of the
explicit Euler method and the eigenvalues of [I ‚àíhA]‚àí1 (b), which determine the stability
of the implicit Euler method. This applies for the ODE system of Example 10.9.
TLFeBOOK

450
NUMERICAL SOLUTION OF ORDINARY DIFFERENTIAL EQUATIONS
0
10
20
30
40
50
60
70
80
90
‚àí3
‚àí2
‚àí1
0
1
2
3
4
Time (t)
Amplitude
Amplitude
xn (h = .020)
yn (h = .020)
0
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.9996
0.9998
1
1.0002
1.0004
1.0006
Step size (h)
(b)
(a)
|l0|
|l1|
Figure 10.13
(a) Explicit Euler method simulation of the DufÔ¨Ång equation; (b) magnitude
of the eigenvalues of I + hA. Both plots were obtained by applying the explicit form of the
Euler method to the ODE system of Example 10.10, which is the DufÔ¨Ång equation expressed
as a coupled system of Ô¨Årst-order ODEs. The simulation is apparently stable for h = 0.02.
This is in agreement with the prediction based on the eigenvalues of I + hA [plot (b)],
which have a magnitude of less than unity for this choice of h.
has the exact solution
x(t) =
Ô£±
Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£≤
Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£≥
c1 ‚àí
7
c2
1 ‚àí1 tanh(
7
c2
1 ‚àí1(t + c2)),
c2
1 > 1
c1 ‚àí(t + c2)‚àí1,
c2
1 = 1
c1 +
7
1 ‚àíc2
1 tanh(
7
1 ‚àíc2
1(t + c2)),
c2
1 < 1
.
(10.112)
The ODE in (10.111) can be rewritten as the system of Ô¨Årst-order ODEs
dx
dt = y,
(10.113a)
dy
dt = y2
x +
 
x ‚àí1
x
!
y.
(10.113b)
TLFeBOOK

SYSTEMS OF FIRST-ORDER ODEs
451
0
10
20
30
40
50
60
‚àí20
‚àí10
0
10
20
Time (t)
Amplitude
0
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.9996
0.9998
1
1.0002
1.0004
1.0006
Step size (h)
Amplitude
xn (h = 0.055)
yn (h = 0.055)
(b)
(a)
|l0|
|l1|
Figure 10.14
(a) Explicit Euler method simulation of the DufÔ¨Ång equation; (b) magnitude
of the eigenvalues of I + hA. Both plots were obtained by applying the explicit form of the
Euler method to the ODE system of Example 10.10, which is the DufÔ¨Ång equation expressed
as a coupled system of Ô¨Årst-order ODEs. The simulation is not stable for h = 0.055. This is
in agreement with the prediction based on the eigenvalues of I + hA [plot (b)], which have
a magnitude exceeding unity for this choice of h.
The initial condition is x0 = x(0), and y(0) = dx(t)
dt |t=0. From (10.94) we have
A =
Ô£Æ
Ô£ØÔ£∞
0
1
‚àíy2
0
x2
0
+ y0
'
1 + 1
x2
0
(
2y0
x0
+
 
x0 ‚àí1
x0
!
Ô£π
Ô£∫Ô£ª.
(10.114)
Using (10.113a), we may obtain y(t) from (10.112). For example, let us consider
simulating the case c2
1 = 1. Thus, in this case
y(t) =
1
(t + c2)2 .
(10.115)
For the choice c2
1 = 1, we have
x0 = c1 ‚àí1
c2
,
(10.116a)
y0 = 1
c2
2
.
(10.116b)
TLFeBOOK

452
NUMERICAL SOLUTION OF ORDINARY DIFFERENTIAL EQUATIONS
0
0.5
1
1.5
2
2.5
3
3.5
‚àí1
0
1
2
3
4
Time (t)
(a)
(b)
Amplitude
Amplitude
x(t)
y(t)
xn (h = 0.020)
yn (h = 0.020)
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0
0.5
1
1.5
2
Step size (h)
|l0|
|l1|
Figure 10.15
Magnitude of the eigenvalues of I + hA is shown in plot (b). Both plots
show the simulation results of applying the explicit Euler method to the ODE system in
Example 10.11. The simulation is (as expected) stable for h = 0.02. Clearly, the simulated
result agrees well with the exact solution.
If we select c1 = 1, then
x0 = 1 ‚àí1
c2
,
y0 = (1 ‚àíx0)2.
(10.117)
Let us assume x0 = ‚àí1, and so y0 = 4. The result of applying the explicit Euler
method to system (10.113) with these conditions is shown in Figs. 10.15 and 10.16.
The magnitudes of the eigenvalues of I + hA are displayed in plots (b) of both
Ô¨Ågures. We see that for Fig. 10.15, h = 0.02 and a stable simulation is the result,
while for Fig. 10.16, we have h = 0.35, for which the simulation is unstable. This
certainly agrees with the stability predictions based on Ô¨Ånding the eigenvalues of
matrix I + hA.
Examples 10.10 and 10.11 illustrate just how easy it is to arrive at differential
equations that are not so simple to simulate in a stable manner with low-order
explicit methods. It is possible to select an h that is ‚Äúsmall‚Äù in some sense, yet
not small enough for stability. The cubic nonlinearity in the DufÔ¨Ång model makes
the implementation of the implicit form of Euler‚Äôs method in this problem quite
TLFeBOOK

SYSTEMS OF FIRST-ORDER ODEs
453
0
1
2
3
4
5
6
2
1
0
1
2
3
4
5
Time (t)
Amplitude
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0
0.5
1
1.5
2
Step size (h)
Amplitude
x(t)
y(t)
xn (h = 0.35)
yn (h = 0.35)
|Œª0|
|Œª1|
(a)
(b)
Figure 10.16
Plot (b) shows magnitude of eigenvalues of I + hA. Both plots show the sim-
ulation results of applying the explicit Euler method to the ODE system in Example 10.11.
The simulation is (as expected) not stable for h = 0.35. Instability is conÔ¨Årmed by the fact
that the simulated result deviates greatly from the exact solution when t is sufÔ¨Åciently large.
unattractive. So, a better approach to simulating the DufÔ¨Ång equation is with a
higher-order explicit method.
For example, Heun‚Äôs method for (10.85) may be stated as
xn+1 = xn + 1
2h[f (xn, yn, tn) + f (xn + k1, yn + l1, tn + h)],
(10.118a)
yn+1 = yn + 1
2h[g(xn, yn, tn) + g(xn + k1, yn + l1, tn + h)],
(10.118b)
where
k1 = hf (xn, yn, tn), l1 = hg(xn, yn, tn).
(10.119)
Also, for example, Chapter 36 of Bronson [8] contains a summary of higher-order
methods that may be applied to (10.85).
Example 10.12
Recall the DufÔ¨Ång equation simulation in Example 10.10.
Figure 10.17 illustrates the simulation of the DufÔ¨Ång equation using both the
explicit Euler method and Heun‚Äôs method for a small h (i.e., h = 0.005 in both
cases).
TLFeBOOK

454
NUMERICAL SOLUTION OF ORDINARY DIFFERENTIAL EQUATIONS
0
10
20
30
40
50
60
70
80
‚àí3
‚àí2
‚àí1
0
1
2
3
Time (t)
Amplitude
0
10
20
30
40
50
60
70
80
‚àí3
‚àí2
‚àí1
0
1
2
3
Time (t)
Amplitude
xn (h = 0.005)
yn (h = 0.005)
xn (h = 0.005)
yn (h = 0.005)
(a)
(b)
Figure 10.17
Comparison of the explicit Euler (a) and Heun (b) method simulations of
the ODE in Example 10.12, which is the DufÔ¨Ång equation. Here the step size h is small
enough that the two methods give similar results.
At this point we note that there are other ways to display the results of numerical
solutions to ODEs that can lead to further insights into the behavior of the dynamic
system that is modeled by those ODEs. Figure 10.18 illustrates the phase portrait
of the DufÔ¨Ång system. This is obtained by plotting the points (xn, yn) on the
Cartesian plane, yielding an approximate plot of (x(t), y(t)). The resulting curve
is the trajectory, or orbit for the system. Periodicity of the system‚Äôs response is
indicated by curves that encircle a point of equilibrium, which in this case would
be the center of the Cartesian plane [i.e., point (0, 0)]. The trajectory is tending to
an approximately ellipse-shaped closed curve indicative of approximately simple
harmonic motion.
The results in Fig. 10.18 are based on the parameters given in Example 10.10.
However, Fig. 10.19 shows what happens when the system parameters become
F = 0.3,
œâ = 1,
m = 1,
Œ± = ‚àí1,
Œ¥ = 1,
k = 0.22.
(10.120)
The phase portrait displays a more complicated periodicity than what appears in
Fig. 10.18. The Ô¨Ågure is similar to Fig. 2.2.5 in Guckenheimer and Holmes [11].
TLFeBOOK

MULTISTEP METHODS FOR ODEs
455
0
20
40
60
80
100
120
140
160
‚àí3
‚àí2
‚àí1
0
1
2
3
Time (t)
Amplitude
‚àí3
‚àí2
‚àí1
0
1
2
3
‚àí3
‚àí2
‚àí1
0
1
2
3
x(t)
y(t)
xn (h = 0.005)
yn (h = 0.005)
(a)
(b)
Figure 10.18
(a) The result of applying Heun‚Äôs method to obtain the numerical solution
of the DufÔ¨Ång system speciÔ¨Åed in Example 10.10; (b) the phase portrait for the system
obtained by plotting the points (xn, yn) [from plot (a)] on the Cartesian plane, thus yielding
an approximate plot of (x(t), y(t)).
As in the cases of the explicit and implicit Euler methods, we may obtain a
theory of stability for Heun‚Äôs method. As before, the approach is to apply the
model problem (10.90) to (10.118) and (10.119). As an exercise, the reader should
show that this yields
xn+1 =

I + hA + 1
2h2A2
xn,
(10.121)
where I is the 2 √ó 2 identity matrix, A is obtained by using (10.94), and, of course,
xn = [xnyn]T . [The similarity between (10.121) and (10.48) is no coincidence.]
Criteria for the selection of step size h leading to a stable simulation can be
obtained by analysis of (10.121). But the details are not considered here.
10.4
MULTISTEP METHODS FOR ODEs
The numerical ODE solvers we have considered so far were either implicit methods
or explicit methods. But in all cases they were examples of so-called single-step
TLFeBOOK

456
NUMERICAL SOLUTION OF ORDINARY DIFFERENTIAL EQUATIONS
0
20
40
60
80
100
120
140
160
‚àí1.5
‚àí1
‚àí0.5
0
0.5
1
1.5
Time (t)
Amplitude
‚àí1.5
‚àí1
‚àí0.5
0
0.5
1
1.5
‚àí1
‚àí0.5
0
0.5
1
x(t)
y(t)
xn (h = 0.005)
yn (h = 0.005)
(a)
(b)
Figure 10.19
(a) The result of applying Heun‚Äôs method to obtain the numerical solu-
tion of the DufÔ¨Ång system speciÔ¨Åed in Example 10.12 [i.e., using the parameter values in
Eq. (10.120)]; (b) the phase portrait for the system obtained by plotting the points (xn, yn)
[from plot (a)] on the Cartesian plane, thus yielding an approximate plot of (x(t), y(t)).
methods; that is, xn+1 was ultimately only a function of xn. A disadvantage of
single-step methods is that to achieve good accuracy often requires the use of
higher-order methods (e.g., fourth- or Ô¨Åfth-order Runge‚ÄìKutta). But higher-order
methods need many function evaluations per step, and so are computationally
expensive.
Implicit single-step methods, although inherently stable, are not more accurate
than explicit methods, although they can track fast changes in the solution x(t)
better than can explicit methods (recall Example 10.5). However, implicit methods
may require nonlinear system solvers (i.e., Chapter 7 methods) as part of their
implementation. This is a complication that is also not necessarily very efÔ¨Åcient
computationally. Furthermore, the methods in Chapter 7 possess their own stability
problems. Therefore, in this section we introduce multistep predictor‚Äìcorrector
methods that overcome some of the deÔ¨Åciencies of the methods we have considered
so far.
In this section we return to consideration of a single Ô¨Årst-order ODE IVP
x(1)(t) = dx(t)
dt
= f (x(t), t), x0 = x(0).
(10.122)
TLFeBOOK

MULTISTEP METHODS FOR ODEs
457
In reality, we have already seen a single-step predictor‚Äìcorrector method in
Section 10.2. Suppose that we have the following method:
Àúxn+1 = xn + hf (xn, tn)
(predictor step)
(10.123a)
xn+1 = xn + 1
2h[f (Àúxn+1, tn+1) + f (xn, tn)]
(corrector step).
(10.123b)
If we substitute (10.123a) into (10.123b), we again arrive at Heun‚Äôs method
[Eq. (10.47)], which overcame the necessity to solve for xn+1 in the implicit method
of Eq. (10.59) (trapezoidal method). Generally, predictor‚Äìcorrector methods replace
implicit methods in this manner, and we will see more examples further on in
this section. When a higher-order implicit method is ‚Äúconverted‚Äù to a predictor‚Äì
corrector method, the need to solve nonlinear equations is eliminated and accuracy
is preserved, but the stability characteristics of the implicit method will be lost, at
least to some extent. Of course, a suitable stability theory will still allow the user
to select reasonable values for the step size parameter h.
We may now consider a few simple examples of multistep methods. Perhaps
the simplest multistep methods derive from the numerical differentiation ideas from
Section 9.6 (of Chapter 9). Recall (9.138), for which
x(1)(t) = 1
2h[x(t + h) ‚àíx(t ‚àíh)] ‚àí1
6h2x(3)(Œæ)
(10.124)
for some Œæ ‚àà[t ‚àíh, t + h]. The explicit Euler method (10.22) can be replaced
with the midpoint method derived from [using t = tn in (10.124)]
f (x(tn), tn) ‚âà1
2h[x(tn+1) ‚àíx(tn‚àí1)],
so this method is
xn+1 = xn‚àí1 + 2hf (xn, tn).
(10.125)
This is an explicit method, but xn+1 depends on xn‚àí1 as well as xn. We may call
it a two-step method. Similarly, via (9.153)
f (x(tn), tn) ‚âà1
2h[‚àí3x(tn) + 4x(tn+1) ‚àíx(tn+2)],
so we have the method
xn+2 = 4xn+1 ‚àí3xn ‚àí2hf (xn, tn)
which can be rewritten as
xn+1 = 4xn ‚àí3xn‚àí1 ‚àí2hf (xn‚àí1, tn‚àí1).
(10.126)
TLFeBOOK

458
NUMERICAL SOLUTION OF ORDINARY DIFFERENTIAL EQUATIONS
Finally, via (9.154), we obtain
f (x(tn), tn) ‚âà1
2h[x(tn‚àí2) ‚àí4x(tn‚àí1) + 3x(tn)],
yielding the method
xn+1 = 4
3xn ‚àí1
3xn‚àí1 + 2
3hf (xn+1, tn+1).
(10.127)
Method (10.126) is a two-step method that is explicit, but (10.127) is a two-step
implicit method since we need to solve for xn+1.
A problem with multistep methods is that the IVP (10.122) provides only one
initial condition x0. But for n = 1 in any of (10.125), (10.126), or (10.127), we
need to know x1; that is, for two-step methods we need two initial conditions, or
starting values. A simple way out of this dilemma is to use single-step methods to
provide any missing starting values. In fact, predictor‚Äìcorrector methods derived
from single-step concepts (e.g., Runge‚ÄìKutta methods) are often used to provide
the starting values for multistep methods.
What about multistep method accuracy? Let us consider the midpoint method
again. From (10.124) for some Œæn ‚àà[tn‚àí1, tn+1]
x(tn+1) = x(tn‚àí1) + 2hf (x(tn), tn) + 1
3h3x(3)(Œæn),
(10.128)
so the method (10.125) has a truncation error per step that is O(h3). The midpoint
method is therefore more accurate than the explicit Euler method [recall (10.43)
and (10.44)]. Yet we see that both methods need only one function evaluation
per step. Thus, the midpoint method is more efÔ¨Åcient than the Euler method. We
recall that Heun‚Äôs method (a Runge‚ÄìKutta method) has a truncation error per
step that is O(h3), too [recall (10.56)], and so Heun‚Äôs method may be used to
initialize (i.e., provide starting values for) the midpoint method (10.125). This
speciÔ¨Åc situation holds up in general. Thus, a multistep method can often achieve
comparable accuracy to single-step methods, and yet use fewer function calls,
leading to reduced computational effort.
What about stability considerations? Let us continue with our midpoint method
example. If we apply the model problem (10.28) to (10.125), we have the difference
equation
xn+1 = xn‚àí1 + 2hŒªxn,
(10.129)
which is a second-order difference equation. This has characteristic equation7
z2 ‚àí2hŒªz ‚àí1 = 0.
(10.130)
7We may rewrite (10.129) as
xn+2 ‚àí2hŒªxn+1 ‚àíxn = 0,
which has the z-transform
(z2 ‚àí2hŒªz ‚àí1)X(z) = 0.
TLFeBOOK

MULTISTEP METHODS FOR ODEs
459
For convenience, let œÅ = hŒª, in which case the roots of (10.130) are easily seen
to be
z1 = œÅ +
7
œÅ2 + 1,
z2 = œÅ ‚àí
7
œÅ2 + 1, .
(10.131)
A general solution to (10.129) will have the form
xn = c1zn
1 + c2zn
2
(10.132)
for n ‚ààZ+. Knowledge of x0 and x1 allows us to solve for the constants c1 and c2 in
(10.132), if this is desired. However, more importantly, we recall that we assume
Œª < 0, and we seek step size h > 0 so that limn‚Üí‚àû|xn| Ã∏= ‚àû. But in this case
œÅ = hŒª < 0, and hence from (10.131), |z2| > 1 for all h > 0. If c2 Ã∏= 0 in (10.132)
(which is practically always the case), then we will have limn‚Üí‚àû|xn| = ‚àû! Thus,
the midpoint method is inherently unstable under all realistic conditions ! Term c1zn
1
in (10.132) is ‚Äúharmless‚Äù since |z1| < 1 for suitable h. But the term c2zn
2, often
called a parasitic term, will eventually ‚Äúblow up‚Äù with increasing n, thus fatally
corrupting the approximation to x(t).
Unfortunately, parasitic terms are inherent in multistep methods. However, there
are more advanced methods with stability theories designed to minimize the effects
of the parasitics. We now consider a few of these improved multistep methods.
10.4.1
Adams‚ÄìBashforth Methods
Here we look at the Adams‚ÄìBashforth (AB) family of multistep ODE IVP solvers.
Section 10.4.2 will look at the Adams‚ÄìMoulton (AM) family. Our approach follows
Epperson [12, Section 6.6]. Both families are derived using Lagrange interpolation
[recall Section 6.2 from Chapter 6 (above)].
Recall (10.122) which we may integrate to obtain (tn = t0 + nh)
x(tn+1) = x(tn) +
 tn+1
tn
f (x(t), t) dt.
(10.133)
Now suppose that we had the samples x(tn‚àík) for k = 0, 1, . . . , m (i.e., m + 1 sam-
ples of the exact solution x(t)). Via Lagrange interpolation theory, we may interpo-
late F(t) = f (x(t), t) [the integrand of (10.133)] for t ‚àà[tn‚àím, tn+1]8 according to
pm(t) =
m

k=0
Lk(t)f (x(tn‚àík), tn‚àík),
(10.134)
A solution to (10.129) exists only if z2 ‚àí2hŒªz ‚àí1 = 0. If the reader has not had a signals and systems
course (or equivalent) then this reasoning must be accepted ‚Äúon faith.‚Äù But it may help to observe that
the reasoning is similar to the theory of solution for linear ODEs in constant coefÔ¨Åcients.
8The upper limit on the interval [tn‚àím, tn] has been extended from tn to tn+1 here. This is allowed
under interpolation theory, and actually poses no great problem in either method development or error
analysis. We are using the Lagrange interpolant to extrapolate from t = tn to tn+1.
TLFeBOOK

460
NUMERICAL SOLUTION OF ORDINARY DIFFERENTIAL EQUATIONS
where
Lk(t) =
m
/
i=0
iÃ∏=k
t ‚àítn‚àíi
tn‚àík ‚àítn‚àíi
.
(10.135)
From (6.14) for some Œæt ‚àà[tn‚àím, tn+1]
F(t) = pm(t) +
1
(m + 1)!F (m+1)(Œæt)
m
/
i=0
(t ‚àítn‚àíi).
(10.136)
However, F(t) = f (x(t), t) = x(1)(t) so (10.136) becomes
F(t) = pm(t) +
1
(m + 1)!x(m+2)(Œæt)
m
/
i=0
(t ‚àítn‚àíi).
(10.137)
Thus, if we now substitute (10.137) into (10.133), we obtain
x(tn+1) = x(tn) +
m

k=0
f (x(tn‚àík), tn‚àík)
 tn+1
tn
Lk(t) dt + Rm(tn+1),
(10.138)
where
Rm(tn+1) =
 tn+1
tn
1
(m + 1)!x(m+2)(Œæt)
m
/
i=0
(t ‚àítn‚àíi)
#
$%
&
=œÄ(t)
dt.
(10.139)
Polynomial œÄ(t) does not change sign for t ‚àà[tn, tn+1] (which is the interval of
integration). Thus, we can say that there is a Œæn ‚àà[tn, tn+1] such that
Rm(tn+1) =
1
(m + 1)!x(m+2)(Œæn)
 tn+1
tn
œÄ(t) dt.
(10.140)
For convenience, deÔ¨Åne
œÅm =
1
(m + 1)!
 tn+1
tn
œÄ(t) dt =
1
(m + 1)!
 tn+1
tn
(t ‚àítn)(t ‚àítn‚àí1) ¬∑ ¬∑ ¬∑ (t ‚àítn‚àím) dt
(10.141)
and
Œªk =
 tn+1
tn
Lk(t) dt.
(10.142)
Thus, (10.138) reduces to [with Rm(tn+1) = œÅmx(m+2)(Œæn)]
x(tn+1) = x(tn) +
m

k=0
Œªkf (x(tn‚àík), tn‚àík) + œÅmx(m+2)(Œæn)
(10.143)
TLFeBOOK

MULTISTEP METHODS FOR ODEs
461
TABLE 10.1
Adams‚ÄìBashforth Method Parameters
m
Œª0
Œª1
Œª2
Œª3
Rm(tn+1)
0
h
1
2h2x(2)(Œæn)
1
3
2h
‚àí1
2h
5
12h3x(3)(Œæn)
2
23
12h
‚àí16
12h
5
12h
3
8h4x(4)(Œæn)
3
55
24h
‚àí59
24h
37
24h
‚àí9
24h
251
720h5x(5)(Œæn)
for some Œæn ‚àà[tn, tn+1]. The order m + 1 Adams‚ÄìBashforth method is therefore
deÔ¨Åned to be
xn+1 = xn +
m

k=0
Œªkf (xn‚àík, tn‚àík).
(10.144)
It is an explicit method involving m + 1 steps. Table 10.1 summarizes the method
parameters for various m, and is essentially Table 6.6 from Ref. 12.
10.4.2
Adams‚ÄìMoulton Methods
The Adams‚ÄìMoulton methods are a modiÔ¨Åcation of the Adams‚ÄìBashforth meth-
ods. The Adams‚ÄìBashforth methods interpolate using the nodes tn, tn‚àí1, . . . , tn‚àím.
On the other hand, the Adams‚ÄìMoulton methods interpolate using the nodes
tn+1, tn, . . . , tn‚àím+1. Note that the number of nodes is the same in both methods.
Consequently, (10.138) becomes
x(tn+1) = x(tn) +
m‚àí1

k=‚àí1
f (x(tn‚àík), tn‚àík)
 tn+1
tn
Lk(t) dt + Rm(tn+1),
(10.145)
where now
Lk(t) =
m‚àí1
/
i=‚àí1
iÃ∏=k
t ‚àítn‚àíi
tn‚àík ‚àítn‚àíi
,
(10.146)
and Rm(tn+1) = œÅmx(m+2)(Œæn) with
œÅm =
1
(m + 1)!
 tn+1
tn
(t ‚àítn+1)(t ‚àítn) ¬∑ ¬∑ ¬∑ (t ‚àítn‚àím+1) dt.
(10.147)
TLFeBOOK

462
NUMERICAL SOLUTION OF ORDINARY DIFFERENTIAL EQUATIONS
Thus, the order m + 1 Adams‚ÄìMoulton method is deÔ¨Åned to be
xn+1 = xn +
m‚àí1

k=‚àí1
Œªkf (xn‚àík, tn‚àík),
(10.148)
where
Œªk =
 tn+1
tn
Lk(t) dt
(10.149)
[same as (10.142) except k = ‚àí1, 0, 1, . . ., m ‚àí1, and Lk(t) is now (10.146)].
Method (10.148) is an implicit method since it is necessary to solve for xn+1.
It also requires m + 1 steps. Table 10.2 summarizes the method parameters for
various m, and is essentially Table 6.7 from Ref. 12.
10.4.3
Comments on the Adams Families
For small values of m in Tables 10.1 and 10.2, we see that the Adams families
(AB family and AM family) correspond to methods seen earlier. To be speciÔ¨Åc:
1. For m = 0 in Table 10.1, Œª0 = h, so (10.144) yields the explicit Euler method
(10.22).
2. For m = 0 in Table 10.2, Œª‚àí1 = h, so (10.148) yields the implicit Euler
method (10.35).
3. For m = 1 in Table 10.2, Œª‚àí1 = Œª0 = 1
2h, so (10.148) yields the trapezoidal
method (10.59).
Stability analysis for members of the Adams families is performed in the usual
manner. For example, when m = 1 in Table 10.1 (i.e., consider the second-order
AB method), Eq. (10.144) becomes
xn+1 = xn + 1
2h[3f (xn, tn) ‚àíf (xn‚àí1, tn‚àí1)].
(10.150)
TABLE 10.2
Adams‚ÄìMoulton Method Parameters
m
Œª‚àí1
Œª0
Œª1
Œª2
Rm(tn+1)
0
h
‚àí1
2h2x(2)(Œæn)
1
1
2h
1
2h
‚àí1
12h3x(3)(Œæn)
2
5
12h
8
12h
‚àí1
12h
‚àí1
24h4x(4)(Œæn)
3
9
24h
19
24h
‚àí5
24h
1
24h
‚àí19
720h5x(5)(Œæn)
TLFeBOOK

MULTISTEP METHODS FOR ODEs
463
Application of the model problem (10.28) to (10.150) yields
xn+1 =
 
1 + 3
2hŒª
!
xn ‚àí1
2hŒªxn‚àí1
or
xn+2 ‚àí
 
1 + 3
2hŒª
!
xn+1 + 1
2hŒªxn = 0.
This has characteristic equation (with œÅ = hŒª)
z2 ‚àí
 
1 + 3
2œÅ
!
z + 1
2œÅ = 0.
(10.151)
This equation has roots
z1 = 1
2
 
1 + 3
2œÅ
!
+
8
1 + œÅ + 9
4œÅ2
	
,
z2 = 1
2
 
1 + 3
2œÅ
!
‚àí
8
1 + œÅ + 9
4œÅ2
	
.
(10.152)
We need to know what range of h > 0 yields |z1|, |z2| < 1. We consider only œÅ < 0
since Œª < 0. Figure 10.20 plots |z1| and |z2| versus œÅ, and suggests that we may
select h such that
‚àí1 < hŒª < 0.
(10.153)
Plots of stability regions for the other Adams families members may be seen in
Figs. 6.7 and 6.8 of Epperson [12]. Note that stability regions occupy the complex
plane as it is assumed in such a context that Œª ‚ààC. However, we have restricted
‚àí2
‚àí1.8
‚àí1.6
‚àí1.4
‚àí1.2
‚àí1
‚àí0.8
‚àí0.6
‚àí0.4
‚àí0.2
0
0
0.5
1
1.5
2
2.5
r
|zk|
|z1|
|z2|
Figure 10.20
Magnitudes of the roots in (10.152) as a function of œÅ = hŒª.
TLFeBOOK

464
NUMERICAL SOLUTION OF ORDINARY DIFFERENTIAL EQUATIONS
our attention to a Ô¨Årst-order ODE IVP here, and so it is actually enough to assume
that Œª ‚ààR.
Finally, observe that AB and AM methods can be combined to yield predictor‚Äì
correctors. An mth-order AB method can act as a predictor for an mth-order
AM method that is the corrector. A Runge‚ÄìKutta method can initialize the
procedure.
10.5
VARIABLE-STEP-SIZE (ADAPTIVE) METHODS FOR ODEs
Accuracy in the numerical solution of ODEs requires either increasing the order
of the method applied to the problem or decreasing the step-size parameter h.
However, high-order methods (e.g., Runge‚ÄìKutta methods of order exceeding 5)
are not very attractive at least in part because of the computational effort involved.
To preserve accuracy while reducing computational requirements suggests that we
should adaptively vary step size h.
Recall Example 10.5, where we saw that low order methods were not accurate
near t = t0. For a method of a given order, we would like in Example 10.5 to
have a small h for t near t0, but a larger h for t away from t0. This would reduce
the overall number of function evaluations needed to estimate x(t) for t ‚àà[t0, tf ].
The idea of adaptively varying h from step to step requires monitoring the error
in the solution somehow; that is, ideally, we need to infer en = |x(tn) ‚àíxn| [xn
is the estimate of x(t) at t = tn from some method] at step n. If en is small
enough, h may be increased in size at the next step, but if en is too big, we
decrease h.
Of course, we do not know x(tn), so we do not have direct access to the error en.
However, one idea that is implemented in modern software tools (e.g., MATLAB
routines ode23 and ode45) is to compute xn for a given h using two methods,
each of a different order. The method of higher order is of greater accuracy, so
if xn does not differ much between the methods, we are lead to believe that h is
small enough, and so may be increased in the next step. On the other hand, if the
xn values given by the different methods signiÔ¨Åcantly vary, we are then lead to
believe that h is too big, and so should be reduced.
In this section we give only a basic outline of the main ideas of this process.
Our emphasis is on the Runge‚ÄìKutta‚ÄìFehlberg (RKF) methods, of which MAT-
LAB routines ode23 and ode45 are particular implementations. Routine ode23
implements second- and third-order Runge‚ÄìKutta methods, while ode45 imple-
ments fourth- and Ô¨Åfth-order Runge‚ÄìKutta methods. Computational efÔ¨Åciency is
maintained by sharing intermediate results that are common to both second- and
third-order methods and common to both fourth- and Ô¨Åfth-order methods. More
speciÔ¨Åcally, Runge‚ÄìKutta methods of consecutive orders have constants such as kj
[recall (10.67)] in common with each other and so need not be computed twice.
We mention that ode45 implements a method based on Dormand and Prince [14],
and a more detailed account of this appears in Epperson [12]. An analysis of the
RKF methods also appears in Burden and Faires [17]. The details of all of this are
TLFeBOOK

VARIABLE-STEP-SIZE (ADAPTIVE) METHODS FOR ODEs
465
quite tedious, and so are not presented here. It is also worth noting that an account
of MATLAB ODE solvers is given by Shampine and Reichelt [13], who present
some improvements to the older MATLAB codes that make them better at solving
stiff systems (next section).
A pseudocode for something like ode45 is as follows, and is based on Algo-
rithm 6.5 in Epperson [12]:
Input t0, x0; { initial condition and starting time }
Input tolerance œµ > 0;
Input the initial step size h > 0, and final time tf > t0;
n := 0;
while tn ‚â§tf do begin
X1 := RKF4(xn, tn, h); { 4th order RKF estimate of xn+1 }
X2 := RKF5(xn, tn, h); { 5th order RKF estimate of xn+1 }
E := |X1 ‚àíX2|;
if 1
4 hœµ ‚â§E ‚â§hœµ then begin { h is OK }
xn+1 := X2;
tn+1 := tn + h;
n := n + 1;
else if E > hœµ then { h is too big }
h := h/2; { reduce h and repeat }
else { h is too small }
h := 2h;
xn+1 := X2;
tn+1 := tn + h;
n := n + 1;
end;
end;
Of course, variations on the ‚Äútheme‚Äù expressed in this pseudocode are possible. As
noted in Epperson [12], a drawback of this algorithm is that it will tend to oscillate
between small and large step size values. We emphasize that the method is based on
considering the local error in going from time step tn to tn+1. However, this does
not in itself guarantee that the global error |x(tn) ‚àíxn| is small. It turns out that if
adequate smoothness prevails [e.g., if f (x, t) is Lipschitz as per DeÔ¨Ånition 10.1],
then small local errors do imply small global errors (see Theorem 6.6 or 6.7 in
Ref. 12).
Example 10.13
This example illustrates a typical application of MATLAB
routine ode23 to the problem of simulating the Colpitts oscillator circuit of Example
10.2.
Figure 10.21 shows a typical plot of vCE(t), and the phase portrait for parameter
values
VTH = 0.75 V (volts),
VCC = 5 (V),
VEE = ‚àí5 (V),
REE = 400  (ohms),
RL = 35 (),
L = 98.5 √ó 10‚àí6 H (henries),
Œ≤F = 200,
RON = 100 (),
C1 = C2 = 54 √ó 10‚àí9 F (farads).
(10.154)
TLFeBOOK

466
NUMERICAL SOLUTION OF ORDINARY DIFFERENTIAL EQUATIONS
0
(a)
(b)
1
2
3
4
5
6
‚àí2.5
‚àí2
‚àí1.5
‚àí1
‚àí0.5
0
0.5
1
VBE (t) (volts)
VCE (t) (volts)
VCE (t) (volts)
2.3
2.4
2.5
2.6
2.7
2.8
2.9
3
3.1
√ó 10‚àí3
1
2
3
4
5
6
t (seconds)
Figure 10.21
Chaotic regime (a) phase portrait of Colpitts oscillator and (b) collector‚Äì
emitter voltage, showing typical results of applying MATLAB routine ode23 to the simula-
tion of the Colpitts oscillator circuit. Equation (10.154) speciÔ¨Åes the circuit parameters for
the results shown here.
These circuit parameters were used in Kennedy [9], and the phase portrait in
Fig. 10.21 is essentially that in Fig. 5 of that article [9]. The MATLAB code that
generates Fig. 10.21 appears in Appendix 10.B.
For the parameters in (10.154) the circuit simulation phase portrait in Fig. 10.21
is that of a strange attractor [11, 16], and so is strongly indicative (although not
conclusive) of chaotic dynamics in the circuit.
We note that under ‚Äúnormal‚Äù circumstances the Colpitts oscillator is intended to
generate sinusoidal waveforms, and so the chaotic regime traditionally represents
a failure mode, or abnormal operating condition for the circuit. However, Kennedy
[9] suggests that the chaotic mode of operation may be useful in chaos-based data
communications (e.g., chaotic‚Äìcarrier communications).
The following circuit parameters lead to approximately sinusoidal circuit out-
puts:
VTH = 0.75 V,
VCC = 5 V,
VEE = ‚àí5 V,
REE = 100 ,
RL = 200 ,
L = 100 √ó 10‚àí6 H,
Œ≤F = 80,
RON = 115 ,
C1 = 45 √ó 10‚àí9 F,
C2 = 58 √ó 10‚àí9 F.
(10.155)
TLFeBOOK

STIFF SYSTEMS
467
‚àí6
‚àí5
‚àí4
‚àí3
‚àí2
‚àí1
0
1
0
0.2
0.4
0.6
0.8
1
1.2
1.4
2.88
2.9
2.92
2.94
2.96
2.98
3
3.02
‚àí4
‚àí3.5
‚àí3
‚àí2.5
‚àí2
‚àí1.5
‚àí1
(a)
(b)
VBE (t) (volts)
VCE (t) (volts)
VCE (t) (volts)
√ó 10‚àí3
t (seconds)
Figure 10.22
Sinusoidal
operations
(a) phase
portrait
of
Colpitts
operator
and
(b) collector‚Äìemitter voltage, showing typical results of applying MATLAB routine ode23
to the simulation of the Colpitts oscillator circuit. Equation (10.155) speciÔ¨Åes the circuit
parameters for the results shown here.
Figure 10.22 shows the phase portrait for the oscillator using these parameter val-
ues. We see that vCE(t) is much more sinusoidal than in Fig. 10.21. The trajectory
in the phase portrait of Fig. 10.22 is tending to an elliptical closed curve indicative
of simple harmonic (i.e., sinusoidal) oscillation.
10.6
STIFF SYSTEMS
Consider the general system of coupled Ô¨Årst-order ODEs
dx0(t)
dt
=
f0(x0, x1, . . . , xm‚àí1, t),
dx1(t)
dt
=
f1(x0, x1, . . . , xm‚àí1, t),
...
dxm‚àí1(t)
dt
=
fm‚àí1(x0, x1, . . . , xm‚àí1, t),
(10.156)
TLFeBOOK

468
NUMERICAL SOLUTION OF ORDINARY DIFFERENTIAL EQUATIONS
which we wish to solve for t ‚â•0 given x(0), where x(t) = [x0(t) x1(t)
¬∑ ¬∑ ¬∑ xm‚àí1(t)]T . If we also deÔ¨Åne f (x(t), t) = [f0(x(t), t) f1(x(t), t) ¬∑ ¬∑ ¬∑ fm‚àí1
(x(t), t)]T , then we may express (10.156) in compact vector form as
dx(t)
dt
= f (x(t), t).
(10.157)
We have so far described a general order m ODE IVP.
If we now wish to consider the stability of a numerical method applied to the
solution of (10.156) [or (10.157)], then we need to consider the model problem
dx(t)
dt
= Ax(t),
(10.158)
where
A =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
‚àÇf0(x(0), 0)
‚àÇx0
‚àÇf0(x(0), 0)
‚àÇx1
¬∑ ¬∑ ¬∑
‚àÇf0(x(0), 0)
‚àÇxm‚àí1
‚àÇf1(x(0), 0)
‚àÇx0
‚àÇf1(x(0), 0)
‚àÇx1
¬∑ ¬∑ ¬∑
‚àÇf1(x(0), 0)
‚àÇxm‚àí1
...
...
...
‚àÇfm‚àí1(x(0), 0)
‚àÇx0
‚àÇfm‚àí1(x(0), 0)
‚àÇx1
¬∑ ¬∑ ¬∑
‚àÇfm‚àí1(x(0), 0)
‚àÇxm‚àí1
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
(10.159)
[which generalizes A ‚ààR2√ó2 in (10.94)]. The solution to (10.158) is given by
x(t) = eAtx(0)
(10.160)
[recall (10.92)].9 Ensuring the stability of the order m linear ODE system in
(10.158) requires all the eigenvalues Œªk of A to have negative real parts (i.e.,
Re[Œªk] < 0 for all k = 0, 1, . . . , m ‚àí1, where Œªk is the kth eigenvalue of A).
Of course, if m = 1, then with x(t) = x0(t), (10.158) reduces to
dx(t)
dt
= Œªx(t),
(10.161)
which is the model problem (10.28) again. Recall once again Example 10.5, for
which we found that
Œª = ‚àÇf (x0, t0)
‚àÇx
= ‚àí2
t0
,
9Note that if we know x(t0) (any t0 ‚ààR), then we may slightly generalize our linear problem (10.158)
to determining x(t) for all t ‚â•t0, in which case
x(t) = eA(t‚àít0)x(t0)
replaces (10.160). However, little is lost by assuming t0 = 0.
TLFeBOOK

MATLAB CODE FOR EXAMPLE 10.8
469
as (10.41) is the solution to dx/dt = t2 ‚àí2x/t for t ‚â•t0 > 0. If t0 is small, then
|Œª| is large, and we saw that numerical methods, especially low-order explicit
ones, had difÔ¨Åculty in estimating x(t) accurately when t was near t0. If we recall
(10.33) (which stated that h ‚â§2/|Œª|) as an example, we see that large negative
values for Œª force us to select small step sizes h to ensure stability of the explicit
Euler method. Since Œª is an eigenvalue of A = [Œª] in (10.161), we might expect
that this generalizes. In other words, a numerical method can be expected to have
accuracy problems if A in (10.159) has eigenvalues with large negative real parts.
In a situation like this x(t) in (10.156) has (it seems) a solution that changes so
rapidly for some time intervals (e.g., fast startup transients) that accurate numerical
solutions are hard to achieve. Such systems are called stiff systems.
So far our deÔ¨Ånition of a stiff system has not been at all rigorous. Indeed,
a rigorous deÔ¨Ånition is hard to come by. Higham and Trefethen [15] argue that
looking at the eigenvalues of A alone is not enough to decide on the stiffness of
(10.156) in a completely reliable manner. It is possible, for example, that A may
have favorable eigenvalues and yet (10.156) may still be stiff.
Stiff systems will not be discussed further here except to note that implicit
methods, or higher-order predictor‚Äìcorrector methods, should be used for their
solution. The paper by Higham and Trefethen [15] is highly recommended reading
for those readers seriously interested in the problems posed by stiff systems.
10.7
FINAL REMARKS
In the numerical solution (i.e., simulation) of ordinary differential equations (ODEs),
two issues are of primary importance: accuracy and stability. The successful sim-
ulation of any system requires proper attention to both of these issues.
Computational efÔ¨Åciency is also an issue. Generally, we prefer to use the largest
possible step size consistent with required accuracy, and as such to avoid any
instability in the simulation.
APPENDIX 10.A
MATLAB CODE FOR EXAMPLE 10.8
%
%
f23.m
%
% This defines function f(x,t) in the differential equation for Example 10.8
% (in Section 10.2).
%
function y = f23(x,t)
y = t*t - (2*x/t);
%
%
Runge.m
%
% This routine simulates the Heun‚Äôs, and 4th order Runge-Kutta methods as
TLFeBOOK

470
NUMERICAL SOLUTION OF ORDINARY DIFFERENTIAL EQUATIONS
% applied to the differential equation in Example 10.8 (Sect. 10.2), so this
% routine requires function f23.m.
It therefore generates Fig. 10.9.
%
function Runge
t0 = .05;
% initial time (starting time)
x0 = 1.0;
% initial condition (x(t0))
% Exact solution x(t)
c = (5*t0*t0*x0) - (t0^5);
te = [t0:.02:1.5];
for k = 1:length(te)
xe(k) = (te(k)*te(k)*te(k))/5 + c/(5*te(k)*te(k));
end;
h = .05;
% Heun‚Äôs method simulation
xh(1) = x0;
th(1) = t0;
for n = 1:25
fn = th(n)*th(n) - (2*xh(n)/th(n));
% f(x_n,t_n)
th(n+1) = th(n) + h;
xn1 = xh(n) + h*fn;
tn1 = th(n+1);
fn1 = tn1*tn1 - (2*xn1/tn1);
% f(x_{n+1},t_{n+1}) (approx.)
xh(n+1) = xh(n) + (h/2)*(fn + fn1);
end;
% 4th order Runge-Kutta simulation
xr(1) = x0;
tr(1) = t0;
for n = 1:25
t = tr(n);
x = xr(n);
k1 = f23(x,t);
k2 = f23(x + .5*h*k1,t + .5*h);
k3 = f23(x + .5*h*k2,t + .5*h);
k4 = f23(x + h*k3,t + h);
xr(n+1) = xr(n) + (h/6)*(k1 + 2*k2 + 2*k3 + k4);
tr(n+1) = tr(n) + h;
end;
plot(te,xe,‚Äô-‚Äô,tr,xr,‚Äô--o‚Äô,th,xh,‚Äô--+‚Äô), grid
legend(‚Äôx(t)‚Äô,‚Äô4th Order Runge-Kutta (h = .05)‚Äô,‚ÄôHeun (h = .05)‚Äô,1);
xlabel(‚Äô Time (t) ‚Äô)
ylabel(‚Äô Amplitude ‚Äô)
APPENDIX 10.B
MATLAB CODE FOR EXAMPLE 10.13
%
%
fR.m
%
TLFeBOOK

MATLAB CODE FOR EXAMPLE 10.13
471
% This is Equation (10.15) of Chapter 10 pertaining to Example 10.2.
%
function i = fR(v)
VTH = 0.75; % Threshold voltage in volts
RON = 100;
% On resistance of NPN BJT Q in Ohms
if v <= VTH
i = 0;
else
i = (v-VTH)/RON;
end;
%
%
vCC.m
%
% Supply voltage function v_CC(t) for Example 10.2 of Chapter 10.
% Here v_CC(t) = V_CC u(t) (i.e., oscillator switches on at t = 0).
%
function v = vCC(t)
VCC = 5;
if t < 0
v = 0;
else
v = VCC;
end;
%
%
Colpitts.m
%
% Computes the right-hand side of the state equations in Equation (10.17a,b,c)
% pertaining to Example 10.2 of Chapter 10.
%
function y = Colpitts(t,x)
C1 = 54e-9;
C2 = 54e-9;
REE = 400;
VEE = -5;
betaF = 200;
RL = 35;
L = 98.5e-6;
y(1) = ( x(3) - betaF*fR(x(2)) )/C1;
y(2) = ( -(x(2)+VEE)/REE - fR(x(2)) - x(3) )/C2;
y(3) = ( vCC(t) - x(1) + x(2) - RL*x(3))/L;
y = y.‚Äô;
%
%
SimulateColpitts.m
%
% This routine uses vCC.m, fR.m and Colpitts.m to simulate the Colpitts
% oscillator circuit of Example 10.2 in Chapter 10.
It produces
% Figure 10.21 in Chapter 10.
%
% The state vector x(:,:) is as follows:
%
x(:,1) = v_CE(t)
TLFeBOOK

472
NUMERICAL SOLUTION OF ORDINARY DIFFERENTIAL EQUATIONS
%
x(:,2) = v_BE(t)
%
x(:,3) = i_L(t)
%
function SimulateColpitts
[t,x] = ode23(@Colpitts, [ 0 0.003 ], [ 0 0 0 ]);
% [ 0 0.003 ] ---> Simulate 0 to 3 milliseconds
% [ 0 0 0 ]
---> Initial state vector
clf
L = length(t);
subplot(211), plot(x(:,1),x(:,2)), grid
xlabel(‚Äô v_{CE} (t) (volts) ‚Äô)
ylabel(‚Äô v_{BE} (t) (volts) ‚Äô)
title(‚Äô Phase Portrait of the Colpitts Oscillator (Chaotic Regime) ‚Äô)
subplot(212), plot(t(L-1999:L),x(L-1999:L,1),‚Äô-‚Äô), grid
xlabel(‚Äô t (seconds) ‚Äô)
ylabel(‚Äô v_{CE} (t) (volts) ‚Äô)
title(‚Äô Collector-Emitter Voltage (Chaotic Regime) ‚Äô)
REFERENCES
1. P. E. Hydon, Symmetry Methods for Differential Equations: A Beginner‚Äôs Guide, Cam-
bridge Univ. Press, Cambridge, UK, 2000.
2. C. T.-C. Nguyen and R. T. Howe, ‚ÄúAn Integrated CMOS Micromechanical Resonator
High-Q Oscillator,‚Äù IEEE J. Solid-State Circuits 34, 450‚Äì455 (April 1999).
3. E. Kreyszig, Introductory Functional Analysis with Applications, Wiley, New York,
1978.
4. E. Kreyszig, Advanced Engineering Mathematics, 4th ed., Wiley, New York, 1979.
5. L. M. Kells, Differential Equations: A Brief Course with Applications, McGraw-Hill,
New York, 1968.
6. E. Beltrami, Mathematics for Dynamic Modeling, Academic Press, Boston, MA, 1987.
7. S. S. Rao, Applied Numerical Methods for Engineers and Scientists, Prentice-Hall,
Upper Saddle River, NJ, 2002.
8. R. Bronson, Modern Introductory Differential Equations (Schaum‚Äôs Outline Series),
McGraw-Hill, New York, 1973.
9. M. P. Kennedy, ‚ÄúChaos in the Colpitts Oscillator,‚Äù IEEE Trans. Circuits Syst. (Part I:
Fundamental Theory and Applications) 41, 771‚Äì774 (Nov. 1994).
10. A. S. Sedra and K. C. Smith, Microelectronic Circuits, 3rd ed., Saunders College Publ.,
Philadelphia, PA, 1989.
11. J. Guckenheimer and P. Holmes, Nonlinear Oscillations, Dynamical Systems, and Bifur-
cations of Vector Fields, Springer-Verlag, New York, 1983.
12. J. F. Epperson, An Introduction to Numerical Methods and Analysis, Wiley, New York,
2002.
13. L. F. Shampine and M. W. Reichelt, ‚ÄúThe MATLAB ODE Suite,‚Äù SIAM J. Sci. Comput.
18, 1‚Äì22 (Jan. 1997).
TLFeBOOK

PROBLEMS
473
14. J. R. Dormand and P. J. Prince, ‚ÄúA Family of Embedded Runge-Kutta Formulae,‚Äù J.
Comput. Appl. Math. 6, 19‚Äì26 (1980).
15. D. J. Higham and L. N. Trefethen, ‚ÄúStiffness of ODEs,‚Äù BIT 33, 285‚Äì303 (1993).
16. P. G. Drazin, Nonlinear Systems, Cambridge Univ. Press, Cambridge, UK, 1992.
17. R. L. Burden and J. D. Faires, Numerical Analysis, 4th ed., PWS-KENT Publ., Boston,
MA, 1989.
PROBLEMS
10.1. Consider the electric circuit depicted in Fig. 10.P.1. Find matrix A ‚ààR2√ó2
and vector b ‚ààR2 such that
Ô£Æ
Ô£ØÔ£ØÔ£∞
diL1(t)
dt
diL2(t)
dt
Ô£π
Ô£∫Ô£∫Ô£ª= A

iL1(t)
iL2(t)
	
+ bvs(t),
where iLk(t) is the current through inductor Lk (k ‚àà{1, 2, 3}).
(Comment: Although the number of energy storage elements in the circuit is
3, there are only two state variables needed to describe the circuit dynamics.)
10.2. The circuit in Fig. 10.P.2 is a simpliÔ¨Åed model for a parametric ampliÔ¨Åer.
The ampliÔ¨Åer contains a reverse-biased varactor diode that is modeled by
the parallel interconnection of linear time-invariant capacitor C0 and linear
time-varying capacitor C(t). You may assume that C(t) = 2C1 cos(œâpt),
where C1 is constant, and œâp is the pumping frequency. Note that
iC(t) = d
dt [C(t)v(t)].
The input to the ampliÔ¨Åer is the ideal cosinusoidal current source is(t) =
2Is cos(œâ0t), and the load is the resistor R, so the output is the current i(t)
vs(t)
R1
R2
L2
L1
L3
iL1(t)
iL2(t)
iL3(t)
+
‚àí
Figure 10.P.1
The linear electric circuit for Problem 10.1.
TLFeBOOK

474
NUMERICAL SOLUTION OF ORDINARY DIFFERENTIAL EQUATIONS
L
C(t)
C0
v(t)
iC0(t)
iC(t)
Load
R
iL(t)
is(t)
i(t)
Varactor  diode  model
+
‚àí
Figure 10.P.2
A model for a parametric ampliÔ¨Åer (Problem 10.2).
into R. Write the state equations for the circuit, assuming that the state vari-
ables are v(t), and iL(t). Write these equations in matrix form. (Comment:
This problem is based on the example of a parametric ampliÔ¨Åer as consid-
ered in C. A. Desoer and E. S. Kuh, Basic Circuit Theory, McGraw-Hill,
New York, 1969.)
10.3. Give a detailed derivation of Eqs. (10.17).
10.4. The general linear Ô¨Årst-order ODE is
dx(t)
dt
= a(t)x(t) + b(t), x(t0) = x0.
Use the trapezoidal method to Ô¨Ånd an expression for xn+1 in terms of xn,
tn, and tn+1.
10.5. Prove that the trapezoidal method for ODEs is unconditionally stable.
10.6. Consider Theorem 10.2. Assume that ÀÜx0 = x(t0). Use (10.23) to Ô¨Ånd an
upper bound on |x(tn) ‚àíÀÜxn|/M for the following ODE IVPs:
(a) dx
dt = 1 ‚àí2x,
x(0) = 1.
(b) dx
dt = 2 cos x,
x(0) = 0.
10.7. Consider the ODE IVP
dx(t)
dt
= 1 ‚àí2x,
x(0) = 1.
(a) Approximate the solution to this problem using the explicit Euler
method with h = 0.1 for n = 0, 1, . . . , 10. Do the computations with a
pocket calculator.
(b) Find the exact solution x(t).
TLFeBOOK

PROBLEMS
475
10.8. Consider the ODE IVP
dx(t)
dt
= 2 cos x,
x(0) = 0.
(a) Approximate the solution to this problem using the explicit Euler
method with h = 0.1 for n = 0, 1, . . . , 10. Do the computations with a
pocket calculator.
(b) Find the exact solution x(t).
10.9. Write a MATLAB routine to simulate the ODE
dx
dt =
x
x + t
for the initial condition x(0) = 1. Use both the implicit and explicit Euler
methods. The program must accept as input the step size h, and the number
of iterations N that are desired. Parameters h and N are the same for both
methods. The program output is be written to a Ô¨Åle in the form of a table
something such as (e.g., for h = 0.05, and N = 5) the following:
time step
explicit x_n
implicit x_n
.0000
value
value
.0500
value
value
.1000
value
value
.1500
value
value
.2000
value
value
.2500
value
value
Test your program out on
h = 0.01,
N = 100
and
h = 0.10,
N = 10.
10.10. Consider
dx(t)
dt
= Œ±(1 ‚àí2Œ≤t2)e‚àíŒ≤t2.
(10.P.1)
(a) Verify that for t ‚â•0, with x(0) = 0, we have the solution
x(t) = Œ±te‚àíŒ≤t2.
(10.P.2)
(b) For what range of step sizes h is the explicit Euler method a stable
means of solving (10.P.1)?
TLFeBOOK

476
NUMERICAL SOLUTION OF ORDINARY DIFFERENTIAL EQUATIONS
(c) Write a MATLAB routine to simulate (10.P.1) for x(0) = 0 using both
the explicit and implicit Euler methods. Assume that Œ± = 10 and Œ≤ = 1.
Test your program out on
h = 0.01,
N = 400
(10.P.3a)
and
h = 0.10,
N = 40.
(10.P.3b)
The program must produce plots of {xn|n = 0, 1, . . . , N} (for both
explicit and implicit methods), and x(t) (from (10.P.2)) on the same
graph. This will lead to two separate plots, one for each of (10.P.3a)
and (10.P.3b).
10.11. Consider
dx(t)
dt
= 2tx + 1,
x(0) = 0.
(a) Find {xn|n = 0, 1, . . . , 10} for h = 0.1 using the explicit Euler method.
Do the calculations with a pocket calculator.
(b) Verify that
x(t) = et2  t
0
e‚àís2 ds.
(c) Does the stability condition (10.33) apply here? Explain.
10.12. Prove that Runge‚ÄìKutta methods of order one have no degrees of freedom
(i.e., we are forced to select c1 in only one possible way).
10.13. A fourth-order Runge-Kutta method is
xn+1 = xn + 1
6h[k1 + 2k2 + 2k3 + k4]
for which
k1 = f (xn, tn),
k2 = f

xn + 1
2hk1, tn + 1
2h

,
k3 = f

xn + 1
2hk2, tn + 1
2h

, k4 = f (xn + hk3, tn + h).
When applied to the model problem, we get xn = œÉ nx0 for n ‚ààZ+. Derive
the expression for œÉ.
10.14. A third-order Runge‚ÄìKutta method is
xn+1 = xn + 1
6[k1 + 4k2 + k3]
TLFeBOOK

PROBLEMS
477
for which
k1 = hf (xn, tn),
k2 = hf (xn + 1
2h, tn + 1
2k1),
k3 = hf (xn + h, tn ‚àík1 + 2k2).
(a) When applied to the model problem, we get xn = œÉ nx0 for n ‚ààZ+.
Derive the expression for œÉ.
(b) Find the allowable range of step sizes h that ensure stability of the
method.
10.15. Consider the fourth-order Runge‚ÄìKutta method in Eqs. (10.80) and (10.81).
Show that if f (x, t) = f (t), then the method reduces to Simpson‚Äôs rule for
numerical integration over the interval [tn, tn+1].
10.16. Recall Eq. (10.98). Suppose that A ‚ààR2√ó2 has distinct eigenvalues Œ≥k such
that Re[Œ≥k] > 0 for at least one of the eigenvalues. Show that I + hA will
have at least one eigenvalue Œªk such that |Œªk| > 1.
10.17. Consider the coupled Ô¨Årst-order ODEs
dx
dt = ‚àíy
7
x2 + y2,
(10.P.4a)
dy
dt = x
7
x2 + y2,
(10.P.4b)
where (x0, y0) = (x(0), y(0)) are the initial conditions.
(a) Prove that for suitable constants r0 and Œ∏0, we have
x(t) = r0 cos(r0t + Œ∏0),
y(t) = r0 sin(r0t + Œ∏0).
(10.P.5)
(b) Write a MATLAB routine to simulate the system represented by (10.P.4)
using the explicit Euler method [which will produce xn = [xn
yn]T
such that xn ‚âàx(tn) and yn ‚âày(tn)]. Assume that h = 0.05 and the
initial condition is x0 = [1
0]T . Plot xn and (x(t), y(t)) (via (10.P.5))
on the (x, y) plane.
(c) Write a MATLAB routine to simulate the system represented by (10.P.4)
using Heun‚Äôs method [which will produce xn = [xn
yn]T such that
xn ‚âàx(tn) and yn ‚âày(tn)]. Assume that h = 0.05 and the initial con-
dition is x0 = [1
0]T . Plot xn, and (x(t), y(t)) [via (10.P.5)] on the
(x, y) plane.
Make reasonable choices about the number of time steps in the simulation.
10.18. In the previous problem the step size is h = 0.05.
(a) Determine whether the simulation using the explicit Euler method is
stable for this choice of step size. (Hint: Recall that one must consider
the eigenvalues of I + hA.)
TLFeBOOK

478
NUMERICAL SOLUTION OF ORDINARY DIFFERENTIAL EQUATIONS
(b) Determine whether the simulation using Heun‚Äôs method is stable for
this choice of step size. [Hint: Consider the implications of (10.121).]
Use the MATLAB eig function to assist you in your calculations.
10.19. A curve in R2 is speciÔ¨Åed parametrically according to
x(t) = A cos(œât),
y(t) = Œ±A cos(œât ‚àíœÜ),
(10.P.6)
where Œ±, A > 0, and t ‚ààR is the ‚Äúparameter.‚Äù When the points (x(t), y(t))
are plotted, the result is what electrical engineers often call a Lissajous Ô¨Ågure
(or curve), which is really just an alternative name for a phase portrait.
(a) Find an implicit function expression for the curve, that is, Ô¨Ånd a descrip-
tion of the form
f (x, y) = 0
(10.P.7)
[i.e., via algebra and trigonometry eliminate t from (10.9.6) to obtain
(10.9.7)].
(b) On the (x, y) plane sketch the Lissajous Ô¨Ågures for cases œÜ = 0, œÜ =
¬± œÄ
2 and œÜ = ¬± œÄ
4 .
(c) An interpretation of (10.9.6) is that x(t) may be the input voltage from
a source in an electric circuit, while y(t) may be the output voltage
drop across a load element in the circuit (in the steady-state condition
of course). Find a simple expression for sin œÜ in terms of B such that
f (0, B) = 0 [i.e., point(s) B on the y axis where the curve cuts the
y axis], and in terms of Œ± and A. (Comment: On analog oscilloscopes
of olden days, it was possible to display a Lissajous Ô¨Ågure, and so use
this Ô¨Ågure to estimate the phase angle œÜ on the lab bench.)
10.20. Verify the values for Œªk in Table 10.1 for m = 2.
10.21. Verify the values for Œªk in Table 10.2 for m = 2.
10.22. For the ODE IVP
dx(t)
dt
= f (x, t),
x(t0) = x0,
write pseudocode for a numerical method that approximates the solution to
it using an AB method for m = 2 as a predictor, an AM method for m = 2
as a corrector, and the third-order Runge‚ÄìKutta (RK) method from Problem
10.14 to perform the initialization.
10.23. From Forsythe, Malcolm, and Moler (see Ref. 5 in Chapter 2)
dx
dt = 998x + 1998y
(10.P.8a)
dy
dt = ‚àí999x ‚àí1999y
(10.P.8b)
TLFeBOOK

PROBLEMS
479
has the solution
x(t) = 4e‚àít ‚àí3e‚àí1000t,
(10.P.9a)
y(t) = ‚àí2e‚àít + 3e‚àí1000t,
(10.P.9b)
where x(0) = y(0) = 1. Recall A as given by (10.94).
(a) By direct substitution verify that (10.P.9) is the solution to (10.P.8).
(b) If the eigenvalues of I + hA are Œª0 and Œª1, plot |Œª0| and |Œª1| versus h
(using MATLAB).
(c) If the eigenvalues of I + hA + 1
2h2A2 [recall (10.121)] are Œª0 and Œª1,
plot |Œª0| and |Œª1| versus h (using MATLAB).
In parts (b) and (c), what can be said about the range of values for h leading
to a stable simulation of the system (10.P.8)?
10.24. Consider the coupled system of Ô¨Årst-order ODEs
dx(t)
dt
= Ax(t) + y(t),
(10.P.10)
where A ‚ààRn√ón and x(t), y(t) ‚ààRn for all t ‚ààR+. Suppose that the eigen-
values of A are Œª0, Œª1, . . . , Œªn‚àí1 such that Re[Œªk] < 0 for all k ‚ààZn.
Suppose that
œÉ ‚â§Re[Œªk] ‚â§œÑ < 0
for all k. A stiffness quotient is deÔ¨Åned to be
r = œÉ
œÑ .
The system (10.P.10) is said to be stiff if r ‚â´1 (again, we assume Re[Œªk] <
0 for all k).
For the previous problem, does A correspond to a stiff system? (Comment:
As Higham and Trefethen [15] warned, the present stiffness deÔ¨Ånition is
not entirely reliable.)
TLFeBOOK

11
Numerical Methods
for Eigenproblems
11.1
INTRODUCTION
In previous chapters we have seen that eigenvalues and eigenvectors are important
(e.g., recall condition numbers from Chapter 4, and the stability analysis of numer-
ical methods for ODEs in Chapter 10). In this chapter we treat the eigenproblem
somewhat more formally than previously. We shall deÔ¨Åne and review the basic
problem in Section 11.2, and in Section 11.3 we shall apply this understanding to
the problem of computing the matrix exponential [i.e., exp(At), where A ‚ààRn√ón
and t ‚ààR] since this is of central importance in many areas of electrical and com-
puter engineering (signal processing, stability of dynamic systems, control systems,
circuit simulation, etc.). In subsequent sections we will consider numerical methods
to determine the eigenvalues and eigenvectors of matrices.
11.2
REVIEW OF EIGENVALUES AND EIGENVECTORS
In this section we review some basic facts relating to the determination of eigen-
values and eigenvectors of matrices. Our emphasis, with a few exceptions, is on
matrices that are diagonalizable.
DeÔ¨Ånition 11.1: Eigenproblem
Let A ‚ààCn√ón. The eigenproblem for A is
to Ô¨Ånd solutions to the matrix equation
Ax = Œªx,
(11.1)
where Œª ‚ààC and x ‚ààCn such that x Ã∏= 0. A solution (Œª, x) to (11.1) is called an
eigenpair, Œª is an eigenvalue, and x is its corresponding eigenvector.
Even if A ‚ààRn√ón (the situation we will emphasize most) it is very possible to
have Œª ‚ààC and x ‚ààCn. We must also emphasize that x = 0 is never permitted to
be an eigenvector for A.
We may rewrite (11.1) as (I is the n √ó n identity matrix)
(A ‚àíŒªI)x = 0,
(11.2a)
An Introduction to Numerical Analysis for Electrical and Computer Engineers, by C.J. Zarowski
ISBN 0-471-46737-5
c‚Éù2004 John Wiley & Sons, Inc.
480
TLFeBOOK

REVIEW OF EIGENVALUES AND EIGENVECTORS
481
or equivalently as
(ŒªI ‚àíA)x = 0.
(11.2b)
Equations (11.2) are homogeneous linear systems of n equations in n unknowns.
Since x = 0 is never an eigenvector, an eigenvector x must be a nontrivial solution
to (11.2). An n √ó n homogeneous linear system has a nonzero (i.e., nontrivial)
solution iff the coefÔ¨Åcient matrix is singular. Immediately, eigenvalue Œª satisÔ¨Åes
det(A ‚àíŒªI) = 0,
(11.3a)
or equivalently
det(ŒªI ‚àíA) = 0.
(11.3b)
Of course, p(Œª) = det(ŒªI ‚àíA) is a polynomial of degree n. In principle, we
may Ô¨Ånd eigenpairs by Ô¨Ånding p(Œª) (the characteristic polynomial of A), then
Ô¨Ånding the zeros of p(Œª), and then substituting these into (11.2) to Ô¨Ånd x. In
practice this approach really works only for small analytical examples. Properly
conceived numerical methods are needed to determine eigenpairs reliably for larger
matrices A.
To set the stage for what follows, consider the following examples.
Example 11.1
From Hill [1], consider the example
A =
Ô£Æ
Ô£∞
1
0
0
2
1
0
0
0
3
Ô£π
Ô£ª.
Here p(Œª) = det(ŒªI ‚àíA) = (Œª ‚àí1)2(Œª ‚àí3) so A has a double eigenvalue (eigen-
value of multiplicity 2) at Œª = 1, and a simple eigenvalue at Œª = 3. The eigenvalues
may be individually denoted as Œª0 = 1, Œª1 = 1, and Œª2 = 3.
For Œª = 3, (3I ‚àíA)x = 0 becomes
Ô£Æ
Ô£∞
2
0
0
‚àí2
2
0
0
0
0
Ô£π
Ô£ª
Ô£Æ
Ô£∞
x0
x1
x2
Ô£π
Ô£ª=
Ô£Æ
Ô£∞
0
0
0
Ô£π
Ô£ª,
and from the application of elementary row operations this reduces to
Ô£Æ
Ô£∞
1
0
0
0
1
0
0
0
0
Ô£π
Ô£ª
Ô£Æ
Ô£∞
x0
x1
x2
Ô£π
Ô£ª=
Ô£Æ
Ô£∞
0
0
0
Ô£π
Ô£ª.
Immediately, x0 = x1 = 0, and x2 is arbitrary (except, of course, it is not allowed
to be zero) so that the general form of the eigenvector corresponding to Œª = Œª2 is
v(2) = [ 0
0
x2 ]T ‚ààC3.
TLFeBOOK

482
NUMERICAL METHODS FOR EIGENPROBLEMS
On the other hand, now let us consider Œª = Œª0 = Œª1. In this case (I ‚àíA)x = 0
becomes
Ô£Æ
Ô£∞
0
0
0
‚àí2
0
0
0
0
‚àí2
Ô£π
Ô£ª
Ô£Æ
Ô£∞
x0
x1
x2
Ô£π
Ô£ª=
Ô£Æ
Ô£∞
0
0
0
Ô£π
Ô£ª,
which reduces to
Ô£Æ
Ô£∞
1
0
0
0
0
1
0
0
0
Ô£π
Ô£ª
Ô£Æ
Ô£∞
x0
x1
x2
Ô£π
Ô£ª=
Ô£Æ
Ô£∞
0
0
0
Ô£π
Ô£ª.
Immediately, x0 = x2 = 0, and x1 is arbitrary, so an eigenvector corresponding to
Œª = Œª0 = Œª1 is of the general form
v(0) = [ 0
x1
0 ]T ‚ààC3.
Even though Œª = 1 is a double eigenvalue, we are able to Ô¨Ånd only one eigenvector
for this case. In effect, one eigenvector seems to be ‚Äúmissing.‚Äù
Example 11.2
Now consider (again from Hill [1])
A =
Ô£Æ
Ô£∞
0
‚àí2
1
1
3
‚àí1
0
0
1
Ô£π
Ô£ª.
Here p(Œª) = det(ŒªI ‚àíA) = (Œª ‚àí1)2(Œª ‚àí2). The eigenvalues of A are thus Œª0 =
1, Œª1 = 1, and Œª2 = 2.
For Œª = 2, (2I ‚àíA)x = 0 becomes
Ô£Æ
Ô£∞
2
2
‚àí1
‚àí1
‚àí1
1
0
0
1
Ô£π
Ô£ª
Ô£Æ
Ô£∞
x0
x1
x2
Ô£π
Ô£ª=
Ô£Æ
Ô£∞
0
0
0
Ô£π
Ô£ª,
which reduces to
Ô£Æ
Ô£∞
1
1
0
0
0
1
0
0
0
Ô£π
Ô£ª
Ô£Æ
Ô£∞
x0
x1
x2
Ô£π
Ô£ª=
Ô£Æ
Ô£∞
0
0
0
Ô£π
Ô£ª,
so the general form of the eigenvector for Œª = Œª2 is
v(2) = x(2) = [ ‚àíx0
x0
0 ]T ‚ààC3.
Now, if we consider Œª = Œª0 = Œª1, (I ‚àíA)x = 0 becomes
Ô£Æ
Ô£∞
1
2
‚àí1
‚àí1
‚àí2
1
0
0
0
Ô£π
Ô£ª
Ô£Æ
Ô£∞
x0
x1
x2
Ô£π
Ô£ª=
Ô£Æ
Ô£∞
0
0
0
Ô£π
Ô£ª,
TLFeBOOK

REVIEW OF EIGENVALUES AND EIGENVECTORS
483
which reduces to
Ô£Æ
Ô£∞
1
2
‚àí1
0
0
0
0
0
0
Ô£π
Ô£ª
Ô£Æ
Ô£∞
x0
x1
x2
Ô£π
Ô£ª=
Ô£Æ
Ô£∞
0
0
0
Ô£π
Ô£ª.
Since x0 + 2x1 ‚àíx2 = 0, we may choose any two of x0, x1, or x2 as free para-
meters giving a general form of an eigenvector for Œª = Œª0 = Œª1 as
v(0) = x0
Ô£Æ
Ô£∞
1
0
1
Ô£π
Ô£ª
#
$%
&
=x(0)
+ x1
Ô£Æ
Ô£∞
0
1
2
Ô£π
Ô£ª
#
$%
&
=x(1)
‚ààC3.
Thus, we have eigenpairs (Œª0, x(0)), (Œª1, x(1)), (Œª2, x(2)).
We continue to emphasize that in all cases any free parameters are arbitrary,
except that they must never be selected to give a zero-valued eigenvector. In
Example 11.2 Œª = 1 is an eigenvalue of multiplicity 2, and v(0) is a vector in
a two-dimensional vector subspace of C3. On the other hand, in Example 11.1
Œª = 1 is also of multiplicity 2, and yet v(0) is only a vector in a one-dimensional
vector subspace of C3.
DeÔ¨Ånition 11.2: Defective Matrix
For any A ‚ààCn√ón, if the multiplicity
of any eigenvalue Œª ‚ààC is not equal to the dimension of the solution space
(eigenspace) of (ŒªI ‚àíA)x = 0, then A is defective.
From this deÔ¨Ånition, A in Example 11.1 is a defective matrix, while A in
Example 11.2 is not defective (i.e., is nondefective). In a sense soon to be made
precise, defective matrices cannot be diagonalized. However, all matrices, diago-
nalizable or not, can be placed into Jordan canonical form, as follows.
Theorem 11.1: Jordan Decomposition
If A ‚ààCn√ón, then there exists a non-
singular matrix T ‚ààCn√ón such that
T ‚àí1AT = diag(J0, J1, . . . , Jk‚àí1),
where
Ji =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
Œªi
1
0
¬∑ ¬∑ ¬∑
0
0
0
Œªi
1
¬∑ ¬∑ ¬∑
0
0
...
...
...
...
...
0
0
0
¬∑ ¬∑ ¬∑
Œªi
1
0
0
0
¬∑ ¬∑ ¬∑
0
Œªi
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
‚ààCmi√ómi,
and k‚àí1
i=0 mi = n.
TLFeBOOK

484
NUMERICAL METHODS FOR EIGENPROBLEMS
Proof
See Halmos [2] or Horn and Johnson [3].
Of course, Œªi in the theorem is an eigenvalue of A. The submatrices Ji are called
Jordan blocks. The number of blocks k and their dimensions mi are unique, but
their order is not unique. Note that if an eigenvalue has a multiplicity of unity (i.e.,
if it is simple), then the Jordan block in this case is the 1 √ó 1 matrix consisting
of that eigenvalue. From the theorem statement the characteristic polynomial of
A ‚ààCn√ón is given by
p(Œª) = det(ŒªI ‚àíA) =
k‚àí1
/
i=0
(Œª ‚àíŒªi)mi.
(11.4)
But if A ‚ààRn√ón, and if Œªi ‚ààC for some i, then Œª‚àó
i must also be an eigenvalue of
A, that is, a zero of p(Œª). This follows from the fundamental theorem of algebra,
which states that complex-valued roots of polynomials with real-valued coefÔ¨Åcients
must always occur in complex‚Äìconjugate pairs.
Example 11.3
Consider (with Œ∏ Ã∏= kœÄ, k ‚ààZ)
A =
 cos Œ∏
‚àísin Œ∏
sin Œ∏
cos Œ∏

‚ààR2√ó2
(2 √ó 2 rotation operator from Appendix 3.A). We have the characteristic equation
p(Œª) = det(ŒªI ‚àíA) = det
'
Œª ‚àícos Œ∏
sin Œ∏
‚àísin Œ∏
Œª ‚àícos Œ∏
	(
= Œª2 ‚àí2 cos Œ∏Œª + 1 = 0
for which the roots (eigenvalues of A) are therefore Œª = e¬±jŒ∏. DeÔ¨Åne Œª0 = ejŒ∏,
Œª1 = e‚àíjŒ∏. Clearly Œª1 = Œª‚àó
0 (i.e., the two simple eigenvalues of A are a conju-
gate pair).
For Œª = Œª0, (Œª0I ‚àíA)x = 0 is
sin Œ∏

j
1
‚àí1
j
	  x0
x1

=
 0
0

,
which reduces to

1
‚àíj
0
0
	  x0
x1

=
 0
0

.
The eigenvector for Œª = Œª0 is therefore of the form
x(0) = a

1
‚àíj

, a ‚ààC.
TLFeBOOK

REVIEW OF EIGENVALUES AND EIGENVECTORS
485
Similarly, for Œª = Œª1, the homogeneous linear system (Œª1I ‚àíA)x = 0 is
sin Œ∏
 ‚àíj
1
‚àí1
‚àíj
  x0
x1

=
 0
0

,
which reduces to
 1
j
0
0
  x0
x1

=
 0
0

.
The eigenvector for Œª = Œª1 is therefore of the form
x(1) = b
 1
j

,
b ‚ààC.
Of course, free parameters a, and b are never allowed to be zero.
Computing the Jordan canonical form when mi > 1 is numerically rather dif-
Ô¨Åcult (as noted in Golub and Van Loan [4] and Horn and Johnson [3]), and so
is often avoided. However, there are important exceptions often involving state-
variable (state-space) systems analysis and design (e.g., see Fairman [5]). Within
the theory of Jordan forms it is possible to Ô¨Ånd supposedly ‚Äúmissing‚Äù eigenvectors,
resulting in a theory of generalized eigenvectors. We will not consider this here as
it is rather involved. Some of the references at the end of this chapter cover the
relevant theory [3].
We will now consider a series of theorems leading to a sufÔ¨Åcient condition for
A to be nondefective. Our presentation largely follows Hill [1].
Theorem 11.2: If A ‚ààCn√ón, then the eigenvectors corresponding to two dis-
tinct eigenvalues of A are linearly independent.
Proof
We employ proof by contradiction.
Suppose that (Œ±, x) and (Œ≤, y) are two eigenpairs for A, and Œ± Ã∏= Œ≤. Assume
y = ax for some a Ã∏= 0 (a ‚ààC). Thus
Œ≤y = Ay = aAx = aŒ±x
and also
Œ≤y = aŒ≤x.
Hence
aŒ≤x = aŒ±x,
implying that
a(Œ≤ ‚àíŒ±)x = 0.
TLFeBOOK

486
NUMERICAL METHODS FOR EIGENPROBLEMS
But x Ã∏= 0 as it is an eigenvector of A, and also a Ã∏= 0. Immediately, Œ± = Œ≤,
contradicting our assumption that these eigenvalues are distinct. Thus, y = ax is
impossible; that is, we have proved that y is independent of x.
Theorem 11.2 leads us to the next theorem.
Theorem 11.3: If A ‚ààCn√ón has n distinct eigenvalues, then A has n linearly
independent eigenvectors.
Proof
Uses mathematical induction (e.g., Stewart [6]).
We have already seen the following theorem.
Theorem 11.4: If A ‚ààRn√ón and A = AT , then all eigenvalues of A are real-
valued.
Proof
See Hill [1], or see the appropriate footnote in Chapter 4.
In addition to this theorem, we also have the following one.
Theorem 11.5: If A ‚ààRn√ón, and if A = AT , then eigenvectors corresponding
to distinct eigenvalues of A are orthogonal.
Proof
Suppose that (Œ±, x) and (Œ≤, y) are eigenpairs of A with Œ± Ã∏= Œ≤. We wish
to show that xT y = yT x = 0 (recall DeÔ¨Ånition 1.6). Now
Œ±x = Ax = AT x
so that
Œ±yT x = yT AT x = (Ay)T x = Œ≤yT x,
implying that
(Œ± ‚àíŒ≤)yT x = 0.
But Œ± Ã∏= Œ≤ so that yT x = 0, that is, x ‚ä•y.
Theorem 11.5 states that eigenspaces corresponding to distinct eigenvalues of a
symmetric, real-valued matrix form mutually orthogonal vector subspaces of Rn.
Any vector from one eigenspace must therefore be orthogonal to any eigenvector
from another eigenspace. If we recall DeÔ¨Ånition 11.2, it is apparent that all sym-
metric, real-valued matrices are nondefective, if their eigenvalues are all distinct.1
In fact, even if A ‚ààCn√ón and is not symmetric, then, as long as the eigenvalues
are distinct, A will be nondefective (Theorem 11.3).
1It is possible to go even further and prove that any real-valued, symmetric matrix is nondefective,
even if it possesses multiple eigenvalues. Thus, any real-valued, symmetric matrix is diagonalizable.
TLFeBOOK

REVIEW OF EIGENVALUES AND EIGENVECTORS
487
DeÔ¨Ånition 11.3: Similarity Transformation
If A, B ‚ààCn√ón, and there is a
nonsingular matrix P ‚ààCn√ón such that
B = P ‚àí1AP,
we say that B is similar to A, and that P is a similarity transformation.
If A ‚ààCn√ón, and A has n distinct eigenvalues forming n distinct eigenpairs
{(Œªk, x(k))|k = 0, 1, . . . , n ‚àí1}, then
Ax(k) = Œªkx(k)
allows us to write
A [x(0)x(1) ¬∑ ¬∑ ¬∑ x(n‚àí1)]
#
$%
&
=P
= [x(0)x(1) ¬∑ ¬∑ ¬∑ x(n‚àí1)]
#
$%
&
=P
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
Œª0
0
¬∑ ¬∑ ¬∑
0
0
0
Œª1
¬∑ ¬∑ ¬∑
0
0
...
...
...
...
0
0
¬∑ ¬∑ ¬∑
0
Œªn‚àí1
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª
#
$%
&
=
,
(11.5)
that is, AP = P, where  = diag(Œª0, Œª1, . . . , Œªn‚àí1) ‚ààCn√ón is the diagonal matrix
of eigenvalues of A. Thus
P ‚àí1AP = ,
(11.6)
and the matrix of eigenvectors P ‚ààCn√ón of A deÔ¨Ånes the similarity transformation
that diagonalizes matrix A. More generally, we have the following theorem.
Theorem 11.6: If A, B ‚ààCn√ón, and A and B are similar matrices, then A and
B have the same eigenvalues.
Proof
Since A and B are similar, there exists a nonsingular matrix P ‚ààCn√ón
such that B = P ‚àí1AP. Therefore
det(ŒªI ‚àíB) = det(ŒªI ‚àíP ‚àí1AP)
= det(P ‚àí1(PP‚àí1Œª ‚àíA)P )
= det(P ‚àí1) det(ŒªI ‚àíA) det(P )
= det(ŒªI ‚àíA).
Thus, A and B possess the same characteristic polynomial, and so possess identical
eigenvalues.
In other words, similarity transformations preserve eigenvalues.2 Note that The-
orem 11.6 holds regardless of whether A and B are defective. In developing (11.6)
2This makes such transformations highly valuable in state-space control systems design, in addition to
a number of other application areas.
TLFeBOOK

488
NUMERICAL METHODS FOR EIGENPROBLEMS
we have seen that if A ‚ààCn√ón has n distinct eigenvalues, it is diagonalizable. We
emphasize that this is only a sufÔ¨Åcient condition. Example 11.2 conÔ¨Årms that a
matrix can have eigenvalues with multiplicity greater than one, and yet still be
diagonalizable.
11.3
THE MATRIX EXPONENTIAL
In Chapter 10 the problem of computing eAt (A ‚ààRn√ón, and t ‚ààR) was associated
with the stability analysis of numerical methods for systems of ODEs. It is also
noteworthy that to solve
dx(t)
dt
= Ax(t) + by(t)
(11.7)
[x(t) = [x0(t)x1(t) ¬∑ ¬∑ ¬∑ xn‚àí1(t)]T ‚ààRn, A ‚ààRn√ón, and b ‚ààRn with y(t) ‚ààR for
all t] required us to compute eAt [recall Example 10.1, which involved an example
of (11.7) from electric circuit analysis; see Eq. (10.9)]. Thus, we see that computing
the matrix exponential is an important problem in analysis. In this section we shall
gain more familiarity with the matrix exponential because of its signiÔ¨Åcance.
Moler and Van Loan [7] caution that computing the matrix exponential is a
numerically difÔ¨Åcult problem. Stable, reliable, accurate, and computationally efÔ¨Å-
cient algorithms are not so easy to come by. Their paper [7], as its title states,
considers 19 methods, and none of them are entirely satisfactory. Indeed, this
paper [7] appeared in 1978, and to this day the problem of successfully computing
eAt for any A ‚ààCn√ón has not been fully resolved. We shall say something about
why this is a difÔ¨Åcult problem later.
Before considering this matter, we shall consider a general analytic (i.e., hand
calculation) method for obtaining eAt for any A ‚ààRn√ón, including when A is
defective. In principle, this would involve working with Jordan decompositions
and generalized eigenvectors, but we will avoid this by adopting the approach
suggested in Leonard [8].
The matrix exponential eAt may be deÔ¨Åned in the expected manner as
(t) = eAt =
‚àû

k=0
1
k!Aktk,
(11.8)
so, for example, the kth derivative of the matrix exponential is
(k)(t) = AkeAt = eAtAk
(11.9)
for k ‚ààZ+ ((0)(t) = (t)). To see how this works, consider the following special
case k = 1:
(1)(t) = d
dt

 ‚àû

k=0
1
k!Aktk

= d
dt

I + 1
1!At + 1
2!A2t2 + ¬∑ ¬∑ ¬∑ + 1
k!Aktk + ¬∑ ¬∑ ¬∑

TLFeBOOK

THE MATRIX EXPONENTIAL
489
= 1
1!A + 2
2!A2t + ¬∑ ¬∑ ¬∑ + k
k!Aktk‚àí1 + ¬∑ ¬∑ ¬∑
= A

I + 1
1!At + ¬∑ ¬∑ ¬∑ +
1
(k ‚àí1)!Ak‚àí1tk‚àí1 + ¬∑ ¬∑ ¬∑

= AeAt = eAtA.
It is possible to formally verify that the series in (11.8) converges to a matrix
function of t ‚ààR by working with the Jordan decomposition of A. However,
we will avoid this level of detail. But we will consider the situation where A
is diagonalizable later on.
There is some additional background material needed to more fully appreciate
[8], and we will now consider this. The main result is the Cayley‚ÄìHamilton theorem
(Theorem 11.8, below).
DeÔ¨Ånition 11.4: Minors and Cofactors
Let A ‚ààCn√ón. The minor mij is the
determinant of the (n ‚àí1) √ó (n ‚àí1) submatrix of A derived from it by deleting
row i and column j. The cofactor cij associated with mij is cij = (‚àí1)i+jmij for
all i, j ‚ààZn.
A formula for the inverse of A (assuming this exists) is given by Theorem 11.7.
Theorem 11.7: If A ‚ààCn√ón is nonsingular, then
A‚àí1 =
1
det(A)adj(A),
(11.10)
where adj(A) (adjoint matrix of A) is the transpose of the matrix of cofactors of
A. Thus, if C = [cij] ‚ààCn√ón is the matrix of cofactors of A, then adj(A) = CT .
Proof
See Noble and Daniel [9].
Of course, the method suggested by Theorem 11.7 is useful only for the hand
calculation of low-order (small n) problems. Practical matrix inversion must use
ideas from Chapter 4. But Theorem 11.7 is a very useful result for theoretical
purposes, such as obtaining the following theorem.
Theorem 11.8: Cayley‚ÄìHamilton Theorem
Any matrix A ‚ààCn√ón satisÔ¨Åes
its own characteristic equation.
Proof
The characteristic polynomial for A is p(Œª) = det(ŒªI ‚àíA), and can be
written as
p(Œª) = Œªn + a1Œªn‚àí1 + ¬∑ ¬∑ ¬∑ + an‚àí1Œª + an
for suitable constants ak ‚ààC. The theorem claims that
An + a1An‚àí1 + ¬∑ ¬∑ ¬∑ + an‚àí1A + anI = 0,
(11.11)
TLFeBOOK

490
NUMERICAL METHODS FOR EIGENPROBLEMS
where I is the order n identity matrix. To show (11.11), we consider adj(¬µI ‚àíA)
whose elements are polynomials in ¬µ of a degree that is not greater than n ‚àí1,
where ¬µ is not an eigenvalue of A. Hence
adj(¬µI ‚àíA) = M0¬µn‚àí1 + M1¬µn‚àí2 + ¬∑ ¬∑ ¬∑ + Mn‚àí2¬µ + Mn‚àí1
for suitable constant matrices Mk ‚ààCn√ón. Via Theorem 11.7
(¬µI ‚àíA) adj(¬µI ‚àíA) = det(¬µI ‚àíA)I,
or in expanded form, this becomes
(¬µI ‚àíA)(M0¬µn‚àí1 + M1¬µn‚àí2 + ¬∑ ¬∑ ¬∑ + Mn‚àí2¬µ + Mn‚àí1)
= (¬µn + a1¬µn‚àí1 + ¬∑ ¬∑ ¬∑ + an)I.
If we now equate like powers of ¬µ on both sides of this equation, we obtain
M0 = I,
M1 ‚àíAM0 = a1I,
M2 ‚àíAM1 = a2I,
...
(11.12)
Mn‚àí1 ‚àíAMn‚àí2 = an‚àí1I,
‚àíAMn‚àí1 = anI.
Premultiplying3 the jth equation in (11.12) by An‚àíj (j = 0, 1, . . . , n), and then
adding all the equations that result from this, yields
AnM0 + An‚àí1(M1 ‚àíAM0) + An‚àí2(M2 ‚àíAM1) + ¬∑ ¬∑ ¬∑ + A(Mn‚àí1 ‚àíAMn‚àí2)
‚àíAMn‚àí1 = An + a1An‚àí1 + a2An‚àí2 + ¬∑ ¬∑ ¬∑ + an‚àí1An‚àí1 + anI.
But the left-hand side of this is seen to be zero because of cancellation of all the
terms, and (11.11) immediately results.
As an exercise the reader should verify that the matrices in Examples 11.1‚Äì11.3,
all satisfy their own characteristic equations.
We will now consider the approach in Leonard [8], who, however, assumes that
the reader is familiar with the theory of solution of nth-order homogeneous linear
ODEs in constant coefÔ¨Åcients
x(n)(t) + cn‚àí1x(n‚àí1)(t) + ¬∑ ¬∑ ¬∑ + c1x(1)(t) + c0x(t) = 0,
(11.13)
3This means that we must multiply on the left.
TLFeBOOK

THE MATRIX EXPONENTIAL
491
where the initial conditions are known. In particular, the reader must know that if
Œª is a root of the characteristic equation
Œªn + cn‚àí1Œªn‚àí1 + ¬∑ ¬∑ ¬∑ + c1Œª + c0 = 0,
(11.14)
then if Œª has multiplicity m, its contribution to the solution of the initial-value
problem (IVP) (11.13) is of the general form
(a0 + a1t + ¬∑ ¬∑ ¬∑ + am‚àí1tm‚àí1)eŒªt.
(11.15)
These matters are considered by Derrick and Grossman [10] and Reid [11]. We
shall be combining these facts with the results of Theorem 11.10 (below).
Leonard [8] presents two theorems that relate the solution of (11.13) to the
computation of (t) = eAt.
Theorem 11.9: Leonard I
Let A ‚ààRn√ón be a constant matrix with charac-
teristic polynomial
p(Œª) = det(ŒªI ‚àíA) = Œªn + cn‚àí1Œªn‚àí1 + ¬∑ ¬∑ ¬∑ + c1Œª + c0.
(t) = eAt is the unique solution to the nth-order matrix differential equation
(n)(t) + cn‚àí1(n‚àí1)(t) + ¬∑ ¬∑ ¬∑ + c1(1)(t) + c0(t) = 0
(11.16)
with initial conditions
(0) = I, (1)(0) = A, . . . , (n‚àí2)(0) = An‚àí2, (n‚àí1)(0) = An‚àí1.
(11.17)
Proof
We will demonstrate uniqueness of the solution Ô¨Årst of all.
Suppose that 1(t) and 2(t) are two solutions to (11.16) for the initial condi-
tions stated in (11.17). Let (t) = 1(t) ‚àí2(t) for present purposes, in which
case (t) satisÔ¨Åes (11.16) with the initial conditions
(0) = (1)(0) = ¬∑ ¬∑ ¬∑ = (n‚àí2)(0) = (n‚àí1)(0) = 0.
Consequently, each entry of the matrix (t) satisÔ¨Åes a scalar IVP of the form
x(n)(t) + cn‚àí1x(n‚àí1)(t) + ¬∑ ¬∑ ¬∑ + c1x(1)(t) + c0x(t) = 0,
x(0) = x(1)(0) = ¬∑ ¬∑ ¬∑ = x(n‚àí2)(0) = x(n‚àí1)(0) = 0,
where the solution is x(t) = 0 for all t, so that (t) = 0 for all t ‚ààR+. Thus,
1(t) = 2(t), and so the solution must be unique (if it exists).
TLFeBOOK

492
NUMERICAL METHODS FOR EIGENPROBLEMS
Now we conÔ¨Årm that the solution is (t) = eAt (i.e., we conÔ¨Årm existence in
a constructive manner). Let A be a constant matrix of order n with characteristic
polynomial p(Œª) as in the theorem statement. If now (t) = eAt, then we recall that
(k)(t) = AkeAt, k = 1, 2, . . . , n
(11.18)
[see (11.9)] so that
(n)(t) + cn‚àí1(n‚àí1)(t) + ¬∑ ¬∑ ¬∑ + c1(1)(t) + c0(t)
= [An + cn‚àí1An‚àí1 + ¬∑ ¬∑ ¬∑ + c1A + c0I]eAt
= p(A)eAt = 0
via Theorem 11.8 (Cayley‚ÄìHamilton). From (11.18), we obtain
(0)(0) = I, (1)(0) = A, . . . , (n‚àí2)(0) = An‚àí2, (n‚àí1)(0) = An‚àí1,
and so (t) = eAt is the unique solution to the IVP in the theorem statement.
Theorem 11.10: Leonard II
Let A ‚ààRn√ón be a constant matrix with char-
acteristic polynomial
p(Œª) = Œªn + cn‚àí1Œªn‚àí1 + ¬∑ ¬∑ ¬∑ + c1Œª + c0,
then
eAt = x0(t)I + x1(t)A + x2(t)A2 + ¬∑ ¬∑ ¬∑ + xn‚àí1(t)An‚àí1,
where xk(t), k ‚ààZn are the solutions to the nth-order scalar ODEs
x(n)(t) + cn‚àí1x(n‚àí1)(t) + ¬∑ ¬∑ ¬∑ + c1x(1)(t) + c0x(t) = 0,
satisfying the initial conditions
x(j)
k (0) = Œ¥j‚àík
for j, k ‚ààZn (x(0)
k (t) = xk(t)).
Proof
Let constant matrix A have characteristic polynomial p(Œª) as in the
theorem statement. DeÔ¨Åne
(t) = x0(t)I + x1(t)A + x2(t)A2 + ¬∑ ¬∑ ¬∑ + xn‚àí1(t)An‚àí1,
where xk(t), k ‚ààZn are unique solutions to the nth-order scalar ODEs
x(n)(t) + cn‚àí1x(n‚àí1)(t) + ¬∑ ¬∑ ¬∑ + c1x(1)(t) + c0x(t) = 0,
TLFeBOOK

THE MATRIX EXPONENTIAL
493
satisfying the initial conditions stated in the theorem. Thus, for all t ‚ààR+
(n)(t) + cn‚àí1(n‚àí1)(t) + ¬∑ ¬∑ ¬∑ + c1(1)(t) + c0(t)
=
n‚àí1

k=0
9
x(n)
k (t) + cn‚àí1x(n‚àí1)
k
(t) + ¬∑ ¬∑ ¬∑ + c1x(1)
k (t) + c0xk(t)
:
Ak
= 0 ¬∑ I + 0 ¬∑ A + ¬∑ ¬∑ ¬∑ + 0 ¬∑ An‚àí1 = 0.
As well we see that
(0)
=
x0(0)I + x1(0)A + ¬∑ ¬∑ ¬∑ + xn‚àí1(0)An‚àí1
= I,
(1)(0)
=
x(1)
0 (0)I + x(1)
1 (0)A + ¬∑ ¬∑ ¬∑ + x(1)
n‚àí1(0)An‚àí1
= A,
...
(n‚àí1)(0) = x(n‚àí1)
0
(0)I + x(n‚àí1)
1
(0)A + ¬∑ ¬∑ ¬∑ + x(n‚àí1)
n‚àí1 (0)An‚àí1 = An‚àí1.
Therefore
(t) = x0(t)I + x1(t)A + ¬∑ ¬∑ ¬∑ + xn‚àí1(t)An‚àí1
satisÔ¨Åes the IVP
(n)(t) + cn‚àí1(n‚àí1)(t) + ¬∑ ¬∑ ¬∑ + c1(1)(t) + c0(t) = 0
possessing the initial conditions
(k)(0) = Ak
(k ‚ààZn). The solution is unique, and so we must conclude that eAt = n‚àí1
k=0 xk(t)Ak
for all t ‚ààR+, which is the central claim of the theorem.
An example of how to apply the result of Theorem 11.10 is as follows.
Example 11.4
Suppose that
A =
 Œ±
Œ≥
0
Œ≤

‚ààR2√ó2,
which clearly has the eigenvalues Œª = Œ±, Œ≤. Begin by assuming distinct eigenvalues
for A, speciÔ¨Åcally, that Œ± Ã∏= Œ≤.
The general solution to the second-order homogeneous ODE
x(2)(t) + c1x(1)(t) + c0x(t) = 0
with characteristic roots Œ±, Œ≤ (eigenvalues of A) is [recall (11.15)]
x(t) = a0eŒ±t + a1eŒ≤t.
We have x(1)(t) = a0Œ±eŒ±t + a1Œ≤eŒ≤t.
TLFeBOOK

494
NUMERICAL METHODS FOR EIGENPROBLEMS
For the initial conditions x(0) = 1, x(1)(0) = 0, we have the linear system
of equations
a0 + a1 = 1,
Œ±a0 + Œ≤a1 = 0,
which solve to yield
a0 =
Œ≤
Œ≤ ‚àíŒ± , a1 = ‚àí
Œ±
Œ≤ ‚àíŒ± .
Thus, the solution in this case is
x0(t) =
1
Œ≤ ‚àíŒ± [Œ≤eŒ±t ‚àíŒ±eŒ≤t].
Now, if instead the initial conditions are x(0) = 0, x(1)(0) = 1, we have the linear
system of equations
a0 + a1 = 0,
Œ±a0 + Œ≤a1 = 1,
which solve to yield
a0 = ‚àí
1
Œ≤ ‚àíŒ± , a1 =
1
Œ≤ ‚àíŒ± .
Thus, the solution in this case is
x1(t) =
1
Œ≤ ‚àíŒ± [‚àíeŒ±t + eŒ≤t].
Via Leonard II we must have
eAt = x0(t)I + x1(t)A
=
1
Œ≤ ‚àíŒ±
 Œ≤eŒ±t ‚àíŒ±eŒ≤t
0
0
Œ≤eŒ±t ‚àíŒ±eŒ≤t

+
1
Œ≤ ‚àíŒ±
 ‚àíŒ±eŒ±t + Œ±eŒ≤t
‚àíŒ≥ eŒ±t + Œ≥ eŒ≤t
0
‚àíŒ≤eŒ±t + Œ≤eŒ≤t

=
1
Œ≤ ‚àíŒ±
 (Œ≤ ‚àíŒ±)eŒ±t
‚àíŒ≥ eŒ±t + Œ≥ eŒ≤t
0
(Œ≤ ‚àíŒ±)eŒ≤t

=
 eŒ±t
Œ≥
Œ≤‚àíŒ±(eŒ≤t ‚àíeŒ±t)
0
eŒ≤t

.
(11.19)
Now assume that Œ± = Œ≤.
The general solution to
x(2)(t) + c1x(1)(t) + c0x(t) = 0
TLFeBOOK

THE MATRIX EXPONENTIAL
495
with characteristic roots Œ±, Œ± (eigenvalues of A) is [again, recall (11.15)]
x(t) = (a0 + a1t)eŒ±t.
We have x(1)(t) = (a1 + a0Œ± + a1Œ±t)eŒ±t.
For the initial conditions x(0) = 1, x(1)(0) = 0 we have the linear system
of equations
a0 = 1,
a1 + a0Œ± = 0,
which solves to yield a1 = ‚àíŒ±, so that
x0(t) = (1 ‚àíŒ±t)eŒ±t.
Now if instead the initial conditions are x(0) = 0, x(1)(0) = 1, we have the linear
system of equations
a0 = 0,
a1 + a0Œ± = 1,
which solves to yield a1 = 1 so that
x1(t) = teŒ±t.
If we again apply Leonard II, then we have
eAt = x0(t)I + x1(t)A
=
 eŒ±t
Œ≥ teŒ±t
0
eŒ±t

.
(11.20)
A good exercise for the reader is to verify that x(t) = eAtx(0) solves dx(t)/dt =
Ax(t) [of course, here x(t) = [x0(t)x1(t)]T is a state vector] in both of the cases
considered in Example 11.4. Do this by direct substitution.
Example 11.4 is considered in Moler and Van Loan [7] as it illustrates problems
in computing eAt when the eigenvalues of A are nearly multiple. If we consider
(11.19) when Œ≤ ‚àíŒ± is small, and yet is not negligible, the ‚Äúdivided difference‚Äù
eŒ≤t ‚àíeŒ±t
Œ≤ ‚àíŒ±
,
(11.21)
when computed directly, may result in a large relative error. In (11.19) the ratio
(11.21) is multiplied by Œ≥ , so the Ô¨Ånal answer may be very inaccurate, indeed.
Matrix A in Example 11.4 is of low order (i.e., n = 2) and is triangular. This type
of problem is very difÔ¨Åcult to detect and correct when A is larger and not triangular.
TLFeBOOK

496
NUMERICAL METHODS FOR EIGENPROBLEMS
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
0
5
10
15
20
t
|| eAt ||2
s/m
s
Figure 11.1
An illustration of the hump phenomenon in computing eAt.
Another difÔ¨Åculty noted in Moler and Van Loan [7] is sometimes called the hump
phenomenon. It is illustrated in Fig. 11.1 for Eq. (11.19) using the parameters
Œ± = ‚àí1.01,
Œ≤ = ‚àí1.00,
Œ≥ = 50.
(11.22)
Figure 11.1 is a plot of matrix 2-norm [spectral norm; recall Equation (4.37) with
p = 2 in Chapter 4] ||eAt||2 versus t. (It is a version of Fig. 1 in Ref. 7.) The
problem with this arises from the fact that one way or another some algorithms for
the computation of eAt make use of the identity
eAt = (eAt/m)m.
(11.23)
When s/m is under the hump while s lies beyond it (e.g., in Fig. 11.1 s = 4
with m = 8), we can have
||eAs||2 ‚â™||eAs/m||m
2 .
(11.24)
Unfortunately, rounding errors in the mth power of eAs/m are usually small only
relative to ||eAs/m||2, rather than ||eAs||2. Thus, rounding errors may be a problem
in using (11.23) to compute eAt.
The Taylor series expansion in (11.8) is not a good method for computing eAt.
The reader should recall the example of catastrophic convergence in the compu-
tation of ex (x ‚ààR) from Chapter 3 (Appendix 3.C). It is not difÔ¨Åcult to imagine
that the problem of catastrophic convergence in (11.8) is likely to be much worse,
and much harder to contain. Indeed this is the case as shown by an example in
Moler and Van Loan [7].
It was suggested earlier in this section that the series in (11.8) can be shown to
converge by considering the diagonalization of A (assuming that A is nondefective).
Suppose that A ‚ààCn√ón, and that A possesses eigenvalues that are all distinct, and
so we may apply (11.6). Since
Ak = [P P ‚àí1]k
= [P P ‚àí1][P P ‚àí1] ¬∑ ¬∑ ¬∑ [P P ‚àí1][P P ‚àí1]
#
$%
&
k factors
= P kP ‚àí1,
(11.25)
TLFeBOOK

THE MATRIX EXPONENTIAL
497
we have
eAt =
‚àû

k=0
1
k!Aktk =
‚àû

k=0
1
k!P kP ‚àí1tk = P
 ‚àû

k=0
1
k!ktk
	
P ‚àí1
= P diag
' ‚àû

k=0
1
k!Œªk
0tk,
‚àû

k=0
1
k!Œªk
1tk, . . . ,
‚àû

k=0
1
k!Œªk
n‚àí1tk
(
P ‚àí1
= P diag(eŒª0t, eŒª1t, . . . , eŒªn‚àí1t)P ‚àí1.
If we deÔ¨Åne
et = diag(eŒª0t, eŒª1t, . . . , eŒªn‚àí1t),
(11.26)
then clearly we can say that
eAt =
‚àû

k=0
1
k!Aktk = P etP ‚àí1.
(11.27)
We know from the theory of Maclaurin series that ex = ‚àû
k=0
1
k!xk converges for
all x ‚ààR. Thus, et converges for all t ‚ààR and is well deÔ¨Åned, and hence the
series in (11.8) converges, and so eAt is well deÔ¨Åned. Of course, all of this suggests
that eAt may be numerically computed using (11.27). From Chapter 3 we infer that
accurate, reliable means to compute ex (x is a scalar) do exist. Also, reliable
methods exist to Ô¨Ånd the elements of , and this will be considered later. But P
may be close to singular, that is, the condition number Œ∫(P ) (recall Chapter 4) may
be large, and so accurate determination of P ‚àí1, which is required by (11.27), may
be difÔ¨Åcult. Additionally, the approach (11.27) lacks generality since it won‚Äôt work
unless A is nondefective (i.e., can be diagonalized). Matrix factorization methods
to compute eAt (including those in the defective case) are considered in greater
detail in Ref. 7, and this matter will not be mentioned further here.
In Chapter 4 a condition number Œ∫(A) was deÔ¨Åned that informed us about the
sensitivity of the solution to Ax = b due to perturbations in A and b. It is possible
to develop a similar notion for the problem of computing eAt. From Golub and
Van Loan [4], the matrix exponential condition number is
ŒΩ(A, t) = max
||E||2‚â§1


 t
0
eA(t‚àís)EeAs ds


2
||A||2
||eAt||2
.
(11.28)
(The theory behind this originally appeared in Van Loan [12].) In this expression
E ‚ààRn√ón is a perturbation matrix. The condition number (11.28) measures the
sensitivity of mapping A ‚ÜíeAt for a given t ‚ààR. For a given t, there is a matrix
E such that
||e(A+E)t ‚àíeAt||2
||eAt||2
‚âàŒΩ(A, t)||E||2
||A||2
.
(11.29)
TLFeBOOK

498
NUMERICAL METHODS FOR EIGENPROBLEMS
We see from this that if ŒΩ(A, t) is large, then a small change in A (modeled by the
perturbation matrix E) can cause a large change in eAt. In general, it is not easy
to specify A leading to large values for ŒΩ(A, t). However, it is known that
ŒΩ(A, t) ‚â•t||A||2
(11.30)
for t ‚ààR+ with equality iff A is normal. (Any A ‚ààCn√ón is normal iff AH A =
AAH .) Thus, it appears that normal matrices are generally the least troublesome
with respect to computing eAt. From the deÔ¨Ånition of a normal matrix we see that
real-valued, symmetric matrices are an important special case.
Of the less dubious means to compute eAt, Golub and Van Loan‚Äôs algorithm
11.3.1 [4] is suggested. It is based on Pad¬¥e approximation, which is the use of
rational functions to approximate other functions. However, we will only refer the
reader to Ref. 4 (or Ref. 7) for the relevant details. A version of Algorithm 11.3.1
[4] is implemented in the MATLAB expm function, and MATLAB provides other
algorithm implementations for computing eAt.
11.4
THE POWER METHODS
In this section we consider a simple approach to determine the eigenvalues and
eigenvectors of A ‚ààRn√ón. The approach is iterative. The main result is as follows.
Theorem 11.11: Let A ‚ààRn√ón be such that
(a) A has n linearly independent eigenvectors x(k), corresponding to the eigen-
pairs {(Œªk, x(k))|k ‚ààZn}.
(b) The eigenvalues satisfy Œªn‚àí1 ‚ààR with
|Œªn‚àí1| > |Œªn‚àí2| ‚â•|Œªn‚àí3| ‚â•¬∑ ¬∑ ¬∑ |Œª1| ‚â•|Œª0|
(Œªn‚àí1 is the dominant eigenvalue).
If y0 ‚ààRn is a starting vector such that
y0 =
n‚àí1

j=0
ajx(j)
with an‚àí1 Ã∏= 0, then for yk+1 = Ayk with k ‚ààZ+
lim
k‚Üí‚àû
Aky0
Œªk
n‚àí1
= cx(n‚àí1)
(11.31)
for some c Ã∏= 0, and (recalling that ‚ü®x, y‚ü©= xT y = yT x for any x, y ‚ààRn)
lim
k‚Üí‚àû
‚ü®y0, Aky0‚ü©
‚ü®y0, Ak‚àí1y0‚ü©= Œªn‚àí1.
(11.32)
TLFeBOOK

THE POWER METHODS
499
Proof
We observe that yk = Aky0, and since Akx(j) = Œªk
jx(j), we have
Aky0 =
n‚àí2

j=0
ajŒªk
jx(j) + an‚àí1Œªk
n‚àí1x(n‚àí1),
implying that
Aky0
Œªk
n‚àí1
= an‚àí1x(n‚àí1) +
n‚àí2

j=0
aj
 Œªj
Œªn‚àí1
!k
x(j).
(11.33)
Since |Œªj| < |Œªn‚àí1| for j = 0, 1, . . . , n ‚àí2, we have limk‚Üí‚àû

 Œªj
Œªn‚àí1
k = 0,
and hence
lim
k‚Üí‚àû
Aky0
Œªk
n‚àí1
= an‚àí1x(n‚àí1).
Now Aky0 = n‚àí1
j=0 ajŒªk
jx(j), so that
‚ü®y0, Aky0‚ü©=
n‚àí1

j=0
ajŒªk
j‚ü®y0, x(j)‚ü©,
and if Œ∑j = ‚ü®y0, x(j)‚ü©, then
‚ü®y0, Aky0‚ü©
‚ü®y0, Ak‚àí1y0‚ü©=
an‚àí1Œªk
n‚àí1Œ∑n‚àí1 + n‚àí2
j=0 ajŒªk
jŒ∑j
an‚àí1Œªk‚àí1
n‚àí1Œ∑n‚àí1 + n‚àí2
j=0 ajŒªk‚àí1
j
Œ∑j
= Œªn‚àí1
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
an‚àí1Œ∑n‚àí1 + n‚àí2
j=0 ajŒ∑j
 Œªj
Œªn‚àí1
!k
an‚àí1Œ∑n‚àí1 + n‚àí2
j=0 ajŒ∑j
 Œªj
Œªn‚àí1
!k‚àí1
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª.
Again, since limk‚Üí‚àû

 Œªj
Œªn‚àí1
k = 0 (for j = 0, 1, . . . , n ‚àí2), we have
lim
k‚Üí‚àû
‚ü®y0, Aky0‚ü©
‚ü®y0, Ak‚àí1y0‚ü©= Œªn‚àí1.
We note that
1 >

Œªn‚àí2
Œªn‚àí1
 ‚â•

Œªn‚àí3
Œªn‚àí1
 ‚â•¬∑ ¬∑ ¬∑ ‚â•

Œª0
Œªn‚àí1
 ,
so the rate of convergence of Aky0/Œªk
n‚àí1 to an‚àí1x(n‚àí1) is, according to (11.33),
dominated by the term containing Œªn‚àí2/Œªn‚àí1. This is sometimes expressed
TLFeBOOK

500
NUMERICAL METHODS FOR EIGENPROBLEMS
by writing


Aky0
Œªk
n‚àí1
‚àían‚àí1x(n‚àí1)

 = O
' Œªn‚àí2
Œªn‚àí1
!k(
.
(11.34)
The choice of norm in (11.34) is arbitrary. Of course, a small value for |Œªn‚àí2/Œªn‚àí1|
implies faster convergence.
The theory of Theorem 11.11 assumes that A is nondefective. If the algorithm
suggested by this theorem is applied to a defective A, it will attempt to converge.
In effect, for any defective matrix A there is a nondefective matrix close to it, and
so the limiting values in (11.31) and (11.32) will be for a ‚Äúnearby‚Äù nondefective
matrix. However, convergence can be very slow, particularly if the dependent
eigenvectors of A correspond to Œªn‚àí1, and Œªn‚àí2. In this situation the results may
not be meaningful.
If y0 is chosen such that an‚àí1 = 0, rounding errors in computing yk+1 = Ayk
will usually give a yk with a component in the direction of the eigenvector x(n‚àí1).
Thus, convergence ultimately is the result. To ensure that this happens, it is best
to select y0 with noninteger components.
From (11.31), if k is big enough, then
yk = Aky0 ‚âàcŒªk
n‚àí1x(n‚àí1),
and so yk is an approximation to x(n‚àí1) (to within some scale factor). However,
if |Œªn‚àí1| > 1, we see that ||yk|| ‚Üí‚àûwith increasing k, while if |Œªn‚àí1| < 1, we
see that ||yk|| ‚Üí0 with increasing k. Either way, serious numerical problems will
certainly result (overÔ¨Çow in the former case and rounding errors or underÔ¨Çow in the
latter case). This difÔ¨Åculty may be eliminated by the proper scaling of the iterates,
and leads to what is sometimes called the scaled power algorithm:
Input N; { upper limit on the number of iterations }
Input y0; { starting vector }
y0 := y0/||y0||2; { normalize y0 to unit norm }
k := 0;
while k < N do begin
zk+1 := Ayk;
yk+1 := zk+1/||zk+1||2;
Œª(k+1) := yT
k+1Ayk+1;
k := k + 1;
end;
In this algorithm yk+1 is the (k + 1)th estimate of x(n+1), while Œª(k+1) is the
corresponding estimate of eigenvalue Œªn‚àí1. From the pseudocode above we may
easily see that
yk =
Aky0
||Aky0||2
(11.35)
TLFeBOOK

THE POWER METHODS
501
for all k ‚â•1. With y0 = n‚àí1
j=0 ajx(j) (aj ‚ààR such that ||y0||2 = 1)
Aky0 =
n‚àí2

j=0
ajAkx(j) + an‚àí1Akx(n‚àí1) = an‚àí1Œªk
n‚àí1x(n‚àí1) +
n‚àí2

j=0
ajŒªk
jx(j)
= an‚àí1Œªk
n‚àí1
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
x(n‚àí1) +
n‚àí2

j=0
aj
an‚àí1
 Œªj
Œªn‚àí1
!k
x(j)
#
$%
&
=u(k)
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
,
and as in Theorem 11.11 we see limk‚Üí‚àû||u(k)||2 = 0, and (11.35) becomes
yk =
an‚àí1Œªk
n‚àí1[x(n‚àí1) + u(k)]
||an‚àí1Œªk
n‚àí1[x(n‚àí1) + u(k)]||2
= ¬µk
x(n‚àí1) + u(k)
||x(n‚àí1) + u(k)||2
,
(11.36)
where ¬µk is the sign of an‚àí1Œªk
n‚àí1 (i.e., ¬µk ‚àà{+1, ‚àí1}). Clearly, as k ‚Üí‚àû, vector
yk in (11.36) becomes a better and better approximation to eigenvector x(n‚àí1). To
conÔ¨Årm that Œª(k+1) estimates Œªn‚àí1, consider that from (11.36) we have (for k
sufÔ¨Åciently large)
Œª(k+1) = yT
k+1Ayk+1 ‚âà(x(n‚àí1))T Ax(n‚àí1)
||x(n‚àí1)||2
2
= Œªn‚àí1
[recall that Ax(n‚àí1) = Œªn‚àí1x(n‚àí1)].
If |Œªn‚àí1| = |Œªn‚àí2| > |Œªj| for j = 0, 1, . . . , n ‚àí3, we have two dominant eigen-
values. In this situation, as noted in Quarteroni et al. [13], convergence may or may
not occur. If, for example, Œªn‚àí1 = Œªn‚àí2, then vector sequence (yk) converges to
a vector in the subspace of Rn spanned by x(n‚àí1), and x(n‚àí2). In this case, since
A ‚ààRn√ón, we must have Œªn‚àí1, Œªn‚àí2 ‚ààR, and hence for k ‚â•1, we must have
Aky0 =
n‚àí3

j=0
ajŒªk
jx(j) + an‚àí2Œªk
n‚àí2x(n‚àí2) + an‚àí1Œªk
n‚àí1x(n‚àí1),
implying that
Aky0
Œªk
n‚àí1
= an‚àí1x(n‚àí1) + an‚àí2x(n‚àí2) +
n‚àí3

j=0
aj
 Œªj
Œªn‚àí1
!k
x(j)
so that
lim
k‚Üí‚àû
Aky0
Œªk
n‚àí1
= an‚àí1x(n‚àí1) + an‚àí2x(n‚àí2),
TLFeBOOK

502
NUMERICAL METHODS FOR EIGENPROBLEMS
which is a vector in a two-dimensional subspace of Rn. On the other hand,
recall Example 11.3, where n = 2 so that Œª0 = ejŒ∏, Œª1 = e‚àíjŒ∏ = Œª‚àó
0. From (11.35)
||yk||2 = 1, so, because A is a rotation operator, yk = Aky0/||Aky0||2 will always
be a point on the unit circle {(x, y)|x2 + y2 = 1}. Convergence does not occur
since it is generally not the same point from one iteration to the next (e.g., consider
Œ∏ = œÄ/2 radians).
Example 11.5
Consider an example based on application of the scaled power
algorithm to the matrix
A =
Ô£Æ
Ô£∞
4
1
0
1
4
1
0
1
4
Ô£π
Ô£ª.
This matrix turns out to have the eigenvalues
Œª0 = 2.58578644,
Œª1 = 4.00000000,
Œª2 = 5.41421356
(as may be determined in MATLAB using the eig function). If we deÔ¨Åne yk =
[yk,0 yk,1 yk,2]T ‚ààR3, then, from the algorithm, we obtain the iterates:
k
yk,0
yk,1
yk,2
Œª(k)
0
0.57735027
0.57735027
0.57735027
‚Äî
1
0.53916387
0.64699664
0.53916387
5.39534884
2
0.51916999
0.67891460
0.51916999
5.40988836
3
0.50925630
0.69376945
0.50925630
5.41322584
4
0.50444312
0.70076692
0.50444312
5.41398821
5
0.50212703
0.70408585
0.50212703
5.41416216
6
0.50101700
0.70566560
0.50101700
5.41420184
7
0.50048597
0.70641885
0.50048597
5.41421089
8
0.50023215
0.70677831
0.50023215
5.41421295
9
0.50011089
0.70694993
0.50011089
5.41421342
10
0.50005296
0.70703187
0.50005296
5.41421353
11
0.50002530
0.70707101
0.50002530
5.41421356
In only 11 iterations the power algorithm has obtained the dominant eigenvalue
to an accuracy of eight decimal places.
Continue to assume that A ‚ààRn√ón is nondefective. Now deÔ¨Åne A¬µ = A ‚àí¬µI
(where I is the n √ó n identity matrix, as usual), and ¬µ ‚ààR is called the shift
(or shift parameter).4 We will assume that ¬µ always results in the existence of
A‚àí1
¬µ . Because A is not defective, there will be a nonsingular matrix P such that
P ‚àí1AP =  = diag(Œª0, Œª1, . . . , Œªn‚àí1) (recall the basic facts from Section 11.2
that justify this). Consequently
A¬µ = P P ‚àí1 ‚àí¬µI ‚áíP ‚àí1A¬µP =  ‚àí¬µI,
(11.37)
4The reason for introducing the shift parameter ¬µ will be made clear a bit later.
TLFeBOOK

THE POWER METHODS
503
and so A‚àí1
¬µ
has the eigenvalues
Œ≥k =
1
Œªk ‚àí¬µ, k ‚ààZn.
(11.38)
P is a similarity transformation that diagonalizes A¬µ, giving  ‚àí¬µI, so the eigen-
values of ( ‚àí¬µI)‚àí1 must be the eigenvalues of A‚àí1
¬µ as these are similar matrices.
A modiÔ¨Åcation of the previous scaled power algorithm is the following shifted
inverse power algorithm:
Input N; { upper limit on the number of iterations }
Input y0; { starting vector }
y0 := y0/||y0||2; { normalize y0 to unit norm }
k := 0;
while k < N do begin
A¬µzk+1 := yk;
yk+1 := zk+1/||zk+1||2;
Œ≥ (k+1) := yT
k+1Ayk+1;
k := k + 1;
end;
Assume that the eigenvalues of A satisfy
|Œªn‚àí1| ‚â•|Œªn‚àí2| ‚â•¬∑ ¬∑ ¬∑ ‚â•|Œª1| > |Œª0|,
(11.39)
and also that ¬µ = 0, so then |Œªk| = 1/|Œ≥k|, and (11.39) yields
|Œ≥0| > |Œ≥1| ‚â•|Œ≥2| ‚â•¬∑ ¬∑ ¬∑ ‚â•|Œ≥n‚àí2| ‚â•|Œ≥n‚àí1|.
(11.40)
We observe that in the shifted inverse power algorithm A¬µzk+1 = yk is equivalent
to zk+1 = A‚àí1
¬µ yk, and so A‚àí1
¬µ effectively replaces A in the statement zk+1 := Ayk in
the scaled power algorithm. This implies that the shifted inverse power algorithm
produces vector sequence (yk) that converges to the eigenvector of A‚àí1
¬µ = A‚àí1
corresponding to the eigenvalue Œ≥0 (= 1/Œª0). Since Ax(0) = Œª0x(0) implies that
A‚àí1x(0) = 1
Œª0 x(0) = Œ≥0x(0), then for a sufÔ¨Åciently large k, the vector yk will approx-
imate x(0). The argument to verify this follows the proof of Theorem 11.11.
Therefore, consider the starting vector
y0 = a0x(0) +
n‚àí1

j=1
ajx(j), a0 Ã∏= 0
(11.41)
(such that ||y0||2 = 1). We see that we must have
A‚àíky0 = a0
1
Œªk
0
x(0) +
n‚àí1

j=1
aj
1
Œªk
j
x(j)
= a0Œ≥ k
0 x(0) +
n‚àí1

j=1
ajŒ≥ k
j x(j),
(11.42)
TLFeBOOK

504
NUMERICAL METHODS FOR EIGENPROBLEMS
and thus
A‚àíky0
Œ≥ k
0
= a0x(0) +
n‚àí1

j=1
aj
 Œ≥j
Œ≥0
!k
x(j).
Immediately, because of (11.40), we must have
lim
k‚Üí‚àû
A‚àíky0
Œ≥ k
0
= a0x(0).
From (11.42), we obtain
A‚àíky0 = a0Œ≥ k
0
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
x(0) +
n‚àí1

j=1
aj
a0
 Œ≥j
Œ≥0
!k
x(j)
#
$%
&
=v(k)
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
(11.43)
In much the same way as we arrived at (11.35), for the shifted inverse power
algorithm, we must have for all k ‚â•1
yk =
A‚àík
¬µ y0
||A‚àík
¬µ y0||2
.
(11.44)
From (11.43) for ¬µ = 0, this equation becomes
yk =
a0Œ≥ k
0 [x(0) + v(k)]
||a0Œ≥ k
0 [x(0) + v(k)]||2
= ŒΩk
x(0) + v(k)
||x(0) + v(k)||2
,
(11.45)
where ŒΩk ‚àà{+1, ‚àí1}. From the pseudocode for the shifted inverse power algorithm,
if k is large enough, via (11.45), we obtain
Œ≥ (k+1) = yT
k+1Ayk+1 ‚âà(x(0))T Ax(0)
||x(0)||2
2
= Œª0.
(11.46)
Thus, Œ≥ (k+1) is an approximation to Œª0.
In summary, for a nondefective A ‚ààRn√ón such that (11.39) holds, the shifted
inverse power algorithm will generate a sequence of increasingly better approxi-
mations to the eigenpair (Œª0, x(0)), when we set ¬µ = 0.
We note that the scaled power algorithm needs O(n2) Ô¨Çops (recall the deÔ¨Ånition
of Ô¨Çops from Chapter 4) at every iteration. This is due mainly to the matrix‚Äìvector
product step zk+1 = Ayk. To solve the linear system A¬µzk+1 = yk requires O(n3)
Ô¨Çops in general. To save on computations in the implementation of the shifted
inverse power algorithm, it is often best to LU -decompose A¬µ (recall Section 4.5)
only once: A¬µ = LU. At each iteration LUzk+1 = yk may be solved using forward
TLFeBOOK

THE POWER METHODS
505
and backward substitution, which is efÔ¨Åcient since this needs only O(n2) Ô¨Çops at
every iteration. Even so, because of the need to compute the LU decomposition of
A¬µ, the shifted inverse power algorithm still needs O(n3) Ô¨Çops overall. It is thus
intrinsically more computation-intensive than is the scaled power algorithm.
However, it is the concept of a shift that makes the shifted inverse power algo-
rithm attractive, at least in some circumstances. But before we consider the reason
for introducing the shift parameter ¬µ, the reader should view the following example.
Example 11.6
Let us reconsider matrix A from Example 11.5. If we apply
the shifted inverse power algorithm using ¬µ = 0 to this matrix, we obtain the
following iterates:
k
yk,0
yk,1
yk,2
Œ≥ (k)
0
0.57735027
0.57735027
0.57735027
‚Äî
1
0.63960215
0.42640143
0.63960215
5.09090909
2
0.70014004
0.14002801
0.70014004
4.39215686
3
0.69011108
‚àí0.21792981
0.69011108
3.39841689
4
0.62357865
‚àí0.47148630
0.62357865
2.82396484
5
0.56650154
‚àí0.59845803
0.56650154
2.64389042
6
0.53330904
‚àí0.65662999
0.53330904
2.59925317
7
0.51623508
‚àí0.68337595
0.51623508
2.58886945
8
0.50782504
‚àí0.69586455
0.50782504
2.58649025
9
0.50375306
‚àí0.70175901
0.50375306
2.58594700
10
0.50179601
‚àí0.70455768
0.50179601
2.58582306
11
0.50085857
‚àí0.70589049
0.50085857
2.58579479
12
0.50041023
‚àí0.70652615
0.50041023
2.58578834
13
0.50019597
‚àí0.70682953
0.50019597
2.58578687
14
0.50009360
‚àí0.70697438
0.50009360
2.58578654
15
0.50004471
‚àí0.70704355
0.50004471
2.58578646
16
0.50002135
‚àí0.70707658
0.50002135
2.58578644
We see that the method converges to an estimate of Œª0 (smallest eigenvalue of
A) that is accurate to eight decimal places in only 16 iterations.
As an exercise, the reader should conÔ¨Årm that the vector yk in the bottom row
of the table above is an estimate of the eigenvector for Œª0. The reader should do
the same for Example 11.5.
Recall again that A‚àí1
¬µ
is assumed to exist (so that ¬µ is not an eigenvalue of A).
Observe that Ax(j) = Œªjx(j), so that (A ‚àí¬µI)x(j) = (Œªj ‚àí¬µ)x(j), and therefore
A‚àí1
¬µ x(j) = Œ≥jx(j). Suppose that there is an m ‚ààZn such that
|Œªm ‚àí¬µ| < |Œªj ‚àí¬µ|
(11.47)
for all j ‚ààZn, but that j Ã∏= m; that is, Œªm is closest to ¬µ of all the eigenvalues of
A. This really says that Œªm has a multiplicity of one (i.e., is simple). Now consider
TLFeBOOK

506
NUMERICAL METHODS FOR EIGENPROBLEMS
the starting vector
y0 =
n‚àí1

j=0
jÃ∏=m
ajx(j) + amx(m)
(11.48)
with am Ã∏= 0, and ||y0||2 = 1. Clearly
A‚àík
¬µ y0 =
n‚àí1

j=0
jÃ∏=m
ajŒ≥ k
j x(j) + amŒ≥ k
mx(m),
implying that
A‚àík
¬µ y0
Œ≥ km
= amx(m) +
n‚àí1

j=0
jÃ∏=m
aj
 Œ≥j
Œ≥m
!k
x(j).
(11.49)
Now via (11.47)

Œ≥j
Œ≥m
 =

Œªm ‚àí¬µ
Œªj ‚àí¬µ
 < 1.
Therefore, via (11.49)
lim
k‚Üí‚àû
A‚àík
¬µ y0
Œ≥ km
= amx(m).
This implies that the vector sequence (yk) in the shifted inverse power algorithm
converges to x(m). Put simply, by the proper selection of the shift parameter ¬µ, we
can extract just about any eigenpair of A that we wish to (as long as Œªm is simple).
Thus, in this sense, the shifted inverse power algorithm is more general than the
scaled power algorithm. The following example illustrates another important point.
Example 11.7
Once again we apply the shifted inverse power algorithm to
matrix A from Example 11.5. However, now we select ¬µ = 2. The resulting
sequence of iterates for this case is as follows:
k
yk,0
yk,1
yk,2
Œ≥ (k)
0
0.57735027
0.57735027
0.57735027
‚Äî
1
0.70710678
0.00000000
0.70710678
4.00000000
2
0.57735027
‚àí0.57735027
0.57735027
2.66666667
3
0.51449576
‚àí0.68599434
0.51449576
2.58823529
4
0.50251891
‚àí0.70352647
0.50251891
2.58585859
5
0.50043309
‚àí0.70649377
0.50043309
2.58578856
6
0.50007433
‚àí0.70700164
0.50007433
2.58578650
7
0.50001275
‚àí0.70708874
0.50001275
2.58578644
TLFeBOOK

THE POWER METHODS
507
We see that convergence to the smallest eigenvalue of A has now occurred in
only seven iterations, which is faster than the case considered in Example 11.6 (for
which we used ¬µ = 0).
We see that this example illustrates the fact that a properly chosen shift param-
eter can greatly accelerate the convergence of iterative eigenproblem solvers. This
notion of shifting to improve convergence rates is also important in practical imple-
mentations of QR iteration methods for solving eigenproblems (next section).
So far our methods extract only one eigenvalue from A at a time. One may
apply a method called deÔ¨Çation to extract all the eigenvalues of A under certain
conditions. Begin by noting the following elementary result.
Lemma 11.1: Suppose that B ‚ààR(n‚àí1)√ó(n‚àí1), and that B‚àí1 exists, and that
r ‚ààRn‚àí1, then

1
rT
0
B
‚àí1
=
 1
‚àírT B‚àí1
0
B‚àí1

.
(11.50)
Proof
Exercise.
The deÔ¨Çation procedure is based on the following theorem.
Theorem 11.12: DeÔ¨Çation
Suppose that An ‚ààRn√ón, that eigenvalue Œªi ‚ààR
for all i ‚ààZn, and that all the eigenvalues of An are distinct. The dominant eigenpair
of An is (Œªn‚àí1, x(n‚àí1)), and we assume that ||x(n‚àí1)||2 = 1. Suppose that Qn ‚àà
Rn√ón is an orthogonal matrix such that Qnx(n‚àí1) = [1 0 ¬∑ ¬∑ ¬∑ 0 0]T = e0; then
QnAnQT
n =

Œªn‚àí1
aT
n‚àí1
0
An‚àí1
	
.
(11.51)
Proof
Qn exists because it can be a Householder transformation matrix (recall
Section 4.6). Any eigenvector x(k) of An can always be normalized so that
||x(k)||2 = 1.
Following (11.5), we have
An [x(n‚àí1)x(n‚àí2) ¬∑ ¬∑ ¬∑ x(1)x(0)]
#
$%
&
=Tn
= [x(n‚àí1)x(n‚àí2) ¬∑ ¬∑ ¬∑ x(1)x(0)]
#
$%
&
=Tn
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
Œªn‚àí1
0
¬∑ ¬∑ ¬∑
0
0
0
Œªn‚àí2
¬∑ ¬∑ ¬∑
0
0
...
...
...
...
0
0
¬∑ ¬∑ ¬∑
Œª1
0
0
0
¬∑ ¬∑ ¬∑
0
Œª0
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
#
$%
&
=Dn
,
TLFeBOOK

508
NUMERICAL METHODS FOR EIGENPROBLEMS
that is, AnTn = TnDn. Thus, (QT
n = Q‚àí1
n
via orthogonality)
QnAnQT
n = QnTnDnT ‚àí1
n QT
n = (QnTn)Dn(QnTn)‚àí1.
(11.52)
Now
QnTn = [e0
Qnx(n‚àí2) ¬∑ ¬∑ ¬∑ Qnx(1)Qnx(0)] =

1
bT
n‚àí1
0
Bn‚àí1
	
,
and via Lemma 11.1, we have

1
bT
n‚àí1
0
Bn‚àí1
	‚àí1
=

1
‚àíbT
n‚àí1B‚àí1
n‚àí1
0
B‚àí1
n‚àí1
	
.
Thus, (11.52) becomes
QnAnQT
n =

1
bT
n‚àí1
0
Bn‚àí1
	 
Œªn‚àí1
0
0
Dn‚àí1
	 
1
‚àíbT
n‚àí1B‚àí1
n‚àí1
0
B‚àí1
n‚àí1
	
=

Œªn‚àí1
bT
n‚àí1(Dn‚àí1 ‚àíŒªn‚àí1In‚àí1)B‚àí1
n‚àí1
0
Bn‚àí1Dn‚àí1B‚àí1
n‚àí1
	
,
which has the form given in (11.51).
From Theorem 11.6, QnAnQT
n and An are similar matrices, and so have the
same eigenvalues. Via (11.51), An‚àí1 has the same eigenvalues as An, except
for Œªn‚àí1. Clearly, the scaled power method could be used to Ô¨Ånd the eigenpair
(Œªn‚àí1, x(n‚àí1)). The Householder procedure from Section 4.6 gives Qn. From The-
orem 11.12 we obtain An‚àí1, and the deÔ¨Çation procedure may be repeated to Ô¨Ånd
all the remaining eigenvalues of A = An.
It is important to note that the deÔ¨Çation procedure may be improved with respect
to computational efÔ¨Åciency by employing instead the Rayleigh quotient iteration
method. This replaces the power methods we have considered so far. This approach
is suggested and considered in detail in Golub and Van Loan [4] and Epperson [14];
we omit the details here.
11.5
QR ITERATIONS
The power methods of Section 11.4 and variations thereof such as Rayleigh quotient
iterations are deÔ¨Åcient in that they are not computationally efÔ¨Åcient methods for
computing all possible eigenpairs. The power methods are really at their best when
we seek only a few eigenpairs (usually corresponding to either the smallest or
the largest eigenvalues). In Section 11.4 the power methods were applied only to
TLFeBOOK

QR ITERATIONS
509
computing real-valued eigenvalues, but it is noteworthy that power methods can
be adapted to Ô¨Ånding complex‚Äìconjugate eigenvalue pairs [19].
The QR iterations algorithms are, according to Watkins [15], due originally to
Francis [16] and Kublanovskaya [17]. The methodology involved in QR iterations
is based, in turn, on earlier work of H. Rutishauser performed in the 1950s. The
detailed theory and rationale for the QR iterations are not by any means straight-
forward, and even the geometric arguments in Ref. 15 (based, in turn, on the work
of Parlett and Poole [18]) are not easy to follow. However, for matrices A ‚ààCn√ón
that are dense (i.e., nonsparse; recall Section 4.7), that are not too large, and that
are nondefective, the QR iterations are the best approach presently known for Ô¨Ånd-
ing all possible eigenpairs of A. Indeed, the MATLAB eig function implements a
modern version of the QR iteration methodology.5
Because of the highly involved nature of the QR iteration theory, we will only
present a few of the main ideas here. Other than the references cited so far, the
reader is referred to the literature [4,6,13,19] for more thorough discussions. Of
course, these are not the only references available on this subject.
Eigenvalue computations such as the QR iterations reduce large problems into
smaller problems. Golub and Van Loan [4] present two lemmas that are involved
in this reduction approach. Recall from Section 4.7 that s(A) denotes the set of all
the eigenvalues of matrix A (and is also called the spectrum of A).
Lemma 11.2: If A ‚ààCn√ón is of the form
A =
 A00
A01
0
A11

,
where A00 ‚ààCp√óp, A01 ‚ààCp√óq, A11 ‚ààCq√óq (q + p = n), then s(A) = s(A00) ‚à™
s(A11).
Proof
Consider
Ax =
 A00
A01
0
A11
  x1
x2

= Œª
 x1
x2

(x1 ‚ààCp and x2 ‚ààCq). If x2 Ã∏= 0, then A11x2 = Œªx2, and so we conclude that
Œª ‚ààs(A11). On the other hand, if x2 = 0, then A00x1 = Œªx1, so we must have
Œª ‚ààs(A00). Thus, s(A) ‚ààs(A00) ‚à™s(A11). Sets s(A) and s(A00) ‚à™s(A11) have
the same cardinality (i.e., the same number of elements), and so s(A) = s(A00) ‚à™
s(A11).
5If A ‚ààCn√ón, then [V, D] = eig(A) such that
A = V DV ‚àí1,
where D ‚ààCn√ón is the diagonal matrix of eigenvalues of A and V ‚ààCn√ón is the matrix whose columns
are the corresponding eigenvectors. The eigenvectors in V are ‚Äúnormalized‚Äù so that each eigenvector
has a 2-norm of unity.
TLFeBOOK

510
NUMERICAL METHODS FOR EIGENPROBLEMS
Essentially, Lemma 11.2 states that if A is block upper triangular, the eigen-
values lie within the diagonal blocks.
Lemma 11.3: If A ‚ààCn√ón, B ‚ààCp√óp, X ‚ààCn√óp (with p ‚â§n) satisfy
AX = XB, rank(X) = p,
(11.53)
then there is a unitary Q ‚ààCn√ón (so Q‚àí1 = QH ) such that
QHAQ = T =

T00
T01
0
T11
	
,
(11.54)
where
T00 ‚ààCp√óp,
T01 ‚ààCp√ó(n‚àíp),
T11 ‚ààC(n‚àíp)√ó(n‚àíp),
and
s(T00) =
s(A) ‚à©s(B).
Proof
The QR decomposition idea from Section 4.6 generalizes to any X ‚àà
Cn√óp with p ‚â§n and rank(X) = p; that is, complex-valued Householder matrices
are available. Thus, there is a unitary matrix Q ‚ààCn√ón such that
X = Q
 R
0

,
where R ‚ààCp√óp. Substituting this into (11.53) yields

T00
T01
T10
T11
	  R
0

=
 R
0

B,
(11.55)
where
QHAQ =

T00
T01
T10
T11
	
.
From (11.55) T10R = 0, implying that T10 = 0 [yielding (11.54)], and also T00R =
RB, implying that B = R‚àí1T00R (R‚àí1 exists because X is full-rank). T00 and
B are similar matrices so s(B) = s(T00). From Lemma 11.2 we have s(A) =
s(T00) ‚à™s(T11). Thus, s(A) = s(B) ‚à™s(T11). From basic properties regarding sets
(distributive laws)
s(T00) ‚à©s(A) = s(T00) ‚à©[s(B) ‚à™s(T11)]
= [s(T00) ‚à©s(B)] ‚à™[s(T00) ‚à©s(T11)]
= s(T00) ‚à©‚àÖ,
implying that s(T00) = s(T00) ‚à©s(A) = s(A) ‚à©s(B). This statement [s(T00) =
s(A) ‚à©s(B)] really says that the eigenvalues of B are a subset of those of A.
TLFeBOOK

QR ITERATIONS
511
Recall that a subspace of vector space Cn is a subset of Cn that is also a vector
space. Suppose that we have the vectors x0, . . . , xm‚àí1 ‚ààCn; then we may deÔ¨Åne
the spanning set as
span(x0, . . . , xm‚àí1) =
Ô£±
Ô£≤
Ô£≥
m‚àí1

j=0
ajxj|aj ‚ààC
Ô£º
Ô£Ω
Ô£æ.
(11.56)
In particular, if S = span(x), where x is an eigenvector of A, then
y ‚ààS ‚áíAy ‚ààS,
and so S is invariant for A, or invariant to the action of A. It is a subspace
(eigenspace) of Cn that is invariant to A. Lemmas 11.2 and 11.3 can be used to
establish the following important decomposition theorem (Theorem 7.4.1 in Ref.
4). We emphasize that it is for real-valued A only.
Theorem 11.13: Real Schur Decomposition
If A ‚ààRn√ón, then there is an
orthogonal matrix Q ‚ààRn√ón such that
QT AQ =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
R00
R01
¬∑ ¬∑ ¬∑
R0,m‚àí1
0
R11
¬∑ ¬∑ ¬∑
R1,m‚àí1
...
...
...
0
0
¬∑ ¬∑ ¬∑
Rm‚àí1,m‚àí1
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª= R,
(11.57)
where each Ri,i is either 1 √ó 1 or 2 √ó 2. In the latter case Ri,i will have a complex‚Äì
conjugate pair of eigenvalues.
Proof
The matrix A ‚ààRn√ón, so det(ŒªI ‚àíA) has real-valued coefÔ¨Åcients, and
so complex eigenvalues of A always occur in conjugate pairs (recall Section 11.2).
Let k be the number of complex‚Äìconjugate eigenvalue pairs in s(A). We will
employ mathematical induction on k.
The theorem certainly holds for k = 0 via Lemmas 11.2 and 11.3 since real-
valued matrices are only a special case. Now we assume that k ‚â•1 (i.e., A possesses
at least one conjugate pair of eigenvalues). Suppose that an eigenvalue is Œª =
Œ± + jŒ≤ ‚ààs(A) with Œ≤ Ã∏= 0. There must be vectors x, y ‚ààRn (with y Ã∏= 0) such that
A(x + jy) = (Œ± + jŒ≤)(x + jy),
or equivalently
A[xy] = [xy]

Œ±
Œ≤
‚àíŒ≤
Œ±

.
(11.58)
Since Œ≤ Ã∏= 0, vectors x and y span a two-dimensional subspace of Rn that is
invariant to the action of A because of (11.58). From Lemma 11.3 there is an
TLFeBOOK

512
NUMERICAL METHODS FOR EIGENPROBLEMS
orthogonal matrix U1 ‚ààRn√ón such that
UT
1 AU1 =

T00
T01
0
T11
	
,
where T00 ‚ààR2√ó2, T01 ‚ààR2√ó(n‚àí2), T11 ‚ààR(n‚àí2)√ó(n‚àí2), and s(T00) = {Œª, Œª‚àó}. By
induction there is another orthogonal matrix U2 such that UT
2 T11U2 has the neces-
sary structure. Equation (11.57) then follows by letting
Q = U1
 I2
0
0
U2

,
where I2 is the 2 √ó 2 identity matrix. Of course, this process may be repeated as
often as needed.
A method that reliably gives us the blocks Ri,i for all i in (11.57) therefore gives
us all the eigenvalues of A since Ri,i is only 1 √ó 1, or 2 √ó 2, making its eigenvalues
easy to Ô¨Ånd in any case. The elements in the Ô¨Årst subdiagonal of QT AQ of (11.57)
are not necessarily zero-valued (again because Ri,i might be 2 √ó 2), so we say that
QT AQ is upper quasi-triangular.
DeÔ¨Ånition 11.5: Hessenberg Form
Matrix A ‚ààCn√ón is in Hessenberg form
if ai,j = 0 for all i, j such that i ‚àíj > 1.
Technically, A in this deÔ¨Ånition is upper Hessenberg. Matrix A in Example 4.4
is Hessenberg. All upper triangular matrices are Hessenberg. The quasi-triangular
matrix QT AQ in Theorem 11.13 is Hessenberg.
A pseudocode for the basic QR iterations algorithm is
Input N; { Upper limit on the number of iterations }
Input A ‚ààRn√ón; { Matrix we want to eigendecompose }
H0 := QT
0AQ0; { Reduce A to Hessenberg form }
k := 1;
while k ‚â§N do begin
Hk‚àí1 := QkRk; { QR-decomposition step }
Hk := RkQk;
k := k + 1;
end;
In this algorithm we emphasize that A is assumed to be real-valued. Generalization
to the complex case is possible but omitted. The statement H0 = QT
0 AQ0 generally
involves applying orthogonal transformation Q0 ‚ààRn√ón to reduce A to Hessenberg
matrix H0, although in principle this is not necessary. However, there are major
advantages (discussed below) to reducing A to Hessenberg form as a Ô¨Årst step.
The basis for this initial reduction step is the following theorem, which proves that
such a step is always possible.
TLFeBOOK

QR ITERATIONS
513
Theorem 11.14: Hessenberg Reduction
If A ‚ààRn√ón, there is an orthogonal
matrix Q ‚ààRn√ón such that QT AQ = H is Hessenberg.
Proof
From Section 4.6 in general there is an orthogonal matrix P such that
P x = ||x||2e0 (e.g., P is a Householder matrix), where e0 = [1 0 ¬∑ ¬∑ ¬∑ 0 0]T ‚ààRn
if x ‚ààRn.
Partition A according to
A(0) = A =

a(0)
00
bT
0
a0
A(0)
11
	
,
where a(0)
00 ‚ààR and a0, b0 ‚ààRn‚àí1, A(0)
11 ‚ààR(n‚àí1)√ó(n‚àí1). Let P1 be orthogonal such
that P1a0 = ||a0||2e0 (e0 ‚ààRn‚àí1). DeÔ¨Åne
Q1 =
 1
0
0
P1

,
and clearly Q‚àí1
1
= QT
1 (i.e., Q1 is also orthogonal). Thus
A(1) = Q1A(0)QT
1 =

a(0)
00
bT
0 P T
1
||a0||2e0
P1A(0)
11 P T
1
	
.
The Ô¨Årst column of A(1) satisÔ¨Åes the Hessenberg condition since it is
[a(0)
00 ||a0||2
0 ¬∑ ¬∑ ¬∑ 0
# $% &
n‚àí2 zeros
]T . The process may be repeated again by partitioning A(1)
according to
A(1) =

A(1)
00
bT
1
[0
a1]
A(1)
11
	
,
where A(1)
00 ‚ààR2√ó2, and [0
a1], b1 ‚ààR(n‚àí2)√ó2, A(1)
11 ‚ààR(n‚àí2)√ó(n‚àí2). Let P2 be
orthogonal such that P2a1 = ||a1||2e0 (e0 ‚ààRn‚àí2). DeÔ¨Åne
Q2 =
 I2
0
0
P2

,
where I2 is the 2 √ó 2 identity matrix. Thus
A(2) = Q2A(1)QT
2 = Q2Q1A(0)QT
1 QT
2
=

A(1)
00
bT
1 P T
2
[0
||a1||2e0]
P2A(1)
11 P T
2
	
,
TLFeBOOK

514
NUMERICAL METHODS FOR EIGENPROBLEMS
and the Ô¨Årst two columns of A(2) satisfy the Hessenberg condition. Of course, we
may continue in this fashion, Ô¨Ånally yielding
A(n‚àí2) = Qn‚àí2 ¬∑ ¬∑ ¬∑ Q2Q1AQT
1 QT
2 ¬∑ ¬∑ ¬∑ QT
n‚àí2,
which is Hessenberg. We may deÔ¨Åne QT = Qn‚àí2 ¬∑ ¬∑ ¬∑ Q2Q1 and H = A(n‚àí2),
which is the claim made in the theorem statement.
Thus, Theorem 11.14 contains a prescription for Ô¨Ånding H0 = QT
0 A0Q0 as well
as a simple proof of existence of the decomposition. Hessenberg reduction is done
to facilitate reducing the amount of computation per iteration. Clearly, A and H0
are similar matrices and so possess the same eigenvalues.
From the pseudocode for k = 1, 2, . . . , N, we obtain
Hk‚àí1 = QkRk,
Hk = RkQk,
which yields
HN = QT
N ¬∑ ¬∑ ¬∑ QT
2 QT
1 H0Q1Q2 ¬∑ ¬∑ ¬∑ QN,
(11.59)
so therefore
HN = QT
N ¬∑ ¬∑ ¬∑ QT
2 QT
1 QT
0 AQ0Q1Q2 ¬∑ ¬∑ ¬∑ QN.
(11.60)
Matrices HN and A are similar, and so have the same eigenvalues for any N. It is
important to note that if Qk is constructed properly then Hk is Hessenberg for all
k. As explained in Golub and Van Loan [4, Section 7.4.2], the use of orthogonal
matrices Qk based on the 2 √ó 2 rotation operator (matrix A from Example 11.3)
is recommended. These orthogonal matrices are called Givens matrices, or Givens
rotations. The result is an algorithm that needs only O(n2) Ô¨Çops per iteration
instead of O(n3) Ô¨Çops. Overall computational complexity is still O(n3) Ô¨Çops, due
to the initial Hessenberg reduction step. It is to be noted that the rounding error
performance of the suggested algorithm is quite good [19].
We have already noted the desirability of the real Schur decomposition of A
into R according to QT AQ = R in (11.57). In fact, with proper attention to details
(many of which cannot be considered here), the QR iterations method is an excellent
means to Ô¨Ånd Q and R as in any valid matrix norm
lim
N‚Üí‚àûHN = R
(11.61)
and
‚àû
/
i=0
Qi = Q
(11.62)
TLFeBOOK

QR ITERATIONS
515
(of course, 0N
i=0 Qi = Q0Q1 ¬∑ ¬∑ ¬∑ QN; the ordering of factors in the product is
important since Qi is a matrix for all i). The formal proof of this is rather difÔ¨Åcult,
and so it is omitted.
Suppose that we have
A =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
0
0
0
¬∑ ¬∑ ¬∑
0
‚àían
1
0
0
¬∑ ¬∑ ¬∑
0
‚àían‚àí1
0
1
0
¬∑ ¬∑ ¬∑
0
‚àían‚àí2
...
...
...
...
...
0
0
0
¬∑ ¬∑ ¬∑
0
‚àía2
0
0
0
¬∑ ¬∑ ¬∑
1
‚àía1
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
‚ààRn√ón.
(11.63)
It can be shown that
p(Œª) = det(ŒªI ‚àíA) = Œªn + a1Œªn‚àí1 + a2Œªn‚àí2 + ¬∑ ¬∑ ¬∑ + an‚àí1Œª + an.
(11.64)
Matrix A is called a companion matrix. We see that it is easy to obtain (11.64) from
(11.63), or vice versa. We also see that A is Hessenberg. Because of (11.61), we
may conceivably input A of (11.63) into the basic QR iterations algorithm (omitting
the initial Hessenberg reduction step), and so determine the roots of p(Œª) = 0 as
these are the eigenvalues of A. Since p(Œª) is essentially arbitrary, except that it
should not yield a defective A, we have an algorithm to solve the polynomial zero-
Ô¨Ånding problem that was mentioned in Chapter 7. Unfortunately, it has been noted
[4,19] that this is not necessarily a stable method for Ô¨Ånding polynomial zeros.
Example 11.8
Suppose that
p(Œª) = (Œª2 ‚àí2Œª + 2)(Œª2 ‚àí
‚àö
2Œª + 1)
= Œª4 ‚àí(2 +
‚àö
2)Œª3 + (2
‚àö
2 + 3)Œª2 ‚àí2(1 +
‚àö
2)Œª + 2,
which has zeros for
Œª ‚àà

1 ¬± j, 1
‚àö
2
¬± 1
‚àö
2
j

.
After 50 iterations of the basic QR iterations algorithm, we obtain
H50 =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
0.5000
‚àí6.0355
0.9239
5.3848
0.2071
1.5000
‚àí0.3827
‚àí2.2304
0.0000
0.0000
0.7071
‚àí0.7071
0.0000
0.0000
0.7071
0.7071
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª=

R0,0
R0,1
0
R1,1
	
,
where Ri,j ‚ààR2√ó2. The reader may wish to conÔ¨Årm that R0,0 has the eigenvalues
1 ¬± j, and that R1,1 has the eigenvalues
1
‚àö
2(1 ¬± j).
TLFeBOOK

516
NUMERICAL METHODS FOR EIGENPROBLEMS
On the other hand, for
p(Œª) = (Œª + 1)(Œª2 ‚àí2Œª + 2)(Œª2 ‚àí
‚àö
2Œª + 1),
which is a slight modiÔ¨Åcation of the previous example, the basic QR iterations
algorithm will fail to converge.
The following point is also important. In (11.60), deÔ¨Åne Q = Q0Q1 ¬∑ ¬∑ ¬∑ QN,
so that HN = QT AQ. Now suppose that A = AT . Clearly, H T
N = QT AT Q =
QT AQ = HN. This implies that HN will be tridiagonal (deÔ¨Åned in Section 6.5)
for a real and symmetric A. Because of (11.61), we must now have
lim
N‚Üí‚àûHN = diag(R0,0, R1,1, . . . , Rm‚àí1,m‚àí1) = D,
where each Ri,i is 1 √ó 1. Thus, D is the diagonal matrix of eigenvalues of A. Also,
we must have 0‚àû
i=0 Qi as the corresponding matrix of eigenvectors of A.
Example 11.9
Suppose that A is the matrix from Example 11.5:
A =
Ô£Æ
Ô£∞
4
1
0
1
4
1
0
1
4
Ô£π
Ô£ª.
We see that A is in Hessenberg form already. After 36 iterations of the basic QR
iteration algorithm, we obtain
H36 =
Ô£Æ
Ô£∞
5.4142
0.0000
0.0000
0.0000
4.0000
0.0000
0.0000
0.0000
2.5858
Ô£π
Ô£ª,
so the matrix is diagonal and reveals all eigenvalues of A. Additionally, we have
36
/
i=0
Qi =
Ô£Æ
Ô£∞
0.5000
‚àí0.7071
0.5000
0.7071
0.0000
‚àí0.7071
0.5000
0.7071
0.5000
Ô£π
Ô£ª,
which is a good approximation to the eigenvectors of A.
We noted in Section 11.4 that shifting can be used to accelerate convergence in
power methods. Similarly, shifting can be employed in QR iterations to achieve
the same result. Indeed, all modern implementations of QR iterations incorporate
some form of shifting for this reason. The previous basic QR iteration algorithm
may be modiÔ¨Åed to incorporate the shift parameter ¬µ ‚ààR. The overall structure of
the result is described by the following pseudocode:
TLFeBOOK

QR ITERATIONS
517
Input N; { Upper limit on the number of iterations }
Input A ‚ààRn√ón; { Matrix we want to eigendecompose }
H0 := QT
0AQ0; { Reduce A to Hessenberg form }
k := 1;
while k ‚â§N do begin
Determine the shift parameter ¬µ ‚ààR;
Hk‚àí1 ‚àí¬µI := QkRk; { QR-decomposition step }
Hk := RkQk + ¬µI;
k := k + 1;
end;
The reader may readily conÔ¨Årm that we still have
HN = QT
N ¬∑ ¬∑ ¬∑ QT
1 QT
0 AQ0Q1 ¬∑ ¬∑ ¬∑ QN
just as we had in (11.60) for the basic QR iteration algorithm. Thus, we again Ô¨Ånd
that Hk is similar to A for all k. Perhaps the simplest means to generate ¬µ is the
single-shift QR iterations algorithm:
Input N; { Upper limit on the number of iterations }
Input A ‚ààRn√ón; { Matrix we want to eigendecompose }
H0 := QT
0AQ0; { Reduce A to Hessenberg form }
k := 1;
while k ‚â§N do begin
¬µk := Hk‚àí1(n ‚àí1, n ‚àí1); { ¬µk is the lower right corner element of Hk‚àí1 }
Hk‚àí1 ‚àí¬µkI := QkRk; { QR-decomposition step }
Hk := RkQk + ¬µkI;
k := k + 1;
end;
We note that ¬µ is not Ô¨Åxed in general from one iteration to the next. Basically, ¬µ
varies from iteration to iteration in order to account for new information about s(A)
as the subdiagonal entries of Hk converge to zero. We will avoid the technicalities
involved in a full justiÔ¨Åcation of this approach except to mention that it is Ô¨Çawed,
and that more sophisticated shifting methods are needed for an acceptable algorithm
(e.g., the double shift [4,19]). However, the following example shows that shifting
in this way does speed convergence.
Example 11.10
If we apply the single-shift QR iterations algorithm to A in
Example 11.9, we obtain the following matrix in only one iteration:
H1 =
Ô£Æ
Ô£∞
4.0000
‚àí1.4142
0.0000
‚àí1.4142
4.0000
0.0000
0.0000
0.0000
4.0000
Ô£π
Ô£ª.
This matrix certainly does not have the structure of H36 in Example 11.9, but the
eigenvalues of the submatrix

4.0000
‚àí1.4142
‚àí1.4142
4.0000

are 5.4142, 2.5858.
TLFeBOOK

518
NUMERICAL METHODS FOR EIGENPROBLEMS
Finally, we mention that our pseudocodes assume a user-speciÔ¨Åed number of
iterations N. This is not convenient, and is inefÔ¨Åcient in practice. Criteria to auto-
matically terminate the QR iterations without user intervention are available, but a
discussion of this matter is beyond our scope.
REFERENCES
1. D. R. Hill, Experiments in Computational Matrix Algebra (C. B. Moler, consulting ed.),
Random House, New York, 1988.
2. P. Halmos, Finite Dimensional Vector Spaces, Van Nostrand, New York, 1958.
3. R. A. Horn and C. R. Johnson, Matrix Analysis, Cambridge Univ. Press, Cambridge,
UK, 1985.
4. G. H. Golub and C. F. Van Loan, Matrix Computations, 2nd ed., Johns Hopkins Univ.
Press, Baltimore, MD, 1989.
5. F. W. Fairman, ‚ÄúOn Using Singular Value Decomposition to Obtain Irreducible Jordan
Realizations,‚Äù in Linear Circuits, Systems and Signal Processing: Theory and Appli-
cation, C. I. Byrnes, C. F. Martin, and R. E. Saeks, eds., North-Holland, Amsterdam,
1988, pp. 35‚Äì40.
6. G. Stewart, Introduction to Matrix Computations, Academic Press, New York, 1973.
7. C. Moler and C. Van Loan, ‚ÄúNineteen Dubious Ways to Compute the Exponential of a
Matrix,‚Äù SIAM Rev. 20, 801‚Äì836 (Oct. 1978).
8. I. E. Leonard, ‚ÄúThe Matrix Exponential,‚Äù SIAM Rev. 38, 507‚Äì512 (Sept. 1996).
9. B. Noble and J. W. Daniel, Applied Linear Algebra, Prentice-Hall, Englewood Cliffs,
NJ, 1977.
10. W. R. Derrick and S. I. Grossman, Elementary Differential Equations with Applications,
2nd ed., Addison-Wesley, Reading, MA, 1981.
11. W. T. Reid, Ordinary Differential Equations, Wiley, New York, 1971.
12. C. Van Loan, ‚ÄúThe Sensitivity of the Matrix Exponential,‚Äù SIAM J. Numer. Anal. 14,
971‚Äì981 (Dec. 1977).
13. A. Quarteroni, R. Sacco, and F. Saleri, Numerical Mathematics (Texts in Applied Math-
ematics series, Vol. 37). Springer-Verlag, New York, 2000.
14. J. F. Epperson, An Introduction to Numerical Methods and Analysis, Wiley, New
York, 2002.
15. D. S. Watkins, ‚ÄúUnderstanding the QR Algorithm,‚Äù SIAM Rev. 24, 427‚Äì440 (Oct.
1982).
16. J. G. F. Francis, ‚ÄúThe QR Transformation: A Unitary Analogue to the LR Transforma-
tions, Parts I and II,‚Äù Comput. J. 4, 265‚Äì272, 332‚Äì345 (1961).
17. V. N. Kublanovskaya, ‚ÄúOn Some Algorithms for the Solution of the Complete Eigen-
value Problem,‚Äù USSR Comput. Math. Phys. 3, 637‚Äì657, (1961).
18. B. N. Parlett and W. G. Poole, Jr., ‚ÄúA Geometric Theory for the QR, LU, and Power
Iterations,‚Äù SIAM J. Numer. Anal. 10, 389‚Äì412 (1973).
19. J. H. Wilkinson, The Algebraic Eigenvalue Problem, Clarendon Press, Oxford, UK,
1965.
TLFeBOOK

PROBLEMS
519
PROBLEMS
11.1. Aided with at most a pocket calculator, Ô¨Ånd all the eigenvalues and eigen-
vectors of the following matrices:
(a) A =
 4
1
1
4

.
(b) B =
Ô£Æ
Ô£∞
0
0
‚àí2
1
0
1
0
1
2
Ô£π
Ô£ª.
(c) C =
Ô£Æ
Ô£ØÔ£∞
0
1
1
4
0
0
‚àí1
4
1
0
1
Ô£π
Ô£∫Ô£ª.
(d) D =

0
1 ‚àíj
‚àíj
2

.
11.2. A conic section in R2 is described in general by
Œ±x2
0 + 2Œ≤x0x1 + Œ≥ x2
1 + Œ¥x0 + œµx1 + œÅ = 0.
(11.P.1)
(a) Show that (11.P.1) can be rewritten as a quadratic form:
xT Ax + gT x + œÅ = 0,
(11.P.2)
where x = [x0
x1]T , and A = AT .
(b) For a conic section in standard form A is diagonal. Suppose that A is
diagonal. State the conditions on the diagonal elements that result in
(11.P.2) describing an ellipse, a parabola, and a hyperbola.
(c) Suppose that (11.P.2) is not in standard form (i.e., A is not a diagonal
matrix). Explain how similarity transformations might be used to place
(11.P.2) in standard form.
11.3. Consider the companion matrix
C =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
0
0
0
¬∑ ¬∑ ¬∑
0
‚àícn
1
0
0
¬∑ ¬∑ ¬∑
0
‚àícn‚àí1
0
1
0
¬∑ ¬∑ ¬∑
0
‚àícn‚àí2
...
...
...
...
...
0
0
0
¬∑ ¬∑ ¬∑
0
‚àíc2
0
0
0
¬∑ ¬∑ ¬∑
1
‚àíc1
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
‚ààCn√ón.
TLFeBOOK

520
NUMERICAL METHODS FOR EIGENPROBLEMS
Prove that
pn(Œª) = det(ŒªI ‚àíC) = Œªn + c1Œªn‚àí1 + c2Œªn‚àí2 + ¬∑ ¬∑ ¬∑ + cn‚àí1Œª + cn.
11.4. Suppose that the eigenvalues of A ‚ààCn√ón are Œª0, Œª1, . . . , Œªn‚àí1. Find the
eigenvalues of A + Œ±I, where I is the order n identity matrix and Œ± ‚ààC
is a constant.
11.5. Suppose that A ‚ààRn√ón is orthogonal (i.e., A‚àí1 = AT ); then, if Œª is an
eigenvalue of A, show that we must have |Œª| = 1.
11.6. Find all the eigenvalues of
A =
Ô£Æ
Ô£ØÔ£ØÔ£∞
cos Œ∏
0
‚àísin Œ∏
0
0
cos œÜ
0
‚àísin œÜ
sin Œ∏
0
cos Œ∏
0
0
sin œÜ
0
cos œÜ
Ô£π
Ô£∫Ô£∫Ô£ª.
(Hint: The problem is simpliÔ¨Åed by using permutation matrices.)
11.7. Consider the following deÔ¨Ånition: A, B ‚ààCn√ón are simultaneously diago-
nalizable if there is a similarity matrix S ‚ààCn√ón such that S‚àí1AS, and
S‚àí1BS are both diagonal matrices. Show that if A, B ‚ààCn√ón are simulta-
neously diagonalizable, then they commute (i.e., AB = BA).
11.8. Prove the following theorem. Let A, B ‚ààCn√ón be diagonalizable. There-
fore, A and B commute iff they are simultaneously diagonalizable.
11.9. Matrix A ‚ààCn√ón is a square root of B ‚ààCn√ón if A2 = B. Show that every
diagonalizable matrix in Cn√ón has a square root.
11.10. Prove the following (Bauer‚ÄìFike) theorem (which says something about
how perturbations of matrices affect their eigenvalues). If Œ≥ is an eigenvalue
of A + E ‚ààRn√ón, and T ‚àí1AT = D = diag(Œª0, Œª1, . . . , Œªn‚àí1), then
min
Œª‚ààs(A) |Œª ‚àíŒ≥ | ‚â§Œ∫2(T )||E||2.
Recall that s(A) denotes the set of eigenvalues of A (i.e., the spectrum of
A). [Hint: If Œ≥ ‚ààs(A), the result is certainly true, so we need consider
only the situation where Œ≥ /‚ààs(A). ConÔ¨Årm that if T ‚àí1(A + E ‚àíŒ≥ I)T
is singular, then so is I + (D ‚àíŒ≥ I)‚àí1(T ‚àí1ET ). Note that if for some
B ‚ààRn√ón the matrix I + B is singular, then (I + B)x = 0 for some x ‚ààRn
that is nonzero, so ||x||2 = ||Bx||2, and so ||B||2 ‚â•1. Consider upper and
lower bounds on the norm ||(D ‚àíŒ≥ I)‚àí1(T ‚àí1ET )||2.]
11.11. The Daubechies 4-tap scaling function œÜ(t) satisÔ¨Åes the two-scale difference
equation
œÜ(t) = p0œÜ(2t) + p1œÜ(2t ‚àí1) + p2œÜ(2t ‚àí2) + p3œÜ(2t ‚àí3),
(11.P.3)
TLFeBOOK

PROBLEMS
521
where suppœÜ(t) = [0, 3] ‚äÇR [i.e., œÜ(t) is nonzero only on the interval
[0, 3]], and where
p0 = 1
4(1 +
‚àö
3),
p1 = 1
4(3 +
‚àö
3),
p2 = 1
4(3 ‚àí
‚àö
3),
p3 = 1
4(1 ‚àí
‚àö
3).
Note that the solution to (11.P.3) is continuous (an important fact).
(a) Find the matrix M such that
MœÜ = œÜ,
(11.P.4)
where œÜ = [œÜ(1)œÜ(2)]T ‚ààR2.
(b) Find the eigenvalues of M. Find the solution œÜ to (11.P.4).
(c) Take œÜ from (b) and multiply it by constant Œ± such that Œ± 2
k=1 œÜ(k) =
1 (i.e., replace œÜ by the normalized form Œ±œÜ).
(d) Using the normalized vector from (c) (i.e., Œ±œÜ), Ô¨Ånd œÜ(k/2) for all
k ‚ààZ.
[Comment: Computation of the Daubechies 4-tap scaling function is the
Ô¨Årst major step in computing the Daubechies 4-tap wavelet. The process
suggested in (d) may be continued to compute œÜ(k/2J ) for any k ‚ààZ, and
for any positive integer J. The algorithm suggested by this is often called
the interpolatory graphical display algorithm (IGDA).]
11.12. Prove the following theorem. Suppose A ‚ààRn√ón and A = AT . Then A > 0
iff A = P T P for some nonsingular matrix P ‚ààRn√ón.
11.13. Let A ‚ààRn√ón be symmetric with eigenvalues
Œª0 ‚â§Œª1 ‚â§¬∑ ¬∑ ¬∑ ‚â§Œªn‚àí2 ‚â§Œªn‚àí1.
Show that for all x ‚ààRn
Œª0xT x ‚â§xT Ax ‚â§Œªn‚àí1xT x.
[Hint: Use the fact that there is an orthogonal matrix P such that P T AP =
 (diagonal matrix of eigenvalues of A). Partition P in terms of its row
vectors.]
11.14. Section 11.3 presented a method of computing eAt ‚Äúby hand.‚Äù Use this
method to
(a) Derive (10.109) (in Example 10.9).
(b) Derive (10.P.9) in Problem 10.23.
TLFeBOOK

522
NUMERICAL METHODS FOR EIGENPROBLEMS
(c) Find a closed-form expression for eAt, where
A =
Ô£Æ
Ô£∞
Œª
1
0
0
Œª
1
0
0
Œª
Ô£π
Ô£ª.
11.15. This exercise conÔ¨Årms that eigenvalues and singular values are deÔ¨Ånitely
not the same thing. Consider the matrix
A =
 0
2
1
1

.
Use the MATLAB eig and svd functions to Ô¨Ånd the eigenvalues and singular
values of A.
11.16. Show that e(A+B)t = eAteBt for all t ‚ààR if AB = BA. Does e(A+B)t =
eAteBt always hold for all t ‚ààR when AB Ã∏= BA ? Justify your answer.
11.17. This problem is an introduction to Floquet theory. Consider a linear system
with state vector x(t) ‚ààRn for all t ‚ààR such that
dx(t)
dt
= A(t)x(t)
(11.P.5)
for some A(t) ‚ààRn√ón (all t ‚ààR), and such that A(t + T ) = A(t) for some
T > 0 [so that A(t) is periodic with period T ]. Let (t) be the fundamental
matrix of the system such that
d(t)
dt
= A(t)(t), (0) = I.
(11.P.6)
(a) Let (t) = (t + T ), and show that
d(t)
dt
= A(t)(t).
(11.P.7)
(b) Show that (t + T ) = (t)C, where C = (T ). [Hint: Equations
(11.P.6) and (11.P.7) differ only in their initial conditions [i.e., (0) =
what ?].]
(c) Assume that C‚àí1 exists for some C ‚ààRn√ón, and that there exists some
R ‚ààRn√ón such that C = eT R. DeÔ¨Åne P (t) = (t)e‚àítR, and show that
P (t + T ) = P (t). [Thus, (t) = P (t)etR, which is the general form of
the solution to (11.P.6).]
(Comment: Further details of the theory of solution of (11.P.5) based on
working with (11.P.6) may be found in E. A. Coddington and N. Levinson,
TLFeBOOK

PROBLEMS
523
Theory of Ordinary Differential Equations, McGraw-Hill, New York, 1955.
The main thing for the student to notice is that the theory involves matrix
exponentials.)
11.18. Consider the matrix
A =
Ô£Æ
Ô£ØÔ£ØÔ£∞
4
2
0
0
1
4
1
0
0
1
4
1
0
0
2
4
Ô£π
Ô£∫Ô£∫Ô£ª.
(a) Create a MATLAB routine that implements the scaled power algorithm,
and use your routine to Ô¨Ånd the largest eigenvalue of A.
(b) Create a MATLAB routine that implements the shifted inverse power
algorithm, and use your routine to Ô¨Ånd the smallest eigenvalue of A.
11.19. For A in the previous problem, Ô¨Ånd Œ∫2(A) using the MATLAB routines that
you created to solve the problem.
11.20. If A ‚ààRn√ón, and A = AT , then, for some x ‚ààRn such that x Ã∏= 0, we
deÔ¨Åne the Rayleigh quotient of A and x to be the ratio xT Ax/xT x (=
‚ü®x, Ax‚ü©/‚ü®x, x‚ü©). The Rayleigh quotient iterative algorithm is described as
k := 0;
while k < N do begin
¬µk := zT
k Azk/zT
k zk;
(A ‚àí¬µkI)yk := zk;
zk+1 := yk/||yk||2;
k := k + 1;
end;
The user inputs z0, which is the initial guess about the eigenvector. Note
that the shift ¬µk is changed (i.e., updated) at every iteration. This has the
effect of accelerating convergence (i.e., of reducing the number of iterations
needed to achieve an accurate solution). However, A¬µk = A ‚àí¬µkI needs
to be factored anew with every iteration as a result. Prove the following
theorem. Let A ‚ààRn√ón be symmetric and (Œª, x) be an eigenpair for A. If
y ‚âàx, ¬µ = yT Ay/yT y with ||x||2 = ||y||2 = 1, then
|Œª ‚àí¬µ| ‚â§||A ‚àíŒªI||2||x ‚àíy||2
2.
[Comment: The norm ||A ‚àíŒªI||2 is an A-dependent constant, while ||x ‚àí
y||2
2 = ||e||2
2 is the square of the size of the error between eigenvector x
and the estimate y of it. So the size of the error between Œª and the estimate
¬µ (i.e., |Œª ‚àí¬µ|) are proportional to ||e||2
2 at the worst. This explains the
fast convergence of the method (i.e., only a relatively small N is usually
needed). Note that the proof uses (4.31).]
TLFeBOOK

524
NUMERICAL METHODS FOR EIGENPROBLEMS
11.21. Write a MATLAB routine that implements the basic QR iteration algorithm.
You may use the MATLAB function qr to perform QR factorizations. Test
your program out on the following matrices:
(a)
A =
Ô£Æ
Ô£ØÔ£ØÔ£∞
4
2
0
0
1
4
1
0
0
1
4
1
0
0
2
4
Ô£π
Ô£∫Ô£∫Ô£ª.
(b)
B =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£∞
0
0
0
‚àí1
2
1
0
0
1 +
1
‚àö
2
0
1
0
‚àí3
2 ‚àí
‚àö
2
0
0
1
1 +
‚àö
2
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
Use other built-in MATLAB functions (e.g., roots or eig) to verify your
answers. Iterate enough to obtain entries for HN that are accurate to four
decimal places.
11.22. Repeat the previous problem using your own MATLAB implementation of
the single-shift QR iteration algorithm. Compare the number of iterations
needed to obtain four decimal places of accuracy with the result from the
previous problem.
11.23. Suppose that X, Y ‚ààRn√ón, and we deÔ¨Åne the matrices
A = X + jY, B =
 X
‚àíY
Y
X

.
Show that if Œª ‚ààs(A) is real-valued, then Œª ‚ààs(B). Find a relationship
between the corresponding eigenvectors.
TLFeBOOK

12
Numerical Solution of Partial
Differential Equations
12.1
INTRODUCTION
The subject of partial differential equations (PDEs) with respect to the matter of
their numerical solution is impossibly large to properly cover within a single chapter
(or, for that matter, even within a single textbook). Furthermore, the development of
numerical methods for PDEs is a highly active area of research, and so it continues
to be a challenge to decide what is truly ‚Äúfundamental‚Äù material to cover at an
introductory level. In this chapter we shall place emphasis on wave propagation
problems modeled by hyperbolic PDEs (deÔ¨Åned in Section 12.2). We will consider
especially the Ô¨Ånite-difference time-domain (FDTD) method [8], as this appears to
be gaining importance in such application areas as modeling of the scattering of
electromagnetic waves from particles and objects and modeling of optoelectronic
systems. We will only illustrate the method with respect to planar electromagnetic
wave propagation problems at normal incidence. However, prior to this we shall
give an overview of PDEs, including how they are classiÔ¨Åed into elliptic, parabolic,
and hyperbolic types.
12.2
A BRIEF OVERVIEW OF PARTIAL DIFFERENTIAL EQUATIONS
In this section we deÔ¨Åne some notation and terminology that is used throughout
the chapter. We explain how second-order PDEs are classiÔ¨Åed. We also summarize
some problems that will not be covered within this book, simply citing references
where the interested reader can Ô¨Ånd out more.
We will consider only two-dimensional functions u(x, t) ‚ààR (or u(x, y) ‚ààR),
where the independent variable x is interpreted as a space variable, and independent
variable t is interpreted as time. The order of a PDE is the order of the highest
derivative. For our purposes, we will never consider PDEs of an order greater than
2. Common shorthand notation for partial derivatives includes
ux = ‚àÇu
‚àÇx ,
ut = ‚àÇu
‚àÇt ,
uxt = ‚àÇ2u
‚àÇt‚àÇx ,
uxx = ‚àÇ2u
‚àÇx2 .
An Introduction to Numerical Analysis for Electrical and Computer Engineers, by C.J. Zarowski
ISBN 0-471-46737-5
c‚Éù2004 John Wiley & Sons, Inc.
525
TLFeBOOK

526
NUMERICAL SOLUTION OF PARTIAL DIFFERENTIAL EQUATIONS
If the PDE has solution u(x, y), where x and y are spatial variables, then often
we are interested only in approximating the solution on a bounded region (i.e., a
bounded subset of R2). However, we will consider mainly a PDE with solution
u(x, t), where, as already noted, t is time, and so we consider u(x, t) only for
x ‚àà[a, b] = [x0, xf ] and t ‚àà[t0, tf ]. Commonly, t0 = 0, x0 = 0, and xf = L, with
tf = T . We wish to approximate the PDE solution u(x, t) at grid points (mesh
points) much as we did in the problem of numerically solving ODEs as considered
in Chapter 10. Thus, we wish to approximate u(xk, tn), where
xk = x0 + hk,
tn = t0 + œÑn,
(12.1)
such that
h = 1
M (xf ‚àíx0),
œÑ = 1
N (tf ‚àít0),
(12.2)
and so k = 0, 1, . . . , M, and n = 0, 1, . . . , N. This implies that we assume sam-
pling on a uniform two-dimensional grid deÔ¨Åned on the xt plane [(x, t) plane].
Commonly, the numerical approximation to u(xk, tn) is denoted by uk,n. The index
k is then a space index, and n is a time index.
There is a classiÔ¨Åcation scheme for second-order linear PDEs. According to
Kreyszig [1], Myint-U and Debnath [2], and Courant and Hilbert [9], a PDE of
the form
Auxx + 2Buxy + Cuyy = F(x, y, u, ux, uy)
(12.3)
is elliptic if AC ‚àíB2 > 0, parabolic if AC ‚àíB2 = 0, and is hyperbolic if AC ‚àí
B2 < 0.1 It is possible for A, B, and C to be functions of x and y, in which case
(12.3) may be of a type (elliptic, parabolic, hyperbolic) that varies with x and y. For
example, (12.3) might be hyperbolic in one region of R2 but parabolic in another.
Of course, the terminology as to type remains the same when space variable y is
replaced by time variable t.
An example of an elliptic PDE is the Poisson equation from electrostatics [10]
Vxx + Vyy = ‚àí1
œµ œÅ(x, y),
(12.4)
where the solution V (x, y) is the electrical potential (e.g., in units of volts) at the
spatial location (x, y) in R2, constant œµ is the permittivity of the medium (e.g.,
in units of farads per meter), and œÅ(x, y) is the charge density (e.g., in units of
coulombs per square meter) at the spatial point (x, y). We have assumed that
the permittivity is a constant, but it can vary spatially as well. Certainly, (12.4)
1This classiÔ¨Åcation scheme is related to the classiÔ¨Åcation of conic sections on the Cartesian plane. The
general equation for such a conic on R2 is
Ax2 + Bxy + Cy2 + Dx + Ey + F = 0.
The conic is hyperbolic if B2 ‚àí4AC > 0, parabolic if B2 ‚àí4AC = 0, and is elliptic if B2 ‚àí4AC < 0.
TLFeBOOK

A BRIEF OVERVIEW OF PARTIAL DIFFERENTIAL EQUATIONS
527
has the form of (12.3), where for u(x, y) = V (x, y) we have B = 0, A = C =
1, and F(x, y, u, ux, uy) = ‚àíœÅ(x, y)/œµ. Therefore, AC ‚àíB2 = 1 > 0, conÔ¨Årming
that (12.4) is elliptic.
To develop an approximate method of solving (12.4), one may employ Ô¨Ånite
differences. For example, from Taylor series theory
‚àÇ2V (xk, yn)
‚àÇx2
= V (xk+1, yn) ‚àí2V (xk, yn) + V (xk‚àí1, yn)
h2
‚àíh2
12
‚àÇ4V (Œæk, yn)
‚àÇx4
(12.5a)
for some Œæk ‚àà[xk‚àí1, xk+1], and
‚àÇ2V (xk, yn)
‚àÇy2
= V (xk, yn+1) ‚àí2V (xk, yn) + V (xk, yn‚àí1)
œÑ 2
‚àíœÑ 2
12
‚àÇ4V (xk, Œ∑n)
‚àÇy4
(12.5b)
for some Œ∑n ‚àà[yn‚àí1, yn+1], where xk = x0 + hk, yn = y0 + œÑn [recall (12.1), and
(12.2)]. The Ô¨Ånite-difference approximation to (12.4) is thus
Vk+1,n ‚àí2Vk,n + Vk‚àí1,n
h2
+ Vk,n+1 ‚àí2Vk,n + Vk,n‚àí1
œÑ 2
= ‚àíœÅ(xk, yn)
œµ
.
(12.6)
Here k = 0, 1, . . . , M, and n = 0, 1, . . . , N. Depending on œÅ(x, y) and boundary
conditions, it is possible to rewrite (12.6) as a linear system of equations in the
unknown (approximate) potentials Vk,n. In practice, N and M may be large, and so
the linear system of equations will be of high order consisting of O(NM) unknowns
to solve for. Because of the structure of (12.6), the linear system is a sparse one,
too. It has therefore been pointed out [3,7,11] that iterative solution methods are
preferred, such as the Jacobi or Gauss-Seidel methods (recall Section 4.7). This
avoids the problems inherent in storing and manipulating large dense matrices.
Epperson [11] notes that in recent years conjugate gradient methods have begun to
displace Gauss‚ÄìSeidel/Jacobi approaches to solving large and sparse linear systems
such as are generated from (12.6). In part this is due to the difÔ¨Åculties inherent
in obtaining the optimal value for the relaxation parameter œâ (recall the deÔ¨Ånition
from Section 4.7).
An example of a parabolic PDE is sometimes called the heat equation, or diffu-
sion equation [2‚Äì4, 11] since it models one-dimensional diffusion processes such
as the Ô¨Çow of heat through a metal bar. The general form of the basic parabolic
PDE is
ut = Œ±2uxx
(12.7)
for x ‚àà[0, L], t ‚ààR+. Here u(x, t) could be the temperature of some material at
(x, t). It could also be the concentration of some chemical substance that is diffusing
out from some source. (Of course, other physical interpretations are possible.)
Typical boundary conditions are
u(0, t) = 0,
u(L, t) = 0 for t > 0,
(12.8a)
TLFeBOOK

528
NUMERICAL SOLUTION OF PARTIAL DIFFERENTIAL EQUATIONS
and the initial condition is
u(x, 0) = f (x).
(12.8b)
The initial condition might be an initial temperature distribution, or chemical con-
centration. If u(x, t) is interpreted as temperature, then the boundary conditions
(12.8a) state that the ends of the one-dimensional medium are held at a constant
temperature of 0 (e.g., degrees Celsius).
Equation (12.7) has the form of (12.3) with B = C = 0, and hence AC ‚àíB2 =
0, which is the criterion for a parabolic PDE. Finite-difference schemes analogous
to the case for elliptic PDEs may be developed. Classically, perhaps the most
popular choice is the Crank‚ÄìNicolson method summarized by Burden and Faires
[3], but given a more detailed treatment by Epperson [11].
Another popular numerical solution technique for PDEs is the Ô¨Ånite-element
(FEL) method. It applies to a broad class of PDEs, and there are many commercially
available software packages that implement this approach for various applications
such as structural vibration analysis, or electromagnetics. However, we will not
consider the FEL method as it deserves its own textbook. The interested reader can
see Strang and Fix [5] or Brenner and Scott [6] for details. A brief introduction
appears in Burden and Faires [3].
As stated earlier, the emphasis in this book will be on wave propagation prob-
lems as modeled by hyperbolic PDEs. We now turn our attention to this class
of PDEs.
12.3
APPLICATIONS OF HYPERBOLIC PDEs
In this section we summarize two problems that illustrate how hyperbolic PDEs
arise in practice. In later sections we will see that although both involve the mod-
eling of waves propagating in physical systems, the numerical methods for their
solution are different in the two cases, and yet they have in common the application
of Ô¨Ånite-difference schemes.
12.3.1
The Vibrating String
Consider an elastic string with its ends Ô¨Åxed at the points x = 0 and x = L (so
that the string is of length L unstretched). If the string is plucked at position
x = xP (xP ‚àà(0, L)) at time t = 0 such as shown in Fig. 12.1, then it will vibrate
for t > 0. The PDE describing u(x, t), which is the displacement of the string at
position x and time t, is given by
utt = c2uxx.
(12.9)
The system of Fig. 12.1 is also characterized by the boundary conditions
u(0, t) = 0,
u(L, t) = 0 for all t ‚ààR+,
(12.10)
TLFeBOOK

APPLICATIONS OF HYPERBOLIC PDEs
529
0
L
x
u(x,t)
P
xP
u(xP,0)
Figure 12.1
An elastic string plucked at time t = 0 at point P, which is located at x = xP .
which specify that the string‚Äôs ends are Ô¨Åxed, and we have the initial conditions
u(x, 0) = f (x),
‚àÇu(x, t)
‚àÇt

t=0
= g(x),
(12.11)
which describes the initial displacement, and velocity of the string, respectively.
As explained, for example, in Kreyszig [1] or in Elmore and Heald [12], the PDE
(12.9) is derived from elementary Newtonian mechanics based on the following
assumptions:
1. The mass of the string per unit of length is a constant.
2. The string is perfectly elastic, offers no resistance to bending, and there is
no friction.
3. The tension in stretching the string before Ô¨Åxing its ends is large enough to
neglect the action of gravity.
4. The motion of the string is purely a vibration in the vertical plane (i.e., the
y direction), and the deÔ¨Çection and slope are small in absolute value.
We will omit the details of the derivation of (12.9), as this would carry us too far
off course. However, note that constant c2 in (12.9) is
c2 = T
œÅ ,
(12.12)
where T is the tension in the string (e.g., units of newtons) and œÅ is the density of
the string (e.g., units of kilograms per meter). A dimensional analysis of (12.12)
quickly reveals that c has the units of speed (e.g, meters per second). It speciÔ¨Åes
the speed at which waves propagate on the string.
TLFeBOOK

530
NUMERICAL SOLUTION OF PARTIAL DIFFERENTIAL EQUATIONS
It is easy to conÔ¨Årm that (12.9) is a hyperbolic PDE since, on comparing (12.9)
with (12.3), we have A = c2, B = 0, and C = ‚àí1. Thus, AC ‚àíB2 = ‚àíc2 < 0,
which meets the deÔ¨Ånition of a hyperbolic PDE.
At this point we summarize a standard method for obtaining series-based analyt-
ical solutions to PDEs. This is the method of separation of variables (also called the
product method). We shall also Ô¨Ånd that Fourier series expansions (recall Chapters 1
and 3) have an important role to play in the solution method. The solutions we
obtain yield test cases that we can use to gauge the accuracy of numerical methods
that we consider later on.
Assume that the solution2 to (12.9) can be rewritten in the form
u(x, t) = X(x)T (t).
(12.13)
Clearly
uxx = XxxT,
utt = XTtt,
(12.14)
which may be substituted into (12.9), yielding
XTtt = c2T Xxx,
(12.15)
or equivalently
Ttt
c2T = Xxx
X .
(12.16)
The expression on the left-hand side is a function of t only, while that on the
right-hand side is a function only of x. Thus, both sides must equal some constant,
say, Œ∫:
Ttt
c2T = Xxx
X
= Œ∫.
(12.17)
From (12.17) we obtain two second-order linear ODEs in constant coefÔ¨Åcients
Xxx ‚àíŒ∫X = 0
(12.18)
and
Ttt ‚àíc2Œ∫T = 0.
(12.19)
We will now ascertain the general form of the solutions to (12.18) and (12.19),
based on the conditions (12.10) and (12.11). From (12.10) substituted into (12.13),
we obtain
u(0, t) = X(0)T (t) = 0,
u(L, t) = X(L)T (t) = 0.
2Theories about the existence and uniqueness of solutions to PDEs are often highly involved, and so
we completely ignore this matter here. The reader is advised to consult books dedicated to PDEs and
their solution for such information.
TLFeBOOK

APPLICATIONS OF HYPERBOLIC PDEs
531
If T (t) = 0 (all t), then u(x, t) = 0 for all x and t. This is the trivial solution, and
we reject it. Thus, we must have
X(0) = X(L) = 0.
(12.20)
For Œ∫ = 0, Eq. (12.18) is Xxx = 0, which has the general solution X(x) = ax + b,
but from (12.20) we conclude that a = b = 0, and so X(x) = 0 for all x. This is the
trivial solution and so is rejected. If Œ∫ = ¬µ2 > 0, we have ODE Xxx ‚àí¬µ2X = 0,
which has a characteristic equation possessing roots at ¬±¬µ. Consequently, X(x) =
ae¬µx + be‚àí¬µx. If we apply (12.20) to this, we conclude that a = b = 0, once
again giving the trivial solution X(x) = 0 for all x. Now Ô¨Ånally suppose that
Œ∫ = ‚àíŒ≤2 < 0, in which case (12.18) becomes
Xxx + Œ≤2X = 0,
(12.21)
which has the characteristic equation s2 + Œ≤2 = 0. Thus, the general solution to
(12.21) is of the form
X(x) = a cos(Œ≤x) + b sin(Œ≤x).
(12.22)
Applying (12.20) yields
X(0) = a = 0,
X(L) = b sin(Œ≤L) = 0.
(12.23)
Clearly, to avoid encountering the trivial solution, we must assume that b Ã∏= 0.
Thus, we must have
sin(Œ≤L) = 0,
implying that we have
Œ≤ = nœÄ
L ,
n ‚ààZ.
(12.24)
However, we avoid Œ≤ = 0 (for n = 0) to prevent X(x) = 0 for all x; and we
consider only n ‚àà{1, 2, 3, . . .} = N because sin(‚àíx) = ‚àísin x, and the minus sign
can be absorbed into the constant b. Thus, in general
X(x) = Xn(x) = bn sin
nœÄ
L x

(12.25)
for n ‚ààN, and where x ‚àà[0, L]. So now we have found that
Œ∫ = ‚àíŒ≤2 = ‚àí
nœÄ
L
2
,
in which case (12.19) takes on the form
Ttt + Œª2
nT = 0,
where
Œªn = nœÄc
L .
(12.26)
TLFeBOOK

532
NUMERICAL SOLUTION OF PARTIAL DIFFERENTIAL EQUATIONS
This has a general solution of the form
Tn(t) = An cos(Œªnt) + Bn sin(Œªnt)
(12.27)
again for n ‚ààN. Consequently, un(x, t) = Xn(x)Tn(t) is a solution to (12.9) for
all n ‚ààN, and
un(x, t) = [An cos(Œªnt) + Bn sin(Œªnt)] sin
nœÄ
L x

.
(12.28)
The functions (12.28) are eigenfunctions with eigenvalues Œªn = nœÄc/L for the PDE
in (12.9). The set {Œªn|n ‚ààN} is the spectrum. Each un(x, t) represents harmonic
motion of the string with frequency Œªn/(2œÄ) cycles per unit of time, and is also
called the nth normal mode for the string. The Ô¨Årst mode for n = 1 is called the
fundamental mode, and the others (for n > 1) are called overtones, or harmonics.
It is clear that un(x, t) in (12.28) satisÔ¨Åes PDE (12.9), and the boundary conditions
(12.10). However, un(x, t) by itself will not satisfy (12.9), (12.10) and (12.11)
all simultaneously. In general, the complete solution is [using superposition as the
PDE (12.9) is linear]
u(x, t) =
‚àû

n=1
un(x, t) =
‚àû

n=1
[An cos(Œªnt) + Bn sin(Œªnt)] sin
nœÄ
L x

,
(12.29)
where the initial conditions (12.11) are employed to Ô¨Ånd the series coefÔ¨Åcients An
and Bn for all n. We will now consider how this is done in general.
From (12.11) we obtain
u(x, 0) =
‚àû

n=1
An sin
nœÄ
L x

= f (x)
(12.30)
and
‚àÇu(x, t)
‚àÇt
|t=0 =
 ‚àû

n=1
[‚àíAnŒªn sin(Œªnt) + BnŒªn cos(Œªnt)] sin
nœÄ
L x
	
t=0
= g(x)
or
‚àû

n=1
BnŒªn sin
nœÄ
L x

= g(x).
(12.31)
The orthogonality properties of sinusoids can be used to determine An and Bn
for all n. Note that (12.30) and (12.31) are particular instances of Fourier series
expansions. In particular, observe that for k, n ‚ààN
 L
0
sin
nœÄ
L x

sin
 kœÄ
L x
!
dx =
 0,
n Ã∏= k
L/2,
n = k
.
(12.32)
TLFeBOOK

APPLICATIONS OF HYPERBOLIC PDEs
533
Plainly, set {sin(nœÄx/L)|n ‚ààN} is an orthogonal set. Thus, from (12.30)
2
L
 L
0
sin
 kœÄ
L x
! 
 ‚àû

n=1
An sin
nœÄ
L x

dx = 2
L
 L
0
f (x) sin
 kœÄ
L x
!
dx,
and via (12.32) this reduces to
Ak = 2
L
 L
0
f (x) sin
 kœÄ
L x
!
dx,
(12.33)
and similarly from (12.31), and (12.32)
Bk =
2
ŒªkL
 L
0
g(x) sin
 kœÄ
L x
!
dx.
(12.34)
Example 12.1
Suppose that g(x) = 0 for all x. Thus, the initial velocity of the
string is zero. Let the initial position (deÔ¨Çection) of the plucked string be triangular
such that
f (x) =
Ô£±
Ô£¥Ô£¥Ô£≤
Ô£¥Ô£¥Ô£≥
2H
L x,
0 < x ‚â§L
2
2H
L (L ‚àíx),
L
2 < x < L
.
(12.35)
In Fig. 12.1 this corresponds to xP = L/2 and u(xP , 0) = H. Since g(x) = 0 for
all x via (12.34), we must have Bk = 0 for all k. From (12.33) we have
Ak = 4H
L2
 L/2
0
x sin
 kœÄ
L x
!
dx +
 L
L/2
(L ‚àíx) sin
 kœÄ
L x
!
dx
	
.
Since

sin(ax) dx = ‚àí1
a cos(ax) + C
and

x sin(ax) dx = ‚àí1
a x cos(ax) + 1
a2 sin(ax) + C
(C is a constant of integration), on simpliÔ¨Åcation we have
Ak = 8H
k2œÄ2 sin
 kœÄ
2
!
.
(12.36)
TLFeBOOK

534
NUMERICAL SOLUTION OF PARTIAL DIFFERENTIAL EQUATIONS
0
0.2
0.4
0.6
0.8
1
0
5
10
15
20
‚àí0.1
‚àí0.05
0
0.05
0.1
Position (x )
Time (t )
Displacement [u (x,t )]
Figure 12.2
Fourier series solution to the vibrating string problem. A mesh plot of u(x, t)
as given by Eq. (12.37) for the parameters L = 1, c/L = 1
8, and H = 1
10. The plot employed
the Ô¨Årst 100 terms of the series expansion.
Thus, substituting (12.36) into (12.29) yields the general solution for our example,
namely
u(x, t) = 8H
œÄ2
‚àû

k=1
1
k2 sin
 kœÄ
2
!
cos
 kœÄc
L t
!
sin
 kœÄ
L x
!
,
but sin(kœÄ/2) = 0 for even k, and so this expression reduces to
u(x, t) = 8H
œÄ2
‚àû

n=1
(‚àí1)n‚àí1
(2n ‚àí1)2 cos
 (2n ‚àí1)œÄc
L
t
!
sin
 (2n ‚àí1)œÄ
L
x
!
.
(12.37)
Figure 12.2 shows a typical plot of the function u(x, t) as given by (12.37) (for
the parameters stated in the Ô¨Ågure caption).
The reader is encouraged to think about Fig. 12.2, and to ask if the picture is
a reasonable one on the basis of his/her intuitive understanding of how a plucked
string (say, that on a stringed musical instrument) behaves. Of course, this question
should be considered with respect to the modeling assumptions that lead to (12.9),
and that were listed earlier.
12.3.2
Plane Electromagnetic Waves
An electromagnetic wave (e.g., radio wave or light) in three-dimensional space R3
within some material is described by the vector magnetic Ô¨Åeld intensity H(x, y, z, t)
TLFeBOOK

APPLICATIONS OF HYPERBOLIC PDEs
535
[e.g., in units of amperes per meter, (A/m)] and the vector electric Ô¨Åeld intensity
E(x, y, z, t) [e.g., in units of volts per meter (V/m)] such that
H(x, y, z, t) = Hx(x, y, z, t)ÀÜx + Hy(x, y, z, t) ÀÜy + Hz(x, y, z, t)ÀÜz,
E(x, y, z, t) = Ex(x, y, z, t)ÀÜx + Ey(x, y, z, t) ÀÜy + Ez(x, y, z, t)ÀÜz,
where ÀÜx, ÀÜy, and ÀÜz are the unit vectors in the x, y, and z directions of R3, respec-
tively. The dynamic equations that H and E both satisfy are Maxwell‚Äôs equations:
‚àá√ó E = ‚àí‚àÇB
‚àÇt
(Faraday‚Äôs law),
(12.38)
‚àá√ó H = ‚àÇD
‚àÇt
(Ampere‚Äôs law).
(12.39)
Here the material in which the wave propagates contains no charges or cur-
rent sources. The magnetic Ô¨Çux density B(x, y, z, t), and the electric Ô¨Çux density
D(x, y, z, t) are assumed to satisfy
D = œµE,
B = ¬µH.
(12.40)
These relations assume that the material is linear, isotropic [i.e., the same in all
directions, and homogeneous i.e., the parameters œµ and ¬µ do not vary with (x, y, z)].
Constant œµ is the material‚Äôs permittivity [units of farads per meter (F/m)], and
constant ¬µ is the material‚Äôs permeability [units of henries per meter (H/m)]. The
permittivity and permeability of free space (i.e., a vacuum) are often denoted by
œµ0 and ¬µ0, respectively, and
œµ = œµrœµ0,
¬µ = ¬µr¬µ0,
(12.41)
where œµr is the relative permittivity and ¬µr is the relative permeability of the
material. Note that
œµ0 = 8.854185 √ó 10‚àí12 F/m,
¬µ0 = 400œÄ √ó 10‚àí9 H/m.
(12.42)
If the material is air, then œµr ‚âà1 and ¬µr ‚âà1 to very good approximation, and
so air is not practically distinguished (usually) from free space. For commonly
occurring dielectric materials (i.e., insulators), we have ¬µr ‚âà1, also to excellent
approximation. On the other hand, for magnetic materials (e.g., iron, cobalt, nickel,
various alloys and mixtures), ¬µr will be very different from unity, and in fact
the relationship B = ¬µH must often be replaced by sometimes quite complicated
nonlinear relationships, often involving the phenomenon of hysteresis. But we will
completely avoid this situation here.
TLFeBOOK

536
NUMERICAL SOLUTION OF PARTIAL DIFFERENTIAL EQUATIONS
In general, for the vector Ô¨Åeld A = Ax ÀÜx + Ay ÀÜy + AzÀÜz, the curl is the determinant
‚àá√ó A =

ÀÜx
ÀÜy
ÀÜz
‚àÇ
‚àÇx
‚àÇ
‚àÇy
‚àÇ
‚àÇz
Ax
Ay
Az

,
(12.43)
so, expanding this expression with A = E and A = H gives (respectively)
‚àá√ó E = ÀÜx
 ‚àÇEz
‚àÇy ‚àí‚àÇEy
‚àÇz
!
+ ÀÜy
 ‚àÇEx
‚àÇz ‚àí‚àÇEz
‚àÇx
!
+ ÀÜz
 ‚àÇEy
‚àÇx ‚àí‚àÇEx
‚àÇy
!
,
(12.44)
and
‚àá√ó H = ÀÜx
 ‚àÇHz
‚àÇy ‚àí‚àÇHy
‚àÇz
!
+ ÀÜy
 ‚àÇHx
‚àÇz ‚àí‚àÇHz
‚àÇx
!
+ ÀÜz
 ‚àÇHy
‚àÇx ‚àí‚àÇHx
‚àÇy
!
.
(12.45)
We will consider only transverse electromagnetic (TEM) waves (i.e., plane
waves). If such a wave propagates in the x direction, then we may assume that
Ex = Ez = 0, and so Hx = Hy = 0. Note that the electric and magnetic Ô¨Åeld com-
ponents Ey and Hz are orthogonal to each other. They lie within the (y, z) plane,
which itself is orthogonal to the direction of travel of the plane wave. From (12.44)
and (12.45), Maxwell‚Äôs equations (12.38) and (12.39) reduce to
‚àÇEy
‚àÇx = ‚àí¬µ‚àÇHz
‚àÇt ,
‚àÇHz
‚àÇx = ‚àíœµ ‚àÇEy
‚àÇt ,
(12.46)
where we have used (12.40). Combining the two equations in (12.46), we obtain
either
‚àÇ2Hz
‚àÇx2 = ¬µœµ ‚àÇ2Hz
‚àÇt2 ,
(12.47)
which is the wave equation for the magnetic Ô¨Åeld, or
‚àÇ2Ey
‚àÇx2 = ¬µœµ ‚àÇ2Ey
‚àÇt2 ,
(12.48)
which is the wave equation for the electric Ô¨Åeld. If we deÔ¨Åne
v =
1
‚àö¬µœµ ,
(12.49)
then the general solution to (12.48) (for example) can be expressed in the form
Ey(x, t) = Eyr(x ‚àívt) + Eyl(x + vt),
(12.50)
TLFeBOOK

APPLICATIONS OF HYPERBOLIC PDEs
537
where the Ô¨Årst term is a wave propagating in the +x direction (i.e., to the right)
with speed v and the second term is a wave propagating in the ‚àíx direction (i.e.,
to the left) with speed v.3 Equation (12.50) is the classical D‚ÄôAlembert solution to
the scalar wave equation (12.48). Clearly, similar reasoning applies to (12.47). Of
course, using (12.49) in (12.48), we can write
‚àÇ2Ey
‚àÇt2
= v2 ‚àÇ2Ey
‚àÇx2
(12.51)
which has the same form as (12.9). In short, the mathematics describing the vibra-
tions of mechanical systems is much the same as that describing electromagnetic
systems, only the physical interpretations differ. Of course, (12.51) clearly implies
that (12.47) and (12.48) are hyperbolic PDEs.
Example 12.2
It is easy to conÔ¨Årm that (12.37) can be rewritten in the form
of (12.50). Via the identity
cos A sin B = 1
2[sin(A + B) ‚àísin(A ‚àíB)]
(12.52)
we see that
cos
(2n ‚àí1)œÄc
L
t

sin
(2n ‚àí1)œÄ
L
x

= 1
2 sin
(2n ‚àí1)œÄ
L
(x + ct)

+1
2 sin
(2n ‚àí1)œÄ
L
(x ‚àíct)

.
Thus, (12.37) may immediately be rewritten as
u(x, t) = 4H
œÄ2
‚àû

n=1
(‚àí1)n‚àí1
(2n ‚àí1)2 sin
(2n ‚àí1)œÄ
L
(x ‚àíct)

#
$%
&
=u1(x‚àíct)
+ 4H
œÄ2
‚àû

n=1
(‚àí1)n‚àí1
(2n ‚àí1)2 sin
(2n ‚àí1)œÄ
L
(x + ct)

#
$%
&
=u1(x+ct)
.
We note that when œµr = ¬µr = 1, we have v = c, where
c =
1
‚àö¬µ0œµ0
.
(12.53)
3Readers are invited to draw a simple sketch and convince themselves that this interpretation is cor-
rect. This interpretation is vital in understanding the propagation of electromagnetic waves in layered
materials, such as thin optical Ô¨Ålms.
TLFeBOOK

538
NUMERICAL SOLUTION OF PARTIAL DIFFERENTIAL EQUATIONS
This is the speed of light in a vacuum. Since for real materials ¬µr > 1 and œµr > 1,
we have v < c, so an electromagnetic wave cannot travel at a speed exceeding that
of light in a vacuum.
Now we will assume sinusoidal solutions to the wave equations such as would
originate from sinusoidal sources.4 SpeciÔ¨Åcally, let us assume that
Ey(x, t) = E0 sin(œât ‚àíŒ≤x),
(12.54)
where Œ≤ = œâ‚àö¬µœµ = 2œÄ/Œª, and Œª is the wavelength (e.g., in units of meters). The
frequency of the source is œâ, a Ô¨Åxed constant, and so the wavelength will vary
depending on the medium. If the free-space wavelength is denoted Œª0, then
2œÄ
Œª0
= œâ
c ,
(12.55)
where c is from (12.53). If the free-space wave then propagates into a denser
material, then
2œÄ
Œª = œâ
v
(12.56)
for v given by (12.49). From (12.55) and (12.56), we obtain
Œª = v
c Œª0.
(12.57)
Since v ‚â§c, we always have Œª ‚â§Œª0; that is, the wavelength will shorten. This
observation is useful in checking numerical methods that model the propagation
of sinusoidal waves across interfaces between different materials (e.g., layered
structures such as thin optical Ô¨Ålms).
From (12.54) we have
‚àÇEy
‚àÇx = ‚àíŒ≤E0 cos(œât ‚àíŒ≤x)
so that from the Ô¨Årst equation in (12.46) we have
Hz =
 Œ≤E0
¬µ
cos(œât ‚àíŒ≤x) dt = Œ≤E0
¬µœâ sin(œât ‚àíŒ≤x).
(12.58)
The characteristic impedance of the medium in which the sinusoidal electromag-
netic wave travels is deÔ¨Åned to be
Z = Ey
Hz
= ¬µœâ
Œ≤
=
8¬µ
œµ =
8¬µr
œµr
Z0,
(12.59)
4For example, the Colpitts oscillator from Chapter 10 could operate as a sinusoidal signal generator to
drive an antenna, thus producing a sinusoidal electromagnetic wave (radio wave) in space.
TLFeBOOK

APPLICATIONS OF HYPERBOLIC PDEs
539
where Z0 = ‚àö¬µ0/œµ0 is the characteristic impedance of free space. The units of
Z are in ohms (). We see that Z is analogous to the concept of impedance that
arises in electric circuit analysis.
The analogy between our present problem and phasor analysis in basic electric
circuit theory can be exploited. For suitable E(x) ‚ààR
Ey = Ey(x, t) = E(x)ejœât
(12.60)
so that
‚àÇ2Ey
‚àÇx2 = d2E(x)
dx2
ejœât,
‚àÇ2Ey
‚àÇt2
= ‚àíœâ2E(x)ejœât.
(12.61)
Substituting (12.61) into (12.48) yields
d2E(x)
dx2
ejœât = ‚àí¬µœµœâ2E(x)ejœât,
which reduces to the second-order linear ODE
d2E(x)
dx2
+ ¬µœµœâ2E(x) = 0.
(12.62)
For convenience, we deÔ¨Åne the propagation constant
Œ≥ = jœâ‚àö¬µœµ = jŒ≤,
(12.63)
so ‚àíŒ≥ 2 = ¬µœµœâ2, and (12.62) is now
d2E
dx2 ‚àíŒ≥ 2E = 0
(12.64)
(E = E(x)). This ODE has a general solution of the form
E(x) = E0e‚àíŒ≥ x + E1eŒ≥ x,
(12.65)
where E0 and E1 are constants. Recalling (12.60), it follows that
Ey(x, t) = E0ejœât‚àíŒ≥ x + E1ejœât+Œ≥ x
= E0e‚àíj(Œ≤x‚àíœât) + E1ej(Œ≤x+œât).
(12.66)
Of course, the Ô¨Årst term is a wave propagating to the right, and the second term is
a wave propagating to the left.
So far we have assumed wave propagation in lossless materials since this is
the easiest case to consider at the outset. We shall now consider the effects of
lossy materials on propagation. This will be important in that it is a more realistic
TLFeBOOK

540
NUMERICAL SOLUTION OF PARTIAL DIFFERENTIAL EQUATIONS
assumption in practice, and it is important in designing perfectly matched layers
(PMLs) in the Ô¨Ånite-difference time-domain (FDTD) method, as will be seen later.
We may deÔ¨Åne electrical conductivity œÉ [units of amperes per volt-meter
[A/(V¬∑m)] or mhos/meter], and magnetic conductivity œÉ ‚àó[units of volts per
ampere-meter [V/(A¬∑m)] or ohms/meter]. In this case (12.38) and (12.39) take
on the more general forms.
‚àá√ó E = ‚àí¬µ‚àÇH
‚àÇt ‚àíœÉ ‚àóH,
(12.67)
‚àá√ó H = œµ ‚àÇE
‚àÇt + œÉE.
(12.68)
Note that œÉ ‚àóis not the complex conjugate of œÉ. In fact, œÉ, œÉ ‚àó‚ààR with œÉ > 0
for a lossy material (i.e., an imperfect insulator, or a conductor), and œÉ ‚àó‚â•0. As
before we will assume E possesses only a y component, and H possesses only a z
component. Since we again have propagation only in the x direction, via (12.44),
(12.45) in (12.67), and (12.68), we have
‚àÇEy
‚àÇx = ‚àí¬µ‚àÇHz
‚àÇt
‚àíœÉ ‚àóHz,
(12.69)
‚àí‚àÇHz
‚àÇx = œµ ‚àÇEy
‚àÇt
+ œÉEy.
(12.70)
If Ey = Ey(x, t) possesses a term that propagates only to the right, then (using
phasors again)
E = Ey(x, t) ÀÜy = E0ejœâte‚àíŒ≥ x ÀÜy
(12.71)
for some suitable propagation constant Œ≥ . For suitable characteristic impedance Z,
we must have
H = Hz(x, t)ÀÜz = 1
Z E0ejœâte‚àíŒ≥ x ÀÜz.
(12.72)
We may use (12.69) and (12.70) to determine Œ≥ and Z. Substituting (12.71) and
(12.72) into (12.69) and (12.70) and solving for Œ≥ and Z yields
Z2 = jœâ¬µ + œÉ ‚àó
jœâœµ + œÉ ,
Œ≥ 2 = (jœâœµ + œÉ)(jœâ¬µ + œÉ ‚àó).
(12.73)
How to handle the complex square roots needed to obtain Z and Œ≥ will be dealt
with below. Observe that the equations in (12.73) reduce to the previous cases
(12.59), and (12.63) when œÉ = œÉ ‚àó= 0. It is noteworthy that when we have the
condition
œÉ ‚àó
¬µ = œÉ
œµ ,
(12.74)
then we have
Z2 =
jœâ¬µ + ¬µ
œµ œÉ
jœâœµ + œÉ
= ¬µ
œµ
jœâ + œÉ
œµ
jœâ + œÉ
œµ
= ¬µ
œµ .
(12.75)
TLFeBOOK

APPLICATIONS OF HYPERBOLIC PDEs
541
Condition (12.74) is what makes the creation of a PML possible, as will be con-
sidered later in this section, and will be demonstrated in Section 12.5.
Now we must investigate what happens to waves when they encounter a sudden
change in the material properties, speciÔ¨Åcally, an interface between layers. This
situation is depicted in Fig. 12.3. Assume that medium 1 has physical parameters
œµ1, ¬µ1, œÉ1, and œÉ ‚àó
1 , while medium 2 has physical parameters œµ2, ¬µ2, œÉ2, and œÉ ‚àó
2 .
The corresponding characteristic impedance and propagation constant for medium
1 is thus [via (12.73)]
Z2
1 = jœâ¬µ1 + œÉ ‚àó
1
jœâœµ1 + œÉ1
,
Œ≥ 2
1 = (jœâœµ1 + œÉ1)(jœâ¬µ1 + œÉ ‚àó
1 ),
(12.76)
while for medium 2 we have
Z2
2 = jœâ¬µ2 + œÉ ‚àó
2
jœâœµ2 + œÉ2
,
Œ≥ 2
2 = (jœâœµ2 + œÉ2)(jœâ¬µ2 + œÉ ‚àó
2 ).
(12.77)
In Fig. 12.3 for some constants E and H we have for the incident Ô¨Åeld
Ei = Ei ÀÜy = Eejœâte‚àíŒ≥1x ÀÜy,
H i = Hi ÀÜz = Hejœâte‚àíŒ≥1x ÀÜz
x
y
z
(Out of the page)
Reflected
wave
Transmitted
wave
Incident
wave
Hi
Et
Ht
Er
Hr
Ei
Interface (boundary) between
two different media
Medium 1
Medium 2
Figure 12.3
A plane wave normally incident on an interface (boundary) between two
different media. The magnetic Ô¨Åeld components are directed orthogonal to the page. The
interface is at x = 0 and is the yz‚àíplane.
TLFeBOOK

542
NUMERICAL SOLUTION OF PARTIAL DIFFERENTIAL EQUATIONS
so that
‚àÇHi
‚àÇt
= jœâHi,
‚àÇEi
‚àÇx = ‚àíŒ≥1Ei,
and via (12.69)
‚àíŒ≥1Ei = ‚àíjœâ¬µ1Hi ‚àíœÉ ‚àó
1 Hi,
implying that
Ei
Hi
= jœâ¬µ1 + œÉ ‚àó
1
Œ≥1
= Z1.
(12.78)
Similarly, for the transmitted Ô¨Åeld, we must have
Et
Ht
= jœâ¬µ2 + œÉ ‚àó
2
Œ≥2
= Z2.
(12.79)
But, for the reÔ¨Çected Ô¨Åeld, again for suitable constants E‚Ä≤, and H ‚Ä≤ we have
Er = Er ÀÜy = E‚Ä≤ejœâteŒ≥1x ÀÜy,
H r = Hr ÀÜz = H ‚Ä≤ejœâteŒ≥1x ÀÜz
so that
‚àÇHr
‚àÇt
= jœâHr,
‚àÇEr
‚àÇx = Œ≥1Er,
and via (12.69)
Œ≥1Er = ‚àíjœâ¬µ1Hr ‚àíœÉ ‚àó
1 Hr,
implying
Er
Hr
= ‚àíjœâ¬µ1 + œÉ ‚àó
1
Œ≥1
= ‚àíZ1.
(12.80)
The electric and magnetic Ô¨Åeld components are tangential to the interface, and so
must be continuous across it. This implies that at x = 0, and for all t (in Fig. 12.3)
Hi + Hr = Ht,
Ei + Er = Et.
(12.81)
If we substitute (12.78)‚Äì(12.80) into (12.81), then after a bit of algebra, we have
Et
Ei
=
2Z2
Z2 + Z1
= œÑ
(12.82)
and
Er
Ei
= Z2 ‚àíZ1
Z2 + Z1
= œÅ.
(12.83)
It is easy to conÔ¨Årm that
œÑ = 1 + œÅ.
(12.84)
TLFeBOOK

APPLICATIONS OF HYPERBOLIC PDEs
543
We call œÑ the transmission coefÔ¨Åcient from medium 1 into medium 2, and œÅ is the
reÔ¨Çection coefÔ¨Åcient from medium 1 into medium 2. The coefÔ¨Åcients œÑ and œÅ are
often called Fresnel coefÔ¨Åcients, especially in the Ô¨Åeld of optics.
If œÉ ‚àó= 0 then from (12.73) the propagation constant for a sinusoidal wave in
a conductor is obtained from
Œ≥ 2 = ‚àí¬µœµœâ2 + jœâ¬µœÉ,
(12.85)
where ¬µœµœâ2 > 0 and œâ¬µœÉ ‚â•0. We may express Œ≥ 2 in polar form: Œ≥ 2 = r1ejŒ∏1.
But Œ≥ = rejŒ∏ = [r1ejŒ∏1]1/2. In general, if z = rejŒ∏, then
z1/2 = r1/2ejŒ∏/2,
or
z1/2 = r1/2ej(Œ∏/2+œÄ).
(12.86)
From (12.85)
r1 = |Œ≥ 2| = œâ¬µ

œÉ 2 + œµ2œâ2,
Œ∏1 = œÄ
2 + tan‚àí1  œµ
œÉ œâ

.
(12.87)
Consequently
Œ≥ = ¬±[œâ¬µ

œÉ 2 + œµ2œâ2]1/2ej[ œÄ
4 + 1
2 tan‚àí1(œµœâ/œÉ)].
(12.88)
A special case is the perfect insulator (perfect dielectric) for which œÉ = 0. Since
tan‚àí1(‚àû) = œÄ/2, Eq. (12.88) reduces to Œ≥ = ¬±j‚àö¬µœµœâ = ¬±jŒ≤. More generally
(œÉ not necessarily zero) we have Œ≥ = ¬±(Œ± + jŒ≤), where
Œ± = [œâ¬µ

œÉ 2 + œµ2œâ2]1/2 cos
œÄ
4 + 1
2 tan‚àí1  œµ
œÉ œâ

,
(12.89a)
Œ≤ = [œâ¬µ

œÉ 2 + œµ2œâ2]1/2 sin
œÄ
4 + 1
2 tan‚àí1  œµ
œÉ œâ

.
(12.89b)
Example 12.3
Assume that ¬µ = ¬µ0, œµ = œµ0, and that œÉ = 0.0001 mhos/meter.
For œâ = 2œÄf [f is the sinusoid‚Äôs frequency in Hertz (Hz)] from (12.89), we obtain
the following table:
f (Hz)
Œ±
Œ≤
1 √ó 106
.015236
0.025911
1 √ó 107
.018762
0.210423
1 √ó 108
.018836
2.095929
1 √ó 109
.018837
20.958455
Keeping the parameters the same, except that now œÉ = 0.01 mhos/meter, we
have the following table:
TLFeBOOK

544
NUMERICAL SOLUTION OF PARTIAL DIFFERENTIAL EQUATIONS
f (Hz)
Œ±
Œ≤
1 √ó 106
.198140
0.199245
1 √ó 107
.611091
0.646032
1 √ó 108
1.523602
2.591125
1 √ó 109
1.876150
21.042254
In general, from (12.89a) we have Œ± ‚â•0. If in Fig. 12.3 medium 1 is free space
while medium 2 is a conductor with 0 < œÉ < ‚àû, then Et has the form [using
(12.82)] for x ‚â•0
Et = œÑEejœâte‚àíŒ≥2x ÀÜy = œÑEejœâte‚àíŒ±2xe‚àíjŒ≤2x ÀÜy.
(12.90)
Of course, we also have Ei = Eejœâte‚àíjŒ≤1x ÀÜy for x ‚â§0 since Œ≥1 = jŒ≤1 (i.e., Œ±1 = 0
in free space), and Er = œÅEejœâtejŒ≤1x ÀÜy for x ‚â§0 [using (12.83)]. Since Œ±2 > 0, the
factor e‚àíŒ±2x will go to zero as x ‚Üí‚àû. The amplitude of the wave must decay as
it progresses from free space (medium 1) into the conductive medium (medium 2).
The rate of decay certainly depends on the size of Œ±2.
Now suppose that medium 1 is again free space, but that medium 2 has both
œÉ2 > 0 and œÉ ‚àó
2 > 0 such that condition (12.74) holds with ¬µ2 = ¬µ0, and œµ2 = œµ0,
speciÔ¨Åcally
œÉ ‚àó
2
¬µ0
= œÉ2
œµ0
(12.91)
which implies [via (12.75)] that Z2 = ‚àö¬µ0/œµ0. Since medium 1 is free space Z1 =
‚àö¬µ0/œµ0, too. The reÔ¨Çection coefÔ¨Åcient from medium 1 into medium 2 is [via
(12.83)]
œÅ = Z2 ‚àíZ1
Z2 + Z1
= Z0 ‚àíZ0
2Z0
= 0.
When wave Ei in medium 1 encounters the interface (at x = 0 in Fig. 12.3), there
will be no reÔ¨Çected component, that is, we will have Er = 0. From (12.73) we
obtain
Œ≥ 2
2 = (œÉ2œÉ ‚àó
2 ‚àíœâ2¬µ0œµ0) + jœâ(œÉ2¬µ0 + œÉ ‚àó
2 œµ0),
(12.92)
and we select the medium 2 parameters so that for Œ≥2 = Œ±2 + jŒ≤2 we obtain Œ±2 > 0,
and Œ±2 is large enough so that the wave is rapidly attenuated in that e‚àíŒ±2x is small
for relatively small x. In this case we may deÔ¨Åne medium 2 to be a perfectly
matched layer (PML). It is perfectly matched in the sense that its characteristic
impedance is the same as that of medium 1, thus eliminating reÔ¨Çections at the
interface. Because it is lossy, it absorbs radiation incident on it. The layer dissipates
energy without reÔ¨Çection. It thus simulates the walls of an anechoic chamber. In
other words, an anechoic chamber has walls that approximately realize condition
(12.74). The necessity to simulate the walls of an anechoic chamber will become
clearer in Section 12.5 when we look at the FDTD method.
TLFeBOOK

THE FINITE-DIFFERENCE (FD) METHOD
545
Finally, we remark on the similarities between the vibrating string problem
and the problem considered here. The analytical solution method employed in
Section 12.3.1 was separation of variables, and we have employed the same ap-
proach here since all of our electromagnetic Ô¨Åeld solutions are of the form u(x, t) =
X(x)T (t). The main difference is that in the vibrating string problem we have
boundary conditions deÔ¨Åned by the ends of the string being tied down somewhere,
while in the electromagnetic wave propagation problem as we have considered it
here there are no boundaries, or rather, the boundaries are at x = ¬±‚àû.
12.4
THE FINITE-DIFFERENCE (FD) METHOD
We now consider a classical approach to the numerical solution of (12.9) that we
call the Ô¨Ånite-difference (FD) method. Note that the method to follow is by no means
the only approach. Indeed, the FDTD method to be considered in Section 12.5 is
an alternative, and there are still others.
Following (12.5)
‚àÇ2u(xk, tn)
‚àÇt2
= u(xk, tn+1) ‚àí2u(xk, tn) + u(xk, tn‚àí1)
œÑ 2
‚àíœÑ 2
12
‚àÇ4u(xk, Œ∑n)
‚àÇt4
(12.93)
for some Œ∑n ‚àà[tn‚àí1, tn+1], and
‚àÇ2u(xk, tn)
‚àÇx2
= u(xk+1, tn) ‚àí2u(xk, tn) + u(xk‚àí1, tn)
h2
‚àíh2
12
‚àÇ4u(Œæk, tn)
‚àÇx4
(12.94)
for some Œæk ‚àà[xk‚àí1, xk+1], where
xk = kh,
tn = nœÑ
(12.95)
for k = 0, 1, . . . , M, and n ‚ààZ+. On substitution of (12.93) and (12.94) into (12.9),
we have
u(xk, tn+1) ‚àí2u(xk, tn) + u(xk, tn‚àí1)
œÑ 2
‚àíc2 u(xk+1, tn) ‚àí2u(xk, tn) + u(xk‚àí1, tn)
h2
= 1
12

œÑ 2 ‚àÇ4u(xk, Œ∑n)
‚àÇt4
‚àíc2h2 ‚àÇ4u(Œæk, tn)
‚àÇx4

#
$%
&
=ek,n
,
(12.96)
where ek,n is the local truncation error. Since uk,n ‚âàu(xk, tn) from (12.96), we
obtain the difference equation
uk,n+1 ‚àí2uk,n + uk,n‚àí1 ‚àíe2uk+1,n + 2e2uk,n ‚àíe2uk‚àí1,n = 0,
(12.97)
where
e = œÑ
hc
(12.98)
TLFeBOOK

546
NUMERICAL SOLUTION OF PARTIAL DIFFERENTIAL EQUATIONS
is sometimes called the Courant parameter. It has a crucial role to play in deter-
mining the stability of the FD method (and of the FDTD method, too). If we solve
(12.97) for uk,n+1 we obtain
uk,n+1 = 2(1 ‚àíe2)uk,n + e2(uk+1,n + uk‚àí1,n) ‚àíuk,n‚àí1,
(12.99)
where k = 1, 2, . . . , M ‚àí1, and n = 1, 2, 3, . . .. Equation (12.99) is the main recur-
sion in the FD algorithm. However, we need to account for the initial and boundary
conditions in order to initialize this recursion. Before we consider this matter note
that, in the language of Section 10.2, the FD algorithm has a truncation error of
O(œÑ 2 + h2) per step [via ek,n in (12.96)].
Immediately on applying (12.10), since L = Mh, we have
u0,n = uM,n = 0
for
n ‚ààZ+.
(12.100)
Since from (12.11) u(x, 0) = f (x), we also have
uk,0 = f (xk)
(12.101)
for k = 0, 1, . . . , M. From (12.101) we have uk,0, but we also need uk,1 [consider
(12.99) for n = 1]. To obtain a suitable expression Ô¨Årst observe that from (12.9)
‚àÇ2u(x, t)
‚àÇt2
= c2 ‚àÇ2u(x, t)
‚àÇx2
‚áí‚àÇ2u(x, 0)
‚àÇt2
= c2 ‚àÇ2u(x, 0)
‚àÇx2
= c2f (2)(x).
(12.102)
Now, on applying the Taylor series expansion, we see that for some ¬µ ‚àà[0, t1] =
[0, œÑ] (and ¬µ may depend on x)
u(x, t1) = u(x, 0) + œÑ ‚àÇu(x, 0)
‚àÇt
+ 1
2œÑ 2 ‚àÇ2u(x, 0)
‚àÇt2
+ 1
6œÑ 3 ‚àÇ3u(x, ¬µ)
‚àÇt3
,
and on applying (12.11) and (12.102), this becomes
u(x, t1) = u(x, 0) + œÑg(x) + 1
2œÑ 2c2f (2)(x) + 1
6œÑ 3 ‚àÇ3u(x, ¬µ)
‚àÇt3
.
(12.103)
In particular, for x = xk, this yields
u(xk, t1) = u(xk, 0) + œÑg(xk) + 1
2œÑ 2c2f (2)(xk) + 1
6œÑ 3 ‚àÇ3u(xk, ¬µk)
‚àÇt3
.
(12.104)
If f (x) ‚ààC4[0, L], then, for some Œ∂k ‚àà[xk‚àí1, xk+1], we have
f (2)(xk) = f (xk+1) ‚àí2f (xk) + f (xk‚àí1)
h2
‚àíh2
12f (4)(Œ∂k).
(12.105)
TLFeBOOK

THE FINITE-DIFFERENCE (FD) METHOD
547
Since u(xk, 0) = f (xk) [recall (12.11) again], and if we substitute (12.105) into
(12.104), we obtain
u(xk, t1) = f (xk) + œÑg(xk) + 1
2
c2œÑ 2
h2 [f (xk+1) ‚àí2f (xk) + f (xk‚àí1)]
+ O(œÑ 3 + œÑ 2h2).
(12.106)
Via (12.98) this yields the required approximation
uk,1 = f (xk) + œÑg(xk) + 1
2e2[f (xk+1) ‚àí2f (xk) + f (xk‚àí1)]
or
uk,1 = (1 ‚àíe2)f (xk) + 1
2e2[f (xk+1) + f (xk‚àí1)] + œÑg(xk).
(12.107)
Taking account of the boundary conditions (12.100), Eq. (12.99) can be expressed
in matrix form as
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
u1,n+1
u2,n+1
...
uM‚àí2,n+1
uM‚àí1,n+1
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
=
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
2(1 ‚àíe2)
e2
0
¬∑ ¬∑ ¬∑
0
0
e2
2(1 ‚àíe2)
e2
¬∑ ¬∑ ¬∑
0
0
...
...
...
...
...
0
0
0
¬∑ ¬∑ ¬∑
2(1 ‚àíe2)
e2
0
0
0
¬∑ ¬∑ ¬∑
e2
2(1 ‚àíe2)
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
√ó
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
u1,n
u2,n
...
uM‚àí2,n
uM‚àí1,n
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
‚àí
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
u1,n‚àí1
u2,n‚àí1
...
uM‚àí2,n‚àí1
uM‚àí1,n‚àí1
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
(12.108)
This matrix recursion is run for n = 1, 2, 3, . . ., and the initial conditions are pro-
vided by (12.101) and (12.107).
We recall from Chapter 10 that numerical methods for the solution of ODE IVPs
can be unstable. The same problem can arise in the numerical solution of PDEs.
In particular, as the FD method is effectively an explicit method, it can certainly
become unstable if h and œÑ are inappropriately selected.
As noted by others [2, 13], we will have
lim
h,œÑ‚Üí0 uk,n = u(hk, œÑn)
provided that 0 < e ‚â§1. This is the famous Courant‚ÄìFriedrichs‚ÄìLewy (CFL) con-
dition for the stability of the FD method, and is originally due to Courant et al.
[23]. The special case where e = 1 is interesting and easy to analyze. In this case
(12.99) reduces to
uk,n+1 = uk+1,n ‚àíuk,n‚àí1 + uk‚àí1,n.
(12.109)
TLFeBOOK

548
NUMERICAL SOLUTION OF PARTIAL DIFFERENTIAL EQUATIONS
We recall that u(x, t) has the form
u(x, t) = v(x ‚àíct) + w(x + ct)
[see (12.50)]. Hence u(xk, tn) = v(xk ‚àíctn) + w(xk + ctn). Observe that, since
h = cœÑ (as e = 1), we have
u(xk+1, tn) ‚àíu(xk, tn‚àí1) + u(xk‚àí1, tn) = v(xk + h ‚àíctn) + w(xk + h + ctn)
‚àív(xk ‚àíctn + cœÑ) ‚àíw(xk + ctn ‚àícœÑ)
+ v(xk ‚àíh ‚àíctn) + w(xk ‚àíh + ctn)
= v(xk ‚àíh ‚àíctn) + w(xk + h + ctn)
= v(xk ‚àíctn ‚àícœÑ) + w(xk + ctn + cœÑ)
= v(xk ‚àíctn+1) + w(xk + ctn+1)
= u(xk, tn+1),
or
u(xk, tn+1) = u(xk+1, tn) ‚àíu(xk, tn‚àí1) + u(xk‚àí1, tn).
(12.110)
Equation (12.110) has a form that is identical to that of (12.109). In other words,
the algorithm (12.109) gives the exact solution to (12.9), but only at x = hk and
t = œÑn with h = cœÑ (which is a rather restrictive situation).
A more general approach to error analysis that conÔ¨Årms the CFL condition is
sometimes called von Neumann stability analysis [2]. We outline the approach as
follows. We begin by deÔ¨Åning the global truncation error
œµk,n = u(xk, tn) ‚àíuk,n.
(12.111)
Via (12.96)
u(xk, tn+1) ‚àí2u(xk, tn) + u(xk, tn‚àí1) ‚àíe2[u(xk+1, tn) ‚àí2u(xk, tn) + u(xk‚àí1, tn)]
= œÑ 2ek,n.
(12.112)
If we subtract (12.97) from (12.112) and simplify the result using (12.111), we
obtain
œµk,n+1 = 2(1 ‚àíe2)œµk,n + e2[œµk+1,n + œµk‚àí1,n] ‚àíœµk,n‚àí1 + œÑ 2ek,n
(12.113)
for k = 0, 1, . . . , M (L = Mh), and n = 1, 2, 3, . . .. Equation (12.113) is a two-
dimensional difference equation for the global error sequence (œµk,n). The term
œÑ 2ek,n is a forcing term, and if u(x, t) is smooth enough, the forcing term will
be bounded for all k and n. Basically, we can show that the CFL condition 0 <
e ‚â§1 prevents limn‚Üí‚àû|œµk,n| = ‚àûfor all k = 0, 1, . . . , M. Analogously to our
TLFeBOOK

THE FINITE-DIFFERENCE (FD) METHOD
549
stability analysis approach for ODE IVPs from Chapter 10, we may consider the
homogeneous problem
œµk,n+1 = 2(1 ‚àíe2)œµk,n + e2[œµk+1,n + œµk‚àí1,n] ‚àíœµk,n‚àí1,
(12.114)
which is just (12.113) with the forcing term made identically zero for all k and
n. In Section 12.3.1 we learned that separation of variables was a useful means
to solve (12.9). We therefore believe that a discrete version of this approach is
helpful at solving (12.114). To this end we postulate a typical solution of (12.114)
of the form
œµk,n = exp[jŒ±kh + Œ≤nœÑ]
(12.115)
for suitable constants Œ± ‚ààR and Œ≤ ‚ààC. We note that (12.115) has similarities to
(12.28) and is really a term in a discrete form of Fourier series expansion. We also
see that
|œµk,n| = | exp(Œ≤nœÑ)| = |sn|.
Thus, if |s| ‚â§1, we will not have unbounded growth of the error sequence (œµk,n)
as n increases. If we now substitute (12.115) into (12.114), we obtain (after sim-
pliÔ¨Åcation) the characteristic equation
s2 ‚àí[2(1 ‚àíe2) + 2 cos(Œ±h)e2]
#
$%
&
=2b
s + 1 = 0.
(12.116)
Using the identity 2 sin2 x = 1 ‚àícos(2x), we obtain
b = 1 ‚àí2e2 sin2
 Œ±h
2
!
.
(12.117)
It is easy to conÔ¨Årm that |b| ‚â§1 for all e such that 0 ‚â§e ‚â§1 because 0 ‚â§
sin2 (Œ±h/2) ‚â§1 for all all Œ±h ‚ààR. We note that s2 ‚àí2bs + 1 = 0 for s = s1, s2,
where
s1 = b +

b2 ‚àí1,
s2 = b ‚àí

b2 ‚àí1.
(12.118)
If |b| > 1, then |sk| > 1 for some k ‚àà{1, 2}, which can happen if we permit e > 1.
Naturally we reject this choice as it yields unbounded growth in the size of œµk,n
as n ‚Üí‚àû. If |b| ‚â§1, then clearly |sk| = 1 for all k. (To see this, consider the
product s1s2 = s1s‚àó
1 = |s1|2.) This prevents unbounded growth of œµk,n. Thus, we
have validated the CFL condition for the selection of Courant parameter e (i.e., we
must always choose e to satisfy 0 < e ‚â§1).
Example 12.4
Figure 12.4 illustrates the application of the recursion (12.108)
to the vibrating string problem of Example 12.1. The simulation parameters are
stated in the Ô¨Ågure caption. The reader should compare the approximate solution
TLFeBOOK

550
NUMERICAL SOLUTION OF PARTIAL DIFFERENTIAL EQUATIONS
0
0.2
0.4
0.6
0.8
1
0
5
10
15
20
‚àí0.1
‚àí0.05
0
0.05
0.1
Position (kh)
Time (nt)
Estimated displacement (uk,n)
Figure 12.4
FD method approximate solution to the vibrating string problem. A mesh
plot of uk,n as given by Eq. (12.108) for the parameters L = 1, c/L = 1
8, and H = 1
10.
Additionally, h = 0.05 and œÑ = 0.1, which meets the CFL criterion for stability of the
simulation.
of Fig. 12.4 to the exact solution of Fig. 12.2. The apparent loss of accuracy as the
number of time steps increases (i.e., with increasing nœÑ) is due to the phenomenon
of numerical dispersion [22], a topic considered in the next section in the context
of the FDTD method. Of course, simulation accuracy improves as h, œÑ ‚Üí0 for
Ô¨Åxed T = NœÑ, and L = Mh.
12.5
THE FINITE-DIFFERENCE TIME-DOMAIN (FDTD) METHOD
The FDTD method is often attributed to Yee [14]. It is a Ô¨Ånite-difference scheme
just as the FD method of Section 12.4 is a Ô¨Ånite-difference scheme. However, it is
of such a nature as to be particularly useful in solving hyperbolic PDEs where the
boundary conditions are at inÔ¨Ånity (i.e., wave propagation problems of the kind
considered in Section 12.3.2).
The FDTD method considers approximations to Hz(x, t) and Ey(x, t) given by
applying the central difference and forward difference approximations to the Ô¨Årst
derivatives in the PDEs (12.69) and (12.70). We will use the following notation
for the sampling of continuous functions such as f (x, t):
fk,n ‚âàf (kx, nt),
fk+ 1
2, n + 1
2
‚âàf ((k + 1
2)x, (n + 1
2)t)
(12.119)
(so x replaces h, and t replaces œÑ here, where h and œÑ were the grid spacings
used in previous sections). For convenience, let E = Ey and H = Hz (i.e., we drop
TLFeBOOK

THE FINITE-DIFFERENCE TIME-DOMAIN (FDTD) METHOD
551
the subscripts on the Ô¨Åeld components). We approximate the derivatives in (12.69)
and (12.70) speciÔ¨Åcally according to
‚àÇH
‚àÇt ‚âà1
t [Hk+ 1
2 ,n+ 1
2 ‚àíHk+ 1
2 ,n‚àí1
2 ],
(12.120a)
‚àÇE
‚àÇt ‚âà1
t [Ek,n+1 ‚àíEk,n],
(12.120b)
‚àÇH
‚àÇx ‚âà
1
x [Hk+ 1
2 ,n+ 1
2 ‚àíHk‚àí1
2 ,n+ 1
2 ],
(12.120c)
‚àÇE
‚àÇx ‚âà1
t [Ek+1,n ‚àíEk,n].
(12.120d)
DeÔ¨Åne œµk = œµ(kx), ¬µk = ¬µ((k + 1
2)x), œÉk = œÉ(kx), and œÉ ‚àó
k = œÉ ‚àó((k + 1
2)
x), which assumes the general situation where the material parameters vary with
x ‚àà[0, L] (computational region). Substituting these discretized material parame-
ters, and (12.120) into (12.69) and (12.70), we obtain the following algorithm:
Hk+ 1
2 ,n+ 1
2 =

1 ‚àíœÉ ‚àó
k
¬µk
t

Hk+ 1
2 ,n‚àí1
2 ‚àí1
¬µk
t
x [Ek+1,n ‚àíEk,n],
(12.121a)
Ek,n+1 =

1 ‚àíœÉk
œµk
t

Ek,n ‚àí1
œµk
t
x [Hk+ 1
2 ,n+ 1
2 ‚àíHk‚àí1
2 ,n+ 1
2 ].
(12.121b)
This is sometimes called the leapfrog algorithm. The dependencies between the esti-
mated Ô¨Åeld components in (12.121) are illustrated in Fig. 12.5. If we assume that
H(‚àí1
2x, t) = H((M + 1
2)x, t) = 0
(12.122)
n ‚àí1
2
1
2
x = 0
x = L
0
M
1
n +
n
n + 1
x
M ‚àí 1
Computational  region
Time (t)
Figure 12.5
An illustration of the dependencies between the approximate Ô¨Åeld components
given by (12.121a,b); the lines with arrows denote the ‚ÄúÔ¨Çow‚Äù of these dependencies [‚ó¶
electric Ô¨Åeld component (Ey); ‚ñ°magnetic Ô¨Åeld component (Hz)].
TLFeBOOK

552
NUMERICAL SOLUTION OF PARTIAL DIFFERENTIAL EQUATIONS
for all t ‚ààR+, then a more detailed pseudocode description of the FDTD algo-
rithm is
Hk+ 1
2 , ‚àí1
2
:= 0 for k = 0, 1, . . . , M ‚àí1;
Ek,0 := 0 for k = 0, 1, . . . , M;
for n := 0 to N ‚àí1 do begin
Em,n := E0 sin(œânt); {0 < m < M}
for k := 0 to M ‚àí1 do begin
Hk+ 1
2, n + 1
2
:=

1 ‚àí
œÉ‚àó
k
¬µk t

Hk+ 1
2, n ‚àí1
2
‚àí1
¬µk
t
x [Ek+1,n ‚àíEk,n];
end;
for k := 0 to M do begin
Ek,n+1 :=

1 ‚àíœÉk
œµk t

Ek,n ‚àí1
œµk
t
x [Hk+ 1
2, n + 1
2
‚àíHk‚àí1
2 , n + 1
2
];
end;
end;
The statement Em,n := E0 sin(œânt) simulates an antenna that broadcasts a
sinusoidal electromagnetic wave from the location x = mx ‚àà(0, L). Of course,
the antenna must be located in free space.
The FDTD algorithm is an explicit difference scheme, and so it may have
stability problems. However, it can be shown that the algorithm is stable provided
we have
e = c t
x ‚â§1
 
or t ‚â§x
c
!
,
(12.123)
Thus, the CFL condition of Section 12.4 applies to the FDTD algorithm as well.
A justiÔ¨Åcation of this claim appears in TaÔ¨Çove [8]. A MATLAB implementation
of the FDTD algorithm may be found in Appendix 12.A (see routine FDTD.m). In
this implementation we have introduced the parameters sx and st (0 < sx, st < 1)
such that
x = sxŒª0,
t = st
x
c .
(12.124)
Clearly, ct/x = st, and so the CFL condition is met. Also, spatial sampling is
determined by x = sxŒª0, which is some fraction of a free-space wavelength Œª0
[recall (12.55)]. Note that the algorithm simulates the Ô¨Åeld for all t ‚àà[0, T ], where
T = Nt. If the wave is propagating only through free space, then the wave will
travel a distance
D = cT = NstsxŒª0,
(12.125)
that is, the distance traveled is Nstsx free-space wavelengths. Since L = MsxŒª0
(i.e., the computational region spans Msx free-space wavelengths), this allows us
to make a reasonable choice for N.
A problem with the FDTD algorithm is that even if the computational region is
only free-space, a wave launched from location x = mx ‚àà(0, L) will eventually
strike the boundaries at x = 0 and/or x = L, and so will be reÔ¨Çected back toward
the source. These reÔ¨Çections will cause very large errors in the estimates of H, and
TLFeBOOK

THE FINITE-DIFFERENCE TIME-DOMAIN (FDTD) METHOD
553
E. But we know from Section 12.3.2 that we may design absorbing layers called
perfectly matched layers (PMLs) that suppress these reÔ¨Çections.
Suppose that the PML has physical parameters ¬µ, œµ, œÉ, and œÉ ‚àó, then, from
(12.73), the PML will have a propagation constant given by
Œ≥ 2 = (œÉœÉ ‚àó‚àíœâ2¬µœµ) + jœâ(œÉ¬µ + œÉ ‚àóœµ).
(12.126)
If we enforce the condition (12.74), namely
œÉ ‚àó
¬µ = œÉ
œµ ,
(12.127)
then (12.126) becomes
Œ≥ 2 = ¬µ
œµ (œÉ 2 ‚àíœâ2œµ2) + 2jœâœÉ¬µ.
(12.128)
If we enforce œÉ 2 ‚àíœâ2œµ2 ‚â§0, then Œ≥ = Œ± + jŒ≤, where
Œ± =
8¬µ
œµ [œÉ 2 + œâ2œµ2]1/2 cos
œÄ
4 + 1
2Tan‚àí1
 œâ2œµ2 ‚àíœÉ 2
2œâœÉœµ
!
.
(12.129)
Equation (12.129) is obtained by the same arguments that yielded (12.89). As
Œ± > 0, then a wave on entering the PML will be attenuated by a factor e‚àíŒ±x,
where x is the depth of penetration of the wave into the PML. A particularly
simple choice for Œ± is to let œÉ 2 = œâ2œµ2, in which case
Œ± = œâ‚àö¬µœµ.
(12.130)
Since œâ = 2œÄ
Œª0 c and c =
1
‚àö¬µ0œµ0 , with ¬µ = ¬µr¬µ0 and œµ = œµrœµ0, we can rewrite
(12.130) as
Œ± = ‚àö¬µrœµr
2œÄ
Œª0
.
(12.131)
If we are matching the PML to free space, then ¬µr = œµr = 1, and so Œ± = 2œÄ
Œª0 , in
which case
e‚àíŒ±x = e‚àí2œÄ
Œª0 x.
(12.132)
If the PML is of thickness x = 2Œª0, then, from (12.132) we have e‚àíŒ±x = e‚àí4œÄ ‚âà
3.5 √ó 10‚àí6. A PML that is two free-space wavelengths thick will therefore absorb
very nearly all of the radiation incident on it at the wavelength Œª0. Since we have
chosen œÉ 2 = œâ2œµ2, it is easy to conÔ¨Årm that
œÉ = 2œÄ
Œª0
œµr
Z0
,
œÉ ‚àó= 2œÄ
Œª0
¬µrZ0
(12.133)
TLFeBOOK

554
NUMERICAL SOLUTION OF PARTIAL DIFFERENTIAL EQUATIONS
0
1
2
3
4
5
6
7
8
9
10
‚àí2
‚àí1.5
‚àí1
‚àí0.5
0
0.5
1
1.5
2
Distance (x l0)
Ey (volts per meter)
Free space
PML 1
Dielectric
PML 2
Antenna
Figure 12.6
Typical output from FDTD.m (see Appendix 12.A) for the system described in
Example 12.5. The antenna broadcasts a sinusoid of wavelength Œª0 = 500 nm (nanometers)
from the location x = 3Œª0. The transmitted Ô¨Åeld strength is E0 = 1 V/m.
[via (12.127)], where we recall that Z0 = ‚àö¬µ0/œµ0. Routine FDTD.m in Appen-
dix 12.A implements PMLs according to this approach.
Example 12.5
Figure 12.6 illustrates a typical output from FDTD.m (Appen-
dix 12.A). The system shown in the Ô¨Ågure occupies a computational region of
length 10Œª0 (i.e., x ‚àà[0, 10Œª0]). Somewhat arbitrarily we have Œª0 = 500 nm (nano-
meters). The antenna (which, given the wavelength, could be a laser) is located
at index m = 150 (i.e., is at x = mx = msxŒª0 = 3Œª0, since sx = .02). The free-
space region is for x ‚àà(2Œª0, 6Œª0). The lossless dielectric occupies x ‚àà[6Œª0, 8Œª0],
and has a relative permittivity of œµr = 4. The entire computational region is non-
magnetic, and so we have ¬µ = ¬µ0 everywhere. Clearly, PML 1 is matched to free
space, while PML 2 is matched to the dielectric.
Since œµr = 4, according to (12.82), the transmission coefÔ¨Åcient from free space
into the dielectric is
œÑ =
2
8 ¬µ0
œµrœµ0
8 ¬µ0
œµrœµ0
+
8¬µ0
œµ0
=
2
1 + ‚àöœµr
= 2
3.
TLFeBOOK

THE FINITE-DIFFERENCE TIME-DOMAIN (FDTD) METHOD
555
Since E0 = 1 V/m, the amplitude of the electric Ô¨Åeld within the dielectric must be
œÑE0 = 2
3 V/m. From Fig. 12.6 the reader can see that the electric Ô¨Åeld within the
dielectric does indeed have an amplitude of about 2
3 V/m to a good approximation.
From (12.57) the wavelength within the dielectric material is
Œª =
1
‚àöœµr
Œª0 = 1
2Œª0.
Again from Fig. 12.6 we see that the wavelength of the transmitted Ô¨Åeld is indeed
close to 1
2Œª0 within the dielectric.
We observe that the PMLs in Example 12.5 do not perfectly suppress reÔ¨Çections
at their boundaries. For example, the wave crest closest to the interface between
PML 2 and the dielectric, and that lies within the dielectric, is somewhat higher
than it should be. It is the discretization of a continuous space that has lead to these
residual reÔ¨Çections.
The theory of PMLs presented here does not easily extend from electromagnetic
wave propagation problems in one spatial dimension into propagation problems in
two or three spatial dimensions. It appears that the Ô¨Årst truly successful extension of
PML theory to higher spatial dimensions is due to B¬¥erenger [15,16]. Wu and Fang
[17] claim to have improved the theory still further by improving the suppression
of the residual reÔ¨Çections noted above.
The problem of numerical dispersion was mentioned in Example 12.4 in the
application of the FD method to the simulation of a vibrating string. We conclude
this chapter with an account of the problem based mainly on the work of Trefethen
[22]. We will assume lossless propagation, so œÉ = œÉ ‚àó= 0 in (12.121). We will also
assume that the computational region is free space, so ¬µ = ¬µ0, and œµ = œµ0 every-
where. If we now substitute E(x, t) = E0 sin(œât ‚àíŒ≤x) and H(x, t) = H0 sin(œât ‚àí
Œ≤x) into either of (12.121a) or (12.121b), apply the appropriate trigonometric iden-
tities, and then cancel out common factors, we obtain the identity
sin
 œât
2
!
= e sin
 Œ≤x
2
!
.
(12.134)
We may use (12.134) and (12.123) to obtain
vp = œâ
Œ≤ = c
e
2
Œ≤x sin‚àí1

e sin
 Œ≤x
2
!
(12.135)
which is the phase speed of the wave of wavelength Œª0 (recall Œ≤ = 2œÄ/Œª0) in the
FDTD method. For the continuous wave E(x, t) [or, for that matter, H(x, t)], recall
from Section 12.3.2 that œâ/Œ≤ = c, so without spatial or temporal discretization
effects, a sinusoid will propagate through free space at the speed c regardless of its
wavelength (or, equivalently, its frequency). However, (12.135) suggests that the
speed of an FDTD-simulated sinusoidal wave will vary with the wavelength. As
TLFeBOOK

556
NUMERICAL SOLUTION OF PARTIAL DIFFERENTIAL EQUATIONS
explained in Ref. 22 (or see Ref. 12), the group speed
vg = dœâ
dŒ≤ = d
dŒ≤ (Œ≤vp) = c
cos

Œ≤x
2

8
1 ‚àíe2 sin2 
Œ≤x
2

(12.136)
is more relevant to assessing how propagation speed varies with the wavelength.
Again, in free space the continuous wave propagates at the group speed dœâ
dŒ≤ =
d
dŒ≤ (Œ≤c) = c. A plot of vg/c versus Œª0/x [with vg/c given by (12.136)] appears
in Fig. 2 of Represa et al. [19]. It shows that short-wavelength sinusoids travel at
slower speeds than do long-wavelength sinusoids when simulated using the FDTD
method.
We have seen that nonsinusoidal waveforms (e.g., the triangle wave of Exam-
ple 12.1) are a superposition of sinusoids of varying frequency. Thus, if we use
the FDTD method, the FD method, or indeed any numerical method to simulate
wave propagation, we will see the effects of numerical dispersion. In other words,
the various frequency components in the wave will travel at different speeds, and
so the original shape of the wave will become lost as the simulation progresses
in time (i.e., as nt increases). Figure 12.7 illustrates this for the case of two
0
1
2
3
4
5
6
7
8
‚àí2
‚àí1
0
1
2
Distance (x l0)
Ey (volts per meter) 
0
1
2
3
4
5
6
7
8
‚àí2
‚àí1
0
1
2
Distance (x l0)
Ey (volts per meter)
(a)
(b)
numerical dispersion
Errors due to
n = 1500 steps
n = 500 steps
Figure 12.7
Numerical dispersion in the FDTD method as illustrated by the propagation
of two Gaussian pulses. The medium is free space.
TLFeBOOK

MATLAB CODE FOR EXAMPLE 12.5
557
Gaussian pulses traveling in opposite directions. The two pulses originally appeared
at x = 4Œª0, and the medium is free space. For N = 1500 time steps, there is a very
noticeable error due to the ‚Äúbreakup‚Äù of the pulses as their constituent frequency
components separate out as a result of the numerical dispersion.
In closing, note that more examples of numerical dispersion may be found in
Luebbers et al. [18]. Shin and Nevels [21] explain how to work with Gaussian
test pulses to reduce numerical dispersion. We mention that Represa et al. [19]
use absorbing boundary conditions based on the theory in Mur [20], which is a
different method from the PML approach we have used in this book.
APPENDIX 12.A
MATLAB CODE FOR EXAMPLE 12.5
%
%
permittivity.m
%
% This routine specifies the permittivity profile of the
% computational region [0,L], and is needed by FDTD.m
%
function epsilon = permittivity(k,sx,lambda0,M)
epsilon0 = 8.854185*1e-12; % free-space permittivity
er1 = 4;
% relative permittivity of the dielectric
Dx = sx*lambda0;
% this is Delta x
x = k*Dx;
% position at which we determine epsilon
L = M*Dx;
% location of right end of computational
% region
if ((x >= 0) & (x < (L-4*lambda0)))
epsilon = epsilon0;
else
epsilon = er1*epsilon0;
end;
%
%
permeability.m
%
% This routine specifies the permeability profile of the
% computational region [0,L], and is needed by FDTD.m
%
function
mu = permeability(k,sx,lambda0,M)
mu0 = 400*pi*1e-9;
% free-space permeability
Dx = sx*lambda0;
% this is Delta x
x = k*Dx;
% position at which we determine mu
L = M*Dx;
% location of right end of computational
% region
mu = mu0;
%
%
econductivity.m
%
% This routine specifies the electrical conductivity profile
TLFeBOOK

558
NUMERICAL SOLUTION OF PARTIAL DIFFERENTIAL EQUATIONS
% of the computational region [0,L], and is needed by FDTD.m
%
function sigma = econductivity(k,sx,lambda0,M)
epsilon0 = 8.854185*1e-12; % free-space permittivity
mu0 = 400*pi*1e-9;
% free-space permeability
er1 = 4;
% dielectric relative permittivity
epsilon1 = er1*epsilon0;
% dielectric permittivity
Z0 = sqrt(mu0/epsilon0);
% free-space impedance
Dx = sx*lambda0;
% this is Delta x
x = k*Dx;
% position at which we determine sigma
L = M*Dx;
% location of right end of computational
% region
star1 = (2*pi/lambda0)*(1/Z0); % conductivity of PML 1 (at x = 0 end)
star2 = er1*star1;
% conductivity of PML 2 (at x = L end)
if ((x > 2*lambda0) & (x < L-2*lambda0))
sigma = 0;
elseif (x <= 2*lambda0)
sigma = star1;
elseif (x >= (L - 2*lambda0))
sigma = star2;
end;
%
%
mconductivity.m
%
% This routine specifies the magnetic conductivity profile of the
% computational region [0,L], and is needed by FDTD.m
%
function sigmastar = mconductivity(k,sx,lambda0,M)
epsilon0 = 8.854185*1e-12; % free-space permittivity
mu0 = 400*pi*1e-9;
% free-space permeability
Z0 = sqrt(mu0/epsilon0);
% free-space impedance
Dx = sx*lambda0;
% this is Delta x
x = (k+.5)*Dx;
% position at which we determine sigmastar
L = M*Dx;
% location of right end of computational
% region
star = (2*pi/lambda0)*Z0;
if ((x > 2*lambda0) & (x < L-2*lambda0))
sigmastar = 0;
else
sigmastar = star;
end;
%
%
FDTD.m
%
% This routine produces the plot in Fig. 12.6
which is associated with
% Example 12.5. Thus, it illustrates the FDTD method.
%
% The routine returns the total electric field component Ey to the
% caller.
%
TLFeBOOK

MATLAB CODE FOR EXAMPLE 12.5
559
function Ey = FDTD
mu0 = 400*pi*1e-9;
% free-space permeability
epsilon0 = 8.854185*1e-12;
% free-space permittivity
c = 1/sqrt(mu0*epsilon0);
% speed of light in free-space
lambda0 = 500;
% free-space wavelength of the source
% in nanometers
lambda0 = lambda0*1e-9;
% free-space wavelength of the source
% in meters
beta0 = (2*pi)/lambda0;
% free-space beta (wavenumber)
sx = .02;
% fraction of a free-space wavelength
% used to determine Delta x
st = .10;
% scale factor used to determine time-step
% size Delta t
Dx = sx*lambda0;
% Delta x
Dt = (st/c)*Dx;
% application of the CFL condition
% to determine time-step Delta t
E0 = 1;
% amplitude of the electric field (V/m)
% generated by the source
m = 150;
% source (antenna) location index
omega = beta0*c;
% source frequency (radians/second)
M = 500;
% number of spatial grid points is (M+1)
N = 4000;
% the number of time steps in the simulation
E = zeros(1,M+1);
% initial electric field
H = zeros(1,M+2);
% initial magnetic field
% Specify the material properties in the computational region
% (which is x in [0,M*Dx])
for k = 0:M
epsilon(k+1) = permittivity(k,sx,lambda0,M);
ce(k+1) = 1 - Dt*econductivity(k,sx,lambda0,M)/epsilon(k+1);
h(k+1) = 1/epsilon(k+1);
end;
h = h*Dt/Dx;
for k = 0:(M-1)
mu(k+1) = permeability(k,sx,lambda0,M);
ch(k+1) = 1 - Dt*mconductivity(k,sx,lambda0,M)/mu(k+1);
e(k+1) = 1/mu(k+1);
end;
e = e*Dt/Dx;
% Run the simulation for N time steps
for n = 1:N
E(m+1) = E0*sin(omega*(n-1)*Dt);
% Antenna is at index m
H(2:M+1) = ch.*H(2:M+1) - e.*(E(2:M+1) - E(1:M));
E(1:M+1) = ce.*E(1:M+1) - h.*(H(2:M+2) - H(1:M+1));
E(m+1) = E0*sin(omega*n*Dt);
Ey(n,:) = E;
% Save the total electric field at time step n
end;
TLFeBOOK

560
NUMERICAL SOLUTION OF PARTIAL DIFFERENTIAL EQUATIONS
REFERENCES
1. E. Kreyszig, Advanced Engineering Mathematics, 4th ed., Wiley, New York, 1979.
2. T. Myint-U and L. Debnath, Partial Differential Equations for Scientists and Engineers,
3rd ed., North-Holland, New York, 1987.
3. R. L. Burden and J. D. Faires, Numerical Analysis, 4th ed., PWS-KENT Publ., Boston,
MA, 1989.
4. A. Quarteroni, R. Sacco and F. Saleri, Numerical Mathematics (Texts in Applied Math-
ematics series, Vol. 37), Springer-Verlag, New York, 2000.
5. G. Strang and G. Fix, An Analysis of the Finite Element Method, Prentice-Hall, Engle-
wood Cliffs, NJ, 1973.
6. S. Brenner and R. Scott, The Mathematical Theory of Finite Element Methods, Springer-
Verlag, New York, 1994.
7. C. T. Kelley, Iterative Methods for Linear and Nonlinear Equations, SIAM, Philadel-
phia, PA, 1995.
8. A. TaÔ¨Çove,
Computational Electrodynamics: The
Finite-Difference Time-Domain
Method, Artech House, Norwood, MA, 1995.
9. R. Courant and D. Hilbert, Methods of Mathematical Physics, Vol. II. Partial Differen-
tial Equations, Wiley, New York, 1962.
10. J. D. Kraus and K. R. Carver, Electromagnetics, 2nd ed., McGraw-Hill, New York,
1973.
11. J. F. Epperson, An Introduction to Numerical Methods and Analysis, Wiley, New York,
2002.
12. W. C. Elmore and M. A. Heald, Physics of Waves, Dover Publ., New York, 1969.
13. E. Isaacson and H. B. Keller, Analysis of Numerical Methods, Wiley, New York,
1966.
14. K. S. Yee, ‚ÄúNumerical Solution of Initial Boundary Value Problems Involving
Maxwell‚Äôs Equations in Isotropic Media,‚Äù IEEE Trans. Antennas Propag. AP-14, 302‚Äì
307 (May 1966).
15. J.-P. B¬¥erenger, ‚ÄúA Perfectly Matched Layer for the Absorption of Electromagnetic
Waves,‚Äù J. Comput. Phys. 114, 185‚Äì200 (1994).
16. J.-P. B¬¥erenger, ‚ÄúPerfectly Matched Layer for the FDTD Solution of Wave-Structure
Interaction Problems,‚Äù IEEE Trans. Antennas Propag. 44, 110‚Äì117 (Jan. 1996).
17. Z. Wu and J. Fang, ‚ÄúHigh-Performance PML Algorithms,‚Äù IEEE Microwave Guided
Wave Lett. 6, 335‚Äì337 (Sept. 1996).
18. R. J. Luebbers, K. S. Kunz and K. A. Chamberlin, ‚ÄúAn Interactive Demonstration of
Electromagnetic Wave Propagation Using Time-Domain Finite Differences,‚Äù IEEE
Trans. Educ. 33, 60‚Äì68 (Feb. 1990).
19. J. Represa, C. Pereira, M. Panizo and F. Tadeo, ‚ÄúA Simple Demonstration of Numerical
Dispersion under FDTD,‚Äù IEEE Trans. Educ. 40, 98‚Äì102 (Feb. 1997).
20. G. Mur, ‚ÄúAbsorbing Boundary Conditions for the Finite-Difference Approximation of
the Time-Domain Electromagnetic-Field Equations,‚Äù IEEE Trans. Electromagn. Compat.
EMC-23, 377‚Äì382 (Nov. 1981).
21. C.-S. Shin and R. Nevels, ‚ÄúOptimizing the Gaussian Excitation Function in the Finite
Difference Time Domain Method,‚Äù IEEE Trans. Educ. 45, 15‚Äì18 (Feb. 2002).
TLFeBOOK

PROBLEMS
561
22. L. N. Trefethen, ‚ÄúGroup Velocity in Finite Difference Schemes,‚Äù SIAM Rev. 24, 113‚Äì136
(April 1982).
23. R. Courant, K. Friedrichs and H. Lewy, ‚Äú ¬®Uber die Partiellen Differenzengleichungen
der Mathematischen Physik,‚Äù Math. Ann. 100, 32‚Äì74 (1928).
PROBLEMS
12.1. Classify the following PDEs into elliptic, parabolic, and hyperbolic types
(or a combination of types).
(a) 3uxx + 5uxy + uyy = x + y
(b) uxx ‚àíuxy + 2uyy = ux + u
(c) yuxx + uyy = 0
(d) y2uxx ‚àí2xyuxy + x2uyy = 0
(e) 4x2uxx + uyy = u
12.2. Derive (12.5) (both equations).
12.3. In (12.6) let h = œÑ, and let N = M = 4, and for convenience let fk,n =
œÅ(xk, yn)/œµ. DeÔ¨Åne the vectors
V = [V1,1 V1,2 V1,3 V2,1 V2,2 V2,3 V3,1 V3,2 V3,3],
f = [f1,1 f1,2 f1,3 f2,1 f2,2 f2,3 f3,1 f3,2 f3,3].
Find the matrix A such that AV = h2f . Assume that
V0,n = Vk,0 = Vk,4 = V4,n = 0
for all k, and n.
12.4. In (12.6) let h = œÑ = 1
4, and N = M = 4, and assume that œÅ(xk, yn) = 0
for all k, and n. Let x0 = y0 = 0. Suppose that
V (0, y) = V (x, 0) = 0, V (x, 1) = x, V (1, y) = y.
Find the linear system of equations for Vk,n with 1 ‚â§k, n ‚â§3, and put it
in matrix form.
12.5. Recall the previous problem.
(a) Write a MATLAB routine to implement the Gauss‚ÄìSeidel method (recall
Section 4.7). Use your routine to solve the linear system of equations
in the previous problem.
(b) Find the exact solution to the PDE
Vxx + Vyy = 0
for the boundary conditions stated in the previous problem.
TLFeBOOK

562
NUMERICAL SOLUTION OF PARTIAL DIFFERENTIAL EQUATIONS
(c) Use the solution in (b) to Ô¨Ånd V (k/4, n/4) at 1 ‚â§k, n ‚â§3, and compare
to the results obtained from part (a). They should be the same. Explain
why. [Hint: Consider the error terms in (12.5).]
12.6. The previous two problems suggest that the linear systems that arise in the
numerical solution of elliptic PDEs are sparse, and so it is worth considering
their solution using the iterative methods from Section 4.7. Recall that the
iterative methods of Section 4.7 have the general form
x(k+1) = Bx(k) + f
[from (4.155)]. Show that
||x(k+1) ‚àíx(k)||‚àû
||x(k) ‚àíx(k‚àí1)||‚àû
‚â§||B||‚àû.
[Comment: Recalling (4.36c), it can be shown that œÅ(A) ‚â§||A||p [which is
really another way of expressing (4.158)]. For example, this result can be
used to estimate the spectral radius of BJ [Eq. (4.171)].]
12.7. Consider Eq. (12.9), with the initial and boundary conditions in (12.10) and
(12.11), respectively.
(a) Consider the change of variables
Œæ = x + ct,
Œ∑ = x ‚àíct
and œÜ(Œæ, Œ∑) replaces u(x, t) according to
œÜ(Œæ, Œ∑) = u
 1
2(Œæ + Œ∑), 1
2c(Œæ ‚àíŒ∑)
!
.
(12.P.1)
Verify the derivative operator equivalences
‚àÇ
‚àÇx ‚â°‚àÇ
‚àÇŒæ + ‚àÇ
‚àÇŒ∑, 1
c
‚àÇ
‚àÇt ‚â°‚àÇ
‚àÇŒæ ‚àí‚àÇ
‚àÇŒ∑.
(12.P.2)
[Hint: ‚àÇœÜ
‚àÇŒæ = ‚àÇu
‚àÇx
‚àÇx
‚àÇŒæ + ‚àÇu
‚àÇt
‚àÇt
‚àÇŒæ , ‚àÇœÜ
‚àÇŒ∑ = ‚àÇu
‚àÇx
‚àÇx
‚àÇŒ∑ + ‚àÇu
‚àÇt
‚àÇt
‚àÇŒ∑.]
(b) Show that (12.9) is replaceable with
‚àÇ2œÜ
‚àÇŒæ‚àÇŒ∑ = 0.
(12.P.3)
(c) Show that the solution to (12.P.3) is of the form
œÜ(Œæ, Œ∑) = P (Œæ) + Q(Œ∑),
and hence
u(x, t) = P (x + ct) + Q(x ‚àíct),
where P and Q are arbitrary twice continuously differentiable functions.
TLFeBOOK

PROBLEMS
563
(d) Show that
P (x) + Q(x) = f (x),
P (1)(x) ‚àíQ(1)(x) = 1
c g(x).
(e) Use the facts from (d) to show that
u(x, t) = 1
2[f (x + ct) + f (x ‚àíct)] + 1
2c
 x+ct
x‚àíct
g(s) ds.
12.8. In (12.11) suppose that
f (x) =
Ô£±
Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£≤
Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£≥
‚àíH
d
 
x ‚àíL
2 ‚àíd
!
,
L
2 ‚â§x ‚â§L
2 + d
H
d
 
x ‚àíL
2 + d
!
,
L
2 ‚àíd ‚â§x ‚â§L
2
0,
elsewhere
,
where 0 < d ‚â§L/2. Assume that g(x) = 0 for all x.
(a) Sketch f (x).
(b) Write a MATLAB routine to implement the FD algorithm (12.108) for
computing uk,n. Write the routine in such a way that it is easy to change
the parameters c, H, h, œÑ, N, M, and d. The routine must produce a mesh
plot similar to Fig. 12.4 and a plot similar to Fig. 12.7 on the same page
(i.e., make use of the subplot). The latter plot is to be of uk,N versus k. Try
out your routine using the parameters c = 1
8, h = 0.05, œÑ = 0.025, H =
0.1, d = L/10, M = 200, and N = 1100 (recalling that L = Mh). Do
you Ô¨Ånd numerical dispersion effects?
12.9. Repeat Example 12.1 for f (x) and g(x) in the previous problem.
12.10. Example 12.1 is about the ‚Äúplucked string.‚Äù Repeat Example 12.1 assuming
that f (x) = 0 and
g(x) =
Ô£±
Ô£¥Ô£¥Ô£≤
Ô£¥Ô£¥Ô£≥
2V
L x,
0 ‚â§x ‚â§L
2
2V
L (L ‚àíx),
L
2 ‚â§x ‚â§L
This describes a ‚Äústruck string.‚Äù
12.11. The MATLAB routines in Appendix 12.A implement the FDTD method,
and generate information for the plot in Fig. 12.6. However, the reÔ¨Çected
Ô¨Åeld component for 2Œª0 ‚â§x ‚â§6Œª0 is not computed or displayed.
Modify the code(s) in Appendix 12.A to compute the reÔ¨Çected Ô¨Åeld com-
ponent in the free-space region and to plot it. Verify that at the interface
TLFeBOOK

564
NUMERICAL SOLUTION OF PARTIAL DIFFERENTIAL EQUATIONS
between the free-space region and the dielectric that |œÅ| = 1
3 (magnitude of
the reÔ¨Çection coefÔ¨Åcient). Of course, you will need to read the amplitude
of the reÔ¨Çected component from your plot to do this.
12.12. Derive Eq. (12.134).
12.13. Modify the MATLAB code(s) in Appendix 12.A to generate a plot similar
to Fig. 12.7. [Hint: Ek,0 = E0 exp

‚àí

k‚àím
iw
2
for k = 0, 1, . . . , M is the
initial electric Ô¨Åeld. Set the initial magnetic Ô¨Åeld to zero for all k.]
12.14. Plot vg/c versus Œª0/x for vg given by (12.136). Choose e = 0.1, 0.5, and
0.8, and plot all curves on the same graph. Use these curves to explain why
the errors due to numerical dispersion (see Fig. 12.7) are worse on the side
of the pulse opposite to its direction of travel.
TLFeBOOK

13
An Introduction to MATLAB
13.1
INTRODUCTION
MATLAB is short for ‚Äúmatrix laboratory,‚Äù and is an extremely powerful software
tool1 for the development and testing of algorithms over a wide range of Ô¨Åelds
including, but not limited to, control systems, signal processing, optimization, image
processing, wavelet methods, probability and statistics, and symbolic computing.
These various applications are generally divided up into toolboxes that typically
must be licensed separately from the core package.
Many books have already been written that cover MATLAB in varying degrees
of detail. Some, such as Nakamura [3], Quarteroni et al. [4], and Recktenwald
[5], emphasize MATLAB with respect to numerical analysis and methods, but
are otherwise fairly general. Other books emphasize MATLAB with respect to
particular areas such as matrix analysis and methods (e.g., see Golub and Van
Loan [1] or Hill [2]). Some books implicitly assume that the reader already knows
MATLAB [1,4]. Others assume little or no previous knowledge on the part of the
reader [2,3,5].
This chapter is certainly not a comprehensive treatment of the MATLAB tool,
and is nothing more than a quick introduction to it. Thus, the reader will have to
obtain other books on the subject, or consult the appropriate manuals for further
information. MATLAB‚Äôs online help facility is quite useful, too.
13.2
STARTUP
Once properly installed, MATLAB is often invoked (e.g., on a UNIX workstation
with a cmdtool window open) by typing matlab, and hitting return. A window
under which MATLAB runs will appear. The MATLAB prompt also appears:
>>
MATLAB commands may then be entered and executed interactively.
If you wish to work with commands in M-Ô¨Åles (discussed further below), then
having two cmdtool windows open to the same working directory is usually desir-
able. One window would be used to run MATLAB, and the other would be used
1MATLAB is written in C, but is effectively a language on its own.
An Introduction to Numerical Analysis for Electrical and Computer Engineers, by C.J. Zarowski
ISBN 0-471-46737-5
c‚Éù2004 John Wiley & Sons, Inc.
565
TLFeBOOK

566
AN INTRODUCTION TO MATLAB
to edit the M-Ô¨Åles as needed (to either develop the algorithm in the Ô¨Åle or to
debug it).
13.3
SOME BASIC OPERATORS, OPERATIONS, AND FUNCTIONS
The MATLAB command
>> diary filename
will create the Ô¨Åle Ô¨Ålename, and every command you type in and run, and every
result of this, will be stored in Ô¨Ålename. This is useful for making a permanent
record of a MATLAB session which can help in documentation and sometimes in
debugging. When writing MATLAB M-Ô¨Åles, always make sure to document your
programs. The examples in Section 13.6 illustrate this.
MATLAB tends to work in the more or less intuitive way where matrix and/or
vector operations are concerned. Of course, it is in the nature of this software tool
to assume the user is already familiar with matrix analysis and methods before
attempting to use it.
When a MATLAB command creates a vector as the output from some operation,
it may be in the form of a column or a row vector, depending on the command. A
typical MATLAB row vector is
>>x = [ 1 1 1];
>>
The semicolon at the end of a line prevents the printout of the result of the command
at that line (this is useful in preventing display clutter). If you wish to turn it into
a column vector, then type:
>>x = x.‚Äô
x =
1
1
1
Making this conversion is sometimes necessary as the inputs to some routines need
the vectors in either row or column format, and some routines do not care. Routines
that do not care whether the input vector is row or column make the conversion to
a consistent form internally. For example, the following command sequence (which
can be stored in a Ô¨Åle called an M-Ô¨Åle) converts vector x into a row vector if it is
not one already:
>> [N,M] = size(x);
>> if N ~= 1
>>
x = x.‚Äô;
>>
end;
>>
TLFeBOOK

SOME BASIC OPERATORS, OPERATIONS, AND FUNCTIONS
567
In this routine N is the number of rows and M is the number of columns in x. (The
size command also accepts matrices.) The related command length(x) will return
the length of vector x. This can be very useful in for loops (below).
The addition of vectors works in the obvious way:
>> x = [ 1 1 1];
>> y = [ 2 -1 2 ];
>> x + y
ans =
3
0
3
>>
In this routine, the answer might be saved in vector z by typing >> z = x + y;.
Clearly, to add vectors without error means that they must be of the same size.
MATLAB will generate an error message if matrix and vector objects are not
dimensioned properly when operations are performed on them. The mismatching
of array sizes is a very common error in MATLAB programming.
Matrices can be entered as follows:
>> A = [ 1 1 ; 1 2 ]
A =
1
1
1
2
>>
Again, addition or subtraction would occur in the expected manner. We can invert
a matrix as follows:
>> inv(A)
ans =
2
-1
-1
1
>>
Operation det(A) will give the determinant of A, and [L, U] = lu(A) will give the
LU factorization of A (if it exists). Of course, there are many other routines for
common matrix operations, and decompositions (QR decomposition, singular value
decomposition, eigendecompositions, etc.). Compute y = Ax + b:
>> x = [ 1 -1 ].‚Äô;
>> b = [ 2
3 ].‚Äô;
>> y = A*x + b
TLFeBOOK

568
AN INTRODUCTION TO MATLAB
y =
2
2
>>
The colon operator can extract parts of matrices and vectors. For example, to
place the elements in rows j to k of column n of matrix B into vector x, use
>> x = B(j : k, n);. To extract the element from row k and column n of matrix A,
use >> x = A(k, n);. To raise something (including matrices) to a speciÔ¨Åc power,
use >> C = A‚àßp, for which p is the desired power. (This computes C = Ap.)
(Note: MATLAB indexes vectors and matrices beginning with 1.)
Unless the user overrides the defaults, variables i and j denote the square root
of ‚àí1:
>> sqrt(-1)
ans =
0 + 1.0000i
>>
Here, i and j are built-in constants. So, to enter a complex number, say, z = 3 ‚àí2j,
type
>> z = 3 - 2*i;
Observe
>> x = [ 1
1+i ];
>> x‚Äô
ans =
1.0000
1.0000 - 1.0000i
>>
So the transposition operator without the period gives the complex‚Äìconjugate
transpose (Hermitian transpose). Note that besides i and j, another useful built-in
constant is pi (= œÄ).
Floating-point numbers are entered as, for instance, 1.5e ‚àí3 (which is 1.5 √ó
10‚àí3). MATLAB agrees with IEEE Ô¨Çoating-point conventions, and so 0/0 will
result in NaN (‚Äúnot a number‚Äù) to more clearly indicate an undeÔ¨Åned operation.
An operation like 1/0 will result in Inf as an output.
TLFeBOOK

SOME BASIC OPERATORS, OPERATIONS, AND FUNCTIONS
569
We may summarize a few important operators, functions, and other terms:
Relational Operators
<
less than
<=
less than or equal to
>
greater than
>=
greater than or equal to
==
equal to
‚àº=
not equal to
Logical Operators
&
and
|
or
‚àº
not
Trigonometric Functions
sin
sine
cos
cosine
tan
tangent
asin
arcsine
acos
arccosine
atan
arctangent
atan2
four quadrant arctangent
sinh
hyperbolic sine
cosh
hyperbolic cosine
tanh
hyperbolic tangent
asinh
hyperbolic arcsine
acosh
hyperbolic arccosine
atanh
hyperbolic arctangent
Elementary Mathematical Functions
abs
absolute value
angle
phase angle (argument of a complex number)
sqrt
square root
TLFeBOOK

570
AN INTRODUCTION TO MATLAB
Elementary Mathematical Functions
real
real part of a complex number
imag
imaginary part of a complex number
conj
complex conjugate
rem
remainder or modulus
exp
exponential to base e
log
natural logarithm
log10
base-10 logarithm
round
round to nearest integer
Ô¨Åx
round toward zero
Ô¨Çoor
round toward ‚àí‚àû
ceil
round toward +‚àû
In setting up the time axis for plotting things (discussed below), a useful com-
mand is illustrated by
>> y = [0:.2:1]
y =
0
0.2000
0.4000
0.6000
0.8000
1.0000
>>
Thus, [x:y:z] creates a row vector whose Ô¨Årst element is x and whose last element
is z (depending on step size y), where the elements in between are of the form x
+ ky (where k is a positive integer).
It can also be useful to create vectors of zeros:
>> zeros(size([1:4]))
ans =
0
0
0
0
>>
Or, alternatively, a simpler way is
>> zeros(1,4)
ans =
0
0
0
0
>>
TLFeBOOK

WORKING WITH POLYNOMIALS
571
Using ‚Äúzeros(n,m)‚Äù will result in an n √ó m matrix of zeros. Similarly, a vector
(or matrix) containing only ones would be obtained using the MATLAB function
called ‚Äúones.‚Äù
13.4
WORKING WITH POLYNOMIALS
We have seen on many occasions that polynomials are vital to numerical analysis
and methods. MATLAB has nice tools for dealing with these objects.
Suppose that we have polynomials P1(s) = s + 2 and P2(s) = 3s + 4 and wish
to multiply them. In this case type the following command sequence:
>> P1 = [ 1 2 ];
>> P2 = [ 3 4 ];
>> conv(P1,P2)
ans =
3
10
8
>>
This is the correct answer since P1(s)P2(s) = 3s2 + 10s + 8. From this we see
polynomials are represented as vectors of the polynomial coefÔ¨Åcients, where the
highest-degree coefÔ¨Åcient is the Ô¨Årst element in the vector. This rule is followed
pretty consistently. (Note: ‚Äúconv‚Äù is the MATLAB convolution function, so if you
don‚Äôt already know this, convolution is mathematically essentially the same as
polynomial multiplication.)
Suppose P (s) = s2 + s ‚àí2, and we want the roots. In this case you may type
>> P = [ 1
1
-2 ];
>> roots(P)
ans =
-2
1
>>
MATLAB (version 5 and later) has ‚Äúmroots,‚Äù which is a root Ô¨Ånder that does a
better job of computing multiple roots.
The MATLAB function ‚Äúpolyval‚Äù is used to evaluate polynomials. For example,
suppose P (s) = s2 + 3s + 5, and we wanted to compute P (‚àí3). The command
sequence is
>> P = [ 1
3
5 ];
>> polyval(P,-3)
TLFeBOOK

572
AN INTRODUCTION TO MATLAB
ans =
5
>>
13.5
LOOPS
We may illustrate the simplest loop construct in MATLAB as follows:
>> t = [0:.1:1];
>> for k = 1:length(t)
>>
x(k) = 5*sin( (pi/3) * t(k) ) + 2;
>>
end;
>>
This command sequence computes x(t) = 5 sin( œÄ
3 t) + 2 for t = 0.1k, where k =
0, 1, . . . , 10. The result is saved in the (row) vector x. However, an alternative
approach is to vectorize the calculation according to
>> t = [0:.1:1];
>> x = 5*sin( pi*t/3 ) + 2*ones(1,length(t));
>>
This yields the same result. Vectorizing calculations leads to faster code (in terms
of runtime).
A potentially useful method to add (append) elements to a vector is
>> x = [];
>> for k = 1:2:6
>>
x = [ x k ];
>>
end;
>> x
x =
1
3
5
>>
where x = [ ] deÔ¨Ånes x to be initially empty, while the for loop appends 1, 3, and
5 to the vector one element at a time.
The format of numerical outputs can be controlled using MATLAB fprintf
(which has many similarities to the ANSI C fprintf function). For example
>> for k = 0:9
fprintf(‚Äô%12.8f\n‚Äô,sqrt(k));
end;
0.00000000
TLFeBOOK

PLOTTING AND M-FILES
573
1.00000000
1.41421356
1.73205081
2.00000000
2.23606798
2.44948974
2.64575131
2.82842712
3.00000000
>>
The use of a Ô¨Åle identiÔ¨Åer can force the result to be printed to a speciÔ¨Åc Ô¨Åle instead
of to the terminal (which is the result in this example). MATLAB also has save
and load commands that can save variables and arrays to memory, and read them
back, respectively.
Certainly, for loops may be nested in the expected manner. Of course, MATLAB
also supports a ‚Äúwhile‚Äù statement. For information on conditional statements (i.e.,
‚Äúif‚Äù statements), use ‚Äú>> help if.‚Äù
13.6
PLOTTING AND M-FILES
Let‚Äôs illustrate plotting and the use of M-Ô¨Åles with an example. Note that M-Ô¨Åles
are also called script Ô¨Åles (use script as the keyword when using help for more
information on this feature).
As an exercise the reader may wish to create a Ô¨Åle called ‚ÄústepH.m‚Äù (open and
edit it in the manner you are accustomed to). In this Ô¨Åle place the following lines:
%
%
stepH.m
%
% This routine computes the unit-step response of the
% LTI system with system function H(s) given by
%
%
K
%
H(s) =
-------------
%
s^2 + 3s + K
%
% for user input parameter K.
The result is plotted.
%
function stepH(K)
b = [ 0 0 K ];
a = [ 1 3 K ];
clf
% Clear any existing plots from the screen
step(b,a);
% Compute the step response and plot it
grid
% plot the grid
TLFeBOOK

574
AN INTRODUCTION TO MATLAB
This M-Ô¨Åle becomes a MATLAB command, and for K = 0.1 may be executed
using
>> stepH(.1);
>>
This will result in another window opening where the plot of the step response will
appear. To save this Ô¨Åle for printing, use
>> print -dps filename.ps
which will save the plot as a postscript Ô¨Åle called ‚ÄúÔ¨Ålename.ps.‚Äù Other printing for-
mats are available. As usual, the details are available from online help. Figure 13.1
is the plot produced by stepH.m for the speciÔ¨Åed value of K.
Another example of an M-Ô¨Åle that computes the frequency response (both mag-
nitude and phase) of the linear time-invariant (LTI) system with Laplace transfer
function is
H(s) =
1
s2 + 1
2s + 1
.
Time (seconds)
Amplitude
0
30
60
90
120
150
180
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Figure 13.1
Step response: typical output from stepH.m.
TLFeBOOK

PLOTTING AND M-FILES
575
If you are not familiar with Laplace transforms, recall phasor analysis from basic
electric circuit theory. You may, for instance, interpret H(jœâ) as the ratio of two
phasor voltages (frequency œâ), such as
H(jœâ) = V2(jœâ)
V1(jœâ).
The numerator phasor V2(jœâ) is the output of the system, and the denominator
phasor V1(jœâ) is the input to the system (a sinusoidal voltage source).
%
%
freqresp.m
%
% This routine plots the frequency response of the Laplace
% transfer function
%
%
1
%
H(s) =
----------------
%
s^2 + .5s
+
1
%
% The magnitude response (in dB) and the phase response (in
% degrees) are returned in the vectors mag, and pha,
% respectively.
The places on the frequency axis where the
% response is computed are returned in vector f.
%
function [mag,pha,f] = freqresp
b = [ 0 0 1];
a = [ 1 .5 1 ];
w = logspace(-2,1,50); % Compute the frequency response for 10^(-2) to 10^1
% radians per second at 50 points in this range
h = freqs(b,a,w);
% h is the frequency response
mag = abs(h);
% magnitude response
pha = angle(h);
% phase response
f = w/(2*pi);
% setup frequency axis in Hz
pha = pha*180/pi;
% phase now in degrees
mag = 20*log10(mag);
% magnitude response now in dB
clf
subplot(211), semilogx(f,mag,‚Äô-‚Äô), grid
xlabel(‚Äô Frequency (Hz) ‚Äô)
ylabel(‚Äô Amplitude (dB) ‚Äô)
title(‚Äô Magnitude Response ‚Äô)
subplot(212), semilogx(f,pha,‚Äô-‚Äô), grid
xlabel(‚Äô Frequency (Hz) ‚Äô)
ylabel(‚Äô Phase Angle (Degrees) ‚Äô)
title(‚Äô Phase Response ‚Äô)
Executing the command
>> [mag,phase,f] = freqresp;
>>
TLFeBOOK

576
AN INTRODUCTION TO MATLAB
10‚àí3
10‚àí2
10‚àí1
100
101
‚àí40
‚àí30
‚àí20
‚àí10
0
10
Frequency (Hz)
Amplitude (decibels)
10‚àí3
10‚àí2
10‚àí1
100
101
‚àí200
‚àí150
‚àí100
‚àí50
0
Frequency (Hz)
Phase angle (degrees)
(a)
(b)
Figure 13.2
The output from freqresp.m: (a) magnitude response; (b) phase response.
will result in the plot of Fig. 13.2, and will also give the vectors mag, phase,
and f. Vector mag contains the magnitude response (in decibels), and vector phase
contains the phase response (in degrees) at the sample values in the vector f, which
deÔ¨Ånes the frequency axis for the plots (in hertz). In other words, ‚Äúfreqresp.m‚Äù is a
Bode plotting routine. Note that the MATLAB command ‚Äúbode‚Äù does Bode plots
as well.
Additional labeling may be applied to plots using the MATLAB command text
(or via a mouse using ‚Äúgtext‚Äù; see online help). As well, the legend statement is
useful in producing labels for a plot with different curves on the same graph. For
example
function ShowLegend
ul = 2.5*pi;
ll = -pi/4;
N = 200;
dt = (ul - ll)/N;
for k = 0:N-1
t(k+1) = ll + dt*k;
x(k+1) = exp(-t(k+1));
y(k+1) = sin(t(k+1));
end;
TLFeBOOK

REFERENCES
577
‚àí1
0
1
2
3
4
5
6
7
8
‚àí1
‚àí0.5
0
0.5
1
1.5
2
2.5
t
e‚àít
sin(t)
Figure 13.3
Illustration of the MATLAB legend statement.
subplot(211), plot(t,x,‚Äô-‚Äô,t,y,‚Äô--‚Äô), grid
xlabel(‚Äô t ‚Äô)
legend(‚Äô e^{-t} ‚Äô, ‚Äô sin(t) ‚Äô)
When this code is run, it gives Fig. 13.3. Note that the label syntax is similar to
that in LaTeX [6].
Other sample MATLAB codes have appeared as appendixes in earlier chapters,
and the reader may wish to view these as additional examples.
REFERENCES
1. G. H. Golub and C. F. Van Loan, Matrix Computations, 2nd ed. Johns Hopkins Univ.
Press, Baltimore, MD, 1989.
2. D. R. Hill, Experiments in Computational Matrix Algebra (C. B. Moler, consulting ed.),
Random House, New York, 1988.
3. S. Nakamura, Numerical Analysis and Graphic Visualization with MATLAB, 2nd ed.
Prentice-Hall, Upper Saddle River, NJ, 2002.
4. A. Quarteroni, R. Sacco, and F. Saleri, Numerical Mathematics (Texts in Applied Math-
ematics series, Vol. 37). Springer-Verlag, New York, 2000.
5. G. Recktenwald, Numerical Methods with MATLAB: Implementation and Application,
Prentice-Hall, Upper Saddle River, NJ, 2000.
6. M. Goossens, F. Mittelbach, and A. Samarin, The LaTeX Companion, Addison-Wesley,
Reading, MA, 1994.
TLFeBOOK

TLFeBOOK

INDEX
Absolute convergence, 95
Additive splitting, 179
Adjoint matrix, 489
Alternating set, 239
Ampere‚Äôs law, 535
Anechoic chamber, 544
Appolonius‚Äô identity, 35
Argmin, 39
ASCII, 339
Asymptotic expansion, 97
Asymptotic series, 97‚Äì103
Asymptotic time complexity, 155
Backtracking line search, 346,353
Backward difference
approximation, 402
Backward Euler method
see Euler‚Äôs method of solution
(implicit form)
Backward substitution, 156
Banach Ô¨Åxed-point theorem, 297,422
see also Contraction theorem
Banach space, 68
Basic QR iterations algorithm, 512
Bauer-Fike theorem, 520
Bernoulli‚Äôs differential equation, 426
Bernoulli‚Äôs inequality, 123
Bernoulli numbers, 398
Bessel‚Äôs inequality, 217
‚ÄúBig O‚Äù notation, 155,296
Binary number codes
One‚Äôs complement, 57
Sign-magnitude, 56
An Introduction to Numerical Analysis for Electrical and Computer Engineers, by C.J. Zarowski
ISBN 0-471-46737-5
c‚Éù2004 John Wiley & Sons, Inc.
Two‚Äôs complement, 57
Binomial theorem, 85
Bipolar junction transistor (BJT)
Base current, 420
Base-emitter voltage, 420
Collector current, 420
Collector-emitter voltage, 420
Forward current gain, 420
On resistance, 420
Bisection method, 292‚Äì296
Block upper triangular matrix, 510
Boundary conditions, 527
Boundary value problem (BVP), 445
C, 38,339,565
C++, 38
Cancellation, 44
Cantor‚Äôs intersection theorem, 294
Cartesian product, 2
Catastrophic cancellation, 62,117
Catastrophic convergence, 90
Cauchy sequence, 64
Cauchy-Schwarz inequality, 139
Cayley-Hamilton theorem, 489
Central difference approximation, 402
Chaotic sequence, 323
Characteristic equation, 458,549
Characteristic impedance, 538
Characteristic impedance of free
space, 539
Characteristic polynomial, 144,481
Chebyshev approximation, 239
Chebyshev norm, 239
579
TLFeBOOK

580
INDEX
Chebyshev polynomials of the Ô¨Årst kind,
218‚Äì225
Chebyshev polynomials
of the second kind, 243‚Äì245
Cholesky decomposition, 167,355
Christoffel-Darboux formula, 211,389
Christoffel numbers, 389
Closed subset, 299
Cofactor, 489
Colpitts oscillator, 419,465,538
Compactly supported, 278
Companion matrix, 515,519
Complementary error function, 98
Complementary sine integral, 100
Complete solution, 532
Complete space, 64
Complete spline, 275
Completing the square, 131
Complex numbers
Cartesian form, 28
Imaginary part, 31
Magnitude, 29
Polar form, 28
Real part, 31
Computational complexity, 154
Computational efÔ¨Åciency, 89
Computational region, 551
Condition number of a matrix,
135‚Äì147
Conic section, 519,526
Conjugate gradient methods, 527
Conjugate symmetry, 35
Continuous mapping, 11
Contraction (contractive)
mapping, 184,297
Contraction theorem, 183,298
Contra-identity matrix, 199
Convergence in the mean, 217
Convergence of a sequence, 63
Convex function, 352
Convolution, 204
Convolution integral, 104,444
Coordinate rotation digital computing
(CORDIC) method, 70,107‚Äì116
Countably inÔ¨Ånite set, 22
Courant-Friedrichs-Lewy (CFL)
condition, 547
Courant parameter, 546
Crank-Nicolson method, 528
Cubic B-spline, 272
Current-controlled current source
(CCCS), 420
Curve Ô¨Åtting, 252
Cyphertext sequence, 325
D‚ÄôAlembert solution, 537
Daubechies 4-tap scaling function, 520
Daubechies wavelets, 18
Deadbeat synchronization, 326
Defective matrix, 483
DeÔ¨Çation theorem, 507
Degree of freedom, 440
Descent algorithm, 352
Detection threshold, 333
Diagonal dominance, 182,281
Diagonalizable matrix, 445
Diffusion equation
see Heat equation
Dimension, 22
Dirichlet kernel, 74,103
Discrete basis, 70,108
Discrete convolution, 34
Discrete Fourier series, 25
Discrete Fourier transform
(DFT), 27,37,112
Divergent sequence, 64
Divided differences, 257‚Äì259
Divided-difference table, 264
Dominant eigenvalue, 498
Double sequence, 69
DufÔ¨Ång equation,
415,447‚Äì448,453‚Äì454
Eavesdropper, 331
Eigenfunction, 532
Eigenpair, 480
Eigenproblem, 480
Eigenspace, 483
TLFeBOOK

INDEX
581
Eigenvalue
Matrix, 143,480
Partial differential equation
(PDE), 532
Eigenvector, 480
Electric Ô¨Çux density, 535
Electrical conductivity, 540
Elementary logic
Logical contrary, 32
Logically equivalent, 32
Necessary condition, 31
SufÔ¨Åcient condition, 31
Elliptic integrals, 410‚Äì411
Elliptic partial differential
equation, 526
Equality constraint, 357
Error control coding, 327
Encryption key, 325
Energy
Of a signal (analog), 13
Of a sequence, 13
Error
Absolute, 45,60,140
Relative, 45,60,140,195
Euler-Maclaurin formula, 397
Error function, 94
Euclidean metric, 6
Euclidean space, 12,16
Euler‚Äôs identity, 19,30
Exchange matrix
see Contra-identity matrix
Faraday‚Äôs law, 535
Fast Fourier transform (FFT), 27
Feasible direction, 357
Feasible incremental step, 357
Feasible set, 357
Fejer‚Äôs theorem, 122
Finite difference (FD) method, 545‚Äì550
Finite-difference time-domain (FDTD)
method, 525,540,550‚Äì557
Finite-element method (FEL), 528
Finite impulse response (FIR) Ô¨Åltering,
34,367
Finite-precision arithmetic effects, 1
Fixed-point dynamic range, 39
Fixed-point number
representations, 38‚Äì41
Fixed-point overÔ¨Çow, 40
Fixed-point method, 296‚Äì305,312‚Äì318
Fixed-point rounding, 41
Floating-point chopping, 47
Floating-point dynamic range, 42
Floating-point exponent, 42
Floating-point mantissa, 42
Floating-point normalization, 45
Floating-point number
representations, 42‚Äì47
Floating-point overÔ¨Çow, 43
Floating-point rounding, 43
Floating-point underÔ¨Çow, 43
Flop, 154
Floquet theory, 522
Forcing term, 548
FORTRAN, 38
Forward difference approximation, 401
Forward elimination, 156
Forward substitution
see Forward elimination
Fourier series, 18,73
Free space, 535
Fresnel coefÔ¨Åcients, 543
Frobenius norm, 140
Full rank matrix, 161
Function space, 4
Fundamental mode, 532
Fundamental theorem
of algebra, 254,484
Gamma function, 90
Gaussian function
see Gaussian pulse
Gaussian probability
density function, 413
Gaussian pulse, 92,556
Gauss-Seidel method, 181,527
Gauss-Seidel successive overrelaxation
(SOR) method, 181
TLFeBOOK

582
INDEX
Gauss transformation matrix, 148
Gauss vector, 149
Generalized eigenvectors, 485
Generalized Fourier series
expansion, 218
Generalized mean-value theorem, 79
Generating functions
Hermite polynomials, 227
Legendre polynomials, 233
Geometric series, 34
Gibbs phenomenon, 77
Givens matrix, 514
Global truncation error, 548
Golden ratio, 347
Golden ratio search method, 346
Gradient operator, 342
Greatest lower bound
see InÔ¨Åmum
Group speed, 556
Haar condition, 239
Haar scaling function, 17,36
Haar wavelet, 17,36
Hankel matrix, 412
Harmonics, 162,532
Heat equation, 527
Henon map, 324
Hermite‚Äôs interpolation formula,
267‚Äì269,387
Hermite polynomials, 225‚Äì229
Hessenberg matrix, 205,512
Hessenberg reduction, 513
Hessian matrix, 344
Hilbert matrix, 133
Hilbert space, 68
Holder inequality, 139
Homogeneous material, 535
Householder transformation, 169‚Äì170
Hump phenomenon, 496
Hyperbolic partial differential
equation, 526
Hysteresis, 535
Ideal diode, 106
Idempotent matrix, 170
IEEE Ô¨Çoating-point standard, 42
If and only if (iff), 32
Ill-conditioned matrix, 132
Increment function, 437
Incremental condition estimation
(ICE), 367
Inequality
Cauchy-Schwarz, 15
Holder, 10
Minkowski, 10
Schwarz, 24,36
InÔ¨Åmum, 5
Initial conditions, 528‚Äì529
Initial value problem (IVP)
Adams-Bashforth (AB) methods
of solution, 459‚Äì461
Adams-Moulton (AM) methods
of solution, 461‚Äì462
Chaotic instability, 437
DeÔ¨Ånition, 415‚Äì416
Euler‚Äôs method of solution (explicit
form), 423,443
Euler method of solution (implicit
form), 427,443
Global error, 465
Heun‚Äôs method of solution, 433,453
Midpoint method of solution,
457‚Äì458
Model problem, 426,443
Multistep predictor-corrector
methods of solution, 456‚Äì457
Parasitic term, 459
Runge-Kutta methods of solution,
437‚Äì441
Runge-Kutta-Fehlberg (RKF)
methods of solution, 464‚Äì466
Single-step methods of solution, 455
Stability, 421,423,427,433,436,
445‚Äì447,458‚Äì459,462‚Äì463
Stability regions, 463
Stiff systems, 432,467‚Äì469
Trapezoidal method of solution, 436
Truncation error per step, 432‚Äì434
Inner product, 14
Inner product space, 14
TLFeBOOK

INDEX
583
Interface between media, 541
Integral mean-value theorem, 96
Integration between zeros, 385
Integration by parts, 90
Intermediate value theorem, 292,383
Interpolation
Hermite, 266‚Äì269,381
Lagrange, 252‚Äì257
Newton, 257‚Äì266
Polynomial, 251
Spline, 269‚Äì284
Interpolatory graphical display
algorithm (IGDA), 521
Invariance (2-norm), 168
Invariant subspace, 511
Inverse discrete Fourier transformi
(IDFT), 27
Inverse power algorithm, 503
Isotropic material, 535
Jacobi algorithm, 203
Jacobi method, 181,527
Jacobi overrelaxation
(JOR) method, 181
Jacobian matrix, 321
Jordan blocks, 484
Jordan canonical form, 483
Kernel function, 215
Key size, 324,329
Kirchoff‚Äôs laws, 418
Known plaintext attack, 331,339
Kronecker delta sequence, 22
L‚ÄôHopital‚Äôs rule, 81
Lagrange multipliers, 142,166,357‚Äì358
Lagrange polynomials, 254
Lagrangian function, 359
Laplace transform, 416
Law of cosines, 169
Leading principle submatrix, 151
Leapfrog algorithm, 551
Least upper bound
see Supremum
Least-squares, 127‚Äì128,132,161
Least-squares approximation using
orthogonal polynomials, 235
Lebesgue integrable functions, 67
Legendre polynomials, 229‚Äì235
Levinson-Durbin algorithm, 200
Limit of a sequence, 63
Line searches, 345‚Äì353
Linearly independent, 21
Lipschitz condition, 421
Lipschitz constant, 421
Lissajous Ô¨Ågure, 478
Local minima, 343
Local minimizer, 365
Local truncation error, 545
Logistic equation, 436
Logistic map, 323,437
Lossless material, 539
Lower triangular matrix, 148
LU decomposition, 148
Machine epsilon, 53
Maclaurin series, 85
Magnetic conductivity, 540
Magnetic Ô¨Çux density, 535
Manchester pulse, 17
Mathematical induction, 116
MATLAB, 34,54,90,102,117‚Äì118,126,
129,134‚Äì135,137,146,185,
186‚Äì191, 197‚Äì198,201,205‚Äì206,
244‚Äì246,248,287‚Äì289,292,336,
338‚Äì339,351,362‚Äì366,409‚Äì411,
413,442,465‚Äì467,469‚Äì472,
475‚Äì479,498,509,522‚Äì524,552,
557‚Äì559,561,563‚Äì564,565‚Äì577
Matrix exponential, 444,488‚Äì498
Matrix exponential condition
number, 497
Matrix norm, 140
Matrix norm equivalence, 141
Matrix operator, 176
Matrix spectrum, 509
TLFeBOOK

584
INDEX
Maxwell‚Äôs equations, 535,540
Mean value theorem, 79,303
Message sequence, 325
Method of false position, 333
Method of separation of variables, 530
Metric, 6
Metric induced by the norm, 11
Metric space, 6
Micromechanical resonator, 416
Minor, 489
Moments, 140,411
Natural spline, 275
Negative deÔ¨Ånite matrix, 365
Nesting property, 199
Newton‚Äôs method, 353‚Äì356
Newton‚Äôs method with backtracking
line search, 354‚Äì356
Newton-Raphson method breakdown
phenomena, 311‚Äì312
Newton-Raphson method, 305‚Äì312,
318‚Äì323
Newton-Raphson method rate of con-
vergence, 309‚Äì311
Nondefective matrix, 483
Nonexpansive mapping, 297
Nonlinear observer, 326
Non-return-to-zero (NRZ) pulse, 17
Norm, 10
Normal equations, 164
Normal matrix, 147,498
Normal mode, 532
Normed space, 10
Numerical dispersion, 550,555‚Äì556
Numerical integration
Chebyshev-Gauss quadrature, 389
Composite Simpson‚Äôs rule, 380
Composite trapezoidal rule, 376
Corrected trapezoidal rule, 394
Gaussian quadrature, 389
Hermite quadrature, 388
Left-point rule, 372
Legendre-Gauss quadrature, 391
Midpoint rule, 373,409
Rectangular rule, 372
Recursive trapezoidal rule, 398
Richardson‚Äôs extrapolation, 396
Right-point rule, 372
Romberg integration formula, 397
Romberg table, 397,399
Simpson‚Äôs rule, 378‚Äì385
Simpson‚Äôs rule truncation error,
380,383
Trapezoidal rule, 371‚Äì378
Trapezoidal rule truncation error for-
mula, 375‚Äì376
Objective function, 341
Orbit
see Trajectory
Order of a partial differential equation,
525
Ordinary differential equation
(ODE), 415
Orthogonality, 17
Orthogonal matrix, 161
Orthogonal polynomial, 208
Orthonormal set, 23
Overdetermined least-squares
approximation, 161
Overrelaxation method, 182
Overshoot, 76‚Äì77
Overtones
see Harmonics
Parabolic partial differential
equation, 526
Parallelogram equality, 15
Parametric ampliÔ¨Åer, 473
Parseval‚Äôs equality, 216
Partial differential equation (PDE), 525
PASCAL, 38
Perfectly matched layer (PML),
540,544,553
Permeability, 535
Permittivity, 535
Permutation matrix, 205
TLFeBOOK

INDEX
585
Persymmetry property, 199
Phase portrait, 454
Phase speed, 555
Phasor analysis, 539
Picard‚Äôs theorem, 422
Pivot, 151
Plane electromagnetic waves, 534‚Äì535
Plane waves, 536
Pointwise convergence, 71
Poisson equation, 526
Polynomial, 4
Positive deÔ¨Ånite matrix, 130
Positive semideÔ¨Ånite matrix, 130
Power method, 498‚Äì500
Power series, 94
Probability of false alarm, 333
Product method
see Method of separation of variables
Projection operator, 170,197
Propagation constant, 539
Pseudoinverse, 192
Pumping frequency, 473
QR decomposition, 161
QR factorization
see QR decomposition
QR iterations, 508‚Äì518
Quadratic form, 129,164
Quantization, 39
Quantization error, 40
Quartz crystal, 416
Radius of convergence, 95
Rate of convergence, 105,296
Ratio test, 233
Rayleigh quotient iteration, 508,523
Real Schur decomposition, 511
ReÔ¨Çection coefÔ¨Åcient, 543
ReÔ¨Çection coefÔ¨Åcients of a Toeplitz
matrix, 201
Regula falsi
see Method of false position
Relative permeability, 535
Relative permittivity, 535
Relative perturbations, 158
Residual reÔ¨Çections, 555
Residual vector, 144‚Äì145,168,180
Riemann integrable functions, 68‚Äì69
Ringing artifacts, 77
Rodrigues formula, 213,230
Rolle‚Äôs theorem, 265
Rosenbrock‚Äôs function, 342
Rotation operator, 112‚Äì113,120,484
Rounding errors, 38
Roundoff errors
see Rounding errors
Runge‚Äôs phenomenon, 257
Saddle point, 343
Scaled power algorithm, 580
Schelin‚Äôs theorem, 109
Second-order linear partial differential
equation, 526
Sequence, 3
Sequence of partial sums, 74,122
Sherman-Morrison-Woodbury
formula, 203
Shift parameter, 502,516
Similarity transformation, 487
Simultaneous diagonalizability, 520
Sine integral, 100
Single-shift QR iterations
algorithm, 517
Singular values, 147,165
Singular value decomposition
(SVD), 164
Singular vectors, 165
Stability, 157
State variables, 416
State vector, 325
Stationary point, 343
Stiffness quotient, 479
Stirling‚Äôs formula, 91
Strange attractor, 466
Strongly nonsingular matrix
see Strongly regular matrix
Strongly regular matrix, 201
TLFeBOOK

586
INDEX
Subharmonics, 162
Submultiplicative property of matrix
norms, 141
Summation by parts, 249
Spectral norm, 147
Spectral radius, 177
Spectrum, 532
Speed of light, 538
Supremum, 5
Symmetric part of a matrix, 203
Taylor series, 78‚Äì96
Taylor polynomial, 85
Taylor‚Äôs theorem, 440
Three-term recurrence formula (relation)
DeÔ¨Ånition, 208
Chebyshev polynomials of the Ô¨Årst
kind, 222
Chebyshev polynomials of the second
kind, 243
Gram polynomials, 249
Hermite polynomials, 229
Legendre polynomials, 234
Toeplitz matrix, 199
Trace of a matrix, 206
Trajectory, 454
Transmission coefÔ¨Åcient, 543
Transverse electromagnetic (TEM)
waves, 586
Tridiagonal matrix, 199,275,516
Truncation error, 85
Truncation error per step, 546
Two-dimensional Taylor series,
318‚Äì319
Tychonov regularization, 368
Uncertainty principle, 36
Underrelaxation method, 182
Uniform approximation, 222,238
Uniform convergence, 71
Unique global minimizer, 344
Unitary space, 12,16
Unit circle, 248,502
Unit roundoff, 53
Unit sphere, 141
Unit step function, 421
Unit vector, 17,170
UNIX, 565
Upper quasi-triangular matrix, 512
Upper triangular matrix, 148
Vandermonde matrix, 254,285
Varactor diode, 473
Vector electric Ô¨Åeld intensity, 534
Vector magnetic Ô¨Åeld intensity, 535
Vector, 9
Vector dot product, 16,48
Vector Hermitian transpose, 16
Vector norm, 138
Vector norm equivalence, 139
Vector space, 9
Vector subspace, 239
Vector transpose, 16
Vibrating string, 528‚Äì534
Von Neumann stability analysis, 548
Wavelength, 538
Wavelet series, 18
Wave equation
Electric Ô¨Åeld, 536
Magnetic Ô¨Åeld, 536
Weierstrass‚Äô approximation
theorem, 216
Weierstrass function, 384
Weighted inner product, 207
Weighting function, 207
Well-conditioned matrix, 137
Zeroth order modiÔ¨Åed Bessel function
of the Ô¨Årst kind, 400
TLFeBOOK


Software Data 
Engineering for 
Network eLearning 
Environments
Santi Caballé
Jordi Conesa   Editors
Analytics and Awareness Learning 
Services
Lecture Notes on Data Engineering
and Communications Technologies 11

Lecture Notes on Data Engineering
and Communications Technologies
Volume 11
Series editor
Fatos Xhafa, Technical University of Catalonia, Barcelona, Spain
e-mail: fatos@cs.upc.edu

The aim of the book series is to present cutting edge engineering approaches to data
technologies and communications. It publishes latest advances on the engineering task
of building and deploying distributed, scalable and reliable data infrastructures and
communication systems.
The series has a prominent applied focus on data technologies and communications
with aim to promote the bridging from fundamental research on data science and
networking to data engineering and communications that lead to industry products,
business knowledge and standardisation.
More information about this series at http://www.springer.com/series/15362

Santi Caballé ⋅Jordi Conesa
Editors
Software Data Engineering
for Network eLearning
Environments
Analytics and Awareness Learning Services
123

Editors
Santi Caballé
Faculty of Computer Science, Multimedia
and Telecommunications
Universitat Oberta de Catalunya
Barcelona
Spain
Jordi Conesa
Faculty of Computer Science, Multimedia
and Telecommunications
eHealth Center
Universitat Oberta de Catalunya
Barcelona
Spain
ISSN 2367-4512
ISSN 2367-4520
(electronic)
Lecture Notes on Data Engineering and Communications Technologies
ISBN 978-3-319-68317-1
ISBN 978-3-319-68318-8
(eBook)
https://doi.org/10.1007/978-3-319-68318-8
Library of Congress Control Number: 2017961751
© Springer International Publishing AG 2018
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part
of the material is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations,
recitation, broadcasting, reproduction on microﬁlms or in any other physical way, and transmission
or information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar
methodology now known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this
publication does not imply, even in the absence of a speciﬁc statement, that such names are exempt from
the relevant protective laws and regulations and therefore free for general use.
The publisher, the authors and the editors are safe to assume that the advice and information in this
book are believed to be true and accurate at the date of publication. Neither the publisher nor the
authors or the editors give a warranty, express or implied, with respect to the material contained herein or
for any errors or omissions that may have been made. The publisher remains neutral with regard to
jurisdictional claims in published maps and institutional afﬁliations.
Printed on acid-free paper
This Springer imprint is published by Springer Nature
The registered company is Springer International Publishing AG
The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland

To Maria, Neus and Aniol: thank you for
being our force of inspiration and motivation

Preface
Data analysis is a cornerstone of online learning environments. Since the ﬁrst
conception of eLearning and collaborative systems to support learning and teach-
ing, data analysis has been employed to support learners, teachers, researchers,
managers, and policy makers with useful information on learning activities and
learning design.
While data analysis originally employed mainly statistical techniques due to the
modest amounts and varieties of data being gathered, with the rapid development of
Internet technologies and increasingly sophisticated online learning environments,
increasing volumes and varieties of data are being generated and data analysis has
moved to more complex analysis techniques, such as educational data mining and,
most recently, and learning analytics.
Now powered by cloud technologies, online learning environments are capable of
gathering and storing massive amounts of data of various formats, and tracking user–
system and user–user interactions as well as rich contextual information in such
systems. This has led to the need to address the deﬁnition, modeling, development,
and deployment of sophisticated learning services, offering analytics and context
awareness information to all participants and stakeholders in online learning.
The book “Software Data Engineering for Network eLearning Environments:
Analytics and Awareness Learning Services” covers scientiﬁc and technical per-
spectives that will contribute to the advance of the state of the art and provide better
understanding of the different problems and challenges of current eLearning and
general education. In particular, the book will address innovative techniques and
experiences in data analytics and context awareness in learning systems, driven by
service-based architectures and cloud technologies.
The ultimate aim of this book is to stimulate research from both theoretical and
practical views, which will allow other educational institutions and organizations to
apply, evaluate, and reproduce the book’s contributions. We hope that industry and
academic researchers, professionals, and practitioners ﬁnd the book fruitful and can
incorporate the research reported in this book into their activities and products.
vii

This book consists of 10 chapters organized into three major areas:
• Strategies and Methodologies based on Learning Data Analysis. The chapters in
this area are concerned about the use of data analytics methods and techniques in
order to provide meaningful services to all the involved stakeholders in academia,
such as monitor the quality of academic processes, provide immediate feedback
about the learning process, and measure and visualize student engagement.
• Applications of Analytics and Awareness Learning Services to eLearning. In
this area the chapters present state-of-the-art educative applications based on
analytics and awareness services, such as multimodal conversational agents,
game-based systems and feedback based on automatic hints.
• Practical Use Cases and Evaluation in Real Context of eLearning. The chapters
covering this area provide practical use cases of eLearning advances involving
data analytics, which evaluate their pedagogical possibilities and impact in real
contexts of learning.
The chapters in the ﬁrst area of Strategies and Methodologies Based on
Learning Data Analysis are organized as follows:
In Chapter “Predictive Analytics: Another Vision of the Learning Process”,
Bañeres and Serra present an approach for teachers who are always in need of new
tools to support the learning process. The approach is based on learning analytics,
which the authors claim it has emerged as a solution to provide feedback about the
learning progress of students. This solution does not only provide meaningful
information to instructors to analyze and improve the learning process but also to
managers and other stakeholders of the learning processes. In this chapter, the authors
extend the vision of learning analytics to predictive analytics. The authors believe that
currently, teachers are ready to see further in the future and predict the behavior of
students based on their actions, and this idea opens a broad potential for educational
settings. This chapter discusses challenges, beneﬁts, and weaknesses of a predictive
system for education. Additionally, the design of a generic predictive system is pro-
posed and experimental results in a real scenario are shown to validate its potential.
Amigud et al. in Chapter “A Procedural Learning and Institutional Analytics
Framework” investigate on data analyses to provide the means for monitoring the
quality of academic processes and the means for assessing the ﬁscal and operational
health of an organization. The authors claim that data-drivendecision-making can
help to empower academic leaders, faculty, and staff with quantitative insights that
guide strategies pertaining to enrollment and retention, student support and quality
assurance, communication, bullying intervention, academic progress, and academic
integrity. However, the integration of analytics into the institutional context is not a
trivial process. Much of the analytics approaches discussed in the literature take a
theoretical stance outlining main considerations but lacking the pragmatic edge. In
this chapter, the authors aim to assist academic leaders in undertaking analytics
design and implementation. To this end, they synthesize the existing research and
propose a procedural framework for integrating data analysis techniques and
methods into a process that facilitates data-drivendecision-making by aligning
institutional needs with actionable strategies.
viii
Preface

Chapter “Engagement Analytics: A Microlevel Approach to Measure and
Visualize Student Engagement” by Balasooriya et al. is devoted to learner disen-
gagement as a persisting issue in the Science Technology Engineering and Math-
ematics (STEM) subjects. The authors believe that student engagement is
dynamically constituted by the behavioral, cognitive, and emotional dimensions of
engagement in a learning environment. They also claim that although strongly
linked with academic achievement, much of the details of engagement becomes lost
in a retrospective measurement. Timely and microlevel data, on the other hand, has
the ability to enrich the traditional learning analytics dataset. From a pilot study
carried out at Universitat Oberta de Catalunya, the authors have designed a
self-reported data capture module that collects microlevel engagement data. The
initial results suggest the validity of the proposed approach and data. In this chapter,
the authors emphasize how their approach enables better understanding of the
student learning process and their characteristics such as cognitive patterns, emo-
tional states, and behaviors that lead to academic success and also enable richer
feedback from teachers and informed decision-making by the institution.
In Chapter “Learning Analytics in Mobile Applications Based on Multimodal
Interaction”, Mota-Macías et al. discuss on the ability for teachers to produce their
own digital solutions by translating teaching concepts into end user computer
systems as one of the most valuable skills of teachers. The authors claim that this
ability often requires the involvement of computing specialists, and as a result, the
development of educational programming environments remains a challenge. They
also believe that learning experiences based multimodal interaction applications
(gesture interaction, voice recognition or artiﬁcial vision) are becoming common-
place in education because they motivate and involve students. This chapter ana-
lyzes the state of the art in LA techniques and user-friendly authoring tools. It
presents a tool to support the creation of multimodal interactive applications
equipped with nonintrusive monitoring and analytics capabilities. This tool enables
teachers with no programming skills to create interactive LA-enriched learning
scenarios. To this end, several components that manage LA activities are included
in the tool, they range from automatically capturing users’ interaction with mobile
applications to querying data and retrieving metrics, to visualizing tables and charts.
The chapters in the second area of Applications of Analytics and Awareness
Learning Services to eLearning are organized as follows:
Chapter “Increasing the Role of Data Analytics in m-Learning Conversational
Applications” by Griol and Callejas addresses the topic of technology integration as
an increasingly crucial element of teaching and learning. The authors claim that
devices such as smartphones, tablets, and wearables open new learning scenarios
that demand more sophisticated interfaces. In this chapter, the authors describe the
rich variety of educative applications of multimodal conversational systems. They
also describe a framework based on conversational interfaces in mobile learning to
enhance the learning process and experience. Their approach focuses on the use of
NLP techniques, such as speech and text analytics, to adapt and personalize stu-
dent’s conversational interfaces. The chapter also presents a practical application
Preface
ix

that shows the possibilities of our framework to develop pedagogical conversational
agents targeted at different users and pedagogical contents.
Karakostas et al. in Chapter “Enhancing Virtual Learning Spaces: The Impact
of the Gaming Analytics” are concerned about online virtual labs, which have been
important to educational practice by providing students with distance courses that
otherwise would be difﬁcult to be offered. However, the authors claim that the
majority of virtual labs cannot be easily applied to different courses or pedagogical
approaches. In order to overcome this, they propose a high-level, easy-to-use
authoring tool that will allow building course-independenthigh-standard virtual
labs. This solution is based on learning and gaming analytics. According to the
authors, in the gaming industry, there have been developed strong game analytics
methods and tools, which could be easily transferred into the learning domain.
Game analytics monitor the users’ activity, model their current behavior through the
use of shallow analytics, and predict the future behavior of the users through the use
of deep analytics. Finally, the authors propose that both of these approaches
combined with visualization methodologies will offer insights on what features are
important and what functionalities users expect to ﬁnd in a virtual lab.
Chapter “Advice for Action with Automatic Feedback Systems” by Whitelock
reviews the role of feedback in supporting student learning. It highlights some
of the problems that persist with providing meaningful feedback, which should
preferably take the form of providing advice that can be actioned by the student. It
then discusses the progress made with automatic feedback through a number of case
studies which include the OpenEssayist, Open Comment, and OpenMentor
computer-assisted feedback systems. Findings suggest feedback that provides
socio-emotive support to students, together with recognizing their effort, in turn
encourages the student to continue working on a problem. The author then claims
that the use of automatic hints also moves the feedback closer to “Advice for
Action”. As a result, building tools with automatic feedback to support both stu-
dents and tutors can relieve some of the continual pressure on staff resources and
three case studies are presented below that address this issue.
The chapters in the third and last area of Practical Use Cases and Evaluation in
Real Context of eLearning are organized as follows:
Hernández-Rizzardini
and
Amado-Salvatierra
in
Chapter
“Towards
Full
Engagement
for
Open
Online
Education.
A
Practical
Experience
from
MicroMasters at edX” present an innovative framework with the aim to create
full engagement for the learners on massive open online learning environments
through a connectivist approach. The proposed framework relies on the importance
of creating engaging experiences before, during, and after the ﬁnish of a course to
increase learners’ participation and reduce dropout rates with the help of learning
analytics. This work presents a compelling idea in the universe of MOOCs: It
intends to expand the efforts of the learning design team to achieve pre- and
post-course engagement, where engagement takes the form of an ongoing com-
munity of learners. This research provides results from the ﬁrst successful experi-
ences in two MicroMasters “Professional Android Developer”, taught in English,
and one specialization taught in Spanish: “E-Learning for teachers: create
x
Preface

innovative activities and content” at the edX platform. The MicroMasters shows to
be a great path for career advancement, especially for the underemployed.
Kasthuriarachchi et al. in Chapter “A DataMining Approach to Identify the Factors
Affecting the Academic Success of Tertiary Students in Sri Lanka” address the topic
of educational data mining, which has become a very popular and highly important
area in the domain of data mining. The authors claim that the application of data
mining to education arena arises as a paradigm oriented to design models, methods,
tasks, and algorithms for discovering data from educational domain. It attempts to
uncover data patterns, structure association rules, establish information of unseen
relationships with educational data and many more operations that cannot be per-
formed using traditional computer-based information systems. Further, the authors
believe that educational data mining grows and adopts statistical methods, data
mining methods, and machine learning to study educational data produced mostly by
students, educators, educational management policy makers, and instructors. Finally,
the authors claim that the main objective of applying data mining in education is
primarily to advance learning by enabling data-orienteddecision-making to improve
existing educational practices and learning materials. This study focuses on ﬁnding
the key factors affecting the performance of the students enrolled for technology-
related degree programs in Sri Lanka. According to the authors, the ﬁndings of this
study will positively affect the future decisions about the progress of the students’
performance, quality of the education process, and the future of the education
provider.
In the last Chapter “Evaluating the Acceptance of e-Learning Systems via
Subjective and Objective Data Analysis”, Bouchrika et al. review the adoption of
eLearning technology by the academic community as a long source of research
from multiple disciplines including education, psychology, and computer science.
The authors ﬁrst claim that as more and more academic institutions have opted to
use online technology for their course delivery and pedagogical activities, there has
been a surge of interest in evaluating the acceptance of the academic community to
adopt and accept the use of eLearning management systems. According to the
authors, this is due to the increasing concerns that despite the wide use and
deployment of eLearning technologies, the intended impact on education is not
achieved. This chapter reviews the conducted studies on the use of objective pro-
cedures for evaluating eLearning systems in tandem with subjective data analysis.
The evaluation process consists of understanding further the factors related to the
acceptance and adoption of online educational systems by instructors and students
in order to devise strategies for improving the teaching and research quality.
Final Words
The book covers scientiﬁc and technical research perspectives that contribute to the
advance of the state of the art and provide better understanding of the different
problems and challenges of current eLearning and general education. In particular,
Preface
xi

the book addresses innovative strategies and methodologies in data analytics and
context awareness applied to real contexts of eLearning systems and validated by
practical use cases.
Researchers will ﬁnd in this book the latest trends in these research topics.
Academics will ﬁnd practical insights on how to use conceptual and experimental
approaches in their daily tasks. Meanwhile, developers from the eLearning com-
munity can be inspired and put in practice the proposed models and methodologies
and evaluate them for the speciﬁc purposes of their own work and context.
Finally, we would like to thank the authors of the chapters and also the referees
for their invaluable collaboration and prompt responses to our enquiries, which
enabled completion of this book on time. We also gratefully acknowledge the
feedback, assistance, and encouragement received from the editorial staff of
Springer, Anjana Bhargavan and Sooryadeepth Jayakrishnan as well as the book
series editor Dr. Fatos Xhafa.
We hope the readers of this book will ﬁnd it a valuable resource in their research,
development, and educational activities in onlineteaching and learning environments.
Barcelona, Spain
Santi Caballé
Jordi Conesa
xii
Preface

Acknowledgements
This edited volume follows the First International Workshop on Analytics &
Awareness Learning Services (A2LS-2016) in conjunction with the 11th Interna-
tional Conference on P2P, Parallel, Grid, Cloud and Internet Computing
(3PGCIC-2016), held in Asan, Korea, November 5–7, 2016. The Web page of the
WS A2LS-2016 is found at: http://smartlearn.uoc.edu/events/A2LS2016
The theme of the Workshop A2LS-2016 and this edited volume is supported by
the research project “ICT-FLAG” (TIN2013-45303-P) funded by the Spanish
Government. This book is also supported by the European Commission through the
project “colMOOC: Integrating Conversational Agents and Learning Analytics in
MOOCs” (588438-EPP-1-2017-1-EL-EPPKA2-KA).
Barcelona, Spain
Santi Caballé
February 2018
Jordi Conesa
xiii

Contents
Predictive Analytics: Another Vision of the Learning Process . . . . . . . .
1
David Bañeres and Montse Serra
A Procedural Learning and Institutional Analytics Framework . . . . . . .
27
Alexander Amigud, Thanasis Daradoumis, Joan Arnedo-Moreno
and Ana-Elena Guerrero-Roldan
Engagement Analytics: A Microlevel Approach to Measure
and Visualize Student Engagement . . . . . . . . . . . . . . . . . . . . . . . . . . . .
47
Isuru Balasooriya, Enric Mor and M. Elena Rodríguez
Learning Analytics in Mobile Applications Based
on Multimodal Interaction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
67
José Miguel Mota, Iván Ruiz-Rube, Juan Manuel Dodero,
Tatiana Person and Inmaculada Arnedillo-Sánchez
Increasing the Role of Data Analytics in m-Learning
Conversational Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
93
David Griol and Zoraida Callejas
Enhancing Virtual Learning Spaces: The Impact
of the Gaming Analytics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
115
Anastasios Karakostas, Anastasios Maronidis, Dimitrios Ververidis,
Efstathios Nikolaidis, Anastasios Papazoglou Chalikias,
Spiros Nikolopoulos and Ioannis Kompatsiaris
Advice for Action with Automatic Feedback Systems. . . . . . . . . . . . . . .
139
Denise Whitelock
Towards Full Engagement for Open Online Education.
A Practical Experience from MicroMasters at edX . . . . . . . . . . . . . . . .
161
Rocael Hernández Rizzardini and Hector R. Amado-Salvatierra
xv

A Data Mining Approach to Identify the Factors Affecting
the Academic Success of Tertiary Students in Sri Lanka . . . . . . . . . . . .
179
K. T. Sanvitha Kasthuriarachchi, S. R. Liyanage and Chintan M. Bhatt
Evaluating the Acceptance of e-Learning Systems via
Subjective and Objective Data Analysis . . . . . . . . . . . . . . . . . . . . . . . . .
199
Imed Bouchrika, Nouzha Harrati, Zohra Mahfouf
and Noureddine Gasmallah
Glossary. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
221
Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
227
xvi
Contents

List of Acronyms
AAL
Ambient Assisted Living
AMOES
Attrition Model for Open Learning Environment
ASR
Automatic Speech Recognition
CR
Conﬁrmation Rate
DM
Dialog Management
DRS
Discourse Representation Structures
DRT
Discourse Representation Theory
ECR
Error Correction Rate
EHEA
European Higher Education Area
ELLI
Effective Lifelong Learning Inventory
ESM
Experience Sampling Method
FEEF
Full Engagement Educational Framework
HMM
Hidden Markov Models
IQ
Interaction Quality
JSON
JavaScript Object Notation
LA
Learning Analytics
M-learning
Mobile learning
MOOC
Massive Open Online Courses
MSE
Microlevel Student Engagement
MSLQ
Motivated Strategies for Learning Questionnaire
NLP
Natural Language Processing
NSSE
National Survey of Student Engagement
PTAT
People Talk About That
QR
Question Rate
SLU
Spoken Language Understanding
STEM
Science Technology Engineering and Mathematics
TTS
Text-to-Speech Synthesis
UOC
Universitat Oberta de Catalunya
VLE
Virtual Learning Environment
xvii

Predictive Analytics: Another Vision
of the Learning Process
David Bañeres and Montse Serra
Abstract Teachers are always in need of new tools to support the learning process.
Learning analytics has emerged as a solution to provide feedback about the learning
progress of students. This solution does not only provide meaningful information to
instructors to analyze and improve the learning process, but also to managers and
other stakeholders of the learning processes. In this chapter, we extend the vision of
learning analytics to predictive analytics. Currently, we are ready to see further in
the future and predict the behavior of students based on their actions, and this idea
opens a broad potential for educational settings. This chapter discusses challenges,
beneﬁts and weaknesses of a predictive system for education. Additionally, the
design of a generic predictive system is proposed and experimental results in a real
scenario are shown to validate its potential.
Keywords Predictive analytics ⋅Learning analytics ⋅Awareness system
e-learning ⋅Machine learning
1
Introduction
At present, Virtual Learning Environments (VLEs) are producing a massive amount
of data related to the behaviors, actions and accomplishments of the students. Some
data is stored structured in databases, but some others are simply logged in an
unstructured way. On this distributed state, this information is useless. Nobody can
analyze this data manually. Here, it comes big data analytics. All this information
can be processed using Extract-Transform-Load (ETL) processes to create
meaningful reports.
D. Bañeres (✉) ⋅M. Serra
Faculty of Computer Science, Multimedia and Telecommunications,
Universitat Oberta de Catalunya, Barcelona, Spain
e-mail: dbaneres@uoc.edu
M. Serra
e-mail: mserravi@uoc.edu
© Springer International Publishing AG 2018
S. Caballé and J. Conesa (eds.), Software Data Engineering for Network eLearning
Environments, Lecture Notes on Data Engineering and Communications
Technologies 11, https://doi.org/10.1007/978-3-319-68318-8_1
1

Learning analytics (LA) appeared to give support to teachers. The knowledge
gathered by these systems helps to generate reports and visualizations on student’s
performance and actions. Their contribution on acquiring knowledge about how the
course is progressing and giving evidences of potential issues is unquestionable.
LA has a limitation. Extensive reports can be generated, dynamic charts can be
provided, comparisons between different groups of students can be provided but all
this information has to be interpreted, analyzed and, in the case of showing some
issue on the learning progress, the instructor is the responsible for ﬁnding a way to
solve it. In other words, LA provides a snapshot of the current state of the learning
process based on the collected data. If an action is applied to the course, the only
way to check their effectiveness is to generate another snapshot and analyze the
impact.
Here, predictive systems arise. They can be deﬁned as the evolution of the
classic LA systems. Based on all the information gathered by those systems, pre-
dictive systems can see beyond today and forecast future events. Note that, these
systems depend on predictive models and those models highly rely on previous
observations. There is no unique model, there is no unique predictive algorithm,
therefore, these models are error-prone. Although, there are several drawbacks
about the conﬁdence with these systems, the interest to combine LA with predictive
models is growing.
Predictive analytics (PA) systems have appeared to improve the support for
teachers. Predicting the student’s performance, such as, to pass an activity or some
student’s action or behavior, such as, chances to drop-out, can be reported. Teachers
will be able to anticipate actions and increase the probability to avoid negative
events.
This chapter aims at describing Predictive Analytics systems. Background on
LA is taken into consideration since PA systems need to handle, manage and
process a large amount of data. Additionally, machine learning techniques (algo-
rithms) are also necessary since PA systems depend on predictive models which
need to be trained. Using all this background, a PA innovative system is built.
The chapter is organized as follows. First, Sect. 2 outlines related work about
predictive models and systems. Section 3 describes challenges related to predictive
systems and their utilization in educational settings. The implementation details of a
PA system are presented in Sects. 4 and 5 summarizes a validation experiment
based on the constructed system. Finally, conclusions are presented in Sect. 6.
2
Previous Work
Analytics in Higher Education has many meanings and applications, ranging from
the use of data to improve business operations to assist both the learner and the
learning process, generally referred as learning analytics (Hastie et al. 2009). With
learning analytics, institutions will need tools to collect, store, analyze, and visu-
alize the data in meaningful and intuitive dashboards. Understanding the available
2
D. Bañeres and M. Serra

solutions and what value each one provides is the key for institutions to understand
how to create an appropriate solution to meet their unique needs.
There are two major categories of predictive learning analytics solutions:
embedded and platform. There are two types of embedded solutions: (1) LMS
(Learning Management System) that contains embedded analytics tools for use by
existing LMS users (Arnold and Pistilli 2012; Greller and Drachsler 2012; Jo et al.
2014); and (2) SIS (Student Information System) with embedded analytics tools
that have built-in triggers or alerts that execute based on transactions or the lack
thereof (Chatti et al. 2012; Mattingly et al. 2012; Siemens 2012).
Dedicated analytics platform solutions are focused on speciﬁc areas, such as
at-risk student retention (Dietz-Uhler and Hurn 2013). These solutions leverage
data streams extracted from a variety of traditional institutional systems (e.g., LMS,
and student information and administrative systems), which are then provided to
advisors or other stakeholders for use in their work with students.
Learning analytics approaches typically rely on data emanating from a user’s
interactions with information and communication technologies (ICTs), such as
LMS, SIS and social media. For example, the trace data (also known as log data)
recorded by LMS contains time-stamped events about views of speciﬁc resources,
attempts and completion of quizzes, or discussion messages viewed or posted. Data
mining techniques are commonly applied to identify patterns in these trace data
(Baker and Yacef 2009). The interpretation of these patterns can be used to improve
the understanding of learning and teaching processes, predict the achievement of
learning outcomes, information about support interventions and aid decisions on
resource allocation. This process has been described as learning analytics by
Siemens and Gasevic (2012).
Research in learning analytics and its closely related ﬁeld of educational data
mining, has demonstrated a large potential for understanding and optimizing the
learning process (Baker and Siemens 2014). To date, much of this research has
focused on developing predictive models of academic success and retention
(Campbell et al. 2007). Speciﬁcally, the prediction of students at risk of failing a
course (i.e. a binary variable with two categories, fail and pass) and the prediction
of students’ grades (i.e., dependent numeric or classiﬁcation variable that represents
the ﬁnal mark) have been two commonly reported tasks in the learning analytics
and educational data mining literature (Dawson et al. 2014). These two types of
successful predictions have been based on the following sources of data such as:
data stored in institutional student information systems (e.g., high school grades,
socio-economic status, parents’ education, and language skills) proposed by Araque
et al. (2009) and Kovacic (2012); trace data recorded by LMS and other online
learning environments proposed by Agudo-Peregrina et al. (2014) and Romero
et al. (2013); and combinations of previous data sources proposed by Barber and
Sharkey (2012) and Jayaprakash et al. (2014).
In addition, many authors, especially those from educational data mining
backgrounds, have also reported highly accurate predictions using different clas-
siﬁcation algorithms such as C4.5, EM, Naïve Bayes, and SVM (Support Vector
Machines). The development of these sophisticated Machine Learning data mining
Predictive Analytics: Another Vision of the Learning Process
3

algorithms, as well as big data storage and processing capabilities, have allowed to
go beyond traditional reporting about the past and move into an era where we can
predict.
At present machine learning has become such a well-known word and it is
because organizations are collecting more and more data and using these algorithms
in order to manage it. Sophisticated Machine Learning algorithms seek to replicate
human intelligence and consciousness. Applications of Machine Learning encom-
pass a variety of challenging and complex problems ranging from spam ﬁltering
and fraud detection, to marketing personalization and online search recommenda-
tions, to smart cars and healthcare diagnostics. Understanding the algorithms
behind these use cases is the ﬁrst step towards an advancement in Machine
Learning. An interesting review about machine learning can be found in Hastie
et al. (2009).
The feasibility of each algorithm depends on several considerations, including
the accuracy and linearity of classiﬁcation required, training time and some
parameters used to yield appropriate results. Each model may also make speciﬁc
assumptions to accelerate performance or deliver useful results. The tradeoff is
better developed after a detailed and thorough understanding of how each model
works and the Machine Learning requirements of the corresponding datasets.
For instance, students are often direct consumers of learning analytics in their
daily habits based on the previous algorithms (i.e. social nets, data search, video
games, success rate) and, particularly through dashboards that support the devel-
opment of self-regulated learning and insight into one’s learning. Predictive
learning analytics help students at the course level, solutions are also emerging to
assist students at the program level by predicting which students may not complete
their degree on time or which course would be the best for a speciﬁc student to take
next.
Furthermore, most of the reported studies investigating the prediction of aca-
demic success have been based on trace data extracted from a single or small
number of courses within a particular discipline (Macfadyen et al. 2014; Romero
et al. 2013).
At this point, research in predictive analytics focused on the capacity for early
identiﬁcation of students at-risk of academic failure allows a proactive approach to
implementing learning interventions and strategies that target teaching quality and
student retention (Siemens and Long 2011). Despite a big progress in this type of
research, a signiﬁcant challenge remains and many examples of it are patent of the
awaken interest.
Let us emphasize some functional systems. In D2L Degree Compass (n.d.),
“Degree Compass” determines which courses are needed for the student to graduate
and ranks them according to how they ﬁt with the sequence of courses in the
student’s degree program and their centrality to the university curriculum as a
whole. That ranking is then overlaid with a collaborative ﬁltering model that pre-
dicts the courses in which the student is most likely to achieve the best grades.
Dashboards such as “Brightspace LeaP by D2L” (Steven and Stephanie 2014)
works with course learning objectives, content, and questions and provides a text
4
D. Bañeres and M. Serra

representation for each component. Then, it uses semantic algorithms to ﬁnd
relationships among these components to make intelligent recommendations for
what should be presented to a learner to meet a particular learning objective, what
questions should be used to determine if a learner has met the objective, and what
content items the learner should read if a particular question is answered incorrectly.
Another example is “Blackboard Analytics” for Learn, which combines data
from the “Blackboard Learn LMS” with student demographic information and
course attributes to create reports and dashboards for faculty, in order to provide a
broad range of insights into course materials, student engagement, and student
performance.
Other applications let instructors see where students stack up against each other
in a course using speciﬁc metrics (i.e., course access, content access, and social
learning) combined with what those numbers typically mean for academic perfor-
mance. For example, the “Brightspace Student Success System” (Brightspace
student success system n.d.) developed by D2L uses regression models to predict
student grades starting from the ﬁrst week. Instructors can monitor the status of
individual students regarding their predicted success.
As the last example application based on graphical views, “Student Explorer”
(Student Explorer n.d) identiﬁes the students who are at the highest risk of failure.
In that case, a list of students is provided to teachers in order to prioritize the
engagement with them.
All previous systems describe examples of the application of predictions to the
learning context. In this chapter, we will focus on the design of a predictive system
to give support to a predictive analytics system.
3
Predictive System
This section discusses the design of a predictive analytics system and how this
system can be used in educational settings. However, ﬁrst, a brief introduction to
predictive models is performed aiming to comprehend the full potential of the
system better.
3.1
Predictive Models and Machine Learning
A Predictive model is a process that allows predicting a future outcome or behavior
based on past events. The model uses statistical analysis to generate a probability of
a certain result. A model is composed of certain indicators as inputs and an
objective outcome as output. By applying different methodologies, the model is
capable of predicting the value of the output based on all the data collected for the
indicators.
Predictive Analytics: Another Vision of the Learning Process
5

Although predictive models are remarkably old (i.e. ﬁrst attempt is dated on the
ﬁfties with the ﬁrst computer game checker by Arthur Samuel), they become
popular on the nineties when computer science took full potential of statistics.
Machine learning algorithms started to appear as a solution to support predictive
models. Data were digitally available and predictive models could be simulated for
accuracy and applied more efﬁciently.
However, it is difﬁcult to generate good models since many factors may affect
their accuracy. Signiﬁcant experimentation is needed to ﬁnd the best indicators
based on the desired output or outcome. Figure 1 outlines the components of a
predictive model. First, the model is characterized by the set of indicators (denoted
as features in machine learning) that help to build and specify the model. Few
indicators will generate a poor model with a high failure rate. A large number of
indicators may generate an overﬁtted model or some indicators may be comple-
mentary or redundant. Also, the model should be trained with data of past events.
The window training of past events is also crucial for the accuracy. The window
should be the largest one since more data improves the quality. Finally, the mon-
itoring checkpoints should be deﬁned. The checkpoints should be placed when
relevant new data values for indicators will be available to produce a prediction.
Note that, a small interval between checkpoints may produce identical prediction
since input data is similar (Fig. 1).
However, notorious improvements have been done in predictive models related
to educational contexts. Since instructors are always concerned about the pro-
gression of their students, extensive research has been performed over the past
decades trying to predict student’s success or ﬁnding the cause-effect relationship
among indicators related to the teaching process. In addition, these models also
helped to identify the most inﬂuent indicators. Prior-learning, demographic
Fig. 1 Predictive model components
6
D. Bañeres and M. Serra

information, student’s actions, acquired competences among others are some
examples of indicators used to create predictive models.
Machine learning is a reliable solution for predictive models. Based on the
extensive research performed during the last decades, good algorithms have been
designed. Note that, there is an extensive literature on machine learning (Hastie
et al. 2009). Here, we brieﬂy outline the type of algorithms that could be taken into
consideration in educational contexts:
• Supervised Learning: Suitable for datasets where there is data available for all
indicators and, additionally, for the outcome.
• Unsupervised Learning: Suitable for dataset where there is no mapping between
indicators and the outcome.
Note that, mixed methods also exist but we consider out of the scope of the
chapter.
3.2
Predictive Models in Educational Contexts
Predictive models could be used to forecast multiple outcomes in educational
contexts. Here, we detail some predictive models that can be applied based on the
related work on Sect. 2:
• Prediction on the student’s success (Campbell et al. 2007): Will the student pass/
fail the course?
• Prediction on competence/skill acquisition (Lee and Brunskill 2012): Has the
competence/skill been acquired?
• The impact of learning resources such as a textbook or a learning tool on the
outcome (Kot and Jones 2014): Does their utilization impact on the outcome?
i.e. the prediction changes when used/not used.
• Prediction on drop-out within a course (Park and Choi 2009): Is possible that the
student is going to drop out?
• Prediction on retention in the program or educational organization (Bohannon
2007): This prediction is different from previous. Here, more indicators are
needed related to the behavior of the learner in the university and effort invested
during the study.
• Prediction on enrollments: (Bohannon 2007). Lifelong learning is a reality and
institutions are interested in keeping graduate students to continue studying and
also get freshmen students. Based on students’ demographic information and
‘academic records, predictions on new enrollments can be made.
• Prediction on student’s satisfaction (Kuo et al. 2013): Is the student satisﬁed
with the learning process?
Predictive Analytics: Another Vision of the Learning Process
7

These are some examples of simple predictions that can be tackled with su-
pervised learning methods. We have data from past students where the value of the
outcome is known for each set of values for the indicators. Thus, a predictive model
can be trained and predictions can be produced. Two types of problems are iden-
tiﬁed: classiﬁcation problem when a limit set of options are possible for the out-
come; and regression models when the outcome is a numerical value, such as, the
grade of the course.
Complex classiﬁcations can also be done:
• Cluster students based on similar proﬁles (Antonenko et al. 2012).
• Cluster learning resources by the impact on the acquisition of knowledge
(Kersting et al. 2012).
Although, generate groups or clusters is not intended for creating predictive
models, it can be used to obtain richer models, i.e. identifying different student’s
proﬁles and correlating with other indicators such as student’s success. Also, it can
be used to inform instructors and, even, the students which proﬁle they belong and
the characteristics of people being in that proﬁle (i.e. chances to pass a course,
behaviors, …). This type of model is done by an unsupervised learning method
since the meaning of each cluster is not explicitly detailed in the datasets (Fig. 2).
Fig. 2 Design of the predictive system
8
D. Bañeres and M. Serra

3.3
Design of a Predictive Analytics System
This section describes the design of a prediction system. The system architecture is
illustrated in Fig. 2. The core system is composed of a machine-learning engine.
Different data sources are accepted as incoming data in order to train the models.
Additionally, the generated models can be used to provide multiple services based
on the knowledge acquired from the classiﬁcation of the learners.
The system can handle multiple predictive models depending on the actual
outcome. However, models will tend to be different for each context since available
indicators may also differ. In the case of an identical set of indicators, the training is
recommended to be performed individually for each course or context. This con-
dition will help to enhance the quality of each model since it will be trained based
on the characteristics and needs of each context (datasets) and avoiding to create
general models that are more error-prone.
Aforementioned, the main challenge together with the selection of the best
algorithm is the selection of indicators to feed the model. A trade-off between
simplicity and quality should be found. One important property of the system is to
avoid static models. They should be dynamically improved over time by increasing
the size of the window training. More data will generate in few iterations ﬁne-tuned
models particularly applicable for each prediction outcome.
Multiple data sources should be considered. All sources where valid information
related to the learner is available are accepted. Note that, it is crucial that the sources
contain valid and accurate data. Non-trustable sources regarding reliability, main-
tainability and integrity should be avoided. Here, we have identiﬁed a set of
potential data sources. Note that, this is an incomplete list:
• Student’s grades or transcripts: This data source provides the performance of
students regarding grades. Transcripts are also accepted. A grade for each
assessment activity (i.e. continuous, projects, collaborative, exams, …) can be
retrieved. Quantitative (numerical) and qualitative (i.e. letter from A to F) grades
are both acceptable depending on the type to grading used in the institution.
• Student’s prior learning: In the literature review (Sect. 2), we have observed that
prior learning indicators have a signiﬁcant impact on predictions mostly in
science programs. However, basic knowledge such as mathematics can be
broadly used in many specializations. Here, previous knowledge regarding
passed courses or acquired skills can be used to improve the quality of the
models.
• Student’s proﬁle: It is also interesting to keep updated the learner’s proﬁle. All
information related to activities out of the institution, family status and pro-
fessional commitments has a critical impact on predictions.
• VLE information: During the instructional process, the students interact with the
VLE and some actions can be logged, for instance, accesses to learning mate-
rials or interaction in the discussion forum. In on face-to-face learning, some
learner actions can also be logged manually such as, the participation level
during the class, whether she submits some non-assessment activities or she
Predictive Analytics: Another Vision of the Learning Process
9

attends all laboratory/practice sessions. Online learning with a VLE is preferable
since actions can be logged automatically.
• Learning analytics system: Some courses have tutoring systems or activities
using some third-party platforms. These systems tend to store all attempts,
accomplishments, badges, and actions performed by the learner and provide an
LA interface to report the results. We consider that all additional information
related to the progression of the learner can be highly valuable.
• Identity/authorship issues: This ﬁnal category refers to all issues detected on the
learner based on successful of failed identity or authorship checking. We con-
sider that this trustworthy information can also be used to create better models.
Note that, these indicators have a high impact on models. For instance, a pla-
giarism detection can be penalized by the academic regulation with a fail score
on a course. Thus, this indicator in case of plagiarism detection will have an
accuracy of 100% on the model.
As an analytical system, the output of the predictive models may have different
applications. Based on learning analytics systems literature, we have identiﬁed the
following solutions for learners and teachers:
• Prediction reports: This service can be for learners and instructors. The objective
is to predict different outcomes as we have described in Sect. 3.2. Note that,
several predictions may be available and they may correspond to different
contexts such as at course, program or institution level.
• Recommender system: This service can also be used for learners and teachers.
For the point of view of the learner, a recommender system should help to
improve a certain outcome (i.e. pass a course, improve knowledge, no drop-out,
…) by guiding the student through the best options to obtain a positive pre-
diction. This recommender system can be integrated into intelligent tutoring
systems to create adaptive learning paths to improve self-regulation; or into
administrative systems in the organization to recommend best choices during
enrollment. For the point of view of the instructor, the system could be used to
improve personalized feedback. Learners highly appreciate proactive feedback
coming from instructors.
• Predictive analytics: This service will only be used by instructors and academic
coordinators. Statistical analysis on predictions, automatic issues detection, and
correlation among indicators and outcomes can be signiﬁcantly useful to
improve the course design and to share good practices.
4
Generic Predictive Analytics System
After describing the design of the system, this section outlines the technical
description of the predictive analytics system. Figure 3 illustrates the complete
system. Note that, we focus on the different modules we need to be able to generate
10
D. Bañeres and M. Serra

predictions. Dashboards and reports generated can be similar to learning analytics
systems (an interesting description can be found on Maldonado et al. (2012), Park
and Jo (2015), Schwendimann et al. (2016) and Widjaia and Santoso (2014). The
modules are summarized next:
• Module to gather evidences: The system systematically collects evidences for
the indicators selected for each model. This module works automatically.
• Module to create a model: This module aims to create models based on the
available indicators and selects the outcome to be predicted. Since, the system is
not aware of the best accurate models, we consider that this module works in
supervised (manual) mode.
• Module to update the datasets for training: As we mentioned, the models must
be dynamically improved by adding new datasets. This updated information can
be obtained from the continuously collected data from the ﬁrst module. This
module also works automatically.
• Module to generate a prediction: Finally, the models are used to generate the
predictions based on some static or rule-based checkpoints.
In the next subsections, each module is described in detail (Fig. 3).
Fig. 3 Predictive analytics system
Predictive Analytics: Another Vision of the Learning Process
11

4.1
Module to Gather Evidences
One of the critical modules of the system is the one responsible for gathering all the
evidences. The main challenge is the interoperability with other services since there
is no standard format to describe each evidence. VLE log systems use one data
model, student’s transcripts or prior learning description are stored in another data
warehouse. The same problem arises with other data sources. There are standard
models such as IEEE PAPI (IEEE 2001 P1484.2.1/D8 ), IMS LIP (IMS LIP 2001),
or xAPI (ADL-Co-Laboratories 2017). Some VLEs partially support these stan-
dards, however most of them with adaptations or using other layer interfaces on top,
i.e. CMI-5 for xAPI.
Another challenge is how this data is retrieved. Some services allow installing
modules to send data periodically to other services. Others have some API that can
be instantiated to gather information. In case that no public documentation is
available, discovery tools can be used (Cánovas and Cabot 2016) to know which
services are exposed to obtain information. The module must be adapted to support
all these characteristics.
Different technologies can be used. A discussion about the best suitable tech-
nologies can be found in Gañán et al. (2016). We propose a data processing ﬂow with
two entry points: a service to receive evidences and a service to request evidences.
• Automated incoming evidences: This entry point is used for services capable of
implementing an interface or installing daemon-like processes to send events.
They need to meet the speciﬁcation to send data (i.e. methods and suitable
arguments).
• Upon request evidences: Some services are old and immutable but some
interface is offered to query some information. In this case, it is the predictive
system that periodically triggers a request to obtain the data. The list of available
sources should be registered in the system describing the location, the interface,
the time window between requests (i.e. at the beginning of the semester, every
week, …).
The data is formatted to a common data model deﬁned as the tuple:
< student, context, evidence, date, value >
where an evidence may correspond to multiple ﬁelds such as the action performed
or multiple ﬁelds to describe the evidence.
To better understand the model, two examples are provided:
• CLASSROOM_USER_ACTIVITY_ASSESSED: An evidence to describe the
grade on an activity of a course.
< Student; Semester; Course; Classroom; Activity; Grade >
12
D. Bañeres and M. Serra

• CLASSROOM_USER_LOG_IN: An evidence to describe the accesses of the
learner to the classroom of the course:
< Semester; Course; Classroom; Time >
Next, the data is validated, clean-up and ﬁxed. Note that, some ﬁelds may not be
provided (i.e. date may not be relevant when retrieving student’s grades or
prior-learning information is not associated with any context such as a course) or
some errors may appear on the data transmission. The operation contributes to
obtain valid tuples before storing them in the system.
Finally, there is an evidence processor responsible for gathering these tuples and
transform them to indicators with the following data model:
< student, indicator, date, value >
where, similar to the evidences, indicator may correspond to multiple ﬁelds.
Note that, there are two types of evidences:
1. Evidences that do not need further processing such as grades or prior-learning
information. These evidences are stored as they are as indicators. For instance,
the evidence CLASSROOM_USER_ACTIVITY_ASSESSED of the previous
example is stored as it is.
2. Evidences that require further processing. Data such as log events should be
aggregated to be useful for the prediction model. For instance, if the event
related to access to the virtual classroom CLASSROOM_USER_LOG_IN is
taken into account, the VLE log system will send this event every time the
student accesses to the classroom. Then, the evidence processor will be
responsible for aggregating this information and for updating the indicator
which counts the number of accesses. The next tuple can be created with the
aggregated information:
< Semester; Course; Classroom; number of accesses >
The relation between evidences and indicators is stored on each model as we
describe in the next section. Note that, evidences not associated with any indicator
are discarded to save time and space (Fig. 4).
4.2
Module to Create Predictive Models
This module is the responsible to support the creation of new models. Aforesaid,
this module is completely manual. It is possible to create semi-automated or fully
automated new models based on previously created high-quality models. However,
Predictive Analytics: Another Vision of the Learning Process
13

at the beginning there is no model or knowledge of the best models. Thus, the
models must be created and tested within the system to check their accuracy.
Internally, a model is deﬁned as a set of indicators and an outcome to be
predicted. Here, the question is how the available indicators are identiﬁed and
selected. There are two options to capture this information:
1. Indicators used on other models: This is the simplest one. On a running system
where previous semesters of data have been collected, the list of the available
indicators can be easily known by performing a query to the Datasets database.
2. Evidences available but not currently stored in the system: As we described in
the previous section, the evidence processor discards evidences not needed by
any model. It is a design decision to save space and avoid further processing on
non-relevant evidences. However, the models of the available evidences are
maintained in the Datasets database. This list can be retrieved to know the
evidences that can be collected.
Although the deﬁnition of the model can be speciﬁed with a formatted ﬁle, such
as JSON ﬁle, we need an easy manner to create the ﬁle. Note that, more sophis-
ticated approaches can be used such as using a Domain Speciﬁc Language
(DSL) (Balderas et al. 2015). Here, we describe the design of a GUI interface that
may help to graphically create the model (See Fig. 4). The indicators are selected
based on: (1) indicators currently used by other models and (2) new indicators
created from available evidences. The Graphical User Interface (GUI) assists the
selection for each indicator of the associated evidence from the available ones.
Also, an indicator can be an aggregated value. Here, a list of functions to aggregate
the values (none option is also available) such as sum or average is available. This
information is used on the gather evidences module by the evidence processor.
After this selection, there is the option to activate the indicator for the model. This
option stands for the problem related to indicators that data is not available for
training the ﬁrst semester of the model. Then, deactivating this option creates a
model without the indicator but meanwhile the information is stored. After a period
Fig. 4 GUI for creating a predictive model
14
D. Bañeres and M. Serra

(the window training we consider for the model), the indicator can be activated to
have models with better quality.
Finally, the indicator to be predicted is selected. We assume a single prediction
model. Other variants could be added in the future, such as, multivariate prediction
models (Pascarella and Terenzini 1980). Additionally, the checkpoints to produce
the prediction and update the model are also deﬁned. Three types of checkpoints are
considered:
• Static checkpoints based on speciﬁc dates: Some predictive models are relevant
only on speciﬁc dates such as retention at the end of the course. Thus, the model
is always triggered at the same period within the course.
• Dynamic ones based on reception on a speciﬁc evidence: The accuracy of the
model high depends on speciﬁc evidences, i.e. student’s success highly depends
on grades of previous activities. Thus, when a speciﬁc evidence is received, the
model is triggered.
• Manual trigger by a user of the system: The user is the responsible to decide
when to run the predictive model. For instance, the instructor may be interested
in knowing possible drop-out on a course at any time.
Note that, all the conﬁguration variables proposed in this section may help to
generate highly customizable prediction models.
4.3
Module to Update Datasets
This module aims to prepare all the data to be used by the models. Recall that, the
information collected by the gather evidences module creates an entry for each
student and indicator. This data model cannot be used straightforwardly by the
predictive models since they use another format.
Basically, this module executes an ETL process to transform the individual
indicators to datasets identiﬁed for each student:
< student, date, value ind 1, value ind 2, . . . , value ind N >
where each tuple is a snapshot of all data collected for a student on a speciﬁc date.
This tuple is built efﬁciently based on the scheduled dates or events when the
predictive models will be used for prediction or when predictive models need to be
updated for training.
Finally, the models can be easily updated from these datasets. A collection of
datasets for all students for a speciﬁc model can be easily retrieved on the scheduled
dates when the model should be updated. For instance, models related to courses
are relevant to be updated at the beginning of each semester. Therefore, datasets
from previous students of the course will be collected, anonymized (i.e. identity is
not relevant for training) and used for training models associated with the course.
Predictive Analytics: Another Vision of the Learning Process
15

4.4
Module to Generate Predictions
The last module performs the predictions using all the datasets generated by the
previous module. There is a prediction generator, which has access to all models of
the system and checks for checkpoints (static dates, trigger events or manual
activation). When a checkpoint is detected, the prediction model associated is
activated. All information for each student is stored and ready when the prediction
must be performed. Thus, this module selects the values for each indicator of the
model and the prediction is performed. Lastly, the prediction is stored and ready to
be used for any of the services described in Sect. 3.3.
Note that, during the description of the system, we did not describe how models
are associated with a speciﬁc context (i.e. course or program) or student. We
consider this conﬁguration layer out of the scope of the chapter.
5
Experimental Results
In this section, a validation experiment has been done using the predictive system
described in the previous section. The experiment has been performed in two
courses in the Open University of Catalonia. The objective is to provide real
prediction results and to evaluate the accuracy of different prediction models.
First, we describe the conﬁguration of the system. Next, the results are shown in
an evaluation of the quality of the models.
5.1
Conﬁguration of the Validation Experiment
A prototype of the predictive system has been implemented with the four modules
described in Sect. 4. Currently, there is no graphical interface in the predictive
system and the conﬁguration is performed manually based on scripting ﬁles. The
output of the predictive models is exported to a CSV ﬁle.
The Open University of Catalonia is a fully online university where all the
learning process is performed through a custom VLE. Each course has an online
classroom where all the learning resources and assessment activities are placed and
the learners can interact using a discussion forum. The assessment activities are
submitted within the classroom using the CAR (Continuous Assessment Registry)
and the grade and personalized feedback is provided using the same tool.
The VLE provides different graphical reports related to the student’s perfor-
mance and teaching process but also limited information is available based on
RESTful web services. List of students, activities, grades for activities can be
retrieved. Currently, the log information related to student’s actions such as inter-
action in the discussion forum or with the learning resources is not available and
16
D. Bañeres and M. Serra

only the technical team of the university (i.e. developers, VLE administrators) has
access to it.
Based on this limitation, the experiment has been performed only using the
information that can be accessed using the upon request evidence collector. List of
students, list of assessment activities and the activity grades REST API within CAR
are queried. The trigger action for prediction has been set on the date when the
grades are published to learners and the trigger action for training has been set at the
beginning of the semester with a minimum window training of two semesters. For
this experiment, we have forced the upon request evidence collector to get data
from previous two semesters for each course at once.
Several models have been created for each course. Precisely, one model for each
submitted assessment activity (i.e. one model with only activity 1, one model with
activity 1 and 2 and so on) where the prediction indicator is to pass the course
(binary variable, pass or fail). Additionally, two variants of each model have been
created denoted as pessimistic and optimistic model. The pessimistic model only
considers the grade of the activities that the learner has submitted. This model of the
ﬁrst activities of the course has few evidences and the model tends to have a low
accuracy. Additionally, the model tends to report that the learner needs a good
grade to pass the course. This model is realistic and it can be valuable for the
teacher. However, this model is not recommended for learner since it may con-
tribute to discouraging to continue when initial low grades are obtained. For this
reason, we proposed an optimistic model where a grade of 5 out of 10 is assigned
for activities not yet submitted. This model is ideal for learners since it motivates to
continue even if bad grades are obtained on ﬁrst activities. Thus, all models have
been created in the system and they have been assigned to each course.
Finally, the predictor generator has been created based on the cross-validation
experiment performed in Baneres (2016). The machine-learning algorithm J48 has
been used using the WEKA system (Hall et al. 2009). This algorithm has been used
since it provided the best performance in cross-validation for grades related to the
assessment activities. WEKA is a collection of machine learning algorithms for data
mining tasks and there is also an API to use the algorithms from JAVA code. The
predictor generator outputs a CSV ﬁle with the results of the prediction for each
model.
To check the accuracy of the system, the prediction results are correlated with
the real performance (i.e. pass or fail the course) of the learners on the current
semester.
5.2
Analysis of the Simulation Results in Computer
Fundamentals
During Computer Fundamentals (CF), a student has to acquire the skills of analysis
and synthesis of small digital circuits and to understand the basic computer
Predictive Analytics: Another Vision of the Learning Process
17

architecture in 150 h. The synthesis process is presented at the logical level without
describing all the inherent electronic problems.
The assessment is divided into three continuous assessment activities (CAA) and
one ﬁnal project. The activities evaluate the contents of the second, third and fourth
unit and the ﬁnal project is related to the design and synthesis of a state machine
explained in the last unit. There is also a ﬁnal exam with problems related to the
second, third, and fourth units (30% each one) and a question of the ﬁfth unit (10%).
The ﬁnal mark (FM) of the course as we can observe on Eq. 1 is obtained by
combining the results of the continuous assessment activities (CAAs), the ﬁnal
project (FP) and a ﬁnal exam (FE). The project and the exam are mandatory while
the CAAs are optional, but they tend to improve the ﬁnal mark.
FM = MAX 35% FP + 35% FE + 30% CAAs, 50% FP + 50% FE
ð
Þ
ð1Þ
Computer Fundamentals is a ﬁrst-year course on the Bachelors of Computer
Science and Telecommunication Technologies and, consequently, there is a large
number of students each semester (larger than 300 students). A recurrent problem in
this course is the high drop-out rate that reaches values nearly to 50% of enrolled
students in some semesters.
Table 1 shows the simulation results for this course. We show the total pre-
dictions performed, the correct ones, the accuracy and RMSE (Root Mean Square
Error) for each model taking as training data the information from 2015 Fall and
2016 Spring semester and taking as test set the 2016 Fall semester. For each model,
the accuracy and RMSE are calculated globally (General) for the model and dis-
tinctively when the model predicts pass or fail. We are interested in observing the
model for each prediction.
For the pessimistic model, the accuracy increases on each activity. More data
helps to predict better whether a student will pass/fail the course. The accuracy
improves from 63% to 95%. Note that, better accuracy also decreases the RMSE of
the models. However, an interesting result is observed when the accuracy is ana-
lyzed for the pass/fail prediction. The fail prediction in all activities is highly
accurate (larger than 90%). This is consistent with the characteristics of the course.
The large drop-out is observed in this prediction. When one student fails on initial
activities or she does not submit the activity tends to fail or drop out the course. The
pass prediction is less accurate, mainly for the ﬁrst activity. This result shows that
the training data set should be increased regarding students that pass the course.
If the optimistic model is analyzed, we can observe that is inaccurate. For CAAs,
the results are too optimistic providing always a pass prediction. Note that,
assuming a grade of 5 out of 10 points for not yet submitted activities produces an
error-prone model. The main problem is in the Final Project mandatory activity.
Assuming that this activity is passed, the model always generates a positive pre-
diction. This model is highly dependable on activities that have a signiﬁcant impact
on the ﬁnal grade of the student. It can be a good model for learners to improve the
morale and engagement when activities are failed. However, it can produce
18
D. Bañeres and M. Serra

Table 1 Accuracy of the models in computer fundamentals
Assessment
activity
Prediction
Pessimistic
Optimistic
Total
predictions
Correct
predictions
Accuracy
RMSE
Total
predictions
Correct
predictions
Accuracy
RMSE
CAA1
Pass
335
156
0,46
0,73
516
164
0,32
0,82
Fail
181
173
0,95
0,21
0
0
0
0
General
516
329
0,63
0,60
516
164
0,32
0,82
CAA2
Pass
165
129
0,78
0,47
516
164
0,32
0,82
Fail
351
316
0,90
0,31
0
0
0
0
General
516
445
0,86
0,37
516
164
0,32
0,82
CAA3
Pass
191
153
0,80
0,44
516
164
0,32
0,82
Fail
325
314
0,96
0,18
0
0
0
0
General
516
467
0,90
0,31
516
164
0,32
0,82
FP
Pass
185
158
0,85
0,38
185
158
0,85
0,38
Fail
331
325
0,98
0,13
331
325
0,98
0,13
General
516
483
0,93
0,25
516
483
0,93
0,25
Predictive Analytics: Another Vision of the Learning Process
19

overconﬁdent learners or it can decrease the trust with the model by always pre-
dicting a pass prediction even when activities are failed.
5.3
Analysis of the Simulation Results in Computer
Structure
Computer Structure extends the concepts that have been seen in the previous course
of Computer Fundamentals. This subject aims to extend the vision of the basic
structure of a computer and to describe the low-level language (assembler). The
computer programming is performed in C language doing calls to assembler
functions. The student learns to develop functions in assembler and to add calls to
these functions in higher-level languages using their structures in C language. The
learning process requires an investment in time of 10 h for week. The average total
investment for a student is 150 h. Note that, this calculation is statistical.
The assessment is divided into two continuous assessment activities (CAAs) and
one ﬁnal compulsory project (FP). The continuous assessment activities are pro-
posed to check the progressive development of the contents of the course. The ﬁnal
compulsory project is used for the synthesis of all the concepts acquired during the
course. The ﬁnal project is divided into two parts (70% FP1, 30% FP2). The ﬁrst
one FP1 is compulsory to pass. However, there is an optional second submission in
case of failing the ﬁrst one. The second part FP2 is optional to reach the maximum
score. There is also a ﬁnal exam (FE) to evaluate these acquired concepts. The ﬁnal
mark (FM) of the course is computed using the same formula than in Computer
Fundamentals (See Eq. 1).
Computer Structure has a strong relationship with the previous course of
Computer Fundamentals of the Bachelor of Computer Engineering. This course
expands the knowledge of the hardware components that a programmer needs to
know to perform his tasks increasing the complexity of the contents successfully.
Thus, the high drop-out rate that reaches values nearly 45% of enrolled students
(over 250 students) on each semester is patent such as in the previous subject,
Computer Fundamentals.
Table 2 summarizes the results with different behavior for the models compared
to CF. For the pessimistic model, the accuracy of the global model starts with a
value higher (from 63% to 70%) but the improvement is similar reaching values
superior to 90% in the last two activities. It is worth noting that the FP1 (2nd
chance) is only for students who failed FP1 and FP2 is optionally for all students.
Even in courses with activities that are not mandatory for all learners (i.e. some
students will not have a grade for the activity), the model prediction is highly
accurate. Note that, having the optional activity at the end of the course also impacts
positively on the accuracy since a complete dataset (i.e. model with several indi-
cators) is available. If the pass/fail predictions are analyzed, a different behavior is
also observed compared to FC. Here, the drop-out is not as relevant as for CF.
20
D. Bañeres and M. Serra

Table 2 Accuracy of the models in computer structure
Assessment
Activity
Prediction
Pessimistic
Optimistic
Total
predictions
Correct
predictions
Accuracy
RMSE
Total
predictions
Correct
predictions
Accuracy
RMSE
CAA1
Pass
99
74
0,74
0,50
239
116
0,48
0,71
Fail
140
98
0,70
0,54
0
0
0
0
General
239
172
0,72
0,52
239
116
0,48
0,71
FP1
Pass
88
76
0,86
0,37
239
116
0,48
0,71
Fail
151
111
0,73
0,51
0
0
0
0
General
239
187
0,78
0,46
239
116
0,48
0,71
CAA2
Pass
123
103
0,84
0,40
237
114
0,48
0,72
Fail
116
103
0,89
0,33
2
2
0
1,00
General
239
206
0,86
0,37
239
116
0,48
0,71
FP1 (2nd
chance)
Pass
118
110
0,93
0,26
122
113
0,93
0,27
Fail
121
115
0,95
0,22
117
114
0,97
0,16
General
239
225
0,94
0,24
239
227
0,95
0,22
FP2
Pass
120
112
0,93
0,25
120
112
0,93
0,25
Fail
119
115
0,97
0,18
119
115
0,97
0,18
General
239
227
0,95
0,22
239
227
0,95
0,22
Predictive Analytics: Another Vision of the Learning Process
21

Then, the accuracy of the fail prediction is smaller and comparable to the pass
prediction.
When the optimistic model is analyzed similar results to CF are also observed.
For CAA1, FP1 and CAA2, the model is inaccurate. The FP1 (2nd chance) is the
activity when the model starts to produce accurate predictions. Similar to CF, this
Final Project activity is decisive to pass or fail the course since it is mandatory to
pass to have chances to pass the course.
Here, it is interesting to observe that the fail prediction fails at all in CAA2. We
revised the training datasets to analyze the reason. We observed that the training set
did not have any data to predict this case. Then, the model produced an error.
Similar to CF a larger window training is needed to get better predictions.
6
Conclusions
In this chapter, the basis of a predictive analytics system has been presented. This
system has a large potential for learners and teachers. Learners can beneﬁt from this
system by knowing in advance some uncertain outcome. Chances to pass a course
or to drop out or even recommenders about enrollment, resources or learning
activities which have helped other students to improve their knowledge are some
examples. Teachers are also potential users of the system. Learning analytics sys-
tem can be enhanced with predictions. Instructors will be aware of the progression
of the courses and actions could be done in advance to avoid negative performance
rates. Early-personalized feedback or ad hoc learning path for students with
learning problems are some examples of actions that can be applied to analyze
predictions.
Technologically, a predictive system is not difﬁcult to design as we have shown.
With standard interoperability technology and combining with machine learning, a
simple system can be easily implemented. The complexity comes when predictive
models are designed. Available evidences should be analyzed; large datasets are
needed to train the models and a large experimentation is required to create accurate
predictive models. Also, the GUI for the predictive analytics system is a critical
component. The requirements for the institution, the instructors and the learners
should be taking into consideration when graphical reports or dashboards are
designed. User-centric design is recommended to generate the best-suited reports.
Acknowledgements This work was partially funded by the Spanish Government through the
projects: TIN2013-45303-P “ICT-FLAG: Enhancing ICT education through Formative assess-
ment, Learning Analytics and Gamiﬁcation” and TIN2016-75944-R “ODA: Open Data for all”,
and by Open University of Catalonia as an educational innovation project of the 2017 APLICA
program.
22
D. Bañeres and M. Serra

References
ADL-Co-Laboratories. (2017). Experience API Version 1.0.3. Retrieved July 14, 2017, from
https://www.adlnet.gov/newest-version-of-xapi-version-1-0-3.
Arnold, K. E., & Pistilli, M. D. (2012). Course signals at Purdue: Using learning analytics to
increase student success. In Proceedings of the 2nd international conference on learning
analytics and knowledge (pp. 267–270). ACM.
Agudo-Peregrina, Á. F., Iglesias-Pradas, S., Conde-González, M. Á., & Hernández-García, Á.
(2014). Can we predict success from log data in VLEs? Classiﬁcation of interactions for
learning analytics and their relation with performance in VLE-supported F2F and online
learning. Computers in Human Behavior, 31, 542–550.
Antonenko, P. D., Toy, S., & Niederhauser, D. S. (2012). Using cluster analysis for data mining in
educational technology research. Educational Technology Research and Development, 60(3),
383–398.
Araque, F., Roldán, C., & Salguero, A. (2009). Factors inﬂuencing university drop out rates.
Computers & Education, 53(3), 563–574.
Baker, R., & Siemens, G. (2014). Educational data mining and learning analytics. In R. K. Sawyer
(Ed.), Cambridge handbook of the learning sciences. Cambridge, UK: Cambridge University
Press.
Baker, R., & Yacef, K. (2009). Educational data mining and learning analytics. In R. K. Sawyer
(Ed.), Cambridge handbook of the learning sciences. Cambridge, UK: Cambridge University
Press.
Balderas, A., Dodero, J. M., Palomo-Duarte, M., & Ruiz-Rube, I. (2015). A domain speciﬁc
language for online learning competence assessments. International Journal of Engineering
Education, 31(3), 851–862.
Baneres, D. (2016). Towards a particular prediction system to evaluate student’s success. In
Proceedings of the 11th International Conference on P2P, Parallel, Grid, Cloud and Internet
Computing, (pp. 935–945).
Barber, R., & Sharkey, M. (2012). Course correction: Using analytics to predict course success. In
Proceedings of the 2nd International Conference on Learning Analytics and Knowledge
(pp. 259–262). New York, NY, USA: ACM.
Bohannon, T. (2007). Predictive modelling in higher education. In SAS Global Forum.
Brightspace student success system. (n.d.). Retrieved from June 15, 2017 from https://www.d2l.
com/products/student-success-system/
Campbell, J., De Blois, P. B., & Oblinger, D. G. (2007). Academic analytics: A new tool for a new
era. EDUCAUSE Review, 42(4), 42–57.
Cánovas, J. L, & Cabot, J. (2016). JSONDiscoverer: Visualizing the schema lurking behind JSON
documents. Knowledge.-Based Systems. 103: 52-55.
Chatti, M. A., Dyckhoff, A. L., Schroeder, U., & Thüs, H. (2012). A reference model for learning
analytics. International Journal of Technology Enhanced Learning, 4(5–6), 318–331.
D2L Degree Compass. (n.d.). Retrieved June 15, 2017 from https://www.d2l.com.
Dietz-Uhler, B., & Hurn, J. E. (2013). Using learning analytics to predict (and improve) student
success: A faculty perspective. Journal of Interactive Online Learning, 12(1), 17–26.
Dawson, S., Gašević, D., Siemens, G., & Joksimovic, S. (2014). Current state and future trends: A
citation network analysis of the learning analytics ﬁeld. In Proceedings of the Fourth
International Conference on Learning Analytics and Knowledge. (pp. 231–240) New York,
NY, USA: ACM.
Gañán, D., Caballé, S., Clarisó, R., & Conesa, J. (2016). Towards the effective software
development of an elearning platform featuring learning analytics and Gamiﬁcation. In The
Fourth International Workshop on Collaborative Enterprise Systems (COLLABES 2016),
(pp. 177–182).
Greller, W., & Drachsler, H. (2012). Translating learning into numbers: A generic framework for
learning analytics. Educational Technology & Society, 15(3), 42–57.
Predictive Analytics: Another Vision of the Learning Process
23

Hall, M., Frank, E., Holmes, G., Pfahringer, B., Reutemann, P., & Witten, I. H. (2009).
The WEKA data mining software: An update. SIGKDD Explorations Newsletter, 11(1), 10–18.
Hastie, T., Tibshirani, R., & Friedman, J. (2009). The Elements of Statistical Learning. Data
Mining, Inference, and Prediction. (2nd ed.) Springer.
IEEE P1484.2.1/D8 (2001) Draft Standard for Learning Technology—Public and Private
Information (PAPI) for Learners (PAPI Learner)–Core Features. Retrieved June 15, 2017,
from
http://metadata-standards.org/Document-library/Meeting-reports/SC32WG2/2002-05-
Seoul/WG2-SEL-042_SC36N0175_papi_learner_core_features.pdf.
IMS LIP (2001). IMS Learner Information Packaging Information Model Speciﬁcation, version
1.0. Retrieved June 15, 2017 from http://www.imsglobal.org/proﬁles/lipinfo01.html.
Jayaprakash, S. M., Moody, E. W., Lauría, E. J. M., Regan, J. R., & Baron, J. D. (2014). Early
alert of academically at-risk students: An open source analytics initiative. Journal of Learning
Analytics, 1(1), 6–47.
Jo, I. H., Kim, D., & Yoon, M. (2014). Analyzing the log patterns of adult learners in LMS using
learning analytics. In Proceedings of the Fourth International Conference on Learning
Analytics and Knowledge (pp. 183-187). ACM.
Kersting, N. B., Givvin, K. B., Thompson, B. J., Santagata, R., & Stigler, J. W. (2012). Measuring
usable knowledge: Teachers’ analyses of mathematics classroom videos predict teaching
quality and student learning. American Educational Research Journal, 49(3), 568–589.
Kot, F. C., & Jones, J. L. (2014). The impact of library resource utilization on undergraduate
students’ academic performance: A propensity score matching design. College & Research
Libraries, crl14–616.
Kovacic, Z. J. (2012). Predicting student success by mining enrolment data. Research in Higher
Education Journal, 15.
Kuo, Y. C., Walker, A. E., Belland, B. R., & Schroder, K. E. (2013). A predictive study of student
satisfaction in online education programs. The International Review of Research in Open and
Distributed Learning, 14(1), 16–39.
Lee, J. I. & Brunskill, E. (2012). The impact on individualizing student models on necessary
practice opportunities. In K. Yacef, O. Zaïane, H. Hershkovitz, M. Yudelson, & J. Stamper
(Eds.), Proceedings of the 5th International Conference on Educational Data Mining, pp. 118–
125.
Macfadyen, L. P., Dawson, S., Pardo, A., & Gasevic, D. (2014). Embracing big data in complex
educational systems: The learning analytics imperative and the policy challenge. Research &
Practice in Assessment, 9(2), 17–28.
Maldonado, R. M., Kay, J., Yacef, K., & Schwendimann, B. (2012). An interactive teacher’s
dashboard for monitoring groups in a multi-tabletop learning environment. In International
Conference on Intelligent Tutoring Systems (pp. 482–492). Springer Berlin Heidelberg.
Mattingly, K. D., Rice, M. C., & Berge, Z. L. (2012). Learning analytics as a tool for closing the
assessment loop in higher education. Knowledge Management & E-Learning: An International
Journal (KM&EL), 4(3), 236–247.
Park, J. H., & Choi, H. J. (2009). Factors inﬂuencing adult learners’ decision to drop out or persist
in online learning. Educational Technology & Society, 12(4), 207–217.
Park, Y., & Jo, I. H. (2015). Development of the Learning Analytics Dashboard to Support
Students’ Learning Performance. J. UCS, 21(1), 110–133.
Pascarella, E. T., & Terenzini, P. T. (1980). Predicting freshman persistence and voluntary dropout
decisions from a theoretical model. The Journal of Higher Education, 51(1), 60–75.
Romero, C., López, M.-I., Luna, J.-M., & Ventura, S. (2013). Predicting students’ ﬁnal
performance from participation in on-line discussion forums. Computers & Education, 68,
458–472. https://doi.org/10.1016/j.compedu.2013.06.009.
Schwendimann, B. A., Rodríguez-Triana, M. J., Vozniuk, A., Prieto, L. P., Boroujeni, M. S.,
Holzer, A., & Dillenbourg, P. (2016). Understanding learning at a glance: An overview of
learning dashboard studies. In Proceedings of the Sixth International Conference on Learning
Analytics & Knowledge (pp. 532–533). ACM.
24
D. Bañeres and M. Serra

Steven, L., & Stephanie D. (2014). Student Explorer: A Tool for Supporting Academic Advising at
Scale, In Proceedings of the ﬁrst ACM conference on Learning @ scale conference, 175–176.
Siemens, G., & Gasevic, D. (2012). Special issue on learning and knowledge analytics.
Educational Technology & Society, 15(3), 1–163.
Siemens, G., & Long, P. D. (2011). Penetrating the fog: Analytics in learning and education.
Educause Review, 46(5), 31–40.
Siemens, G. (2012). Learning analytics: envisioning a research discipline and a domain of practice.
In Proceedings of the 2nd international conference on learning analytics and knowledge
(pp. 4-8). ACM.
Student Explorer. (n.d). http://ai.umich.edu/portfolio/student-explorer/. Last accessed: 2017-06-15.
Widjaja, H. A., & Santoso, S. W. (2014). University dashboard: An implementation of executive
dashboard to university. In Information and Communication Technology (ICoICT), 2014 2nd
International Conference on (pp. 282–287). IEEE.
Predictive Analytics: Another Vision of the Learning Process
25

A Procedural Learning and Institutional
Analytics Framework
Alexander Amigud, Thanasis Daradoumis, Joan Arnedo-Moreno
and Ana-Elena Guerrero-Roldan
Abstract Data analyses provide the means for monitoring the quality of academic
processes and the means for assessing the ﬁscal and operational health of an orga-
nization. Data-driven decision making can help to empower academic leaders, fac-
ulty, and staﬀwith quantitative insights that guide strategies pertaining to enroll-
ment and retention, student support and quality assurance, communication, bullying
intervention, academic progress, and academic integrity. However, the integration
of analytics into the institutional context is not a trivial process. Much of the ana-
lytics approaches discussed in the literature take a theoretical stance outlining main
considerations but lacking the pragmatic edge. Our aim in this chapter is to assist
academic leaders in undertaking analytics design and implementation. To this end,
we synthesize the existing research and propose a procedural framework for integrat-
ing data analysis techniques and methods into a process that facilitates data-driven
decision making by aligning institutional needs with actionable strategies.
Keywords
Learning analytics ⋅Educational data mining ⋅Integration
framework ⋅Academic integrity ⋅Learning technology
A. Amigud (✉) ⋅J. Arnedo-Moreno ⋅A.-E. Guerrero-Roldan
Faculty of Computer Science, Multimedia and Telecommunications,
Universitat Oberta de Catalunya (UOC), Barcelona, Spain
e-mail: aamigud@uoc.edu
J. Arnedo-Moreno
e-mail: jarnedo@uoc.edu
A.-E. Guerrero-Roldan
e-mail: aguerreror@uoc.edu
T. Daradoumis
Faculty of Computer Science, Multimedia and Telecommunications,
Universitat Oberta de Catalunya (UOC), University of the Aegean, Mytilene, Greece
e-mail: adaradoumis@uoc.edu
© Springer International Publishing AG 2018
S. Caballé and J. Conesa (eds.), Software Data Engineering for Network eLearning
Environments, Lecture Notes on Data Engineering and Communications
Technologies 11, https://doi.org/10.1007/978-3-319-68318-8_2
27

28
A. Amigud et al.
1
Introduction
Academic institutions are complex hierarchical organizations that serve diverse
stakeholder groups. To manage stakeholder interactions more eﬃciently, they employ
information technologies. With the gradual transition from paper to computer-based
forms of communication, the sheer volume of processed, managed and stored data
has been increasing over the past three decades. Login attempts, payment transac-
tions, library records, enrollment inquiries, and course grades are some of the data
that academic institutions generate and retain as part of normal business activities.
These data can be further broken down into smaller chunks for detailed analyses. For
example, enrollment inquiries contain meta-data such as time-stamps and referring
URLs. These data can be helpful in optimizing the marketing campaign and tracking
the technology trends. Similarly, library logs can be used to identify bottlenecks in
resource sharing or make recommendations related to the current readers’ interests.
The ample volume of readily available data bears a massive potential of useful infor-
mation that can provide the insights and the means to control business and learning
matters. To take advantage of the available data, one needs to have access to the data
sources, analysis tools, methods and techniques.
Data analysis is not a new concept—student performance, enrolment targets,
annual budgets, and other metrics have been tracked since long before personal com-
puters became the de facto standard tool for data analysis. What has changed is the
breadth, depth, speed, and techniques of information processing and the rate at which
the outcomes are incorporated back into the learning and administrative processes.
Data is now analyzed and visualized beyond the traditional spreadsheet. The results
of the analyses are available on-demand in near real-time eliminating the need to
wait until the next monthly meeting to receive an update. The rationale stems from
the availability of minable data coupled with advances in data retrieval and compu-
tation. The past decade was marked by an increasing interest in data science, which
resulted in the development and adaptation of methods and tools for processing and
analyzing large volumes of data. New educational programs were created to address
the needs of the information age, and new professions emerged, such as data scien-
tists, who are now leading the way in developing techniques for deriving knowledge
from data.
Data science became a new branch of computer science and subsequently
branched oﬀinto data mining and analytics. The literature on learning and insti-
tutional analytics represents an amalgam of perspectives on trends and approaches.
Much of the literature proposes to solve isolated academic or institutional issues in
an ad-hoc fashion. In this paper, we take a pragmatic stance on analytics and propose
a procedural modular framework for developing and integrating data analysis tasks
into the institutions’ learning and administrative processes. The proposed framework
is objective-oriented and focuses on stakeholder needs and actionable outcomes.
The rest of the chapter is organized as follows. In Sect. 2 we commence with
the discussion of the main concepts that set the stage for the theoretical framework.
Next, we discuss related works and touch on the manner in which institutions go

A Procedural Learning and Institutional Analytics Framework
29
about analyzing data. In Sect. 3 we introduce a theoretical framework for implemen-
tation of learning and institutional analytics. In Sect. 4 we apply the framework to
address the issue of academic integrity in the e-learning environment. In Sect. 5 we
discuss the implications of the proposed framework. Section 6 concludes this chapter
summarizing the main points and outlining future directions.
2
Background
In this section we introduce the key concepts and review related works on educational
analytics and data mining. Our aim is not to provide an exhaustive review as literature
is rapidly changing, but to present select studies that discuss the main approaches.
Technology is an intermediate communication layer that much of the administra-
tive and academic transactions go through. These transactions can be classiﬁed into
two groups: those that pertain to learning, and those that relate to the administrative
aspects of educational services. Learners interact with content, peers, and instructors
(Moore and Kearsley 1996) through the use of technology such as learning manage-
ment systems (LMS), email, and social media. Learners also interact with academic
administrators and support staﬀusing tools such as online portals for administrative
inquiries and support services. Similarly, academic, administrative and support staﬀ
may use a variety of information management tools such as the education manage-
ment systems (EMS), student information systems (SIS), enterprise resource plan-
ning systems (ERP), and customer relationship management systems (CRM) to keep
track of resources and transactions. Any of these platforms may interface with each
other. The transactional data are being logged and stored in a format congruent with
a particular system architecture. The knowledge that can be inferred from the data is
diﬀerent from the raw data collected. For example, curricula vitae contain person-
ally identiﬁable information such as addresses and phone numbers, as well as for-
mal education and work experience. Similarly, enrollment inquiries are composed
of meta-data such as a time-stamp, and type of communication device. Although the
former type of data are collected for the purposes of screening job candidates in an
eﬃcient fashion, it can yield additional knowledge such as how much time a faculty
member requires to travel to and from work, the applicant’s age, as well as demo-
graphic characteristics based on the area of residence. The data from the enrollment
inquiries are collected for the purposes of optimizing marketing campaigns, but can
be used to proﬁle a student in terms of location and technology preferences. The
collected data can be combined with or compared against external data sources such
as census data, expanding the breadth of analysis and inferred knowledge.

30
A. Amigud et al.
2.1
Key Concepts
Data is the smallest unit of information that can be explicitly provided by users (e.g.,
ﬁrst name) or derived from interaction between users and information systems (e.g.,
number of login attempts). Information is inferred from data and may be absolute
(e.g., ﬁnal course grade) or conditional (e.g., probability of successful course com-
pletion.) The type of data available and the desired outcome will determine the type
of processing and analysis the data will undergo. For example, a summary of the
course grades for a single course may use statistical techniques, whereas course rec-
ommendations based on the previous choices of the student may use machine learn-
ing techniques.
Much of the data analyses are performed in response to an academic, administra-
tive, or technical problem and as such can be organized by its purpose (e.g., helping
students meet learning objectives, keeping track of institutional ﬁnancial targets, etc.)
Another way to classify the analysis is by the methodology (e.g., automatic knowl-
edge discovery, data visualization, etc.) Analysis may take the form of hypothesis
testing, where the hypothesis is presented beforehand and the results either support
or refute it. It may also be posed as a generalization and interpretation task, where
the output of an algorithm is reviewed upon completion of the analysis. These dis-
tinctions raise diﬀerent research questions and delineate the purposes of the analysis
from its methods. The two main approaches for constructing knowledge from data
are analytics and data mining. These, put in the academic context, become Educa-
tional Data Mining (EDM) and Learning Analytics (LA). The International Edu-
cational Data Mining Society deﬁnes EDM as: “an emerging discipline, concerned
with developing methods for exploring the unique types of data that come from edu-
cational settings, and using those methods to better understand students, and the
settings which they learn in” (Siemens 2012). The Society for Learning Analytics
Research deﬁnes Learning Analytics as: “the measurement, collection, analysis and
reporting of data about learners and their contexts, for purposes of understanding
and optimizing learning and the environments in which it occurs” (Siemens 2012). A
wider deﬁnition that encompasses computation has been proposed, and deﬁnes ana-
lytics as the “use of data, statistical analysis, and explanatory and predictive models
to gain insight and act on complex issues” (Brooks and Thayer 2016). Consider-
ing the broad scope of analytics and its applications in the academic context, we
deﬁne analytics as the process of uncovering insights about the state of learning and
administrative aﬀairs by applying computational techniques on the available data
and acting upon them. Its scope depends on the particular institutional context and
may range from student success to ﬁnancial liability to technology management to
accreditation deﬁciencies. The analyses can be conducted at the institutional level
(e.g., procurement practices), at the course level (e.g., quality of the learning mate-
rials), and also at the individual level (e.g., learning style).
Both analytics and data mining draw upon computer science to provide the nec-
essary theoretical underpinnings and practical methods for working with data. The
dichotomy between LA and EDM is not clear cut because in both approaches,

A Procedural Learning and Institutional Analytics Framework
31
processing, computation, and reporting functions play key roles. In much of the liter-
ature and popular media the terms analytics, mining, machine learning, and artiﬁcial
intelligence are used interchangeably to describe diﬀerent elements of the data anal-
ysis process, which may lead to confusion (Gudivada et al. 2016). Some posit that
the EDM is a part of LA that deals with computational aspects of the analysis, noting
that learning analytics takes advantage of educational data mining for the retrieval
of the information (Greller and Drachsler 2012). Siemens (2012) described LA in
terms of the purpose of the analysis (e.g., determine a sentiment, analyze discourse,
and predict learner success rate), and EDM in terms of analysis tasks (e.g., classiﬁca-
tion, clustering, Bayesian modeling, and visualization). However, both the purpose
and means of carrying out data analyses go hand in hand, for example sentiment
analysis—which falls under the umbrella of LA—is often posed as a classiﬁcation
task (EDM process), which by itself may be executed using Bayesian algorithms
(EDM process), and its results visualized (EDM process).
Another way to delineate LA and EDM is to examine the role of hypothesis: the
former is hypothesis-driven, whereas the latter explores data without a preconceived
hypothesis (Baepler and Murdoch 2010). Data mining has also been described in
terms of the analysis types that provide: description, estimation, prediction, classi-
ﬁcation, clustering, and association of features in data (Larose 2005). A literature
review by Dutt et al. (2017) of 166 articles on the use of clustering techniques of
educational data suggests that much of the onus is placed on the user to interpret
why the clusters were formed in one way or another, which may lead to variance in
interpretation. Analytics, on the other hand, has been described in terms of actionable
decision making that provides: diagnostic, descriptive, predictive, and prescriptive
guidance (Herschel et al. 2015).
Analytics employed by the academic institutions can be organized into two cate-
gories: “learning analytics”—pertaining to learning and teaching and “institutional
analytics”—pertaining to organizational processes, and business practices (Brooks
and Thayer 2016). The latter aims at tracking operating eﬃciency, while the former
aims to improve learning experience and also to reduce student attrition. Institu-
tional analytics can be further divided into subcategories that focus on a speciﬁc
institutional issue. For example, Educause’s report on the use of data analysis in
the academic environment entitled “The Analytics Landscape in Higher Education”
organizes analytics into ﬁve areas pertaining to learning, business, student man-
agement, faculty performance, and degree completion (Brooks and Thayer 2016).
Table 1 depicts the main focus areas of analytics and its applications. The report
suggests that much of the focus is directed towards operational and business pro-
cesses, whereas the scope of learning analytics is limited to the tracking of learning
outcomes and assessment. In spite of a growing number of proposals to use data in
resolution of academic-related issues, learning analytics appears to be underutilized.

32
A. Amigud et al.
Table 1
Use of analytics
Focus area
Type of analytics
Finance and budgeting
Business
Central IT
Business
Progress of institutional strategic plan
Business
Human resources
Business
Library
Business
Facilities
Business
Procurement
Business
Enrollment management, admissions, and
recruiting
Student management
Undergraduate student progress
Student management
Student degree planning
Student management
Instructional management
Student management
Student learning (learning outcomes)
Learning
Student learning (assessment and feedback)
Learning
Other student objectives
Learning
Faculty teaching performance
Faculty performance
Faculty promotion and tenure
Faculty performance
Faculty research performance
Faculty performance
Time to complete a degree
Degree completion
Cost to complete a degree
Degree completion
2.2
Applications and Frameworks
With the advancements in information retrieval and computing techniques, data-
driven solutions to academic issues started to draw more interest. New, intuitive
tools have emerged and attracted researchers from outside of the traditionally com-
putational disciplines who were eager to start exploring data generated by the aca-
demic institutions. The problem of student retention was examined in a study by
Elbadrawy et al. (2016) through identiﬁcation of at-risk students, and predicting their
performance. A study by Black et al. (2008) proposed to apply analytics to promote
connectedness and classroom community. A relationship between student percep-
tions of course community and the frequency of events in the LMS activity logs was
examined using statistical methods. Analytics has also been aimed at supporting stu-
dent wellbeing. One study tackled the issue of cyber-bullying, much of which occurs
outside of the classroom (Nitta et al. 2013). Traditionally, the monitoring activities
are performed manually by members of the parent-teacher association, which makes
detection diﬃcult and ineﬃcient. The dataset contains real-world data from bulletin
boards. The experiments were conducted using statistical techniques and their results
exceeded that of the baseline. A more elaborate approach to a school-wide bullying

A Procedural Learning and Institutional Analytics Framework
33
intervention leveraging machine learning techniques in combination with hardware
such as mobile devices, heart rate monitors, and video cameras was proposed by
Brahnam et al. (2015). The notion of providing student academic support through
learning analytics has been discussed by Joorabchi et al. (2016). They applied text
mining techniques in order to identify subject areas which learners ﬁnd the most
challenging.
Analytics can be considered a tool for measuring the quality of the academic pro-
cesses, an instrument for monitoring the health of the organization, and the means
for promoting an institutional agenda through automation of the decision-making
process. A number of frameworks support the implementation of analytics. A frame-
work proposed by Campbell and Oblinger (2007) portrays academic analytics “as an
engine to make decisions or guide actions”. It presupposes having a set of objectives
that need to be fulﬁlled. The ﬁve steps of their model include: capture data, produce
a report, predict trends, act on predictions, and reﬁne the analysis. The framework
also stresses the need for a stakeholder assessment and a division of role responsibil-
ities, the need for appropriateness of the interventions, the need for a quality-control
process to improve the outcomes, and the need for understanding challenges and
risks. Clow (2012) proposed to ground analytics in a learning theory, putting more
weight on the outcomes and eﬀectiveness of interventions. A framework for imple-
menting learning analytics proposed by Greller and Drachsler (2012) is aimed to pro-
vide quality assurance, curriculum development, and improve teacher eﬀectiveness
and eﬃciency. The framework comprises six dimensions: stakeholders, objective,
data, instruments, external limitations, and internal limitations. This contribution is
important in that it emphasizes the limitations and stresses the need for expertise to
successfully operate analytics applications; this point of view has been stressed else-
where (Larose 2005; Gašević et al. 2016). It also stresses that stakeholders vary in
their information needs, which entails a customized approach to analytics. Another
learning analytics framework proposed by Chatti et al. (2012) comprises four dimen-
sions: data and environment, stakeholders, objectives, and methods. It also highlights
the variance of stakeholder requirements and emphasizes the goal-oriented nature
of the analytics processes. A framework proposed by Khalil and Ebner (2015) com-
prises four dimensions that include: learning environment, big data, analytics, and
actions; again, this emphasizes various stakeholder interests and goal-driven inter-
ventions. It also outlines eight constraints that inﬂuence the design of the analytics
and include: privacy, access, transparency, policy, security, accuracy, restrictions,
and ownership.
These frameworks share the view that, ﬁrstly, analytics cannot be a one size-ﬁts-
all solution as diﬀerent stakeholders have diﬀerent requirements, and secondly, that
the analyses are oriented towards some objective. The frameworks can be broken
down into four questions: what is being analyzed, why is it being analyzed, how
is it being analyzed, and who is involved? Some scholars have warned about the
perils of using the cookie-cutter approach to analytics and stressed that the use of
analytics should be carefully planned and executed to avoid costly mistakes arising
from methodologically ﬂawed analyses (Larose 2005). The availability of ready-out-
of-the-box data analytics solutions may seem to be a viable option due to intuitive

34
A. Amigud et al.
design and low cost, but making accurate predictions and generalizations requires
knowledge of the computational methods and the context in which the institution
operates. Even the results of descriptive analysis may lead diﬀerent stakeholders to
diﬀerent conclusions. What follows is that the success of analytics implementation
hinges upon the stakeholders’ ability to accurately interpret and act upon the anal-
ysis results (Greller and Drachsler 2012). On the one hand, opening the possibility
of customization of the analysis parameters can expand the scope of analyses; on
the other hand, this feature may lead to faulty assumptions and potential hazards.
One way to address this problem is to provide multi-level access to analytics, for
example, through the use of dashboards (West 2012) where access privileges are
commensurate with a stakeholder’s level of expertise or organizational role.
The literature on data-driven approaches and solutions to the problems in edu-
cation is abundant. However, much of it is theoretical and lacking the pragmatic
dimension. In the next section we attempt to bridge this gap by presenting a proce-
dural framework.
3
Proposed Framework
This study is motivated by the gap in the literature on pragmatic methods for inte-
grating analytics into an educational context. We aim to bridge this gap by presenting
a procedural framework for developing and implementing analytics tasks. The main
challenge of creating such a framework stems from the context-speciﬁc nature of
the data analysis, which requires it to be able to accommodate a variety of com-
putational techniques, as well as to address any environmental constraints that may
arise. The strategy that has worked well at one institution may not always translate
well to successfully capturing and measuring data in another. Our framework shares
many of the considerations with the earlier proposals (Campbell and Oblinger 2007;
Chatti et al. 2012; Clow 2012; Greller and Drachsler 2012; Khalil and Ebner 2015)
and encompasses a set of processes for creating and implementing analytics tasks.
Analytics is viewed as a sum of parts that may change with environmental shifts. It
is also considered a needs-based measure, where each analytics task has an owner
and is designed to address a speciﬁc need, which contrasts with the one-size-ﬁts-all
approach (Larose 2005; Gašević et al. 2016).
Our proposal is grounded in the assumption that analytics serves the purpose of
gaining insights about the status of academic and administrative activities which lead
to actions. Analytics is a set of procedural steps designed to meet an institutional goal.
The notion of actionable knowledge is of paramount importance because gaining
insights and monitoring trends without taking actions lacks purpose and eﬃciency.
Although the issue tracking and trending has been argued to promote accountability
(West 2012), it cannot be the means in itself. Analytics provides the necessary means
for informed decision making to control or improve one or more aspects of the learn-
ing, administrative, or business processes. It is further predicated on the assumption
that the performance indicators, threshold levels, and responses are known. It would

A Procedural Learning and Institutional Analytics Framework
35
Fig. 1
Conceptual framework
be diﬃcult to act and achieve a desired result without knowing what the outcome
should be.
The advantage of the proposed framework over the existing ones is fourfold: First,
it emphasizes actionable metrics. The purpose of analytics is to take action and inte-
grate the knowledge-based decisions back into the context. Second, the framework is
ﬂexible enough to accommodate any computational approach or data management
strategy. Third, the framework is needs-based and each analytics task has a clear
objective and an owner. Fourth, the framework is modular, which allows for expan-
sion and adaptation to a variety of environments. The framework is depicted in Fig. 1
and comprises four steps and three layers.
The ﬁrst layer is the institutional context, which stipulates the objectives and
ﬁnancial, legal, social, and ethical constraints. The need to fulﬁll the objectives
comes from the environment in which the stakeholders operate, so do the limita-
tions on the types of interventions that arise from the analysis of the state of the
environment. The second layer comprises the procedural steps for monitoring and
acting upon the changes in the state of the context. The information, both expected
and actual, are drawn from the environment, analyzed, and turned into actions when
necessary, which in turn are incorporated back into the environment. The third
layer denotes the functional roles. Data and analyses are the prerogative of the data
scientists—a group of stakeholders responsible for implementation and support of
analytics, whereas the management of objectives and execution of interventions fall
on the shoulders of the faculty and staﬀ. The needs analysis, formulation of objec-
tives, and actions are the administrative functions, whereas the functions of man-
aging and processing information that supports the actions are the products of the
data science and information technology. This suggests a need for a strong relation-
ship between the stakeholders as well as a mutual understanding of the contextual

36
A. Amigud et al.
Fig. 2
Process ﬂow
peculiarities and overall institutional objectives. The proposed framework further
assumes that the environment is dynamic and may undergo changes in response to
certain interventions resulting from the analysis of data. This requires the analysis
techniques to be continuously evaluated and readjusted when necessary to ensure
validity; the analytics processes may themselves become the subject of analysis to
ensure quality of information and decisions.
The process view, depicted in Fig. 2, summarizes the procedural layer and shows
the process ﬂow from the problem deﬁnition to the actions taken. Analytics is imple-
mented to solve a speciﬁc problem or address a deﬁned need. It commences with
the needs analysis, where the challenges and key actors are identiﬁed and metrics
and actions are deﬁned. The data acquisition phase is concerned with identifying
data sources and types. It is also concerned with data acquisition. In this phase, the
data analysis methods and techniques are identiﬁed and the data are analyzed. The
results are the inputs to the actions phase, which is concerned with putting decisions
into actions. In the next section, the framework components are discussed in greater
detail followed by an application of the framework, using academic integrity as an
example.

A Procedural Learning and Institutional Analytics Framework
37
Table 2
The objectives phase
Process step
Guiding questions
Deﬁne problem
What is the scope of the problem?
Identify stakeholders
Who is in charge of mitigating the problem?
Identify limitations
What are the technical, budgetary, ethical,
legal, administrative, and logistical limitations?
Deﬁne metrics
How is the problem being tracked and
measured?
Deﬁne actions and thresholds
What needs to be done when the problem
reaches a certain level?
3.1
Organizational Needs and Objectives
The process of developing an analytics task commences with identifying the needs
and deﬁning the objectives the analytics is expected to attain. Table 2 depicts the
procedural steps as well as the guiding questions of the objectives phase. The assess-
ment of needs is a key element in the framework. Analytics should be viewed as a
precision tool that targets speciﬁc issues rather than presents a broad report where a
plan of action is developed ad-hoc. Each identiﬁed need or objective has an owner
and is mapped to a set of metrics for tracking and measurement. Each analysis out-
put is mapped to a conditional response. The intervention should be commensurate
with the problem at hand. Considering that much of the problems are constructs, it is
imperative to establish construct validity and validate metrics. The methods and tech-
niques used for identifying needs, ranking their importance, and ﬁnding appropriate
interventions vary among institutions, as do the needs and objectives themselves.
The role of analytics may vary among institutions. It may provide an advisory
function to human experts making the decisions, or it may completely automate the
decision-making process, minimizing the human involvement. Analytics may also
provide reactive or proactive responses to a problem. The objectives phase lays the
foundation for the subsequent steps and is concerned with narrowing the scope of
the analysis task, mapping stakeholders to the speciﬁc problem being solved, creat-
ing metrics, and outlining the types of interventions and conditions under which the
interventions will be triggered. The scope of issues that will be addressed through
the use of analytics depends on the speciﬁc institutional context. For example, a
survey of 861 research papers was carried out by Bozkurt et al. (2015) to classify
current trends in distance education research published between 2009 and 2013 in
seven peer-reviewed journals. The results suggest that much of the research exam-
ined the learners’ emotional states, focusing on gauging student satisfaction and
learner perception, and examining links to other variables such as gender and age.
The results suggest a keen interest in proﬁling learners and identifying their individ-
ual diﬀerences. Some of the variables used in the distance learning research include:
perceptions, communication, age, satisfaction, academic-performance, self-eﬃcacy,

38
A. Amigud et al.
participation, gender impact, collaboration, interaction, social-presence, and moti-
vation.
After the problem and key actors are identiﬁed, an assessment of constraints that
may inﬂuence the design of the analytics is performed. Its aim is to identify environ-
mental constraints that, among others, may include technical, ﬁnancial, ethical, legal,
administrative and logistical issues. The monitoring of the objectives may require
data that is not readily available, is expensive, or the collection of which is subject to
legal restrictions and ethical reviews. Assessing the means required for the success-
ful execution of the analysis early on will prevent shortfalls during implementation
and delivery.
3.2
Data
The data phase of the framework aims to collect and feed the analysis component
with the relevant data. The process steps and related questions are presented in
Table 3. Data can be broadly organized into two categories: the internal data—that
the organization collects (e.g., student-generated content), and the external data—
that is acquired from a third party service (e.g., IP address geolocation database). In
many of the cases the analytics application will use a mixture of data sources and
types. For example, event logs from computer systems are commonly used for anal-
ysis, as they are readily available and capture a variety of user behaviors. The logs
may be combined with student-produced content such as forum postings and papers
to perform tasks such as the sentiment analysis. Learner produced-content is readily
available and constitutes a valuable resource as it may be used to support a number
of academic processes, such as enforcement of academic integrity and student sup-
port. Textual data can also be used for personality proﬁling (Argamon et al. 2005;
Noecker et al. 2013), which has a variety of applications in the learning environment.
Data are often stored in a variety of formats by diﬀerent systems, which requires
a tailored approach. After the data are acquired and before it can be analyzed, the
data need to undergo a pre-processing step which ﬁlters out the unnecessary parts.
For example, running a sentiment analysis of the course reviews does not involve
information about the type of the web browser used to post messages and therefore,
it may be safely removed. The steps of data acquisition and processing are technical;
therefore, the end users of the analytics system do not need to interact with the raw
data, but only access the outcomes of the analysis.
3.3
Analysis
The analysis phase of the framework is concerned with identifying the methods and
techniques for analyzing the data and carrying out the analyses. The process steps
and related questions are presented in Table 4. The analyses can be conducted using

A Procedural Learning and Institutional Analytics Framework
39
Table 3
The data phase
Process step
Guiding questions
Identify data sources and types
What is the data source?
Acquire data
How can the data be retrieved?
Process data
What parts of the data are irrelevant or
redundant?
Table 4
The analysis phase
Process step
Guiding questions
Identify analysis methods
What is the analysis technique?
Optimize performance
Are the results valid?
Create analysis task
Can a non-expert operate it?
Analyze data
What are the results?
statistical and machine learning techniques. While the former is often used in quanti-
tative research, the latter is a growing ﬁeld of computer science concerned with com-
putational theories and techniques for discovering patterns in data. A key advantage
of machine learning over statistical techniques is that machine learning algorithms
learn from data without being speciﬁcally programmed for each task. The analyses
can be organized into four categories: diagnostic, descriptive, predictive, and pre-
scriptive (Herschel et al. 2015). The ﬁrst two categories deal with past or present
events, while the latter two categories attempt to glimpse into the future.
The analysis techniques are expected to demonstrate the validity of the selected
approaches before the results are translated into actions. To this end, the proposed
framework provides an optimization loop between the data and analysis phases
that enables ﬁnding the optimal balance between the data and the computational
approach. It also facilitates the ﬁne-tuning of the data selection and processing,
and testing of the computational methods prior to deployment into the produc-
tion environment. The computational performance can and should be quantitatively
assessed; a number of metrics are available to estimate the performance (Sokolova
and Lapalme 2009; Fawcett 2006). Data may be analyzed on-demand or continu-
ously depending on the stakeholder requirements, computational resources, and the
type of data used. For example, enrollment data may be analyzed every time the
new data becomes available, whereas a comparison of the ﬁnal grades between two
courses is performed upon request. The process of running the analysis should be
friendly enough for non-experts to use and communicate to others.
The output of the analysis process is the input to the decision making process
where actions are congruent with the stakeholder requirements. In some cases, the
analysis results may be presented in the form of a report issued to select stakeholders
who then carry out the actions; in other cases, actions may be automated.

40
A. Amigud et al.
Table 5
The action phase
Process step
Guiding questions
Action required
Are the conditions satisﬁed for taking action?
Perform action
Was the intervention delivered?
3.4
Actions
The actions phase of the framework is concerned with translating the analysis results
into interventions. Actions are deﬁned in the needs phase and constitute measurable
and conditional responses to the output of the data analysis. They aim to answer
the question—“What if?” For example, what would happen if the student grades fell
beyond a pre-deﬁned threshold level? Analytics may serve as an advisor to the stake-
holders who act upon the received information, or may serve as an actor that carries
out the decisions automatically without human involvement. The process steps of
the action phase and corresponding questions are presented in Table 5.
Analytics systems may grow with time and add multiple objectives, analyses and
interventions. Some analytics tasks will only yield the reports to the stakeholders,
who will then take the necessary actions (e.g., conduct academic integrity review),
while other tasks will trigger automatic actions (e.g., produce a list of recommended
courses). When the actions are automated, the stakeholders may still monitor the
quality of the task execution and intervene if necessary. Interventions, akin to data
analyses and data structures, are all context-speciﬁc items. For example, an analytics
task that performs automatic screening of plagiarism in student-produced content
may be conﬁgured to produce a warning message for the student’s eyes only, or it
may trigger a report for the instructor to investigate the issue.
4
Application of the Framework
In this section we discuss application of the framework. We chose to target the issue
of academic integrity because it is universally applicable to all modes of education
and particularly challenging in the distance learning context, and because academic
integrity strategies reinforce the foundation of trust, which is imperative to accred-
itation and institutional credibility. The need for academic integrity has long been
stressed in the literature (McCabe et al. 1999; Cronan et al. 2017; Bertram Gal-
lant 2017). Academic institutions have legal and moral obligations to maintain the
culture of trust and promote accountability. The problem can be narrowed down
to veriﬁcation of the learner identities and validation of the learner-produced con-
tent. The former task is often delegated to exam invigilators who check the students’
identity documents, while the latter task is delegated to the course instructors or
their assistants who validate the veracity of the submitted content. This two part

A Procedural Learning and Institutional Analytics Framework
41
veriﬁcation process is disruptive, inconvenient and resource intensive. The provision
of academic integrity through data analysis enables concurrent identity veriﬁcation
and validation of authorship. It also yields additional beneﬁts such as integration
into the continuous assessments process, greater eﬃciency due to automation, and
minimal disruption of the learning and teaching processes. The issue of academic
integrity—traditionally tackled by observing the students—has been addressed com-
putationally through the analysis of learner-produced content (Amigud et al. 2016,
2017a, b), and this is the analysis framework that will be adopted in this example.
Student-produced textual data are readily available and can be used to map stu-
dent identities to their academic work in a single analysis. The behavioral patterns in
the student-produced textual data are thought to be author-speciﬁc and are expected
to show greater similarity to each other than to that of other students. Given two sets
of written assignments, any misattribution of authorship will prompt an intervention
by the instructor. Therefore, the analysis task can be posed as a classiﬁcation prob-
lem, where instances of misclassiﬁcation are ﬂagged for manual review by the course
instructor. Because the assessment is done algorithmically, this approach provides
a more eﬃcient alternative to having human proctors physically observe students.
Each procedural step and corresponding outputs are elaborated in Tables 6, 7, 8
and 9.
5
Discussion
The literature suggests that in spite of growing research interest and potential ben-
eﬁts, much of the colleges and universities in the U.S. demonstrate only a mod-
erate level of maturity towards analytics, where institutional research, ﬁnance, IT,
and advancement are the main areas that take advantage of analytics (Brooks and
Thayer 2016). Analytics can and should do much more than analyze business trends
or satisfy accreditation requirements. The literature is abundant with proposals to
solve issues in education using data analysis that are waiting to be integrated into the
learning and administrative processes. This denotes a need for a pragmatic frame-
work that facilitates the integration of the research into a coherent set of analytics
tasks. The proposed framework provides just that by guiding the integration of the
data analysis approaches into institutional practices.
In the previous section we discussed the procedural steps for bridging the need for
academic integrity with the application of machine learning. In this example, ana-
lytics serves an advisory function to the course instructors. However, the instructors
will not be able to receive the beneﬁts of data analysis without the required support
of the IT and research teams. Analytics is a team eﬀort. The development teams need
to work jointly with the experts to create sound analysis processes and provide any
necessary technical training on the use of analytics and interpretation of the results.
Analytics is a tool that helps to keep a grip on the goals and objectives estab-
lished by stakeholders; however, in itself it should not be considered a sole remedy
to problems that arise, because solutions entail actions. It is important to highlight
a distinction between eﬀectiveness of the data analysis techniques and that of the
administrative processes. If the stakeholders are unable to interpret or reluctant to

42
A. Amigud et al.
Table 6
The objectives phase
Process step
Output
Delegated to
Deﬁne problem
The need to provide academic integrity is stipulated by
the accreditors. The objective is to provide assurance
that students are who they say they are and that they did
the work they say they have done. The context of the
analytics task implementation is a graduate-level
research methods course delivered online by a European
university. The course comprises four written
assignments
Admin., faculty
Identify
stakeholders
The main stakeholders in this process are: the students,
whose assignments will be analyzed; the instructors
who are the owners of the analytics task; and academic
administrators who control the quality of the educational
process. The analytics task is developed by the research
team in cooperation with the IT department who will
provide the necessary access to the student data,
facilitate the development, and provide training to users
Admin., faculty
Identify
limitations
Data collection procedures require compliance with the
university policies. Written assignments are the
principal means for course evaluation. The analyses will
be conducted after the second assignments, as a
minimum of two assignments is required to perform
classiﬁcation tasks. There is no LMS integration and
analyses and reporting are performed externally to the
LMS. Classiﬁcation results cannot be calculated to a
certainty and instructor review will be required in cases
of misclassiﬁcation. Training on how to use the
analytics task will be required and provided to the
instructors by the development team
Admin, faculty, IT
Deﬁne metrics
This task is concerned with the measurement of the
patterns of language use, such as the frequency of word
pairs and triplets compared across assignment pairs of
each student. Cases of misclassiﬁcation will be ﬂagged
for the instructor intervention
Admin, faculty, IT
Deﬁne actions
The analysis will produce a classiﬁcation report. Any
instance of misclassiﬁcation will be considered a case of
potential misconduct and trigger a manual review by the
instructor. Any remedial actions will be taken at the
instructor’s discretion
Admin, faculty
act upon the information conveyed by the data analysis, the problem that the ana-
lytics was implemented to address will not be resolved. In the case of automated
actions, such as recommender systems, the stakeholders need to maintain quality
control and ensure that analytics is eﬀective. The system can yield highly accurate
predictions and a vast array of reports, but if the stakeholders fail to act upon the
information and resolve the problem, the analytics is not going to make the problem
disappear.

A Procedural Learning and Institutional Analytics Framework
43
Table 7
The data phase
Process step
Output
Delegated to
Identify data sources
and types
The data source for this analytics task will be
limited to the textual content produced by
students. The students submit their assignments
through the LMS, which stores the ﬁles in their
native formats. The ﬁle-naming convention
includes the student’s name, number of the
learning activity, a part of the original title, and a
timestamp, which will be used for identiﬁcation
and labeling of the document authors. The course
comprises four learning activities; therefore, four
ﬁles per student will be retrieved
Faculty, IT
Acquire data
The IT team will facilitate access to the LMS data
store from where the documents will be retrieved
IT
Process data
The assignments are produced in a variety of
formats and include Word document, the
portable document format, and Latex, which
requires text to be parsed from each ﬁle type
prior to the analysis. Additional pre-processing
steps such as the removal of noise, which
includes symbols, names, headers, footers, and
direct citations, also need to be performed
IT
The relationship between information and actions is of a paramount importance.
The success of an analytics strategy is predicated on a clear understanding of what
the needs are, how they are tracked and measured, and what actions should be taken
to address them. Analytics is a goal-oriented tool for supporting learning and opera-
tions. The type of information monitored through the use of analytics and the way it is
processed depends on the institutional context. This suggests that implementation of
analytics is the process of adaptation and customization. One should be careful when
choosing the out-of-the-box analytics products and ensure that the metrics reﬂect
their own institutional context. Analytics is in a dynamic state, because the needs
and contextual limitations are subject to change and it requires continuous evalua-
tion. As new types of data emerge, so do new computational techniques. “Install it
and forget it” may not be the best strategy to follow—when it comes to analytics
tools—as it will inevitably lead to poor decisions. Reevaluation of analytics is ben-
eﬁcial not only for the improvement of the quality of information and decisions, but
also for compliance with legal requirements and ethical standards that themselves
are subject to change.

44
A. Amigud et al.
Table 8
The analysis phase
Process Step
Output
Delegated to
Identify analysis methods
The analysis will follow the protocol outlined in
Amigud et al. (2016). The data will be retrieved,
processed and analyzed to test the techniques and
adjustments will be made to make the process
more eﬃcient and eﬀective. Accuracy measure is
used to assess classiﬁcation performance
Research, IT
Optimize
To improve performance, the protocol is
amended to include feature selection as outlined
in (Amigud et al. 2017b). Feature weighing is
employed and limited to the top 300 features. The
feature set comprises bigrams with stop words
preserved. Texts are split into 500 word chunks
Research
Create analysis task
Analyses are conducted using Scikit-Learn
(Pedregosa et al. 2011) machine learning library
in the Python language. An intuitive graphical
interface is created (Amigud et al. 2017a) to
allow the faculty to perform analysis on-demand
IT
Analyze data
The assignments are passed on to the analytics
engine. The outcome of the analysis is a report
that identiﬁes students whose work requires a
closer look
Faculty
Table 9
The actions phase
Process step
Output
Delegated to
Action required
Upon examining the analysis
report and the students’ work,
the instructor is satisﬁed with
the outcome
Faculty
Perform action
No additional administrative
actions will be taken at this
time
Faculty
6
Conclusion and Future Directions
In this chapter we proposed and discussed a procedural framework for integrating
data analysis research into learning and institutional analytics with an aim to address
the needs of a particular institutional environment. The framework provides the
basis for aligning institutional needs with remedial actions, and assigning the own-
ership of problems to stakeholders. It builds upon the earlier theoretical frameworks
(Campbell and Oblinger 2007; Chatti et al. 2012; Clow 2012; Greller and Drachsler
2012; Khalil and Ebner 2015), expanding the scope of analytics to address both aca-
demic and institutional issues by providing a pragmatic foundation for developing

A Procedural Learning and Institutional Analytics Framework
45
analytics tasks. We expressed our concerns regarding analytics being a one-size-ﬁts-
all solution and stressed that continuous evaluation and context-speciﬁc adaptations
are required to maintain the quality and validity of the decisions. We also suggested
taking a lean and modular approach to analytics and address only the well-deﬁned
needs, curbing the temptation to monitor arbitrary data structures without having a
response plan, just because the data are available. To demonstrate with an exam-
ple, we applied the framework to the issue of academic integrity and outlined the
procedural steps for creating an analytics task for providing identity and authorship
assurance in the e-learning environment.
Future research should focus on mapping analysis techniques to the problems
they address in order to draw comparisons and organize data analysis techniques by
their type. Because the proposed framework views analytics as a sum of parts, this
would enable institutions to pick and choose the best in class analytics tasks that
are congruent with their needs and objectives. Such modular approach promotes
scalability and eﬃciency by investing in only what is required to solve a particular
issue.
Acknowledgements This work was partly funded by the Spanish Government through the Enhanc-
ing ICT education through Formative assessment, Learning Analytics and Gamiﬁcation project
(grant TIN2013-45303-P), CO-PRIVACY (grant TIN2011-27076-C03-02), and SMARTGLACIS
(grant TIN2014-57364-C2-2-R); and also by the Spanish Ministry of Economy and Competitive-
ness (grant TRA2013-48180-C3-3-P).
References
Amigud, A., Arnedo-Moreno, J., Daradoumis, T., & Guerrero-Roldan, A.-E. (2016). A behavioral
biometrics based and machine learning aided framework for academic integrity in e-assessment.
In 2016 International conference on intelligent networking and collaborative systems (INCoS)
(pp. 255–262). IEEE.
Amigud, A., Arnedo-Moreno, J., Daradoumis, T., & Guerrero-Roldan, A.-E. (2017a). Open proctor:
An academic integrity tool for the open learning environment. In International conference on
intelligent networking and collaborative systems (pp. 262–273). Springer.
Amigud, A., Arnedo-Moreno, J., Daradoumis, T., & Guerrero-Roldan, A.-E. (2017b). Using learn-
ing analytics for preserving academic integrity. The International Review of Research in Open
and Distributed Learning, 18(5).
Argamon, S., Dhawle, S., Koppel, M., & Pennebaker, J. W. (2005). Lexical predictors of personality
type. In Proceedings of the 2005 Joint Annual Meeting of the Interface and the Classiﬁcation
Society of North America.
Baepler, P., & Murdoch, C. J. (2010). Academic analytics and data mining in higher education.
International Journal for the Scholarship of Teaching and Learning, 4(2), 17.
Bertram Gallant, T. (2017). Academic integrity as a teaching & learning issue: From theory to
practice. Theory Into Practice (in-press).
Black, E. W., Dawson, K., & Priem, J. (2008). Data for free: Using lms activity logs to measure
community in online courses. The Internet and Higher Education, 11(2), 65–70.
Bozkurt, A., Akgun-Ozbek, E., Yilmazel, S., Erdogdu, E., Ucar, H., & Guler, E., et al. (2015).
Trends in distance education research: A content analysis of journals 2009–2013. The Interna-
tional Review of Research in Open and Distributed Learning, 16(1).
Brahnam, S., Roberts, J. J., Nanni, L., Starr, C. L., & Bailey, S. L. (2015). Design of a bullying detec-
tion/alert system for school-wide intervention. In International conference on human-computer
interaction (pp. 695–705). Springer.

46
A. Amigud et al.
Brooks, D. C., & Thayer, T.-L. B. (2016). Institutional analytics in higher education, Technical
report, Research report. Louisville, CO: ECAR.
Campbell, J. P., & Oblinger, D. G. (2007). Academic analytics. EDUCAUSE Review, 42(4), 40–57.
Chatti, M. A., Dyckhoﬀ, A. L., Schroeder, U., & Thüs, H. (2012). A reference model for learning
analytics. International Journal of Technology Enhanced Learning, 4(5–6), 318–331.
Clow, D. (2012). The learning analytics cycle: closing the loop eﬀectively. In Proceedings of the
2nd international conference on learning analytics and knowledge (pp. 134–138). ACM.
Cronan, T. P., McHaney, R., Douglas, D. E., & Mullins, J. K. (2017). Changing the academic
integrity climate on campus using a technology-based intervention. Ethics Behav., 27(2), 89–
105.
Dutt, A., Ismail, M. A., & Herawan, T. (2017). A systematic review on educational data mining.
IEEE Access.
Elbadrawy, A., Polyzou, A., Ren, Z., Sweeney, M., Karypis, G., & Rangwala, H. (2016). Predicting
student performance using personalized analytics. Computer, 49(4), 61–69.
Fawcett, T. (2006). An introduction to roc analysis. Pattern Recognition Letters, 27(8), 861–874.
Gašević, D., Dawson, S., Rogers, T., & Gasevic, D. (2016). Learning analytics should not pro-
mote one size ﬁts all: The eﬀects of instructional conditions in predicting academic success. The
Internet and Higher Education, 28, 68–84.
Greller, W., & Drachsler, H. (2012). Translating learning into numbers: A generic framework for
learning analytics. Educational Technology & Society 15(3), 42–57.
Gudivada, V., Irfan, M., Fathi, E., & Rao, D. (2016). Cognitive analytics: Going beyond big data
analytics and machine learning. Handbook of Statistics, 35, 169–205.
Herschel, G., Linden, A. & Kart, L. (2015). Magic quadrant for advanced analytics platforms. Gart-
ner Report G, 270612.
Joorabchi, A., English, M., & Mahdi, A. E. (2016). Text mining stackoverﬂow: An insight into chal-
lenges and subject-related diﬃculties faced by computer science learners. Journal of Enterprise
Information Management, 29(2), 255–275.
Khalil, M., & Ebner, M. (2015). Learning analytics: Principles and constraints. In Proceedings of
world conference on educational multimedia, hypermedia and telecommunications (pp. 1326–
1336).
Larose, D. T. (2005). An introduction to data mining. Traduction et adaptation de Thierry Vallaud.
McCabe, D. L., Trevino, L. K., & Butterﬁeld, K. D. (1999). Academic integrity in honor code and
non-honor code environments: A qualitative investigation. The Journal of Higher Education,
70(2), 211–234.
Moore, M. G., & Kearsley, G. (1996). Distance education: A systems approach. Boston, MA:
Wadsworth.
Nitta, T., Masui, F., Ptaszynski, M., Kimura, Y., Rzepka, R., & Araki, K. (2013). Detecting cyber-
bullying entries on informal school websites based on category relevance maximization. In IJC-
NLP (pp. 579–586).
Noecker, J., Ryan, M., & Juola, P. (2013). Psychological proﬁling through textual analysis. Literary
and Linguistic Computing, 28(3), 382–387.
Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., & Grisel, O., et al. (2011).
Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12, 2825–
2830.
Siemens, G. (2012). Learning analytics: Envisioning a research discipline and a domain of practice.
In Proceedings of the 2nd international conference on learning analytics and knowledge (pp. 4–
8). ACM.
Sokolova, M., & Lapalme, G. (2009). A systematic analysis of performance measures for classiﬁ-
cation tasks. Information Processing & Management, 45(4), 427–437.
West, D. M. (2012). Big data for education: Data mining, data analytics, and web dashboards.
Governance Studies at Brookings, 4, 1.

Engagement Analytics: A Microlevel
Approach to Measure and Visualize
Student Engagement
Isuru Balasooriya, Enric Mor and M. Elena Rodríguez
Abstract Learner disengagement is a persisting issue in the Science Technology
Engineering and Mathematics (STEM) subjects. Student engagement is dynami-
cally constituted by the behavioural, cognitive and emotional dimensions of
engagement in a learning environment. Although strongly linked with academic
achievement, much of the details of engagement becomes lost in a retrospective
measurement. Timely and microlevel data on the other hand has the ability to enrich
the traditional learning analytics dataset. From a pilot study carried out at
Universitat Oberta de Catalunya, where we have designed a self-reported data
capture module that collects microlevel engagement data, initial results suggest the
validity of the proposed approach and data. In this paper we emphasize how our
approach enables better understanding of the student learning process and their
characteristics such as cognitive patterns, emotional states and behaviours that leads
to academic success and also enable richer feedback from teachers and informed
decision making by the institution.
Keywords Learning analytics ⋅Student engagement ⋅Self-reported student
data ⋅Virtual learning environments ⋅Programming education
I. Balasooriya (✉) ⋅E. Mor ⋅M. E. Rodríguez
Faculty of Computer Science, Multimedia and Telecommunications,
Universitat Oberta de Catalunya, Barcelona, Spain
e-mail: ibalasooriya@uoc.edu
E. Mor
e-mail: emor@uoc.edu
M. E. Rodríguez
e-mail: mrodriguezgo@uoc.edu
© Springer International Publishing AG 2018
S. Caballé and J. Conesa (eds.), Software Data Engineering for Network eLearning
Environments, Lecture Notes on Data Engineering and Communications
Technologies 11, https://doi.org/10.1007/978-3-319-68318-8_3
47

1
Introduction
The ﬁeld of online education is seeing an increasing number of courses, utilization
of state-of-the-art technologies and a multitude of online learners. In this transition
from traditional to online learning modes, the traditional concepts of the learning
process and factors such as traditional interactions between teachers and students
themselves are also in need of a transition. What was once a traditional face-to-face
lesson delivered by a teacher standing in front of a classroom is now an instruc-
tional video or a recorded presentation of slides delivered through a webpage.
Unlike an observable physical classroom where clear signs of learner disengage-
ment and emotions such as boredom and stress can be identiﬁed, virtual spaces are
not equipped well to see beyond the mere behavioural data students leave behind in
a virtual space. This logged behavioural data therefore becomes the record of
students’ engagement with their learning and their achievement is merely reﬂected
in the grades they receive. In recent years learner disengagement has become a
troublesome issue, particularly in the highly sought-out and challenging area of
Science Technology Engineering and Mathematics (STEM). High dropout rates and
lower grades across different subjects of STEM are tell-tale signs of student dis-
engagement and have created a barrier between highly sought after scientiﬁc careers
and the knowledge output of the general student body.
Student engagement has been deﬁned as the quality of student interactions with
the learning activities and other components of their learning environment (Shernoff
2013) and is constituted by a dynamic relationship between the behavioural, cog-
nitive and emotional types of engagements (Fredricks et al. 2004). In other words
the engagement of students is tightly bound to their environment, and unless studied
within that context, may not produce a thorough representation of their
engagement with learning. There is strong evidence that links student engagement
with academic achievement (Alexander et al. 1997; Voelkl 1997; Marks 2000;
Fredricks et al. 2004; Sirin and Rogers-Sirin 2004; Glanville and Wildhagen 2007;
Kuh et al. 2007; Hughes et al. 2008; Kelly 2008; Shernoff and Schmidt 2008; Ladd
and Dinella 2009) and has furthermore been associated with student attrition,
retention, motivation and institutional success (Beer et al. 2010). Arguably pro-
gramming education is an important stepping stone in STEM higher education and
usually a prerequisite for other subjects in technology. As it is with many other
STEM subjects, the dropout rates of programming courses are high and many
students face difﬁculties in learning successfully (Thomas et al. 2002; Robins et al.
2003; Wiedenbeck et al. 2004; Berglund and Eckerdal 2015). In an era where
programming is considered a literacy such as reading or writing for the top intel-
lectual classes (Prensky 2008), where the global, fast-changing economy requires
knowledgeable workers who can assimilate new information and think critically to
solve problems (Fredricks et al. 2004) these challenges have to be duly answered.
48
I. Balasooriya et al.

The traditional and also the contemporary approach of measurement of student
engagement is the retrospective data collection using large survey instruments at the
end of a semester. Some newer approaches to engagement measurement and
analysis has focused on solely the behavioural aspects of students. These different
approaches either do not provide the timely data that engagement creates or heavily
infers engagement based on the single dimension of student behaviour. Although
online learning environments constantly produce and record student engagement,
we observe a lack of timely information, particularly when it can be used effectively
in the STEM related areas where timely information matter. To explore this critical
gap in the online education paradigm as a research problem and to present a
technology-based proposal grounded in education theory, we present our proposal
for a microlevel engagement measurement which unites system-level behavioural
data as well as self-reported student data that covers the cognitive and the emotional
dimensions of engagement and thereby extend the current utilization of learning
analytics (LA). In order to measure the engagement dimensions of cognitive and
emotional which are hard to infer, we have designed and developed an engagement
data capture module which can be integrated into the online learning environment.
To present the theoretical framework of student engagement and our own
research design in order to address the research problem, this chapter is organized
as follows. Section 2 presents the state of the art where student engagement
research is deﬁned and discussed, as well as different levels of granularity and
various student engagement instruments and approaches. The third section presents
the missing links of student engagement in contemporary research and present the
research gap we aim to answer and also illustrate the proposed design of student
engagement data capture and the empirical study we carried out to answer our
research gap. The fourth section presents our preliminary ﬁndings as well as the
scope of our results as a whole. In the ﬁfth section we discuss what these results
mean to the current research on student engagement, the limitations of the study and
also the possible lessons for institutions who aim to measure student engagement in
all of its dimensions. The ﬁnal section summarizes the main ﬁndings and their
importance and proposes future directions and improvements.
2
State of the Art
The act of engagement is deﬁned in terms of action, or the behavioural, emotional,
and cognitive manifestations of motivation (Skinner et al. 2009). It reﬂects an
individual’s interaction with the context (Fredricks et al. 2004; Russell et al. 2005).
Thus, the individual and the act of engagement cannot be separated from the
environment where it takes place (Fredricks and McColskey 2012). These basic
deﬁnitions help us bridge the concept to student engagement in a learning envi-
ronment. Student engagement has been deﬁned as an aggregate of behavioural,
cognitive and emotional engagements (Finn 1989; Connell and Wellborn 1991;
Engagement Analytics: A Microlevel Approach …
49

Jimerson et al. 2003; Fredricks et al. 2004; Wigﬁeld et al. 2008; Archaumbault
2009; Ryu and Lombardi 2015).
Behavioural engagement is based on the idea of overall participation, such as
involvement in academic, social or extracurricular activities. In the academic
classiﬁcation, behavioural traits such as effort, concentration, persistence, attention,
raising questions, class participation, on-task behaviour and participation in dis-
cussions are considered relevant. Furthermore in the social classiﬁcation adhering to
school regulations, abstaining from disruptive behaviours such as skipping classes
are considered. In the extracurricular classiﬁcation participation in sports or other
governance related posts, societies and clubs are present (Kong et al. 2003; Fre-
dricks et al. 2004). Behavioural engagement is considered crucial for positive
academic outcomes (Marks 2000; Fredricks et al. 2004) as it allows students to
participate more with their own learning and the learning environment and leads to
increased completion rates and reduced drop-out rates (Voelkl 2012).
The deﬁnitions for cognitive engagement draws on ideas of psychological
investment (Newmann et al. 1992), self-regulation, or being strategic (Fredricks
et al. 2004). Furthermore the ideas of students using mental faculties in order to
understand concepts, solve problems and complete challenging tasks to master
difﬁcult skills (Wehlage and Smith 1992) are central in the cognitive engagement
deﬁnition. Compared to behavioural engagement, cognitive tasks are more internal
and less observable (Appleton et al. 2006). Cognitive engagement has also been
shown to directly inﬂuence and predict achievement (Greene et al. 2004). To
measure cognitive engagement, students are asked how they set goals, organize
study efforts, concentrate and complete work effectively and how they monitor their
cognition (Fredricks et al. 2004).
Emotional engagement encompasses positive and negative emotions towards
students’ own learning, teachers, classmates, academics, and school. Identiﬁcation
with the school and the learning environment is a common emotion found in many
deﬁnitions of emotional engagement which leads to a sense of belonging in the
student and is presumed to create ties to an institution and inﬂuence willingness to
do the work (Connell and Wellborn 1991). Positive or negative emotions such as
interest, boredom, excitement, and anxiety can play a crucial part in the engagement
of a student and as is the case with cognitive engagement it may not be clearly
observable in a learning space, particularly in an online environment; therefore it is
important to have e-learning systems that can capture emotions from computer
interactions (Feidakis et al. 2012).
The combination of behaviour, cognition and emotion, under the main concept
of engagement, provides a richer characterization than what is possible through
single components. In reality these factors do not act in isolation and have a
dynamic interrelationship within the individual as antecedents and consequences,
with additive or interactive effects. Therefore deﬁning and examining the separate
engagement components undermines their holistic meaning and value (Fredricks
et al. 2004) and must be measured simultaneously and dynamically.
50
I. Balasooriya et al.

Given the link between engagement and academic success and the central role of
STEM courses in a range of ﬁelds, it is crucial that we understand how to improve
engagement in STEM courses. STEM education plays an important role in the
secondary and higher education levels in order to develop higher order skills in the
growing technological careers. The new globalized and fast-changing economy
requires knowledgeable workers who can integrate and evaluate new information,
think critically, and solve problems (Fredricks et al. 2004). Even programming
education, a relatively new ﬁeld of science is identiﬁed as a new form of literacy
(Prensky 2008) in the modern world, as important as reading and writing in the
century before. Although the values and stakes of this particular ﬁeld are high, the
dropout rates are higher and engagement and grades are lower in STEM classrooms
than in other ﬁelds. These trends are alarming and still persist today (Berglund and
Eckerdal 2015).
Moreover, different student engagement measurement instruments were found to
misrepresent student engagement levels in computer science (Sinclair et al. 2015),
where some measures in these instruments could not accurately capture the STEM
related aspects in computer science education. This raises a serious question of
validity in measuring and understanding student engagement in different settings.
STEM subjects are characterized by learning by problems and project based
exercises (Carter 2013), the need for extensive and systematic feedback (Froyd
2008), application of knowledge in in the real world (Breiner et al. 2012), and
reasoning and problem-solving skills (Smith et al. 2009). Considered in an online
environment, these traditional challenges of STEM become even more serious. As
student engagement is a well-researched and known contributor to academic
achievement, it is a timely move to address online STEM education and particularly
measure and understand the low engagement levels and prevent dropout by
implementing remedies.
2.1
Granularity of Measurement
Student engagement can be conceptualized, observed, and measured at different
granularity levels (Sinatra et al. 2015). They range from basic activity levels or
microlevel to a broader institutional level or macrolevel (Skinner and Pitzer 2012;
Henrie et al. 2015; Sinatra et al. 2015). Contemporary student engagement mea-
surement surveys such as National Survey of Student Engagement (NSSE) have
measures for campus environment in terms of quality of interactions and supportive
environment to overall educational experience (Sinclair et al. 2015). Meanwhile at
the microlevel, much more ﬁne-tuned data can be captured such as situational
engagement with learning resources or assessments, emotional or cognitive
engagement with a task at hand: before, during and at the conclusion of the task.
Therefore it is possible to capture a variety of data during a learning period, whether
it be a semester or an hour. Situational engagement is another term that describes a
microlevel of learning that combines situational interest as a background
Engagement Analytics: A Microlevel Approach …
51

psychological process that energizes and directs student learning (Ainley 2012).
Many contemporary student engagement surveys tend to focus on a macrolevel
approach studying the overall effect of learning in a semester or a year through a
survey administered at the end of the period. However at the macrolevel of student
engagement measurement, situational engagement is merely inferred in retrospect
or altogether lost in the macrolevel context.
In our research we look at how this microlevel and situational student engage-
ment informs better on how the student is performing and how this data could be
used to produce timely information that is valuable for both the student and the
teacher.
2.2
Student Engagement Instruments
There are extensive resources and research regarding student engagement instru-
ments (Fredricks and McColskey 2012) to be found in the literature. We present a
summary of key instruments. While some of these instruments are designed for
measuring engagement speciﬁcally, some are based on learning strategies, moti-
vation and other learning techniques and processes that lead to engagement.
A common aspect of these instruments is that they focus on retrospective collection
of engagement data after a semester or a year of learning is concluded (Olejnik and
Nist 1992; Pintrich et al. 1993; Crick et al. 2004; Kuh 2009). They also cover a
larger scope of engagement on a macrolevel, including the process of learning itself
as well as extracurricular activities, relationships with the school or the university
and its services etc. In order to capture such a variety of data, these instruments are
usually lengthy, as long as 70–100 questions per student. Particularly engagement
survey instruments clearly distinguish between the behavioural, cognitive and
emotional engagements and have questions aimed at each dimensions in the survey.
The most widely known instruments are the NSSE (Kuh 2009), Motivated
Strategies for Learning Questionnaire/MSLQ (Pintrich and De Groot 1990) and
Effective Lifelong Learning Inventory/ELLI (Crick et al. 2004). There have also
been proposals for subject speciﬁc engagement instruments such as in Mathematics
(Kong et al. 2003) or in reading (Wigﬁeld et al. 2008). However, none of these
instruments offers simultaneous data collection at a microlevel addressing the
cognitive and emotional aspects of engagement, as our proposed instrument does.
The aforementioned NSSE is an annual survey of undergraduate students that
measures students’ participation in educationally purposeful activities primarily in
the North American continent that has been shown to be linked with desirable
outcomes of college (Pascarella and Terenzini 2005). The instrument is structured
in the areas of Student behaviours, Institutional actions and requirements, Student
perceptions, Student background information and Student learning and develop-
ment (Kuh 2009) and therefore takes the macrolevel of student engagement into
account.
52
I. Balasooriya et al.

MSLQ on the other hand is a 44 item survey aimed at measuring motivational
orientation and the use of different learning strategies. Motivation is often seen as
an internal affective force that energizes engagement (Boekaerts 2016) and is
viewed as an essential source of engagement. Similarly ELLI is a lifelong learning
strategy survey with 75 items aimed at measuring learning capacities and learning
identity among other aspects. While the main goals of these surveys differ from
student engagement at its core, their components overlap with the dimensions of
student engagement.
2.3
Self-Reported Data
Self-disclosed metadata is a common data collection mechanism in student
engagement research. This sort of metadata usually reveals a higher order of
information that is not directly observable and is harder to infer from system-level
log data (Shum and Crick 2012; Fredricks and McColskey 2012) especially when it
comes to the engagement dimensions such as cognition and emotions. External
measurements of behaviours or artefacts cannot be considered a valid instrument
when it comes to how a student thinks or feels, therefore self-disclosed student data
becomes an essential component to the overall engagement. It is critical to capture
student’s subjective perceptions instead of simply capturing objective data on
behavioural indicators such as attendance or homework completion rates which are
common in many settings (Appleton et al. 2006; Garcia and Pintrich 1996).
Self-report measures capture student values, attitudes and dispositions (Shum and
Crick 2012) at a level where the engagement measurement does not disrupt the
learning environment and are the most common method for assessing student
engagement. The majority of these engagement measures are general, though there
are some subject speciﬁc measures in domains such as Math (Kong et al. 2003) or
reading (Wigﬁeld et al. 2008).
Much self-report research has the disadvantage of being conducted after an
experience has concluded, rather than in real time. Our approach solves this
problem by incorporating the Experience Sampling Method (ESM). The ESM is a
procedure that consists of asking individuals to “provide systematic self-reports at
random occasions during the waking hours of a normal week” (Larson and
Csikszentmihalyi 2014). Queries such as if they are enjoying what they are doing,
how hard they are concentrating, whether it is interesting (Shernoff et al. 2016),
how important the activity is to them, how active they are during the activity
(Salmela-Aro et al. 2016) provide microlevel insights into the respondent’s expe-
rience of the task at hand. The ESM improves on traditional self-report research
because it collects the data in real-time, as respondents are having experiences.
We adapt the ESM approach to an online learning environment to collect
self-reported data at intervals. This approach allows us to measure engagement in
learning at a microlevel.
Engagement Analytics: A Microlevel Approach …
53

3
Towards a Microlevel Approach
While the retrospective student engagement surveys such as the aforementioned
NSSE, MSLQ have a larger scope and extensive in their measurement of
engagement, due to the retrospective nature of data collection, much of the
day-to-day dynamics of engagement becomes lost since the data is captured a long
after the learning activities have been concluded. Goetz et al. (2013) states that
retrospective judgments strongly and systematically differ from real-time experi-
ences. Different research projects in LA use behavioural student engagement data
extracted from system-level logs as a representation of student engagement (Liu
et al. 2015; Pardo et al. 2017). When it comes to cognitive or emotional engage-
ment data, they are not easily inferred from system-level logs (Shum and Crick
2012).
Timely data provides a diagnostic ﬁne tuning of students’ learning (Coates 2005)
and particularly Programming courses generate a large amount of data produced by
learner activity that allows greater analytical capability. This is a situation where the
microlevel data has the ability to magnify the view of ongoing learner engagement
from the overall macrolevel context and has the potential to inform on the short-
comings of student learning and engagement. While behavioural engagement is
preserved in course statistics, a student’s degree of cognitive and emotional
engagement becomes invisible as soon as a learning activity concludes. For
example the cognitive efforts during watching a video material, emotional effect
leading up to an assignment are best captured timely and in context. Therefore
unless strategies are in place for them to be captured, an important set of data is
missing from the engagement dataset.
Taking into account the challenges of STEM education and the high dropout
rates and low grades as discussed earlier, it has become a critical requirement to
assist students to engage in learning by having timely information about their
behaviours, cognitions and emotions. Here, the retrospective engagement mea-
surement approach and the behavioural-only engagement inference approach both
have their limitations where dynamic subject areas like STEM and online learning
environments are concerned.
Therefore in this chapter we emphasize the measurement of timely data in the
form of Microlevel Student Engagement (MSE) and how it can be implemented as
an extension of the traditional macrolevel model of student engagement measure-
ment. In the increasingly online educational models of the present, we also explore
MSE as an enabler for timely and accurate decision-making particularly in online
STEM education for both teachers and students, in order to better understand
learners and their characteristics such as behaviours, cognitive patterns, emotional
states that lead to academic success and also enable richer feedback from teachers
and informed decision-making by the institution. In order to design our data capture
process, we explored online learning activities that are relevant for capturing stu-
dent engagement and which MSE (behavioural, cognitive and emotional) data
54
I. Balasooriya et al.

should be associated with the identiﬁed activities. In doing so, we have imple-
mented a preliminary version of MSE Analytics as an extension of LA for the
teacher, the student and the academic institution.
In a typical course structure in a Virtual Learning Environment (VLE), a course
contains components such as virtual classrooms, a teaching plan, learning materials
(text, videos and graphics), integrated learning tools, assessment activities and
communication channels such as notice boards and student forums. The VLE itself
is a much broader entity that encompasses other services such as virtual library
facilities or channels that connect the student and the institution. But from an
academic learning point of view we focus on student engagement inside a course.
Each of these components as well as any other additional component in a virtual
classroom produces and logs a variety of system-level data related to learner
activity on a daily basis. This data can be navigational data (logging student nav-
igation paths and times inside the virtual classroom), academic data (academic
grades, completed lessons etc.) among others. However the cognitive and emotional
engagement data that can be captured during these activities at a microlevel have to
be identiﬁed separately. In order to do that we deﬁne short questions in a 1-5 Likert
scale aimed at collecting data on cognitive and emotional engagement based on the
learning activities of the learning environment. They are deﬁned according to each
speciﬁc task such as reading a reference text, submitting an assessment task,
watching a video or reading the objectives of a lesson.
Cognitive MSE refers to whether the user ﬁnds the learning material/assignment
descriptions to be comprehensible, the user ﬁnds the assignment/learning activity
easy to complete, the user ﬁnds the learning material relevant to the activity, the
user ﬁnds the number of examples adequate etc. Furthermore the short question-
naires placed at the end of each assessment activity provide cognitive engagement
data related to each assessment activity. Similarly, emotional MSE is the affective
nature of learning and how the student feels during the time spent on learning. This
set of data includes whether the user ﬁnds the learning material interesting,
instructions or resources helpful, the user feels the teacher/instructor’s involvement
is satisfactory, the user ﬁnds the lesson valuable to the overall goals. The short
questionnaires at the end of assessment activities also contribute to the emotional
engagement dataset. We primarily focus on the cognitive and emotional aspects of
MSE in the templates since behavioural engagement is reﬂected in the system-level
data capture, such as resource access (video play, document view/download, tab
click, and page load) counts, navigation in the learning space etc. Cognitive and
emotional aspects of engagement are particularly relevant because in practice there
is an overemphasis on behavioural engagement (Appleton et al. 2006). Compared
to traditional engagement instruments, this data is timely, and focused on a
microlevel of learning activity, which allows for deeper knowledge of how students
are engaged, and if they are not, pinpoint the investigation into clearly identiﬁed
points of the learning process.
These questions in the form of micro-surveys are formatted as templates that can
be incorporated into different sections of a lesson. In order to prompt these
Engagement Analytics: A Microlevel Approach …
55

micro-surveys that contain 2 or 3 engagement questions, we have developed a MSE
data capture module that has been integrated to the learning environment.
This MSE data capture module retrieves question templates that are stored in a
server and are loaded to the interface of the module when a student accesses it. We
implemented this module to have a feedback button to be visible to the student next
to a learning resource such as a table of contents, a video, an algorithm tab so that
students are able to take their time to click on the module and leave feedback. The
module provides a 5 point scale for the students to ﬁll in their self-reported
engagement data.
Figure 1 presents our proposal for an MSE data capture and analysis model
within a VLE. We illustrate a VLE setting where system-level log data is already
being captured and stored in a database as a complementary dataset which is
common in many educational institutes. Our design is aimed at the MSE analytics
module to be a plug-in to such a system, where student engagement data can be
captured directly from the students and integrated seamlessly with the system-level
dataset for analysis. While the traditional analysis of student data falls under LA, we
propose that our model could form the basis of branch of LA entitled MSE
Analytics.
Fig. 1 MSE data capture design using question templates that allows self-reported engagement
data to be submitted by the students
56
I. Balasooriya et al.

4
A Case Study in Programming
With our focus on online engagement in STEM subjects, Universitat Oberta de
Catalunya (UOC), an established Internet-based open university in Barcelona,
Spain has become a highly favourable location for this research where a large body
of students exist and all student learning and assessment is done online. Our
research approach is based on action research (Susman and Evered 1978) in order to
test our design in such an actual learning environment and iteratively improve the
design based on the results. The UOC produces a large amount data from the VLE
activity logs and already has an archival system in place and they can be accessed
through a central data-mart. Our pilot study of engagement data collection was
based on Fundamentals of Programming, a mandatory 6 ECTS credit course in the
ﬁrst academic year offered to both the Telecommunication Engineering and
Computing Engineering degrees at UOC. The course is taken by 200 to 300 stu-
dents in a semester and they are divided into several virtual classrooms in the VLE.
The student self-reported data collection is an anonymized process supported by the
e-Learn centre at UOC, therefore no special permissions were required from the
students in order to capture the data and it was also an optional process for the
students to provide self-reported engagement data.
The Fundamentals of Programming course uses a Wiki platform (xWiki) to
deliver learning materials to the students as well as an experimental program
submission system to deliver assessments and test programs. UOC data-mart pro-
vides general demographic data and additional behavioural data such as whether the
user has been assigned to subject/instructor, the user accesses the virtual classroom,
the user views the teaching plan, the user accesses a learning tool, the user accesses
the virtual library materials and the user interacts in the forum. In the ﬁrst iteration
we have aggregated three sources of data: demographics and behavioural engage-
ment data from UOC data-mart; behavioural engagement data from the xWiki; and
the MSE data from our data-capture module in order to assess the capabilities of
MSE Analytics.
Our data analysis focused mainly on the capabilities of the MSE data received
from 50 students, in order for its ability to answer questions such as levels of
average student engagement during the semester, components of the course (videos,
examples, and code snippets) that students engage with most and least, which of
those components involve the most cognitive engagement or the most emotional
engagement. During the ﬁrst few weeks of the semester the following were some of
the results obtained by analysing the data.
4.1
Measuring Cognitive Engagement
As an aspect of cognitive engagement, our data collection focused on the cognitive
effort for different types of resources in the xWiki. The data capture module
Engagement Analytics: A Microlevel Approach …
57

prompted questions regarding how easy it was to understand the various resources
such as algorithms, examples and summaries etc. They answered in the scale of 1 to
5, 1 being very hard to understand and 5 being very easy to understand. This is a
more microlevel view from the lesson level. We have calculated an average cog-
nitive effort value based on all the feedback that each resource type has received
and it shows algorithms, examples and videos being better enablers of under-
standing programming concepts (easy to understand), and that students have dif-
ﬁculty understanding tables and code snippets. Figure 2 visualizes the available
types of resources in terms of their cognitive effect to students.
MSE analysis also informed on the cognitive efforts of students in different
lessons. This is a step forward in terms of timeliness and in validity than inferring
from a list of grades or the end of the semester feedback. The students reported data
for questions such as how easy it was to understand individual algorithms, code
snippets, images or overall lesson etc. This microlevel cognitive engagement data
was used to calculate an average cognitive value for each lesson as illustrated in
Fig. 3. The value 5 indicates low cognitive effort (lessons easier to understand) as
reported by students and value 1 indicates higher cognitive effort (hard to under-
stand) based on the aggregation of cognitive components of the questions.
Comparatively, students have reported lower cognitive effort (easy to under-
stand) in lessons such as Abstract Data Types and Strings, whereas higher cognitive
effort has been reported in lessons such as Modularity and Pointers. Given the
information in a timely manner, it is a crucial input for the teachers to navigate
through the more difﬁcult lessons with a better understanding of the audience.
Fig. 2 Average cognitive engagement effort by resource type
58
I. Balasooriya et al.

4.2
Measuring Emotional Engagement
The microlevel data that we collected from the students also focused on emotional
engagement on certain elements of the course. While cognitive engagement was
based on the cognitive effort needed to engage with the resources and understand
them, emotional engagement often dealt with their usefulness to the students and
how they made them feel in some cases.
One of the aspects of cognitive engagement we have measured through the
self-reported data is the student interest levels in the lessons. Integrating the data
capture module in each of the table of contents of each lesson in the xWiki, we
obtained feedback for the students’ interest (an emotional engagement) in the les-
sons. The average values of interest for each lesson shows a variation that is timely
and is of importance to the teacher in terms of the emotional level in the classroom
regarding the lessons. Figure 4 represents a plot derived from this data, where for
Fig. 3 Average cognitive engagement levels by lesson
Fig. 4 Average student interest levels in lessons
Engagement Analytics: A Microlevel Approach …
59

example the lessons titled ‘Modularity in Codelite’ and ‘Tuples’ show lower levels
of interest from students and could help the teacher deliver more motivation and
cultivate interest.
Figure 5 shows emotional levels reported by students related to speciﬁc types of
resources or characteristics of a lesson. The ﬁndings show that the study guide has
impacted very positively on an emotional level (helpful), as have the summary
documents provided towards the end of the semester as a summarization of the
main topics. Lesson content refers to the interest shown to lesson contents, which
has also shown positive levels. The lower emotional values reported were regarding
the satisfaction towards the number of examples provided in lessons, PDF docu-
ments that contain theoretical text and the satisfaction towards the number of
resources that were available for the students to make use of.
Similar to the cognitive engagement measurement, the students reported data for
questions such as how interesting the lessons were, if they were satisﬁed with the
number of examples in each lesson etc. These microlevel factors such as interest,
satisfaction were used to calculate an average emotional engagement value for each
lesson as illustrated in Fig. 6. However in this case value 5 indicates high emotional
engagement as reported by students and value 1 indicates low emotional engage-
ment based on the aggregation of the emotional components of the questions.
It can be seen that lessons such as Pointers, Changing Data Types, Tables and
Tuples have scored relatively low emotional levels from the students. Coupled with
the cognitive engagement counterpart of each lesson, it can be particularly useful
for the teachers to approach certain lessons (for example Pointers lesson has scored
low on both cognitive and emotional responses) with more consideration towards
the students’ learning capabilities.
Fig. 5 Average emotional engagement levels by resource type
60
I. Balasooriya et al.

5
Discussion, Limitations and Future Work
Our results have allowed a closer view at the daily engagement of students in their
learning space. In terms of analysis capability and decision making, ﬁndings such as
interest levels in lessons (Fig. 4), engagement levels with different lessons (Figs. 3
and 6) and engagement levels with different resource types (Figs. 2 and 5) give a
teacher capability to address drawbacks in activity design or content creation.
Unlike in a retrospective dataset collected at the end of a semester, this information
is actionable and timely, providing not only an overview of student engagement
patterns but also opportunities of course correction and to better assist students
more in lessons with low engagement and/or poor performance. Furthermore, the
aggregation of behavioural engagement data from the UOC data-mart and the
xWiki has provided us data that has the potential to discover the navigational
behaviours plotted against course milestones, comparison of access patterns to
active engagement as well as potential analyses on engagement levels by degree
program (computer science, telecommunications), by gender, by academic
achievement (assessment grades, ﬁnal grades). The addition of microlevel cognitive
and emotional data can also further our understanding of the behavioural engage-
ment of students and vice versa.
In our ﬁrst iteration of the proposed approach, we have focused only on the
content management system: xWiki space that contained learning materials and the
MSE data from the xWiki due to the openness and the ﬂexibility with the data
capture module integration. However in order to integrate VLEs, and especially
closed-source VLEs more work has to be carried out. In the future iterations we
plan to integrate the UOC VLE as a point of MSE data capture as well. Further-
more, in the subsequent iterations once the programming and submission tool that is
Fig. 6 Average emotional engagement levels by lesson
Engagement Analytics: A Microlevel Approach …
61

used in the Fundamentals of Programming course can be integrated into the data
collection process, a comprehensive dataset that adds student engagements in
programming practice sessions can be incorporated. Particularly in programming
courses these continued practice tasks and the engagement data they provide such
as lines of code written, number of successful compilations etc. would further be
useful. Also apart from programming, further cases have to be tested using more
subjects within the STEM area. One of the challenges in our approach in reducing
the engagement measurement scope into a microlevel is the level of intrusiveness.
Our instrument design has allowed us to distribute the engagement data capture
across the entire semester and for individual learning tasks but having to reduce
some aspects of student engagement measurement such as students’ behavioural,
cognitive and emotional characteristics. We have also had to limit collecting
emotional engagement data at certain points in a lesson to reduce the number of
engagement questions and lower the intrusiveness of the instrument. In this sense,
our approach does not replace the existing methods of engagement measurement
but rather extends them and enhances them.
MSE analysis therefore provides a richer insight into the learning process of
students and the dynamic interrelationship between the three engagement compo-
nents. It also enables extensive and compounded comparisons and visualizations
such as navigational patterns with respect to emotional patterns, cognitive patterns
throughout a semester leading to assessment activities, clusters of behavioural,
cognitive or emotional traits that align with ﬁnal success etc. When channelled
through feedback mechanisms such as dashboard applications, recommender sys-
tems and prediction systems, students as well as teachers and the institutions can
utilize this information with regards to learning, teaching and course design.
Compared with the traditional approach of retrospective engagement surveys, the
MSE approach allows timely data to be gathered and analysed with more precision.
Also the engagement data is not only behaviour-based but also complimented by
the cognitive and emotional aspects that allow for a wider range of analyses. In
recent years a number of success-prediction systems and at-risk prediction systems
have been implemented that utilize the potential of learner data and data mining
techniques. Purdue Course Signals (Arnold and Pistilli 2012) and OUAnalyse
(Kuzilek et al. 2015) along with other systems for recommendations and learning
dashboards are based on behavioural engagement data. The integration of the three
aspects of engagement has the potential to be insightful and potentially ﬁne-tune the
prediction methods.
6
Conclusions
Student engagement has become an umbrella term to deﬁne various levels of stu-
dent interaction with the learning environment. A range of literature provides
evidence to the importance of student engagement to successful academic
achievement. In a challenging yet highly in-demand subject area such as STEM, it
62
I. Balasooriya et al.

has become important to assist students and design better course modules based on
richer information. While we intended the major application of the MSE approach
for online STEM education, it has potential to be used in any other online learning
environment regardless of the ﬁeld. Many engagement studies and instruments
focus on different levels of engagement using self-report measures from macrolevel
to microlevel in order to capture the three dimensions of student engagement. In the
trending mode of online learning, we have carried out a MSE data capture and
analysis from online students in order to gain timely data that provides extensive
insights into the learning process rather than in a retrospective way. By combining
the system level data capture already in place with the proposed multi-dimensional
engagement data we are able to derive valuable information such as engagement
levels by resource types and lessons, cognitive engagement patterns, emotional
states and behaviours that helps achieve academic success that could enhance the
learning experience of students and also ensure informed and timely decision
making by teachers and institutions.
References
Ainley, M. (2012). Students interest and engagement in classroom activities’ in Handbook of
research on student engagement (pp. 283–302). US: Springer.
Alexander, K. L., Entwisle, D. R., & Horsey, C. S. (1997). From ﬁrst grade forward: Early
foundations of high school dropout. Sociology of Education, 87–107.
Appleton, J. J., Christenson, S. L., Kim, D., & Reschly, A. L. (2006). Measuring cognitive and
psychological engagement: Validation of the student engagement instrument. Journal of
School Psychology, 44(5), 427–445.
Archambault, I. (2009). Adolescent behavioral, affective, and cognitive engagement in school:
Relation to dropout. Journal of School Health, 79, 408–415.
Arnold, K. E., Pistilli, M. D. (2012). Course signals at purdue: using learning analytics to increase
student success. In Proceedings of the 2nd International Conference on Learning Analytics
and Knowledge (pp. 267–270).
Beer, C., Clark, K., & Jones, D. (2010). Indicators of engagement. In ASCILITE-Australian
Society for Computers in Learning in Tertiary Education Annual Conference (pp. 75–86).
Berglund, A., Eckerdal, A. (2015). Learning practice and theory in programming education:
Students’ lived experience, learning and teaching in computing and engineering (LaTiCE),
2015 International Conference (pp. 180–186).
Boekaerts, M. (2016). Engagement as an inherent aspect of the learning process. Learning and
Instruction, 43, 76–83.
Breiner, J. M., Harkness, S. S., Johnson, C. C., & Koehler, C. M. (2012). What is STEM? A
discussion about conceptions of STEM in education and partnerships. School Science and
Mathematics, 112(1), 3–11.
Carter, V. R. (2013). Deﬁning Characteristics of an Integrated STEM Curriculum in K–12
Education, University of Arkansas.
Coates, H. (2005). The value of student engagement for higher education quality assurance.
Quality in Higher Education, 11(1), 25–36.
Connell, J., & Wellborn, J. G. (1991). Competence, autonomy, and relatedness: A motivational
analysis of self-system process. In M. R. Gunnar & L. A. Sroufe (Eds.), Self process in
development: Minnesota Symposium on Child Psychology, 2 (pp. 167–216). Hillsdale, NJ:
Lawrence Erlbaum.
Engagement Analytics: A Microlevel Approach …
63

Crick, R. D., Broadfoot, P., & Claxton, G. (2004). Developing an effective lifelong learning
inventory: The ELLI project. Assessment in Education: Principles, Policy & Practice, 11(3),
247–272.
Feidakis, M., Daradoumis, T., Caballé, S., & Conesa, J. (2012). Design of an emotion aware
e-learning system. International Journal of Knowledge and Learning, 8(3–4), 219–238.
Finn, J. D. (1989). Withdrawing from school. Review of Educational Research, 59(2), 117–142.
Fredricks, J. A., & McColskey, W. (2012). The measurement of student engagement: A
comparative analysis of various methods and student self-report instruments. Handbook of
research on student engagement (pp. 763–782). US: Springer.
Fredricks, J. A., Blumenfeld, P. C., & Paris, A. H. (2004). School engagement: Potential of the
concept, state of the evidence. Review of Educational Research, 74(1), 59–109.
Froyd, J. E. (2008). White paper on promising practices in undergraduate STEM education, The
National Academies Board on Science Education.
Garcia, T., & Pintrich, P. (1996). Assessing students’ motivation and learning strategies in the
classroom context: The motivation and strategies in learning questionnaire. In M. Birenbaum &
F. J. Dochy (Eds.), Alternatives in assessment of achievements, learning processes, and prior
knowledge (pp. 319–339). New York: Kluwer Academic/Plenum Press.
Glanville, J. L., & Wildhagen, T. (2007). The measurement of school engagement: Assessing
dimensionality and measurement invariance across race and ethnicity. Educational and
Psychological Measurement, 67(6), 1019–1041.
Goetz, T., Bieg, M., Lüdtke, O., Pekrun, R., & Hall, N. C. (2013). Do girls really experience more
anxiety in mathematics? Psychological Science, 24(10), 2079–2087.
Greene, B. A., Miller, R. B., Crowson, H. M., Duke, B. L., & Akey, K. L. (2004). Predicting high
school students’ cognitive engagement and achievement: Contributions of classroom
perceptions and motivation. Contemporary Educational Psychology, 29, 462–482.
Henrie, C. R., Halverson, L. R., & Graham, C. R. (2015). Measuring student engagement in
technology-mediated learning: A review. Computers & Education, 90, 36–53.
Hughes, J. N., Luo, W., Kwok, O. M., & Loyd, L. K. (2008). Teacher-student support, effortful
engagement, and achievement: A 3-year longitudinal study. Journal of Educational
Psychology, 100(1), 1–14.
Jimerson, S. R., Campos, E., & Greif, J. L. (2003). Toward an understanding of deﬁnitions and
measures of school engagement and related terms. The California School Psychologist, 8(1),
7–27.
Kelly, S. (2008). Race, social class, and student engagement in middle school English classrooms.
Social Science Research, 37(2), 434–448.
Kong, Q. P., Wong, N. Y., & Lam, C. C. (2003). Student engagement in mathematics:
Development of instrument and validation of construct. Mathematics Education Research
Journal, 15(1), 4–21.
Kuh, G. D., Kinzie, J., Buckley, J. A., Bridges, B. K., & Hayek, J. C. (2007). Piecing together the
student success puzzle: Research, propositions, and recommendations. ASHE Higher
Education Report, 32(5), 1–182.
Kuh, G. D. (2009). The national survey of student engagement: Conceptual and empirical
foundations. New directions for institutional research, 141, 5–20.
Kuzilek, J., Hlosta, M., Herrmannova, D., Zdrahal, Z. and Wolff, A. (2015). OU Analyse:
analysing at-risk students at The Open University, Learning Analytics Review, 1–16.
Ladd, G. W., & Dinella, L. M. (2009). Continuity and change in early school engagement:
Predictive of children’s achievement trajectories from ﬁrst to eighth grade? Journal of
Educational Psychology, 101(1), 190–206.
Larson, R., & Csikszentmihalyi, M. (2014). The experience sampling method in ﬂow and the
foundations of positive psychology (pp. 21–34). Dordrecht: Springer.
Liu, D. Y. T., Froissard, J. C., Richards, D., & Atif, A. (2015). An enhanced learning analytics
plugin for Moodle: student engagement and personalised intervention, Globally Connected,
Digitally Enabled. Proceedings Ascilite, pp. 180–189.
64
I. Balasooriya et al.

Marks, H. M. (2000). Student engagement in instructional activity: Patterns in the elementary,
middle, and high school years. American Educational Research Journal, 37(1), 153–184.
Newmann, F., Wehlage, G. G., & Lamborn, S. D. (1992). The signiﬁcance and sources of student
engagement. In F. Newmann (Ed.), Student engagement and achievement in American
secondary schools (pp. 11–39). New York: Teachers College Press.
Olejnik, S., & Nist, S. L. (1992). Identifying latent variables measured by the Learning and Study
Strategies Inventory (LASSI). The Journal of experimental education, 60(2), 151–159.
Pardo, A., Han, F., & Ellis, R. A. (2017). Combining university student self-regulated learning
indicators and engagement with online learning events to predict academic performance. IEEE
Transactions on Learning Technologies, 10(1), 82–92.
Pascarella, E. T., & Terenzini, P. T. (2005). How college affects students: A third decade of
research. San Francisco: Jossey-Bass.
Pintrich, P. R., & De Groot, E. V. (1990). Motivational and self-regulated learning components of
classroom academic performance. Journal of Educational Psychology, 82(1), 33–40.
Pintrich, P. R., Smith, D. A., García, T., & McKeachie, W. J. (1993). Reliability and predictive
validity of the motivated strategies for learning questionnaire (MSLQ). Educational and
Psychological Measurement, 53(3), 801–813.
Prensky, M. (2008) Programming is the New Literacy, Available at: http://www.edutopia.org/
literacy-computer-programming (Accessed: 18 June 2017)
Robins, A., Rountree, J., & Rountree, N. (2003). Learning and teaching programming: A review
and discussion. Computer Science Education, 13(2), 137–172.
Russell, V. J., Ainley, M., & Frydenberg, E. (2005). Student motivation and engagement.
Schooling Issues Digest, 2, 1–11.
Ryu, S., & Lombardi, D. (2015). Coding classroom interactions for collective and individual
engagement. Educational Psychologist, 50, 70–83.
Salmela-Aro, K., Moeller, J., Schneider, B., Spicer, J., & Lavonen, J. (2016). Integrating the light
and dark sides of student engagement using person-oriented and situation-speciﬁc approaches.
Learning and Instruction, 43, 61–70.
Shernoff, D. J. (2013). Optimal learning environments to promote student engagement. New York,
NY: Springer.
Shernoff, D. J., & Schmidt, J. A. (2008). Further evidence of an engagement–achievement paradox
among US high school students. Journal of Youth and Adolescence, 37(5), 564–580.
Shernoff, D. J., Kelly, S., Tonks, S. M., Anderson, B., Cavanagh, R. F., Sinha, S., et al. (2016).
Student engagement as a function of environmental complexity in high school classrooms.
Learning and Instruction, 43, 52–60.
Shum, S. B., & Crick, R. D. (2012). Learning dispositions and transferable competencies:
pedagogy, modelling and learning analytics. In Proceedings of the 2nd International
Conference on Learning Analytics and Knowledge. (pp. 92–101).
Sinatra, G. M., Heddy, B. C., & Lombardi, D. (2015). The challenges of deﬁning and measuring
student engagement in science. Educational Psychologist, 50(1), 1–13.
Sinclair, J., Butler, M., Morgan, M., & Kalvala, S. (2015). Measures of student engagement in
computer science. In Proceedings of the 2015 ACM Conference on Innovation and Technology
in Computer Science Education. (pp. 242–247).
Sirin, S. R., & Rogers-Sirin, L. (2004). Exploring school engagement of middle-class African
American adolescents. Youth & Society, 35(3), 323–340.
Skinner, E.A., Kindermann, T.A., Connell, J.P., Wellborn, J.G. (2009). Engagement and
disaffection as organizational constructs in the dynamics of motivational development.
Handbook of Motivation at School. (pp. 223–245).
Skinner, E. A., & Pitzer, J. R. (2012). Developmental dynamics of student engagement, coping,
and everyday resilience. Handbook of research on student engagement (pp. 21–44). US:
Springer.
Smith, K. A., Douglas, T. C., & Cox, M. F. (2009). Supportive teaching and learning strategies in
STEM education. New Directions for Teaching and Learning, 117, 19–32.
Engagement Analytics: A Microlevel Approach …
65

Susman, G.I., Evered, R.D. (1978). An assessment of the scientiﬁc merits of action research.
Administrative Science Quarterly. 582–603.
Thomas, L., Ratcliffe, M., Woodbury, J., & Jarman, E. (2002). Learning styles and performance in
the introductory programming sequence. ACM SIGCSE Bulletin, 34(1), 33–37.
Voelkl, K. E. (1997). Identiﬁcation with school. American Journal of Education, 294–318.
Voelkl, K. E. (2012). School Identiﬁcation. Handbook of research on student engagement
(pp. 193–218). US: Springer.
Wehlage, G.G., Smith, G.A. (1992). Building new programs for students at risk. Student
engagement and achievement in American secondary schools, pp. 92–118.
Wiedenbeck, S., Labelle, D., Kain, V.N. (2004). Factors affecting course outcomes in introductory
programming. 16th Annual Workshop of the Psychology of Programming Interest Group,
pp. 97–109.
Wigﬁeld, A., Guthrie, J. T., Perencevich, K. C., Taboada, A., Klauda, S. L., McRae, A., et al.
(2008). Role of reading engagement in mediating effects of reading comprehension instruction
on reading outcomes. Psychology in the Schools, 45(5), 432–445.
66
I. Balasooriya et al.

Learning Analytics in Mobile Applications
Based on Multimodal Interaction
José Miguel Mota, Iván Ruiz-Rube, Juan Manuel Dodero, Tatiana Person
and Inmaculada Arnedillo-Sánchez
Abstract One of the most valuable skills for teachers is the ability to produce their
own digital solutions, translating teaching concepts into end-user computer systems.
This often requires the involvement of computing specialists. As a result, the devel-
opment of educational programming environments remains a challenge. Learning
experiences based multimodal interaction applications (gesture interaction, voice
recognition or artiﬁcial vision) are becoming commonplace in education because
they motivate and involve students. This chapter analyses the state-of-the-art in LA
techniques and user-friendly authoring tools. It presents a tool to support the creation
of multimodal interactive applications equipped with non-intrusive monitoring and
analytics capabilities. This tool enables teachers with no programming skills to cre-
ate interactive LA-enriched learning scenarios. To this end, several components that
manage LA activities are included in the tool, they range from automatically cap-
turing users’ interaction with mobile applications, to querying data and retrieving
metrics, to visualising tables and charts.
Keywords
Learning analytics ⋅Mobile apps ⋅Visual programming language
Language learning ⋅Human-machine interaction
J. M. Mota (✉) ⋅I. Ruiz-Rube ⋅J. M. Dodero ⋅T. Person
University of Cádiz, Avenida de la Universidad de Cádiz, 10, Puerto Real, Cádiz, Spain
e-mail: josemiguel.mota@uca.es
I. Ruiz-Rube
e-mail: ivan.ruiz@uca.es
J. M. Dodero
e-mail: juanma.dodero@uca.es
T. Person
e-mail: tatiana.personmontero@uca.es
I. Arnedillo-Sánchez
School of Computer Science and Statistics, Trinity College, Dublin, Ireland
e-mail: macu.arnedillo@scss.tcd.ie
© Springer International Publishing AG 2018
S. Caballé and J. Conesa (eds.), Software Data Engineering for Network eLearning
Environments, Lecture Notes on Data Engineering and Communications
Technologies 11, https://doi.org/10.1007/978-3-319-68318-8_4
67

68
J. M. Mota et al.
1
Introduction
Our competitive society demands more eﬃcient and capable professionals, educated
with additional skills. One of the most valuable skills for professionals is the ability
to produce their own digital solutions, applications and products to solve common
issues in their speciﬁc disciplines. The rationale for this is to put technology at their
service rather than to be driven by it (Rushkoﬀ2010). The previous is of particularly
interesting in the ﬁeld of education, where translating teaching concepts and instru-
ments into end-user computer systems usually requires the involvement of computing
specialists. As a result, the development of education-oriented programming envi-
ronments remains a challenge.
Choosing an appropriate instructional design delivery format is important to stim-
ulate learner’s motivation (Rodgers and Withrow-Thorton 2005). A myriad of new
instructional environments based on multimodal interaction such as gesture, voice
recognition, augmented reality, tactile or artiﬁcial vision, are emerging as delivery
formats, already available for teachers. Learning experiences based on multimodal,
interactive applications are becoming increasingly common in all educational levels.
Arguably, it is easier to motivate and involve students when learning experiences
involves these types of interaction (Balderas et al. 2013; Di Serio et al. 2013; Jian
et al. 2015).
Multimodal interactive applications generate a large amount of students’ interac-
tion data that can provide insights about their proﬁle, behaviour and performance.
The design of assessments for multimodal interactive environments can be under-
taken applying Learning Analytics (LA) techniques, which are fostered by compu-
tational advances in big data and online learning environments (Ferguson 2012).
In this regard, the Society for Learning Analytics Research (SoLAR),1 deﬁne LA
as the measurement, collection, analysis and reporting of data about learners and
their contexts, for purposes of understanding and optimising learning and the envi-
ronments in which it occurs. LA is regularly utilised applied in Virtual Learning
Environments (VLE) to improve learning outcomes, learning processes and even to
predict students’ performance (Agudo-Peregrina et al. 2014; de-la Fuente-Valentín
et al. 2015). Furthermore, LA methods and techniques to collect and process interac-
tion data from students’ behaviour can inform the assessment and iterative design of
learning processes in multimodal interaction environments (Lindgren and Johnson-
Glenberg 2013). However, this information is not readily accessible to teachers who
don’t possess signiﬁcant computer skills. Thus, it is diﬃcult for teachers to integrate
interaction data from third-party applications.
This chapter provides an analysis of the state-of-the-art of LA techniques and
user-friendly authoring tools. It also present VEDILS (Mota et al. 2017), an author-
ing tool to support the creation of multimodal interactive applications equipped
with non-intrusive monitoring and analytic capabilities. VEDILS provides teachers
with no programming skills an easy-to-use software platform to create ubiquitous,
contextual and interactive LA-enriched learning scenarios.
1https://solaresearch.org/.

Learning Analytics in Mobile Applications Based on Multimodal Interaction
69
2
Background
Technology Enhanced Learning (TEL) has beneﬁted from advances in ubiquitous
and mobile technologies and LA to improve user’s experiences and satisfaction in
enriched, multimodal learning environments (NMC Horizon Report 2014). The fol-
lowing is an in-depth look at diﬀerent emerging technologies used in TEL’s research
area and discuss the diﬃculties of their use in mobile learning applications.
2.1
Multimodal Mobile Interaction
In recent years, the ﬁeld of Human-Machine Interaction (HMI) has experienced an
exponential growth of technologies. Many of these can be implemented in mobile
devices. HMI devices and multimodal interfaces have also proliferated (Turk 2014;
Shen and Pantic 2013; Cruz-Benito 2016) and have been used to improve the teach-
ing/learning process experiences through diﬀerent ways of interacting, as the follow-
ing:
∙Tactile interaction: In addition to common tactile user interface elements such
as buttons, text boxes and timepickers, more advanced interfaces which avail of
sensitive surfaces and haptic devices (Esteban et al. 2014) are now available.
∙Verbal interaction: Interaction through orders and auditory tracks (Bain et al.
2002) via voice recognition and synthesis technologies is becoming more and
more common in applications such as car navigation, and operating systems like
Apple Siri, Google Assistant and Microsoft Cortana (see Fig. 1f). These technolo-
gies are also used to improve software accessibility and provide conversational
user experiences in natural language.
∙Gestural interfaces: Human-computer interaction can be developed by using exter-
nal gadgets that recognise human gestures and movements (Wei et al. 2013).
For instance, Leap Motion (Fig. 1a), a small peripheral device that can track the
motion of hands and ﬁngers through two cameras and three infra-red LEDs for
light sources. Also Myo armband (Fig. 1b), a wearable gesture control device that
uses a set of electromyographic sensors to detect electrical activity in the fore-
arm muscles. These devices combined with a gyroscope, an accelerometer and a
magnetometer, can be used to recognise diverse hand and arm gestures.
∙Brain interfaces: The development of brain-computer interfaces based on
electroencephalography devices, such as the headsets manufactured by Emotiv
(Fig. 1c), provides a new range of applications to monitor emotions, track cogni-
tive performance, and even control objects through learning a set of mental activ-
ity patterns that can be trained and interpreted as mental commands (Stopczynski
et al. 2014).
∙Virtual and Augmented Reality: By taking advantage of image processing and
computer vision technologies, experiences based on Augmented Reality (AR) and
Virtual Reality (VR) can be developed for mobile devices. While VR (Fig. 1d)

70
J. M. Mota et al.
(a) Leap Motion
(b) Myo armband
(c) Emotiv+ headset
(d) Smartphone VR
(e) Smartphone AR
(f) Google Speak Now
Fig. 1
Multimodal interaction devices provide with new ways of human-machine interaction
consists on the immersion of the user in computer-generated world which replaces
the real one, AR (Fig. 1e) means the inclusion of virtual elements on actual views
of physical environments to create a mixed reality in real time (Milgram and
Kishino 1994).
2.2
Mobile Sensing
Mobile sensing is the use of sensors to acquire data from the environment (Guo
et al. 2014). The capability of mobile devices to generate very large amounts of data
has placed a particular interest in smartphones as information sources for analytic
purposes. While we use mobile and ubiquitous technologies for learning, leisure,
work or to perform an activity, data is being generated as a result of contextual actions
and interactions.
The context may be implicitly captured by the device sensors or explicitly by the
users themselves. For example, a prototype demonstrates indoor light scavenging as
a practical method to extend battery life on smartphones is presented in Ferreira et al.
(2016). In this work various data sources such as battery life, voltage and tempera-
ture are captured and used. Other studies look into the connection between individ-
uals’ social behaviour and their ﬁnancial status, network eﬀects in decision making,
and a novel intervention aimed at increasing physical activity in the subject popu-
lation (Aharony et al. 2011). Likewise, in Bhih et al. (2016) a comprehensive study

Learning Analytics in Mobile Applications Based on Multimodal Interaction
71
of smartphones usage in relation to user interactions, network traﬃc, energy drain,
demographics and geographic locations is presented.
The ingestion and exploration of data generated by smartphones and wearable
devices about users’ interactions and sensor events remains a challenge. To make it
easier, a number of mobile learning analytics tools can be used.
MyExperience gathers and stores data from mobile device sensors. It combines
sensing and self-report mechanisms to collect quantitative and qualitative data on
users’ behaviour, their attitudes and activities (Froehlich et al. 2007). MyExperience
has over 50 built-in sensors and others can be easily added through plug-ins. In addi-
tion, triggers and actions can be deﬁned by means of an XML-based scripting lan-
guage. Thus researchers can deﬁne actions to be launched when certain conditions
are met.
Sensus is another example of mobile sensing tool. It encompasses the data collec-
tion, anonymising and storage in a repository. Sensus can monitor a wide range of
sensors and data events. It is designed to interact with the user to ask for additional
information via scheduled or sensor-triggered surveys (Xiong et al. 2016). It also
provides external wearable and beacon-based sensors. A package for the R statistics
environment is provided in order to ingest, analyse, and visualise Sensus data.
AWARE is an open and extensible platform for context-aware mobile computing
research applications development. AWARE shifts the focus from software develop-
ment to quantitative and qualitative data analysis (Ferreira et al. 2015). It includes an
experience sampling sensor for displaying survey-like pop-up questions in the phone,
either as free text, single-choice or multiple-choice input options. Sensor data and
answers are time-stamped and sent to a central server. AWARE comes with a web
dashboard to manage ﬁeld studies and create visualisations of the retrieved data. It
also provides developers with an API to make richer context-aware applications for
end-users.
Funf, developed by MIT Media Labs, is an open-source sensing and data process-
ing framework for mobile devices. It provides a reusable set of functions enabling the
collection, upload and conﬁguration for a wide range of data types (Aharony et al.
2011). Funf journal is an Android application built using this framework, which
allows users to conﬁgure data collection parameters for over 30 diﬀerent built-in
data probes. The data is securely stored on the phone and it can be extracted by ﬁle
transfer of an SQLite ﬁle, or by setting up a remote server. It is used in conjunc-
tion with a desktop tool to explore the data with some ﬁxed visualisations. Funf in a
box is a service provided for automatically generated mobile sensing Android apps,
with no programming required. After deﬁning the sensors and the data collection
frequency, data is collected and stored in a Dropbox account.
Menthal Balance aims to bridge the gap between Psychology and Computing by
providing an end-user tool that collects and analyses mobile users’ data (Aharony
et al. 2016). The app tries to answer questions as “how much time do I really spend
with my phone?”, “how often do I interrupt myself, by engaging the phone?”, or
“which apps are most addictive for me?” Menthal tracks the mobile phone usage and
takes into account classical self-report psychological inventories (e.g. recording the
self-perceived anxiety or sociability). It does so by asking questions from abbreviated

72
J. M. Mota et al.
versions of personality assessment scales. Data is stored in a private server of their
authors.
ContextPhone is one of the ﬁrst prototyping platforms for developing mobile
applications that collect phone usage context data (Raento et al. 2005). Emphasises
context as an understandable resource to the user. Using application widgets, users
had control over the sensors data collection. ContextPhone is compose of four mod-
ules: 1. Sensors acquire context data from diﬀerent sources, 2. Communications con-
nect to external services via standard Internet protocols, 3. Customizable applica-
tions, such as ContextLogger, ContextContacts, and ContextMedia, can seamlessly
augment or replace built-in applications such as the Contacts and Recent Calls lists,
4. System services automatically launches background services, error logging and
recovery, and the Status display.
Context-Aware Machine learning Framework (CAMF) for the Android platform
that addresses these shortcomings as well as incorporating machine learning (Wan
2010). Combine machine learning and context-aware computing, to provide proac-
tive services based on the users’ usage patterns of the mobile device combined with
the environmental context of the user.
Dynamix (Carlson and Schrader 2012) is a plug-and-play context framework for
Android, which automatically discovers, downloads, and installs the plugins needed
for a given context sensing task. Dynamix is a standalone application that tries
to understand new environments using pluggable context discovery and reasoning
mechanisms.
Table 1 presents a summary of the mobile sensing tools, which have been analysed
according to the following criteria:
∙Stakeholders: target users of the tool, such as researchers, software developers,
and others.
∙Mobile platform: mobile operating system supported by the tool.
∙Data: available information channels, such as motion, environmental or position
sensors, implicit and explicit user actions and so forth.
∙Storage: location, type and structure of the data storage system.
∙Analytical features: such as statistics, visualisation, data mining and graph analy-
sis.
∙Personalisation mode: availability of an environment for customising data collec-
tion, processing and analytics.
∙Automation: how easy is to program behaviours or automated actions as a response
to contextual information changes.
2.3
Learning Analytics
LA aims to analyse learners’ data to understand and optimise learning and teaching
processes in real-time. LA loosely clusters various data gathering tools and analyti-
cal techniques to measure students’ engagement, performance, and practical progress

Learning Analytics in Mobile Applications Based on Multimodal Interaction
73
Table 1
Comparison between mobile sensing tools for analytic purposes
Tool
Stakeholders
Mobile platform
Data
Storage
Analytical
features
Personalisation
mode
Automation
MyExperiencea
Researchers
Windows mobile
Surveys, internal
sensors, images,
video and audio
Local SQL
Compact edition
database
No support
XML-based
scripting
language
Triggers use
sensor event data
to conditionally
launch a
predeﬁned set of
actions
Sensusb
Researchers
iOS and Android
Surveys, device
info, internal
sensors, external
wearable devices
JSON text ﬁles,
Amazon S3 cloud
storage
R statistics
package
In-app
conﬁguration
None
Menthalc
General users
Android
Surveys and
device info
Private data
server
Predeﬁned
visualisations
None
None
AWAREd
Developers and
researchers
iOS and Android
Surveys, device
info and internal
sensors
MySQL server
Web dashboard
with custom data
visualisations
API for iOS and
Android
Full automation
Funfe
Developers and
researchers
Android
Device info and
internal sensors
SQLite and
Dropbox
Desktop tool with
predeﬁned data
visualisations
In-app
conﬁguration
(Funf Journal) or
using the Android
API and a JSON
conﬁguration ﬁle
Full automation
(continued)

74
J. M. Mota et al.
Table 1
(continued)
Tool
Stakeholders
Mobile platform
Data
Storage
Analytical
features
Personalisation
mode
Automation
ContextPhonef
Developers and
researchers
Symbian OS
Internal sensors
Local ﬁle and
periodically
uploads to server
None
Depends on the
app
Full automation
CAMF
Developers and
researchers
Android
Internal and
external context
sources
SQL Lite
Weka
Generalized
interface
Context sources
can disable or
disable
Dynamixg
Developers
Android
Local Hardware
and Platform
APIs
Local cache and
public and/or
private
network-based
repositories
Local capability
to analyse
Context Plug-in
Description in
XML
Controlled and
customised using
its user interface
ahttp://myexperience.sourceforge.net/
bhttps://github.com/predictive-technology-laboratory/sensus/wiki
chttps://menthal.org/
dhttp://www.awareframework.com/
ehttp://funf.org/
f https://www.cs.helsinki.ﬁ/group/context/
ghttp://ambientdynamix.org/

Learning Analytics in Mobile Applications Based on Multimodal Interaction
75
Fig. 2
Learning analytics process
with the goal of knowing what is learned, in order to revise curricula, teaching prac-
tise, and assessment (The New Media Consortium (2011). Although the previous
is slightly diﬀerent from SoLAR’s deﬁnition of LA, they are both aligned with the
steps of the LA process (Chatti et al. 2012) (see Fig. 2), as follows:
∙Data collection and pre-processing: LA techniques rely on the availability of edu-
cational data. Often, the amount of data collected, as ingested from diﬀerent
sources, is very large and it comes in diﬀerent formats. Thus, a pre-processing
step is required to convert the data into a suitable format for LA methods.
∙Analytics and action: Analytical techniques explore data in order to discover hid-
den patterns or understand what happened during the learning process. During
this phase, monitoring, analysis, prediction, intervention, assessment, adaptation,
penalisation, recommendation and reﬂection can be implemented.
∙Post-processing: This phase comprises several actions for the continuous improve-
ment of the analytical process. For instance, compiling new data from additional
data sources, reﬁning the data set, determining new attributes and metrics, or
choosing a new analytic method.
2.3.1
Learning Data Sources
When classifying data sources used in LA, two types of systems can be distin-
guished: (1) centralised, such as Learning Management Systems (LMS); and (2)
decentralised, such as PLE. In centralised systems, the analysis is based on the logged
data collected from a single source. It may involve ﬁnding out when, how often and
from where have students logged in to the LMS, as well as which resources and
activities have they accessed or downloaded the most. Through the combination of
logged data and other students’ objective indicators, diverse skills or competences
can be assessed (Balderas et al. 2015).

76
J. M. Mota et al.
Despite the success of centralised LMS, they can be too rigid systems for many
students who might want to use applications of their choice to deploy their online
learning processes. To this end, students’ engagement with Web 2.0 environments in
Higher Education is in sharp contrast with their engagement with the LMS provided
by their institutions (Sclater 2008). PLEs are thought to build a learner-centred envi-
ronment that embeds every tool, service, content, evidence and person involved in
the online learning process. Due to PLE decentralised nature, a vast amount of data
from diﬀerent sources is generated, which requires additional steps of data gathering
and pre-processing prior to analysis.
Besides, data can also be classiﬁed in relation to where the learning process
actually takes place. In this regard, mobile learning or mLearning refers to
learning that occurs when learners have anywhere, anytime access to information
via mobile devices, in order to perform authentic learning activities in context
(Martin and Ertzberger 2013). Mobile devices have not only become powerful com-
puting tools, but they are also equipped with connectivity features and a wide range
of sensors such as accelerometers, GPS, light sensors, distance sensors, video cam-
eras, and microphones (Wan 2010).
2.3.2
Learning Analytics Speciﬁcations and Standards
There are two main speciﬁcations for managing interoperability in learning analytics:
(1) The Experience API or xAPI2; and (2) Caliper Analytics.3 The xAPI speciﬁca-
tion is hosted by the Advanced Distributed Learning (ADL) Initiative, an US gov-
ernment program that conducts research and development on distributed learning.
xAPI describes how to record, assess and perform analytics on e-learning experi-
ences that are deployed on a set of independent, distributed web applications. This
speciﬁcation enables tracking of activities from any LMS, learning app, simulation
or any web application where learning may happen. xAPI supports data capture as
activity streams on learners’ performance. The xAPI data are implemented as JSON
streams that are stored in Learning Record Stores (LRS) and manipulated by means
of ReST-based protocols.
Caliper Analytics is an alternative speciﬁcation managed by IMS Global, which
is the provider of well-known learning standards such as LTI. Caliper deﬁnes a basic
common language to describe students’ activity, by using the so-called metric pro-
ﬁles. Additionally, it supports the deﬁnition of standard learning events and the cap-
ture of metrics across learning environments by means of client implementations of
the Caliper Sensor API.
Both xAPI and Caliper Analytics rely on the presence of an LRS. These are data
storage systems that serve as repositories for learning records collected from systems
where learning activities take place. In addition to providing support for data stor-
age, LRS usually come with security mechanisms, connectors with common LMS,
2https://www.adlnet.gov/xAPI.
3https://www.imsglobal.org/activity/caliper.

Learning Analytics in Mobile Applications Based on Multimodal Interaction
77
reporting capabilities, and built-in dashboards. Despite the fact that both xAPI and
Caliper pursue the same goal, they present some diﬀerences. These are mainly con-
cerned with licensing, governance, vocabulary development processes and data pro-
tection issues (Griﬃths and Hoel 2016). Whereas xAPI opted for an Apache License
and a community-driven approach to develop its vocabulary and xAPI statements,
Caliper Analytics decided for a more restrictive licensing option and it ships with a
set of predeﬁned metric proﬁles that cover the more common use cases.
2.4
Barriers for Creating Context-Aware Mobile Learning
Apps
Whereas the areas of educational data mining and LA have signiﬁcantly beneﬁted
from the ability to track data from individual students’ work within computer medi-
ated learning environments, a primary goal for multimodal LA is the ability to study
collaborative, real-world, non-computer mediated environments (Ochoa and Supe-
rior 2016). This research is focused on mobile environments, beyond traditional com-
puter mediated ones.
According to the SoLAR’s deﬁnition of LA, context is essential to properly under-
stand and optimise the learning environment. In this vein, could mobile sensing
frameworks and tools be applied to support LA To date, the literature concerned
with mobile sensing and LA is scarce. As a consequence, further research and appli-
cations in LA supported by mobile contexts are required.
Multimodal verbal, gestural or neural interfaces are not currently supported by
existing mobile sensing tools. As a consequence, further software developments are
required in order to conduct contextual analysis of these kinds. An additional short-
coming of current mobile sensing tools is their inability to analyse the context within
a virtual or augmented reality scenario. Thus, the applicability of LA in this type of
enriched apps is constrained.
Despite the fact that mobile sensing tools can support researchers to analyse users’
interactions, the use of these tools to analyse students’ activities remains a challenge
for teachers. We suggest two reasons for this: ﬁrst, the lack of a proper semantics
that is familiar to educators to describe events captured by the tools; and second, the
lack of comprehensive and easy-to-use mechanisms for programming actions on the
apps themselves which depend on contextual information. For example, tools such
as AWARE or Funf provide full support for automation but they require considerable
programming skills. Other tools as MyExperience rely on easier scripting languages,
but only provide a limited set of predeﬁned actions.
A particularly valuable skill for educators is the ability to create their own
e-learning experiences (Santos et al. 2013). Doing so, educators are endowed with
agency over the technology to put it at their service rather than to be driven by it
(Rushkoﬀ2010). The full potential of technologies as LA and the use of HMI devices
for teaching is often underutilised. There is a substantial lack of speciﬁc educational

78
J. M. Mota et al.
tools that can be made available not only to researchers and developers, but also
to teachers (Romero 2010). The need of having programming skills to be able to
develop customised LA experiences is a major barrier for teachers to overcome this
challenge.
Consequently, developing apps for self-adaptive and context-aware educational
apps with LA support is often a task beyond teachers’ skills, regardless of whether
they have computer programming skills in languages such Java for Android or
Objective-C/Swift for iOS, for instance.
3
Designing Multimodal Learning Apps with Analytics
Support
This section describes an authoring tool to support the creation of multimodal inter-
active applications equipped with non-intrusive monitoring and analytical capabil-
ities. First, the method which underpins the tool and its features are described; and
then, the components to address the requirements of the LA strategies are presented.
3.1
Visual Environment for Designing Interactive Learning
Scenarios
The VEDILS framework is aimed at supporting users without programming skills
to create mobile applications. This framework comprises a method to design and
deploy learning activities and a authoring tool.
The method (Fig. 3) is characterised by an iterative methodology, the development
of onsite interventions, and the commitment with teaching and learning objectives.
The four stages of the method to design and deploy learning activities are: Design
and development of new components for the VEDILS tool; Training in the use of the
authoring tool; Iterative design of the mobile applications to be incorporated into the
educational activities; and Assessment of the students and the apps.
VEDILS provides teachers that are not skilled in computer programming with the
opportunity to create ubiquitous, contextual and interactive LA-enriched learning
scenarios. VEDILS tries to facilitate the inclusion of diverse computing technologies
in educational contexts. Instead of creating a new speciﬁc tool, which may limit the
choices for teachers who are experienced with programming, we have based our
approach on the MIT’s App Inventor environment,4 which is an easy-to-use online
tool for developing Android apps. This democratises mobile programming through
a block-based visual programming language called Blockly. This language has been
successfully used in other tools, such as Scratch,5 and it has proven to be suitable in
4http://appinventor.mit.edu/explore/.
5http://scratch.mit.edu.

Learning Analytics in Mobile Applications Based on Multimodal Interaction
79
Fig. 3
Method to design and deploy learning activities
diﬀerent successful experiences of teaching programming at early ages, like the One
Hour of Code.6 Consequently, the tool allows educators without prior programming
experience to easily develop mobile apps that can be used in their teaching/learning
processes (Hsu et al. 2012).
In addition to the features that are already available in App Inventor, VEDILS
provides a set of extensions to develop AR and VR scenarios that use diverse HMI
technologies for multimodal interactions, such as gestural and brain interfaces, as
well as the capabilities to support LA. Regarding its architecture, the platform has
several modules: an GWT application for designing the user interface of the new
apps, a JS Blocky editor for programming the mobile applications logic, a build
server to turn the interface design and logic into an exportable Android application
ﬁle (apk), and an interpreter that runs on the mobile device for debugging the apps.
Figure 4 shows the Design and Blocks views of VEDILS.
3.2
Supporting LA in VEDILS
Several components have been created to include LA in VEDILS. These allow the
development of apps that include objectives like monitoring, analysis, intervention,
assessment, feedback, adaptation, customisation and recommendation of learning
activities. An elaboration of the possibilities of these components for the develop-
ment of mobile learning activities with LA is provided below.
6http://code.org.

80
J. M. Mota et al.
(a) Screen to design the app and deﬁne the properties of the components
(b) Use of a visual language based in blocks to deﬁne the apps behaviour
Fig. 4
VEDILS editor

Learning Analytics in Mobile Applications Based on Multimodal Interaction
81
3.2.1
Event Data Sources
Applications developed with VEDILS allow gathering events from diﬀerent sources.
Some of them are collected from the App Inventor’s built-in components and others
from the components speciﬁcally developed to support HMI:
∙Tactile interaction: events triggered when learners interact with the application
using the common components in graphical user interfaces, such as pushing a
Button, writing in a Textbox, selecting a item in a ListPicker, etc.
∙Verbal interaction: events produced when the SpeechRecognizer component
detects a voice command of the user or when the TextToSpeech component syn-
thesises an audible message.
∙Gestural interfaces: events managed by the VEDILS’s components to capture
human gestures. The HandGestureSensor component recognises hand and ﬁn-
ger gestures through a Leap Motion device, whereas the ArmBandGestureSen-
sor component senses electrical activity in the forearm muscles through the Myo
ArmBand wearable.
∙Brain interfaces: the BrainWaveSensor component allows gathering data from the
brain sensors manufactured by Emotiv. These sensors measure neural impulses on
the head at the skin’s surface and generate the proper events. This is accomplished
by measuring the voltage of the impulses at diﬀerent locations on the scalp.
∙Virtual and Augmented Reality: the ARScene and VRScene components enable
us to create mixed or virtual environments. In the former, virtual objects, such
as AR3DModelAssets or ARImageAssets can be overlapped on the actual image
after recognising ARMarkerTrackers, ARImageTrackers or ARTextTrackers. In the
latter, VRImage360s, VRVideo360s or VR3DObjects can be rendered to provide
immersive experiences. These components were developed by using the Vuforia,7
jPCT-AE8 and Google Cardboard9 APIs. All of these components generate diﬀer-
ent events such as recognition of AR trackers or collisions between virtual objects,
among others.
∙Sensor data: AppInventor provides some built-in components to manage speciﬁc
sensor data, such as geo-position, orientation, accelerometer primitives, proximity
information and near-ﬁeld communications (NFC), among others.
3.2.2
Data Capture
VEDILS provides a component, ActivityTracker, which can gather all user’s interac-
tion data. Google Fusion Tables10 is used to provide cloud storage and basic support
for analytics.
7https://www.vuforia.com/.
8http://www.jpct.net/.
9https://vr.google.com/intl/en_en/cardboard/developers/.
10https://wwww.google.com/fusiontables.

82
J. M. Mota et al.
(a) Basic conﬁguration
(b) Choosing activities to be tracked
Fig. 5
ActivityTracker setup
The component is highly conﬁgurable, enabling us to deﬁne: communication
mode, via WiFi or with any data connection; synchronisation mode, to deﬁne when
to send data in order to properly manage the data traﬃc, namely in real time, on user
demand, or in batch mode at a certain period of time (see Fig. 5a); and the set of
observers to be attached to the apps’ components in order to capture data.
In this way, the component automatically sends data when a certain event is
received from an app component, a function is invoked, or if the value of any of
their properties is modiﬁed or read. For example, Fig. 5 illustrates the options to
track the button actions in an automatic way. ActivityTracker is able to capture the
action that happened, the name of the aﬀected component and the corresponding
input and output data, among others.
Additionally, ActivityTracker allows apps designer to send data on request
with speciﬁc semantics for collecting, for example, students’ interaction, activi-
ties, achievements, and so forth. A set of new programming blocks are available
in VEDILS in order to enable users to include LA features on their apps (Fig. 6).
Furthermore, ActivityTracker contextualises all that information by including the
following data: date and time when the event occurred, identiﬁers of the application
and screen where the event was generated, information about the device (IP, MAC
and IMEI if available), and geographic coordinates.

Learning Analytics in Mobile Applications Based on Multimodal Interaction
83
Fig. 6
Visual blocks to save interactions
The ActivityTracker component was internally developed using aspect-oriented
programming techniques. In particular, AspectJ11 is the responsible for the non-
intrusive compile-time interlinking of the code needed to capture and send data
inside the source code of each VEDILS’s components.
3.2.3
Data Processing
Two components to issue queries to the storage service and to retrieve the informa-
tion collected by the ActivityTracker component have been developed.
∙ActivitySimpleQuery: this component internally prepares and launches the SQL
queries over the data storage endpoint. The app designer may indicate in the Block
view of VEDILS how to ﬁlter the data according to several attributes such the
application ID, the screen ID, the component ID, the action occurred or the user
involved. Figure 7 illustrates the conﬁguration panel of this component.
∙ActivityAggregationQuery: this component enables the aggregation of certain
ﬁelds from the data storage by using common operators such as count, maximum,
minimum, sum, and others. Figure 7 illustrates the conﬁguration panel of this com-
ponent.
These components share the method blocks to SendQuery, to send a query to the
server, and DataReceived event which provides an array of data to be processed.
Additionally, both components provide app designers with some blocks to apply
ﬁlters to the data queries (Fig. 8).
3.2.4
Data Visualisation and Intervention
Data visualisation techniques are a common approach to study the representation
of data, by abstracting them in some schematic form. Dashboards can be easily
11http://www.eclipse.org/aspectj.

84
J. M. Mota et al.
Fig. 7
Data processing components
Fig. 8
Visual blocks to query the data

Learning Analytics in Mobile Applications Based on Multimodal Interaction
85
developed with VEDILS. Towards this end, components based on Google Chart
API12 were developed:
∙Chart: this component enables the creation of simple graphics such as bar, line
and pie charts, line or bar charts.
∙DataTable: this component is intended to present the data in a tabular format.
Both components can be fed with data from a ActivitySimpleQuery, an Activ-
ityAggregationQuery previously deﬁned, or any arbitrary array of data. Furthermore,
the components share some properties and functions such as the data refresh inter-
val. These dashboards can be included in the apps so the students can follow their
progress throughout the learning activities, and teachers can monitor their learners.
In addition to the analysis and visualisation of the information, it is necessary to
provide teachers with tools for taking actions in the learning process. These inter-
ventions may consist on providing students with feedback or adapting the mobile
learning activities according to the needs of the individual learners. To this end,
the InstantMessaging component of VEDILS, based on Google Cloud Messaging,13
may be used to send text messages and notiﬁcations to other devices with the same
app as exempliﬁed below.
∙Students–Teacher: the app can be programmed to send a notiﬁcation when certain
conditions are fulﬁlled, such as when the student ﬁnishes an activity or repeats
many times the reading of a piece of a lesson, among others.
∙Teacher–Students: during the use of the app in classroom, teachers can send mes-
sages with further explanations and notiﬁcations to draw attention the students’
attention by automatically redirecting their apps to a given screen.
4
Case Study
Language learning is a teaching area in which the incorporation of mobile applica-
tions can be of interest. For this reason, we have developed an app using VEDILS
that implements the game Chinese Whispers.14 The game consists on the oral trans-
mission of a message from one person to the next by whispering the same. When the
message reaches the last person s/he says it, allowing the ﬁnal message to be com-
pared with the initial message. In class teachers generally use the game to work on
speeches that emphasize the distinctive features between two phonemes. The game
supports the practice and evaluation of listening as well as speaking skills. Some
other case studies using the VEDILS authoring tool can be found in the website of
the project.15
12https://developers.google.com/chart.
13https://developers.google.com/cloud-messaging/.
14https://play.google.com/store/apps/details?id=appinventor.ai_vedils.TheChineseWhispers.
15http://vedils.uca.es/web/index.html#portfolio.

86
J. M. Mota et al.
4.1
Application Design
In order to implement the game Chinese Whispers, the next steps were deﬁned in the
app:
∙User registration: The student must enter name and surname to be identiﬁed later
by the teacher. All user’s interactions with the app will be saved using this infor-
mation (Fig. 9a).
∙Game Creation: When the students are enrolled, the teacher creates the game,
deﬁnes the groups and its membership. S/he also deﬁnes the initial message to be
whispered choosing one of the available options: load a csv ﬁle with the sentences,
connect to a fusion table to retrieve this or record a voice message there and then
(Fig. 9b).
∙Game development: One student in each group receives the sentence/s to be whis-
pered, an event is sent to inform about this. The message can be listened as many
times as necessary. When student thinks s/he has understood the message received,
s/he records it and send it on to another student who intern listens to the message
received, records it and sends it on (Fig. 9c). The last student to receive the mes-
sage send his/her recording on to the teacher. Figure 9d illustrates the list of games
students are participating in. These can be pending to respond, message recorded
and sent to peers, or game over.
∙View Interactions: The teacher can then visualise the interactions recorded by each
student during the game by logging into the app as an administrator.
Several components of VEDILS were used to deploy this app:
∙ActivityTracker: This component allows to automatically save the students’ inter-
action while using the app to Google Fusion Tables.
∙FusionTablesControl: All data generated by the app is saved in Google Fusion
Tables. This component deﬁnes where data about the information used in the app
is stored (users and sentences).
∙TextToSpeech: It is used to convert normal language text into speech, so students
can listen to the sentences recorded by the teacher or other students as many times
as they need.
∙SpeechRecognizer: To recognise the voice of the students and convert the message
into text format.
∙ActivityAggregationQuery: To query from the app the data recorded by the Activ-
ityTracker component.
∙Chart: To graphically display the results obtained by the ActivityAggregation-
Query component.

Learning Analytics in Mobile Applications Based on Multimodal Interaction
87
(a) Register screen for students
(b) Screen to create new games
(c) Screen to record and send messages
(d) Games in which the user is participating
Fig. 9
Screens of Chinese Whispers app

88
J. M. Mota et al.
Fig. 10
Students evaluating the Chinese Whispers app
4.2
Evaluation
This section deﬁnes the context of the evaluation of the app using the reference model
for LA based on four dimensions: What, Who, How and Why.
∙Stakeholders (Who?): The app was evaluated with a group of 60 students. The
duration of the test was around 45 minutes and was carried out in the subject Ger-
man II (level A1/A2 of MCERL). During the evaluation, each student participated
in eight rounds of the game and they worked in dyads. Figure 10 show the students
using the app during the evaluation process.
∙Data and environments (What?): In this case, in order to evaluate the listening
and speaking skills of the students, the ActivityTracker component was conﬁg-
ured to automatically capture the number of times each message was listened or
recorded. A large volume of data was collected in Google Fusion Tables16 during
the evaluation.
∙Methods (How?): The data was queried using the ActivityAggregationQuery com-
ponent. In this case, the data were grouped by each round of the game and the
average number of times each message was listened/recorded was calculated.
∙Objectives (Why?): The teacher incorporated in the app 35 sentences categorised
into four levels of diﬃculty. In order to identify the sentences that proved to be
more diﬃcult for students. After analysing the recorded data some interesting
results could be extracted. Figure 11 illustrates the results. These highlight the
sentences assigned to round 3 and 4 were the most diﬃcult for students because,
on average they were the most listened/recorded. Finally, the teacher veriﬁed that
students remembered the vocabulary and meaning of the sentences used in the.
In addition, she concluded that the app was a great resource to encourage the
students.
16https://fusiontables.google.com/data?docid=1rTpotc_7XWof8mIGrDeZ_
Ia1nrdNc0Ilq4zDIbnD#rows:id=1.

Learning Analytics in Mobile Applications Based on Multimodal Interaction
89
(a) Recordings average for students.
(b) Listening average for students.
Fig. 11
Results obtained in the evaluation
5
Conclusions
This chapter provides an analysis of the state-of-the-art in LA techniques and user-
friendly authoring tools. It also presents several multimodal mobile interactions to
draw the possibilities available to improve users’ experiences and satisfaction in
enriched, multimodal learning environments. Despite the fact mobile that sensing
tools support researchers to analysis users’ interactions, their use to analyse students’
activities remains a challenge for teachers. This is due to: on the one hand, the lack of
proper semantics familiar to educators to describe events captured by the tools; and,
on the other hand, the lack of comprehensive and easy mechanisms for programming
actions on the apps themselves depending on the contextual information.
To overcome the barriers for creating context-aware learning mobile apps and
empower users without programming skills to create their own apps the VEDILS
framework has been developed. This framework comprises both a method to design
and deploy learning activities and a supporting tool. The tool is based on MIT’s
App Inventor, an easy-to-use platform for creating mobile apps. VEDILS provides
with a number of additional extensions to deal with some breakthroughs in HMI and
a toolkit for analytical purposes. A set of components were developed to support
the LA activities, namely ActivityTracker, for automatically capture of contextual
data about the students’ interactions; ActivitySimpleQuery and ActivityAggregation-
Query, for retrieving data by applying projections or aggregations, respectively; and
Chart and DataTable, to create and embed visualisations into the apps themselves.
With respect to the case study evaluation of the implementation of the game Chi-
nese Whispers, we found that block-based programming languages can help teach-
ers overcome their lack of programming skills, enabling them to develop their own
mobile applications using LA techniques. Furthermore, in this case our implementa-
tion allowed us to identify the sentences that proved to be more diﬃcult for students.

90
J. M. Mota et al.
Despite the fact Fusion Tables are able to manage larger amounts of data than
spreadsheets typically do, they present a number of limitations regarding the number
of requests allowed and the available space per table. Furthermore, this usage may
cause privacy issues concerning students’ data. To tackle these issues, an improved
version of the ActivityTracker component is being developed. The new component
will support the storage of data in an in-house non-relational database, such as a
MongoDB17 server. Additionally, by using a ﬂexible data scheme, the app designer
will be able to use the VEDILS query components with a more ﬁne-grained conﬁg-
uration.
Another future line of work is the adoption of LA speciﬁcations such as xAPI
in VEDILS. Instead of sending pre-deﬁned SQL insert statements, a more sophisti-
cated ActivityTracker would send JSON streams to be stored in an LRS. Besides, in
order to properly support the xAPI speciﬁcation, it is necessary to annotate the xAPI
statements with a number of verbs coming from a controlled vocabulary. For this pur-
pose, a new set of visual blocks for those verbs has to be developed in the VEDILS
blocks view. This data integration generated by VEDILS in the xAPI ecosystem will
enable a more dynamic tracking of activities performed by students regardless of the
learning devices (mobile or traditional ones) that are used.
References
Agudo-Peregrina, Á. F., Iglesias-Pradas, S., Conde-González, M. Á., & Hernández-García, Á.
(2014). Can we predict success from log data in VLEs? classiﬁcation of interactions for learn-
ing analytics and their relation with performance in VLE-supported F2F and online learning.
Computers in Human Behavior, 31, 542–550.
Aharony, N., Pan, W., Ip, C., Khayal, I., & Pentland, A. (2011). Social fMRI: Investigating and
shaping social mechanisms in the real world. Pervasive and Mobile Computing, 7(6), 643–659.
ISSN 1574-1192. https://doi.org/10.1016/j.pmcj.2011.09.004.
Andone, I., Eibes, M., Trendaﬁlov, B., Montag, C., & Markowetz, A. (2016). Menthal: A framework
for mobile data collection and analysis. In Proceedings of the 2016 ACM International Joint
Conference on Pervasive and Ubiquitous Computing (pp. 624–629).
Bain, K., Basson, S. H., & Wald, M. (2002). Speech recognition in university classrooms: Liber-
ated learning project. In Proceedings of the Fifth International ACM Conference on Assistive
Technologies (pp. 192–196). ACM.
Balderas, A., Dodero, J. M., Palomo-Duarte, M., & Ruiz-Rube, I. (2015). A domain speciﬁc
language for online learning competence assessments. International Journal of Engineering
Education—Special issue on Innovative Methods of Teaching Engineering, 31(3), 851–862.
Baloian, N., Pino, J. A., & Vargas, R. (2013). Tablet gestures as a motivating factor for learning.
In Proceedings of the 2013 Chilean Conference on Human-Computer Interaction (pp. 98–103).
ACM.
Bhih, A. A., Johnson, P., & Randles, M. (2016). Diversity in smartphone usage. In Proceedings
of the 17th International Conference on Computer Systems and Technologies 2016 (pp. 81–
88). ISSN 15277755. https://doi.org/10.1145/2983468.2983496. http://dl.acm.org/citation.cfm?
doid=2983468.2983496.
17https://www.mongodb.com.

Learning Analytics in Mobile Applications Based on Multimodal Interaction
91
CAMF—Context-Aware Machine Learning Framework for Android (2010). IEEE Perva-
sive Computing. https://doi.org/10.2316/P.2010.725-003. http://www.actapress.com/PaperInfo.
aspx?paperId=41706.
Carlson, D., & Schrader, A. (2012). Dynamix: An open plug-and-play context framework for
android. In 2012 3rd IEEE International Conference on the Internet of Things (pp 151–158),
October 2012. https://doi.org/10.1109/IOT.2012.6402317.
Chatti, M. A., Dyckhoﬀ, A. L., Schroeder, U., & Thüs, H. (2012). A reference model for learning
analytics. International Journal of Technology Enhanced Learning, 4(5/6), 318. https://doi.org/
10.1504/IJTEL.2012.051815.
de-la Fuente-Valentín, L., Pardo, A., Hernández, F. L., & Burgos, D. (2015). A visual analytics
method for score estimation in learning courses. Journal of UCS, 21(1), 134–155.
Di Serio, Á., Ibáñez, M. B., & Kloos, C. D. (2013). Impact of an augmented reality system on
students’ motivation for a visual art course. Computers & Education, 68, 586–596.
Esteban, G., Fernández, C., Conde, M. Á., & García-Peñalvo, F. J. (2014). Playing with shule:
Surgical haptic learning environment. In Proceedings of the Second International Conference
on Technological Ecosystems for Enhancing Multiculturality (pp. 247–253). ACM.
Ferguson, R. (2012). Learning analytics: Drivers, developments and challenges. International Jour-
nal of Technology Enhanced Learning, 4(5–6), 304–317.
Ferreira, D., Kostakos, V., & Dey, A. K. (2015). AWARE: Mobile context instrumentation frame-
work. Frontiers in ICT, 2(April), 1–9. ISSN 2297-198X. https://doi.org/10.3389/ﬁct.2015.
00006. http://journal.frontiersin.org/article/10.3389/ﬁct.2015.00006/abstract.
Ferreira, D., Schuss, C., Luo, C., Goncalves, J., Kostakos, V., & Rahkonen, T. (2016). Indoor light
scavenging on smartphones. In Proceedings of the 15th International Conference on Mobile and
Ubiquitous Multimedia—MUM ’16 (pp 369–371). https://doi.org/10.1145/3012709.3017603.
http://dl.acm.org/citation.cfm?doid=3012709.3017603.
Froehlich, J., Chen, M. Y., Consolvo, S., Harrison, B., & Landay, J. A. (2007). MyExperience: A
system for in situ tracing and capturing of user feedback on mobile phones. In Proceedings of the
5th International Conference on Mobile Systems, Applications and Services (pp. 57–70). ISBN
9781595936141.
Griﬃths, D., & Hoel, T. (2016). Comparing xAPI and Caliper. LACE: Technical report.
Guo, B., Yu, Z., Zhou, X., & Zhang, D. (2014). From participatory sensing to mobile crowd sensing.
In 2014 IEEE International Conference on Pervasive Computing and Communications Work-
shops (PERCOM Workshops) (pp. 593–598). IEEE.
Hsu, Y. C. , Rice, K., & Dawley, L. (2012). Empowering educators with Google’s Android App
Inventor: An online workshop in mobile app design. British Journal of Educational Technology,
43(1). https://doi.org/10.1111/j.1467-8535.2011.01241.x.
Jian, M.-S., Shen, J.-H., Huang, T.-C., Chen, Y.-C. & Chen, J.-L. (2015). Language learning in
cloud: Modular role player game-distance-learning system based on voice recognition. In Future
Information Technology-II (pp. 129–135). Springer.
Lindgren, R., & Johnson-Glenberg, M. (2013). Emboldened by embodiment six precepts for
research on embodied learning and mixed reality. Educational Researcher, 42(8), 445–452.
Martin, F., & Ertzberger, J. (2013). Here and now mobile learning: An experimental study on the
use of mobile technology. Computers and Education, 68, 76–85. ISSN 03601315. https://doi.
org/10.1016/j.compedu.2013.04.021.
Milgram, P., & Kishino, F. (1994). A taxonomy of mixed reality visual displays. IEICE Transactions
on Information and Systems, 77(12), 1321–1329.
Mota, J. M., Ruiz-Rube, I., Dodero, J. M., & Arnedillo-Sánchez, I. (2017). Augmented reality
mobile app development for all. Computers & Electrical Engineering.
NMC Horizon Report (2014). Games and Gamiﬁcation. The New Media Consortium. ISBN
9780989733557.
Ochoa, X., & Superior, E. (2016). Augmenting learning analytics with multimodal sensory data.
Journal of Learning Analytics, 3(2), 213–219.

92
J. M. Mota et al.
Raento, M., Oulasvirta, A., Petit, R., & Toivonen, H. (2005). ContextPhone: A prototyping platform
for context-aware mobile applications. ISSN 15361268.
Rodgers, D. L., & Withrow-Thorton, B. J. (2005). The eﬀect of instructional media on learner
motivation. International Journal of Instructional Media, 32(4), 333.
Romero, C. (2010). Educational data mining: A review of the state-of-the-art. IEEE Transactions
on Systems, Man, and Cybernetics, Part C, 40(X), 601–618. ISSN 1094-6977. https://doi.org/
10.1109/TSMCC.2010.2053532.
Rushkoﬀ, D. (2010). Program or be programmed: Ten commands for a digital age. Or Books.
Santos, M. E. C., Yamamoto, G., Taketomi, T., Miyazaki, J., & Kato, H. (2013). Authoring aug-
mented reality learning experiences as learning objects. In 2013 IEEE 13th International Con-
ference on Advanced Learning Technologies (pp. 506–507).
Sclater, N. (2008). Web 2.0, Personal Learning Environments, and the Future of Learning Manage-
ment Systems. EDUCAUSE Research. Bulletin, 13, 2008.
Shen, J., & Pantic, M. (2013). Hci2 a software framework for multimodal human-computer interac-
tion systems. IEEE Transactions on Cybernetics, 43(6), 1593–1606, Dec 2013. ISSN 2168-2267.
https://doi.org/10.1109/TCYB.2013.2271563.
Stopczynski, A., Stahlhut, C., Petersen, M. K., Larsen, J. E., Jensen, C. F., & Ivanova, M. G., et al.
(2014). Smartphones as pocketable labs: Visions for mobile brain imaging and neurofeedback.
International Journal of Psychophysiology, 91(1), 54–66.
The New Media Consortium (2011). The Horizon Report. Media (pp. 40). http://cmsweb1.lcps.org/
cms/lib4/VA01000195/Centricity/Domain/52/2011-Horizon-Report-K12.pdf.
Therón, R., García-Peñalvo, F., & Cruz-Benito, J. (2016). Software architectures supporting human-
computer interaction analysis: A literature review. In Learning and Collaboration Technologies
(page In press). Springer International Publishing.
Turk, M. (2014). Multimodal interaction: A review. Pattern Recognition Letters, 36, 189–195.
Wei, L., Zhou, H., Soe, A. K., & Nahavandi, S. (2013). Integrating kinect and haptics for interac-
tive stem education in local and distributed environments. In 2013 IEEE/ASME International
Conference on Advanced Intelligent Mechatronics (AIM) (pp. 1058–1065). IEEE.
Xiong, H., Huang, Y., Barnes, L. E., & Gerber, M. S. (2016). Sensus: A cross-platform, general-
purpose system for mobile crowdsensing in human-subject studies. In Proceedings of the 2016
ACM International Joint Conference on Pervasive and Ubiquitous Computing (pp. 415–426).
https://doi.org/10.1145/2971648.2971711.

Increasing the Role of Data Analytics
in m-Learning Conversational
Applications
David Griol and Zoraida Callejas
Abstract Technological integration is currently a key factor in teaching and
learning. New interaction handheld devices (such as smartphones and tablets) are
opening new learning scenarios that require more sophisticated applications and
learning strategies. This chapter is focused on the high variety of educational
applications that multimodal conversational systems offer. We also describe a
framework based on conversational interfaces in mobile learning to enhance the
learning process and experience. Our approach focuses on the use of NLP tech-
niques, such as speech and text analytics, to adapt and personalize student’s con-
versational interfaces. Using this framework, we have developed a practical app that
offers different kinds of educative exercises and academic information, which can
be easily adapted according to the pedagogical contents and the students’ progress.
Keywords Mobile learning (m-learning) ⋅Data analytics ⋅Conversational
interfaces ⋅Multimodal ⋅User modeling ⋅Context of the interaction
Adaptation of the provided services
D. Griol (✉)
Department of Computer Science, Carlos III University of Madrid,
Avda. de la Universidad, 30, 28911 Leganés, Spain
e-mail: dgriol@inf.uc3m.es
Z. Callejas
Department of Languages and Computer Systems, University of Granada,
CITIC-UGR, Granada, Spain, C/Periodista Daniel Saucedo Aranda s/n,
18071 Granada, Spain
e-mail: zoraida@ugr.es
© Springer International Publishing AG 2018
S. Caballé and J. Conesa (eds.), Software Data Engineering for Network eLearning
Environments, Lecture Notes on Data Engineering and Communications
Technologies 11, https://doi.org/10.1007/978-3-319-68318-8_5
93

1
Introduction
During the last years we are immersed in a new technological revolution where
conversational interfaces play a key role to provide a more natural human-machine
interaction (Becker et al. 2013; Oulasvirta et al. 2012; Pérez-Marín and
Pascual-Nieto 2011). Clear examples of these smart systems are virtual personal
assistants that allow anyone with a smartphone to request for information and
services virtually and interact with applications executed on the device or in the
cloud (for example, Microsoft’s Cortana, Google Now, or Siri), and the embodi-
ment of this kind of assistants into connected devices that enriches human-computer
interaction and deﬁnes new ways of interacting with large repositories of knowl-
edge on the Internet of things using advanced devices with spoken language
recognition (such as Google Home or Amazon Echo) or socially intelligent robots
(such as Pepper or Jibo).
Conversational interfaces are computer programs that offer dialog capabilities
very similar to human-human communication (López-Cózar and Araki 2005; Griol
et al. 2014; McTear et al. 2016). The term conversational is then used to describe
systems that show more characteristics similar to human ones and that support the
use of natural spontaneous language. Usually, these interfaces perform ﬁve main
processes, which are usually completed by distinguishing modules in the archi-
tecture of the conversational system:
• Automatic Speech Recognition (ASR),
• Spoken Language Understanding (SLU),
• Dialog Management (DM),
• Natural Language Generation (NLG),
• Text-to-Speech Synthesis (TTS).
Recent advances in the ﬁeld of Artiﬁcial Intelligence that studies the develop-
ment of software using human-machine interaction by means of natural language
(Computational Linguistics) has signiﬁcantly improved the capabilities related to
the processes of automatic speech recognition and synthesis, and natural language
understanding. The dialog management process has also been signiﬁcantly
improved during recent years by means of statistical techniques that consider the
dialog context and the speciﬁc requirements and preferences of each user in order to
select the next response of the conversational system. These advances have made it
possible to extend the number of initial applications of conversational interfaces,
from simple question-answer systems that provided simple information sources and
services, to advanced systems that provided complex human-machine communi-
cation, surveys systems, applications applied to e-commerce, advanced assistants
able of even detecting the emotional states of users, advanced communication
within and between vehicles, remote interaction in smart environments and intel-
ligent devices, Ambient Assisted Living (AAL) and e-Health applications, virtual
companions, and e-learning and tutoring systems (McTear et al. 2016).
94
D. Griol and Z. Callejas

Within the educative ﬁeld, most educators acknowledge that conversational
interfaces can improve the learning and teaching processes, allow the implemen-
tation of inclusive educational activities, customize the teaching and learning
processes, and provide and advanced educative environment (Fryer and Carpenter
2006; Baylor and Kim 2005). These aspects are addressed by establishing a more
attractive and more similar human relationship between students and the system.
The kinds of educational systems in which conversational interfaces can be inte-
grated include tutoring systems, question-answering applications, conversation
systems for language learners, learning partners and pedagogic agents, dialog
applications for computer-assisted speech therapy, or conversational systems that
provide metacognitive functionalities.
Multimodal conversational interfaces also employ a variety of techniques that
can be used to improve the learning and teaching achievements in e-learning and
mobile learning (m-learning) experiences (Salse et al. 2015; O’Halloran 2015). The
use of natural language to facilitate communication between students and educa-
tional software allows students to dedicate their cognitive aptitudes for the learning
task, rather than dedicating time and effort to learn how to interact with the
graphical interface and menus provided by the application. In addition, the
anthropomorphic and dialog mechanisms provided by multimodal educative con-
versational systems beneﬁt the development of social educational activities and
increases the motivation for learning and knowledge acquisition in on-line activities
and e-learning scenarios. One of the most important advantages of combining
mobile devices and applications with conversational interfaces for educational
purposes is the increased participation of students in their learning process and the
personalization of this process according to their speciﬁc evolution and the pro-
vision of student’s adapted activities and pedagogical strategies.
Speech and text analytics play a key role in achieving these educative objectives.
By means of spoken communication, we transmit a great deal of information, not
only in what we say, but also in how we say it. Paralinguistic information
encompasses many intentional and unintentional aspects of the speakers, including
their emotional state, their intention during the dialog, their personalities, cultural
and demographic characteristics, etc.
Thus, there is currently a growing interest within the Natural Language Pro-
cessing (NLP) and Speech Technologies communities in the use of test and speech
analytics to detect the users’ behaviors and their mental states during the interaction
with the conversational interface, and adapt the operation of the system according to
these valuable information sources. Speech and text analytics can thus be combined
with learning analytics to develop enhanced student’s models and use them to
personalize the learning process by means of student’s adapted conversational
interfaces.
In this chapter, we describe a proposal based on a modular architecture a for
developing pedagogical conversational interfaces. This framework is focused on the
use on these interfaces for the interaction with mobile devices with the Android
Increasing the Role of Data Analytics in m-Learning …
95

operating system. The main objectives are to improve the interaction with the
students taking into account the context of the interaction, and the adaptation of the
provided services according to the user’s preferences. The proposed architecture
integrates a statistical user model that employs text and speech analytics for the
estimation of the intention of the user each time a new system response is selected.
These analytics can also be extended with the use of external context information.
Also, we present the application of the proposed architecture for developing a
conversational app for mobile devices. This application provides a number of
functionalities:
• Generation of personalized information,
• Connection to social networks,
• Provision of various types of educational exercises.
The application is focused on the generation of practical educational exercises
that promote autonomous learning and self-assessment. Our proposal to develop
pedagogical conversational interfaces improves the interaction to solve the pro-
posed exercises, to personalize the selection of the different learning activities
taking into account the preferences of the student using the system and its previous
uses, and allows the immediate feedback by means of the automatic evaluation of
learning activities. The application we have developed allows to generate practical
educational exercises that the student can solve autonomously and whose assess-
ment is performed automatically by the system. Through the developed application
and the conversational interface that it provides, students can access and solve the
activities that are proposed in a more intuitive and natural way. The application
further personalizes the provision of these activities in accordance with the pref-
erences provided and the progress of the student. These adapted answers are also
provided immediately after the automatic evaluation of the answers provided by the
students.
We address very important topics and guidelines corresponding to the main axes
of the book, with the aim of completing the following objectives:
• Show the beneﬁts and challenges of using conversational interfaces for educa-
tive purposes;
• Describe how speech and learning analytics can be combined to consider the
preferences of each student when they maintain a long-term relationship with
the system, and their speciﬁc evolution to personalize the educative process,
improve the interaction and selection of contents, motivate students, and
increase students’ participation and performance;
• Present the main computational models, tools, and techniques that can be used
to interpret speech and text in diverse ways, and describe how the operation of
an educational conversational application can be improved by means of the set
of learning analytics that can be computed from the spoken interaction and the
transcription of the speech utterances;
96
D. Griol and Z. Callejas

• Present the main research work to integrate these analytics in the SLU module
by means of Natural Language Processing techniques, detect the user’s emo-
tional states and main personality traits to generate enhanced user models, try to
predict the intentions of the users after each system response, and anticipate the
kind of actions selected by the educational application during the dialog man-
agement process according to these models;
• Show a practical educative application integrating these analytics to design
student’s adapted conversational interfaces and mobile learning apps.
The rest of the chapter is divided in the following way. Section 2 provides an
outline on the main principles involved in the development of multimodal dialog
systems for educative purposes. The section also provides examples of multimodal
systems implemented in this domain. We describe the proposed framework for the
implementation of educational applications integrating conversational interfaces in
Sect. 3. In Sect. 4, we present the practical application of this framework for the
development of an educative mobile app. The results of the assessment of this
practical application are summarized in Sect. 5. Finally, Sect. 6 provides the main
conclusions and outlines possibilities for research directions and future work.
2
Educative Conversational Interfaces:
The Role of Data Analytics
As it has been previously described in the previous section, a conversational
interface can be deﬁned as a software application that process inputs in natural
language and generates a natural language output to involve a human-machine
conversation. Many technologies are involved in the complex task of developing a
conversational interface: signal processing, linguistics, phonetics, speech tech-
nologies, natural language processing, graphical design of interfaces, affective
computing, animation techniques, sociology, psychology, and telecommunication
technologies and infrastructures. Complexity is often addressed by dividing the
architecture into problems assigned to the different modules described in the
introduction section: ASR, SLU, DM, NLG and TTS.
The speech recognition process obtains a text string (one or more sentences in
natural language) from a speech signal (Rabiner and Huang 1993). A variety of
factors can affect and modify the input of this module: some of them related to the
speaker, and other related to the transmission channel and the interaction envi-
ronment. Different applications require different complexity for this task. Cole et al.
(1997) identiﬁed eight parameters that allow an optimal adaptation of the recog-
nizer: speech mode, speech style, dependence, vocabulary, language model, per-
plexity, signal-to-noise ratio (SNR) and transduction. At present, general purpose
ASR systems are generally based on Hidden Markov Models (HMM) (Rabiner and
Juang 1993).
Increasing the Role of Data Analytics in m-Learning …
97

With respect to understanding spoken language, this process has as its main
objective the extraction of the semantic content of the text provided by the auto-
matic speech recognizer (Minker 1998). In a ﬁrst stage, the lexical and morpho-
logical knowledge is usually used to extract the constituents of the words (lexemes
and morphemes). Through syntactic analysis the structure of sentences is obtained
in a hierarchical way and semantic analysis obtains the semantic content (meaning)
of its constituents. Traditionally, two fundamental methodologies have been used
for the development of the Spoken Language Understanding module: the,
rule-based methods (Mairesse et al. 2009) and statistical methodologies (Meza-Ruíz
et al. 2008). They also include hybrid models (Tur and De Mori 2011).
The main objective of the dialog management process is to select the next
answer (system’s action) that the conversational interface will execute. The simplest
dialog management models are based on ﬁnite-state machines (each state represents
a state of dialogue and the corresponding transitions indicate actions taken by the
user to move from one state to another). Frame-based methods offer greater ﬂexi-
bility and, for this reason, are currently integrated into many commercial systems.
In this approach, the system will require the user to provide certain data that will be
stored in slots within a data structure. Users can usually supply one or several data
pieces in the same dialog turn to thereby complete more than one slot per turn. In
more complex application domains, plan-based dialog models are based on the fact
that human beings establish objectives that must be achieved at the end of the
conversation and that can cause our mental states to change throughout them (Chu
et al. 2005). As in the understanding of language, models based on rules and
statistical models based on the learning of the dialogue manager have been pro-
posed from data corpus related to conversations in the domain of application of the
system (Griol et al. 2008; Williams and Young 2007; Cuayáhuitl et al. 2006;
Lemon et al. 2006).
With respect to the generation of natural language, the main objective of this
process is to start from the action selected by the dialogue manager (non-linguistic
representation) and generate a message in natural language (one or more sentences).
This process is usually carried out in ﬁve phases:
• organization of contents,
• distribution of contents into sentences,
• lexicalization,
• generation of referential expressions
• linguistic realization.
The simplest approach to generating natural language is to use predeﬁned
messages that associate the action of the system with one or several predeﬁned
responses that convey a similar message (Reiter 1995). This approach is very
intuitive, but also very little ﬂexible in systems applied to complex application
domains. A more sophisticated approach consists in the use of data ﬁles with text
98
D. Griol and Z. Callejas

templates. Each of the possible actions for the system has one or more templates
associated with these ﬁles, in ways that all those associated with the same action
transmit a similar message. There are approximations that use more general tem-
plates in which sentences are structured according to grammatical rules at the level
of sentence or level of discourse (Elhadad and Robin 1996). Another approach is
based on the use of characteristics in which each of the possible minimum units of
expression is symbolized by a single feature that guarantees greater ﬂexibility and
generalization (Oh and Rudnicky 2000).
Text-to-speech synthesizers translate text strings into a voice signal (Nagy and
Németh 2016). The modules in the front end of the TTS synthesizer translate the
symbols as abbreviations and numbers that may be present in the text to be syn-
thesized (tokenization, preprocessing or normalization of the text) and assign to
each word a phonetic transcription dividing the text into prosodic units. The
back-end modules translate the words from text format to voice signal. In synthesis
by concatenation, it joins pre-recorded audio ﬁles to form words. These techniques
usually produce more natural audios, but the differences between the variations in
speech can produce audible technical problems.
With the recent advances in the technologies necessary for the development of
conversational interfaces, the possibilities of using these interfaces for the devel-
opment of educational applications in a wide range of possible applications have
increased:
• tutoring applications (Pon-Barry et al. 2006),
• question-answering systems for the preparation of tests (Wang et al. 2007),
• pedagogical partners and learning agents (Cavazza et al. 2010),
• applications for language learning (Freidora and Carpintero, 2006),
• dialog systems to provide help in pathologies related to language and voice
(Vaquero et al. 2006)
• conversational systems that promotes metacognitive skills (Kerly et al. 2008b).
Students can also, in some systems, interact with other agents or classmates
(Dillen-bourg and Self 1992), have to face learning problems posed by the system
itself (Aimeur et al. 1992), or even having to teach the agent itself (Muñoz et al.
2015).
The use of new technologies in the classroom also makes it easier for educa-
tional institutions to follow the guidelines of the European Higher Education Area
(EHEA), in terms of learning aimed at achieving competences and guiding students
so that they feel they are protagonists of the learning process.
To make these objectives possible, it is necessary to promote the autonomous
learning of the students, so that the fundamental role of the teachers is to transmit
the learning contents and, above all, generate advanced learning environments
where strategies are used to that students get involved in the process of their own
learning. According to Roda et al. (2001), the use of innovative technologies in the
learning process:
Increasing the Role of Data Analytics in m-Learning …
99

• reduces the time required in the learning process,
• helps to personalize it taking into account the progress of each student
• facilitates the access to education,
• generates more advanced learning environments.
They also describe three fundamental categories in which conversational inter-
faces can facilitate the achievement of these objectives: personal trainers that
facilitate help within a restricted ﬁeld of knowledge, advanced help systems and
applications that facilitate learning objectives, avatars and simulated actors that
usually allow interaction in virtual environments. The complexity of the conver-
sational interfaces varies considerably according to the speciﬁc category among the
previously described in which the system is applied: there are simple systems that
use dialogs based on text forms that are completed using the keyboard (Heffernan
2003) to advance systems that employpersoniﬁed avatars that, in addition to
interacting using speech, are able to recognize and generate gestures and emotions
(Kerly et al. 2008b). Most of the current systems integrate speech recognition and
synthesis (Graesser et al. 2001; Litman and Silliman 2004; Fryer and Carpenter
2006).
Conversational systems developed to interact as personal trainers represent and
update in a continuous way the cognitive and social states of the users. With this,
they aim to guide the students, provide suggestions and enable them to interact not
only with the system, but also with the rest of the students through interactive and
realistic interfaces. The CALM system (Kerly et al. 2008a) follows this paradigm,
allowing students to facilitate the answers to the questions asked and, in addition,
the level of conﬁdence they have in the answers provided. Based on these data
pieces, the system estimates the student’s level of knowledge, encourages them to
participate in their self-assessment and to discuss the differences found between
their opinions and those estimated by the system. Conversational interfaces
designed to interact in simulated environments and virtual worlds often carry out
very speciﬁc functions within the learning process and interact with students in a
very realistic way in a simulated real-world environment.
The most common application of conversational interfaces are systems oriented
to tutoring (Kumar and Rose 2011). These systems have also been integrated more
recently into robotics (Sidner et al. 2004; Dowding et al. 2006) with the main
objective of developing robots with social skills applied to both education purposes,
help in the treatment of diseases and entertainment (Gorostiza and Salichs 2011).
Theobalt et al. (2002) used a set of agents developed by means of the Open Agent
Architecture toolkit to integrate the possibility for dialog between robots and human
beings according to the Theory of Discourse Representation (DRT).
In recent years, the use of data analysis techniques and user modeling has been
proposed to adapt the behavior of the conversational interfaces taking into account
the different ways of transmitting the messages and the context of the interaction.
User models usually generate responses that simulate those that would be trans-
mitted by a real user and integrate an error model to simulate speech recognition
and comprehension processes.
100
D. Griol and Z. Callejas

Some systems also consider modeling emotions, affect and/or personality in
order to build more advanced user models and generate more expressive and
credible conversations with the students. Emotion models are often based on the-
ories that come from the ﬁelds of psychology, sociology, neuroscience and lin-
guistics (McTear et al. 2016). However, most of the current research in the ﬁeld of
emotional modeling is based only on the recognition and synthesis of emotions
(Schuller and Batliner 2013, Calvo et al. 2014).
In some conversational applications, such as intelligent tutors or systems
developed for emergency services, it is also very important to recognize the
affective state of the users, adapting or modifying completely the operation of the
system according to the affective states that have been recognized. In other appli-
cation domains, the recognition of the user’s affective state can be very helpful to
resolve dialog scenarios that generate negative emotional states, prevent them, and
favor positive states in future interactions with the same users.
Personality can be deﬁned as those characteristics of a person that inﬂuence
different situations in a unique way in their motivations, cognitions and behavior.
The modeling of the personality is, therefore, a very important factor to understand
the behaviors of the users interacting with the system and modify the behavior of
the system according to them. The personality is usually modeled through patterns,
called traits, that make up feelings or actions that remain stable throughout our lives
and cause them to react in a similar way every time they occur.
The OCEAN model or “big ﬁve” traits (McCrae and John 1992) is one of the
most used to model personality in conversational systems. Other systems use
characteristics deﬁned according to the speciﬁc application domains of each system
and integrate these characteristics by means of continuous or discrete models
deﬁned ad hoc.
Some representative examples of the improvements that can be obtained with the
integration of this type of models are those described in systems that have been
applied to the domain of tutoring systems (Graesser et al. 2005; Rosé et al. 2001;
Kumar and Rose 2011; Wang and Johnson 2008). These systems have focused
especially on the recognition of emotional states of students and their use to
improve the tutorial action of the system (D’Mello et al. 2005).
The AutoTutor system (Graesser et al. 1999) facilitates tutoring through dia-
logues within the application domains of physics and computer science. The models
used by the system are based on real behaviors learned through the practices of
university tutors. The evaluation of the system proved that these models provided a
signiﬁcant improvement in the students’ learning process, memory capacity and
achievements.
The Geometry Explanation Tutor system (Aleven et al. 2004) also uses these
techniques and models to require students to solve geometry problems using natural
language. The Oscar system (Latham et al. 2012) also simulates a human tutor by
modeling different strategies of tutoring and personalization of the learning style to
improve the student’s conﬁdence in the system. The system also uses natural lan-
guage to provide its answers to the user.
Increasing the Role of Data Analytics in m-Learning …
101

Other systems combine data analysis and the use of avatars capable of simu-
lating verbal and nonverbal communication (Cassell et al. 2000). In domains of
application of counselors (Gratch et al. 2002, Feng et al. 2017), healthy life
counselors (de Rosis et al. 2005), or personal trainers (Bickmore, 2003).
These characteristics favor that conversational interfaces can be applied suc-
cessfully with pedagogical purposes (Johnson et al. 2004) and in other application
domains where it is important to establish a long-term relationship with their users
(Bickmore and Picard 2005). These models allow an emphatic relationship with the
user (de Rosis et al. 2005; Cassell et al. 2000; Ai et al. 2006; Bailly et al. 2010;
Edlund et al. 2008), which can be maintained continuously through the use of
mobile devices.
Other studies have also proposed usability models (Schmitt and Ultes 2015) that
allow evaluating user’s satisfaction after each system response and automatically
modify the system’s response according to the evaluation of the quality of the
interaction (IQ) by means of parameters that are independent of the speciﬁc
application domain of the system.
3
Proposed Framework
In the previous section we have seen that multimodal conversational interfaces
enable communication with their users through different input and/or output modes
with which they allow the user to facilitate their entries and show the results. To
deal with this enhanced communication, several academic and laboratory systems
integrate not only the main modules described in the previous section (automatic
speech recognition, spoken language understanding, dialog management, database
repositories, natural language generation, and text-to-speech synthesis), but also
additional modules, which are usually to speciﬁc tasks and make the portability of
the systems very difﬁcult. Thus, it is a challenge to develop frameworks and toolkits
providing an easy installation of the system’s functionalities and avoiding the
restriction of only using these system for in-lab studies.
3.1
Main Components
To facilitate the development of multimodal conversational interfaces, we have
developed the architecture shown in Fig. 1. Through this architecture it is possible
to develop educational applications in which students can interact through the voice
and/or using the graphic interface of the application. The systems developed
through this architecture allow to generate test questionnaires that students can
complete by answering the corresponding questions, allow the integration of per-
soniﬁed avatars, analyze the answers provided by the students and generate
102
D. Griol and Z. Callejas

adequate feedback according to the comparative analysis between the response
provided by the student and the annotated in the system.
The modules in the proposed architecture can be implemented in such a way that
commercial speech recognizers and synthesizers, different methodologies and
techniques for language understanding and dialog management can be easily
integrated.
Next, the main objective of the User Answer Analyzer is to check whether the
answer provided by the SLU module after the analysis of the student’s input
matches the reference one in the database of the application. This module computes
the percentage of success according to the similarity of both responses and selects
the recommendations and feedback that should be transmitted to the student. To do
this, this module uses grammars and data analysis techniques that allow to carry out
the comparison of the student’s response and the reference, increasing the per-
centage of similarity and correction each time a coincidence is detected. Through
these grammars the ﬂexibility of the system is increased, the interaction can be
completed using natural language and it is possible to adapt the contents of the
application according to the subjects and disciplines in which the system is applied.
From the previous analysis, the dialog manager determines the next system
response. To do this, the DM uses the outputs generated by the ASR and SLU
modules, the conﬁdence measures provided by both modules and the degree of
similarity calculated by the User Answer Analyzer. Error handling is another key
point of the system to guarantee a correct communication with students, especially
to detect errors during the ASR and avoid that these errors could be transmitted to
the SLU and DM modules. The usual techniques for the detection and management
of errors during these processes include the use of conﬁdence measures, n-best
options for each of the modules, and different strategies for the conﬁrmation of user
inputs (explicit, implicit or mixed conﬁrmations) and the use of different dialog
initiatives (by the system, by the user or mixed initiatives).
Fig. 1 Proposed architecture to develop conversational interfaces for educational applications
Increasing the Role of Data Analytics in m-Learning …
103

Next, the dialog manager can use these information sources to decide if it is
necessary to conﬁrm the response provided by the student or consider that it is valid
and select other possible actions deﬁned for the system. For the development of the
dialog manager, in our proposal we recommend the use of statistical methodologies
(Griol et al. 2014; Williams and Young 2007), which reduce the time and effort
required in case of using rule-based models based for dialog management.
The action selected by the dialog manager must be translated into a natural
language message consisting of one or several sentences. The system provides this
response to the user through the natural language generation modules, Multimodal
Answer Generation and text-to-speech synthesis. For the development of the natural
generation module, we propose the use of methodologies based on text templates,
by means of which a set of responses in natural language is associated with each
action of the system. With respect to the TTS module, a commercial synthesizer can
be used or reproduce pre-recorded audio messages stored in one of the application’s
data repositories. The Multimodal Answer Generator module decides which parts of
the message generated by the system should be transmitted through the graphic
interface of the system and which parts should be provided through the TTS, so that
the contents of these messages are complementary. The Questionnaire Generator
module modiﬁes the questions with theoretical and practical contents selected by
the system taking into account the output of the previously described modules of
the application.
3.2
Use of Data Analytics
The architecture that we propose in this chapter integrates two main data reposi-
tories. The ﬁrst one is used to store the educational contents according to the
deﬁned division into units (set of questions, corresponding options to show the
student, correct option, and feedback for each of the options). Both the questions
and possible responses can integrate diverse types of multimedia contents (text,
images, videos, interactive content designed for the web, scripts developed in
different programming languages, etc.). With the use of this database, the appli-
cation code is isolated from the educational contents, so that it is also very easy to
use a speciﬁc graphic interface to access databases on the web, update the contents
of this database, add new contents, or delete previously stored contents without
requiring advanced knowledge about programming languages or the development
of computer applications and conversational interfaces.
The second information repository is used to store logs with information cor-
responding to the students’ previous interactions with the application (time spent in
each of the sessions, number of selected questions, numbers of correct answers,
erroneous answers provided, and errors detected during the interaction). The use of
this database also allows adapting the educational contents according to the pro-
gress and preferences of each student.
104
D. Griol and Z. Callejas

With both data repositories, we guarantee the efﬁcient development of multi-
modal questionnaires to increase student motivation for the study and allow them to
self-assess the degree of knowledge reached, detect errors and correctly assimilate
the concepts transmitted in each one of the questions. Students can interact with the
application in different sessions, continue the forms at the point where they left
them in the last interaction, draw conclusions about the results previously to the
exams, etc. Teachers also receive data analytics related to the degree of assimilation
achieved by the students for the different contents, how they follow the subjects,
and main mistakes that are made.
The proposed architecture can also be extended by the integration of additional
modules for the recognition of the user’s intention and emotional state (Callejas
et al. 2012). In our proposal, this additional module uses the previous history of the
dialog and a set of features extracted from of the speech signal to select one of the
possible deﬁned emotional states. To recognize the intention, the previous history
of the dialog and the result provided by the SLU module for the last utterance
provided by the user is also considered. This result is compared with that estimated
by the user model. The output of the emotion and intention models is processed by
a composition module of the user’s state, whose output is transmitted to the dialog
manager as an additional input to decide more precisely the next system response.
4
Practical Educative Application
To demonstrate the validity of our framework to develop multimodal applications,
we have implemented an m-learning application that provides student-tailored
exercises and services. The interface of the app merges traditional GUIs with voice
interaction. Additional technologies have been also integrated:
• MySQL database,
• JSON format (JavaScript Object Notation) packages to access contents on the
web,
• JSoup Java library to extract HTML contents and connect to web pages,
When the students are posed exercises, they can provide their answer on the
graphical interface or via voice and the system’s response is both synthesized and
shown in the screen. The main aim of the app is to facilitate continuous formative
self-assessment to engage students and facilitate regular study and feedback.
There are two scenarios for the usage of our app. In the ﬁrst scenario, the student
selects the courses and lessons they want work with during the session. In the
second scenario, the student may have programmed previously the lessons to study
at different time periods and so the app actively warns the student when the
activities are ready and must be completed to meet the predeﬁned agenda.
In both cases the app poses questions about the lessons selected. The exercises
are selected not only in accordance to the speciﬁc lesson but also considering the
Increasing the Role of Data Analytics in m-Learning …
105

performance of the student in previous attempts to solve similar exercises, so that
the ones that are more difﬁcult for the student are asked more frequently until they
are more easily solved by the student. After each question, the student provides an
answer that is automatically assessed by the system to provide immediate feedback.
At each point the student can query the result of the exercises he has previously
solved. After each session, the student is assigned a grade and the system shows all
the exercises for the session including the answers provided and the correct
answers.
The ﬁrst type of activities provided by the app consists of questions whose only
possible answers are True or False (ﬁrst image in Fig. 2). Students can answer these
questions using any of the available input modalities. In addition, they receive the
corresponding feedback as they select the correct options or provide incorrect
responses.
The application also allows to incorporate questions with multiple answers, in
which students must select all the options that are correct among the different
possibilities provided by the system (second image in Fig. 2). As in the previous
case, students can use the different input modalities to answer these questions and
receive feedback according to the number of correct and incorrect options selected.
There is also the possibility of solving practical exercises that incorporate dif-
ferent types of multimedia contents (text, images and videos) both in the statement
of the exercises, their possible answers, and the feedback provided to the student
(third image in Fig. 2). To complete these exercises, there is a text ﬁeld that
students can complete using the keyboard or providing a speech input. They can
also reproduce the audio ﬁles generated by the application as feedback after
selecting each one of the proposed options.
Fig. 2 Screenshots of the types of educative activities
106
D. Griol and Z. Callejas

The application also provides statistics of the activities that the student has
previously solved (ﬁrst image in Fig. 3), with the results obtained in the last
exercise and the marks obtained after solving exercises in the previous sessions.
5
Preliminary Evaluation
Ten professors of our department have already participated in a preliminary eval-
uation of the application developed. To do this, they have used a questionnaire that
assesses the educational potential of the application and the degree of naturalness in
the interaction. The answers to all the questions in the questionnaire (Table 1) were
evaluated on a scale from 1 (complete disagreement) to 5 (complete agreement).
Professors were also required to provide a grade between 0 and 10 to assess the
overall performance of the application. Finally, they were also asked to provide the
comments they considered relevant about the overall operation of the application.
An objective assessment of the app by means of the same interactions with
professors has also been completed. The following parameters were evaluated:
• Question rate (QR). Number of correct interactions with the student. Each
interaction includes a cycle in which the system asks a question, the student
provides a response, and the system provides an adequate feedback.
• Conﬁrmation rate (CR). It refers to the number of system shifts with conﬁr-
mations with respect to the total number of shifts in the system;
• Rate of corrected errors (ECR). Number of correctly detected and handled errors
with regard the total number of errors.
Fig. 3 Screenshots of the app with the statistics and results of the proposed activities
Increasing the Role of Data Analytics in m-Learning …
107

The previously described set of parameters were evaluated by means of 30
interactions with the professors, in which they were completely free to use the
different functionalities of the application according to their preferences. Table 2
shows the results of the evaluation. As it can be observed, the satisfaction of the
professors with the application is high. They also perceived a high educational
potential and valued that the activities were relevant, and system was appropriate to
achieve the educational objectives proposed. The global mark assigned to the
overall operation of the application was 8.7.
Although the results were
very positive, professors
described possible
improvements, especially in terms of facilitating that the ASR module of the
application was active since the same moment of loading the different screens of the
app, and extend the users’ adaptation functionalities of the application to cover a
greater number of possibilities.
Regarding the objective assessment, the value of the SR (97.15%) shows the
correct interaction with the application in the great majority of use scenarios and the
good operation of the different modules in the proposed (especially in the case of
the ASR and SLU modules, and the dialog manager). The strategies used by the
application for the detection and correction of errors through the use of different
types of conﬁrmations and the possible initiatives of the dialog have also allowed to
obtain a percentage of 94.12% for the ECR measure.
Table 1 Questionnaire designed for the evaluation of the application
Technical quality
TQ01. The interactivity is good enough
TQ02. It is easy to use the system
TQ03. I know what to do at each moment
TQ04. The application displays an adequate amount of information on the screen
TQ05. The arrangement of information on the screen is logical
TQ06. The application can be helpful
TQ07. The visual interface of the application is attractive
TQ08. The app has reacted in a consistent way
TQ09. The application does not distract or interfere when it complements the activities
TQ010. The verbal feedback that is provided is adequate
Didactic potential
DP01. The system fulﬁlls the objective of appreciate the educative contents that are provided
DP02. The proposed activities are very important to achieve this objective.
DP03. The design of the activities was adequate according to the requirements of the subjects
DP04. The application supports signiﬁcant learning
DP05. The feedback provided by the agent improves the learning and teaching processes
DP06. The system encourages correcting and improving after errors
108
D. Griol and Z. Callejas

6
Conclusions and Future Work
We have discussed in this chapter the rich variety of applications of data analytics
and multimodal conversational interfaces to education, covering a number of
objectives and strategies that include the development of tutoring assistants,
application for learning languages, the design of pedagogical companions and
educational agents, etc. The beneﬁts reported for experts and students include an
improvement in the evaluations, enhanced motivation, strengthen compromise and
deeper training of metacognitive skills. We have also covered different topics
related to the use of data analytics addressed to solve these challenges, paying
special attention to aspects related to their design.
We have also described our own efforts towards providing a general framework
for the development of conversational systems for education, and we have illus-
trated how it can be used to develop a mobile application for university students.
The results of the preliminary assessment of the developed app shows the range of
possible application uses of our framework for developing educational conversa-
tional interfaces providing different pedagogical contents.
For future research, additional work is needed in several directions to make these
systems more usable by a wider range of potential users and educative purposes.
For example, the use of data analytics to develop emotional conversational agents
represents a promising ﬁeld of research, as emotions play a key role in
Table 2 Results of the evaluation of the application by experts
Min/Max
Average
Std. deviation
TQ01
3/5
4.11
0.69
TQ02
3/4
3.63
0.51
TQ03
4/5
4.87
0.41
TQ04
5/5
5.00
0.00
TQ05
4/5
4.71
0.44
TQ06
4/5
4.79
0.35
TQ07
4/5
4.81
0.33
TQ08
4/5
4.47
0.53
TQ09
4/5
4.85
0.41
TQ10
4/5
4.63
0.46
DP01
5/5
5.00
0.00
DP02
4/5
4.62
0.51
DP03
4/5
4.86
0.34
DP04
5/5
5.00
0.00
DP05
4/5
4.62
0.48
DP06
4/5
4.81
0.32
SR
CR
ECR
97.15%
11.55%
94.12%
Increasing the Role of Data Analytics in m-Learning …
109

decision-making processes, perception and human-to-human interaction. Also, a
very interesting trend is multimodal social systems which rely on the fact that in
real settings people do not only speak about topics concerned with the task at hand,
but also about other open topics. Hence, additional efforts must be made by the
research community to make conversational agents more human-like employing
dialog strategies based on this kind of very genuine human behavior.
References
Ai, H., Littman, D., Forbes-Riley, K., Rotaru, M., Tetreault, J., & Purandare, A. (2006). Using
systems and user performance features to improve emotion detection in spoken tutoring
dialogs. In Proceedings of 9th International Conference on Spoken Language Processing
(Interspeech ‘06-ICSLP) (pp. 797–800). Pittsburgh, USA.
Aleven, V., Ogan, A., Popescu, O., Torrey, C., & Koedinger, K. (2004). Evaluating the
effectiveness of a tutorial dialog system for self-explanation. In Proceedings of 7th
International Conference on Intelligent Tutoring Systems (ITS’04), (pp. 443–454). Maceió,
Alagoas, Brazil.
Aimeur, E., Dufort, H., Leibu, D., & Frasson, C. (1992). Some justiﬁcations for the learning by
disturbing strategy. In Proceedings of 8th World Conference on Artiﬁcial Intelligence in
Education (AI-ED’97), (pp. 119–126). Kobe, Japan.
Bailly, G., Raidt, S., & Elisei, F. (2010). Gaze, conversational agents and face-to-face
communication. Speech Communication, 52(6), 598–612.
Baylor, A., & Kim, Y. (2005). Simulating instructional roles through pedagogical agents.
International Journal of Artiﬁcial Intelligence in Education, 15(2), 95–115.
Becker, R., Caceres, R., Hanson, K., Isaacman, S., Loh, J., Martonosi, M., et al. (2013). Human
mobility characterization from cellular network data. Communications of the ACM, 56(1), 74–
82.
Bickmore, T. (2003). Relational Agents: Effecting Change through Human-Computer Relation-
ships. Ph.D. thesis Media Arts and Sciences, Massachusetts Institute of Technology,
Cambridge, USA.
Bickmore, T., & Picard, R. (2005). Establishing and maintaining long-term human-computer
relationships. ACM Transactions on Computer Human Interaction, 12, 293–327.
Callejas, Z., Griol, D., & López-Cózar, R. (2012). Merging intention and emotion to develop
adaptive dialogue systems. Communications in Computer and Information Science, 328, 168–
177.
Calvo, R. E., Riva, G., & Lisetti, C. L. (2014). Affect and wellbeing: Introduction to special
section. IEEE Transactions Affective Computing, 5(3), 215–216.
Cassell, J., Sullivan, J., Prevost, S., & Churchill, E.F. (2000) Embodied Conversational Agents.
The MIT Press.
Cavazza, M., de la Camara, R.-S., & Turunen, M. (2010). How Was Your Day? A
Companion ECA. In Proceedings of 9th International Conference on Autonomous Agents
and Multiagent Systems (AAMAS’10), (pp. 1629–1630). Toronto, Canada
Chu, S.-W., O’Neill, I., Hanna, P., & McTear, M. (2005). An approach to multistrategy dialogue
management. In Proceedings of European Conference on Speech Communication and
Technology (Interspeech’05-Eurospeech), (pp. 865–868). Lisbon, Portugal.
Cole, R., Mariani, J., Uszkoreit, H., Varile, G. B., Zaenen, A., Zampolli, A., & Zue, V. (Eds.).
(1997). Survey of the state of the art in human language technology. Cambridge University
Press.
110
D. Griol and Z. Callejas

Cuayáhuitl, H., Renals, S., Lemon, O., & Shimodaira, H. (2006). Reinforcement learning of
dialogue strategies with hierarchical abstract machines. In Proceedings of IEEE/ACL Workshop
on Spoken Language Technology (SLT’06), Palm Beach, Aruba, (pp. 182–186.
Dillenbourg, P., & Self, J. (1992). People power: A human-computer collaborative learning
system. In Proceedings of Second International Conference on Intelligent Tutoring Systems
(ITS ‘92), (pp. 651–660). Montréal, Canada.
D’Mello, S., Craig, S., Gholson, B., Frankin, S., Picard, R., & Graesser, A. (2005). Integrating
affect sensors in an intelligent tutoring system. In Proceedings of Workshop on Affective
Interactions: The Computer in the Affective Loop (IUI’05), (pp. 7–13). San Diego, California,
USA.
Dowding, J., Clancey, W., & Graham, J. (2006). Are You Talking to Me? Dialogue systems
supporting mixed teams of humans and robots. In Proceedings of AIAA Fall Symposium
Annually Informed Performance: Integrating Machine Listing and Auditory Presentation in
Robotic Systems, (pp. 22–27). Washington DC, USA.
Edlund, J., Gustafson, J., Heldner, M., & Hjalmarsson, A. (2008). Towards human-like spoken
dialogue systems. Speech Communication, 50(8–9), 630–645.
Elhadad, M., & Robin, J. (1996). An overview of surge: A reusable comprehensive syntactic
realization component (pp. 1–4). Philadelphia, USA: Proceedings of the Eight International
Natural Language Generation Workshop.
Feng, D., Jeong, D. C., Krämer, N. C., Miller, L. C., & Marsella, S. (2017). Is It Just Me?:
Evaluating attribution of negative feedback as a function of virtual instructor’s gender and
proxemics. In: Proceedings of AAMAS Conference, (pp. 810–818). Sao Paulo, Brazil.
Fryer, L., & Carpenter, R. (2006). Bots as Language Learning Tools. Language Learning and
Technology, 10(3), 8–14.
Gorostiza, J., & Salichs, M. (2011). End-user programming of a social robot by dialog. Robotics
and Autonomous Systems, 59, 1102–1114.
Graesser, A., Chipman, P., Haynes, B., & Olney, A. (2005). AutoTutor: An intelligent tutoring
system with mixed-initiative dialog. IEEE Transactions in Education, 48, 612–618.
Graesser, A., Person, N., & Harter, D. (2001). Teaching Tactics and Dialog in AutoTutor.
International Journal of Artiﬁcial Intelligence in Education, 12, 23–39.
Graesser, A., Wiemer-Hastings, K., Wiemer-Hastings, P., & Kreuz, R. (1999). AutoTutor: A
Simulation of a Human Tutor. Journal of Cognitive Systems Research, 1, 35–51.
Gratch, J., Rickel, J., Andre, J., Badler, N., Cassell, J., & Petajan, E. (2002). Creating interactive
virtual humans: some assembly required. In Proceedings of IEEE Conference on Intelligent
Systems, (pp. 54–63). Varna, Bulgaria.
Griol, D., Callejas, Z., López-Cózar, R., & Riccardi, G. (2014). A domain-independent statistical
methodology for dialog management in spoken dialog systems. Computer Speech &
Language, 28(3), 743–768.
Griol, D., Hurtado, L. F., Segarra, E., & Sanchis, E. (2008). A statistical approach to spoken dialog
systems design and evaluation. Speech Communication, 50(8–9), 666–682.
Heffernan, N. (2003). Web-Based evaluations showing both cognitive and motivational beneﬁts of
the Ms. Lindquist Tutor. In Proceedings of International Conference on Artiﬁcial Intelligence
in Education (AIEd2003), (pp. 115–122). Sydney, Australia.
Johnson, W., Labore, L., & Chiu, Y. (2004). A Pedagogical Agent for Psychosocial Intervention
on a Handheld Computer (pp. 22–24). Arlington, Virginia, USA: Proceedings of AAAI Fall
Symposium on Dialogue Systems for Health Communication.
Kerly, A., Ellis, R., & Bull, S. (2008a). CALMsystem: A dialog system for learner modelling.
Knowledge Based Systems, 21, 238–246.
Kerly, A., Ellis, R., & Bull, S. (2008b). Conversational Agents in E-Learning. In Proceedings of
27th SGAI International Conference on Artiﬁcial Intelligence (AI-2007), (pp. 169–182).
Cambridge, USA.
Kumar, R., & Rose, C. (2011). Architecture for building dialog systems that support collaborative
learning. IEEE Transactions on Learning Technologies, 4, 21–34.
Increasing the Role of Data Analytics in m-Learning …
111

Latham, A., Crockett, K., McLean, D., & Edmonds, B. (2012). A conversational intelligent
tutoring system to automatically predict learning styles. Computers & Education, 59, 95–109.
Lemon, O., Georgila, K., & Henderson, J. (2006). Evaluating Effectiveness and Portability of
Reinforcement Learned Dialogue Strategies with real users: the TALK TownInfo Evaluation.
In Proceedings of IEEE/ACL Workshop on Spoken Language Technology (SLT’06), (pp. 178–
181). Palm Beach, Aruba.
Litman, D., & Silliman, S. (2004). ITSPOKE: An Intelligent Tutoring Spoken Dialog System. In
Proceedings of Human Language Technology Conference: North American Chapter of the
Association for Computational Linguistics (HLT-NAACL-2004), (pp. 5–8). Boston, Mas-
sachusetts, USA.
López-Cózar, R., & Araki, M. (2005). Spoken. Multilingual and Multimodal Dialogue Systems.
Development and Assessment: Wiley.
Mairesse, F., Gasic, M., Jurcícek, F., Keizer, S., Thomson, B., Yu, K., Young, & S.J. (2009).
Spoken language understanding from unaligned data using discriminative classiﬁcation
models. In Proceedings of IEEE International Conference on Acoustics, Speech, and Signal
Processing (ICASSP’09). (pp. 4749–4752). Taipei, Taiwan.
McCrae, R. R., & John, O. P. (1992). An introduction to the ﬁve-factor model and its applications.
Journal of Personality, 60(2), 175–215.
McTear, M. F., Callejas, Z., & Griol, D. (2016). The Conversational Interface. Springer, New
York, U.S.A.
Meza-Ruíz, I.V., Riedel, S., & Lemon, O. (2008). Accurate statistical spoken language
understanding from limited development resources. In Proceedings of IEEE International
Conference on Acoustics, Speech, and Signal Processing (ICASSP’08). (pp. 5021–5024). Las
Vegas, Nevada, USA.
Minker, W. (1998). Stochastic versus rule-based speech understanding for information retrieval.
Speech Communication, 25(4), 223–247.
Muñoz, A., Lasheras, J., Capel, A., Cantabella, M., & Caballero, A. (2015). Ontosakai: On the
optimization of a learning management system using semantics and user proﬁling. Expert
Systems with Applications, 42, 5995–6007.
Nagy, P., & Németh, G. (2016). Improving HMM speech synthesis of interrogative sentences by
pitch track transformations. Speech Communication, 82, 97–112.
O’Halloran, K. (2015). The language of learning mathematics: A multimodal perspective. Journal
of Mathematical Behavior, 40, 63–74.
Oh, A., & Rudnicky, A. (2000). Stochastic language generation for spoken dialog systems. In
Proceedings of ANLP/NAACL Workshop on Conversational Systems, Seattle, (pp. 27–32).
Washington, USA.
Oulasvirta, A., Rattenbury, T., Ma, L., & Raita, E. (2012). Habits make smartphone use more
pervasive. Personal and Ubiquitous Computing, 16(1), 105–114.
Pérez-Marín, D., & Pascual-Nieto, I. (2011). Conversational Agents and Natural Language
Interaction: Techniques and Effective Practices. Hershey, PA, USA: IGI Global.
Pon-Barry, H., Schultz, K., Bratt, E.-O., Clark, B., & Peters, S. (2006). Responding to student
uncertainty in spoken tutorial dialog systems. IJAIED Journal, 16, 171–194.
Rabiner, L. R., & Juang, B. H. (1993). Fundamentals of speech recognition. Prentice-Hall.
Reiter, E. (1995). NLG vs. templates. In Proceedings of the Fifth European Workshop in Natural
Language Generation, (pp. 95–105). Leiden, Netherland.
Roda, C., Angehrn, A., & Nabeth, T. (2001). Dialog systems for Advanced Learning: Applications
and Research. In Proceedings of BotShow’01 Conference, Paris, France, pp. 1–7.
Rosé, C., Moore, J., VanLehn, K., & Allbritton, D. (2001). A Comparative Evaluation of Socratic
versus Didactic Tutoring. In Proceedings of 23rd Annual Conference of the Cognitive Science,
(pp. 869–874). Edinburgh, Scotland.
de Rosis, F., Cavalluzzi, A., Mazzotta, I., & Novielli, N. (2005). Can embodied dialog systems
induce empathy in users? In Proceedings of AISB’05 Virtual Social Characters Symposium,
(pp. 1–8), Hatﬁeld, UK.
112
D. Griol and Z. Callejas

Salse, M., Ribera, M., Satorras, R., & Centelles, M. (2015). Multimodal campus project: Pilot test
of voice supported reading. Procedia - Social and Behavioral Sciences, 196, 190–197.
Schmitt, A., Ultes, S. (2015). Interaction quality: Assessing the quality of ongoing spoken dialog
interaction by experts - And how it relates to user satisfaction. Speech Communication, 74,
12–36.
Schuller, B. W., & Batliner, A. M. (2013). Computational Paralinguistics: Emotion, Affect and
Personality in Speech and Language Processing. John Wiley and Sons.
Sidner, C., Kidd, C., Lee, C., & Lesh, N. (2004). Where to look: a study of human-robot
engagement. In Proceedings of 9th International Conference on Intelligent user interfaces
(IUI’04), Funchal, Portugal, pp. 78–84.
Theobalt, C., Bos, J., Chapman, T., Espinosa-Romero, A., Fraser, M., Hayes, G., Klein, E., &
Reeve, R. (2002). Talking to Godot: dialogue with a mobile robot. In Proceedings of IEEE/RSJ
International Conference on Intelligent Robots and Systems (IROS2002), (pp. 1338–1343).
Lausanne, Switzerland.
Tur, G., & De Mori, R. (2011). Spoken Language Understanding: Systems for Extracting
Semantic Information from Speech. Wiley.
Vaquero, C., Saz, O., Lleida, E., Marcos, J., & Canalís, C. (2006). VOCALIZA: An application for
computer-aided speech therapy in Spanish language. In: Proceedings of IV Jornadas en
Tecnología del Habla, (pp. 321–326). Zaragoza, Spain.
Wang, N., & Johnson, L. (2008). The Politeness Effect in an intelligent foreign language tutoring
system. Proc. of Intelligent Tutoring Systems Conference (ITS’08), Montreal, Canada,
pp. 270–280.
Wang, Y., Wang, W., & Huang, C. (2007). Enhanced Semantic Question Answering System for
e-Learning Environment. n Proceedings of 21st International Conference on Advanced
Information Networking and Applications (AINAW’07), (pp. 1023–1028). Niagara Falls,
Canada.
Williams, J., & Young, S. (2007). Partially Observable Markov Decision Processes for Spoken
Dialog Systems. Computer Speech & Language, 21(2), 393–422.
Increasing the Role of Data Analytics in m-Learning …
113

Enhancing Virtual Learning Spaces:
The Impact of the Gaming Analytics
Anastasios Karakostas, Anastasios Maronidis, Dimitrios Ververidis,
Efstathios Nikolaidis, Anastasios Papazoglou Chalikias,
Spiros Nikolopoulos and Ioannis Kompatsiaris
Abstract Online virtual labs have been important to educational practice by
providing students with distance courses that otherwise would be difﬁcult to be
offered. However, the majority of them cannot be easily applied to different courses
or pedagogical approaches. In order to overcome this, we propose a high-level,
easy-to-use authoring tool that will allow building course-independent high-standard
virtual labs. This solution is based on learning and gaming analytics. In the gaming
industry, there have been developed strong game analytics methods and tools, which
could be easily transferred into the learning domain. Game analytics monitor the
users’ activity; model their current behavior through the use of shallow analytics and
predict the future behavior of the users through the use of deep analytics. We
propose that both of these approaches combined with visualization methodologies
will offer insights on what features are important and what functionalities users
expect to ﬁnd in a virtual lab.
A. Karakostas (✉) ⋅A. Maronidis ⋅D. Ververidis ⋅E. Nikolaidis ⋅A. P. Chalikias ⋅
S. Nikolopoulos ⋅I. Kompatsiaris
Information Technologies Institute, Centre for Research and Technology Hellas, Thermi,
Greece
e-mail: akarakos@iti.gr
A. Maronidis
e-mail: amaronidis@iti.gr
D. Ververidis
e-mail: ververid@iti.gr
E. Nikolaidis
e-mail: stathis.nikolaidis@iti.gr
A. P. Chalikias
e-mail: tpapazoglou@iti.gr
S. Nikolopoulos
e-mail: nikolopo@iti.gr
I. Kompatsiaris
e-mail: ikom@iti.gr
© Springer International Publishing AG 2018
S. Caballé and J. Conesa (eds.), Software Data Engineering for Network eLearning
Environments, Lecture Notes on Data Engineering and Communications
Technologies 11, https://doi.org/10.1007/978-3-319-68318-8_6
115

1
Introduction and Motivation
Information and Communication Technologies (ICT) has enabled the spread of
personalized learning across the largest part of community. One of the advantages
of technology-enhanced learning is the ability to offer automated methods, which
are easy to be accessed for example through the web. In this context, recently there
have been developed a number of virtual labs emulating real lab environments,
where users can accomplish a number of learning tasks and conduct various
experiments with no cost and risk. However, the big challenge of such labs is to ﬁnd
effective ways to boost the experience factor in order to motivate the engagement of
students with the learning system and prevent them from churning out. In this
context, the overarching goal of our framework is to enhance the design of virtual
labs leading to optimal personalized learning processes. Towards this end, the
framework consists of an authoring environment, which equipped with data ana-
lytics methods and visualization tools that have been developed and reached
maturity in the gaming industry and is suitable for iteratively evolving the design of
virtual labs and for dynamically adapting the learning content to the users of the
virtual labs.
This environment operates in a high level requiring minimal programming skills,
so as to be a powerful tool in the hands of every designer (e.g., tutor) who will be
able to construct a lab that meets the requirements of the users. For the optimization
of a virtual lab, it is imperative that the designer has access to data and information
about learners and their contexts. In our framework we make use of available data
logs encoding user activities that have been acquired from already existing virtual
labs. Moreover, apart from these data, it is also needed to keep track of the activity
of users in new labs so as to keep all crucial information updated providing
feedback to the process of enhancing virtual labs. Understanding the data obtained
from both existing and new virtual labs, is possible to be reached by means of
Learning Analytics (LA).
Learning analytics is a useful tool that allows for deciphering the trails that
learners leave during their engagement with the learning system. They can have a
strong impact on the learning process (e.g., for educators to identify at-risk learners,
or for learners to reﬂect on their learning process), but also allow for more informed
decisions on the institutional, national and international level (e.g., for determining
the value generated by faculty activity, or for implementing what-if scenarios using
predictive behavior models). Given the volatile nature of the data and information
acquired during the learning process, analytics must be conducted at a pace that can
instantly inform decision-making in both aspects of feedback to learner and
adjustment of the learning system. This means obtaining real-time or near real-time
answers from data by having automated systems that continually update user
proﬁles, contextual information, etc.
Towards this end, we anticipate that the gaming industry and its knowledge
resources can give the answer. Indeed, due to the recent intense activity in games,there
have recently been developed strong game analytics methods and tools, which could
116
A. Karakostas et al.

be easily and seamlessly transferred into the learning domain. Gameanalytics have the
potential to contribute in two ways. First by monitoring the activity of the users and
modeling their current behavior through the use of shallow analytics (simple statistics
on tracked data). Second by predicting in a reliable manner the future behavior of the
users through the use of deep analytics (outcome of the application of machine
learning algorithms). Both of these approaches combined with state-of-the-art visu-
alization methodologies will offer insights on what features are important and what
functionalities users expect to ﬁnd in a virtual lab. These insights will allow for
(a) optimizing the design and implementation of a virtual lab and (b) enhancing the
learning process in a virtual lab by offering personalized learning content.
Summarizing the above, the overall concept of our solution is based on iterating
the process of improving virtual labs through a pipeline that (i) starts from the
current version of a lab, (ii) collects shallow analytics extracted from user behav-
ioral data, (iii) digs deeper into the obtained analytics using machine learning
methods, (iv) integrates the obtained information under the authoring tool, (v) em-
ploys the authoring tool to build an improved version of the virtual lab and ﬁnally
(vi) iterates the above process.
2
Background and Literature Review
2.1
Learning Analytics and Game Analytics
Learning analytics focus on assessing the learning process, and thus are useful on a
course level (for e.g. identifying the learning trails and engagement of individual
learners) as well as on a department level (for e.g. predicting dropout rates and
clustering learners) (Siemens and Long 2011). The critical dimensions of learning
analytics are the stakeholders (e.g. learners or tutors), the objectives (e.g. reﬂec-
tion), the data (e.g. which measures are important, whether a learner’s history will
be considered), the instruments (e.g. data visualization or statistical analysis), the
external constraints (privacy, ethics, etc.), and the internal limitations (e.g. whether
the analytics can be interpretable by the stakeholders) (Greller and Drachsler 2012).
Choosing along each of these dimensions can severely affect the quality, accuracy
but also the usefulness of the analytics. As indicative examples along the dimen-
sions of data and instruments, Marist College used demographic data (such as the
learner’s age and gender), general scores (such as the learner’s SAT score) and
LMS-speciﬁc data (such as the number of times a learner accessed a Lesson section)
as input data, in order to assess a learner’s academic risk (output data) (Lauria et al.
2013). Another example is SICKO, a web-based educational game where positive
and negative feedback is provided by a virtual doctor; the included Surgery Mode
mini-game has the players determine the fate of a virtual patient by answering
multiple-choice questions (Shieh et al. 2012; Tsui et al. 2014). The game’s collected
data include an action log of the learner’s answers to the multiple-choice questions,
Enhancing Virtual Learning Spaces …
117

along with the elapsed time between answers and score progression. Since the score
was a direct indication of the number of right and wrong questions (and rewards to
score is scaled to the difﬁculty of the question), the score could be used directly as
output data for assessing the success of students. Moreover, by visualizing the
distribution of scores in a ﬁgure (with the y-axis being the number of students
achieving this score), the stakeholder (in this case the game designers) could assess
whether there were many learners who under-performed—in which case the game
may have required a re-design.
Of particular importance our solution is the data and instruments dimensions, i.e.
which data from a learner’s interaction with the LMS can or should be collected,
and which methods are used to inform the stakeholder of this interaction. In our
framework, the stakeholders are the educators as well as the designers of the new
virtual labs (these roles may overlap). Therefore, importance is also placed on
which data could be valuable to these stakeholders, and how to visualize it in order
to best inform them of the interaction process of learners. For instance, educators
using the learning analytics to observe how a large number of learners interact with
the LMS (Learning Management System) are likely to be interested in a single
metric denoting number of problem cases with insufﬁcient learning which are likely
to dropout. In such cases, game analytics can be employed to derive a likelihood of
dropout; similar methods are used to predict when players stop playing a particular
game, i.e. churn prediction (Runge et al. 2014). On the other hand, a designer of the
new virtual lab will not ﬁnd such a metric sufﬁcient; instead, more informative data
include session times (long or short session times may indicate problems) or a
visualization of which elements of the interface were most clicked at (for identi-
fying interface design problems where some buttons were missed). Game analytics
for identifying how users engage with game development tools (Liapis et al. 2014)
can be transferred to such visualization and assessment tasks.
2.2
Visualization
The almost ubiquitous use of the computer (in any of its forms, including the
personal computer, mobile devices, surveillance systems) in all aspects of everyday
life—not least of which in education—necessitates an efﬁcient way to present this
data to different stakeholders. Similar to learning or game analytics, which identify
key performance indicators and ﬁlter the vast amounts of data into comprehensible
numbers, the visualization of data manages to distill information in an intuitive and
easy to grasp imagistic representation. The core idea behind data visualization is
that a diagram, through its use, serves as a vehicle of cognitive processes,
embodying the various aspects of the problem. The viewer’s (e.g. a stakeholder’s)
mind is extended (Clark 1998) onto the diagram and reasoning proceeds through
structural (rather than semantic or syntactical) entailment. One therefore thinks
through the diagram rather than its use as a simple image. Moreover, the possi-
bilities one sees for constructing, altering or transforming a given diagram are part
118
A. Karakostas et al.

of one’s comprehension of the diagram itself; the functions of the diagram both on
the semantic and pragmatic level are determined in part by these possibilities
(Sloman 2002). Therefore the visualization of data serves three purposes to a user
(or stakeholder): (a) to promptly understand the current state of a particular domain
based on the key performance indexes that interest them, (b) to reason about the
data, detect re-occurring patterns and predict future or unseen data by projecting the
current data into unknown problems, (c) to envision ways of changing the diagrams
themselves and the necessary steps taken to achieve that. Using more concrete
examples, the ﬁrst purpose is achieved when observing e.g. a summary of the hours
spent by each group of learners (class) on a speciﬁc virtual lab, the second purpose
is achieved when observing e.g. a timeline of the correct responses of one learner
which shows an increase in correct responses from the beginning of the semester
until now and the educator can extrapolate a similar trend occurring until the end of
the semester. Finally, the third purpose can be achieved by observing e.g. how
students are automatically clustered based on their engagement time with the virtual
lab, showing a cluster of students which have low participation and low correct
answers; the educator can attempt to indirectly affect this diagram by attempting to
reinforce the learners’ engagement with the tool and improve their rate of correct
answers by either in-class seminars or by adapting the virtual lab directly to provide
more help to at-risk learners.
Commercial games often collect a broad range, large volume of data from their
players; this is facilitated by the modern advances in network speeds and band-
widths, but also by the more robust ways that the industry has adopted to reason
about and visualize such data. The data is necessary for deriving game analytics—
including player satisfaction, game balance, and many others—which in turn act as
the key performance indicators and need to be communicated back to the relevant
stakeholders. For visualizing this data, several methods have been developed by the
game industry (and other domains): (a) displaying descriptive statistics numerically
(e.g. in a table), which can be beneﬁcial as they are easy to read and maintain the
ﬁne details (e.g. numerical precision), (b) displaying trends over time (e.g. values of
key performance indicators on a timeline) which allows stakeholders to predict
future outcomes based on current and past trends, (c) displaying heatmaps which
can easily identify user’s spatial navigation and highlight elements in level design
or interface design, (d) display groupings of players clustered based on certain
shared (actual or predicted) attributes. These visualizations do not need to exclu-
sively consider the game’s designers (or producers, in case of ﬁnancial ﬁgures of the
game) as the only stakeholders: in many massive multiplayer online games or
competitive games, the end-users (players) largely beneﬁt from such visualizations
to adjust their strategies when facing other players: this is done by e.g. observing
which hero classes are trending currently based on how often they are picked by
other players, visualized both as quantitative descriptive statistics and as a timeline.
Heatmaps “are basically frequency maps—they show how often a particular event
has occurred—traditionally a player defeat event—on a ﬁne-meshed grid, overlain a
map of the game level in question” (Drachen and Schubert 2013). Beyond player
defeat, heatmaps have often been used to visualize how players traverse a level or
Enhancing Virtual Learning Spaces …
119

interact with elements of the user interface. Finally, clustering can be used to group
similar game elements (e.g. game items, players, questionnaire responses) based on
real-world gameplay analytics collected about them in a way that elements in the
same cluster are more similar with each other than with those in other clusters: for
instance, clusters can be visualized in order to show the number and ratio of distinct
types of players by interpreting a broad range of their gameplay.
When moving from games to learning environments, the goals of data visual-
ization do not particularly change: treating either learners or educators as the pri-
mary stakeholders, visualizing data in more or less concise—yet intuitive—ways
can provide an overview of the learner’s progress and can be used to predict (by
human viewers) the outcomes of future tasks. Our approach explores how the best
practices of data visualization used in games can be used to provide information to
both stakeholders: learners and educators. For learners, data visualization of their
progress (e.g. of their correct answers) over time, possibly compared with the trends
(average values) of other learners can motivate them to continue using the virtual
lab (averting churn). For educators, who take a similar role to the game designer in
game industry terms, the full spectrum of data visualization can be used very
effectively. For educators, displaying descriptive statistics numerically can provide
them with enough information (at a glance) during the progress of a lecture to
intervene directly, while visualizations of progress over time can allow them to
predict the outcomes of future exercises (since they have an overview of the dif-
ﬁculty of the virtual labs they designed) and take precautions by editing upcoming
labs. When designing a new virtual lab, on the other hand, the educator can make
use of heatmaps of learners from previous virtual labs’ interfaces to improve
usability of the interface in future iterations.
3
Research Motivation
Work on coupling game analytics with learning analytics we will advance the
research in both ﬁelds, as (a) expertise in deep game analytics which can predict
future trends can be transferred directly into predicting learning outcomes of
learners engaging in virtual labs (using learning analytics as input or predicted
output); (b) the translation of play metrics in educational settings will provide
signiﬁcant insights going forward in games which are used for educational pur-
poses, such as SICKO (Tsui et al. 2014); (c) application and testing of deep game
analytics in educational software can result in new algorithms which can be used for
any type of data, including game industry applications, personalized playlists and
product suggestions etc.
Moreover, during the course of our framework, the research and implementation
of data visualization practices in the realm of educational software will advance the
state of the art in this domain in several ways: (a) new methods of visualizing the
ﬁndings of deep game analytics (or deep learning analytics) for non-expert users in
machine learning and artiﬁcial intelligence will open up the user base which can
120
A. Karakostas et al.

beneﬁt from such methods; (b) new ﬁndings on usability of interfaces using
sophisticated heatmaps, play-traces, and quantitative results can be transferred back
to the game industry (where the focus is less on interface elements and more on
spatial navigation); (c) can study the impact of thinking through diagrams in
educators
and
learners
(possibly
comparing
between
the
two)
from
a
human-computer interaction perspective, increasing the knowledge going forward
for the needs and priorities of these types of stakeholders.
4
Proposed Approach
Our framework is aimed at providing education with all those technologies
necessary to enhance virtual labs from the perspective of both a teacher and a
student. Towards this direction, we adopt a data-driven approach by transferring to
education well-deﬁned and mature data analytics technologies, which have shown
extremely high performance in the gaming industry. The use of a data-driven
approach is motivated by the continuously increasing need to apply learning
Fig. 1 Concept diagram
Enhancing Virtual Learning Spaces …
121

analytics to massive, varied and time-sensitive data, acquired from the engagement
of users with virtual learning environments. Learning analytics focus on assessing
the learning process, and thus are useful on a course level, e.g. for identifying the
learning trails and engagement of individual learners, as well as on a department
level, e.g. for predicting dropout rates and clustering learners (Long and Siemens
2011). In this context, the critical dimensions are
• the data generated by the learners’ activities;
• the analytics tools that are used for collecting, aggregating, interpreting, visu-
alizing and understanding these data; and
• an authoring environment that integrates the above in the process of improving
virtual labs. In the following we give a description of these critical dimensions
(Fig. 1).
The virtual labs authoring tool consists of three components, namely (a) the
front-end that shows simple visualizations to the educators in order to author games,
(b) the back-end that shows advanced visualizations to administrators of the website
and how to deﬁne new Game Project, Scenes, and Assets3D taxonomies, and
(c) the assembler-compiler that combines all Game Scenes, Assets and Settings into
a Unity3D project and compiles it into a game. These components will be outlined
next.
4.1
Front-End Interface
The front-end interface is a user-friendly Graphical User Interface (GUI) that allows
the management of Game Projects, Scenes, and 3D Assets, effectively targeting
novice users, that have limited or no knowledge of game authoring processes.
Firstly, after successfully logging into the website, the user enters the Game
Project Manager screen that offers functionality to create a new game, and edit or
Fig. 2 Overall organization of the front-end GUI
122
A. Karakostas et al.

delete an existing game. Next, the Scenes Manager screen is shown, in which an
existing scene can be edited or deleted, and a new scene can also be created. There
is a link to the 3D Asset Manager screen and if desired, the user can compile the full
game from this screen. If the user selects to edit a scene, then the Scene Editor
screen appears, with contents that depend on the type of scene. For 2D scenes a
form is shown that the user can submit to change its data. For 3D scenes a fully
functional 3D editor is show so that the user can spatially manipulate and arrange
3D assets in a plane. All screens are described in greater detail in the subsequent
sections that follow (Fig. 2).
5
Game Project Manager
An implementation of the Game Project Manager is shown in Fig. 3. In this screen,
the educator can create a new game project and delete or edit an existing one. To
access a preexisting project, the educator must click on one of the list entries at the
left Projects section. There is also a delete button that creates a warning popup
window to make sure that the educator really wants to delete this project. A new
game project can be created by entering the title of the game project and by
selecting its type. When clicking on the CREATE button, a new game is suc-
cessfully created and the educator is transported to the Scenes Manager screen.
Fig. 3 Implementation of the Game Project Manager
Enhancing Virtual Learning Spaces …
123

6
Scenes Manager
As shown in Fig. 4, in this interface each scene of the game is represented as a card.
Each card has a thumbnail of the scene that also serves as a link to the scene editor,
a scene title, a description and two buttons for the edit and delete scene function-
alities. There are some scenes that the game project manager creates by default.
These scenes are required and cannot be deleted so the delete functionality is
disabled, although all scenes that are created by the educator are deletable.
In this screen there is also the ‘Compile Game’ button that when clicked initiates
the compilation process of the whole game. A new screen appears that allows the
educator to compile the game in various formats such as WebGL, Windows, Mac,
or Linux. Upon successful compilation a link is provided to download the game in
zip format. For WebGL games, a second link also appears to play the game in the
web browser.
By clicking on the ‘Add New Scene’ button, a new section expands that enables
the creation of a new scene by ﬁlling in the necessary information. This information
includes a title, a description and an image that serves as a scene thumbnail.
7
Scene Editor
When the scene that is being edited is two-dimensional, the 2D scene editor
launches that is in fact a form with ﬁelds that vary according to the scene. If the
scene that is being edited is three-dimensional then the 3D scene editor launches.
Fig. 4 Implementation of the scenes manager
124
A. Karakostas et al.

We have developed a web-based 3D Editor to enable modiﬁcations of a scene
through a web-browser. In order to achieve this fast, we used the three.js1 library
that allows to develop 3D graphic elements using HTML5 and WebGL through
high level commands. Three.js allows saving a scene in the JSON format where we
have standardized our own custom structure that serves the need of converting the
scene setup to the Unity3D scene format. A screenshot of the 3D editor can be seen
in Fig. 5.
The 3D Editor screen consists of three basic parts, namely the 3D view of the
Scene where the user can manipulate 3D objects on a plane, the left panel that
features controls and editable parameters of each selected object, and a right panel
where all available 3D assets are listed and can be dropped inside the 3D plane,
edited or deleted. There is also a search function for the scenarios where one Game
Project has numerous 3D assets.
The main functionality of the 3D editor is to allow an educator to drag and drop
3D assets on the 3D plane. This action adds an instance of the 3D asset to the scene.
Multiple instances of the same asset can exist in the scene, i.e. multiple wind
turbines. The educator can edit the rotation, position, and scale of an instance either
through GUI controls (gizmos) or by entering numerical values for a more accurate
result in the left panel. Other functionalities supported are typical 3D editing
functionalities such as (a) view the scene either in 3rd person view or 1st person
view; (b) orbit, pan, or zoom to an object for a better angle view; and (c) select
object with raycasting (click on 3D items).
Fig. 5 A scene editor for the web using Three.js
1http://threejs.org.
Enhancing Virtual Learning Spaces …
125

The 3D editor, apart from the editing functionalities, can convert a three.js scene
into a JSON ﬁle. A three.js scene is comprised of objects in the browser’s memory
that are structured in a tree like format with parameters such as object name,
translation, rotation and scale. We have developed a JSON converter function that
stores these parameters inside a JSON ﬁle following a protocol. We have deﬁned
our own protocol as for the time being there is no standard format for saving three.js
3D scenes. If the educator re-opens the 3D editor for a particular scene, then the
JSON ﬁle is loaded and the three.js scene is recreated exactly like it was saved the
last time it was edited. We decided to use the JSON format instead of the Uni-
ty3D YAML scene format because it is more compatible with web technologies
such as three.js. Only when the game is compiled, the JSON scene is converted into
a Unity3D YAML scene.
In a recent version of the plugin, game analytics are embodied in a tab next to 3D
editor as shown in Fig. 6. The analytics contain information regarding the entities
Fig. 6 Analytics are displayed in a tab next to scene 3D editor
126
A. Karakostas et al.

such as the game, the scenes, and the assets. The parameters for each entity regard
the frequency of use, the duration of use, game score statistics etc.
8
3D Asset Manager
The educator can upload a new 3D asset using the “Add new Asset” button, which
is only available for senior educators, and is accessible from the Scenes Manager or
the 3D editor screens. The 3D Asset Manager screen is shown in Fig. 7. Each 3D
Asset has various ﬁelds such as ﬁles for 3D representation (obj, mtl, jpg texture)
and various ﬁelds for its parameters (e.g. power consumption mean and variance).
The category of the asset affects the kind of ﬁelds of the asset.
Analytically, the steps to create a new 3D asset are (a) select the category of the
asset based on the type of the current game project, (b) write the title and an
optional description, (c) upload the 3D representation ﬁles that include an mtl
(material) ﬁle, an obj (mesh) ﬁle, and a jpg texture ﬁle, and (d) set the asset ﬁelds
based on its category. The 3D model is rendered in a panel and the user can save a
snapshot of the 3D model to be used as an icon during the 3D Editor.
A 3D asset can be edited, when clicking on the ‘Edit’ button from the 3D Editor
(Scene Editor) screen. All information of a 3D asset can be edited except from its
category.
Fig. 7 The 3D Asset Manager for the energy “Consumer” asset of the “Energy” Game Type
Enhancing Virtual Learning Spaces …
127

8.1
How a Generated Game Looks like
The games contain by default certain 2D scenes, which allow the basic function-
alities of games with GUI elements. These 2D scenes are outlined in the following:
Fig. 8 Main menu scene
Fig. 9 Login scene
128
A. Karakostas et al.

Main Menu—It is the central point of the game where the learner can select
what to do next as shown in next ﬁgure. The title, e.g. “Renewable Energy VR Lab”
and the image below the title are editable from the virtual labs authoring tool. All
the other GUI elements are ﬁxed. An option is provided to hide Login, Settings and
Help if the educator does not wish to have these buttons available (Fig. 8).
Login—Button loads a Scene where the input ﬁelds for the learner name, the
surname, and the school can be found. The information is encrypted before
transmission into a unique identiﬁer that can not be inversed, i.e. there it is not
possible to extract a learner’s name from its encrypted identiﬁer. This scene is not
editable from the authoring tool (Fig. 9).
Settings—Button loads a scene that provides controls for changing screen size
and details level as shown in Fig. 10. This screen is useful in low-end devices that
should have a low resolution and details level in order for the game to be played
smoothly. This scene is not editable from the authoring tool (Fig. 10).
Play—Button loads a scene named as “Scene Selector” that allows the user to
select an Educational Scene to play among several choices. This scene is auto-
matically generated from the authoring tool based on how many Educational Scenes
are available. The title, description, and image of the scene are those fed as input
during the creation of each scene. The title “Select a Scene” is editable. Next, the
Educational Scenes are described.
Educational Scenes—These are the energy production-consumption simulation
scenes that contain the main interaction for achieving the learning objective. An
example is shown in Fig. 11. Here the energy consumers are the buildings which
Fig. 10 Scene Selector scene allows to select a “level”
Enhancing Virtual Learning Spaces …
129

are colored as red indicating that they are underpowered. Above each building, a
billboard shows the mean and the variance of the consumption of each building.
The circles in the terrain indicate candidate positions for inserting a wind turbine.
On hovering above each cycle, information of the candidate turbine is shown. On
clicking a cycle, a turbine is built over it. Depending on the size of the turbine rotor
the nearby candidate positions are destroyed automatically in a range of 1.5 times of
the rotor size. When hovering on each turbine a billboard over the turbine shows the
characteristics of the turbine as well as the current output for the current wind
speed. On bottom-left, the current state of the game is shown. This state consists of
the following metrics: the total energy produced so far, the money earned, the
required power, the generated power, and the wind (current, mean, variance). The
simulation lasts 6 min in real time that corresponds to 24 h. A turbine can be
damaged after some time, and the learner can click on it to repair it with some cost.
Upon ﬁnishing the simulation the Reward scene is shown which is described next.
Reward Scene—Contains the ﬁnal score and an overall evaluation for the
simulation session. The learner can see his or her score as the money earned,
the energy produced and the balance of among overpower, correct power, and
under-power time.
Fig. 11 The simulation of energy production-consumption constitute the educational scenes
130
A. Karakostas et al.

8.2
What the Educator Can Do with the Virtual Labs
Authoring Tool Using the “Energy” Lab Template
The “Energy” template can be used to generate an arbitrary number of games, with
an arbitrary number of Educational Scenes, and with an arbitrary number of game
objects per Educational Scene. In the following lines, we will describe which
parameters the educator can change by using the vlabs authoring tool. Brieﬂy, the
vlabs authoring tool copes with the following requirements.
• Allow the learner to select an Educational Scene to play among several Edu-
cational Scenes. Each scene has its own pros and cons that should be explained
to learner.
• The energy consumption per scene should be modiﬁable.
• The wind speed per scene should be modiﬁable.
• Wind energy turbines should have the following modiﬁable parameters
– Power Generation
– Size
– Cost to buy
The educator can do the following actions in the ﬁrst prototype of the vlabs
authoring tool
[Action A] Create multiple Educational Scenes: Each of these scenes is a
certain area where the wind energy generators can be placed.
Fig. 12 Inserting a terrain with drag-n-drop from right toolbar
Enhancing Virtual Learning Spaces …
131

[Action B] Insert a Terrain: A Terrain is a ground where turbines can be
placed. This action is feasible by drag-n-drop an Asset3D to the scene. Figure 12
shows a scene with a terrain. Only one Terrain can be placed in a scene.
The Terrain has the following ﬁelds that should be deﬁned by the educator by
pressing the “Edit” or “Create new Asset”.
[Action C] Insert a Decorator: A Decorator is a game object that can improve
the immersiveness such as “Archaeological site”, “Power lines”, “Trees”, etc. Their
category, which should be selected when creating a new asset, is named as
“Decorator”. Decorators can be dragged-n-dropped an arbitrary number of times in
the scene.
[Action D] Insert a Consumer: A Consumer is a game object that consumes
energy (e.g. a building). Several Consumers (block-of-ﬂats, single houses, facto-
ries) will be available for drag-n-drop in the scene for multiple times. The total
energy consumption is the sum of the consumption of all Consumers. A Consumer
turns red if underpowered, blue if overpowered, and normal color if correctly
powered.
[Action E] Insert a Producer: A Producer is a game object that generates
energy (e.g. a Wind Turbine or a Solar Panel). Producers can be dragged-n-dropped
several times in the game by the educator. When the game starts they do not appear
but a marker is shown on the ground to indicate to the learner that in this place
where a Producer can be built (Candidate position).
8.3
Learner Actions Allowed in the Produced Games
Here we provide a summary of the actions that learner can perform with respect to
the learning objective.
[Action A] Select an area: to place the turbine among several choices where the
pros and the cons are stated.
[Action B] Place turbines in candidate positions: A candidate position is
shown with a marker.
[Action C] Turn off a turbine—The learner can turn-off a turbine by clicking
on it when the power generation greater than the consumption.
[Action D] Repair a turbine—The learner can repair a turbine if the turbine
outputs smoke by clicking on it.
[Action E] Change simulation speed—The learner can change the simulation
speed using the top-middle dropdown button.
Other navigation and visualization features are:
• The learner can orbit-zoom-pan around the scene to see the turbines from all
sides.
• The learner can view money earned, current energy production-consumption,
and current wind speed in the lower-left panel.
132
A. Karakostas et al.

8.4
Visualization of Deep and Shallow Analytics
In order to analyze, model, and eventually visualize, learner behavior through
shallow and deep analytics, we are implementing three separate technical solutions.
The three solutions are interdependent and form a logical and operational stack, that
can be deployed as one, and hence all three of them are described in this section.
The user-facing layer of the stack is the visualization service. This service dis-
plays observed and inferred information about the learners of a given virtual lab and
informs the author of behaviors in the currently implemented version of the virtual
lab as well as potential outcomes of changes to the virtual labs. The visualization
service in turn receives its information from the shallow analytics service.
Fig. 13 Overview of the visualization, shallow analytics, and deep analytics stack. Blue: services
and their core functions; Yellow: necessary platforms; and Orange: used libraries and custom code
Enhancing Virtual Learning Spaces …
133

The shallow analytics service performs data aggregation, abstraction, and stor-
age tasks, taking raw measurements gathered from the virtual labs and turning these
into metrics that can be analyzed by the user. The shallow analytics service com-
municates with the deep analytics service that receives data, models the data, and
transmits this back to the shallow analytics service which in turn stores the results
and provides these to consumers i.e. the visualization service (Fig. 13).
8.5
Measured Data
The raw data points as they were described in Sect. 4, Table 4.1, are converted into
a number of metrics by the shallow analytics service. Depending on the use case
these may be converted on-line in the user’s browser or off-line and stored with the
data set. All events are grouped by user and session and turned into list data
structures with one list per user per session. Individual sessions are demarcated
using the “launch” event described in Table 4.1. If no further events are received
from the same user for an extensive amount of time, the session is considered
concluded.
With each list representing a series of events for each session for each learner, it
now becomes possible to leverage the other event types to evaluate the learner’s
travel path through the application, as well as calculating the ﬁve key metrics of
interest, deﬁned in previous deliverables. Further, combinations of these features
constructed from expert knowledge, as well as the raw event data, may be trans-
ferred to the deep analytics service for treatment.
9
Validation
The general objectives of piloting and evaluation tasks are to conduct a series of
small-scale test implementations of virtual labs, accompanied by the authoring,
analytics and visualization tools, in the context of well-deﬁned educational sce-
narios. This is done to evaluate the effectiveness of the developed technologies with
respect to their ability to beneﬁt educational organizations utilizing the our solution
towards the optimization of virtual lab design and functionality. The evaluation is
foreseen to provide quantitative and qualitative feedback on three separate
conditions:
a. The process of using our authoring tool as a means for building virtual labs.
b. The offered analytics and visualization tools as means of support towards
improving virtual labs.
c. The delivered virtual labs with respect to their effectiveness to meet the goals
and expectations of both teachers and students in the learning process.
134
A. Karakostas et al.

In particular, the process of using the authoring tool as a means for building
virtual labs will be evaluated by teachers and e-learning experts as test users.
Predeﬁned tasks of gradual complexity will be given to them and feedback will be
collected through various means and methods in order to evaluate the effectiveness,
usability and functionality of the authoring tool as well the overall user experience.
The evaluation outcomes will be utilized to develop, update and release an
improved version of the tool.
With respect to the support offered by the analytics and visualization tools in the
process of improving virtual labs, this will be evaluated also by teachers and
experts. Similarly, these tools’ effectiveness, usability, functionality and user
experience will be assessed towards their improvement and upgrade.
The delivered virtual labs and the learning content will be evaluated by both
teachers and students as well as experts. The evaluation process will focus on
measuring to which extent the virtual labs have achieved to motivate and to engage
students in the learning process, and also to which extent their utilization in the
classroom facilitated the teaching and learning objectives set or expected by
teachers.
10
Discussion
Analytics, apart from the gaming industry, have also been used in many different
contexts, e.g., healthcare, stock trading, web store customer analysis, retailing and
marketing optimization, just to name a few. However, there are two main reasons
that make game analytics special in the context of learning, which are described
hereunder.
Coherence between gaming and learning: Gaming and learning are intertwined
processes, both of which simultaneously appear in the early stages of a child’s
development and thereby they bear some fundamental similarities. Indeed, gaming
and learning aggregate numerous similar traits (e.g., they stimulate fantasy, trigger
curiosity, intrigue users by offering challenging goals, arouse enthusiasm, entice
users by offering several types of rewards, foster creativity through the use of
non-linear approaches, etc.), which have permitted the fusion of these processes
towards producing hybrid approaches like for instance “learning by playing” and
“edutainment”. Due to this inherent connection between gaming and learning, the
users in both of these processes generate common types of data and therefore it is
anticipated that data analytics methods that have been successfully applied to
gaming have a strong potential to also be successfully transferred to learning. Based
on the above principle, our solution will guarantee this smooth transfer by desig-
nating and using a set of learning-biased metrics for assessing the data collected
from users interacting with virtual labs.
Strength of game analytics: Due to its popularity and the demanding require-
ments of the several stakeholders involved in it, the gaming industry has revolu-
tionized the development and optimization of analytics. While conventional
Enhancing Virtual Learning Spaces …
135

analytics have been built mainly to optimize the e-commerce experience, e.g.
through naïve statistical approaches for measuring the recency, frequency and
monetary value of a user, game analytics basically focus on making a game more
engaging, e.g. through user behavior modeling, predictive analytics and visual
analytics. Along these lines, there have been developed game analytics tools
offering a variety of metrics (e.g., average session duration, performance metrics,
player demographics, etc.) that allow for adapting the game play so as to suit the
style of different game player-individuals or segments. As such analytics have
demonstrated tremendous accuracy in games, it is anticipated that they can be
equally effective when transferred to learning
To demonstrate their effectiveness, the transferred technologies adopted by our
solution will be rigorously tested and evaluated by real-users and in realistic
small-scale learning scenarios. The goal of this process will be on the one hand to
validate the integrity of the technologies developed throughout the project and on
the other hand to verify that the developed solution matches the end-user expec-
tations. Succeeding in both of these goals will imply the smooth transfer of game
technologies to education.
11
Conclusions
As a term, learning analytics has only recently been introduced to describe the
“measurement, collection, analysis and reporting of data about learners and their
contexts, for purposes of understanding and optimizing learning and the environ-
ments in which it occurs”.2 While still nascent, the ﬁeld of learning analytics has
received considerable attention. The primary reason for the interest in learning
analytics is the vast and diverse data being collected by online Learning Man-
agement Systems (LMS); LMS themselves have also seen overwhelming adoption
rates, with 99% of US colleges and universities using such systems. While adoption
rates in Europe have been less pronounced in the past, recent years have seen a
growing European interest in e-Learning, leading to growth in this sector.3 Learning
analytics are both an affordance and a necessity for the immediate future of learning
institutions, since (a) data is now accessible in real-time (as it is produced),
(b) computers possess the necessary computing power to process vast volumes of
data, and (c) new types of data (e.g. via sensors or connected devices) become
available.4 Identifying and implementing appropriate learning analytics can have an
important impact both on the learning process on the individual level (for educators
2G. Siemens. Call for papers of the 1st international conference on learning analytics and
knowledge. https://tekri.athabascau.ca/analytics/call-papers, 2010.
3E. Group. Increased e-learning investment in Europe points to a market on the turn.   http://
edxusgroup.com/increased-e-learning-investment-in-europe-points-to-a-market-on-the-turn/.
4M. Mayer. Innovation at google: the physics of data. Presentation at the PARC forum, 2009.
[Slides available online at http://www.slideshare.net/PARCInc/].
136
A. Karakostas et al.

to identify at-risk learners, or for learners to reﬂect on their learning process), but
also allow for more informed decisions on the institutional, national and interna-
tional level (for determining the value generated by faculty activity, or for imple-
menting what-if scenarios using predictive models) (Long and Siemens 2011).
In this context and motivated by the expected impacts of the call, one of the main
objectives of our framework is to identify which game analytics technologies could
be migrated to education and explore effective ways to make a smooth and seamless
transfer. As current virtual labs already use several types of analytics tools, it is
important to detect and exploit the advantages that game analytics can offer against
current technologies. As the optimization of such technologies is synonymous to
impact maximization, keeping in-touch with state-of-the-art analytics is very crucial
for the survival of businesses in the educational industry. For this purpose, the
progress of performance and optimization of analytics tools outside of the gaming
domain
will
be
continuously
monitored
and
reported
through
systematic
state-of-the-art reviews and through active participation to relevant conferences and
workshop events. The ultimate goal is to integrate advances in the state-of-the-art so
as to improve the analytics outcomes and thereby achieve the optimal impact.
Acknowledgements The research leading to these results has received funding from ENVISAGE
project that has received funding from the European Union’s Horizon 2020 research and inno-
vation programme under grant agreement No. 731900.
References
Clark, A. (1998). Being there: Putting brain, body, and world together again. MIT Press.
Drachen, A., Thurau, C., Yannakakis, G., Togelius, J., & Bauckhage, C. (2013). Game data
mining. In: M. Seif El-Nasr, A. Drachen & A. Canossa (Eds.), Game analytics—Maximizing
the value of player data (pp. 205–253). Springer Publishers. http://www.springer.com/
computer/hci/book/978-1-4471-4768-8.
Greller, W., & Drachsler, H. (2012). Translating learning into numbers: A generic framework for
learning analytics. Educational Technology & Society, 15(3), 42–57.
Lauría, E. J., Moody, E. W., Jayaprakash, S. M., Jonnalagadda, N., & Baron, J. D. (2013, April).
Open academic analytics initiative: Initial research ﬁndings. In Proceedings of the Third
International Conference on Learning Analytics and Knowledge (pp. 150–154). ACM.
Liapis, A., Yannakakis, G. N., & Togelius, J. (2014, August). Designer modeling for sentient
sketchbook. In 2014 IEEE Conference on Computational Intelligence and Games (CIG)
(pp. 1–8). IEEE.
Runge, J., Gao, P., Garcin, F., & Faltings, B. (2014, August). Churn prediction for high-value
players in casual social games. In 2014 IEEE Conference on Computational Intelligence and
Games (CIG) (pp. 1–8). IEEE.
Shieh, L., Pummer, E., Tsui, J., Tobin, B., & Hooper, K. (2012). Increasing knowledge of sepsis
recognition and management through a mobile educational game. In AAMC Annual Meeting.
Siemens, G., & Long, P. (2011). Penetrating the fog: Analytics in learning and education.
EDUCAUSE Review, 46(5), 30.
Sloman, A. (2002). Diagrams in the mind? (pp. 7–28). London: Springer.
Tsui, J., Lau, J., & Shieh, L. S. (2014). SICKO: implementing and using learning analytics and
gamiﬁcation in medical education. Educause.
Enhancing Virtual Learning Spaces …
137

Advice for Action with Automatic
Feedback Systems
Denise Whitelock
Abstract This Chapter reviews the role of feedback in supporting student learning.
It highlights some of the problems that persist with providing meaningful feedback,
which should preferably take the form of providing advice, that can be actioned by
the student. It then discusses the progress made with automatic feedback through a
number of case studies which include the OpenEssayist, Open Comment and
OpenMentor computer assisted feedback systems. Findings suggest feedback that
provides socio-emotive support to students, together with recognising their effort, in
turn encourages the student to continue working on a problem. The use of automatic
hints also moves the feedback closer to “Advice for Action”. Building tools with
automatic feedback to support both students and tutors can relieve some of the
continual pressure on staff resources and three case studies are presented below that
address this issue.
Keywords Automatic feedback ⋅Advice for action ⋅OpenMentor
Essay feedback
1
Introduction
There is a general recognition in the Assessment ﬁeld that times are changing and
that assessment needs to become embedded in the teaching/learning cycle, and not
purely as a checking device for the awarding institution. e-Assessment has tried to
address this issue by providing timely and constructive feedback to students
through the development of a number of interactive tasks that can be automatically
marked, often presented to the student in a multiple choice question format but
more importantly can provide immediate feedback to the learner. There is also a
recognition that assessment tasks themselves must change and that feedback too
merits a reconceptualization (Merry et al. 2013).
D. Whitelock (✉)
The Open University, Milton Keynes, UK
e-mail: denise.whitelock@open.ac.uk
© Springer International Publishing AG 2018
S. Caballé and J. Conesa (eds.), Software Data Engineering for Network eLearning
Environments, Lecture Notes on Data Engineering and Communications
Technologies 11, https://doi.org/10.1007/978-3-319-68318-8_7
139

2
The Role of Feedback in Teaching and Learning
Feedback is a common feature of educational practice (e.g. Black and Wiliam
1998), and one that has been widely researched but not necessarily implemented or
understood to its full potential in practice. This has led to a large amount of research
attempting to deﬁne what feedback is, when it should be used, and how it could be
made more beneﬁcial for students and tutors. Beaumont, O’Doherty and Shannon
(2011) for instance identify the “fundamental aim of feedback practice, which is to
progressively and explicitly develop students’ self-evaluative skills through
engagement in the process” (p. 683). From this we can see that feedback should
have the intention not just of reporting back on ﬁnished work, but also of offering
advice to self-motivated learners on where they can improve in future work.
As Evans (2013) explained, “Even when “good” feedback has been given, the gap
between receiving and acting on feedback can be wide given the complexity of how
students make sense of, use, and give feedback (Taras 2003)” (p. 94). Therefore
feedback needs to be viewed by tutors and students as an ongoing activity within the
cycle of course learning, which feeds into further learning, rather than as an add-on or
end point of summative assessment. This is the concept that other researchers have
referred to as “feed-forward” (Evans 2013; Hattie and Timperley 2007). Thus feed-
back must be presented in a way that participants can understand, and that they can
interpret in terms of where improvements can be made in the future. Hattie and
Timperley argued that feedback must be a follow-up to information given to learners,
so that they are aware of task requirements before their work is judged by their teacher.
Therefore, feedback is a central part of the teaching and learning process, but one
that must follow task instruction and be followed by space for reﬂection and scope
to implement suggestions. Narciss (2013) progressed this notion through classifying
the functions of feedback as cognitive, metacognitive and motivational. It has been
proposed that ‘mediators’ (especially ‘understanding feedback’ and ‘agreement
with feedback’) operate between the provision of feedback features and imple-
mentation of suggestions (Nelson and Schunn 2009). The researchers suggested
that cognitive feedback factors were most likely to inﬂuence understanding, and
affective factors were more likely to inﬂuence agreement. If we want students to
make use of feedback, it is important in designing course resources to consider how
to ensure that feedback is understood. Nelson and Schunn (2009) also claimed that
feedback involved motivation, reinforcement and information. These collective
functions of feedback may be particularly important for students who are returning
to study after a period of time in employment, who may ﬁnd it more difﬁcult to
understand and access Higher Education modes of study (Scott et al. 2011).
In terms of the purpose of feedback, Chickering and Gamson (1987) outlined
seven principles of good practice for undergraduate education, of which the third
was “encourages active learning”. Likewise, Nicol and Macfarlane-Dick (2006)
stated that students should be urged to be proactive rather than reactive with regard
to feedback, using it as a springboard for improvement rather than a stop point.
Therefore, feedback or tutor input must do more than just identify misconceptions
140
D. Whitelock

in students’ work. It must motivate learners to engage with the topic and the task, so
that their work comes from and demonstrates understanding rather than just doing
enough to get a mark.
For some years now, many courses and universities have made increasing use of
technology to support assignment delivery and submission, as well as the medium
for offering feedback.
Learning has become radically more open and self-regulated, as well as hugely
evolved with the innovative uses of new technology. As Steffens (2006) high-
lighted, “In parallel to the rising interest in self-regulation and self-regulated
learning, the rapid development of the Information and Communication Tech-
nologies
(ICT)
has
made
it
possible
to
develop
highly
sophisticated
Technology-Enhanced Learning Environments (TELEs)” (p. 353).
Computer-provided feedback and assessment has some way to go to catch up
with these innovations, particularly where courses cater for large numbers of stu-
dents. The ability to offer automated guidance and feedback at the point of student
need to large numbers could help to revolutionise the experience and performance
of teaching and learning in higher education. This is particularly pertinent as many
universities, including the institution where the study reported in this paper took
place, are increasingly catering for distance and round-the-clock learners, many of
whom are out of the practice of academic writing.
Feedback involves motivation, reinforcement and information (Nelson and
Schunn 2009). The researchers addressed ﬁve features of feedback: summarization;
speciﬁcity; explanations; scope (local or global); and affective language. We have
drawn on these ﬁve features in determining the types of feedback to offer on
students’ draft essays. Referring to the ﬁrst feature, summarization, it was claimed
that ‘receiving summaries has previously been found to beneﬁt performance: when
college students received summaries about their writing, they made more sub-
stantial revisions…. Therefore, receiving summaries in feedback is expected to
promote more feedback implementations’ (Nelson and Schunn 2009, p. 378).
Feedback is moving forward at a pace with data analysis playing a prominent role.
Data can now be collected unobtrusively and during learning activities. However
collecting more data does not mean it is necessarily informative and cannot support the
teaching and learning dynamic of the classroom. Often it is the feedback from the data
analysis that is important and with the development of automatic feedback systems, it
is the “Advice for Action” (Whitelock 2011) that is advocated in this chapter.
3
Feedback Problems and Progress with Automatic
Feedback
There are a number of problems in Higher Education which include progression and
retention. Student “dropout” is an even more fundamental problem in distance
education. Simpson (2012) has reported that students drop out before they submit
Advice for Action with Automatic Feedback Systems
141

their ﬁrst assignment and this can be as high as a 30% dropout rate for some
modules studied at the UK’s Open University. Therefore there is a need to increase
students’ conﬁdence and skills during the early weeks of study. The ideal solution
would be for students to receive bespoke feedback from their tutors but as this is a
resource intensive solution, could automated formative feedback provide a
solution?
The following Sect. 4 describes an application known as “Open Essayist” that
delivers automated formative feedback designed to help university students
improve their assignments. If automated feedback were to be delivered to students
to assist them to acquire certain skills, such as essay writing, which will then reduce
dropout rates, will the machine feedback always be well received? Is there not an
emotional response to feedback that needs to be recognised? Human tutors convey
empathy and modulate their feedback to match their students’ immediate response
to feedback in face-to-face settings using tone of voice and facial expression.
However recognising the students’ contribution and effort is important too. This is a
course of action recommended by Mueller and Dweck (1998), where constructive
feedback that recognises effort, in turn encourages the student to continue working
on a problem and also moves the feedback closer to “Advice for Action”. An
automatic feedback system that addresses this particular issue is known as “Open
Comment” and is discussed in Sect. 5.
Another problem with feedback, when it is delivered through an electronic
medium, is a lack of recognition of the socio-emotive effect it might evoke in the
learner. Too much negativity can be demoralising and again results in student
dropout, while a lack of praise can also be demotivating. Teachers need to recognise
this issue and receive support in giving feedback that will assist the student emo-
tionally and match the mark that was awarded for an assessment task.
Section 6 describes a system, known as OpenMentor, which analyses and dis-
plays the different types of tutor comments provided as feedback to the student. It
then provides reﬂective comments to the tutor about their use of feedback, in order
to encourage them to provide a balanced combination of socio-emotive and cog-
nitive support together with ensuring that the feedback is relevant to the assigned
grade. Building tools with automatic feedback to support both students and tutors
can relieve some of the continual pressure on staff resources and three case studies
are presented below that address this issue.
4
OpenEssayist
OpenEssayist (Field et al. 2013) made use of a Natural Language Analytics engine
to provide direct feedback to students when they were preparing an essay for
summative assessment. The challenge was to provide meaningful feedback to the
students themselves so that they could self-correct rather than providing them with
a recommender system which elicits a tutor intervention (Arnold and Pistilli 2012).
OpenEssayist provides automated feedback on draft essays, and was developed as
142
D. Whitelock

part of the SAFeSEA project, to speciﬁcally help the UK’s Open University stu-
dents with their essay writing skills.
OpenEssayist is a real-time learning analytics tool, which operates through the
combination of a linguistic analysis engine, which processes the text in the essay,
and a Web application that uses the output of the linguistic analysis engine to
generate the feedback. OpenEssayist was built because many students at the Open
University return to study after some time spent in the workforce, and so it is
common that a signiﬁcant period of time has passed since their last experience of
writing academic essays. It is therefore not surprising that many ﬁnd this task
difﬁcult, and without adequate support may decide to leave their course. This is one
crucial reason why a system that can intervene and offer support between students’
draft and ﬁnal submitted essays might be so valuable for students and tutors alike.
In creating a system that can go some way to meeting these needs, a number of
preliminary studies were made (Alden et al. 2014; Alden Rivers et al. 2014). These
illustrated that the underlying premises for the construction of OpenEssayist were
sound and hence merited further development.
The ﬁnal system was then developed to process open-text essays and offered
feedback through key phrase extraction and extractive summarisation. Key phrase
extraction identiﬁes which individual words or short phrases are the most sug-
gestive of an essay’s content, while extractive summarisation essentially identiﬁes
whole key sentences. This operates under the assumption that the quality and
position of key phrases and key sentences within an essay illustrate how complete
and well-structured the essay is, and therefore provide a basis for building suitable
models of feedback.
There are a number of ‘automated essay scoring’ (AES) or ‘automated writing
evaluation’ (AWE) systems in existence and some are commercially available (see
Criterion (Burstein et al. 2003)). Pearson’s WriteToLearn (based on Landauer’s
Intelligent Essay Assessor (Landauer et al. 2003a, b)) and Summary Street (Franzke
and Streeter 2006), IntelliMetric (Rudner and Garcia 2006), and LightSIDE
(Mayﬁeld and Rose 2013), all include feedback functionality, though they have
their roots in systems designed to attribute a grade to a piece of work. The primary
concern of these systems is to help the user make stepwise improvements to a piece
of writing. In contrast, the primary concern of OpenEssayist is to promote
self-regulated learning, self-knowledge, and metacognition. Rather than telling the
user in detail how to ﬁx the incorrect and poor attributes of her essay, OpenEssayist
encourages the user to reﬂect on the content of her essay. It uses linguistic tech-
nologies, graphics, animations, and interactive exercises to enable the user to
comprehend the content of his/her essay more objectively, and to reﬂect on whether
the essay adequately conveys his/her intended meanings. Writing-Pal (Dai et al.
2011; McNamara et al. 2011) is the system that is most similar to ours in that it aims
to improve the user’s skills. Like OpenEssayist, Writing-Pal also uses interactive
exercises
to
promote
understanding.
Writing-Pal
is
very
different
from
OpenEssayist in terms of its underlying linguistic technologies and the design of its
exercises. There is educational research that argues that using summaries in for-
mative feedback on essays is very helpful for students. Nelson and Schunn (2009)
Advice for Action with Automatic Feedback Systems
143

concluded that summaries make effective feedback because they are associated with
understanding. They found that understanding of the problem concerning some
aspect of an essay was the only signiﬁcant mediator of feedback implementation,
whereas understanding of the solution was not. Nelson and Schunn meant by the
term ‘summaries’ the traditional notion of a short précis, and also some simpler
representations, such as lists of key topics. Therefore automatic summarisation
techniques became the main thrust of OpenEssayist development. An important
consequence of this choice meant that OpenEssayist is domain independent. This
also sets OpenEssayist apart from existing automated essay scoring systems.
Some of the existing technical systems that provide automated feedback on
essays for summative assessment have been reviewed (Alden Rivers et al. 2014).
Such systems focus on assessment rather than feedback, which is where
OpenEssayist is different in providing hints for action for students to improve their
essays. See Fig. 1.
In other research, hints have been used but have been given as responsive
prompts, when students have requested help for a certain task or problem (e.g.
Aleven et al. 2010), rather than as broad supportive information before starting
tasks. In the study by Aleven and colleagues, the researchers focused on
“help-seeking behaviour”, in considering when students requested the hints in order
to gradually arrive at the answer, compared with those who were using hints to
understand the question and how best to respond.
Fig. 1 Feedback back from Open Essayist with hints
144
D. Whitelock

The purpose and design of OpenEssayist are very different from existing auto-
mated assessment systems. The system is primarily focused on user understanding
and self-directed learning, rather than on essay improvement, and it engages the
user on matters of content, rather than pointing out failings in grammar, style, and
structure.
When the system was used with Open University postgraduate students, a wide
variety of usage was found with respect to how long they engaged with the system
and which features they accessed. Some continued to access the system and submit
drafts after the course. Features concerning key words were popular, followed by
highlighting key sentences and extracting these as a summary. A signiﬁcant cor-
relation was found between students’ grades for Essay 1 and the number of drafts
they submitted. Perhaps those students who submitted more drafts gained higher
grades, or those students who tend to get higher grades also engaged more with the
process of submitting drafts. We also found that this cohort of students, who had
access to OpenEssayist, achieved signiﬁcantly higher overall grades than the pre-
vious cohort, who did not. OpenEssayist has been used with students on a computer
science course at Hertfordshire University in the UK and with students writing a
research methods report at Dubai University. This is a learning analytics tool which
has been rolled out in practice, and which has yielded evidence that students can
and do beneﬁt from using such a system in writing their academic assignments.
5
Open Comment
There is a recognition that e-assessment accompanied by an appropriate feedback to
the student is beneﬁcial for learning (DiBattista et al. 2004; Pitcher et al. 2002;
Whitelock and Raw 2003). Distance Learning too is forging ahead with electronic
delivery of courses together with addressing the complexities of e-assessment for
large cohorts of students. The Open Comment project sat within an external
demand for electronic assessment from policy makers (see Whitelock and Brasher
2006) and the Arts disciplines where analysis of more free text student responses
were required rather than systems where students ticked multiple choice answers to
questions. Open Comment, unlike OpenEssayist, was designed for a speciﬁc
knowledge domain and cannot be used for any subject.
Free text response processing is at the cutting edge of linguistics. Certainly a
completely human-like response to free text is still beyond the state-of-the art, but
experience has shown that sometimes it is possible to provide effective responses
based on surface features of a free text response. Carefully constructed language,
conversational in form, can be even more important to guiding learning than the
content being communicated (Holmberg 1983). Instead of providing feedback on
the answer, the project Open Comment’s approach was, to some extent like ELIZA
(Weizenbaum 1966), to couch just enough analysis of the text in reﬂective language
to help the learner assess their own work.
Advice for Action with Automatic Feedback Systems
145

The speciﬁc objective of the project was to construct some simple tools in the
form of Moodle extensions that allow a Moodle author to ask free-text response
questions that can provide a degree of interactive formative feedback to students. In
parallel with this was the aim to begin to develop a methodology for constructing
such questions and their feedback effectively, together with techniques for con-
structing decision rules for giving feedback.
Open Comment provided a formative feedback technology designed to be
integrated in the Moodle virtual learning environment. It delivers a simple system
allowing questions to be written in Moodle, and for students’ free text responses to
these questions to be analysed and used to provide individually customised for-
mative feedback. Open Comment is related to traditional free text assessment
technologies, such as the ETS e-rater system and Landauer et al.’s (1998) IEA,
although it has a very different emphasis. In particular, it makes no attempt to
provide grading information; instead, it provides reﬂective feedback, designed to
guide the students in their learning.
5.1
Pedagogical Principles Driving the Feedback Engine
This section reports on the pedagogical principles which drove Open Comment’s
development since the pedagogical rationale for this system was to engage students
in a series of electronic formative assessment tasks that would provide more free
text entry with automatic feedback. This would promote a more challenging
experience for the students than just checking their learning for revision purposes
and promote a more personalised learning environment for self-reﬂection.
The guidance text arose from an analysis of what feedback actually was, and
how learners used it. Throughout the development work, Whitelock and Watt
(2008) worked closely with expert tutors in several Arts disciplines, using a range
of techniques to elicit the processes they used to provide appropriate feedback.
These ranged from role play (becoming a student) through to analysing collections
of real answers and constructing sample solutions.
A preliminary analysis of 68 History assignments together with 100 plus
assignments from different disciplines revealed a common pattern of tutor
responses. These were clustered around the main categories of praise, advice on
structure and presentation, particular misunderstandings, and developing and
understanding particular issues.
The underlying model of feedback centred around:
• Identiﬁcation of salient variables
• A description of these variables
• Identiﬁcation of trends and relationships between these variables
The result of these analyses were formalised as an operational model for for-
mative feedback generation, as set out in the Table 1.
146
D. Whitelock

Table 1 Operational feedback model for Open Comment
Stages of analysis by computer
of students’ free text entry for
Open Comment
Advice with respect to content
Socio-emotional support
Stylised example
STAGE 1a: DETECT
ERRORS
E.g. Incorrect dates, facts.
(Incorrect inferences and
causality is dealt with below)
Instead of concentrating on X,
think about Y in order to answer
this question
Recognise effort (Dweck) and
encourage to have another go
You have done well to start answering this
question but perhaps you misunderstood it.
Instead of thinking about X which did
not…….. Consider Y
STAGE 1b:
IF NO INCORRECT
STATEMENTS GO TO 2
A good start………
STAGE 2a:
REVEAL FIRST OMISSION
Consider the role of Z in your
answer
Praise what is correct and point out
what is missing
Good but now consider the role X plays in
your answer
STAGE 2b:
REVEAL SECOND
OMISSION
Consider the role of P in your
answer
Praise what is correct and point out
what is missing
Yes but also consider P. Would it have
produced the same result if P is neglected?
STAGE 3:
REQUEST
CLARIFICATION OF KEY
POINT 1
Explain X more fully
What do you mean by X
Conﬁrm and concur about what is
correct encourage to take the
analysis further
STAGE 4:
REQUEST
FURTHER ANALYSIS OF
KEY POINT 1
(Stages 3 and 4 repeated with all
the key points)
Analyse X more fully
Conﬁrm and concur about what is
correct encourage to take the
analysis further
Very interesting point—X is very complex
perhaps it would have been effective to look
at things slightly differently and consider how
X contributes to Y
(continued)
Advice for Action with Automatic Feedback Systems
147

Table 1 (continued)
Stages of analysis by computer
of students’ free text entry for
Open Comment
Advice with respect to content
Socio-emotional support
Stylised example
STAGE 5:
REQUEST
THE INFERENCE FROM
THE ANALYSIS OF
KEY POINT 1 IF IT IS
MISSING
Request the conclusion that can
be drawn from X
Praise effort and reiterate progress
is being made
This is a sound description but it would be
good if you explain what X is contributing to
this situation
STAGE 6:
REQUEST
THE INFERENCE FROM
THE ANALYSIS OF
KEY POINT 1 IF IT IS NOT
COMPLETE
What is X causing in this
situation?
Reafﬁrm progress but encourage
student to take the analysis process
one step further
Yes what you have written is correct but can
you elaborate and explain what X means?
STAGE 7:
CHECK THE CAUSALITY
What is X, Y and Z causing in
this situation?
Praise persistence and effort and
ask the user to think about the
reasoning behind a particular
response
You are certainly improving your answer to
this question. Well done. In order to improve
your answer further could you say something
about the role X played in Y I’m thinking
particularly of the following example where
X was seen with respect to Z
STAGE 7:
REQUEST ALL THE
CAUSAL FACTORS ARE
WEIGHTED
Do X, Y and Z contribute in the
same way to producing situation
C, i.e. do the variables have
equal weighting
Praise persistence and effort and
ask the user to think about the
importance and relative weightings
of the causal factors
You have made a good stab at this question.
From your answer I think you are allowing a
considerable role to X. Does this mean you
accept that X alone causes Y
148
D. Whitelock

This model, as illustrated by Table 1, operates by and large through a sequential
set of rules identifying sources of evidence within the student’s response, and
escalating in level of analysis, in some sense following Anderson, Krathwold, and
Bloom’s (2000) revised taxonomy of educational objectives. Importantly, also,
there is a strong causal element to many of the rules. These rules were implemented
in a bespoke feedback engine within Open Comment. An example of this feedback
in the Open Comment system can be found in Fig. 2.
Upon closer inspection, Table 1 reveals speciﬁc advice with respect to the
content of the student answer and also has a socio-emotional dimension, where the
student effort is recognised and praise given for what has been correctly answered.
This design approach was based upon the research of Mueller and Dweck (1998)
who found that praising just ability can hinder the learner, but praising effort can
motivate students to continue with their studies. This type of feedback can promote
a growth mindset and lead to a lack of tension when learning, as the students know
they can improve if they stretch themselves and confront obstacles as challenges
(Dweck 2008).
An important result from this project has been an increased understanding of
the differences between even closely related disciplines. In both History and
Fig. 2 Feedback from the OpenComment system
Advice for Action with Automatic Feedback Systems
149

Philosophy, as with many humanities and social sciences, there is a greater
emphasis on developing each student’s ability to reason, and to use arguments and
evidence in ways that are in keeping with a discipline-speciﬁc methodological
ethos. Questions could rarely be taken at face value—especially in the more
advanced levels. Open Comment feedback focused far more on evidence than on
getting the answer right. In the future effective development of formative feedback
technologies in these disciplines is totally dependent on effective involvement of
tutors with both pedagogical and domain expertise.
6
Open Mentor
One of the challenges of today’s education is that students are expecting better
feedback, more frequently, and more quickly. Unfortunately, in today’s educational
climate, the resource pressures are higher, and tutor feedback is often produced
under greater time pressure, and often later. Although feedback is considered
essential to learning, what is it and how can tutors be supported to provide pertinent
feedback to their students when automatic feedback is unavailable?
Human feedback is, put simply; additional tutoring that is tailored to the lear-
ner’s current needs. In the simplest case, this means that there is a mismatch
between students’ and the tutors’ conceptual models, and the feedback is reducing
or correcting this mismatch, very much as feedback is used in cybernetic systems.
This is not an accident, for the cybernetic analogy was based on Pask’s (1976)
work, which has been a strong inﬂuence on practice in this area (e.g., Laurillard
1993).
One of the problems with tutor feedback to students is that a balanced combi-
nation of socio-emotive and cognitive support is required from the teaching staff,
and the feedback needs to be relevant to the assigned grade. Is it possible to
capitalise on technology to build training systems for tutors in Higher Education,
that will support them with their feedback to students, and which will encourage
their students to become more reﬂective learners? Since feedback is very much at
the cutting edge of personal learning, this OpenMentor project set out to see how it
could work with tutors to improve the quality of their feedback. To achieve this
Open Mentor was developed as an open source tool which tutors can use to analyse,
visualise, and compare their use of feedback.
With Open Mentor, feedback is not seen as error correction, but as part of the
dialogue between student and tutor. This is important for several reasons: ﬁrst,
thinking of students as making errors is unhelpful—as Norman (1988) says, errors
are better thought of as approximations to correct action. Thinking of the student as
making mistakes may lead to a more negative perception of their behaviour than is
appropriate. Secondly, learners actually need to test out the boundaries of their
knowledge in a safe environment, where their predictions may not be correct,
without expecting to be penalised for it. Finally, feedback does not really imply
guidance (i.e., planning for the future) and OpenMentor has been designed to
150
D. Whitelock

incorporate
that
type
of
support
without
resorting
to
the
rather
clunky
‘feed-forward’.
In order to provide feedback, Open Mentor has to analyse the tutor comments. So
how could these comments be meaningfully grouped together? The classiﬁcation
system used in Open Mentor was based on that of Bales (1950). Bales’s system was
originally devised to study social interaction, especially in collaborating teams; its
strength is that it brings out the socio-emotive aspects of dialogue as well as the
domain level. In previous work Whitelock et al. (2004) found that the distribution of
comments within these categories correlates very closely with the grade assigned.
Bales’s model provides four main categories of interaction: positive reactions,
negative reactions, questions, and answers. These interactional categories illustrate
the balance of socio-emotional comments that support the student. We found
(Whitelock et al. 2004) that tutors use different types of questions in different ways,
both to stimulate reﬂection, and to point out, in a supportive way, that there are
problems with parts of an essay. These results showed that about half of Bales’s
interaction categories strongly correlated with grade of assessment in different
ways, while others were rarely used in feedback to learners. This evidence of
systematic connections between different types of tutor comments and level of
attainment in assessment was the platform for the current work.
The
advantage
of
the
Bales
model
is
that
the
classes
used
are
domain-independent—we used this model to classify feedback in a range of dif-
ferent academic disciplines, and it has proven successful in all of them. An auto-
matic classiﬁcation system, therefore, can be used in all ﬁelds, without needing a
new set of example comments and training for each different discipline.
Others (e.g., Brown and Glover 2006) have looked at different classiﬁcation
systems, including Bales, and from these developed their own to bring out addi-
tional aspects of the tutor feedback, bringing back elements of the domain. In
practice, no (useful) classiﬁcation system can incorporate all comments. We
selected, and still prefer, Bales because of its relative simplicity, its intuitive grasp
by both students and tutors, and because it brings out the socio-emotive aspects of
the dialogue, which is the one aspect tutors are often unaware of.
A second point is that Bales draws out a wider context: the team found that as they
started to write tools that supported feedback, they began to question the notion of
feedback itself. Instead, the concept seemed to divide naturally into two different
aspects: learning support and learning guidance. Support encourages and motivates
the learner, guidance shows them ways of dealing with particular problems.
6.1
Questions Which Tested the Underlying Pedagogical
Model for Open Mentor
Previous work by Whitelock et al. (2004) on student feedback has postulated that
work that is awarded high grades should attract feedback from tutors that is high in
praise, has few questions and does not ask the student to reﬂect on their work.
Advice for Action with Automatic Feedback Systems
151

Conversely, work that is awarded low grades should attract less praise, more
questions and suggestions and invite more reﬂection. A number of questions in the
Open Mentor Evaluation Study are able to throw light on these postulated outcomes
and the results are summarised below.
A signiﬁcant majority of both students and tutors respondents indicated that they
expected high grades to attract more positive comments and low grades to attract
more answers, suggestions and questions. Tutors gave a strong indication that they
expected assessments with low grades to attract negative comments. Student
responses followed a similar trend that was however not statistically signiﬁcant.
Students also indicated strongly that they expected no difference. All these ﬁndings
support the pedagogical model postulated by Whitelock et al.
A further analysis, using cross tabulation revealed:
• Both students and tutors who feel that low grades would result in more questions
also indicated that low grades would attract more answers
• Tutors who judged that high grades attract more positive comments also indi-
cated strongly that low grades attract more answers and suggestions
• Tutors who felt that low grades attract more questions also indicated that low
grades attract negative comments
• Both students and tutors felt that lower grades should attract more detailed
comments and a deeper level of explanation. Higher grades should attract more
positive comments
These ﬁndings from both groups of stakeholders supported a pedagogically
driven development process which is described below.
OpenMentor was conceived as a tool to support tutors’ feedback practices by
classifying comments added to an assignment using Bales interaction analysis
taxonomy (see Table 2) and reporting the results of the analysis in summarized
views. Summary views show the proportion of the actual number of comments
given by the tutors versus an ideal number. This calculated ideal is based on grade
distribution and total comments included in the assignment, making the analysis
unique to the student, tutor and feedback comments provided. Under Bales tax-
onomy, tutors’ feedback comments are classiﬁed as Positive, Questions, Negative
and Teaching Points. Examples of text identiﬁed by OpenMentor when classifying
comments can be seen in Figs. 3 and 4.
OpenMentor has been used in anger by Southampton University and King’s
College London (Whitelock et al. 2012a, b) and improvements made under the
auspices of the OMTetra project. One of the important outcomes of the OMTetra
project and the dissemination of OpenMentor is the positive effect in tutors’
feedback practice, which would reﬂect on students’ learning and performance. By
supporting tutors’ feedback practices through a strong formative function where the
tutor can use the output of the system (reports and classiﬁcations) to engage in
reﬂection about the quality and appropriateness of his/her feedback, students are
more likely to receive feedback that is ultimately useful. Interestingly however, is
the fact that students may also need to receive a form of training to interpret their
152
D. Whitelock

Table 2 Bales’s interaction categories
Categories
Speciﬁc examples
Positive reactions
A1
1. Shows solidarity
Jokes, gives help, rewards others
A2
2. Shows tension
release
Laughs, shows satisfaction
A3
3. Shows agreement
Understands, concurs, complies, passively accepts
Attempted answers
B1
4. Gives suggestion
Directs, proposes, controls
B2
5. Gives opinion
Evaluates, analyses, expresses feelings or wishes
B3
6. Gives information
Orients, repeats, clariﬁes, conﬁrms
Questions
C1
7. Asks for information
Requests orientation, repetition, conﬁrmation, clariﬁcation
C2
8. Asks for opinion
Requests evaluation, analysis, expression of feeling or
wishes
C3
9. Asks for suggestion
Requests directions, proposals
Negative reactions
D1
10. Shows
disagreement
Passively rejects, resorts to formality, withholds help
D2
11. Shows tension
Asks for help, withdraws
D3
12. Shows antagonism
Deﬂates others, defends or asserts self
Fig. 3 OpenMentor analysis of tutor comments by category
Advice for Action with Automatic Feedback Systems
153

tutors’ feedback in order to beneﬁt from receiving good quality feedback (Buhagiar
2012). Further development of OpenMentor may include a student module where
learners are asked to make notes on how they made use of their tutors’ feedback.
These notes could then be read by the tutor and mismatches between intended
purpose of the feedback provided and that interpreted by the student are negotiated.
For tutors, there are signiﬁcant opportunities in the use of OpenMentor as an
academic development tool as it can generate dialogue about effective feedback
between (a) tutors and academic developers and (b) peer reviewers during ‘peer
observation’ of assessment practice. Consequently qualitative and quantitative
outputs of the system which have been perceived as very useful during the pilots
can be complemented by the function of the tool as generator of discussion and
reﬂection on assessment practice. For students the tool can play a signiﬁcant role in
generating a dialogue between tutors and students about feedback and help them to
close the loop (Sadler 1989). This dialogue can achieve a consensus and a better
understanding of standards of quality in student assessed work.
Fig. 4 Analysis of total number of comments by OpenMentor
154
D. Whitelock

7
Conclusions
It has been proposed by Nelson and Schunn (2009) that there are ‘mediators’ that
operate between the provision of feedback features, and implementation of sug-
gestions. These authors addressed these mediators as ‘understanding feedback’ and
‘agreement with feedback’. They suggested cognitive feedback factors are most
likely to inﬂuence understanding, and affective factors are more likely to inﬂuence
agreement. These are then said to inﬂuence implementation. Their results therefore
showed a focus on how understanding feedback is critical to implementing sug-
gestions from feedback. Thus it is important in designing course resources that we
consider how to increase the likelihood that feedback is understood, if we want
students to make use of it in current and future work—to learn from it (and improve
performance) by understanding it, rather than just improving one-off performance
by blind implementation. These proposals support the ﬁndings from the
OpenMentor and Open Comment projects where socio-emotive support is recog-
nised, together with cognitive assistance to provide students with “Advice for
Action”.
Equally important is the issue of students’ ‘mindsets’, in their capacity for
learning and improving performance, and in terms of students’ beliefs that change is
possible (Dweck 2008; Yeager et al. 2013). The researchers refer to and contrast a
‘ﬁxed mindset’, where students believe intelligence is relatively predetermined and
ﬁnite; compared to a ‘growth mindset’, where students believe they can change
their capacity and capabilities, respond to challenges, and try something again
which they may initially ﬁnd difﬁcult. Thus students need to be given feedback that
supports them in understanding requirements, but that also motivates them to
believe they can make changes and improve their own performance. When such
feedback can be given in a non-threatening way, for instance through live, personal
use of an automated feedback system before formal submission, students may feel
empowered that they can implement points raised in formative feedback to realize
genuine improvements in performance. Both Open Comment and OpenMentor has
taken on board the notion of changing mindsets in the feedback that is offered to
both students and tutors alike.
The OMtetra project was successful in taking up Open Mentor and completing
its transfer into two Higher Education Institutions. Interest shown by tutors from
these institutions has translated into ideas to facilitate assignment analysis through
Open Mentor and to encourage adoption of the system across institutional
departments. Further development of Open Mentor features and promotion for
adoption of the system at a larger scale are on-going efforts that will ensure that
Open Mentor has an impact on a core task of HEIs: the delivery of quality feedback
that will support the teaching and learning process.
OpenEssayist is unique in being a content-free tool that has been developed to
offer automated feedback on students’ draft essays, rather than an assessment on
their ﬁnished work. OpenEssayist is a system that offers opportunities for students
to engage with and reﬂect on their work, in any subject domain, and to improve
Advice for Action with Automatic Feedback Systems
155

their work through understanding of the requirements of academic essay writing. In
trial use of the system in a genuine Open University course, students made use of it
to varying degrees (Whitelock et al. 2015), which is perhaps likely with any study
resource. From Whitelock et al’s analysis the team were also able to conclude that a
signiﬁcant positive correlation exists in this sample of students and the number of
drafts submitted and the ﬁnal grades for these essays. Another ﬁnding was that
students who had access to OpenEssayist achieved signiﬁcantly higher grades for
this course than the previous year of students, who had no such access.
Furthermore OpenEssayist is a system that has been shown to offer opportunities
for students to engage with and reﬂect on their work, in any subject domain, and to
improve their work through understanding of the requirements of academic essay
writing. In trialling use of the system in a genuine course, it was found that students
made use of it to varying degrees, which is perhaps likely with any study resource.
Those who took the time to explore system affordances and what they could be used
for however tended to report more positively on its perceived value.
In today’s educational climate, with the continued pressure on staff resources,
making individual learning work is always going to be a challenge. But it is
achievable, so long as we manage to maintain our empathy with the learner. Tools
can help us achieve this by giving us frameworks where we can reﬂect on our social
interaction, and ensure that it provides the emotional support as well as the con-
ceptual guidance that our learners need.
8
Future Work
OpenEssayist has many potential advantages for students and tutors, which will
beneﬁt from further research and exploration. As OpenEssayist is designed to offer
feedback to students during the drafting process, this has considerable implications
for supporting students to improve their work, and also supporting students to
believe that they can improve their academic work. This is no small feat for learners
who may often feel isolated and stretched trying to squeeze study around other
commitments and demands on their time.
Assessment with automatic feedback in Higher Education and any vocational
training environment is a far more widespread issue than was fully realised by
Whitelock et al. (2015). After building OpenEssayist, it became apparent that there
are many other potential applications for this technology. These include:
• Providing students with formative feedback on their assessments, with feedback
properly adjusted to the students’ needs
• Supporting the review process in academic conferences and competitive project
proposals
• Automated generation of high quality reports (both in content and in presen-
tation) based on complex data
156
D. Whitelock

With respect to OpenMentor, in the future, the current taxonomy used to analyse
feedback in OpenMentor can be complemented with a dynamic algorithm that
‘learns’ from tutors feedback and classiﬁes text using natural language processing
techniques. This addition to the analysis algorithm of OpenMentor should address
the needs of individual institutions where feedback practice is aligned to that of the
culture of the organization. Our assumption, after the lessons learned from the
implementation of OpenMentor in two Higher Education Institutions (Whitelock
et al. 2012a, b), is that the more conﬁgurable OpenMentor is, the more attractive it
will be to disseminate its use across institutions.
Technology to enhance assessment and feedback is still developing but the
problems are not technical: feedback coupled with assessment raises far wider
social issues, and technologists have struggled in the past to resolve these issues
with the respect they deserve. Automatic feedback is starting to deliver potential
improvements; but there is still much work to be done.
Acknowledgements The OpenEssayist Research was supported by the Engineering and Physical
Sciences Research Council (EPSRC, grant numbers EP/J005959/1 & EP/J005231/1). Thanks are
also due to the SAFeSEA team who produced OpenEssayist, namely John Richardson, Alison
Twiner, Debora Field and Stephen Pulman. I would also like to thank Stuart Watt and the JISC for
their support in the development of OpenMentor. Stuart Watt deserves special thanks as he also
developed the Open Comment system.
References
Alden, B., Van Labeke, N., Field, D., Pulman, S., Richardson, J. T. E., & Whitelock, D. (2014).
Using student experience as a model for designing an automatic feedback system for short
essays. International Journal of e-Assessment, 4(1), article no. 68.
Alden Rivers, B., Whitelock, D., Richardson, J. T. E., Field, D., & Pulman, S. (2014). Functional,
frustrating and full of potential: Learners’ experiences of a prototype for automated essay
feedback. In Proceedings of Computer Assisted Assessment international conference: Research
into eAssessment, Zeist, the Netherlands, 30 June–1 July.
Aleven, V., Roll, I., McLaren, B. M., & Koedinger, K. R. (2010). Automated, unobtrusive,
action-by-action assessment of self-regulation during learning with an intelligent tutoring
system. Educational Psychologist, 45, 224–233. https://doi.org/10.1080/00461520.2010.
517740.
Anderson, L. W., & Krathwohl, D. R. (Eds.), (2000). A taxonomy for learning, teaching, and
assessing: a revision of Bloom’s taxonomy of educational objectives. Allyn & Bacon.
Arnold, K. E., & Pistilli, M. D. (2012). Course signals at Purdue: Using learning analytics to
increase student success. Paper presented at the 2nd International Conference on Learning
Analytics
and
Knowledge,
April
29th–May
2nd,
Vancouver,
BC,
Canada.
ACM
978-1-4503-1111-3/12/04.
Bales, R. F. (1950). A set of categories for the analysis of small group interaction. American
Sociological Review, 15, 257–263.
Beaumont, C., O’Doherty, M., & Shannon, L. (2011). Reconceptualising assessment feedback: A
key to improving student learning? Studies in Higher Education, 36, pp. 671–687. https://doi.
org/10.1080/03075071003731135.
Black, P., & Wiliam, D. (1998). Assessment and classroom learning. Assessment in Education,
5(1), 7–74.
Advice for Action with Automatic Feedback Systems
157

Brown, E., & Glover, C. (2006). Written feedback for students: too much, too detailed or too
incomprehensible to be effective? Bioscience Education e-Journal, 7(3).
Buhagiar, M. A. (2012). Mathematics student teachers’ views on tutor feedback during teaching
practice. European Journal of Teacher Education, iFirst Article, 1-13.
Burstein, J., Chodorow, M., & Leacock, C. (2003). CriterionSM online essay evaluation: An
application for automated evaluation of student essays. In J. Riedl & R. Hill (Eds.),
Proceedings of the Fifteenth Conference on Innovative Applications of Artiﬁcial Intelligence
(pp. 3–10). Cambridge, MA: MIT Press.
Chickering, A. W., & Gamson, Z. F. (1987). Seven principles for good practice in undergraduate
education. American Association of Higher Education Bulletin, 39(7), 3–7.
Dai, J., Raine, R. B., Roscoe, R. D., Cai, Z., & McNamara, D. S. (2011). The Writing-Pal tutoring
system: Development and design. Journal of Engineering and Computer Innovations, 2(1), 1–
11. ISSN 2141-6508 2011 Academic Journals.
DiBattista, D., Mitterer, J. O., & Leanne, G. (2004) Acceptance by undergraduates of the
immediate feedback assessment technique for multiple-choice testing. Teaching in Higher
Education, 9(1), 17–28. ISSN 1356-2517.
Dweck, C. (2008). Mindset: The new psychology of success. New York: Ballantine Books.
Evans, C. (2013). Making sense of assessment feedback in higher education. Review of
Educational Research, 83, 70–120. https://doi.org/10.3102/0034654312474350.
Field, D., Pulman, S., Van Labeke, N., Whitelock, D., & Richardson, J. T. E. (2013). Did I really
mean that? Applying automatic summarisation techniques to formative feedback. In: G.
Angelova, K. Bontcheva & R. Mitkov (Eds.), Proceedings International Conference Recent
Advances in Natural Language Processing (pp. 277–284), 7–13 September, 2013. Hissar,
Bulgaria: Association for Computational Linguistics.
Franzke, M., & Streeter, L. A. (2006). Building student summarization, writing and reading
comprehension skills with guided practice and automated feedback. Pearson Knowledge
Technologies: White paper.
Hattie, J., & Timperley, H. (2007). The power of feedback. Review of Educational Research, 77,
81–112. https://doi.org/10.3102/003465430298487.
Holmberg, B. (1983). Guided didactic conversation in distance education. In D. Sewart, D.
Keegan, & B. Holmberg (Eds.), Distance education: International perspectives (pp. 114–122).
London: Croom Helm.
Landauer, T. K., Foltz, P. W., & Laham, D. (1998). An introduction to latent semantic analysis.
Discourse Processes, 25(2–3), 259–284.
Landauer, T. K., Laham, D., & Foltz, P. W. (2003a). Automatic essay assessment. Assessment in
Education: Principles, Policy and Practice, 10(3), 295–308.
Landauer, T. K., Laham, D., & Foltz, P. W. (2003b). Automated scoring and annotation of essays
with the Intelligent Essay Assessor. In Automated essay scoring: A cross-disciplinary
perspective (pp. 87–112).
Laurillard, D. (1993). Rethinking University Teaching: A framework for the effective use of
educational technology. London: Routledge.
Mayﬁeld, E., & Rosé, C. P. (2013). LightSIDE: Open Source Machine Learning for Text. In M.
D. Shermis & J. C. Burstein (Eds.), Handbook of automated essay evaluation: Current
applications and new directions (pp. 124–135). Oxon: Routledge.
McNamara, D. S., Raine, R., Roscoe, R., Crossley, S., Jackson, G. T., Dai, J., Cai, Z., Renner, A.,
Brandon, R., Weston, J., Dempsey, K., Lam, D., Sullivan, S., Kim, L., Rus, V., Floyd, R.,
McCarthy, P., & Graesser, A. (2011). The Writing- Pal: Natural language algorithms to support
intelligent tutoring on writing strategies. In P. M. Mc-Carthy & Chutima Boonthum-Denecke
(Eds.), Applied Natural Language Processing and Content Analysis: Advances in Identiﬁca-
tion, Investigation and Resolution (pp. 298–311). Hershey, PA: IGI Global.
Merry, S., Price, M., Carless, D., & Taras, M. (Eds.). (2013). Reconceptualising feedback in higher
education: Developing dialogue with students. London and New York: Routledge.
158
D. Whitelock

Mueller, C. M., & Dweck, C. S. (1998). Praise for intelligence can undermine children’s
motivation and performance. Journal of Personality and Social Psychology, 75(1), 33–52.
https://doi.org/10.1037/0022-3514.75.1.33.
Narciss, S. (2013). Designing and evaluating tutoring feedback strategies for digital learning
environments on the basis of the interactive tutoring feedback model. Digital Education
Review, 23, 7–26.
Nelson, M. M., & Schunn, C. D. (2009). The nature of feedback: How different types of peer
feedback affect writing performance. Instructional Science, 37(4), 375–401.
Nicol, D. J., & Macfarlane-Dick, D. (2006). Formative assessment and self-regulated learning: A
model and seven principles of good feedback practice. Studies in Higher Education, 31, 199–
218. https://doi.org/10.1080/03075070600572090.
Norman, D. (1988). The psychology of everyday things. New York: Basic Books.
Pask, G. (1976). Conversation theory: Applications in education and epistemology. Amsterdam:
Elsevier.
Pitcher, N., Goldﬁnch, J., & Beevers, C. (2002). Aspects of computer bases assessment in
mathematics. Active Learning in Higher Education, 3(2), 19–25.
Rivers, B. A., Whitelock, D., Richardson, J. T., Field, D., & Pulman, S. (2014). Functional,
frustrating and full of potential: Learners’ experiences of a prototype for automated essay
feedback. In: Kalz, Marco & Ras, Eric (Eds.), Computer Assisted Assessment: Research into
E-Assessment. Communications in Computer and Information Science (439) (pp. 40–52).
Cham, Switzerland: Springer.
Rudner, L. M., Garcia, V., & Catherine Welch. (2006). An evaluation of the IntelliMetricSM essay
scoring system. The Journal of Technology, Learning, and Assessment, 4(4).
Sadler, D. R. (1989). Formative assessment and the design of instructional systems. Instructional
Science, 18, 119–144.
Scott, D., Evans, C., Hughes, G., Burke, P. J., Watson, D., Walter, C., & Huttly, S. (2011).
Facilitating transitions to masters-level learning: Improving formative assessment and feedback
processes. Executive summary. Final extended report. London, UK: Institute of Education.
Simpson, O. (2012). Supporting students for success in online and distance education (3rd ed.).
London: Routledge.
Steffens, K. (2006). Self-regulated learning in technology-enhanced learning environments:
Lessons from a European review. European Journal of Education, 41, 353–380. https://doi.
org/10.1111/j.1465-3435.2006.00271.x.
Taras, M. (2003). To feedback or not to feedback in student self-assessment. Assessment and
Evaluation in Higher Education, 28, 549–565. https://doi.org/10.1080/0260293032000120415.
Weizenbaum, J. (1966). ELIZA: A computer program for the study of natural language
communication between man and machine. Communications of the ACM, 9(1), 36–45.
Whitelock, D. (2011). Activating assessment for learning: Are we on the way with Web 2.0. In M.
J. W. Lee & C. McLoughlin (Eds.), Web 2.0-Based-E-Learning: Applying Social Informatics
for Tertiary Teaching (Vol. 2, pp. 319–342). IGI Global.
Whitelock, D., & Raw, Y. (2003). Taking an electronic mathematics examination from home:
What the students think. In C. P. Constantinou & Z. C. Zacharia (Eds.), Computer Based
Learning in Science, New Technologies and their Applications in Education, (Vol. 1, pp. 701–
713). Nicosia, Cyprus: Department of Educational Sciences, University of Cyprus. ISBN
9963-8525-1-3.
Whitelock, D., Watt, S. N. K., Raw, Y., & Moreale, E. (2004). Analysing tutor feedback to
students: ﬁrst steps towards constructing an electronic monitoring system. ALT-J, 1(3), 31–42.
Whitelock, D., & Brasher, A. (2006). Developing a roadmap for e-Assessment: Which way now?
In 10th International Computer Assisted Assessment Conference (pp. 487–501), Loughbor-
ough University, 4/5 July 2006. ISBN: 0-9539572-5-X.
Whitelock, D., & Watt, S. (2008). Putting pedagogy in the driving seat with Open Comment: An
open source formative assessment feedback and guidance tool for history students. In Farzana
Khandia (Ed.), CAA Conference 2008 (pp. 347–356), Loughborough University, 8/9 July
2008.
Advice for Action with Automatic Feedback Systems
159

Whitelock, D. M., Gilbert, L., Hatzipanagos, S., Watt, S., Zhang, P., Gillary, P., & Recio, A.
(2012a). Addressing the challenges of assessment and feedback in higher education: A
collaborative effort across three UK Universities. In Proceedings INTED 2012, Valencia,
Spain. ISBN: 978-84-615-5563-5.
Whitelock, D., Gilbert, L., Hatzipanagos, S., Watt, S., Zhang, P., Gillary, P. Recio, A. (2012b).
Assessment for learning: Supporting tutors with their feedback using an electronic system that
can be used across the higher education sector. In Proceedings 10th International Conference
on Computer Based Learning in Science, CBLIS, 26–29 June 2012, Barcelona, Spain.
Whitelock, D., Twiner, A., Richardson, J. T., Field, D., & Pulman, S. (2015). OpenEssayist: A
supply and demand learning analytics tool for drafting academic essays. Paper presented at the
Proceedings of the Fifth International Conference on Learning Analytics and Knowledge,
Poughkeepsie, NY, USA.
Yeager, D. S., Paunesku, D., Walton, G. M., & Dweck, C. (2013). How can we instill productive
mindsets at scale? A review of the evidence and an initial R&D agenda. A White Paper
prepared for the White House meeting on Excellence in Education: The Importance of
Academic Mindsets.
160
D. Whitelock

Towards Full Engagement for Open
Online Education. A Practical Experience
from MicroMasters at edX
Rocael Hernández Rizzardini and Hector R. Amado-Salvatierra
Abstract This work presents an innovative framework with the aim to create full
engagement for the learners on massive open online learning environments through
a connectivist approach. The proposed framework relies on the importance of
creating engaging experiences before, during and after the ﬁnish of a course to
increase learners’ participation and reduce drop-out rates with the help of learning
analytics. This work presents a compelling idea in the universe of MOOCs: It
intends to expand the efforts of the learning design team to achieve pre and
post-course engagement, where engagement takes the form of an ongoing com-
munity of learners. This research provides results from the ﬁrst successful experi-
ences in two MicroMasters “Professional Android Developer”, taught in English,
and one specialization taught in Spanish: “E-Learning for teachers: create inno-
vative activities and content” at the edX platform. The MicroMasters shows to be a
great path for career advancement, especially for the under-employed.
Keywords Interaction ⋅Analytics ⋅Awareness ⋅MOOCs
e-learning ⋅Engagement
R. H. Rizzardini ⋅H. R. Amado-Salvatierra (✉)
GES Department, Galileo University, Guatemala, Guatemala
e-mail: hr_amado@galileo.edu
R. H. Rizzardini
e-mail: roc@galileo.edu
© Springer International Publishing AG 2018
S. Caballé and J. Conesa (eds.), Software Data Engineering for Network eLearning
Environments, Lecture Notes on Data Engineering and Communications
Technologies 11, https://doi.org/10.1007/978-3-319-68318-8_8
161

1
Introduction
Learning Analytics is a cornerstone of online learning environments, and this
afﬁrmation is particularly true as Ferguson (2012) recalls that Learning Analytics
can be deﬁned as the measurement, collection, analysis, reporting and data about
learners and their contexts. The ﬁnal aim of this process is to understand and
optimize the learning experience and the environment in which it occurs. Addi-
tionally, in (Ferguson 2012) it is highlighted that this deﬁnition of Learning Ana-
lytics could be taken to cover the large myriad of educational research, but it is
typically coupled with two main assumptions: that learning analytics make use of
pre-existing, machine-readable data, and that its techniques are envisaged to handle
large set of data that would not be practicable to deal with manually. This case is
evident while managing massive online open courses (MOOCs). It is important to
notice the observation from Baker and Siemens (2014), stating that the use of
analytics in education has grown nowadays for four fundamental reasons: (1) a
substantial increase in data quantity, (2) improved data formats, (3) advances in
computing, and (4) increased sophistication of tools available for analytics. In terms
of Awareness Learning Services, Dey and Abowd (1999) provided a deﬁnition of
Context, identiﬁed as any information that can be used to characterize the situation
of a learner. At the same time, authors stated that a system is context-aware if it uses
context to provide relevant information and/or services to the user, where relevancy
depends on the user’s tasks, actions and preferences.
In this sense, taking into consideration the particular case of massive online open
courses (MOOCs), it is possible to identify three phases related to the learner
life-cycle in a MOOC: pre-MOOC, MOOC and post-MOOC, moreover, there are
interesting studies in literature (Rivard 2013; Kloft et al. 2014; Onah et al. 2014;
Halawa et al. 2014; Gütl et al. 2014; Hernández et al. 2016) about the use of
learning analytics to identify the high drop-out and low approval rates from
learners. In terms of learner engagement, Kuh et al. (2007) deﬁned the term as a
two-fold condition. The ﬁrst one is represented by the amount of time and effort
learners put into their learning activities and self-study. The second component of
learner engagement is represented on how the institution deploys its resources and
organizes the learning activities in order to induce learners to participate in the
proposed activities that lead to the experiences and desired outcomes such as
persistence, satisfaction, learning, and ﬁnally, course completion and certiﬁcation.
Both components represent study ﬁelds based on the data analysis, but more
important based on the context and progress of each learner. In the same line, there
are studies in literature (Quaye and Harper 2014; Clark and Mayer 2016; Muntean
2011; Hernández et al. 2012) that explore on different approaches to motivate and
engage learners to be persistent and complete a course with a strong component of
service-based architectures and cloud technologies used for learning activities.
Nevertheless, it is not possible to ﬁnd an online framework proposing speciﬁc
actions to be performed to involve learners in all the phases of a MOOC
(pre-MOOC, MOOC and post-MOOC) while at the same time perform learning
162
R. H. Rizzardini and H. R. Amado-Salvatierra

analytics providing context-aware information based on each of the phases of a
MOOC.
This chapter presents an innovative online framework with the aim to create full
engagement for learners on massive open online learning environments. The pro-
posed framework relies on the importance of creating engaging experiences before,
during and after the ﬁnish of a course to increase learners’ participation and reduce
drop-out rates based on a strong component of learning analytics and providing
context-aware information based on the actual status of the learner. Moreover, we
are proposing an additional level of engagement based on the speciﬁc topic of the
course, as an example it is possible to consider the case of a learners interested in a
speciﬁc theme: “the Development of Android Applications”. In this sense, the
learner has an eager interest in the topic, the ﬁrst level of engagement, then it is
possible to identify the engagement in the course itself, this is related to the tasks,
contents, and learning activities within the learning environment. For this, it is
important that the learners are engaged at both levels, interested in the topic and the
learning activities presented in the course. This work presents the ﬁrst successful
experience results from two “MicroMasters” specializations in the edX platform:
“Professional Android Developer” and one specialization taught in Spanish:
“E-Learning for teachers: create innovative activities and content”. For this, the
chapter is organized as follows: Sect. 2 presents a literature review related to virtual
communities and MOOC frameworks. Then Sect. 3 describes the proposed Full
Engagement Educational Framework (FEEF), complemented with a ﬁrst validation
of the proposal in Sect. 4. Finally, in Sect. 5 conclusions are presented with a
lookout for future work.
2
Related Work
The creation of virtual communities around a common topic, but especially in the
context of e-Learning, is a well explored topic in literature (Hagel 1999; Barab
2003; Chiu et al. 2006). In this sense, the work by Barab (2003) clearly identiﬁes
that there is an evident gap between a real community of learners and a group of
individuals learning collaboratively, enrolled in a common virtual space but without
a genuinely sense of belonging. It is interesting to mention the experiences from the
“EdTechX: Educational Technology XSeries Program” and the “U.lab: Leading
From the Emerging Future course” taught through the edX platform using external
forums to enable different communication channels and a closer relationship with
the learners. Overall, the learning experiences have been improved through the
creation of a community of learners. In this sense, this concept is related to the term
“communities of practice” that was introduced by Wenger as: “… are groups of
people who share a concern or a passion for something they do and learn how to
do it better as they interact regularly” (Wenger 1998). Moreover, according to
Hlavac (2014) social communities can be classiﬁed into either Passion or Trigger
Event Communities. In a Passion Community, people join because it addresses
Towards Full Engagement for Open Online Education. A Practical …
163

things that speak to their deep needs and ideals. In seeking community, they look to
engage and interact with like-minded individuals, as well as hearing new infor-
mation around this topic. On the other hand, Trigger Event Communities are related
to speciﬁc events like the life of parents with kids or a speciﬁc holiday in the year.
In this sense, in order to create real engagement it is necessary to involve partici-
pants in real Passion Communities. This concept is particularly important in
MOOCs because in general, a good amount of the participants of MOOC courses
are professionals that are looking to update their knowledge and improve their
career with specialized content. It is important to highlight that although the
aforementioned courses used external forums and channels to improve the learning
experiences, it is not possible to identify an on-going and active community before
or after the end of the courses.
In terms of engaging experiences, the work by Malthouse and Peck (2011),
highlights that the most engaging experiences in media content that can be applied
to a learning scenario are related to prepare content that fulﬁlls and ﬁts into the
learners’ lives. The following concepts developed by Malthouse and Peck (2011)
are related to the impact that a content in a course should evoke in a learner:
• The Talk About and Share Experience: A few examples in this concept are: “I
bring up things I have read in the course in conversations with many other
people”, “Reading the course content gives me something to talk about”, “A big
reason I read this blog is to make myself more interesting to other people”, “I
show things in the course to people in my family”.
• The Civic Experience: This idea is related with comments as the following:
“Reading this course content makes me feel like a better citizen”, “Studying this
course makes me more a part of my community”.
• The Utilitarian Experience: “I get good ideas from this blog”, “I learn how to
improve myself from this course”, “It shows me how to do things the right way”,
“This Web gives good advice”, “It helps me make up my mind and make
decisions”, “The content makes me smarter”, “It looks out for my interests”, “It
is convenient”.
• The Timeout Experience: “It is my reward for doing other things”, “It’s an
escape”, “I like to go to this site when I am eating or taking a break”, “It takes
my mind off other things that are going on”.
• The Feel Good Experience: “Reading this content makes me feel good about
myself”, “When reading this content, I am worry-free”.
Additionally, it is possible to afﬁrm based on literature (Bolton et al. 2013; Davis
et al. 2010; Fischer 2011) that the interaction among participants is the cornerstone
to achieve valuable learning experiences and that at the same time, the participants
are using several ways of communication using social networks. This afﬁrmation is
particularly true based on the work of Bolton et al. (2013) explaining the new
generations and their use of social media and how Davis et al. (2010) highlights the
use of online media to increase engagement. In this sense, Irving et al. (2013) also
explore on the multi-access learning using different communication channels.
164
R. H. Rizzardini and H. R. Amado-Salvatierra

Complementarily, there is a model used to identify the behavior from students in
a MOOC, it is called Attrition Model for Open Learning Environment Setting
model (AMOES), developed by Gütl et al. (2014) with three sections: (1) attrition
and retention factors, (2) the open online learner classiﬁcation, and (3) the funnel of
involvement in an open learning setting. The Model identiﬁes that there are
external, internal, and student related factors, which inﬂuence whether a learner
belongs to a healthy, unhealthy, or persistence group of learners. In the context of
this chapter this is useful in order to identify the commitment and reduce the
drop-out rates. Examples of external factors include competing courses in the
MOOC space, the local technological infrastructure and culture. As these factors are
outside the control of a MOOC provider, institutions are eager to identify strategies
to mitigate them. Examples of internal forces include aspects of the organization of
the MOOC that are under the control of the MOOC provider.
Student factors relate to a learner’s desire to follow the MOOC and prior
knowledge in the ﬁeld of study. For example, some students enroll in a MOOC
because of their job, some for general interest, and some to gain a qualiﬁcation and
so on. The funnel of involvement in the learning setting has administrative aspects
(awareness and registration) and pedagogical aspects (activities and success) of the
MOOC. As the focus of attrition analysis is on the learners, Hernández et al. (2016)
have deﬁned three groups: healthy attrition, unhealthy attrition, and persistence
Fig. 1 Attrition model for
open learning environment
setting (AMOES) Hernández
et al. (2016)
Towards Full Engagement for Open Online Education. A Practical …
165

learners. Depending on the intention and motivation of the learning, ultimately they
can be divided further: exploring user, content reader learner, restricted learner,
disengage, and completer. Figure 1 presents the identiﬁed AMOES model that will
be used as a foundation for the online framework presented in Sect. 3.
Finally, In terms of online frameworks related to MOOCs, it is possible to ﬁnd in
literature interesting proposals for frameworks intended for educators to describe
and design MOOC courses (Alario-Hoyos et al. 2014), to improve the learning
outcomes in MOOCs based on different learning strategies (Fidalgo-Blanco et al.
2015), and a framework to take into account the accessibility in the different phases
of a virtual learning project (Amado-Salvatierra et al. 2016). However, there are no
proposals related to the different actions that need to be taken to maintain and
increase the engagement in MOOCs while at the same time measuring, collecting,
analyzing and reporting of data about the learners and their contexts in order to
perform speciﬁc actions to enhance the learners’ engagement.
3
Framework Proposal
The aim of the Full Engagement Educational Framework (FEEF) (Hernández and
Amado-Salvatierra 2017a, b) is to create a holistic learning experience that will last
before, during and especially after a MOOC course is ﬁnished. This framework is
composed of different strategies to identify speciﬁc target audiences in order to
create engaging experiences through valuable and interesting content based.
Moreover, the main idea is to move learners between each of the following groups:
Potential Learner, New Learner, Low-Activity Learner and Active Learner with a
strong use of learning analytics. The different strategies are planned to increase
learners’ activity and create a long-lasting relationship through high content value
and a sense of belonging in an active community. The deﬁnition of the proposed
Framework (FEEF), is based on three building blocks: The concept of the FEEF,
the components and the expected results. In this section a proposed Case Study and
the three building blocks will be presented.
3.1
Case Study
In order to identify each of the phases of the learner life-cycle (pre-MOOC, MOOC,
post-MOOC), the use case of a participant enrolled in a course is presented. In the
pre-MOOC phase, the learner was enrolled to the course two months before the
beginning of the learning experience. During this waiting time several scenarios can
happen, including a loss of interest from the learner in the course topics or the
appearance of new time-consuming tasks that will hinder the participation of the
learner in the course, leading to a potential drop-out. In this sense, it is important to
mention the high amount of learners that enroll on a course and actually never
166
R. H. Rizzardini and H. R. Amado-Salvatierra

log-into start the learning experience. The second phase of the participation cycle is
the learning experience within the MOOC, this phase involves the speciﬁc duration
of the course and the different activities planned by the teaching staff.
The post-MOOC phase begins after the end of the course and it is important to
highlight that nowadays a good part of the learners that are enrolled in MOOC
courses are looking to improve their careers or learn new competencies to apply to a
new job. It is interesting to mention that Jennings and Wargnier (2011) explored on
the so-called 70:20:10 rule (Eichinger and Lombardo 1996). This rule states that
only 10% of relevant knowledge and expertise is acquired through formal training
and education (e.g., MOOC courses), 20% through coaching and mentoring (e.g.,
from team-workers and bosses), and 70% via on-the-job learning, learning by
doing, and other actual experience-building activities. This rule is well-accepted
and used in the corporate training world, at the same time this could be interpreted
that learners need to continue learning, apply the content of the courses in their jobs
and get feedback from peers. This fact gives a potential opportunity to create a
community from the participants of a course interested in a common subject. At the
same time the idea to be part of a long-lasting and active community could engage
participants after the end of a course.
3.2
The FEEF Concept
The aim of the Full Engagement Educational Framework (FEEF) is to create an
holistic learning experience. The proposal is intended to tackle the engagement in
online courses with ﬁxed cohorts with speciﬁc begin and end dates, but at the same
time it is possible to be used with self-paced courses where there are no speciﬁc
cohorts with begin and end dates, aspects that require special attention in order to
give learners appropriate automatic or personal follow-up. This online framework is
composed of different strategies to identify speciﬁc target audiences in order to
drive engagement through valuable and interesting content. The online framework
is enhanced with a strong learning analytics components that will provide context-
aware information for an appropriate follow-up to the students based on their
activity and performance.
3.3
The Components of the FEEF
The proposed Full Engagement Educational Framework (FEEF) is composed by the
following components:
• An online community with open forums to discuss MOOCs topics and speciﬁc
topics not tied to the MOOCs contents
• Production of edutainment content to create engaging experiences
Towards Full Engagement for Open Online Education. A Practical …
167

• A blog to publish such high value content to targeted audiences
• Distribution of content to enrolled learners
• Distribution of at least 20% of the MOOC content as open tutorials
• Social media channels for content distribution to increase the reach to targeted
audience beyond enrolled learners
• Speciﬁc segmentation of the different types of enrolled learners with the aim of
providing targeted communication to take them to the next level of engagement
and course participation
Figure 2 presents the process of the Online Framework, moving learners from
the main categories: (potential learner, low activity learner and active learner).
3.4
The FEEF Expected Results
The expected results from the Full Engagement Educational Framework (FEEF) are
detailed as follows:
• Learning analytics related with the engagement perception and proper context-
aware information to learners
• Identiﬁcation of at least three main levels: potential learner, low to moderate
participation, high participation
Fig. 2 Process of identiﬁed learner participation in FEEF
168
R. H. Rizzardini and H. R. Amado-Salvatierra

• Identiﬁcation of the following stages: potential user stage, enrollment phase,
pre-MOOC, becoming active user, MOOC, post-MOOC
• Generation of a sense of belonging materialized with the enrollment in the next
courses of a proposed series of courses from the same institution
• Increased awareness and enrollment
• Generation of positive open inﬂuence on the Web in order to attract potential
learners continually
• Maintain a long-term relationship with the learner, independently of her current
engagement and participation level.
4
Framework Validation and First Results
The experiences presented in this chapter were prepared by Galileo University
within the edX platform with two MicroMasters specializations “Professional
Android Developer” and “E-Learning for teachers: create innovative activities and
content”. Figure 3 presents the main page of the “Professional Android Developer”
specialization. This specialization has ﬁve courses and the ﬁrst cohort had more
than 30,000 enrolled participants. It is important to notice that both specializations
Fig. 3 Professional android developer MOOC. Available at: https://www.edx.org/micromasters/
galileox-professional-android-developer
Towards Full Engagement for Open Online Education. A Practical …
169

are offered online with a series of courses with ﬁxed cohorts with speciﬁc begin and
end dates. However, even that there are ﬁxed cohorts, the framework was prepared
to be used with self-paced modality, a fact that allows to validate the framework
through time and with different scenarios.
For the proposed innovative framework two engaging communities for each of
the aforementioned specializations were prepared. The name of the communities
are: Android Developers (http://androiddeveloper.galileo.edu/) and e-Learning
Masters (http://elearningmasters.galileo.edu/). Figure 4 present the home page of
one of the communities to enhance the learning experience of the learners in all
phases. The communities, part of the full engagement educational framework, were
prepared following the seven principles proposed by Wenger et al. (2002):
1. The communities are designed for evolution with dynamic and updated content.
2. Facilities for an open dialogue between inside and outside perspectives.
3. Participation is encouraged at all levels, from starters to professionals.
4. The interaction was developed with public and private community spaces.
5. The communities have a focus on value.
6. There is a combination of familiarity and excitement.
7. The communities have a rhythm related to the publication of contents and
interaction.
For this innovative framework, speciﬁc engaging actions were identiﬁed for each
of the three phases of the learner life-cycle: pre-MOOC, MOOC and post-MOOC.
The proposed engaging experiences are intended to take the participants from a
Fig. 4 Android XCommunity. Available at: http://androiddevelper.galileo.edu
170
R. H. Rizzardini and H. R. Amado-Salvatierra

very low interest in pursuing the course at a speciﬁc time, to an increased level of
engagement that will enable the learner to gain real interest in the topic and invest
more time to learn in the near future.
Related to the pre-MOOC and MOOC phases, the teaching staff, part of the
proposed educational framework, prepared engaging and informative content to
periodically send e-mail messages to keep the learners interested and informed even
if they enrolled in the course three months before the start of the course and the high
value content is sent through all the MOOC course.
For this full engagement educational framework a real community is built
around each MOOC specialization. The aim is to create a community that persists
after the learner ﬁnished the course through the post-MOOC phase. While nurturing
a sense of belonging, sharing knowledge and increase skills, the community also
serves as a place where participants can ask for help with real job questions and
problems. The discussion forums are the heart of the community, thus all questions
and answers are done through the community. The communities provide blogs,
high quality content and videos related to the topic of the courses. It is important to
mention that the community resides outside of the MOOC platform, but is fully
integrated with it. In terms of the expected results from the FEEF. Table 1 presents
examples of high value content published in the Android blog’s community fol-
lowing the recommendation in Sect. 2 in order to create experiences as: the talk
about and share, the civic, the utilitarian, the timeout and the feel good experiences.
The blog posts are also sent through e-mail messages to the subscribed participants.
It is important to mention some of the main activities related to analytics within
the FEEF Framework. In this sense, Fig. 5 presents a country based distribution
from participants in the blog of the MicroMasters on E-Learning, it presents a
monthly average of 13,000 visitors, with an average session duration of more than
3 min, an equivalent of reading at least two posts in each visit. Figure 6, presents
the top channels of distribution and sessions daily average of 500 visits, the most
important channel is represented by the interaction from social networks. In this
Table 1 Title of high value content blog publications in android developers
High value content title
Best android courses and tutorials in 2017
The 5 perks of launching free android apps
Programming languages and tools you should learn to develop android apps
To be or not to be an android developer entrepreneur
How is android development changing the world?
How does ‘Google Play Protect’ keep your android device and data secure?
How to pick the best idea to create an android app
Monetize android Apps with business models
How to set a price on your android App
Google has released 25 versions of the android operating system since 2008
Towards Full Engagement for Open Online Education. A Practical …
171

Framework, the authors have been including new ways of interaction, one of this
innovative actions is the use of Push Notiﬁcation provided by OneSignal (n.d.), this
modality includes a notiﬁcation into the browser of visitors, and if they approve the
notiﬁcation they will be immediately notiﬁed on new interesting posts from the
course while browsing into Internet. Using this technique, Fig. 7 presents great
results showing that the average session from this channel is more than 6 min,
showing a sense of belonging while making the proposed blog as an important
reference.
Fig. 5 Data analytics for MicroMasters on e-learning, distribution of participants in blog based on
country of origin
Fig. 6 Data analytics for MicroMasters on e-learning, top channels of distribution and sessions
daily average of 500 visits
172
R. H. Rizzardini and H. R. Amado-Salvatierra

Additionally, an important example of the use of Data Analytics is the use of
actual search trends in order to publish attractive content and trending topics to the
users of the community. In this sense, Fig. 8 presents a screenshot of one of the
most shared blog post titled: “Do you know how to use Neuro-Learning in your
Virtual Education Environment?” This high value content was prepared based on
current search trends on the keyword “neurolearning”. Based on this keyword and
the objective groups the content was prepared and published in the blog. The result
Fig. 7 Innovative technique based on push notiﬁcations, with an average session duration of more
than 6 min
Fig. 8 Screenshot from e-learning masters blog presenting one of the most shared posts (more
than 1,900 shares) about neuro-learning
Towards Full Engagement for Open Online Education. A Practical …
173

was a blog post with great sharing rates within social networks and the attraction of
more visits to the online community.
For the particular case of the “Professional Android Developer” MicroMasters,
the FEEF has proven to create long-lasting engaging experiences with an average of
12,000 weekly readers in the blog (http://androiddeveloper.galileo.edu). Speciﬁ-
cally, using the PTAT (People Talking About This) metric, which represents the
number of unique people that created a story about a page or on a page via different
actions as the following: Like to page, Like to post, content sharing; Mentions, tags,
event registration; Comments on the wall, retweets, answer to a poll. For the
particular case of both blogs, the monthly average PTAT is 10,000. Reaching an
average of 50,000 viewers per week, and with peaks of over 200,000 viewers per
week.
Currently in the MicroMasters MOOCs the Active Learners are 20%. Further-
more, New Learners transitioning to Active Learners are 49%, and New Learners
that become Low Activity Learners are 51%. Overall, it is possible to afﬁrm that
40% of enrolled learners keep engaged with the Android topic independently of
their level of progress in the MOOCs.
The Full Engagement Educational Frameworks has proven to create lasting
engaging experiences while moving learners from being inactive to low activity to
higher activity within the MOOC.
Additionally, it is important to mention that the discussion forums that are used
during the MOOC course provide enhanced and easy tools to foster collaboration,
and increase visibility of community leaders and major contributors, providing
means for community recognition. Gamiﬁcation instruments are introduced as part
of this process. An important fact is that the community is fully open, and will
remain open after the end of the course so the learners are able to browse through it
without login, and also is possible to participate into it without being member of a
MOOC in order to create a live and growing community to enhance the
post-MOOC phase. Actually for the common conﬁguration of MOOCs, the dis-
cussion forums represent and internal learning activity, but the idea is to open the
access to general topic discussion forums to all participants so that the content and
contribution will not be lost at the end of the course, even the enrolled learners are
not able to review the discussion forums again from the expiration date of the
courses, in this sense it is important to provide an open space to involve learners,
potential and future learners and public in general to make use of the interesting
discussions and questions resolutions of topics of general interest. At the same time,
internal forums to discuss particular aspects of the course and methodology will be
taken into account.
174
R. H. Rizzardini and H. R. Amado-Salvatierra

5
Conclusions and Future Work
Nowadays the MOOC movement brings together thousands of learners around a
common topic for a short period of time. However, the learner’s experience may
last up to three to four months since the enrollment, creating a long waiting time
that could be enhanced by creating engaging experiences. On the other hand, for
self-paced MOOC courses the students are starting the learning experience every
day, and they could feel alone or without attention if they do not get the appropriate
follow-up. In the particular case of the both aforementioned MicroMasters, the ﬁrst
editions were performed as a self-paced course, but now the courses are having
ﬁxed starting dates, with small timeouts in order to have cohesive groups. In the
particular case of specializations with more than three courses, it is convenient to
deﬁne a speciﬁc deadline in comparison of a self-pace modality.
This chapter presents a thought-provoking work to evolve the MOOC concep-
tion to a wider scope through the use of engaging experiences within an external
community. The authors explore on the three phases related to the learner life-cycle
in a MOOC: pre-MOOC, MOOC and post-MOOC with the aim to reduce the high
drop-out rates and propose actions to engage learners from the enrollment steps.
This chapter proposes a Full Engagement Educational Framework (FEEF) in the
context of virtual learning but especially for MOOCs. The deﬁnition of the pro-
posed Online Framework (FEEF), is based on three building blocks: The concept of
the FEEF, the components and the expected results. The FEEF is based on the use
of communities of learners, active interaction and high quality content to motivate
the learners to start, ﬁnish and approve a MOOC course, while at the same time
giving the learner the opportunity to be part of a strong and long-lasting commu-
nity, a Passion Community.
This work presents the experience of the validation phase, showing the ﬁrst
FEEF expected results, highlighting the high open rates of email notiﬁcation and
the low bounce rates from the external blog/communities, prepared for two
“MicroMasters” specialization in edX platform. As a future work, the metrics and
best practices of running the FEEF in different context will be prepared, showing
how the learners perceived, interacted and engaged with the communities, serving
as a motivation mean to conclude the courses and being part of a learning com-
munity. The main aim of this research is to present an innovative strategy for
Higher Education, which enables institutions to provide a holistic learning expe-
rience for today’s learners. Learners that live in a highly connected and ﬂat world,
where high quality knowledge is available at the reach of their ﬁngertips. Com-
plementarily, future work includes the external communities will be expanded as a
showcase for Portfolios and Job Market Place to be evolved into a strong profes-
sional network for the post-MOOC phase. The professional networks will be
enhanced with external applications like LinkedIn (n.d.) and regional face-to-face
meetings and workshops with Meetup (n.d.).
Towards Full Engagement for Open Online Education. A Practical …
175

Acknowledgements This work is partially supported by European Union through the Eras-
mus + programme—projects MOOC-Maker and ACAI-LA.
References
Alario-Hoyos, C., Pérez-Sanagustín, M., Cormier, D., & Kloos, C. D. (2014). Proposal for a
conceptual framework for educators to describe and design MOOCs. Journal of Universal
Computer Science, 20(1), 6–23.
Amado-Salvatierra, H. R., Hilera, J. R., Tortosa, S. O., Rizzardini, R. H., & Piedra, N. (2016).
Towards a semantic deﬁnition of a framework to implement accessible e-Learning projects.
Journal of Universal Computer Science, 22(7), 921–942.
Baker, R. S., & Siemens, G. (2014). Educational data mining and learning analytics. In Learning
analytics (pp. 61–75). Springer, New York.
Barab, S. (2003). An introduction to the special issue: Designing for virtual communities in the
service of learning. The Information Society, 19(3), 197–201.
Bolton, R. N., Parasuraman, A., Hoefnagels, A., Migchels, N., Kabadayi, S., Gruber, T., et al.
(2013). Understanding generation Y and their use of social media: A review and research
agenda. Journal of Service Management, 24(3), 245–267.
Chiu, C. M., Hsu, M. H., & Wang, E. T. (2006). Understanding knowledge sharing in virtual
communities: An integration of social capital and social cognitive theories. Decision Support
Systems, 42(3), 1872–1888.
Clark, R. C., & Mayer, R. E. (2016). E-learning and the science of instruction: Proven guidelines
for consumers and designers of multimedia learning. Wiley.
Davis, R., Malthouse, E. C., & Calder, B. J. (2010). Engagement with online media. Journal of
Media Business Studies, 7(2), 39–56.
Dey, A., & Abowd, G. (1999). Towards a better understanding of context and context-awareness.
In Handheld and ubiquitous computing (pp. 304–307). Springer, Berlin, Heidelberg.
Eichinger, R., & Lombardo, M. (1996). The career architect development planner. Minneapolis:
Lominger Limited.
Ferguson, R. (2012). Learning analytics: drivers, developments and challenges. International
Journal of Technology Enhanced Learning, 4(5–6), 304–317.
Fidalgo-Blanco, Á., Sein-Echaluce, M. L., & García-Peñalvo, F. J. (2015). Methodological
approach and technological framework to break the current limitations of MOOC model.
Journal of Universal Computer Science, 21(5), 712–734.
Fischer, G. (2011). Understanding, fostering, and supporting cultures of participation. Interactions,
18(3), 42–53.
Gütl, C., Rizzardini, R. H., Chang, V., & Morales, M. (2014). Attrition in MOOC: Lessons learned
from drop-out learners. In International workshop on learning technology for education in
cloud (pp. 37–48). Springer International Publishing.
Hagel, J. (1999). Net gain: Expanding markets through virtual communities. Journal of Interactive
Marketing, 13(1), 55–65.
Halawa, S., Greene, D., & Mitchell, J. (2014). Dropout prediction in MOOCs using learner activity
features. Experiences and best practices in and around MOOCs (Vol. 7).
Hernández, R., & Amado-Salvatierra, H. R. (2017a). Towards full engagement for open online
education: A practical experience for a micromaster. In European conference on massive open
online courses (pp. 68–76). Springer, Cham.
Hernández, R., & Amado-Salvatierra, H. R. (2017b). Full engagement educational framework: A
practical experience for a micromaster. In Proceedings of the Fourth (2017) ACM Conference
on Learning@ Scale (pp. 145–146). ACM.
176
R. H. Rizzardini and H. R. Amado-Salvatierra

Hernández, R., Amado-Salvatierra, H. R., Guetl, C., & Smadi, M. (2012). Facebook for CSCL,
Latin-American experience for professors. In IEEE 12th International Conference on
Advanced Learning Technologies (ICALT) (pp. 327–328). IEEE.
Hernández, R., Morales, M., & Guetl, C. (2016). An attrition model for MOOCs: Evaluating the
learning strategies of gamiﬁcation. Formative assessment, learning data analytics and
gamiﬁcation, Chapter: 14 (pp. 295–310). Elsevier.
Hlavac, R. (2014). Social IMC Social Strategies with Bottom-Line ROI. Createspace Independent
Publishing Platform.
Irvine, V., Code, J., & Richards, L. (2013). Realigning higher education for the 21st century
learner through multi-access learning. Journal of Online Learning and Teaching, 9(2), 172.
Jennings, C., & Wargnier, J. (2011). Effective learning with 70:20:10. CrossKonledge: The new
frontier for the extended enterprise.
Kloft, M., Stiehler, F., Zheng, Z., & Pinkwart, N. (2014). Predicting MOOC dropout over weeks
using machine learning methods. In Proceedings of the EMNLP 2014 Workshop on Analysis of
Large Scale Social Interaction in MOOCs (pp. 60–65).
Kuh, G. D., Kinzie, J. Buckley, J. A., Bridges, B. K., & Hayek, J. C. (2007). Piecing together the
learner success puzzle: Research, propositions, and recommendations. ASHE higher education
report (Vol. 32, No. 5). San Francisco, CA: Jossey-Bass.
Malthouse, E. C., & Peck, A. (Eds.). (2011). Medill on media engagement. Hampton Press.
Muntean, C. I. (2011). Raising engagement in e-learning through gamiﬁcation. In Proceedings of
6th International Conference on Virtual Learning ICVL (pp. 323–329).
Onah, D. F., Sinclair, J., & Boyatt, R. (2014). Dropout rates of massive open online courses:
Behavioural patterns. In EDULEARN14 Proceedings, pp. 5825–5834.
Quaye, S. J., & Harper, S. R. (2014). Learner engagement in higher education: Theoretical
perspectives and practical approaches for diverse populations. Routledge.
Rivard, R. (2013). Measuring the MOOC dropout rate. Inside Higher Ed, 8.
Wenger, E. (1998). Communities of practice: Learning, meaning, and identity. Cambridge
university press.
Wenger, E., McDermott, R., & Snyder, W. M. (2002). Seven principles for cultivating
communities of practice. In Cultivating communities of practice: A guide to managing
knowledge (Vol. 4).
LinkedIn (n.d.) Linkedin Social Network. Retreived from https://www.linkedin.com/.
Meetup (n.d.) Meetup. Retreived from https://www.meetup.com/.
OneSignal (n.d.). OneSignal. Retreived from https://www.onesignal.com/.
Towards Full Engagement for Open Online Education. A Practical …
177

A Data Mining Approach to Identify
the Factors Aﬀecting the Academic Success
of Tertiary Students in Sri Lanka
K. T. Sanvitha Kasthuriarachchi, S. R. Liyanage and Chintan M. Bhatt
Abstract Educational Data Mining has become a very popular and highly impor-
tant area in the domain of Data mining. Application of data mining to education
arena arises as a paradigm oriented to design models, methods, tasks and algorithms
for discovering data from educational domain. It attempts to uncover data patterns,
structure association rules, establish information of unseen relationships with edu-
cational data and many more operations that cannot be performed using traditional
computer based information systems. It grows and adopts statistical methods, data
mining methods and machine-learning to study educational data produced mostly
by students, educators, educational management policy makers and instructors. The
main objective of applying data mining in education is primarily to advance learning
by enabling data oriented decision making to improve existing educational practices
and learning materials. This study focuses on ﬁnding the key factors aﬀecting the
performance of the students enrolled for technology related degree programs in Sri
Lanka. The ﬁndings of this study will positively aﬀect the future decisions about
the progress of the students’ performance, quality of the education process and the
future of the education provider.
Keywords
Data mining ⋅Educational data mining ⋅Classiﬁcation
Knowledge discovery ⋅Feature extraction
K. T. Sanvitha Kasthuriarachchi (✉)
Faculty of Graduate Studies, University of Kelaniya, Dalugama, Sri Lanka
e-mail: sanvithat85@gmail.com
S. R. Liyanage
Faculty of Computing and Technology, University of Kelaniya, Dalugama, Sri Lanka
e-mail: sidath@kln.ac.lk
C. M. Bhatt
Chandubhai S. Patel Institute of Technology, Charotar University of Science and Technology,
Gujarat, India
e-mail: chintanbhatt.ce@charusat.ac.in
© Springer International Publishing AG 2018
S. Caballé and J. Conesa (eds.), Software Data Engineering for Network eLearning
Environments, Lecture Notes on Data Engineering and Communications
Technologies 11, https://doi.org/10.1007/978-3-319-68318-8_9
179

180
K. T. Sanvitha Kasthuriarachchi et al.
1
Introduction and Motivation
Higher education institutes are expected to achieve a plethora of development goals
while operating in a progressively complex and competitive environment. In this
socio-economic context, institutions of higher education are expected to respond to
factors such as the rising need to increase the percentage of students in Science,
Technology, Engineering and Mathematics (STEM), including workplace graduate
attributes and ensuring that the quality of teaching and learning programs are both
nationally and globally applicable. Various stakeholders are expect higher education
institutions to proﬁt while support from government and private sectors are decreas-
ing (Hazelkorn 2007).
Factors aﬀecting a student’s performance can be uncovered if proper analysis of
the learning process (Chatti et al. 2012) is carried out. This context has prompted
the advent of the new domain; Educational Data Mining (EDM), where data mining
practices are applied to data from educational settings to address signiﬁcant educa-
tion related questions. The main objective of applying data mining in education is
mainly to advance the learning by enabling data oriented decision making to improve
existing educational practices and learning materials. Identifying the factors aﬀect-
ing students’ learning by analysing their learning behaviours or interactive content
can be used for delivering adaptive learning and providing personalized learning
contents, user interfaces or practices. The analysis of large-scale educational data
from multiple facets can lead to the identiﬁcation of important indicators for eval-
uating the educational status and to have insights into students’ interactions with
course materials, fellow students and teachers.
Furthermore, EDM can be used to explain students’ learning performance in edu-
cational settings via analyzing the learning behavioral patterns of diﬀerent groups
of students. EDM studies can explore the ways of updating existing theories while
building new theories as well. This is expected if EDM aims to advance research and
the practice of learning, teaching, and education (Gasevic 2017). The accessibility
of digital data has made it possible to analyse diverse aspects of students’ learning
skills (Clarke et al. 2013). Observing the activities of students in learning manage-
ment systems (LMS) provides a vast amount of data that can be employed in helping
students in learning, enhancing the overall learning experience and engaging in the
learning environment. In addition, the data is also created by instructive bodies which
use applications to classes, students and manage courses (Sin 2015). Bring your own
device (BYOD), advanced learning analytics, blended, hybrid and disruptive educa-
tional environments, blended learning experiences, networks of connected learners,
simulation based inquiry learning with virtual manipulative provide new opportuni-
ties to apply EDM to understand and enhance learning and teaching (Salomon 2016).
Student information can be collected from many diﬀerent contexts such as store-
houses of data, reﬂecting the way the students are learn, students’ behavior in web
based education or e-learning, information about interaction between teachers and
learners. Modeling student performance is an important task for educators and stu-
dents, since the educators are able to mine the data and gain insights into how the

A Data Mining Approach to Identify the Factors ...
181
quality of teaching and learning steps must be improved, what factors aﬀect the per-
formance of the students and how better student role models are introduced to the
society in order to increase the proﬁt of the educational provider. Prediction could be
a technique in EDM that predicts a future state instead of a current state (Jindal and
Borah 2013). As an example, a man used educational data of current CGPA score to
predict future CGPA score of the scholar within the ﬁnal semester of their studies.
Furthermore, this method was also used to predict the dropout rate by Dekker et al.
(2009) and retention management of students by Zhang et al. (2010).
Another technique in EDM is Classiﬁcation. It’s a two way technique (training
and testing) that maps information into a predeﬁned category. It’s the method of
supervised learning to separate information into totally diﬀerent category data sets.
An example of the utilization of this system is within the use of classiﬁcation of
scholars into three groups; low, medium and high risk students. High risk students
have a more chance of failure. There are numerous techniques and approaches for
data classiﬁcation like Support Vector Machine, Artiﬁcial Neural Network, Bayesian
Networks etc.
The key objective of this research is the analysis and prediction of student perfor-
mance and identifying the key factors which aﬀect their performance using a recent
real-world data gathered from a degree awarding institute in Sri Lanka by querying
their database. Three main data mining algorithms were used to model the perfor-
mance such as Naïve Bayes algorithms (NB), Decision Tree (C5.0) algorithm and
Random Forest algorithm (RF). Then, the results of each algorithm have been tested
against each other in order to identify the most signiﬁcant factors which determine
the students’ performance with a higher accuracy level. Further, the factors derived
by the most signiﬁcant algorithms have been taken and tested for their impact with
regard to the prediction of the target attribute. The predicted results will be helpful
in making important decisions in the domain of education such as giving recom-
mendations to the students, providing feedback to students on how to improve their
grades enlightening instructional developers on eﬀective course alignment, reducing
the amount of course dropouts, managing the course enrollment procedure and etc.
2
Background and Literature Review
2.1
Knowledge Discovery in Databases (KDD)
KDD is an iterative process of ﬁnding knowledge from raw data of large databases.
It mainly consists of data selection, preprocessing, data transformation, data mining
and data interpretation (Fig. 1).

182
K. T. Sanvitha Kasthuriarachchi et al.
Selection of  Raw Data
Preprocesing Target Data
Transforme Preprocessed Data
Data Mining
Interpretation of Knowledge
Fig. 1
The steps of knowledge discovery in databases
2.1.1
Data Preprocessing
The data found in databases is usually gathered from questionnaires, surveys, and
interviews. It’s a common knowledge that there is some incomplete data which is
known as lack of some attribute values or an aggregated values for attributes, noisy
data which means the data with errors or containing outlier values and inconsis-
tent data known as data with several discrepancies between attribute values. When
the applicable data cannot be used further due to misinterpretation, unavailability of
attributes of interest or apparent irrelevance, such data may not be collected. As a
result, data collection instrument may be developed with some errors or errors can
occur during the manual data entering process. All these cause incomplete, noisy
and inconsistent data in the database. Under data preprocessing, the data has to be
cleaned by handling the missing values, removing outliers, identifying and smooth-
ing noisy data, and determining contradictions to generate a consistent collection of
data for analysis.
2.1.2
Data Integration and Transformation
Data integration often requires to merge data from multiple data stores such as
databases, data cubes or ﬂat ﬁles into a coherent data store. There may be nam-
ing conﬂicts between diﬀerent sources even though they mean the same. There may
be redundant data as well. Redundant data also has to be avoided in order to per-
form the data mining tasks with a complete set of data. Once the data is integrated,
it should be transformed into a platform which is capable of performing data min-
ing. There are diﬀerent data transformation methods such as smoothing, aggregation,

A Data Mining Approach to Identify the Factors ...
183
generalization, normalization, attribute construction and feature construction. Data
reduction can be performed using attribute/attribute subset selection, cube aggrega-
tion, numerosity reduction, dimensionality reduction, concept hierarchy generation
and discretization.
2.1.3
Data Mining
Next, an appropriate data mining method should be selected. There are two types in
mining; predictive mining and descriptive mining. In predictive data mining, predicts
unknown or future values of a variable according to the change of one or more vari-
ables in the data set. Description is used to ﬁnd human-interpretable patterns which
describes the data. classiﬁcation, clustering, regression, summarization, dependency
modeling and deviation detection are the types of performing descriptive and pre-
dictive data mining tasks.
1. Classiﬁcation—Classiﬁcation is used for the prediction of the class of each data
item.
2. Regression—Regression is predicting a real-valued of a variable.
3. Clustering—Clustering ﬁnds clusters of data that are equal in some way to each
other.
4. Summarization—This is used to ﬁnd a compacted description for a dataset.
5. Dependency Modeling—This is used to ﬁnd a model which deﬁnes dependen-
cies between variables.
6. Change and Deviation Detection—This determines the important variations in
the data from formerly digniﬁed values.
2.1.4
Data Interpretation and Consolidating the Discovered Knowledge
The data analyst visualizes the extracted patterns and models in an interpretation. In
consolidating the knowledge discovered, it incorporates the discovered knowledge
into the performance system and documents it. This step can be used to check and
resolve potential conﬂicts with previously believed knowledge.
2.2
Data Mining Methods and Algorithms
Classiﬁcation, clustering, regression are the most frequently used data mining algo-
rithms for data mining tasks.

184
K. T. Sanvitha Kasthuriarachchi et al.
2.2.1
Classiﬁcation
This method assigns data items in a collection to target categories or classes. The
main task of classiﬁcation is predicting the target class for each case in the data. The
target attribute can be binary or categorical. Classiﬁcation algorithms measure how
the model can understand a single aspect of data from a combination of other aspects
of the data. Naïve Bayes, Decision Trees, Random Forest, Rule-Based Classiﬁcation
are some popular classiﬁcation mining methods.
Naïve Bayes Classiﬁcation
The Bayesian classiﬁcation represents a supervised learning method and statis-
tical method for classiﬁcation. The Naïve Bayes simpliﬁes the calculation of prob-
abilities by assuming that the probability of each attribute of a given class value is
independent of all other attributes. This classiﬁer is able to handle a random number
of independent variables in forms of either continuous or categorical. Given a set
of variables, X = x1, x2, x3 ..., xd, to build the posterior likelihood for the event Cj
among a set of possible outcomes C = c1, c2, c3 ..., cd. Using Bayes’ rule:
p(Cj|x1, x2, … , xd) ∞p(x1, x2, … , xd|Cj)p(Cj)
(1)
where, p(Cj|x1,x2, ..., xd) is the posterior likelihood of class membership which the
probability that X belongs to Cj.
p(X|Cj) ∞∏d
k=1 p(Xk|Cj)
(2)
and rewrite the posterior as:
p(Cj|X) ∞p(Cj)
d
∏
k=1
(p(Xk|Cj)
(3)
Using the above Bayes’ rule, label a new case X with a class level Cj that realizes
the highest posterior probability/likelihood.
Decision Tree Induction
ID3, C4.5, C5.0, CART are diﬀerent types of tree algorithms use in data mining
task. A decision tree is an algorithm that uses a root node, leaf nodes and branches to
connect nodes in classiﬁcation of data. Except root and leaf, other nodes represent a
test on a variable, results of a test is represented by a branch, and each leaf represents
a class. Decision tree algorithms use some selected attributes of the data set based on
the information gain measurement which is known as entropy of the attribute. The
attributes which have the high information gain value and high gain ratio value will
be selected for splitting the attributes.

A Data Mining Approach to Identify the Factors ...
185
Random Forest
Random forest gives more precise predictions even for a large sample size. This
captures the discrepancy of several input variables at the same time and allows high
number of observations to participate in the prediction. This is a very convenient
machine learning technique which performs both classiﬁcation and regression activ-
ities. It also commences dimensional reduction methods, outlier detection, treats,
missing values and other important steps of data study, and does a better prediction.
Rule-Based Classiﬁcation
In this algorithm, the model will represent as a set of IF-THEN rules which are
useful in representing information or bits of knowledge. The IF-THEN rule is written
as;
IF condition THEN statement
Since the results of decision tree is diﬃcult to interpret, the rule based classiﬁca-
tion is used by extracting IF-THEN rules based on the decision tree. The rules are
generated by traversing the tree from top/root node to the bottom/leaf nodes. In writ-
ing the IF-THEN rule, each splitting point is connected via a logical AND operation.
2.2.2
Clustering
In clustering, the data is subdivided into clusters or groups. When the distance
between objects is minimal, those objects have similar characteristics. When the dis-
tance between two objects is sizeable, then those objects are not similar and they are
in diﬀerent clusters.
K-Means Algorithm
K-means algorithm, groups or clusters observations into subsets based on the sim-
ilarities of responses on multiple variables in this method. The observations with
similar responses are grouped together to perform clusters. This is an unsupervised
learning method which has no speciﬁc response variable included in the analysis.
Less variance within cluster means that the observations within the cluster are sim-
ilar to each other in their pattern of response on the clustering variables. More vari-
ance between clusters means the clusters are distinct and no overlap exists between
the clusters. This method can also be used as a data reduction technique that occu-
pies many variables and reduces them to a single categorical variable that has many
categories as the number of clusters identiﬁed in the data set.
2.2.3
Regression
In regression, the predicted variable is a continuous variable. There are diﬀerent
famous regression methods used for educational data mining such as linear regres-
sion, logistic regression and neural networks.

186
K. T. Sanvitha Kasthuriarachchi et al.
Linear regression
This uses to model the association between two variables by ﬁt into a linear equa-
tion to data. The accuracy of the model is assessed by the Mean Square Error (MSE)
which is the diﬀerence between the model estimated value of the response variable
denoted as ̂y, and the observed value of the Y variable. When the complexity of the
model increases, the variance will increase and the bias will decrease. Variance is
the chance in parameter estimated across diﬀerent data sets, where the bias can be
described as how distant the model estimated values are from the true values.
MSE = 1
n
∑n
i=1(ŷi −yi)2
(4)
Logistic Regression
Logistic regression is used to describe data and the relationship between one
dependent binary variable and one or more nominal, ordinal, interval or ratio-level
independent variables. The accuracy of logistic regression model can be described
as how well a model correctly classiﬁes the observations. Model with low predic-
tion error will have a high percentage of correctly classiﬁed observations and a low
percentage of misclassiﬁed observations.
Neural Network
Neural network is a data mining algorithm which is modeled for data handling and
are especially valuable for distinguishing the key relationships among an arrange-
ment of factors or patterns in the data. Mining undertakings, for example, pattern
classiﬁcation, time series analysis, prediction, and clustering should be possible with
this algorithm.
Neural networks are nonlinear models. As real world data or relationships are
inherently nonlinear, traditional linear tools may suﬀer from signiﬁcant biases in
data mining. Neural networks with their nonlinear and non-parametric nature are
more to display complex data mining issues. They can take care of issues that have
uncertain patterns or data containing inadequate and noisy information with a sub-
stantial number of factors.
2.3
Educational Data Mining
EDM is an emerging discipline focused on developing techniques for exploring
unique data that originates from educational settings, and using those strategies to
better understand students and the settings which they learn in. EDM investigates an
array of areas, including individual learning from educational software, computer
supported collaborative learning, computer-adaptive testing and the factors that are
associated with student failure or non-retention in courses. There are many potential
contexts for applying data mining in education, for example, advancement of student
models, creating strategies for pedagogical support, making choices to developing

A Data Mining Approach to Identify the Factors ...
187
better learning frameworks, enhancing the performance of students, reducing the
dropout rate of students etc.
2.4
Recent Studies Using Educational Data Mining Methods
Data mining has been widely used for to analyse ﬁnancial data in banking indus-
try for loan payment prediction, classiﬁcation and clustering the customers and the
detection of ﬁnancial crimes. In the marketing industry, it collects a large quantity of
data from transportation, consumption of goods, analyzing purchase history of cus-
tomers. In order to identify the customers’ buying patterns, customer retention and
satisfaction are the most important areas, which facilitates the mining advantages.
Telecommunication is one other important industry which deals with large sets of
data with the services such as fax, cellular phone, pager, images, internet messenger,
web data transmission, e-mail, etc. Mining the telecommunication data and analyz-
ing the fraudulent patterns, unusual patterns, mobile telecommunication services are
the main beneﬁts of data analysis for this industry. Health care industry, Climate data
analysis, education are other areas in which data mining can be applied for analyses
using the data gathered daily.
As the above mentioned industries, data mining can be successfully applied in
education industry to analyze students’ data to derive knowledge, and give predic-
tions. Most of the studies were based on modeling the students, visualization of stu-
dents’ data, giving support for course creators, grouping students, analyzing the stu-
dent social network usage and online learning data analysis. Students’ performance
prediction is also another educational task that could be done using data mining.
2.4.1
Students’ Modeling
The intention of student modelling is to construct cognitive models, by modelling the
skills and knowledge of the students. Data mining techniques were used to analyze
enthusiasm, happiness, learning methods, emotional status to model the students.
Logistic regression, support vector machines, and decision trees were used to
test the results of diﬀerent prediction type data mining activities such as student
mental models in Intelligent Tutoring Systems (Rus et al. 2009). It has constructed
student models using sequential pattern mining. In this instance, the knowledge had
been acquired automatically (Antunes 2008). Clustering and classﬁcation methods
were used to minimize the development expenses in constructing user models and
to enable transferability in intelligent learning environments (Amershi and Conati
2009).

188
K. T. Sanvitha Kasthuriarachchi et al.
2.4.2
Visualization of Students’ Data
Visualization of educational data in diﬀerent behaviours was a way to perform min-
ing using graphical techniques. Students’ online activities such as students’ involve-
ment in learning and answering, mistakes, students’ attendance, teachers’ comments
on students’ work, overview of discussions, access to resources, and results on
quizzes and assignments are diﬀerent types of information that can be construed
using visualization methods.
2.4.3
Support for Course Creators
The objective of this is to provide comments to course creators or administrative
staﬀin decision making to decide the way to enhance learning of students’ and form
instructional resources.
Clustering, Association rules, Classﬁcation, sequential pattern analysis, depen-
dency modelling, and prediction have been used to improve web-based learning
environments to assess the learning process (Kay et al. 2006). Cluster analysis, Asso-
ciation analysis, and case-based reasoning have also been used to establish course
instruments and allocate homework at various diﬃculty levels (Shen et al. 2003).
In order to ﬁnd information, related to teachers to analyse students’ data further, or
identifying teaching ingredients and assessments in adaptive learning environments
a study has also been done (Tsai et al. 2001).
2.4.4
Grouping Students
The main objective of this category is to generate categories of students according
to their individual behaviours.
There are more studies conducted in this area by diﬀerent researchers. Cluster
analysis has been used to cluster the task of a set of students into subsections to
identify the features and characteristics which are common to the instances in the
cluster (Romesburg 2004). Another study was an analysis of learners’ personality
and learning styles based on the data collected from online courses using fuzzy clus-
tering algorithm (Tian et al. 2008). Clustering and Bayesian network were used to
group students according to their skills by another set of researchers and also K-
means clustering algorithm was used to group students who show similar learning
behaviours in online learning records, exam scores and assignment scores (Chen
et al. 2007).
2.4.5
Social Network Analysis
Social Network Analysis is used to categories people by social relationships such as
friendship,
informative
exchange
or
collaborative
relations.
Diﬀerent
data

A Data Mining Approach to Identify the Factors ...
189
mining methods were used to mine social networks in educational arena. Collab-
orative ﬁltering is the most commonly used method of ﬁltering educational data.
That has been used to give recommendations by identifying the similarities and dif-
ferences between the favorites of students and proposed to develop an e-learning
endorsement service systems (Li et al. 2007).
2.4.6
Students’ Performance
There are some studies that have addressed the issue of identifying the factors that
aﬀect students’ performances. Data mining algorithms have been used to classify
students based on their ﬁnal exam marks through moodle usage data (Romero et al.
2008). Students’ grades were predicted by feed forward neural networks and back
propagation algorithms (Gedeon et al. 1993). Another set of researchers used Naïve
Bayes algorithm to predict the performance of the students (Haddway et al. 2007) and
diﬀerent rule based systems were used to predict the performance in an e-learning
environment (Nebot et al. 2006). Feature examination from logged data in a web
based system has been applied by another study for the prediction, monitoring and
evaluation of students’ performance (Shangping et al. 2008). Prediction of univer-
sity students’ satisfaction was another research that has been done using regression
and decision tree algorithms (Myller et al. 2002). Diﬀerent regression techniques
were used to predict students’ marks in an open university using locally weighed
linear regression, linear regression, model trees, neural networks, and support vector
machines (Kotsiantis 2005) and for predicting high school students’ prospect of suc-
cess in university was another study conducted in educational data mining domain.
2.5
Signiﬁcance of the Proposed Method
Most of the existing research studies have used statistical methods for the analysis.
Only very few studies have been done based on data mining algorithms to predict
the future of students and the educational environment/instruments that they use.
This factor motivated the authors to use data mining algorithms in performing the
analysis and evaluating the accuracy of the model using the algorithms. Therefore,
three data mining algorithms were used in this analysis to model the performance of
students which will be beneﬁcial to educators and students in many aspects.
3
Problem Statement
Data mining is an important technique used by educators to extract essential data
and make decisions related to the pedagogical development as well as to improve
the instructional design. Data mining tools are available to be used by educators and

190
K. T. Sanvitha Kasthuriarachchi et al.
non-Data mining experts to perform mining tasks conveniently. Not only educators
but also course designers, will beneﬁt from educational data analysis.
When the factors that motivate students to excel in tertiary education are identi-
ﬁed, the key people involved in educational decision making will be able to make
prediction easily, by observing the patterns of those attribute values of a set of stu-
dents. Then, in order to align the students towards success in tertiary education, the
analysis can be used.
Even though many studies have been conducted in this ﬁeld, most of them have
concluded that they need to redo the analysis using diﬀerent algorithm to test the
accuracy of the study. Since there exists miner accuracy levels of the existing studies,
it’s diﬃcult to implement in the real environment. Therefore, the course creators are
still not able to design their courses according to the level of the students in the
current intake. Institution decision makers are not in a position to make accurate
decisions in order to increase the proﬁt of the organization.
This research is aimed at identifying the factors that aﬀect the quality of learning,
how the standard of the teaching learning process of the institute is improving and
how the proﬁt of the business can be improved through a better service.
4
Research Methodology
In order to overcome the problems mentioned above, a comprehensive study has
been conducted to identify the factors that aﬀect university students’ performance.
The research study was performed based on the steps of Knowledge Discovery in
Databases process.
Initially the business context must be understood by studying existing problems
in educational institutes which can be solved by applying data mining. This can be
done by reading similar research studies and by interviewing educators and students.
Then the data set extracted from a relevant domain can be understood by performing
descriptive statistics.
4.1
Understanding the Data
Before the actual analysis starts, the data set must be understood. It begins with data
preprocessing. This will be the most important task during the KDD process to derive
a quality output in the end of the KDD process. The dataset selected for this study
consists of 13 attributes, which are used to explain the sample of students registered
for technological degree programs as shown in Table 1.

A Data Mining Approach to Identify the Factors ...
191
Table 1
Description of the data set
Attribute
Description
Sex
Student’s gender (binary-Male: M, Female: F)
Age
Student’s age (numeric)
Fjob
Does the student’s father have a job (binary:
yes, no)
Failure
Does student have previous failure modules
(binary: yes, no)
ExtraPayment
Has the student done extra payment (binary:
yes, no)
MoreYears
Has the student stayed more than three years
(binary: yes, no)
S1
Student’s GPA of semester 1 (nuemeric: from
0–4)
S2
Student’s GPA of semester 2 (nuemeric: from
0–4)
S3
Student’s GPA of semester 3 (nuemeric: from
0–4)
S4
Student’s GPA of semester 4 (nuemeric: from
0–4)
S5
Student’s GPA of semester 5 (nuemeric: from
0–4)
S6
Student’s GPA of semester 6 (nuemeric: from
0–4)
IsPass
Has the student passed the degree (binary: yes,
no)
4.2
Data Preparation
Next, data preparation becomes an important step. When the data has been collected
from questionnaires or surveys, they should be recorded electronically and should
be transformed into a suitable format for the analysis in R software package. The
dataset described in Table 1 has 3794 instances and in the beginning the principal
component analysis (PCA) has been conducted to generate a new dataset which are
of linearly uncorrelated variables under the dimension reduction technique. All the
variables which occupy a lower cumulative proportion were ignored and others have
been selected for the analysis. All missing values, incomplete values were handled
using, median imputation (feature extraction and feature reduction algorithms) algo-
rithm. There were some records with more missing values for several variables. Case
deletion has been done and they were simply removed from the data set (Fig. 2).
There are 2307 male students and 1487 female students in the selected data set.
As a percentage, 60.8% male students and 39.2% female students. The histogram
which describes the behavior of our target variable; ﬁnal grade is shown in Fig. 3.

192
K. T. Sanvitha Kasthuriarachchi et al.
Fig. 2
Composition of male
and female students
Female - 1487
Male - 2307
Fig. 3
Histogram for the
students’ ﬁnal grade (two
level and ﬁve level)
4.3
Data Mining
The next approach is application of data mining algorithms on the preprocessed
sample data. R statistical software package would support data mining using var-
ious libraries and methods. The data set has inputted to classiﬁcation algorithms to
categorize them. The input data set consists of vector values of attributes with corre-
sponding classes. In the analysis, the data set has been divided into training datasets
and testing datasets. Training set is used to train the model and the model learns from
this. Testing data set is used to measure how much the model has learnt using the
train data.
Diﬀerent algorithms for classiﬁcation were applied in this research to get a better
result. They are;
(i) Naïve Bayes algorithm
(ii) Decision Tree algorithm
(iii) Random Forest algorithm.

A Data Mining Approach to Identify the Factors ...
193
Table 2
Results of tests
Attribute
Info gain
Gain ratio
Chi-squared
Sex
0.008258
0.012333
0.12746
Age
0.013725
0.021807
0.167125
Fjob
0
0
0
Failure
0.020783
0.160653
0.202960
ExtraPayment
0.020783
0.160653
0.202960
MoreYears
0.020783
0.160653
0.202960
S1
0.247752
0.244080
0.648855
S2
0.270783
0.268776
0.661043
S3
0.340661
0.321588
0.756178
S4
0.224633
0.238065
0.603340
S5
0.241414
0.249744
0.625158
S6
0.247361
0.219418
0.648092
Results of the various data mining algorithms must be evaluated to come up with
a better classiﬁcation model. For this evaluation of classiﬁcation accuracy, 5 fold
cross validation method has been used. The cross validation was repeated several
times (k-times) and each time one sub set was used as a test set. Under the cross
validation; prediction accuracy, Kappa statistics, Precision, Recall and F-measure
were recorded.
In the beginning of this study the impact of input attributes have been analyzed
in order to come up with a better prediction result. For the identiﬁcation of impor-
tance of each attributes, Info Gain/entropy, Gain Ratio and Chi-squared tests were
conducted using R software package. Table 2 illustrates the results of all these tests
and there average rank.
Filter methods include techniques for evaluation attributes values trusting on
heuristics based on the general data characteristics. The InfoGain which describes
the entropy of the attribute, GainRatio and Chi-Squared methods were applied for
feature selection. Gain Ratio represents an assessment of attribute value by mea-
suring the informative nature in relation to the class. Attributes with less than 0.01
assessment result had to be excluded from the dataset. According to Table 2, all of
the attributes in the dataset were included in the further analysis except the Fjob
(Fathers’ job) attribute.
Then the performance of the three algorithms were measured. The classiﬁcation
methods discussed in the above section NB, DT, RF were used and the results of
the tests are shown in following tables. Table 3 illustrates the performance of three
models which were evaluated against several criteria.
According to the above results shown in Table 3, it can be observed that all three
classiﬁcation algorithms produce relatively good results which are more similar to
each other. The highest result is obtained by Random Forest classiﬁcation. The clas-
siﬁcation method has derived that the age of the student, number of failure modules

194
K. T. Sanvitha Kasthuriarachchi et al.
Table 3
Performance of classiﬁcation methods
Criteria
Naïve Bayes
Decision tree (C5.0)
Random forest
Correctly classiﬁed
instances
1153
1220
1250
Incorrectly classiﬁed
instances
111
44
14
Prediction accuracy
(%)
92.17
97.1
98.9
Kappa statistic (%)
81.91
93.57
97.65
Precision (%)
89.403
94.85
95
Recall (%)
99.75%
98.71%
97
F-measure (%)
87.5
95.79
80
and performance of past semesters (S1, S2, S3, S4, S5 and S6) are the most important
factors which aﬀect the ﬁnal grade of the students.
4.4
Null Hypothesis
H0a: There exists no relationship between Age of the student with his/her ﬁnal grade.
H0b: There exists no relationship between number of previous failure modules of the
student with his/her ﬁnal grade.
H0c: There exists no relationship between year 1 semester performance of the student
with his/her ﬁnal grade.
H0d: There exists no relationship between year 2 semester performance of the student
with his/her ﬁnal grade.
H0e: There exists no relationship between year 3 semester performance of the student
with his/her ﬁnal grade.
4.5
Alternative Hypothesis
H1a: There exists a relationship between Age of the student with his/her ﬁnal grade.
H1b: There exists a relationship between number of previous failure modules of the
student with his/her ﬁnal grade.
H1c: There exists a relationship between year 1 semester performance of the student
with his/her ﬁnal grade.
H1d: There exists a relationship between year 2 semester performance of the student
with his/her ﬁnal grade.
H1e: There exists a relationship between year 3 semester performance of the student
with his/her ﬁnal grade.

A Data Mining Approach to Identify the Factors ...
195
Testing the signiﬁcance level of each attributes towards the predictor variable
have been done using T-Test.
P-value of each of these hypothesis tests were tested and as they indicated a lesser
value than 0.05, all the null hypothesis from H0a to H0e were rejected with a 95% of
a conﬁdence level. Further, it has shown that the age, number of failure modules and
performance of past semesters of the student has a negative correlation with the ﬁnal
grade. Hence, there is a signiﬁcant correlation between the variables in the data set
with the target attribute.
According to the results obtained in Random Forest classiﬁer; age of the student,
number of failure modules and performance of past semesters are the most inﬂuential
factors that can be used to predict whether the students will be able to successfully
complete the degree or not. When the age is low, there exists a high chance of passing
the degree and when the students complete the semesters without failures, they are
more likely to obtain the degree and if the grades of previous semesters are high, the
probability of top grades for the ﬁnal examination is relatively very high.
Therefore, the model derived by Random Forest has the potential to be used to
predict how students will perform in their tertiary education in the institute.
5
Conclusion
Educational Data Mining is a rich ﬁeld of research that has gained popularity in
recent years. This research study was carried out to determine the possible factors
which aﬀect the performance of the students in higher education system. There were
three classiﬁcation methods used in this analysis which had provided better results
of the prediction which varies between 92 and 98%. Among 12 attributes except
the target, age of the student, number of failure modules and performance of past
semesters were identiﬁed as the most correlated factors that predict the ﬁnal grade
of the students. It would be beneﬁcial for all stakeholders in education sector such
as educators, course coordinators, students and parents to identify these factors in
making educational decisions for a better future of students and the institute.
Further, the research results generated by the selected data mining algorithms
can be used to make predictions to investigate the impact of providing personalized
support, making predictions regarding students’ learning performances, visualiza-
tions of learning activities, developing theories and models of learning, analyzing
students’ behavioral patterns to explain their performances in learning with diﬀerent
strategies, tools or technologies.
However, some limitations are observed in this study. There were only 3794
instances in the dataset and it had only 12 attributes in the data set. This result might
be diﬀerent for another set of data with more tuples and more diﬀerent attributes.
There might be a possibility of generating more accurate results by another mining
algorithm except Random Forest.

196
K. T. Sanvitha Kasthuriarachchi et al.
6
Future Work
This research was conducted by gathering data from classroom teaching and learning
records. Therefore, the authors expect to continue this research by analyzing the
students’ data with other data mining algorithms and with diﬀerent data sets of more
records including online learning records as well.
Acknowledgements The authors would like to acknowledge the support provided by Sri Lanka
Institute of Information Technology by providing a valuable dataset of the institute to carry out the
research study.
References
Amershi, S., & Conati, C. (2009). Combining unsupervised and supervised classiﬁcation to build
user models for exploratory learning environments. Journal of Educational Data Mining, 1871.
Antunes, C. (2008). Acquiring background knowledge for intelligent tutoring systems. In Proceed-
ings in International Conference Educational Data Mining (p. 1827) Montreal, QC, Canada.
Chatti, M. A., et al. (2012). A reference model for learning analytics. International Journal of
Technology Enhanced Learning, 318–331.
Chen, C., Chen, M., & Li, Y. (2007). Mining key formative assessment rules based on learner
proﬁles for web-based learning systems. In Proceedings of IEEE International Conference
Advanced Learning Technology (p. 15). Niigata, Japan.
Clarke, J. A., Stoodley, I. D., & Nelson, K. J. (2013). Using a maturity model to move student
engagement practices beyond the generational approach.
Dekker, G., Mykola, P., & Jan, V. (2009). Predicting students drop out: A case study. Educational
Data Mining.
Gasevic, D., Siemens G., & Rose C. P. (2017). Guest editorial: special section on learning analytics.
IEEE Transactions on Learning Technologies 10.1, 35.
Gedeon, T. D., & Turner, H. S. (1993). Explaining student grades predicted by a neural network.
In International Conference in Neural Network (pp. 609–612).
Haddway, P., Thi, N., & Hien, T. N. (2007). A decision support system for evaluating international
student applications. In Proceedings of Frontiers Educational Conference (pp. 1–4).
Hazelkorn, E. (2007). The impact of league tables and ranking systems on higher education decision
making. Higher Education Management and Policy, 19(2), 1–24.
Jindal, R., & Borah, M. D. (2013). A survey on educational data mining and research trends. Inter-
national Journal of Database Management Systems, 5(3).
Kay, J., Maisonneuve, N., Yacef, K., & Zaiane, O. R. (2006). Mining patterns of events in students
teamwork data. In Proceedings Workshop Educational Data Mining (Vol. 18). Taiwan.
Kotsiantis, S. B., & Pintelas, P. E. (2005). Predicting students marks in Hellenic Open University.
In Proceedings of IEEE International Conference of Advanced Learning Technology (pp. 664–
668). Washington, DC.
Li, X., Luo, Q., & Yuan, J. (2007). Personalized recommendation service system in e-learning
using web intelligence. In Proceedings of 7th International Conference in Computer Science
(pp. 531–538). Beijing, China.
Myller, N., Suhonen, J., & Sutinen, E. (2002). Using data mining for improving web-based course
design. In Proceedings of International Conference in Computer Education (pp. 959–964).
Nebot, A., Castro, F., Vellido, A., & Mugica, F. (2006). Identiﬁcation of fuzzy models to predict
students performance in an e-learning environment. In Proceedings of International Conference
Web-Based Education (pp. 74–79).

A Data Mining Approach to Identify the Factors ...
197
Romero, C., Ventura, S., Hervs, C., & Gonzales, P. (2008). Data mining algorithms to classify
students. In Proceedings of International Conference in Educational Data Mining (pp. 8–17).
Romesburg, H. C. (2004). Cluster analysis for researchers. Melbourne, FL: Krieger.
Rus, V., Lintean, M., & Azevedo, R. (2009). Automatic detection of student mental models during
prior knowledge activation in MetaTutor. In Proceedings in International Conference Educa-
tional Data Mining (pp. 161–170). Cordoba, Spain.
Salomon, G. (2016). Its not just the tool but the educational rationale that counts. In Educational
technology and polycontextual bridging (pp. 149–161). Sense Publishers.
Shangping, D., & Ping, Z. (2008). A data mining algorithm in distance learning. In Proceedings of
International Conference in Computer Supported Cooperative Work in Design (pp. 1014–1017).
Shen, R., Han, P., Yang, F., Yang, Q., & Huang, J. (2003). Data mining and case-based reasoning
for distance learning. Journal of Distance Education Technology, 46, 58.
Sin, K., & Muthu, L. (2015). Application of big data in education data mining and learning analyt-
ics: A literature review. ICTACT Journal on Soft Computing, 5(4), 1035.
Tian, F., Wang, S., Zheng, C. & Zheng, Q. (2008). Research on e-learning personality group based
on fuzzy clustering analysis. In Proceedings International Conference Computer Supported
Cooperative Work Design (pp. 1035–1040). Xian, China.
Tsai, C. J., Tseng, S. S., & Lin, C. Y. (2001). A two-phase fuzzy mining and learning algorithm
for adaptive learning environment. In Proceedings International Conference Computer Science
(pp. 429–438). San Francisco.
Zhang, Y., et al. (2010). Using data mining to improve student retention in HE: a case study.

Evaluating the Acceptance of e-Learning
Systems via Subjective and Objective Data
Analysis
Imed Bouchrika, Nouzha Harrati, Zohra Mahfouf
and Noureddine Gasmallah
Abstract The adoption of e-learning technology by the academic community, has
been a long source of research from multiple disciplines including education, psy-
chology and computer science. As more and more academic institutions have opted
to use online technology for their course delivery and pedagogical activities, there
has been a surge of interest in evaluating the acceptance of the academic commu-
nity to adopt and accept the use of e-learning management systems. This is due to
the increasing concerns that despite the wide use and deployment of e-learning tech-
nologies, the intended impact on education is not achieved. We review the conducted
studies on the use of objective procedures for evaluating e-learning systems in tan-
dem with subjective data analysis. The evaluation process consists of understand-
ing further the factors related to the acceptance and adoption of online educational
systems by instructors and students in order to devise strategies for improving the
teaching and research quality.
Keywords
e-learning ⋅Usability evaluation ⋅Learning management system
Subjective evaluation ⋅Objective evaluation ⋅e-learning adoption
1
Introduction
Because of the ubiquitous use of computers and smart devices combined with the
availability and aﬀordability of internet connectivity in most places, information sys-
I. Bouchrika (✉) ⋅N. Harrati ⋅Z. Mahfouf ⋅N. Gasmallah
Faculty of Science & Technology, University of Souk Ahras, 41000 Souk Ahras, Algeria
e-mail: imed@imed.ws
N. Harrati
e-mail: n.harrati@univ-soukahras.dz
Z. Mahfouf
e-mail: z.mahfouf@univ-soukahras.dz
N. Gasmallah
e-mail: gasmallahedi@univ-soukahras.dz
© Springer International Publishing AG 2018
S. Caballé and J. Conesa (eds.), Software Data Engineering for Network eLearning
Environments, Lecture Notes on Data Engineering and Communications
Technologies 11, https://doi.org/10.1007/978-3-319-68318-8_10
199

200
I. Bouchrika et al.
tems have become an integral part of our daily life in such a modern society. The
uptake of online technology within the academic arena has greatly reshaped and
transformed the way we teach, work and conduct research. Considerable amounts
of funding and eﬀorts are being devoted to deploy and modernize information sys-
tems in order to improve individual and institutional performance for course deliv-
ery. This is eased with the birth of a new generation of undergraduate students being
considered as the digital natives who have grown up for their whole lives surrounded
by the use of computers and online technologies (Joo and Choi 2015). Although,
there are advocates within the university community who still prefer traditional
teaching methods which include face-to-face communication, unprecedented eﬀorts
are set to promote and embrace the use of new technology and e-learning for teach-
ing, communication and research. Numerous recent studies (McGill et al. 2014) have
stressed that educational innovations can wither and be subverted if technological
initiatives are not maintained and adopted by the educational community. In fact,
academics play a pivotal role for the successful uptake and acceptance of digital
infrastructure via enriching the e-learning platforms with pedagogical materials to
supplement their teaching activities in addition to publishing their e-textbooks and
research contributions within the academic online portals.
The use of e-learning in academic and corporate institutions has gained popular-
ity mainly due to the perceived advantages of ﬂexibility around ﬁtting the learner’s
time requirements and overcoming the issue around the geographical restrictions.
In fact, the geographical gap is virtually bridged with the deployment of tools that
make people collaborate and interact together remotely with the feeling that they are
inside the same room. The time aspect is one of the issues that instructors and learn-
ers both have to deal with in learning or tutoring sessions. In the case of traditional
face-to-face teaching, the arrangement of time can be restrictive for the attendance to
a certain group of students who have the ability and availability to attend at a speciﬁc
time. Along with the timing restrictions, traveling and being present at the location
where the learning would take place can be a major obstacle. On the other hand, e-
learning oﬀers the beneﬁts to facilitate the learning process without having to worry
about when or where every learner can be available and present to attend the course.
In other words, e-learning provides the students with the capability to accommodate
learning and training around their busy lifestyles, granting eﬀectively the opportunity
even to the busiest person to pursue further their career to earn new qualiﬁcations. In
a study published by Welsh et al. (2003), the authors reported that organizations can
accomplish numerous beneﬁts from implementing e-learning programs, including
consistency in training, reduced cycle time and cost, better convenience for learn-
ers and improved tracking capabilities. Zhang and Nunamaker (2003) suggested that
eﬀective and eﬃcient computer-based training methods are in great demand by the
industry to ensure that employees and partners are equipped with the most advanced
skills. In the same way, academics and practitioners alike consider e-learning soft-
ware systems to be a valuable platform for knowledge sharing and transfer tool
in the educational world. Garrison et al. (2011) pointed out that apart from rea-
sons of knowledge transfer and education, academic institutions pursue the deploy-
ment of e-learning systems as a means to boost their revenues and retain market share
of students in addition to improve national recognition.

Evaluating the Acceptance of e-Learning Systems ...
201
As more and more academic institutions have opted to use online technology for
their course delivery and pedagogical activities, there has been an increasing interest
in evaluating the acceptance of the academic community to adopt learning manage-
ment systems. In this book chapter, we review the conducted studies on the use of
objective procedures for evaluating e-learning systems in tandem with subjective
data analysis. Regardless the widespread use of e-learning systems and the substan-
tial investments in purchasing, developing and maintaining learning management
systems, there is no consensus yet on devising a standard framework or taxonomy
for evaluating the quality and eﬀectiveness of e-learning systems. The evaluation
consists of understanding further the factors related to the acceptance and adoption
of online information systems by higher education staﬀin order to devise strategies
and to enhance and improve the teaching and research quality. Further, research on
e-learning systems in addition to the linkage between information systems and staﬀ
performance have attracted unprecedented interest in order to better apprehend how
eﬀective and usable e-learning systems in terms of principles related to human com-
puter interaction (Navimipour and Zareie 2015; Bringula 2013; Escobar-Rodriguez
and Monge-Lozano 2012) and human behavior (Roca and Gagné 2008; Liaw et al.
2007). Numerous research studies concern the analytical quantiﬁcation of the various
factors that determine and shape the acceptance of academic online systems (Albert
and Tullis 2013; Hornbæk 2006) in addition to assessing the behavioral aspect of
users including students and academic staﬀ.
2
e-Learning Management Systems
The main components which contribute to the functioning of an e-learning process
can be identiﬁed as: technological infrastructure, educational content, participants
and e-learning management system. The technological infrastructure refers to the
communication medium and hardware platform hosting the e-learning operations.
Educational content used to be delivered via the postal services using a blend of
traditional computer-based media such as CD-ROM where users can learn remotely
in total asynchronous mode. Nowadays, transmission of teaching materials is done
via the internet where the learners can get fresh content with instant feedback and
can even collaborate and communicate with other peers or instructors. Technolog-
ical tools for supporting the e-learning process involve the use of some or all of
the following devices: desktop and laptop computers, interactive whiteboards, video
cameras, mobile and wireless tools, including mobile phones.
The most vital component for the e-learning process is the e-learning software
platform which is usually named as the Learning Management System (LMS) or
also known as Virtual Learning Environment (VLE). In fact, there is no consensus
on the precise deﬁnition of an LMS as e-learning systems are continuously evolving
to accommodate new features and adopt emerging concepts. The LMS is a software
system developed for the purpose of managing online courses including administra-
tion, documentation, reporting and delivery of educational and training programs.
The e-learning software allows the instructor or institution administrator to manage

202
I. Bouchrika et al.
every aspect of courses from the enrollment of students, delivering educational mate-
rials in addition to the assessments part via digital delivery of assignments and exam
preparations. A learning management system can assist academic or corporate insti-
tutions to protect and safeguard their teaching and training materials as they have
invested substantially to create them as they cannot risk lost revenues when making
such valuable resources publicly available. Further, one of the merit of deploying
learning management software is to provide a “walled garden” where learners can
develop a sense of community away from abusive and disruptive internet users under
the guidance of their instructors (Mott 2010).
Most learning management systems are developed as web applications using var-
ious platforms including PHP, .NET and Java integrated with a classical relational
database engine for storing data such as PostgreSQL, SQL server and MySQL. There
are a number of features and functionalities that a learning management system
should minimally oﬀer for achieving the ideal e-learning experience. Most systems
are likely to include most of the following features: Course Content Delivery, Stu-
dent Registration and Administration, Event Scheduling, Tracking, Curriculum and
Certiﬁcation Management, Assignment and Assessment, Reporting and Courseware
Authoring. Further, the LMS provides a platform for interaction between students
and lecturers via the use of chat rooms or discussion boards or video conferenc-
ing. There is a plethora of diﬀerent e-learning systems in the market either com-
ing as freely available as open source or commercial products. We have classiﬁed
learning management systems into two main categories namely: (i) On-premise and
(ii) Cloud-Based SaaS. The classiﬁcation is based on the installation paradigm as
explained in this section. Further, we review some of the popular learning manage-
ment systems having the dominant market share within the e-learning sector. The
list of the reviewed software systems are summarized in Table 1.
2.1
On-premise Software
Learning management systems which run on-premise are usually installed and self-
managed either locally or even installed remotely on a traditional rented dedicated
hosting space. The main beneﬁt of using on-premise software is to have complete
control of ownership to the software license and ability to modify or upgrade the
system. Further, the system can be customized or extended to suit the requirements
that the institutions believe would be relevant to their teaching paradigm and cur-
riculum. A prime reason for schools and universities to use in-house solutions is the
privacy concern as academic institutions can be legally bound to keep their student
data private and therefore in-house hosting for their learning management system
is a solution to comply with legislative requirements. The main drawback of self-
managed learning management system is the maintenance which involves upgrad-
ing the software and hardware infrastructure from time to time in addition to taking
regular backups and conducting data recovery in case of a hardware failure. For the
cost aspect, on-premise software can cost substantially more and requires dedicated
and well-trained staﬀto maintain the platform.

Evaluating the Acceptance of e-Learning Systems ...
203
Table 1
List of learning management systems
On-premise LMS
Saas cloud-based LMS
Moodle
Caroline
Blackboard
Sakai
edX
Google C.
TalentLMS
DoceboLMS
Free
✓
✓
✓
✓
✓
e-Assessment
✓
✓
✓
✓
✓
✓
✓
✓
Personalized Lear.
✓
✓
✓
✓
APIs
✓
✓
✓
✓
✓
✓
Gamiﬁcation
✓
✓
✓
M-Learning
✓
✓
✓
✓
✓
✓
✓
✓
Forums
✓
✓
✓
✓
✓
✓
✓
✓
Cloud storage
✓
✓
✓
✓
Localization
✓
✓
✓
✓
✓
✓
✓
✓
Collaboration
✓
✓
✓
✓
✓
✓
✓
✓
SCORM/xAPI
✓
✓
✓
✓
✓
e-Commerce
✓
✓
✓

204
I. Bouchrika et al.
Moodle
is an acronym for Modular Object-Oriented Dynamic Learning Envi-
ronment developed by Martin Dougiamas in 2002 using the PHP programming
language. The software is an online Learning Management system that can be
downloaded by academic institutions for free to enable their lecturers and instruc-
tors to create webpages ﬁlled with dynamic courses that extend learning anytime
and anywhere. Although Moodle provides an Application Programming Interface
(API) access, installing and integrating the platform with existing infrastructures
require high level of technical expertise. Developed on pedagogical principles,
Moodle is used for blended learning, distance education, ﬂipped classroom and
other e-learning projects in schools, universities, workplaces and other sectors.
The recent version of Moodle (3.3.2) supports responsive design giving the users
the ability to create mobile-friendly online courses and integrate third party add-
ons. In terms of usage, Moodle is the second largest provider with 23% market
share, following Blackboard (41%) whilst having the most number of users esti-
mated to be over 70 million registered students. Although, the software enjoys
richer functionalities and robustness, the main drawback for using Moodle is the
perceived complexity for new users (Harrati et al. 2016).
Claroline
is a collaborative online learning management system which is provided
to download and install as an open source platform released freely under the
GPL open source license. Claroline is compatible with GNU/Linux, Mac OS and
Microsoft Windows. It is based on PHP and MySQL as the widely used relational
database management system. The software oﬀers the possibility for many insti-
tutions to create and administrate collaborative online learning spaces featured
with many tools including blogs, wikis and forums. Claroline is being deployed
in more than 100 countries and translated to 35 languages. The use of Claroline is
intuitive, easy and does not require particular skills. Although Caroline provides
support for integration with existing infrastructure using LDAP, the platform does
not have an API for ﬂexible integration.
Blackboard Learn
which is commonly known as Blackboard, is a web-based
content management system created in 1997 by faculty members at Cornell Uni-
versity as a course management system for education. It is one of the most pop-
ular and successful commercial e-learning systems. Blackboard helps creating
a virtual place or classroom where the interaction between students and their
instructors is achieved through the use of discussion forums, email, chat rooms
and other functionalities. Blackboard Learn supports seamless integration with
cloud-based providers for synchronizing and downloading ﬁles including Drop-
box and OneDrive. Further, the platform provides personalized learning for stu-
dents through the use of proﬁles. The LMS can be extended and customized
according to various needs of the institutions. In the same way to Moodle, Black-
board created an API for the learning management system for the ease of integra-
tion with other software and database systems.
Sakai
is a service-oriented Java-based open source learning management system
founded in 2004 by the universities of Michigan, Indiana, Stanford and the Mas-
sachusetts Institute of Technology (MIT) with the purpose to develop a new learn-
ing management system as scalable, reliable, interoperable and extensible. The
project was funded by a grant from the Mellon Foundation. Sakai is deployed
at over 300 academic institutions for oﬀering online education. Although the

Evaluating the Acceptance of e-Learning Systems ...
205
platform features a rich list of functionalities for collaboration, teaching and com-
munication, Sakai does not provide an API for developers to integrate the platform
with their existing systems.
edX
is an open-source and free learning management system oﬀered by edX.org.
It is the same framework that universities such as MIT and Harvard utilize to oﬀer
online education to over 100,000 students. It was released as open source in March
2013, and the goal was to act as the WordPress for Massive Open Online Course
(MOOC) platforms, allowing developers and users to integrate plug-ins to expand
the core functionality of the system. edX has a fast, modern feel, with the ability
to accommodate large enrollments. Although it is an open source, investment will
need to be made in both installation and maintenance of the system. An API is
provided for institutions to easily integrate edX with their existing systems mainly
to enroll and manage students. The platform is made responsive to improve the
accessibility aspect for mobile platforms.
2.2
Cloud-Based SaaS
e-Learning systems can be deployed easily as Software as a service (SaaS). Aca-
demic or corporate institutions can create a working e-learning management sys-
tem for their students without the requirement to download or install any extra
software. Usually, there are commercial providers who provide an online interface
for customers to create and manage their learning management systems which can
be created as in instance on a centralized hosting platform commonly known as a
cloud-based platform. The cloud in computing is deﬁned as “a large pool of eas-
ily usable and accessible virtualized resources. These resources can be dynamically
re-conﬁgured to adjust to a variable load, allowing also for an optimum resource uti-
lization” (Vaquero et al. 2008). The main reason for people to migrate towards using
cloud-based solution is the scalability concern in order to account for the increas-
ing numbers of users, teaching materials and computing resources. This is because
cloud-based solutions can handle sudden and increase spikes of usage via load bal-
ancing or distribution of requests and data across multiple servers.
Outsourcing the software as a service within a cloud-based environment can cost
less whilst the technical aspect is taken care of by the service provider including
support and assistance. Barlow et al. (2007) reported that Arizona State Univer-
sity has made a saving of $450,000 per year when migrating to use cloud-based
email services provided by Google. Although, there are risks and drawback to rely
heavily on cloud-based solutions as there is no guarantee that the service provider
will last in business forever in addition to future pricing plans, policy changes or
software upgrades can impact severely academic institutions who have relinquished
total control to the cloud provider for the data and the e-learning software. In fact,
a small change to the interface of the learning management system could make all
the documentation or instructional videos provided by the institutions totally obso-
lete and misleading. There are a number of learning management systems which are

206
I. Bouchrika et al.
developed purely for a cloud environment including the list reviewed in this section
followingly. Interestingly, there are companies emerging recently to oﬀer the instal-
lation of on-premise software within their cloud farm for a monthly cost.
Google Classroom
is a learning management system developed by Google as part
of the G Suite for Education in 2014. The platform is oﬀered at no cost to aca-
demic institutions along with other cloud-based applications including word pro-
cessor, email, calender and unlimited access to Google Drive storage services.
Google Classroom aims to create a paperless learning experience where teachers
can create online classes, set assignments and monitor their students. The platform
has a mobile native applications for Android and iOS smartphones to support m-
learning. For developers, Google released an API to interact programmatically
with the cloud-based learning management system.
DoceboLMS
is a fully featured cloud-based SaaS e-learning management system
used mostly for companies and corporation to train their staﬀor sell courses
online. The platform was initially developed as on-premise software and later was
moved to operate as a cloud-hosted platform. DoceboLMS supports major fea-
tures including e-assessment, wikis, localization, e-commerce and certiﬁcation.
Gamiﬁcation is supported within DoceboLMS to help increase the users’ engage-
ments with the online system through the deployment of gaming mechanics in a
non-gaming context. The platform is available for smart phones. For integration
and enrollment of users, there are APIs available that can be invoked to interact
with the cloud-based platform.
TalentLMS
is Service as a Software e-Learning platform created with a num-
ber of functionalities including e-assessment, forums, certiﬁcation, gamiﬁcation
and online authoring tools. Recently, The cloud LMS is reported to have a bet-
ter usability rate of its user interface compared to other complex systems. There
is the possibility for users to integrate other external cloud-based services such
as Dropbox and Gmail. For mobile learning, there are native applications imple-
mented for iOS and Android phones with the same features as the desktop online
applications. For integration with existing infrastructures, TalentLMS provides
extensive REST API functions to import and manage users as well as content and
course creation.
3
Adoption of e-Learning Systems
The decision of whether a user will accept and adopt to use a speciﬁc innova-
tion or technological product along with the time frame involved to decide to use,
have attracted considerable research interest across multiple research communities
to explore why there are people who choose to accept a technology whilst there are
other individuals who resist (Straub 2009; Harrati et al. 2016). The adoption goes
beyond the simple choice to accept an innovation to the extent where the new product
is integrated and deployed into the appropriate context of use (Straub 2009). There

Evaluating the Acceptance of e-Learning Systems ...
207
Fig. 1
Rogers’ diﬀusion for the innovation adoption (Rogers Everett 1995)
are two common terms when it comes to the acceptance of technology; adoption
and diﬀusion. Rogers et al. (2010) discussed thoroughly the adoption and diﬀusion
of technological innovations and new products by a social system. Straub (2009)
argued that the adoption is a micro-perspective on change to accept or reject an
innovation, focusing on the smaller pieces that make up the whole. In contrast, the
diﬀusion theory explains how an innovation spreads among larger population consid-
ering factors including time and social inﬂuence to describe how individuals adopt,
reject or adapts to a technological product. Rogers (2010) visualized the process of
adoption and diﬀusion over time as a normal distribution as shown in Fig. 3. The
adoption model shows that the ﬁrst group of individuals to adopt an innovation are
called “innovators” who require a shorter adoption period. Followed by the “early
adopters” who are described as opinion leaders and willing to try new ideas with
caution. The next group is the “early majority” who are thoughtful and careful peo-
ple to accepting change. The other group of people is called “late majority” being
described as skeptic individuals who adopt a technology only when the majority of
people use it. The last group to adopt innovation are called “laggards” who accept the
technology only when it becomes mainstream or tradition. Rogers (2010) explained
that the diﬀusion of innovation model implies that the patterns of technological adop-
tion within a network of individuals, are determined and steered through a process
of communication and social inﬂuence such that later adopters are informed and
persuaded of the availability and usefulness of new products by the early adopters
(Fig. 1).
The idea for distance education has been around for more than a century whilst
e-learning has started to evolve during the last two decades having a prominent
impact on the educational and training paradigm for academic institutions, corpo-
rations and public administrations. For the topic of adopting e-learning technology
by the academic community, it has been a long source of academic research from
multiple disciplines including education, psychology and computer science. This is
motivated by the fact that the process for introducing e-learning systems and their
adoption is bound to have a slow and complex trend (Persico et al. 2014). Numerous
recent studies (McGill et al. 2014) have stressed that educational innovations can
wither and be subverted if technological initiatives are not maintained and adopted

208
I. Bouchrika et al.
by the educational community. In fact, academics and teachers play a pivotal role
for the successful adoption and diﬀusion of e-learning innovations. There is only
a limited number of research studies on the acceptance and adoption of academics
for the uptake of e-learning systems compared to the considerably body of research
publications focusing on students (Hrtoňová et al. 2015; Šumak et al. 2011). Many
theoretical models have been proposed to study and evaluate the adoption process
whilst most of them explained that the adoption and acceptance process is driven by
the following main constructs; ease of use, usefulness and social inﬂuence.
3.1
Ease of Use and Usability
Positive user experience emerges as an important pillar for the adoption of edu-
cational learning systems. This is mainly because the availability of technological
infrastructures and systems is not adequate to enforce the uptake of new educa-
tional approaches from the academic community (Persico et al. 2014; Phillips et al.
2012; Laurillard et al. 2009). Considerable criticism regarding the quality of existing
e-learning systems are being cited by a number of studies (Chua and Dyson 2004) in
addition to further issues including low performance and poor usability. The usabil-
ity nature of educational software systems is deﬁned as the extent to which a product
can be easily used by speciﬁed users to achieve certain goals with eﬀectiveness,
eﬃciency and satisfaction (Mayhew 1999). The usability is a key characteristic to
achieve the acceptance and adoption of technological product by academic staﬀ
regardless of their background, experience or orientation. The satisfaction part is
related to how the users believe or feel positively that the system meets their require-
ments. Meanwhile, other researchers have deﬁned satisfaction as the gap between
the expected gain and the actual gain when using the system (Tsai et al. 2007). Davis
(1989) explained the ease of use as the perception of users that the system will be
free from eﬀort arguing that the ease of use has a direct impact for the intention to
adopt the innovation.
There is an emerging body of literature on relating the usability aspect of infor-
mation systems as important factor to inﬂuence the human behavior to accept new
technological products. In practice, the usability aspect of software products is
marginalized during the classical stages of software development life-cycles push-
ing more eﬀorts and resources into the software back-end to address the functional
requirements (Burton-Jones and Grange 2012). In fact, regardless of how software
are neatly coded or sophisticated, recent studies of software sales reports that soft-
ware failures are due to usability reasons where simply the user does not know how
to use the purchased product (Cassino et al. 2015). Software systems are valued
on the basis of its graphical interface and the related power of communication and
expression for the implemented functionalities. It is no doubt that usability is now
recognized as an important software quality attribute, earning its place among more
traditional attributes such as performance, robustness, content and security (Hen-
riksson et al. 2007; Ismailova 2017). Moreover, research focus has shifted recently

Evaluating the Acceptance of e-Learning Systems ...
209
from the study of use to exploring ways of eﬀective and ease of use for information
systems to improve the adoption levels of technological products (Burton-Jones and
Grange 2012).
3.2
Usefulness and Utility
e-Learning management systems are devised to supplement traditional teaching
methods and improve the quality of knowledge and skill retentions. Academic insti-
tutions and corporations invested substantial amounts of funding to deploy learning
management systems to assist their students and staﬀperform their desired tasks
within an academic or training context. Davis (1989) deﬁned the perceived useful-
ness as the extent to which an individual believes that using a particular technological
product would improve their job performance. Usefulness is also deﬁned by Phillips
et al. (1994) as as “the prospective adopter’s subjective probability that applying the
new technology from foreign sources will be beneﬁcial to his personal and/or the
adopting company’s well being”. The authors explained that the usefulness or util-
ity comprises of two dimensions including the perceived utility for the organization
in addition to the perceived usefulness on the individual. For the academic com-
munity, the usefulness of educational systems is perceived from two perspectives
including assisting academic staﬀand students to acquire more knowledge, decrease
the time to perform a job and more eﬃciency. From another perspective, academic
institutions and corporation perceive the utility of learning management systems as
a way to improve its academic ranking, reputation as well as cut operational costs.
Many studies have stressed that the perceived usefulness is a strong factor that shapes
the user adoption to technological products (Davis et al. 1989; Phillips et al. 1994;
Venkatesh and Morris 2000; Venkatesh and Davis 2000).
3.3
Social Inﬂuence
Consistently with the theory of diﬀusion for the innovation adoption (Rogers 2010),
the social system has a strong inﬂuence to bring late adopters to accept an innovation.
The social inﬂuence refers to the beliefs for an individual of whether other users want
them to perform the desired task using the technological tool (McGillandKlobas
2009). Fishbein et al. (1975) explained the social norm as the degree to which a per-
son believes that other important people to him/her would want him/her to perform
this particular behavior i.e. make use of the technology to perform a particular task.
The list of related people can include both peers and superiors whose inﬂuence is
reported to be a strong factor for the social norm (Taylor and Todd 1995; Venkatesh
and Morris 2000). Previous studies aﬃrmed the existence of a direct inﬂuence of
social factors on the successful uptake of technological systems (Hsu and Lin 2008).
Within the academic context, the adoption of academics and students to use online

210
I. Bouchrika et al.
technology can be inﬂuenced by administrative staﬀ(head of the department, dean of
the faculty...), work colleagues and even students who prefer to access pedagogical
materials from home.
4
Acceptance Evaluation Methods
Evaluation is used to refer to the process of comparing or measuring a unit, course,
program or other elements of e-learning against some set of performance or
outcome criteria. Comprehensive evaluation spans to measures of satisfaction, per-
ception of learning, costing and cost beneﬁts, and other criteria for assessing the suc-
cess as deﬁned by the relevant stakeholders and participants. Eﬀective evaluation of
e-learning process requires a close examination of the instructional design incorpo-
rated during the course. Evaluation of e-learning applications in terms of user expe-
rience, satisfaction and acceptance has received recently considerable attention from
the research community in order to assess and quantify the satisfaction and eﬀective-
ness level for academic users. This is due to the increasing concern that despite the
wide use and deployment of e-learning technologies, the intended impact on edu-
cation is not achieved (Phillips et al. 2012; Asarbakhsh and Sandars 2013). In spite
of the widespread use of e-learning systems and the substantial investments in pur-
chasing, developing and maintaining learning management systems, there is no con-
sensus yet on devising a standard framework or taxonomy for evaluating the quality
and eﬀectiveness of e-learning systems. The dearth of conventional e-learning sys-
tem quality models is in stark contrast compared to the considerable body of work
on software quality assurance. In this section, two major categories are proposed to
survey the diﬀerent approaches for the adoption of e-learning systems based on the
evaluation paradigms which are subjective and objective.
Garrison et al. (2011) listed four types of proactive evaluation starting with the
determination of the strategic intent of the e-learning program. Being able to clearly
determine the reasons why the particular pedagogical program has been devised for
e-learning is important to assess its eﬀectiveness. The second type of proactive eval-
uation is to look closely at the educational content of the courses and examine the
cohesion and consistency facet in addition to the ease of access of modiﬁcation. The
third element of evaluation focuses on an examination of the interface design for the
learning management system. An eﬀective graphical interface is mastered by users
with ease and gives the possibility to present the educational content in a variety of
formats including graphics, video, and other advanced interactive and dynamic for-
mats. The design of the interface should be based on a familiar metaphor that will
help the users navigate among the diﬀerent components of the course. The graphical
interface should be customizable by both the students and the educators to increase
their comfort and the readability of the educational content. The fourth form of eval-
uation is about to assess the amount of interactivity supported by the course and the
learning management system. Garrison (2011) concluded that the ﬁnal evaluation

Evaluating the Acceptance of e-Learning Systems ...
211
process revolves around the quality, quantity and thoroughness of the assessment of
student learning and engagement for using the e-learning system.
4.1
Subjective Methods
Measurements obtained from subjective evaluation are derived from expressions,
feedback and opinions of users about how they perceive the system or their inter-
action with the system. Subjective evaluation methods can be qualitative oriented
where assessment is based on interviews, user comments or open-format question-
naire responses. Alternatively, measurements can be obtained quantitatively from
users using mostly closed-format responses. In the arena of technology adoption, the
majority of evaluation methods are based on creating an instrument to include ques-
tions set with responses on a Likert scale where respondents specify their answers as
the level of agreement or disagreement in a symmetric fashion. Questionnaires are
usually made to conform to a speciﬁc model composed of many inter-related con-
structs. There is a number of models and theories in the literature for understanding,
predicting, and assessing the interaction process with its involved parts including
personal factors, behavior, and the environment.
In order to assess the user acceptance of technological products, one of the most
well established models is the Technology Acceptance Model (TAM), which was
proposed by Davis et al. (1989). The TAM is tailored to include questions to explore
two aspects of the user satisfaction which are: perceived ease-of-use and perceived
usefulness. The ease of use refers to how users believe that adopting a particular
technological product would require no eﬀort and hassle to use it (Davis et al. 1989).
The perceived usefulness concerns the degree to which a user believes that using
a particular software system would improve their job performance. The Technol-
ogy Acceptance Model has been used in various studies to assess the factors aﬀect-
ing individual’s to the use of technology (Venkatesh and Davis 2000). The model
assess the acceptance of the system in terms of perceived usefulness and ease of
use against actual usage behavior as shown in Fig. 2. For research studies related to
assess the adoption aspect of the Moodle e-learning platform, Persico et al. (2014)
employed the Technology Acceptance Model to investigate the willingness of uni-
versity users for the adoption of e-learning systems. Evaluation is based on three
dimensions including usefulness, ease of use and eﬀectiveness. Escobar-Rodriguez
and Monge-Lozano (2012) analyzed how university students use the Moodle plat-
form in order to determine and understand the factors which might inﬂuence their
intention to use the platform.
Due to the limitation of the Technology Acceptance Model speciﬁcally for
addressing the technology as a whole and its lack of task focus, Goodhue and Thomp-
son (1995) introduced the Technology to Performance Chain (TPC) model to account
for such drawback via combining both the utilization and Technology Task Fit (TTF).
The TTF is deﬁned by (Goodhue and Thompson 1995) as the degree to which a
technology is utilized to assist a user to perform their tasks. For the Technology

212
I. Bouchrika et al.
Fig. 2
Technology acceptance model (Davis et al. 1989)
Task Fit, the primary objective is the ﬁtness between the task requirements and the
characteristics of the technology which both have a direct impact on the TTF. The
performance and utilization are in turn inﬂuenced by the TTF for performing a spe-
ciﬁc task using a particular technology. In spite of the fact that individuals perceive
technology as an innovative advanced solution for their well-being, users will not
uptake technological products if they think they are unsuitable to perform their tasks
or unable to improve their work performance. In other words, the TTF argues that
technological systems need to be willingly accepted by individuals as well as ﬁt well
with the tasks and users to prove its eﬀectiveness and improved performance. Previ-
ous empirical studies have reported that combining the TTF and utilization models
give better insight about the impact of technology on user performance better than
the TAM alone (Dishaw and Strong 1999). There are other studies which proposed
other variations via combining the TAM with the TTF including the work of Dishaw
and Strong (1999). For the literature related to the use of TPC model in the aca-
demic arena, a few research studies have explored the interrelationship of techno-
logical products, academic needs, performance and TTF. McGill employed the TPC
model for a number of educational case studies. In McGill and Hobbs (2008), the
ﬁt for using virtual learning environment is investigated for both teachers and stu-
dents. Further, learning management systems are evaluated in terms of the ﬁt degree
for tasks performed by students (McGillandKlobas 2009) in addition to pedagogical
tasks conducted by academic instructors (McGill et al. 2011). In a diﬀerent study,
Raven et al. (2010) used the TTF model to explore the ﬁt for using digital video tools
for giving presentation inside the classroom. The authors reported that signiﬁcant ﬁt
between improving oral presentation skills and using video tools. Further, D’Ambra
et al. (2013) applied the TTF model to assess the adoption of e-books by univer-
sity students. Recently, Yi et al. (2016) considered a reduced model from the TPC
to investigate the perceived performance for students to use their smart phones for
accessing educational content within the academic context (Fig. 3).
There are other related models and theories such as the System Usability Scale
(SUS) which was proposed mainly for the evaluation of web application for two
aspects; the learnability and usability. The SUS is a well-researched and widely
used questionnaire for assessing the usability of mostly web applications. The Sys-
tem Usability Scale (SUS) (Brooke 1996) is one of the most popular methods in

Evaluating the Acceptance of e-Learning Systems ...
213
Fig. 3
The technology to performance chain (TPC) model (Goodhue and Thompson 1995)
the literature which is devised mainly to evaluate the usability for web applications.
Its popularity is gained among the HCI community mainly due to its desirable psy-
chometric metrics including high reliability and validity (Sauro and Lewis 2009).
The SUS questionnaire is composed of ten questions with a mix of positive and
negative items. For each question, the respondent rates the magnitude of their agree-
ment using a 5-point Likert scale with statements going from strongly disagree to
strongly agree. The SUS scores ranges between 0 and 100 in 2.5-point increments
where higher values reﬂect higher satisfaction from the user. Only a few studies in the
literature have used SUS to evaluate the perceived usability of e-learning manage-
ment systems (Orfanou et al. 2015). The ﬁrst study of using the SUS for e-learning
system was conducted by Renaut et al. (2006) to inspect usability problems for the
SPIRAL platform. The researchers employed the SUS scale as a post-assessment of
the usability reporting a score of 72% of the participating university lecturers who
described the platform as positively easy to use. In Simões and de Moraes (2012),
Simoes examined the usability of the Moodle e-learning platform using three diﬀer-
ent evaluation methods including the SUS questionnaire to assess user’s satisfaction
for a sample size of 59 students. The authors concluded that the SUS is an eﬀective
tool for exploring the usability aspect without reporting the obtained SUS score.
Marco et al. (2013) proposed a way of remote collaboration in real time within the

214
I. Bouchrika et al.
platform Moodle through the use of Drag and Share. The collaborative tool enables
sharing and synchronization of ﬁles. The eﬃciency of users was quantiﬁed using the
time taken for task completion meanwhile user satisfaction was assessed using the
SUS questionnaire with a reported score of 89.5%.
4.2
Objective Methods
Although the majority of studies are purely based on subjective data analysis, Ivory
et al. (2001) argued that automating the evaluation process for software systems
in terms of acceptance and usability would help to increase the coverage of test-
ing as well as reduce signiﬁcantly the costs and time for the evaluation process.
Objective methods are based on the quantiﬁcation of variables by instrumentation
as opposed to using by subjective human assessment. This is motivated by the fact
that the process for introducing e-learning systems is bound to have a slow and com-
plex trend (Persico et al. 2014) that needs to be understood and evaluated beyond
the use of just summative and automated ways. Primarily, it is not surprising that a
number of empirical studies have compared both self-reported subjective and objec-
tive measures for using an information system concluding that self-reported data are
observed to be less accurate than objective measurements (Szajna 1996; Pentland
1989). Objective methods can range from analyzing completion tasks, interaction
logs, usage frequency and even aﬀective and medical data. Interestingly, there is a
recent trend of using medical machines for assessing the user satisfaction level for
using information systems. Dimoka et al. (2012) pointed out to the potentials of
employing brain imaging and psychophysiological tools such as skin conductance
response, eye tracking and facial Electromyography (Eckhardt et al. 2012). Liapis
(2015) conducted research experiments to recognize stress through analysing skin
conductance signals. This was carried out as part of an evaluation of user emotional
experience in order to identify stressful tasks in human-computer interaction.
Several automated evaluation methods are conceived for auto discovery of usabil-
ity faults at the same time alleviating the drawbacks in terms of reducing costs and
time through liberating usability experts from conducting repetitive tasks manually.
Further the coverage of tested features can be remarkably increased through the use
of automated procedures (Quade et al. 2013). Furthermore, because of the immense
volume of data acquired from usability evaluation, the total or partial use of auto-
mated methods can be very beneﬁcial during the development of web applications
(de Santana and Baranauskas 2015; Cassino et al. 2015). However, the majority of
the surveyed research studies are purely based on manual or statistical analysis of
recorded usage data for the participants. Methods for usability evaluation are con-
ventionally grouped into two main categories; the ﬁrst class is based on analyzing
the graphical interface through reading the source code of the website to examine
the content and structure of the application. Cassino and Tucci (2011) assessed the
source code to infer the design model of the interface and the interaction styles
implemented on every page of the website to generate a quantitative report of the

Evaluating the Acceptance of e-Learning Systems ...
215
evaluation based on heuristic factors. Meanwhile, other methods rely on examin-
ing the usage data i.e. logs. The user logs used for usability evaluation are captured
at either the server-side or the client-side. Many studies advocate that logging tech-
niques are proven to be more reliable and eﬃcient in terms of providing useful usabil-
ity insights for the evaluators (de Santana and Baranauskas 2015).
Paganelli and Paternò (2002) developed a desktop-based application for record-
ing and analysing interaction logs for website systems based on a predeﬁned task
model. The activities to be performed on a website is speciﬁed using the notations
for the ConcurTaskTrees environment (Paternò et al. 2012) which provides a graph-
ical representation for the hierarchical logical structure of the task model. Tiedtke
et al. (2002) described a framework implemented in Java and XML for automated
usability evaluation of interactive websites combining diﬀerent techniques for data-
gathering and analysis. The system uses a task-based approach and incorporates
usability issues. Atterer and Schmidt (2007) presented an implementation of a sys-
tem called UsaProxy which is an application that provides usage tracking function-
ality using an HTTP proxy approach. Recently, de Vasconcelos and Baldochi (2012)
implemented an automated system called USABILICS for remote evaluation. Tasks
to be performed by a user are predeﬁned using an intuitive approach that can be
applied for larger web systems. The evaluation is based on matching a usage pattern
performed by the user against the one conducted by an expert of the system provid-
ing a usability index for the probed application. Harrati et al. (2016, 2015) proposed
an online automated system for formalizing user interactions with a given system
guided through a set of rules describing certain goals is proposed. A task model is
constructed to capture all the interactions and navigation path to be carried out by the
university staﬀ. Empirical client-side log data is collected from university lecturers
within the usability evaluation of the e-learning system in a non-intrusive fashion
without the need to install additional tools. Empirical results performed to inspect
the usability and utilization of the e-learning platform have revealed that potential
reasons to impede the adoption of new technologies within the teaching process is
primarily related to the complex nature of software interface where the majority of
lecturers failed to achieve satisfactory utilization.
5
Conclusions
The use of e-learning in academic and corporate institutions has gained popular-
ity mainly due to the perceived advantages of ﬂexibility around ﬁtting the learner’s
time requirements and overcoming the issue around the geographical restrictions.
Evaluation of e-learning applications in terms of user experience, satisfaction and
acceptance has received recently considerable attention from the research commu-
nity in order to assess and quantify the satisfaction and eﬀectiveness level for aca-
demic users. This is due to the increasing concern that despite the wide use and
deployment of e-learning technologies, the intended impact on education is not
achieved. The dearth of conventional e-learning system quality models is in stark

216
I. Bouchrika et al.
contrast compared to the considerable body of work on software quality assurance.
There is a number of models and theories in the literature for understanding, predict-
ing, and assessing the interaction process with its involved parts including personal
factors, behavior, and the environment. Two major categories are discussed to survey
the diﬀerent approaches for the adoption of e-learning systems based on the evalua-
tion paradigms which are subjective and objective.
References
Ajzen, I., & Fishbein, M. (1975). Belief, attitude, intention and behavior: An introduction to theory
and research.
Albert, W., & Tullis, T. ( 2013). Measuring the user experience: Collecting, analyzing, and pre-
senting usability metrics. Newnes.
Asarbakhsh, M., & Sandars, J. (2013). E-learning: The essential usability perspective. Clin. Teach.,
10(1), 47–50.
Atterer, R., & Schmidt, A. (2007). Tracking the interaction of users with AJAX applications for
usability testing. In Proceedings of the SIGCHI Conference on Human Factors in Computing
Systems (pp. 1347–1350). ACM.
Barlow, K., & Lane, J. (2007). Like technology from an advanced alien culture: Google apps for
education at ASU. In Proceedings of the 35th Annual ACM SIGUCCS Fall Conference (pp. 8–
10). ACM.
Bringula, R. P. (2013). Inﬂuence of faculty-and web portal design-related factors on web portal
usability: A hierarchical regression analysis. Computers & Education, 68, 187–198.
Brooke, J. (1996). SUS—A quick and dirty usability scale. In Usability evaluation in industry (Vol.
189, no. 194, pp. 4–7).
Burton-Jones, A., & Grange, C. (2012). From use to eﬀective use: A representation theory perspec-
tive. Information Systems Research, 24(3), 632–658.
Cassino, R., & Tucci, M. (2011). Developing usable web interfaces with the aid of automatic veriﬁ-
cation of their formal speciﬁcation. Journal of Visual Languages & Computing, 22(2), 140–149.
Cassino, R., Tucci, M., Vitiello, G., & Francese, R. (2015). Empirical validation of an automatic
usability evaluation method. Journal of Visual Languages & Computing, 28, 1–22.
Chua, B. B., & Dyson, L. E. (2004). Applying the ISO 9126 model to the evaluation of an e-learning
system. In Proceedings of ASCILITE (pp. 5–8).
D’Ambra, J., Wilson, C. S., & Akter, S. (2013). Application of the task-technology ﬁt model to
structure and evaluate the adoption of e-books by academics. Journal of the Association for
Information Science and Technology, 64(1), 48–64.
Davis, F. D., Bagozzi, R. P., & Warshaw, P. R. (1989). User acceptance of computer technology: A
comparison of two theoretical models. Management Science, 35(8), 982–1003.
de Santana, V. F., & Baranauskas, M. C. C. (2015). Welﬁt: A remote evaluation tool for identify-
ing web usage patterns through client-side logging. International Journal of Human-Computer
Studies, 76, 40–49.
de Vasconcelos, L. G., & Baldochi Jr., L. A. (2012). Towards an automatic evaluation of web appli-
cations. In Proceedings of the 27th Annual ACM Symposium on Applied Computing (pp. 709–
716). ACM.
Dimoka, A., Banker, R. D., Benbasat, I., Davis, F. D., Dennis, A. R., Gefen, D., et al. (2012). On
the use of neurophysiological tools in is research: Developing a research agenda for neurois. MIS
Quarterly, 36(3), 679–702.
Dishaw, M. T., & Strong, D. M. (1999). Extending the technology acceptance model with task-
technology ﬁt constructs. Information & Management, 36(1), 9–21.

Evaluating the Acceptance of e-Learning Systems ...
217
Eckhardt, A., Maier, C., & Buettner, R. (2012). The inﬂuence of pressure to perform and experience
on changing perceptions and user performance: A multi-method experimental analysis. In ICIS
2012 Proceedings.
Escobar-Rodriguez, T., & Monge-Lozano, P. (2012). The acceptance of moodle technology by busi-
ness administration students. Computers & Education, 58(4), 1085–1093.
Garrison, D. R. (2011). E-learning in the 21st century: A framework for research and practice.
Taylor & Francis.
Goodhue, D. L., & Thompson, R. L. (1995). Task-technology ﬁt and individual performance. MIS
Quarterly, 213–236.
Harrati, N., Bouchrika, I., & Mahfouf, Z. (2016). Investigating the uptake of educational systems
by academics using the technology to performance chain model. Library Hi Tech, 35(4).
Harrati, N., Bouchrika, I., Tari, A., & Ladjailia, A. (2015). Automating the evaluation of usability
remotely for web applications via a model-based approach. In 2015 First International Confer-
ence on New Technologies of Information and Communication (NTIC) (pp. 1–6). IEEE.
Harrati, N., Bouchrika, I., Tari, A., & Ladjailia, A. (2016). Exploring user satisfaction for e-learning
systems via usage-based metrics and system usability scale analysis. Computers in Human
Behavior, 61, 463–471.
Henriksson, A., Yi, Y., Frost, B., & Middleton, M. (2007). Evaluation instrument for e-government
websites. Electronic Government, an International Journal, 4(2), 204–226.
Hornbæk, K. (2006). Current practice in measuring usability: Challenges to usability studies and
research. International Journal of Human-Computer Studies, 64(2), 79–102.
Hrtoňová, N., Kohout, J., Rohlíková, L., & Zounek, J. (2015). Factors inﬂuencing acceptance of
e-learning by teachers in the Czech Republic. Computers in Human Behavior, 51, 873–879.
Hsu, C.-L., & Lin, J. C.-C. (2008). Acceptance of blog usage: The roles of technology acceptance,
social inﬂuence and knowledge sharing motivation. Information & Management, 45(1), 65–74.
Ismailova, R. (2017). Web site accessibility, usability and security: A survey of government web
sites in Kyrgyz Republic. Universal Access in the Information Society, 16(1), 257–264.
Ivory, M. Y., & Hearst, M. A. (2001). The state of the art in automating usability evaluation of user
interfaces. ACM Computing Surveys, 33(4), 470–516.
Joo, S., & Choi, N. (2015). Factors aﬀecting undergraduates selection of online library resources
in academic tasks: Usefulness, ease-of-use, resource quality, and individual diﬀerences. Library
Hi Tech, 33(2), 272–291.
Laurillard, D., Oliver, M., Wasson, B., & Hoppe, U. (2009). Implementing technology-enhanced
learning. In Technology-enhanced learning (pp. 289–306). Springer.
Liapis, A., Katsanos, C., Sotiropoulos, D., Xenos, M., & Karousos, N. (2015). Recognizing
emotions in human computer interaction: Studying stress using skin conductance. In Human-
Computer Interaction–INTERACT 2015 (pp. 255–262). Springer.
Liaw, S.-S., Huang, H.-M., & Chen, G.-D. (2007). Surveying instructor and learner attitudes toward
e-learning. Computers & Education, 49(4), 1066–1080.
Marco, F. A., Penichet, V. M. R., & Gallud, J. A. (2013). Collaborative e-learning through drag and
share in synchronous shared workspaces. Journal of UCS, 19(7), 894–911.
Mayhew, D. J. (1999). The usability engineering lifecycle. In CHI’99 Extended Abstracts on Human
Factors in Computing Systems (pp. 147–148). ACM.
McGill, T. J., & Hobbs, V. (2008). How students and instructors using a virtual learning envi-
ronment perceive the ﬁt between technology and task. Journal of Computer Assisted Learning,
24(3), 191–202.
McGill, T. J., & Klobas, J. E. (2009). A task-technology ﬁt view of learning management system
impact. Computers & Education, 52(2), 496–508.
McGill, T. J., Klobas, J. E., & Renzi, S. (2014). Critical success factors for the continuation of
e-learning initiatives. The Internet and Higher Education, 22, 24–36.
McGill, T., Klobas, J., & Renzi, S. (2011). Lms use and instructor performance: The role of task-
technology ﬁt. International Journal on E-Learning, 10(1), 43–62.

218
I. Bouchrika et al.
Mott, J. (2010). Envisioning the post-LMS era: The open learning network. Educause Quarterly,
33(1), 1–9.
Navimipour, N. J., & Zareie, B. (2015). A model for assessing the impact of e-learning systems on
employees satisfaction. Computers in Human Behavior, 53, 475–485.
Orfanou, K., Tselios, N., & Katsanos, C. (2015). Perceived usability evaluation of learning man-
agement systems: Empirical evaluation of the system usability scale. The International Review
of Research in Open and Distributed Learning, 16(2).
Paganelli, L., & Paternò, F. (2002). Intelligent analysis of user interactions with web applications.
In International Conference on Intelligent User Interfaces (pp. 111–118).
Paternò, F., Santoro, C., & Spano, L. D. (2012). Improving support for visual task modelling. In
Human-centered software engineering (pp. 299–306). Springer.
Pentland, B. T. (1989). Use and productivity in personal computing: An empirical test. In Proceed-
ings of the Tenth International Conference on Information Systems, MA, Boston (pp. 211–222).
Persico, D., Manca, S., & Pozzi, F. (2014). Adapting the technology acceptance model to evaluate
the innovative potential of e-learning systems. Computers in Human Behavior, 30, 614–622.
Phillips, L. A., Calantone, R., & Lee, M.-T. (1994). International technology adoption: Behavior
structure, demand certainty and culture. Journal of Business & Industrial Marketing, 9(2), 16–
28.
Phillips, R., McNaught, C., & Kennedy, G. (2012). Evaluating e-learning: Guiding research and
practice. Routledge.
Quade, M., Lehmann, G., Engelbrecht, K.-P., Roscher, D., & Albayrak, S. (2013). Automated
usability evaluation of model-based adaptive user interfaces for users with special and spe-
ciﬁc needs by simulating user interaction. In User modeling and adaptation for daily routines
(pp. 219–247). Springer.
Raven, A., Leeds, E. M., & Park, C. (2010). Digital video presentation and student performance:
A task technology ﬁt perspective. International Journal of Information and Communication
Technology Education, 6(1), 17.
Renaut, C., Batier, C., Flory, L., & Heyde, M. (2006). Improving web site usability for a better e-
learning experience. In Current developments in technology-assisted education (pp. 891–895).
Badajoz, Spain: FORMATEX.
Roca, J. C., & Gagné, M. (2008). Understanding e-learning continuance intention in the workplace:
A self-determination theory perspective. Computers in Human Behavior, 24(4), 1585–1604.
Rogers, E. M. (2010). Diﬀusion of innovations. Simon and Schuster.
Rogers Everett, M. (1995). Diﬀusion of innovations 12. New York.
Sauro, J., & Lewis, J. R. (2009). Correlations among prototypical usability metrics: Evidence for
the construct of usability. In Proceedings of the SIGCHI Conference on Human Factors in Com-
puting Systems (pp. 1609–1618). ACM.
Simões, A. P., & de Moraes, A. (2012). The ergonomic evaluation of a virtual learning environment
usability. Work-Journal of Prevention Assessment and Rehabilitation, 41, 1140.
Straub, E. T. (2009). Understanding technology adoption: Theory and future directions for informal
learning. Review of Educational Research, 79(2), 625–649.
Šumak, B., HeričKo, M., & PušNik, M. (2011). A meta-analysis of e-learning technology accep-
tance: The role of user types and e-learning technology types. Computers in Human Behavior,
27(6), 2067–2077.
Szajna, B. (1996). Empirical evaluation of the revised technology acceptance model. Management
Science, 42(1), 85–92.
Taylor, S., & Todd, P. A. (1995). Understanding information technology usage: A test of competing
models. Information Systems Research, 6(2), 144–176.
Tiedtke, T., Märtin, C., & Gerth, N. (2002). AWUSA—A tool for automated website usability
analysis. In Workshop on Interactive Systems. Design, Speciﬁcation, and Veriﬁcation. Rostock,
Germany, June (pp. 12–14).

Evaluating the Acceptance of e-Learning Systems ...
219
Tsai, P. C.-F., Yen, Y.-F., Huang, L.-C., & Huang, C. (2007). A study on motivating employees
learning commitment in the post-downsizing era: Job satisfaction perspective. Journal of World
Business, 42(2), 157–169.
Vaquero, L. M., Rodero-Merino, L., Caceres, J., & Lindner, M. (2008). A break in the clouds:
Towards a cloud deﬁnition. ACM SIGCOMM Computer Communication Review, 39(1), 50–55.
Venkatesh, V., & Davis, F. D. (2000). A theoretical extension of the technology acceptance model:
Four longitudinal ﬁeld studies. Management Science, 46(2), 186–204.
Venkatesh, V., & Morris, M. G. (2000). Why don’t men ever stop to ask for directions? gender,
social inﬂuence, and their role in technology acceptance and usage behavior. MIS Quarterly,
115–139.
Welsh, E. T., Wanberg, C. R., Brown, K. G., & Simmering, M. J. (2003). E-learning: Emerging uses,
empirical results and future directions. International Journal of Training and Development, 7(4),
245–258.
Yi, Y. J., You, S., & Bae, B. J. (2016). The inﬂuence of smartphones on academic performance: The
development of the technology-to-performance chain model. Library Hi Tech, 34(3), 480–499.
Zhang, D., & Nunamaker, J. F. (2003). Powering e-learning in the new millennium: An overview
of e-learning and enabling technology. Information Systems Frontiers, 5(2), 207–218.

Glossary
Action Research A form of iterative and progressive research aimed at solving
problems in real contexts.
Aggregation A process which the information is gathered and expressed in a
summarised format, for purposes analysis.
AMOES is the Attrition Model for Open Learning Environment with the aim to
identify the behavior from students in a MOOC.
Attribute Selection The process of selecting a subset of attributes for building a
model.
Attrition may refer to the gradual reduction of interest from the learner, it is the
loss of participants during an online course.
Automatic Speech Recognition (ASR) Technique to determine the word
sequence in a speech signal. To do this, this technology ﬁrst detects basic units
in the signal, e.g. phonemes, which are then combined to determine words.
Case deletion this is a method of imputation in when all cases with a missing value
are deleted.
Chi Squared test A statistical hypothesis test where in the sampling distribution of
the test statistic when the null hypothesis is true.
Classiﬁcation The process in which the objects are understood and group into
classes.
Cluster Grouping a set of objects into objects in the same group are similar.
Cognition A mental process of acquiring and understanding knowledge.
Cognitive model Simulating human problem solving and mental task processes in
a computerized model.
Context is any information that can be used to characterize the situation of a
learner.
© Springer International Publishing AG 2018
S. Caballé and J. Conesa (eds.), Software Data Engineering for Network eLearning
Environments, Lecture Notes on Data Engineering and Communications
Technologies 11, https://doi.org/10.1007/978-3-319-68318-8
221

Conversational interface Computer program designed that emulates a dialog with
a human being using natural language and a set of input and/or output modalities
(e.g., speech, gestures, emotion recognition, visual information, etc.).
Cross validation division of data set into training and testing set and validate the
results of them.
Data integration Combining data from different sources into one location.
Data interpretation The process of doing collection, analysis and present the data.
Data-mart An upper layer of a data warehouse which is focused on user acces-
sibility and querying.
Data mining collection of methods use for analysis of data.
Decision tree induction A classiﬁcation data mining algorithm.
Dependency modeling Searches for relationships between variables.
Descriptive analysis use of different procedures to summarise, organize, and
simplify data.
Descriptive statistics A quantitative description of summarise features of a col-
lection of data.
Deviation detection known as anomaly detection.
Dialog management (DM) Implementation of the “intelligent” behavior of the
conversational system. It receives some sort of internal representation obtained
from the user input and decides the next action the system must carry out.
Educational data mining application of data mining techniques on educational
data.
edX is a nonproﬁt organization that acts as a massive open online course (MOOC)
provider and runs on the free Open edX open-source software platform.
Engagement is the emotional commitment that the learner has within a course and
its goals.
Engagement Dimension One of the three dimensions which constitutes engage-
ment, namely behaviour, cognition and emotion.
Entropy This is deﬁned as the number of ways a system can be arranged. When
there are more ways the system can be arranged, the higher the entropy.
F measure combination of precision and recall values.
FEEF Full Engagement Educational Framework is an approach composed of
different strategies to identify speciﬁc target audiences in order to drive
engagement through valuable and interesting content in online learning courses.
222
Glossary

Framework is a structure, process, components and a logical way to classify,
segment, categorize and operate with a speciﬁc target.
Fuzzy clustering
A type of clustering which each data point can belong to more
than one cluster.
Gain ratio This is a ratio of information gain.
Granularity the extent which a system can be represented to be comprised of
smaller components.
Info gain Measures how one probability distribution deviates from a second
expected probability distribution.
K means algorithm A mining algorithm for clustering.
Kappa statistic This measures inter-rater agreement for qualitative attributes.
Knowledge discovery in databases An iterative process of extracting knowledge
from raw data.
Learning Analytics The measurement, collection, analysis and reporting of data
about learners and their contexts, for purposes of understanding and optimizing
learning and the environments in which it occurs.
Learning Dashboard A visualization tool that gives easily accessible overviews
and summaries from learner data.
Likert Scale A psychometric scale commonly involved in research that employs
questionnaires.
Linear regression An approach for modeling the relationship between a scalar
dependent variable y and one or more independent variables x.
Macrolevel Data Data describing an overall view.
Metadata data which describes other data.
Microlevel data from a context in a minute level.
Micromasters is a graduate-level certiﬁcate program offered online by some
universities in collaboration with edX platform. Unlike a traditional master’s
degree, a MicroMasters credential allows a learner to transfer coursework toward
credit for a traditional master’s program (at participating universities) should he
or she later choose.
Missing values No data values in the data set.
Mobile learning (M-learning) M-learning or mobile learning can be deﬁned as
the integration of mobile technologies, mobile devices, and e-learning activities
to improve personal and distance education.
Glossary
223

MOOC Massive Open Online Courses are educational, lifelong learning or
training courses offered with a virtual learning environment with a methodology
aimed at unlimited participation of learners and open access via the web.
MSE Analytics Analytics based on the microlevel student engagement data.
Naïve Bayes classiﬁer A classiﬁcation data mining algorithm.
Natural language generation (NLG) Creation of messages in text mode, gram-
matical and semantically correct, which will be either displayed on screen or
converted into speech by means of text-to-speech synthesis.
Noisy data Corrupted data in the data set.
Nonlinear regression This is type of regression analysis which observational data
are shown by a function which is a nonlinear combination of the model
parameters and depends on one or more independent variables.
Normalization The process of organizing the attributes and relations of a relational
database to reduce data redundancy and improve data integrity.
Outliers are values that stays in an abnormal distance from other values in a
random sample from a population.
Precision Positive predictive values.
Predictive analysis This is used to make predictions about unknown future events.
Principal component analysis This is a technique which used to keep the varia-
tion and bring out strong patterns in a dataset.
PTAT is a metric, which represents the number of unique people that created a
story about a page or on a page via different actions in a social network.
Random forest A classiﬁcation type data mining algorithm.
Redundant data data duplication in the database.
Regression A data mining method.
Rule based classiﬁcation A classiﬁcation type data mining algorithm.
Self-Report A data collection which allows respondents to supply subjective data.
Situational Engagement engagement in context.
Speech synthesis Artiﬁcial generation of human-like speech. Text-To-Speech
synthesis (TTS) transforms text sentences into a speech signal.
Spoken Language Understanding (SLU) Technique to obtain the semantic
content of the sequence of words provided by the ASR module. It must face a
variety of phenomena, for example, ellipsis, anaphora and ungrammatical
structures typical of spontaneous speech.
224
Glossary

Student Engagement the behavioural, cognitive and emotional manifestation of
student learning.
Virtual Learning Environment an online platform designed to manage student
learning and related resources and services.
Visualization Presentation of data in graphical way.
Glossary
225

Index
A
Academic integrity, 27, 29, 36, 38, 40–42, 45
Acceptance evaluation, 210
Action research, 57
Adoption, 207
Advice for action, 139, 141, 142, 155
Aggregation, 182, 183
AMOES, 165, 166
Artiﬁcial Neural Network, 181
Attribute construction, 183
Attribute selection, 195
Attrition, 165
Automatic feedback, 139, 141, 142, 146, 150,
156, 157
Automatic Speech Recognition (ASR), 94,
102, 221
B
Blackboard, 203
Blackboard learn, 204
C
Case deletion, 191
Chi-squared tests, 221
Claroline, 204
Classiﬁcation, 181, 183–187, 192, 193, 195,
224
Cloud systems, 202, 205
Clustering, 183, 185–188, 223
Cognitive engagement, 50, 51, 55, 57–60, 63
Cognitive model, 187, 221
Computer-based training, 200
Context, 162, 163, 165–168, 175
Conversational interface, 93–100, 102–104,
109, 222
Corporate training, 200
Cross validation, 193, 222
CSV. See comma-separated values
D
Data integration, 182, 222
Data interpretation, 181, 183, 222
Data mining, 179–187, 189, 190, 192, 193,
195, 196, 222, 224
Decision tree induction, 184, 222
Dependency modeling, 183, 222
Descriptive analysis, 222
Descriptive mining, 183
Descriptive statistics, 190, 222
Deviation detection, 183, 222
Dialog Management (DM), 94, 97, 98,
102–104, 222
Difussion, 207
Docebo LMS, 206
E
Educational data mining, 30, 31
edX, 163, 169, 175, 203, 205
Emotional engagement, 49, 50, 52, 54, 55, 57,
59–62
Engagement, 162–164, 166–171
Entropy, 184, 193, 222
ETL. See Extract-Transform-Load
F
Feature construction, 183
FEEF, 163, 166–168, 171, 174, 175
F measure, 222
Framework, 162, 163, 166, 168–172, 174, 175
G
Gain ratio, 184, 193, 223
Generalization, 183
Google classroom, 206
Granularity, 49, 51
I
Info gain, 193, 223
© Springer International Publishing AG 2018
S. Caballé and J. Conesa (eds.), Software Data Engineering for Network eLearning
Environments, Lecture Notes on Data Engineering and Communications
Technologies 11, https://doi.org/10.1007/978-3-319-68318-8
227

Integration, 182, 222
Interpretation, 183
K
Kappa statistic, 193, 194, 223
K means algorithm, 223
L
LA. See Learning Analytics
Learning analytics, 2, 30, 31, 33, 45, 49
Learning and gaming analytics., 115
Learning management system, 201, 202,
204–206, 209, 210
Linear regression, 185, 186, 189, 223
M
Machine learning, 2, 6
Massive Open Online Courses (MOOC), 162,
164, 166, 167, 174, 175
Microlevel data, 54, 59
MicroMasters, 163, 169, 171, 172, 174, 175
Missing values, 182, 185, 191, 223
Mobile learning (M-learning), 93, 95, 97, 223
MOOCs. See Massive Open Online Courses
Moodle, 203, 204
Motivation, 48, 49, 52, 53, 60
MSE Analytics, 55–57
N
Naïve Bayes classiﬁer, 224
Natural Language Generation (NLG), 94, 102,
104, 224
Noisy data, 182, 224
Nonlinear regression, 224
Normalization, 183, 224
O
Objective evaluation, 214
Online virtual labs, 115
On-premise systems, 202
OpenMentor, 139, 142, 150, 152, 154, 155,
157
Outliers, 182, 224
P
PA. See Predictive Analytics
Precision, 193, 194, 222, 224
Prediction accuracy, 193, 194
Predictive analytics, 2
Predictive mining, 183
Predictive model, 5
Preprocessing, 181, 182, 190
Principal component analysis, 191, 224
Programming education, 48, 51
PTAT, 174
Q
Questionnaires, 182, 191
R
Recall, 193, 194, 222
Redundant data, 182, 224
Regression, 183, 185, 189, 224
Rule based Classiﬁcation, 185
S
Sakai, 203, 204
Self-report, 49, 53, 56, 57, 59, 63
Smoothing, 182
Speech synthesis, 94, 102, 104, 224
Spoken Language Understanding (SLU), 94,
98, 102, 224
STEM education, 51, 54, 63
Student engagement, 48, 49, 51–57, 61–63
Subjective evaluation, 211
Summarization, 183
Supervised learning, 7, 8
Support vector machine, 181, 187, 189
System usability scale, 212
T
Target data, 181, 184, 191, 195
Technology acceptance model, 211
Technology to performance chain, 211
Transformation, 181, 182
U
Unsupervised learning, 7, 8
Usability, 208
V
Virtual Learning Environment (VLEs), 55
Visualization, 187, 188, 195, 225
Visualization methodologies, 115, 116
VLE. See Virtual Learning Environment
228
Index


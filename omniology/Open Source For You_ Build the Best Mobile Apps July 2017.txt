Volume: 05 | Issue: 10 | Pages: 108 | July 2017
ISSN-2456-4885
` 120
Regular Expressions In Programming 
Languages: A Peek At Python
Get Started With Contributing 
To Open Source
Hybrid 
Mobile App 
Development
Kotlin: A Fun 
Language For Android 
App Development
Integrating 
CouchBase Lite With 
Android Studio
Build
The BesT
Apps
Mobile
An Interview With  
Keith Bergelt, CEO, Open Invention Network


PLATINUM
PLAN
GOLD
PLAN 
Within 15 mins
Lowest Initial Response (sev 1) 
Within 60 mins 
Within 30 mins
Standard Escalation Time (sev 1) Within 90 mins 
7 years
Release Lifetime 
7 years 
24/7
Operational Hours 
24/7 
5
Named Contacts 
5 
Yes
Fix guaranteed by SLA 
No 
Unlimited
Number of Incidents 
Unlimited 
15 min (bug fix within 24h)
Severity 1 initial response 
1 hour 
2 hours (bug fix within 10d)
Severity 2 initial response 
2 hours 
Next business day (bug fix within 20d)
Severity 3 initial response 
Next business day 
1 week (bug fix within 60d)
Severity 4 initial response 
1 week 
PostgreSQL 24/7 Support
WHEN IT’S CRITICAL
YOU CAN COUNT ON US
https://2ndQuadrant.com   |   info@2ndQuadrant.com
USA  +1 650 378 1218   |   UK +44 (0)870 766 7756   |   India +91 20 4014 7882

Admin
20 
The DevOps Series  
Deploying Graphite 
Using Ansible
23 
A Guide to Running 
OpenStack on AWS
Developers
30  Using MongoDB to Improve 
the IT Performance of an 
Enterprise
40  Building REST APIs with the 
LoopBack Framework
44  Cross-Platform Mobile App 
Development Using the 
Meteor Framework
50 
Kotlin: A Fun Language for 
Android App Development
56 
Software Automation 
Testing Using Sikuli
59  Developing HTML5 and Hybrid 
Apps for Mobile Devices
62 
Using the Onsen UI with 
Monaca IDE for Hybrid 
Mobile App Development
65 
Why We Should Integrate 
Couchbase Lite with 
Android Studio
67 
Eight Easy-to-Use Open 
Source Hybrid Mobile 
Application Development 
Platforms
76 
Regular Expressions in 
Programming Languages:  
A Peek at Python
81 
An Overview of Convertigo
FOR U & ME
83  Get Started with  
Contributing to Open Source 
86 
Browser Fingerprinting: How 
EFF Tools Can Protect You
89 
Mobile App Development in 
the Fast-changing Mobile 
Phone Industry
REGULAR FEATURES
06 
FOSSBytes
  16  New Products
  104  Tips & Tricks
71
34
The Best Open Source Mobile App 
Development Frameworks
The AWK Programming Language: 
A Tool for Data Extraction
ISSN-2456-4885
4 | July 2017 | OPEN SOuRCE FOR yOu | www.OpenSourceForu.com

Live Gnome, 64-bit 
JULY 2017
CD 
Tea
m e-
mail
: cd
tea
m@
efy.i
n
Rec
omm
end
ed S
yste
m Re
quir
eme
nts: 
P4, 
1GB
 RA
M, D
VD-R
OM 
Driv
e
In c
ase 
this
 DV
D do
es n
ot w
ork 
prop
erly,
 wri
te t
o us
 at s
upp
ort
@ef
y.in 
for 
a fre
e re
plac
eme
nt.
Here’s the latest and the most 
stable version of Debian.
DVD of The MonTh
• Debian 9 Stretch GNOME Live
106
Keith Bergelt, CEO of Open 
Invention Network
26
100
NetLogo: An Open Source Platform  
for Studying Emergent Behaviour
94 
An Introduction to Deep 
(Machine) Learning
102 Caricaturing with Inkscape
Columns
13 
CodeSport
18 
Exploring Software: Connecting 
LibreOffice Calc with Base 
and MariaDB/MySQL
“At the 
heart of 
the Open 
Invention 
Network 
is its 
powerful 
cross-
licence”
Editor
Rahul chopRa
Editorial, SubScriptionS & advErtiSing
Delhi (hQ)
d-87/1, okhla industrial area, phase i, new delhi 110020
ph: (011) 26810602, 26810603; Fax: 26817563
E-mail: info@efy.in
MiSSing iSSuES
e-mail: support@efy.in
back iSSuES
Kits ‘n’ Spares
new delhi 110020 
ph: (011) 26371661,  26371662
E-mail: info@kitsnspares.com
nEwSStand diStribution
ph: 011-40596600
E-mail: efycirc@efy.in 
advErtiSEMEntS 
mumbai
ph: (022) 24950047, 24928520 
E-mail: efymum@efy.in
beNGaluRu
ph: (080) 25260394, 25260023 
E-mail: efyblr@efy.in
PuNe
ph: 08800295610/ 09870682995 
E-mail: efypune@efy.in
GuJaRaT
ph: (079) 61344948 
E-mail: efyahd@efy.in
chiNa
power pioneer group inc.  
ph: (86 755) 83729797, (86) 13923802595 
E-mail: powerpioneer@efy.in
JaPaN
tandem inc., ph: 81-3-3541-4166 
E-mail: tandem@efy.in
SiNGaPORe
publicitas Singapore pte ltd 
ph: +65-6836 2272 
E-mail: publicitas@efy.in
TaiwaN 
J.k. Media, ph: 886-2-87726780 ext. 10 
E-mail: jkmedia@efy.in
uNiTeD STaTeS
E & tech Media 
ph: +1 860 536 6677 
E-mail: veroniquelamarque@gmail.com
printed, published and owned by ramesh chopra. printed at tara 
art printers pvt ltd, a-46,47, Sec-5, noida, on 28th of the previous 
month, and published from d-87/1, okhla industrial area, phase i, new 
delhi 110020. copyright © 2017. all articles in this issue, except for 
interviews, verbatim quotes, or unless otherwise explicitly mentioned, 
will be released under creative commons attribution-noncommercial 
3.0 unported license a month after the date of publication. refer to 
http://creativecommons.org/licenses/by-nc/3.0/  for a copy of the 
licence. although every effort is made to ensure accuracy, no responsi-
bility whatsoever is taken for any loss due to publishing errors. articles 
that cannot be used are returned to the authors if accompanied by a 
self-addressed and sufficiently stamped envelope. but no responsibility 
is taken for any loss or delay in returning the material. disputes, if any, 
will be settled in a new delhi court only.
SubScRiPTiON RaTeS 
Year 
Newstand Price 
You Pay 
Overseas
 
(`) 
(`)
Five 
7200 
4320 
—
three 
4320 
3030 
—
one 
1440 
1150  
uS$ 120
kindly add ` 50/- for outside delhi cheques.
please send payments only in favour of eFY enterprises Pvt ltd.
non-receipt of copies may be reported to support@efy.in—do mention 
your subscription number.
www.OpenSourceForu.com | OPEN SOuRCE FOR yOu | July 2017 | 5

Compiled by: 
Jagmeet Singh
NEC establishes centre for 
Big Data and analytics in India
Japanese IT giant NEC Corporation has announced the launch of its Center of 
Excellence (CoE) in India to promote the adoption of Big Data and analytics 
solutions. Based in Noida, the new centre is the first by the company and is set to 
simplify digital transformation for clients across sectors such as telecom, retail, 
banking, financial services, retail and government.
NEC is set to invest US$ 10 million over the next three years to achieve a 
revenue of US$ 100 million in the next three years through the new CoE. While the 
Tokyo-headquartered company is initially targeting markets including Japan, India, 
Singapore, Philippines and Hong Kong, the centre will expand services throughout 
APAC and other regions.
“The new CoE is an important step towards utilising Big Data analytics and 
NEC’s Data Platform for Hadoop to provide benefits for government bodies and 
enterprises in India and across the world,” said Tomoyasu Nishimura, senior vice 
president, NEC Corporation.
NEC, in partnership with NEC India and NEC Technologies India (NTI), 
is leveraging Hadoop for its various Big Data and analytics developments. 
The company has implemented its hardware in partnership with Hortonworks 
and Red Hat.
“The latest CoE is designed as a single platform that can be used to cater to 
the needs of processing structured and unstructured data,” Piyush Sinha, general 
manager for corporate planning and business management, NEC India, told Open 
Source For You.
Sinha is handling a team of around 20 people who are initiating operations 
through the new centre. However, NEC has plans to organise a 100-member 
professional team to address requirements in the near future. There are also plans to 
hire fresh talent to enhance Big Data developments.
The Data Platform for Hadoop (DPH) that NEC is using at the centre features a 
business intelligence layer in the background — alongside the Hadoop deployment, 
thus relieving customers of the chore of searching the individual building blocks 
of Big Data analytics. This combination also provides them the ability to generate 
value from the enormous pools of data.
NEC already has a record of contributing to national projects in the country. 
The company enabled the mass-level adoption of Aadhaar in partnership with 
the Unique Identification Authority of India (UIDAI) by providing a large-scale 
FOSSBYTES
GNOME 3.26 desktop to offer 
enhanced Disk Utility
While GNOME maintainers are gearing 
up with some major additions to the 
GNOME 3.26 desktop environment, 
the latest GNOME Disk Utility gives 
us a glimpse of the features that will 
enhance your experience.
GNOME 3.25.2 was published 
recently, and came with updated 
components as well as apps. That same 
release included the first development 
release of GNOME Disk Utility 3.26.
The new GNOME Disk Utility 
includes multiple mounting options 
along with an insensitive auto-clear 
switch for unused loop devices. Also, 
there are clearer options for encryption.
The GNOME team has 
additionally provided certain visual 
changes. The new GNOME Disk 
Utility version allows you to create an 
empty disk image from the App Menu. 
Similarly, the behaviour of the file 
system dialogue has been improved, 
and the UUID (Universally Unique 
Identifier) of selected volumes is 
displayed by default.
The update enables disk images to 
use MIME types. The disk mounting 
functionality in the software has been 
improved, and now has the ability 
to auto-clear the handling for lock 
and unmount buttons. Besides, the 
latest version prompts users while 
closing the app.
GNOME Disk Utility 3.25.2 is 
presently available for public testing. 
It will be released along with GNOME 
3.26 later this year. Those wanting to 
try the preview version can download 
and compile GNOME Disk Utility 
3.25.2 for their GNU/Linux distro.
6 | JuLY 2017 | OPEN SOuRCE FOR YOu | www.OpenSourceForu.com

FOSSBYTES
Google Photos gets AI 
integration to archive users’ 
pictures
Google Photos has been enhanced with 
an all-new AI-powered upgrade that 
suggests which trivial pictures from your 
collection should be archived. The latest 
development is a part of the Google 
Assistant support.
The option to archive unwanted 
photos from your timeline is indeed 
welcome, in order to make your albums 
clutter-free. But what is more interesting 
is the artificial intelligence (AI) that 
Google has used to deliver this feature.
You might have a mixed collection 
of your pictures that include various 
memorable moments alongside some 
images that you no longer need. These 
could be photographs of documents, 
billboards or posters. The new AI 
integration within Google Photos 
detects the difference intelligently, 
while the Assistant tab helps you select 
the not-so-important ones for archiving. 
“You can review the suggestions 
and remove any photos you don’t want 
archived, and tap to confirm when you 
are done,” the Google Photos team 
wrote in a Google+ post, explaining the 
latest feature. Once archived, you can 
view those unimportant pictures in the 
‘Archive’ tab.
Google has started rolling out the 
new feature to Android, iOS and desktop 
versions of the Photos app. Going 
forward, the machine learning input 
powering the archiving feature is likely 
to be available to developers to test its 
effectiveness across new areas.
biometrics identification system. It is also supporting the Delhi-Mumbai Industrial 
Corridor (DMIC) that tracks nearly 70 per cent of container transactions in India.
Krita painting app enhances experience for GIMP files
Krita, the open source digital painting app, has announced v3.1.4. The new version 
is especially designed to deliver an enhanced experience for GIMP files.
Krita 3.1.3 was released in early 
May. And now, it’s already time for the 
new release. The new minor update has 
improved the loading time of GIMP 2.9 
files. The file format used to crash while 
attempting to quickly cycle through 
layers that contained a colour tag.
It is worth noting that the GIMP 2.9 
file format is not officially supported by 
Krita. Nevertheless, its development team has fixed the crashing issue in v3.1.4.
With the new version of Krita, you can no longer hide the template selector in the 
‘New Image’ dialogue box. But the menu entries in the macro recorder plugin in Krita 
are still visible. However, you can expect them to be removed in subsequent updates.
Krita 3.1.4 also has a patch for the crash that occurs while attempting to play an 
animation with the OpenGL option active.
Among the list of other minor bug fixes, Krita has fixed a crash that could 
occur during closing the last view on the last document. Krita 3.1.4 also improves 
rendering of animation frames.
Krita 3.1.4 with its bug fixes is a stability release that is a recommended update 
for all users. The latest version is available for Linux, Windows and MacOS on the 
official website.
Happiest Minds enters open source world by acquiring OSSCube
Happiest Minds Technologies, a Bengaluru-headquartered emerging IT company, 
has acquired US-based digital transformation entity OSSCube. The new 
development is aimed to help the Indian 
company enter the market of enterprise 
open source solutions and expand its 
presence in the US.
With the latest deal, Happiest Minds 
is set to widen its portfolio of digital 
offerings and strengthen its business in 
North America, the market that OSSCube is focused on. The team of 240 from 
OSSCube will now be a part of Happiest Minds. The acquisition will bring the total 
workforce of Happiest Minds to 2,400 and its active customer base to 170.
“We are delighted to welcome the OSSCube team into the Happiest Minds 
family,” said Ashok Soota, executive chairman, Happiest Minds, in a joint statement.
Founded in 2008 by Indian entrepreneurs Lavanya Rastogi and Vineet Agarwal, 
OSSCube is one of the leading open source companies around the globe. It is an 
exclusive enterprise partner for the open source platform Pimcore in North America. 
Apart from the open source developments, the company recently expanded the verticals 
it operates in to include the cloud, Big Data, e-commerce and enterprise mobility.
“Coming together with Happiest Minds offers us the scale, global reach, 
complementary skills and expertise, enabling us to offer ever more innovative 
solutions to our global customers. We believe that the great cultural fit, 
www.OpenSourceForu.com | OPEN SOuRCE FOR YOu | JuLY 2017 | 7

FOSSBYTES
marketplace synergies, and critical mass of consulting and IP-led offerings will 
unlock tremendous value for all our stakeholders,” said Lavanya Rastogi, co-
founder and CEO, OSSCube.
Rastogi will join Happiest Minds’ DTES business as the CEO. The Houston, 
Texas-based OSSCube will operate as a subsidiary of Happiest Minds. This is not the 
first acquisition by Happiest Minds. The Indian company has already bought out IoT 
services startup, Cupola Technology, this year.
With all these developments, Happiest Minds is moving forward to become the 
fastest Indian IT services company to reach a turnover of US$ 100 million. The 
company is also in plans to go public within the next three years.
Happiest Minds presently operates in Australia, the Middle East, USA, UK and 
the Netherlands. It offers domain-centric solutions applying skills, IP and functional 
expertise in IT services, product engineering, infrastructure management and security 
using technologies such as Big Data analytics, Internet of Things, mobility, the cloud, 
and unified communications.
Microsoft’s Cognitive Toolkit 2.0 debuts with open source 
neural library Keras
Microsoft’s deep learning and artificial intelligence (AI) solution, Cognitive Toolkit, 
has reached version 2.0. The new update is 
designed to handle production-grade and 
enterprise-grade deep learning workloads.
To enable the developments around 
deep learning applications, Cognitive 
Toolkit 2.0 supports Keras, an open source 
neural networking library. The latest 
integration helps developers receive a 
faster and more reliable deep learning platform without any code changes.
Chris Basolu, a partner engineering manager at Microsoft — who has been 
playing a key role in developing the Cognitive Toolkit builds — has tweaked the 
existing tools to make them accessible to enthusiasts with basic programming skills. 
There are also customisations available for highly-skilled developers who are all set 
to accelerate training for their own deep neural networks with large-sized data sets 
and across multiple servers.
Cognitive Toolkit 2.0 supports the latest versions of NVIDIA Deep Learning SDK 
and advanced GPUs like NVIDIA Volta.
In addition to the Keras support, the latest Cognitive Toolkit comes with model 
evaluations using Java bindings. There are also a few other tools that compress 
trained models in real-time. The new version is capable of compressing models even 
on resource-constrained devices such as smartphones and embedded hardware.
Microsoft has designed the AI solution after observing the problems faced 
organisations ranging from smart startups to large tech companies, government 
agencies and NGOs. The team, led by Basolu, aims to make the major Cognitive 
Toolkit features accessible to a wider audience.
Developers working with the previous release are recommended to upgrade to the 
latest Microsoft Cognitive Toolkit 2.0. You can find the code for the latest version on 
a GitHub repository.
Raspberry Pi 3 can now monitor vital health signs
Bengaluru-based hardware startup ProtoCentral has launched a multi-
parameter patient monitoring add-on for Raspberry Pi 3. Called HealthyPi, 
Tor 7.0 debuts with 
sandbox feature
Tor, the open source browser that 
is popular for maintaining user 
anonymity, has received an update to 
version 7.0, which includes a sandbox 
feature. The new version is designed 
for privacy-minded folks, offering 
them a more secure platform to surf 
the Web.
The new sandbox integration 
hides your real IP and MAC address 
and even your files. The feature 
makes sure that the information 
that the Tor browser learns about 
your computer is limited. Moreover, 
it makes it harder for hackers 
leveraging Firefox exploits to learn 
about user identities. “We know 
there are people who try to de-
anonymise Tor users by exploiting 
Firefox. Having the Tor browser 
run in a sandbox makes their life a 
lot harder,” Tor developer Yawning 
Angel wrote in a Q&A session.
Angel has been experimenting 
with the sandbox feature since 
October 2016. The feature was in 
the unstable and testing phase back 
then. However, the new release for 
Linux and Mac is a stable version to 
protect user identities.
In addition to the sandbox 
feature, the latest Tor version comes 
with Firefox 52 ESR. This new 
development includes tracking 
and fingerprinting resistance 
improvements such as the isolation 
of cookies, view-source requests and 
the permissions API to the first-party 
URL bar domain. It also includes 
WebGL2, the WebAudio, Social, 
SpeechSynthesis and Touch APIs.
With the Firefox 52 ESR 
addition, the latest Tor build will not 
work on non-SSE2-capable Windows 
hardware. Users also need to be using 
Mac OS X 10.9 or higher on Apple 
hardware, and use e10s on Linux and 
MacOS systems to begin with the 
sandboxing feature.
8 | JuLY 2017 | OPEN SOuRCE FOR YOu | www.OpenSourceForu.com


FOSSBYTES
the new development offers you a completely open source solution for 
monitoring vital health signs.
The HealthyPi board comes with an Atmel ATSAMD21 Cortex-M0 MCU that is 
compatible with Arduino Zero, and can monitor health signs like ECG and respiration. 
Additionally, there is a pulse oximetry front-end along with an LED driver and 22-bit 
ADC. The board includes a three-electrode cable that has a button and stereo connector.
Through the available 40-pin header connector, HealthyPi can connect with your 
Raspberry Pi 3. There is also a USB CDC device interface and UART connector that 
enables connectivity with an external blood pressure module.
It is worth noting here that the HealthyPi is not yet a certified medical device.
Microsoft Azure gets an open source tool to streamline 
Kubernetes developments
Microsoft has announced the first open source developer tool for Azure. 
Called Draft, the new tool is designed to help developers create cloud-native 
applications on Kubernetes. 
Draft lets developers get started with 
container-based applications without any 
knowledge of Kubernetes or Docker. 
There is no need to even install either 
of the two container-centric solutions 
to begin with the latest tool.
“Draft targets the ‘inner loop’ 
of a developer’s workflow — while 
developers write code and iterate, but before they commit changes to version 
control,” said Gabe Monroy, the lead project manager for containers on Microsoft 
Azure, in a blog post. Draft uses a simple detection script that helps in identifying 
the application language. Thereafter, it writes out simple Docker files and a 
Kubernetes Helm chart into the source tree to begin its action. Developers can also 
customise Draft using configurable packs.
The Azure team has specifically built Draft to support languages that include 
Python, Java, Node.js, Ruby GO and PHP. The tool can be used for streamlining any 
application or service. Besides, it is well-optimised for a remote Kubernetes cluster.
Monroy, who joined Microsoft as a part of the recent Deis acquisition, 
highlighted that hacking an application using Draft is as simple as typing ‘draft up’ 
on the screen. The command deploys the build in a development environment using 
the Helm chart.
The overall packaging of the tool is similar to Platform-as-a-Service (PaaS) 
systems such as Cloud Foundry and Deis. However, it is not identical to build-oriented 
PaaS due to its ability to construct continuous integration (CI) pipelines.
You can access Draft and its entire code as well as documentation from GitHub. The 
tool needs to spin up a Kubernetes Cluster on ACS to kickstart the new experience.
Toyota starts deploying Automotive Grade Linux
While Apple’s CarPlay and Google’s Android Auto are yet to gain traction, Toyota has given 
a big push to Automotive Grade Linux (AGL) and launched the 2018 Toyota Camry as its 
first vehicle with the open source in-car solution. The Japanese company is set to launch an 
AGL-based infotainment platform across its entire new portfolio of cars later this year.
“The flexibility of the AGL platform allows us to quickly roll out Toyota’s 
infotainment system across our vehicle line-up, providing customers with greater 
connectivity and new functionalities at a pace that is more consistent with consumer 
Google releases open source 
platform Spinnaker 1.0
Google has released v1.0 of 
Spinnaker. The new open source 
tool is designed for the multi-cloud 
continuous delivery platform.
The search giant started working 
on Spinnaker back in November 
2015 along with Netflix. The tool was 
aimed at offering large companies 
fast, secure and repeatable production 
deployments. It has so far been used by 
organisations like Microsoft, Netflix, 
Waze, Target and Oracle.
The Spinnaker version 1.0 comes 
with a rich UI dashboard and offers 
users the ability to install the tool 
on premise, on local infrastructure 
and the cloud. You can run it from 
a virtual machine as well as using 
Kubernetes. The platform can be 
integrated with workflows like Git, 
Docker registries, Travis CI and 
pipelines. It can be used for best-
practice deployment strategies as 
well. “With Spinnaker, you simply 
choose the deployment strategy you 
want to use for each environment, e.g., 
red/black for staging, rolling red/black 
for production, and it orchestrates 
the dozens of steps necessary under-
the-hood,” said Christopher Sanson, 
the product manager for Google 
Cloud Platform.
Google has optimised Spinnaker 
1.0 for Google Compute Engine, 
Container Engine, App Engine, 
Amazon Web Services, Kubernetes, 
OpenStack and Microsoft Azure. 
The company plans to add support 
for Oracle Bare Metal and DC/OS in 
upcoming releases.
10 | JuLY 2017 | OPEN SOuRCE FOR YOu | www.OpenSourceForu.com


FOSSBYTES
technology,” said Keiji Yamamoto, executive vice president of the connected 
company division, Toyota Motor Corporation.
Hosted by the Linux Foundation, AGL presently has more than 100 members 
who are developing a common platform 
to serve as the de facto industry standard. 
The Linux-based operating system comes 
with an application framework that allows 
automakers and suppliers to customise 
the experience with their specific features, 
services and branding.
According to Automotive Grade Linux 
executive director, Dan Cauchy, among 
all the AGL members, Toyota is one of the 
more active contributors. “Toyota has been a driving force behind the development 
of the AGL infotainment platform, and we are excited to see the traction that it is 
gaining across the industry,” said Cauchy.
Around 70 per cent of the AGL-based platform on the latest Toyota Camry 
sedan is said to have generic coding, while the remaining 30 per cent includes 
customised inputs. There are features like multimedia access and navigation.
In the future, the Linux Foundation and AGL partners are planning to deploy the 
open source platform for building even unmanned vehicles. This model could make 
competition tougher for emerging players including Google and Tesla, which are 
both currently testing their self-driving cars.
In addition to automotive companies like Toyota and Daimler, AGL is supported 
by tech leaders such as Intel, Renesas, Texas Instruments and Qualcomm. The 
open source community built AGL Unified Code Base 3.0 in December last year to 
kickstart developments towards the next-generation auto space.
Perl 5.26.0 is now out with tons of improvements
The Perl community has received version 26 of Perl 5. This new release is a 
collaborative effort from 86 authors.
Perl 5.26.0 contains approximately 360,000 lines of code across 2600 
files. Most of the changes in the latest version have originated in the CPAN 
(Comprehensive Perl Archive Network) 
modules. According to the official 
changelog, new variables such as @
{^CAPTURE}, %{^CAPTURE} and 
%{^CAPTURE_ALL} are included in the 
Perl 5.26.0 release.
“Excluding auto-generated files, 
documentation and release tools, there 
were approximately 230,000 lines of 
changes to 1,800 .pm, .t, .c and .h files,” 
Perl developer Sawyer X wrote in an email announcement.
The new version comes with some improvements like ‘here-docs’ can 
now strip leading whitespace. The original POSIX::tmpnam () command has 
also been replaced with File::Temp. Moreover, lexical sub-routines that were 
debuted with the Perl 5.18 are no longer experimental.
Google brings reCAPTCHA 
API to Android
Ten years after beginning to offer 
protection to Web users, Google has 
now taken a step towards protecting 
mobile devices and brought its 
renowned reCAPTCHA API to 
Android. The move is to protect 
Android users from spam and abuse, 
as well as countermoves by its 
competitors in the smartphone space, 
which include Apple and Microsoft.
“With this API, reCAPTCHA can 
better tell human and bots apart to 
provide a streamlined user experience 
on the mobile,” said Wei Liu, product 
manager for reCAPTCHA, Google, in 
a blog post.
Available as a part of Google 
Play Services, the reCAPTCHA 
Android API comes along with 
Google SafetyNet that provides 
services 
such as 
device 
attestation 
and safe 
browsing to protect mobile apps. 
The combination enables developers 
to perform both the device and user 
attestations using the same API.
Carnegie Mellon alumni Luis 
von Ahn, Ben Maurer, Colin 
McMillen, David Abraham and 
Manuel Blum built reCAPTCH as 
a CAPTCHA-like system back in 
2007. The development was acquired 
by Google two years after its 
inception, in September 2009. The 
solution has benefited more than a 
billion users so far.
Google is not just limiting the 
access of reCAPTCHA to Android 
but also plans to bring it to Apple’s 
iOS as well. It would be interesting 
to see how it is adopted by iOS 
developers. Meanwhile, you can use 
the reCAPTCHA Android Library 
under SafetyNet APIS to integrate 
the secured solution into your 
Android apps.
For more news, visit www.opensourceforu.com
12 | JuLY 2017 | OPEN SOuRCE FOR YOu | www.OpenSourceForu.com

CODE
SPORT
Sandya Mannarswamy
www.OpenSourceForU.com | OPEN SOURCE FOR YOU | JULY 2017 | 13
A
s requested by a few of our readers, we 
decided to feature practice questions for 
computer science college students, for their 
upcoming on-campus and off-campus interviews.  
One small piece of advice to those preparing for 
interviews:  Make sure that you are very clear 
about the fundamental concepts. Interviewers will 
not hold it against you if you don’t know a specific 
algorithm. But not being able to answer some of 
the key fundamental concepts in different areas of 
computer science would definitely be an obstacle 
to clearing interviews. Also, make sure that you 
have updated your GitHub and SourceForge pages 
on which you might have shared some of your 
coding projects. Contributing to open source and 
having interesting, even if small, coding projects 
online in your GitHub pages would definitely be a 
big plus. In this month’s column, we will discuss 
a medley of interview questions covering a wide 
range of topics such as algorithms, operating 
systems, computer architecture, machine 
learning and data science. 
1. Given the advances in memory-driven 
computing, let us assume that you have 
been given an enterprise Class 16 core 
server with a huge 2TB RAM. You have 
been asked to design an intrusion detection 
system which can analyse the network logs 
of different systems and identify potential 
intrusion candidates.  Note that you have to 
monitor close to 256 systems and analyse 
their network logs for potential issues. Let us 
also assume that you have been told which 
standard state-of-art approach (for intrusion 
detection) you would need to implement. 
Now the question is: Would you design the 
intrusion detection application as a scale up 
or a scale out application?  Can you justify 
your choice? Would your choice change if, 
instead of the enterprise Class 16 server with 
2TB RAM, you are given multiple standard 
workstations with 32GB RAM. 
2.  You have been asked to build a supervised 
binary classification system for detecting 
spam emails. Basically, given an email, your 
system should be able to classify it as spam 
or non-spam. You are given a training data set 
containing 400,000 rows and 10,000 columns 
(features). The 10,000 features include both 
numerical variables and categorical variables. 
Now, you need to create a model using this 
training data set. Consider the two options 
given in Question 1 above, namely a server 
with 2TB RAM or a simple workstation with 
32GB RAM.  For each of these two options, 
can you explain how your approach would 
change in training a model?  (Clue: The 
workstation is memory constrained, hence 
you will need to figure out ways to reduce 
your data set. Think about dimensionality 
reduction, sampling, etc.)  Now, assume 
that you are able to successfully create two 
different models — one for the enterprise 
server and another for the workstation. Which 
of these two models would do better on a 
held-out test data set? Is it always the case 
that the model you built for the enterprise 
class server with huge RAM would have a 
much lower test error than the model you built 
for the workstation?  Explain the rationale 
behind your answer.  Here is a tougher 
bonus question:  For this specific computer 
system which has a huge physical memory, 
do you really need paging mechanisms in 
the operating system? If you are told that 
all your computer systems would have such 
huge physical memory, would you be able 
to completely eliminate the virtual memory 
In this month’s column, we discuss some computer science 
interview questions. 

CodeSport
Guest Column
14 | JULY 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com
By: Sandya Mannarswamy
The author is an expert in systems software and is currently 
working as a research scientist at Conduent Labs India 
(formerly, Xerox India Research Centre). Her interests include 
compilers, programming languages, file systems and natural 
language processing. If you are preparing for systems software 
interviews, you may find it useful to visit Sandya’s  LinkedIn 
group Computer Science Interview Training India at http://www.
linkedin.com/groups?home=HYPERLINK “http://www.linkedin.
com/groups?home=&gid=2339182”&HYPERLINK “http://www.
linkedin.com/groups?home=&gid=2339182”gid=2339182
(VM) sub-system from the operating system?  If you 
cannot completely eliminate the VM sub-system, what 
are the changes in paging algorithms (both data structures 
and page replacement algorithms) that you would like 
to implement so that the cost of the virtual memory sub-
system is minimised for this particular server? 
3. You were asked to create a binary classification system 
using the supervised learning approach, to analyse 
X-ray images when detecting lung cancer.  You have 
implemented two different approaches and would like 
to evaluate both to see which performs better.  You find 
that Approach 1 has an accuracy of 97 per cent and 
Approach 2 has an accuracy of 98 per cent.  Given that, 
would you choose Approach 2 over Approach 1? Would 
accuracy alone be a sufficient criterion in your selection? 
(Clue: Note that the cancer image data set is typically 
imbalanced – most of the images are non-cancerous and 
only a few would be cancerous.)
4. Now consider Question 3 again.  Instead of getting 
accuracies of 98 per cent and 97, both your approaches have 
a pretty low accuracy of 59 per cent and 65 per cent. While 
you are debating whether to implement a totally different 
approach, your friend suggests that you use an ensemble 
classifier wherein you can combine the two classifiers you 
have already built and generate a prediction.  Would the 
combined ensemble classifier be better in performance 
than either of the classifiers?  If yes, explain why. If not, 
explain why it may perform poorly compared to either 
of the two classifiers. When is an ensemble classifier a 
good choice to try out when individual classifiers are not 
able to reach the desired accuracy? 
5. You have built a classification model for email spam such 
that you are able to reach a training error which is very 
close to zero. Your classification model was a deep neural 
network with five hidden layers. However, you find that 
the validation error is not small but around 25. What is 
the problem with your model?
6. Regularisation techniques are widely used to avoid 
overfitting of the model parameters to training data so 
that the constructed network can still generalise well for 
unseen test data.  Ridge regression and Lasso regression 
are two of the popular regularisation techniques. When 
would you choose to use ridge regression and when 
would you use Lasso? Specifically, you are given a data 
set, in which you know that out of 100 features, only 
10 to 15 of them are significant for prediction, based on 
your domain knowledge. In that case, would you choose 
ridge regression over Lasso?
7. Now let us move away from machine learning and 
data science to computer architecture. You are given a 
computer system with 16 cores, 64GB RAM, 512KB 
per core L1 cache (both instruction and data caches are 
separate and each is of size 512KB), 2MB per core L2 
cache, and 1GB L3 cache which is shared among all 
cores. L1 cache is direct-mapped whereas L2 and L3 
caches are four-way set associative. Now you are told to 
run multiple MySQL server instances on this machine, 
so that you can use this machine as a common back-
end system for multiple Web applications.  Can you 
characterise the different types of cache misses that would 
be encountered by the MySQL instances? 
8. Now consider Question 7 above, where you are not given 
the cache sizes of L1, L2 and L3, but have been asked to 
figure out the approximate sizes of L1, L2 and L3 caches. 
You are given the average cache access times (hit and 
miss penalty for each access) for L1, L2 and L3 caches. 
Other system parameters are the same. Can you write a 
small code snippet which can determine the approximate 
sizes of L1, L2 and L3 caches?  Now, is it possible to 
figure out the cache sizes if you are not aware of the 
average cache access times? 
9. Consider that you have written two different versions of 
a multi-threaded application. One version extensively 
uses linked lists and the second version uses mainly 
arrays. You know that the memory accesses performed 
by your application do not exhibit good locality due to 
their inherent nature.  You also know that your memory 
accesses are around 75 per cent reads.  Now you have 
the choice between two computer systems — one with 
an L3 cache size of 24MB shared across all eight cores, 
and another with an L3 cache size of 2MB independent 
for each of the eight cores.  Are there any specific reasons 
why you would choose System 1 rather than System 2 for 
each of your application versions? 
10. Can you explain the stochastic gradient descent 
algorithm? When would you prefer to use the co-
ordinate gradient descent algorithm instead of the regular 
stochastic gradient algorithm? Here’s a related question. 
You are told that the cost function you are trying to 
optimise is non-convex. Would you still be able to use the 
co-ordinate descent algorithm?
If you have any favourite programming questions/software 
topics that you would like to discuss on this forum, please 
send them to me, along with your solutions and feedback, at 
sandyasm_AT_yahoo_DOT_com. Till we meet again next 
month, wishing all our readers a wonderful and productive 
month ahead!  


German audio company Sennheiser has 
unveiled a pair of UC-certified headsets 
that employ adaptive active noise 
cancellation (AANC), called the MB 
660 UC and MB 660 UC MS.
AANC muffles the background 
sounds to a certain extent offering a 
more pleasant user experience.
The headphones offer stereo audio 
and CD quality streaming with aptX. 
They come with Sennheiser’s unique 
SpeakFocus technology, delivering crystal 
clear sound even in noisy offices or play 
areas. The headphones are designed with 
a touchpad to access controls like answer/
end call, reject, hold, mute, redial, change 
track, volume, etc. The pillow soft leather 
ear pads and ear shaped cups are designed 
with premium quality material to ensure 
maximum comfort and style.
The MB 660 UC series provides up 
new products
Headphones with active noise 
cancellation from Sennheiser
Connect between the digital 
and analogue worlds with 
Electropen 4 
Sony has launched its wireless 
Bluetooth headset, the NW-WS623, 
a model specially designed for sports 
enthusiasts. Weighing just 32 grams, 
the NW-WS623 has a battery life of 
up to 12 hours once fully charged. As 
per company claims, the device comes 
with the quick charge capability, 
charging the device for one hour of 
playback in just three minutes.
Enabled with NFC and Bluetooth, 
the device allows users to connect their 
headset to their smartphones or any 
other device. The NW-WS623 comes 
with 4GB built-in storage.
It has IP65/IP68 certification for 
water and dust resistance, making 
it suitable for use in outdoor sports 
activities. It can also endure extreme 
temperatures from -5 degrees to 45 
degrees Celsius, allowing it to be used 
while hiking or climbing at high and 
very cold altitudes.
The device features an ergonomic, 
slim and light design, and comes with 
the ambient sound mode, enabling 
users to hear external sounds without 
having to take off the headset.
The Sony NW-WS623 is available 
in black colour via Sony Centres and 
other electronics retail stores.
to 30 hours of battery back-up. The two 
variants – MB660 UC and MB660 UC 
MS – are available via any authorised 
Sennheiser distribution channel.
Address: Sennheiser Electronics 
India Pvt Ltd, 104, A, B, C First Floor, 
Time Tower, M.G. Road, Sector 28, 
Gurugram, Haryana – 122002
Wireless Bluetooth 
headset for sports 
enthusiasts
Portable devices manufacturer, 
Portronics, has launched its latest digital 
pen, the Electropen 4, which allows 
users to write notes or make sketches 
on paper and then convert these into 
a document or JPEG image on their 
smartphone, laptop or computer. 
The digital pen has an in-built Flash 
memory to store A4 sized pages and 
later sync or transfer these to the 
smartphone or tablet, eliminating the 
need to carry a mobile device or laptop 
to meetings for taking notes.
In addition, the Bluetooth digital pen 
uses the microphone of the smartphone 
via an app that allows users to record 
an audio clip while they take notes. The 
user can also erase or edit the notes taken 
Address: Portronics Digital Pvt Ltd,  
4E/14 Azad Bhavan, Jhandewalan,  
New Delhi – 110055; Ph: 91-9555245245
with or sketches made using Electropen 
4. The digital pen is compatible with 
Bluetooth 4.1 and is powered by a Li-
polymer rechargeable battery.
The Portronics Electropen 4 is 
available in black colour via online 
and retail stores.
Price:  
 ` 6,999
Address: Sony India Pvt Ltd, A-18, 
Mathura Road, Delhi – 110044;  
Ph: 91-11-66006600
Price:  
 ` 8,990 
Price:  
 ` 41,990 for MB 660 
UC and  ` 60,094 for 
MB 660 UC MS
16 | JuLY 2017 | OPEN SOuRCE FOR YOu | www.OpenSourceForu.com

offers two 5MP cameras – front and 
rear, both with flash.
The 4G VoLTE smartphone is 
available with a gold back cover and an 
additional back cover via mobile stores.
The prices, features and specifications are based on information provided to us, or as available on various 
websites and portals. OSFY cannot vouch for their accuracy. 
Samsung launches its new Tizen smartphone
Compiled by: Aashima Sharma
Enjoy loud, clear and uniform sound 
with Bose’s 360 degree speakers
Samsung has recently unveiled its 
latest smartphone, the Samsung Z4, 
with the Tizen OS.
The device is equipped with 
an 11.43cm (4.5 inch) screen, 
allowing users to enjoy games and 
other multimedia with a superior 
resolution.
Designed with 2.5D glass, the 
phone has a comfortable grip. It 
comes with 1.5 quad-core processor, 
1GB RAM and 8GB internal 
memory, which is further expandable 
up to 128GB via microSD card. It 
is backed with a 2050mAh battery 
boosted by Samsung’s unique ‘Ultra 
Power Saving Mode’.
According to company sources, 
the Tizen OS gives users an effortless 
mobile experience as it comes with 
features developed specifically for 
Indian consumers. The smartphone 
Price:  
 ` 5,790 
Address: Samsung India Electronics 
Pvt Ltd, 20th to 24th Floors, Two 
Horizon Centre, Golf Course Road, 
Sector-43, DLF PH-V, Gurugram, 
Haryana 122202
Address: Bose Corporation India 
Pvt Ltd, 3rd Floor, Salcon Aurum, 
Plot No. 4, Jasola District Centre, 
New Delhi – 110025
International audio equipment 
manufacturer, Bose, has launched its 
latest mid-range Bluetooth speakers 
– SoundLink Revolve and SoundLink 
Revolve +. The speakers are designed 
to offer clear sound in all directions, 
giving uniform coverage.
The aluminium covered cylindrical 
speakers feature a ¼-20 thread to make 
it easy to mount them on a tripod, in 
yards or outdoors. As they are IPX4 
rated, they can withstand spills, rain 
and pool splashes.
The SoundLink Revolve measures 
15.24cm (6 inch) in height, 8.25cm (3.25 
inch) in depth and weighs 680 grams. 
The SoundLink Revolve + is bigger at 
18.41cm (7.25 inch) in height, 8.25cm 
(3.25 inch) in depth and weighs 907 
grams. The Revolve + promises up to 
16 hours of playback, while the Revolve 
offers up to 12 hours of play time.
Both the variants can be charged via 
micro-B USB port and can be paired via 
NFC. Integrated microphones can also 
be used as a speakerphone or with Siri 
and Google Assistant. The speakers are 
available in Triple Black and Lux Gray 
colours via Bose stores.
Price:  
 ` 19,900 for SoundLink 
Revolve and  ` 24,500 
for SoundLink Revolve +
www.OpenSourceForu.com | OPEN SOuRCE FOR YOu | JuLY 2017 | 17

Guest Column
Exploring Software
18 | JUly 2017 | OPEN SOURCE FOR yOU | www.OpenSourceForU.com
import MySQLdb
SQL_INSERT=”INSERT INTO navtable(isin,nav) VALUES(‘%s’,%s)”
db = MySQLdb.connect(“localhost”,”anil”,”secretpw”,”navdb”)
c = db.cursor()
try:
    for line in open(‘navall.txt’).readlines():
        cols=line.split(‘;’)
        if len(cols) > 4 and len(cols[1])==12 and not 
cols[4][0].isalpha():
            c.execute( SQL_INSERT%(cols[1],cols[4]) )
    db.commit()
    db.close()
except:
    db.rollback()
Linking the MySQL database and 
LibreOffice Base
In addition to LibreOffice Base, you will need to install the 
MySQL JDBC driver. On Fedora, the package you need is 
mysql-connector-java. 
LibreOffice needs to be able to find this driver. For 
Fedora, add the archive /usr/share/java/mysql-connector-
java.jar in the class path of Java Options. You can find this 
option in Tools/Options/LibreOffice/Advanced.
The next step is to enable this as a data source in 
LibreOffice, as follows. 
1. Create a new database.
2. Connect to an existing database using 
MySQL JDBC connection.
3. The database is navdb on localhost.
4. Enter the credentials to access the database.
5. You will be prompted to register the database with 
LibreOffice.
6. You will be prompted for the password.
You may create additional tables, views, and SQL queries 
on the database, which may then be accessed through Calc.
Access the data through Calc
Your personal table of your funds will have at least the 
This article demonstrates how using LibreOffice Calc with Base 
and a Structured Query Language (SQL) database like MariaDB 
or MySQL can help anyone who wishes to keep track of mutual 
fund investments. The convenience afforded is amazing!
Connecting LibreOffice Calc 
with Base and MariaDB/MySQL
L
et’s assume that you have a list of mutual funds that 
you can easily manage in a spreadsheet. You will 
need to check the NAV (net asset value) of the funds 
occasionally to keep track of your assets. You can find out 
the NAV of all the mutual funds (MFs) you have invested in 
at one convenient spot—the website of the Association of 
Mutual Funds,  amfiindia.com.
As time goes by, your list of funds keeps increasing 
and at some point you will want to automate the process 
of keeping track of them. The first step is to download the 
NAV for all the funds in a single text file; you may use wget 
to do so:
$ wget http://www.amfiindia.com/spages/NAVAll.txt
The downloaded file is a text file with fields separated 
by ‘;’. It can easily be loaded into a spreadsheet and 
examined.
Creating a usable format
Each mutual fund can be identified by an International 
Securities Identification Number (ISIN). Hence, ISIN (the 
second column) can be the key to organise the data. The 
minimum additional data you need is the NAV for the day 
(the fifth column) for each security.
LibreOffice has a Base module, which I had never used 
and which works well with any SQL database using ODBC/
JDBC drivers. If you are using Linux, you may already 
have installed MariaDB or MySQL. Python will easily 
work with MySQL as well.
So, you create a MySQL database NAVDB 
and a table called navtable:
CREATE TABLE navtable (isin char(12) primary key, nav 
real); 
The following code is a minimal Python program to 
load the data into a MySQL table while skipping all the 
extraneous rows and rows for which no NAV is available.
Anil Seth

Guest Column
Exploring Software
www.OpenSourceForU.com | OPEN SOURCE FOR yOU | JUly 2017 | 19
By: Dr Anil Seth
The author has earned the right to do what interests him. You 
can find him online at http://sethanil.com, http://sethanil.
blogspot.com, and reach him via email at anil@sethanil.com.
following columns:
ISIN
No. of 
units
NAV
Current value
INF189A01038
500
To get from 
navtable
 = <A2>*<B2>
Ideally, Calc should have had a function to select the 
NAV from the navtable using the ISIN value as the key. 
Unfortunately, that is not possible.
The simplest option appears to be to use a Pivot Table. 
So, after opening your spreadsheet, take the following steps: 
1.  Insert the Pivot Table.
2.  Select Data Source registered with LibreOffice.
3. Choose NAVDB that you had registered above and the 
table navdb.navtable.
4.  After a while, you will be asked to select the fields for the 
Pivot Table layout. Select both isin and nav as row fields.
It will create a new sheet with the values. For the sake of 
simplicity, rename the sheet as ‘Pivot’.
Now, go back to your primary sheet and define a 
lookup function in the NAV column. For example, for the 
second row, it will be:
=VLOOKUP(A2, Pivot.A:B, 2)
This function will search for the value in A2 in the first 
column of the  A and B columns of the Pivot sheet.
The process of using Base turned out to be a lot more 
complex than I had anticipated. However, the same complexity 
also suggests that using a spreadsheet as a front-end to a 
database is a very attractive option for decision taking, 
especially knowing how often one suffers trying to understand 
the unexpected results only to find that a value in some cell had 
been accidentally modified! 

Admin How To
G
raphite is a monitoring tool that was written by Chris 
Davis in 2006. It has been released under the Apache 
2.0 licence and comprises three components: 
1. Graphite-Web 
2. Carbon 
3. Whisper 
Graphite-Web is a Django application and provides a 
dashboard for monitoring. Carbon is a server that listens 
to time-series data, while Whisper is a database library for 
storing the data. 
Setting it up 
A CentOS 6.8 virtual machine (VM) running on KVM is used 
for the installation. Please make sure that the VM has access 
to the Internet. The Ansible version used on the host (Parabola 
GNU/Linux-libre x86_64) is 2.2.1.0. The ansible/ folder 
contains the following files: 
ansible/inventory/kvm/inventory
ansible/playbooks/configuration/graphite.yml
ansible/playbooks/admin/uninstall-graphite.yml
The IP address of the guest CentOS 6.8 VM is added to 
the inventory file as shown below: 
graphite ansible_host=192.168.122.120 ansible_connection=ssh 
ansible_user=root ansible_password=password
Also, add an entry for the graphite host in the /etc/hosts 
file as indicated below: 
192.168.122.120 graphite
Graphite 
The playbook to install the Graphite server is given below: 
---
- name: Install Graphite software
  hosts: graphite
  gather_facts: true
  tags: [graphite]
  tasks:
    - name: Import EPEL GPG key
      rpm_key:
        key: http://dl.fedoraproject.org/pub/epel/RPM-GPG-
KEY-EPEL-6
        state: present
    - name: Add YUM repo
      yum_repository:
        name: epel
        description: EPEL YUM repo
        baseurl: https://dl.fedoraproject.org/pub/
epel/$releasever/$basearch/
        gpgcheck: yes
20 | JuLY 2017 | OPEN SOuRCE FOR YOu | www.OpenSourceForu.com
The DevOps Series  
Deploying Graphite Using Ansible
In this fifth article in the DevOps series we will learn to install 
and set up Graphite using Ansible. 

Admin
How To
    - name: Update the software package repository
      yum:
        name: ‘*’
        update_cache: yes
    - name: Install Graphite server
      package:
        name: “{{ item }}”
        state: latest
      with_items:
        - graphite-web
We first import the keys for the Extra Packages for 
Enterprise Linux (EPEL) repository and update the software 
package list. The ‘graphite-web’ package is then installed 
using Yum. The above playbook can be invoked using the 
following command: 
$ ansible-playbook -i inventory/kvm/inventory playbooks/
configuration/graphite.yml --tags “graphite”
MySQL
A backend database is required by Graphite. By default, the 
SQLite3 database is used, but we will install and use MySQL 
as shown below: 
- name: Install MySQL
  hosts: graphite
  become: yes
  become_method: sudo
  gather_facts: true
  tags: [database]
  tasks:
    - name: Install database
      package:
        name: “{{ item }}”
        state: latest
      with_items:
        - mysql
        - mysql-server
        - MySQL-python
        - libselinux-python
    - name: Start mysqld server
      service:
        name: mysqld
        state: started
    - wait_for:
        port: 3306
    - name: Create graphite database user
      mysql_user:
        name: graphite
        password: graphite123
        priv: ‘*.*:ALL,GRANT’
        state: present
    - name: Create a database
      mysql_db:
        name: graphite
        state: present
    - name: Update database configuration
      blockinfile:
        path: /etc/graphite-web/local_settings.py
        block: |
          DATABASES = {
            ‘default’: {
            ‘NAME’: ‘graphite’,
            ‘ENGINE’: ‘django.db.backends.mysql’,
            ‘USER’: ‘graphite’,
            ‘PASSWORD’: ‘graphite123’,
           }
          }
    - name: syncdb
      shell: /usr/lib/python2.6/site-packages/graphite/
manage.py syncdb --noinput
    - name: Allow port 80
      shell: iptables -I INPUT -p tcp --dport 80 -m state 
--state NEW,ESTABLISHED -j ACCEPT
    - name:
      lineinfile:
        path: /etc/httpd/conf.d/graphite-web.conf
        insertafter: ‘           # Apache 2.2’
        line: ‘           Allow from all’
    - name: Start httpd server
      service:
        name: httpd
        state: started
As a first step, let’s install the required MySQL dependency 
packages and the server itself. We can then start the server and 
wait for it to listen on port 3306. A graphite user and database 
is created for use with the Graphite Web application. For this 
example, the password is provided as plain text. In production, 
use an encrypted Ansible Vault password. 
The database configuration file is then updated to use the 
MySQL credentials. Since Graphite is a Django application, 
the manage.py script with syncdb needs to be executed to 
create the necessary tables. We then allow port 80 through 
the firewall in order to view the Graphite dashboard. The 
graphite-web.conf file is updated to allow read access, and the 
www.OpenSourceForu.com | OPEN SOuRCE FOR YOu | JuLY 2017 | 21

22 | JuLY 2017 | OPEN SOuRCE FOR YOu | www.OpenSourceForu.com
Admin How To
Apache Web server is started. 
The above playbook can be invoked as follows: 
$ ansible-playbook -i inventory/kvm/inventory playbooks/
configuration/graphite.yml --tags “database”
Carbon and Whisper 
The Carbon and Whisper Python bindings need to be installed 
before starting the carbon-cache script. 
- name: Install Carbon and Whisper
  hosts: graphite
  become: yes
  become_method: sudo
  gather_facts: true
  tags: [carbon]
  tasks:
    - name: Install carbon and whisper
      package:
        name: “{{ item }}”
        state: latest
      with_items:
        - python-carbon
        - python-whisper
    - name: Start carbon-cache
      shell: /etc/init.d/carbon-cache start
The above playbook is invoked as follows: 
$ ansible-playbook -i inventory/kvm/inventory playbooks/
configuration/graphite.yml --tags “carbon”
The Graphite dashboard
You can open http://192.168.122.120 in the browser on the 
host to view the Graphite dashboard. A screenshot of the 
Graphite Web application is shown in Figure 1.
Uninstalling Graphite 
An uninstall script to remove the Graphite server and its 
dependency packages is required for administration. The 
Ansible playbook for the same is available in the playbooks/
admin folder and is given below: 
---
- name: Uninstall Graphite and dependencies
  hosts: graphite
  gather_facts: true
  tags: [remove]
  tasks:
    - name: Stop the carbon-cache server
      shell: /etc/init.d/carbon-cache stop
    - name: Uninstall carbon and whisper
      package:
        name: “{{ item }}”
        state: absent
      with_items:
        - python-whisper
        - python-carbon
    - name: Stop httpd server
      service:
        name: httpd
        state: stopped
    - name: Stop mysqld server
      service:
        name: mysqld
        state: stopped
    - name: Uninstall database packages
      package:
        name: “{{ item }}”
        state: absent
      with_items:
        - libselinux-python
        - MySQL-python
        - mysql-server
        - mysql
        - graphite-web
The script can be invoked as follows: 
$ ansible-playbook -i inventory/kvm/inventory playbooks/
admin/uninstall-graphite.yml                         
By: Shakthi Kannan
The author is a free software enthusiast and blogs at 
shakthimaan.com.
[1] Graphite documentation. https://graphite.readthedocs.
io/en/latest/ 
[2]  Carbon. https://github.com/graphite-project/carbon 
[3]  Whisper database. http://graphite.readthedocs.io/en/
latest/whisper.html
References
 Figure 1: Graphite Web 

www.OpenSourceForu.com | OPEN SOuRCE FOR YOu | JuLY 2017 | 23
Admin
How To
T
his article will guide you on installing OpenStack on 
top of AWS EC2. Installing OpenStack on a nested 
hypervisor environment is not a big deal when a 
QEMU emulator is used for launching virtual machines 
inside the virtual machine (VM). However, unlike the usual 
nested hypervisor set-up, installing OpenStack on AWS EC2 
instances has a few restrictions on the networking side, for 
the OpenStack set-up to work properly. This article outlines 
the limitations and the solutions to run OpenStack on top of 
the AWS EC2 VM.
Limitations
The AWS environment will allow the packets to flow in their 
network only when the MAC address is known or registered 
in the AWS network environment. Also, the MAC address and 
the IP address are tightly mapped. So, the AWS environment 
will not allow packet flow if the MAC address registered for 
the given IP address is different.
You may wonder whether the above restrictions will 
impact the OpenStack set-up on AWS EC2.
Yes, they certainly will! While configuring Neutron 
networking, we create a virtual bridge (say, br-ex) for 
the provider network, where all the VM’s traffic will 
reach the Internet via the external bridge, followed by the 
actual physical NIC (say, eth1). In that case, we usually 
configure the external interface (NIC) with a special type of 
configuration, as given below.
The provider interface uses a special configuration without 
an IP address assigned to it. Configure the second interface as 
the provider interface. Replace INTERFACE_NAME with the 
actual interface name, for example, eth1 or ens224.
Next, edit the /etc/network/interfaces file to 
contain the following:
# The provider network interface
auto INTERFACE_NAME
iface INTERFACE_NAME inet manual
up ip link set dev $IFACE up
down ip link set dev $IFACE down
You can go to  http://docs.openstack.org/mitaka/install-
guide-ubuntu/environment-networking-controller.html for 
more details.
Due to this special type of interface configuration, the 
restriction in AWS will hit OpenStack networking. In the 
mainstream OpenStack set-up, the above-mentioned provider 
OpenStack is a cloud operating system that controls vast computing 
resources through a data centre, while AWS (Amazon Web services) offers 
reliable, scalable and inexpensive cloud computing services. The installation of 
OpenStack on AWS is an instance of Cloud-on-a-Cloud.  
A Guide to Running 
OpenStack on AWS

24 | JuLY 2017 | OPEN SOuRCE FOR YOu | www.OpenSourceForu.com
Admin How To
interface is configured with a special NIC configuration that 
will have no IP for that interface, and will allow all packets 
via that specially configured NIC. Moreover, the VM packets 
reaching the Internet via this specially configured NIC will 
have the OpenStack tenant router’s gateway IP address as the 
source in each packet.
As I mentioned earlier, in the limitations above, AWS will 
only allow the packet flow when the MAC address is known/
registered in its environment. Also, the IP address must match 
the MAC address. 
In our case, the packet from the above-mentioned 
OpenStack tenant router will have the IP address of the 
router’s gateway in every single packet, and the packet 
source’s MAC address will be the MAC address of the 
router’s interface.
 Note:  These details are available if you use ip netns 
show followed by ip netns exec qr-<router_ID> ifconfig 
commands in the OpenStack controller’s terminal. Since 
the MAC address is unknown or not registered in the AWS 
environment, the packets will be dropped when they reach 
the AWS switch. To allow the VM packets to reach the 
Internet via the AWS switch, we need to use some tricks/
hacks in our OpenStack set-up.
Making use of what we have
One possible way is to register the router’s MAC address 
and its IP address with the AWS environment. However, this 
is not feasible. As of now, AWS does not have the feature of 
registering any random MAC address and IP address inside 
the VPC. Moreover, allowing this type of functionality will be 
a severe security threat to the environment.
The other method is to make use of what we have. Since 
we have used a special type of interface configuration for 
the provider NIC, you will note that the IP address assigned 
to the provider NIC (say, eth1) is left unused. We could use 
this available/unused IP address for the OpenStack router’s 
gateway. The command given below will do the trick:
neutron router-gateway-set router provider --fixed-ip ip_
address=<Registered_IP_address*>
IP address and MAC address mismatches
After configuring the router gateway with the AWS registered 
IP address, each packet from the router’s gateway will have 
the AWS registered IP address as the source IP address, but 
with the unregistered MAC address generated by the OVS. As 
I mentioned  earlier, while discussing the limitations of AWS, 
the IP address must match the MAC address registered; else, 
all the packets with the mismatched MAC and IP address will 
be dropped by the AWS switch. To make the registered MAC 
address match with the IP address, we need to change the 
MAC address of the router’s interface. 
The following steps will do the magic.
Step 1) Install the macchanger tool.
Step 2) Note down the actual/original MAC address of 
the provider NIC (eth1).
Step 3) Change the MAC address of the 
provider NIC (eth1).
Step 4) Change the MAC address of the router’s 
gateway interface to the original MAC address of eth1.
Step 5) Now, try to ping 8.8.8.8 from the router 
namespace. 
If you get a successful ping response, then we are done 
with the Cloud-on-the-Cloud set-up.
Key points to remember
Here are some important things that one needs to 
keep track of.
Changing the MAC address: In my case, I had used 
the Ubuntu 14.04 LTS server, with which there was no 
issue in changing the MAC address using the macchanger 
tool. However, when I tried the Ubuntu 16.04 LTS, I 
got an error saying, “No permission to modify the MAC 
address.” I suspect the error was due to the cloud-init 
tool not allowing the MAC address’ configuration. So, 
before setting up OpenStack, try changing the MAC 
address of the NIC.
Floating IP disabled: Associating a floating IP to any 
of OpenStack’s VMs will send the packet via the router’s 
gateway with the source IP address as the floating IP’s IP 
address. This will make the packets hit the AWS switch 
with a non-registered IP and MAC address, which results 
in the packets being dropped. So, I could not use the 
floating IP functionality in this set-up. However, I could 
access the VM publicly using the following NAT process.
Using NAT to access OpenStack’s VM: As mentioned 
earlier, I could access the OpenStack VM publicly using 
the registered IP address that was assigned to the router’s 
gateway. Use the following NAT command to access the 
OpenStack VM using the AWS EC2 instance’s elastic IP:
$ ip netns exec qrouter-f85bxxxx-61b2-xxxx-xxxx-xxxxba0xxxx 
iptables -t nat -A PREROUTING -p tcp -d 172.16.20.101 
--dport 522 -j DNAT --to-destination 192.168.20.5:22
  Note: In the above command, I had NAT forwarding 
for all packets for 172.16.20.101 with Port 522. Using 
the above NAT command, all the packets reaching 
172.16.20.101 with Port number 522 are forwarded to 
192.168.20.5:22. 
 Here, 172.16.20.101 is the registered IP address of the 
AWS EC2 instance which was assigned to the router’s 
gateway. 192.168.20.5 is the local IP of the OpenStack 
VM. Notably, 172.16.20.101 already has NAT with the 
AWS elastic IP, which means all the traffic that comes to 
the elastic IP (public IP) will be forwarded to this VPC 

www.OpenSourceForu.com | OPEN SOuRCE FOR YOu | JuLY 2017 | 25
Admin
How To
local IP (172.16.20.101).
In short, [Elastic IP]:522 π 172.16.20.101:522  π 
192.168.20.5:22
This means that you can SSH the OpenStack VM 
globally by using the elastic IP address and the respective 
port number.
Elastic IP address: For this type of customised 
OpenStack installation, we require at least two NICs for an 
AWS EC2 instance. One is for accessing the VM terminal 
for installing and accessing the dashboard. In short, it acts 
as a management network, a VM tunnel network or an API 
network. The last one is for an external network with a 
unique type of interface configuration and is mapped with 
the provider network bridge (say br-ex with eth1).
AWS will not allow any packets to travel out of the 
VPC unless the elastic IP is attached to that IP address. To 
overcome this problem, we must attach the elastic IP for 
this NIC. This is so that the packets of the OpenStack’s VM 
reach the OpenStack router’s gateway and from the gateway, 
the packets get embedded with the registered MAC address. 
Then, the matching IP address will reach the AWS switch 
(VPC environment) via br-ex and eth1 (a special type of 
interface configuration), and then hit the AWS actual VPC 
gateway. From there, the packets will reach the Internet.
Other cloud platforms
In my analysis, I noticed that most of the cloud providers 
like Dreamhost and Auro-cloud have the same limitations 
for OpenStack networking.  So we could use the tricks/hacks 
mentioned above in any of those cloud providers to run an 
OpenStack cloud on top of them.
  Note:  Since we are using the QEMU emulator without 
KVM for the nested hypervisor environment, the VM’s 
performance will be slow.
If you want to try OpenStack on AWS, you can register 
for CloudEnabler’s Cloud Lab-as-a-Service offering available 
at https://claas.corestack.io/ which provides a consistent and 
on-demand lab environment for OpenStack in AWS. 
By: Vinoth Kumar Selvaraj 
The author is a DevOps engineer at Cloudenablers Inc. You 
can visit his LinkedIn page at https://www.linkedin.com/
in/vinothkumarselvaraj/. He has a website at http://www.
hellovinoth.com and you can tweet to him @vinoth6664.
[1] http://www.hellovinoth.com/ 
[2] http://www.cloudenablers.com/blog/ 
[3] https://www.opendaylight.org/ 
[4] http://docs.openstack.org/ R
References

Keith Bergelt, CEO of Open Invention Network
The Open Invention Network was set 
up in 2005 by IBM, Novell, Philips, Red 
Hat and Sony. As open source 
software became the driving force 
for free and collaborative innovation 
over the past decades, Linux and 
associated open source applications 
faced increased litigation attacks from 
those who benefited from a patent 
regime. The OIN effectively plays the 
role of protecting those working in 
Linux. It is the largest patent non-
aggression community—with more 
than 2,200 corporate participants and 
financial supporters such as Google, 
NEC and Toyota. But how exactly 
does it protect and support liberty for 
all open source adopters? Jagmeet 
Singh of OSFY finds the answer to 
that in an exclusive conversation 
with Keith Bergelt, CEO of Open 
Invention Network. Edited excerpts...
Q
What does the Open Invention 
Network have for the open source 
community?
The Open Invention Network (OIN) is a 
collaborative enterprise that enables innovation in 
Linux and other open source projects by leveraging 
a portfolio of more than 1,200 strategic, worldwide 
patents and applications. That is paired with our 
unique, royalty-free licence agreement.
While our patent portfolio is worth a great 
deal - in the order of hundreds of millions of 
dollars - any person or company can gain access 
to OIN’s patents for free, as long as they agree 
not to sue anyone in the collection of software 
packages that we call the ‘Linux System’. A 
new licensee also receives royalty-free access 
to the Linux System patents of the 2,200 other 
licensees. In essence, a licensee is agreeing to 
For U & Me
Interview
26 | July 2017 | OPEN SOuRCE FOR yOu | www.OpenSourceForu.com
Interview

support patent non-aggression in Linux. 
Even if an organisation has no patents, 
by taking an OIN license, it gains 
access to the OIN patent portfolio and 
an unrestricted field of use.
Q
How has OIN grown since 
its inception in November 
2005?
Since the launch of OIN, Linux and 
open source have become more widely 
used and are incredibly prevalent. The 
average person today has an almost 
countless number of touch points 
with Linux and open source software 
on a daily basis. Search engines, 
smartphones, cloud computing, financial 
and banking networks, mobile networks 
and automobiles, among many other 
industries, are all leveraging Linux and 
open source.
As Linux development, distribution 
and usage have grown -- so too has 
the desire of leading businesses and 
organisations to join OIN. In addition 
to the benefit of gaining access to 
patents worth hundreds of millions of 
dollars, becoming an OIN community 
member publicly demonstrates an 
organisation’s commitment to Linux 
and open source. These factors have 
led OIN to grow at a very significant 
pace over the last five years.
We are now seeing new community 
members from across all geographic 
regions as well as new industries and 
platforms that include vehicles, NFV 
(network function virtualisation) 
telecommunications services, Internet of 
Things, blockchain digital ledgers and 
embedded parts.
Q
How did you manage to build 
a patent ‘no-fly’ zone around 
Linux?
At the heart of OIN is its powerful cross-
licence. Any organisation, business or 
individual can join OIN and get access to 
its patents without paying a royalty even 
if the organisation has no patents. The key 
stipulation in joining is that community 
members agree not to sue anyone based 
on a catalogue of software libraries we 
call the Linux System.
Q
What were the major 
challenges you faced 
in building OIN up as the 
largest patent non-aggression 
community?
There is no analogy in the history 
of the technology industry for OIN. 
Where people really struggled was in 
understanding our business model. 
Because we do not charge a fee to join or 
access our patents, it was challenging for 
executives to grasp the fact that OIN was 
created to help all companies dedicated 
to Linux. For the first few years of our 
existence, this aspect created uncertainty 
as it is not common for leaders like IBM, 
Red Hat, SUSE, Google and Toyota 
to come together to contribute capital 
to support a common good with no 
financial return. But instead of seeking 
a financial return, these companies have 
had the foresight to understand that the 
modalities of collaborative invention 
will benefit all those who currently rely 
on, as well as those who will come to 
rely on, open source software. In this 
transformation arise market opportunities 
and a new way of creating value in the 
new economy which is open to all who 
have complementary technology, talent 
and compelling business models.
Q
What practices does OIN 
encourage to eliminate low-
quality patents for open source 
solutions?
Low-quality patents are the foodstuff 
of NPEs (non-practising entities) and 
organisations looking to hinder their 
competitors, because their products are 
not competitive in the marketplace. OIN 
has taken a multi-pronged approach 
towards eliminating low-quality patents. 
We have counselled and applaud the 
efforts of the USPTO (United States 
Patent and Trademark Office), other 
international patent assigning entities 
and various government agencies in 
the EU and Asia that are working to 
create a higher bar for granting a patent. 
Additionally, in the US, Asia and the 
EU, there are pre-patent peer review 
organisations that are working to ensure 
that prior art is well catalogued and 
available for patent examiners. These 
initiatives are increasingly effective. 
Q
What are the key steps an 
organisation should take 
to protect itself from a patent 
infringement claim related to an 
open source deployment?
A good first step is to join the Open 
Invention Network. This gives Linux 
and open source developers, distributors 
and users access to very strategic 
patents owned directly by OIN, and 
cross-licensed with thousands of other 
OIN community members. Another 
significant step is to understand the 
various licences used by the open 
source community. These can be found 
at the Open Source Initiative.
Q
How do resources like 
Linux Defenders help 
organisations to overcome 
patent issues?
We encourage the open source 
community to contribute to defensive 
publications, as that will help to 
improve patent examination and 
therefore will limit the future licensing 
of other patents. Linux Defenders aims 
to submit prior art on certain patents 
while they are at the application stage to 
prevent or restrict the issuance of these 
patents. Finally, we educate the open 
source community on other defensive 
intellectual property strategies that 
can be employed to complement their 
membership in OIN.
For U & Me
Interview
www.OpenSourceForu.com | OPEN SOuRCE FOR yOu | July 2017 | 27

Q
Unlike the proprietary world, 
awareness regarding the 
importance of patents and 
licensing structures is quite low 
in the open source space. How 
does OIN help to enhance such 
knowledge?
We conduct a fairly extensive educational 
programme in addition to spreading 
the word through interviews and 
media coverage. We have speakers that 
participate at numerous open source 
events around the globe. Also, we help to 
educate intellectual property professionals 
through our support of the European 
Legal Network and the creation of the 
Asian Legal Network, including regular 
Asian Legal Network meetings in India.
Q
In what way does OIN help 
developers opting to use open 
source technologies?
We believe that all products and 
services, and in particular those based 
on open source technologies, should be 
able to compete on a level playing field. 
We help ensure this by significantly 
reducing the risk of patent litigation from 
companies that want to hinder new 
open source-based entrants. We also 
provide developers, distributors and 
users of Linux and other open source 
technologies that join OIN with access 
to valuable patents owned by us directly, 
and cross-licensed by our community 
members.
Q
As over 2,200 participants 
have already joined in with the 
founding members, have patent 
threats to Linux reduced now?
There will always be the threat of patent 
litigation by bad actors. OIN was created 
to neutralise patent lawsuit threats 
against Linux – primarily through patent 
non-aggression community building and 
the threat of mutually assured destruction 
in patent litigation.
As the Open Invention Network 
community continues to grow, the amount 
of intellectual property that is available for 
cross-licensing will continue to grow. This 
will ensure that innovation in the core will 
continue, allowing organisations to focus 
Linux and open source.
Q
How do you view the integration 
of open source into major 
Microsoft offerings?
We see all this as recognition by 
Microsoft that its success does not have 
to come solely from proprietary software 
and standards. If it becomes a good open 
source citizen, it can also reap the benefits 
of innovation and cost efficiencies derived 
through the development, distribution and 
use of Linux. We have and will continue 
to cheer Microsoft to become an OIN 
licensee. This would publicly and tangibly 
demonstrate its support and commitment 
to patent non-aggression in Linux and 
open source.
Q
In addition to your present role 
at OIN, you have successfully 
managed IP strategies at various 
tech companies. What is unique 
about developing an IP strategy for 
an open source company?
Over the last few years, as market forces 
have shifted, new legislation has been 
passed, and judicial precedents have been 
set – all technology companies have had 
to review their IP strategies. With the rapid 
adoption of open source by most technology 
vendors, it is becoming increasingly 
challenging to bifurcate organisations as 
open source vs non-open source. 
We believe that today and in the future, 
successful companies — whether they are 
hybrid or pure-play open source companies 
— will take advantage of the benefits of 
shared core technology innovation. They 
will look to invest the majority of their 
resources higher in the technology stack. 
This move will help ensure the continued 
sales of their higher-value products through 
an IP strategy employing patents, defensive 
publications, or both—and by participating 
in an organisation like OIN.
Q
Lastly, could it be an entirely 
patent-free future for open 
source in the coming years?
While software patents are not part of the 
landscape in India, they are part of the 
landscape elsewhere in the world.  We don’t 
foresee this changing anytime soon. 
their energies and resources higher in 
the technology stack, where they can 
significantly differentiate their products 
and services.
Q
Where do you see major 
patent threats coming from 
— are they from the cloud world, 
mobility or the emerging sectors 
like Internet of Things and 
artificial intelligence?
Linux and open source technology 
adoption will increase as more general 
computing technologies become the 
key building blocks in areas like the 
automotive industry. Many of the 
computing vendors that have been 
aggressive or litigious historically may 
try to use a similar strategy to extract 
licensing fees related to Linux. 
This is one of the reasons that 
companies like Toyota, Daimler-Benz, 
Ford, General Motors, Subaru and 
Hyundai-Kia, among many others, have 
either joined OIN or are very close 
to signing on. In fact, Toyota made a 
significant financial contribution to OIN 
and is our latest board member.
Q
Don’t you think that 
ultimately, this is a war with 
just Microsoft?
In the last few years, Microsoft has 
taken some positive steps towards 
becoming a good open source citizen. 
We have and will continue to encourage 
it to join OIN and very publicly 
demonstrate its commitment to shared 
innovation and patent non-aggression in 
For U & Me
Interview
28 | July 2017 | OPEN SOuRCE FOR yOu | www.OpenSourceForu.com


30 | JuLY 2017 | OPEN SOuRCE FOR YOu | www.OpenSourceForu.com
Developers Let’s Try
I
n the last decade or so, the amount of data generated has 
grown exponentially. The ways to store, manage and 
visualise data have shifted from the old legacy methods 
to new ways. There has been an explosion in the number 
and variety of open source databases. Many are designed 
to provide high scalability, fault tolerance and have core 
ACID database features. Each open source database has 
some special features and, hence, it is very important for a 
developer or any enterprise to choose with care and analyse 
each specific problem statement or use case independently. 
In this article, let us look at one of the open source 
databases that we evaluated and adopted in our enterprise 
ecosystem to suit our use cases.
MongoDB, as defined in its documentation, is an 
open source, cross-platform, document-oriented database 
that provides high performance, high availability 
and easy scalability. 
MongoDB works with the concept of collections, 
which you can associate with the table in an RDBMS 
like MySQL and Oracle. Each collection is made up of 
documents (like XML, HTML or JSON), which are the core 
entity in MongoDB and can be compared to a logical row 
in Oracle databases.
MongoDB has a flexible schema as compared to the 
normal Oracle DB. In the latter we need to have a definite 
table with well-defined columns and all the data needs to fit 
the table row type. However, MongoDB lets you store data 
in the form of documents, in JSON format and in a non-
relational way. Each document can have its own format and 
structure, and be independent of others. The trade-off is the 
inability to perform joins on the data. One of the major shifts 
that we as developers or architects had to go through while 
adopting Mongo DB was the mindset shift  — of getting used 
to storing normalised data, getting rid of redundancy in a 
world where we need to store all the possible data in the form 
of documents, and handling the problems of concurrency.
The horizontal scalability factor is fulfilled by the 
‘sharding’ concept, where the data is split across different 
machines and partitions called shards, which help further 
scaling. The fault tolerance capabilities are enabled by 
replicating data on different machines or data centres, thus 
making the data available in case of server failures. Also, 
This article targets developers and architects who are looking for open source adoption 
in their IT ecosystems. The authors describe an actual enterprise situation, in which they 
adopted MongoDB in their work flow to speed up processes. 
Using MongoDB to Improve the IT 
Performance of an Enterprise  

www.OpenSourceForu.com | OPEN SOuRCE FOR YOu | JuLY 2017 | 31
Developers
Let’s Try
www.OpenSourceForu.com | OPEN SOuRCE FOR YOu | JuLY 2017 | 31
an automatic leader election process provides high 
availability across the cluster of servers. 
Traditionally, databases have been supporting single 
data models like key value pairs, graphs, relational, 
hierarchical, text searches, etc; however, the databases 
coming out today can support more than one model. 
MongoDB is one such database that has multi-model 
capabilities. Even though MongoDB provides geo-
spatial and text search capabilities, it’s not as good or 
as up to the mark as Solr or Elastic Search, which make 
better search engines.
An analysis of our use case
We currently work in the order management space, where 
the order data is communicated to almost 120+ applications 
using 270+ integrations through our middleware. 
One of the main components we have implemented 
in-house is our custom logger, which is a service to log 
the transaction events, enabling message tracking and 
error tracking for our system. Most of the messaging 
is asynchronous. The integrations are heterogeneous, 
whereby we connect to Oracle, Microsoft SQL 
relational databases, IBM’s MQ, Service Bus, Web 
services and some file based integrations. 
Our middleware processes generate a large number 
of events in the path through which the order travels 
within the IT system, and these events usually contain 
order metadata, a few order attributes required for 
searching; a status indicating the success, errors, 
warnings, etc; and in some cases, we store the whole 
payload for debugging, etc. 
Our custom logger framework is traditionally used 
to store these events in plain text log files in each of the 
server’s local file systems, and we have a background 
Python job to read these log files and shred them into 
relational database tables. The logging is fast; however, 
tracking a message across multiple servers and trying 
to get a real-time view of the order is still not possible. 
Then there are problems around the scheduler and the 
background jobs which need to be monitored, etc. In a 
cluster with both Prod and DR running in active mode 
with 16 physical servers, we have to run 16 scheduler 
jobs and then monitor them  to ensure that they run all the 
time. We can increase the speed of data-fetching using 
multiple threads or schedule in smaller intervals; however, 
managing them across multiple domains when we scale 
our clusters is a maintenance headache.
To get a real-time view, we had rewritten our 
logging framework with a lightweight Web service, 
which could write directly to RDBMS database tables. 
This brought down the performance of the system. 
Initially, when we were writing to a file on local file 
systems, the processing speed was around 90-100k 
messages per minute. Now, with the new design of 
writing to a database table, the performance was only 
4-5k messages per minute. This was a big trade-off in 
performance levels, which we couldn’t afford. 
We rewrote the framework with Oracle AQs in 
between, wherein the Web service writes data into 
Oracle AQs; there was a scheduler job on the database, 
which dequeued messages from AQ and inserted data 
to the tables. This improved the performance to 10k 
messages per minute. We then hit a dead end with 
Oracle database and the systems. Now, to get a real-
time view of the order without losing much of the 
performance, we started looking out at the open source 
ecosystem and we hit upon MongoDB. 
It fitted our use case appropriately. Our need 
was a database that could take in high performance 
writes where multiple processes were logging events 
in parallel. Our query rate of this logging data was 
substantially lower. We quickly modelled the document 
based on our previous experience, and were able to 
swiftly roll out the custom logger with a MongoDB 
backend. The performance improved dramatically to 
around 70k messages per minute. 
This enabled us to have a near real-time view of 
the order across multiple processes and systems on a 
need basis, without compromising on the performance. 
It eliminated the need for multiple scheduler processes 
across a cluster of servers and also having to manage each 
of them. Also, irrespective of how many processes or how 
many servers our host application scaled to, our logger 
framework hosted on separate infrastructure is able to cater 
to all the needs in a service-oriented fashion.
Currently, we are learning through experience. Some 
of the challenges we faced while adopting MongoDB 
involved managing the data growth and the need to 
have a purge mechanism for the data. This is something 
that is not explicitly available, and needs to be planned 
and managed when we create the shards. The shard 
management needs to be improved to provide optimal 
usage of storage. Also, the replicas and the location of 
the replicas define how good our disaster recovery will 
be. We have been able to maintain the infra without 
much hassle and are looking at the opportunity to roll 
out this logging framework into other areas, like the 
product master or customer master integration space in 
our IT. This should be possible without much rework 
or changes because of MongoDB’s flexible JSON 
document model.
By: Raj Thilak and Gautham D.N.
Raj Thilak is a lead developer at Dell Technologies. He can 
be reached at Raj_Thilak_R@Dell.com. 
Gautham D.N. is an IT manager at Dell Technologies. He 
can be contacted at Gautham.D.N@dell.com

Earn up to 
₹1,00,000 
per hour 
 
Curious? Mail us at 
contact@loonycorn.com 
 
 Step 1: You work with us to create a course proposal 
for a 2-10 hour course 
 
 Step 2: We pay you an advance of ₹ 5,000/hour upon 
course approval 
 
 Step 3: You build the course, we help 
 
 Step 4: We grade your work and pay according to the 
rate card below (rates per hour) 
Grade A:  ₹100,000 | B:  ₹50,000 | C:   ₹25,000  | F:  ₹5,000     

Loonycorn 
Our Content: 
 
 The Complete Machine Learning Bundle 
 
10 courses |  63 hours |  $39 
 
 The Complete Computer Science Bundle 
8 courses  |  78 hours |  $39 
 
 The Big Data Bundle 
9 courses  |  64 hours |  $45 
 
 The Complete Web Programming Bundle 
 
8 courses  |  61 hours |  $41 
 
 The Complete Finance & Economics Bundle 
9 courses  |  56 hours |  $49 
 
 The Scientific Essentials Bundle 
 
7 courses  |  41 hours |  $35 
 
 ~20 courses on Pluralsight  
 
~70 on StackSocial 
~65 on Udemy 
 
About Us: 
 
 ex-Google | Stanford | IIM-Ahmedabad | INSEAD 
 50,000+ students  

34 | JuLY 2017 | OPEN SOuRCE FOR YOu | www.OpenSourceForu.com
Developers Let’s Try
A
WK, one of the most prominent text-processing 
utilities on GNU/Linux, takes its name from the 
initials of its authors — Aho, Weinberger and 
Kernighan. It is an extremely versatile programming language 
that looks a little like C. It is a scripting language that 
processes data files, especially text files that are organised in 
rows and columns. 
AWK really is a consistent tool with a few data types. 
Its portability and stability have made it very popular. It’s 
a concise scripting language that can tackle a vast array 
of problems. It can teach the reader how to implement a 
database, a parser, an interpreter and a compiler for a small 
project-specific computer language. 
If you are already aware of regex (regular expressions), 
it’s quite easy to pick up the basics of AWK. This article will 
be useful for software developers, systems administrators, 
or any enthusiastic reader inclined to learn how to do text 
processing and data extraction in UNIX-like environments. 
Of course, one could use Perl or Python, but AWK makes it 
so much simpler with a concise single line command. Also, 
learning AWK is pretty low cost. You can learn the basics in 
less than an hour, so it doesn’t require as much effort and time 
as learning any other programming/scripting language.
The original version of AWK was written in 1977 at 
AT&T Bell Laboratories. In 1985, a new version made the 
programming language more powerful, introducing user-
defined functions, multiple input streams and computed 
regular expressions.
• 
AWK’s first version came out in 1977 (old AWK)
• 
NAWK – this was an extended version that was 
released in 1985 (new AWK)
• 
MAWK – an extended version by Michael Brennan
• 
GAWK – GNU-AWK, which is faster and provides 
better error messages
Typical applications of AWK include generating reports, 
validating data, creating small databases, etc. AWK is very 
powerful and uses a simple programming language. It can 
solve complex text processing tasks with a few lines of code. 
Starting with an overview of AWK, its environment, and 
workflow, this article proceeds to explain its syntax, variables, 
operators, arrays, loops and functions.
 AWK installation
Generally, AWK is available by default on most GNU/Linux 
distributions. We can use the which command to check whether 
Introducing AWK, a programming language designed for text processing and 
typically used as a data extraction and reporting tool. This language is a standard 
feature of most UNIX-like operating systems. 
The AWK Programming Language: 
A Tool for Data Extraction

www.OpenSourceForu.com | OPEN SOuRCE FOR YOu | JuLY 2017 | 35
Developers
Let’s Try
it is present on your system 
or not. In case you don’t 
have AWK, then install it 
on Debian based GNU/
Linux using the Advanced 
Package Tool (APT) package 
manager, as follows:
sudo apt-get install gawk
AWK is used for 
stream processing, where 
the basic unit is the string. 
It considers a text file as 
a collection of fields and 
records. Each row is a 
record, and a record is a 
collection of fields.
The syntax of AWK is as follows.
On the command line: 
awk  [options] ‘pattern {action}’ input file
As an AWK script:
awk  [options]  script_name input file 
The most commonly used command-line options of awk 
are -F and -f : 
-F : to change input field separator
-f : to name script file
A basic AWK program consists of patterns and actions — if 
the pattern is missing, the action is applied to all lines, or else, 
if the action is missing, the matched line is printed. There are 
two types of buffers used in AWK – the record buffer and field 
buffer. The latter is denoted as $1, $2… $n, where ‘n’  indicates 
the field number in the input file, i.e., $ followed by the field 
number (so $2 indicates the second field). The record buffer is 
denoted as $0, which indicates the whole record.
For example, to print the first field in a file, use the 
following command: 
awk ‘{print $1}’ filename
To print the third and first field in a file, use the command 
given below:
awk ‘{print $3, $1}’ filename
AWK process flow
So how does one write an AWK script?
AWK scripts are divided into the following three parts 
— BEGIN (pre-processing), body (processing) and END 
(post-processing).
BEGIN {begins actions}
Patterns{Actions} 
END {ends actions}
BEGIN is the part of the AWK script where variables 
can be initialised and report headings can be created. The 
processing body contains the data that needs to be processed, 
like a loop. END or the post-processing part analyses or prints 
the data that has been processed.
Let’s look at an example for finding the total marks and 
averages of a set of students.
The AWK script is named as awscript.  
#Begin Processing
BEGIN {print “To find the total marks & average”}
{    
 #body processing
           tot=$2+$3+$4
           avg=tot/3
           print “Total of “ $1 “:”, tot
           print “Average of “$1 “:”, avg 
}
#End processing
END{print “---Script Finished---”}
Input file is named as awkfile
Input   file (awkfile)   
Aby 20 21 25                                     
Amy 22 23 20                                            
Running the awk script as :
awk –f awscript awkfile
Output
To find the total marks & average
Total of Aby is : 66
Average of Aby is : 22
Total of Amy is : 65
Average of Amy is : 21.66
Classification of patterns
Expressions: AWK requires two operators while writing 
Figure 1: A file with four records, each with three fields
Figure 2: AWK process flow
Ann
21
CSE
Manu
23
EEE
Amy
24
CSE
Jack
21
ECE
Record 1
Name
(Field 1)
Age
(Field 2)
Department
(Field 3)
Record 2
Record 4
Start
BEGIN
Actions
More Data
and
Not Quit?
Read Next
Record
Apply Actions
to Record
More Instructions
in Script?
END
Actions
Stop
No
Yes
Yes
No

36 | JuLY 2017 | OPEN SOuRCE FOR YOu | www.OpenSourceForu.com
Developers Let’s Try
regular expressions (regex) — match (~) and doesn’t 
match (!~). Regular expressions must be enclosed 
in /slashes/, as follows:
   
awk ‘$0 ~ /^[a-d]/’ file1    (Identify all lines in a file that 
starts with a,b,c or d)
       awk ‘$0 !~ /^[a-d]/’ file1   (Identify all lines in a 
file that do not start with a,b,c or d)
An example of an expression is counting the number of 
occurrences of the pattern ‘unix’ in the input file ‘data’.
Awk supports the following:
Arithmetic operators: + , - , * , /, % , ^ .
Relational operators: >, >=, < ,<=, ==, !=  and…
Logical operators: &&, ||, !. 
As an example, consider the file awktest:
1 
unix   10  50
2 
shell  20   10   
3 
unix   30   30
4 
linux  20   20
• 
Identify the records with second field, “unix”  and 
value of third field > 40
   awk ‘$2 == “unix” && $3 > 40 {print}’ awktest
 
 1  unix   10  50
• 
Identify the records where, product of third  & fourth 
field is greater than 500
 
           awk ‘$3 * $4 > 500 {print}’ awktest   
 
 
 
  
 
3 
unix   30   30
Hence, no address pattern is entered, and AWK applies 
action to all the lines in the input file.
The system variables used by AWK are listed below.
FS:  
  
Field separator (default=whitespace)
RS:  
  
Record separator (default=\n)
NF: 
 
 Number of fields in the current record
NR: 
 
 Number of the current record
OFS: 
 
 Output field separator (default=space)
ORS: 
 
 Output record separator (default=\n)
FILENAME: 
 Current file name
There are more than 12 system variables used by AWK. 
We can define variables (user-defined) also while creating an 
AWK script.
• 
awk '{OFS="-";print $1 , $2}' marks
 
 john-85
 
 andrea-89
• 
awk '{print NR, $1, $3}' marks 
 
 1 john cse
 
 2 andrea ece
Range patterns: These are associated with a range of 
records, which match a range of consecutive input lines:
Start-pattern, end-pattern{actions}
Range starts with the record that matches the start pattern 
and ends with the record that matches the end pattern.
Here is an example:    
Print 3rd line to 5th line, along with line numbers of the 
file, marks
• 
awk ‘NR==3, NR==5 {print NR, $0}’ marks
Action statements
Expression statements: An expression is evaluated and 
returns a value, which is either true or false. It consists of 
any combination of numeric and string constants, variables, 
operators, functions, and regular expressions.
Here is an example: 
{$3 = “Hello”}
{sum += ($2+4)}
Output statements: There are three output actions in 
AWK: print, printf and sprint.
 
print writes the specified data to the standard output file.
awk ‘{print $1, $2, $3}’ file name prints first, second 
and third columns.
 
printf uses a format specifier in a ‘format-string’ that 
requires arguments of a matching type.
 
string printf (sprintf) stores the formatted print 
string as a string.
 
 str = sprintf(“%2d %-12s %9.2f”, $1, $2, $3)
Figure 3: Classification of patterns
Figure 4: Regular Expression in AWK
Expression
Regular
Arithmetic
Relational
Logical
Nothing
END
BEGIN
Patterns
Simple
Range

www.OpenSourceForu.com | OPEN SOuRCE FOR YOu | JuLY 2017 | 37
Developers
Let’s Try
As an example, consider the file, ‘data’:
12 
 abcd  12.2
13 
mnop 11.1
• 
awk ‘{printf(“%@d %-3s %0.3f”, $1, $2, $3)}’ data
  
 the above command appends an @ before first field, left 
assign second field, print third field with 3 decimal places
 
o/p
@12abcd 12.200
@13mnop 11.100 
Decision statements: An if-else decision statement 
evaluates an expression and takes proper action. Nested if 
statements are also applied.
As an example, to print all records with more than three 
fields, type:
BEGIN{}
{
If(NF > 3)
 
print $0
else
 
print “Less than 3 fields” 
}
To print the marks and grades of a student, type:
BEGIN{print “Mark & grade of a student”}
{
 
If($2==”Amy”)
 
S=S+$3
}
END{print “Total marks: “S
if(S>50)
 
print “Grade A”
else
 
print “Grade B”
}
Loop statements: While, do.. while and for are the 
loop statements in AWK. The AWK while loop checks 
the condition first, and if the condition is true, it executes 
the list of actions. This process repeats until the condition 
becomes false.
Here is an example: 
BEGIN {print “Display even numbers from 10 to 20” }
{ #initialization  
 
 
 
 
 
 
 I = 10 
 
#loop limit test  
 
 
 
 
 
 while (I <=20)  
 
 
 
 
 { 
#action  
 
 
 
 
  
print I  
 
 
 
 
  
I+=2 
 
#update  
 
 
 } 
 
 
 
 
 
 
} # end script
do.. while loop
The AWK do while loop executes the body once, then 
repeats the body as long as the condition is true. Here is an 
example that displays numbers from 1 to 5:
awk ‘BEGIN {I=1; do {print i; i++ } while(i < 5)} ‘
Here is an example of the for loop: 
program name : awkfor
BEGIN { print “ Sum of fields in all lines”}
{ 
  
 
 
 
 
 
for ( i=1; i<=NF; i++) 
 
 
 
 
{  
 
 
 
 
 
 t=t+$i   //sum of $1 + sum of $2
 
}  
 
 
 
END { print “Sum is “ t} 
Consider the input file : data
 
10 30
 
10 20
Running the script : awk –f awkfor data
 
Sum of fields in all lines
 
Sum is 70
Control statements: next, getline and exit are the control 
statements in AWK. The ‘next’ statement alters the flow of 
the program — it stops the current processing of pattern 
space. The program reads the next line and starts executing 
commands with the new line.
Getline is similar to next, but continues executing the script.
The exit statement causes AWK to immediately stop 
processing the input, and any remaining lines are ignored.
Mathematical functions in AWK
The various mathematical functions in AWK are:
int(x) --  truncates the floating point to the integer
cos(x) -- returns the cosine of x
exp(x) -- returns e^x 
Figure 5: Action statements

38 | JuLY 2017 | OPEN SOuRCE FOR YOu | www.OpenSourceForu.com
Developers Let’s Try
log(x) -- returns the natural logarithm of x
sin(x)  -- returns the sine of x
sqrt(x) -- returns the square root of x 
Here is an example:
 
{  
 
 x = 5.3241 
 
 y = int(x)  
 
 printf “truncated value is “,  y
 
}
Output: truncated value is 5
String functions in AWK
1. length(string): Calculates the length of a string.
2. index(string, substring): Returns the first position 
of the substring within a string. For example, in x= 
index(“programming”, “gra”), x returns the value 4.
3. substr(): Extracts the substring from a string. The two 
different ways to use it are: substr(string, position) and 
substr(string, position, length).
Here is an example: 
 
{
 
 x = substr(“methodology”,3) 
 
 y = substr(“methodology”,5,4)
 
 print “substring starts at “ x
 
 print “ substring of length “ y
 
}
 
 The output of the above code is:
Substring starts at hodology 
Substring of length dolo
4. sub(regex, replacement string, input string) or gsub(regex, 
replacement string, input string)
sub(/Ben/,” Ann “, $0): Replaces Ben with Ann 
(first occurrence only).
gsub(/is/,” was “, $0):   Replaces all occurrences 
of ‘is’ with ‘was’.
5. match(string, regex)
 
{
 
x=match($0,/^[0-9]/)             #find all lines that 
start with digit
 
if(x>0) 
 
 
 
#x returns a value > 0 if there’s a 
match
 
 print $0
 
}
6. toupper() and tolower(): This is used for convenient 
conversions of case, as follows:
 
{
 
print toupper($0)      #converts entire file to uppercase
 
}
User defined functions in AWK 
 
AWK allows us to define our own functions, which 
enables reusability of the code. A large program can be 
divided into functions, and each one can be written/tested 
independently.  
The syntax is:
 
Function Function_name (parameter list)
 
{
 
  
Function body 
 
}
The following example finds the largest of two numbers. 
The program’s name is awfunct.
{
print large($1,$2)  
 
 
}
function large(m,n) 
 
 
{ 
  
 
 
 
 
 
return m>n ? m : n
}
Input file is: doc
 
      100 400
Running the script: awk –f awfunct doc
 
  
400
Associative arrays
AWK allows one-dimensional arrays, and their size and 
elements need not be declared. An array index can be a 
number or a string.
The syntax is:
 
arrayName[index] = value
Index entry is associated with an array element, so AWK 
arrays are known as associative arrays.
For example, dept[$2]  indicates an element in the second 
column of the file and is stored in the array, dept.
 
Program name : awkarray
BEGIN{print “Eg of arrays in awk”}
{
 
dept[$2]
 
for (x in dept)
 
{
 
 print a[x]

www.OpenSourceForu.com | OPEN SOuRCE FOR YOu | JuLY 2017 | 39
Developers
Let’s Try
 
}
}
Consider the input file, data
 
 S3 CSE A
 
 S4 ECE B
 
 S4 EEE A
Running the script : awk –f awkarray data
 
Eg of arrays in awk
 
 CSE
 
 ECE
 
 EEE
AWK is oriented towards delimited fields on a per-
line basis. It has very robust programming constructs 
including decision statements like if..else, and loops like 
while and do.. while. 
We can conclude by saying that AWK is another keystone 
of UNIX shell programming. It really shines when it comes 
to simplifying things like processing multiline records and 
interpolating multiple files simultaneously. AWK inherits the 
features of conventional programming languages. 
So not only was AWK popular when it was introduced but 
it has also led to the creation of other popular languages. 
More details about this text processing utility 
can be found in the books: ‘The AWK Programming 
Language by Alfred V. Aho, Brian W. Kernighan, and 
Peter J. Weinberger (1988-01-01); and ‘UNIX and Shell 
Programming’ by Behrouz A. Forouzan and Richard F. 
Gilberg, (Cengage Learning). 
By: Neethu C. Sekhar
The author is an open source enthusiast, currently working 
as assistant professor in the Department of Computer 
Science, AmalJyothi College of Engineering, Kerala. She can 
be reached at nitucskr@gmail.com
[1] https://en.m.wikipedia.org/wiki/AWK
[2]  www.grymoire.com/Unix/Awk.html
[3]  www.thegeekstuff.com/tag/awk-tutorial-examples/
References

Developers Let’s Try
L
oopBack is a Node.js framework that is used to design 
highly-extensible, powerful back-end services for 
mobile and Web applications. It enables dynamic end-
to-end REST APIs with little or zero coding effort through its 
user-friendly wizard support. It is built on the top of Express.
js and adapts most of its functionality like models, routing, 
middleware, etc, but with minimal coding requirements. It 
was introduced by StrongLoop and is currently maintained 
jointly by StrongLoop and IBM. It also comes with good 
integration support for client-side application development 
with the available SDKs. This article covers some LoopBack 
concepts and takes you through the few steps to build a small 
server application for a weather station.
Installation and building your first application
The current production release is LoopBack v3.0,  with long 
term support (LTS) for v2.x. Install the LoopBack CLI tool, 
which provides various wizards for generating applications, 
models, data sources, access control rules, etc, as follows. 
Choose any LTS verions of Node.js and a compatible npm 
utility for the current work.
npm install loopback-cli -g
The IBM API Connect Developer Kit is another solution 
which uses LoopBack internally. In this article, we’ll focus 
on the LoopBack CLI tool and v2.x LTS. Let’s follow 
the steps shown below to create a simple app using the 
application generator.
lb app
? What’s the name of your application? (osfy-demo)
? Which version of LoopBack would you like to use? 
 
  
 
 
   2.x (long term support)
? What kind of application do you have in mind? api-server  
Alternatively, you could also choose hello-world or notes as 
initial templates. After completing this wizard, all the required 
modules will be installed in the node_modules sub-directory.
Then run the server as follows:
node .   
 
(or)
npm start
…which will prompt the following URLs to the launch 
home page and a Swagger UI for the REST interface.
Web server listening at: http://0.0.0.0:3000
Browse your REST API at: http://0.0.0.0:3000/explorer
You may choose any process manager like pm2 for better 
management of the server.
LoopBack is an open source Node.js framework, with which you can quickly create 
REST APIs, and connect devices and browsers to data and services. LoopBack helps 
to easily create client apps with Android, iOS and AngularJS SDKs, and comes with 
add-on components for file management, third-party login and OAuth support.
40 | JuLY 2017 | OPEN SOuRCE FOR YOu | www.OpenSourceForu.com
Building REST APIs  
with the LoopBack Framework

Developers
Let’s Try
Working with models
Models, which represent back-end data sources or services, 
are the heart of LoopBack. They come with full-featured 
REST APIs. Built-in models are created implicitly as per the 
application template; users add custom models with the help 
of generation wizards.
Let’s add a model using a generator for this app, which 
represents some environmental parameters like temperature, 
humidity, pressure, etc.
lb model
? Enter the model name: Weather
? Select the datasource to attach weather to: db (memory)
? Select model’s base class : PersistedModel
? Expose weather via the REST API? Yes
? Custom plural form (used to build REST URL): Weather
? Common model or server only? common
You can identify the generated file for the created 
model as weather.json in the common/models sub-
directory. The default plural form is mostly suitable for 
model names, e.g., ‘Devices’ is applicable if the model 
name is ‘Device’. But for the current model, we’ll use the 
custom plural form, as ‘Weathers’ is not a suitable plural 
form. Let’s add a few properties to this model, say device-
id as the string, and temperature, humidity, pressure as 
the number with a suitable choice for Required fields and 
Default values. You can also add more properties after 
completing the wizard using the following command or by 
manually editing weather.json:
lb property
Connecting with data sources
Model instances (property values) are stored under the 
in-memory database, by default. You can opt for better 
persistency support with suitable connectors for popular SQL 
and NoSQL based databases like MySQL, MongoDB, etc.
Let’s use the following steps to create a new data source 
and attach the model to it:
lb datasource
? Enter the datasource name: mydb
? Select the connector for mydb: MySQL 
? Connection String url to override other settings: 
? host: localhost
? port: 3306
? user: root
? password: ****
? database: loopback
? Install loopback-connector-mysql@^2.2 (Y/n)Yes
This will install loopback-connector-mysql in node_
modules and generate JSON content for the data source in 
server/datasources.json. This file can be edited for further 
changes once the wizard completes its tasks. To attach this 
data source to our ‘weather’ model, edit server/model-
config.json as follows, replacing db with mydb:
“Weather”: {
 
“dataSource”: “mydb”,
 
“public”: true
}
LoopBack comes with support for preparing schema with 
applicable tables or collections in chosen databases; so, let’s 
add a script in server/boot, say prepare-schema.js with the 
following content:
module.exports = function(app) {
  app.dataSources.mydb.autoupdate(‘Weather’, function(err) {
    if (err) throw err;
     console.log(‘Schema created for Weather model: \n’);
  }); 
Create the database loopback in MySQL and launch the 
server. You can observe that any updation in model instances 
is reflected in the chosen database under the table with the 
same name as the model. You can follow similar steps for 
other databases like MongoDB, with suitable connectors.
Exploring REST APIs
Each generated model comes with full-featured REST 
APIs with ‘create, read, update and delete’ (CRUD) 
operational support on model instances. You can 
explore these APIs using the Swagger UI available at 
hostname:3000/explorer. Let’s perform the following 
REST operations to add a new instance: 
POST /api/Weather
{
 
“device-id”:”rpi123”
 
“temperature”: 18,
 
“humidity”: 72,
 
“pressure”: 1060
}
And to retrieve all model instances, use the following 
command:
GET /api/Weather
You can also add filters to retrieve specific instances in a 
particular order. Examples are given below. 
To list instances with a specific device ID, type:
{“where” : {“device-id”: “rpi3”}}
www.OpenSourceForu.com | OPEN SOuRCE FOR YOu | JuLY 2017 | 41

42 | JuLY 2017 | OPEN SOuRCE FOR YOu | www.OpenSourceForu.com
Developers Let’s Try
To retrieve instances matching certain criteria, give the 
following command:
{“where” : {“temperature”: {“gt”:”20”}}}
To list in an ascending order, type:
{“order” : “temperature ASC”}
These filters are encoded in the URL query string as 
‘filter=<url-encoded-criteria>’ with HTTP requests.
You can play around with other REST endpoints and 
verbs exposed on the current model through this UI, and 
also observe the equivalent Curl command usage for making 
requests using any other client.
Adding a Web template
You can add some static Web content as the home page. 
For this, edit the files section in server/middleware.json as 
follows, to define the static middleware:
“files”: {
 
“loopback#static”: {
 
“params”: “$!../client” 
  }
},
Then disable server/boot/root.js by removing or renaming 
it with an extension other than .js.
Now place any static pages with HTML, CSS and client-
side scripting content under the client sub-directory, which 
will serve as a home page for your application.
Authentication and authorisation
LoopBack comes with good support for authentication 
and access controls through its built-in models — User, 
AccessToken, ACL, RoleMapping and Role. To create database 
schema for these models, add a small procedure in a .js file as 
described in http://loopback.io/doc/en/lb2/Creating-database-
tables-for-built-in-models.html and execute it once.
To create a new user, do a POST operation on /api/Users 
with the following payload:
{
 
“email”: “abc@strongloop.com”,
 
“password”: “hello123”,
 
“emailVerified”:true
}
You may add other properties like realm for real name, 
and username in the above payload. Now log in with a 
POST operation on /api/Users/login using the payload 
with a registered email ID and password, which returns 
a response with accessToken, with property name id and 
a ttl value for validity. The default validity period for 
accessToken is two weeks.
{ 
 
“id”:”xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx”,
 
“ttl”: 1209600,
}
You can use this generated token for further 
authentication. In the Swagger UI, this token can be set 
in the field which appears at the top right corner, which 
adds a name-value pair to query string for authentication. 
For example, to retrieve the list of instances, the following 
REST operation is used.
GET /api/Weather?access_token=xxxxxxxxxxxxxxxxxxxxxxx
You can perform a POST operation on /Users/logout to 
invalidate the token explicitly.
ACL rules specify the access privileges on created models 
for users. Let’s deny access to all properties of the current 
model to all users, initially.
lb acl
? Select the model to apply the ACL entry to: Weather
? Select the ACL scope: All methods and properties
? Select the access type: All (match all types)
? Select the role All users
? Select the permission to apply Explicitly deny access
Now, let’s grant access to any authenticated user:
? Select the role : Any authenticated user
? Select the permission to apply : Explicitly grant access
You can observe these generated ACL rules under the 
model-specific JSON file in common/models, i.e., weather.
json in our example. With these rules, you can observe that 
only authenticated users can perform CRUD operations via 
REST on the Weather model.
Figure 1: Swagger UI for REST APIs
Figure 2:  Access token in REST Explorer

www.OpenSourceForu.com | OPEN SOuRCE FOR YOu | JuLY 2017 | 43
Developers
Let’s Try
By: Rajesh Sola
The author is a faculty member of C-DAC’s Advanced 
Computing Training School, and an evangelist in the 
embedded systems and IoT domains. You can reach 
him at rajeshsola@gmail.com
Feature summary
Here is the summary of some more features of 
LoopBack, which do prove that it can be the perfect 
choice for a ‘Backend as a Service’ (BaaS) for Web 
applications, and for connected smart devices in 
IoT applications.
 
Extends REST API using remote methods, 
remote hooks and operational hooks.
 
Model hierarchy and relationships for real 
world object mapping.
 
Data validation mechanism for model instances 
before storing in data source.
 
Routing and middleware similar to Express.js.
 
Real-time ‘server sent events’ (SSE) support for 
live updates.
 
Client-side SDKs for Android, iOS, Xamarian 
and Angular.JS apps to interact with models 
in an elegant mode, eliminating clunky HTTP 
interfaces and complex data formats.
 
Sends push notifications using Apple Push 
Notification Service (APNS) for iOS and 
Google Cloud Messaging (GCM) services for 
Android apps.
 
Auto generation of Angular services for client-
side representation of models.
 
OAuth2.0 and Passport based authentication for 
third party logins.
 
Components for content management on 
popular cloud storage providers.
 
Offline synchronisation support.
 
Ease of deployment on PaaS platforms like 
Bluemix, OpenShift, Heroku, etc.
LoopBack can also be a perfect choice 
for ‘Mobile Backend as a Service’ (MBaas), 
especially with its support for client SDKs, push 
notifications, offline synchronisation and cloud 
deployment. You can find the entire code for the 
example discussed in this article at github.com/
rajeshsola/iot-examples/tree/master/loopback-
demo, and some more hints on the README page 
of this directory. 
[1] http://loopback.io/doc/en/lb2/index.html
[2] http://loopback.io/doc/en/lb2/LoopBack-
core-concepts.html
References
•  www.lulu.com
•  www.magzter.com
•  Createspace.com
•  www.readwhere.com

44 | JuLY 2017 | OPEN SOuRCE FOR YOu | www.OpenSourceForu.com
Developers Let’s Try
Today, the Internet is being accessed more via the mobile phone rather than desktop 
platforms. However, the multiplicity of devices and platforms makes app development 
on mobile phones a difficult and expensive task. The solution to this problem lies in 
using a cross-platform mobile app development framework like Meteor.
T
here are so many platforms and operating systems for 
mobile phones available in the global market today 
that inter-device installation and the compatibility 
of mobile apps has turned into a huge challenge. Most of 
the Web based services for banking, billing, social media, 
education, games, administration, governance, etc, are 
available as mobile apps for different mobile platforms. So a 
prime challenge is to develop apps that can run on multiple 
mobile operating systems (OSs).
Different mobile OSs function on diverse programming 
paradigms with their own software development kits (SDKs) 
including Android, iOS, BlackBerry, Windows, MeeGo, 
Symbian, Tizen, Bada, webOS, Firefox OS, Palm OS, 
Sailfish OS, ZenUI, MIUI, HTC Sense, LineageOS, EMUI, 
Cyanogen, and many others. All these operating systems have 
their own programming model, structure and architecture.
Portability as well as compatibility of mobile apps on 
all these operating systems is a challenge. In the current 
scenario, the development of a single mobile app that 
can run on all these operating systems is very expensive, 
because for each operating system there is a different 
programming language and scripting technology. A mobile 
app developed using the Android SDK cannot be executed 
on BlackBerry, Symbian or any other operating system. For 
this reason, there is a need to develop mobile apps using 
cross-platform programming approaches.
There are many programming languages and technologies 
that can be used to develop cross-platform mobile apps. These 
platforms have excellent features to help you code using the 
‘Write once, use anywhere’ strategy, which means that the 
mobile app will work on all mobile devices irrespective of the 
OS and architecture. Developers are now creating apps that 
can be launched on any mobile OS without any compatibility 
and performance issues.
Phone Gap, Accelerator, Xamarin, Sencha Touch, 
Monocross, Codename One, Kony, Convertigo, Nativescript, 
RhoMobile, iFactr, FeedHenry, Cocos2d, Unity 3D, Corona, 
Qt, Alpha Anywhere 5App, etc, are all tools for cross-
platform mobile app development.
Free and open source tools for cross-platform 
mobile app development
There are a number of platforms and technologies that help 
to develop cross-platform mobile apps, but many of them 
are proprietary. Thankfully, there are sufficient free and open 
source tools available, using which a customised mobile 
Cross-Platform 
Mobile App 
Development 
Using the Meteor 
Framework

www.OpenSourceForu.com | OPEN SOuRCE FOR YOu | JuLY 2017 | 45
Developers
Let’s Try
app can be developed and deployed as per the functions and 
features required.
Some prominent open source tools for cross-platform app 
development are:
• 
Meteor
• 
Apache Cordova
• 
Appcelerator
• 
Qt
• 
Xamarin
In this article, we look at how to use the Meteor 
framework for developing apps.
Meteor 
Meteor is a JavaScript based Web framework for mobile app 
development. Its URL is https://www.meteor.com, and the 
current version is 1.4.
Meteor or MeteorJS is a free and open source framework 
written in C, C++ and JavaScript. The source code repository 
of MeteorJS is available on https://github.com/meteor/meteor. 
The development of code for mobile apps can be done easily 
in the JavaScript programming model.
Meteor follows the reactive programming model, in which 
the client application/browser is not only used to display the 
data, but the reactions on real-time changes and updates are 
also done in parallel. Meteor escalates the performance of the 
mobile app as it works in real-time, by default.
Mobile apps for different OSs including Android, iOS and 
many others can be developed using MeteorOS. This framework 
has excellent features to integrate MongoDB for Big Data and 
high performance computing in mobile applications.
Installation of Meteor on Linux/OSX
To install Meteor on Linux/OSX, give the following command:
$ curl https://install.meteor.com/ | sh
The above command performs the following tasks: 
1. Connects with install.meteor.com.
2. Downloads the latest stable release of Meteor.
3. Installs the required Meteor version.
Installation of Meteor on Windows 
The installation of Meteor in Windows is quite 
straightforward. Simply download the installer from https://
install.meteor.com/windows. The installation wizard will 
install the Meteor framework.
Once the MeteorJS framework is installed, the 
verification of the installed version can be done using the 
following command:
PathToMeteor\> meteor --version
The features of MeteorJS are:
 
 Cross-platform native mobile app development.
Figure 1: Web page of Meteor or MeteorJS
Figure 2: Installation Web page of Meteor
Figure 3: Installing Meteor on Windows
 
 Generation of apps for multiple working environments 
including Web browsers and mobile platforms.
 
 Enormous packages are available for different types of 
functions and modules.
 
Integration of Meteor Galaxy for compatibility 
with the cloud.
 
 The apps created using Meteor work in real-time, 
as default.
 
The programming structure of JavaScript is required for 
both client side as well as server side development.
Creating apps using Meteor
To create a new app in Meteor, the command meteor create is 
executed in the installation folder.
To create the app, run the meteor create command from 
the command prompt. Any name can be assigned to the app.
PathToMeteor>meteor create FirstMeteorApp

46 | JuLY 2017 | OPEN SOuRCE FOR YOu | www.OpenSourceForu.com
Developers Let’s Try
Running the app
In the app folder, the command meteor is executed as follows:
FirstMeteorApp >meteor
Output
 
Started proxy.
 
Started MongoDB
 
Started you app.
 
App running at: http://localhost:3000/
Type Control-C twice to stop
The results of the app can be viewed on the Web browser 
at the URL http://localhost:3000/.
Tags and levels in Meteor
There are three tags in Meteor—head, body and template. 
These are used to integrate HTML and JavaScript.
MyMeteorApp.html
<head>
   <title>MyMeteorApp</title>
</head>
<body>
   <h1>Meteor Programming</h1>
   {{> myAppParagraph}}
</body>
 <template name = “myAppParagraph”>
   <p>{{mytext}}</p>
</template>
MyMeteorApp.js
if (Meteor.isClient) {
   // Code for Client-Side
   Template.myAppParagraph.helpers({
      mytext: ‘Hello to All’
   });
}
Using forms and getting dynamic user data
The form elements are created in the HTML page, as follows:
meteorForm.html
<head>
   <title>meteorForm</title>
</head>
<body>
   <div>
      {{> myFormTemplate}}
   </div>
</body>
 <template name = “myFormTemplate “>
   <form>
      Name <input type = “text” name = “UserForm”>
      <input type = “submit” value = “Click Here”>
   </form>
</template>
The JavaScript code is integrated with submit event, as 
shown below: 
meteorForm.js
if (Meteor.isClient) {
   Template. myFormTemplate.events({
      ‘submit form’: function(event){
         event.preventDefault();
         var mytext= event.target. UserForm.value;
         console.log(mytext);
         event.target.UserForm.value = “”;
      }
   });
}
Sessions handling in Meteor
Sessions are used to store the data when the app is in use. 
The validity of this data is traditionally up to the browsing of 
the app by the user. Once the user leaves the app, this data is 
removed with the destruction of the session.
In Meteor, sessions handling is easy to integrate using the 
session object and, finally, returning the stored data:
meteorSession.html
<head>
   <title>meteorSession</title>
</head>
<body>
   <div>
      {{> myMeteorTemplate}}
   </div>
</body>
 <template name = “myMeteorTemplate”>
</template>
To store the data, the Session.set() method is used. The 
Session.get() method can be called to retrieve the stored data 
in the session, as follows:
meteorSession.js
Figure 4: Displaying the output using MeteorJS
Meteor Programming
Hello to all
localhost:3000

www.OpenSourceForu.com | OPEN SOuRCE FOR YOu | JuLY 2017 | 47
Developers
Let’s Try
if (Meteor.isClient) {
   var mySessionData = {
      mykey1: “ParameterValue1”,
      mykey2: “ParameterValue2”
   }
  Session.set(‘myMeteorSession’, mySessionData);
   var mysessiondatalog = Session.get(‘myMeteorSession’);
   console.log(mysessiondatalog);
}
On execution of the scripts, the log of stored data 
can be viewed:
Object{mykey1: “ParameterValue1”, mykey2: “ParameterValue2”}
Extending Meteor with smart packages
Meteor apps can be integrated with a number of APIs and 
smart packages to extend the existing features. Meteor smart 
packages can be downloaded with the related installation 
instructions from https://atmospherejs.com.
The main reason for adopting and working with MeteorJS 
is that developers can code the entire app in one language, 
which is JavaScript. While in other platforms the integration 
By: Dr Gaurav Kumar
The author is the MD of Magma Research and Consultancy 
Pvt Ltd, Ambala. He is associated with various academic and 
research institutes, where he delivers lectures and conducts 
technical workshops on the latest technologies and tools. 
You can contact him at kumargaurav.in@gmail.com. Website: 
www.gauravkumarindia.com. 
of multiple languages may be required, this is not an issue 
in Meteor. In addition, the apps developed in Meteor are in 
real-time and based on the reactive programming model, 
by default; so they work in high performance mode without 
any delays. A number of smart packages and modules are 
available with Meteor to integrate Google Apps, Facebook, 
Twitter and other platforms without any complexities. 
Figure 5: Displaying the form output using MeteorJS
Figure 6: Exploring AtmosphereJS for packages of Meteor
Figure 7: Searching the package for Twitter accounts on AtmosphereJS
Name
Click Here
localhost:3000
Month
theMe
March 2017
Open Source Firewall, Network security and Monitoring
April 2017
Databases management and Optimisation
May 2017
Open Source Programming (Languages and tools)
June 2017
Open Source and IoT
July 2017
Mobile App Development and Optimisation
August 2017
Docker and Containers
September 2017
Web and desktop app Development
October 2017
Artificial Intelligence, Deep learning and Machine Learning
November 2017
Open Source on Windows
December 2017
BigData, Hadoop, PaaS, SaaS, Iaas and Cloud
January 2018
Data Security, Storage and Backup
February 2018
Best in the world of Open Source (Tools and Services)
oSFY Magazine Attractions During 2017-18



Developers Let’s Try
K
otlin is a statically typed open source modern 
programming language targeting JVM, Android and 
even JavaScript. It is named after the Russian island 
called Kotlin, just as Java is named after the Indonesian 
island. It works everywhere where Java works – on the server 
side, the desktop, Android, and other places.
The biggest selling point of Kotlin is its 100 per cent 
interoperability with Java. So instead of throwing away 
or re-writing all your libraries in Kotlin, you can use all 
your existing Java libraries and frameworks directly with 
it. Due to this characteristic, you can gradually adopt 
Kotlin in your Java code base.
Kotlin has been created by JetBrains, the maker of IntelliJ, 
Android Studio and other amazing IDEs. 
It is open source and has been released under the 
Apache licence. 
Why you should use Kotlin
The biggest reason to learn Kotlin is that at Google I/O 17, 
Google announced first-class support for Kotlin for Android. 
So now you can expect full support from Google, like 
documentation to write Android apps in Kotlin. 
Another reason is that it’s an amazing language and many 
developers (especially Android developers like me) are in 
love with it and have fun writing code in it. People who are 
tired, or in some cases irritated with Java, will definitely find 
Kotlin concise and fun to work with.
Problems and issues with Java and the 
Android platform
Let’s start with the current problems with Java that are usually 
faced by developers coming from other languages.
Java is extremely old. It’s been around for more than 20 
years. It doesn’t have the modern higher order functions and 
features that newer languages like Swift or Golang ship with. 
Though Java 8 is trying to bring out some good stuff, it still 
is verbose, which means that to perform even small tasks, 
we end up writing a lot (seriously, a lot) of code. It also ships 
with NullPointerException, which I’m pretty sure scares you 
and your customers.
Let’s dive deeper into the problems in the Java-Android 
world, which are much worse.
 
Android developers are stuck between Java 6 and 7. This 
means that we don’t have features like Lambdas, higher 
Kotlin:  
A Fun Language 
for Android App 
Development 
Kotlin is a statically typed programming language for modern multi-platform 
applications. It is 100 per cent interoperable with Java. Existing libraries and 
frameworks in Java can directly be run on Kotlin, which also happens to be open 
source and freely distributed under the Apache licence. 
50 | JuLY 2017 | OPEN SOuRCE FOR YOu | www.OpenSourceForu.com

Developers
Let’s Try
order functions, and Java Time APIs. We can get these 
features by targeting higher versions of Android, which is 
not a feasible solution. This problem can also be solved 
using third party libraries like RetroLambda, RxJava, etc, 
which is acceptable. 
 
There are also restrictions to adding functionality to 
Android APIs. I’m sure you might have felt like adding 
a particular feature such as being able to check if a string 
is an email or not, but you were restricted from doing so. 
Due to such restrictions, we end up making lots of ‘Util’ 
classes to solve the problem. 
 
Moreover, Android loves inheritance; it requires 
extending things like Activity, etc, which is not totally 
bad but limits flexibility. 
 
In Android, we need to write a lot of boilerplate code to 
achieve something. For example, if you want to write 
a model POJO class, you end up writing 40-50 lines of 
code, depending upon the number of attributes you add. 
 
In Java, all the empty values are set as null for efficiency. 
Android tries to be efficient by setting ‘null’ as the default 
value everywhere, which again comes with NPEs and 
more instances of the app crashing.
 
And there’s a lot more. 
Some big players using Kotlin in production
JetBrains is already using Kotlin to build its IDEs. Its new 
IDE Rider is completely written in Kotlin while new features 
in older IDEs are also written in it. Pinterest, Coursera, 
Atlassian, Evernote, Basecamp and many more players are 
using Kotlin in their Android apps, either partially or fully. 
Gradle is introducing Kotlin as a language for writing build 
scripts, instead of Groovy. 
Syntax of Kotlin
Let’s take a crash course in the basic syntax of Kotlin. We 
won’t cover all of it; hence, I recommend going to the Kotlin 
website and reading the documentation, which is pretty 
good, and also trying out the Kotlin Koans — they are very 
interesting and help you learn faster. 
Declaring variables
Declaring variables in Kotlin is simple; you either write ‘val’ 
or ‘var’ and the name of the variable. Then the type of the 
variable which is optional provided that the compiler is able 
to figure it out. 
// String (Implicitly inferred)
val spell = “Lumos”
// Int
val number = 74
The type can be explicitly specified after the variable 
name with a colon (:).
// Explicit type
var spells: List<String> = ArrayList()
Did you notice there is no ‘new’ keyword? That is because 
there is no ‘new’ keyword in Kotlin.
As we discussed earlier, there are two ways to declare 
variables in Kotlin:
1. val
2. var
In Kotlin, immutable values are preferred whenever 
possible. The fact that most parts of our program are 
immutable provides lots of benefits, such as a more 
predictable behaviour and thread safety.
Functions
fun add(a: Int, b: Int): Int {
   return a + b
}
This is how we write functions in Kotlin. The functions 
start with a ‘fun’ keyword (expecting the function to be fun to 
write for you), the parameters are reversed, and the return is 
typed at the end after the colon ‘:’. Pretty simple, right?
You can even cut the code of the function to this, as 
follows:
fun add(a: Int, b: Int) = a + b
And you can add default values to the function, 
as shown below: 
fun add(a: Int = 0, b: Int = 0) = a + b
You also get a named argument! Did you notice that there 
is no semi-colon in the return statement? That’s because 
Kotlin is a semi-colon free language.
Some amazing features of Kotlin
Let’s look at what makes Kotlin so interesting.
Null safety 
One of the common reasons for Android apps to crash is 
‘NullPointerException’. A lot of time is wasted debugging 
an issue and solving it. The great thing about Kotlin is that 
it comes with null safety, which helps in catching nulls at 
the compile time rather than during a runtime crash. The 
following code shows how this is done:
// Compile time error
val spell: String = null
// OK
val spell: String? = null
www.OpenSourceForu.com | OPEN SOuRCE FOR YOu | JuLY 2017 | 51

52 | JuLY 2017 | OPEN SOuRCE FOR YOu | www.OpenSourceForu.com
Developers Let’s Try
In Kotlin, by default, all variables are marked as ‘not null’ 
unless you explicitly tell the compiler that the variable can 
be null with a question mark (?) operator. This means that in 
order to declare a null variable, you need to add ‘?’ at the end 
of the variable type.
Even if you try to set a non-null variable to null, the 
compiler will ‘yell’ at you.
// Compile time error
var spell: String = “Lumos”
spell = null
// OK
var spell: String? = “Lumos”
spell = null
Using null variables
Directly accessing null variables results in compile time error, 
unless protection has been cast before using these variables. If 
the null is safely handled then the compiler is smart enough to 
know that the variable (the ‘spell’ variable, in our case) won’t 
show an error on using it further in the if block.
// Compile time error
val spell: String? = null
val length = spell.length
// OK
val spell: String? = null
if (spell != null) {
  // Smart cast
  val length = spell.length
}
The idiomatic way is as follows:
// Safe
val spell: String? = null
val length = spell?.length
// Safe with else
val length = spell?.length ?: -1
In the first case, if the value for spell exists then the 
length will be returned, else null will be returned without any 
exception. A point to note over here is the ‘val length’ will be 
a nullable value, which means the compiler will force you to 
check for null value in case of the ‘length’ variable.
The second case is pretty straightforward. Unlike the first 
case, the ‘val length’ won’t be nullable over here because we 
have handled it using the else condition.
String interpolation
String interpolation is a nifty feature, which makes your code 
cleaner. It makes it easier to build long strings.
fun sayHello(message: String) {
   println(“Welcome $message”)
}
fun sayTime() {
   println(“Time: ${System.currentTimeMillis()}”)
}
Using $ allows you to avoid String concat or StringBuilder. 
You can execute a small piece of code using ${}. Internally, it 
uses StringBuilder for performance on the JVM level.
Lambda expressions
Lambda expressions are anonymous functions, which can 
be passed as arguments to other functions.  For people who 
are not aware about Lambda expressions, I recommend 
they read more about them. 
// Java
mButton.setOnClickListener(new OnClickListener() {
        public void onClick(View v) {
            // Your logic
        }
});
// Kotlin
mButton.setOnClickListener {
   // Your logic
}
mButton.setOnClickListener {
   // Using the view parameter
   v -> v.visibility = GONE
}
Higher order functions
Kotlin supports Higher order functions which means 
functions can be passed to another function as parameters.
An example is given below:
fun consume(f: () -> Unit): Boolean {
   f()
   return true
}
The ugly part of receiving functions as arguments is that 
the compiler needs to create classes for them, which can 
impact performance. But in Kotlin, this can be easily solved 
by using the keyword inline. An inline function will have less 
impact on performance, because it will substitute the call to 
the function with its code in compilation time. So it won’t 

www.OpenSourceForu.com | OPEN SOuRCE FOR YOu | JuLY 2017 | 53
Developers
Let’s Try
By: Akshay Chordia
The author is an Indie Android Developer and is well versed 
with the latest mobile technologies. He can be reached at 
akshaychordiya2@gmail.com
require the use of an extra object for this.
inline fun consume(f: () -> Unit): Boolean {
   f()
   return true
}
Extension functions
All of us, at some stage, want to add a new function to an 
existing class but this means modifying the original class 
and doing some more work on the code. This is where 
extension functions come to our rescue. As the name 
suggests, they are used to inject functionality to already 
existing classes without modifying the original class. People 
who know C# may be aware of this feature. These functions 
eliminate the need for creating ‘Util’ classes. 
Here is an example:
fun String.isEmail(): Boolean {
   return Patterns.EMAIL_ADDRESS.matcher(this).matches()
}
The usage is as follows:
val email = “go.kotlin@android.com”
if (email.isEmail()) {
// Do login
}
Extension function with receiver
This feature is unique to Kotlin, and is like a combination of 
an extension function and a higher order function whereby 
you pass an extension function as a parameter to another 
extension function.
inline fun SharedPreferences.edit(func: SharedPreferences.
Editor.() -> Unit) {
   val editor = edit()
   editor.func()
   editor.apply()
}
//Usage
val pref = PreferenceManager.getDefaultSharedPreferences(con  
 
text)
 
pref.edit{
 
putString(“first_name”, “A”)
 
putString(“last_name”, “J”)
 
remove(“last_name”)
}
Smart cast
Smart casting is done by the Kotlin compiler on our 
behalf. It even handles the negative scenario, like if the 
‘view’ is not a TextView.
// Java way
if (view instanceof TextView) {
 
((TextView) view).setText(“Avifors”);
}
// Kotlin way
if (view is TextView) {
 
// Smart casting
 
view.text = “Avifors”
}
There are however a few, minor issues with Kotlin. 
These are:
 
Slightly increased build time
 
Adds Dex Count (6K) to your app
 
Difficult to use the power of functional programming 
as most of us come from object-oriented programming
That said, if you find Kotlin an awesome language, 
then these issues won’t be a deal breaker for you.
With Kotlin, we can enjoy writing the code. It 
shrinks the code base and even reduces the number 
of crashes. There is so much more about Kotlin that 
hasn’t been covered here. For instance, Kotlin v1.1 has 
more new treats for asynchronous programming. 
So go on, get your hands dirty with Kotlin! And have 
lots of fun writing code. There is great tooling support for 
Kotlin in IntelliJ and Android Studio (from v3). 
The latest from the Open Source world is here.
OpenSourceForU.com
Join the community at facebook.com/opensourceforu
Follow us on Twitter @OpenSourceForU
THE COMPLETE MAGAZINE ON OPEN SOURCE



T
esting of any software project is as important as its 
development, and is done to check or validate different 
aspects like functional testing, security testing and 
database testing. The testing process can be manual or 
automated. Manual testing is performed by a person sitting in 
front of a computer, carefully executing tedious and time-
consuming tests. Testing can also be done by using suitable 
automation tools, which makes it more reliable and faster. 
Whether one opts for manual or automated testing depends 
on various factors like the project’s requirements, the budget, 
timelines, the expertise available and suitability. 
A few major reasons why one should opt for automation 
testing are listed below:
 
Manual testing is very time consuming when it comes to 
overall flow testing and covering all scenarios. Regression 
testing also becomes a very tedious task when done 
manually as it needs repetition of the same actions/steps.
 
Manual testing becomes physically tiring. 
 
Manual testing is also less thorough than automation.
 
To err is human; so manual testing is more error prone.
Automation testing may be useful in some cases but, 
sometimes, it may be too high-tech and could wind up 
costing you way more than it’s worth. So it becomes very 
important to choose the correct automation tool for your 
project. The open source tools available include Selenium, 
Robotium, Autoit, Sahi and Sikuli, while some others are 
licensed like HP Unified Functional Tool (UFT) and Tosca, 
for which one has to pay to use.  Choosing open source 
tools can reduce the project cost; however, paid tools have 
many more features, are less time consuming and have 
wonderful support teams. So depending upon your project’s 
requirement, you may opt for any automation tool to 
enhance your testing scope and speed. 
Let’s now discuss an emerging automation tool called Sikuli.
Introducing Sikuli
Sikuli is an open source automation tool that uses image 
recognition to identify and control GUI components. It can 
be integrated with the Selenium Web driver to automate Flash 
content and Java applets.
According to the official site, in the Huichol Native 
American language, Sikuli refers to God’s Eye, implying the 
power to see and understand things unknown. It is basically 
a software framework licensed under MIT2.0 and is cross-
platform. It was started in mid-2009 as an open source 
project by Tom Yeh and Tsung-Hsiang at the User Interface 
Design Group in MIT, USA. Both developers worked with 
Sikuli till Sikuli-X-1.0rc3 in 2012. Then Raimund Hocke 
(aka RaiMan) took over development support for Sikuli and 
maintained it. He developed it further as the SikuliX (where X 
denotes eXperimental) package together with the open source 
community, and continues to maintain it with its help.
Software Automation 
Testing Using Sikuli
Sikuli is a scripting language that can carry out automated software testing of graphical 
user interfaces (GUI) using screenshot images of the software under test.
Developers Let’s Try
56 | JuLY 2017 | OPEN SOuRCE FOR YOu | www.OpenSourceForu.com

Developers
Let’s Try
Sikuli basically automates anything you see on the 
screen of your desktop. It uses image recognition to identify 
and control GUI components. It comes up with basic text 
recognition OCR powered by Tesseract, which can be used to 
search for text in images.
So we can say that using Sikuli is WYSIWYS or What 
You See Is What You Script. 
Sikuli can be used to automate testing through screenshots 
using Power Point slides while code lovers can use scripts 
in IDE to enhance its functionality. This framework is very 
useful in many scenarios like the following:
 
It is best for use on Flash applications. For using the 
Selenium Web driver, we need the source code to develop 
the API. For example, if we need to automate the validation 
of Adobe Photoshop (whether an image got opened or not), 
then Sikuli can be very useful without using any API.
 
It can be very useful in some scenarios where applications 
have a very complex source code yet very simple 
visualisation. So without going into the source code or 
some Xpath, we can automate and test the functionality of 
that application.
 
It is very useful in cases in which the application code 
gets changed frequently but GUI components remain the 
same. In such cases, the functionality of an application 
can be validated using Sikuli.  
 
Sikuli can be very useful for game testers as well. Without 
using an API they can do some sort of testing on it. 
The system requirements for Sikuli are:
• 
Windows XP and later, including Windows 8 and 10 
(32-bit and 64-bit).
• 
Linux/UNIX systems, depending on what 
prerequisites are available (32-bit or 64-bit).
• 
Mac OSX 10.5 and later (64-bit only).
Installation of Sikuli on Windows
The path for downloading and setting up Sikuli is https://
launchpad.net/sikuli/+download.
Download the Sikuli setup.jar file from this link. 
Once the download is complete, click on the sikulixsetup-
1.1.1.jar executable file and follow the instructions to 
install the Sikuli IDE.
Now, to run the Sikuli IDE, open command prompt, 
go to the path where you have just installed Sikuli and run 
runsikulix.cmd. This will open Sikulix IDE homepage.
Sikuli can be divided into two parts:
 
Integrated Development Environment (IDE): This is used 
to make scripts by taking screenshots.
 
API/Sikuli script: This part is used for GUI interaction of 
Jython and the Java library with keyboard or mouse events.
Both these components are part of SikuliX.
Some basic features of Sikuli
Let us go through some basic functions of Sikuli.
Type (): The type command is a very basic command, 
which we can use to enter input or text:
type (“This is Sample text example of type command”)
The type command can also be used with a focused 
image, as while scripting we can focus on a particular area 
of application; then during execution, the type () command 
will search that region first and type there. We can also use 
a modifier (as an option) with the type command to provide 
modifier keys as shown in the example below:
Type (“text”, KeyModifier.ALT)
wait () and waitVanish () method: Both methods are used 
to slow down the script to wait for something or to make 
something vanish. They take an optional duration parameter, 
which can be a number of seconds, or the global parameter 
FOREVER, which will wait until something happens.
Find () and findAll (): These two are other common 
operations in Sikuli to search for things and interact with them. 
They are used when operating on a bunch of similar items on 
the screen. We can use some variable r to store the region as 
shown below:
r = find ( )
And later we can use that variable to call wait (), click (), 
type (), and other functions so that it will restrict the search 
area and, hence, will help in speeding up the script. Selecting a 
region and assigning it to a variable also helps when there are 
multiple similar items on screen and we want to deal with a 
particular one at a time. For example:
r.click ()
Highlight (): This is another basic command used to draw 
a box around a particular region.
Flow control technique in Sikuli
Sikuli uses some sort of control structure like a FOR loop with 
a combination of the findAll () function.  A sample coding of 
the FOR loop is given below:
….
Below_options= find (image1.png)
Checkboxes = below_options.findAll (image.png)
For checkbox in checkboxes:
Checkbox.highlight(1)
….
Similarly, if and while flow control mechanisms can 
also be used, which allows you to do some more complex 
interactions through scripting.
www.OpenSourceForu.com | OPEN SOuRCE FOR YOu | JuLY 2017 | 57

58 | JuLY 2017 | OPEN SOuRCE FOR YOu | www.OpenSourceForu.com
Developers Let’s Try
Use of Python in scripting Sikuli
To enhance the scripting capabilities, we can access the entire 
Python language. As an example, let’s suppose we run our 
Sikuli script unattended, and it has failed. Then by using the 
captureto ( ) method we can save the images that can be used 
to debug script failure.
Accessing Java from Sikuli
To provide some kind of on-screen display, we can use Java 
classes and, in this way, give a GUI representation to our 
script. Sikuli starts by importing the Swing classes, which 
are some of Java’s GUI libraries, and then uses Swing to 
show a borderless window with the designated text over 
everything on the screen for the specified number of seconds 
(the default is 1 second).
Start your first script with Sikuli
To begin with, let’s build a very basic and simple ‘Hello 
world’ example of a testing script for an application WordPad 
to understand what a simple script looks like. Figure 1 shows 
how our complete script will look. 
First of all, start Sikuli, select the editor and write the 
following line of code:
App = App.open(r”C:\Program Files\Windows NT\Accessories\
wordpad.exe”)
This will start the WordPad application using Sikuli.
 Note: The character r in this code is to use space in the 
application path, if any.
Type ‘wait’ in the editor, and then click on the Take 
Screenshot button before selecting the area of screen that you 
would like Sikuli to wait for the text to appear.
Wait (screenshot1.png)
Now add input text to the WordPad application using 
type(), as follows:
type (“Hello, this is my first Sikuli Code!”)
You can check whether the text that you entered appears 
as expected or not, using the wait() command:
By: Aakash Beniwal
The author works with Infosys Limited, Pune, as a testing 
engineer and has over two years’ experience in this domain. He 
can be reached at aakashnavodaya@gmail.com. 
Wait (Hello, this is my first Sikuli Code!)
Now save and run the script. It will open the WordPad, 
and write input text to it. In this way, you can start coding and 
further customise it according to your requirement.
A trick you can try out
If we get two similar buttons like the Save button on the same 
screen and the script is unable to distinguish between the two 
prior to selecting which one is to be clicked, use a larger portion 
of the screen in scripting. This will help Sikuli to get a better 
understanding of the buttons.
Advantages and disadvantages of Sikuli  
Pros:
 
Sikuli is an open source tool, so it is better than tools like 
UFT.
 
It works very well with Flash objects.
 
It is very handy in automation when working with Web 
elements that have dynamic Xpaths and IDs.
 
It has multiple scripting and programming language support 
like Scala, JRuby and Jpython.
 
It is very good with boundary value analysis testing.
 
With proper and smart use of scripting, it is easy to identify 
application crashes and bugs.
 
The Sikuli set-up and use is very simple and easy.
 
Automation testing of mobile applications can also be done 
with the help of emulators.
 
Its integration with Selenium makes it worth using. It can solve 
the browser dialogue box handling problems of Selenium.
 
It can read texts on images with the help of its basic text 
recognition OCR.
 
It supports almost every platform including Windows XP+, 
most Linux flavours and Mac OS 10.5+.
 
Sikuli is very useful in functional testing where input and 
output are predefined; so it can be used efficiently for 
testing the overall behaviour of applications.
Cons:
 
The script cannot be run on the back-end as it needs a 
visible application GUI during the time it is being executed.
 
It is platform and resolution dependent.
 
Running multiple scripts automatically, one by one, is very 
tricky in Sikuli.
 
A slight change in the text label or image of the GUI of the 
application can result in the failure of the script.
 
Maintenance of scripting is very hard if the GUI of the apps 
changes frequently. 
Figure 1: First program code

www.OpenSourceForu.com | OPEN SOuRCE FOR YOu | JuLY 2017 | 59
Developers
How To
HTML5 is a mark-up language that is used to structure and present content on the 
World Wide Web. Mobile apps developed with HTML5 have a distinct advantage 
over native apps. They can be used on any platform and are comparatively 
inexpensive to develop. The future, however, may belong to hybrid apps, which 
combine HTML5 and native apps, to get the best of both worlds.
W
eb traffic analysis tool StatCounter indicated that 
as on March 2017, Indians accessed the Internet 
through their mobiles nearly 78 per cent of the time 
compared to using their desktops to go online just 22 per cent 
of the time. This has altered the way in which companies, 
content providers and firms interact with their customers. 
They are now shifting more towards mobility to usher in new 
trends in their businesses and to reap the maximum profits. 
But there is one bottleneck that can hurt the transition to 
mobile phones and that is the prevalence of different mobile 
operating systems. Though Apple’s iOS and Google’s Android 
OS are the market leaders, other operating systems like 
BlackBerry and Windows can’t be written off straight away. 
Different OSs require the use of different languages that the 
respective platforms support (iOS uses objective C whereas 
Android uses Java). This also implies developers and testers 
must have the skills needed to work in the different languages. 
Additionally, updates have to be rolled out separately for 
the individual OSs, which adds to the cost borne by the app 
development companies.
HTML5 has proven to be a boon to the mobile application 
development market. HTML5 apps have many advantages 
over native apps. They are cheaper, easier to create and 
maintain, and can be deployed on different platforms without 
any additional costs. Also, users do not need to install new 
updates to use the latest functionalities incorporated in the 
app. Organisations, these days, are looking at HTML5 mobile 
app development to streamline implementation, as well as to 
Developing HTML5 and Hybrid 
Apps for Mobile Devices
apps for 
mobile devices
Developing

60 | JuLY 2017 | OPEN SOuRCE FOR YOu | www.OpenSourceForu.com
Developers How To
reduce costs and complexity. Also, Cascading 
Style Sheets (CSS) and JavaScript can be 
combined with HTML5 to provide a high-end 
UI experience and validation support.
How HTML5 mobile application 
development works
HTML5 is a mark-up language used to 
structure and display content on websites. 
It provides ‘write-once-run-anywhere’ 
methodology, which makes distribution 
much easier than the native apps. It contains 
a set of Web pages optimised for mobile 
devices and to deliver high-end video 
and audio data manipulation capabilities, 
graphics handling and  offline support. 
It also adds elements like form controls, 
multimedia components and other APIs that 
support geo location services and drag-and-
drop operations. Almost all popular browsers 
support HTML5, which enables developers 
to implement this language across different 
platforms and devices. 
Although HTML is supported by almost 
all browsers, a slight difference in the mobile 
devices and browsers may affect how an 
application works and its look and feel. To 
overcome this, apps are developed at the level of the lowest 
possible denominator so that they can work effortlessly on 
high-end versions. Now let’s look at how the development of 
HTML5 powered mobile applications takes place.
Generally, the development takes place with the help 
of various frameworks available in the market, each with 
its own pros and cons, which we will discuss later. These 
frameworks contain various libraries of CSS and JavaScript 
files that app developers can use for rapid development. 
Some frameworks also have the capability to handle most 
of the issues arising out of the apps on their own. HTML is 
based on the Document Object Model (DOM), which you 
can think of as a tree-like logical structure used to access and 
manipulate a document or Web page. Sometimes, navigating 
through a complex and lengthy DOM can slow the 
application; so these frameworks can help programmers to 
improve the performance by recycling and reusing different 
DOM components. The use of a framework also makes it 
effortless to implement various functionalities such as screen 
transitions or list scrolling, which depend on JavaScript. As 
a result, it becomes easier for beginners also to develop apps 
that can provide a high-end user experience (UX). 
Hybrid apps: The future
Although HTML5 mobile app development is catching on 
fast and has come a long way, it still cannot compete with 
some of the capabilities provided by native apps, like multi-
Native Container
Application Files
WebView
NativeUI
Plugins
Operating System
Figure 1: The same HTML5 code 
will work on all platforms
Figure 2: Architecture of hybrid apps
touch gestures and smooth integration with 
built-in mobile components, such as GPS, 
cameras, accelerometers and contacts. Offline 
storage and security are the other limitations 
of HTML5 apps, which also lose out to native 
apps on performance because the latter are 
usually packaged with machine code.
To get the best of both worlds, we can 
combine both HTML5 and native apps to 
build hybrid apps. A simple hybrid app is 
built using HTML5, JavaScript and CSS, 
but is wrapped inside a thin native container 
known as Webview (which you can think of 
as a windowless browser that’s configured 
to run full screen). This enables mobile 
applications to access a device’s hardware 
capabilities like the camera, etc, through 
JavaScript APIs like PhoneGap. Hybrid apps 
can also support offline operations and multi-
touch gestures. 
Though they look and feel like native 
apps due to being wrapped inside containers, 
hybrid apps still lag behind native apps a little 
bit in terms of performance due to the extra 
layers of abstraction involved in building 
them. You already know that native apps are 
installed on the device from the OS’ app store 
and HTML5 apps are viewed through Web browsers. But it 
may interest you to know how hybrid apps are installed or 
used. There are actually two ways -- local and server -- to 
implement hybrid apps in the device.
 
Local: Just like a native app, one can package the source 
code containing HTML and JavaScript files inside the 
mobile binary. 
 
Server: Alternatively, one can also use a server to 
implement the mobile application.
If hybrid apps are well written, it is extremely difficult to 
determine how they have been developed. Since these apps 
combine the best of two worlds, they are definitely the future 
in mobility and can help organisations reduce the costs and 
liabilities associated with developing native apps.
Frameworks for hybrid app development
Having looked at HTML5, native and hybrid apps and their 
pros and cons, let’s take a look at the top eight frameworks 
that you can use to develop your own hybrid mobile 
applications.  The following top eight have been selected 
based on my own experience and usage.
1. Ionic: If you are a beginner in the world of hybrid mobile 
application development, then the Ionic framework is the 
way to go. It is an open source framework (protected under 
the MIT licence) with a focus on performance and gets 
regularly updated, keeping it ahead of its competitors. It 
can support iOS, Android and Windows platforms. One of 

www.OpenSourceForu.com | OPEN SOuRCE FOR YOu | JuLY 2017 | 61
Developers
How To
its striking features is the presence of AngularJS at its 
core and support for the Syntactically Awesome Style 
Sheets (SASS) CSS extension. To interact with mobile 
hardware and other device capabilities, it makes use of 
Apache Cordova. To learn more about this framework, 
you can visit https://ionicframework.com/.
2. Onsen UI: This is a comparatively new framework 
that is moving up on the development scene very fast. 
It is also open source and licensed under Apache, 
and has two versions -- Onsen UI and Onsen UI2. It 
is somewhat similar to the Ionic framework in terms 
of how it works with AngularJS, but Onsen UI2 is 
really useful because it is platform agnostic. If you are 
comfortable with jQuery, you can use jQuery too. It is 
a very flexible and performance driven framework. For 
more information, you can go to https://onsen.io/.
3. jQuery Mobile: This is one of the oldest frameworks 
still in use, having been first released in 2010. It is 
an HTML5 mark-up based framework. This means 
that most of the work is done through HTML5. Apps 
designed using this framework will run on anything 
that supports the HTML5 standard. It has a lot of 
documentation and online support available. Though 
it has some performance concerns, it still is the most 
widely used after the Ionic framework. You can read 
more about it on https://jquerymobile.com/.
4. Sencha Touch: This is one of the oldest licensed tools 
and is mostly used by enterprises for building cross-
platform, high-performance mobile apps quickly and 
effectively. It uses Apache Cordova and PhoneGap to 
support device hardware. It comes packaged with ready-
to-use widgets that can work on all popular platforms 
like iOS, Android, etc. You can visit https://www.sencha.
com/ for more information on Sencha Touch.
5. Intel XDK: This is new in the hybrid mobile app 
framework space but due to its ordered workflow and 
ready-to-use templates, it is becoming the favoured 
choice for developing cross-platform apps. It contains 
all the tools required, from the development to the 
testing of HTML5 hybrid apps.
6. Kendo UI: This is supported by Telerik. It makes 
By: Vinayak Vaid
The author works as an automation engineer at Infosys 
Limited, Pune. He has worked on different testing technologies 
and automation tools like QTP, Selenium and Coded UI. He can 
be contacted at vinayakvaid91@gmail.com.
use of jQuery for development. It can also interact with 
AngularJS and the Bootstrap UI framework. It is available 
as an open source as well as a commercial version, though 
the former has fewer features. Do visit http://www.telerik.
com/kendo-ui for more information on Kendo UI.
7. Framework 7: This is a totally free and open source 
framework licensed under MIT and used only for 
developing iOS and Android apps. It is not a cross-platform 
framework but due to its look and feel, seems like a native 
app. It is being widely used nowadays. You can find a 
number of apps on the Apple App store and Google Play 
store that have been developed using Framework 7. If you 
wish to read more about it, visit https://framework7.io/.
8. Famo.us: This framework is mostly used when you have 
to deal with high-end rendering and graphics for games. It 
is an open source framework and can provide hybrid apps 
with the look of native apps. Visit https://famous.co/ for 
more information.
Making the right choice
Mobility is definitely the future of technology. With the entry 
of so many technologies and frameworks, the issue is of 
making the right choice when developing mobile applications. 
This choice depends on various aspects such as the mobile 
platforms to target, the distribution of the applications, cost 
considerations, the technical competence of the development 
team,  mobile hardware capabilities, security, the target 
audience, the user experience and the time to market. 
Watching how HTML5 has advanced over the past few 
years, one can definitely speculate that the future is bright for 
Web developers who want to break into the world of mobile 
application development. 
Figure 4: Source code in the jQuery Mobile framework with the 
‘data-role’ and ‘data-theme’ members 
Figure 3: Source code snippet in the Ionic framework

62 | JuLY 2017 | OPEN SOuRCE FOR YOu | www.OpenSourceForu.com
Developers How To
C
ross-platform mobile apps work on multiple operating 
systems with a single code base. There are two types 
of cross-platform apps.
Native cross-platform apps: Most mobile operating 
systems have their own SDKs to create mobile apps. 
These work on specific programming languages. Android 
developers, for example, prefer the Java language while 
developers working on iOS prefer the Objective C and Swift 
programming languages. In cross-platform app development, 
we use the APIs provided by the native SDK in another 
programming language which is not supported by other 
vendors. With unified APIs,  code written in one particular 
language will work on multiple platforms  -- that’s how the 
native cross-platform app works.
In this kind of approach, the final app still uses the native 
APIs, and it achieves nearly the same performance as native 
apps. Nowadays, the GUI plays a major role in mobile app 
development. It is somewhat complex to implement rich GUIs 
using unified APIs in native cross-platform apps. For this reason, 
many developers prefer hybrid HTML5 cross-platform apps.
Hybrid HTML5 cross-platform apps: Almost 60 per cent of 
the code of any mobile application deals with the GUI. Android, 
iOS and Windows all have browser components in their SDKs. By 
leveraging these components, developers can use HTML5 to design 
mobile applications. So the final application is built using the native 
framework and HTML/JavaScript in a Web view. This is the reason 
why it’s called a hybrid cross-platform app. 
Figure 1 gives a comparison between native apps and 
hybrid apps.
One of the limitations of hybrid HTML5 based mobile 
applications is that their look and feel is not as good as that of 
native apps. Onsen UI is a UI framework that resolves this issue 
by providing a rich UI, which provides a look and feel that’s very 
similar to that of a native app.
Onsen UI is a Web component driven UI framework for 
Cordova/PhoneGap apps. It supports all kinds of JS frameworks 
and works with AngularJS 1 and 2. 
Popular hybrid frameworks
There are many hybrid mobile frameworks available in the market 
but none of them provide exactly the same look and feel as the 
native apps. All do the same things based on the requirements, so 
developers can choose from these frameworks.
Let’s take a look at how a few popular hybrid frameworks 
compare with each other.
Ionic: This is a new entrant, and is quick and reliable. It is 
based on the AngularJS framework. It can be used just for hybrid 
mobile applications, while a few functionalities will also work with 
ordinary Web applications. It does not have a very large community 
Today, a mobile app is a must for organisations that want to ensure a strong presence 
in a challenging market. There are different popular mobile operating systems and each 
has its own user base. To develop mobile applications separately for all these different 
operating systems and maintain them requires a lot of resources, in terms of both time and 
cost. Hence, many organisations are shifting towards cross-platform mobile application 
development. Onsen UI is a framework that enables such app development.
Using the Onsen UI with Monaca IDE for 
Hybrid Mobile App Development
Figure 1: Native apps vs hybrid apps

www.OpenSourceForu.com | OPEN SOuRCE FOR YOu | JuLY 2017 | 63
Developers
How To
www.OpenSourceForu.com | OPEN SOuRCE FOR YOu | JuLY 2017 | 63
backing it, yet it is a somewhat stable framework now. However, 
as AngularJS 2 has come out into the market, designers are 
continually rolling out improvements and new features.  
Onsen UI: To a great extent, this is very similar to 
Ionic. Everything said about Ionic also applies to Onsen UI. 
One of its plus points is that it can be used for classic Web 
development. Currently, it has a small community, which is 
growing slowly.
Kendo UI: Kendo UI is built around the JQuery framework. 
It can also be used with AngularJS. There are two versions -- one 
is free and the other is the commercial version. It also has theme 
builder support, offering many themes that look similar to native 
themes. It is used for mobile and classic Web development.
Sencha Touch: Sencha Touch uses vanilla JavaScript 
as the core (ExtJS). It used to be faster, but recent ExtJS 
changes are making it slower. It is a commercial product 
with a very good IDE.
JQuery Mobile: This is an HTML5 mark-up driven 
framework. JQuery is the additional layer of JavaScript. It 
is very easy to understand and implement. It is very slow 
compared to other frameworks, though, but has the support of 
a very large community. 
Famo.US: Famo.US is built around AngularJS and 
follows a completely different approach for mobile 
development. It is a 2D/3D engine for game development and 
currently is one of the faster frameworks. But there are very 
few samples available and very little community support.
AngularJS is difficult to understand compared to JQuery. 
Many people like the JQuery framework, but it is slow 
compared to other frameworks.
Monaca IDE, a great tool for Onsen UI 
Monaca is a development platform for hybrid mobile app 
development with PhoneGap/Cordova. It is very easy to use 
and integrate with existing workflows. It also supports Cloud 
IDE, remote online build and CLI debugger. 
Figure 4: Choosing a framework and version
Figure 2: Installing Monaca CLI, an Onsen UI toolkit
Figure 3: Create a project
Installing Onsen UI
There are different ways to work with Onsen UI. Let’s first 
deploy it with the Monaca CLI.
 
First, we need to install Node.js from https://nodejs.org/en/. 
 
The next step is to install the Monaca CLI using npm.
npm install -g monaca
 
By installing the Monaca CLI, we will get a command 
line interface for Onsen UI development.  For GUI mode, 
install the Monaca Toolkit from the link https://monaca.
io/localkit.html  
 
Now, let’s create a project based on Onsen UI:
monaca create helloonsenUI
Choose any framework that you want for development. 
Once you have selected one, download the appropriate 
template and create the project.
 
We will now run this project on the device by using the 
following commands:
cd helloonsenUI
monaca preview # preview in the browser
monaca debug # Run on the device in debugger
monaca remote build # Build the app
monaca remote build android # Build the android app (Choose 
any OS, which you want to build)
Figure 5: App preview

64 | JuLY 2017 | OPEN SOuRCE FOR YOu | www.OpenSourceForu.com
Developers How To
Figure 7: Monaca build started
Figure 8: Monaca build completed
Figure 9: Monaca Cloud
Figure 10: Application code on Monaca Cloud
By: Maulik Parekh
The author has an M. Tech degree in cloud computing 
from VIT University, Chennai. He can be reached at 
maulikparekh2@gmail.com. Website: https://www.linkedin.
com/in/maulikparekh2.
[1] https://onsen.io/v2/docs/guide/js/
[2] https://onsen.io/
[3] https://monaca.io/
References
As you can see in Figures 5,6,7 and 8, the project has 
been successfully updated in the cloud and we have an APK 
file created, which we can install on an Android mobile.  
Likewise, we can create it for other operating systems too.
Now we will log in into Monaca Cloud to check if the 
application has been deployed or not.
From Figure 9, we can see that the app is deployed on the 
cloud. We can do the entire app development online in Monaca 
Cloud and also build it online. Such is the power of Monaca. 
We have seen the basic ‘hello world’ version of the Onsen 
UI using the Monaca CLI. There are many samples available 
in the links given under ‘References’ below. The link https://
onsen.io/samples/  has more examples of Onsen UI.  
Figure 6: Monaca debug

www.OpenSourceForu.com | OPEN SOuRCE FOR YOu | JuLY 2017 | 65
Developers
How To
Couchbase Lite is an embedded NoSQL database for mobile devices that 
runs locally on the device with full CRUD and query capabilities. In this article, 
we discover how to integrate Couchbase Lite with Android Studio. 
A
ndroid for mobile devices currently comes with an 
inbuilt local database—SQLite. This is an RDBMS 
based lightweight database that is available by 
default on all Android operating systems and provides CRUD 
operations to efficiently 
power your apps. SQLite 
is really a great choice 
when the requirement 
is just to have a simple 
database for your app to 
manage structured data. 
However, when the need is 
to store semi-structured or 
unstructured data and also 
to handle complex queries 
at scale without worrying 
about the schema of tables, 
then SQLite may not suit 
all of the developer’s requirements. A NoSQL database can 
be a better fit with these scaling requirements. Comparisons 
between an SQL and a NoSQL database have fuelled 
many debates, but both complement each other rather than 
compete with each other. 
In this article, we start by discussing the general 
database requirements for mobile devices, followed by 
NoSQL’s prominence in today’s mobile world and, finally, 
look at integrating a NoSQL database called Couchbase Lite 
with Android Studio.
Databases for mobile devices
Deciding on a database for mobile devices requires us to 
consider various factors like memory constraints, the user 
experience, a lightweight UI, etc – parameters that are 
very different compared to what would be required for 
a desktop or Web environment. So before we jump into 
integrating Couchbase Lite with Android, let us first check 
out the various requirements for databases in the mobile 
environment, which are listed below.
 
Unlike desktops or servers, mobile devices tend to have a 
lower battery life and relatively slower CPUs. Databases 
should hence not be performance intensive and should 
be able to effectively perform frequent operations like 
searches and updates. 
 
Mobile databases should have a smaller memory footprint. In 
certain ways, higher memory requirements would also lead 
to increased CPU cycles, where the kernel tries to intensively 
search for available memory space in the RAM. Lower 
footprint demands not only lead to lower CPU cycles but 
also ensure other mobile apps don’t get impacted.
 
All the winning mobile 
apps are high performance 
with a fast loading time. 
Apps that freeze constantly 
are always on the 
backbench. Data consistency 
is another requirement when 
talking about these local 
databases on the mobile. 
If one were to go with a 
distributed database, the data 
may become inconsistent 
with respect to its remote 
counterpart if not taken 
care of, and the device might even discover this only on 
connecting to the Internet. 
A mobile developer with a cloud based database backend, 
like say Firebase, needn’t worry about most of these 
constraints and requirements, but needs to include these 
factors in the equation when opting for a local database.
NoSQL’s relevance in the mobile world
With the increased usage of mobile devices, a tremendous 
amount of data is being generated these days. This fact, 
clubbed with technology proliferation into new spaces like 
mobility, IoT and analytics, has led to a demand for mobile 
devices and apps to handle this high volume of data at a high 
speed. Besides, the nature of data (especially when it comes 
from IoT devices for which data exchange is in real-time) is 
continuous and either semi-structured or the application needs 
to cater or adapt to various schemas. Some of the fundamental 
philosophies that NoSQL brings in address these challenges in 
the mobile space, as discussed below.
 
The very fact that NoSQL is schema-less will help 
developers handle the data that lacks schema or structure. 
Besides, this property will also let them scale to changing 
or evolving requirements of data. The change in schema 
or structure could be done with ease at any point in time, 
in an independent way, without affecting the existing 
code. All of this will directly result in the agile delivery 
of apps, a quick turnaround time to market as against 
Why We Should Integrate Couchbase 
Lite with Android Studio

66 | JuLY 2017 | OPEN SOuRCE FOR YOu | www.OpenSourceForu.com
Developers How To
the time consuming process of design considerations, and 
limited scope for scalability and modularity in code when 
using a relational database.
 
The distributed architecture of NoSQL databases ensures that 
they perform better than RDBMS. Besides, NoSQL doesn’t 
have complex join operations and normalised data nor does it 
include complex queries. These factors give it the upper hand 
when it comes to database performance. Effective performance 
directly results in the better user experience of mobile apps 
because of the reduced load time of UI components and 
activities. This directly improves the battery life too.
 
Security is another aspect that should never be ignored 
while trying to achieve all these goals. The databases 
should be able to communicate to the server over secure 
channels. Besides, the channels that communicate with 
mobile devices over the Internet demand low latency for 
an improved mobile user experience. Also,exchange of 
data on the network should be lightweight to meet these 
performance requirements. 
All that said, we still may not be able to eliminate SQL 
databases which complement NoSQL in many ways. NoSQL 
doesn’t guarantee atomicity or integrity of data which an 
RDBMS is capable of. So, it is the developer’s needs at the end 
of day that decide which database to go with.
Integrating Couchbase Lite with Android Studio
Couchbase Lite is an open source project available under 
Apache License 2.0. It is an embedded JSON database that can 
work as a standalone, in a P2P network, or as a remote endpoint 
for a Sync Gateway. In this article, we explain how to power 
your Android apps with Couchbase Lite. 
Before getting onto integration, let us check out a few key 
features about this database.
 
Couchbase Lite stores and manages all the data locally on 
your mobile device in a lightweight JSON format.
 
Considering the requirements for a mobile database, 
Couchbase Lite qualifies with its lower memory footprint, 
built-in security with user authentication, AES based data 
based encryption, and transport to the server through TLS.
 
Couchbase Lite provides CRUD and query support through 
native APIs, and also works well with existing REST 
architectures with its programmatic access through REST API.
 
Stream and batch APIs from Couchbase Lite enable the 
transfer of real-time data in batches with low network 
latency and throughput, thereby addressing the exact 
demands of mobile apps.
Let us now go through the steps of installing Couchbase 
Lite and the other basic operations to get started.
It is assumed that readers are already conversant with 
the Android Studio IDE for developing Android apps. 
Integrating Couchbase dB with Android is straightforward. 
You could start off by adding the dependency elements 
given below in your application’s build.gradle: 
     dependencies {
compile 'com.couchbase.lite:couchbase-lite-android:+'
     }
In the Java part of the application, you would need the 
following basic set of packages to start with:
import com.couchbase.lite.*;
import com.couchbase.lite.android.AndroidContext;
Now that you are all set to use CouchBase APIs in your 
Android app, I’d like to illustrate a sample code, used for 
creating a database, and do an insert, update and delete of a 
document in it as mentioned in (1), (2), (3) and (4), respectively.
// (1) Get the database or create it if it doesn’t already 
exist. 
Manager manager = new Manager(new JavaContext(),Manager.
DEFAULT_OPTIONS); 
Database db = manager.getDatabase("couchdB"); 
// (2) Create a new document (a record) in the database. 
Document doc = db.createDocument(); 
Map properties = new HashMap(); 
properties.put("firstName", "OSFY"); 
doc.putProperties(properties); 
// (3) Update a document. 
doc.update(new Document.DocumentUpdater() { 
    @Override 
    public boolean update(UnsavedRevision newRevision) { 
      Map properties = newRevision.getUserProperties();    
      properties.put("firstName", "Johnny");   
      newRevision.setUserProperties(properties); 
      return true; 
    }
}); 
// (4) Delete a document. 
doc.delete();                                                                   
By: Shravan I.V.
The author currently works as a software developer at Cisco 
Systems India and is interested in open source technologies. 
You can reach him at iv.shravan@gmail.com. 
[1] https://developer.couchbase.com/mobile/
[2]  http://www.thegeekstuff.com/2014/01/sql-vs-nosql-db 
[3] https://android-arsenal.com/details/1/841
[4] https://developer.couchbase.com/documentation/
mobile/1.2/develop/guides/couchbase-lite/native-api/
database/index.html 
References

www.OpenSourceForu.com | OPEN SOuRCE FOR YOu | JuLY 2017 | 67
Developers
Overview
Mobile applications are either pre-installed on phones by the manufacturer, or delivered 
as Web applications using server-side or client-side processing to provide an application-
like experience within a Web browser. Developing apps for different platforms, however, 
is a challenge, which can be addressed by using some of the best mobile application 
development frameworks discussed in this article.
F
or a large number of people, developing a high-end 
mobile application is a cumbersome task that requires 
a lot of programming skills and effort. Today, it is 
more important that the app you develop runs on all popular 
mobile operating systems. Hybrid mobile app developers are 
now able to write code once and build mobile applications 
that run on the main platforms with no extra effort. The 
same code for the mobile application that runs on Android 
and iOS can be reused for progressive Web applications and 
even desktop applications. 
A framework is regarded as a complex software 
development environment. It includes many sub-components 
that help developers to create their respective apps. Sub-
components may include tool sets, compilers, debuggers, 
application programming interfaces, different code libraries 
and many other components. A framework creates a strong 
base for mobile applications. Its professional usage simplifies 
the entire app development life cycle. Developers these days 
prefer to use frameworks to write apps instead of writing an 
application from scratch and dealing with thousands of lines 
of code to make the app multi-platform compliant.
In order to develop a mobile app that runs on multiple 
mobile operating system platforms, choosing the desired 
framework can be a complex task. Here is a list of some 
of the best open source mobile application development 
frameworks which are highly effective, helping developers 
to write apps in an easy manner.
1. Framework 7
Framework 7 is regarded as a full-featured mobile app 
development framework for  iOS. It is a free and open source 
mobile HTML framework for developing mobile apps. Now it 
also offers Android app development support and has become 
an indispensable prototyping apps tool that builds a working 
app prototype very quickly. 
Framework 7 gives app developers the opportunity to 
create iOS and Android apps with HTML, CSS and JavaScript 
in an easy and concise manner. It is not compatible with 
all platforms; rather, it mainly focuses on iOS and Google 
Material Design to offer the best-in-industry experience. 
Features
 
Easy to use: Framework 7 is easy to understand. It 
requires just basic knowledge of HTML, CSS and JS. It 
doesn’t require developers to write custom tasks that will 
be converted by JavaScript to something else. You can 
write the entire app in simple HTML, and expect to get 
what you write.
 
iOs-Specific: Framework 7 is iOS-specific. It provides 
an incredible platform for developers to realise all its 
features, which range from UI elements visualisation to 
complex animation and touchscreen interactions.
 
UI components: Framework 7 is bundled with ready-to-
use UI elements and widgets like modals, pop-ups, action 
sheers, pop-overs, list views, media lists, tabs, side panels, 
layout grids, pre-loaders, form elements, etc. 
Eight Easy-to-Use Open Source 
Hybrid Mobile Application 
Development Platforms

68 | JuLY 2017 | OPEN SOuRCE FOR YOu | www.OpenSourceForu.com
Developers Overview
 
Easy to customise: 
Framework 7 provides 
everything in a simple manner 
— all styles are divided  into 
smaller units. Lesser files are 
required to bring out custom 
layout styles.
 
Other features: Framework 
7 provides many other 
features. It has native 
scrolling, is library agnostic, 
and has page-transition 
animation and multiple views 
support. It has hardware 
accelerated animation 
using CSS3, and routes 
pages by using a combination of XHR, caching, browser 
history and preloading.
Official website: https://framework7.io/
Latest version: 1.6.4
2. Ionic Framework
Ionic Framework is an open source software development kit 
(SDK) for hybrid mobile app development. It is developed 
on top of Angular.js and Apache Cordova, and provides 
developers with state-of-art tools and services for developing 
apps using various Web languages like CSS, HTML5 and 
Sass. Ionic offers a library of mobile-optimised HTML, 
CSS and JS CSS components, gestures and tools, and comes 
with pre-loaded components. Users can develop apps and 
customise them for Android or iOS, and deploy them via 
Cordova. Ionic includes mobile components, typography, 
interactive paradigms and an extensible base theme. 
The Ionic Framework is a 100 per cent free and open 
source project, released under the MIT licence. It will always 
remain free to use, and is powered by a massive world-wide 
community. The framework has over 120 native features like 
Bluetooth, HealthKit, fingerprint authentication, and more, with 
Cordova and PhoneGap plugins, and TypeScript extensions.
Figure 1: Framework 7 GUI
Figure 2: Ionic Framework interface
Figure 3: The jQuery user interface
Official website: https://ionicframework.com/
Latest version: 2.0.0
3. jQuery Mobile
jQuery Mobile is regarded as a robust framework to 
develop cross-platform mobile apps. It supports a wide 
range of platforms for app development like desktops, 
smartphones, tablets and e-book reading devices like Kindle. 
It even integrates various features like semantic mark-up, 
progressive enhancement, themable design and PhoneGap/
Cordova support. One of the many features this framework 
offers is ThemeRoller, which helps you to create a unique 
design for your app.
jQuery offers many documents that will help you get in 
touch with this flexible framework. jQuery Mobile is built on 
the jQuery base. Therefore, you will have no problems with 
this framework if you are familiar with the jQuery syntax. 
The framework is powered by the Ajax navigation system. It 
ensures smooth animation of pages without any errors.
jQuery is a module-based framework, which allows you 
to create as many custom builds as you need.
Features
 
Compatible with all major desktop browsers as well as 
all major mobile platforms, including Android, iOS, 
Windows Phone, BlackBerry, WebOS and Symbian.
 
As it is built on top of jQuery core, it has a minimal learning 
curve for people already familiar with jQuery syntax.
 
Has a theming framework that allows users to create 
custom themes.
 
Limited dependencies and lightweight; hence, optimises speed.
 
The same underlying code base will automatically scale to 
any screen.
 
HTML5-driven configuration for laying out pages with 
minimal scripting.
 
Ajax-powered navigation with animated page transitions 
that provide the ability to create semantic URLs through 
pushState.
 
UI widgets that are touch-optimised and platform-
agnostic.
Official website: http://jquerymobile.com/
Latest version: 1.4.5

www.OpenSourceForu.com | OPEN SOuRCE FOR YOu | JuLY 2017 | 69
Developers
Overview
4. Kendo UI
Developed by Telerik, Kendo UI is basically an HTML5 based 
mobile app development framework for developing hybrid 
mobile applications. It primarily makes use of jQuery and 
has over 70 jQuery widgets built-in. It has a simple UI for 
programming, a rock-solid data source, out-of-the-box themes, 
an MVVM framework, and a lot more. Kendo UI is an open 
source framework for developing Web ad mobile apps.
Kendo UI Complete incorporates many products 
like Kendo UI Professional, as well as UIs for ASP.net 
MVC,  JSP and for PHP.
 
Features
 
Includes 70+ UI components like data grids, drop down 
menus, buttons to advanced components like Gantt 
Charts, spreadsheets, PivotGrid and Maps.
 
Easy and powerful data binding to local and remote data, 
offline storage and support for popular Web services like 
REST and OData.
 
Integrates and supports Angular JS directives.
 
Integrates grid-layout frameworks like Bootstrap and 
Zurb Foundation for developing cross-platform Web apps 
customised to the desktop, phone and tablet.
 
Easy to learn and use, and works with all the latest Web 
browsers.
Official website: http://www.telerik.com/kendo-ui
Latest version: 2017 SP1
5. NativeScript
NativeScript is a powerful open source framework for 
developing Android and iOS apps. Mobile apps based 
on NativeScript are built using various programming 
languages like JavaScript and TypeScript. It directly 
supports the Angular JS framework. NativeScript is 
basically designed to allow developers to re-use significant 
amounts of code when switching between developing Web 
and mobile phone apps.
NativeScript and all the required plugins are installed using 
the package manager, npm. Projects are created, configured 
Figure 4: Kendo UI 
Figure 5: NativeScript
and compiled via the command line. Platform independent user 
interfaces are defined using XML files. NativeScript then uses 
the abstractions described in the XML files to call the native 
elements of each platform. Application logic developed in 
Angular2 and TypeScript can be developed independent of the 
target platform as well. A NativeScript mobile application is 
built using the Node.js runtime and tooling.
Features
 
Provides support for the development of robust and 
professional charts, calendars and graphs.
 
Integrates XML for rich application development.
 
Has full TypeScript support for .NET developers to enable 
them to use generics, enums, interfaces, static analysis 
and compile-time errors.
 
Developers can choose their own architecture to use 
either JavaScript, TypeScript or Angular based application 
development.
Official website: https://www.nativescript.org/
Latest version: 3.0
6. Onsen UI
Onsen UI is an open source framework for HTML5 based 
hybrid mobile app development based on PhoneGap/
Cordova. It is also compatible with Angular and jQuery. 
Onsen UI provides a comprehensive set of Web-based UI 
components and features like a two-column view of tables 
and material design for unique app development. It also has 
a drag-and-drop GUI tool which is under development at 
Tokyo-based Asial, which has also developed Monaca.
Features:
 
Fastest platform to develop HTML5 based hybrid mobile 
apps using JavaScript, HTML and CSS and also in 
making real-time use of Cordova.
 
Has tools like a powerful CLI and LocalKit to perform 
debugging tasks.
 
Support for Android Material Design with automatic styling, 
including full support for making rich quality iOS apps.

70 | JuLY 2017 | OPEN SOuRCE FOR YOu | www.OpenSourceForu.com
Developers Overview
By: Prof. Anand Nayyar
The author is an assistant professor in the department of 
computer applications and IT at KCL Institute of Management 
and Technology, Jalandhar, Punjab. He loves to work and 
research on open source technologies, cloud computing, 
sensor networks, hacking and network security. He can be 
reached at anand_nayyar@yahoo.co.in. You can watch his 
YouTube videos at Youtube.com/anandnayyar.
Figure 6: Onsen UI
Figure 7: React Native UI
Figure 8: Intel XDK user interface
Official website: https://onsen.io/
Latest version: 2.0
7. React Native
React Native is an open source spin off of Facebook’s React 
JavaScript framework, which famously replaced the earlier 
HTML5 Foundation. As the name suggests, this high-end, 
iOS-focused program is more of a native app package than a 
cross-platform framework. But with its new Android support, it 
loosely fits our requirements, as you can essentially write once 
in JavaScript and port to both platforms. Currently, only OS X 
desktops are fully supported, although there are experimental 
Linux and Windows versions for Android development.
Features:
 
Full integration and strong platform support for developing 
Android and iOS apps. Facebook used React Native to build its 
own App Manager app, both for Android and iOS platforms.
 
React Native’s building blocks are reusable ‘native 
components’ that compile directly to it.
 
It is easy to incorporate React Native components into 
applications code.
 
React Native is more UI focused, making it more like a 
JavaScript library rather than being a simple framework.
 
Enables efficient app development and has strong support for 
third party plugins, in addition to less memory utilisation.
rich, interactive and responsive apps that run on any device. 
This framework offers a complete range of tools that support 
development, emulation, testing, debugging and publishing. 
Recently, the developers made some big changes by supporting 
all the capabilities like developing mobile HTML5 apps 
(including Apache Cordova) for Android, iOS and Windows 
10 UAP. They have now included software development 
capabilities for Node.js-based, on-board, IoT apps too.
Intel XDK provides a live preview of the app on the 
connected device while you are developing it, along with 
many other useful tools. You can create apps using a 
drag-and-drop approach, although this does create a lot of 
unnecessary code.
 Features:
 
Code hinting, code completion, emulator and device 
testing via Intel App Preview Mobile app.
 
Cordova and third party plugins support, as well as 
support for template and expanded device APIs.
 
Drag-and-drop UI layout builder, one-click store 
deployment and remote server compiling.
Official website: https://software.intel.com/en-us/intel-xdk 
Latest version: Version 3977  
[1] https://framework7.io/
[2] https://ionicframework.com/
[3] http://jquerymobile.com/
[4] http://www.telerik.com/kendo-ui
[5] https://www.nativescript.org/
[6] https://onsen.io/
[7] http://www.reactnative.com/
[8] https://software.intel.com/en-us/intel-xdk
References
Official website: http://www.reactnative.com/
Latest version: 0.43
8. Intel XDK
Intel XDK allows you to build cross-platform apps for each and 
every store. It includes Web services and plugins for content-

www.OpenSourceForu.com | OPEN SOuRCE FOR YOu | JuLY 2017 | 71
Developers
Overview
Have you ever imagined a day without your mobile (rather, smart mobile) 
phone? Probably not, since mobile applications have now become part and 
parcel of our lives. Read on to learn more about the popular mobile app 
development languages and frameworks.
I
f we flash back to 1973, when Martin Cooper and John F. 
Mitchell of Motorola came up with the first mobile phone, 
they probably never dreamt that their invention would one 
day become such an essential part of everyone’s life. As we 
traverse from 3G to 4G and now inch towards 5G, our mobile 
phones have become smarter and smarter. So what is it that 
makes them smarter? We will all agree that it is the wide 
range of applications on our mobile phones that should be 
given the credit for this. 
According to Statista, a Germany-headquartered portal 
for market research, statistics and business intelligence, 
as of March 2017, Android mobile users can now choose 
from as many as 2.8 million mobile apps in the Google Play 
store, while iOS users have as many as 2.2 million mobile 
applications available in Apple’s App store for their use. 
Many everyday tasks are now performed with the help of a 
mobile app. 
A mobile application is just software designed to run 
on different mobile devices or tablets. The saga of mobile 
app development began in 1998 with Snake—the first 
mobile app that came as a standard preloaded game in Nokia 
mobile phones. Since then, we have made many giant leaps 
forward. In fact, most Web apps are now switching to mobile 
versions because of the tremendous increase in the number of 
smartphone users. People find their smartphones handier and 
more compact as compared to laptops or desktops. A mobile 
app gives developers the power to put their product directly 
in the hands of the user. All this has led to a sudden surge 
in the development of various mobile applications. Google 
came up with the Google Play store (earlier Android Market) 
in October 2008, and it now supports around 493 million 
downloads all across the globe on a daily basis. Since India 
is going digital at a super speed, this again offers tremendous 
opportunities to develop different mobile applications. Just 
after demonetisation, BHIM, Paytm and other such online 
payment applications were adopted widely. 
Mobile application developers really need to look into 
all the possible glitches that could arise when the app is in 
The Best Open Source  
Mobile App Development 
Frameworks

72 | JuLY 2017 | OPEN SOuRCE FOR YOu | www.OpenSourceForu.com
Developers Overview
use and work on all of them. The ultimate users should 
not suffer any major consequences due to shortcomings 
in the app. A mobile app developer not only takes care of 
the development of the mobile application but also needs 
to address aspects like usability, performance, efficiency 
and other such user expectations. In order to incorporate 
all these aspects without any bugs, a mobile application 
development framework is used, which brings different 
mobile application developers onto the same platform.  
Mobile app development for different OSs 
Developing apps for mobile devices requires taking into 
consideration the different constraints and features of these 
devices. These include a wide array of specifications such 
as screen size, hardware specs, mobile configurations and 
the platform or OS used by the mobile device. And because 
operating systems are so different, apps developed for iOS, 
for instance, may not work on Android. 
Mobile application development for Android: As 
of 2016, Android had the largest installed base of any 
mobile operating system, and the most number of mobile 
apps built so far are for Android. The Android OS uses 
a Linux kernel with some higher-level APIs, which are 
written in C. Different mobile apps running on this OS 
are normally programmed in Java, and they run with 
the Dalvik virtual machine using so called just-in-time 
compilation to translate the Java byte code into Dalvik 
dex-code. This adds some of the secure features to these 
apps, like efficient shared memory management, UNIX 
user identifiers (UIDs), pre-emptive multi-tasking, and file 
permission with the type safe concept of Java. Android 
Studio is widely used to develop different mobile apps.
Mobile application development for iOS: iOS is 
a mobile operating system developed by Apple Inc. and 
is used exclusively for Apple hardware. It’s built on 
the open source Darwin core operating system.  Most 
of the apps in Apple’s App store are written using the 
Objective-C programming language. iOS mobile app 
developers typically use Xcode. To build an iOS app, 
a developer must use Mac OS X since other operating 
systems are not supported. The development tools that 
are required for this are iOS 7 SDK and Xcode 5. The 
built iOS mobile app can be run using the iOS simulator, 
which is a part of the iOS SDK. In order to run these 
apps on a real device, they have to be made available in 
Apple’s App store.
Mobile application development for Windows: 
Windows Phone is a closed source smartphone operating 
system developed by Microsoft. The Windows Phone 
kernel handles access to the low-level device drivers. 
It also takes care of basic security, storage and the 
networking features of the mobile phone. Any Windows 
mobile app can make use of these features. Apart from 
these, there are three libraries - App Model for application 
Mobile Application Development
Multimedia 
Applications
Utility
Applications
Travel
applications
Web based
applications
Enterprise 
applications
Social 
Networking
Applications
Entertainment
applications
Location-
based
applications
Figure 1: Categories of mobile applications for development 
(Image source: googleimages.com)
Figure 2: Some important facts for mobile applications and users  
(Image source: googleimages.com)
management, a cloud integration module for Web searches 
via Bing, and a UI model for user interface management. 
All three of these can be used to develop apps for Windows. 
XAML is used by Windows Phone apps for the UI and C# 
or Visual Basic for the code development. Those familiar 
with C++ can develop a Direct3D app for Windows Phone 
8 in that language by taking full advantage of the phone’s 
graphics hardware.
Cross-platform technology of mobile app 
development
The mobile apps developed using cross-platform or hybrid 
technology can run on more than one platform at a time, 
without any intervention from developers to create separate 
versions of application code. Hybrid mobile applications 
are essentially written as Web applications using HTML, 
JavaScript and CSS. They are already embedded within a 
‘native wrapper’ allowing different applications built using 

www.OpenSourceForu.com | OPEN SOuRCE FOR YOu | JuLY 2017 | 73
Developers
Overview
this technology to run on any device while bypassing the 
restrictions of browser-only application functionality. Cross-
platform software basically compiles a single application 
source code into the native code that runs on different OSs, 
letting developers work on the same source code.
Requirements for mobile app development
Let’s have a look at all that a mobile app developer needs to 
take care of before starting off.
1. Understand and locate the problem or pain points that 
people experience and come up with the simplest, most 
effective and unique solution for this. Some market 
research may be needed for this.
2. Set targets and build a strategic plan to work upon 
according to the requirements of the client. This helps to 
avoid any unexpected changes in between and complete 
the development task in time.
3. Choose the right technology as this really plays a decisive 
role in the long run. Technology is really evolving at a 
rapid pace and developers always want to opt for the 
latest technology during app development. They need to 
be in sync with the latest technology trends if they want 
to make a strong impression in the application store. It’s 
always recommended that a mobile app should first be 
focused at one specific platform. It can be launched on 
multiple platforms, subsequently.
4. Always go for user-centric designs as the user experience 
(UX) is an extremely crucial consideration in the design 
of any mobile application. The UX is closely linked to 
other aspects like the user input mechanism and user 
interaction design.
5. Prioritise the efficiency and performance of the mobile 
app that needs to be developed. Rapid response time 
and a stable performance without crashing are the key 
performance criteria by which any user ranks a mobile 
application.
6. Define the target audience or the end users of the 
mobile application as they will impact how you 
develop your mobile app. All questions, such as the 
end users of the app, and how it’s going to help them, 
need to be answered well before hand. If we meet our 
users’ expectations, our smartphone application is 
likely to get more popular.
7. Plan for an effective testing process to be followed 
before the app actually hits the market. Keep in mind 
the performance, efficiency and load testing of the 
mobile app.
8. Help people save time and money, as that increases 
the chance of your app being downloaded. If you are 
successful in this, then you have nailed it, and your 
application is likely to get the deserved attention.
9. Know your budget in advance. Consider all the steps 
included in the development process which involve 
money, and properly allocate a budget for each stage.
Open source programming languages used for 
mobile application development
We know that a mobile app is nothing but a piece of code 
which performs the specified task, and code can be written 
in multiple programming languages. Now let’s look at some 
of the open source programming languages widely used for 
mobile app development.
Objective-C: This is the primary open source 
programming language used for different iOS applications. 
Objective-C was actually chosen by Apple to build apps 
that are scalable and robust. In spite of being a C-language 
superset, it does actually have a number of functions that 
precisely deal with I/O, graphics and display functions. 
Objective-C is completely integrated into all the iOS and 
MacOS frameworks. However, it’s now being slowly replaced 
by a more powerful open source language called Swift.
HTML5: HTML5 is one of the ideal open source 
programming languages if you are looking to build a Web 
front-end app for mobile devices. It makes various data 
types quite simple to insert, rationalises input parameters and 
accounts for different screen sizes, and also levels the browser 
playing field. It’s currently supported in different ways by 
various browsers. If we look at HTML5 from a cost-efficiency 
point of view, it has the advantage of building on the latest 
version of HTML, which makes the learning process simpler 
than that for a completely new language.
Swift: Swift is the latest open source programming 
language to enter the Apple ecosystem. It is mainly used to 
write code for Apple’s latest APIs, Cocoa and even Cocoa 
Touch. In spite of the fact that this language is written to 
work along with Objective-C, the Cupertino based company 
is always encouraging iOS developers to switch to Swift for 
complete programming. It’s designed to eliminate many of the 
security vulnerabilities that are associated with Objective-C. 
Java: Java is one of the most preferred languages 
for Android app development. It’s an object-oriented 
programming language which was developed at Sun 
Microsystems. It can be run in two different ways — by 
using a browser window, or in a virtual machine without 
Figure 3: The most popular mobile applications (Information source: Statista)

74 | JuLY 2017 | OPEN SOuRCE FOR YOu | www.OpenSourceForu.com
Developers Overview
a browser. This is quite useful in reusing code and even 
updating software. Although it does not have much to do with 
iOS mobile app development, it certainly can be on our list, 
particularly for apps for different platforms.
Python: Python is a high-level language that can be used 
to develop Android and desktop applications from scratch. It 
is an object-oriented language, which is processed at runtime 
by the interpreter. It’s easy to learn, easy to read and is quite 
easy to scale. It supports all types of GUI apps and runs on 
Windows, Mac, UNIX and Linux.
Best open source mobile app development 
frameworks
As discussed in the beginning, mobile app developers make 
use of different frameworks in order to come out with bug-
free and error-free apps. Here are some great open source 
frameworks.
Framework 7: This framework has been the top choice 
for the development of different iOS apps. Since it offers 
Android support as well now, it’s actually a good option if 
we want to start with iOS and then build an Android version 
of an app, with an iOS look and feel. Its main advantage is 
that it helps developers to build different iOS applications 
with just JavaScript, CSS and HTML, which most app 
developers already know. 
Features
1. Provides support for building different cross-platform 
hybrid mobile applications
2. Finest and rich framework with the most features for iOS 
hybrid application development
3. Material design UI
4. Native scrolling
5. 1:1 page animation
6. A custom DOM library
7. XHR caching and preloading
jQuery Mobile: This open source mobile application 
development framework has a large and committed user 
base in spite of the fact that it lacks many of the advanced 
features of most packages. This is because of its simplicity 
and other features like ‘write once and run anywhere’. It’s 
actually a good choice for the simple applications that need 
to run on Windows and BlackBerry phones. The main focus 
of jQuery is to empower its developers to build mobile apps 
that run seamlessly and with a unique user experience across 
all mobiles and tablets. It does not actually focus on providing 
the native look and feel to apps, for individual platforms like 
iOS and Android.
Features
1. Lightweight
2. Equipped with different theme designs
3. Semantic mark-up 
4. Progressive enhancement 
5. Phone Gap/Cordova support
Kendo UI:  Kendo UI is a framework used for building 
any kind of hybrid mobile application with JavaScript, 
HTML5 and CSS. It is available in open source as well 
as in commercial versions. It offers a wide range of UI 
widgets and plugins. It is being maintained and supported 
by Telerik, and has a large customer base, which includes 
different organisations like NASA, Sony, Microsoft, 
Toshiba and Volvo.
Features
1. Relies heavily on jQuery
2. Equipped with more than 70 ready-to-use jQuery widgets
3. Has numerous prebuilt themes added with Material Design 
styling, and a theme builder for customised options
4. Angular and Bootstrap UI integration
5. Performance can be optimised as per needs
Mobile Angular UI: This open source framework 
combines AngularJS and the modified version of Twitter’s 
Bootstrap, and converts them into a mobile UI development 
framework. It supposedly retains most of Bootstrap 3’s syntax 
for easier Web-to-mobile portability while adding different 
mobile components that are missing from Bootstrap, like 
overlays, switches, sidebars, fixed-positioned navigation 
bars and scrollable areas. It consists of different libraries like 
overthrow.js and fastclick.js. It is available for free use under 
the MIT licence.
Features
1. Provides the best of both the Angular framework and 
Bootstrap 3 to build different HTML5 mobile applications
2. No dependencies to jQuery or Bootstrap js
3. Uses fastclick.js and overthrow.js libraries for a smooth 
mobile experience
Figure 4: Features of Framework 7 (Image source: googleimages.com)
Figure 5: KundoUI architecture (image source: googleimages.com)

www.OpenSourceForu.com | OPEN SOuRCE FOR YOu | JuLY 2017 | 75
Developers
Overview
4. Mobile Angular UI used in this framework provides 
various directives for building UI components like 
switches, sidebars, etc
Sencha Touch: This is a framework used for building 
cross-platform end-to-end mobile applications with HTML5 
and JavaScript. It is available in both open source and 
licensed versions. It provides a visual application builder 
for HTML5, helping to reuse different custom components. 
Figure 6: Hybrid mobile app development using Sencha Touch  
(Image source: googleimages.com)
By: Vivek Ratan 
The author has completed his B. Tech in electronics and 
instrumentation engineering.  He currently works as an 
automation test engineer at Infosys, Pune. He can be reached 
at ratanvivek14@gmail.com.
[1] http://www.wikipedia.org/
[2] https://dzone.com/
[3] https://www.statista.com/
References
ExtJS, one of the popular JavaScript frameworks, is actually 
at the core of Sencha touch platform. It helps to create high 
performance applications with a near-native experience. 
Features
1. Consists of ready-to-use widgets with a native look 
and feel for all the leading platforms like Android, iOS, 
Windows Phone and BlackBerry
2. Also has a drag-and-drop HTML5 visual application 
builder with a lot of ready-to-use templates 
3. Customised components can be built and added to the 
library for reuse across mobile applications  
Would You
Like More
DIY Circuits?

76 | JuLY 2017 | OPEN SOuRCE FOR YOu | www.OpenSourceForu.com
Developers Insight
O
ften when people discuss regular expressions, they 
use the term ‘regex’, which leads to a mix up between 
regular expressions and the tools that use regular 
expressions for string searching. A classic example of this sort 
of mistake is people confusing regular expressions and grep, 
a UNIX utility for pattern matching. grep is a very powerful 
tool but whatever it may be, it definitely is not a synonym 
for regular expressions. A regular expression is not a tool but 
rather, a concept in formal language theory that defines how a 
sequence of characters can define a search pattern. On the other 
hand, grep is a tool that uses regular expressions for pattern 
matching. But there are a lot of other utilities and programming 
languages that use regular expressions for pattern matching. 
The main problem with regular expressions is that the 
syntax as well as the manner of calling regular expressions 
differs for the various languages, with some of them closely 
resembling each other, while others having major differences 
between them. So, in this series of articles, we discuss how 
to use regular expressions in the following six programming 
languages — Python, Perl, Java, JavaScript, C++ and PHP. 
This doesn’t mean that these are the only programming 
languages or software that support regular expressions. There 
are a lot of others that do. For example, you can find regular 
expressions in programming languages like AWK, Ruby, 
Tcl, etc, and in software like sed, MySQL, PostgreSQL, 
etc. Moreover, even absolute beginners typing *.pdf in their 
search boxes on Windows to search all the PDF files in their 
system are using regular expressions. Since a lot of articles 
about regular expressions cover grep in detail, this article will 
not cover the most famous tool that uses regular expressions. 
In fact, the one tool that solely depends on regular expressions 
for its survival is grep. The main reason one needs to study 
regular expressions is that many results obtained from 
powerful data mining tools like Hadoop and Weka can often 
be replicated by using simple regular expressions.
Some of the popular regular expression syntaxes include 
Perl-style, POSIX-style, Emacs-style, etc. The syntax of the 
regular expression used in a tool or programming language 
depends on the regular expression engine used in it. The 
ability to use more than one regular expression engine in a 
single tool itself leads to the support of more than one regular 
expression style. For example, the immensely popular regular 
expression tool by GNU called grep, by default, uses a regular 
expression engine that supports POSIX regular expressions. 
But it is also possible to use Perl-style regular expressions 
in grep by enabling the option -P. In Perl-style regular 
expressions, the notation \d defines a pattern with a digit, 
whereas in POSIX-style regular expressions, this regular 
expression does not have any special meaning. So, in the 
default mode of this regular expression, it will match the letter 
Regular Expressions in 
Programming Languages: 
A Peek at Python
This new series of articles will take readers on a journey that explores the regular 
expressions in various programming languages. The first article in this series takes 
a detailed look at the use of regular expressions in Python. 

www.OpenSourceForu.com | OPEN SOuRCE FOR YOu | JuLY 2017 | 77
Developers
Insight
d and not a digit. But if the utility grep is using the Perl-style 
regular expressions then a digit will be matched by the same 
regular expression. In this series, all the regular expressions, 
and the strings and results obtained by using them are 
italicised to highlight them from normal text. 
Figure 1 shows the output obtained when the default mode 
and the Perl style regular expressions are used with grep. The 
texts highlighted in red are the portions of the string matched 
by the given regular expression. In the figure, you can observe 
that the same regular expression while processing the same 
text in two different modes, matches different patterns.   
As a side note, I would like to point out that all pattern 
matching utilities called grep are not the same. There are 
minor differences between the different implementations of 
grep. For example, all the implementations by GNU, IBM 
AIX and Solaris differ at least on certain functionalities. 
There are also variants of grep like egrep, fgrep, etc, which 
differ from grep in functionality as well as syntax.  
Regular expressions in Python
Python is a general-purpose programming language invented 
by Guido van Rossum. The two active versions of it are 
Python 2 and Python 3, with Python 2.7 most likely being 
the last version of Python 2 and Python 3.6 being the current 
stable version of Python 3. Since we are concentrating 
on regular expressions in Python, we don’t need to worry 
too much about the general differences between these two 
versions. Since both Python 2 and Python 3 use the same 
module for handling regular expressions, there is no real 
difference between the two. I have executed all the scripts 
in this article with Python 2.7.12. The Python module that 
supports regular expressions is called re. The module re 
supports Perl-style regular expressions by using a regular 
expression engine called PCRE (Perl Compatible Regular 
Expressions). There is another module called regex which 
also supports regular expressions in Python. Even though this 
module offers some additional features when compared with 
the module re, we will use the module re in this tutorial for 
two reasons. First, regex is a third-party module whereas re 
is part of the Python standard library. Second, regex has an 
old and a new version, known respectively as version 0 and 
version 1 with major differences between the two. This makes 
a study of the module regex even more difficult.
The module re
Python regular expressions simplify the task of pattern 
matching a lot by specifying a pattern that can match strings. 
The first thing you must do is import the module re with the 
command import re. Python does not support a new type for 
representing regular expressions; instead, strings are used for 
representing regular expressions. For this reason, a regular 
expression should be compiled into a pattern object, having 
methods for various operations like searching for patterns, 
performing string substitutions, etc. If you want to search for 
the word ‘UNIX’, the required regular expression is the word 
itself, i.e., UNIX. So, this string should be compiled with the 
function compile( ) of module re. The required command is 
pat = re.compile(‘UNIX’), where the object pat contains the 
compiled regular expression pattern object.  
Optional flags of the function compile( ) 
The function compile( ) has a lot of optional flags. Some of 
the important ones are DOTALL or S, IGNORECASE or I, 
LOCALE or L, MULTILINE or M, VERBOSE or X, and 
UNICODE or U. DOTALL changes the behaviour of the 
special symbol dot (.). With this flag enabled, even the new 
line character \n will be matched by the special symbol dot (.). 
IGNORECASE allows case insensitive search. LOCALE will 
enable a locale-aware search by considering the properties 
of the system being used. This allows users to perform 
searches based on the language preferences of their system. 
MULTILINE enables separate search on multiple lines in a 
single string. VERBOSE allows the creation of more readable 
regular expressions. UNICODE allows searches dependent on 
the Unicode character properties database. 
Now, consider the regular expression with the flag 
IGNORECASE enabled, pat = re.compile(‘UNIX’, 
re.IGNORECASE). What are the strings that will be matched 
by this regular expression? Well, we are performing a 
case insensitive search on the word ‘UNIX’, so words like 
‘UNIX’, ‘Unix’, ‘unix’ and even ‘uNiX’ will be matched by 
the given regular expression.
A Python script for regular expression 
processing    
Consider the text file named file1.txt shown below to 
understand how a regular expression based pattern match 
works in Python.
unix is an operating system
Unix is an Operating System
UNIX IS an OPERATING SYSTEM
Linux is also an Operating System
Consider the Python script run.py, which reads a file name 
from the keyboard and opens it. The program then carries out 
Figure 1: Two regular expression styles in grep

78 | JuLY 2017 | OPEN SOuRCE FOR YOu | www.OpenSourceForu.com
Developers Insight
a line by line search on the file for the pattern given by the 
compiled regular expression object called pat. The object pat 
describing the regular expression is compiled in the Python 
shell and not in the script, so that the same Python script 
run.py can be called to process different regular expressions 
without any modification to the script. The Python shell can 
be invoked by typing the command python in the terminal. 
The script run.py reads the file name from the keyboard; so 
different text files can be processed with this Python script.  
filename = raw_input('Enter File Name to Process: ')
with open(filename) as fn:
 
for line in fn:
 
 m = pat.search(line)
 
 if m:
 
  
print m.group( )
Now it is time to understand how the script run.py works. The 
name of the file to be processed is read into a variable called the 
filename. The with statement of Python, introduced in Python 2.5, 
is used to open and close the required file. The file is then read 
line by line to find a match for the required regular expression 
with a for loop. The line of code, m = pat.search(line) searches 
for the pattern described by the regular expression in the compiled 
pattern object ‘pat’ in the string stored in the variable ‘line’. It 
returns a ‘Match’ object if a match is found or a ‘None’ object if a 
match is not found. This returned object is saved in the object ‘m’ 
for further processing. The line of code ‘if m:’ checks whether the 
object ‘m’ contains a ‘Match’ object or a ‘None’ object. If object 
‘m’ is ‘None’ then the if conditional fails and no action is taken. 
But on the other hand if ‘m’ contains a ‘Match’ object, then the 
matched string is printed on the screen by the line print m.group( 
). The method group( ) is defined for the object ‘Match’ and it 
returns the string matched by the regular expression. 
Figure 2 shows the output obtained by the regular expression 
with and without the compiler flag IGNORECASE enabled. If 
you observe the figure carefully, you will see that the module re 
is imported first and then the pattern is compiled in the Python 
shell with the line of code, pat = re.compile(‘UNIX’). Then 
the line of code, execfile(run.py) executes the Python script 
run.py and the output of this case-sensitive search results in 
the match of a single string UNIX. As mentioned earlier, the 
function compile( ) has many optional flags. The pattern object 
pat is recompiled a second time with the line of code, pat = 
re.compile(‘UNIX’,re.IGNORECASE) executed on the Python 
shell with the flag IGNORECASE enabled. The script run.
py is executed again and this case-insensitive search results in 
the match of strings unix, Unix and UNIX.      
In the script run.py, if you replace the line of code 
print m.group( ) with the code print line, the whole line 
in which a match is found will be printed. This Python 
script is called line.py. For example, for the pattern, pat = 
re.compile(‘UNIX’) the modified script will print UNIX IS AN 
OPERATING SYSTEM instead of UNIX. 
The method group( ) is not the only method defined for the 
object match. The other methods defined are start( ), end( ), 
span( ) and groups( ). The method start( ) returns the starting 
position of the match and the method end( ) returns the ending 
position of the match. The method span( ) returns the starting 
and ending positions as a tuple. For example, if you replace the 
line of code print m.group( ) in the script run.py with the code 
print m.span( ) with the pattern pat = re.compile(‘UNIX’) then 
the tuple (0,4) will be printed. This Python script is called span.
py. In order to understand the working of groups( ) method we 
need to understand the meaning of the special symbols used in 
Python regular expressions.
Special symbols in Python regular expressions
The following characters: . (dot), ^ (caret), $ (dollar), * 
(asterisk), + (plus), ? (question mark), { (opening curly 
bracket), } (closing curly bracket), [ (opening square bracket), 
] (closing square bracket), \ (backslash), | (pipe), ( (opening 
parenthesis) and ) (closing parenthesis) are the special 
symbols used in Python regular expressions. They have 
special meaning and hence using them in a regular expression 
will not lead to a literal match for these characters. 
The most important special symbol is backslash (\) which 
is used for two purposes. First, backslash can be used to create 
more meta characters in regular expressions. For example, \d 
means any decimal digit, \D means any non-decimal digit, \s 
means any whitespace character, \S means any non-whitespace 
character and \n, \t, etc, all have their usual meaning. Second, 
if a special symbol is prefixed with a backslash, then its special 
meaning is removed and thereby results in the literal match of 
that special symbol. For example, \\ matches a \ and \$ matches 
a $. The backslash creates some problems because it is a special 
symbol in Python regular expressions as well as Python strings. 
So, if you want to search for the pattern \t in a string, you first 
need to precede \ with another \ for a literal match resulting 
in the string \\t. But when you are passing this as an argument 
to re.compile( ) as a string, you have to precede each of these 
\ with yet another \ because Python strings also consider \ as 
a special symbol. Thus, the simply insane regular expression 
\\\\t only will result in a match for \t. In order to overcome this 
problem, Python regular expressions use the raw string notation 
which keeps the regular expressions simple. In raw string 
notation, every regular expression string is prefixed with an r 
Figure 2: Optional flags in the compile( ) function 

www.OpenSourceForu.com | OPEN SOuRCE FOR YOu | JuLY 2017 | 79
Developers
Insight
Let us have the following pattern: pat = re.compile(‘a{3}’) 
executed on the Python terminal and then call our script run.
py to do the rest. You might expect to see just one line selected, 
the line aaa. But the output in Figure 3 shows you that the lines, 
aaa, aaaa and aaaaa are also matched by this pattern because 
the string aaa is printed thrice. The textbook definition kind of 
suggests to you that only aaa should be matched but you are 
getting much more than that selected. Most of the textbooks that 
deal with regular expressions fail to explain this anomaly and 
that is the one point I would like to clarify once and for all, in 
this article, if nothing else. Theoretical Computer Science 101 
says that finite automata do not have the ability to count. Regular 
expressions and finite automata are different ways of describing 
the same thing. I can’t explain this any further but you have 
to believe me on this. Now what is the reason for three lines 
getting selected instead of the single line aaa? If you look at the 
two additional lines selected, aaaa and aaaaa, both contain the 
sub-string, aaa. That again tells us regular expressions are not 
counting; instead, they match for patterns and nothing more.   
The other possibilities with this notation are a{,m} which 
searches for patterns with m or less number of a’s; a{m,} 
which searches for patterns with m or more a’s; and a{m,n} 
which searches for patterns with m to n number of a’s, where 
m can be any integer constant. But do remember that just 
like a{m}, the regular expressions a{,m} and a{m,n} will 
also lead to counter intuitive results due to the same reasons 
mentioned earlier. 
The last two special symbols to be explained are the 
opening and closing parenthesis. These are used to indicate 
the start and end of a group. For example, (abc)+ will match 
strings like abc, abcabc, abcabcabc, etc. The contents of 
a group can be retrieved after a match, and can be used to 
match with the later parts of a string with the \number special 
sequence. The groups( ) method of the match object left 
unexplained earlier can also be discussed now. Let us assume 
we are searching for a pattern where three two-digit numbers 
are separated by a colon, like 11:22:33, 44:55:66, etc. Then 
one possible regular expression is (\d\d):(\d\d):(\d\d). The text 
file file3.txt contains the following text. 
11:22:33
aa:bb:cc
dd:cc:ee
44:55:66
Figure 3: Counting not possible with regular expressions     
so that you don’t need to add backslash multiple times. So the 
following two regular expressions: pat = re.compile(‘\\\\t’) and 
pat = re.compile(r’\\t’) will match the same pattern \t. 
The symbol * results in the matching of zero or more 
repetitions of the preceding regular expression. The regular 
expression ab* will match all the strings starting with an a and 
ending with zero or more b’s. The set of all strings matched by 
the regular expression is {a, ab, abb, abbb, ...}. The symbol 
+ results in the matching of one or more repetitions of the 
preceding regular expression. The regular expression ab+ will 
match all the strings starting with an a and ending with one or 
more b’s. The set of all strings matched by the regular expression 
is {ab, abb, abbb, ...}. The difference between the two is that ab* 
will match the single character string a, whereas ab+ will not 
match this string. The symbol ? results in the matching of zero 
or one repetition of the preceding regular expression. The regular 
expression ‘ab?’ will match the strings a and ab.
The two symbols [ and ] are used to denote a character 
class. For example, [abc] will match all strings having the 
letters a, b or c. A hyphen can be used to denote a set of 
characters. The regular expression [a-z] matches all strings 
having lower case letters. Inside the square brackets used for 
specifying the character class, all the special characters will 
lose their special meaning. [ab*] matches strings containing 
the characters a, b or *. 
The caret symbol ^ has two purposes. First, it checks for 
a match at the beginning of a string. ^a matches all the strings 
starting with an a. Second, the caret symbol inside square 
brackets means negation. ^[^a] matches all the lines that start 
with a character other than a. So, a line like aaabbb will not 
be matched whereas a line like bbbaaa will be matched. The 
symbol $ matches at the end of a string. a$ will result in the 
matching of all the strings ending with an a.    
As explained earlier, the special symbol dot (.) results in 
the match of any character except the new line character \n, and 
the DOTALL flag of compile( ) will result in a match of even 
a new line character. a.c will match strings like aac, abc, acc, 
a9c, etc. The symbol | is the or operator of a regular expression. 
black|white will match the strings with the sub-strings, black or 
white. So, strings like blackboard, whitewash, black & white, 
etc, will be matched by the regular expression. 
The special symbols, opening and closing curly brackets, are 
used for searching repeating patterns. This is the one notation 
that has confused many people who use regular expressions. I 
would like to analyse why this occurs. Every textbook and article 
on regular expressions declares that the regular expression a{m} 
matches all the patterns with m number of a’s, and rightly so. 
Now consider the contents of the text file file2.txt. 
a
aa
aaa
aaaa
aaaaa

80 | JuLY 2017 | OPEN SOuRCE FOR YOu | www.OpenSourceForu.com
Developers Insight
Now with the command, pat = re.compile(‘(\d\d):(\d\d):(\
d\d)’) and the script run.py executed on the Python shell, we 
will get the output shown in Figure 4. This time, there are 
no surprises; the output shown on the screen is as expected. 
The figure also shows the execution of a modified script 
modified_run.py with the line of code print m.group( ) in run.
py replaced with the line print m.groups( ). From the figure, it 
is clear that the groups( ) method of the match object returns a 
tuple with all the selected values, unlike the group( ) method 
which returns a string.  
Functions in module re
We have already discussed the functions compile( ) and search( ) 
in the module re. There are also other functions like match( ), 
split( ), findall( ), sub( ), escape( ), purge( ), etc, in the module 
re. The function match( ) is used for matching at the beginning of 
a string with the given regular expression pattern. For example, 
after executing the command, pat = re.compile(‘UNIX’) in the 
Python shell, the command, pat.match(“OS is UNIX”) will 
not give a match, whereas the command pat.match(“UNIX is 
OS”) will give a match. The function split( ) splits a string by 
the occurrences of the specified regular expression pattern. 
The command, re.split(‘\d’, ‘a1b2c3’) returns the list of 
elements selected. In this case, the selected list is [‘a’, ‘b’, ‘c’] 
because the separating character is a decimal digit. 
The function findall( ) returns all the non-overlapping 
matches of a pattern in the given string, as a list of strings. 
The command, print re.findall(‘aba’, ‘abababa’) will return 
the list [‘aba’, ‘aba’] as the result. In this case, only two 
occurrences of the strings, aba, are found because findall( ) 
searches for non-overlapping matches of a string. Regular 
expressions are generally used for pattern matching, but 
Python is a very powerful programming language and this 
makes even its regular expressions far more powerful than 
the ordinary. An instance of the enhanced power of the 
Python regular expressions can be found in the function 
sub( ), which returns the string obtained by replacing the 
leftmost non-overlapping occurrences of the given regular 
expression in a string with the replacement string provided. 
For example, with the command, pat = re.compile(‘Regex’) 
executed in the Python shell, the command pat.
sub(‘Python’, ‘Regex is excellent’) will return the string, 
Python is excellent. The function escape( ) is used to escape 
all the characters in the given pattern, except alphanumeric 
characters in ASCII. The command print re.escape(‘a.b.c’) 
executed in the Python shell will return a\.b\.c. The function 
purge( ) clears the regular expression cache.
A few examples in Python
Now that most of the regular expression syntax has been 
presented to you, let us go through a few examples where Python 
regular expressions are called into action. What will be the string 
matched by the regular expression ^a\.z$ ? The caret symbol 
^ makes sure that there should be an a at the beginning of the 
required pattern. The dollar symbol $ at the end ensures that the 
matched string should end with an z. The regular expression \. 
makes sure that there is a literal match for a dot (.) in between 
characters a and z. So only the lines containing the string a.z will 
be matched by this regular expression. Now, what does the regular 
expression a.z mean? Well, this matches any string with a sub-
string containing an a followed by any character other than a new 
line character and then followed by an z. So, strings like a.z, aaz, 
abz, azz, etc, will be matched by this regular expression. What 
is the pattern matched by the regular expression ^(aa).*\(zz)$ ? 
This regular expression matches all the strings that start with the 
sub-string aa and end with the sub-string zz with zero or more 
characters in between them. So, strings like aazz, aaazzz, aabzz, 
etc, will be matched by this regular expression. 
If you want to test a new regular expression pattern, you 
should follow these steps — open a terminal and type the 
command python to invoke the Python shell. Then, execute the 
command import re on the shell. Now, execute the command, 
pat = re.compile(‘###’), where you have to replace ### with the 
regular expression you want to test. Then execute the script run.
py with the command execfile(‘run.py’) to view the results. This 
article has also discussed a number of ways to modify the script 
run.py. This script, its modified versions and all the text files used 
for testing in this article can be downloaded from opensourceforu.
com/article_source_code/july17regex.zip.
This is just the beginning of our journey. The Python regular 
expressions discussed in this article are not comprehensive but they 
are more than sufficient for good data scientists to get on with their 
work. By the end of this series, you will have a good command 
over regular expressions. In the next article, we will discuss yet 
another programming language where regular expressions perform 
their miracles. But the best thing is that even if you are interested 
in just one programming language, say Python, the remaining 
articles in this series will still interest you because we will discuss a 
different set of regular expressions. So, with a little effort, you will 
be able to convert those regular expressions in other programming 
languages to Python regular expressions. The same applies to 
enthusiasts of other programming languages also. 
Figure 4: Regular expressions and groups( ) method    
By: Deepu Benson
The author has nearly 16 years of programming experience. 
He currently works as an assistant professor in Amal 
Jyothi College of Engineering, Kerala. He can be reached at 
deepumb@hotmail.com 

www.OpenSourceForu.com | OPEN SOuRCE FOR YOu | JuLY 2017 | 81
Developers
Insight
C
onvertigo is the first open source mobile platform 
to provide a complete, end-to-end solution — from 
back-end enablement to mobile UI development tools 
— integrated in one unique mobile application development 
platform (MADP) and MBaaS (Mobile Backend-as-a-Service). 
The platform comprises several components that 
include the Convertigo server, Studio and third party SDKs. 
Convertigo delivers a secured and scalable, all-in-one solution 
that integrates rapid cross-platform mobile development tools 
and a powerful MBaaS. It provides challenging back-end 
enablement and features middleware optimised for mobility. 
Architecture of the mobile platform
A mobile platform differs from a simple mobile application 
development tool by providing all the components needed 
to build, run, manage and connect mobile applications to the 
existing enterprise information system.
The capabilities that a mobile platform should 
possess are listed below.
 
Mobile back-end connectors: These allow mobile 
apps to connect to the enterprise database and to 
business applications.
 
Mobile service orchestrator: This allows back-end data to 
be aggregated, filtered and combined to provide a mobile-
friendly service API. The orchestrator can also augment 
an existing back-end application with mobile-specific 
capabilities such as push notifications or locator services.
 
Cross-platform UI: This allows developers to work on 
multiple operating systems.
 
Security manager: This is used to encrypt sensitive data on 
the network or on the mobile device.
 
Mobile application SDKs: These provide the capabilities to 
integrate other third party mobile UI development efforts.
What are mobile services?
Mobile applications need mobile services to interact with 
the data. Mobile services are made on top of existing back-
end services provided by ESBs (enterprise service bus) or 
other SOA based architecture. Mobile applications interact 
with mobile services using standard protocols like HTTP/ 
HTTPS, XML format, etc.
Mobile services can be defined using either a bottom-
to-top approach or a top-down approach, where the service 
model is defined by the mobile UI developer. A very 
Convertigo is an open source mobile app development platform that enables 
enterprises to integrate any mobile application with back-end applications or data 
sources — all at a considerably lower cost and within less time. 
An Overview of Convertigo 

82 | JuLY 2017 | OPEN SOuRCE FOR YOu | www.OpenSourceForu.com
Developers Insight
common situation is that existing enterprise services are 
not designed for mobility. 
Convertigo’s MBaaS components address these 
requirements as follows:
 
Platform transformation that enables the changing of 
existing internal services.
 
Data filtering to expose only the required data model to 
mobile devices from the existing mobile services.
 
Business logic augmentation to enhance existing internal 
services for mobile processes.
 
These components are built on the very current concept 
called ‘sequences’, defined as ‘steps’ needed to get the 
data, transform it and handle it based on the business rules.
Creating sequences does not require any programming 
language; it is simply done by using object configuration in 
Eclipse based on the Convertigo studio GUI. The sequences 
can also be enhanced by using JavaScript steps to perform 
multi-faceted business logic. 
Mobile security
An enterprise mobile platform must have security capabilities 
to secure data and processes.
These capabilities are:
 
User authentication in order to be sure which user is 
accessing the enterprise data.
 
Access control to control which part of the data should be 
seen for a particular mobile user.
 
Protocol encryption to prevent network spies from 
reading data coming or going to the mobile devices. This 
is built on TLS 1.2 encryption and supports client and 
server certificates.
 
Mobile device data encryption to prevent attackers from 
reading data on devices if they are lost or stolen.
 
Identity manager to handle unique mobile identity and 
several different credentials to access the back-end system.
IoT integration
The Internet of Things (IoT), the next challenge facing 
enterprises, connects numerous devices at the same time. 
These devices take part in the enterprise ecosystem by 
interacting with existing systems and applications already 
deployed by companies.
Tracking objects, detecting smoke, monitoring machines 
and alerts for empty parking slots are some obvious use cases 
where IoT technology can help. 
The IoT world connects a number of fields with each 
other in the following ways:
 
Devices from IoT vendors are able to exchange data with 
network operators.
 
IoT network operators deploy radio networks over the 
planet, which are able to handle IoT device traffic. 
 
Platform vendors provide software to connect 
the IoT network with active back-end systems 
or data repositories. 
The Convertigo mobility platform is able to handle data 
coming for IoT devices through an IoT network operator. In 
this way, all the back-end services from Convertigo, such as 
connectors, sequences and security managers can be used 
to push and pull data from the back-end. The Convertigo 
mobility platform is fully integrated with the Sigfox network 
as a P3 platform.
The platform can be connected to the Sigfox network 
using the latter’s call-back API. In this way, any IoT device 
triggering an event will be captured by the platform and will 
be processed by sequences to orchestrate activities to be done 
in the back-end systems.
Offline data capabilities 
Nowadays, most of the mobile processing taking place, 
for example, at enterprises uses large files on the mobile 
devices. These files can be PDFs or some media files such 
as MPEG videos. It is easy to transfer large files online, but 
in offline mode, this is still difficult. This can be solved by 
using the Convertigo mobile platform, which provides a 
powerful file transfer device based on FullSync technology. 
So how exactly is this done? Basically, Convertigo can get 
a file from any enterprise control manager and transfer it 
to one or several mobile devices. The transfer is done in 
chunks so that if a file is not transferred totally when the 
network crashes, only the untransferred contents will be 
retransmitted. File transfers are done as background tasks 
as long as the mobile application is running. Convertigo 
provides these capabilities out-of-the-box as an SDK API 
that can be used on Android and iOS devices.
Mobile platforms are key components of the digital 
enterprise as most companies have designed their 
information systems for the Web and not for mobile 
devices. Mobile devices enable companies to gain agility, 
reduce development and maintenance costs, preserve 
system integrity and improve security. 
With the increasing requirement for digitisation, 
enterprises will need constant mobile application 
development. The Convertigo mobility platform offers all 
the necessary components for enterprises to access the digital 
world, and for their customers, employees and their partners 
too — all with a control on costs and project timelines. 
Using the Convertigo platform helps enterprises avoid 
having to redevelop the same services each time they need a 
new mobile application, hence slashing costs. 
By: Neetesh Mehrotra 
The author works at TCS as a systems engineer. His areas of 
interest are Java development and automation testing. You 
can contact him at mehrotra.neetesh@gmail.com.
https://www.convertigo.com
Reference

www.OpenSourceForu.com | OPEN SOuRCE FOR YOu | JuLY 2017 | 83
For U & Me
Let’s Try
‘O
pen source software’ is a term we come across fairly 
often in our daily lives, but have a faint idea of what 
it entails. I remember my first questions about it.
“Is it code? Like huge files with complicated functions 
that I have no clue about?”
The answer to that is, it’s not. Not all open source 
software (OSS) comprise code bases that can cover the 
surface of the pyramids. And hardly any of it is as esoteric as 
hieroglyphics. In fact, most open source software comprises 
easily understood code maintained by diverse people ranging 
from 12-year-old school kids in Russia to 80-year-old Nobel 
laureates in Antarctica, and everyone in between. Most open 
source enthusiasts take it up as a hobby or pastime, while 
contributing to the ‘greater good’.
“Alright, but I can’t code!”
Open source contributions are not just about writing code. 
In fact, OSS would have died out a long time ago if it hadn’t 
been for those ‘non-coders’ who helped the community out by 
adding documentation for the code. 
Equally important are the people who contributed their 
time to answering questions regarding these projects and 
others that contributed towards adding examples to the project 
in order to make it easier for beginners to understand use-
cases for the software. There are people who have curated the 
mailing lists for the project, in turn learning much more about 
it than they had originally expected. Life surprises you and so 
does OSS, it would seem.
If none of this appeals to you, there are still others who 
contribute by organising meet-ups or video conferences 
for a particular OSS project or even just a fun get-together! 
Everyone is an open source contributor as long as one is 
helping others out in whatever capacity one can. It can be an 
extremely rewarding process, as you will eventually find out!
“So it’s probably stuff I can do. But why should I do it?”
There are a number of reasons that can motivate you to 
take up open source contribution, ranging from community 
welfare to personal gain:
1. It results in quality code being accessible to developers 
who rely on it for their work, thus avoiding having to 
rewrite functions from scratch since they have free 
access to open source software.
2. The sense of accomplishment after submitting your 
contributions, which could be used internationally, is 
simply fantastic! Imagine someone in a remote cabin in 
Alaska reading your solution, and sending up prayers of 
thanks to God for your help in resolving an issue with 
his  installation. 
3. The badge you wear with respect to your track record is 
for all to see. Regular contributors are valued for their 
skills as well as respected in the open source community. 
Who knows when a recruiter might come across it and 
recognise your efforts…
Here’s a beginner’s guide to making open 
source contributions. 
Get Started with  
Contributing to Open Source

84 | JuLY 2017 | OPEN SOuRCE FOR YOu | www.OpenSourceForu.com
For U & Me Let’s Try
4. Developing people skills and engaging with like-
minded others — it is essential to interact with a 
lot of really friendly and often erudite people while 
contributing to OSS. This can be a really enlightening 
as well as an entertaining experience, while helping 
you expand your network. 
“Okay, I’m sold! Now how do I get started?”
There’s a very well-documented and easy series of steps on 
how to get started with open source contributions. I looked back 
at my first time and came up with what’s shown in Figure 1.
It turns out that the first time I contributed to some open 
source project, it wasn’t really software that I was adding 
to. It was a JetBrains list of university website domains for 
student email verification!
So let’s go over some basic concepts here. In order to 
manage any set of data that is being modified by hundreds of 
contributors, there needs to be a system which can:
1. Manage all the different copies of the data, 
2. Allow the editing of the same data by multiple 
people, and…
3. Concatenate all of it into a single chunk when completed.
Thus, we have a ‘version control system’ that, as the 
name states, offers to control the multiple versions of 
data that may concurrently exist, given that hundreds of 
people edit the same code. GitHub is one of the sites that 
offers such a system, providing free hosting to open source 
software, files or data.
On GitHub, software is stored in different repositories 
which are essentially like separate folders (or workspaces) 
for each project that you may want to host. Each repository 
has its own set of issues, bugs, code, documentation, and so 
on. We will look at how to start with contributing using the 
JetBrains repository as our example (Figure 2). 
As you can see, it consists of a number of files and can be 
overwhelming to understand. Hence, we have the README 
file that is supposed to contain information about the project 
and help a newcomer understand what it is all about. Take a 
look at the JetBrains README.md file (Figure 3).
In order to create our own version of this repository 
(project), we will have to create a GitHub account and then 
‘fork’ a copy of this project to our repository. Forking is 
basically creating a separate copy of a project for your use.
Often when software is being edited, the ‘master’ branch 
remains untouched while the changes are tested on a ‘forked’ 
copy (the ‘development’ branch) of the project. Once it is 
verified that the ‘development’ branch passes all tests, it is 
‘merged’ back into the ‘master’ branch. 
Figure 4 illustrates how you can ‘fork’ a copy of the 
‘master’ branch, thus creating your branch(es), and how you 
‘merge’ your branch(es) back into the ‘master’ branch when 
you’re done editing your branch(es) of the project.
Figure 1: The first pull request I submitted
Figure 4: Forking a branch and merging it back
Figure 2: The JetBrains repository named ‘swot’
Figure 3: ‘README’ file for the ‘swot’ repository
What was the first pull request you sent on GitHub?
SwapneeIM sent this pull request a year ago
See every pull request by SwapneeIM
Made by Andrew and Brain ● Source on GitHub
swapneelm
Branch ‘cool-feature’
Branch 
‘coolest-feature’
Branch 
‘most-coolest-feature’
Branch ‘master’

www.OpenSourceForu.com | OPEN SOuRCE FOR YOu | JuLY 2017 | 85
For U & Me
Let’s Try
Now, visit your copy of the repository and make the 
changes in the code as per the instructions in the README.
md file. Add your institution domain name as per the folder 
structure described. Use the UI to create a new file with the 
name of your institution.
For instance, I’ve added mine as lib/domains/in/edu/djsce.
txt since my college email address is www.djsce.edu.in. It 
contains the name of my institution (Figure 6).
Once you’re done, it is time to merge this updated code 
back into the master branch! Go to the ‘Pull Requests’ section. 
Now, since we have forked the code, the procedure to merge 
it back is known as creating a ‘pull request’. Essentially, we 
request the master branch to ‘pull’ our changes back into it. 
It will ask you to add a description and name for the edit or 
‘commit’ that you’re providing. Once you have created one, it 
will show up on the JetBrains repository like the other edits/
commits there (Figure 8).
Now, since you don’t own the JetBrains repository, you 
need to wait for the admins to approve your pull request. 
Once it’s been merged, it will appear to be something like 
what’s shown in Figure 9.
And that’s it. Congratulations, you are now an open 
source contributor!
Because of you, every student at your institute can now 
request a JetBrains student discount using their university’s 
email address. You may now feel awesome for helping 
generations of students at your institute with just ten minutes 
worth of effort; keep at it!
Figure 5: Forking your copy 
of a repository 
Figure 7: The modification I made for my college
Figure 6: The forked copy you created
Figure 8: Pull request created
Figure 9: Pull request merged
You can also contribute towards addressing the ‘Issues’ 
on various repositories. Most of them have issues reserved 
for beginners or those labelled ‘first-timers only’. There 
are also websites like MunGell’s list of Awesome Beginner 
Projects and Up For Grabs that will help you get started on 
submitting your first pull request. 
By: Swapneel Mehta
The author has worked at Microsoft Research, CERN and 
startups in AI and cyber security. He is an open source 
enthusiast who enjoys spending time organising software 
development workshops for school and college students. You 
can contact him at https://www.linkedin.com/in/swapneelm; 
https://github.com/SwapneelM or http://www.ccdev.in.
Assuming that you have created 
your GitHub account, log in and 
visit the JetBrains repository. Click 
the ‘Fork’ button on the top right 
side of the screen and select your 
repository as the option (if you 
have multiple organisations as 
options, don’t worry; just select 
your personal page). If it doesn’t 
show your personal page, you 
probably already have a copy of it 
forked on your page (Figure 5).

86 | JuLY 2017 | OPEN SOuRCE FOR YOu | www.OpenSourceForu.com
For U & Me Insight
Even though we think we have secured our browsing by using conventional means, 
are we really secure from targeted advertising? Is our privacy secured when we 
browse the World Wide Web? Learn how we can secure our browsing and protect 
ourselves from browser fingerprinting. 
S
ign out, clear the cookies, and switch over to 
the private browsing mode — this is the privacy 
mantra of most Internet users. If you too fall into 
this category, then you better be aware that you’re being 
tracked. User accounts and cookies have become archaic 
when it comes to tracking. You might have noticed 
personalised ads which are related to your previous search 
queries, which seem to match your tastes accurately. This 
happens even when you are not logged in and have no 
cookies retained. This is made possible by a new tracking 
technique called browser fingerprinting, according to the 
Electronic Frontier Foundation (EFF).
The EFF is a non-profit organisation founded in 1990, 
aimed at protecting user privacy and freedom of expression in 
the digital era. It still fights this battle by conducting campaigns, 
doing research, and developing security and privacy tools. This 
is a valuable effort considering the fact that user tracking and 
immoral use of private information by websites have increased 
recently. You can learn more about EFF at eff.org.
Thanks to EFF and some software projects, we now have 
a bunch of free software tools to protect our privacy while 
browsing the Web. Let’s first learn what browser fingerprinting 
is and find out the EFF tools that present us with the solution.
 Note: This article addresses trackers only. Blocking 
trackers can help you prevent things like personalised ads. 
But to be truly anonymous, you have to use anonymity 
networks like Tor, which is a topic beyond the scope of this 
article. Visit torproject.org for more details, and use it only 
for legal purposes.
Browser fingerprinting
Browser fingerprinting is a technique that can be employed by 
websites to track your activities even when you are not signed 
in and are browsing in private mode. But how does it work?
Well, it’s obvious that websites can track and record your 
activities while you are logged in since they clearly know 
who you are (at least what your user name is). Even if you are 
not logged in, they can still drop a cookie in your browser and 
associate any activity from the same browser with you, which 
is enough to serve you personalised ads.
That’s why we switch over to the private mode, whereby 
we expect to be immune to the tracking mechanism just 
because we are not signed in and the browser doesn’t retain 
any cookies from our previous session. The site may still 
be able to obtain our IP address, but we feel it is of least 
importance when it comes to identifying us, since the IP 
Browser Fingerprinting:  
How EFF Tools Can Protect You

www.OpenSourceForu.com | OPEN SOuRCE FOR YOu | JuLY 2017 | 87
For U & Me
Insight
address changes each time we connect to the Internet (unless 
we have a static IP connection).
But this assumption is false. Websites can still track you 
even though you are not willing to sign in or retain cookies. 
What they use to identify you is the fingerprint of the browser 
you use. Unlike a serial number, this is not something imposed 
on the browser by the developer. Instead, it is calculated by the 
websites based on the configuration and settings of the browser. 
They even consider the extensions that you use.
To put it simply, consider the example of identifying a 
vehicle. The most straightforward and accurate way would 
be to use the registration number (exclude spoofs). But in 
most cases, we simply say something like, “…a red SUV 
with an eagle sticker on the hood, tops removed, one mirror 
slightly broken.” This would be more than enough since such 
a combination will be unique in a particular city.
Browsers can also be identified like this. The parameters 
start from basic configurations and the operating system to 
some unimaginable ones, such as:
 
Extensions used
 
Plugins and file formats supported
 
Screen resolution and colour depth
 
Language and time zone
 
Touchscreen support
Quoting panopticlick.eff.org, “Browser fingerprinting is 
a method of tracking Web browsers by the configuration and 
settings information they make visible to websites, rather 
than traditional tracking methods such as IP addresses and 
unique cookies.”
How to test your browser and what to do next
We’ve seen how a website can track our browser using 
browser fingerprinting. Now let’s figure out how 
susceptible our browser is. To check this, simply visit 
panopticlick.eff.org, which is the result of an important 
research undertaken by EFF.
You can see a TEST ME button on the home page itself. 
Click on that, and your browser gets tested. It takes a while, 
and once complete, you are presented with a quick report 
showing how your browser is susceptible to different aspects 
of tracking. You can get a detailed report, and install the 
privacy extension Privacy Badger from the same page.
Blocking trackers with Privacy Badger
There are hidden trackers embedded in many websites in 
the form of scripts and cookies. They are used to track you, 
i.e., to create a record of the pages you visit, things that you 
like and things you don’t. These details are usually used to 
serve personalised ads, and sometimes are sold to a third 
party for business purposes. The EFF Privacy Badger is 
a useful extension that blocks such trackers. Visit eff.org/
privacybadger to install this add-on in your browser.
When Privacy Badger realises that the ads in a page 
are tracking you, it simply blocks the trackers from loading 
more content. To borrow the words used by EFF, “…to the 
advertiser, it’s like you suddenly disappeared.”
When you install Privacy Badger and visit a website with 
trackers, it will display a list of these with sliders if you click 
on its icon in the browser toolbar. The slider will probably be 
green the first time, which means that the tracker is still not 
tracking you and hence is allowed. As you continue browsing, 
the slider might turn yellow and then red, indicating Privacy 
Badger’s opinion on them, and the blocking level.
You can say that Privacy Badger acts like an ad blocker 
and in fact, it is based on the code of AdBlockPlus. Still, it’s 
a bit different. Its aim is not to block every ad, but to block 
every tracking element. This is important since not all ads 
Figure 1: panopticlick.eff.org shows that the incognito mode of the 
Chromium Web browser is clearly susceptible to tracking
Figure 2: Privacy Badger detects 16 potential trackers 
on a popular shopping website

88 | JuLY 2017 | OPEN SOuRCE FOR YOu | www.OpenSourceForu.com
For U & Me Insight
are tracking you and we don’t want to block some innocent 
ads that help the creators run their websites. Moreover, 
tracking elements don’t always come as part of ads. Privacy 
Badger targets trackers regardless of their relationship 
with ads, which is a behaviour that requires additional 
configuration in regular ad blockers.
The Do Not Track header
Do Not Track (DNT) is a proposed HTTP header field that 
tells a website that you wouldn’t like to be tracked. Firefox’s 
Private Mode has this feature enabled, by default, whereas 
Chromium does not. You can enable this feature even in the 
regular mode by visiting the privacy and security sections of 
your browser’s settings page, if it supports it.
To know more and to check your browser, visit 
allaboutdnt.com (not from EFF).
HTTPS Everywhere
We all know that HTTPS is a mechanism that helps us encrypt 
our communication with Web servers, making it impossible 
for eavesdroppers to tap or manipulate our data. The URLs of 
HTTPS-enabled pages start with https:// instead of http://.
However, not all websites use HTTPS. And among the 
ones that are HTTPS-enabled, not all enforce it. This means 
you might be at risk even if you are visiting an HTTPS-enabled 
website, but have forgotten to ensure that your address bar 
shows https:// instead of http://. Another unfair practice is 
websites loading some third-party or additional resources over 
HTTP, even if the whole page’s address starts with https:// 
(some browsers display a broken lock icon to indicate this).
The HTTPS Everywhere extension helps you in such 
situations by rewriting the HTTP requests from your browser 
with HTTPS, if the servers support it. Of course, it can’t make a 
By: Nandakumar Edamana
The author is a free software user, developer and activist who 
has developed a few software packages including Sammaty 
Election Software. He writes for mainstream media and 
composes music as a passion. Website: nandakumar.co.in. 
You can contact him at nandakumar96@gmail.com. 
website HTTPS-enabled if the server doesn’t have an SSL/TLS 
certificate installed or it intentionally disables HTTPS for certain 
pages. Then what is the use of such an extension, you might ask. 
But recall the fact that not all HTTPS-enabled sites default to it, 
and not all secure pages are 100 per cent secure since they load 
non-HTTPS elements also. This is where HTTPS Everywhere 
can be of some help and it really matters.
You can download this extension from eff.org/https-
everywhere. It is worth noting that the Tor Project has also 
contributed to it.
What about the sites that don’t support HTTPS yet? Tell 
the webmasters how important HTTPS is, and how it can 
improve their reputation. Webmasters can get Let’s Encrypt 
SSL/TLS certificates for free, just by installing the EFF tool 
Certbot (certbot.eff.org). If you are a webmaster who hasn’t 
got HTTPS yet, take action quickly!
More from EFF
If you are interested to learn more about Internet privacy, visit 
eff.org/pages/tools where the organisation showcases its privacy 
resources. The list includes Surveillance Self-Defence (ssd.eff.
org), a portal that highlights how you lose your privacy online, 
and how to protect it by following some step-by-step instructions.
You can also visit emailselfdefense.fsf.org, to learn 
everything you should know about e-mail encryption. 
www.electronicsb2b.com
Log on to www.electronicsb2b.com and be in touch with the Electronics B2B Fraternity 24x7
Read more stories on Components in 
• The latest in power converters
• India’s leading component distributors
• Growth of Indian electronics components industry
• The latest launches of components for LEDs
• The latest launches of components for electronics
TOPCOMPONENTS STORIES
ELECTRONICS 
INDUSTRY IS AT A 

www.OpenSourceForu.com | OPEN SOuRCE FOR YOu | JuLY 2017 | 89
For U & Me
Insight
Mobile App Development in the 
Fast-changing Mobile Phone Industry
Change is the only 
constant in the IT industry, 
and this change has been 
particularly dramatic in the 
case of the mobile phone 
industry. Let’s take a quick 
look at the transformation 
in this industry over the 
last two decades, and how 
this has impacted mobile 
app development.
M
obile devices have changed drastically, starting 
off from Nokia’s basic feature phones, moving to 
BlackBerry’s smart/business phones, and then to the 
revolutionary Google and Apple smartphones. With time, 
hardware has improved with increasing RAM and processor 
capabilities. Today, there are lot of cool hardware being 
built into phones to make them complete devices, right from 
cameras and GPS to accelerometers and fingerprint readers.
Over the years, plethora of mobile OS/platforms have been 
created and launched by companies like Sun, Qualcomm, RIM, 
Microsoft, Google and Apple — right from J2ME/BREW 
to BlackBerry (RIM), and Android, iOS and WP8. All these 
platforms support different native API suites and vary in terms 
of the framework they offer. In fact, the base programming 
languages of these platforms have been different. Java was used 
for J2ME, RIM and Android, while C++ was used for BREW, 
C#/C++ for WP8 and Objective C for iOS. 
With the mobile market being shared by these different 
platforms/OSs, till a few years back it was difficult for 
developers and companies to create mobile applications for 
all platforms. It needed a diverse set of skills to develop and 
maintain mobile applications for each of the mobile platforms 
and, thus, presented cost and time-to-market challenges.
The alternate means tried included creating hybrid apps 
(Web based applications wrapped in native mobile app 
containers). Hybrid apps had their own limitations and were 
deficient in terms of performance, user engagement and 
effective utilisation of device hardware capabilities.   
The recent innovation of bot platforms has introduced 
new possibilities for creating device and platform neutral 
apps. In the long run, bots (chat-bots) are predicted to 
substitute a major chunk of mobile apps. Though new, bot 
platforms are limited in terms of the API set, features and 
capabilities, as of date.
Hence, considering the diverse OS ecosystem, a mobile 
application design and development strategy becomes very 
important. A smart, strategic approach allows supporting 
multiple OSs/platforms and devices, without compromising 
on aspects like performance and user engagement.
This article is based on my years of experience in designing 
and developing games as well as consumer and enterprise-
grade rich mobile client applications for all the earlier-
mentioned platforms. Individual developers or organisations 
that plan to develop mobile client applications may find my 
experiences helpful in deciding on the most suitable mobile 
design and development strategy, after evaluating the pros 
and cons of each. I intend to share information about different 
mobile OSs/platforms for different era, the challenges they 
presented at the time, followed by the design and development 
strategy adopted to deal with those challenges.
Early 2000s: The era of feature phones with 
J2ME/BREW apps 
During the early 2000s and until around 2004, BREW and 
J2ME games and apps were predominant in the market. The 
reason was simple — people were using feature phones and 
they needed lightweight platforms. Most feature phones 
supported either J2ME or BREW. 

90 | JuLY 2017 | OPEN SOuRCE FOR YOu | www.OpenSourceForu.com
For U & Me Insight
Qualcomm’s BREW was based on C/C++. On the other 
hand, J2ME (Java 2 Micro-Edition) was a stripped down version 
of Java’s Standard edition, with a limited API set meant for 
devices with minimal hardware. It was mostly used to build 
lightweight games, and basic applications for feature phones 
with limited functions, like those related to PIM (Personal 
Information Management). J2ME devices hardly allowed 
anything related to system/hardware interfacing and even if they 
did, things rarely worked as intended. This limited J2ME to be 
used mostly for gaming applications on feature phones and not 
for any sophisticated client application.
The challenges - poor hardware, as well as limited APIs 
and multi-tasking: The early feature phones came with quite a 
few challenges.
1. Building games or apps on J2ME was challenging owing to 
limited hardware capabilities. Many of the phones allowed 
a maximum of 64KB of JAR (J2ME binary size) and/or 
allowed 128KB of RAM. Even the most popular J2ME 
device in 2005, the Nokia 6600, supported 1MB of RAM 
for J2ME apps.
2. Also, since the API set was very limited, it only enabled 
the creation of very basic applications.
3. Multi-tasking was missing on most of the feature phones. 
J2ME apps used to pause as soon as they were moved to 
the background.
Alternative approaches to deal with small RAM and 
app binary sizes: Over the years, the design and development 
strategies adopted to overcome different challenges included 
the following.
1. Apps were packaged with very few resources (images), 
while other required resources were fetched once from the 
server via data calls and then cached locally to the record 
management system. 
2. Apps were designed to load only a portion of the data or 
resource (image) required at any moment of time. Later, 
loaded resources were replaced with other resources needed 
for subsequent flows. For example, for one of the gaming 
applications (Shadow Showdown’s FOP), we chopped the 
big image of a character animation into three independent 
parts to keep the memory in check by optimising the size of 
the image part. We then used one part of the image at a time 
and then replaced it with another part when needed.
3. Usually, basic apps were built without a rich API set and no 
multi-tasking.
2005-09: The BlackBerry era heralds the first 
smart, business phone
Roughly between 2005 and 2010 was when BlackBerry phones 
enjoyed the most popularity. BlackBerry from RIM (Research 
in Motion) was the first smartphone introduced in the league 
of phones/platforms discussed in this article. Because of its 
security, it was considered the best business phone and preferred 
by enterprises. BlackBerry had its own OS, which provided 
multi-tasking and initially supported devices with a trackwheel 
and trackpad. The brand had many popular series of phones, like 
Curve, Pearl and Storm, to name a few.
BlackBerry provided a Java based API set. With rich APIs 
and multi-tasking supported by the OS, it became possible for 
developers to build sophisticated native business apps which 
hadn’t been the case for J2ME apps. Being able to use built-
in device capabilities allowed apps to have features like file 
operations, native email, phone and SMS. 
Rich applications, right from Reuter’s stock market app 
and the Stitcher radio (audio) app to OpenTable’s restaurant 
booking app and Broadworks Assistant VoIP calling 
background app, were all possible with RIM. Yet, creating 
mobile websites or Web based applications wasn’t feasible 
owing to very limited JavaScript support.
The challenges: Some of the main shortcomings of the 
BlackBerry included a small display, a complex UI API, an old 
Web kit and difficult backward compatibility. 
1. One of the biggest challenges for BlackBerry’s popular 
4.x device series (that included Curve and Pearl) was 
designing the UX and implementing it. BlackBerry phones 
already had only a single column view compared to 
screens that permitted the multi-column view for a wider 
Web area. On top of this, the hard keypad hindered the 
possibility of increasing the display height effectively.
2. The only way to create a user interface (UI) for 
BlackBerry’s Java apps was by using RIM native APIs. 
The UI API set and layout principles were complex, and 
this made it difficult to develop a rich UI.
3. The majority of BlackBerry users in the market had the old 
RIM OS version (4.x series) running on their devices, and 
the Web kit engine that came with 4.x was limited in terms 
of functionality and JavaScript capabilities. Hence, building 
mobile websites became really difficult. I remember that 
building an interactive Web calendar control became almost 
impossible for the BlackBerry OS 4.x series.
4. RIM supported static class verification, that is, if an app 
was compiled and packaged with a higher OS version’s 
native library API suite and possessed a higher OS API, 
then the packaged app would not install on devices 
with old OS versions. This made it difficult to enable 
backward compatibility while creating apps targeted for 
latest OS versions.
Dealing with the challenges: Listed below are some of 
the design and development strategies used to overcome the 
different challenges.
1. All key modules were served as Home Screen options, 
after which standard linear or tabbed (for big apps) 
navigation followed.
2. A reusable custom compounded and UI component 
library was created. This was done by extending standard 
containers and components (like a field manager, vertical/
horizontal field managers, label/text fields, etc). 
3. As 4.x devices covered a majority of the market initially, 
native RIM applications were the only option until the 

www.OpenSourceForu.com | OPEN SOuRCE FOR YOu | JuLY 2017 | 91
For U & Me
Insight
launch of RIM OS 6.x/ 7.x series with the Web kit 2 
engine during late 2010-12. 
4. Backward compatibility was smartly achieved by 
compiling a base version of the app with the minimum 
OS version supported by it. For all add-on delta features, 
add-on binary files were created for the app with a higher 
OS API and advanced functionality. The base code had 
a smart factory to check which OS version the app was 
running on, and dynamically loaded the implementation 
of the higher OS version if the device had the required OS 
version. While distributing the app, the distribution server 
would check the OS version of the device requesting the 
app download. For higher OS versions, the distribution 
server could load the JAD (Java application descriptor) 
file, which possessed entries of all binary files, including 
the base binary packaged with the minimum OS as well as 
add-on binary files packaged with higher OS versions to 
achieve advanced functionality.
2010: How Android and iOS transformed the 
mobile industry 
The mobile industry underwent a disruptive transformation 
with the launch of Android and iOS during the years 2008-10. 
Google’s Android and Apple’s iOS were launched to run on 
high-end mobile devices (like HTC, iPhone, etc). These devices 
were not just mobile devices but were almost small computers 
with great processing and computing capabilities. They had a 
lot of useful hardware built-in, like GPS receivers, cameras, 
accelerometers, etc. Android and iOS provisioned native APIs 
to access and use these built-in capabilities within applications. 
Along with such great hardware capabilities, both Android 
and iOS came up with full touchscreen devices without hard 
keypads. This ensured that most of the screen height became 
usable and thus enhanced the usability of applications. 
Excellent gesture detection and touch capabilities of these 
devices provided an extraordinary user experience.
The challenge: J2ME, BREW and RIM were already 
around, and with the launch of iOS and Android, the mobile 
industry was using several mobile platforms between 2010 
and 2012-13. Different platforms with different native APIs 
made it really difficult for developers/companies to build apps 
for mobile users. 
It now became imperative for developers and companies 
to support all these platforms to cover the entire market. With 
different native APIs and different base languages, the task 
became really challenging. It required a different skillset to 
build mobile apps for different platforms, and this increased 
costs and time-to-market substantially, because different 
versions of an application were built and maintained for 
each platform. Domain knowledge couldn’t be reused with 
different teams which resulted in inconsistent app behaviour.
Web/hybrid apps: The industry was struggling with the 
many variations around mobile platforms and APIs. Moreover, 
there were new mobile OSs/platforms, which were on the verge 
of being launched. As a result, developers and companies started 
channelling their efforts and energies around creating platform-
independent, Web/HTML based mobile applications.
Web kit engines were getting better with each new version 
of an OS. Also, the introduction of HTML5 gave developers 
the confidence to try and build mobile websites which worked 
across all mobile platforms. Website UIs got redesigned for 
single column view mobile devices and content was then served 
dynamically with the help of JavaScript (Ajax). 
Mobile websites, a.k.a. thin clients, provided platform-
independent solutions but observed less end user adoption 
owing to difficulties in using and managing different websites at 
the same time on mobile devices. Users always preferred quick 
app icons. Also, pure mobile websites running in browsers 
lacked the capability to use the inbuilt device hardware like the 
camera, GPS, etc.
Hybrid mobile applications overcame mobile website 
problems. They served Web based (HTML) content within a 
native mobile app container. The mobile app container houses 
the Web view, which loads the URL for the Web content (which 
can be thought of as the website’s URL). Hybrid mobile apps 
with a quick launch app icon freed the user from managing 
different website URLs. Additionally, hybrid mobile apps 
leveraged the Web to native bridge interfaces (plugins), to 
utilise built-in device hardware capabilities within the app.
So hybrid mobile apps became the alternate option to 
deal with the time-to-market and cost problems arising from 
supporting multiple mobile platforms during the period 2010-
12. These hybrid apps, unfortunately, soon came under scrutiny 
due to the adverse impact of Web content in terms of user 
experience and performance. This led to the famous discussion 
on ‘native versus hybrid mobile applications’.
Hybrid vs native mobile apps: This has been a topic 
of much debate for many years amongst developers and 
companies wishing to create mobile applications for different 
platforms — whether to opt for native or hybrid mobile apps.
The best way to decide between the two options is by 
answering a few questions like:
1. How much does cost and time to market matter to you?
a. If you have a highly skilled team, and time and 
performance matter to you, then native is a better choice. 
A skilled team can quickly learn the syntax of new mobile 
platforms and port a native mobile app to a platform. 
b. If your team is small and you are targeting a quick 
launch for your mobile app, then hybrid is the way to 
go, as the same code can be reused across different 
mobile OSs/platforms.
2. Who are your target users? Is it going to be an in-house app 
meant for those within the company or a public app for your 
customers/end consumers?
a. If it is going to be a public app for your customers, then 
native is the better choice over hybrid because the user 
experience is of utmost importance to mobile users. 
Reports on the Internet suggest that around 80 per cent 

92 | JuLY 2017 | OPEN SOuRCE FOR YOu | www.OpenSourceForu.com
For U & Me Insight
have resulted in developers and companies quickly adopting 
these two platforms. As a result, all other mobile platforms/
OSs like J2ME, RIM, and even Windows Phone, which came 
into the market late, have lost their limited market share and 
almost vanished. This has solved the cost and time-to-market 
problem caused by supporting multiple mobile OSs/platforms. 
Developers/companies can now just create apps for Android 
and iOS. Moreover, with great and simple design wizards to 
create UIs on Android and with the introduction of simpler 
programming languages like Swift on iOS, it is now quick and 
easy to create iOS and Android native apps.
The challenge of apps flooding mobile devices: Developers 
have started creating separate Android and iOS mobile 
applications for every small function and are inducing end users 
to install many of them. This is now creating a problem with 
users’ mobile devices getting flooded with too many applications.
The future: Bot platforms and 
specialised mobile apps
Today, most mobile users are always connected to social 
messenger channels like WhatsApp and Facebook Messenger 
or to mobile OS specific messenger channels like iMessage. 
This increasing user engagement with messenger channels has 
influenced messenger app companies like Facebook and Slack 
to innovate and create bot platforms, which enable companies 
to create business-specific chat-bots to connect with their end 
customers on social messenger channels. Apple’s iMessage and 
Google’s Allo are both expected to have their bot platforms to 
help create chat-bots. 
Bot platforms are allowing companies to mark their presence 
on channels where users are investing most of their time. This 
increases their chances of business. Chat-bots appear as normal 
messenger contacts to the end user in a messenger list, and thus do 
away with the need to have separate mobile apps for every small 
need. Moreover, bot platforms are device and platform neutral — 
for example, Facebook Messenger runs on all devices, be it mobile 
phones or wearables, for both Android and iOS.
Bot platforms are new, and deficient in terms of feature 
support and built-in device capability. So, for the next few years, 
chat-bots are expected to be used for basic functions and needs 
instead of having separate mobile apps. Native Android and iOS 
mobile applications are expected to serve specialised needs/
requirements. As an example, it makes more sense to create high 
performing native mobile banking app for Android and iOS that 
leverage a built-in fingerprint reader to authorise app access, or 
use a GPS receiver to identify the location of a user’s mobile 
device and the server list of nearby banks/ATMs. 
of users only retry a mobile app once or at the most, 
twice, if it fails to work as intended or if it is slow. It is 
important to keep users engaged at all times and that 
can be done via fast performing native apps. One key 
example is of Facebook migrating from HTML to a 
native mobile app to keep users engaged by serving more 
feeds at the same time.
b. If it is going to be an in-house app for users within the 
organisation, then hybrid apps can work, particularly if 
the app is meant for routine business operations. Internal 
apps need not be very exciting for users, as long as they 
solve the purpose in the expected timeframe.
3. What is the geography of your majority user base? How 
many platforms do you actually want to cover?
a. If you are targeting just one geographic region with 
the launch of your app, then native is a better choice, 
as you can just target the one platform that commands 
the highest market share in that geographic region 
(like iOS for USA). 
b. If you are targeting to release your app globally, then 
a hybrid app can be opted for as the market will be 
shared by different mobile OSs/platforms.
4. How big or rich does your mobile application need to be?
a. If you are targeting to launch a full blown, rich mobile 
app with built-in device hardware capabilities like a 
camera, GPS, etc, then native is a better choice for 
enhancing the performance of the app, and effectively 
achieving touch/gesture handling and built-in device 
hardware capabilities.
b. If you are targeting to launch a minimum viable 
product (MVP) for your mobile app, then hybrid 
can be the option as performance may not be 
hampered much with less frequent Web-to-native 
layer interactions. Also, it gives opportunities to the 
developers to try out different ideas in no time — in 
the form of MVP of different hybrid mobile apps. They 
can later opt for the native app route for ideas that gain 
traction and, hence, need to be extended and scaled in 
the form of a mobile app (the way Facebook did).
2017: Android and iOS lead the mobile market
With great technological advances continuously enhancing 
the hardware, both Android and iOS have become the most 
popular choice of consumers today. With a growing user base, 
developer adoption and increasing app volume on Play/App 
Store, Android and iOS are now leading the mobile market 
with a major share of the global market. 
As opposed to BlackBerry, both Android and iOS were 
launched with their initial focus on the end user. Gradually, 
the latter two also built security and APIs to cater to business 
and enterprise needs. With these platforms leading the market 
and with the growing BYOD culture, both these platforms 
have become the choice for business apps as well. 
Ease of development via an easy and great API suite 
By: Gaurav Vohra
The author is a senior solution architect at Impetus Technologies 
and has more than 13 years of experience in the IT industry. He 
has been predominantly handling software products and services 
in the enterprise mobility & digital marketing space. He can be 
contacted at gaurav.vohra@impetus.co.in.


94 | JuLY 2017 | OPEN SOuRCE FOR YOu | www.OpenSourceForu.com
For U & Me Insight
For years, humans have tried to get computers to replicate the thinking processes of the 
human brain. To a limited extent, this has been possible by using deep learning and deep 
neural networks. This article provides insights into deep or machine learning.
W
hen surfing the Net or while on social media, you 
must have wondered how you automatically get 
pop-ups of things that interest you. Did you know that 
there are lots of things happening behind the scenes? In fact, lots 
of computations and algorithms are running in the background to 
automatically find and display things that interest you, based on 
your search history… And this is where  deep learning begins. 
Deep learning is one of the hottest topics nowadays. If you 
do a Google search, you will come across a lot that’s happening 
in this field and it is getting better every day, as one can gauge 
when reading up on, ‘Artificial intelligence: Google’s AlphaGo 
beats Go master Lee Se-dol’ at www.bbc.com.
In this article, we will look at  how we can practically 
implement a three-layer network for deep learning and the 
basics to understand the network.
Definition
It all started with machine learning – a process by which we 
humans wanted to train machines to learn as we do. Deep 
learning is one of the ways of moving machine learning closer 
to its original goal—artificial intelligence.
As we are dealing with computers here, inputs to these 
are data such as images, sound and text. Problem statements 
include image recognition, speech recognition, and so on. We 
will focus on the image recognition problem here.
History
When humans invented computers, scientists started 
working on machine learning by defining the properties 
of objects. For instance, the image of a cup was defined as 
cylindrical and semi-circular objects placed close to each 
other. But in this universe, there are so many objects and 
many of them have similar properties. So expertise was 
needed in each field to define the properties of the objects. 
This seemed to be a logically incorrect method as its 
complexity would undoubtedly increase with an increase in 
the number of objects.  
This triggered new ways of machine learning whereby 
machines became capable of learning by themselves, which in 
turn led to deep learning.
Architecture
This is a new area of research and there have been many 
architectures proposed till now. These are:
1. Deep neural networks
2. Deep belief networks
3. Convolutional neural networks
4. Convolutional deep belief networks
5. Large memory storage and retrieval (LAMSTAR) 
neural networks
6. Deep stacking networks
An Introduction to  
Deep (Machine) Learning

www.OpenSourceForu.com | OPEN SOuRCE FOR YOu | JuLY 2017 | 95
For U & Me
Insight
Deep neural networks (DNNs)
Let us now look at how deep neural networks work.
The word ‘neural’ in DNN is related to biology. Actually, 
the soul of these networks is how the biological neural system 
works. So, let’s take a brief look at how two biological 
neurons communicate.
There are three main parts in a biological neuron 
as shown in Figure 1.
1. Dendrite: This acts as input to the neuron from 
another neuron.
2. Axon: This passes information from one 
neuron to another.
3. Synaptic connection: This acts as a connection between 
two neurons. If the strength of the received signal is 
higher than some threshold, it activates another neuron.
Neuron types
Let us try to express human decisions and biological 
neural networks mathematically so that computers can 
comprehend them.
Let’s suppose that you want to go from city A to city B. 
Prior to making the journey, there are three factors that will 
influence your travel decision. These are:
a. If the weather (x1) is good (represented by 1) or bad 
(represented by 0) has a weight of (w1)
b. If your leave (x2) is approved (represented by 1), or not 
(represented by 0) has a weight of (w2)
c. If transport (x3) is available (represented by 1) or not 
(represented by 0) has a weight of (w3)
And you will decide as follows: Irrespective of whether 
your leave is approved or not and transport is available or not, 
you will go if the weather is good. This problem statement 
can be drawn as shown in Figure 2.
According to the figure, if the sum of the product of the 
inputs (xi) and their respective weights (wi) is greater than 
some threshold (T), then you will go (1), else you will not (0).
As your input and output is fixed, you have to choose 
weights and thresholds to satisfy the equation.
For example, let us choose w1=6, w2=2, w3=2 and T=5.
You will be able to make a correct decision if we choose 
the above values for equation (1), i.e., if your leave is not 
approved (0) and transport is not available (0) but the weather 
is good (1), then you should be going.
Similarly, you can check other conditions as well.
It will be easy to manually calculate these weights 
and thresholds for small decision-making problems but as 
complexity increases, we need to find other ways – and this is 
where mathematics and algorithms come in.
f(y), the function represented in Figure 3, produces 
output in terms of 0 and 1. This says that for a small change 
in input, the change in output is high — represented by 
a step function. This can cause problems in many cases. 
Hence, we need to define a function f(y) such that for 
small changes in input, changes in the output are small — 
represented by the Sigmoid function.
Depending upon these conditions, there are two neuron 
types defined, as shown in Figure 3.
Figure 1: Biological neural network (Source: https://en.wikipedia.org/
wiki/Biological_neural_network)
Figure 3: Neuron types 
Figure 2: Simple human decision in mathematical form

96 | JuLY 2017 | OPEN SOuRCE FOR YOu | www.OpenSourceForu.com
For U & Me Insight
Defining DNN
In Figure 2, Xi represents the input neuron while f(y) 
represents the output neuron, while layers are called the input 
and output layer, respectively.
In DNN, there are multiple hidden layers of units between 
the input and output layers, as shown in Figure 4.
Let us see how the theory discussed so far is used 
practically, in the following text.
Training a network
There are two stages involved in a DNN:
1. Training the network
2. Testing the network for how well it has been trained
Before we move on, let’s get familiar with the 
mathematical terms and definitions that we will use, 
going forward.
To begin with, you can refer to Box 1 for all the 
relevant formulae.
Next, the gradient descent is used to find the local minima 
of a function (Figure 5).
The gradient descent works as follows:
 
The given performance function f(p) is dependent on w1.
 
We have to calculate the minima of the function (i.e., the 
value for which the function is minimum).
 
We randomly initialise the value of w1 to get the 
starting point.
 
Then we calculate the slope of the function with respect to 
w1 and update the weight as follows:
 
As per Figure 5, the slope is positive, w1 is decreasing, 
and we are going in the correct (i.e., downward) direction.
Back propagating errors occur from the output to 
the input layer.
Now, let’s look at how to implement a three-layer DNN.
Understanding problem statements
In Figure 6, we have four images in binary format given as 
inputs to the network. Each image is 3 bits long. With respect 
to each binary input, we have the input neuron defined. So 
there are three input neurons in the input layer. There are four 
neurons in the hidden layer. And one output neuron.
With respect to each image, there is a predefined desired 
output (d).  
Assume that Images 1 and 4 are of digit 0 with the desired 
output-binary 0; and Images 2 and 3 are of digit 1 with the 
desired output-binary 1.  
There are 12 weights in Layer L1 and four in Layer L2 for 
connection between the two layers.
As our input and output is fixed, we need to find weights 
in Layer L1- w1 and weights in Layer L2- w2 such that our 
actual output (z) is equal to the desired output (d).
Figure 6 can be drawn as a block diagram as shown in 
Figure 7 to derive the required equations.
Figure 4: DNN block diagram
Figure 5: Gradient descent
Figure 6: Three-layer DNN
Box 2 gives different equations for DNNs.                                    
Steps for implementing a three-layer DNN
1. Initialise the weights w1 randomly. Dot product of input-x 
and weights w1 produces P1, which acts as input to the 
Figure 7: Three-layer DNN block diagram

www.OpenSourceForu.com | OPEN SOuRCE FOR YOu | JuLY 2017 | 97
For U & Me
Insight
Sigmoid function1 in layer L1.
2. The output of the Sigmoid function is y, which acts as 
input to the output layer.
3. Again, randomly initialise weights w2. The dot product of 
y and weights w2 produces P2, which acts as input to the 
Sigmoid function2 in layer L2.
4. The output of Sigmoid function2 is z, which is our actual 
output. Give this output to the performance function P 
along with the desired output to produce an error.
5. Minimise this error by backpropagating it and using  
the gradient descent algorithm, i.e., change weights  
w1 and w2 to find the minima of the performance 
function.
6. When the error is minimum, the actual output will be 
close to the desire output.
The Python code given below shows the practical 
implementations:
import numpy as np
import matplotlib.pyplot as plt
def nonlin(x,deriv=False):
    if(deriv==True):
        return x*(1-x)  
 
 
 
 
#derivative of the 
Sigmoid function
    return 1/(1+np.exp(-x)) 
 
 
 
 
#Sigmoid 
function
def show_error(l):
    e=[];e1=[];e2=[];e3=[];e4=[]
    count1=0;count2=1;count3=2;count4=3
    for i in l:
        for j in i:
            for k in j:
                e.append(k)
    for i in range(10000):
        e1.append(e[count1]);e2.append(e[count2]);e3.
append(e[count3]);e4.append(e[count4])
        count1+=4;count2+=4;count3+=4;count4+=4
    plt.plot(e1,label='input1');plt.
plot(e2,label='input2');plt.plot(e3,label='input3');plt.
plot(e4,label='input4')
    plt.xlabel('Iteration',fontsize=17);plt.
ylabel('Error',fontsize=17)
    plt.legend();plt.grid(True)
    plt.show()
x = np.array([[0,0,1],  
 
 
 
#input dataset
            [0,1,1],
            [1,0,1],
            [1,1,1]])
                
d = np.array([[0],  
 
 
 
 
#desire output dataset
            [1],
            [1],
            [0]])
np.random.seed(1)
w1 = 2*np.random.random((3,4)) - 1 
#Randomally initializing 
weights in Layer1
w2 = 2*np.random.random((4,1)) - 1 
#Randomally initializing 
weights in Layer2
l=[]   
 
 
 
#list
for j in xrange(10000): 
 
 
#Find slop/Derivative for n 
number of steps
    l0 = x
    y = nonlin(np.dot(l0,w1)) #dot product of inputs and 
weights are given to Sigmoid function which produces output y
    z = nonlin(np.dot(y,w2)) #dot product of inputs and 
weights are given to Sigmoid function which produces output z
    p = z - d  
 
 
 
 
 
#Error
    l2_delta = p*nonlin(z,deriv=True)
    l1_error = l2_delta.dot(w2.T)
    l1_delta = l1_error * nonlin(y,deriv=True)
    w2 -= y.T.dot(l2_delta) 
 
#new_weight(w2)=old_
weight(w2)-change in weight(i.e.derivative w.r.t w2)
    w1 -= l0.T.dot(l1_delta)  
#new_weight(w1)=old_
weight(w1)-change in weight(i.e.derivative w.r.t w1)
Box 1: Math basics
Box 2:  Equations for DNNs

98 | JuLY 2017 | OPEN SOuRCE FOR YOu | www.OpenSourceForu.com
For U & Me Insight
    arry_to_list=p.tolist()
    l.append(arry_to_list)
show_error(l) 
The basic code is taken from https://iamtrask.github.
io/2015/07/27/python-network-part2/ and updated for the 
practical understanding of the learning rate and graph output.
Output
After adding debug prints, we get the output as shown in 
the box below, for the above program. You can see that after 
training is complete, the error is almost zero and the actual 
output is close to the desired output.
If you use the weights that we get after training and the 
new input (other than those that we used in training), you will 
get the actual output in terms of trained inputs only.
If you closely look at the code, the main things are 
happening at Lines 63 and 65. These two lines are just using 
the derivation equations derived in Box 2.
Figure 8:  Case 1
Figure 9: Case 2
Learning rate
Please refer to Figures 8, 9, 10 and 11. The graphs given in 
these figures are plotted for different test scenarios by slightly 
modifying the code.
Case 1 (Figure 8)
 
Keep updating weights by calculating the slope for 10,000 
iterations (same as code above).
 
We actually don’t need these many iterations to reach an 
error close to zero.
Case 2 (Figure 9)
 
If the number of iterations is too low, in our case, 
100, we will not get an error close to zero. And our 
actual output will not be close to the desired output. It 
will be unpredictable.
Case 3 (Figure 10)
 
There is a way with which we can reduce the error close 
to zero for a small number of iterations. The only update 
you will need is to multiply the slope by some constant.
 
Here, I used the constant 20, which is called 
the learning rate.

www.OpenSourceForu.com | OPEN SOuRCE FOR YOu | JuLY 2017 | 99
For U & Me
Insight
By: Sandip Tambde
The author is currently working at Nvidia. His interests include 
automation, deep learning and Python coding. He can be 
reached at sandip.tambde89@gmail.com.
[1] http://neuralnetworksanddeeplearning.com/index.html
[2] https://iamtrask.github.io/2015/07/27/python-network-
part2/
[3] http://blogs.nvidia.com/blog/category/auto/
[4] https://en.wikipedia.org/wiki/Biological_neural_network
References
Figure 10: Case 3
Figure 11: Case 4
Figure 12:  cuDNN performance comparison with CAFFE, using several well 
known networks (Source: https://devblogs.nvidia.com)
Case 4 (Figure 11)
 
You cannot make the learning rate too small or too big.
 
If the learning rate is too big (I used 50), there is 
the possibility of the system becoming unstable, as 
shown in Figure 11.
 
If the learning rate is too small, the learning 
time will increase
I would like to conclude the article by discussing the 
computing power needed in building large DNNs, and where 
the world is headed in this area. The average human brain has 
100 billion neurons while one of the artificial DNNs which 
won the Imagenet competition had 650,000 neurons.
With the increase in the number of neurons, the 
complexity increases and hence there is a need for more 
computation. The examples given above train in a few 
milliseconds. DNNs with millions of neurons and high 
training datasets (inputs) take months and years to train 
a network on a CPU. We can reduce this training time 
drastically by using GPUs.
Some comparative data is given in Figure 12. Here, 
cuDNN v2 performance is compared with CAFFE using 
several well-known networks. The CPU is 16-core Intel 
Haswell E5-2698 2.3GHz with 3.6GHz Turbo. The GPU is 
NVIDIA GeForce GTX TITAN X.
(The NVIDIA CUDA Deep Neural Network library 
(cuDNN) is a GPU-accelerated library of primitives for deep 
neural networks to help increase their performance. The latest 
cuDNN v4 library yields more performance.)
All major tech companies such as Google, Facebook, 
IBM, Apple, etc, (the list is too big to cover here) are moving 
to deep learning nowadays, and using it for applications that 
range from speech recognition to self-driving cars. 
The latest from the Open Source world is here.
OpenSourceForU.com
Join the community at facebook.com/opensourceforu
Follow us on Twitter @OpenSourceForU
THE COMPLETE MAGAZINE ON OPEN SOURCE

100 | JuLY 2017 | OPEN SOuRCE FOR YOu | www.OpenSourceForu.com
OpenGurus Let’s Try
Here’s a brief note on NetLogo, which can be described as a multi-agent programmable 
modelling environment. It is widely used by teachers, students and researchers to model 
and study complex systems by creating simulations and ‘playing’ with them.
O
ptimisation is inherent in nature. We have seen the 
key aspects of optimisation in previous editions of 
OSFY. Another interesting phenomenon is called 
emergent behaviour, which could be natural or artificial. 
Swarms of ants, schools of fish, flocks of birds, etc, are some 
of the natural examples of emergent behaviour. The increased 
complexity of routing air traffic is considered to be artificial 
emergent behaviour. Studying these complex behaviours 
using numerical techniques is not easy. Hence, an agent based 
simulation platform called NetLogo is used to understand 
such phenomena.
Introducing NetLogo
NetLogo is GNU GPL licensed free software, designed by Uri 
Wilensky, director of Northwestern University, in 1999. It is 
a JVM based cross-platform multi-agent simulation platform. 
NetLogo doesn’t require coding knowledge or any other 
prerequisites. It is designed for a broad audience. So anyone 
NetLogo: An Open Source Platform 
for Studying Emergent Behaviour
Figure 1: NetLogo interface 1

www.OpenSourceForu.com | OPEN SOuRCE FOR YOu | JuLY 2017 | 101
OpenGurus
Let’s Try
By: Hithu Anand
The author has a teaching background and is currently 
researching on smart grids. He can be reached at 
hithuanand@gmail.com.
Figure 2: NetLogo interface 2
Figure 4: NetLogo interface 4
Figure 3: NetLogo interface 3
Figures 5: Simulation results 1
Figures 6: Simulation results 2
from secondary school students to graduates can make use of 
this software to study their field of interest.
Installation
Installation of NetLogo in Ubuntu 14.04 is easy. First, download 
the NetLogo package from http://ccl.northwestern.edu/netlogo/. 
I have a 64-bit Linux computer, so I have downloaded the 
package accordingly. For NetLogo version 6.0.1, memory size 
of 197 MB is required. The downloaded file is compressed 
and must be extracted in the home directory (preferably in the 
opt folder). Once extracted, you are ready to use NetLogo on 
your computer. In the NetLogo directory, double-click on the 
NetLogo executable to open the GUI of the software.
Particle swarm optimisation (PSO) in NetLogo
PSO is a nature inspired swarming theory based algorithm 
invented by Kennedy and Eberhart in the 1990s. Flocks of 
birds migrating from place to place in search of food and 
shelter are the basis for this algorithm. Fortunately, NetLogo 
has inbuilt model libraries where many models like PSO 
are readily available. The model demonstrates how the true 
optimum value 1 is found for various parameter settings 
like landscape smoothness (structure of valleys and hills), 
population size (number of birds), etc.
The NetLogo interface is shown in Figure 1. Figure 3 
shows the PSO model file available in its library. Figure 2 
shows the PSO finding its true value in 30 ticks. Figure 4 
shows the same attained in 22 ticks. Note that fewer ticks are 
observed for a population of 10. Figures 5 and 6 show the 
above-mentioned initial and final results. 

102 | JuLY 2017 | OPEN SOuRCE FOR YOu | www.OpenSourceForu.com
For U & Me How To
G
IMP and Inkscape are the two 
main open source software 
available for graphics artists. 
Very generally speaking, GIMP is for 
photos and Inkscape is for drawings. 
Since drawings are vector images, 
Inkscape is the better option for such 
graphics. Inkscape is also a better option 
if you do not have a drawing tablet-
stylus device such as those from Wacom. 
This is because vector drawing tools 
such as Inkscape rely on a combination 
of shapes rather than raw raster content 
and brush strokes to create images.
Many open source artists work only 
with GIMP, being unaware of Inkscape. 
I had tried Inkscape in the past and 
concluded that it was powerful but 
difficult. I created some custom desktop 
icons like the one shown in Figure 1 
and forgot about it.
Inkscape is a great open source tool for drawing cartoons. In this article, 
you can learn how to use it to create simple caricatures.
Caricaturing with Inkscape
But the task was still pretty 
laborious. Later, when reading a 
book on Dilbert cartoons, I realised 
that it need not be so difficult. If you 
observe carefully, Dilbert cartoons are 
extremely simple. The caricatures are 
a combination of simple lines twisted 
here and there, minimal shading and that 
is about it. If you were to follow Scott 
Adam’s (Dilbert’s creator) production 
technique, then your Inkscape work 
would be much easier. What this means 
is that you can start drawing cartoons 
and caricatures with just your mouse! 
Figure 1: A custom 
desktop icon created 
using Inkscape
Figure 3: Dilbert caricatures are a 
combination of simple lines
Figure 2: An anatomically 
accurate shark caricature 
created using Inkscape
Lately, I have been watching some 
cartoons and this made me get back 
to working on Inkscape. I now have a 
small ‘clip art’ library with images like 
what’s shown in Figure 2. (I believe 
this image depicts a more anatomically 
accurate shark than Sharko!)

www.OpenSourceForu.com | OPEN SOuRCE FOR YOu | JuLY 2017 | 103
For U & Me
How To
By: V. Subhash 
The author is a writer, programmer, cartoonist and FOSS fan. He can be 
contacted at tech.writer@outlook.com.
Figure 4: The Nodes tool lets you tweak shapes
Figure 5: The Donald Trump cartoon becomes recognisable when you add colour to his trademark mop of hair
Figure 6: A caricature does not have to look like a mirror image of the person 
Figure 7: A caricature is identified not just by the drawing but also by the text appearing along with it
Yes, Inkscape lets you do that.
Figure 3 shows the various stages 
of my rendering of Dilbert. First, I 
drew a quadrilateral shape, roughly 
resembling Dilbert’s head with the 
Pencil tool. To draw straight lines, use 
click-move-click sequences. Do not 
drag your mouse on the page, as it will 
create a curved line. Then, I selected 
the created shape (the quadrilateral) 
and chose the Nodes tool from the 
toolbar (Figure 4). This enabled me 
to tweak the four line segments of the 
quadrilateral. The bottom line segment 
was moved at the centre to make it 
curved. I then selected the top line 
segment and clicked the ‘+’ toolbar icon 
several times to add several new nodes 
to the segment. This broke up the top 
segment into several smaller segments. 
I curved each of those segments to form 
the outline of Dilbert’s hair. I also drew 
two straight lines of different lengths on 
the left side of Dilbert’s face. I curved 
them to form Dilbert’s ear. After that, 
I selected both lines and pressed the 
Ctrl+D key combination to duplicate 
the ear. I pressed the ‘H’ key to flip the 
ear horizontally, and used arrow keys 
to move the new ear to the right side 
of Dilbert’s face. The rims of Dilbert’s 
glasses were drawn with the Circle tool. 
The folding arms of the glasses were 
drawn as straight lines with the Pencil 
tool. The nose was drawn with the Circle 
tool but with the ‘Arc’ option activated.
Figure 5 shows a Donald Trump 
caricature. It requires very simple lines. 
The caricature in Figure 6 is more 
sophisticated. I drew a circle and then 
divided it with an arc shape using 
options under the Path drop-down 
menu. The top half was duplicated 
and reduced in height to form the hair. 
The bottom half was widened to make 
the beard. The eyes and the nose were 
drawn using the circle shapes. A ‘filter’ 
called ‘Roughen’ has been applied to the 
hair at the back of the head and on the 
chin for a rugged outline. The hair was 
given a grey colour. Then, another filter 
called ‘Rough paper’ was applied to 
bring out the hairy texture.
A caricature is a distorted and 
exaggerated drawing of a person. 
Although your initial Inkscape 
drawings may not compare well 
against the cartoons in a mainstream 
newspaper or magazine, these will 
be good enough for a beginner. 
The text appearing in callouts and 
thought bubbles will provide the 
context. People will identify the real 
person when they read the text that 
accompanies your caricature. 
After a few days of working with 
Inkscape, you can stop forwarding the 
funny cartoons that others send you. 
Instead, you can create your own. 

TIPS
TRICKS
&
Install the progress bar in Debian/Ubuntu/
Red Hat
pv is used to measure the progress of the dd command and 
monitor the progress of data through a pipe. To use this, 
make sure pv is installed in the Linux box. Use the steps 
given below to install it from repositories.
For Debian/Ubuntu, type:
#apt-get install pv 
For Red Hat/CentOS, type:
#yum install pv
 
#(pv -n /dev/sda | dd of=/dev/sdb bs=128M conv=notrunc,noerror) 
2>&1 | dialog --gauge “Running dd command (cloning), please 
wait...” 10 70 0
Here -n is for numeric output, and dialog --gauge is 
used for the dialog box and the message in quotes. 
Important note: dd (disk clone) should be used very 
carefully. It can damage the disk, if it is wrongly used.
— Prasanna PNA, 
pnaprasanna@gmail.com
Transferring files between remote hosts 
using the command prompt
If you want to copy a file from a remote machine in the 
same network to your PC, it can be done simply by using the 
command  ‘scp’. The syntax of the command is as follows:
# scp username_of_remote_pc@ip_of_remote_pc:file_location 
location_to_store_in_my_pc
For example, if you want to copy a file named ‘test.php’ 
located in the directory  /var/www/html/HisWeb of the PC 
with ip 172.16.117.49 and user name R_user to /var/www/
html/MyWeb of my PC, the command is:
 
# scp R_user@172.16.117.49:/var/www/html/HisWeb/test.php /var/
www/html/MyWeb
Then you need to enter the password of R_user.
If you want to send a file from your PC to a remote PC, 
the syntax is:
# scp file_to_send username_of_remote_pc@ip_of_remote_pc:file_
location location_to_store_in_remote_pc
You can also transfer files between two remote PCs. 
The syntax in this case is:
# scp username_of_remote_host_1@ip_remote_host_1:file_to_send 
username_of_remote_host_2@ip_remote_host_2:location_to_store
Enjoy transferring files! 
—Md Shakeel Iqbal Saikia, 
shakeel.smtp@gmail.com
Get to know more about the process 
running in Linux
Here is what you can do to know more about the process 
running in Linux.
1.  Create a small process with the sleep command:
[root@blrvm15-vm2 proc]# sleep 1000 &
[1] 23735
2. The kernel will create a directory related to each process 
in /proc. Go to the folder /proc/23735; you can see all the 
information about the process here. There may be many 
dynamic results too. To view the details you can use the 
cat command as shown below. proc is generally called 
‘Process Information pseudo-file system’. 
[root@blrvm15-vm2 proc]# cd 23735
[root@blrvm15-vm2 23735]# ll
total 0
dr-xr-xr-x. 2 root root 0 Jan 18 19:17 attr
104 | JuLY 2017 | OPEN SOuRCE FOR YOu | www.OpenSourceForu.com

Track executed commands
It’s important for a sysadmin to log all the commands 
on the production server in order to know how and when the 
command was executed.
To do this, we can use Snoopy for Linux based systems. 
Installation is simple with the script available from GitHub (the 
stable version):
# wget https://github.com/a2o/snoopy/raw/install/doc/install/bin/
snoopy-install.sh
# chmod u+x snoopy-install.sh
# ./snoopy-install.sh stable
Restart the relevant (log) service, if required. Watch the logs 
as per your syslog/rsyslog configuration. In CentOS, it is /var/
log/secure, and for Debian and Ubuntu, it is /var/log/auth.log.
To make any changes, you can go to the config file in /etc/
snoopy.ini.
To disable, run the following command: 
#snoopy-disable
To enable, use the command given below: 
#snoopy-enable 
—Natraj Solai, 
linuxraja@gmail.com
Want to watch a movie in the powerful terminal?
Now, you can watch ‘Star Wars’ in the terminal. This is 
one of the exciting things I discovered through this terminal 
tweak. You need to install the telnet package, as follows: 
$sudo apt-get install telnet
 Now run the following command:
$telnet towel.blinkenlights.nl
 And the movie begins! 
—Joyce George, 
joycegeo@mail.com
Share Your Linux Recipes!
The joy of using Linux is in finding ways to get around 
problems—take them head on, defeat them! We invite you to 
share your tips and tricks with us for publication in OSFY so 
that they can reach a wider audience. Your tips could be related 
to administration, programming, troubleshooting or general 
tweaking. Submit them at www.opensourceforu.com. The 
sender of each published tip will get a T-shirt.
-rw-r--r--. 1 root root 0 Jan 18 19:17 autogroup
-r--------. 1 root root 0 Jan 18 19:17 auxv
-r--r--r--. 1 root root 0 Jan 18 19:17 cgroup
<Output Snipped>
-r--r--r--. 1 root root 0 Jan 18 19:17 status
-r--r--r--. 1 root root 0 Jan 18 19:17 syscall
dr-xr-xr-x. 3 root root 0 Jan 18 19:17 task
-r--r--r--. 1 root root 0 Jan 18 19:17 timers
-rw-r--r--. 1 root root 0 Jan 18 19:17 uid_map
-r--r--r--. 1 root root 0 Jan 18 19:17 wchan 
[root@blrvm15-vm2 proc]# cat status
The above command will display the complete details of 
this process.
—Bhagyaraj M.C., 
bhagym@india.altair.com
Getting a file using the telnet command
Telnet is a very useful command. Here is a tip on how to 
get files from a website, using telnet.
Here is how you can get http://example.com/test.html using 
the telnet command: 
$telnet example.com 80
GET /test.html 
The output of the above command will display the 
complete test.html file.
—Remin Raphael, 
remin13@gmail.com
Merge two or more PDF files into a single one
We often need to combine several PDF files into a single 
PDF file.  For this purpose, you can use the pdfunite command.
To use the command, you first need to install it on the 
computer. You can use your OS package manager to do so, 
as follows:
#pdfunite <file1.pdf> <file2.pdf> <output.pdf>
The above command merges file1.pdf and file2.pdf into 
output.pdf.
#pdfunite <file1.pdf> <file2.pdf>...<fileN> <output.pdf>
 
 
The above command merges file1.pdf, file2.pdf and so on, 
till fileN into output.pdf. 
pdfunite merges several PDF files in the order of their 
occurrence on the command line, into one PDF result file. 
None of the PDFs, file1 to filen, should be encrypted.
—Lokeswara Rao Bonta, 
nagarajunice@gmail.com
www.OpenSourceForu.com | OPEN SOuRCE FOR YOu | JuLY 2017 | 105

106 | July 2017 | OPEN SOuRCE FOR yOu | www.OpenSourceForu.com
DVD Of The MOnTh
Here’s the latest and the most stable version of Debian.
Live Gnome, 64-bit 
JULY 2017
CD 
Tea
m e-
mail
: cd
tea
m@
efy.i
n
Rec
omm
end
ed S
yste
m Re
quir
eme
nts: 
P4, 
1GB
 RA
M, D
VD-R
OM 
Driv
e
In c
ase 
this
 DV
D do
es n
ot w
ork 
prop
erly,
 wri
te t
o us
 at s
upp
ort
@ef
y.in 
for 
a fre
e re
plac
eme
nt.
What is a live DVD?
A live CD/DVD or live disk contains a bootable 
operating system, the core program of any computer, 
which is designed to run all your programs and manage 
all your hardware and software.
Live CDs/DVDs have the ability to run a 
complete, modern OS on a computer even without 
secondary storage, such as a hard disk drive. The CD/
DVD directly runs the OS and other applications from 
the DVD drive itself. Thus, a live disk allows you 
to try the OS before you install it, without erasing 
or installing anything on your current system. Such 
disks are used to demonstrate features or try out 
a release. They are also used for testing hardware 
functionality, before actual installation. To run a live 
DVD, you need to boot your computer using the disk 
in the ROM drive. To know more about how to set 
a boot device in BIOS, please refer to the hardware 
documentation for your computer/laptop.
OSFY DVD
Debian 9 Stretch GNOME Live
After 26 months of development, the Debian Project 
has released its stable version 9 (code name ‘Stretch’). 
With the combined efforts of the Debian security 
team and of the Debian Long Term Support team, this 
release will be supported for the next five years. In 
Stretch, the default MySQL variant has been replaced 
with MariaDB. Firefox and Thunderbird return to 
Debian with this release and replace their debranded 
versions Iceweasel and Icedove, which were in the 
archive for more than 10 years.
Administrators and those in security-sensitive 
environments can take comfort knowing that the X 
display system no longer requires root privileges to 
run. The Stretch release is the first version of Debian to 
feature the ‘modern’ branch of GnuPG (GNU Privacy 
Guard) in the ‘gnupg’ package.
The bundled DVD can boot in to the live version of the 
Debian 9 GNOME (64-bit) desktop environment and 
also has an image of the Cinnamon Live version in the 
other_isos folder. The default live user is user and the 
password is live.


Earn up to 
₹1,00,000 
per hour 
 
Curious? Mail us at 
contact@loonycorn.com 
 
 Step 1: You work with us to create a course proposal 
for a 2-10 hour course 
 
 Step 2: We pay you an advance of ₹ 5,000/hour upon 
course approval 
 
 Step 3: You build the course, we help 
 
 Step 4: We grade your work and pay according to the 
rate card below (rates per hour) 
Grade A:  ₹100,000 | B:  ₹50,000 | C:   ₹25,000  | F:  ₹5,000     



Basic Radar Analysis


For a complete listing of titles in the Artech House Radar Series, turn to the back of this book.

Basic Radar Analysis
Mervin C. Budge, Jr.
Shawn R. German

Library of Congress Cataloging-in-Publication Data
A catalog record for this book is available from the U.S. Library of Congress.
British Library Cataloguing in Publication Data
A catalogue record for this book is available from the British Library.
Cover design by John Gomes
ISBN 13: 978-1-60807-878-3
© 2015 ARTECH HOUSE
685 Canton Street
Norwood, MA 02062
All rights reserved. Printed and bound in the United States of America. No part of this book may be reproduced or utilized in
any form or by any means, electronic or mechanical, including photocopying, recording, or by any information storage and
retrieval system, without permission in writing from the publisher.
All terms mentioned in this book that are known to be trademarks or service marks have been appropriately capitalized.
Artech House cannot attest to the accuracy of this information. Use of a term in this book should not be regarded as affecting
the validity of any trademark or service mark.
DISCLAIMER OF WARRANTY
The technical descriptions, computer programs and documents in this book and on the accompanying compact disc have been
developed with the greatest of care and they have been useful to the authors in a broad range of applications; however, they
are provided as is, without warranty of any kind. Artech House, Inc. and the authors and editors of the book titled Basic Radar
Analysis make no warranties, expressed or implied, that the equations, programs, and procedures in this book or its associated
software and documents on the compact disc are free of error, or are consistent with any particular standard of merchantability,
or will meet your requirements for any particular application. They should not be relied upon for solving a problem whose
incorrect solution could result in injury to a person or loss of property. Any use of the programs, documents or procedures in
such a manner is at the user’s own risk. The editors, authors and publisher disclaim all liability for direct, incidental or
consequent damages resulting from the use of the programs, documents or procedures in this book or on the accompanying
compact disc.
10 9 8 7 6 5 4 3 2 1

Contents
Preface
Acknowledgements
Chapter 1
Radar Basics
1.1
Introduction
1.2
Radar Types
1.3
Range Measurement
1.4
Ambiguous Range
1.5
Usable Range and Instrumented Range
1.6
Range-Rate Measurement (Doppler)
1.7
Decibels
1.8
dB Arithmetic
1.9
Complex Signal Notation
1.10
Radar Block Diagram
1.11
Exercises
References
Chapter 2
Radar Range Equation
2.1
Introduction
2.2
Basic Radar Range Equation
2.2.1
Derivation of ES
2.2.2
Derivation of EN
2.3
A Power Approach to SNR
2.4
Example 1
2.5
Detection Range
2.6
Search Radar Range Equation
2.7
Example 2
2.8
Radar Range Equation Summary
2.9
Exercises
References
Appendix 2A: Derivation of Search Solid Angle Equation
Chapter 3
Radar Cross Section
3.1
Introduction
3.2
RCS of Simple Shapes
3.3
Swerling RCS Models
3.3.1
Swerling Statistics
3.3.2
Swerling Fluctuation Models
3.3.3
Math Behind the Fluctuation Model
3.4
Relation of Swerling Models to Actual Targets
3.4.1
Simulating Swerling Targets

3.5
Frequency Agility and SW2 or SW4 Targets
3.5.1
Special Cases
3.6
Exercises
References
Chapter 4
Noise
4.1
Introduction
4.2
Noise in Resistive Networks
4.2.1
Thevenin Equivalent Circuit of a Noisy Resistor
4.2.2
Multiple Noisy Resistors
4.3
Equivalent/Effective Noise Temperature for Active Devices
4.4
Noise Figure
4.4.1
Derivation of Noise Figure
4.4.2
Attenuators
4.5
Noise Figure of Cascaded Devices
4.6
An Interesting Example
4.7
Output Noise Power When the Source Temperature Is Not T0
4.8
A Note About Cascaded Devices and the Radar Range Equation
4.9
Exercises
References
Chapter 5
Radar Losses
5.1
Introduction
5.2
Transmit Losses
5.3
Antenna Losses
5.4
Propagation Losses
5.5
Receive Antenna and RF Losses
5.6
Processor and Detection Losses
5.7
Exercises
References
Appendix 5A: Waveguide Attenuation
5A.1 Exercises
Appendix 5B: Atmospheric and Rain Attenuation
5B.1 Function tropatten.m
5B.1.1 Compute International Civil Aviation Organization (ICAO)
Standard Atmosphere 1964
5B.1.2 Absorption Coefficient for Oxygen
5B.1.3 Absorption Coefficient for Water Vapor
5B.2 Function troprefract.m
5B.3 Function troploss.m
5B.4 Function rainAttn2way.m
Chapter 6
Detection Theory
6.1
Introduction
6.2
Noise in Receivers

6.2.1
IF Configuration
6.2.2
Baseband Configuration
6.3
Signal in Receivers
6.3.1
Introduction and Background
6.3.2
Signal Model for SW0/SW5 Targets
6.3.3
Signal Model for SW1/SW2 Targets
6.3.4
Signal Model for SW3/SW4 Targets
6.4
Signal-Plus-Noise in Receivers
6.4.1
General Formulation
6.4.2
Signal-Plus-Noise Model for SW1/SW2 Targets
6.4.3
Signal-Plus-Noise Model for SW0/SW5 Targets
6.4.4
Signal-Plus-Noise Model for SW3/SW4 Targets
6.5
Detection Probability
6.5.1
Introduction
6.5.2
Amplitude Detector Types
6.5.3
Detection Logic
6.5.4
Calculation of Pd and Pfa
6.5.5
Behavior Versus Target Type
6.6
Determination of False Alarm Probability
6.6.1
Example 1—Computing Pfa
6.6.2
Example 2—Detection Contour
6.7
Summary
6.8
Exercises
References
Chapter 7
Matched Filter
7.1
Introduction
7.2
Problem Definition
7.3
Problem Solution
7.4
Matched Filter Examples
7.4.1
General Formulation
7.4.2
Response for an Unmodulated Pulse
7.4.3
Response for an LFM Pulse
7.5
Summary
7.6
Exercises
References
Chapter 8
Detection Probability Improvement Techniques
8.1
Introduction
8.2
Coherent Integration
8.2.1
SNR Analysis
8.2.2
Detection Analysis
8.3
Noncoherent Integration
8.3.1
Example 1

8.3.2
Example 2
8.4
Cumulative Detection Probability
8.4.1
Example 3
8.5
m-of-n Detection
8.6
Exercises
References
Appendix 8A: Noise Autocorrelation at the Output of a Matched Filter
Appendix 8B: Probability of Detecting SW1 and SW3 Targets on m Closely
Spaced Pulses
Appendix 8C: Cumulative Detection Probability
Chapter 9
Ambiguity Function
9.1
Introduction
9.2
Ambiguity Function Development
9.3
Example 1—Unmodulated Pulse
9.4
Example 2—LFM Pulse
9.5
Numerical Techniques
9.6
Ambiguity Function Generation Using the FFT
9.7
Exercises
References
Chapter 10
Waveform Coding
10.1
Introduction
10.2
FM Waveforms
10.2.1
LFM with Amplitude Weighting
10.2.2
Nonlinear FM (NLFM)
10.3
Phase Coded Pulses
10.3.1
Frank Polyphase Coding
10.3.2
Barker Coded Waveforms
10.3.3
PRN Coded Pulses
10.4
Step Frequency Waveforms
10.4.1
Doppler Effects
10.5
Closing Comments
10.6
Exercises
References
Appendix 10A: LFM and the sinc2(x) Function
Chapter 11
Stretch Processing
11.1
Introduction
11.2
Stretch Processor Configuration
11.3
Stretch Processor Operation
11.4
Stretch Processor SNR
11.4.1
Matched Filter
11.4.2
Stretch Processor
11.5
Stretch Processor Implementation

11.6
Doppler Effects
11.6.1
Expanded Transmit and Receive Signal Models
11.6.2
Effect of Doppler Frequency on Range Resolution
11.6.3
Effect of Doppler Frequency Mismatch on Range Error
11.7
Exercises
References
Chapter 12
Phased Array Antenna Basics
12.1
Introduction
12.2
Two-Element Array Antenna
12.3
N-Element Linear Array
12.4
Directive Gain Pattern (Antenna Pattern)
12.5
Beamwidth, Sidelobes, and Amplitude Weighting
12.6
Steering
12.7
Element Pattern
12.8
Phase Shifters
12.9
Computation of Antenna Patterns
12.10
Planar Arrays
12.10.1 Weights for Beam Steering
12.10.2 Array Shapes and Element Locations (Element Packing)
12.10.3 Feeds
12.10.4 Amplitude Weighting
12.10.5 Computing Antenna Patterns for Planar Arrays
12.10.6 Directive Gain Pattern
12.10.7 Grating Lobes
12.11
Polarization
12.12
Reflector Antennas
12.13
Other Antenna Parameters
12.14
Exercises
References
Appendix 12A: An Equation for Taylor Weights
Chapter 13
Signal Processor Analyses
13.1
Introduction
13.2
Clutter Model
13.2.1
Ground Clutter Radar Cross Section (RCS) Model
13.2.2
Ground Clutter Spectrum Model
13.2.3
Rain Clutter RCS Model
13.2.4
Rain Clutter Spectral Model
13.3
Signal Model
13.3.1
Signal Model Generation
13.3.2
Signal Analysis
13.4
Signal Processor Analyses
13.4.1
Background

13.4.2
Moving Target Indicator (MTI)
13.4.3
Pulsed Doppler Processors
13.4.4
Chaff Analysis
13.5
Exercises
References
Appendix 13A: Derivation of (13.43)
Appendix 13B: Proof that r(t) is Wide-Sense Cyclostationary Appendix 13C:
Derivation of (13.170)
Chapter 14
Radar Receiver Basics
14.1
Introduction
14.2
Single-Conversion Superheterodyne Receiver
14.3
Dual-Conversion Superheterodyne Receiver
14.4
Receiver Noise
14.5
The 1-dB Gain Compression Point
14.6
Dynamic Range
14.6.1
Sensitivity
14.6.2
Minimum Detectable and Minimum Discernable Signal
14.6.3
Intermodulation Distortion
14.6.4
Required Dynamic Range
14.7
Cascade Analysis
14.7.1
Cascade Analysis Conventions
14.7.2
Procedure
14.7.3
Power Gain
14.7.4
Noise Figure and Noise Temperature
14.7.5
1-dB Compression Point
14.7.6
Second-Order Intercept
14.7.7
Third-Order Intercept
14.8
Digital Receiver
14.8.1
Analog-to-Digital Converter
14.9
Receiver Configurations
14.10
Exercises
References
Chapter 15
Introduction to Synthetic Aperture Radar Signal Processing
15.1
Introduction
15.2
Background
15.2.1
Linear Array Theory
15.2.2
Transition to SAR Theory
15.3
Development of SAR-Specific Equations
15.4
Types of SAR
15.4.1
Theoretical Limits for Strip Map SAR
15.4.2
Effects of Imaged Area Width on Strip Map SAR Resolution
15.5
SAR Signal Characterization

15.5.1
Derivation of the SAR Signal
15.5.2
Examination of the Phase of the SAR Signal
15.5.3
Extracting the Cross-Range Information
15.6
Practical Implementation
15.6.1
A Discrete-Time Model
15.6.2
Other Considerations
15.7
An Algorithm for Creating a Cross-Range Image
15.8
Example 1
15.9
Down-Range and Cross-Range Imaging
15.9.1
Signal Definition
15.9.2
Preliminary Processing Considerations
15.9.3
Quadratic Phase Removal and Image Formation
15.10
Algorithm for Creating a Cross- and Down-Range Image
15.11
Example 2
15.12
An Image-Sharpening Refinement
15.13
Closing Remarks
15.14
Exercises
References
Chapter 16
Introduction to Space-Time Adaptive Processing
16.1
Introduction
16.2
Spatial Processing
16.2.1
Signal Plus Noise
16.2.2
Signal Plus Noise and Interference
16.2.3
Example 1
16.3
Temporal Processing
16.3.1
Signal
16.3.2
Noise
16.3.3
Interference
16.3.4
Doppler Processor
16.3.5
Example 2
16.4
Adaptivity Issues
16.5
Space-Time Processing
16.5.1
Example 3
16.5.2
Example 4
16.6
Adaptivity Again
16.7
Practical Considerations
16.8
Exercises
References
Chapter 17
Sidelobe Cancellation
17.1
Introduction
17.2
Interference Canceller
17.3
Interference Cancellation Algorithm

17.3.1
Single Interference Signal
17.3.2
Example 1
17.3.3
Multiple Interference Sources
17.4
Implementation Considerations
17.4.1
Form of vm(t) and va(t)
17.4.2
Properties of vs(t), vI(t), nm(t), and nan(t)
17.4.3
Scaling of Powers
17.4.4
Example 2
17.4.5
Practical Implementation Considerations
17.4.6
Example 3
17.5
Howells-Applebaum Sidelobe Canceller
17.5.1
Howells-Applebaum Implementation
17.5.2
IF Implementation
17.5.3
Example 4
17.6
Sidelobe Blanker
17.7
Exercises
References
Appendix 17A: Derivation of ϕ (17.40)
Chapter 18
Advances in Radar
18.1
Introduction
18.2
MIMO Radar
18.3
Cognitive Radar
18.4
Other Advancements in Radar Theory
18.5
Hardware Advancements
18.6
Conclusion
References
Appendix A
Suboptimal Filtering
A.1
Introduction
A.1.1
Example: Rectangular Pulse—Ideal Lowpass (Rectangular)
Filter
A.1.2
Example: Rectangular Pulse—One-Stage Single-Tuned RLC
Resonant Circuit
A.1.3
Example: Gaussian Pulse—Gaussian Filter
A.1.4
Example: Rectangular Pulse—Gaussian Filter or Gaussian
Filter—Rectangular Pulse
A.2
Summary
A.3
Exercises
References
Appendix B
Data Windowing Functions
References
Acronyms and Abbreviations
About the Authors

Index

Preface
This book is based on lecture notes for a three-course sequence in radar taught by Dr. Budge
at the University of Alabama in Huntsville. To create this book, we filled in some details that
are normally covered in lectures and added information in the areas of losses, waveforms,
and signal processing. We also added a chapter on receiver basics.
The first of the three courses, which focuses on the radar range equation and its various
progressions, provides an introduction to basic radar analysis covered in Chapters 1 through
9 of this book. Chapter 1 begins with definitions of radar-related terms and terminology,
which is followed in Chapter 2 by a detailed derivation of the radar range equation and
discussions of its various parameters. Following that, in Chapter 3, we discuss radar cross
section (RCS) with emphasis on the Swerling RCS models. We next discuss noise, noise
temperature, and noise figure in Chapter 4 and losses in Chapter 5 to round out our treatment
of the radar range equation. Following this, in Chapter 6, we discuss one of the main uses of
radar, which is the detection of target signals embedded in noise. We address detection theory
for several radar receiver configurations and Swerling RCS models. This leads naturally to
matched filter theory, discussed in Chapter 7, and its extension to the ambiguity function of
Chapter 9. We complete discussions of the radar range equation and detection theory with
discussions of methods of increasing detection probability and decreasing false alarm
probability in Chapter 8. This includes coherent integration, noncoherent integration, m-of-n
detectors, and cumulative detection probability.
The second course covers the material in Chapters 12 and 13. Chapter 12 includes analysis
of phased array antennas and signal processing. The phased array discussions include linear
and planar phased arrays and provide explanations of efficient methods for generating
antenna radiation patterns and computing directivity. The phased array discussions also
include discussions of time delay steering, phase steering, phase shifters, element patterns,
grating lobes, feeds, and polarization. The signal processor studies of Chapter 13 include
ground and rain clutter modeling, and the analysis of the clutter rejection and signal-to-noise
improvement of moving target indicator (MTI) and pulsed-Doppler signal processors. Also
included are detailed discussions of phase noise and range correlation, plus chaff modeling
and analysis. Chapter 14 contains a discussion of basic receiver analysis, which we plan to
include in future courses.
Finally, the third course covers advanced topics that include stretch processing, covered in
Chapter 11; phase coded waveforms, discussed in Chapter 10; synthetic aperture radar (SAR)
processing, discussed in Chapter 15; space-time adaptive processing (STAP), covered in
Chapter 16; and sidelobe cancellation (SLC), covered in Chapter 17. In all of these areas we
focus on implementation. For example, we discuss how to implement a SAR processor and
process actual SAR data from the RADARSAT I SAR platform. We show how to implement a
stretch processor, STAP processors, and both open- and closed-loop SLCs.
The main audience for the courses, and the intended audience for this book, consists of
practicing radar engineers who are pursuing an advanced degree with a radar specialization,

or have a need for a detailed understanding of radar analysis. As such, the courses and this
book focus on providing the theory and tools radar engineers need to perform their day-to-
day work in the fields of radar analysis, radar modeling and simulation, and radar design.
The homework exercises and the examples in this book are derived from real-world analysis
problems. In fact, one of the common phrases of radar engineers working at Dynetics, Inc.,
the authors’ company, is that the project they are working on is “Homework 16.”
This book focuses on analysis of radars and developing a firm understanding of how
radars and their various components work. As such, it does not avoid the sometimes
complicated mathematics needed to fully understand some of the concepts associated with the
analysis and design of radars. However, we try to summarize the results of mathematical
derivations into easily usable equations and, in some instances, convenient rules of thumb. We
hope you find this book useful, and we welcome your feedback.

Acknowledgments
We would like to express thanks to several people who contributed to this book. First, we
thank David Barton, our Artech reviewer, for his meticulous review of the chapters and his
many valuable suggestions on how to improve the material in the book. We also thank Stacy
Thompson, Shawn’s sister, for reviewing the various chapters and fixing our grammatical
errors. We would also like to thank others who reviewed portions of the book and/or offered
technical ideas, including Dr. B. K. Bhagavan, Dr. Steve Gilbert, Dr. Jeff Skinner, Alan Volz,
David Hardaker, John A. Cribbs, Joshua Robbins, Bill Myles, Alexandria Carr, and Zach
Hubbard.
On the publishing side, we owe a special thanks to the Dynetics Creative Media Solutions
department, especially Julie Wypyszynski for preparing and editing the manuscript and Bruce
Shelton for preparing the illustrations. We also thank Todd German for the photo used in the
Chapter 15 SAR homework.
We would also like to thank Professor Emeritus Ian G. Cumming of the University of
British Columbia, who authored Digital Processing of Synthetic Aperture Radar Data:
Algorithms and Implementation, for allowing us use of his SAR data and programs in
authoring this book. For providing the RADARSAT data of a scene of Vancouver used in the
SAR homework, we would like to thank Mr. Gordon Staples of MDA and the Canadian Space
Agency/Agence Spatiela Canadienne.
Finally, we thank Carmie, Merv’s wife, and Karen German, Shawn’s mother, for their
unwavering encouragement and support during the preparation of this book.

Chapter 1
Radar Basics
1.1 INTRODUCTION
According to Skolnik and other sources, the first attempt to detect targets using
electromagnetic radiation took place in 1904 (patent date for the telemobiloscope), when
Düsseldorf engineer Christian Hülsmeyer bounced waves off a ship [1–5]. During the 1920s,
several researchers, including R. C. Newhouse, G. Breit, M. A. Tuve, G. Marconi, L. S. Alder,
and probably many others in the United States and other countries, were obtaining patents on,
and conducting experiments with, radar. Although these appear to be the first instances of
radar usage, the term “radar” was not applied then. The name for radar was coined in 1940 by
two U.S. Navy officers (Lieutenant Commanders Samuel M. Tucker and F. R. Furth) as a
contraction of RAdio Detection And Ranging [6–8]. As with many other technological
advancements, significant early achievements in radar occurred during World War II. Since
then, radar technology has grown rapidly and continues to advance at a quick pace. We now
see wide application of radars in both commercial (airport radars, police radars, weather
radars) and military (search radars, track radars) arenas.
1.2 RADAR TYPES
Radars can use two types of signals:
• Pulsed, where the radar transmits a sequence of pulses of radio frequency (RF) energy;
• Continuous wave (CW), where the radar transmits a continuous signal.
When Hülsmeyer bounced electromagnetic waves off a ship, he used a CW radar; Breit and
Tuve used a pulsed radar.
CW radars typically use separate transmit and receive antennas because it is not usually
possible to receive with full sensitivity through an antenna while it is transmitting a high
power signal. Pulsed radars avoid this problem by using what we might think of as time
multiplexing. Specifically, the antenna connects to the transmitter while the pulse transmits
and connects to the receiver after the transmit phase. A transmit/receive (T/R) switch in the
radar performs this switching function. Such pulsed-signal radars constitute the most
common type because they require only one antenna.
The two basic types of radars are monostatic or bistatic radars:
• In a monostatic radar, the transmitter and receiver (as well as their associated antennas)
are collocated. This is the most common type of radar because it is the most compact. A
pulsed monostatic radar usually employs the same antenna for transmit and receive. A CW
monostatic radar usually employs separate transmit and receive antennas, with a shield

between them.
• In a bistatic radar, the transmitter and receiver are separated, often by very large distances
(> 1 km). Such a radar might be used in a missile seeker, with the transmitter located on
the ground or in an aircraft and the receiver located in the missile.
As indicated previously, the word radar is a contraction of RAdio Detection And Ranging.
This contraction implies that radars both detect the presence of a target and determine its
location. The contraction also implies that the quantity measured is range. While these
suppositions are correct, modern radars can also measure range rate and angle. Measuring
such parameters permits reasonably accurate calculations of the x-y-z location and velocity of
a target, and in some instances, reasonable estimates of the higher derivatives of x, y, and z.
Radars operate in the RF band of the electromagnetic spectrum between about 5 MHz (high
frequency, HF) and 300 GHz (millimeter wave, mm). Table 1.1 lists frequency bands [U.S.
Institute of Electrical and Electronics Engineers (IEEE) waveband specifications] and
associated frequencies. Another set of waveband specifications, the European and U.S. ECM
(electronic countermeasure) experienced some popularity in recent times, but has waned in
the past 10 or so years.
Typically, but not always,
• Search radars operate at very high frequency (VHF) to C-band;
• Track radars operate in S-, C-, X-, and Ku-bands, and sometimes in Ka-band;
• Active missile seekers operate in X-, Ku-, K-, and Ka-bands;
• Instrumentation radars and short-range radars sometimes operate in the Ka-band and
above.
Some notes on operating frequency considerations [9]:
• Low-frequency radars require large antennas or have broader beams (broader distribution
of energy in angle space—think of the beam of a flashlight). They are not usually
associated with accurate angle measurement.
• Low-frequency radars have limitations on range measurement accuracy because fine
range measurement implies large instantaneous bandwidth of the transmit signal. This
causes problems with the transmitter and receiver design because the bandwidth could
represent a significant percentage of transmit frequency.
• Range-rate measurement is not accurate in low-frequency radars because Doppler
frequency is directly related to transmit frequency.
• High power is easier to generate at low frequencies because the devices can be larger, thus
accommodating higher voltages and currents.
• Search calls for high power but not necessarily fine range or angle measurement. Thus,
search radars tend to use lower frequencies.
• Track calls for fine range and angle measurement but not necessarily high power. Thus,
track radars tend to use higher frequencies.
• The above notes often lead to the assignment of search and track functions to different
radars. However, modern radars tend to be “multifunction,” incorporating both purposes
in one. This usage often leads to tradeoffs in operating frequency and in search and track

functions.
Table 1.1
Radar Frequency Bands
Band
Frequency Range
Origin of Name
HF
3–30 MHz
High frequency
VHF
30–300 MHz
Very high frequency
UHF
300–1,000 MHz
Ultrahigh frequency
L
1–2 GHz
Long wave
S
2–4 GHz
Short wave
C
4–8 GHz
Compromise between S- and X-bands
X
8–12 GHz
Described fire control radars in World War II. X stands for “cross,” as in
“crosshairs”
Ku
12–18 GHz
Kurz—under
K
18–27 GHz
Kurz—German for “short wave”
Ka
27–40 GHz
Kurz—above
V
40–75 GHz
Very short
W
75–110 GHz
W follows V in the alphabet
mm
110–300 GHz
millimeter
Source: [10–12].
1.3 RANGE MEASUREMENT
The common way to measure range with a radar is to measure the time delay from
transmission to reception of a pulse. Figure 1.1 illustrates this notion. Since RF energy travels
at the speed of light, c ≈ 3 × 108 m/s,1 the time required for the transmit pulse to travel to a
target at a range of R is
The time required for the pulse to return from the target back to the radar is
Thus, the total round-trip delay between transmission and reception of the pulse is
Since we can measure τR in a radar, we can compute range by solving (1.3) for R, that is,

As a note, the term slant range suggests measurement along a line (often slanted) from the
radar to the target; as such, the term applies here to R. The term ground range, the distance
from the radar to the vertical projection of the target onto the ground, will be discussed later.
A rule of thumb for range measurement can be derived as follows. Suppose τR = τ μs. In
other words, suppose we express the time delay, often called range delay, in units of
microseconds. We can then write
Thus, we can compute the range by multiplying the range delay, in microseconds, by 150
m; the range computation scaling factor is 150 m/μs.
Figure 1.1 Illustration of range delay.
1.4 AMBIGUOUS RANGE
Since pulsed radars transmit a sequence of pulses, the determination of range to the target
poses a problem. The issue is where we choose t = 0 to compute range delay. The common
method is to choose t = 0 at the time of a transmit pulse; thus, t = 0 resets upon each transmit
pulse.
To define the problem, consider Figure 1.2, which shows transmit pulses spaced 400 µs
apart. Given a target range of 90 km, the range delay to the target is
This means the return from pulse 1 is not received until after pulse 2 is transmitted; the return
from pulse 2 is not received until after pulse 3 is transmitted; and so on. Since all transmit
pulses are the same and all received pulses are the same, we have no way of associating
received pulse 1 with transmit pulse 1. In fact, since the radar resets t = 0 on each transmit
pulse, it will associate received pulse k with transmit pulse k + 1. Further, it would measure the

range delay as 200 µs and conclude, in error, that the target range is
Because of this, we say that we have an ambiguity, or uncertainty, in measuring range.
If the spacing between pulses is τPRI, we say the radar has an unambiguous range of
Figure 1.2 Illustration of ambiguous range.
If the target range is less than Ramb, the radar can measure its range unambiguously. For a
target range greater than Ramb, the radar cannot measure its range unambiguously.
In the notation above, the term PRI stands for pulse repetition interval, or the spacing
between transmit pulses. A related term, pulse repetition frequency, or PRF, is the reciprocal
of the PRI.
To avoid range ambiguities, radar designers typically choose the PRI of a pulse train (a
group of two or more pulses) to exceed the range delay of the longest range targets of
interest. They also select transmit power to further diminish the possibility for long-range
target detection by the radar.
Ambiguous range sometimes presents a problem in search, but generally not in track. In
track, the radar tracking filters or algorithms provide an estimate of target range, which
enables the radar to “look” in the proper place, even given ambiguous returns.
Using waveforms with multiple PRIs provides another method for circumventing
ambiguous range problems. Figure 1.3 depicts an example of a multiple PRI transmit
waveform and appropriate received signal. In this example, the spacings between pulses are
400 µs, 300 µs, and 350 µs. As in the previous example, a target range delay of 600 µs is
posited. It will be noted that the time delay between the number 1 received pulse and the
number 2 transmit pulse is 200 µs, and the time delay between the number 2 received pulse
and the number 3 transmit pulse is 300 µs. The fact that the time delay between the most
recently transmitted and received pulses is changing can be used to indicate ambiguous range
operation. The radar can use this property to ignore the ambiguous returns.

Alternatively, the radar could use the measured range delays in a range resolve algorithm to
compute the true target range. Such an approach is used in pulsed Doppler radars because the
PRIs used in these radars almost always result in ambiguous range operation.
Figure 1.3 Staggered PRI waveform and ambiguous range.
Changing the operating frequency, fc, on each pulse provides yet another method of
circumventing the ambiguous range problem. In this case, if the return from pulse k arrives
after the transmission of pulse k + 1, the receiver will be tuned to the frequency of pulse k + 1
and will not “see” the return from pulse k.
Phased array radars, which steer the antenna beam electronically rather than mechanically,
often transmit a single pulse and then re-steer the beam to a different angular position. In this
situation, the concept of a PRI, and thus unambiguous range, is not strictly defined since there
is only one pulse. The term is used in such cases, nevertheless. The unambiguous range is
taken to be the range delay during which the beam stays in one position before moving to
another position. The time the beam stays in one position is termed a beam dwell.
1.5 USABLE RANGE AND INSTRUMENTED RANGE
The preceding discussion of ambiguous range might lead one to conclude that a radar can
detect (and track) targets at all ranges between 0 and Ramb. However, in practice, this is not the
case. The pulse received from a target at a range of zero would arrive at the radar
simultaneously with the transmission of the sounding pulse. Since the receiver is off during
this time, it cannot process the pulse. The minimum usable range is therefore equal to
where τp is the radar pulsewidth.
Similarly, a pulse from a target arriving at the receiver too near the next transmit pulse
prevents the entire pulse from entering the receiver before the receiver turns off for the next
transmit pulse. Thus, the receiver cannot completely process the received signal. This leads to
the conclusion that the maximum usable range is

That is, the maximum usable range extends to one pulsewidth before the next transmit pulse.
We define the time interval between Rmin and Rmax as the processing window. Occasionally,
these bounds can be exceeded somewhat; however, this does not often occur because it can
lead to processing difficulties.
Although Rmax defines the maximum usable range, most radars operate over a shorter
range interval, termed the instrumented range. The instrumented range is set by system
requirements and allows for factors such as display limits, circuit transients, radar
calibration, radar mode changes, and the like.
1.6 RANGE-RATE MEASUREMENT (DOPPLER)
In addition to measuring range, radars can also measure the rate of change of range, or range
rate. The radar accomplishes this by measuring the Doppler frequency; that is, the frequency
difference between the transmitted and received signals. To examine this further, consider the
geometry of Figure 1.4. The aircraft in this figure moves in a straight line at a velocity of v.
As a result, the range to the target changes continually. Indeed, over a differential time of dt,
the range changes by an amount dR, from R to R + dR. The resulting range rate is
We note that the range rate of Figure 1.4 is negative because the range decreases from time t
to time t + dt. Also, in general, Ṙ ≠ v. Equality holds only if the target flies along a radial path
relative to the radar.
Figure 1.4 Geometry for Doppler calculation.

Figure 1.5 Depiction of a transmit pulse.
We will briefly digress to consider the relationship of range rate to target position and
velocity in a Cartesian coordinate system centered at the radar. Suppose the target position and
velocity state vector is given by
where the superscript T denotes the transpose. We can write the target range as
and the range rate as
Now, let’s return to the problem of measuring range rate with a radar. To start, think about
the nature of the transmit pulse. In its simplest form, the transmit pulse constitutes a snippet of
a sinusoid, whose frequency is equal to the operating frequency of the radar (e.g., 100 × 106
Hz or 100 MHz for a radar operating at VHF, 109 Hz or 1 GHz for a radar operating at L-
band, or 10 × 109 Hz or 10 GHz for a radar operating at X-band). We normally term the
operating frequency the carrier frequency of the radar and denote it as fc or fo. Figure 1.5
depicts a transmit pulse example. This figure is not to scale, as it shows only 10 cycles of the
carrier over the duration of the pulse. For an X-band radar and a pulse duration, or
pulsewidth, of τp = 1 μs, there will be 10,000 cycles of the carrier over the duration of the
pulse.
We can mathematically represent the transmit pulse as
where

Ideally, the radar receives an attenuated, delayed version of the transmit signal, that is,
where the amplitude scaling factor, A, comes from the radar range equation (see Chapter 2),
and the delay, τR, is the range delay discussed in the previous section. To compute Doppler
frequency, we acknowledge that the range delay in (1.17) is a function of time, t, and write
Substituting (1.18) into (1.17) gives
The argument of the cosine term contains the necessary information.2 We wish, then, to
examine
Expanding R(t) into its Taylor series about t = 0 gives
Substituting (1.21) into (1.20) yields
or

In (1.23),
• φR = −2πfc(2R/c) is a phase shift due to range delay and is of little use in practical radars.
• g′(t2, t3,...) = −4πfcg(t2, t3, …)/c is a nonlinear phase term usually ignored (until it creates
problems in advanced signal processors).
• fd = −fc(2Ṙ/c) is the Doppler frequency of the target.
Recalling the radar wavelength, λ, is given by
allows us to rewrite fd in its more standard form
Dropping the g′(t2, t3, …) term and substituting (1.23) into (1.19), we get
where we have used τR = 2R(0)/c. In (1.26), we note that the frequency of the returned signal is
fc + fd, instead of simply fc. Thus, comparing the frequency of the transmit signal to the
frequency of the received signal permits us to determine the Doppler frequency, fd. Obtaining
fd allows computation of the range rate from (1.25).
In practice, it is not as easy to measure Doppler frequency as the calculations above imply.
The problem lies in the relative magnitudes of fd and fc. Consider the following example,
whereby a target travels at approximately Mach 0.5, or about 150 m/s. For now, assume the
target flies directly toward the radar, so Ṙ = −v = −150 m/s. Assume, furthermore, that the
radar operates at X-band with a specific carrier frequency of fc = 10 GHz. With these
assumptions, we get
and

Comparing fc to fd yields the observation that fd is a million times smaller than fc.
While measuring Doppler frequency is not easy, it is achievable. Such Doppler frequency
measurement requires a very long transmit pulse (on the order of milliseconds rather than
microseconds) or the processing of several pulses.
1.7 DECIBELS
A measurement convention commonly used in radar analyses is the decibel. Engineers at Bell
Telephone Laboratories (now “Bell Labs”) originally formulated the concept of the decibel to
measure losses over given distances of telephone cable [13]. By definition, a decibel
representation of a quantity equals 10 times the logarithm to the base 10 of that quantity. As
implied by its name, a decibel is 1/10 of a bel, a logarithm of quantity that was coined by the
Bell Labs engineers to honor Alexander Graham Bell.
The decibel, abbreviated dB, is useful in radar analyses because of the large range of
numbers encountered in such analyses. The abbreviation “dB,” by itself, means 10log (power
ratio). Thus, the signal-to-noise ratio in decibels is computed using
where PS is the signal power [in watts (W), milliwatts (mW), kilowatts (kW), and so forth]
and PN is the noise power in the same units as PS.
Equation (1.29) provides the “standard” use for the decibel, as originally conceived by Bell
Labs. Since then, analysts in the fields of radar, electronics, and communications have
expanded the definition of the decibel to include many other forms:
• The abbreviation “dBW” denotes power level relative to 1 watt, or 10log(P), where P is
power in watts.
• The abbreviation “dBm” denotes power level relative to 1 milliwatt, or 10log(P/0.001) =
30 + 10log (P).
• The abbreviation “dBV” denotes voltage level relative to 1 volt root mean square (rms),
or 20log(|V|).3
• The abbreviation “dBsm” denotes area in square meters relative to 1 square meter, or
10log(A), where A is area in square meters. We use this to represent the radar cross
section of a target.
• The abbreviation “dBi” denotes antenna directivity (gain) relative to the directivity of an
isotropic antenna, or 10log(G), where G is the antenna directivity in watts per watt (W/W).
The gain of an isotropic antenna is taken to be 1 W/W.
1.8 dB ARITHMETIC
In radar analyses, it is often convenient or necessary to perform conversions from ratios to

dB values without a calculator. To aid in this, some common relations between ratios and dB
are contained in Table 1.2. These relations can be used to find other conversions by
remembering that multiplication of ratios translates to addition of dB values. For example, to
compute the dB value for a ratio of 4, we recognize that 4 = 2 × 2. Thus, the dB value
corresponding to a ratio of 4 is 3 + 3 = 6 dB. As another example, the dB value for a ratio of
50 can be found by recognizing that 50 = 10 × 10/2, and thus the dB value is 10 + 10 − 3 = 17
dB.
Table 1.2
Relation Between Ratios and Decibels
dB
Power Ratio
−10
0.1000
−9
0.1259
−6
0.2512
−3
0.5012
0
1.0000
3
1.9953
6
3.9811
10
10
20
100
30
1,000
1.9 COMPLEX SIGNAL NOTATION
When we wrote the equations for vT(t) and vR(t) in (1.15) and (1.19), we used what we term
real signal notation. With this notation, the signal equations are real functions of time that
include sines and cosines. We find that when we need to work with such real functions, we are
faced with the manipulation of these sines and cosines, which can be cumbersome because of
the need to use trigonometric identities.
To circumvent the problems associated with the manipulation of sines and cosines, radar
analysts commonly use an alternate signal notation termed complex signal notation. With this
notation, signals are represented as complex functions through the use of exponentials with
complex arguments. For example, we would write the transmit signal of (1.15) in complex
signal notation as
and the receive signal as

where the superscript c is used to distinguish these signals from vT(t) and vR(t). In these
equations, 
.
Through the use of the Euler identity
we can relate vT(t) and vR(t) to their complex signal counterparts as
and
In these equations, real[x] denotes the real part of the complex number x. The imaginary part
would be denoted as imag[x].
The primary reason for using complex signal notation is ease of mathematical
manipulation. Specifically, multiplying exponentials is easier than multiplying sines and
cosines. However, we also find that complex signal notation often provides a convenient and
clear means of describing signal properties. For example, a complex signal notation for
(1.25) might be
In this equation, we recognize Arect[(t − τR− τp/2)/τp] as the magnitude of the complex (and
real) signal. We recognize ϕR as a constant phase part of the signal (since it is the argument of

the complex exponential, ejϕR). The term 2πfdt is a phase that depends on the Doppler
frequency. We recognize the last term as the carrier frequency part of the signal. Thus, with
complex signal notation, we can conveniently characterize the various properties of the signal
by separating them into separate complex exponential and magnitude terms.
An extension of complex signal notation is baseband signal notation. With this notation, we
drop the carrier exponential. This is the approach commonly used in alternating current (AC)
circuit analysis (steady-state analysis of resistor-inductor-capacitor circuits excited by a
sinusoid). In those analyses, the sinusoid is not explicitly used. Instead, the voltages and
currents in the circuit are represented by their amplitude and phase, and the circuit analyses
are performed using complex mathematics.
A means of “dropping” the carrier term is to set fc to zero. This is the basis of the word
“baseband.” That is, baseband signals are assumed to have a carrier frequency of zero.
We will use all three signal notations in this book, depending on need. Generally, we will
use real signals when we need to specifically address the real properties of signals. We will
use complex signal notation when we need to explicitly discuss the operating frequency but do
not want to have to manipulate real signals. We will use baseband signal notation when we are
not specifically concerned with the operating frequency of the signal. We will also use
baseband signal notation when we discuss signal processing, since many digital signal
processors explicitly operate on baseband signals.
1.10 RADAR BLOCK DIAGRAM
Figure 1.6 contains a generic radar block diagram that includes the major areas we will
discuss in this book. We start in Chapter 2 by tracing a signal from the transmitter throught the
antenna to the target and back to the matched filter (through the antenna and receiver) to
derive one of the key equations of radar theory: the radar range equation. We follow that in
Chapters 3, 4, and 5 with discussions of the radar cross section, noise, and loss terms of the
radar range equation. In Chapter 6, we present detailed discussions of false alarm probability
and detection probability for the Swerling target types and targets with a constant radar cross
section. The discussions of detection theory naturally lead to matched filter theory, which we
cover in Chapter 7. In Chapter 8, we discuss signal processing from the perspective of
improving detection probability. Later, in Chapter 13, we provide detailed discussions of
signal processing from the perspective of clutter mitigation. In Chapter 14, we discuss
receivers, including the modern field of digital receivers.
In Chapters 9 and 10, we address the waveform generator portion of the radar by
discussing the ambiguity function and an assortment of waveform codings. In Chapter 12, we
move to the antenna and present a discussion of phased array antennas. The remainder of the
text is devoted to a discussion of advanced topics, including stretch processing, synthetic
aperture radar, space-time adaptive processing, sidelobe cancellation, and others.

Figure 1.6 Generic radar block diagram.
1.11 EXERCISES
1.
Find the round-trip time delay for a target at the following ranges:
a)
15 km
b)
37 mi
c)
350 kft
d)
673 nmi
2.
What minimum PRIs does a radar require in order to operate unambiguously in range
for the target cases of Exercise 1? What PRFs correspond to these minimum PRIs?
Ignore pulsewidth in formulating your answer. What would your answers be if you
include a pulsewidth of 200 µs?
3.
Find the Doppler frequencies for an 8-GHz (low X-band) radar and the following target
range rates:
a)
−100 m/s
b)
150 mph
c)
−30 m/s
4.
A target has a state vector given by
where the state vector is referenced to the radar. Find the range (R) and range rate (Ṙ) of
the target. Find the round-trip time delay (τR) to the target. Find the Doppler frequency
for a radar operating at 8.5 GHz.
5.
Skolnik [1] poses an interesting problem: if the moon is located approximately 384,400

km from the Earth, what is the range (time) delay (τR) to the moon? What PRF should we
use to operate unambiguously in range?
6.
In Section 1.6, we chose to ignore the term
We want to verify this as a valid assumption, at least for the second derivative term. In
particular, we want to show, for a specific, realistic example, that the variation across the
pulse of the phase
is small compared to the variation across the pulse of the phase
To do this, compare
to
We intend for you to show ∆ϕg ≪ ∆ϕd. For the equation above, t0 = τR = 2R/c, the time it
takes for the pulse to return from the target. For this exercise, use
a pulsewidth of τp = 1μs, and a carrier frequency of fc = 10 GHz (X-band). Assume the
higher derivatives of x, y, and z equal zero. Verify the assumption by computing the
following:

a)
Ṙ
b)
c)
fd
d)
Δϕg
e)
Δϕd
Is the assumption valid? Explain briefly.
7.
If a radar generates a power of 100 kW (100,000 W), what is the power in dBW? What is
the power in dBm?
8.
If the radar in Exercise 7 receives a return target power of -84 dBm, what is the received
power in decibels relative to 1 W? What is the received power in watts? What is the
received power in milliwatts (1 milliwatt = 10−3 W)?
References
[1]
Skolnik, M. I., Introduction to Radar Systems, 3rd ed., New York: McGraw-Hill, 2001, pp. 14–16.
[2]
Fink, D. G., Radar Engineering, New York: McGraw-Hill, 1947, pp. 3–8.
[3]
Ridenour, L. N., Radar System Engineering, vol. 1 of MIT Radiation Lab. Series, New York: McGraw-Hill, 1947;
Norwood, MA: Artech House (CD-ROM edition), 1999, pp. 13–17.
[4]
Guarnieri, M., “The Early History of Radar,” IEEE Ind. Electron. Mag., vol. 4, no. 3, Sept. 2010, pp. 36–42.
[5]
Pritchard, D., Radar War: Germany’s Pioneering Achievement, 1904–45, Wellingborough, Northamptonshire,
England: Patrick Stephens, 1989, Chapter 1.
[6]
Page, R. M., The Origin of Radar, Garden City, NY: Anchor Books, 1962.
[7]
Buderi, R., The Invention that Changed the World: How a Small Group of Radar Pioneers Won the Second World
War and Launched a Technical Revolution, New York: Simon & Schuster, 1996.
[8]
Gebhard, L. A., “Evolution of Naval Radio-Electronics and Contributions of the Naval Research Laboratory,” Naval
Research Laboratory, Washington, D.C., Rep. No. 8300, 1979. Available from DTIC as ADA084225.
[9]
Budge, M. C., Jr., “EE 619: Intro to Radar Systems,” www.ece.uah.edu/courses/material/EE619/index.htm.
[10]
IEEE Standard Dictionary of Electrical and Electronic Terms, 6th ed., New York: IEEE, 1996.
[11]
Barton, D. K., Radar System Analysis and Modeling, Norwood, MA: Artech House, 2005.
[12]
P-N Designs, Inc., “Frequency Letter Bands.” www.microwaves101.com/encyclopedias/frequency-letter-bands.
[13]
Martin, W.H., “Decibel—The Name for the Transmission Unit,” Bell Syst. Tech. J., vol. 8, no.1, Jan. 1929, pp.1–2.
1The exact value for the speed of light in a vacuum is 299,792,458 m/s; c ≈ 3 × 108 m/s provides a useful rule of thumb.
2 The impact of the time-varying range delay on the rect[•] function is that the returned pulse will be slightly shorter or longer
than the transmit pulse. This difference in pulsewidths is usually very small and can be neglected.
3 Differing applications for dBV use 1 volt root mean square, peak-to-peak, or peak as reference.

Chapter 2
Radar Range Equation
2.1 INTRODUCTION
One of the simpler equations of radar theory is the radar range equation. Although it is one of
the simpler equations, ironically, it is an equation that is easily misunderstood and misused.
The problem lies not with the equation itself, but with the various terms that it is composed of.
It is our belief that an understanding of the radar range equation leads to a solid foundation in
the fundamentals of radar theory. Because of the difficulties associated with using and
understanding the radar range equation, a considerable portion of this book is devoted to its
terms and the items it impacts, such as antennas, receivers, matched filters, signal processors,
and detection theory.
According to David K. Barton, the radar range equation was developed during World War
II, with the earliest associated literature subject to military security restrictions [1]. Kenneth A.
Norton and Arthur C. Omberg of the U.S. Naval Research Laboratory authored the first
published paper on the radar range equation in 1947, titled “The Maximum Range of a Radar
Set” [1–3].
2.2 BASIC RADAR RANGE EQUATION
One form of the basic radar range equation is
where [4]
• SNR denotes the signal-to-noise ratio in units of joules per joule, or J/J. The equivalent
units are watt-seconds per watt-seconds, or W-s/W-s.
• ES denotes the signal energy, in joules (J) or watt-seconds (W-s), at some point in the
radar receiver—usually at the output of the matched filter or the signal processor.
• EN denotes the noise energy, in joules, at the same point that Es is specified.
• PT, termed the peak transmit power, denotes the average power, in watts, during radar
signal transmission. Although PT can be specified at the output of the transmitter or at
some other point, such as the output of the antenna feed, we specify it here as the power at
the output of the transmitter.
• GT denotes the directivity, or directive gain, of the transmit antenna in units of watts per
watt.

• GR denotes the directivity, or directive gain, of the receive antenna in units of watts per
watt. In many cases, GR = GT.
• λ denotes the radar wavelength in units of meters.
• σ denotes the average target radar cross section (RCS) in units of square meters.
• τp is the transmit pulsewidth, in seconds. In this book, we assume the transmit pulse has a
rect[x] function envelope.1 τp is the width of that rect[x] function.
• R denotes the slant range from the radar to the target in units of meters.
• k denotes Boltzmann’s constant and is equal to 1.3806503×10−23 W/(Hz K) or (W-s)/K,
although it is often truncated to 1.38×10−23 (W-s)/K.
• Ts = Ta + (Fn − 1) T0 W-s is the system noise temperature, where Ta is an antenna noise
temperature used to characterize environment noise.
• T0 denotes a reference temperature in degrees kelvin (K). The IEEE defines noise figure
in terms of a noise temperature of T0 = 290 K [5], which results in the rule of thumb
approximation of kT0 = 4×10−21 W/Hz.
• Fn denotes the overall radar noise figure2 and is dimensionless or in units of watts per
watt.
• L denotes all the losses one takes into consideration when using the radar range equation.
This term, which accounts for losses that apply to the signal and not the noise, has the
units of watts per watt. L accounts for a multitude of factors that degrade radar
performance, including those related to the radar itself, the environment in which the
radar operates, the operator of the radar, and, often, the inexperience of the radar analyst.
Loss factors are covered in more detail in Chapter 5.
The radar range equation of (2.1) is termed the single-pulse radar range equation because
the SNR calculation is based on a single transmit pulse. If the radar transmits and processes
several pulses, the equation can be modified by adding a multiplicative term, GSP, that
accounts for the transmission and processing of multiple pulses. This is addressed in Chapter
8.
The form of the radar range equation of (2.1) is nontraditional [7–9] in that SNR is
specified as an energy ratio rather than a power ratio. The latter form is briefly discussed in
Section 2.3. The energy ratio form is used in this book because the power ratio form requires
specification of a noise bandwidth. It has been the authors’ experience that this sometimes
causes confusion for both students and practicing radar analysts. The energy ratio
formulation circumvents this problem by using τp instead of a noise bandwidth. As a note, τp
is the uncompressed pulsewidth; it is the width of the envelope of the transmit pulse. Any phase
(or frequency) modulation on the pulse is not a factor in the radar range equation, except,
possibly, as a loss due to differences between the received pulse and the matched filter
impulse response (see Chapter 5).
We will derive the radar range equation and attempt to carefully explain its various terms
and their origins. We start by deriving ES, the signal energy component, and follow this by
deriving EN, the noise energy component.

2.2.1 Derivation of Es
2.2.1.1 The Transmitter
We begin the derivation at the transmitter output and go through the waveguide and antenna
out into space (see Figure 2.1). For now, assume the radar is in free space. We can account for
the effects of the atmosphere in the loss term, L. We assume the transmitter generates a single
pulse with a rectangular envelope that has a width of τp. Figure 2.2 contains a simplified
representation of this pulse. In this example, the pulse is modulated with a constant frequency
of fc, the carrier frequency.3
Figure 2.1 Transmit section of a radar.
Figure 2.2 Depiction of a transmit pulse.
The average transmit power in the signal over the duration of the pulse, is termed the peak
transmit power and is denoted as PT. We term this power the peak transmit power because
later we will consider the transmit power averaged over many pulses.
The waveguide in Figure 2.1 carries the signal from the transmitter to the antenna feed
input. The waveguide’s only feature of interest in the radar range equation is that it is a lossy
device that attenuates the signal. Although we only refer to the “waveguide” here, there are
several devices included between the transmitter and antenna feed of a practical radar (see
Chapter 5).
Because it is a lossy device, we characterize the waveguide in terms of its loss, which we
denote as Lt and term transmit loss. Since Lt is a loss, it is greater than unity. With this, the
power at the input to the antenna feed takes the form

Generally, the feed and other components of the antenna attenuate the signal further. If we
consolidate all these losses into an antenna loss term, Lant, the radar finally radiates the power
Since the pulse envelope width is τp, the energy radiated by the antenna is
2.2.1.2 The Antenna
The purpose of the radar antenna is to concentrate, or focus, the radiated energy in a small
angular sector of space. As an analogy, the radar antenna works much like the reflector in a
flashlight. Like a flashlight, a radar antenna does not perfectly focus the beam. For now,
however, we will assume it does. Later, we will account for imperfect focusing by using a
scaling term.
Given the purpose above, we assume all the radiated energy is concentrated in the area,
Abeam, indicated in Figure 2.3. With this, the energy density over Abeam is
To extend (2.5) to the next step, we need an equation for Abeam. Given lengths for the major
and minor axes of the ellipse in Figure 2.3 of RθA and RθB, we can write the area of the
ellipse:
We recognize that the energy is not uniformly distributed across Aellipse and that some of
the energy will “spill” out of the area Aellipse (i.e., the antenna does not focus the energy
perfectly, as indicated earlier). We account for this by replacing π/4 with a scale factor KA.
Further discussion of KA will follow shortly. We can write Abeam, then, as follows:
Substituting (2.7) into (2.5) produces the following:

We now define a term, GT, the transmit antenna directivity, or directive gain, as
Using (2.9) to rewrite (2.8), we get
Figure 2.3 Radiation sphere with antenna beam.
We reiterate: the form of antenna directivity given in (2.9) depends upon the assumption that
Lant captures the losses associated with the feed and other components of the antenna. Some
analysts combine the feed and antenna losses with the transmit antenna directivity and term the
result the power gain, or simply gain, of the antenna [10]. We will avoid doing so here, owing
to the confusion it produces when using (2.9) and the difficulties associated with another form
of directivity, presented shortly.
The form of GT in (2.10) and the radar range equation tacitly assume an antenna pointed
directly at the target. If the antenna is not pointed at the target, we must modify GT to account
for this. We do this by means of an antenna pattern, which is a function that gives the value of
GT at the target, relative to the antenna’s pointing direction.
2.2.1.3 Effective Radiated Power
We temporarily interrupt our derivation to define the quantity termed effective radiated

power. To do so, we ask the question: What power would we need at the output of an isotropic
radiator to produce an energy density of SR at all points on a sphere of radius R? An isotropic
radiator (ideal point source) is a hypothetical antenna that does not focus energy but instead
distributes it uniformly over the surface of a sphere centered on the antenna. Though it cannot
exist in the real world, the isotropic radiator serves a mathematical and conceptual function in
radar theory, not unlike that of the impulse function in mathematical theory.
By denoting the effective radiated power as Peff and recalling the surface area of a sphere
of radius R is 4πR2, we can write the energy density on the surface of the sphere (assuming
lossless propagation) as
If we equate (2.10) and (2.11) and solve for Peff, we obtain
as the effective radiated power (ERP).
We emphasize that Peff is not the power at the output of the antenna. The power at the output
of the antenna is PT/LtLant. The antenna’s purpose is to focus this power over a relatively
small angular sector.
2.2.1.4 Antenna Directivity
We turn next to the factor KA in (2.9). As we indicated, KA accounts for the properties of the
antenna. Specifically, it accounts for two facts:
• The energy is not uniformly distributed over the ellipse.
• Not all of the energy is concentrated in the antenna beam (the ellipse of Figure 2.3). Some
energy “spills” out the ellipse into what we term the antenna sidelobes.
The value 1.65 is a somewhat common value for KA [11, p. 143]. Using this figure, we can
write the antenna directivity as
We term the quantities θA and θB the antenna beamwidths, which have the units of radians. In
many applications, θA and θB are specified in degrees. In this case, we can write the directivity
as

where the two beamwidths in the denominator are in degrees. The derivation of (2.14) is
straightforward and left as an exercise.
While (2.14) uses a numerator of 25,000, various authors provide alternative
approximations, accounting for factors such as antenna type, beamshape, sidelobe
characteristics, and so on. For example, some authors use 41,253, which would apply to a
rectangular beam pattern with no sidelobes and would be indicative of an ideal antenna with
maximum directivity [7]. Similarly, some authors use 32,383 for a rectangular aperture with
uniform illumination and 33,709 for circular apertures with uniform illumination [10]. As
still another variant, some authors prefer 26,000 over 25,000 [10]. It has been the authors’
experience that 25,000 or 26,000 apply well to antennas that use some type of weighting to
reduce sidelobes. As a note, the different approximations correspond to different values of
KA.
To visualize the concept of beamwidth, consider Figure 2.4, which is a plot of GT(α,ε)
versus α for ε = 0. As discussed in Chapter 12, GT(α,ε) is a means of representing antenna
directivity as a function of target location relative to antenna pointing angles. If α = 0 and ε =
0, the beam is pointed directly at the target and the directivity is maximum. As illustrated in
Chapter 12, α and ε are orthogonal angles roughly related to azimuth and elevation,
respectively.
The unit of measurement on the vertical axis of Figure 2.4 is dBi, or decibels relative to
isotropic (see Chapter 1), the common unit of measurement for GT in radar applications. We
define the antenna beamwidth as the distance between the 3-dB points4 of Figure 2.4. These 3-
dB points are the angles where GT(α,ε) is 3 dB below its maximum value. With this, we find
the antenna represented in Figure 2.4 has a beamwidth of 2°. We might call this θB, of Figure
2.3 and (2.14). If we were to plot GT(α,ε) versus ε for α = 0, and find a distance between the 3-
dB points of 2.5°. We would then say the beamwidth, θA, was 2.5° The antenna directivity
would be computed as

Figure 2.4 Sample antenna pattern.
In subsequent sections, we drop the notation dBi and use dB.
The humps on either side of the central antenna beam depicted in Figure 2.4 are the antenna
sidelobes discussed above.
2.2.1.5 The Target and Radar Cross Section
To return to our derivation, we have an equation for SR, the energy density at the location of
the target. As the electromagnetic wave passes the target, the target captures some of its
energy and reradiates it toward the radar. More accurately, the electromagnetic wave induces
currents on the target, and the currents generate another electromagnetic wave that propagates
away from the target. Analysts occasionally designate this as energy reflection, a technically
incorrect term. The process of capturing and reradiating energy is very complicated and the
subject of much research. For now, we take a simplified approach to the process by using the
concept of radar cross section (or RCS). We note that SR has the units of W-s/m2. Therefore, if
we were to multiply SR by an area, we would convert it to an energy. This is what we do with
RCS, which we denote by σ and ascribe the units of m2, or dBsm if represented in dB units.
Hence, we represent the energy captured and reradiated by the target as
To continue our idealized assumption, we posit the target acts as an isotropic radiator and
radiates Etgt uniformly in all directions. The target, in fact, behaves much like an actual
antenna and radiates energy with different amplitudes in different directions.
Given the assumption that Etgt is the energy radiated by a target and the target acts as an
isotropic antenna, we can represent the energy density at the radar as

or, by substituting (2.10) into (2.16) and the result into (2.17),
2.2.1.6 Antenna Again
As the electromagnetic wave from the target passes the radar, the radar antenna captures a part
of this wave and sends it to the radar receiver. If we extend the logic we applied to the target,
we can formulate the energy at the output of the antenna feed as
where Ae denotes the effective area of the antenna and is a measure describing the antenna’s
ability to capture the returned electromagnetic energy and convert it into usable power. A
more common term for Ae is effective aperture of the antenna.
The effective aperture is related to the physical area of the antenna. That is,
where Aant is the area of the antenna projected onto a plane placed directly in front of the
antenna and ρant denotes the antenna efficiency. We make this clarification of area because we
do not want to confuse it with the actual surface area of the antenna. For example, if the
antenna is a parabola of revolution (a paraboloid), a common type of antenna, the actual area
of the antenna would be the area of the paraboloidal surface of the antenna, whereas Aant is the
area of the disc defined by the front rim of the antenna. In most phased array antennas (flat-
face phased array antennas), Aant is the area of the part of the antenna containing the array
elements.
While the antenna efficiency can take on any value between 0 and 1, it is seldom below 0.5
or above 0.8 [12]. A rule of thumb for the antenna efficiency value is ρant = 0.6.
Substituting (2.18) into (2.19) yields
2.2.1.7 Antenna Directivity Again
Equation (2.21) is not very easy to use because of the Ae term. We can characterize the antenna
more conveniently by using directivity, much as we did on transmit. According to antenna
theory, we can relate antenna directivity to effective aperture by the equation [13, p. 61; 8, p. 6]

Substituting (2.22) into (2.21) produces the following:
We next need to propagate the signal through the receiver. We do this by including a gain
term, G, which accounts for all of the receiver components up to the point where we measure
SNR. With this, we get
2.2.1.8 Losses
As a final step in this part of the development, we need to account for losses we have ignored
thus far. There are many losses that we will need to account for (see Chapter 5). For now, we
will consolidate all these losses with LtLant and denote them by L. Using this approach, we say
the signal energy in the radar is given by
which is Erec, with the additional losses included.
We said ES denotes the signal energy in the radar, although we did not say where in the
radar. We will defer this discussion for now and turn our attention to the noise energy term,
EN.
2.2.2 Derivation of EN
The two main contributors to noise in radars are the environment and the electronic
components of the receiver. Environment noise includes radiation from the earth, galactic
noise, atmospheric noise, and, in some instances, manmade noise such as noise jammers.
Galactic noise includes cosmic background radiation and solar or other star noise. The
environment noise we consider is earth and galactic noise.
Electronic equipment noise is termed thermal noise (also known as Johnson noise) and arises
from agitation of electrons caused by heat [14, p. 752; 15]. This form of noise was discovered
by Johnson [15] and first analyzed by Nyquist [16]. One of the equations in Nyquist’s paper
leads to a definition of noise power spectral density, or energy, for resistive devices as

where k = 1.38×10−23 W-s/K denotes Boltzmann’s constant and T denotes the noise
temperature of the resistor in degrees kelvin (K).
Equation (2.26) is actually a limiting case of one form of Planck’s law. This is discussed
further in Chapter 4. An implication of (2.26) is that the noise energy (in resistive devices) is
independent of frequency.
Device manufacturers and communication analysts [17, 18] have adopted a modified form
of (2.26) for electronic devices given by
where F is termed the noise figure of the device and T0 is the previously discussed reference
temperature normally referred to as “room temperature.” In fact, T0 = 290 K or 16.84ºC (0ºC
= 273.16 K), or about 62ºF, which, by some standards, may be room temperature. It is
interesting to note that kT0 = 4×10−21 W/Hz, which makes one think the (somewhat arbitrary)
value of T0 = 290 K was chosen to make kT0 a “nice” number, and not because it is room
temperature. While T0 = 290 K is now the standard (the IEEE defines noise figure in terms of
a noise temperature of 290 K [5]), other reference temperatures have been used in the past
(e.g., 291, 292, 293, and 300 K [2, 19–22]).
The N0 terms of (2.26) and (2.27) were developed for electronic components and not
environment noise. However, radar analysts have adopted (2.26) as a way of characterizing
the energy in a radar due to noise in the environment as well as in the electronics. We will do
the same here. Thus, we define the noise energy at the input to the matched filter as
In this equation, G is the same overall receiver gain that appeared in (2.25). Ts is termed the
system noise temperature and Ta is termed the antenna temperature. Fn is the overall noise
figure of the radar from the “antenna face” to the input to the matched filter. It includes the
noise figures of all active and passive devices in the radar, including any antenna components
(e.g., phase shifters, waveguides, feeds, combiners) that exhibit an ohmic, or dissipative, loss.
Equation (2.28) is derived in Chapter 4.
The antenna temperature, Ta, provides a means of characterizing the environment noise in
the radar. Blake [23] provides an equation for Ta for the case where the radar beam is pointing
into the sky but not directly at the sun or a star (an example of the latter is given in Chapter 4).
His equation is
This equation also takes into consideration that earth noise is entering through the antenna

sidelobes and backlobes. It assumes an antenna without ohmic losses, which would be the case
here since the ohmic losses of the antenna are included in Fn. The temperature, T′a, is
determined from Figure 2.5, which comes from [23]. In the figure, θ is the elevation angle of
the radar beam, relative to the horizon. The assumptions upon which the figure is based are
provided after Figure 2.5 as a quote from Blake’s NRL report.
Figure 2.5 Noise temperature of an idealized antenna. [Source: L. V. Blake, “A Guide to Basic Pulse-Radar Maximum-Range
Calculation,” NRL Report 6930, Naval Research Report Laboratory (1969).]
As Blake describes in NRL Report 6930 [23], Figure 2.5 shows the
noise temperature of an idealized antenna (lossless, no earth-directed side lobes) at the
earth’s surface as a function of frequency for a number of beam elevation angles. The
solid curves are for the geometric-mean galactic temperature, sun noise 10 times the
quiet level, the sun in a unity-gain side lobe, a cool temperate-zone troposphere, 30ºK
cosmic blackbody radiation, and zero ground noise. The upper dashed curve is for
maximum galactic noise (center of galaxy, narrow-beam antenna), sun noise 100 times
the quiet level, zero elevation angle, and other factors the same as for the solid curves.
The lower dashed curve is for minimum galactic noise, zero sun noise, and a 90º

elevation angle. The slight bump in the curves at about 500 MHz is due to the sun noise
characteristic. The curves for low elevation angles lie below those for high angles at
frequencies below 400 MHz because of the reduction of galactic noise by atmospheric
absorption. The maxima at 22.2 GHz and 60 GHz are due to water-vapor and oxygen
absorption resonances.
An alternative to the system noise temperature, Ts = Ta + (Fn − 1) T0, used in (2.28) is
This form uses the assumption Ta =T0 and would be a reasonable approximation for the case
where Fn was large (greater than about 7 dB) or where one was performing preliminary radar
range equation calculations. Otherwise, the Ts of (2.28) should be used. As a note, [11] points
out that Ta would equal T0 if the radar beam was pointing directly at the ground, an unlikely
event in ground-based radars.
An important reminder is the overall radar noise figure, Fn, contains all of the ohmic loss
terms of the receive path of the radar, including antenna ohmic losses. As a result, those
losses should not be included in the loss term, L, of (2.25). This is a common mistake that is
easily made by both novice and experienced radar analysts.
Combining (2.29) and (2.28) with the relation SNR = ES/EN results in (2.1), or
Neither (2.1) nor (2.31) states where the radar characterization of the SNR takes place. Such
characterization occurs at the matched filter output, as discussed in Chapter 7.
2.3 A POWER APPROACH TO SNR
This approach defines the SNR as the ratio of signal power to noise power. Recall that (2.25)
denotes the signal energy in the radar while (2.28) denotes the noise energy. We use these to
write (2.31) in a different form as
If we move τp from the numerator to denominator and define

we get
which is SNR expressed as a power ratio.
Note that we defined Beff as 1/τp. It must be emphasized that Beff may not be an actual
bandwidth anywhere in the radar. Because of the possibility of misinterpreting Beff, readers
are advised to avoid using (2.34) and use only (2.1). An exception to this recommendation
would be for the case of CW radars. In these types of radars, it would be appropriate to use
(2.34) with Beff equal to the bandwidth of the Doppler filter of the signal processor. In that
case, SNR would be the SNR at the output of the Doppler filter.
2.4 EXAMPLE 1
To illustrate the use of the radar range equation, we consider an example of a monostatic
radar with the parameters given in Table 2.1.
We wish to compute the SNR on a 6-dBsm target at a range of 60 km. To perform the
computation, we need to find the parameters of the radar range equation [(2.1) or (2.31)] and
ensure that they are in consistent units. Most of the parameters are in Table 2.1 or can be
derived from the parameters of Table 2.1. The two remaining parameters are the target range
and the target RCS, which are given above. We will need to compute the wavelength, λ, the
total losses, and the system noise temperature, Ts. Table 2.2 gives the appropriate parameters
in dB units and MKS units and show the calculation of λ and L. Ts was computed from [see
(2.28)]
and [see (2.29)]
Table 2.1
Radar Parameters
Radar Parameter
Value
Peak transmit power at power tube, PT
1 MW
Transmit losses, including feed and antenna, LtLant
2 dB
Pulsewidth, τp
0.4 µs
Antenna directivity, GT, GR
38 dB
Operating frequency, fc
8 GHz

System noise figure Fn
8 dB
Other losses, Lother
2 dB
Table 2.2
Radar Range Equation Parameters
Radar Range Equation Parameter
Value (MKS)
Value (dB)
PT
106 W
60 dBW
GT
6,309.6 W/W
38 dB
GR
6,309.6 W/W
38 dB
λ = c/fc
0.0375 m
−14.26 dB(m)
σ
3.98 m2
6 dBsm
R
60×103 m
47.78 dB(m)
k
1.38×10−23 W-s/K
−228.6 dB(W-s/K)
τp
0.4×10−6 s
−64 dB(s)
L = LtLantLother
2.51 W/W
4 dB
Ts
3,423 K
35.3 dB(K)
T′a ≈ 30 K was obtained from Figure 2.5 using θ = 5°.
Substituting the MKS values from Table 2.2 into (2.31) yields
To double check, we compute (2.31) using dB values, using
where all quantities are the dB units from Table 2.2. Substituting yields

which agrees with (2.37).
2.5 DETECTION RANGE
An important use of the radar range equation is the determination of detection range, or the
maximum range at which a target has a given probability of being detected by a radar. The
criterion for detecting a target is that the SNR be above some threshold value. If we consider
the above radar range equation, we note that SNR varies inversely with the fourth power of
range. This means that if the SNR is a certain value at a given range, it will increase as range
decreases. We therefore define the detection range as the range at which we achieve a certain
SNR. To find the detection range, we solve the radar range equation for R. Doing so by using
(2.1) as the starting point yields
Suppose, for example, we want the range at which the SNR on a 6-dBsm target is 13 dB.
Using the Table 2.2 values in (2.40) yields
This means target detection occurs at a maximum range of 66 km or at all ranges of 66 km
or less.
The value of 13 dB used in this example is a somewhat standard detection threshold. In
Chapter 6, we show that an SNR threshold of 13 dB yields a single-pulse detection probability
of 0.5 on an aircraft-type target (a Swerling 1 target).
2.6 SEARCH RADAR RANGE EQUATION
We now want to discuss an extension to the radar range equation used to analyze and design

search radars. Its most common use is in the initial sizing of search radars in terms of power
and physical size. In fact, the measure of performance usually used to characterize these types
of radars is average power-aperture product, PAAe [8, p. 311], which is the product of the
average power times the effective aperture of the radar.
We begin by assuming the radar searches an angular region, or sector, denoted Ω. The term
Ω takes units of rad2 or steradians. One of the more common search sectors is a section of the
surface of a sphere bounded by some elevation and azimuth extents; Figure 2.6 shows an
example of such a surface. The figure indicates an azimuth extent of ∆α and an elevation
extent from ε1 to ε2. As shown in the appendices, the angular area of this search sector is
where all angles are in radians.
Figure 2.6 Search sector illustration.
In Section 2.2.1.2, it was shown that the area of the beam on the surface of a sphere of
radius R could be written as
Dividing by R2 results in an angular beam area of
This gives the number of beams required to cover the search sector as
Equation (2.46) is ideal in that it essentially assumes a rectangular search sector and
rectangular beams. In practice, the number of beams required to fill a search sector is given
by

where Kpack denotes the packing factor and accounts for how the beams are arranged within
the search sector [24]. For the simple case of rectangular beams, or elliptical beams that touch
at their 3-dB points, Kpack = 1. If the radar uses anything other than rectangular packing or if
the beams touch somewhere other than their 3-dB points, Kpack will deviate from unity.
Recall that one of the parameters of PAAe is the average power, PA. If the radar has a
pulsewidth of τp and a PRI of T, the average power is
where d represents the duty cycle of the radar.
One of the requirements imposed on a search radar is that it must cover the search sector in
Tscan seconds. This means that the radar must process signals from n beams in Tscan seconds.
Given this requirement, the time allotted to each beam is
Allowing one PRI per beam gives
Equation (2.47) suggests n = KpackΩ/KAθAθB, which we can combine with (2.51) to get
We use d = τp/T to obtain
Substituting (2.52) and (2.48) into the radar range equation (2.1) produces

Finally, we arrive at the search radar range equation by using (2.9) and (2.22) in (2.53):
In arriving at (2.53), we made the assumption GT = GR. We leave the details of deriving
(2.54) as an exercise.
Note (2.54) does not explicitly depend upon operating frequency (via λ), antenna directivity,
or pulsewidth—as does the standard radar range equation. This can be of value in performing
preliminary search radar designs because we need not specify a lot of parameters.
It must be emphasized that the search radar range equation leads to a preliminary radar
design. At best, it provides a starting point for a more detailed design in which the specific
parameters not in (2.53) are defined. This will be discussed further in the following example.
2.7 EXAMPLE 25
As an interesting example, we consider a requirement placed on search radars used for
ballistic missile defense. Specifically, the Strategic Arms Limitation Talks I (SALT I) treaty
specifies that the power aperture product be limited to 3×106 W-m2[25]. Given this limitation,
we wish to perform a first-cut design of a radar to be used for ballistic missile search.
We begin by assuming the search will cover a region of space that extends from 0° to 45°
in elevation and 30° in azimuth. Further, we wish to traverse the search sector in 10 s. The
targets of interest have an RCS of -10 dBsm and we must achieve an SNR of 13 dB to declare
a detection. Current technology for this hypothetical radar supports a noise figure of 4 dB and
total losses of 6 dB. We assume an average beam (elevation) angle of 10°, which, from Figure
2.5, gives Ta′ = 15 K. This, with (2.35) and (2.36), leads to Ts = 487 K. We assume Kpack = 1 for
this preliminary design. Table 2.3 summarizes these parameters.
To determine the detection range of the radar, we first solve (2.54) for R:

Table 2.3
Search Radar Range Equation Parameters
Parameter
Value
Azimuth search extent
30°
Elevation search extent
0–45°
Power aperture product
3×106 W-m2
Search scan time, Tscan
10 s
Target RCS, σ
−10 dBsm
Detection SNR
13 dB
Total losses, L
6 dB
Packing factor, Kpack
1
System noise temperature, Ts
487 K
We compute Ω from (2.40):
which, when combined with the values in Table 2.3, yields:
which we hope will prove sufficient.
We extend the example and establish some additional characteristics for this radar. We start
by requiring the radar operate unambiguously in range. The means that we need to choose the
PRI, T, to satisfy the following equation:
We choose T = 7.5 ms.
If we devote one PRI per beam, over the course of 10 s we would need to transmit and
receive

By assuming a circular beam, we can use (2.46) to calculate the beamwidth:
If we operate the radar at a frequency of 1 GHz (L-band), we get a wavelength of
From (2.14) we have
Using (2.22), we can write
If we assume an antenna efficiency of 60%, we compute a physical area:
Finally, by assuming a circular aperture, we obtain an antenna diameter of
which is approximately the height of a seven-story building.
Our final calculation yields the peak power of the radar. We assume we want a range
resolution of 150 m, which translates to a 1-µs pulsewidth if we use an unmodulated pulse.
With the computed PRI of 7.5 ms, we can calculate the duty cycle as
From (2.63) and the given average power aperture of 3×106 W-m2, we compute an average
power of

Combining this result with (2.66) leads us to compute a peak power of
which is larger than desired.
We can reduce the peak power by using a longer pulse and pulse compression (see Chapter
10). A 100-µs pulse, with pulse compression, would reduce peak power to the more
reasonable value of 690 kW.
This completes our preliminary design for a search radar. In practice, this would serve as a
starting point for a much more detailed design where we would specifically revisit all of the
terms of the radar range equation (not the search radar range equation) with actual hardware
constraints.
2.8 RADAR RANGE EQUATION SUMMARY
Table 2.4 and Table 2.5 summarize various equations related to the radar range equation and
the search radar range equation.
Table 2.4
Radar Range Equation Summary
Equation Name
Equation
Radar range equation
Antenna directivity (GT, GR)
 or 
 where Ae = ρAant and ρ = 0.6
Effective radiated power
System noise temperature
Ts = Ta + (Fn − 1)T0
Antenna temperature
Ta = 0.8767T′a + 36 where Ta′ is from Figure 2.5
Table 2.5
Search Radar Range Equation Summary
Equation Name
Equation
Search radar range equation
Average power
ΡΑ=Ρττp/Τ, T = PRI

Effective aperture
Ae = PantAant
Scan period
Tscan: time to cover search volume
Search solid angle
Ω = 2Aα(sinε2 − sinε1)
∆α: azimuth extent of search sector
ε1: lower elevation limit of search sector
ε2: upper elevation limit of search sector
2.9 EXERCISES
1.
Derive the equation
from
In these equations, θA and θB denote beamwidths in degrees and 
 and 
 denote
beamwidths in radians. KA = 1.65.
2.
A radar has a peak power of 1 MW, combined transmit and antenna losses of 1 dB, and a
transmit antenna directivity of 41 dB. The radar is operating in free space so there is
nothing to absorb the radiated energy. It uses a pulse with an envelope width of 1 µs.
a)
Calculate the total energy on the surface of a (hypothetical) sphere with a radius of
100 km centered on the radar.
b)
Repeat part a) for a sphere with a radius of 200 km centered on the radar.
c)
Do your answers make sense? Explain.
3.
Consider a monostatic radar with the following parameters:
•
Peak transmit power at the power amp output—10 kW
•
Transmit losses—1 dB
•
Antenna losses—1 dB (transmit)
•
Antenna losses—1 dB (receive)
•
Operating frequency—6 GHz
•
PRF—1,000 Hz
•
Pulsewidth—100 µs
•
Transmit antenna effective aperture—0.58 m2
•
Receive antenna beamwidth—1.2° Az × 2.5° El (the radar has separate transmit and
receive antennas positioned next to each other)
•
Other losses—8 dB

•
System noise temperature, Ts—1,155 K
a)
Calculate the transmit antenna directivity, in dB.
b)
Calculate the effective aperture, in square meters, for the receive antenna, given an
antenna efficiency of 60%.
c)
Calculate the ERP for the radar, in dBW.
d)
Given a detection threshold of 20 dB, what is the detection range, in km, for a target
with a radar cross section of 10 dBsm?
4.
Consider a monostatic radar that has the following parameters:
• Peak transmit power at power amp output—100 kW
• Transmit and antenna losses—2 dB
• Operating frequency—10 GHz
• PRF—2,000 Hz
• Antenna diameter—1.5 m (circular aperture)
• Antenna efficiency—60%
• Other losses—12 dB
• Noise figure—4 dB
• The radar transmits a 10-µs rectangular pulse.
• The beam elevation angle is in the range of 1º to 5º.
a)
Create a table containing all parameters necessary for the radar range equation.
Derive those parameters missing explicit values above. List as TBD those
parameters with insufficient information for entering a value.
b)
Calculate the unambiguous range of the radar.
c)
Plot SNR, in dB, versus target range, in km, for a 6-dBsm target. Vary the range
from 5 km to the radar’s unambiguous range.
d)
Given a 13-dB SNR requirement for detection, calculate the detection range, in km,
for a 6-dBsm RCS target.
e)
What is the maximum detection range, in km, if the minimum SNR required for
detection is raised to 20 dB?
f)
Calculate the antenna beamwidth, in degrees.
5.
A radar generates 200 kW of peak power at the power tube and has 2 dB of loss between
the power tube and the antenna. The radar is monostatic with a single antenna that has a
directivity of 36 dB and a loss of 1 dB. The radar operates at a frequency of 5 GHz.
Determine the ERP, in dBW, for the radar. Determine the ERP in watts. Determine the
power at the receive antenna output, in dBm, for the following conditions:
a)
A 1.5-m2 RCS target at a range of 20 km
b)
A 20-dBsm target at a range of 100 km
6.
How does doubling the range change the powers in Exercise 5? Give your answer in dB.
This problem illustrates an important rule of thumb for the radar range equation.
7.
A radar with losses of 13 dB and a noise figure of 8 dB must detect targets within a
search sector 360° in azimuth and from 0° to 20° in elevation. The radar must cover the

search sector in 6 s. The targets of interest have an RCS of 6 dBsm, and the radar
requires 20 dB of SNR to declare a detection. The radar must have a detection range of
75 km. Calculate the average power aperture (PavgAe), in W-m2, required by the radar to
satisfy the search requirements above.
8.
The radar of Exercise 7 uses an antenna with fan beamwidths of 1° in azimuth and 5° in
elevation. The radar operates at a frequency of 4 GHz. What average power, in kW, must
the radar have? Given an antenna efficiency of 60%, calculate the approximate antenna
dimensions, in m. Hint: The relative height and width of the antenna are inversely
proportional to the relative beamwidths.
9.
Assuming the radar of Exercise 7 uses one PRI per beam, determine the PRI for the
radar. Can the radar operate unambiguously in range? Explain.
10.
We typically describe the range resolution of a radar as the width of its pulses, if the
radar uses unmodulated pulses. What pulsewidth does the radar of Exercise 7 require for
a range resolution of 150 m? What is the peak power of the radar, in MKS units?
11.
Derive (2.54).
References
[1]
Barton, D. K., Radar Equations for Modern Radar, Norwood, MA: Artech House, 2013.
[2]
Norton, K. A., and A. C. Omberg, “The Maximum Range of a Radar Set,” Proc. IRE, vol. 35, no. 1, Jan. 1947, pp. 4–
24. First published Feb. 1943 by U.S. Army, Office of Chief Signal Officer in the War Department, in Operational
Research Group Report, ORG-P-9-1.
[3]
Barton, D. K., ed., Radars, Vol. 2: The Radar Range Equation (Artech Radar Library), Dedham, MA: Artech House,
1974.
[4]
Budge, M. C., Jr., “EE 619: Intro to Radar Systems.” www.ece.uah.edu/courses/material/EE619/index.htm.
[5]
Erst, S. J., Receiving Systems Design, Dedham, MA: Artech House, 1984, p. 46.
[6]
Skolnik, M. I., ed., Radar Handbook, New York: McGraw-Hill, 1970, p. 9–5.
[7]
Skolnik, M. I., Introduction to Radar Systems, 3rd ed., New York: McGraw-Hill, 2001.
[8]
Hovanessian, S. A., Radar System Design and Analysis, Norwood, MA: Artech House, 1984
[9]
Stutzman, W. L., “Estimating Directivity and Gain of Antennas,” IEEE Antennas Propagat. Mag., vol. 40, no. 4, Aug.
1998, pp. 7–11.
[10]
Barton, D. K., Radar System Analysis and Modeling, Norwood, MA: Artech House, 2005.
[11]
Rihaczek, A. W., Principles of High-Resolution Radar, New York: McGraw-Hill, 1969, p. 64. Reprinted: Norwood,
MA: Artech House, 1995, p. 64.
[12]
Stutzman, W. L., and G. A. Thiele, Antenna Theory and Design, New York: Wiley & Sons, 1981.
[13]
Ziemer, R. E., and W. H. Tranter, Principles of Communications, 3rd ed., Boston, MA: Houghton Mifflin, 1990.
[14]
Johnson, J. B., “Thermal Agitation of Electricity in Conductors,” Phys. Rev., vol. 32, Jul. 1928, pp. 97–109.
[15]
Nyquist, H., “Thermal Agitation of Electric Charge in Conductors,” Phys. Rev., vol. 32, Jul. 1928, pp. 110–113.
[16]
Rohde, U. L., J. Whitaker, and T. T. N. Bucher, Communications Receivers, 2nd ed., New York: McGraw-Hill, 1997.
[17]
Losee, F. A., RF Systems, Components, and Circuits Handbook, 2nd ed., Norwood, MA: Artech House, 2005.
[18]
IEEE 100, The Authoritative Dictionary of IEEE Standards Terms, 7th ed., New York: IEEE, 2000.
[19]
Ridenour, L. N., Radar System Engineering, vol. 1 of MIT Radiation Lab. Series, New York: McGraw-Hill, 1947.
Reprinted: Norwood, MA: Artech House (CD-ROM edition), 1999, p. 33.
[20]
Uhlenbeck, G. E., and J. L. Lawson, Threshold Signals, vol. 24 of MIT Radiation Lab. Series, New York: McGraw-
Hill, 1950. Reprinted: Norwood, MA: Artech House (CD-ROM edition), 1999, p. 99.
[21]
Lathi, B. P., Signals, Systems and Communication, New York: Wiley & Sons, 1965, pp. 548–551.
[22]
Van Voorhis, S. N., Microwave Receivers, vol. 23 of MIT Radiation Lab. Series, New York: McGraw-Hill, 1948.
Reprinted: Norwood, MA: Artech House (CD-ROM edition), 1999, p. 4.
[23]
Blake, L. V., “A Guide to Basic Pulse-Radar Maximum-Range Calculation, Part 1—Equations, Definitions, and Aids to

Calculation,” Naval Research Laboratory, Washington, D.C., Rep. No. 6930, Dec. 23, 1969, p. 49. Available from
DTIC as 701321.
[24]
Curry, G. R., Radar System Performance Modeling, Norwood, MA: Artech House, 2001, pp. 117– 118.
[25]
Mahan, E.R., and E. C. Keefer, eds., Foreign Relations of the United States, 1969–1976, vol. 32, SALT I, 1969–1976,
Washington, D.C.: U.S. Government Printing Office, 2010, p. 814.
APPENDIX 2A: DERIVATION OF SEARCH SOLID ANGLE EQUATION
Figure 2A.1 Geometry for computing solid angle.
We can write the area of the small square in Figure 2A.1 as
or
To get the total area over the angles [ε1, ε2], we integrate (2A.1) and (2A.2) over these angle
ranges. This yields
Performing the integral results in
Dividing by R2 yields the solid angle as

1 An assumption of this form of the radar range equation is that the radar is pulsed, not CW. For CW radars, it would be more
appropriate to use the form of Section 2.3.
2 Noise figure and noise factor are often treated as synonyms, although some authors make a distinction [6]. Specifically, the
term “noise figure” is used when in logarithmic form, while noise factor is used when in linear form. We will use noise figure for
both the W/W and dB version in this book.
3 We assume nothing in the transmit, propagation, or receive path of the radar, up to the matched filter, distorts the rectangular
pulse envelope. Clearly, this will not be the case, since a rectangular pulse has infinite bandwidth and the transmitter,
environment, and receiver have finite bandwidth. As discussed in Chapters 5 and 7, we accommodate envelope distortion by
including a loss factor. As a note, in practical radars, the loss due to pulse envelope distortion is usually small (< 1 dB).
4 The concept of 3-dB points should be familiar from control and signal processing theory as the standard measure used to
characterize bandwidth.
5 This example is adapted from lecture notes by Dr. Stephen Gilbert.

Chapter 3
Radar Cross Section
3.1 INTRODUCTION
In this chapter, we discuss radar cross section, or RCS. The concept of scattering of
electromagnetic waves by objects (what the concept of RCS attempts to quantify) dates back to
1861, when Alfred Clebsch discussed the topic in a memoir [1]. In 1871, Lord Rayleigh,1
whose name is often associated with electromagnetic scattering and RCS, published a paper
titled “On the Incidence of Aerial and Electric Waves Upon Small Obstacles” [2].
Although many authors wrote about electromagnetic scattering in the late 1800s and early
1900s, the first mention of RCS did not occur until 1947, when Ridenour introduced it in an
article in the MIT Radiation Laboratory Series [3, p. 21]. In that reference, Ridenour provided
a definition of RCS as
The units of the numerator of (3.1) are watts while the units of the denominator are watts/m2.
Thus, the unit of RCS is m2.
Two of the key phrases in the definition of (3.1) are “reradiated” and “toward the source.”
This says the RCS parameter attempts to capture, in a single number, the ability of the target to
capture energy from the radar and reradiate it back toward the radar.
In general, computation of RCS is very complicated. In fact, except for some very simple
surfaces, RCS can be only approximately computed. This may explain why there is a large
amount of current research in methods to more reliably predict the RCS characteristics of
practical targets [4–10].
3.2 RCS OF SIMPLE SHAPES
In general, the RCS of a target depends upon its physical size. However, this is not always the
case. An example of the case where RCS depends upon physical size is a sphere. Specifically,
the RCS of a perfectly conducting sphere of radius r is
provided r ≫ λ [1, p. 65].
A case where RCS does not depend upon physical size is a cone where the nose of the cone

is facing toward the radar, as shown in Figure 3.1. For the case of Figure 3.1, the RCS is given
by [11, p. 89]
It will be noted that the RCS is proportional to wavelength but is not dependent on the overall
size of the cone. If the cone had any other orientation relative to the line of sight (LOS) to the
radar (see Figure 3.1 for a definition of LOS), its RCS would depend upon the length of the
cone and the diameter of the base [12]. Also, if the point of the cone is not perfectly sharp, the
RCS will depend upon the radius of the nose (see Figure 3.2) [13–15].
Figure 3.1 Cone geometry.
Figure 3.2 Ideal reentry vehicle RCS (blunted cone)—nose-on aspect.
In this case, the RCS [14] is the sum of the expressions for a cone [16] and the rounded tip
[17]. That is, σ = σcone + σtip where

k = 2π/λ, θ is the cone half angle, n = 3/2 + α/π, a is the cone base radius, and b is the sphere
radius.
In most cases, the RCS is dependent on both the size of the object and the radar wavelength.
It also depends on what the object is made of, as metal objects generally have a larger RCS
than nonmetallic objects of the same size.
Examples of other simple shapes and their RCSs are contained in Figure 3.3 [11, 12, 18].2
For the case of the chaff dipole, the given equation for RCS as a function of only wavelength
results from the assumption that the length of the chaff dipole is equal to the wavelength and
that the dipole is oriented normal to the LOS. If one were to consider all orientations of a
chaff dipole, the average RCS would be σavg = 0.153λ2, σavg = 0.166λ2, and σavg = 0.184λ2 for
half-wave, full-wave, and 1.5-wave dipoles, respectively [19].
Figure 3.3 RCSs of some simple shapes.

Figure 3.4 Normalized RCS of a perfectly conducting sphere vs. normalized size.
A classical plot in RCS theory is shown in Figure 3.4 [3, p. 65]. This figure contains a plot
of normalized RCS versus normalized radius for a perfectly conducting sphere. It provides an
illustration that the RCS of an object is generally a complicated function of both the size of
the object and the wavelength of the electromagnetic wave that impinges on the object.
The equation for the curve of Figure 3.4 is [20, pp. 35–36]
where
r is the radius of the sphere.
k = 2π/λ is the wave number.
Jn(kr) is the spherical Bessel function of the first kind of order n and argument kr.
Yn(kr) is the spherical Bessel function of the second kind of order n and argument kr
(also called Weber’s function).
 is the spherical Bessel function of the third kind of order

n and argument kr (also called a Hankel function).
Equation (3.6) is usually referred to as the Mie3 series and is one of the few tractable cross
section equations [21].
If the object size is less than a wavelength, we say that the object is in the Rayleigh region
of the incident electromagnetic wave. In this region, the RCS of the object is a function of the
size of the object relative to a wavelength. As an example, the RCS of a perfectly conducting
sphere whose radius places it in the Rayleigh region (see Figure 3.4) is given by [22, p. 101]
The most common example objects that are in the Rayleigh region for many radars are rain
and clouds [23, p. 149; 24, p. 41]. Another example would be insects.
The center region of Figure 3.4 is termed the resonance, or Mie, region. The Mie
designation is in honor of Gustav Mie, who first gave the exact equation for the curve of
Figure 3.4 [20]. (This equation was later detailed by Stratton [25].) In this region, the object
size is on the order of a wavelength and the RCS is transitioning from being dependent upon
both object size and wavelength to being dependent mainly on object size. As indicated in this
figure, the RCS of a sphere can appear to be larger than dictated by its size. Typical objects
that could be in the resonance region would be birds, bullets, artillery shells, some missiles,
and very small aircraft, depending upon frequency.
The third RCS region is termed the “optical region” and is where most large objects fall. In
this region, the object is much larger than a wavelength. Further, the RCS is (or can be) a
strong function of the size of the object.
In general, the RCS of an object depends upon the orientation of the object relative to the
LOS. As an example, the RCS of the flat plate illustrated in Figure 3.5 is given by [22, p. 105;
26, p. 457]
where sinc(x) = sin(πx)/(πx).

Figure 3.5 RCS of a 1-m2 flat plate at a frequency of 1 GHz.
The plot of Figure 3.5 was created for a flat plate with d = w = 1 m and ϕ = 0 and λ = 0.3 m
(L-band). As can be seen, the RCS varies significantly as the angle of the LOS changes. It will
also be noted that the peak RCS is significantly larger than the 1-m2 area of the plate.
Most targets of interest are not the simple shapes indicated thus far. In fact, targets such as
airplanes consist of many different shapes that are in different orientations. Further, as the
targets move relative to the radar LOS, the relative orientations of the various shapes change
significantly. As a result, a typical plot of target RCS versus orientation relative to the LOS
has a very complex appearance. A classical plot that illustrates this variation of RCS is in
Figure 3.6 [3, p. 77]. This figure shows the measured variation in RCS of an AT-11 Kansan [a
twin engine aircraft used during World War II for bombing and gunnery training by the
United States Army Air Forces (USAAF)] as a function of azimuth orientation relative to the
LOS. As can be seen, the RCS varies by quite a large amount and in a random-looking
fashion. If one considers that the orientation of the aircraft will change continually as the
aircraft flies toward the radar, the angular variation in RCS will translate to a time variation
of RCS that would look random.

Figure 3.6 Experimental RCS of an AT-11 Kansan. (Source: L. N. Ridenour, Radar System Engineering, Vol. 1 of MIT
Radiation Laboratory Series, 1947. Reprinted with permission.)
3.3 SWERLING RCS MODELS
In an attempt to capture target RCS fluctuation effects in a mathematical model that could be
easily used in detection studies, Peter Swerling [27] developed statistical representations of
RCS, which are commonly referred to as the Swerling RCS models. There are four Swerling
models, termed Swerling 1, Swerling 2, Swerling 3, and Swerling 4. Many radar analysts
refer to a fifth Swerling model that is termed Swerling 0 or Swerling 5. The fifth Swerling
model is defined as a target that has a constant RCS.4 This Swerling model would be
representative of a sphere since the ideal RCS of a sphere is a constant function of orientation
angle.

The four Swerling models attempt to represent both statistical and temporal variations of
RCS. The statistical properties of Swerling 1 and Swerling 2 RCS variations (which we will
refer to Swerling 1 or Swerling 2 targets, or SW1 and SW2 targets) are the same and are
governed by the density function
where U(σ) is the unit step function. Equation (3.9) is the equation for an exponential density
function. σAV is the average RCS of the target and is the value that would be used in the radar
range equation.
The statistical properties of SW3 and SW4 targets are also the same and are governed by
the density function
Again, σAV is the average RCS of the target.
Equations (3.9) and (3.10) are special cases of the chi-squared density function [3]. Equation
(3.9) is a chi-squared density function with two degrees of freedom, and (3.10) is a chi-
squared density function with four degrees of freedom. The general, k-degree-of-freedom,
chi-squared density function is the density function of the sum of the squares of k,
independent, zero-mean, equal variance Gaussian random variables.
The difference between a SW1 and SW2 target lies in the difference in the time variation of
RCS. The same is true for a SW3 and SW4 target. With a SW1 or SW3 target, the RCS
fluctuates slowly over time and with a SW2 or SW4 target, the RCS fluctuates rapidly over
time. In the classical definitions given by Swerling [27], SW1 and SW3 targets maintain a
constant RCS during the time the radar illuminates it on a particular scan, but its RCS changes
independently (in a random fashion) on a scan-to-scan basis. For SW2 and SW4 targets, the
RCS changes independently (and randomly) on a pulse-to-pulse basis.
Scan-to-scan means that the radar “looks” at the target infrequently—on the order of once
every several seconds. Pulse-to-pulse means that the radar “looks” at the target every PRI. The
phrase scan-to-scan derives from search radar terminology where the radar constantly rotates
and “scans” by the target only every few seconds.5
3.3.1 Swerling Statistics
Plots of the density functions of (3.9) and (3.10) are shown in Figure 3.7. These plots indicate
that the RCS values for SW1 and SW2 targets vary about a value below σAV, whereas the RCS
values for SW3 and SW4 targets are concentrated at values fairly close to the average RCS.
This is further illustrated in Figure 3.8, which contains plots of RCS versus dimensionless
time for SW1/SW2 and SW3/SW4 targets with an average RCS of 1 m2 or 0 dBsm. As will be

noted, the RCS values for the SW1/SW2 case tend to vary significantly about a value below
the average RCS of 0 dBsm, whereas the RCS values for the SW3/SW4 case tend to cluster
more tightly around a value slightly below the average RCS of 0 dBsm.
Figure 3.7 Density functions for Swerling RCS models.
Figure 3.8 RCS vs. time for SW1/SW2 and SW3/SW4 targets.
3.3.2 Swerling Fluctuation Models
As was indicated earlier, the difference between SWodd (SW1, SW3) targets and SWeven
(SW2, SW4) targets lies in the rate at which the RCS is assumed to vary. It was stated that the
SWodd model assumes that the RCS changes on a scan-to-scan basis. In the search radar range

equation discussions of Chapter 2, we referred to a search volume and indicated that the
search radar covers the search volume within a certain time we termed Tscan. This process of
covering the search volume is termed a scan, and Tscan is termed the scan time. If we were
using a SWodd target model in the search radar analysis, we would assume the RCS changed
from scan to scan but stayed constant during the scan. Thus, we would assume that the RCS
changed every Tscan seconds, but stayed constant over any specific Tscan interval. If we were
using a SWeven target model, we would assume the target RCS changed every PRI, or every T
seconds. Thus, the SWeven RCS models imply rapid RCS fluctuation, whereas the SWodd
models imply slow RCS fluctuation. This difference in RCS fluctuation is illustrated
notionally in Figure 3.9, which is a plot of RCS versus pulse number, or PRI, for the two
cases. For the SW1 model, the RCS changes every 500 pulses and the RCS changes every
pulse for the SW2 model.
The concept of SWodd and SWeven represents an idealization that is not achieved in
practice. Actual targets exhibit RCS variations that lie somewhere between SWodd and
SWeven. How close the fluctuation lies to either model depends upon the complexity of the
target, the operating frequency of the radar, and the time between RCS observations. As an
example, we consider a target that we can model by five spheres, or point sources. In this
example, we “fly” the target model toward the radar with a constant x velocity of 75 m/s and a
y and z velocity of zero. The center starts at x = 20,100 m, y = 5,000 m, and z = 0 m. We
assume all of the scatterers have the same RCS (1 m2) and compute the composite RCS (the
total RCS of all five scatterers) as a function of time.
Figure 3.10 contains plots of composite RCS over a 3-second interval for cases where the
carrier frequency is 8.136 GHz (low X-band) and 97.632 GHz (W-band).
For the X-band case, the RCS remains fairly constant for time periods of tens of
milliseconds. However, over periods of seconds, the RCS variation becomes unpredictable
(i.e., random). Thus, at X-band, this target exhibits an RCS behavior that is consistent with a
SWodd target.
For the W-band case, the RCS variation (with time) is much more rapid so that the RCS
varies significantly over time intervals of tens of milliseconds. In this case, it might be
appropriate to represent the target with a SWeven model.

Figure 3.9 SW1 and SW2 RCS fluctuation models.
Figure 3.10 Sample RCS variation.
3.3.3 Math Behind the Fluctuation Model
To understand the above relation between RCS variation rate and operating frequency, we
need to consider how the signals from the scatterers combine to form the composite signal in

the radar. We start by considering the unmodulated pulse we discussed earlier. For this case,
we can write the voltage pulse at the transmitter output as
This voltage is converted to an electric field by the antenna and propagates to the target,
which creates another electric field. The electric field created by the target propagates back to
the radar, where the antenna converts it to a voltage. If the target is a sphere (a point scatterer),
the voltage at the antenna output or some point in the receiver (before the matched filter) can
be written as6
In (3.11) and (3.12)
and
where PT is the transmit power and PS is the received signal power. The notation ∝ means
“proportional to.”
If we have N point targets clustered close together, their electric fields, at the radar, will
add. Because of this, the total voltage in the radar receiver is
where
In (3.16), σk is the RCS of the kth scatterer (sphere, target) and Rk is the range to that scatterer.

If we assume the scatterers are close together so that the various Rk are close to the average
range to the cluster, R, we can write
where
With some manipulation, (3.17) becomes [30]
Finally, if we define σ as the net RCS of the N scatterers, we can write
where
and
With some thought, it will be observed that σ is a strong function of Rk. Indeed, variations
in Rk of λ/2 can cause the phase of the voltage from the kth scatterer to vary by 2π. Thus, it
does not take much relative movement of the scatterers to dramatically affect the value of the
sum in (3.21). Also, as the carrier frequency increases, λ decreases and smaller changes in the
relative positions of the scatterers can have larger effects on the variations of σ, the total RCS.
The above is what led to the difference in RCS variation demonstrated in Figure 3.10. In
both cases (top plot and bottom plot), the changes in the relative positions of the scatterers is
the same over the three-second period considered. However, at the lower carrier frequency

(top plot), the relative positions change by less than a wavelength over the three-second
period. On the other hand, for the higher frequency, the relative positions change by several
wavelengths.
In addition to the changes in net RCS, the variation in the phase of the return signal, as
given by (3.22), will exhibit similar differences in temporal behavior. This is illustrated in
Figure 3.11. As can be seen, the phase variations are more rapid for the higher carrier
frequency than for the lower carrier frequency. We will make use of this property when we
discuss how to simulate the various types of Swerling targets.
Figure 3.11 Sample signal phase variation.
3.4 RELATION OF SWERLING MODELS TO ACTUAL TARGETS
Our discussions of the Swerling RCS models have thus far been theoretical. To be of use in
practical radar problems, we need to attempt to relate the various models to actual targets.
One of the standard assumptions is that the SW1/SW2 RCS fluctuation model is associated
with complex targets such as aircraft, tanks, ships, and cruise missiles. These would be targets
that have a large number of surfaces and joints, all with different orientations. In practice,
detection measurements indicate that, indeed, the SW1/SW2 model provides a reasonably
good representation of complex targets [31]. Interestingly, in his paper, Swerling has an
underlined statement that states, “Most available observational data on aircraft targets
indicates agreement with the exponential density…” [27]. His phrase “exponential density” is
referring to an equation of the same form as (3.9).
The standard assumption concerning the SW3/SW4 fluctuation model is that it applies to
somewhat simple targets such as bullets, artillery shells, and reentry vehicles and the like.

According Swerling, the SW3/SW4 model is consistent with a target that consists of a
predominant scatterer and several smaller scatterers, or one large scatterer with small
changes in orientation [27]. In terms of application to practical targets, Swerling goes on to
say, “More definite statements as to actual targets for which [the SW3/SW4] or the
nonfluctuating [SW0/SW5] model apply must await further experimental data.”
3.4.1 Simulating Swerling Targets
Analysts and radar testers often have a need to simulate the returns from fluctuating targets.
This might occur in simulation when attempting to reconcile the detection performance of
radar simulations with predictions based on theory. It can also occur when evaluating the
impact of target RCS fluctuations on target acquisition and tracking. In tower testing of actual
radars (testing with signals generated from a test tower on a test range or through RF or IF
injection in a laboratory environment), the use of fluctuating target returns provides more
realistic estimates of detection performance than does the use of constant amplitude target
returns.
Because of this perceived need, we present methods of simulating target returns with
Swerling-like fluctuation characteristics. The methods make use of the fact that Swerling
fluctuation statistics are governed by chi-squared probability density functions. As indicated
earlier, the RCS (probability) density functions for SW1 and SW2 targets is a chi-squared
density with two degrees of freedom. This means the density results from summing the square
of two independent, zero-mean, equal variance, Gaussian random variables. In equation form,
if x1 and x2 are random variables with the properties just described, then the random variable
will be governed by a chi-squared, two-degree-of-freedom density function. This further tells
us that, if we want to generate random numbers that have statistics consistent with the
SW1/SW2 RCS model, we can obtain them by generating two independent, zero-mean, equal
variance, Gaussian random numbers, squaring them and taking the average of the squares.
The variance of the random numbers should be equal to the average RCS of the target, σAV.
The resulting random variable will be governed by the density function of (3.9).
To simulate a SW2 target, we would create a new random number on every return pulse.
This stems from the fact that SW2 RCS values are, by definition, independent from pulse to
pulse.
To simulate a SW1 target, we would generate a random number once every group of N
pulses and maintain that as the RCS over the N pulses. Here N would be the number of pulses
processed by the coherent and/or noncoherent processor (see Chapter 8). The idea of
maintaining the RCS constant over the N pulses stems from the definition of SW1 RCS
fluctuations, which states that the RCS remains constant during the time the radar beam scans
by the target on a particular scan, but changes randomly from scan to scan.

As a note, the phase of the SW2 target also varies randomly from pulse to pulse and the
phase of the SW1 target remains constant over the N pulses, but varies randomly from one
group of N pulses to the next. We can achieve this phase behavior by defining the phase as
where the tan−1 is the four-quadrant arctangent. An alternate way of thinking about the above
is to treat x1 and x2 as the real and imaginary parts of a complex number and defining the RCS
as one-half times the magnitude squared and phase of the complex number, respectively.
While the above method of generating SW1 RCS fluctuations is accurate in terms of the
SW1 fluctuation model, it can be cumbersome from an implementation perspective and is not
representative of the fluctuation of RCS for actual targets. As illustrated in Figures 3.10 and
3.11, RCS tends to fluctuate continuously over time at rates that depend upon carrier
frequency.
A method of achieving such a temporal characteristic and maintaining the SW1 statistics is
to filter the Gaussian random numbers before squaring and adding them. Filtering the random
numbers correlates them but does not change their Gaussian statistics.7 Thus, when the
random numbers at the output of the filter are squared and added, the result will be a set of
correlated, chi-squared, two degree-of-freedom, random variables that change fairly slowly
over time.
A block diagram of the proposed method for generating SW1-like RCS values is shown in
Figure 3.12. Sequences of independent, zero-mean, unit variance, Gaussian random numbers
are generated and combined into a sequence of complex random numbers. The complex
sequence is then filtered by a lowpass filter (LPF). The output of the LPF is then scaled so that
the variance of the real and imaginary parts is equal to σAV. After scaling, the square of the
magnitude is computed and divided by two [in compliance with (3.23)] to obtain the RCS. The
angle of the complex number is formed to obtain the phase of the voltage that would result
when the RCS is used to generate the complex return signal from the target.
In computer simulations, we prefer implementing the filter as an ideal “brick wall” LPF
using the FFT.8 We prefer the FFT approach over a recursive filter approach because of the
need to consider filter transients in the latter. We use the brick wall LFP because it is easy to
implement. The length of the FFT is determined by the number of RCS samples needed in one
execution of the simulation.
To set the filter bandwidth, we need the time between RCS samples. We normally choose
this as the radar PRI for testing detection. For tracking studies we use the track update period
or the PRI, depending upon whether or not we are modeling the signal processor.
As indicated by Figures 3.10 and 3.11, the bandwidth of the filter should be based on the
operating frequency of the radar. If we assume the behavior in Figures 3.10 and 3.11 is
representative, we would choose a bandwidth of about 0.5 Hz for radars operating in the S- to
X-band and scale the bandwidth according to frequency from there.

In testing applications, it would be better to use recursive digital filters to generate the RCS
values because the signals must persist over long time periods.
Figure 3.12 Block diagram of SW1 RCS generation algorithm.
Figures 3.13 and 3.14 contain plots that were generated by this technique. The filter
bandwidth was set to 0.5 Hz for the top plot of the figures and 5 Hz for the bottom plot. As can
be seen, the behavior is similar to the five-scatterer example of Figures 3.10 and 3.11.
Figure 3.13 RCS vs. time for SW1 RCS model.

Figure 3.14 Phase vs. time for SW1 RCS model.
The RCS generation technique for SW3 and SW4 targets is similar to the method used for
SW1 and SW2 targets except that the RCS is based on the sum of four terms instead of two.
This is because SW3 and SW4 RCS fluctuations are governed by a chi-squared density with
four degrees of freedom. In equation form,
To simulate a SW4 target, we would create a new random number on every return pulse. To
simulate a SW3 target, we would generate a random number once every group of N pulses
and maintain that as the RCS over the N pulses.
It is not clear how the phase should be modeled for this case. One approach would be to use
(3.24). An alternative might be to use
That is, average the phase from two complex numbers represented by xa = x1 + jx2 and xb = x3
+ jx4.
An alternative for the SW3 case would be to use an extension of the filter method suggested
for SW1 targets. A block diagram of this method is shown in Figure 3.15. As can be seen, the
method uses two of the SW1 filters and then averages the outputs of the magnitude square and
angle computation blocks.

Figure 3.15 Block diagram of SW3 RCS generation algorithm.
Figures 3.16 and 3.17 contain plots of RCS and phase generated by the model of Figure
3.15. It is interesting that the RCS variations of Figure 3.16 appear to be smaller than those of
Figure 3.13 and tend to be closer to the average RCS of 5 m2. This is consistent with the
expected difference in RCS behavior between SW1 and SW3 targets.
Figure 3.16 RCS vs. time for SW3 RCS model.

Figure 3.17 Phase vs. time for SW3 RCS model.
3.5 FREQUENCY AGILITY AND SW2 OR SW4 TARGETS
In Chapter 8, we show that if targets exhibit SW2 or SW4 fluctuation statistics, noncoherent
integration can provide a significant increase in detection probability relative to that which
can be obtained with a single pulse. As an example, the single-pulse SNR required to provide
a detection probability of 0.9 with a false alarm probability of 10−6 on a SW2 target is 21 dB.
If the radar noncoherently integrates 10 pulses, the single-pulse SNR required to achieve the
same detection and false alarm probabilities is reduced by 14 dB to 7 dB. This leads us to
consider whether there is anything that can be done in the radar to change the fluctuation
statistics from SW1/SW3 to SW2/SW4. One way is to change the operating frequency on a
pulse-to-pulse basis. The question is: how large of a frequency change is needed?
To be rigorously applicable to target detection theory, a statistical approach is used here to
address this problem. The results derived here indicate that for a target with a length L, the
frequency separation required for target returns at the two frequencies to be statistically
uncorrelated is given by
where c is the speed of light.
As a note, the fact that the target returns are statistically uncorrelated does not imply that
they are statistically independent. However, this is the standard assumption.

We assume the target consists of N scatterers distributed across range in some fashion. In
particular, we assume the ranges to the scatterers are random and that all of the ranges are
governed by the same density function. We assume each scatterer has a different RCS and that
the RCSs are random, mutually independent, and independent of the ranges.
Let the target be illuminated by a pulse whose compressed pulsewidth is larger than the
target extent, L, in range. With this, we can write the peak of the complex voltage at the output
of the matched filter as
where fc is the carrier frequency, Vk is the complex voltage associated with the kth scatterer,
and Rk is the (slant) range to the kth scatterer. The magnitude of Vk is related to the RCS
through the radar range equation, and it is assumed that the phase of Vk is a random variable
uniformly distributed over 2π. The phases are assumed to be mutually independent and
independent of the ranges and RCSs. With this, the Vk are independent, complex, random
variables. The Vk are also independent of the Rk.
We will be interested in deriving the correlation coefficient between v(fc) and v(fc + Δf). We
assume that Δf is small relative to fc (tens of MHz versus GHz). We use this assumption so we
can further assume Vk is the same at both frequencies. We write v(fc + Δf) as
The correlation coefficient between v(fc) and v(fc + Δf) can be written as
where C(Δf) is the covariance between v(fc) and v(fc + Δf), σ2(fc) is the variance on v(fc), and
σ2(fc + Δf) is the variance on v(fc + Δf).
To ease the computation of the three elements of r(Δf), we show that the means of v(fc) and
v(fc + Δf) are zero. We write
by virtue of the fact that the phase of Vk is uniform on [0,2π], E{Vk} = 0, and thus E{v(f)} = 0.

Because of (3.31) we can write
Recognizing that ej4πf(Rl − Rk)/c = 1 when l = k, (3.32) can be rewritten as
where Pk is the average power due to scatterer k, which is derived from the radar range
equation using the average RCS of scatterer k. The double sum of (3.33) is zero by virtue of
the fact that
since Vk and Vl are independent for l ≠ k and E{Vk} = E{V*l} = 0.
We can write the covariance as
From above, we recognize that E{VkV*l} = 0 for l ≠ k, E{VkV*l} = Pk for l = k and 
 for l = k. With this, (3.35) reduces to
Or, recognizing that 
 is the same for all k,
Substituting (3.37) and (3.33) into (3.30) yields

3.5.1 Special Cases
For the case where the target scatterers are uniformly distributed over some R0±L/2, we can
compute a specific function for r(Δf). Specifically, with
where
we get
If we say that v(fc) and v(fc + Δf) become uncorrelated for all Δf greater than the Δf where the
sinc function first goes to zero, then v(fc) and v(fc + Δf) become uncorrelated for
This is the same as (3.27).
As another example, we consider the case where the Rk obey a Gaussian distribution. That
is,
In this case, r(Δf) becomes
If we let σL = L/2, 95.5 percent of the scatterers will lie between ±L/2. At Δf = c/2L, r(Δf) =
0.007. Thus, we can say that the returns derived from carrier frequencies separated by this Δf
are uncorrelated.
The results presented herein indicate that Δf does not need to be large to cause

decorrelation of RCS from pulse to pulse. For example, a target with a range extent of 15 m
requires a Δf of only 10 MHz from pulse to pulse. For larger aircraft such as a Boeing 747,
which is about 71 m long, only a 2.1-MHz frequency change from pulse to pulse is needed.
These examples used the assumption that the scatterers were distributed across the length of
the target. In practice, it is likely that this will not be the case. Instead, it is likely that the
scatterers will be grouped along different parts of the target (e.g., near the nose, near the
wings, and near the tail for aircraft). Because of this, the lengths of the groups of scatterers
will be smaller than the length of the aircraft. This means that the frequency changes indicated
above are most likely low. More reasonable values may be in the range of tens of megahertz.
3.6 EXERCISES
1.
A classical example in RCS discussions is termed the two-scatterer problem. In this
exercise, we seek to find the composite RCS of two equal-size scatterers separated by a
distance of 2d. The geometry for this exercise is shown in Figure 3.18.
Show that the composite RCS is given by
where σ0 is the RCS of each scatterer. Generate plots of σ versus θ for d/λ = 1 and d/λ = 3
with σ0 = 1 m2. These plots will demonstrate that the degree to which σ varies as a
function of θ depends upon the separation of the scatterers relative to the radar
wavelength.
2.
Generate plots like Figure 3.11 for the case of 10 scatterers randomly located in a square
with x and y dimensions of 10 m. Assume all of the scatterers have and equal RCS of 1
m2.
3.
Implement a SW1 model as discussed in Section 3.5 and generate curves like Figures
3.13 and 3.14.
4.
Repeat Exercise 3 for a SW3 target and generate curves like Figures 3.16 and 3.17.

Figure 3.18 Two-scatterer RCS problem.
References
[1]
Logan, N. A., “Survey of Some Early Studies of the Scattering of Plane Waves by a Sphere,” Proc. IEEE, vol. 53, no.
8, 1965, pp. 773–785.
[2]
Strutt, J. W. (3rd Baron Rayleigh), “On the Incidence of Aerial and Electric Waves Upon Small Obstacles in the Form
of Ellipsoids or Elliptic Cylinders and on the Passage of Electric Waves Through a Circular Aperture in a Conducting
Screen,” Philosophical Mag., vol. 44, Jul. 1897, pp. 28–52.
[3]
Ridenour, L. N., Radar System Engineering, vol. 1 of MIT Radiation Lab. Series, New York: McGraw-Hill, 1947;
Norwood, MA: Artech House (CD-ROM edition), 1999.
[4]
Jernejcic, R. O., A. J. Terzuoli, Jr., and R. F. Schindel, “Electromagnetic Backscatter Predictions Using XPATCH,” in
IEEE/APS Int. Symp. Dig., vol. 1, Seattle, WA, Jun. 20–24, 1994, pp. 602– 605.
[5]
Song, J. M., et al., “Fast Illinois Solver Code (FISC),” IEEE Antennas Propagat. Mag., vol. 40, no. 3, Jun. 1998, pp.
27–34.
[6]
Hastriter, M. L., and W. C. Chew, “Comparing Xpatch, FISC, and ScaleME Using a Cone-Cylinder,” IEEE Antennas
Propag. Soc. Int. Symp., vol. 2, Monterey, CA, Jun. 20–25, 2004, pp. 2007–2010.
[7]
Emerson, W. H., and H. B. Sefton, Jr., “An Improved Design for Indoor Ranges,” Proc. IEEE, vol. 53, no. 8, Aug.
1965, pp. 1079–1081.
[8]
Shields, M., “The Compact RCS/Antenna Range at MIT Lincoln Laboratory,” 3rd European Conf. Antennas Propag.,
2009 (EuCAP 2009), Berlin, Mar. 23–27, 2009, pp. 939–943.
[9]
Zhang, L. et al., “High-Resolution RCS Measurement Inside an Anechoic Chamber,” Int. Forum Inf. Technol. Applicat.
(IFITA) 2010, vol. 3, Washington, D.C., Jul. 16–18, 2010, pp. 252–255.
[10]
Sevgi, L., Z. Rafiq, and I. Majid, “Testing Ourselves: Radar Cross Section (RCS) Measurements,” IEEE Antennas
Propagat. Mag., vol. 55, no. 6, Dec. 2013, pp. 277–291.
[11]
Barton, D. K., Radar System Analysis and Modeling, Norwood, MA: Artech House, 2005.
[12]
Ruck, G. T., et al., eds., Radar Cross Section Handbook, New York: Plenum Press, 1970.
[13]
Weiner, S., and S. Borison, “Radar Scattering from Blunted Cone Tips,” IEEE Trans. Antennas Propag., vol. 14, no. 6,
Nov. 1966, pp. 774–781.
[14]
Blore, W. E., “The Radar Cross Section of Spherically Blunted 8° Right-Circular Cones,” IEEE Trans. Antennas
Propag., vol. 21, no. 2, Mar. 1973, pp. 252–253.
[15]
Roscoe, B. J., and Banas, J. F., “Cross Section of Blunted Cones,” Proc. IEEE, vol. 61, no. 11, Nov. 1973, p. 1646.
[16]
Bechtel, M. E., “Application of Geometric Diffraction Theory to Scattering from Cones and Disks,” Proc. IEEE, vol. 53,
no. 8, Aug. 1965, pp. 877–882.
[17]
Stuart, W. D., “Cones, Rings, and Wedges,” in Radar Cross Section Handbook, vol. 1 (G. Ruck et al., eds.), New York:
Plenum Press, 1970.
[18]
Mack, C. L., Jr., and B. Reiffen, “RF Characteristics of Thin Dipoles,” Proc. IEEE, vol. 52, no. 5, May 1964, pp. 533–
542.
[19]
Peebles, P. Z., Jr., “Bistatic Radar Cross Sections of Chaff,” IEEE Trans. Aerosp. Electron. Syst., vol. 20, no. 2, Mar.
1984, pp. 128–140.

[20]
Mie, G., “Beiträge zur Optik trüber Medien, spezeill kolloidaler Metallösungen,” Annalen der Physik, Seris IV, vol.
25, no. 3, 1908, pp. 377–445.
[21]
Currie, N. C., R. D. Hayes, and R. N. Trebits, Millimeter-Wave Radar Clutter, Norwood, MA: Artech House, 1992.
[22]
Blake, L. V., Radar Range-Performance Analysis, Norwood, MA: Artech House, 1986.
[23]
Nathanson, F. E., J. P. Reilly, and M. N. Cohen, eds, Radar Design Principles, 2nd ed., New York: McGraw-Hill,
1991.
[24]
Skolnik, M. I., Introduction to Radar Systems, 3rd ed., New York: McGraw-Hill, 1962.
[25]
Stratton, J. A., Electromagnetic Theory, New York: McGraw-Hill, 1941.
[26]
Kerr, D. E., Propagation of Short Radio Waves, vol. 13 of MIT Radiation Lab. Series, New York: McGraw-Hill, 1951;
Norwood, MA: Artech House (CD-ROM edition), 1999.
[27]
Swerling, P., “Probability of Detection for Fluctuating Targets,” RAND Corp., Santa Monica, CA, Res. Memo. RM-
1217, Mar. 17, 1954. Reprinted: IRE Trans. Inf. Theory, vol. 6, no. 2, Apr. 1960, pp. 269–308.
[28]
Rice, S. O., “Mathematical Analysis of Random Noise,” Bell Syst. Tech. J., vol. 23, no. 3, Jul. 1944, pp. 282–332; vol.
24, no. 1, Jan. 1945, pp. 461–556.
[29]
Marcum, J. I., “A Statistical Theory of Target Detection by Pulsed Radar,” RAND Corp., Santa Monica, CA, Res.
Memo. RM-754-PR, Dec. 1, 1947. Reprinted: IRE Trans. Inf. Theory, vol. 6, no. 2, Apr. 1960, pp. 59–144. Reprinted:
Detection and Estimation (S. S. Haykin, ed.), Halstad Press, 1976, pp. 57–121.
[30]
Budge, M. C., Jr., “EE 619: Intro to Radar Systems,” www.ece.uah.edu/courses/material/EE619/index.htm.
[31]
Wilson, J. D., “Probability of Detecting Aircraft Targets,” IEEE Trans. Aerosp. Electron. Syst., vol. 8, no. 6, Nov.
1972, pp. 757–76.
1 John William Strutt, 3rd Baron Rayleigh.
2 Figure 3.3 shows a full wavelength dipole. For a half wavelength dipole, σ = 0.86λ2.
3 After Gustav Adolf Feodor Wilhelm Ludwig Mie [20].
4 If we were to give credit where credit is due, this should be termed the Rice model after the analyst [28] who discussed it in
terms of single-pulse detection, or the Marcum model after the analyst [29] who first considered it in terms of multiple pulse
detection.
5 As a note, Swerling was considering pulsed, presumably rotating, search radars in his analyses [27].
6 We are treating the transmitter, antenna, receiver, and environment as ideal so that the pulse is rectangular when it leaves the
antenna and rectangular when it reaches the input to the matched filter.
7 This is an interesting property of Gaussian random processes that does not apply to random processes governed by other
density functions.
8 A brick wall response is essentially a rect[x] function. It is unity over a given frequency range and zero elsewhere.

Chapter 4
Noise
4.1 INTRODUCTION
In this chapter, we discuss the noise, noise temperature, and noise figure terms of the radar
range equation. We start with the basic definition of noise as it applies to radar theory and then
progress to the topics of noise temperature and noise figure.
The type of noise of interest in radar theory is termed thermal noise or Johnson noise and
is generated by the random motion of charges in conductors. John Bertrand Johnson and
Harry Theodor Nyquist discovered this type of noise in 1927 [1, 2]. Johnson observed the
noise in experiments and Nyquist developed a theoretical basis for Johnson’s measurements.
Their papers do not make clear whether Nyquist developed the theory to support Johnson’s
observations or Johnson performed the experiments to verify Nyquist’s theory. We suspect a
somewhat collaborative effort, given the dates of the papers.
One of the equations in Nyquist’s paper defines the mean-square voltage appearing across
the terminals of a resistor of R ohms at a temperature T kelvin, in a (differential) frequency
band dv hertz wide, as
where k = 1.38×10−23 W-s/K is Boltzmann’s constant. Johnson had a similar equation, but for
mean-square current.
We retained Nyquist’s notation in (4.1); however, from here on, we will adopt a more
common notation. To that end, we denote the noise voltage generated by a resistor in a
differential frequency interval, df, as v(t). We stipulate v(t) is a zero-mean, wide-sense
stationary, real random process with a mean-square value and variance of
Since v(t) is zero-mean, its mean-square value equals its variance.
The stipulation of zero-mean says the noise voltage does not have a direct current (DC)
component, which is reasonable since such a component would have been noted by Johnson.
The stipulation of wide-sense stationary implies the mean and variance are constant. This is
reasonable since we already stipulated a mean of zero and we expect the noise power (i.e.,
mean-square value, variance) to be constant over any time period of interest to us.
Nyquist showed that the noise energy term of (4.2), kT, is a limiting case of one form of the
more general Planck’s law, which is

where h = 6.6254×10−34 W-s2 is the Planck constant and f is frequency, in Hz. As f → 0, this
degenerates to
An implication of (4.4) is that E is constant over frequencies applicable to most radars (see
Exercise 10). This further implies that 
 is independent of frequency, unless R is a function
of frequency.
This background prepares us to consider noise energy in a radar receiver. However, we
will first discuss how 
 translates to power and energy delivered to a load.
4.2 NOISE IN RESISTIVE NETWORKS
4.2.1 Thevenin Equivalent Circuit of a Noisy Resistor
Figure 4.1 shows the Thevenin equivalent circuit of a noisy resistor. It consists of a noise
source with a voltage characterized by (4.2) and a noiseless resistor with a value of R.
Figure 4.1 Thevenin equivalent circuit of a noisy resistor.
Figure 4.2 Diagram for computing the power delivered to a load.
If we connect the noisy resistor to a noiseless resistor, RL, we can find the power delivered
to RL by the noisy resistor using the equivalent circuit of Figure 4.1 to compute the voltage
across RL, and then use this voltage to find the power delivered to RL. Figure 4.2 shows the
resulting circuit. The voltage across RL is given by
Using (4.2), the power delivered to RL in a differential bandwidth, df, is

If the load is matched to the source resistance (i.e., if RL = R), we have
If we divide PL by df, we obtain the energy delivered to the load as
which is the familiar form used in the radar range equation.
Figure 4.3 Schematic diagrams for the two-resistor problem.
4.2.2 Multiple Noisy Resistors
If we have a network consisting of multiple noisy resistors, we can find its Thevenin
equivalent circuit by using superposition. To see this, consider the example of Figure 4.3. The
left schematic of the figure shows two parallel noisy resistors, and the center schematic shows
their equivalent circuits based on Figure 4.1. The right schematic shows the overall Thevenin
equivalent circuit for the pair of resistors. To find vo(t), we first consider one voltage source
at a time and short all other sources. Thus, with only source v1(t), we get
and with only source v2(t), we get
By superposition, we have

To get the equivalent resistance, we short both voltage sources of the center figure and find
the equivalent resistance across the terminals. When we short the sources, we note that R1 and
R2 are in parallel, which allows us to compute the equivalent resistance as
We next need to compute the mean-square value of vo(t). To facilitate this, we must further
stipulate that the noise voltages generated by the noisy resistors are independent. We justify
this restriction by rationalizing that the random motion of charges in one resistor should be
independent of the random motion of charges in any other resistor. With this restriction, we
are able to say
where the last equality is because v1(t) and v2(t) are zero-mean. With this and some algebraic
manipulation, we have
where we have made use of (4.12), 
, and 
. The details of (4.14)
are left as an exercise.
4.3 EQUIVALENT/EFFECTIVE NOISE TEMPERATURE FOR ACTIVE
DEVICES
For passive devices, such as resistive attenuators, it is possible to find the noise energy
delivered to a load by extending the technique used in the above example. For active devices,
this is not possible. Measurement provides the only method for determining the noise energy
an active device delivers to a load.
In general, the noise energy delivered to the load depends upon the input noise energy to
the device and the internally generated noise. The standard method of representing this is to
write the noise energy delivered to the load as the sum of the amplified input noise and the
noise generated internally by the active device [3, 4]:1
where

• G denotes the gain of the device.
• kTa denotes the input, or source, noise energy.
• Ta denotes the noise temperature of the source.
• GkTe denotes the noise energy generated by the device.
• Te denotes the equivalent/effective noise temperature of the device.2
In (4.15), the term GEnin represents the portion of the output noise energy due only to the
noise into the device. This component of the output noise is the input noise amplified by the
gain of the device. The term GkTe represents the energy of the noise generated by the device.
Its form is chosen to be consistent with the standard kT representation discussed above.
Including G in this term is a convenience and allows us to write
thus allowing consistent expression for the noise energy equation.
In (4.16), Ts denotes the noise temperature, or combined noise temperature, of the device. It
is the combined temperature of the noise source and the equivalent/effective noise
temperature of the device. We termed this the system noise temperature in Chapter 2. For a
radar, Ta represents the temperature of the noise entering the antenna from the environment.
The value of Ta ranges from tens of degrees kelvin when the antenna beam points at clear sky,
to many thousands of degrees kelvin when the beam points at the sun [5, p. 208].
For resistors, Te is the actual temperature of the resistors. For active devices, it is not an
actual temperature, but the temperature necessary for a resistor to produce the same noise
energy as the active device—thus the origin of the words equivalent or effective.
We introduced G into the noise power equation because it will enter into computation of the
overall Te for cascaded devices and because of its inclusion in the radar range equation (see
Chapter 2).
4.4 NOISE FIGURE
4.4.1 Derivation of Noise Figure
An alternative to using equivalent/effective noise temperature is to use noise figure. Harald
Trap Friis formalized the early research on noise figure in a 1944 paper [6] that defined noise
figure as the ratio of the SNR at the input of the device to the SNR at the output of the device.
In equation form,
where Psin denotes the signal power into the device; Pnin denotes the noise power into the

device; Psout denotes the signal power out of the device; and Pnout denotes the noise power out
of the device.
In this book, we use the IEEE definition [7]. An interpretation of that definition is: noise
figure is the noise energy delivered to a load by the actual device divided by the noise energy
delivered to the load by an ideal device with the same gain. In equation form
where
and
The IEEE definition goes further to say that the noise figure equation is defined for the case
where the noise temperature of the input to the device is the reference value of T0 = 290 K.
Using this and (4.16) gives
and
which leads to
Alternately, we can solve for Te in terms of Fn as
An important point from (4.23) is that the minimum noise figure of a device is Fn = 1.
4.4.2 Attenuators
For most devices, noise figure is determined by measurement. Attenuators represent the
exception to this rule. For attenuators, the noise figure is normally taken to be the attenuation.
Thus, for an attenuator with an attenuation of L (a number greater than 1), the noise figure is
assumed to be

The rationale behind this is that an attenuator matched to the source and the load
impedances (which are assumed identical) produces a noise energy out of the attenuator equal
to the noise energy input to the attenuator [8, 9]. Indeed, using (4.15) with Enout = Enin and G =
1/L gives
If we further assume a source temperature of T0 (recall the necessity for using this
temperature when computing noise figure), we get
or
and, by association with (4.24), Fn = L.
The authors have always been concerned with the assumption that the noise energy out of
an attenuator is identical to the noise energy into the attenuator. To investigate this further, we
analyzed a T-type attenuator that consisted of noisy resistors. Figure 4.4 contains a schematic
of the circuit we analyzed. The values of R1, R2, and R3 were computed so that the input and
output resistance of the attenuator was R and the attenuation was L W/W. When the
temperature of the source and the three resistors of the attenuator was the same, we found the
energy out of the attenuator was the same as the energy into the attenuator. However, when the
temperature of the source differed from the temperature of the resistors, the energy into the
attenuator did not equal the energy out of the attenuator. Thus, for this simple example, we
verified that the noise energy into and out of the attenuator are equal if the source and
resistors are at the same temperature. We assume this is also the case for a general attenuator.
Figure 4.4 Schematic of a T-type attenuator.
We carried the T-type attenuator example a step further and considered some cases where
the source temperature was Ta but the temperature of the attenuator resistors was some other
temperature, TR. We found, at least for the example cases we considered, an output noise
energy given by

We derived (4.29) from (4.26) and (4.28) with the temperature T0 replaced by TR and Enin =
kTa. This handy equation lets us analyze attenuators with different source noise powers and
circumstances where the attenuator is not at a temperature of T0. We caution that we have not
proved (4.29) valid for a general attenuator, only for our T-type resistive attenuator. However,
it agrees with a similar equation in Blake’s NRL report [10].
For those (ambitious) readers who are interested, we included the above problem as
Exercise 7.
4.5 NOISE FIGURE OF CASCADED DEVICES
Since a typical radar has several devices that contribute to the overall equivalent/effective
noise temperature or noise figure, we need a method of computing the equivalent/effective
noise temperature and noise figure of a cascade of components. To this end, we consider the
block diagram of Figure 4.5. In this figure, the circle to the left denotes a noise source,
represented in a radar by the antenna or other radar components. For the purpose of
computing noise figure, we assume the temperature of the noise source is T0 (consistent with
the definition of noise figure). The blocks following the noise source represent various radar
components, such as amplifiers, mixers, attenuators, and so on. The various blocks are
characterized by their gain, Gk, noise figure, Fk, and equivalent/effective noise temperature,
Tk.
To derive the equation for the overall noise figure and equivalent/effective noise
temperature of the N devices, we will consider only Device 1, then Devices 1 and 2, then
Devices 1, 2, and 3, and so forth. This will allow us to develop a pattern we can extend to N
devices.
Figure 4.5 Block diagram for computing system noise figure.
Recalling that we always assume a source temperature of T0 when computing noise figure,
we posit an input noise energy for Device 1:
The noise energy out of Device 1 is [see (4.15)]

From (4.23), the system equivalent/effective noise temperature is Te1 = T1 and the system
noise figure from the source through Device 1 is
For Device 2, the input noise energy is
and the noise energy out of Device 2 is
From (4.34), we see the equivalent/effective noise temperature of the cascade of Devices 1 and
2 is
The system noise figure from the source through Device 2 is
or, with T2 = T0(F2 – 1),
Notice how the gain of the first device reduces the equivalent/effective noise temperature and
noise figure of the second device. We will examine this concept again in an example. For now,
we proceed with determining the system noise figure from the source through the third
device.
The noise energy at the output of a cascade of Devices 1, 2, and 3 is

Rearranging the terms yields
With this result, the equivalent/effective noise temperature of the cascade of Devices 1, 2, and
3 is
The system noise figure from the source through Device 3 is
or, using (4.24),
Here, we note the product of the gains of the preceding two devices reduces the noise figure
and temperature of Device 3.
With some thought, we can extend (4.40) and (4.42) to write the equivalent/effective noise
temperature of the system, from the source through Device N, as [11]
The noise figure of the system, from the source through Device N, is
In the equations above, we found the system noise figure between the input of Device 1
through the output of Device N. If we wanted the equivalent/effective noise temperature and
noise figure between the input of any other device (say, Device k) and the output of some
other succeeding device (say, Device m), we would assume the source of Figure 4.5 (at a
temperature of T0) is connected to the input of Device k and we would include terms like

(4.43) and (4.44) that would carry to the output of Device m. Thus, for example, the
equivalent/effective noise temperature of Devices 2, 3, and 4 is
and the noise figure from the input of Device 2 to the output of Device 4 is
We leave the derivation of (4.45) and (4.46) as an exercise.
4.6 AN INTERESTING EXAMPLE
We now consider an example of why, as a general rule of thumb, radar designers normally
include an RF amplifier as an early element in a receiver. In this example, we consider the two
options of Figure 4.6. In the first option, we have an amplifier followed by an attenuator, and
in the second option we reverse the order of the two components. The gains and noise figures
of the two devices are the same in both configurations. For Option 1, the noise figure from
the input of the first device to the output of the second device is
Figure 4.6 Two configurations options.
For the second option, the noise figure from the input of the first device to the output of the
second device is
This is a dramatic difference in noise figure of the combined devices. In general, if the

preceding devices have a net gain, the noise contributed by a device is reduced relative to its
individual noise figure. If the preceding devices have a net loss, the noise contributed by the
device is increased relative to its individual noise figure.
In Option 1 of the example, the combination of the two devices had a noise figure close to
that of the amplifier. However, for the second option, the noise figure was the combined noise
figures of the two devices. This is why radar designers like to include an amplifier early in
the receiver chain: it essentially sets the noise figure of the receiver. As a general rule of
thumb, a nominal gain of 20 to 25 dB in the RF amplifier usually ensures the noise figure of
the receiver primarily due to the noise figure of the RF amplifier.
4.7 OUTPUT NOISE POWER WHEN THE SOURCE TEMPERATURE IS
NOT T0
In the discussion above, we considered a source temperature of T0. We now want to examine
how to compute the noise energy out of a device for a source temperature other than T0. From
(4.15), we have
where Ta is the noise temperature of the source. If we were to rewrite (4.49) using noise
figure, we would have
If we have a cascade of N devices, G denotes the combined gain of the N devices; Te denotes
the equivalent/effective noise temperature of the N devices; and Fn denotes the noise figure of
the N devices. In the exercises, we consider specific examples of how different source
temperatures can affect Pnout and, more importantly, SNR.
4.8 A NOTE ABOUT CASCADED DEVICES AND THE RADAR RANGE
EQUATION
Sometimes radar analysts are uncertain about whether to include the loss of lossy, passive
components between the antenna and the first active device in the loss term of the radar range
equation, or in the equivalent/effective noise temperature, Te, and noise figure. The simple
answer is: if Ta = T0, it does not matter, as long as it is not included in both places. If Ta ≠ T0,
the losses should be included in Te and the noise figure. If it is not, (2.29)—which is used to
compute Ts in Chapter 2— would be invalid.
4.9 EXERCISES

1.
A radar receiver has the components and parameters indicated in Table 4.1. Their
relative locations are as in the table.
Table 4.1
Receiver Components
Device
Gain (dB)
Noise Figure (dB)
Waveguide (attenuator)
−2
N/A
RF amplifier
20
6
First mixer
−3
10
IF amplifier
100
20
a)
Compute the noise figure through each of the four devices, referenced to the
waveguide input. Note the increase in noise figure each device causes.
b)
Repeat part a) for the case where the RF amplifier has a low noise figure of 2 dB.
Again, note the increase in noise figure created by each device. Note how the
devices following the low-noise RF amplifier seem to have more effect on noise
figure than does the RF amplifier with the higher noise figure.
c)
What is the equivalent/effective noise temperature, in kelvin, of the RF amplifier of
part a)?
d)
Based on the values used for part a), what is the equivalent/effective noise
temperature, in kelvin, of the receiver, through the IF amplifier, referenced to the
waveguide input?
e)
Compute the noise power, in dBm, at the output of the IF amplifier, assuming a noise
temperature for the antenna (input to the waveguide) of 290 K. Use the values from
part a). Assume a bandwidth of 1 MHz.
f)
Repeat part e) using an antenna noise temperature of 100 K. Note that this is almost
the same as for 290 K. This indicates that the internal noise of the receiver is the
major contributor the total system noise energy for this particular case.
g)
Repeat part d) using an antenna noise temperature of 6,000 K. This result indicates
that the antenna noise propagated through the receiver is the major contributor to the
total system noise energy.
2.
Assume a radar with noise figure Fn = 6 dB referenced to the antenna feed.
a)
Compute the equivalent/effective noise temperature of the receiver.
b)
Assume a noise bandwidth of 1 MHz and a receiver gain of 100 dB. Compute the
noise power in dBW at the receiver output.
3.
The radar of Exercise 2 has an SNR of 20 dB for a particular scenario. Suppose it
operates at night and looks into a clear sky with a Ta′ of 10 K (see Chapter 2).
a)
Assume a noise bandwidth of 1 MHz and a receiver gain of 100 dB. Compute the
noise power, in dBW at the receiver output.
b)
What is the change in SNR, in dB, relative to the case of Exercise 2? In Exercise 2,
we assumed a Ta = T0.

4.
Repeat Exercise 3 with Ta = 20,000 K.
5.
Derive (4.14).
6.
Derive equations for the three resistors of the attenuator from Figure 4.4, in terms of the
input and output resistance, R, and the loss, L. The loss has the units of W/W.
7.
Derive an equation for the noise energy delivered to the attenuator of Figure 4.4 by the
source and the noise energy delivered to the load by the attenuator. Assume a source
noise temperature of Ta and a noise temperature for all attenuator resistors of TR.
8.
Derive (4.43).
9.
Derive (4.45) and (4.46).
10.
Plot E in (4.3) versus f. Let f vary logarithmically from 1 GHz to 1,000 GHz. Plot E with
the units of dB relative to a milli-joule. Generate curves for temperatures of 2.9, 29, and
290 K. Does your plot support the statement that E is insensitive to frequency in the
range of frequencies used in radars?
References
[1]
Johnson, J. B., “Thermal Agitation of Electricity in Conductors,” Phys. Rev., vol. 32, July 1928, pp. 97–109.
[2]
Nyquist, H., “Thermal Agitation of Electric Charge in Conductors,” Phys. Rev., vol. 32, Jul. 1928, pp. 110–113.
[3]
Erst, S. J., Receiving Systems Design, Dedham, MA: Artech House, 1984, p. 49.
[4]
Gao, J., RF and Microwave Modeling and Measurement Techniques for Field Effect Transistors, Raleigh, NC:
SciTech, 2010, p. 104.
[5]
Barton, D. K., Radar Equations for Modern Radar, Norwood, MA: Artech House, 2013.
[6]
Friis, H.T., “Noise Figures of Radio Receivers,” Proc. IRE, vol. 32, no. 7, Jul. 1944, pp. 419–422.
[7]
IEEE Standard Dictionary of Electrical and Electronic Terms, 6th ed., New York: IEEE, 1996.
[8]
Egan, W. F., Practical RF System Design, New York: Wiley & Sons, 2003, pp. 55–56.
[9]
Pozar, D. M., Microwave Engineering, New York: Addison-Wesley, 1990, pp. 590–591.
[10]
Blake, L. V., “A Guide to Basic Pulse-Radar Maximum-Range Calculation, Part 1—Equations, Definitions, and Aids to
Calculation,” Naval Research Laboratory, Washington, D.C., Rep. No. 6930, Dec. 23, 1969, p. 49. Available from
DTIC as 701321.
[11]
Budge, M. C., Jr., “EE 619: Intro to Radar Systems,” www.ece.uah.edu/courses/material/EE619/index.htm.
1 Until now, we have been diligent in providing units for the various quantities we use. Henceforth, we will assume the reader is
familiar with the appropriate units and no longer include them in equations, unless necessary to avoid confusion.
2 We use the terminology “equivalent/effective noise temperature” because equivalent noise temperature and effective noise
temperature are used interchangeably in the literature.

Chapter 5
Radar Losses
5.1 INTRODUCTION
For our last radar range equation-related topic, we address the loss term, L. Losses have been
included in the radar range equation since it first appeared in Norton and Omberg’s 1947
paper [1]. Losses have been continuously studied and new loss factors have been introduced as
radar technology has advanced [2–11].
In an attempt to organize our discussion of losses, we will trace the losses through the steps
we used to derive the radar range equation in Chapter 2. That is, we start with the transmitter
and antenna and progress to propagation losses. On receive, we will address losses between
the antenna and RF amplifier, which we term RF losses, and then proceed to losses associated
with the matched filter, signal processor, and detection process.
5.2 TRANSMIT LOSSES
Transmit losses are losses in components between the final RF power source and antenna
feed. Radars that use reflector antennas, space-fed phased arrays, or constrained feed-phased
arrays could have the components shown in Table 5.1 [12–17]; however, not all radars have
all of the devices listed. As examples, Figure 5.1 contains two representative transmitter block
diagrams illustrating the different devices that might be included in each.
Waveguide run (sometimes referred to as “plumbing”) is all of the pieces of waveguide
used between components as transmission line. Waveguide is typically used because of its
extremely low loss and high power handling capability. Waveguide switches (which can be
manual or automated) are used for routing signals. For example, the transmitter can be
switched to a dummy load (high power terminator) for test purposes. Similarly, a waveguide
switch (usually manual) can be used to open a waveguide for test signal injection. Power
dividers in general (a 12-way is given in Table 5.1) are used for power distribution and
combining. One example is a corporate antenna feed.
Table 5.1
Representative RF Losses
Component
Loss (dB)
Waveguide run
0.1–0.3
Waveguide switch
0.7
Power divider (12-way)
1.6
Duplexer
0.3–1.5
TR switch
0.5–1.5
Circulator/isolator
0.3–0.5

Receiver protection
0.2–1.0
Preselector (receive only)
0.5–2.5 (0.5 typical)
Directional coupler
0.3–0.4
Rotary joint
0.2–0.5
Mode adapter
0.1
Waveguide step attenuator
0.8
Feed (monopulse or simple)
0.2–0.5
Figure 5.1 Representative transmitter block diagrams.
A duplexer is a fast-acting, nonreciprocal device that allows for a common antenna to be
used for transmit (Tx) and receive (Rx) in a radar using pulsed waveforms and protects the
receiver from high-power returns. Often a high-power circulator serves as duplexer, but it
can also be implemented using a balanced network of transmit-receive (TR) switches and a
receiver protector [18]. In transmit-receive (T/R) modules, the duplexer is usually a
circulator. In high-power radars, the duplexer can be a TR switch of the gas discharge type
(T/R tube, see below) [18]. A TR switch is an automatic device employed in a radar for
preventing the transmitted energy from reaching the receiver but allowing the receive energy
to reach the receiver without appreciable loss [18].
Circulators are three port devices where the signal into one port can only leave the next

port, and so on (like a roundabout where you must exit on the street following the street you
entered). Terminating one port of a circulator results in an isolator, which is a two-port
device where the signal can travel in only one direction. Any reflected energy returned to the
isolator is shunted to a terminating load. Isolators are often used prior to poorly matched
components (e.g., a filter or switch).
While the duplexer provides a certain amount of protection for the receiver against high-
power returns, it does not always provide enough receiver protection. Receiver protection in
Table 5.1 refers to devices specifically used to protect the receiver (in addition to the
duplexer), such as diode or ferrite limiters. A TR tube is a gas-filled RF switching tube. When
high power from the transmitter enters the TR tube, the tube arcs, shorting out, which reflects
the incoming power, thus protecting the receiver. TR tubes are very fast acting.
A preselector is a filter (often implemented in waveguide) used in the receiver to limit the
bandwidth. For frequency agile radars, the agility bandwidth is passed; for single frequency
radars, the preselector is matched to the channel bandwidth. Directional couplers are used to
sample or couple signals out of the transmitter for test purposes. The power ratio between the
input signal and the sampled signal is a calibrated amount (e.g., 10, 20, 40 dB). A low
coupling ratio (e.g., 40 dB) allows transmitter power measurements to be made using low-
power test equipment.
A rotary joint is a device used to couple RF energy from a fixed transmission line to a
device that is rotating, such as an antenna. A rotary joint can also be used with antennas that
stow or pack themselves. A mode adapter is generally any device that changes the mode of
propagation (e.g., from coaxial line transmission to rectangular waveguide transmission).
Waveguide attenuators are sometimes used in front of the receiver RF low noise amplifier
(LNA) for automatic gain control/sensitivity time control (AGC/STC) (see Chapter 14).
Figure 5.2 contains plots of theoretical waveguide loss versus frequency for several
standard waveguides [19, 20]. From this we see that the waveguide losses indicated in Table
5.1 are representative of radars that contain from 1 to 2 m of waveguide connecting the
various components of the transmitter.

Figure 5.2 Theoretical rectangular waveguide loss (copper).
The calculations associated with the theoretical loss in a rectangular waveguide, which are
dependent upon the broad and short wall dimensions, the permeability and permittivity of the
dielectric filling the waveguide, and the waveguide material, can be cumbersome (see
Appendix 5A). Barton presents a convenient approximation for waveguide loss in dB/m of [9,
p. 359]1
where f is the frequency in GHz (f < 200 GHz). This approximation is plotted in Figure 5.2 for
comparison as the dashed curve.
For active phased arrays that use T/R modules [21], the losses are primarily due to a switch
or circulator used to route signals from the power amplifier to the antenna and from the
antenna back to the receiver LNA. This is illustrated in Figure 5.3. As a result of the T/R
modules’ collocation with associated array elements, the transmit losses associated with active
phased arrays are generally much lower than those associated with radars that use passive
antennas such as reflectors, space-fed phased arrays, and constrained feed-phased arrays.

Figure 5.3 Example T/R module block diagram.
As an example of total transmit RF losses, Table 5.2 contains a summary of these losses for
the three transmitter configurations of Figures 5.1 and 5.3. In computing the waveguide
losses, we will assume the radar with the reflector (top drawing of Figure 5.1) is an L-band
search radar. The space-fed phased array in the lower drawing is an S-band multifunction
radar and the T/R module in Figure 5.3 is used in an X-band multifunction radar. The
difference in operating frequencies is the reason for choosing the different waveguide losses.
Table 5.2
Example Transmit RF Losses
5.3 ANTENNA LOSSES
The next element of the transmit chain is the antenna and its associated feed. A representative
list of losses associated with the various feed and antenna components is contained in Table

5.3. The entries for waveguide and stripline feed apply to antennas that use constrained feeds,
and the difference between parallel and series feed networks is illustrated in Figure 5.4 [22]. In
the series feed, the energy enters on one end of an RF transmission line (such as a rectangular
waveguide, stripline, or microstrip) and is extracted at different points along the line. In a
parallel feed network, the energy enters an RF transmission line and is subsequently split
several times before being delivered to the radiating elements. As a note, it is possible for an
antenna to use both series and parallel feed networks [23, pp. 5–8]. As an example, the rows
the array could be fed by a series feed, while the elements in each row would be fed by a
parallel feed network. It will be noted that the feed loss assigned to active arrays is 0 dB. This
is because the radiating element driven by a T/R module is very close to the power amplifier.
The phase shifter losses apply to passive and constrained feed phased arrays. As a note, the
losses apply to the entire array and not to each phase shifter of the array. The losses are
shown as 0 dB for active phased arrays because the phase shifter is not in the path between the
antenna and the power amplifier or LNA, where loss is important (see Figure 5.3).
Table 5.3
Antenna Dissipative Losses
Location
Component
Typical loss (d
Feed system
Feed horn for reflector or lens
0.1
Waveguide series feed
0.7
Waveguide parallel feed
0.4
Stripline series feed
1.0
Stripline parallel feed
0.6
Active module at each element
0.0
Phase shifter
Nonreciprocal ferrite, or Faraday rotator
0.7
Reciprocal ferrite
1.0
Diode (3- or 4-bit)
1.5
Diode (5- or 6-bit)
2.0
Diode (per bit)
0.4
Active module at each element
0.0
Array
Mismatch (no electronic scan)
0.2
Mismatch (electronic scan 60º)
1.7
Exterior
Radome
0.5-1.0
Source: [9, 44].

Figure 5.4 Series and parallel feeds. (After: [22].)
Mismatch loss also applies to phased arrays and is a loss due to impedance mismatch
between the radiating elements of the antenna and free space. Mismatch loss is given by
where
Γ is the reflection coefficient and VSWR is the voltage to standing wave ratio [19, 20]. For a
scanning array, the mismatch loss is given by [9]
where the element power gain is represented by
and 1 < β < 2 (usually ≈ 1.5) [9]. Given a VSWR of 1.5, mismatch loss for β = 1.25, 1.5, 1.75
and 2 is plotted in Figure 5.5. The average mismatch loss is 0.41, 0.66, 0.91, and 1.2 dB for β =
1.25, 1.5, 1.75, and 2, respectively (over 60º scan).

Figure 5.5 Mismatch loss vs. angle. (After: [9].)
As a note, some antenna analysts subtract the antenna losses from the antenna directivity
(see Chapter 2) and term the result the antenna gain. Because of this, one must take care when
using antenna directivity, antenna gain, and antenna losses in the radar range equation.
When the beam of a phased array antenna is scanned off of broadside (off of array
normal), the antenna directivity decreases. If this is not explicitly included when generating
the antenna pattern at the scanned angle, it should be included as a loss. Barton suggests a
factor of
where θ is the scan angle [9, p. 369].2 Figure 5.6 contains a plot of (5.1) for β = 1.0, 1.5, and
2.0. The average scan loss is 1.3, 1.6, 1.9, and 2.2 dB for β = 1.25, 1.5, 1.75, and 2, respectively
(over 60-deg scan).
The next loss we discuss is beamshape loss.3 This loss is associated with the situation where
the antenna beam is not pointed directly at the target or where the beam is scanning across the
target during the time the radar is coherently or noncoherently integrating a sequence of
pulses (coherent and noncoherent integration is discussed in Chapter 8). In both cases, the full
effect of the antenna directivity (GT and GR) terms of the radar range equation will not be
realized. This most often happens during search. It is not applicable during track because it is
assumed the target is very close to beam center during track.

Figure 5.6 Scan loss vs. angle. (After: [9].)
We account for both of the above situations by including beamshape loss as one of the loss
factors. Historically, radar analysts have used the values of 1.6 or 3.2 that were derived by
Blake in his 1953 paper [25]. However, Hall and Barton [5, 9, 26] revisited this problem in the
1960s and derived revised loss numbers of 1.24 and 2.48 dB. It should be noted that Barton
and Hall indicate that there are many factors that affect scan loss, such as beam step size in
phased array radars, number of pulses noncoherently integrated, whether or not the radar is
continuously scanning, and detection probability. As such, the values of 1.24 and 2.48 dB
should be considered rule-of-thumb numbers that would be suitable for preliminary radar
analysis or design. In a more detailed analysis, these numbers should be revised based on the
factors discussed by Barton and Hall.
The value of 1.24 dB is related to what is termed 1-D scanning, and the value of 2.48 dB
relates to 2-D scanning. 1-D scanning would be associated with search radars that use a fan
beam (a beam with a large beamwidth in one dimension (usually elevation) and a narrow
beam in the other dimension). The radar would then rotate (or nod) in the narrow beam
dimension but remain fixed in the wide beam direction. An example of such a radar is
considered in Example 2 of Chapter 6, where we analyze a search radar with a cosecant
squared elevation beam. In these fan beam types of radars, we assume the antenna directivity
does not change much in the wide direction and that there is no need to include another loss. If
this is not the case, we would want to use the 2-D beamshape loss.
In these situations, the antenna directivity (in the direction of the target) changes as the
beam scans by the target, thus not all of the pulses will exhibit the same SNR. This, in turn,
could affect the computation of detection probability (see Chapters 6 and 8). To account for
this, we include the 1-D beamshape loss in the loss term of the radar range equation.
An example of where the use of the 2-D beamshape loss would be appropriate is in phased
arrays radars (such as the second and third examples of Table 5.2) that scan a sector by
stepping the beam in orthogonal directions (azimuth and elevation or u and v—see Chapter
12). In this situation, the radar would move to a beam position and transmit a pulse, or burst
of pulses, and then move to another beam position. Because of this action, it is likely that the
target could be off of beam center in two dimensions, thus the need for the 2-D beamshape

loss. In this situation, it may also be appropriate to include the scanning loss of (5.6) if the
angular extent of the search sector is large.
A situation where we might want to use only a 1-D beamshape loss with a phased array
radar is where we are generating a detection contour (see Example 2 of Chapter 6). In such a
case, we would use the antenna directivity plot in, for example, elevation, and have the
directivity as a function of elevation. However, we would need to account for the fact that the
target is not on beam center in azimuth. Thus, we would include a 1-D beamshape loss in the
radar range equation.
In discussing the phased array examples, we made the tacit assumption that the beams of the
search sector were spaced close together as illustrated by Figure 5.7. This is similar to what
Barton terms dense packing [9] and is characterized by the fact that there is no angular region
that is not covered by the 3-dB beam contour of the radar (the 3-dB beam contours are the
circles in Figure 5.7). Barton discusses another type of packing he terms sparse packing,
wherein there may be parts of the angle space that are not covered by beams on any one scan
(but hopefully will be covered on successive scans). In this situation, he points out that the
beamshape loss now becomes a function of detection probability. This is something that
should be considered in detailed studies of the impact of search methodology on detection
performance of the radar.
We continue our previous example by adding antenna losses to Table 5.2 to generate Table
5.4.
We assumed the L-band search radar is a scanning radar with a cosecant squared beam.
Therefore, we included only the feed and 1-D scan loss. We assume that the S- and X-band
radars are conducting a wide sector search and include mismatch (VSWR = 1.5) and scan
losses we computed from (5.4) and (5.6), respectively, using β = 1.5 and θ = 30º, which is one-
half the assumed ±60° extent of the search sector. We assumed the beams in the S-band radar
were tightly packed and used Barton’s 2-D scan loss of 2.48 dB. For the X-band radar, we
assumed the beams were not as tightly packed and thus used the historical 2-D scan loss of 3.2
dB. We assumed the radome on the S-band radar was cloth and use a fairly low value of
radome loss. We assumed a hard radome on the X-band array and used a larger value of
radome loss.
Figure 5.7 Examples of dense and sparse beam packing.

Table 5.4
Example Transmit RF and Antenna Losses
5.4 PROPAGATION LOSSES
The next losses we consider are propagation losses. The two main sources of propagation
losses are those due to oxygen and water vapor absorption and rain. Absorption losses depend
upon operating frequency, elevation angle of the target, and range to the target. They also
depend upon temperature, humidity, atmospheric pressure, and other such atmospheric
conditions. However, the atmospheric conditions are usually ignored and a standard
atmosphere is used [9, 27, 28].
Atmospheric losses were historically determined from graphs [3, 4, 10, 11]. However, with
today’s computers, they are easily calculated using the equations given in Appendix 5B. For
illustration purposes, Figures 5.8, 5.9, and 5.10 contain plots of two-way loss versus target
range for different elevation angles and frequencies of 1, 3, and 10 GHz (L-, S-, and X-band).
The plots were generated using the equations in Appendix 5B.
Figure 5.11 contains plots of two-way loss, in dB/km, versus frequency for different
rainfall rates. This plot was also generated from equations presented in Appendix 5B. As a
note, a somewhat standard rainfall rate for modeling purposes appears to be 4 mm/hr.
According to Blake, this corresponds to moderate rain. For comparison, rainfall rates of 0.25
mm/hr, 1 mm/hr, and 16 mm/hr are considered a drizzle, a light rain, and a heavy rain,
respectively [28, p. 219; 29].

Figure 5.8 Atmospheric attenuation—standard atmosphere—1 GHz.
Figure 5.9 Atmospheric attenuation—standard atmosphere—3 GHz.

Figure 5.10 Atmospheric attenuation—standard atmosphere—10 GHz.
Figure 5.11 Rain attenuation.
We will continue our example by adding atmospheric loss. We assume the L-band radar is a

long-range search radar that operates out to about 500 km. Because of its long operating
range, we assume the elevation angles of interest are in the range of 1°. With this, the two-way
atmospheric attenuation at 500 km will be about 2.2 dB (see Figure 5.6). At shorter ranges it
will be less. For example, at 200 km, the two-way attenuation will be about 1.7 dB. We will use
a compromise value of 2.0 dB.
We will assume the two phased array radars operate at ranges out to about 100 km and at
elevation angles of 0° to 60°. For this case, the atmospheric losses for the S-band radar can
vary from 0 dB to about 1.7 dB (see Figure 5.7). For the X-band radar, the atmospheric losses
can vary from 0 dB to about 2.8 dB. We will use a compromise value of 1 dB for the S-band
radar and 2 dB for the X-band radar.
We will ignore rain attenuation in this particular analysis. With this, our loss table is now as
shown in Table 5.5.
5.5 RECEIVE ANTENNA AND RF LOSSES
In general, the receive antenna losses will be the same as the transmit antenna losses. The
possible exception to this is the case where the radar uses separate transmit and receive
antennas or separate transmit and receive feeds. In that case, it may be necessary to derive a
separate set of losses for the receive antenna.
Like the antenna, the RF components in the receive path will generally be the same as in the
transmit path. Thus, the losses in Table 5.1 apply to receive, with the addition of the
preselector losses. We will assume that the RF portions of the L- and S-band radars are as
shown in Figure 5.12. The block diagram of the X-band T/R module used in the active phased
array is as shown in Figure 5.3.
Table 5.5
Example Transmit RF, Antenna, and Propagation Losses

Figure 5.12 Representative receiver RF block diagrams.
Continuing with our example, the loss table now becomes that shown in Table 5.6. We have
assumed that the antennas and feeds are the same in the three radars so that the receive antenna
losses will be the same as the transmit antenna, except for the scan loss. Scan loss is calculated
for both transmit and receive and needs only be applied on one or the other.
Table 5.6
Example Transmit RF, Antenna, Propagation, and Receive Losses
For the L- and S-band radars, we will need to add the losses for the various components

between the feed and the RF amplifier (the LNA in Figure 5.10). The waveguide attenuator
loss shown for the L-band radar applies to the case where the attenuation is set to 0 dB. It is the
insertion loss of the attenuator [20, 30]. The loss will increase as the attenuation increases, on
a dB for dB basis. In both the L- and S-band radars, we used the typical loss value for the
preselector. Also, we used 0.2 dB for the rotary joint loss. We used the transmit values for the
components that are common to the transmitter and receiver. For the X-band radar case, the
only RF receive losses we need to include are for the circulator.
As discussed in Chapter 2, the losses in Table 5.6 should be included in the system noise
figure, and not in the loss term of the radar range equation. It is part of the Fn term of (2.28)
of Chapter 2. Also, see the discussions in Section 4.7. Because of this, the individual losses are
itemized but not included in the total losses. This is the reason the total losses of Table 5.6
equal the prior losses.
5.6 PROCESSOR AND DETECTION LOSSES
The final set of losses we discuss are those associated with the matched filter, the signal
processor, and the constant false alarm rate (CFAR) circuitry (Table 5.7).
The mismatch loss associated with the matched filter mainly applies to matched filters for
unmodulated pulses or for the chips of phase coded pulses. (See Chapter 10.) This loss occurs
because the ideal rectangular pulse generated by the transmitter becomes distorted because of
the bandwidth limiting that takes place as the pulse travels through the transmitter and antenna,
to and from the target and back through the antenna and receiver to the matched filter. The
estimate provided in Table 5.7 was derived by considering rectangular pulse that has been
passed through different types of bandlimiting devices. A summary of the results of the
analysis is shown in Table 5.8.4 In that table, the N-stage tuned filters are filters of different
orders that have a bandwidth equal to the reciprocal of the pulsewidth. As can be seen, the
nominal loss is about 0.5 dB.
Table 5.7
Processor and Detection Losses
Source
Typical Values (dB)
Matched filter loss
Mismatch loss
0.5
Sidelobe reduction weighting loss
1.5
MTI loss with staggered waveforms
0–1
Doppler filter sidelobe reduction loss
1–3
Range straddle loss
0.3–1.0
Doppler straddle loss
0.3–1.0
CFAR loss
1–2.5
Table 5.8
Matched Filter Mismatch Loss

Input Signal
Filter
Mismatch Loss (dB)
Rectangular pulse
Gaussian
0.51
Rectangular pulse
1-stage single-tuned
0.89
Rectangular pulse
2-stage single-tuned
0.56
Rectangular pulse
3-stage single-tuned
0.53
Rectangular pulse
5-stage single-tuned
0.50
Rectangular pulse
Matched
0.00
The sidelobe reduction weighting loss applies to waveforms that use linear frequency
modulation (LFM) for pulse compression (see Chapter 10). It is an amplitude taper used to
reduce the range sidelobes of the compressed pulse. Since it is an amplitude taper, it also
reduces the peak of the matched filter output. The amount of reduction generally depends on
the type of weighting and the desired sidelobe levels. A list of various types of amplitude
tapers and the associated SNR loss is shown in Table 5.9.5 (For a summary of some common
weighting functions, see Appendix B.)
The table also contains the peak sidelobe level associated with the weighting, along with the
associated straddle loss. Straddle loss will be discussed later in this chapter. Some common
weightings used with LFM are Hamming, Hann, and Gaussian. The other amplitude tapers are
often used for sidelobe reduction in antennas and in Doppler processors. Amplitude
weighting is not used with phase coded waveforms because the phase coding sets the sidelobe
levels. In fact, if amplitude weighting were used with a phase coded waveforms (see Chapter
10), it is likely that the compression properties of the waveform would be destroyed.
With the increasing use of digital signal processors, renewed attention is being given to the
use of phase weighting with LFM waveforms to produce nonlinear LFM waveforms [31–34].
These waveforms have a desirable property of reduced sidelobes without the attendant
weighting loss. They have the disadvantages of being difficult to generate and process.
Nonlinear LFM is discussed further in Chapter 10.
As is discussed in Chapter 13, for radars that use moving target indicator (MTI)
processors, it is common practice to use waveforms with staggered PRIs [35, 36]. That is,
waveforms with PRIs that change from pulse to pulse. The reason is that radars that use MTI
processors and constant PRIs have frequency responses that have nulls in the range of
expected target Doppler frequencies. The range rates corresponding to these nulls are termed
blind velocities.
Table 5.9
Amplitude Weighting and Associated Properties

a The parameter k controls pedestal height.
b The parameter α is inversely proportional to sidelobe level.
c The parameter  controls the extent of constant level sidelobes, specified in dB, nearest the main lobe.
With a staggered PRI waveform, the nulls are “filled in” by the stagger so that the nulls
move out of the range of expected target Doppler frequencies. With staggered PRIs, the MTI
frequency response will vary quite a bit (5–10 dB) over the range of velocities. However, for
reasonable sets of PRIs, the average response will be close to 0 dB across the frequency range
of interest. Thus, the average SNR loss across the frequency range is between 0 and 1 dB, and
most of the time is closer to 0 dB than to 1 dB. If the output of an MTI is noncoherently
integrated, the noise correlation effect of the MTI will cause an additional loss [6, 7]. Barton

indicates that this loss is approximately 1.5 and 2.5 dB for two- and three-pulse MTIs,
respectively [9, p. 384].
As with LFM waveforms, amplitude weighting is also used to reduce the sidelobes of
Doppler signal processors. In this case, the sidelobe reduction is needed in order to increase
the clutter rejection capability of the Doppler processor. This topic is discussed further in
Chapter 13. As with LFM weighting, use of amplitude weighting in Doppler processors causes
a loss in SNR (and spectral broadening) relative to the case of no weighting (rectangular
weighting in Table 5.9).
A common amplitude weighting in modern radars that use digital signal processing and
FFTs is the Chebyshev with a sidelobe level determined by the cutter rejection requirements.
However, Blackman and Blackman-Harris are also used. These amplitude weightings are
attractive because of the low sidelobe levels that can be obtained with them.
For illustration, the Doppler response of a 45-dB Chebyshev-weighted FFT processor is
presented in Figure 5.13. As discussed previously, we note Figure 5.13 indicates a 1.4 dB-
Doppler weighting loss. The Doppler filter responses are dotted, and the straddle loss, which
we discuss below, is represented by the heavy black line. The scalloped shape of the straddle
loss is why the term scalloping loss is sometimes used. A single Doppler filter centered as 10
kHz is shown by a solid line. For radar range equations purposes, we use the average of the
straddling loss (see Table 5.9).
Detection decisions in radars are made by sampling the output of the matched filter or
signal processor in range, and sometimes, in Doppler. Generally, the range samples are
spaced between ½ and 1 range resolution cell width apart and the Doppler samples are spaced
½ to 1 Doppler resolution cell apart. Because of this finite spacing, it is likely that the samples
will not occur at the peak of the range or Doppler response. The result is a loss in SNR.
Figure 5.13 Straddle (scalloping) loss—45 dB Chebyshev weighting—N = 16, Fs = 20 kHz.

Figure 5.14 Range straddle loss.
Representative curves for this loss, which is called straddle loss, are indicated in Figure
5.14. The dashed curve applies to Doppler straddle loss and to range straddle loss when the
radar uses LFM pulses. Nominal values of loss for these cases vary from about 0.3 to 1 dB for
typical sample spacings of 0.5 to 1 resolution cell. For unmodulated pulses, or pulses with
phase modulation, the loss is somewhat more severe and ranges from about 1 to 2.4 dB.
The final loss in Table 5.9 is CFAR loss. In modern radars, the detection threshold is
computed by a CFAR because this circuit or algorithm can easily adapt to different noise (and
jammer) environments. The CFAR attempts to determine the desired threshold-to-noise (TNR
—See Chapter 6) ratio based on a limited number of samples of the noise at the output of the
signal processor. Because of the limited number of samples used, the threshold will not be
precisely set relative to theory. This impreciseness is accommodated by adding a CFAR loss
to L.
The precise CFAR loss value depends upon the type of CFAR and the number of noise
samples (number of reference cells)6 used to determine the threshold. It also depends upon the
desired false alarm probability (Pfa), the detection probability (Pd) (though minimally), and
the type of target (Swerling model—0 through 5—see Chapter 3) [37]. The analysis of CFAR
loss for particular parameters can become quite involved [8, 38–43].
For preliminary designs, we choose a simpler expression that is applicable in general. One
such expression is provided by Hansen and Sawyers for the CFAR loss of a greatest of (GO)
cell averaging (CA) CFAR, given a square law detector and a Swerling 1 target [8] is
where Pfa is the desired probability of false alarm, Pd is the desired probability of detection,
and M is the number of reference cells used to form the noise estimate. Equation (5.7) can be
approximated by

where x is obtained from
It turns out that this equation is also a reasonably good approximation when considering
linear and log detectors, SO-CFAR (smallest-of CFAR) and CA-CFAR (cell-averaging
CFAR), as well as the other Swerling targets. An example of the dependency of CFAR loss
upon various parameters is shown in Figure 5.15 for a CA-CFAR.
Perhaps the simplest approximation for CFAR loss (valid for M > 16) commonly used is
provided by Nitzberg [41]
Figure 5.15 Loss for a cell averaging CFAR, Pd = 0.9.
To complete our example loss table, we will add processor and detection losses. For the L-
band search radar we assume that the radar is using LFM pulses with Hamming weighting to
reduce the range sidelobes. Since it is a search radar, we assume that the range samples are
spaced one range resolution cell apart. The radar has the ability to use MTI processing, but
for the long-range search uses only the LFM pulses (because the targets are expected to be
beyond the horizon and we are not considering rain). The radar uses a CA-CFAR with a
reference window of 18 range cells. The desired Pfa is 10-6.
The S-band radar also uses LFM with Hamming weighting. Since this radar may need to
operate in ground clutter, it uses an MTI processor with a staggered PRI waveform. Analyses

of the frequency response of the MTI indicates that the average SNR loss across the range
rates of interest is about 0.2 dB. During search, the radar spaces the range samples one range
resolution cell apart. It uses a GOCFAR designed to provide a Pfa of 10–8. The CFAR uses are
reference window of 22 cells.
The X-band radar uses phase coded waveforms and a pulsed-Doppler signal processor.
Since the radar has a stringent clutter rejection requirement, the pulsed-Doppler processor
uses 100-dB Chebyshev weighting. The radar samples in range at one range resolution cell
and in Doppler at ½ Doppler resolution cell. The radar uses a GO-CFAR with 32 reference
cells and a Pfa of 10–4. Even though the X-band radar uses Doppler processing, it performs
CFAR and detection in only the range direction. Specifically, it performs CFAR and detection
on each Doppler cell.
Table 5.10 contains the total losses with the processor and detection losses included.
Table 5.10
Total Losses for the Example
The losses introduced in this chapter are what we consider representative of those one
would use in a preliminary radar design or analysis. We did not attempt to present an
exhaustive list of losses, as that would require hundreds of pages instead of the few devoted to
this chapter. For more detailed expositions of the many loss terms that would need to be
considered in a final radar design, the reader is directed to [1, 2, 4, 6–8, 26, 35, 39–48]. A very
good reference is Barton’s 2013 text [9], which contains approximately 200 pages dedicated
to the discussion of losses. Another notable reference is Blake [28].
5.7 EXERCISES
1.
The figures of merit for window functions (with respect to a rectangular window) are
given by
Table 5.11

Window Figures of Merit—Equations [49]
Calculate the parameters in Table 5.11 for Hamming, Hann, and Gaussian windows for N
= 32. Relate these parameters to those in Table 5.9.
2.
There are two forms for a window function, referred to as symmetric and periodic.7
Appendix B lists some window functions in causal symmetric forms (identical
endpoints) is generally used for FIR filter design. Periodic forms, characterized by a
missing (implied) endpoint to accommodate periodic extension, are generally used for
spectral estimation (divide by N versus N – 1).
Pick one window function from Appendix B and plot the symmetric and periodic forms
on the same chart for N = 16. Select parameters using Table 5.9 as necessary. Generate a
separate chart showing the FFT of the periodic and symmetric forms. Zero pad as
necessary for a clear plot. What differences are evident in the frequency domain?
3.
Generate Figure 5.13 for a 32-point Gaussian weighing.
4.
The parameters in Table 5.9 are a weak function of N. For a Hamming weighting,
generate plots of SLL, SNR Loss, and peak straddle loss versus N. Let N vary from 0 to
500.
5.
Several antenna pattern models often used for analysis are listed in Table 5.12. Plot all of
the power patterns on the same figure. Let θ3 = 2.3º. How do they compare?
6.
Nitzberg uses the simple CFAR loss approximation given by (5.10). Generate Figure
5.15, and overlay Nitzberg’s approximation. How do they compare?
Table 5.12
Antenna Pattern Models

Source: [9].
References
[1]
Norton, K. A., and A. C. Omberg, “The Maximum Range of a Radar Set,” Proc. IRE, vol. 35, no. 1, Jan. 1947, pp. 4–24.
First published Feb. 1943 by U.S. Army, Office of Chief Signal Officer in the War Department, in Operational Research
Group Report, ORG-P-9-1.
[2]
Hall, W. M., “Prediction of Pulse Radar Performance,” Proc. IRE, vol. 44, no. 2, Feb. 1956, pp. 224–231. Reprinted:
Barton, D. K., Radars, Vol. 2: The Radar Range Equation, Norwood, MA: Artech House, 1974, pp. 31–38.
[3]
Blake, L. V., “Recent Advancements in Basic Radar Range Calculation Technique,” IRE Trans. Mil. Electron., vol. 5, no.
2, Apr. 1961, pp. 154–164.
[4]
Blake, L. V., “Curves of Atmospheric-Absorption Loss for Use in Radar Range Calculation,” Naval Research
Laboratory, Washington, D.C., Rep. No. 5601, Mar. 23, 1961. Available from DTIC as AD255135.
[5]
Hall, W. M., and D. K. Barton, “Antenna Pattern Loss Factor for Scanning Radars,” Proc. IEEE, vol. 53, no. 9, Sept.
1965, pp. 1257–1258.
[6]
Hall, W. M., and H. R. Ward, “Signal-to-Noise Loss in Moving Target Indicator,” Proc. IEEE, vol. 56, no. 2, Feb. 1968,
pp. 233–234.
[7]
Trunk, G. V., “MTI Noise Integration Loss,” Proc. IEEE, vol. 65, no. 11, Nov. 1977, pp. 1620– 1621.
[8]
Hansen, V. G., and J. H. Sawyers, “Detectability Loss Due to ‘Greatest Of’ Selection in a Cell-Averaging CFAR,” IEEE
Trans. Aerosp. Electron. Syst., vol. 16, no. 1, Jan. 1980, pp. 115–118.
[9]
Barton, D. K., Radar Equations for Modern Radar, Norwood, MA: Artech House, 2013.
[10] Blake, L. V., “A Guide to Basic Pulse-Radar Maximum-Range Calculation, Part 1—Equations, Definitions, and Aids to
Calculation,” Naval Research Laboratory, Washington, D.C., Rep. No. 6930, Dec. 23, 1969, p. 49. Available from
DTIC as 701321.
[11] Blake, L. V., “A Guide to Basic Pulse-Radar Maximum-Range Calculation, Part 2—Derivations of Equations, Bases of
Graphs, and Additional Explanations,” Naval Research Laboratory, Washington, D.C., Rep. No. 7010, Dec. 31, 1969.
Available from DTIC as 703211.
[12] Mini-Circuits, RF/IF Designer’s Handbook, Brooklyn, NY: Mini-Circuits, 1992.
[13] Watkins-Johnson Co., RF and Microwave Component Designer’s Handbook, Palo Alto, CA: Watkins-Johnson Co.,
1988.
[14] Losee, F. A., RF Systems, Components, and Circuits Handbook, 2nd ed., Norwood, MA: Artech House, 2005.
[15] Saad, T. S., R. C. Hansen, and G. J. Wheeler, eds., Microwave Engineers’ Handbook, vol. 1, Dedham, MA: Artech

House, 1971.
[16] Saad, T. S., R. C. Hansen, and G. J. Wheeler, eds., Microwave Engineers’ Handbook, vol. 2, Dedham, MA: Artech
House, 1971.
[17] Kahrilas, P. J., Electronic Scanning Radar Systems (ESRS) Design Handbook, Norwood, MA: Artech House, 1976.
[18] IEEE Standard Dictionary of Electrical and Electronic Terms, 6th ed., New York: IEEE, 1996.
[19] Gardiol, F. E., Introduction to Microwaves, Dedham, MA: Artech House, 1984.
[20] Pozar, D. M., Microwave Engineering, New York: Addison-Wesley, 1990.
[21] Agrawal, A. K., and E. L. Holzman, “Beamformer Architectures for Active Phased-Array Radar Antennas,” IEEE Trans.
Antennas Propag., vol. 47, no. 3, Mar. 1999, pp. 432–442.
[22] Mailloux, R. J., Phased Array Antenna Handbook, 2nd ed., Norwood, MA: Artech House, 2005.
[23] Hansen, R. C., ed., Microwave Scanning Antennas, Vol. III: Array Systems, New York: Academic Press, 1966.
[24] Brookner, E., “Right Way to Calculate Reflector and Active-Phased-Array Antenna System Noise Temperature Taking
into Account Antenna Mismatch,” IEEE Int. Symp. Phased Array Syst. and Technology 2003, Boston, MA, Oct. 14–17,
2003, pp. 130–135.
[25] Blake, L. V., “The Effective Number of Pulses Per Beamwidth for a Scanning Radar,” Proc. IRE, vol. 41, no. 6, Jun.
1953, pp. 770–774.
[26] Hall, W. M., “Antenna Beam-Shape Factor in Scanning Radars,” IEEE Trans. Aerosp. Electron. Syst., vol. 4, no. 3, May
1968, pp. 402–409.
[27] United States Committee on Extension to the Standard Atmosphere (COESA), U.S. Standard Atmosphere, 1976,
Washington, D.C.: U.S. Government Printing Office, Oct. 1976.
[28] Blake, L. V., Radar Range-Performance Analysis, Norwood, MA: Artech House, 1986.
[29] Humphreys, W. J., Physics of the Air, 3rd ed., New York: McGraw-Hill, 1940.
[30] Vizmuller, P., RF Design Guide: Systems, Circuits and Equations, Norwood, MA: Artech House, 1995.
[31] Cook, C. E., and M. Bernfeld, Radar Signals: An Introduction to Theory and Application, New York: Academic Press,
1967. Reprinted: Norwood, MA: Artech House, 1993.
[32] Lewis, B. L., et al., Aspects of Radar Signal Processing, Norwood, MA: Artech House, 1986.
[33] Boukeffa, S., Y. Jiang, and T. Jiang, “Sidelobe Reduction with Nonlinear Frequency Modulated Waveforms,” in Proc.
IEEE 7th Int. Colloquium on Signal Proc. and Its Applications (CSPA ’11), Penang, Malaysia, Mar. 2011, pp. 399–403.
[34] Barton, D. K., ed., Radars, Vol. 3: Pulse Compression (Artech Radar Library), Dedham, MA: Artech House, 1975.
[35] Schleher, D. C., MTI and Pulsed Doppler Radar with MATLAB, 2nd ed., Norwood, MA: Artech House, 2010.
[36] Barton, D. K., Radar System Analysis, Englewood Cliffs, NJ: Prentice-Hall, 1964; Dedham, MA: Artech House, 1976.
[37] Minkler, G., and J. Minkler, CFAR, Baltimore, MD: Magellan, 1990.
[38] Mitchell, R. L., and J. F. Walker, “Recursive Methods for Computing Detection Probabilities,” IEEE Trans. Aerosp.
Electron. Syst., vol. 7, no. 4, Jul. 1971, pp. 671–676.
[39] Gregers-Hansen, V., “Constant False Alarm Rate Processing in Search Radars,” Radar— Present and Future, IEE Conf.
Publ. No. 105, London, Oct. 1973, pp. 325–332.
[40] Hansen, V. G., and H. R. Ward, “Detection Performance of the Cell Averaging LOG/CFAR Receiver,” IEEE Trans.
Aerosp. Electron. Syst., vol. 8, no. 5, Sept. 1972, pp. 648–652.
[41] Nitzberg, R., “Analysis of the Arithmetic Mean CFAR Normalizer for Fluctuating Targets,” IEEE Trans. Aerosp.
Electron. Syst., vol. 14, no. 1, Jan. 1978, pp. 44–47.
[42] Weiss, M., “Analysis of Some Modified Cell-Averaging CFAR Processors in Multiple-Target Situations,” IEEE Trans.
Aerosp. Electron. Syst., vol. 18, no. 1, 1982, pp. 102–114.
[43] Nathanson, F. E., J. P. Reilly, and M. N. Cohen, eds, Radar Design Principles, 2nd ed., New York: McGraw-Hill, 1991.
[44] Barton, D. K., Radar System Analysis and Modeling, Norwood, MA: Artech House, 2005.
[45] Naval Air Warfare Center Weapons Division (NAWCWD), Electronic Warfare and Radar Systems Engineering
Handbook, 4th ed., NAWCWD Technical Communications Office, Point Mugu, CA, Rep. No. NAWCWD TP 8347, Oct.
2013.

[46] Meikle, H., Modern Radar Systems, 2nd ed., Norwood, MA: Artech House, 2008.
[47] Skolnik, M. I., Introduction to Radar Systems, 3rd ed., New York: McGraw-Hill, 2001.
[48] Skolnik, M. I., ed., Radar Handbook, 3rd ed., New York: McGraw-Hill, 2008.
[49] Harris, F., “On the Use of Windows for Harmonic Analysis with the Discrete Fourier Transform,” Proc. IEEE, vol. 66, no.
1, Jan. 1978, pp. 51–83.
[50] Southworth, G. C., “Hyper-Frequency Wave Guides—General Considerations and Experimental Results,” Bell Syst. Tech.
J., vol. 15, no. 2, Apr. 1936, pp. 284–309.
[51] Barrow, W. L., “Transmission of Electromagnetic Waves in Hollow Tubes of Metal,” Proc. IRE, vol. 24, Oct. 1936, pp.
1298–1328.
[52] Tyrrell, W. A., “Hybrid Circuits for Microwaves,” Proc. IRE, vol. 35, no. 11, Nov. 1947, pp. 1294– 1306.
[53] Montgomery, C. G., R. H. Dicke, and E. M. Purcell, eds., Principles of Microwave Circuits, vol. 8, New York:
McGraw-Hill, 1948; Norwood, MA: Artech House (CD-ROM edition), 1999.
[54] Gardiol, F. E., Introduction to Microwaves, Dedham, MA: Artech House, 1984.
[55] Pozar, D. M., Microwave Engineering, 4th ed., New York: Wiley & Sons, 2011.
[56] Martin, W.H., “Decibel—The Name for the Transmission Unit,” Bell Syst. Tech. J., vol. 8, no.1, Jan. 1929, pp.1–2.
[57] Blake, L. V., “Ray Height Computation for a Continuous Nonlinear Atmospheric Refractive-Index Profile,” Radio
Science, vol. 3, no. 1, Jan. 1968, pp. 85–92.
[58] Bean, B. R., and G. D. Thayer, “Models of the Atmospheric Radio Refractive Index,” Proc. IRE, vol. 47, no. 5, May
1959, pp. 740–755.
[59] Minzner, R.A., W.S. Ripley, and T. P. Condron, “U.S. Extension to the ICAO Standard Atmosphere,” U.S. Dept. of
Commerce Weather Bureau and USAF ARDC Cambridge Research Center, Geophysics Research Directorate, U.S.
Government Printing Office, Washington, D.C., 1958
[60] Sissenwine, N. D., D. Grantham, and H. A. Salmela, “Humidity Up to the Mesopause,” USAF Cambridge Res. Labs.,
Bedford, MA, Rep. No. AFCRL-68-0050, Oct. 1968.
[61] Van Vleck, J. H., “The Absoption of Microwaves by Oxygen,” Phys. Rev., vol. 71, no. 7, Apr. 1947, pp. 413–424.
[62] Van Vleck, J. H., “The Absorption of Microwaves by Uncondensed Water Vapor,” Phys. Rev., vol. 71, no. 7, Apr. 1947,
pp. 425–433.
[63] Leibe, H. J., “Calculated Tropospheric Dispersion and Absorption Due to the 22-GHz Water Vapor Line,” IEEE Trans.
Antennas Propag., vol. 17, no. 5, Sept. 1969, pp. 621–627.
[64] Meeks, M. L., and A. E. Lilley, “The Microwave Spectrum of Oxygen in the Earth’s Atmosphere,” J. Geophys. Res., vol.
68, no. 6, Mar. 15, 1963, pp. 1683–1703.
[65] Reber, E. E., R. L. Mitchel, and C. J. Carter, “Attenuation of the 5-mm Wavelength Band in a Variable Atmosphere,”
IEEE Trans. Antennas Propag., vol. 18, no. 4, Jul. 1970, pp. 472–479.
[66] Van Vleck, J. H., and V. F. Weisskopf, “On the Shape of Collision-Broadened Lines,” Rev. Mod. Phys., vol. 17, nos. 2 &
3, Apr. 1945, pp. 227–236.
[67] Ulaby, F. T., and A. W. Straiton, “Atmospheric Absorption of Radio Waves Between 150 and 350 GHz,” IEEE Trans.
Antennas Propag., vol. 18, no. 4, Jul. 1970, pp. 479–485.
[68] Bauer, J. R., W. C. Mason, and F. A. Wilson, “Radio Refraction in a Cool Exponential Atmosphere,” MIT Lincoln Lab.,
Cambridge, MA, Tech. Rep. No. 186, Aug. 27, 1958.
APPENDIX 5A: WAVEGUIDE ATTENUATION
Waveguide, invented by Bell Telephone Laboratories [50] and studied in parallel at MIT [51]
in the 1930s, is one RF transmission line commonly used in radar because of its low loss and
high power handling capability. This is especially applicable for the types of transmitters
depicted in Figure 5.1, where all of the transmit power travels through a single RF path. The
waveguide’s dimensions (square, rectangular, circular) determine the operating frequency

range and the material (gold, silver, copper, aluminum, brass) affects the loss.
Rectangular waveguide is frequently used in radar.8 For illustration, Figure 5A.1 shows a
magic T (or tee) constructed with WR-90 waveguide. The magic T is a four-port, 180°, 3-dB
hybrid developed during World War II [52] and is used as both a power combiner and power
divider, depending upon the ports used and is very low loss [53–55]. Copper and copper alloy
are standard waveguide materials (solid or plating).9
Table 5A.1 contains the Electronic Industries Association (EIA) waveguide (WG)
designations,10 inner dimensions, frequency range, and theoretical attenuation for a number
of standard rectangular copper waveguides [15]. For the EIA designation, the WR number is
the internal dimension in inches of the broad wall.Figure 5.2 contains plots of the theoretical
waveguide loss versus frequency for several of the waveguides in Table 5A.1. We note that
loss is inversely proportional to frequency. Also, the frequency boundaries are not coincident
with radar designators. The general rule of thumb used to decide between multiple waveguide
possibilities is to select the larger waveguide, which has lower loss.
Figure 5A.1 Waveguide magic T (WR-90).
Table 5A.1
Standard Rectangular Waveguide Specifications (Copper)

Figure 5A.2 Rectangular waveguide.
As a side note, waveguide is often pressurized, typically using dry air, nitrogen, or argon
to prevent moisture buildup inside the waveguide, which can cause corrosion of the
conducting surfaces, thus increasing loss.11 In addition to using dry gas as dielectric, the
slight overpressure help to keep out moisture in the event of small leaks. Microwave
transparent windows are used to prevent pressure loss where the waveguide would be open
(e.g., feed horn).
For example, consider a copper-plated, pressurized, rectangular WR-90 waveguide,
depicted in Figure 5A.2, operating at 10 GHz (X-band) filled with nitrogen. For the dominant
mode,12 we want to determine the cutoff frequency in GHz and the attenuation due to
conductor loss in dB/m.
From Table 5A.1, we see that WR-90 waveguide has interior dimensions of the broad and
short walls of a = 2.286 cm (0.90 in) and b = 1.016 cm (0.4 in), respectively. Since the wall
length ratio is ~ 2:1, the dominate mode of propagation is the TE10 mode. The cutoff

frequency for the mn mode is given by [55, p.113]
From Table 5A.1, we see that WR-90 waveguide has interior dimensions of the broad and
short walls of a = 2.286 cm (0.90 in) and b = 1.016 cm (0.4 in), respectively. Since the wall
length ratio is ~ 2:1, the dominate mode of propagation is the TE10 mode. The cutoff
frequency for the mn mode is given by [55, p.113]
where
is the cutoff wave number. To clarify cutoff frequency as used here, for frequencies above
the cutoff frequency for a given mode, the electromagnetic energy can be transmitted through
the guide for that particular mode with minimal attenuation (which is backwards compared to
lowpass filter cutoff terminology).
For typical gaseous dielectrics used to fill waveguides (air, nitrogen, argon), the
permittivity and permeability are essentially identical to those of free space (vacuum). Recall
the permittivity of free space is μ0 = 400 π ≈ 1256.637061 nH/m and the permeability of free
space is ε0 = 1/μ0c2 ≈ 8.8541878176 pF/m. For the TE10 mode, (5A.1) and (5A.2) simplify to
and
The upper bound on propagation is the TE20 mode waveguide cutoff frequency calculated
using (5A.1). Recall that above the cutoff frequency for a given mode, the electromagnetic
energy will propagate through the guide for that particular mode with minimal attenuation.

Therefore, TE10 mode will propagate at frequencies above 6.56 GHz and below 13.11 GHz.
Digressing for a moment, we note that 6.56 GHz to 13.11 GHz does not match the operating
frequency range given in Table 5A.1. We illustrate the rationale for this discrepancy by
comparing the waveguide loss for both the theoretical and recommended frequency ranges,
presented in Figure 5A.3. While the TE10 mode will technically propagate with up to ~3-dB
loss, the amount of loss considered acceptable for waveguide is much lower.
Figure 5A.3 Theoretical loss for copper-plated WR-90 waveguide over theoretical and recommended operating ranges.
Returning to our loss example, the attenuation due to conductor loss (loss due to the metal
of the waveguide)13 is given by [55, p. 115]
where we recall that nepers14 (Np), defined in the same Bell Labs paper as dB [56], is a unit
based upon the natural logarithm, and is given, for voltage, by [55, p. 63]

and for power by
The propagation constant, β, for the TE10 mode is given by [55, p. 112]
where
is the free space wave number. The intrinsic impedance of the dielectric is
The last component of (5A.6) is the surface resistivity of the metal in the waveguide, given by
[55, p. 28]
The conductivity for copper and other common waveguide materials is listed in Table 5A.2.
For this example, copper is specified, which has a conductivity of 5.813×107 mho/m [54, p.
458]. This results in a surface resistance of Rs = 0.0261Ω. Substitution into (5A.6) gives
Table 5A.2
Material Conductivity
Material
Conductivity (mho/m)

Aluminum
3.816 · 107
Brass
2.564 · 107
Copper
5.813 · 107
Gold
4.098 · 107
Silver
6.173 · 107
Source: [54].
Converting this to dB/m gives
where nepers are related to dB by [55, p. 63]
For completeness, we convert the answer to dB/100 ft, since historically, many waveguide
tables are presented in dB/100 ft.
5A.1 EXERCISES
1.
Typical waveguide plating materials are aluminum, brass, copper, gold, and silver (or
alloys thereof). For these materials, calculate the theoretical loss across the recommend
operating frequency for WR-90. Assume the waveguide is filled with pressurized dry
nitrogen. Generate a comparison plot similar to Figure 5.2. For reference, use Table
5A.2.
2.
Generate Figure 5.2 for a silver-plated waveguide. Also plot the approximation given by
(5.1). How does the approximation compare?
3.
Consider an air-filled rectangular waveguide operating at 2 GHz. Select an appropriate
waveguide size. What are the interior dimensions? Calculate the upper and lower
frequency bounds for propagation. Recall that for a wall length ratio of ~2:1, the
dominate mode of propagation is the TE10 mode. For reference, recall the permittivity
of free space is μ0 = 400 π ≈ 1256.637061 nH/m and the permeability of free space is ε0
= 1/μ0c2 ≈ 8.8541878176.
APPENDIX 5B: ATMOSPHERIC AND RAIN ATTENUATION
For reference, the equations and data outlined below are used to generate Figures 5.8 through

5.10 (two-way atmospheric loss) and Figure 5.11 (rain attenuation). The equations
summarized in this appendix are coded in the MATLAB functions listed in Table 5B.1 and
included on the CD. Note: the equations are presented in the order of execution in their
associated function (e.g., terms are calculated for use in functions defined subsequently). For a
complete explanation of the origins and theory for atmospheric absorption, please refer to [4,
9, 28, 57, 58].
Table 5B.1
Atmospheric and Rain Attenuation Function Summary
Function Description
Resulting
Figure
[R, L, Lox, Lwv] = troploss(f, ang):
This function computes the accumulated two-way tropospheric absorption loss (in dB) for an RF signal with
frequency f along a refracted path that originates at the earth’s surface and has an elevation angle ang. It returns
the loss for oxygen, water vapor, total loss and the associated ranges.
Figure
5.8, 
5.9,
5.10
[Re, h, phi] = troprefract(ang):
This function computes refracted RF path through the troposphere for the elevation angle ang. It returns range,
height, and the angular position PHI of the refracted path.
Called by
troploss.m
[g, gox, gwv] = tropatten(f, h):
Given frequency f and altitude h, this function computes tropospheric absorption coefficient versus frequency and
altitude. It returns the tropospheric absorption coefficients, g, (in dB/km) as well as the component absorption
coefficients for oxygen, gox, and water vapor, gwv.
Called by
troploss.m
[K] = rainAttn2way(f, rr):
Given frequency, f, in GHz and rain rate, rr, in mm/hr, this function uses the standard model for rain attenuation to
compute two-way loss (dB/km) as a function of operating frequency and rain rate [1, p. 215; 2, p. 246].
Figure
5.11
5B.1 FUNCTION TROPATTEN.M
5B.1.1 Compute International Civil Aviation Organization (ICAO) Standard Atmosphere
196415
In determining atmospheric attenuation, knowledge of the atmosphere’s pressure,
temperature, and water vapor density, all of which varies with altititude, is necessary. Given
the varying nature of the atmosphere due to such factors as location, time of day, or season, a
standard model is used [27, 59]. The standard atmosphere model (based upon experimental
data) provides a defined variation of mean values of temperature, pressure, and water vapor
density as a function of altitude. Specifically, temperature and pressure are modeled using an
empirical equation, while water vapor density is determined via table lookup.
As the first step in computing the tropospheric absorption coefficient versus frequency and
altitude, the function tropatten.m first calculates the geopotential altitude (based upon the
assumption of constant gravity at all altitude), hg, which is related to the geometric altitude
(referenced to mean sea level), ha, by [28, p. 206; 59]16

where r0 = 6,371 km is the radius of the earth.
Using the results of (5B.1), the atmosphere temperature and pressure as a function of
geopotential altitude are determined by [28, p. 205; 59]
which are equations describing the absolute atmospheric temperature, T (degrees kelvins),
and total atmospheric pressure, p (millibars), for the standard atmospheric model [59]. The
coefficient values are: α = 5.2561222, β = 0.034164794, and γ = 11.388265. The water vapor
density for the U.S. standard atmosphere is provided in Table 5B.2 [28, p. 207].
The values in Table 5B.2 are the mid-latitude mean water vapor densities for a surface value
(h = 0) of 5.947 g/m3 [60]. However, the current standard is to use a surface value of water
vapor density of 7.5 g/m3. As such, we translate the vapor density values in Table 5B.2 such
that the surface water vapor density is 7.5 g/m3 [28, p. 206] using
To determine water vapor, interpolate as necessary into Table 5B.2.
Table 5B.2
Mid-Latitude Mean Water Vapor Densities

Source: [28].
5B.1.2 Absorption Coefficient for Oxygen
According to Blake [28], the original theory for determining the absorption coefficient for
oxygen was presented by Van Vleck [61–63], with further refinements being made later on
[64, 65]. To determine absorption, we take the summation of the contributions of several
oxygen resonance lines, each of which has two resonant frequencies. These resonant
frequencies are listed in Table 5B.3 [28, p. 201; 9, p. 233; 64].17
Table 5B.3
Oxygen Resonance Frequencies
Source: [28].
Using the parameters in Table 5B.3, we calculate the following values [28, p. 200]

and
where p0 = 1013.25 mbar (760 torr) is the pressure at sea level, and T0 = 360 K18 and [28, 65]
The values calculated above are components of [28, p. 200; 66]
This is the Van Vleck-Weisskopf formula, which provides the shapes of the resonance lines
[28]. The nonresonant contribution is of the form [28, p. 200]
Next, compute the terms given by [28, p. 200]
Finally, the complete expression for absorption coefficient due to oxygen is given by the
summation [28, p. 201]

where f is frequency, p is atmospheric pressure, T is absolute temperature, and C = 2.0058 for
γ in decibels per kilometer [28].
5B.1.3 Absorption Coefficient for Water Vapor
There are two primary components to water vapor absorption (below 100 GHz). First, handle
water due to vapor resonance at 22.235 GHz. To do this, start by computing the water vapor
partial pressure (in Torr), which is a function of water vapor density and temperature. The
partial pressure of water is given by [28, p. 203]
where ρ is water vapor density and T is temperature. Recall that 1 mbar ~ 0.75 Torr.
Converting to Torr we then use [28, p. 203]
Next, we use the equation provided by Liebe for ∆f [28, p. 203; 65]
Similarly to what was done for oxygen (except for the additional factor f / fr), we now use
the Van Vleck-Weisskopf formula again to determine F [28, p. 203; 66]
where fr = 22.235 GHz. Finally, using the terms determined above, calculate the absorption
coefficient due to vapor resonance at 22.235 GHz using [28, p. 203]
Second, compute the residual effect of water vapor absorption lines above 100 GHz, using the
simpler expression [28, p. 204; 67]

Now finish off the water vapor result, which is given by
For the total absorption, we sum the oxygen and the water vapor absorption, using
5B.2 FUNCTION TROPREFRACT.M
When RF waves travel through the atmosphere, their path is bent, or refracted. This is because
the atmosphere is a stratified medium whose refractive index varies with altitude. To
determine atmospheric absorption properly, which is a function of distance, we must calculate
the actual path traveled versus the straight line path.
For our calculations, we use the exponential model of refractive index [28, p. 182]. More
specifically, Table 5B.4 provides the parameters that define the Central Radio Propagation
Laboratory (CRPL)19 exponential reference atmosphere [28, 58]. The values associated with
index k = 5 (Ns = 313, ce = 0.1439) are representative of the average values over the United
States [28] and will be used for our calulations.
Table 5B.4
Value of Parameters of CRPL Exponential Reference Atmosphere
Ns
ce (per km)
hs (ft)
200.0
0.118400
10,000
250.0
0.125625
—
252.9
0.126255
5,000
301.0
0.139632
1,000
313.0
0.143859
700
344.5
0.156805
0
350.0
0.159336
0
377.2
0.173233
0
400.0
0.186720
0
404.9
0.189829
0
450.0
0.223256
0
The values in Table 5B.4 are ce, which is a constant related to refractive index gradient (per
km), hs, which corresponds to altitude above sea level in ft and Ns, the surface refractivity in
ppm [28, p. 183].

First, convert to hs to km
Next, convert surface refractivity to n0, which is the exponential refractive index at the
Earth’s surface (h = 0). To do this, use the relationship between Ns and n0 of [28, p. 183]
or
Using (5B.26) and the parameters from Table 5B.4, the exponential model for refractive
index as function of altitude is [28, p. 182; 58, 68]
Finally, to determine the total distance over the refracted path, we compute the ray tracing
integral given by [28, p. 182; 9, p. 232; 57]
The integral (5B.28) provides the distance used to determine atmospheric loss, as opposed
to slant range.
5B.3 FUNCTION TROPLOSS.M
Using the above functions, we are ready to compute atmospheric loss as follows:
• First do the ray tracing to determine the refracted RF path
◦Call [R, h] = troprefract(angle, M)
• Now compute the absorption coefficients versus h at the desired frequencies
◦Call [γ, γox, γwv] = tropatten(f, h)
• The total atmospheric loss is then determined by taking the integral over the RF path [28,
p. 199]

As indicated by (5B.29) the two components of atmospheric loss are the atmospheric loss
due to oxygen, Lox, and the atmospheric loss due to water vapor, Lwv.
5B.4 FUNCTION RAINATTN2WAY.M
The standard model for rain attenuation coefficient kαr in dB/km given a rainfall rate rr in
mm/h takes the form [19, p. 246; 28, p. 215]
The terms a and b are a multiplicative factor and an exponent, respectively, both of which are
dependent upon frequency.
Barton provides the following empirical expression for a and b (an updated version of the
expression presented by Blake [28, p. 217]) that applies for a temperature of ~ 291 K [9, p.
246]:
where [9, p. 246]
and C0 = 3.1 × 10–5 is a frequency parameter. The following are break frequencies
(determined empirically to match various theoretical computations published by a number of
authors20) in GHz:

and
1 Barton notes that (5.1) matches the line losses plotted in Saad and Hansen, Microwave Engineers′ Handbook, Artech House,
1971 [15, 16]. Equation (5.1) appears in the file titled “10-1 Loss Factors” included on the accompanying DVD of [9].
2 When the scan loss from (5.6) results from β > 1.0, it includes the effect of the mismatch loss expressed by Figure 5.5. The
mismatch remains important because it may cause an increase in the noise temperature of a phased array antenna [24].
3 Synonym: antenna-pattern loss.
4 Example calculations of several suboptimal match filters are provided in Appendix A.
5 Metrics generated using N=1k, zero padded to 64k because of weak dependency on N.
6 While the exact number of reference cells depends upon the application and dimensionality of the CFAR, from ~10 to ~40
cells is fairly typical. There is a tradeoff between more reference cells lowering CFAR loss but resulting in slower threshold
transitions at clutter boundaries. One rule of thumb is to use enough reference cells for ~ 1-dB CFAR loss.
7 For some windows (but not all), MATLAB® can generate either symmetric (default) or periodic windows, via a flag in the
particular window function, with symmetric being the default. The spectral difference is minimal, decreasing as N increases.
8 The rule of thumb for standard rectangular waveguide design is to use a 2:1 wall length ratio, which ensures that only the
TE10 mode (dominant mode in rectangular waveguide) will propagate. In practice, wall ratios vary slightly, ~ 2:1 to 2:2.
9 Copper waveguide provides good heat dissipation. Additional heat transfer can be achieved by brazing coolant lines directly
onto the waveguide.
10 Joint Army Navy (JAN) designators exist as well.
11 Interestingly, extreme over pressurization, perhaps from a pressure regulator failure, will turn rectangular waveguide into
round waveguide.
12 The dominant mode is the mode with the lowest cutoff frequency, which for a rectangular waveguide (a > b) is the TE10
mode.
13 The attenuation due to dielectric loss (the material filling the waveguide) is negligible for typical gaseous dielectrics.
14 Derived from the name of John Napier, who invented the natural logarithm [56].
15 The current standard atmosphere is the 1976 version, but below 32,000 km, the altitudes of interest for most ground-based
radars, the models are equivalent [28, p. 227].
16 The difference between geopotential and geometric altitude is very small for altitudes less than 30 km, but most standard
atmospheric tables quote geopotential altitude.
17 N is comprised of odd integers because it is the quantum rotational number. Also, for values of N greater than 45, the
absorption contribution is negligible [28, p. 200].
18 P0, T0, and ρ0 define the standard atmosphere [9].
19 CPRL is now the National Oceanic and Atmospheric Agency (NOAA).
20 Blake acknowledges Wayne Rivers, a senior scientist at Technology Service Corporation, for coming up with the original
expressions for (5B.31) through (5B.34) [28, p. 215].

Chapter 6
Detection Theory
6.1 INTRODUCTION
In the radar range equation exercises of Chapter 2, we considered an example of computing
detection range based on SNRs of 13 and 20 dB. We now want to develop some theory
explaining the use of these particular SNR values. More specifically, we will examine the
concept of detection probability, Pd. Our need to study detection from a probabilistic
perspective stems from our dealings with signals that are noise-like. From our studies of
RCS, we found that, in practice, the signal return looks random. In fact, Peter Swerling has
convinced us to use statistical models to represent target signals [1]. In addition to these target
signals, we found that the signals in the radar contain a noise component, which also needs to
be dealt with using the concepts of random variables, random processes, and probabilities.
The early work in detection theory, as applies to radar, was published by Stephen Oswald
Rice in the Bell System Technical Journal [2]. Rice considered the problem of detecting a
constant amplitude signal in the presence of noise, based on a single sample of the signal plus
noise. A SW0/SW5 target (see Chapter 3) produces such a signal. In his 1947 paper, J. I.
Marcum extended Rice’s work to the case of detection after the integration of a number of
signal-plus-noise samples [3, 4]. In 1954, Swerling introduced his concepts of noise-like
signals caused by a target with a fluctuating RCS [1]. He developed equations for determining
detection probability for single and multiple sample cases. Since then, other authors have
extended Swerling’s work to other target fluctuation models [5–10]. However, the standards
are still the Rice model and the Swerling models.
In this chapter, we will be concerned with detection based on returns from a single pulse. In
Chapter 8, we will extend the results to the case where detection is based on returns from
several pulses. Since we are considering a single pulse, the detection equations we develop
are termed single pulse, single sample, or single hit detection probabilities. We will develop
detection equations for the five target RCS types discussed in Chapter 3: SW0/SW5, SW1,
SW2, SW3, and SW4. We will also derive the “detection” equation for noise, which we term
false alarm probability.
Table 6.1
Single Pulse Detection Probability Equations for SW0 through SW5 Targets

The various signal models and probability derivations presented in this chapter are not new.
As indicated above, they have been carried out by Rice and Swerling, and many others [11].
We include them in this book because we feel it is very important to understand the origin of
the detection and false alarm probability equations, along with the limitations on when and
where they can be applied. For those readers who are interested only in the final results, Table
6.1 contains a summary of the detection and false alarm probability equations derived in this
chapter, along with the noise and signal-plus-noise density function equations upon which
probability equations are based.
In the table,
• Pd is the single pulse detection probability.
• SNR is the single pulse SNR (see (2.1), Chapter 2).
• Pfa is the probability of false alarm.
• Q1 is the Marcum Q-function.
• TNR is the threshold-to-noise ratio.
• I0 is the modified Bessel function of the first kind, order zero.

•
 is the amplitude of the signal return for a SW0/SW5 target.
• PS is the signal power from the radar range equation (see Chapter 2).
• σ2 is the noise power at the output of the matched filter.
• U(x) is the unit step function.
These parameters are defined more fully in the discussions that follow.
To develop the requisite detection probability equations, we need to develop a mathematical
characterization of the target signal, the noise signal, and the target-plus-noise signal at
various points in the radar. We start with a characterization of noise and then progress to the
target and target-plus-noise signals.
6.2 NOISE IN RECEIVERS
We characterize noise for the two most common types of receiver implementations. The first
is illustrated in Figure 6.1 and is termed the IF representation [12]. In this representation, the
matched filter is implemented at some intermediate frequency, or IF. The second receiver
configuration is illustrated in Figure 6.2 and is termed the baseband representation [12]. In
this configuration, the signal is converted to a baseband signal, a complex signal centered at a
frequency of zero, instead of ωIF. The IF configuration is common in older radars, and the
baseband representation is common in modern radars, especially those using digital signal
processing.
Both the IF and baseband representations contain a matched filter, which serves as the
signal processor for the case where the radar bases detection decisions on a single pulse. As
we will see in Chapter 7, the matched filter is a necessary component because it maximizes
SNR, which is a requirement for maximizing detection probability.
Figure 6.1 IF receiver representation.
Figure 6.2 Baseband receiver representation.

6.2.1 IF Configuration
In the IF configuration, we represent the noise by
where nIF (t), N(t), and φ(t) are random processes. Expanding (6.1) using trigonometric
identities gives
where nI(t) and nQ(t) are also random processes. In (6.2), we stipulate nI(t) and nQ(t) as joint,
wide-sense stationary (WSS), zero-mean, equal variance, Gaussian random processes. They
are also such that the random variables nI = nI(t)|t=t1 and nQ = nQ(t)|t=t1 are independent. The
variance of nI(t) and nQ(t) is σ 2. Under these conditions, the density functions of nI(t) and
nq(t) are given by
We now show that N(t) is Rayleigh and φ(t) is uniform on (–π,π ]. We will further show that
the random variables N = N(t)|t=t1 and φ = φ(t)|t=t1 are independent.
From random variable theory [13], if x and y are real random variables,
and
where tan−1(y/x) denotes the four-quadrant arctangent, then the joint density of r and φ can be
written in terms of the joint density of x and y as
where, as a reminder, rect[x] is1

U[x] is the unit step function,2 which is defined as
In our case, x = nI, y = nQ, r = N, and φ = φ. Thus, we have
and
Since nI and nQ are independent
Using this result in (6.11) with nI = Ncos(ϕ) and nQ = Nsin(ϕ), we get
From random variable theory, we can find the marginal density from the joint density by
integrating with respect to the variable we want to eliminate. Thus,

and
This proves the assertion that N(t) is Rayleigh and φ(t) is uniform on (–π,π ]. To prove the
random variables N = N(t)|t=t1 and φ = φ (t)|t=t1 are independent, we note from (6.13), (6.14),
and (6.15) that
which means N and φ are independent.
Since we will need it later, we want to find an equation for the noise power out of the
matched filter, and into the detection logic. Since nIF(t) is WSS, we use (6.2) to write
In (6.17), the term on the third line is zero because nI = nI(t)|t=t1 and nQ =nQ(t)|t=t1 are
independent and zero-mean.
6.2.2 Baseband Configuration
In the baseband configuration of Figure 6.2, we represent the noise into the detection logic as
a complex random process of the form
where nI(t) and nQ(t) are joint, WSS, zero-mean, equal variance, Gaussian random processes.
They are also such that the random variables nI = nI (t)|t=t1 and nQ = nQ(t)|t=t1 are independent.
The variance of nI(t) and nQ(t) is σ2. The constant of 
 is included to provide consistency
between the noise powers in the baseband and IF receiver configurations. The power in nB(t)
[making use of the properties of nI(t) and nQ(t)] is given by

We write nB(t) in polar form as
where
and
We note that the definitions of nI(t), nQ(t), N(t), and φ(t) are consistent between the IF and
baseband representations. This means the two representations are equivalent in terms of the
statistical properties of the noise. We will reach the same conclusion for the signal. As a
result, the detection and false alarm performances of both types of receiver configurations
are the same. Thus, the detection and false alarm probability equations we derive in the future
will apply to either receiver configuration.
If the receiver being analyzed is not of one of the two forms indicated above, the detection
and false alarm probability equations derived herein may not apply. A particular example is
the case where the receiver uses only the I or Q channel in baseband processing. While this is
not a common receiver configuration, it is sometimes used. In this case, one would need to
derive a different set of detection and false alarm probability equations specifically applicable
to the configuration.
6.3 SIGNAL IN RECEIVERS
6.3.1 Introduction and Background

We now turn our attention to developing a representation of the signals at the input to the
detection logic. Consistent with the noise case, we consider both IF and baseband receiver
configurations. Thus, we will use Figures 6.1 and 6.2, but replace n(t) with s(t), N(t) with S(t),
φ(t) with θ(t), nI(t) with sI(t), and nQ(t) with sQ(t).
We will develop three signal representations: one for SW0/SW5 targets, one for SW1/SW2
targets, and one for SW3/SW4 targets. We have already acknowledged that the SW1 through
SW4 target RCS models are random process models. To maintain consistency with this idea,
and consistency with what happens in an actual radar, we also use a random process model for
the SW0/SW5 target.
In Chapter 3, we learned the SW1 and SW2 targets share one RCS fluctuation model and the
SW3 and SW4 targets share a second RCS fluctuation model. The difference between SWodd
(SW1, SW3) and SWeven (SW2, SW4) was in how their RCS varies with time. SWodd targets
have an RCS that is constant from pulse to pulse, but varies from scan to scan. SWeven targets
have an RCS that varies from pulse to pulse. All cases assumed the RCS did not vary during a
PRI. Because of this assumption, the statistics for SW1 and SW2 targets are the same on any
one pulse. Likewise, the statistics for SW3 and SW4 targets are the same on any one pulse.
Consequently, in terms of single pulse probabilities, we can combine SW1 and SW2 targets
and we can combine SW3 and SW4 targets. This accounts for our use of the terminology
“SW1/SW2 targets” and “SW3/SW4 targets” when discussing single pulse detection
probability. In Chapter 8, we will develop separate equations for each of the Swerling target
types, since we will base detection decisions on the results from processing several pulses.
Since the target RCS models are random processes, we also represent the target voltage
signals in the radar (henceforth termed the target signal) as random processes. To that end,
the IF representation of the target signal is
where
and
The baseband signal model is
We note that both of the signal models are consistent with the noise model of the previous
sections. We assume S = S (t)|t=t1 and θ = θ(t)|t=t1 are independent.

We have made many assumptions concerning the statistical properties of the signal and
noise. A natural question is: are the assumptions reasonable? The answer is that radars are
usually designed so that the assumptions are satisfied. In particular, designers endeavor to
make the receiver and matched filter linear. Because of this and the central limit theorem, we
can reasonably assume nI(t) and nQ(t) are Gaussian. Further, if we enforce reasonable
constraints on the bandwidth of receiver components, we can reasonably assume the validity
of the independence requirements. The stationarity requirements are easily satisfied if we
assume the receiver gains and noise figures do not change with time. We enforce the zero-
mean assumption by using AC coupling and bandpass filters (BPFs) to eliminate DC
components. For signals, we will not need the Gaussian requirement. However, we will need
the stationarity, zero-mean, and other requirements. These constraints are usually satisfied for
signals by using the same assumptions as for noise, by requiring a WSS random process for
the target RCS, and by requiring θ(t) be wide sense stationary and uniform on (–π, π]. The
latter two assumptions are valid for practical radars and targets.
At this point, we need to develop separate signal models for the different types of targets
because each signal amplitude fluctuation, S(t), is governed by a different model.
6.3.2 Signal Model for SW0/SW5 Targets
For the SW0/SW5 target case, we assume a constant target RCS. This means the target power,
and thus the target signal amplitude, is constant. With this assumption, we let
The IF signal model becomes
We introduce the random variable θ to force sIF(t) to be a random process. We specifically
choose θ to be uniform on (–π,π]. This makes sI and sQ random variables, rather than random
processes. sIF(t) is a random process because of the presence of the ωIFt term. This model is
actually consistent with what happens in an actual radar. Specifically, the phase of the signal is
random for any particular target return.
The density functions of sI and sQ are the same and are given by [13]:
We cannot assert the independence of random variables sI and sQ because we have no means
of showing fsIsQ (SI,SQ) = fsI (SI) fsQ (SQ).

The signal power is given by
In the above, we can write
Similarly, we get
and
Substituting (6.31), (6.32), and (6.33) into (6.30) results in
From (6.26), the baseband signal model is
and the signal power is

6.3.3 Signal Model for SW1/SW2 Targets
For the SW1/SW2 target case, we have already stated that the target RCS is governed by the
density function (see Chapter 3)
Since the power is a direct function of the RCS (from the radar range equation), the signal
power at the detection logic input has a density function identical in form to (6.37). That is,
where
Random variable theory shows the signal amplitude, S(t), governed by the density function,
which is recognized as a Rayleigh density function [13]. This, combined with the fact that θ(t)
in (6.21) is uniform, and the assumption of the independence of random variables S = S (t)|t=t1
and θ = θ(t)|t=t1, leads to the interesting observation that the signal model for a SW1/SW2
target takes the same form as the noise model. That is, the IF signal model for a SW1/SW2
target takes the form
where S(t) is Rayleigh and θ(t) is uniform on (–π,π]. If we adapt the results from our noise
study, we conclude that sI(t) and sQ(t) are Gaussian with the density functions

Furthermore, sI = sI (t)|t=t1 and sQ = sQ(t)|t=t1 are independent.
The signal power is given by
Invoking the independence of sI = sI (t)|t=t1 and sQ = sQ (t)|t=t1, and the fact that sI(t) and sQ(t)
are zero mean and have equal variances of PS, lead to the conclusion that
The baseband representation of the signal is
where the various terms are as defined above. The power in the baseband signal
representation can be written as
as expected.
6.3.4 Signal Model for SW3/SW4 Targets
For the SW3/SW4 target case, we have already stated that the target RCS is governed by the
density function (see Chapter 3)
Since the power is a direct function of the RCS (from the radar range equation), the signal

power at the signal processor output has a density function that takes the same form as (6.47).
That is,
where PS is defined earlier in (6.37). From random variable theory it can be shown that the
signal amplitude, S(t), is governed by the density function
Unfortunately, this is about as far as we can carry the signal model development for the
SW3/SW4 case. We can invoke the previous statements and write
and
However, we do not know the form of the densities of sI(t) and sQ(t). Furthermore, deriving
their form has proven very laborious and elusive.
We can find the power in the signal from
We will need to deal with the inability to characterize sI(t) and sQ(t) when we consider the
characterization of signal-plus-noise.
6.4 SIGNAL-PLUS-NOISE IN RECEIVERS
6.4.1 General Formulation
Now that we have characterizations for the signal and noise, we want to develop
characterizations for the sum of signal and noise. That is, we want to develop the appropriate
density functions for

If we are using the IF representation, we write
and if we are using the baseband representation, we write
In either representation, the primary variable of interest is the magnitude of the signal-plus-
noise voltage, V(t), since this quantity is used in computing detection probability. We will
compute the other quantities as needed, and as we are able.
We begin the development with the easiest case—the SW1/SW2 case—and progress
through the SW0/SW5 case to the most difficult—the SW3/SW4 case.
6.4.2 Signal-Plus-Noise Model for SW1/SW2 Targets
For the SW1/SW2 case, we found the real and imaginary parts of both signal and noise were
zero-mean, Gaussian random processes. We will use the baseband representation to derive the
density function of V(t). Since sI(t) and nI(t) are Gaussian, vI(t) will also be Gaussian. Since
sI(t) and nI(t) are zero-mean, vI(t) will also be zero-mean. Finally, since sI(t) and nI(t) are
independent, the variance of vI(t) will equal to the sum of the variances of sI(t) and nI(t). That
is,
With this, we get
By similar reasoning, we get

Since sI = sI(t), nI = nI(t)|t=t1, sQ = sQ (t)|t=t1, and nQ = nQ(t)|t=t1 are mutually independent, vI(t) =
vI(t)|t=t1 and vQ (t) = vQ(t)|t=t1 are independent. This, coupled with our reasoning above and our
previous discussions of noise and the SW1/SW2 signal model, leads to the observation that
V(t) is Rayleigh with density
6.4.3 Signal-Plus-Noise Model for SW0/SW5 Targets
Since sI(t) and sQ(t) are not Gaussian for the SW0/SW5 case, when we add them to nI(t) and
nQ(t), the resulting vI(t) and vQ(t) are not Gaussian. This means that directly manipulating
vI(t) and vQ(t) to obtain the density function of V(t) will be difficult. Therefore, we will take a
different tack and invoke some properties of joint and marginal density functions [13, 14].
Specifically, we use
We then use
to get the density function of V(t). This procedure involves some tedious math, but it is math
that can be found in many books on random variable theory [13–17].
To execute the derivation, we start with the IF representation and write
where we made use of (6.28). When we expand (6.62) and group terms, we get
According to the conditional density of (6.60), we want to consider (6.63) for the specific
value of θ = θ. Doing this, we get

With this, we note [Scosθ + nI(t)] and [Ssinθ + nQ(t)] are Gaussian random variables with
means of Scosθ and Ssinθ. These variables also have the same variance of σ 2. Furthermore,
since nI = nI (t) and nQ = nQ(t)|t=t1 are independent, (Scosθ + nI) and (Ssinθ + nQ)|t=t1 are
independent. With this, we can write
Invoking the discussions related to (6.4), (6.5), and (6.6), we get
If we substitute from (6.65), we get
and manipulate the exponent to yield
Finally, we use
along with (6.60), to write
For the next step, we integrate fVψθ(V, ψ, θ) with respect to ψ and θ to derive the desired
marginal density, fv(V). That is (after a little manipulation),

We first consider the integral with respect to ψ, or
We recognize the integrand is periodic with a period of 2π and that the integral is performed
over one period. This means we can evaluate the integral over any period. Specifically, we
choose the period from θ to 2π + θ and get
With the change of variables α = ψ – θ, the integral becomes [18]:
where I0(x) is a modified Bessel function of the first kind and order zero [19].
Substituting (6.74) into (6.71) yields
where the last step derives from the fact that the integral with respect to θ is equal to 1.
Equation (6.75) is the desired result, which is the density function of V(t).
6.4.4 Signal-Plus-Noise Model for SW3/SW4 Targets
As with the SW0/SW5 case, sI(t) and sQ(t) are not Gaussian for the SW3/SW4 case. Thus,

when we add them to nI(t) and nQ(t), the resulting vI(t) and vQ(t) are not Gaussian and directly
manipulating them to obtain the density function of V(t) is difficult. Based on our experience
with the SW0/SW5 case, we again use the joint/conditional density approach. We note that the
IF signal-plus-noise voltage is given by
In this case, we find the joint density of V(t), S(t), ψ(t), and θ(t), and perform the appropriate
integration to obtain the marginal density of V(t). More specifically, we will find
and
Drawing on our work from the SW0/SW5 case, we write
Further, since S(t) and θ(t) are, by definition, independent, we write
Substituting (6.79) and (6.80) into (6.77) results in
From (6.78), with some manipulation, we write

where
and
We recognize (6.84) as the same double integral of (6.71). Thus, using discussions related to
(6.74), we get
and
To complete the calculation of fv(V), we compute the integral
where
Using a symbolic mathematics software package to compute the integral, we get
With this result, fv(V) becomes

which, after manipulation, can be written as
Now that we have completed the characterization of noise, signal, and signal-plus-noise, we
are ready to attack the detection problem.
6.5 DETECTION PROBABILITY
6.5.1 Introduction
A functional block diagram of the detection process is illustrated in Figure 6.3. This process
consists of an amplitude detector and a threshold device. The amplitude detector determines
the magnitude of the signal coming from the matched filter, and the threshold device—a
binary decision device—outputs a detection declaration if the signal magnitude is above some
threshold, or a no-detection declaration if the signal magnitude is below the threshold.
Figure 6.3 Block diagram of the detector and threshold device.
Figure 6.4 IF and baseband detectors—linear and square law.

6.5.2 Amplitude Detector Types
The amplitude detector can be a square-law detector or a linear detector. Figure 6.4 provides a
functional illustration of both variants for the IF implementation and the baseband
implementation. In the IF implementation, the detector consists, functionally, of a diode
followed by a lowpass filter (LPF). If the circuit design uses small voltage levels, the diode
will be operating in its small signal region and will result in a square-law detector. If the
circuit design uses large voltage levels, the diode will be operating in its large signal region
and will result in a linear detector.
For the baseband case, the digital hardware (which we assume in the baseband case)
actually forms the square of the magnitude of the complex signal out of the receiver/matched
filter by squaring the real and imaginary components of the receiver/matched filter output and
then adding them. This operation results in a square-law detector. In some instances, the
detector also performs a square root to form the magnitude.
In either the IF or baseband representation, the square-law detector outputs N2(t) when only
noise is present at the receiver/matched filter output and V2(t) when signal-plus-noise is
present at the receiver/matched filter output. The linear detector outputs N(t) when only noise
is present at the receiver/matched filter output and V(t) when signal-plus-noise is present at
the receiver/matched filter output.
6.5.3 Detection Logic
Since both N(t) and V(t) are random processes, we must use concepts from random processes
theory to characterize the detection logic performance. In particular, we use probabilities.
Since we have two signal conditions (noise only and signal-plus-noise) and two outcomes
from the threshold check, we have four possible events to consider:
1. signal-plus-noise ≥ threshold ⇒ detection
2. signal-plus-noise < threshold ⇒ missed detection
3. noise ≥ threshold ⇒ false alarm
4. noise < threshold ⇒ no false alarm
Of the above examples, the two desired events are 1 and 4. That is, we want to detect targets
when they are present, and we do not want to detect noise when targets are not present. Since
events 1 and 2 are related and events 3 and 4 are related, we need only find probabilities
associated with events 1 and 3. We term the probability of the first event occurring the
detection probability, and the probability of the third event occurring the false alarm
probability. In equation form
and

where V = V (t)|t=t1 indicates signal-plus-noise voltage evaluated at a specific time, and N =
N(t)|t=t1 indicates noise voltage evaluated at a specific time.
The definition above carries some subtle implications. First, when one finds detection
probability, it is tacitly assumed that the target return is present at the time the output of the
threshold device is checked. Likewise, when one finds false alarm probability, it is tacitly
assumed that the target return is not present at the time the output of the threshold device is
checked.
In practical applications, it is more appropriate to say: at the time the output of the threshold
device is checked, the probability of a threshold crossing equals Pd if the signal contains a
target signal and Pfa if the signal does not contain a target signal.
It will be noted that the above probabilities are conditional probabilities. In normal practice,
we do not explicitly use the conditional notation, and write
and
Further, we recognize that we should use signal-plus-noise when we assume the target is
present and noise only when we assume the target is not present, and that the probabilities are
conditional.
The discussion above relates to a linear detector. If the detector is square law, the
appropriate equations would be
and
6.5.4 Calculation of Pd and Pfa
From probability theory, we can write [13]
and

In the expression above, T denotes the threshold voltage level and T2 denotes the threshold
expressed as normalized power.
To avoid having to use two sets of Pd and Pfa equations, we will digress to show how we
can compute Pd and Pfa using either of the integrals of (6.98) and (6.99).
It can be shown [13] that if 
 and y ≥ 0, then
If we write
we can use (6.100) to write
With the change of variables x = v2, we have
Similar results apply to Pfa and indicate we can use either form to compute detection and false
alarm probability.
We note that the integrals for Pd and Pfa are over the same limits. Figure 6.5 provides an
illustration of this. Notice Pd and Pfa are areas under their respective density functions, to the
right of the threshold value. Increasing the threshold decreases the probabilities, and
decreasing the threshold increases the probabilities.
This is not exactly what we want. Ideally, we want to select the threshold so that we have Pfa
= 0 and Pd = 1. Because this is not possible, we usually choose the threshold as some sort of
tradeoff between Pd and Pfa. In fact, we choose the threshold to achieve a certain Pfa and find
other means of increasing Pd (see Chapter 8).
Referring to (6.12), the only parameter that affects fN(n) is the noise power, σ2. While we
have some control over this via noise figure, executing that control can be very expensive. On

the other hand, fv(v) depends upon both Ps and σ2. This gives us some degree of control. In
fact, we usually try to affect both fN(n) and fv(v) by increasing PS and decreasing σ2. The net
result of this is that we try to maximize SNR.
Figure 6.5 Probability density functions for noise and signal-plus-noise.
6.5.4.1 False Alarm Probability
Using (6.14) in (6.99), we can derive an equation for false alarm probability as
In this equation, we define
as the threshold-to-noise ratio (TNR). We usually select a desired Pfa and, from this, derive
the required TNR as
6.5.4.2 Detection Probability
We compute the detection probability for the three target classes by substituting (6.59), (6.75),
and (6.91) into (6.103).
SW0/SW5 Target
For the SW0/SW5 case, we have

where we took advantage of T > 0 to eliminate U(V) from the integrand.
Equation (6.107) is in the form of the Marcum Q function [3, 4], which has the general
form
In (6.107), we make the change of variables x = v/σ and get
This is of the form of (6.108), with a = S/σ, b = T/σ, and M = 1. Thus, we have
Since we are interested in finding Pd as a function of SNR and Pfa, we want to manipulate
(6.110) so it is a function of these variables. From (6.105) and (6.106), we have
From (6.17) or (6.19), we have
and from (6.34) or (6.36), we have
We note that

which leads to
Substituting (6.111) and (6.115) into (6.110) results in
Unfortunately, Q1(a,b) has no simple form. However, Steen Parl has developed an algorithm
that appears to work quite well [20]. Parl’s algorithm is described in Appendix 8B.
Skolnik presents the approximation [2; 21, p. 27]
where
and
is one form of the error function. It has been the authors’ experience that Skolnik’s
approximation degrades as SNR approaches and falls below TNR.
A recent paper by Barton [9] presents an equation attributed to David A. Shnidman [22]. In
this equation,
where erfc denotes the complementary error function.
SW1/SW2 Target
For the SW1/SW2 case, we substitute (6.59) into (6.103) and write

With the change of variables, x = V 2/2(Ps + σ 2) we get
Using lnPfa = –T2/2σ 2 and SNR = PS/σ2, we can write (6.122) as
SW3/SW4 Target
For the SW3/SW4 case, we substitute (6.91) into (6.103), and write
With the change of variables, x = V2/2(Ps + 2σ2) we get
Substituting TNR = T2/2σ 2 and SNR = PS/σ 2 and manipulating yields

Finally, with TNR = –lnPfa
As a reminder, for all the Pd equations, SNR denotes the signal-to-noise ratio computed from
the radar range equation (see Chapter 2).
6.5.5 Behavior Versus Target Type
Figure 6.6 contains plots of Pd versus SNR for the three target types and Pfa = 10−6, a typical
value [23, p. 45]. It is interesting to note the Pd behavior for the three target types. In general,
the SW0/SW5 target provides the largest Pd for a given SNR; the SW1/SW2 target provides
the lowest Pd; and the SW3/SW4 falls somewhere between the other two. With some thought,
this makes sense. For the SW0/SW5 target model, only the noise affects a threshold crossing
(since the target RCS is constant). For the SW1/SW2, the target RCS can fluctuate
considerably; thus both noise and RCS fluctuation affect the threshold crossing. The standard
assumption for the SW3/SW4 model is that it consists of a predominant (presumably constant
RCS) scatterer and several smaller scatterers. Thus, RCS fluctuation affects the threshold
crossing for the SW3/SW4 target somewhat, but not to the extent of the SW1/SW2 target.

Figure 6.6 Pd vs. SNR for three target types and Pfa = 10−6.
Figure 6.7 Illustration of when to compute Pd and Pfa.

It is interesting to note that a SW1/SW2 target requires an SNR of about 13 dB for Pd = 0.5,
with Pfa = 10−6. This same SNR gives a Pd = 0.9 on a SW0/SW5 target. A Pd = 0.9 on a
SW1/SW2 target requires an SNR of about 21 dB. These are the origins of the 13-dB and 20-
dB SNR numbers used in the radar range equation examples of Chapter 2.
To reiterate an earlier statement, we term the Pfa and Pd variables given above the single
pulse, single sample, or single hit Pfa and Pd. This term derives from the fact that the
threshold check (i.e., check for a target detection) is based on target returns from a single
pulse. If the signal contains both a target and noise component [i.e., s(t) and n(t)], we are
computing Pd. If the sample contains only noise, we are computing Pfa. Figure 6.7 illustrates
this concept.
6.6 DETERMINATION OF FALSE ALARM PROBABILITY
One parameter included in the detection probability equations is the threshold-to-noise ratio,
TNR. As indicated in (6.105), TNR = –lnPfa, where Pfa is the false alarm probability. System
requirements set false alarm probability.
In a radar, false alarms result in wasted radar resources (energy, timeline, and hardware),
because every time a false alarm occurs, the radar must expend resources determining
whether the alarm was the result of a random noise peak or of an actual target that can be
redetected at or near that location. Said another way, every time the output of the amplitude
detector exceeds the threshold, T, a detection is recorded. The radar data processor does not
know, a priori, whether the detection is a target detection or the result of noise (i.e., a false
alarm). Therefore, the radar must verify each detection, a process that usually requires
transmission of another pulse and another threshold check (an expenditure of time and
energy). Further, until the detection is verified, it must be carried in the computer as a valid
target detection (an expenditure of hardware or software).
To minimize wasted radar resources, we want to minimize the probability of false alarm.
Said another way, we want to minimize Pfa. However, we cannot set Pfa to an arbitrarily small
value because this increases TNR and reduces detection probability, Pd. As a result, we set Pfa
to provide an acceptable number of false alarms within a given time period. This last
statement provides the criterion normally used to compute Pfa. Specifically, Pfa is chosen to
provide an average of one false alarm within a time period termed the false alarm time, Tfa.
Tfa is usually set by some criterion driven by radar resource limitations.
The classical method of determining Pfa is based strictly on timing [24]. Figure 6.8, which
contains a plot of noise at the output of the amplitude detector, helps illustrate this concept.
The horizontal line labeled “Threshold, T” represents the detection threshold voltage level. It
will be noted that the noise voltage is above the threshold for four time intervals of length t1,
through t4. Further, the spacings between threshold crossings are T1, T2, and T3. Since a
threshold crossing constitutes a false alarm, one can say that over the interval T1, false alarms

occur for a period of t1. Likewise, over the interval T2, false alarms occur for a period of t2,
and so forth. Averaging all tk produces an average time, 
, when the noise is above the
threshold. Likewise, averaging all of Tk produces the average time between false alarms (i.e.,
the false alarm time, Tfa). To determine the false alarm probability, we find the ratio of  to
Tfa, that is,
While Tfa is reasonably easy to specify, the specification of  is not obvious. The standard
assumption sets  to the range resolution expressed as time, τ∆R. For an unmodulated pulse,
τ∆R is the pulsewidth. For a modulated pulse, τ∆R is the reciprocal of the modulation
bandwidth.
It has been the authors’ experience that the above method of determining Pfa is not very
accurate. While it would be possible to place the requisite number of caveats on (6.128) to
make it more accurate, with modern radars, this is not necessary.
Figure 6.8 Illustration of false alarm time.
The previously described method of determining Pfa relies on the assumption that hardware
operating on a continuous-time signal records the detections. Modern radars base detection
on the examination of signals that have been converted to the discrete-time domain by
sampling or by an analog-to-digital converter. This makes determination of Pfa easier, and
more intuitively appealing, in that we can deal with discrete events. With modern radars, we
compute the number of false alarm chances, Nfa, within the desired false alarm time, Tfa, and
compute the probability of false alarm from
Computing Nfa requires us to know certain things about the radar’s operation. We will outline

some such thoughts.
A typical radar samples the return signal from each pulse with a period equal to the range
resolution, τ∆R, of the pulse. As indicated above, this would equal the pulsewidth for an
unmodulated pulse and the reciprocal of the modulation bandwidth for a modulated pulse.
These range samples are usually taken over the instrumented range, ∆T. In a search radar, ∆T
might be only slightly less than the PRI, T. However, for a track radar, ∆T may be
significantly less than T. With the above, we compute the number of range samples per PRI as
Each range sample provides the opportunity for a false alarm.
In a time period of Tfa, the radar transmits
pulses. Thus, the number of range samples (and, thus, chances for false alarm) over the time
period of Tfa is
In some radars, the receiver contains several (NDop) parallel Doppler channels. Such radars
also contain NDop amplitude detectors. Each amplitude detector generates NR range samples
per PRI. Thus, in this case, the total number of range samples in the time period Tfa would be
In either case, (6.129) gives the false alarm probability.
6.6.1 Example 1—Computing Pfa
To illustrate the discussion above, we consider the simple example of a search radar with a
PRI of T = 400 μs. This radar uses a 50-μs pulse with LFM and a bandwidth of 1 MHz. With
this we get tΔR = 1 μs. We assume the radar starts its range samples one pulsewidth after the
transmit pulse and stops taking range samples one pulsewidth before the succeeding transmit
pulse. From these parameters, we get ΔT = 300 µs. The signal processor is not a multichannel
Doppler processor. The radar has a search scan time of Tscan = 1 s, and we want no more than
one false alarm every two scans. With this, we get Tfa = 2Tscan = 2 s, and if we combine this
with the PRI, we get

From ∆T and τ∆R, we get
This results in
and
6.6.2 Example 2—Detection Contour
For this example, we combine the radar range equation discussions of Chapter 2 with the Pd
and Pfa discussions of this chapter to plot a detection contour for a search radar. As used here,
a detection contour is a boundary, in altitude versus downrange space, on which a radar
achieves a given Pd. For all points outside the area bounded by the contour, Pd is less than the
desired value. For all points inside the boundary, Pd is greater than the desired value.
Table 6.2 lists the radar parameters. The table also includes other parameters we will need.
As implied by Table 6.2, the antenna constantly rotates in azimuth and the antenna’s directivity
varies with elevation angle. This directivity variation can be represented by the equation
where Table 6.3 provides the values of ak, Nk, and εk. Figure 6.9 contains a plot of G(ε).
Table 6.2
Radar and Other Parameters Used for Example 2
Parameter
Value
Peak transmit power at the power amp output
50 kW
Operating frequency
2 GHz
PRF
1,000 Hz
Pulsewidth, τp
100 µs
Pulse modulation bandwidth
1 MHz

Antenna directivity (transmit and receive)
See function
Total losses
13 dB
System noise figure
5 dB
Target type and RCS
SW1, 6 dBsm
Antenna rotation rate
6 rpm
Instrumented range
PRI – 2τp – 50 µs
False alarm criterion
No more than one false alarm per 360° rotation
Detection probability
0.5
Table 6.3
Parameter Values for (6.137)
ak
Nk
εk (deg)
1
45
1
1
25
3
1
13
7
0.5
7
15
Figure 6.9 Antenna elevation directivity pattern.
We acknowledge that the directivity also varies with azimuth and, as a result, the SNR
varies as the antenna sweeps by the target. We account for this variation by including a scan
loss in the total losses of Table 6.2.
The elevation angle is computed from
where h denotes the target height, or altitude, and Rd denotes the downrange position of the
target. We relate h and Rd to slant range, R, and ε by

To generate the detection contour, we solve the radar range equation for R in terms of the
other parameters. That is,3
and use (6.140) to plot h(ε) and Rd(ε) as we vary ε from 0° to 90°.
Most of the required parameters are in Table 6.2, or can be easily computed from the
parameters in the table. The exception is SNR. We will use the specified Pd and the false alarm
specification, along with the target type, to compute SNR.
Since the table specifies a SW1 target, we can solve (6.123) for SNR and get
We already have Pd, but need to compute Pfa. To do this, we must first compute Nfa.
Since the antenna rotates at a rate of frot revolutions per minute (rpm), the time to complete
a rotation is
Since the false alarm criterion is one false alarm per revolution (see Table 6.2), the false
alarm time is
Had we specified no more than one false alarm every three rotations, we would have had Tfa =
3Trot = 30 s.
Given the PRF of 1,000 Hz, we can compute the PRI as (see Chapter 1):
From (6.134), we compute the number of pulses in Tfa (as a reminder, we are considering
single pulse detection) as

If we sample the return at the modulation bandwidth, we get
From the instrumented range specified in Table 6.2, we get
and from (6.135)
We assume the radar does not implement any type of Doppler processing. Therefore, we can
use (6.132) to compute
and find
Finally, we use (6.142) to compute
Substituting the appropriate values into (6.141) gives
Figure 6.10 contains a plot of h(ε) versus Rd(ε). The curved grid lines on the figure result
from the round earth model used to plot the detection contour. The solid curved line
represents the Earth’s surface, and the slanted, numbered lines represent elevation lines.

Figure 6.10 Detection contour.
6.7 SUMMARY
The major results of this chapter are the three detection probability equations for the three
different target types and the methodology for computing Pfa. The three Pd equations are
• SW0/SW5 targets— 
• SW1/SW2 targets—Pd=eln Pfa/(SNR+1)
• SW3/SW4 targets— 
As yet another reminder, the Pd and Pfa discussed in this chapter are single sample, or single
pulse, values. In Chapter 8, we consider the problem of computing Pd and Pfa based on
processing several samples of signal-plus-noise and noise.
6.8 EXERCISES
1.
A phased array radar searches a volume of space with a search raster containing 400
beams. The dwell time per beam is 10 ms. and the radar uses a pulsed Doppler
waveform. The signal processor has 10 range gates with a 64-point fast Fourier
transformer (FFT) on each range gate. This produces a 10-by-64 range-Doppler map on
each dwell. The detection logic checks each range-Doppler cell once per beam dwell.
We want the radar to support a 20-s time between false alarms. For purposes of
computing Pd and Pfa, we consider a dwell a single sample, or single pulse. Thus, the Pd
and Pfa equations of this chapter apply to this problem.

a)
What false alarm probability, Pfa, is necessary to support the specified false alarm
rate?
b)
What SNR, in dB, is required at the signal processor output for the radar to provide
a single-sample detection probability of 0.95 on a SW0/SW5 target?
c)
What SNR, in dB, is required at the signal processor output for the radar to provide
a single-sample detection probability of 0.95 on a SW1 target?
d)
What SNR, in dB, is required at the signal processor output for the radar to provide
a single-sample detection probability of 0.95 on a SW3 target?
2.
A monostatic radar has the following parameters:
•
Peak transmit power at power amplifier output—100 kW
•
Transmit losses—2 dB
•
Operating frequency—10 GHz
•
PRF—2,000 Hz
•
Antenna diameter—1.5 m (circular aperture)
•
Antenna efficiency—60%
•
Other losses—10 dB
•
System noise figure—6 dB
•
Antenna temperature (Ta)—100 K
•
The radar transmits a 10-µs rectangular pulse.
•
The radar maintains a Pfa of 10−9
Plot Pd versus target range, in km, for a 6-dBsm, SW1 target. Let the range vary from 5
km to the unambiguous range of the radar.
3.
Derive (6.14) and (6.15).
4.
Derive (6.17) and (6.19).
5.
Show that (6.71) can be obtained by manipulating (6.70).
6.
Show that (6.91) follows from (6.90).
7.
Show that (6.100) is correct.
8.
Show that (6.126) follows from (6.125).
References
[1]
Swerling, P., “Probability of Detection for Fluctuating Targets,” RAND Corp., Santa Monica, CA, Res. Memo. RM-
1217, Mar. 17, 1954. Reprinted: IRE Trans. Inf. Theory, vol. 6, no. 2, Apr. 1960, pp. 269–308.
[2]
Rice, S. O., “Mathematical Analysis of Random Noise,” Bell Syst. Tech. J., vol. 23, no. 3, Jul. 1944, pp. 282–332; vol.
24, no. 1, Jan. 1945, pp. 461–556.
[3]
Marcum, J. I., “A Statistical Theory of Target Detection by Pulsed Radar,” RAND Corp., Santa Monica, CA, Res.
Memo. RM-754-PR, Dec. 1, 1947. Reprinted: IRE Trans. Inf. Theory, vol. 6, no. 2, Apr. 1960, pp. 59–144. Reprinted:
Detection and Estimation (S. S. Haykin, ed.), Halstad Press, 1976, pp. 57–121.
[4]
Marcum, J. I., “A Statistical Theory of Target Detection by Pulsed Radar (Mathematical Appendix),” RAND Corp.,

Santa Monica, CA, Res. Memo. RM-753-PR, Jul. 1, 1948. Reprinted: IRE Trans. Inf. Theory, vol. 6, no. 2, Apr. 1960,
pp. 145–268.
[5]
Swerling, P., “Radar Probability of Detection for Some Additional Fluctuating Target Cases,” IEEE Trans. Aerosp.
Electron. Syst., vol. 33, no. 2, Apr. 1997, pp. 698–709.
[6]
Shnidman, D. A., “Expanded Swerling Target Models,” IEEE Trans. Aerosp. Electron. Syst., vol. 39, no. 3, Jul. 2003, pp.
1059–1069.
[7]
Heidbreder, G. R., and R. L. Mitchell, “Detection Probabilities for Log-Normally Distributed Signals,” IEEE Trans.
Aerosp. Electron. Syst., vol. 3, no. 1, Jan. 1967, pp. 5–13.
[8]
Barton, D. K., “Simple Procedures for Radar Detection Calculations,” IEEE Trans. Aerosp. Electron. Syst., vol. 5, no. 5,
Sept. 1969, pp. 837–846. Reprinted: Barton, D. K., ed., Radars, Vol. 2: The Radar Range Equation (Artech Radar
Library), Dedham, MA: Artech House, 1974, pp. 113–122.
[9]
Barton, D. K., “Universal Equations for Radar Target Detection,” IEEE Trans. Aerosp. Electron. Syst., vol. 41, no. 3, Jul.
2005, pp. 1049–1052.
[10] Weinstock, W. W., Target Cross Section Models for Radar Systems Analysis, Ph.D. Dissertation in Electrical
Engineering, University of Pennsylvania, 1964.
[11] DiFranco, J. V., and W. L. Rubin, Radar Detection, Prentice-Hall, 1968. Reprinted: Dedham, MA: Artech House, 1980.
[12] Budge, M. C., Jr., “EE 619: Intro to Radar Systems,” www.ece.uah.edu/courses/material/EE619/index.htm.
[13] Papoulis, A., Probability, Random Variables, and Stochastic Processes, 3rd ed., New York: McGraw-Hill, 1991.
[14] Ross, S. M., A First Course in Probability, 5th ed., Upper Saddle River, NJ: Prentice-Hall, 1998.
[15] Cooper, G. R., and C. D. McGillem, Probabilistic Methods of Signal and System Analysis, 2nd ed., Fort Worth, TX:
Holt, Rinehart and Winston, 1986.
[16] Stark, H., and J. W. Woods, Probability, Random Processes and Estimation Theory for Engineers, 2nd ed., Upper
Saddle River, NJ: Prentice-Hall, 1994.
[17] Peebles, P. Z., Jr., Probability, Random Variables, and Random Signal Principles, 3rd ed., New York: McGraw-Hill,
1993.
[18] Gradshteyn, I. S., and I. M. Ryzhik, Table of Integrals, Series, and Products, 8th ed., D. Zwillinger and V. Moll, eds.,
New York: Academic Press, 2015. Translated from Russian by Scripta Technica, Inc.
[19] Abramowitz, M., and I. A. Stegun, eds., Handbook of Mathematical Functions with Formulas, Graphs, and
Mathematical Tables, National Bureau of Standards Applied Mathematics Series 55, Washington, DC: U.S. Government
Printing Office, 1964; New York: Dover, 1965.
[20] Parl, S., “A New Method of Calculating the Generalized Q Function,” IEEE Trans. Inf. Theory, vol. 26, no. 1, Jan. 1980,
pp. 121–124.
[21] Skolnik, M. I., Introduction to Radar Systems, 2nd ed., New York: McGraw-Hill, 1980.
[22] Shnidman, D. A., “Determination of Required SNR Values,” IEEE Trans. Aerosp. Electron. Syst., vol. 38, no. 3, Jul.
2002, pp. 1059–1064.
[23] Barton, D. K., Radar System Analysis and Modeling, Norwood, MA: Artech House, 2005.
[24] Meyer, D. P., and H. A. Mayer, Radar Target Detection, New York: Academic Press, 1973.
1 We modified the definition of the rect[x] function definition of Chapter 2 slightly by making the upper bound closed.
2 Also known as the Heaviside step function.
3 We claim that we are performing a preliminary analysis and, as a result, set Ta to T0 (see Chapter 2).

Chapter 7
Matched Filter
7.1 INTRODUCTION
In the detection probability equations [(6.109), (6.122), and (6.126)], we noted that Pd depends
directly on SNR. That is, Pd increases as SNR increases. Because of this, we want to try to
ensure the receiver is designed to maximize SNR by including a matched filter in the receiver.
In most radars, the matched filter is included immediately before the signal processor, and in
some the matched filter is the signal processor.
J. H. Van Vleck and David Middleton coined the term “matched filter” in a 1946 Journal of
Applied Physics article [1]. They credited D. O. North with arriving at the same formulation
for the matched filter but by a different approach based on calculus of variations, instead of
the Cauchy-Schwarz inequality they used. North’s development first appeared in a classified
report, which was later published in a 1963 journal article [2]. Van Vleck and Middleton
indicated the matched filter equations were also developed by Henry Wallman as a specific
case of a more general theory developed by Norbert Wiener.
7.2 PROBLEM DEFINITION
The statements in the first paragraph of this chapter provide the design requirement for the
matched filter. Specifically, given some signal, s(t), and noise, n(t), we find a filter impulse
response, h(t), that maximizes SNR at the filter output. For purposes of this discussion, we
assume the signal is not a random process. Actually, we assume the form (e.g., an LFM pulse)
of the signal is deterministic; its amplitude and phase can be a random variable. As a note, we
are using complex signal notation (see Chapter 1) in this chapter. This is consistent with the
notation used in Chapter 6 and applies to both the IF and baseband representation. Thus, for
example, n(t) could be a representation of nIF(t) or nB(t) as appropriate.
As indicated in Figure 7.1, if the input to the matched filter is s(t), the output will be so(t),
and if the input is n(t), the output will be no(t). The output, instantaneous, normalized, signal
power is
For purposes of the matched filter design, we define the normalized, peak signal power at the
matched filter output as1

Since no(t) is a random process [that we assume is wide-sense stationary (WSS)], we must
work with its average power. Thus, the normalized average noise power at the output of the
matched filter is
where we use expected values (E{x}) because we are dealing with random processes [3].
With the above, we can define the design criterion for the matched filter. Specifically, we
choose the matched filter to maximize the ratio of peak signal power to average noise power
at the output of the matched filter. In equation form
Figure 7.1 Matched filter block diagram.
7.3 PROBLEM SOLUTION
Equation (7.4) states we must first write the ratio of PS and PN in terms of h(t) and then
maximize it with respect to h(t).
We assume h(t) is linear and write
and
where ∗ denotes convolution. We choose to solve the optimization problem in the frequency
domain through the use of Fourier transforms. To this end, we write

and
and recall that
As a note, S(f) is the signal spectrum (the signal voltage spectral density) at the input to the
matched filter, as such it will experience the receiver gain of 
 discussed in Chapter 2. (We
will again encounter the factor of G when we consider noise.)
Since n(t) and no(t) are random processes, we must deal with them as such, which means
we write [3]
and
In the above, Rn(t) and Rno(t) are the autocorrelation functions of n(t) and no(t), respectively,
As a reminder, we note that n(t) and no(t) are WSS, which implies the autocorrelation is a
function of time difference only, not absolute time.
We recognize No(f) is a power spectral density (noise energy). Thus, the noise power at the
output of the matched filter is
From (7.2), the normalized peak signal power at the matched filter output is given by
However, we can write
If we combine (7.16), (7.15), (7.14), and (7.4), we get [4]

At this point, we make the assumption that n(t) noise power spectral density at the input to
the matched filter is (see Chapters 2 and 4)
where G is the receiver gain. With this, we have
We perform the maximization process by applying one of the Cauchy-Schwarz inequalities
to the numerator [5], specifically
with the equality valid only when A(f) is proportional to the complex conjugate of B(f) [5].
That is, when
where K is an arbitrary (complex) constant. If we apply (7.20) to the ratio of (7.19) with the
associations
and
we get

where we made use of
We note that (7.24) reduces to
Equation (7.26) tells us that for all H(f), the upper bound on the left side is equal to the right
side. That is, we have found the maximum value of PS/PN (the ratio of peak signal power to
average noise power at the matched filter output) over all h(t) and have solved part of the
maximization problem. To find the h(t) that yields the maximum PS/PN, we invoke the second
part of the Cauchy-Schwarz inequality given in (7.21). Specifically, we say
when we choose H(f) as
Thus, we have found the Fourier transform of the filter impulse response that maximizes peak
signal to average noise power at the filter output. Furthermore, we have an equation for the
maximum in the form of (7.27) and have determined that the maximum occurs at t = to.
We note from the form of (7.28) that
In other words, the matched filter frequency response has the same shape as the frequency
spectrum of the signal. They simply differ by a scaling factor |K|. This is the reason Van Vleck

and Middleton termed H(f) a matched filter.
We now want to look at the specific form of h(t) relative to s(t). We can write
Thus, h(t) is the conjugate of a scaled (by K), time reversed (because of the –t), and shifted
(by to) version of the signal, s(t) at the input to the matched filter. This operation is illustrated
in Figure 7.2. The left sketch of this figure is s(t), while the center figure is a sketch of s*(–t).
Finally, the right figure is Ks*(to – t), or h(t). We normally assume that s(t) has the same shape
as the signal generated in the transmitter (i.e., a pulse with a rect[x] envelope). That is, we
ignore any distortion that may have been that occurs in the transmit, propagation and receive
paths. We account for the distortion by incorporating a mismatch loss in the radar range
equation (see Chapter 5).
Now that we have established the equation for the maximum value of the SNR at the output
of the matched filter and have a filter that can provide the maximum SNR, we want to
determine its value. Specifically, we want to relate the maximum SNR to the value of SNR we
compute from the radar range equation.
From (7.27), we have
Recalling Parseval’s theorem (also known as Rayleigh’s energy theorem) [6], which can be
expressed as
and noting s(t) has finite energy and power, we write

Figure 7.2 Evolution of h(t).
We recognize the numerator of (7.33) as the energy in the signal at the input to the matched
filter. From Chapter 2, we found this to be
With this we get
We recognize (7.35) as the SNR given by the radar range equation (see Chapter 2). This tells
us the peak value of SNR (the peak power ratio) at the output of the matched filter is the SNR
(the energy ratio) we obtain from the radar range equation. In essence, the matched filter ekes
out the maximum possible SNR from the signal and noise the radar must deal with. For the
case where the interference is due to white noise at the input to the matched filter, there is no
other linear filter that will give a larger value of SNR for the transmitted signal. If the
interference is other than white noise (e.g., clutter), there are other filters that will provide
larger values of signal-to-interference power ratio (SIR) than the filter defined by (7.30). This
is discussed further in Chapter 13.
In Chapter 6, we found that Pd depended on the SNR power ratio. The results above say that
the maximum SNR power ratio is equal to the SNR energy ratio derived in Chapter 2. Thus,
the SNR provided by the radar range equation will provide the maximum Pd for a given Pfa.
Further, this maximum Pd can be achieved if the radar includes a matched filter. As a re-
reminder: we are dealing with single-pulse, or single-sample, Pd and Pfa, and with the SNR
for a single transmitted (and received) pulse. We will consider how to handle multiple pulses
in Chapter 8.
7.4 MATCHED FILTER EXAMPLES
7.4.1 General Formulation
We want to derive a general equation for the matched filter response for a signal, s(t), then
use it to derive the matched filter response for the cases where s(t) is an unmodulated pulse

and a pulse with LFM.
From (7.30), we have
where K is an arbitrary (complex) constant and to is the value of t at which the matched filter
response to s(t) will reach its peak.
Since K and to can be anything we want, without loss of generality we let K = 1 and to = 0.
The latter statement says that the output of the matched filter will reach its peak at a relative
time of zero. With this, we get
The response of h(t) to s(t) is given by
But h(t) = s*(–t) so h(t – γ) = s*[–(t – γ)] = s*(γ – t) and
We note that this integral is the complex, time autocorrelation of s(t) [5].
7.4.2 Response for an Unmodulated Pulse
For an unmodulated (rectangular) pulse
where A is the amplitude of the pulse and θ is the phase. With this, we have
A plot of s(t) is shown in Figure 7.3. The plot of s*(t) would look the same except the “height”
would be Ae–jθ rather than Aejθ.

Figure 7.3 Unmodulated pulse.
In the so(t) integral of (7.39), we note that t is the separation between s(γ) and s*(γ – t) as
shown in Figure 7.4. Figure 7.4 corresponds to the case where t ≥ 0.
When t ≥ τp, s(γ) and s*(γ – t) do not overlap and we have s(γ)s*(γ – t) = 0 and so(t) = 0.
Thus
For 0 ≤ t < τp, the overlap region of s(γ) and s*(γ – t) is t ≤ γ < τp. In the overlap region,
s(γ)s*(γ – t) = AejθAe–jθ = A2 and thus
Since t ≥ 0, we can use the substitution |t| = t (we are doing this because we will need it to
compare the form of (7.43) to the case where t < 0) and write (7.42) as
With (7.42), we get
The arrangement of s(γ) and s*(γ – t) for t < 0 is shown in Figure 7.5.
Figure 7.4 Plot of s(γ) and s*(γ – t) for t ≥ 0.

Figure 7.5 Plot of s(γ) and s*(γ – t) for t < 0.
It should be clear that if t ≤ – τp, so(t) = 0. If we multiply both sides of the inequality by –
1,we get –t ≥ τp. Since t < 0, we can write – t = |t| and
The overlap region is 0 ≤ γ < t + τp, which yields
Since t < 0, t = –|t| and we replace t with –|t| to get
Since this is the same form as for 0 ≤ t < τp, we can combine these to get
Finally, if we combine this with the result for |t| ≥ τp, we get
We note that this is the same form as (7.45). Thus, (7.50) and (7.45) apply for all t. We can
combine the two parts of (7.50) and use the rect[x] function to write so(t) in a more compact
form as

Figure 7.6 Plot of matched filter output for an unmodulated pulse.
A plot of so(t) is shown in Figure 7.6. It will be noted that so(t) is a triangle with a height of
A2τp and a base width of 2τp. This is a property of all matched filter responses: They are twice
as wide as the pulse.
7.4.3 Response for an LFM Pulse
There are very few practical pulses that lead to simple expressions for so(t). One is the
unmodulated pulse of the previous example, and another is a pulse with LFM across the pulse.
For an LFM pulse, the form of s(t) is
We note the difference between (7.52) and (7.40) is that we replaced the constant phase, θ, with
a time varying phase
If we take the derivative of θ(t), we get the frequency modulation
and note the frequency changes linearly across the pulse. This is the origin of the term “linear
frequency modulation.”
The parameter α is termed the LFM slope. If α > 0, we say we have increasing LFM because
the frequency increases across the pulse. If α < 0, we have decreasing LFM. An LFM
waveform is also termed a chirp waveform because of the sound it makes at audio
frequencies. Increasing LFM is termed up chirp, and decreasing LFM is termed down chirp.
The frequency, f(t), starts at zero at the beginning of the pulse and increases (decreases) to
ατp (–ατp) at the end of the pulse. Thus, the total frequency extent is |ατp|. This is termed the
LFM bandwidth. As we will see, the width of the central lobe of so(t) is approximately 1/|ατp|,
or the reciprocal of the LFM bandwidth.

To compute the matched filter output for an LFM pulse, we start with (7.39) and consider
the t ≥ 0 and t < 0 intervals as before. Like the unmodulated pulse, we note so(t) = 0 for |t| ≥ τ
p.
Similar to (7.43) for 0 ≤ t ≤ τp, we have
If we factor exp[jπαt(τp + t)] from the bracketed term, we get
We note that so(t) is complex. Since we are concerned only with the shape of so(t), we can use
|so(t)| and write
Multiplying by |τp – t|/|τp – t| gives
where we were able to remove the absolute value from τp – t because we are only considering

0 ≤ t ≤ tp.
If we perform similar math for –τp < t < 0, we get
As with the unmodulated pulse, we can use t = |t| in (7.58) and t = –|t| in (7.59) to get
for both (7.58) and (7.59). To get the last term in (7.60), we made use of the even property of
the sinc function to eliminate the first absolute value of t in the argument of the sinc function.
If we combine (7.60) with so(t) = 0 for |t| ≥ τp and make use of the rect[x] function, we get
Figure 7.7 contains a plot of |so(t)| for an example case of a τp = 15 μs pulsewidth, an LFM
bandwidth of B = |ατp| = 1 MHz, and α > 0. The spacing between the points where the response
is A2/2 is approximately 1 µs or 1/B. Also, the height is 15 × 10–6, or τp, since A = 1. The total
extent of the response is 30 µs or twice the pulsewidth of 15 µs. As a note, for modulated
waveforms, τp is termed the uncompressed pulsewidth, and the aforementioned spacing
between A2/2 points is termed the compressed pulsewidth. The details of the sidelobe structure
of |so(t)| depends on |Bτp|, which is termed the time-bandwidth product or BT product of the
waveform. Plots for other BT products are considered in the exercises.

Figure 7.7 Plot of matched filter output for an LFM pulse.
7.5 SUMMARY
We summarize this chapter by repeating that the impulse response of a matched filter for
some signal, s(t) is given by
where K and to are arbitrary. The sole function of a matched filter is to maximize SNR. The
matched filter is under no constraint to preserve the shape of the signal. We also note that:
1.
The SNR at the output to the matched filter is peak signal to average noise power and is
equal to ratio of the signal energy to the noise energy at the input to the matched filter, as
given by the radar range equation.
2.
The matched filter impulse response is the time reversed impulse response of the
waveform to which it is matched.
3.
The frequency response of the matched filter is the spectrum (Fourier transform) of the
complex conjugate of the waveform to which it is matched.
4.
The matched filter output is the autocorrelation of the waveform to which it is matched.
In this chapter, we developed matched filter responses for an unmodulated rectangular
pulse and a rectangular pulse with LFM. The equations for these responses are
Unmodulated:
and
LFM:
In Chapter 10, we will consider other types of waveform modulation and how to compute
their matched filter responses.
In the developments of this chapter, we noted that we were using an idealized form of s(t)
by inclusion of the rect[x] function. We also noted that actual radars cannot generate these
ideal pulses. Furthermore, if the radar could generate an idealized pulse, it would become
distorted by the time it propagates to and from the target and passes through the receiver
components prior to the matched filter. However, in most situations, the deviation from a

rectangular pulse will be small enough to not significantly affect the shape of the matched
filter response. As an example, Figure 7.8 contains the envelope of a 1.5-V, 1-µs unmodulated
pulse at the input to the matched filter. In generating the plot, it was assumed that the receiver
components prior to the matched filter could be represented by a filter with a bandwidth 4
MHz to pass almost all of the frequency components of the pulse. We assumed the transmitter
generated a pulse with a rectangular envelope, a point target and no propagation distortion.
As can be seen, the envelope is not perfectly rectangular, but is reasonably close.
We also assumed the noise at the input to the matched filter was white. Again, this is not the
case since the noise spectrum will be shaped by the frequency response of the receiver.
Finally, we also assumed the target is a point scatterer (see Chapter 3). For targets that are
small, relative to the range resolution of the pulse, this is a reasonable assumption. For cases
where the target is large, relative to the range resolution, the return pulse will be longer than
the transmit pulse (the pulse to which the filter is matched), and thus the point scatterer
assumption is not valid.
To be accurate, we should have accounted for the transmitter, environment, antenna, target,
and receiver when deriving the matched filter equation. However, the math associated with
that derivation would very quickly become untenable. We reconcile this problem by
recognizing that the output of a practical matched filter will not look exactly like the
theoretical |so(t)|. For the unmodulated pulse and phase coded pulses to be considered in
Chapter 10, |so(t)| will not have a sharp peak like that shown in Figure 7.6. Instead, the peak
will be rounded. Also, the shape will not be perfectly triangular. For example, the matched
filter output for the 1 µs unmodulated pulse of Figure 7.8 is plotted in Figure 7.9. In addition
to the distortion of the shape |so(t)|, its peak is slightly smaller than the ideal value of A2τp2.
This is normally accommodated by including a matched filter mismatch loss term in the radar
range equation (see Chapter 5). There is also a time delay resulting from filtering. Time delay
through receiver and signal processer circuitry is accounted for via radar calibration.
Figure 7.8 Envelope of an ideal and actual pulse at the matched filter input.

Figure 7.9 Plot of matched filter output for an unmodulated pulse.
For LFM pulses, the response of Figure 7.7 is representative of an actual response, except
that it is likely it will not be perfectly symmetric as shown in that figure.
For cases where the target is large relative to the range resolution, the peak of the matched
filter output may not simply broaden but could exhibit several peaks. This will also translate
to an SNR loss relative to the case of a point target with the same RCS, since the target RCS
will be essentially distributed in range. It is something that must be taken into account when
analyzing the output of the detection logic, τ. This problem arises in radars that have very
narrow compressed pulsewidths (less than about 0.05 to 0.1 µs depending upon target size) or
very large targets such as ships or very large aircraft (e.g., blimps).
In some radars, the designers intentionally use filters that are not matched to the transmit
pulse. The most common case is an LFM pulse where the filter is intentionally mismatched to
reduce the range (time) sidelobes (the lobes around the main lobe—see Figure 7.7) [7]. In
such cases, the designer is concerned with interference (from other targets or clutter) and is
willing to accept the loss in SNR caused by using a mismatched filter.
Radars that use unmodulated pulses and analog processing may not include a matched
filter, per se. Instead, they use a narrowband filter that will pass most of the pulse power and
minimize the noise power to some extent. This approach is usually taken as a cost savings
where the designer is willing to accept the potential 1 or 2 dB loss in SNR associated with
such an implementation.
In analog radars that use LFM pulses, the matched filter can be implemented with surface or
bulk wave acoustic devices with piezo-electric transducers at the input and output [8, 9] and, in
some instances, lumped parameter filters. In modern radars that use digital signal processing,
the matched filter could be implemented using a fast convolver based on fast Fourier
transformers (FFTs) or some other digital processing methodology.
7.6 EXERCISES
1.
Derive (7.59) and show that it can be combined with (7.58) and so(t) = 0 for |t| > τp to

arrive at (7.60).
2.
Derive so(t) for the unmodulated pulse of Section 7.4.2 when the phase is θ(t) = 2πfIFt + ϕ
instead of a constant. You will note that so(t) is not real as was the case of the example.
Because of this, one would plot |so(t)| versus t instead of so(t) versus t.
3.
Plot Re[s(t)] versus t for –5 µs ≤ t ≤ 20 µs, a pulsewidth τp = 15 µs, a chirp bandwidth B
= ατp = 1 MHz, and an amplitude A = 1. Use θ(t) = 2πfIFt + παt2 instead of θ(t) = παt 2.
Let fIF = 1.5 MHz. This plot illustrates the increasing frequency behavior of an LFM
pulse.
4.
Plot Re[s(t)] versus t for –5 µs ≤ t ≤ 20 µs, a pulsewidth τp = 15 µs, a chirp bandwidth B
= ατp = –1 MHz, and an amplitude A = 1. Use θ(t) = 2πfIFt + παt 2 instead of θ(t) = παt2.
Let fIF = 1.5 MHz. This plot illustrates the decreasing frequency behavior of an LFM
pulse for a negative chirp slope.
5.
Plot |so(t)| for an LFM pulse with amplitude A = 1, pulsewidth τp = 15 µs, and LFM
bandwidths of 0.2, 0.5, 2.0, and 5.0 MHz. Note the difference in the sidelobe structure.
6.
Find an equation for the impulse response, h(t), of a matched filter for a pulse defined
by
where β < 0. Sketch s(t) and h(t). Find and sketch the matched filter output, so(t).
References
[1]
Van Vleck, J. H., and D. Middleton, “A Theoretical Comparison of the Visual, Aural, and Meter Reception of Pulsed
Signals in the Presence of Noise,” J. Appl. Phys., vol. 17, no. 11, Nov. 1946, pp. 940–971.
[2]
North, D. O., “An Analysis of the Factors which Determine Signal/Noise Discrimination in Pulsed Carrier Systems,” RCA
Labs, Princeton, NJ, Tech. Rep. PTR-6C, Jun. 25, 1943. Reprinted: Proc. IEEE, vol. 51, no. 7, Jul. 1963, pp. 1016–1027.
Reprinted: Detection and Estimation (S. S. Haykin, ed.), Stroudsburg, PA: Halstad Press, 1976, pp. 10–21.
[3]
Papoulis, A., Probability, Random Variables, and Stochastic Processes, 3rd ed., New York: McGraw-Hill, 1991.
[4]
Budge, M. C., Jr., “EE 619: Intro to Radar Systems,” www.ece.uah.edu/courses/material/EE619/index.htm.
[5]
Urkowitz, H., Signal Theory and Random Processes, Dedham, MA: Artech House, 1983.
[6]
Barkat, M., Signal Detection and Estimation, 2nd ed., Norwood, MA: Artech House, 2005.
[7]
Klauder, J. R. et al., “The Theory and Design of Chirp Radars,” Bell Syst. Tech. J., vol. 39, no. 4, Jul. 1960, pp. 745–808.
[8]
Brookner, E., Radar Technology, Dedham, MA: Artech House, 1977.
[9]
Skolnik, M. I., ed., Radar Handbook, New York: McGraw-Hill, 1970.
1 The PS and PN in this chapter are the same as in Chapter 6.

2 In practice, the matched filter output is scaled to some convenient level, for example, unity signal gain through the matched
filter, some peak voltage via AGC, since only proportionality is required by (7.21).

Chapter 8
Detection Probability Improvement Techniques
8.1 INTRODUCTION
In Chapters 6 and 7, we derived equations for single-pulse detection probability and showed
that the use of a matched filter provides the maximum SNR and Pd that can be obtained for a
given set of radar parameters and a given, single transmitted pulse. We termed the resultant
SNR and Pd single-pulse SNR and Pd. We now want to address the improvement in Pd that we
can obtain by using multiple transmit pulses. We will examine four techniques:
1. Coherent integration;
2. Noncoherent integration;
3. m-of-n detection; and
4. Cumulative probability.
According to a correspondence from David K. Barton,1 the earliest published mention of
coherent integration was in a paper by D. O. North [1] where he discussed coherent
integration and some of the problems (that he saw at the time) of implementation. In a 1950
book by Lawson and Uhlenbeck [2], the authors referenced a 1944 MIT Radiation
Laboratories report by Emslie titled “Coherent Integration” [3]. In the early 1950s, Lincoln
Laboratories developed a pulsed-Doppler radar called Porcupine [4], and in 1956,
Westinghouse built an airborne intercept radar using coherent integration in the form of a
Doppler processor. By 1957, coherent integration was being widely discussed in literature
[5–8].
Noncoherent integration has apparently been used since the early days of radar since this is
the type of integration performed by radar displays such as A-scopes, plan position indicators
(PPIs), and the like [2, 9]. However, it appears that the first rigorous treatment of noncoherent
integration was presented by Marcum in his seminal paper [10]. Shortly after Marcum
published his paper, Swerling expanded upon Marcum’s analyses and considered noncoherent
integration of signal returns for his four target fluctuation models [11].
The first publication of a paper on m-of-n detection, which is also known as binary
integration, coincidence detection, and dual threshold detection, appears to have been the 1955
paper by Harrington [12]. Papers by Dinneen and Reed in 1956 [13] and Schwartz [14]
followed.
Marcum discussed cumulative detection in his 1947 paper [10]. Hall also discusses
cumulative detection probability in a 1956 paper [15].

8.2 COHERENT INTEGRATION
With coherent integration, we insert a coherent integrator (a type of signal processor)
between the matched filter and amplitude detector, as shown in Figure 8.1. This coherent
integrator adds returns (thus the word integrator) from n pulses. After accumulating the n-
pulse sum, amplitude detection and the threshold check are performed.
In practice, the process of forming the n-pulse sum is somewhat complicated. In one
implementation, the coherent integrator samples the return from each transmit pulse at a
spacing equal to the range resolution of the radar. Thus, for example, if we are interested in a
range window from 5 to 80 km and have a range resolution of 150 m, the signal processor
forms 75,000/150 or 500 samples for each pulse return. The coherent integrator stores the
500 samples for each pulse. After it has stored n sets of 500 samples, it sums across n to form
500 sums. In modern, phased array radars with digital signal processors, the summation is
accomplished by summers or FFTs. In older, analog radars, the summation (integration) is
performed by filters [16] or integrate-and-quench circuits similar to those used in
communications receivers.
We will first consider the effects of coherent integration on SNR and then discuss its effect
on Pd. As we did in previous chapters, we will separately consider the signal and noise for the
SNR analysis and noise and signal-plus-noise for the detection analyses.
Figure 8.1 Location of the coherent integrator.
8.2.1 SNR Analysis
For the signal, we assume the complex amplitude of the signal on pulse k at the matched filter
output is given by
where S > 0 is the signal amplitude and θ is the phase. We assume we are looking at the
specific range cell—out of the 500 discussed in the above example—that contains the target
return. Further, the sample timing corresponds to matched range (see Chapter 9).
The formulation of s(k) in (8.1) carries several assumptions about the target. It implies that
the amplitude and phase of the signal returned from the target is constant, at least over the n
pulses that are to be integrated. This means we are assuming the target is SW0/SW5, SW1, or
SW3. It does not admit SW2 or SW4 targets. As we will show later, coherent integration
offers no SNR benefit for SW2 and SW4 targets.
The formulation also implies there is nothing in the radar or environment that would cause

the signal amplitude or phase to vary across the n pulses. In particular, the radar and
environment must be such that all of the parameters of the radar range equation remain
constant across the n pulses. Thus, for example, the antenna beam must be stationary, the
transmit power must be constant, the target must be stationary, the radar frequency must be
constant, the parameters of the radar receiver must not change, and the environment between
the radar and the target must not change.
Another implication of (8.1) is that there is no Doppler on the target return. If the target is
moving, it will have a Doppler frequency and thus a changing phase. This Doppler frequency
must be removed by the coherent integrator before the summation takes place. In digital
signal processors that use FFTs, Doppler removal is effectively accomplished by the FFT. In
analog processors, Doppler is removed through the use of bandpass filters tuned to various
Doppler frequencies that cover the range of expected Doppler frequencies, or by mixers
before the integrate-and-quench circuits.
It should be noted that not all of the aforementioned constraints can be perfectly satisfied.
We account for the fact that some will be violated by including loss terms in the radar range
equation. These were discussed in Chapter 5 and will be reviewed later in this chapter.
If we sum over n pulses, the output of the summer will be (for the range cell or sample
being investigated)
If the signal power at the input to the summer is
the signal power at the output of the summer will be
In these equations, PS is the single-pulse signal power from the radar range equation. We can
write the noise at the input to the coherent integrator on the kth pulse as
Consistent with our previous noise discussions (see Chapters 6 and 7), we assume nI(k) and
nQ(k) are wide sense stationary (WSS), zero mean, and independent. They each have a
variance of σ2. Although we do not need it here, we will also assume they are Gaussian.
If we sum the n pulses, the noise at the output of the summer will be

The noise power at the output of the summer will be
In (8.7), we made use of the fact that nI(k) and nQ(k) being independent and zero mean implies
that noutI and noutQ are independent and zero mean.
We can write
Since nI(k) is WSS and zero mean,
We also assume the noise samples are uncorrelated from pulse to pulse.2 This means nI(k) and
nI(l) are uncorrelated ∀k ≠ l. Since nI(k) and nI(l) are also zero mean, we get
If we use (8.9) and (8.10) in (8.8), we get
where Pnin is the noise power at the output of the matched filter (the “single-pulse” noise term
from the radar range equation with B=1/τp; see Chapters 2 and 4).
By similar reasoning, we have
and, from (8.7),

If we combine (8.4) and (8.13), we find that the SNR at the output of the coherent integrator
is or n times the SNR at the output of the matched filter (the SNR given by the radar range
equation). With this, we conclude the coherent integrator provides an SNR gain, or SNR
improvement, of n.
If the target is SW2 or SW4, coherent integration does not increase SNR. This stems from
the fact that, for SW2 and SW4 targets, the signal is not constant from pulse to pulse but,
instead, behaves like noise. This means we must treat the target signal the same as we do
noise. Thus, in place of (8.2), we would write
Following the procedure we used for the noise case, we have
and
This leads to the result
In other words, the SNR at the coherent integrator output would be the same as the SNR at the
matched filter output, and the coherent integrator would offer no integration gain.
8.2.2 Detection Analysis
We have addressed the signal power, the noise power, and the SNR at the output of the
coherent integrator. In order to compute Pd, we need to consider the forms of the density
functions of the noise and signal plus noise at the output of the signal processor. We address
the noise first.
From (8.6), we have

We already made the assumption that the nI(k) and nQ(k) are independent, zero-mean,
Gaussian random variables with equal variances of σ2. This means noutI and noutQ are zero-
mean, Gaussian random variables and have variances of nσ2/2. They are also independent.
This is exactly the same as the conditions we had on the I and Q components of noise in the
single-pulse case. This means the density of the noise magnitude, Nout, at the detector output
will be of the form of (6.14) (Chapter 6), and the Pfa equation is given by (6.104). They will
differ in that the σ2 in these two equations will be replaced by nσ2. The specific equations are
and
where TNR is the threshold to noise ratio used in the detection logic (see Chapter 6).
We now turn our attention to signal plus noise. For the SW0/SW5 target, we can write the
signal-plus-noise voltage at the coherent integrator output as
where each of the vI(k) and vQ(k) are independent, Gaussian random variables with equal
variances of σ2. The mean of vI (k) is Scosθ, and the mean of vQ(k) is Ssinθ (see Section 6.4 of
Chapter 6). With this, voutI and voutQ are also Gaussian. Their variances are equal to nσ2 and
their means are nScosθ and nSsinθ. They are also independent. In this case, the density of the
signal-plus-noise magnitude, vout, at the detector output is of the form given in (6.75) with S
replaced by nS and σ2 replaced by nσ2. With this, we conclude Pd is given by (6.116) with SNR
replaced by
where SNR is the single-pulse SNR given by the radar range equation. Specifically, we have

where, from Chapter 6, Q1(a,b) is the Marcum Q function.
For the SW1 and SW3 target, we need to take an approach similar to that used in Chapter 6
for SW3 targets. For SW1 and SW3 targets, the signal amplitude, S, and phase, θ, are constant
across the n pulses that are coherently integrated. However, the amplitude of the group of
pulses, termed the coherent dwell, is governed by the SW1 or SW3 amplitude fluctuation
density [see (6.40) and (6.49)]. The phase of the group of pulses is governed by the uniform
probability density function as discussed in Chapter 6. This means that, during the n pulses,
the signal plus noise for SW1 and SW3 targets is the same form as for the SW0/SW5 target.
That is, vI(k) and vQ(k) are independent, Gaussian random variables with variances of σ2 and
means of Scosθ and Ssinθ. This implies that the densities of voutI and voutQ, given that S and θ
are fixed, are also Gaussian with variances of nσ2/2 and means of nScosθ and nSsinθ. This
was the same form of the conditional density presented in Chapter 6. If we follow this
argument through and follow the procedure of Chapter 6, we can derive the density function
of the magnitude of vout as
for the SW1 target and
for the SW3 target.
By performing the appropriate integrations, we can show the equations for Pd are of the
same form as (6.123) and (6.127) with SNR replaced by nSNR. In particular,
for SW1 targets and
for SW3 targets.
For a SW2 target, the signal-plus-noise, 
, is independent from pulse to
pulse (across the n pulses). Further, vI(k) and vQ(k) are zero mean and Gaussian with

variances of Ps + σ2 [see (6.57)]. Their sums are also zero mean and Gaussian, but have
variances of n(Ps + σ2)/2. This means the magnitude of vout has the density
By performing the appropriate integration, we find Pd is as given by (6.123), with SNR
equal to the single-pulse SNR. In other words, the coherent integrator does not improve
detection probability.
Derivation of a similar result for SW4 targets is not as easy as for SW2 targets because we
cannot claim that vI(k) and vQ(k) are Gaussian for SW4 targets. This means we cannot easily
find the density functions of the coherent integrator output, voutI and voutQ, for the SW4 target.
Without these density functions, we cannot compute Pd. As a consequence, we have no
rigorous mathematical basis for claiming that coherent integration will or will not improve
Pd for a SW4 target. The standard assumption appears to be that, like SW2 targets, coherent
integration offers no Pd improvement for SW4 targets.
In the above development, we made some ideal assumptions concerning the target, radar,
and environment based on the fact that we were collecting and summing returns from a
sequence of n pulses. In particular, we assumed the target amplitude was constant from pulse
to pulse. Further, we assumed that we sampled the output of the matched filter at its peak. In
practice, neither of these is strictly true. First, we really cannot expect to sample the matched
filter output at its peak. Because of this, the SNR in the Pd equations will not be the peak SNR
at the matched filter output (the SNR given by the radar range equation). It will be some
smaller value. We usually account for this by degrading SNR by a factor we call range
straddling loss [17, p. 236] (see Chapter 5). If the samples (the 500 samples of the
aforementioned example) are spaced one range resolution cell apart, the range straddling loss
is usually taken to be 3 dB.
There are other reasons that the signal into the coherent integrator will vary. One is target
motion. This will create a Doppler frequency, which will cause phase variations from pulse to
pulse (which translate to amplitude variations in the I and Q components). If the Doppler
frequency is large enough to cause large phase variations, the gain of the coherent integrator
will be nullified. In general, if the Doppler frequency is greater than about PRF/n, the
coherent integration gain will be nullified. In fact, the coherent integration could result in an
SNR reduction. Doppler frequency offsets can be circumvented by using banks of coherent
integrators that are tuned to different Doppler frequencies. This is usually accomplished by
FFTs in digital signal processors and bandpass filters in analog processors.
Another degradation related to Doppler is termed range gate walk. Because of the nonzero
range rate, the target signal will move relative to the time location of the various samples fed
to the coherent integrator. This means that, over the n pulses, the signal amplitude will change.
As indicated above, this could result in a degradation of SNR at the output of the coherent

integrator. In practical radars, designers take steps to avoid range walk by not integrating too
many pulses. Unavoidable range walk is usually accounted for by including a small (less than
1 dB) SNR degradation (SNR loss). Also, if the radar computer has some knowledge of target
range rate, it can adjust the range samples to account for range walk. This is reasonably easy
to accomplish when the radar is tracking. It may be more difficult during search.
Still another factor that causes the signal amplitude to vary is the fact that the coherent
integration may take place while the radar scans its beam across the target. The scanning
beam will cause the GT and GR terms in the radar range equation to vary across the n pulses
that are coherently integrated. As before, this will degrade the SNR, and its effects are
included in what is termed a beamshape loss [17, p. 493] (Chapter 5). This loss, or
degradation, is usually 1 to 3 dB in a well-designed radar.
Phased array radars have a similar problem. For phased array radars, the beam does not
move continuously (in most cases), but in discrete steps. This means the phased array radar
may not point the beam directly at the target. In turn, the GT and GR of the radar range
equation will not be their maximum values. As with the other cases, this phenomena is
accommodated through the inclusion of a beamshape loss term (see Chapter 5).
8.3 NONCOHERENT INTEGRATION
We now want to discuss noncoherent, video, or post-detection integration. The term post-
detection integration derives from the fact that the integrator, or summer, is placed after the
amplitude or square law detector, as shown in Figure 8.2. The term noncoherent integration
derives from the fact that, since the signal has undergone amplitude or square law detection,
the phase information is lost. The synonym video appears to be a carryover from older radars
and refers to the video displayed on PPIs, A-scopes, and the like. The noncoherent integrator
operates in the same fashion as the coherent integrator in that it sums the returns from n
pulses before performing the threshold check. However, where the coherent integrator
operates on the output of the matched filter, the noncoherent integrator operates on the output
of the amplitude detector.
Figure 8.2 Location of the noncoherent integrator.
A noncoherent integrator can be implemented in several ways. In older radars, it was
implemented via the persistence on displays plus the integrating capability of a human
operator. These types of noncoherent integrators are very difficult to analyze and will not be
considered here. The reader is referred to [2, 9, 16].
A second implementation is termed an m-of-n detector and uses more of a logic circuit
rather than a device that integrates. Simply stated, the radar examines the output of the
threshold device for n pulses. If a DETECT is declared on any m or more of those n pulses,
the radar declares a target detection. This type of implementation is also termed a dual

threshold detector or a binary integrator [18–20]. We will consider this type of noncoherent
integrator later in this chapter.
The third type of noncoherent integrator is implemented as a summer or integrator. In
older radars, lowpass filters were used to implement them. In newer radars, they are
implemented in special purpose hardware or the radar computer as digital summers.
In a fashion similar to coherent integration, the noncoherent integrator samples the
(amplitude detected) return from each transmit pulse at a spacing equal to the range resolution
of the radar. Repeating the previous example, if we are interested in a range window from 5 to
80 km and have a range resolution of 150 m, the noncoherent integrator forms 75,000/150 or
500 samples for each pulse return. The noncoherent integrator stores the 500 samples for
each pulse. After it has stored n sets of 500 samples, it sums across n to form 500 sums.
For SW0/SW5, SW1, and SW3 targets, the main advantage of a noncoherent integrator
over a coherent integrator is hardware simplicity. As indicated in earlier discussions,
coherent integrators must contend with the effects of target Doppler. In terms of hardware
implementation, this usually translates to increased complexity of the coherent integrator.
Specifically, it is usually necessary to implement a bank of coherent integrators that are tuned
to various Doppler frequencies. Because of this, one will need a number of integrators equal
to the number of range cells in the search window multiplied by the number of Doppler bands
needed to cover the Doppler frequency range of interest. Although not directly stated earlier,
this will also require a larger number of amplitude (or square law) detectors and threshold
devices.
Since the noncoherent integrator is placed after the amplitude detector, it does not need to
accommodate multiple Doppler frequencies. This lies in the fact that the amplitude detection
process recovers the signal (plus noise) amplitude without regard to phase (i.e., Doppler).
Because of this, the number of integrators is reduced; it is equal to the number of range cells
in the search window.
Recall that coherent integration offers no improvement in detection probability for SW2 or
SW4 targets. In fact, it can degrade detection probability relative to that which can be obtained
from a single pulse. In contrast, noncoherent integration can offer significant improvement in
detection probability relative to a single pulse. It is interesting to note that some radar
designers are using various schemes, such as frequency hopping, to force targets to exhibit
SW2 or SW4 characteristics and exploit the significant detection probability improvement
offered by noncoherent integration [21, 22].
Analysis of noncoherent integrators is much more complicated than analysis of coherent
integrators because the integration takes place after the nonlinear process of amplitude or
square law detection. From our previous work in Chapter 6, we note that the density functions
of the magnitude of noise and signal-plus-noise are somewhat complicated. More
importantly, they are not Gaussian. Therefore, when we sum the outputs from successive
pulses, we cannot conclude that the density function of the sum of signals will be Gaussian (as
we can if the density function of each term in the sum was Gaussian). In fact, the density
functions become very complicated. This has the further ramification that the computation of

Pfa and Pd becomes very complicated. Analysts such as DiFranco and Rubin, Marcum,
Swerling, and Meyer and Mayer have devoted considerable energy to analyzing noncoherent
integrators and documenting the results of these analyses [10, 11, 23, 24]. We will not attempt
to duplicate the analyses here; instead, we present the results of their labor.
An equation for Pfa at the output of an n-pulse, noncoherent integrator is
where Γ(n,TNR) is the incomplete gamma function [25, p. 112] defined by3
For a = n, where n is a positive integer, Γ(n) becomes the factorial operation [26, p. 98]. That
is
Many modern software packages, such as MATLAB and Mathcad®, include the incomplete
gamma function in their standard library. These software packages also have the inverse
incomplete gamma function, which is necessary for determining TNR for a given Pfa.
Specifically,
where Γ−1(n,1 − Pfa) is the inverse of the incomplete gamma function.
The Pd equations for the five target types we have studied are
SW0/SW5:
SW1:

SW2:
SW3:
SW4:
In the above, Q1(a,b) is the Marcum Q function, Ir(x) is the modified Bessel function of the
first kind and order r [26, p. 104], and Γ(a,x) is the aforementioned incomplete gamma
function. TNR is the threshold-to-noise ratio and is computed from (8.33).
SNR in the above equations is the single-pulse SNR defined by the radar range equation (see
Chapter 2).
With the exception of (8.37), (8.34) through (8.38) are exact equations. Equation (8.37) is
usually taken to be an exact equation, but is actually an approximation, as indicated by the use
of ≈ instead of =. An exact equation for the SW3 case can be found in the appendix of [24].
In a paper [27] and his recent books [17, 25], Barton provides a set of “universal” equations
for the SW1 through SW4 cases. He attributes the original formulation of these equations to
the Russian author, P. A. Bakut [28]. The universal equation for Pd is
where
and TNR is computed from (8.33). SNR is the single-pulse SNR.
The integer, ne is the number of degrees of freedom associated with the different Swerling

target types (see Chapter 3). This stems from Swerling’s definition of his four target types or,
more accurately, signal fluctuation models [11, 27]. Specifically, he defined four signal
fluctuation models whose amplitude statistics are governed by a chi-square density function
having 2ne degrees of freedom (DOF). The four values of ne associated with the four
Swerling target types are:
• SW1, ne = 1
• SW2, ne = n
• SW3, ne = 2
• SW4, ne = 2n
where n is the number of pulses noncoherently integrated.
Barton also gives a universal equation for determining the single-pulse SNR required to
provide a desired Pd. This equation is quite useful. Before its introduction, the single-pulse
SNR was found by using a root solver in conjunction with the exact equations of (8.35) to
(8.38). The “inverse” universal equation is
where
and Γ−1(k,z) is the inverse of the incomplete gamma function.
Barton compared the universal equations to the exact equations for several values of Pfa
and a range of SNRs and n [27]. His results indicate that the universal equations are quite
accurate for Pd greater than about 0.2 and Pfa less than about 10−4. As an interesting note, the
universal equation are exact for SW2 targets.
The universal equations are not recommended for SW0/SW5 targets. However, in his 2005
book [17, pp. 42–53], Barton provides an approximation to the exact equation of (8.34), along
with its inverse. Those equations are, using Barton’s notation
and

where
and
erfc(x) is the complementary error function [29, p. 214] and Q−1(x) is its inverse, erfc−1(x).
Both of these functions are included as standard functions in software packages such as
MATLAB, and Mathcad.
In an internal memo,4 Hardaker recasts (8.43) through (8.45) in a form directly in terms of
erfc(x) and erfc−1(x). These are
and
with
The noncoherent Pd equations discussed herein are based on the assumption that the
amplitude detector of Figure 8.2 is a square law detector.5 According to Meyer and Mayer
[24], Marcum [30] considered the effect on Pd of using an amplitude (linear) detector instead
of a square law detector. Marcum showed that the Pd performance using either detector was
very similar (~0.2-dB difference) for a constant RCS target (SW0/SW5 target). It is not clear
whether Swerling or other analysts have performed such a comparison for other Swerling
target types. However, it is commonly accepted that the Pd equations developed for the square

law detector also apply to the case where the radar use a linear detector.
8.3.1 Example 1
Figures 8.3 through 8.7 contain plots that provide a comparison of coherent integration,
noncoherent integrations and single-pulse operation for the five Swerling target models. The
figures contain plots of Pd versus required single-pulse SNR. Figures 8.3, 8.4, and 8.6 contain
plots for the single-pulse case and two sets of two plots for the cases of coherent and
noncoherent integration of 10 and 100 pulses. Figures 8.5 and 8.7 contain plots for the single-
pulse case and noncoherent integration of 10 and 100 pulses. Coherent integration was not
considered for the SW2 and SW4 targets since we already concluded that coherent integration
offers no improvement in Pd for these two target types.
All of the plots were generated for a Pfa of 10−6. As a reminder, for the coherent and
noncoherent integrators, this is the Pfa at the output of the detector that follows the integrators.
For the single-pulse case, it is the Pfa for a single detection attempt (i.e., no integration). The
“required single-pulse SNR” label on the horizontal axis means that this is the SNR required
at the matched filter output to achieve the indicated Pd at the output of the threshold device that
follows the coherent or noncoherent integrator, or the matched filter for the single-pulse case.
As expected, Figures 8.3, 8.4, and 8.6 show that, with coherent integration of 10 pulses, the
required single-pulse SNR is 10 dB lower than when only a single pulse is used. For coherent
integration of 100 pulses, the required single-pulse SNR is 20 dB lower. For noncoherent
integration, the reduction in required single-pulse SNR depends upon the number of pulses
noncoherently integrated and the desired Pd after integration. Examples of the reduction for
the three target types (SW0/SW5, SW1, and SW3) are contained in Table 8.1. As indicated, the
values range from 7 to 8 dB for noncoherent integration of 10 pulses and 14 to 15 dB for
noncoherent integration of 100 pulses. This relation leads to a useful rule of thumb for the
reduction in required single-pulse SNR for noncoherent integration. Specifically, the
reduction is
Some authors term I(n) noncoherent integration gain [31, 32]. For preliminary calculation
of Pd, they suggest adding I(n) to the single-pulse SNR (from the radar range equation) and
using it in the single-pulse Pd equation to compute Pd at the output of the noncoherent
integrator (for SW0/SW5, SW1, and SW3 targets).
The curves for the SW2 (Figure 8.5) and SW4 (Figure 8.7) indicate that noncoherent
integration can offer significant reductions in single-pulse SNR requirements when compared
to basing detection on only a single pulse. For example, for a SW2 target and 10 pulses
integrated, the reduction is about 15 dB for a desired Pd of 0.9. This increases to 23 dB for a
desired Pd of 0.99. For 100 pulses integrated, the reductions are 22 and 31 dB for the two Pd
cases. This is a significant reduction in single-pulse SNR requirements and is a reason for

radar designers to try to arrange for aircraft targets to appear as SW2 targets to the radar.
The reduction in single-pulse SNR requirements is not as dramatic for the SW4 case, but
they are still significant, as indicated by Figure 8.7.
Table 8.1
Reduction in Required Single-Pulse SNR for Noncoherent Integration

Figure 8.3 Plots of desired Pd vs required single-pulse SNR for a SW0/SW5 target and coherent and noncoherent integration
of 10 and 100 pulses—Pfa = 10−6.

Figure 8.4 Plots of desired Pd vs required single-pulse SNR for a SW1 target and coherent and noncoherent integration of 10
and 100 pulses—Pfa = 10−6.

Figure 8.5 Plots of desired Pd vs. required single-pulse SNR for a SW2 target and noncoherent integration of 10 and 100
pulses—Pfa = 10−6.

Figure 8.6 Plots of desired Pd vs. required single-pulse SNR for a SW3 target and coherent and noncoherent integration of
10 and 100 pulses—Pfa = 10−6.

Figure 8.7 Plots of desired Pd vs. required single-pulse SNR for a SW4 target and noncoherent integration of 10 and 100
pulses—Pfa = 10−6.
8.3.2 Example 2
For this example, we consider the radar of Example 2 in Section 6.7. The radar parameters
are listed in Table 2.2 of Chapter 2 and are repeated in Table 8.2. We have added some specific
antenna parameters since we will need to use them in this example. We assume the radar has a
fan beam (see Figure 6.9) with a peak directivity of 32 dB. The azimuth beamwidth is 1.3°.
We want to generate a plot of detection probability versus target range for a 0.1 m2, SW1
target. We will assume cases where the radar coherently and noncoherently integrates the
number of pulses received as the beam scans by the target. We will assume the elevation to the
target is at the peak of the antenna beam in elevation.
Table 8.2

Radar Parameters for Example 2
Parameter
Value
Peak transmit power at the power amp output
50 kW
Operating frequency
2 GHz
PRF
1,000 Hz
Pulsewidth—τp
100 µs
Pulse modulation bandwidth
1 MHz
Antenna directivity
32 dB
Elevation beamwidth
Fan beam
Azimuth beamwidth
1.3°
Total losses—excluding beamshape loss
13 dB
Noise figure—referenced to the antenna feed
5 dB
Antenna rotation rate
6 rpm
Instrumented range
PRI − 2τp − 50 μs
False alarm criterion
No more than one false alarm per 360° rotation
To start, we need to find the single-pulse SNR for the case where the radar beam is pointed
directly at the target. We use the radar range equation of Chapter 2 to obtain this. That is,
We next need to compute the number of pulses that can be coherently or noncoherently
integrated. We said this would be the number of pulses received as the beam scans (in
azimuth) across the target. The standard way to compute this is to see how many pulses are in
the 3-dB azimuth beamwidth, which is 1.3° in this example.
The antenna rotation rate is 6 rpm or 6 × 360° per minute. This gives a scan rate of
The time for the antenna to travel one beamwidth is
The waveform PRF is 1,000 Hz, which means the radar transmits (and receives) 1,000
pulses per second. We can use this to compute the number of pulses per beam as
This tells us that we can coherently or noncoherently integrate up to 36 pulses as the beam

scans by the target. Thus, this is the n we need to include in the appropriate Pd equation.
We note that, as the beam scans by the target, the SNR associated with the 36 pulses will not
be constant. As we discussed earlier, we will account for this by incorporating a beamshape
loss in the computation of single-pulse SNR. Since we assumed the target was on the peak of
the antenna pattern in elevation, we need only account for the variation of SNR due to azimuth
scanning. This means that we need to include an additional 1.6 dB loss to the single-pulse SNR
calculation [33].6 This reduces the single-pulse SNR we use in the detection calculations to
As a note, we will assume the coherent integrator has been tuned to the Doppler frequency
of the target. This is an idealization, since we do not know the target Doppler in a search
radar. In practice, the coherent integrator would actually consist of many coherent integrators
tuned to different Doppler frequencies. As indicated earlier, this complicates the design of the
coherent integrator. Also, because of the multiple Doppler channels, with their associated
detection circuits, we should adjust the Pfa to account for the multiple Doppler channels.
The other term we need is the Pfa at the output of the integrator. We will assume that the
integrator performs a running sum, or integration, and makes a detection decision on every
pulse. Thus, we can use the Pfa we computed in Example 2 of Chapter 2. That Pfa was 1.33 ×
10−7.
To create plots of Pd versus range, we use the SNR from (8.55) along with n = 36 and Pfa =
1.33 × 10−7 in (8.27) and (8.35). The specific equations are
for coherent integration and
for noncoherent integration.
Figure 8.8 contains plots of Pd versus R for the two integration cases. As expected, the
coherent integrator allows a given Pd at longer ranges. Also, coherent integration gives Pd =
50% at 90 km, which is not achievable with noncoherent integration until 70 km.

Figure 8.8 Plots of Pd vs. slant range for Example 2.
8.4 CUMULATIVE DETECTION PROBABILITY
The third technique we examine for increasing detection probability is the use of multiple
detection attempts. The premise behind using multiple detection attempts is that if we attempt
to detect the target several times, we will increase the overall detection probability. We can
formally state the multiple detection problem as follows.
If we check for a threshold crossing on several occasions, what is the probability that the
signal-plus-noise voltage will cross the threshold at least once? Thus, suppose for
example we check for a threshold crossing on 3 occasions. We want to determine the
probability of a threshold crossing on any 1, 2, or 3 of the occasions.
To compute the appropriate probabilities, we must use probability theory. The details are
somewhat involved and are presented in Appendix 8C. The main results are as follows.
We assume we have N detection events (i.e., the chance to detect the target on N tries) and
that they are independent. This limits when we can use cumulative detection concepts.
Specifically, we should use cumulative detection concepts only on a scan-to-scan basis. If we
do, we will satisfy the constraints on all of the Swerling target types. Specifically, for SW1
and SW3 targets, the signal-plus-noise samples are, by definition, independent from scan to
scan. For SW0/SW5, SW2, and SW4 targets, the signal-plus-noise samples are independent
from pulse to pulse and will thus also be independent from scan to scan. Having said this, we
must also assure that the coherent or noncoherent integrator does not cause the independence
restriction to be violated. The restriction will not be violated if the time between target
illuminations is significantly larger than the coherent or noncoherent processing time.

If the detection probability on each detection try is Pdk, the probability of detecting the
target on at least one of the tries is
where Pdcum is the cumulative detection probability over N tries.
In addition to increasing detection probability, the use of cumulative detection techniques
also increases false alarm probability. In fact, if we consider the false alarm case, we can
express (8.58) as
In the case of false alarm probability, we usually have that Pfak=Pfa ∀k ∈[1,N] and write
If we further recall that Pfa≪1 we can write
Equation (8.61) tells us that when we use cumulative detection concepts, we should compute
the individual Pdk detection probabilities using Pfak = Pfacum/N where Pfacum is the desired
false alarm probability.
As a rule of thumb, one should be careful about invoking cumulative detection concepts in
a fashion that allows any Pdk to be such that the SNR per scan falls below 10 to 13 dB. If the
SNR is below 10 to 13 dB, the radar may not be able to establish track on a target it has
detected. If this is stated in terms of Pdk, Pdk should not be allowed to fall below about 0.5. A
reviewer of this book pointed out that this rule of thumb may not be “applicable to a
multifunction radar or a system in which a multiple-target phased array tracker is assigned to
validate a single detection from an associated search radar.” It is assumed that the rationale
behind this statement is that even if the detection probability is low (because of losses
associated with computing SNR during detection), the SNR on verify may be sufficient to
establish track since it may be possible to devote more radar resources to verify, and thus
increase SNR.
8.4.1 Example 3
Suppose we have a phased array radar that is performing search. It illuminates the target with
a search beam every 20 seconds and transmits a single pulse. We will assume an aircraft-type
target, which means we can assume it is a SW1 target. Because of this and the 20 seconds

between search illuminations, we can safely assume the signal-plus-noise samples will be
independent from look to look.
We will assume the radar achieves detection probabilities of 0.5, 0.51, 0.52, 0.53, and 0.54
on five consecutive search beams (looks). With this we get a cumulative detection probability
of
Suppose we want to compute the required SNRs on each look to achieve the various Pdk and
obtain a Pfa of 10−6 over the five looks. From (8.61) we get
This says we must set the detection threshold for each threshold check such that we obtain
We can then use (6.123) to determine the required SNR values. Specifically, we would have
This would give SNR values of 13.3, 13.4, 13.5, 13.7, and 13.8 dB.
8.5 M-OF-N DETECTION
We can think of m-of-n detection as an extension of cumulative detection where, instead of
requiring one or more detections on n tries, we require m or more detections on the n tries.
This is the origin of the term m-of-n detection [34–36].
Since m-of-n detection usually operates on target returns that are closely spaced in time, we
cannot necessarily assume independent detection events. However, for the same reason, we
can reasonably assume the detection probability will be the same on each detection attempt.
That is, Pdk = Pd.
For the case of SW0/SW5, SW2, and SW4 targets, we can assume the detection events are
independent. For SW0/SW5 targets, the randomness in the signal-plus-noise is due only to
noise since the signal amplitude is a known constant across the n detection tries. Since we
assume the noise is independent from pulse to pulse, the detection events will be independent
from try to try.

For SW2 and SW4 targets, we assume both the signal amplitude and noise are random, and
independent, from pulse to pulse. Thus, the detection events will be independent from try to
try.
Based on the discussions of the previous two paragraphs, we can directly extend the
cumulative detection discussions to m-of-n detection for SW0/SW5, SW2, and SW4 targets.
For SW1 and SW3 targets, the signal amplitude is constant across the n detection tries.
However, it is not a known constant, as was the case for SW0/SW5 targets. Instead, it is a
random variable that is governed by (6.40) and (6.49).
To accommodate the fact that the signal amplitude is random, we will approach the SW1
and SW3 m-of-n detection problem by using the approach we used in Chapter 6 to find Pd for
SW1 and SW3 targets. Specifically, we will determine the m-of-n detection probability by
assuming a constant, known, signal amplitude across the n detection attempts. We will then
form a weighted average across all possible signal amplitudes using the appropriate density
function for the Swerling target type being considered. In equation form, we have
where Pmofn (S) is the m-of-n detection probability for a given signal amplitude and fS(S) is
the amplitude density function associated with the particular Swerling target type of interest.
The analyses leading to (8.66) is included in Appendix 8B.
Once we fix the signal amplitude, the density function of signal-plus-noise is the same as
the density function of signal-plus-noise for a SW0/SW5 target.
We will first develop the m-of-n detection probability equations applicable to SW0/SW5,
SW2, and SW4 targets and then extend the results to SW1 and SW3 targets.
If Dk is the detection event on any one try, the detection event on exactly m of n tries will be
(in the following, ∩ denotes intersection and ∪ denotes union—see Appendix 8C)
where the first term is the intersection across the m detection events, and the second term is the
intersection over the n-m events of a missed detection. As an example, we consider the case of
exactly two of three detections. Let the three detection events be D1, D2, and D3 and their
corresponding missed detection events be 
1, 
2, and 
3. The event of exactly two of three
detections can be either

In this case, we see that there are three ways we can have exactly two of three detection events.
The event consisting of any two of three detection events is
We want
We note that the events Da, Db, and Dc are mutually exclusive since, for example, if there
are detections on tries 1 and 2 and not 3, we cannot have the possibility of detections on tries 1
and 3 and not 2. In other words, the occurrence of any one of Da, Db, or Dc precludes the
occurrence of any of the others. Since Da, Db, and Dc are mutually exclusive, the probability
of their union is equal to the sum of their individual probabilities. Thus,
We now want to examine the individual probabilities on the right side of (8.71). Recall that
we assumed the probability of each of the n detection events was the same. For our 2 of 3
example, this means
We also note that
Given the assumption that D1, D2, and D3 are independent, we have, as an example,
Extending this further, we have that

If we use this in (8.71), we have
If we extend our 2 of 3 example to the general case, we can write the probability of a
particular combination of m of n detections occurring as
To get all possible combinations of m detections and n − m missed detections we need to
ask how many ways we can combine the mDk detection events and the (n − m) 
k missed
detection events. For the 2 of 3 case, this was three. For the general case, we turn to
combinatorial theory [29] and ask how many ways can m objects be arranged in a string of n
objects. The answer is
For our 2 of 3 example, we have
Given (8.78), we find that the probability of having exactly m detections and n − m missed
detections in n tries is
In our original problem statement, we said we wanted the probability of obtaining
detections on at least m of n tries. Said another way, we want to find the probability of
obtaining detections on m, m + 1, m + 2, … on n tries. Thus, we want to find the probability of
Since the event of having exactly m detections and n – m missed detections precludes the
possibility of having, say, exactly r detections and n – r missed detections, all of the events of
(8.81) are mutually exclusive. Thus,

or
Substituting (8.80) into (8.83) gives our final answer of
As indicated earlier, (8.84) does not directly apply to SW1 and SW3 targets. For these
targets, the appropriate equation is
In this equation, Pmofn0 (S) is the m-of-n detection probability for a SW0/SW5 target as a
function of signal amplitude, S, and fS(S) is the density function of the signal amplitude for a
SW1 or SW3 target. Specifically, for a SW1 target
and for the SW3 target
Substituting (8.84) into (8.85) gives
or with

For the specific cases of SW1 and SW3 targets we get, with some manipulation (see Appendix
8B),
for SW1 targets and
for SW3 targets. The integrals of (8.91) and (8.92) must be computed numerically.
The next subject we want to address is how to handle false alarm probability. Since false
alarms are detection due to noise, and since we assume noise samples are independent, we can
directly use (8.84) to write
where Pfa is the single sample (single pulse) false alarm probability.
Since the false alarm probability at the m-of-n detector output, Pfamofn, is usually specified
as a requirement, we need to use (8.93) to solve for Pfa. This can be done through a root
solving approach. To obtain the initial guess of Pfa we take advantage of the fact that Pfa is
small to simplify (8.93). Specifically, we note that (Pfa)m will be larger than (Pfa) m + 1, usually
by several orders of magnitude. Also (1 − Pfa)n−m will be approximately one since Pfa≪1.
With this we claim that the first term of the sum will be much larger than the subsequent terms.
We can thus drop all but the first term of the sum. Further, we can replace the (1 − Pfa)n−m term
by one. With this, we get a first-order approximation of (8.93) as

which we can use to start the root finding algorithm to find a more accurate value of Pfa.
To illustrate this, we consider the case where m = 3 and n = 5 (3-of-5 detection) and Pfamofn
= 10−8. From (8.94) we have
which yields 
. If we use this to seed the solution to (8.93), we get a final
value of Pfa = 1.0005 × 10−3, which is very close to the initial guess. It must be noted that, this
is not always the case. As Pfamof becomes larger, the initial guess of Pfa will become poorer.
However, it still provides a good starting point for the root-solving routine.
Figure 8.9 contains a plot of Pmofn versus Pd for m = 3 and n = 5, using (8.84). This curve
demonstrates an interesting feature of the m-of-n detector. Specifically, for Pd above a certain
value, 0.5 for the 3-of-5 detector, Pmofn will be larger than Pd. However, for Pd below this
value, Pmofn will be less than Pd. This feature tells us mof-n detectors tend to increase
detection probability while simultaneously decreasing false alarm probability relative to their
single sample values.
Figure 8.9 Pmofn vs. Pd for a 3-of-5 detector.
In these discussions, we assumed that the m-of-n detector operated on single-pulse
detections. An m-of-n detector can follow a coherent or noncoherent integrator if proper
restrictions are placed on its use. Specifically, the signal-plus-noise samples into the m-of-n

detector must be independent. This must also be true of the noise samples. These conditions
can be satisfied during search if the time between target illuminations is long relative to the
coherent or noncoherent processing time. This would be an example of m-of-n detection on a
scan-to-scan basis. The problem with the use of an m-of-n detector in this fashion is that the
Pdk on each scan will most likely not be the same. Because of this, one of the basic
assumptions of the m-of-n development of this section will be violated. Extension to the case
of unequal Pdk is very tedious. Several authors have attempted to approach the problem
through the use of Markov chains [35–36].7 However, they have been able to do so for only a
limited number of small values of m and n.
Figures 8.10 and 8.11 contain plots of detection probability for SW1 (Figure 8.10) and SW2
(Figure 8.11) targets for the case of single-pulse detection, 5-pulse noncoherent integration
and a 3-of-5 detector. The detection thresholds (i.e., the TNR) was adjusted to provide a Pfa of
10−6 at the output of the detection process for all cases. It will be noted that, in both cases, the
3-of-5 detector is not as good as noncoherent integration of the 5 pulses.
For the SW1 target, Pfa and the m and n considered in this example, the performances of the
noncoherent integrator and m-of-n detector are very similar, with the plots of Pd vs. required
single-pulse SNR in Figure 8.10 having the same shape and differing by only ~ 1 dB.

Figure 8.10 Pd vs. required single-pulse SNR for noncoherent integration and m-of-n detection— SW1 target.
For the SW2 case, we see the same dramatic improvement over using the single pulse that
we saw for the case of the noncoherent integrator. However, consistent with the SW1 case,
noncoherent integration appears to offer a performance advantage over m-of-n detection.
Specifically, for the example considered, the 3-of-5 detector offers 3- to 5-dB less reduction
in required single-pulse SNR than does the noncoherent integrator.
The slight loss in detection performance (1 dB for SW1 and 3 to 5 dB for SW2) may be an
acceptable exchange for simplicity of implementation of an m-of-n detector versus a
noncoherent integrator. Also, the m-of-n detector may be less susceptible to false alarms due
to random pulse interference.

Figure 8.11 Pd vs. required single-pulse SNR for noncoherent integration and m-of-n detection— SW2 target.
As a closing thought, we note that coherent integration and noncoherent integration is
performed before the threshold check; that is, before the radar checks for detections. The
cumulative probability calculation and the m-of-n detection is performed after the threshold
check. The fact that the m-of-n detector is placed after the threshold check is why it is also
termed a dual threshold detector.
8.6 EXERCISES
1.
A certain radar achieves an SNR of 13 dB, with a Pfa of 10−6 on a SW1 target. What is the
SNR after the coherent integration of 10 pulses? What is the Pd?
2.
Repeat Exercise 1 for a SW2 target.
3.
Repeat Exercise 1 for a SW3 target.

4.
Repeat Exercise 1 for a SW4 target.
5.
Repeat Exercise 1 for a SW0/SW5 target.
6.
What is the Pd for the conditions of Exercise 1 for the case where the 10 pulses are
noncoherently integrated?
7.
Repeat Exercise 6 for a SW2 target.
8.
Repeat Exercise 6 for a SW3 target.
9.
Repeat Exercise 6 for a SW4 target.
10. Repeat Exercise 6 for a SW0/SW5 target.
11. A certain radar noncoherently integrates 10 pulses from a SW1 target. What single-pulse
SNR is required to achieve a Pd of 0.99 and Pfa of 10−6 at the output of the noncoherent
integrator.
12. Repeat Exercise 11 for a SW2 target.
13. Repeat Exercise 11 for a SW3 target.
14. Repeat Exercise 11 for a SW4 target.
15. Repeat Exercise 11 for a SW0/SW5 target.
16. A search radar achieves an SNR of 13 dB on a SW1 target at a range of 100 km. The
radar scan period is 10 s. That is, it illuminates the target every 10 seconds. The target is
approaching the radar with a range rate of 500 m/s. What is the cumulative detection
probability after three scans? How many scans are required to achieve a cumulative
detection probability of 0.999? In both cases, the radar must maintain a cumulative false
alarm probability of 10−6.
17. Repeat Exercise 16 for a SW2 target.
18. Repeat Exercise 16 for a SW3 target.
19. Repeat Exercise 16 for a SW4 target.
20. Repeat Exercise 16 for a SW0/SW5 target.
21. For this exercise we want to compare the exact and approximate equations for
noncoherent integration of 10 pulses on a SW2 target. To do so, generate a plot like
Figure 8.5 where the three curves correspond to 1) single-pulse Pd; 2) Pd using the exact
equation; and 3) Pd using the approximate equation.
22. Repeat Exercise 21 for a SW1 target.
23. Repeat Exercise 21 for a SW3 target.
24. Repeat Exercise 21 for a SW4 target.
25. Repeat Exercise 21 for a SW0/SW5 target.
26. Create a figure like Figure 8.9 for 5 of 10, 6 of 10, and 7 of 10 detection.

27. Create a figure like Figure 8.10 for a SW3 target.
28. Create a figure like Figure 8.10 for a SW4 target.
References
[1]
North, D. O., “An Analysis of the Factors Which Determine Signal/Noise Discrimination in Pulsed Carrier Systems,” RCA
Labs., Princeton, NJ, Tech. Rep. No. PTR-6C, Jun. 25, 1943. Reprinted: Proc. IEEE, vol. 51, no. 7, Jul. 1963, pp. 1016–
1027. Reprinted: Detection and Estimation (S. S. Haykin, ed.), Stroudsburg, PA: Halstad Press, 1976, pp. 10–21.
[2]
Uhlenbeck, G. E., and J. L. Lawson, Threshold Signals, vol. 24 of MIT Radiation Lab. Series, New York: McGraw-Hill,
1950; Norwood, MA: Artech House (CD-ROM edition), 1999.
[3]
Emslie, A. G., “Coherent Integration,” MIT Radiation Lab. Series, Rep. 103–5, May 16, 1944. Listed as a reference in
Lawson and Uhlenbeck, Threshold Signals, MIT Rad. Lab. Series, vol. 24, McGraw-Hill, 1950, p. 166, 331; Norwood,
MA: Artech House (CD-ROM edition), 1999.
[4]
Delaney, W. P., and W. W. Ward, “Radar Development at Lincoln Laboratory: An Overview of the First Fifty Years,”
Lincoln Lab. J., vol. 12, no. 2, 2000, pp. 147–166.
[5]
Miller, K. S., and Bernstein, R. I., “An Analysis of Coherent Integration and Its Application to Signal Detection,” IRE
Trans. Inf. Theory, vol. 3, no. 4, Dec. 1957, pp. 237–248.
[6]
Mooney, R. K., and G. Ralston, “Performance in Clutter of Airborne Pulse, MTI, CW Doppler and Pulse Doppler
Radar,” IRE Conv. Rec., 1961, pt. 5, pp. 55–62.
[7]
Meltzer, S. A., and S. Thaler, “Detection Range Predictions for Pulse Doppler Radar,” Proc. IRE, vol. 49, no. 8, Aug.
1961, pp. 1299–1307. Reprinted: Barton, D. K., Radars, Vol. 2: The Radar Range Equation (Artech Radar Library)
Dedham, MA: Artech House, 1974, pp. 61–70.
[8]
Steinberg, B. D., “Predetection Integration,” in Modern Radar (R. S. Berkowitz, ed.), New York: Wiley & Sons, 1965.
[9]
Soller, T., M. A. Starr, and G. E. Valley, Jr., eds., Cathode Ray Tube Displays, vol. 22 of MIT Radiation Lab. Series,
New York: McGraw-Hill, 1948; Norwood, MA: Artech House (CD-ROM edition), 1999.
[10] Marcum, J. I., “A Statistical Theory of Target Detection by Pulsed Radar,” RAND Corp., Santa Monica, CA, Res.
Memo. RM-754-PR, Dec. 1, 1947. Reprinted: IRE Trans. Inf. Theory, vol. 6, no. 2, Apr. 1960, pp. 59–144. Reprinted:
Detection and Estimation (S. S. Haykin, ed.), Halstad Press, 1976, pp. 57–121.
[11] Swerling, P., “Probability of Detection for Fluctuating Targets,” RAND Corp., Santa Monica, CA, Res. Memo. RM-
1217, Mar. 17, 1954. Reprinted: IRE Trans. Inf. Theory, vol. 6, no. 2, Apr. 1960, pp. 269–308.
[12] Harrington, J. V., “An Analysis of the Detection of Repeated Signals in Noise by Binary Integration,” IRE Trans., vol. 1,
no. 1, Mar. 1955, pp. 1–9.
[13] Dinneen, G. P., and I. S. Reed, “An Analysis of Signal Detection and Location by Digital Methods,” IRE Trans. Inf.
Theory, vol. 2, no. 1, Mar. 1956, pp. 29–38.
[14] Schwartz, M., “A Coincidence Procedure for Signal Detection,” IRE Trans., vol. 2, no. 6, Dec. 1956, pp. 135–139.
[15] Hall, W. M., “Prediction of Pulse Radar Performance,” Proc. IRE, vol. 44, no. 2, Feb. 1956, pp. 224–231. Reprinted:
Barton, D. K., Radars, Vol. 2: The Radar Range Equation (Artech Radar Library), Dedham, MA: Artech House, 1974,
pp. 31–38.
[16] Chance, B., et al., Waveforms, vol. 19 of MIT Radiation Lab. Series, New York: McGraw-Hill, 1949; Norwood, MA:
Artech House (CD-ROM edition), 1999.
[17] Barton, D. K., Radar System Analysis and Modeling, Norwood, MA: Artech House, 2005.
[18] Swerling, P, “The ‘Double Threshold’ Method of Detection,” RAND Corp., Santa Monica, CA, Res. Memo. RM-1008,
Dec. 1952.
[19] Shnidman, D. A., “Binary Integration for Swerling Target Fluctuations,” IEEE Trans. Aerosp. Electron. Syst., vol. 34, no.
3, Jul. 1998, pp. 1043–1053.
[20] Worley, R., “Optimum Thresholds for Binary Integration (Corresp.),” IEEE Trans. Inf. Theory, vol. 14, no. 2, Mar. 1968,
pp. 349–353.

[21] Ray, H., “Improving Radar Range and Angle Detection with Frequency Agility,” Microwave J., vol. 9, no. 5, p. 63–68,
May 1966. Reprinted: Barton, D. K., Radars, Vol. 6: Frequency Agility and Diversity (Artech Radar Library), Dedham,
MA: Artech House, 1977, pp. 13–18.
[22] Fuller, J. B., “Implementation of Radar Performance Improvements by Frequency Agility,” IEEE Conf. Publ. No. 105,
London, Oct. 1973, pp. 56–61.
[23] DiFranco, J. V., and W. L. Rubin, Radar Detection, Englewood Cliffs, NJ: Prentice-Hall, 1968. Reprinted: Dedham, MA:
Artech House, 1980.
[24] Meyer, D. P., and H. A. Mayer, Radar Target Detection, New York: Academic Press, 1973.
[25] Barton, D. K., Radar Equations for Modern Radar, Norwood, MA: Artech House, 2013.
[26] Barkat, M., Signal Detection and Estimation, 2nd ed., Norwood, MA: Artech House, 2005.
[27] Barton, D. K., “Universal Equations for Radar Target Detection,” IEEE Trans. Aerosp. Electron. Syst., vol. 41, no. 3,
Jul. 2005, pp. 1049–1052.
[28] Bakut, P. A., et. al., Problems in the Statistical Theory of Radar, vol. 1, Moscow: Soviet Radio Publishing House, 1963
(in Russian; translation available from DTIC as AD608462).
[29] Urkowitz, H., Signal Theory and Random Processes, Dedham, MA: Artech House, 1983.
[30] Marcum, J. I., “A Statistical Theory of Target Detection by Pulsed Radar (Mathematical Appendix),” RAND Corp.,
Santa Monica, CA, Res. Memo. RM-753, Jul. 1, 1948. Reprinted: IRE Trans. Inf. Theory, vol. 6, no. 2, Apr. 1960, pp.
145–268.
[31] Skolnik, M. I., Introduction to Radar Systems, 3rd ed., New York: McGraw-Hill, 2001.
[32] Skolnik, M. I., ed., Radar Handbook, 3rd ed., New York: McGraw-Hill, 2008.
[33] Blake, L. V., “Recent Advancements in Basic Radar Range Calculation Technique,” IRE Trans. Mil. Electron., vol. 5, no.
2, Apr. 1961, pp. 154–164.
[34] Papoulis, A., Probability, Random Variables, and Stochastic Processes, 3rd ed., New York: McGraw-Hill, 1991.
[35] Castella, F. R., “Sliding Window Detection Probabilities,” IEEE Trans. Aerosp. Electron. Syst., vol. 12, no. 6, Nov.
1976, pp. 815–819.
[36] Williams, P. “Evaluating the State Probabilities of M Out of N Sliding Window Detectors,” Aeronautical and Maritime
Research Laboratory, Rep. No. AR-010-448, Feb. 1998. Approved for public release by DSTO Aeronautical and
Maritime Research Laboratory, Rep. No. DSTO-TN-0132, Pyrmont, New South Wales, Australia, 2009.
[37] Parl, S., “A New Method of Calculating the Generalized Q Function,” IEEE Trans. Inf. Theory, vol. 26, no. 1, Jan.
1980, pp. 121–124.
APPENDIX 8A: NOISE AUTOCORRELATION AT THE OUTPUT OF A
MATCHED FILTER
In this appendix, we consider the correlation of the noise at the output of a matched filter. In
particular, we show that noise samples are uncorrelated if they are separated by more than the
duration of the matched filter impulse response.
Let the normalized impulse response of the matched filter be
where ϕ(t) is an arbitrary phase modulation.
Let the noise into the matched filter be

where nI(t) and nQ(t) are wide sense stationary (WSS), zero-mean, white, Gaussian random
processes with equal power spectral densities of N/2. Further, assume that nI(t) and nQ(t) are
uncorrelated.
The noise out of the matched filter is
The autocorrelation of the noise at the output of the matched filter is
where δ(x) is the Dirac delta function.
Evaluating the β integral yields
From matched filter theory (see Chapter 7), we can write this as
where m(τ) captures the fine detail of the autocorrelation. As an example, for ϕ(t) = 0; h(t)
would be the impulse response of a filter matched to an unmodulated pulse and we would
have

The key thing to note about (8A.6) is that
Since no(t) is zero-mean (because n(t) is zero-mean), Ro(τ) = Co(τ) where Co(τ) is the
autocovariance of no(t). Since the autocovariance is zero for |τ| > τp, output noise samples
separated, in time, by more than τp will be uncorrelated. Since the noise samples are Gaussian,
they will also be independent.
APPENDIX 8B: PROBABILITY OF DETECTING SW1 AND SW3
TARGETS ON m CLOSELY SPACED PULSES
In this appendix, we address the problem of computing the probability of detecting a SW1 or
SW3 target on m closely spaced pulses. The instinctive method of computing this probability
is to say that if Pd1 is the probability of detecting the target on a single pulse, the probability
of detecting the target on m pulses is Pd1m. This method makes the assumption the detection
events on each pulse are independent. This is true for SW0/SW5, SW2, and SW4 targets. It is
not true for SW1 and SW3 targets.
Let Di be the event of detecting the target on the ith pulse. We can write
where Vi is the magnitude of the signal plus noise on the ith received pulse (in a particular
range cell) and T is the detection threshold. We are interested in determining
where fV1V2…Vm (V1, V2…,Vm) is the joint density of V1, V2, …, Vm. The issue becomes one
of finding this joint density, and then performing the integrations.
For SW0/SW5, SW2, and SW4 targets, the random variables V1, V2, …, Vm are
independent and
This leads to the aforementioned statement that

For SW1 and SW3 targets, we cannot assume that V1, V2, …, Vm are independent. This
stems from the fact that they all depend upon the target RCS, which is a random variable
governed by the SW1 and SW3 target models. To compute the joint density, we resort to
conditional density functions and write
We note that once we fix S to S and Θ to θ, the random variables Vi|S = S, Θ = θ are no longer
dependent on the random variables S and Θ. They are dependent on the noise component of
the signal-plus-noise. However, the noise is independent from pulse to pulse. Therefore, the
random variables Vi|S = S, Θ = θ are independent from pulse to pulse. With this we conclude
that
Further, from our experience with determining the density functions of the magnitude of
signal-plus-noise, the density functions of the in-phase and quadrature components of the
conditioned signal-plus-noise are Gaussian with variances of σ2 and means of Scosθ and
Ssinθ (see Chapter 6). Further, we know that
For a SW1 target
and
For a SW3 target

and fΘ(θ) is as in (8B.9). In the above, σ2 is the noise power and PS is the signal power.
We can use (8B.9) to eliminate the random variable Θ through the appropriate integration
to yield
If we use (8B.11) in (8B.2), we get
We can now use (8B.6) and write
If we use (8B.7), we recognize the inner integral of (8B.13) as the detection probability for
a SW0/SW5 target. Further, since we have assumed that the thresholds, T, are the same, we can
rewrite (8B.13) as
In the inner integral, we let u=V/σ and write
We can use the Marcum Q function [10] to evaluate the inner integral and write

Substituting for fS(S) from (8B.9) results in
for the SW1 case. Substitution for fS(S) from (8B.10) results in
for the SW3 case. In (8B.17) and (8B.18), we make the change of variables x = S2 / 2σ2.
Manipulation of some of the arguments, with the change of variables, yields
Finally, we recognize SNR as the signal-to-noise ratio given by the radar range equation and
TNR = −lnPfa, where Pfa is the false alarm probability.
With the above substitutions, the equation that must be implemented for SW1 targets is
The equation that must be implemented for SW3 targets is

These are the integrals that we need to evaluate. We will need to do so via numerical
integration.
The Marcum Q-function is defined as
An efficient and accurate algorithm for computing Q1(a,b) was developed by Steen Parl [37].
The Parl algorithm for computing Q1(a,b) is:
• Initialization
• Iteration, n = 1, 2, …
• Final Step
Typical values of p in the termination criterion are p = 3 to 9. A reasonable value seems to be
p = 6.
APPENDIX 8C: CUMULATIVE DETECTION PROBABILITY

This appendix contains a derivation of the cumulative detection probability equations
enumerated in Section 8.4.
To develop the technique, we start by considering the events [26, 29, 34] of the occurrence
of a threshold crossing on two occasions. We denote these two events as
• D1: Threshold crossing on occasion 1; and
• D2: Threshold crossing on occasion 2.
If we form the event
where ⋃ denotes the union operation [26], then D is the event consisting of a threshold
crossing on occasion 1, or occasion 2, or occasions 1 and 2. Since D is the event of interest to
us, we want to find the probability that it will occur. That is, we want
From probability theory, we can write
where D1∩D2 represents the intersection of D1 and D2 and is the event consisting of a
threshold crossing on occasion 1 and occasion 2. The first two probability terms on the right
side, P(D1) and P(D2), are computed using the appropriate single or n pulse probability
equation discussed in Chapter 6 and Sections 8.2 and 8.3, depending upon the target type and
whether or not coherent or noncoherent integration is used.
To compute the third term, P(D1∩D2), we need to make an assumption about the events D1
and D2. Specifically, we assume they are independent. This, in turn, limits when we can use
cumulative detection concepts. Specifically, we should use cumulative detection concepts only
on a scan-to-scan basis. If we do, we will satisfy the constraints on all of the Swerling target
types. Specifically, for SW1 and SW3 targets, the signal-plus-noise samples are, by definition,
independent from scan to scan. For SW0/SW5, SW2, and SW4 targets, the signal-plus-noise
samples from pulse to pulse and will thus also be independent from scan to scan. Having said
this, we must also assure that the coherent or noncoherent integrator does not cause the
independence restriction to be violated. The restriction will not be violated if the time between
target illuminations is significantly larger than the coherent or noncoherent processing time.
If D1 and D2 are independent, we can write
and

As an example, suppose P(D1) = P(D2) = 0.9. Using (8C.5), we would obtain
While (8C.5) is reasonably easy to implement for two events, its direct extension to many
events is tedious. In order to set the stage for a simpler extension, we consider a different
means of determining P(D). We begin by observing that
where S is the universe and 
 i is the complement of Di. 
 i contains all elements that are in S
but not in Di. By the definition of 
 i we note that Di and 
 i are mutually exclusive. We also
note that P(S) = 1. With this we get
and
To proceed with the derivation, we let
and
From (8C.8) we get
By DeMorgan’s Law [29] we can write
and

Now, since D1 and D2 are independent, so are 
1 and 
2. If we use this along with (8C.10),
we can write
Finally, making use of (8C.9), we obtain
We can now generalize (8C.16) to any number of events. Specifically, if
where D1, D2, D3… DN are independent, then
As an example of the use of (8C.16) or (8C.18), we consider the previous example wherein
P(D1) = P(D2) = 0.9. With this we get
We now want to restate (8C.19) in terminology more directly related to detection
probability. To that end, we write
where Pdcum is the cumulative detection probability over N scans, and Pdk is the detection
probability on the kth scan.
1 David K. Barton, private communication to author containing historical notes on coherent integration, cumulative integration,
and binary integration, September 15, 2014. Portions are paraphrased in this introduction.
2 This assumption carries implications about the spacing between pulses relative to the impulse response of the matched filter.
Specifically, the spacing between noise samples must be greater than the length of the impulse response of the matched filter. If
the matched filter is matched to a rectangular pulse with a width of τp, the spacing between noise samples must greater than τp.
Since the noise (in a particular range cell—of the 500 of the previous example) is sampled once per pulse, the pulses must be
spaced more than one pulsewidth apart. This is easily satisfied in pulsed radars since the pulses can never be spaced by less
than one pulsewidth. This is discussed in more detail in Appendix 8A.
3 Some forms/implementations of the incomplete gamma function omit the 1/Γ(a) = 1/(a-1)! term.

4 David A. Hardaker, Application of Barton’s Universal Equations for Radar Target Detection, September 15, 2014.
5 For analysis, a square law detector is typically assumed because the resulting mathematical analysis tends to be more
tractable.
6 Blake suggests 1.6 dB for 1-D scanning and 3.2 dB for 2-D scanning. Barton suggests 1.24 dB and 2.48 dB, respectively,
for a typical radar beam (see Chapter 5 for a discussion of this).
7 B. K. Bhagavan, internal memo, Markov Chain for M out of N Detection Schemes, 2014.

Chapter 9
Ambiguity Function
9.1 INTRODUCTION
The ambiguity function, which is denoted as |χ(τ, f)|2, is primarily used to gain an
understanding of how a signal processor responds, or reacts, to a given returned signal. As
indicated in the notation, the independent variables of the ambiguity function are time (τ) and
frequency (f). The time variable is normally associated with target range, and the frequency
variable is normally associated with target Doppler frequency. The magnitude square (i.e., | |2)
is used to indicate we are characterizing the amplitude squared of the signal processor output.
In a strict sense, when one uses the phrase “ambiguity function,” there is an underlying
assumption that the signal processor is matched to the transmitted waveform. If the signal
processor is not matched to the transmitted waveform, the proper terminology, in the
ambiguity function context, would be to refer to the “cross ambiguity function.” In practice,
however, we do not always make the distinction and simply use the phrase “ambiguity
function.”
We will derive a general equation for the (cross) ambiguity function and then derive
specific ambiguity function expressions for two simple waveforms and signal processors. We
will then discuss a representation that is suitable for numerically computing the ambiguity
function using the FFT. This will allow generation of ambiguity functions of advanced
waveforms and signal processors.
The ambiguity function was first developed by a British mathematician named Philip M.
Woodward [1]. As such, the ambiguity function is sometimes referred to as the Woodward
ambiguity function. In 2009, Woodward received the IEEE Dennis J. Picard Medal for radar
technologies and applications “[f]or pioneering work of fundamental importance in radar
waveform design, including the Woodward Ambiguity Function, the standard tool for
waveform and matched filter analysis” [2].
9.2 AMBIGUITY FUNCTION DEVELOPMENT
Since the ambiguity function can be thought of as the response of a signal processor to a
received radar waveform, this is the approach we take in deriving the ambiguity function.
Let the normalized, transmitted waveform be represented by the baseband signal, u(t).1 The
normalized (baseband) signal received from a (constant range rate) target at a range R and
range rate Ṙ is given by

where τR = 2R/c is the range delay and fd = –2Ṙ/λ is the target Doppler frequency. λ is the
wavelength of the transmitted signal, and c is the speed of light.
The signal processor configuration we use in deriving the ambiguity function is shown in
Figure 9.1. In this figure, h(t) is a lowpass function and fs is thought of as the frequency to
which the signal processor is tuned. Thus, the overall signal processor is a bandpass filter
centered at fs. This is indicated in Figure 9.2, where the left plot is the frequency response of
h(t) and the right plot is the frequency response of the signal processor.
Figure 9.1 Signal processor.
Figure 9.2 Signal processor frequency response.
In keeping with the concept of matched filters, we normally define h(t) in terms of the
waveform to which it is matched. Specifically, we use
and set K = 1 and t0 = 0 to get
In (9.3), v(t) is the waveform to which the signal processor is matched and the superscript *
denotes the complex conjugate.
With the above, the impulse response of the signal processor is
The output of the signal processor is the convolution of vR(t) and hSP(t) or

Using (9.1) and (9.4) in (9.5) yields
If we make the change of variable t = α – τR, (9.6) can be rewritten as
where we made the substitutions τ = τR − γ and f = fd − fs.
The variables τ and f are often termed the mismatched range and Doppler of the ambiguity
function. More specifically, τ is the difference between the target range delay and the time we
look at the signal processor output. If τ = 0, we say that we are at matched range. That is, we
are looking at the signal processor output at a time equal to the time delay of the target. f is the
difference between the target Doppler frequency and the frequency to which the signal
processor is matched. If f = 0, we say the signal processor is matched to the target Doppler
frequency, or vice versa. In this case, we say we are at matched Doppler.
Since τ and f are the variables of interest, we rewrite (9.7) in terms of them and change the
dependent variable from vo (γ) to χ(τ, f). Thus, we get
Finally, if we take the magnitude squared of (9.8), we get the ambiguity function or
We often attribute special names to plots of |χ(τ, f)|2 for specific values of τ and f. In particular:
• If we let f = 0 to yield |χ(τ, 0)|2, we have the matched-Doppler, range cut of the ambiguity
function. This is what we normally think of as the output of the classical matched filter.
• If we let τ = 0 to yield |χ(0, f)|2, we have the matched-range, Doppler cut of the ambiguity

function.
• If we let f = fk to yield |χ(τ, fk)|2, we have a range cut at some mismatched Doppler of fk.
• If we let τ = τk to yield |χ(τk, f)|2, we have a Doppler cut at some mismatched range of τk.
As we will discuss in Chapter 10, the ambiguity function provides a wealth of information
about radar waveforms and how they interact with the environment and the radar signal
processor. For now, as examples, we want to derive equations for the ambiguity function of
an unmodulated pulse and a pulse with LFM.
9.3 EXAMPLE 1—UNMODULATED PULSE
We want to derive the equation for the ambiguity function of an unmodulated pulse of width
τp. We will assume the signal processor is matched to the transmitted pulse. With this, we can
write2
If we substitute this into (9.9), we get
As we did for the matched filter derivation (see Chapter 7), we need to consider several
regions of τ. To see this, refer to Figures 9.3 and 9.4.
In these figures, u(t) = rect[(t − τp/2)/τp] and v*(t + τ) = rect[(t − τp/2 + τ) / τp]. Note that for |
τ | ≥ τp the rect functions will not overlap and the integral of (9.11) will be zero. This leads to
the observation that |χ(τ, f)|2 = 0 for |τ | ≥ τp. We will account for this by multiplying | χ (τ, f)|2
by rect[τ/2τp].
Figure 9.3 Plot of u(t) and v* (t + τ) for τ < 0.

Figure 9.4 Plot of u(t) and v*(t + τ) for τ ≥ 0.
For −τp < τ < 0, the rect functions overlap from −τ to τp (see Figure 9.3). Thus, (9.11)
becomes
If we factor ejπf (τp −τ) from both terms on the far right side if (9.12) and then multiply by |τp +
τ|/|τp + τ|, we get
Finally, if we recognize τ < 0, we can use |τ = −τ, and rewrite (9.13) as
For τp > τ ≥ 0 the rect functions overlap from 0 to τp −τ (see Figure 9.4). Thus, (9.11) becomes
If we factor ejπf(τp − τ) from both terms on the far right side (9.15) and then multiply by | τp − τ
|/| τp − τ |, we get

If we observe that τ ≥ 0, we can use |τ| = τ, and rewrite (9.16) as
which is the same result we obtained for –τp < τ < 0.
If we combine all of the above, we arrive at our final answer of
We note that the square root of the matched-Doppler, range cut, which we obtain by setting f =
0, is the same form as the matched filter output we found in Chapter 7. Specifically,
A sketch of |χ(τ, 0)| is shown in Figure 9.5.3
The square root of the matched-range, Doppler cut, which we obtain by setting τ = 0, is
A plot of this function is shown in Figure 9.6.
Finally, a plot of the center portion of |χ(τ, f)| is shown in Figure 9.7 for the specific case
where τp = 1 µs. The plot has been normalized to a height of unity. Its actual height is τp, or 1
µs.
Figure 9.5 Matched-Doppler range cut for an unmodulated pulse.

Figure 9.6 Matched-range Doppler cut of an unmodulated pulse.
Figure 9.7 Ambiguity function plot of an unmodulated pulse.
9.4 EXAMPLE 2—LFM PULSE
For the LFM pulse, we let
where α is the LFM slope. Recall that the LFM slope is related to the LFM bandwidth by (see
Chapter 7)

α > 0 means the modulation frequency increases across the pulse, and α < 0 means the
modulation frequency decreases across the pulse.
If we substitute (9.21) into (9.9), we get
which, after manipulation, becomes
where we made use of |e −jπατ2|= 1 to eliminate it from the absolute value.
If we follow the integration steps of Example 1, we get |χ(τ, f)|2 = 0 for |τ| ≥ τp. For −τp < τ <
0 we get [see (9.12)]
If we factor ejπ(f−ατ)(τp −τ) from the third term of (9.25), we get
As with Example 1, we multiply (9.26) by |τp + τ|/|τp + τ|, and manipulate it to get
For τp > τ ≥ 0 we get [see (9.15)]

If we factor e jπ(f − ατ)(τp − τ) (from both terms on the far right side (9.28) and then multiply by |
τp − τ |/| τp − τ |, we get
Since τ < 0 in (9.27), we can replace τp + τ by τp − |τ| in that equation. In (9.29) τ > 0 and we
can replace τp − τ with τp − | τ |. When we do this, (9.27) and (9.29) have the same form.
Finally, if we combine this with the | τ | ≥ τp condition, we have
We note that (9.30) is the same form as (9.18) except f is replaced by f – ατ. In fact, if we
compare (9.24) to (9.11), the only difference is the f in (9.11) is replaced by f – ατ in (9.24).
Thus, we could have replaced the f in (9.18) to get (9.30) and avoided the various integration
steps of this example.
As a specific example, we consider the LFM pulse of Chapter 7. For that case we had a
pulsewidth of τp = 15 μs and an LFM bandwidth of B = 1 MHz. Further, we stipulated that α >
0.
The square root of the matched-Doppler range cut of the ambiguity function is obtained by
setting f = 0. The result is,

Figure 9.8 Matched-Doppler range cut for the LFM pulse example of Chapter 7.
This is plotted in Figure 9.8 and is the same as the matched filter output from Chapter 7.
The square root of the matched-range, Doppler cut, which we obtain by setting τ = 0, is
This has exactly the same form as for the unmodulated pulse. As an interesting note, the
matched-range, Doppler cut is given by the same equation [i.e.. (9.20) and (9.32)] for any u(t)
and v(t) that satisfies
In this equation φ(t) is an arbitrary phase function. The proof of this is left as an exercise.
Figure 9.9 contains a plot of |χ(τ, f)| for the case where −τp ≤ τ ≤ τp and −B ≤ f ≤ B. As with
Figure 9.7, the height has been normalized to unity. The actual height is τp, or 15 µs. This
exhibits the same triangle ridge as does the plot of |χ(τ, f)| for an unmodulated pulse, except
that the ridge slants across range-Doppler space for the LFM pulse while it is concentrated
along f = 0 for the unmodulated pulse. This slanting of the LFM ridge makes the LFM
waveform useful in search radars, when compared to an unmodulated pulse of the same
duration.

Figure 9.9 Ambiguity function plot of an LFM pulse.
9.5 NUMERICAL TECHNIQUES
While the analytical approach discussed above is suitable for deriving the ambiguity function
of simple waveforms and signal processors, it becomes very tedious for complex waveforms
and signal processors. Because of this, we present a numerical technique for generating the
ambiguity function. This technique relies on the fact that modern computers are very fast and
that efficient software algorithms are available—specifically, FFT algorithms.
With this technique, we develop the ambiguity function as a sequence of range cuts. If we
recall the general representation of the ambiguity function, we note that χ(τ, f) can be
interpreted as the correlation of u(τ)ej2πfτ with v(τ). That is,
where ⊗ denotes correlation.
From Fourier transform theory, we recognize that

or
where the superscript * denotes complex conjugation. Also, the symbolology ℑ{x} denotes
the Fourier transform operator and
In the algorithm discussed in the next section, we use the FFT to approximate the two
Fourier transforms on the right of (9.36). We then perform the indicated multiplication in the
θ domain and use the inverse FFT (IFFT) to determine χ (τ, f). We do this for the values f = fk
of interest. Thus, every FFT-multiply-IFFT will result in a range cut of the ambiguity function
at f = ft. This is why we say that we develop the ambiguity function as a sequence of range
cuts.
9.6 AMBIGUITY FUNCTION GENERATION USING THE FFT
We have already discussed how we can use Fourier transforms to compute the ambiguity
function by computing individual range cuts. Specifically, let u(t) be the baseband transmitted
signal and v(t) be the signal to which the matched filter (or signal processor) is matched. Let f
be the Doppler mismatch at which we want to generate a range cut. The algorithm used to
generate the range cut, χ(τ, f), is
• Find ℑ[u(t)ej2πft] = Fu(θ)
• Find ℑ [v(t)] = Fv(θ)
• Find Fχ (θ) = Fu(θ)Fv*(θ) = ℑ[ χ (τ, f)]
• Find χ(τ, f) = ℑ–1 [fχ(θ)]
In the above, we use the FFT and inverse FFT to approximate ℑ[•] and ℑ−1[•]. When using
the FFT, we need to be sure we satisfy the Shannon sampling theorem4 and account for some
time properties of the FFT and ambiguity function [3, 4]. The algorithm is as follows:
1. Create a time array, t, whose length, N, is an integer power of 2 (i.e., N = 2M where M is a
positive integer)5 and extends from 0 to T, where T is equal to, or greater than, the sum
of the durations of u(t) and v(t). This assures the resulting range cut will cover all time
values over which the range cut is not zero. Some restrictions on N are:
• Choose N so that N/T satisfies Shannon’s sampling theorem. That is, choose N such that
N/T > 2F where F is the highest frequency of u(t) and/or v(t) that is of interest. F
should be chosen so that it is more than twice the waveform bandwidth. Five times the
waveform bandwidth works fairly well.

• The larger N is, the better each range cut will look.
2. Compute v = v(t). Note: v(t) will contain several zero values at the end.
3. Compute FFT(v) = Fv.
4. Select a Doppler mismatch frequency, f.
5. Compute u = u(t)ej2πft. u(t)ej2πft will also contain several zero values at the end.
6. Compute FFT(u) = Fu.
7. Compute |χ(τ', f)| = X = |IFFT(FuFv*)|.
• Because of the way the FFT and IFFT is implemented, τ' = 0 corresponds to the first
tap, τ' = T/N corresponds to the second tap, τ' = T – T/N corresponds to the Nth FFT tap,
and so forth.
• To get the τ' = 0 point in the center of a plot of |χ(τ', f)| versus τ', which is desired, we
need to rearrange the IFFT outputs and redefine a time array.
8. Swap the upper and lower N/2 samples of X. In MATLAB, this is accomplished by using
the fftshift function.
9. Create a τ array that contains N samples and extends from –T/2 to T/2-T/N in steps of T/N.
• When we plot X versus τ, we have a range cut of the square root of the ambiguity
function at a mismatch Doppler of f.
10. To create another range cut at a different Doppler mismatch, repeat steps 4 through 9
with a different f.
11. To create a three-dimensional-looking depiction of |χ(τ, f)| (as in Figures 9.7 and 9.9),
assemble the series of range cuts and plot them using a 3-D plotting routine such as the
mesh or surf plotting function in MATLAB. Figures 9.7 and 9.9 were plotted using the
mesh plotting function.
9.7 EXERCISES
1.
Show that the matched range Doppler cut of the ambiguity function of a rectangular pulse
with a width of τp an arbitrary phase modulation [see (9.33)] is given by (9.32). That is,
that the matched-range, Doppler cut is a (sinc[x])2 function
2.
Implement the algorithm of Section 9.6 and use it to recreate a plot like Figure 9.7. Figure
9.7 was created for a 1–µs unmodulated pulse. The range delay axis extends from –1 μs to
1 μs and the Doppler axis extends from –5 MHz to 5 MHz.
3.
Use the algorithm from Exercise 2 to recreate a plot like Figure 9.9. The waveform
corresponding to the plot of Figure 9.9 is a 15-µs LFM pulse with a bandwidth of 1 MHz
and an increasing frequency (positive α). The range delay axis of the plot extends from –
15 μs to 15 μs and the Doppler axis extends from –1 MHz to 1 MHz. Also reproduce the
range cut of Figure 9.8.
4.
Show that if u(t) = v(t), |χ(τ, f)|2 = |χ(−τ, –f)2|. This will prove useful when we consider the
ambiguity function of more complicated waveforms in Chapter 10.

References
[1]
Woodward, P. M., Probability and Information Theory with Applications to Radar, 2nd ed., New York: Pergamon
Press, 1953; reprinted: Dedham, MA: Artech House, 1980.
[2]
Pace, T., “From the Editor-in-Chief [Dr. Philip Woodward, Recipient of the IEEE Picard Model],” IEEE Aerosp.
Electron. Syst. Mag., vol. 25, no. 2, 2010, p. 3.
[3]
Shannon, C. E., “A Mathematical Theory of Communication,” Bell Syst. Tech. J., vol. 27, no. 3, Jul. 1948, pp. 379–423.
[4]
Shannon, C. E., “Communication in the Presence of Noise,” Proc. IRE, vol. 37, no. 1, Jan. 1949, pp. 10–21. Reprinted as
a classic paper in Proc. IEEE, vol. 86, no. 2, Feb. 1998.
[5]
Nyquist, H., “Certain Topics in Telegraph Transmission Theory,” Trans. AIEE, vol. 47, no. 2, Apr. 1928, pp. 617–644.
Reprinted as a classic paper in Proc. IEEE, vol. 90, no. 2, Feb. 2002.
[6]
Kotel’nikov, V. A., “On the Capacity of the ‘Ether’ and Cables in Electrical Communications,” in Proc. First All-Union
Conf. Technolog. Reconstruction of the Commun. Sector and Develop. of Low-Current Eng., Moscow: 1933.
Translated by C. C. Bissell and V. E. Katsnelson. http://ict.open.ac.uk/classics/1.pdf.
1 Recall that if an actual signal is vRF(t) = A(t) cos[ωRFt – φ(t)], the baseband representation of that signal is vB (t) = A(t)ejϕ(t).
That is, the baseband signal captures the amplitude and phase modulation of the actual signal in the form of a complex
variable. Baseband notation is a special case of complex signal notation (see Chapter 1).
2 We are using a normalized pulse amplitude of unity. For the more general case, we would need to multiply u(t) and v(t) by
appropriate amplitudes.
3 As a note, the standard convention is to plot |χ(τ, f)| since it usually provides more detail about the structure of the matched
filter response, especially in the sidelobes.
4 Also known as the Nyquist sampling theorem (perhaps erroneously) after Harry Nyquist [5]. In Russia, the equivalent
theorem is known by the name Kotel’nikov after Vladimir Aleksandrovich Kotel’nikov (Bлaдимиp Aлeкcaндpoвич
Кoтeльников) [6].
5 This restriction on N is not a necessity with modern computers and FFT algorithms. It turns out that if N is a product of
relatively small prime numbers, modern FFT routines are quite fast. Even if this is not the case, they are reasonably fast. The
restriction stated here is convenient, but could result in the computation of range-cut values at unnecessary points.

Chapter 10
Waveform Coding
10.1 INTRODUCTION
Waveform coding means a phase modulation is applied to the transmit pulse. Specifically, we
assume the transmit pulse is of the form
where φ(t) is the phase modulation function and τp is the pulsewidth. The inclusion of the
rect[x] function means we assume the transmit pulse has a rectangular envelope or, more
specifically, a constant amplitude. 1 The assumption of a constant amplitude is consistent with
current transmitter technology in that the final amplifier of most transmitters operate in
saturation and thus cannot support pulses with amplitude modulation [1, 2].
Our first encounter with a phase coded pulse was the LFM pulse (see Chapter 9, Section
9.4), which had a φ(t) of the form φ(t) = παt2, a quadratic function of time. Because of this, we
say the pulse has quadratic phase coding. As may be recalled, the term linear FM derives from
the fact that the frequency variation across the pulse is a linear function of time. That is, f(t) =
αt.
A variant of LFM that we will examine in this chapter is nonlinear FM, or NLFM. With
NLFM, the frequency variation across the pulse is a nonlinear function of time. The attraction
of NLFM is that the matched-Doppler range cut of the ambiguity function of an NLFM
waveform can have lower sidelobes than an equivalent bandwidth LFM waveform.
With LFM and NLFM waveforms, φ(t) is a continuous function of time. Another type of
phase coded waveform is one where φ(t) is a discrete function of time. That is, φ(t) is of the
form
In other words, the phase is constant over some time period, τc, but can change from time
period to time period. Examples of this type of phase coded pulse include Frank polyphase
pulses, Barker coded pulses, and pseudo random noise (PRN) coded pulses, all of which we
will consider in this chapter.
FM and the discrete phase coding just mentioned are applied to a single pulse. Another type

of waveform coding we will discuss is frequency coding, or frequency hop waveforms. The
frequency coded waveforms we will consider consist of a group, or burst, of pulses, where
each pulse has a different carrier frequency and the pulses are spaced such that the return
from pulse k is received before pulse k + 1 is transmitted (unambiguous range operation—see
Chapter 1).
The main tool we will use to analyze phase coded waveforms is the ambiguity function or,
more accurately, the square root of the ambiguity function, |χ(τ, f)|. This implies the coded
pulses are processed by a matched filter, which we will assume. The exception to this will be
the LFM pulse. In that case, we will consider a mismatched filter designed to reduce range
sidelobes (the sidelobes of the matched Doppler range cut of |χ(τ, f)|).
The ambiguity function is the analysis tool of choice because we can use it to examine
range resolution and range sidelobes, as well as the sidelobes in the regions off of the range
cut (matched-Doppler range cut) and Doppler cut (matched-range Doppler cut).
It appears Robert H. Dickey developed the concept of waveform coding in the early 1940s
[3]. In 1945, he applied for a patent for a system that used an LFM waveform [4]. Sidney
Darlington also worked on coded waveforms during that time, but Dickey beat Darlington to
print [5–7]. According to Skolnik, the first use of a coded waveform in a fielded radar
occurred in the mid-1950s. That radar used a pulse with 200 discrete phase changes [K = 200
in (10.2)] [8]. The phases changed randomly between 0 and π (binary phase coding). Skolnik
indicated the first use of LFM in a radar occurred sometime after that.
In his patent description, Dickey termed his matched filter a compression filter. That was
most likely the origin of the term pulse compression that is commonly used in connection with
phase coded waveforms and their processing.
Since their introduction in the 1940s and 1950s, many different types of waveform coding
have been developed or adapted from other disciplines, such as cryptography, cell phones,
spread spectrum, GPS, communications, and information theory [9–16].
We will begin our discussions by revisiting LFM pulses. We will specifically investigate the
use of a mismatched filter that incorporates amplitude weighting for the purpose of reducing
range sidelobes. After that, we will discuss pulses with NLFM. We will present a method for
synthesizing Φ(t) for NLFM pulses.
We next consider discrete phased coded pulses. We start by discussing two classic codings:
Frank polyphase and Barker. With the latter, we also briefly discuss polyphase Barker codes
and minimum peak sidelobe codes. We next discuss coding based on PRN sequences. PRN
sequences are widely used in communications and have interesting properties that make them
attractive as radar waveforms.
We close the chapter with a discussion of step frequency waveforms. Step frequency
waveforms provide a means of achieving fine range resolution without requiring the radar to
have a large instantaneous bandwidth.
10.2 FM WAVEFORMS

10.2.1 LFM with Amplitude Weighting
One of the characteristics of LFM waveforms is that the first few sidelobes of the matched
filter output are somewhat large. This is illustrated in the left half of Figure 10.1, which is a
plot of the matched filter output for a 15-µs pulse with an LFM bandwidth of 2 MHz. As the
figure shows, the first and second sidelobes are about 14 and 19 dB below the peak.2 This
ratio is fairly consistent for different BT products, where we recall that the BT product is the
product of the pulsewidth, τp, and the LFM bandwidth, B. For example, the waveform we are
considering has a BT product of 2 MHz × 15 µs = 30. With LFM waveforms, it is possible to
apply an amplitude weighting in the matched filter to reduce range sidelobes, which is not
possible for other phase coded waveforms.
The result of applying an amplitude weighting is illustrated in the right half of Figure 10.1.
In this case, the weighting function was a n = 6, 30-dB Taylor window. As can be seen, the
range sidelobes have been significantly reduced.
The amplitude weighting has had two other effects: the peak response is about 0.6 dB below
the peak of the unweighted response and the main lobe is wider. The reduction in peak value
translates to a loss in SNR, and the width increase translates to a degradation in range
resolution.
Figure 10.1 Matched filter response for an unweighted and weighted LFM pulse.
An example of a weighted, mismatched filter implementation is illustrated in Figure 10.2,
which contains a functional block diagram of an FFT-based (mis)matched filter processor.
The processor implements (actually, approximates) the equation
That is, it correlates the received signal, u(t), with a weighted version of the conjugate of the
transmit signal, w(t)v*(t) (see Chapter 9). In (10.3)

is the Fourier transform of u(t) and
is the Fourier transform of w(t)v(t). As a note, the weight (Taylor in the above example) is
real, so w*(t) = w(t). V*(f) is precomputed and stored.
As an implementation note, in search, the received pulse could be anywhere in the
instrumented range interval (see Chapter 1). Thus the FFT, IFFT (inverse FFT), and the stored
matched filter frequency response must be long enough to accommodate the number of
samples of u(t) that are in the instrumented range interval.
Figure 10.2 FFT-based matched filter.
The minimum sample rate necessary to satisfy the Nyquist criterion [17–19] is the
waveform bandwidth, assuming complex samples. However, we have found that we should
sample u(t) at about twice the waveform bandwidth to avoid distortion of the range sidelobes
of the processor output. This means the FFT, memory, multiplier, and IFFT lengths would
need to be 2BτI, where τI is the instrumented range interval. Suppose the PRI associated with
the 15-µs, 2-MHz pulse of the previous example was 500 µs and the instrumented range
window was 400 µs. This would give 2BτI = 2 × 2 × 400 = 1,600 samples and indicate that the
FFT, memory, multiplier, and IFFT sizes should be 2,048 or 211.
Since the target range is known reasonably well in track, a smaller FFT, memory,
multiplier, and IFFT can be used during track. However, the sizes of the devices must be as
long as the number of samples in a time interval of twice the pulsewidth. If this is not satisfied,
there will be aliasing of U(f) and V(f), which will cause m(τ) to be incorrect. Thus, the
minimum sizes of the components of the matched filter must be greater than 2Bτp. For our
example, 2Bτp = 2 × 2 × 15 = 60 samples, so the minimum size of each component of the
matched filter should be 64, or 26.
The implementation of Figure 10.2 was used to generate the matched filter responses and

the ambiguity function plots in this book. For these cases, the signals were sampled at 5 to 10
times the pulse bandwidth to produce smooth matched filter plots and plots of |χ(τ, f)|. To
compute the various range cuts of |χ(τ, f)|, u(t) was offset in frequency by an amount equal to
the Doppler mismatch, f, of the range cut.
10.2.2 Nonlinear FM (NLFM)
An alternate method of reducing range sidelobes of FM waveforms is through the use of a
nonlinear frequency variation across the pulse. This idea was originally conceived by Kay et
al. [20, 21] and Watters [22], according to statements by Fowle [23]. The technique has also
been discussed by other authors [24–28]. In this chapter, we outline the technique presented by
Fowle in his 1964 paper.
Fowle developed his technique through the use of stationary phase integration [29, 30]. We
will not attempt to repeat Fowle’s development here. Instead, we present the results needed to
use the technique to design NLFM waveforms.
In his paper, Fowle addressed the following problem.
Given a function of the form
and its Fourier transform
how does one determine the phase function φ(t) so that um2(t) and Um2(f) closely
approximate desired functions? As a note, um(t) and Um(f) are real and usually positive.
The idea that this approach will provide low range sidelobes stems from the results of the
previous section where we found that applying an amplitude weighting in the LFM-matched
filter can reduce range sidelobes. Amplitude weighting changes the magnitude of the matched
filter frequency response. The question addressed in Fowle’s paper is whether changing φ(t)
from a quadratic function of time (which it is for LFM) to some other function of time will
similarly change the waveform and matched filter spectrums to produce low range sidelobes.
Stated another way, will changing the frequency deviation from a linear function of time to a
nonlinear function of time result in lower range sidelobes?
Fowle’s technique is as follows: given a desired time function, um(t), and a desired
frequency function, Um(f), evaluate the integrals

to obtain
Solve (10.9) to obtain
Next, use
to determine the desired phase function for u(t). Um(ζ) and um(γ) must be chosen to satisfy
Parseval’s theorem [13]. That is, they must be chosen so that
This can be accomplished through the use of scaling factors, assuming the integrals of (10.12)
exist.
The algorithm is applicable to any Um(ζ) and um(γ). However, the case of interest to us is
where um(γ) is a rectangular pulse. That is, where
where, for convenience, we scaled um(γ) so that the left integral of (10.12) will be unity. With
this we have
which is a phase modulated, rectangular pulse. With the assumption of (10.13), (10.12)
becomes

10.2.2.1 Example 1
As a first example of Fowle’s method, we derive a pulse with LFM. It can be shown that the
magnitude of the spectrum of an LFM pulse with a bandwidth of B is close to a rectangle
function (see Figure 11.1, Chapter 11). Thus we choose Um2(ζ) as
where we chose the scale factor, 1/B, so that the right side of (10.12) was unity [since the left
side is unity for the um(γ) of (10.13)].
Using this we get, over the interval |t| ≤ tp/2,
Using the region |f(t)| ≤ B/2, we get
or

and
In other words, the method results in an LFM pulse
10.2.2.2 Example 2
As another example, we consider the case where um(γ) is as in (10.13) and Um2(ζ) is a “cosine
on a pedestal” function (e.g., Hamming, Hanning, and so forth.). With this we get
where b ≤ 1, a = 1 − b, and K = π/[B(πa + 2b)] is chosen so that the right side of (10.12) is
equal to unity. The derivation of K is left as an exercise. With this we get
The above leads to
which we must solve for f(t). Herein lies one of the difficulties with Fowle’s method:
numerical techniques are often needed to find f(t) for Um2(ζ) functions of interest.
For this particular example, we can get a closed form solution of

and
for the case where b = 1 (and, thus, a = 0). The details are left as an exercise.
Figure 10.3 NLFM frequency and phase plots.
Figure 10.4 Matched filter response for an NLFM pulse and LFM pulse.
Figure 10.3 contains plots of f(t) and φ(t) and Figure 10.4 contains plots of the matched
Doppler range cut of the ambiguity function for an LFM waveform and the nonlinear FM
waveform we obtained in this example. As the figures show, the frequency modulation is quite
nonlinear. Also, the first range sidelobe has been reduced from -14 dB to -20 dB.

10.2.2.3 NLFM Design Procedures
In the examples above, we made some assumptions that allowed us to develop closed form
expressions for f(t) and φ(t). In general, this is not possible, and we must resort to numerical
techniques. To that end, we outline a procedure for deriving f(t) and φ(t). The technique
assumes the steps are performed using numerical methods. However, some of them could be
completed using analytical techniques, if the various functions are conducive to analytical
methods.
1. Select a desired Um(ζ) or Um2(ζ) and compute it for several values of ζ over the interval
of −B/2 to B/2, where B is the desired NLFM bandwidth. The values of ζ should be
chosen close enough to capture the shape Um(ζ). A rule of thumb is to space them less
than 1/τp apart.
2. Square Um(ζ) to get Um2(ζ). This step can be omitted if one starts with Um2(ζ). We did this
in the two examples.
3. Numerically compute the integral
for f between −B/2 and B/2. Scale P(f) so that P(B/2) = 1. This is needed to satisfy (10.12).
These three steps result in
or
4. Use the results of Step 3 to generate a tabulation of t versus f and use interpolation to find
f as a function of t for values of t between – τp/2 and τp/2 with a spacing of Δt < 1/B. A
rule of thumb is to start with Δt = 1/(10B). The result of this will be f(t).
5. To find Φ(t), numerically compute the integral
10.3 PHASE CODED PULSES

In this section, we consider phase coding where the phase changes in discrete steps, rather
than continuously. We consider a single pulse that is subdivided into a series of subpulses, or
chips, where the durations of the chips are equal. This is not a requirement, but a convenience
for our purposes. We then assign a different phase to each chip according to some rule
defined by a phase coding algorithm. We assume the amplitudes of all chips are equal. This is
a “semi requirement” of most phase coding schemes in that they were developed under the
assumption that the amplitudes of the chips are the same. In most applications, the chips are
adjacent. That is, the waveform has a 100% duty cycle. Again, this is not a hard requirement
but is a standard to which waveform designers generally adhere.
We can use this definition to write the normalized, baseband equation of a phase coded
pulse as
where τc is the chip width and the pulse consists of ϕk chips. The phases, ϕk are assigned
according to some phase coding algorithm.
As indicated earlier, the first phase coding algorithm used in a radar was based on a
random selection of 0 or π phase shifts across 200 chips. Since that time, analysts have
developed a wide variety, and a large number, of phase coding algorithms [9, 31–41].
In this book, we consider only a few phase coding algorithms. Two of these are Frank
polyphase and Barker codes, which are classical phase codes discussed in many radar books
[9, 42–48]. As an extension of Barker codes, we briefly discuss minimum peak sidelobe codes
[31, 32, 34, 49] and polyphase Barker codes [50–53].
The other phase coding algorithm we consider is derived from PRN codes. These are also
called maximal length codes, shifter register codes, shift register sequences, LSR (for linear
shift register) codes and a host of other names [44, 54]. PRN codes are used in many
applications including digital television, GPS (global positioning system), cell phones, spread
spectrum communications, and deep-space communications. They are attractive for radars
because they exhibit “good” range sidelobes and “good” off-axis sidelobes. They are also
useful in multiple radar applications, such as MIMO (multiple input, multiple output) radars,
[55, 56] because there are PRN codes of the same length that are almost orthogonal.
10.3.1 Frank Polyphase Coding
Frank polyphase coding is a digital representation of a quadratic phase shift, the phase shift
exhibited by LFM. Frank polyphase codes have lengths that are perfect squares, that is, K = L2
where L is an integer. The code can be formed by first creating an L × L matrix of the form

Next, the rows or columns are concatenated to form a vector of length K = L2. Finally, the
phase is determined by multiplying each element of the vector by
We illustrate this by an example. We consider L = 4, which produces a K = L2 = 16 element
Frank polyphase code. The Frank polyphase matrix is
and
The vector of phase shifts is

Figure 10.5 Plot of |χ(τ, f)| for a 16-chip Frank polyphase pulse.
The resulting Frank polyphase coded pulse is
A plot of |χ(τ, f)| for this example is shown in Figure 10.5. In the plot, Doppler ranges from
0 to 1/τc and range delay goes from –16τc to 16τc, where 16τc is the total duration of the pulse.
The plot of Figure 10.5 allows us to visualize the structure of the overall |χ(τ, f)| function
while still being able to visualize the matched-Doppler range cut. 3
It will be noted that the plot of Figure 10.5 exhibits some semblance of the ridge that is
characteristic of LFM waveforms. We might have expected this since the Frank polyphase
waveform is a discrete version of an LFM waveform. For reference, a plot of |χ(τ, f)| for an
LFM waveform with a BT product of 16 is contained in Figure 10.6. 4

Figure 10.6 Plot of |χ(τ, f)| for an LFM pulse with a BT product of 16.
Figure 10.7 Phases for 16-chip Frank polyphase and equivalent LFM pulse.
Figure 10.7 contains a plot of φ(k) (with appropriate phase unwrapping) for the 16-chip
example above. It also contains a plot of the phase shift of an LFM pulse that has a BT product
of 16, the same as the BT product of the 16-chip Frank polyphase pulse. As the figure
illustrates, the Frank polyphase pulse has approximately the same quadratic phase
characteristic as an equivalent LFM pulse.
Several other digital approximations of LFM waveforms have been developed over the

years. Two of these are the Zadoff and Chu codes discussed in [9, 57–59].
10.3.2 Barker Coded Waveforms
A simplification of polyphase coded pulses are those that use only two phase shifts, usually
separated by π (e.g., 0 and π, or −π/2 and π/2). These are termed binary phase codes. A
common set of binary phase codes found in radar texts are the Barker codes [60, 61]. Barker
codes have the interesting property that the peak level of the range sidelobes is 1/K, assuming
the peak of |χ(τ, f)| is normalized to unity. Although Barker codes have low range sidelobes,
the sidelobe levels of |χ(τ, f)| off of matched Doppler can be high, as shown in Figures 10.8
and 10.9.
Figure 10.8 is a plot of |χ(τ, f)|, similar to Figures 10.5 and 10.6. Figure 10.9 is a contour
plot of |χ(τ, f)|, showing f versus τ with 10log(|χ(τ, f)|) shown in grayscale. The bar to the right
of the plot provides the relation between the values of 10log(|χ(τ, f)|) and gray level.
As the plots of Figures 10.8 and 10.9 illustrate, the region near f = 0 has low amplitude
sidelobes. However, the off-axis regions exhibit several ancillary lobes. This is a property of
all of the Barker coded pulses.
Figure 10.8 Plot of |χ(τ, f)| for an 11-chip Barker coded pulse.

Figure 10.9 Contour plot of log(|χ(τ, f)|) for an 11-chip Barker coded pulse.
The off-axis behavior of Barker coded pulses is an illustration of a property of the
ambiguity function proved by Woodward, the inventor of the ambiguity function [62].
Specifically, the volume under the ambiguity function is constant and equal to its peak value.
That is
This says that if a coding reduces the ambiguity function in one region, the volume in that
region is distributed, in some fashion, to other regions. Sometimes it results in ancillary
lobes, as in Figures 10.8 and 10.9, and in other cases, it spreads out somewhat uniformly over
the τ-f region, as is the case with PRN coded pulses. As a note, this property applies to the
matched filter case, not to the mismatched filter.
There are only 7 known Barker codes. They have lengths of 2, 3, 4, 5, 7, 11, and 13. The
phase shifts for the 7 codes are shown in Table 10.1.
The low-range sidelobe characteristics of Barker coded pulses has motivated researchers
to find other, longer binary phase coded pulses that exhibit low range sidelobes. One of these
is a class of pulses termed minimum peak sidelobe coded pulses. According to Levanon and
Mozeson [9], sets of these have been developed by Linder [32], Cohen et al. [31, 39], and
Coxson et al. [33, 49]. The peak range sidelobes are not 1/K as with Barker coded pulses;
however, they are quite small. Table 10.2 contains a list of minimum peak sidelobe codes of

lengths 15 to 25. Other lists can be found in [31, 33, 34, 36].
Table 10.1
Phase Shifts for Barker Codes
Code Length
Phase Shifts
2
0  0  or  0  π
3
0  0  π
4
0  0  0  π  or  0  0  π  0
5
0  0  0  π  0
7
0  0  0  π  π  0  π
11
0  0  0  π  π  π  0  π  π  0  π
13
0  0  0  0  0  π  π  0  0  π  0  π  0
Table 10.2
Partial List of Minimum Peak Sidelobe Codes
Length
Code
15
001100000101011
16
0110100001110111
17
00111011101001011
18
011001000011110101
19
1011011101110001111
20
01010001100000011011
21
101101011101110000011
22
0011100110110101011111
23
01110001111110101001001
24
011001001010111111100011
25
1001001010100000011100111
Figure 10.10 contains range cuts of a 25-chip, minimum peak sidelobe pulse and a 25-chip,
Frank polyphase pulse. Note the peak sidelobes of the minimum peak sidelobe pulse are
considerably lower than those of the Frank polyphase pulse.
Another extension of Barker codes are generalized Barker codes or polyphase Barker
codes [50]. As the second name implies, these are not binary phase codes but polyphase codes.
The range sidelobes of pulses with these codes are below 1/K. Listings of polyphase Barker
codes can be found in [9, 35, 37, 38, 50–53], covering lengths of 4 to 45.

Figure 10.10 Matched filter response for 25-chip Frank polyphase and minimum peak sidelobe pulses.
10.3.3 PRN Coded Pulses
PRN coded pulses are another class of pulses that use binary phase coding. In this case, the
coding is based on PRN codes, which consist of sequences of 0s and 1s and most often have
lengths of K = 2M – 1, where M is an integer. The sequences of 0s and 1s are generated by
feedback shift register devices [54]. A functional block diagram of a feedback shift register is
contained in Figure 10.11. The boxes with Stage 1, Stage 2, and so forth, represent shift
register elements (flip-flops) and the adder is a modulo-2 adder. The block with z−1 is a delay,
or buffer, that holds the result of the modulo-2 addition before it is loaded in the first shift
register. The feedback configuration is usually chosen such that the sequence of 0s and 1s at
the output repeats only after K = 2M – 1 samples. Such a sequence of 0s and 1s is termed a
maximal length sequence or m-sequence [16, 42, 63–66]. The phase codes used on the chips of
the PRN coded pulse is the PRN sequence multiplied by π. Solomon Wolf Golomb is
generally credited with developing and characterizing maximal length sequences [15, 54].
However, in his book [16], Golomb gives credit to James Singer as the actual inventor of
maximal length sequences. 5
Table 10.3 contains a partial list of feedback configurations that can be used to generate
maximal length sequences for M between 3 and 10. The numbers in the table denote the shift
register outputs that are added and fed back to the input. The tap numbering corresponds to the
stage number in Figure 10.11. For example, the M = 4 case shown in the table is (4, 3) and
indicates that the output of shift registers 3 and 4 would be added and fed back to the first shift
register input. This specific example is illustrated in Figure 10.12. The entries in Table 10.3
were obtained from a website hosted by New Wave Instruments [70], which has a much more
complete list. Other sources include [63, 64, 71].
As pointed out in [70], the entries in Table 10.3 represent only half of the possible feedback
configurations. If one of the entries in the table is (M, a, b, c), the companion to that entry
would be (M, M − a, M − b, M − c). For example, one of the entries for M = 6 is (6, 5, 4, 1) so
its companion would be (6, 6 − 5, 6 − 4, 6 − 1) = (6, 1, 2, 5).

Figure 10.11 M-stage feedback shift register.
Table 10.3
Feedback Tap Configurations for M = 3 to 10
M
Feedback Taps
3
3, 2
4
4, 3
5
(5, 3), (5, 4, 3, 2), (5, 4, 3, 1)
6
(6, 5), (6, 5, 4, 1), (6, 5, 3, 2)
7
(7, 6), (7, 4), (7, 6, 5, 4), (7, 6, 5, 2), (7, 6, 4, 2), (7, 6, 4, 1), (7, 5, 4, 3), (7, 6, 5, 4, 3, 2), (7, 6, 5,
4, 2, 1)
8
(8, 7, 6, 1), (8, 7, 5, 3), (8, 7, 3, 2), (8, 6, 5, 4), (8, 6, 5, 3), (8, 6, 5, 2), (8, 7, 6, 5, 4, 2), (8, 7, 6,
5, 2, 1)
9
(9, 5), (9, 8, 7, 2), (9, 8, 6, 5), (9, 8, 5, 4), (9, 8, 5, 1), (9, 8, 4, 2), (9, 7, 6, 4), (9,7, 5, 2), (9, 6, 5,
3), (9, 8, 7, 6, 5, 3), (9, 8, 7, 6, 5, 1), (9, 8, 7, 6, 4, 3), (9, 8, 7, 6,4, 2), (9, 8, 7, 6, 3, 2), (9, 8, 7,
6, 3, 1), (9, 8, 7, 6, 2, 1), (9, 8, 7, 5, 4, 3), (9, 8, 7,5, 4, 2), (9, 8, 6, 5, 4, 1), (9, 8, 6, 5, 3, 2), (9,
8, 6, 5, 3, 1), (9, 7, 6, 5, 4, 3), (9, 7,6, 5, 4, 2), (9, 8, 7, 6, 5, 4, 3, 1)
10
(10, 7), (10, 9, 8, 5), (10, 9, 7, 6), (10, 9, 7, 3), (10, 9, 6, 1), (10, 9, 5, 2), (10, 9, 4, 2), (10, 8, 7,
5), (10, 8, 7, 2), (10, 8, 5, 4), (10, 8, 4, 3), (10, 9, 8, 7, 5, 4), (10, 9, 8, 7, 4, 1), (10, 9, 8, 7, 3, 2),
(10, 9, 8, 6, 5, 1), (10, 9, 8, 6, 4, 3), (10, 9, 8, 6, 4, 2), (10, 9, 8, 6, 3, 2), (10, 9, 8, 6, 2, 1), (10,
9, 8, 5, 4, 3), (10, 9, 8, 4, 3, 2), (10, 9, 7, 6, 4, 1), (10, 9, 7, 5, 4, 2), (10, 9, 6, 5, 4, 3), (10, 8, 7,
6, 5, 2), (10, 9,8, 7, 6, 5, 4, 3), (10, 9, 8, 7, 6, 5, 4, 1), (10, 9, 8, 7, 6, 4, 3, 1), (10, 9, 8, 6, 5, 4, 3,
2), (10, 9, 7, 6, 5, 4, 3, 2)
Figure 10.12 Shift register configuration of the (4, 3) entry of Table 10.3.
A particular maximal length sequence is generated by initializing the shift register with any
M digit binary number except zero. As an interesting note, the sequence generated with any
one shift register initialization (initial load) is not a unique sequence, but a circular shift of a
sequence that results from some other initial load (see Exercise 12). Unique sequences are
generated by choosing a different feedback configuration. Although changing the initial load
does not produce a unique sequence, it can have a significant effect of the range sidelobes of
the PRN coded pulse.

The operation of the shift register generator is as follows.
1. The modulo 2 addition is performed and the result is loaded into the z–1 buffer.
2. The shift register contents is shifted one bit to the right, and the output of shift register
element M is shifted into an output buffer.
3. The result stored in the z–1 buffer is loaded into shift register stage 1.
4. Steps 1 through 3 are repeated until the output buffer contains 2M – 1 elements.
An interesting feature of PRN coded pulses is that codes based on different feedback
configurations will be almost orthogonal. By “almost orthogonal” we mean that if the PRN
pulse based on one feedback configuration is processed through a matched filter matched to a
PRN pulse based on a different feedback configuration, the output will not have a peak, but
will look like noise (see Exercise 13).
Figure 10.13 contains a plot of |χ(τ, f)| for a 15-chip PRN coded pulse where the PRN code
was generated with the feedback configuration of Figure 10.12 and an initial load of 1111. The
range cut does not have sidelobes that are as low as comparable length Frank polyphase or
Barker pulses. However, |χ(τ, f)| does not have the ridge or peaks that the other two pulses
exhibit. This is a characteristic of PRN coded waveforms: their sidelobe levels are generally
“okay” but not extremely low or high. Long PRN coded waveforms have |χ(τ, f)| that approach
the ideal “thumbtack” function [17].

Figure 10.13 Plot of |χ(τ, f)| for a 15-chip PRN coded pulse.
The particular initial load used to generate Figure 10.13 resulted in low sidelobe levels
near the central peak. It turns out that the initial load can have a fairly significant impact on the
matched-Doppler range sidelobes of the ambiguity function (see Exercise 15). It also has a
lesser impact on the other range-Doppler sidelobes. The only known way to choose an initial
load that provides the desired sidelobe characteristics is to experiment.
10.3.3.1 Mismatched PRN Processing
We now want to investigate a special type of processing of PRN coded pulses that takes
advantage of an interesting property of PRN codes. The property we refer to is that the
circular autocorrelation of a PRN sequence has a value of either K or –1. With a circular
correlation, when we shift the sequence to the right by K chips, we take the K chips that “fall
off” the end of the shifted sequence and place them at the beginning of the shifted sequence.
This is illustrated in Figure 10.14. In this figure, we used the 7-bit PRN code of 1001110 to
generate the PRN coded sequence of –111–1–1–11.
To perform the circular autocorrelation, we make a copy of the sequence to produce two
sequences. We then circularly shift one sequence by K chips, multiply the result in K-chip
pairs, and form a sum across the K-chip result. This is illustrated in Figure 10.15.
Mathematically, we can write the circular correlation as
where (m)K denotes evaluation of m modulo K. The interesting property of PRN sequences is
that
Figure 10.14 Illustration of a 2-bit circular shift.

Figure 10.15 Illustration of circular correlation for k = 2.
We now apply this property to examine a special type of PRN coded waveforms. We will
assume that we encode a 0 of the PRN code to a phase of 0 and a 1 to a phase shift of β instead
of π. Thus, a PRN coded pulse corresponding to the 7-bit PRN code of Figures 10.14 and
10.15 (i.e., 1001110) would be as shown in Figure 10.16.
We assume the transmit waveform, u(t), is as shown in Figure 10.16. We define a matched
filter that is matched to a signal v(t), where v(t) is a concatenation of three u(t)s. Thus, v(t)
would be as shown in Figure 10.17. The t = 0 reference points in Figures 10.16 and 10.17 are
used to denote the time alignment for matched range. Thus, when the received signal (a scaled
version of Figure 10.16) is aligned with the center of the three PRN coded pulses of Figure
10.17, the matched filter is matched in range to the received pulse.
The matched filter output is
Figure 10.16 Seven-chip, PRN coded waveform.
Figure 10.17 Waveform to which the matched filter is matched.

Figure 10.18 Formation of u(t)v*(t + nτc)
We want to examine vMF(t) for τ = nτc where n is an integer between –(N – 1) and N – 1. We
particularly want to examine the form of u(t)v*(t + nτc). This is illustrated in Figure 10.18 for
the 7-chip PRN coded waveform and n = 3.
When we form u(t)v*(t + nτc), we get
and
We note from Figure 10.18 that ϕk − ϕ(k+3)N is equal to either 0, β, or −β. We note further that
there are three cases where ϕk − ϕ(k+3)K = 0, 2 cases where ϕk − ϕ(k+ 3)N = β and two cases
where ϕk − ϕ(k+3)K = −β. With this, we get
It turns out that for all τc < |τ| < (N – 1) τc
In fact, for any K-chip (K = 2M − 1) PRN-coded waveform with u(t) and v(t) chosen by the
above rule,

Stated in words, the range sidelobes within K − 1 chips of the mainlobe have a constant value
as given by (10.46).
As an interesting extension of the above, if we choose β such that
or
we get
That is, range sidelobes within K − 1 chips of the mainlobe are zero. This has the potential of
being useful when a radar must be able to detect or track a very small target in the presence of
a very large target, provided both targets are at the same Doppler frequency.
Figure 10.19 contains a plot of |χ(τ, f)| for the 7-chip PRN example above for the case
where β was chosen to be

Figure 10.19 Plot of |χ(τ, f)| for 7-chip PRN pulse with mismatched filter.
As predicted, the range sidelobes around the central peak are zero. However, the sidelobes
off of matched Doppler rise significantly. Also, the range cut contains two extra peaks. These
peaks are range ambiguities and are due to the fact that u(t) correlates with each of the other
two end PRN coded pulses of v(t). The range sidelobes adjacent to these range ambiguities are
the normal range sidelobes associated with PRN coded waveforms.
10.4 STEP FREQUENCY WAVEFORMS
In a step frequency waveforms the carrier frequency is changed from pulse to pulse. The
specific case we consider is shown in Figure 10.20. For this analysis, and in most practical
applications, we assume the radar operates unambiguously in range. That is, the signal from
pulse k is received before pulse k + 1 is transmitted. Thus, we can think of processing one
pulse at a time and saving the results for later, further, processing.
We assume the frequency, fk, of the kth pulse is given by
where f0 is the carrier frequency and Δf is the frequency step. To simplify the development,
we assume the individual pulses are unmodulated, though this is not necessary, or desired, in

practical applications. We can write the normalized transmit signal for the kth pulse as
The normalized signal returned from a target at a range delay of τR is
Figure 10.20 Step frequency waveform.
We assume we know τR well enough to be able to sample the matched filter output near its
peak. A more accurate measurement of τR will be obtained from the output of the step
waveform signal processor. For now, we assume the radar and target are fixed so that τR is
constant.
The heterodyne signal is given by
where τ′R is close to τR and τh is large enough for the rect[x] of (10.54) to overlap the rect[x]
of (10.53). The frequency of the heterodyne signal is different for every pulse and hk(t) is
perfectly coherent with vTk(t). The output of the heterodyne operation is
The first term is a constant phase shift that is common to all pulses. We will lump it into some
constant that we normalize to unity.
For the next step, we process vHk(t) by a matched filter matched to rect[t/τp] to produce a
normalized output of

where tri[x] is a triangle centered at x = 0 with a base width of 2 and a height of unity.
Finally, we sample vMk(t) at some τ, close to τR, to obtain
After we obtain vMk(τ) from N pulses, we form the sum
where the ak are complex weight coefficients that we want to choose to maximize |V(τ − τR)|.
We recognize (10.58) as the form of the sum we encountered in our antenna and will
encounter in our stretch processing analyses. We can use this knowledge to postulate that the
ak that will maximize |V(τ − τR)| are
With this we write V(τ − τR) as
which we evaluate and normalize to yield
A plot of |V(τ − τR)| versus (τ − τR)∆f is shown in Figure 10.21 for N = 10 and without the tri[x]
function. The central peak occurs at (τ − τR)∆f = 0 and the first null occurs at |(τ − τR)∆f| = 1/N
= 0.1. The other peaks, which are range ambiguities, are located at integer values of (τ − τR)∆f.
This tells us the range resolution of the waveform is

and the range ambiguities are located at
Figure 10.21 Plot of |V(τ – τR)| without tri[x].
In the above development, we ignored the tri[x] function to emphasize the location of range
ambiguities. If we now include it, we can quantify the effect of the single-pulse matched filter
on |V(τ − τR)|. We will add the extra step of recognizing that V(τ − τR) is the matched-Doppler
range cut |χ(τ, f)| for vTk (t) and, in future references, use V(τ − τR) = χ(τ − τR, 0). With this,
Figure 10.22 contains plots of |χ(τ − τR, 0)| for ∆fτp = 0.5, 1 and 2. The top plot corresponds to
the case of ∆fτp = 0.5 and the bottom plot corresponds to the case of ∆fτp = 2. The dashed
triangles are the single-pulsed matched filter responses.
For∆fτp = 0.5 and 1, the single-pulse matched filter response nullifies the range ambiguities.
However, when ∆fτp = 2, the range ambiguities are present. This interaction between the
single-pulse matched filter response and the presence of range ambiguities is a limitation that
must be considered when designing step frequency waveforms.
Equation (10.62) tells us we can improve resolution by either increasing ∆f or N. If we
increase ∆f we must consider the properties indicated in Figure 10.22 and ensure that

Figure 10.22 Plots of |χ(τ – τR, 0)| for ∆fτp = 0.5, 1, and 2.
We note that (10.64) can also be satisfied by decreasing τp. Thus, we could increase ∆f to
improve resolution if we can effectively reduce τp to satisfy (10.64).
A means of effectively reducing τp is to phase code the individual pulses. In that case, τp
would be the compressed pulsewidth, τc.
Increasing the number of pulses to improve resolution must be done with care because it
can have negative consequences in terms of timelines and the potential impact of target
motion.
10.4.1 Doppler Effects
We now consider the effects of target motion. For now, we will be concerned only with target
Doppler frequency. To include target Doppler frequency, we write the target range as
and the target range delay as
We can write the target range delay at the time of the kth transmit pulse as

where T is the PRI.
If the transmit signal as defined in (10.52), the received signal is
Manipulating the above using (f0 + kΔf)(fd/f0) ≈ fd and τRk ≈ τR0 (in the rect[x] function), we get
We note that the approximation of (f0 + k∆f)(fd/f0) ≈ fd may not be very good because the k∆f
term could cause a degradation in resolution. However, the approximation allows us to focus
on the effects of target Doppler, and not the potential resolution degradation. This would need
to be considered in a more complete analysis.
If we compare (10.69) with (10.53), we note the only difference is the appearance of the
term related to Doppler. Thus, if we repeat the heterodyning and matched filtering math from
above, we get
Forming the weighted sum of the vMk(τ − τR0) yields
As we did earlier, we could choose the bk to maximize |V(τ − τR)|. However, a more general
form would be to use
which would yield

or, evaluating and normalizing the sum,
Figure 10.23 contains a matched-range, Doppler-cut (a plot of |χ(0, f – fd)| versus f − fd,) of
the 10-pulse waveform discussed earlier. In this case, we needed specific values for the
parameters and thus chose a PRI of 500 µs, τp = 1 µs, and Δf = 1 MHz. It will be noted that the
Doppler resolution of this waveform is 200 Hz or 1/NT = 1/(10 × 500 µs), as expected.
Figure 10.23 Matched-range, Doppler cut for a step frequency waveform.
Figure 10.24 Range cuts of a step frequency waveform.
Figure 10.24 contains range cuts at matched Doppler and at a Doppler offset of one

Doppler resolution cell (200 Hz). Note that a Doppler offset of one Doppler resolution cell
causes a range error of one range resolution cell. This indicates the step frequency waveform
is very sensitive to Doppler and that, if we want accurate absolute range measurement, the
range shift due to target Doppler must be removed.
This can be done if the target is in track and the relative velocity between the radar and
target is known with reasonable accuracy.
If the step frequency waveform is used in its more common role of target imaging, the
various scatterers of the target should be moving at about the same range rate so that range
errors due to Doppler differences of the scatterers should be small. One would still want to
remove the gross Doppler to minimize losses due to Doppler mismatch. Note that the range
cut at f = 1/NT shown in Figure 10.24 is down about –20log(0.9) ≈ 1 dB.
10.5 CLOSING COMMENTS
With the exception of LFM pulses, Barker coded pulses, and some short PRN coded pulses,
waveforms of the type mentioned in this chapter were very difficult to implement in older
radars because they were difficult to generate and process with older, analog hardware.
However, the advent of direct digital synthesizers [72–74] and FFT-based signal processors
[75–77] has essentially removed the hardware constraint. This means we can expect to see
waveforms such as those discussed in this chapter, and even more complicated waveforms,
come into fairly common use.
10.6 EXERCISES
1.
We did not discuss matched range Doppler cuts of |χ(τ, f)|. This is because the matched
range Doppler cut does not depend on the phase modulation on the pulse, φ(t). Prove that
this is a correct statement by showing that the matched range Doppler cut of the
waveform of (10.1) is given by |χ(0, f)| = K|sinc(fτp)|.
2.
Reproduce the plots of Figure 10.1.
3.
Show that the scaling constant, I, of (10.22) is K = π/[ B(πa + 2b)].
4.
Derive (10.25) and (10.26).
5.
Reproduce the plots of Figures 10.3 and 10.4.
6.
Use the numerical technique of Section 10.2.2.3 to design an NLFM waveform for the
case where Um2(ζ) is a Hamming weighting function (see Example 2 of Section 10.2.2.2).
7.
Reproduce the plot of Figure 10.3. Produce a similar plot for a 25-chip pulse with Frank
polyphase coding.
8.
Generate a plot like Figure 10.8 for a 25-chip pulse with minimum peak sidelobe coding.
9.
Generate a plot like Figure 10.8 for a 13-chip Barker coded pulse.

10. Generate a plot like Figure 10.13 for a 63-chip PRN coded pulse.
11. Generate a plot like Figure 10.19 where the base pulse is a 15 -chip PRN coded pulse.
12. Show, by example, that two PRN codes generated by two different loads of a shift register
generator are circular shifts of each other. Use a 7- or 15-element code to simplify the
problem.
13. It was stated that PRN coded pulses based on different shift register feedback
configurations were “almost orthogonal.” That is, if a PRN coded pulse is based on one
feedback configuration and the matched filter is based on a pulse derived with a different
feedback configuration, the output of the matched filter would not exhibit a predominant
peak as it would if the matched filter was matched to the input pulse. To verify this
assertion, generate a PRN coded pulse, u(t), using one of the feedback configurations for
M = 6 in Table 10.3. Match the matched filter to another pulse, v(t), based on a different
feedback tap configuration for M = 6. Process u(t) through the matched filter matched to
v(t) and plot the output. Does it behave as claimed?
14. Reproduce the plots of Figures 10.21 and 10.22.
15. Generate plots like Figure 10.13 for different initial loads of the feedback shift register.
Make a note of the range sidelobe levels close to the main peak as you change initial
loads.
References
[1]
Ewell, G. W., Radar Transmitters, New York: McGraw-Hill, 1981.
[2]
Ostroff, E. D., et al., Solid-State Radar Transmitters, Dedham, MA: Artech House, 1985.
[3]
Cooke, C.E., “The Early History of Pulse Compression Radar: The History of Pulse Compression at Sperry Gyroscope
Company,” IEEE Trans. Aerosp. Electron. Syst., vol. 24, no. 6, Nov. 1988, pp. 825—833.
[4]
Dicke, R. H., “Object Detection System,” U.S. Patent 2,624,876, Jan. 6, 1953.
[5]
Darlington, S., “Pulse Transmission,” U.S. Patent 2,678,997. May 18, 1954.
[6]
Cook, C. E., “Pulse Compression-Key to More Efficient Radar Transmission,” Proc. IRE, vol. 48, no. 3 Mar. 1960, pp.
310–316. Reprinted in: Barton, D. K., Radars, Vol. 3: Pulse Compression (Artech Radar Library), Dedham, MA: Artech
House, 1975.
[7]
Klauder, J. R. et al., “The Theory and Design of Chirp Radars,” Bell Syst. Tech. J., vol. 39, no. 4, Jul. 1960, pp. 745–
808.
[8]
Skolnik, M. I., “Fifty Years of Radar,” Proc. IEEE, vol. 73, no. 2, Feb. 1985, pp. 182–197.
[9]
Levanon, N., and E. Mozeson, Radar Signals, New York: Wiley-Interscience, 2004.
[10] Rihaczek, A. W., Principles of High-Resolution Radar, New York: McGraw-Hill, 1969; Norwood, MA: Artech House,
1995.
[11] Oppliger, R., Contemporary Cryptography, 2nd ed., Norwood, MA: Artech House, 2011.
[12] Holmes, J. K., Spread Spectrum Systems for GNSS and Wireless Communications, Norwood, MA: Artech House,
2007.
[13] Ziemer, R. E., and W. H. Tranter, Principles of Communications, 3rd ed., Norwood, MA: Houghton Mifflin, 1990.
[14] Glisic, S., and B. Vucetic, Spread Spectrum CDMA Systems for Wireless Communications, Boston, MA: Artech House,
1997.

[15] Golomb, S. W., ed., Digital Communications with Space Applications, Englewood Cliffs, NJ: Prentice-Hall, 1964.
[16] Golomb, S. W., and G. Gong, Signal Design for Good Correlation: For Wireless Communication, Cryptography, and
Radar, New York: Cambridge University Press, 2005.
[17] Nyquist, H., “Certain Topics in Telegraph Transmission Theory,” Trans. AIEE, vol. 47, no. 2, Apr. 1928, pp. 617–644.
Reprinted as a classic paper in Proc. IEEE, vol. 90, no. 2, Feb. 2002.
[18] Shannon, C. E., “Communication in the Presence of Noise,” Proc. IRE, vol. 37, no. 1, Jan. 1949, pp. 10–21. Reprinted as
a classic paper in Proc. IEEE, vol. 86, no. 2, Feb. 1998.
[19] Kotel’nikov, V. A., “On the Capacity of the ‘Ether’ and Cables in Electrical Communications,” in Proc. First All-Union
Conf. Technolog. Reconstruction of the Commun. Sector and Develop. of Low-Current Eng., Moscow: 1933. Translated
by C. C. Bissell and V. E. Katsnelson. http://ict.open.ac.uk/classics/1.pdf.
[20] Key, E. L., E. N. Fowle, and R. D. Haggarty, “A Method of Pulse Compression Employing Non-Linear Frequency
Modulation,” MIT Lincoln Lab., Lexington, Mass., Tech. Rep. No. 207, Aug. 1959.
[21] Key, E. L., E. N. Fowle, and R. D. Haggarty, “A Method of Designing Signals of Large Time-Bandwidth Product,” in
IRE Int. Conv. Rec., pt. 4, 1961, pp. 146–154.
[22] Watters, E. C. “A Note on the Design of Coded Pulses,” in Proc. Pulse Compression Symp., Rome Air Develop. Ctr.,
Rep. No. TR–59–161, Griffis AFB, NY, Defense Tech. Inf. Ctr., Sept. 1959.
[23] Fowle, E., “The Design of FM Pulse Compression Signals,” IEEE Trans. Inf. Theory, vol. 10, no. 1, Jan. 1964, pp. 61–
67.
[24] Labitt, M., “Obtaining Low Sidelobes Using Non-Linear FM Pulse Compression,” MIT Lincoln Lab., Lexington, MA,
Rep. No. ATC-223, Nov. 4, 1994. Made available by Nat. Tech. Inf. Service, Springfield, VA.
[25] Doerry, A. W., “Generating Nonlinear FM Chirp Waveforms for Radar,” Sandia Nat. Labs., Albuquerque, NM, Rep. No.
SAND2006–5856, Sept. 2006.
[26] Collins, T., and P. Atkins, “Nonlinear Frequency Modulation Chirps for Active Sonar,” IEEE Proc. Radar, Sonar and
Navigation, vol. 146, no. 6, Dec. 1999, pp. 312–316.
[27] Varshney, L. R., and D. Thomas, “Sidelobe Reduction for Matched Filter Range Processing,” in Proc. 2003 IEEE Radar
Conf., Huntsville, AL, May 5–8, 2003, pp. 446–451.
[28] Yichun, P., et al., “Optimization Design of NLFM Signal and Its Pulse Compression Simulation,” 2005 IEEE Int. Radar
Conf., Arlington, VA, May 9–12, 2005, pp. 383–386.
[29] Erdelyi, A., Asymptotic Expansions, New York: Dover, 1956.
[30] Bleistein, N., and R. A. Handelsman, Asymptotic Expansions of Integrals, New York: Holt, Rinehart and Winston, 1975.
[31] Cohen, M. N., M. R. Fox, and J. M. Baden, “Minimum Peak Sidelobe Pulse Compression Codes,” in Proc. 1990 IEEE
Int. Radar Conf., Arlington, VA, May 7–10, 1990, pp. 633–638.
[32] Lindner, J. “Binary Sequences Up to Length 40 with Best Possible Autocorrelation Function,” Proc. IEEE, vol. 11, no. 21
(Oct. 1975); Electron. Lett., vol. 11, no. 21, Oct. 16, 1975, p. 507.
[33] Coxson, G., and J. Russo, “Efficient Exhaustive Search for Optimal-Peak-Sidelobe Binary Codes,” IEEE Trans. Aerosp.
Electron. Syst., vol. 41, no. 1, Jan. 2005, pp. 302–308.
[34] Leukhin, A. N., and E. N. Potekhin, “Optimal Peak Sidelobe Level Sequences Up to Length 74,” 2013 European Radar
Conf. (EuRAD), Nuremberg, Germany, Oct. 9–11, 2013, pp. 495–498.
[35] Gabbay, S. “Properties of Even-Length Barker Codes and Specific Polyphase Codes with Barker Type Autocorrelation
Functions,” Naval Research Laboratory, Washington, D.C., Rep. 8586, Jul. 12, 1982. Available from DTIC as AD
A117415.
[36] Nunn, C. J., and G. E. Coxson, “Best-Known Autocorrelation Peak Sidelobe Levels for Binary Codes of Length 71 to
105,” IEEE Trans. Aerosp. Electron. Syst., vol. 44, no. 1, Jan. 2008, pp. 392–395.
[37] Nunn, C. J., and G. E. Coxson, “Polyphase Pulse Compression Codes with Optimal Peak and Integrated Sidelobes,”
IEEE Trans. Aerosp. Electron. Syst., vol. 45, no. 2, Apr. 2009, pp. 775–781.
[38] Borwein, P., and R. Ferguson, “Polyphase Sequences with Low Autocorrelation,” IEEE Trans. Inf. Theory, vol. 51, no.
4, Apr. 2005, pp. 1564–1567.
[39] Cohen, M. N., J. M. Baden, and P. E. Cohen, “Biphase Codes with Minimum Peak Sidelobes,” Proc. 1989 IEEE Nat.
Radar Conf., Dallas, TX, Mar. 29–30, 1989, pp. 62–66.

[40] Frank, R. L., “Phase Coded Communication System,” U.S. Patent 3,099,795, Jul. 30, 1963.
[41] Frank, R. L., “Polyphase Codes with Good Nonperiodic Correlation Properties,” IEEE Trans. Inf. Theory, vol. 9, no. 1,
Jan. 1963, pp. 43–45.
[42] Cook, C. E., and M. Bernfeld, Radar Signals: An Introduction to Theory and Application, New York: Academic Press,
1967; Norwood, MA: Artech House, 1993.
[43] Lewis, B. L., et al., Aspects of Radar Signal Processing, Norwood, MA: Artech House, 1986.
[44] Skolnik, M. I., Introduction to Radar Systems, 3rd ed., New York: McGraw-Hill, 2001.
[45] Meikle, H., Modern Radar Systems, 2nd ed., Norwood, MA: Artech House, 2008.
[46] Hovanessian, S. A., Radar System Design and Analysis, Norwood, MA: Artech House, 1984
[47] Eaves, J. L., and E. K. Reedy, Principles of Modern Radar, New York: Van Nostrand Reinhold, 1987.
[48] Nathanson, F. E., J. P. Reilly, and M. N. Cohen, eds., Radar Design Principles, 2nd ed., New York: McGraw-Hill, 1991.
[49] Coxson, G. E., A. Hirschel, and M. N. Cohen, “New Results on Minimum-PSL Binary Codes,” Proc. 2001 IEEE Nat.
Radar Conf., Atlanta, GA, May 2001, pp. 153–156.
[50] Bömer, L., and M. Antweiler, “Polyphase Barker Sequences,” Electron. Lett., vol. 25, no. 23, pp. 1577–1579, Nov.
1989.
[51] Friese, M., and H. Zottmann, “Polyphase Barker Sequences Up to Length 31,” Electron. Lett., vol. 30, no. 23, Nov.
1994, pp. 1930–1931.
[52] Friese, M., “Polyphase Barker Sequences Up to Length 36,” IEEE Trans. Inf. Theory, vol. 42, no. 4, Jul. 1996, pp.
1248–1250.
[53] Brenner, A. R., “Polyphase Barker Sequences Up to Length 45 with Small Alphabets,” Electron. Lett., vol. 34, no. 16,
pp. 1576–1577, Aug. 1998.
[54] Golomb, S. W., Shift Register Sequences, San Francisco, CA: Holden-Day, 1967.
[55] Guerci, J. R., Cognitive Radar: The Knowledge-Aided Fully Adaptive Approach, Norwood, MA: Artech House, 2010.
[56] Li, J., and P. Stoica, eds., MIMO Radar Signal Processing, New York: Wiley & Sons, 2009.
[57] Zadoff, S. A., “Phase Coded Communication System,” U. S. Patent 3,099,796, Jul. 30, 1963.
[58] Chu, D. C., “Polyphase Codes with Good Periodic Correlation Properties,” IEEE Trans. Inf. Theory, vol. 18, no. 4, Jul.
1972, pp. 531–532.
[59] Antweiler, M., and L. Bömer, “Merit Factor of Chu and Frank Sequences,” Electron. Lett., vol. 26, no. 25, Dec. 6, 1990,
pp. 2068–2070.
[60] Barker, R. H., “Group Synchronization of Binary Digital Systems,” in Communication Theory (W. Jackson, ed.), London:
Academic Press, 1953, pp. 273–287.
[61] Turyn, R., “On Barker Codes of Even Length,” Proc. IEEE, vol. 51, no. 9, Sept. 1963, p. 1256.
[62] Woodward, P. M., Probability and Information Theory with Applications to Radar, 2nd ed., New York: Pergamon
Press, 1953; Dedham, MA: Artech House, 1980.
[63] Dixon, R. C., Spread Spectrum Systems with Commercial Applications, 3rd ed., New York: Wiley & Sons, 1994.
[64] Michelson, A. M., and A. H. Levesque, Error-Control Techniques for Digital Communication, New York: Wiley &
Sons, 1985.
[65] Berkowitz, R. S., Modern Radar: Analysis, Evaluation, and System Design, New York: Wiley & Sons, 1965.
[66] Peterson, W. W., and E. J. Weldon, Error-Correcting Codes, Cambridge, MA: MIT Press, 1972.
[67] IEEE Information Theory Society (ITSOC), “Claude E. Shannon Award.” http://www.itsoc.org/honors/claude-e-shannon-
award.
[68] IEEE, “IEEE Richard W. Hamming Medal Recipients.” http://www.ieee.org/documents/hamming_rl.pdf.
[69] National Science Foundation (NSF), “US NSF–About Awards.” http://www.nsf.gov.
[70] New Wave Instruments, “Linear Feedback Shift Registers—Implementation, M-Sequence Properties, Feedback Tables,”
Apr. 5, 2005. www.newwaveinstruments.com.
[71] Skolnik, M. I., ed., Radar Handbook, 2nd ed., New York: McGraw-Hill, 1990.

[72] Cushing, R., “A Technical Tutorial on Digital Signal Synthesis,” 1999. www.analog.com.
[73] Analog Devices, “AD9858 1 GSPS Direct Digital Synthesizer,” 2003. www.analog.com.
[74] Kester, W., ed., The Data Conversion Handbook, New York: Newnes, 2005.
[75] Rabiner, L. R., and B. Gold, Theory and Application of Digital Signal Processing, Englewood Cliffs, NJ: Prentice-Hall,
1975.
[76] Martinson, L., and R. Smith, “Digital Matched Filtering with Pipelined Floating Point Fast Fourier Transforms (FFT’s),”
IEEE Trans. Acoust., Speech, Signal Process., vol. 23, no. 2, Apr. 1975, pp. 222–234.
[77] Blankenship, P., and E. M. Hofstetter, “Digital Pulse Compression Via Fast Convolution,” IEEE Trans. Acoust., Speech,
Signal Process., vol. 23, no. 2, Apr. 1975, p. 189–201.
APPENDIX 10A: LFM AND THE sinc2(x) FUNCTION
In footnote 2 of Section 10.2.1, we noted that the matched filter output for an unweighted LFM
pulse had a shape similar to a sinc2(x) function, but the match was not exact. In particular, we
noted the first two sidelobe levels were about –14 and –19 dB instead of –13.2 and –17.8 dB.
This can be explained by examining the equation for the matched-Doppler range cut of the
ambiguity function we derived in Chapter 9 [see (9.30)]. That equation is
We note the matched-Doppler range cut does contain a sinc2(x) function, but with the added
term (τp –| τ |) in the argument. It is the presence of this term that causes the sidelobes to be
lower than those of the sinc2(x) function. As the BT product of the waveform is increased, the
(τp –| τ |) term has less of an effect on the first few sidelobes, which means they would
approach those of a sinc2(x) function.
We can also explain this from a frequency domain perspective. To that end, Figure 10A.1
contains a plot of the frequency spectrum of the 15-μs, 2-MHz LFM pulse considered in
Section 10.2 [Figure 10A.1 was generated using (11.6)]. The figure also contains an ideal
spectrum with the same bandwidth. If the ideal spectrum was that of some hypothetical pulse,
the matched-Doppler range cut of the pulse would be a sinc2(x) function. The nature of the
matched-Doppler range cut of the LFM ambiguity function is due to the ripples (which are
termed Fresnel ripples) and skirts of the LFM pulse spectrum. The ripples and skirts of the
LFM spectrum are also what caused the sidelobes of the matched filter output of the weighted
LFM pulse (Figure 10.1) to be other than the expected –30 dB normally associated with 30-dB
Taylor weighting.

Figure 10A.1 Spectrum of a 15-μs, 2-MHz LFM pulse and an ideal 2-MHz spectrum.
As a comparison, Figure 10A.2 contains the matched-Doppler range cut and spectrum for a
LFM pulse with a duration of 150 µs and a bandwidth of 2 MHz (a BT product of 300 instead
of 30). The full extent of the matched-Doppler range cut is not shown so we could more easily
see the first few sidelobes. Note that the spectrum more closely approximates the ideal
spectrum and the first two sidelobe of the matched-Doppler range cut are closer to –13.2 and
–17.8 dB.
Figure 10A.2 Matched-Doppler range cut (left) and spectrum (right) of a 150-µs, 2-MHz LFM pulse.
1 Strictly speaking, vT(t) is an idealized form of the transmit pulse. The actual pulse cannot have a true rectangular envelope
because such an envelope implies the transmitter has infinite bandwidth. Practically, the envelope of the transmit pulse is close
to rectangular.

2 As a note, as the BT product becomes larger, the first and second sidelobes will approach those of the first and second
sidelobes of a sinc(x) function, 13.3 and 17.8 dB, respectively. See Appendix 10A for a more detailed explanation.
3 This plotting methodology was adapted from that used in Levanon and Mozeson [9].
4 The BT product of a K-chip phase coded pulse is normally equal to K. This derives from the observation that the pulse
bandwidth is B = 1/τc and the duration of the pulse is τp = Kτc. Thus BT = Bτp = (1/τc)(Kτc) = K.
5 Among other awards for his contributions to information theory and shift register sequence theory and their application in
digital communications, Golomb was awarded the IEEE Shannon Award in 1985 [67], IEEE Richard W. Hamming Medal in
2000 [68], and National Medal of Science in 2011 [69].

Chapter 11
Stretch Processing
11.1 INTRODUCTION
Stretch processing is a way of processing large bandwidth waveforms using narrow band
techniques. For our present purposes, we want to look at stretch processing as applied to
waveforms with linear frequency modulation (LFM). The concepts of stretch processing also
appear in other applications such as frequency modulated continuous wave (FMCW) radar [1]
and, as we will see in Chapter 15, synthetic aperture radar (SAR).
Stretch processing was developed by Dr. William J. Caputi, Jr. [2]. In recognition of this
and other efforts in SAR, Dr. Caputi was awarded the IEEE Dennis Picard Medal “for
conception and development of innovative range and Doppler bandwidth reduction techniques
used in wideband radars and high resolution synthetic aperture radars” [3].
We consider a normalized, LFM, transmit waveform of the form
where
α is the LFM slope and τp is the uncompressed pulsewidth. The instantaneous phase of v(t) is
and the instantaneous frequency is
Over the duration of the pulse, f (t) varies from –ατp/2 to ατp/2. Thus, the bandwidth of the
LFM signal, v(t), is

We can also determine the bandwidth of v(t) by finding and plotting its Fourier transform.
Specifically,
where
is the Fresnel integral and C(x) and S(x) are the cosine and sine Fresnel integrals, respectively,
defined by [4], [p. 296]
and
A normalized plot of | V(f) | for an LFM bandwidth of B = 500 MHz and a pulsewidth of τp =
100 μs is shown in Figure 11.1. Note that the bandwidth is 500 MHz.
Figure 11.1 Spectrum of an LFM pulse with B = 500 MHz and τp = 100 µs.
If we were to process the LFM pulse using a matched filter, the normalized impulse
response of the matched filter would be

where we have made use of the fact that rect [x] is an even function.
The form of h(t) means the matched filter would need to have a bandwidth of B = |ατp|.
Herein lies the problem: large bandwidth matched filters are still difficult and costly to build.
Two methods of building LFM matched filters (LFM pulse compressors, LFM signal
processors) are surface acoustic wave (SAW) devices and digital signal processors [5–7]. A
cursory survey of manufacturer literature and other sources indicates that the current state of
SAW technology limits these types of processors to 1,000 MHz bandwidth and BT products
on the order of 10,000.
The bandwidth of digital signal processors is usually limited by the sample rate of the
analog-to-digital converters (ADCs) needed to convert the analog signal to a digital signal.
Although the technology is progressing rapidly, the current limit on ADC rates is 1,000 MHz
or so [5]. If an upper limit on ADC sample rate is 1,000 MHz, then the maximum bandwidth of
an LFM signal processor would also be 1,000 MHz (assuming complex signals and
processors).
Stretch processing relieves the signal processor bandwidth problem by giving up all-range
processing to obtain a narrowband signal processor. If we were to use a matched filter, we
could look for targets over the entire waveform pulse repetition interval (PRI). With stretch
processing, we are limited to a range extent that is usually smaller than an uncompressed
pulsewidth. Thus, we could not use stretch processing for search because search requires
looking for targets over a large range extent, usually many pulsewidths long. We could use
stretch processing for track because we already know range fairly well but want a more
accurate measurement of it. However, we point out that, in general, wide bandwidth
waveforms, and thus the need for stretch processing, is “overkill” for tracking. Generally
speaking, bandwidths of 1s to 10s of MHz are sufficient for tracking.
One of the most common uses of wide bandwidth waveforms and stretch processing is in
discrimination, where we need to distinguish individual scatterers on a target. Another use is
in SAR. In that application we only try to map a small range extent of the ground but want
very good range resolution to distinguish the individual scatterers that constitute the scene.
In the above discussion, we focused on the signal processor and have argued, without proof
at this point, that we can use stretch processing to ease the bandwidth requirements on a signal
processor used to compress wide bandwidth waveforms. Stretch processing does not relieve
the bandwidth requirements on the rest of the radar. Specifically, the transmitter must be
capable of generating and amplifying the wide bandwidth signal, the antenna must be capable
of radiating the transmit signal and capturing the return signal, and the receiver must be
capable of heterodyning and amplifying the wide bandwidth signal. This places stringent
requirements on the transmitter, antenna, and receiver, but current technology has advanced to
cope with the requirements [8–11].

11.2 STRETCH PROCESSOR CONFIGURATION
Figure 11.2 contains a functional block diagram of a stretch processor. It consists of a mixer,
an LFM generator, timing circuitry, and a spectrum analyzer. If the transmit signal is as given
in (11.1), the normalized (idealized) signal returned from a point scatterer at a range delay of
τR is
where (PS)1/2 is a scaling factor that we will use when we address signal-to-noise ratio (SNR).
PS is the peak signal power at the matched filter output and comes from the radar range
equation (see Chapters 2 and 7).
Figure 11.2 Stretch processor.
The normalized heterodyne signal generated by the LFM generator is
In the above, τM is the range delay to which the stretch processor is “matched” and is usually
close to τR. Actually, usually is not the correct word. A more precise statement is that τM must
be close to the τR of the scatterers we wish to resolve. τh is the duration of the heterodyne
signal and, as we will show, must satisfy τh > τp.
Notional sketches of the frequency behavior of r(t) and hs(t) are shown in Figure 11.3. The
horizontal axis is time and the vertical axis is frequency. The frequency of each signal is
shown only over the time that the signal itself is not zero. Since r(t) and hs(t) are LFM signals,
we note that their frequencies increase linearly over their respective durations. Furthermore,
by design, both frequency versus time plots have the same slope of α. The top plot
corresponds to the case where the target range delay, τR, is greater than τM and the lower plot
corresponds to the case where the range delay is less than τM. It will be noted that when τR >
τM, the frequency of hs(t) is greater than the frequency of r(t). When τR < τM, the frequency of

hs(t) is less than the frequency of r(t). Further, the size of the frequency difference between
r(t)and hs(t) depends upon the difference between τR and τM.
Figure 11.3 also tells us how to select the value of τh, the duration of the heterodyne signal.
Specifically, we want to choose τh so that r(t) is completely contained within hs(t) for all
expected values of τR relative to τM. From the bottom plot of Figure 11.3 we conclude we want
to choose τh such that
From the top plot we want to choose it such that
Figure 11.3 Sketches of r(t) and hs(t).
In (11.13) and (11.14) τRMIN and τRMAX are the minimum and maximum expected values of
τR. Equations (11.13) and (11.14) lead to the requirement on τh that it satisfy

where
is the range delay extent over which we want to use stretch processing. If τh satisfies the above
constraint and
then hs(t) will completely overlap r(t) and the stretch processor will offer almost the same
SNR performance as a matched filter. If the various timing parameters are such that hs(t) does
not completely overlap r(t), the stretch processor will experience an SNR loss proportional to
the extent of r(t) that does not lie within the extent of hs(t). It could also suffer a loss in
resolution ability.
11.3 STRETCH PROCESSOR OPERATION
Given that r(t) and hs(t) satisfy the above requirements, we can write the output of the mixer as
or
The first exponential term of (11.19) is simply a phase term. However, the second exponential
term tells us the output of the mixer is a constant frequency signal with a frequency that
depends upon the difference between the target range delay, τR, and the range delay to which
the stretch processor is tuned, τM. Thus, if we can determine the frequency of the signal out of
the mixer, we can determine the target range. Specifically, if we define the frequency out of
the mixer as
we get

The spectrum analyzer of Figure 11.2 is used to measure fm. Ideally, the spectrum analyzer
computes the Fourier transform of vo(t). Thus, we can write
or
where
Figure 11.4 Plot of |Vo(f – fm)| for B = 500 MHz and τp = 100 µs.
The information of interest is contained in │Vo(f)│, a normalized plot of which is contained
in Figure 11.4. As we would expect, the sinc[x] function is centered at fm and has a nominal
width of 1/τp. Thus, we can measure fm, but not with perfect accuracy. This is consistent with
the result we would get with a matched filter. That is, the range measurement accuracy is
related to the width of the main lobe of the output of a matched filter. For an LFM signal with
a bandwidth of B, the nominal width of the main lobe is 1/B (see Chapter 7).
We now want to examine the range resolution of the stretch processor. Since the nominal
width of the sinc[x] function is 1/τp, we normally say that the frequency resolution at the
output of the spectrum analyzer is also 1/τp. Suppose we have a target at a range of τR1 and a
second target at a range of τR2 > τR1. The mixer output frequencies associated with the two
targets will be

and
Suppose further τR1 and τR2 are such that
That is, the frequencies are separated by a (frequency) resolution cell of the stretch processor.
With this we can write
or
or that the stretch processor has the same range resolution as a matched filter.
With LFM we can use an amplitude taper, implemented by a filter at the input or output of
the matched filter, to reduce the range sidelobes at the matched filter output [12–15]. We can
apply a similar taper to a stretch processor by applying an amplitude taper to vo(t) before
sending it to the spectrum analyzer.
11.4 STRETCH PROCESSOR SNR
At this point we want to compare the SNR at the output of a matched filter to the SNR at the
output of a stretch processor. Since neither processor includes nonlinearities, we can invoke
superposition and treat the signal and noise separately.
11.4.1 Matched Filter
For the matched filter case, we can write the signal voltage at the output of the matched filter
as (see Chapter 9)
where τmm and fmm are the range delay and Doppler frequency mismatch, respectively,
between the target return and matched filter. v(t) is given by (11.1). We are interested in the
power out of the matched filter at matched range and Doppler. That is, we want

Substituting (11.30) into (11.31) yields
The noise voltage at the output of the matched filter is given by
where n(t) is zero-mean, wide-sense stationary, white noise with
No = kTs is the noise power spectral density (see Chapter 2) and δ(x) is the Dirac delta
function1 [16], [p. 41]. Since n(t) is a random process, so is vnm(t). Thus, the average noise
power out of the matched filter is given by
where we have made use of (11.33) and (11.10) [The relation of (11.35) was derived in
Appendix 8A.] With this, we get the SNR at the matched filter output as
which we recognize from radar range equation theory (see Chapter 2).
11.4.2 Stretch Processor
For the stretch processor, we are interested in the signal power at the target range delay, τR.
Thus, we are interested in the output of the spectrum analyzer at f = fm. (This assumes that the
stretch processor is matched to τR, that is, τM = τR). With this we get, using (11.23),
If the noise into the mixer part of the stretch processor is n(t), the noise out of the mixer is

Recall that the signal power was computed at the spectrum analyzer output where f = fm. The
noise signal at the same frequency tap of the spectrum analyzer output is
and the average power at the output is
where we have made use of (11.34) and (11.12).
From (11.37) and (11.40), the SNR at the output of the stretch processor is
Combining (11.36) and (11.41), we get
Thus, the stretch processor encounters an SNR loss of τh/τp relative to the matched filter. This
means we should be careful about using stretch processing for range extents that are
significantly longer of the transmit pulsewidth.
At first inspection, it appears as if stretch processing could offer better SNR than a matched
filter, which would contradict the fact that the matched filter maximizes SNR (see Chapter 7).
This apparent contradiction is resolved by the stretch processor constraint imposed by
(11.15). Specifically, τh ≥ ΔτR + τp. Equation (11.42) also demonstrates another reason why
stretch processing should not be used in a search function: it would be too lossy, as τh would
need to be significantly larger than τp.
11.5 STRETCH PROCESSOR IMPLEMENTATION
Next, we turn our attention to practical implementation issues. The mixer, timing, and
heterodyne generation are reasonably straightforward. However, we want to address how to
implement the spectrum analyzer. The most obvious method of implementing the spectrum
analyzer is to use a fast Fourier transformer (FFT). To do so, we need to determine the
required ADC (analog-to-digital converter) sample rate and the number of points to use in the
FFT. To determine the ADC rate we need to know the expected frequency limits of the signal

out of the mixer.2
If τRMIN – τM and τRMAX – τM are the minimum and maximum range delays, relative to τM,
over which stretch processing is performed, the corresponding minimum and maximum
frequencies out of the mixer are
and
Thus, the expected range of frequencies out of the mixer is
Thus, the ADC sample rate should be at least ∆fm.
The FFT will need to operate on data samples taken between τRMIN – τp/2 and τRMAX + τp/2
or over a time window of at least
The total number of data samples processed by the FFT will be
This means the FFT length will need to be some power of 2 that is greater than Nsamp.
As an example of the above calculations we consider the following parameters.
• τp = 100 μs
• B = 500 MHz
• Stretch processing performed over 1,500 m
With this we get
and
To compute Δfm, we first need to compute α as

With this we get
Thus, the minimum required ADC sample rate is 50 MHz. The number of samples to be
processed by the FFT is
This means that we would want to use an 8,192-point FFT. One method of getting to 8,192
samples would be to increase the size of the range window. This would cause both Δfm and τh
to increase. An alternative would be to zero-pad the FFT by filling the last 8,192–5,500 taps
with zero.
If we continue the calculations, we find that the time extent of the heterodyne window is
The SNR loss associated with the use of stretch processing, relative to a matched filter, is τh/τp
= 110/100 or about 0.4 dB.
11.6 DOPPLER EFFECTS
We now want to examine the effects of Doppler frequency on the output of the stretch
processor. Since we have established the equivalency between the stretch processor output and
the output of a matched filter, we will approach the discussion from the perspective of
matched filter theory. We start by extending the definition of v(t) from (11.1) to include a
carrier term. We then specifically examine how range rate affects the returned signal, r(t).
After this we examine the matched filter response to r(t) from the specific perspectives of
range resolution degradation and range error due to Doppler frequency.
11.6.1 Expanded Transmit and Receive Signal Models
We extend the previous definition of the transmitted LFM pulse to include the carrier term.
Thus, we write
where the first exponential is the carrier term and fc is the carrier frequency.
The signal returned from the target is

It will be noted that the range delay, τR(t), is now shown as a function of time to account for
the fact that range changes with time because the range rate is not zero. We will assume the
target range rate is a constant. With this, we can write
where R0 is the range at t = 0 (the center of the transmit pulse in this case) and Ṙ is the range
rate.
Substituting (11.56) into (11.55) results in
where ϕ1(t) is a phase term we associate with Doppler effects due to the interaction of the
target range rate with the carrier and ϕ2(t) is a phase term we associate with the interaction of
range rate with the LFM modulation. Expanding ϕ1(t) gives
where the first term on the right is the carrier component, the second term is a phase shift
associated with the initial target position, and the third term is the Doppler frequency term.
This term [ϕ1(t)] is the same as we developed in Chapter 1 when we discussed Doppler
frequency.
The second phase term can be written as
In this case we want to examine the frequency or

Note that the LFM slope of the received signal, αr, is slightly different from the LFM slope, α,
of the transmit signal. As we will see, this slight difference can degrade the range resolution
of the LFM waveform in some cases.
The increase in slope of the received LFM signal is caused by a slight shortening of the
pulse as it is “reflected” by the (point) target (assuming Ṙ < 0, i.e., an approaching target). To
see this, we consider a specific example. Suppose we have a 1-ms pulse and a target moving at
7,500 m/s. Let t0 be the time the leading edge of the pulse reaches the target. During the time
the pulse is interacting with the target, the target moves about (7,500 m/s) × (0.001 s) or 7.5 m.
This translates to an effective round-trip time delay of 2 × 7.5/c or 50 ns. This means the
length of the pulse returned to the radar is shorter than the transmit pulse by 50 ns. Since the
frequency still varies the same amount over the duration of the pulse, the LFM slope must
increase.
11.6.2 Effect of Doppler Frequency on Range Resolution
To quantify the effect of the change in received waveform LFM slope on range resolution, we
consider specific examples. We assume the radar uses a matched filter to perform pulse
compression. We further assume the matched filter is matched to the transmit waveform plus
some frequency offset, fM, to account for the target Doppler frequency. As we did earlier, we
will shift our time reference so that the center of the received pulse is at t = 0. Thus, we can
write the normalized received signal as
The first exponential term is the Doppler term discussed above [see the discussion related to
(11.58)—we have omitted the carrier frequency term since we assume that it has been
removed by a heterodyning process in the receiver]. We have temporarily ignored the term f0
in (11.60). We will address this later. The term τPWr is the reduced pulsewidth discussed above.
We can write the matched filter impulse response as

The response of the matched filter to the received signal is the convolution of r(t) and h(t).
That is,
Substituting for r(t) and h(t) results in
After considerable manipulation (see Exercise 6), it can be shown that
In (11.65):
• F is the Fresnel integral
• Δα = αr – α
• Δf = fd – fM
• U = min (t + τp/2, τPWr/2)
• L = max (t – τp/2, –τPWr/2)
• K is a complex constant we normalize away.
Equation (11.65) applies only to the case where the received LFM slope and rect[x] function
widths are different (the mismatched case). If the LFM slope and rect[x] function widths are
the same, | vo(t) | can be derived from the ambiguity function of v(t) and is
In (11.66), K1 is a complex constant that we normalize away.
To see the effect of range rate on the matched filter response, we consider two examples. In
both cases, we consider the LFM waveform of previous examples. Specifically, we consider a
waveform with a bandwidth of 500 MHz and a pulsewidth of 100 µs. We assume further that
the matched filter is matched to the target Doppler. That is, fd = fM. For the first case, we
consider an aircraft with a range-rate of –150 m/s, and for the second case we consider a

ballistic missile with a (extreme) range rate of –7,500 m/s. Plots of the matched filter outputs
for the two cases are shown in Figure 11.5 and Figure 11.6. Each plot contains a curve where
we ignore the effects of range rate on the LFM slope (dashed curve) and another curve where
the range-rate effects are included (solid curve).
Figure 11.5 Matched filter response—target range rate = –150 m/s.
Figure 11.6 Matched filter response—target range rate = –7,500 m/s.
For the aircraft example, the difference in LFM slope caused by the range rate does not
have a significant effect on the output of the matched filter (by very careful examination of
Figure 11.5 you can see the dashed line in the first null of the solid curve). For the ballistic
missile example, the difference in LFM slopes causes a significant degradation of range

resolution. This, in turn, could cause problems in isolating closely spaced scatterers. To
eliminate this effect, the LFM slope of the matched filter should be changed to match the
expected LFM slope of the received waveform. An alternative would be to change the LFM
slope of the transmit signal so that the LFM slope of the received signal matches that of the
matched filter.
Since we expect the stretch processor behavior will be similar to the matched filter
behavior, LFM slope mismatch should have the same effect on the output of the stretch
processor. That is, for large range rates, and long, large bandwidth waveforms, LFM slope
mismatch will cause a degradation of range resolution if not corrected. We will investigate
this in one of the exercises. As a note, for stretch processing to be useful, the radar must be
tracking the target (recall that τM must be close to τR). Thus, the radar will have a good
estimate of range rate and can correct for it.
11.6.3 Effect of Doppler Frequency Mismatch on Range Error
We next want to examine the effect of Doppler mismatch on range error. We consider the
aircraft target example from above. Figure 11.7 contains a plot of the matched filter output for
the case where the matched filter is matched to the target Doppler (the dashed curve) and the
case where the matched filter is matched to zero Doppler (the solid curve). It will be noted that
the mismatched Doppler case has a range error of 0.3 m, which is the range resolution of the
waveform. To understand the cause of this range error, it will be helpful to look at the
matched filter response in terms of the square root of the ambiguity function of the transmit
signal. This relation is (see Chapter 9)
where Δf = fd – fM is the Doppler mismatch.
In (11.67) we note that if the Doppler mismatch is Δf = fd, (i.e., the matched filter is not
tuned to the target Doppler frequency) the peak of │ χLFM(t,Δf) │ occurs at
From (11.68), we note that a Doppler mismatch of Δf = fd = 1/τp will cause a range error equal
to the range resolution of the LFM waveform. Said another way, a Doppler mismatch equal to
the reciprocal of the uncompressed pulsewidth will cause a range error of one range
resolution cell. In the specific example above, if we assume the radar is operating at X-band
with λ = 0.03, a range rate of –150 m/s causes a Doppler frequency of 10 kHz. Coincidently,
the uncompressed pulsewidth of our example was 100 µs so that 1/τp = 10 kHz. Thus, by the
above, we expect the peak of the ambiguity function (matched filter response) will be at one
range-resolution cell instead of zero. Given that our waveform bandwidth was 500 MHz, the
range resolution of the waveform is 2 ns, or 0.3 m, which is where the peak in Figure 11.7 is

located.
Figure 11.7 Effects of Doppler mismatch on matched filter response.
As with the LFM slope, we expect that the response of the stretch processor to Doppler
mismatch will be the same as the matched filter. Suppose the return signal, r(t), into the stretch
processor has a Doppler frequency of fd. This will mean that the return signal into the mixer
(see Figure 11.1) will be
If we repeat the math of Section 11.3 using rd(t) in place of r(t), the signal out of the mixer
will be
In other words, the frequency out of the mixer will be shifted by the Doppler frequency
(albeit with a negative sign). This, in turn, will cause the peak of the spectrum analyzer output
shift by –fd. That is, the output of the spectrum analyzer will be Vod(f) = Vo(f + fd). If fd = 1/τp
then the peak will shift by –1/τp. From the discussions of Section 11.3, a frequency deviation
in the spectrum analyzer output of 1/τp corresponds to a range shift of 1/B, or one range
resolution cell. From this we observe that the response of the stretch processor to Doppler is
the same as for a matched filter. In other words, a Doppler shift of 1/τp causes the range to be
in error by one range resolution cell.
11.7 EXERCISES
1.
Derive (11.6) and generate a plot like Figure 11.1.

2.
Implement a stretch processor as discussed in Section 11.5. In your implementation, use
an (unrealistic) 216 = 65,536-point FFT to provide a smooth output plot for visualization
purposes. Zero pad the input to the FFT by loading the last 65,536–5,500 input taps with
zero. Generate a plot like Figure 11.4 by plotting the magnitude of the FFT output. You
will need to appropriately assign ranges to the FFT output taps. In this exercise, you will
need to actually generate the received LFM pulse using (11.1).
3.
Apply a window to your stretch processor to reduce the range sidelobes. Use a Hamming
window function. Apply the Hamming window across the 5,500 samples out of your
simulated ADC, not across the 65,536 FFT input taps.
4.
Use Ṙ = –7,500 m/s and use your stretch processor from Exercise 2 to produce plots like
Figure 11.6. For this exercise, you will need to recreate the input LFM pulse with α = αr
[see (11.60)] and a slightly smaller τp as discussed in the paragraphs below (11.60).
5.
Repeat Exercise 4 for Ṙ = –150 m/s.
6.
Derive (11.65).
7.
Derive (11.32), (11.35), (11.36), (11.37), (11.40), (11.41), and (11.42).
8.
Derive (11.70).
References
[1]
Cook, C. E., and M. Bernfeld, Radar Signals: An Introduction to Theory and Application, New York: Academic Press,
1967. Reprinted: Norwood, MA: Artech House, 1993.
[2]
Caputi, W. J., “Stretch: A Time-Transformation Technique,” IEEE Trans. Aerosp. Electron. Syst., vol. 7, no. 2, Mar. 1971,
pp. 269–278. Reprinted: Barton, D. K., ed., Radars, Vol. 3: Pulse Compression (Artech Radar Library), Dedham, MA:
Artech House, 1975.
[3]
IEEE 
document, 
“IEEE 
Dennis 
J. 
Picard 
Medal 
for 
Radar 
Technologies 
and 
Applications,”
www.ieee.org/documents/picard_rl.pdf.
[4]
Abramowitz, M., and I. A. Stegun, eds., Handbook of Mathematical Functions with Formulas, Graphs, and
Mathematical Tables, National Bureau of Standards Applied Mathematics Series 55, Washington, DC: U.S. Government
Printing Office, 1964; New York: Dover, 1965.
[5]
Analog Devices, “AD9680 Data Sheet: 14-Bit, 1 GSPS JESD204B, Dual Analog-to-Digital Converter,” 2014.
www.analog.com.
[6]
Dufilie, P., C. Valerio, and T. Martin, “Improved SAW Slanted Array Compressor Structure for Achieving >20,000 Time-
Bandwidth Product,” 2014 IEEE Int. Ultrasonics Symp. (IUS), Chicago, IL, Sept. 3–6, 2014, pp. 2019–2022.
[7]
Skolnik, M. I., ed., Radar Handbook, 3rd ed., New York: McGraw-Hill, 2008.
[8]
Yu, J., et al., “An X-band Radar Transceiver MMIC with Bandwidth Reduction in 0.13 µm SiGe Technology,” IEEE J.
Solid-State Circuits, vol. 49, no. 9, Sept. 2014, pp. 1905–1915.
[9]
Baturov, B. B., et al., “An S-band High-Power Broadband Transmitter,” 2000 IEEE MTT-S Int. Microwave Symp. Dig.,
vol. 1, Boston, MA, Jun. 11–16, 2000, pp. 557–559.
[10] Abe, D. K., et al., “Multiple-Beam Klystron Development at the Naval Research Laboratory,” 2009 IEEE Radar Conf.,
Pasadena, CA, May 4–8, 2009, pp. 1–5.
[11] Ender, J. H. G., and A. R. Brenner, “PAMIR—A Wideband-Phased Array SAR/MTI System,” IEEE Proc. Radar, Sonar
and Navigation, vol. 150, no. 3, Jun. 2003, pp. 165–172.
[12] Klauder, J. R. et al., “The Theory and Design of Chirp Radars,” Bell Syst. Tech. J., vol. 39, no. 4, Jul. 1960, pp. 745–

808.
[13] Powell, T. H. J., and A. Sinsky, “A Time Sidelobe Reduction Technique for Small Time-Bandwidth Chirp,” IEEE Trans.
Aerosp. Electron. Syst., vol. 10, no. 3, May 1974, pp. 390–392.
[14] Barton, D. K., ed., Radars, Vol. 3: Pulse Compression (Artech Radar Library), Dedham, MA: Artech House, 1975.
[15] Wehner, D. R., High Resolution Radar, Norwood, MA: Artech House, 1987.
[16] Picinbono, B., Principles of Signals and Systems: Deterministic Signals, Norwood, MA: Artech House, 1988.
1 Introduced in quantum mechanics by Paul Dirac.
2 We will assume baseband processing in these discussions. In practice, the mixer output will be at some intermediate frequency
(IF). The signal could be brought to baseband using a synchronous detector or, as in some modern radars, by using IF
sampling (i.e., a digital receiver). In either case, the effective ADC rate (the sample rate of the complex, digital, baseband
signal) will be as derived here.

Chapter 12
Phased Array Antenna Basics
12.1 INTRODUCTION
In this chapter, we discuss the basics of phased array antennas. We specifically develop
equations and techniques to find antenna radiation and directive gain patterns. That is, we
develop equations for G(α, ε), where α and ε are orthogonal angles such as azimuth and
elevation or angles relative to a normal to the antenna face. We develop equations and
algorithms to produce plots similar to the plot shown in Figure 12.1. We also discuss
beamwidth, directive gain, sidelobes, and grating lobes and how these relate to antenna
dimensions and other factors.
Figure 12.1 Sample antenna pattern.
We begin with a simple two-element array antenna to illustrate some of the basic aspects of
computing antenna radiation patterns and some of the properties of antennas. We then
progress to linear arrays and planar phased arrays. After that, we discuss polarization and
how phased array analysis methods can be used to generate antenna patterns for simple
reflector antennas
It appears that the first use of array antennas was by Guglielmo Marconi in a

communication experiment in 1901 [1, 2]. According to Mailloux [3], Friis and Feldman
reported the use of a “fully electromechanically scanned array” in a 1937 paper [4]. In a 1947
paper [5], Friis and Lewis described a phased array antenna for the Navy Mark 8 shipboard
radar. Since the 1950s, the development of phased array antennas has progressed steadily,
both from a theory and hardware perspective, to the point where such antennas are becoming
the norm rather than the exception [6].
12.2 TWO-ELEMENT ARRAY ANTENNA
Assume we have two isotropic radiators, or isotropes, [7] separated by a distance, d, as shown
in Figure 12.2. In Figure 12.2, the arc represents part of a sphere located at a distance of r
relative to the center of the radiators. For these studies, we assume that r ≫ d, the far-field
condition. The sinusoids represent the electric fields (E-fields) generated by each radiator.
Since the radiators are isotropic, the power, P each radiates is uniformly distributed over a
sphere at some radius r. Thus, the power over some small area, ΔA, due to either radiator is
given by
where Prad is the is the power delivered to the radiator. Since the E-field intensity, |E|, at r is
proportional to the square root of P, we can write
where KR is a resistance parameter that gives Vs in volts. KR is termed radiation resistance,
which is 377 Ω for free space [7, p. 12]. Since the signal is a sinusoid at a carrier frequency of
ωo, we can write the E-field at ΔA as
where τr is the time required for the E-field to propagate from the source to the area ΔA.

Figure 12.2 Two-element array antenna.
For the next step, we invoke the relations τr = r/c, ωo = 2πƒo, and ƒo = c/λ, where c is the
speed of light and λ denotes wavelength. With this, we can write the E-field at ΔA as
We now derive an equation for the E-field at ΔA when we have the two radiators of Figure
12.2. We use the geometry of Figure 12.3 to aid the derivation. We denote the upper radiator
(point source) of Figure 12.3 as radiator 1 and the lower radiator as radiator 2. The distances
from the individual radiators to ΔA are r1 and r2, and the E-field intensity of each radiator is
Es = (Prad/2)1/2. Prad is the total power delivered to the radiators. The factor of 2 is included to
denote the fact that the power is split evenly between the radiators (uniform weighting). We
further assume the radiation resistance is 1 Ω. Figure 12.3 shows the other needed terms.
The E-fields of the two radiators at the far-field point are
and
Figure 12.3 Geometry for two-element radiator problem.
From Figure 12.3,
and

As indicated earlier, we assume r ≫ d. With this, we get
and
where we have used the relation
Since r1 and r2 are functions of ε, the E-fields are also functions of ε. With this, we get
and
In (12.12) and (12.13), we can set the denominator terms to r since d/2r ≪ 1. We cannot do this
in the exponential terms because phase is measured modulo 2π.
The total E-field at ΔA is
or

We define an antenna radiation pattern as
The radiation pattern for the dual, isotropic radiator antenna is thus
We are interested in R(ε) for | ε | < π/2. We call the region | ε | < π/2 visible space.
Figure 12.4 contains plots of R(ε) for d = λ, λ/2, and λ/4. For d =λ, the radiation pattern has
peaks at 0, π/2, and –π/2. The peaks at ±π/2 are termed grating lobes and are usually
undesirable. For d = λ/4, the radiation pattern does not return to zero, and the width of the
central region is broad. This is also a generally undesirable characteristic. The case of d = λ/2
is a good compromise that leads to a reasonably narrow center peak and levels that go to zero
at ±π/2. In the design of phased array antennas, we find that d ≈ λ/2 is usually a desirable
design criterion.
The central region of the plots in Figure 12.4 is termed the main beam, and the angle
spacing between the 3-dB points (the points where the radiation pattern is down 3 dB from its
peak value) is termed the beamwidth. From Figure 12.4, we conclude that, for our two
radiator example, the beamwidth is inversely proportional to the spacing between the
radiators. A more accurate statement is that the beamwidth is inversely proportional to the
length of the array, or the dimensions of the array for a planar array. We will investigate this
relation in Section 12.13.

Figure 12.4 Radiation pattern for a two-element array with various element spacings.
We just solved the transmit problem. That is, we supplied power to the radiators and
determined how it was distributed on a sphere. We now want to consider the reverse problem
and examine the receive antenna. The results of that analysis will illustrate an important
property known as reciprocity. Reciprocity says we can analyze an antenna from a transmit or
receive perspective and obtain the same radiation pattern.
For this case, we consider the two “radiators” of Figure 12.2 as receive antennas that are
isotropic. Here, we call them receive elements. We assume an E-field radiates from a point
located at a range r from the center of the two receive elements. The receive elements are
separated by a distance of d. Figure 12.5 shows the required geometry.
Outputs of the receive elements are multiplied by 1 over the square root of 2 and summed.
The voltage out of each element is proportional to the E-field at each element and is
represented as a complex number to account for the fact that the actual signal, which is a
sinusoid, is characterized by amplitude and phase.
Figure 12.5 Two-element array, receive geometry.

The E-field at all points on a circle (or a sphere in three dimensions) has the same
amplitude and phase. Also, since d ≪ r, the circle becomes a line at the location of the receive
elements. The line is oriented at an angle of ε relative to the vertical, and is termed the
constant E-field line. ε is also the angle between the horizontal line and the point from which
the E-field radiates. We term the horizontal line the antenna broadside. In more general terms,
the antenna broadside is normal to the plane containing the elements.
The distance from the constant E-field line to the elements is (d/2)sin ε. If we define the E-
field at the center point between the elements as
then the E-field at the elements is
and
where we made use of the approximation in (12.11)
Since the voltage out of each element is proportional to the E-field at each element, the
voltages out of the elements are
and
With this, the voltage at the summer output is
We define the radiation pattern as

which yields
This is the same result we obtained for the transmit case described by (12.17) and
demonstrates that reciprocity applies to this antenna. This allows us to use either the receive
or transmit approach when analyzing more complex antennas.
12.3 N-ELEMENT LINEAR ARRAY
We now extend the results of the previous section to a linear array of elements shown in
Figure 12.6. As Figure 12.6 implies, we use the receive approach to derive the radiation
pattern. The array consists of N elements (the sideways “v” symbol on the right of each block)
with a spacing of d between the elements. The output of each element is weighted by a factor
of an, and the results summed to form the signal out of the antenna. In general, the weights, an,
are complex. In fact, we find that we move, or steer, the antenna beam by assigning
appropriate phases to an. We assume each element is an isotropic radiator.
Figure 12.6 Geometry for N-element linear array.
We placed the origin of a coordinate system at the lower element. The axes labels, z and y,

were chosen to be consistent with the planar array geometry discussed in Section 12.10.
The distance from the nth element to the field point is
Since r is much greater than the array length, we can drop the last term of the radical and
factor r2 from the square root to get
Since the magnitude of the second term of the radical is much less than 1, we can invoke
(12.11) and write
This means the E-field at the nth element is
and the voltage out of the nth element is
where Vr is the magnitude of the voltage out of each element.
The voltage out of the summer is1
We let x = 2πd sin ε/λ, and write

As before, we define the radiation pattern as
which yields
We now consider the special case of a linear array with constant, or uniform, weighting of
an = (N)–1/2. For the sum term, we write
We invoke the relation [8]
to write
Finally, we get
Figure 12.7 contains plots of R(ε) versus ε for N = 20 and d = λ, λ/2, and λ/4. As with the
two-element example, grating lobes appear for the case of d = λ. Also, the width of the main
lobe varies inversely with element spacing. Since N is fixed, the larger element spacing
implies a larger antenna, which leads to the observation that, as with the two element array, the

beamwidth varies inversely with array length. The peak value of R(ε) is 20, or N, and occurs
at ε = 0. This value can also be derived by taking the limit of R(ε) as ε→0, or by evaluating
A(ε) at x = ε = 0 and squaring it.
For the general case where an is not constant, we directly compute R(ε) using
Computation of R(ε) is addressed in Section 12.9.
Figure 12.7 Radiation pattern for an N-element linear array with different element spacings.
12.4 DIRECTIVE GAIN PATTERN (ANTENNA PATTERN)
The radiation pattern is useful when determining antenna properties such as beamwidth,
grating lobes, and sidelobe levels. However, it does not provide an indication of antenna
directivity. To obtain this, we define a directive gain pattern. The directive gain pattern
indicates antenna directivity, or directive gain, as a function of angle.2 This is the gain we use
in the radar range equation.
The directive gain pattern is defined as [7, p. 125; 9]
or

where dΩ is a differential area on the sphere.
To compute the denominator integral, we consider the geometry of Figure 12.8, where the
vertical row of dots represents the linear array. The differential area can be written as
and the integral becomes
For the linear array, we have R(α,ε) = R(ε) and
Figure 12.8 Geometry used to compute 
.

For the special case of a linear array with uniform weighting, we get
where the last equality is a result of the fact that the integrand is an even function.
After considerable computation, it can be shown that (see Exercise 4)
As a “sanity check,” we consider a point-source (isotropic) radiator. This can be considered
a special case of an N-element linear array with uniform illumination, and an element spacing
of d = 0. For this case, we get sinc(2kd/λ) = 1
and
It can also be shown that (see Exercise 6), for a general N-element, uniformly illuminated
linear array with an element spacing of d = λ/2, and weights of an = (N)–1/2, that 
 = 1 and
For the case of a general nonuniformly illuminated linear array, R must be computed
numerically from (12.44).
Directive gain, G, is defined as the maximum value of G(ε) [9, 10]. For the example of
(12.49), G = G(0). Figure 12.9 contains a plot of G, normalized by N, (i.e., G/N) versus d/λ for
several values of N.
The shapes of the curves in Figure 12.9 are interesting, especially around integer multiples
of d/λ. For example, for d/λ is slightly less than 1, G/N is between about 1.7 and 1.9, whereas
when d/λ is slightly greater than 1, G/N is about 0.7. In other words, a small change in element

spacing, relative to wavelength, causes the directive gain to vary by a factor of about 1.8/0.7
or 4 dB. The reason for this is illustrated in Figure 12.10, which contains a plot of R(ε) for d/λ
values of 0.9, 1.0, and 1.1. In this case, R(ε) is plotted versus sine to better illustrate the widths
of the grating lobes (the lobes not at zero).
Figure 12.9 Normalized directive gain vs. element spacing.
For the case where d/λ is 0.9 (top plot of Figure 12.10), the radiation pattern does not
contain grating lobes. This means most of the transmitted power is focused in the main beam.
For the cases where d/λ is either 1.0 or 1.1, the radiation pattern contains grating lobes, and
some of the transmitted power is transferred from the main lobe to the grating lobes. This
reduces the directive gain of the antenna relative to the case where d/λ is 0.9. Furthermore,
since there are two grating lobes for d/λ = 1.1 and only one grating lobe for d/λ = 1.0 (½ lobe
at sin ε = 1 and ½ lobe at sin ε = –1), the directive gain is less when d/λ is 1.1 than when it is
1.0.
Based on Figure 12.9, we expect similar behavior of the directive gain for values of d/λ
near other integer values, which is what happened. However, the variation in directive gain, as
d/λ transitions from below to above integer values, decreases as the integer value of d/λ
increases. This is due to the number of grating lobes. As d/λ becomes larger, the number of
grating lobes increases. Therefore, the addition of one grating lobe for integer d/λ, and the
two grating lobes for d/λ slightly larger than an integer, has an increasingly smaller impact
on the overall directive gain variation.

Figure 12.10 Radiation patterns for d/λ close to 1.0.
As a note, since the number of elements is fixed at 20, the length of the array increases as d/
λ increases. Because of this, the beamwidth decreases, and the directive gain increases. This
increase is offset by the increase in the number of grating lobes. This is what causes the curve
of Figure 12.9 to vary about the nominal value of 1.
12.5 BEAMWIDTH, SIDELOBES, AND AMPLITUDE WEIGHTING
Figure 12.11 contains a plot of G(ε) for a 20-element array with an element spacing of d/λ =
0.5 and uniform weighting. In this case, the units on the vertical scale are in dBi. The unit
notation, dBi, stands for decibel relative to an isotropic radiator and indicates the directive
gain is referenced to the directive gain of an isotropic radiator, which is unity.
As discussed earlier, the lobe near ε = 0 is termed the main beam. The lobes surrounding
the main beam are the sidelobes. The first couple of sidelobes on either side of the main beam
are termed the near-in sidelobes, and the remaining sidelobes are termed the far-out
sidelobes. For this antenna, the directive gain is 10log(20) = 13 dB, and the near-in sidelobes
are about 13 dB below the peak of the main beam (13 dB below the main beam). The far-out
sidelobes are greater than 20 dB below the main beam.

The beamwidth is defined at the width of the main beam measured at the 3-dB points on the
main beam. For the pattern of Figure 12.11, the beamwidth is 5°.
The near-in sidelobe level of 13 dB is often considered undesirably high. To reduce this
level, antenna designers usually apply an amplitude taper to the array by setting an to different
values. Generally, the values of an are varied symmetrically across the elements so that the
elements on opposite sides of the center of the array have the same value of an. Designers
usually try to choose the an so that they achieve a desired sidelobe level while minimizing the
beamwidth increase and directive gain decrease usually engendered by weighting.
Figure 12.11 Directive gain for a 20-element linear array with a uniform taper.
The optimum weighting in this regard is Chebyshev weighting [11, 12]. Up until recently,
Chebyshev weights were difficult to generate. However, during the past 10 or so years,
standard algorithms have become available. Chebyshev weights can be chosen to provide a
specified sidelobe level.
A popular antenna weighting is Taylor [13–16]. Like Chebyshev, it allows specification of
sidelobe levels. An algorithm for computing Taylor weights is given in Appendix 12A.
Another popular weighting is the cosn weighting discussed in Chapter 5 [14, 15].
In space-fed phased arrays and reflector antennas, the amplitude taper is created by the feed.
As a result, the type of taper is limited by the design of the feed. In constrained-feed phased
arrays, the taper is controlled by the way power is delivered to, or combined from, the
various elements. Again, this limits the type of amplitude taper that can be obtained. In solid-
state phased arrays, considerable flexibility exists in controlling the amplitude taper on
receive. However, it is currently difficult to obtain an amplitude taper on transmit because all
of the transmit/receive (T/R) modules are typically operated at full power to maximize
efficiency [17].
Figure 12.12 contains a plot of G(ε) for a 20-element linear array with d/λ = 0.5 and
Chebyshev weighting. The Chebyshev weighting was chosen to provide a sidelobe level of –
30 dB, relative to the main beam. The directive gain is about 12.4 dB rather than the 13-dB

gain associated with a 20-element linear array with uniform weighting. Thus, the amplitude
taper has reduced the antenna gain by about 0.6 dB. Also, the beamwidth of the antenna has
increased to 6.32° (a broadening factor of 1.26).
Figure 12.12 Directive gain for a 20-element linear array with Chebyshev weighting.
12.6 STEERING
Thus far, the antenna patterns we have generated have their main beams located at 0°. We now
want to address the problem of placing the main beam at some desired angle. This is termed
beam steering. First, we address the general problem of time-delay steering, and then we
develop the degenerate case of phase steering.
To address this problem, we refer to the N-element linear array geometry of Figure 12.6.
Let the idealized, normalized E-field from the point source be
where τp is the pulsewidth, ƒo is the carrier frequency, and rect[x] is the rectangle function. We
assume the point-source radiator is stationary and located at some range, R.
The idealized, normalized voltage out of the nth antenna element (before the weighing, an)
is
where τn is the time delay from the point-source radiator to the nth element and is given by

Instead of treating the weights, an, as multiplication factors, we treat them as operators on
the voltages at the output of the antenna elements. With this, we write the voltage out of the
summer as
We want to determine how the weighting functions, a(vn(t),n), must be chosen to focus the
beam at some angle εo.
Figure 12.13 Sketch of |vn(t) |.
Figure 12.13 contains a sketch of the envelopes of the various vn(t). The main point
illustrated by Figure 12.13 is that the pulses out of the various antenna elements are not
aligned. This means the weighting functions, a(vn(t),n), must effect some desired alignment of
the signals. More specifically, the a(vn(t),n) must be chosen so that the signals out of the
weighting functions are aligned (and in-phase) at some desired εo. To accomplish this, the
a(vn(t),n) must introduce appropriate time delays (and possibly phase shifts) to the various
vn(t). The a(vn(t),n) must also appropriately scale the amplitudes of the various vn(t). This
introduction of time delays to focus the beam at some angle εo is termed time-delay steering.
Substituting for τn into the general vn(t), we get
where τdε = d sin ε/c.
To time align all of the pulses out of the weighting functions, the weighting function must

introduce a time delay that cancels the nτdε term in vn(t). Specifically, a(vn(t),n) must be
chosen such that
where τdo = d sin εo/c. Using this with the Vn(t) are
Note that at ε = εo, τdε = τdo, and
In other words, the pulses out of the weighting functions are time aligned and properly
amplitude weighted.
Time delay steering is expensive and not easy to implement. It is needed in radars that use
compressed pulsewidths that are small relative to antenna dimensions. This can be seen from
examining Figure 12.13. If τp is small relative to (N – 1)τdε, then, for some ε, not all of the
pulses align. Stated another way, the pulse out of the first element is not aligned with the pulse
out of the Nth element. However, this implies either a very small τp or a very large antenna
(large (N – 1)τdε). For example, if τp was 1 ns and the antenna was 2 m wide, we would have
(N – 1)τdε = 6.7 ns > τp, and time-delay steering would be needed. If τp was 1 µs, the pulses
would not be aligned, but the misalignment would be much less than the pulsewidth. This
means that the pulses can be summed, and time-delay steering is not necessary.
Figure 12.14 contains a plot of the boundary where time-delay steering would and would
not be necessary. The line in Figure 12.14 corresponds to the case where the antenna diameter,
D, is 25% of the compressed pulsewidth. The choice of 25% is somewhat arbitrary but is
probably representative of practical situations where the beam is steered to a maximum angle
of 60°. For combinations of antenna diameter and compressed pulsewidth to the right of the
line, phase steering would be adequate. For regions to the left of the line, time-delay steering
may be needed.

Figure 12.14 Antenna diameter vs. compressed pulsewidth trade.
The two regions of Figure 12.14 indicate that the alternative to time-delay steering is phase
steering. Indeed, if we assume the pulses are aligned, we can write
or an = |an|exp(j2πnfoτdo). That is, the weights, an, modify the amplitudes and phases of the
various vn(t). Therefore, this technique is called phase steering.
Substituting for τdo in the phase term results in
12.7 ELEMENT PATTERN
In the equations above, it was assumed that all of the elements of the antenna were isotropic
radiators. In practice, antenna elements are not isotropic but have their own radiation pattern.
This means the voltage (amplitude and phase) out of each element depends upon ε,
independent of the phase shift caused by the element spacing. If all of the elements are the
same, and oriented the same relative to broadside, the dependence voltage upon ε is the same

for each element (again, ignoring the phase shift caused by the element spacing). In equation
form, the voltage out of each element is
and the voltage out of the summer (assuming phase steering) is
The resulting radiation pattern is
In other words, to get the radiation pattern of an antenna with nonisotropic elements, we
multiply the array radiation pattern (found by the aforementioned techniques) by the radiation
pattern of the element.
As a closing note, in general, the element pattern is not steered.
12.8 PHASE SHIFTERS
In the above discussions, a tacit assumption is that the phase of each weight, an, can take on a
continuum of values. In practice, the phase can only be adjusted in discrete steps because the
devices that implement the phase shift, the phase shifters, are digital. Typical phase shifters
use 3 to 6 bits to set the phase shift. If Βϕ is the number of bits used in the phase shifter, then
the number of phases is Νϕ = 2Bϕ. As an example, a 3-bit phase shifter has 8 phases that range
from 0 to 2π – 2π/8 in steps of 2π/8. As shown in Exercise 11, the phase quantization caused
by the phase shifters can have a deleterious effect on the sidelobes when the beam is steered to
other than broadside. Discussions of various types of phase shifters can be found in several
texts [14, 15, 18–20].
12.9 COMPUTATION OF ANTENNA PATTERNS
In Section 12.3, we determined that we could compute the radiation pattern, R(ε), from
where

and
The “brute force” way to compute A(ε) would be to implement (12.64) in a loop (e.g., FOR
loop, DO loop) and repeat this for the ε values of interest. While this is sufficient for small
values of N and few values of ε, it can be time consuming when either or both of these are
large. By recasting (12.64) in a vector form, the computation of A(ε) can be sped up when
using software with efficient matrix and vector routines.
Let Wa be a row vector (a weight vector) of the an, and KN be a column vector of integers
that range from 0 to N – 1. That is,
and
where the superscript T denotes the transpose operation. Define X as
where ε1, ε2, …, εNε are the angles at which we want to compute A(ε)
With the above definitions, A(ε) can be written as
Equation (12.69) circumvents the need for loops in higher-level languages (such as MATLAB,
Mathcad, and Python®) and executes very quickly. It also results in computer code that is very
concise.
12.10 PLANAR ARRAYS
We now want to extend the linear array development to planar arrays. In a planar array, the
antenna elements are located on some type of grid in a plane. Generally, the grid pattern is
rectangular or triangular (this is discussed further in Section 12.10.3). Figure 12.15 shows an
example that would apply to a rectangular grid.

The array lies in the x-y plane, and the array broadside is the z-axis. The dots with the
numbers by them are the elements. The line located at the angles α and ε point to the field
point (the target on transmit or the source, which could also be the target, on receive). The
field point is located at a range of r that is large relative to the dimensions of the array (far-
field assumption).
The array shown in Figure 12.15 is oriented vertically. With this orientation, ε is elevation
angle to the field point and is measured from the x-z plane, which would be the local ground
plane for a ground-based radar. The angle, α, is azimuth and is measured in the x-z plane,
relative to the z axis. If the array is tilted back from vertical, as is typical, ε and α are still
thought of as elevation and azimuth angles; although strictly speaking, they are not.
Figure 12.15 Example geometry for planar arrays.
The angles we are using are not the traditional angles used to develop the radiation pattern
for planar arrays [21–24]. The traditional angles are those associated with a standard
spherical coordinate system [25]. These angles are θ and ϕ, where θ is measured from the z
axis, and ϕ is measured from the x axis in the x-y plane. We are using α and ε because they
generally correspond to azimuth and elevation, and they simplify the derivation of 
 for
planar arrays (see Section 12.10.7).
In the coordinate system of Figure 12.15, the field point is located at

The 00 element is located at the origin, and the mnth element is located at (mdx,ndy), where
dx is the spacing between elements in the x direction, and dy is the spacing between elements in
the y direction. With this and (12.70), the range from the mnth element to the field point is
Since r is much larger than the array width and height, we can drop the last two terms of the
radical, and factor r2 from the square root to give
Since the second term of the radical of (12.72) is small relative to 1, we can use (12.11) to
write
We invoke reciprocity and consider the receive case to write the voltage out of the mnth
element as
where amn is the weight applied to the mnth element. Summing the outputs of all elements
gives
where M is the number of elements in the x direction, and N is the number of elements in the y
direction. Dividing by Vr and ignoring the first exponential term (which disappears when we
form the radiation pattern), we get

At this point, we adopt a notation that is common in phased array antennas: sine space. We
define
and
and write
Consistent with our work on linear arrays, we write the radiation pattern as
and the directive gain as
We will consider 
 shortly.
When we plot R(u,v) or G(u,v), we are plotting the radiation or directive gain pattern in sine
space. When we plot R(α,ε) or G(α,ε), we are plotting the radiation or directive gain pattern in
angle space.
From (12.77) and (12.78), it can be shown (see Exercise 18) that u and v satisfy the
constraint, u2 + v2 ≤ 1. These u and v constitute visible space. We will use this when we discuss
how to generate and plot radiation and directive gain patterns.
12.10.1 Weights for Beam Steering
In the equation for A(u,v), amn are the weights used to provide a proper taper and steer the
beam; these are of the general form
where u0,v0 are the desired steering angles in sine space. Equation (12.82) assumes phase
steering.

12.10.2 Array Shapes and Element Locations (Element Packing)
The development of Section 12.10 is applicable to rectangular arrays with the elements placed
on a rectangular grid. Many antennas are nonrectangular (e.g., circular or elliptical), and their
elements are not placed on a rectangular grid (i.e., rectangular packing). In both cases, the
deviations from rectangular shape and/or rectangular packing are usually made to conserve
array elements and increase the efficiency of the antenna (the elements at the corners of
rectangular arrays do not contribute much to the directive gain and can cause the ridges in the
radiation pattern).
The most common element packing scheme, besides rectangular packing, is triangular
packing. Figure 12.16 shows sections of a planar array with rectangular and triangular
packing. With triangular packing, the elements are arranged in a triangular pattern.
12.10.3 Feeds
An antenna feed is the mechanism by which the energy from the transmitter is conveyed to the
array so that it can be radiated into space. On receive, it is used to collect the energy from the
array elements. Two broad classes of feed types are used in phased arrays: space feed and
constrained feed. These two types of feed mechanisms are illustrated notionally in Figures
12.17 and 12.18.
In a space-fed array, the feed is some type of small antenna that radiates the energy to the
array, through space. The feed could be a horn antenna or even another, smaller, phased array.
The feed generates an antenna pattern, on transmit, which is captured by small antennas on the
feed side of the array. These are represented by the sideways v-shaped symbols on the left side
of the array of Figure 12.17. The outputs of the small antennas undergo a phase shift
(represented by the circles with ϕ in them) and are radiated into space by the small antennas
represented by the sideways v-shaped symbol on the right of the array. On receive, the reverse
of the above occurs:
• Antennas on the right of the array capture energy from the source.
• Phase shifters apply appropriate phase shifts.
• Antennas on the left of the array radiate the energy to the feed.
• Feed sends the energy to the receiver.
The phase shifters provide the beam steering and perform what is called a spherical
correction. The E-field radiated from the feed nominally has constant phase on a sphere,
which is represented by the arcs in Figure 12.17. This means the phase will not vary linearly
across the array, which is necessary to form a beam at a desired angle. This must be
accounted for in the setting of the phase shifters. The process of adjusting the phase to account
for the spherical wave front is spherical correction.

Figure 12.16 Illustration of rectangular and triangular element packing.
The feed produces its own directive gain pattern. This means the signals entering each of
the phase shifters are at different amplitudes. Thus, the feed is applying the amplitude
weighting, |amn|, to the array. The feed is usually designed so that its directive gain pattern
provides a desired sidelobe level for the overall antenna. Feed patterns typically approximate
a cosn function. To obtain a good trade-off between directive gain and sidelobe levels for the
overall antenna, the feed pattern is such that the level at the edge of the array is between 10 and
20 dB below the peak value, termed an edge taper. A feed that provides a 20-dB edge taper
results in lower array sidelobes than a feed that provides a 10-dB edge taper (see Exercise 17).
However, a space-fed phased array with a 10-dB edge taper feed has higher directive gain
than a space-fed phased array with a 20-dB edge taper feed.

Figure 12.17 Space-fed phased array.

Figure 12.18 Constrained-feed phased array.
In a constrained-feed phased array, the energy is routed from the transmitter, and to the
receiver, by a waveguide network. This is represented by the network of connections to the
left of the arrays in Figure 12.18. The drawing on the left depicts a parallel feed network, and
the drawing on the right depicts a series feed network. Some antennas use both feed types. For
example, the rows of an antenna might be fed by a series network, and the individual elements
in each row might be fed by a parallel network. In some applications, the waveguide network
can be structured to provide an amplitude taper.
The phase shifters in a constrained-feed array must include phase shifts to account for the
different path lengths of the various legs of the waveguide network.
Space-fed phased arrays are less expensive to build than constrained-feed arrays because
they do not require the waveguide network of the constrained-feed phased array. However, the
constrained-feed phased array is smaller than the space-fed phased array. The space-fed
phased array is generally as deep as it is tall or wide to allow proper positioning of the feed.
The depth of a constrained-feed phased array is only about twice the depth of the array
portion of a space-fed phased array. The extra depth is needed to accommodate the waveguide
network. Finally, the constrained-feed phased array is more rugged than the space-fed array

since almost all hardware is on the array structure.
A “limiting” case of the constrained-feed phased array is the solid-state phased array. For
this array, the phase shifters of Figure 12.18 are replaced by solid-state T/R modules. The
waveguide network can be replaced by cables since they carry only low-power signals. The
transmitters in each of the T/R modules are low power (typically 10 to 1,000 W). However, a
solid-state phased array can contain thousands of T/R modules so that the total transmit power
is comparable to that of a space-fed or constrained-feed phased array.
12.10.4 Amplitude Weighting
As with linear arrays, planar phased arrays use amplitude weighting to reduce sidelobes. The
type of weighting (Taylor, Chebyshev, or cosn, for example) is the same as in linear arrays.
The difference is that the weights are applied in two dimensions. Weights can be applied in
two basic ways:
• Multiplicative weighting3 and
• Elliptically symmetric weighting.
For multiplicative weighting, we would write the magnitudes of the weights as
This type of weighting is sometimes used in constrained-feed phased arrays because of the
way the feed structures are designed.
Approximations to elliptically symmetric weighting occur in space-fed phased arrays
because the weights are created by the feed pattern. This type of weighting usually provides
sidelobe levels that are symmetric over the u-v plane.
The following procedure can be used to generate elliptically symmetric weights for
antenna modeling purposes:
1.
Generate a set of appropriate weights that has a number of terms equal to Nwt ≥
2Dmax/dmin, where 2Dmax is the maximum antenna dimension, and dmin is the minimum
element spacing. Create an Nwt element array of numbers, xw, evenly spaced between –1
and 1. Compute an Nwt array of weights, Wd, based on the desired weighting function
(e.g., Chebyshev, Taylor, cosn).
2.
Find the location of all of the antenna elements relative to the center of the array. Let dxmn
and dymn be the x and y locations of the mnth element relative to the center of the array.
Let 2Dx and 2Dy be the antenna widths in the x and y directions. Find the normalized
distance from the center of the array to the mnth element using

3.
Use xmn to interpolate into the array Wd to get the |amn|.
4.
If xmn > 1, set amn = 0. This causes the array to have the shape of an ellipse.
12.10.5 Computing Antenna Patterns for Planar Arrays
In Section 12.4, we presented a method of computing the radiation pattern of a linear array
that takes advantage of efficient matrix calculation routines available in modern software
(e.g., MATLAB, Mathcad, Python). We now want to extend the method to planar arrays. We
will consider variations for rectangular and triangular
packing.4
12.10.5.1 Rectangular Packing
We start by combining (12.82) with (12.79), and write
Similar to (12.66), we collect the |amn| into a matrix
and define
and

We combine (12.86) through (12.90) to write
This will produce a matrix of A(u,v)’s at all combinations of the u’s and v’s specified in the U
and V vectors.
To generate an elevation principal plane pattern (elevation cut), we would replace the first
exponential with a row vector containing M–1’s. For an azimuth principal plane pattern
(azimuth cut), we would replace the second exponential with a column vector containing N–
1’s.
To generate a radiation pattern over some region of the u,v plane, we would use (12.87) and
(12.88) with the desired values of u and v and some desired steering angles u0 and v0. As an
example, Figure 12.1 was generated using the following:
• dx/λ = dy/λ = 1/2
• u0 = v0 = 0
• U = [–1 –1+Δu … 0 … 1– Δu 1]
• V = [–1 –1+Δv … 0 … 1–Δv 1]
• Δu and Δv were set to small values.
• Amn was chosen to provide an elliptically symmetric, Taylor weighting with 
 = 6 and SL
= 30 (Appendix 12A).
• N and M were set to 51.
Because of the way U and V are defined, (12.91) can have nonzero values for u2 + v2 > 1,
which is not in visible space. This is taken into account by forcing the plotting routine to
ignore A(u,v) values for u,v pairs where u2 + v2 > 1.
12.10.5.2 Triangular Packing
Calculation of A(U,V) for triangular packing is more complicated in that it must be computed
in two parts. Figure 12.19 contains an illustration of triangular packing that we will use to
describe the method. In Figure 12.19, the circles and squares denote elements and form two
rectangular lattices that are offset in x and y by dx and dy. Let the weights associated with the
circles be amn and the weights associated with the squares be bmn. With this, we write

Equation (12.92) tells us the overall A(u,v) is the sum of A(u,v)’s for two offset arrays with
rectangular packing. Since the equations for Aa(u,v) and Ab(u,v) are the same form as (12.89),
we can use the methodology of Section 12.10.5.1 and write
To compute Aa(U,V) we would use
and
Figure 12.19 Illustration of triangular packing used to explain calculation method.
To compute Aa(U,V), we would use

and
The amplitude weights can be computed using the techniques of Section 12.10.4. When using
the product method, we would start with Ma + Mb weights in the x direction and alternately
allocate them to am and bm. Likewise, for the y direction, we would start with Na + Nb weights
and alternately allocate them to an and bn. For the elliptically symmetric case, we recognize
that the location of the ma, nath element is (xa,ya) = madx, nady for the Wamn array. For the
Wbmn array, the location of the mb, nbth element is (xb,yb) = (mbdx, nbdy). In these equations, ma
varies from 0 to 2Ma in steps of 2, na varies from 0 to 2Na in steps of 2, mb varies from 1 to
2Mb – 1 in steps of 2, and nb varies from 1 to 2Nb – 1 in steps of 2.
12.10.6 Directive Gain Pattern
In Section 12.4, we found that the directive gain pattern was given by
where
For planar arrays, we usually compute the radiation pattern as a function of u and v instead of
α and ε. Because of this, we write the directive gain pattern as
Since we have R(u,v) and not R(α,ε), we want an equation for 
 in terms of R(u,v). From
(12.76), (12.77), and (12.78), we note that R(α,ε) is a function of u = sinα cosε and v = sinε.
Also, we normally assume R(α,ε) is zero on the back of the array. Thus, we assume R(α,ε) is
zero for α outside of the range [–π/2,π/2]. With this, we can write (12.101) as
We begin the derivation by making the change of variables v = sin ε, and write

where we made use of cosε ≥ 0 for ε ∈[–π/2,π/2].
Next, we manipulate the α integral by making the change of variables
which gives
From (12.70), we have
and thus
where we made use of cosα ≥ 0 over the integration limits. Using this and some manipulation
(see Exercise 20), we get
When computing 
 by numerical integration, be careful to avoid samples on the unit circle
of the u-v plane. One way to do this is to set the integrand to zero for all u,v such that u2 + v2 ≥
1. Also, it has been the authors’ experience that computing 
 is sensitive to the integration
step size. Therefore, it is recommended that care be exercised in its use.
As an interesting example, we consider the case from Chapter 2 where
This tells us that all of the transmit energy is concentrated in a small rectangular area centered
on u = v = 0. For this case, we have

where we made use of the facts that Δu is small and v is near zero. Performing the integration
of (12.111) gives
where we made use of the fact that Δv is small. With this, we get a directive gain pattern
The directive gain is the maximum of G(u,v) or
Saying the beam is centered on u = v = 0 is the same as saying it is centered on α = ε = 0. This,
with the assumption that Δu and Δv are small, gives, Δu = Δα and Δv = Δε [see (12.77) and
(12.78)]. This leads to
which agrees with the form of directive gain discussed in Chapter 2.

12.10.7 Grating Lobes
We introduced the topic of grating lobes in Section 12.4 and noted that they are radiation
pattern peaks at angles other than the location of the main beam. Grating lobes are undesirable
because they take energy away from the main beam or can point toward interfering objects,
such as the ground. In this section, we extend the discussion of grating lobes to planar arrays.
We discuss grating lobes for arrays that use rectangular packing and arrays that use triangular
packing.
12.10.7.1 Grating Lobes in Arrays with Rectangular Packing
We start by examining A(u,v) from (12.85):
We use A(u,v) because it is easier to work with than R(u,v) or G(u,v). Since R(u,v) = |A(u,v)|2
and 
, grating lobe observations we derive from A(u,v) also apply to
R(u,v) and G(u,v).
At the main beam location, (u0,v0), we have
At any other (u,v) = (ug,vg) where 2πdx(ug–u0)/λ and 2πdy(vg–v0)/λ are both multiples of 2π,
the exponentials are unity for all m and n, and we have
The peaks at ug,vg are grating lobes, and the values of ug,vg are the locations of the grating
lobes. Thus, we say that grating lobes are located at the ug,vg that satisfy
and
where p and q are integers. Solving (12.119) and (12.120) for the pair ug,vg, we get

where p and q are integers that are not both zero (this would denote the main beam). Equation
(12.121) tells us that the grating lobes are located at integer multiples of λ/dx and λ/dy relative
to the main beam and move with the main beam.
Figure 12.20 Grating lobe locations for rectangular packing.
Figure 12.20 contains a sketch showing the locations of the main beam (the square) and the
grating lobes. The unit circle of Figure 12.20 denotes the boundary of visible space. All lobes
(grating or main beam) that are within the circle translate to lobes in visible space (real α, ε
space), and lobes outside of the unit circle do not (they translate to lobes in imaginary α, ε
space). In the example of Figure 12.20, the main beam and one grating lobe are in the unit
circle. Thus, the main beam and one grating lobe are in visible space5 since the spacing
between grating lobes is λ/dx and λ/dy. Whether or not grating lobes enter visible space
depends on the element spacing (dx and dy) and the desired set of steering angles (u0,v0). This
represents a trade-off array antenna designers must face. On one hand, there is a desire to
make dx and dy large to minimize the number of elements, and thus array cost.6 On the other
hand, dx and dy must be small to avoid grating lobes.7
The first grating lobes to enter visible space are the eight that surround the main beam. Of
these, the four immediately adjacent to the main beam are most likely to enter visible space
before the four located on the diagonals. When a grating lobe enters visible space (as the
main beam is moved), it does so on the unit circle boundary of the u, v plane. We can use this
to specify the maximum dx and dy that avoid grating lobes in visible space at some maximum
steering angles of |u0|max and |v0|max. Specifically, we get
or

Table 12.1
d/λ Spacing to Avoid Grating Lobes as Various Maximum Steering Angles—Rectangular Packing
Maximum Steering Angle
d/λ
30
0.67
45
0.59
60
0.54
90
0.50
Table 12.1 contains a list of dxmax/λ and dymax/λ values for a few maximum steering angles.
Note that an element spacing of λ/2 prevents grating lobes from entering visible space at all
steering angles except the limiting case of ±90°.
12.10.7.2 Grating Lobes in Arrays with Triangular Packing
For triangular packing, we start with (12.92), but we write it in a slightly different form as
where
with (N1,M1) = (Ma,Na), (N2,M2) = (Mb,Nb), and cmn replaced by amn or bmn as appropriate. At
(u, v) = (u0, v0), we have
If 4πdx(ug – u0)/λ and 4πdy(vg – v0)/λ are both multiples of 2π, we have A1(ug,vg) = A1(u0,v0)
and A2(ug,vg) = A2(u0,v0). This results in

If dx(ug – u0)/λ + dy(vg – v0)/λ is an integer, the product of exponentials is unity, and we have
If we combine the conditions that 4πdx(ug – u0)/λ and 4πdy(vg – v0)/λ and follow the logic that
led to (12.121), we get that the grating lobes are located at
The condition that dx(ug – u0)/λ + dy(vg – v0)/λ must be an integer leads to an additional
constraint on (12.129) that p + q must be even.
Equation (12.129) tells us that grating lobes are located at the positions indicated by
(12.121) with the added constraint that p + q must be even. Figure 12.21 contains a sketch of
the grating lobes for triangular packing. As before, the square denotes the main beam, and the
unit circle denotes the boundary of visible space.
The first grating lobe to enter visible space is one of the eight surrounding the main beam
(denoted by the dashed rings). The grating lobe to enter first depends on the specific x and y
locations of the elements. The grating lobes to the left and right of the main beam enter
visible space when
The grating lobes above and below the main beam enter visible space when
Finally, one of the grating lobes on the diagonals enters visible space when
where

and Dmax is the maximum beam steering angle in the θ direction.
Figure 12.21 Grating lobe locations for triangular packing.
We consider the classical textbook condition [14, 18, 21, 27] where the array elements are
arranged on an equilateral triangle with 2dx as the width of the base and other two legs, we get
 (see Figure 12.22). With this, θ = 30° and (12.132) becomes
With some manipulation (see Exercise 18), (12.134) can be solved to determine
Table 12.2 contains a list of values for dx and dy for example maximum scan angles.

Figure 12.22 Triangular element packing parameters.
Table 12.2
d/λ Spacings to Avoid Grating Lobes as Various Maximum Steering Angles—“Standard” Triangular Packing
Maximum Steering Angle
(dx/λ, dy/λ)
30
(0.38, 0.67)
45
(0.34, 0.59)
60
(0.31, 0.54)
90
(0.29, 0.50)
12.11 POLARIZATION
Thus far in our discussions, we have played down the role of E-field orientation in antennas.
We now discuss E-field orientation for the specific purpose of discussing polarization. E-
fields have both direction and magnitude (and frequency). In fact, an E-field is a vector that is
a function of both spatial position and time. If we consider a vector E-field that is traveling in
the z direction of a rectangular coordinate system, we can express it as [28]
where 
 and 
 are unit vectors. Figure 12.23 contains a graphic showing the above E-field.
In this drawing, the z axis is the line-of-sight (LOS) vector from the radar to the target. The x-
y plane is in the neighborhood of the face of the antenna. The y axis is generally up, and the x
axis is oriented to form a right-handed coordinate system. This is the configuration for
propagation from the antenna to the target. When considering propagation from the target, the
z axis points along the LOS from the target to the antenna, the y axis is generally up, and the x
is again oriented to form a right-handed coordinate system.

Figure 12.23 Axes convention for determining polarization.
When we speak of polarization, we are interested in how the E-field vector, 
, behaves
as a function of time for a fixed z, or as a function of z for a fixed t. To proceed further, we
need to write the forms of Ex (t,z) and Ey (t, z). We use the simplified form of sinusoidal
signal. With this, we get
where, Exo and Eyo are positive numbers and represent the E-field strength. ƒo is the carrier
frequency, and λ is the wavelength, which is related to ƒo by λ = c/ƒo. ϕ is a phase shift used to
control polarization orientation.
If 
 remains fixed in orientation as a function of t and z, the E-field is said to be
linearly polarized. In particular,
• If ϕ = 0, Exo ≠ 0, and Eyo = 0, we say the E-field is horizontally polarized.
• If ϕ = 0, Eyo ≠ 0, and Exo = 0, we say the E-field is vertically polarized.
• If ϕ = 0, and Exo = Eyo ≠ 0, we say the E-field has a slant 45° polarization.
• If ϕ = 0, and Exo ≠ Eyo ≠ 0, we say the E-field has a slant polarization at some angle other
than 45°. The polarization angle is given by tan-1 (Eyo / Exo).
• If ϕ = ±π/2 and Exo = Eyo ≠ 0, we say we have circular polarization.
If ϕ = +π/2, the polarization is left-circular because 
 rotates counter-clockwise, or
to the left, as t or z increases. If ϕ = –π/2, the polarization is right-circular because 
rotates clockwise, or to the right, as t or z increases.
• If ϕ is any other angle besides ±π/2, 0, or π, and/or Exo ≠ Eyo ≠ 0, we say the polarization
is elliptical. It can be left (ϕ = +π/2) or right (ϕ = –π/2) elliptical.
As a note, polarization is always measured in the direction of propagation of the E-field
to/from the antenna from/to the target. When polarization of an antenna is specified, it is the

polarization in the main beam. The polarization in the sidelobes can be dramatically different
than the polarization in the main beam.
12.12 REFLECTOR ANTENNAS
Older radars, and some modern radars where cost is an issue, use reflector types of antennas
rather than phased arrays. Reflector antennas are much less expensive than phased arrays
(thousands to hundreds of thousands of dollars as opposed to millions or tens of millions of
dollars). They are also more rugged than phased arrays and are generally easier to maintain.
They can be designed to achieve good directivity and low sidelobes. The main disadvantages
of reflector antennas, compared to phased array antennas, are that they must be mechanically
scanned. This means radars that employ reflector antennas have limited multiple target
capability. In fact, most target-tracking radars that employ reflector antennas can track only
one target at a time. Search radars that employ reflector antennas can detect and track multiple
targets, but the track update rate is limited by the scan time of the radar, which is usually on
the order of 1’s to 10’s of seconds. This, in turn, limits the track accuracy of these radars.
Another limitation of radars that employ reflector antennas is that separate radars are
needed for each function. Thus, separate radars would be needed for search, track, and missile
guidance. This requirement for multiple radars leads to trade-offs in radar system design.
With a phased array, it may be possible to use a single radar to perform the three
aforementioned functions (referred to as a multifunction radar). Thus, while the cost of a
phased array is high, relative to a reflector antenna, the cost of three radars with reflector
antennas may be even more expensive than a single phased array radar.
Almost all reflector antennas use some variation of a paraboloid (parabola of revolution)
[14]. An example of such an antenna is shown in Figure 12.24. The feed shown in Figure 12.24
is located at the focus of the parabolic reflector (focal point). Since it is in the front, this
antenna would be termed a front-fed antenna. The lines from the reflector to the feed are struts
used to keep the feed in place.
A parabola is used as a reflector because of its focusing properties. This is illustrated in
Figure 12.25. In Figure 12.25, the feed is at the focus of the parabola. From, analytic geometry
[29], if rays emanate from the focus and are reflected from the parabola, the reflected rays
will be parallel [14; 30, p. 147]. In this way, the parabolic antenna focuses the divergent E-
field from the feed into a concentrated E-field [31–33]. Stated another way, the parabolic
reflector collimates the E-field of the feed.
As with space-fed phased arrays, the feed pattern is used to control the sidelobe levels of a
reflector antenna by concentrating the energy at the center of the reflector and causing it to
taper toward the edge of the reflector.
The process of computing the radiation pattern for a parabolic reflector antenna, where the
feed is at the focus, is reasonably straightforward. Referring to Figure 12.25, we place a
hypothetical plane parallel to the face of the reflector, usually at the location of the feed. This
plane is termed the aperture plane. We then put a grid of points in this plane. The points are
typically arranged on a rectangular grid and are spaced λ/2 apart. The boundary of the points

is a circle that follows the edge of the reflector. The points are used as elements in a
hypothetical phased array.
We think of the points, pseudo array elements, as being in the x-y plane whose origin is at
the feed. The z axis of this coordinate system is normal to the aperture plane.
Figure 12.24 Example of a parabolic reflector antenna.
Figure 12.25 Geometry used to find reflector radiation pattern.
If we draw a line, in the x-y plane, from the origin to the point (x, y), the angle it makes with
the x axis is

where the arctangent is the four-quadrant arctangent. The distance from the origin to the point
is
We can draw a line from the point, perpendicular to the aperture plane, to the reflector.
Examples of this are the lines l1 and l2 in Figure 12.25. The next step is to find the angle, θ,
between the z axis and the point on the reflector. From Figure 12.25
where ƒ is the focus of the parabola. Also
With this, we can solve for d and θ to yield
and
Next, the angles ϕ and θ are used to find the directive gain of the feed at the point where the
ray intersects the parabolic reflector. This directive gain gives the amplitude of the pseudo
element at (x, y).
The above process is repeated for all of the pseudo elements in the aperture plane. Finally,
the reflector antenna radiation pattern is found by treating the pseudo elements as a planar
phased array.8
There is no need to be concerned about the phase of each pseudo element since the distance
from the feed to the reflector to all points in the aperture plane is the same. This means the
various rays from the feed take the same time to get to the aperture plane. This further implies
that the E-fields along each array have the same phase in the aperture plane, and that the beam
is steered to broadside.
If the feed is not located at the focus of the paraboloid, the calculations needed to find the
amplitudes and phases of the E-field at the pseudo elements become considerably more
complicated. It is well beyond the scope of this book.

12.13 OTHER ANTENNA PARAMETERS
In Sections 12.2 and 12.3, we showed that the beamwidth of a linear array was a function of
the length of the array. We want to revisit that problem and develop a specific relation between
the array length and the beamwidth. For an N-element, uniformly illuminated linear array
steered to broadside, we have [see (12.38)]
To determine the beamwidth, we need to find the value of ε3dB for which R(ε3dB) = ½. With
this, we have that the beamwidth is εB = 2ε3dB. Through a numerical search, we find that
R(ε3dB) = ½ when
which gives
We recognize that the antenna length is D = Nd and make use of the fact that D is large relative
λ to arrive at (which means sin(ε3dB) is small)
If the array uses some type of amplitude weighting for sidelobe reduction, the factor of 0.886
can increase. Experimentation with a few weights indicates that the factor can be as high as
about 1.4 for heavy Chebyshev weighting. The 30-dB Chebyshev weighting used to generate
Figure 12.12 resulted in a factor of 1.1, which would yield a beamwidth of
This is a rule of thumb the authors use both for linear and planar arrays that employ an
amplitude taper to reduce sidelobes, and for reflector antennas. Our rule of thumb for arrays
that use uniform illumination is (12.147).
The beamwidth equations are based on the assumption that the beam is steered to broadside.
As the beam is steered off of broadside, the beamwidth increases and these equations are no
longer valid.

In Chapter 2, we defined directivity in terms of effective aperture as
where Ae is the effective aperture and is related to the physical area of the antenna by Ae =
ρantA where A is the physical area of the antenna and ρant is the antenna efficiency. ρant
accounts for amplitude tapers and the fact that the beam may not be steered to broadside.
The derivation of (12.149) for the general antenna is very difficult because of the difficulty
in expressing 
 in terms of the antenna area. However, Balanis [22, Section 11.5.1] has a
derivation for the case of a rectangular aperture in an infinitely conducting ground plane. The
aperture has dimensions of a and b and the electric and magnetic fields across the aperture are
uniform with magnitudes of E0 and E0/η, respectively. η is the radiation resistance. Under
these conditions, Balanis shows that the maximum radiation intensity is
and
where Prad is the total radiated power.
From (12.150) and (12.151), Balanis derives the maximum directive gain, or directivity, as
where A = ab is the area of the aperture. In Balanis’ example, the aperture is uniformly
illuminated and the main beam is pointing along the normal to the aperture (broadside).
Because of this, the antenna efficiency is ρant = 1 and Ae = A. While this development does not
prove that (12.152) holds for all antennas, experience indicates that it does. That is, the
maximum directivity is directly proportional to effective aperture and inversely proportional
to the square of wavelength.
12.14 EXERCISES
1.
Generate the plot of Figure 12.4. What are the beamwidths for the three cases?
2.
Derive (12.35).
3.
Generate the plot of Figure 12.7. What are the beamwidths for the three cases?

4.
Derive (12.46).
5.
Derive (12.47).
6.
Show that 
 = 1 for an N-element array with d = λ/2 and 
.
7.
Reproduce Figure 12.9.
8.
Implement the computation algorithm described in Section 12.9, and use it to generate and
plot the radiation pattern for a 20-element linear array with 30-dB Chebyshev weighting.
Your plot should look similar to Figure 12.12. In particular, the peak sidelobe level
should be 30 dB below the main beam.
9.
Compute 
 for the radiation pattern of Exercise 8, and use it to reproduce the directive
gain pattern of Figure 12.12. Steer to 30°. What happens to the directive gain [peak of
G(ε)] relative to the value at ε0 = 0°? What happens to the beamwidth? Repeat this for ε0 =
60°.
10. Repeat Exercises 8 and 9 but with N = 50 (50 elements) and Taylor weighting with 
 = 6
and SL= –30 dB (see Appendix 12A).
11. Recompute the weights you used to steer the beam of Exercise 10 with the assumption that
the phases are set by a 6-bit phase shifter. Repeat for the case where the phases are set by a
3-bit phase shifter. Repeat this for the case where the beam is steered to ε0 = 0°. Can you
explain the difference in the effect of quantization for this case when compared to the
cases where the beam was steered to 30° and 60°?
12. Implement the computation algorithm of Section 12.10.5.1. Use it to generate a radiation
pattern for a square array that has 51 rows and 51 columns of elements (M and N = 51).
Assume an element spacing of dx/λ = dy/λ = ½ and uniform weighting. Steer the beam to
(u0,v0) = (0,0). Generate a three-dimensional (3-D) plot of the form shown in Figure 12.1.
Generate azimuth and elevation principal plane cuts for azimuth and elevation angles that
range from –5° to 5°. The 3-D plot for this exercise should have the principal plane
ridges indicated in Section 12.10.5.12. Change the square array of Exercise 11 to a
circular array by using the method indicated in step 4 of the method for applying
elliptically symmetric weighting (Section 12.10.5). Generate the plots indicated in
Exercise 11. In this case, you will note that the principal plane ridges are no longer
present.
13. Repeat Exercise 12 with a circularly symmetric, Taylor, 
 = 6, SL = –30 dB weighting
applied to the array. In this case, the 3-D plot should look similar to Figure 12.1.
14. Compute 
 and the directive gain pattern G(u,v) for the array of Exercise 11. Generate
the plots indicated in Exercise 11.
15. Apply multiplicative, Taylor, 
 = 6, SL = –30 dB weighting to the array of Exercise 11,
and recompute 
 and G(u,v). Generate the plots indicated in Exercise 11. Note how the
directive gains (directivities) and beamwidths of the patterns compare to those of
Exercise 13.

16. Apply an elliptically symmetric taper cos taper (cosn taper with n = 1) to the array of
Exercise 12. For the first case use a 10-dB edge taper. This means that you want the
amplitude at the edge of the array to be 10–(10/20) relative to a peak value of 1. Repeat this
for a 20-dB edge taper. Discuss the difference in directive gain, beamwidth, and sidelobe
levels for the two tapers.
17. In Table 12.1, we indicate that if dx/λ = dy/λ, grating lobes enter visible space when the
beam is scanned more than 45° from broadside. To verify this, change dx/λ and dy/λ of the
array of Exercise 11 to 0.59. Steer the beam to (u0, v0 = 10, sin 50°). Generate an elevation
principal plane cut. This plot will show that the main beam is steered to 50°, but there is
another lobe, the grating lobe, at a negative angle. You should also note that the width of
the grating lobe is larger than the main lobe.
18. Derive (12.135).
19. Using (12.77) and (12.78), show that u and v satisfy the constraint, u2 + v2 ≤ 1.
20. Derive (12.108).
References
[1]
Bondyopadhyay, P. K., “The First Application of Array Antenna,” Proc. 2000 IEEE Int. Conf. on Phased Array Syst.
Technol., Point Dana, CA, May 21–25, 2000, pp. 29–32.
[2]
Brittain, J. E., “Electrical Engineering Hall of Fame: Guglielmo Marconi,” Proc. IEEE, vol. 92, no. 9, Aug. 2004, pp.
1501–1504.
[3]
Mailloux, R. J., “A Century of Scanning Array Technology,” 2014 IEEE Antennas Propag. Soc. Int. Symp. (APSURSI),
Memphis, TN, Jul. 6–11, 2014, pp. 524–525.
[4]
Friis, H. T., and C. B. Feldman, “A Multiple Unit Steerable Antenna for Short-Wave Reception,” IRE Proc., vol. 25, no.
7, Jul. 1937, pp. 841–917; Bell System Tech. J., vol. 16, no. 3, Jul. 1937, pp. 337–419.
[5]
Friis, H. T., and W. D. Lewis, “Radar Antennas,” Bell System Tech. J., vol. 26, no. 2, Apr. 1947, pp. 219–317.
[6]
Schell, A. C., “Antenna Developments of the 1950s to the 1980s,” 2001 IEEE Antennas Propag. Soc. Int. Symp., vol. 1,
Boston, MA, Jul. 8–13, 2001, pp. 30–33.
[7]
Blake, L. V., Antennas, Dedham, MA: Artech House, 1984.
[8]
Gradshteyn, I. S., and I. M. Ryzhik, Table of Integrals, Series, and Products, 8th ed., (D. Zwillinger and V. Moll, eds.)
New York: Academic Press, 2015. Translated from Russian by Scripta Technica, Inc.
[9]
Silver, S., Microwave Antenna Theory and Design, vol. 12 of MIT Radiation Lab. Series, New York: McGraw-Hill,
1949; Norwood, MA: Artech House (CD-ROM edition), 1999.
[10] IEEE Standard Dictionary of Electrical and Electronic Terms, 6th ed., New York: IEEE, 1996.
[11] Dolph, C. L., “A Current Distribution for Broadside Arrays Which Optimizes the Relationship Between Beam Width and
Side-Lobe Level,” Proc. IRE, vol. 34, no. 6, Jun. 1946, pp. 335–348.
[12] Riblet, H. J., “Discussion of Dolph’s Paper,” Proc. IRE, vol. 35, no. 5, May 1947, pp. 489–492.
[13] Taylor, T. T., “Design of Line-Source Antennas for Narrow Beamwidth and Low Side Lobes,” Trans. IRE Prof. Group
Antennas Propag., vol. 3, no. 1, Jan. 1955, pp. 16–28.
[14] Johnson, R. C., ed., Antenna Engineering Handbook, 3rd ed., New York: McGraw-Hill, 1992.
[15] Skolnik, M. I., ed., Radar Handbook, 3rd ed., New York: McGraw-Hill, 2008.
[16] Rhodes, D. R., “On the Taylor Distribution,” IEEE Trans. Antennas Propag., vol. 20, no. 2, Mar. 1972, pp. 143–145.

[17] Ostroff, E. D., et al., Solid-State Radar Transmitters, Dedham, MA: Artech House, 1985.
[18] Kahrilas, P. J., Electronic Scanning Radar Systems (ESRS) Design Handbook, Norwood, MA: Artech House, 1976.
[19] Hansen, R. C., ed., Microwave Scanning Antennas, Vol. III: Array Systems, New York: Academic Press, 1966.
[20] Koul, S. K., and B. Bhat, Microwave and Millimeter Wave Phase Shifters, Vol. I and II, Norwood, MA: Artech House,
1991.
[21] Brookner, E., ed., Practical Phased Array Antenna Systems, Norwood, MA: Artech House, 1991.
[22] Balanis, C. A., Antenna Theory: Analysis and Design, 3rd ed., New York: Wiley & Sons, 2005.
[23] Ma, M. T., Theory and Application of Antenna Arrays, New York: Wiley & Sons, 1974.
[24] Von Aulock, W. H., “Properties of Phased Arrays,” Proc. IRE, vol. 48, no. 10, Oct. 1960, pp. 1715–1727.
[25] Schelkunoff, S. A., Advanced Antenna Theory, New York: Wiley & Sons, 1952.
[26] Mofrad, R. F., R. A. Sadeghzadeh, and S. Alidoost, “Design of a Benchmark Antenna for the Multi Function Phased
Array Radar Simulation Test Bed,” 2011 Loughborough Antennas and Propag. Conf. (LAPC), Nov. 14–15, 2011, pp.
1–4.
[27] Mailloux, R. J., Phased Array Antenna Handbook, 2nd ed., Norwood, MA: Artech House, 2005.
[28] Stutzman, W. L., Polarization in Electromagnetic Systems, Norwood, MA: Artech House, 1993.
[29] Leithold, L., The Calculus with Analytic Geometry, 5th ed., New York: Harper & Row, 1986.
[30] Barton, D. K., Radar System Analysis and Modeling, Norwood, MA: Artech House, 2005.
[31] Cutler, C. C., “Parabolic-Antenna Design for Microwave,” Proc. IRE, vol. 35, no. 11, Nov. 1947, pp. 1284–1294.
[32] Berkowitz, B. “Antennas Fed by Horns,” Proc. IRE, vol. 41, no. 12, Dec. 1953, pp. 1761–1765.
[33] Jones, E. M. T., “Paraboloid Reflector and Hyperboloid Lens Antennas,” Trans. IRE Prof. Group on Antennas Propag.,
vol. 2, no. 3, Jul. 1954, pp. 119–127.
APPENDIX 12A: AN EQUATION FOR TAYLOR WEIGHTS
The following are equations for calculating Taylor weights for an array antenna. It is similar
to the equation on page 20-8 of the Antenna Engineering Handbook by Richard C. Johnson
[14], with some clarifications and corrections.
The un-normalized weight for the nth element of the N-element linear array is
where

and
SL is the desired sidelobe level, in decibels, relative to the peak of the main beam and is a
positive number. For example, for a sidelobe level of −30 dB, SL = 30. This indicates that the
sidelobe is 30 dB below the peak of the main beam.  is the number of sidelobes on each side
of the main beam that we want to have a level of approximately SL below the main beam peak
amplitude.
The xn can be computed using the following MATLAB notation:
Finally, normalize the weights by dividing all of the an by 
.
1 Sometimes the wave number, k = 2π/λ, is used to simplify notation.
2 In this book, we use the terms directive gain pattern and antenna pattern synonymously. We also use directivity and directive
gain synonymously.
3 Also referred to as array separable.
4 The algorithm discussed in this section was provided by Joshua Robbins of Dynetics, Inc.
5 The main beam will always be in visible space since (u0,v0) = (sinα0 cosε0, sinε0) satisfy 
.
6 Some arrays, termed limited-scan arrays, are specifically designed to use small scan angles [19].
7 In some instances, there are other requirements on element spacing such as mutual coupling [26] and packaging.
8 Technically, the feed and supporting struts result in aperture blockage, which causes perturbations in the radiation pattern (e.g.,
reduced gain, elevated sidelobes). This blockage can be accounted for by subtracting the antenna pattern of the blockage
from the antenna pattern without blockage.

Chapter 13
Signal Processor Analyses
13.1 INTRODUCTION
In this chapter, we turn our attention to signal processors and analysis of their performance.
Our first encounters with signal processors were in Chapters 7 and 8, where we studied
matched filters and coherent integrators. In those studies, the purpose of the signal processor
was to improve SNR. In this chapter, we will be concerned with signal processors whose
primary function is clutter rejection, with SNR improvement as a secondary objective. By
clutter, we mean returns from unwanted sources such as the ground or rain.
Rather than discuss clutter and signal processor analyses in general terms, we will explain
how to perform specific analyses. To this end, we will select specific radar, target, and clutter
scenarios, as well as specific signal processors. We assume the radar is ground based and has
the job of detecting and tracking airborne targets such as aircraft, helicopters, and cruise
missiles. We assume the targets are flying at low altitudes so that the radar is receiving returns
from the target, the ground, and possibly rain. In our case, the ground and rain are the clutter
sources (termed ground clutter and rain clutter). We assume the radar is transmitting a pulsed
(as opposed to CW) signal. We will consider the cases where the radar transmits a finite and
infinite (actually, semi-infinite) series, or burst, of pulses with a given PRI. We will also
consider the case where the PRI varies from pulse-to-pulse. We assume the target RCS is
SW0/SW5, SW1, or SW3. This means we assume the target RCS is constant over time
intervals during which the signal processor is operating (referred to as a coherent processing
interval, or CPI). We also assume the clutter RCS is constant over this time interval.
We will consider two types of signal processors: moving target indicator (MTI) processors
and Doppler processors. MTI processors are used strictly for clutter rejection (usually
ground clutter) and Doppler processors are used to provide both clutter rejection and SNR
improvement. Doppler processors are often subdivided into high PRF (HPRF) pulsed Doppler
processors, medium PRF (MPRF) pulsed-Doppler processors, and low PRF (LPRF) pulsed
Doppler processors. The latter are also sometimes termed moving target detector (MTD)
processors [1–7].
Since we will be concerned with analyzing the clutter rejection properties of signal
processors, we begin our studies by defining simple ground and rain clutter models. We next
derive the form of the clutter, target and noise signals at the input to the signal processor.
During this derivation, we discuss some characteristics of the radar that affect the clutter
spectrum so as to also characterize their impact on signal processor performance.
We follow the above characterizations with discussions of MTI and pulsed Doppler
processors. In addition to analyzing their clutter rejection and SNR improvement properties,
we address related topics such as the use of staggered PRI waveforms with MTI, MTI velocity

response, phase noise, analog-to-digital converter (ADC) quantization, and the transient
response of signal processors. Toward the end of the chapter, we develop a simple chaff
clutter model and address the performance of MTI and Doppler processors in the presence of
chaff.
It appears that the first use of signal processing to mitigate clutter occurred during World
War II when radar operators noted they could distinguish targets from clutter by the fluttering
of the clutter return on an A-scope, which came to be known as the butterfly effect. This effect
was incorporated into a target-in-clutter detection system that came to be known as a
noncoherent MTI [8]. In the mid-1940s, Alfred G. Emslie invented coherent MTI, for which he
was granted several patents [9–18].
Harry B. Smith, who was the president of the Westinghouse Defense and Electronics
Systems Center for 10 years, was inducted into the Innovation Hall of Fame in 1987 “for
invention of pulsed Doppler radar and other innovations in airborne electronics” [19].
Likewise, Smith, Leroy C. Perkins, and David H. Mooney were awarded the IEEE Pioneer
Award in 1984 “for Contributions to the development of the high-repetition-rate Airborne
Pulse Doppler Radar” [20, 21]. Smith, Perkins, and Mooney were awarded patents for a “Pulse
Doppler Radar System” in 1961 and 1962 [22, 23].
It is not clear who invented the MTD processor. In 2005, Charles E. Muehe was awarded the
IEEE Aerospace and Electronic Systems Society (AESS) Pioneer Award “for the invention of
the Moving Target Detector (MTD) digital signal processor for aircraft surveillance radar”
[24–26]. However, a 1977 MIT Lincoln Laboratories report seems to indicate that Ronald S.
Bassford, William Goodchild, and Alfred de la Marche developed it to solve problems
associated with air route surveillance radars (ARSRs) [27].
13.2 CLUTTER MODEL
13.2.1 Ground Clutter Radar Cross Section (RCS) Model
A drawing we will use to develop the ground clutter model is shown in Figure 13.1 [28; 29, p.
63; 30; 31, p. 16–22; 32]. The top drawing represents a top view and the bottom drawing
represents a side view. For the initial development of the ground clutter model, we assume the
earth is flat. Later, we will add a correction factor to account for the fact that the earth is not
flat. The clutter model we develop is termed a smooth earth clutter model, which means we
are not modeling specific terrain features such as trees, rocks, buildings, hills, and valleys.
The triangle and semicircle on the left of Figure 13.1 represents the radar, which is located
a height of h above the ground. When we discuss radar height, we refer to the height to the
phase center of the antenna. The phase center is usually taken to be the location of the feed for
a reflector antenna or the center of the array for a phased array antenna [33–35].
The dashed lines on the side and top views denote the 3-dB boundaries of the antenna main
beam. The angles εB and αB denote the elevation and azimuth 3-dB beamwidths, respectively.
The horizontal line through the antenna phase center is a reference line. It is not the elevation

angle to which the main beam is steered. The target is located at a range of R and an altitude
of hT. The elevation angle from the antenna phase center to the target is
Figure 13.1 Geometry for a ground clutter model.
In the geometry of Figure 13.1, the clutter patch of interest is also located at a range of R
from the radar. In most applications, this is the region of clutter that is of interest because we
are concerned with the clutter that competes with the target. However, for some cases, most
notably MPRF and HPRF pulsed Doppler radars, the ground clutter that competes with the
target will not be at the target range, but at a much shorter range.
The width of the clutter patch along the R direction is ΔR. In most cases, ΔR is taken as the
range resolution of the radar because almost all signal processors quantize the incoming
signal into range cells that have a width of one range resolution cell. In some cases a range
resolution cell is large enough to cause problems with the accuracy of the ground clutter
model. In this case, ΔR is taken to be smaller than a range resolution cell, and the signal
processor calculations must include integration of clutter power across ΔR intervals.
The ground region that extends over ΔR at a range of R is an annulus centered on the radar.

This is depicted in the top view where a portion of the annulus is shown. For purposes of
calculating the RCS of the ground in this annulus, it is divided into two regions. One of these
is termed the main beam clutter region and represents the ground area illuminated by the main
beam of the radar. The other is termed the sidelobe clutter region and represents the ground
area illuminated by the sidelobes of the radar. We will assume the sidelobe clutter region
extends from −π/2 to π/2. In other words, we assume there are no clutter returns from the back
of the radar.1 As implied by the statements above, the ground clutter model incorporates the
transmit and receive antenna beam characteristics. In this development, we assume a
monostatic radar that uses the same antenna for transmit and receive.
The size of the clutter RCS will depend upon the size of the ground area illuminated by the
radar (the region discussed in the previous paragraph) and the reflectivity of the ground. We
denote this reflectivity by the variable σ0. Consistent with the previous discussions of target
RCS, we can think of clutter reflectivity as the ability of the ground to absorb and reradiate
energy. In general, clutter reflectivity depends upon the type of ground (soil, water, asphalt,
gravel, sand, grass, or trees) and its roughness. It also depends upon moisture content and
other related phenomena. Finally, it also depends upon the angle to the clutter patch (εR in
Figure 13.1). Detailed discussions of σ0 can be found in [1, 2, 31, 33, 36–44].
For the analyses we consider in this book, we will use three values for σ0: σ0 = −20 dB, σ0 =
−30 dB, and σ0 = −40 dB (Table 13.1). These are fairly standard values currently used for
radars that operate in the 5- to 10-GHz range. The first case corresponds to moderate clutter
and would be representative of trees, fields, and choppy water. The second value is light
clutter and would be representative of sand, asphalt, and concrete. The third value is very light
clutter and would be representative of smooth ice and smooth water [2, 28–30, 32, 39].
With the above, we can write the RCS of the main beam ground area as
where the various parameters are shown on Figure 13.1. An assumption in this equation is that
the azimuth beamwidth, αB, is small so that the arc subtended by αB can be assumed to be a
straight line that is perpendicular to Δd.
From the bottom part of Figure 13.1, we note that the clutter area is not located at beam
center. This means the clutter patch is not being fully illuminated, in elevation, by the antenna
main beam. To account for this, we include a loss term (actually, a gain that is less than unity)
that depends upon the normalized directive gain pattern of the antenna. One approach is to use
a normalized version of directive gain pattern of the antenna, G(α,ε), evaluated at α = 0 [see
(12.40) of Chapter 12]. An alternate is to use a generic pattern that provides a reasonable
approximation of the actual, (normalized), pattern, at least in the main beam [2, p. 150; 28; 29,
pp. 147–148]. One of these generic patterns is the pattern of an antenna with uniform
illumination. In the main beam region, this pattern is a sinc2(x) function and leads to the
generic form

Another generic form that works reasonably well is the Gaussian approximation
In (13.3) and (13.4), ε is the elevation angle off of beam center and εB is the elevation
beamwidth of the antenna. Showing that the above models are reasonably good
approximations in the mainbeam region is left as an exercise (Exercise 1). Of the two, the
second is easier to use because it is not a piecewise function.
Table 13.1
Ground Clutter Backscatter Coefficients
Backscatter Coefficient, σ0 (dB)
Comment
−20
Moderate clutter—trees, fields, choppy water
−30
Light clutter—sand, asphalt, concrete
−40
Very light clutter—smooth ice, smooth water
With this, we can modify the equation for the main beam clutter as
where εo is the elevation pointing angle of the main beam and εR = sin−1(h/R) is the angle to
the ground patch. In some applications, we assume the main beam is pointed at the target so
that εo = εT.
Equation (13.5) carries the assumption that the transmit and receive elevation antenna
patterns are the same. If they are different, we would replace G2(ε) with GT(ε)GR(ε), where
GT(ε) and GR(ε) are the transmit and receive antenna patterns.
The basic approach for sidelobe clutter is the same as for the main beam clutter, but in this
case we need to account for the fact that the sidelobe clutter represents ground areas that are
illuminated through the transmit antenna sidelobes and whose returns enter through the
receive antenna sidelobes. The ground area of concern is the semicircular annulus excluding
the mainbeam region. Relatively speaking, the ground area illuminated by the main beam is
small compared to the ground area illuminated by the sidelobes. Because of this, it is
reasonable to simplify calculations by including the main beam area in with the sidelobe area.
With this, the RCS of the clutter in the sidelobe region is

where SL is the average antenna sidelobe level relative to the main beam peak. A typical value
for SL is −30 dB or 0.001. However, it could be as low as −40 to −45 dB for “low sidelobe”
antennas. The equation above includes (SL)2 to account for the fact that the clutter is in the
sidelobes of the transmit and receive antenna. If the sidelobes of the transmit and receive
antennas are different, we would use SLT × SLR instead of (SL)2.
To get the total clutter RCS from both the main lobe and the sidelobes, we assume the
clutter signals are random processes that are uncorrelated from angle to angle. (We also
assume the clutter signals are uncorrelated from range cell to range cell.) Since the clutter
signals are uncorrelated random processes, and since RCS is indicative of energy or power,
we can get the total clutter RCS by adding the main beam and sidelobe RCSs. Thus,
In this equation, the terms d and ∆d are related to range, R, and range resolution, ∆R, by d =
RcosεR and ∆d = ∆R/cosεR (see Figure 13.1). We use this to rewrite (13.7) as
For the final step, we need a term to account for the fact that the earth is round and not flat.
We do this by including a pattern propagation factor. This pattern propagation factor allows
the clutter RCS to gradually decrease as clutter cells move beyond the radar horizon. David
Barton performed detailed analyses that led to sophisticated models for computing the pattern
propagation effects [1, 37, 38, 45]. He also provided a simple approximation that works well.
Specifically, he defined a loss factor as
where Rh is the range to the radar horizon and is defined as
with RE = 6,371,000 m being the mean radius of the earth. The 4/3 factor in the above equation
invokes the constant 4/3 earth model. This model states that, to properly account for
diffraction, we need to increase the earth radius to effectively reduce its curvature. The 4/3
earth model is discussed in several references [1, 2, 46–49]. The derivation of (13.10) is left
as an exercise (Exercise 2).
If we combine (13.9) and (13.8), we get

Figure 13.2 contains a plot of clutter RCS for a typical scenario. In particular, the radar uses
a circular beam with a beamwidth of 2°. Thus, αB = εB = 2(π/180) rad. We assume a sidelobe
level of SL = 0.001 (−30 dB) and a range resolution of ΔR = 150 m (a 1-µs pulse). The phase
center of the antenna is at h = 6 m. The three curves of Figure 13.2 correspond to beam
pointing angles (εo) of 0, ½, and 1 beamwidth above horizontal. The clutter backscatter
coefficient was assumed to be σ0 = 0.01 (−20 dB).
The first observation from Figure 13.2 is that the ground clutter RCS is quite large for low
beam elevation angles and short ranges. This means, for low altitude targets at short ranges
(less than about 30 km), the clutter RCS will be larger than typical aircraft targets, which have
RCSs in the range of 6 to 10 dBsm [2, 30]. Thus, unless the radar includes signal processing
to reduce the clutter returns, they will dominate the target returns. At larger elevation angles,
the problem is less severe because the ground is no longer being illuminated by the main
beam.
Figure 13.2 Ground clutter RCS for σ0 = −20 dB.
The shape of the curves of Figure 13.2 requires some discussion. Examination of the
equation for clutter RCS indicates that the numerator term increases with increasing range to
the clutter. However, for ranges past the radar horizon, which is at a range of 9.2 km for this
radar, the pattern propagation factor of (13.9) starts to predominate and reduces the clutter
RCS. This is what causes the curves of Figure 13.2 to first increase and then decrease.
13.2.2 Ground Clutter Spectrum Model
The main signal characteristic we use to distinguish clutter from targets is Doppler frequency.
Because of this, we need a model for the spectrum of signals returned from clutter.

The simplest Doppler spectrum model for ground clutter is to assume the Doppler
frequency is zero. However, this is not strictly correct because, in most cases, the elements
that make up ground clutter (leaves, grass, or waves, for example) are in motion and thus
have a nonzero range rate. This will cause the Doppler frequency to have a small spread. The
spread is important because, as we will learn, it is a significant factor in the ability of some
signal processors, notably MTI processors, to reject the clutter.
Several models for the frequency spectrum of ground clutter have been proposed over the
years [2, p. 152]. A standard model used in many texts is the Gaussian model, defined by
where σf = 2σv/λ and σv is the velocity spread of the clutter, in m/s. Skolnik provides values of
σv for several environment and wind conditions [50]. A sampling of these is contained in
Table 13.2.
The spectrum currently believed to be the best for land clutter was developed by MIT
Lincoln Laboratories as part of an extensive clutter characterization effort [27, 39]. The
Lincoln Laboratories tests considered an environment that consisted of trees and “vegetation”
and gathered data at low elevation angles and several frequencies. A form of the model
presented in [51–53] is
where β is a parameter that depends on wind speed and r is a parameter that apportions the
spectrum between the spectral line at f = 0 and the spread part defined by the exponential. λ is
the wavelength and δ(f) is the Dirac delta. Table 13.3 contains values of β provided in [39,
51–53] for different wind speeds. J. Barrie Billingsley points out that the entries for the first
three wind conditions are based on measurements, but the entries for gale force winds are
estimates. We term (13.13) the exponential spectrum model in this book.
Table 13.2
Sample Values of σv
Environment
σv m/s
Sparse woods, calm winds
0.017
Wooded hills, 20-knot wind
0.22
Wooded hills, 40-knot wind
0.32
Source: [50].
Table 13.3
Clutter Spectrum Shape Parameters

Source: [39, p. 578].
Billingsley provides an equation for r, which is [39, p. 580]
where w is the wind speed in mph and fo is the radar carrier frequency in GHz.
The β values given above have not, to the authors’ knowledge, been extended to sea clutter.
Skolnik provides a related parameter that we might use to infer a β value for sea clutter [50].
Specifically, he provides a standard deviation parameter, σv, that is used in the Gaussian
clutter spectrum model. That parameter has the units of m/s, as opposed to the s/m units of β.
Skolnik provides value of σv = 0.22 m/s for “wooded hills” in a 20-knot wind (23-mph
wind). We note that 1/σv = 4.55 s/m, which is somewhat close to the value of 5.2 to 5.8 listed in
Table 13.3 for wind speeds between 15 and 30 mph. This, and a comparison of units, suggests
an inverse relation between β and σv. For sea clutter, Skolnik has values of σv that range
between 0.46 and 1.1 m/s. If we use the inverse relation between β and σv, we can speculate that
reasonable values of β for sea clutter might be 0.91 s/m to 2.2 s/m. We might further speculate
that the lower value corresponds to a high sea state and the larger value corresponds to a
lower sea state.
In addition to spectrum spread, sea clutter can also have a center value that depends on wind
velocity and its direction relative to the radar. Nathanson provides a chart2 that shows a mean
velocity of 3.4 m/s for sea state 4 and looking directly into the wind [30, p. 294]. He indicates
that, depending upon look direction, this mean velocity could vary anywhere between −3.4
m/s and 3.4 m/s depending on the direction of the wind relative to the beam direction.
An implication of characterizing the clutter spectral properties by C(f) is that the clutter is a
wide-sense stationary (WSS) random process [54, 55]. Also, since
for the clutter spectrum models of (13.12) and (13.13) (see Exercise 4), the clutter spectrum is
normalized to unity power. To get the actual clutter spectrum, we multiply C(f) by the clutter
power, which we compute from the clutter RCS and the radar range equation.
13.2.3 Rain Clutter RCS Model

Figure 13.3 contains a sketch of the geometry we use for the rain clutter model. We term the
volume of the elliptical cone frustum the main beam clutter volume, VCM, and use it to
compute the rain clutter RCS in the main beam (the main beam is represented by the two
slanted lines). If we assume the elevation and azimuth beamwidths αB and εB, are small, we can
treat the cone frustum as an elliptical cylinder and compute its volume as3
Similar to the ground clutter case, the rain RCS (due to rain in the main beam region) is
determined by multiplying VCM by a rain reflectivity, or backscatter coefficient, η. That is
Figure 13.3 Geometry for rain clutter model.
Table 13.4
Example Rain Reflectivity Values [dB(m2/m3)]
Barton provides an equation for η as

where r is the rainfall rate in mm/hr and λ is the radar wavelength in m [29, p. 341]. Several
examples of η based on this equation are given in Table 13.4.
Figure 13.4 contains plots of σC versus R for a rainfall rate of 4 mm/hr, the four
frequencies of Table 13.4, αB = εB = 2°, and ΔR = 150 m. Unlike ground clutter RCS, which
increased then decreased with increasing range, rain clutter RCS continues to increase as
range increases. This makes sense, as VCM increases with increasing range.
Two assumptions not previously mentioned are that the rain RCS model assumes the rain
occupies the entire main beam and that it is present at all ranges. The assumption that the rain
occupies the entire main beam may be reasonable for pencil beam radars since their
beamwidths are, at most, a few degrees. For fan beam radars, this assumption becomes
questionable. Also, David Barton noted that, in fact, the RCS would eventually decrease when
the top of the beam moves above the top of the rain. To account for this, he states that, in
(13.16), εBR should be replaced by hm − h0 where h0 = R2/2(4RE/3) is the altitude at which the
rain is at the horizon for the range R and hm is the maximum rain altitude. He states that hm is
on the order of a few kilometers. To avoid a negative area, we suggest using the maximum of
hm − h0 and zero.
The assumption that the rain is present at all ranges is also questionable. However, for
preliminary investigations, it is probably a reasonable assumption since it would represent a
worst-case scenario in terms of the clutter rejection the signal processor must provide.
Figure 13.4 Rain clutter RCS plots for 4 mm/hr rainfall rate.

Figure 13.5 Side view of hemispherical shell used to compute sidelobe rain clutter RCS.
Another assumption of the above formulation of σC is that it includes only main beam rain
clutter. In general, this is a good assumption. To support this claim, we consider a specific
example.
Suppose the antenna has a uniform sidelobe level of SL over a hemisphere of radius R
centered on the radar. Further suppose rain is present over the volume encompassed by the
hemisphere, and that the backscatter coefficient, η, is the same throughout the volume. We are
interested in the rain RCS in a hemispherical shell with a width of ΔR located at a range of R.
A cross section of the shell is depicted in Figure 13.5.
The approximate volume of the hemispherical shell is (see Exercise 6)
Thus, the RCS of the rain in the shell is
where we included SL to reflect the fact that the power (energy) returned from the rain in the
shell is in the sidelobes of the transmit and receive antenna radiation patterns.
The ratio of sidelobe to main lobe RCS is
As a specific example, we consider the case where the sidelobe levels of the transmit and
receive antennas are uniformly 30 dB below the main beam peak (SL = 10−3) and εB = αB = 2°.
With this, we get

In other words, the RCS of the rain clutter in the shell is well below the main beam RCS. We
note that (13.21) has assumed the rain exists at all altitudes and ranges, which is clearly not
realistic. Our only purpose for including it is to point out that, generally, the major
contributor to rain RCS is the rain in the main beam. If the geometry is such that the rain is not
in the main beam, the major contributor becomes rain in the sidelobes.
13.2.4 Rain Clutter Spectral Model
According to several sources [1, 30, 56], the spectrum width of rain clutter depends upon
several factors including wind shear, turbulence, fall velocity of the rain, and variation in the
fall velocity across the main beam. Of these, wind shear appears to be the main contributor to
the width of the spectrum. Fred Nathanson provides an equation for wind shear as4
where Rkm is range in km and k = 4.0 m/(sec-km) [30]. Nathanson states that this value of k is
averaged over all azimuths. He goes on to say that σvshear is limited to 6 m/s for elevation
beamwidths, εB, less than 2.5°. In (13.23), σvshear is the standard deviation of a Gaussian
spectrum model. Nathanson indicates that the other three contributors (turbulence, fall
velocity, and fall velocity variation), combined, are in the vicinity of 1.5 m/s.
Nathanson provides graphs of measured data that indicate the total rain velocity standard
deviation, σvrain, is between 0.5 and 1.5 m/s for ranges below about 20 km and between 2 and 3
m/s for ranges of about 60 km. In another chart corresponding to a high shear case, he shows
σvrain values that vary between 1 and 5 m/s, independently of range, over a 0- to 100-km
range. Barton [1] shows measured data for a 2-D search radar that indicates a σvrain of 5 m/s.
These values are summarized in Table 13.5.
σvrain is the standard deviation for a Gaussian spectrum model. Since there is no
justification for using an exponential spectrum model for rain, it should probably not be used.
Table 13.5
Sample Values of σvrain From Several Sources
σvrain m/s
Source
<6
Nathanson [30]
0.5 to 1.5 at 20 km range
Nathanson Graph [30]
2 to 3 at 60 km range
Nathanson Graph [30]
1 to 5 for ranges between 0 and 100 km
Nathanson [30]
5
Barton [1]
1.8 to 4
Skolnik [50]
Rain can have a mean velocity that depends on wind velocity and direction of the wind

relative to the beam direction. Nathanson has an example that shows peak mean velocities of
about 30 m/s at moderate altitudes [30, p. 294].
13.3 SIGNAL MODEL
We will evaluate the clutter rejection properties of signal processors using frequency domain
techniques. To do so, we need to develop equations for the signal, clutter, and noise spectra at
the input to the signal processor. As we will show, these spectra will depend not only on the
spectra of the signal, clutter, and noise, but also on other radar properties such as phase noise,
antenna scanning, the matched filter, and the sampling (or ADC) operation.
To develop the required spectrum models, we will trace a signal from its generation,
through the transmitter and antenna, to the target or clutter, back to the antenna, and through
the receiver, matched filter, and sampler (or ADC). As we will see, the various components
indicated above influence the spectrum at the input to the signal processor.
13.3.1 Signal Model Generation
A simplified block diagram of a radar transmitter and receiver is shown in Figure 13.6. The
block diagram contains only the elements essential to our development. Specifically, it does
not contain any of the IF amplifiers and filters, nor the mixers needed to upconvert and
downconvert the various signals, except for the STALO (STAble Local Oscillator), which we
need to include because of phase noise. We have not lost any generality with this technique
because we will use normalized, complex signal notation. This allows us to ignore IF
processes.
Figure 13.6 Transmitter, receiver, and signal processor.
Complex signal notation has an advantage of being easy to manipulate since the signals are
represented by complex exponentials rather than sines and cosines. Operations such as
filtering, sampling, and transforms are treated the same with complex signals and real signals.
We must take care when using complex signals in nonlinear operations such as mixing. For
example, in the transmit mixer of Figure 13.6 we used vLO(t), whereas on the receive mixer
we used its conjugate, v*LO(t). We knew we needed to do this based on real signal analyses.

Specifically, we performed real signal analyses and used the results to determine what
complex signal operations we needed to perform.
In Figure 13.6, vp(t) is the pulse train and is a complex, baseband signal. This means that its
energy, or power, is concentrated around 0 Hz, as opposed to some IF. As a note, signals that
have a Doppler frequency are usually considered baseband signals, even though their energy
is not truly concentrated around 0 Hz.
The typical vp(t) of interest is a sequence of rectangular pulses with a width of τp and a PRI
of T. vp(t) could consist of a burst of pulses or a semi-infinite string of pulses, depending on
the radar and the waveform. In an older, dish type of radar that tracks a single target, vp(t)
would consist of a semi-infinite string of pulses. In phased array, multifunction radars, vp(t)
would contain a burst of tens to hundreds of pulses.
A graphical representation of vp(t) (actually |vp(t)|) is shown in Figure 13.7.
Figure 13.7 Depiction of |vp(t)|.
In equation form, vp(t) is
where δ(t) is the Dirac delta and * denotes convolution. The summation notation denotes a
summation over the number of pulses that make up vp(t)—that is, the number of pulses in the
burst.
The form of vp(t) implies that the pulses are unmodulated. A more general form would be
where p(t) is a complex signal notation of a complicated waveform such as a phase-coded
pulse or an LFM pulse.
The STALO signal, vLO(t), is of the form
In (13.26), fc = ωc/2π is the carrier frequency. ϕ(t) is termed the phase noise [57–61] on the

STALO signal and represents the instability of the oscillator that generates the STALO signal.
As implied by its name, ϕ(t) is a random process and is such that exp[jϕ(t)] is WSS. We will
address phase noise later. Phase noise is included because it is often a limiting factor on
clutter attenuation capabilities of the signal processor.
In most radars, vLO(t) also includes an amplitude noise component such that
However, A(t) is usually made very small by the radar designer and is normally considered to
have a much smaller influence on signal processor performance than ϕ(t). For this reason, it
is almost always ignored in signal processor analyses. Having said this, it should be noted that
modern STALOs are becoming so stable that the amplitude noise may soon overtake phase
noise as the limiting factor in signal processor performance [62–68].5
vT(t) is the signal at the transmitter output and is given by
vS(t) is a term we include to account for the fact that the antenna may be scanning (which is
generally taken to mean the beam is rotating horizontally, as in a search radar). If we are
considering a tracking radar or a phased array radar that moves its beam in steps and
transmits a burst of pulses at each beam position, vS(t) = 1. A standard form of vS(t) for the
scanning case is [69, p. 134]
where
TSCAN is the scan period (in seconds) and αB is the azimuth beamwidth (in radians). If the
antenna is scanning in elevation instead of azimuth, we would replace αB with εB, the elevation
beamwidth. The form of (13.29) is based on the antenna pattern model of (13.4).
In practice, vS(t) is a periodic function with a period of TSCAN. However, since the time
period of interest in the signal processor is small relative to TSCAN, it is assumed the radar
beam scans by the target only one time. The time period of interest in the signal processor is
the coherent processing interval (or CPI).
vobj(t) is the object signal and is our means of capturing the power spectrum properties of
the clutter or target. vobj(t) is a random process and is assumed to be WSS.6 For clutter we use

vobj(t) = C(t) and for targets we use vobj(t) = T(t) where T(t) represents the target signal.
The spectrum of vobj(t) is given by
where
is the autocorrelation of vobj(t).
For clutter, we replace Vobj(f) by C(f) where C(f) is given by (13.12) or (13.13). For targets,
we replace Vobj(f) with
That is, we assume the target is represented by a single spectral line at the target Doppler
frequency of fd. As discussed in Chapter 3, we normally assume the target signal is a random
process. In this chapter, we further assume it is WSS.
To complete our definitions, vrec(t) is the received signal after it goes through the antenna,
vm(t) is the output of the receiver’s mixer, and vMF(t) is the matched filter output. vo(t) is the
sampled version of vMF(t) and is the signal that goes to the signal processor. The matched
filter is usually matched to a single pulse of the original pulse train, vp(t).
13.3.2 Signal Analysis
We now want to develop an equation for vo(t) and, ultimately, its power spectrum, So(f). We
start our analysis by noting that the mixer is a multiplication process. Thus, the signal sent to
the antenna is
If the antenna is scanning, its pattern modulates the amplitude of vT(t). We model this as a
multiplication of vT(t) by vS(t). Thus, the signal that leaves the antenna is
Recall that we set vS(t) = 1 if we consider the tracking problem or a phased array where the
beam is fixed during the CPI.
After the signal leaves the antenna, it propagates a distance of R to the object (clutter or

target). We represent this propagation by incorporating a delay, which we denote as τd/2, into
vantT(t). We should also include an attenuation that depends on R. However, we will ignore it
for now. We will consider the actual power in the clutter and target signal at a later time.
With the above, the signal that arrives at the object is
The object “reflects” the signal back to the radar and imposes its temporal, and spectral,
characteristics on the reflected signal. We represent this operation by multiplying vCT(t) by
vobj(t), the function that we use to represent the temporal properties of the object. We represent
the operation by multiplication because the interaction of the signal with the clutter (or target)
is essentially a modulation process. We learned this in Chapter 1 when we found the motion of
a target caused a shift in the frequency of the signal (Doppler shift) and the amplitude of the
return signal was a function of the target RCS [70].
The signal reflected by the object is
and the signal at the receive antenna is
This signal next picks up the scan modulation and is then heterodyned by the receiver mixer
to produce the matched filter input, vm(t). In equation form,
We now want to study and manipulate this equation. We start by simplifying the equation
and making some approximations. Since the antenna will not move much over the round trip
delay, τd, we can assume vS(t) does not change much over τd. This means vS(t − τd) ≈ vS(t).
With this we get
The output of the matched filter is

where m(t) is the matched filter impulse response. Finally, the signal sent to the signal
processor is
That is, vo(t) is a sampled and held version of vMF(t). As a note, in practice, many sets of
vMF(kT) will be sent to the signal processor—one set for each range cell of interest.
The next step in the development is to manipulate (13.40) through (13.42) to eventually
derive an equation for So(f). The development is very interesting, but also tedious and lengthy.
As a result, we have moved it to Appendix 13A. We present the final results here and use them
to begin our signal processor analyses.
The spectrum input to the signal processor is
MF(f) is the matched-range, Doppler cut of the cross ambiguity function of p(t) and q(t),
the signal to which the matched filter, m(t), is matched (see Appendix 13A). Specifically,
Typically, q(t) = p(t). For uncoded pulses, phase coded pulses, and LFM pulses that do not
incorporate weighting for range sidelobe reduction, MF(f) is of the form
where τp is the uncompressed pulsewidth.
From Appendix 13A,
where
and

The overbar in (13.47) denotes the averaged autocorrelation of r(t) and is necessary because
the (generally) periodic nature of v2S(t) makes r(t) wide-sense cyclostationary (WSCS)
instead of simply WSS (see Appendix 13A and Appendix 13B).
We can reasonably assume the random processes vobj(t) and Φ(t) are independent because
the statistical properties of one has no influence on the statistical properties of the other. With
this, we can write
where Robj(τ) is given by (13.32)
and
is the averaged time autocorrelation of v2S(t).
Since Rr(τ) is a product of autocorrelations, Sr(f) is the convolution of their associated
spectrums. Thus
The scanning function spectrum, VS(f), is of the form [1, 69]
where
If the radar antenna is not scanning, VS(f) reduces to
For clutter, we replace Vobj(f) by C(f), where C(f) was given earlier as [see (13.12) and

(13.13)]
for the Gaussian spectrum model and
for the exponential spectrum model. For targets, we replace Vobj(f) by the target spectrum
T(f), where
and fd is the target Doppler frequency.
Φ(f) represents the phase noise spectrum of the radar. As shown in Appendix 13A, we can
write
where ∆ϕ(t) is the total transmit and receive phase noise of the STALO. We note that ∆ϕ(t) is
small relative to unity and write [57]
With this we get the autocorrelation of vPH(t) as
where we made the tacit assumption that ∆ϕ(t) is zero-mean and WSS.
From (13.61) we get the power spectrum of vPH(t) as
If we assume ∆ϕ(t) is white, we get

where Φ0 is termed the phase noise sideband level. Later we will consider phase noise models
where ∆ϕ(t) is not white and investigate other forms of Φ∆ϕ(f).
Φ0 is caused by noise in the STALO circuitry. In dB terms, it has the units of dBc/Hz, which
means dB relative to the power in the carrier of the radar, measured in a 1-Hz bandwidth.
Rough estimates for Φ0 are −125 to −150 dBc/Hz for radars that use STALOs that employ
very narrowband filters or phase-locked loops (such as klystron-based STALOs), −110 to
−130 dBc/Hz for radars that use frequency multiplied crystal or digitally synthesized
STALOs, and around −90 dBc/Hz for radars that use magnetron transmitters. To repeat, these
values are rough estimates because the field of STALOs is moving rapidly, especially crystal-
based STALOs. Modern, well-designed radars that use good STALOs have phase noise values
in the vicinity of −125 to −135 dBc/Hz. Some advanced radar designs appear to be pushing
phase noise to −150 to −160 dBc/Hz.
If we ignore phase noise, Φ(f) reduces to
δ(f) is the center spectral line, or carrier, and represents a pure sinusoid.
The power at the output of the sampler, which is also the input to the signal processor, is
given by (see Appendix 13A)
If we reverse the order of summation and integration, we get
In each of the integrals, we make the change of variables α = f − l/T to get
We recognize (13.67) as an infinite sum of nonoverlapping integrals, which we can write as
or, using (13.52),

where we changed the variable of integration from α back to f. As a note, because power is
preserved in the sampling process, Po is also the power at the output of the matched filter.
Po, as defined by (13.69), is a normalized power because each of the spectra in the brackets
has an area of unity [if we ignore the phase noise part of Φ(f)]. To properly scale this power
for clutter and targets, we need to associate their respective powers with their spectra. If we do
this, we get clutter and target power at the matched filter output (which is the same as the
power at sampler output and the input to the signal processor) as
and
where, for our purposes, PC and PS are scaling factors related to the clutter and target RCS
and the various terms of the radar range equation. We assign values to these scaling factors
based on SNR and CNR, which we can compute using the radar range equation.
We can write the SNR and CNR at the output of the matched filter as
Assuming we know PNo, we can solve (13.72) for PSo and PCo. Further, for the ideal
conditions where we assume T(f) = C(f) = δ(f), the antenna is not scanning, and we ignore
phase noise (all of which are tacit assumptions we make when we use the radar range
equation), the integrals of (13.70) and (13.71) are unity (see Exercise 7). From this and
(13.72), we have
This gives us a means of scaling clutter and signal power relative to some arbitrary noise
power through the radar range equation.
13.4 SIGNAL PROCESSOR ANALYSES
13.4.1 Background

Now that we have equations for clutter and target spectra at the input to the signal processor,
we turn our attention to considering how to use them to perform signal processor analyses.
We will consider sampled data signal processors. This could include signal processors that
use analog components or signal processors that are implemented with digital hardware. The
characteristic that dictates we use sampled data techniques is the assumption that we are
dealing with pulsed radars. Because of this, the signals into the signal processor are sampled
once per PRI. The sampling can be performed by a sample-and-hold device for processors
that include analog devices, or an ADC for processors that use digital signal processing. As a
note, the sampler (sample-and-hold or ADC) gathers several samples within every PRI [71,
72]. It gathers one sample for each range cell to be processed, and the signal processor is
replicated for every range cell that is processed. In some cases, notably with analog
processors, the signal processor replication may require replication of hardware. For
example, if Nrngcell range cells are processed, Nrngcell signal processors may be needed. In
digital implementations, the replication is accomplished by time multiplexing. That is, a
single digital signal processor sequentially processes the signals from each range cell.
We initially consider digital signal processors and assume we have a signal processor with
a z-transfer function of H(z) and equivalent frequency response of
We are using the form of frequency response usually used in analyzing random processes
because, by assumption, our clutter (and target) signal is a random process at the signal
processor input.
The standard way of performing digital signal processor analyses in the frequency domain
is to find So(f) from (13.43) [with the appropriate Sr(f)], multiply it by H(f), and integrate the
result over (−1/2T, 1/2T] to find the power at the output of the signal processor. Recall that we
use this approach because, for digital signals, the only valid frequency region is (−1/2T,
1/2T].
As we did to find Po, we propose a different approach. Rather than use So(f) over (−1/2T,
1/2T], we use SMF(f) = MF(f)Sr(f) over (−∞,∞). We also use H(f) over (−∞,∞). As before, we
multiply these and integrate to find the power; except this time we integrate over (−∞,∞). With
this approach, we are “unfolding” So(f) and H(f) and then “refolding” them when we find the
power. This approach has the advantage of avoiding the So(f) summation of (13.43).
We digress to show that the approach we propose is valid in terms of computing the power
out of the signal processor. We start by noting that the power at the output of the signal
processor is (see Appendix 13A)

We substitute for So(f) and bring H(f) inside of the sum to yield
We note H(f) is periodic with a period of 1/T. This allows us to replace H(f) with H(f − l/T)
since H(f − l/T) = H(f). Doing this, and reversing the order of summation and integration,
results in
In each of the integrals of the sum, we make the change of variables α = f − l/T to get
Finally, we recognize the above as an infinite sum of nonoverlapping integrals, which we can
write as a single integral over (−∞,∞). That is,
which is the desired result. Note that we changed the variable of integration from α back to f.
13.4.2 Moving Target Indicator (MTI)
We are now ready to consider our first signal processor: a moving target indicator, or MTI.
An MTI is a highpass digital filter designed to reject clutter, but not targets that are moving. A
block diagram of a two-pulse MTI is shown in Figure 13.8. It is termed a two-pulse MTI
because it operates on two pulses at a time. It successively subtracts the returns from two
adjacent pulses. For signal processor buffs, it is a first-order, nonrecursive, highpass, digital
filter.
The block with z−1 represents a one PRI delay. In modern MTI processors, the delay is
implemented using digital memory. In older MTI processors, it was implemented using delay
lines.
A time domain model of the filter is

Note that if vo(k) = K, then vSP(k) = vo(k) − vo(k − 1) = K − K = 0. Thus, the MTI perfectly
cancels DC, or zero-frequency signals.
Taking z-transform of both sizes of (13.80), we get
which we solve to yield the filter transfer function
Figure 13.8 Two-pulse MTI.
where we use the subscript U to denote the fact that the filter transfer function is
unnormalized. We will discuss normalization of the MTI shortly. From (13.82) we find the
filter frequency response as
A plot of HU(f) is shown in Figure 13.9 for the case where T = 400 µs.
13.4.2.1 MTI Response Normalization
Before we turn our attention to computing the clutter rejection capabilities of an MTI, we need
to normalize the MTI response to something. Without normalization, it is difficult to quantify
the clutter rejection capabilities of the MTI because we have no reference. The instinct is to
say the clutter rejection is a measure of the clutter power out of the MTI relative to the clutter
power into the MTI. However, we can make this anything we want with the appropriate MTI
gain. To avoid this problem, we normalize the MTI so that it has a noise gain of unity. In this
way, we can compute the clutter rejection by comparing the CNR at the output of the MTI to
the CNR at the input, since we have noise power as a common reference. In a similar fashion,
we will be able to characterize the SNR improvement, or degradation, through the MTI.

Figure 13.9 Frequency response of an unnormalized two-pulse MTI.
Figure 13.10 Normalized frequency response of a 2-pulse MTI.
We assume the noise into the MTI, n(k), is zero-mean and white with a power of PNo =
E{|n(k)|2}, add a gain, KMTI, to (13.80), let vo(k) = n(k), and write
As a reminder, PNo is the noise power at the sampler, and matched filter, output. The noise
power at the MTI output is
In (13.85), the cross expectations on the third line are zero because of the assumption that n(k)
is white and zero-mean. The relation E{KMTI2|n(k)|2} = E{KMTI2|n(k − 1)|2} comes from the

assumption that n(k) is WSS. From the above, note that, for PNout = PNo, we require 
. If we apply this to our previous derivation of H(f), we get
A plot of the normalized H(f) is shown in Figure 13.10.
13.4.2.2 MTI Clutter Performance
Now that we have normalized the MTI response, we want to compute its clutter attenuation
and signal-to-clutter ratio (SCR) improvement. SCR is the ratio of signal (target) power to
clutter power. The IEEE Dictionary defines what we term SCR improvement as the MTI
improvement factor [29, 73].
We start with clutter attenuation, which is defined as the ratio of the CNR at the input to the
MTI to the CNR at the output of the MTI. The CNR at the input to the MTI is the CNR at the
output of the sampler (and matched filter) and is given by the radar range equation. The CNR
at the output of the MTI is the clutter power out of the (normalized) MTI divided by the noise
power at the output of the MTI. However, the noise power at the output of the MTI is equal to
the noise power at the input. Thus, the clutter attenuation is the ratio of the clutter power at the
input to the MTI and the clutter power at the output of the MTI. In equation form
where the last equality follows from (13.73).
The clutter power at the output of the MTI is
where SCr(f) is [see (13.52) and (13.70)]
With this
Comparing (13.90) to (13.87), we have

This means we only need compute
The form of (13.92) does not lend itself to a simple closed form solution. However, we can
obtain approximate closed form solutions by making some assumptions about VS (f), C(f) and
Φ(f). The first assumption is: we temporarily ignore phase noise and let Φ(f) = δ(f). We will
include phase noise later.
We will derive approximate values for CA for the Gaussian and exponential clutter spectra
models of Sections 13.2.2 and 13.2.4. The Gaussian model leads to the CA formulation that
appears in most radar texts that discuss MTI [1, 2]. We will also use the VS(f) of (13.53). With
this, we have
where 
. If the radar is not scanning, VS(f) = δ(f) and we would use σT = σf. We
made use of the fact that the convolution of two Gaussian functions is another Gaussian
function [54].
Substituting (13.93) into (13.92) gives
It can be shown (see Exercise 8) that
for typical values of σT and τp. With this, we can simplify (13.94) to

This integral does not have a simple, closed-form solution. However we can simplify the
integral by observing that, over the region of f where exp[−f2/(2σT2)] is large, πfT is small
and sin(πfT) ≈ πfT. Over the rest of f, exp[−f2/(2σT2)] is very small. Thus, the integrand is very
small and adds little to the value of the integral (see Exercise 9). With this, GCGauss becomes
[28]
From properties of Gaussian density functions [54], we recognize the term in parentheses as 
 and write
From (13.91)
which is the form found in many radar texts [1, 2].
For the exponential model, we use (13.13) and approximate VS(f) by a similar exponential
model.7 Specifically,
With this, we have
In (13.100) and (13.101), we used βS = 2/(λσS) and βC comes from Table 13.3. The derivation
of (13.101) is left as an exercise. As a reminder, we are ignoring phase noise so Φ(f) = δ(f).
We next use (13.101) in (13.92) with H(f) = 2sin2(πfT) to compute GCexp. While the resulting
integral can be evaluated in closed form, the closed form expression is somewhat
complicated. A simpler form of the integral can be obtained by using sin(πfT) ≈ πfT. The
result is (see Exercise 11)

For the nonscanning case, this reduces to
The clutter attenuation is
We next examine SCR improvement, which is defined as the SCR out of the MTI divided by
the SCR into the MTI, averaged over all Doppler frequencies of interest. Averaging is needed
because the signal power out of the MTI will depend upon the target Doppler frequency. As a
result, the SCR improvement will be a function of Doppler frequency, which is cumbersome.
To compute the signal power at the MTI output, we use an equation of the form of (13.90)
but replace C(f) with T(f) and PC with PS. The result is
For target signals, we can ignore scanning and phase noise. We do this by using VS(f) = δ(f)
and Φ(f) = δ(f). With this and T(f) = δ(f − fd) (13.105) becomes
In most situations we can assume MF(fd) = 1 for Doppler frequencies of interest.8 With this
which means the signal power at the output of the MTI depends on Doppler frequency. This
says the SNR improvement is a function of Doppler frequency and cannot be conveniently
represented by a single number, as we would like. To circumvent this inconvenience, PSout is
averaged over Doppler frequencies of interest.
From (13.86), the average of H(fd) is unity, as is the noise gain. Because of this, the average
SNR gain through the MTI is unity. That is, SNRout = SNRin. With this, and the clutter
attenuation results from above, we get

That is, the (average) SCR improvement is equal to the clutter attenuation. We note that the
peak SCR improvement is KMTI2CA, or 2CA in this case, since the peak SNR gain through the
MTI is KMTI2 W/W.
13.4.2.3 Example 1
For this example, we consider radar with a carrier frequency of 8 GHz and a PRI of 400 µs.
We assume ground clutter that consists of wooded hills in a 20-knot wind and use σv = 0.22
m/s (Table 13.2) for the Gaussian spectrum model. From this, we compute the frequency
spread as
For the exponential spectrum model, we assume a 20-knot wind represents a windy
condition and use βC = 5.5 s/m from Table 13.3. With a change of units from knots to mph, we
get a wind speed of w = 23 mph and use this in (13.14) to compute
We first assume the case where the radar beam is stationary during the CPI, which we
accommodate by VS(f) = δ(f). With this we get σT = σf = 11.7 Hz. The clutter attenuation and
SCR improvement using the Gaussian spectrum model is
For the exponential spectrum model, we get
which is very close to the value we obtained with the Gaussian spectrum model.9
As an extension, we assume the same radar and clutter parameters but use a scanning radar
that has a two-second scan period. We use the beamwidth associated with the example of
Figure 13.2 (i.e., αB = 2°). From (13.54), we have

For the Gaussian spectrum model, we get a total spectrum width of
It is interesting to note that scanning is the major contributor to the frequency spread. For the
exponential spectrum model, we get βS = 2/(λσS) = 2.24 s/m. In this equation, we used the
inverse relation between β and σv.
The resulting clutter attenuation for the Gaussian spectrum model is
For the case of the exponential spectrum, we use (13.102) and (13.104) to obtain
In this case, the clutter attenuation using the Gaussian spectrum model is slightly larger than
with the exponential spectrum model.
Let us carry this example further and examine SNR, CNR, and SIR. SIR is the acronym for
signal-to-interference ratio and is defined as
It is the ratio of the signal power to the total interference power. In a clutter environment, SIR
is the parameter used to evaluate detection and tracking performance. For a target to be
detected, the signal power must be greater than the total interference power by some margin
(i.e., the detection threshold). Since SIR is a measure of signal power to total interference
power, it is the quantity that should be used. The same argument applies to tracking
performance.
In addition to the aforementioned parameters, we assume the additional radar parameters
listed in Table 13.6.
Using the parameters of Table 13.6, the SNR, CNR, and SIR versus R at the sampler (and
thus matched filter) output is as shown in Figure 13.11.
Table 13.6
Radar Parameters for Example 1
Peak power
50 kW

System noise temperature
1,000 K
Pulsewidth
4 µs (a 4-chip, Barker coded pulse)
Total losses for the target and clutter
10 dB
Height of the antenna phase center
5 m
Azimuth and elevation beamwidth
2°
Antenna directivity on transmit and receive
38 dB
rms antenna sidelobes
−30 dB
Clutter backscatter coefficient
−20 dB
Target RCS
6 dBsm
Ranges of interest
2 km to 50 km
Figure 13.11 SNR, CNR, and SIR at matched filter output.
The SNR is reasonable, but the SIR is too low to support detection and track. (The hook in
the CNR plot is caused by the fact that we assumed the radar beam was pointed at the target,
rather than at a fixed angle, as was the case for the plots of Figure 13.2.) Also, the Gaussian
antenna pattern model [see (13.4)] was used in the clutter RCS generation routine.
Figure 13.12 contains plots similar to those of Figure 13.11 for the two cases (nonscanning
and scanning) where an MTI is used. Since the clutter attenuation was almost the same for the
two spectrum models, only one set of plots is shown. For the nonscanning case, the MTI
provided enough clutter attenuation to give an SIR that remained above 13 dB (a value we
used for detection threshold in previous examples) for ranges below 50 km. For the scanning
case, the clutter attenuation was not quite adequate, and the SIR dipped to fairly low values at
short ranges. This indicates that it might be necessary to consider a higher order MTI, with
the hope that it will provide better clutter attenuation and, thus, SIR improvement.

Figure 13.12 SNR, CNR, and SIR at MTI output.
As an extension to this example, we examine the behavior of the radar in rain clutter. We
use the Gaussian spectrum model with σv = 3 m/s. This velocity spread is an intuitive average
of the values given in Table 13.5. We compute the rain clutter RCS from (13.17) and (13.18)
for a rainfall rate of 4 mm/hr and a carrier frequency of 8 GHz (η = −66 dB(m2/m3). We
assume the mean velocity of the rain is zero. We also ignore phase noise and assume a
stationary beam.
To determine the clutter attenuation and SCR improvement, we use (13.99) with σT = 2σv/λ =
160 Hz. This gives

which is clearly not very large. Figure 13.13 illustrates the impact of the small value of CA.
The top plot corresponds to the case where the MTI is not used, and the bottom plot
corresponds to the case where the radar uses a 2-pulse MTI. As can be seen, the SIR is
unacceptably low in both cases.
13.4.2.4 Phase Noise
We next examine the impact of phase noise on the MTI clutter attenuation and SCR
improvement. We use Φ(f) = δ(f) + Φ0 for the phase noise (see Section 13.3.2). With this, we
get
where we took advantage of
With this, we get
where GC is given by (13.98), (13.102), or (13.103) depending of the clutter spectrum model
and whether or not the antenna is scanning. Derivation of the second term is left as an
exercise. The resulting cutter attenuation is
To get an idea of the impact of phase noise on the performance of MTI signal processors,
we revisit the previous example and plot clutter attenuation versus phase noise level, Φ0. This
plot is shown in Figure 13.14 for scanning and nonscanning cases.

Figure 13.13 SNR, CNR, and SIR at MTI input (top) and output (bottom)—rain clutter.

Figure 13.14 Phase noise effects on MTI clutter attenuation.
For the nonscanning case, the phase noise starts to degrade the clutter attenuation at a phase
noise level of about −95 dBc/Hz. For the scanning case, the phase noise degradation is
delayed until a phase noise level of about −85 dBc/Hz. The reason for this difference is due to
the relative sizes in the denominator of (13.122). If GC is small (meaning the clutter
attenuation without phase noise is large), the Φ0/τp term begins to predominate the overall
clutter attenuation for relatively small values of Φ0. However, if GC is large (meaning the
clutter attenuation without phase noise is small), Φ0 must be fairly large before it begins to
predominate the overall clutter attenuation.
13.4.2.5 Higher Order MTI Processors
In the previous example, we found the 2-pulse MTI did not provide sufficient clutter
attenuation to mitigate rain clutter. This leads to the question of how much clutter attenuation
could we obtain if we use a 3-pulse, 4-pulse or even higher order MTI. Or alternately, what
order MTI is needed to obtain a desired clutter attenuation.
To obtain an NMTI-pulse MTI we cascade NMTI − 1, 2-pulse MTIs. Specifically, if the
transfer function of a 2-pulse MTI is H(z), the transfer function of an NMTI-pulse MTI is
where the constant KNMTI is used to normalize HNMTI(z) to provide unity noise gain.
The specific transfer functions for 2-, 3-, 4-, and 5-pulse MTIs are [28]

Note that the coefficients of the powers of z are binomial coefficients with alternating signs
[31].
Following the method we used for the 2-pulse MTI, we can compute the MTI gain as
where the bm are the binomial coefficients indicated above. Specific values of K2NMTI for the
2-, 3-, 4- and 5-pulse MTI are summarized in Table 13.7. K2NMTI for an NMTI-pulse MTI with
binomial coefficients is
where (2m − 1)!! = 1 × 3 × 5 × ··· × (2m − 1) and (2m)!! = 2 × 4 × ··· × 2m, (0)!! = 1.
Table 13.7
 for Various-Size MTIs
MTI Order—NMTI
2
1/2
3
1/6
4
1/20
5
1/70
NMTI
If we extend the results of the 2-pulse analysis, we can write the normalized frequency
response of an NMTI-pulse MTI as

Figure 13.15 contains plots of the normalized frequency responses of 3- and 4-pulse MTIs.
Note that the peaks of the response become narrower, and the valleys become wider as the
order of the MTI increases. This means we should expect higher clutter attenuation and SCR
improvement as the MTI order increases.
We can compute the clutter attenuation for the general NMTI-pulse MTI by extending the
work we did for the two pulse MTI. For the Gaussian spectrum model, we again use the
approximation that sin(πfT) ≈ πfT. With this, we get
where GCGauss now becomes
Figure 13.15 Normalized frequency response of a 3- and 4- pulse MTI.
Evaluation of this integral yields [54]

We can write the clutter attenuation as [28]
As with the 2-pulse MTI case, we can show that the MTI gain, averaged across all expected
target Doppler frequencies, is equal to unity. With this, the SCR improvement, as before, is
Specific values of CAGauss and IscrGauss for a 3- and 4-pulse MTI are
and
Table 13.8 contains values of CAGauss for the nonscanning, scanning, and rain cases of
Example 1. The clutter attenuation is large for the case of ground clutter. However, even the 4-
pulse MTI does not provide adequate clutter attenuation for rain clutter.
Table 13.8
Clutter Attenuation for Gaussian Spectrum Model—dB
An equation for the clutter attenuation and SCR improvement for higher order MTIs and
the exponential spectrum is

where
The derivation of (13.136) is tedious, but straightforward, and left as an exercise.
13.4.2.6 Staggered PRIs
Examination of the MTI frequency response plots of Figures 13.10 and 13.15 indicates that the
SNR gain through the MTI can vary considerably with target Doppler frequency. This is
quantified in Figure 13.16, which is a plot of the percent of time the MTI gain is above the
value indicated on the horizontal axis. For example, the MTI gain is above −5 dB 73% of the
time for the 2-pulse MTI, and 60% and 52% of the time for the 3- and 4- pulse MTIs. If we
say, arbitrarily, that the MTI is blind when the gain drops below -5 dB, we can say the 2-pulse
MTI is blind 27% of the time, and the 3- and 4-pulse MTIs are blind 40% and 48% of the time.
Figure 13.16 Percent of time MTI gain above x-axis levels.
Figure 13.17 Two-position stagger waveform.

We can improve this situation by using staggered PRIs. That is, we use waveforms where
the spacing between pulses changes on a pulse-to-pulse basis. With staggered PRIs, we “break
up” the orderly structure of the MTI frequency response and “fill in” the nulls. We also reduce
the peaks in the frequency response. The net effect is to provide an MTI frequency response
that has fewer deep nulls and large peaks but, rather, a somewhat constant level. The response
still has the null at zero frequency and still provides clutter rejection.
To analyze the frequency response of an MTI with a staggered PRI, we start by examining
the output of the sampler for the staggered PRI waveform shown in Figure 13.17. We assume
the sampler samples the matched filter output at a delay of τd, after each pulse. Thus, the
sampler samples the output at τd, τd + T1, τd + T1 + T2, τd + T1 + T2 + T1, and so forth. We
further assume τd is such that we are sampling the matched filter output on the peak of its
response to a target return.
If the target return consists of a (complex) sinusoid at a Doppler frequency of fd, we can
write the sampler output for the kth PRI as
where
and PRIl is the interpulse spacing of the lth PRI interval. For the waveform of Figure 13.17,
PRI0 = 0, PRI1 = T1, PRI2 = T2, PRI3 = T1, PRI4 = T2, and so forth.
For the 2-pulse MTI we have, from (13.80), with the addition of KMTI
The output of the MTI after the first two pulses is
After the second two pulses, the output is

In general, after the kth pair of pulses the output will be
We can extend this to an NMTI-pulse MTI and write
where bl are binomial coefficients defined by
We would start computing vSP(k) for k = NMTI − 1 to allow time for the MTI transients to settle
(more on this shortly).
If we were to plot |vSP(k)|2 versus k, we would find that it is not constant, as was the case for
the unstaggered waveform. To smooth the variation with k, we average the |vSP(k)|2 over
several k. That is, we form
Figure 13.18 Block diagram of an MTI with output power averaging.
K is determined by the waveform and MTI order. We will discuss this shortly. We added fd as
an argument of VSP to recognize that it depends on Doppler frequency. As a note, the
exponential on the outside of the sum of (13.143) goes away when we form |vSP(k)|2.
Because of the KNMTI normalization, VSP(fd) is power gain of the MTI at a frequency fd.
That is

The averaging process just discussed is used in actual MTI implementations. This is
illustrated in Figure 13.18.
If the waveform consists of a burst of Npulse pulses and the MTI is of order NMTI, a typical
value of K is
This requires Npulse ≥ NMTI. We average the MTI output through the burst and record a single
output at the end of the burst. If the waveform consists of a semi-infinite string of pulses, a
rule of thumb is to choose K as the length of the PRI sequence (this applies to higher order
MTIs and staggered PRI waveforms). For example, suppose a sequence of PRIs was T1, T2,
T3, T2, T1, T2, T3, T2, and so forth. This sequence repeats every 4 PRIs. That is, the PRI
sequence is T1, T2, T3, T2 and has a length of 4. We would thus choose K = 4.
We can assemble the above into an algorithm for generating MTI frequency responses or,
as they are commonly termed, MTI velocity responses. The latter name derives from the fact
that we normally plot H(fd) versus v where v = λfd/2. The algorithm is
• Identify the number of pulses, Npulse, in the burst, along with their PRIs.
• If the waveform is semi-infinite (semi-infinite burst of pulses) use Npulse = NPRI + 1 where
NPRI is the number of PRIs in the PRI sequence.
• Compute the Tk using (13.138) for k = 0 to Npulse − 1.
• Select the MTI order, NMTI.
• Compute vSP(k) using (13.143) for k = NMTI − 1 to Npulse − 1 (without the exponential in
front of the summation).
• Compute H(fd) using (13.146) with K as discussed in the previous paragraph.
• Repeat the above steps for the fd (or v) of interest.
As an example, we consider a burst of Npulse = 10 pulses with repeating PRIs of T1 = 385 µs
and T2 = 415 µs. We consider a 3-pulse MTI so NMTI = 3. This gives K = Npulse − NMTI + 1 = 8,
so we need to compute 8 values of vSP(k) using10
with
for k = NMTI – 1 to Npulse – 1 or 2 to 9. We then average the 8 values of |vSP(k)|2 to get H(fd)

[see (13.146)]. The result of this is shown in Figure 13.19. In the figure, the horizontal axis is
range rate and was computed using the conversion λfd/2 with λ = 0.0375 m (8-GHz RF).
Actually, we started with range rates and computed the Doppler frequencies from them.
The response with the staggered waveform still has a considerable variation in MTI gain as
a function of range rate because we only used a two-position stagger (a PRI sequence repeats
after two PRIs). This can often be improved by using more than two values of Tk. That is, a
higher position stagger with more interpulse periods. Skolnik discusses this in his Radar
Handbook [31].
Figure 13.19 3-pulse MTI response with stagger.
To determine the clutter attenuation of an MTI with a staggered waveform, we use the same
formulas as for the unstaggered case. To find the SNR gain through the MTI, we find the
average signal gain from the MTI frequency response (e.g., Figure 13.19) and use this as the
SNR gain. We can do this because the MTI is still normalized and provides unity noise gain.
We often find the average MTI gain via the “eyeball” method; we estimate it from the plot. A
better method would be to numerically average the gain (in W/W) across the range rates of
interest. The MTI gain indicated via the “eyeball” method for the response of Figure 13.19 is
about 0 dB. The calculated gain is −0.08 dB.
13.4.2.7 MTI Transients
In the previous section, we noted that we would not use the MTI output until it had processed
NMTI pulses. That is, until k = NMTI – 1 (recall, k starts at 0). We do this because the MTI is in
its transient phase for the first NMTI – 1 pulses. If the input contains clutter, the clutter rejection
of the MTI will not be realized until after the transient. As an illustration of this, consider a 3-
pulse MTI where the input is a sequence of ones. That is, for k ≥ 0, vo(k) = 1 and for k < 0,

vo(k) = 0. By using these values in (13.148) we note that vSP(0) = K3MTI, vSP(1) = −K3MTI,
vSP(2) = 0, vSP(3) = 0, and so forth. That is, the output does not settle to zero until k = 2 = NMTI
– 1. To avoid having the transient affect detection and tracking functions that use the MTI
output, the MTI output is usually gated off during the transient period.
13.4.3 Pulsed Doppler Processors
The exact origin of the phrase “pulsed Doppler” is not clear. It probably derives from early
pulsed Doppler radars, which performed CW processing using pulsed waveforms.
Specifically, classical CW radars work primarily in the frequency (and angle) domain,
whereas pulsed radars work primarily in the time (and angle) domain. It is assumed that the
phrase pulsed Doppler was coined when designers started using pulsed radars that worked
primarily in the frequency, or Doppler, domain. Early pulsed Doppler radars used a 50% duty
cycle pulsed waveform and had virtually no range resolution capability, only Doppler
resolution. The use of a pulsed waveform was motivated by the desire to use only one antenna
and to avoid isolation problems caused by CW operation. Modern pulsed Doppler radars are
actually low-, medium-, or high-PRF pulsed radars with typical duty cycles in the 5% to 10%
range. They are used for both range and Doppler measurement.
Three classes of pulsed Doppler waveforms have evolved over the years.
1.
The “classical” pulsed Doppler waveform has a high PRF and operates ambiguously in
range, but is unambiguous in Doppler. High-PRF (HPRF) waveforms have PRFs in the
approximate range of 50 to over 100 kHz and pulsewidths in the range of 0.5 to 2 µs.
2.
Medium PRF (MPRF) pulsed Doppler waveforms are ambiguous in both range and
Doppler. These waveforms have PRFs in the approximate range of 10 to 50 kHz and
pulsewidths in the range of 2 to 10 µs. In some instances, the pulses of (MPRF)
waveforms are phase modulated to improve range resolution and reduce clutter power
entering the signal processor.
3.
Low-PRF (LPRF) waveforms are unambiguous in range and ambiguous in Doppler.
LPRF waveforms have PRFs in the range of 1 to 10 kHz and pulsewidths in the range of
10 to 100 µs. LPRF waveforms almost always use phase modulated pulses to provide
adequate range resolution and energy, and reduce clutter power entering the signal
processor.
When we say the waveform is ambiguous in range, we mean the PRI is shorter than target
ranges of interest. When we say the waveform is ambiguous in Doppler, we mean the PRF is
smaller than the target Doppler frequencies of interest.
Some of the benefits of using pulsed Doppler waveforms in a radar are:
• The waveform can be used for detection of short- and long-range targets without the need
to change pulsewidths to maintain sufficient energy and counter blind range (recall that a
radar is “blind” to targets whose range is less than the radar pulsewidth).
• Pulsed Doppler processors can directly measure range rate by measuring Doppler
frequency. This can be helpful in tracking and mitigating ECMs such as range-gate pull

off (RGPO) [74–76].
• Pulsed Doppler processors are Doppler selective in that they can be designed to reject
returns not at the target Doppler frequency. Because of this, they are capable of mitigating
clutter whose Doppler frequency is not zero, such as rain and chaff.
• Pulsed Doppler processors can provide both range and frequency information to the
operator or computer. This can be used to detect and counter separating targets or various
types of pull-off ECM such as RGPO, velocity deceptive jamming, or range and velocity
deceptive jamming.
Some of the myths, or misconceptions, associated with radars that use pulsed Doppler
waveforms are:
• They are better at rejecting ground clutter. From a radar system perspective, this is not
totally correct. Pulsed Doppler processors (usually) provide higher SCR improvement
than radars with MTI processors. However, with MPRF and HPRF waveforms, the SCR at
the processor input is much lower than with waveforms used with MTI processors. This is
because, in range ambiguous pulsed Doppler radars, the target must compete with clutter
at much shorter ranges. With LPRF waveforms, the target competes with clutter at the
target range.
• Pulsed Doppler radars are less susceptible to noise jamming. This is not correct for
broadband noise. Mitigation of broadband noise depends on the ratio of the target and
jamming energy at the radar receiver input. This is not changed by the signal processor.
Pulsed Doppler waveforms could help mitigate noise jamming if the jammer bandwidth is
less than the radar PRF.
Some problems associated with pulsed Doppler waveforms and processors are:
• Pulsed Doppler signal processors are generally more complicated than MTI processors
because of the added dimension of Doppler frequency. This extends to post processing
such as detection logic and track algorithms.
• Local oscillators must have low phase noise since this is often a limiting factor in pulsed
Doppler SCR improvement. Pulsed Doppler radars also have stringent timing jitter
requirements since timing jitter translates to phase noise.
• MPRF an HPRF pulsed Doppler radar receivers must have large dynamic range to
simultaneously accommodate large clutter returns from the first range ambiguity, and
small signal returns from subsequent ambiguous regions. This extends to the ADC in
radars where clutter rejection is performed by the digital portion of the signal processor.
13.4.3.1 Pulsed Doppler Clutter
The ground clutter environment in MPRF and HPRF pulsed Doppler radars is generally more
severe than in pulsed radars that are unambiguous in range. This is because, in these pulsed
Doppler radars, the signal returned from long-range targets must compete with clutter at
short ranges.11 This is illustrated in Figure 13.20. The solid triangle in the figure is a target
return from the first (left-most) pulse in the burst of pulses and indicates that the target return
does not arrive until several PRIs after the transmit pulse that caused the return. The dashed

triangles are returns from the same target, but different pulses. The solid, curved line through
the solid triangle represents the clutter from the pulse immediately preceding the triangle, and
previous pulses. The dashed, curved lines are clutter returns related to previous pulses (and
pulses before them). The significance of what signal comes from which pulse has to do with
range attenuation. The target is at a range of Rtgt and will have a range attenuation of Rtgt4.
The clutter in the target range cell is at a range of Rclut and will undergo a range attenuation
of Rclut3 (recall that clutter attenuation varies as R3).
Since Rtgt » Rclut, the target will undergo much more attenuation than the clutter. The result
of this is that the SCR at the input to the signal processor, in pulsed Doppler radars that use
range ambiguous waveforms, is much lower than for the same scenario in radars that use
range unambiguous waveforms.
Figure 13.20 Target and clutter returns in an MPRF or HPRF pulsed Doppler radar.
We will explain this difference with the help of Figure 13.21. The top curve is a plot of SNR
versus range and is applicable to both pulsed and pulsed Doppler waveforms that use the same
pulsewidth. The middle curve is a plot of CNR for a radar that uses a range unambiguous
waveform (e.g., LPRF waveform), at least over the 50 km range interval shown. In this case,
the CNR continuously decreases with range. (The SNR, CNR, and SCR discussed here are the
SNR, CNR, and SCR at the output of the single-pulse matched filter.)
The bottom curve is a plot of CNR for a radar that uses a range ambiguous waveform (e.g.,
a MPRF or HPRF waveform). In this case, the CNR decreases for a while and then resets to a
large value. This reset occurs with every pulse of the waveform, which means the CNR stays
large over the 50-km range of the plot. At the same time, the SNR is decreasing. Thus, the
SCR will continually decrease as target range increases.

Figure 13.21 Plots of SNR and CNR for LPRFand MPRF or HPRF waveforms.
Figure 13.22 Plots of SCR for LPRF and MPRF or HPRF waveforms.
The net effect of the resetting of CNR and continual decrease in SNR is illustrated in Figure
13.22, which is a plot of SCR for LPRF and MPRF or HPRF waveforms. As illustrated in
Section 13.2.1, the SCR for the LPRF waveform initially decreases and then increases.
However, the SCR for the MPRF/HPRF waveform continually decreases. Also, the SCR

values for the MPRF/HPRF waveform are much lower than for the LPRF waveform. This
means the pulsed Doppler signal processor must provide much larger SCR improvement for
MPRF or HPRF waveform than it would for the LPRF waveform.
The aforementioned resetting phenomenon can be explained with the help of Figure 13.23,
which shows notional clutter returns from three successive pulses, plus a composite return
signal at the bottom.
Figure 13.23 Illustration of clutter return resetting phenomenon.
The first pulse causes a clutter return that peaks after the pulse and decays as the range to
the clutter increases. The same thing happens on the second and third pulses. As returns from
the successive pulses are received, their power is added to the power from the previous pulses
and causes the sum to increase after each pulse.
The bottom plot shows that, not only does the composite return peak after each pulse, but
each peak is a little larger than the previous peak because residual clutter returns from
previous pulses. In practice, this increase will level out with increasing pulse number because
the contribution of earlier pulses decreases with range. The buildup of CNR is a clutter
transient. The CNR resetting is sometimes termed clutter folding.
The discussions above indicated the clutter return in a particular range cell is the sum of the
clutter returns from the current pulse and all previous pulses. Since the returns are from
clutter at different ranges, and we assume the clutter returns from different ranges are
uncorrelated, we sum the clutter powers. To derive the appropriate equations, we consider a
clutter cell at a range R where R is greater than some start range, Rstart, and less than the PRI
range, RPRI (= cT/2) minus some stop range, Rstop (i.e., R < RPRI − Rstop). Rstart is usually
chosen greater than c τp/2 because the receiver is off during the transmit pulse and cannot
fully process returns from clutter cells (or targets) at shorter ranges. Rstop is also chosen to be
larger than c τp/2 to allow time for the receiver to fully process pulse returns before it shuts
off in preparation for transmit. When the radar receives a signal from clutter at a range R

close to the most recent pulse, it also receives signals from clutter at R – RPRI due to the
immediately prior pulse, R – 2RPRI from two pulses back, R – 3RPRI from three pulses back,
and so forth. Since the powers from these returns add, the total power associated with the
clutter return from the most recent pulse is
As k increases, the associated PC1(R − kRPRI) contributes less and less to the sum because it is
due to clutter at longer and longer ranges. In many applications, the contribution becomes
very small after only a few pulses.
A means of incorporating this clutter folding into the previous RCS model (Section 13.2.1)
is as follows.
• Generate σC and CNR using the equations in Section 13.2.1 and the radar range equation.
Extend the range to the point where the CNR is about 20 dB below its peak level. For
HPRF waveforms, this will be about 10 PRIs. For LPRF waveforms it will usually be one
PRI and for MPRF waveforms it will be between 1 and 10.
• Implement (13.150) for Npul equal to the number of PRIs determined in the previous step
and R between Rstart and RPRI –Rstop in steps of ΔR where ΔR is the range resolution of the
waveform.
• To generate a CNR plot like Figure 13.21, replicate PCpd(R) for the number of PRIs
needed to cover the range extent of interest.
To generate an associated SNR and SCR plots:
• Generate an array of SNR values over the range extent of interest.
• Blank the range cells between Rstart and RPRI–Rstop for each PRI.
• Generate the SCR by dividing the SNR array by the CNR array.
Figure 13.24 contains the result of implementing these algorithms for the parameters of
Example 1 with a waveform PRF of 50 kHz.
The aforementioned procedures can also be used for rain clutter. The result of such an
application for the rain clutter example of Section 13.4.2.3, and the 50-kHz waveform, is
contained in Figure 13.25.

Figure 13.24 Plot of SNR, CNR, and SCR for the parameters of Example 1 and a 50-kHz PRF waveform—ground clutter.

Figure 13.25 Plot of SNR, CNR, and SCR for the parameters of Example 1 and a 50-kHz PRF waveform—rain clutter.
13.4.3.2 Signal Processor Configuration
The signal processor configuration we will use to evaluate the clutter attenuation, SNR
improvement, and, ultimately, SCR and SIR improvement is illustrated in Figure 13.26. This is
a generic pulsed Doppler signal processor that is applicable to all pulsed Doppler processors
considered in this chapter. It applies to analog or digital processors for HPRF, MPRF, and
LPRF pulsed Doppler waveforms. It is also similar to the configuration we used to analyze
MTI processors, with the MTI replacing the highpass filter (HPF) and bandpass filter (BPF).
The block diagram can be extended to hybrid processors (analog HPF and digital BPF) by
adding an ADC between the HPF and BPF.
As before, the matched filter is matched to a single pulse of the transmit waveform. The
sampler samples the matched filter output, in range, once per PRI. For our analyses, we
assume it samples on the peak of the matched filter response. As indicated in the MTI
discussions, the sampler actually generates several samples per PRI (one for each range gate)
and stores them for processing after it has gathered samples for all pulses in the burst, or CPI.
The samples within a PRI are usually spaced one range resolution cell apart.
Figure 13.26 Pulsed Doppler signal processor.
In the previous paragraph, we indicated the sampler output from all pulses in a burst is
stored and then sent to the processor. This would be the standard approach for radars that
transmit the waveforms in bursts, such as phased arrays. In older, dish-type radars, the
waveform consists of a semi-infinite string of pulses, and the processor would process them
continuously, mostly using analog hardware for the HPF and BPF. In those cases, the “burst,”
or CPI, is roughly the inverse of the BPF bandwidth.
Since we are using frequency-domain techniques in the analyses, they apply to both the
burst of pulses and the semi-infinite string of pulses. A caveat regarding the burst of pulses is
we assume processor transients have settled so that the frequency domain analyses apply
(since they only apply to steady state conditions). This is a consideration in the design and
implementation of pulsed Doppler signal processors.12
In radars that use digital HPFs and/or BPFs, the sampler is an ADC. In radars that use
analog or hybrid processing, it is a sample-and-hold device.
The HPF following the sampler is used to reduce the clutter power located near zero
Doppler. In addition to reducing clutter power, it also serves to reduce the dynamic range
requirements on the BPF following the HPF. It is usually included in processors for HPRF and
MPRF waveforms because of their high clutter attenuation requirements. It can be omitted in

LPRF pulsed Doppler processors since the clutter attenuation requirements of those
waveforms are generally more modest. Having said this, modern radars that use high
dynamic range digital signal processors can eliminate the HPF and rely on the BPF to provide
both clutter attenuation and SNR improvement
In digital signal processors, the HPF is sometimes implemented before the ADC to limit the
dynamic range of the signal into the ADC. In the past, it was thought that the dynamic range of
the ADC needed to be greater than the SCR at the ADC input. However, recent analyses [77]
indicate this is not the case. We will address the impact of ADC dynamic range in Section
13.4.3.6.
The final device in the signal processing chain is the BPF. In the diagram, we show it as a
single BPF, which is all that is needed for these analyses. We assume the BPF is centered on
the target Doppler frequency. We account for this not being the case in practice by including a
Doppler mismatch loss in the radar range equation for the target signal (see Chapter 5). The
main purpose of the BPF is to increase SNR (and thus SCR), although it also provides
additional clutter rejection by reducing phase noise power.
In practice, the HPF output could feed several BPFs centered at different frequencies. The
processor used during search would require enough BPFs to span the PRF [recall that, in
sampled data systems, the sampler “folds” the entire signal spectrum of the matched filter
output into a frequency band between –PRF/2 and PRF/2 (between −1/2T and 1/2T)]. The
processor used during track needs only a few BPFs since the target Doppler frequency is
known reasonably well during track.
The implementation of pulsed Doppler signal processors has evolved over the years from
all analog to all, or almost all, digital. The evolution has generally been driven by the speed,
availability, and cost of ADCs and digital signal processing components. Older radars (pre-
1980s or so) used all-analog signal processors. Radars designed between about 1980 and
2000 used a mix of digital and analog components. Modern pulsed Doppler signal processors
are almost exclusively digital. Some digitize the signal at the matched filter output, as in
Figure 13.26. Others digitize the signal at the IF amplifier output and implement the matched
filter in the digital domain (see Chapter 14).
In digital processors, the BPFs used in search are often implemented using FFTs with
amplitude weighting to reduce Doppler sidelobes. The FFT is attractive because, by default,
its taps span the PRF. It is also computationally efficient. Since only a few BPFs are required
in the signal processor used in the track channel, they can be implemented using finite
impulse response (FIR) filters. It is not unusual that the HPF, when used, is implemented with
an infinite impulse response (IIR) filter because it generally requires sharp cutoff
characteristics.
13.4.3.3 Digital Signal Processor Analysis Techniques
We analyze digital, pulsed Doppler signal processors using techniques very similar to those
used for MTI processors. Specifically, we compute the clutter and target power at the output
of the signal processor using equations similar to (13.90) for clutter and (13.106) for target

signals with H(f) replaced by HH(f)HB(f). We normally ignore scanning in pulsed-Doppler
analyses, but always consider phase noise. With this, we have
for the clutter signal and
for the target signal. We will discuss the noise shortly. The integral of (13.151) is usually
computed numerically because a closed form solution is usually impossible to derive. PSout is
also sometimes evaluated numerically because of the forms of HH(f) and HB(f).
Because of the impulse function (Dirac delta), we can write PSout as
and
In most applications, the main lobe of MF(fd) (the matched-range Doppler cut of the
ambiguity function) is wider than the expected span of target Doppler frequencies so that
MF(fd) ≈ 1. Also, the target Doppler frequency is normally assumed to be in the pass band of
the HPF, and the BPF is assumed to be centered very close to the target Doppler frequency so
that HH(fd) ≈ 1 and HB(fd) ≈ 1. Combining these leads to the observation that GS ≈ 1. We
account for the fact that the various terms of (13.154) are not exactly unity by including a loss
term in the radar range equation. However, the general form of GS is useful for determining
the limits the HPF might place on the ability of the radar to detect and track low Doppler
targets, or targets whose Doppler frequency approaches a multiple of the PRF (ambiguous
Doppler operation). In this case, the fact that GS ≠ 1 is not included in the losses.
Figure 13.27 contains a sketch of the various spectra discussed above. Note that because of
sampling, the clutter spectrum, C(f), and the target spectrum, T(f), are repeated at intervals of
1/T. Also, because of the HPF and BPF are digital, their responses are periodic with a period
of 1/T. As indicated, MF(f) is very wide relative to the other spectra.

Figure 13.27 Spectra applicable to digital pulsed Doppler signal processor analyses.
For the clutter, C(f) is one of the forms discussed in Sections 13.2.2 and 13.2.4. For Φ(f), we
use the general form of (13.62). PC is computed via CNR using the folded clutter discussed in
Section 13.4.3.1. Using this, we have
where
and
We term GC the central line clutter gain and Gϕ the phase noise clutter gain. GC is a
measure of the ability of the processor to reject clutter if there was no phase noise. Gϕ is a

measure of the effect of phase noise on the ability of the signal processor to reject clutter. As
we will show, Gϕ is usually much larger than GC. That is, phase noise is usually the limiting
factor on the ability of the signal processor to reject clutter.
We treat receiver noise (what we have called noise) differently than target and clutter
signals because the target and clutter signal methodology does not apply to noise. For the
former, we developed the appropriate equations by propagating a signal from the transmitter
to the target or clutter, back to the radar, and through the receiver to the output of the sampler.
Noise originates in all receiver stages (including the ADC), but the current practice is to
reference it to the receiver input by specifying system noise temperature, Ts, or a system noise
figure, Fn.13 This is then used to compute the (white) noise power spectral density N0 = kTs or
N0 = kT0Fn depending on the noise model used (see Chapter 4). To be consistent with the
terminology we have used thus far, we need noise power at the matched filter output, not noise
power spectral density at the receiver input. If we had an appropriate bandwidth, we could
compute the noise power at the matched filter output. However, there is an easier way to
approach the problem. Specifically, we reference everything to the noise power at the matched
filter output. That way, we do not need to specifically know PN and we can determine PS and
PC from SNR and CNR at the matched filter output, which we can compute from the radar
range equation.
Since the power at the output of a (theoretical) sampler (or ADC) is the same as the power
at its input, we have PNo = PN. Also, since the bandwidth of the noise out of the matched filter
is much larger than the sample frequency, 1/T, we can reasonably assume the noise at the
output of the sampler is white (see Exercise 18). By definition, if the power associated with
white sampled data (discrete time) noise is PNo, its power spectral density is also PNo, that is,
Ns = PNo = PN. With this, we can write the noise power at the signal processor output as
where
is the noise gain of the signal processor.
Recalling that PSo = PS and PCo = PC, we can use (13.132) through (13.135) to derive
equations for the SNR, CNR, and SCR gains through the signal processor. The SNR gain is

The CNR gain is
CA is the reciprocal of GCNR. The SCR gain, or SCR improvement, is
13.4.3.4 Phase Noise
In high- and medium-PRF pulsed Doppler radars, phase noise is the major factor that limits
clutter attenuation and, as a result, SCR and SIR improvement. Because of this, we extend the
phase noise model beyond the simple form of (13.63). In particular, we want to derive an
expression for ΦΔϕ(f). From (13.61) and (13.62), we can write
where
and (see Appendix 13A)
In (13.165), τd = 2RC/c is the time delay to the clutter (one of the point clutter sources that
make up the clutter patch illuminated by the pulse, or prior pulses—see Section 13.4.3.1) and
ϕ(t) is the local oscillator (LO) phase noise. Using (13.165) in (13.164), we get

where
is the autocorrelation of the LO phase noise. Substituting (13.166) into (13.163) results in
where
is the LO phase noise spectrum.
Equation (13.168) is interesting because it indicates the phase noise component of the
clutter return depends on the LO phase noise and the range delay to the (point) clutter source.
This dependency is termed range correlation [1, 50, 57, 78, 79]. It indicates that returns from
clutter at close range, due to phase noise, will be correlated and will cancel in the mixer
where the LO signal is removed from the return signal. This assumes the same LO signal is
used in the transmitter and receiver. If they use different LOs, there will be no correlation and
the bracketed term of (13.168) would not be 2, assuming the phase noise spectra of the
different LOs are the same.
Equation (13.168) applies to a single, point source of clutter. Since clutter is distributed
over a range extent, the spectrum of (13.168) must be integrated over the range region of
interest. In this integration, we must also account for the variation of clutter power with range.
Thus, to find the phase noise spectrum for a clutter region, we compute the integral
where 
 is a region that contains the clutter ranges of interest and K is a normalizing
constant. The R3 factor accounts for the nominal cubic decrease in ground clutter power with

range. For rain clutter, we would use R2. R0 is a reference range. It is the range to the front of
the closest clutter patch.
The equation for HR(f,R0) is somewhat complicated and is included in Appendix 13C.
Appendix 13C also contains an approximation that works well. That approximation is
Figure 13.28 Range correlation effect.
Figure 13.28 contains plots of HR(f,R0) using the equation of Appendix 13C and
approximation of (13.171). As indicated, they match reasonably well. The plot was generated
for a waveform with a 100-kHz PRF, a 1-µs unmodulated pulse and R0 = 700 m (R0 is Rstart). It
is interesting to note that the curve levels out at 3 dB. This is because the clutter “voltage”
[vobj(t) in Figure 13.6] is multiplied by vLO(t) on transmit and receive. Thus, the phase noise
component of vLO(t) is added twice.
As indicated, Sϕ(f) is the phase noise spectrum of the LO. Figure 13.29 contains a sample
phase noise spectrum for an 8.64-GHz LO. The LO signal was created by multiplying the
frequency of a 320-MHz surface acoustic wave (SAW) oscillator by a factor of 27. This phase

noise spectrum represents mid-level technology in that the spectrum floor, Φ0, is about −146
dBc/Hz.
The dashed line on Figure 13.29 was generated using a mathematical model developed by
D. B. Leeson [80], with modifications suggested by Rick Poore in an Agilent Technologies
report [81]. That model is
As can be seen, the Leeson-Poore model fits the measured curve very well.
Figure 13.29 Measured and modeled phase noise plot. (Source: Bill Myles, Dynetics, Inc. Used with permission.)
Figure 13.30 contains a notional Bode plot [82] of Sϕ(f). For frequencies below f3, the Bode
plot has a slope of zero. Between f3 and f1, the slope is −30 dB/decade, which indicates a f−3
variation of Sϕ(f). At f1, the slope changes to −20 dB/decade, and at f2, it changes to zero. Sϕ(f)
converges to the phase noise floor of Φ0. For the dashed curve of Figure 13.29, we used f1 = 3
kHz, f2 = 15 kHz, f3 = 10 Hz, and Φ0 = −146 dBc/Hz.

Figure 13.30 Bode plot of the modified Leeson phase noise spectrum model.
Figure 13.31 Total phase noise spectrum with and without range correlation.
Figure 13.31 contains a plot of the ΦΔϕ(f) that results from using the HR(f,R0) of Figure
13.28 and the Sϕ(f) model of (13.172). An interesting feature of Figure 13.31 is that the rise in
Sϕ(f) at low frequencies is canceled by HR(f,R0) to produce a total phase noise spectrum that is
essentially flat. It is not clear whether this is a coincidence of this example or a general
behavior.
The next step is to perform the convolution of C(f) with ΦΔϕ(f) that is indicated in (13.157).
The result of using the Gaussian clutter spectrum of Example 1, with σv = 0.22 m/s, is shown

in Figure 13.32. The solid curve is the spectrum after convolution, and the dashed curve is the
spectrum before convolution. The slight difference in the amplitudes is caused by the
frequency step size used in the numerical convolution.
Figure 13.32 Plot of C(f)*Φ∆ϕ(f) and Φ∆ϕ(f).
As can be seen, convolving ΦΔϕ (f) with C(f) had almost no effect on the shape of the phase
noise spectrum. This is expected because, relative to the variations in ΦΔϕ (f), C(f) is virtually
an impulse function (Dirac delta). Thus, convolving ΦΔϕ(f) with C(f) produces almost the
same result as convolving ΦΔϕ(f) with a Dirac delta.
13.4.3.5 Summary and Rules of Thumb
Table 13.9 contains a summary of the results we obtained in the above discussions. It also
contains some rules of thumb that were discussed, or will be discussed shortly.
Table 13.9
Summary of Digital Pulsed Doppler Signal Processor Analysis Equations

To derive the rule of thumb for GN, we assume the stopband of the HPF is much narrower
than the PRF (narrower than 1/T), and the BPF is ideal with a bandwidth of B and a gain of
unity. We assume the BPF passband is centered at some frequency, fB, in the passband of the
HPF. Under these conditions,

Deriving the rule of thumb for Gϕ is a little more involved. With C(f)*Φ∆ϕ(f) = Φ0, we have
We can approximate MF(f) as an ideal LPF with a two-sided bandwidth of 1/τp. We assume T/
τp is an integer, N, so that HH(f) and HB(f) will be repeated N times over the interval of 1/τp.
We further use the GN rule-of-thumb assumptions on HH(f) and HB(f) we used to arrive at
(13.173). With this, and a little thought, Gϕ becomes
where we made use of (13.173). We leave it as an exercise to verify (13.175) via simulation.
The remainder of the rules of thumb were discussed previously.
13.4.3.6 Example 2
To illustrate the above procedures, we consider two examples. The first is a pulsed Doppler
radar that uses a PRF of 100 kHz and an unmodulated pulse with a width of 1 µs. The
remaining radar, target, and clutter parameters are given in Table 13.10.
For this analysis, we assume the radar is searching and only consider the case where the
radar beam is ½ beamwidth above the horizon, which is at 0° elevation. We assume the target
is flying radially toward the target at the azimuth and elevation angle of the radar beam.14 In
the clutter RCS generation computer code, we use the sinc(x) antenna pattern of (13.3).
Although we are interested in target ranges between 2 and 50 km, we must model clutter
returns from much shorter ranges. We assume the receiver timing is such that the radar
receives returns from clutter located at 225 m. This means the receiver is off during the
transmit pulse (150 m) and for ½ pulsewidth after the transmit pulse. The receiver remains on
until 1½ pulsewidths before the next transmit pulse. Thus, the receiver processes returns from
clutter (and targets) over a range window that extends from 225 m to 1,275 m after the leading
edge of the transmit pulse. Of course, it receives returns from multiples of this window
repeated every PRI. During the time of 225 m before and after the leading edge of the transmit
pulse, the receiver is off, which means the radar is blind during these times. In search, this
does not generally pose a problem because targets will fly through the blind regions quickly.
During track, it can pose problems. However, during track, pulsed Doppler radars adjust the
PRF to assure the target is not in a range or Doppler blind region.
Figure 13.33 contains plots of SNR, CNR, SCR, and SIR at the matched filter output for this
example. At long ranges, the SNR is about -10 dB, which is too low to support detection and

tracking. To raise the SNR to about 13 dB, the Doppler processor needs to provide about 23
dB of SNR gain.
Table 13.10
Radar, Target, and Clutter Parameters for Example 2
Peak power
10 kW
Operating frequency
8 GHz
System noise temperature
1,500 K
PRF
100 kHz (PRI = 10 µs)
Burst length
7 ms (700 pulses per burst)
Pulsewidth
1 µs
Total losses for the target and clutter
6 dB
Height of the antenna phase center
3 m
Antenna gain
38 dB
Azimuth and elevation beamwidth
2°
Beam angle
Beam parked at ½ beamwidth above 0° elevation
RMS antenna sidelobes
30 dB below the peak gain
Clutter backscatter coefficient
−20 dB
Target RCS
−10 dBsm
Ranges of interest
2 km to 50 km

Figure 13.33 SNR, CNR, SCR, and SIR at matched filter output for Example 2.
The CNR has peaks of about 80 dB, which cause the SCR and SIR to be very low. It is
estimated that the signal processor will need to provide 80 to 90 dB of clutter attenuation to
raise the SCR and SIR to reasonable levels. The blank regions of the SNR and other plots are
the regions where the receiver is gated off.
The signal processor consists of an HPF for clutter rejection followed by a bank of BPFs to
provide SNR gain.
We want the radar to be able to detect and track targets with range rates down to about 40
m/s. This means we must choose the cutoff frequency of the HPF to be
We choose fch = 2,000 Hz. We use a fifth-order, Butterworth HPF [83]. An approximate
HH(f) for this filter is

with
and
This response is derived from the H(f) of an analog LPF by using the substitutions of
(13.178) and (13.179) to make the response periodic with a period of 1/T. Equations (13.178)
and (13.179) are derived from the bilinear transform [83, 84].
We typically want to choose the bandwidth of the BPF to be as small as possible since this
sets the limit on SNR and SCR improvement. For the radar of this example, the burst length of
7 ms sets an absolute lower limit of about 140 Hz.15 However, we must allow for clutter
transients in the HPF, the duration of which is typically set by the HPF cutoff frequency. A
rough rule of thumb is that the transients will settle in a time period equal to about five times
the reciprocal of the HPF cutoff frequency. For the HPF of this example, this would be about
5/2000 s or about 2.5 ms. We will allow 3.5 ms for clutter transients. In other words, we gate
the output of the HPF off for 3.5 ms and send the last 3.5 ms of pulses to the BPFs. Because of
this, the effective burst length, in terms of BPF bandwidth, is 3.5 ms and means the minimum
BPF bandwidth is 1/(3.5 ms) or 286 Hz. We choose a bandwidth of 350 Hz.
The idea of gating the HPF output off to allow for transients is sometimes termed clutter
gating. In some applications, even the clutter gating is not sufficient to mitigate the deleterious
effects of HPF transients. In those cases, the signal out of the clutter gate is weighted, in the
time domain, by some type of function that starts at zero and increases to unity over a short
period of time. This is often termed cosine weighting because its shape is of the form [1 –
cos(αt)]/2. In still other instances, the clutter gate can be eliminated and only cosine weighting
used. This is a design trade-off that depends upon many factors including burst length, clutter
rejection requirements, and desired/required SNR improvement, among others.
We assume the BPF is a sixth-order Butterworth filter. For purposes of our analyses, we
assume it is centered on the target Doppler frequency of 8,000 Hz (which corresponds to a
target range rate of −150 m/s). An approximate HB(f) for the BPF is
where

and
In these equations, fd = 8,000 Hz and fcb = 350 Hz. The BPF is derived from a third-order
Butterworth LPF by using a frequency transformation derived from the bilinear transform,
with a frequency shift to center the response at fd.16
The clutter spectrum is the Gaussian model of Section 13.2.2 with σv = 0.22 m/s. Also, we
use the C(f)*ΦΔϕ(f) of Figure 13.32 but represent it by a constant value of Φ0 = −143 dBc/Hz
since Figure 13.32 indicates C(f)*ΦΔϕ(f) is fairly constant.
Since MF(fd)≈1 and the target Doppler is well within the passband of the HPF, we can use
the rule of thumb that GS = 1.
GN is computed by numerically evaluating
with HH(f) and HB(f) from (13.177) and (13.180). This results in GN = 0.0037 W/W.
Alternately, we could have used the rule of thumb from Table 13.9 to arrive at a value of GN =
0.0035 W/W. From this, the SNR gain is
which is a little larger than the desired value of 23 dB.
The center line clutter gain is
which, as predicted, is very small. The phase noise component of the total clutter gain is

The rule-of-thumb value (see Table 13.9) is also −107 dB. Equations (13.185) and (13.186) are
computed using numerical integration.
With the above, the SCR improvement is
which is quite large. The result of applying the gains to the plot of Figure 13.33 is shown in
Figure 13.34. The shape and level of the SIR curve is very close to the SNR curve, which
means the signal processor has effectively eliminated the clutter, and resulted in noise-limited
operation. The SIR and SNR values are much improved over the values at the matched filter
output, but still a little low at long ranges.
A consideration in digital signal processors is the impact of the ADC on SCR improvement.
The specific ADC properties of concern are the number of bits in the ADC, quantization
noise, internal ADC noise, and ADC dynamic range. A common rule of thumb used to
characterize the impact of the ADC on SCR improvement is to say the ADC imposes an
absolute limit on performance of
Where Nbit is the number of bits in the ADC. While Iscr is influenced by the number of bits in
the ADC, the hard limit given by (13.188) is not valid when processing gain is taken into
consideration. A more representative equation for Iscr that includes the effects of GC, phase
noise, and the ADC is [77]

Figure 13.34 SNR, CNR, SCR, and SIR at digital signal processor output for Example 2.
PADC is the level of the clutter at the ADC input relative to the ADC saturation level. It is
normally taken to be −6 dB to ensure clutter fluctuations do not occasionally cause ADC
saturation.17 The presence of PADC implies there is some type of gain control that monitors
the clutter level into the ADC and adjusts the gain to keep the level 6 dB below ADC saturation
(full scale input).
The term PNADC encompasses quantization noise, noise generated internally by ADC
circuitry, and any additional dither noise that is added to the ADC input to assure linear
operation of the ADC. This raises an important issue concerning the ADC: For the ADC to
preserve the relative sizes of signal, clutter, and noise after quantization, there must always be
sufficient noise at the ADC input (see Section 14.8.1.5). A reasonable value of PNADC is [77]
where Nbit is the number of bits in the ADC and q is the number of quantization levels of the
dither noise at the input to the ADC. Typical values of q are 1/2 to 1. [Note: if we say dither
noise toggles the least significant bit (lsb) of the ADC, q=1; if it toggles the lower two bits,
q=3.]

Fs is the ADC sample rate. It is normally taken to be the modulation bandwidth of the
waveform if range gating is performed after the ADC. For an unmodulated pulse, Fs = 1/τp. If
the radar uses IF sampling with digital downconversion, Fs can be much larger than the
modulation bandwidth.
The Iscr equation of (13.189) is written in terms used for a pulsed Doppler signal processor.
It is also applicable to the MTI processor with GSNR = 1 and GC = 1/CA.
For our example, if we use a 12-bit ADC with one bit of quantization noise (i.e., q = 1), the
SCR improvement would be limited to about 84 dB instead of the 107 dB indicated in
(13.187). To get close to 107 dB, we would need a 16-bit ADC. With a 14-bit ADC, the SCR
improvement would be 96 dB, which would be sufficient to raise the SIR curve close to the
SNR curve.
13.4.3.7 Example 3
For this example, we consider a LPRF pulsed Doppler radar. We want to maintain
approximately the same burst length and want the radar to operate range unambiguously at the
maximum range of 50 km. To satisfy these constraints, we use a PRI of 400 µs, which will
provide an unambiguous range of 60 km. We also use a burst of 16 pulses with the thought
that an FFT will be used as the signal processor. We will sacrifice some minimum range
capability and choose a pulsewidth of 25-µs. To maintain the same resolution as with Example
2, we use LFM modulation with a bandwidth of 1 MHz. The remainder of the radar, clutter,
and target parameters are the same as in Example 2.
Figure 13.35 contains a plot of CNR, SNR, SCR, and SIR at the matched filter output for
this case. It does not exhibit the periodic behavior of Figures 13.33 and 13.34 because the
waveform is unambiguous in range over the range interval of interest. Consistent with the 25
µs pulsewidth, the minimum range of the curves is 3.75 km. The SIR and SCR curves are
virtually coincident because SCR is the main contributor to SIR. Recall that

Figure 13.35 SNR, CNR, SIR, and SCR at the matched filter output for Example 3 with ground clutter.
For the HPRF waveform of Example 2, the SCR ranged from zero to about −90 dB. With
this waveform, the SCR ranges from about −5 to −30 dB, because there is no folding of the
clutter in range (see (13.150). The difference in SCR values is due to two factors: 1) the
closest clutter cell is located at 3,750 m as opposed to 225 m for the HPRF waveform, and 2)
the target return only competes with clutter at its range, rather than with clutter at shorter
ranges. Since the SCR is not extremely low, the signal processor does not need to provide the
large clutter attenuation and SCR improvement of the HPRF case. The SNR for this case is
lower than desired so the signal processor must provide SNR improvement. Since high clutter
attenuation is not needed, the HPF can be deleted from the signal processor. Also, since the
waveform contains 16 pulses, we will implement the BPF with a 16-tap FFT. We include a 50-
dB Chebyshev weighting to provide clutter attenuation by suppressing the Doppler sidelobes.
Figure 13.36 FFT frequency response.
A plot of the frequency response of one FFT tap is contained in Figure 13.36. The tap is

centered on a Doppler frequency we arbitrarily identified as fd, which we assume is the target
Doppler frequency. The span of the plot is the PRF of 2,500 Hz.
The plot of Figure 13.36 has been normalized to the response of a 16-tap FFT with uniform
weighting to show the loss due to the Chebyshev weighting. This loss is about 1.6 dB. This
loss is incorporated into the SNR gain of the processor.
We could analyze the performance of this signal processor using the techniques
summarized in Table 13.9. However, since it contains only an FFT processor, which is an FIR
filter, we can use techniques similar to those we used in the MTI analyses. Denoting the
Chebyshev weights by w(k) and assuming the input voltage is vin(k), the voltage at the
aforementioned tap is
If vin(k) is (sampled data) white noise, n(k), with a power of PN, the noise at the FFT output
is
and
Since we assumed the signal is centered on the FFT tap of interest, we can write
and
With this, the SNR at the signal processor output is

where SNRMF is the SNR at the output of the matched filter (see Section 13.3.2). With this we
get
To compute the SCR gain through the signal processor, we first assume the sidelobe region
has a constant value equal to the peak of the sidelobes of Figure 13.36. Thus, the sidelobe
level is −50 dB. If the clutter spectrum is contained in the sidelobe region of Figure 13.36,
then, relative to the target, the clutter will experience an attenuation of 50 dB. With this, a rule-
of-thumb estimate for GSCRC is 50 dB. This is equivalent to GS/GC using the terminology of
Table 13.9 and is thus termed the central line SCR improvement. This is the logic behind
assigning it the symbol GSCRC.
To determine the impact of phase noise, we consider the Gϕ term of Table 13.9 and use the
rule of thumb for Gϕ along with the equation for GSNR. Combining these, we get
It is interesting to note that, for this signal processor, the limit on SCR improvement is the
central line clutter rejection capability of the signal processor and not the phase noise.
Combining (13.199) with GSCRC, the overall SCR improvement due to both central line
clutter and phase noise is
Finally, we can compute GCNR using Table 13.9 as
which means the signal processor provides a clutter attenuation of 39.5 dB.
Figure 13.37 contains the result of applying the above values to the curves of Figure 13.35.

The 50-dB SCR improvement offered by the signal processor was adequate because it moved
the SIR curve close to the SNR curve at long ranges and still provided reasonable SIR values
at short ranges. The SNR is still less than desired at the maximum range of 50 km.
As an extension to this example, we examine how well the processor performs in rain
clutter. Recall from Example 1 that the MTI was not able to adequately reject rain clutter.
Figure 13.38 contains plots of SNR, CNR, SIR, and SCR at the matched filter and signal
processor output for the rain parameters used in Example 1. As with the ground clutter, we
assumed the rain spectrum was in the sidelobe region of the signal processor frequency
response. Although the Doppler processor provides better performance than the MTI, it is still
not quite sufficient, as evidenced by the fact that the SIR at the signal processor output is lower
than the SNR curve. If the clutter attenuation was increased by 10 dB, to 49.5 dB, the
performance would be acceptable.
Figure 13.37 SNR, CNR, SIR, and SCR at the signal processor output for Example 3 with ground clutter.

Figure 13.38 SNR, CNR, SIR, and SCR at the matched filter (top) and signal processor (bottom) output for Example 3 with
rain clutter.
As a note, the calculations used in the example were based mainly on rule-of-thumb
equations. An extension of the analysis would be to use the more exact integrals summarized
in Table 13.9. It is expected that the result will be a slight increase in clutter attenuation, which
might help for the rain clutter case.
13.4.3.8 Analog Pulsed Doppler Processors
In analog processors, HH(f) and HB(f) are not periodic functions of frequency. As a result, we
cannot use the same analysis techniques we used for digital pulsed Doppler processors and the
MTI processor of Section 13.4.2. Instead, we must compute the folded spectrum, So(f) and
work with it. We must also specifically include the hold part of the sampler, which we assume
to be a zero-order hold (ZOH). This configuration is illustrated in Figure 13.39.
Figure 13.39 Analog Doppler processor block diagram.

The frequency response of the ZOH is [83, 84] is
As before, we assume the receiver noise at the sampler output is white with a power of PNo
= PN, where PN is the noise power at the matched filter output. Since the noise at the sampler
output is a sampled signal, its power spectral density is also PN. That is, SoN(f) = PNo = PN.
The noise power spectrum at the output of the ZOH is
and the noise spectrum at the processor output is
The noise power at the processor output is
and the noise gain through the processor is
Figure 13.40 contains sketches of SNo(f), HZ(f), HH(f), and HB(f). As shown, HZ(f) is a sinc2(x)
function that has a first null at f = ±1/T.
Figure 13.40 Sketches of SNo(f), HZ(f), HH(f), and HB(f).
An important point to note is that the BPF is centered below 1/2T (i.e., PRF/2). This is a
requirement because of frequency folding. That is, all of the relevant frequency information
in the signal folds into a region between f = −1/2T and f = 1/2T.

We can develop a rule-of-thumb equation for GN by making some simplifying assumptions
about HH(f) and HB(f). We assume HB(f) is
where fB is in the passband of the HPF and B is small relative to 1/T. We further assume the
HPF has a passband gain of unity. With this, we get
which is close to the form we derived for the digital processor.
The target spectrum at the sampler output is
where SrS(f) = δ(f − fd) and PS is the target signal power at the matched filter output. With this
we have
The target spectrum at the ZOH output is
The spectrum at the signal processor output is

Figure 13.41 Sketches of T(f), HZ(f), HH(f), and HB(f).
Figure 13.41 contains depictions of the various signal-related spectra. In this figure, the BPF
is centered on the target return. Note that even though there are many target spectral lines
present in the ZOH output, only one is in the passband of the BPF. Again, note that the BPF and
target spectral line of interest are in the range f ∈ (−1/2T,1/2T].
The target signal power at the processor output is
with
If we use the assumptions about HH(f) and HB(f) that we used for the noise rule of thumb
and assume fB = fd, we obtain a rule-of-thumb equation for signal gain as
If we further make the (reasonable) assumption MF(fd) ≈ 1, we get
We can combine the results we obtained thus far to derive an equation for SNR gain
through the processor as
where GS is given by (13.214) and GN is given by (13.206). If we make use of the rules of

thumb for GS and GN, we get a rule-of-thumb equation for GSNR as
which is the same result we obtained for the digital signal processor.
The spectrum at the output of the sampler for the clutter signal is
where
The spectrum at the signal processor output is
Figure 13.42 contains a sketch of the various spectrum components that make up SCout (f).
The clutter power at the signal processor output is
with

and
In general, these integrals must be evaluated numerically. However, for the case where the
range correlation results in Sϕ(f)HR(f,R0) = Φ0, (13.224) reduces to
Figure 13.42 Sketches of S∆ϕ(f), C(f), HZ(f), HH(f), and HB(f).
If we use the assumption we used to derive (13.208) and represent MF(f) by an ideal LPF with
a bandwidth of 1/τp, the summation of (13.225) reduces to N where N = T/τp. If we carry this a
step further and use the HH(f) and HB(f) assumptions we used to derive the rule-of-thumb
equation for GN, we can further reduce (13.225) to
From (13.162), we can write the SCR gain of the signal processor as

The above equations and rules of thumb are summarized in Table 13.11.
13.4.3.9 Example 4
To illustrate the analog processor analysis procedures, we consider the Example 2 pulsed
Doppler radar that uses a PRF of 100 kHz and an unmodulated pulse with a width of 1 µs. The
remaining radar, target, and clutter parameters are given in Table 13.10 and discussed in
Example 2. Because of this, the plots of SNR, CNR, SCR, and SIR at the matched filter output
are as shown in Figure 13.33.
We will use the same types of filters as in Example 2, except that they are analog as
opposed to digital. The frequency responses are
for the HPF and
for the BPF. In the above fh = 2,000 Hz, fB = 8,000 Hz, and B = 350 Hz.
Table 13.11
Summary of Analog Pulsed Doppler Signal Processor Analysis Equations


GN is computed by numerically evaluating
This results in GN = 0.0036 W/W. Alternately, we could use the rule of thumb from Table
13.11 to arrive at a value of GN = 0.0034 W/W.
GS is computed from
which yields GS = 0.98 W/W. The rule-of-thumb equation of Table 13.11 also results in GS =
0.98 W/W.
Combining GS and GN results in a processor SNR gain of
The central line and phase noise clutter gains are computed from
and
We used the form of (13.234) to compute the phase noise clutter gain because we previously
showed that, for all practical purposes, Sϕ(f)HR(f,R0) = Φ0 where Φ0 = −143 dBc/Hz. The
resulting values for these two gains are GC = −242 dB and Gϕ = −107 dB
With the above, the SCR improvement is

Figure 13.43 Plots of SNR, CNR, SCR, and SIR at analog processor output.
The results of applying these gains are shown in Figure 13.43. If we compare the
performance results using the analog and digital processors, we note they are very close, as
expected.
For a hybrid signal processor, we would use the approach of this section to determine the
noise, signal, and clutter spectrums at the output of the analog portion of the processor. We
would then use these in place of MF(f)Sr(f) in the digital processor analyses. We would
compute the various powers out of the digital portion using (13.79) with substitutions for
MF(f)Sr(f).
13.4.4 Chaff Analysis
Another form of clutter of concern in military radars is chaff. Chaff consists of short tuned
dipoles made of strips of aluminum foil or aluminum coated pieces of glass, fiberglass, or
Mylar which, when dispensed, bloom into a cloud that has a very large RCS [30, 75, 85].18
Since the dipoles have large aerodynamic drag, they can remain aloft for long periods of
time. The RCS, center velocity, and velocity spread of chaff can cause problems in tracking
radars not designed to mitigate chaff. Specifically, the chaff has an initial velocity close to that
of the dispensing aircraft. Since the chaff RCS is large, it can capture the radar tracking gates

and cause the radar to track the chaff instead of the aircraft. Its ability to do this depends on
how the chaff is dispensed, the characteristics of the radar track loops, and the type of signal
processor. In general, pulsed Doppler signal processors are less susceptible to chaff than are
MTI processors, and radars that do not use some type of clutter mitigation technique.
The time behavior of chaff RCS consists of four phases denoted as: 1) the transient phase,
2) the bloom phase, 3) the mature phase, and 4) the decay phase. For self-screening chaff, the
transient phase is the explosive birth phase of the chaff cloud and is the time a chaff cartridge
is ejected from the aircraft and explodes to dispense the chaff dipoles.19 During this time, the
RCS is small since the dipoles are still tightly packed. This is also the time the initial velocity
is highest and the velocity spread is highest. The latter property is most likely due to the fact
that the explosion imparts widely differing velocities to the chaff particles.
The bloom phase begins immediately after the chaff cartridge has exploded and is
characterized by rapid chaff cloud growth, which is important for masking the aircraft
quickly and becomes more critical the faster the aircraft is traveling and the better rejection
the opposing radar has. This is also the time period in which the RCS rapidly increases
because of the spreading of the chaff cloud. During this phase, the chaff center velocity and
velocity spread decrease to a level that depends on factors such as wind speed, wind shear,
turbulence, and fall velocity [85].
The mature phase is the time period when the full RCS of the chaff cloud is realized.
During this period, the center frequency and velocity spread are likewise determined by
factors such as wind shear, turbulence, and fall velocity [85].
In the decay phase, the chaff dipoles spread to the point where the chaff may no longer
appear as a single cloud. As a result, the RCS decays and the velocity spread becomes
narrower.
Several authors have presented discussions of chaff RCS, center frequency, and spectral
spread (velocity spread). However, there is limited data on the transient behavior of chaff. We
present models that might be useful in evaluating the transient behavior. The models use an
exponential decay or increase since exponential functions are easy to use, and we have no
justification for more complex models. The model we propose for RCS dynamics is
where σSS is the bloom RCS and σI is the RCS at the start of the bloom phase. τσ is the RCS
time constant. Equation (13.236) is a variation of a model found in [87, 88] and allows an
initial RCS other than 0 dBsm. Nathanson [30] provides an equation for σSS that he attributes
to Schlesinger [89]. That equation is [30, p. 335]

where Wchaff is the weight of the chaff bundle, in pounds, and fc is the radar operating
frequency, in GHz. This equation applies to chaff made of aluminum foil that is 0.001 inch
thick, λ/2 long, and 0.01 inch wide. He notes that one pound of this chaff at 3-GHz operating
frequency would have an RCS of 1,000 m2 [30, 90, 91]. Nathanson points out that one pound
of chaff designed to cover a frequency range of 1 to 10 GHz might have a σSS of 60 m2. It is
not clear what value should be chosen for σI. A guess might be 0 to 5 dBsm since this is the
beginning of the bloom phase.
Determining τσ is more difficult since there seems to be a dearth of published information
in this area. Nathanson notes, “Chaff dipoles have high aerodynamic drag: their velocity
drops to that of the local wind a few seconds after they are dispensed.” Based on this, it seems
as if a reasonable value of τσ might be 1 to 2 seconds.
The model we propose for chaff spectral width is
In this equation, BI is the 3 dB bandwidth at the beginning of the bloom phase, BSS is the
bandwidth at the end of the bloom stage and τB is the time constant associated with the change
in bandwidth. Nathanson notes that the same components that affect the spectral spread of rain
also affect the spectral spread of chaff. In particular, he notes that the shear component is the
same as rain. According to a graph in his book [30], the spread due to shear can range from 0
to 5 m/s depending upon altitude and beam elevation angle. He goes on to note that a
reasonable value for the turbulence component is 0.7 to 1 m/s. He provides equations for the
spread due to fall velocity and spectrum broadening due to its elevation dispersion across the
radar beam. Although he does not state it, we assumed that these values are for BSS. If we were
to summarize the various numbers into rule-of-thumb values, we would suggest a range of 1
to 5 m/s.
It is more difficult to guess values for BI. However, if we use the premise that BI is larger
than BSS, a guess might be to choose BI 2 to 5 times BSS. As with the case of RCS, a reasonable
value of τB might be one to two seconds.
Our proposed model for mean chaff radial velocity is
where VI is the radial velocity at the beginning of the bloom stage, Vwind is the radial velocity
of the wind and τV is a chaff deceleration time constant [88]. The chaff center frequency is
therefore f(t) = 2V(t)/λ. As with rain, Vwind depends on the actual wind velocity and direction,
and the direction of the radar antenna beam. For chaff that is “dropped,” VI would equal the

target range rate, or smaller if it is assumed the cartridge does not explode until it has
separated from the aircraft by some distance. If the chaff cartridge is ejected forward of the
aircraft, VI could be considerably larger than the aircraft range rate. As with the other models,
a reasonable value of τV might be one to two seconds. The separation between the chaff
centroid and aircraft can be expressed as
Given the nature of chaff, a suggested spectral model for chaff is the Gaussian model given
by (13.12) discussed in Section 13.2.2 [92]. This suggestion is based on the assertion that the
velocities of the dipoles are most likely governed by a Gaussian distribution. Because of the
direct relation between velocity and frequency, this leads to a Gaussian frequency spectrum
[54]. This spectral model for chaff can be expressed as
where Pchaff is the total chaff power, fm is the mean chaff frequency and σB is the chaff
frequency standard deviation. The chaff frequency standard deviation is related to the chaff 3
dB bandwidth by (see Exercise 29)
where B(t) is the distance between 3 dB points of the chaff power spectrum given by (13.238).
To evaluate a radar’s response to chaff, we apply the chaff and target to the signal
processor in question, using the dynamic characteristics of the chaff. For an initial analysis,
we assume the track point stays centered on the aircraft. Utilizing models of the chaff and
signal processing we quantify ISCR, which is a function of the mismatch between the track
point and the chaff centroid. Given the spectral spread of the chaff model, we use numerical
integration to determine ISCR. Once the ISCR is determined, we can calculate the SCR at the
output of the processor. The general sequence of computation used here for a preliminary
chaff analysis is:
• Select time
• Compute chaff RCS from (13.236)
• Compute S/C input from S/CIN = σt/σC
• Compute chaff spectral width from (13.238) and f = 2V/λ
• Compute chaff center frequency from (13.239)
• Compute S/C improvement based upon signal processing
• Compute output S/C from S/CIN = ISCR(σt/σC)
• Repeat for next selected time

While more detailed analysis can be performed, the results of the procedure above generally
provide a case useful for initial chaff performance assessments.
Table 13.12
Chaff Parameters Used in Chaff Simulation
Parameter
Value
Pulsewidth
30 μs
LFM bandwidth
1 MHz
Target range rate
150 m/s
Initial chaff RCS
5 dBsm
Steady-state chaff RCS
30 dBsm
Chaff RCS time constant
1 sec
Initial chaff bandwidth
1 kHz
Final chaff bandwidth
250 Hz
Chaff bandwidth time constant
2 sec
Location of MTI notch
5 m/s
Initial chaff velocity
150 m/s
Final chaff velocity
5 m/s
Chaff velocity time constant
1 sec
Figures 13.44 and 13.45 contain plots from a simulation of the use of chaff against a radar
that uses MTI. For this example, the radar operates at 8 GHz and uses a staggered PRI
waveform with PRIs of 520 and 572 µs. It uses a 3-pulse MTI, with wind compensation used to
place the MTI notch at the final chaff velocity. The target RCS was set to 0 dBsm. The chaff
parameters used in the examples are listed in Table 13.12.
For Figure 13.44, the aircraft dispensed a single chaff cartridge (sometimes referred to as a
puff) and in Figure 13.45, the aircraft dispensed five chaff cartridges spaced one second apart.
For the single chaff cartridge, the slowdown of the chaff cloud caused the target and chaff to
separate and thus allowed the SCR to rise. We assumed the range gate of the radar remained
with the target.
For the multiple cartridge case, the chaff continued to obscure the target since it was
dispensing cartridges with a spacing equal to the various time constants associated with the
chaff model. Again, we assumed the range gate remained with the target.

Figure 13.44 MTI performance for single chaff cartridge.
Figure 13.45 S/C vs. time—cartridge ejected every second for 5 seconds—MTI.
An interesting extension of this study would be to include a range track loop model to see
what would be required for the chaff to cause the radar to break target lock and track the
chaff.
Figures 13.46 and 13.47 contain plots like those of Figures 13.44 and 13.45, for a radar that
uses a pulsed Doppler waveform and processor. For this example, the radar uses a 25 kHz,
semi-infinite pulsed Doppler burst with unmodulated, 4-µs pulses. The signal processor
consists of a fifth-order Butterworth HPF with a cutoff frequency of 1,000 Hz and a sixth-
order Butterworth BPF with a bandwidth of 800 Hz. The range gate and BPF are centered on
the target range and Doppler frequency, respectively. As illustrated in Figure 13.46, when one
chaff cartridge was used, the SCR rose very quickly and stopped obscuring the aircraft, in less
than 0.5 seconds. This is because the chaff and target separated in Doppler frequency very
quickly. Figure 13.47 shows the results of ten chaff cartridges dispensed every 0.2 seconds,
which extends the target masking due to chaff to about 2 seconds. This would seem to support
an assertion that a pulsed Doppler processor might be effective in countering chaff.

Figure 13.46 Pulsed Doppler performance for single chaff cartridge.
Figure 13.47 S/C vs. time—cartridge ejected every 0.2 seconds for 2 seconds—Pulsed Doppler.
13.5 EXERCISES
1.
Show that (13.3) and (13.4) provide reasonable approximations to the main beam of an
antenna radiation pattern. Show this for a linear array with uniform weighting and a
linear array with  = 6, 30-dB Taylor weighting.
2.
Derive (13.10).
3.
Reproduce the plot of Figure 13.2.
4.
Show that (13.13) satisfies (13.15).
5.
Reproduce the plot of Figure 13.4.
6.
Derive (13.19).
7.
Show that the integrals of (13.70) and (13.71) equal 1 for the case where VS(f) = C(f) =
T(f) = Φ(f) = d(f).

8.
Show the approximation, MF(f) = 1, is valid for the parameters of Example 1.
9.
Show the approximation, sin(πfT) ≈ πfT, is valid for clutter spectrum spread values
considered in this chapter.
10. Derive (13.101).
11. Derive (13.102) and (13.103).
12. Show that
for the case where H(f) is given by (13.74) and MF(f) is given by (13.45).
13. Repeat Example 1 and reproduce all of the plots.
14. Derive (13.126).
15. Derive (13.130).
16. Derive (13.136).
17. Implement the algorithm of Section 13.4.2.6 and reproduce Figure 13.19.
18. Show that
for all NMTI.
19. Use (13.43) to experimentally show (i.e., by simulation) that the noise spectrum at the
sampler output of Figure 13.26 is essentially constant. That is, that the noise is white. Use
a matched filter matched to an unmodulated, 1-µs pulse and sample periods of 10 and 100
µs.
20. Derive (13.175) and verify it by simulation using the parameters of Examples 2 and 3.
21. Reproduce the plot of Figure 13.32.
22. Repeat Example 2 and produce the plots of Figures 13.33 and 13.34.
23. Verify the statements in the paragraph above Section 13.4.3.5.
24. Repeat Example 3 and produce the plots of Figures 13.35, 13.37, and 13.38.
25. Repeat Exercise 23 using the integrals of Table 13.9, rather than the rules of thumb.
26. Derive (13.194) and (13.196).
27. Repeat Example 4 and reproduce Figure 13.43.
28. Use (13.239) to derive the equation for the range to the chaff centroid.

29. Derive the relationship between the standard deviation and the 3 dB bandwidth of the
power spectrum of (13.242).
30. Implement the algorithm of Section 13.4.2.9 for an MTI processor and generate a plot like
Figure 13.45. Use a target range rate of 300 m/s and 1.2 s between chaff puffs or else use
the parameters listed in Table 13.12. Generate plots of chaff RCS, bandwidth, velocity,
and center frequency as a function of time. How fast should chaff be dispensed to
improve performance?
31. Using a target range rate of 300 m/s, implement the pulsed Doppler processor described
in the example and generate a plot like Figure 13.47. Use the chaff parameters in Table
13.12. Generate plots of chaff RCS, bandwidth, velocity, and center frequency as a
function of time. How fast should chaff be dispensed to mask the aircraft for two
seconds? How many chaff bundles are necessary?
References
[1]
Barton, D. K., Radar System Analysis and Modeling, Norwood, MA: Artech House, 2005.
[2]
Skolnik, M. I., Introduction to Radar Systems, 3rd ed., New York: McGraw-Hill, 2001.
[3]
Lijun, W., L. Feng, and W. Shunjun, “An Improved Design and Practical Application of MTD,” Fire Control Radar
Technology, vol. 34, no. 1, Mar. 2005, pp. 9–12, 25.
[4]
Kun, H., et al., “Design and Implementation of Doppler Filter Bank in MTD Radar,” Fire Control Radar Technology,
vol. 35, 2006, pp. 57–59.
[5]
GuoRong, H., et al., “ASICs Design for an MTD Radar,” 2nd Int. Conf. ASIC, 1996, Shanghai, China, Oct. 21–24,
1996, pp. 69–72.
[6]
Yanhang, L. I., and W. Xuegang, “Design of Radar MTD Based on ADSP-TS101,” Commun. Inf. Technol., no. 5, 2007,
pp. 66–68.
[7]
Yan-ping, L. I., “Radar Moving Target Detection System Based on Single Chip FPGA,” Shipboard Electron.
Countermeasure, vol. 31, no. 1, Feb. 2008, pp. 78–81.
[8]
Gillespie, N. R., J. B. Higley, and N. MacKinnon, “The Evolution and Application of Coherent Radar Systems,” IRE
Trans. Mil. Electron., vol. 5, no. 2, Apr. 1961, pp. 131–139.
[9]
Barton, D. K., “A Half Century of Radar,” IEEE Trans. Microwave Theory and Techniques, vol. 32, no. 9, Sept. 1984,
pp. 1161–1170.
[10] Emslie, A. G., and R. A. McConnell, “Moving Target Indication,” in Radar Systems Engineering, vol. 1 of MIT
Radiation Lab. Series (L. N. Ridenour, ed.), New York: McGraw-Hill, 1947; Norwood, MA: Artech House (CD-ROM
edition), 1999.
[11] Emslie, A. G., “MTI Using Coherent IF,” MIT Radiation Lab. Series, Rep. No. 104, Aug. 22, 1945. Listed as reference in
Radar Systems Engineering, vol. 1 of MIT Radiation Lab. Series (L. N. Ridenour, ed.), New York: McGraw-Hill, 1947,
pp. 640–645; Norwood, MA: Artech House (CD-ROM edition), 1999.
[12] Emslie, A. G., “Moving Target Indication on MEW,” MIT Radiation Lab. Series, Rep. No. 1080, Feb. 19, 1946. Listed as
a reference in Radar Systems Engineering, vol. 1 of MIT Radiation Lab. Series (L. N. Ridenour, ed.), New York:
McGraw-Hill, 1947, p. 645; Norwood, MA: Artech House (CD-ROM edition), 1999.
[13] Emslie, A. G., “Moving Object Radio Pulse System,” U.S. Patent 2,543,448, Feb. 27, 1951.
[14] Emslie, A. G., “Moving Target Indication Radar System,” U.S. Patent 2,555,121, May 29, 1951.
[15] Emslie, A. G., “Moving Target Detecting System,” U.S. Patent 2,617,983, Nov. 11, 1952.
[16] Emslie, A. G., “Moving Object Radio Pulse-Echo System,” U.S. Patent 2,659,076, Nov. 10, 1953.
[17] Emslie, A. G., “Moving Object Radio Pulse-Echo System,” U.S. Patent 2,659,077, Nov. 10, 1953.

[18] Emslie, A. G., “Moving Target Indicating Radar System,” U.S. Patent 2,710,398, Jun. 7, 1955.
[19] University of Maryland, “Innovation Hall of Fame,” www.eng.umd.edu/html/ihof/inductees/smith.html.
[20] IEEE, “1984 Pioneer Award,” IEEE Trans. Aerosp. Electron. Syst., vol. 20, no. 31, May 1984, pp. 290–291.
[21] Perkins, L. C., H. B. Smith, and D. H. Mooney, “The Development of Airborne Pulse Doppler Radar,” IEEE Trans.
Aerosp. Electron. Syst., vol 20, no. 3, May 1984, pp. 292–303.
[22] Fell, T. T., et al., “Pulse Doppler Radar System,” U.S. Patent 3,011,166, Nov. 28, 1961.
[23] Smith, H. B., D. H. Mooney, Jr., and W. Ewanus, “Pulse Doppler Radar System,” U.S. Patent 3,023,409, Feb. 27, 1962.
[24] IEEE, “2005 Pioneer Award,” IEEE Trans. Aerosp. Electron. Syst., vol. 42, no. 3, Jul. 2006, pp. 1171–1172.
[25] Stone, M., “2005 Pioneer Award to C.E. Muehe: Introductory Remarks,” IEEE Trans. Aerosp. Electron. Syst., vol. 42,
no. 3, Jul. 2006, pp. 1173–1176.
[26] Muehe, C. E., “The Moving Target Detector,” IEEE Trans. Aerosp. Electron. Syst., vol. 42, no. 3, Jul. 2006, pp. 1177–
1181.
[27] Bassford, R. S., W. Goodchild, and A. De La Marche, “Test and Evaluation of the Moving Target Detector (MTD)
Radar,” Lincoln Laboratories, Rep. No. FAA-RD-77-118, 1977. Available from DTIC as ADA047887.
[28] Budge, M. C., Jr., “EE 725: Advanced Radar Technique,” www.ece.uah.edu/courses/material/EE725/index.htm.
[29] Barton, D. K., Radar Equations for Modern Radar, Norwood, MA: Artech House, 2013.
[30] Nathanson, F. E., J. P. Reilly, and M. N. Cohen, Radar Design Principles: Signal Processing and the Environment, 2nd
ed., New York: McGraw-Hill, 1991.
[31] Skolnik, M. I., ed., Radar Handbook, 3rd ed., New York: McGraw-Hill, 2008.
[32] Mahafza, B. R., Radar Signal Analysis and Processing Using MATLAB, New York: CRC Press, 2008.
[33] Blake, L. V., Antennas, Dedham, MA: Artech House, 1984.
[34] Schelkunoff, S. A., Advanced Antenna Theory, New York: Wiley & Sons, 1952.
[35] Stutzman, W. L., and G. A. Thiele, Antenna Theory and Design, New York: Wiley & Sons, 1981.
[36] Barton, D. K., ed., Radars, Vol. 5: Radar Clutter (Artech Radar Library), Dedham, MA: Artech House, 1975.
[37] Barton, D. K., “Ground Clutter Model,” Raytheon Memo 7101-80-159, Feb. 8, 1980 (rev. Apr. 3, 1980).
[38] Barton, D. K., “Flat and Rolling Terrain Clutter Model,” Raytheon Memo 7011-82-169, May 17,
[39] Billingsley, J. B., Low-Angle Radar Land Clutter: Measurements and Empirical Models, Norwich, NY: William Andrew,
2002.
[40] Feng, S., and J. Chen, “Low-Angle Reflectivity Modeling of Land Clutter,” IEEE Geoscience and Remote Sensing Lett.,
vol. 3, no. 2, Apr. 2006, pp. 254–258.
[41] Eaves, J. L., and E. K. Reedy, Principles of Modern Radar, New York: Van Nostrand Reinhold, 1987.
[42] Ulaby, F. T., and M. C. Dobson, Handbook of Radar Scattering Statistics for Terrain (Artech House Remote Sensing
Library), Norwood, MA: Artech House, 1989.
[43] Barton, D. K., Modern Radar System Analysis, Norwood, MA: Artech House, 1988.
[44] Long, M. W., Radar Reflectivity of Land and Sea, 3rd ed., Norwood, MA: Artech House, 2001.
[45] Barton, D. K., “Land Clutter Models for Radar Design and Analysis,” Proc. IEEE, vol. 73, no. 2, Feb. 1985, pp. 198–
204.
[46] Kerr, D. E., ed., Propagation of Short Radio Waves, vol. 13 of MIT Radiation Lab. Series, New York: McGraw-Hill,
1951. Reprinted: Norwood, MA: Artech House (CD-ROM edition), 1999.
[47] Bean, B. R., and G. D. Thayer, “Models of the Atmospheric Radio Refractive Index,” Proc. IRE, vol. 47, no. 5, May
1959, pp. 740–755.
[48] Bean, B. R., “The Radio Refractive Index of Air,” Proc. IRE, vol. 50, no. 3, Mar. 1962, pp. 260– 273.
[49] Doerry, A. W., “Earth Curvature and Atmospheric Refraction Effects on Radar Signal Propagation,” Sandia Nat. Labs.,
Albuquerque, NM, Rep. No. SAND2012-10690, Jan. 2013.
[50] Skolnik, M. I., ed., Radar Handbook, 2nd ed., New York: McGraw-Hill, 1990.

[51] Billingsley, J. B., et al., “Impact of Experimentally Measured Doppler Spectrum of Ground Clutter on MTI and STAP,”
Radar 97 (Conf. Publ. No. 449), Edinburgh, Scotland, Oct. 14–16, 1997, pp. 290–294.
[52] Greco, M., et al., “Analysis of Clutter Cancellation in the Presence of Measured L-band Radar Ground Clutter Data,”
Rec. IEEE 2000 Int. Radar Conf., 2000, Alexandria, VA, May 7–12, 2000, pp. 422–427.
[53] Lombardo, P., et al., “Impact of Clutter Spectra on Radar Performance Prediction,” IEEE Trans. Aerosp. Electron. Syst.,
vol. 37, no. 3, Jul. 2001, pp. 1022–1038.
[54] Papoulis, A., Probability, Random Variables, and Stochastic Processes, 3rd ed., New York: McGraw-Hill, 1991.
[55] Davenport, W. B., Jr., and W. L. Root, An Introduction to the Theory of Random Signals and Noise, New York:
McGraw-Hill, 1958.
[56] IEEE Aerospace and Electronic Systems Society and IEEE New Hampshire Section, “Free Video Course in Radar
Systems Engineering,” (R. M. O’Donnell, lecturer). aess.cs.unh.edu.
[57] Raven, R. S., “Requirements on Master Oscillators for Coherent Radar,” Proc. IEEE, vol. 54, no. 2, Feb. 1966, pp. 237–
243.
[58] Raven, R. S., “Correction to ‘Requirements on Master Oscillators for Coherent Radar,’” Proc. IEEE, vol. 55, no. 8, Aug.
1967, p. 1425.
[59] Scheer, J. A., and J. L. Kurtz, eds., Coherent Radar Performance Estimation, Norwood, MA: Artech House, 1993.
[60] Rogers, R. G., Low Phase Noise Microwave Oscillator Design (Artech Microwave Library), Norwood, MA: Artech
House, 1991.
[61] Lee, T. H., and A. Hajimiri, “Oscillator Phase Noise: A Tutorial,” IEEE J. Solid-State Circuits, vol. 35, no. 3, Mar. 2000,
pp. 326–336.
[62] Boroditsky, R., and J. Gomez, “Ultra Low Phase Noise 1 GHz OCXO,” IEEE Int. Frequency Control Symp., 2007 Joint
with the 21st European Frequency and Time Forum, Geneva, Switzerland, May 29–June. 1, 2007, pp. 250–253.
[63] Poddar, A. K., and U. L. Rohde, “The Pursuit for Low Cost and Low Phase Noise Synthesized Signal Sources: Theory &
Optimization,” 2014 IEEE Int. Frequency Control Symp. (FCS), Taipei, Taiwan, May 19–22, 2014, pp. 1–9.
[64] Hoover, L., H. Griffith, and K. DeVries, “Low Noise X-band Exciter Using a Sapphire Loaded Cavity Oscillator,” 2008
IEEE Int. Frequency Control Symp. (FCS), Honolulu, HI, May 19–21, 2008, pp. 309–311.
[65] Wenzel 
Associates, 
Inc., 
“Low 
Noise 
Crystal 
Oscillators 
> 
Sorcerer 
II.” 
www.wenzel.com/wp-
content/uploads/Sorcerer-II.pdf.
[66] Wenzel Associates, Inc., “Low Noise Crystal Oscillators > Golden MXO (PLO w/Dividers).” www.wenzel.com/wp-
content/uploads/GMXO-PLD.pdf.
[67] Budge, M.C., Jr., and S. M. Gilbert, “Timing Jitter Spectrum in Pulsed and Pulsed Doppler Radars,” Proc. IEEE
Southeastcon ’93, vol. 4, Apr. 4–7, 1993.
[68] Budge, M.C., Jr., “Timing Jitter Characterization for Pulsed and Pulsed and Pulsed Doppler Radars,” Proc. IEEE
Southeastcon ’92, vol.1, Apr. 12–15, 1992, pp. 199–201.
[69] Skolnik, M. I., Introduction to Radar Systems, 2nd ed., New York: McGraw-Hill, 1980.
[70] Budge, M. C., Jr., “EE 725: Advanced Radar Technique,” www.ece.uah.edu/courses/material/EE725/index.htm.
[71] Tsui, J., Digital Techniques for Wideband Receivers, 2nd ed., Norwood, MA: Artech House, 2001.
[72] Kester, W., ed., The Data Conversion Handbook, New York: Newnes, 2005.
[73] Meikle, H., Modern Radar Systems, 2nd ed., Norwood, MA: Artech House, 2008.
[74] Chrzanowski, E. J., Active Radar Electronic Countermeasures, Norwood, MA: Artech House, 1990.
[75] Schleher, D.C., Introduction to Electronic Warfare, Dedham, MA: Artech House, 1986.
[76] Naval Air Warfare Center Weapons Division (NAWCWD), Electronic Warfare and Radar Systems Engineering
Handbook, 4th ed., NAWCWD Tech. Commun. Office, Point Mugu, CA, Rep. No. NAWCWD TP 8347, Oct. 2013.
[77] Budge, M. C., Jr. and S. R. German, “The Effects of an ADC on SCR Improvement,” IEEE Trans. Aerosp. Electron.
Syst., vol. 49, no. 4, Oct. 2013, pp. 2463–2469.
[78] Budge, M. C., Jr., and M. P. Burt, “Range Correlation Effects on Phase and Amplitude Noise,” IEEE Proc. Southeastcon
’93, Apr. 1993.

[79] Budge, M. C., Jr., and M. P. Burt, “Range Correlation Effects on Phase Noise Spectra,” in Proc. SSST 25th Southeastern
Symp. Syst. Theory, 1993, pp. 492–496.
[80] Leeson, D. B., “A Simple Model of Feedback Oscillator Noise Spectrum,” Proc. IEEE, vol. 54, no. 2, Feb. 1966, pp.
329–330.
[81] Poore, R., “Overview on Phase Noise and Jitter,” May 17, 2001. www.agilent.com.
[82] D’Azzo, J. J., and C. H. Houpis, Linear Control System Analysis and Design: Conventional and Modern, New York:
McGraw-Hill Book Company, 1981.
[83] Rabiner, L. R., and B. Gold, Theory and Application of Digital Signal Processing, Englewood Cliffs, NJ: Prentice-Hall,
1975.
[84] Oppenheim, A. V., and R. W. Schafer, Discrete-Time Signal Processing, Englewood Cliffs, NJ: Prentice-Hall, 1989.
[85] Schleher, D. C., Electronic Warfare in the Information Age, Norwood, MA: Artech House, 1999.
[86] Napier, I. M., and Thompson, I. L., “Explosive Generation of Chaff,” Defence Research Centre Salisbury South Australia,
Jun. 1979. Available from DTIC as ADC020069.
[87] Golden, A., Radar Electronic Warfare, New York: AIAA, 1987.
[88] Rohrs, R. J., “An Empirical Self-Protection Chaff Model Thesis,” Air Force Institute of Technology, Wright Patterson Air
Force Base, Ohio, Rep. No. 84D-54, 1984. Available from DTIC as AD-A151 928.
[89] Schlesinger, R. J., Principles of Electronic Warfare, Englewood Cliffs, NJ: Prentice-Hall, 1961.
[90] Cassedy, E. S., and J. Fainberg, “Back Scattering Cross Sections of Cylindrical Wires of Finite Conductivity,” IRE Trans.
Antennas Propag., vol. 8, no. 1, Jan. 1960, pp. 1–7.
[91] Mack, C. L., Jr., and B. Reiffen, “RF Characteristics of Thin Dipoles,” Proc. IEEE, vol. 52, no. 5, pp. 533–542, May
1964.
[92] Barlow, E. J., “Doppler Radar,” Proc. IRE, vol. 37, no. 4, Apr. 1949, pp. 340–355.
[93] Gradshteyn, I. S., and I. M. Ryzhik, Table of Integrals, Series, and Products, 8th ed., (D. Zwillinger and V. Moll, eds.)
New York: Academic Press, 2015. Translated from Russian by Scripta Technica, Inc.
APPENDIX 13A: DERIVATION OF (13.43)
We start the derivation at (13.40), which is
For the first step, write the product of the last two terms as
where Δϕ(t) = ϕ(t−τd) − ϕ(t). Δϕ(t) represents the total (transmit and receive) phase noise in the
radar. We note that Δϕ(t) is small relative to unity so that [57, 58]
With this, vm(t) becomes
Note that we dropped the phase term, exp(−jωc τd). We were able to do this because we can
normalize it away in future calculations.

We further simplify (13A.3) by shifting the time origin by −τd. This yields
We argued earlier that vS(t) changes slowly relative to τd so that vS2(t + τd) ≈ vS2(t). Also,
vobj(t) and Φ(t) are WSS random processes. This means their means and autocorrelations do
not depend on time origin. Thus, we can replace vobj(t + τd/2) with vobj(t) and Φ(t + τd) with
Φ(t) and not change their means and autocorrelations [the autocorrelation is what we
eventually use to find the power spectrum of vm(t)]. With this, we get
where
We dropped the prime and reverted to the notation vm(t) for convenience.
The next step in our derivation is to process vm(t) through the matched filter and then
sample the matched filter output via the sampler/ADC (see Figure 13.6). Before we do this, we
need to examine vm(t) more closely. If we substitute for vp(t) [see (13.25)] into (13A.5), we get
Since vobj(t) and Φ(t) are random processes, the product r(t) = vS2(t)vobj(t)Φ(t) is also a
random process. However, because vS(t) is periodic, r(t) is not WSS, though we show in
Appendix 13B that r(t) is wide sense cyclostationary (WSCS). As a result of this, we can use
the averaged statistics of r(t) and treat it as a WSS process in the following development. With
this we write
where we treat r(t) as if it was a WSS random process. We note that vm(t) is not stationary
because of the p(t – kT) term. We address this in the following discussions.
If we represent the impulse response of the matched filter as m(t), we can write the output of
the matched filter as [28]

We normally derive m(t) by saying the matched filter is matched to some signal q(t).
Recalling matched filter theory, this means we can write
As a reminder, the matched filter is termed a single-pulse matched filter. The matched filter is
often matched to the transmit pulse, p(t), in which case we would use.
When p(t) is an LFM pulse, m(t) could include an amplitude taper to reduce range sidelobes.
In that case q(t) will not exactly equal p(t). In the remainder of this derivation we will use the
more general form of (13A.10).
Substituting (13A.10) into (13A.9) yields
or
where we replaced the convolution notation (*) by the integral it represents.
Figure 13A.1 contains depictions of |vm(τ)|, |q*(τ − t)| and |vMF(t)| for the case where p(t) is
an unmodulated pulse and m(t) is matched to p(t) [i.e., q(t) = p(t)]. As expected from matched
filter theory, vMF(t) is a series of triangle shaped pulses whose amplitudes depend upon r(t).
Since vm(t) is a nonstationary random process, so is vMF(t). This makes vMF(t) difficult to
deal with since we do not have very sophisticated mathematical tools and procedures that
allow us to efficiently analyze nonstationary random processes. Fortunately, because of the
sampler/ADC, we do not need to deal directly with vMF(t). We will only work with samples of
vMF(t).
r(t) is a WSCS random process with an averaged autocorrelation of Rr(τ) and
corresponding power spectral density of Sr(f).
For now, we assume the sampler/ADC samples the output of the matched filter, vMF(t), once
per PRI, T, at the peak of the matched filter response. We also, without loss of generality,
assume the matched filter peaks occur at t = lT. With this we can write the output of the
sampler/ADC as

Figure 13A.1 Depictions of |vm(τ) | (top plot), |q*(τ – t) | (center plot), and |vMF(t) | (bottom plot).
If we assume p(t) and q(t) are of the form
and τp < T/2, then all of the terms of the summation of (13A.14) are zero except for the case
where k = l. With this (13A.14) reduces to
or, with the change of variables from l back to k
To find the power spectrum of vo(k), we must first show vo(k) is WSS. To that end we form

From previous discussions we note that
where the overbar denotes the averaged expected value (see Appendix 13B). Using this for the
expectation in (13A.18) gives
By making use of
we can write
We now make the change of variables, α = τ − t, dα = dτ to write

We next make the change of variables β = α + t – k1T, dβ = dα and get
Rearranging yields
For the next change of variables we let γ = t – k2T, dγ = dt to yield
The first thing we note about (13A.26) is the right side is a function of k1 – k2. This
constitutes the proof that vo(k) is WSS. The next thing we note is that the two integrals in the
brackets are conjugates of each other. Finally, from ambiguity function theory, we recognize
that we can write the product of the integrals as
where χpq(0,f) is the matched-range, Doppler cut of the cross ambiguity function of p(t) and

q(t). In the remainder we will use the notation χpq(0,f) = MF(f). With all of the statements in
this paragraph, we can write
We next want to find the power spectrum of vo(k). We could do this by taking the discrete-
time Fourier transform of Ro(k). However, the math associated with this will probably be quite
involved. We will take an indirect approach.
Let v(t) be a WSS random process with an autocorrelation of R(τ) and a power spectrum of
S(f). Further assume that we can sample v(t) to get vo(k). That is
vo(k) is the same as the random process defined by (13A.14). From random processes theory
we can write
Further, from the theory of discrete-time signals and their associated Fourier transforms, if
S(f) is the power spectrum of v(t) the power spectrum of vo(k) is
From this same theory we can write
If we substitute (13A.31) into (13A.32), we get
or

We now make the change of variables x = f – l/T to get
where we made use of ej2πkl = 1.
We recognize that the last term is an infinite summation of integrals over nonoverlapping
intervals, and that the total of the nonoverlapping intervals cover the range of x ∊ (−∞,∞).
With this we can write
where we made the change of variables, x = f.
If we compare (13A.36) to (13A.28), we have
With this and (13A.31) we arrive (13.43). That is,
Since we will need it later, we note that we can write the power in vo(k) as
APPENDIX 13B: PROOF THAT r(t) IS WIDE-SENSE
CYCLOSTATIONARY
In this appendix, we show that the process [28]

is wide-sense cyclostationary (WSCS). To show that r(t) is WSCS, we must show
for some TSCAN. That is, we must show that the autocorrelation of r(t) is a periodic function of
t.
We recall C(t) and Φ(t) are WSS random processes. Thus the product C(t)Φ(t) is also WSS.
The function vs2(t) is a deterministic function and is periodic with a period of TSCAN where
TSCAN is the scan period of the antenna. If we form
we get
where we made use of the fact that C(t) and Φ(t) are independent and WSS. In a similar
fashion, we can write
However, since vS2(t) is periodic with a period of TSCAN, we have
and
which says r(t) is WSCS.
From the theory of WSCS random processes [54], we can use the averaged autocorrelation
of r(t) to characterize the average behavior of r(t). Specifically, in place of Rr(t,τ) we use
where the integral notation means to perform the integration over one period of Rr(t,τ). As a
note, (13B.8) shows a system will respond on average to r(t) in the same manner as a WSS
process that has the autocorrelation 
. We will dispense with the overbar and use the

notation Rr(τ).
APPENDIX 13C: DERIVATION OF (13.170)
In this appendix, we present the equations necessary to compute (13.170). More specifically,
we derive an equation for
We begin by examining the clutter spectrum at the input to the matched filter. The equation for
that spectrum is
We included R in the argument of SC(f) and ΦΔϕ(f) to acknowledge that the spectrum is a
function of range. PC(R) is the total clutter power at the input to the matched filter for a single,
point clutter source at a range of R.
If we ignore the R4 attenuation for clutter past the radar horizon (which is reasonable
because the predominant contributors to PC(R), for pulsed Doppler waveforms, are at ranges
close to the radar), PC(R) is of the form
With this we get
To obtain the contribution of the clutter in the range region of interest, 
, we integrate
SC(f,R) over 
. For pulsed Doppler waveforms, 
 is defined by
where ΔR = cT/2 is the range equivalent of the PRI and δR is the range resolution of the pulses

of the burst. R0 is the range to the front of the clutter cell closest to the radar. With this we get
where we anticipated the final answer and wrote
From (13.137) we have
With this we get
and
To evaluate this integral, we use the trigonometric identity sin2θ = (1 – cos2θ)/2 and write
Evaluation of the first integral is simple. The basis for evaluating the second integral is
(2.639.2) in [93, p. 187]

where
is the cosine integral is (8.230.2) in [93, p. 928]
After some manipulation,
with
and
Figure 13C.1 contains a plot of HR(f,R0) for R0 = 700 m, ∆R = 1,500 m and δR = 150 m. It
also contains the approximation
that fits the exact curve well.

Figure 13C.1 P lots of (13C.14) and (13C.17) for R0 = 700 m, ∆R = 1,500 m and δR = 150 m.
1 This is valid for ground-based radars. However, for airborne radars, there will be clutter returns from the entire annulus.
Further, the Doppler frequency will vary around the annulus. For an example of this, see Example 4 of Chapter 16.
2 Sea state 4 is termed a moderate sea state. It is associated with wave heights of 1.25 to 2.5 m.
3 David Barton pointed out that the model of (13.16) assumes uniform illumination across the ellipse of Figure 13.3. To account
for the fact that the illumination is not uniform, π/4 should be replaced by 1/1.77. This ratio is based on a Gaussian beamshape.
4 Nathanson uses a factor of 0.42 instead of 0.3. However, in his model, εB is a two-way beamwidth rather than the one-way
beamwidth used in (13.23). The value 0.3 is approximately 0.42/(2½).
5 Another factor that affects signal processor performance is timing jitter [67, 68]. It is also normally ignored, but could
become a limiting factor as the phase noise of STALOs continues to improve.
6 A note about stationarity: Realistically, none of the random processes we are dealing with are truly WSS. However, over the
CPI, we can reasonably assume they are stationary (actually, cyclostationary). From random processes theory, we know that if
a process is stationary in the wide sense, over a CPI, then we can reasonably assume that it is WSS. This stems from the fact
that we are interested only in the random process over the CPI.
7 The validity of this as the spectrum due to scanning is questionable. However, we use it anyway because Gaussian form of
VS(f) convolved with the exponential clutter model does not lead to an easily computed closed form expression for G.
8 This is a valid assumption for narrow pulses. However, for long, modulated pulses and large Doppler frequencies, it may be
necessary to include the MF(fd) term.
9 David Barton indicated it would be worth noting that use of the exponential clutter model is important when values of CA ≫
20 dB are needed.
10 Had the burst been semi-infinite, we would have used K = 2, the length of the PRI sequence.
11 This is not the case for LPRF pulsed Doppler radars since LPRF waveforms are unambiguous in range.
12 Some analysts ascribe the terms clutter transients or clutter fill time to signal processor transients.
13 Actually, the “receiver” noise also contains environment noise as discussed in Chapters 2 and 4.
14 This target trajectory is obviously unrealistic. However, it is an assumption used in some search radar analyses. An alternate

would be the more realistic assumption that the target is flying toward the radar at a constant altitude.
15 This is somewhat of a “soft” lower limit. In some instances, we could use a BPF with a very small bandwidth and not be
concerned with the filter response reaching steady state. Such a filter is sometimes termed a bandpass integrator.
16 The filter defined by (13.149) is ideal in that it only has one passband at fd rather than passbands at ±fd. Such a filter could
be built with digital hardware that allows complex filter coefficients.
17 ADC full scale has been normalized to 0 dB in (13.189).
18 Typically, chaff clouds are designed to cover the frequency bands associated with the intended victim radars. This is
accomplished by mixing dipoles within the cloud that are cut to the appropriate lengths so that resonance occurs at each of the
victim radar frequencies.
19 Some chaff dispensers rely on air turbulence generated by the aircraft to disperse chaff packets. Others use small
pyrotechnic charges shortly after the chaff leaves the aircraft. One explosive technique achieved an average time to bloom as
short as 12 ms [17].

Chapter 14
Radar Receiver Basics
14.1 INTRODUCTION
The general function of a radar receiver is to amplify, filter, shift frequency, and demodulate
signal returns without distortion of the waveform modulation. The objective of a radar
receiver is to facilitate discrimination between desired signals and unwanted noise and
interference such as galactic noise, receiver noise, other radars, jamming, and clutter. In
doing so, two principal design goals for a receiver are to ensure sufficient sensitivity to detect
weak returns and to ensure adequate dynamic range to operate linearly for all expected return
power levels.
There have been a number of receiver configurations used for radar over the decades (e.g.,
crystal, superregenerative, tuned RF, homodyne, heterodyne). However, the focus of this
chapter is the superheterodyne receiver.1 While the earliest receivers were entirely analog,
receiver technology is rapidly trending more digital, less analog. In this chapter, we consider
a superheterodyne receiver employing direct IF sampling.
Major Edwin Howard Armstrong invented the superheterodyne receiver in 1918 while
serving with the U.S. Army Signal Corps during World War I [1–3].2 Since then, the
superheterodyne receiver has become predominant and is employed in virtually all radar
receivers. For this and other contributions to the art of radio electronics, Armstrong was the
first recipient of the Institute of Radio Engineers’ (IRE) Medal of Honor in 1917 and was later
awarded the American Institute of Electrical Engineers’ (AIEE) Edison Medal in 1942 [4].
The superheterodyne receiver is based on the heterodyne principle, invented by Reginald
Aubrey Fessenden in 1902 [5]. Heterodyning is the process of combining or mixing signals
with carrier frequencies of f1 and f2 to generate two other signals with frequencies of f1 + f2
(sum frequency) and f1 –f2 (difference frequency). Fessenden coined the term “heterodyne”
from the Greek words for difference (hetero) and force (dyne) [7]. In early heterodyne
receivers, the difference frequency was in the audible range so a telephone or telegraph signal
could be heard in a headset [8].
Armstrong’s superheterodyne receiver design was made possible by the first triode vacuum
tube, called an audion, invented by Lee De Forest in 1906 [9–12]. The audion was the first
vacuum tube device that could both detect and amplify signals [13, 14]. In contrast to the audio
signal used by Fessenden’s heterodyne receiver, Armstrong’s superheterodyne uses a
comparatively high (inaudible) frequency he termed the intermediate receiver frequency.
Armstrong used the moniker “super” because of the supersonic IF.

14.2 SINGLE-CONVERSION SUPERHETERODYNE RECEIVER
A block diagram of a basic, single-conversion, superheterodyne receiver found in radars is
given in Figure 14.1. Such a block diagram is sometimes referred to as a receiver chain. In
current receiver vernacular, the components from the receiver input to the input to the
detector are referred to as the radio frequency (RF) front end. Earlier usage of the term
stopped at the output of the first mixer. Single-conversion receivers are typically used in
radars with bandwidth less than 20 MHz with limited tuneable bandwidth.
Ideally, the receiver input consists of signals from targets of interest. One problem is there
is a chance unwanted signals will also be present in the received signal. A bandpass filter
(BPF), called a “preselector,” is used to preserve the desired signal while eliminating
unwanted signals (interference) such as those from other radars, jammers, or other sources of
“out-of-band” energy [15].
The preselector is a low-loss device (typically < 1 dB), such as a cavity or waveguide filter,
to minimize its impact on system noise figure. The preselector is usually low order, typically
2nd to 5th, to minimize unwanted ringing and overshoot.3 Also, low-order filters generally
have lower insertion loss, have less sensitivity to temperature, are less costly, and are smaller
and lighter than higher order filters.
Figure 14.1 Superheterodyne receiver with amplitude detection.
The attenuators shown in Figure 14.1 are used to extend receiver dynamic range by
reducing the power level of large returns in order to prevent saturating subsequent receiver
components. The total attenuation necessary is typically distributed between RF and IF, as
shown in Figure 14.1. The exact amount of attenuation required, and how it is distributed in a
receiver chain, is determined via cascade analysis (see Section 14.7).
Controlling the amount of attenuation is generally accomplished using automatic gain
control AGC or sensitivity time control (STC) circuitry. AGC monitors the power level of a
tracked target signal and adjusts receiver gain to establish a desired constant power level at
the detector output.4 STC uses an attenuation profile unrelated to target presence. When
considering AGC, STC, and dynamic range, Barton notes that the following general rules
apply:
• A target echo that saturates the receiver remains detectable in search radar, while angle
tracking receivers (including both sum and difference channels in monopulse radar) must
avoid saturation by the target to be tracked or measured.
• Clutter must not saturate the receiver or exceed the linear region in either search or

tracking radars if high clutter attenuation is required. With modern digital signal
processing, receiver gain control using a high-resolution clutter map offers an effective
method of avoiding clutter nonlinearity on clutter peaks with minimal loss of target
detections.
• Search radars cannot use AGC other than from a high-resolution clutter map because
detection is required on small targets in the same beam as the strong signal on which
conventional AGC is based.
• STC can be used in both search and tracking radars with LPRF waveforms (and possibly
with MPRF if the STC action is limited to a small fraction of the PRI), never in HPRF
radar. Receiver saturation on short-range clutter extends the region after the transmission
in which targets are suppressed (eclipsed).
• A tracking radar can combine STC, AGC, and transmitter power variation to keep the
target signal within the linear region, since echoes from ranges beyond the target need not
be detectable.
When a low-PRF waveform is used, STC circuitry can be used to extend receiver dynamic
range. STC reduces gain over the initial portion of the PRI according to an STC law,
restoring full gain for the remainder of the PRI. The type of gain variation depends on the
particular application. For example, when targets are expected to dominate the short-range
returns, the law is 1/R4; if surface clutter is expected to dominate, the law is 1/R3 [16–18].
Figure 14.2 STC attenuation profile for 1/R4 STC law.
The portion of each PRI during the beam dwell over which STC is applied affects more
than just a few µs (or meters of range), but typically tens of µs or km in range. The STC law
must be established for a given application and applied without knowledge of target presence
or power in any given beam dwell.
For illustration, Figure 14.2 shows a 1/R4 STC law designed to operate from 1.5 km to 10
km. The attenuation is held constant at 33 dB for the first 1.5 km and then decreases to 0 dB at
10 km. A 1/R4 STC law results in the receiver output power for targets being range
independent (within the range STC is active). For a 1/R3 STC law, receiver output power for
clutter becomes range independent.
The RF amplifier of Figure 14.1 is a low noise amplifier (LNA) used to establish the
receiver noise figure. The receiver noise figure is also influenced by prior lossy components

before and stages after the LNA. Lossy components prior to the LNA increase the noise figure
by their insertion loss, dB for dB. This makes it important to minimize losses prior to the
LNA.
The LNA also has fairly high gain so that, ideally, devices following the RF LNA do not
significantly contribute to the overall receiver noise figure (see Section 4.6). For higher noise
figure systems, an LNA gain of 20–25 dB is usually sufficient to ensure this. However, while
the intent may be to allow negligible contributions from the stages following the LNA, this is
not always realistic (especially when LNA technology achieves noise figures of 1 or 2 dB).
Early radars did not use RF amplifiers and, as a result, their noise figure was set by the
mixer and components after the mixer. Skolnik notes that early receivers achieved noise
figures of 12 to 15 dB, with 1960s vintage receivers having typical noise figures of 7 to 8 dB
[19]. The noise figure of modern LNAs is typically in the range of 1 to 5 dB, and continues to
fall as technology improves.
The RF LNA in older systems is usually tube-based technology, for example, traveling
wave tube (TWT), backward wave amplifier (BWA), electrostatic amplifier (ESA), cyclotron
wave electrostatic amplifier (CWESA), and electrostatic combined amplifier (ESCA).5 Tube
amplifiers tend to be robust; some, such as the CWESA and ESCA, are essentially self-
protected from overload [19–23]. The self-protecting nature of cyclotron devices led to
production by ISTOK6 of the cyclotron protective device (CPD), functioning as receiver
protection rather than LNA [24–27].
Radars are trending toward solid-state LNAs such as bipolar junction transistor (BJT),
gallium arsenide (GaAs), and gallium nitride (GaN). Solid-state LNAs require more care to
protect from overload, but generally have a lower noise figure. The overall noise figures and
gains of tube amplifiers versus solid state LNAs preceded by diode limiter overload
protection circuitry are similar.
Solid-state LNAs must not only be protected from overload but from destruction by
leakage from the transmitter. Whether this protector is considered part of the receiver or
assigned to the duplexer, the use of a solid-state limiter is the current practice, and its loss
must be included in calculating radar system noise.
The mixer is a nonlinear device used as a frequency heterodyne to translate the signal from
the incoming RF to a desired IF using a reference frequency. In addition to the
superheterodyne receiver, invention of the mixer is usually attributed to Armstrong as well
[28]. The reference oscillator used for the RF to IF conversion in a receiver is referred to as
the local oscillator (LO) [29]. This reference frequency is denoted as the LO frequency. This
process is referred to as mixing.
The ideal mixer is a multiplier. Specifically, if the RF and LO signal are represented by

The mixing, or heterodyning, process is represented by
Thus, an ideal mixer produces an output that contains the sum of the RF and LO frequencies
and the difference of the RF and LO frequencies. The LO frequency can be either above or
below the RF.7 The absolute value in (14.2) underlines the fact that either fLO > fRF or fLO < fRF
are valid RF to LO relationships.8 For downconversion, the difference of the RF and LO
frequencies is what we are after. The sum of the RF and LO frequencies is removed by the IF
BPF in Figure 14.1.
The absolute value in (14.2) also means that there are two RF frequencies that result in the
same IF. One is the desired RF used in the receiver; we term the other the image frequency.
Any signal at the image frequency, when mixed with the target signal, results in another signal
at the desired IF. Jammers can exploit this by placing interference at the image frequency. The
image frequency is one of the unwanted signals the preselector must reject. It is not unusual
for a preselector to provide more than 45 dB of image signal rejection.
Since there are two RF frequencies that will translate to the desired IF, an image reject filter
(IRF) is sometimes necessary after the LNA to suppress unwanted receiver noise generated by
the LNA in the image band. This has the benefit of preventing image noise from entering the
passband, resulting in noise components from both the main and image responses adding,
which would double the noise figure of the mixer. To minimize the effect of image noise, ~20
dB of image rejection is generally sufficient.
For frequency plans with a low IF, image filters can be difficult to implement because the
image is too close to the passband. In these cases, an image reject (single sideband) mixer is
often used instead of a filter. As the name implies, an image reject mixer uses phase
cancelling techniques to suppressed the image in order to prevent the image sideband from
converting to the IF passband. An image reject mixer can usually provide 25 to 35 dB of
image suppression [30, p. 230]. The absence of an image reject filter after the LNA in the
receiver in Figure 14.1 tacitly implies the use of an image reject mixer.
Figure 14.3 contains an illustration of an image frequency. In the figure, fRF is 4,040 MHz
and the desired fIF of 40 MHz. The LO frequency we chose is fLO = fRF –fIF = 4,040 –40 =
4,000 MHz (low side). The image frequency is then fIMAGE = fRF –2fIF = fLO –fIF = 3,960 MHz.
If we had used an LO where fLO > fRF (high side), the LO frequency would be 4,080 MHz and
the image would be 4,120 MHz.

Figure 14.3 Low side mixer downconversion example.
Practical mixers are implemented by nonlinear devices such as diodes. As a result, in
addition to signals at |fRF –fLO| and fRF + fLO, signals at harmonics of fRF and fLO are generated.
This can be expressed as
where m and n are integers [19, 28, 31]. These harmonics are unwanted mixer byproducts that
are referred to as spurious signals, or simply spurs. The order of a spur is given by |m| + |n|.
Except when an image reject mixer is necessary, double-balanced mixers are the most widely
used in radar receivers. This is because a double-balanced mixer is designed to suppress the
LO, the RF, and even ordered products at the output of the mixer. Double-balanced mixers
also provide isolation between all mixer ports.9
The mixer spurs for the above example, up to |m| + |n| = 10th order, are plotted in Figure
14.4. Only the spurs from 0 to 450 MHz and above -120 dBc are visible. The spurs shown in
Figure 14.4 are multiples of the 40 MHz IF. The other spurs are either much farther away in
frequency or are too low in power to be of concern. This is because as spur order increases,
the level of the spur decreases.10
For this example, there are only a few spurs above -100 dBc within the 450 MHz shown.
The BPF following the mixer in Figure 14.1 is used to reduce these spurs further. For
illustration, the frequency response of a 4th order Butterworth filter with an 8-MHz passband
centered on an IF of 40 MHz is also shown in Figure 14.4.11 The spurs at 80 MHz and 120
MHz would be further suppressed by > 56 dB by the BPF shown. Since the closest spur to fIF
is 40 MHz away, the design of the post mixer BPF is easier than if the spur were, say, 5 MHz
away, which would require a higher order filter for the same amount of spurious rejection
because the spur is closer to the filter passband.
Considering the large number of spurs generated according to (14.3), there are sometimes
spurs within the passband of the BPF following the mixer that are unavoidable. Beyond about
9th order though, mixer spurs are usually low enough to be ignored, but not always. Selection
of the fRF, fLO, and fIF combination is an important consideration to insure that no low order
spurs fall within the IF passband.
The IF BPF is generally a filter that has a fairly constant passband gain, with a linear phase
in the passband. The constant gain and linear phase are desirable to minimize distortion. The

order of this BPF is typically low (4th or 5th) to avoid excessive ringing and overshoot.
Lumped-element LC filter technology, which is suitable for lower frequencies, is typically
used for implementation.
Figure 14.4 Mixer output spur example.
The typical IF for a superheterodyne receiver is 30 to 100 MHz. In addition to the spurious
considerations described earlier, a particular IF is selected because of the performance of
available components. Generally speaking, as the IF is decreased, the cost of components
goes down and performance improves. For example, a low IF simplifies the design of
narrowband filters.
The IF amplifier is used to make up for the losses in the previous devices and to amplify
the signal to desired levels for subsequent components. The detector is typically the last stage
considered to be part of the receiver. When coherent processing is not required, linear or
square law amplitude detectors are typically used (see Chapter 6) and the BPF is replaced by a
pulse matched filter. For coherent processing, a synchronous (or quadrature) detector is used
to preserve phase information in the signal, with matched filtering occurring in the signal
processor.12
If we consider a frequency agile radar, where the RF can vary rapidly over a fairly large
range, we need a wideband preselector and a higher IF. While increasing the IF simplifies
image rejection by placing the image frequency further in the stopband of the preselector, the
complexity of narrowband filter design is increased. Also, frequency agility complicates the
issue of avoiding spurious mixer products within the IF passband greatly.
The approach typically used to alleviate these issues is to add more downconversion stages.
The higher IF in the first frequency conversion stage has the benefit of good image rejection
via the preselector [31]. The lower IF in the second stage of conversion enjoys the benefit of
easier implementation of narrowband filters [31]. An additional key point to adopting
multiple downconversion stages is that additional stages simplify the problem of mixer spurs.
Two downconversion stages are typically sufficient to ensure that passband spurs are at least
8th or 9th order (which have very low power levels).

Figure 14.5 Dual-conversion superheterodyne receiver with synchronous detection.
14.3 DUAL-CONVERSION SUPERHETERODYNE RECEIVER
A block diagram of a dual-conversion superheterodyne receiver is shown in Figure 14.5.
From the preselector to the IF attenuator output, the topology is identical to that of the single-
conversion superheterodyne receiver of Figure 14.1. The principal differences are a
wideband preselector, an agile first LO, and the inclusion of a second downconversion stage
consisting of a mixer, filter, and amplifier cascade. Additionally, we have chosen to use an
analog I/Q demodulator to preserve phase information.13
The wideband preselector is used to limit frequencies to the agile range of interest. As with
the single-conversion superheterodyne receiver, the preselector is a low-loss device to
minimize its impact on the system noise figure. The preselector passband must be wide
enough to accommodate the desired frequency agility band. A typical RF agility range is 100
to 500 MHz.
As with the single downconversion receiver, the attenuators are used to extend receiver
dynamic range. Likewise, the LNA and prior lossy elements establish the overall noise figure
of the radar, but must now be broadband. The IF amplifiers make up for losses in previous
devices and amplify the signal to desired levels for subsequent components.
To simplify subsequent IF filtering, we chose to use an agile 1st LO for the first
downconversion stage. This agile LO1 tracks with the RF, resulting in fixed first and second

IFs. As a result, we only need one IF filter per downconversion stage. The first IF is selected
to optimize suppression of the image frequency and other spurious signals generated by the
first mixer. Because of the wide bandwidth, higher IFs generally simplify rejection of the
image and spurs. Additionally, the first IF must be high enough to accommodate the RF agility
bandwidth. A general rule of thumb is for the first IF to be 1.5 to 2 times the agility bandwidth
to simplify component design.
The second IF is now analogous to the IF of a single downconversion superheterodyne
receiver. The second IF is generally chosen to be low (<100 MHz) to simplify design of the
narrowband IF filtering and other components. Spurious considerations are simplified
because the bandwidth at the second IF is relatively narrow. Using a high IF followed by a low
IF in our receiver design, we get the benefits of both.
Both IF BPFs in Figure 14.5 are used to reject mixer spurs. The bandwidth of the first IF
filter is generally on the order of the RF channel spacing (e.g., 10 or 20 MHz) to reduce spurs
while simplifying filter design. The bandwidth of the second BPF is usually somewhat
narrower than the first, on the order of two to three times the modulation bandwidth, to ensure
the modulation is undistorted (see Chapter 7). For example, if the radar uses an LFM
waveform with a bandwidth of 2 MHz, the second IF BPF should have a bandwidth of about 6
MHz. As a note, choice of RFs, IFs, and LO frequencies is sometimes referred to as a
frequency plan.
At this point, we should emphasize that we are generally not interested in the second IF in
and of itself, but the amplitude and phase information it carries via modulation. It is the
modulation that contains the information we want, such as waveform, delay to target, and
Doppler information.
This brings us to the I/Q demodulator14 shown in Figure 14.5, which translates a real
bandlimited signal, xIF(t), to baseband. Quadrature demodulation is usually done after
downconversion to a low IF, which for this example is the second IF.
The demodulator topology depicted in Figure 14.5 is the classical approach of splitting
xIF(t) and then using two matched phase detectors using reference frequencies which are 90
degrees out of phase (in phase quadrature). Each detector is implemented as a mixer, which
performs a downconversion of xIF(t) to baseband, followed by an LPF to remove unwanted
harmonics. The difference here is that the desired IF is 0 Hz.
Recall that a real bandlimited IF signal can be represented as

where A(t) and ϕ(t) represent the amplitude and phase modulation, respectively. The signals
xI(t) = A(t)cos[ϕ(t)] and xQ(t) = A(t)sin[ϕ(t)] are the in-phase and quadrature baseband signals
of interest. The output of the in-phase channel mixer is
After lowpass filtering, the I-channel phase detector output becomes
Similarly, the Q-channel phase detector output becomes
Equations (14.6) and (14.7) contain all of the modulation information of (14.4) without the IF.
The quadrature detector uses an extremely stable reference oscillator for phase detection.
This reference oscillator is usually of equal frequency to the IF and always phase coherent.
The term coined for this oscillator is the COHerent Oscillator, or COHO [29], because it is
related to the IF.
A problem with analog quadrature demodulation is amplitude and phase misalignments in
the circuity, which can cause imbalances between the I and Q channels. This imbalance can
generate unwanted image and DC signals, which have a negative impact on subsequent signal
processing. A means of avoiding such problems is to perform the quadrature detection with
digital hardware [34]. The resulting receiver is termed a digital receiver and will be discussed
in Section 14.8.
14.4 RECEIVER NOISE
As discussed in Chapter 2, the two main contributors to noise in radars are the environment
(via the antenna) and thermal noise generated by the electronic components of the receiver
[35]. As discussed in Chapter 4, the noise level present in a radar can be quantified in terms of
equivalent/effective noise temperature, or noise figure.
When considering radars with very low noise figures, where environmental noise is a
major noise contributor, an effective noise temperature approach is favored [19]. For radars
with larger noise figures (greater than about 7 dB), where receiver noise normally dominates
environment noise, a noise figure approach is generally preferred [19]. Radars in the VHF
band have such high environmental noise that the noise temperature characterization is
appropriate even when receiver noise figures are not very low.
In this chapter, we take a measurement point of view, considering the receiver, and perhaps

the signal processor in cascade, but not the entire radar. The importance of this is that analysis
and measurement of the receiver or receiver and signal processor as a subsystem can be
carried out under the assumption that the input is terminated in a resistor at 290 K. For this
reason, we will use a noise figure approach as opposed to noise temperature approach in this
chapter.
Likewise, we will consider thermal noise generated in the receiver, not environment noise.
When making receiver noise measurements, noise figure is used, and the reference
temperature is T0 = 290 K by definition [29]. Also, in a measurement setting, the receiver is
usually disconnected from the antenna, with test equipment used to inject test signals into the
receiver and to make measurements of various parameters such as gain, bandwidth, dynamic
range, and noise figure.
While much of radar theory is concerned with ratios (e.g., SNR, CNR, SCR, SIR), when
considering receivers, knowing absolute levels is a key consideration. One important level in
a receiver is the noise level, often referred to as the noise floor, since receiver noise
generally establishes the noise level competing with weak signals. The notable exception to
this is for radars operating at frequencies below about 300 MHz. This is because of the steep
increase in cosmic and other environmental noise below about 300 MHz (see Figure 2.5)
resulting in receiver noise no longer dominating the system noise temperature and
equivalently the system noise figure [36].
Given a reference temperature of T0 = 290 K for our measurements, the power spectral
density, and thus noise power or noise floor in a receiver, is established at the output of the
LNA. The noise floor at the output of each receiver stage can be determined using
or in logarithmic form
where k is Boltzmann’s constant, 1.38×10–23 W/(Hz K) and T0 = 290 K. F, Bn, and G are the
noise figure, bandwidth and gain, respectively, up to the output each receiver stage.
In using the forms of (14.8) and (14.9), we make the tacit assumption that bandwidths
remain the same or decrease as one progresses through the various components of the
receiver [37]. While not always true, this is a common assumption used for cascade analysis,
where the overall RF to IF bandwidth is usually set by the last filter (or tuned amplifier) in the
chain [38, p. 15]. We also make the implied assumption that there is sufficient gain ahead of
any ADC to minimize the impact of an ADC’s relatively high effective noise figure (e.g., 30
dB), which requires a lot of receiver gain prior to the ADC to make the ADC noise a small
fraction of total noise (see Section 14.8.1.4).
For example, consider the RF front end shown in Figure 14.6. The preselector we have
chosen is a 4th order Chebyshev Type I filter with 0.1 dB ripple and a 10-MHz bandwidth15 It
has a loss of 0.7 dB, which means its gain is –0.7 dB. The second filter is a 5th order Bessel

filter with an 8-MHz bandwidth16 and a gain of –1.5 dB. The amplifiers are assumed to have
bandwidths equal to the preceding filter. Note also that we are driving the circuit under test
with a calibrated noise source at standard temperature, T0.
From Chapter 4, if we assume the filters are lossy passive devices, their noise figures equal
their losses (Fn = L) [39–42]. We would like to calculate the noise power generated by the RF
front end as well as the noise power after each component. Note that carrier frequency is
irrelevant for this analysis.
Figure 14.6 Noise floor example.
At the cascade input, the noise is assumed white with a power spectral density expressed, in
various units, as
Until we impose a bandwidth, the noise power is theoretically infinite. Using (14.9), the noise
power out of the preselector is
The noise power out of the RF amplifier is
This process is continued stage by stage. Note that after the amplifier, we need to use the
Friis formula for cascade noise figure (see Section 14.7 and Chapter 4) [43]. The RF to IF
bandwidth is 8 MHz. The gain, noise figure, and noise power out of the entire chain are 30.8
dB, 5.4 dB, and –68.7 dBm, respectively. The remaining details are left as an exercise.
We have now reflected on what is often considered the low end for signals in a receiver,

namely the noise floor. Signals below this level are said to be buried in the noise and cannot
be discerned (without subsequent signal processing).17 We now will consider the top end for
signal level.
We have thus far assumed the amplifiers amplify signals in a linear fashion. In practice, this
is not the case because above a certain input power level, the amplifier will saturate. This is
because a finite DC voltage is used to power the amplifiers, which limits how large a signal
can be linearly amplified. This leads to two important radar receiver concepts: the 1-dB
compression point, and dynamic range. These terms apply equally to components and
receivers, with some minor differences, as will be discussed.
14.5 THE 1-dB GAIN COMPRESSION POINT
Ideally, analog components amplify signals in a perfectly linear fashion. However, if over
driven by large input signals, the amplifier gain will become nonlinear. This, in turn,
generates unwanted spurious signals. The compression point of a device, which is defined as
the level of the output signal at which the gain of a device is reduced by a specific amount, is a
useful index of the amount of distortion that can be accepted [29]. A specific index used in
amplifier analyses is termed the 1-dB compression point. Consistent with the definition of
compression point, it is the output signal level where the gain is reduced by 1 dB (from its
nominal, constant value) [16].
The definition of 1-dB compression point of the previous paragraph is the formal
definition used for components, in general. For receivers, the standard definition is that it is
the input level at which the gain decreases by 1 dB from its (nominally) constant value.
Figure 14.7 contains a plot of output power versus input power for a notional device. We
will denote the 1-dB compression point as P1. To avoid ambiguity, we will use prefixes, with
O designating output and I designating input, for example, OP1 stands for 1-dB compression
point at the output. The 1-dB compression point at the input is related to the 1-dB compression
point at the output by [44, p. 541]
where G is the nominal device gain, in dB.
The general procedure used to measure the 1-dB compression point is to inject a signal and
increase its amplitude until the gain is decreased by 1 dB [45]. This test is sometimes referred
to as a “transfer test” because a transfer curve is usually generated. The transfer curve
associated with a typical 1-dB compression point measurement is also illustrated in Figure
14.7.
For the example presented in Figure 14.7, we consider an IF amplifier with a gain, G, of 30
dB. We sweep the input test signal power from –40 dBm to 0 dBm, while measuring the output
power. The 1 dB compression point at the output, OP1, occurred at 10 dBm. Using (14.13), the
1 dB compression point at the input is –19 dBm. Thus, we say that this amplifier has an output,

1-dB compression point of 10 dBm and an input, 1-dB compression point of -19 dBm.
The amplifier output saturated at an output power level, PSAT, of 14 dBm. As a general rule
of thumb, compression usually starts about 5 to 10 dB below the output 1-dB compression
point. Similarly, saturation typically occurs around 3 to 6 dB higher than the output 1-dB
compression point.
Figure 14.7 A 1-dB compression point example.
14.6 DYNAMIC RANGE
The dynamic range of a receiver, depicted by Figure 14.8, is commonly defined as the ratio of
the maximum input signal that can be handled to the minimum signal input capable of being
detected [19]. The maximum level is usually taken to be the 1-dB compression point because
that is where we normally assume the device is departing from linear operation. The
minimum level is often denoted MDS. There are however a number of variations of what is
meant by MDS, as we shall see.
Dynamic range can be expressed as
There is no shortage of definitions concerning both dynamic range and the minimum

levels used for determining the dynamic range of a receiver [46–49]. We also note that
various definitions and terminology can potentially clash. To help explain and hopefully
avoid some of the confusion about MDS, we will present MDS definitions applicable to radar
receivers [30, 50–54]. We will, in short order, have three “standard” definitions that are
commonly used in radar and radar receivers.
Figure 14.8 Dynamic range.
For this chapter, though, we define dynamic range in terms of the receiver sensitivity,
which is taken as the minimum input signal required to produce a specified output signal
having a specified signal-to-noise ratio [55]. For measurement purposes, we typically
consider the output signal detectable when it is at or above the noise level, or SNR = 0 dB (S =
N). Defining dynamic range in terms of receiver sensitivity allows us to measure receiver
dynamic range without regard to noise sources from the environment. Thus, if we consider
receiver noise without its preceding subsystems, the input noise will come from a resistive
termination at 290 K by definition.
Likewise, receiver dynamic range can be characterized without considering the effects of
signal processing that provides detectability of signals below noise level. However we often
do measure the dynamic range of both the receiver and the receiver and signal processor
combined.
14.6.1 Sensitivity
As mentioned already, receiver sensitivity is defined as the minimum input signal required to
produce an output signal with a specified SNR. Sensitivity is only concerned with internally

generated receiver noise [50, p. 76]. This is because external noise from the antenna is not
something we can control when designing a receiver. While a receiver’s sensitivity can be
expressed in terms of power or voltage, we usually use power in dBm.
A receiver’s sensitivity is largely determined by the RF front-end components, since the
noise floor in a receiver is the limiting factor on receiver sensitivity. Also, this definition for
sensitivity relates to power levels, not detection performance. It does not include
specifications for Pd or Pfa. In receiver vernacular, sensitivity is sometimes used
synonymously with minimum detectable signal and minimum discernable signal, adding to
the multiple MDS definition confusion [31, 33, 49].
The preselector input is a common input reference point used when defining or measuring
sensitivity. When considering just the receiver, the output measurement point typically used is
the IF amplifier output just prior to detection. When considering the receiver input to the
signal processor output, the term “system sensitivity” is used. For this discussion, we confine
ourselves to just the receiver and choose to use the input to the preselector and IF amplifier
output prior to detection as our analysis, or measurement, points.
Receiver sensitivity can be expressed as
or in dBm
where PSmin is the minimum signal level at the receiver input (preselector input) and SNRmin is
a specified SNR (at the IF amplifier output). Using an SNRmin of either 0 dB or 3 dB is
customary in the context of receiver sensitivity [19, 44, 49, 56]. Equations (14.15) and (14.16)
are similar to (14.8) and (14.9) where gain is replaced by SNR, since the relevant quantity is
SNR instead of absolute levels. Also, the terms in (14.15) and (14.16) are the overall
bandwidth and noise figure of the receiver (or receiver/signal processor).
If we consider the RF front end shown in Figure 14.6, and stipulate a minimum acceptable
SNR (SNRmin) of 3 dB, the sensitivity becomes
If we compare PSmin and Pn, we see that they differ. Specifically, they differ by G–SNRmin
rather than SNRmin because for (14.15) the SNR is specified at the output, but PSmin is specified
at the receiver input. For (14.8) Pn is specified at the output of the receiver, not the input.
Sensitivity, PSmin, can be measured by injecting a calibrated target signal with a constant
power level (SW0) at the receiver input and determining the SNR at the output of the

receiver.18 As discussed earlier, we typically use an SNRmin = 1 or 0 dB, for this measurement.
This results in
at the output of the radar’s receiver (signal power = noise power) [49, 57]. This can be related
to Pn using
which is the same as (14.8). Skolnik refers to (14.19) as the minimum signal of interest [46, p.
3.4].19 Stephen Erst presents a test methodology based upon the relationship between S/N and
(S + N)/N rms voltage ratios the authors have put to good use in the field which can be used to
determine when SNR = 0 dB at the output of the receiver (or the entire receiver signal
processor chain) [50, pp. 76–78].
Measuring a receiver’s sensitivity is important because it is closely tied to detection
performance and can help determine or verify gain and loss terms in the radar range
equation, overall noise figure, and overall bandwidth. Measuring receiver noise directly also
results in a consistent quantity verifiable using only test equipment and a SW0 target signal
injected directly into the RF front end.
14.6.1.1 Tangential Sensitivity
The criterion of SNR = 0 dB at the output of a receiver is sometimes referred to erroneously
as tangential sensitivity (TSS). Tangential sensitivity, defined in [18, p. 456], corresponds
(approximately) to SNR = 8 dB. TSS gets its name from the use of what is called a tangential
signal to estimate sensitivity. Specifically, noise and signal plus noise are viewed on an
oscilloscope, and the signal power is adjusted until the bottom of the signal-plus-noise trace
aligns with, or is tangent to, the top of the noise-only trace (see Exercise 7).
TSS is generally accurate to within ±1 dB, and is influenced by the RF bandwidth, the video
bandwidth, the noise figure, and the detector characteristic [48]. There is no theoretical value
for tangential sensitivity, since it depends on a subjective matching of the “peak level” of
noise with the minimum level of signal plus noise outputs on an A-scope display, neither level
having a measureable value.
14.6.2 Minimum Detectable and Minimum Discernable Signal
Minimum detectable signal (MDS) for radar detection applications is the minimum signal
power necessary to give reliable detection performance in the presence of white, Gaussian
noise, that is, the minimum signal level needed to give a specified Pd with a specified Pfa [29].
Specifying Pd and Pfa is necessary for this definition of MDS because minimum detectable
signal is a statistical quantity.

Minimum detectable signal in the radar range equation can be expressed as
where Bn is the equivalent noise bandwidth (see Chapter 2). Equation (14.20) is of the same
form as (14.15), with the addition of a loss term L. In this context, MDS encompasses the
entire receiver and signal processor, and is related to the measureable quantity of (14.15).
This definition of MDS is also concerned with both internally generated noise and externally
generated noise.
Minimum discernable signal (MDS) is defined as “The minimum detectable signal for a
system using an operator and display or aural device for detection” [29]. Including sensitivity,
which is sometimes referred to as MDS, we now have three “standard” definitions for the
acronym MDS.
14.6.3 Intermodulation Distortion
Dynamic range can also be defined in terms of spurious signals, which is called the “spurious
free dynamic range.” While we use mixers as intentionally nonlinear devices to generate
harmonics of the input signals, other active devices, such as amplifiers, are only
approximately linear, and also act like mixers (just not very good ones), generating unwanted
signals which are harmonics of the input. This phenomenon of generating spurious signals is
termed intermodulation distortion (IMD). The term “two tone” is sometimes used when
discussing intermodulation products because two tones are used to measure and characterize
intermodulation distortion.
Like a mixer, if we let the desired frequencies at the input to an amplifier be f1 and f2, the
spurious signals occur at frequencies of
where m and n are integers. The harmonics that usually cause difficulties in receivers are the
2nd- and 3rd-order harmonics (recall the order of a harmonic |m| + |n|). This is because the 2nd-
and 3rd-order harmonics tend to be the largest spurs and closest in frequency to f1 and f2,
respectively.
Because they are apt to be the biggest spurs, intermodulation distortion performance is
usually specified in terms of the 2nd- and 3rd-order intercept points. For superheterodyne
receivers, the 3rd order intercept is most important because they are closest in frequency to the
desired tones, making them problematic or impossible to filter out. The 2nd order intercept is
more important than the 3rd order intercept point in homodyne receivers [45]. We will
compute both in this chapter.
For example, let f1 = 30 MHz and f2 = 31 MHz. The 3rd-order intermodulation frequencies
are 2f2 –f1 = 32 MHz and 2f1 –f2 = 29 MHz. The 2nd-order intermodulation frequencies occur

farther away at 1 MHz and 61 MHz. This demonstrates a major problem. The 2nd-order
products, while potentially large, can possibly be filtered out. The 3rd-order intermodulation
products in this example cannot be filtered out because of their close proximity to the desired
tones generating them.
Figure 14.9 depicts the concept of intercept points. The 2nd-order intercept point, IP2,
corresponds to a projected power level at which the 2nd-order intermodulation product
crosses a perfectly linear response. The 2nd-order intermodulation product gain has a slope
twice that of the linear gain of the desired input.
Figure 14.9 Diagram of 2nd and 3rd order intercept points.
Similarly, the order intercept point, IP3, relates to a projected power level at which the 3rd-
order intermodulation product would intersect a projection of the linear gain. The 3rd-order
intermodulation product gain has a slope three times that of the desired input. As a general
rule of thumb, the order intercept point is 10 to 15 dB higher than the 1-dB compression point.
For example, increasing the desired (first order) signal by 3 dB increases the 2nd-order
signal by 6 dB, and the 3rd-order signal by 9 dB. This results in the 2nd-and 3rd-order
intermodulation distortion products rapidly becoming nearly the same amplitude as the

desired input. Power exceeding the 3rd-order intercept point causes intermodulation
distortion.
It should be emphasized that both the 2nd and 3rd intercept points are much higher than the
1-dB compression point. Because of this, neither the 2nd or 3rd order intercept points can be
measured directly. Instead, they are projected from measurements made using lower signal
levels.
The spurious free dynamic range of a receiver is defined as the range over which a
receiver does not compress an input signal and no spurious signal is above the receiver’s
noise floor [58]. The spurious free dynamic range is then defined in terms of the third-order
intercept point as [58, 59]
Note that the SFDR of (14.22) is defined in terms of the minimum required signal being equal
to the noise level of the receiver rather than minimum discernable or minimum detectable
signals.
For example, consider a receiver with a 3rd order intercept of IIP3 = –14 dBm, an RF to IF
bandwidth of 4 MHz, and a noise figure of 5 dB. Using (14.22), the spurious free dynamic
range becomes
14.6.4 Required Dynamic Range
Now that we can quantify dynamic range, it would be useful to estimate what the dynamic
range needs to be. Radar echoes can cover a wide range (typically 90 to 100 dB) due to
variations in target RCS, clutter RCS (see Section 13.2), and the 1/R4 range dependency. At a
minimum we can predict the variation due to RCS and that due to 1/R4.
Recall from Chapter 2 that the signal power at the input to the receiver is
where PT is the peak transmit power, GT denotes directivity of the transmit antenna, GR
denotes the directivity of the receive antenna, λ denotes the radar wavelength, σ denotes the

average target or clutter RCS, and R denotes the slant range from the radar to the target.
Let us specify a minimum and maximum target or clutter RCS of σmin and σmax,
respectively. Likewise, let us specify a minimum and maximum range of Rmin and Rmax,
respectively. The maximum power present at the receiver is then
and the minimum power is
The minimum dynamic range necessary to accommodate for σmin and σmax and Rmin and
Rmax is
In logarithmic form
Equation (14.28) gives a preliminary lower bound, since it does not take into account target
fluctuation. Fluctuations in RCS can potentially span another 30 dB [20, p. 737].
For example, consider a radar transmitting a 10-μs pulse with a maximum instrumented
range of 100 km. This radar is expected to accommodate RCSs between σmin = 0.001 m2 (–30
dBsm) and σmax = 1,000 m2 (30 dBsm). We determine means of the pulsewidth as c-x/2 = 1.5
km. Using (14.28) results in

For this example, we need 133 dB of dynamic range. This does not mean our radar has 133
dB of dynamic range, but we would like it to. The required dynamic range of 133 dB is
available only if the receiver gain is changed by STC, AGC, and/or change in transmitted
power. This gets us back to the rules mentioned in Section 14.2, which depend on the radar
objective and waveform.
It should be noted that the dynamic range of inputs in most surface-based radars is
determined not by maximum target RCS and range, but maximum clutter RCS and range.
Only for a tracking radar operating on targets more than one beamwidth above the surface is
the maximum target RCS of concern. In search radar, the target remains detectable and
reportable even if it is above the saturation level of the receiver. The calculations here should
use maximum possible clutter RCS at the range where the lower edge of the radar beam first
encounters the clutter source. Any number of land- and ship-based radars encounter clutter
from structures that project into the beam at short range, which is why STC is used.
14.7 CASCADE ANALYSIS
Dynamic range and noise figure (sensitivity) considerations, along with how much gain is
necessary at any particular receiver stage to get the final desired overall receiver gain and
output signal level, leads to the necessity of considering component selection and gain
distribution through the receiver.
For example, some receiver parameters, such as gain and dynamic range, are antithetical.
As such, when selecting components for a receiver design, a certain amount of compromise
is necessary. As another example, we may use a high gain LNA as the first amplifier, but if we
follow it with high gain high noise figure amplifiers, the LNA (and prior lossy components)
will no longer dominate the total noise figure.
This makes it important to track noise and signal power levels through the stages of the
receiver to ensure receiver operation remains linear (avoid exceeding the 1-dB compression
point) over desired range of signal amplitude.
The tool used to evaluate the above considerations is called cascade analysis, which refers
to the process of tracking parameters such as signal power, signal gain, noise gain, noise
figure, 1-dB compression point, noise floor, dynamic range, and bandwidth through the
stages of a receiver. Cascade analysis helps predict and evaluate the interaction between
various receiver parameters and guides component selections.
As an example, to design a receiver’s 1 dB compression point, we need to know in what
order these devices reach the 1-dB compression point. Ideally, the last component in a
receiver chain (e.g., the amplifier prior to detection) is the first and only device to saturate.
Knowing which device saturates first, second, third, and so forth determines where to put gain
control and how much is necessary.
14.7.1 Cascade Analysis Conventions

It should be noted that some parameters do not apply to some components. For instance,
passive devices, such as a waveguide filter, do not compress or saturate. For these devices, we
generally use a very high compression point, such as 150 dBm, to effectively remove these
components from the cascade 1-dB compression point calculations.
It is conventionally assumed that the narrowest bandwidth component in the receiver chain
sets the overall RF to IF bandwidth [31, p. 15]. This is frequently the last IF filter bandwidth.
For components with comparatively broad bandwidths (e.g., several GHz), such as
waveguide, isolators, and couplers, we generally use a large token bandwidth, for example,
900 MHz. With the cascade bandwidth taken to be the narrowest bandwidth up to the point
being analyzed in the chain, using 900 MHz effectively removes these components from the
cascade bandwidth calculations.
The noise figure of a passive device, such as an attenuator, is frequently considered equal
to its loss (see Section 4.4.2). While this results in the noise level into and out of devices
remaining unchanged, it should be stressed that the same cannot be said for signals. Similarly,
for well-designed passive mixers, a common rule of thumb is that the noise figure of a mixer
is equal to its conversion loss [60].20 Conversion loss is the difference between the input RF
power level and the output IF power [19, 44].
It is presumed all devices are impedance matched. While 1 Ω is often assumed when
absolute power is not important, we will use a system impedance of R0 = 50 Ω. A 50-Ω
impedance is typical of RF and microwave devices used in radar receivers. Likewise, RF test
equipment is usually matched to 50 Ω. When there is an impedance mismatch (e.g., an ADC
with a 1 kΩ input) impedance matching must be used (or the loss accounted for).
14.7.2 Procedure
The general forms of cascade equations can quickly become unwieldy when applied to a
receiver chain, for example, (4.43) and (4.44). As a result, cascade calculations are often
performed iteratively, two stages at a time. This technique simplifies the analysis and is
amenable to computer programming. The general procedure is as follows [19, 44]:
• Start at the first stage.
• Perform two-stage analysis on first two stages computing cascaded noise figure, gain, 1-
dB compression point, and so forth.
• Replace these first two components by an equivalent single stage with above cascaded
parameter.
• Perform two-stage analysis on the equivalent component and the third stage.
• Repeat until all stages are included.
As we will see below, the various cascade equations are used in linear form, logarithmic
form, or a combination thereof. The choice of form is generally a matter of programming
simplicity or preference. For convenience, results are usually carried along in both linear and
logarithmic forms. Likewise, since the various parameters can be referenced to either the
device input or output, we generally compute the cascade for one, such as the output, and

relate this to the input, carrying both results.
We will use the receiver chain shown in Figure 14.10 for illustration of the cascade of
various receiver parameters. The various component parameters are summarized in Table
14.1.
Figure 14.10 Example 1: superheterodyne receiver block diagram.
Table 14.1
Example 1: Device Specifications
We note that, for cascade analysis, the various RFs and IFs are not important beyond their
impact on individual component parameters. Also, we need to keep in mind the compression
point of an amplifier is usually defined referenced to the output, while for mixers the input is

the typical reference point.
As a note on terminology, for passive components with a loss, the term “insertion loss”
(IL) is often used. For mixers, the term “conversion loss” (CL) is used. Mixers typically have
a conversion loss of 4.5 to 9 dB [60].
14.7.3 Power Gain
The cascaded gain, in dB, at the output of a particular device is the sum, in dB, of all of the
prior stage gains to the point in the receiver chain of interest. It is sometimes more convenient
to use linear gain and use the product of gains. The gain of a receiver, from the source
through Device N is [38, 44]
Following the procedure outlined above, we first consider a two-stage cascade of the
preselector and the LNA. Using (14.30) for N = 2, we get
Next we consider the mixer, using the results of (14.32), which results in
This iterative, two-device cascade process is repeated as necessary.
14.7.4 Noise Figure and Noise Temperature
In a typical design, the receiver noise figure is established by the RF LNA and prior lossy
elements. To ensure that subsequent components do not increase noise figure appreciably, we
evaluate the noise figure through the entire receiver chain when selecting components. The
Friis formula for the cascade of noise figure and noise temperature were covered in Section
4.5 of Chapter 4, specifically, (4.43) and (4.44) [43]. In keeping with our iterative procedure,
we consider a two-stage cascade. The noise figure for a two-stage cascade is given in linear
and logarithmic form, respectively, by [61–63]

Similarly, the equivalent/effective noise temperature for a two-stage cascade is given by
[61–63]
where the relationship between noise figure and noise temperature is [61]
We continue our example and find the noise figure of the first three devices. Applying
(14.34), the combined noise figure of the preselector and LNA is
The result is the sum of the LNA noise figure and the loss of the preceding device, which in
this case is the preselector. This demonstrates the importance of minimizing losses prior to
the LNA, which add dB for dB to overall noise figure [19, 61].
Using the results of (14.32) and (14.38), we proceed to the output of the mixer stage. Using
(14.34) again we get
Note that because of the high gain of the LNA, we see that there is very little change in noise
figure due to the following mixer.
14.7.5 1-dB Compression Point
The 1-dB compression point is explained in Section 14.5. Like gain and noise figure, we can
determine the 1-dB compression point at each stage of the receiver. The 1-dB compression
point at the output of a two-device cascade is [64, p. 58]

which is usually used when specifying an amplifier. If we reference this at the input, typically
used for receivers, we can use
For our purposes, we first use (14.40) and then apply (14.41).
Considering the preselector and the LNA in cascade, we use (14.40) to get
where we use OIP11 of 150 dBm for the filter, since it does not compress. The 1-dB
compression point is therefore equal to the compression point of the LNA for this case.
Next, we consider the mixer in cascade. We first note the 1-dB compression point is
specified at the input. Relating this to the output, we get
Applying (14.40) again, we get
As before, we would continue the process iteratively.
14.7.6 Second-Order Intercept
As noted earlier, the 2nd order intercept is usually referenced at the output for amplifiers, and

at the input for mixers. The 2nd order intercept should be kept as high as possible because
signal powers exceeding the 2nd order intercept point causes intermodulation distortion. As a
general rule of thumb, the 2nd order intercept point is 20 to 25 dB higher than the 1-dB
compression point.
When performing a cascade analysis, we assign a suitably high intercept point to passive
devices so they do not affect the overall system intercept point (> 100 dBm is usually
sufficient). This is because passive devices do not have intercept points.
The 2nd order intercept compression point of a two-device cascade, referenced to the
device output, is given by [49, 51]
or in logarithmic form [62]
We can reference the device input using
While we could use (14.45) or (14.46) to determine the cascade intercept point, we take a
lesson from the 1-dB compression point example and note that since the 2nd order intercept
for a passive filter is treated as essentially infinite, the 2nd order intercept for the cascade of
the preselector and LNA is simply that of the LNA or 39 dBm. Adding the mixer to the
cascade, we use (14.46) to get
The 2nd order intercept point is not always specified by manufacturers, nor used in a
cascade analysis because the spurious free dynamic range for a superheterodyne receiver is
usually a function of the 3rd order intercept, which typically manifests before the 2nd order
intercept.

14.7.7 Third-Order Intercept
The 3rd order intercept is an extrapolated value (see Section 14.6.3) that occurs when the
output power of the desired input tones are equal to the 3rd order intercept power level (four
tones total) [45]. Using the 3rd order intercept as a measure of linearity was first suggested by
Avantek around 1964 [45]. The 3rd order intercept is usually specified at the output for
amplifiers and at the input for mixers, but not always.
Signal powers exceeding the 3rd order intercept point cause intermodulation distortion, so
the higher the better. As a general rule of thumb, the 3rd order intercept point is typically 10 to
15 dB greater than the 1-dB compression point [45, p. 397].
Since the 3rd order intercept is lower (typically by about 10 dB) than the 2nd order intercept,
3rd order intermodulation products appear earlier than 2nd order products. This is why the 3rd
order intercept is used to specify spurious free dynamic range. When receiver gains are
significant, the 3rd order intercept of the last stage dominates the cascade.
The 3rd order intercept compression point for two stages is given by [38, 65, 66]
In logarithmic form, we get [50, 62]:
If we use the device input as a reference, we can relate to the device output using [45]
As before, by observing the very high intercept of the preselector, we can note that the
cascaded 3rd order intercept for the preselector and LNA is that of the LNA, or 27 dBm. Using
(14.50), we now add the mixer to the cascade, resulting in

Our results thus far are summarized in Table 14.2. The remaining cascade analysis is left as
an exercise. For the last column in Table 14.2, we recall that the convention is for the
narrowest bandwidth component to set the overall bandwidth [38, p. 15]. The cascade
bandwidth is therefore set to the narrowest bandwidth of previous devices and the current
device. A summary of cascade equations is provided in Table 14.3.
Table 14.2
Example 1: Cascade Example—First Three Stages
Table 14.3
Two-Stage Cascade Equations Used for Iterative Analysis

Figure 14.11 contains plots of the results of cascading gain and noise figure for the entire
receiver chain of Figure 14.10. As we can see, the noise figure, and thus the sensitivity of the
receiver, is dominated by the LNA. The only noticeable bump (~0.3 dB) in noise figure is
because of the ADC, which we treated as a zero gain amplifier with a very large noise figure
(see Section 14.8.1.4). The general up-down trend of gain tends to yield the largest dynamic
range.

Figure 14.11 Example 1: gain and noise figure.
While the noise figure of Figure 14.11 is established by the RF LNA (and prior lossy
elements), it should be noted that an LNA with 23-dB gain and no preceding RF attenuator is
likely to be saturated by short-range clutter in a land-based radar. Either STC attenuation
preceding the LNA or a lower LNA gain may be necessary to avoid saturation of the LNA.
Either of these choices leads to a receiver noise figure higher than that of the LNA itself.
Figure 14.12 contains plots of the noise floor (see Section 14.4) and 1 dB compression
point referenced to each device input. As indicated, the LNA and the preselector loss
establishes the noise floor. Subsequent amplifiers add negligibly small amounts of noise.
Passive components do not change the noise floor at all. The only lowering of noise floor is a
result of narrowing bandwidth via the filters. The smooth decreasing trend of the 1-dB
compression point indicates no components with a detrimentally low 1-dB compression point.
Figure 14.12 Example 1: noise floor and compression point.

Figure 14.13 Example 1: dynamic range.
The difference between the 1-dB compression point and the noise floor is one of the
definitions for dynamic range (see Section 14.6), and is plotted in Figure 14.13. We note here
that dynamic range is generally decreased by lossy components and active devices, and
increased when the noise bandwidth is decreased. The mixer usually causes a dip in dynamic
range because it is a lossy device early in the chain. For this example, the overall dynamic
range is 72 dB and is constrained by the amplifier prior to the ADC, which is a limiting
amplifier. We chose the limiting amplifier saturation point to be ~ 1 dB below the full-scale
value of the ADC of 10 dBm to prevent potential damage to the ADC due to overloads.
If we have an overall goal of 100 dB of dynamic range for our radar design, we would
need about 30 dB of AGC and/or STC to extend the dynamic range of the receiver (see
Section 14.2). Coherent integration also increases the overall dynamic range of the radar. The
cascade analysis presented thus far can be extended past the receiver to the output of the signal
processor by accounting for SNR improvements and losses after the receiver. For example,
coherent integration of N pulses yields a 10·log(N2) increase in signal power and an increase
of 10·log(N) in noise power—likewise for various losses. Another way to capture this is to
use the measured bandwidth of the signal processor.
14.8 DIGITAL RECEIVER
As mentioned in Section 14.3, analog I/Q detectors suffer from I/Q channel imbalance issues.
One method of avoiding imbalance problems is to use digital hardware to perform
quadrature detection [34]. A block diagram of a wideband, frequency agile, digital,
superheterodyne receiver or, more simply, a digital receiver, is presented in Figure 14.14.

Figure 14.14 Digital receiver with direct IF sampling.
The use of direct IF sampling in the superheterodyne receiver of Figure 14.14 is what earns
the moniker digital.21 ADCs are normally used in digital receivers because digital signals are
more reliable and more flexible than analog parallels and offer reduced cost, size, weight,
and power dissipation.
Up to the differential amplifier preceding the ADC, the configuration is the same as the
double downconversion receiver of Section 14.3. However, we replaced the synchronous
detector shown in Figure 14.5 with an ADC that is directly sampling the IF signal—thus the
term, “direct IF sampling.” The amplitude and phase balance is much better (theoretically
perfect) than that achieved by baseband sampling [34].22
Since we are digitizing the IF, the analog signal needs to pass through an antialiasing filter
(AAF) designed to pass expected modulation bandwidths prior to analog-to-digital
conversion. The AAF reduces noise bandwidth and assures that negligible amounts of aliasing
occur as a result of analog-to-digital conversion. For the receiver shown in Figure 14.14, the
second IF BPF serves as an antialiasing filter, in addition to eliminating spurious signals
output of the second mixer.
We do need to make a clarification, though, when talking about the AAF. An AAF is
classically lowpass in accordance with the Nyquist sampling theorem, which applies to
lowpass signals (signals centered about 0 Hz). The Nyquist sampling theorem states that if a
time-varying signal is sampled periodically, the sampling frequency should be at least twice
the highest frequency component of the signal to prevent aliasing [67–70]. This theorem also
bears the monikers of the Shannon sampling theorem [71, 72] and the Kotel’nikov sampling
theorem [73] (as well as others). This theorem can be represented as
where fs is the sampling frequency, and B is the highest frequency contained in the signal.

Equation (14.53) is referred to as the Nyquist criterion. The values 2B and fs/2 are called the
Nyquist rate and Nyquist frequency, respectively. Satisfying the Nyquist criterion allows the
original signal to be perfectly recovered from the sampled values.
For example, let us consider sampling a 40-MHz IF and a 4-MHz chirp waveform.
According to the Nyquist criterion, we should use a sampling frequency of
which corresponds to a Nyquist frequency of fs/2 = 42 MHz. The driving factor is the 40-MHz
IF, not the 4-MHz modulation, which contains the information in the signal.
However, when using a signal centered about some IF, we often use a special case of the
Nyquist sampling theorem, referred to as the bandpass sampling theorem, which only takes
the signal bandwidth, B, that contains the information we want from the signal, into
consideration [74–76]. Brigham explains that a bandpass signal can be reconstructed from
samples if the sampling frequency, fs, satisfies the relationships [74, 75, p. 322]
and
where n is an integer. The variables fH and fL are the highest and lowest frequency component
of a signal, respectively. Expressing the minimum sample frequency in terms of signal
bandwidth, B = fH –fL, we can use (14.55) and (14.56) to form [70]
Equation (14.57) requires the sampling frequency used for direct IF sampling to be at least
twice the modulation bandwidth B. The necessary AAF is now bandpass rather than lowpass.
When using direct IF sampling, the signal is usually allowed to alias intentionally, by
undersampling with respect to the IF, acting as another downconversion stage [77].23 For
cases where the IF is under sampled, the criteria of (14.55) and (14.56) ensure that we avoid
spectrum overlap corrupting the aliased signal bandwidth [75].
The concept of Nyquist zones is often used to help in visualizing aliasing when using direct
IF sampling [59]. Nyquist zones, depicted in Figure 14.15, are bands of frequency fs/2 wide
[70].

Analog frequencies centered on fIF in an odd Nyquist zone are downconverted via aliasing
to a digital IF of
where rem(a,b) denotes the remainder after division of a divided by b [59, 78]. Similarly,
analog frequencies in an even Nyquist zone alias to [59, 78]
Analog signals that fall within odd Nyquist zones result in a mirrored spectrum. Signals that
fall within even Nyquist zones alias without frequency mirroring [70]. This is generally not a
concern, so long as we know if the aliased spectrum is mirrored (conjugated) or not.
Figure 14.15 Analog spectrum divided into Nyquist zones (After: Kester [70]).
A design goal is to place the signal to be sampled in the center of a Nyquist zone.
According to Walt Kester at Analog Devices, the sampling rate associated with Nyquist zone
centers is given by [70, p. 81]
where NZ is an integer corresponding to Nyquist zone. The largest Nyquist zone satisfying
(14.55) is generally preferable, since it produces the lowest sample rate [70].
For example, let us again consider sampling a 40-MHz IF, with a 4-MHz chirp waveform.
The minimum required sampling frequency according to (14.57) is 8 MHz. Increasing the
sample rate increases the potential distance between images, allowing for more margin for
the AAF. Let our ADC operate at 60 MSPS (megasamples per second). The resultant Nyquist
zone is

which is not an integer.
To place the 40-MHz IF in the second IF zone, we would need fs to be 53.33 MHz. If a NZ is
not an integer, this indicates that our spectrum is not exactly centered on a Nyquist zone.
Depending upon the available clock frequencies, this may be unavoidable, or at least an
acceptable compromise, with the exciter design (e.g., all clocks are a multiple of 10 MHz). So
long as the AAF provides sufficient rejection of the aliased image, this is tolerable. For this
reason, we will continue this example using a sampling rate of 60 MSPS.
The digital downconverter (DDC) shown in Figure 14.14 is typical of the type used for
direct IF sampling. The operation of a DDC is analogous to the operation of the I/Q
demodulator of Figure 14.5 described in Section 14.3. A DDC is used to shift the analog
spectrum of interest from its IF to baseband. A DDC often includes a decimation stage at the
output to reduce the data rate of subsequent processing.
Continuing our example of a digital receiver with a second IF of 40 MHz sampled at a rate
of 60 MSPS in a radar using a 4-MHz chirp waveform, let us look at the spectra of the signals
produced in the DDC. This places our analog waveform in the second Nyquist zone (bands of
frequency fs/2 wide explained above) which is from 30 MHz to 60 MHz. Using (14.58), the
aliased digital IF is rem(60, 40) = 20 MHz, as depicted in the top graph of Figure 14.16.
Figure 14.16 DDC spectra.

The next step in the digital downconversion process is to translate either the upper image
(centered on 20 MHz) or the lower image (centered on –20 MHz) to baseband (centered on 0
Hz) using a numerically controlled oscillator (NCO). The NCO generates two digital
reference frequencies with quadrature phase. For this example, we choose 20 MHz as the
NCO frequency. The lower image is translated up 20 MHz to baseband as depicted in the
middle graph of Figure 14.16. The upper image is translated up to 40 MHz, but aliases (wraps
in frequency) to –20 MHz. Recall that the spectrum generated by the Fourier transform is
periodic.
After translating the signal to baseband, we want to remove the image centered at –20 MHz.
To do this, we use digital LPFs in the I and Q channels to reject the unwanted image, which
are analogous to the LPFs used in the I/Q detector of Figure 14.5. The digitally filtered output
results in the I and Q terms we are after. The spectrum at the output of the LPFs is depicted in
the bottom graph of Figure 14.16. At this point in our example, we have a complex baseband
representation of our waveform, suitable for digital signal processing.
We now have a 4-MHz baseband signal that is sampled at 60 MSPS. Since the sample
frequency is 15 times larger than the signal bandwidth, the signal is greatly oversampled. To
reduce the amount of processing necessary, the signal is decimated, or downsampled
according to the modulation bandwidth. A slightly higher sampling rate (~1.5xB) is sometime
used to avoid affecting the sidelobe levels of LFM waveforms. Matching the chirp bandwidth
exactly can potentially elevate the sidelobe levels in the compressed waveform due to aliasing
caused by insufficient rejection of the aliased image. Higher sample rates also result in lower
straddle loss.
Thus, relative to the sample rate we need for our example, we are oversampled by a factor
of 60/(1.5×4) = 10. This means we can decimate the signal by a factor of 10 and still have a
sample rate that is adequate for subsequent processing. For this reason, most DDCs include a
stage of decimation.
Since the heart of a digital receiver is the ADC and DDC, the choice of the 2nd IF, sampling
frequency, and ADC properties are critical. The 2nd IF is generally low (<50 MHz or so),
depending upon the ADC used and sampling rate used.
A single-ended input to differential output amplifier is shown driving the ADC in Figure
14.14 to make note of the fact that many high performance (high sample rate, high dynamic
range) ADCs are now being designed with differential inputs [67]. An RF transformer, such as
the ADT4-1WT from Mini-Circuits, can also be used to couple into a differential input ADC
[67]. Using differential inputs offers benefits such as better distortion performance,
cancellation of even harmonics and common mode rejection of noise [70].
14.8.1 Analog-to-Digital Converter
Incorporating an ADC into a radar receiver primarily affects dynamic range and sensitivity.
Because of this, we will examine some key ADC parameters and how they factor into a
receiver design. It is also important to understand the effects of noise present at the input to

the ADC (usually called dither), quantization noise generated as a result of quantizing an input
signal, and noise generated internally by the ADC due to circuit noise and timing instabilities.
An ADC, depicted functionally in Figure 14.17, performs the operations of sampling in
time and quantizing in amplitude. Specifically, it samples and quantizes a continuous time
signal, x(t), to produce a digitized output, xq(n), that is a discrete time number sequence. This
many to one mapping occurs because an ADC represents each signal sample using a finite
number of Binary digITs or bits, b.24
Figure 14.17 Block diagram of A/D converter. (After: Rabiner & Gold, 1975 [79].)
ADCs are designed to operate on either unipolar or bipolar inputs. For our application, we
will discuss bipolar converters, which are typical of ADCs used in digital downconversion.
Representative input ranges for bipolar converters include ±1, ±2, ±2.5, ±5, and ±10 V,25 with
faster converters generally having smaller input ranges.
14.8.1.1 Quantization
The difference between the maximum, Vmax, and the minimum, Vmin, input values to an ADC
is referred to as the full-scale range (FSR)
For example, the full-scale range of a bipolar ADC with a specified analog input range of ±1
V is
Given an output word length of b bits, we may represent L = 2b unique discrete levels, which
are mapped to particular voltage levels, depending upon the full-scale voltage of the ADC.
Each level at the ADC output is separated by
which is known synonymously as the quantization interval, a quanta, or the least significant bit
(lsb) of the ADC [59]. As an example, for a word length of b = 4 bits, we can represent L = 24
= 16 discrete levels. For a full-scale range of 2 V (±1 V), the lsb size of the ADC becomes

The mapping from analog input to digital output (a nonlinear mapping) is typically
performed via truncation or rounding. We will consider quantization via rounding, or [80, p.
11]
which is typical for digital signal processing applications. Quantization via rounding also
results in a quantization error that is symmetrical about zero, which is mathematically
convenient.26 For illustration, a full-scale sinusoidal input quantized by a 4 bit ADC with a
full-scale range of 2 V is depicted in Figure 14.18.
Note that using (14.66) results in 17 levels instead of 16. For quantizers with greater than
about L = 32 levels (5 bits) or so, the effect of this extra level is negligible. In practical ADCs,
the encoded range is usually –L/2 to L/2 –1. We will choose to ignore this extra level, which
simplifies simulation of ADC quantization.
14.8.1.2 Quantization Error
The difference in amplitude between the analog input to an ADC and the quantized digital
output is referred to as the quantization error. The quantization error for the quantized
sinusoid given in Figure 14.18 is shown in Figure 14.19.
Figure 14.18 Quantized sinusoid, 4 bits, ±1 V analog input range, 2V FSR.

Figure 14.19 Quantization error for quantized sinusoid, 4 bits, ±1 V analog input range, 2V FSR.
Figure 14.20 Ideal ADC transfer function and ideal quantization error.
The ideal transfer function for our example 4 bit, ±1 V, bipolar ADC example is presented
in Figure 14.20. For this example, the ideal quantization error shown in Figure 14.20 has an
extent of ± ∆/2. Quantization errors in excess of ± ∆/2 indicate overload of the ADC.
Let us now quantify the mean-square value for the sawtooth quantization error shown in

Figure 14.20. We will follow the derivation by Walt Kester presented in [67].The equation for
a sawtooth can be expressed by
The mean-square value of the sawtooth error voltage may be derived as
and the root-mean-square (rms) value of the quantization error is then given by
where ∆ is the lsb (volts).27
We could next quantify the mean-square value for the quantization error associated with a
sinusoid, shown in Figure 14.19. However, for a sinusoidal signal that spans several quanta,
(14.69) can serve as an approximation for the rms quantization error [70, p. 83].
Let us now compare the rms value of the quantization error to the rms value of a full-scale
sinusoidal input signal. The full-scale rms voltage for a sinusoidal input is given by [67]
The ideal rms “signal-to-noise ratio”28 in (W/W) with respect to quantization error for an
ideal ADC is then
over the Nyquist bandwidth from dc to fs/2 where fs is the sampling frequency in Hz [81]. In
decibel form, (14.71) becomes [82]

Equation (14.72) represents the ideal signal-to-quantization error ratio of an ADC given
sinusoidal inputs often quoted in literature [38, 39, 54, 70, 83, 84]. It should be emphasized
that the achieved SNR for a practical ADC is always less than the theoretical SNRq calculated
from the number of bits. The theoretical performance of an ADC is however a useful gauge
for comparison.
In keeping with relating parameters in the receiver to absolute levels, let us put the full-
scale input and rms quantization voltage in terms of dBm. The full-scale signal power into an
ADC in dBm is given by [70, p. 133]
where we recall the addition of 30 results in dBm instead of dBW. For ±1 V input range to the
ADC matched to a system impedance of 50 Ω, results in a full-scale power of 10 dBm. The
rms quantization error level becomes
Equation (14.72) is just one of several SNRq equations associated with ADCs [38, 53].29 The
level of quantization error can also be estimated via FFT [70, 82].
It should be noted that for these classical examples of quantization error, the error is
completely deterministic. We have not yet approached quantization error analysis using a
stochastic interpretation, which we will do shortly.
Even though we have not performed any stochastic analysis, eq of (14.69) is frequently
referred to synonymously as the rms quantization noise [59, 70]. Likewise, SNRq of (14.72) is
usually referred to synonymously as the signal to quantization noise ratio [59]. This is
because, when approached from a stochastic point of view, the results are the same as (14.69)
and (14.72) [84]. This is explored in Exercise 22.
14.8.1.3 Quantization Noise
While we can perform deterministic quantization error analysis, we usually treat quantization

error as a random process and label it “quantization noise.” We use this stochastic
interpretation because we have the tools to handle random quantization error (i.e.,
quantization noise) but we do not have the tools needed to analyze deterministic quantization
error in terms of its effect on signal processing.
The general assumptions used for quantization noise are [85, 86]:
1. The quantization noise is additive and white.
2. The quantization noise is uncorrelated with the signal being quantized.
3. The quantization noise is uniformly distributed between ± ∆/2, resulting in zero mean
and variance of ∆ 2/12 [see (14.68)].
The three assumptions listed above allow our treatment of quantization error as noise to be
closer to being “theoretically” valid. The three assumptions are true if some amount of
receiver noise is presented to the input of the ADC. The amount of noise required is set by
making the expected standard deviation of the noise greater than the quantization level ∆.
Without noise present at the input of the ADC, the resulting quantization error can be
deterministic and harmonic, which produces spurs. As ADC bit lengths grow, however,
internal instabilities in the ADC become more dominant, reducing the chance of spurious
quantization noise. The additive noise present at the input to an ADC is often referred to as
dither (Section 14.8.1.5).
Despite the third assumption listed above, Bennet notes that quantization noise is
approximately Gaussian and essentially spread uniformly over the Nyquist bandwidth of dc to
fs/2 [81]. Interestingly, Widrow and Kollar refer to the general assumptions used for
quantization noise listed above as more rumor than fact, but they do concede that these
rumors are true under most circumstances, or are at least a very good approximation [80].
One important motivation for these assumptions is that they result in greatly simplified
mathematical analysis since a nonlinear system now behaves like a linear system (the system
has been linearized) [88–90].
Since we treat quantization noise at the output of the ADC as being uniformly distributed
across the Nyquist bandwidth, we need to account for times when we filter the output of the
ADC because the signal bandwidth in our receiver is less than the Nyquist bandwidth. This
filtering eliminates quantization noise outside the signal bandwidth. To account for this, we
modify (14.72) by including a processing gain, (fs/2)/B, which results in [70]
where B is the signal bandwidth, or technically the filter bandwidth if it is wider than the
modulation bandwidth.30
14.8.1.4 ADC Noise Figure
Having derived the ideal SNR performance for an ideal ADC, we would like to quantify how

this compares to a practical ADC, which never achieves the ideal SNR. More importantly, we
would like to incorporate the performance of a practical ADC into our receiver cascade
analysis. The SNR of a practical ADC is difficult to predict analytically. The actual SNR of an
ADC is generally provided by ADC manufacturers though.
ADC manufacturers usually measure SNRADC using a sinusoidal test signal at the ADC
input. The test signal is usually full scale, or 0.5 to 1 dB below full scale (dBFS). Staying just
under full scale at the input to the ADC is sometimes done because it results in better spurious
behavior than a full-scale input.
Walt Kester at Analog Devices and James Karki at Texas Instruments both present a
technique we can use to incorporate an ADC into a cascade analysis using an equivalent noise
figure for the ADC, which is derived using the measured ADC SNRADC [70, 91, 92]. We will
follow their example here. Specifically, an ADC can be thought of as a unity gain amplifier,
with a given noise figure, and included in a cascade analysis (see Section 14.7). An equivalent
ADC noise figure in excess of 30 dB is not unusual.
To derive the noise figure of an ADC, we first need to think about the power spectral
density of the ADC noise, which includes quantization noise and noise internally generated by
the ADC circuitry. Given SNRADC by the manufacturer, the ADC noise power can be expressed
as
where PFS is the full-scale power into the ADC given by (14.73). The factor of 1 subtracted
from the full-scale input power in (14.76) is indicative of the manufacturer using a –1 dBFS
test signal to measure SNRADC.
To express (14.76) in terms of power spectral density, we note SNRADC is specified for
noise evenly distributed across the Nyquist bandwidth from dc to fs/2. Adding a bandwidth
term to relate (14.76) to a 1-Hz bandwidth, we get the ADC noise power spectral density given
by James Karki in his derivation, which is [92]
Equation (14.77) represents the power spectral density of quantization noise and noise
internally generated by the ADC combined.
In formulating noise figure, we also need the noise into the ADC. The reference power
spectral density into the ADC from thermal noise, is given by
Recall that noise figure can be expressed as

Noting that the ideal gain through the ADC is 1, for example, SI = SO, and substituting (14.77)
and (14.78) into (14.79), we get
We can approximate (14.80) as given by Walt Kester (see Exercise 13) as [70, p. 102; 93]
As an example, let the full-scale power into the ADC be +10 dBm, corresponding to an
analog input range of ±1 V and let the input resistance of the ADC be 50 Ω [see (14.73)]. We
assume a 14-bit converter, operating at fs= 60 MSPS, with a specified SNR of 74.8 dB.
Substitution into (14.81) yields
We have made the tacit assumption of matched impedances, which are typically 50 Ω. The
input impedance of an ADC is not always 50 Ω. ADC input impedances of 200 Ω and 1 KΩ are
not unusual. To avoid impedance mismatch, one practice is to use an impedance matching
transformer to match the system impedance to the ADC impedance [70, 92].
James Karki uses the example of an ADC with an input impedance of 200 Ω. Matching the
50 Ω system output impedance to a 200 Ω ADC input impedance requires a 1:4 impedance
ratio (1:2 turns ratio) transformer [92]. Compared to (14.81), the ADC noise figure is reduced
by the 4:1 impedance ratio of the system and the ADC impedance, which can be expressed as
For this example, the ADC noise figure is reduced by
Gain Prior to ADC

An important design consideration is to establish the right amount of amplified receiver noise
to act as dither at the input to the ADC (see Section 14.8.1.5) [53]. We can use the ADC noise
figure of (14.81) to determine how much gain (and analog noise figure) is necessary prior to
the ADC in order to minimize its impact on overall system noise figure, while still dithering
the input. The receiver gain in combination with the full-scale level of the ADC, determines
the maximum signal input to the receiver. As such, there is a tradeoff between system noise
figure and maximum input signal or dynamic range.
For illustration, we will consider a two-stage cascade of the RF front end followed by an
ADC, depicted in Figure 14.21. The gain and noise figure of the analog portion of a digital
receiver are encompassed in GRF and FRF, respectively. For the ADC stage, GADC = 1, and
FADC is the NF of the ADC.
Let ∆F represent the amount of acceptable noise figure degradation. By comparing the
noise figure of the receiver front end and ADC cascade to the noise figure of the receiver
front end, we can write (see Exercise 22) [94]
A general rule of thumb for ∆F is to allow a few tenths of dB increase in noise figure due
to the ADC. This offers a reasonable compromise between gain and sensitivity. For example,
let the noise figure of the RF front end be 5 dB with an ADC noise figure of 30 dB. In our
design, let the acceptable amount of degradation be 0.4 dB [93]. The necessary amount of RF
front end gain up to the ADC becomes
Determining how much dither noise is applied to the ADC input by using this technique is left
as an exercise. We note that 35.2 dB is fairly high compared to the 20 to 25 dB RF LNA gain
generally required by an analog design to establish noise figure. This is due to the very high
noise figure of the ADC.
Figure 14.21 ADC cascade.
In addition to performing a cascade analysis using an equivalent noise figure for the ADC,
there are a number of other approaches and guidelines used for establishing the correct
amount of dither into the ADC. One general rule is to use 1 to 1.5 bits of dither. Lyons
suggests an rms level of 1/3 to 1 lsb voltage level for wideband dither and 4 to 6 lsb voltage

levels for out-of-band dither [95, p. 708].
Barton characterizes the quantizing noise voltage added by the ADC as
which is a combination of (14.64) and (14.69) [63, p. 220].
Similarly to the ADC noise figure approach discussed earlier, Barton recommends
adjusting the gain prior to the ADC such that the rms noise voltage at the output of the ADC,
resulting from thermal noise and quantization, is
where Barton suggests q ≈ 1.5, which is a constant chosen to provide a practical compromise
between the conflicting needs of dynamic range and small quantizing noise (e.g., sensitivity).
This results in a thermal noise power at the input to the ADC that is 12q2 time the quantizing
noise power [63]. It is left as an exercise to see how this approach compares to the ADC noise
figure approach [91, 92].
14.8.1.5 Dither
Dithering is the deliberate use of a small amount of noise at the input to an ADC that is
uncorrelated with the signal to be digitized. This noise is usually referred to as “dither noise”
or simply “dither.” One purpose of dither is to counter the effects of quantization noise by
controlling the statistical properties of quantization error. Another is to linearize the
characteristics of the ADC, thus improving the effective resolution of the ADC.
Dithering is imperative in radar receivers for a number of reasons. For instance, digitizing
sinusoidal signals can result in quantization noise that is highly correlated, resulting in
spurious signals at harmonics of the input. Dithering randomizes the quantization error,
reducing spurious levels.
Dithering is also very important when considering weak, or subquanta, signals [96]. A weak
signal that exercises only a single quanta results in clipping, causing numerous spectral
harmonics. Subquanta signals, which would not exercise even a single quanta, are irrevocably
lost due to the ADC. Dithering preserves the information from weak or subquanta inputs
(including their power ratios) by whitening the signal and clutter components of the ADC
input. Dithering causes these signals to exercise at least a few quanta, allowing signal and
clutter components to be recovered by via coherent integration.31
Some of the earliest work on the ability of dither to extend ADC dynamic range via
coherent integration was published in 1963 by G. G. Furman in two RAND Corp reports [88,
89]. Furman considered sinusoidal and sawtooth dither signals, asserting that dither improves
quantizer performance by enabling coarse quantizers to emulate ultrafine ones [90].
Vanderkooy and Lipshitz, showed that, by the use of dither, the resolution of an ADC can be

improved to well below the least significant bit [97]. Oppenheim emphasizes that to preserve
dynamic range, at least the lowest level of the ADC must be dithered by noise [98, p. 309].
Dither noise can be generated in a number of ways. One common method employed in
radar is to use amplified thermal noise from the receiver front end (see Section 14.8.1.4),
where the receiver gain is designed to establish the desired level of noise into the ADC (see
Section 14.8.1.5). This type of dither is bandlimited according to the RF-IF bandwidth, which
is typically less than the Nyquist frequency. Similarly, we can inject random noise from a
calibrated external noise source [95]. The downside of these approaches is that the dither falls
within the passband of the receiver, resulting in a loss in sensitivity.
There are techniques aimed at avoiding this loss of sensitivity by removing the dither after
it has served its purpose. One approach is referred to as subtractive dither, which uses
digitally generated pseudorandom noise. The pseudorandom noise is converted to analog and
added to the signal into the ADC. It is then removed via subtraction after conversion.32 An
analog variation of this, referred to as out-of-band dither, is to use band limited dither, usually
low frequency noise, which is designed to be rejected by subsequent digital filtering [95, 99].
14.9 RECEIVER CONFIGURATIONS
We close this chapter with a brief discussion of some receiver configurations and discuss
some of their balance, alignment, and calibration requirements. Figure 14.22 contains a
simplified block diagram of a three-channel, monopulse receiver. A monopulse receiver is
used on radars where there is a requirement to provide a three dimensional measurement of
target position. These radars typically measure a range-related quantity, ∆r, and two
orthogonal, angle-related quantities, ∆u and ∆v. ∆r is usually measured relative to an expected
target range, such as the output of a range tracker. ∆u and ∆v are angle quantities relative to
boresight, which is the direction the radar beam is pointing. ∆r, ∆u, and ∆v can be combined
with other range and angle parameters to determine the target location relative to some
coordinate system such as a Cartesian coordinate system centered at the radar.

Figure 14.22 Three-channel monopulse receiver and processor.
The term “monopulse” derives from the fact that the radar, ideally, measures ∆r, ∆u, and ∆v
based on the return from a single (mono) pulse, or a burst of pulses if the radar is using
coherent processing. The modifier “three-channel” derives from the use of separate
receivers, or channels, for each parameter: ∆r, ∆u, and ∆v.
The three signals processed by the receiver channels are formed in the feed/array, indicated
notionally, on the left of the diagram. In a radar that uses a reflector antenna or a space-fed
phased array (see Chapter 12), the device is the antenna feed and in a constrained-feed phased
array, the device is the array itself. In one of the simplest forms, the feed consists of four horn
antennas spaced close together. In practice, the feed can consist of several horn antennas
where some of the horn antennas are multimode [38, 100–104]. Multihorn, multimode feeds
are used when there is a desire or requirement to simultaneously provide sidelobe control of
both sum and difference antenna patterns. In a constrained-feed phased array, again in the
simplest form, the array is divided into four quadrants to provide the necessary signals.
The outputs of the four ports of the feed, or the four quadrants of the array, are combined
in the monopulse combiner to create the three signals used by the monopulse receiver and the
subsequent monopulse processor. In one case, the four outputs are summed to form the sum,
or Σ, signal. One of the orthogonal angle channel signals (e.g., ∆v) is formed by summing the
signals from ports 1 and 2, summing the signals from ports 3 and 4, and subtracting the two
sums. This is termed the ∆v difference channel signal. The other orthogonal angle channel
signal, ∆u, is formed by summing the signals from ports 1 and 3, summing the signals from
ports 2 and 4, and subtracting the two sums. In equation form

We note that we are forming the sums of voltages, with the sums being performed at the RF.
The implication of this is that the relative phases and amplitudes of v1(t) through v4(t) must be
preserved in the monopulse combiner for all RFs of interest. This places restrictions on the
combiner. Also, as discussed in Section 14.7.4, the combiner must be a low loss device since
its loss contributes directly to receiver noise figure.
If we plot normalized versions of vΣ(t1), v∆u(t1), and v∆v(t1) as we vary the target location
relative to boresight, we have a normalized sum and two normalized difference voltage
patterns. In these expressions, t1 is time the target return is present. Examples of a sum and
one of the difference patterns are contained in Figure 14.23 [normalized to the peak of vΣ(t1)].
The difference voltage plot has two “main beams” and, more importantly, is zero when the
target is at boresight. Also, the sign and magnitude of the difference voltage is directly related
to the location of the target relative to boresight. This is the information we use to determine
∆u and ∆v, the target angles relative to boresight.
After the monopulse combiner, the vΣ(t), v∆u(t), and v∆v(t) signals are sent to three identical
receiver channels. A key term here is “identical.” The receivers must have identical gain and
phase characteristics over their entire operating frequency range. Also, the gain and phase
characteristics should be independent of signal amplitudes. This is important because the vΣ(t)
and v∆(t) signals have much different amplitudes. If the target is at, or close to, boresight,
vΣ(t) will be large and v∆(t) will be small. If the receivers do not provide the same gain and
phase shifts to vΣ(t) and v∆(t), the subsequent processing used to determine ∆u and ∆v will not
give the expected result.
Figure 14.23 Sum and difference patterns.

Since the three channels are not generally identical, the receivers and the ∆u and ∆v
formation circuits/algorithms must be calibrated, which is usually accomplished by creating
discriminator curves. This can be done by radiating a test signal from a test tower in the far
field of the antenna and moving the antenna boresight while measuring ∆u and ∆v. The plots
of ∆u and ∆v versus the angle between the test signal and boresight are the discriminator
curves. The ∆u discriminator curve is generated for ∆v = 0, and vice versa. As a note,
calibration is especially important in digital receivers where the ADC can introduce
significant nonlinearities at small signal levels (see Section 14.8).
The calibration and alignment must be performed at several frequencies within the
operating band of the radar since phase errors can be caused by path length differences in the
combiner and other plumbing between the feed outputs (outputs of the four horns or four
array quadrants) and the first mixer (see Figure 5.12). Also, gain and phase characteristics of
the three RF amplifiers will most likely be different over the RF operating range. Since the
characteristics of the various receiver components can change over relatively short periods
of time, it is often necessary that calibration be performed regularly. This is most often
accomplished by injecting a test signal, termed a pilot pulse, into the receiver front end and
determining the amount of phase and gain imbalance between channels. Channel balance is
then maintained by controlling attenuators and phase shifters in each receiver channel
accordingly (or in the calculating of the monopulse output if implemented via computer)
[105, p. 69].
As indicated earlier, the ∆u and ∆v signals are formed in the angle discriminators. The
angle discriminators of Figure 14.22 would apply to a reflector antenna or a space-fed phased
array antenna because the form of ∆ (∆u or ∆v) is
For a constrained-feed phased array, the real operator would be replaced by the imaginary
operator since the angle information in this type of an array is contained in the imaginary part
of v∆o/vΣo [105].
In (14.90), vΣo = GΣvΣ(t1) and v∆o = G∆v∆(t1) where ∆ could be ∆u or ∆v and t1 is the time at
which the matched filter output is sampled (hopefully at the target range delay). G∆ and GΣ are
the total, complex voltage gains of the sum and difference receivers, from the feed output to
the inputs of the discriminators. K∆ is a scale factor that converts the ratio to an angle. Figure
14.24 contains a plot of ∆ versus u for the sum and difference pattern plots of Figure 14.23.
The angle, u, is the angle between the antenna boresight and the LOS to the target, and has the
units of sines (see Chapter 12). K∆ was chosen so that the slope of the curve is unity.

Figure 14.24 Angle discriminator—amplitude imbalance.
The solid curve corresponds to the balanced case where G∆ = GΣ and the dashed curve
corresponds to the case where G∆ = (21/2)GΣ, a 3-dB gain imbalance. As can be seen, the slope
of the discriminator curve is no longer unity for the imbalance case. This can have an impact
on track loop performance in that the slope of the discriminator curve directly affects the
closed-loop bandwidth of the track loop, and thus the track accuracy.
Figure 14.25 contains a plot for the case where |G∆| = |GΣ| but where the phases differ by
30°. As with the gain imbalance, the phase imbalance caused a change in the slope of the
discriminator curve.
Receiver calibration can also affect the output of the range discriminator, but not as much
as for angle. This is because of the way the ∆r signal is usually formed. Specifically, the ∆r
signal is formed by some variation of the equation33
where |VL| and |VE| are termed the late and early gate signals and are defined by

where τtrk is the expected target range delay from the range tracker and ∆τ is an offset about
τtrk. Typically, ∆τ is one-half of the compressed pulsewidth [106]. This is illustrated in Figure
14.26 for the case of an ideal, unmodulated pulse. Kr is chosen so that ∆r has the desired units
(e.g., m).
Figure 14.25 Angle discriminator—phase imbalance.
Figure 14.26 Illustration of range samples.
The forms of (14.91) and (14.92) mean that receiver calibration and channel balance are not
an issue in range tracking because GΣ cancels in the numerator and denominator of (14.91). A

calibration factor that must be considered is pulse shape at the output of the matched filter, or
imbalances in the signal processing between the matched filter output and the input to the
range discriminator. As with the angle discriminator, these can be accounted for by
calibration.
Early radars tried to conserve hardware by using two receiver channels instead of three. An
example of one such implementation is shown in Figure 14.27. In this diagram, the ∆ signal is
switched between the ∆u and ∆v signals from the combiner and the difference signal is added
to and subtracted from the sum signal to form vΣ(t) + v∆(t) and vΣ(t) –v∆(t). While this
implementation saves hardware, it also doubles the amount of time required to determine all
three of the ∆r, ∆u, and ∆v parameters.
The use of vΣ(t) + v∆(t) and vΣ(t) –v∆(t) relieves some of the gain and phase linearity issues
in that both vΣ(t) + v∆(t) and vΣ(t) –v∆(t) are about the same size when the radar is tracking the
target. However, gain and phase imbalance becomes more of an issue. In the example of
Figure 14.27, the ∆ signal is formed as
If |GΔ+Σ| = |GΔ–Σ| and |vΔ (t1)| ≪ |vΣ(t1)|, (14.93) reduces to [105, p. 167]
Figure 14.27 Two-channel monopulse receiver.
If |G∆+Σ| ≠ |G∆–Σ|, ∆ can vary significantly from this ideal value and can even lead to bias
errors in the angle tracker.
To mitigate the problems caused by channel imbalance in two-channel receivers, some
early radars also reversed the signals into the receiver on alternate dwells (pulses or coherent

processing intervals). One example of this is illustrated in Figure 14.28. In this case, the
receiver channels alternately carry vΣ(t) –/+ v∆(t) and vΣ(t) +/–v∆(t). This will cause errors
due to channel imbalance to average out over time. However, now the update rate has been
decreased by a factor of four relative to full monopulse.
Instead of reducing the number of receiver channels, we can increase them to four and
eliminate the monopulse combiner. This is illustrated in Figure 14.29. With this configuration,
we process v1(t) through v4(t) in separate receivers and form the sum and difference signals
at the output of the signal processors. If we use a digital receiver or a digital signal processor,
we would have an implementation of digital beam forming. This technique would enhance
flexibility in that we could form (tightly spaced) multiple simultaneous beams, or
simultaneously implement amplitude and phase comparison monopulse (for a constrained-
feed phased array or an active array) or perform some other angle functions such as sidelobe
cancellation. The price paid for this flexibility is that receiver balance, calibration and
alignment, over the operational RFs of the radar, become much more important. If the
receivers are not properly balanced, we would likely introduce significant angle bias errors
and could significantly degrade the monopulse discriminator. Depending on how the signal
processor outputs are combined to determine range error, this could also be seriously
degraded by channel imbalance.
Figure 14.28 Two-channel receiver with Δ sign switching.
Figure 14.29 Four-channel receiver.
An extension to the four-channel receiver that is being implemented in some modern radars
[107, 108] is to divide the array (usually an active array) into a large number of subarrays
with a separate receiver, and possibly signal processor, for each subarray. With this, the
concept of digital beamforming can be expanded to include multiple simultaneous beams,
adaptive nulling, difference pattern sidelobe control, and/or some form of super-resolution

technique (e.g., MUltiple SIgnal Classification, or MUSIC [109], that has been only
theoretically considered in the past. However, as implied by the above discussions, this added
capability comes at the cost of a more stringent balance requirements. A counter to this would
be to move the ADCs closer to the LNA output of the T/R modules and have a true “digital
radar,” wherein calibration and alignment can be performed digitally.
14.10 EXERCISES
1.
Given an RF of 10 GHz, a 1st IF of 60 MHz, and assuming a low side LO, what does the
LO frequency need to be? What is the image frequency? If we use a 4th order Butterworth
filter with an 8-MHz passband as preselector, how much image rejection is provided?
2.
Repeat Exercise 1 using a 1st IF of 540 MHz and assuming a high side LO.
3.
Calculate the noise bandwidth of a Butterworth filter for orders 1 through 5. How does
the noise bandwidth compare to the 3-dB bandwidth? Noise bandwidth is given by [63, p.
198]
where H(f) is the frequency response and f0 is the center of the frequency response.
4.
Show that the units in (14.10) are correct.
5.
Simulate Gaussian noise passed through a 4th order Butterworth filter with a 3-dB
bandwidth of 10 MHz at a sample rate of 100 MHz. Use a noise figure of 5 dB and a gain
of 0 dB. Look at the ensemble average of the output for 100 runs in the time domain.
Does the result correlate to (14.8)? Hint: to generate the input noise power, use B = 100
MHz.
6.
For the RF chain shown in Figure 14.6, calculate the gain, noise figure, and noise power
at each device output.
7.
Simulate noise and signal plus noise for a 10-µs pulse in additive white Gaussian noise
passed through a 4th order Butterworth filter with a 3-dB bandwidth of 8 MHz. Run the
simulation as a sample rate of 100 MHz. Use a noise figure of 5 dB and a gain of 0 dB to
generate the input noise. Look at the time domain output and adjust the SNR until the TSS
requirement is met. Is the SNR as expected?
8.
Complete the cascade analysis for the receiver chain shown in Figure 14.10. Do your
results match those indicated in Figures 14.11, 14.12, and 14.13?
9.
We want to choose a sample rate to center our signal in a Nyquist zone so that we
maximize the amount of transition we have. For a 5-MHz chirp waveform on a 30 MHz
IF, what is the required ADC rate to center the signal in the second Nyquist zone? What is

the aliased digital IF? What is the spacing between images?
10. Using the parameters from Exercise 9, generate a figure like Figure 14.16.
11. Generate a baseband 3-MHz chirp waveform with a 100-µs pulsewidth. Using a sample
rate of 3 MHz, digitally match filter the waveform. Repeat for sample rates of 4.5 MHz, 6
MHz, and 7.5 MHz. Compare the sidelobe levels of the compressed pulse. Is there a
benefit to oversampling slightly?
12. Why is the noise figure expression of an (14.81) approximation? Hint: derive in terms of
the linear definition for noise figure.
13. Given an RF of 8 GHz and a first IF of 30 MHz, what is the LO for highside injection?
What is the image frequency? If we use a 2nd order Butterworth as a preselector, how
much image rejection do we get? If we need 60 dB of image rejection, what is the
minimum filter order?
14. Using the parameters of Exercise 13, calculate all of the spurious frequencies generated
by mixing up to the eighth order. What is the nearest frequency to the passband of the
preselector? Are there any spurious responses in the passband of the preselector?
15. Consider, for example, two tones at f1 = 60 MHz and f2 = 63 MHz. What are the
frequencies of the 2nd- and 3rd-order intermodulation products? Which tones are nearest
the desired signals in frequency? Which tones are farthest away? Is it practical to reject
some or all of the intermodulation products by filtering?
16. A receiver has a 3rd order input intercept point of –10 dBm and a noise figure of 7 dB and
a bandwidth of 4 MHz. What is the spurious-free dynamic range?
17. Considering a radar transmitting a 1-MHz chirp with a 40 µs pulsewidth with a maximum
instrumented range of 120 km. This radar is to accommodate target RCSs between σmin =
0.01 m2 (–20 dBsm) and σmax = 1,000 m2 (30 dBsm). What is the minimum dynamic range
we need to design the radar for?
18. Consider a 16-bit ADC with a bipolar input of ±10 V, what is the full scale range in dBm
given a 50 Ω system impedance? What is the LSB? If the manufacturers specified SNR is
78 dB, what is the effective noise figure of the ADC?
19. The ADC described in Exercise 18 has a 1 kΩ impedance instead of the 50 Ω impedance
of the rest of the receiver chain. If we use an inductive impedance transformer, what is the
effect on the noise figure of the ADC?
20. Consider the cascade shown in Figure 4.21. If the receiver and ADC have noise figures of
7 dB and 30 dB, respectively, if we allow ∆F = 0.3 dB of noise figure degradation, how
much gain to I need in the receiver front end to use amplified thermal noise from the RF
front end as dither? Determine the thermal noise power into the ADC and the total noise
power at the output of the ADC.
21. Treat the quantization error voltage as a random variable ε with a uniform probability
density function spanning ±q/2 with an amplitude of 1/q. Calculate the mean-square and

root mean-square values. How do they compare to the results of (14.69) and (14.72).
22. Derive (14.85). Hint: start with the cascade noise figure of the receiver front end and the
ADC and compare to the noise figure of the receiver front end.
23. For the example associated with Figure 4.21, determine how much dither noise is applied
to the ADC input.
24. Using the parameters from Exercise 20, determine the gain necessary prior to the ADC,
the thermal noise into the ADC, and the total noise at the output of the ADC using
Barton’s approach given by (14.87) and (14.88). How does this compare to the results of
Exercise 20? If the results differ, what ∆F would I need to match the results?
References
[1]
Armstrong, E. H., “Some Recent Developments in the Audion Receiver,” Proc. Inst. Radio Eng., vol. 3, no. 3, Sept.
1915, pp. 215–238.
[2]
Armstrong, E. H., “Method of Receiving High Frequency Oscillations,” U.S. Patent 1,342,885, Jun. 8, 1920.
[3]
Armstrong, E. H., “The Super-Heterodyne-Its Origin, Development, and Some Recent Improvements,” Proc. Inst. Radio
Eng., vol. 12, no. 5, Oct. 1924, pp. 539–552.
[4]
Brittain, J. E., “Electrical Engineering Hall of Fame: Edwin H. Armstrong,” Proc. IEEE, vol. 92, no. 3, Mar. 2004, pp.
575–578.
[5]
Fessenden, R. A., “Wireless Signaling,” U.S. Patent 706,740, Aug. 12, 1902.
[6]
Schottky, W. H., “On the Origin of the Super-Heterodyne Method,” Proc. IRE, vol. 14, no. 5, Oct. 1926, pp. 695–698.
[7]
Belrose, J. S., “Reginald Aubrey Fessenden and the Birth of Wireless Telephony,” IEEE Antenn. Propagat. Mag., vol.
44, no. 2, Apr. 2002, pp. 38–47.
[8]
Brittain, J.E., “Electrical Engineering Hall of Fame: Reginald A. Fessenden,” Proc. IEEE, vol. 92, no. 11, Nov. 2004, pp.
1866–1869.
[9]
De Forest, L., “Space Telegraphy,” U.S. Patent 879,532, Feb. 18, 1908.
[10] De Forest, L., “The Audion: A New Receiver for Wireless Telegraphy,” Proc. AIEE, vol. 25, no. 10, Oct. 1906, pp. 719–
747. Also published as De Forest, L., “The Audion; A New Receiver for Wireless Telegraphy,” Trans. AIEE, vol. XXV,
Jan. 1906, pp.735–763.
[11] De Forest, L., “The Audion-Detector and Amplifier,” Proc. IRE, vol. 2, no. 1, Mar. 1914, pp. 15–29.
[12] Brittain, J. E., “Electrical Engineering Hall of Fame: Lee de Forest,” Proc. IEEE, vol. 93, no. 1, Jan. 2005, pp. 198–202.
[13] Hong, S., Wireless: From Marconi’s Black-Box to the Audion, Cambridge, MA: MIT Press, 2001.
[14] Godfrey, D. G., and F. A. Leigh, eds., Historical Dictionary of American Radio, Westport, CT: Greenwood Publishing
Group, 1998.
[15] Pound, R. V., Microwave Mixers, vol. 16 of MIT Radiation Lab. Series, New York: McGraw-Hill, 1948. Reprinted:
Norwood, MA: Artech House (CD-ROM edition), 1999.
[16] Skolnik, M. I., Introduction to Radar Systems, 3rd ed., New York: McGraw-Hill, 2001.
[17] Hall, J. S., ed., Radar Aids to Navigation, vol. 2 of MIT Radiation Lab. Series, New York: McGraw-Hill, 1947.
Reprinted: Norwood, MA: Artech House (CD-ROM edition), 1999.
[18] Van Voorhis, S. N., ed., Microwave Receivers, vol. 23 of MIT Radiation Lab. Series, New York: McGraw-Hill, 1948.
Reprinted: Norwood, MA: Artech House (CD-ROM edition), 1999.
[19] Skolnik, M. I., Introduction to Radar Systems, 3rd ed., New York: McGraw-Hill, 2001.
[20] Manheimer, W. M., and G. W. Ewell, “Cyclotron Wave Electrostatic and Parametric Amplifiers,” Naval Research
Laboratory, Washington D.C., Rep. No. MR/6707-97-7910, Feb. 28, 1997. Available from DTIC as ADA322103.

[21] Manheimer, W. M., and G. W. Ewell, “Electrostatic and Parametric Cyclotron Wave Amplifiers,” IEEE Trans. Plasma
Science, vol. 26, no. 4, Aug. 1998, pp. 1282–1296.
[22] Vanke, V. A., H. Matsumoto, and N. Shinohara, “Cyclotron Wave Electrostatic Amplifier,” J. Radioelectronics, no. 10,
1999.
[23] Budzinsky, Y. A., and S. P. Kantyuk, “A New Class of Self-Protecting Low-Noise Microwave Amplifiers,” 1993 IEEE
Int. Microwave Symp. Dig., vol. 2, Atlanta, GA, Jun. 14–18, 1993, pp. 1123–1125.
[24] Budzinskiy, Y.A., et al., “On the Increase of the Frequency Band of Cyclotron Protective Device,” 2005 15th Int.
Crimean Conf. Microwave & Telecommun. Technol. (CriMiCo), vol.1, Oct. 12–16, 2005, pp. 205, 206.
[25] Budzinskiy, Y.A., and S. V. Bykovskiy, “Cyclotron Protective Device with Increased Frequency Band,” IEEE Int.
Vacuum Electron. Conf. (IVEC ’09), Apr. 28–30, 2009, pp.60, 61.
[26] Budzinskiy, Y.A., et al., “Electron Optical System of Cyclotron Protective Device,” 2014 24th Int. Crimean Conf.
Microwave & Telecommun. Technol. (CriMiCo), Sept. 7–13, 2014, pp. 157, 158.
[27] Budzinskiy, Y.A., S. V. Bykovskiy, and V. G. Kalina, “Engineering Calculation of Cyclotron Protective Devices,” 2014
24th Int. Crimean Conf. Microwave & Telecommun. Technol. (CriMiCo), Sept. 7–13, 2014, pp.159, 160.
[28] Maas, S. A., Microwave Mixers, 2nd ed., Norwood, MA: Artech House, 1993.
[29] IEEE Standard Dictionary of Electrical and Electronic Terms, 6th ed., New York: IEEE, 1996.
[30] Poisel, R. A., Electronic Warfare Receivers and Receiving Systems, Norwood, MA: Artech House, 2014.
[31] Carson, R.S., Radio Communications Concepts: Analog, New York: Wiley & Sons, 1990.
[32] Faria, D., L. Dunleavy, and T. Svensen, “The Use of Intermodulation Tables (IMT) for Mixer Simulation,” Apr. 2008.
www.agilent.com.
[33] Butterworth, S., “On the Theory of Filter Amplifiers,” Experimental Wireless & the Wireless Engineer, vol. 7, Oct.
1930, pp. 536–541.
[34] Waters, W. M., and B. R. Jarrett, “Bandpass Signal Sampling and Coherent Detection,” IEEE Trans. Aerosp. Electron.
Syst., vol. 18, no. 6, Nov. 1982, pp. 97–109.
[35] Johnson, J. B., “Thermal Agitation of Electricity in Conductors,” Phys. Rev., vol. 32, Jul. 1928, pp. 97–109.
[36] Blake, L. V., “A Guide to Basic Pulse-Radar Maximum-Range Calculation,” NRL Report 6930, Naval Research Report
Laboratory, 1969.
[37] Goldberg, H., “Some Notes on Noise Figures,” Proc. IRE, vol. 36, no. 10, Oct. 1948, pp. 1205–1214.
[38] Tsui, J., Digital Techniques for Wideband Receivers, 2nd ed., Norwood, MA: Artech House, 2001.
[39] Pozar, D. M., Microwave Engineering, New York: Addison-Wesley, 1990.
[40] Rezavi, B., RF Microelectronics, Upper Saddle River, NJ: Prentice-Hall, 1998.
[41] Egan, W. F., Practical RF System Design, New York: Wiley & Sons, 2003.
[42] Pozar, D. M., Microwave and RF Design of Wireless Systems, New York: Wiley & Sons, 2001.
[43] Friis, H.T., “Noise Figures of Radio Receivers,” Proc. IRE, vol. 32, no. 7, Jul. 1944, pp. 419–422.
[44] Ludwig, R., and G. Bogdanov, RF Circuit Design: Theory and Applications, 2nd ed., Upper Saddle River, NJ: Prentice-
Hall, 2008.
[45] Lee, T.H., The Design of CMOS Radio-Frequency Integrated Circuits, 2nd ed., Cambridge Univ. Press, 2003.
[46] Skolnik, M. I., ed., Radar Handbook, 2nd ed., New York: McGraw-Hill, 1990.
[47] Barton, D. K., Radar System Analysis and Modeling, Norwood, MA: Artech House, 2005.
[48] Naval Air Warfare Center Weapons Division (NAWCWD), Electronic Warfare and Radar Systems Engineering
Handbook, 4th ed., NAWCWD Tech. Commun. Office, Point Mugu, CA, Rep. No. NAWCWD TP 8347, Oct. 2013.
[49] Rohde, U. L., J. Whitaker, and T.T.N. Bucher, Communications Receivers, 2nd ed., New York: McGraw-Hill, 1997.
[50] Erst, S. J., Receiving Systems Design, Dedham, MA: Artech House, 1984.
[51] Tsui, J. B., Microwave Receivers and Related Components, Los Altos Hills, CA: Peninsula Publishing, 1985.
[52] Tsui, J., Digital Microwave Receivers: Theory and Concepts, Norwood, MA: Artech House, 1989.
[53] Tsui, J., Special Design Topics in Digital Wideband Receivers, Norwood, MA: Artech House, 2010.

[54] Pace, P. E., Advanced Techniques for Digital Receivers, Norwood, MA: Artech House, 2000.
[55] IEEE 100, The Authoritative Dictionary of IEEE Standards Terms, 7th ed., New York: IEEE, 2000.
[56] Carr, J. J., Practical Radio Frequency Test and Measurement: A Technician’s Handbook, Boston: Newnes, 1999.
[57] Poberezhskiy, Y. S., “On Dynamic Range of Digital Receivers,” 2007 IEEE Aerosp. Conf., Big Sky, MT, Mar. 3–7,
2007, pp. 1–17.
[58] Dixon, R.C., Radio Receiver Design, New York: Marcel Dekker, 1998.
[59] Reed, J. H., Software Radio: A Modern Approach to Radar Engineering, Upper Saddle River, NJ: Prentice-Hall, 2002.
[60] Marki, F., and C. Marki, “Mixer Basics Primer,” 2010. www.markimicrowave.com.
[61] Vizmuller, P., RF Design Guide: Systems, Circuits and Equations, Norwood, MA: Artech House, 1995.
[62] Norton, D. E., “The Cascading of High Dynamic Range Amplifiers,” Microwave J., vol. 16, no. 6, Jun. 1973, pp. 57–71.
[63] Barton, D. K., Radar Equations for Modern Radar, Norwood, MA: Artech House, 2013.
[64] East, P.W., Microwave System Design Tools and EW Applications, 2nd ed., Norwood, MA: Artech House, 2008.
[65] Kenington, P. B., High Linearity RF Amplifier Design, Norwood, MA: Artech House, 2000.
[66] Jardon-Aguilar, H., R. Acevo-Herrera, and G. Monserrat Galvan Tejada, “Intermodulation Interception Points of
Nonlinear Circuits Connected in Cascade,” 1st Int. Conf. Electrical and Electron. Eng., 2004 (ICEEE), Acapulco,
Mexico, Sept. 8–10, 2004, pp. 11–16.
[67] Kester, W., ed., Analog-Digital Conversion, Analog Devices, 2004.
[68] Nyquist, H., “Certain Factors Affecting Telegraph Speed,” Bell Syst. Tech. J., vol. 3, no. 2, Apr. 1924, pp. 324–346.
[69] Nyquist, H., “Certain Topics in Telegraph Transmission Theory,” Trans. AIEE, vol. 47, no. 2, Apr. 1928, pp. 617–644.
Reprinted as a classic paper in Proc. IEEE, vol. 90, no. 2, Feb. 2002.
[70] Kester, W., ed., The Data Conversion Handbook, New York: Newnes, 2005.
[71] Shannon, C. E., “A Mathematical Theory of Communication,” Bell Syst. Tech. J.,vol. 27, no. 3, Jul. 1948, pp. 379–423.
[72] Shannon, C. E., “Communication in the Presence of Noise,” Proc. IRE, vol. 37, no. 1, Jan. 1949, pp. 10–21. Reprinted as
a classic paper in Proc. IEEE, vol. 86, no. 2, Feb. 1998.
[73] Kotel’nikov, V. A., “On the Capacity of the ‘Ether’ and Cables in Electrical Communications,” Proc. 1st All-Union Conf.
Technolog. Reconstruction of the Commun. Sector and Develop. of Low-Current Eng., Moscow: 1933. Translated by
C. C. Bissell and V. E. Katsnelson. http://ict.open.ac.uk/classics/1.pdf.
[74] Vaughan, R. G., N. L. Scott, and D. R. White, “The Theory of Bandpass Sampling,” IEEE Trans. Signal Processing, vol.
39, no. 9, Sept. 1991, pp. 1973–1984.
[75] Brigham, E. O., The Fast Fourier Transform and Its Applications, Englewood Cliffs, NJ: Prentice-Hall, 1988.
[76] Hill, G., “The Benefits of Undersampling,” Electron. Des., Jul. 1994, pp. 69–79.
[77] Coulson, A. J., R. G. Vaughan, and M.A. Poletti, “Frequency Shifting Using Bandpass Sampling,” IEEE Trans. Signal
Proc., vol. 42, no. 6, Jun. 1994, pp. 1556–1559.
[78] Akos, D. M., et al., “Direct Bandpass Sampling of Multiple Distinct RF Signals,” IEEE Trans. Comm., vol. 47, no. 7,
1999, pp. 983–988.
[79] Rabiner, L. R., and B. Gold, Theory and Application of Digital Signal Processing, Englewood Cliffs, NJ: Prentice-Hall,
1975.
[80] Widrow, B., and I. Kollar, Quantization Noise: Round Off Error in Digital Computation, Signal Processing, Control and
Communications, New York: Cambridge University Press, 2008.
[81] Bennett, W. R., “Spectra of Quantized Signals,” Bell System Tech. J., vol. 27, Jul. 1948, pp. 446–472.
[82] Kester, W., “MT-001 Tutorial: Taking the Mystery out of the Infamous Formula, ‘SNR = 6.02N + 1.76dB,’ and Why
You Should Care,” 2009. www.analog.com/static/imported-files/tutorials/MT-001.pdf.
[83] Williston, K., Digital Signal Processing: World Class Designs, New York: Newness, 2009.
[84] Orfandis, S. J., Introduction to Digital Signal Processing, Englewood Cliffs, NJ: Prentice-Hall, 1996.
[85] Wu, Y., and J. Li. “The Design of Digital Radar Receiver,” IEEE Nat. Radar Conf., Syracuse, NY, May 13–15,1997, pp.
207–210.

[86] Oppenheim, A. V., ed., Applications of Digital Signal Processing, Englewood Cliffs, NJ: Prentice-Hall, 1978.
[87] Kester, W., “MT-004 Tutorial: The Good, the Bad, and the Ugly Aspects of ADC Input Noise— Is No Noise Good
Noise?,” 2009. www.analog.com/static/imported-files/tutorials/MT-004.pdf.
[88] Furman, G. G., “Removing the Noise from the Quantization Process by Dithering: Linearization,” RAND Corp., Santa
Monica, CA, Res. Memo RM-3271-PR, Feb. 1, 1963. Available from DTIC as AD 296598.
[89] Furman, G. G., “Improving the Quantization of Random Signals by Dithering,” RAND Corp., Santa Monica, CA, Memo
RM-3504-PR, May 1, 1963. Available from DTIC as AD 405473.
[90] Gray, R. M., and T. G. Stockham, “Dithered Quantizers,” IEEE Trans Inf. Theory, vol. 39, no. 3, May 1993, pp. 805–
812.
[91] Kester, W., “MT-006 Tutorial: ADC Noise Figure—An Often Misunderstood and Misinterpreted Specification,” 2009.
www.analog.com/static/imported-files/tutorials/MT-006.pdf.
[92] Karki, J., “Calculating Noise Figure and Third-Order Intercept in ADCs,” Texas Instruments Incorporated, 2003.
http://www.ti.com/lit/an/slyt090/slyt090.pdf.
[93] Li, Z., et al., “Trade-off Between Sensitivity and Dynamic Range in Designing Digital Radar Receivers,” Int. Conf.
Microwave and Millimeter Wave Technology (ICMMT), Nanjing, China, Apr. 21–24, 2008, pp. 1368–71.
[94] Li, Z., et al., “Design Considerations of the RF Front-end for High Dynamic Range Digital RADAR Receivers,” 17th Int.
Conf. Microwaves, Radar, and Wireless Commun. (MIKON), Wroclaw, Poland, May 19–21, 2008, pp. 1–4.
[95] Lyons, R. G., Understanding Digital Signal Processing, 3rd ed., New York: Prentice-Hall, 2011.
[96] Budge, M. C., Jr., and S. R. German, “The Effects of an ADC on SCR Improvement,” IEEE Trans. Aerosp. Electron.
Syst., vol. 49, no. 4, Oct. 2013, pp. 2463–2469.
[97] Vanderkooy, J., and S. P. Lipshitz, “Resolution Below the Least Significant Bit in Digital Systems with Dither,” J. Audio
Eng. Soc., vol. 32, no. 3, Mar. 1984.
[98] Oppenheim, A. V., ed., Applications of Digital Signal Processing, Englewood Cliffs, NJ: Prentice-Hall, 1978.
[99] Wu, Y., “The Application of All-Digital Array Receiver to OTH Radar,” 2006 CIE Int. Conf. Radar, Shanghai, China,
Oct. 16–19, 2006, pp. 1–4.
100] Elliott, R. S., Antenna Theory and Design, Revised Edition, New York: Wiley & Sons,
2003.
101] Barton, D. K., Modern Radar System Analysis, Norwood, MA, Artech House, 1988.
102] Barton, D. K., ed., Radars, Vol. 1: Monopulse Radar (Artech Radar Library), Dedham,
MA: Artech House, 1974.
103] Hannan, P. W., “Optimum Feeds for All Three Modes of a Monopulse Antenna,” IRE
Trans. Antennas Propag., vol. 9, no. 5, Sept. 1961, pp. 444, 461. Reprinted in Radars, Vol.
1, Monopulse Radar, (D.K. Barton, ed.), Artech Radar Library, Dedham, MA: Artech
House, 1974.
104] Ricardi, L. and L. Niro, “Design of a Twelve-Horn Monopulse Feed,” 1958 IRE Int.
Conv. Rec., vol. 9, Mar. 21–25, 1966, pp. 49, 56.
105] Sherman, S. M., and D. K. Barton, Monopulse Principles and Techniques, 2nd ed.,
Norwood, MA, Artech House, 2011.
106] Budge, M. C., Jr., “EE 710: Radar Tracking—Fall 2014, Class Notes, UAH”
http://www.ece.uah.edu/courses/material/EE710-Merv-Fall2014.
107] Wirth, W. D., Radar Techniques Using Array Antennas, London, UK: IEE Press, 2001.
108] Brookner, E., “Phased-Array and Radar Astounding Breakthroughs—An Update,” IEEE
2008 Radar Conf. (RADAR ’08), May 26–30, 2008, pp.1, 6.
109] Schmidt, R.O., “Multiple Emitter Location and Signal Parameter Estimation,” IEEE
Trans. Antennas Propag., vol. 34, no. 3, Mar. 1986, pp. 276, 280.

1 Sometimes shortened to superhet receiver.
2 Walter Hermann Schottky, working in Germany, conceived of the superheterodyne receiver independently [6].
3 When stating filter order, we are using the lowpass equivalent. The passband order is double.
4 Some radars use a default track gate location, such as the middle of the PRI, when not in track, to drive the AGC circuitry. In
this case, a noise AGC (NAGC) is usually run in parallel and combined with the signal AGC. As a result, the noise level or
signal level requiring the larger attenuation dominates control of the overall AGC.
5 Development of devices based on electron beam cyclotron waves has been carried out largely in Russia by ISTOK and the
Moscow State University. Early experiments were performed in the United States, but work was dropped in favor of solid state
amplifiers [20–22].
6 Former Soviet Union, Joint Stock Company, State Research & Production Corporation ISTOK, named after A. I. Shokin
(www.istokmw.ru). ISTOK is the oldest Russian microwave organization.
7 The term high side mixer or high side injection is used when the fLO > fRF. The term low side mixer or low side injection is
used when the fLO < fRF [31].
8 Strictly speaking, we do not need the absolute value since the cosine is an even function.
9 Double-balanced mixers can be match sensitive.
10 Predicting spur levels can be fairly complicated. Mixer spur levels are usually measured directly, or more often,
manufacturer-supplied mixer spur tables provide the spurs levels for a particular mixer [32].
11 Named for British physicist Stephen Butterworth [33].
12 The examples in this chapter presume matched filtering post ADC. Some radar systems implement the matched filter at IF
and then I/Q detect for signal processing.
13 When phase coherency is required, the moniker Stable Local Oscillator (STALO) is sometimes used to describe the first LO.
14 Terms such as I and Q detector, synchronous detector, quadrature detector, and coherent demodulator are used
synonymously with quadrature demodulator.
15 Based upon Chebyshev polynomials, which are named for Russian mathematician Pafnuty Lvovich Chebyshev (Πaфнýтий
Львόвич ЧебьIшëв). Also transliterated as Chebychev, Chebysheff, Tchebychev.
16 For Butterworth and Bessel filters above 2nd order, the difference between 3-dB bandwidth and equivalent noise bandwidth
is negligible.
17 Technically a signal needs to be about 5 to 8 dB below the noise floor for it not to make discernable “bump” in an
averaged detected output.
18 The SNR is often determined at the output of the signal processor instead.
19 In the third edition of Skolnik’s handbook, SNR = 0 is no longer stipulated with the term minimum signal of interest. Skolnik
points out that digital signal processing techniques allow detection well below the receiver noise floor, depending upon the
processing performed [46].
20 This convention is usually within 0.5 to 1 dB of a passive mixer’s actual noise figure.
21 Skolnik observes there is no unique definition for digital receiver [17, p. 742]. Yuanbin Wu and Jinwen Li suggest the use of
direct IF sampling and direct digital synthesis (DDS) to generate the LO earns the moniker [60].
22 Direct IF sampling requires a higher sampling rate compared to baseband sampling the output of an I/Q demodulator.
23 The term used for lower frequency radars where the RF is sampled is direct RF sampling.
24 The number of bits in an ADC is also referred to as the resolution of the ADC.
25 Unipolar converters can be used to convert bipolar signals by using a proper input driver to convert bipolar signals into
unipolar signals.
26 We have made the tacit assumption that inputs are confined to the linear range of the ADC. We will not consider input
overload.
27 Note that an impedance of R0 = 1 Ω is implied.
28 We are calling this a signal to noise ratio although, strictly speaking it is not. More accurately, it is a signal-to-quantization
error ratio.
29 Noise generated internally by ADCs is usually characterized as input referenced noise and expressed in terms of LSBs rms,
corresponding to an rms voltage referenced to the ADC full-scale input range [70, 87].
30 Sampling in excess of Nyquist can be used to take advantage of the processing gain resulting from a fixed amount of
quantization noise being spread over a larger bandwidth is referred to, as might be expected, as oversampling.
31 Integration time (and its attendant bandwidth) predicates the level to which very small signals may be recovered.
32 This type of dither is incorporated into some ADCs and controlled by an enable bit.

33 The form of (14.91) assumes digital signal processing. In analog processing, this ratio is sometimes formed differently, and
in a fashion where channel balance can affect the output of the range discriminator [106].

Chapter 15
Introduction to Synthetic Aperture Radar Signal Processing
15.1 INTRODUCTION
The term synthetic aperture radar (SAR) derives from the fact that the motion of an aircraft
(airplane, satellite, or UAV, for example) is used to artificially create, or synthesize, a very
long, linear array, or aperture. The reason for creating a long array is to provide the ability to
resolve targets that are closely spaced in angle, or cross range (usually azimuth). This, in
turn, is driven by one of the main uses of SAR: to image the ground or targets. In both cases,
the radar needs to be able to resolve very closely spaced scatterers. Specifically, resolutions
in the order of less than a meter to a few meters are needed. To realize such resolutions in the
range coordinate, the radar uses wide bandwidth waveforms. To realize such resolutions in
cross range, very long antennas are required.
To get an idea of what we mean by “long” antenna, we consider an example. Suppose we
are trying to image a ground patch at a range of 20 km. To do so, we want a cross-range
resolution of 1 m. We can approximately relate cross-range distance, δy, to antenna
beamwidth, θB, and range, R, by
as shown in Figure 15.1. For δy = 1 m and R = 20 km, we get θB = 5 × 10−5 rad or about
0.003°!
Figure 15.1 Relation of cross-range distance to beamwidth.
The beamwidth of a linear array with uniform illumination can be approximately related to
antenna length by [1, 2]
If we assume the radar of the above example operates at X-band and λ = 0.03 m, we get

Clearly, it would not be practical to use a real antenna that is as long as six football fields.
Instead, a SAR synthesizes such antenna by using aircraft motion and signal processing. An
interesting property illustrated by (15.1) and (15.3) is that the resolution and SAR antenna
length depends upon wavelength. This means that if a certain resolution is desired, and there
are limits on how long the synthetic array can be made, we are driven to shorter wavelength
or higher frequency radars. This will also affect down-range resolution since it is related to
waveform bandwidth, and large waveform bandwidths are easier to obtain at higher operating
frequencies.
15.2 BACKGROUND
According to a paper by H. D. Griffiths [3], the concept of aperture synthesis was introduced
by Ryle and Hawkins in the 1940s or 1950s in relation to their work in radio astronomy.
However, the recognized father of SAR, as it is known today, is Carl A. Wiley, who conceived
of the concept in 1951 and termed it Doppler beam sharpening [4–6]. Shortly after that, in
1952, scientists at the University of Illinois experimentally demonstrated the concept [2]. Since
that time, SAR has found wide use in both commercial and military applications [7–12].
15.2.1 Linear Array Theory
Before we discuss SAR processing, we consider some properties of SAR. We start with a
review of linear arrays since a SAR synthesizes a linear array. Suppose we have a 2N + 1
element linear array1 as shown in Figure 15.2. We have a target, located at some xi,yi, that
emits an E-field Eoej2πfot that eventually reaches each antenna element. We can write the E-field
at the nth element as
where we included the function of ri,n to indicate that the magnitude of the E-field intensity at
the nth element depends on the range from the target to that element. In general, Eo(ri,n) will,
for all practical purposes, be the same at each element.
The resulting voltage at the output of the nth element is
In general, the Vo(ri,n) is a function of the range from the scatterer to the nth element.
However, since ro is assumed large relative to the array dimensions (even for SAR arrays),
we can assume Vo(ri,n) is the same at all elements and replace Vo(ri,n) with Vo. Vo is the
magnitude of the voltage out of each element.

We can write ri,n as
where we used yi = r0 sin θ (see Figure 15.2).
Figure 15.2 2N + 1 element linear array.
Consistent with the linear array theory of Chapter 12, we claim r0 ≫ nd and approximate
ri,n as2
We next substitute (15.7) into the exponent of vn(t) (15.5) to get

To form the total output of the array, we sum the vn(t) to get
We next form a scaled antenna radiation pattern (see Chapter 12) as3
where PS is the normalized power returned from the target.
Figure 15.3 Normalized radiation pattern vs. target angle.
When we formulated the antenna radiation pattern as above, we were interested in how R(θ)
varied with target angle, θ. As given in (15.10), the peak of R(θ) occurs at a target angle of θ =
0 as shown in Figure 15.3, which is a plot of R(θ) for PS = 1 W.
As an extension to the above, we steer the beam to an angle of θS by including a linear
phase shift across the array elements as shown in Figure 15.4. In Chapter 12, we found we can
do this by multiplying the vn(t) by an and letting an = exp(–j2π(d/λ) sin θS). With this we get
which leads to a more general R(θ) of

In standard array theory, we are interested in how R(θ) varies with θ for a fixed θS. In this case
the peak of R(θ) would occur at θ = θS, as shown in the example of Figure 15.5.
Figure 15.4 Linear array with phase shifters.
Figure 15.5 Normalized radiation pattern vs. target angle—beam steered to 0.01°, Ps = 1 W.

15.2.2 Transition to SAR Theory
In SAR theory, we need to reorient ourselves by thinking of the target angle, θ, as being fixed
and examining how R(θ) varies with θS. In other words, we consider a fixed, θ, and plot R(θS).
An example plot of R(θS) for θ = 0.01° (i.e., the target location is fixed at 0.01°) is shown in
Figure 15.6. In this plot, R(θS) peaks when the beam is steered to an angle of 0.01°.
Figure 15.7 contains a plot of R(θS) for the case where there is a target at 0.01° and a second
target at −0.02°. Further, the second target has twice the RCS (radar cross section), and thus
twice the power, of the first target. Here we note that the plot of R(θS) tells us the location of
the two targets and their relative powers. This is the type of information we want when we
form SAR images.
Figure 15.6 Normalized radiation pattern vs. beam steering angle—target located at 0.01°, Ps = 1 W.
Figure 15.7 Normalized radiation pattern vs. beam steering angle—two targets located at −0.02 and 0.01°.
R(θS) gives us information in one dimension. To form an image, the other information we
use is P(r), the power out of the matched filter for a target range of r. We compute R(θS) and
P(r) for various values of θS and r and then plot |R(θS)P(r)|1/2 as intensities on a rectangular

grid. The discrete values of θS and r will be separated by the angle resolution of the SAR
array and the range resolution of the waveform. The resulting image is a SAR image.
15.3 DEVELOPMENT OF SAR-SPECIFIC EQUATIONS
With the above background, we now address issues associated with forming R(θS) in practical
SAR situations. We begin by modifying the above array theory so that it more directly applies
to the SAR problem.
In standard array theory, we generate a one-way antenna pattern because we consider an
antenna radiating toward a target (the transmit antenna case) or a target radiating toward an
array (the receive antenna case). In SAR theory, we consider a two-way problem since we
transmit and receive from each element of the synthetic array. If we refer to Figure 15.2, we
can think of each element as the position of the SAR aircraft as it transmits and receives
successive pulses. When the aircraft is located at y = nd, the normalized transmit “voltage” is4
The resultant received signal (voltage) from a scatterer 5 at xi,yi is
where PSi is the return signal power and is determined from the radar range equation. ri,n is
the range to the ith scatterer when the aircraft is at y = nd.
We note that the difference between (15.5) and (15.14) is that the latter has twice the phase
shift as the former.
Modifying (15.7) as
and repeating the math of Section 15.2.1, we get the equation for the scaled radiation pattern
of a SAR antenna as
Figure 15.8 contains plots of R(θS) for the standard linear array (15.12) and the SAR array

(15.16). In both cases, we used PSi = 1 W. The notable difference between the two plots is that
the width of the main beam of the SAR array is half that of the standard linear array. This
leads to one of the standard statements in SAR books that a SAR has twice the resolution
capability of a standard linear array [13]. In fact, this is not quite true. If we were to consider
the two-way antenna pattern of a standard linear array, we would find that its beamwidth lies
between the one-way beamwidth of a standard linear array and the beamwidth of a SAR array.
The reason that the two-way beamwidth of a standard linear array is not equal to the
beamwidth of a SAR array has to do with the interaction between “elements” in the two arrays.
In a standard linear array, each receive element receives returns from all of the elements of
the transmit array. However, in the SAR array, each receive “element” receives returns only
from itself.
Adapting (15.2), we have, for the SAR array,
If we combine this with the equation for cross-range distance (15.1), we get, again for the
SAR array,
Figure 15.8 Normalized radiation patterns for a standard linear array (top plot) and a SAR array (bottom plot).
which is termed the cross-range resolution of the SAR. This equation indicates that the cross-
range resolution of a SAR can be made arbitrarily small (fine) by increasing the length of the
SAR array. In theory, this is true for a spotlight SAR [14, 15]. In the case of strip map SAR, the

size of the actual antenna on the SAR aircraft (the “element” of the SAR array) is the
theoretical limiting factor on resolution. In either case, there are several other factors related
to phase coherency that place further limits on the cross-range resolution.
15.4 TYPES OF SAR
Figures 15.9 and 15.10 contain illustrations of the geometry associated with strip map and
spotlight SAR, respectively. With strip map SAR, the actual antenna remains pointed at the
same angle, while the aircraft flies past the area being imaged. This angle is shown as 90° in
Figure 15.9 but can be almost any angle. For spotlight SAR, the actual antenna is steered to
constantly point towards the area being imaged. The term “strip map” derives from the fact
that this type of SAR can continually map strips of the ground as the aircraft flies by. The term
“spotlight” derives from the fact that the actual antenna constantly illuminates, or spotlights,
the region being imaged. A spotlight SAR must map a strip of ground in segments.
Figure 15.9 Strip map SAR geometry.
As might be deduced from Figure 15.9, a limitation of the strip map SAR geometry is that
the region imaged during any one processing interval must remain in the actual antenna beam
during that processing interval. This does not limit the total area imaged since a strip map
SAR can perform imaging continuously, dropping off data as it picks up new data. It only
limits the size of the area in any one processing interval.
For the case of spotlight SAR, the antenna is always pointed at the region being imaged so
that the length of the synthetic array can, in theory, be as large as desired. In practice, the
length of the synthetic array for the spotlight SAR is limited by other factors such as

coherency and signal processing limitations. Since the cross-range resolution of a SAR is
related to the length of the synthetic array, spotlight SARs can usually attain finer cross-range
resolution than strip map SARs.
Figure 15.10 Spotlight SAR geometry.
15.4.1 Theoretical Limits for Strip Map SAR
The theoretical limit on cross-range resolution for a strip map SAR can be deduced with the
help of Figure 15.11. As illustrated in this figure and discussed previously, the point to be
imaged must be in the actual antenna beam over the processing interval [the coherent
processing interval (CPI)]. The cross-range span of the main beam of the actual antenna is
where ri is the perpendicular range from the aircraft flight path to the point being imaged. In
the geometry of Figure 15.11, note that the point being imaged will remain in the actual
antenna beam as the aircraft traverses a distance of L. Thus the length of the synthetic array
applicable to any CPI is L.
Using (15.2), we can write the beamwidth of the actual antenna as
where LANT is the horizontal width of the actual antenna. If we substitute (15.20) into (15.19),
we get
which we can combine with (15.18) to get

Figure 15.11 Resolution limit for strip map SAR.
Thus, the finest cross-range resolution a strip map SAR can achieve is half of the horizontal
width of the actual antenna. This cross-range resolution applies only to the case where a point
is being imaged. The resolution for a finite-sized area will be slightly worse, as shown in the
next subsection.
15.4.2 Effects of Imaged Area Width on Strip Map SAR Resolution
Figure 15.12 illustrates a case where the width of the region to be imaged is w. It can be
observed from this figure that L′ = L – w. From this we conclude that the modified cross-
range resolution is
In practice, the term w/L will be small so that δy′ ≈ δy. As an example of this, we consider the
earlier example where the SAR processed returns over L = 600 m. From (15.18), the resulting
resolution for a point target is, in theory, δy = 0.5 m. Suppose we wanted to image an area
with a width of 50 m. For this case, we would need to shorten the distance over which we
process returns to L′ = L – w = 550 m. As a result, from (15.23), the resolution would be 0.546
m instead of 0.5 m.
These discussions of the relation between distance over which we process returns (the
length of the synthetic array during the CPI) and resolution are based on the assumption that
the antenna directivity is constant over θS and zero elsewhere. This will clearly not be the case
in an actual SAR. Therefore, the relation between resolution and LANT should be considered
as an approximate limitation, rather than a hard constraint.

Figure 15.12 Effect of finite area width on strip map SAR resolution.
15.5 SAR SIGNAL CHARACTERIZATION
To formulate a SAR processor, we need to characterize the signal that the SAR processor will
operate upon. Although our previous discussions treated SAR crossrange imaging as an
antenna problem, for the rest of the development we will cast the problem in the Doppler
domain. We make this change because experience indicates that the Doppler formulation is
easier to understand than the antenna formulation. Also, the Doppler formulation is consistent
with other texts and articles that discuss SAR processing [16–24].
15.5.1 Derivation of the SAR Signal
As we have done thus far, we will initially consider only the cross-range problem. We will
later extend the discussions to the down-range and cross-range problem. Since we are
considering the cross-range problem, we start by considering a normalized CW transmit
signal of the form
From the geometry of Figure 15.13, the appropriately normalized signal returned from the
“ith” scatterer located at xi,yi is
where
and d(t) is the y position of the aircraft at some time t. For now, we are assuming that the

aircraft (platform) is at an altitude of zero. The extension to nonzero altitude is
straightforward.
If we assume the aircraft is flying at a constant velocity of V and t = 0 occurs at y = 0 of
Figure 15.13, we get
We assume the total time for the aircraft to travel a distance of L is TL and that the aircraft
starts at –L/2 when t = –TL/2. With this we get
Figure 15.13 Geometry used to develop signal representation—cross-range imaging.
The area to be imaged has a cross-range width of w and a down-range length of l. The region
is centered in cross range at y = 0 and in down range at x = r0.
In (15.25), PSi is the normalized signal power associated with the ith scatterer. It is related to
scatterer RCS through the radar range equation. Thus, PSi characterizes the relative sizes of
the scatterers in the imaged area. 
is analogous to brightness or contrast in a
photographic image.
We can rewrite (15.25) as

Since the information needed to form the image is in the second exponential term, we
eliminate the first exponential term by heterodyning (which is done in the actual radar) to
yield the baseband signal
If we have Ns scatterers in the image region, the resulting composite baseband signal would
be
15.5.2 Examination of the Phase of the SAR Signal
Since the information we seek is in the phase of vi(t), we examine it. To proceed, we examine
ri(t), which we can write as
We note that ri, ≈ r0, yi ≪ r0 and Vt ≪ r0 ∀ Vt ∈ [−L/2, L/2]. This means the second and third
terms of the last square root are small relative to 1. This, in turn, allows us to write
Substituting this into (15.30) yields
15.5.2.1 Linear Phase, or Constant Frequency, Term
The first exponential is a phase caused by range delay to the scatterer. For a single scatterer,
the exponential is of no concern because it will disappear when we form the magnitude of the
processed version of vi(t). For multiple scatterers, however, the exponential can cause
constructive and destructive interference, which leads to speckle in SAR images [13, 18].
Speckle is usually mitigated by image processing techniques [13, 25].
The second term of (15.34) is a linear phase term or a term that we associate with

frequency, which is
This tells us vi(t) has a frequency component that depends upon scatterer cross-range position,
yi. fyi also depends upon the aircraft velocity, V, and the radar wavelength, λ. However, both of
these are known (and fixed). Finally, fyi also depends upon ri. If we assume all of the scatterers
are at the same xi = r0 (which we can do here because we are concerned only with the cross-
range problem), and we note that yi ≪ r0, we get the previous assertion that
From this discussion, we conclude that we can determine yi if we can measure ƒyi.
Specifically,
15.5.2.2 Quadratic Phase, or LFM, Term
The third exponential of (15.34) is a quadratic phase, or linear frequency modulation, term
that causes problems. We can write the quadratic phase as
With the previous assumption of ri ≈ r0, ϕQ(t) is approximately the same for all scatterers.
This means we can remove it by a mixing or heterodyning process.6 If we do this, we will be
left with only the magnitude, constant phase term, and the yi-dependent frequency term. This is
what we want.
15.5.3 Extracting the Cross-Range Information
Once we remove the quadratic phase, we have
for a single scatterer. For the more general case of Ns scatterers, we have

The forms of (15.39) and (15.40) tell us we can extract the information we want by taking the
Fourier transform of vIi(t) or, more generally, vI(t). From our experience with Fourier
transforms, this will give us a response that has peaks at the frequencies ƒyi. The heights of the
peaks will be proportional to (PSi)1/2. If we use (15.37) to plot this as amplitude [(PSi)1/2]
versus yi, we have a one-dimensional image.7
We compute the Fourier transform of vIi(t) using
We recognize that vi(t), and thus vIi(t), is measured only over t∈[–TL/2, TL/2]. Thus, we
assume vIi(t) is zero outside of these limits and write
where, as a reminder,
Figure 15.14 contains a plot |VIi(ƒ)| versus (ƒ – ƒyi)TL. Note that the response has a peak at ƒ –
ƒyi = 0, or at ƒ = ƒyi, and that the peak has a height of|VIi(ƒ)| = (PSi)1/2. The width of the peak is
1/TL, which means that the SAR image will have a resolution of

Figure 15.14 Plot of |VIi|(PSi)1/2 vs. (ƒ – ƒyi)TL.
Figure 15.15 Plot of VIi|(PSi)1/2 vs. (y – yi)/δy.
If we change the horizontal axis to y using the relation [see (15.37)]
we get the plot of |VIi(y)| versus (y – yi) of Figure 15.15. This plot has a peak at y = yi with a
height of (PSi)1/2.
From (15.44), if the resolution of Figure 15.14 is δƒ = 1/TL, the resolution of Figure 15.15
is

since L = TLV. This is the same as the resolution we obtained from the linear array approach
[see (15.18)].
15.6 PRACTICAL IMPLEMENTATION
In the previous section, we established that the processing methodology we must use to form
an image is to form a Fourier transform of vI(t) [or vIi(t)]. However, this approach makes the
tacit assumption that vI(t) is a continuous function of time. Thinking ahead to when we will
consider both cross-range and down-range imaging, we realize the SAR will transmit a
pulsed signal rather than a CW signal. Because of this, we recognize that vI(t) will not be a
continuous-time signal but a discrete-time signal with samples spaced by the radar PRI. In
recognition of this, we replace vI(t) with vI(kT) or vI(k) where we are using k to represent the
kth PRI, or pulse when we consider pulsed signals.
15.6.1 A Discrete-Time Model
For a single scatterer [i.e., vIi(t)], we get [from (15.34)]
After we (digitally) remove the quadratic phase term, the signal we process to form the image
is
Since vIi(k) (and vI(k) for multiple scatterers) is a discrete-time signal, we use the discrete-
time Fourier transform (DFT). Specifically, we find
As with the continuous-time Fourier transform, we limit the sum by considering that we
gather data only from –L/2 to L/2 or for |t| ≤ TL/2. If we use t = kT, the limits on k become
where it is understood that we round, or truncate, TL/2T to the nearest integer.
Combining (15.50) with (15.49), (15.48), and (15.35), we get

We note that this is similar to (15.42).
Figure 15.16 contains a normalized plot of |VIi(f)| versus (ƒ – ƒyi)TL. As can be seen, it has a
peak at ƒ – ƒyi = 0, as did Figure 15.14. However, it also has peaks at ƒ – ƒyi = ±1/T. In fact, if
we recall the theory associated with discrete-time signals and the DFT, we recognize that
|vIi(ƒ)| will have peaks at ƒ – ƒyi = ±npeak/T, where npeak is an integer. All peaks except the one
corresponding to npeak = 0 are ambiguities and are undesirable. In terms of SAR, they result
in what are termed ghost images. T and the characteristics of the SAR antenna are usually
chosen to avoid these ghosts since they can result in misleading SAR images. The SAR
antenna was mentioned because it acts as a spatial antialiasing filter [13, 17, 18].
Figure 15.16 Plot of VIi|(PSi)1/2 vs. (ƒ – ƒyi)TL using a discrete-time signal with T = 0.1TL.
15.6.2 Other Considerations
As we did before, we want to change the horizontal axis of Figure 15.16 to crossrange
distance rather than frequency. To do so, we use (15.45). This results in the plot of |vIi(y)|
shown in Figure 15.17. The ambiguities (ghosts) are shown in this figure and are located at

Figure 15.17 Plot of VIi|(PSi)1/2 vs. (y-yi)/δy using a discrete-time signal with T = 0.1TL.
Equation (15.52) tells us that we want to choose the PRI such that all scatterers lie within ±1/2
ambiguity. That is, we want to choose the PRI such that all yi satisfy
All scatterers of interest lie within the imaged area; therefore, we want to choose the PRI such
that
In fact the PRI is usually chosen such that
to be sure the SAR antenna beam (of the physical antenna on the SAR platform) adequately
attenuates targets outside of the imaged region. Because of the constraint of (15.53), the SAR
processor will form an image of an area wider than w. The desired image is determined by
truncating the generated image to the desired width.
We can turn (15.54) around and use it to find an upper bound on PRI. Specifically, we solve
(15.54) for T to yield
or from (15.55)

If we consider an earlier example where r0 = 20 km, λ = 0.03 m, and w = 50 m, and consider
an aircraft velocity of V = 50 m/s, we get
which is an easy constraint to satisfy. When we consider down-range imaging, we impose a
lower limit on T to satisfy unambiguous range operation. However, that lower limit is
generally well below the upper limit of (15.58).
We now want to summarize the above as an algorithm we can implement to form a cross-
range image.
15.7 AN ALGORITHM FOR CREATING A CROSS-RANGE IMAGE
• Assume a baseband, CW signal [see (15.30) and (15.31)]
• Sample this signal at intervals of T and generate 2KL+1 samples where
•
 (15.57)
• KL = TL/2T (15.50)
• TL = L/V (15.28)
• L = r0 λ/2δy (15.46)
In these equations, λ, r0, V, w, and δy are desired, known parameters. The samples are taken
for kT between –TL/2 and TL/2 or for k between –KL and KL.
• Remove the quadratic phase by multiplying the sampled signal by
This gives vIi(k) for a single scatterer and vI(k) for several scatterers.
• Compute the DFT of vIi(k) or vI(k), as appropriate. This is most easily done using an FFT.
The minimum FFT length is 2KL + 1, although we usually choose the FFT length to be a
power of 2 greater than 2KL + 1. In “real” applications, we often adjust various SAR
parameters so that 2KL + 1 is close to a power of 2. For purposes of problems discussed
herein, we choose an FFT length much greater (4 to 16 times) than 2KL + 1 so that the
resulting frequency plot is smooth.
• If LFFT is the length of the FFT, the frequency spacing between output FFT taps is

After the front and rear halves of the FFT outputs are swapped (to place the zero-
frequency tap in the center of the FFT output), the frequencies of the taps are
Transform the frequency scale to cross range using (15.45) and plot the magnitude of the
FFT output versus y. This does not produce an image, but instead produces a linear plot as
shown in Figure 15.16.
• To generate a pseudo image, create an array of zeros where the number of columns, Ncol,
is equal to the number of samples needed to cover the width, w, of the image area. Set the
number of rows, Nrow, equal to Ncol. (This is a somewhat arbitrary choice and can be
changed.) Finally, replace row Nrow/2 with the FFT outputs that cover w. The resulting
array is then used to create the pseudo image.
15.8 EXAMPLE 1
To illustrate the above, we consider a specific example. The parameters of this example are
given in Table 15.1.
Given these, we can compute some of the SAR parameters indicated in the algorithm
description. Specifically:
and
Table 15.1
Parameters Used in SAR Example 1
Parameter
Value
Width of image area, w
50 m
Depth of image area, l
50 m
Range to image area center, r0
20 km
SAR wavelength, λ
0.03 m
Aircraft velocity, V
50 m/s
Synthetic array length, L
600 m
Number of scatterers, NS
3

Scatterer locations, (xi,yi)
(r0,0), (r0,20), (r0,−15) m
Scatterer powers, PSi
1, 0.25, 0.09 W
We will choose a PRI of 50 ms. That is, we choose
This gives
With this, the SAR starts sampling at t = –6s and samples until t = 6 s. The samples are taken
every T = 50 ms and a total of 2KL + 1 = 241 samples are used. This means that we need, as a
minimum, a 256-point FFT. However, to produce a smooth plot, we use a 2,048-point FFT.
We note that, since we chose T = 50 ms, the actual width of the area included in the image is
To form the image, we discard the FFT outputs outside of the range of ±25 m (after the
conversion from frequency to y position).
The resolution of the SAR image is
This means we should be able to distinguish scatterers separated by about 1 m or greater, and
maybe down to 0.5-m separation if their relative powers and phases allow this.
Before processing the SAR signal using the previously discussed algorithm, we need to
generate the SAR signal. To do so, we use (15.31) with NS = 3. We generate 241 samples of
v(t) starting at t = –6 s and ending at t = 6 s in steps of T = 0.05 s. Specifically, we generate
ri(t), i = 1, 2, 3, using (15.26) and (15.27). We then combine these with the PSi values in (15.30)
to compute the three vi(t). Finally, we sum the three vi(t) to form v(t).
Figure 15.18 is a linear plot of the |VI(y)| for –25 m ≤ y ≤ 25 m, and Figure 15.19 is a
pseudo image. The pseudo image was created by starting with an array of zeros that had 101
rows (which is l/δy + 1) and a number of columns equal to the number of y values in the linear
plot. The |VI(y)| values from the linear plot were loaded into the 51st row of the array, and the
pseudo image was generated using image plotting software. The image of Figure 15.19 is a
negative image. That is, large amplitudes are black and zero is white. This was done to make
the experimental images look better, while also conserving printer ink.

In examining Figure 15.18, we note that |VI(y)| has three peaks at the y positions of the
scatterers. Further, the heights of the peaks are (PSi)1/2. The image (Figure 15.19) shows three
dots at the given scatterer positions, and the dots are different shades of gray, indicating
different amplitudes.
Figure 15.18 Linear plot of |VI(y)|—three scatterers at –15 m, 0 m, 20 m.
Figure 15.19 Image of |VI(y)|—three scatterers at –15 m, 0 m, 20 m.
To check the aforementioned resolution statement, the simulation was rerun with scatterer y
positions of –1, 0, and 1 m. The results are shown in Figures 15.20 and 15.21. The linear plot
clearly shows three peaks, but the relative amplitudes are somewhat different than those of

Figure 15.18. This is due to the sidelobes of the Fourier transform response function and the
way the responses to the three scatterers constructively and destructively combine. The
presence of the three scatterers can also be seen in the image of Figure 15.21.
Figure 15.20 Linear plot of |VI(y)|—three scatterers at –1 m, 0 m, 1 m.
Figure 15.21 Image of |VI(y)|—three scatterers at −1 m, 0 m, 1 m.
As another interesting experiment, the quadratic phase removal step of the SAR processing
algorithm was eliminated. The results are shown in Figures 15.22 and 15.23. (The original
scatterer locations of −15, 0, and 20 m were used.) As can be seen, the peaks are spread and
the image is blurred in the y direction. In SAR terminology, we say the image is not focused.
In fact, the process of removing the quadratic phase is sometimes termed focusing of the SAR

image.
Figure 15.22 Linear plot of |VI(y)|—three scatterers at −15 m, 0 m, 20 m, without quadratic phase removal.
Figure 15.23 Image of |VI(y)|—three scatterers at −15 m, 0 m, 20 m, without quadratic phase removal.
15.9 DOWN-RANGE AND CROSS-RANGE IMAGING
We now extend the previous work to both down-range and cross-range imaging. We will also
extend the problem to include a more general case of squinted SAR [2, 13, 15, 20]. Squinted
SAR is normally associated with strip map SAR, but the development here also applies to
spotlight SAR. As before, we will start by defining the signal that the SAR processor must
work with, since this will give insight into how to process the signal.

15.9.1 Signal Definition
The geometry of interest is a modification of the geometry of Figure 15.13 and is contained
in Figure 15.24. The main difference between Figure 15.13 and Figure 15.24 is that in Figure
15.13, the center of the imaged area lies on the x-axis of the coordinate system, while in
Figure 15.24 it does not. This offset of the imaged area center will result in additional
Doppler considerations plus a phenomenon termed range cell migration (RCM) [13], both of
which complicate SAR processing. Another minor difference is that the coordinates of the
scatterer are relative to the center of the imaged area. We did this as a convenience.
Since we are considering both down-range and cross-range imaging, the transmit
waveform will be pulsed instead of CW. In practical SAR, the pulses are phase coded, usually
with LFM, to achieve the dual requirements of large bandwidth for fine range resolution and
long duration to provide sufficient energy. In this development, we will use narrow, uncoded
(unmodulated) pulses to avoid complicating the development with pulse coding and the
associated matched filter or stretch processing. The extension to coded pulses is relatively
straightforward. The use of narrow, uncoded pulses also helps clarify the concept of RCM
correction (RCMC).
Given the above, we write the transmit signal as
where, as a reminder,
and τp is the pulsewidth. The sum notation means a sum over all k and is used to indicate that
the waveform is, in theory, infinite duration. We will later make it finite duration.

Figure 15.24 Geometry for down- and cross-range imaging.
The signal from a single scatterer at (xi,yi) (see Figure 15.24) is
where
15.9.1.1 Removal of the Carrier and Gross Doppler
As before, the first operation we perform is removal of the carrier. However, in addition, we
will also remove what we term gross Doppler [13]. Removal of gross Doppler is necessary in
some applications in that this Doppler is large relative to the PRF and has the potential of
causing problems with aliasing and Doppler ambiguities (ghosts).
To determine the gross Doppler, we examine the phase of the returned (RF) signal. From
(15.70), this phase is

We can find the frequency as
The first term is the carrier frequency and the second is the Doppler frequency. We define the
gross Doppler, ƒdg, as the Doppler frequency at xi = 0, yi = 0 and t = 0. That is,
In (15.74), θsquint is the squint angle [2, 13, 15, 20]. For the unsquinted SAR we considered in
the CW development, θsquint was zero because y0 was zero.
Given the above, we remove ƒo and ƒdg from the received signal by multiplying vnRF(t) by
the heterodyne signal,
With this, we get the baseband signal,
15.9.1.2 Single-Pulse Matched Filter
The next step in processing is to send vi′(t) through a matched filter matched to the transmit
pulse. The (normalized) output of the matched filter is
where
15.9.1.3 Generation of the Sampled Signal

Recall that for the CW case, we sampled vi(t) at intervals of T. We will do the same for the
pulsed case. However, for each pulse (each T) we will also subsample vi(t) at intervals of τp,
the pulsewidth. We start sampling, relative to each transmit pulse, at some
This is the minimum range delay between the front edge of the imaged region and the SAR
platform.
We continue sampling to
where
rmax is the maximum range between the SAR platform and the back of the imaged region.
Between τmin and τmax, we obtain approximately
range samples. We do this 2KL + 1 times to form M × (2KL + 1) samples, which we will collect
into an M by 2KL+1 element array for further processing.
Mathematically, we sample vi(t) at
where m is the range cell number. This gives
Equation (15.84) is the equation that generates the samples we use in the SAR processor
simulations discussed in the upcoming sections, and in the exercises. A separate vi(k,m) array
is generated for each simulated scatterer, and the composite return is created by summing the

vi(k,m) across i. m varies from 0 to M − 1 and k varies from –2KL to KL.
We previously defined an upper bound on T [see (15.56) and (15.57)] based on the width of
the imaged area. A lower bound on T is that it must be such that T > τmax – τmin. Since τmax –
τmin is usually on the order of µs and the upper bound on T is on the order of ms, both of these
bounds on T are easy to satisfy unless the imaged area becomes very wide and deep.
Figure 15.25 contains a simplified block diagram of the operations that have been discussed
thus far. The diagram is a functional representation of the operations that would be performed
in an actual SAR receiver. The voltage symbols above the line would apply when one thinks
of processing returns from a single scatterer. The symbols below the line, without the i
subscript, would apply when one thinks of processing returns from more than one scatterer.
The actual SAR receiver will need to perform many other functions such as mixing and
amplification to convert the RF signal to the digital, baseband signal sent to the SAR
processor. As a note, the mixing operation does not always remove all of the gross Doppler.
This sometimes needs to be removed as part of the subsequent processing [13].
15.9.2 Preliminary Processing Considerations
If we were to directly extend our CW processing methodology, we would, for each m, remove
a quadratic phase term and then perform a DFT across k. Unfortunately, the situation is
complicated by the range sampling so that this straightforward approach is not directly
applicable. We will need to first perform an interim step of RCMC.
Figure 15.25 Preliminary SAR processing block diagram.
15.9.2.1 Range Cell Migration Correction
Figure 15.26 contains a plot of |vi(k,m)|. The axes are range cell number, m, and cross-range
sample number, k. White corresponds to a level of zero and black corresponds to the
maximum value of |vi(k,m)|. The plot is not an image of the form of Figures 15.19, 15.20, or
15.23. It is a means of representing the magnitude of the analog to digital converter (ADC)
output as a function of two variables, m and k. The plot was generated for a single scatterer at
(xi, yi) = (0,0) m, the center of the imaged region. |vi(k,m)| was generated using the basic
parameters of EXAMPLE 1 (Section 15.8, Table 15.1) with the added parameters of (x0,y0) =
(20000,200) m, which defines the center of the imaged area. With this specification, we are
considering a squinted SAR with a squint angle of θsquint = sin−1(200/20000) = 0.0573° [see
(15.74)]. We assume a pulse with a width of τp = 3.33 ns, which translates to a range resolution
of 0.5 m. As indicated earlier, this is an unrealistic pulse for an actual SAR. We use it only to

avoid having to complicate the development by considering long, modulated pulses.
For each pulse, we start sampling in range at τmin = 133.1667 µs and stop at τmax = 133.5459
µs. We used (15.79), (15.80), and (15.81) to compute τmin and τmax. With this τmin and τmax, and
the pulsewidth of 3.33 ns, we will have M = 114 range samples [see (15.82)]. Since we want
sample 0 to correspond to r0, we let m vary from −50 to 63. From EXAMPLE 1, k varies from
−120 to 120 for a total of 241 cross-range samples.
Since we are considering a single scatterer in the center of the imaged area, our initial
expectation is that the return should be located at range cell zero (m = 0) on all pulses.
However, this is not correct because range cell location of the return depends on the range to
the scatterer, not its x location. The range to the scatterer on the kth pulse is
which means it will vary with k. This is why the line in Figure 15.26 is curved. The curving of
the line is the aforementioned RCM. This name derives from the fact that the return from a
single scatterer migrates across several range cells.
Range cell migration becomes a problem when we apply the quadratic phase correction
then take the Fourier transform to form the image. To form the image, we want to adapt the
procedure we developed for the CW case and perform these operations for each range cell
(for each row of the M by 2KL + 1 array of |vi(k,m)|). However, because of RCM we cannot do
this. Instead we should apply the quadratic phase removal and Fourier transform procedures
to the range and cross-range samples along the curved line. Since this is difficult, we take the
approach of “straightening” the curved line of Figure 15.26 [13]. Said another way, we will
remove the effect of RCM through the process of RCMC.
There are several methods of applying RCMC [13]. All of them involve some type of
interpolation, and some are more effective than others. In this book, we discuss a technique
based on the Fourier transform. It derives from a property of Fourier transforms that a linear
phase gradient applied in the frequency domain will result is a time shift in the time domain.
A characteristic of the Fourier transform technique is that it moves all range cells the same
amount for a particular k. This is adequate for small squint angles. However, for a squint
angle more than a few degrees, it is a questionable approach because, in that case, different
range cells must be moved different amounts. This is discussed further in [13].
15.9.2.2 RCMC Algorithm
As indicated above, the Fourier transform RCMC algorithm takes advantage of the time shift
property of the Fourier transform. We consider a time function v(t) with a Fourier transform

We next consider a shifted version of v(t), vS(t) = v(t - τ). The Fourier transform of vS(t) is
Equation (15.87) says that if we want to shift some v(t) by some τ we
• Find the Fourier transform of v(t), V(ƒ).
• Multiply V(ƒ) by e–j2πƒτ [apply a linear phase gradient to V(ƒ)].
• Find the inverse Fourier transform of the result.
This is the essence of the RCMC algorithm. We develop the algorithm for a single scatterer at
(xi, yi) = (0,0) m and apply it to all scatterer locations.
A suggested algorithm is as follows: from (15.85), the minimum value of ri(kT), for xi = yi
= 0, occurs when y0 – kVT = 0 and is equal to x0. We decide that we want this range to
correspond to a down-range delay of τ = 0. For each k we compute
This Δτ(k) then becomes the range correction based on the assumption that τ = 0 when y0 −
kVT = 0. We use this with the Fourier transform method to move the samples in range. The
specific algorithm is
• For each k compute Δτ(k) from (15.88).
• Compute the Fourier transform (discrete-time Fourier transform) of vi(k,m).
• Multiply the Fourier transform by e−j2πfΔτ(k).
• Compute the inverse Fourier transform.
The FFT can be used to approximate the Fourier transform and inverse Fourier transform.
The length of the FFT should be the next power of 2 that is equal or greater than M. In the
example used here, a 128-point FFT was used since M = 114. When the inverse FFT is
computed, the last 14 samples (of the 128) are discarded. The frequency values would be
computed from ƒ = s/(NFFTτp) s∈[−NFFT/2,NFFT/2 – 1]. Be sure that the FFT algorithm you use
places the zero frequency tap in the center of the FFT output.

Figure 15.26 Plot of | vi(k,m) | for a single scatterer at (xi, yi)=(0,0) m.
Figure 15.27 Plot of | vi(k,m) | for a single scatterer at (xi yi) = (0,0) m with RCMC.
The result of applying the above methodology to the plot of Figure 15.26 is shown in
Figure 15.27. As can be seen, the curved line of Figure 15.26 is now a straight line, but
somewhat blurred. The blurring is caused by the fact that the output of our matched filter is
not matched to the type of interpolation the Fourier transform performs. The Fourier
transform uses a sinc(x) interpolation but our matched filter output is a triangle function. If we
had modeled our matched filter output as a sinc(x) function, the line of Figure 15.27 would be
a straight line with no blurring.
The RCMC methodology discussed above was derived for a scatterer at the center of the
imaged region. There is a question of whether it will perform RCMC for all other scatterers
in the imaged region. To address this question, we consider two examples. In the first, we
place three scatterers at yi = 0 m and xi = −23 m, 0 m, and 23 m (range sample, or cell,
numbers of −46, 0, and 46). The resulting uncorrected plot of v(k,m) is shown in Figure 15.28
and the RCM-corrected image is shown in Figure 15.29. It will be noted that there are three
straight lines located at m = –46, 0, and 46 in the RCMCed image.

Figure 15.28 Plot of │v(k,m)│for a three scatterers at (xi, yi) = (−23,0), (0,0), (23,0) m.
Figure 15.29 Plot of │v(k,m)│ for a three scatterers at (xi, yi) = (−23,0), (0,0), (23,0) m after RCMC.
As another example, we place the three scatterers at (xi,yi) = (–23,23), (0,0), (23,–23) m.
That is, at diagonal corners and the center of the imaged area. The resulting uncorrected and
corrected plots of v(k,m) are shown in Figures 15.30 and 15.31. Careful examination of Figure
15.30 shows that the three curved lines are not exactly the same. Also, the top and bottom
straight lines of Figure 15.31 are not exactly horizontal. In some applications, this can cause
problems and an interim processing step must be used to eliminate the problem.
15.9.3 Quadratic Phase Removal and Image Formation
Now that we have an algorithm that performs RCMC, we need to develop an algorithm for
removing the quadratic phase. We will want to remove the quadratic phase from the RCMCed
signal. The information we need is in the phase of vi(k,m) (for a single scatterer, v(k,m) for
multiple scatterers) (15.84) at the peak of the tri(x) function (i.e., along the curved ridge
before RCMC).

Figure 15.30 Plot of | v(k,m) | for a three scatterers at (xi, yi)=(−23,23), (0,0), (23,−23) m.
Figure 15.31 Plot of | v(k,m) | for a three scatterers at (xi, yi)=(−23,23), (0,0), (23,−23) m after RCMC.
If we refer to vi(t) of (15.77), we find we want to examine the information in the phase of
vi(t) at
A problem with this equation is that t appears on both sides and is embedded in ri(t) on the
right side. As a result, solving for t will involve the solution of a rather complicated equation.
To avoid this, we seek a simpler approach. Specifically, we ask the question: Does the phase
of vi(t) vary slowly enough to allow the use of an approximate value of t?
We write the phase of vi(t), from (15.77), as
From calculus, we know that we can relate variations of ϕ(t) to variations of t by

Computing the partial derivative, we get
We are interested in the variation of ϕ(t) over the times we are taking measurements.
Specifically, from t = kT + τmin to t = kT + τmax. We use t0 = kT + τmin. and let Δt = τmax – τmin =
Δτ. With this we have
Figure 15.32 contains a plot of ∆ϕ(kT + τmin) versus k as the top plot. For reference, the bottom
curve is a plot of pulse-to-pulse phase change versus k. Note that the pulse-to-pulse phase
change ranges between about −1,000° and +1,000° while the phase variation, or phase error,
over ∆τ is between −0.006° and +0.006°. This indicates that ϕ(t) varies slowly over ∆τ, and
thus, it will be reasonable to compute ϕ(t) at kT + τmin, or even kT, rather than via the more
accurate form of (15.90).

Figure 15.32 Phase change and phase error vs. pulse number.
Given this, we now examine ϕ(kT) to formulate a quadratic phase correction scheme. We
write
where we made use of (15.33) to approximate the square root.
The first term of the last equality of (15.95) is a constant phase that we do nothing about.
The second term is zero, since, by (15.74), ƒdg = 2Vy0/(λr0). Finally, the third term is the
quadratic phase that we want to eliminate. It will be noted that this quadratic phase term is
exactly the same as the quadratic phase term in the CW problem. Thus, to perform the
quadratic phase correction, we multiply each row of the RCMCed signal array by
We are now in a position to formulate an algorithm for creating a cross-/down-range
image.
15.10 ALGORITHM FOR CREATING A CROSS- AND DOWN-RANGE
IMAGE
An algorithm for creating a cross- and down-range image is:
• Assume a sampled baseband signal of the form given by (15.84) (for a single scatterer—
for multiple scatterers, we would sum across i). Note: this signal has had the gross
Doppler, ƒdg, removed, even though the term appears in the equation.
• Perform RCMC. The RCMC is applied to all range cells for each k.
• Perform the quadratic phase correction by multiplying the returns for each range cell by
the vq(k) of (15.95).
• Take the FFT across pulses, for each range cell. As before, be sure that the FFT algorithm
you use places the zero frequency tap in the center of the FFT output.

• Transform the frequency and range delay axes of the output of the FFTs to cross-range
and down-range and plot the image.
Figure 15.33 contains an update to the block diagram of Figure 15.25 that includes the image
generation algorithm discussed above.
Figure 15.33 SAR processor block diagram.
15.11 EXAMPLE 2
We extend Example 1 of Section 15.8 to include cross- and down-range imaging. Table 15.2 is
a repeat of Table 15.1 with additions and modifications consistent with the cross- and down-
range image generation methodology.
Table 15.2
Parameters Used in SAR Example 2
Parameter
Value
Width of image area, w
50 m
Depth of image area, l
50 m
SAR wavelength, λ
0.03 m
Aircraft velocity, V
50 m/s
Synthetic array length, L
600 m
Number of scatterers, Ns
3
Waveform PRI, T
50 ms
Down-range resolution, δx
0.5 m
Center of imaged area (x0,y0)
(20000, 200) m
Scatterer locations, (xi,yi)
(−23, 0), (0, 0), (23, 0) m
Scatterer powers, PSi
1, 1, 1 W
As with Example 1, we have KL = 120 so that k goes from −120 to 120, and we transmit 241
pulses over a time period of −6 to 6 seconds. We use the 3.33 ns, unmodulated pulse we
considered in the RCMC discussions. Recall that since our T is smaller than the minimum
dictated by the width of the imaged area, our SAR image will need to be trimmed in cross
range before we plot the image.

We start the range sampling at τmin and let m vary from −50 to 63 as we did when we
performed RCMC. As a result, the down-range extent of the image will be −25 m to 32 m
relative to scene center. Since we are interested only in a downrange extent of −25 m to 25 m,
we will also trim the down-range coordinate of the image.
In Example 1, we used an FFT length that was longer than the number of samples because
we wanted a smooth linear plot. Since we are forming only an image for this example, we can
limit the FFT length to the nearest power of two greater than 2KL + 1. Since 2KL + 1 is 241, a
256-point FFT will suffice.
Figure 15.34 contains the image for this example. Note that the three dots are approximately
where they should be. The center dot is at (0,0) m and is fairly sharp. This is expected since
the RCMC and quadratic phase correction is based on a scatterer at the center of the imaged
area. The other two dots are somewhat smeared and are offset slightly in the cross-range
direction. The offset is due to a residual Doppler, and the smearing is due to a residual
quadratic phase.
Figure 15.34 Image for Example 2.

Figure 15.35 Image for scatterers at (−23,23), (0,0), and (23,−23) m.
Figure 15.35 contains an image that resulted when the three scatterers were placed at
(−23,23), (0,0), and (23,−23) m. Again, the center dot is reasonably sharp, but the other two
dots are offset in the cross-range dimension and smeared in both the cross- and down-range
dimensions. The cross-range offset is due to the aforementioned residual Doppler, and the
smearing is due to the residual quadratic phase. The down-range smearing is due to the
imperfect RCMC discussed in association with Figures 15.30 and 15.31.
15.12 AN IMAGE-SHARPENING REFINEMENT
We noted in the generation of Figure 15.34 that there was a slight skewing of the upper and
lower dots. Given that the skewing was in opposite directions at the top and bottom, we
surmise that it is due to a frequency shift, and possibly FM slope variation (cross range
residual quadratic phase), that is dependent upon the downrange location of the scatterer, xi.
We now examine this further.
For a scatterer at (x0 + xi, y0) we have
where we are temporarily using kT = t for convenience. We manipulate this as

where 
 is the range to the scatterer at t = 0.
With this, the phase of vi(t) is [see (15.90)]
During the quadratic phase removal step, we essentially add
to the above phase to get a corrected phase of
We first examine the linear phase, or frequency, term. We write it as
Recalling that ƒdg = 2y0V/λr0, we have
Now,
where we made use of ro2≫ 2xix0 + xi2, 2xi x 0≫ xi2, and r0 ≈ x0, since x02 ≫ y02. With this we

have
where we used r0 ri ≈ r02.
From (15.104), we see that we have a residual frequency of
When the scatterer is at scene center, xi = 0 and thus Δf = 0. That is, there is no frequency
offset. When xi ≠ 0, there will be a residual frequency offset, which will lead to a cross-range
offset.
To see if the frequency offset could be the cause of the skewing in Figure 15.34, we recall
that cross-range position is related to frequency by [see (15.45)]
With this we can write
In our case, r0 = 20 km, y0 = 200 m, xi = 25 m, and
or half of a cross-range resolution cell, which is about the shift noted in Figure 15.34. This
leads us to conclude that it might be a good idea to include a range-cell-dependent frequency
correction to the quadratic phase correction. When such a correction was included, the image
of Figure 15.36 was obtained. As the figure shows, the skewing is no longer present.

Figure 15.36 Case of Figure 15.34 with additional Doppler correction.
Very careful examination of Figure 15.36 reveals a slight cross-range smearing of the
upper and lower dots relative to the center dot. From our experience with stretch processing,
we postulate that this could be due to the residual quadratic phase term of (15.101).
From (15.101) we can write the residual quadratic phase term as
With approximations similar to the previous development, we obtain
which is a residual quadratic phase that depends on the x location of the scatterer. This
indicates that we should apply a residual quadratic phase correction that is range dependent.
The result of applying this correction is contained in Figure 15.37. Very careful examination
of this figure reveals that all three dots are equally sharp in the cross range direction.

Figure 15.37 Case of Figure 15.32 with added residual quadratic phase correction.
Figure 15.38 Case of Figure 15.35 with additional Doppler and quadratic phase correction.
Figure 15.38 contains an image equivalent to Figure 15.35 with the aforementioned residual
frequency and quadratic phase corrections included. As can be seen, the dots of Figure 15.38

seem to be slightly more focused than those of Figure 15.35. However, the upper and lower
dots are still smeared in the downrange direction. As discussed earlier, this down-range
smearing is caused by the fact that the RCM is due to cross-range position of the scatterer,
whereas the RCMC is based on a scatterer at zero cross range. Cummings and Wong [13]
present an alternate RCMC algorithm that corrects this problem. We will not discuss it here.
The reader is referred to [13].
Note that the images in Figures 15.34 through 15.38 exhibit some smearing in cross range.
This is due to sidelobes of the Fourier transform operation used to create the cross-range
dimension of the image. The smearing can be reduced by applying sidelobe reduction
weighting to the input to the FFT. However, such weighting will slightly degrade cross-range
resolution.
15.13 CLOSING REMARKS
The discussions presented in this chapter are very preliminary when compared to the body of
literature on SAR processing. The technique presented is a bare, basic image formation
method, with the exception of the image refinement technique of Section 15.12. There are
several texts that discuss other image formation and sharpening techniques [13, 17, 18]. Many
of these provide sharper images but are also more difficult to implement and run slowly when
compared to the technique discussed herein.
The technique discussed herein is applicable to both strip map and spotlight SAR for the
case where the SAR platform is moving in a straight line. There is another class of spotlight
SAR termed circular SAR. In this type of SAR, the SAR platform follows a circular path
relative to some point in the imaged area. The techniques developed in this chapter are not
applicable to this type of SAR because the RCMC technique developed herein cannot be
directly extended to the circular SAR case. The most common techniques applicable to
circular SAR appear to be a matched filter technique and a technique termed back projection
[23, 24], both of which require a large amount of computation and computer time. These
techniques are also applicable to the type of SAR considered in this chapter.
In the derivations of this chapter, it was (somewhat unrealistically) assumed that the SAR
platform was flying in the x-y plane (i.e., at an altitude of zero). The extension to a nonzero,
but constant, altitude is straightforward. In essence, when the nonzero altitude case is
considered, the image that results is in slanted plane. The points in this slanted plane can be
mapped to the ground by a coordinate transformation.
The assumption that the SAR platform was flying at a constant altitude, crossrange
position, and velocity is reasonable for satellite-based SAR because satellite trajectories are
very stable and, over L, reasonably straight relative to the imaged area, which is also
reasonably flat over w. For aircraft-based SAR, this is not the case. In this type of SAR, an
interim step of “straightening” the aircraft trajectory must be performed [2, 16].
The discussions herein make the assumption that synthetic antenna length (distance the SAR
platform travels) and the dimensions of the imaged area are small compared to the slant range
to the imaged area. If this is not the case, a somewhat more complicated method of accounting

for SAR platform motion must be used [7, 15, 16]. Also, RCM and RCMC become more of an
issue.
The developments of this chapter were based on the assumption that the transmit signal was
a narrow, unmodulated pulse. As was indicated, such a pulse is unrealistic in practical SARs
because it would dictate high peak power to get a reasonable SNR at the matched filter output.
Most practical SARs use LFM pulses of reasonable length. The only impact of this as it relates
to the processing presented herein, is that the matched filter of Figures 15.25 and 15.33 must
be matched to an LFM pulse rather than an unmodulated pulse. In some instances where
extremely high bandwidth pulses are used (to get fine down-range resolution), stretch
processing may be necessary.
Finally, one of the assumptions is that the phase and frequency of the transmit signal are
fixed over the processing interval. In other words, the signal remains coherent over the
processing interval. This could become questionable for long processing intervals. In any
event, it is something that must be considered when designing the SAR sensor and
determining the size of the image area and the attainable cross-range resolution.
15.14 EXERCISES
1.
Derive (15.10) starting from (15.9).
2.
Recreate the plot of Figure 15.7 using an appropriately modified version of (15.11).
3.
Generate the plots of Figure 15.8 but add a third plot that is the two-way, normalized,
radiation pattern for a linear array. Discuss the relation between the beamwidths of the
three plots.
4.
Recreate Figures 15.14 and 15.15.
5.
Implement SAR signal generation and processing routines using the methodology of
Section 15.7. Test your routines by duplicating Figures 15.18 through 15.21. Use the
parameters in Table 15.1. When you set up your signal generation routine, make it
general enough to accommodate any number of scatterers located at any position and
with any powers. Make it general enough to accommodate any sample period and any
SAR array length.
6.
In the signal generation code from Exercise 5, decrease the number of scatterers to one
centered at y1 = 0 with an amplitude of unity. In your SAR processing code, do not
perform the quadratic phase correction. Finally, decrease the sample period to T = 10 ms
and form the pseudo image. Is this what you expected? Explain.
As an interesting experiment, try a few different values of T to see what happens to the
pseudo image. Discuss your results.
7.
In the signal generation code from Exercise 5, place scatterers at y = 20 and y = 30 and
give them amplitudes of unity. Process the signal from the two scatterers through your
SAR processor and produce the pseudo image. Is the pseudo image what you expected?

Explain.
8.
Change T to its maximum value of 120 ms and repeat Exercise 7.
9.
Implement a SAR signal generation algorithm as described in Section 15.9 and generate
the plot of Figure 15.26.
10. Implement a RCMC algorithm and reproduce the image of Figure 15.27. Generate the
images of Figures 15.28 through 15.31.
11. In the discussion of Figure 15.27, it was indicated that the blurring was caused by the fact
that an unmodulated pulse was used in the signal generation routine. This type of pulse is
not ideally compatible with the use of the Fourier transform to perform interpolation. If
the signal generation routine had used an LFM pulse, the resulting matched filter output
would have been more compatible with Fourier transform interpolation, and the blurring
to the line would not be present. The output of a matched filter for an LFM pulse can, for
the purposes of this exercise, be approximated by
where τp is the compressed pulsewidth and τu is the width of the uncompressed LFM
pulse. For this exercise, use the compressed pulsewidth of Exercise 9 and use an
uncompressed pulsewidth of 50 µs. Use this equation in the signal generation code you
developed for Exercise 9 and generate plots like Figures 15.26 through 15.31. You should
note that the blurring in Figure 15.37 is now significantly reduced.
12. Extend the RCMC algorithm of Exercise 10 to include the image formation algorithm of
Sections 15.6.6 and 15.6.7. Reproduce the figures of Example 2 (Figures 15.34 and 15.35).
Place five scatterers in the imaged area and generate the resulting image. Use the same
amplitude for the five scatterers. Are the scatterers where you expected them to be?
Explain.
Try this exercise with the unmodulated pulse discussed in the text, and with the LFM
pulsed introduced in Exercise 11.
13. Implement the image sharpening algorithms discussed in Section 15.12 and reproduce
images like those of Figures 15.36 through 15.38. Repeat this with the five scatterers you
used in Exercise 12.
14. Use the SAR processor you developed in the previous exercises to create an image from

the data in either the file named Trinity.txt or the file named Trinity.mat. The file named
Trinity.mat is a Matlab mat file which you can read with the command “load Trinity.” This
will cause v(k,m) [see (15.84) and (15.85)] to be loaded into the 114 by 241 complex array
with the name RD. The Trinity.txt file is a text file that contain 241 columns of data with
228 entries in each column. The first 114 rows of the file are the real part of v(k,m), and
the last 114 rows are the imaginary part of v(k,m). The image generated by the SAR
processor will be a photo since v(k,m) was generated from a photo using (15.84) and
(15.85). The photo can be found in the file Trinity.jpg. The various parameters that were
used to generate the signal are those of Table 15.2. Thus, your processing algorithm
should use the same parameters. When you form the image, do not use a negative as
discussed in the text (unless you want to see a negative of the photo). Also, when you
form the image, turn the axis labels off so the image will look like a photo. Try the
processor with and without the image refinement algorithms of Section 15.12.
15. For this exercise, you will use some actual SAR data to form and image. The data was
obtained from the RADARSAT1 spaced-based SAR platform. The data is a subset of the
SAR data found on a compact disc that accompanies [13]. The files were preprocessed to
put them in a form that is compatible with the signals discussed in this chapter.
Specifically, the signals were preprocessed to create v(k,m). The preprocessed data is
contained in the text file labeled SARData.mat. This file contains 1,536 columns of ASCII
data where each column contains 2,048 rows. The first 1,024 rows of the file contain the
real part of v(k,m) and the last 1,024 rows contain the imaginary part of v(k,m). In
MATLAB, the data can be loaded by using the command “s=load(‘SATData,txt’);”
followed by the command “v=s(1:1024,:)+j*s(1025:end,:);”
16. For this exercise, you will use some actual SAR data to form and image. The data was
obtained from the RADARSAT1 spaced-based SAR platform. The data is a subset of the
SAR data found on a compact disc that accompanies [13]. The files were preprocessed to
put them in a form that is compatible with the signals discussed in this chapter.
Specifically, the signals were preprocessed to create v(k,m). The preprocessed data is
contained in the text file labeled SARdata.txt. The file contains two columns of data. The
first column is the real part of v(k,m) and the second column is the imaginary part of
v(k,m) [see (15.84) and (15.85)]. The file has 1,024 × 1,536 = 1,572,864 rows. After you
load the data file, reshape it into a 1,024-by-1,536 array of complex numbers.
Specifically, the array should contain v(k,m) for k = 1 to 1,024 and m = 1 to 1,536.
A photo of the imaged region of the RADAR SAT 1 data is in the lower left part Figure
15.39. The dark area is water and the gray area is land. The two projections into the water
are docks. The data supplied is for an image of the larger dock and the edge of the
smaller dock. The image you create will also show a ship or two that is not in the photo.
The geometry for this case is somewhat different than the one indicated in Figure 15.24.
Specifically, the squint angle, θ, is negative for this data. Also, the imaged area is well
behind the satellite. Because of these factors, the Δτ(k) (15.86) used for RCMC must be
changed to

where
The reason for this change is that the minimum range, for RCMC purposes, is the
distance between the position of the satellite at y = L/2 and the upper left corner of the
imaged area. x0 was also redefined, as shown in Figure 15.40.
The various SAR parameters you need are contained in Table 15.3. You should be able to
compute the other parameters from those given in the table.
Since there are 1,536 down-range samples for each cross-range position of the SAR, use
a 1,536-point FFT in your RCMC algorithm. While this is not an exact power of 2, the
FFT should also be fast since 1,536 = 210 + 29
Table 15.3
SAR Parameters for Exercise 15
Parameter
Value
L
8,624 m
x0
993.4627 km
y0
−27.466 km
fdg
−6,750 Hz
V
7,062 m/s
T
1/PRF
PRF
1,256.98 Hz
λ
0.05657 m
τρ
(1/32.317) µs
As with Exercise 14, create a positive image. It may be necessary to adjust the contrast of the
final image. If you use MATLAB, this can be done through the clim parameter of the imagesc
image generation routine. A value of clim that seems to work is clim = [3,000 15,000].

Figure 15.39 Photo of the region for which RADARSAT1 SAR data was provided in this exercise. (RADARSAT Data ©
Canadian Space Agency/Agence Spatiela Canadienne 2002—All Rights Reserved.)

Figure 15.40 SAR geometry applicable to Exercise 15.
References
[1]
Skolnik, M. I., ed., Radar Handbook, 3rd ed., New York: McGraw-Hill, 2008.
[2]
Hovanessian, S. A., Introduction to Synthetic Array and Imaging Radars, Dedham, MA: Artech House, 1980.
[3]
Griffiths, H. D., “Developments in Modern Synthetic Aperture Radar,” Proc. 2007 IEEE Radar Conf., Boston, MA, Apr.
17–20, 2007, pp. 734–739.
[4]
Wiley, C. A., “Pulsed Doppler Radar Methods and Apparatus,” U.S. Patent 3,196,436, Jul. 20, 1954.
[5]
Wiley, C. A., “Synthetic Aperture Radars: A Paradigm for Technology Evolution,” IEEE Trans. Aerosp. Electron. Syst.,
vol. 21, no. 3, May 1985, pp. 440–443.
[6]
Love, A. W., “In Memory of Carl A. Wiley,” IEEE Antennas Propag. Soc. Newsletter, vol. 27, no. 3, Jun. 1985, pp. 17–
18.

[7]
Morris, G. V., Airborne Pulsed Doppler Radar, Norwood, MA: Artech House, 1988.
[8]
Cutrona, L. J., and G. O. Hall, “A Comparison of Techniques for Achieving Fine Azimuth Resolution,” IRE Trans.
Military Electron., vol. 6, no. 2, Apr. 1962, pp. 119–121.
[9]
Lee, H., “Extension of Synthetic Aperture Radar (SAR) Technique to Undersea Applications,” IEEE J. Oceanic Eng.,
vol. 4, no. 2, Apr. 1979, pp. 60–63.
[10] Reigber, A., et al., “Very-High-Resolution Airborne Synthetic Aperture Radar Imaging: Signal Processing and
Applications,” Proc. IEEE, vol. 101, no. 3, Mar. 2013, pp. 759–783.
[11] Elachi, C. et al., “Spaceborne Synthetic-Aperture Imaging Radars: Applications, Techniques, and Technology,” Proc.
IEEE, vol. 70, no. 10, Oct. 1982, pp. 1174–1209.
[12] Ottl, H., and F. Valdoni, “X-Band Synthetic Aperture Radar (X-SAR) and Its Shuttle-Borne Application for
Experiments,” Microwave 17th European Conf. 1987, Rome, Italy, Sept. 7–11, 1987, pp. 569–574.
[13] Cumming, I. G., and F. H. Wong, Digital Processing of Synthetic Aperture Radar Data: Algorithms and Implementation,
Norwood, MA: Artech House, 2005.
[14] Carrara, W. G., et al., Spotlight Synthetic Aperture Radar: Signal Processing Algorithms, Norwood, MA: Artech
House, 1995.
[15] Brookner, E., Radar Technology, Dedham, MA: Artech House, 1977.
[16] Kovaly, J. J., Synthetic Aperture Radar, Dedham, MA: Artech House, 1976.
[17] Mensa, D. L., High Resolution Radar Imaging, Dedham, MA: Artech House, 1981.
[18] Oliver, C., and S. Quegan, Understanding Synthetic Aperture Radar Images, Norwood, MA: Artech House, 1998.
[19] Rihaczek, A. W., Principles of High-Resolution Radar, New York: McGraw-Hill, 1969. Reprinted: Norwood, MA:
Artech House, 1995.
[20] Wehner, D. R., High Resolution Radar, Norwood, MA: Artech House, 1987.
[21] Brown, W. M., and L. J. Porcello, “An Introduction to Synthetic-Aperture Radar,” IEEE Spectrum, vol. 6, no. 9, 1969,
pp. 52–62.
[22] Currie, A., “Synthetic Aperture Radar,” Electron. & Comm. Eng. J., vol. 3, no. 4, Aug. 1991, pp. 159–170.
[23] Fitch, J. P., Synthetic Aperture Radar, New York: Springer-Verlag, 1988.
[24] Soumekh, M., Synthetic Aperture Signal Processing with MATLAB Algorithms, New York: Wiley & Sons, 1999.
[25] Argenti, F. et al., “A Tutorial on Speckle Reduction in Synthetic Aperture Radar Images,” IEEE Geoscience Remote
Sensing Mag., vol. 1, no. 3, Sept. 2013, pp. 6−35.
1We chose an odd number of elements to simplify some of the notation to follow.
2 As we develop the detail of SAR processing, we will find we need to abandon this approximation. For now we take note of
this and proceed.
3 We scaled the radiation pattern of Chapter 12 by Ps and (2N + 1) to normalize it to the units of power.
4 In our initial discussions, we will be concerned only with cross-range imaging and can thus use a CW signal. We will
consider a pulsed signal when we add the second dimension.
5 We are changing terminology from “target” to “scatterer” since the latter is common in SAR theory.
6 Note that this is similar to stretch processing, wherein we remove the quadratic phase in the mixer.
7Again, note the similarity to stretch processing.

Chapter 16
Introduction to Space-Time Adaptive Processing
16.1 INTRODUCTION
In this chapter, we provide an introduction to Space-Time Adaptive Processing, or STAP.
When we discuss radars, we normally consider the processes of beam forming, matched
filtering, and Doppler processing separately. By doing this, we are forcing the radar to
operate in only one domain at a time: space for beam forming, fast time for matched filtering,
and slow time for Doppler processing. This separation of functions sacrifices capabilities
because the radar does not make use of all available information, or degrees of freedom.
Suppose we have a linear phased array that has N elements. In terms of beam forming, to
maximize the target return and minimize returns from interference (e.g., clutter, jammers, and
noise), we say that we have 2N degrees of freedom. If we also process K pulses in a Doppler
processor, we say we have an additional 2K degrees of freedom. With normal processing
methods, whereby we separate beam forming and Doppler processing, we have a total of 2K +
2N degrees of freedom. If we were to consider that we could simultaneously perform beam
forming and Doppler processing, we would have 2KN degrees of freedom. This is the
premise of the “ST” part of STAP.
Figure 16.1 might provide further help in visualizing this. It contains a depiction of angle-
Doppler space. Each of the squares corresponds to a particular angle and Doppler. There are
N beam positions and K Doppler cells. The dark square indicates a beam position and Doppler
cell that contains interference. With standard processing techniques, we would suppress the
interference by independently placing a null at the beam position and Doppler cell containing
the interference. The beam null is denoted by the crosshatched squares, and the Doppler null is
denoted by the dotted squares. With this approach, the process of suppressing the interference
will also cause any signals in the cross-hatched and dotted regions to be suppressed, including
target signals. This happens because we separately process in angle and Doppler space.

Figure 16.1 Clutter nulling using conventional methods.
With STAP, we would, ideally, simultaneously process in angle and Doppler space. With
this simultaneous processing, the processor can be made to place a null at only the angle and
Doppler of the interference (at the location of the dark square of Figure 16.1). Thus, it is
possible to suppress only interference, and not suppress other signals that might be located at
the same angle or Doppler of the interference.
According to [1], it appears that the concept of STAP was first introduced in a 1973 paper
by Brennan and Reed [2]. STAP has been, and still is, extensively studied in applications such
as SAR, GMTI, MIMO radar, array antennas, tracking radar, SONAR, early warning, and
jamming suppression [3–7]. Despite the relatively high processing burden, there are many
implemented and fielded STAP platforms [8–10].
We begin the discussion of STAP by first discussing spatial processing (the “S”) and then
temporal processing (the “T”). We next discuss how these are combined to perform space-
time processing. Following that, we briefly discuss some topics related to the “A,” or
adaptive, part of STAP.
The general approach used in STAP is to design the processor to maximize signal-to-
interference-plus-noise ratio (SINR) [11–13]. This is the same as the approach used in the
matched filter development of Chapter 7. In fact, for the case where the interference is “white”
in the space-time domain, the space-time processor is equal to the space-time representation
of the signal. That is, the space-time processor is matched to the signal. As a further
illustration of the relation between the matched filter and STAP, we note that one of the
Cauchy-Schwarz inequalities is used to design the space-time processor [14, 15].
16.2 SPATIAL PROCESSING
As indicated, we begin the STAP development by first considering spatial processing, or beam
forming. We start by considering the signal and receiver noise and then address a combination
of signal, receiver noise and interference, such as jamming or clutter.

16.2.1 Signal Plus Noise
We start with the N element linear array shown in Figure 16.2.1 In that figure, it is assumed
that the target is located at an angle of θs relative to broadside. From linear array theory (see
Chapter 12) we can write the output of the array as2
where PS is the signal power from the target at each of the array elements. It is the signal
power term of the radar range equation, without the receive directivity term (see Chapter 2).
Figure 16.2 Linear phased array.
We define
and
WH is the weight vector from Chapter 123 and S(θs) is the target, or signal, steering vector.
The superscript H denotes the Hermitian, or conjugate-transpose operation [16] (this notation

will come into play shortly). WH is also sometimes thought of as weights in a spatial filter.
Using (16.2) and (16.3), we can write V(θs) as
We assume there is a separate receiver connected to each element. This makes the noise at
each of the antenna elements of Figure 16.2 uncorrelated. This is depicted in Figure 16.3 by
the separate nn in each block. The nn are complex random variables that we assume are zero-
mean and uncorrelated. That is
We further stipulate
where PN is the noise power at the input to each of the an of Figures 16.2 and 16.3. Equation
(16.6) implies the noise power is the same at the output of each receiver. Strictly speaking, this
is not necessary. We included it here as a convenience.
The noise voltage at the output of the summer of Figure 16.3 can be written as
where
Figure 16.3 Array with only noise.
As a point of clarification, the signal and noise in the above equations are at the output of
the matched filter of each receiver. That is, the weights an are applied to the signal and noise

at the outputs of the matched filters. More specifically, the signal-plus-noise (plus
interference) is sampled at the output of the matched filters and then sent to the processor.
Ideally, the samples are taken at a time corresponding to the range delay to the target to be
sure that the signal is present in the matched filter output. If the target range delay is not
known, several range (time) samples and processors will be needed.
Receivers are needed at each element to implement STAP in its pure form. If we are willing
to give up spatial degrees of freedom, receivers could be applied to groups of elements, or
subarrays. However, with this approach, we limit where the STAP can place nulls. Further
discussion of subarraying and STAP can be found in STAP literature [3, 7, 12].
The STAP design criterion is maximization of SINR (SNR for the noise-only case) at the
processor output. Therefore we need to develop equations for the signal and noise power at
the processor output. From (16.4), the signal power at the processor output is
Since the noise is a random process, we write the noise power at the output of the summer
as
In (16.10)
and is termed the receiver noise covariance matrix. In (16.11), I is the identity matrix. With
(16.11), the output noise power becomes
The SNR at the output of the summer is
At this point we invoke one of the Cauchy-Schwarz inequalities [14, 15]. In particular, we
use

with equality when
where κ is an arbitrary, complex constant, which we will set to unity. With this, we get
where we made use of
Equation (16.16) tells us that the SNR at the array output has an upper bound equal to the
sum of the SNRs at (the outputs of the matched filters of the receivers attached to) each
element. Further, the actual SNR at the array output will equal the upper bound if W is chosen
according to (16.15), that is W is matched to S(θs).
16.2.2 Signal Plus Noise and Interference
We now consider a case where we have interference that is correlated across the array. This
interference could be clutter and/or jammers. The appropriate model for this situation is
given in Figure 16.4. In this figure, nIi represents the interference “voltage” and is a zero-
mean, complex, random variable. The subscript i is used to represent the ith interference
source (which we will need shortly when we consider multiple interference sources). The fact
that the same random variable is applied to each of the antenna elements makes the outputs of
the elements random variables that are correlated. We write VIi(ϕi) as
where
and

Figure 16.4 Array with interference.
D(ϕi) is the steering vector for the ith interference source.
We accommodate multiple interference sources by simply summing the voltages for the
multiple sources. Specifically,
We further assume the Ni interference sources are independent so that
The interference power (from the Ni interference sources) is
In (16.23)

where we made use of (16.22).
Combining (16.10) with (16.23), we get the total noise plus interference power as
and write the signal-to-interference-plus-noise ratio (SINR) at the output of the summer as
As before, we want to choose the spatial filter that maximizes SINR. To do this using the
Cauchy-Schwarz inequality, we need to manipulate (16.27). We start by noting that, because of
the receiver noise, R will be positive definite [14]. Because of this, we can define a matrix,
R1/2, such that R = R1/2R1/2. Further, R1/2 is Hermitian and its inverse, R–1/2, exists, and is
Hermitian [11, 14]. We use this to write
where WR = R1/2W and SR(θs) = R–1/2S(θs).
Equation (16.28) has the same form as (16.13). Thus, we conclude that the SINR is
maximized when
If we let κ = 1 and substitute for WR and SR (θs) we get the solution
The net effect of the above equation is that the weight, W, are, ideally, selected to place the
main beam on the target and simultaneously attempt to place nulls at the angular locations of
the interference sources. We used the qualifier “ideally” because it is possible that the
algorithm will not place the main beam at the target angle or a null at the interference angle.
This might happen if the target and interference angles were close to each other (see Exercise
7).

A critical part of this development is that the total interference consists of both receiver
noise and other interference sources. The inclusion of receiver noise is what makes the R
matrix positive definite and thus nonsingular. If R was singular, R–1 would not exist, and we
would need to use another approach for finding W. On occasion, R will become ill
conditioned because the jammer-to-noise ratio (JNR) is large. If this happens, alternate
methods of finding W may be needed. One of these is to use a mean-square criterion such as
least-mean-square estimation or pseudo inverse [17–21]. Another is termed diagonal loading,
which is discussed later.
16.2.3 Example 1
As an example, we consider a 16-element linear array with ½ wavelength element spacing (d/
λ = ½). We assume that we have a per-element SNR of 0 dB (at the output of the matched
filters of the receivers associated with each of the elements). That is, PS/PN = 1 W/W. We have
two noise jammers with per-element JNRs of 40 dB (again, at the outputs of the matched
filters). The target is located at an angle of zero, and the jammers are located at angles of
+18º and −34º. The selected jammer angles place the jammers on the second and fourth
sidelobes of the antenna pattern that results from using uniform illumination (see Figure
16.5). The above specifications lead to the following parameters PS = 1, PN = 1, PI1 = 104, PI2
= 104, 6S= 0, ϕ1 = 18°, and ϕ2 = – 34°.
For the first case, we consider only receiver noise (no jammers). From (16.15) with κ = 1,
we have
and SNRmax = 16PS/PS = 16 W/W or 12.4 dB. The weight vector, W, results in an array with
uniform weighting, or uniform illumination (see Chapter 12). A plot of the normalized
radiation pattern for this case is shown as the dotted curve in Figure 16.5, which is mostly
obscured by the solid curve. As a note, the patterns of Figure 16.5 were generated using
where θ was varied from –90° to 90°.4

Figure 16.5 Normalized radiation pattern with and without optimization—16-element linear array.
If we use the W given by (16.31) and include the two interference sources, the SINR, at the
processor output, is about –24 dB. If we include the interference properties in the calculation
of W by using (16.30), the SINR increases to 12 dB, which is close to the noise-only case of
about 12.4 dB (10log16). To accomplish this, the algorithm chose the weights to place nulls in
the antenna pattern at the locations of the interference sources. This is illustrated by the solid
curve of Figure 16.5, which is a plot of the radiation pattern when the new weights are used.
16.3 TEMPORAL PROCESSING
The temporal processing part of STAP is most often thought of as Doppler processing. In
particular, we consider the returns (signal and interference) from several pulses and, similar
to spatial processing, weight and sum them. As with spatial processing, we choose the weights
to maximize SINR at the output of the processor. The input to the Doppler processor is the
output of the matched filter. Thus, we need to characterize the signal, noise, and interference
at the matched filter output.
16.3.1 Signal
We consider a transmit waveform that consists of a string of K pulses and write it as
where p(t) is a general representation of a pulse and T is the spacing between pulses, or pulse
repetition interval, PRI (see Chapter 1). As examples, for an unmodulated pulse

and for an LFM pulse
where τp is the (uncompressed) pulsewidth and α is the LFM slope (see Chapter 7). The
exponential term in (16.33) represents the carrier part of the transmit signal (see Chapter 1).
The normalized return signal, from a point target, is a delayed and scaled version of vT(t).
We define it as
where PS is the signal power at the matched filter output and r(t) is the range to the target.
If the target is moving at a constant range rate, we can write r(t) as
where r0 is the target range at t = 0 and  is the range rate (see Chapter 1). We usually set t = 0
at the beginning of the train of K pulses.
With (16.37) vr(t) becomes
In (16.38), fd = −2 /λ, τr = 2r0/c and λ = c/fo is the wavelength of the transmit signal. If we
assume that the phase across the pulse is constant, we can write
In the receiver, we heterodyne to remove the carrier, normalize away the first exponential,
and process the signal through the matched filter to obtain

where m(t) is the response of the matched filter to p(t). We assume m(t) is normalized to a
peak value of m(0) = 1.
For the next step, we sample vM (t) at times τ = τRC + kT. That is, we sample the output of the
matched filter once per PRI at a time τRC relative to the leading edge of each transmit pulse.5
The result is a sequence of samples we denote as
where mRC is the (generally complex) value of m(t – τr – kT) evaluated at t = τRC + kT. If we
sample the matched filter output at its peak, we will have τRC = τr and mRC = 1.
16.3.2 Noise
The noise at the matched filter output is also sampled at t = τRC + kT. This produces a sequence
of K, uncorrelated, zero-mean, random variables with equal variances (and mean-square
values, or powers) of PN. We denote these as
As a note, it is not necessary that the noise samples be uncorrelated and have equal
variances. However, this is the standard assumption when discussing STAP [11].
Assuming we sample the matched filter output at its peak when only signal and noise are
present, the signal power in each sample is PS and the noise power for each sample is PN.
Thus, the SNR at the sampler output, and the input to the processor, is SNR = PS/PN.
16.3.3 Interference
We assume the interference bandwidth is narrow relative to the transmit waveform PRF (PRF
= 1/T). More specifically, we assume the interference signal at the sampler output is a wide-
sense stationary, zero-mean random process with an autocorrelation given by
where vI(k) is the interference voltage at the sampler output. It is equal to the output of the
matched filter, sampled at t = τRC + kT, when the input is the signal returned from the
interference.
In general, RI(k) is a complicated function of k. For the special case where the interference
is a tone with a Doppler frequency of fI and a random amplitude with a mean-square value
(power) of PI, RI (k), becomes

16.3.4 Doppler Processor
We assume the Doppler processor is a K-length finite impulse response (FIR) filter with
coefficients of ωk. If the input to the processor is vin(k), the output, after K samples have been
processed, is
where
and
When the input is the signal, we have, from (16.41)
If we further assume the sampler samples the matched filter output at t = τR + kT, we have
mRC = 1. Using (16.47) we have
The signal voltage at the Doppler processor output is
For the noise, we write
and the output of the Doppler processor is
We write the interference input to the Doppler processor as

and the processor output as
As with the spatial processing case, we choose the Ω that maximizes SINR at the Doppler
processor output. Thus, we need an equation for the peak signal power, PSo, and the total,
average interference power, PNo + PIo, at the processor output. By using the sum of the noise
and interference powers, we are assuming the receiver noise and the interference are
uncorrelated. This is a standard assumption.
The peak signal power is
and the average noise power is
Since we assumed the noise samples were uncorrelated and had equal power,
and
The interference power at the processor output is
where

where RI(k) is defined in (16.43).
For the case where the interference is a tone,
where
For multiple interference sources
where the sum is taken over the total number of interference sources.
The SINR at the Doppler processor output is
This is the same form as in the spatial processing case. Applying those results here gives
16.3.5 Example 2
As an example, we consider a Doppler processor with K = 16. We assume an input SNR of 0
dB (at the output of the matched filter). That is, PS/PN = 1W/W. We also assume we sample the
matched filter output at t = τR + kT. We have two tone interferences with JNRs of 40 dB (again,

at the output of the matched filter). We assume a PRF of 1,000 Hz, which gives T = 0.001 s.
The target is located at a Doppler frequency of zero, and the interferences are located at
Doppler frequencies of 217 Hz and –280 Hz. These Doppler frequencies place the
interferences on the second and fourth sidelobes of the Doppler processor frequency
response that results from using uniform weighting. The above specifications lead to the
following parameters: PS = 1, PN = 1, PIl = 104, PI2 = 104, fd = 0, fIl = 217 Hz, and fI2 = –280
Hz.
For the first case, we consider only receiver noise. From (16.65) with R = PNI we have
And SNRmax = 16 PS/PN = 16 W/W or 12.4 dB. The weight vector, Ω results in a Doppler
processor with uniform weighting. A plot of the normalized frequency response of the
Doppler processor is shown as the dotted curve in Figure 16.6, which is mostly obscured by
the solid curve. As a note, the frequency responses of Figure 16.6 were generated using
where f was varied from –PRF/2 to PRF/2, or –500 Hz to 500 Hz.
If we use the Ω given by (16.66) and include the two interferences, the SINR, at the
processor output, is about –22 dB. If we include the interference in the calculation of Ω by
using (16.65), the SINR increases to 12 dB, which is close to the noise-only case of about 12.4
dB (10log16). To accomplish this, the algorithm chose the weights to place nulls in the
frequency response of the Doppler processor at the Doppler frequencies of the interferences.
This is illustrated by the solid curve of Figure 16.6, which is a plot of the frequency response
when the new weights are used.

Figure 16.6 Normalized frequency response with and without optimization—16 tap Doppler processor.
16.4 ADAPTIVITY ISSUES
We have discussed both the space and time parts of STAP. However, we have not addressed the
adaptive part. Since the target and interference angles and Dopplers could change every dwell
(sequence of K pulses), the target steering vector and the R matrices must be recomputed on
each dwell. This means that new weights would be computed on each dwell to adapt to the
target and interference environment— thus the adaptive part. In Section 16.6, we discuss
another aspect of adaptivity that involves measuring the environment to estimate the R matrix.
16.5 SPACE-TIME PROCESSING
We now address the issue of combined space and time processing. In space-time processing,
rather than form a function of angle or a function of Doppler, we combine spatial and
temporal equations for the signal [(16.1) and (16.50)] to form a combined function of angle
and Doppler at the output of the space-time processor. In equation form, we write
We recognize the above as a sum of KN terms. Generalizing the product of the weights to
KN distinct weights we get

We next organize the weights into a general weight vector, w, and the e−j2πndsinθ/λ ej2k/ft
terms into a generalized steering vector, S, and write V (θs,fd) in matrix form as
Extending the interference representation of Sections 16.2 and 16.3, we can write the
interference at the space-time processor output as
where
In (16.72), N is the receiver noise and D(ϕr,fr) is the steering vector to the interference in
angle-Doppler space. With this representation of interference, we are limiting ourselves to
tone interferences.
We use the techniques discussed in Sections 16.2 and 16.3 to place the “main beam” in
angle-Doppler space on the target and to place nulls at the angle-Doppler locations of the
interferences. Specifically, we find that the optimum weight vector is given by
where
and K is an arbitrary, complex constant that we normally set to unity.
At this point, we need to further discuss the signal and interference steering vectors, S(θs,fd)
and D(θr,fr), and how to compute R. We note that the exponential terms of (16.68) and (16.69)
contain all possible KN combinations of e−j2πndsinθ/λ and ej2klfr. We organize the N
exponentials containing θS into a vector
and the K exponentials containing fd in to a vector

We next use these vectors to form a matrix
that contains all KN combinations of the elements of S(θs) and S(fd). To form the KN element
vector, S(θs,fd), we concatenate the columns of S(θs,fd). The D(θr,fr) vector for each
interference source is formed in a similar fashion.
From (16.72) and (16.74), we can form R as
where we made use of the standard assumption that the receiver noise, N, and interference, nI,
are independent.
There are N receivers and matched filters, and each receiver processes K pulses though the
matched filter and sampler. Thus, we will have KN receiver noise samples. We assume they
are all zero-mean, uncorrelated, and have equal powers of PN. Thus,
where I is an KN by KN identity matrix.
For each interference we have
where PIi is the power associated with the ith interference. With this we get
where the sum is taken over the total number of interference sources.
With some thought, it should be clear that the dimensionality of the STAP problem has
increased substantially, when compared to only spatial or temporal processing. If we perform
STAP separately in angle and Doppler, we would need to compute K + N weights. If we
simultaneously perform STAP in angle and Doppler space, we must compute KN weights. To
complicate the problem further, remember that we need to compute a separate set of weights

for each range cell that is processed. This represents a considerable computational burden. To
minimize the burden, much of today’s research in STAP is concerned with avoiding the
computation of KN weights, while still trying to maintain acceptable performance [11].
16.5.1 Example 3
As an illustration of the space-time processing, we extend Examples 1 and 2 to a full space-
time processor. We again assume a 16-element array and a Doppler processor that uses 16
pulses. We use the classical STAP approach and process all 16 × 16 = 256 signal-plus-noise-
plus-interference samples in one processor with 256 weights. (Recall that we do this for each
range cell of interest.) We assume the target is located at an angle of zero and a Doppler
frequency of zero. The element spacing is ½ wavelength and the PRF is 1,000 Hz. The single-
pulse, per-element SNR is 0 dB (at the outputs of the matched filters). We consider two tone
interference sources. They are located at angles of +18º and −34º. Their Doppler locations,
corresponding to the above angles, are 217 Hz and −280 Hz respectively. The JNRs of the two
interference sources are 50 dB. With these specifications, we get the following parameters: PS
= 1, θs, = 0, fd = 0, PN = 1, P I1 = 105, PI2 = 105, ϕ1 = 18°, ϕ2 = −34°, f1 = 217 Hz and f2 = −280
Hz.
We compute R using (16.79) through (16.81). Since θS = 0 and fd = 0
or a vector of 256 ones. Finally, we compute w using (16.73) with k = 1.
In an actual STAP implementation, we would compute the output of the STAP processor
using
where Vin is a vector that contains the KN outputs from the samplers in each receiver. The first
N elements of Vin are the outputs from the N receivers on the first pulse. The next N elements
are the outputs from the N receivers on the second pulse, and so forth.
For this example problem, we want to generate a three-dimensional plot of the processor
output as a function of angle and frequency. We can do this in several ways. One would be to
use (16.70) and compute
for θ and f of interest. An alternate method would be to use the FFT to implement [see (16.69)]

and use
This was the method used to generate the plots of Figures 16.7 and 16.8. The weight vector
is formed into a two-dimensional weight matrix, W, by reversing the algorithm used to form
S(θs,fd) and D(ϕr,fr). That is, we let the first column of W be the first N elements of w, the
second column be the second N elements, and so forth. We next compute V(θ,f) by computing
the Fourier transform of W using a two-dimensional (2-D) FFT. Finally, G(θ,f) is computed
using (16.86).
The results of this process are shown in Figures 16.7 and 16.8. The figures are contour
plots where shading is used to indicate power in dB. The bar to the right provides the relation
between power level and shading. The y-axis is sin(θ) and has the units of sines (see Chapter
12). This vertical axis scaling was chosen because it was compatible with the routine used to
generate the plots. A 512 by 512, 2-D FFT (rather than a 16 by 16, 2-D FFT) was used to
generate the plots. This was done to provide a plot that showed the gradations in power level.
Figure 16.7 is a plot of G(θ,f) for the case where the interference consisted of only receiver
noise. Since the target was located at (θs,fd) = (0,0), the resulting weight, w, was a vector of
256 ones. As expected, the peak of G(θ,f) occurs at (0,0). Note that the two interference
sources are located on the peaks of two angle-Doppler sidelobes G(θ,f). Because of this, the
only rejection of these sources offered by the processor is due to the amplitudes of the
sidelobes relative to the response at (0,0). The SINR for this case was –13.4 dB. When the
interference sources were omitted, the SNR was the expected, noise-limited value of
10log(256) = 24.1 dB.

Figure 16.7 Angle-Doppler map—weights based on only receiver noise.
Figure 16.8 Angle-Doppler map— two interference sources included in weight computation.

Figure 16.8 is a plot of G(θ,f) for the case where the interference sources were included in
the weight computation. The two nulls at the locations of the interferences are clearly visible,
as is the main beam at (0,0). With this set of weights, the SINR was 24.1 dB, which is the noise-
limited value.
As an experiment, the optimization was extended to include two desired targets: one at (0,0)
and another at (θs2,fd2) = (39°, 217 Hz). Both targets had the same normalized power of PS1 =
PS2 = 1. The second target was also placed so that its Doppler frequency was the same as one
of the interference sources. However, it was separated in angle from the interference source.
The other interference source was left at location shown in Figures 16.7 and 16.8.
Figure 16.9 contains G(θ,f) for the case where the weight computation was based on only
receiver noise. As can be seen, the calculated weights are such that there are two main lobes at
the locations of the two targets. The distortion in the angle-Doppler map is due to the
interaction of the two targets. Specifically, the targets were placed so that one was on the peak
of a sidelobe of the other. When the interference sources were omitted, the SNR was about
21.1 dB for each of the targets. However, when the interference sources were included, the
SINR for each of the targets was –31.1. The noise-only SNR of 21.1 is 3 dB less than the
single target case because of the presence of two targets rather than one.
Figure 16.10 corresponds to the case where the two interference sources were included in
the computation of w. As would be expected, the peaks at the locations of the targets are still
present. However, the weights have altered the angle-Doppler sidelobe structure to place a
null at the angle location of the interference sources that was at the same Doppler frequency
as one of the targets. For this case, the combined SINR at the output of the processor was
about 21.2 dB for target 1 [the target at (0,0)] and 21 dB for the other target, which is about the
same as the noise only case. This indicates that the weight calculation algorithm chose the
weights so that both interference sources were greatly attenuated.
We note that the examples of this section are “academic.” In practice, it is unlikely that
interference would be at only two specific angle-Doppler locations (or that we would want to
place beams on two targets at the same time). More likely, the interference would be a line
through angle-Doppler space. This might be the situation encountered in an airborne radar
application where STAP was used to mitigate ground clutter. We consider this in the next
example.

Figure 16.9 Angle-Doppler map—weights based on only receiver noise—two targets.

Figure 16.10 Angle-Doppler map—two interference sources included in weight computation—two targets.
16.5.2 Example 4
As another example of STAP, we consider the simplified airborne radar problem shown in
Figure 16.11. The aircraft in the center of the concentric circles contains a search radar (e.g.,
AWACS—airborne warning and control system) that is flying at an altitude of 3 km, in the
direction of the arrow, at a velocity of 100 m/s. The target is also at an altitude of 3 km and is
flying in the direction shown at a velocity of 50 m/s. At the time of interest, the angle to the
target is αT = −30°. The range to the target, rT = 10 km.
To simplify the example, we (unrealistically) assume the antenna consists of 16
omnidirectional (isotropic) radiators that are located on the bottom of the aircraft. The array
is oriented along the length of the aircraft and the element spacing is ½ wavelength. The radar
transmits 16 pulses. Thus, the antenna and waveform are consistent with Example 3. We will
use STAP to form a beam and nulls in azimuth-Doppler space. We assume the radar is using
an operating frequency of 3 GHz and a PRI of T = 200 µs. Since we do not need it, we will
leave the pulsewidth unspecified.
The ring of Figure 16.11 represents the ground region illuminated by the radar at the range
to the target (10 km). The radar will also illuminate clutter at ranges of 10 km, plus ranges
corresponding to multiples of the PRI. That is, at ranges of rT + ncT/2, where n is an integer
and c is the speed of light. For this example, we ignore those clutter returns.
Figure 16.11 Geometry for Example 4.
We assume the per-pulse and per-element SNR and SCR are 0 dB and −50 dB, respectively.

Assuming a normalized noise power of PN = 1 W, the normalized signal power is PS = 1 W,
and a normalized interference (clutter) power is PI = 105 W. The powers are defined at the
output of the single-pulse matched filter.
Given that the aircraft altitude is hA = 3 km and the range to the ground clutter is rg = 10
km, the ground range to the clutter annulus is
We can use this, along with VT, to compute the Doppler frequency of the ground clutter as
where we note that ug varies from −1 to 1 as θg varies from 0 to 2π.
Since the aircraft and the target are at the same altitude, we can write the equation for the
target Doppler frequency, at the radar, as
The target is located at (θT, fdT) = (−π/6, −0.5 kHz) in angle-Doppler space.
Rather than being concentrated at point in angle-Doppler space, the clutter is distributed
along a line defined by (16.96). This is illustrated in Figure 16.12, which is a plot like Figure
16.7 with the “beam” in angle-Doppler space steered to (θT, fdT), the target location. The white
line is a plot of (16.96) and the black circle indicates the target location. The brightest square
is the main beam and the other squares are sidelobes. The vertical axis is u = sin(θ) and the
horizontal is frequency, f, in kilohertz (kHz). For this example, we assumed the clutter
spectrum width was zero. In practice, the width will be not be zero because of internal clutter
spectral spread (see Chapter 13) and the aircraft motion. As can be seen, the clutter “line”
skirts the main beam and passes close to the target. We did this intentionally to stress the STAP
algorithm.
In its basic form, the STAP algorithm developed in this chapter is designed to accommodate
only point sources of interference in angle and Doppler. However, we can approximate the
continuous line of Figure 16.12 by a series of closely spaced point sources. We choose the
point sources so that the spacing between them is much less than the angle and Doppler
resolution of the waveform and linear array.

Figure 16.12 Illustration of angle-Doppler plot for interference (white line) and target (black circle), overlaid on the
unoptimized angle-Doppler contour plot.
As a reminder, the Doppler resolution of the waveform is equal to the reciprocal of its
duration, or 1/16T in this case. The angle resolution of the linear array is equal to its length,
which is 16(λ/2) in this case. To satisfy the point source spacing requirement, we represented
the line by 40 point sources. We set the angle spacing between the point sources to the length
of the line (2 sines) divided by 40. We computed the corresponding fdg from (16.96).
To compute the R matrix, we need to form 40 interference, angle-Doppler steering vectors.
The i th angle-Doppler steering vector, D(iΔu, iΔf), is a 16 × 16 = 256 element vector whose
elements are given by
with Δu = 2/40 and Δf = 1,900Δu.
With this, we use (16.81) to form the R matrix as
Finally, we use (16.73), with κ = 1, to find the optimum weight. We use (16.75), (16.76), and
(16.77), with θs = θT and fs = fdT, to find S(θT,fdT). The result of computing the weights and

applying them in the STAP processor is shown in Figure 16.13. Note that there is now a deep
notch where the white line of Figure 16.12 was located. The SINR before optimization was
−31 dB. After optimization, it was 23.5 dB, which is close to the noise limited case of 24.1 dB.
This means the STAP processor has effectively attenuated the clutter. As with Figure 16.12, the
black circle is the target location and the white square is the main beam.
In this example, we knew location of the target in range, angle, and Doppler and we knew
the angle-Doppler distribution of the ground clutter. We also knew the SNR and SCR at the
matched filter output for each antenna element and pulse. In practice, we may not know all of
this. If the radar was conducting search, we would effectively know the range, angle, and
Doppler of interest for each search interrogation. Thus, we would know where we want to
steer the angle-Doppler main beam, which means we can compute S(θT,fdT). However, we may
not know the angle-Doppler distribution or power of the clutter. Without this information, we
could not compute R, and would need to determine it from measurements.
As a note, since we assumed a linear array of omnidirectional elements, when the STAP
algorithm formed an angle-Doppler beam at (θT, fdT) = (−π/6, −0.5 kHz), it formed another
one at (θT, fdT) = (π + π/6, −0.5 kHz). We ignored this second beam.
Figure 16.13 Angle-Doppler contour plot with the optimum weights. The black circle is the target location.
16.6 ADAPTIVITY AGAIN
In our work so far, we assumed we knew the various parameters needed to compute the

optimum weights. In particular, we assumed we had enough information to compute the R
matrix. In most applications, this is not the case, and we must estimate R through
measurements. This is part of the adaptive part of STAP: that the environment is probed and
the results are used to experimentally formulate the R matrix. A potential procedure for doing
this follows.
For each antenna element (T/R module) and pulse, we sample the combined noise and
interference in range cells we believe contain the interference but not the target.6 We then use
the samples to estimate R. Specifically, if we write the combined noise and interference
voltage on a particular sample as VlN + I, we can form an estimate of R as
where L is the number of samples taken. As a point of clarification, it should be noted that Vln
+ I is a KN element vector.
A question that arises is: how large does L need to be? If L = 1, we will be multiplying a KN
element vector by its Hermitian to produce an KN by KN matrix. This matrix will have a rank
of 1 since it was formed as the outer product of two vectors and thus has only one independent
column. This means that 
 has only one nonzero eigenvalue, is thus singular, and 
 does
not exist. Because of this, solving for w by the previous method will not work.
Given VlN + I consists of random variables, there is a chance that 
 will have a rank equal
to L (for L ≤ KN). Thus, to have any chance of obtaining a 
 that is nonsingular, at least KN
samples of VlN + I must be taken. As L becomes larger, R will converge to reasonable
approximation of R, and will be nonsingular. A relation that gives an idea of how large L must
be is [11]
In this equation, ρ is the ratio of achievable SINR with 
 to the SINR improvement when the
actual R is used. For L = KN
which says that the SINR improvement actually achieved will be significantly less than the
theoretical SINR improvement possible with the actual R. As a specific example, in Examples
3 and 4, KN = 256. Thus, the expected SINR based on 256 samples of VlN+I will be 2/257 or
about 21 dB below the optimum SINR improvement. If we increase L to 2KN or 512 samples
we would get

Thus, the expected SINR improvement based on 
 would be about 3 dB below the optimum
SINR improvement. However, we note that this represents a large number of samples, which
will require extensive time and radar resources. Also, for the aircraft case of Example 4, the
environment would change before the STAP algorithm could gather enough samples to form
the 
 matrix. We will briefly address this in the next section.
16.7 PRACTICAL CONSIDERATIONS
In practice, it may be possible to use fewer samples of VlN + I if we have a reasonable estimate
of the receiver noise power. We would use the aforementioned approximation to form an
estimate of RI, the interference covariance matrix. If we term this estimate 
, we would form 
 from
where PN is the receiver noise power estimate (per antenna element and pulse). This approach
is termed diagonal loading [11, 22, 23]. Adding the term PN I ensures that 
 will be positive
definite and that 
 exists.
With this method, the number of samples, L, can theoretically be as small as the anticipated
number of interference sources [11]. Note that this will generally be much smaller than KN.
This method can have problems in that sometimes 
 can become ill-conditioned [14],
which can cause the optimization to put nulls in the wrong locations. To circumvent this
problem, it may be necessary to use more samples in the computation of 
 and/or artificially
increase PN. Taking more samples is problematic because this requires an extra expenditure
of time and radar resources. However, increasing PN will cause the SINR improvement to
degrade, potentially to unacceptable levels.
For the aircraft clutter problem, it may be possible to use aircraft information such as
altitude or velocity to form somewhat of an analytical estimate of the clutter distribution over
angle-Doppler space. Still another approach suggested in [24] is somewhat of an extension of
the method used in sidelobe cancellation. Specifically, a portion of the array would be used to
gather data and another portion would be used in the actual STAP algorithm. This would
reduce the degrees of freedom available to the STAP algorithm, but it may make it possible to
afford clutter rejection that could be obtained by other means.
More information about these and other practical aspects of STAP can be found in [3,
11–13, 24].
16.8 EXERCISES

1.
Show that (16.1) follows from (16.4).
2.
Derive the form of (16.11). Specifically, show that Rn is a diagonal matrix.
3.
Derive (16.17).
4.
Derive (16.24). Specifically, explain why the double sum reduces to a single sum.
5.
Derive (16.28) starting with (16.27).
6.
Implement a spatial optimization algorithm and generate the plot of Example 1.
7.
Repeat Exercise 6 with interference 1 located at 4° instead of 18°. This places the
interference slightly more than ½ beamwidth from the target. You will note that the
algorithm places a null in the main beam and moves the peak of the mainbeam slightly
off of the target.
8.
Derive (16.38) using (16.36) and (16.37).
9.
Implement a temporal optimization algorithm and generate the plot of Example 2.
10. Implement a space-time optimization algorithm and generate the four plots of Example 3.
11. Repeat Exercise 10 with the second target located at (θs2,fd2) = (34°, –217 Hz). Note the
difference in the angle-Doppler maps when compared to Figures 16.9 and 16.10.
References
[1]
Melvin, W. L., “A STAP Overview,” IEEE Aerosp. Electron. Syst. Mag., vol. 19, no. 1, Jan. 2004, pp. 19–35.
[2]
Brennan, L. E., and I. S. Reed, “Theory of Adaptive Radar,” IEEE Trans. Aerosp. Electron. Syst., vol. 9, no. 2, Mar.
1973, pp. 237–252.
[3]
Klemm, R., ed., Applications of Space-Time Adaptive Processing, London, UK: Institute of Engineering and Technology,
2004.
[4]
Sjogren, T. K. et al., “Suppression of Clutter in Multichannel SAR GMTI,” IEEE Trans. Geosci. Remote Sens., vol. 52,
no. 7, Jul. 2014, pp. 4005–4013.
[5]
Wei, L. et al., “Application of Improved Space-Time Adaptive Processing in Sonar,” 2012 Int. Conf. Comput. Sci.
Electron. Eng. (ICCSEE), vol. 3, Hangzhou, China, Mar. 23–25, 2012, pp. 344– 348.
[6]
Ahmadi, M., and K. Mohamed-pour, “Space-Time Adaptive Processing for Phased-Multiple-Input–Multiple-Output
Radar in the Non-Homogeneous Clutter Environment,” IET Radar, Sonar & Navigation, vol. 8, no. 6, Jul. 2014, pp.
585–596.
[7]
Dan, W., H. Hang, and Q. Xin, “Space-Time Adaptive Processing Method at Subarray Level for Broadband Jammer
Suppression,” 2011 IEEE Int. Conf. Microwave Tech. & Comput. Electromagnetics (ICMTCE), Beijing, China, May 22–
25, 2011, pp. 281–284.
[8]
Zei, D., et al., “Real Time MTI STAP First Results from SOSTAR-X Flight Trials,” 2008 IEEE Radar Conf., Rome,
Italy, May 26–30, 2008, pp. 1–6.
[9]
Paine, A. S. et al., “Real-Time STAP Hardware Demonstrator for Airborne Radar Applications,” 2008 IEEE Radar
Conf., Rome, Italy, May 26–30, 2008, pp. 1–5.
[10]
Nohara, T. J. et al., “SAR-GMTI Processing with Canada’s Radarsat 2 Satellite,” IEEE 2000 Adaptive Syst. Signal
Proc., Commun., Control Symp., Lake Louise, Alberta, Canada, Oct. 1–4, 2000, pp. 379–384.
[11]
Guerci, J. R., Space-Time Adaptive Processing for Radar, 2nd ed., Norwood, MA: Artech House, 2014.
[12]
Klemm, R., Principles of Space-Time Adaptive Processing, 3rd ed., London, UK: Institute of Engineering and
Technology, 2006.

[13]
Klemm, R., Space-Time Adaptive Processing: Principles and Applications, London, UK: Institute of Engineering and
Technology, 1998.
[14]
Franklin, J. N., Matrix Theory, Englewood Cliffs, NJ: Prentice-Hall, 1968.
[15]
Urkowitz, H., Signal Theory and Random Processes, Dedham, MA: Artech House, 1983.
[16]
Shores, T. S., Applied Linear Algebra and Matrix Analysis, New York: Springer, 2007.
[17]
Widrow, B., et al., “Adaptive Antenna Systems,” Proc. IEEE, vol. 55, no. 12, Dec. 1967, pp. 2143– 2159.
[18]
Nitzberg, R., Adaptive Signal Processing for Radar, Norwood, MA: Artech House, 1992.
[19]
Nitzberg, R., Radar Signal Processing and Adaptive Systems, Norwood, MA: Artech House, 1999.
[20]
Manolakis, D. G., et al., Statistical and Adaptive Signal Processing: Spectral Estimation, Signal Modeling, Adaptive
Filtering, and Array Processing, Norwood, MA: Artech House, 2005.
[21]
Golan, J. S., The Linear Algebra A Beginning Graduate Student Ought to Know, 2nd ed., New York: Springer, 2012.
[22]
Gabriel, W. R., “Using Spectral Estimation Techniques in Adaptive Processing Antenna Systems,” IEEE Trans. Antennas
Propag., vol. 34, no. 3, Mar. 1986, pp. 291–300.
[23]
Carlson, B. D., “Covariance Matrix Estimation Errors and Diagonal Loading in Adaptive Arrays,” IEEE Trans. Aerosp.
Electron. Syst., vol. 24, no. 4, Jul. 1988, pp. 397–401.
[24]
Wirth, W. D., Radar Techniques Using Array Antennas, London, UK: IEE Press, 2001.
1 We will restrict the development to linear arrays as a convenience. The extension to a planar array is reasonably
straightforward.
2 Consistent with other developments in this book, we are using complex signal notation as a convenient means of representing
the amplitude and phase of RF or IF signals.
3 In Chapter 12, we used W instead of WH. We made the switch here to be more consistent with the notation used in STAP.
4 As a caution R(θ) is the radiation pattern, which is not to be confused with the covariance matrix, R [without (θ)].
5This carries the assumption that the radar is operating unambiguously in range.
6 In practice, we can allow the range cells to contain the target return if the overall SNR and SIR (signal-to-interference ratio)
is very small for each antenna element and pulse.

Chapter 17
Sidelobe Cancellation
17.1 INTRODUCTION
Sidelobe cancellation (SLC) is similar to STAP, or more accurately SAP (spatial adaptive
processing), in that it is aimed at removing interference. With SAP, also called adaptive
nulling and adaptive beam forming, the antenna properties are changed to place a null in the
radiation pattern at the angular location of the interference. A sidelobe canceller does not
modify the main antenna. Instead, it attempts to subtract the interference from the main
antenna output by using signals from auxiliary antennas. Since SAP modifies the antenna
radiation pattern, it can only be implemented, in its pure form, for phased array antennas
where the output of each element, or a reasonably large number of subarrays, is available for
manipulation. As a result, SAP will not work on reflector antennas or space-fed phased array
antennas. Since SLC does not modify the radiation pattern of the antenna, it can be used with
all types of antennas.
The design criteria for SAP and SLC are also different. The SAP criterion is based on
maximizing SINR, while SLC is based on minimizing the interference at the SLC output. The
basic SLC design methodology is an application of Wiener filtering, which is the theory used
in communication systems for mitigation of multipath and other types of interference signals
[1–4].
SLC is designed to operate against active electronic attack (EA) devices (jammers) and not
against clutter or passive interference such as chaff. It is usually assumed the EA signal is
noise-like with a bandwidth that exceeds the IF bandwidth of the radar receiver. However, this
is not a requirement, and an SLC can cancel narrowband noise. We note, however, that the
time required for the SLC to gather sufficient noise data is inversely proportional to the noise
bandwidth at the point where the noise data is obtained. This creates the possibility that the
SLC may not be able to gather the noise data needed to cancel it. It is also usually assumed the
EA signal is entering the radar antenna through one of the sidelobes of the main antenna
radiation pattern. The fact that SLC cancels interference entering the radar through the main
antenna sidelobes is believed to be the origin of the term sidelobe cancellation.
Paul W. Howells invented the sidelobe canceller in the 1960s and was awarded a patent for
it on August 24, 1965 [5]. Shortly thereafter, Sydney P. Applebaum published a classified
report on his analysis of Howells’ SLC [6]. Since that time, the SLC implementation invented
by Howells has usually been called the Howells-Applebaum SLC.
The original Howells-Applebaum SLC was an analog, closed-loop, servomechanism
device, which was later implemented as digital loops. With the advent of high-speed analog-
to-digital converters and high-speed digital signal processors, SLCs have evolved into open-
loop, digital implementations. Both types are discussed in this chapter. We begin by discussing

interference cancellation, which is the theory upon which SLC is based. We then describe the
open-loop implementation of the SLC. We next discuss SLC weight computation via the
gradient technique and use it to derive the Howells-Applebaum form of the SLC. Finally, we
close the chapter with a brief discussion of sidelobe blanking (SLB). While SLC attempts to
cancel interference, the sidelobe blanker simply turns off the receiver when it determines that
interference is obscuring the desired return.
17.2 INTERFERENCE CANCELLER
Figure 17.1 contains a functional block diagram of the interference canceller we will
consider. The top antenna represents the main antenna of the radar and the bottom antenna is
an auxiliary antenna used to gather information on the interference signal. The block with w*
(t) is a gain, or weight. The arrow through the box indicates that the weight is adjusted based
on the error voltage, ve(t). The error voltage is formed by subtracting a weighted version of
the auxiliary channel signal, va(t), from the main channel signal, vm(t). In equation form1
where * denotes the complex conjugate. The error signal is sent to the rest of the radar
receiver and signal processor.
If the interference canceller is working correctly, ve(t) will contain only echoes received
through the main beam of the main antenna, which we will consider to be the desired, or
target, echoes.2 Indeed, suppose vm(t) consists of a desired signal, vs(t), and an interference
signal, vI(t). That is,
Figure 17.1 Interference canceller block diagram.
The auxiliary channel signal, va(t), also consists of vs(t) and vI(t) but in different proportions.
That is

Suppose we are able to choose the weight, w*(t), as
With this we get
Thus, as hoped, the error signal consists of a scaled version of the desired signal and no
interference signal. This tells us the configuration of Figure 17.1 has the potential of
accomplishing the desired objective and gives us incentive to develop a more practical
algorithm.
17.3 INTERFERENCE CANCELLATION ALGORITHM
We first derive an algorithm for cancellation of a single interference signal and then extend it
to the case where there are multiple interference signals.
17.3.1 Single Interference Signal
We assume both the desired and the interference signals, vs(t) and vI(t), are complex random
processes. We need this assumption because the interference signals of interest are noise-like,
and we have established that we should treat target return signals as random processes. We
consider them as complex because they have random amplitudes and phases. We further
assume vI(t) is zero-mean and wide-sense stationary (WSS). We assume vs(t) is zero-mean,
but we cannot assume it is WSS because, in general, it is a pulsed signal. Since vs(t) and vI(t)
are complex, zero-mean random processes, so are vm(t) and va(t). We assume vs(t) and vI(t)
are independent. As a note, vm(t) and va(t) will also contain components due to receiver noise.
We will ignore the receiver noise for now, but will consider it in the more general
development of Section 17.3.3.
Since vs(t), vI(t), vm(t), and va(t) are zero-mean, complex random processes, for some t =
t1, vs(t1), vI(t1), vm(t1), and va(t1) are zero-mean, complex random variables. We will denote
these as vs, vI, vm, and va. Also, because of the WSS assumption, the mean-square value, or
power, of vI(t1) is independent of t1.
We define the error voltage at t = t1 as

We note that ve is zero-mean.
We now define a criterion for determining w. In STAP (Chapter 16), we used maximization
of SINR as the design criterion. For the SLC, we use minimization of the mean-square value
of ve as the criterion. In equation form, we choose w according to
We use the magnitude of ve because it is complex; we use the expected value, E{x}, because ve
is a random variable; and we use the square because it is reasonably easy to work with.
Equation (17.7) is termed a least mean-square (LMS) criterion that appears in Wiener filter
theory [1, 3, 7].
From Wiener filter theory, a necessary and sufficient condition for w to minimize the
mean-square error is to choose it so that
The symbol ∇ denotes the gradient operator.
It is easily shown that (17.8) (see Exercise 1) reduces to
Equations (17.8) and (17.9) are also known as the orthogonality condition. They tell us the
optimum weight, wopt, is chosen so that the error signal, ve, is orthogonal, in a statistical
sense, to the auxiliary channel signal, va.
Solving (17.9) for wopt gives
which is one form of the Wiener-Hopf equation [1–3, 8].
17.3.2 Example 1
To illustrate the procedure of Section 17.3.1, we consider an example using (17.1) through
(17.3). Using (17.2) and (17.3), we get

where we made use of the assumption that vs(t) and vI(t) are independent. In (17.11), Ps(t1) is
the power (or energy) of the desired signal at t = t1 and PI is the power (or energy) of the
interference signal.3 The powers are measured at the point where the SLC is implemented.
This could be before or after the matched filter. We left the time parameter on vs because we
will discuss this time dependency later in this example.
The denominator of (17.10) is
Combining (17.10) through (17.12) gives
If we assume PI ≫ Ps(t1) (the interference signal is much larger than the desired signal, at the
faces of the main and auxiliary antennas), (17.13) reduces to
which is the solution we postulated in Section 17.2.
If Ps(t1) ≫ PI, (17.13) reduces to
Substituting this into (17.1) gives the disturbing result

Equation (17.16) says that if we compute the weight based on data at the time the desired
signal is present, and if the desired signal is much larger than the interference signal, the SLC
will cancel the desired signal and pass the interference.
This result leads to the observation that, if possible, the weight, wopt, should be based on
measurements obtained when the input to the main and auxiliary antennas contains only the
interference signal. This may be possible if the samples are obtained shortly before the
transmit pulse since it is it is unlikely that any desired echo signals will be present at this time.
As another extension of this example, we examine the case where there are two independent
interference sources, vI1(t) and vI2(t), with powers PI1 and PI2. We assume PI1 ≫ Ps and PI2 ≫
Ps. For this example, we have
and
With this, we get
and
The optimum weight is

or with the assumptions PI1 ≫ Ps and PI2 ≫ Ps
We note that wopt(t1) is a function of not just K2 and K4 as in (17.14), but also K5, K6, PI1,
and PI2. If we substitute this into (17.6), we get
It is not clear if either vI1(t) or vI2(t) will be canceled or even reduced. Also, the impact of
the SLC on both the signal and interferences will depend on the interference powers. This
leads to the observation made by Applebaum in his original SLC analysis [6] that the SLC
may not be able to cancel all interference signals if the number of interferences exceeds the
number of auxiliary channels.
Before we discuss SLC performance further, we will extend the development to the case of
multiple interferences and multiple auxiliary channels. We will also add noise to va(t) and
vm(t).
17.3.3 Multiple Interference Sources
Figure 17.2 contains a functional block diagram configuration for multiple interferences and
multiple auxiliary channels. We assume one main channel, N auxiliary channels, and K
interference sources. The output of each auxiliary channel, van(t), is multiplied by a weight,
wn*. The results are summed and subtracted from the main channel signal, vm(t), to form the
error signal, ve(t). The equation for ve(t) is
where
and

Figure 17.2 Multiple channel, multiple interference SLC problem.
The superscripts H and T denote the conjugate-transpose (Hermitian) and transpose,
respectively.
As before, the design criterion is minimization of the mean-square error. That is,
where, as before, ve = ve(t1), vm = vm(t1), and so forth. Extending (17.8) to the vector case
results in
The gradient, ∇J, is
Using (17.29) in (17.28), with (17.27), results in

which we can solve to give
where
and
R is a covariance matrix much like the one discussed in Chapter 16.
17.4 IMPLEMENTATION CONSIDERATIONS
Now that we have a general formulation of the SLC equations, we consider topics we need to
analyze and model an SLC.
17.4.1 Form of vm(t) and va(t)
From Figure 17.2, we note that the signal in the main channel, vm(t), and each of the auxiliary
channels, van(t), are functions of the desired signal, vs(t), and all of the interference signals,
vIk(t). As indicated earlier, they will also include a noise component. With this we can write
where Am(utgt,vtgt) is the (complex) “voltage directivity” of the main antenna in the direction
of the desired signal and
is a vector of voltage directivities of the main antenna in the directions of the K interference
sources. The A(u,v) are given by
for an array with rectangular packing (see Chapter 12). (u,v) is the location of the desired
signal source or interference, as appropriate, and (u0,v0) is the direction to which the beam is

steered. These were the functions used to compute the radiation pattern [i.e., R(u,v) = |A(u,v)|2]
in Chapter 12.
vI(t) is a vector of interference signals represented by
and nm(t) is the noise in the main channel.
va(t) is a vector of auxiliary channel signals and is given by (17.19). Each of the auxiliary
channel signals is of the form
In (17.38), nan(t) is the noise in the nth auxiliary channel and Aan(utgt,vtgt) is the voltage
directivity nth auxiliary channel antenna in the direction of the desired signal source. Aan is a
vector of voltage directivities in the directions of the interference sources. It has the same
form as Am. That is,
The Aan(u,v) contain a phase that depends on the pointing angle to the kth interference
source, (uk,vk), and the location of the nth auxiliary antenna relative to the main antenna. If the
location of the center (phase center) of the nth auxiliary antenna relative to the center (phase
center) of the main antenna is (xn, yn, zn), this phase is (see Appendix 17A)
The Aan(u,v) also contain a phase that accounts for any inherent phase shifts of the main and
auxiliary channels. This carries the tacit assumption that the main and auxiliary channels are
calibrated so that the phase can be determined. If the SLC determines R and η from
measurements, all of the phases of the Aan(u,v), and the Am(u,v) will be accounted for by the
measurement process and thus do not need to be known.
The va(t) vector is given by
where

and vI(t) is given by (7.37).
17.4.2 Properties of vs(t), vI(t), nm(t), and nan(t)
The standard assumption is that the interference sources are independent and generate noise-
like signals. Thus, we assume the elements of vI(t) are independent, zero-mean, WSS, random
processes. We also assume the receiver noises are zero-mean, WSS, random processes and
the elements of vI(t) and the receiver noises are mutually independent. With this, the
covariance matrix of vI(t) is
That is, RI is a diagonal matrix of the interference powers. Also Pnm = E{|nm(t)|2} and Pnan =
E{|nan(t)|2} are the receiver noise powers. The fact that these powers are represented by
constant values is due to the WSS assumption. We collect the noise powers of the auxiliary
channels into an auxiliary channel noise covariance matrix that we write as

The power in the desired signal voltage is Ps(t) = E{|vs(t)|2}. This power is not constant
because vs(t) may or may not be present at the time of interest (the time when the weights are
computed). vs(t) is independent of vI(t), nm(t), and nan(t).
17.4.3 Scaling of Powers
To be able to simulate and analyze an SLC, the various powers indicated in Section 17.4.2
must be specified. To avoid the difficulty of directly specifying the various powers, we will
work with SNR and interference-to-noise ratio, JNR.4 We suggest the procedure outlined
below. This procedure is based on the assumption that the main and auxiliary channel
receivers use a matched filter and that the SLC is implemented after the matched filter.
• Compute the SNR from the radar range equation.
• Compute the JNR of each interference source at the radar main antenna from
where (Pik/Bik) is the effective radiated energy of the kth interference source, and Rk is the
range to the kth interference source. GR is the receive directivity of the main antenna, LI
captures the receive losses in the main channel associated with the interference, and Fn is
the noise figure of the main channel receiver.5
• Normalize Am so its magnitude at (u0,v0) is unity.
• Scale the Aan so their magnitudes at some (u0an,v0an) are at some level relative to unity,
and account for any difference in receive losses between the main and auxiliary channel.
This is a somewhat standard way of specifying the directivities of the auxiliary antennas.
That is, their directivities are often specified as being a certain number of dB below the
main antenna directivity. The directivity of the auxiliary antennas should be above the
sidelobe levels of the main antenna to prevent the SLC from significantly raising the noise
floor of the main channel.
• Set Pnm = 1 W. This, along with the use of SNR and JNR, means that all of the powers are
normalized relative to a main channel noise power of 1 W at the matched filter output.
• Compute the auxiliary channel noise powers from Pnan = (Fan/Fn) where the Fan are the
system noise figures of the auxiliary channels. This allows for different noise powers in
the various receivers.
• Compute the signal and interference powers using Ps = SNR and PIk = JNRk.
17.4.4 Example 2
To illustrate the previously discussed procedure, we consider an example where we have two
interference sources and two auxiliary channels. For the example, we consider a 16-element

linear array with uniform weighting. The element spacing is d = λ/2, making the total length
of the array 15λ/2. We assume the beam is steered to u = 0. We assume the center of the array
is located at x = 0.
The two auxiliary antennas are located at x1 = −10λ and x2 = 12λ. We assume both auxiliary
antennas are isotropic radiators with a normalized directivity of −15 dB relative to the peak
directivity of the main channel antenna. The noise figures of the two auxiliary channel
receivers are the same as the noise figure of the main channel receiver. The main and
auxiliary channels use matched filters, and the SLC is implemented after the matched filter.
The two interference sources are located at u1 = sin(18°) and u2 = sin(−34°), and their JNRs
are JNR1 = 40 dB and JNR2 = 50 dB. The desired signal source (the target) is located at utgt = 0
and the SNR is 20 dB.
Figure 17.3 contains a depiction of the antenna geometry, and the various parameters are
listed in Table 17.1.
Figure 17.3 Antenna geometry for Example 2.
Table 17.1
Parameters for Example 2
u1 = sin(18°), JNR1 = 40 dB
u2 = sin(−34°), JNR2 = 50 dB
utgt = 0, SNR = 20 dB
Pnm = Pna1 = Pna2 = 1 W
Ps = 102 W, PI1 = 104 W, PI2 = 105 W

φ1,tgt = 4π(−10λ/λ)(0) rad, φ11 = 4π(−10λ/λ)u1 rad, φ12 = 4π(−10λ/λ)u2 rad
φ2,tgt = 4π(12λ/λ)(0) rad, φ21 = 4π(12λ/λ)u1 rad, φ22 = 4π(12λ/λ)u2 rad
Am = 1, Am = [0.1334 0.0809]
Aa1 = Aa11 = Aa12 = 10−15/20, Aa2 = Aa21 = Aa22 = 10−15/20 (Constant gain aux antennas)
We assume the weight, wopt, is calculated before the transmit pulse and that there is no
desired signal present. With this, we get (see Exercise 2)
where, from (17.44),
from (17.46), (17.47), and (17.35)
and Am is given in Table 17.1.
The resulting weight vector is, from (17.31)
A standard measure of the performance of an SLC is the cancellation ratio (CR), which is
defined as the total interference power in ve if the SLC was not present (w = 0) divided by the
total interference power when the SLC is active [6, 7, 9]. By total interference power, we mean
the combined power of the interference sources and the receiver noise. In equation form

With the terms delineated earlier, this reduces to
Another measure of performance is a comparison of the SINR without the SLC (i.e., w = 0)
and the SINR with the SLC. We can compute the SINR without the SLC as (see Exercise 2)
This low value of SINR is due mainly to the interference source at −34°. It has a JNR of 50 dB
that is attenuated by the −22 dB sidelobe (see Figure 17.3). This alone would result in an SINR
of −8 dB (SNR − JNR − SLL = 20 − 50 + 22). The remaining −1.2-dB degradation is due to the
other interference source and the main channel noise.
With the SLC, the SINR is
The SINR at the output of the SLC is close to the SNR of 20 dB specified in the problem
definition. The SLC resulted in a signal power increase of about 2 dB and a noise power
increase of about 3.3 dB. This interesting coincidence meant the SNR (exclusive of the
interference sources) went down by about 1.3 dB, which means the SLC was quite effective at
removing almost all of the interference due to the interference sources.
The increase in noise power is due to the last two terms in the denominator of (17.58).
These terms tell us the overall noise level will be equal to the main channel receiver noise
plus some portion due to the noise in the auxiliary channels. In this particular example, the
magnitude of the weight vector was such that the auxiliary channels added a noise power
slightly different than the noise power of the main channel (about 1.2 W versus the main
channel noise of 1 W).

The reason the auxiliary channel noises did not add much to the overall receiver noise was
because the directivity of the auxiliary channel receivers was greater than the directivity of the
main antenna sidelobes containing the interferences (see Figure 17.3). Had the directivities of
the auxiliary antennas been below the sidelobe levels, the SLC weights would have had a
magnitude greater than unity. This would have amplified the noises in the auxiliary channels
and caused the overall receiver noise to increase substantially. This is considered further in
Exercise 7.
17.4.5 Practical Implementation Considerations
While the methods discussed in Sections 17.4.1 through 17.4.4 are suitable for analyzing
sidelobe cancellers, they cannot be directly used in an actual SLC implementation because the
various parameters (e.g., Table 17.1) are not known a priori. As a result, the various expected
values must be estimated based on measurements of vm(t) and va(t). Strictly speaking, the
expected values are ensemble averages and cannot be evaluated from a single set of vm(t) and
va(t) measurements. To obtain a valid ensemble average, we would need to average across
many radars, desired signals, environments and interference sources (all of the same type and
in the same location) to obtain a true ensemble average. Clearly this is not possible since we
have only one radar, etc. To get around this problem, we invoke the concept of ergodicity [10,
11]. This concept states that, if a random process is ergodic, ensemble averages can be
replaced by time averages. Proving that a process is ergodic is very difficult, if not
impossible. However, it is a standard assumption as long as one is confident that the processes
are at least WSS.
We will assume the measurements are made right before the transmitted pulse. This is
necessary to ensure the interference and receiver noises will satisfy the WSS restriction. For
phased array antennas, we impose the additional constraint that the measurements are made
after the main and auxiliary beams have been steered to their new location and after any local
oscillators and such have been retuned.
Figure 17.4 contains a possible timing diagram illustrating how the SLC power estimation
and weight computation would fit into the overall radar timeline. As shown, time is allotted at
the end of a PRI for (1) frequency retuning, (2) beam steering, (3) SLC power estimation, (4)
SLC weight computation, and (5) receiver noise measurement (for AGC or detection
threshold determination, not SLC). For a high-PRF burst waveform, there is not sufficient
time before every pulse to compute SLC weights, so they are computed before the burst and
held throughout the burst.
Actually, if the radar performs coherent processing (MTI, pulsed Doppler, coherent
integration) the weights are usually computed and held constant for the coherent processing
interval (CPI). If the weights were computed before each pulse, they could affect the pulse-to-
pulse phase characteristics of the main channel target and clutter signals, which could degrade
the clutter rejection and/or SNR improvement of the signal processor.

Figure 17.4 SLC timing diagram.
The frequency retuning and beam steering operations could be reversed; however, both
must be performed before the SLC weight computation. The frequency retuning must precede
the SLC weight computation because it affects the phase shifts in the main and auxiliary
channels. The beam steering must precede the SLC weight computation because it establishes
the main antenna sidelobe levels and the directivities of the auxiliary antennas (both amplitude
and phase). The noise measurement is made after the SLC because the SLC will affect the
noise floor in the main receiver.
We further assume there is no desired return signal, only interference and receiver noise.
We assume the interference and receiver noise samples are mean and autocorrelation ergodic
[10, 11]. If we assume a digital implementation of the SLC and use samples of va(t) and vm(t),
ergodicity tells us we can estimate R and η using
and
where va(l) and vm(l) are samples of the auxiliary and main channel signals. We then use these
to form the weight estimate as
We use this weight estimate throughout the PRI, or CPI if the radar performs coherent
processing of the desired return signals. That is, for all range cells in the PRI or CPI, we use

where m is the range cell index.
The idea that we can use the same weight throughout the PRI or CPI is a consequence of the
WSS assumption.6 This means that 
 and , and thus ŵ, are constant.
The method of determining the SLC weight based on estimates of R and η is termed the
sample matrix inversion (SMI) technique [9, 12–15]. Its name derives from the fact that the
weights are found using (17.61), which involves the inversion of a matrix based on samples
of va(l).
As with STAP, there is a question of how many samples are needed to obtain a reliable
estimate of R and η. If the SLC has N auxiliary channels, va(l) will have N elements and 
 will
be an N-by-N matrix. Thus, the minimum number of samples needed is N. Otherwise, 
 will
be singular. As indicated in Chapter 16, Nitzburg and Reed [4] and Mallet and Brennan [13]
developed an efficiency parameter for STAP. That parameter indicated how the SINR
improvement using a SMI approach would deviate from some theoretical SINR based on
complete knowledge of the system, desired signal, interference, and noise. If we adapt that
parameter to the SLC case, we would have
As an example, if we had N = 3 auxiliary channels and used Lsamp = 3, we would get
and would expect an improvement that is about 3 dB less than theoretical. Doubling the
number of samples would increase this to about 1.5 dB less than theoretical. This assumes
there will be inaccuracies only in 
. There will also be inaccuracies in 
 because of the
limited number of samples. Because of this, additional samples will be needed to account for
the measurement of vm(l).
The spacing between samples should be equal to, or greater than, the inverse of the
bandwidth of the interference signal(s) at the point where the interference power is computed.
For interference whose bandwidth is greater than the bandwidth of the receiver components
up to where the interference power is measured, the sample spacing should be the inverse of
that bandwidth. If the interference (and noise) powers are measured after the matched filter,
the spacing between the samples should be the larger of the inverse of the waveform
modulation bandwidth or the inverse of the interference bandwidth. This will ensure the
samples are uncorrelated and thus that the estimate will not have a bias.
This bandwidth requirement can have an impact on how much of the radar timeline is

allocated to SLC. If the SLC is to be able to counter narrowband interference, a significant
amount of time needs to be allotted to the power estimation phase of the SLC weight
computation. With modern hardware, the weight computation should be fairly quick, possibly
in the order of microseconds.
17.4.6 Example 3
To investigate the relation between Lsamp and expected SLC performance, the SMI technique
of Section 17.4.5 was implemented for the system of Example 2. Lsamp was varied from 2 to
40. Lsamp samples of va(l) and vm(l) were used to compute R and  using (17.59) and (17.60).
These were then used in (17.61) to compute ŵ. Next, one more sample of va(l) and vm(l) was
chosen to compute ve(l) from (17.62). These were used to compute the powers indicated in the
numerator and denominator of (17.55) (in all cases vs(l) was set to zero since we wanted to
compute the cancellation ratio). Finally, these powers were averaged over 100,000 Monte
Carlo runs and used to compute CR from (17.55).
Figure 17.5 contains a plot of CR versus Lsamp. According to (17.63), it was expected that ρ
would be (2 − 2 + 2)/(2 + 1) = 2/3 for Lsamp = 2. With this, the CR should have been about 1.8
dB below the theoretical value of 26 dB [see (17.56)]. Clearly this did not happen. However, as
Lsamp increased, the SMI method did give an ultimate cancellation ratio very close to the
theoretical value. Based on this one example, it would seem that (17.63) should be considered
only as a guide to how large Lsamp should be. To reiterate a previous statement, (17.63) was
derived for a STAP application and not for a SLC application.
Figure 17.5 indicates the CR is within 1 dB of its theoretical value with Lsamp = 10. If the
SLC was designed to handle a broadband jammer and had the samples been taken at the output
of a matched filter matched to a 1-µs pulse, 10 µs would be needed to gather the interference
samples needed to compute 
 and . However, if the SLC were to have the requirement that it
cancel interference with a bandwidth of 100 kHz (and the samples were taken at the matched
filter output), a 100-µs data gathering period would be needed since the samples would need
to be spaced 1/(100 kHz) = 10 μs apart.

Figure 17.5 Cancellation ratio versus Lsamp.
17.5 HOWELLS-APPLEBAUM SIDELOBE CANCELLER
The SMI methodology discussed in Section 17.4 requires the use of high-speed digital
processors to compute 
 and  and solve for ŵ. Such processors are available to modern
radar designers but were not available to radar designers in earlier years of SLC. Designers
of those radars had to use an analog SLC. Most used the Howells-Applebaum SLC or a
modification thereof.
17.5.1 Howells-Applebaum Implementation
The weight calculation technique upon which the Howells-Applebaum SLC is based is termed
a gradient search technique [1, 3, 7, 16]. The gradient search technique is also sometimes
termed the LMS technique. It is used extensively for interference mitigation in
communications equipment and in other applications such as noise canceling headphones. The
gradient search technique iteratively computes weights to eventually minimize the mean-
square error
In the implementation of the technique, the expected value is approximated by the simple
square error, or
The gradient algorithm is given by the equation

In (17.67), ∇e is the gradient of the error evaluated at wk and is given by
Basically, the gradient is used to update the latest estimate by adding a correction that is
proportional to the negative of the slope, or gradient, of the error evaluated at the latest
estimate. This is illustrated in Figure 17.6. In this figure, wk > wopt and we note the slope is
positive. We also note we want wk+1 to be less than wk if we are to move toward wopt. Thus, we
see that we want to move in a direction that is opposite to the sign of the slope. With some
thought, we also note that if wk is far away from wopt, we would like to change wk by a large
amount, whereas if wk is close to wopt, we want to change wk by a small amount. Thus, the
amount of change is related to the magnitude of the slope. This is what the algorithm of
(17.67) does.
The parameter μ controls the rate at which the estimate approaches wopt. If μ is small, wk
will approach wopt in small steps; if μ is large, wk will approach wopt in large steps. If μ is too
small, convergence will be very slow. However, if μ is too large, the solution could diverge.
Thus, choosing μ is one of the important aspects of implementing a Howells-Applebaum SLC.
Equation (17.67) is a difference equation. However, early Howells-Applebaum SLCs were
implemented in the continuous time domain. We can convert (17.67) to a differential equation
of the form
where we have made use of (17.68). We changed the parameter μ to μc to denote it is different
for discrete-time and continuous-time implementations.
Figure 17.6 Illustration of gradient technique.

Figure 17.7 Functional block diagram of a Howells-Applebaum SLC.
Equation (17.69) also contains another subtle change relative to (17.67). Specifically, in
(17.69), we allow the error signal and auxiliary channel signal to change with time as the
weight is being updated. In (17.67), we used one sample of the error signal and auxiliary
channel signal to iterate on the weight. Allowing the signals to change incorporates averaging
into the SLC loop.
If we represent (17.69) as a block diagram, we have the functional block diagram of the
Howells-Applebaum SLC shown in Figure 17.7.
17.5.2 IF Implementation
The Howells-Applebaum loop is sometimes implemented at some IF. As such, the lower
multiply of Figure 17.7 is generally performed by a mixer, whereas the upper multiply is a
variable gain amplifier. The vm(t), va(t), and ve(t) are IF signals, while w*(t) is a baseband
signal. The conjugation on the right side (the block with * in it) is implemented as a 90º phase
shift. The block diagram of Figure 17.7 uses complex signal notation. In an actual
implementation, quadrature signals are used to capture the operations implied by the complex
signal notation.
An example block diagram for an IF implementation is contained in Figure 17.8. In this
figure, the circles with crosses are mixers and the squares with crosses are variable gain
amplifiers. The gain is bipolar. That is, the weight can vary the amplifier gain and the sign of
the product depending upon the signs of the weight components.

Figure 17.8 IF implementation of an SLC.
The blocks with integral signs in them are typically implemented using lowpass filters,
where the bandwidth of the lowpass filter is set somewhat lower than the reciprocal of the
integration time of the SLC.
The block diagrams of Figures 17.7 and 17.8 leave the impression that the SLC is
continually updating the weights. This is not necessary, or even desirable. The loop could be
allowed to update the weights during some time period before the transmit pulse and then
hold the weights for the rest of the PRI, or CPI for the case where the radar performs coherent
processing (see Figure 17.4).
17.5.3 Example 4
As an example, a digital version of the Howells-Applebaum SLC of Figure 17.7 was
implemented. We chose to use a digital implementation because it was easier to program. In
the digital version, the integrator of Figure 17.7 is replaced by a summer. Another variation
that was needed was to normalize the value of va(k) by dividing by |va(k)|. We found this
necessary because, without it, the convergence time and stability of the SLC were very
dependent on the interference power. In an actual SLC, this normalization would be
performed by some type of instantaneous AGC, such as an IF limiter [4; 17, p. 119].
Figure 17.9 contains a block diagram of the Howells-Applebaum SLC that was
implemented. The vm(k) and va(k) signals were created using the parameters of Example 2.
Specifically, we used the parameters corresponding to interference source 1 and auxiliary
channel 1. As before, vs(k) was set to zero since we were concerned with only the calculation
of the weights. The particular area of interest in this example was the variation of the weight

and the cancellation ratio as a function of stage, k (time), for different values of interference
power. The output of the simulation for JNRs (interference power levels; see Example 2) of
40, 50, and 60 dB are contained in Figure 17.10. The left graph contains plots of |w(k)| versus
k and the right graph contains plots of CR(k) versus k. As with Example 3, the curves are
based on 10,000 Monte Carlo runs. The value of μ used in the simulation was 0.005 and was
somewhat arbitrarily chosen.
Figure 17.9 SLC simulation block diagram.
Figure 17.10 SLC simulation results.
As expected, the time it takes the SLC to reach steady state increases as the jammer power
decreases. However, in all three cases, the SLC reached steady state by about 70 samples. If we
were to assume that the radar uses a waveform with a compressed pulse width of 1 µs, and if

we assume the samples are spaced 1 µs apart to satisfy the independence requirement, the SLC
would settle in about 70 µs. This means that about 70 µs would be needed for the SLC to
stabilize (see Figure 17.4) before the weight was held and used. This example demonstrates
that the JNR affects both convergence time and CR. This is because the effective loop gain is
proportional to the JNR. This proportionality is sometimes described as a potential
disadvantage of the Howells-Applebaum SLC [17, pp. 119–120].
17.6 SIDELOBE BLANKER
Rather than trying to cancel interference, the SLB gates the receiver off for those range cells
where the signal in the auxiliary channel is larger than the main channel signal by some
specified amount. A functional block diagram of the circuity that accomplishes this is shown
in Figure 17.11. As with the SLC, the SLB operates on signals from the main channel and an
auxiliary channel. In fact, the auxiliary channel receiver used for the SLC could also be used
for the SLB. One arrangement would be to process the signals through the SLC to try to
cancel interference and then use the SLB to turn the main channel receiver off for any
interferences that were not rejected by the SLC. An example of such an interference would be
random pulses from some source (another radar, for example). Such random pulses would
not be rejected by the SLC. However, the SLB would detect their presence and gate the
receiver off during the time the interference was present [9; 18, p. 368; 19; 20].
Referring to Figure 17.11, the log detectors contain a square law detector and logarithm
circuits. The subtraction of the two log detector outputs effectively forms the logarithm of the
ratio of the auxiliary and main channel powers. If this ratio exceeds some threshold, T, the
gate, which is effectively a switch, opens and blocks the main channel signal from passing to
the rest of the radar receiver. The block diagram shows that the log output of the receiver is
sent to the gate. In fact, vm(t) could be sent to the gate and on to the rest of the receiver.
Figure 17.11 Functional diagram of a sidelobe blanker.
The normal design criterion for the SLB is that the directivity of the auxiliary antenna will
be larger than the sidelobe levels of the main antenna, but well below the directivity of the
main antenna. Thus, if the interference is entering through the sidelobes of the main antenna,
and is large enough, log(|va|2/|vm|2) would be greater than T and the main channel signal
would not be allowed to pass to the rest of the receiver.

For a signal entering through the main lobe of the main antenna, |vm|2 would be much
larger than |va|2. This means log(|va|2/|vm|2) would be less than T and the gate would allow the
signal to pass to the rest of the receiver. If the interference was entering through the main lobe
of the main antenna, log(|va|2/|vm|2) would also be less than T and the interference would be
allowed to pass to the rest of the receiver. Thus, the SLB would not be helpful in mitigating
main lobe interference.
In a 1968 paper, Louis Maisel showed that, as might be expected, an SLB can affect both
false alarm and detection probability [19]. In that paper, he discussed how these probabilities
were affected by the interaction between T and the relation between the auxiliary channel
directivity and the sidelobe levels of the main antenna. An interesting observation from his
paper is that the auxiliary channel directivity should be well above the sidelobe levels of the
main antenna, but well below the maximum directivity of the main antenna. Maisel’s analysis,
where the detection was limited to the case of a single radar pulse with a Marcum or Swerling
0 target fluctuation, was later expanded to account for arbitrary numbers of pulses integrated
and additional target fluctuation models based on the gamma distribution. [21, 22].
17.7 EXERCISES
1.
Derive (17.9) from (17.8). As a hint, if w = a + jb, and J = |e|2,
2.
Derive (17.49), (17.53), (17.56), and (17.57).
3.
Rewrite (17.57) and (17.58) in terms of Ps, RI, Pnm, Ran, Aa, Am, and Aa, without wopt.
4.
Repeat Example 2.
5.
Repeat Example 3.
6.
Repeat Example 4.
7.
Extend Exercise 4 to generate a plot of SINR versus auxiliary antenna directivity relative
to the main antenna directivity. Let the relative auxiliary antenna directivity vary from –10
to –30 dB relative to the peak directivity of the main antenna. Plot the ratio of the noise
powers with and without the SLC [i.e., (Pnm + wHoptRanwopt)/Pnm]. The results of this
exercise will demonstrate why the rule of thumb is that the directivity of the auxiliary
antenna should be well above the sidelobes of the main antenna.
8.
Move the first interference source of Example 2 from the second sidelobe to the third
sidelobe of the main antenna directivity pattern and repeat the example. As a note, the sign
of Am(u,v) is negative on the third sidelobe.
9.
Decrease the number of interference sources to one and repeat Example 2. Does the SLC

still work?
10. Increase the number of interferences sources of Example 2 to three by adding a third
interference source at u3 = sin(–18°). Assign it a JNR of 40 dB. Does the SLC still work?
11. Extend Example 2 to accommodate three auxiliary channels and see if it rejects the three
interferences of Exercise 10. Place the third auxiliary antenna at 6λ. Assume its directivity
is the same as the other two auxiliary antennas.
References
[1]
Manolakis, D. G. et al., Statistical and Adaptive Signal Processing: Spectral Estimation, Signal Modeling, Adaptive
Filtering, and Array Processing, Norwood, MA: Artech House, 2005.
[2]
Barkat, M., Signal Detection and Estimation, 2nd ed., Norwood, MA: Artech House, 2005.
[3]
Haykin, S., Adaptive Filter Theory, 3rd ed., Upper Saddle River, NJ: Prentice-Hall, 1996.
[4]
Nitzberg, R., Adaptive Signal Processing for Radar, Norwood, MA: Artech House, 1992.
[5]
Howells, P. W., “Intermediate Frequency Side-Lobe Canceller,” U.S. Patent 3,202,990, Aug. 24, 1965.
[6]
Applebaum, S. P., “Steady-State and Transient Performance of the Sidelobe Canceller,” Special Projects Laboratory,
Syracuse Univ. Res. Corp., Syracuse, NY, Apr. 8, 1966. Available from DTIC as AD 373326.
[7]
Nitzberg, R., Radar Signal Processing and Adaptive Systems, Norwood, MA: Artech House, 1999.
[8]
Wiener, N. Extrapolation, Interpolation, and Smoothing of Stationary Time Series, New York: Wiley, 1949. Reprinted:
Cambridge, MA: The M.I.T. Press, 1964.
[9]
Farina, A., Antenna-Based Signal Processing Techniques for Radar Systems, Norwood, MA: Artech House, 1992.
[10] Papoulis, A., Probability, Random Variables, and Stochastic Processes, 3rd ed., New York: McGraw-Hill, 1991.
[11] Urkowitz, H., Signal Theory and Random Processes, Dedham, MA: Artech House, 1983.
[12] Guerci, J. R., Cognitive Radar: The Knowledge-Aided Fully Adaptive Approach, Norwood, MA: Artech House, 2010.
[13] Reed, I. S., et al., “Rapid Convergence Rate in Adaptive Arrays,” IEEE Trans. Aerosp. Electron. Syst., vol. 10, no. 6,
Nov. 1974, pp. 853–863.
[14] Gerlach, K., “Adaptive Array Transient Sidelobe Levels and Remedies,” IEEE Trans. Aerosp. Electron. Syst., vol. 26,
no. 3, May 1990, pp. 560–568.
[15] Fenn, A. J., Adaptive Antennas and Phased Arrays for Radar and Communications, Norwood, MA: Artech House,
2008.
[16] Widrow, B., et al., “Adaptive Antenna Systems,” Proc. IEEE, vol. 55, no. 12, Dec. 1967, pp. 2143– 2159.
[17] Lewis, B. L., F. F. Kretschmer, and W. W. Shelton, Aspects of Radar Signal Processing, Norwood, MA: Artech House,
1986.
[18] Barton, D. K., Radar System Analysis and Modeling, Norwood, MA: Artech House, 2005.
[19] Maisel, L. J., “Performance of Sidelobe Blanking Systems,” IEEE Trans. Aerosp. Electron. Syst., vol. 4, no. 2, Mar.
1968, pp. 174–180.
[20] Maisel, L. J., “Noise Cancellation Using Ratio Detection,” IEEE Trans. Inf. Theory, vol. 14, no. 4, Jul. 1968, pp. 556–
562.
[21] Shnidman, D. A., and S. S. Toumodge, “Sidelobe Blanking with Integration and Target Fluctuation,” IEEE Trans. Aerosp.
Electron. Syst., vol. 38, no. 3, Jul. 2002, pp. 1023–1037.
[22] Shnidman, D. A., and N. R. Shnidman, “Sidelobe Blanking with Expanded Models,” IEEE Trans. Aerosp. Electron. Syst.,
vol. 47, no. 2, Apr. 2011, pp. 790–805.
APPENDIX 17A: DERIVATION OF ϕ (17.40)

Figure 17A.1 contains a depiction of the geometry used to derive φ, the phase difference
caused by the path length difference between the interference and the main and auxiliary
antennas. r, is the range from the center of the main antenna to the interference and ra is the
range from the center of the auxiliary antenna to the interference. The center of the main
array is located at the origin of a coordinate system and the center of the auxiliary antenna is
located at (xa, ya, za). We want to find ra − r as r approaches infinity (the far field condition).
We can write
and
Manipulating (17A.2) gives
recognizing that r2 is much larger than the rest of the terms and using
results in
or, since r is large relative to the numerator of the term after the plus sign, we can drop that
term and write
With this we get
We recognize that uI = xI/r and vI = yI/r. Also

Thus,
The phases due to range delay from the interference to the centers of the main and auxiliary
antennas are
and
Thus, the difference of the two phases is
Figure 17A.1 Geometry for calculating φ.
1 Throughout this chapter, we are using complex signal notation. We also assume the various signals are scaled to properly
account for their relative power levels.
2 The signal entering through the main beam could also contain returns from clutter or repeater jammers. From the SLC
perspective, they are also “desired” signals that the SLC is not designed to cancel. That job falls to the signal and data
processors.
3 The parenthetical term “(or energy)” is included to indicate that these quantities could be interpreted as either. In the future,
we will use the term power, but the reader should keep in mind that they could refer to either power or energy.
4 We will use the acronym JNR for interference-to-noise ratio. JNR is the acronym for jammer-to-noise ratio.

5 In (17.48) we are using kT0Fn as the noise energy. An alternate would be to use kTs where Ts is the system noise temperature
(see Chapter 4).
6 As a note, the assumption of ergodicity implies that the interferences and noises are WSS.

Chapter 18
Advances in Radar
18.1 INTRODUCTION
As with many areas of science and engineering, the field of radar is advancing at an ever-
increasing rate. This includes advances in both theory and hardware. Two of the newer theory
areas are multiple-input, multiple-output (MIMO) radar [1, 2] and cognitive radar [1, 3–5].
However, other areas of theory being studied include advanced phased array system
techniques [6, 7], advanced waveforms [4, 8], and advanced tracking algorithms [9–11]. On
the hardware side, there have been significant advances in virtually all subsystems. Examples
of these include direct digital synthesizers (DDSs), extremely quiet local oscillators, advances
in transmit/receive (T/R) module technology, very low noise RF amplifiers, highspeed ADCs
with large dynamic range, high-speed digital signal processors, and incredibly fast computers
with massive memory.
18.2 MIMO RADAR
The concept of MIMO has been used in the wireless and cell phone industry for the past
several years [12]. It provides a means of having several cell phones operate on the same
frequency and uses orthogonal, or almost orthogonal, waveforms to separate the signals.
Radar engineers are currently analyzing the application of this methodology to radars [1, 2,
13, 14].
Probably the simplest example of where MIMO might be applied to radars is in multistatic
radar. As one example, suppose we have N widely spaced radars operating on the same
frequency. We want to use the radars to perform tracking by the method of trilateration. This
is illustrated in Figure 18.1 for the case of N = 3. Each radar can transmit a single beam and
can form multiple receive beams over an angular sector. If Radar 2 illuminates a target and
the return signal is in one of the multiple beams of the other two radars, all three radars will
receive the signal from the target. If the control center knows the locations of the three radars,
and knows which receive beam contains the target return, it can use this information to refine
the estimate of target position via trilateration.

Figure 18.1 Tracking by trilateration.
If only one radar is transmitting, this is a fairly straightforward problem. However, if more
than one radar illuminates the target, the receive parts of the radars need a means of knowing
which radar transmitted the signal they received. This is where the concept of MIMO enters. If
the three radars transmit orthogonal, or nearly orthogonal, waveforms, such as different PRN
coded pulses (see Chapter 10), the sources of the received signals would be known. With that
knowledge, the control center can more precisely determine the target location through
trilateration. This is but one example of a MIMO radar application. Others can be found in [2].
18.3 COGNITIVE RADAR
The general idea of a cognitive radar is that it can sense and adapt to its environment to
improve its operation. In the limit, researchers talk about giving the radar “human” qualities,
a laudable but questionable end objective.
Different authors view cognitive radar from different perspectives. For example, Haykin
[5] approaches it from an adaptive tracking perspective, Guerci [3, 15] approaches it from a
STAP perspective, and Pillai and his colleagues [4] approach it from a waveform selection
perspective.
“Cognition” in radar has been around for a long time. A simple example is the sidelobe
canceller. It senses the radar environment and adapts to it by adjusting the weights of the
canceller algorithm to minimize the interference (see Chapter 17).
Another example is a frequency agile radar that performs clear channel search.

Specifically, it interrogates its environment seeking frequency bands that contain the
minimum interference. It then chooses its operating frequency to use one of the clear
channels. An example of this would be in a sky wave, over-the-horizon radar that depends on
clear channel search to find operating frequencies free of interference from other sources.
Other examples include:
• Clutter maps that the radar uses to avoid clutter, or change processor characteristics to
better mitigate the clutter based on intensity, Doppler characteristics, and/or spatial
characteristics.
• Maneuver detectors that adjust track loop characteristics to accommodate target
maneuvers.
• Interacting multiple model (IMM) trackers that change track filters depending upon target
types and/or kinematic characteristics.
• Constant false alarm rate (CFAR) algorithms that change detection thresholds based on the
sensed noise or jamming environment.
These are but a few examples of “cognitive” radar techniques that have been in use for many
years.
18.4 OTHER ADVANCEMENTS IN RADAR THEORY
Although MIMO and cognitive radar are the current “hot” radar research topics, there are
other radar theory advancements that are being studied. Three examples of these are:
• Advanced Phased Array Techniques—There has been a good deal of research aimed at
taking advantage of the ability to do digital beam forming in modern, active phased array
radars. One example of this includes the adaptive nulling discussed Chapter 16. Other
examples include advanced angle super-resolution techniques such as MUSIC (MUltiple
SIgnal Classification) or the examples mentioned in [7], such as adaptive-adaptive array
processing or principal components processing.
• Advanced Waveforms—Analysts are constantly developing new waveforms or adapting
waveforms from other fields such as communications [16]. Examples of the former are
recent finds in minimum peak sidelobe codes [17–20] and polyphase Barker codes [21,
22]. Examples of adaptations of communications waveforms are Costas coding, Huffman
codes, and codes based on pseudo-random noise sequences [8]. Actually, the theories
behind advanced waveforms have been studied for a long time [23–26]. However, it is
because of the invention of the DDS and high-speed digital signal processors that they can
now be more widely and easily used in radars. In fact, “advanced” waveforms have been
used in radars in the past, although not to the extent now possible with DDSs and high-
speed digital signal processing.
• Advanced Tracking Algorithms—Up until fairly recently, digital track algorithms consisted
of g-h (α-β), g-h-k (α-β-γ) and low order, or partitioned [27, 28], Kalman filters. The main
reason for this was speed and memory limitations of digital computers.
With the development of high-speed computers and advanced programming languages,

researchers are investigating more advanced filters such as high order Kalman filters,
unscented Kalman filters, IMM filters, and particle filters [9–11, 29, 30]. The latter offer the
potential of improved tracking performance and also improved target detection through the
use of a technique termed “track before detect” [31].
18.5 HARDWARE ADVANCEMENTS
Some of the more interesting advancements have taken place, and are still taking place, in the
area of hardware. One of these is the DDS, or Direct Digital Synthesizer. This device is used
to digitally generate waveforms and has almost unlimited waveform generation capability.
This means that waveforms we could previously consider only in theoretical and simulation
studies can now be easily generated in radar hardware. This includes waveforms such as
nonlinear FM, PRN-coded pulses, minimum peak sidelobe waveforms, and the other types of
waveforms discussed in Chapter 10 and elsewhere [4, 8]. The DDS, along with modern digital
signal processing, will allow changing waveforms “on the fly” to contend with changes in the
environment and/or to counter jamming. The DDS will also be an enabler of MIMO and
cognitive radars.
DDSs are advancing to the point where they are suitable for Agile LO frequency synthesis
and can generate high bandwidth, high-BT product waveforms. An example would be the
AD9858, which is a 1 gigasample per second (GSPS) DDS [32]. This DDS is capable of
generating a frequency-agile analog output sine wave at up to 400 MHz. The AD9858 also
includes an automatic frequency sweeping feature, simplifying LFM generation for chirped
radar. Another notable example is the AD9914, which is a 3.5 GSPS DDS with 12-bit digital-
to-analog converter (DAC), which can generate frequencies up to 1.4 GHz and includes 12bit
amplitude scaling for fast amplitude hopping [33].
Another significant hardware advancement is the solid-state T/R module [34, 35]. From an
operational perspective, T/R modules will improve radar reliability because they allow
elimination of the single point of failure that exists with transmitters that use a single, high-
power transmit tube. Also, since they are solid-state, they do not use high-voltage power
supplies, which should enhance safety.
The newer generation of T/R modules that are based on gallium nitride (GaN) transistors
have five times the power density compared to gallium arsinide (GaAs) transistors [36]. This
increased power density requires fewer GaN transistors in parallel for a given output power,
which lends itself to wideband operation. This is because the matching networks tend to be the
limiting factor for bandwidth. Needing fewer GaNs requires simpler matching networks,
which exhibit wider frequency response in general [36]. GaN transistors also offer the
promise of allowing variable output power. This will allow transmit amplitude weighting to
reduce antenna sidelobes. It may also lead to the ability to use amplitude weighting (in time)
across the transmit waveform. Neither of these capabilities can be easily obtained with tube
transmitters or T/R modules that use older technology such as GaAs or silicon. To operate at
maximum efficiency, these types of transmitters must be operated at full power output (class
C).

The ability to time weight could lead to the development of a new class of waveforms that
take advantage of the ability to amplitude weight on transmit [25, 26]. It will be interesting to
see where research in this area leads.
T/R modules will be a key element of MIMO and cognitive radar designs that are currently
being analyzed. They will also be key elements in radars that use advanced beam forming or
STAP algorithms. The reason for this is that radars that use STAP, MIMO, or cognitive ideas
will need access to, and control of, individual antenna elements or, at a minimum, a fairly
large number of subarrays.
Another transmitter (and receiver) component that has advanced significantly over the past
few years is the local oscillator of the exciter. Only a few years ago, oscillators with phase
noise sidebands of about -150 dBc/Hz were difficult to find. Today, it is not uncommon to see
oscillators with phase noise sideband levels of -170 dBc/Hz [37–41].
A key hardware element on the receive side is the ADC. Recent technology has pushed
ADCs to sample rates in the hundreds of MHz to low GHz and dynamic ranges in the 16- to
20-bit range [42–46]. For example, the AD9680 from Analog Devices is a 14-bit ADC
operating at 1 GSPS [47]. As another example, Texas Instruments offers ADCs for direct RF
sampling that are 12 bit and operate at rates up to 3.6 GSPS (ADC12Dxx00RF family) [48–50].
Pushing sampling rates further (at the expense of ADC bits), ApisSys produces a 10-bit, 10
GSPS ADC on a 3U VPX board [51]. Because of this, digital receivers (see Chapter 14) are
being studied and implemented in radars, taking advantage of direct IF or RF sampling.
Further, the ADC is being moved closer to the RF front end of the radar. It would not be
unreasonable to expect that, in the near future, radars could use direct RF sampling and a
completely digital receiver.
A major limitation of moving the ADC closer to the antenna will be the effective noise
figure of ADCs, which currently are on the order of 20 dB to 40 dB. If such an ADC were
placed directly at the antenna, the receiver noise figure will be in this 20- to 40-dB range
because it will set the noise figure of the ADC (see Chapter 4). Even if the ADC were moved
to immediately after the LNA, the large noise figure of the ADC would have a significant
impact on the overall receiver noise figure. For example, suppose we consider an LNA with a
noise figure of 2 dB and a gain of 25 dB. If the noise figure of the ADC is 30 dB, the overall
noise figure of the combined LNA and ADC will be
which is significantly larger than the 2-dB noise figure of the LNA. Until ADC noise figure
values are reduced to more reasonable values, the ADC will need to reside further down the
receiver chain so as to minimize its impact on overall receiver noise figure.
This ability to digitize the received signal early in the receiver chain, coupled with high-
speed and small footprint digital signal processors, will be instrumental in MIMO and
cognitive radars currently being considered. The reason for this is that such radars will need
to process data from a very large number of T/R modules (or subarrays) [36]. This will

require a very large number of receivers, something that will be difficult to achieve without
small, digital devices that, individually, consume small amounts of power. Also, the
advantages of digitizing at the module level, rather than at the subarray level, may not, at least
in the near term, be sufficient to justify the cost. It will be interesting to see how the
technology and techniques progress in this area.
Two other key hardware advancements are small, high-speed digital signal processors and
computers. These are currently key elements in advanced radar signal processors and will
become more important enablers in future radar concepts.
18.6 CONCLUSION
Even though this book focuses on basic radar analysis, when we wrote it, we had in mind the
future theory and hardware advances mentioned in this chapter. This was part of our
motivation for including the detailed mathematics and some of the advanced topics discussed
in the various chapters. It is our belief that a thorough understanding of the basic radar theory
presented in this book is critical to implementing advanced theories and making use of state-
of-the art hardware.
References
[1]
Melvin, W. L., and J. A. Scheer, eds., Principles of Modern Radar, Vol. II: Advanced Techniques, Edison, NJ: Scitech,
2013.
[2]
Li, J., and P. Stoica, eds., MIMO Radar Signal Processing, New York: Wiley & Sons, 2009.
[3]
Guerci, J. R., Cognitive Radar: The Knowledge–Aided Fully Adaptive Approach, Norwood, MA: Artech House, 2010.
[4]
Pillai, U., et al., Waveform Diversity, Theory and Applications, New York: McGraw-Hill, 2011.
[5]
Haykin, S., “Cognitive Radar: A Way of the Future,” IEEE Signal Proc. Mag., vol. 23, no. 1, Jan. 2006, pp. 30–40.
[6]
Wirth, W. D., Radar Techniques Using Array Antennas, London, UK: IEE Press, 2001.
[7]
Brookner, E., “Active-Phased-Arrays and Digital Beamforming: Amazing Breakthroughs and Future Trends,” 2008 IEEE
Radar Conf., Rome, Italy, May 26–30, 2008, Session TU-S2.1.
[8]
Levanon, N., and E. Mozeson, Radar Signals, New York: Wiley-Interscience, 2004.
[9]
Julier, S. J., and J. K. Uhlmann, “Unscented Filtering and Nonlinear Estimation,” Proc. IEEE, vol. 92, no. 3, Mar. 2004,
pp. 401–422.
[10] Herman, S., and P. Moulin, “A Particle Filtering Approach to FM-Band Passive Radar Tracking and Automatic Target
Recognition,” IEEE Aerosp. Conf. Proc., vol. 4, 2002, pp. 1789–1808.
[11] Mazor, E., et al., “Interacting Multiple Model Methods in Target Tracking: A Survey,” IEEE Trans. Aerosp. Electron.
Syst., vol. 34, no. 1, Jan. 1998, pp. 103–123.
[12] Jankiraman, M., Space-Time Codes and MIMO Systems, Norwood, MA: Artech House, 2004.
[13] Sen, S., and A. Nehorai, “OFDM MIMO Radar With Mutual-Information Waveform Design for Low-Grazing Angle
Tracking,” IEEE Trans. Signal Processing, vol. 58, no. 6, Jun. 2010, pp. 3152–3162.
[14] Sen, S., and A. Nehorai, “OFDM MIMO Radar for Low-Grazing Angle Tracking,” 2009 Conf. Rec. 43rd Asilomar
Conf. Signals, Syst., and Comput., Nov. 1–4, 2009, pp. 125–129.
[15] Guerci, J. R., Space-Time Adaptive Processing for Radar, 2nd ed., Norwood, MA: Artech House, 2014.
[16] Mow, W. H., K.-L. Du and W. H. Wu, “New Evolutionary Search for Long Low Autocorrelation Binary Sequences,”
IEEE Trans Aerosp. Electron. Syst., vol. 51, no. 1, Jan. 2015, pp. 290–303.

[17] Coxson, G., and J. Russo, “Efficient Exhaustive Search for Optimal-Peak-Sidelobe Binary Codes,” IEEE Trans. Aerosp.
Electron. Syst., vol. 41, no. 1, Jan. 2005, pp. 302–308.
[18] Nunn, C. J., and G. E. Coxson, “Best-Known Autocorrelation Peak Sidelobe Levels for Binary Codes of Length 71 to
105,” IEEE Trans. Aerosp. Electron. Syst., vol. 44, no. 1, Jan. 2008, pp. 392–395.
[19] Leukhin, A. N., and E. N. Potekhin, “Binary Sequences with Minimum Peak Sidelobe Level up to Length 68,” Int.
Workshop on Coding and Cryptography, Bergen, Norway, Apr. 15–19, 2013.
[20] Leukhin, A. N., and E. N. Potekhin, “Optimal Peak Sidelobe Level Sequences Up to Length 74,” 2013 European Radar
Conf. (EuRAD), Nuremberg, Germany, Oct. 9–11, 2013, pp. 495–498.
[21] Gabbay, S. “Properties of Even-Length Barker Codes and Specific Polyphase Codes with Barker Type Autocorrelation
Functions,” Naval Research Laboratory, Washington, D.C., Rep. 8586, Jul. 12, 1982. Available from DTIC as AD-
A117415.
[22] Nunn, C. J., and G. E. Coxson, “Polyphase Pulse Compression Codes with Optimal Peak and Integrated Sidelobes,”
IEEE Trans. Aerosp. Electron. Syst., vol. 45, no. 2, Apr. 2009, pp. 775–781.
[23] Rihaczek, A. W., Principles of High-Resolution Radar, New York: McGraw-Hill, 1969. Reprinted: Norwood, MA:
Artech House, 1995.
[24] DeLong, D., and E. M. Hofstetter, “On the Design of Optimum Radar Waveforms for Clutter Rejection,” IEEE Trans. Inf.
Theory, vol. 13, no. 3, Jul. 1967, pp. 454–463.
[25] Rummler, W. D., “Clutter Suppression by Complex Weighting of Coherent Pulse Trains,” IEEE Trans. Aerosp. Electron.
Syst., vol. 2, no. 6, Nov. 1966, pp. 689–699.
[26] Rummler, W. D., “A Technique for Improving the Clutter Performance of Coherent Pulse Train Signals,” IEEE Trans.
Aerosp. Electron. Syst., vol. 3, no. 6, Nov. 1967, pp. 898–906.
[27] Johnson, G., “Choice of Coordinates and Computational Difficulty,” IEEE Trans. Automatic Control, vol. 19, no. 1, Feb.
1974, pp. 77–78.
[28] Mehra, R. K., “A Comparison of Several Nonlinear Filters for Reentry Vehicle Tracking,” IEEE Trans. Automatic
Control, vol. 16, no. 4, Aug. 1971, pp. 307–319.
[29] Ye, B., and Y. Zhang, “Improved FPGA Implementation of Particle Filter for Radar Tracking Applications,” 2nd Asian-
Pacific Conf. Synthetic Aperture Radar (APSAR 2009), Xian, Shanxi, Oct. 26–30, 2009, pp. 943–946.
[30] Budge, M. C., Jr., “A Variable Order IMM Algorithm,” Proc. Huntsville Simulation Conf., Huntsville, AL, 2005.
[31] Boers, Y., et al., “A Track Before Detect Algorithm for Tracking Extended Targets,” IEE Proc. Radar, Sonar and
Navig., vol. 153, no. 4, Aug. 2006, pp. 345–351.
[32] Analog Devices, “AD9858 1 GSPS Direct Digital Synthesizer,” 2003. www.analog.com.
[33] Analog Devices, “AD9914, 3.5 GSPS Direct Digital Synthesizer with 12-Bit DAC,” 2014. http://www.analog.com.
[34] Manqing, W., “Digital Array Radar: Technology and Trends,” 2011 IEEE CIE Int. Conf. Radar, vol. 1, Chengdu, China,
Oct. 24–27, 2011, pp. 1–4.
[35] Li, T., and X.-G. Wang, “Development of Wideband Digital Array Radar,” 2013 10th Int. Comput. Conf. Wavelet Active
Media Tech. Inf. Process. (ICCWAMTIP), Chengdu, China, Dec. 17–19, 2013, pp. 286–289.
[36] Farina, A., et al., “AESA Radar — Pan-Domain Multi-Function Capabilities for Future Systems,” 2013 IEEE Int. Symp.
Phased Array Syst. & Tech., Waltham, MA, Oct. 15–18, 2013, pp. 4–11.
[37] Wenzel Associates, Inc., “Low Noise Crystal Oscillators > Golden MXO (PLO w/Dividers),” www.wenzel.com.
[38] Wenzel Associates, Inc., “Low Noise Crystal Oscillators > Sorcerer II,” www.wenzel.com.
[39] Boroditsky, R., and J. Gomez, “Ultra Low Phase Noise 1 GHz OCXO,” IEEE Int. Frequency Control Symp., 2007
Joint with the 21st European Frequency and Time Forum, Geneva, Switzerland, May 29–Jun. 1, 2007, pp. 250–253.
[40] Hoover, L., H. Griffith, and K. DeVries, “Low Noise X-band Exciter Using a Sapphire Loaded Cavity Oscillator,” 2008
IEEE Int. Frequency Control Symp. (FCS), Honolulu, HI, May 19–21, 2008, pp. 309–311.
[41] Poddar, A. K., and U. L. Rohde, “The Pursuit for Low Cost and Low Phase Noise Synthesized Signal Sources: Theory &
Optimization,” 2014 IEEE Int. Frequency Control Symp. (FCS), Taipei, Taiwan, May 19–22, 2014, pp. 1–9.
[42] Jha, A. R., “High Performance Analog-to-Digital Converters (ADCs) for Signal Processing,” 2nd Int. Conf. Microwave
and Millimeter Wave Technology Proc., Beijing, China, Sept. 14–16, 2000, pp. 32–35.

[43] Smith, A., “High Sensitivity Receiver Applications Benefit from Unique Features in 16-Bit 130Msps ADC,” October
2005. cds.linear.com/docs/en/design-note/DSOL44.pdf.
[44] Linear 
Technology 
Corporation, 
“LTC2207/LTC2206 
16-Bit, 
105Msps/80Msps 
ADCs,” 
2006.
cds.linear.com/docs/en/datasheet/22076fc.pdf.
[45] Linear Technology Corporation, “LTC2338-18, 18-Bit, 1Msps, ±10.24V True Bipolar SAR ADC,” 2013.
cds.linear.com/docs/en/datasheet/233818f.pdf.
[46] Linear Technology Corporation, “LTC2378-20, 20-Bit, 1Msps, Low Power SAR ADC with 0.5ppm INL,” 2013.
cds.linear.com/docs/en/datasheet/237820fa.pdf.
[47] Analog Devices, “AD9680 Data Sheet: 14-Bit, 1 GSPS JESD204B, Dual Analog-to-Digital Converter,” 2014.
www.analog.com.
[48] Texas Instruments, “RF-Sampling and GSPS ADCs, Breakthrough ADCs Revolutionize Radio Architectures,” 2012.
www.ti.com/gigadc.
[49] Texas 
Instruments, 
“ADC12D1800, 
12-Bit, 
Single 
3.6 
GSPS 
Ultra 
High-Speed, 
ADC,” 
Jan. 
2014.
http://www.ti.com/lit/ds/symlink/adc12d1800.pdf.
[50] Texas 
Instruments, 
“ADC12J4000 
12-Bit 
4 
GSPS 
ADC 
With 
Integrated 
DDC,” 
Sept. 
2014.
http://www.ti.com/lit/ds/symlink/adc12j4000.pdf.
[51] ApisSys, 
“AV101, 
10-bit 
10 
GSPS 
ADC 
and 
Signal 
Processing 
3U 
VPX 
Board,” 
Sept. 
2013.
www.apissys.com/pdf/AV101.pdf.

Appendix A
Suboptimal Filtering
A.1 INTRODUCTION
While analog filter implementations can provide very good approximations to matched
filters, if a small amount of loss is acceptable, suboptimal filters are often simpler to build
and are more economical. To quantify the performance degradation (SNR loss) of a
suboptimal filter implementation (mismatched filter, matched filter approximation), we
compare the performance of the matched filter approximation to the performance of an ideal
matched filter.
For this analysis, consider the linear time invariant (LTI) system, depicted in Figure A.1.
The specified waveform is s(t), the filter impulse response is h(t) (mismatched filter), and n(t)
is the noise present at the input to the filter.
The instantaneous SNR at the output of a LTI system can be shown to be [1]
where
Figure A.1 LTI system block diagram.
 denotes the Fourier transform of f(x). N(f) is the power spectral density of the input
noise. If we stipulate that the input noise to the LTI system is stationary zero-mean additive
white noise with power spectral density of1

and that we observe the peak signal power at the output of the filter at (arbitrarily) time t0, we
get
Recall, the peak SNR out of an MF given AWGN, is (see Chapter 7)
where we note the energy in the signal (according to Parseval’s theorem) is [1]
Taking the ratio of the peak SNR out of the suboptimal filter and the peak SNR out of the
matched filter results in
Equation (A.9) is the efficiency of a matched filter approximation, which is the criterion
used to populate Table 5.9. The reciprocal of (A.9) is matching loss [2, p. 376]

A.1.1 Example: Rectangular Pulse—Ideal Lowpass (Rectangular) Filter
Consider a rectangular pulse given by2
where V is the pulse amplitude and T is the pulsewidth. The frequency response for an ideal
lowpass filter can be expressed as [3, p. 270]
Taking the Fourier transform of (A.11), we get the spectrum S(f) of the rectangular input
signal s(t)
where sinc(x) = sin(πx)/(πx) [4].3 Alternatively, we could apply the Fourier transform pair4

Substitution of (A.12) and (A.13) into the SNR ratio given by (A.9) yields
Noting the noted that |ejx| = e0 = 1, and carrying out the corresponding integral, (A.14)
simplifies to
We observe that sinc(x) is an even function, so for f and T positive, we get
Using (A.15) in the denominator of (A.14), we get

To evaluate the (A.14), we make use of the indefinite integrals
where Si(x) is the sine integral given by
which results in
Noting again that B and T are positive and real, which results in
we arrive at the expression
which is plotted in Figure A.2.

Figure A.2 SNR for ideal lowpass filter with respect to the matched filter for rectangular input.
The peak relative SNR for an ideal LPF is 0.825 (–0.838 dB), which occurs at BT = 0.686.
This is the basis for the rule-of-thumb 0.8 to 1.0 dB match filter loss [5–7].
The question that arises is why does this disagree with the BT product in Table 5.9 in
Chapter 5. The answer lies in the fact that the BT product values listed in Table 5.9 are for
bandpass filters. While we performed a lowpass equivalent analysis for simplicity, recall that
the bandpass bandwidth is twice that of the lowpass equivalent, or BT = 1.371.5 The resultant
equation for an ideal BPF, becomes
A.1.2 Example: Rectangular Pulse—One-Stage Single-Tuned RLC Resonant Circuit
The description 1-stage single-tuned refers to a tuned amplifier with a single RLC resonant
circuit (tank circuit) providing frequency selectivity. We will analyze a lowpass RLC circuit
equivalent, which is a single-pole RC LPF for simplicity (remember the factor of 2 difference
in bandwidth). Instead of using the SNR ratio equation as we did with the earlier example, we
will take an alternative approach suggested in [3, 8]. For this example, consider the RC LPF
(RC integrator) with gain G depicted in Figure A.3.
Recall for an RC integrator the step response is [3]
where α = 1/RC is the time constant of the RC LPF. Taking the derivative of the step response
gives the impulse response

Figure A.3 RC lowpass filter.
The output to a single rectangular pulse s(t) represented by
can be expressed as
Evaluating (A.29) results in
which is depicted in Figure A.4.
The frequency response of an RC LPF with gain G is
where fc = B is the 3-dB cutoff frequency given by

Figure A.4 RC LPF output to a pulse input.
The amplitude is given by
The power transfer function is
The maximum output signal occurs at t = T,6 where we note for τ < T, the output so(t) is
increasing and for τ > T, so(t) decreases. Substitution into (A.30) results in a peak magnitude
of
Noise power is
where we made use of
The output SNR is then

Recall the output SNR for MF is
compared to ideal matched filter
which is plotted in Figure A.5.
Figure A.5 SNR for single-pole lowpass filter with respect to the matched filter for rectangular input.

The peak relative SNR for a lowpass RC filter is 0.815 (0.891 dB), which occurs at BT =
0.200. For a bandpass RLC filter, the BT product is 0.400. The bandpass RLC filter equation
becomes
This BT product provides the optimum single-stage filter approximation to an MF. Carlock
investigates the two-stage RC lowpass filter incorporating unity gain isolation amplifiers
between the filter stages [9]. Fine’s thesis contains an analysis for higher order RC filters as
well as experimental verification [10].
A.1.3 Example: Gaussian Pulse—Gaussian Filter
The Gaussian pulse in the time domain (impulse response for an ideal Gaussian filter) can be
formulated in a number of ways. For this analysis, we chose to express the Gaussian pulse and
Gaussian spectrum as [11, 12]
and
respectively. The scale factor α is chosen based upon how pulsewidth and bandwidth are
defined. Here we choose
which results in τ representing the half-power pulsewidth and B representing the half-power
bandwidth. The associated BT product is 0.44 [13]. Substitution into (A.9) yields
The equation above simplifies to 1, since the waveform and filter are matched.
A.1.4 Example: Rectangular Pulse—Gaussian Filter or Gaussian Filter— Rectangular

Pulse
Substitution of (A.14) and (A.43) into (A.9) yields
Equation (A.46) is plotted below in Figure A.6.
Figure A.6 SNR for Gaussian filter with respect to the matched filter for rectangular input.
A.2 SUMMARY
The results of the various examples are summarized in Table A.1. A comparison graph is
provided in Figure A.7.
Table A.1
Matched Filter Approximations

Figure A.7 Relative SNR with respect to the time bandwidth product BT.
A.3 EXERCISES
1.
Derive (A.1), the SNR at the output of an LTI system.
2.
Calculate peak magnitude and BT of (A.25). Hint: equate derivative to zero.
3.
Derive (A.30).
4.
From [3], the transfer function of a filter composed of RC filters in cascade can be given
by the approximate expression [14]

which is a variation on the Gaussian filter definition given earlier. What is the relative
efficiency of such a filter for the detection of a rectangular pulse with amplitude A and
duration T? What should be the value of the optimal sampling instant t0?
References
[1]
Minkoff, J., Signal Processing: Fundamentals and Applications for Communications and Sensing Systems, Norwood,
MA: Artech House, 2002, p. 134.
[2]
Barton, D. K., Radar Equations for Modern Radar, Norwood, MA: Artech House, 2013.
[3]
deCoulon, F., Signal Theory and Processing, Dedham, MA: Artech House, 1986.
[4]
Woodward, P. M., Probability and Information Theory, with Applications to Radar, New York: McGraw-Hill, 1953.
Reprinted: Dedham, MA: Artech House, 1980.
[5]
Meikle, H., Modern Radar Systems, 2nd ed., Norwood, MA: Artech House, 2008.
[6]
Skolnik, M. I., ed., Radar Handbook, New York: McGraw-Hill, 1970.
[7]
Uhlenbeck, G. E., and J. L. Lawson, Threshold Signals, vol. 24 of MIT Radiation Lab. Series, New York: McGraw-Hill,
1950. Reprinted: Norwood, MA: Artech House (CD-ROM edition), 1999.
[8]
Urkowitz, H., Signal Theory and Random Processes, Dedham, MA: Artech House, 1983.
[9]
Carlock, G., “The Two-Stage RC Low-Pass Matched Filter,” IEEE Trans. Commun., vol. 20, no. 1, Feb. 1972, pp. 73–
74.
[10] Fine, A. M., Optimum Filters for Pulsed Signals in Noise, Thesis for the Degree of Master of Electrical Engineering,
Polytechnic Institute of Brooklyn, Jun. 1957.
[11] Barton, D. K., Radar System Analysis, Englewood Cliffs, NJ: Prentice-Hall, 1964. Reprinted: Dedham, MA: Artech
House, 1976.
[12] Bradford, W. R., “Response of a Gaussian Band-Pass Filter to a Gaussian Video Pulse,” Proc. IEEE, vol. 54, no. 1, Jan.
1966, pp. 89–90.
[13] Spencer, R. E., “The Detection of Pulse Signals Near the Noise Threshold,” J. British Inst. Radio Eng., vol. 11, no. 10,
Oct. 1951, pp. 435–454.
[14] Valley, G., and H. Wallman, Vacuum Tube Amplifiers, vol. 18 of MIT Radiation Lab. Series, New York: McGraw-Hill,
1948. Reprinted: Norwood, MA: Artech House (CD-ROM edition), 1999.
1 The factor of 2 accounts for a two-sided power spectral density.
2 Since delay is arbitrary, a noncausal signal is chosen for notational simplicity.
3 In 1952, Philip M. Woodward coined the term sinc function defined by sin(πx)/πx in his classic work on radar waveforms.
While sinc is a contraction of sine cardinal, the sinc function differs slightly from the sine cardinal function defined by sin(x)/x.
Woodward states he normalized the sinc function by absorbing the π term into the definition for a cleaner notation.
4 This result can be derived by considering the rect[x] function and applying the superposition, time-delay, and scale-change
Fourier transform theorems.
5 Radian frequency is occasionally used when expressing BT product, resulting in a 2π scale factor.
6 According to Urkowitz [8], setting t = T does not, strictly speaking, result in the optimal SNR, but rather t = T/1.25. The
difference in SNR efficiency is minimal, so we will follow the common convention of using, t = T.

Appendix B
Data Windowing Functions
Table B.1 contains continuous and discrete time forms of some common window functions
whose uses include range, Doppler, and antenna sidelobe reduction [1–19]. The discrete time
window functions are in causal symmetric form (identical endpoints), which is generally used
for FIR filter design. Periodic forms, characterized by a missing (implied) endpoint to
accommodate periodic extension, are generally used for spectral estimation (divide by N
versus N – 1).
Table B.1
Window Functions1


a The parameter α is inversely proportional to sidelobe level. Values for α of 2.5 to 3.5 are typical.
b I0 is the zero-order modified Bessel function of the first kind. Sometimes β = πα is used in the expression. The parameter β is
inversely proportional to sidelobe level. Values for β of 2.0π to 3.5π are representative.
References
[1]
Kunt, M., Digital Signal Processing, Norwood, MA: Artech House, 1986.
[2]
Tsui, J., Digital Microwave Receivers: Theory and Concepts, Norwood, MA: Artech House, 1989.
[3]
Tsui, J., Digital Techniques for Wideband Receivers, 2nd ed., NorwoodBoston, MA: Artech House, 2001.
[4]
Harris, F. J., “On the Use of Windows for Harmonic Analysis with the Discrete Fourier Transform,” Proc. IEEE, vol. 66,
no. 1, Jan. 1978, pp. 51–83.

[5]
Nuttall, A. H., “Some Windows with Very Good Sidelobe Behavior,” IEEE Trans. Acoust., Speech, Signal Process., vol.
29, no. 1, Feb. 1981, pp. 84–91.
[6]
Cook, C. E., and M. Bernfeld, Radar Signals: An Introduction to Theory and Application, New York: Academic Press,
1967. Reprinted: Norwood, MA: Artech House, 1993.
[7]
John, D. G. M., and G. Proakis, Digital Signal Processing, Principles, Algorithms, and Applications, 2nd ed., New
York: Macmillan, 1992.
[8]
Lyons, R. G., Understanding Digital Signal Processing, 3rd ed., Englewood Cliffs, NJ: Prentice-Hall, 2011.
[9]
Oppenheim, A. V., ed., Applications of Digital Signal Processing, Englewood Cliffs, NJ: Prentice-Hall, 1978.
[10] Rabiner, L. R., and B. Gold, Theory and Application of Digital Signal Processing, Englewood Cliffs, NJ: Prentice-Hall,
1975.
[11] Schaefer, R. T., R. W. Schaefer, and R. M. Mersereau, “Digital Signal Processing for Doppler Radar Signals,” Proc. 1979
IEEE Int. Conf. on Acoust., Speech, Signal Process., Washington, D.C.: 1979, pp. 170–173.
[12] Blackman, R. B., and J. W. Tukey, The Measurement of Power Spectra from the Point of View of Communications
Engineering, New York: Dover, 1958.
[13] Harris, F. J., “Windows, Harmonic Analysis, and the Discrete Fourier Transform,” Department of the Navy, Undersea
Surveillance Department, Naval Undersea Center, San Diego, CA, Rep. No. NUC TP 532, Sept. 1976.
[14] Poularikas, A. D., Handbook of Formulas and Tables for Signal Processing, Boca Raton, FL: CRC Press, 1999.
[15] Rapuano, S., and F. J. Harris, “An Introduction to FFT and Time Domain Windows,” IEEE Instrum. Meas. Mag., vol. 10,
no. 6, Dec. 2007, pp. 32–44.
[16] Hamming, R. W., Digital Filters, 2nd ed., Englewood Cliffs, NJ: Prentice-Hall, 1983.
[17] Kuo, F. F. and J. F. Kaiser, (Eds.), System Analysis by Digital Computer, New York: John Wiley & Sons, 1966.
[18] Elliot, D. F. and K. R. Rao, Fast Transforms: Algorithms, Analysis, Applications, New York: Academic Press, 1982.
[19] Parzen, E., “Mathematical Considerations in the Estimation of Spectra,” Technometrics, vol. 3, 1961, pp. 167–190.
1 The terms windowing and weighting, are sometimes associated with time domain and frequency domain application,
respectively [18, p. 192].

Acronyms and Abbreviations
°C
Degrees Celsius
°F
Degrees Fahrenheit
3-D
Three-Dimensional
AAF
Anti-Aliasing Filter
AC
Alternating Current
ADC
Analog-to-Digital Converter
AESS
Aerospace and Electronic Systems Society
AGC
Automatic Gain Control
AIEE
American Institute of Electrical Engineers
ARSR
Air Route Surveillance Radar
AWGN
Additive White Gaussian Noise
BJT
Bipolar Junction Transistor
BPF
Bandpass Filter
BT
Time-Bandwidth
BWA
Backward Wave Amplifier
CA
Cell Averaging
CFAR
Constant False Alarm Rate
CL
Conversion Loss
CNR
Clutter-to-Noise Ratio
COHO
COHerent Oscillator

CPD
Cyclotron Protective Device
CPI
Coherent Processing Interval
CR
Cancellation Ratio
CRPL
Central Radio Propagation Laboratory
CW
Continuous Wave
CWESA
Cyclotron Wave Electrostatic Amplifier
dB
Decibel
dB(m)
Decibel Relative to 1 Meter
dB(W-s)
Decibel Relative to 1 Watt-Second
dB/km
Decibels Per Kilometer
dB/m
Decibels Per Meter
dBFS
Decibel Below Full Scale
dBi
Antenna Directivity (Gain) Relative to the Directivity of an Isotropic Antenna
dBm
Power Level Relative to 1 Milliwatt
dBsm
Area in Square Meters Relative to 1 m2
dBV
Voltage Level Relative to 1 Volt Root Mean Square
dBW
Power Level Relative to 1 Watt
DC
Direct Current
DDC
Digital Downconverter
DDS
Direct Digital Synthesis (or Synthesizer)
deg
Degree
DFT
Discrete-time Fourier Transform

DOF
Degrees of Freedom
EA
Electronic Attack
ECM
Electronic Countermeasure
E-field
Electric Field
EIA
Electronic Industries Association
ERP
Effective Radiated Power
ESA
Electrostatic Amplifier
ESCA
Electrostatics Combined Amplifier
FFT
Fast Fourier Transformer
FIR
Finite Impulse Response
FM
Frequency Modulation
FMCW
Frequency Modulated Continuous Wave
FSR
Full-Scale Range
ft
Foot
GaAs
Gallium Arsenide
GaN
Gallium Nitride
GHz
Gigahertz
GMTI
Ground Moving Target Indication
GO
Greatest Of
GPS
Global Positioning System
GSPS
Giga Sample Per Second
HF
High Frequency

HPF
Highpass Filter
Hz
Hertz
IEEE
Institute of Electrical and Electronics Engineers
IF
Intermediate Frequency
IFFT
Inverse FFT
IIR
Infinite Impulse Response
IL
Insertion Loss
IMD
Intermodulation Distortion
IMM
Interacting Multiple Model
IRE
Institute of Radio Engineers
IRF
Image Reject Filter
J/J
Joules per Joule
JAN
Joint Army Navy
JNR
Jammer-to-Noise Ratio
K
Degrees Kelvin
kft
Kilofoot
kHz
Kilohertz
km
Kilometer
kW
Kilowatt
LFM
Linear Frequency Modulation
LMS
Least Mean-Square

LNA
Low Noise Amplifier
LO
Local Oscillator
LOS
Line of Sight
LPF
Lowpass Filter
lsb
Least Significant Bit
LSR
Linear Shift Register
LTI
Linear Time Invariant
m
Meter
m/s
Meter Per Second
m/μs
Meter Per Microsecond
m2
Square Meter
MDS
Minimum Detectable Signal
MDS
Minimum Discernable Signal
MF
Matched Filter
MHz
Megahertz
mi
Mile
MIMO
Multiple-Input, Multiple-Output
MIT
Massachusetts Institute of Technology
MKS
Meter, Kilogram, Second
mm
Millimeter Wave
mm
Millimeter
mm/hr
Millimeter Per Hour

mph
Mile Per Hour
MSPS
Mega Samples Per Second
MTD
Moving Target Detector
MTI
Moving Target Indicator
MUSIC
Multiple Signal Classification
NAGC
Noise Automatic Gain Control
Number of Constant Level Sidelobes
NCO
Numerically Controlled Oscillator
NLFM
Nonlinear FM
nmi
Nautical Mile
Np/m
Neper per meter
ns
Nanosecond
PPI
Plan Position Indicator
PRF
Pulse Repetition Frequency
PRI
Pulse Repetition Interval
PRN
Pseudo Random Noise
rad2
Steradian
Radar
RAdio Detection And Ranging
RC
Resistor Capacitor
RCM
Range Cell Migration
RCMC
RCM Correction

RCS
Radar Cross Section
RF
Radio Frequency
RGPO
Range-Gate Pull Off
RLC
Resistor Inductor Capacitor
rms
Root Mean Square
rpm
Revolutions Per Minute
RRE
Radar Range Equation
Rx
Receive
s
Second
SALT
Strategic Arms Limitation Talk
SAP
Spatial Adaptive Processing
SAR
Synthetic Aperture Radar
SAW
Surface Acoustic Wave
SCR
Signal-to-Clutter Ratio
SINR
Signal-to-Interference-Plus-Noise Ratio
SIR
Signal-to-Interference Power Ratio
SLB
Sidelobe Blanking
SLC
Sidelobe Cancellation
SMI
Sample Matrix Inversion
SNR
Signal-to-Noise Ratio
SO
Smallest Of
SONAR
SOund Navigation And Ranging

STALO
STAble Local Oscillator
STAP
Space-Time Adaptive Processing
STC
Sensitivity Time Control
T/R and TR Transmit/Receive
TNR
Threshold-to Noise Ratio
TSS
Tangential Sensitivity
TWT
Traveling Wave Tube
Tx
Transmit
UAV
Unmanned Aerial Vehicle
UHF
Ultra High Frequency
USAAF
United States Army Air Forces
VHF
Very High Frequency
W
Watt
w.r.t.
With Respect To
W/Hz
Watt Per Hertz
W/m2
Watt Per Square Meter
W/W
Watt Per Watt
WG
Waveguide
W-m2
Watt-Meter-Square
W-s
Watt-Second
WSCS
Wide-Sense Cyclostationary

WSS
Wide-Sense Stationary
ZOH
Zero-Order Hold
Μs
Microsecond
℧/m
Mho Per Meter
Ω
ohm

About the Authors
Dr. Mervin C. Budge
Mervin Budge, Ph.D., is chief scientist of Dynetics where he is responsible for overall
technical quality. He also conducts research in the area of radars and air defense systems, with
emphasis on foreign radars and air defense systems. In this role, he also conducts analysis and
design of advanced signal processing and antenna methodologies and intelligence
assessments of foreign radars and integrated air defense systems. This includes performance
analyses and specific radar designs.
Dr. Budge has served as an adjunct professor at the University of Alabama Huntsville
(UAH) since 1973 and previously served as a part-time professor at the Southeastern Institute
of Technology, an instructor at Texas A&M University, and both a graduate and
undergraduate teaching assistant at the University of Louisiana in Lafayette. He currently
teaches courses in radar and Kalman filters at UAH.
He holds a Ph.D. in electrical engineering from Texas A&M University and master’s and
bachelor’s degrees in electrical engineering from the University of Louisiana in Lafayette.
Dr. Budge’s graduate research included “Prediction of Clear Air Turbulence,” sponsored by
NASA Marshall Space Flight Center; “Space Shuttle Aeroelastic Stability and Control
Properties,” sponsored by NASA Johnson Space Center; “Parameter Identification for Linear
Systems,” a Themis Grant; “Electric Power System Control,” sponsored by Philco-Ford; and
“Analytical Techniques for Predicting Flow Fields in Oil Wells.” His graduate thesis and
dissertational work was in the areas of the effect of interaction in multivariable control
systems and the design of recursive and nonrecursive digital filters. He has authored more
than three dozen publications.
Dr. Budge’s honors include Outstanding Educator Award from the Huntsville Section of
IEEE; the Dynetics R. Duane Hays Award for Technical Excellence; the Outstanding Engineer
Award from the Huntsville Section IEEE; Outstanding PhD Student, Electrical Engineering
Department, Texas A&M University; and Outstanding PhD Student, Texas A&M University.
He holds memberships in Eta Kappa Nu, Tau Beta Pi, Sigma Xi, Phi Kappa Phi, the Institute of
Electrical and Electronic Engineers, Phi Mu Alpha Sinfonia, and Kappa Delta Pi.

Shawn R. German
Shawn R. German is a senior principal engineer at Dynetics, where he has been employed
since 1995. He is responsible for performing radar system design and analysis. His current
areas of research include radar receivers, advanced signal processing methodologies, and
detection theory. He is responsible for generating intelligence assessments of foreign radars,
seekers, and integrated air defense systems, including performance analyses and specific
radar designs. Mr. German has served as a subject matter expert in a number of radar
disciplines including: foreign material exploitation (FME), system test and measurement, RF
and IF system testing, and performance analysis. He has worked with several different
intelligence Integrated Technical Evaluation and Analysis of Multiple Sources (ITEAMS)
groups and subgroups and acts as a consultant on various technologies and theories.
Mr. German’s expertise includes the following areas: radar system repair and alignment,
radar system and subsystem performance analysis, environment and target modeling,
waveform analysis and design, signal processor analysis, radar system design, radar
subsystem design, signal processor hardware prototyping, development of mathematical
models from measured data, radar system and subsystem simulation and emulation, radar
ECM and ECCM, and phased array theory.
He has primarily supported the Defense Intelligence Agency/Missile and Space Intelligence
Center (DIA/MSIC) with his research at Dynetics. He performs all source/ELINT analysis for
various foreign systems as well as ITEAMS and pre-ITEAMS analysis, generating reports
and briefings on a number of radar systems.
Mr. German has been recognized by MSIC for technical excellence in support of Chinese
ITEAMS and for outstanding service and dedication to the agency’s Tri National Program. He
is a senior member of the Institute of Electrical and Electronic Engineers and holds
memberships in Eta Kappa Nu, Tau Beta Pi, Phi Kappa Phi, and the Association of Old Crows.
He holds bachelor’s and master’s degrees in electrical engineering from Mississippi State
University and an associate’s degree in electronics technology from the Mississippi Gulf
Coast Community College. He is completing a PhD in electrical engineering from the
University of Alabama Huntsville (UAH). His undergraduate research in “Error Control
Coding in Satellite Communication” was sponsored by NASA.
Mr. German has served as a graduate teaching assistant in the Electrical Engineering

Department at Mississippi State University. He has coauthored several publications.

Index
1-dB gain compression point (See compression point, 1 dB)
1-D scanning, 103
2-D scanning, 103
3-dB points, 28
Absorption coefficient for oxygen, 132
Absorption coefficient for water vapor, 134
Adaptive Nulling
example, 630
Adaptivity, 638, 651
ADC (See analog-to-digital converter)
ADC noise figure, 691
Advanced phased array techniques, 687, 689
Advanced tracking algorithms, 690
Advanced waveforms, 687
AGC (See automatic gain control)
Airborne STAP, 646
Ambiguity function, 251, 268
Barker code, 283
cross, 251
derivation of, 252
Frank polyphase, 280
LFM, 281
LFM pulse, 260
mismatched PRN, 293
numerical calculation of, 262
PRN, 289
unmodulated pulse, 257
Woodward, 251
Ambiguous range, 5
Amplitude
detection, 202, 210, 212
noise, 400
taper, 345
Analog Doppler processor, 467
Analog-to-digital converter, 538
full-scale power, 544
full-scale range, 539

full-scale rms, 543
gain prior to, 548
ideal transfer function, 542
least significant bit, 540
noise figure, 545
quanta, 540
quantization, 539
error, 540
interval, 540
noise, 544
SNR, 546
Angle-Doppler map airborne STAP, 651
Angle-Doppler response optimum, 644
Angle-Doppler space, 621
Antenna
aperture, 30
beamwidth, 27
broadside, 335
directivity, 25, 27, 31, 45
effective area, 30
gain, 26
loss, 24, 26
noise temperature, 33, 34, 45
pattern, 26
Gaussian model, 388
sinc2(x) model, 388
phase center, 385
phased array, 30
sidelobes, 27, 29
Antialiasing filter, 533
Aperture, 28
effective, 30
plane, 376
Applebaum, Sydney P. 656
Armstrong, Edwin Howard, 497
AT-11 Kansan, 56
Atmospheric loss, 129
Attenuator, 85
Audion, 498
Automatic gain control, 499
Avantek, 528

Average power, 24, 45
aperture product, 39
Backward wave amplifier, 500
Bakut, P. A., 214
Bandpass sampling theorem, 534
Barker code
Barker coded pulses, 282
table of, 284
Barton, David K., 21, 201
Baseband, 15
representation, 141
signal notation, 15
Bassford, Ronald S., 384
Beam area, 40
Beam dwell, 7
Beam forming, 621, 623
linear phased array, 624
signal plus noise, 623
signal plus noise and interference, 627
Beam packing, 105
dense, 104
sparse, 104
Beam steering, 346
Beamshape loss, 210
Beamwidth, 27, 28, 334, 345
Bell, Alexander Graham, 12
Binary integration, 202, 211
Bistatic, 2
Boltzmann’s constant, 22, 79, 509
Brennan and Reed
STAP, 622
Broadside, 335
BT product, 194
Butterworth, Stephen, 503
Calibration
pilot pulse, 553
Cancellation ratio, 669
Caputi, William J., 307

Carrier frequency, 9
Cascade, 87
cascade analysis, 521
1 dB compression point, 526
2nd order intercept, 527
3rd order intercept, 528
conventions, 521
equations, 529
gain, 524
noise figure, 524
noise temperature, 524
procedure, 522
effective noise temperature, 89
equivalent noise temperature, 89
noise figure, 87, 90
Cauchy-Schwarz
in STAP, 626
inequality, 185
Central line clutter gain, 445
CFAR, 110
Chaff, 476
radial velocity, 479
RCS, 477
self-screening, 477
spectral width, 478
time behavior, 477
Chebyshev, Pafnuty Lvovich, 509
Chirp, 192
Chi-square, 214
Circular polarization, 374
Clebsch, Alfred, 51
Clutter
attenuation, 413, 417, 427
fill time, 442
folding, 438
gating, 457
model, 385
geometry, 385
smooth earth, 385
nulling, 622
sea, 392
Cognitive radar, 687, 688

Coherent
demodulator (See I/Q demodulator)
integration, 201, 202, 212
oscillator, 508
processing interval, 576
COHO (See coherent oscillator)
Coincidence detection, 202
Combined noise temperature, 84
Complementary error function, 216
Complex signal notation, 14
Compressed pulsewidth, 194
Compression filter, 268
Compression point 1 dB, 511
Conjugate-transpose, 624
Constant false alarm rate, 110
Conversion loss, 522, 524
Cosine Fresnel integral, 308
Cosine weighting, 457
Cosmic noise, 509
Covariance matrix estimate STAP, 653
CPI, 576 (See coherent processing interval)
CR (See cancellation ratio)
Cross ambiguity function, 251
Cross range, 565
Cumulative probability of detection, 201, 227, 248
CW radar, 1
Cyclotron protective device, 501
Cyclotron wave electrostatic amplifier, 500
Darlington, Sidney
waveforms, 268
Data windowing functions, 711
dB, 12
De Forest, Lee, 498
de la Marche, Alfred, 384
Decibel
arithmetic, 13
dBi, 13, 344
dBm, 13

dBsm, 13
dBV, 13
dBW, 13
Decibel relative to an isotropic radiator, 344
Degrees of freedom, 621
Density function, 140
chi-squared, 58
exponential, 58
Desired signal, 656
Detection
analysis, 206
probability, 39, 139, 140, 160
range, 38
theory, 139
threshold, 39
Diagonal loading, 653
Dickey, Robert H.
waveforms, 268
Difference
channel, 552
pattern, 552
Digital
beam forming, 557
downconverter, 537
receiver, 532
signal processor, 443
Diode limiter, 97, 501
Dirac, Paul, 316
Direct IF sampling, 533
Direct RF sampling, 535
Directive gain, 22, 25, 342
pattern, 340
Directivity, 22, 25, 27
Discriminator
angle, 554
curve, 553
range, 555
Dither, 549
noise, 549
Doppler, 319
beam sharpening, 566

frequency, 8, 11, 319
processing
STAP, 631
processor
STAP, 634
Double-balanced mixer, 503
Down chirp, 192
Downconversion, 502
Downrange, 579
Dual-threshold detection, 202, 211
Duty cycle, 40
Dynamic range, 512
required, 519
Edge taper, 357
Effective
aperture, 30, 45
area, 30
noise temperature, 83, 89
radiated power, 26, 27, 45
EIA designation, 123
Electronic Industries Association, 123
Electrostatic
amplifier, 500
combined amplifier, 501
Element
packing, 356
pattern, 350
Elliptical, 374
Emslie, Alfred G., 384
Environmental noise, 509
Equivalent noise temperature, 83, 89
ERP, 27 (See effective radiated power)
Error function, complementary, 216
Euler identity, 14
False alarm
probability, 140, 161, 162, 164, 170
time, 170
Fast time, 621
Feed, 356

constrained, 356
parallel, 100
series, 100
space, 356
stripline, 100
Feedback shift register, 286, 287
Feedback tap configurations, 287
Ferrite limiter, 97
Fessenden, Reginald Aubrey, 497
FFT, 202, 203, 210, 263
Field point, 337
Filter order, 498
FM waveforms, 269
Four-channel receiver, 557
Fourier transform, 262
Fowle
nonlinear FM synthesis, 272, 277
examples, 274, 275
Frank
code, 279, 280
matrix, 279
polyphase
ambiguity function, 280
code, 279, 280
Frequency
agility, 71, 504
bands, 2
coding, 268
hop waveforms, 268, 293, (See step frequency waveforms)
response
optimized, 638
Fresnel integral, 308
Friis formula, 510
cascade noise figure, 524
cascade noise temperature, 524
Gamma function
incomplete, 212, 213
Gaussian, 204, 207
pulse, 707
spectrum, 707

Generalized Barker codes, 285
Golomb, S. W.
PRN codes, 286
Goodchild, William, 384
Grating lobes, 333
Gross Doppler, 595
Ground clutter
airborne STAP, 647
Ground clutter spectrum, 391
Gaussian model, 391
Ground range, 4
Harald Trap Friis, 84
Harmonics, 503
Hermitian, 624, 629 (See conjugatetranspose)
Heterodyne, 497, 501
Horizontally polarized, 374
Howells, Paul W., 656
Hülsmeyer, Christian, 1
I and Q detector (See I/Q demodulator)
I/Q demodulator, 506, 507
IF representation, 141
Image
frequency, 502
noise, 502
reject filter, 502
Incomplete gamma function, 212, 213
Independent, 227, 236
Insertion loss, 524
Instrumented range, 7, 8
Integration
binary, 202
coherent, 201, 202, 203, 206, 210
noncoherent, 201, 210, 212, 217, 218
Interference, 621
canceller, 656
multiple, 628
plus noise, 629
signal, 656

steering vector, 628
Intermodulation distortion, 516
Interpolation, 600
Isotropic, 330
radiator, 26, 330
ISTOK, 501
Johnson noise, 32, 79
Johnson, John Bertrand, 79
Joint density, 143, 144, 157
Karki, James, 546
Kester, Walt, 546
Kotel’nikov, Vladimir
Aleksandrovich, 263
Kotel’nikov sampling theorem, 534
Least mean-square, 658
Least significant bit, 540
Leeson, D. B., 450
Left-circular, 374
LFM, 307
and the sinc function, 306
bandwidth, 193, 259, 308
frequency response high BT product, 307
frequency response, low BT produce, 307
pulse, 192, 258, 267, 307
slope, 192, 259, 307
spectrum of, 309
with amplitude weighting, 269
Linear array, 336
Linear frequency modulation, 192, 307
Linearly polarized, 374
LNA, 97 (See low noise amplifier)
Local oscillator, 501
Losses, 22, 31
atmospheric, 105
beamshape, 102
CFAR, 111, 116
circulator, 96
directional coupler, 96

Doppler straddle loss, 110
duplexer, 96
examples, 99, 105, 108, 109, 117
feed, 96
isolator, 96
matched filter, 110
mismatch, 100, 102
mode adapter, 96
phase shifter, 100
power divider, 96
preselector, 96
propagation, 105
radar, 95
radome, 100
range straddle loss, 110
receiver protection, 96
RF, 95
rotary joint, 96
scan, 103
T/R switch, 96
transmit, 95
waveguide, 96, 97, 98
waveguide attenuator, 96
waveguide switch, 96
Low noise amplifier, 97, 500
Main beam, 334
Main beam clutter region, 386
Maisel, Louis, 681
Marconi, Guglielmo, 330
Marcum, J. I., 139
Marcum Q function, 165, 208
Marginal density, 144, 154
Matched
Doppler, 254
range, 254
Matched filter, 32, 181, 186, 201, 242, 309
FFT-based, 270, 271
approximations, 709
response
LFM, 270
NLFM, 276

Maximal length sequences, 286
Maximize SINR, 622, 626
Middleton, David, 181
Mie, Gustave, 55
MIMO, 687 (See multiple-input, multiple-output)
radar, 687
Minimum detectable signal, 514, 516
Minimum discernable signal, 514, 516
Minimum peak sidelobe codes, 284 table of, 285
Minimum signal of interest, 515
Mismatched
Doppler, 253
filter, 268
PRN Processing, 289
range, 253
Mixer
conversion loss, 522, 524
double balanced, 503
harmonics, 503
ideal, 501
image reject, 502
practical, 503
spur, 503
Modulation
amplitude, 267
frequency, 267
linear frequency, 267
non-linear frequency, 267
phase, 267
quadratic phase, 267
m-of-n
detection, 201, 202, 229
Monopulse, 551
calibration, 553
combiner, 551
gain imbalance, 554
phase imbalance, 554
three-channel, 551
two-channel, 556
sign switching, 557
Monostatic, 2

Moving target indicator, 410
MTI, 410
clutter performance, 413
improvement factor, 413
response normalization, 412
transients, 433
velocity response, 431
Muehe, Charles E., 384
Multiple PRIs, 6
Multiple signal classification, 558
Multiple simultaneous beams, 557
Multiple-input, multiple-output, 687
MUSIC (See multiple signal classification)
Nepers, 127
NLFM, 268, 272
frequency and phase plots, 276
Noise
cosmic, 509
dither, 549
energy, 22, 31
environmental, 509
factor, 22
figure, 22, 32
floor, 509
power, 35
power spectral density, 32
receiver, 508
temperature, 22, 32
thermal, 32, 508
Noise covariance matrix, 626
Noise figure, 84, 85, 90
analog-to-digital converter, 545
attenuator, 85
Noncoherent integration, 201, 210, 212, 217, 218
Nonlinear FM, 267, 272
synthesis, 272
North, D. O., 181, 201
Norton, Kenneth A., 21
Number of samples, 652
Nyquist

criterion, 534
for FFT matched filter, 271
frequency, 534
Nyquist, Harry Theodor, 79
rate, 534
sampling theorem, 533
zones, 535
Omberg, Arthur C. 21
Optimum weight
airborne STAP, 650
temporal, 637
vector
space-time, 640
Orthogonal waveforms, 687
Orthogonality condition, 659
Packing
factor, 40
rectangular, 356
triangular, 356
Parabolic reflector, 376
Parseval’s theorem, 187, 273
Passive component, 92
Pattern
difference, 552
sum, 552
Pattern propagation factor, 389
Peak transmit power, 22, 24
Permeability of free space, 126
Permittivity of free space, 126
Phase code
Barker, 282
Frank polyphase, 279
generalized Barker, 285
minimum peak sidelobe, 284
polyphase Barker, 285
PRN, 286
pseudo-random noise, 286
Phase-coded pulse
general equation for, 278
Phase detectors, 507

Phase modulation, 267
Phase noise, 400, 422, 447
clutter gain, 445
Phase steering, 346, 349
Phased array
element packing, 356
linear, 336
N-element, 336
radiation pattern, 354
shape, 356
sidelobes, 360
two element, 330
weighting, 360
Pilot pulse, 553
Planar arrays, 352
Planck’s law, 80
Planck constant, 80
Polarization, 373
circular, 374
elliptical, 374
linear, 374
slant, 374
Polyphase Barker codes, 285
Post-detection integration, 210
Power gain, 26
Practical considerations
STAP, 653
Preselector, 498, 505
PRF, 6 (See pulse repetition frequency)
PRI, 6 (See pulse repetition interval)
PRN (See pseudo-random noise)
ambiguity function, 289
for mismatched waveform, 293
code generator, 287
coded waveforms, 286
feedback tap configurations, 287
maximal length sequences, 286
mismatched processing, 289
optimum phase shift, 292
sequence, 286
Probability

cumulative, 201, 238
cumulative detection, 227, 248
detection, 38, 209, 212, 227
false alarm, 227, 228, 235
m-of-n, 230
Processing interval, 575, 576
Processing window, 8
Pulse
compression, 45, 268
constant amplitude, 267
LMF, 267
phase-coded, 267
repetition
frequency, 6
interval, 6
Pulsed Doppler
clutter, 435
processor, 433
signal processor, 441
Pulsed radar, 1, 5
Pulsewidth, 9
Q function, 208
Quadratic phase, 581
coding, 267
Quadrature demodulator (See I/Q demodulator)
Quadrature detector (See I/Q demodulator)
Quanta, 540
Quantization, 539
interval, 540
noise, 544
asumptions, 544
rounding, 540
Radar
block diagram, 16
cognitive, 687
cross section, 22, 29, 51
frequency bands, 2, 3
losses, 95
multiple-input, multiple-output, 687
origin of term, 1

range equation, 21, 45
basic, 21
search, 39
summary, 45
receiver, 80
types, 1
continuous wave, 1
pulsed, 1
RADARSAT, 620
Radiated power
effective, 26, 27
ERP, 27
Radiation pattern, 333, 336, 338
optimized, 631
Radiator
isotropic, 330
Radius of the earth, 131, 390
Rain attenuation, 107, 129
Rain clutter
spectral model, 397
Rain clutter RCS, 393
Random variables, 142
Range
cell migration, 594
correction, 599
correlation, 448
effect, 449
cut
ambiguity function of, 267
matched Doppler, 267
delay, 4, 5, 311
detection, 38
slant, 22
discriminator, 555
gate walk, 210
measurement, 3, 4
rate, 8
range-rate measurement, 3
resolve, 6
sidelobes, 268
Rank, 652
Rayleigh, 3rd Baron, 51 (See Strutt, John William)

Rayleigh density, 150
Rayleigh’s energy theorem, 187
RC integrator, 703
RCM (See range cell migration)
RCMC (See range cell migration correction)
RCS, 22, 29, 51
AT-11 Kansan, 57
flat plate, 55
perfectly conducting sphere, 52, 54
simple shapes, 52
Real signal notation, 14
Receive elements, 334
Receiver, 90, 497
chain, 498
configurations, 550
digital, 532
four-channel, 557
function, 497
noise floor, 509
sensitivity, 514
three-channel, 551
two-channel, 556
Receiver noise, 508
Reciprocity, 334
Rect(x), 10
Rectangular
envelope, 267
packing, 356, 361
pulse, 189
Reference temperature, 508, 509
Reflectivity, 387
RF amplifier, 90, 91
RF front end, 498
Rice, Stephen Oswald, 139
Rice model, 57, 139
Right-circular, 374
Room temperature, 32
SALT I, 42
Sample matrix inversion, 673

Sampling theorem
bandpass, 534
Kotel’nikov, 534
Nyquist, 533
Shannon, 534
SAR, 307
image, 572, 592
image focusing, 592
platform, 579
processing interval, 575
processor, 578, 598
signal characterization, 578
spotlight, 574
strip map, 574
Scan loss, 225
Scan period, 401
Scanning
1-D, 103
2-D, 103
Scatterer, 579
Schottky, Walter Hermann, 497
SCR improvement, 417, 459
Sea clutter, 392
Search
radar, 39
range equation, 45
sector, 39
solid angle, 45, 49
Sensitivity, 514
measuring, 515
tangential, 516
time control, 499
law, 499
Shannon sampling theorem, 534
Sidelobe, 27, 29
blanker, 680, 681
cancellation, 653 (See sidelobe canceller)
canceller, 656
auxiliary antenna, 656
closed loop, 656
coherent processing, 672
Howells-Applebaum, 675

main antenna, 656
open loop, 656
sample matrix inversion, 673
SMI, 673
timing, 672
weight, 656
clutter region, 387
Signal energy, 21, 31
Signal processor, 251, 252
Signal-to-clutter ratio improvement, 413
Signal-to-noise ratio, 21, 35
energy, 35
power, 35
sinc(x), 700
sine Fresnel integral, 308
sine integral, 701
Singer, James
PRN codes, 286
Single-pulse SNR, 201
SINR
maximimized, 629
Slant range, 4, 22
SLB (See sidelobe blanker)
SLC (See sidelobe canceller)
Slow time, 621
SMI (See sample matrix inversion)
Smith, Harry B., 384
Smooth earth clutter model, 385
SNR, 21, 35, 201, 203, 206, 215
single-pulse, 201, 214
Solid-state LNA, 501
Space-Time Adaptive Processing (See STAP)
Space-time processing, 622, 639
example, 641
Spatial filter, 624, 629
Spatial processing, 622, 623
Spectrum analyzer, 310
Spherical correction, 356
Spotlight SAR, 574

Spurious free dynamic range, 518
Spurious signal, 503
order, 503
passband, 504
Square law
detection, 210
detector, 210, 216
Stable local oscillator, 506
Staggered PRIs, 428
STAP, 621
practical considerations, 653
STC (See sensitivity time control)
Steering, 346
phase, 346
time delay, 346
Steering vector, 624, 628
airborne clutter, 649
interference, 628
space-time, 640
temoral, 636
signal, space-time, 639
space-time, 640
Step frequency waveform, 293
Doppler effects, 298
range ambiguities, 296
range resolution, 296
Stretch processor, 310
block diagram, 311
configuration, 310
implementation, 317
operation, 313
SNR, 315
Strip map SAR, 574, 576
Strutt, John William, 51
Subarray, 626
Suboptimal filter, 697
Sum pattern, 552
Superheterodyne receiver, 498
dual conversion, 505
single conversion, 498
Swerling, 202, 214

Swerling RCS fluctuation models, 57, 60, 139
Swerling 0/5 (SW0/SW5), 57, 140, 148, 154, 177, 213, 219
Swerling 1 (SW1), 57, 58, 213, 220
SW1/SW2, 57, 140, 150, 153, 177
Swerling 2 (SW2), 57, 58, 213, 221
Swerling 3 (SW3), 57, 58, 213, 222
SW3/SW4, 57, 140, 151, 157, 177
Swerling 4 (SW4), 57, 58, 213, 223
Swerling, Peter, 139
Synchronous detector (See I/Q demodulator)
Synthetic aperture radar, 307 (See SAR)
System noise
figure, 87, 90
temperature, 33, 45, 84
T/R modules, 98, 99
MIMO, 691
Tangential sensitivity, 516
Taylor weights, 384
Telemobiloscope, 1
Temperature
reference, 508, 509
Temporal processing, 631
example, 637
Thermal noise, 32, 79, 508
Three-channel monopulse, 551
Time delay, 4
steering, 346, 348
Time-bandwidth product, 194
TNR, 207, 214
Transmit loss, 24
Transmit power
average, 24
peak, 24
Traveling wave tube, 500
Triangle function, 295
Triangular packing, 356, 363
Trilateration, 687
T-type attenuator, 86
Two-channel monopulse, 556

sign switching, 557
Unambiguous range, 5
Uncompressed pulsewidth, 194, 404
Unmodulated pulse, 189, 255
Up chirp, 192
Usable range
maximum, 7
minimum, 7
Van Vleck, J. H., 181
Van Vleck-Weisskopf formula, 134
Vertically polarized, 374
Video, 210
Visible space, 333, 355
VSWR, 101
Wallman, Henry, 181
Wave number, 338
Waveform woding, 267
Waveguide, 95
attenuation, 123
Wavelength, 11, 22
Weight, 657
vector, 624
Weighting
Bartlett, 112
Blackman, 112
Blackman-Harris, 112
Chebyshev, 112, 345
cosn(x), 112, 345
elliptically symmetric, 360
Gaussian, 112
Hamming, 112
Hann, 112
multiplicative, 360
Nuttall, 112
rectangular, 112
Taylor, 112, 345
uniform, 331
Wiener, Norbert, 181

Wiener filter, 655, 658
Wide sense stationary, 142, 148
Woodward
ambiguity function, 251
Woodward, Philip M., 251, 700
WR number, 123
WSS, 401

Recent Titles in the Artech House Radar Series
Dr. Joseph R. Guerci, Series Editor
Adaptive Antennas and Phased Arrays for Radar and Communications, Alan J. Fenn
Advanced Techniques for Digital Receivers, Phillip E. Pace
Advances in Direction-of-Arrival Estimation, Sathish Chandran, editor
Airborne Pulsed Doppler Radar, Second Edition, Guy V. Morris and Linda Harkness, editors
Basic Radar Analysis, Mervin C. Budge, Jr. and Shawn R. German
Bayesian Multiple Target Tracking, Second Edition Lawrence D. Stone, Roy L. Streit, Thomas
L. Corwin, and Kristine L Bell
Beyond the Kalman Filter: Particle Filters for Tracking Applications, Branko Ristic, Sanjeev
Arulampalam, and Neil Gordon
Cognitive Radar: The Knowledge-Aided Fully Adaptive Approach, Joseph R. Guerci
Computer Simulation of Aerial Target Radar Scattering, Recognition, Detection, and Tracking,
Yakov D. Shirman, editor
Design and Analysis of Modern Tracking Systems, Samuel Blackman and Robert Popoli
Detecting and Classifying Low Probability of Intercept Radar, Second Edition, Phillip E. Pace
Digital Techniques for Wideband Receivers, Second Edition, James Tsui
Electronic Intelligence: The Analysis of Radar Signals, Second Edition, Richard G. Wiley
Electronic Warfare in the Information Age, D. Curtis Schleher
Electronic Warfare Target Location Methods, Second Edition, Richard A. Poisel
ELINT: The Interception and Analysis of Radar Signals, Richard G. Wiley
EW 101: A First Course in Electronic Warfare, David Adamy
EW 102: A Second Course in Electronic Warfare, David Adamy
EW 103: Tactical Battlefield Communications Electronic Warfare, David Adamy
Fourier Transforms in Radar and Signal Processing, Second Edition, David Brandwood

Fundamentals of Electronic Warfare, Sergei A. Vakin, Lev N. Shustov, and Robert H. Dunwell
Fundamentals of Short-Range FM Radar, Igor V. Komarov and Sergey M. Smolskiy
Handbook of Computer Simulation in Radio Engineering, Communications, and Radar, Sergey
A. Leonov and Alexander I. Leonov
High-Resolution Radar, Second Edition, Donald R. Wehner
Highly Integrated Low-Power Radars, Sergio Saponara, Maria Greco, Egidio Ragonese,
Giuseppe Palmisano, and Bruno Neri
Introduction to Electronic Defense Systems, Second Edition, Filippo Neri
Introduction to Electronic Warfare, D. Curtis Schleher
Introduction to Electronic Warfare Modeling and Simulation, David L. Adamy
Introduction to RF Equipment and System Design, Pekka Eskelinen
Introduction to Modern EW Systems, Andrea De Martino The Micro-Doppler Effect in Radar,
Victor C. Chen
Microwave Radar: Imaging and Advanced Concepts, Roger J. Sullivan
Millimeter-Wave Radar Targets and Clutter, Gennadiy P. Kulemin
Modern Radar Systems, Second Edition, Hamish Meikle
Modern Radar System Analysis, David K. Barton
Modern Radar System Analysis Software and User’s Manual, Version 3.0, David K. Barton
Monopulse Principles and Techniques, Second Edition, Samuel M. Sherman and David K.
Barton
MTI and Pulsed Doppler Radar with MATLAB®, Second Edition, D. Curtis Schleher
Multitarget-Multisensor Tracking: Applications and Advances Volume III, Yaakov Bar-
Shalom and William Dale Blair, editors
Precision FMCW Short-Range Radar for Industrial Applications, Boris A. Atayants,
Viacheslav M. Davydochkin, Victor V. Ezerskiy, Valery S. Parshin, and Sergey M. Smolskiy
Principles of High-Resolution Radar, August W. Rihaczek
Principles of Radar and Sonar Signal Processing, François Le Chevalier

Radar Cross Section, Second Edition, Eugene F. Knott, et al.
Radar Equations for Modern Radar, David K. Barton
Radar Evaluation Handbook, David K. Barton, et al.
Radar Meteorology, Henri Sauvageot
Radar Reflectivity of Land and Sea, Third Edition, Maurice W. Long
Radar Resolution and Complex-Image Analysis, August W. Rihaczek and Stephen J.
Hershkowitz
Radar Signal Processing and Adaptive Systems, Ramon Nitzberg
Radar System Analysis, Design, and Simulation, Eyung W. Kang
Radar System Analysis and Modeling, David K. Barton
Radar System Performance Modeling, Second Edition, G. Richard Curry
Radar Technology Encyclopedia, David K. Barton and Sergey A. Leonov, editors
Radio Wave Propagation Fundamentals, Artem Saakian
Range-Doppler Radar Imaging and Motion Compensation, Jae Sok Son, et al.
Robotic Navigation and Mapping with Radar, Martin Adams, John Mullane, Ebi Jose, and Ba-
Ngu Vo
Signal Detection and Estimation, Second Edition, Mourad Barkat
Signal Processing in Noise Waveform Radar, Krzysztof Kulpa
Space-Time Adaptive Processing for Radar, Second Edition, Joseph R. Guerci
Special Design Topics in Digital Wideband Receivers, James Tsui
Theory and Practice of Radar Target Identification, August W. Rihaczek and Stephen J.
Hershkowitz
Time-Frequency Signal Analysis with Applications, Ljubiša Stanković, Miloš Daković, and
Thayananthan Thayaparan
Time-Frequency Transforms for Radar Imaging and Signal Analysis, Victor C. Chen and Hao
Ling

For further information on these and other Artech House titles, including previously
considered out-of-print books now available through our In-Print-Forever® (IPF®) program,
contact:
Artech House
685 Canton Street
Norwood, MA 02062
Phone: 781-769-9750
Fax: 781-769-6334
e-mail: artech@artechhouse.com
Artech House
16 Sussex Street
London SW1V HRW UK
Phone: +44 (0)20 7596-8750
Fax: +44 (0)20 7630-0166
e-mail: artech-uk@artechhouse.com
Find us on the World Wide Web at: www.artechhouse.com


Fernando G. Lobo, Cl´audio F. Lima and Zbigniew Michalewicz (Eds.)
Parameter Setting in Evolutionary Algorithms

Studies in Computational Intelligence, Volume 54
Editor-in-chief
Prof. Janusz Kacprzyk
Systems Research Institute
Polish Academy of Sciences
ul. Newelska 6
01-447 Warsaw
Poland
E-mail: kacprzyk@ibspan.waw.pl
Further volumes of this series
can be found on our homepage:
springer.com
Vol. 33. Martin Pelikan, Kumara Sastry, Erick
Cant´u-Paz (Eds.)
Scalable Optimization via Probabilistic
Modeling, 2006
ISBN 978-3-540-34953-2
Vol. 34. Ajith Abraham, Crina Grosan, Vitorino
Ramos (Eds.)
Swarm Intelligence in Data Mining, 2006
ISBN 978-3-540-34955-6
Vol. 35. Ke Chen, Lipo Wang (Eds.)
Trends in Neural Computation, 2007
ISBN 978-3-540-36121-3
Vol. 36. Ildar Batyrshin, Janusz Kacprzyk, Leonid
Sheremetor, LotﬁA. Zadeh (Eds.)
Preception-based Data Mining and Decision Making
in Economics and Finance, 2006
ISBN 978-3-540-36244-9
Vol. 37. Jie Lu, Da Ruan, Guangquan Zhang (Eds.)
E-Service Intelligence, 2007
ISBN 978-3-540-37015-4
Vol. 38. Art Lew, Holger Mauch
Dynamic Programming, 2007
ISBN 978-3-540-37013-0
Vol. 39. Gregory Levitin (Ed.)
Computational Intelligence in Reliability Engineering,
2007
ISBN 978-3-540-37367-4
Vol. 40. Gregory Levitin (Ed.)
Computational Intelligence in Reliability Engineering,
2007
ISBN 978-3-540-37371-1
Vol. 41. Mukesh Khare, S.M. Shiva Nagendra (Eds.)
Artiﬁcial Neural Networks in Vehicular Pollution
Modelling, 2007
ISBN 978-3-540-37417-6
Vol. 42. Bernd J. Kr¨amer, Wolfgang A. Halang (Eds.)
Contributions to Ubiquitous Computing, 2007
ISBN 978-3-540-44909-6
Vol. 43. Fabrice Guillet, Howard J. Hamilton (Eds.)
Quality Measures in Data Mining, 2007
ISBN 978-3-540-44911-9
Vol. 44. Nadia Nedjah, Luiza de Macedo
Mourelle, Mario Neto Borges,
Nival Nunes de Almeida (Eds.)
Intelligent Educational Machines, 2007
ISBN 978-3-540-44920-1
Vol. 45. Vladimir G. Ivancevic, Tijana T. Ivancevic
Neuro-Fuzzy Associative Machinery for Comprehensive
Brain and Cognition Modeling, 2007
ISBN 978-3-540-47463-0
Vol. 46. Valentina Zharkova, Lakhmi C. Jain
Artiﬁcial Intelligence in Recognition and Classiﬁcation
of Astrophysical and Medical Images, 2007
ISBN 978-3-540-47511-8
Vol. 47. S. Sumathi, S. Esakkirajan
Fundamentals of Relational Database Management
Systems, 2007
ISBN 978-3-540-48397-7
Vol. 48. H. Yoshida (Ed.)
Advanced Computational Intelligence Paradigms
in Healthcare, 2007
ISBN 978-3-540-47523-1
Vol. 49. Keshav P. Dahal, Kay Chen Tan, Peter I. Cowling
(Eds.)
Evolutionary Scheduling, 2007
ISBN 978-3-540-48582-7
Vol. 50. Nadia Nedjah, Leandro dos Santos Coelho,
Luiza de Macedo Mourelle (Eds.)
Mobile Robots: The Evolutionary Approach, 2007
ISBN 978-3-540-49719-6
Vol. 51. Shengxiang Yang, Yew Soon Ong, Yaochu Jin
Honda (Eds.)
Evolutionary Computation in Dynamic and Uncertain
Environment, 2007
ISBN 978-3-540-49772-1
Vol. 52. Abraham Kandel, Horst Bunke, Mark Last (Eds.)
Applied Graph Theory in Computer Vision and Pattern
Recognition, 2007
ISBN 978-3-540-68019-2
Vol. 53. Huajin Tang, Kay Chen Tan, Zhang Yi
Neural Networks: Computational Models
and Applications, 2007
ISBN 978-3-540-69225-6
Vol. 54. Fernando G. Lobo, Cl´audio F. Lima
and Zbigniew Michalewicz (Eds.)
Parameter Setting in Evolutionary Algorithms, 2007
ISBN 978-3-540-69431-1

Fernando G. Lobo
Cl´audio F. Lima
Zbigniew Michalewicz
(Eds.)
Parameter Setting
in Evolutionary Algorithms
With 100 Figures and 24 Tables

Fernando G. Lobo
Departamento de Engenharia
Electr´onica e Inform´atica
Universidade do Algarve
Campus de Gambelas
8000-117 Faro Portugal
E-mail: ﬂobo@ualg.pt
Cl´audio F. Lima
Departamento de Engenharia
Electr´onica e Inform´atica
Universidade do Algarve
Campus de Gambelas
8000-117 Faro Portugal
E-mail: clima@ualg.pt
Zbigniew Michalewicz
School of Computer Science
University of Adelaide
SA 5005 Adelaide Australia
E-mail: zbyszek@cs.adelaide.edu.au
Library of Congress Control Number: 2006939345
ISSN print edition: 1860-949X
ISSN electronic edition: 1860-9503
ISBN-10
3-540-69431-5 Springer Berlin Heidelberg New York
ISBN-13
978-3-540-69431-1 Springer Berlin Heidelberg New York
This work is subject to copyright. All rights are reserved, whether the whole or part of the material
is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations, recitation, broad-
casting, reproduction on microﬁlm or in any other way, and storage in data banks. Duplication of
this publication or parts thereof is permitted only under the provisions of the German Copyright Law
of September 9, 1965, in its current version, and permission for use must always be obtained from
Springer-Verlag. Violations are liable to prosecution under the German Copyright Law.
Springer is a part of Springer Science+Business Media
springer.com
c⃝Springer-Verlag Berlin Heidelberg 2007
The use of general descriptive names, registered names, trademarks, etc. in this publication does not
imply, even in the absence of a speciﬁc statement, that such names are exempt from the relevant
protective laws and regulations and therefore free for general use.
Cover design: deblik, Berlin
Typesetting by the editors using a Springer LATEX macro package
Printed on acid-free paper
SPIN: 11543152
89/SPi
5 4 3 2 1 0

Preface
One of the main diﬃculties that a user faces when applying an evolutionary
algorithm (or, as a matter of fact, any heuristic method) to solve a given prob-
lem is to decide on an appropriate set of parameter values. Before running
the algorithm, the user typically has to specify values for a number of pa-
rameters, such as population size, selection rate, and operator probabilities,
not to mention the representation and the operators themselves. Over the
years, there have been numerous research studies on diﬀerent approaches to
automate control of these parameters as well as understand their interactions.
At the 2005 Genetic and Evolutionary Computation Conference, held in
Seattle, the ﬁrst two editors of this book organized a workshop entitled Pa-
rameter setting in evolutionary algorithms. Shortly after we announced the
workshop, we were approached by Janusz Kacprzyk to prepare a volume con-
taining extended versions of some of the papers presented at the workshop,
as well as contributions from other authors in the ﬁeld.
We gladly accepted the invitation and Zbigniew Michalewicz joined us in
the project.
The resulting work is in front of you, a book with 15 chapters covering the
topic on various areas of evolutionary computation, including genetic algo-
rithms, evolution strategies, genetic programming, estimation of distribution
algorithms, and also discussing the issues of speciﬁc parameters used in par-
allel implementations, multi-objective EAs, and practical consideration used
for real-world applications. Some of these chapters are overview oriented while
others describe recent advances in the area.
In the ﬁrst chapter, Ken De Jong gives us an historical overview on para-
meterized evolutionary algorithms, as well as his personal view on the issues
related to parameter adaptation. De Jong was the ﬁrst person to conduct a
systematic study of the eﬀect of parameters on the performance of genetic
algorithms, and it is interesting to hear his view now that more than 30 years
have passed since his 1975 PhD dissertation.
In the second chapter, Agoston Eiben et al. present a survey of the area
giving special attention to on-the-ﬂy parameter setting.

VI
Preface
In chapter 3, Silja Meyer-Nieberg and Hans-Georg Beyer focus on self-
adaptation, a technique that consists of encoding parameters into the indi-
vidual’s genome and evolving them together with the problem’s decision vari-
ables, and that has been mainly used in the area of evolutionary programming
and evolution strategies.
Chapter 4 by Dirk Thierens deals with adaptive operator allocation. These
rules are often used for learning probability values of applying a given oper-
ator from a ﬁxed set of variation operators. Thierens surveys the probability
matching method, which has been incorporated in several adaptive operator
allocation algorithms proposed in the literature, and proposes an alternative
method called the adaptive pursuit strategy. The latter turns out to exhibit
a better performance than the probability matching method, in a controlled,
non-stationary environment.
In chapter 5, Mike Preuss and Thomas Bartz-Beielstein present the sequen-
tial parameter optimization (SPO), a technique based on statistical design of
experiments. The authors motivate the technique and demonstrate its useful-
ness for experimental analysis. As a test case, the SPO procedure is applied
to self-adaptive EA variants for binary coded problems.
In chapter 6, Bo Yuan and Marcus Gallagher bring a statistical technique
called Racing, originally proposed in the machine learning ﬁeld, to the con-
text of choosing parameter settings in EAs. In addition, they also suggest an
hybridization scheme for combining the technique with meta-EAs.
In chapter 7, Alan Piszcz and Terrence Soule discuss structure altering
mutation techniques in genetic programming and observe that the parameter
settings associated with the operators generally show a nonlinear response
with respect to population ﬁtness and computational eﬀort.
In chapter 8, Michael Samples et al. present Commander, a software so-
lution that assists the user in conducting parameter sweep experiments in a
distributed computing environment.
In chapter 9, Fernando Lobo and Cl´audio Lima provide a review of vari-
ous adaptive population sizing methods that have been proposed for genetic
algorithms. For each method, the major advantages and disadvantages are dis-
cussed. The chapter ends with some recommendations for those who design
and compare self-adjusting population sizing mechanisms for genetic algo-
rithms.
In chapter 10, Tian-Li Yu et al. suggest an adaptive population sizing
scheme for genetic algorithms. The method has strong similarities with the
work proposed by Smith and Smuda in 1993, but the components of the pop-
ulation sizing model are automatically estimated through the use of linkage-
learning techniques.
In chapter 11, Martin Pelikan et al. present a parameter-less version of
the hierarchical Bayesian optimization algorithm (hBOA). The resulting al-
gorithm solves nearly decomposable and hierarchical problems in a quadratic
or subquadratic number of function evaluations without the need of user inter-

Preface
VII
vention for setting parameters. The chapter also discusses how the parameter-
less technique can be applied to other estimation of distribution algorithms.
In chapter 12, Kalyanmoy Deb presents a functional decomposition of
an evolutionary multi-objective methodology and shows how a speciﬁc algo-
rithm, the elitist non-dominated sorting GA (NSGA-II), was designed and
implemented without the need of any additional parameter with respect to
those existing in a traditional EA. Deb argues that this property of NSGA-II
is one of the main reasons for its success and popularity.
In chapter 13, Erick Cant´u-Paz presents theoretical models that predict
the eﬀects of the parameters in parallel genetic algorithms. The models explore
the eﬀect of communication topologies, migration rates, population sizing, and
migration strategies. Although the models make assumptions about the class
of problems being solved, they provide useful guidelines for practitioners who
are looking for increased eﬃciency by means of parallelization.
In chapter 14, Zbigniew Michalewicz and Martin Schmidt summarize their
experience of tuning and/or controlling various parameters of evolutionary
algorithms from working on real word problems. A car distribution system
is used as an example. The authors also discuss prediction and optimization
issues present in dynamic environments, and explain the ideas behind Adaptive
Business Intelligence.
The last chapter of the book, by Neal Wagner and Zbigniew Michalewicz,
presents the results of recent studies investigating non-static parameter set-
tings that are controlled by feedback from the genetic programming search
process in the context of forecasting applications.
We hope you will ﬁnd the volume enjoyable and inspiring; we also invite
you to take part in future workshops on Parameter setting in evolutionary
algorithms!
Faro, Portugal,
Fernando Lobo
Faro, Portugal,
Cl´audio Lima
Adelaide, Australia,
Zbigniew Michalewicz
November 2006

Contents
Parameter Setting in EAs: a 30 Year Perspective
Kenneth De Jong
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
Parameter Control in Evolutionary Algorithms
A.E. Eiben, Z. Michalewicz, M. Schoenauer, J.E. Smith . . . . . . . . . . . . . . 19
Self-Adaptation in Evolutionary Algorithms
Silja Meyer-Nieberg, Hans-Georg Beyer
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 47
Adaptive Strategies for Operator Allocation
Dirk Thierens . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77
Sequential Parameter Optimization Applied to Self-
Adaptation for Binary-Coded Evolutionary Algorithms
Mike Preuss, Thomas Bartz-Beielstein . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91
Combining Meta-EAs and Racing for Diﬃcult EA Parameter
Tuning Tasks
Bo Yuan, Marcus Gallagher . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121
Genetic Programming: Parametric Analysis of Structure
Altering Mutation Techniques
Alan Piszcz and Terence Soule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143
Parameter Sweeps for Exploring Parameter Spaces of Genetic
and Evolutionary Algorithms
Michael E. Samples, Matthew J. Byom, Jason M. Daida
. . . . . . . . . . . . . 161
Adaptive Population Sizing Schemes in Genetic Algorithms
Fernando G. Lobo, Cl´audio F. Lima . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 185

X
Contents
Population Sizing to Go: Online Adaptation Using Noise and
Substructural Measurements
Tian-Li Yu, Kumara Sastry, David E. Goldberg . . . . . . . . . . . . . . . . . . . . . . 205
Parameter-less Hierarchical Bayesian Optimization Algorithm
Martin Pelikan, Alexander Hartmann, and Tz-Kai Lin. . . . . . . . . . . . . . . . 225
Evolutionary Multi-Objective Optimization Without
Additional Parameters
Kalyanmoy Deb . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 241
Parameter Setting in Parallel Genetic Algorithms
Erick Cant´u-Paz . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 259
Parameter Control in Practice
Zbigniew Michalewicz, Martin Schmidt. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 277
Parameter Adaptation for GP Forecasting Applications
Neal Wagner, Zbigniew Michalewicz . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 295
Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 311

List of Contributors
Thomas Bartz-Beielstein
Dortmund University
D-44221 Dortmund, Germany
thomas.bartz-beielstein@udo.edu
Hans-Georg Beyer
Vorarlberg University of Applied
Sciences
Hochschulstr. 1
A-6850 Dornbirn, Austria
hans-georg.beyer@fhv.at
Matthew J. Byom
University of Michigan
Ann Arbor, MI 48109-2143, USA
mjbyom@umich.edu
Erick Cant´u-Paz
Yahoo!, Inc.
701 First Avenue
Sunnyvale, CA 94089
cantupaz@acm.org
Jason M. Daida
University of Michigan
Ann Arbor, MI 48109-2143, USA
daida@umich.edu
Kalyanmoy Deb
Indian Institute of Technology
Kanpur
Kanpur, PIN 208016, India
deb@iitk.ac.in
Kenneth De Jong
George Mason University
4400 University Drive, MSN 4A5
Fairfax, VA 22030, USA
kdejong@gmu.edu
A.E. Eiben
Free University Amsterdam
The Netherlands
gusz@cs.vu.nl
Marcus Gallagher
University of Queensland
Qld. 4072, Australia
marcusg@itee.uq.edu.au
David E. Goldberg
University of Illinois at Urbana-
Champaign
Urbana, IL 61801
deg@illigal.ge.uiuc.edu
Alexander Hartmann
Universit¨at G¨ottingen
Friedrich-Hund-Platz 1
37077 G¨ottingen, Germany
hartmann@physik.uni-goettingen.de
Cl´audio F. Lima
University of Algarve
Campus de Gambelas
8000-117 Faro, Portugal
clima@ualg.pt

XII
List of Contributors
Tz-Kai Lin
University of Missouri–St. Louis
One University Blvd.
St. Louis, MO 63121
tlkq4@studentmail.umsl.edu
Fernando G. Lobo
University of Algarve
Campus de Gambelas
8000-117 Faro, Portugal
flobo@ualg.pt
Silja Meyer-Nieberg
Universt¨at der Bundeswehr M¨unchen
D-85577 Neubiberg, Germany
silja.meyer-nieberg@unibw.de
Zbigniew Michalewicz
University of Adelaide
Adelaide, SA 5005, Australia
zbyszek@cs.adelaide.edu.au
Martin Pelikan
University of Missouri–St. Louis
One University Blvd.
St. Louis, MO 63121
pelikan@cs.umsl.edu
Alan Piszcz
University of Idaho
Moscow, ID, 83844-1010
apiszcz@acm.org
Mike Preuss
Dortmund University
D-44221 Dortmund, Germany
mike.preuss@cs.uni-dortmund.de
Michael E. Samples
University of Michigan
Ann Arbor, MI 48109-2143, USA
msamples@umich.edu
Kumara Sastry
University of Illinois at Urbana-
Champaign
Urbana, IL 61801
kumara@illigal.ge.uiuc.edu
Martin Schmidt
SolveIT Software
PO Box 3161
Adelaide, SA 5000, Australia
martin.schmidt@solveitsoftware.com
M. Schoenauer
INRIA
France
marc@lri.fr
J.E. Smith
UWE
United Kingdom
james.smith@uwe.ac.uk
Terence Soule
University of Idaho
Moscow, ID, 83844-1010
tsoule@cs.uidaho.edu
Dirk Thierens
Universiteit Utrecht
The Netherlands
dirk.thierens@cs.uu.nl
Neal Wagner
Augusta State University
Augusta, GA 30904, USA
nwagner@aug.edu
Tian-Li Yu
University of Illinois at Urbana-
Champaign
Urbana, IL 61801
tianliyu@illigal.ge.uiuc.edu
Bo Yuan
University of Queensland
Qld. 4072, Australia
boyuan@itee.uq.edu.au

Parameter Setting in EAs: a 30 Year
Perspective
Kenneth De Jong
Department of Computer Science
George Mason University
4400 University Drive, MSN 4A5
Fairfax, VA 22030, USA
kdejong@gmu.edu
Summary. Parameterized evolutionary algorithms (EAs) have been a standard
part of the Evolutionary Computation community from its inception. The wide-
spread use and applicability of EAs is due in part to the ability to adapt an EA
to a particular problem-solving context by tuning its parameters. However, tun-
ing EA parameters can itself be a challenging task since EA parameters interact
in highly non-linear ways. In this chapter we provide a historical overview of this
issue, discussing both manual (human-in-the-loop) and automated approaches, and
suggesting when a particular strategy might be appropriate.
1 Introduction
More than 30 years have passed since I completed my thesis [18], and more
than 40 years since the pioneering work of Rechenberg [34], Schwefel [36],
Foget et al. [17] and Holland [24]. Since then the ﬁeld of Evolutionary Com-
putation (EC) has grown dramatically and matured into a well-established
discipline.
One of the characteristics of the ﬁeld from the very beginning was the no-
tion of a parameterized evolutionary algorithm (EA), the behavior of which
could be modiﬁed by changing the value of one or more of its parameters. This
is not too surprising since EAs consist of populations of individuals that pro-
duce oﬀspring using reproductive mechanisms that introduce variation into
the population. It was clear from the beginning that changing the population
size, the type and amount of reproductive variation, etc. could signiﬁcantly
change the behavior of an EA on a particular ﬁtness landscape and, con-
versely, appropriate parameter settings for one ﬁtness landscape might be
inappropriate for others.
It is not surprising, then, that from the beginning EA practitioners have
wanted to know the answers to questions like:
K. De Jong: Parameter Setting in EAs: a 30 Year Perspective, Studies in Computational Intel-
ligence (SCI) 54, 1–18 (2007)
www.springerlink.com
© Springer-Verlag Berlin Heidelberg 2007

2
Kenneth De Jong
• Are there optimal settings for the parameters of an EA in general?
• Are there optimal settings for the parameters of an EA for a particular
class of ﬁtness landscapes?
• Are there robust settings for the parameters of and EA than produce good
performance over a broad range of ﬁtness landscapes?
• Is it desirable to dynamically change parameter values during an EA run?
• How do changes in a parameter aﬀect the performance of an EA?
• How do landscape properties aﬀect parameter value choices?
A look at the table of contents of this book will convince you that there
are no simple answers to these questions. In this chapter I will provide my
perspective on these issues and describe a general framework that I use to
help understand the answers to such questions.
2 No Free Lunch Theorems
It has been shown in a variety of contexts including the areas of search, opti-
mization, and machine learning that, unless adequate restrictions are placed
on the the class of problems one is attempting to solve, there is no single algo-
rithm that will outperform all other algorithms (see, for example, [83]). While
very general and abstract, No Free Lunch (NFL) theorems provide a starting
point for answering some of the questions listed in the previous section. In par-
ticular, these theorems serve as both a cautionary note to EA practitioners to
be careful about generalizing their results, and as a source of encouragement
to use parameter tuning as a mechanism for adapting algorithms to particular
classes of problems.
It is important at this point to clarify several frequently encountered mis-
conceptions about NFL results. The ﬁrst misconception is that NFL results
are often interpreted as applying only to algorithms that do not self-adapt
during a problem solving run. So, for example, I might imagine a two-stage
algorithm that begins by ﬁrst ﬁguring out what kind of problem it is being
asked to solve and then choosing the appropriate algorithm (or parameter
settings) from its repertoire, thus avoiding NFL constraints. While these ap-
proaches can extend the range of problems for which an algorithm is eﬀective,
it is clear that the ability of the ﬁrst stage to identify problem types and select
appropriate algorithms is itself an algorithm that is subject to NFL results.
A second misconception is the interpretation of NFL results as leaving
us in a hopeless state unable to say anything general about the algorithms
we develop. A more constructive interpretation of NFL results is that they
present us with a challenge to deﬁne more carefully the classes of problems
we are trying to solve and the algorithms we are developing to solve them.
In particular, NFL results provide the context for a meaningful discussion
parameter setting in EAs, including:
• What EA parameters are useful for improving performance?

Parameter Setting in EAs: a 30 Year Perspective
3
• How does one choose appropriate parameter values?
• Should parameter values be ﬁxed during a run or be modiﬁed dynami-
cally?
These issues are explored in some detail in the following sections.
3 Parameter Setting in Simple EAs
Before we tackle more complicated EAs, it is helpful to ascertain what we
know about parameter setting for simple EAs. I use the term “simple EA” to
refer to algorithms in which:
• A population of individuals is evolved over time.
• The current population is used as a source of parents to produce some
oﬀspring.
• A subset of the parents and oﬀspring are selected to “survive” into the
next generation.
This is the form that the early EAs took (Evolution Strategies (ESs), Evolu-
tionary Programming (EP), and Genetic Algorithms (GAs)), and it represents
the basic formulation of many EAs used today. With any algorithm, the easi-
est things to consider parameterizing are the numerical elements. In the case
of the early EAs, the obvious candidate was the population size. For ESs, the
parent population size µ was speciﬁed independently of the oﬀspring popu-
lation size λ, while early EP and GA versions deﬁned both populations to
be of the same size controlled by a single parameter m. The values of these
parameters were speciﬁed at the beginning of a run and remained unchanged
throughout a run.
Somewhat more subtle were the parameters deﬁned to control reproductive
variation (the degree to which oﬀspring resemble their parents). Early ES and
EP algorithms produced oﬀspring via asexual reproduction, i.e., by cloning
and mutation. Hence, the amount of reproductive variation is controlled by
varying the parameters of the mutation operator, one of which speciﬁes the
(expected) number of “genes” undergoing mutation. A second parameter con-
trols how diﬀerent (on average) a mutated gene is from its original form.
Since many ES and EP algorithms were evolving populations of individu-
als whose genes were real-valued parameters of an objective ﬁtness function,
a Gaussian mutation operator was used with a mean of zero and a variance
of σ2. It became clear early on that leaving σ ﬁxed for the duration of an
entire evolutionary run was suboptimal, and mechanisms were developed for
dynamically adapting its value were developed. The earliest of these mecha-
nisms was the “1/5th rule”’ developed by Rechenberg and Schwefel [36] in
which the ratio of “successful” mutations (improvements in ﬁtness) to unsuc-
cessful ones was monitored during the course of a run. If the ratio fell below
1/5, σ was decreased. If the ratio exceeded 1/5, σ was increased. The amount

4
Kenneth De Jong
that σ was increased or decreased was a parameter as well, but set to a ﬁxed
value at the beginning of a run.
By contrast, early GA versions used an internal binary string represen-
tation, and produced reproductive variation via two-parent (sexual) recom-
bination and a “bit-ﬂipping” mutation operator. In this case the degree of
reproductive variation was controlled by the amount of recombination and
the number of bits ﬂipped. The amount of recombination was controlled in
two ways: by specifying the percentage of oﬀspring to be produced using re-
combination, and the number of crossover!points to be used when performing
recombination (see, for example, [18]). Interestingly, there was less concern
about dynamically adapting these GA parameters, in part because the varia-
tion due to recombination decreases automatically as the population becomes
more homogeneous.
So, already with the early canonical EAs we see examples of parameters
whose values remained ﬁxed through out an EA run and parameters whose
values are dynamically modiﬁed during a run. Both of these approaches are
examined in more detail in the following sections.
4 Static Parameter Setting Strategies
The No Free Lunch results place an obligation on the EA practitioner to
understand something about the particular properties of the problems that
(s)he is trying to solve that relate to particular choices of EA parameter
settings. This poses a bit of a dilemma in that often an EA is being used
precisely because of a lack of knowledge about the ﬁtness landscapes being
explored. As a consequence, the parameter setting strategy frequently adopted
is a multiple run, human-in-the-loop approach in which various parameter
settings are tried in an attempt to ﬁne-tune an EA to a particular problem.
Since this can be a somewhat tedious process, it is not uncommon to au-
tomate this process with a simple, top-level “parameter sweep” procedure
that systematically adjusts selected parameter values. However, unless care is
taken, this can lead to a combinatorial explosion of parameter value combi-
nations and require large amounts of computation time. One possible escape
from this is to replace the parameter sweep procedure with a parameter op-
timization procedure. Which optimization procedure to use is an interesting
question. The ﬁtness landscapes deﬁned by EA parameter tuning generally
have the same unknown properties as the underlying problems that suggested
the use of EAs in the ﬁrst place! So, a natural consequence is the notion
of a two-level EA, the top level of which is evolving the parameters of the
lower level EA. The best example of this approach is the “nested” Evolution
Strategy in which a top-level ES evolves the parameters for a second-level ES
[35]. An interesting (and open question) is how to choose the parameters for
top-level EAs!

Parameter Setting in EAs: a 30 Year Perspective
5
Since exploring EA parameter space is generally very time consuming and
computationally expensive, it is done in a more general “oﬄine” setting. The
idea here is to ﬁnd EA parameter settings that are optimal for a class of
problems using a sweep or optimization procedure on a selected “test suite”
of sample problems, and then use the resulting parameter settings for all of
the problems of that class when encountered in practice (see, for example,
[18] or [34]). The key insight from such studies is the robustness of EAs with
respect to their parameter settings. Getting “in the ball park” is generally
suﬃcient for good EA performance. Stated another way, the EA parameter
“sweet spot” is reasonably large and easy to ﬁnd [18]. As a consequence most
EAs today come with a default set of static parameter values that have been
found to be quite robust in practice.
5 Dynamic Parameter Setting Strategies
A more diﬃcult thing to ascertain is whether there is any advantage to be
gained by dynamically changing the value of a parameter during an EA run
and, if so, how to change it. The intuitive motivation is that, as evolution
proceeds, it should be possible to use the accumulating information about the
ﬁtness landscape to improve future performance. The accumulating informa-
tion may relate to global properties of the ﬁtness landscape such as noise or
ruggedness, or it may relate to the local properties of a particular region of
the landscape. A second intuition is a sense of the need to change EA parame-
ters as it evolves from a more diﬀuse global search process to a more focused
converging local search process.
One way to accomplish this is to set up an a priori schedule of parame-
ter changes in a fashion similar to those used in simulating annealing [16].
This is diﬃcult to accomplish in general since it is not obvious how to predict
the number of generations an EA will take to converge, and set a parameter
adjustment schedule appropriately. A more successful approach is to monitor
particular properties of an evolutionary run, and use changes in these proper-
ties as a signal to change parameter values. A good example of this approach
is the 1/5 rule described in the previous section. More recently, the use of a
covariance matrix for mutation step size adaptation has been proposed [23].
Another example is the adaptive reproductive operator selection procedure
developed by Davis [17] in which reproductive operators are dynamically cho-
sen based on their recent performance.
The purist might argue that inventing feedback control procedures for EAs
is a good example of over-engineering an already sophisticated adaptive sys-
tem. A better strategy is to take our inspiration from nature and design our
EAs to be self-regulating. For example, individuals in the population might
contain “regulatory genes” that control mutation and recombination mecha-
nisms, and these regulatory genes would be subject to the same evolutionary
processes as the rest of the genome. Historically, the ES community has used

6
Kenneth De Jong
this approach as a way of independently controlling the mutation step size
σi of each objective parameter value xi [3]. Similar ideas have been used to
control the probability of choosing a particular mutation operator for ﬁnite
state machines [16]. Within the GA community Spears [76] has used a binary
control gene to determine which of two crossover operators to use.
These two approaches to setting parameters dynamically, using either an
ad hoc or a self-regulating control mechanism, have been dubbed “adaptive”
and “self-adaptive” in the literature [11].
Perhaps the most interesting thing to note today, after more than 30 years
of experimenting with dynamic parameter setting strategies, is that, with
one exception, none of them are used routinely in every day practice. The
one exception are the strategies used by the ES community for mutation
step size adaptation. In my opinion this is due is the fact that it is diﬃcult
to say anything deﬁnitive and general about the performance improvements
obtained through dynamic parameter setting strategies. There are two reasons
for this diﬃculty. First, our EAs are stochastic, non-linear algorithms. This
means that formal proofs are extremely diﬃcult to obtain, and experimental
studies must be carefully designed to provide statistically signiﬁcant results.
The second reason is that a legitimate argument can be made that comparing
the performance of an EA with static parameter settings to one with dynamic
settings is unfair since it is likely that the static settings were established via
some preliminary parameter tuning runs that have not been included in the
comparison.
My own view is that there is not much to be gained in dynamically adapt-
ing EA parameter settings when solving static optimization problems. The
real payoﬀfor dynamic parameter setting strategies is when the ﬁtness land-
scapes are themselves dynamic (see, for example, [4], [5], or [32]).
6 Choosing the EA Parameters to Set
An important aspect to EA parameter setting is a deeper understanding of
how changes in EA parameters aﬀect EA performance. Without that under-
standing it is diﬃcult to ascertain which parameters, if properly set, would
improve performance on a particular problem. This deeper understanding can
be obtained in several ways. The simplest approach is to study parameter
setting in the context of a particular family of EAs (e.g., ESs, GAs, etc.). The
advantage here is that they are well-studied and well-understood. For exam-
ple, there is a large body of literature in the ES community regarding the
role and the eﬀects of the parent population size µ, the oﬀspring population
size λ, and mutation step size σ. Similarly, the GA community has studied
population size m, various crossover operators, and mutation rates.
The disadvantage to this approach is that the EAs in use today seldom ﬁt
precisely into one of these canonical forms. Rather, they have evolved via re-
combinations and mutations of these original ideas. It would be helpful, then,

Parameter Setting in EAs: a 30 Year Perspective
7
to understand the role of various parameters at a higher level of abstraction.
This is something that I have been interested in for several years. The ba-
sis for this is a uniﬁed view of simple EAs [9]. From this perspective an EA
practitioner makes a variety of design choices, perhaps the most important
of which is representation. Having made that choice, there are still a num-
ber of additional decisions to be made that aﬀect EA performance, the most
important of which are:
• The size of the parent population m.
• The size of the oﬀspring population n.
• The procedure for selecting parents p select.
• The procedure for producing oﬀspring reproduction.
• The procedure for selecting survivors s select.
Most modern EA toolkits parameterize these decisions allowing one to choose
traditional parameter settings or create new and novel variations. So, for ex-
ample, a canonical (µ + λ)-ES would be speciﬁed as:
• m = µ
• n = λ
• p select = deterministic and uniform
• reproduction = clone and mutate
• s select = deterministic truncation
and a canonical GA would be speciﬁed as:
• m = n
• p select = probabilistic and ﬁtness-proportional
• reproduction = clone, recombine, and mutate
• s select = oﬀspring only
It is at this level of generality that we would like to understand the eﬀects of
parameter settings. We explore this possibility in the following subsections.
6.1 Parent Population Size m
From my perspective the parent population size m can be viewed as a measure
of the degree of parallel search that an EA supports, since the parent popula-
tion is the basis for generating new search points. For simple landscapes like
the 2-dimensional (inverted) parabola illustrated in Figure 1, little, if any,
parallelism is required since any sort of simple hill climbing technique will
provide reasonable performance.
By contrast, more complex, multi-peaked landscapes may require popula-
tions of 100s or even 1000s of parents in order to have some reasonable chance
of ﬁnding globally optimal solutions. As a simple illustration of this, consider
the 2-dimensional landscape deﬁned by the ﬁtness function f(x1, x2) = x2
1+x2
2
in which the variables x1 and x2 are constrained to the interval [−10, 5].

8
Kenneth De Jong
-10
-5
0
5
10
X
-10
-5
0
5
10
0
20
40
60
80
100
120
140
160
180
200
Fig. 1. A simple 2-dimensional (inverted) parabolic ﬁtness landscape.
-10
-8
-6
-4
-2
0
2
4
X
-10
-8
-6
-4
-2
0
2
4
0
50
100
150
200
Fig. 2. A 2-dimensional parabolic objective ﬁtness landscape with multiple peaks
and a unique maximum.

Parameter Setting in EAs: a 30 Year Perspective
9
This landscape has four peaks and a unique optimal ﬁtness value of 200 at
⟨−10, −10⟩as shown in Figure 2.
Regardless of which simple EA you choose, increasing m increases the prob-
ability of ﬁnding the global optimum. Figure 3 illustrates this for a standard
ES-style EA. What is plotted are best-so-far curves averaged over 100 runs
for increasing values of m while keeping n ﬁxed at 1 and using a non-adaptive
Gaussian mutation operator with an average step size of 1.0.
40
60
80
100
120
140
160
180
200
0
100
200
300
400
500
600
700
800
900
1000
Fitness: Average Best-so-far
Number of Births
ES(m=1,n=1)
ES(m=10,n=1)
ES(m=20,n=1)
Fig. 3. The eﬀects of parent population size on average best-so-far curves on a
multi-peaked landscape.
In this particular case, we see that the average behavior of ES(m, n = 1)
improves with increasing m, but at a decreasing rate. The same thing can be
shown for GA-like EAs and other non-traditional simple EAs.
For me, this is the simplest and clearest example of a relationship between
ﬁtness landscape properties and EA parameters, and provides useful insight
into possible strategies for choosing parent population size. At the beginning
of an EA run it is important to have suﬃcient parallelism to handle possible
multi-modalities, while at the end of a run an EA is likely to have converged
to a local area of the ﬁtness landscape that is no more complex than Figure 1.
That, in turn, suggests some sort of “annealing” strategy for parent population
size that starts with a large value that decreases over time. The diﬃculty is
in deﬁning a computational procedure to do so eﬀectively since it is diﬃcult
to predict a priori the time to convergence. One possibility is to set a ﬁxed

10
Kenneth De Jong
time limit to an EA run and deﬁne an annealing schedule for it a priori (e.g.,
[30]).
Studies of this sort conﬁrm our intuition about the usefulness of dynam-
ically adapting population size, but they achieve it by making (possibly)
suboptimal a priori decisions. Ideally, one would like parent population size
controlled by the current state of an EA. This has proved to be quite diﬃ-
cult to achieve (see, for example, [39], [2], or [12]). Part of the reason for this
is that there are other factors that aﬀect parent population size choices as
well, including the interacting eﬀects of oﬀspring population size (discussed in
the next section), the eﬀects of noisy ﬁtness landscapes [20], selection pressure
[13], and whether generations overlap or not [37]. As a consequence, most EAs
used in practice today run with a ﬁxed population size, the value of which
may be based on existing oﬀ-line studies (e.g., [34]) or more likely via manual
tuning over multiple runs.
6.2 Oﬀspring Population Size n
By contrast the oﬀspring population size n plays a quite diﬀerent role in a
simple EA. The current parent population reﬂects where in the solution space
an EA is focusing its search. The number of oﬀspring n generated reﬂects the
amount of exploration performed using the current parent population before
integrating the newly generated oﬀspring back into the parent population.
Stated in another way, this is the classic exploration-exploitation tradeoﬀthat
all search techniques face.
This eﬀect can be seen quite clearly if we keep the parent population size
m constant and increase the oﬀspring population size n. Figure 4 illustrates
this for the same ES-like algorithm used previously with m = 1. Notice how
increasing n on this multi-peaked landscape results in a decrease in average
performance unlike what we saw in the previous section with increases in m.
This raises an interesting EA question: should the parameters m and n
be coupled as they are in GAs and EP, or is it better to decouple them as
they are in ESs? Having fewer parameters to worry about is a good thing,
so is there anything to be gained by having two population size parameters?
There is considerable evidence to support an answer of “yes”, including a
variety of studies involving “steady-state” GAs (e.g., [42]) that have large m
and n = 1, and other simple EAs (e.g., [27]). However, just as we saw with
parent population size, deﬁning an eﬀective strategy for adapting the oﬀspring
population size during an EA run is quite diﬃcult (see, for example, [26]). As
a consequence, most EAs in use today run with a ﬁxed oﬀspring population
size, the default value of which is based on some oﬀ-line studies and possibly
manually tuned over several preliminary runs.

Parameter Setting in EAs: a 30 Year Perspective
11
40
60
80
100
120
140
160
180
200
0
100
200
300
400
500
600
700
800
900
1000
Fitness: Average Best-so-far
Number of Births
ES(m=1,n=1)
ES(m=1,n=10)
ES(m=1,n=20)
Fig. 4. The eﬀects of oﬀspring population size on average best-so-far curves on a
multi-peaked landscape.
7 Selection
Selection procedures are used in EAs in two diﬀerent contexts: as a procedure
for selecting parents to produce oﬀspring and as a procedure for deciding
which individuals “survive” into the next generation. It is quite clear that the
more “elitist” a selection algorithm is, the more an EA behaves like a local
search procedure (i.e, a hill-climber, a “greedy” algorithm) and is less likely
to converge to a global optimum. Just as we saw with parent population size
m, this suggests that selection!pressure should be adapted during an EA run,
weak at ﬁrst to allow for more exploration and then stronger towards the end
as an EA converges. However, the same diﬃculty arises here in developing
an eﬀective mechanism for modifying selection pressure as a function of the
current state of an EA. For example, although ﬁtness-proportional selection
does vary selection pressure over time, it does so by inducing stronger pressure
at the beginning of a run and very little at the end.
One source of diﬃculty here is that selection pressure is not as easy to “pa-
rameterize” as population size. We have a number of families of selection pro-
cedures (e.g, tournament selection, truncation selection, ﬁtness-proportional
selection, etc.) to choose from and a considerable body of literature analyzing
their diﬀerences (see, for example, [19] or [15]), but deciding which family to
choose or even which member of a parameterized family is still quite diﬃcult,
particularly because of the interacting eﬀects with population size [13].

12
Kenneth De Jong
Another interesting question is whether the selection algorithm chosen
for parent selection is in any way coupled with the choice made for survival
selection. The answer is a qualiﬁed “yes” in the following sense: the overall
selection pressure of an EA is due to the combined eﬀects of both selection
procedures. Hence, strong selection pressure for one of these (e.g, truncation
selection) is generally combined with weak selection pressure (e.g., uniform
selection) for the other. For example, standard ES and EP algorithms use
uniform parent selection and truncation survival selection, while standard
GAs use ﬁtness-proportional parent selection and uniform survival selection.
One additional “parameterization” of selection is often made available:
the choice between overlapping and non-overlapping generations. With non-
overlapping models, the entire parent population dies oﬀeach generation and
the oﬀspring only compete with each other for survival. Historical examples of
non-overlapping EAs include “generational GAs” and the “,” version of ESs.
The alternative is an overlapping-generation model such as a steady-state
GA, a (µ+λ)-ES, or any EP algorithm. In this case, parents and oﬀspring com-
pete with each other for survival. The eﬀects of this choice are quite clear. An
overlapping-generation EA behaves more like a local search algorithm, while
a non-overlapping-generation EA exhibits better global search properties [9].
Although an intriguing idea, I am unaware of any attempts to change this
parameter dynamically.
8 Reproductive Operators
Most EAs produce oﬀspring using two basic classes of reproductive mecha-
nisms: an asexual (single parent) mutation operator and a sexual (more than
one parent) recombination operator. Deciding if and how to parameterize this
aspect of an EA is a complex issue because the eﬀects of reproductive opera-
tors invariably interact with all of the other parameterized elements discussed
so far.
At a high level of abstraction reproductive operators introduce variation
into the population, counterbalancing the reduction in diversity due to selec-
tion. Intuitively, we sense that population diversity should start out high and
decrease over time, reﬂecting a shift from a more global search to a local one.
The diﬃculty is in ﬁnding an appropriate “annealing schedule” for population
diversity. If selection is too strong, convergence to a local optimum is highly
likely. If reproductive variation is too strong, the result is undirected random
search. Finding a balance between exploration and exploitation has been a
diﬃcult-to-achieve goal from the beginning [18].
What is clear is that there is no one single answer. ES and EP algorithms
typically match up strong selection pressure with strong reproductive varia-
tion, while GAs match up weaker selection pressure with weaker reproductive
variation. Finding a balance usually involves holding one of these ﬁxed (say,
selection pressure) and tuning the other (say, reproductive variation).

Parameter Setting in EAs: a 30 Year Perspective
13
This is complicated by the fact that what is really needed from reproduc-
tive operators is useful diversity. This was made clear early on in [33] via
the notion of ﬁtness correlation between parents and oﬀspring, and has been
shown empirically in a variety of settings (e.g., [31], [1], [28]). This in turn is a
function of the properties of the ﬁtness landscapes being explored which makes
it clear that the ability to dynamically improve ﬁtness correlation will improve
EA performance. Precisely how to achieve this is less clear. One approach is
to maintain a collection of plausible reproductive operators and dynamically
select the ones that seem to be helping most (see, for example, [17] or [16]). Al-
ternatively, one can focus on tuning speciﬁc reproductive operators to improve
performance. This is a somewhat easier task about which considerable work
has been done, and is explored in more detail in the following subsections.
8.1 Adapting Mutation
The classic one-parent reproductive mechanism is mutation that operates by
cloning a parent and then providing some variation by modifying one or more
genes in the oﬀspring’s genome. The amount of variation is controlled by
specifying how many genes are to be modiﬁed and the manner in which genes
are to be modiﬁed. These two aspects together determine both the amount
and usefulness of the resulting variation.
Although easy to parameterize, the expected number of modiﬁed genes
is seldom adapted dynamically. Rather, there are a number of studies that
suggest a ﬁxed value of 1 is quite robust, and is the default value for traditional
GAs. By contrast, traditional ES algorithms mutate every gene, typically by
a small amount. This seeming contradiction is clariﬁed by noting that GAs
make small changes in genotype space while the ES approach makes small
changes in phenotype space.
Both of these approaches allow for dynamic adaptation. The “1/5” rule
discussed earlier is a standard component of many ES algorithms. Adapting
GA mutation rates is not nearly as common, and is typically done via a self-
adaptive mechanism in which the mutation rate is encoded as a control gene
on individual genomes (see, for example, [3]).
Mutating gene values independently can be suboptimal when their eﬀects
on ﬁtness are coupled (i.e., epistatic non-linear interactions). Since these in-
teractions are generally not known a priori, but diﬃcult to determine dynam-
ically during an EA run. The most successful example of this is the pair-wise
covariance matrix approach used by the ES community [23]. Attempting to
detect higher order interactions is computational expensive and seldom done.
8.2 Adapting Recombination
The classic two-parent reproductive mechanism is a recombination operator
in which subcomponents of the parents’ genomes are cloned and reassembled

14
Kenneth De Jong
to create an oﬀspring genome. For simple ﬁxed-length linear genome repre-
sentations, the recombination operators have traditionally taken the form of
“crossover” operators, in which the crossover points mark the linear subseg-
ments on the parents’ genomes to be copied and reassembled. For these kinds
of recombination operators, the amount of variation introduced is dependent
on two factors: how many crossover points there are and how similar the
parents are to each other. The interesting implication of this is that, unlike
mutation, the amount of variation introduced by crossover diminishes over
time as selection makes the population more homogeneous.
This dependency on the contents of the population makes it much more
diﬃcult to estimate the level of crossover-induced variation, but has the virtue
of self-adapting along the lines of our intuition: more variation early in an EA
run and less variation as evolution proceeds. Providing useful variation is more
problematic in that, intuitively, one would like oﬀspring to inherit important
combinations of gene values from their parents. However, just as we saw for
mutation, which combinations of genes should be inherited is seldom known
a priori. For example, it is well-known that the 1-point crossover operator used
in traditional GAs introduces a distance bias in the sense that the values of
genes that are far apart on a chromosome are much less likely to be inherited
together than those of genes that are close together [25]. Simply increasing
the number of crossover points reduces that bias but increases the amount of
reproductive variation at the same time [10]. One elegant solution to this is to
switch to a parameterized version of uniform crossover. This simultaneously
removes the distance bias
and provides a single parameter for controlling
diversity [40]. Although an intriguing possibility, I am unaware of any current
EAs that dynamically adjust parameterized uniform crossover.
Alternatively, one might consider keeping a recombination operator ﬁxed
and adapting the representation in ways that improve the production of useful
diversity. This possibility is discussed in the next section.
9 Adapting the Representation
Perhaps the most diﬃcult and least understood area of EA design is that of
adapting its internal representation. It is clear that choices of representation
play an extremely important role in the performance of an EA, but are diﬃcult
to automate. As a ﬁeld we have developed a large body of literature that
helps us select representations a priori for particular classes of problems.
However, there are only a few examples of strategies for dynamically adapting
the representation during an EA run. One example is the notion of a “messy
GA” [21] in which the position of genes on the chromosome are adapted over
time to improve the eﬀectiveness 1-point crossover. Other examples focus on
adapting the range and/or resolution of the values of a gene (see, for example,
[38] or [82]). Both of these approaches have been shown to be useful in speciﬁc
contexts, but are not general enough to be included in today’s EA toolkits.

Parameter Setting in EAs: a 30 Year Perspective
15
10 Parameterless EAs
Perhaps the ultimate goal of these eﬀorts is to produce an eﬀective and general
problem-solving EA with no externally visible parameters. The No Free Lunch
discussion at the beginning of this chapter makes it clear that this will only
be achieved if there are eﬀective ways to dynamically adapt various internal
parameters. However, as we have seen throughout this chapter, there are very
few techniques that do so eﬀectively in a general setting for even one internal
parameter, much less for more than one simultaneously. The few examples
of parameterless EAs that exist in the literature involve simpliﬁed EAs in
particular contexts (e.g., [29]). To me this is a clear indication of the diﬃculty
of such a task.
An approach that appears more promising is to design an EA to perform
internal restarts, i.e. multiple runs, and use information from previous runs to
improve performance on subsequent (internal) runs. The most notable success
in this area is the CHC algorithm developed by [14]. Nested ESs are also quite
eﬀective and based on a similar ideas but still require a few external parameter
settings [35].
Clearly, we still have a long way to go in achieving the goal of eﬀective
parameterless EAs.
11 Summary
The focus in this chapter has be primarily of parameter setting in simple EAs
for two reasons. First, this is where most of the eﬀorts have been over the
past 30 or more years. Second, although many new and more complex EAs
have been developed (e.g., spatially distributed EAs, multi-population island
models, EAs with speciation and niching, etc.), these new EAs do little to
resolve existing parameter tuning issues. Rather, they generally exacerbate
the problem by creating new parameters that need to be set.
My sense is that, for static optimization problems, it will to continue to be
the case that particular types of EAs that have been pre-tuned for particular
classes of problems will continue to outperform EAs that try to adapt too
many things dynamically. However, if we switch our focus to time-varying
ﬁtness landscapes, dynamic parameter adaptation will have a much stronger
impact.
References
1. L. Altenberg. The schema theorem and Price’s theorem. In M. Vose and
D. Whitley, editors, Foundations of Genetic Algorithms 3, pages 23–49. Morgan
Kaufmann, 1994.

16
Kenneth De Jong
2. J. Arabas, Z. Michalewitz, and J. Mulawka. Gavaps - a genetic algorithm with
varying population size. In Proceedings of the First IEEE Conference on Evo-
lutionary Computation, pages 73–78. IEEE Press, 1994.
3. T. B¨ack. Evolutionary Algorithms in Theory and Practice. Oxford University
Press, New York, 1996.
4. T. B¨ack. On the behavior of evolutionary algorithms in dynamic ﬁtness land-
scapes. In IEEE International Conference on Evolutionary Computation, pages
446–451. IEEE Press, 1998.
5. J. Branke. Evolutionary Optimization in Dynamic Environments. Kluwer,
Boston, 2002.
6. L. Davis. Adapting operator probabilities in genetic algorithms. In Third Inter-
national Conference on Genetic Algorithms, pages 61–69. Morgan Kaufmann,
1989.
7. L. Davis. The Handbook of Genetic Algorithms. Van Nostrand Reinhold, New
York, 1991.
8. K. De Jong. Analysis of Behavior of a Class of Genetic Adaptive Systems. PhD
thesis, University of Michigan, Ann Arbor, MI, 1975.
9. K. De Jong. Evolutionary Computation: A Uniﬁed Approach. MIT Press, Cam-
bridge, MA, 2006.
10. K. De Jong and W. Spears. A formal analysis of the role of multi-point crossover
in genetic algorithms. Annals of Mathematics and Artiﬁcial Intelligence, 5(1):
1–26, 1992.
11. A. Eiben, R. Hinterding, and Z. Michalewicz. Parameter control in evolution-
ary algorithms. IEEE Transactions on Evolutionary Computation, 3(2):124–141,
1999.
12. A. Eiben, E. Marchiori, and V. Valko. Evolutionary algorithms with on-the-ﬂy
population size adjustment. In X. Yao et al., editor, Proceedings of PPSN VIII,
pages 41–50. Springer-Verlag, 2004.
13. A. Eiben, M. Schut, and A. de Wilde. Is self-adaptation of selection pressure and
population size possible? In T. Runarsson et al., editor, Proceedings of PPSN
VIII, pages 900–909. Springer-Verlag, 2006.
14. L. Eshelman. The CHC adaptive search algorithm. In G. Rawlins, editor, Foun-
dations of Genetic Algorithms 1, pages 265–283. Morgan Kaufmann, 1990.
15. S. Ficici and J. Pollack. Game–theoretic investigation of selection methods used
in evolutionary algorithms. In D. Whitley, editor, Proceedings of CEC 2000,
pages 880–887. IEEE Press, 2000.
16. L. Fogel, P. Angeline, and D. Fogel. An evolutionary programming approach
to self-adaptation on ﬁnite state machines. In J. McDonnell, R. Reynolds, and
D. Fogel, editors, Proceedings of the 4th Annual Conference on Evolutionary
Programming, pages 355–365. MIT Press, 1995.
17. L.J. Fogel, A.J. Owens, and M.J. Walsh. Artiﬁcial Intelligence through Simulated
Evolution. John Wiley & Sons, New York, 1966.
18. D. Goldberg. The Design of Innovation: Lessons from and for Competent Ge-
netic Algorithms. Kluwer, Boston, 2002.
19. D. Goldberg and K. Deb. A comparative analysis of selection schemes used in
genetic algorithms. In G. Rawlins, editor, Proceedings of the First Workshop on
Foundations of Genetic Algorithms, pages 69–92. Morgan Kaufmann, 1990.
20. D. Goldberg, K. Deb, and J. Clark. Accounting for noise in sizing of populations.
In D. Whitley, editor, Foundations of Genetic Algorithms 2, pages 127–140.
Morgan Kaufmann, 1992.

Parameter Setting in EAs: a 30 Year Perspective
17
21. D. Goldberg, K. Deb, and B. Korb. Don’t worry, be messy. In R. K. Belew
and L. B. Booker, editors, Proceedings of the Fourth International Conference
on Genetic Algorithms, pages 24–30. Morgan Kaufmann, 1991.
22. J. Grefenstette. Optimization of control parameters for genetic algorithms.
IEEE Transactions on Systems, Man, and Cybernetics, 16(1):122–128, 1986.
23. N. Hansen and A. Ostermeier. Completely derandomized step-size adaptation
in evolution strategies. Evolutionary Computation, 9(2):159–195, 2001.
24. J.H. Holland. Outline for a logical theory of adaptive systems. JACM, 9:297–
314, 1962.
25. J.H. Holland. Adaptation in Natural and Artiﬁcial Systems. University of Michi-
gan Press, Ann Arbor, MI, 1975.
26. T. Jansen, K. De Jong, and I. Wegener. On the choice of oﬀspring population
size in evolutionary algorithms. Evolutionary Computation, 13(4):413–440, 2005.
27. Thomas Jansen and Kenneth De Jong. An analysis of the role of oﬀspring popu-
lation size in EAs. In W. B. Langdon et al., editor, Proceedings of the Genetic and
Evolutionary Computation Conference (GECCO 2002), pages 238–246. Morgan
Kaufman, 2002.
28. T. Jones. Evolutionary algorithms, ﬁtness landscapes, and search. PhD thesis,
University of New Mexico, 1995.
29. C. Lima and F. Lobo. Parameter-less optimization with the extended compact
genetic algorithm and iterated local search. In K. Deb et al, editor, Proceedings
of GECCO-2004, pages 1328–1339. Springer-Verlag, 2004.
30. S. Luke, G. Balan, and L. Panait. Population implosion in genetic programming.
In Proceedings of GECCO-2003, volume 2724, pages 1729–1739. Springer LNCS
Series, 2003.
31. B. Manderick, M. de Weger, and P. Spiessens. The genetic algorithm and the
structure of the ﬁtness landscape. In R. K. Belew and L. B. Booker, editors, The
Fourth International Conference on Genetic Algorithms and Their Applications,
pages 143–150. Morgan Kaufmann, 1991.
32. R. Morrison. Designing Evolutionary Algorithms for Dynamic Environments.
Springer-Verlag, Berlin, 2004.
33. George Price. Selection and covariance. Nature, 227:520–521, 1970.
34. I. Rechenberg. Cybernatic solution path of an experimental problem. In Library
Translation 1122. Royal Aircraft Establishment, Farnborough, 1965.
35. I. Rechenberg. Evolutionsstrategie ’94. Frommann-Holzboog, Stuttgart, 1994.
36. H.-P. Schwefel. Evolutionsstrategie und numerishe Optimierung. PhD thesis,
Technical University of Berlin, Berlin, Germany, 1975.
37. H.-P. Schwefel. Evolution and Optimum Seeking. John Wiley & Sons, New York,
1995.
38. C. Shaefer. The ARGOT strategy: adaptive representation genetic optimizer
technique. In J. Grefenstette, editor, Proceedings of the Second International
Conference on Genetic Algorithms, pages 50–58. Lawrence Erlbaum, 1987.
39. R. Smith. Adaptively resizing populations: an algorithm and analysis. In
S. Forrest, editor, Proceedings of the Fifth International Conference on Genetic
Algorithms and their Applications, page 653. Morgan Kaufmann, 1993.
40. W. Spears and K. De Jong. On the virtues of parameterized uniform crossover.
In R. K. Belew and L. B. Booker, editors, International Conference on Genetic
Algorithms, volume 4, pages 230–236. Morgan Kaufmann, 1991.

18
Kenneth De Jong
41. W.M. Spears. Adapting crossover in evolutionary algorithms. In J. McDonnell,
R. Reynolds, and D.B. Fogel, editors, Proceedings of the Fourth Conference on
Evolutionary Programming, pages 367–384. MIT Press, 1995.
42. D. Whitley.
The Genitor algorithm and selection pressure: Why rank-based
allocation of reproductive trials is best. In J.D. Schaﬀer, editor, Proceedings
of the Third International Conference on Genetic Algorithms, pages 116–121.
Morgan Kaufmann, 1989.
43. D. Whitley, K. Mathias, and P. Fitzhorn.
Delta coding: an iterative search
strategy for genetic algorithms.
In R. K. Belew and L. B. Booker, editors,
Proceeding of the Fourth International Conference on Genetic Algorithms, pages
77–84. Morgan Kaufmann, 1991.
44. D. Wolpert and W. Macready. No free lunch theorems for optimization. IEEE
Transactions on Evolutionary Computation, 1:67–82, 1997.

Parameter Control in Evolutionary Algorithms
A.E. Eiben1, Z. Michalewicz2, M. Schoenauer3, and J.E. Smith4
1 Free University Amsterdam, The Netherlands gusz@cs.vu.nl
2 University of Adelaide, Australia zbyszek@cs.adelaide.edu.au
3 INRIA, France marc@lri.fr
4 UWE, United Kingdom james.smith@uwe.ac.uk
Summary. The issue of setting the values of various parameters of an evolutionary
algorithm is crucial for good performance. In this paper we discuss how to do this,
beginning with the issue of whether these values are best set in advance or are best
changed during evolution. We provide a classiﬁcation of diﬀerent approaches based
on a number of complementary features, and pay special attention to setting para-
meters on-the-ﬂy. This has the potential of adjusting the algorithm to the problem
while solving the problem.
This paper is intended to present a survey rather than a set of prescriptive details
for implementing an EA for a particular type of problem. For this reason we have
chosen to interleave a number of examples throughout the text. Thus we hope to
both clarify the points we wish to raise as we present them, and also to give the
reader a feel for some of the many possibilities available for controlling diﬀerent
parameters.
1 Introduction
Finding the appropriate setup for an evolutionary algorithm is a long standing
grand challenge of the ﬁeld [22, 25]. The main problem is that the description
of a speciﬁc EA contains its components, such as the choice of representation,
selection, recombination, and mutation operators, thereby setting a frame-
work while still leaving quite a few items undeﬁned. For instance, a simple
GA might be given by stating it will use binary representation, uniform cross-
over, bit-ﬂip mutation, tournament selection, and generational replacement.
For a full speciﬁcation, however, further details have to be given, for instance,
the population size, the probability of mutation pm and crossover pc, and the
tournament size. These data – called the algorithm parameters or strat-
egy parameters – complete the deﬁnition of the EA and are necessary to
produce an executable version. The values of these parameters greatly deter-
mine whether the algorithm will ﬁnd an optimal or near-optimal solution, and
whether it will ﬁnd such a solution eﬃciently. Choosing the right parameter
values is, however, a hard task.
A.E. Eiben et al.: Parameter Control in Evolutionary Algorithms, Studies in Computational
Intelligence (SCI) 54, 19–46 (2007)
www.springerlink.com
© Springer-Verlag Berlin Heidelberg 2007

20
A.E. Eiben, Z. Michalewicz, M. Schoenauer, and J.E. Smith
Globally, we distinguish two major forms of setting parameter values: pa-
rameter tuning and parameter control. By parameter tuning we mean
the commonly practised approach that amounts to ﬁnding good values for the
parameters before the run of the algorithm and then running the algorithm
using these values, which remain ﬁxed during the run. Later on in this section
we give arguments that any static set of parameters having the values ﬁxed
during an EA run seems to be inappropriate. Parameter control forms an al-
ternative, as it amounts to starting a run with initial parameter values that
are changed during the run.
Parameter tuning is a typical approach to algorithm design. Such tuning
is done by experimenting with diﬀerent values and selecting the ones that give
the best results on the test problems at hand. However, the number of possible
parameters and their diﬀerent values means that this is a very time-consuming
activity. Considering four parameters and ﬁve values for each of them, one has
to test 54 = 625 diﬀerent setups. Performing 100 independent runs with each
setup, this implies 62,500 runs just to establish a good algorithm design.
The technical drawbacks to parameter tuning based on experimentation
can be summarised as follows:
• Parameters are not independent, but trying all diﬀerent combinations
systematically is practically impossible.
• The process of parameter tuning is time consuming, even if parameters
are optimised one by one, regardless of their interactions.
• For a given problem the selected parameter values are not necessarily
optimal, even if the eﬀort made for setting them was signiﬁcant.
This picture becomes even more discouraging if one is after a “generally
good” setup that would perform well on a range of problems or problem in-
stances. During the history of EAs considerable eﬀort has been spent on ﬁnd-
ing parameter values (for a given type of EA, such as GAs), that were good
for a number of test problems. A well-known early example is that of [18], de-
termining recommended values for the probabilities of single-point crossover
and bit mutation on what is now called the DeJong test suite of ﬁve functions.
About this and similar attempts [34, 62], it should be noted that genetic algo-
rithms used to be seen as robust problem solvers that exhibit approximately
the same performance over a wide range of problems [33, page 6]. The contem-
porary view on EAs, however, acknowledges that speciﬁc problems (problem
types) require speciﬁc EA setups for satisfactory performance [12]. Thus, the
scope of “optimal” parameter settings is necessarily narrow. There are also
theoretical arguments that any quest for generally good EA, thus generally
good parameter settings, is lost a priori, such as the No Free Lunch theorem
[83].
To elucidate another drawback of the parameter tuning approach recall
how we deﬁned it: ﬁnding good values for the parameters before the run of the
algorithm and then running the algorithm using these values, which remain
ﬁxed during the run. However, a run of an EA is an intrinsically dynamic,

Parameter Control in Evolutionary Algorithms
21
adaptive process. The use of rigid parameters that do not change their values
is thus in contrast to this spirit. Additionally, it is intuitively obvious, and
has been empirically and theoretically demonstrated, that diﬀerent values of
parameters might be optimal at diﬀerent stages of the evolutionary process
[6, 7, 8, 16, 39, 45, 63, 66, 68, 72, 73, 78, 79].
To give an example, large mutation steps can be good in the early gener-
ations, helping the exploration of the search space, and small mutation steps
might be needed in the late generations to help ﬁne-tune the suboptimal
chromosomes. This implies that the use of static parameters itself can lead to
inferior algorithm performance.
A straightforward way to overcome the limitations of static parameters
is by replacing a parameter p by a function p(t), where t is the generation
counter (or any other measure of elapsed time). However, as indicated earlier,
the problem of ﬁnding optimal static parameters for a particular problem is
already hard. Designing optimal dynamic parameters (that is, functions for
p(t)) may be even more diﬃcult. Another possible drawback to this approach
is that the parameter value p(t) changes are caused by a “blind” deterministic
rule triggered by the progress of time t, without taking any notion of the actual
progress in solving the problem, i.e., without taking into account the current
state of the search. A well-known instance of this problem occurs in simulated
annealing [49] where a so-called cooling schedule has to be set before the
execution of the algorithm.
Mechanisms for modifying parameters during a run in an “informed” way
were realised quite early in EC history. For instance, evolution strategies
changed mutation parameters on-the-ﬂy by Rechenberg’s 1/5 success rule
using information on the ratio of successful mutations. Davis experimented
within GAs with changing the crossover rate based on the progress realised
by particular crossover operators [16]. The common feature of these and simi-
lar approaches is the presence of a human-designed feedback mechanism that
utilises actual information about the search process for determining new pa-
rameter values.
Yet another approach is based on the observation that ﬁnding good para-
meter values for an evolutionary algorithm is a poorly structured, ill-deﬁned,
complex problem. This is exactly the kind of problem on which EAs are often
considered to perform better than other methods. It is thus a natural idea
to use an EA for tuning an EA to a particular problem. This could be done
using two EAs: one for problem solving and another one – the so-called meta-
EA – to tune the ﬁrst one [32, 34, 48]. It could also be done by using only
one EA that tunes itself to a given problem, while solving that problem. Self-
adaptation, as introduced in Evolution Strategies for varying the mutation
parameters, falls within this category. In the next section we discuss various
options for changing parameters, illustrated by an example.

22
A.E. Eiben, Z. Michalewicz, M. Schoenauer, and J.E. Smith
2 A case study: Evolution Strategies
The history of Evolution Strategies (ES) is a typical case study for parameter
tuning, as it went through several successive steps pertaining to many of the
diﬀerent approaches listed so far.
Typical ES work in a real-valued search space (typically IRn, or a subset
of IRn, for some integer n).
2.1 Gaussian Mutations
The main operator (and almost the trademark) of ES is the Gaussian mu-
tation, that adds centered normally distributed noise to the variables of the
individuals. The most general Gaussian distribution in IRn is the multivariate
normal distribution N(m, C), with mean m and covariance matrix C, a
n × n positive deﬁnite matrix, that has the following Probability Distribution
Function
Φ(X) = exp(−1
2(X −m)tC−1(X −m))

(2π)n|C|
where |C| is the determinant of C.
It is then convenient to write the mutation of a vector X ∈IRn as
X →X + σN(0, C)
i.e. to distinguish a scaling factor σ, also called the step-size, from the direc-
tions of the Gaussian distribution given by the covariance matrix C.
For example, the simplest case of Gaussian mutation assumes that C is the
identity matrix in IRn (the diagonal matrix that has only 1s on the diagonal).
In this case, all coordinates will be mutated independently, and will have
added to them some Gaussian noise with variance σ2.
Tuning an ES algorithm therefore amounts to tuning the step-size and
the covariance matrix – or simply tuning the step-size in the simple case
mentioned above.
2.2 Adapting the Step-Size
The step-size of the Gaussian mutation gives the scale of the search. To make
things clear, suppose that you are minimizing x2 in one dimension, running a
(1+1)-ES (one parent gives birth to one oﬀspring, and the best of both is the
next parent) with a ﬁxed step-size σ. Then the average distance between par-
ent and successful oﬀspring is proportional to σ. This has two consequences:
ﬁrst, starting from distance d0 from the solution, it will take an average of
d0/σ steps to reach a region close to the optimum. On the other hand, when
hovering around the optimum, the precision you can hope is again propor-
tional to σ. Those arguments naturally lead to the optimal adaptive setting

Parameter Control in Evolutionary Algorithms
23
of the step-size for the sphere function: σ should be proportional to the dis-
tance to the optimum. Details can be found in the studies of the so-called
progress rate: early work was done by Schwefel [66], completed and extended
by Beyer and recent work by Auger [5] gave a formal global convergence proof
of this . . . impractical algorithm: indeed, the distance to the optimum is not
known real situations!
But another piece of information is always available to the algorithm: the
success rate (the proportion of successful mutations, where the oﬀspring is
better than the parent). This can indirectly give information about the step-
size: this was Rechenberg’s main idea to propose the ﬁrst practical method for
an adaptive step-size, the so-called one-ﬁfth rule: if the success rate over some
time window is larger than the success rate when the step-size is optimal (0.2,
or one-ﬁfth!), the the step-size should be increased (the algorithm is making
too many small steps); on the other hand, if the success rate is smaller than
0.2, then the step-size should be decreased (the algorithm is constantly missing
the target, because it shoots too far). Though formally derived from studies
on the sphere function and the corridor function (a bounded linear function),
the one-ﬁfth rule was generalized to any function.
However, there are many situations where the one-ﬁfth rule can be mislead.
Moreover, it does not in any way handle the case of non-isotropic functions,
where a non-diagonal covariance matrix is mandatory. Hence, it is no longer
used today.
2.3 Self-Adaptive ES
The next big step in ESs was the invention of the self-adaptive mutation:
the parameters of the mutation (both the step-size and the covariance matrix)
are attached to each individual, and are subject to mutation, too. Those per-
sonal mutation parameters range from a single step-size, leading to isotropic
mutation, where all coordinates are mutated independently with the same
variance, to the non-isotropic mutation, that use a vector of n “standard de-
viations” σi, equivalent to a diagonal matrix C with σi on the diagonal, and
to the correlated mutations, where a full covariance matrix is attached to each
individual. Mutating an individual then amounts to ﬁrst mutating the muta-
tion parameters themselves, and then mutating the variables using the new
mutation parameters. Details can be found in [66, 13].
The rationale for SA-ES are that algorithm relies on the selection step to
keep in the population not only the best ﬁt individuals, but also the individuals
with the best mutation parameters – according to the region of the search
space they are in. Indeed, although the selection acts based on the ﬁtness, the
underlying idea beneath Self-Adaptive ES (SA-ES) is that if two individuals
starts with the same ﬁtness, the oﬀspring of one that has “better” mutation
parameters will reach regions of higher ﬁtness faster than the oﬀspring of the
other: selection will hence keep the ones with the good mutation parameters.
This is what has often been stated as “mutation parameters are optimized for

24
A.E. Eiben, Z. Michalewicz, M. Schoenauer, and J.E. Smith
free” by the evolution itself. And indeed, SA-ES have long been the state-of-
the-art in parametric optimization [9].
But what are “good” mutation parameters? The issue has already been dis-
cussed for the step-size in previous section, and similar arguments can be given
for the covariance matrix itself. Replace the sphere model (min  x2
i ≡XtX)
with an elliptic function (min 1
2XtHX for some positive deﬁnite matrix H).
Then it is clear that the mutation should progress slower along the directions
of steepest descent of H: the covariance matrix should be proportional to H−1.
And whereas the step-size actually self-adapts to quasi-optimal values [9, 19],
the covariance matrix that is learned by the correlated SA-ES is not the actual
inverse of the Hessian [4].
2.4 CMA-ES: a Clever Adaptation
Another defect of SA-ES is the relative slowness of adaptation of the mutation
parameters: even for the simple case of the step-size, if the initial value is not
the optimal one (proportional to the distance to the optimum in the case of
the sphere function), it takes some time to reach that optimal value and to
start being eﬃcient.
This observation led Hansen and Ostermeier to propose deterministic
schedules to adapt the mutation parameters in ES, hence heading back to
an adaptive method for parameter tuning. Their method was ﬁrst limited
to the step-size [38], and later addressed the adaptation of the full covariance
matrix [36]. The complete Covariance Matrix Adaptation (CMA-ES) algo-
rithm was ﬁnally detailed (and its parameters carefully tuned) in [37] and an
improvement for the update of the covariance matrix was proposed in [35].
The basic idea in CMA-ES is to use the path followed by the algorithm to
deterministically update the diﬀerent mutation parameters, and a simpliﬁed
view is given by the following: suppose that the algorithm has made a series
of steps in colinear directions; then the step-size should be increased, to allow
larger steps and increase speed. Similar ideas undermine the covariance matrix
update(s).
Indeed, using such clever learning method, CMA-ES proved to outperform
most other stochastic algorithms for parametric optimization, as witnessed by
its success in the 2005 contest that took place at CEC’2005.
2.5 Lessons Learned
This brief summary of ES history witnesses that
• Static parameters are not only hard but can be impossible to tune: there
doesn’t exist any good static value for the step-size in Gaussian mutation
• Adaptive methods use some information about the current state of the
search, and are as good as the information they get: the success rate is a
very raw information, and lead to the “easy-to-defeat” one-ﬁfth rule, while

Parameter Control in Evolutionary Algorithms
25
CMA-ES uses high-level information to cleverly update all the parameters
of the most general Gaussian mutation
• Self-adaptive methods are eﬃcient methods when applicable, i.e. when the
only available selection (based on the ﬁtness) can prevent bad parameters
from proceeding to future generations. They outperform basic static and
adaptive methods, but are outperformed by clever adaptive methods.
3 Case Study: Changing the Penalty Coeﬃcients
Let us assume we deal with a numerical optimisation problem to minimise
f(x) = f(x1, . . . , xn),
subject to some inequality and equality constraints
gi(x) ≤0, i = 1, . . . , q,
and
hj(x) = 0, j = q + 1, . . . , m,
where the domains of the variables are given by lower and upper bounds
li ≤xi ≤ui for 1 ≤i ≤n.
For such a numerical optimisation problem we may consider an evolution-
ary algorithm based on a ﬂoating-point representation, where each individ-
ual x in the population is represented as a vector of ﬂoating-point numbers
x = ⟨x1, . . . , xn⟩.
In the previous section we described diﬀerent ways to modify a parameter
controlling mutation. Several other components of an EA have natural para-
meters, and these parameters are traditionally tuned in one or another way.
Here we show that other components, such as the evaluation function (and
consequently the ﬁtness function) can also be parameterised and thus var-
ied. While this is a less common option than tuning mutation (although it is
practised in the evolution of variable-length structures for parsimony pressure
[84]), it may provide a useful mechanism for increasing the performance of an
evolutionary algorithm.
When dealing with constrained optimisation problems, penalty functions
are often used. A common technique is the method of static penalties [60],
which requires ﬁxed user-supplied penalty parameters. The main reason for
its widespread use is that it is the simplest technique to implement: it requires
only the straightforward modiﬁcation of the evaluation function as follows:
eval(x) = f(x) + W · penalty(x),
where f is the objective function, and penalty(x) is zero if no violation oc-
curs, and is positive,5 otherwise. Usually, the penalty function is based on the
5 For minimisation problems.

26
A.E. Eiben, Z. Michalewicz, M. Schoenauer, and J.E. Smith
distance of a solution from the feasible region, or on the eﬀort to “repair” the
solution, i.e., to force it into the feasible region. In many methods a set of
functions fj (1 ≤j ≤m) is used to construct the penalty, where the function
fj measures the violation of the jth constraint in the following way:
fj(x) =
 max{0, gj(x)} if
1 ≤j ≤q,
|hj(x)|
if
q + 1 ≤j ≤m.
(1)
W is a user-deﬁned weight, prescribing how severely constraint violations are
weighted. In the most traditional penalty approach the weight W does not
change during the evolution process. We sketch three possible methods of
changing the value of W.
First, we can replace the static parameter W by a dynamic parameter,
e.g., a function W(t). Just as for the mutation parameter σ, we can develop a
heuristic that modiﬁes the weight W over time. For example, in the method
proposed by Joines and Houck [46], the individuals are evaluated (at the
iteration t) by a formula, where
eval(x) = f(x) + (C · t)α · penalty(x),
where C and α are constants. Since
W(t) = (C · t)α,
the penalty pressure grows with the evolution time provided 1 ≤C, α.
Second, let us consider another option, which utilises feedback from the
search process. One example of such an approach was developed by Bean and
Hadj-Alouane [14], where each individual is evaluated by the same formula as
before, but W(t) is updated in every generation t in the following way:
W(t + 1) =
⎧
⎪
⎨
⎪
⎩
(1/β1) · W(t)
if b
i ∈F
for all
t −k + 1 ≤i ≤t,
β2 · W(t)
if b
i ∈S−F for all
t −k + 1 ≤i ≤t,
W(t)
otherwise.
In this formula, S is the set of all search points (solutions), F ⊆S is a set of all
feasible solutions, b
i denotes the best individual in terms of the function eval
in generation i, β1, β2 > 1, and β1 ̸= β2 (to avoid cycling). In other words,
the method decreases the penalty component W(t + 1) for the generation
t + 1 if all best individuals in the last k generations were feasible (i.e., in
F), and increases penalties if all best individuals in the last k generations
were infeasible. If there are some feasible and infeasible individuals as best
individuals in the last k generations, W(t + 1) remains without change.
Third, we could allow self-adaptation of the weight parameter, similarly
to the mutation step sizes in the previous section. For example, it is possible
to extend the representation of individuals into
⟨x1, . . . , xn, W⟩,

Parameter Control in Evolutionary Algorithms
27
where W is the weight. The weight component W undergoes the same changes
as any other variable xi (e.g., Gaussian mutation and arithmetic recombina-
tion).
To illustrate this method,which is analogous to using a separate σi for each
xi, we need to redeﬁne the evaluation function. Let us ﬁrst introduce penalty
functions for each constraint as per Eq. (1). Clearly, these penalties are all
non-negative and are at zero if no constraints are violated. Then consider a
vector of weights w = (w1, . . . , wm), and deﬁne
eval(x) = f(x) +
m

j=1
wjfj(x),
as the function to be minimised and also extend the representation of indi-
viduals into
⟨x1, . . . , xn, w1, . . . , wm⟩.
Variation operators can then be applied to both the x and the w part of
these chromosomes, realising a self-adaptation of the constraint weights, and
thereby the ﬁtness function.
It is important to note the crucial diﬀerence between self-adapting mu-
tation step sizes and constraint weights. Even if the mutation step sizes are
encoded in the chromosomes, the evaluation of a chromosome is independent
from the actual σ values. That is,
eval(⟨x, σ⟩) = f(x),
for any chromosome ⟨x, σ⟩. In contrast, if constraint weights are encoded in
the chromosomes, then we have
eval(⟨x, w⟩) = fw(x),
for any chromosome ⟨x, W⟩. This could enable the evolution to “cheat” in the
sense of making improvements by minimising the weights instead of optimising
f and satisfying the constraints. Eiben et al. investigated this issue in [22] and
found that using a speciﬁc tournament selection mechanism neatly solves this
problem and enables the EA to solve constraints.
3.1 Summary
In the previous sections we illustrated how the mutation operator and the eval-
uation function can be controlled (adapted) during the evolutionary process.
The latter case demonstrates that not only can the traditionally adjusted
components, such as mutation, recombination, selection, etc., be controlled
by parameters, but so can other components of an evolutionary algorithm.
Obviously, there are many components and parameters that can be changed
and tuned for optimal algorithm performance. In general, the three options we

28
A.E. Eiben, Z. Michalewicz, M. Schoenauer, and J.E. Smith
sketched for the mutation operator and the evaluation function are valid for
any parameter of an evolutionary algorithm, whether it is population size,
mutation step, the penalty coeﬃcient, selection pressure, and so forth.
The mutation example above also illustrates the phenomenon of the scope
of a parameter. Namely, the mutation step size parameter can have diﬀerent
domains of inﬂuence, which we call scope. Using the ⟨x1, . . . , xn, σ1, . . . , σn⟩
model, a particular mutation step size applies only to one variable of a single
individual. Thus, the parameter σi acts on a subindividual, or component,
level. In the ⟨x1, . . . , xn, σ⟩representation, the scope of σ is one individual,
whereas the dynamic parameter σ(t) was deﬁned to aﬀect all individuals and
thus has the whole population as its scope.
These remarks conclude the introductory examples of this section. We
are now ready to attempt a classiﬁcation of parameter control techniques for
parameters of an evolutionary algorithm.
4 Classiﬁcation of Control Techniques
In classifying parameter control techniques of an evolutionary algorithm, many
aspects can be taken into account. For example:
1. What is changed? (e.g., representation, evaluation function, operators, se-
lection process, mutation rate, population size, and so on)
2. How the change is made (i.e., deterministic heuristic, feedback-based
heuristic, or self-adaptive)
3. The evidence upon which the change is carried out (e.g., monitoring per-
formance of operators, diversity of the population, and so on)
4. The scope/level of change (e.g., population-level, individual-level, and so
forth)
In the following we discuss these items in more detail.
4.1 What is Changed?
To classify parameter control techniques from the perspective of what com-
ponent or parameter is changed, it is necessary to agree on a list of all major
components of an evolutionary algorithm, which is a diﬃcult task in itself.
For that purpose, let us assume the following components of an EA:
• Representation of individuals
• Evaluation function
• Variation operators and their probabilities
• Selection operator (parent selection or mating selection)
• Replacement operator (survival selection or environmental selection)
• Population (size, topology, etc.)

Parameter Control in Evolutionary Algorithms
29
Note that each component can be parameterised, and that the number of
parameters is not clearly deﬁned. For example, an oﬀspring v produced by an
arithmetical crossover of k parents x1, . . . , xk can be deﬁned by the following
formula:
v = a1x1 + . . . + akxk,
where a1, . . . , ak, and k can be considered as parameters of this crossover. Pa-
rameters for a population can include the number and sizes of subpopulations,
migration rates, and so on for a general case, when more then one population
is involved. Despite the somewhat arbitrary character of this list of compo-
nents and of the list of parameters of each component, we will maintain the
“what-aspect” as one of the main classiﬁcation features, since this allows us
to locate where a speciﬁc mechanism has its eﬀect.
4.2 How are Changes Made?
As discussed and illustrated in the two earlier case studies, methods for chang-
ing the value of a parameter (i.e., the “how-aspect”) can be classiﬁed into one
of three categories.
• Deterministic parameter control
This takes place when the value of a strategy parameter is altered by some
deterministic rule. This rule modiﬁes the strategy parameter in a ﬁxed,
predetermined (i.e., user-speciﬁed) way without using any feedback from
the search. Usually, a time-varying schedule is used, i.e., the rule is used
when a set number of generations have elapsed since the last time the rule
was activated.
• Adaptive parameter control
This takes place when there is some form of feedback from the search
that serves as inputs to a mechanism used to determine the direction or
magnitude of the change to the strategy parameter. The assignment of
the value of the strategy parameter may involve credit assignment, based
on the quality of solutions discovered by diﬀerent operators/parameters,
so that the updating mechanism can distinguish between the merits of
competing strategies. Although the subsequent action of the EA may de-
termine whether or not the new value persists or propagates throughout
the population, the important point to note is that the updating mecha-
nism used to control parameter values is externally supplied, rather than
being part of the “standard” evolutionary cycle.
• Self-adaptive parameter control
The idea of the evolution of evolution can be used to implement the self-
adaptation of parameters (see [10] for a good review). Here the parameters
to be adapted are encoded into the chromosomes and undergo mutation
and recombination. The better values of these encoded parameters lead to
better individuals, which in turn are more likely to survive and produce
oﬀspring and hence propagate these better parameter values. This is an

30
A.E. Eiben, Z. Michalewicz, M. Schoenauer, and J.E. Smith
important distinction between adaptive and self-adaptive schemes: in the
latter the mechanisms for the credit assignment and updating of diﬀerent
strategy parameters are entirely implicit, i.e., they are the selection and
variation operators of the evolutionary cycle itself.
This terminology leads to the taxonomy illustrated in Fig. 1.
before the run
during the run
Parameter setting
Parameter tuning
Parameter control
Deterministic
Adaptive
Self−adaptive
Fig. 1. Global taxonomy of parameter setting in EAs
Some authors have introduced a diﬀerent terminology. Angeline [2] dis-
tinguished “absolute” and “empirical” rules, which correspond to the “un-
coupled” and “tightly-coupled” mechanisms of Spears [76]. Let us note that
the uncoupled/absolute category encompasses deterministic and adaptive
control, whereas the tightly-coupled/empirical category corresponds to self-
adaptation. We feel that the distinction between deterministic and adaptive
parameter control is essential, as the ﬁrst one does not use any feedback from
the search process. However, we acknowledge that the terminology proposed
here is not perfect either. The term “deterministic” control might not be the
most appropriate, as it is not determinism that matters, but the fact that
the parameter-altering transformations take no input variables related to the
progress of the search process. For example, one might randomly change the
mutation probability after every 100 generations, which is not a deterministic
process. The name “ﬁxed” parameter control might provide an alternative
that also covers this latter example. Also, the terms “adaptive” and “self-
adaptive” could be replaced by the equally meaningful “explicitly adaptive”
and “implicitly adaptive” controls, respectively. We have chosen to use “adap-
tive” and “self-adaptive” for the widely accepted usage of the latter term.
4.3 What Evidence Informs the Change?
The third criterion for classiﬁcation concerns the evidence used for determin-
ing the change of parameter value [67, 74]. Most commonly, the progress of
the search is monitored, e.g., by looking at the performance of operators, the
diversity of the population, and so on. The information gathered by such a
monitoring process is used as feedback for adjusting the parameters. From

Parameter Control in Evolutionary Algorithms
31
this perspective, we can make further distinction between the following two
cases:
• Absolute evidence
We speak of absolute evidence when the value of a strategy parameter
is altered by some rule that is applied when a predeﬁned event occurs.
The diﬀerence from deterministic parameter control lies in the fact that
in deterministic parameter control a rule ﬁres by a deterministic trigger
(e.g., time elapsed), whereas here feedback from the search is used. For
instance, the rule can be applied when the measure being monitored hits a
previously set threshold – this is the event that forms the evidence. Exam-
ples of this type of parameter adjustment include increasing the mutation
rate when the population diversity drops under a given value [53], chang-
ing the probability of applying mutation or crossover according to a fuzzy
rule set using a variety of population statistics [52], and methods for resiz-
ing populations based on estimates of schemata ﬁtness and variance [75].
Such mechanisms require that the user has a clear intuition about how
to steer the given parameter into a certain direction in cases that can be
speciﬁed in advance (e.g., they determine the threshold values for trigger-
ing rule activation). This intuition may be based on the encapsulation of
practical experience, data-mining and empirical analysis of previous runs,
or theoretical considerations (in the order of the three examples above),
but all rely on the implicit assumption that changes that were appropriate
to make on another search of another problem are applicable to this run
of the EA on this problem.
• Relative evidence
In the case of using relative evidence, parameter values are compared ac-
cording to the ﬁtness of the oﬀspring that they produce, and the better
values get rewarded. The direction and/or magnitude of the change of
the strategy parameter is not speciﬁed deterministically, but relative to
the performance of other values, i.e., it is necessary to have more than
one value present at any given time. Here, the assignment of the value of
the strategy parameter involves credit assignment, and the action of the
EA may determine whether or not the new value persists or propagates
throughout the population. As an example, consider an EA using more
crossovers with crossover rates adding up to 1.0 and being reset based
on the crossovers performance measured by the quality of oﬀspring they
create. Such methods may be controlled adaptively, typically using “book-
keeping” to monitor performance and a user-supplied update procedure
[16, 47, 64], or self-adaptively [7, 29, 51, 65, 71, 76] with the selection
operator acting indirectly on operator or parameter frequencies via their
association with “ﬁt” solutions.

32
A.E. Eiben, Z. Michalewicz, M. Schoenauer, and J.E. Smith
4.4 What is the Scope of the Change?
As discussed earlier, any change within any component of an EA may aﬀect
a gene (parameter), whole chromosomes (individuals), the entire population,
another component (e.g., selection), or even the evaluation function. This is
the aspect of the scope or level of adaptation [2, 40, 74]. Note, however, that
the scope or level is not an independent dimension, as it usually depends
on the component of the EA where the change takes place. For example, a
change of the mutation step size may aﬀect a gene, a chromosome, or the entire
population, depending on the particular implementation (i.e., scheme used),
but a change in the penalty coeﬃcients typically aﬀects the whole population.
In this respect the scope feature is a secondary one, usually depending on the
given component and its actual implementation.
It should be noted that the issue of the scope of the parameter might be
more complicated than indicated in Sect. 3.1. First of all, the scope depends
on the interpretation mechanism of the given parameters. For example, an
individual might be represented as
⟨x1, . . . , xn, σ1, . . . , σn, α1, . . . , αn(n−1)/2⟩,
where the vector α denotes the covariances between the variables σ1, . . . , σn.
In this case the scope of the strategy parameters in α is the whole individual,
although the notation might suggest that they act on a subindividual level.
The next example illustrates that the same parameter (encoded in the
chromosomes) can be interpreted in diﬀerent ways, leading to diﬀerent al-
gorithm variants with diﬀerent scopes of this parameter. Spears [76], follow-
ing [30], experimented with individuals containing an extra bit to determine
whether one-point crossover or uniform crossover is to be used (bit 1/0 stand-
ing for one-point/uniform crossover, respectively). Two interpretations were
considered. The ﬁrst interpretation was based on a pairwise operator choice: If
both parental bits are the same, the corresponding operator is used; otherwise,
a random choice is made. Thus, this parameter in this interpretation acts at
an individual level. The second interpretation was based on the bit distribu-
tion over the whole population: If, for example, 73% of the population had bit
1, then the probability of one-point crossover was 0.73. Thus this parameter
under this interpretation acts on the population level. Spears noted that there
was a deﬁnite impact on performance, with better results arising from the in-
dividual level scheme, and more recently Smith [69] compared three versions of
a self-adaptive recombination operator, concluding that the component-level
version signiﬁcantly outperformed the individual or population-level versions.
However, the two interpretations of Spears’ scheme can be easily combined.
For instance, similar to the ﬁrst interpretation, if both parental bits are the
same, the corresponding operator is used, but if they diﬀer, the operator is
selected according to the bit distribution, just as in the second interpretation.
The scope/level of this parameter in this interpretation is neither individual
nor population, but rather both. This example shows that the notion of scope

Parameter Control in Evolutionary Algorithms
33
can be ill-deﬁned and very complex. This, combined with the arguments that
the scope or level entity is primarily a feature of the given parameter and only
secondarily a feature of adaptation itself, motivates our decision to exclude it
as a major classiﬁcation criterion.
4.5 Summary
In conclusion, the main criteria for classifying methods that change the values
of the strategy parameters of an algorithm during its execution are:
1. What component/parameter is changed?
2. How is the change made?
3. Which evidence is used to make the change?
Our classiﬁcation is thus three-dimensional. The component dimension
consists of six categories: representation, evaluation function, variation oper-
ators (mutation and recombination), selection, replacement, and population.
The other dimensions have respectively three (deterministic, adaptive, self-
adaptive) and two categories (absolute, relative). Their possible combinations
are given in Table 1. As the table indicates, deterministic parameter con-
trol with relative evidence is impossible by deﬁnition, and so is self-adaptive
parameter control with absolute evidence. Within the adaptive scheme both
options are possible and are indeed used in practice.
Deterministic Adaptive Self-adaptive
Absolute
+
+
–
Relative
–
+
+
Table 1. Reﬁned taxonomy of parameter setting in EAs: types of parameter con-
trol along the type and evidence dimensions. The – entries represent meaningless
(nonexistent) combinations
5 Examples of Varying EA Parameters
Here we review some illustrative examples from the literature concerning all
major components. For a more comprehensive overview the reader is referred
to [22].
5.1 Representation
The choice of representation forms an important distinguishing feature be-
tween diﬀerent streams of evolutionary computing. From this perspective GAs

34
A.E. Eiben, Z. Michalewicz, M. Schoenauer, and J.E. Smith
and ES can be distinguished from (historical) EP and GP according to the
data structure used to represent individuals. In the ﬁrst group this data struc-
ture is linear, and its length is ﬁxed, that is, it does not change during a run
of the algorithm. For (historical) EP and GP this does not hold: ﬁnite state
machines and parse trees are nonlinear structures, and their size (the number
of states, respectively nodes) and shape can change during a run. It could be
argued that this implies an intrinsically adaptive representation in traditional
EP and GP. On the other hand, the main structure of the ﬁnite state machines
does not change during the search in traditional EP, nor do the function and
terminal sets in GP (without automatically deﬁned functions, ADFs). If one
identiﬁes “representation” with the basic syntax (plus the encoding mecha-
nism), then the diﬀerently sized and shaped ﬁnite state machines, respectively
trees, are only diﬀerent expressions in this unchanging syntax. Based on this
view we do not consider the representations in traditional EP and GP intrin-
sically adaptive.
We illustrate variable representations with the delta coding algorithm of
Mathias and Whitley [82], which eﬀectively modiﬁes the encoding of the func-
tion parameters. The motivation behind this algorithm is to maintain a good
balance between fast search and sustaining diversity. In our taxonomy it can
be categorised as an adaptive adjustment of the representation based on ab-
solute evidence.
The GA is used with multiple restarts; the ﬁrst run is used to ﬁnd an
interim solution, and subsequent runs decode the genes as distances (delta
values) from the last interim solution. This way each restart forms a new
hypercube with the interim solution at its origin. The resolution of the delta
values can also be altered at the restarts to expand or contract the search
space. The restarts are triggered when population diversity (measured by
the Hamming distance between the best and worst strings of the current
population) is not greater than one. The sketch of the algorithm showing the
main idea is given in Fig. 2.
Note that the number of bits for δ can be increased if the same solution
INTERIM is found. This technique was further reﬁned in [57, 58] to cope with
deceptive problems.
5.2 Evaluation Function
Evaluation functions are typically not varied in an EA because they are often
considered as part of the problem to be solved and not as part of the problem-
solving algorithm. In fact, an evaluation function forms the bridge between
the two, so both views are at least partially true. In many EAs the evaluation
function is derived from the (optimisation) problem at hand with a simple
transformation of the objective function. In the class of constraint satisfaction
problems, however, there is no objective function in the problem deﬁnition
[20]. Rather, these are normally posed as decision problems with an Boolean
outcome φ denoting whether a given assignment of variables represents a valid

Parameter Control in Evolutionary Algorithms
35
BEGIN
/* given a starting population and genotype-phenotype encoding */
WHILE (
HD > 1 ) DO
RUN GA with k bits per object variable;
OD
REPEAT UNTIL (
global termination is satisfied ) DO
save best solution as INTERIM;
reinitialise population with new coding;
/*
k-1 bits as the distance δ to the object value in
*/
/*
INTERIM and one sign bit */
WHILE (
HD > 1 ) DO
RUN GA with this encoding;
OD
OD
END
Fig. 2. Outline of the delta coding algorithm
solution. One possible approach using EAs is to treat these as minimisation
problems where the evaluation function is deﬁned as the amount of constraint
violation by a given candidate solution. This approach, commonly known as
the penalty approach, can be formalised as follows. Let us assume that we
have constraints ci (i = {1, . . . , m}) and variables vj (j = {1, . . . , n}) with the
same domain S. The task is to ﬁnd one variable assignment ¯s ∈S satisfying
all constraints. Then the penalties can be deﬁned as follows:
f(¯s) =
m

i=1
wi × χ(¯s, ci),
where
χ(¯s, ci) =

1 if ¯s violates ci,
0 otherwise.
Obviously, for each ¯s ∈S we have that φ(¯s) = true if and only if f(¯s) = 0,
and the weights specify how severely the violation of a certain constraint is
penalised. The setting of these weights has a large impact on the EA perfor-
mance, and ideally wi should reﬂect how hard ci is to satisfy. The problem
is that ﬁnding the appropriate weights requires much insight into the given
problem instance, and therefore it might not be practicable.
The stepwise adaptation of weights (SAW) mechanism, introduced by
Eiben and van der Hauw [26] as an improved version of the weight adaptation
mechanism of Eiben, Rau´e, and Ruttkay [23, 24], provides a simple and eﬀec-
tive way to set these weights. The basic idea behind the SAW mechanism is

36
A.E. Eiben, Z. Michalewicz, M. Schoenauer, and J.E. Smith
that constraints that are not satisﬁed after a certain number of steps (ﬁtness
evaluations) must be diﬃcult, and thus must be given a high weight (penalty).
SAW-ing changes the evaluation function adaptively in an EA by periodically
checking the best individual in the population and raising the weights of those
constraints this individual violates. Then the run continues with the new eval-
uation function. A nice feature of SAW-ing is that it liberates the user from
seeking good weight settings, thereby eliminating a possible source of error.
Furthermore, the used weights reﬂect the diﬃculty of constraints for the given
algorithm on the given problem instance in the given stage of the search [27].
This property is also valuable since, in principle, diﬀerent weights could be
appropriate for diﬀerent algorithms.
5.3 Mutation
A large majority of work on adapting or self-adapting EA parameters concerns
variation operators: mutation and recombination (crossover). As we discussed
above, the 1/5 rule of Rechenberg constitutes a classical example for adaptive
mutation step size control in ES. In the same paper we also showed that
self-adaptive control of mutation step sizes is traditional in ES.
Hesser and M¨anner [39] derived theoretically optimal schedules within GAs
for deterministically changing pm for the counting-ones function. They sug-
gest:
pm(t) =
	α
β × exp

 −γt
2

λ
√
L
,
where α, β, γ are constants, L is the chromosome length, λ is the population
size, and t is the time (generation counter). This is a purely deterministic
parameter control mechanism.
A self-adaptive mechanism for controlling mutation in a bit-string GA is
given by B¨ack [6]. This technique works by extending the chromosomes by
an additional 20 bits that together encode the individuals’ own pm. Mutation
then works by:
1. Decoding these bits ﬁrst to pm
2. Mutating the bits that encode pm with mutation probability pm
3. Decoding these (changed) bits to p′
m
4. Mutating the bits that encode the solution with mutation probability p′
m
This approach is highly self-adaptive since even the rate of variation of the
search parameters is given by the encoded value, as opposed to the use of an
external parameters such as learning rates for step-sizes. More recently Smith
[70] showed theoretical predictions, veriﬁed experimentally, that this scheme
gets “stuck” in suboptimal regions of the search space with a low, or zero,
mutation rate attached to each member of the population. He showed that a
more robust problem-solving mechanism can simply be achieved by ignoring
the ﬁrst step of the algorithm above, and instead using a ﬁxed learning rate as

Parameter Control in Evolutionary Algorithms
37
the probability of applying bitwise mutation to the encoding of the strategy
parameters in the second step.
5.4 Crossover
The classical example for adapting crossover rates in GAs is Davis’s adaptive
operator ﬁtness. The method adapts the rates of crossover operators by re-
warding those that are successful in creating better oﬀspring. This reward is
diminishingly propagated back to operators of a few generations back, who
helped setting it all up; the reward is a shift up in probability at the cost
of other operators [17]. This, actually, is very close in spirit to the “implicit
bucket brigade” credit assignment principle used in classiﬁer systems [33].
The GA using this method applies several crossover operators simultane-
ously within the same generation, each having its own crossover rate pc(opi).
Additionally, each operator has its “local delta” value di that represents the
strength of the operator measured by the advantage of a child created by us-
ing that operator with respect to the best individual in the population. The
local deltas are updated after every use of operator i. The adaptation mech-
anism recalculates the crossover rates after K generations. The main idea is
to redistribute 15% of the probabilities biased by the accumulated operator
strengths, that is, the local deltas. To this end, these di values are normalised
so that their sum equals 15, yielding dnorm
i
for each i. Then the new value for
each pc(opi) is 85% of its old value and its normalised strength:
pc(opi) = 0.85 · pc(opi) + dnorm
i
.
Clearly, this method is adaptive based on relative evidence.
5.5 Selection
It is interesting to note that neither the parent selection nor the survivor se-
lection (replacement) component of an EA has been commonly used in an
adaptive manner, even though there are selection methods whose parameters
can be easily adapted. For example, in linear ranking there is a parameter
s representing the expected number of oﬀspring to be allocated to the best
individual. By changing this parameter within the range of [1 . . . 2] the selec-
tive pressure of the algorithm can be varied easily. Similar possibilities exist
for tournament selection, where the tournament size provides a natural para-
meter.
Most existing mechanisms for varying the selection pressure are based on
the so-called Boltzmann selection mechanism, which changes the selection
pressure during evolution according to a predeﬁned “cooling schedule” [55].
The name originates from the Boltzmann trial from condensed matter physics,
where a minimal energy level is sought by state transitions. Being in a state
i the chance of accepting state j is

38
A.E. Eiben, Z. Michalewicz, M. Schoenauer, and J.E. Smith
P[accept j] =

1
if
Ei ≥Ej,
exp

Ei−Ej
Kb·T

if
Ei < Ej,
where Ei, Ej are the energy levels, Kb is a parameter called the Boltz-
mann constant, and T is the temperature. This acceptance rule is called the
Metropolis criterion.
We illustrate variable selection pressure in the survivor selection (replace-
ment) step by simulated annealing (SA). SA is a generate-and-test search
technique based on a physical, rather than a biological analogy [1]. Formally,
however, SA can be envisioned as an evolutionary process with population
size of 1, undeﬁned (problem-dependent) representation and mutation, and a
speciﬁc survivor selection mechanism. The selective pressure changes during
the course of the algorithm in the Boltzmann style. The main cycle in SA is
given in Fig. 3.
BEGIN
/* given a current solution i ∈S */
/* given a function to generate the set of neighbours Ni of i */
generate j ∈Ni;
IF (f(i) < f(j)) THEN
set i = j;
ELSE
IF ( exp

f(i)−f(j)
ck

> random[0, 1)) THEN
set i = j;
FI
ESLE
FI
END
Fig. 3. Outline of the simulated annealing algorithm
In this mechanism the parameter ck, the temperature, decreases accord-
ing to a predeﬁned scheme as a function of time, making the probability of
accepting inferior solutions smaller and smaller (for minimisation problems).
From an evolutionary point of view, we have here a (1+1) EA with increasing
selection pressure.
A successful example of applying Boltzmann acceptance is that of Smith
and Krasnogor [50], who used it in the local search part of a memetic algorithm
(MA), with the temperature inversely related to the ﬁtness diversity of the
population. If the population contains a wide spread of ﬁtness values, the
“temperature” is low, so only ﬁtter solutions found by local search are likely
to be accepted, concentrating the search on good solutions. However, when

Parameter Control in Evolutionary Algorithms
39
the spread of ﬁtness values is low, indicating a converged population which
is a common problem in MAs, the “temperature” is higher, making it more
likely that an inferior solution will be accepted, thus reintroducing diversity
and oﬀering a potential means of escaping from local optima.
5.6 Population
An innovative way to control the population size is oﬀered by Arabas et al.
[3, 59] in their GA with variable population size (GAVaPS). In fact, the popu-
lation size parameter is removed entirely from GAVaPS, rather than adjusted
on-the-ﬂy. Certainly, in an evolutionary algorithm the population always has
a size, but in GAVaPS this size is a derived measure, not a controllable pa-
rameter. The main idea is to assign a lifetime to each individual when it is
created, and then to reduce its remaining lifetime by one in each consecutive
generation. When the remaining lifetime becomes zero, the individual is re-
moved from the population. Two things must be noted here. First, the lifetime
allocated to a newborn individual is biased by its ﬁtness: ﬁtter individuals are
allowed to live longer. Second, the expected number of oﬀspring of an indi-
vidual is proportional to the number of generations it survives. Consequently,
the resulting system favours the propagation of good genes.
Fitting this algorithm into our general classiﬁcation scheme is not straight-
forward because it has no explicit mechanism that sets the value of the popu-
lation size parameter. However, the procedure that implicitly determines how
many individuals are alive works in an adaptive fashion using information
about the status of the search. In particular, the ﬁtness of a newborn indi-
vidual is related to the ﬁtness of the present generation, and its lifetime is
allocated accordingly. This amounts to using relative evidence.
5.7 Varying Several Parameters Simultaneously
One of the studies explicitly devoted to adjusting more parameters (and also
on more than one level) is that of Hinterding et al. on a “self-adaptive GA”
[41]. This GA uses self-adaptation for mutation rate control, plus relative-
based adaptive control for the population size.6 The mechanism for controlling
mutation is similar to that from B¨ack [6], (Sect. 5.3), except that mutating the
bits encoding the mutation strength is not based on the bits in question, but is
done by a universal mechanism ﬁxed for all individuals and all generations. In
other words, the self-adaptive mutation parameter is only used for the genes
encoding a solution. As for the population size, the GA works with three sub-
populations: a small, a medium, and a large one, P1, P2, and P3, respectively
6 Strictly speaking, the authors’ term “self-adaptive GA” is only partially correct.
However, this paper is from 1996, and the contemporary terminology distinguish-
ing dynamic, adaptive, and self-adaptive schemes as we do it here was only pub-
lished in 1999 [22].

40
A.E. Eiben, Z. Michalewicz, M. Schoenauer, and J.E. Smith
(the initial sizes respectively being 50, 100, and 200). These populations are
evolved in parallel for a given number of ﬁtness evaluations (an epoch) inde-
pendently by the same GA setup. After each epoch, the subpopulations are
resized based on some heuristic rules, maintaining a lower and an upper bound
(10 and 1000) and keeping P2 always the medium-sized subpopulation. There
are two categories of rules. Rules in the ﬁrst category are activated when
the ﬁtnesses in the subpopulations converge and try to move the populations
apart. For instance, if P2 and P3 have the same ﬁtness, the size of P3 is dou-
bled. Rules from another set are activated when the ﬁtness values are distinct
at the end of an epoch. These rules aim at maximising the performance of
P2. An example of one such rule is: if the performance of the subpopulations
ranks them as P2 < P3 < P1 then size(P3) = (size(P2) + size(P3))/2. In
our taxonomy, this population size control mechanism is adaptive, based on
relative evidence.
Lis and Lis [54] also oﬀer a parallel GA setup to control the mutation
rate, the crossover rate, and the population size during a run. The idea here
is that for each parameter a few possible values are deﬁned in advance, say
lo, med, hi, and only these values are allowed in any of the GAs, that is, in
the subpopulations evolved in parallel. After each epoch the performances of
the applied parameter values are compared by averaging the ﬁtnesses of the
best individuals of those GAs that use a given value. If the winning parameter
value is:
1. hi, then all GAs shift one level up concerning this parameter in the next
epoch;
2. med, then all GAs use the same value concerning this parameter in the
next epoch;
3. lo, then all GAs shift one level down concerning this parameter in the
next epoch.
Clearly, the adjustment mechanism for all parameters here is adaptive, based
on relative evidence.
Mutation, crossover, and population size are all controlled on-the-ﬂy in
the GA “without parameters” of B¨ack et al. in [11]. Here, the self-adaptive
mutation from [6] (Sect. 5.3) is adopted without changes, a new self-adaptive
technique is invented for regulating the crossover rates of the individuals,
and the GAVaPS lifetime idea (Sect. 5.6) is adjusted for a steady-state GA
model. The crossover rates are included in the chromosomes, much like the
mutation rates. If a pair of individuals is selected for reproduction, then their
individual crossover rates are compared with a random number r ∈[0, 1] and
an individual is seen as ready to mate if its pc > r. Then there are three
possibilities:
1. If both individuals are ready to mate then uniform crossover is applied,
and the resulting oﬀspring is mutated.
2. If neither is ready to mate then both create a child by mutation only.

Parameter Control in Evolutionary Algorithms
41
3. If exactly one of them is ready to mate, then the one not ready creates a
child by mutation only (which is inserted into the population immediately
through the steady-state replacement), the other is put on the hold, and
the next parent selection round picks only one other parent.
This study diﬀers from those discussed before in that it explicitly com-
pares GA variants using only one of the (self-)adaptive mechanisms and the
GA applying them all. The experiments show remarkable outcomes: the com-
pletely (self-)adaptive GA wins, closely followed by the one using only the
adaptive population size control, and the GAs with self-adaptive mutation
and crossover are signiﬁcantly worse. These results suggest that putting ef-
fort into adapting the population size could be more eﬀective than trying to
adjust the variation operators. This is truly surprising considering that tra-
ditionally the on-line adjustment of the variation operators has been pursued
and the adjustment of the population size received relatively little attention.
The subject certainly requires more research.
6 Discussion
Summarising this paper a number of things can be noted. First, parameter
control in an EA can have two purposes. It can be done to avoid suboptimal
algorithm performance resulting from suboptimal parameter values set by the
user. The basic assumption here is that the applied control mechanisms are
intelligent enough to do this job better than the user could, or that they can do
it approximately as good, but they liberate the user from doing it. Either way,
they are beneﬁcial. The other motivation for controlling parameters on-the-
ﬂy is the assumption that the given parameter can have a diﬀerent “optimal”
value in diﬀerent phases of the search. If this holds, then there is simply no
optimal static parameter value; for good EA performance one must vary this
parameter.
The second thing we want to note is that making a parameter (self adap-
tive) does not necessarily mean that we have an EA with fewer parameters.
For instance, in GAVaPS the population size parameter is eliminated at the
cost of introducing two new ones: the minimum and maximum lifetime of
newborn individuals. If the EA performance is sensitive to these new parame-
ters then such a parameter replacement can make things worse. This problem
also occurs on another level. One could say that the procedure that allocates
lifetimes in GAVaPS, the probability redistribution mechanism for adaptive
crossover rates (Sect. 5.4), or the function specifying how the σ values are mu-
tated in ES are also (meta) parameters. It is in fact an assumption that these
are intelligently designed and their eﬀect is positive. In many cases there are
more possibilities, that is, possibly well-working procedures one can design.
Comparing these possibilities implies experimental (or theoretical) studies
very much like comparing diﬀerent parameter values in a classical setting.

42
A.E. Eiben, Z. Michalewicz, M. Schoenauer, and J.E. Smith
Here again, it can be the case that algorithm performance is not so sensitive
to details of this (meta) parameter, which fully justiﬁes this approach.
Finally, let us place the issue of parameter control in a larger perspec-
tive. Over the last 20 years the EC community shifted from believing that
EA performance is to a large extent independent from the given problem in-
stance to realising that it is. In other words, it is now acknowledged that EAs
need more or less ﬁne-tuning to speciﬁc problems and problem instances. Ide-
ally, it should be the algorithm that performs the necessary problem-speciﬁc
adjustments. Parameter control as discussed here is a step towards this.
References
1. E.H.L. Aarts and J. Korst. Simulated Annealing and Boltzmann Machines. Wi-
ley, Chichester, UK, 1989.
2. P.J. Angeline. Adaptive and self-adaptive evolutionary computations. In Com-
putational Intelligence, pages 152–161. IEEE Press, 1995.
3. J. Arabas, Z. Michalewicz, and J. Mulawka. GAVaPS – a genetic algorithm
with varying population size. In ICEC-94 [42], pages 73–78.
4. A. Auger. Contributions th´eoriques et num´eriques `a l’optimisation continue par
algorithmes ´evolutionnaires. PhD thesis, Universit´e Paris 6, December 2004. in
French.
5. A. Auger, C. Le Bris, and M. Schoenauer. Dimension-independent convergence
rate for non-isotropic (1, λ)−es. In proceedings of GECCO2003, pages 512–524,
2003.
6. T. B¨ack. The interaction of mutation rate, selection and self-adaptation within
a genetic algorithm. In M¨anner and Manderick [56], pages 85–94.
7. T. B¨ack. Self adaptation in genetic algorithms. In F.J. Varela and P. Bourgine,
editors, Toward a Practice of Autonomous Systems: Proceedings of the 1st Euro-
pean Conference on Artiﬁcial Life, pages 263–271. MIT Press, Cambridge, MA,
1992.
8. T. B¨ack. Optimal mutation rates in genetic search. In Forrest [31], pages 2–8.
9. T. B¨ack. Evolutionary Algorithms in Theory and Practice. New-York:Oxford
University Press, 1995.
10. T. B¨ack. Self-adaptation. In T. B¨ack, D.B. Fogel, and Z. Michalewicz, editors,
Evolutionary Computation 2: Advanced Algorithms and Operators, chapter 21,
pages 188–211. Institute of Physics Publishing, Bristol, 2000.
11. T. B¨ack, A.E. Eiben, and N.A.L. van der Vaart. An empirical study on GAs
“without parameters”. In M. Schoenauer, K. Deb, G. Rudolph, X. Yao, E. Lut-
ton, J.J. Merelo, and H.-P. Schwefel, editors, Proceedings of the 6th Conference
on Parallel Problem Solving from Nature, number 1917 in Lecture Notes in Com-
puter Science, pages 315–324. Springer, Berlin, Heidelberg, New York, 2000.
12. T. B¨ack, D.B. Fogel, and Z. Michalewicz, editors. Handbook of Evolutionary
Computation. Institute of Physics Publishing, Bristol, and Oxford University
Press, New York, 1997.
13. T. B¨ack, M. Sch¨utz, and S. Khuri. A comparative study of a penalty function,
a repair heuristic and stochastic operators with set covering problem. In Pro-
ceedings of Evolution Artiﬁcial 1995, number 1063 in LNCS. Springer-Verlag,
1995.

Parameter Control in Evolutionary Algorithms
43
14. J.C. Bean and A.B. Hadj-Alouane. A dual genetic algorithm for bounded integer
problems. Technical Report 92-53, University of Michigan, 1992.
15. R.K. Belew and L.B. Booker, editors. Proceedings of the 4th International Con-
ference on Genetic Algorithms. Morgan Kaufmann, San Francisco, 1991.
16. L. Davis. ‘Adapting operator probabilities in genetic algorithms. In Schaﬀer
[61], pages 61–69.
17. L. Davis, editor. Handbook of Genetic Algorithms. Van Nostrand Reinhold, 1991.
18. K.A. De Jong. An Analysis of the Behaviour of a Class of Genetic Adaptive
Systems. PhD thesis, University of Michigan, 1975.
19. K. Deb and H.-G. Beyer. Self-adaptive genetic algorithms with simulated binary
crossover. Evolutionary Computation.
20. A.E. Eiben. Evolutionary algorithms and constraint satisfaction: Deﬁnitions,
survey, methodology, and research directions. In L. Kallel, B. Naudts, and
A. Rogers, editors, Theoretical Aspects of Evolutionary Computing, pages 13–58.
Springer, Berlin, Heidelberg, New York, 2001.
21. A.E. Eiben, R. Hinterding, and Z. Michalewicz. Parameter control in evolution-
ary algorithms. IEEE Transactions on Evolutionary Computation, 3(2):124–141,
1999.
22. A.E. Eiben, B. Jansen, Z. Michalewicz, and B. Paechter. Solving CSPs using
self-adaptive constraint weights: how to prevent EAs from cheating. In Whitley
et al. [81], pages 128–134.
23. A.E. Eiben, P.-E. Rau´e, and Z. Ruttkay. GA-easy and GA-hard constraint satis-
faction problems. In M. Meyer, editor, Proceedings of the ECAI-94 Workshop on
Constraint Processing, number 923 in LNCS, pages 267–284. Springer, Berlin,
Heidelberg, New York, 1995.
24. A.E. Eiben and Z. Ruttkay. Self-adaptivity for constraint satisfaction: Learning
penalty functions. In ICEC-96 [43], pages 258–261.
25. A.E. Eiben and J.E. Smith. Introduction to Evolutionary Computation.
Springer, 2003.
26. A.E. Eiben and J.K. van der Hauw. Solving 3-SAT with adaptive genetic algo-
rithms. In ICEC-97 [44], pages 81–86.
27. A.E. Eiben and J.I. van Hemert. SAW-ing EAs: adapting the ﬁtness function for
solving constrained problems. In D. Corne, M. Dorigo, and F. Glover, editors,
New Ideas in Optimization, Chapter 26, pages 389–402. McGraw Hill, London,
1999.
28. L.J. Eshelman, editor. Proceedings of the 6th International Conference on
Genetic Algorithms. Morgan Kaufmann, San Francisco, 1995.
29. D.B. Fogel. Evolutionary Computation. IEEE Press, 1995.
30. D.B. Fogel and J.W. Atmar. Comparing genetic operators with Gaussian
mutations in simulated evolutionary processes using linear systems. Biological
Cybernetics, 63(2):111–114, 1990.
31. S. Forrest, editor. Proceedings of the 5th International Conference on Genetic
Algorithms. Morgan Kaufmann, San Francisco, 1993.
32. B. Friesleben and M. Hartfelder. Optimisation of genetic algorithms by genetic
algorithms.
In R.F. Albrecht, C.R. Reeves, and N.C. Steele, editors, Artiﬁ-
cal Neural Networks and Genetic Algorithms, pages 392–399. Springer, Berlin,
Heidelberg, New York, 1993.
33. D.E. Goldberg. Genetic Algorithms in Search, Optimization and Machine Learn-
ing. Addison-Wesley, 1989.

44
A.E. Eiben, Z. Michalewicz, M. Schoenauer, and J.E. Smith
34. J.J. Grefenstette. Optimisation of control parameters for genetic algorithms.
IEEE Transaction on Systems, Man and Cybernetics, 16(1):122–128, 1986.
35. N. Hansen, S. M¨uller, and P. Koumoutsakos. Reducing the Time Complexity
of the Derandomized Evolution Strategy with Covariance Matrix Adaptation
(CMA-ES). Evolution Computation, 11(1), 2003.
36. N. Hansen and A. Ostermeier. Adapting arbitrary normal mutation distribu-
tions in evolution strategies: The covariance matrix adaption. In ICEC96, pages
312–317. IEEE Press, 1996.
37. N. Hansen and A. Ostermeier. Completely derandomized self-adaptation in evo-
lution strategies. Evolutionary Computation, 9(2):159–195, 2001.
38. N. Hansen, A. Ostermeier, and A. Gawelczyk. On the adaptation of arbitrary
normal mutation distributions in evolution strategies: The generating set adap-
tation. In Eshelman [28], pages 57–64.
39. J. Hesser and R. Manner. Towards an optimal mutation probablity in genetic
algorithms. In H.-P. Schwefel and R. M¨anner, editors, Proceedings of the 1st
Conference on Parallel Problem Solving from Nature, number 496 in Lecture
Notes in Computer Science, pages 23–32. Springer, Berlin, Heidelberg, New
York, 1991.
40. R. Hinterding, Z. Michalewicz, and A.E. Eiben. Adaptation in evolutionary
computation: A survey. In ICEC-97 [44].
41. R. Hinterding, Z. Michalewicz, and T.C. Peachey. Self-adaptive genetic algo-
rithm for numeric functions. In Voigt et al. [80], pages 420–429.
42. Proceedings of the First IEEE Conference on Evolutionary Computation. IEEE
Press, Piscataway, NJ, 1994.
43. Proceedings of the 1996 IEEE Conference on Evolutionary Computation. IEEE
Press, Piscataway, NJ, 1996.
44. Proceedings of the 1997 IEEE Conference on Evolutionary Computation. IEEE
Press, Piscataway, NJ, 1997.
45. A. Jain and D.B. Fogel. Case studies in applying ﬁtness distributions in evolu-
tionary algorithms. II. comparing the improvements from crossover and gaussian
mutation on simple neural networks. In X. Yao and D.B. Fogel, editors, Proc. of
the 2000 IEEE Symposium on Combinations of Evolutionary Computation and
Neural Networks, pages 91–97. IEEE Press, 2000.
46. J.A. Joines and C.R. Houck. On the use of non-stationary penalty functions to
solve nonlinear constrained optimisation problems with ga’s. In ICEC-94 [42],
pages 579–584.
47. B.A. Julstrom. What have you done for me lately?: Adapting operator proba-
bilities in a steady-state genetic algorithm. In Eshelman [28], pages 81–87.
48. Y. Kakuza, H. Sakanashi, and K. Suzuki. Adaptive search strategy for genetic
algorithms with additional genetic algorithms. In M¨anner and Manderick [56],
pages 311–320.
49. S. Kirkpatrick, C. Gelatt, and M. Vecchi. Optimization by simulated anealing.
Science, 220:671–680, 1983.
50. N. Krasnogor and J.E. Smith. A memetic algorithm with self-adaptive local
search: TSP as a case study. In Whitley et al. [81], pages 987–994.
51. N. Krasnogor and J.E. Smith. Emergence of proﬁtable search strategies based
on a simple inheritance mechanism. In Spector et al. [77], pages 432–439.
52. M. Lee and H. Takagi. Dynamic control of genetic algorithms using fuzzy logic
techniques. In Forrest [31], pages 76–83.

Parameter Control in Evolutionary Algorithms
45
53. J. Lis. Parallel genetic algorithm with dynamic control parameter. In ICEC-96
[43], pages 324–329.
54. J. Lis and M. Lis. Self-adapting parallel genetic algorithm with the dynamic
mutation probability, crossover rate, and population size. In J. Arabas, editor,
Proceedings of the First Polish Evolutionary Algorithms Conference, pages 79–
86. Politechnika Warszawska, Warsaw, 1996.
55. S.W. Mahfoud. Boltzmann selection. In B¨ack et al. [12], pages C2.5:1–4.
56. R. M¨anner and B. Manderick, editors. Proceedings of the 2nd Conference on
Parallel Problem Solving from Nature. North-Holland, Amsterdam, 1992.
57. K.E. Mathias and L.D. Whitley. Remapping hyperspace during genetic search:
Canonical delta folding. In L.D. Whitley, editor, Foundations of Genetic Algo-
rithms 2, pages 167–186. Morgan Kaufmann, San Francisco, 1993.
58. K.E. Mathias and L.D. Whitley. Changing representations during search: A
comparative study of delta coding. Evolutionary Computation, 2(3):249–278,
1995.
59. Z. Michalewicz. Genetic Algorithms + Data Structures = Evolution Programs.
Springer, Berlin, Heidelberg, New York, 3rd edition, 1996.
60. Z. Michalewicz and M. Schoenauer. Evolutionary algorithms for constrained
parameter optimisation problems. Evolutionary Computation, 4(1):1–32, 1996.
61. J.D. Schaﬀer, editor. Proceedings of the 3rd International Conference on Genetic
Algorithms. Morgan Kaufmann, San Francisco, 1989.
62. J.D. Schaﬀer, R.A. Caruana, L.J. Eshelman, and R. Das. A study of control
parameters aﬀecting online performance of genetic algorithms for function op-
timisation. In Schaﬀer [61], pages 51–60.
63. J.D. Schaﬀer and L.J. Eshelman. On crossover as an evolutionarily viable strat-
egy. In Belew and Booker [15], pages 61–68.
64. D. Schlierkamp-Voosen and H. M¨uhlenbein. Strategy adaptation by compet-
ing subpopulations.
In Y. Davidor, H.-P. Schwefel, and R. M¨anner, editors,
Proceedings of the 3rd Conference on Parallel Problem Solving from Nature,
number 866 in Lecture Notes in Computer Science, pages 199–209. Springer,
Berlin, Heidelberg, New York, 1994.
65. H.-P. Schwefel. Numerische Optimierung von Computer-Modellen mittels der
Evolutionsstrategie, volume 26 of ISR. Birkhaeuser, Basel/Stuttgart, 1977.
66. H.-P. Schwefel. Numerical Optimisation of Computer Models. Wiley, New York,
1981.
67. J.E. Smith. Self Adaptation in Evolutionary Algorithms. PhD thesis, University
of the West of England, Bristol, UK, 1998.
68. J.E. Smith. Modelling GAs with self-adaptive mutation rates. In Spector et al.
[77], pages 599–606.
69. J.E. Smith. On appropriate adaptation levels for the learning of gene linkage.
J. Genetic Programming and Evolvable Machines, 3(2):129–155, 2002.
70. J.E. Smith. Parameter perturbation mechanisms in binary coded gas with self-
adaptive mutation.
In Rowe, Poli, DeJong, and Cotta, editors, Foundations
of Genetic Algorithms 7, pages 329–346. Morgan Kaufmann, San Francisco,
2003.
71. J.E. Smith and T.C. Fogarty. Adaptively parameterised evolutionary systems:
Self adaptive recombination and mutation in a genetic algorithm. In Voigt et al.
[80], pages 441–450.
72. J.E. Smith and T.C. Fogarty. Recombination strategy adaptation via evolution
of gene linkage. In ICEC-96 [43], pages 826–831.

46
A.E. Eiben, Z. Michalewicz, M. Schoenauer, and J.E. Smith
73. J.E. Smith and T.C. Fogarty. Self adaptation of mutation rates in a steady state
genetic algorithm. In ICEC-96 [43], pages 318–323.
74. J.E. Smith and T.C. Fogarty. Operator and parameter adaptation in genetic
algorithms. Soft Computing, 1(2):81–87, 1997.
75. R.E. Smith and E. Smuda. Adaptively resizing populations: Algorithm, analysis
and ﬁrst results. Complex Systems, 9(1):47–72, 1995.
76. W.M. Spears. Adapting crossover in evolutionary algorithms. In J.R. McDonnell,
R.G. Reynolds, and D.B. Fogel, editors, Proceedings of the 4th Annual Con-
ference on Evolutionary Programming, pages 367–384. MIT Press, Cambridge,
MA, 1995.
77. L. Spector, E. Goodman, A. Wu, W.B. Langdon, H.-M. Voigt, M. Gen, S. Sen,
M. Dorigo, S. Pezeshk, M. Garzon, and E. Burke, editors. Proceedings of the
Genetic and Evolutionary Computation Conference (GECCO-2001). Morgan
Kaufmann, San Francisco, 2001.
78. C.R. Stephens, I. Garcia Olmedo, J. Moro Vargas, and H. Waelbroeck. Self-
adaptation in evolving systems. Artiﬁcial life, 4:183–201, 1998.
79. G. Syswerda. A study of reproduction in generational and steady state genetic
algorithms. In G. Rawlins, editor, Foundations of Genetic Algorithms, pages
94–101. Morgan Kaufmann, San Francisco, 1991.
80. H.-M. Voigt, W. Ebeling, I. Rechenberg, and H.-P. Schwefel, editors. Proceedings
of the 4th Conference on Parallel Problem Solving from Nature, number 1141
in Lecture Notes in Computer Science. Springer, Berlin, Heidelberg, New York,
1996.
81. D. Whitley, D. Goldberg, E. Cantu-Paz, L. Spector, I. Parmee, and H.-G. Beyer,
editors. Proceedings of the Genetic and Evolutionary Computation Conference
(GECCO-2000). Morgan Kaufmann, San Francisco, 2000.
82. L.D. Whitley, K.E. Mathias, and P. Fitzhorn. Delta coding: An iterative search
strategy for genetic algorithms,. In Belew and Booker [15], pages 77–84.
83. D.H. Wolpert and W.G. Macready. No Free Lunch theorems for optimisation.
IEEE Transactions on Evolutionary Computation, 1(1):67–82, 1997.
84. B. Zhang and H. M¨uhlenbeim. Balancing accuracy and parsimony in genetic
programming. Evolutionary Computing, 3(3):17–38, 1995.

Self-Adaptation in Evolutionary Algorithms
Silja Meyer-Nieberg1 and Hans-Georg Beyer2
1 Department for Computer Science,
Universt¨at der Bundeswehr M¨unchen,
D-85577 Neubiberg, Germany
silja.meyer-nieberg@unibw.de
2 Research Center Process and Product Engineering,
Department of Computer Science,
Vorarlberg University of Applied Sciences,
Hochschulstr. 1, A-6850 Dornbirn, Austria
hans-georg.beyer@fhv.at
Summary. In this chapter, we will give an overview over self-adaptive methods in
evolutionary algorithms. Self-adaptation in its purest meaning is a state-of-the-art
method to adjust the setting of control parameters. It is called self-adaptive because
the algorithm controls the setting of these parameters itself – embedding them into
an individual’s genome and evolving them. We will start with a short history of adap-
tation methods. The section is followed by a presentation of classiﬁcation schemes
for adaptation rules. Afterwards, we will review empirical and theoretical research of
self-adaptation methods applied in genetic algorithms, evolutionary programming,
and evolution strategies.
1 Introduction
Evolutionary algorithms (EA) operate on basis of populations of individuals.
Their performance depends on the characteristics of the population’s distri-
bution. Self-Adaptation aims at biasing the distribution towards appropriate
regions of the search space – maintaining suﬃcient diversity among individuals
in order to enable further evolvability.
Generally, this is achieved by adjusting the setting of control parameters.
Control parameters can be of various forms – ranging from mutation rates,
recombination probabilities, and population size to selection operators (see
e.g. [6]).
The goal is not only to ﬁnd suitable adjustments but to do this eﬃciently.
The task is further complicated by taking into account that the optimizer is
faced by a dynamic problem since a parameter setting that was optimal at
the beginning of an EA-run might become unsuitable during the evolution-
ary process. Thus, there is generally the need for a steady modiﬁcation or
adaptation of the control parameters during the run of an EA.
S. Meyer-Nieberg and H.-G. Beyer: Self-Adaptation in Evolutionary Algorithms, Studies in
Computational Intelligence (SCI) 54, 47–75 (2007)
www.springerlink.com
© Springer-Verlag Berlin Heidelberg 2007

48
Silja Meyer-Nieberg and Hans-Georg Beyer
We will consider the principle of self-adaptation which is explicitly used in
evolutionary programming (EP) [27, 26] and evolution strategies (ES) [49, 55]
while it is rarely used in genetic algorithms (GA) [35, 36].
Individuals of a population have a set of object parameters that serves as a
representative of possible solutions. The basic idea of explicit self-adaptation
consists in incorporating the strategy parameters into the individual’s genome
and evolving them alongside with the object parameters.
In this paper, we will give an overview over the self-adaptive behavior of
evolutionary algorithms. We will start with a short overview over the histor-
ical development of adaptation mechanisms in evolutionary computation. In
the following part, i.e., Section 2.2, we will introduce classiﬁcation schemes
that are used to group the various approaches. Afterwards, self-adaptive
mechanisms will be considered. The overview is started by some examples
– introducing self-adaptation of the strategy parameter and of the crossover
operator. Several authors have pointed out that the concept of self-adaptation
may be extended. Section 3.2 is devoted to such ideas. The mechanism of self-
adaptation has been examined in various areas in order to ﬁnd answers to the
question under which conditions self-adaptation works and when it could fail.
In the remaining sections, therefore, we present a short overview over some of
the research done in this ﬁeld.
2 Adaptation and Self-Adaptation
2.1 A Short History of Adaptation in Evolutionary Algorithms
In this section, we will shortly review the historical development of adaptation
mechanisms. The ﬁrst proposals to adjust the control parameters of a compu-
tation automatically date back to the early days of evolutionary computation.
In 1967, Reed, Toombs, and Barricelli [51] experimented with the evolution
of probabilistic strategies playing a simpliﬁed poker game. Half of a player’s
genome consisted of strategy parameters determining, e.g., the probabilities
for mutation or the probabilities for crossover with other strategies. Interest-
ingly, it was shown for a play for which a known optimal strategy existed that
the evolutionary simulation realized nearly optimal plans.
Also in 1967, Rosenberg [52] proposed to adapt crossover probabilities.
Concerning genetic algorithms, Bagley [9] considered incorporating the con-
trol parameters into the representation of an individual. Although Bagley’s
suggestion is one of the earliest proposals of applying classical self-adaptive
methods, self-adaptation as usually used in ES appeared relatively late in
genetic algorithms. In 1987, Schaﬀer and Morishima [54] introduced the self-
adaptive punctuated crossover adapting the number and location of crossover
points. Some years later, a ﬁrst method to self-adapt the mutation operator
was suggested by B¨ack [4, 4]. He proposed a self-adaptive mutation rate in
genetic algorithms similar to evolution strategies.

Self-Adaptation in Evolutionary Algorithms
49
The idea of using a meta-GA can be found quite early. Here, an upper-
level GA tries to tune the control parameters of a lower-level algorithm which
tries in turn to solve the original problem. The ﬁrst suggestion stems from
Weinberg [71] giving rise to the work by Mercer and Sampson [45].
Concerning evolution strategies, the need to adapt the mutation strength
(or strengths) appropriately during the evolutionary process was recognized
in Rechenberg’s seminal book Evolutionsstrategie [63].
He proposed the well-known, which was originally developed for (1+1)-ES.
It relies on counting the successful and unsuccessful mutations for a certain
number of generations. If more than 1/5th of mutations leads to an improve-
ment the mutation strength is increased and decreased otherwise. The aim
was to stay in the so-called evolution window guaranteeing nearly optimal
progress.
In addition to the 1/5th rule, Rechenberg [63] also proposed to couple the
evolution of the strategy parameters with that of the object parameters. The
strategy parameters were randomly changed. The idea of (explicit) was born.
To compare the performance of this learning population with that of an ES
using the 1/5th rule, Rechenberg conducted some experiments on the sphere
and corridor model. The learning population exhibited a higher convergence
speed and even more important it proved to be applicable in cases where it
is improper to use the 1/5th rule. Self-adaptation thus appeared as a more
universally usable method.
Since then various methods for adapting control parameters in evolution-
ary algorithms have been developed – ranging from adapting crossover prob-
abilities in genetic algorithms to a direct adaptation of the distribution [16].
Schwefel [71, 72] introduced a self-adaptive method for changing the strat-
egy parameters in evolution strategies which is today commonly associated
with the term self-adaptation. In its most general form, the full covariance
matrix of a general multidimensional normal distribution is adapted. A simi-
lar method of adapting the strategy parameters was oﬀered by Fogel et al. [25]
in the area of evolutionary programming – the so-called meta-EP operator for
changing the mutation strength.
A more recent technique, the cumulative path-length control, stems from
Ostermeier, Hansen, and Gawelczyk [48]. One of the aims is to derandomize
the adaptation of the strategy parameters. The methods developed, the cu-
mulative step-size adaptation (CSA) as well as the covariance matrix adap-
tation (CMA) [30], make use of an evolution path, p(g+1) = (1 −c)p(g) +

c(2 −c)z(g+1)
sel
, which cumulates the selected mutation steps. To illustrate
this concept, consider an evolution path where purely random selection is ap-
plied. Since the mutations are normally distributed, the cumulated evolution
path is given by u(g) = g
k=1 σN (k)(0, 1), where N(0, 1) is a random vector
with identically independently distributed N(0, 1) normally distributed com-
ponents with zero mean and variance 1. The length of u(g) is χ-distributed
with expectation u = σχ. Fitness based selection changes the situation. Too
large mutation steps result in a selection of smaller mutations. Thus, the

50
Silja Meyer-Nieberg and Hans-Georg Beyer
path-length is smaller than u and the step size should be decreased. If on the
other hand the path-length is larger than the expected u, the step-size should
be increased. CSA is also used in the CMA-algorithm. However, additionally
the CMA adapts the whole covariance matrix [30] and as such it represents
the state-of-the-art in real-coded evolutionary optimization algorithms.
2.2 A Taxonomy of Adaptation
As we have seen, various methods for changing and adapting control parame-
ters of evolutionary algorithms exist and adaptation can take place on diﬀerent
levels.
Mainly, there are two taxonomy schemes [2, 22] which group adaptive
computations into distinct classes – distinguishing by the type of adapta-
tion, i.e., how the parameter is changed, and by the level of adaptation, i.e.
where the changes occur. The classiﬁcation scheme of Eiben, Hinterding, and
Michaelewicz [22] extends and broadens the concepts introduced by Angeline
in [2].
Let us start with Angeline’s classiﬁcation [2]. Considering the type of adap-
tation, adaptive evolutionary computations are divided into algorithms with
absolute update rules and empirical update rules.
If an absolute update rule is applied, a statistic is computed. This may
be done by sampling over several generations or by sampling the population.
Based on the result, it is decided by means of a deterministic and ﬁxed rule
if and how the operator is to be changed. Rechenberg’s 1/5th-rule [63] is one
well-known example of this group.
In contrast to this, evolutionary algorithms with empirical update rules
control the values of the strategy parameters themselves. The strategy para-
meter may be interpreted as an incorporated part of the individual’s genome,
thus being subject to “genetic variations”. In the case the strategy parameter
variation leads to an individual with a suﬃciently good ﬁtness, it is selected
and “survives”. Individuals with appropriate strategy parameters should –
on average – have good ﬁtness values and thus a higher chance of survival
than those with badly tuned parameters. Thus, the EA should be able to
self-control the parameter change.
As Smith [63] points out, the diﬀerence of the algorithms lies in the nature
of the transition function. The transition function maps the set of parameters
at generation t on that at t+1. In the case of absolute update rules, it is deﬁned
externally. In the case of self-adaptive algorithms, the transition function is a
result of the operators and is deﬁned by the algorithm itself.
Both classes of adaptive evolutionary algorithms can be further subdivided
based on the level the adaptive parameters operate on. Angeline distinguished
between population-, individual-, and component-level adaptive parameters.
Population-level adaptive parameters are changed globally for the whole
population. Examples are for instance the mutation strength and the covari-
ance matrix adaptation in CSA and CMA evolution strategies.

Self-Adaptation in Evolutionary Algorithms
51
Adaptation on the individual-level changes the control parameters of an
individual and these changes only aﬀect that individual. The probability for
crossover in GA is for instance adapted in [54] on the level of individuals.
Finally, component-level adaptive methods aﬀect each component of an
individual separately. Self-Adaptation in ES with correlated mutations (see
Section 3.1) belongs to this type.
Angeline’s classiﬁcation was broadened by Eiben, Hinterding, and
Michaelewicz [22]. Adaptation schemes are again classiﬁed ﬁrstly by the type
of adaptation and secondly – as in [2] – by the level of adaptation. Considering
the diﬀerent levels of adaptation a fourth level, environment level adaptation,
was introduced to take into account cases where the responses of the environ-
ment are not static.
Concerning the adaptation type, in [22] the algorithms are ﬁrst divided
into static, i.e., no changes of the parameters occur, and dynamic algorithms.
The term “dynamic adaptation” is used to classify any algorithm where the
strategy parameters according to some rule, i.e., without external control.
Based on the mechanism of adaptation three subclasses are distinguished: de-
terministic, adaptive, and ﬁnally self-adaptive algorithms. The latter comprise
the same class of algorithms as in [2].
A deterministic adaptation is used if the control parameter is changed ac-
cording to a deterministic rule without taking into account any present infor-
mation by the evolutionary algorithm itself. Examples of this adaptation class
are the time-dependent change of the mutation rates proposed by Holland [36]
and the cooling schedule in simulated annealing like selection schemes.
Algorithms with an adaptive dynamic adaptation rule take feedback from
the EA itself into account and change the control parameters accordingly.
Again, a well known member of this class is Rechenberg’s 1/5th-rule. Further
examples (see [22]) include Davis’ adaptive operator ﬁtness [15] and Julstrom’s
adaptive mechanism [38]. In both cases, the usage probability of an operator
depends on its success or performance.
3 Self-Adaptation: The Principles
3.1 Self-Adapted Parameters: Some Examples
Self-Adaptation of Strategy Parameters
The technique most commonly associated with the term was introduced by
Rechenberg [63] and Schwefel [71, 57] in the area of evolution strategies and
independently by Fogel [25] for evolutionary programming. The control para-
meters considered here apply to the mutation process and parameterize the
mutation distribution. The mutation is usually given by a normally distribu-
tion random vector, i.e. Z ∼N(0, C). The entries cij of the covariance matrix
C are given by cii = var(Zi) or by cij = cov(Zi, Zj) if j ̸= i. The density
function reads

52
Silja Meyer-Nieberg and Hans-Georg Beyer
pZ(Z1, . . . , ZN) =
e−1
2 ZTC−1Z

(2π)Ndet(C)
,
(1)
where N is the dimensionality of the search space. The basic step in the
self-adaptation mechanism consists of a mutation of the mutation parameters
themselves. In contrast to the additive change of the object variables, the mu-
tation of the mutation strength strengths (i.e., the standard deviations √cii in
(1)) is realized by a multiplication with a random variable. The resulting mu-
tation parameters are then applied in the variation of the object parameters.
It should be mentioned here that concerning evolution strategies, the concept
of self-adaptation was originally developed for non-recombinative (1, λ)-ES.
Later on it was transferred to multi-parent strategies.
Figure 1 illustrates the basic mechanism of a multi-parent (µ/ρ, λ)-ES with
σ-self-adaptation. At generation g the ES maintains a population of µ can-
didate solutions – with the strategy parameters used in their creation. Based
on that parent population, λ oﬀspring are created via variation. The variation
process usually comprises recombination and mutation. For each oﬀspring, ρ
parents are chosen for the recombination. First, the strategy parameters are
changed. The strategy parameters of the chosen ρ parents are recombined
and the result is mutated afterwards. The change of the object parameters
occurs in the next step. Again, the parameters are ﬁrst recombined and then
mutated. In the mutation process, the newly created strategy parameter is
used. After that, the ﬁtness of the oﬀspring is calculated. Finally, the µ-best
individuals are chosen according to their ﬁtness values as the next parental
population. Two selection schemes are generally distinguished: “comma” and
“plus”-selection. In the former case, only the oﬀspring population is consid-
ered. In the latter, the new parent population is chosen from the old parent
population and the oﬀspring population.
Depending on the form of C, diﬀerent mutation distributions have to be
taken into account. Considering the simplest case Z = σN(0, I), the mutation
of σ is given by
σ′ = σeτϵ
(2)
and using the new σ′, the mutation of the object parameters reads
x′
i = xi + σ′N(0, 1).
(3)
The ϵ in Eq. (2) is a random number, often chosen as
ϵ ∼N(0, 1),
(4)
thus, producing log-normally distributed σ′ variants. This way of choosing ϵ
is also referred to as the “log-normal mutation rule”. Equation (2) contains a
new strategy speciﬁc parameter – the learning rate τ to be ﬁxed. The general
recommendation is to choose τ ∝1/
√
N, which has been shown to be optimal
with respect to the convergence speed on the sphere [11].

Self-Adaptation in Evolutionary Algorithms
53
BEGIN
g:=0
INITIALIZATION
P(0)
µ
:= 
y(0)
m , σ(0)
m , F(y(0)
m )
REPEAT
FOR EACH OF THE λ OFFSPRING DO
Pρ:=REPRODUCTION(P(g)
µ )
σ′
l :=RECOMBσ(Pρ);
σl :=MUTATEσ(σ′
r);
y′
l :=RECOMBy(Pρ);
yl :=MUTATEy(x′
l, σl);
Fl := F(yl);
END
P(g)
λ :=
(yl, σl, Fl)
CASE “,”-SELECTION: P(g+1)
µ
:=SELECT(P(g)
λ )
CASE “+”-SELECTION: P(g+1)
µ
:=SELECT(P(g+1)
µ
,P(g)
λ )
g:=g+1
UNTIL stop;
END
Fig. 1. The (µ/ρ, λ)-σSA-ES.
If diﬀerent mutation strengths are used for each dimension, i.e., Zi =
σiN(0, 1), the update rule
σ′
i = σi exp

τ ′N(0, 1) + τNi(0, 1)

(5)
x′
i = xi + σ′
iN(0, 1)
(6)
has been proposed. It is recommended [57] to choose the learning rates τ ′ ∝
1/
√
2N and τ ∝1/

2
√
N. The approach can also be extended to allow
for correlated mutations [6]. Here, rotation angles αi need to be taken into
account leading to the update rule
σ′
i = σi exp

τ ′N(0, 1) + τNi(0, 1)

(7)
α′
i = αi + βNi(0, 1)
(8)
x′ = x + N

0, C(σ′, α)

(9)
where C is the covariance matrix [6]. The parameter β is usually [6] chosen
as 0.0873.
In EP [25], a diﬀerent mutation operator, called meta-EP, is used
σ′
i = σi

1 + αN(0, 1)

(10)
x′
i = xi + σ′
iN(0, 1).
(11)
Both operators lead to similar results – provided that the parameters τ and
α are suﬃciently small.

54
Silja Meyer-Nieberg and Hans-Georg Beyer
The log-normal operator, Eqs. (2), (3), and the meta-EP operator intro-
duced above are not the only possibilities. Self-Adaptation seems to be rela-
tively robust as to the choice of the distribution. Another possible operator is
given by ϵ = ±δ, where +|δ| and −|δ| are generated with the same probability
of 1/2. That is, the resulting probability distribution function (pdf) of δ is a
two-point distribution giving rise to the so-called two-point rule. It is usually
implemented using δ = 1/τ ln(1 + β), thus, leading with (2) to
σ′
i =
 σi(1 + β),
if u ≤0.5
σi/(1 + β), if u > 0.5
(12)
where u is a random variable uniformly distributed on (0, 1].
Another choice was proposed by Yao and Liu [73]. They substituted the
normal distribution of the meta-EP operator with a Cauchy-distribution.
Their new algorithm, called fast evolutionary programming, performed well on
a set of test functions and appeared to be preferable in the case of multi-modal
functions. The Cauchy-distribution is similar to the normal distribution but
has a far heavier tail. Its moments are undeﬁned. In [42], Lee and Yao pro-
posed to use a L´evy-distribution. Based on results on a suite of test functions,
they argued that using L´evy-distributions instead of normal distribution may
lead to higher variations and a greater diversity. Compared to the Cauchy-
distribution, the L´evy-distribution allows for a greater ﬂexibility since the
Cauchy-distribution appears as a special case of the L´evy-distribution.
Self-Adaptation of Recombination Operators
Crossover is traditionally regarded as the main search mechanism in genetic
algorithm and most eﬀorts to self-adapt the characteristics of this operator
stem from this area.
Schaﬀer and Morishima [54] introduced punctuated crossover which adapts
the positions where crossover occurs. An individual’s genome is complemented
with a bitstring encoding crossover points. A position in this crossover map is
changed in the same manner as its counterpart in the original genome. Schaﬀer
and Morishima reported that punctuated crossover performed better than
one-point crossover. Spears [67] points out, however, that the improvement
of the performance might not necessarily be due to self-adaptation but due
to the diﬀerence between crossover with more than one crossover point and
one-point crossover.
Spears [67] self-adapted the form of the crossover operator using an addi-
tional bit to decide whether two-point or uniform crossover should be used for
creating the oﬀspring. Again, it should be noted that Spears attributes the
improved performance not to the self-adaptation process itself but rather to
the increased diversity that is oﬀered to the algorithm.
Smith and Fogarty [62] introduced the so-called LEGO-algorithm, a link-
age evolving genetic algorithm. The objects which are adapted are blocks, i.e.

Self-Adaptation in Evolutionary Algorithms
55
linked neighboring genes. Each gene has two additional bits which indicate
whether it is linked to its left and right neighbors. Two neighboring genes are
then called linked if the respective bits are set. Oﬀspring are created via suc-
cessive tournaments. The positions of an oﬀspring are ﬁlled from left to right
by a competition between parental blocks. The blocks have to be eligible, i.e.,
they have to start at the position currently considered. The ﬁttest block is
copied as a whole and then the process starts anew. More than two parents
may contribute in the creation of an oﬀspring. Mutation also extends to the
bits indicating linked genes.
3.2 A Generalized Concept of Self-Adaptation
In [6], two of self-adaptation have been identiﬁed: Self-adaptation aims at
biasing the population distribution to more appropriate regions of the search
space by making use of an indirect link between good strategy values and
good object variables. Furthermore, self-adaptation relies on a population’s
diversity. While the adaptation of the operator ensures a good convergence
speed, the degree of diversity determines the convergence reliability. More
generally speaking, self-adaptation controls the relationship between parent
and oﬀspring population, i.e., the transmission function (see e.g. Altenberg
[1]). The control can be direct by manipulating control parameters in the
genome or more implicit. In the following, we will see that self-adaptation of
strategy parameters can be put into a broader context.
Igel and Toussaint [37] considered the eﬀects of neutral genotype-phenotype
mapping. They pointed out that neutral genome parts give an algorithm the
ability to “vary the search space distribution independent of phenotypic vari-
ation”. This may be regarded as one of the main beneﬁts of neutrality. Neu-
trality induces a redundancy in the relationship between genotype-phenotype.
But neutral parts can inﬂuence the exploration distribution and thus the pop-
ulation’s search behavior. This use of neutrality is termed generalized self-
adaptation. It also comprises the classical form of self-adaptation since the
strategy parameters adapted in classical self-adaptation belong to the neutral
part of the genome.
More formally, generalized self-adaptation is deﬁned as “adaptation of the
exploration distribution P (t)
P
by exploiting neutrality – i.e. independent of
changing phenotypes in the population, of external control, and of changing
the genotype-phenotype mapping” [37]. Igel and Toussaint showed addition-
ally that neutrality cannot be seen generally as a disadvantage. Although the
state space is increased, it might not necessarily lead to a signiﬁcant degra-
dation of the performance.
In [28], Glickman and Sycara referred to an implicit self-adaptation caused
by a non-injective genotype-phenotype mapping. Again there are variations of
the genome that do not alter the ﬁtness value but inﬂuence the transmission
function which induces a similar eﬀect.

56
Silja Meyer-Nieberg and Hans-Georg Beyer
Beyer and Deb [18] pointed out that in well-designed real-coded GAs,
the parent oﬀspring transmission function is controlled by the characteristics
of the parent population. Thus, the GA performs an implicit form of self-
adaptation. In contrast to the explicit self-adaptation in ES, an individual’s
genome does not contain any control parameters. Deb and Beyer [20] examined
the dynamic behavior of real-coded genetic algorithm (RCGA) that apply
simulated binary crossover (SBX) [17, 21]. In SBX, two parents x1 and x2
create two oﬀspring y1 and y2 according to
y1
i = 1/2

(1 −βi)x1
i + (1 + βi)x2
i

y2
i = 1/2

(1 + βi)x1
i + (1 −βi)x2
i

.
(13)
The random variable β has the density
p(β) =

1/2(η + 1)βη,
if 0 ≤β ≤1
1/2(η + 1)β−η−2,
if β > 1
.
(14)
The authors pointed out that these algorithms show self-adaptive behavior
although an individual’s genome does not contain any control parameters.
Well-designed crossover operators create oﬀspring depending on the diﬀerence
in parent solutions. The spread of children solutions is in proportion to the
spread of the parent solutions, and near parent solutions are more likely to be
chosen as children solutions than solutions distant from parents [20]. Thus, the
diversity in the parental population controls that of the oﬀspring population.
Self-adaptation in evolution strategies has similar properties. In both cases,
oﬀspring closer to the parents have a higher probability to be created than
individuals further away. While the implicit self-adaptability of real-coded
crossover operators is well understood today, it is interesting to point out
that even the standard one- or k-point crossover operators operating on binary
strings do have this property: Due to the mechanics of these operators, bit
positions which are common in both parents are transferred to the oﬀspring.
However, the other positions are randomly ﬁlled. From this point of view,
crossover can be seen as a self-adaptive mutation operator, which is in contrast
to the building block hypothesis [29] usually oﬀered to explain the working of
crossover in binary GAs.
3.3 Demands on the Operators: Real-coded Algorithms
Let us start with rules [12, 11, 13] for the design of mutation operators that
stem from analyzes of implementations and theoretical considerations in evo-
lution strategies: reachability, scalability, and unbiasedness. They state that
every ﬁnite state must be reachable, the mutation operator must be tunable in
order to adapt to the ﬁtness landscape (scalability), and it must not introduce
a bias on the population. The latter is required to hold also for the recom-
bination operator [12, 40]. The demand of unbiasedness becomes clear when

Self-Adaptation in Evolutionary Algorithms
57
considering that the behavior of an EA can be divided into two phases: Ex-
ploitation of the search space by selecting good solutions (reproduction) and
exploration of the search space by means of variation. Only the former gen-
erally makes use of ﬁtness information, whereas ideally the latter should only
rely on search space information of the population. Thus, under a variation
operator, the expected population mean should remain unchanged, i.e., the
variation operators should not bias the population. This requirement, ﬁrstly
made explicit in [12] may be regarded as a basic design principle for variation
operators in EAs. The basic work [12] additionally proposed design principles
with respect to the changing behavior of the population variance. Generally,
selection changes the population variance. In order to avoid premature conver-
gence, the variation operator must counteract that eﬀect of the reproduction
phase to some extend. General rules how to do this are, of course, nearly im-
possible to give but some minimal requirements can be proposed concerning
the behavior on certain ﬁtness landscapes [12].
For instance, Deb and Beyer [12] postulated that the population variance
should increase exponentially with the generation number on ﬂat or linear
ﬁtness functions. As pointed out by Hansen [31] this demand might not be
suﬃcient. He proposed a linear increase of the expectation of the logarithm of
the variance. Being based on the desired behavior in ﬂat ﬁtness landscapes,
Beyer and Deb [12] advocate applying variation operators that increase the
population variance also in the general case of unimodal ﬁtness functions.
While the variance should be decreased if the population brackets the opti-
mum, this should not be done by the variation operator. Instead, this task
should be left to the selection operator. In the case of crossover operators in
real-coded genetic algorithms (RCGA), similar guidelines have been proposed
by Kita and Yamamura [40]. They suppose that the distribution of the par-
ent population indicates an appropriate region for further search. As before,
the ﬁrst guideline states that the statistics of the population should not be
changed. This applies here to the mean as well as to the variance-covariance
matrix. Additionally, the crossover operator should lead to as much diversity
in the oﬀspring population as possible. It is noted that the ﬁrst guideline
may be violated since the selection operator typically reduces the variance.
Therefore, it might be necessary to increase the present search region.
4 Self-Adaptation in EAs: Theoretical and Empirical
Results
4.1 Genetic Algorithms
In this section, we will review the empirical and theoretical research that has
been done in order to understand the working of self-adaptive EAs and to
evaluate their performance.

58
Silja Meyer-Nieberg and Hans-Georg Beyer
Self-Adaptation of the Crossover Operator
Real-Coded GAs in Flat Fitness Landscapes
Beyer and Deb [19] analyzed three crossover operators commonly used in real-
coded GAs, i.e., the simulated binary crossover (SBX) by Deb and Agrawala
[17], the blend crossover operator (BLX) of Eshelman and Schaﬀer [23], and
the fuzzy recombination of Voigt et al. [70].
In [19], expressions for the mean and the variance of the oﬀspring popula-
tion in relation to the parent population are derived. The aim is to examine
if and under which conditions the postulates proposed in Section 3.3 are ful-
ﬁlled. The ﬁtness environments considered are ﬂat ﬁtness landscapes and the
sphere. As mentioned before in Section 3.3, the self-adaptation should not
change the population mean in the search space, i.e., it should not introduce
a bias, but it should – since a ﬂat ﬁtness function is considered – increase the
population variance and this exponentially fast.
It was shown in [19] that the crossover operator leaves the population mean
unchanged regardless of the distribution of the random variable. Concerning
the population variance, an exponential change can be asserted. Whether the
variance expands or contracts depends on the population size and on the
second moment of the random variable. Thus, a relationship between the
population size and the distribution parameters of the random variables can
be derived which ensures an expanding population.
Kita [39] investigated real-coded genetic algorithms using UNDX-crossover
(unimodal normal distribution) and performed a comparison with evolution
strategies. Based on empirical results, he pointed out that both appear to work
reasonably well although naturally some diﬀerences in their behavior can be
observed. The ES for example widens the search space faster when the system
is far away from an optimum. On the other hand, the RCGA appears to have
a computational advantage in high-dimensional search spaces compared to an
ES which adapts the rotation angles of the covariance matrix according to
Eqs. (7)–(9). Kita used a (15, 100)-ES with the usual recommendations for
setting the learning rates.
Self-Adaptation of the Mutation Rate in Genetic Algorithms
Traditionally, the crossover (recombination) operator is regarded as the main
variation operator in genetic algorithms, whereas the mutation operator was
originally proposed as a kind of “background operator” [36] endowing the
algorithm with the potential ability to explore the whole search space. Actu-
ally, there are good reasons to consider this as a reasonable recommendation
in GAs with genotype-phenotype mapping from Bℓ→Rn. As has been shown
in [12], standard crossover of the genotypes does not introduce a bias on the
population mean in the phenotype space. Interestingly, this does not hold for
bit-ﬂip mutations. That is, mutations in the genotype space result in a biased

Self-Adaptation in Evolutionary Algorithms
59
phenotypic population mean – thus violating the postulates formulated in
[12]. On the other hand, over the course of the years it was observed that for
GAs on (pseudo) boolean functions (i.e., the problem speciﬁc search space is
the Bℓ) the mutation operator might also be an important variation operator
to explore the search space (see e.g. [68]). Additionally, it was found that the
optimal mutation rate or mutation probability does not only depend on the
function to be optimized but also on the search space dimensionality and the
current state of the search (see e.g. [5]).
A mechanism to self-adapt the mutation rate was proposed by B¨ack [4, 4]
for GA using the standard ES approach. The mutation rate is encoded as a bit-
string and becomes part of the individual’s genome. As it is common practice,
the mutation rate is mutated ﬁrst which requires its decoding to [0, 1]. The
decoded mutation rate is used to mutate the positions in the bit-string of the
mutation rate itself. The mutated version of the mutation probability is then
decoded again in order to be used in the mutation of the object variables.
Several investigations have been devoted to the mechanism of self-adap-
tation in genetic algorithms. Most of the work is concentrated on empirical
studies which are directed to possible designs of mutation operators trying to
identify potential beneﬁts and drawbacks.
B¨ack [4] investigated the asymptotic behavior of the encoded mutation
rate by neglecting the eﬀects of recombination and selection. The evolution
of the mutation rate results in a Markov chain3. Zero is an absorbing state of
this chain which shows the convergence of the simpliﬁed algorithm.
The author showed empirically that in the case of GA with an extinctive
selection scheme4 self-adaptation is not only possible but can be beneﬁcial [4].
For the comparison, three high-dimensional test functions (two unimodal, one
multimodal) were used.
In [4], a self-adaptive GA optimizing the bit-counting function was ex-
amined. Comparing its performance with a GA that applies an optimal de-
terministic schedule to tune the mutation strength, it was shown that the
self-adaptive algorithm realizes nearly optimal mutation rates.
The encoding of the mutation rate as a bit-string was identiﬁed in [8]
as an obstacle for the self-adaptation mechanism. To overcome this problem,
B¨ack and Sch¨utz [8] extended the genome with a real-coded mutation rate
p ∈]0, 1[. Several requirements have to be fulﬁlled. The expected change of
p should be zero and small changes should occur with a higher probability
than large ones. Also, there is the symmetry requirement that a change by a
factor c should have the same probability as by 1/c. In [8], a logistic normal
distribution with parameter γ was used. The algorithm was compared with a
GA without any adaptation and with a GA that uses a deterministic time-
dependent schedule. Two selection schemes were considered. The GA with
3 A Markov chain is a stochastic process which possesses the Markov property, i.e.,
the future behavior depends on the present state but not on the past.
4 A selection scheme is extinctive iﬀat least one individual is not selected (see [4]).

60
Silja Meyer-Nieberg and Hans-Georg Beyer
the deterministic schedule performed best on the test-problems chosen with
the self-adaptive GA in second place. Unfortunately, the learning rate γ was
found to have a high impact.
Considering the originally proposed algorithm [4], Smith [65] showed that
prematurely reduced mutation rates can occur. He showed that this can be
avoided by using a ﬁxed learning rate for the bitwise mutation of the mutation
rate.
In 1996, Smith and Fogarty [74] examined empirically a self-adaptive
steady state (µ + 1)-GA ﬁnding that self-adaptation may improve the per-
formance of a GA. The mutation rate was encoded again as a bit-string and
several encoding methods were applied. Additionally, the impact of crossover
in combination with a self-adaptive mutation rate was investigated. The self-
adaptive GA appeared to be relative robust with respect to changes of the
encoding or crossover.
In [66], the authors examined the eﬀect of self-adaptation when the cross-
over operator and the mutation rate are both simultaneously adapted. It ap-
peared that at least on the ﬁtness functions considered synergistic eﬀects
between the two variation operators come into play.
Trying to model the behavior of self-adaptive GAs, Smith [75] developed
a model to predict the mean ﬁtness of the population. In the model several
simpliﬁcations are made. The mutation rate is only allowed to assume q diﬀer-
ent values. Because of this, Smith also introduced a new scheme for mutating
the mutation rate. The probability of changing the mutation rate is given by
Pa = z(q −1)/q, where z is the so-called innovation rate.
In [77], Stone and Smith compared a self-adaptive GA using the log-normal
operator with a GA with discrete self-adaptation, i.e., a GA implementing the
model proposed in [75]. To this end, they evaluated the performance of a self-
adaptive GA with continuous self-adaptation and that of their model on a
set of ﬁve test functions. Stone and Smith found that the GA with discrete
self-adaptation behaves more reliably whereas the GA with continuous self-
adaptation may show stagnation. They attributed this behavior to the eﬀect
that the mutation rate gives the probability of bitwise mutation. As a result,
smaller diﬀerences between mutation strengths are lost and more or less the
same amount of genes are changed. The variety the log-normal operator pro-
vides cannot eﬀectively be carried over to the genome and the likelihood of
large changes is small. In addition, they argued that concerning the discrete
self-adaptation a innovation rate of one is connected with an explorative be-
havior of the algorithm. This appears more suitable for multimodal problems
whereas smaller innovation rates are preferable for unimodal functions.
4.2 Evolution Strategies and Evolutionary Programming
Research on self-adaptation in evolution strategies has a long tradition. The
ﬁrst theoretical in-depth analysis has been presented by Beyer [10]. It focused
on the conditions under which a convergence of the self-adaptive algorithm

Self-Adaptation in Evolutionary Algorithms
61
can be ensured. Furthermore, it also provided an estimate of the convergence
order.
The evolutionary algorithm leads to a stochastic process which can be
described by a Markov chain. The random variables chosen to describe the
system’s behavior are the object vector (or its distance to the optimizer, re-
spectively) and the mutation strength.
There are several approaches to analyze the Markov chain. The ﬁrst [14, 3]
considers the chain directly whereas the second [59, 60, 34] analyzes induced
supermartingales. The third [11, 18] uses a model of the Markov chain in order
to determine the dynamic behavior.
Convergence Results using Markov Chains
Bienven¨ue and Fran¸cois [14] examined the global convergence of adaptive and
self-adaptive (1, λ)-evolution strategies on spherical functions. To this end,
they investigated the induced stochastic process zt = ∥xt∥/σt. The parameter
σt denotes the mutation strength, whereas xt stands for the object parameter
vector.
They showed that (zt) is a homogeneous Markov chain, i.e., zt only de-
pends on zt−1. This also conﬁrms an early result obtained in [10] that the
evolution of the mutation strength can be decoupled from the evolution of
∥xt∥. Furthermore, they showed that (xt) converges or diverges log-linearly –
provided that the chain (zt) is Harris-recurrent5.
Auger [3] followed that line of research focusing on (1, λ)-ES optimizing
the sphere model. She analyzed a general model of an (1, λ)-ES with
xt+1 = arg min

f(xt + σtη1
t ξ1
t ), . . . , f(xt + σtηλ
t ξλ
t )

σt+1 = σtη∗(xt), η∗given by xt+1 = xt + σtη∗(xt)ξ∗(xt),
(15)
i.e., σt+1 is the mutation strength which accompanies the best oﬀspring. The
function f is the sphere and η and ξ are random variables. Auger proved
that the Markov chain given by zt = xt/σt is Harris-recurrent and positive if
some additional assumptions on the distributions are met and the oﬀspring
number λ is chosen appropriately. As a result, a law of large numbers can be
applied and 1/t ln(∥xt∥) and 1/t ln(σt) converge almost surely6 to the same
5 Let NA be the number of passages in the set A. The set A is called Harris-recurrent
if Pz(NA = ∞) = 1 for z ∈A. (Or in other words if the process starting from z
visits A inﬁnitely often with probability one.) A process (zt) is Harris-recurrent
if a measure ψ exists such that (zt) is ψ-irreducible and for all A with ψ(A) > 0,
A is Harris-recurrent (see e.g. [47]). A Markov process is called ϕ-irreducible if
a measure ϕ exists so that for every set A with ϕ(A) > 0 and every state x
holds: The return time probability to set A starting from x is greater than zero.
A process is then called ψ-irreducible if it is ϕ-irreducible and ψ is a maximal
irreduciblity measure fulﬁlling some additional propositions (see e.g. [47])
6 A sequence of random variables xt deﬁned on the probability space (Ω, A, P)
converges almost surely to a random variable x if P({ω ∈Ω| limt→∞xt(ω) =

62
Silja Meyer-Nieberg and Hans-Georg Beyer
quantity – the convergence rate. This ensures either log-linearly convergence
or divergence of the ES – depending on the sign of the limit. Auger further
showed that the Markov chain (zt) is also geometrically ergodic (see e.g. [47])
so that the Central Limit Theorem can be applied. As a result, it is possible
to derive a conﬁdence interval for the convergence rate. This is a necessary
ingredient, because the analysis still relies on Monte-Carlo simulations in order
to obtain the convergence rate (along with its conﬁdence interval) numerically
for the real (1, λ)-ES. In order to perform the analysis, it is required that the
random variable ξ is symmetric and that both random variables ξ and η must
be absolutely continuous with respect to the Lebesgue-measure. Furthermore,
the density pξ is assumed to be continuous almost everywhere, pξ ∈L∞(R),
and zero has to be in the interior of the support of the density, i.e., 0 ∈
˚
supp pξ.
Additionally, it is assumed that 1 ∈
˚
supp pη and that E[|ln(η)|] < ∞holds.
The requirements above are met by the distribution functions normally used
in practice, i.e., the log-normal distribution (mutation strength) and normal
distribution (object variable). In order to show the Harris-recurrence, the
positivity, and the geometric ergodicity, Forster-Lyapunov drift conditions
(see e.g. [47]) need to be obtained. To this end, new random variables are to
be introduced
ˆη(λ)ˆξ(λ) = min

η1ξ1, . . . , ηλξλ
.
(16)
They denote the minimal change of the object variable. For the drift conditions
a number α is required. Firstly, α has to ensure that the expectations E[|ξ|α]
and E[(1/η)α] are ﬁnite. Provided that also E[|1/ˆη(λ)|α] < 1, α can be used to
give a drift condition V . More generally stated, α has to decrease the reduction
velocity of the mutation strength associated with the best oﬀspring of λ trials
suﬃciently. Thus, additional conditions concerning α and the oﬀspring number
λ are introduced leading to the deﬁnition of the sets
Γ0 = {γ > 0 : E [|1/η|γ] < ∞and E [|ξ|γ] < ∞}
(17)
and
Λ =

α∈Γ0
Λα =

α∈Γ0
{λ ∈N : E [1/ˆη(λ)α] < 1} .
(18)
Finally, the almost sure convergence of 1/t ln(∥xt∥) and 1/t ln(σt) can be
shown for all λ ∈Λ. It is not straightforward to give expression for Λ or
Λα in the general case although Λα can be numerically obtained for a given
α. Only if the densities of η and ξ have bounded support, it can be shown
that Λα is of the form Λα = {λ : λ ≥λ0}.
x(ω)}) = 1. Therefore, events for which the sequence does not converge have
probability zero.

Self-Adaptation in Evolutionary Algorithms
63
Convergence Theory with Supermartingales
Several authors [59, 60, 34] use the concept of martingales or supermartin-
gales7 to show the convergence of an ES or to give an estimate of the conver-
gence velocity. As before, the random variables most authors are interested in
are the object variable and the mutation strength.
Semenov [59] and Semenov and Terkel [60] examined the convergence
and the convergence velocity of evolution strategies. To this end, they consider
the stochastic Lyapunov function Vt of a stochastic process Xt. By showing
the convergence of the Lyapunov function, the convergence of the original
stochastic process follows under certain conditions.
From the viewpoint of probability theory, the function Vt may be regarded
as a supermartingale. Therefore, a more general framework in terms of conver-
gence of supermartingales can be developed. The analysis performed in [60]
consists of two independent parts. The ﬁrst concerns the conditions that im-
ply almost surely convergence of supermartingales to a limit set. The second
part (see also [59]) proposes demands on supermartingales which allow for an
estimate of the convergence velocity. Indirectly, this also gives an independent
convergence proof.
The adaptation of the general framework developed for supermartingales
to the situation of evolution strategies requires the construction of an appro-
priate stochastic Lyapunov function. Because of the complicated nature of
the underlying stochastic process, the authors did not succeed in the rigor-
ous mathematical treatment of the stochastic process. Similar to the Harris-
recurrent Markov chain approach, the authors had to resort to Monte-Carlo
simulations in order to show that the necessary conditions are fulﬁlled.
In [59] and [60], (1, λ)-ES are considered where the oﬀspring are generated
according to
σt,l = σteϑt,l
xt,l = xt + σt,lζt,l
(19)
and the task is to optimize f(x) = −|x|. The random variables ϑt,l and ζt,l
are uniformly distributed with ϑt,l assuming values in [−2, 2] whereas ζt,l is
deﬁned on [−1, 1]. For this problem, it can be shown that the object variable
and the mutation strength converge almost surely to zero – provided that
there are at least three oﬀspring. Additionally, the convergence velocity of the
mutation strength and the distance to the optimizer is bounded from above
by exp(−at) which holds asymptotically almost surely.
Hart, DeLaurentis, and Ferguson [34] also used supermartingales in their
approach. They considered a simpliﬁed (1, λ)-ES where the mutations are
modeled by discrete random variables. This applies to the mutations of the
7 A random process Xt is called a supermartingale if E[|Xt|] < ∞and E[Xt+1|Ft] ≤
Xt where Ft is e.g. the σ-algebra that is induced by Xt.

64
Silja Meyer-Nieberg and Hans-Georg Beyer
object variables as well as to those of the mutation strengths. Oﬀspring are
generated according to
σt,l = σtD
xt,l = xt + σt,lB.
(20)
The random variable D assumes three values {γ, 1, η} with γ < 1 < η. The
random variable B takes a value of either +1 or −1 with probability 1/2 each.
Under certain assumptions, the strategy converges almost surely to the mini-
mum x∗of a function f : R →R which is assumed to be strictly monotonically
increasing for x > x∗and strictly monotonically decreasing for x < x∗.
As a second result, the authors proved that their algorithm fails to locate
the global optimum of an example multimodal function with probability one.
We will return to this aspect of their analysis in Section 5.
Instead of using a Lyapunov function as Semenov and Terkel did, they in-
troduced a random variable that is derived from the (random) object variable
and the mutation strength. It can be shown that this random variable is a
nonnegative supermartingale if certain requirements are met. In that case, it
can be shown that the ES converges almost surely to the optimal solution if
the oﬀspring number is suﬃciently high.
The techniques introduced in [34] can be applied to the multi-dimensional
case [33] provided that the ﬁtness function is separable, i.e., g(x)
=
N
k=0 gk(xk) and the gk fulﬁll the conditions for f. The authors considered an
ES-variant where only one dimension is changed in each iteration. The dimen-
sion k is chosen uniformly at random. Let Xt
λ,k and Σt
λ,k denote the stochastic
process that results from the algorithm. It can be shown that Xt
λ,1, . . . , Xt
λ,N
are independent of each other. This also holds for Σt
λ,1, . . . , Σt
λ,N . Therefore,
the results of the one-dimensional analysis can be directly transferred.
Although the analysis in [34, 33] provides an interesting alternative, it
is restricted to very special cases: Due to the kind of mutations used, the
convergence results in [34, 33] are, however, not practically relevant if the
number of oﬀspring exceeds six.
Step-by-step Approach: The Evolution Equations
In 1996, Beyer [10] was the ﬁrst to provide a theoretical framework for the
analysis of self-adaptive EAs. He used approximate equations to describe
the dynamics of self-adaptive evolution strategies. Let the random variable
r(g) = ∥X(g) −ˆX∥denote the distance to the optimizer and ς(g) the mutation
strength. The dynamics of an ES can be interpreted as a Markov process as
we have already seen. But generally, the transition kernels for

r(g)
ς(g)

→

r(g+1)
ς(g+1)

(21)

Self-Adaptation in Evolutionary Algorithms
65
cannot be analytically determined. One way to analyze the system is therefore
to apply a step by step approach extracting the important features of the
dynamic process and thus deriving approximate equations.
The change of the random variables can be divided into two parts. While
the ﬁrst denotes the expected change, the second covers the stochastic ﬂuctu-
ations
r(g+1) = r(g) −ϕ(r(g), ς(g)) + ϵR(r(g), ς(g))
ς(g+1) = ς(g)
1 + ψ(r(g)ς(g))

+ ϵσ(r(g), ς(g)).
(22)
The expected changes ϕ and ψ of the variables are termed progress rate if the
distance is considered and self-adaptation response in the case of the mutation
strength.
The distributions of the ﬂuctuation terms ϵR and ϵσ are approximated
using Gram-Charlier series’, usually cut oﬀafter the ﬁrst term. Thus, the
stochastic term is approximated using a normal distribution. The variance
can be derived considering the evolution equations. Therefore, the second
moments have to be taken into account leading to the second-order progress
rate and to the second-order self-adaptation response.
To analyze the self-adaptation behavior of the system, expressions for the
respective progress rate and self-adaptation response have to be found. Gen-
erally, no closed analytical solution can be derived. Up to now, only results for
(1, 2)-ES [11] using two-point mutations could be obtained. Therefore, several
simpliﬁcations have to be introduced. For instance, if the log-normal operator
is examined, the most important simpliﬁcation is to consider τ →0. The so
derived expressions are then veriﬁed by experiments.
Self-Adaptation on the Sphere Model
It has been shown in [11] that an (1, λ)-ES with self-adaptation converges
to the optimum log-linearly. Also the usual recommendation of choosing the
learning rate proportionally to 1/
√
N, where N is the search space dimension-
ality, is indeed approximately optimal. In the case of (1, λ)-ES, the dependency
of the progress on the learning rate is weak provided that τ ≥c/
√
N where
c is a constant. As a result, it is not necessary to have N-dependent learning
parameters.
As has been shown in [11], the time to adapt an ill-ﬁtted mutation strength
to the ﬁtness landscape is proportionally to 1/τ 2. Adhering to the scaling rule
τ ∝1/
√
N results in an adaptation time that linearly increases with the
search space dimensionality. Therefore, it is recommended to work with a
generation-dependent or constant learning rate τ, respectively, if N is large.
The maximal progress rate that can be obtained in experiments is always
smaller than the theoretical maximum predicted by the progress rate theory
(without considering the stochastic process dynamics). The reason for this
is that the ﬂuctuations of the mutation strength degrade the performance.

66
Silja Meyer-Nieberg and Hans-Georg Beyer
The average progress rate is deteriorated by a loss part stemming from the
variance of the strategy parameter. The theory developed in [11] is able to
predict this eﬀect qualitatively.
If recombination is introduced in the algorithm the behavior of the ES
changes qualitatively. Beyer and Gr¨unz [30] showed that multi-recombinative
ES that use intermediate/dominant recombination do not exhibit the same
robustness with respect to the choice of the learning rate as (1, λ)-ES. Instead
their progress in the stationary state has a clearly deﬁned optimum. Nearly
optimal progress is only attainable for a relatively narrow range of the learn-
ing rate τ. If the learning rate is chosen sub-optimally, the performance of
the ES degrades but the ES still converges to the optimum with a log-linear
rate. The reason for this behavior [46] is due to the diﬀerent eﬀects recombi-
nation has on the distance to the optimizer (i.e. on the progress rate) and on
the mutation strength. An intermediate recombination of the object variables
reduces the harmful parts of the mutation vector also referred to as “genetic
repair eﬀect”. Thus, it reduces the loss part of the progress rate. This enables
the algorithm to work with higher mutation strengths. However, since the
strategy parameters are necessarily selected before recombination takes place,
the self-adaptation response cannot reﬂect the after selection genetic repair
eﬀect and remains relatively inert to the eﬀect of recombination.
Flat and Linear Fitness Landscapes
In [12], the behavior of multi-recombinative ES on ﬂat and linear ﬁtness land-
scapes was analyzed. Accepting the variance postulates proposed in [12] (see
Section 3.3) the question arises whether the standard ES variation operators
comply to these postulates, i.e., whether the strategies are able to increase
the population variance in ﬂat and linear ﬁtness landscapes. Several com-
mon recombination operators and mutation operators were examined such
as intermediate/dominant recombination of the object variables and inter-
mediate/geometric recombination of the strategy parameters. The mutation
rules applied for changing the mutation strength are the log-normal and the
two-point distribution.
The analysis started with ﬂat ﬁtness landscapes which are selection neu-
tral. Thus, the evolution of the mutation strength and the evolution of the
object variables can be fully decoupled and the population variance can be
easily computed. Beyer and Deb showed that if intermediate recombination is
used for the object variables, the ES is generally able to increase the popula-
tion variance exponentially fast. The same holds for dominant recombination.
However, there is a memory of the old population variances that gradually
vanishes. Whether this is a beneﬁcial eﬀect has not been investigated up to
now.
In the case of linear ﬁtness functions, only the behavior of (1, λ)-ES has
been examined so far. It has been shown that the results obtained in [11] for
the sphere model can be transferred to the linear case if σ∗:= σN/R →0 is

Self-Adaptation in Evolutionary Algorithms
67
considered because the sphere degrades to a hyperplane. As a result, it can be
shown that the expectation of the mutation strength increases exponentially
if log-normal or two-point operators are used.
5 Problems and Limitations of Self-Adaptation
Most of the research done so far seems to be centered on the eﬀects of self-
adapting the mutation strengths. Some of the problems that were reported are
related to divergence of the algorithm (see e.g. Kursawe [41]) and premature
convergence. Premature convergence may occur if the mutation strength and
the population variance are decreased too fast. This generally results in a
convergence towards a suboptimal solution. While the problem is well-known,
it appears that only a few theoretical investigations have been done. However,
premature convergence is not a speciﬁc problem of self-adaptation. Rudolph
[53] analyzed an (1+1)-ES applying Rechenberg’s 1/5th-rule. He showed for a
test problem that the ES’s transition to the global optimum cannot be ensured
when the ES starts at a local optimum and if the step-sizes are decreased to
fast.
Stone and Smith [77] investigated the behavior of GA on multimodal func-
tions applying Smith’s discrete self-adaptation algorithm. Premature conver-
gence was observed for low innovation rates and high selection pressure which
causes a low diversity of the population. Diversity can be increased by using
high innovation rates. Stone and Smith additionally argued that one copy
of the present strategy parameter should be kept while diﬀerent choices are
still introduced. This provides a suitable relation between exploration and
exploitation.
Liang et al. [43, 44] considered the problem of a prematurely reduced
mutation strength. They started with an empirical investigation on the loss of
step size control for EP on ﬁve test functions [43]. The EP used a population
size of µ = 100 and a tournament size of q = 10. Stagnation of the search
occurred even for the sphere model. As they argued [43, 44], this might be
due to the selection of an individual with a mutation strength far too small
in one dimension but with a high ﬁtness value. This individual propagates
its ill-adapted mutation strength to all descendants and, therefore, the search
stagnates.
In [44], Liang et al. examined the probability of loosing the step size con-
trol. To simplify the calculations, a (1 + 1)-EP was considered. Therefore, the
mutation strength changes whenever a successful mutation happens. A loss of
step size control occurs if after κ successful mutations the mutation strength
is smaller than an arbitrarily small positive number ϵ. The probability of such
an event can be computed. It depends on the initialization of the mutation
strength, the learning parameter, on the number of successful mutations, and
on ϵ. As the authors showed, the probability of loosing control of the step size
increases with the number of successful mutations.

68
Silja Meyer-Nieberg and Hans-Georg Beyer
A reduction of the mutation strength should occur if the EP is already close
to the optimum. However, if the reduction of the distance to the optimizer
cannot keep pace with that of the mutation strength, the search stagnates.
This raises the question whether the operators used in this EP implementation
comply to the design principles postulated in [19] (compare Section 3.3 in this
paper). An analysis of the EP behavior in ﬂat or linear ﬁtness landscapes might
reveal the very reason for this failure. It should be also noted that similar
premature convergence behaviors of self-adaptive ES are rarely observed. A
way to circumvent such behavior is to introduce a lower bound for the step
size. Fixed lower bounds are considered in [43]. While this surely prevents
premature convergence of the EP, it does not take into account the fact that
the ideal lower bound of the mutation strength depends on the actual state
of the search.
In [44], two schemes are considered proposing a dynamic lower bound
(DLB) of the mutation strength. The ﬁrst is based on the success rate remi-
niscent of Rechenberg’s 1/5th-rule. The lower bound is adapted on the pop-
ulation level. A high success rate leads to an increase of the lower bound, a
small success decreases it. The second DLB-scheme is called “mutation step
size based”. For all dimensions, the average of the mutation strengths of the
successful oﬀspring is computed. The lower bound is then obtained as the
median of the averages. These two schemes appear to work well on most ﬁt-
ness functions of the benchmark suite. On functions with many local optima,
however, both methods exhibit diﬃculties.
As mentioned before, Hart, Delaurentis, and Ferguson [34] analyzed an
evolutionary algorithm with discrete random variables on a multi-modal func-
tion. They showed the existence of a bimodal function for which the algorithm
does not converge to the global optimizer with probability one if it starts close
to the local optimal solution.
Won and Lee [72] addressed a similar problem although in contrast to Hart,
DeLaurentis, and Ferguson they proved suﬃcient conditions for premature
convergence avoidance of a (1+1)-ES on a one-dimensional bimodal function.
The mutations are modeled using Cauchy-distributed random variables and
the two-point operator is used for changing the mutation strengths themselves.
Glickman and Sycara [28] identiﬁed possible causes for premature reduc-
tion of the mutation strength. They investigated the evolutionary search be-
havior of an EA without any crossover on a complex problem arising from the
training of neural networks with recurrent connections.
What they have called bowl eﬀect may occur if the EA is close to a local
minimum. Provided that the mutation strength is below a threshold then the
EA is conﬁned in a local attractor and it cannot ﬁnd any better solution. As
a result, small mutation strengths will be preferred.
A second cause is attributed to the selection strength. Glickman and
Sycara suspect that if the selection strength is high, high mutation rates
have a better chance of survival compared to using low selection strength:
A large mutation rate increases the variance. This is usually connected with

Self-Adaptation in Evolutionary Algorithms
69
a higher chance of degradation as compared to smaller mutation rates. But if
an improvement occurs it is likely to be considerably larger than those achiev-
able with small mutation rates. If only a small percentage of the oﬀspring is
accepted, there is a chance that larger mutation strengths “survive”. Thus,
using a high selection strength might be useful in safeguarding against prema-
ture stagnation. In their experiments, though, Glickman and Sycara could not
observe a signiﬁcant eﬀect. They attributed this in part to the fact that the
search is only eﬀective for a narrow region of the selection strength. Recently,
Hansen [31] resumed the investigation of the self-adaptive behavior of multi-
parent evolution strategies on linear ﬁtness functions started in [19]. Hansen’s
analysis is aimed at revealing the causes why self-adaptation usually works
adequately on linear ﬁtness functions. He oﬀered conditions under which the
control mechanism of self-adaptation fails, i.e., that the EA does not increase
the step size as postulated in [19]. The expectation of the mutation strength
is not measured directly. Instead, a function h is introduced the expectation
of which is unbiased under the variation operators. The question that now
remains to be answered is whether the selection will increase the expecta-
tion of h(σ). In other words, is the eﬀect of an increase of the expectation a
consequence of selection (and therefore due to the link between good object
vectors and good strategy values) or is it due to a bias introduced by the
recombination/mutation-operators chosen?
Hansen proposed two properties an EA should fulﬁll: First, the descen-
dants’ object vectors should be point-symmetrically distributed after mutation
and recombination. Additionally, the distribution of the strategy parameters
given the object vectors after recombination and mutation has to be identical
for all symmetry pairs around the point-symmetric center. Evolution strate-
gies with intermediate multi-recombination fulﬁll this symmetry assumption.
Their descendants’ distribution is point-symmetric around the recombination
centroid.
Secondly, Hansen oﬀered a so-called σ-stationarity assumption. It postula-
tes the existence of a monotonically increasing function h whose expectation is
left unbiased by recombination and mutation. Therefore, E[h(S
σi;λ|i=1,...,µ
k
)] =
1/µ µ
i=1 h(σi;λ) must hold for all oﬀspring. The term S
σi;λ|i=1,...,µ
k
denotes the
mutation strength of an oﬀspring k created by recombination and mutation.
Hansen showed that if an EA fulﬁlls the assumptions made above, self-
adaptation does not change the expectation of h(σ) if the oﬀspring number is
twice the number of parents.
The theoretical analysis was supplemented by an empirical investigation of
the self-adaptation behavior of some evolution strategies examining the eﬀect
of several recombination schemes on the object variables and on the strategy
parameter. It was shown that an ES which applies intermediate recombination
to the object variables and to the mutation strength increases the expectation
of log(σ) for all choices of the parent population size. On the other hand,
evolution strategies that fulﬁll the symmetry and the stationarity assumption,

70
Silja Meyer-Nieberg and Hans-Georg Beyer
increase the expectation of log(σ) if λ < µ/2, keep it constant for λ = µ/2
and decrease it for λ > µ/2.
Intermediate recombination of the mutation strengths results in an in-
crease of the mutation strength. This is beneﬁcial in the case of linear problems
and usually works as desired in practice, but it might also have unexpected
eﬀects in other cases.
6 Outlook
Self-adaptation usually refers to an adaptation of control parameters which
are incorporated into the individual’s genome. These are subject to variation
and selection - thus evolving together with the object parameters. Stating
it more generally, a self-adaptive algorithm controls the transmission func-
tion between parent and oﬀspring population by itself without any external
control. Thus, the concept can be extended to include algorithms where the
representation of an individual is augmented with genetic information that
does not code information regarding the ﬁtness but inﬂuences the transmis-
sion function instead.
Interestingly, real-coded genetic algorithms where the diversity of the par-
ent population controls that of the oﬀspring may be regarded as self-adaptive.
Surprisingly, even binary GAs with crossover operators as 1-point or k-point
crossover share self-adaptive properties to a certain extend.
Self-Adaptation is commonly used in the area of evolutionary program-
ming and evolution strategies. Here, generally the mutation strength or the
full covariance matrix is adapted. Analyses done so far focus mainly on the
convergence to the optimal solution. Nearly all analyses use either a simpli-
ﬁed model of the algorithm or have to resort to numerical calculations in
their study. The results obtained are similar: On simple ﬁtness functions con-
sidered, conditions can be derived that ensure the convergence of the EA to
local optimal solutions. The convergence is usually log-linear.
The explicit use of self-adaptation techniques is rarely found in genetic
algorithm and, if at all, mainly used to adopt the mutation rate. Most of the
studies found are directed at ﬁnding suitable ways to introduce self-adaptive
behavior in GA. As we have pointed out, however, crossover in binary standard
GAs does provide a rudimentary form of self-adaptive behavior. Therefore, the
mutation rate can be often kept at a low level provided that the population
size is reasonably large. However, unlike the clear goals in real-coded search
spaces, it is by no means obvious to formulate desired behaviors the self-
adaptation should realize in binary search spaces. This is in contrast to some
real-coded genetic algorithms where it can be shown mathematically that they
can exhibit self-adaptive behavior in simple ﬁtness landscapes.
It should be noted that self-adaptation techniques are not the means to
solve all adaptation problems in EAs. Concerning evolution strategies, multi-
recombinative self-adaptation strategies show sensitive behavior with respect

Self-Adaptation in Evolutionary Algorithms
71
to the choices of the external learning rate τ. As a result, an optimal or a
nearly optimal mutation strength is not always realized.
More problematic appear divergence or premature convergence to a subop-
timal solution. The latter is attributed to a too fast reduction of the mutation
strength. Several reasons for that behavior have been proposed although not
rigorously investigated up to now. However, from our own research we have
found that the main reason for a possible failure is due to the opportunis-
tic way how self-adaptation uses the selection information obtained from just
one generation. Self-adaptation rewards short-term gains. In its current form,
it cannot look ahead. As a result, it may exhibit the convergence problems
mentioned above.
Regardless of the problems mentioned, self-adaptation is a state-of-the-art
adaptation technique with a high degree of robustness, especially in real-coded
search spaces and in environments with uncertain or noisy ﬁtness information.
It also bears a large potential for further developments both in practical ap-
plications and in theoretical as well as empirical evolutionary computation
research.
Acknowledgements
This work was supported by the Deutsche Forschungsgemeinschaft (DFG)
through the Collaborative Research Center SFB 531 at the University of Dort-
mund and by the Research Center for Process- and Product-Engineering at
the Vorarlberg University of Applied Sciences.
References
1. L. Altenberg. The evolution of evolvability in genetic programming.
In
K. Kinnear, editor, Advances in Genetic Programming, pages 47–74. MIT Press,
Cambridge, MA, 1994.
2. P.J. Angeline. Adaptive and self-adaptive evolutionary computations. In
M. Palaniswami and Y. Attikiouzel, editors, Computational Intelligence: A Dy-
namic Systems Perspective, pages 152–163. IEEE Press, 1995.
3. A. Auger. Convergence results for the (1, λ)-SA-ES using the theory of φ-
irreducible Markov chains. Theoretical Computer Science, 334:35–69, 2005.
4. T. B¨ack. The interaction of mutation rate, selection, and self-adaptation within
a genetic algorithm. In R. M¨anner and B. Manderick, editors, Parallel Problem
Solving from Nature, 2, pages 85–94. North Holland, Amsterdam, 1992.
5. T. B¨ack. Optimal mutation rates in genetic search. In S. Forrest, editor, Pro-
ceedings of the Fifth International Conference on Genetic Algorithms, pages 2–8,
San Mateo (CA), 1993. Morgan Kaufmann.
6. T. B¨ack. Self-adaptation. In T. B¨ack, D. Fogel, and Z. Michalewicz, editors,
Handbook of Evolutionary Computation, pages C7.1:1–C7.1:15. Oxford Univer-
sity Press, New York, 1997.

72
Silja Meyer-Nieberg and Hans-Georg Beyer
7. Th. B¨ack. Self-adaptation in genetic algorithms. In F. J. Varela and P. Bourgine,
editors, Toward a Practice of Autonomous Systems: proceedings of the ﬁrst Eu-
ropean conference on Artiﬁcial Life, pages 263–271. MIT Press, 1992.
8. Th. B¨ack and M. Sch¨utz. Intelligent mutation rate control in canonical genetic
algorithms. In ISMIS, pages 158–167, 1996.
9. J. D. Bagley. The Behavior of Adaptive Systems Which Employ Genetic and
Correlation Algorithms. PhD thesis, University of Michigan, 1967.
10. H.-G. Beyer. Toward a theory of evolution strategies: Self-adaptation. Evolu-
tionary Computation, 3(3):311–347, 1996.
11. H.-G. Beyer. The Theory of Evolution Strategies. Natural Computing Series.
Springer, Heidelberg, 2001.
12. H.-G. Beyer and K. Deb. On self-adaptive features in real-parameter evolution-
ary algorithms. IEEE Transactions on Evolutionary Computation, 5(3):250–270,
2001.
13. H.-G. Beyer and H.-P. Schwefel. Evolution strategies: A comprehensive intro-
duction. Natural Computing, 1(1):3–52, 2002.
14. A. Bienven¨ue and O. Fran¸cois. Global convergence for evolution strategies in
spherical problems: Some simple proofs and d diﬃculties. Theoretical Computer
Science, 308:269–289, 2003.
15. L. Davis. Adapting operator probabilities in genetic algorithms. In J. D. Schaf-
fer, editor, Proc. 3rd Int’l Conf. on Genetic Algorithms, pages 61–69, San Mateo,
CA, 1989. Morgan Kaufmann.
16. M. W. Davis. The natural formation of gaussian mutation strategies in evolu-
tionary programming. In roceedings of the Third Annual Conference on Evolu-
tionary Programming, San Diego, CA, 1994. Evolutionary Programming Society.
17. K. Deb and R. B. Agrawal. Simulated binary crossover for continuous search
space. Complex Systems, 9:115–148, 1995.
18. K. Deb and H.-G. Beyer. Self-adaptation in real-parameter genetic algorithms
with simulated binary crossover. In W. Banzhaf, J. Daida, A.E. Eiben, M.H.
Garzon, V. Honavar, M. Jakiela, and R.E. Smith, editors, GECCO-99: Proceed-
ings of the Genetic and Evolutionary Computation Conference, pages 172–179,
San Francisco, CA, 1999. Morgan Kaufmann.
19. K. Deb and H.-G. Beyer. Self-adaptive genetic algorithms with simulated binary
crossover. Series CI 61/99, SFB 531, University of Dortmund, March 1999.
20. K. Deb and H.-G. Beyer. Self-adaptive genetic algorithms with simulated binary
crossover. Evolutionary Computation, 9(2):197–221, 2001.
21. K. Deb and M. Goyal. A robust optimization procedure for mechanical com-
ponent design based on genetic adaptive search. Transactions of the ASME:
Journal of Mechanical Desiogn, 120(2):162–164, 1998.
22. A. E. Eiben, R. Hinterding, and Z. Michalewicz. Parameter control in evolution-
ary algorithms. IEEE Transactions on Evolutionary Computation, 3(2):124–141,
1999.
23. L. J. Eshelman and J. D. Schaﬀer. Real-coded genetic algorithms and interval
schemata. In L. D. Whitley, editor, Foundations of Genetic Algorithms, 2, pages
187–202. Morgan Kaufmann, San Mateo, CA, 1993.
24. D. B. Fogel. Evolving Artiﬁcial Intelligence. PhD thesis, University of California,
San Diego, 1992.
25. D. B. Fogel, L. J. Fogel, and J. W. Atma. Meta-evolutionary programming. In
R.R. Chen, editor, Proc. of 25th Asilomar Conference on Signals, Systems &
Computers, pages 540–545, Paciﬁc Grove, CA, 1991.

Self-Adaptation in Evolutionary Algorithms
73
26. L. J. Fogel, A. J. Owens, and M. J. Walsh. Artiﬁcial Intelligence through Simu-
lated Evolution. Wiley, New York, 1966.
27. L.J. Fogel. Autonomous automata. Industrial Research, 4:14–19, 1962.
28. M. Glickman and K. Sycara. Reasons for premature convergence of self-adapting
mutation rates. In Proc. of the 2000 Congress on Evolutionary Computation,
pages 62–69, Piscataway, NJ, 2000. IEEE Service Center.
29. D.E. Goldberg. Genetic Algorithms in Search, Optimization, and Machine
Learning. Addison Wesley, Reading, MA, 1989.
30. L. Gr¨unz and H.-G. Beyer. Some observations on the interaction of recombi-
nation and self-adaptation in evolution strategies.
In P.J. Angeline, editor,
Proceedings of the CEC’99 Conference, pages 639–645, Piscataway, NJ, 1999.
IEEE.
31. N. Hansen. Limitations of mutative σ-self-adaptation on linear ﬁtness functions.
Evolutionary Computation, 2005. accepted for publication.
32. N. Hansen and A. Ostermeier. Completely derandomized self-adaptation in evo-
lution strategies. Evolutionary Computation, 9(2):159–195, 2001.
33. W. E. Hart and J. M. DeLaurentis. Convergence of a discretized self-adapative
evolutionary algorithm on multi-dimensional problems. submitted.
34. W.E. Hart, J.M. DeLaurentis, and L.A. Ferguson. On the convergence of an im-
plicitly self-adaptive evolutionary algorithm on one-dimensional unimodal prob-
lems. IEEE Transactions on Evolutionary Computation, 2003. To appear.
35. J. H. Holland. Outline for a logical theory of adaptive systems. JACM, 9:297–
314, 1962.
36. J. H. Holland. Adaptation in Natural and Artiﬁcial Systems. The University of
Michigan Press, Ann Arbor, 1975.
37. Ch. Igel and M. Toussaint. Neutrality and self-adaptation. Natural Computing:
an international journal, 2(2):117–132, 2003.
38. B. A. Julstrom.
Adaptive operator probabilities in a genetic algorithm that
applies three operators. In SAC, pages 233–238, 1997.
39. H. Kita. A comparison study of self-adaptation in evolution strategies and real-
coded genetic algorithms. Evolutionary Computation, 9(2):223–241, 2001.
40. H. Kita and M. Yamamura. A functional specialization hypothesis for designing
genetic algorithms. In Proc. IEEE International Conference on Systems, Man,
and Cybernetics ’99, pages 579–584, Piscataway, New Jersey, 1999. IEEE Press.
41. F. Kursawe. Grundlegende empirische Untersuchungen der Parameter von
Evolutionsstrategien — Metastrategien. Dissertation, Fachbereich Informatik,
Universit¨at Dortmund, 1999.
42. Ch.-Y. Lee and X. Yao. Evolutionary programming using mutations based on
the levy probability distribution. Evolutionary Computation, IEEE Transac-
tions on, 8(1):1–13, Feb. 2004.
43. K.-H. Liang, X. Yao, Ch. N. Newton, and D. Hoﬀman. An experimental in-
vestigation of self-adaptation in evolutionary programming. In V. W. Porto,
N. Saravanan, Waagen D. E., and A. E. Eiben, editors, Evolutionary Pro-
gramming, volume 1447 of Lecture Notes in Computer Science, pages 291–300.
Springer, 1998.
44. K.-H. Liang, X. Yao, and Ch.S. Newton. Adapting self-adaptive parameters
in evolutionary algorithms. Artiﬁcial Intelligence, 15(3):171 – 180, November
2001.
45. R. E. Mercer and J. R. Sampson. Adaptive search using a reproductive meta-
plan. Kybernetes, 7:215–228, 1978.

74
Silja Meyer-Nieberg and Hans-Georg Beyer
46. S. Meyer-Nieberg and H.-G. Beyer. On the analysis of self-adaptive recombina-
tion strategies: First results. In B. McKay et al., editors, Proc. 2005 Congress
on Evolutionary Computation (CEC’05), Edinburgh, UK, pages 2341–2348, Pis-
cataway NJ, 2005. IEEE Press.
47. S. P. Meyn and R.L. Tweedie. Markov Chains and Stochastic Stability. Springer,
1993.
48. A. Ostermeier, A. Gawelczyk, and N. Hansen. A derandomized approach to
self-adaptation of evolution strategies. Evolutionary Computation, 2(4):369–380,
1995.
49. I. Rechenberg. Cybernetic solution path of an experimental problem. Royal
Aircraft Establishment, Farnborough, page Library Translation 1122, 1965.
50. I. Rechenberg. Evolutionsstrategie: Optimierung technischer Systeme nach
Prinzipien der biologischen Evolution. Frommann-Holzboog Verlag, Stuttgart,
1973.
51. J. Reed, R. Toombs, and N.A. Barricelli. Simulation of biological evolution
and machine learning. i. selection of self-reproducing numeric patterns by data
processing machines, eﬀects of hereditary control, mutation type and crossing.
Journal of Theoretical Biology, 17:319–342, 1967.
52. R.S. Rosenberg. Simulation of genetic populations with biochemical properties.
Ph.d. dissertation, Univ. Michigan, Ann Arbor, MI, 1967.
53. G. Rudolph. Self-adaptive mutations may lead to premature convergence. IEEE
Transactions on Evolutionary Computation, 5(4):410–414, 2001.
54. J.D. Schaﬀer and A. Morishima. An adaptive crossover distribution mechanism
for genetic algorithms. In J.J. Grefenstette, editor, Genetic Algorithms and their
Applications: Proc. of the Second Int’l Conference on Genetic Algorithms, pages
36–40, 1987.
55. H.-P. Schwefel. Kybernetische Evolution als Strategie der exprimentellen
Forschung in der Str¨omungstechnik.
Master’s thesis, Technical University of
Berlin, 1965.
56. H.-P. Schwefel. Adaptive Mechanismen in der biologischen Evolution und ihr
Einﬂuß auf die Evolutionsgeschwindigkeit. Technical report, Technical Univer-
sity of Berlin, 1974. Abschlußbericht zum DFG-Vorhaben Re 215/2.
57. H.-P. Schwefel. Numerische Optimierung von Computer-Modellen mittels der
Evolutionsstrategie. Interdisciplinary systems research; 26. Birkh¨auser, Basel,
1977.
58. H.-P. Schwefel. Numerical Optimization of Computer Models. Wiley, Chichester,
1981.
59. M.A. Semenov. Convergence velocity of evolutionary algorithm with self-
adaptation. In GECCO 2002, pages 210–213, 2002.
60. M.A. Semenov and D.A. Terkel. Analysis of convergence of an evolutionary al-
gorithm with self-adaptation using a stochastic lyapunov function. Evolutionary
Computation, 11(4):363–379, 2003.
61. J. Smith and T. C. Fogarty. Self-adaptation of mutation rates in a steady state
genetic algorithm.
In Proceedings of 1996 IEEE Int’l Conf. on Evolutionary
Computation (ICEC ’96), pages 318–323. IEEE Press, NY, 1996.
62. J. Smith and T.C. Fogarty. Recombination strategy adaptation via evolution
of gene linkage. In Proc. of the 1996 IEEE International Conference on Evolu-
tionary Computation, pages 826–831. IEEE Publishers, 1996.
63. J. E. Smith. Self-Adaptation in Evolutionary Algorithms. PhD thesis, University
of the West of England, Bristol, 1998.

Self-Adaptation in Evolutionary Algorithms
75
64. J. E. Smith. Modelling GAs with self adaptive mutation rates. In L. Spector
and et al., editors, Proceedings of the Genetic and Evolutionary Computation
Conference (GECCO-2001), pages 599–606, San Francisco, California, USA,
7-11 July 2001. Morgan Kaufmann.
65. J. E. Smith. Parameter perturbation mechanisms in binary coded gas with self-
adaptive mutation. In K. DeJong, R. Poli, and J. Rowe, editors, Foundations
of Genetic Algorithms 7, pages 329–346. Morgan Kauﬀman, 2004.
66. J. E. Smith and T. C. Fogarty. Operator and parameter adaptation in genetic
algorithms. Soft Computing, 1(2):81–87, June 1997.
67. W. Spears. Adapting crossover in evolutionary algorithms. In Proceedings of the
Evolutionary Programming Conference, pages 367–384, 1995.
68. W.M. Spears. Evolutionary Algorithms: The Role of Mutation and Recombina-
tion. Springer-Verlag, Heidelberg, 2000.
69. Ch. Stone and J.E. Smith.
Strategy parameter variety in self-adaptation of
mutation rates. In W.B. Langdon and et al., editors, GECCO, pages 586–593.
Morgan Kaufmann, 2002.
70. H.-M. Voigt, H. M¨uhlenbein, and D. Cvetkovi´c. Fuzzy recombination for the
breeder genetic algorithm. In L.J. Eshelman, editor, Proc. 6th Int’l Conf. on
Genetic Algorithms, pages 104–111, San Francisco, CA, 1995. Morgan Kauf-
mann Publishers, Inc.
71. R. Weinberg. Computer Simulation of a Living Cell. PhD thesis, University of
Michigan, 1970.
72. J. M. Won and J. S. Lee. Premature convergence avoidance of self-adaptive
evolution strategy,. In The 5th International Conference on Simulated Evolution
And Learning, Busan, Korea, Oct 2004.
73. X. Yao and Y. Liu.
Fast evolutionary programming.
In L. J. Fogel, P. J.
Angeline, and T. B¨ack, editors, Proceedings of the Fifth Annual Conference on
Evolutionary Programming, pages 451–460. The MIT Press, Cambridge, MA,
1996.

Adaptive Strategies for Operator Allocation
Dirk Thierens
Department of Information and Computing Sciences
Universiteit Utrecht
The Netherlands
dirk.thierens@cs.uu.nl
Summary. Learning the optimal probabilities of applying an exploration operator
from a set of alternatives can be done by self-adaptation or by adaptive allocation
rules. In this chapter we discuss the latter approach. The allocation strategies consid-
ered in the literature usually belong to the class of probability matching algorithms.
These strategies adapt the operator probabilities in such a way that they match the
reward distribution. We will also discuss an alternative adaptive allocation strategy,
called the adaptive pursuit method, and compare this method with the probability
matching approach in a controlled, non-stationary environment. Calculations and
experimental results show the performance diﬀerences between the two strategies. If
the reward distributions stay stationary for some time, the adaptive pursuit method
converges rapidly and accurately to an operator probability distribution that re-
sults in a much higher probability of selecting the current optimal operator and a
much higher average reward than with the probability matching strategy. Impor-
tantly, the adaptive pursuit scheme also remains sensitive to changes in the reward
distributions.
1 Introduction
Genetic algorithms usually apply their exploration operators with a ﬁxed
probability. There are however no general guidelines to help determine an opti-
mal value for these probability values. In practice, the user simply searches for
a reasonable set of values by running a series of trial-and-error experiments.
Clearly, this is a computationally expensive procedure and since genetic al-
gorithms are often applied to computational intensive problems, the number
of probability values explored has to remain limited. To make matters worse,
the problem is compounded by the fact that there is no single, ﬁxed set of
values that is optimal during the entire run. Depending on the current state
of the search process the optimal probability values continuously change.
This problem has long been recognized and diﬀerent adaptation methods
have been proposed to solve it [4][5][11]. In general two classes of adaptation
methods can be found:
D. Thierens: Adaptive Strategies for Operator Allocation, Studies in Computational Intelligence
(SCI) 54, 77–90 (2007)
www.springerlink.com
© Springer-Verlag Berlin Heidelberg 2007

78
Dirk Thierens
1. Self-adaptation. The values of the operator probabilities are directly
encoded in the representation of the individual solutions. These values
basically hitchhike with the solutions that are being evolved through the
regular search process. The idea is that the operator probability values
and problem solutions co-evolve to (near)-optimal settings.
2. Adaptive allocation rule. The values of the operator probabilities are
adapted following an ‘out-of-the-evolutionary-loop’ learning rule accord-
ing to the quality of new solutions created by the operators.
Self-adaptation is particularly applied within Evolutionary Strategies and
Evolutionary Programming for numerical optimization problems. When ap-
plied to discrete optimization or adaptation problems using genetic algo-
rithms, its success is somewhat limited as compared to the adaptive allocation
rule method. In this chapter we will focus on the latter class. Looking at the
literature it becomes clear that most adaptive allocation rules used belong to
the probability matching type [2][3][6][7][8][9][10][15]. Here we also look at an
alternative allocation rule, called the adaptive pursuit method. We compare
both approaches on a controlled, non-stationary environment. Results indi-
cate that the adaptive pursuit method possess a number of useful properties
for adaptive operator allocation rule which are less present in the traditionally
used probability matching algorithm.
The chapter is organized as follows. Section 2 describes the probability
matching method and speciﬁes an implementation particularly suited for non-
stationary environments. Section 3 explains the adaptive pursuit algorithm.
Section 4 shows experimental results of both adaptive allocation techniques.
Finally, Section 5 concludes this contribution.
2 Probability Matching
An rule is an algorithm that iteratively chooses one of its operators to apply
to an external environment [12]. The environment returns a reward - possibly
zero - and the allocation rule uses this reward and its internal state to adapt
the probabilities with which the operators are chosen. It is crucial to note
that the environment we consider here is non-stationary, meaning that the
probability distribution specifying the reward generated when applying some
operator changes during the runtime of the allocation algorithm. Unfortu-
nately, the non-stationarity requirement excludes the use of a large number of
adaptive strategies that have been developed for the well-known multi-armed
bandit problem [1].
Formally, we have a set of K operators A = {a1, . . . , aK}, and a proba-
bility vector P(t) = {P1(t), . . . , PK(t)} (∀t : 0 ≤Pi(t) ≤1; K
i=1 Pi(t) = 1).
The adaptive allocation rule selects an operator to be executed in proportion
to the probability values speciﬁed in P(t). When an operator a is applied to
the environment at time t, a reward Ra(t) is returned. Each operator has

Adaptive Strategies for Operator Allocation
79
an associated reward, which is a non-stationary random variable. All rewards
are collected in the reward vector R(t) = {R1(t), . . . , RK(t)}. In addition to
the operator probability vector P(t) the adaptive allocation rule maintains
a quality vector Q(t) = {Q1(t), . . . , QK(t)} that speciﬁes a running estimate
of the reward for each operator. Whenever an operator is executed his cur-
rent estimate Qa(t) is adapted. The allocation algorithm is run for a period
of T time steps. The goal is to maximize the expected value of the cumu-
lative reward E[R] = T
t=1 Ra(t) received by the adaptive allocation rule.
Since the environment is non-stationary, the estimate of the reward for each
operator is only reliable when the rewards received are not too old. An ele-
gant, iterative method to compute such a running estimate is the exponential,
recency-weighted average that updates the current estimate with a fraction of
the diﬀerence of the target value and the current estimate:
Qa(t + 1) = Qa(t) + α[Ra(t) −Qa(t)]
(1)
with the adaptation rate α : 0 < α ≤1. The basic rule computes each oper-
ator’s selection probability Pa(t) as the proportion of the operator’s reward
estimate Qa(t) to the sum of all reward estimates K
i=1 Qi(t). However, this
may lead to the loss of some operators. Once a probability Pa(t) becomes
equal to 0, the operator will no longer be selected and its reward estimate can
no longer be updated. This is an unwanted property in a non-stationary envi-
ronment because the operator might become valuable again in a future stage
of the search process. We always need to be able to apply any operator and
update its current value estimate. To ensure that no operator gets lost we en-
force a minimal value Pmin : 0 < Pmin < 1 for each selection probability. As a
result the maximum value any operator can achieve is Pmax = 1−(K−1)Pmin
where K is the number of operators. The rule for updating the probability
vector Pa(t) now becomes:
Pa(t + 1) = Pmin + (1 −K · Pmin)
Qa(t + 1)
K
i=1 Qi(t + 1)
.
(2)
It is easy to see in the above equation that when an operator does not receive
any reward for a long time its value estimate Qa(t) will converge to 0 and
its probability of being selected Pa(t) converges to Pmin. It is also clear that
when only one operator receives a reward during a long period of time - and all
other operators get no reward - then its selection probability Pa(t) converges
to Pmin + (1 −K · Pmin).1 = Pmax. Furthermore, 0 < Pa(t) < 1 and their
sum equals 1:
K

a=1
Pa(t) = K · Pmin + 1 −K · Pmin
K
i=1 Qi(t)
K

a=1
Qa(t) = 1.
Finally, our probability matching algorithm is speciﬁed as:

80
Dirk Thierens
ProbabilityMatching(P, Q, K, Pmin, α)
1
for i ←1 to K
2
do P(i) ←1
K ; Q(i) ←1.0
3
while NotTerminated?()
4
do as ←ProportionalSelectOperator(P)
5
Ras(t) ←GetReward(as)
6
Qas(t + 1) = Qas(t) + α[Ras(t) −Qas(t)]
7
for a ←1 to K
8
do Pa(t + 1) = Pmin + (1 −K · Pmin)
Qa(t+1)
K
i=1 Qi(t+1)
The probability matching allocation rule as speciﬁed in equation 2 is able
to adapt to non-stationary environments. Unfortunately, it pays a heavy price
for this in terms of reward maximization. This is most obvious when we con-
sider a non-stationary environment. Suppose we have only two operators a1
and a2 with constant reward values R1 and R2. From equation 2 it follows
that
P1(t) −Pmin
P2(t) −Pmin
→R1
R2
.
Assume that R1 > R2. An ideal adaptive allocation rule should notice in
this stationary environment that the operator a1 has a higher reward than
operator a2. The allocation rule should therefore maximize the probability of
applying operator a1 and only apply a2 with the minimal probability Pmin.
However, the closer the rewards R1 and R2 are, the less optimal the proba-
bility matching rule behaves. For instance, when Pmin = 0.1, R1 = 10, and
R2 = 9 then P1 = 0.52 and P2 = 0.48, which is far removed from the desired
values of P1 = 0.9 and P2 = 0.1.
Matching the reward probabilities is not an optimal strategy for allocating
operator probabilities in an optimizing algorithm. In the next section we dis-
cuss the as an alternative allocation method that is better suited to maximize
the rewards received while still maintaining the ability to swiftly react to any
changes in a non-stationary environment.
3 Adaptive Pursuit algorithm
Pursuit algorithms are a class of rapidly converging algorithms for learning
automata proposed by Thathachar and Sastry [13]. They represent adaptive
allocation rules that adapt the operator probability vector P(t) in such a way
that the algorithm pursues the operator a∗that currently has the maximal
estimated reward Qa∗(t). To achieve this, the pursuit method increases the se-
lection probability Pa∗(t) and decreases all other probabilities Pa(t), ∀a ̸= a∗.
Pursuit algorithms originated from the ﬁeld of learning automata. However,
they are designed for stationary environments for which it can be proved that
they are ϵ-optimal. The ϵ-optimality property means that in every stationary

Adaptive Strategies for Operator Allocation
81
environment, there exists a learning rate β∗> 0 and time t0 > 0, such that
for all learning rates 0 < β ≤β∗≤1 and for any δ ∈[0 . . . 1] and ϵ ∈[0 . . . 1]:
Prob[Poptimal
a
(t) > 1 −ϵ] > 1 −δ,
∀t > t0.
In practice this means that if the the learning rate β is small enough as a
function of the reward distribution correct convergence is assured.
As in the probability matching allocation rule, the pursuit algorithm
proportionally selects an operator to execute according to the probabil-
ity vector P(t), and updates the corresponding operator’s quality or esti-
mated reward Qa(t). Subsequently, the current best operator a∗is chosen
(a∗= argmaxa[Qa(t + 1)]), and its selection probability is increased
Pa∗(t + 1) = (1 −β)Pa∗(t) + β,
while the other operators have their selection probability decreased
∀a ̸= a∗: Pa(t + 1) = (1 −β)Pa(t).
It is clear from (1 −β)Pa∗(t) + β = Pa∗(t) + β(1 −Pa∗(t)) that if a particular
operator is repeatedly the best operator its selection probability will converge
to 1, while the selection probabilities of the other operators will converge
to 0 and they will no longer be applied. Consequently, the pursuit algorithm
cannot be used in a non-stationary environment. To make the method suitable
for non-stationary environments the probability updating scheme needs to be
adjusted [14]. The modiﬁed update rule ensures that the probability vector
is still pursuing the current best operator at the same rate as the standard
method, but now the exponential, recency-weighted averages of the operator
probabilities are enforced to stay within the interval [Pmin . . . Pmax] with 0 <
Pmin < Pmax < 1. Calling a∗= argmaxa[Qa(t+1)] the current best operator,
we get:
Pa∗(t + 1) = Pa∗(t) + β[Pmax −Pa∗(t)]
(3)
and
∀a ̸= a∗: Pa(t + 1) = Pa(t) + β[Pmin −Pa(t)]
(4)
under the constraint:
Pmax = 1 −(K −1)Pmin.
(5)
The constraint ensures that if K
a=1 Pa(t) = 1 the sum of the updated proba-
bilities remains equal to 1:
K

a=1
Pa(t + 1) = 1
⇔rhs. eqt.(3) + rhs. eqt.(4) = 1
⇔(1 −β)
K

a=1
Pa(t) + β[Pmax + (K −1)Pmin] = 1
⇔Pmax = 1 −(K −1)Pmin.

82
Dirk Thierens
Note that since Pmin < Pmax the constraint can only be fulﬁlled1 if Pmin < 1
K .
An interesting value for the minimal probability is Pmin =
1
2K which results
in the maximum probability Pmax =
1
2 +
1
2K . An intuitive appealing way
to look at these values is that the optimal operator will be selected half the
time, while the other half of the time all operators have an equal probability
of being selected.
Finally, we can now specify more formally the adaptive pursuit algorithm:
AdaptivePursuit(P, Q, K, Pmin, α, β)
1
Pmax ←1 −(K −1)Pmin
2
for i ←1 to K
3
do P(i) ←1
K ; Q(i) ←1.0
4
while NotTerminated?()
5
do as ←ProportionalSelectOperator(P)
6
Ras(t) ←GetReward(as)
7
Qas(t + 1) = Qas(t) + α[Ras(t) −Qas(t)]
8
a∗←Argmaxa(Qa(t + 1))
9
Pa∗(t + 1) = Pa∗(t) + β[Pmax −Pa∗(t)]
10
for a ←1 to K
11
do if a ̸= a∗
12
then Pa(t + 1) = Pa(t) + β[Pmin −Pa(t)]
Consider again the 2-operator stationary environment at the end of the
previous section with Pmin = 0.1, R1 = 10, and R2 = 9. As opposed to the
probability matching rule, the adaptive pursuit method will play the better
operator a1 with maximum probability Pmax = 0.9. It also keeps playing the
poorer operator a2 with minimal probability Pmin = 0.1 in order to maintain
its ability to adapt to any change in the reward distribution.
4 Experimental results
To get an idea of the dynamic behavior of these adaptive allocation rules we
compare the probability matching algorithm, the adaptive pursuit method, and
the non-adaptive, equal-probability strategy, on the following non-stationary
environment. We consider an environment with 5 operators (or arms in the
multi-bandit problem terminology). Each operator a receives a uniformly dis-
tributed reward Ra between the respective boundaries R5 = U[4 . . . 6], R4 =
U[3 . . . 5], R3 = U[2 . . . 4], R2 = U[1 . . . 3], and R1 = U[0 . . . 2]. After a ﬁxed
time interval ∆T these reward distributions are randomly reassigned to the
operators, under the constraint that the current best operator-reward associa-
tion eﬀectively has to change to a new couple. Speciﬁcally, the non-stationary
1 Strictly speaking, Pmin can be equal to
1
K . This happens in the case of only 2
operators (K = 2) and a lower bound probability of Pmin = 0.5. However, now
Pmax also equals 0.5 so there is no adaptation possible.

Adaptive Strategies for Operator Allocation
83
environment in the simulation switches 10 times with the following pattern:
01234 →41203 →24301 →12043 →41230 →31420 →04213 →23104 →
14302 →40213, where each sequence orders the operators in descending value
of reward. For instance ‘41203’ means that operator a4 receives the highest
reward R5, operator a1 receives the second highest reward R4, operator a2
receives reward R3, operator a0 receives reward R2, and ﬁnally operator a3
receives the lowest reward R1. If we had full knowledge of the reward distrib-
utions and their switching pattern we could always pick the optimal operator
a∗and achieve an expected reward
E[ROpt] = Ra∗= 5 .
Clearly, this value can never be obtained by any adaptive allocation strategy
since it always needs to pay a price for exploring the eﬀects of alternative ac-
tions. Nevertheless, it does represent an upper bound of the expected reward.
When operating in a stationary environment the allocation strategies converge
to a ﬁxed operator probability distribution. Using this distribution we can
compute the maximum achievable expected reward for each allocation rule,
which is preferably close to the theoretical upper bound. In a non-stationary
environment, we aim to achieve this value as quick as possible, while still
being able to react swiftly to any change in the reward distributions. In the
experiments we have taken the value Pmin =
1
2K = 0.1 for the minimum prob-
ability each operator will be applied in the adaptive allocation schemes. For a
stationary environment - this is, when the assignment of reward distributions
to the arms are not switched - we can compute the expected reward and the
probability of choosing the optimal operator once the operator probability
vectors have converged.
1. Non-adaptive, equal-probability allocation rule.
This strategy simply selects each operator with equal probability. The
probability of choosing the optimal operator a∗
F ixed is
Prob[as = a∗
F ixed] = 1
K
= 0.2 .
The expected reward becomes
E[RF ixed] =
K

a=1
E[Ra]Prob[as = a]
=
K
a=1 E[Ra]
K
= 3 .
2. Probability matching allocation rule.
For the probability matching updating scheme the probability of choosing
the optimal operator a∗
P robMatch is

84
Dirk Thierens
Prob[as = a∗
P robMatch]
= Pmin + (1 −K.Pmin)
E[Ra∗]
K
a=1 E[Ra]
= 0.2666 . . . .
The expected reward becomes
E[RP robMatch]
=
K

a=1
E[Ra]Prob[as = a]
=
K

a=1
a[Pmin + (1 −K · Pmin)
E[Ra]
K
a=1 E[Ra]
]
= 3.333 . . . .
3. Adaptive pursuit allocation rule.
For the adaptive pursuit updating scheme the probability of choosing the
optimal operator a∗
AdaP ursuit is
Prob[as = a∗
AdaP ursuit] = 1 −(K −1) · Pmin
= 0.6 .
The expected reward becomes
E[RAdaP ursuit]
=
K

a=1
E[Ra]Prob[as = a]
= Pmax E[Ra∗] + Pmin
K

a=1,a̸=a∗
E[Ra]
= 4 .
The computed expected rewards and probabilities of applying the optimal
operator show that both adaptive allocation rules have a better performance
than the non-adaptive strategy that simply selects each operator with equal
probability. More interesting, they also show that - after convergence - the
adaptive pursuit algorithm has a signiﬁcantly better performance than the
probability matching algorithm in the stationary environment. The probabil-
ity matching algorithm will apply the optimal operator in only 27% of the
trials while the pursuit algorithm will be optimal in 60% of the cases. Simi-
larly, the probability matching algorithm has an expected reward of 3.3 versus
an expected reward of 4 for the pursuit method. Of course, this assumes that
both adaptive strategies are able to converge correctly and rapidly. For non-
stationary environments it is vital that the adaptive allocation techniques

Adaptive Strategies for Operator Allocation
85
 0
 0.2
 0.4
 0.6
 0.8
 1
 0
 50
 100
 150
 200
 250
 300
 350
 400
 450
 500
Probability optimal operator applied
Time steps
Adaptive pursuit
Probability matching
Fig. 1. The probability of selecting the optimal operator at each time step in the
non-stationary environment with switching interval ∆T = 50 time steps (learning
rates α = 0.8; β = 0.8; Pmin = 0.1; K = 5; results are averaged over 100 runs).
The horizontal lines show the expected values for the non-switching, stationary en-
vironment for resp. adaptive pursuit (0.6), probability matching (0.27), and random
selection (0.2).
 2
 2.5
 3
 3.5
 4
 4.5
 5
 0
 50
 100
 150
 200
 250
 300
 350
 400
 450
 500
Average reward
Time steps
Adaptive pursuit
Probability matching
Fig. 2. The average reward received at each time step in the non-stationary envi-
ronment with switching interval ∆T = 50 time steps (learning rates α = 0.8; β =
0.8; Pmin = 0.1; K = 5; results are averaged over 100 runs). The horizontal lines
show the expected values for the non-switching, stationary environment for resp.
adaptive pursuit (4), probability matching (3.33), and random selection (3).

86
Dirk Thierens
 0
 0.2
 0.4
 0.6
 0.8
 1
 0
 200
 400
 600
 800
 1000
 1200
 1400
 1600
 1800
 2000
Probability optimal operator applied
Time steps
Adaptive pursuit
Probability matching
Fig. 3. The probability of selecting the optimal operator at each time step in the
non-stationary environment with switching interval ∆T = 200 time steps (learning
rates α = 0.8; β = 0.8; Pmin = 0.1; K = 5; results are averaged over 100 runs).
The horizontal lines show the expected values for the non-switching, stationary en-
vironment for resp. adaptive pursuit (0.6), probability matching (0.27), and random
selection (0.2).
 2
 2.5
 3
 3.5
 4
 4.5
 5
 0
 200
 400
 600
 800
 1000
 1200
 1400
 1600
 1800
 2000
Average reward
Time steps
Adaptive pursuit
Probability matching
Fig. 4. The average reward received at each time step in the non-stationary envi-
ronment with switching interval ∆T = 200 time steps (learning rates α = 0.8; β =
0.8; Pmin = 0.1; K = 5; results are averaged over 100 runs). The horizontal lines
show the expected values for the non-switching, stationary environment for resp.
adaptive pursuit (4), probability matching (3.33), and random selection (3).

Adaptive Strategies for Operator Allocation
87
converge quickly and accurately, and at the same time maintain the ﬂexibility
to swiftly track any changes in the reward distributions. Experimental results
on the above speciﬁed non-stationary environment show that the adaptive
pursuit method does indeed possess these capabilities. In our ﬁrst simulation
we have taken a switching interval ∆T = 50 time steps. The results shown
are all averaged over 100 independent runs. Figures 1 and 2 clearly show that
the adaptive pursuit algorithm is capable of accurate and fast convergence.
At the same time it is very responsive to changes in the reward distribution.
Whenever the operator-reward associations are reassigned the performance of
the adaptive pursuit algorithm plunges since it is now pursuing an operator
that is no longer optimal. It does not take long though for the strategy to
correct itself, and to pursue the current optimal operator again. This is in
contrast with the probability matching algorithm where the diﬀerences be-
tween the operator selection probabilities are much smaller and the changes
in the reward distributions cause only minor adaptations. Of course a more
signiﬁcant reaction would be observed for the probability matching method if
the rewards would have a much large diﬀerence between them. The key point
though is that in practice one usually will have to deal with reward diﬀerences
of a few percent, not an order of magnitude.
In a second experiment we have increased the switching interval ∆T to
200 times steps (Figures 3 and 4). Given more time to adapt one can see that
both adaptive allocation strategies approach the values that where computed
above for the stationary environment.
The results in the Figures 1, 2, 3 and 4 were obtained for a learning
rate α = 0.8 when updating Qa(t) in Equation 1, and a learning rate β =
0.8 when updating Pa(t) in Equations 3 and 4. These values gave the best
performance for this particular problem instance. Tables 1, 2, 3, and 4 show
the performance for diﬀerent settings of the learning rates. For low values
of the learning rates the adaptive schemes do not react swiftly enough to
the rapidly changing reward distributions. Naturally, the high learning rates
are only possible because at each time step an actual reward is given by the
environment. If the rewards would only be given with a probability less than
1, the learning rates would necessarily be small to ensure meaningful running
estimates that are exponentially, recency-weighted. It should be noted though
that whatever the values of the learning rates the adaptive pursuit method
keeps outperforming the probability matching scheme.
5 Conclusion
Adaptive allocation rules are often used for learning the optimal probability
values of applying a ﬁxed set of exploration operators. Traditionally, the al-
location strategies adapt the operator probabilities in such a way that they
match the distribution of the rewards. In this chapter, we have compared this
probability matching algorithm with an adaptive pursuit allocation rule in

88
Dirk Thierens
Probab.
Adaptive Pursuit: (β)
α
Match.
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
0.10
0.218
0.248 0.272 0.281 0.276 0.290 0.284 0.289 0.288 0.287
0.20
0.229
0.281 0.298 0.315 0.321 0.327 0.329 0.332 0.329 0.340
0.30
0.243
0.313 0.356 0.373 0.381 0.388 0.392 0.386 0.393 0.397
0.40
0.249
0.352 0.401 0.411 0.429 0.427 0.434 0.439 0.436 0.445
0.50
0.254
0.381 0.423 0.443 0.451 0.456 0.448 0.459 0.467 0.471
0.60
0.259
0.392 0.447 0.461 0.474 0.477 0.484 0.484 0.492 0.488
0.70
0.259
0.404 0.448 0.477 0.480 0.490 0.492 0.496 0.496 0.493
0.80
0.257
0.408 0.462 0.478 0.482 0.491 0.495 0.499 0.507 0.502
0.90
0.262
0.404 0.455 0.468 0.476 0.492 0.497 0.493 0.502 0.495
Table 1. The average probability of selecting the optimal operator in the non-
stationary environment with switching interval ∆T = 50 time steps for diﬀerent
adaptation rates α and β (Pmin = 0.1; K = 5; results are averaged over 100 runs).
Probab.
Adaptive Pursuit: (β)
α
Match.
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
0.10
3.115
3.269 3.392 3.434 3.442 3.472 3.478 3.489 3.478 3.474
0.20
3.166
3.428 3.515 3.568 3.597 3.607 3.621 3.612 3.625 3.635
0.30
3.221
3.489 3.619 3.669 3.689 3.703 3.713 3.710 3.717 3.730
0.40
3.243
3.553 3.685 3.715 3.746 3.751 3.767 3.777 3.778 3.788
0.50
3.270
3.589 3.715 3.765 3.787 3.791 3.787 3.803 3.812 3.825
0.60
3.276
3.612 3.742 3.791 3.807 3.822 3.831 3.839 3.848 3.844
0.70
3.286
3.634 3.740 3.808 3.815 3.840 3.839 3.856 3.846 3.842
0.80
3.288
3.627 3.758 3.808 3.829 3.830 3.853 3.859 3.871 3.862
0.90
3.308
3.627 3.743 3.789 3.815 3.844 3.845 3.840 3.861 3.851
Table 2. The average reward received in the non-stationary environment with
switching interval ∆T = 50 time steps for diﬀerent adaptation rates α and β
(Pmin = 0.1; K = 5; results are averaged over 100 runs).
a controlled, non-stationary environment. Calculations and experimental re-
sults show a better performance of the adaptive pursuit method. The adaptive
pursuit strategy converges accurately and rapidly, yet remains able to swiftly
react to any change in the reward distributions.
References
1. P. Auer, N. Cesa-Bianchi, Y. Freund, and R.E. Schapire (2002) The nonstochas-
tic multiarmed bandit problem. SIAM j. Computing Vol.32, No.1, pp. 48–77
2. D.W. Corne, M.J. Oates, and D.B. Kell (2002) On ﬁtness distributions and
expected ﬁtness gain of mutation rates in parallel evolutionary algorithms. Proc.
7th Intern. Conf. on Parallel Problem Solving from Nature. LNCS Vol. 2439, pp.
132–141

Adaptive Strategies for Operator Allocation
89
Probab.
Adaptive Pursuit: (β)
α
Match.
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
0.10
0.247
0.399 0.414 0.416 0.422 0.423 0.427 0.422 0.423 0.429
0.20
0.257
0.491 0.498 0.508 0.508 0.509 0.515 0.514 0.511 0.516
0.30
0.260
0.520 0.530 0.537 0.537 0.538 0.542 0.540 0.543 0.547
0.40
0.264
0.534 0.546 0.550 0.551 0.554 0.556 0.555 0.559 0.558
0.50
0.265
0.539 0.553 0.557 0.557 0.559 0.559 0.561 0.561 0.562
0.60
0.264
0.537 0.552 0.556 0.558 0.561 0.562 0.565 0.564 0.563
0.70
0.264
0.538 0.552 0.555 0.556 0.560 0.560 0.561 0.560 0.561
0.80
0.267
0.528 0.541 0.549 0.550 0.552 0.557 0.554 0.556 0.560
0.90
0.266
0.521 0.537 0.538 0.546 0.547 0.547 0.549 0.550 0.553
Table 3. The average probability of selecting the optimal operator in the non-
stationary environment with switching interval ∆T = 200 time steps for diﬀerent
adaptation rates α and β (Pmin = 0.1; K = 5; results are averaged over 100 runs).
Probab.
Adaptive Pursuit: (β)
α
Match.
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
0.10
3.233
3.719 3.757 3.767 3.768 3.775 3.778 3.780 3.776 3.789
0.20
3.287
3.834 3.853 3.877 3.879 3.879 3.893 3.891 3.887 3.892
0.30
3.302
3.873 3.896 3.916 3.912 3.914 3.922 3.921 3.923 3.934
0.40
3.315
3.886 3.915 3.926 3.932 3.933 3.939 3.942 3.948 3.938
0.50
3.320
3.891 3.925 3.940 3.939 3.945 3.940 3.946 3.946 3.950
0.60
3.323
3.890 3.926 3.936 3.941 3.949 3.947 3.956 3.955 3.951
0.70
3.322
3.894 3.928 3.936 3.943 3.948 3.948 3.947 3.947 3.951
0.80
3.333
3.878 3.912 3.934 3.937 3.934 3.946 3.940 3.945 3.951
0.90
3.329
3.881 3.916 3.913 3.933 3.933 3.933 3.938 3.936 3.944
Table 4. The average reward received in the non-stationary environment with
switching interval ∆T = 200 time steps for diﬀerent adaptation rates α and β
(Pmin = 0.1; K = 5; results are averaged over 100 runs).
3. L. Davis (1989) Adapting operator probabilities in genetic algorithms. Proc.
Third Intern. Conf. on Genetic Algorithms and their Applications. pp. 61–69
4. A.E. Eiben, R. Hinterding, and Z. Michalewicz (1999) Parameter control in
evolutionary algorithms. IEEE Transactions on Evolutionary Computation,
3(2):124-141
5. D.E. Goldberg (1990) Probability matching, the magnitude of reinforcement,
and classiﬁer system bidding. Machine Learning. Vol.5, pp. 407–425
6. T.P. Hong, H.S. Wang, and W.C. Chen (2000) Simultaneously applying multiple
mutation operators in genetic algorithms. Journal of Heuristics. Vol.6, pp. 439–
455
7. C. Igel, and M. Kreutz (2003) Operator adaptation in evolutionary computation
and its application to structure optimization of neural networks. Neurocomput-
ing Vol.55, pp. 347–361

90
Dirk Thierens
8. B. Julstrom (1995) What have you done for me lately? Adapting operator proba-
bilities in a steady-state genetic algorithm. Proc. Sixth Intern. Conf. on Genetic
Algorithms. pp. 81–87
9. F. G. Lobo, and D. E. Goldberg (1997) Decision making in a hybrid genetic
algorithm. Proc. IEEE Intern. Conf. on Evolutionary Computation. pp. 122–
125
10. D. Schlierkamp-Voosen, and H. M¨uhlenbein (1994) Strategy adaptation by com-
peting subpopulations. Proc. Intern. Conf. on Parallel Problem Solving from
Nature pp. 199–208
11. J.E. Smith, and T.C. Fogarty (1997) Operator and parameter adaptation in
genetic algorithms. Soft Computing No.1, pp. 81–87
12. R.S. Sutton, and A.G. Barto (1998) Reinforcement Learning: an introduction.
MIT Press
13. M.A.L. Thathachar, and P.S. Sastry (1985) A Class of Rapidly Converging
Algorithms for Learning Automata. IEEE Transactions on Systems, Man and
Cybernetics. Vol.SMC-15, pp. 168-175
14. D. Thierens (2005) An Adaptive Pursuit Strategy for Allocating Operator Prob-
abilities. Proc. Intern. Conf. on Genetic and Evolutionary Computation. pp.
1539–1546
15. A. Tuson, and P. Ross (1998) Adapting operator settings in genetic algorithms.
Evolutionary Computation Vol.6, No.2, pp. 161–184

Sequential Parameter Optimization Applied
to Self-Adaptation for Binary-Coded
Evolutionary Algorithms
Mike Preuss and Thomas Bartz-Beielstein
Dortmund University, D-44221 Dortmund, Germany.
mike.preuss@cs.uni-dortmund.de, thomas.bartz-beielstein@udo.edu
Summary. Adjusting algorithm parameters to a given problem is of crucial impor-
tance for performance comparisons as well as for reliable (ﬁrst) results on previously
unknown problems, or with new algorithms. This also holds for parameters con-
trolling adaptability features, as long as the optimization algorithm is not able to
completely self-adapt itself to the posed problem and thereby get rid of all para-
meters. We present the recently developed sequential parameter optimization (SPO)
technique that reliably ﬁnds good parameter sets for stochastically disturbed algo-
rithm output. SPO combines classical regression techniques and modern statistical
approaches for deterministic algorithms as Design and Analysis of Computer Exper-
iments (DACE). Moreover, it is embedded in a twelve-step procedure that targets
at doing optimization experiments in a statistically sound manner, focusing on an-
swering scientiﬁc questions.
We apply SPO to a question that did not receive much attention yet: Is self-
adaptation as known from real-coded evolution strategies useful when applied to
binary-coded problems? Here, SPO enables obtaining parameters resulting in good
performance of self-adaptive mutation operators. It thereby allows for reliable com-
parison of modiﬁed and traditional evolutionary algorithms, ﬁnally allowing for well
founded conclusions concerning the usefulness of either technique.
1 Introduction
The evolutionary computation (EC) ﬁeld currently seems to experience a state of
ﬂux, at least as far as experimental research methods are concerned. A purely theo-
retical approach is not reasonable for many optimization problems. In fact, there is
a huge gap between theory and experiment in evolutionary computation. However,
empiricism in EC cannot compensate this shortcoming due to a lack of standards.
A broad spectrum of presentation techniques makes new results almost incompara-
ble. At present, it is intensely discussed which experimental research methodologies
should be used to improve the acceptance and quality of evolutionary algorithms
(EA).
Several authors from related ﬁelds ([32], [57], and [39]) and from within EC ([81],
[23]) have criticized usual experimental practice. Besides other topics, they ask for
M. Preuss and T. Bartz-Beielstein: Sequential Parameter Optimization Applied to Self-
Adaptation for Binary-Coded Evolutionary Algorithms, Studies in Computational Intelligence
(SCI) 54, 91–119 (2007)
www.springerlink.com
© Springer-Verlag Berlin Heidelberg 2007

92
Mike Preuss and Thomas Bartz-Beielstein
increased thoughtfulness when selecting benchmark problems, a better structured
process of experimentation, including presentation of results, and proper use of sta-
tistical techniques.
In the light of the no free lunch theorem (NFL, [82]), research aims are gradually
changing from demonstration of superiority of new algorithms towards experimental
analysis, addressing questions as: What makes an algorithm work well on a certain
problem class? In spite of this development, the so-called horse race papers1 still
seem to prevail. [23] report about the situation in experimental EC and name the
reasons for their discontentment with current practice. Put into positive formulation,
their main demands are:
• assembly of concrete research questions and concrete answers to these, no claims
that are not backed up by tests
• selection of test problems and instances motivated by these questions
• utilization of adequate performance measures for answering the questions, and
• reproducibility, which requires all necessary details and/or source code to repeat
experiments
This line is carried further in [22]. Partly in response to these demands, [7] pro-
poses SPO as a structured experimentation procedure based on modern statistic
techniques (§4). In its heart, SPO contains a parameter tuning method that enables
adapting parameters to the treated test problems.
Undoubtedly, comparing badly parametrized algorithms is rather useless. How-
ever, a good parameter set allows for near-optimal performance of an algorithm.
As this statement suggests, parameter tuning is of course an optimization problem
on its own. Following from that, some interesting questions arise when algorithms
with parameter control operators are concerned, because their own adaptability is in
turn guarded by control parameters, which can be tuned. The broadest of these ques-
tions may be where to put eﬀort into when striving for best performance—tune the
simple algorithm, or tune the adaptability of more complex algorithms—eventually
resulting in recommendations when to apply adaptive operators.
1.1 Parameter Control or Parameter Tuning?
Parameter Control refers to parameter adaptation during the run of an optimization
algorithm, whereas parameter tuning improves their setting before the run is started.
These two diﬀerent mechanisms are the roots of the two subtrees of parameter setting
methods in the global taxonomy given by [21]. One may get the impression that they
are contradictory and researchers should aim at using parameter control as much as
possible. In our view, the picture is somewhat more complex, and the two are rather
complementary.
In a production environment, an EA and any alternative stochastic optimization
algorithm would be run several times, not once, possibly solving the next or trying
the same problem again. This obliterates the separating time factor of above. What
is more, the EA as well as the problem representation will most likely undergo
structural changes during these iterations (new operators, criteria, etc.). These will
entail changes in the “optimal” parameters, too.
1 [39] uses this term for papers that aim at showing predominance of one algo-
rithm over (all) others by reporting performance comparisons on (standard) test
problems.

SPO Applied to Self-Adaptation for Binary-Coded EAs
93
Furthermore, parameter control methods do not necessarily decrease the number
of parameters. For example, the 1/5th adaptation rule by [63] provides at least three
quantities where there has been one—mutation step size—before, namely the time
window length used to measure success, the required success rate (1/5), and the rate
of change applied. Even if their number remains constant, is it justiﬁed to expect
that parameter tuning has become simpler now? We certainly hope so, but need
more evidence to decide.
In principle, every EA parameter may be (self-)adapted. However, concurrent
adaptation of many does not seem particularly successful and is probably limited
to two or three at a time. We thus cannot simply displace parameter tuning with
parameter control. On the other hand, manual tuning is surely not the answer, as
well, and grid based tuning seems intractable for higher order design spaces. At this
point, we have three possibilities, either to
• fall back on default values,
• abandon tuning alltogether and take random parameters, or
• to apply a tuning method that is more robust than doing it manually, but also
more eﬃcient than grid search.
We argue that SPO is such a method and thus of particular importance for
experimental studies on non-trivial EAs, as such utilizing self-adaptation. As the
experimental analysis in EC currently is a hot topic, other parameter tuning ap-
proaches are developed, too, e.g. [61]. But whatever method is used, there is hardly
a way around parameter tuning, at least for scientiﬁc investigations. SPO will be
discussed further in §4.
1.2 Self-adaptation for Binary Representations
When comparing recent literature, it is astounding that for real-coded EAs as e.g.
evolution strategies (ES), self-adaptation is an almost ubiquitously applied feature,
at least as far as mutation operators are concerned. In binary-coded EA however,
it did not become a standard method. Optimization practitioners working with
binary representations seem largely unconvinced that it may be of any use. What
may be the reason for this huge diﬀerence? One could ﬁrst think of traditional
reasons. Of the three standard evolutionary methods for numerical optimization,
genetic algorithms (GA), evolutionary programming (EP) and evolution strategies,
the last one ﬁrst adopted self-adaptation in various forms to improve mutation
operators ([64, 71, 72]), followed by EP in [25]. These two mainly work with real-
valued representations. Nevertheless, since then so much time has passed that it is
hard to imagine that others would not employ a technique that clearly provides an
advantage. Considering this, tradition appears not as a substantial reason for the
divergent developments.
In fact, meanwhile, several authors have tried to transfer self-adaptation to EAs
for binary encodings: [4, 3, 70, 74, 75, 77, 29]. If we accept the view of an evolutionary
epistemology underlying the development of our scientiﬁc domain, so that mostly
successful changes are inherited to the next stages, these studies have apparently
not been very convincing. Either they were ignored by the rest of the ﬁeld, or they
failed to provide reason good enough for a change. Let us assume the latter case.
In consequence, most EAs for binary representations published nowadays still do
without self-adaptation.

94
Mike Preuss and Thomas Bartz-Beielstein
Evolutionary algorithms employing diﬀerent forms of self-adaptation are usually
applied to continuous, or, more rarely, ordinal discrete or mixed search spaces. So
the distinction may result from diﬀerences in representation. When dealing with
real-valued numbers, it is quite easy to come by a good intuition why adapting
mutation strengths (also called mutation step sizes) during optimization can speed
up search. Features of a ﬁtness landscape changing slowly compared to the ﬁtness
values themselves can be learned, like gradients or covariances ([31, 30]). In a binary
space, this intuition is lacking. Deﬁnition of gradients is meaningless if a variable
can only take two values. Consequently, research is trying diﬀerent ways here, e.g.
linkage learning or distribution estimation as in [59]. As much as the experiences from
continuous search spaces cannot be simply transferred, neither can the methods. Self-
adaptation of mutation rates, as straightforward continuation of existing techniques
for real-valued variables, if working at all for binary representations, can be expected
to work diﬀerently.
2 Aims and Methods
Our aims are twofold: To demonstrate usefulness of the SPO approach for experimen-
tal analysis, and to perform an experimental analysis of self-adaptation mechanisms.
Methodologically, we want to convey that SPO is a suitable tool for ﬁnding good
parameter sets (designs) within a ﬁxed, low budget of algorithm runs. It seems safe
to assume that a parameter set exists in the parameter design space that is better
than the best of a small (≈100) sample. We therefore require that the best conﬁg-
urations detected by SPO are signiﬁcantly better than the best of a ﬁrst sample.
Furthermore, we want to suggest a ﬂexible yet structured methodology for perform-
ing and documenting experimentation by employing a parameter tuning technique.
Concerning the experimental analysis of self-adaptation mechanisms on binary
represented problems, our aim is to collect evidence for or against the following
conjectures:
• Well parametrized self-adaptation signiﬁcantly speeds up optimization when
compared to well tuned constant mutation rates for many problems.
• Detecting good parameter sets for self-adaptive EAs is usually not signiﬁcantly
harder than doing so for non self-adaptive EAs.
• Problem knowledge, e.g., shared properties or structural similarities of problem
classes, gives useful hints for selecting a self-adaptation mechanism.
Before these conjectures are detailed, we give an overview of existing parameter
tuning methods.
3 Parameter Optimization Approaches
Modern search heuristics have proved to be very useful for solving complex real–
world optimization problems that cannot be tackled through classical optimization
techniques [73]. Many of these search heuristics involve a set of exogenous parame-
ters, i.e., values are speciﬁed before the run is performed, that aﬀect their conver-
gence properties. The population size in EA is a typical example for an exogenous
strategy parameter. The determination of an adequate population size is crucial for

SPO Applied to Self-Adaptation for Binary-Coded EAs
95
many optimization problems. Increasing the population size from 10 to 50 while
keeping the number of function evaluations constant might improve the algorithm’s
performance—whereas a further increase might result in a performance decrease, if
the number of function evaluations remains constant.
SPO is based on statistical design of experiments (DOE) which has its ori-
gins in agriculture and industry. However, DOE has to be adapted to the special
requirements of computer programs. For example, computer program are per se de-
terministic, thus a diﬀerent concepts of randomness has to be considered. Law &
Kelton [46] and Kleijnen [43, 44] demonstrated how to apply DOE in simulation.
Simulation is related to optimization (simulation models equipped with an objec-
tive function deﬁne a related optimization problem), therefore we can beneﬁt from
simulation studies.
DOE related parameter studies were performed to analyze EA: Schaﬀer et al. [67]
proposed a complete factorial design experiment, Feldt & Nordin [24] use statisti-
cal techniques for designing and analyzing experiments to evaluate the individual
and combined eﬀects of genetic programming parameters. Myers & Hancock [58]
presented an empirical modeling of genetic algorithms. Fran¸cois & Lavergne [26]
demonstrate the applicability of generalized linear models to design evolutionary
algorithms. These approaches require up to 100,000 program runs, whereas SPO is
applicable even if a small amount of function evaluations are available only.
Because the search for useful parameter settings of algorithms itself is an opti-
mization problem, meta-algorithms have been proposed. B¨ack [2] and Kursawe [45]
presented meta-algorithms for evolutionary algorithms. But these approaches do not
solve the original problem completely, because they require the determination of ad-
ditional parameter setting of the meta-algorithm. Furthermore, we argue that the
experimenter’s skill plays an important role in this analysis. It cannot be replaced
by automatic “meta” rules.
SPO can also be run on auto-pilot without any user intervention. This conve-
nience cannot be obtained for free: The user gains limited insight into the working
mechanisms of the tuned algorithm and, which is even more severe, the validity and
thus the predictive power of the regression model might be quite poor.
The reader should note that our approach is related to the discipline of experi-
mental algorithmics, which oﬀers methodologies for the design, implementation, and
performance analysis of computer programs for solving algorithmic problems [18,
56]. Further valuable approaches have been proposed by McGeoch [50], Barr &
Hickman [5], and Hooker [32].
Design and analysis of computer experiments (DACE) as introduced in Sacks
et al. [65] models the deterministic output of a computer experiment as the realiza-
tion of a stochastic process. The DACE approach focuses entirely on the correlation
structure of the errors and makes simplistic assumptions about the regressors. It
describes “how the function behaves,” whereas regression as used in classical DOE
describes “what the function is” [40, p. 14]. DACE requires other experimental de-
signs than classical DOE, e.g., Latin hypercube designs (LHD) [51].
We claim that it is beneﬁcial to combine some of these well-established ideas
from DOE, DACE, and further statistical techniques to improve the acceptance
and quality of evolutionary algorithms.

96
Mike Preuss and Thomas Bartz-Beielstein
4 Sequential Parameter Optimization Methodology
How can optimization practitioners determine if concepts developed in theory work
in practice? Hence, experiments are necessary. Experiment has a long tradition in
science. To analyze experimental data, statistical methods can be applied. It is not
a trivial task to answer the ﬁnal question “Is algorithm A better than algorithm B?”
Results that are statistically signiﬁcant are not automatically scientiﬁcally mean-
ingful.
Example 4.1 (Floor and ceiling eﬀects). The statistical meaningful result “all
algorithms perform equally” can be scientiﬁcally meaningless, because the problem
instances are too hard for any algorithm. A similar eﬀect occurs if the problem
instances are too easy. The resulting eﬀects are known as ﬂoor or ceiling eﬀects,
respectively.
□
SPO is more than a simple combination of existing statistical approaches. It is based
on the new experimentalism, a development in the modern philosophy of science,
which considers that an experiment can have a life of its own. SPO provides a
statistical methodology to learn from experiments, where the experimenter should
distinguish between statistical signiﬁcance and scientiﬁc meaning.
An optimization run is considered as an experiment. An optimal parameter set-
ting, or statistically speaking, an optimal algorithm design, depends on the problem
at hand as well as on the restrictions posed by the environment (i.e., time and hard-
ware constraints). Algorithm designs are usually either determined empirically or
set equal to widely used default values. SPO is a methodology for the experimental
analysis of optimization algorithms to determine improved algorithm designs and
to learn, how the algorithm works. The proposed technique employs computational
statistic methods to investigate the interactions among optimization problems, algo-
rithms, and environments.
An optimization practitioner is interested in robust solutions, i.e., solutions inde-
pendent from the random seeds that are used to generate the random numbers during
the optimization run. The proposed statistical methodology provides guidelines to
design robust algorithms under restrictions, such as a limited number of function
evaluations and processing units. These restrictions can be modeled by considering
the performance of the algorithm in terms of the (expected) best function value for
a limited number of function evaluations.
To justify the usefulness of our approach, we analyze the properties of several
algorithms from the viewpoint of a researcher who wants to develop and understand
self-adaptation mechanisms for evolutionary algorithms. SPO provides numerical
and graphical tools to test if the statistical results are really relevant or have been
caused by the experimental setup only. It is based on a framework that permits a de-
linearization of the complex steps from raw data to scientiﬁc hypotheses. Substantive
scientiﬁc questions are broken down into several local hypotheses, that can be tested
experimentally. The optimization process can be regarded as a process that enables
learning. SPO consists of the twelve steps that are reported in Table 1. These steps
and the necessary statistical techniques will be presented in the following. SPO has
been applied on search heuristics in the following domains:
1. machine engineering: design of mold temperature control [53, 80, 52]
2. aerospace industry: airfoil design optimization [11]

SPO Applied to Self-Adaptation for Binary-Coded EAs
97
3. simulation and optimization: elevator group control [13, 49]
4. technical thermodynamics: nonsharp separation [10]
5. economy: agri-environmental policy-switchings [17]
Other ﬁelds of application are in fundamental research:
1. algorithm engineering: graph drawing [79]
2. statistics: selection under uncertainty (optimal computational budget alloca-
tion) for PSO [8]
3. evolution strategies: threshold selection and step-size adaptation [6]
4. other evolutionary algorithms: genetic chromodynamics [76]
5. computational intelligence: algorithmic chemistry [10]
6. particle swarm optimization: analysis and application [9]
7. numerics: comparison and analysis of classical and modern optimization algo-
rithms [12]
Further projects, e.g., vehicle routing and door-assignment problems and the appli-
cation of methods from computational intelligence to problems from bioinformatics
are subject of current research. An SPO-toolbox is freely available under the follow-
ing link: http://www.springer.com/3-540-32026-1.
4.1 Tuning
In order to ﬁnd an optimal algorithm design, or to tune the algorithm, it is necessary
to deﬁne a performance measure. Eﬀectivity (robustness) and eﬃciency can guide
Table 1. Sequential parameter optimization (SPO). This approach combines meth-
ods from computational statistics and exploratory data analysis to improve (tune)
the performance of direct search algorithms.
Step Action
(S-1) Preexperimental planning
(S-2) Scientiﬁc claim
(S-3) Statistical hypothesis
(S-4) Speciﬁcation of the
(a) optimization problem
(b) constraints
(c) initialization method
(d) termination method
(e) algorithm (important factors)
(f) initial experimental design
(g) performance measure
(S-5) Experimentation
(S-6) Statistical modeling of data and prediction
(S-7) Evaluation and visualization
(S-8) Optimization
(S-9) Termination: If the obtained solution is good enough, or the maximum num-
ber of iterations has been reached, go to step (S-11)
(S-10) Design update and go to step (S-5)
(S-11) Rejection/acceptance of the statistical hypothesis
(S-12) Objective interpretation of the results from step (S-11)

98
Mike Preuss and Thomas Bartz-Beielstein
the choice of an adequate performance measure. Note that optimization practitioners
do not always choose the absolute best algorithm. Sometimes a robust algorithm or
an algorithm that provides insight into the structure of the optimization problem
is preferred. From the viewpoint of an experimenter, design variables (factors) are
the parameters that can be changed during an experiment. Generally, there are two
diﬀerent types of factors that inﬂuence the behavior of an optimization algorithm:
• problem speciﬁc factors, e.g., the objective function
• algorithm speciﬁc factors, i.e., the population size or other exogenous parameters
We will consider experimental designs that comprise problem speciﬁc factors and
exogenous algorithm speciﬁc factors. Algorithm speciﬁc factors will be considered
ﬁrst. Endogenous can be distinguished from exogenous parameters [16]. The former
are kept constant during the optimization run, whereas the latter, e.g., standard
deviations, are modiﬁed by the algorithms during the run. Consider DA, the set of
all parameterizations for one algorithm. An algorithm design XA is a set of vectors,
each representing one speciﬁc setting of the design variables of an algorithm. A design
can be speciﬁed by deﬁning ranges of values for the design variables. A design point
xa ∈DA presents exactly one parameterization of an algorithm. Note that a design
can contain none, one, several or even inﬁnitely many design points. The optimal
algorithm design is denoted as X∗
A. The term “optimal design” can refer to the best
design point x∗
a as well as the most informative design points [60, 66].
Let DP denote the set of all problem instances for one optimization problem.
Problem designs XP provide information related to the optimization problem, such
as the available resources (number of function evaluations) or the problem’s dimen-
sion.
An experimental design XE ∈D consists of a problem design XP and an al-
gorithm design XA. The run of a stochastic search algorithm can be treated as an
experiment with a stochastic output Y (xa, xp), with xa ∈DA and xp ∈DP . If the
random seed is speciﬁed, the output would be deterministic. This case will not be
considered further, because it is not a common practice to specify the seed that is
used in an optimization run. Our goals of the experimental approach can be stated
as follows:
(G-1) Eﬃciency. To ﬁnd a design point x∗
a ∈DA that improves the performance of
an optimization algorithm for one speciﬁc problem design point xp ∈DP .
(G-2) Robustness. To ﬁnd a design point x∗
a ∈DA that improves the performance
of an optimization algorithm for several problem design points xp ∈DP .
Performance can be measured in many ways, e.g., as the best or the average function
value for n runs. Statistical techniques to attain these goals will be presented next.
4.2 Stochastic Process Models as Extensions of Classical
Regression Models
The classical DOE approach consists of three steps: Screening, modeling, and opti-
mization. Each step requires diﬀerent experimental designs. Linear regression mod-
els are central elements of the classical design of experiments approach [19, 55]. We
propose an approach that extends the classical regression techniques, because the
assumption of a linear model for the analysis of computer programs and the implicit

SPO Applied to Self-Adaptation for Binary-Coded EAs
99
model assumption that observation errors are independent of one another are highly
speculative [7]. To keep the number of experiments low, a sequential procedure has
been developed. Our approach relies on a stochastic process model, that will be
presented next.
We consider each algorithm design with associated output as a realization of a
stochastic process. Kriging is an interpolation method to predict unknown values
of a stochastic process and can be applied to interpolate observations from com-
putationally expensive simulations. Our presentation follows concepts introduced in
Sacks et al. [65], Jones et al. [40], and Lophaven et al. [48].
Consider a set of m design points x = (x(1), . . . , x(m))T with x(i) ∈Rd. In the
design and analysis of computer experiments (DACE) stochastic process model, a
deterministic function is evaluated at the m design points x. The vector of the m
responses is denoted as y = (y(1), . . . , y(m))T with y(i) ∈R. The process model
proposed in Sacks et al. [65] expresses the deterministic response y(x(i)) for a d-
dimensional input x(i) as a realization of a regression model F and a stochastic
process Z,
Y (x) = F(β, x) + Z(x).
(1)
DACE Regression Models
We use q functions fj : Rd →R to deﬁne the regression model
F(β, x) =
q

j=1
βjfj(x) = f(x)T β.
(2)
Regression models with polynomials of orders 0, 1, and 2 have been used in our
experiments. Regression models with a constant term only, i.e., f1 = 1, have been
applied successfully to model the data and to predict new data points in the sequen-
tial approach.
DACE Correlation Models
The random process Z(·) (Equation 1) is assumed to have mean zero and covariance
V (w, x) = σ2R(θ, w, x) with process variance σ2 and correlation model R(θ, w, x).
Consider an algorithm with d factors (parameters). Correlations of the form
R(θ, w, x) =
d

j=1
Rj(θ, wj −xj)
will be used in our experiments. The correlation function should be chosen with
respect to the underlying process [34]. Lophaven et al. [47] discuss seven diﬀerent
models. The Gaussian correlation function is a well-known example. It is deﬁned as
GAUSS :
Rj(θ, hj) = exp(−θjh2
j),
(3)
with hj = wj −xj, and for θj > 0. The regression matrix R is the matrix with
elements
Rij = R(xi, xj)
(4)

100
Mike Preuss and Thomas Bartz-Beielstein
that represent the correlations between Z(xi) and Z(xj). The vector with correla-
tions between Z(xi) and a new design point Z(x) is
r(x) = (R(x1, x), . . . , R(xm, x)) .
(5)
Large θj’s indicate that variable j is active: function values at points in the vicinity
of a point are correlated with Y at that point, whereas small θj’s indicate that
also distant data points inﬂuence the prediction at that point. The empirical best
unbiased linear predictor (EBLUP) can be shown to be
ˆy(x) = f T (x)ˆβ + rT (x)R−1(y −F ˆβ),
(6)
where
ˆβ = 
F T R−1F−1 F T R−1y
(7)
is the generalized least-squares estimate of β in Equation 1, f(x) are the q regression
functions in Equation 2, and F represents the values of the regression functions in
the m design points.
Maximum likelihood estimation methods to estimate the parameters θj of the
correlation functions from Equation 3 are discussed in Lophaven et al. [47]. DACE
methods provide an estimation of the prediction error on an untried point x, the
mean squared error (MSE) of the predictor
MSE(x) = E (ˆy(x) −y(x)) .
(8)
The stochastic process model, which was introduced as an extension of the classical
regression model will be used in our experiments. Next, we have to decide how to
generate design points, i.e., which parameter settings should be used to test the
algorithm’s performance.
Space Filling Designs and Expected Improvement
Often, designs that use sequential sampling are more eﬃcient than designs with ﬁxed
sample sizes. Therefore, we specify an initial design X(0)
A
∈D(0)
A
ﬁrst. Latin hyper-
cube sampling (LHS) was used to generate the initial algorithm designs. Consider n
number of levels being examined and d design variables. A Latin hypercube is a ma-
trix of n rows and d columns. The d columns contain the levels 1, 2, . . . , n, randomly
permuted, and the d columns are matched at random to form the Latin hypercube.
The resulting Latin hypercube designs are space-ﬁlling designs. McKay et al. [51] in-
troduced LHDs for computer experiments, Santner et al. [66] give a comprehensive
overview.
Information obtained in the ﬁrst runs can be used for the determination of the
second design X(1)
A
in order to choose new design points sequentially and thus more
eﬃciently.
Sequential sampling approaches have been proposed for DACE. For example,
in Sacks et al. [65] sequential sampling approaches were classiﬁed to the existing
meta-model. We will present a sequential approach that is based on the expected
improvement. In Santner et al. [66, p. 178] a heuristic algorithm for unconstrained
global minimization problems is presented. Consider one problem design point xp.
Let y(t)
min denote the smallest known minimum value after t runs of the algorithm,

SPO Applied to Self-Adaptation for Binary-Coded EAs
101
SequentialParameterOptimization(DA, DP )
1
Select p ∈DP and set t = 0
/* Select problem instance */
2
X(t)
A = {x1, x2, . . . , xk}
/* Sample k initial points, e.g., LHS */
3
repeat
4
yij = Yj(xi, p)∀xi ∈X(t)
A
and j = 1, . . . , r(t) /* Fitness evaluation */
5
Y
(t)
i
= r(t)
j=1 y(t)
ij /r(t)
/* Sample statistic for the ith design point */
6
xb with b = arg mini(yi)
/* Determine best point */
7
Y (x) = F(β, x) + Z(x)
/* DACE model from Eq. 1 */
8
XS = {xk+1, . . . , xk+s}
/* Generate s sample points, s ≫k */
9
y(xi), i = 1, . . . , k + s
/* Predict ﬁtness from DACE model */
10
I(xi) for i = 1, . . . , s + k
/* Expected improvement (Eq. 9) */
11
X(t+1)
A
= X(t)
A ∪{xk+i}m
i=1 /∈X(t)
A
/* Add m promising points */
12
if x(t)
b
= x(t+1)
b
13
then r(t+1) = 2r(t)
/* Increase number of repeats */
14
t = t + 1
/* Increment iteration counter */
15
until BudgetExausted?()
Fig. 1. Sequential parameter optimization
y(x) be the algorithm’s response, i.e., the realization of Y (x) in Equation (1), and
let xa represent a speciﬁc design point from the algorithm design XA. Then the
improvement is deﬁned as
I(xa) =

y(t)
min −y(xa),
y(t)
min −y(xa) > 0
0,
otherwise
(9)
for xa ∈DA. As Y (·) is a random variable, its exact value is unknown. The goal is
to optimize its expectation, the so-called expected improvement. New design points,
which are added sequentially to the existing design, are attractive “if either there
is a high probability that their predicted output is below [minimization] the cur-
rent observed minimum and/or there is a large uncertainty in the predicted out-
put.” This leads to the expected improvement heuristic [7]. Based on theorems from
Schonlau [68, p. 22] we implemented a program to estimate and plot the main fac-
tor eﬀects. Furthermore, three dimensional visualizations produced with the DACE
toolbox [48] can be used to illustrate the interaction between two design variables
and the associated mean squared error of the predictor.
Figure 1 describes the SPO in a formal manner. The selection of a suitable
problem instance is done in the pre-experimental planning phase to avoid ﬂoor and
ceiling eﬀects (l.2). Latin hypercube sampling can be used to determine an initial set
of design points (l.3). After the algorithm has been run with these k initial parameter
settings (l.5), the DACE process model is used to discover promising design points
(l.10). Note that other sample statistics than the mean, e.g., the median, can be
used in l.6. The m points with the highest expected improvement are added to
the set of design points, where m should be small compared to s. The update rule
for the number of reevaluations r(t) (l.13–15) guarantees that the new best design
point x(t+1)
b
has been evaluated at least as many times as the previous best design
point x(t)
b . Obviously, this is a very simple update rule and more elaborate rules

102
Mike Preuss and Thomas Bartz-Beielstein
are possible. Other termination criteria exist besides the budget based termination
(l.17).
4.3 Experimental Reports
Surprisingly, despite around 40 years of empirical tradition, in EC a standardized
scheme for reporting experimental results never developed. The natural sciences, e.g.
physics, possess such schemes as de-facto standards. We argue that for both groups,
readers and writers, an improved report structure is beneﬁcial: As with the com-
mon overall paper structure (introduction, conclusions, etc.), a standard provides
guidelines for readers, what to expect, and where. Writers are steadily reminded to
describe the important details needed to understand and possibly replicate their ex-
periments. For the structured documentation of experiments, we propose organizing
their presentation into 7 parts, as follows.
ER-1:
Focus/Title
Brieﬂy names the matter dealt with, the (possibly very general) objective,
preferably in one sentence.
ER-2:
Pre-experimental planning
Reports the ﬁrst—possibly explorative—program runs, leading to task and setup
(steps ER-3 and ER-4) . Decisions on used benchmark problems or performance
measures may often be inﬂuenced by the outcome of preliminary runs. This
may also include negative results, e.g. modiﬁcations to an algorithm that did
not work, or a test problem that turned out to be too hard, if they provide new
insight.
ER-3:
Task
Concretizes the question in focus and states scientiﬁc and derived statistical hy-
potheses to test. Note that one scientiﬁc hypothesis may require several, some-
times hundreds of statistical hypotheses. In case of a purely explorative study,
as with the ﬁrst test of a new algorithm, statistical tests may be not applicable.
Still, the task should be formulated as precise as possible.
ER-4:
Setup
Speciﬁes problem design and algorithm design, containing ﬁxed and variable
parameters and criteria of tackled problem, investigated algorithm and chosen
performance measuring. The information in this part should be suﬃcient to
replicate an experiment.
ER-5:
Experimentation/Visualization
Gives raw or produced (ﬁltered) data on the experimental outcome, additionally
provides basic visualizations where meaningful.
ER-6:
Observations
Describes exceptions from the expected, or unusual patterns noticed, without
subjective assessment or explanation. As an example, it may be worthwhile to
look at parameter interactions. Additional visualizations may help to clarify
what happens.
ER-7:
Discussion
Decides about the hypotheses speciﬁed in step 4.3, and provides necessarily
subjective interpretations for the recorded observations.
This scheme is tightly linked to the 12 steps of experimentation suggested in [7]
and depicted in Table 1, but on a slightly more abstract level. The scientiﬁc and
statistical hypothesis steps are treated together in part ER-3, and the SPO core

SPO Applied to Self-Adaptation for Binary-Coded EAs
103
(parameter tuning) procedure, much of which may be automated, is included in
part ER-5. In our view, it is especially important to divide parts ER-6 and ER-7,
to facilitate diﬀerent conclusions drawn by others.
5 Self-Adaptation Mechanisms and Test Problems
In order to prepare a meaningful experiment, we want to take up the previously cited
warning words from [39] and [23] concerning the proper selection of ingredients for
a good setup and carefully select mechanisms and problems to test.
5.1 Mechanisms: Self-Adaptive, Constant, Asymmetrical
[54], to be found in this volume, give an extensive account of the history and current
state of adaptation mechanisms in evolutionary algorithms. However, in this work,
we consider only self-adaptive mechanisms for binary represented (often combinato-
rial) problems. Self-adaptiveness basically means to introduce unbiased deviations
into control parameters and let the algorithm chose the value that apparently works
best. Another possibility would be to use a rule set as in the previously stated 1/5th
rule by [63], which grants adaptation, but not self-adaptation. The mechanism sug-
gested by [29] is of this type. Others aim for establishing a feedback loop between
the probability of using one operator and the ﬁtness advancement this operator is
responsible for, e.g. [42, 33, 78].
Thus, whereas several adaptation techniques have been proposed for binary rep-
resentations, few are purely self-adaptive. The mechanisms proposed in [3] and [75]
have this property, though they origin from very diﬀerent sources. The former is a
variant of a self-adaptation scheme designed for real-valued search spaces; the lat-
ter has been shaped especially for binary search spaces and to overcome premature
convergence. Mutation of the mutation rate in [3] is accomplished according to the
formula:
p′
k =
1
1 + 1−pk
pk
exp(−γNk(0, 1))
,
(10)
where Nk(0, 1)) is a standard normally distributed random variable. p′
k stands for
the new, and pk the current mutation rate, which must be prevented from be-
coming 0 as this is a ﬁxpoint of the iteration. The approach of [75] is completely
diﬀerent; it employs a discrete, small number q of mutation rates, one of which
is selected according to the innovation rate z, so that the probability for alter-
ation in one generation respects pa = z · (q −1)/q. The q values are given as
pm ∈{0.0005, 0.001, 0.0025, 0.005, 0.0075, 0.01, 0.025, 0.05, 0.075, 0.1} and thus un-
balanced in the interval [0, 1]. We employ a diﬀerent set to facilitate comparison
with the ﬁrst mechanism that produces balanced mutation rates. Thereby, the diﬀer-
ences are reduced to continuity or discreteness with and without causal relationship
between new and old values (the second mechanism is stateless). In the following
experiment, we also set q to ten, with pm ∈{0.001, 0.01, 0.05, 0.1, 0.3, 0.7, 0.9, 0.95,
0.99, 0.999}.
Depending on the treated problem, it sometimes pays to use asymmetric muta-
tion, that is, diﬀerent mutation rates for ones and zeroes, as has been shown e.g.

104
Mike Preuss and Thomas Bartz-Beielstein
for certain instances of the Subset Sum problem by [37]. In [38] a meta-algorithm—
itself an EA—was used to learn good mutation rate pairs. In this work, we also
want to enable comparison between constant and self-adaptive asymmetric muta-
tion rates by simply extending the two named symmetric self-adaptive mechanisms
to two independently adapted mutation rates in the fashion of the asymmetric con-
stant rate operator. It shall not be concealed that recent theoretical investigations
also deal with asymmetric mutation operators, e.g. [36], even though their notion of
asymmetry is slightly diﬀerent.
5.2 Problems and Expectations
We chose a set of six test problems that maybe divided into three pairs, according to
similar properties. Our expectation is that shared problem attributes lead to com-
parable performance and parametrization of algorithms. However, we are currently
not able to express problem similarity quantitatively. Thus, our experimental study
may be seen as a ﬁrst explorative attempt in this direction.
wP-PEAKS and SUFSAMP
wP-PEAKS stands for weighted P-PEAKS generator, a modiﬁcation of the original
problem by [41], which employed random N-bit strings to represent the location of
P peaks in search space. A small/large number of peaks results in weakly/strongly
epistatic problems. Originally, each peak represented a global optimum. We employ
a modiﬁed version ([27]) by adding weights wi ∈R+ with only w1 = 1.0 and
w[2...P ] < 1.0, thereby requiring the optimization algorithm to ﬁnd the one peak
bearing the global optimum instead of just any peak. In our experiments, P and N
were 100, and wi ∈[0.9, 0.99] for local optima.
The SUFSAMP problem has been introduced by [35] as test for the fraction of
neighborhoods actually visited by an EA when building the oﬀspring generation.
The key point is that only one of the direct neighbours, the one that provides
the highest ﬁtness gain, leads towards the global optimum. Thus, it is essential
to perform suﬃcient sampling (high selection pressure) to maintain the chance of
moving in the right direction. We used an instance with bitlength 30 for our tests,
this is just beyond the point where the problem is solved easily.
Both problems are by far not similar, but still somewhat related because they
are extreme in requiring global and local exploration. In case of the wP-PEAKS,
the whole search space is to cover to ﬁnd the best peak, in SUFSAMP the local
neighborhood must be covered well to get hold of the path to the global optimum.
MMDP and COUNTSAT
The Massively Multimodal Deceptive Problem (MMDP) as suggested by [28] is a
multiple of a 6 bit deceptive subproblem which has its maximum for 6 or 0 bits set,
its minimum for 1 or 5 bits set, and a deceptive local peak at 3 set bits. The ﬁtness
of a solution is simply the sum over all subproblems. From the mode of construction,
it becomes clear that mutation is deliberately mislead here whether recombination
is essential. We use an instance with 40 blocks of 6 bit, each.
The COUNTSAT problem is an instance of the MAXSAT problem suggested
by [20]. Its ﬁtness only depends on the number of set bits, all set to 1 in the global
optimum. Our test instance is of length 40.

SPO Applied to Self-Adaptation for Binary-Coded EAs
105
These two problems share the property that their ﬁtness values are invariant to
permutations of the whole (COUNTSAT) or separate parts (MMDP) of the genome.
Consequently, [27] empirically demonstrate the general aptitude of self-adaptive mu-
tation mechanisms on these problems.
Number Partitioning and Subset Sum
The (Min) Number Partitioning Problem (MNP) requires arranging a set of long
numbers, here 35 with 10 digits length each, into two groups adding up to the same
sum. Fitness of a solution candidate is measured as the remaining diﬀerence in sums,
meaning this is a minimization problem. It was treated e.g. in [15, 14] by means of a
memetic algorithm. We used a randomly determined instance for each optimization
run.
The Subset Sum problem is related to the MNP in that from a given collection of
positive integers, a subset must be chosen to achieve a predetermined sum. This time,
the target sum does not directly depend on the overall sum of the given numbers but
can be any attainable number. Additionally, all solution candidates approaching it
from above are counted as infeasible. We use the same setup as in [38], made up of
100 numbers from within [0, 2100] and a density of 0.1, thus constructing the target
sum from 10 of the original numbers. For each optimization run, an instance of this
form is randomly created.
As stated, both are set selection problems. However, the expected fraction of
ones in a good solution is diﬀerent: Around 0.5 for the MNP, and 0.1 for the Subset
Sum. Therefore, me may expect asymmetric mutation rates (self-adpative or not)
to perform well on the latter, and symmetric on the former.
6 Assessment via Parameter Optimization
Within this section, we perform and discuss an SPO-based experiment to collect
evidence for or against the claims made in §2. However, allowing for multiple starts
during an optimization run on one and simultaneously employing algorithms with
enormous diﬀerences in speed and quality on the other hand complicates assess-
ing performance by means of standard measures like the MBF (mean best ﬁtness),
AES (average evaluations to solution), or SR (success rate). This is also conﬁrmed
by other views, e.g. [62], who express that dealing with aggregated measures, es-
pecially for multistart algorithms, requires some creativity. Additionally, we want
to investigate the “tunability” of an algorithm-problem pair, for which no common
measures exist. We therefore resort to introducing the needed, prior to describing
the experiment itself.
6.1 Adequate Measures
LHS Average and Best
For assessing tunability of an algorithm towards a problem, the best performance
found by a parameter optimization method shall be related to a base performance
that is easily obtained without, or by manual tuning. Additionally, comparison to a

106
Mike Preuss and Thomas Bartz-Beielstein
simple hillclimber makes sense, as the more complex EAs should be able to attain
better solutions than these to claim any importance.
The performance achieved by manual tuning surely depends on the expertise of
the experimenter. As a substitute for this hardly quantiﬁable measure we propose to
employ the best performance contained in an LHS of size 10 × #parameters, which
in our case resembles the result of an initial SPO step. In contrast to the best, the
average performance of all LHS design points is an approximation of the expected
quality of a random conﬁguration. Moreover, the variance of this quantity hints to
low large the diﬀerences are. Large variances may indicate two things at once: a)
There are very bad conﬁgurations the user must avoid, and b) there exist very good
conﬁgurations that cover only a small amount of the parameter space and are thus
hard to ﬁnd. Low variances, however, mean that the algorithm is neither tunable
nor mis-conﬁgurable. In consequence, we suggest to observe the measures fhc, the
MBF of a standard hillclimber, fLHSa, the average ﬁtness of an LHS design, σLHSa,
its standard deviation, fLHSb, ﬁtness of the best conﬁguration of an LHS, and fSPO,
the best performance achieved when tuning is ﬁnished.
AEB: Average Evaluations to Best
The AES needs a deﬁnition of success and is thus a problem-centric measure. If the
speciﬁed success rarely occurs, its meaning is questionable. Lowering the quality of
what counts as success may lead from ﬂoor to ceiling eﬀect if performance diﬀerences
of the algorithms in scope are large and the meaning of success is not carefully
balanced. We turn this measure into an algorithm-centric one by taking the average
of the number of evaluations needed to obtain the peak performance in each single
optimization run. Stated diﬀerently, this is the average time an algorithm continues
to make progress. This quantity still implies dependency of the time horizon of the
experiment (the total number of evaluations allowed), but in contrast to the AES, it
is always deﬁned. The AEB alone is of limited value, as fast converging algorithms
are prefered, regardless of the quality of their ﬁnal best solution. However, it is useful
for deﬁning the following measure.
RFA: Robustness Favoring Aggregate
Assessing robustness and eﬃciency of an optimization algorithm at the same time
is diﬃcult, especially in situations where the optimum is seldomly reached, or even
worse, is unknown. Common ﬁtness measures as the MBF, AES, or SR, apply only
to one of the two aspects. Combined measures as the success performance suggested
by [1] require deﬁnition of success, as well. Two possibilities remain to resolve this
dilemma: Multicriterial assessment, or aggregation. We choose the latter and use a
robustness favoring aggregate (RFA), deﬁned as:
RFA = ∆f ·
 ∆f
AEB
 , ∆f =

MBF −fhc
:
minimization
fhc −MBF
:
maximization
(11)
The RFA consists of ﬁtness progress, relative to the hillclimber, multiplied by the
linear relative progress rate, which depends on the average number of evaluations
to reach the best ﬁtness value (AEB). It weights robustness higher than eﬃciency,
but takes both into account. Using solely the linear relative progress rate is no
alternative; it rates fast algorithms achieving poor quality equal to slow algorithms

SPO Applied to Self-Adaptation for Binary-Coded EAs
107
able to attain good quality. If an algorithm performs better than the hillclimber
(in quality), its RFA is negative, positive otherwise. Note that this is only one of a
virtually unlimited number of ways to deﬁne an aggregate.
6.2 Experiment
Apply SPO to EAs including diﬀerent self-adaptive mechanisms and ﬁxed
rate mutation, compare tunability and achievable quality.
Table 2. Algorithm design for the six EA variants. Only asymmetric mutation oper-
ators need a second mutation rate and its initialization range (∗). The learning rate
(∗∗) is only applicable to adaptive mutation operators, hence not used for variants
with constant mutation rates. Algorithm designs thus have 7, 8, 9, or 10 parameters.
Parameter name
N/R Min Max Parameter name
N/R Min Max
Population size µ
N
1 500 Maximum age κ
N
1
50
Selection pressure λ/µ
R+
1
10 Recombination prob. pr
R
0
1
Stagnation restart rs
N
5
30 Learning rate∗∗τ
R
0.0
1.0
Mutation rate pm
R+ 10−3
1.0 Mut. rate range r(pm)
R
0
0.5
Mutation rate 2∗pm2
R+ 10−3
1.0 Mut. rate 2 range∗r(pm2) R
0
0.5
Pre-experimental planning:
The originally intended performance measure (MBF) has been exchanged with the
RFA ﬁtness measure, due to ﬁrst experiences with certain EA conﬁgurations on
the MMDP problem, namely with asymmetric mutation operators. RFA provides a
gradient even between the best found conﬁgurations that always solved the problem
to optimality before, even for increased problem sizes (100 or more 6-bit groups).
Task:
Based on the aims named in §2, we investigate two scientiﬁc claims:
1. Tuned self-adaptative mutation allows for better performance than tuned con-
stant mutation for many problems.
2. The tuning process is not signiﬁcantly harder for self-adaptive mutation.
Additionally, the third aim shall be achieved by searching for patterns in the
virtual relation between problem class properties and performance results. We per-
form statistical hypothesis testing by employing bootstrap permutation tests with
the commonly used signiﬁcance level of 5%. For accepting the ﬁrst hypothesis, we
demand a signiﬁcant diﬀerence between the fastest self-adaptive and the fastest non-
adaptive EA variant for at least half of the tested problems. The second claim is
more diﬃcult to test; in absence of a well-deﬁned measure, we have to resort to just
expressing the impression obtained from evaluating data and visualizations.
Setup:
The optimization algorithm used is a panmictic (µ, κ, λ)-ES with diﬀerent mutation
operators and 2-point crossover for recombination. As κ is one of the factors the
parameter optimization is allowed to change, the degree of elitism (complete for
κ ≥max(#generations), none for kappa = 1), that is the time any individual may

108
Mike Preuss and Thomas Bartz-Beielstein
survive within the population, maybe varied. As common for binary representations,
mutation works with bit ﬂip probabilities (rates). We employ 6 diﬀerent modes of
performing mutation, namely constant rates, the self-adaptative method of [70], and
the self-adaptive method of [75], each in turn symmetrical and asymmetrical (2 mu-
tation rates, one for 0s and 1s, respectively). Table 2 shows the algorithm design
for all variants. Note that not every parameter applies to all mutation operators.
Parameter ranges are chosen relatively freehanded, bounded only by resource lim-
its (µ, λ) or to enable a reasonable gradient (rs, κ) in conﬁguration qualities. Two
features of the used algorithm variants may appear unusual, namely the recombi-
nation probability, allowing for gradually switching recombination on and oﬀ, and
the stagnation restart, enabling restarts after a given number of generations without
improvement.
Fig. 2. SPO performance on weighted P-PEAKS (left) and SUFSAMP (right) prob-
lems. Points denote the best conﬁgurations found up to the corresponding number
of evaluations (runs), using all repeats available after 1000 runs to reduce noise.
Except the last one, all points stand for changed conﬁgurations regarded as better
by SPO. The ones tried in between are estimated as worse than the current best.
The graphs start on the left with the best conﬁguration of the initial LHS (fLHSb).
SPO parameters are kept at default values where possible. However, the total
budget of allowed runs always is a design choice. We set this to 1000 as compromise
between eﬀectivity and operability. The initial LHS size is set to 10 · #parameters
—following practical experience, also suggested by [69]— with 4 repeats per conﬁg-
uration, the maximum number of repeats to 64. Testing is performed at signiﬁcance
level 5%.

SPO Applied to Self-Adaptation for Binary-Coded EAs
109
Fig. 3. SPO performance on COUNTSAT (left) and MMDP (right) problems. Re-
marks of Fig. 2 apply here as well.
The tackled problems are the 6 named in §5. For each problem and mutation
operator variant (6 × 6 = 36), we perform a separate SPO run (1000 algorithm runs
each).
Experimentation/Visualization:
The development of the SPO runs on either of the 6 problems is presented in ﬁgures
2, 3, and 5. Note that for the depicted conﬁgurations, we use information gained
after a SPO is ﬁnished, which is more than that available at runtime, due to possible
re-evaluations (additional runs) of good conﬁgurations. Restriction to runtime infor-
mation leads to heavily increased noise that would render the ﬁgures almost useless.
Numerical results for the weighted P-PEAKS, Subset Sum, and MMDP problems
are given in Table 4, the last column giving error probabilities for rejecting the hy-
pothesis that fLHSb and fSP O are equal (p-values), based on all measures of this
conﬁguration obtained during the SPO run.
Observations:
As is evident from ﬁgures 2 to 5, there are two diﬀerent behaviors of the six algo-
rithm variants on on the considered test problems: Either a clear distinction into two
groups of three can be recognized (SUFSAMP, COUNTSAT, and MMDP), or the
curves are much more intermingled. A closer look at the constituents of these groups
reveals that the separating feature is the symmetry type. For the COUNTSAT and
MMDP problems, the better group is formed by all three asymmetric mutation vari-
ants, whereas all three symmetric variants are performing better on the SUFSAMP
problem.
For the wP-PEAKS problem, we obtain two surprising observations:
1. At both ends of the performance spectrum, we ﬁnd the self-adaptation variant
of Smith, the symmetric being the best, the asymmetric the worst.

110
Mike Preuss and Thomas Bartz-Beielstein
2. Ordering of the symmetric/asymmetric variants of one type is not the same for
all, for the variants using the self-adaptation of Sch¨utz, it is reversed.
As the SUFSAMP problem was constructed to favor enumerating the local neigh-
borhood, it can be expected that SPO adjusts the selection pressure to high values.
A look at the parameter distributions of the best variant, ﬁgure 4, reveals that SPO
surprisingly circumvented doing so by more frequently using higher maximum age
values; at the same time, the mean stagnation restart time was also increased.
Fig. 4. Parameter distributions of conﬁgurations chosen by SPO (including the
initial LHS) on the SUFSAMP problem with symmetric self-adaptation after Sch¨utz,
separated into three equally sized groups according to measured ﬁtness.
The number partitioning problem results surprisingly indicate constant asym-
metric mutation as the best and self-adaptation after Sch¨utz as the worst variant,
the latter being obviously very hard to improve. The tuning process on this as well
as on the Subset Sum problem (ﬁgure 5), looks much more disturbed than e.g. on
the MMDP, containing many huge jumps.
Discussion:
A look at Table 4 reveals that for the chosen, discrete test problems, averaged
performances and standard deviations are most often very similar. In contrast to
the situation usually found for real-valued test problems treated with SPO, the noise
level here makes further improvement very diﬃcult. It obviously cannot be easily
lowered by increasing the number of repeats as the best conﬁgurations typically
already reached the imposed maximum of 64 runs. This hardship is probably due
to the discrete value set of the objective functions which hinder achieving narrow
result distributions. It does not render SPO useless, but its performance seems to
be limited tighter than for the real-valued case.
Another unexpected conclusion is that asymmetric mutation operators often
perform very well on functions usually treated with symmetric constant mutation
rates. COUNTSAT and MMDP are best approximated with asymmetric mutation
operators, the former with the constant, the latter with the self-adaptive variant by
Sch¨utz. Table 3 lists the 2 best variants for each problem and the p-Values obtained
from the comparison. Concerning aim 1 of §2, we cannot verify the original claim.

SPO Applied to Self-Adaptation for Binary-Coded EAs
111
Fig. 5. SPO performance on Number Partitioning (left) and Subset Sum (right).
There are problems for which self-adaptation is very useful, and there are some
where it is not. Only in 2 of the six test cases, they have signiﬁcant advantage.
Table 3. Bootstrap Permutation hypothesis tests between performances of the best
constant and the best self-adaptive variant for each problem. The tests use all best
conﬁguration runs available after SPO is ﬁnished, this is in most cases 64.
Problem
Best Variant
Second Variant
p-Value Signiﬁcant?
wP-PEAKS
Smith, symmetric
constant, symmetric
0.125
No
SUFSAMP
Sch¨utz, symmetric
constant, symmetric
2e-05
Yes
COUNTSAT
constant, asymmetric Sch¨utz, asymmetric
0.261
No
MMDP
Sch¨utz, asymmetric
constant, asymmetric
0.008
Yes
Number Part.
constant, asymmetric Sch¨utz, symmetric
0.489
No
Subset Sum
constant, asymmetric Sch¨utz, asymmetric
0.439
No
Concerning the eﬀort needed for tuning constant and self-adaptive mutation
operators, we cannot account for a clear diﬀerence, thereby adhering to the original
claim that tuning eﬀorts are similar. However, there is neither statistical evidence
in favor nor against this statement.
Nevertheless, when comparing the ﬁnal best conﬁgurations to the average LHS
results, it becomes clear that without any tuning, algorithms may easily be miscon-
ﬁgured, leading to performance values much worse than that of a simple EA-based
hillclimber. Moreover, the best point of an initial LHS appears as a reasonable es-
timate for the quality achievable by tuning. Within this experiment, the LHS best
ﬁtness fLHSb has always been superior to that of the (single-start) hillclimber.

112
Mike Preuss and Thomas Bartz-Beielstein
Is it possible to derive any connection between the properties of the tackled
problems and the success of diﬀerent self-adaptation variants? We can detect simi-
larities in the progression of the SPO runs, e.g. between COUNTSAT and MMDP,
and Number Partitioning and Subset Sum, like the huge jumps found in case of
the latter; these probably indicate result distributions much more complex than for
the former. However, it appears diﬃcult to foresee if self-adaptation enhanced EAs
will perform better or worse. In case of the MMDP, recombination is likely to pre-
serve existing good blocks that may be optimized individually so that mutation rate
schedules, once learned, can be successfully reapplied. But without further tests,
this is rather speculation.
7 Conclusions
We motivated and explained the SPO procedure and, as a test case, applied it to
self-adaptive EA variants for binary coded problems. This study revealed why self-
adaptation is rarely applied for these problem types: It is hard to predict whether it
will increase performance, and which variant to choose. However, if properly para-
metrized, it can be signiﬁcantly faster than well tuned standard mutation operators.
Modeling only a rough problem-mechanism interaction would require a much more
thoroughly conducted study than presented here.
Surprisingly, specialized and seldomly used operators like the (constant and self-
adaptive) asymmetric mutation performed very well when parametrized accordingly.
This probably leads the way to superior operators still to develop—with or without
self-adaptation. Mutation rate learning seems to provide rather limited potential.
The invented performance measures, RFA and AEB, were found capable of lead-
ing the meta-search into the right direction. Unfortunately, the absence of well-
deﬁned measures for the tuning process itself currently prevents eﬀectively using
SPO for answering questions concerning the tunability of an algorithm-problem
combination. This problem we want to tackle in future work.
Recapitulating, SPO works on binary problems, and proves to be a valuable
tool for experimental analysis. However, there is room for improvement, ﬁrst and
foremost by taking measures to reduce the variances within the results of a single
conﬁguration.
Acknowledgment
The research leading to this paper has been supported by the DFG (Deutsche
Forschungsgemeinschaft) as project no. 252441, “Mehrkriterielle Struktur- und Pa-
rameteroptimierung verfahrenstechnischer Prozesse mit evolution¨aren Algorithmen
am Beispiel gewinnorientierter unscharfer destillativer Trennprozesse”. T. Bartz–
Beielstein’s research was supported by the DFG as part of the collaborative research
center “Computational Intelligence” (531).
The autors want to thank Thomas Jansen for providing his code of the SUF-
SAMP problem, and Silja Meyer-Nieberg and Nikolaus Hansen for sharing their
opinions with us.

Table 4. Best found conﬁguration performances, mean value of LHS, best of LHS, and best after SPO is ﬁnished, on the weighted P-
PEAKS, Subset Sum, and MMDP problems. Besides the RFA ﬁtness measure used within SPO (to minimize, relative to hillclimber), we
also give the MBF and AEB performance measures. AEB values are meant as multiples of 103. Note that wP-PEAKS and MMDP(40)
are to be maximized with optimal values at 1.0 and 40.0, respectively, and Subset Sum is to be minimized to 0.0.
Hillclimber, fhc
LHS mean, fLHSa
LHS best, fLHSb
SPO best, fSP O
Adaptation
MBF AEB
RFA
stddev
MBF AEB
RFA
stddev
MBF AEB
RFA
stddev
MBF AEB p-val
Problem: wP-PEAKS
none, sym
0.945
<5
4.7e-07 9.5e-07
0.852 61.9
-6.2e-08 6.7e-08
0.984 52.1
-8.1e-08 8.2e-08
0.993 50.4 0.29
none, asym
0.945
<5
4.2e-07 8.8e-07
0.851 63.4
-2.9e-08 8.3e-08
0.964 48.8
-6.1e-08 2.5e-08
0.983 30.4 0.00
Sch¨utz, sym
0.945
<5
5.0e-07 9.1e-07
0.844 60.5
-5.7e-08 3.1e-08
0.995 57.3
-6.9e-08 1.1e-07
0.980 46.2 0.46
Sch¨utz, asym
0.945
<5
4.1e-07 1.1e-06
0.858 62.0
-7.4e-08 8.2e-08
0.984 48.3
-8.2-08 4.4e-08
0.984 24.2 0.35
Smith, sym
0.945
<5
4.9e-07 1.0e-06
0.846 58.5
-7.9e-08 1.2e-07
0.990 54.2
-1.1e-07 1.6e-07
0.987 46.1 0.12
Smith, asym
0.945
<5
4.0e-07 8.6e-07
0.858 63.1
-5.4e-08 3.4e-08
0.984 41.6
-5.4e-08 3.4e-08
0.984 41.6 0.50
Problem: Subset Sum
none, sym
5.5e+26 36.0 3.4e+59 3.9e+60 4.7e+31 54.7 -7.3e+48 1.2e+49 1.6e+26 49.3 -7.8e+48 6.8e+48 8.1e+25 49.2 0.44
none, asym
5.5e+26 36.0 1.4e+59 6.4e+59 3.2e+31 52.5 -7.9e+48 1.0e+49 7.7e+25 55.6 -1.1e+49 1.6e+49 7.7e+25 38.0 0.18
Sch¨utz, sym
5.5e+26 36.0 1.9e+59 1.1e+60 5.0e+31 53.9 -4.9e+48 6.6e+48 2.0e+26 51.7 -7.7e+48 5.6e+48 4.3e+25 51.2 0.04
Sch¨utz, asym
5.5e+26 36.0 2.5e+59 1.9e+60 2.9e+31 49.4 -1.0e+49 1.2e+49 4.2e+25 51.0 -1.1e+49 1.4e+49 7.1e+25 48.1 0.42
Smith, sym
5.5e+26 36.0 2.6e+59 1.7e+60 5.0e+31 55.7 -5.1e+48 5.2e+48 1.3e+26 58.3 -7.0e+48 9.0e+48 8.8e+25 52.6 0.21
Smith, asym
5.5e+26 36.0 3.2e+59 4.5e+60 2.8e+31 51.1 -9.7e+48 1.3e+49 7.3e+25 52.5 -9.7e+48 1.3e+49 7.3e+25 52.5 0.50
Problem: MMDP
none, sym
23.80 58.1
-1.1e-03 1.4e-03
29.3 62.0
-4.3e-03 2.3e-03
37.0 54.3
-4.3e-03 4.3e-04
37.8 45.7 0.48
none, asym
23.80 58.1
-0.225
0.783
39.9
7.4
-4.60
4.83
40
<5
-7.13
11.7
40
<5 0.23
Sch¨utz, sym
23.80 58.1
-1.3e-03 1.7e-03
30.0 63.0
-4.3e-03 2.8e-03
36.0 48.3
-4.6e-03 2.3e-03
37.6 54.3 0.37
Sch¨utz, asym
23.80 58.1
-0.155
0.274
39.8
8.1
-2.28
0.894
40
<5
-12.0
10.9
40
<5 0.00
Smith, sym
23.80 58.1
-1.4e-03 1.8e-03
30.0 62.1
-4.1e-03 2.4e-03
37.3 59.6
-4.6e-03 2.1e-03
36.8 46.1 0.18
Smith, asym
23.80 58.1
-0.143
0.234
39.8
7.4
-1.56
0.774
40
<5
-8.41
6.59
40
<5 0.00

114
Mike Preuss and Thomas Bartz-Beielstein
References
1. Anne Auger and Nikolaus Hansen.
Performance Evaluation of an Advanced
Local Search Evolutionary Algorithm. In B. McKay et al., editors, Proc. 2005
Congress on Evolutionary Computation (CEC’05), Piscataway NJ, 2005. IEEE
Press.
2. T. B¨ack. Evolutionary Algorithms in Theory and Practice. Oxford University
Press, New York NY, 1996.
3. T. B¨ack and M. Sch¨utz. Evolution strategies for mixed-integer optimization of
optical multilayer systems.
In J. R. McDonnell, R. G. Reynolds, and D. B.
Fogel, editors, Evolutionary Programming IV: Proceedings of the Fourth Annual
Conference on Evolutionary Programming, pages 33–51. MIT Press, Cambridge,
MA, 1995.
4. Th. B¨ack. Self-adaptation in genetic algorithms. In FJ Varela and P Bourgine,
editors, Toward a Practice of Autonomous Systems: proceedings of the ﬁrst Eu-
ropean conference on Artiﬁcial Life, pages 263–271. MIT Press, 1992.
5. R. Barr and B. Hickman. Reporting computational experiments with parallel
algorithms: Issues, measures, and experts’ opinions. ORSA Journal on Comput-
ing, 5(1):2–18, 1993.
6. Thomas Bartz-Beielstein. Evolution strategies and threshold selection. In M. J.
Blesa Aguilera, C. Blum, A. Roli, and M. Sampels, editors, Proceedings Second
International Workshop Hybrid Metaheuristics (HM’05), volume 3636 of Lecture
Notes in Computer Science, pages 104–115, Berlin, Heidelberg, New York, 2005.
Springer.
7. Thomas Bartz-Beielstein.
Experimental Research in Evolutionary Computa-
tion—The New Experimentalism. Springer, Berlin, Heidelberg, New York, 2006.
8. Thomas Bartz-Beielstein, Daniel Blum, and J¨urgen Branke. Particle swarm op-
timization and sequential sampling in noisy environments. In Richard Hartl and
Karl Doerner, editors, Proceedings 6th Metaheuristics International Conference
(MIC2005), pages 89–94, Vienna, Austria, 2005.
9. Thomas Bartz-Beielstein, Marcel de Vegt, Konstantinos E. Parsopoulos, and
Michael N. Vrahatis.
Designing particle swarm optimization with regression
trees. Interner Bericht des Sonderforschungsbereichs 531 Computational Intel-
ligence CI–173/04, Universit¨at Dortmund, Germany, Mai 2004.
10. Thomas Bartz-Beielstein, Christian Lasarczyk, and Mike Preuß. Sequential pa-
rameter optimization. In B. McKay et al., editors, Proceedings 2005 Congress
on Evolutionary Computation (CEC’05), Edinburgh, Scotland, volume 1, pages
773–780, Piscataway NJ, 2005. IEEE Press.
11. Thomas Bartz-Beielstein and Boris Naujoks. Tuning multicriteria evolutionary
algorithms
for
airfoil
design
optimization.
Interner
Bericht
des
Sonder-
forschungsbereichs 531 Computational Intelligence CI–159/04, Universit¨at Dort-
mund, Germany, Februar 2004.
12. Thomas
Bartz-Beielstein,
Konstantinos
E.
Parsopoulos,
and
Michael
N.
Vrahatis. Design and analysis of optimization algorithms using computa-
tional statistics. Applied Numerical Analysis & Computational Mathematics
(ANACM), 1(2):413–433, 2004.
13. Thomas Bartz-Beielstein, Mike Preuß, and Sandor Markon. Validation and
optimization of an elevator simulation model with modern search heuristics. In

SPO Applied to Self-Adaptation for Binary-Coded EAs
115
T. Ibaraki, K. Nonobe, and M. Yagiura, editors, Metaheuristics: Progress as
Real Problem Solvers, Operations Research/Computer Science Interfaces, pages
109–128. Springer, Berlin, Heidelberg, New York, 2005.
14. Regina Berretta, Carlos Cotta, and Pablo Moscato. Enhancing the performance
of memetic algorithms by using a matching-based recombination algorithm. In
Metaheuristics: computer decision-making, pages 65–90. Kluwer Academic Pub-
lishers, Norwell, MA, 2004.
15. Regina Berretta and Pablo Moscato. The number partitioning problem: an open
challenge for evolutionary computation? In New ideas in optimization, pages
261–278. McGraw-Hill Ltd., Maidenhead, UK, 1999.
16. H.-G. Beyer and H.-P. Schwefel. Evolution strategies—A comprehensive intro-
duction. Natural Computing, 1:3–52, 2002.
17. Marcel de Vegt. Einﬂuss verschiedener Parametrisierungen auf die Dynamik
des Partikel-Schwarm-Verfahrens: Eine empirische Analyse.
Interner Bericht
der Systems Analysis Research Group SYS–3/05, Universit¨at Dortmund, Fach-
bereich Informatik, Germany, Dezember 2005.
18. Camil Demetrescu and Giuseppe F. Italiano. What do we learn from experi-
mental algorithmics? In MFCS ’00: Proceedings of the 25th International Sym-
posium on Mathematical Foundations of Computer Science, pages 36–51, Berlin,
Heidelberg, New York, 2000. Springer.
19. N. R. Draper and H. Smith. Applied Regression Analysis. Wiley, New York NY,
3rd edition, 1998.
20. S. Droste, T. Jansen, and I. Wegener. A natural and simple function which is
hard for all evolutionary algorithms. In IEEE International Conference on In-
dustrial Electronics, Control, and Instrumentation (IECON 2000), pages 2704–
2709, Piscataway, NJ, 2000. IEEE Press.
21. A. E. Eiben, R. Hinterding, and Z. Michalewicz. Parameter control in evolution-
ary algorithms. IEEE Transactions on Evolutionary Computation, 3(2):124–141,
1999.
22. A. E. Eiben and J. E. Smith. Introduction to Evolutionary Computing. Springer,
Berlin, Heidelberg, New York, 2003.
23. A.E. Eiben and M. Jelasity. A critical note on experimental research methodol-
ogy in EC. In Proceedings of the 2002 Congress on Evolutionary Computation
(CEC’2002), pages 582–587, Piscataway NJ, 2002. IEEE.
24. Robert Feldt and Peter Nordin. Using factorial experiments to evaluate the eﬀect
of genetic programming parameters. In Riccardo Poli et al., editors, Genetic
Programming, Proceedings of EuroGP’2000, volume 1802 of Lecture Notes in
Computer Science, pages 271–282, Berlin, Heidelberg, New York, 15-16 April
2000. Springer.
25. D. B. Fogel. Evolving Artiﬁcial Intelligence. PhD thesis, University of California,
San Diego, 1992.
26. O. Fran¸cois and C. Lavergne. Design of evolutionary algorithms—a statistical
perspective. IEEE Transactions on Evolutionary Computation, 5(2):129–148,
April 2001.
27. Mario Giacobini, Mike Preuß, and Marco Tomassini. Eﬀects of scale-free and
small-world topologies on binary coded self-adaptive CEA.
In 6th European
Conf. Evolutionary Computation in Combinatorial Optimization, Proc. (Evo-
COP’06), Lecture Notes in Computer Science, pages 86–98, Berlin, 2006.
Springer.

116
Mike Preuss and Thomas Bartz-Beielstein
28. D. E. Goldberg, K. Deb, and J. Horn. Massively multimodality, deception and
genetic algorithms.
In R. M¨anner and B. Manderick, editors, Parallel Prob.
Solving from Nature II, pages 37–46. North-Holland, 1992.
29. Garrison W. Greenwood. Adapting mutations in genetic algorithms using gene
ﬂow principles. In R. Sarker et al., editors, Proc. 2003 Congress on Evolution-
ary Computation (CEC’03), Canberra, pages 1392–1397, Piscataway NJ, 2003.
IEEE Press.
30. N. Hansen and A. Ostermeier.
Completely derandomized self-adaptation in
evolution strategies. Evolutionary Computation, 9(2):159–195, 2001.
31. N. Hansen, A. Ostermeier, and A. Gawelczyk. On the adaptation of arbitrary
normal mutation distributions in evolution strategies: The generating set adap-
tation. In L. J. Eshelman, editor, Proc. 6th Int’l Conf. on Genetic Algorithms,
pages 57–64, San Francisco, CA, 1995. Morgan Kaufmann Publishers, Inc.
32. J. Hooker.
Testing heuristics: We have it all wrong.
Journal of Heuristics,
1(1):33–42, 1996.
33. Christian Igel and Martin Kreutz. Operator adaptation in evolutionary compu-
tation and its application to structure optimization of neural networks. Neuro-
computing, 55(1-2):347–361, 2003.
34. E. H. Isaaks and R. M. Srivastava. An Introduction to Applied Geostatistics.
Oxford University Press, Oxford, U.K., 1989.
35. Thomas Jansen, Kenneth A. De Jong, and Ingo Wegener. On the choice of the
oﬀspring population size in evolutionary algorithms. Evolutionary Computation,
13(4):413–440, 2005.
36. Thomas Jansen and Dirk Sudholt. Design and analysis of an asymmetric muta-
tion operator. In B. McKay et al., editors, Proc. 2005 Congress on Evolutionary
Computation (CEC’05), Edinburgh, Scotland, volume 1, pages 190–197, Piscat-
away NJ, 2005. IEEE Press.
37. Mark Jelasity. A wave analysis of the subset sum problem. In Thomas B¨ack, ed-
itor, Proceedings of the Seventh International Conference on Genetic Algorithms
(ICGA97), pages 89–96, San Francisco, CA, 1997. Morgan Kaufmann.
38. M´ark Jelasity, Mike Preuß, and Agoston E. Eiben. Operator learning for a prob-
lem class in a distributed peer-to-peer environment. In J. J. Merelo Guerv´os,
P. Adamidis, H.-G. Beyer, J.L. Fern´andez-Villaca˜nas, and H.-P. Schwefel, ed-
itors, Parallel Problem Solving from Nature – PPSN VII, Proc. Seventh Int’l
Conf., Granada, pages 172–183, Berlin, 2002. Springer.
39. David S. Johnson. A theoretician’s guide to the experimental analysis of al-
gorithms. In M. H. Goldwasser, D. S. Johnson, and C. C. McGeoch, editors,
Data Structures, Near Neighbor Searches, and Methodology: Fifth and Sixth DI-
MACS Implementation Challenges, pages 215–250, Providence, 2002. American
Mathematical Society.
40. D.R. Jones, M. Schonlau, and W.J. Welch. Eﬃcient global optimization of ex-
pensive black-box functions. Journal of Global Optimization, 13:455–492, 1998.
41. Kenneth A. De Jong, Michael A. Potter, and William M. Spears. Using problem
generators to explore the eﬀects of epistasis. In T. B¨ack, editor, Proceedings of
the Seventh ICGA, pages 338–345. Morgan Kaufmann, 1997.
42. Bryant A. Julstrom. Adaptive operator probabilities in a genetic algorithm that
applies three operators. In SAC ’97: Proceedings of the 1997 ACM symposium
on Applied computing, pages 233–238, New York, NY, 1997. ACM Press.
43. J. P. C. Kleijnen. Statistical Tools for Simulation Practitioners. Marcel Dekker,
New York NY, 1987.

SPO Applied to Self-Adaptation for Binary-Coded EAs
117
44. J. P. C. Kleijnen. Experimental design for sensitivity analysis, optimization, and
validation of simulation models. In J. Banks, editor, Handbook of Simulation.
Wiley, New York NY, 1997.
45. F. Kursawe.
Grundlegende empirische Untersuchungen der Parameter von
Evolutionsstrategien – Metastrategien.
Dissertation, Fachbereich Informatik,
Universit¨at Dortmund, Germany, 1999.
46. A.M. Law and W.D. Kelton. Simulation Modeling and Analysis. McGraw-Hill,
New York NY, 3rd edition, 2000.
47. S.N. Lophaven, H.B. Nielsen, and J. Søndergaard. Aspects of the Matlab Tool-
box DACE. Technical Report IMM-REP-2002-13, Informatics and Mathemati-
cal Modelling, Technical University of Denmark, Copenhagen, Denmark, 2002.
48. S.N. Lophaven, H.B. Nielsen, and J. Søndergaard. DACE—A Matlab Kriging
Toolbox. Technical Report IMM-REP-2002-12, Informatics and Mathematical
Modelling, Technical University of Denmark, Copenhagen, Denmark, 2002.
49. Sandor Markon, Hajime Kita, Hiroshi Kise, and Thomas Bartz-Beielstein, edi-
tors. Modern Supervisory and Optimal Control with Applications in the Control
of Passenger Traﬃc Systems in Buildings. Springer, Berlin, Heidelberg, New
York, 2006.
50. Catherine C. McGeoch.
Experimental Analysis of Algorithms.
PhD thesis,
Carnegie Mellon University, Pittsburgh PA, 1986.
51. M. D. McKay, R. J. Beckman, and W. J. Conover.
A comparison of three
methods for selecting values of input variables in the analysis of output from a
computer code. Technometrics, 21(2):239–245, 1979.
52. J¨orn Mehnen, Thomas Michelitsch, Thomas Bartz-Beielstein, and Nadine
Henkenjohann. Systematic analyses of multi-objective evolutionary algorithms
applied to real-world problems using statistical design of experiments. In R. Teti,
editor, Proceedings Fourth International Seminar Intelligent Computation in
Manufacturing Engineering (CIRP ICME’04), volume 4, pages 171–178, Naples,
Italy, 2004.
53. J¨orn Mehnen, Thomas Michelitsch, Thomas Bartz-Beielstein, and Chris-
tian W. G. Lasarczyk. Multiobjective evolutionary design of mold temperature
control using DACE for parameter optimization. In H. Pf¨utzner and E. Leiss,
editors, Proceedings Twelfth International Symposium Interdisciplinary Electro-
magnetics, Mechanics, and Biomedical Problems (ISEM 2005), volume L11-1,
pages 464–465, Vienna, Austria, 2005. Vienna Magnetics Group Reports.
54. Silja Meyer-Nieberg and Hans-Georg Beyer.
Self-adaptation in evolutionary
algorithms. In Fernando Lobo, Claudio Lima, and Zbigniew Michalewicz, edi-
tors, Parameter Setting in Evolutionary Algorithms, Studies in Computational
Intelligence. Springer, Berlin Heidelberg New York, 2006.
55. D. C. Montgomery. Design and Analysis of Experiments. Wiley, New York NY,
5th edition, 2001.
56. B. M. E. Moret. Towards a discipline of experimental algorithmics. In M.H.
Goldwasser, D.S. Johnson, and C.C. McGeoch, editors, Data Structures, Near
Neighbor Searches, and Methodology: Fifth and Sixth DIMACS Implementa-
tion Challenges, DIMACS Monographs 59, pages 197–213, Providence RI, 2002.
American Mathematical Society.
57. Bernard M. Moret and Henry D. Shapiro. Algorithms and experiments: The new
(and old) methodology. Journal of Universal Computer Science, 7(5):434–446,
2001.

118
Mike Preuss and Thomas Bartz-Beielstein
58. R. Myers and E.R. Hancock. Empirical modelling of genetic algorithms. Evo-
lutionary Computation, 9(4):461–493, 2001.
59. Martin Pelikan, David E. Goldberg, and Erick Cant´u-Paz. Linkage problem,
distribution estimation, and bayesian networks.
Evolutionary Computation,
8(3):311–340, 2000.
60. F. Pukelsheim. Optimal Design of Experiments. Wiley, New York NY, 1993.
61. Iloneide C. O. Ramos, Marco C. Goldbarg, Elizabeth G. Goldbarg, and Adri˜ao
D. D´oria Neto. Logistic Regression for Parameter Tuning on an Evolutionary
Algorithm. In B. McKay et al., editors, Proc. 2005 Congress on Evolutionary
Computation (CEC’05), volume 2, pages 1061–1068, Piscataway NJ, 2005. IEEE
Press.
62. Ronald L. Rardin and Reha Uzsoy. Experimental evaluation of heuristic opti-
mization algorithms: A tutorial. J. Heuristics, 7(3):261–304, 2001.
63. I. Rechenberg.
Evolutionsstrategie: Optimierung technischer Systeme nach
Prinzipien der biologischen Evolution. Frommann-Holzboog Verlag, Stuttgart,
1973.
64. I. Rechenberg. Evolutionsstrategien. In B. Schneider and U. Ranft, editors, Sim-
ulationsmethoden in der Medizin und Biologie, pages 83–114. Springer-Verlag,
Berlin, 1978.
65. J. Sacks, W. J. Welch, T. J. Mitchell, and H. P. Wynn. Design and analysis of
computer experiments. Statistical Science, 4(4):409–435, 1989.
66. T. J. Santner, B. J. Williams, and W. I. Notz. The Design and Analysis of
Computer Experiments. Springer, Berlin, Heidelberg, New York, 2003.
67. J. D. Schaﬀer, R. A. Caruana, L. Eshelman, and R. Das.
A study of con-
trol parameters aﬀecting online performance of genetic algorithms for function
optimization. In J. D. Schaﬀer, editor, Proceedings of the Third International
Conference on Genetic Algorithms, pages 51–60, San Mateo CA, 1989. Morgan
Kaufman.
68. M. Schonlau.
Computer Experiments and Global Optimization.
PhD thesis,
University of Waterloo, Ontario, Canada, 1997.
69. Matthias Schonlau, William J. Welch, and Ronald R. Jones. Global versus local
search in constrained optimization of computer models. In New Development
and Applications in Experimental Design, number 34 in IMS Lecture Notes,
pages 11–25. Institute of Mathematical Statistics, Beachwood, OH, 1998.
70. Martin Sch¨utz.
Eine Evolutionsstrategie f¨ur gemischt-ganzzahlige Optimier-
probleme mit variabler Dimension. Interner Bericht der Systems Analysis Re-
search Group SYS–1/96, Universit¨at Dortmund, Fachbereich Informatik, Au-
gust 1996.
71. H.-P. Schwefel. Adaptive Mechanismen in der biologischen Evolution und ihr
Einﬂuß auf die Evolutionsgeschwindigkeit. Technical report, Technical Univer-
sity of Berlin, 1974. Abschlußbericht zum DFG-Vorhaben Re 215/2.
72. H.-P. Schwefel. Numerical Optimization of Computer Models. Wiley, Chichester,
1981.
73. H.-P. Schwefel, I. Wegener, and K. Weinert, editors.
Advances in Computa-
tional Intelligence—Theory and Practice.
Springer, Berlin, Heidelberg, New
York, 2003.
74. J. Smith and T. C. Fogarty. Self-adaptation of mutation rates in a steady state
genetic algorithm.
In Proceedings of 1996 IEEE Int’l Conf. on Evolutionary
Computation (ICEC ’96), pages 318–323. IEEE Press, NY, 1996.

SPO Applied to Self-Adaptation for Binary-Coded EAs
119
75. J. E. Smith. Modelling gas with self adaptive mutation rates. In L. Spector
and et al., editors, Proceedings of the Genetic and Evolutionary Computation
Conference (GECCO-2001), pages 599–606, San Francisco, California, USA, 7-
11 July 2001. Morgan Kaufmann.
76. Catalin Stoean, Mike Preuss, Ruxandra Gorunescu, and Dan Dumitrescu. Elitist
Generational Genetic Chromodynamics - a New Radii-Based Evolutionary Al-
gorithm for Multimodal Optimization. In B. McKay et al., editors, Proc. 2005
Congress on Evolutionary Computation (CEC’05), volume 2, pages 1839 – 1846,
Piscataway NJ, 2005. IEEE Press.
77. Ch. Stone and J. E. Smith. Strategy parameter variety in self-adaptation of
mutation rates. In W. B. Langdon and et al., editors, GECCO, pages 586–593.
Morgan Kaufmann, 2002.
78. Dirk Thierens.
An adaptive pursuit strategy for allocating operator proba-
bilities.
In GECCO ’05: Proceedings of the 2005 conference on Genetic and
evolutionary computation, pages 1539–1546, New York, NY, 2005. ACM Press.
79. Marko Tosic. Evolution¨are Kreuzungsminimierung. Diploma thesis, University
of Dortmund, Germany, January 2006.
80. Klaus Weinert, J¨orn Mehnen, Thomas Michelitsch, Karlheinz Schmitt, and
Thomas Bartz-Beielstein. A multiobjective approach to optimize temperature
control systems of moulding tools. Production Engineering Research and Devel-
opment, Annals of the German Academic Society for Production Engineering,
XI(1):77–80, 2004.
81. Darrell Whitley, Soraya B. Rana, John Dzubera, and Keith E. Mathias. Evalu-
ating evolutionary algorithms. Artiﬁcial Intelligence, 85(1-2):245–276, 1996.
82. D.H. Wolpert and W.G. Macready. No free lunch theorems for optimization.
IEEE Transactions on Evolutionary Computation, 1(1):67–82, 1997.

Combining Meta-EAs and Racing for Diﬃcult
EA Parameter Tuning Tasks
Bo Yuan and Marcus Gallagher
School of Information Technology and Electrical Engineering
University of Queensland, Qld. 4072, Australia
boyuan@itee.uq.edu.au, marcusg@itee.uq.edu.au
Summary. This chapter presents a novel framework for tuning the parameters of
Evolutionary Algorithms. A hybrid technique combining Meta-EAs and statistical
Racing approaches is developed, which is not only capable of eﬀectively exploring
the search space of numerical parameters but also suitable for tuning symbolic pa-
rameters where it is generally diﬃcult to deﬁne any sensible distance metric.
1 Introduction
One of the major issues in applying Evolutionary Algorithms (EAs) is how to choose
an appropriate parameter setting. The importance of parameter tuning is at least
threefold. Firstly, EAs are not completely parameter robust and may not be able
to solve challenging problems eﬀectively with inappropriate values. Secondly, when
two or more EAs are compared, arbitrarily speciﬁed parameter values may make the
comparison unfair and conclusions misleading. Finally, ﬁnding the optimal setting
may be also helpful for better understanding the behavior of EAs.
There has been a long history of work on ﬁnding the optimal parameter values
of EAs [11]. However, it has shown that this kind of optimal setting does not exist in
general and there are diﬀerent optimal values for diﬀerent problems [17]. To better
understand the relationship between parameter setting and performance, for each
pair of algorithm and problem, a performance landscape could be deﬁned with one
dimension for each parameter and an extra dimension for the performance. As a
result, the global optimum of this landscape corresponds to the optimal parameter
setting of the EA on that particular problem. Note that, the structure of such
landscapes is unlikely to be always trivial (e.g., monotonic or separable).
One principled approach to parameter tuning is to exhaustively explore the
performance of an EA with systematically varied parameter values. Statistical tech-
niques such as ANalysis Of VAriance (ANOVA) could then be applied on the results
to formally investigate the inﬂuence and contribution of each parameter towards the
algorithm’s performance as well as the interaction among parameters [4, 5, 16], which
may be used as heuristic knowledge in choosing the appropriate parameter setting.
However, a large number of experiments are usually required to collect enough data
for such statistical analysis, which is ineﬃcient for the purpose of parameter tuning.
B. Yuan and M. Gallagher: Combining Meta-EAs and Racing for Diﬃcult EA Parameter Tun-
ing Tasks, Studies in Computational Intelligence (SCI) 54, 121–142 (2007)
www.springerlink.com
© Springer-Verlag Berlin Heidelberg 2007

122
Bo Yuan and Marcus Gallagher
Alternatively, mathematical modeling techniques such as Response Surfaces could
be employed to search for the optimum parameter setting by only testing a relatively
small number of parameter combinations [2].
If the performance landscape is suspected to be multimodal and there is no prior
knowledge to come up with a good starting position, global optimization methods
could be applied to conduct parameter tuning. A typical example is to use an exter-
nal EA, called a Meta-EA [1, 8] as the high-level optimization procedure to conduct
searching on the performance landscape. In these search-based methods, all para-
meters to be tuned are usually encoded into a vector and each instantiation of
this vector/individual/algorithm instance corresponds to a fully-speciﬁed EA. Con-
sequently, the process of parameter tuning is to ﬁnd the individual or algorithm
instance that yields the best performance.
It is worth mentioning that, in typical parameter tuning, algorithm parameters
are chosen in advance and remain ﬁxed during evolution. Another branch of research
is parameter control in which algorithm parameters are allowed to vary with time
as the solution vectors of the objective function are optimized [7]. The advantage
is that diﬀerent parameter values may be needed in diﬀerent stages of evolution
in order to achieve optimal performance. However, there are still some exogenous
parameters used to control those self-adaptive parameters. Furthermore, not all
algorithm parameters can be conveniently adapted in this manner. After all, ﬁnding
the optimal settings in diﬀerent situations may also provide valuable information
for designing better parameter control strategies.
The major contributions of this chapter are listed as follows. Firstly, the proper-
ties of traditional search-based methods are given a critical analysis. We point out
that, in addition to being very time-consuming in evaluating each candidate, they
have inherent diﬃculty in tuning nominal or symbolic parameters where it is very
hard to deﬁne any sensible distance metric over the parameter space. For these para-
meters, no search-based methods could be expected to be better than the exhaustive
or random search.
Secondly, a novel statistical technique called Racing [12], which is originally
proposed to solve the model selection problem in Machine Learning, is introduced
to reliably choose the best parameter setting. The advantage of Racing is that it
does not require a well-deﬁned parameter space but is still much more eﬃcient than
the exhaustive search.
Finally, with a good understanding of the properties of the two distinct classes
of tuning techniques, a framework of hybridization is proposed, which is not only
able to handle symbolic parameters but also to search and explore the parameter
space eﬃciently. Two case studies on tuning the parameters of a Genetic Algorithm
(GA) are also conducted to highlight the eﬀectiveness of the proposed techniques.
2 The Challenge of Symbolic Parameters
As mentioned above, in search-based methods, the EA to be tuned is often encoded
as a vector and each individual/algorithm instance corresponds to a unique point
in the parameter/search space. The major reason that these search-based methods
can be expected to outperform random or exhaustive search is largely due to the
assumption of the existence of some regular structure in the performance landscape.

Combining Meta-EAs and Racing
123
In other words, there should be some kind of “path” in the landscape that could be
exploited by the search operators to eﬀectively drive the overall tuning process.
Fig. 1. An example of the tuning process conducted by a hill-climbing method.
Figure 1 shows the tuning process conducted by a hill-climbing method working
on a single parameter. After evaluating the two neighbors A and B of the current
individual, it turns out that A is better than B as well as the current individual. As
a result, in the next step, the hill-climbing method will move its current position to
A (i.e., towards the optimum).
Although this example is very simple, it does reveal a key factor of all non-trivial
search-based methods: a meaningful distance metric among individuals, which could
create a performance landscape compatible with the search operators. In general, the
distance metric in the search space (i.e., the spatial relationship among parameter
settings) is determined by the way that parameters are encoded. For example, if the
continuous crossover rate and mutation rate parameters are directly encoded using
their original data type, it would be reasonable to say that two algorithm instances
similar to each other in terms of these two parameters are also close to each other
in the search space deﬁned by the Euclidean distance. However, if they are encoded
into binary strings, these two algorithm instances may be quite distant from each
other in terms of the Hamming distance.
The major issue here is that EAs with similar parameter settings should be
represented by individuals that are also close to each other in the search space.
Note that the actual distance between two individuals in terms of searching is also
inﬂuenced by the speciﬁc search operator in use [10]. If the distance metric does not
match the similarity among algorithm instances, the ﬁtness values/performance of
individuals close to each other in the corresponding search space may not be closely
correlated.
From the landscape point of view, this means that the performance landscape
may present a signiﬁcant level of noisy structure, which could reduce the eﬀectiveness

124
Bo Yuan and Marcus Gallagher
of almost all search-based methods. In fact, ﬁnding the appropriate encoding scheme
is also a well-known common issue in the ﬁeld of Evolutionary Computation.
Unfortunately, unlike those numerical parameters such as population size and
mutation rate, some parameters are nominal and only have symbolic meanings,
which cannot be encoded in a natural way. For instance, an EA could adopt one of
many diﬀerent selection strategies but it is hard to design a meaningful encoding
scheme for this parameter. The reason is that there is usually no clear knowledge
about the relationship among these selection strategies and it is diﬃcult to quantify
the distance between each pair of them (e.g., the distance between the Tournament
selection and the Truncation selection). In fact, it is even diﬃcult to say which one
is more similar to which one in some general sense. Certainly, it is always possible
to apply any arbitrary encoding scheme on this parameter but the shape of the
performance landscape could also be arbitrary, which can hardly be guaranteed to
be solved eﬃciently by a given search-based method.
Suppose that there are 10 candidate selection strategies and the performance of
the EA with each strategy is shown in Figure 2 (top), labeled according to a certain
encoding scheme compatible with their inherent similarity. Note that this is an
idealized situation and if an arbitrary encoding scheme is applied, selection strategies
that produce similar performance could be mapped into quite diﬀerent individuals
(i.e., large distances along the horizontal axis). As a result, the original smooth
landscape may turn into something pretty nasty as shown in Figure 2 (bottom).
1
2
3
4
5
6
7
8
9
10
0
0.2
0.4
0.6
0.8
1
Selection Strategies
Performance
3
1
5 
2
8
4
7 
10
6
9
0
0.2
0.4
0.6
0.8
1
Selection Strategies
Performance
Fig. 2. The inﬂuence of the encoding scheme on the performance landscape: A
smooth landscape deﬁned in consistence with the similarity of selection strategies
(top) and a nasty landscape deﬁned by reordering the sequence (bottom).
More speciﬁcally, in Figure 2 (top), selection strategies close to each other usu-
ally result in similar algorithm performance and the global optimum could be easily
reached by the hill-climbing method starting from any position. By contrast, in Fig-

Combining Meta-EAs and Racing
125
ure 2 (bottom), although the algorithm’s performance with each selection strategy is
kept unchanged, the spatial relationship has been altered. As a result, the previous
smoothness is broken and the new landscape contains multiple local optima.
There are further challenges associated with applying search-based methods to
parameter tuning as shown in previous research. Firstly, it is possible that some
parameters only play a role in certain situations. As a result, the encoding scheme
may have to contain some redundancies and the tuning algorithms also need to be
speciﬁcally customized to handle these parameters. Secondly, in order to evaluate
each individual, the corresponding EA must be run for several trials in order to
get reliable performance. However, in practice, it would often require an enormous
amount of computational time and may become impractical in many cases.
3 Racing and Model Selection
Racing was originally proposed to eﬃciently solve the model selection problem in
Machine Learning [12, 13]. Typically, the performance of a group of M candidate
supervised learning models is evaluated on a data set of interest containing N test
points using a validation approach. The general mechanism of the leave-one-out cross
validation (LOOCV) is to use all but a single test point to train the model. After
training, the model is tested on the test point absent from the training. This process
is repeated N times with each time a unique test point left out and the model with
the lowest average error is regarded as the best model. In other words, the perfor-
mance of each model is determined by the mean of its performance distribution,
which consists of the outputs from a series of experiments.
The most straightforward approach would be to exhaustively test each model on
all available test points. With this brute force method, after ﬁnishing N experiments,
the true performance Etrue of a certain model could be found and after totally M ×N
experiments, it will guarantee to ﬁnd the best model according to the LOOCV
measure. However, the complexity of this method is O(M · N), which may become
quite time consuming for large M and/or N values. After all, each experiment may
involve quite heavy computation as the model needs to be trained on N −1 test
points.
By contrast, Racing techniques approach this model selection problem from a
diﬀerent angle: whether it is necessary to test a model for N times? Certainly, in
principle, each model must undertake all tests to reveal its Etrue. However, the
ultimate goal here is not to examine Etrue but to ﬁnd the best model. From a
statistical point of view, the errors of a model on all N test points create a population
and it is possible to estimate the population mean Etrue based on Esam, which is
the mean of a random sample of size N ′ (i.e., N ′ < N). Usually, it is more helpful
to estimate the true diﬀerence between two models based on the diﬀerence of their
sample means.
The basic idea behind Racing (Table 1) is to test all available models in parallel
on a single unseen test point at each step. The errors of each model on all test
points that have been selected are maintained and used in some statistical test.
If there is enough evidence, as indicated by the result of the statistical test, that
model i is signiﬁcantly worse than another model, it will be removed from the
model selection process and undertake no further testing because it is unlikely to
be the best model. By doing so, models are forced to race against each other and

126
Bo Yuan and Marcus Gallagher
only promising ones could survive through to the next iteration. As a result, the
overall computational cost could be reduced compared to the brute force approach
described above by avoiding running unnecessary experiments on inferior models.
Figure 3 gives an illustration of a virtual Racing experiment with 50 starting models
Table 1. The framework of Racing for model selection problems.
Repeat steps 1-4 until there is no more unseen data point or only
one model is remaining:
1. Randomly select a new data point
2. Calculate the LOOCV error of each remaining model on it
3. Apply some statistical test to conduct pair-wise comparison
4. Remove models that are signiﬁcantly worse than others
5. Return the remaining model with the lowest average error
and totally 200 test points where the length of each bar indicates when a model was
removed from the Racing process. It is clear that after being tested on 20 test points
quite a few models had already been removed and only 12 models could pass through
60 iterations. There were only two models (i.e., No. 34 and No. 47) left after 102
iterations and model No. 34 won the competition against No. 47 after being tested
on additional 24 test points and thus the Racing process was terminated. Note that,
in practice, it is also possible that some models are indistinguishable even at the
end of Racing.
0
20
40
60
80
100
120
140
160
180
200
5
10
15
20
25
30
35
40
45
50
Iterations
Models
Fig. 3. An illustration of a virtual Racing experiment with 50 candidate models and
200 test points in the dataset. It shows the number of iterations that each model
has gone through before being removed from the Racing process.

Combining Meta-EAs and Racing
127
The advantage of Racing is obvious from the above example: there is no need to
test all models thoroughly on 200 test points and most of the models were actually
tested on far less number of test points before being discarded. The cost of Racing
is indicated by the area of the bars (i.e., 2,264 tests), compared to the cost of the
brute force method, which is equal to the entire box area (i.e., 50 × 200=10,000
tests). The heart of the Racing technique, as shown in Table 1, is the statistical test
used to judge whether one model is signiﬁcantly worse than another, which directly
determines the eﬃciency and reliability of Racing. Depending on the properties
of the performance distributions, there are two major tests that have been used
in previous work: the non-parametric Hoeﬀding’s bounds [9] and the parametric
Student-t test [14]. For a single model, suppose its errors are bounded within [b, a].
Then, for a −b > ξ > 0, the probability of Esam being more than ξ away from Etrue
after n iterations is:
P(|Esam −Etrue| ≥ξ) ≤2e
−2nξ2
(a−b)2
(1)
Based on Eq. 1, it is easy to derive the value of ξ for a given signiﬁcance level α:
ξ =
	
(a −b)2 log(2/α)
2n
(2)
Fig. 4. An example of Hoeﬀding Racing at a speciﬁc iteration with 5 remaining
models left. The current threshold is indicated by the dashed line.
According to Eq. 2, at each step, the upper and lower boundary of Etrue is
estimated for each remaining model: [Esam −ξ, Esam + ξ]. The eliminating rule is to
remove models with lower boundaries (i.e., best possible errors) still higher than the
upper boundary (i.e., worst possible error) of the best model [12]. Figure 4 shows
the competition among ﬁve models at a certain stage of Racing where the dashed

128
Bo Yuan and Marcus Gallagher
line is the threshold above which all models are to be eliminated (e.g., the second
rightmost model).
In addition to Eq. 2, it would also be possible to utilize another Hoeﬀding’s
bound for the diﬀerence of two sample means with regard to the diﬀerence of the
true means [9]:
P(|E1
sam −E2
sam −(E1
true −E2
true)| ≥ξ) ≤2e
−nξ2
(a−b)2
(3)
The major advantage of Hoeﬀding’s bound is that it has no assumption on the
underlying probability distribution and can be applied in a variety of situations.
However, the bounds acquired are typically not very tight and consequently quite a
lot of test points may be needed in order to distinguish models [18].
If it is reasonable to assume that the errors are approximately normally dis-
tributed, the parametric Student-t test is usually more powerful. Under the null
hypothesis that there is no diﬀerence between the true means and the assumption
of equal variances, the test statistic is given by:
z = E1
sam −E2
sam
s

2/n
(4)
In Eq. 4, s is the pooled estimator of the standard deviation σ and the test
statistic z follows the Student-t distribution with 2n −2 degrees of freedom. The
null hypothesis is to be rejected if the value of z falls into the critical region based
on the desired signiﬁcance level.
If there is reason to doubt the assumption of equal variances, the test statistic
is then given by (i.e., s1 and s2 are the estimators of σ1 and σ2):
z = E1
sam −E2
sam

(s2
1 + s2
2)/n
(5)
The corresponding degree of freedom is approximated by:
d.f. ≈(s2
1 + s2
2)2
s4
1+s4
2
n−1
(6)
Figure 5 shows an example with ﬁve remaining models at a certain stage of
Racing where the performance distributions are approximately normal but have
diﬀerent variances. In this case, based on the Student-t test, the model in the middle
is likely to be eliminated.
Note that in the Hoeﬀding Racing and the Student-t Racing with equal variances,
a model only needs to be tested against the currently best model because if it is not
signiﬁcantly worse than the best model, it cannot be eliminated by any other models.
However, in the Student-t Racing with unequal variances, a model that cannot be
eliminated by the best model with a large variance might be eliminated by a good
model with a small variance. As a result, each model needs to be compared against
all models that have better sample errors.
Furthermore, in Table 1, all remaining models are tested on the same data
point in each step. However, both statistical tests above assume that samples are
independently drawn from the populations and take no advantage of this paired
experiment. Nevertheless, for statistical tests with blocking design, this feature could

Combining Meta-EAs and Racing
129
0
Error
Models
Eliminated
Fig. 5. An example of Racing based on the Student-t test (5 remaining models).
be explicitly exploited to reduce the variance among test points and increase the
power of statistical tests.
In summary, the major advantage of Racing is that it only works on the statistics
of the experimental results instead of the structure of models and could be applied
in a much wider situation than many other methods.
4 Racing Evolutionary Algorithms
4.1 Overview
There is a clear similarity between the model selection problem in Machine Learning
and the task of parameter tuning in EAs. In each case, there is a meta-optimization
problem: to ﬁnd values for all of the adjustable parameters of the model or algo-
rithm in order to produce the best results when applied to the problem of interest.
More importantly, the performance of a model or an algorithm is often deﬁned as
the mean value of the outputs from a series of random experiments or trials, which
provides the basis for applying statistical tests. In fact, the set of EAs to be raced
do not necessarily need to be diﬀerent instances of the same EA. Hence, Racing can
be used in quite a general sense in an attempt to reduce the experimental eﬀort re-
quired whenever a comparison is needed over a range of experimental conﬁgurations
provided that the evaluation of each conﬁguration involves iterating over a number
of independent trials (restarts).
In addition to some early attempts in applying Racing in the ﬁeld of EAs [3, 19],
there are a few important diﬀerences between model selection and evaluation of EAs
that must be made clear.

130
Bo Yuan and Marcus Gallagher
Firstly, the number of test points in model selection problems is ﬁxed, which
means that it is always possible to ﬁnd the true performance of each model (i.e.,
assume that its behavior is not stochastic). However, for EAs, the number of trials
that could be conducted for each pair of algorithm and problem is unlimited, which
means that it is impossible to know the true performance of EAs. Since, in practice,
50 to 100 trials are usually believed to be suﬃcient to produce trustable results, it
is reasonable to put a similar upper boundary on the maximum number of trials in
Racing experiments. Otherwise, it may need an extremely long period to terminate.
For example, in scientiﬁc studies, it is preferable to compare the best model or
algorithm found by the brute force method and that found by Racing to show its
reliability (i.e., whether the best model/algorithm was eliminated incorrectly at some
stage). Although it is very straightforward in model selection as the performance is
deterministic, in EAs, the only claim that can be made is to say whether the top
EAs found by the brute force method (i.e., certainly it is based on a limited number
of trials and the results are still stochastic) are also available at the end of Racing.
Secondly, in model selection, all models could be tested on the same test point
at each time and the blocking techniques may improve the power of statistical tests.
However, in the experiments of EAs, the output from the ith trial of one EA typically
has nothing to do with that of another EA, which means that there is no explicit
correspondence/relationship between the results. Consequently, it is not beneﬁcial
to use any paired statistical tests in the comparison of EAs with regard to random
trials.
Thirdly, in some model selection problems, the error range is known in advance.
For example, in classiﬁcation problems, the model’s error at each test point is either
0 or 1, representing correct or incorrect classiﬁcation. As a contrast, it is usually
not easy to give a tight estimation of the range of the performance of EAs on test
problems, which may make it diﬃcult to apply the Hoeﬀding’s method.
Finally, the number of test points in a typical dataset could easily reach a few
hundreds or even thousands while the number of trials to be conducted in an EA
experiment is usually no more than 100. A major implication is that, in model
selection, it is acceptable to start Racing after a number of, say 30, test points have
been selected and this would not have a signiﬁcant impact on the eﬃciency of Racing
methods. The advantage is that, although it is usually not known in advance whether
the errors of those models are normally distributed, it is still reasonable to apply
those statistical tests based on the assumption of normality such as the Student-t
test as long as the sample size is relatively large. However, for EAs, this would make
Racing methods require at least 60% cost of the brute force method if the maximum
number of trials is 50.
4.2 Statistical Tests
Based on the above analysis, a good statistical test for racing EAs should be able
to work with small sample sizes and not rely on the assumption of normality. Tradi-
tionally, the nonparametric Wilcoxon rank-sum test is used as the alternative of the
Student-t test. However it still assumes that the two underlying populations follow
distributions with similar shapes, which is not always reasonable.
To overcome the diﬃculty faced by many existing statistical tests, a hypothesis
test based on the bootstrap method is explained next. The bootstrap method is a

Combining Meta-EAs and Racing
131
computationally intensive statistical technique based on the resampling of data [6].
It could be used to calculate many important statistics such as standard errors,
bias and conﬁdence intervals without the need of using complicated mathematical
models. When used in hypothesis testing, it could avoid the danger of making various
assumptions such as normality and equal variances required by other statistical tests.
The basic procedure of the bootstrap method is described in Figure 6. Given
a sample X containing n data points, B bootstrap samples of the same size are
generated by sampling from X with replacement. This means that X is regarded
as the whole population and each X∗is a sample from it (i.e., each data point
in X has equal probability to be selected). For each bootstrap sample X∗(i), the
bootstrap replication of the statistic of interest θ denoted by θ∗(i) is calculated from
a certain function S such as the mean or the median of the sample. Finally, all such
replications θ∗are combined together to, for example, calculate the standard error
of θ or create a bootstrap table like the Student-t table for estimating conﬁdence
intervals.
Fig. 6. The basic procedure of the bootstrap method.
In general, better estimation could be achieved with larger B values such as
B = 2000. Obviously, this requires much more computational resource compared to
classical statistical tests. As a result, in the following experiments, the bootstrap test
is applied on samples with less than 30 data points combined with the Student-t test
in other situations to try to ﬁnd a balance between reliability and computational
cost.
4.3 Racing Experiments
In this section, Racing is applied to the situation where researchers are interested in
ﬁnding out which algorithm out of a set of candidates performs best on a particular
benchmark problem. For this purpose, we chose the well-known Rastrigin function,

132
Bo Yuan and Marcus Gallagher
which is a multimodal problem (i.e., to be minimized) with the global optimum at
the origin:
f(X) = 10n +
n

i=1
(X2
i −10 cos(2πXi)), Xi ∈[−5, 10]
(7)
The algorithm to be tuned is a continuous EDA (Estimation of Distribution
Algorithm) based on Gaussian distributions, which is similar to RECEDA [15] but
with some additional features.
Table 2. The framework of the continuous EDA based on Gaussian distributions.
Initialize and evaluate the starting population P
while stopping criteria not met
1.
Select top n individuals Psel (Truncation selection)
2.
Fit a multivariate Gaussian G(µ, Σ) to Psel
µ = 1
n
n
i=1 Psel
Σ = 1
n(Psel −µ)T · (Psel −µ)
3.
Update µ towards the best individual Xbest
µ′ = (1 −α) · µ + α · Xbest
4.
Update Σ by an ampliﬁcation scalar γ
Σ′ = γ2 · Σ
5.
Sample a set of new individuals P ′ from G(µ′, Σ′)
6.
Evaluate individuals in P ′
7.
Choose the best individuals from the union of P and P ′ to form the
new population P
end while
A set of 5 × 4 × 3=60 candidate algorithms were created by systematically vary-
ing the following three parameters described in Table 2 (i.e., each of these algorithm
instances corresponds to a unique parameter setting):
• Selection ratio (step 1): [0.1, 0.3, 0.5, 0.7, 0.9]
• Learning rate (step 3): [0.0, 0.2, 0.5, 1.0]
• Ampliﬁcation scalar (step 4): [1.0, 1.5, 2.0]
Other experimental conﬁgurations were ﬁxed as: dimensionality = 5, population
size = 100, number of generations = 100 and maximum number of trials = 100. The
ﬁtness value of the best individual found in each trial was recorded as the perfor-
mance criterion.
In order to provide a benchmark for the performance of Racing, an exhaustive
experiment was conducted by running all 60 algorithms for 100 trials (i.e., 6,000
trials in total) and the overall performance (i.e., sorted based on the mean values)
is summarized in Figure 7 with error bars (i.e., one standard deviation above and
below the mean). It is possible to inspect these results visually and determine, for
example, which algorithms are most likely to ﬁnd good solutions. Note that there
is clear evidence that the variances tend to be diﬀerent, which violates the widely
adopted assumption of equal variances.

Combining Meta-EAs and Racing
133
0 
5 
10
15
20
25
30
35
40
45
50
55
60
0
5
10
15
20
25
Algorithms
Fitness Values
Fig. 7. The performance distributions of 60 candidates on Rastrigin’s function.
Using Racing techniques, we aim to make a similar observation at a fraction of
the computational eﬀort required for the above exhaustive experiment. The proce-
dure operates here by eliminating poorly performing algorithms on the basis of a
small number of restarts. The signiﬁcance level α was 0.05 and the Racing algorithm
was started after 5 trials, taking into account the limited maximum number of trials
(100).
Note that since the brute force search has already been conducted, in the Racing
experiment, the performance of an algorithm instance in each trial was retrieved as a
sample from the corresponding performance population without actually re-running
the EA (i.e., certainly this is only applicable for comparison purpose). Since the
sequence of such samples may have more or less impact on the performance of
Racing, the Racing experiment was repeated by 10 times with random sequences of
samples.
The average number of algorithm instances remaining at each step is shown in
Figure 8 from which it is clear that Racing eliminated a large portion of algorithms in
the early stage and usually there were only around 5 algorithm instances remaining
after 20 restarts. The average number of candidate algorithms left after 100 restarts
was 1.6 and sometimes Racing was terminated even without the need of fully testing
any candidate for 100 trials. The eﬃciency of Racing can be visually examined by
comparing the area under the curve and the area of the whole rectangular region.
From a quantitative point of view, the average cost of Racing is 10.89% of the cost
of the brute force method in terms of the number of trials that have actually been
conducted. In other words, Racing reduced the cost by almost 90%, which could be
very signiﬁcant for expensive experiments. Certainly, the actual eﬃciency of Racing
depends on the properties of the speciﬁc performance distributions encountered.
Another important performance metric of Racing is reliability: whether the top
algorithms could be preserved and avoid being removed incorrectly. In the above

134
Bo Yuan and Marcus Gallagher
0
10
20
30
40
50
60
70
80
90
100
0
10
20
30
40
50
60
Restarts
Number of Algorithms
Fig. 8. The average number of remaining algorithm instances during Racing. The
cost of Racing is indicated by the area below the curve.
Racing experiments, the best algorithm based on the exhaustive experiment was
remaining till the end of Racing in 9 out of 10 times, showing pretty impressive
robustness. After all, for the single Racing experiment in which the best problem
was eliminated, three other top algorithms were still able to survive through to the
end of Racing. If greater reliability is desired, a higher signiﬁcance level could be
used to make Racing more conservative in eliminating models. Furthermore, Racing
could be allowed to start after more samples have been collected in order to improve
the accuracy of estimation.
5 Hybrid Techniques
5.1 Racing vs. Meta-EA
As shown in the last section, Racing does not work on the parameters of EAs in the
way that a search-based method might. Instead, Racing is based on the statistical
comparison of diﬀerent algorithm instances, which are regarded as atomic black
boxes (i.e., only the performance evaluation of the instances is required). By doing
so, the requirement of deﬁning an appropriate encoding scheme or distance metric
is removed. However, a closer look at Racing reveals that there is an inherent lack
of any mechanism of exploration. In fact, Racing always starts with a ﬁxed set of
candidates and no other candidates can be reached. If Racing is to be applied to the
task of parameter tuning, the optimal parameter setting must be included in the
initial set. Otherwise, there is no hope to ﬁnd it by Racing.
Unfortunately, there is usually no such guarantee in practice. One solution would
be to start with an exhaustive set containing all possible combinations of param-
eter values. However, for continuous parameters, the candidate set might be very

Combining Meta-EAs and Racing
135
large assuming certain discretization of these parameters, which makes this approach
impractical. Also, the accuracy of searching may also suﬀer accordingly.
In summary, the major challenge faced by all search-based methods is the diﬃ-
culty in deﬁning a distance metric over those nominal/symbolic parameters. Without
a sensible distance metric, the resulting landscape may present high level irregularity
and no search algorithm is expected to do well on it. For this reason, these para-
meters are generally referred as non-searchable parameters. Another practical issue
is that in search-based methods each individual is to be evaluated independently of
others and need to be tested thoroughly on the benchmark problem, just like the
brute force method. If the size of the search space is relatively small, the advan-
tage of search-based methods may not be signiﬁcant as they still need to evaluate
a large portion of all possible candidates (i.e., this is particularly true for those
population-based methods such as Meta-EAs).
By contrast, the diﬃculty in Racing is mainly due to the ﬁxed initial set of
algorithm instances. If the number of all possible candidates is not too large, say
a few hundreds, Racing is expected to work reasonably well. If the number is far
beyond this ﬁgure, it would be diﬃcult to apply Racing directly on such a huge
set of candidates (e.g., even testing one million candidates for a single trial would
require an enormous amount of time). In this situation, it is only practical to require
Racing to work on a small number of candidates at each time.
5.2 Hybridization
It is easy to see that the properties of Racing and search-based methods are com-
plimentary to each other. In order to eﬃciently handle situations where the search
space is potentially very large and/or contains symbolic parameters, it is useful to
consider a hybrid tuning framework combining Racing and search-based methods.
In this section, two examples are given to demonstrate how this hybrid approach
works.
In the ﬁrst scheme, a relatively simple situation is considered where a (1+λ)
ES, which is a special case of EAs, is combined with Racing and it is assumed that
all parameters to be tuned are numerical and thus searchable. At each step, a set
of λ new individuals/algorithm instances are sampled from a Gaussian distribution
centered at the current solution.
The major computational cost in this method is on ﬁnding the best individ-
ual from a set of candidates, which is exactly what Racing is designed for. In the
proposed hybrid approach, instead of sequentially evaluating each individual, all in-
dividuals are brought into Racing to ﬁnd out the best one and as shown in previous
experiments, it is expected that many of them may be eliminated after just a few
trials, saving a large portion of computational resource (Figure 9).
The second scheme is proposed to handle the more general and complicated
situation where parameters to be tuned include symbolic ones, which are generally
not searchable. Also, it is assumed that a more general and population-based Meta-
EA is in use.
The general idea is to treat these two classes of parameters separately: only
searchable parameters are encoded into individuals in the population of the Meta-
EA and each individual no longer corresponds to a fully speciﬁed algorithm instance.
Instead, it represents a set of complete EAs sharing the same searchable parame-
ters and diﬀerent from each other on those non-searchable parameters. Note that

136
Bo Yuan and Marcus Gallagher
Fig. 9. The framework of the hybrid (1+λ) ES + Racing scheme.
the size of the set depends on the number of non-searchable parameters and their
cardinalities. In order to evaluate each individual, Racing is applied to ﬁnd the best
algorithm instance from the corresponding set of EAs and its performance is re-
turned as the ﬁtness of the individual (Figure 10). An alternative approach would
be to apply the brute force method to evaluate those algorithm instances, which
may be very time-consuming.
Fig. 10. The framework of the hybrid Meta-EA + Racing scheme.

Combining Meta-EAs and Racing
137
5.3 Case Studies
In this section, a standard GA with binary representation is used in the experi-
ments to illustrate the hybrid approach. Two parents from the current population
are selected at each time and two oﬀspring are generated through recombination
with probability Pc (the crossover rate). Otherwise, these two parents are kept un-
changed and copied to the mating pool. When the mating pool is full, mutation is
applied, which changes the value of each bit by ﬂipping it from 0 to 1 and vice versa
with probability Pm. Finally, new individuals are evaluated and replace the old pop-
ulation. If elitism is applied, the best individual found so far in previous generations
will be copied to the new population, replacing a randomly chosen individual.
There are a number of parameters to be speciﬁed. The population size (P) is
a discrete parameter of high cardinality (i.e., it should be a positive even number
because individuals are generated in pairs). It is well-known that a very small popu-
lation may cause premature convergence while a very large one may result in a slow
convergence rate. The crossover rate Pc and mutation rate Pm are both continuous
parameters within [0, 1], which are important for controlling the balance between
exploration and exploitation. The selection strategy (S) is a symbolic parameter,
which contains various candidates (e.g., Tournament Selection) combined with their
corresponding parameters (e.g., Tournament size). Also, a symbolic parameter (C)
is required to specify the type of crossover (e.g., one-point or two-point). At last, a
binary parameter (E) is used to indicate whether elitism is used or not. The feasible
values of S, C & E are given in Table 3.
Table 3. Feasible values of S, C & E.
Parameters
Values
S
“Truncation” 0.1, 0.2, 0.3, 0.4, 0.5
“Tournament” 2, 4, 6
“Linear Ranking” 1.5, 1.8, 2.0
C
“One-Point”, “Two-Point”, “Uniform”
E
“0” (without elitism), “1” (with elitism)
According to the above discussion, the GA could be fully speciﬁed by a six-
element tuple: < P, Pc, Pm, S, C, E >. Note that some parameters are continuous
(i.e., the population size could be regarded as a continuous parameter during op-
timization due to its high cardinality and then rounded up to the nearest feasible
value) while others are symbolic, which created a mixed-value optimization problem.
In the ﬁrst case study, a simple situation was considered in which only P, Pc and
Pm were to be tuned while the rest of the parameters were set to some ﬁxed values. In
this experiment, although all parameters were searchable, evaluating each individual
using the brute force method could still be very time-consuming. As a result, the
hybrid (1+λ) ES+Racing approach was applied to help reduce this computational
burden.

138
Bo Yuan and Marcus Gallagher
The (1+λ) ES in use was based on a ﬁxed Gaussian distribution with a diagonal
covariance matrix. The boundaries of the search space and the standard deviations
are given in Table 4, which were chosen based on some general knowledge without
any speciﬁc tuning. Note that for some values of the population size, it is possible
that, given the ﬁxed number of ﬁtness evaluations (i.e., 500), the number of gener-
ations may not be an integer. The solution adopted was to increase the population
size in the ﬁnal generation to accommodate those extra individuals to maintain the
same number of ﬁtness evaluations among all candidates.
Table 4. Deﬁnition of search space and standard deviations.
Range
Standard Deviation
P [20, 100]
5
Pc
[0, 1]
0.1
Pm [0, 0.2]
0.02
The 100-bit One-Max problem was used as the benchmark problem. The (1+λ)
ES with λ = 20 started from the middle of the search space while other GA pa-
rameters were arbitrarily set to (i.e., not necessarily a good choice) ⟨“Truncation”,
0.5, “One-Point”, 1⟩. Note that, in addition to identifying the best candidate, its
performance/ﬁtness value is usually needed in search-based methods. As a result,
the starting individual was evaluated exhaustively. The best candidate found by
Racing (i.e., signiﬁcance level 0.10) was also required to go through all trials (100)
to ﬁnd out its mostly reliable performance for the comparison against the currently
best one even if it was the only one left.
The parameter settings found during evolution in a typical trial (i.e., 20 gener-
ations) and the corresponding performance of the GA are shown in Figure 11. It is
clear that this approach could ﬁnd much better parameter settings than the initial
one and the average performance of the GA increased from 71.51 to 83.97 within 15
generations.
In the meantime, all three parameters under tuning showed a clear pattern. The
population size dropped from 60 to 20 while the crossover rate increased from 0.5
to 1.0. The mutation rate also quickly settled down to a very small value around
0.02. Since there is no dependence among the variables of the One-Max problem and
it has a simple structure with only one optimum, it seems unnecessary to employ
a large population to maintain the genetic diversity and a small mutation rate is
well justiﬁed in this case. As for the crossover rate, it plays a major role in GAs in
combining building blocks and a quite large value is often recommended.
Obviously, the hybrid approach did a good job in this case study and found some
very good parameter settings for the GA. After all, it only took around 13.4% of the
cost of an ordinary (1+λ) ES (i.e., individuals are exhaustively evaluated) in terms
of the number of trials conducted. Note that the lower limit of the cost of Racing
in this experiment is around 9.75% as all 20 candidates needed to be tested for at
least 5 trials and the best one needed to be fully tested for 100 trials.
Next, we will show how the GA’s performance can be further improved through
the simultaneous tuning of all six parameters.

Combining Meta-EAs and Racing
139
0
5
10
15
20
70
75
80
85
Generations
Fitness Values
0
5
10
15
20
20
25
30
35
40
45
50
Generations
Population Sizes
0
5
10
15
20
0.2
0.4
0.6
0.8
1
Generations
Crossover Rates
0
5
10
15
20
0
0.02
0.04
0.06
0.08
0.1
Generations
Mutation Rates
Fig. 11. The evolution process of the hybrid (1+λ) ES + Racing scheme.
In the second case study, a (µ+λ) ES with µ = λ = 20 was employed as the Meta-
EA (i.e., the initial population was randomly generated) in which each individual
consisted of three parameters P, Pc and Pm and represented a total of 66 algorithm
instances (i.e., 11 selection strategies × 3 crossover operators × 2 options of elitism).
Note that although E is a binary parameter, it was still included in the Racing
so that the Meta-EA only needed to handle a continuous optimization problem.
As a result, the role of Racing here was to, for each individual, ﬁnd out the best
algorithm instance out of a set of 66 candidates as eﬃciently as possible and return
its performance as the ﬁtness value of that individual. Also, the best algorithm was
to be fully tested even if it was the only one left.
Figure 12 presents the evolution process of this hybrid scheme in which all results
were averaged over the population. It is clear that much better parameter settings
were found while the superiority of small population sizes, large crossover rates
and small mutation rates was again evident. Furthermore, the uniform crossover
dominated the ﬁnal population while a strong selection pressure was created by
either the Tournament selection with tournament size 6 or the Truncation selection
with selection ratio 0.2. As to Elitism, it was not shown to be beneﬁcial in this case.
The cost of Racing was only around 7.4% of the cost of the brute force method, had
it been applied to ﬁnd the best algorithm instances.
6 Summary
In this Chapter, the major issue addressed is how to eﬃciently ﬁnd the best parame-
ter settings for EAs through experimental studies, assuming no speciﬁc knowledge
on those parameters. Two classes of tuning techniques: search-based methods and

140
Bo Yuan and Marcus Gallagher
0
5
10
15
20
75
80
85
90
95
Generations
Fitness Values
0
5
10
15
20
20
30
40
50
60
Generations
Population Sizes
0
5
10
15
20
0.5
0.6
0.7
0.8
0.9
1
Generations
Crossover Rates
0
5
10
15
20
0
0.02
0.04
0.06
0.08
0.1
Generations
Mutation Rates
Fig. 12. The evolution process of the hybrid Meta-EA + Racing scheme.
statistical Racing techniques, which are based on distinctly diﬀerent mechanism, are
given a critical review on their strengths and weaknesses.
We point out that the applicability of Racing is inherently restricted by its lack
of exploration capability while it could be very diﬃcult for typical search-based
methods such as Meta-EAs to eﬃciently handle nominal parameters that only have
symbolic meanings. In order to take the advantages of both methods, two hybrid
schemes are proposed in which search-based methods are responsible for exploring
the parameter space while Racing is applied, in the place of the brute force method,
to signiﬁcantly reduce the cost associated with ﬁnding the best performing algorithm
instance from a moderate size of candidates.
In the case studies, Racing has shown to be able to reduce the cost of the brute
force method by around 90% while maintaining quite high reliability. The major
direction for future work would be to establish some theoretical framework for Rac-
ing in order to conduct principled analysis of the inﬂuence of various experimental
factors. It is also an important topic to investigate the possibility of combining more
features into Racing from other tuning techniques.
Finally, some rules of thumb are given as follows to assist in choosing appropriate
parameter tuning techniques:
1. If the size of the parameter space is small (e.g., ten candidates or even less), it
is usually straightforward and reliable to conduct an exhaustive search to ﬁnd
the best parameter setting.
2. If the size of the parameter space is moderate (e.g., up to a few hundreds of
candidates), Racing could be applied to signiﬁcantly reduce the computational
time required, regardless of whether the parameter space is searchable or not.
3. If the sizes of the searchable parameter space and non-searchable parameter
space are both large, it is worthwhile to consider the proposed scheme of Meta-
EA + Racing.

Combining Meta-EAs and Racing
141
4. If the size of the searchable parameter space is large while there is no nominal
parameter or the size of the non-searchable parameter space is small, it is pos-
sible to use Meta-EAs only (i.e., apply random/exhaustive search on nominal
parameters if necessary).
5. An exception of rule No. 4: if the Meta-EA in use is similar to the (1 + λ) or
(1, λ) ES, which requires choosing the best individual from a set of candidates,
it is still possible to consider the scheme of Meta-EA + Racing.
References
1. T. B¨ack. Evolutionary Algorithms in Theory and Practice. Oxford University
Press, New York, 1996.
2. T. Bartz-Beielstein. Experimental analysis of evolution strategies - overview
and comprehensive introduction. Technical Report Reihe CI 157/03, SFB 531,
Universit¨at Dortmund, 2003.
3. M. Birattari, T. Stutzle, L. Paquete, and K. Varrentrapp. A racing algorithm
for conﬁguring metaheuristics. In Genetic and Evolutionary Computation Con-
ference (GECCO 2002), pages 11–18, 2002.
4. P. A. Castillo-Valdivieso, J. J. Merelo, and A. Prieto. Statistical analysis of the
parameters of a neuro-genetic algorithm. IEEE Transactions on Neural Net-
works, 13(6):1374–1393, 2002.
5. A. Czarn, C. MacNish, K. Vijayan, B. Turlach, and R. Gupta. Statistical ex-
ploratory analysis of genetic algorithms. IEEE Transactions on Evolutionary
Computation, 8(4):405–421, 2004.
6. B. Efron and R. J. Tibshirani. An Introduction to the Bootstrap. Chapman and
Hall, 1993.
7. A. E. Eiben, R. Hinterding, and Z. Michalewicz. Parameter control in evolution-
ary algorithms. IEEE Transactions on Evolutionary Computation, 3(2):124–141,
1999.
8. J. J. Grefenstette. Optimization of control parameters for genetic algorithms.
IEEE Transactions on System, Man, And Cybernetics, 16(1):122–128, 1986.
9. W. Hoeﬀding. Probability inequalities for sums of bounded random variables.
Journal of the American Statistical Association, 58(301):13–30, 1963.
10. T. Jones. Evolutionary Algorithms, Fitness Landscapes and Search. PhD thesis,
University of New Mexico, 1995.
11. K. De Jong.
The Analysis of the Behavior of a Class of Genetic Adaptive
Systems. PhD thesis, University of Michigan, 1975.
12. O. Maron and A. W. Moore.
Hoeﬀding races: accelerating model selection
search for classiﬁcation and function approximation.
In Advances in Neural
Information Processing Systems 6, pages 59–66, 1994.
13. O. Maron and A. W. Moore. The racing algorithm: model selection for lazy
learners. Artiﬁcial Intelligence Review, 11(1):193–225, 1997.
14. D. S. Moore. Introduction to the practice of statistics. W. H. Freeman, New
York, 4th edition, 2003.
15. T. K. Paul and H. Iba. Real-coded estimation of distribution algorithm. In Pro-
ceedings of the 5th Metaheuristics International Conference (MIC2003), pages
61–66, 2003.

142
Bo Yuan and Marcus Gallagher
16. I. Rojas, J. Gonzalez, H. Pomares, J. J. Merelo, P.A. Castillo, and G. Romero.
Statistical analysis of the main parameters involved in the design of a ge-
netic algorithm. IEEE Transactions on Systems, Man, And Cybernetics-Part
C, 32(1):31–37, 2002.
17. D. H. Wolpert and W. G. Macready. No free lunch theorems for optimization.
IEEE Transactions on Evolutionary Computation, 1(1):67–82, 1997.
18. F. Y-H. Yeh and M. Gallagher.
An empirical study of heoﬀding racing for
model selection in k-nearest neighbor classiﬁcation. In J. Hogan M. Gallagher
and F. Maire, editors, Sixth International Conference on Intelligent Data Engi-
neering and Automated Learning (IDEAL’05), volume 3578 of Lecture Notes in
Computer Science, pages 220–227. Springer, 2005.
19. B. Yuan and M. Gallagher. Statistical racing techniques for improved empirical
evaluation of evolutionary algorithms. In X. Yao et al., editor, Proc. Parallel
Problem Solving from Nature (PPSN VIII), volume 3242 of Lecture Notes in
Computer Science, pages 172–181. Springer, 2004.

Genetic Programming: Parametric Analysis
of Structure Altering Mutation Techniques
Alan Piszcz and Terence Soule
Department of Computer Science
University of Idaho, Moscow, ID, 83844-1010
apiszcz@acm.org, tsoule@cs.uidaho.edu
Summary. We hypothesize that the relationship between parameter settings,
speciﬁcally parameters controlling mutation, and performance is non-linear in ge-
netic programs. Genetic programming environments have few means for a priori
determination of appropriate parameters values. The hypothesized nonlinear behav-
ior of genetic programming creates diﬃculty in selecting parameter values for many
problems. In this paper we study three structure altering mutation techniques using
parametric analysis on a problem with scalable complexity. We ﬁnd through parame-
ter analysis that two of the three mutation types tested exhibit nonlinear behavior.
Higher mutation rates cause a larger degree of nonlinear behavior as measured by
ﬁtness and computational eﬀort. Characterization of the mutation techniques us-
ing parametric analysis conﬁrms the nonlinear behavior. In addition, we propose
an extension to the existing parameter setting taxonomy to include commonly used
structure altering mutation attributes. Finally we show that the proportion of mu-
tations applied to internal nodes, instead of leaf nodes, has a signiﬁcant aﬀect on
performance.
1 Introduction
Parameter setting includes speciﬁcation of values for population size, selection rate,
crossover probability, mutation probability, reproduction rate, the number of genera-
tions and other environment attributes. In many problems, the parameters of genetic
programming (GP) algorithms interact, making optimal value selection diﬃcult [3].
This interaction leads to unpredictable behavior with respect to computational ef-
fort and may limit the ability to evolve a solution. If linear changes at the input
parameter space translate into linear changes in performance, a transfer function
or model would enable appropriate parameter selection. Luke and Spector studied
crossover and mutation noting interesting nonlinearities in the results, they suggest
that further analysis is needed [12]. Identiﬁcation of speciﬁc nonlinearity in GP pa-
rameter settings will improve the researchers probability of choosing appropriate
values. The GP community typically treats mutation as a secondary operator, when
implemented it often has less emphasis than crossover and reproduction. Optimizing
A. Piszcz and T. Soule: Genetic Programming: Parametric Analysis of Structure Altering
Mutation Techniques, Studies in Computational Intelligence (SCI) 54, 143–160 (2007)
www.springerlink.com
© Springer-Verlag Berlin Heidelberg 2007

144
Alan Piszcz and Terence Soule
population size and crossover rates appear frequently as methods to improve GP so-
lutions, whereas mutation rates are rarely considered. Feldt and Nordin identiﬁed the
importance of population size and mutation in [4]. In the following experiments, we
examine the aﬀects of a single parameter related to mutation across three structure
altering mutation techniques, and several mutation selection methods. We observe
that nonlinearity is the behavior for the majority of the structure altering mutation
technique (SAMT) cases examined.
2 Background
Mutation is an operator for genetic programming that introduces diversity in the
building blocks created during evolution. Due to a lack of research on mutation in
GP, choosing mutation parameters is diﬃcult, which is likely to lead to suboptimal
parameter choices. Additionally, improved characterization of mutation parameters
could further our understanding on the aﬀects of mutation on the evolutionary
process. It is typical to use
1
size of individual for mutation probability when it is
possible to estimate or control the problem size.
Koza’s view of mutation and crossover considers the value of mutation versus
crossover. Koza introduces a framework for comparing mutation with crossover in
GP [8] [9]. The framework describes the diﬀerence of the source of the new subtree
introduced by mutation or crossover. Koza advises that mutation is unlikely to be
beneﬁcial when compared to a large population and crossover. Few of Koza’s early
problems include mutation, and he states two reasons for the omission of mutation
in problems investigated in his ﬁrst book [6]. First, it is unlikely to lose diversity
when using a suﬃcient population size, and therefore mutation is simply not needed
in GP. Second, when the crossover operation occurs using endpoints in both trees
the aﬀect is “very similar to point mutation” [6].
Feldt and Nordin applied the design of experiments techniques from mathemat-
ical statistics to evaluate the aﬀects of genetic programming parameters on three
problems [4]. The ﬁndings show the population size was the most important para-
meter (with an aﬀect score of 11.51), followed by mutation (5.21) and the number of
generations (5.14). Mutation and large crossover probabilities had a positive aﬀect
on one of the three screening experiments. We attempt to conﬁrm mutation has a
similar positive inﬂuence on our problem with the SAMT.
In addition to aﬀecting performance there is research suggesting that mutation
can also aﬀect rates of code growth. Luke considers modiﬁcation point depth as a new
model for genome growth in GP [11]. The ﬁndings suggest correlation of code growth
with node depth bias. The survivability of the child improves with deep mutation and
crossover points. Luke’s meaning of deep node depth bias refers to locations near the
terminal nodes. In this paper, we attempt to conﬁrm that the relationship between
the depth and size of mutation replacement trees and population performance is
nonlinear.
In addition to the rate of mutation, the ratio of internal/external mutations may
have a signiﬁcant aﬀect on performance. Few researchers report this ratio. Excep-
tions do exist, Luke, Ross, and Burke et al. reference the internal/external mutation
parameter in descriptions for the genetic programming environment problems in
their work [12] [13] [1]. However none of the research describes or reports the aﬀect

Parametric Analysis of SAMT
145
of this parameter value. In this work we report preliminary results regarding the
importance of this ratio.
3 Parameter Setting Taxonomy
To understand the aﬀects of mutation we ﬁrst need to deﬁne the diﬀerent parame-
ters that can be used to control mutation. Eiben et al. studied parameter settings
for evolutionary algorithms (EA) and introduces a parameter setting taxonomy that
deﬁnes attributes related to EA environments [3]. Eiben et al. examines parameter
settings for mutation, crossover, evaluation functions, replacement operator or sur-
vivor selection, and population size in evolutionary algorithms. Eiben et al. divide
the research into parameter tuning and parameter control. Figure 3 shows the para-
meter setting taxonomy. Eiben et al. prefaces the discussion of mutation operators
and their probabilities with “There have been several eﬀorts to tune the proba-
bility of mutation in genetic algorithms. Unfortunately, the results (and hence the
recommended values) vary, leaving practitioners in the dark.” [3].
We propose an extension to Eiben et al. global taxonomy for parameter setting,
starting at the “Parameter tuning” node shown in Figure 3. The proposed extensions
follow, the underlined attributes indicate the three SAMTs examined in this paper
with parameter analysis.
• Parameter tuning: The extended taxonomy further reﬁnes relationships for mu-
tation behavior.
• Population proportion determines the number of individuals selected for muta-
tion.
• Individual proportion determines the techniques applied to each individual.s
structure. Individual structure modiﬁcations include terminals versus non-
terminals, or introns versus exons.
• Selection bias determines what members of the population are chosen for mu-
tation, examples include the best, worst, or random individuals.
• Individual proportion is further subdivided into Intron and exon ratios that
speciﬁcally mutate introns or exons.
• Depth ratios that speciﬁcally mutate nodes based on sub-tree depth.
• Variable shallow node mutation modiﬁes 1 or more nodes by a controlled depth.
An example would be to randomly modify 5 nodes throughout an individual.
Shallow indicates a depth of 0 or a single node mutated, instead of the normal
variable size replacement tree depth.
• Replacement tree depth controls the size of the replacement subtree randomly
created and inserted by the mutation operation.
• Internal, external ratio controls which nodes within a tree for mutation based
on their position in the tree hierarchy.
4 Structure Altering Mutation Techniques (SAMT)
The possible permutations, and interaction with other parameters make rationali-
zation of the mutation rate parameter diﬃcult to optimize for many problems. To

146
Alan Piszcz and Terence Soule
Fig. 1. Parameter Setting Taxonomy. Eiben original.
Fig. 2. Parameter Setting Taxonomy. The original taxonomy on the left is extended
on the right starting at the ‘Parameter tuning’ node.

Parametric Analysis of SAMT
147
address the evaluation of structure altering mutation we introduce three SAMT
techniques, each modiﬁes the genetic program tree in the mutation phase. Figure 3
depicts visual examples for the descriptions of SAMT 1, 2, and 3.
SAMT-1 implements shallow variable node count mutation (SVNCM) and op-
erates by selecting 1 to n random locations in a tree, mutating a single node at each
of the locations.
SAMT-2 controls the depth of replacement tree for mutation. Genetic program-
ming algorithms often consider the replacement tree depth for mutation a tunable
parameter. SAMT-2 mutates the tree structure using increasing mutation replace-
ment tree depth.
SAMT-3 controls selection frequency for internal versus external nodes or non
terminal nodes to mutate. Researchers often omit deﬁning the internal/external
(INEX) mutation rate parameter, instead relying on the default values.
Fig. 3. SAMT Examples. The SAMT technique number is above each tree. A white
node indicates no mutation, a black node indicates a mutation. 0 is the original tree.
1 depicts 5 single node locations mutated. 2 depicts a mutation replacement tree
depth of 2. 3 depicts mutation of the internal nodes, the INEX ratio is 1.0.
5 The MAX Binary Tree Problem
The goal of MAX binary tree problem (BTP) is to create a full tree of a given depth
that produces the maximum value. This problem has several advantages: it is easy
to understand, ﬁtness evaluation is fast and simple, and the problem complexity is
easy to inﬂuence by changing the tree depth. The MAX BTP problem is similar
to the MAX problem introduced by Gathercole for researching the interaction of
crossover and tree depth [5]. Langdon and Poli extended the analysis of the MAX
problem by Gathercole and Ross and developed a quantitative model that indicates
the rate of improvement and shows that solution time grows exponentially with
depth [10]. Langdon and Poli cite two reasons why GP ﬁnds the MAX problem dif-
ﬁcult. (1) the tendency for GP populations to converge in the ﬁrst few generations
to suboptimal solutions from which they can never escape. (2) convergence to sub-
optima from which escape can only be made by slow search similar to randomized
hill climbing [10].

148
Alan Piszcz and Terence Soule
In the MAX BTP the root node is at level (or depth) 0. All internal nodes have
a degree of three, supporting one parent and two child nodes, and the terminal
nodes have a degree of one (the parent). The problem operators are +, −and the
terminals are ephemeral random constants 0 and 1. The maximum value for the
tree is obtained when all internal nodes are + and all terminals are 1 in which case
the maxvalue = 2depth. Tree evaluation only occurs once for each individual. Thus,
ﬁtness evaluation is fast compared to many other GP problems, such as symbolic
regression that may require tens to hundreds of tree evaluations to measure the
ﬁtness of a single individual.
If we consider a full tree with binary values as follow: +, −for internal node’s,
and 0, 1 for leaf nodes, the number of permutations for a given depth is:
Permutations = 2(Maximum T ree Size)
For example, a depth 3 problem with 15 vertices has 256 possible permutations.
Structural complexity is deﬁned by Koza as a count of the number of points in a
solution, this is the sum of functions and terminals. Structural complexity is one
characteristic of a tree representation that can be measured [7]. The permutations
nearly double with each level of depth and correlates with the structural complexity.
The ratio of permutations to structural complexity increases exponentially. Through
control of the tree depth, we can control the number of permutations, which deter-
mines the size of the search space, which determines the problem complexity. Thus,
increasing the depth signiﬁcantly increases problem complexity.
6 Experiment Approach
We present three mutation selection techniques to test for nonlinear performance
by presenting an increasing linear input that mutates the tree structure. Each ex-
periment performs a sweep of the parameter setting under study. Each problem run
has a unique random seed assigned. We test three mutation selection modes: none
(n), random (r) and best (b). none indicates no mutation mode or mutation phase.
random, P percent of the population is randomly selected to undergo mutation. best,
the best P percent of the population is selected for mutation. Table 1 lists the ex-
periment parameters. The table has a few diﬀerences in parameter values, this does
not aﬀect analysis which takes place within an SAMT scope and not across SAMTs.
6.1 SAMT-1, Shallow Variable Node Count Mutation
The goal of this experiment is to determine if the relationship between the number
of nodes mutated and the mean performance is linear or non-linear. Knowing the
optimal mutation function will be useful for researchers using this type of mutation,
and inform the community on optimal parameter settings for other types of mu-
tation. This experiment consists of three trials, a control run with no mutation or
none, then 5 repetitions for random, and best selection mutation mode (abbreviated
n, r, and b in the ﬁgures). Each of the 5 repetitions consists of mutating 1, 2, 3, 4,
and 5 nodes while iterating 100 separate runs diﬀerentiated by the random seed.

Parametric Analysis of SAMT
149
6.2 SAMT-2 and SAMT-2b, Mutation Replacement
Tree Depth
The goal of this experiment is to determine if changes in the mutation replacement
tree depth cause nonlinear changes in the mean ﬁtness of the population. In this
experiment, we test for nonlinearity by measuring the mean ﬁtness of the population
while the depth of the mutation replacement tree is increased linearly. We test
three mutation selection modes including none. The replacement tree depth is tested
with a range of values from 0 to the maximum tree depth (total replacement). The
subtree created is the result of the same method used for the initial population at
generation 0. We have chosen to continuously generate new replacement trees until
one meets the maximum depth criteria. The new tree is tested for compliance with
the maximum tree depth limit. If a new individual exceeds the depth limit another
subtree be will be generated. For example in lilgp, a common GP engine which is
used in these experiments the default behavior omits the mutation if the total tree
depth exceeds the maximum. If the keep trying ﬂag is enabled lilgp will continuously
search for a replacement until one meets the overall depth requirement. By default,
lilgp also generates the new random subtree with a default depth range of 0 to 4 [2].
This depth range represents 1 to 15 new nodes for the subtree in the binary tree
problem. For example depth 0 results in a single node replacement, depth 1 results
in 3 nodes undergoing replacement, etc. For each of the replacement tree sizes we
measure the resulting mean population ﬁtness. Experiment SAMT-2b evaluates the
use of the keep trying ﬂag at the default setting of false. keep trying determines
if the genetic programming simulation tree replacement method, should give up
if a replacement depth exceeds the maximum tree depth, or continue to generate
new subtrees until one complies with the maximum tree depth speciﬁcation. This
frequently overlooked parameter setting signiﬁcantly changes the behavior of the
algorithm and subsequent results.
6.3 SAMT-3, Internal/External Mutation
Selection Ratios
The goal of this experiment is to understand if a linear input change in the INEX
mutation ratio has a linear aﬀect on the mean ﬁtness of population. This experi-
ment employed parametric control of the INEX mutation ratio while measuring the
resulting computational eﬀort to evolve a solution. We test three mutation selection
modes including none. Each experiment increased the problem depth and corre-
sponding problem complexity. Each data point presented in the following graphs
is the average of 100 runs. The INEX ratio varies from 0.0 to 1.0 in steps of 0.01
Thus, each plot line is composed of 10,100 runs. For each problem depth, the CEAIS
value computes the eﬀort expended to evolve a solution. The following terms and
calculations determine the minimum computational eﬀort average for each accept-
able solution. Mean, mean generation for an acceptable solution across 100 runs.
Average Computational Eﬀort (ACE) based on mean generation for all 100 runs.
Number of Acceptable Solutions (NS). ACE counts generation 0 by adding 1 to the
ﬁnal generation value.
ACE = (MeanGeneration + 1) ∗PopulationSize
(1)

150
Alan Piszcz and Terence Soule
ComputationalEffortAverageIndividualSolution(CEAIS) = ACE
NS
(2)
Low values of CEAIS indicate the minimum average computational eﬀort per solu-
tion that corresponds to an optimal conﬁguration of parameter settings. A prior
experiment determined optimal crossover and mutation rates for SAMT-3.
Table 1. Experiment Parameters
Parameter
SAMT-1
SAMT-2
SAMT-3
Objective
2Depth
2Depth
2Depth
Terminal Set
0, 1
0, 1
0, 1
Functional Set
+, −
+, −
+, −
Fitness Cases
1
1
1
Fitness
Value of tree Value of tree Value of tree
Population Size
10,000
1,000
4,000
Max Generations
1,000
1,000
1,000
Initialization Method
Half & Half
Full
Full
Initialization Depth Ramp
2-6
7
maxdepth −1
Maximum Tree Depth
8
8
3,4,5,6,7,8
Crossover selection mode
ﬁtness
ﬁtness
ﬁtness
Crossover rate
0.9 −MR
0.89
optimal
Reproduction selection
ﬁtness
ﬁtness
ﬁtness
Reproduction rate
0.1
0.1
0.1
Mutation selection mode
(n),(r),(b)
(n),(r),(b)
(n),(r),(b)
Mutation rate
(n)=0
(r,b)=0.01
optimal
(MR)
(r,b)=0.01
0.11, 0.21,
for each
0.11,0.21,0.31
0.31
problem size
Internal mutation ratio
0.9
0.9
variable
External mutation ratio
0.1
0.1
variable
7 Results
The following subsections give a brief description of experiment results. The
legend in Figure 4 and 5 represent the four population selection mutation
rates.
SAMT-1, we examine four mutation rates applied to a population size of
10,000. The mutation rates of 0.01, 0.11, 0.21 and 0.31, aﬀect only the best
individuals for mutation. The four rates select the best 100, 1100, 2100 and
3,100 individuals of the population. Results for the none selection mode which
is equivalent to 0 nodes mutated, and for the random mutation selection mode
produced no acceptable solutions and these data are subsequently omitted
from SAMT-1 results. The numbers of nodes mutated at each mutation rate
are 1, 2, 3, 4 and 5 are selected independently. The maximum value that

Parametric Analysis of SAMT
151
can be obtained with a depth of 8 is 256, this represents all 255 internal
nodes containing a + and all 256 external nodes contain a 1. Figure 4 and 5
show the results of the number nodes mutated for 0 to 5, the 0 indicates no
mutation. Figure 4 plots the mean population ﬁtness against the number of
nodes mutated. The deleterious impact to ﬁtness beyond 2 nodes is signiﬁcant
and severely limits mean ﬁtness the population obtains.
Figure 5 plots the change in mean population ﬁtness between each interval
of the number of nodes mutated. The curves between the points reﬂect a cubic
spline interpolation. The spline curves provide a projection that shows the
trend of the data points. y −axis values above 0 indicate improvement over
the previous number of nodes mutated, values below 0 indicate a reduction in
mean ﬁtness.
Table 3 shows the results of each experiment run, the column labels indi-
cate: NM - Nodes Mutated, MF column shows the mean ﬁtness for all gen-
erations and the second value is the the number of acceptable solutions. The
none trial had no mutation phase.
Table 2 shows the the rates used for each of the four trials for the random
and best mutation selection methods.
Table 2. SAMT-1 Trial Rates
Trial Crossover Mutation Reproduction
MF-1
0.89
0.01
0.10
MF-2
0.79
0.11
0.10
MF-3
0.69
0.21
0.10
MF-4
0.59
0.31
0.10
Table 3. SAMT-1 Result Summary
Mode NM
MF-1
MF-2
MF-3
MF-4
none
0 0.004, 00 0.004, 00 0.004, 00 0.004, 00
random
1 0.004, 00 0.004, 00 0.004, 00 0.004, 00
random
2 0.004, 00 0.004, 00 0.004, 00 0.004, 00
random
3 0.004, 00 0.004, 00 0.004, 00 0.004, 00
random
4 0.004, 00 0.004, 00 0.004, 00 0.004, 00
random
5 0.004, 00 0.004, 00 0.004, 00 0.004, 00
best
1 0.588, 48 0.739, 66 0.842, 81 0.730, 66
best
2 0.527, 47 0.564, 52 0.405, 37 0.027, 01
best
3 0.414, 38 0.271, 23 0.024, 01 0.009, 00
best
4 0.241, 21 0.057, 04 0.008, 00 0.007, 00
best
5 0.004, 00 0.005, 00 0.004, 00 0.004, 00

152
Alan Piszcz and Terence Soule
Fig. 4. SAMT-1: Absolute Scale of Mean Fitness versus Number of Nodes Mutated
Fig. 5. SAMT-1: Relative Change in Mean Fitness versus Number of Nodes Mutated
For SAMT-2, we examine four mutation rates applied to a population
size of 1000. The four rates randomly select 10, 110, 210 and 310 individuals
for mutation. The mutation rates of 0.01, 0.11, 0.21 and 0.31, aﬀect only

Parametric Analysis of SAMT
153
the best individuals for mutation. Results for the random mutation selection
mode produced no acceptable solutions, and have been omitted from SAMT-
2 results. SAMT-2 results are given in Table 4. Figure 6 and 7 indicate no
mutation mean population ﬁtness by the x −axis (n) label. Figure 6 and 7
show the impact of controlled replacement tree depth mutation. In Figure 6
the mean ﬁtness improves from a depth 0 or size 1 replacement tree through
a depth 3 replacement tree. Fitness decreases from depth 2 through depth
8. Figure 7 shows the relative change in ﬁtness for each depth replacement
tree. All positive contributions occur at depths of 0 and 1. Further increases
in depth cause a negative ﬁtness inﬂuence on the population relative to the
previous value. Mutations beyond a depth of 4 evolved no acceptable solutions.
Table 4. SAMT-2 Result Summary
Mode D
MF-1
MF-2
MF-3
MF-4
none 0 0.0040, 00 0.0040, 00 0.0040, 00 0.0040, 00
random 0 0.0040, 00 0.0039, 00 0.0039, 00 0.0039, 00
random 1 0.0040, 00 0.0040, 00 0.0040, 00 0.0040, 00
random 2 0.0040, 00 0.0040, 00 0.0040, 00 0.0040, 00
random 3 0.0040, 00 0.0040, 00 0.0041, 00 0.0041, 00
random 4 0.0040, 00 0.0041, 00 0.0041, 00 0.0041, 00
random 5 0.0040, 00 0.0042, 00 0.0042, 00 0.0042, 00
random 6 0.0041, 00 0.0043, 00 0.0042, 00 0.0043, 00
random 7 0.0042, 00 0.0045, 00 0.0043, 00 0.0043, 00
random 8 0.0044, 00 0.0045, 00 0.0045, 00 0.0045, 00
best 0 0.1692, 12 0.2353, 20 0.0482, 02 0.0112, 00
best 1 0.1889, 15 0.4405, 37 0.2503, 20 0.0661, 03
best 2 0.0700, 04 0.1692, 13 0.0376, 01 0.0146, 00
best 3 0.0000, 00 0.0246, 01 0.0118, 00 0.0093, 00
best 4 0.0000, 00 0.0189, 01 0.0085, 00 0.0072, 00
best 5 0.0000, 00 0.0065, 00 0.0066, 00 0.0062, 00
best 6 0.0000, 00 0.0056, 00 0.0057, 00 0.0063, 00
best 7 0.0000, 00 0.0053, 00 0.0056, 00 0.0056, 00
best 8 0.0000, 00 0.0046, 00 0.0045, 00 0.0046, 00
SAMT-2b has identical parameters to that of SAMT-2 with one excep-
tion, keep trying is set to false or the default for some genetic programming
environments. SAMT-2b results are given in Table 5. Figure 8 and 9 present
the results of the mean ﬁtness at each depth tested and the relative change in
ﬁtness between the current and prior depth. Both plots indicate a decreasing
change in ﬁtness. Due to the behavior of the genetic programming system
treatment of mutation replacement subtrees we observe increasing ﬁtness in-
stead of decreasing. The reason postulated is that as the mutation replacement

154
Alan Piszcz and Terence Soule
Fig. 6. SAMT-2: Absolute Scale of Mean Fitness versus Depth of Mutation Re-
placement Tree
subtree increases in depth it becomes less likely to meet the maximum depth
constraints. The result is that less subtree mutations occur than are speci-
ﬁed due to geometry constraints for the individual. The impact is ﬁtness is
reduced since fewer replacement subtrees ﬁt the individual size constraints
as the replacement subtree grows. So the absolute number of trees mutated
decreases, the result is an increase in the mean ﬁtness of the population as
the depth increases.
In the SAMT-3 experiment, ﬁve increasingly complex versions of the MAX
binary tree problem had replacement tree depths ranging from 3 to 8. Due to
space limitations, only the depth 6 results appear. Figure 7 plots the CEAIS
values for the MAX BTP of depth 6 against the internal/external selection
ratio. The 5 point spline interpolation of Figure 7 shows the best CEAIS
decreasing from 2402 to 1084 over the internal mutation rate range. None has
a CEAIS value of 1830 and outperforms random across the entire internal
mutation rate range. The random selection mode has CEAIS higher then
none and has increasing CEAIS as internal mutation rate increases. Note the
observation for this even depth problem is that the slope of the best CEAIS
decreases with increasing internal mutation rate.

Parametric Analysis of SAMT
155
Fig. 7. SAMT-2: Relative Change in Mean Fitness versus Depth of Mutation Re-
placement Tree
Fig. 8. SAMT-2b: Absolute Scale of Mean Fitness versus Depth of Mutation Re-
placement Tree

156
Alan Piszcz and Terence Soule
Table 5. Experiment SAMT-2b Result Summary
Mode D
MF-1
MF-2
MF-3
MF-4
none 0 0.0040, 00 0.0040, 00 0.0040, 00 0.0040, 00
random 0 0.0039, 00 0.0039, 00 0.0039, 00 0.0039, 00
random 1 0.0040, 00 0.0040, 00 0.0040, 00 0.0040, 00
random 2 0.0040, 00 0.0040, 00 0.0040, 00 0.0040, 00
random 3 0.0040, 00 0.0040, 00 0.0041, 00 0.0041, 00
random 4 0.0040, 00 0.0041, 00 0.0041, 00 0.0041, 00
random 5 0.0040, 00 0.0041, 00 0.0042, 00 0.0042, 00
random 6 0.0041, 00 0.0042, 00 0.0042, 00 0.0043, 00
random 7 0.0042, 00 0.0043, 00 0.0043, 00 0.0043, 00
random 8 0.0043, 00 0.0044, 00 0.0045, 00 0.0045, 00
best 0 0.1692, 12 0.2353, 20 0.0482, 02 0.0113, 00
best 1 0.2566, 20 0.5673, 49 0.4177, 35 0.2689, 23
best 2 0.4464, 33 0.6296, 50 0.5787, 42 0.5960, 46
best 3 0.4610, 32 0.5223, 36 0.5817, 45 0.5677, 41
best 4 0.5376, 40 0.5889, 45 0.5458, 41 0.5406, 39
best 5 0.5469, 41 0.5757, 45 0.5059, 37 0.5592, 43
best 6 0.5474, 42 0.5547, 43 0.5686, 43 0.5521, 44
best 7 0.5309, 40 0.5261, 39 0.5550, 43 0.5092, 37
best 8 0.4862, 34 0.5317, 40 0.4968, 37 0.5829, 47
Fig. 9. SAMT-2b: Relative Change in Mean Fitness versus Depth of Mutation
Replacement Tree

Parametric Analysis of SAMT
157
Fig. 10. SAMT-3: Depth 6, CEAIS versus ratio of mutation at non-terminal nodes,
Linear-Log Plot
Fig. 11. SAMT-3: Depth 6, CEAIS versus ratio of mutations at non-terminal nodes,
Linear-Log Plot, 5 point spline interpolation

158
Alan Piszcz and Terence Soule
Table 6 summarizes results from each experiment. The following abbre-
viations appear in the table header. Mean of best (MB), CEAIS values, and
Standard deviation (SD) of best CEAIS values.
Table 6. SAMT-3 Experiment Summary
Depth
MB
SD
3
7
0.8
4
40
7.3
5
161
24.6
6
1827
415.4
7
12024
2023.2
8
8638296 10639.0
The following observations concern the best mutation selection mode in
the cases examined above. For the odd depth MAX BTP tested, we note the
internal mutation ratio range from 0.0 to 1.0 produces CEAIS values that
decrease by 100%. The payoﬀfor identiﬁcation of the optimal INEX value
needs careful balance with the eﬀort of discovery. For the even depth prob-
lems we note a trend that identiﬁes higher internal mutation rates providing
signiﬁcantly lower CEAIS values than low internal mutation ratio rates. Even
depth problems show a consistent reduction of CEAIS values as the internal
mutation ratio increases from 0.0 to 1.0. The even depth problems also have a
higher CEAIS variance across the entire mutation rate range as a fraction of
the mean than the odd depth problems. In this situation, it indicates the im-
provement with decreasing CEAIS values as the mutation rate changes from
0.0 to 1.0.
7.1 Observations
In SAMT-1 we found that the best mutation mode performed signiﬁcantly bet-
ter than random and none showing that mutation can improve performance.
Mutating the best individuals results in the better performance, possibly be-
cause those individuals are mostly likely to have an aﬀect on the future path
of evolution. The critical points for mutation in four separate rate trials show
the beneﬁcial aﬀects of mutation occur only for depth 1. Higher mutation rates
accelerate the deleterious impact of multi-node mutation, although mutating
two nodes is still better than none. Mutation boost is describes the signiﬁcant
improvement in evolved best solutions over the other two selection modes,
none and random.
SAMT-2, 22.2% is the optimal tree replacement depth for the binary tree
problem with a depth of 8. Depths 0 to 2 perform better than no mutation.

Parametric Analysis of SAMT
159
SAMT-3, Selection of the best individuals and performing mutation on
INEX ratio of 50/50 should provide a consistent and neutral aﬀect on the
resulting computational eﬀort. Signiﬁcant variance is noted for the best muta-
tion selection mode for even problems between internal mutation rates below
and above 50% or 0.5. This suggests deleterious aﬀects of mean population
ﬁtness for the MAX BTP using high external mutation rates. Optimal pa-
rameter tuning for the INEX rate leads to improved performance with even
depth MAX BTP problems. For odd depth problems we observe improve-
ments as the INEX rate approaches ≈0.60. When complexity reaches depth 7,
a decrease of CEAIS shows higher INEX rates outperform lower INEX rates.
8 Summary
Parameter settings signiﬁcantly inﬂuence the eﬀectiveness of the structure al-
tering mutation techniques evaluated and generally show a nonlinear response
to population ﬁtness and computational eﬀort. Each experiment discovered
unique genetic programming behavior using parametric analysis of values in
isolation. The key results from the experiments:
• SAMT-1: The behavior of mutating 1 to 5 nodes is near linear when 1%
to 11% of the population is mutated. When more of the population is sub-
jected to mutation (21% and 31%) the relationship between the number
of nodes mutated and the performance becomes increasingly nonlinear.
The rate of decrease of mean population ﬁtness increases with mutation
rate as more nodes are mutated.
• SAMT-2: Varying the replacement tree depth produced a nonlinear re-
sponse as measured by the mean population ﬁtness. Performance with
SAMT-2 peaks at approximately 22% of the maximum tree depth across
all four mutation rates.
• SAMT-3: Increasing the rate at which non-terminal nodes are mutated
results in a linear response of computational eﬀort for the random and
best mutation selection modes. The computational eﬀort correlation with
increasing the INEX rate was negative for random and positive for best
mutation selection modes.
• SAMT-3: As the internal mutation selection ratio increases the computa-
tional eﬀort decreases. This indicates that higher internal mutation selec-
tion rates are optimal for the best mutation selection mode on the MAX
BTP. The average response of the population to random, best and none
are near linear with respect to the internal mutation parameter value.
References
1. Edmund K. Burke, Steven Gustafson, and Kendall Kendall. Diversity in ge-
netic programming: An analysis of measures and correlation with ﬁtness. IEEE
Transactions on Evolutionary Computation, 8(1):47–62, 2004.

160
Alan Piszcz and Terence Soule
2. Bill Punch Douglas Zongker. lil-gp 1.0 user’s manual. Technical report, Michigan
State University, 1995.
3. Agoston Endre Eiben, Robert Hinterding, and Zbigniew Michalewicz. Para-
meter control in evolutionary algorithms. IEEE Transations on Evolutionary
Computation, 3(2):124–141, 1999.
4. Robert Feldt and Peter Nordin. Using factorial experiments to evaluate the
eﬀect of genetic programming parameters. In Riccardo Poli, Wolfgang Banzhaf,
William B. Langdon, Julian F. Miller, Peter Nordin, and Terence C. Fogarty,
editors, Genetic Programming, Proceedings of EuroGP’2000, volume 1802 of
LNCS, pages 271–282, Edinburgh, 15-16 April 2000. Springer-Verlag.
5. Chris Gathercole and Peter Ross. An adverse interaction between crossover and
restricted tree depth in genetic programming. In John R. Koza, David E. Gold-
berg, David B. Fogel, and Rick L. Riolo, editors, Genetic Programming 1996:
Proceedings of the First Annual Conference, pages 291–296, Stanford University,
CA, USA, July 1996. MIT Press.
6. John R. Koza. Genetic Programming: On the Programming of Computers by
Means of Natural Selection, pages 106–107. MIT Press, Cambridge, MA, USA,
1992.
7. John R. Koza. Genetic Programming: On the Programming of Computers by
Means of Natural Selection. MIT Press, Cambridge, MA, USA, 1992.
8. John R. Koza, Forrest H. Bennett III, David Andre, and Martin A. Keane.
Genetic Programming III: Darwinian Invention and Problem Solving. Morgan
Kaufmann Press, May 1999.
9. John R. Koza, Martin A. Keane, Matthew J. Streeter, William Mydlowec, Jessen
Yu, and Guido Lanza. Genetic Programming IV: Routine Human-Competitive
Machine Intelligence. Kluwer Academic Publishers, July 2003.
10. W. B. Langdon and R. Poli. An analysis of the MAX problem in genetic pro-
gramming. In John R. Koza, Kalyanmoy Deb, Marco Dorigo, David B. Fogel,
Max Garzon, Hitoshi Iba, and Rick L. Riolo, editors, Genetic Programming
1997: Proceedings of the Second Annual Conference, pages 222–230, Stanford
University, CA, USA, 13-16 July 1997. Morgan Kaufmann.
11. Sean Luke. Modiﬁcation point depth and genome growth in genetic program-
ming. Evolutionary Computation, 11(1):67–106, 2003.
12. Sean Luke and Lee Spector. A comparison of crossover and mutation in genetic
programming. In John R. Koza, Kalyanmoy Deb, Marco Dorigo, David B. Fogel,
Max Garzon, Hitoshi Iba, and Rick L. Riolo, editors, Genetic Programming
1997: Proc. of the Second Annual Conf., pages 240–248, San Francisco, CA,
1997. Morgan Kaufmann.
13. Brian J. Ross. Searching for search algorithms: Experiments in meta-search.
Technical report, Brock University, December 2002.
Technical Report CS-
02-23.

Parameter Sweeps for Exploring Parameter
Spaces of Genetic and Evolutionary Algorithms
Michael E. Samples, Matthew J. Byom, and Jason M. Daida
Center for the Study of Complex Systems and the Department of Atmospheric,
Oceanic, and Space Sciences, University of Michigan. Ann Arbor, Michigan USA
48109-2143 (msamples,mjbyom,daida)@umich.edu
Summary. The quality of experimental research in Genetic and Evolutionary Com-
putation can suﬀer from experiments that are too small to generalize results. Ex-
periments can be too small by not being statistically signiﬁcant with a given set
of algorithm parameters, or by not identifying the eﬀect of modifying an algorithm
parameter, or by not understanding the causal relationships between diﬀerent algo-
rithm parameters. Parameter Sweep Experiments solve these problems by sampling
many diﬀerent parameter conﬁgurations enough times to achieve statistical signif-
icance and identify trends in parameter modiﬁcations. Unfortunately, Parameter
Sweep Experiments have a high cost of labor to setup and run experiments and
analyze data. As a result, these experiments are rarely conducted in Genetic and
Evolutionary Computation research. This chapter argues that although Parameter
Sweep Experiments are diﬃcult to conduct, they are required to understand com-
plicated nonlinear interactions between an algorithm’s parameters. We present a
software solution, Commander, that works in conjunction with a genetic and evolu-
tionary algorithm to assist users in conducting Parameter Sweep Experiments in a
distributed computing environment.
1 Introduction
In recent years, a number of authors have provided critiques of the existing ex-
perimental methodology in Genetic and Evolutionary Computation (GEC). This
chapter continues that trend through an analysis of the design, execution, and data
reduction stages of GEC experiments with respect to how the parameter values for
GEC algorithms are chosen. Although genetic programming (GP) is referenced in
speciﬁc examples, the methodology and tools presented in this chapter are applica-
ble to both the wider GEC community and the even larger ﬁeld of parameterized
computer simulations. (For examples from GP, see [13, 14, 23, 26, 29].)
The execution of a GEC algorithm to solve a particular problem can be consid-
ered as a search in the space of all solutions expressible by a given representation
(e.g., a tree or a vector). Observations that are made during the execution of a search
are the result of many low-level, nonlinear interactions. The rules, which govern these
low-level interactions and thus inﬂuence experimental results, are highly dependent
Michael E. Samples et al.: Parameter Sweeps for Exploring Parameter Spaces of Genetic and
Evolutionary Algorithms, Studies in Computational Intelligence (SCI) 54, 161–184 (2007)
www.springerlink.com
© Springer-Verlag Berlin Heidelberg 2007

162
Michael E. Samples, Matthew J. Byom, and Jason M. Daida
on algorithm conﬁgurations and parameter values. Slight changes to these rules can
produce dramatically diﬀerent outcomes of the search process.
To eﬀectively use GEC algorithms as a search process, users should understand
how potentially diﬀerent outcomes are inﬂuenced by choices of algorithm parameters.
Towards this end, one common goal is to characterize a mapping between arbitrary
algorithm conﬁgurations and their corresponding outcomes on speciﬁc classes of
problems. Since an exhaustive sampling of an inﬁnitely large parameter space is
computationally infeasible, users often rely instead on a partial sampling to generate
a set of results from which predictions about untested conﬁgurations can be based.
Not surprisingly, a theory is often relied upon to provide a framework under
which interpolations and extrapolations of algorithm outcomes can be made. Un-
fortunately, it is still quite possible that interpolation and extrapolation of a partial
sampling—even when guided by theory—can remain ineﬀective. Too small of a sam-
pling can miss something, if only because an underlying assumption was incorrectly
applied.
In this chapter, we take the position that the eﬀectiveness of interpolation and
extrapolation can be increased by large, comprehensive, multivariate experiments.
We further take the position that these experiments are challenging to conduct, par-
ticularly in GEC, because even small experiments can have demanding requirements
for data storage and computational power. We examine similar experimental frame-
works in other ﬁelds and propose a software solution that assists an investigator in
administering these types of experiments in a distributed-computing environment.
In Section 2, we discuss the background of large computational experiments
(particularly experimental frameworks), and discuss how a particular type of ex-
periment is helpful in characterizing GEC dynamics. We also outline criteria for a
distributed-computing software tool to assist in experiment administration. Section
3 presents our software solution, Commander, and discusses architectural concerns
for robustness and eﬃcient execution. Section 4 presents results from applying our
tool to common GP test problems that show the utility of a methodology fostered
by Commander. Section 5 discusses the results of this demonstration. Section 6
concludes, while Section 7 suggests future work.
2 Background
2.1 Previous Work
The experimental methodology that we propose is not limited to GEC. In fact,
such experiments have often been practiced in other ﬁelds that involve the char-
acterization of parameterized behavior. A diverse body of work—from ﬁelds such
as high-energy physics [5], biomedical molecular modeling [10], analog circuit analy-
sis [30], and agent-based modeling [3]—have all beneﬁted from large parameter
sweep experiments. While this methodology is not new to science, it is not com-
mon in GEC. (Some exceptions include Daida, Samples et al. [15, 16], Daida et al.,
[17], Luke & Panait [23], and Luke & Spector [24].) This type of experimental
methodology frequently involves large numbers of trials and can be computation-
ally prohibitive. Such prohibitions are particularly relevant regarding empirical GEC

Parameter Sweeps for Parameter Space Exploration
163
research, where even small experiments can be computationally intensive. (For ex-
ample, in [15] alone, empirical results required two CPU-years to generate and used
several terabytes of data to store experimental results).
Our ﬁeld is not the ﬁrst to undertake computationally challenging problems and
several existing solutions for distributed computing are worth mentioning. One of
the earliest large-scale distributed computing projects, SETI@home [21], distributes
radio data to hundreds of thousands of volunteers who donate their spare CPU
hours to perform signal analysis. Another project, the Great Internet Mersenne
Prime Search (GIMPS) [33], searches for large prime numbers of the form 2n −1 for
an integer n. Like SETI@home, GIMPS is distributed among many volunteers who
run small portions of a larger experiment on their own desktop computers.
A number of software packages exist to perform such distributed computation,
notably: BOINC [2], APST [10, 11], APSE [8], Drone [6], Condor [5, 32], and Nim-
rod/G [1]. In general, these packages are designed to distribute a list of computa-
tional tasks across a network to remote computers. A downside of these packages is
that only some have support for the type of experiment we propose. For example,
Drone, Nimrod/G, and APSE support this experiment type. However, APST, Con-
dor, and BOINC all require the presence of custom external scripts to control an
experiment.
Distributed computing frameworks should be robust to most common failures
and operate transparently with respect to computer and network characteristics.
APST, Condor, and Nimrod/G contain support for only a limited number of grid
protocols. Drone does not support common grid protocols, but is designed for clus-
ter environments where a user has a local login account. It has also been out of
development for several years and is not robust to common network failures such
as disconnections and local user interference. Although APSE shares very similar
design goals to our project, robust behavior was not incorporated into its initial
design and the closed-source nature of APSE has precluded its extension. Currently,
BOINC is the most widespread utility for distributed public computing and its open-
source structure allows many projects to be built upon it (see, e.g., Stardust@home
[25] and Folding@home [28]).
2.2 Transitions
Modifying the parameter values of an algorithm frequently induces an observable
change in that algorithm’s behavior. Usually, minor parameter value modiﬁcations
result in only small diﬀerences to algorithmic behavior. However, there can exist
ranges of parameter values such that small changes in those parameter values can
result in large, nonlinear changes in the observable behavior of an algorithm. We
consider such parameter values to be within a transition region. Although such large-
scale, nonlinear, behavioral transitions may be analogous to phase transitions in
physical systems (e.g., water transitioning from liquid to solid phases), this remains
to be rigorously studied. It is suﬃcient for the purposes of this chapter to understand
that the mapping between parameters and algorithmic behavior may exhibit such
transitions. This is discussed in detail in Section 5.
An example of transitions in GEC is shown in Figure 1 from the Highlander
experiment initially discussed in [15]. In that paper, we developed a GP test problem
to probe for limits of building block mixing and assembly. In the ﬁgure, β represents

164
Michael E. Samples, Matthew J. Byom, and Jason M. Daida
100
80
60
40
20
0
100
80
60
40
20
100
80
60
40
20
0
100
80
60
40
20
 tournament
 
% Successful
% Successful
M = 50
M = 1500
Tuning Parameter β
100
80
60
40
20
0
% Successful
M = 500
100
80
60
40
20
 fitness-proportionate
Fig. 1. Diﬃculty curves for the Highlander experiment. Each plot corresponds to
a population size (M) and depicts the percentage of successful trials (out of 1000
trials) as a function of tuning parameter β. Parameter β values that correspond to
easier settings have higher percentages of successful trials.
a tuning parameter in the ﬁtness function that rewards GP trees with particular
characteristics (chosen by β) of building block mixing and assembly.
Experimental results on the Highlander problem demonstrated at least two tran-
sitions: one based on the tuning parameter β, and the other based on the population
size M. The β-transition yields highly successful trials for small values of β (e.g.,
when M = 1500 and β < 10). As β increases through a transition region, the
problem rapidly becomes impossible to solve. Each point in each graph represents a
number of successes from 1000 trials, yet GP was unable to solve the test problem
for β ≥18 when M = 1500. The second transition—the M-transition—changes the
location and shape of the β-transition. Small values of M yield a wide and grad-
ual transition while large values yield a narrow and steep transition. Note that the
β-transition covers three orders-of-magnitude (from 1000 successful trials out of a
1000 total trials, to 0 successful trials out of 1000 trials). Further information on the
Highlander test problem is available in this chapter’s appendix.
As stated previously, experimentalists often rely upon a theory to assist in the
interpolation and extrapolation of results to untested conﬁgurations. However, if this
theory does not suggest the existence of transition regions, such transition regions
might not be discovered except through direct experimental sampling of parameter
conﬁgurations. Consequently, experiments that involve too few unique parameter
conﬁgurations might not ﬁnd any evidence of a transition region. As a result, theory

Parameter Sweeps for Parameter Space Exploration
165
that is developed from small experiments might only be valid within boundaries of
unobserved (but nonetheless existent) transitions.
To determine the location of such transitions, general knowledge of an algo-
rithm’s behavior should be constructed by testing multiple classes of parameter
conﬁgurations. If experiments were run under multiple conﬁgurations, one would be
able to develop laws and theories that encapsulate knowledge of the entire parameter
space and not merely the space on one side of a particular transition. One should
observe an algorithm’s behavior by conducting multiple tests and changing input
parameters.
2.3 Notation
In this section, we deﬁne the terms that we use to describe parameter sweep experi-
ments.
Deﬁnition 1. Parameter - A parameter is broadly deﬁned to be a modiﬁable
algorithm-speciﬁc value that is capable of inﬂuencing some aspect of that algorithm’s
runtime behavior.
In other words, a parameter is a controllable variable that inﬂuences observable
data that is collected during the execution of an algorithm. Parameters can be
numeric (e.g., population size p = 1000) or algorithmic (e.g., tournament selection,
steady-state replacement, elitism). Frequently, the choice of a particular parameter
value leads to more parameter choices (e.g., we also need to choose a tournament
size under tournament selection).
Deﬁnition 2. Parameter Conﬁguration - The map of parameters to their cor-
responding values is called a parameter conﬁguration and is denoted by the symbol
C .
A parameter conﬁguration is considered valid if it completely speciﬁes the input
to an algorithm without containing conﬂicting parameter values (i.e., a GEC algo-
rithm cannot simultaneously employ ﬁtness-proportionate and tournament selection
as the selection method without some way of explicitly resolving what is meant by
specifying both). We denote the set of all possible valid parameter conﬁgurations as
C ∗. To refer to the value of a parameter, we use index notation (e.g., the population
size of a particular conﬁguration could be C [population size] = 1000).
Deﬁnition 3. Trial - A trial t in GEC research is deﬁned to be the execution of
a GEC algorithm to solve a particular problem P (e.g., regression or ant). A trial
has a speciﬁed parameter conﬁguration Ct and an initial random number generator
(RNG) seed rt.
In common experimental practice, a trial is executed and observable behaviors
are recorded for later analysis. In GEC research, this typically takes the form of
simple values (e.g., each generation’s average ﬁtness), but more complicated au-
dit procedures can be used to record more complicated observable states (e.g., a
complete ancestry tree for each individual). Since a trial t is uniquely deﬁned in
terms of its problem P, its parameter conﬁguration Ct and its RNG seed rt, we can
denote the observations from a trial as Ω({P, Ct, rt}), or shorter, as Ω(t) where
t = {P, Ct, rt}.

166
Michael E. Samples, Matthew J. Byom, and Jason M. Daida
Deﬁnition 4. Datapoint - A datapoint D is a set of trials with identical parameter
conﬁgurations but unique RNG seeds.
Since a nondeterministic algorithm’s behavior for all possible RNG seeds is un-
observable, an experimentalist needs to make do with taking observations from a
ﬁnite—and thus observable—number of trials. A ﬁnite-size datapoint D consists of
m trials {t1, . . . tm}, where unique trials ti and tj use the same problem P and
the same parameter conﬁguration (i.e., Cti = Ctj), but diﬀerent RNG seeds. We
can discuss the observations ΩD available from a datapoint D, and note that the
observations made from a datapoint are also capable of expressing statistics (e.g.,
the average ﬁtness of the m best-of-trial individuals). In our notation, a datapoint
is the smallest unit of experiment capable of conveying meaningful results regard-
ing a particular parameter conﬁguration. We denote the set of all datapoints in an
experiment as D∗.
Deﬁnition 5. Parameter Sweep Experiment - A parameter sweep experiment
(PSE) is conducted by examining multiple datapoints, i.e., by executing an algorithm
numerous times with varying parameter conﬁgurations.
No two trials in a PSE should share the same random number seed, and two
trials would only share the same parameter conﬁguration if they belong to the same
datapoint. We denote the size of an experiment as the number of datapoints tested
and the dimension of an experiment as the number of parameters altered across
diﬀerent parameter conﬁgurations. Parameter sweeps can be one-dimensional (e.g.,
each datapoint could have a diﬀerent population size parameter value) or multidi-
mensional (e.g., studying a set of datapoints in which each datapoint has a diﬀerent
2-tuple of population size and maximum allowable generations). Multidimensional
PSEs are unique in their ability to empirically reveal the eﬀects of interactions be-
tween parameters.
2.4 Parameter Sweep Challenges
Large n-dimensional PSEs are more diﬃcult to perform than small single-datapoint
experiments for several key reasons:
• Experimental Setup and Administration - For small experiments, there is often
only one parameter conﬁguration being tested. Since that conﬁguration does
not generally change, it is common just to write a shell script that is capable of
simply executing the associated GEC algorithm multiple times. For those ex-
periments, parameters can be speciﬁed inside this script or precompiled into the
GEC algorithm’s runtime code. An investigator can distribute this experiment
on multiple computers easily by merely running the script on each computer and
manually collecting data from each computer after an experiment concludes.
In a PSE, a shell script should be capable of generating a list of all parameter
conﬁgurations under study. To do this, complex scripts allow for iteration over
possible parameter conﬁgurations (a subset of the Cartesian product of the pa-
rameters). The simplest instantiation of PSE scripts is constructed with nested
loops to iterate over desired parameter combinations, but the serial nature of
such an instantiation is not robust to normal computer failures and does not
utilize distributed computing resources. Satisfactory scripts are generally both

Parameter Sweeps for Parameter Space Exploration
167
lengthy and non-portable. Construction of a new experiment would generally
involve large, time-consuming changes to the scripts.
• Required Computation Time - A single trial in GEC can be computationally ex-
pensive. However, because of the nondeterministic behavior of GEC algorithms,
a datapoint should consist of many trials to collect enough data to understand
the behavior of a particular parameter conﬁguration. PSEs amplify this eﬀect
since, by design, they evaluate a larger number of datapoints. This eﬀect can be
somewhat ameliorated through the use of parallel processing, though scripting
becomes more complicated because of provisions for network programming and
robust network behavior.
• Data Reduction - Single datapoint experiments generally produce the same type
of data since they are generated by a common parameter conﬁguration. As a
result, the extraction of relevant data is a straightforward process. In PSEs, the
extraction of relevant data often necessitates the use of scripts to arrange the
data in meaningful ways. Due to a possibly large number of conﬁgurations, a
PSE’s data sets should be labeled with the parameter conﬁguration that gener-
ated it. Analysis scripts have another degree of complexity, because they interact
with the tagging scheme used to identify data sets with their parameter con-
ﬁgurations. In a similar fashion to a PSE’s setup scripts, these data reduction
scripts can be diﬃcult to maintain between large experiments as conﬁgurations
change.
For several years, our research group maintained a large collection of Perl scripts
to partially automate the process of running PSEs. To reduce turnaround time be-
tween experiments, we would partition an experiment among several machines. Each
machine’s script would be manually executed on idle network machines in a high-
use multi-user environment. From these experiences, we learned that distributing a
computational load has its own unique challenges:
• Distribution Algorithms - When running distributed processes on multiple ma-
chines, it is essential to have some way of determining which tasks get assigned
to which machines. Ineﬃcient procedures waste valuable resources, but eﬃcient
procedures can be either time-consuming to set up manually or diﬃcult to set
up automatically. A recent increase in grid computing availability has suggested
that distributed algorithms should take advantage of common grid protocols.
This introduces an additional diﬃculty, since diﬀerent grids can operate under
diﬀerent job-control protocols.
• Robustness in Job Management - While remotely executing a trial, there are
a number of normal ways in which a machine or algorithmic process can fail
without implying a ﬂaw in experiment design. This means that a distributed
experiment may not complete unless the experimental method gracefully han-
dles such failures. In our group, this often involved the manual re-execution
of trials until successful completion of an experiment. Such a procedure de-
mands complex scripts with conditional behavior and methods for determining
the validity of collected experimental data. Since the distribution and validation
processes were written for a speciﬁc experiment, signiﬁcant retooling time was
spent on these scripts between experiments.
• Data Collection - Any data that is generated during a trial is typically stored lo-
cally on a machine. While this method poses no problems for small experiments,
large distributed experiments should have some way of consolidating data from

168
Michael E. Samples, Matthew J. Byom, and Jason M. Daida
all of the machines used in an experiment. The data transmission process is
susceptible to errors and thus this process should be robustly designed.
Our research group has experienced all of these diﬃculties associated with the
remote distribution of PSEs. Our scripts for conducting PSEs grew from collections
of scripts to manage smaller experiments. When we needed to make modiﬁcations
to an experiment, we would change the scripts. When trials inevitably failed (for
reasons completely divorced from GEC, such as running out of disk space), we manu-
ally solved the problem and restarted portions of our experiment. We stress that the
time-consuming aspect of experiment administration should not be underestimated,
particularly when the addition of robust behavior to experiment scripts increases
their complexity. These diﬃculties, which are inherent to parameter sweeps and
distributed computation, led us to explore automated experiment administration.
2.5 Major Design Criteria
We have developed the following three criteria for a generic distributed parameter
sweep engine. Namely, a sweep engine should:
• Facilitate the easy setup of new PSEs. In deﬁning experiments, a user should
not need to write a script to generate a list of parameter conﬁguration combi-
nations. Instead, a sweep engine should derive parameter conﬁgurations from a
simple grammar provided by a user. Existing algorithms that a user may already
possess should not require modiﬁcation to be used in a PSE. There should be no
reason that the program we are executing—such as a GEC algorithm—needs to
be rewritten or extensively modiﬁed before it is used with the parameter sweep
engine. Of the previous work, only Drone and Nimrod/G met these standards
for easy experimental setup.
• Operate transparently and seamlessly as middleware across many types of com-
puting networks. It should be able to take advantage of large-scale grid networks
but should also be able to take advantage of local clusters and available desk-
top machines. In taking advantage of these remote resources, a user should not
need to specify or submit experiments in diﬀerent manners—all remote distrib-
ution should remain transparent to a user. This system should be robust; data
integrity should be veriﬁed at multiple steps.
• Support automated data reduction and analysis. When an experiment concludes,
a parameter sweep engine should perform initial steps of data reduction. A
sweep engine should use the knowledge of the parameters under study to create
graphs and charts as requested by a user in advance. Previously, only Nimrod/G
attempted this feature. A simple, integrated analysis tool to create graphs and
charts would allow a user to quickly see results from a PSE. This feature would
be particularly valuable for GEC investigators in both problem feedback and
ﬁnal analysis. These standard graphical tools could encapsulate overwhelming
amounts of information immediately available after a GEC experiment, thereby
giving a user invaluable feedback about the correctness of the results.
In fulﬁlling each of the above design criteria, a parameter sweep engine should
act as a simple-to-use generic tool for scientiﬁc exploration of parameterized com-
putational problems. It is presumed that users of this tool have little interest in
writing setup algorithms, distribution algorithms for multiple grid protocols, and

Parameter Sweeps for Parameter Space Exploration
169
large analysis scripts. As such, a parameter sweep engine should hide the tedious
operational decisions from a user. This allows a user to spend time and eﬀort on
design and analysis instead of tedious experiment administration.
Although our software solution is not novel in either its ability to perform large
PSEs or its ability to run processes on distributed systems, its blend of automated
parameter sweep experimentation with transparent, robust, distributed computation
is unique. In the next section, we discuss the implementation-level details of our
software solution.
3 Commander: Parameter Sweep Engine
Commander is our software solution that meets the criteria developed in Section
2.5 for a distributed parameter sweep engine.
The Commander system consists of one computer running a host program and an
arbitrary number of computers independently running a client program. The host is
manually launched by a user and has complete knowledge of the current experiment.
Clients may be started manually, through scripts on cluster machines, or through
common grid protocols. Clients have no initial knowledge of any existing PSEs;
instead they rely upon the host to provide them with instructions to perform trials.
Unlike several previous solutions, Commander relies on no speciﬁc grid technology
to distribute its workload. Instead, Commander’s distributed system is modeled
with a master-worker farm architecture, popularized by the SETI@home project.
How the Commander system works can be brieﬂy outlined as follows. A user
creates an experiment conﬁguration ﬁle that the Commander host parses to obtain
necessary information about how to conduct a speciﬁc PSE. A user then launches
the client software on an arbitrary number of computers. Each client is provided
with the IP address of the host machine. Through a process described in Section
3.2, the clients individually begin to run trials to complete the PSE. As the clients
complete each trial, results data is stored on the host machine for data reduction
and subsequent analysis. Clients continue to perform trials until the PSE is ﬁnished.
To allow clients to run on as many platforms as possible, Commander was writ-
ten entirely in Python, a cross-platform interpreted language. Consequently to run
a client, a computer only needs to have a local Python interpreter and network
access to the host computer. Our research group regularly runs clients on various
Unix, Linux, and OSX platforms, spread across diﬀerent grid architectures, cluster
networks, and desktop computers. In Section 2.5, we described our three criteria
for a distributed parameter sweep engine: experiment construction, support for dis-
tributed computing, and data validation and analysis. Figure 2 shows our design
of Commander’s architecture in response to those criteria. The following sections
subsequently detail how Commander’s design meets those criteria.
3.1 Experiment Construction with Commander
The ﬁrst design criterion is that a parameter sweep engine should facilitate the easy
setup of new PSEs. We consequently designed a simple procedure for a PSE to be
run on Commander. A user creates a conﬁguration ﬁle that Commander parses to
create a new PSE. In this ﬁle, a user can deﬁne what parameter values to sample

170
Michael E. Samples, Matthew J. Byom, and Jason M. Daida
during a PSE, and the number of trials desired for each datapoint. Also in the
conﬁguration ﬁle, a user can deﬁne the location of a project repository that contains
all project-speciﬁc ﬁles needed to run the experiment.
The project repository is commonly made available through traditional ﬁle repos-
itory software (such as CVS [20] or subversion [12]), and may be run on a diﬀerent
computer to conserve host bandwidth. The project repository contains four types of
items:
• Project Source Code (Required) - This would include all of the ﬁles needed to
interpret, compile, link, and/or execute an experiment’s trial. This could include
C++ source code, Java class ﬁles, data input ﬁles, and any required libraries.
At runtime, a project’s main executable—a GEC algorithm—should be able to
accept as input either the name of a data ﬁle (containing runtime parameters)
or a list of command-line runtime parameters. Since clients may run on any
architecture, project algorithms should also be capable of running on any ar-
chitecture. The use of source code in this process is particularly recommended
because fresh compilations can sometimes take advantage of specialty hard-
ware. Cross-platform compilation frequently requires more eﬀort in algorithm
construction, but a user can usually be aided by an array of compilation and
conﬁguration utilities.
• Conﬁguration Scripts (Optional) - Before executing a PSE trial on a client
computer, it may be necessary to perform platform-speciﬁc conﬁguration and
compilation routines. A user should include such custom conﬁguration scripts,
so that an algorithm would be ready to run after the successful conclusion of a
conﬁguration process. This stage is frequently accomplished through common
build utilities, such as Make or Apache ant. As the complexity of the underlying
source code increases, the conﬁguration utilities also become more complicated,
particularly when cross-platform compatibility is desired. Tools such as GNU
autoconf and automake can greatly simplify this process and help programmers
write portable conﬁguration utilities. These conﬁguration scripts could then
perform additional work before attempting code compilation.
• Validation Script (Optional) - After the successful conclusion of a trial, a client
can optionally perform veriﬁcation that the data output appears correct. When
operating on a heterogeneous (and sometimes hostile) multi-user network, trials
may not correctly ﬁnish. Local users can kill processes and machines may halt
or run out of memory. It is possible to have the centralized host check for data
errors, but it is more eﬃcient to have the numerous clients each perform their
own validation procedure on any locally generated data before that data is
returned to the host.
• Analysis Measures (Optional) - Commander can automatically perform basic
types of data reduction and generate graphs to display results for analysis.
Project-speciﬁc analysis measures would be included in the project repository
to analyze the data produced by a user’s speciﬁc project. Analysis measures
would usually be used to compute a project-speciﬁc statistic from results col-
lected for a datapoint. For example, in a GEC experiment, it may be useful
to have an analysis measure which computes the average best-of-trial ﬁtness
for a datapoint. Commander could then employ such an analysis measure to
display the average best-of-trial information for each datapoint in the PSE.
These analysis measures are discussed in greater detail in Section 3.3. On a side

Parameter Sweeps for Parameter Space Exploration
171
note, all statistics on experimental results presented in Section 4 were originally
generated by the automatic execution of Commander analysis measures.
To ﬁnish writing a conﬁguration ﬁle, a user would append information about
the ﬁle structure of the project repository. Speciﬁcally, a user could include the
names of any optional conﬁguration scripts, the name of an algorithm executable
(the resultant binary name after the conﬁguration script is executed), and the names
of the optional validation scripts.
For many projects, the project repository would not drastically change between
experiments. A key diﬀerentiator between experiments lies in which datapoints are
examined. When creating an experiment conﬁguration ﬁle, a user deﬁnes the set
of datapoints used in a PSE by deﬁning the sets of values to which parameter
conﬁgurations can be set. Experiment parameters can be of three types:
• Option - This type of parameter has no associated value. Every datapoint in a
PSE would have this option and thus every trial that is executed would run with
this parameter. An example of a command line option from MGP (a genetic pro-
gramming engine) could be “mgp -useLowMem”, which tells the engine to make
tradeoﬀs favoring a small memory footprint over execution speed. Regardless
of any other parameters subsequently issued, each trial that is conducted by
Commander would subsequently be run with the “-useLowMem” ﬂag.
• Constant - This type of parameter has a value that is constant for every dat-
apoint in the PSE. An example from MGP could be “mgp -maxGenerations
200”.
• List - The list parameters deﬁne the parameter conﬁgurations over which Com-
mander will sweep. These are like constants in that they have a value. Unlike
the constant parameters, however, diﬀerent datapoints can have diﬀerent para-
meter values. For example, sweeping over population size {1000, 5000} in MGP
would generate two unique datapoints with the command lines “mgp -popSize
1000” and “mgp -popSize 5000”.
When creating experiments, Commander creates one datapoint for each element
in the Cartesian product of the List-type values. Given that list of datapoints, Com-
mander than adds to each datapoint the set of constant and option parameters.
After creating a set of datapoints for a PSE, Commander creates trial packages
for each datapoint. A trial package consists of all information needed to conﬁgure,
start, and analyze a single trial—e.g., a URL of a project repository, a parameter
conﬁguration, an RNG seed, and the names of the conﬁguration, execution, and vali-
dation scripts. When the host and clients are operational, clients receive compressed
trial packages and use the contained information to execute a trial.
Table 1. Sample parameter types in an experiment conﬁguration ﬁle. This experi-
ment would contain four datapoints, representing the Cartesian product of parame-
ter sets “-popSize” and “-selectionMethod”.
Parameter Type
Name
Value
Option
useLowMem
None
Constant
maxGenerations
200
List
popSize
{1000, 5000}
List
selectionMethod {Tournament, Fitness-Proportionate}

172
Michael E. Samples, Matthew J. Byom, and Jason M. Daida
While it is possible to have only one trial package per datapoint, it is necessary in
most GEC experiments to create multiple trial packages to achieve accuracy in later
statistical analysis. After a user speciﬁes the number of trials per datapoint, Com-
mander adds trial packages with unique RNG seeds to each datapoint. For example,
Table 1 contains a parameter sweep encoding for four datapoints. If the number of
trials per datapoint is ten, then Commander would create 40 trial packages, each
with unique RNG seeds. The assignment of RNG seeds is particularly important in a
distributed computing environment: if two clients simultaneously run the same trial
package, the results will not be identical unless both applications use the same RNG
seed. Consequently, if the host receives diﬀerent results for the same trial package,
it might not be able to determine which trial package’s data to accept without in-
troducing sampling bias. Since an RNG seed is permanently associated with each
trial package, every client that would complete the same trial would return the same
data in correspondence with the trial package’s RNG seed.
Commander manages RNG seeds automatically. If at a later date this experiment
is rerun, there is no danger that Commander would accidentally reassign the same
RNG seeds. However, experiment ﬁles are saved for reuse and Commander does
allow the reuse of old RNG seeds at a user’s option.
After generating trial packages for each datapoint, Commander saves the exper-
iment to a local directory on the host machine for later recovery. At this point, the
host is able to start serving the experiment and only waits for clients to connect and
request trial packages.
3.2 Commander’s Distributed System Architecture
The second design criterion is that a parameter sweep engine should operate trans-
parently and seamlessly as middleware across many types of computer networks.
Consequently, Commander uses a master-worker architecture that has been designed
for cross-platform compatibility and robust behavior to network failures.
In general, master-worker architectures have been shown to be particularly ro-
bust and eﬃcient (e.g., in [19]) when the master relies on no single worker and each
worker can complete trials without employing complicated scheduling algorithms on
the host. These conditions are generally satisﬁed in a PSE because individual trials
are decoherent—i.e., the order of trial execution does not inﬂuence the PSE results—
and workers may be eﬃcient by starting a new trial immediately after the conclusion
of a current trial without requiring the host to rerun a scheduling algorithm.
The master-worker architecture of Commander functions as follows. The host
maintains both a list of uncompleted trial packages and a ﬁlesystem directory of data
packages that are generated by the results from completed trial packages. Comman-
der client programs are launched on an arbitrary number of computers. Each client
ﬁrst conducts an initialization procedure, which includes creating a temporary direc-
tory somewhere on that client machine’s ﬁlesystem. The location of this temporary
directory is machine-speciﬁc, and can depend on how a client was locally installed.
Clients use this directory to store project-speciﬁc ﬁles and any data ﬁles produced
during a trial.
When ready to begin executing trials, each client connects to the host using an
XML-RPC protocol and downloads a randomly-selected trial package. Upon receipt
of a trial package, each client subsequently processes the package’s contents to obtain

Parameter Sweeps for Parameter Space Exploration
173
Fig. 2. Commander’s Architectural Diagram. Commander clients connect to the
host, receive trial packages, conﬁgure packages, run trials, and return data packages.
The master-worker architecture allows an arbitrary number of clients to be used in
running experiments.
algorithm source code from the project repository, conﬁgure the source code, and
execute an algorithm with the speciﬁed parameter conﬁguration and RNG seed. In
the event of an error, a client returns any error messages to the host and requests a
new trial package.
During the execution of a trial, an algorithm needs to write all output ﬁles to
that client’s temporary directory (which was created by Commander). After a trial
concludes and any data is analyzed for correctness, optional data reduction scripts

174
Michael E. Samples, Matthew J. Byom, and Jason M. Daida
are executed. A client then places all available results in a data package that is
subsequently transferred back to the host using the XML-RPC protocol. After the
host receives this data package, the data is stored in the host’s local data directory
for later analysis. The host then removes the corresponding trial package from the
list of uncompleted trial packages.
After successful transfer of the data package, a client deletes all local copies of
its data, but temporarily retains its copy of the project repository since it would
likely need that information again for another trial. Each client repeats the process
of “fetch trial package, decrypt, fetch experiment ﬁles, conﬁgure, execute, validate,
analyze, encrypt, and return data package” until no more uncompleted trial pack-
ages remain on the host. Each client then shuts down, removing all project-speciﬁc
material and any working directories for the purpose of returning a client’s local
ﬁlesystem’s directory structure back to an original state.
In some cases, a user may desire secure communications between clients and
the host. This would be particularly important on a multi-user network, such as
those found on many university campuses. Host-client message encryption would be
important for three reasons. First, encryption can help thwart man-in-the-middle
attacks, which could induce clients to execute arbitrary code. Second, it guarantees
that only trusted clients (i.e., those with the appropriate AES key) would be able
to return results to the host machine. Third, the security of conﬁdential algorithm
source code would be restricted to trusted clients. Commander optionally supports
AES encryption, but requires that AES keys be manually distributed to client com-
puters before the start of an experiment. Our research group uses AES keys to secure
communications on our university’s computer network. We normally distribute AES
keys through a shared ﬁlesystem.
3.3 Data Reduction and Analysis with Commander
The third design criterion is that a parameter sweep engine should support auto-
mated data reduction and analysis. Consequently, Commander has provisions that
automatically collate and process data. Furthermore, Commander also has provi-
sions for arbitrary visualizations to be generated. To understand how this is done,
we describe a formal approach that allows for an otherwise intuitive process to be
automated.
Although visualizations between parameters and resulting output data are help-
ful, the high-dimensional nature of GEC parameters and observations precludes
the existence of a simple visualization strategy. Commander attempts to assist a
researcher by preparing certain visualizations that are available for browsing imme-
diately after the conclusion of an experiment. The types of graphs that are produced
are based on mathematical constructs that are deﬁned by a user and speciﬁed within
an experiment conﬁguration ﬁle.
In mathematics, a graph (or more precisely, a map) is deﬁned as a set of points,
G =

(d, r)
 (d, r) ∈D × R, and P(a, b) is True

,
(1)
for a domain D, a range R, and a property P that determines whether the point
(d, r) exists in the graph. Every datapoint D ∈D∗is represented by a (d, r)-tuple
in each graph type. The domain and range of each datapoint’s representation is a
user-deﬁned function of the form:

Parameter Sweeps for Parameter Space Exploration
175
F : D∗→D × R.
(2)
Usually, a user’s domain and range mapping takes the form:
F(D) =

FD

CD, ΩD

, FR

CD, ΩD

,
(3)
where user-written functions FD and FR operate on both a datapoint D’s parameter
conﬁguration CD and the collection of observations Ω(D), which is available from
all of the datapoint’s trials.
After mapping each datapoint to domain and range values, Commander could
create a single graph deﬁned as:
G = 
F(D)
 D ∈D∗
.
While such a conglomeration would display characteristics from every data-point, it
is only useful for graphs where the domain is a low-dimensional space. A common
technique to visualize domain-range relationships embedded in higher dimensions is
to use multiple graphs. Each datapoint is represented in only one graph, and the
choice of graph membership is made based on its domain classiﬁcation. For example,
to view 4D data, one could construct a set of 3D graphs where each graph expresses
a particular value for some fourth dimensional parameter in the domain.
Commander supports such higher-dimensional graph visualizations using math-
ematical concepts of partitions and equivalency relations. A set S of nonempty sets
of datapoints is called a partition of D∗if for some sets of datapoints s1, s2 ∈S,
s1 ∩s2 = ∅and ∪s∈Ss = D∗. A user deﬁnes an equivalence relation EQ such that
whenever datapoints D1, D2 ∈D∗are equivalent (i.e., D1 ≡EQ D2), they belong to
the same partition set. Since the partition set is fully speciﬁed by EQ, we can denote
the partition set as SEQ. Then, for each set of datapoints s ∈SEQ, Commander
deﬁnes a graph:
Gs =

F(D)
 D ∈s ⊆D∗
.
(4)
For each created graph Gs, Commander saves a list of the graph’s points to disk
in the host’s local directory where such data can later be viewed and interpreted by
other programs.
These formalisms do not by themselves present a new way of examining exper-
imental results. However, the development of a formal treatment for interpreting
datapoints allows Commander to use a generic process for each PSE. This helps
to simplify analysis methods for investigators who would otherwise need to develop
scripts to handle data set partitioning and domain and range classiﬁcation for each
datapoint.
To revisit the earlier example from Table 1, we could choose to partition D∗
using an equivalency relation where two datapoints are considered equivalent if
and only if they have the same value for the “selectionMethod” parameter. This
would produce two partition sets: one partition set would contain datapoints with
“selectionMethod” equal to Tournament while the other partition set would contain
datapoints with “selectionMethod” equal to Fitness-Proportionate. We could then
deﬁne the domain function FD as returning the value for the parameter conﬁguration
“popSize”. Such a conﬁguration would create two 2D graphs, each of which would
have values for “popSize” as their domain.

176
Michael E. Samples, Matthew J. Byom, and Jason M. Daida
4 Parameter Sweep Examples
In this section, we demonstrate the use of Commander on a sample PSE. For this
experiment, we examine the eﬀect of selection and replacement methods on two
common GP test problems.
A number of authors [4, 7, 9, 18, 27, 31] have studied the behavior of selec-
tion and replacement methods from both mathematical and empirical perspectives.
Although their discussions of factors such as ﬁtness distribution, takeover time, selec-
tive pressure, and loss of diversity have fostered an understanding of these methods’
basic behaviors, the eﬀects of combining various selection and replacement strate-
gies turns out to be not well understood. For example, although an empirical study
could reveal population convergence traits of an algorithm conﬁgured to use tour-
nament selection with steady-state replacement, that empirical study would reveal
little information about the behavior of a similar algorithm conﬁgured to use ﬁtness-
proportionate selection with a generational replacement strategy. In general, as a
larger number of parameter values are modiﬁed between experiments, the results
become increasingly diﬃcult to predict.
To demonstrate the utility of PSEs in understanding the nonlinear interactions
between various GEC parameters, we studied two well-known GP problems while
sweeping across various parameter values of diﬀerent selection and replacement
methods. We used 6-input multiplexer and 4-bit parity as test problems because they
have been shown to be tunably-diﬃcult under varying population size and maximum
number of generations (e.g., [24]). We conﬁgured 6-input multiplexer to use logical
NAND and NOR as functions while 4-bit parity used AND, OR, NAND, and NOR.
Our selection methods included tournament (q = 7) and ﬁtness-proportionate selec-
tion and our replacement methods included both a generational and a steady-state
strategy. Trials were conducted with the MGP engine and input parameters were
chosen from the available selection methods, the available replacement methods, and
diﬀerent combinations of population size and maximum number of generations. The
PSE’s parameters are described in Table 2.
In this experiment, Commander swept over 2 problems, 2 selection methods, 2
replacement methods, 10 population sizes and 10 maximum allowable generations
for a total of 800 datapoints. Each datapoint used 100 trials for statistical validity.
Commander distributed the 80,000 trial packages over 50 clients. The experiment
took 2 CPU-months, but was completed, with analysis, within 3 days.
Table 2. Parameter conﬁgurations for parameter sweep experiments with the com-
mon GP test problems 6-input multiplexer and 4-bit parity.
Type
Field Name
Value
Constant
Max tree depth
512
List
Population Size
{2x  2 ≤x ≤11, x ∈Z}
List
Max Generations
{2x  0 ≤x ≤9, x ∈Z}
List
Selection Method
{tournament, ﬁtness-proportionate}
List
Replacement Method
{generational, steady-state}
Commander Setting Trials per datapoint
100

Parameter Sweeps for Parameter Space Exploration
177
Table 3 contains the total number of successful trials for each conﬁguration class,
grouped by selection and replacement methods. The results clearly show that some
conﬁgurations are more diﬃcult than others by a few orders of magnitude.
Figures 3 and 4 represent the total number of successes with a more ﬁne-grained
analysis in which the number of successes is measured as a function of population
size, maximum allowed generations, and choice of selection and replacement meth-
ods. The data presented in Figures 3 and 4 was produced using the procedure for
automated data reduction as described in Section 3.3. For these ﬁgures, the range
value of each datapoint is merely the number of successes summed over all trials
associated with that datapoint. The domain value of each datapoint is represented
by the parameter conﬁguration combination, but partitioned into separate graphs
based on the selection and replacement methods. Datapoint-based statistics are nat-
urally algorithm-speciﬁc due to the data formatting policies of the algorithm under
study. Although new experimental algorithms used in a PSE usually incorporate
new domain and range functions, those domain and range functions can be reused
in later experiments with the same algorithm.
5 Discussion
Observation: Knowledge of selection or replacement methods by themselves is not
suﬃcient in predicting which combinations of the two will yield the better perfor-
mance. The relative ordering of parameter conﬁgurations based on success rate in
Table 3 cannot be explained by simple linear eﬀects acting independently on each
parameter. For both the multiplexer and parity test problems, tournament selec-
tion outperformed ﬁtness-proportionate selection in every conﬁguration class. How-
ever, a similar statement cannot be made for replacement methods. When paired
with tournament selection, generational replacement achieved more successes than
steady-state replacement. However, when paired with ﬁtness-proportionate selec-
tion, steady-state replacement outperformed generational replacement. Researchers
who assume that parameter choices exert linear and independent inﬂuences on the
results would be surprised to learn that generational replacement has a beneﬁcial
eﬀect on success when tournament selection is used but an adverse eﬀect when
ﬁtness-proportionate selection is used. Although it is beyond the scope of this chap-
ter to analyze the cause of this, this example does illustrate the need to conduct
PSEs to characterize the mapping between parameter conﬁgurations and results.
Otherwise, a GEC experiment runs the risk of being too small.
Table 3. Total number of successes out of 10,000 possible trial successes per conﬁg-
uration summed for each conﬁguration class and presented for both 4-bit parity and
6-input multiplexer. The conﬁgurations exhibit the same ordering of success counts
for both test problems.
Conﬁguration Class
Parity Success Multiplexer Success
Tournament, Generational
1066
1739
Tournament, Steady State
780
1609
Fitness-Proportionate, Steady State
43
76
Fitness-Proportionate, Generational
4
21

178
Michael E. Samples, Matthew J. Byom, and Jason M. Daida
Fig. 3. Contour plots of success counts for the GP test problem 6-input multi-
plexer. Success counts are represented as a function of membership in a parameter
conﬁguration class. Data was collected from a PSE described in Table 2 using 40,000
trials.
In general, there are three ways in which GEC experiments can be too small:
• The stochastic nature of GEC algorithms implies that observations collected
from merely a few trials may not have a conﬁdence level high enough to be
statistically signiﬁcant. The use of larger numbers of trials allows a researcher
to generate statistics that approximate the real unobservable behavior of a par-
ticular parameter conﬁguration.
• It is diﬃcult to characterize the unknown (and likely nonlinear) behavior that is
associated with a set of parameters with only a small number of datapoints. To
characterize and predict nonlinear behavior, one needs to sample enough values
to understand the degree of nonlinearity involved. This is particularly true if
modiﬁcation to the value of a parameter can induce an abrupt transition on
the observed behavior. If the mapping between an algorithm’s parameter con-
ﬁgurations and search behavior contains abrupt, nonlinear transitions, there is
the possibility that theory derived from experimental results may be conditional

Parameter Sweeps for Parameter Space Exploration
179
Fig. 4. Contour plots of success counts for the GP test problem 4-bit parity. Success
counts are represented as a function of membership in a parameter conﬁguration
class. Data was collected from a PSE described in Table 2 using 40,000 trials.
only on a certain set of parameter values. Consequently, the failure to identify
nonlinear behaviors because of an inadequate sampling of a parameter space
may lead to ﬂawed assumptions regarding corresponding theories. Extrapola-
tion of theory across multiple transitions ﬁrst requires that the location of the
transitions be identiﬁed—a process that can only happen while sampling mul-
tiple datapoints for each parameter. Conversely, conducting experiments with
many datapoints increases an investigator’s ability to locate transitions and
account for them when developing theory.
• It is diﬃcult to understand correlations between parameters without conducting
PSEs with multiple parameters. Consequently, experiments that involve too few
algorithmic parameters might not be capable of generating results from which
inferences about causality and correlations can be made. As demonstrated by
this chapter’s case study, parameters exhibit correlated and dependent behav-
ior in inﬂuencing experimental results. Such results are based not solely on the

180
Michael E. Samples, Matthew J. Byom, and Jason M. Daida
behavior of one parameter, but on the coupled behavior of a collection of para-
meters that together inﬂuenced outcomes. A researcher who develops a theory
of behavioral mappings between parameter conﬁgurations and results should ac-
count for such correlations between the parameters. Such correlations may also
exhibit nonlinear eﬀects resulting in transitions in higher-dimensional spaces.
Without a theory in place, these multidimensional transitions can only be lo-
cated by performing PSEs.
In highly nonlinear spaces, even minor modiﬁcations to an algorithm’s parame-
ter conﬁgurations can result in large qualitative changes in an experiment’s results.
Table 3 exhibits orders-of-magnitude improvements in trial success rate merely by
switching the selection and replacement methods used in each experiment. This sug-
gests that researchers may need to reconsider their notions of signiﬁcant improve-
ments in GEC research. Small performance improvements might not be signiﬁcant
when compared to the order-of-magnitude changes that can be discovered through
the use of a PSE to locate transitions. Furthermore, it is diﬃcult to judge the ef-
ﬁcacy of GEC algorithm modiﬁcations without using PSEs because any beneﬁts
gained from such modiﬁcations might be dependent on a speciﬁc class of conﬁgura-
tion settings. Consequently, such modiﬁcations could lose their advantage when an
algorithm is conﬁgured with diﬀerent parameter values.
We are aware that these suggestions for improved GEC methodology involve
signiﬁcant work with standard tools. Consequently, to provide a “big-picture view,”
we have presented a parameter sweep engine, Commander, to automatically perform
many of the tedious tasks involved with administrating, executing, and analyzing
PSEs. The process of constructing and analyzing experiments should not be tedious.
Likewise, there is no fundamental reason to suggest that network trial distribution
should be an inherently diﬃcult task. We hope that tools like Commander enable
and encourage the community to perform large PSEs, especially since Commander
was intentionally designed to easily integrate with existing approaches to conducting
GEC research (e.g., it has already been used with the popular GEC algorithms ECJ
[22] and lilgp [34]).
6 Conclusions
Parameter sweep experiments are useful. The presence of nonlinearities inherent
to the lower-level interactions of GEC algorithms implies that small experiments
can yield an incomplete view of GEC dynamics. Large PSEs can examine multiple
conﬁgurations to determine inﬂuential parameter settings. This process can lead
to a more complete mapping of parameter conﬁgurations to results and a deeper
understanding of GEC theory.
Parameter sweep experiments are also challenging. This methodology requires
that a GEC algorithm can accept externally speciﬁed parameter values and that a
researcher can deﬁne a script that can iterate over collections of parameter conﬁg-
urations. Conducting these experiments is expensive in terms of both human labor
and computational costs because these experiments have a high administration over-
head and can require many CPU hours to conduct an inherently large number of
trials. An increased number of trials allows for a greater chance that errors can oc-
cur, which in turn could invalidate results. Further detection and resolution of these

Parameter Sweeps for Parameter Space Exploration
181
errors requires more scripts and more human labor. Distributed computation can al-
leviate the pains of lengthy trials, but introduces time-consuming code development
for network distribution and robust data collection.
Commander is our solution to a generic parameter sweep engine. Commander is
designed to automate as much of the experimentation process as possible by allevi-
ating tedious human tasks, providing robust remote trial distribution, and ensuring
validity in data collection. Through simple interfaces, Commander allows researchers
to quickly deﬁne new experiments, execute them, and use analysis measures to per-
form immediate data reduction and quickly see results. This software addresses many
of the challenges commonly associated with large PSEs. This should encourage re-
searchers to perform more experiments that examine nonlinear correlations between
multiple parameters in the conﬁgurations of GEC algorithms.
Commander has been publicly released under an open-source GPL license. Ver-
sioned downloads and current development status can be found on our website at
http://sitemaker.umich.edu/umacers/commander
7 Future Work
There are numerous ways to increase the eﬀectiveness of automated experimenta-
tion. Currently, Commander conducts a factorial design experiment that is based on
a user’s parameter inputs. Future versions could support other optional design-of-
experiment techniques, such as fractional factorial design or Latin Squares to reduce
the requisite number of datapoints for a particular experiment. Further automation
is possible if data analysis stages are allowed to provide feedback in Commander’s
subsequent choice of datapoint parameter conﬁgurations. Future versions of Com-
mander could perform analysis of variance tests and statistical correlation tests to
determine how many times to sample a stochastic datapoint and which new para-
meter conﬁgurations to sample for new datapoints. Such variance and correlation
statistics could also be reported to a user without necessitating direct requests for
such analysis.
Finally, observational data frequently exists in high-dimensional results spaces,
which makes analysis diﬃcult. Standardized techniques exist for data dimension-
reduction (e.g., principal component analysis or manifold learning), which could
take advantage of correlations in a results space to identify the structure of the
mapping between parameter conﬁgurations and results. Such a structural mapping
could then provide a deeper theoretical insight into the behavior of the algorithm
under study.
8 Acknowledgments
The authors would like to thank the other members in our research group, UM-
ACERS, for computational support and helpful comments on an earlier draft of this
chapter. The ﬁrst author would like to thank his colleagues at the Toyota Techni-
cal Center for their discussions. The ﬁrst and third authors gratefully acknowledge
I. Kristo.

182
Michael E. Samples, Matthew J. Byom, and Jason M. Daida
Appendix: The Highlander problem
The Highlander GP problem was designed to test the limits of building block mixing
and assembly by coercing a GP system to attempt to construct a solution with
certain characteristics, as determined by a tunable parameter β ∈[0, 1].
The Highlander problem proceeded as follows. An initial population of trees was
constructed from the arity-2 function set {Join} and the terminal set {x}. During
the course of evolution, these trees were not evaluated for content. Instead, each
tree was assigned ﬁtness based on the number of ancestral building blocks that
participated in the composition of the current tree.
To audit this procedure, every node of every tree in the initial population was
assigned a unique integer ID number. Subsequent generations were constructed using
only the crossover operation—no mutation was applied. The crossover operation was
modiﬁed to preserve the integer ID of each node. Consequently, it was possible to
examine a tree in a later generation and determine the initial population ancestor
of each node in that tree.
Fitness was based on the fraction of initial population nodes found within each
tree. In particular, a percentage p was calculated, representing the percentage of
nodes from the initial population found within the current tree. Raw ﬁtness was
established as
f = |β −p|.
(5)
In this fashion, a tree attains maximum ﬁtness when p = β, i.e., when the
fraction of initial population genetic material within a tree reaches a user-speciﬁed
value. A value of β ≈1 would cause GP to attempt to evolve a solution using all
of the nodes available from the initial population. Likewise, a value of β ≈0 would
cause GP to evolve a solution using very little initial population material.
We found that the Highlander problem exhibits at least two transition regions.
As shown in Figure 1, there are values of β for which GP can easily ﬁnd a solution.
This region of the settings for β is, however, small—there is a much larger region of
β-values for which GP is unable to evolve a successful solution. A second transition—
based on the population size—was identiﬁed that determined the behavior of the
β-transition. The ﬁndings presented shed light on the ease at which a GP system
may incorporate initial population material into any single tree of a later generation.
Speciﬁcally, the Highlander problem suggests a critical point at the β-transition as
an upper bound for the fraction of initial population material available to a later
generation solution.
We refer the interested reader to our full discussions in [15].
References
1. D. Abramson, R. Sosic, J. Giddy, and M. Cope. The laboratory bench: Dis-
tributed computing for parametised simulations. In Proceedings from the 1994
Conference on Parallel Computing and Transputers, pages 12–27, 1994.
2. David Anderson. Boinc documentation. http://boinc.berkeley.edu, 2005.
3. Robert Axelrod. The dissemination of culture: A model with local convergence
and global polarization. Journal of Conﬂict Resolutions, 41(2):203–226, 1997.

Parameter Sweeps for Parameter Space Exploration
183
4. Thomas B¨ack. Selective pressure in evolutionary algorithms: A characteriza-
tion of selection mechanisms. In Proceedings of the First IEEE Conference on
Evolutionary Computation, pages 57–62, Piscataway, NJ, 1994. IEEE Press.
5. J. Basney, M. Livny, and P. Mazzanti. Harnessing the capacity of computa-
tional grids for high energy physics. In Proceedings of the 2000 Conference on
Computing in High Energy and Nuclear Physics, 2000.
6. Ted Belding. Drone manual. http://drone.sourceforge.net, 2005.
7. Tobias Blickle and Lothar Thiele. A comparison of selection schemes used in
genetic algorithms. TIK-Report 11, Technische Informatik und Kommunikation-
snetze Institut, Computer Engineering and Networks Laboratory, ETH, Swiss
Federal Institute of Technology, Gloriastrasse 35, 8092 Zurich, Switzerland, De-
cember 1995.
8. Sven A. Brueckner and H. Van Dyke Parunak. Resource-aware exploration of
the emergent dynamics of simulated systems. In Proceedings of the Second In-
ternational Joint Conference on Autonomous Agents and Multiagent Systems,
pages 781–788. ACM Press, 2003.
9. Erick Cant´u-Paz. Order statistics and selection methods of evolutionary algo-
rithms. Information Processing Letters, 82(1):15–22, 2002.
10. Henri Casanova, T. Bartol, J. Stiles, and Francine Berman. Distributing mcell
simulations on the grid. International Journal of High Performance Computing
Applications, 15(3):243–257, 2001.
11. Henri Casanova, Graziano Obertelli, Francine Bermand, and Richard Wolski.
The apples parameter sweep template: User-level middleware for the grid. In
Proceedings of Super Computing 2000, pages 75–76, 2000.
12. Ben Collins-Sussman, Brian Fitzpatrick, and C. Pilato. Version control with
subversion. http://svnbook.red-bean.com, 2005.
13. Jason M. Daida, Derrick S. Ampy, Michael Ratanasavetavadhana, Hsiaolei Li,
and Omar A. Chaudhri. Challenges with veriﬁcation, repeatability, and mean-
ingful comparison in genetic programming: Gibson’s magic. In Proceedings of the
Genetic and Evolutionary Computation Conference, volume 2, pages 1851–1858,
Orlando, Florida, USA, 13-17 July 1999. Morgan Kaufmann.
14. Jason M. Daida, Steven Ross, Jeﬀrey McClain, Derrick Ampy, and Michael
Holczer. Challenges with veriﬁcation, repeatability, and meaningful comparisons
in genetic programming. In Genetic Programming 1997: Proc. of the Second
Annual Conference, pages 64–69, Stanford University, CA, USA, 13-16 July
1997. Morgan Kaufmann.
15. Jason M. Daida, Michael E. Samples, and Matthew J. Byom. Probing for limits
to building block mixing with a tunably-diﬃcult problem for genetic program-
ming. In GECCO 2005: Proceedings of the 2005 conference on Genetic and
Evolutionary Computation, volume 2, pages 1713–1720, Washington DC, USA,
25-29 June 2005. ACM Press.
16. Jason M. Daida, Michael E. Samples, Bryan Hart, Jeﬀry Halim, and Aditya Ku-
mar. Demonstrating constraints to diversity with a tunably diﬃculty problem
for genetic programming. In Proceedings of the 2004 IEEE Congress on Evo-
lutionary Computation, pages 1217–1224, Portland, Oregon, 20-23 June 2004.
IEEE Press.
17. Jason M. Daida, David Ward, Adam Hilss, Stephen Long, and Mark Hodges.
Visualizing the loss of diversity in genetic programming. In Proceedings of the
2004 IEEE Congress on Evolutionary Computation, pages 1225–1232, Portland,
Oregon, 20-23 June 2004. IEEE Press.

184
Michael E. Samples, Matthew J. Byom, and Jason M. Daida
18. David Goldberg and Kalyanmoy Deb. A comparative analysis of selection
schemes used in genetic algorithms. In Proceedings of the 1991 Foundations of
Genetic Algorithms, pages 69–93, San Mateo, California, 1991. Morgan Kauﬀ-
mann.
19. Jean-Pierre Goux, Sanjeev Kulkarni, JeﬀLinderoth, and Michael Yoder. An
enabling framework for master-worker applications on the computational grid.
In Proceedings from the 9th IEEE Symposium on High Performance Distributed
Computing, pages 43–50. IEEE Press, 2000.
20. Dick Grune, Brian Berliner, and JeﬀPolk. Concurrent versions system.
http://nongnu.org/cvs, 2006.
21. Eric Korpela, Dan Werthimer, David Anderson, JeﬀCobb, and Matt Lebof-
sky. SETI@Home - Massively Distributed Computing for SETI. Computing in
Science and Engineering, 3(1):78–83, 2001.
22. Sean Luke. ECJ Manual. http://cs.gmu.edu/∼eclab/projects/ecj/, 2005.
23. Sean Luke and Liviu Panait. Is the perfect the enemy of the good? In GECCO
2002: Proceedings of the Genetic and Evolutionary Computation Conference,
pages 820–828, New York, 9-13 July 2002. Morgan Kaufmann Publishers.
24. Sean Luke and Lee Spector. A revised comparison of crossover and mutation in
genetic programming. In Genetic Programming 1998: Proceedings of the Third
Annual Conference, pages 208–213, University of Wisconsin, Madison, Wiscon-
sin, USA, 1998. Morgan Kaufmann.
25. Bryan Mendez. Stardust@home. http://stardustathome.ssl.berkeley.edu,
2006.
26. Mark M. Meysenburg and James A. Foster.
Random generator quality and
GP performance. In Proceedings of the Genetic and Evolutionary Computation
Conference, volume 2, pages 1121–1126, Orlando, Florida, USA, 13-17 July 1999.
Morgan Kaufmann.
27. Tatsuyo Motoki. Calculating the expected loss of diversity of selection schemes.
Evolutionary Computation, 10(4):397–422, 2002.
28. Vijay Pande. Folding@home. http://folding.stanford.edu, 2006.
29. Norman Paterson and Michael Livesey.
Performance comparison in genetic
programming. In Late Breaking Papers at the 2000 Genetic and Evolutionary
Computation Conference, pages 253–260, Las Vegas, Nevada, USA, 8 July 2000.
30. T. Quarles, D. Pederson, R. Newton, A. Sangiovanni-Vincentelli, and Christo-
pher Wayne. Spice documentation.
http://bwrc.eecs.berkeley.edu/Classes/IcBook/SPICE/.
31. Gilbert Syswerda. A study of reproduction in generational and steady state
genetic algorithms. In Foundations of Genetic Algorithms 1990, pages 94–101,
San Mateo, USA, 15 July 1991. Morgan Kauﬀman.
32. D. Thain and M. Livny. Building Reliable Clients and Servers, chapter 19, pages
285–318. Morgan Kauﬀman, 2003.
33. George Woltman. Great internet mersenne prime search. www.mersenne.org,
2006.
34. Doug Zongker and Dr. Bill Punch. lilgp.
http://garage.cse.msu.edu/software/lil-gp/, 1998.

Adaptive Population Sizing Schemes
in Genetic Algorithms
Fernando G. Lobo1,2 and Cl´audio F. Lima1
1 UAlg Informatics Lab,DEEI-FCT, University of Algarve,
Campus de Gambelas, 8000-117 Faro, Portugal
2 Also a member of IMAR - Centro de Modela¸c˜ao Ecol´ogica.
flobo@ualg.pt
clima@ualg.pt
Summary. This chapter presents a review of adaptive population sizing schemes
used in genetic algorithms. We start by brieﬂy revisiting theoretical models which
rely on a facetwise design decomposition, and then move on to various self-adjusting
population sizing schemes that have been proposed in the literature. For each
method, the major advantages and disadvantages are discussed. The chapter ends
with recommendations for those who design and compare self-adjusting population
sizing mechanisms for genetic and evolutionary algorithms.
1 Introduction
In a traditional genetic algorithm (GA), the population size is set by the user to a
ﬁxed value at the beginning of the search and remains constant through the entire
run. Having to specify this initial parameter value is problematic in many ways.
If it is too small the GA may not be able to reach high quality solutions. If it is
too large the GA spends unnecessary computational resources. Finding an adequate
population size is a diﬃcult task. It has been shown, both theoretically and em-
pirically, that the optimal size is something that diﬀers from problem to problem.
Moreover, some researchers have observed that at diﬀerent stages of a single run,
diﬀerent population sizes might be optimal.
A somewhat widely accepted intuition behind population sizing is that it should
be set proportionally to the problem’s size and diﬃculty. However, problem diﬃculty
is very hard to estimate for real-world problems, which brings us back to the diﬃculty
of setting the appropriate population size. Faced with such diﬃculties, many users
end up either using a so-called “standard setting” (50-100 individuals), guessing a
number, or doing some experimentation with a number of diﬀerent sizes to see which
one works best. Guessing right is pure luck, and most likely a user guesses wrong
by choosing a population size that is either too small or too large for the problem
at hand. The scenario is illustrated in Figure 1.
F.G. Lobo and C.F. Lima: Adaptive Population Sizing Schemes in Genetic Algorithms, Studies
in Computational Intelligence (SCI) 54, 185–204 (2007)
www.springerlink.com
© Springer-Verlag Berlin Heidelberg 2007

186
Fernando G. Lobo and Cl´audio F. Lima
oversized
population
optimal
population size
time penalty
undersized population
time
quality penalty
quality
solution
Fig. 1. Population sizing in genetic algorithms. Too small and the user pays a quality
penalty. Too large and the user pays a time penalty. (Copied with permission from
Lobo and Goldberg [17])
Based on these observations, researchers have suggested various schemes that
try to learn a good population size during the GA run itself. The various techniques
basically diﬀer in the motivation that supports the reasoning behind each one. Typi-
cally, methods that adapt the population size are based either on biological grounds,
population sizing theory, or simply empirical evidence.
The rest of this chapter is organized as follows. We start by brieﬂy reviewing
theoretical work on population sizing which is largely based on Goldberg’s facetwise
decomposition for designing competent GAs and relies on the notion of a building
block (BB) [7]. After that, we review a number of techniques that have been pro-
posed to adjust the population size during the GA run itself. The advantages and
disadvantages of each method are highlighted along the way. The chapter ends with
recommendations for those who propose and compare adaptive population sizing
schemes for genetic and evolutionary algorithms.
2 A Brief Review of Population Sizing Theory
In this section we review relevant theoretical studies that help our understanding
on the role that the population size has in terms of GA performance and solution
quality. Most of these studies use a facetwise approach to get a better insight in pop-
ulation sizing dynamics, and for that some assumptions are made: (1) work with se-
lectorecombinative GAs (no use of mutation), (2) use ﬁxed-length and binary-coded
strings, (3) use ﬁxed-size and non-overlapping populations, and (4) solve stationary
objective functions. Although these assumptions are made for computational and
analytical tractability, most of them can be relaxed with small or no changes.
Despite these assumptions and the operational simplicity of GAs, they are still
complex systems to analyze. To have a better understanding on how to design bet-
ter GAs a decomposition approach [7] has been taken by many. In this way, it is

Adaptive Population Sizing Schemes in GAs
187
assumed that the problem to be solved is additively decomposable in a number of
m subfunctions. Each subfunction maps to k decision variables and corresponds to
a building block partition. Under this deﬁnition, it is possible to have 2k BBs in a
partition, one of which is superior to the others and belongs to the global optimal
solution.
We start by reviewing the work on initial supply of BBs. Next, the issue of decid-
ing well between competing BBs is revisited and major results are presented under
the form of dimensionless models that relate the population size (N) with certain
problem features, such as problem size (ℓ), BB size (k), number of subfunctions (or
BB partitions) (m), and others. Finally, the population size requirements for correct
identiﬁcation and mixing of BBs is addressed in the context of Probabilistic Model
Building Genetic Algorithms (PMBGAs), also known as Estimation of Distribution
Algorithms (EDAs) [16, 31].
2.1 Initial Supply of Building Blocks
When using selectorecombinative GAs (no mutation), the only source of diversity
is the supply of raw BBs in the initial generation. Once the randomly initialized
population is ﬁlled up with enough BBs, it is more likely that the GA will be able
to propagate and mix them, converging to good solutions. A simple supply model
considers the number of BBs present in the initial random population (generated
under a uniform distribution), where the probability to generate a single BB of size
k is 1/2k, for binary encodings. Therefore, the initial supply of BBs can be estimated
as x0 = N/2k. From this simple relation it is possible to notice that the population
required to supply all possible BBs grows exponentially with the BB size k. This
suggests that problems with short BBs require smaller populations than the ones
with longer BBs.
2.2 Decision Making Between Competing Building Blocks
The second aspect where population size plays an important role is in the decision-
making between competing BBs. Holland [13] early recognized this issue as a sta-
tistical decision problem. He recasted this problem as a cluster of parallel and inter-
connected two-armed bandit problems. Although this is an idealization of the GA
behavior, his results gave an optimistic bound on the allocation of trials in a GA.
De Jong [4] also used this analogy and presented equations for trial allocation that
removed some of the assumptions made by Holland [13]. He also recognized the role
of signal-to-noise ratio on the population sizing question. Years later, Goldberg and
Rudnick [9] presented a method to calculate the variance of schema ﬁtness using
Walsh transforms, and also proposed an estimate for the population size based on
the variance of ﬁtness. Following this work, Goldberg, Deb, and Clark [8] proposed
a decision-based model that gives a conservative bound on the convergence quality
of selectorecombinative GAs for a given population size. The model focuses on the
correct decision between the best BB and its closer competitor in a given partition.
This decision process takes place in the presence of noise that comes from the other
partitions.
Let us consider a competition between two individuals that contain the best BB
and the second best BB (in the partition of interest). The GA will probably choose

188
Fernando G. Lobo and Cl´audio F. Lima
the individual with the best BB given its higher ﬁtness contribution to the overall
ﬁtness. However, in a single trial, the wrong decision can also be made because
the remaining m −1 partitions also contribute to the ﬁtness of the individuals
in competition, and act as noise in the decision making process in the particular
partition. By increasing the population size, it is possible to make this decision
error as small as possible.
Based on this observation, Goldberg et al. [8] derived a population sizing equa-
tion which gave, for additive decomposable problems with uniformly scaled BBs ,
the required population size (in order to correctly solve each BB with a speciﬁed
error probability α) in terms of the BB size k, the total number of BBs m, and the
ﬁtness diﬀerence d between the best and second best BBs.
Goldberg et al. [8] conﬁrmed that the model conservatively estimates the con-
vergence quality of the GA. This somewhat pessimistic estimation is due to the fact
that the model approximates the behavior of the GA by the outcome of the ﬁrst
generation. If the wrong BBs are selected in the initial generation the model assumes
that the GA will be unable to recover from the error.
A few years later, Harik, Cantu-Paz, Goldberg, and Miller [10] reﬁned this model
by incorporating the initial BB supply as well as cumulative eﬀects of decision
making over an entire GA run. Their model was based on the well-known gambler’s
ruin problem and resulted in the following population sizing equation
N =
−2k−1 ln(α)

π (m −1) σBB
d
,
(1)
where σBB is the ﬁtness variance within each BB partition, and α is the failure rate.
Experimental results from the original paper show that this equation is a reliable
estimate for population sizing on decomposable problems with uniformly-scaled and
near-uniformly-scaled building blocks. Additionally, the authors also proposed an
expression to population sizing in the presence of exogenous noise as well as for
diﬀerent selection pressures.
However, if the ﬁtness contribution to the overall ﬁtness function is exponentially-
scaled, the model can not estimate the population size correctly. For that case,
Thierens, Goldberg, and Pereira [39], for BBs with unit size, and later, Lobo, Gold-
berg, and Pelikan [18], for a general BB size, developed a model based on the concepts
of domino convergence and random genetic drift.
In this kind of problems, BBs converge in a sequential manner from the most
salient BBs to the least salient ones. The least salient BBs only get the attention
of the selection operator later in the run, and may be lost from the population due
to chance variation alone, a process known as random genetic drift. It has been
shown that the drift process is the crucial aspect regarding population sizing for
this type of problems. That is, the population size has to be large enough to prevent
the low salient BBs from getting extinct from the population during the GA run.
The theoretical models for exponentially-scaled BBs reveal that the population size
should grow as O(m), instead of O(√m) for the uniformly-scaled case.
2.3 Proper Identiﬁcation and Mixing of Building Blocks
The population sizing models discussed above assume perfect mixing between BBs,
which in practice might not occur when using ﬁxed, problem-independent recom-
bination operators. The issue of correct identiﬁcation and mixing of BBs can be

Adaptive Population Sizing Schemes in GAs
189
related to the population size if we consider a recent class of GAs known as esti-
mation of distribution algorithms (EDAs) [16, 31]. In EDAs the underlying problem
structure is learned on-the-ﬂy for proper identiﬁcation and mixing of BBs. EDAs
have very strong connections with Data Mining algorithms. Both interpret the pop-
ulation as data and use techniques to infer patterns in that data that are associated
with high ﬁtness. Without suﬃcient data it is not possible to do accurate inference.
Fortunately, within a GA framework, having more data translates into working with
larger populations.
EDAs use probabilistic models to induce non-linearities between variables in
order to detect the proper decomposition of the problem. In order to be able to do
that the EDA requires a minimal population size to detect the correlations. Pelikan,
Goldberg, and Cantu-Paz [20] were the ﬁrst to approach this issue, reporting that
for the Bayesian Optimization Algorithm (BOA) the population size in order for the
model to be of required quality grows at least linearly with the number of BBs (m)
in a problem.
A more detailed analysis was made later by Pelikan, Sastry, and Goldberg [24]
to investigate the critical population size to discovers good dependencies (between
variables that are correlated) and bad dependencies (between variables that are
independent) with BOA. They concluded that the population size should be greater
than O(ℓ1.05) and smaller than O(ℓ2.1), where ℓis the problem size. The upper
bound has to be respected in order to avoid that the probabilistic model discover
dependencies between independent variables.
Typically, the population size requirements for initial supply and decision-
making are bounded by the requirements for correct model building.
Before ﬁnishing this part of the chapter, we would like to stress that although
not trivial to put in practice, the theoretical models on population sizing are very
important and crucial for understanding the role of the population in a GA. Among
other things, an important lesson of those models is that setting the population size
to a ﬁxed value regardless of the problem’s size and diﬃculty, is a mistake that is
likely to result in poor performance.
3 Adaptive Population Sizing Schemes
This section reviews a number of techniques that have been developed to adjust
the population size during the GA run itself. The motivation for developing these
schemes are essentially threefold: (1) the recognition that an adequate sizing of the
population is problem dependent and is hard to estimate, (2) the observation that a
GA with an adaptive population sizing scheme may yield a better solution quality
and better performance than a GA with a ﬁxed (and poorly set) population size,
and (3) to make life easier for users by eliminating the population sizing parameter.
The exposition of the various methods follows in chronological order, starting
with Smith and Smuda’s [28] 1993 work and ending with Yu et al. [33] 2005 work. In-
cidentally, both works are attempts to incorporate existing population sizing theory
in an algorithmic form.

190
Fernando G. Lobo and Cl´audio F. Lima
3.1 Population Sizing Through Estimates of Schema Variances
(1993)
Smith and Smuda [28, 29] tried to transfer the population sizing theory of Goldberg
et al. [8] to a more practical context by incorporating their population sizing equation
in the GA itself. The authors wanted to remove the parameters of the GA, and
starting with the population size seemed like a logical ﬁrst step because experience
had shown that the GA was quite robust regarding the other parameters.
Based on Goldberg et al.’s theoretical work, Smith and Smuda suggested an
algorithm for autonomously adjusting the population size as the search progresses.
They argued that the parameters of the GA are hard to relate to the user’s need.
That is, the user typically does not know how the population size aﬀects the solution
quality of the problem. The authors suggested that the parameters should have more
meaning from the user’s point of view, and proposed an algorithm that only required
the user to specify a desired accuracy for the overall search, something equivalent to
the building block’s signal from Goldberg et al.’s equation. In order to do that, Smith
and Smuda deﬁned the expected selection loss between two competing schemata as
the probability that the GA makes a selection error (selection chooses the lower ﬁt
schema) weighted by their ﬁtness diﬀerence. Then, they suggested an algorithm that
does online estimates of schema ﬁtness variances, and sizes the population so that
the expected selection loss approximates the user’s speciﬁed target accuracy.
Although they suggested and implemented such a scheme, the resulting algo-
rithm had several limitations. First, it is hard to relate the user’s speciﬁed selection
target loss with the actual accuracy of the GA in solving the problem. Second, the
estimation of schema ﬁtness variances is very noisy, because the GA samples many
schemata simultaneously. Third, the population sizing model [8] assumes that the
building block mixing is going to be nearly perfect, which may not occur in prac-
tice. Nonetheless, the work was very important because up to that point, adaptive
population sizing schemes were practically inexistent.
3.2 Population Sizing in GAVaPS (1994)
Arabas, Michalewicz, and Mulawka [1] proposed the Genetic Algorithm with Varying
Population Size (GAVaPS). As opposed to Smith and Smuda’s work, this method
does not rely on the population sizing theory [8]. Instead, it relies on the concept of
age and lifetime of an individual. When an individual is created, either during the
initial generation or through a variation operator, it has age zero. This step corre-
sponds to the birth of the individual. Then, for each generation that the individual
stays alive its age is incremented by 1.
At birth, every individual is assigned a lifetime which corresponds to the num-
ber of generations that the individual stays alive in the population. When the age
exceeds the individual’s lifetime, the individual dies and is eliminated from the
population. At every generation, a fraction ρ (called the reproduction ratio) of the
current population is allowed to reproduce. Every individual of the population has
an equal probability of being chosen for reproduction. Thus, GAVaPS does not have
an explicit selection operator as traditional GAs do. Instead, selection is achieved
indirectly through the lifetime that is assigned to individuals. Those with above-
average ﬁtness have higher lifetimes than those with below-average ﬁtness. The idea
is that the better an individual is, the more time it should be allowed to stay in

Adaptive Population Sizing Schemes in GAs
191
the population, and therefore increase the chance to propagate its traits to future
individuals.
Arabas et al. mentioned that diﬀerent lifetime strategies could be implemented
based on the idea of reinforcing individuals with high ﬁtness and restricting individ-
uals with low ﬁtness, and suggested three diﬀerent strategies: proportional, linear,
and bi-linear allocation.
All those strategies rely on two parameters, MinLT and
MaxLT, which correspond to the minimum and maximum lifetime value allowable
for an individual. The proportional strategy is inspired in the roulette wheel selec-
tion scheme, the linear strategy relates the ﬁtness of an individual with the best
ﬁtness found so far, the bi-linear strategy is a compromise between the ﬁrst two
strategies. For completeness, and because it is also used in the APGA algorithm
(see later in the chapter), the details of the bi-linear strategy used to compute the
lifetime of an individual i is shown below.
LT(i) =
⎧
⎨
⎩
MinLT + η
fit[i]−MinF it
AvgF it−MinF it
if AvgFit ≥fit[i]
MinLT +MaxLT
2
+ η
fit[i]−AvgF it
MaxF it−AvgF it if AvgFit < fit[i]
where η = (MaxLT −MinLT)/2, MinFit, MaxFit, and AvgFit are the minimum,
maximum, and average ﬁtness of the population, and fit[i] is the ﬁtness of individual
i. Figure 2 shows the pseudo-code of GAVaPS.
Arabas et al. argued that this approach seems natural because the aging process
is well-known in all natural environments. In their original work, the authors tested
GAVaPS on four test functions, compared its performance with that of a simple GA
using a ﬁxed population size, and observed that GAVaPS seemed to incorporate a
self-tuning process of the population size.
GAVaPS requires the user to specify the initial population size, but the authors
referred that GAVaPS is robust with respect to that, i.e., the initial population size
seemed to have no inﬂuence on the performance for the test functions chosen. The
same thing did not hold with the reproduction ratio ρ which was found to have a
large inﬂuence in the performance. Work done more recently [31, 5] also reveals that
GAVaPS is very sensitive to the reproduction ratio parameter.
For the MinLT and MaxLT parameters, Arabas et al. used 1 and 7 respectively.
However, they did not give recommendations on how those parameters should be
set.
The ideas presented in GAVaPS were later explored by Fernandes and Rosa [6]
and by B¨ack, Eiben, and Van der Vaart [3] (more about that later).
3.3 Strategy Adaptation by Competing Subpopulations (1994,
1996)
Schlierkamp-Voosen and M¨uhlenbein [26] proposed an adaptive scheme which is
inspired by the biological observation that well adapted species increase and poorly
adapted species decrease when competing for the same resource (e.g. food).
In their scheme there are S competing populations, each running a diﬀerent
search strategy. The search strategies run independently in each population, and at
regular intervals the populations compete with each other. The idea is to increase
the size of the best performing population, and decrease the size of the others.

192
Fernando G. Lobo and Cl´audio F. Lima
procedure GAVaPS
begin
t = 0;
initialize pop(t);
evaluate pop(t);
while not termination-condition do
begin
t = t+1;
pop(t) = pop(t-1);
increase age of pop(t) members by 1;
cross and mutate pop(t);
evaluate pop(t);
remove from pop(t) all individuals
with age greater than their lifetime;
end
end
Fig. 2. Pseudocode of the genetic algorithm with varying population size (GAVaPS).
In order to do so, the authors deﬁne a quality criterion which scores the perfor-
mance of a population, as well as a gain criterion which gives the amount of increase
or decrease in the size of the competing populations. In their scheme, the sum of
the sizes of all populations remains constant through time. After each competition,
in addition to adapting the population sizes, the best performing individual mi-
grates to the other populations, giving them a chance to perform better for future
competitions.
Later on, the same authors [27] extended the model by allowing a consumption
factor γ to be assigned to each population. The idea was motivated by the recognition
that diﬀerent search strategies seem to require diﬀerent population sizes to perform
eﬃciently. For example, the authors recognized that mutation is more eﬃcient with
small populations and recombination works best with large populations. As opposed
to the basic competition model, the extended one allowed the total population size
to change during the whole simulation. According to the authors, the extended
competition scheme is useful for location good regions of attraction with a breadth
search algorithm, and also useful to ﬁne-tune solutions using a more direct search
method.
The scheme suggested by Schlierkamp-Voosen and M¨uhlenbein is, in some sense,
an hybrid algorithm consisting of multiple search strategies, each utilizing diﬀerent
resources (diﬀerent population sizes and potentially diﬀerent variation operators).
The overall scheme reinforces those strategies that appear to work well, and penalizes
those that appear to work poorly.
The basic competition model has a severe limitation that the sum of the pop-
ulation sizes of all strategies remained constant during the whole simulation. The
extended model allowed the total population size to change during the whole simula-
tion but it is not clear how sensitive the algorithm is with respect to the consumption
factor γ, the loss factor used in the gain criterion, and the evaluation interval be-

Adaptive Population Sizing Schemes in GAs
193
tween successive competitions. On the other hand, the idea of competing populations
is interesting and has subsequently been explored by others [12, 11].
3.4 Population Sizing in SAGA (1996)
Hinterding, Michalewicz, and Peachey [12] proposed the Self-Adaptive Genetic Al-
gorithm (SAGA) which included an adaptive population sizing scheme. The basic
idea is to run three independent GAs, each with its own population (call them P1,
P2, P3 with size(P1) < size(P2) < size(P3)) which are initially set to 50, 100,
and 200, and have rules that adjust the population sizes in order to “optimize” the
performance of the mid-sized population P2.
Hinterding et al. allowed the populations to range between 10 and 1000 individu-
als, and made sure that there should always be a diﬀerence of at least 20 individuals
in their sizes in order to avoid population sizes which are too similar.
At regular intervals (an epoch in the authors’ terminology), the best ﬁtness
found in each GA is used as a criterion to adjust the population sizes. The adjusting
mechanisms are based on a number of rules. Speciﬁcally, there is a “move-apart”
rule that states that two populations should be moved apart when the ﬁtness of
their best individuals are within a threshold ϵ = 10−9. In such cases, moving the
populations apart means halving the size of the smallest population, and doubling
the size of the largest population. This situation also holds for the case that all three
populations are within the ϵ ﬁtness threshold. In such case, the mid-size population
is left untouched, while the others grow and shrink.
In case the ﬁtness values of the best individuals of the three populations are not
within ϵ distance, the following rules apply (below, f stands for the ﬁtness of the
best individual):
if f(P1) < f(P2) < f(P3): move right
if f(P3) < f(P2) < f(P1): move left
if f(P1) < f(P3) < f(P2)
or f(P2) < f(P1) < f(P3): compress left
if f(P2) < f(P3) < f(P1)
or f(P3) < f(P1) < f(P2): compress right
where move right, move left, compress left, and compress right are deﬁned as
follows:
move right: size(P1) = size(P2);
size(P2) = size(P3);
size(P3) = size(P3)*2;

194
Fernando G. Lobo and Cl´audio F. Lima
move left: size(P1) = size(P1)/2;
size(P2) = size(P1);
size(P3) = size(P2);
compress left: size(P1) = (size(P1)+size(P2))/2;
compress right: size(P3) = (size(P2)+size(P3))/2;
The reasoning behind these rules is to increase (or decrease) all the population
sizes if the “ideal” population size appears to be out of the range of the current
population sizes. On the other hand, if the “ideal” population size appears to be
within the current population sizes, then the rules should adjust the size of the worst
performing population so that its size becomes closer to the size of the other two
populations.
The time span of an epoch was chosen by Hinterding et al. as 1000 ﬁtness function
evaluations for each GA. The authors also described diﬀerent strategies that either
allowed the populations to mix or to continue running separately after each epoch.
The major drawback of SAGA is, in our opinion, the time span parameter as
well as the existence of a maximum population size bound. It is not clear why 1000
ﬁtness evaluations is utilized for the time span. Also, setting an upper bound for
the population size goes against all existing theoretical work on population sizing [8,
10, 39, 18, 20, 24]. On the other hand, the population resizing heuristics introduced
by SAGA are quite interesting and have some similarities with those implemented
later by the parameter-less GA technique that we are about to explore next.
3.5 Population Sizing in the Parameter-less GA (1999)
The parameter-less GA introduced by Harik and Lobo [11] was developed with
the assumption that solution quality grows monotonously with the population size.
That is, if after some number of generations t, a GA is able to reach a solution
quality q with some population size N1, then (in a statistical sense) it would also
be able to reach at least that same solution quality had it started with a population
size N2 > N1. Based on that observation, Harik and Lobo suggested a scheme
that simulates an unbounded (potentially inﬁnite) number of populations running
in “parallel” with exponentially increasing sizes. Their scheme gives preference to
smaller sized populations by allowing them do do more function evaluations than the
larger ones. The rationale for taking such a decision relies on the observation that all
other things being equal, a smaller sized population should be preferred. After all, if
a GA can reach a good solution quality (goodness depends on the stopping criteria,
of course) with a population size N1, then why bother spending more eﬀorts with a
population size N2 > N1.
Initially the algorithm only has one population whose size is a very small number
N0 (say 4). As time goes by, new populations are spawned and some can be deleted
(more about that later). Thus, at any given point in time, the algorithm maintains
a collection of populations. The size of each new population is twice as large as
the previous last size. The parameter-less GA does have one parameter (although
Harik and Lobo ﬁxed its value to 4). That parameter (call it m) tells how much

Adaptive Population Sizing Schemes in GAs
195
4
2
1
3
4
8
16
Generation number
Population size
8
9
11
12
13
10
14
1
2
3
4
7
8
5
6
6
5
7
Fig. 3. The parameter-less GA simulates an unbounded number of populations.
The numbers inside the rectangles (populations) denote the sequence in which the
generations are executed by the algorithm. The next step of the algorithm (step 15,
not shown in the ﬁgure) would spawn a new population with size 32.
preference the algorithm gives to smaller populations. Speciﬁcally, it tells how many
generations are done by a population before the next immediate larger population
has a chance to do a single generation.
An example is helpful to illustrate the mechanism. Figure 3 depicts an example
with m = 2. Notice that the number of populations is unbounded. The ﬁgure shows
the state of the parameter-less GA after 14 iterations. At that point, the algorithm
maintains three populations with sizes 4, 8, and 16. Notice how a population of a
given size does m = 2 more generations than the next larger population.
This special sequence can be implemented with a m-ary counter as suggested
by the original authors [11], and also with a somewhat simpler implementation as
suggested by Pelikan and Lin [22].
In addition to maintaining an unbounded collection of populations, the algorithm
uses a heuristic to eliminate populations when certain events occur. In particular,
when the average ﬁtness of a population is greater than the average ﬁtness of a
smaller sized population, the algorithm eliminates the smaller sized one. The ratio-
nale for taking this decision is based on the observation that the larger population
appears to be performing better than the smaller one, and it is doing so with less
computational resources (recall that the algorithm gives preference to smaller pop-
ulations). Thus, whenever such an event occurs, Harik and Lobo reasoned that as
a strong evidence that the size of the smaller population was not large enough, and
the algorithm should not waste any more time with it. By doing so, the algorithm
maintains an invariant that the average ﬁtness of the populations are in decreas-
ing order, with smaller sized populations having higher average ﬁtness than larger
populations.

196
Fernando G. Lobo and Cl´audio F. Lima
 8192
 4096
 2048
 1024
 512
 256
 128
 64
 32
 16
 8
 4
 0
 50000
 100000
 150000
population size
function evaluations
population sizes in the parameter-less GA
Fig. 4. Range of population sizes maintained by the parameter-less GA on a typical
run. The dashed line is the minimum population size, and the solid line is the
maximum population size maintained by the algorithm at any given point in time.
(Copied with permission from [19])
In the absence of mutation, the parameter-less GA also eliminates populations
that converge (convergence meaning that the whole population consists of copies of
identical individuals) since its not possible to generate new individuals thereafter.
Figure 4 shows the population sizes maintained by the parameter-less GA as time
goes by on a typical run. In the ﬁgure there are two lines. The bottom one is for
the lowest sized population and the line above it is for the largest sized population
maintained by the algorithm at any given point in time. Notice how early in the
run, the algorithm is using small sized populations, but fairly quickly detects that
those small sizes are not enough and eliminates them. Each vertical step in the
lower line corresponds to an event where a population is deleted. Regarding the top
line, each vertical step corresponds to an event where a new population is being
created for the ﬁrst time. For example, on the particular run shown in Figure 4,
the population of size 1024 was ﬁrst created when the algorithm had already spent
around 30 thousand evaluations. The lower line represents in some sense the current
guess of the parameter-less GA as to what might be a good population size for the
problem at hand.
Pelikan and Lobo [34] did a theoretical analysis for the worst-case performance of
the parameter-less GA as compared to a GA that starts with an optimal population
size. The worst-case scenario occurs when populations are never eliminated (either
never converge, or are never overtaken by a larger population). That work revealed
that a value of m = 2 is the better one in terms of the time complexity of the worst-
case performance, yielding an increase in the number of ﬁtness function evaluations
of only a logarithmic factor when compared with a GA that uses an optimal ﬁxed
population size.
This population sizing technique has been used not only with traditional GAs
but also with EDAs [16, 22, 15].

Adaptive Population Sizing Schemes in GAs
197
Recently, a related scheme where the population size is increased by a factor of
2 at each restart has been suggested in the context of Evolution Strategies [2]. The
major diﬀerence regarding the parameter-less GA technique occurs in the criteria
to decide when a restart should be conducted.
The technique used in the parameter-less GA for removing the ﬁxed population
size parameter is very eﬀective, can be used with most type of GAs, and has only a
small overhead when compared with a GA that starts with an optimal ﬁxed popu-
lation size. The mechanism is able to adapt the population size to diﬀerent orders of
magnitude, solving problems that need small populations as well as problems that
need very large populations.
A criticism that can be made to the technique is the fact that the algorithm keeps
increasing the population size forever, something that goes against the common
intuition that GAs need large populations at the beginning and small populations
at the end of the run. The parameter-less technique, however, assumes that there
is no way to know if a given population size is large enough to do a suﬃciently
wide search at the beginning. Thus, is keeps increasing the population size as time
goes by. Another criticism that can be made to the technique is the existence of the
m parameter. In the absence of mutation, Harik and Lobo suggested a value of 4
as a compromise for problems where the population takes a long time to converge
relative to those where the population converges quickly. Pelikan and Lobo [34],
however, have shown that a value of m = 2 is the best in terms of the worst-case
time complexity of the algorithm, even tough the worst-case scenario is very unlikely
to occur in practice. In any case, the algorithm is relatively robust regarding the m
parameter, and there are sound recommendations on how to specify it.
3.6 Population Sizing in APGA (2000)
The Genetic Algorithm with Adaptive Population Size (APGA) proposed by B¨ack,
Eiben, and Van der Vaart [3] is a slight variation of the GAVaPS algorithm. The
diﬀerence between the two is that APGA is a steady-state GA, the best individual in
the population does not get older, and in addition to the selection pressure obtained
indirectly by the lifetime mechanism, APGA also uses an explicit selection operator
for choosing individuals to reproduce. Thus, APGA uses a stronger selection pressure
than GAVaPS. An algorithmic description of APGA is presented in Figure 5.
Rather than using 1 and 7 as in the original GAVaPS, B¨ack et al. set the values
of MinLT and MaxLT to 1 and 11, because according to them, initial runs with
diﬀerent values indicated that MaxLT=11 delivers good performance. APGA also
needs the initial population size to be speciﬁed (B¨ack et al. used 60 individuals in
their experiments).
Similarly to GAVaPS, APGA can also use diﬀerent lifetime strategies. B¨ack et
al. [3] used a bi-linear strategy similar to the one proposed for GAVaPS.
Although some researchers [3, 5] have claimed this method to be very eﬀective
at adapting the population size, a recent study [19] shows that APGA is not able to
adapt the population size. Speciﬁcally, it is given a formal proof that after MaxLT
generations (which corresponds to 2 MaxLT ﬁtness evaluations), the population size
in APGA is upper bounded by 2 MaxLT +1, and from that point on until the end of
the search, the population size does not raise beyond that bound [19]. Independently
of the problem being solved, the newly introduced parameters, MinLT and MaxLT,
simply act as the actual population size parameter of a traditional GA.

198
Fernando G. Lobo and Cl´audio F. Lima
procedure APGA
begin
t = 0;
initialize pop(t);
evaluate pop(t);
compute RLT for all members of pop(t);
while not termination-condition do
begin
t = t+1;
pop(t) = pop(t-1);
decrement RLT by 1 for all but
the best member of pop(t);
select 2 individuals from pop(t);
cross and mutate the 2 individuals;
evaluate the 2 individuals;
insert the 2 offspring into pop(t);
remove from pop(t) those members with RLT=0;
compute RLT for the 2 new members of pop(t);
end
end
Fig. 5. Pseudocode of the genetic algorithm with adaptive population size (APGA).
RLT stands for remaining lifetime.
3.7 Population Sizing in PRoFIGA (2004)
Eiben, Marchiori, and Valk´o [5] and Valk´o [31] proposed the Population Resizing on
Fitness Improvement GA (PRoFIGA).
PRoFIGA is similar to a traditional GA but at the end of the typical selection,
reproduction, and mutation steps, the population size can grow or shrink based on
an improvement of the best ﬁtness contained in the population. The population
grows either when (1) there is an improvement in best ﬁtness, or (2) there is no
improvement in the best ﬁtness for a “long time”. If neither of the above occurs,
the population size shrinks by a small percentage (1-5%). According to the authors,
the motivation behind PRoFIGA is to use large population sizes for exploration
and small population sizes for exploitation. In their work, the growth rate X for a
population is given by,
X = increaseFactor × (maxEvalNum −currEvalNum)
× maxFitnessnew −maxFitnessold
initMaxFitness
where increaseFactor is a parameter in the interval (0,1), maxEvalNum is the
maximum number of ﬁtness evaluations allowed for the whole run (or a very large
number if one does not know it ahead of time), currEvalNum is the current evalu-
ation number, and maxFitnessnew, maxFitnessold, and initMaxFitness, are the
best ﬁtness values in the current, previous, and initial generation.

Adaptive Population Sizing Schemes in GAs
199
A user of PRoFIGA also has to specify an initial population size, as well as
a minimum and maximum population sizes in which the algorithm must operate.
Thus, although PRoFIGA eliminates the traditional ﬁxed population sizing parame-
ter, it introduces 6 new parameters: (1) initial population size, (2) increaseFactor,
(3) the so-called “long time without improvement” (V ), (4) decreaseFactor, (5)
minPopSize, and (6) maxPopSize.
In their comparative experiments, the authors used initialPopSize = 100,
increaseFactor = 0.1, V = 500, decreaseFactor = 0.4, minPopSize = 15, maxPop
Size = 1000. Unfortunately, it is not clear from a user’s perspective, how should the
newly introduced parameters should be set in order to solve a particular problem.
In their study, Eiben et al. did an empirical comparison of PRoFIGA with other
adaptive population sizing schemes, including APGA and the parameter-less GA.
The winner of the competition was APGA, followed by PRoFIGA. Those claims,
however, have been shown to be unjustiﬁed [19].
3.8 Population Sizing Based on Noise and Substructural
Measurements (2005)
Yu, Sastry, and Goldberg [33] proposed a method to adjust the population using
noise and substructural measurements. Their approach focus on estimating the pop-
ulation size according with the population size models described in Section 2. The
parameters of the population size model are estimated through the substructural
information learned by linkage model-building techniques used in EDAs [16, 31].
This methodology was demonstrated using the dependency structure matrix genetic
algorithm (DSMGA) [32].
The population sizing model used, which incorporates decision-making and
model-building requirements [8, 20, 24, 25],
predicts that the population size re-
quired to solve a decomposable problem with m subfunctions (or BB partitions),
each of size k, with an error probability of α = 1/m, is given by
N = cn 2k σ2
BB
d2
m ln(m),
(2)
where cn is a problem-dependent constant.
Given the linkage model learned by DSMGA (or other EDA) that identiﬁes which
variables form a BB partition, the estimation of the above parameters becomes a
straightforward process. The number or BB partitions m and their respective size k
are directly extracted from the linkage model, where if diﬀerent sizes of BBs are
detected the average size is considered.
The ﬁtness variance of BBs (σ2
BB) and the ﬁtness signal between the two best
BBs (d) are calculated in a simple way according with the structure of the model
learned by DSMGA. First, Yu et al. [33] deﬁned the ﬁtness of a BB as the average
ﬁtness of the correspondent schema relatively to the average ﬁtness of the popu-
lation. Thus, the variance of ﬁtness in a BB partition is given by the variance of
the ﬁtnesses of all possible BBs in that partition. Similarly, the ﬁtness signal d is
given by the diﬀerence between the two BBs with highest ﬁtness in the partition of
interest. Again, in the presence of diﬀerent values for σ2
BB and d, the average over
all BB partitions is the value considered for the population sizing model.
After the linkage model is built the size of the oﬀspring population is estimated
using the above methodology. Yu et al. veriﬁed that this method can correctly adapt

200
Fernando G. Lobo and Cl´audio F. Lima
the population size on-the-ﬂy to the ideal size, where the number of evaluations
spent by the GA with adaptive population size remains practically constant when
compared with a GA with a ﬁxed population size, as the initial population size
is increased. However, for very small initial population sizes (much smaller than
the ideal size) both algorithms failed to converge. To deal with this situation, Yu
et al. proposed a more robust scheme where new individuals (that are randomly
generated) are inserted into the oﬀspring population to ensure diversity when the
parent population is much smaller than the oﬀspring population.
Although the accuracy of the present method relies on the same assumptions
of the population sizing model—problems additively decomposable in uniformly-
scaled subfunctions—it is an important step towards black-box optimization with
GAs, where both operators and parameters are automatically updated on-the-ﬂy
according to the underlying problem structure. For further details about this method
please refer to Chapter 11.
4 Summary and Conclusions
This chapter has made a review of population sizing in genetic algorithms. We have
addressed both theoretical models which rely on a facetwise decomposition of GAs,
as well as automated population sizing schemes that have been proposed by various
researchers in the ﬁeld.
Some adaptive population sizing schemes that have been proposed were devel-
oped having in mind the population sizing theory for genetic algorithms as a foun-
dation to engineer the adaptive algorithms, other methods were primarily inspired
by what happens in Nature, and included concepts such as age and lifetime of an
individual, as well as competition among species for limited resources.
Several authors have argued that it is beneﬁcial to have a varying population size
through a GA run because it seems that the population sizing requirements diﬀer
depending on the stage of the search. Typically, it is argued that the GA needs a
larger population size early on in order to do a breadth search to ﬁnd good regions
of the search space, and then the GA needs less population size at the end of the
run in order to ﬁne-tune the solutions.
We agree with the argument above. However, the critical aspect is to discover
a population size which is large enough to ﬁnd those good regions of attraction.
Without suﬃcient knowledge about the problem it is very diﬃcult to know what
should be a large enough value. At this point, we would like to stand back and give
some recommendations for authors that propose and compare adaptive population
sizing schemes.
• Do not enforce a bound for the maximum population size. Several
adaptive population sizing algorithms have substituted the ﬁxed population
size parameter by an interval MinPopsize and MaxPopsize where the adaptive
algorithm is supposed to work. We strongly believe that the upper bound for
population sizing should not exist.
Both theoretical and empirical work have shown that the population sizing
requirement for GAs grow with respect to the problem’s size and diﬃculty.
Thus, it is not rational to impose an upper bound on the size of the population.
In other words, a MaxPopsize parameter should be eliminated from adaptive

Adaptive Population Sizing Schemes in GAs
201
population sizing schemes. If that does not happen, then the user ends up having
to guess a value for MaxPopsize more or less in the same way that he has to
guess a value for the ﬁxed population size parameter of a traditional GA. In our
opinion, there are only two reasons for setting an upper bound on the size of
the population:
1. when the maximum number of ﬁtness function evaluations allowed for the
whole simulation, T, is known in advance. In such case, the upper bound
for the size of the population could be set to a value which is a function of
T (and obviously less than T). As an extreme case, one generation of a pop-
ulation of size T would terminate the GA run, and that would correspond
to random search.
2. when there are memory constraints that do not allow our computers to run
populations larger than a certain amount.
• Test on problems with known population sizing requirements. The
diﬀerent adaptive algorithms that we have reviewed were tested by their authors
on diﬀerent test problems. It is our strong belief that the test suit should include
problem instances of varying size and diﬃculty and whose optimal population
sizing requirements are well known. That is the only reasonable way to assess
if a given algorithm is capable or not of properly adapting the population size
to solve the problem at hand. Moreover, the problems should have diﬀerent
population sizing requirements, some requiring small and others requiring large
population sizes. A good self-adjusting population sizing algorithm should be
able to detect both cases. We suggest that, among other problems, the class of
additive decomposable problems, both with uniform and exponentially scaled
building blocks, as well as problems with external sources of noise, should be
included in the test suite because their population sizing requirements are well
understood from a theoretical point of view.
• Do fair comparisons. There are several ways to make comparative studies
on the performance of algorithms. Two of the most used methods are (1) given
a ﬁxed computational time T measure the solution quality Q, and (2) given
a solution quality Q, measure the time T that the algorithm needs to reach
that quality. Both components should be addressed on comparative studies to
conclude about the eﬃciency and robustness of diﬀerent adaptive population
sizing methods. Otherwise, comparisons may be unfair and totally insigniﬁcant.
• Do scalability analysis. It is important to perform time and space complexity
analysis for the proposed algorithms, even if they can only be done on a par-
ticular class of problems. The analysis should be complemented by computer
experiments that verify how the algorithm scales up with problems of diﬀerent
size.
• Make life easier for users. All the adaptive population sizing algorithms that
we have reviewed in this paper eliminate the need to specify a ﬁxed population
size for the GA run. Unfortunately, eliminating one parameter and introducing
several other parameters is certainly not making life any easier from a user’s
perspective. If new parameters are introduced, either there should be recom-
mendations on how to set them, or the adaptive algorithm has to be robust
enough so that the performance is not much aﬀected by these new parameters.

202
Fernando G. Lobo and Cl´audio F. Lima
Acknowledgments
The authors thank the support of the Portuguese Foundation for Science and
Technology (FCT/MCES) under grants POSI/SRI/42065/2001, POSC/EEA-ESE/
61218/2004, and SFRH/BD/16980/2004.
We would also like to thank Kumara Sastry for his helpful discussions about
population sizing theory.
References
1. J. Arabas, Z. Michalewicz, and J. Mulawka. GAVaPS – a genetic algorithm
with varying population size. In Proc. of the First IEEE Conf. on Evolutionary
Computation, pages 73–78, Piscataway, NJ, 1994. IEEE Press.
2. A. Auger and H. Nikolaus. A restart CMA evolution strategy with increasing
population size.
In D. Corne et al., editors, Proceedings of the 2005 IEEE
Congress on Evolutionary Computation, volume 2, pages 1769–1776. IEEE
Press, 2005.
3. Thomas B¨ack, A. E. Eiben, and N. A. L. van der Vaart. An empirical study
on GAs “without parameters”. In Parallel Problem Solving from Nature, PPSN
VI, pages 315–324, 2000.
4. K. A. De Jong. An analysis of the behavior of a class of genetic adaptive systems.
PhD thesis, University of Michigan, Ann Arbor, 1975.
5. A. E. Eiben, E. Marchiori, and V. A. Valko. Evolutionary algorithms with on-
the-ﬂy population size adjustment. In X. Yao et al., editors, Parallel Problem
Solving from Nature PPSN VIII, LNCS 3242, pages 41–50. Springer, 2004.
6. Carlos Fernandes and Agostinho Rosa. A study on non-random mating and
varying population size in genetic algorithms using a royal road function. In
Proceedings of the 2001 Congress on Evolutionary Computation CEC2001, pages
60–66. IEEE Press, 2001.
7. D. E. Goldberg. The Design of Innovation - Lessons from and for Competent
Genetic Algorithms. Kluwer Academic Publishers, Norwell, MA, 2002.
8. D. E. Goldberg, K. Deb, and J. H. Clark. Genetic algorithms, noise, and the
sizing of populations. Complex Systems, 6:333–362, 1992. Also IlliGAL Report
No. 91010.
9. D. E. Goldberg and M. Rudnick. Genetic algorithms and the variance of ﬁtness.
Complex Systems, 5(3):265–278, 1991.
10. G. Harik, E. Cant´u-Paz, D. E. Goldberg, and B. L. Miller. The gambler’s ruin
problem, genetic algorithms, and the sizing of populations. Evolutionary Com-
putation, 7(3):231–253, 1999.
11. G. R. Harik and F. G. Lobo. A parameter-less genetic algorithm. In W. Banzhaf
et al., editors, Proceedings of the Genetic and Evolutionary Computation Confer-
ence GECCO-99, pages 258–265, San Francisco, CA, 1999. Morgan Kaufmann.
12. Robert Hinterding, Zbigniew Michalewicz, and Thomas C. Peachey.
Self-
adaptive genetic algorithm for numeric functions. In Parallel Problem Solving
from Nature, PPSN IV, pages 420–429, 1996.
13. John H. Holland. Genetic algorithms and the optimal allocation of trials. SIAM
Journal of Computing, 2(2):88–105, June 1973.

Adaptive Population Sizing Schemes in GAs
203
14. Pedro Larra˜naga and Jose A. Lozano, editors. Estimation of distribution algo-
rithms: a new tool for Evolutionary Computation. Kluwer Academic Publishers,
Boston, MA, 2002.
15. C.F. Lima and F.G. Lobo. Parameter-less optimization with the extended com-
pact genetic algorithm and iterated local search. In K. Deb et al., editors, Pro-
ceedings of the Genetic and Evolutionary Computation Conference (GECCO-
2004), Part I, LNCS 3102, pages 1328–1339. Springer, 2004.
16. F.G. Lobo. The parameter-less genetic algorithm: Rational and automated para-
meter selection for simpliﬁed genetic algorithm operation. PhD thesis, University
of Lisbon, Portugal, 2000. Also IlliGAL Report No. 2000030.
17. F.G. Lobo and D.E. Goldberg. The parameter-less genetic algorithm in practice.
Information Sciences, 167:217–232, 2004.
18. F.G. Lobo, D.E. Goldberg, and M.Pelikan. Time complexity of genetic algo-
rithms on exponentially scaled problems. In Darrell Whitley, David Goldberg,
Erick Cantu-Paz, Lee Spector, Ian Parmee, and Hans-Georg Beyer, editors, Pro-
ceedings of the Genetic and Evolutionary Computation Conference (GECCO-
2000), pages 151–158, Las Vegas, Nevada, 2000. Morgan Kaufmann.
19. F.G. Lobo and C.F. Lima. Revisiting evolutionary algorithms with on-the-ﬂy
population size adjustment. In Proceedings of the ACM Genetic and Evolution-
ary Computation Conference (GECCO-2006), pages 1241–1248. ACM Press,
2006.
20. Martin Pelikan, David E. Goldberg, and Erick Cant´u-Paz. Bayesian optimiza-
tion algorithm, population sizing, and time to convergence. In Darrell Whitley,
David Goldberg, Erick Cantu-Paz, Lee Spector, Ian Parmee, and Hans-Georg
Beyer, editors, Proceedings of the Genetic and Evolutionary Computation Con-
ference (GECCO-2000), pages 275–282, Las Vegas, Nevada, 10-12 July 2000.
Morgan Kaufmann. Also IlliGAL Report No. 2000001.
21. Martin Pelikan, David E. Goldberg, and Fernando Lobo. A survey of optimiza-
tion by building and using probabilistic models. Computational Optimization
and Applications, 21(1):5–20, 2002. Also IlliGAL Report No. 99018.
22. Martin Pelikan and Tz-Kai Lin. Parameter-less hierarchical BOA. In K. Deb
et al., editors, Proceedings of the Genetic and Evolutionary Computation Con-
ference (GECCO-2004), Part II, LNCS 3103, pages 24–35. Springer, 2004.
23. Martin Pelikan and Fernando G. Lobo. Parameter-less genetic algorithm: A
worst-case time and space complexity analysis. IlliGAL Report No. 99014, Uni-
versity of Illinois at Urbana-Champaign, Illinois Genetic Algorithms Laboratory,
Urbana, IL, 1999.
24. Martin Pelikan, Kumara Sastry, and David E. Goldberg.
Scalability of the
bayesian optimization algorithm. International Journal of Approximate Rea-
soning, 31(3):221–258, 2003. Also IlliGAL Report No. 2002024.
25. K. Sastry and D.E. Goldberg.
Designing competent mutation operators via
probabilistic model building of neighborhoods. In K. Deb and et al., editors,
Proceedings of the Genetic and Evolutionary Computation Conference (GECCO-
2004), Part II, LNCS 3103, pages 114–125. Springer, 2004. Also IlliGAL Report
No. 2004006.
26. Dirk Schlierkamp-Voosen and Heinz M¨uhlenbein. Strategy adaption by compet-
ing subpopulations. In Parallel Problem Solving from Nature, PPSN III, pages
199–208, 1994.

204
Fernando G. Lobo and Cl´audio F. Lima
27. Dirk Schlierkamp-Voosen and Heinz M¨uhlenbein.
Adaptation of population
sizes by competing subpopulations. In Proceedings of 1996 IEEE International
Conference on Evolutionary Computation, pages 330–335, 1996.
28. R. E. Smith. Adaptively resizing populations: An algorithm and analysis. Tech-
nical Report No. 93001, The University of Alabama, Tuscaloosa, AL, 1993.
29. R. E. Smith and E. Smuda. Adaptively resizing populations: Algorithm, analy-
sis, and ﬁrst results. Complex Systems, 9:47–72, 1995.
30. Dirk Thierens, David E. Goldberg, and Angela G. Pereira. Domino convergence,
drift, and the temporal-salience structure of problems. In Proceedings of 1998
IEEE International Conference on Evolutionary Computation, pages 535–540,
Piscataway, NJ, 1998. IEEE Press.
31. V. A. Valk´o.
Self-calibrating Evolutionary Algorithms: Adaptive Population
Size. Master’s thesis, Free University Amsterdam, 2003.
32. T.-L. Yu, D. E. Goldberg, A. Yassine, and Y.-P. Chen. A genetic algorithm
design inspired by organizational theory: Pilot study of a dependency structure
matrix driven genetic algorithm. In Proceedings of the Artiﬁcial Neural Networks
in Engineering 2003 (ANNIE 2003), pages 327–332, 2003. Also IlliGAL Report
No. 2003007.
33. T.-L. Yu, K. Sastry, and D. E. Goldberg. Online population size adjusting using
noise and substructural measurements. In D. Corne et al., editors, Proceedings of
the 2005 IEEE Congress on Evolutionary Computation, volume 3, pages 2491–
2498. IEEE Press, 2005.

Population Sizing to Go: Online Adaptation
Using Noise and Substructural Measurements
Tian-Li Yu1,2, Kumara Sastry1,3, and David E. Goldberg1,3
1 Illinois Genetic Algorithm Laboratory
2 Department of Computer Science
3 Department of Industrial & Enterprise Systems Engineering
University of Illinois at Urbana-Champaign
Urbana, IL 61801
tianliyu@illigal.ge.uiuc.edu
kumara@illigal.ge.uiuc.edu
deg@illigal.ge.uiuc.edu
Summary. This chapter presents an online population size adjustment scheme
for genetic algorithms. It utilizes substructural identiﬁcation techniques to calcu-
late the parameters used in facetwise population-sizing models. The methodology is
demonstrated using the dependency structure matrix genetic algorithm on a set of
boundedly-diﬃcult problems. Empirical results indicate that the proposed method
is both eﬃcient and robust. If the initial population size is too large, the proposed
method automatically decreases the population size, and thereby yields signiﬁcant
savings in the number of function evaluations required to obtain high-quality so-
lutions; on the other hand, if the initial population size is too small, the proposed
scheme increases the population size on-the-ﬂy, thereby avoiding premature conver-
gence.
1 Introduction
One of the challenges faced by genetic and evolutionary algorithm (GEA) users
is making appropriate choices of codings, operators, and parameter values. Sev-
eral linkage-learning methods have been developed to relieve GEA practitioners
from having to develop problem-speciﬁc operators and encodings [5]. These linkage-
learning methods automatically identify key sub-structures of the underlying search
problem and solve them quickly, reliably, and accurately. Additionally, facetwise
models have been developed to estimate the genetic-algorithm (GA) parameter val-
ues such as population size, run duration, crossover and mutation probabilities, and
selection pressure [6]. Subsequently, the results of the facetwise models have been
used in the design of so-called parameterless GEAs [10, 12, 16] to alleviate the
guesswork out of parameter settings.
T.-L. Yu et al.: Population Sizing to Go: Online Adaptation Using Noise and Substructural
Measurements, Studies in Computational Intelligence (SCI) 54, 205–223 (2007)
www.springerlink.com
© Springer-Verlag Berlin Heidelberg 2007

206
Tian-Li Yu, Kumara Sastry, and David E. Goldberg
However, the parameterless GEAs do not completely use the population-sizing
models and are restricted to having multiple populations with exponential increas-
ing population sizes. While some of the factors in the population-sizing models are
not known a priori, they can be estimated on-the-ﬂy with linkage-learning GEAs
in a straightforward manner. Therefore, the purpose of this chapter is to develop
an on-line population-sizing estimate which automatically identiﬁes values of key
components of the population-sizing model. Speciﬁcally, we perform an online mea-
surement of key factors of existing facetwise population-sizing models and alter the
population size on-the-ﬂy in accordance with the population-sizing models. Eﬀective
linkage identiﬁcation via linkage-learning GEAs enables us to perform sub-structural
measurements and to compute diﬀerent factors of the facetwise population-sizing
models.
We demonstrate the eﬀectiveness of the proposed method using design structure
matrix GA [28]—a linkage-learning GEA that is inspired by organization theory—on
a class of boundedly diﬃcult test problems. We consider population sizing dictated
by building-block supply [7], accurate decision making [3, 9], and accurate linkage
learning [14, 15, 19, 20] in this study. However, we note that the proposed method
is not restricted to the above models, but other population-sizing models can be
easily incorporated. We demonstrate that the proposed method is not only robust,
but also eﬃcient in terms of number of function evaluations required to obtain high
quality solutions.
This chapter is organized as follows. Section 2 revisits some related work con-
cerning adjusting population size on-the-ﬂy. Then we give background for several
facetwise population-sizing models in Section 3. A brief introduction of dependency
structure matrix genetic algorithm (DSMGA) is given in Section 4. The population
size needed for DSMGA is empirically veriﬁed with the population-sizing model for
linkage learning. Section 5 describes the basic idea of adjusting population size on-
the-ﬂy by utilizing population-sizing models, including how to calculate parameters
needed by population-sizing models, and how to generate individuals for the next
generation to maintain both eﬃciency and robustness. Section 6 proposes a scheme
to improve both the robustness and eﬃciency when the GA starts from a small
population. It also demonstrates how to choose a reasonable initial population size.
Finally, section 7 concludes this chapter.
2 Related Work
Since the inception of GAs, numerous studies have been conducted on on-line and
oﬀ-line parameter settings. A detailed survey of which is beyond the scope of this
study and and the interested reader is referred elsewhere [10, 12]. Since we propose
an on-line population resizing mechanisms for linkage-learning GEAs in this chapter,
we will brieﬂy review related work on adaptive resizing of population sizes in the
remainder of this section.
One of the early eﬀort on resizing the population during the GA run can be
backtracked to Smith and Smuda [23, 22]. They dynamically resized the population
to match a target selection loss Lt. They calculated the estimated selection loss ˆL
for each mating pair. By adopting a sigmoid function, the fraction
ˆL
Lt is used to
conduct a growth factor, which is then used to adjust the population size. One of

Online Population Size Adjusting
207
the weakness of the methodology proposed by Smith and Smuda [23, 22] is that
users need to determine three parameter values: the target selection loss, and two
parameters (α and β) in the sigmoid function used to compute the growth factor.
Harik and Lobo [10] proposed a parameter-less GA, which is composed of two
parts: eliminating selection pressure and crossover rate, and eliminating population
size. The parameter-less GA eliminates population size by simulating a set of pop-
ulations of diﬀerent sizes; starting with a population of base size, the subsequent
populations are twice the size of the previous one. Then, the GA operators are ap-
plied to each population in a special sequence. For every two GA generations for a
populations of a particular size, a single GA generation is performed on a population
of twice the size. Therefore, the number of function evaluations performed on each
population are roughly the same. The parameter-less GA terminates a population
when no improvement is expected from that population. The parameter-less GA
stops when no improvement is expected from the larger populations as well. Sup-
pose that a regular GA with an optimal population-size setting needs n∗
fe number
of function evaluations for a particular problem. It is shown that the parameter-less
GA uses no more than O(n∗
fe log(n∗
fe)) [34] number of function evaluations.
The parameter-less GA technique has been applied not only to simple GAs [10],
but also to linkage-learning GAs such as the extended compact GA (eCGA) [12],
and the hierarchical Bayesian optimization algorithm (hBOA) [16].
The method used in this chapter to estimate the ﬁtness variance of building
blocks is similar to Smith and Smuda’s work [23, 22], but most factors/components
are estimated on-the-ﬂy. Though our method requires an initial population size as
we demonstrate in the result the performance—in terms of number of function eval-
uations and solution quality—is not very dependent on the choice of that value.
Moreover, there is a signiﬁcantly large sweet-spot for the on-line population adjust-
ment scheme to be working with a wide range of the initial population size. The
method will be detailed in Section 5.
3 Population-Sizing Models
Facetwise and dimensional models have been very eﬀective not only in the design of
genetic algorithms, but also in understanding GA dynamics and mechanisms. Since
our methodology depends on the facetwise models of population sizing, we brieﬂy
outline the models dictated by building block supply, decision making, and accurate
linkage learning in the remainder of this section.
3.1 Building-Block Supply Model
The ﬁrst step towards understanding population sizing is to tackle the issue of
building-block (BB) supply, where the minimum population size required to ensure
the presence of at least on copy of all raw schemata is modeled. Holland [11] esti-
mated the number of BBs that receive at least a speciﬁed number of trials using
Poisson distribution. A later study [4] calculated the same quantity more exactly
using binomial distribution and studied their eﬀects on population sizing in serial
and parallel computation. Reeves [18] proposed a population sizing model for supply
of alphabets with ﬁxed cardinality. Recently, Goldberg et al. [7] developed facetwise

208
Tian-Li Yu, Kumara Sastry, and David E. Goldberg
models for ensuring BB supply in the initial population for genetic algorithms. They
considered a population of ﬁxed-length strings consisting alphabets of arbitrary car-
dinality χ. Goldberg et al predicted that the population size required to ensure the
presence of all competing building blocks with a tolerance of ϵ = 1/m is given by
n = χk (k log χ + log m) ,
(1)
where χ is the alphabet cardinality, k is BB size, and m is the number of BBs.
3.2 Gambler’s Ruin Population-Sizing Model
Goldberg et al. [3] proposed population-sizing models for correctly deciding between
competing BBs. They incorporated noise arising from other partitions into their
model. However, they assumed that if wrong BBs were chosen in the ﬁrst generation,
the GAs would be unable to recover from the error. Harik et al. [9] reﬁned the above
model by incorporating cumulative eﬀects of decision making over time rather than
in ﬁrst generation only. Harik et al. [9] modeled the decision making between the
best and second best BBs in a partition as a gambler’s ruin problem. Here we use
an approximate form of the gambler’s ruin population-sizing model:
n =
√π
2
σBB
d
2k√m log m,
(2)
where k is the BB size, m is the number of BBs, d is the signal between the competing
BBs, and σBB is the ﬁtness variance of a building block. The above equation assumes
a failure probability, α = 1/m.
3.3 Model-Building+Decision Making Population Sizing
Facetwise models for incorporating the eﬀects of model building and BB-wise deci-
sion making on the population size have been analyzed for estimation of distribution
algorithms (EDAs) in general, and Bayesian optimization algorithm and extended
compact genetic algorithm in particular [14, 15, 19, 20]. The population-sizing model
which incorporates the eﬀect of model-building and its accuracy on the population
sizing of the GA, and predicts the population size required to solve a problem with
m building blocks of size k with a failure rate of α = 1/m, is given by
n = cn · 2k σBB
d
2
m log m,
(3)
where n is the population size, cn is a problem-dependent constant, k is the BB
length, α is the probability of failure.
4 An Introduction to DSMGA
This section gives a brief introduction to the DSMGA. Readers who are interested
in DSM clustering are referred to Yu et al. [27]. For more details about DSMGA,
please refer to [28].

Online Population Size Adjusting
209
4.1 DSM and DSM Clustering
A DSM is a matrix where each entry dij represents the dependency between node i
and node j [24, 26]. Entries dij can be real numbers or integers. The larger the dij
is, the higher the dependency is between node i and node j. The diagonal entries
(dii) have no signiﬁcance and are usually set to zero or blacked-out. Figure 1 gives
an example of DSM.
A B C D E F GH
A
B
C
D
E
F
G
H
Fig. 1. A DSM. “x” means that dependency exists; the blank squares means no
dependency. This ﬁgure illustrates, for example, that A and B are independent, and
that A and C are dependent. Clustering is not so obviously at the ﬁrst glance.
The goal of DSM clustering is to ﬁnd subsets of DSM elements (i.e., clusters)
so that nodes within a cluster are maximally dependent and clusters are minimally
interacting. DSM clustering is a sophisticated task which requires expertise [21]. For
example, it is not intuitive how to cluster the DSM in Figure 1. However, after we
reorder the nodes (Figure 2), it is easily seen that the DSM can be cleanly clustered
into three parts: {B, D, G}, {A, C, E, H}, and {F}.
B D GA CE H F
B
D
G
A
C
E
H
F
Fig. 2. The same DSM in Figure 1 but after reordered. The DSM can be cleanly
clustered as ((B,D,G)(A,C,E,H),(F)).
Yu et al. [27] proposed the following objective function by using the minimal
description length principle.

210
Tian-Li Yu, Kumara Sastry, and David E. Goldberg
fDSM(M) = (nc log(nc) + log(nn)Σnc
i=1cli)
+ (|S|(2 log(nn) + 1)) ,
(4)
where f measures the description length that a model M needs to describe a given
DSM; nc is the number of clusters in M; nn is the number of nodes of the DSM;
cli is the size of the i-th cluster in M; S is the set of mis-described data of M.
The above objective function has shown capable to cluster a given DSM, and the
clustering results competes with human experts.
4.2 Utilizing DSM Clustering to Identify BBs: The DSMGA
The DSM clustering algorithm can be thought as an linkage-identiﬁcation algorithm
which turns pair-wise linkage information into high-order linkage information, and
DSMGA [28] is motivated based on this thought.
DSMGA utilizes statistical analysis to estimate the ﬁtness of order-2 schemata.
Based on a nonlinearity-detection method similar to LINC [13], a DSM where each
entry dij represents the linkage between gene i and gene j is created. By apply-
ing the DSM clustering algorithm, the pair-wise linkage information is turned into
BB information, which is then utilized to achieve the BB-wise crossover [25]. The
DSMGA has shown capable to correctly identify BBs and eﬃciently solve problems
with uniformly-scaled BBs.
4.3 Population Sizing for DSMGA
Unlike EDAs, the linkage model in DSMGA is not exactly a probability density
function. Nevertheless, empirical ﬁnding shows that the population-sizing model
(Equation 3) for EDAs predicts well the population size needed by DSMGA
(Figure 3). The test function is an (m,k)-trap [1], where m = 10 and k = 3. The
k-bit trap function is deﬁned as follows:
trapk(u) =

1.0
u = k
(1 −d) k−u−1
k−1
otherwise ,
(5)
where u is unitary (the number of 1’s among the k bits) and d is the minimal signal
between the two most competing schemata. In this chapter, d is set to 0.2. By taking
average, the constant cn is empirically found to be roughly 0.24 for the (m, k)-trap
where m = 10 and k = 3. This test function is also used for the experiments in later
sections.
5 The Basic Idea of Population Size Adjustment
This section describes how to calculate parameters needed in population-sizing mod-
els and how to generate individuals for the next generation so that both eﬃciency
and robustness are maintained.

Online Population Size Adjusting
211
1
2
3
4
5
6
7
8
0
20
40
60
80
100
120
140
160
Number of BBs (m)
Population Size Needed (n)
DSMGA
Theory (cn=0.24)
eCGA
Theory (cn=0.368)
Fig. 3. The population-sizing model for EDAs predicts well the population size
needed by DSMGA.
5.1 Population Size Estimation: Calculating Parameters
in Population-Sizing Models
The BB information given by DSMGA tells us explicitly which genes form a BB.
Therefore, calculating the number of BBs (m) and the size of BBs (k) is straight-
forward given the DSM clustering arrangement in DSMGA. For example, suppose
that DSMGA tells us that BB1 = {x2, x4} and BB2 = {x1, x3, x5} (where x’s are
genes), we know that k1 = 2, k2 = 3, and m = 2. For problems with the same size
of BBs, we can simply take the average and then the estimated parameters would
be ˆk = 2.5 and ˆm = 2.
The calculations of the ﬁtness variance of BBs (σBB) and the signal size be-
tween competing BBs (d) are not any more diﬃcult. First, deﬁne HBB,i as the
schemata [11] which have allele values (0 or 1) for those genes in that BB and
wild cards (*) for any other positions; i is just an enumerated number. In the pre-
vious example, BB1 = {x2, x4}, we have HBB1,1 = ∗0 ∗0∗, HBB1,2 = ∗0 ∗1∗,
HBB1,3 = ∗1 ∗0∗, HBB1,4 = ∗1 ∗1∗. Note that there are 2k such schemata for a BB
of size k. Deﬁne f(H) as the ﬁtness value for schema H. We can then express σBB
and d as follows:
σ2
BB = vari[f(HBB,i)],
(6)
d = maxi[f(HBB,i)] −second maxi[f(HBB,i)],
(7)
where var is the variance function, and here we assume a maximization problem.
The ﬁtness of a schema can be estimated from the current population P by exam-
ining each individual xi: ˆf(H) = meani,xi∈H[f(xi)], where ‘mean’ is the arithmetic
averaging function.

212
Tian-Li Yu, Kumara Sastry, and David E. Goldberg
5.2 Population Size Adjustment: First Attempt and Empirical
Results
After estimating the population size for the next generation, the most straight-
forward way to generation the next population is to simply apply GA operators to
generate oﬀspring of the desired size.
We test the proposed method on a (m,k) deceptive trap problem, where m = 10
and k = 3. We compare the performance of a DSMGA with on-line population-size
adjustmentADJ and a DSMGA with ﬁxed population sizes (FIX). In particular,
we focus on the eﬃciency and robustness of each of the two GA methodologies. By
eﬃciency we mean the number of function evaluations that the GAs need to converge
to a solution of predeﬁned quality. By robustness we mean that the rate that the
GAs fail to converge. The termination condition is that on average (m −1)/m =
90% percent of BBs converge correctly. The failure rate used in the population size
estimation is set to 1/m = 10%. All results are averaged over 30 independent runs.
Figure 4 shows the number of function evaluations that the GAs need to converge
solutions with 90% correct BBs by varying the initial population size (n0). Given
the empirical ﬁnding that cn ≃0.24 for DSMGA on this test function and the
failure rate 0.1, the population size needed by FIX is roughly 137. As shown in
Figure 4, FIX consumes the least number of function evaluation (nfe) when the
initial population size n0 is set close to n∗= 137 (we do not count the case for
n0 = 100, since FIX does not usually converge for that ﬁxed population size). If the
population size in FIX is less than the required size (even about 30% less), the GA
converges to a local optimum and therefore does not yield high quality solutions. On
the other hand, if the population size is greater than the required size, the number
of function evaluations grows linearly with the initial population size n0, thereby
wasting signiﬁcant amount of computation resources unnecessarily. In contrast to
FIX, ADJ is able to properly shrink the population size if n0 is large, and hence
consumes roughly a constant number of function evaluations when n0 is slight larger
than n∗.
We can estimate the trend of the behavior for these two algorithms. Let n∗
fe be
the number of function evaluations needed for FIX when n0 = n∗. Empirically, n∗
fe ≃
2700. Following the facetwise modeling approach [6], we assume the running duration
is nearly independent of the population size. For n0 = 600, we can compute that the
number of generations that FIX runs before convergence g ≃14.8. Therefore, we
can estimate the number of function evaluations need for FIX for a ﬁxed population
size n0 is roughly (the FIX-trendline in Figure 4):
nfe−F IX = g · n0.
(8)
In ADJ, for a large initial population size n0, we can assume that linkage model
is quite accurate because n0 is large. Hence, ADJ should estimate the population
size accurate (close to n∗) for the next generation. Hereafter, we assume that ADJ
performs in a similar behavior of FIX at an optimal population size n∗. Therefore,
we model the total number of function evaluations needed for ADJ is the number
of function evaluations in the ﬁrst generation (n0) plus the number of function
evaluations needed for the optimal population size setting (n∗
fe) is roughly (the
ADJ-trendline in Figure 4):
nfe−ADJ = n∗
fe + n0.
(9)

Online Population Size Adjusting
213
0
200
400
600
800
1000
0
1000
2000
3000
4000
5000
6000
7000
8000
9000
Initial population size n0
Number of function evalutions nfe
FIX
FIX−trendline
ADJ
ADJ−trendline
Fig. 4. Number of function evaluations consumed by DSMGAs with a ﬁxed popula-
tion (FIX) and with the population adjustment scheme (ADJ). ADJ needs slightly
more number of function evaluations than FIX when a near-optimal population size
(n∗) is used. For large initial population size, however, ADJ constantly outperforms
FIX. For a too small initial population size, both FIX and ADJ do not converge.
Comparing Equations 8 and 9, we can see a dramatic reduction in the number of
function evaluations when the on-line population size adjustment scheme is applied,
0
50
100
150
200
250
300
0
0.2
0.4
0.6
0.8
1
Initial population size n0
Success rate
FIX
ADJ
Fig. 5. The failure rates for FIX and ADJ for diﬀerent n0. FIX rarely converges for
n0 < n∗as the population size predicts. ADJ is slightly more robust than FIX, but
it still suﬀers from insuﬃcient BB supply and the lack of diversity for small n0.

214
Tian-Li Yu, Kumara Sastry, and David E. Goldberg
especially for a large initial population size. Now we examine the algorithms from
another direction: robustness for small initial population size. Figure 5 shows the rate
that FIX and ADJ fail to converge for diﬀerent small n0. For small n0, FIX rarely
converges. According to the population-sizing theory, FIX does not have enough BB
supply and cannot decide well when n0 < n∗. When the initial population size is very
small, n0 ≪n∗, both FIX and ADJ fail to yield high-quality solutions. However,
when the initial population is slightly increased (but still n0 < n∗), the failure rate
of ADJ decreases rapidly, while FIX consistently fails to yield high-quality solutions.
The robustness of ADJ can be further enhanced by ensuring suﬃcient BB supply and
maintaining diversity, especially in the early stage of the GA run (the method will
be described in the next section). To understand the reason for ADJ to fail on a very
small initial population size, imagine that n∗= 1000, and ADJ starts from n0 = 10.
Even if the linkage model is luckily accurate and estimates the the population size for
the next generation should be 1000, ADJ generates the 1000 oﬀspring chromosomes
from only the 10 parent chromosomes. As can be anticipated, the newly generated
population lacks of diversity and ADJ converges prematurely.
6 A More Robust Scheme for Small Initial Population
Size
The online population size adjustment scheme described in the previous section
allows users to start with a large population which guarantees good solution quality
without spending too many extra function evaluations. While the on-line population
adjustment scheme in the previous section (ADJ) is signiﬁcantly more robust and
eﬃcient than a GA with a ﬁxed population size, the performance—both in terms
of eﬃciency and robustness—can be further enhanced when the GA practitioner
sets the initial population size to be much smaller than the required size. This
section proposes an new-individual-injection scheme to improve the robustness of
the population size adjustment scheme.
6.1 New Individual Injection Scheme
As mentioned in the previous section, in ADJ the new population is generated
via the variation operators—crossover and mutation—of DSMGA. While this is a
straightforward method, it does not alleviate the problem of insuﬃcient BB supply
and the lack of diversity in the population. To circumvent these problems, new
individuals need to be introduced into the population at the right time. One of the
most intrinsic way is to introduce new individuals whenever the estimated population
size is larger than the current population size.
In the previous example, suppose that n∗= 1000, n0 = 10, and the GA estimates
the population size for the next generation should be 1000. Then the GA generates
10 oﬀspring chromosomes by recombining the 10 parent chromosomes and the gen-
erate the other 990 oﬀspring chromosome by random initialization. However, the
injection of randomly generated solutions into the population, especially during the
later stages of the GA run has one sever drawback. Due to random initialization,
the average solution quality goes down and the ﬁtness variance goes up causing an

Online Population Size Adjusting
215
elongation of the run duration [2]. As shown in Figure 6, whenever randomly ini-
tialized solutions are injected into the population, the proportion of correct BBs in
the population is reduced causing a delay in the convergence.
0
5
10
15
20
0.1
0.2
0.3
0.4
0.5
Proportion of correct BBs
0
5
10
15
20
0
500
1000
1500
Generation
Estimated population size
Fig. 6. One run of the GA with the introducing-new-individual scheme switched on
all the time. The GA suﬀers from slow convergence because new individuals keep
joining. The arrows indicates that whenever new individuals joins the population,
the number of correct BBs goes down.
To alleviate the elongation in the run duration and also to ensure adequate sup-
ply of BBs, we should inject randomly initialized solutions into the population during
the initial stages of the GA run and then create the new solutions only through re-
combination in the later stages. Using recombination during the later stages of the
GA run not only circumvents the delay in convergence, but also signiﬁcantly im-
proves the exchange of building blocks and thereby allowing rapid creation of good
quality solutions. One choice for the switching time can be the generation when the
estimated population size for the next generation is smaller than the current popula-
tion size. Intuitively this reduction in the population size indicates that the GA has
suﬃcient supply of raw BBs and good decision making can be ensured. Therefore,
after the ﬁrst time that the estimated population size is smaller than the current
one, we do not inject any new individual into the population.
The proposed scheme not only should improve the robustness, but also the ef-
ﬁciency, especially for small initial population size, n0. As shown in Figure 4, the
number of function evaluations needed for ADJ increases substantially for small n0.
It is because that for small initial population size, the linkage model is not accu-
rate, and hence the estimation of the order of BBs, k, can be much larger than that

216
Tian-Li Yu, Kumara Sastry, and David E. Goldberg
required. Noting that the population size predicted by the population-sizing model
(Equation 3) is exponential to k, if the value of k is overestimated, then the popula-
tion size estimated on-the-ﬂy is signiﬁcantly larger than that required. However, one
underlying assumption in the design of competent GAs is to solve boundedly diﬃ-
cult problems in polynomial time [6]. So, the bounded-diﬃculty assumption implies
that GAs are eﬃcient in solving those problems where the necessary4 interactions
are of lower order [6].
By the assumption of bounded diﬃculty, it is reasonable to limit the order of
BBs during the model-building phase.
6.2 Empirical Results
Similar to the previous section, we use the (m,k)-trap functions with m = 10 and k =
3 for the experiments. Two diﬀerent GAs are compared: (1)DSMGA + population-
size-adjustment, and (2) DSMGA + population-size-adjustment + new-individual-
injection. For simplicity, we call the ﬁrst GA ADJ and the second ADJ+INJECT.
The parameter settings are identical to those used in the previous section. Based
on the assumption of bounded diﬃculty, we set the maximal order of BBs to 10 for
ADJ+INJECT. All results are averaged out of 30 independent runs.
Figure 7 shows the number of function evaluations that the GAs need to converge
by varying the initial population size (n0). As shown in the ﬁgure, the number of
function evaluations needed for ADJ and ADJ+INJECT are roughly the same for
a wide range of initial population size n0.
Figure 8 shows the failure rate of ADJ and ADJ+INJECT. As can be seen in
the ﬁgure, ADJ+INJECT is signiﬁcantly more robust than ADJ. Even for n0 = 2
(which is unreasonably small), ADJ+INJECT still converges to the global optimum
83.3% of the time. The reason that a very small population size still works for
ADJ+INJECT is as follows. For small population size, the pair-wise dependency
detection is not accurate. The inaccuracy usually overestimate k than required. As
a result, the estimated population size for the next generation is usually larger than
the initial population size, which is small. Since the population size increases, the
injection scheme introduces more newly-initialized individuals. If the population size
estimated is still not large enough for the model building, the proposed population
adjustment scheme would keep increasing the population size until the population
size is large enough to give an accurate estimation. More number of function evalu-
ations might be needed for small initial population size; nevertheless, the proposed
method is extremely robust, and works with a large range of initial population sizes.
Figure 9 illustrates the population estimation behavior of ADJ+INJECT for dif-
ferent initial population size n0. We can observe that for the ﬁrst ﬁve generations,
a smaller initial population yields larger estimated population sizes. However, after
that, the estimated population sizes are roughly the same for diﬀerent n0. That is
because once the population size is large enough to build an accurate model, the
population estimation becomes accurate after that. An important and interesting
4 Note that this does not mean that GAs can only solve problems with lower-
order interactions. Problems such as hierarchical and spin-glasses have long-range
interactions, but the necessary interactions required to solve the problem are still
lower-order.

Online Population Size Adjusting
217
0
200
400
600
800
1000
2800
3000
3200
3400
3600
3800
4000
4200
4400
Initial population size n0
Number of function evalutions nfe
ADJ
ADJ+INJECT
Fig. 7. Number of function evaluations consumed by DSMGAs with the population
adjustment scheme (ADJ) and with population adjustment scheme + the injection
scheme (ADJ+INJECT). The number of function evaluations needed for both algo-
rithms are roughly the same.
observation is that at the beginning, the linkage model indicates that more individ-
uals are needed. Towards the end of the GA run, the population loses the diversity
0
50
100
150
200
250
300
0
0.2
0.4
0.6
0.8
1
Initial population size, n0
Success rate
ADJ
ADJ+INJECT
Fig. 8. The failure rates for ADJ and ADJ+INJECT for diﬀerent n0. ADJ+INJECT
is signiﬁcantly more robust than ADJ. ADJ+INJECT successfully converges for a
wide range of initial population size n0.

218
Tian-Li Yu, Kumara Sastry, and David E. Goldberg
(ﬁtness variance of BB in the current population is small), and hence not many
individuals are needed. Since the population size is adjusted in accordance with the
facetwise models, we obtain high-quality solutions requiring about the same number
of function evaluations over a wide range of initial population sizes.
0
5
10
15
20
25
30
0
100
200
300
400
500
600
700
Generation
Population
n0=10
n0=70
n0=130
n0=190
n0=250
n*=137
Fig. 9. The population estimation behavior of ADJ+INJECT for diﬀerent initial
population size n0. At the beginning of the GA runs, a smaller n0 yields a larger
estimated population size. The estimated population sizes are less dependent on n0
when the linkage model is more accurate after ﬁrst few generations.
One other thing that we could try in the future is to apply this scheme to
other linkage-learning GEAs (e.g. eCGA [8]) and test the scalability. We could also
add an update rule to the population-sizing scheme. For example, if the estimated
population size is ˆn and the current population size is nt, the next population size
is decided by nt+1 = αnt + (1 −α)ˆn. Although the update rule introduce a new
parameter α, it might add some values of stability.
One question remaining is how to set the initial population size. While we have
shown that even when using a population size as small as 2, we obtain the global op-
timum with a high probability (∼80%). Nevertheless, we need to obtain high-quality
solutions with very high probability (∼1) using reasonable number of function eval-
uations (close to theoretical minimum). We need some guideline to set the initial
population size which we discuss in the following section.
6.3 Setting the Initial Population Size
Empirical
results
show
that
the
online
population
adjustment
scheme
(ADJ+INJECT) works with a wide range of the initial population sizes. In
real-world applications, we suggest to use the population size predicted by the
gambler’s ruin model [9] of the OneMax problem (call it nOneMax) as an initial

Online Population Size Adjusting
219
guess n0. The reasons are that (1) n0 provides as a lower bound of the population
size needed since the OneMax problem is one of the simplest problems for GAs, and
(2) yet n0 is still large enough for the GAs with the online population adjustment
scheme to converge.
Figure 10 shows the minimal population sizes nmin needed for ADJ+INJECT
to converge. The test functions are (m, k)-traps with k = 3 and k = 5. The con-
vergence requirement is that the GAs converge to the global optimum 30 times out
of 30 independent runs. It is observable that nmin is slightly larger for a larger k.
Nevertheless, for problems with bounded diﬃculty, the population size predicted
by the gambler’s ruin model [9] of the OneMax problem should be large enough to
ensure the GA convergence as showed in the ﬁgure.
8
10
12
14
16
18
20
0
50
100
150
200
250
Number of BBs, m
Population size
Minimal n0 for k=3
Minimal n0 for k=5
Gambler’s ruin model
Fig. 10. The minimal population sizes needed for ADJ+INJECT to converge. The
success rate is held constantly at 1.
However, recall that a smaller initial population size, we may require more num-
ber of function evaluations (Figure 7). A good initial guess should also yields roughly
the same number of function evaluations compared to the optimum population size
n∗predicted by the EDA population-sizing model (Equation 3). Figure 11 shows
the number of function evaluations needed for the GA with initial population sizes
set to nOneMax and n∗. The order of BBs is 3. As Figure 11 indicates, nOneMax is a
good initial guess since it results in almost the same number of function evaluations
as n∗does.
7 Conclusion
This chapter presents an online population size adjustment scheme based on main
parameters estimated by facetwise population-sizing models. Given the linkage

220
Tian-Li Yu, Kumara Sastry, and David E. Goldberg
8
10
12
14
16
18
20
10
3
10
4
Number of BBs, m
Number of function evaluations, nfe
n0=n*
n0=nOneMax
Fig. 11. Number of function evaluations needed for the initial population size set to
(1) what the EDA population-sizing model predicted for the trap problem (n∗) and
(2) what the gambler’s ruin model predicted for the OneMax problem (nOneMax).
This ﬁgure suggests that nOneMax provides a good initial guess. The success rate is
held constantly at 1.
model, we calculate the parameters required by population-sizing models and es-
timate the population size in every generation according to the linkage-learning
population-sizing model (Equation 3). Although we adopts the linkage-learning
population-sizing model in this chapter, the proposed method is not limited to this
particular population-sizing model.
When the estimated population size for the next generation is larger than the
current population size, two methods are conducted to generate the individuals for
the next generation. When the population size is more than required, the individ-
uals are generated by ordinary GA recombination and variation operators. When
the population size is less than required, the individuals are generated by random
initialization to supply enough BBs. In this chapter, the ﬁrst shrinkage of the pop-
ulation size estimated is adopted as the signal which indicates that the population
size is more than required. In addition, we set a limit of maximal order of BBs for
model building to prevent the estimated population size from being too large.
The proposed method provides suﬃcient BB supply at the beginning of the GA
run even for a small initial population size, and concentrate on mixing to ensure
good solution quality toward the end of the GA run. The proposed method is shown
to have both eﬃciency and robustness. With the proposed online population size
adjustment scheme, the number of function evaluations needed grows slowly with
large initial population size, and the GA has a signiﬁcantly higher probability to
converges even with a very small initial population size. Even with an unreasonably
small initial population size of 2, the GA converges to the global optimum of the
(m,k)-trap, where m = 10 and k = 3, with a high probability of about 80%. With
just a slightly larger initial population size, the GA converges to the high-quality

Online Population Size Adjusting
221
solutions with a probability close to 1 and does not consume much more number of
function evaluations. Empirical result also shows that the population size estimated
by the gambler’s ruin model for the OneMax problem, nOneMax, provides a good
initial population size for the proposed method. By setting n0 = nOneMax, the GA
consumes almost the same number of function evaluations as predicted by the GA
theory, and it always converges to the global optimum during the experiments.
Empirical results demonstrate that the GA requires a larger population size at
the beginning to ensure good supply, decision making and exchange of important
building blocks. On the other hand, at the later stages of the GA run, only a smaller
population size is required since the population loses diversity and a number of
the subproblems are solved. The proposed method enables competent GAs to use
a small population size close to that required at every generation and yields high
quality solutions using only a small number of function evaluations.
The work in this chapter suggests the possibility of taking theoretical facetwise
models into practice. We believe that the online population sizing scheme is ready
to use for real-world applications. Although in this chapter, we have only tested the
scheme on problems with uniformly-scaled BBs with the same orders, there is no
reason not to believe that the proposed method also works on problems with non-
uniform salience with diﬀerent orders by modifying the population-sizing model or
plugging in a new one. In addition, once the problem structure is identiﬁed, we
should be able to choose the most appropriate population-sizing model for the given
problem. To conclude, this chapter presents a new direction of practical population-
sizing and shows preliminary but promising results. We believe that more attention
should be drawn in this direction to make GAs more accessible to real-world appli-
cations as well as to better understand population-sizing in GAs.
Acknowledgments
This work was sponsored by the Air Force Oﬃce of Scientiﬁc Research, Air Force
Materiel Command, USAF under grant FA 9550-06-1-0096, and the National Sci-
ence Foundation under ITR grant DMR-03-25939 at the Materials Computation
Center. The US Government is authorized to reproduce and distribute reprints for
Government purposes notwithstanding any copyright notation thereon.
The views and conclusions contained herein are those of the authors and should
not be interpreted as necessarily representing the oﬃcial policies or endorsements,
either expressed or implied, of the Air Force Oﬃce of Scientiﬁc Research, the Tech-
nology Research, Education, and Commercialization Center, the Oﬃce of Naval
Research, or the U.S. Government.
References
1. K. Deb and D. E. Goldberg. Analyzing deception in trap functions. Foundations
of Genetic Algorithms 2, pages 93–108, 1993.
2. D. E. Goldberg. Using time eﬃciently: Genetic-evolutionary algorithms and the
continuation problem. Proceedings of the Genetic and Evolutionary Computa-
tion Conference, pages 212–219, 1999.

222
Tian-Li Yu, Kumara Sastry, and David E. Goldberg
3. D. E. Goldberg, K. Deb, and J. H. Clark. Genetic algorithms, noise, and the
sizing of populations. Complex Systems, 6:333–362, 1992. (Also IlliGAL Report
No. 91010).
4. David E. Goldberg. Sizing populations for serial and parallel genetic algorithms.
Proceedings of the Third International Conference on Genetic Algorithms, pages
70–79, 1989.
5. David E. Goldberg. The race, the hurdle, and the sweet spot: Lessons from
genetic algorithms for the automation of design innovation and creativity. In
P. Bentley, editor, Evolutionary Design by Computers, chapter 4, pages 105–118.
Morgan Kaufmann, San Mateo, CA, 1999.
6. David E. Goldberg. The design of innovation: Lessons from and for competent
genetic algorithms. Kluwer Academic Publishers, Boston, MA, 2002.
7. David E. Goldberg, Kumara Sastry, and Thomas Latoza.
On the supply of
building blocks. Proceedings of the Genetic and Evolutionary Computation Con-
ference, pages 336–342, 2001.
8. G. Harik. Linkage Learning via Probabilistic Modeling in the ECGA. IlliGAL
Report No. 99010, University of Illinois at Urbana-Champaign, Urbana, IL,
February 1999.
9. G. Harik, E. Cant´u-Paz, D. E. Goldberg, and B. L. Miller. The gambler’s ruin
problem, genetic algorithms, and the sizing of populations. Evolutionary Com-
putation, 7(3):231–253, 1999. (Also IlliGAL Report No. 96004).
10. G.R. Harik and F.G. Lobo. A parameter-less genetic algorithm. Proceedings of
the Genetic and Evolutionary Computation Conference, pages 258–265, 1999.
11. J. H. Holland. Adaptation in natural and artiﬁcial systems. University of Michi-
gan Press, Ann Arbor, MI, 1975.
12. Fernando Lobo. The parameter-less genetic algorithm: Rational and automated
parameter selection for simpliﬁed genetic algorithm operation. phdthesis, Uni-
versity of Lisbon, Portugal, July 2000.
13. M. Munetomo and D. E. Goldberg.
Identifying linkage groups by nonlinea-
rity/non-monotonicity detection. Proceedings of the Genetic and Evolutionary
Computation Conference 1999: Volume 1, pages 433–440, 1999.
14. M. Pelikan, D. E. Goldberg, and E. Cant´u-Paz. Bayesian optimization algo-
rithm, population sizing, and time to convergence. Proceedings of the Genetic
and Evolutionary Computation Conference, pages 275–282, 2000. (Also IlliGAL
Report No. 2000001).
15. M. Pelikan, K. Sastry, and D.E. Goldberg. Scalability of the Bayesian optimiza-
tion algorithm. International Journal of Approximate Reasoning, 31(3):221–258,
2003. (Also IlliGAL Report No. 2002024).
16. Martin Pelikan and Tz-Kai Lin. Parameter-less hierarchical boa. Proceedings of
Genetic and Evolutionary Computation Conference 2004 (GECCO-2004), pages
24–35, 2004.
17. Martin Pelikan and Fernando Lobo. Parameter-less genetic algorithm: A worst-
case time and space complexity analysis. IlliGAL Report No. 99014, Illinois
Genetic Algorithms Laboratory, University of Illinois at Urbana-Champaign,
Urbana, IL, 1999.
18. C.R. Reeves. Using genetic algorithms with small populations. Proceedings of
the Fifth International Conference on Genetic Algorithms, pages 92–99, 1993.
19. K. Sastry and D. E. Goldberg. On extended compact genetic algorithm. Late-
Breaking Paper at the Genetic and Evolutionary Computation Conference, pages
352–359, 2000. (Also IlliGAL Report No. 2000026).

Online Population Size Adjusting
223
20. K. Sastry and D. E. Goldberg. Designing competent mutation operators via
probabilistic model building of neighborhoods. Proceedings of the 2004 Genetic
and Evolutionary Computation Conference, 2:114–125, 2004. Also IlliGAL Re-
port No. 2004006.
21. D. Sharman, A. Yassine, and P. Carlile. Characterizing modular architectures.
ASME 14th International Conference, DTM-34024, Sept. 2002.
22. R. E. Smith and E. Smuda. Adaptively resizing populations: Algorithm, analy-
sis, and ﬁrst results. Complex Systems, 1(9):47–72, 1995.
23. Robert E. Smith. Adaptively resizing populations: An algorithm and analysis.
Proceedings of the Fifth International Conference on Genetic Algorithms, page
653, 1993.
24. Donald V. Steward.
The design structure system: A method for managing
the design of complex systems. IEEE Transactions on Engineering Managment,
28:77–74, 1981.
25. D. Thierens and D.E. Goldberg. Convergence models of genetic algorithm se-
lection schemes. In Parallel Problem Solving from Nature, PPSN III, pages 119–
129, 1994.
26. A. Yassine, Donald R. Falkenburg, and Ken Chelst. Engineering design manage-
ment: An informatoin structure approach. International Journal of production
research, 37(13):2957–2975, 1999.
27. T.-L. Yu, A. Yassine, and D.E. Goldberg. A genetic algorithm for developing
modular product architectures. Proceedings of the ASME 2003 International
Design Engineering Technical Conferences, 15th International Conference on
Design Theory and Methodology (DETC 2003), pages DTM–48657, 2003. (Also
IlliGAL Report No. 2003024).
28. Tian-Li Yu, David E. Goldberg, Ali Yassine, and Ying-ping Chen. Genetic al-
gorithm design inspired by organizational theory: Pilot study of a dependency
structure matrix driven genetic algorithm. Proceedings of Artiﬁcial Neural Net-
works in Engineering 2003 (ANNIE 2003), pages 327–332, 2003. (Also IlliGAL
Report No. 2003007).

Parameter-less Hierarchical Bayesian
Optimization Algorithm
Martin Pelikan1, Alexander Hartmann2, and Tz-Kai Lin1
1 Missouri Estimation of Distribution Algorithms Laboratory (MEDAL) and
Dept. of Mathematics and Computer Science, 320 CCB
University of Missouri–St. Louis
One University Blvd., St. Louis, MO 63121
pelikan@cs.umsl.edu
tlkq4@studentmail.umsl.edu
2 Institut f¨ur Theoretische Physik
Universit¨at G¨ottingen
Friedrich-Hund-Platz 1
37077 G¨ottingen, Germany
hartmann@physik.uni-goettingen.de
Summary. This chapter describes the parameter-less hierarchical Bayesian opti-
mization algorithm (hBOA), which enables the use of hBOA without the need for
tuning parameters for solving each problem instance. There are three crucial pa-
rameters in hBOA: (1) the selection pressure, (2) the window size for restricted
tournaments, and (3) the population size. Although both the selection pressure and
the window size inﬂuence hBOA performance, the asymptotic time complexity of
hBOA should not change with typical choices of these two parameters. However,
there is no population size that is suitable for all problems of interest because the
population size strongly depends on problem size and problem diﬃculty. To elim-
inate the population size, the parameter-less hBOA adopts the population-sizing
technique of the parameter-less genetic algorithm. Based on the existing theory, the
parameter-less hBOA should be able to solve nearly decomposable and hierarchical
problems in a quadratic or subquadratic number of function evaluations without the
need for setting any parameters whatsoever. A number of experiments are presented
to verify scalability of the parameter-less hBOA. The chapter also discusses how the
parameter-less population-sizing scheme can be incorporated into other estimation
of distribution algorithms (EDAs).
1 Introduction
When the hierarchical Bayesian optimization algorithm (hBOA) was designed [28,
30], it was argued that hBOA can solve diﬃcult nearly decomposable and hierar-
chical problems without the need for setting any parameters. As a result, to solve
M. Pelikan et al.: Parameter-less Hierarchical Bayesian Optimization Algorithm, Studies in
Computational Intelligence (SCI) 54, 225–239 (2007)
www.springerlink.com
© Springer-Verlag Berlin Heidelberg 2007

226
Martin Pelikan, Alexander Hartmann, and Tz-Kai Lin
a new optimization problem, it should be suﬃcient to plug the new problem into
hBOA, press the start button, and wait until hBOA ﬁgures out where the optimum
is. It was argued that all hBOA parameters except for the population size can be set
to their default values without aﬀecting good scalability of hBOA [35, 26, 35]. How-
ever, all experiments assumed that the population size is set optimally to obtain best
performance while retaining reliable convergence [35, 26]. That is why the dream of
having a fully parameter-less optimizer for the entire class of nearly decomposable
and hierarchical problems remained one parameter away from reality.
The purpose of this chapter is to propose a fully parameter-less hBOA by imple-
menting the parameter-less population-sizing scheme of the parameter-less genetic
algorithm (GA) [11] in hBOA. The parameter-less hBOA simulates a collection
of populations of diﬀerent sizes in parallel, starting with a small, base population
size. Each next population is twice as large as the previous one. Each population is
allowed to proceed for at least twice as many iterations as the next bigger population.
This ensures that although there is no upper bound on the number of populations
simulated in parallel and on their size, the overall computational overhead is still
reasonable compared to the case with an optimal population size. In fact, theory
exists that shows that the parameter-less population sizing scheme in the parameter-
less GA does not increase the number of function evaluations until convergence by
more than a logarithmic factor [34]. As a result, hBOA can be expected to perform
within a logarithmic factor from the case with the optimal population size. This is
veriﬁed with experiments on nearly decomposable and hierarchical problems. The
parameter-less hBOA is also tested in a hybrid algorithm created by combining the
parameter-less hBOA with cluster exact approximation (CEA) [15] on the problem
of ﬁnding ground states of two-dimensional Ising spin glasses with ±J couplings and
periodic boundary conditions. The results indicate that although the performance
of the parameter-less hBOA+CEA is expected to be within a logarithmic factor of
the performance of hBOA+CEA with optimal population size, the hybrid setting
may increase the diﬀerence between these two cases by aﬀecting the division of labor
between the local and global searchers.
The chapter starts with a brief description of the hierarchical BOA (hBOA)
in Section 2. Section 3 describes the parameter-less hBOA. Section 4 describes,
presents and discusses experimental results. Section 5 outlines how the parameter-
less population-sizing scheme can be used in many other EDAs. Finally, Section 6
summarizes and concludes the chapter.
2 Hierarchical Bayesian Optimization Algorithm
Estimation of distribution algorithms (EDAs) [24, 31, 16, 4] replace traditional varia-
tion operators of genetic and evolutionary algorithms by a two-step procedure:
1. Build a probabilistic model of promising solutions after selection.
2. Sample the built probabilistic model to generate new solutions.
By replacing standard crossover and mutation operators with machine learning
techniques that allow automatic discovery of problem regularities from populations
of promising solutions, EDAs can solve broad classes of diﬃcult problems quickly, ac-
curately, and reliably, including many problems intractable using other optimization
methods [28, 30].

Parameter-less hBOA
227
For an overview of EDAs, please see references [31] and [16]. EDAs are also
referred to as the probabilistic model-building genetic algorithms (PMBGAs) [31]
and iterated density-estimation algorithms (IDEAs) [3]. The remainder of this sec-
tion describes the hierarchical Bayesian optimization algorithm (hBOA) [28, 30, 26],
which is one of the most advanced and powerful EDAs for discrete representations.
2.1 Hierarchical BOA (hBOA)
The hierarchical Bayesian optimization algorithm (hBOA) [28, 30, 26] evolves a
population of candidate solutions represented by ﬁxed-length discrete strings over
a ﬁnite alphabet. Usually, binary strings are considered but any other ﬁnite-base
alphabet can be used analogically.
The ﬁrst population of candidate solutions is usually generated at random ac-
cording to a uniform distribution over all candidate solutions. The population is
updated for a number of iterations using two basic operators: (1) selection and
(2) variation. The selection operator selects better solutions at the expense of the
worse ones from the current population, yielding a population of promising can-
didates. In this chapter, binary tournament selection is used. Binary tournament
selection selects one solution at a time by ﬁrst choosing two random candidate so-
lutions from the current population and then selecting the best solution out of this
subset. Random tournaments are repeated until the selected population has the
same size as the original population and thus each candidate solution is expected to
participate in two tournaments.
The variation operator starts by learning a probabilistic model of the selected
solutions. hBOA uses Bayesian networks with local structures [5] to model promising
solutions. The variation operator then proceeds by sampling the probabilistic model
to generate new solutions.
The new solutions are incorporated into the original population using the re-
stricted tournament replacement (RTR) [12], which ensures that useful diversity
in the population is maintained for long periods of time. RTR has one parameter
w > 1 called window size. RTR incorporates each new solution X into the original
population (the one before selection) using the following three steps:
1. Select a random subset W of w solutions from the original population.
2. Denote Y the candidate solution in W that is most similar to X.
3. X replaces Y if X is better than Y ; otherwise, X is discarded.
A robust rule of thumb is to set w = min{n, N/20} where n is the number of decision
variables in the problem and N is the population size [29, 26].
The run is terminated when a good enough solution has been found, when the
population has not improved for a long time, or when the number of generations
has exceeded a given upper bound. Figure 1 shows the pseudocode of hBOA. The
procedure of hBOA is illustrated in ﬁgure 2. For a more detailed description of
hBOA, see [28, 25, 26].
3 Parameter-less hBOA
The parameter-less genetic algorithm (GA) [11] eliminates the need for setting
parameters—such as the population size, selection pressure, crossover rate, and
mutation rate—in genetic algorithms.The crossover rate and selection pressure are

228
Martin Pelikan, Alexander Hartmann, and Tz-Kai Lin
Hierarchical BOA (hBOA)
t := 0;
generate initial population P(0);
while (not done) {
select population of promising solutions S(t);
build Bayesian network B(t) with local struct. for S(t);
sample B(t) to generate offspring O(t);
incorporate O(t) into P(t) using RTR yielding P(t+1);
t := t+1;
};
Fig. 1. The pseudocode of the hierarchical BOA (hBOA).
Fig. 2. hBOA starts by generating the initial population of candidate solutions at
random. In each iteration, hBOA generates new solutions by sampling a Bayesian
network built for the selected population of candidate solutions. The iteration con-
cludes by incorporating new solutions into the original population using restricted
tournament replacement. The new iteration is executed if the termination criteria
are not met.
set to ensure consistent growth of building blocks based on the schema theorem.
The mutation rate can be eliminated in a similar manner. The population size is
eliminated by simulating a collection of populations of diﬀerent sizes. In the con-
text of hBOA, eliminating the population size is the most important part of the
parameter-less GA, because the selection pressure inﬂuences performance of hBOA
by only a constant factor and there are no crossover or mutation rates in hBOA.
The parameter-less hBOA incorporates the population-sizing technique of the
parameter-less GA into hBOA by simulating a collection of populations
P = {P0, P1, . . .}.
The size of the ﬁrst population P0 is denoted by N0 and it is called the base
population size. The size of the population Pi is denoted by Ni and it can be obtained
by multiplying the base population size by a factor of 2i:

Parameter-less hBOA
229
Parameter-less hBOA
initialize P[0];
generation[0]=0;
max_initialized=0;
i=0;
while (not done) {
simulate one generation of P[i];
generation[i] = generation[i]+1;
if (generation[i] mod k = 0) {
i = i + 1;
if (i>max_initialized) {
initialize P[i];
max_initialized=i;
}
}
else
i = 0;
};
Fig. 3. The pseudocode of the parameter-less hBOA.
Ni = N02i.
For all i ∈{1, 2, . . .}, one generation of Pi is executed after executing k ≥2
generations of Pi−1. Here we use k = 2 so that all populations proceed at the same
speed with respect to the number of evaluations. Each population is initialized just
before its ﬁrst iteration is executed. The pseudocode that can be used to simulate the
collection of populations described here is shown in Figure 3. This implementation
is slightly diﬀerent from the one based on a k-ary counter described in the ﬁrst
parameter-less GA study [11].
3.1 When to Terminate a Population?
To terminate populations in the collection, the parameter-less hBOA can use the
same termination criteria as the parameter-less GA [11]. However, since hBOA uses
niching, no population in the collection can be expected to converge for a long time.
To improve performance of the parameter-less hBOA, we upper-bound the number
of generations for each population using the estimates predicted by theory [23, 38,
10, 39, 35, 26] so that the number of iterations is at least several times greater than
necessary to solve each problem.
Nonetheless, similarly as in the case of the parameter-less GA [34, 33], the
parameter-less hBOA can run all populations indeﬁnitely long without increasing the
logarithmic growth of the factor by which the computational complexity of hBOA
increases compared to the case with the optimal population size.

230
Martin Pelikan, Alexander Hartmann, and Tz-Kai Lin
4 Experiments
This section describes experiments, presents experimental results, and discusses the
results.
4.1 Description of Experiments
Both hBOA and the parameter-less hBOA have been applied to artiﬁcial hierarchical
and nearly decomposable problems and 2D ±J spin glasses with nearest neighbor
interactions and periodic boundary conditions. Standard hBOA uses the minimum
population size required for reliable convergence to the global optimum, whereas
the parameter-less hBOA uses the parameter-less population sizing scheme with the
base population size N0 = 10.
For all problems, problem size was varied to investigate the scalability of both
algorithms. For spin glasses, systems of diﬀerent size were tested and 1000 random
instances were examined for each problem size to ensure that the results would
provide an insight into hBOA performance on a wide range of spin glass instances.
To enhance eﬃciency of hBOA on spin glasses, cluster exact approximation (CEA)
is used to improve each candidate solution in the population [15, 32].
For each problem instance and problem size, the parameter-less hBOA is ﬁrst run
to ﬁnd the optimum in 100 independent runs and the total number of evaluations in
every run is recorded. The average number of function evaluations is then displayed.
These results are compared to the results for hBOA with the minimum population
size that ensures that 30 independent runs converge to the optimum. The minimum
population size is determined using bisection until the width of the resulting interval
is at most 10% of the lower bound [25, 26].
In all experiments, binary tournament selection without replacement is used to
select promising solutions, and RTR is used as a replacement strategy where the
window size is w = min{n, N/20}.
The remainder of this section discusses test problems and experimental results.
4.2 Concatenated Trap of Order 5 (trap-5)
In concatenated traps of order 5 (trap-5) [1, 8], the number of bits in solution strings
must be divisible by 5. The input string is ﬁrst partitioned into independent groups
of 5 bits each. This partitioning is unknown to the algorithm, but it does not change
during the run. A 5-bit trap function is applied to each group of 5 bits and the
contributions of all traps are added together to form the ﬁtness.
The 5-bit trap used to compute the contribution of each group of 5 bits is
deﬁned as
trap5(u) =

5
if u = 5
4 −u
if u < 5 ,
(1)
where u is the number of 1s in the input string of 5 bits. Trap-5 has one global
optimum in the string of all ones and (2n/5 −1) other local optima. Trap-5 is
fully deceptive [7]. Consequently, variation operators should not break interactions
between the bits in each group because all statistics of lower order lead away from
the optimum.

Parameter-less hBOA
231
25
50
75
100 125 150
10
3
10
4
10
5
10
6
Number of bits
Number of evaluations
hBOA (parameter−less)
hBOA (opt. pop. size)
Fig. 4. Parameter-less hBOA and hBOA with the optimal population size on trap-5
of n = 25 to n = 150 bits.
Because trap-5 is a fully deceptive decomposable problem of bounded diﬃculty,
this function provides a good test for optimization algorithms that address decom-
posable problems of bounded diﬃculty. If an algorithm is not capable of identifying
appropriate problem decomposition, it should be mislead away from the optimum.
That is why standard crossover and mutation operators fail to solve trap-5 scal-
ably [10, 37, 22].
Figure 4 shows the number of evaluations of hBOA with the optimal population
size determined by the bisection method and the parameter-less hBOA on trap-5 of
n = 25 to n = 150 bits. The results indicate that the number of function evaluations
for decomposable problems increases only by a nearly constant factor, and elimi-
nating the parameters of hBOA thus does not seem to aﬀect hBOA scalability on
decomposable problems qualitatively; the same result is expected due to the existing
worst-case theory of the parameter-less population sizing scheme [34].
4.3 Hierarchical Traps of Order 3 (hTrap)
In an order-3 hierarchical trap (hTrap) with L levels [28, 27], the total number
of bits in a candidate solution is n = 3L. A candidate solution is evaluated on
multiple levels and the overall value of the hierarchical trap is computed as the sum
of contributions on all levels.
In the variant of hTrap used in this work, on the lowest level, groups of 3 bits
contribute to the overall ﬁtness using a generalized 3-bit trap
trap3(u) =

fhigh
if u = 3
flow −u flow
2
otherwise ,
(2)
where fhigh = flow = 1.
Each group of 3 bits corresponding to one of the traps is then mapped to a
single symbol on the next level; a 000 is mapped to a 0, a 111 is mapped to a 1, and
everything else is mapped to the null symbol ‘-’. The bits on the next level again

232
Martin Pelikan, Alexander Hartmann, and Tz-Kai Lin
27
81
243
729
10
3
10
4
10
5
10
6
10
7
Number of bits
Number of evaluations
hBOA (parameter−less)
hBOA (opt. pop. size)
Fig. 5. Parameter-less hBOA and hBOA with the optimal population size on the
hierarchical trap of n = 27 to n = 729 bits.
contribute to the overall ﬁtness using 3-bit traps deﬁned above, and the groups are
mapped to an even higher level. This continues until the top level is evaluated that
contains 3 bits total. However, on the top level, a trap with fhigh = 1 and flow = 0.9
is applied. Any group of bits containing the null symbol does not contribute to the
overall ﬁtness.
To make the overall contribution at each level of the same magnitude, the con-
tributions of traps on ith level from the bottom are multiplied by 3i.
hTraps have many local optima, but only one global optimum in the string of
all ones. Nonetheless, any single-level decomposition into subproblems of bounded
order will lead away from the global optimum. That is why hTraps necessitate
an optimizer that can build solutions hierarchically by juxtaposing good partial
solutions over multiple levels of diﬃculty until the global optimum if found.
Figure 5 shows the number of evaluations of the hierarchical BOA with optimal
population size determined by the bisection method and the parameter-less hBOA
on hierarchical traps of n = 27, 81, 243, and 729 bits. The results indicate that the
number of function evaluations for hierarchically decomposable problems increases
only by a constant factor, and eliminating the parameters of hBOA does not aﬀect
scalability of hBOA on this class of problems qualitatively.
4.4 Ising Spin Glasses
Ising spin glasses provide a class of prototypical models for disordered systems and
have played a central role in statistical physics during the last three decades [2, 20, 9,
40]. Here we discuss a simple two-dimensional Ising spin glass model, which provides
a rich class of diﬃcult optimization problems that are intractable with most standard
crossover and mutation operators [26] as well as state-of-the-art Markov chain Monte
Carlo methods [6]. There are three primary sources of problem diﬃculty of Ising spin
glasses: (1) Ising spin glasses contain a large number of local optima, which often
grows exponentially with problem size, (2) local optima in the objective-function

Parameter-less hBOA
233
landscape are surrounded by low-quality points, and (3) eﬀective exploration of the
space of candidate solutions (spin conﬁgurations) necessitates that the exploration
operator considers interactions between a large number of spins (problem variables).
A 2D spin-glass system consists of a regular 2D grid containing n nodes where
each node i corresponds to a spin si and each edge ⟨i, j⟩corresponds to a coupling
between two spins si and sj. Each edge has a real value associated with it that deﬁnes
the relationship between the two connected spins. To approximate the behavior of
the large-scale system, periodic boundary conditions are often used that introduce
a coupling between the ﬁrst and the last element along each dimension.
For the classical Ising model, each spin si can be in one of two states: si = +1
or si = −1. Note that this simpliﬁcation corresponds to highly anisotropic systems,
which do indeed exist in some experimental situations. Nevertheless, the two-state
Ising model comprises all basic eﬀects also found in models with more degrees of
freedom. A speciﬁc set of coupling constants deﬁne a spin glass instance. Each
possible setting of the states of all spins is called a spin conﬁguration.
Given a set of coupling constants Ji,j, and a conﬁguration of spins C = {si},
the energy can be computed as
E(C) =

⟨i,j⟩
siJi,jsj ,
(3)
where i, j ∈{0, 1, . . . , n−1} and ⟨i, j⟩denote the nearest neighbors in the underlying
grid (allowed edges).
Here we consider the task of ﬁnding ground states of a 2D spin glass with speciﬁed
coupling constants, where a ground state is a conﬁguration of spins that minimizes
the energy of the system given by equation 3.
Each spin glass conﬁguration is represented by a string of bits where each bit
corresponds to one spin: a 0 represents a spin −1, and a 1 represents a spin +1.
We created 1000 random spin glasses of sizes 20 × 20 (400 spins) to 50 × 50 (2500
spins) by generating coupling constants to be +1 or −1 with equal probabilities. Spin
glasses with coupling constants restricted to +1 and −1 are called ±J spin glasses.
The optimum for most 2D instances was veriﬁed with the branch-and-cut algorithm
provided at the Spin Glass Ground State Server at the University of K¨oln [36].
To enhance hBOA eﬃciency, cluster exact approximation (CEA) [15] is incor-
porated into hBOA to improve all solutions in the population similarly as in [32].
CEA proceeds by selecting a cluster of non-frustrated spins and changes the cluster
optimally given the ﬁxed values of all remaining spins. This can be done eﬃciently
using the maximum ﬂow algorithm. CEA can be repeated a number of times because
the clusters of selected spins tend to diﬀer and so do the eﬀects of the corresponding
updates.
In the hybrid algorithm hBOA+CEA created by combining hBOA and CEA,
before evaluating each spin glass conﬁguration in hBOA, CEA is applied to improve
the conﬁguration for a number of steps. The application of CEA to each conﬁguration
is terminated once the conﬁguration cannot be improved for a certain number of
iterations; more speciﬁcally, the bound on the number of failures is √n where n is
the number of spins. Since one iteration of CEA can be expected to be at least as
expensive as one evaluation of a conﬁguration [21], instead of counting the number
of evaluated candidate solutions, we consider the number of CEA iterations. The
overall number of CEA iterations ran on candidate solutions during the entire run
represents the time complexity of this run.

234
Martin Pelikan, Alexander Hartmann, and Tz-Kai Lin
400
625
900
1225 1600
2500
10
3
10
4
10
5
Number of bits (spins)
Number of CEA iterations
hBOA+CEA (parameterless)
hBOA+CEA (opt. pop. size)
Fig. 6. Parameter-less hBOA and hBOA with the optimal population size on 2D
Ising spin glasses with random ±J couplings and periodic boundary conditions. Both
methods are combined with cluster exact approximation (CEA).
Figure 5 shows the number of CEA iterations for hBOA+CEA with the op-
timal population size determined by the bisection method and the parameter-less
hBOA+CEA on 2D ±J spin glasses. The good news is that the number of CEA
iterations for the parameter-less hBOA still grows as a low-order polynomial with
respect to the number of decision variables. The bad news is that, unlike for single-
level and hierarchical traps, in this case the parameter-less population sizing does
inﬂuence the asymptotic complexity of hBOA. Although the number of CEA iter-
ations seems to be still upper-bounded by a low-order polynomial, the order of the
polynomial that approximates the number of CEA iterations increases slightly by
the factor bounded by O(n0.75).
The increase in the order of the polynomial that approximates the asymptotic
growth of the number of CEA steps seems to disagree with the existing theory,
which claims that the factor for the number of evaluations should be at most log-
arithmic [34]. One factor that leads to these results is that for hybrid variants of
hBOA, the time complexity of evaluating a candidate solution depends on the solu-
tion itself. For any population, early in the run, it can be expected to be rather easy
to improve most conﬁgurations with CEA and each solution will most likely lead to
a number of CEA iterations because conﬁgurations are random or nearly random.
On the other hand, later in the run, most conﬁgurations should be near a local
optimum and CEA would usually lead to no improvement or only little improve-
ment at a time. In the parameter-less hBOA, the proportion of solutions that were
processed only for a few generations to the overall number of evaluated candidate
solutions increases faster than in the standard hBOA with the optimum population
size. Consequently, the number of CEA iterations per evaluation of a candidate solu-
tion is expected to increase, leading to the increase of the asymptotic complexity of
hBOA+CEA using the parameter-less population sizing. However, this factor still
does not fully explain the results because based on the analysis of experimental

Parameter-less hBOA
235
results, this leads to the factor of only about O(n0.05), leaving the overall increase
in the asymptotic time complexity unexplained.
Another hypothesis is that due to the change in the structure of the global
population that consists of all evaluated solutions, the division of labor between
the local and global searcher may change, and more bias is given to one of the
searchers at the expense of the other one. This hypothesis can be supported by the
experiments with the parameter-less variant of hBOA hybridized with other local
search methods [33], where hBOA was combined with a simple discrete hill climber
and the parameter-less version of this hybrid algorithm also led to an increase of the
asymptotic growth of the number of evaluations.
5 Eliminating Population Size in Other EDAs
Eliminating the parameters of hBOA is important on its own right, but an important
question is whether the proposed scheme can be used to eliminate parameters of
other EDAs. Intuitively, the same approach can be used to eliminate the need for
setting the population size in most other EDAs. We present two examples that
should apply to many EDAs.
5.1 Eliminating Population Size in Population-Based EDAs
Any EDA that maintains a population of candidate solutions can be modiﬁed
by incorporating the parameter-less population-sizing scheme of the parameter-
less genetic algorithm analogically to the parameter-less hBOA. For example, the
parameter-less population-sizing can be used in the extended compact genetic al-
gorithm (ECGA) [19, 17]. Nonetheless, if there are other parameters that crucially
depend on the problem, such as the standard deviation of Gaussian kernel distribu-
tions, more parameters would have to be eliminated to create a true parameter-less
EDA.
5.2 Parameter-less Compact Genetic Algorithm
Although some EDAs do not maintain an explicit population of candidate solutions,
they represent a population implicitly using a probabilistic model; this probabilistic
model is updated based on the results of tournaments between candidate solutions
generated according to the model. Despite the absence of the population, there is
usually a parameter that can be seen as a counterpart of the population size and it
is just as important as the population size in population-based EDAs. One of the
algorithms that use incremental updates of the probabilistic model is the compact
genetic algorithm [13, 14].
In the compact genetic algorithm (cGA) [13, 14], the population is replaced
by a probability vector that stores the estimated probability of a 1 in each string
position. The probability-vector entries are initialized to 0.5 (uniform distribution).
In each iteration, the probability-vector entries are updated by performing a binary
tournament between two solutions generated according to the probability vector
and modifying vector entries for the bits that diﬀer in the two competing solutions.

236
Martin Pelikan, Alexander Hartmann, and Tz-Kai Lin
N=10, generation=4, learning rate=1/10
N=20, generation=2, learning rate=1/20
N=40, generation=1, learning rate=1/40
0.40   0.50   0.70   0.50   0.60   0.60
0.55   0.55   0.60   0.50   0.50   0.50
0.53   0.50   0.53   0.50   0.50   0.53
Fig. 7. Probability vectors of the parameter-less compact genetic algorithm after
running the ﬁrst iteration on the third probability vector for a 6-bit onemax. Since
the optimum of onemax is in the string of all ones, it is expected that the proportions
of ones increase over time.
The update rule depends on the learning rate, which corresponds to the eﬀective
population size.
The parameter-less cGA can be created by replacing the multiple populations
of the parameter-less genetic algorithm by probability vectors; each population is
represented by one probability vector [18]. In each probability vector, the learning
rate 1/N is used to represent the population size N. Using the population sizes that
grow with an integer power of two, this leads to a series of exponentially decreasing
learning rates. See ﬁgure 7 for an illustration of this concept.
All probability vectors can be simulated indeﬁnitely, similarly as in the
parameter-less hBOA. Another approach is to terminate each probability vector
when its ﬁtness average over a speciﬁc period of time becomes lower than the ﬁtness
average for the probability vector representing a larger population size.
Since the learning rate is the only parameter of cGA, the parameter-less cGA
is a true parameter-less algorithm. A similar approach could be used to update
the learning rate in other similar EDAs, such as PBIL, although the update of the
appropriate parameters may not be as straightforward as in cGA.
6 Summary and Conclusions
This chapter described, implemented and tested the parameter-less hierarchical
Bayesian optimization algorithm (hBOA). The parameter-less hBOA enables the
practitioner to simply plug the problem into hBOA without requiring the practi-
tioner to ﬁrst estimate an adequate population size or other problem-speciﬁc para-
meters. Despite the parameter-less scheme, low-order polynomial time complexity
of hBOA on broad classes of optimization problems is retained. The parameter-
less hBOA is thus a true black-box optimization algorithm, which can be applied
to hierarchical and nearly decomposable problems without setting any parameters
whatsoever.
The parameter-less population-sizing scheme can be used in other estimation of
distribution algorithms, including the algorithms that replace the population by a
probabilistic model and use incremental updates of the probabilistic model.
Acknowledgments
This project was sponsored by the National Science Foundation under CAREER
grant ECS-0547013, by the Air Force Oﬃce of Scientiﬁc Research, Air Force Materiel

Parameter-less hBOA
237
Command, USAF, under grant FA9550-06-1-0096, by the University of Missouri in
St. Louis through the High Performance Computing Collaboratory sponsored by
Information Technology Services, and the Research Award and Research Board pro-
grams, and by the VolkswagenStiftung (Germany) within the program “Nachwuchs-
gruppen an Universit¨aten”. Most experiments were done using the hBOA software
developed by Martin Pelikan and David E. Goldberg at the University of Illinois
at Urbana-Champaign. The experiments presented in this paper were completed at
the Beowulf cluster at the University of Missouri in St. Louis.
References
1. D. H. Ackley. An empirical study of bit vector function optimization. Genetic
Algorithms and Simulated Annealing, pages 170–204, 1987.
2. K. Binder and A.P. Young. Spin-glasses: Experimental facts, theoretical con-
cepts and open questions. Rev. Mod. Phys., 58:801, 1986.
3. Peter A. N. Bosman and Dirk Thierens. Continuous iterated density estimation
evolutionary algorithms within the IDEA framework. Workshop Proceedings of
the Genetic and Evolutionary Computation Conference (GECCO-2000), pages
197–200, 2000.
4. Erick Cant´u-Paz, Martin Pelikan, and Kumara Sastry, editors. Scalable opti-
mization via probabilistic modeling: From algorithms to applications. Springer-
Verlag, 2006.
5. David M. Chickering, David Heckerman, and Christopher Meek. A Bayesian
approach to learning Bayesian networks with local structure. Technical Report
MSR-TR-97-07, Microsoft Research, Redmond, WA, 1997.
6. P. Dayal, S. Trebst, S. Wessel, D. ¨urtz, M. Troyer, S. Sabhapandit, and S. Cop-
persmith. Performance limitations of ﬂat histogram methods and optimality of
Wang-Langdau sampling. Physical Review Letters, 92(9):097201, 2004.
7. K. Deb and D. E. Goldberg. Analyzing deception in trap functions. IlliGAL
Report No. 91009, University of Illinois at Urbana-Champaign, Illinois Genetic
Algorithms Laboratory, Urbana, IL, 1991.
8. K. Deb and D. E. Goldberg. Suﬃcient conditions for deceptive and easy binary
functions. Annals of Mathematics and Artiﬁcial Intelligence, 10:385–408, 1994.
9. K.H. Fischer and J.A. Hertz.
Spin Glasses.
Cambridge University Press,
Cambridge, 1991.
10. David E. Goldberg. The design of innovation: Lessons from and for competent
genetic algorithms, volume 7 of Genetic Algorithms and Evolutionary Compu-
tation. Kluwer Academic Publishers, 2002.
11. Georges Harik and Fernando Lobo. A parameter-less genetic algorithm. Proceed-
ings of the Genetic and Evolutionary Computation Conference (GECCO-99),
I:258–265, 1999.
12. Georges R. Harik. Finding multimodal solutions using restricted tournament
selection. Proceedings of the International Conference on Genetic Algorithms
(ICGA-95), pages 24–31, 1995.
13. Georges R. Harik, Fernando G. Lobo, and David E. Goldberg. The compact
genetic algorithm. IlliGAL Report No. 97006, University of Illinois at Urbana-
Champaign, Illinois Genetic Algorithms Laboratory, Urbana, IL, 1997.

238
Martin Pelikan, Alexander Hartmann, and Tz-Kai Lin
14. Georges R. Harik, Fernando G. Lobo, and David E. Goldberg. The compact
genetic algorithm. Proceedings of the International Conference on Evolutionary
Computation (ICEC-98), pages 523–528, 1998.
15. A. K. Hartmann. Cluster-exact approximation of spin glass ground states. Phys-
ica A, 224:480, 1996.
16. Pedro Larra˜naga and Jose A. Lozano, editors. Estimation of Distribution Al-
gorithms: A New Tool for Evolutionary Computation.
Kluwer, Boston, MA,
2002.
17. Claudio F. Lima and Fernando G. Lobo. Parameter-less optimization with the
extended compact genetic algorithm and iterated local search. Proceedings of
the Genetic and Evolutionary Computation Conference (GECCO-2004), pages
1328–1339, 2004.
18. Tz-Kai Lin and Martin Pelikan.
Parameter-less compact genetic algorithm.
Unpublished Technical Report, Department of Mathematics and Computer
Science, University of Missouri, St. Lous, 2004.
19. Fernando G. Lobo. The parameter-less genetic algorithm: Rational and auto-
mated parameter selection for simpliﬁed genetic algorithm operation. PhD thesis,
Faculdade de Ciencias e Tecnologia da Universidade Nova de Lisboa, Lisbon,
Portugal, 2000.
20. M. Mezard, G. Parisi, and M.A. Virasoro. Spin glass theory and beyond. World
Scientiﬁc, Singapore, 1987.
21. Alan Middleton and Daniel S. Fisher. The three-dimensional random ﬁeld Ising
magnet: Interfaces, scaling, and the nature of states. Phys. Rev. B, 65:134411,
2002.
22. H. M¨uhlenbein. How genetic algorithms really work: I.Mutation and Hillclimb-
ing. In R. M¨anner and B. Manderick, editors, Parallel Problem Solving from
Nature, pages 15–25, Amsterdam, Netherlands, 1992. Elsevier Science.
23. H. M¨uhlenbein and D. Schlierkamp-Voosen. Predictive models for the breeder
genetic algorithm: I. Continuous parameter optimization. Evolutionary Compu-
tation, 1(1):25–49, 1993.
24. Heinz M¨uhlenbein and Gerhard Paaß.
From recombination of genes to the
estimation of distributions I. Binary parameters. Parallel Problem Solving from
Nature, pages 178–187, 1996.
25. Martin Pelikan. Bayesian optimization algorithm: From single level to hierarchy.
PhD thesis, University of Illinois at Urbana-Champaign, Urbana, IL, 2002. Also
IlliGAL Report No. 2002023.
26. Martin Pelikan. Hierarchical Bayesian optimization algorithm: Toward a new
generation of evolutionary algorithms. Springer-Verlag, 2005.
27. Martin Pelikan and David E. Goldberg. Hierarchical problem solving by the
Bayesian optimization algorithm. Proceedings of the Genetic and Evolutionary
Computation Conference (GECCO-2000), pages 275–282, 2000. Also IlliGAL
Report No. 2000002.
28. Martin Pelikan and David E. Goldberg. Escaping hierarchical traps with com-
petent genetic algorithms. Proceedings of the Genetic and Evolutionary Compu-
tation Conference (GECCO-2001), pages 511–518, 2001. Also IlliGAL Report
No. 2000020.
29. Martin Pelikan and David E. Goldberg. Hierarchical boa solves ising spin glasses
and maxsat. Proceedings of the Genetic and Evolutionary Computation Confer-
ence (GECCO-2003), II:1275–1286, 2003. Also IlliGAL Report No. 2003001.

Parameter-less hBOA
239
30. Martin Pelikan and David E. Goldberg.
A hierarchy machine: Learning to
optimize from nature and humans. Complexity, 8(5):36–45, 2003.
31. Martin Pelikan, David E. Goldberg, and Fernando Lobo. A survey of optimiza-
tion by building and using probabilistic models. Computational Optimization
and Applications, 21(1):5–20, 2002. Also IlliGAL Report No. 99018.
32. Martin Pelikan and Alexander Hartmann.
Searching for ground states of
Ising spin glasses with hierarchical BOA and cluster exact approximation. In
E. Cant´u-Paz, M. Pelikan, and K. Sastry, editors, Scalable optimization via prob-
abilistic modeling: From algorithms to applications. Springer, 2006.
33. Martin Pelikan and Tz-Kai Lin. Parameter-less hierarchical boa. Proceedings
of the Genetic and Evolutionary Computation Conference (GECCO-2004),
2:24–35, 2004.
34. Martin Pelikan and Fernando G. Lobo.
Parameter-less genetic algorithm: A
worst-case time and space complexity analysis. IlliGAL Report No. 99014, Uni-
versity of Illinois at Urbana-Champaign, Illinois Genetic Algorithms Laboratory,
Urbana, IL, 1999.
35. Martin Pelikan, Kumara Sastry, and David E. Goldberg.
Scalability of the
Bayesian optimization algorithm. International Journal of Approximate Rea-
soning, 31(3):221–258, 2002. Also IlliGAL Report No. 2001029.
36. Spin Glass Ground State Server. http://www.informatik.uni-koeln.de/ls
juenger/research/sgs/sgs.html, 2004. University of K¨oln, Germany.
37. D. Thierens. Analysis and design of genetic algorithms. PhD thesis, Katholieke
Universiteit Leuven, Leuven, Belgium, 1995.
38. D. Thierens and D.E. Goldberg. Convergence models of genetic algorithm se-
lection schemes. Parallel Problem Solving from Nature, pages 116–121, 1994.
39. Dirk Thierens, David E. Goldberg, and Angela G. Pereira. Domino convergence,
drift, and the temporal-salience structure of problems. Proceedings of the Inter-
national Conference on Evolutionary Computation (ICEC-98), pages 535–540,
1998.
40. A.P. Young, editor. Spin glasses and random ﬁelds. World Scientiﬁc, Singapore,
1998.

Evolutionary Multi-Objective Optimization
Without Additional Parameters
Kalyanmoy Deb
Department of Mechanical Engineering
Indian Institute of Technology Kanpur
Kanpur, PIN 208016, India
deb@iitk.ac.in
Summary. The present-day evolutionary multi-objective optimization (EMO) al-
gorithms had a demonstrated history of evolution over the years. The initial EMO
methodologies involved additional niching parameters which made them somewhat
subjective to the user. Fortunately, soon enough parameter-less EMO methodolo-
gies have been suggested thereby making the earlier EMO algorithms unpopular
and obsolete. In this paper, we present a functional decomposition of a viable EMO
methodology and discuss the critical components which require special attention
for making the complete algorithm free from any additional parameter. A critical
evaluation of existing EMO methodologies suggest that the elitist non-dominated
sorting GA (NSGA-II) is one of EMO algorithms which does not require any ad-
ditional implicit or explicit parameters other than the standard EA parameters,
such as population size, operator probabilities, etc. This parameter-less property of
NSGA-II is probably the reason for its popularity to most EMO studies thus far.
1 Introduction
Evolutionary algorithms (EAs) came out as serious contenders in optimization stud-
ies during the past two decades. EAs have certain features (such as, ﬂexible op-
erators, no need for using gradients, ease in tackling mixed-integer problems and
combinatorial problems, etc.) which are rare to be found in any other single op-
timization method, including classical methods. However, the ﬂexibilities in their
usage come with some onus on the part of the user. To put diﬀerent ﬂexible EA
operators together to make an eﬀective search, they have to be tuned properly as
to make a proper balance [22, 19] of two conﬂicting aspects needed in a successful
optimization algorithm: (i) exploitation of available resource and (ii) exploration of
search space. The former aspect deals with the selection operator and is related to
the selection probabilities assigned to better population members for them to be
used as parents. The more the assigned selection probability, the larger is the ex-
ploitation of available resource. But since a population early on need not have the
optimal solution, too much exploitation may lead to a premature convergence of the
K. Deb: Evolutionary Multi-Objective Optimization Without Additional Parameters, Studies
in Computational Intelligence (SCI) 54, 241–257 (2007)
www.springerlink.com
© Springer-Verlag Berlin Heidelberg 2007

242
Kalyanmoy Deb
overall algorithm. Whether a premature convergence actually takes place or not is
also closely related to the associated explorative search power of the algorithm. This
aspect is related to the variation (recombination and mutation) operators used in
an EA. If these operators are designed in a way to have a large search power (large
ergodicity, as deﬁned by Radcliﬀe [32]), a large exploitation may be allowed to con-
stitute a successful search [22]. Thus, it is clear that a successful application of an
EA is far from being simple and a proper balancing act of exploitation-exploration
issue is essential in an EA. The balance between these issues is controlled by a
proper choice of diﬀerent EA parameters (such as, population size, selection pres-
sure parameter, EA operators and their probabilities, etc.). Unfortunately, diﬀerent
optimization problems require diﬀerent values of some or all of these parameters
and a successful application of an EA often requires a proper tuning of these para-
meters from problem to problem. Since these parameters are essential to be ﬁxed in
any form of an EA, it is not surprising that these parameters must have to be set
properly in a successful application of a multi-objective EA.
As the name suggests, multi-objective optimization deals with multiple con-
ﬂicting objectives and usually the optimal solution of one of the objectives is not
necessarily the optimum for any of the other objectives. In such a scenario, instead
of one optimal solution, a number of solutions are optimal. These solutions are called
Pareto-optimal solutions. In the growing literature on evolutionary multi-objective
optimization (EMO) [5], the task is to ﬁrst ﬁnd a well-distributed set of Pareto-
optimal solutions and then, based on the trade-oﬀinformation about the solutions,
choose one speciﬁc solution for implementation [10].
Intuitively, EMO algorithms have a more stringent task to be achieve than the
single-objective EAs. While in most single-objective EAs, the task is to ﬁnd a sin-
gle optimal solution, an EMO is expected to ﬁnd a set of Pareto-optimal solutions
causing an optimal trade-oﬀamong multiple conﬂicting objectives. Besides the re-
quirement of these optimized solutions to be as close to the true Pareto-optimal
front as possible, they also have an additional requirement of being well distributed
over the entire Pareto-optimal region. The task of ﬁnding multiple, well spread-out
optimal solutions resembles the task in a niching EA, in which the target is to ﬁnd
multiple optimal solutions in a multi-modal, single-objective optimization problem
[23, 11]. Besides, an EMO algorithm must have another important consideration –
the need of assigning an appropriate implicit or explicit ﬁtness measure so that so-
lutions can progress towards the true Pareto-optimal frontier. All these operations
seem demanding and it is not obvious whether an EMO algorithm can be devised
without the need of any additional parameter.
In this paper, we ﬁrst discuss the nature of optimal solutions in a multi-objective
optimization problem and then outline some standard EA parameters which are usu-
ally associated with any single-objective EA. Thereafter, we attempt to decompose
diﬀerent tasks necessary to a multi-objective EA functionally into three major com-
ponents and discuss the diﬃculties which one can face in trying to devise them as
parameter-free procedures. Finally, we consider a number of existing EMO method-
ologies and show that a speciﬁc algorithm has a systematic implementation which
allowed the complete algorithm to have no additional parameter. This speciﬁc al-
gorithm (the elitist non-dominated sorting GA or NSGA-II [7, 8]) is by far the
most popular EMO methodology used and cited in the EMO literature. Through
a comparison of NSGA-II with other contemporary EMO algorithms according
to their requirement of additional parameters, we argue that the parameter-less

EMO Without Additional Parameters
243
approach of NSGA-II is one of the main reasons for its success and popularity among
researchers and practitioners. The systematic discussion for achieving parameter-less
algorithms presented in this paper should encourage readers to develop more eﬀec-
tive parameter-less EMO or EA approaches.
2 Multi-Objective Optimization
A multi-objective optimization problem involves a number of objective functions
which are to be either minimized or maximized. As in the single-objective opti-
mization problem, the multi-objective optimization problem usually has a number
of constraints which any feasible solution (including the optimal solution) must sat-
isfy. In the following, we state the multi-objective optimization problem (MOOP)
in its general form:
Minimize/Maximize fm(x),
m = 1, 2, . . . , M;
subject to gj(x) ≥0,
j = 1, 2, . . . , J;
hk(x) = 0,
k = 1, 2, . . . , K;
x(L)
i
≤xi ≤x(U)
i
, i = 1, 2, . . . , n.
⎫
⎪
⎬
⎪
⎭
(1)
A solution x is a vector of n decision variables: x = (x1, x2, . . . , xn)T . The solutions
satisfying the constraints and variable bounds constitute a feasible decision variable
space S, or simply the variable space. One of the striking diﬀerences between single-
objective and multi-objective optimization is that in multi-objective optimization
the objective functions constitute a multi-dimensional space, in addition to the usual
variable space. For each solution x in the variable space, there exists a point in the
objective space, denoted by f(x) = z = (z1, z2, . . . , zM)T . A mapping exists between
an n-dimensional solution vector and an M-dimensional objective vector through
the objective function, constraints, and variable bounds. Figure 1 illustrates these
two spaces and a mapping between them.
Although the fundamental diﬀerence between single and multiple objective opti-
mization lies in the cardinality in the optimal set, from a practical standpoint a user
needs only one solution, no matter whether the associated optimization problem is
single-objective or multi-objective. In the case of multi-objective optimization, the
user is now in a dilemma. Which of these optimal solutions must one choose? This
is not an easy question to answer. It involves many higher-level information which
are often non-technical, qualitative and experience-driven. However, if a set of many
trade-oﬀsolutions are already worked out or available, one can evaluate the pros
and cons of each of these solutions based on all such non-technical and qualitative,
yet still important, considerations and compare them to make a choice. Thus, in a
multi-objective optimization, ideally the eﬀort must be made in ﬁnding the set of
trade-oﬀoptimal solutions by considering all objectives to be important. After a
set of such trade-oﬀsolutions are found, a user can then use higher-level qualitative
considerations to make a choice. In view of these discussions, the following principle
was suggested as an ideal multi-objective optimization procedure:
Step 1 Find multiple trade-oﬀoptimal solutions with a wide range of values for
objectives.
Step 2 Choose one of the obtained solutions using higher-level information.

244
Kalyanmoy Deb
1
f1
f2
2
3
x
x
x
z
x
Decision space
Objective space
Fig. 1. Representation of the decision variable space and the corresponding objective
space.
Multi−objective
optimizer
IDEAL
Higher−level
information
Minimize f
Minimize f
Multiple trade−off
solutions found
......
Minimize f
subject to constraints
Multi−objective
optimization problem
Step 1
Step 2
1
2
M
Choose one
solution
Fig. 2. Schematic of an ideal multi-objective optimization procedure.
Figure 2 shows schematically the principles in an ideal multi-objective optimiza-
tion procedure. In Step 1 (vertically downwards), multiple trade-oﬀsolutions are
found. Thereafter, in Step 2 (horizontally, towards the right), higher-level informa-
tion is used to choose one of the trade-oﬀsolutions. With this procedure in mind,
it is easy to realize that single-objective optimization is a degenerate case of multi-
objective optimization. In the case of single-objective optimization with only one

EMO Without Additional Parameters
245
global optimal solution, Step 1 will ﬁnd only one solution, thereby not requiring
us to proceed to Step 2. In the case of single-objective optimization with multiple
global optima, both steps are necessary to ﬁrst ﬁnd all or many of the global optima
and then to choose one from them by using the higher-level information about the
problem. Realizing this degeneracy between single and multi-objective optimization,
we have designed an omni-optimizer, which is capable of solving diﬀerent types of
optimization problems [14]. We shall discuss this generic procedure in Section 7,
but ﬁrst enumerate the necessary components of an evolutionary multi-objective
optimization.
2.1 Evolutionary Principles
Since an evolutionary algorithm deals with a number of population members in each
generation, an EA is an ideal candidate for ﬁnding multiple Pareto-optimal solutions
in a multi-objective optimization problem. In Sections 4 to 7, we shall discuss a few
modiﬁed EAs for doing this task, but all of these methods perform the following
tasks:
1. Emphasize non-dominated solutions for progressing towards the Pareto-optimal
front.
2. Emphasize less-crowded solutions for maintaining a good diversity among ob-
tained solutions.
3. Emphasize elites to provide a faster and reliable convergence near the Pareto-
optimal front.
Diﬀerent algorithms implement the above three features diﬀerently, but they are
mostly implemented in the selection operator of an EA. They can be used while
choosing parent solutions for recombination and they can also be used while decid-
ing to accept an oﬀspring solution into the population. Potentially, a naive imple-
mentation may involve setting additional parameters for each of the above features,
but intuitively the latter two tasks (determining less-crowded solutions and deter-
mining elite population members) may become diﬃcult to implement without any
user-deﬁned parameter. We discuss these two matters in the following paragraphs.
The less-crowded solutions are those which are not surrounded by too many so-
lutions. First of all, the neighborhood can be deﬁned in either of the two spaces: (i)
objective space and (ii) variable space. Secondly, how many neighboring solutions
are too many? The answer to this question may require a parameter to resolve. The
elite solutions are those which are best in the population. In a multi-objective prob-
lem, how does a solution deﬁne to be the best, when there are multiple conﬂicting
objectives? This question can be answered somewhat by declaring all non-dominated
solutions to be the best. As shown elsewhere [5], problems having a large number
of objectives, almost all population members belong to the non-dominated front. It
is also natural to understand that not all solutions can be declared as elites and be
passed on to the next population. This leaves us with a question of which or what
proportion of non-dominated solutions are to be redeﬁned as elites? This involves
setting another parameter.
Thus, it is easy to think of an evolutionary multi-objective optimization algo-
rithm which will have components to take care of the above three essential tasks, but
such an algorithm may involve a number of additional parameters which the user
must have to set. It becomes a challenging task to come up with an EMO algorithm

246
Kalyanmoy Deb
which would perform all three tasks eﬃciently but without any additional para-
meter. We shall discuss how parameter-less implementation of such salient tasks is
achieved in a few popular EMO methodologies later in this study. But before we do
this, we would like to mention a few essential parameters which a single-objective EA
run usually demands. These parameters are also required to be supplied in running
most multi-objective EA algorithms.
3 Essential Parameters in an Evolutionary Algorithm
Many chapters of this book have dealt with the essential parameters needed in a
single-objective evolutionary algorithm (EA) and suggested various procedures of
setting up the adequate values of these parameters. Here, we brieﬂy mention the
essential parameters often needed to be ﬁxed in an EA:
1. Population size (N),
2. Number of generations (T),
3. Parameters related to Selection (selection pressure (s), selection operator, etc.),
4. Parameters related to recombination (crossover probability (pc), crossover op-
erator, etc.),
5. Parameters related to mutation (mutation probability (pm), mutation operator,
etc.),
6. Generation gap parameter (G),
7. Number of independent runs of an EA.
Population size is a crucial parameter in the successful application of an EA. For an
application without any limit in the number of overall function evaluations, there
exists a critical population size below which the EA is not expected to perform to
its best and on or beyond of which the EA performs at its best. However, additional
population members demand more function evaluations compared to that needed
by the EA which uses the critical population size. However, if an EA is applied for
a limited number of function evaluations, there exists a critical range of population
sizes below and above of which the performance of an EA gets degraded. Thus,
in such cases, it is essential to choose a population size as close to the critical
population size as possible. Some statistics-based estimation of the size of a random
initial population exist [21, 24, 33] and some numerical procedures do exist as well
[1, 35].
Even with an adequate population size and other EA operators, the EA must be
run for a critical number of generations for a convergence near the optimal solution.
For some problems (mostly test problems), a bound on number of generations is
computed from a complexity analysis [40]. However, such an analysis is usually
problem-speciﬁc and cannot be generalized for any arbitrary problem [41].
An EA is a ﬂexible optimization algorithm, because it allows the user to choose
any suitable EA operators. Since it is ﬂexible, it also puts an onus on the part of
the user to choose proper operators. There are mainly two diﬀerent EA operators –
a selection operator mimicking the Darwin’s survival of the ﬁttest principle and one
or more variation operators causing the generation of a new oﬀspring population.
Usually, two variation operators are used in most applications: (i) recombination or
crossover operator and (ii) mutation operator. Each of these operators have many
variations and each variation may involve diﬀerent additional user-deﬁned parame-
ters. For example, the following is a list of selection operators popularly used:

EMO Without Additional Parameters
247
1. s-ary tournament selection, in which s solutions are picked with or without
replacement and the best solution (in terms of their ﬁtness values) is selected.
2. Ranking selection, in which population members are ranked according to the
ﬁtness values and a higher rank (better solution) solution is selected propor-
tionately more often (by assigning s copies to the best solution).
In these selection operators, the parameter s is loosely called the selection pressure
and the performance of an EA procedure depends on the choice of this parameter s.
Goldberg and Deb [20] computed the selection pressure for the above two operators
and a few others.
Similarly, diﬀerent recombination operators and their associated parameters af-
fect the performance of an EA and must be chosen properly. One common parameter
among diﬀerent crossover operators is the ‘probability of crossover’ (pc), which de-
notes the expected proportion of the population which participates in the crossover
operation. However, more operator-speciﬁc parameters exist and must also be set
by the user. For example, the binary-coded crossover operator requires the user to
specify the number of cross-sites. For real-parameter recombination operators such
as SBX [6], BLX [16] and FR [39] require a parameter, controlling the extent of the
search, whereas some recombination operators such as PCX [9], UNDX [29], SPX
[25], diﬀerential evolution (DE) [38], operator in particle swarm optimization (PSO)
[28] require more than one parameters to be set by the user.
Mutation operator also involves a probability of mutation pm to be set. In ad-
dition, at least one mutation-speciﬁc parameter is often required to be supplied by
the user.
Besides, the generation gap is an important matter which controls the overall
selection pressure per function evaluation. On one extreme (called the ‘generational
EA’), the generation gap can be equal to the population size, meaning that all N
oﬀspring solutions must be created and evaluated before any of them is put back into
the population. On the other extreme (called the ‘steady-state EA’), the generation
gap is one, meaning that after every new oﬀspring solution is created, it is considered
for its inclusion to the population. The latter causes a larger selection pressure and
the EA is often termed as an ‘greedy EA’. Other intermediate propositions are
certainly possible [34, 27].
3.1 Desired and Undesired Parameters
In most optimization studies, an algorithm involving parameters which the users are
required to be set is considered negatively, simply because of the obtained optimized
solution may be sensitive to the parameter values. In fact, more the number of para-
meters involved in the algorithm, the worse is the situation. Often, good optimization
studies make a parametric analysis and present the sensitivities of each parameter
on the obtained result. Often researchers suggest ways to estimate parameters which
are found to have worked well on known test problems.
However, there may exist certain parameters which are desired in an algorithm
and provide the user (a designer or an applicationist) ﬂexibility and control in the
obtained solution. Although the outcome of the optimization depends on these tun-
able parameters, instead they being controlled by the developers of the algorithm,
the users prefer to choose them on their own. In multi-objective optimization algo-
rithms, such tunable parameters (for example, ϵ in the ϵ-MOEA [12] or a ﬁxation of

248
Kalyanmoy Deb
one or more reference points in a reference-point based EMO [13]) allows a decision-
maker to ﬁnd solutions closer to the desired region on the Pareto-optimal frontier.
We call these parameters as ‘desired parameters’ for a problem solving task.
4 NSGA, Contemporary Algorithms, and Additional
Parameters
In this section, we investigate the non-elitist EMO methodologies for their require-
ment of any additional parameter for achieving two tasks: (i) in determining
non-dominated solutions and (ii) in determining less-crowded solutions. Since these
methods did not use any elite-preservation operator, we ignore the third task in
these ﬁrst-generation EMO algorithms. A detail description of these algorithms can
be found in [5].
Non-dominated sorting GA or NSGA [37] and multi-objective GA or MOGA
[18] emphasized non-dominated solutions without any additional parameter, but
the niched-Pareto GA or NPGA [26] introduced a subpopulation size parameter for
deﬁning the non-dominated solutions. This parameter is required to be set by the
user.
Next, we discuss their requirement for additional parameter in implementing the
niching operator. The non-elitist EMO methodologies, which were able to ﬁnd a set
well-distributed near-Pareto-optimal solutions, all used an explicit niche-preserving
operator involving a niching parameter. Thus, these methodologies demanded an
additional parameter on top of EA parameters mentioned above. For example, the
non-dominated sorting GA or NSGA [37] required the niching parameter σshare,
which deﬁned the maximum Euclidean distance among two solutions in the variable
space for them to qualify as solutions from the same niche. Although a subsequent
study [4] has suggested a procedure of estimating this parameter, the performance
of NSGA was shown to depend on the choice of this parameter [37]. The purpose of
the niching parameter is to deﬁne the maximum distance between any two solutions
to remain in a single niche. If a small niching parameter is used, fewer solutions
will qualify to remain in a single niche, thereby clustering all solutions to more
number of niches. The eﬀect of choosing a small niching parameter in an EMO is,
therefore, to converge to many Pareto-optimal solutions. There are some diﬃculties
in using a ﬁxed niching parameter throughout the run. In most problems, the search
region where an algorithm is started is often wider than the Pareto-optimal region in
which the algorithm is expected to converge at the end. By keeping the same niching
parameter throughout, many diﬀerent niches must have to be encountered early on
to keep a good distribution of solutions, thereby requiring a larger population size.
However, near the Pareto-optimal region, a large population may not be required and
with a large population, multiple solutions will exist in each niche. Although the use
of population-best and population-worst function values can be used to normalize
the objective values and an identical niching parameter can be used throughout, this
idea causes another diﬃculty. Early on, the eﬀective niche size will be large for a
similar argument given above and there may not be enough representative solutions
to search the region eﬀectively. Thus, for both of the above cases (with or without
normalization), there is a need of a variable population size to make an eﬀective
search. Although an adaptive change of population size with generation is in order,
most EMO methodologies use a ﬁxed population size.

EMO Without Additional Parameters
249
Similarly, MOGA [18] required an identical niching parameter as in NSGA, but
the distance measure was computed in the objective space. The study suggested a
clever procedure for estimating the niching parameter and a later study [5] suggested
a normalized version of the procedure and presented a table showing the σshare value
as a function of a ﬁxed population size and number of objectives. However, these
estimation procedures are not free from approximations and some tuning may be
necessary for their proper working in an arbitrary problem.
NPGA [26] required two parameters: σshare and an additional parameter tdom –
the number of population members chosen for counting the number of similar
solutions to two competing solutions. The original study did not provide any speciﬁc
guidelines for estimating these parameters.
The need for these extra parameters for multi-objective optimization and their
demonstrated dependence on the performance of the corresponding EMO procedure
made researchers think of parameter-less EMO procedures.
5 NSGA-II and No Additional Parameters
Next, we consider the elitist EMO methodologies and begin our discussion with the
elitist non-dominated sorting or NSGA-II [8] algorithm.
The requirement of the additional parameter setting (σshare) in NSGA is
avoided in the elitist non-dominated sorting GA or NSGA-II [8]. Although the non-
dominated sorting approach is used, the NSGA-II procedure is somewhat diﬀerent
from NSGA in the following aspects:
1. NSGA-II eliminates the need of the additional parameter, which NSGA required.
2. NSGA-II uses an elite-preserving strategy, whereas NSGA did not have any
elite-preservation mechanism.
3. NSGA-II uses a computationally fast niching strategy which is applied with
objective values, instead of decision parameter values used in NSGA.
4. NSGA-II allows a ﬂexible platform to develop diﬀerent strategies, such as a
steady-state EMO, a preference-based EMO, and others, which were not possible
with the proportionate selection framework of NSGA.
The determination of the non-dominated solutions does not require any additional
parameter. The clever method of combining oﬀspring and parent populations to-
gether and choosing the best half of the combined population according to a non-
dominated sorting and crowding scheme did not require any additional parameter in
implementing the overall elite-preservation scheme. This way, if a parent solution is
better than all oﬀspring solutions, it naturally has a higher chance of getting included
in the next population, thereby implementing the elite-preservation principle with-
out any extra parameter. Such a scheme has also been used in other single-objective
EA methodologies, such as CHC [15] and evolution strategy studies [36].
Next, we discuss how the niching strategy is implemented without requiring any
additional niching parameter in NSGA-II. The original NSGA procedure degraded
the rank assigned to each solution (based on its non-dominated ranking) by the
niche count – a measure of crowdedness near the solution in the decision space. This
procedure required a niching parameter. The NSGA-II procedure used a completely
diﬀerent niching strategy. Instead of degrading the ﬁtness or assigning a rank to
each solution, a two-stage procedure is adopted here. First, the solutions are chosen

250
Kalyanmoy Deb
based on the non-domination ranking. Second, solutions, belonging to a maximum
ranking which could not be completely accepted to keep the size of the population
constant, are evaluated for their crowdedness. This two-stage procedure allows a
niching procedure to be applied to a non-dominated set of population members. The
niching task remains as the one of ﬁnding a subset of population members having
as few neighbors as possible. Such a niching task is possible to achieve without
requiring any niching parameter.
There are two ways such a task can be achieved: (i) a block reduction strategy or
(ii) a one-at-a-time pruning strategy. In both approaches, every population member
can be assigned a crowding metric value based on the extent of neighboring solutions
in the objective space or in the variable space. Thereafter, in the block reduction
strategy, the required number of solutions having the largest crowding metric value
(meaning more crowding) can be eliminated at once. In the one-at-a-time pruning
strategy [31], only one solution having the largest crowding metric value is elim-
inated and the crowding metric value is recomputed for the remaining solutions.
One solution is eliminated again and the procedure is continued till the required
number of solutions are eliminated. It is clear that the latter strategy would con-
stitute a better niching operation than the former one, but at the expense of more
computations.
Thus it is clear that a two-phase task of ﬁtness assignment and niching operation
allow no additional parameters for achieving both the tasks. NSGA-II procedure only
requires the standard EA parameters (population size, operator probabilities etc.)
to be supplied.
We argue that requirement of no further parameters in NSGA-II is one of the
main reasons of its popularity in EMO studies in the recent past. We observe that
the recent EMO conference proceedings (EMO-2005 [2]) had a total of 59 papers on
multi-objective optimization and 33 papers (about 56%) used NSGA-II directly or
used as basis for comparison. Similarly, 19 of 56 papers (about 34%) of EMO-2003
conference proceedings [17] used NSGA-II. A recent recognition of a high rate of
citation of the article describing NSGA-II by the ISI Web of Science [3] is another
testimony to its success as an eﬃcient multi-objective optimization algorithm.
6 Other EMO Methodologies and Additional Parameters
The strength Pareto EA or SPEA
[43] uses a parameter-less ﬁtness assignment
scheme based on domination level of each solution. However, the niching operator
and the elite-preserving operators involve additional parameters.
SPEA uses a clustering technique for niching to make a one-at-a-time deletion
of crowded solutions. In this approach, every non-dominated solution is assumed
to reside in a separate cluster. Thereafter, two clusters having the shortest dis-
tance between them are merged together and the solution closest to the centroid
of the merged cluster is retained and other solutions in the merged cluster are not
considered. This procedure is continued as many times as the required attrition in
population members. This way more-crowded solutions are deleted and less-crowded
solutions get emphasized. Although this niching strategy apparently does not require
any additional niching parameter, the extended SPEA or SPEA2 [42] used the dis-
tance to the k-th nearest neighbor to cluster solutions and suggested k =
√
N + N ′

EMO Without Additional Parameters
251
(N is the EA population size and N ′ is the archive size). In this sense, the origi-
nal SPEA procedure used an additional niching parameter but authors suggested a
ﬁxed value of k = 1 (in the original SPEA) or k =
√
N + N ′ in SPEA2. Besides,
the one-at-a-time deletion strategy causes SPEA or SPEA2 to have a large compu-
tational burden [12], to have a much better distribution of solutions particularly for
problems having more than two objectives.
SPEA and SPEA2 also require an archive, the size of which must be carefully
chosen to provide an adequate selection pressure to the better solutions (another
parameter!). This is because the archive contains the best non-dominated solutions
(elites) found thus far in any generation and a combination of archive and EA
population is used for choosing potential parent solutions. Thus, the ratio between
the archive size and EA population size sets up a critical selection pressure for the
elite solutions. The developers of SPEA suggested to use an archive size which is
about 1/5-th to the size of the EA population. Potentially, the archive size is another
additional parameter which SPEA or SPEA2 requires the user to set. It is interesting
to note both these parameters (k and archive size N ′) in associated in the proper
working of SPEA or SPEA2 and are not the ‘desired parameters’ from the point of
view of a user.
The Pareto-archived evolution strategy or PAES [30] used a pairwise domination-
based selection scheme, which does not require any additional parameter. However,
the niching approach compartmentalizes the objective space into a ﬁxed number of
hyper-boxes, which required an additional niching parameter. In PAES, the number
of divisions in each objective or the size of hyper-boxes in each objective is used
for this purpose. The PAES procedure attempts to have only one solution in each
hyper-box, thereby ensuring a diversity among population members. Thus, a smaller
box-size is, in eﬀect, capable of ﬁnding more optimized solutions. Since this para-
meter directly controls the number of optimized solutions found at the end of the
optimization run, it can be called as a ‘desired parameter’. By ﬁxing the box-size
(niching parameter), the user can control the number of optimized solutions. How-
ever, the user does not have a direct control of the exact number of non-dominated
solutions to be found, as the the box-size parameter is pre-speciﬁed and it is not
known beforehand the extent of the Pareto-optimal set. The elite-preserving opera-
tor compared the newly-created child solution with the parent which created it and
used a parameter-less acceptance rule based on domination level of two solutions
and the number of solutions resident to hyper-boxes of child and parent.
7 Omni-Optimizer and No Additional Parameters
Recently, the author, with the help of his student, suggested an omni-optimizer which
is capable of solving four diﬀerent optimization tasks without any intervention and
additional parameter [14]:
1. Problems having single-objective, single optimum,
2. Problems having single-objective, multiple optima,
3. Problems having multi-objective, single optimum for each Pareto-optimal point,
and
4. Problems having multi-objective, multiple optimal for each Pareto-optimal
point.

252
Kalyanmoy Deb
The advantage of such an optimization algorithm is that the algorithm degenerates
itself to ﬁnd the necessary optimal solutions (one or more) depending on the supplied
objective(s). This way the user needs only to know a single optimization algorithm
and may be able to solve diﬀerent kinds of optimization problems oﬀered by the
description of the problem (objectives and constraints).
The algorithm has a generic structure of a multi-objective optimization algo-
rithm (NSGA-II). The domination criterion of comparing two solutions for superi-
ority degenerates to making a real-valued comparison of two solutions in the case
of single-objective optimization. Thus, this ﬁrst feature of the omni-optimizer is
parameter-free. For single-objective optimization, the algorithm also degenerates to
an elite-preserving operation by ﬁrst combining both parent and oﬀspring popula-
tions and then systematically choosing half the population. Thus, the elite-preserving
operator is also parameter-free.
Since this algorithm claims to ﬁnd multiple optimal solutions simultaneously, the
omni-optimizer has an explicit niching operator. Since the algorithm also claims also
to solve both single and multi-objective optimization problems, the niching operator
is also expected to perform niching in both spaces: (i) variable space and (ii) objective
space. The diﬃculty with such a requirement is that the niching information in one
space can be completely diﬀerent from that in the other space. Thus, it may be
diﬃcult to come up with a combined niching metric to deﬁne the extent of crowding
in both spaces. If a sharing function based approach [23] is to be used, two diﬀerent
niching parameters must have to be introduced, thereby requiring the user to set two
niching parameters. However, the omni-optimizer uses a parameter-less approach
which make a hierarchical comparison of the two niching quantities and constitute
a viable algorithm. We give a brief description of the procedure below.
First, every solution in a front is compared with its neighbors objective-wise
and an objective-space crowding distance (say Do) (similar to that in NSGA-II [8])
is computed. The extreme solutions are assigned a very large crowding distance to
ensure their presence in the population. Thereafter, a variable-space crowding dis-
tance (say Dv) is computed by using a similar procedure, but all computations are
performed in the variable space. Here, the extreme solutions in the variable space
are not assigned an arbitrary large crowding distance, instead twice the crowding
distance from their nearest solutions are assigned. This way, every solution in the
population is assigned two niching quantities computed without using any extra
niching parameter: the crowding distance in the objective space and crowding dis-
tance in the variable space. To choose a subset of solutions from a non-dominated
set, the next task is to use these two measures and decide which all solutions to keep
and which all solutions to delete. One way to do this would be to choose a thresh-
old for each space and keep all solutions having the crowding distance greater than
the chosen threshold value. But this simple-minded procedure will require two new
niching parameters to be set by the user. The omni-optimizer uses a parameter-less
procedure, which we discuss next.
First, we compute the front-average of objective-wise and variable-wise crowding
distance values. Thereafter, instead of comparing the individual crowding distance
values with a threshold of some sort, we compare them with these front-wise average
values. If a solution does not have above-front-average crowding distance in both
spaces, we de-emphasize the solution by assigning the smallest of the two crowding
distance values as its overall crowding distance measure. On the other hand, if a
solution has above-average crowding distance value in any of the two spaces, we

EMO Without Additional Parameters
253
encourage it by assigning it the larger of the two crowding distance values. This
procedure is free from any niching parameter and emphasizes any solution which
has above-average crowding distance in any of the two spaces.
In the event of a single-objective problem having multiple, identical optimal
function value, all high-performing solutions will have very similar objective val-
ues, thereby making the objective-wise crowding distance value small (and more or
less identical) for each solution. In this case, solutions are chosen mainly based on
the variable-wise crowding distance value. The eﬀect is that solutions residing near
diﬀerent and well-separated optima in the variable space get emphasized in the pop-
ulation, thereby allowing multiple optimal solutions to be remain in the population.
Most niching procedures work using a similar principle, but unfortunately using a
niching parameter. For example, the sharing function approach can ﬁnd multiple op-
timal solutions simultaneously, but requires a sharing parameter to be set properly.
By comparing a crowding measure with the population-average crowding measure,
solutions can be given diﬀerential preference, thereby encouraging niche-formation
without the need of any additional niching parameter.
Similarly, if in a multi-objective optimization problem, each Pareto-optimal so-
lution corresponds to multiple variable vectors, it would be desirable to locate all or
as many such optimal variable vectors as possible. The above two-space crowding
distance consideration allows a nice way to ﬁnd such multiple optimal solutions as
well. To illustrate, we consider the following two-objective minimization problem:
Minimize f1(x) =
n
i=1
sin(πxi),
Minimize f2(x) =
n
i=1
cos(πxi),
0 ≤xi ≤6,
i = 1, 2, . . . , n.
(2)
Here, n = 5 and both objectives are periodic functions with period of 2. The Pareto-
optimal solutions are xi ∈[2m + 1, 2m + 3/2], where m is an integer. In the above
bound, there are exactly three solutions (in [1.0,1.5], [3.0,3.5] and [5.0,5,5]) which
produce the same objective values. For every eﬃcient point in the objective space
there are in general 35 = 243 Pareto-optimal solutions in the variable space. We
choose a population of size 1,000 and run the algorithm for 500 generations to
capture as many Pareto-optimal solutions as possible. It is interesting to note that
both omni-optimizer and the original NSGA-II ﬁnd the entire range of eﬃcient
points in the objective space, as shown in Figures 3 and 4. However, the variable
space plots show a diﬀerent scenario. The lower diagonal plots in Figure 5 show the
performance of the omni-optimizer and the upper diagonal plots show that of the
original NSGA-II. It is clear that in a pair-wise plot of variables, all 32 or 9 optimal
regions are found by the omni-optimizer, whereas since no variable-space crowding
is considered in the original NSGA-II, not all optimal combinations are found.
8 Conclusions
In this paper, we have functionally decomposed the tasks needed in an evolutionary
multi-objective optimization (EMO) algorithm and identiﬁed the critical compo-
nents which are vulnerable to be designed for additional parameters. These are the

254
Kalyanmoy Deb
niching operator and the elite-preserving operator. By systematically analyzing a
number of existing EMO methodologies, we have shown that an EMO methodology
can be designed without introducing any additional parameter to those needed in a
standard single-objective EA. The elitist non-dominated sorting GA or NSGA-II is
one such EMO algorithm and a major reason for its popularity can be contributed to
the fact that it does not demand setting any further parameters than those required
by single-objective EAs. Often, such parameter-less implementations are innova-
tive and attempt to design such procedures may spur novel ideas in other similar
algorithm developmental tasks.
Acknowledgments
The author wishes to thank Santosh Tiwari for implementing the idea of omni-
optimizer. The author also acknowledges the support provided by STMicroelectron-
ics for performing this study.
References
1. T. P. Bagchi and K. Deb. Calibration of GA parameters: The design of experi-
ments approach. Computer Science and Informatics, 26(4):46–56, 1996.
2. C. A. Coello Coello, A. H. Aguirre, and E. Zitzler, editors. Evolutionary Multi-
Criterion Optimization: Third International Conference. Berlin, Germany:
Springer, 2005. LNCS 3410.
3. K. Deb. IEEE TEC Paper (2002, vol. 6, pp. 182–197) on NSGA-II: Fast
breaking paper in the ﬁeld of engineering. ISI Web of Science, http://esi-
topics.com/fbp/2004/february04-KalyanmoyDeb.html.
4. K. Deb. Multi-objective evolutionary optimization: Past, present and future.
In I. C. Parmee, editor, Evolutionary Design and Manufacture, pages 225–236.
London: Springer, 2000.
5. K. Deb. Multi-objective optimization using evolutionary algorithms. Chichester,
UK: Wiley, 2001.
6. K. Deb and R. B. Agrawal. Simulated binary crossover for continuous search
space. Complex Systems, 9(2):115–148, 1995.
7. K. Deb, S. Agrawal, A. Pratap, and T. Meyarivan. A fast elitist non-dominated
sorting genetic algorithm for multi-objective optimization: NSGA-II. In Proceed-
ings of the Parallel Problem Solving from Nature VI (PPSN-VI), pages 849–858,
2000.
8. K. Deb, S. Agrawal, A. Pratap, and T. Meyarivan. A fast and elitist multi-
objective genetic algorithm: NSGA-II. IEEE Transactions on Evolutionary
Computation, 6(2):182–197, 2002.
9. K. Deb, A. Anand, and D. Joshi. A computationally eﬃcient evolutionary al-
gorithm for real-parameter optimization. Evolutionary Computation Journal,
10(4):371–395, 2002.
10. K. Deb, S. Chaudhuri, and K. Meitinnen. I-EMO: An interactive evolution-
ary multi-objective optimization tool. Technical Report KanGAL Report Num-
ber 2005005, Department of Mechanical Engineering, IIT Kanpur, India, 2005.

EMO Without Additional Parameters
255
Also in Proceedings of Pattern Recognition in Machine Intelligence (PReMI’05),
Springer.
11. K. Deb and D. E. Goldberg. An investigation of niche and species formation in
genetic function optimization. In Proceedings of the Third International Con-
ference on Genetic Algorithms, pages 42–50, 1989.
12. K. Deb, M. Mohan, and S. Mishra. Towards a quick computation of well-
spread pareto-optimal solutions.
In Proceedings of the Second Evolution-
ary Multi-Criterion Optimization (EMO-03) Conference (LNCS 2632), pages
222–236, 2003.
13. K. Deb and J. Sundar.
Reference point based multi-objective optimization
using evolutionary algorithms. In Proceeedings of the Genetic and Evolutionary
Computation Conference (GECCO-2006), in press.
14. K. Deb and S. Tiwari. Omni-optimizer: A generic evolutionary algorithm for
global optimization. European Journal of Operations Research (EJOR), in press.
15. L. J. Eshelman. The CHC adaptive search algorithm: How to have safe search
when engaging in nontraditional genetic recombination.
In Foundations of
Genetic Algorithms 1 (FOGA-1), pages 265–283, 1991.
16. L. J. Eshelman and J. D. Schaﬀer. Real-coded genetic algorithms and interval-
schemata. In Foundations of Genetic Algorithms 2 (FOGA-2), pages 187–202,
1993.
17. C. Fonseca, P. Fleming, E. Zitzler, K. Deb, and L. Thiele. Proceedings of the Sec-
ond Evolutionary Multi-Criterion Optimization (EMO-03) Conference (Lecture
Notes in Computer Science (LNCS) 2632). Heidelberg: Springer, 2003.
18. C. M. Fonseca and P. J. Fleming. An overview of evolutionary algorithms
in multi-objective optimization. Evolutionary Computation Journal, 3(1):1–16,
1995.
19. D. E. Goldberg.
Genetic Algorithms for Search, Optimization, and Machine
Learning. Reading, MA: Addison-Wesley, 1989.
20. D. E. Goldberg and K. Deb. A comparison of selection schemes used in genetic
algorithms. In Foundations of Genetic Algorithms 1 (FOGA-1), pages 69–93,
1991.
21. D. E. Goldberg, K. Deb, and J. H. Clark. Genetic algorithms, noise, and the
sizing of populations. Complex Systems, 6(4):333–362, 1992.
22. D. E. Goldberg, K. Deb, and D. Thierens. Toward a better understanding of
mixing in genetic algorithms. Journal of the Society of Instruments and Control
Engineers (SICE), 32(1):10–16, 1993.
23. D. E. Goldberg and J. Richardson. Genetic algorithms with sharing for multi-
modal function optimization. In Proceedings of the First International Confer-
ence on Genetic Algorithms and Their Applications, pages 41–49, 1987.
24. G. Harik, E. Cant´u-Paz, D. E. Goldberg, and B. L. Miller.
The gambler’s
ruin problem, genetic algorithms, and the sizing of populations. Evolutionary
Computation Journal, 7(3):231–254, 1999.
25. T. Higuchi, S. Tsutsui, and M. Yamamura. Theoretical analysis of simplex cross-
over for real-coded genetic algorithms. In Parallel Problem Solving from Nature
(PPSN-VI), pages 365–374, 2000.
26. J. Horn, N. Nafploitis, and D.E. Goldberg. A niched Pareto genetic algorithm
for multi-objective optimization. In Proceedings of the First IEEE Conference
on Evolutionary Computation, pages 82–87, 1994.
27. K.A. De Jong and J. Sharma. Generation gap revisited.
In Foundations of
Genetic Algorithms (FOGA-II), pages 19–28, 1992.

256
Kalyanmoy Deb
28. James Kennedy and Russell C. Eberhart. Swarm intelligence. Morgan Kauf-
mann, 2001.
29. H. Kita, I. Ono, and S. Kobayashi. Multi-parental extension of the unimodal
normal distribution crossover for real-coded genetic algorithms. In Proceedings
of the 1999 Congress on Evolutionary Computation, pages 1581–1587, 1999.
30. Joshua D. Knowles and David W. Corne. Approximating the non-dominated
front using the Pareto archived evolution strategy. Evolutionary Computation
Journal, 8(2):149–172, 2000.
31. S. Kukkonen and K. Deb. Improved pruning of non-dominated solutions based
on crowding distance for bi-objective optimization problems. In Proceedings of
the Congress on Evolutionary Computation (CEC-06), in press.
32. N. J. Radcliﬀe. Forma analysis and random respectful recombination. In Pro-
ceedings of the Fourth International Conference on Genetic Algorithms, pages
222–229, 1991.
33. C. R. Reeves. Using genetic algorithms with small populations. In Proceedings
of the Fifth International Conference on Genetic Algorithms, pages 92–99, 1993.
34. H. Satoh, M. Yamamura, and S. Kobayashi.
Minimal generation gap model
for gas considering both exploration and exploitation.
In Proceedings of the
IIZUKA: Methodologies for the Conception, Design, and Application of Intelli-
gent Systems, pages 494–497, 1996.
35. J. D. Schaﬀer, R. A. Caruana, L. J. Eshelman, and R. Das. A study of control
parameters aﬀecting online performance of genetic algorithms. In Proceedings
of the International Conference on Genetic Algorithms, pages 51–60, 1989.
36. H.-P. Schwefel. Evolution and Optimum Seeking. New York: Wiley, 1995.
37. N. Srinivas and K. Deb. Multi-objective function optimization using non-
dominated sorting genetic algorithms. Evolutionary Computation Journal,
2(3):221–248, 1994.
38. R. Storn and K. Price. Diﬀerential evolution – A fast and eﬃcient heuristic
for global optimization over continuous spaces. Journal of Global Optimization,
11:341–359, 1997.
39. H.-M. Voigt, H. M¨uhlenbein, and D. Cvetkovi´c. Fuzzy recombination for the
Breeder Genetic Algorithm. In Proceedings of the Sixth International Conference
on Genetic Algorithms, pages 104–111, 1995.
40. I. Wegener and T. Jansen. Real royal road functions - where crossover provably
is essential. Discrete Applied Mathematics, 149:111–125, 2005.
41. D. H. Wolpert and W. G. Macready. No free lunch theorems for optimization.
IEEE Transactions on Evolutionary Computation, 1(1):67–82, 1977.
42. E. Zitzler, M. Laumanns, and L. Thiele. SPEA2: Improving the strength pareto
evolutionary algorithm for multiobjective optimization. In K. C. Giannakoglou,
D. T. Tsahalis, J. P´eriaux, K. D. Papailiou, and T. Fogarty, editors, Evolu-
tionary Methods for Design Optimization and Control with Applications to In-
dustrial Problems, pages 95–100, Athens, Greece, 2001. International Center for
Numerical Methods in Engineering (Cmine).
43. E. Zitzler and L. Thiele. Multiobjective evolutionary algorithms: A comparative
case study and the strength pareto approach. IEEE Transactions on Evolution-
ary Computation, 3(4):257–271, 1999.

EMO Without Additional Parameters
257
1
2
With niching
(Omni−optimizer)
−5
−4.5
−4
−3.5
−3
−2.5
−2
−1.5
−1
−0.5
 0
−5 −4.5 −4 −3.5 −3 −2.5 −2 −1.5 −1 −0.5
 0
f (x)
f (x)
Fig. 3. Eﬃcient points using omni-
optimizer.
1
2
Without niching
(NSGA−II)
−5
−4.5
−4
−3.5
−3
−2.5
−2
−1.5
−1
−0.5
 0
−5 −4.5 −4 −3.5 −3 −2.5 −2 −1.5 −1 −0.5
 0
f (x)
f (x)
Fig. 4. Eﬃcient points using NSGA-II.
1
2
1
3
1
4
1
5
2
3
2
4
2
5
3
4
3
5
2
1
3
2
4
3
4
5
5
4
5
3
4
2
4
1
3
1
5
1
5
2
x_1
x_4
x_3
x_2
x_5
 0
 1
 2
 3
 4
 5
 6
 0
 1
 2
 3
 4
 5
 6
x
x
 0
 1
 2
 3
 4
 5
 6
 0
 1
 2
 3
 4
 5
 6
x
x
 0
 1
 2
 3
 4
 5
 6
 0
 1
 2
 3
 4
 5
 6
x
x
 0
 1
 2
 3
 4
 5
 6
 0
 1
 2
 3
 4
 5
 6
x
x
 0
 1
 2
 3
 4
 5
 6
 0
 1
 2
 3
 4
 5
 6
x
x
 0
 1
 2
 3
 4
 5
 6
 0
 1
 2
 3
 4
 5
 6
x
x
 0
 1
 2
 3
 4
 5
 6
 0
 1
 2
 3
 4
 5
 6
x
x
 0
 1
 2
 3
 4
 5
 6
 0
 1
 2
 3
 4
 5
 6
x
x
 0
 1
 2
 3
 4
 5
 6
 0
 1
 2
 3
 4
 5
 6
x
x
 0
 1
 2
 3
 4
 5
 6
 0
 1
 2
 3
 4
 5
 6
x
x
 0
 1
 2
 3
 4
 5
 6
 0
 1
 2
 3
 4
 5
 6
x
x
 0
 1
 2
 3
 4
 5
 6
 0
 1
 2
 3
 4
 5
 6
x
x
 0
 1
 2
 3
 4
 5
 6
 0
 1
 2
 3
 4
 5
 6
x
x
 0
 1
 2
 3
 4
 5
 6
 0
 1
 2
 3
 4
 5
 6
x
x
 0
 1
 2
 3
 4
 5
 6
 0
 1
 2
 3
 4
 5
 6
x
x
 0
 1
 2
 3
 4
 5
 6
 0
 1
 2
 3
 4
 5
 6
x
x
 0
 1
 2
 3
 4
 5
 6
 0
 1
 2
 3
 4
 5
 6
x
x
 0
 1
 2
 3
 4
 5
 6
 0
 1
 2
 3
 4
 5
 6
x
x
 0
 1
 2
 3
 4
 5
 6
 0
 1
 2
 3
 4
 5
 6
x
x
 0
 1
 2
 3
 4
 5
 6
 0
 1
 2
 3
 4
 5
 6
x
x
 0
 1
 2
 3
 4
 5
 6
 0
 1
 2
 3
 4
 5
 6
 0
 1
 2
 3
 4
 5
 6
 0
 1
 2
 3
 4
 5
 6
 0
 1
 2
 3
 4
 5
 6
 0
 1
 2
 3
 4
 5
 6
 0
 1
 2
 3
 4
 5
 6
 0
 1
 2
 3
 4
 5
 6
 0
 1
 2
 3
 4
 5
 6
 0
 1
 2
 3
 4
 5
 6
Fig. 5. Pareto-optimal solutions with omni-optimizer (left) and NSGA-II (right).
The axes in a (i, j)-plot correspond to variables xi and xj.

Parameter Setting in Parallel Genetic
Algorithms
Erick Cant´u-Paz
Yahoo!, Inc.
701 First Avenue
Sunnyvale, CA 94089
cantupaz@acm.org
Summary. Parallel genetic algorithms (GAs) have numerous parameters that aﬀect
their eﬃciency and accuracy. Traditionally, these parameters have been studied using
empirical studies whose generality and limitations are diﬃcult to assess. This chapter
reviews existing theoretical models that predict the eﬀects of the parameters. The
models are used to examine the eﬀect of communication topologies, migration rates,
population sizing, and the choice of migrants and the individuals they replace in the
receiving populations. The models should help practitioners make informed decisions
about the setting of parameters of parallel GAs.
1 Introduction
One of the most promising alternatives to improve the eﬃciency of genetic algorithms
(GAs) is to use parallel implementations
[1, 40, 11, 4, 60]. While it is relatively
easy to implement eﬃcient parallel versions of GAs, parallel GAs are complex non-
linear algorithms that are controlled by many parameters that aﬀect their eﬃciency
and the quality of their search. For example, one must decide whether to use a
single or multiple populations. In either case, the size of the populations must be
determined carefully, and for multiple populations, one must decide how many to
use. In addition, the populations may remain isolated or they may communicate
during the run by exchanging individuals or some other information. Communication
involves extra costs and additional decisions on the pattern of communications, on
the number of individuals to be exchanged, and on the frequency of communications.
This chapter reviews the existing theory that explains the eﬀect of the numerous
parameters of these algorithms on their accuracy and eﬃciency.
The chapter deals with two common types of parallel GAs. Single-population
master-slave GAs have a master node that stores the population and executes the
GA operations (selection, crossover, and mutation). The evaluation of ﬁtness is
distributed among several slave processors (see Figure 1). Despite being very simple
algorithms, master-slave implementations can be very eﬃcient as will be shown in
section 2.
E. Cant´u-Paz: Parameter Setting in Parallel Genetic Algorithms, Studies in Computational
Intelligence (SCI) 54, 259–276 (2007)
www.springerlink.com
© Springer-Verlag Berlin Heidelberg 2007

260
Erick Cant´u-Paz
Master
Slaves
Fig. 1. A schematic of a master-slave parallel GA. The master stores the population,
executes the GA operations, and distributes individuals to the slaves. The slaves
evaluate the ﬁtness of the individuals.
Fig. 2. A schematic of a multiple-population parallel GA. Each circle represents
a simple GA, and there is (infrequent) communication between the populations. In
this example, the populations are arranged in a ring, but many other communication
topologies have been used.
Multiple-population GAs are the most sophisticated and popular type of parallel
GAs. They consist of several subpopulations that exchange individuals (Figure 2 has
a schematic). This exchange of individuals is called migration, and is controlled by
several parameters such as the frequency of migration, the number and destination
of migrants, and the method used to select which individuals migrate. The eﬀect
of these parameters on the search is complex and these algorithms are the focus of
most of the research on parallel GAs. Section 3 reviews the theory relevant to these
algorithms.
2 Master-Slave Parallel GAs
Probably the easiest way to implement GAs on parallel computers is to distribute
the evaluation of ﬁtness among several slave processors while one master executes
the GA operations (selection, crossover, and mutation). Master-slave parallel GAs
are interesting and important for several reasons: (1) they explore the search space
in exactly the same manner as serial GAs, and therefore the parameter settings used
for simple GAs are directly applicable; (2) they are very easy to implement, which

Parameter Setting in Parallel Genetic Algorithms
261
Tc
Tc
Tcomp
Tcomp
Tc
Tc
Tc
1 generation
idle
master
slave1
slave2
time
Fig. 3. One generation in a master-slave parallel GA when the master evaluates a
fraction of the population.
makes them popular with practitioners; and (3) in many cases master-slave GAs
result in signiﬁcant improvements in performance.
The only parameter introduced by this mode of parallelization is the number of
processors. Although at ﬁrst it may seem best to use all available processors, the
communication time might increase to the point of obliterating the gains obtained
by distributing the computing load between the processors.
Centering our attention on the master processor, Figure 3 depicts the sequence
of events in every generation. First, the master sends a fraction of the population
to each of the slaves, using time Tc to communicate with each. Next, the master
evaluates a fraction of the population using time
nTf
P , where Tf is the time required
to evaluate one individual, n is the size of the population, and P is the number of
processors used.
The slaves start evaluating their portion of the population as soon as they receive
it, and return the evaluations to the master as soon as they ﬁnish. We ignore the
time consumed by selection, crossover, and mutation because it is usually much
shorter than the time used to evaluate and to communicate individuals. In addition,
we assume that the same number of individuals are assigned to each slave and that
evaluation time is the same for all individuals. With these assumptions, the elapsed
time for one generation of the parallel GA may be estimated as the sum of elapsed
time spent in computations and time used to communicate information among the
processors:
Tp = PTc + nTf
P .
(1)
As more slaves are used, the computation time decreases as desired, but the
communication time increases. Making
∂Tp
∂P = 0 and solving for P, we obtain the
optimal number of processors that minimizes the execution time:
P∗=
	
nTf
Tc .
(2)
Deﬁning the parallel speedup as
Ts
Tp =
nTf
nTf
P
+ PTc
(3)

262
Erick Cant´u-Paz
1
5
10
50
100
500
1000
Processors
1
5
10
50
100
500
1000
Speedup
Fig. 4. Theoretical speedups of a master-slave GA varying the value of γ. The
thinest line corresponds to γ = 1, the intermediate to γ = 10, and the thickest to
γ = 100. The dotted line is the ideal (linear) speedup.
and substituting P∗, gives the maximum speedup possible as
S∗
p = 1
2P∗.
(4)
Figure 4 shows the theoretical speedups of a master-slave GA varying the ratio
γ = Tf/Tc. As γ increases, more processors may be used eﬀectively to reduce the ex-
ecution time. The ﬁgure considers a master-slave GA with a population of n = 1000
individuals, and the speedups are plotted for γ = 1, 10, 100. In many practical prob-
lems, the function evaluation time is much greater than the time of communications,
Tf ≫Tc (γ ≫1), and master-slave parallel GAs can deliver near-linear speedups
for a large range of processors.
However, equation 4 clearly points out that at their optimal conﬁguration, the
simple master-slave GAs have an eﬃciency of 50%: half of the time the processors
are idle or communicating. A major cause of this ineﬃciency is that the master
waits for all the slaves to ﬁnish the ﬁtness evaluations before selecting the parents
to the next generation. This synchronization ensures that the master-slave produces
exactly the same results as a simple serial GA, but it is easy to avoid if we are willing
to accept a diﬀerent behavior of the algorithm.
In asynchronous master-slave GAs, the master generates individuals and sends
them to the slaves to be evaluated. As soon as a slave ﬁnishes its share of the eval-
uations, the master inserts the evaluated individual(s) into the population. Then,
the master generates new individuals, and sends them to any available slaves. Asyn-
chronous master-slaves have the potential to be more eﬃcient than the synchronous
algorithm (e.g., [63, 55]), especially if the evaluation times are not constant for all
individuals, but they introduce additional parameters.
In particular, there are several options about how many individuals are generated
at a time, and about how they are incorporated into the population. Some of these
choices may increase the selection pressure and require larger population sizes [13,
15].
In addition, in asynchronous master-slaves, individuals may return from the
slaves in a diﬀerent order in which they were created, because some slaves will
ﬁnish their evaluations faster than others. Davison et al. [23] identiﬁed this problem
and studied experimentally the eﬀect of allowing random returns while varying the

Parameter Setting in Parallel Genetic Algorithms
263
number of slaves. They found that by replacing a low-ﬁt individual that is similar
to the new one, the eﬀect of accepting individuals in random order was very small.
Asynchronous master-slave GAs are only slightly more diﬃcult to implement
than the synchronous, and the gains in performance may easily oﬀset the extra cost
of development. However, their mathematical analysis is signiﬁcantly more diﬃcult.
Until this point, the calculations have assumed that Tc is constant, and that each
slave received a single message from the master with all the individuals that the slave
must evaluate. In reality, a better expression for Tc may be Tc = Bx + L, where
B is the inverse of the bandwidth of the network, x is the amount of information
transmitted, and L is the latency of communications. The latency is the overhead
per message that depends on the operating system, the programming environment,
and on the particular hardware, and can easily dominate the communication time.
Cantu-Paz [13] contains calculations that indicate that equation 2 is a good estimator
of the optimal number of slaves.
Gagn´e et al. [24] perform similar calculations and include a term to account for
possible failures of the slave nodes. Their calculations also take into account that
the master can use multiple messages to send the individuals that each slave will
evaluate.
Of course, the master-slave method has been used to parallelize other evolution-
ary algorithms, such as evolution strategies [5], evolutionary programming [37], and
genetic programming [45]. The models presented in this section are applicable to
those algorithms as well as to GAs.
3 Multi-Population Parallel GAs
The existing theory of multi-population parallel GAs deals mainly with (1) the
sizing of each population and (2) with the eﬀects of migration. These two factors
are related, but we will look ﬁrst at the eﬀects of population sizing, since this is
extremely important for the quality and eﬃciency of the algorithm.
3.1 Models of Population Sizing
The existing population sizing models for multi-population parallel GAs are derived
from the gambler’s ruin model of population sizing for simple GAs [34]. This is a very
simple model that originates from considering the number of correct building blocks
in a partition of the population as a random quantity (see Figure 5). A partition is
given by a template of k ﬁxed symbols F and “don’t care” symbols ∗that match any
character in the alphabet chosen to represent solutions. Assuming binary alphabets,
a partition speciﬁed by k F symbols contains 2k schemata. A schema represents the
class of individuals that match each of the ﬁxed positions. The schema with the
highest average ﬁtness and that matches the global optimum in the ﬁxed positions
is called the correct building block.
The gambler’s ruin model views the GA search as a series of competitions that
ends either when the entire population contains the building block or when the
building block has completely disappeared from the population. For some problems,
we can calculate the probability p that selection will add one copy of the correct
building block to the population [27]. Given p, the probability that the population

264
Erick Cant´u-Paz
x=n/2^k
x=0
x=n
p
q
Fig. 5. The bounded one-dimensional space of the gambler’s ruin problem. The
absorbing barrier on the right represents a population full of representatives of
the correct building block. The probability of reaching that barrier depends on
the starting point and on the probability of adding one building block in each step,
given by p.
will be eventually full of copies of the correct building block can be approximated
as [34]:
Pbb ≈1 −

1 −p
p
n/2k
,
(5)
where n is the population size and k is the order of the building blocks we are
considering. The exponent of the equation above is the number of correct building
blocks of order k we expect in a randomly initialized population of size n if we are
using binary encodings. For a given desired probability of success ˆ
P1, we can solve
the equation above for n and obtain a population-sizing equation:
n = 2k ln(1 −ˆ
P1)
ln 
 1−p
p

.
(6)
This simple yet accurate model for simple GAs is the basis for modeling the ef-
fect of diﬀerent parameters on the solution quality of multipopulation parallel GAs.
An extremely simple case of a multi-population parallel GA is to run r independent
GAs and report the answer of the best of the runs to the user. Intuitively, this strat-
egy makes sense: Since GAs are stochastic and there will be some variance in the
quality of the results, reporting the best of r runs should result in a better solution
than running the GA only once. An alternative view is that if we keep the desired
quality constant, we could reduce the population size, execute the independent (re-
duced size) GAs in parallel, and reach the desired solution faster than with a single
GA. However, at least for linearly decomposable functions, this strategy is not very
eﬃcient [17, 18]. To reduce the population size and still reach the same solution
quality, we would need to substitute ˆ
P1 in our population sizing equation by
ˆ
Pr = ˆ
P1 −
√
ln r
√
2m
,
(7)
where m is the number of partitions in the problem. This new probability of success
does not diﬀer much from ˆ
P1 as more independent runs r as used. Therefore, the
population size required by multiple independent runs is not much smaller than the
size required by a single GA to reach the same solution, and there are almost no
time savings.
Empirical studies [58, 18] conﬁrm that using isolated populations is not a very ef-
fective or eﬃcient approach. Most multi-population parallel GAs exchange a number
of individuals between the populations. Empirically, this exchange has been known

Parameter Setting in Parallel Genetic Algorithms
265
for a long time to improve the eﬀectiveness and eﬃciency of the algorithms, and
early studies [58] suggest that using “moderate” values for the migration parame-
ters results in a good tradeoﬀbetween eﬃciency and accuracy.
The modeling of multi-population GAs concentrates on one population and ﬁnd-
ing out the eﬀect of the migration parameters on the solution quality. Examining
the gambler’s ruin model (equation 5) we can see that the probability of success
depends on the value of p and the initial position of the imaginary particle on the
random walk space. The value of p depends only on the problem to be solved, not
on the algorithm or any of its parameters. On the other hand, the initial position
of the particle corresponds to the number of BBs present at the beginning of a GA
run, and is the focus of the analysis.
To use of the gambler’s ruin model, we consider an extreme case where mi-
gration occurs only after the populations converge [17, 19]. After convergence, the
populations communicate, each sending a number of individuals to some of the other
populations. The populations incorporate the migrants and restart. If we can com-
pute the expected number of BBs contained on the exchanged individuals, we can
predict the quality of solutions after the populations restart simply by changing the
exponent of Equation 5 to reﬂect the expected number of correct building blocks
after the ﬁrst migration.
Following the idea of changing the initial state for the gambler’s ruin model,
Cant´u-Paz and Goldberg [19] analyzed the eﬀect of arbitrary migration rates and
sparse topologies.
Essentially, their calculations assumed that to reach a particu-
lar solution quality, the populations must start with some initial number of correct
building blocks x1 (replacing this number in the exponent of the gambler’s ruin
model gives the required success probability). For a given a migration rate, it is easy
to compute the probability that a population receives at least x1 building blocks
from its neighbors. Actually, there are several combinations of migration rate, num-
ber of neighbors, and population size that result in the same probability of success:
large populations with few neighbors have the same chance of contributing x1 copies
of the BB as small populations with many neighbors. Since the size of the popula-
tions mainly determines the computation time and the number of neighbors largely
determines the communications time, there is a tradeoﬀbetween communications
and computation similar to the one for the master-slaves. This tradeoﬀcan be op-
timized to ﬁnd the optimal number of neighbors and corresponding population size
that minimizes the parallel execution time. Interestingly, the optimal number of
neighbors is O

nTf/Tc

, which is the same as the optimal number of processors
in master-slaves.
With multiple restarts, the populations are inﬂuenced not only by their direct
neighbors, but by the neighbors of their neighbors and so on. The multiple restarts
in eﬀect create an “extended neighborhood” composed of all the populations that
contribute migrants to a population. The gambler’s ruin model again gives us a way
to calculate the probability of success. For some topologies of communications where
the populations have δ neighbors, after τ epochs the extended neighborhood grows
linearly reaching δ(τ −1) + 1 = δτ ′ + 1 populations. The number of individuals in
these extended neighborhood is
nτ = (
√
δτ ′ + 1)nd,
(8)

266
Erick Cant´u-Paz
20
40
60
80
100 120 140
Deme size
0.2
0.4
0.6
0.8
1
Proportion BBs
Fig. 6. Theoretical predictions (line) and experimental results (dots) of the average
quality per deme after 1, 2, 3, and 4 epochs (from right to left) using eight demes
connected by a +1+2 topology.
50
100
150
200
250
Pop size
0.2
0.4
0.6
0.8
Pbb
(a) 20-BB 4-bit trap.
500
1000
1500
2000
2500
3000 Pop size
0.2
0.4
0.6
0.8
Pbb
(b) 10-BB 8-bit trap.
Fig. 7. In the limit, a parallel GA with r fully connected populations using a
maximal migration rate (dots) has the same chance of ﬁnding the solution as a
simple GA with an aggregate population (continuous line).
where nd is the number of individuals in each population. Simply substituting this
extended population size into the gambler’s ruin model (Equation 5) gives very
accurate predictions of the probability of success, as evidenced in ﬁgure 6. Empirical
evidence suggests that even when the extended neighborhood does not grow linearly,
using nτ in the gambler’s ruin model is very accurate [13]. These results imply that
the degree of the connectivity is a crucial factor in the solution quality and that
the speciﬁc details of how the populations are connected is not as important. This
observation is corroborated by other empirical studies [20] as well as by a more exact
analysis with Markov chains [14].
Markov chains allow us to predict the solution quality after each epoch and also
in the long run, when all the populations converge to the same solution and no
further improvement is possible. Figure 7 presents plots of the long-run behavior of
four demes with 0 ≤nd ≤100 individuals each, and of Pbb (Equation 5) using a
population size of n = 4nd. The plots overlap perfectly, suggesting that the proba-
bility that r fully-connected demes of size nd converge correctly in the long run is

Parameter Setting in Parallel Genetic Algorithms
267
0.05
0.1
0.15
0.2
0.25
Mig rate
0.7
0.8
0.9
P
Fig. 8. The probability of converging to the correct BB increases with higher mi-
gration rates. The theoretical predictions (continuous line) are compared against
experimental results.
the same as the probability of success of a GA with a single population with rnd
individuals. This is important because it suggests that, in the long run, the solu-
tion’s quality does not degrade or improve when a population is partitioned into
smaller fully-connected demes that communicate with the maximum migration rate
possible.
Figure 8 illustrates the probability of reaching the correct solution as a function
of the migration rate. The example uses four fully-connected demes with 50 indi-
viduals each; the test function is a 20-BB 4-bit trap problem; and only the ﬁrst two
epochs are considered (i.e., the populations run independently, converge, exchange
individuals, restart, and converge again). Note that the probability of success in-
creases rapidly with higher migration rates.
Figure 9 shows the probability that, in the long run, the parallel GA will converge
to the correct solution as a function of the migration rate. The plot suggests that
only a moderate migration rate is suﬃcient to reach the same solution as a simple
GA with a aggregate population. Note that since all the individuals are the same
when migration occurs, there are no cost penalties associated with higher rates: only
one individual needs to be sent and it can be replicated any number of times at the
receiving deme.
The next set of results compares the quality of the solutions reached using diﬀer-
ent topologies. Figure 10 shows plots of the quality versus the number of epochs for
a fully-connected topology and three topologies frequently used by practitioners: a
uni-directional ring, a bi-directional ring, and a hypercube. Each of the eight demes
has 30 individuals, and the migration rate was set to the maximum value possible
for each topology: 50% for the uni-directional ring, 33% for the bi-directional ring,
and 25% for the hypercube.
In all cases, the quality increases as more epochs are used, but the rate of increase
depends on each topology. The uni-directional ring needs the most epochs to reach
the highest quality possible—which is the same quality that simple GA with an
aggregate population would reach—while the fully-connected topology realizes its
full potential using the fewest epochs.
The results above suggest that, at any given time, topologies where the demes
have more neighbors reach solutions of higher quality than sparse topologies: at
any epoch the fully-connected topology reached the best solutions, while the uni-
directional ring reached the worst. To visualize the eﬀect of the degree more clearly,

268
Erick Cant´u-Paz
0.05
0.1
0.15
0.2
0.25
Mig rate
0.6
0.7
0.8
0.9
1
P
Fig. 9. The long-run probability of converging to the correct BB increases rapidly
with higher migration rates. The example considers four fully-connected demes, with
50 individuals each, working on a 20-BB 4-bit trap function. The horizontal line is
the probability that four fully-connected demes with maximal migration (or a simple
GA with 200 individuals) will eventually converge to the right BB.
1
2
3
4
5
6
7
Epochs
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Proportion BBs
(a) Theory.
1
2
3
4
5
6
7
Epochs
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Proportion BBs
(b) Experiments.
Fig. 10. Solution quality after several epochs. The graphs include data for uni- and
bi-directional rings, a hypercube, and a fully-connected topology (from bottom to
top, respectively). The horizontal line in both graphs is the prediction of the quality
that would be reached by a simple GA with an aggregate population.
Figure 11 shows experiments where the degree varies while the deme size and mi-
gration rate are constant. The experiments used the four topologies used previously
(with degrees 1, 2, 3, and 7). The experiments considered eight demes with 40 indi-
viduals each, and the quality was measured at the end of the second epoch. In the
ﬁrst experiment, the migration rate was set to 10% per deme (so the number of mi-
grants received by a deme increases with the degree), and in the second experiment
the total number of migrants was set to 20.
The analyses presented in these section allow us to make some observations
relating the parameters with the quality of solutions. First, we observed that the
greatest gain in quality always occurs after the second epoch. Second, the results
suggest that—in the long run—the parallel GAs reach solutions of the same quality
as a simple GA with a population equivalent to the aggregate of the demes. Parti-
tioning the population does not degrade or improve the solution quality, as long as

Parameter Setting in Parallel Genetic Algorithms
269
1
2
3
4
5
6
7
Degree
0.5
0.6
0.7
0.8
0.9
1
Proportion BBs
(a) 10% migration per deme.
1
2
3
4
5
6
7
Degree
0.5
0.6
0.7
0.8
0.9
1
Proportion BBs
(b) 20 migrants total.
Fig. 11. Solution quality as a function of the degree of the topology. The continu-
ous lines are the theoretical predictions, and the dashed lines are the experimental
results.
the migration rate is not very low. Furthermore, it appears that only a few epochs
are necessary to reach the same solution as the simple GA. Another important obser-
vation is that the quality improves with higher migration rates and more neighbors
per deme. of the migration rate.
3.2 Other Eﬀects of Migration
The previous section showed that migration greatly aﬀects the solution quality and
that extensions of the gambler’s ruin model can predict the quality very accurately,
given certain assumptions. In particular, in the previous section we assumed that
migration occurred after the populations had converged to a unique solution. In that
case, there is no question about which individuals should migrate, because all the
individuals are identical. This section reviews theory that considers other eﬀects of
migration and pays particular attention to the choice of migrants and how they are
incorporated into the receiving population.
All evolutionary algorithms have a selection mechanism that identiﬁes which
individuals in the population will survive to generate new solutions. The choice
of selection method inﬂuences the speed of convergence of the algorithm, and it
has been demonstrated that an excessively slow or fast convergence may cause the
algorithm to fail [28, 59]. In addition, the selection pressure is related to the optimal
mutation rate and to the population size [43, 6]. This is relevant to us because
migration can cause changes in the selection pressure.
The speed of convergence of diﬀerent selection methods was ﬁrst studied by
Goldberg et al. [26], who introduced the concept of takeover time.
We consider that individuals are selected to migrate either randomly or based
on their ﬁtness. Likewise, existing individuals can be replaced by incoming migrants
randomly or based on their ﬁtness.
One way to study selection methods—and therefore migration policies—is to
examine the selection intensity, which is the diﬀerence of the average ﬁtness of
the selected individuals and of the original population, normalized by the standard
deviation of the ﬁtnesses of the original population:

270
Erick Cant´u-Paz
0.1
0.2
0.3
0.4
0.5 Mig rate
0.2
0.4
0.6
0.8
1
1.2
1.4
Intensity
(a) Best replace worst.
0.1
0.2
0.3
0.4
0.5 Mig rate
0.2
0.4
0.6
0.8
1
1.2
1.4
Intensity
(b) Best replace random.
0.1
0.2
0.3
0.4
0.5 Mig rate
0.1
0.2
0.3
0.4
0.5
Intensity
(c) Random replace worst.
Fig. 12. Selection intensity for diﬀerent migration policies varying the migration
rate and the number of neighbors (δ=1,2,..,5, from bottom to top in each graph).
I =
¯fs −¯f
σ
.
Cantu-Paz [16] derived equations for the additional selection intensity caused by
selecting migrants and their replacements based on their ﬁtnesses. The calculations
work for arbitrary migration rates and any regular communication topology where
each population communicates with δ neighbors. If the ﬁtness is distributed nor-
mally, we can derive closed-form equations for the increases in selection intensity
caused by migration. When a fraction ρ of the top individuals in a population mi-
grate, the added selection intensity is
Ie ≈δφ(Φ−1(1 −ρ)),
(9)
where φ(z) = exp(−z2/2)/
√
2π is the density of a normal distribution and Φ−1(x)
is the value of z where  z
−∞φ(y)dy = x. If the migrants are chosen randomly, there
is no increase in the selection pressure and Ie = 0.
Replacing the worst individuals by incoming migrants causes an increase in the
selection pressure given by
Ir =≈φ(Φ−1(1 −δρ)).
(10)
The maximum of Ir is φ(0) = 1/
√
2π = 0.3989, which is a fairly low value, but
it is not negligible (consider that the intensity of pairwise tournament selection is
0.5642). When individuals are replaced randomly in the receiving population, Ir = 0.
The overall selection intensity caused by migration is simply Im = Ie + Ir.
Figure 12 presents plots of Im for diﬀerent migration policies considering topologies
with diﬀerent degrees and varying the migration rate. The plots show that the
migration policy with the highest intensity is when the best individuals migrate and
replace the worst.
A higher selection pressure causes the algorithms to converge faster. Faster con-
vergence can be desirable, but converging too quickly without allowing enough time
for crossover to explore and combine useful building blocks may cause the algorithm
to converge prematurely to a solution of poor quality.
The additional selection pressure can also be the cause of some controversial
claims of superlinear parallel speedups. Despite the attempts of careful experi-
menters to set up “fair” experiments (using the same parameters for mutation and
crossover rates, keeping the total population sizes equal, using the same selection

Parameter Setting in Parallel Genetic Algorithms
271
method, and comparing algorithms when they reach the same solutions, etc.), the
migration and replacement of individuals is usually done according to their ﬁtness.
As we have seen, these choices increase the selection pressure and cause the parallel
algorithm to converge faster than it would if migrants and replacements were chosen
randomly. Clearly, comparing parallel and serial algorithms with diﬀerent selection
intensities is not fair. Superlinear speedups can also be caused by other reasons.
For example, the smaller populations might ﬁt completely in cache memory [8] or
the parallel code that executes unnecessary communications is used to measure the
serial performance [7]. More details and discussions about fair comparisons can be
found elsewhere [13, 3].
4 Conclusions
Parallel implementations of genetic and other evolutionary algorithms have long
been recognized as a promising alternative to improve the eﬃciency of these algo-
rithms. Most of the research in this ﬁeld has been empirical, and while important
observations were made even in very early studies, it is diﬃcult to assess their gen-
erality or limitations. The existing theory summarized in this chapter explains many
of the observations and provides guidelines that should be useful to practitioners.
There is, however, much that remains to be done. All theoretical models make
assumptions about the problem, the algorithm, or both. Relaxing those assump-
tions suggests opportunities to extend the theory in several directions. For example,
extending models for multiple populations to consider frequent migrations or com-
munication behavior that adapts to the progress of the search. In addition, there is
still relatively little theory related to cellular GAs. Continuing the study of these
algorithms seems to be a promising avenue of research.
References
1. P. Adamidis. Review of parallel genetic algorithms bibliography. Tech. rep.
version 1, Aristotle University of Thessaloniki, Thessaloniki, Greece, 1994.
2. E. Alba, M. Giacobini, M. Tomassini, and S. Romero. Comparing synchronous
and asynchronous cellular genetic algorithms. In J.J. Merelo et al., editor, Par-
allel Problem Solving from Nature VII, pages 601–610, Berlin Heidelberg, 2002.
Springer Verlag.
3. E. Alba and M. Tomassini. Parallelism and evolutionary algorithms. IEEE
Transactions on Evolutionary Computation, 6(5):443–462, 2002.
4. E. Alba and J. M. Troya. A survey of parallel distributed genetic algorithms.
Complexity, 4(4):31–52, 1999.
5. T. B¨ack. Parallel optimization of evolutionary algorithms. In Y. Davidor, H.-P.
Schwefel, and R. M¨anner, editors, Parallel Problem Solving fron Nature, PPSN
III, pages 418–427, Berlin, 1994. Springer-Verlag.
6. T. B¨ack. Evolutionary algorithms in theory and practice. Oxford University
Press, New York, 1996.
7. D.H. Bailey. Misleading performance reporting in the supercomputing ﬁeld.
Scientiﬁc Programming, 1(2):141–151, 1992.

272
Erick Cant´u-Paz
8. T. C. Belding. The distributed genetic algorithm revisited. In L. Eschelman,
editor, Proceedings of the Sixth International Conference on Genetic Algorithms,
pages 114–121, San Francisco, CA, 1995. Morgan Kaufmann.
9. F. H. Bennett III, J. R. Koza, J. Shipman, and O. Stiﬀelman. Building a par-
allel computer system for $18,000 that performs a half peta-ﬂob per day. In
W. Banzhaf, J. Daida, A. E. Eiben, M. H. Garzon, V. Honavar, M. Jakiela, and
R. E. Smith, editors, Proceedings of the Genetic and Evolutionary Computation
Conference 1999: Volume 2, pages 1484–1490, San Francisco, CA, 1999. Morgan
Kaufmann Publishers.
10. P. R. Cal´egari. Parallelization of Population-Based Evolutionary Algorithms for
Combinatorial Optimization Problems. Unpublished doctoral dissertation, ´Ecole
Polytechnique F´ed´erale de Lausanne (EPFL), 1999.
11. E. Cant´u-Paz. Using Markov chains to analyze a bounding case of parallel ge-
netic algorithms. Genetic Programming 98, pages 456–462, 1998.
12. E. Cant´u-Paz. Migration policies and takeover times in parallel genetic algo-
rithms. In W. Banzhaf, J. Daida, A. E. Eiben, M. H. Garzon, V. Honavar,
M. Jakiela, and R. E. Smith, editors, GECCO-99: Proceedings of the 1999 Ge-
netic and Evolutionary Computation Conference, page 775, San Francisco, CA,
1999. Morgan Kaufmann Publishers.
13. E. Cant´u-Paz. Eﬃcient and Accurate Parallel Genetic Algorithms. Kluwer Aca-
demic Publishers, Boston, MA, 2000.
14. E. Cant´u-Paz. Markov chain models of parallel genetic algorithms. IEEE Trans-
actions on Evolutionary Computation, 4(3), 2000. Also available as IlliGAL Re-
port No. 98010.
15. E. Cant´u-Paz. Selection intensity in genetic algorithms with generation gaps.
In D. Whitley, D. E. Goldberg, E. Cant´u-Paz, L. Spector, I. Parmee, and
H.-G. Beyer, editors, GECCO-2000: Proceedings of the Genetic and Evolution-
ary Computation Conference, pages 911–918, San Francisco, CA, 2000. Morgan
Kaufmann.
16. E. Cant´u-Paz. Migration policies, selection pressure, and parallel evolutionary
algorithms. Journal of Heuristics, 7(4):311–334, 2001.
17. E. Cant´u-Paz and D. E. Goldberg. Modeling idealized bounding cases of parallel
genetic algorithms. In J. Koza, K. Deb, M. Dorigo, D. Fogel, M. Garzon, H. Iba,
and R. Riolo, editors, Genetic Programming 1997: Proceedings of the Second
Annual Conference, pages 353–361, San Francisco, CA, 1997. Morgan Kaufmann
Publishers.
18. E. Cant´u-Paz and D. E. Goldberg. Predicting speedups of idealized bound-
ing cases of parallel genetic algorithms. In T. B¨ack, editor, Proceedings of the
Seventh International Conference on Genetic Algorithms, pages 113–121, San
Francisco, 1997. Morgan Kaufmann.
19. E. Cant´u-Paz and D. E. Goldberg. Eﬃcient parallel genetic algorithms: theory
and practice. Computer Methods in Applied Mechanics and Engineering, 1999.
20. E. Cant´u-Paz and M. Mej´ıa-Olvera. Experimental results in distributed genetic
algorithms. In International Symposium on Applied Corporate Computing, pages
99–108, Monterrey, Mexico, 1994.
21. Fuey-Sian Chong. Java-based distributed genetic programming on the internet.
In A. Wu, editor, Evolutionary Computation and Parallel Processing Workshop,
Proceedings of the 1999 GECCO Workshops, pages 163–166, July 1999.
22. Y. Davidor. The ECOlogical framework II: Improving GA performance at vir-
tually zero cost. In S. Forrest, editor,Proceedings of the Fifth International

Parameter Setting in Parallel Genetic Algorithms
273
Conference on Genetic Algorithms, pages 171–176, San Mateo, CA, 1993.
Morgan Kaufmann.
23. Brian D. Davison and Khaled Rasheed. Eﬀect of global parallelism on a steady
state ga. In A. Wu, editor, Evolutionary Computation and Parallel Processing
Workshop, Proceedings of the 1999 GECCO Workshops, pages 167–170, July
1999.
24. C. Gagne, M. Parizeau, and M. Dubreuil. The master-slave architecture for
evolutionary computations revisited. In E. Cant´u-Paz et al., editor, Genetic
and Evolutionary Computation—GECCO-2003, pages 1578–1579, Berlin, 2003.
Springer-Verlag.
25. M. Giacobini, E. Alba, A. Tettamanzi, and M. Tomassini. Modeling selection
intensity for toroidal cellular evolutionary algorithms. In K. Deb et al., edi-
tor, Genetic and Evolutionary Computation—GECCO 2004, pages 1138–1149,
Berlin Heidelberg, 2004. Springer Verlag.
26. D. E. Goldberg and K. Deb. A comparative analysis of selection schemes used
in genetic algorithms. Foundations of Genetic Algorithms, 1:69–93, 1991. (Also
TCGA Report 90007).
27. D. E. Goldberg, K. Deb, and J. H. Clark. Genetic algorithms, noise, and the
sizing of populations. Complex Systems, 6:333–362, 1992.
28. D. E. Goldberg, K. Deb, and D. Thierens. Toward a better understanding of
mixing in genetic algorithms. Journal of the Society of Instrument and Control
Engineers, 32(1):10–16, 1993.
29. V. S. Gordon and D. Whitley. Serial and parallel genetic algorithms as function
optimizers. In S. Forrest, editor, Proceedings of the Fifth International Con-
ference on Genetic Algorithms, pages 177–183, San Mateo, CA, 1993. Morgan
Kaufmann.
30. M. Gorges-Schleuter. Explicit parallelism of genetic algorithms through popu-
lation structures. In H.-P. Schwefel and R. M¨anner, editors, Parallel Problem
Solving from Nature, pages 150–159, Berlin, 1991. Springer-Verlag.
31. M. Gorges-Schleuter. An analysis of local selection in evolution strategies. In
W. Banzhaf, J. Daida, A. E. Eiben, M. H. Garzon, V. Honavar, M. Jakiela, and
R. E. Smith, editors, Proceedings of the Genetic and Evolutionary Computation
Conference 1999: Volume 1, pages 847–854, San Francisco, CA, 1999. Morgan
Kaufmann Publishers.
32. J. J. Grefenstette. Parallel adaptive algorithms for function optimization. Tech.
Rep. No. CS-81-19, Vanderbilt University, Computer Science Department,
Nashville, TN, 1981.
33. J.-P. Gwo, F.M. Hoﬀman, and W.W Hargrove. Mechanistic-based genetic al-
gorithm search on a Beowulf cluster of Linux PCs. In Proceedings of the High
Performance Computing Conference, Washington, DC, 2000.
34. G. Harik, E. Cant´u-Paz, D. E. Goldberg, and B. L. Miller. The gambler’s ruin
problem, genetic algorithms, and the sizing of populations. In Proceedings of
1997 IEEE International Conference on Evolutionary Computation, pages 7–12,
Piscataway, NJ, 1997. IEEE.
35. W. E. Hart. Adaptive Global Optimization with Local Search. PhD thesis, Uni-
versity of California, San Diego, 1994.
36. W. E. Hart, S. Baden, R. K. Belew, and S. Kohn. Analysis of the numerical
eﬀects of parallelism on a parallel genetic algorithm. In Proceedings of the 10th
International Parallel Processing Symposium, pages 606–612. IEEE Press, 1996.

274
Erick Cant´u-Paz
37. J.E. Hirsh and D.K. Young. Evolutionary programming strategies with self
adaptation applied to the design of rotorcraft using parallel processing. In
V. W. Porto, N. Saravanan, D. Waagen, and A.E. Eiben, editors, Proceedings of
the Seventh Annual Conference on Evolutionary Programming, pages 147–156,
Berlin, 1998. Springer Verlag.
38. M. Kirley. An empirical investigation of optimisation in dynamic environments
using the cellular genetic algorithm. In D. Whitley, D. E. Goldberg, E. Cant´u-
Paz, L. Spector, I. Parmee, and H.-G. Beyer, editors, GECCO-2000: Proceedings
of the Genetic and Evolutionary Computation Conference, pages 11–18, San
Francisco, CA, 2000. Morgan Kaufmann.
39. S.-C. Lin, E. D. Goodman, and W. F. Punch III. Investigating parallel genetic
algorithms on job shop scheduling problems. In P. J. Angeline, R. G. Reynolds,
J. R. McDonnell, and R. Eberhart, editors, Evolutionary Programming VI, pages
383–393, Berlin, 1997. Springer.
40. S-C Lin, W. Punch, and E. Goodman.
Coarse-grain parallel genetic algo-
rithms: Categorization and new approach. In Sixth IEEE Symposium on Parallel
and Distributed Processing, Los Alamitos, CA, October 1994. IEEE Computer
Society Press.
41. B. Manderick and P. Spiessens. Fine-grained parallel genetic algorithms. In J. D.
Schaﬀer, editor, Proceedings of the Third International Conference on Genetic
Algorithms, pages 428–433, San Mateo, CA, 1989. Morgan Kaufmann.
42. H. M¨uhlenbein. Evolution in time and space-The parallel genetic algorithm. In
G. J. E. Rawlins, editor, Foundations of Genetic Algorithms, pages 316–337,
San Mateo, CA, 1991. Morgan Kaufmann.
43. H. M¨uhlenbein and D. Schlierkamp-Voosen. The science of breeding and its
application to the breeder genetic algorithm (BGA). Evolutionary Computation,
1(4):335–360, 1994.
44. P. Nangsue and S. E. Conry. An agent-oriented, massively distributed paral-
lelization model of evolutionary algorithms. In J. R. Koza, editor, Late Breaking
Papers at the Genetic Programming 1998 Conference, pages 160–168, Madison,
WI, 1998. Omni Press.
45. M. Oussaid`ene. Genetic programming methodology, parallelization and applica-
tions. Unpublished doctoral dissertation, Universit´e de Gen`eve, Gen`eve, 1997.
46. W. F. Punch. How eﬀective are multiple programs in genetic programming. In
J. R. Koza, W. Banzhaf, K. Chellapilla, K. Deb, M. Dorigo, D. B. Fogel, M. H.
Garzon, D. E. Goldberg, H. Iba, and R. L. Riolo, editors, Genetic Programming
98, pages 308–313, San Francisco, 1998. Morgan Kaufmann Publishers.
47. G. G. Robertson. Parallel implementation of genetic algorithms in a classiﬁer
system. In J. J. Grefenstette, editor, Proceedings of the Second International
Conference on Genetic Algorithms, pages 140–147, Hillsdale, NJ, 1987. Lawrence
Erlbaum Associates.
48. G. Rudolph. On takeover times in spatially structured populations: Array and
ring. In K.K. Lai, O. Katai, M. Gen, and B. Lin, editors, Proceedings of the
Second Asia-Paciﬁc Conference on Genetic Algorithms and Applications, pages
144–151, Hong Kong, 2000. Global Link Publishing Company.
49. G¨unter Rudolph and Joachim Sprave. A cellular genetic algorithm with self-
adjusting acceptance threshold. In Proceedings of the First IEE/IEEE Interna-
tional Conference on Genetic Algorithms in Engineering Systems: Innovations
and Applications, pages 365–372, London, 1995. Institution of Electrical Engi-
neers and Institute for Electrical and Electronics Engineers.

Parameter Setting in Parallel Genetic Algorithms
275
50. J. Sarma and K. De Jong. An analysis of the eﬀects of neighborhood size and
shape on local selection algorithms. In H.-M. Voigt, W. Ebeling, I. Rechenberg,
and H.-P. Schwefel, editors, Parallel Problem Solving from Nature, PPSN IV,
pages 236–244, Berlin, 1996. Springer-Verlag.
51. J. Sarma and K. De Jong. An analysis of local selection algorithms in a spatially
structured evolutionary algorithm. In T. B¨ack, editor, Proceedings of the Seventh
International Conference on Genetic Algorithms, pages 181–187, San Francisco,
1997. Morgan Kaufmann.
52. J. Sarma and K. De Jong. The behavior of spatially distributed evolutionary al-
gorithms in non-stationary environments. In W. Banzhaf, J. Daida, A. E. Eiben,
M. H. Garzon, V. Honavar, M. Jakiela, and R. E. Smith, editors, Proceedings of
the Genetic and Evolutionary Computation Conference 1999: Volume 1, pages
572–578, San Francisco, CA, 1999. Morgan Kaufmann Publishers.
53. P. Spiessens and B. Manderick. A massively parallel genetic algorithm: Imple-
mentation and ﬁrst analysis. In R. K. Belew and L. B. Booker, editors, Pro-
ceedings of the Fourth International Conference on Genetic Algorithms, pages
279–286, San Mateo, CA, 1991. Morgan Kaufmann.
54. J. Sprave. A uniﬁed model of non-panmictic population structures in evolution-
ary algorithms. In Peter J. Angeline, Zbyszek Michalewicz, Marc Schoenauer,
Xin Yao, and Ali Zalzala, editors, Proceedings of the Congress on Evolutionary
Computation, volume 2, pages 1384–1391. IEEE Press, 1999.
55. T. J. Stanley and T. Mudge. A parallel genetic algorithm for multiobjective
microprocessor design. In L. Eschelman, editor, Proceedings of the Sixth Inter-
national Conference on Genetic Algorithms, pages 597–604, San Francisco, CA,
1995. Morgan Kaufmann.
56. T. Sterling. Beowulf-class clustered computing: Harnessing the power of par-
allelism in a pile of PCs. In J. R. Koza, W. Banzhaf, K. Chellapilla, K. Deb,
M. Dorigo, D. B. Fogel, M. H. Garzon, D. E. Goldberg, H. Iba, and R. L. Riolo,
editors, Genetic Programming 98, pages 883–887, San Francisco, 1998. Morgan
Kaufmann Publishers.
57. R. Tanese. Parallel genetic algorithm for a hypercube. In J. J. Grefenstette, edi-
tor, Proceedings of the Second International Conference on Genetic Algorithms,
pages 177–183, Hillsdale, NJ, 1987. Lawrence Erlbaum Associates.
58. R. Tanese. Distributed genetic algorithms. In J. D. Schaﬀer, editor, Proceedings
of the Third International Conference on Genetic Algorithms, pages 434–439,
San Mateo, CA, 1989. Morgan Kaufmann.
59. D. Thierens and D. E. Goldberg. Mixing in genetic algorithms. In S. Forrest,
editor, Proceedings of the Fifth International Conference on Genetic Algorithms,
pages 38–45, San Mateo, CA, 1993. Morgan Kaufmann.
60. M. Tomassini. Parallel and distributed evolutionary algorithms: A review. In
K. Miettinen, M. M¨akel¨a, P. Neittaanm¨aki, and J. Periaux, editors, Evolutionary
Algorithms in Engineering and Computer Science, pages 113–133. J. Wiley and
Sons, Chichester, UK, 1999.
61. D. Whitley. A free lunch proof for gray versus binary encodings. Proceedings of
the Genetic and Evolutionary Computation Conference 1999: Volume 1, pages
726–733, 1999.
62. D. Whitley and T. Starkweather. Genitor II: A distributed genetic algorithm.
Journal of Experimental and Theoretical Artiﬁcial Intelligence, 2:189–214, 1990.

276
Erick Cant´u-Paz
63. B. P. Zeigler and J. Kim. Asynchronous genetic algorithms on parallel comput-
ers. In S. Forrest, editor, Proceedings of the Fifth International Conference on
Genetic Algorithms, page 660, San Mateo, CA, 1993. Morgan Kaufmann.

Parameter Control in Practice
Zbigniew Michalewicz1 and Martin Schmidt2
1 School of Computer Science, University of Adelaide, Adelaide, SA 5005,
Australia, also with Institute of Computer Science, Polish Academy of Sciences
and Polish-Japanese School of Information Technology, Warsaw, Poland
zbyszek@cs.adelaide.edu.au
2 SolveIT Software, PO Box 3161, Adelaide, SA 5000, Australia
martin.schmidt@solveitsoftware.com
Summary. In this chapter we summarize our experience of tuning and/or con-
trolling various parameters of evolutionary algorithms from working on a variety of
complex real word problems. We illustrate some issues on one particular case study
(car distribution system). This chapter contains also general discussion on the pre-
diction and optimization issues present in dynamic environments, and explains the
ideas behind Adaptive Business Intelligence.
1 Introduction
The two major steps in applying any heuristic search algorithm (in particular: evolu-
tionary algorithm) to any problem are: (1) the speciﬁcation of the representation and
(2) the evaluation function. These two items form the bridge between the original
problem context and the problem-solving framework. When deﬁning an evolutionary
algorithm one needs to choose its components, such as variation operators (mutation
and recombination) that suit the representation, selection mechanisms for selecting
parents and survivors, and an initial population. Each of these components may
have parameters, for instance: the probability of mutation, the tournament size of
selection, or the population size. The values of these parameters greatly determine
whether the algorithm will ﬁnd a near-optimum solution, and whether it will ﬁnd
such a solution eﬃciently.
Choosing the right parameter values, however, is a time-consuming task and
considerable eﬀort has gone into developing good heuristics for it. As discussed in
Chapter 2, we distinguish two major forms of setting parame-ter values: parameter
tuning and parameter control. By parameter tuning we mean the commonly prac-
ticed approach that amounts to ﬁnding good values for the parameters before the run
of the algorithm and then running the algorithm using these values, which remain
ﬁxed during the run. Pa-rameter control, on the other hand, forms an alternative, as
it amounts to starting a run with initial parameter values which are changed during
the run.
Z. Michalewicz and M. Schmidt: Parameter Control in Practice, Studies in Computational
Intelligence (SCI) 54, 277–294 (2007)
www.springerlink.com
© Springer-Verlag Berlin Heidelberg 2007

278
Z. Michalewicz and M. Schmidt
There are many theoretical and experimental results on parameter tuning and
parameter control. However, very often real world problems (here we mean problems
for which a production system is required to optimize some aspects of company’s
operations in real time) call for shortcuts and/or some ad hoc solutions. Some of
them work very well, some others often result in disappointing performance.
In this chapter we summarize our experience of tuning and/or controlling var-
ious parameters of evolutionary algorithms from working on a variety of complex
real word problems. We illustrate some issues on one particular case study (car dis-
tribution system). The chapter is organized as follows. The next section presents the
problem of recommending the best distribution of used cars among many available
auction sites. Section 3 provides a brief overview of some issues connected with para-
meter control methods. Section 4 discusses “good practices that work,” and section
5 discusses “uncertain environments” — typical scenarios for real world problems.
Section 6 introduces the concept of Adaptive Business Intelligence and section 7
concludes this chapter.
2 A Case Study
The problem is to recommend the best distribution of used cars (which are coming
back to the company after the term of the lease or rental is over) among many
available auction sites [7]. By “best distribution,” we mean a distribution, which
maximizes the net total proceeds from all these sales. Many issues have to be con-
sidered in the process of making optimal recommendation, which vary from price
prediction for various types of cars at diﬀerent locations, to price depreciation and
volume eﬀect, to transportation issues. We discuss these in this section.
The leased/rented car is owned by a manufacturing company; the company gets
the car back when it is returned. Each car is diﬀerent, and these diﬀerences include
make and model, mileage, model year, color, type of transmission, body style and
options, wear and tear on the vehicle (i.e., the damage level), etc. There are hundreds
of auctions around the US, and each returned car has to be sold at one of them.
The characteristics listed above (plus some others) inﬂuence the sales price of a car
at each particular location. The central question is: Where should each car be sent
to maximize the total proceeding from all these sales?
The total annual number of returned leased/rental cars for just one large US-
based car manufacturer is more than 1,000,000. This would correspond to approx-
imately 4,000 cars per day. In other words, every day a remarketing team has to
make 4,000 decisions: they have to assign each car to the best possible auction site
that would maximize the sales price. Nevertheless, due to volume eﬀects, the as-
signment of cars to auctions is highly interrelated and hence the number of possible
decisions becomes astronomical! If a company operates on, say, 50 auction sites and
processes “only” 1,000 cars a day, there are 501,000 distribution choices. This gives
roughly 10,000,000,000,...,000,000 possible choices — a number where 1 at the front
is followed by 1,700 zeros! This is a mind-boggling number and no computer can
check out all combinations in a human lifetime. Never-theless, it is necessary to
make decisions on all of the cars today!
Again, the task is to distribute cars to auction sites. For example, we can look
at four-door silver 2002 Toyota Corolla with 34,983 miles on its odometer, sunroof,
automatic transmission, power windows, power seats, etc., which, at the moment,

Parameter Control in Practice
279
sits at a dealership in North Carolina, and decide where to send it. At ﬁrst glance,
this looks easy. We might be tempted just take one car at a time, look up in some
ﬁle (or a book)3 the average sale price for this particular type of car (remember to
adjust the price for mileage, options, etc.) at each auction, and decide to send the
car to the auction with the highest current average sales price. Of course, we should
also estimate the transportation cost (usually, the longer distance, the higher cost),
but all calculations seem manageable. So, what is the problem? Well, the problem is
that the above approach would not work very well. When you ship a whole truckload
of cars (say, 10 cars) from one place to another, then you will get a cheaper rate per
car than if you send only one car (or a few cars) at a time. A typical transportation
cost structure usually looks as follow (again, we assume ﬁx locations “from” and
“to”):
• Transporting 1 - 6 cars costs $120 per car.
• Transporting 7 - 10 cars costs $95 per car.
• Transporting 11 - 14 cars costs $85 per car.
• The cost of transporting more than 14 cars usually is calculated as follows: You
pay for transporting 14 cars (on one truck, it would cost $85 per car, as indicated
above) and you repeat it as many times as possible (to get the cheapest rate).
Once the number of remaining cars is less than 14, you use other rates. For
example, transporting 20 cars would cost you $85 per car for 14 cars and $120
per car for 6 cars, making 14 x $85 + 6 x $120 = $1,910.
On top of the transportation price, you also have to deal with additional issues,
such as risk factors (cars can fall down from trucks and get damaged, cars might
get stolen, etc.), insurance, depreciation, and calendars of auctions (auctions are not
organized every day). We will discuss some of these issues later in this section.
To complicate matters further, if you have, say, 45 red cars of the same
make/model and you decide to send all of them to the same auction site, which
might seem reasonable since the selected auction site oﬀers the best net price. How-
ever, if you do this then the so-called volume eﬀect kicks in, which results in a
reduced price per car.
Figure 1 illustrates the volume eﬀect phenomena: you get more money per car
when you sell a few cars than when you sell many similar cars. In other words, the
volume eﬀect would adjust the price of a car on the basis of the number of cars. For
example, the current average sale price for a particular car on a particular auction
site is $10,400. You can get the same price if you ship up to seven cars to that
location. However, if you transport 30 similar4 cars there, the average price per car
will drop to $9,450 per car.
To complicate the matter even further, every auction has a typical sales day
(e.g., every second Friday at 11 am.) Assume that at one particular location you
3 There are plenty of such books available, e.g., Black Book, Kelly Blue Book,
Manheim Auction Report, etc.
4 Note that the term “similar” can mean much more than just same make/model
or color. For example, you could have many white compact cars of diﬀerent makes
and models, which would still compete for the same buyers, so they would press
down the average sales price per car. Note also, that the volume eﬀect might be
diﬀerent for diﬀerent types of cars, e.g., the curve of the volume eﬀect is diﬀerent
for Honda Accords and for Porsche 911s.

280
Z. Michalewicz and M. Schmidt
Fig. 1. Volume eﬀect
have 20 cars that you would like to ship to an auction site. Usually the transport
between these two locations takes 10 days. The closest auction is 11 days away; if
there is a slight delay in delivery of these 20 cars, then they might miss the auction
by one day! If that happens then the cars remaining on the auction’s parking lot will
stay there for almost two weeks. This is bad for the company because it would like
to sell the cars as quickly as possible. Apart from other reasons (money is received
only after the cars are sold), the cars lose value each day. This phenomenon is called
price depreciation and is often (for an average car) around $10 per day.
Another thing that makes your job diﬃcult is that the market prices for cars
change very frequently — sometimes slightly and sometimes dramatically (e.g., after
the terrorist attack on September 11th, 2001, or after a major hurricane storm in
Florida). So, on top of all the other issues that you have to consider, you also have
to stay up-to-date with prices that may change from one day to the next. Further,
remember that car prices may change in diﬀerent regions in diﬀerent ways. For
example, it is not easy to sell a convertible Corvette in Boston in the fall, but it
might be an excellent idea to sell the car in Florida since the temperature is becoming
just right for a convertible. This is referred to as the seasonality eﬀect. Hence, you
have to stay on top of car prices at each auction each day. This is diﬃcult, to say
the least.
At the end of the day, you are ready with all your decisions. Whether they
are “optimal” or not, whether they maximize the proﬁt for the leasing company or
not, whether there is another distribution that would increase net sales by $150 per
car (if you distribute 4,000 cars per day, this would be equivalent increasing total
net sales by $600,000 on that day) or not, you can visualize the overall picture for
that day. In Figure 2, the darker circles illustrate areas where the leased cars were
returned on a particular day. The larger the circle, the more cars were returned in
that area (clearly, most cars were returned in the east of the US). Note that the
distribution of cars would look diﬀerent on the following day since diﬀerent people
and organizations might return their cars at diﬀerent locations. The lighter circles,
on the other hand, illustrate 50 auction sites that are available for sending selected

Parameter Control in Practice
281
the cars to. The locations of these auction sites are ﬁxed: they are the same every
day for any number of returned cars.5
Fig. 2. Cars to be distributed (darker circles) and 50 auction sites (lighter circles)
Clearly, this is not a trivial problem to deal with. In order for you to make your
decision you have to deal with the details of each car, many diﬀerent possible auc-
tions with their timetables and inventories, complex transportation costs, volume
eﬀects, country-wide inventory of vehicles currently on the way to auctions, price
depreciations, dynamic price changes at each auction, being able to predict sale
prices anywhere from one week to two months into the future, etc. Every leasing
company therefore has a whole remarketing team dedicated to distributing cars that
are returned from leases and rentals. These teams face the formidable task of recom-
mending the best possible distribution of the daily load of cars, day after day, week
after week, month after month. A small mistake or an infe-rior recommendation
may result in a net loss of “only” $150 per car, but this can equate into hundreds of
thousands of dollars lost in a single day if many cars are distributed! On the other
hand, if a smart decision-support system is used to improve the daily distribution of
cars in such a way that the net sales lift is around $200 per car (note here, that this
corresponds to only 1.33% increase in the price of an average oﬀ-lease car valued at
5 It may happen, however, that from time to time (usually, once a year) a set of
auction sites that the company does business with is changed. For example, a few
current auction sites are dropped, and several new auction sites are added. This
may raise additional questions: how to evaluate such business decision? Do we
in-crease or decrease proﬁts by swapping some auction sites?

282
Z. Michalewicz and M. Schmidt
$15,000), then the leasing company would increase annual proﬁts by hun-dreds of
million of dollars! Deﬁnitely, it is worthwhile to examine this possibility further.
3 Some Issues
During the 1980s, a standard genetic algorithm (GA) based on bit-representation,
one-point crossover, bit-ﬂip mutation and roulette wheel selection (with or without
elitism) was widely applied (see chapter 1). Algorithm design was thus limited to
choosing the so-called control parameters, or strategy parameters (i.e., the parame-
ters of the algorithm, not those of the problem), such as mutation rate, crossover
rate, and population size. Many researchers based their choices on tuning the con-
trol parameters “by hand,” that is experimenting with diﬀerent values and selecting
the ones that gave the best results. Later, they reported their results of applying a
particular algorithm to a particular problem, paraphrasing here:
“...for these experiments, we have used the following parameters: population
size of 100, probability of crossover equal to 0.85, etc.”
without much justiﬁcation of the choice made.
Two main approaches were tried to improve GA design in the past. First, De Jong
[1] put a considerable eﬀort into ﬁnding parameter values (for a traditional GA),
which were good for a number of numeric test problems. Later, Grefenstette [3] used
a GA as a meta-algorithm to optimize values for the same parameters for both on-
line and oﬀ-line performance. Note, however, that in both of these approaches, an
attempt was made to ﬁnd the optimal and general set of parameters; in this context,
the word “general” means that the recommended values can be applied to a wide
range of optimization problems. Formerly, genetic algorithms were seen as robust
problem solvers that exhibit approximately the same performance over a wide range
of problems [2]. The contemporary view on EAs, however, acknowledges that speciﬁc
problems (problem types) require speciﬁc EA settings for satisfactory performance
[6]. Thus, the scope of “optimal” parameter settings is necessarily narrow. Any quest
for generally (near-)optimal parameter settings is lost a priori [12]. This stresses the
need for eﬃcient techniques that help ﬁnding good parameter settings for a given
problem, in other words, the need for good parameter tuning methods.
As an alternative to tuning parameters before running the algorithm (see chapter
2), controlling them during a run was realized quite early (e.g., mutation step sizes in
the evolution strategy (ES) community). Analysis of the simple corridor and sphere
problems in large dimensions led to Rechenberg’s 1/5 success rule [9], where feedback
was used to control the mutation step size. Later, self-adaptation of mutation was
used, where the mutation step size and the preferred direction of mutation were
controlled without any direct feedback. For certain types of problems, self-adaptive
mutation was very successful and its use spread to other branches of evolutionary
computation (EC).
As mentioned earlier, parameter tuning by hand is a common practice in evolu-
tionary computation. Typically one parameter is tuned at a time, which may cause
some sub-optimal choices, since parameters often interact in a complex way. Simul-
taneous tuning of more parameters, however, leads to an enormous amount of ex-
periments. The technical drawbacks to parameter tuning based on experimentation
can be summarized as follows:

Parameter Control in Practice
283
• Parameters are not independent, but trying all diﬀerent combinations system-
atically is practically impossible.
• The process of parameter tuning is time consuming, even if parameters are
optimized one by one, regardless to their interactions.
• For a given problem the selected parameter values are not necessarily optimal,
even if the eﬀort made for setting them was signiﬁcant.
Other options for designing a good set of static parameters for an evolution-
ary method to solve a particular problem include parameter setting by analogy and
the use of theoretical analysis. Parameter setting by analogy amounts to the use of
parameter settings that have been proved successful for “similar” problems. How-
ever, it is not clear whether similarity between problems as perceived by the user
implies that the optimal set of EA parameters is also similar. As for the theoretical
approach, the complexities of evolutionary processes and characteristics of interest-
ing problems allow theoretical analysis only after signiﬁcant simpliﬁcations in either
the algorithm or the problem model. Therefore, the practical value of the current
theoretical results on parameter settings is unclear.
A general drawback of the parameter tuning approach, regardless of how the
parameters are tuned, is based on the observation that a run of an EA is an intrin-
sically dynamic, adaptive process. The use of rigid parameters that do not change
their values is thus in contrast to this spirit. Additionally, it is intuitively obvi-
ous that diﬀerent values of parameters might be optimal at diﬀerent stages of the
evolutionary process. For instance, large mutation steps can be good in the early
generations helping the exploration of the search space and small mutation steps
might be needed in the late generations to help ﬁne tuning the suboptimal chro-
mosomes. This implies that the use of static parameters itself can lead to inferior
algorithm performance. The straightforward way to treat this problem is by using
parameters that may change over time, that is, by replacing a parameter p by a
function p(t), where t is the generation counter. However, as indicated earlier, the
problem of ﬁnding optimal static parameters for a particular problem can be quite
diﬃcult, and the optimal values may depend on many other factors (like the ap-
plied recombination operator, the selection mechanism, etc.). Hence designing an
optimal function p(t) may be even more diﬃcult. Another possible drawback to this
approach is that the parameter value p(t) changes are caused by a deterministic
rule triggered by the progress of time t, without taking any notion of the actual
progress in solving the problem, i.e., without taking into account the current state
of the search. Yet researchers have improved their evolutionary algorithms, i.e., they
improved the quality of results returned by their algorithms while working on par-
ticular problems, by using such simple deterministic rules. This can be explained
simply by superiority of changing parameter values: suboptimal choice of p(t) often
leads to better results than a suboptimal choice of p.
To this end, recall that ﬁnding good parameter values for an evolutionary al-
gorithm is a poorly structured, ill-deﬁned, complex problem. But on this kind of
problem, EAs are often considered to perform better than other methods! It is thus
seemingly natural to use an evolutionary algorithm not only for ﬁnding solutions
to a problem, but also for tuning the (same) algorithm to the particular problem.
Technically speaking, this amounts to modifying the values of parameters during
the run of the algorithm by taking the actual search process into account. Basically,
there are two ways to do this. Either one can use some heuristic rule which takes
feedback from the current state of the search and modiﬁes the parameter values

284
Z. Michalewicz and M. Schmidt
accordingly, or incorporate parameters into the chromosomes, thereby making them
subject to evolution. The ﬁrst option, using a heuristic feedback mechanism, allows
one to base changes on triggers diﬀerent from elapsing time, such as population
diversity measures, relative improvements, absolute solution quality, etc. The sec-
ond option, incorporating parameters into the chromosomes, leaves changes entirely
based on the evolution mechanism. In particular, natural selection acting on solu-
tions (chromosomes) will drive changes in parameter values associated with these
solutions.
In general, methods for changing the value of a parameter can be classiﬁed into
one of three categories:
• Deterministic parameter control. This takes place when the value of a strategy
parameter is altered by some deterministic rule. This rule modiﬁes the strat-
egy parameter deterministically without using any feedback from the search.
Usually, a time-varying schedule is used, i.e., the rule will be used when a set
number of generations have elapsed since the last time the rule was activated.
• Adaptive parameter control. This takes place when there is some form of feed-
back from the search that is used to determine the direction and/or magnitude
of the change to the strategy parameter. The assignment of the value of the
strategy parameter may involve credit assignment, and the action of the EA
may determine whether or not the new value persists or propagates throughout
the population.
• Self-adaptive parameter control. The idea of the evolution of evolution can be
used to implement the self-adaptation of parameters. Here the parameters to be
adapted are encoded into the chromosomes and undergo mutation and recombi-
nation. The better values of these encoded parameters lead to better individuals,
which in turn are more likely to survive and produce oﬀspring and hence prop-
agate these better parameter values.
The main criteria for classifying methods that change the values of the strategy
parameters of an algorithm during its execution are: (1) What is changed? and (2)
How is the change made? These two questions address two issues: the component of
the evolutionary algorithm (which incorporates the parameter) and the type of con-
trol. The component and type entries are orthogonal and encompass typical forms
of parameter control within EAs. The component of parameters’ change consists
of six categories: representation, evaluation function, variation operators (mutation
and recombination), selection, replacement, and population. The type of parame-
ters’ change consists of three (discussed earlier) categories: deterministic, adaptive,
and self-adaptive mechanisms. In the following section, we give a few practical ob-
servations on some good practices regarding some components of EAs and types of
control that worked well on a variety of commercial projects we executed over the
last decade.
4 Good Practices That Work
Section 2 introduced an example of a complicated real-life problem and addressed
some problem issues. Let us have a look at some easy to implement and yet very ef-
ﬁcient good practices for solving complex real-life problems. A general strategy is to
apply forms of adaptation. However, very much like the Ockham’s Razor Principle

Parameter Control in Practice
285
for prediction and classiﬁcation systems, simplicity is more often than not the key to
quick, robust and stable implementations that perform well. Let us discuss diﬀerent
parameter settings and controls that follow our “simplicity principle”; in the fol-
lowing discussion we would refer to a solution we developed for the car distribution
problem introduced in section 2.
Initial population size. Often the initial population size is determined by mea-
suring the runtime length and quality of the solutions found by the optimizer on a
few test cases. While this might work for simple problems, a better (and still very
simple approach) is to make the size of an initial population linearly dependent
on the size of the search space.6 Obviously, this is not perfect as the convergence
of an evolutionary algorithm is not linear in the population size, but it is simple
and robust. Apart from the obvious imperfection, the authors have repeatedly used
this simple adaptation/control of the initial population size with good results. Of-
ten minimum and maximum initial population sizes should also be enforced. The
minimum population size usually is set to 10–100, which assures a reasonable level
of performance on very small problems (a very small population more or less per-
forms like a population-based hill-climbing). The maximum population size usually
is determined by the user requirements and the available computer resources, i.e.,
by how fast the computer is and how much memory it has. The reason for a max-
imum initial population size is that the end-user often wants 1) an answer within
a certain time frame, and 2) has a limited amount of available computer resources.
If we set the maximum initial population size too large then 1) the evolutionary
process would take too long time and 2) the population would require too much
memory (which results in swapping memory content, which, in turn, would reduce
the performance dramatically). Generic guidance for the maximum population size
can not be given as it depends on the speciﬁc problem, the available hardware, and
the user requirements.
In the system developed for the car distribution example (section 2) the length
of the chromosome is equal to the number of cars to be distributed. For twice as
many cars, the length of chromosomes is doubled, and we also double the initial
population size. We use the minimum population size of 100 and the maximum of
5,000.
Operator probability adaptation. Quite often problem-speciﬁc operators are
developed (or local search methods are added) to improve the performance of evo-
lutionary algorithm. It is not uncommon to work with an EA that incorporates
ﬁve or more operators. Most practitioners still identify the operators’ probabilities
just by testing (see section 3 of this chapter); ﬁnally some static probabilities are
chosen and stay ﬁxed during the whole run. However, in our commercial applica-
tions we have been using a very simple and eﬀective parameter control, which falls
into “adaptive parameter control” methods (section 3). The idea is to adapt the
probability of each operator during the evolutionary process based on the operator’s
performance. Initially each operator has a given probability for being applied (e.g.,
each operator could initially have the same probability). For a number of iterations
6 If the size of the search space is very diﬃcult or time consuming to calculate then
one can often create an initial population size that is linearly depending on the
chromosome length.

286
Z. Michalewicz and M. Schmidt
I we simply count how often each operator creates an oﬀspring that is better than
all of its parents and we calculate the success ratio SR for operator i by dividing
the number of improved oﬀspring7 created by this operator by the number of all
improved oﬀspring (whichever operator was responsible for an improvement) in the
last N iterations. Hence, for operator i we would ﬁnd
SR[i] = improvement[i]/all improvements,
where SR[i] is the success ratio of operator i, improvement[i] is the number of
improvements operator i created, and all improvements is the total number of
improved oﬀspring. If all improvements = 0 then SR[i] is undeﬁned and we don’t
update any operators’ probabilities. Otherwise, the probability of each operator is
afterwards updated:
probability[i] = probability[i] + k · SR[i],
and normalized such that the sum of all probability[i]’s adds up to 1.0 (where k
is an adjustment constant 0 ≤k ≤1). Also, just before the normalization step is
executed, it is often a good idea to make sure no operator probability[i] becomes
smaller than some threshold l (and a reasonable value for l could be l = 0.01).
Hence, if probability[i] < l then probability[i] is set to l.
With these adaptation steps the operators that do relatively well (i.e., have a
relatively high success ratios SR[i]) will slowly increase their probabilities (assum-
ing k > 0) and those that have a below average performance will decrease their
probabilities. Nevertheless, just like in Machine Learning one has to pay attention
to adapt carefully and avoid too large adjustments. The adjustments are controlled
by the constants k and I, and while good values for k and I are problem dependent,
often a good starting point is k = 0.001 and I is set in such a way, that the oper-
ators are updated every time 1,000 oﬀspring are generated. However, for diﬀerent
problems and diﬀerent types of evolutionary algorithms the values of k and I have
to be adjusted accordingly.
The beneﬁts of adaptation of the operator probabilities are multi-fold:
1. Time consuming tuning of operators is not needed anymore.
2. If an operator is added or removed at some stage of the development process,
the probabilities of all operators are re-adjusted.
3. When instances (of the same problem type) with very diﬀerent sizes are exe-
cuted (e.g., distribution of 2,000 cars versus 6,000 cars), or the characteristics of
diﬀerent instances are quite diﬀerent (e.g., one instance includes a majority of
similar cars — many white Fords returned by a large government organization
in one location, whereas the other instance has many cars of diﬀerent types
returned in many locations all over the nation), then the adaptation process
sometimes creates very diﬀerent adaptations of the operator probabilities to
compensate for problem speciﬁc details.8
7 An improved oﬀspring is an oﬀspring that is better than all its parents.
8 This was experienced many times by the authors. Often to our initial surprise
but further investigation revealed that a particular problem input had certain
characteristics that favored one type of operators over the others, e.g. mutation
versus crossover based operators.

Parameter Control in Practice
287
4. Many studies were and are still focusing on self-adaptive mutation operators.
However, very often practitioners simply implement multiple problem-speciﬁc
mutation operators and assign them some static probabilities. Some of these
mutation operators may act like large macro-mutations while others may act
more like small local hill-climbing mutations. With the described simple adap-
tation scheme of the operator probabilities, no time-consuming process of as-
signing operator probabilities is required: whenever large or small mutations are
appropriate they are more likely to be applied.
5. Finally, it is very important to understand that the operator probability adap-
tation has its clear limits. One of the most important (and often easy to avoid)
mistakes is to implement operators that have very diﬀerent computational com-
plexities. For example, if one compares the performance of uniform mutation
with an exhaustive neighborhood search operator then, not surprisingly, the
neighborhood search operator will be more successful than the blind uniform
mutation. However, this is due to the simple fact that the neighborhood search
operator takes many more computational cycles. A simple way to avoid this
problem is to “limit” the neighborhood search operator to look at a very limited
neighborhood, e.g., just one neighbor. In other words, each operator is allowed
to make a simple iteration using its own logic. A simple way to think about this
is that each operator should (roughly) take the same amount of computational
time. If this is the case, then the operator probability adaptation most often
works surprisingly well.
In the system developed for the car distribution example (section 2) we have used
six problem-speciﬁc adaptive operators ranging from “large” mutation via “small”
mutation and permutations to diﬀerent crossovers. Initially all operator probabilities
were the same: 1/6. In the system we used adaptive operators with the control
parameters I = 1, 000 and k = 0.001.
Diversity preservation. As indicated earlier, most real-world applications of
evolutionary algorithms require several problem-speciﬁc operators, decoders, ﬁne-
tuning, etc. However, many practitioners neglect the issue of diversity. Our experi-
ence indicates that it is very important to deal with this issue and it is usually a
mistake not to address it. Diversity preservation need not be a hassle to implement
and tune; at least some aspects of diversity are often easy and computationally fast
to implement. Diﬀerent schemes for diversity preservation have been proposed over
the years (e.g., sharing, crowding), but these are time consuming. Instead of try-
ing to ensure that most genotypes and/or phenotypes are diﬀerent one can simply
ensure that the ﬁtness of each individual is diﬀerent. If the ﬁtness value of a new
oﬀspring already exists in the population, one can either drop the oﬀspring (death
penalty, i.e., not insert the oﬀspring into the population) or continue to mutate the
oﬀspring until its ﬁtness is diﬀerent from any other individual (this can take a signif-
icant amount of extra computational cycles). Such diversity preservation approach
typically works well for ﬁtness functions that result in ﬂoating point numbers; it
does ensure that all solutions in the population are diﬀerent from each other and
hence represent a diverse set of solutions. However, this approach obviously has
its limitations, when the number of all possible ﬁtness values is very small (e.g., a
limited range of integers).

288
Z. Michalewicz and M. Schmidt
In the system developed for the car distribution example (section 2) the ﬁtness
of each solution is a ﬂoating point value9 and it turned out to be very beneﬁcial to
apply the above diversity preservation scheme.
Termination issues. A simple and popular stopping criteria is to quit after a
number of steps L without any improvement of the best individual (again, the para-
meter L is usually found by manual tweaking). However, often large-scale complex
optimization problems can take a very long time to ﬁne-tune at the end of the evo-
lutionary optimization process. Hence, it often makes more sense to terminate the
evolutionary algorithm based on a percentage of ﬁtness improvement of the best
individual. In other words, if the percentage of the ﬁtness improvement of the best
individual over the last L iterations is below a certain limit (e.g. 0.1%) then the evo-
lutionary optimization process terminates. Obviously the population could consist
of individuals that potentially might be further ﬁne-tuned.
To address this issue, we found it beneﬁcial to run a post-processing phase (after
the termination of the basic iterations of evolutionary algorithm) during which we
continue with a ﬁnal phase of evolutionary optimization using only the N best
individuals (N is much smaller than the initial population size). This approach
combined with the earlier described adaptive operator probabilities and diversity
preservation creates N ﬁne-tuned and diﬀerent solutions. This approach focuses the
computational cycles on the N best individuals instead of the whole population.
One could also apply an explicit hill-climber on each of the N solutions, but often
this can be implemented as a hill-climbing operator (typically a mutation based
operator). Of course, one could try to have intermediate steps where the population
is gradually reduced to focus more and more on the N best individuals. However,
this creates additional complexity which might not be worthwhile.
In the system developed for the car distribution example (section 2) we ap-
plied such simple 2-phased approach where we terminate the ﬁrst EA phase after
L = 50, 000 iterations, using the termination limit of 0.01%. This is followed by a
much smaller population (N = 20) performing local ﬁne tuning. Interestingly, many
mutation operators adaptively had their probabilities signiﬁcantly increased dur-
ing this second EA phase, since they tended to be better than the crossover-based
operators.
5 Uncertain Environments
Let us address one additional issue we experienced while solving many real world
problems. Real world problems are very often set in uncertain (and possibly chang-
ing) environments, which are usually categorized into four categories: (1) noise, (2)
robustness, (3) approximation, and (4) time-varying environments [4]. Before we
present and discuss the ﬁfth category, and argue that this ﬁfth category is the most
common in real word situations, let’s ﬁrst discuss the main features of these four
categories.
9 The ﬁtness of each solution is the sum of the net proﬁt for each car, and each
allele assigns an auction site to a car.

Parameter Control in Practice
289
Noise. Sometimes evaluation functions are subject to noise. This happens when
evaluation functions return sensory measurements or results of randomised simu-
lations. In other words, the evaluation procedure for the same solution (i.e., the
solution deﬁned as a vector of some design variables) may return diﬀerent values.
The common approach in such scenarios is to approximate a noisy evaluation func-
tion eval by an averaged sum of several evaluations:
eval(x) = 1/n
n

i=1
(f(x) + zi),
(1)
where x is a vector of design variables (i.e., variables controlled by a method), f(x)
is the evaluation function, zi represents additive noise, and n is the sample size.
Note that the only measurable (returned) values are f(x) + z.
Robustness. Sometimes design variables, other variables, or constraints of the prob-
lem are subject to perturbations after the solution is determined. The general idea is
that such (slightly modiﬁed) solutions should have quality evaluations (thus making
the original solution robust). This is important in scenarios involving manufactur-
ing tolerances, or when it is necessary to modify the original solution because of
employee illness or machine failure. The common approach to such scenarios is to
use evaluation function eval based on the probability distribution of possible distur-
bances δ, which is approximated by Monte Carlo integration:
eval(x) = 1/n
n

i=1
f(x + δi).
(2)
Note that eval(x) depends on the shape of f(x) at point x; in other words, the
neighbourhood of x determines the value of eval(x).
Approximation. Sometimes it is too expensive to evaluate a candidate solution. In
such scenarios, evaluation functions are often approximated based on experimental
or simulation data (the approximated evaluation function is often called the meta-
model). In such cases, evaluation function eval becomes:
eval(x) = f(x) + E(x),
(3)
where E(x) is the approximation error of the meta-model. Note that the approxima-
tion error is quite diﬀerent than noise, as it is usually deterministic and systematic.
Time-varying environments. Sometimes evaluation functions depend on an ad-
ditional variable: time. In such cases, evaluation function eval becomes:
eval(x) = f(x, t),
(4)
where t represents the time variable. Clearly, the landscape deﬁned by the function
f changes over time; consequently, the best solution may change its location over
time. There are two main approaches for handling such scenarios: (1) to restart the
method after a change, or (2) require that the method is capable of chasing the
changing optimum.

290
Z. Michalewicz and M. Schmidt
However, it seems the largest class of real world problems is not included in the
above four categories. From our business/industry experience of the last decade, it
is clear that in many real world problems the evaluation functions are based on the
predicted future values of some variables. In other words, evaluation function eval
is expressed as:
eval(x) = f(x, P(x, y, t)),
(5)
where P(x, y, t) represents an outcome of some prediction for solution vector x and
additional (environmental, beyond our control) variables y at time t. Let’s compare
this category with the four categories deﬁned earlier to see the diﬀerences between
them.
First of all, noise may or may not be involved. If the prediction model is deter-
ministic, then there is no noise in the scenario: every solution vector x is evaluated
to the same value. On the other hand, if the prediction model involves simulations,
noise might be present. Second, the meaning of robustness is quite diﬀerent. Unex-
pected disturbances (e.g., delays) inﬂuence the outcomes of the prediction model,
and should be handled accordingly. Third, the concept of approximation is diﬀerent.
Note, that we can evaluate a candidate solution precisely (i.e., the evaluation func-
tion is not expensive), however, approximation is connected with uncertainties of
the predictions. Finally, the time-changing environment also has a diﬀerent mean-
ing. As the real world changes, the prediction model needs constant updates and/or
parameter adjustments, thus changing the problem landscape in an implicit way.
Such real world problems call for so-called Adaptive Business Intelligence solu-
tions, i.e., intelligent decision support systems, which combine prediction, optimiza-
tion, and adaptiveness [7], [8]. Clearly, the car distribution problem falls into this
category, as it requires prediction of car prices at diﬀerent auction sites at diﬀerent
times and the recommended distributions — outcomes of optimization process —
are based on predicted prices. We discuss the basic concepts of such systems in the
following section.
6 Adaptive Business Intelligence
Adaptive Business Intelligence systems assume the existence of a few key modules
that interact with each other. We discuss them in turn.
The prediction module, in general, generates a predicted output based upon
some input (see Figure 3).
Fig. 3. Prediction module
There are many techniques for constructing a prediction model, from classic fore-
casting methods [5] to modern heuristic methods, such as neural networks, evolution-
ary programming, and genetic programming [11], [6]. A recent study [10] investigated

Parameter Control in Practice
291
the development of a new dynamic genetic programming model speciﬁcally tailored
for prediction in time-changing environments. The model incorporated methods to
automatically adapt to changes in the environment, as well as retain knowledge
learned from previously encountered environments. A similar approach was proved
to be eﬀective in the price prediction module of the car distribution system.
Next, the optimisation module has to be capable of recommending the best
answer. Note, that the optimisation module’s recommendation is based on the pre-
diction module’s output, so there is a strong relationship between the prediction
and optimization modules. The overall concept is displayed in Figure 4.
Fig. 4. Prediction and optimization modules
What happens here is that the optimization module generates some possible
solutions to the problem at hand (e.g., creates a possible distribution of cars to the
auction sites), which serves as input data for the prediction module. The predicted
output data is then used evaluate the generated answers. In other words, the op-
timisation module tries diﬀerent input data combinations in order to ﬁnd the best
predicted output.
A prediction and optimisation module are necessary components of an intelli-
gent software system, but by themselves they are not suﬃcient in today’s constantly
changing environment. The prediction module has to be adaptive and learn from
changes in the environment. Today’s accurate prediction might not be that accu-
rate tomorrow! Adaptability can be accomplished by slightly changing the learned
relationship between input and output each time it is needed. This could be every
minute, hour, day, week, month, or for any other time period. The update frequency
depends on how fast the environment changes. Some classic forecasting methods
(e.g., exponential smoothing methods) [5] approach this problem by putting more
emphasis on more recent data. However, a really good adaptive solution can make
its own decision on the frequency of update: it will continuously measure its own
prediction errors and adjust its parameters. Hence, a really good adaptive system
would adapt its own speed of adaptation!
Figure 5 illustrates the adaptation process. The recent input and recent output
are taken from very recent history, and historic data is used to construct and train
the prediction module. The adaptation module would take the recent input and
output and, if necessary, adapt the parameters of the prediction module to decrease
the prediction error — in other words, to adapt the prediction module to changes in
the environment so it can make better predictions in the future. This was essential
component for Adaptive Business Intelligence system for car distribution problem,

292
Z. Michalewicz and M. Schmidt
Fig. 5. Adaptation and prediction modules
as — on daily basis — new sales data were coming in, so it was possible to compare
the predicted and real values and make appropriate adjustments.
Such a system was implemented for the car distribution problem presented in
section 2. The system is in daily production use since 2002 and is running on Pentium
4 1GHz PC. The total running time for optimization is around 1.5 hours, but the
system needs additional time to load all required input ﬁles (including the whole US
inventory).
The system receives the inventory of the cars to be distributed from the Inventory
Management System that the client maintains. The system automatically downloads
from the ftp server and processes the input ﬁles early in the morning. The input
ﬁles contain data about the cars that need to be distributed daily; the current, up-
to-date inventory of the cars at the auctions and on the way to the auctions; and
the information about the cars that have been sold (the latter is used to adapt the
price prediction module).
The optimization starts automatically once the input ﬁles have been loaded
and processed. By the start of the business day the optimization is completed and
the proposed solution ready. However, the solution is implemented only after a
remarketing oﬃcer checks the results and (possibly) makes some small adjustments.
Less than 1% the cars from the recommended distribution are changed manually —
these changes usually correspond to last-minute actions based on newest information
(e.g., an approaching ice-storm). The ﬁnal output ﬁles are sent to the system that
manages the transportation of the cars to the auctions.
The implemented system helps the remarketing team with decisions on daily,
near-optimal distribution of cars. As discussed earlier, the problem is extremely
complex and the implemented software addresses the issues of transportation, vol-
ume sensitivity eﬀect, price depreciation, recent istory, current inventory, risk fac-
tors, and dynamic market changes. The system recommends (on daily basis) the
best distribution of cars and generates a multi-million dollar net lift per year. And
parameter control methods, described in section 4 of this chapter, contributed to
this lift in a signiﬁcant way!
There were a few possibilities to validate this lift. For example, a daily sample
of 4,000 can be divided into two sets of 2,000 cars each (with almost identical
distribution of make/models in these two sets). One set is distributed by the “old”
business model, whereas the other set by the system described above, and the results
are compared later when all cars are sold. Alternatively, on selected days of the

Parameter Control in Practice
293
week (e.g., Mondays, Wednesdays, and Fridays) the old system is used, whereas on
remaining days (e.g., Tuesdays and Thursdays) — a new system. Again, the results
are compared later, when all cars are sold. Both methods were used to evaluate our
new system.
7 Conclusions
Adaptive Business Intelligence systems address two fundamental questions that are
of the utmost importance for people, businesses, and government agencies. These
are:
• What will happen next?
• What is the best decision right now?
These two questions deal with daily lives of individuals and various organizations,
and express the desire to make “good” decisions. Everyone would like to know what
is likely to happen next (prediction) and everyone would like to make the best
decision under uncertainty and risk (optimization). For example, when people drive
to work in the morning, they try to predict the traﬃc pattern and select the best
driving route. On a larger scale, banks may need to predict future currency exchange
rates, and based upon these predictions, make decisions that will maximize proﬁt.
Eﬀective solutions for complex business problems require software that is capa-
ble of operating in time-changing environments, detecting current trends, and using
optimisation to recommend a near-optimum decision. Further, self-learning modules
continuously adapt to improve future predictions and recommendations. Through
the concept of adaptability, a software system is able to learn and adapt by evaluat-
ing the actual outcome of each decision made. This gives enterprises the ability to
monitor business trends, evolve and adapt quickly as situations change, and make
intelligent decisions on uncertain and incomplete information [8]. All these activ-
ities require adaptation of many parameters in the prediction, optimization, and
adaptation modules.
References
1. De Jong, K., “The Analysis of the Behavior of a Class of Genetic Adaptive
Systems,” PhD Dissertation, Department of Computer Science, University of
Michigan, Ann Arbor, Michigan, 1975.
2. Goldberg, D.E., Genetic Algorithms in Search, Optimization and Machine
Learning, Addison-Wesley, 1989.
3. Grefenstette, J.J., Optimization of Control Parameters for Genetic Algorithms,
IEEE Transactions on Systems, Man, and Cybernetics, Vol.16, No.1, pp.122-
128, 1986.
4. Jin, Y. and Branke, J,. Evolutionary Optimization in Uncertain Environments
- A Survey, IEEE Transactions on Evolutionary Computation, Vol.9, No.3,
pp.303-317, 2005.
5. Makridakis, S., Wheelwright, S.C., and Hyndman, R.J., Forecasting. Methods
and Applications, Wiley, 1998.

294
Z. Michalewicz and M. Schmidt
6. Michalewicz, Z. and Fogel, D.B., How to Solve It: Modern Heuristics, 2nd edi-
tion, Springer, 2004.
7. Michalewicz, Z., Schmidt, M., Michalewicz, M., and Chiriac, C.,A Decision-
Support System based on Computational Intelligence: A Case Study, IEEE In-
telligent Systems, Vol.20, No.4, July-August 2005, pp.44-49.
8. Michalewicz, Z., Schmidt, M., Michalewicz, M., and Chiriac, C., Adaptive Busi-
ness Intelligence, Springer, Berlin, 2006.
9. Rechenberg, I., Evolutionsstrategie: Optimierung technischer Syseme nach
Prinzipien der biologischen Evolution, Frommann-Holzboog, Stuttgart, 1973.
10. Wagner, N., Michalewicz, Z., Khouja, M., and McGregor, R.R., Time Series
Forecasting for Non-static Environments: the DyFor Genetic Program Model,
to appear in IEEE Transactions on Evolutionary Computation, 2006.
11. Weiss, S.M. and Indurkhya, N., Predictive Data Mining, Morgan Kaufmann,
San Francisco, 1998.
12. Wolpert, D.H. and Macready, W.G., No Free Luch Theorems for Optimization,
IEEE Transactions on Evolutionary Computation, Vol.1, No.1, pp.67-82, 1997.

Parameter Adaptation for GP Forecasting
Applications
Neal Wagner1 and Zbigniew Michalewicz2
1 Department of Mathematics and Computer Science, Augusta State University,
Augusta, GA 30904, USA nwagner@aug.edu
2 School of Computer Science, University of Adelaide, Adelaide, SA 5005,
Australia, Institute of Computer Science, Polish Academy of Sciences, ul.
Ordona 21, 01-237 Warsaw, Poland, and Polish-Japanese Institute of
Information Technology, ul. Koszykowa 86, 02-008 Warsaw, Poland
zbyszek@cs.adelaide.edu.au
Summary. Genetic Programming (GP) has been applied to time series forecast-
ing often with favorable results. However, for forecasting tasks several open issues
concerning parameter settings exist. Many real-world forecasting tasks are dynamic
in nature and, thus, static parameter settings may lead to inferior performance.
This paper presents the results of recent studies investigating non-static parameter
settings that are controlled by feedback from the GP search process. Speciﬁcally,
non-static settings for population size and training data size are explored.
Applications based on Evolutionary Algorithms (EA) rely on numerous para-
meters (e.g., population size, mutation and recombination rates, etc.) to guide and
control algorithm execution. The values of these parameters have a signiﬁcant eﬀect
on the performance of an EA. A common practice is to tune the parameters, that
is to make several pre-experiment runs with various parameter settings in order to
ﬁnd good values. These values are then used for the actual experiment and typically
remain ﬁxed for the duration of the experiment. Manually tuning parameters is
problematic because parameters are not necessarily independent of each other and,
thus, trying all possible combinations requires an enormous amount of experiments.
For many applications, especially those used in dynamic environments, optimal
parameter settings may vary over the course of a run. For such applications the use
of static parameters can lead to inferior performance. Adaptive parameter control
refers to the use of feedback from the evolutionary search process to adjust (adapt)
parameter values during a run [34].
EA and other biologically inspired methods (e.g., Neural Networks) have often
been applied to forecasting tasks [4, 7, 8, 11, 13, 17, 20, 27, 37, 38, 40]. Genetic Pro-
gramming (GP) is a sub-category of EA that is also commonly used for forecasting
[1, 5, 6, 14, 15, 16, 19, 21, 22, 23, 24, 25, 32, 35, 36, 41]. While GP forecasting applica-
tions often yield good results, a number of open issues concerning parameter settings
N. Wagner and Z. Michalewicz: Parameter Adaptation for GP Forecasting Applications, Studies
in Computational Intelligence (SCI) 54, 295–309 (2007)
www.springerlink.com
© Springer-Verlag Berlin Heidelberg 2007

296
Neal Wagner and Zbigniew Michalewicz
exist. Kaboudan [26] recognizes some of these issues including: 1) how to determine
the training data size (i.e., the number of data to be utilized for training) and 2)
how to determine the population size. A few GP forecasting studies have examined
good (static) values for training data and population sizes empirically (for example,
[10, 12]), but there is still no agreement in the literature on what may be optimal.
This paper presents the results of two recent studies which investigate adaptive
parameter control for training data size and population size, respectively. Addition-
ally, we will discuss the another aspect of the GP search process for forecasting, that
of ﬁnding an optimal evaluation (ﬁtness) function.
The rest of this paper is organized as follows: section 1 gives a brief description
of the application of GP to forecasting tasks, sections 2 and 3 detail algorithms for
adaptively controlling the training data and population sizes, respectively, section 4
discusses the search for an optimal ﬁtness function, and section 5 concludes.
1 The Use of GP for Forecasting
In order to have a clear picture of how GP is used for forecasting, it is necessary to
understand the diﬀerent kinds of forecasting problems that exist.In general, there
are three types:
1. classiﬁcation problems,
2. regression problems, and
3. time series problems.
In classiﬁcation problems the goal is to predict the class or category of a newly
occurring event by examing old events and their corresponding classes. For example,
credit card companies need to classify newly occurring credit card transactions into
one of two classes, fraudulent or non-fraudulent. For this type of problem, the issue of
time is of secondary importance and either does not play a part in the classiﬁcation
process or is incorporated as one of many features of the process.
Regression problems try to learn about the relationship between several inde-
pendent (predictor) variables and a dependent (criterion) variable. For regression,
the desired output is a number. For example, business analysts may wish to predict
a manager’s salary given knowledge of his or her position, number of supervised
employees, number of years at the position, education level, etc. As for classiﬁcation
problems, the issue of time is either non-existant or is included as a feature of the
problem.
For time series problems the dominant feature is time and the data to be analyzed
presents itself in the form of numerous observations/measurements given in sequen-
tial order of time. The goal of this type of problem is to predict a future (numeric)
value of some variable by examing past values of that variable and/or past values
of other related variables. For example, investment ﬁrms try to predict the future
value of a particular stock based on past values of that stock and past values of
other related variables such as trading volume, interest rates, etc.
GP has been used for all three of the above-listed forecasting problems, however
we will focus on time series problems as GP applications for this type of problem
are prevalent.
In GP solutions are represented as tree structures. Internal nodes of solution
trees represent appropriate operators and leaf nodes represent input variables or

Parameter Adaptation for GP Forecasting Applications
297
constants. For time series forecasting applications, the operators are mathematical
functions and the inputs are lagged time series values and/or explanatory variables.
Each solution tree represents a candidate forecasting model. Figure 1 gives an ex-
ample solution tree for time series forecasting. Variables xt1 and xt2 represent time
series values one and two periods in the past, respectively. Crossover is performed by
+
xt1
sin
×
5.31
xt2
Fig. 1. GP representation of forecasting solution xt1 + sin(5.31xt2)
exchanging subtrees from two parent solution trees. Mutation is performed by select-
ing a subtree of a single solution tree and replacing it with a randomly-constructed
subtree.
GP starts by creating an initial population of candidate forecasting models. A
candidate model is produced by (randomly) constructing a tree made up of operators
and inputs. Each model is ranked based on its prediction error over a set of training
data and new populations are generated by selecting ﬁtter models and applying the
crossover and mutation operations. New populations are created until the ﬁttest
model has a suﬃciently small prediction error, repeated generations produce no
reduction of error, or some limit for maximum generations has been reached. GP
was developed by Koza [29] as a problem-solving tool with applications in many
areas. He was the ﬁrst to use GP to search for model speciﬁcations that can replicate
patterns of observed time series.3
2 Adapting the Training Data Size
The primary objective of time series forecasting is to ﬁnd a model that accurately
represents the underlying data generating process and use this model to forecast the
future. In order to accomplish this, GP requires a set of training data that suﬃciently
represents the data generating process in question. If the training data size is too
small, forecasting models evolved by GP are unlikely to accurately represent the data
generating process and, thus, will have poor forecasting performance. If the training
data size is too large, GP models may suﬀer from “over-ﬁtting” [9]. Over-ﬁtting
3 In [29] Koza refers to this as “symbolic regression.”

298
Neal Wagner and Zbigniew Michalewicz
refers to forecasting models that ﬁt both the underlying data generating process
and extent noise in the time series. The problem of setting the training data size
is compounded when a time series is produced by a non-constant process (i.e., one
that varies over time). In such a case diﬀerent segments of a time series may have
diﬀerent underlying processes. If data from two diﬀerent segments is speciﬁed for
GP training, forecasting models evolved may be skewed. Consider the subset of time
series data shown in ﬁgure 2. Suppose this represents recent historical data and has
22, 33, 30, 27, 24, 20, 21, 20, 20



segment1
, 23, 26, 29, 30, 28, 29, 32, 30, 31



segment2
| . . .

future
Fig. 2. Time series containing segments with diﬀering underlying processes.
been chosen as training data for GP. Suppose further that the subset consists of two
segments each with a diﬀerent underlying process. The second segment’s underlying
process represents the current process and is valid for forecasting future data. The
ﬁrst segment’s process represents an older environment that no longer exists. Because
both segments are analyzed, evolved forecasting models will be distorted.
A recent study by Wagner and Michalewicz [42] investigates a novel algorithm
for adapting the training data size by using “windows” of training data that slide
with time and expand or contract based on feedback from the GP search. In this GP
forecasting application (called “Dynamic Forecasting GP” or “DyFor GP”), training
starts at the beginning of the available historical time series data. Some initial
windowsize (training data size) is set and several generations are run (in the manner
described in the previous section) to evolve a population of solutions. Then the data
window slides to include the next time series observation. Several generations are
run with the new data window and then the window slides again. This process is
repeated until all available data have been analyzed up to and including the most
recent historical data. Figure 3 illustrates this process. In the ﬁgure, | marks the
end of available historical data. The set of several generations run on a single data
window is referred to as a “dynamic generation.” Thus, a single run of DyFor GP
includes several dynamic generations (one for each window slide) on several diﬀerent
consecutive data windows.
DyFor GP adjusts the data windowsize using feedback from the GP search
process. This is accomplished in the following way.
1. Select two initial windowsizes, one of size n and one of size n + i where n and i
are positive integers.
2. Run a dynamic generation at the beginning of the historical data with window-
size n.
3. At the end of the dynamic generation, use the best evolved solution (forecasting
model) to predict a number of future data and then measure the prediction’s
accuracy.
4. Run another dynamic generation also at the beginning of the historical data
with windowsize n + i.
5. At the end of the dynamic generation, predict future data and measure the
prediction’s accuracy. Note which windowsize generated the better prediction.

Parameter Adaptation for GP Forecasting Applications
299
22, 33, 30, 27, 24, 20, 21, 20, 20



window1
, 23, 26, 29, 30, 28, 29, 32, 30, 31| . . .

future
22, 33, 30, 27, 24, 20, 21, 20, 20, 23



window2
, 26, 29, 30, 28, 29, 32, 30, 31| . . .

future
•
•
•
22, 33, 30, 27, 24, 20, 21, 20, 20, 23, 26, 29, 30, 28, 29, 32, 30, 31



windowi
| . . .

future
Fig. 3. A sliding data window.
6. Select another two windowsizes based on which windowsize had better accu-
racy. For example if the smaller of the 2 windowsizes (size n) predicted more
accurately, then choose 2 new windowsizes, one of size n and one of size n −i.
If the larger of the 2 windowsizes (size n + i) predicted more accurately, then
choose windowsizes n + i and n + 2i.
7. Slide the data windows to include the next time series observation. Use the two
selected windowsizes to run another two dynamic generations, predict future
data, and measure their prediction accuracy.
8. Repeat the previous two steps until the the data windows reach the end of
historical data.
Thus, predictive accuracy is used to determine the direction in which to adjust
the windowsize toward the optimal. Successive window slides bring the windowsize
closer and closer to this destination.
Consider the following example. Suppose the time series given in ﬁgure 2 is to
be analyzed and forecasted. As depicted in the ﬁgure, this time series consists of
two segments each with a diﬀerent underlying data generating process. The second
segment’s underlying process represents the current environment and is valid for
forecasting future data. The ﬁrst segment’s process represents an older environment
that no longer exists. If there is no knowledge available concerning these segments,
automatic techniques are required to discover the correct windowsize needed to
forecast the current setting. DyFor GP starts by selecting two initial windowsizes,
one larger than the other. Then, two separate dynamic generations are run at the
beginning of the historical data, each with its own windowsize. After each dynamic
generation, the best solution is used to predict some number of future data and
the accuracy of this prediction is measured. Figure 4 illustrates these steps. In the
ﬁgure win1 and win2 represent data windows of size 3 and 4, respectively, and
pred represents the future data predicted.
The data predicted in these initial steps lies inside the ﬁrst segment’s process
and, because the dynamic generation involving data window win2 makes use of

300
Neal Wagner and Zbigniew Michalewicz
win2



22,
win1



33, 30, 27,
pred



24, 20, 21, 20, 20



segment1
, 23, 26, 29, 30, 28, 29, 32, 30, 31



segment2
| . . .

future
Fig. 4. Initial steps of window adaptation.
a greater number of appropriate data than that of win1, it is likely that win2’s
prediction accuracy is better. If this is true, two new windowsizes for win1 and
win2 are selected with sizes of 4 and 5, respectively. The data windows then slide
to include the next time series value, two new dynamic generations are run, and the
best solutions for each used to predict future data. Figure 5 depicts these steps. In
the ﬁgure data windows win1 and win2 now include the next time series value, 24,
and pred has shifted one value to the right.
win2



22,
win1



33, 30, 27, 24,
pred



20, 21, 20, 20



segment1
, 23, 26, 29, 30, 28, 29, 32, 30, 31



segment2
| . . .

future
Fig. 5. Window adaptation after the ﬁrst window slide. Note: win1 and win2 have
size 4 and 5, respectively.
This process of selecting two new windowsizes, sliding the data windows, running
two new dynamic generations, and predicting future data is repeated until the data
windows reach the end of historical data. It may be noted that while the prediction
data, pred, lies entirely inside the ﬁrst segment, the data windows, win1 and win2,
expand to encompass a greater number of appropriate data. However, after several
window slides, when the data windows span data from both the ﬁrst and second
segments, it is likely that the window adjustment reverses direction. Figures 6 and 7
show this phenomenon. In ﬁgure 6 win1 and win2 have sizes of 4 and 5, respectively.
22, 33, 30, 27, 24, 20, 21, 20, 20



segment1
,
⎫
⎪
⎪
⎪
⎬
⎪
⎪
⎪
⎭
win1
⎫
⎪
⎪
⎪
⎪
⎪
⎪
⎬
⎪
⎪
⎪
⎪
⎪
⎪
⎭
win2
23, 26,
pred



29, 30, 28, 29, 32, 30, 31



segment2
| . . .

future
Fig. 6. Window adaptation when analysis spans both segments. Note: the smaller
window, win1, is likely to have better prediction accuracy because it includes less
erroneous data.
As the prediction data, pred, lies inside the second segment, it is likely that the
dynamic generation involving data window win1 has better prediction accuracy

Parameter Adaptation for GP Forecasting Applications
301
22, 33, 30, 27, 24, 20, 21, 20, 20



segment1
,
win1



23
⎫
⎪
⎪
⎪
⎪
⎬
⎪
⎪
⎪
⎪
⎭
win2
, 26, 29,
pred



30, 28, 29, 32, 30, 31



segment2
| . . .

future
Fig. 7. Window adaptation when analysis spans both segments. Note: win1 and
win2 have contracted to include less erroneous data.
than that involving win2 because win1 includes less erroneous data. If this is so,
the two new windowsizes selected for win1 and win2 are sizes 3 and 4, respectively.
Thus, as the data windows slide to incorporate the next time series value, they also
contract to include a smaller number of spurious data. In ﬁgure 7 this contraction
is shown.
After the data windows slide past the end of the ﬁrst segment, they expand
again to encompass a greater number of appropriate data. Figures 8 and 9 depict
this expansion.
22, 33, 30, 27, 24, 20, 21, 20, 20



segment1
, 23,
win2



26,
win1



29, 30, 28,
pred



29, 32, 30, 31



segment2
| . . .

future
Fig. 8. Window adaptation when analysis lies entirely inside the second segment.
Note: the larger data window, win2, is likely to have better prediction accuracy
because it includes a greater number of appropriate data.
22, 33, 30, 27, 24, 20, 21, 20, 20



segment1
, 23,
win2



26,
win1



29, 30, 28, 29,
pred



32, 30, 31



segment2
| . . .

future
Fig. 9. Window adaptation when analysis lies entirely inside the second segment.
Note: win1 and win2 have expanded to include a greater number of appropriate
data.
As illustrated in the above example, DyFor GP uses predictive accuracy to adapt
the size of its training data automatically towards the optimal. Automatic deter-
mination of an appropriate training window is beneﬁcial for forecasting concerns in
which the number of recent historical data whose underlying data generating process
corresponds to the current environment is not known. Furthermore, this adaptation
takes place dynamically. This means that as DyFor GP moves through (analyzes)

302
Neal Wagner and Zbigniew Michalewicz
historical time series data, the training window expands or contracts depending on
the environment encountered. In the study conducted by Wagner and Michalewicz
[42], DyFor GP with adaptive training data size is compared to conventional GP
(with static training data size) for forecasting performance. Experimental results
show that DyFor GP yields the more eﬃcient forecasts. The following section de-
scribes how population size can be controlled for GP forecasting applications.
3 Adapting the Population Size
Bloat in GP is the tendency for solution trees to grow large as they approach the
optimal [30, 31]. Solutions may become so large that they exhaust computer re-
sources. Additionally, bloat hinders a GP model’s adaptability as solutions become
too specialized to adjust to changing conditions. Bloat is a problem for any GP
model regardless of the application [3]. GP evolves a population of solutions for a
given problem and, thus, must contain some method to control the number and size
of solutions in the population. The standard method of GP population control is
due to Koza [29] and uses a static population cardinality and a maximum tree depth
for solutions.4 However, this method does not protect a GP model from bloat. If nu-
merous solutions in a population have full or nearly full trees of depth close to the
maximum, available resources may be exhausted. Additionally, the artiﬁcial limit
for tree depth prohibits the search process from exploring solutions of greater com-
plexity, which, especially for many real-world problems, may be solutions of higher
quality.
An alternative method for GP population control is presented Wagner and
Michalewicz [41] that allows natural growth of complex solutions in a setting that
more closely emulates the one found in nature. In nature the number of organisms
in a population is not static. From generation to generation, the population cardi-
nality changes depending on the quality and type of individual organisms present.
The proposed natural non-static population control (NNPC) is based on a variable
population cardinality with a limit on the total number of tree nodes present in a
population and no limit for solution tree depth. This method addresses the following
issues:
1. allowing natural growth of complex solutions of greater quality,
2. keeping resource consumption within some speciﬁed limit, and
3. allowing the population cardinality to vary naturally based on the make-up of
individual solutions present.
By not limiting the tree depth of individual solutions, natural evolution of complex
solutions is permitted. By restricting the total number of tree nodes in a population,
available resources are conserved. Thus, for a GP model that employs NNPC, the
number of solutions in a population grows or declines naturally as the individual
solutions in the population vary. This method is described in more detail below.
NNPC works in the following way. Two node limits for a population are speciﬁed
as parameters: the soft node limit and the hard node limit. The soft node limit is
deﬁned as the limit for adding new solutions to a population. This means that if
4 Koza [29] used population sizes of 500, 1000, and 2000 and maximum solution
tree depth of 17 in his early GP experiments.

Parameter Adaptation for GP Forecasting Applications
303
adding a new solution to a population causes the total nodes present to exceed
the soft node limit, then that solution is the last one added. The hard node limit
is deﬁned as the absolute limit for total nodes in a population. This means that
if adding a new solution to a population causes the total nodes present to exceed
the hard node limit, then that solution may be added only after it is repaired (the
tree has been trimmed) so that the total nodes present no longer exceeds this limit.
During the selection process, a count of the total nodes present in a population
is maintained. Before adding a new solution to a population, a check is made to
determine if adding the solution will increase the total nodes present beyond either
of the speciﬁed limits.
Wagner and Michalewicz [41] compare a GP forecasting model with NNPC to
one with the standard population control (SPC) method introduced by Koza [29].
Observed results indicate that the model with NNPC was signiﬁcantly more eﬃcient
in its consumption of computer resources than the model with SPC while the quality
of forecasts produced by both models remained equivalent. The following section
discusses the search for an optimal ﬁtness function for GP forecasting applications.
4 The Search for an Optimal Fitness Function
Evolution-based techniques such as GP require that some ﬁtness function be used to
measure the quality of candidate solutions. However, it may not be clear how to select
such a measure for a particular problem. It may be that a single measure performs
well under certain conditions but badly in others. Most GP forecasting applications
use a mean squared error (MSE) ﬁtness function for model evolution. To date, there
have been no signiﬁcant studies investigating alternative ﬁtness functions for GP
forecasting applications. One alternative measure that might be considered is the
mean absolute deviation (MAD). Comparing the MSE and MAD measures, it can be
seen that the error value of MSE grows quicker than that of MAD when outlier data
are present. Thus, outliers tend to inﬂuence analyses based on MSE more than they
do analyses based on MAD. An outlier datum can represent one of two possibilities:
noise (which should be ignored or have reduced impact on model construction) or
new information representing a shift in the underlying process. For series in which
outlier data represent noise, MAD might be the more eﬀective measure. For series in
which outlier data represent a process shift, MSE might be preferable. The question
of which measure to employ would depend upon the characteristics of the time series
to be forecast.
A novel ﬁtness function is presented by Wagner and Michalewicz [42] that incor-
porates aspects of both the MSE and MAD measures. The idea is to use MSE when
data encountered is not considered an outlier and MAD when data encountered is
considered an outlier. This new measure (called “CF” for “combined ﬁtness”) re-
quires a user-speciﬁed parameter, Ω, to determine which data are outliers and which
are not. Figure 10 gives a graphical depiction of the CF measure as a function of
the relative error. From the ﬁgure, when the relative error is within the threshold
given by Ω, CF measure values follow those of the squared error. However, when
the relative error falls outside of the Ωthreshold, CF measure values follow those
of the absolute deviation.
A number of new GP forecasting experiments were conducted on real data to
compare the MSE, MAD, and CF ﬁtness functions. Here, we forecast the U.S. Gross

304
Neal Wagner and Zbigniew Michalewicz
−Ω
Ω
Fig. 10. The CF measure as a function of the relative error.
Domestic Product (GDP) using several relevant economic variables. The following
three sections describe the GDP time series, experimental setup, and observed re-
sults, respectively.
4.1 Forecasting the U.S. Gross Domestic Product
According to the U.S. Department of Commerce [39], the GDP is deﬁned as “the
market value of goods and services produced by labor and property in the United
States.” The GDP is a metric frequently employed as a measure of the nation’s econ-
omy. The GDP series was selected because it is a widely-studied, non-linear time
series with a well-known set of explanatory variables. Figure 11 gives a graphical
depiction of the quarterly GDP (growth) time series. In the ﬁgure real GDP growth
is calculated as a quarter-over-quarter annualized percent change. A contemporary
1950
1960
1970
1980
1990
2000
−10.0%
−5.0%
0.0%
5.0%
10.0%
15.0%
20.0%
Fig. 11. Gross Domestic Product (growth): 1947-2003.

Parameter Adaptation for GP Forecasting Applications
305
study conducted by Kitchen and Monaco [28] forecasts the GDP, a time series with
quarterly frequency, using multiple economic indicators that are measured monthly.
Thirty indicators are utilized in all and can be subdivided into the following cat-
egories: employment (6), ﬁnancial (4), survey (6), production and sales (12), and
other (2). The results of their study show that forecasting models constructed using
these indicators provided eﬃcient forecasting performance for the period of 1995Q1
through 2003Q1.
4.2 Experimental Setup
For this set of GP forecasting experiments, three ﬁtness functions are compared:
MSE, MAD, and the CF measure described in section 4. Recall that the CF measure
requires that a parameter, Ω, be speciﬁed representing the threshold between outlier
and non-outlier data. For these experiments three diﬀerent settings of Ωare tested:
one that is 5.0% of the median level of the time series, one that is 7.5% of the median
level, and one that is 10.0%.
29 of the 30 economic indicators listed in the Kitchen and Monaco’s GDP fore-
casting study [28] are utilized as inputs5 by all competing models and the outputs
are one-step-ahead, quarterly forecasts for the current quarter when only one month
of historical data for that quarter is available. Historical GDP data dating back to
1965Q1 is used for training the GP and one-step-ahead forecasts for 1995Q1 through
2003Q1 are produced. Table 1 gives the GP parameter values used by all competing
models.
Table 1. GP parameter settings.
Parameter
Value
crossover rate
0.9
reproduction rate
0.0
mutation rate
0.1
population size
1000
max. no. of generations
51
elitism used?
yes
ﬁtness function
MSE/MAD/CF
All parameter values listed in table 1 were selected to match those used by Koza
[29] with the following exceptions.
1. Elitism (reproduction of the best solution of the population) is used.
2. Parameter values for “reproduction rate” and “mutation rate” were exchanged.
This was done for two reasons: 1) increasing the mutation rate allows for greater
search-space exploration [33] and 2) decreasing the reproduction rate to zero was
5 One of the indicators, “Business Week Production Index,” could not be obtained
at the time of the experiments.

306
Neal Wagner and Zbigniew Michalewicz
not thought to harm the eﬀectiveness of the evolutionary process since elitism
is used.
Because the GP search process is a stochastic one, a set of GP runs is executed
rather than just a single run. Setsize = 20 is used for all GP forecasting experiments
executed here.
4.3 Results
As mentioned in the previous section, a set of runs is executed (setsize = 20) for
each competing model. For a single run, forecasting performance is measured by
calculating the MSE of all forecasts. For a set of runs, forecasting performance is
measured by calculating the mean and standard deviation of MSE values over all
(20) runs. Table 2 gives the observed results for the GDP experiments. In the table,
the ﬁrst column shows the ﬁtness function used to drive the GP process. Note that
three separate experiments employing the CF measure are shown, one for each of
three Ωsettings (5.0%, 7.5%, and 10.0%, respectively).
Table 2. GDP forecasting results.
GP ﬁtness function
mean MSE
std. dev.
MSE
4.96
1.62
MAD
4.46
1.42
CF(Ω= 5.0%)
3.99
1.19
CF(Ω= 7.5%)
3.69
0.70
CF(Ω= 10.0%)
3.96
1.19
Table 2 reveals some interesting results. All three CF ﬁtness functions outper-
form both the MSE and MAD ﬁtness functions with the one utilizing a 7.5% setting
for Ωgiving the best performance overall. Between the MSE and MAD ﬁtness func-
tions, MAD yields the better forecasts.
The MAD ﬁtness function’s better performance when compared to MSE may
mean that for the GDP series outlier data more frequently represents noise rather
than a shift in the underlying data generating process. The fact that the CF ﬁtness
function outperforms both MSE and MAD may mean that it is better able to dis-
tinguish noise from true process shifts and, thus, can lessen the eﬀect of noisy data
and still respond to changes in the process.
The following section draws conclusions and discusses potential areas for future
work.
5 Conclusions and Future Work
In this paper, parameter adaptation for GP forecasting applications is discussed.
Novel algorithms from recent studies are presented that seek to control two sig-
niﬁcant GP parameters, training data size and population size. Additionally, GP

Parameter Adaptation for GP Forecasting Applications
307
ﬁtness functions MSE and MAD are detailed and compared. A new ﬁtness function
from a recent study that combines aspects of both the MSE and MAD measures
is described and new experiments are conducted that compare the performance of
this new measure, called the CF measure, with that of the MSE and MAD measures
for a real time series of U.S. GDP data. The CF measure is shown to give better
forecasting performance than MSE and MAD measures for the GDP series.
This CF measure contains a parameter, Ω, that represents a threshold between
outlier and non-outlier data in a time series. The experiments presented in this
study test several diﬀerent settings for Ω. Further studies might examine the CF
measure for other time series and, perhaps, develop some algorithm for adapting the
Ωparameter toward its optimal setting.
Although the algorithm detailed here for adapting the training data size is only
applicable to GP forecasting applications, the described algorithm for adapting the
population size may be appropriate for other GP applications as well. Future inves-
tigations could compare this adaptive population size to the commonly-used static
population size for several prevalent GP applications such as circuit design, database
query optimization, etc.
References
1. Andrew M. and Prager R. ‘Genetic programming for the acquistion of double
auction market strategies.’ Advances in Genetic Programming, vol. 1 (1994),
pg. 355-368.
2. Angeline P. ‘Genetic programming and emergent intelligence.’ Advances in Ge-
netic Programming, vol. 1 (1994), pg. 75-98.
3. Banzhaf W. and Langdon W. ‘Some considerations on the reason for bloat.’
Genetic Programming and Evolvable Machines, vol. 3 (2002), pg. 81-91.
4. Chambers L., editor. Practical Handbook of Genetic Algorithms: Applications.
CRC Press, 1995.
5. Chen S. and Yeh C. ‘Toward a computable approach to the eﬃcient market
hypothesis: an application of genetic programming.’ Journal of Economics Dy-
namics and Control, vol. 21 (1996), pg. 1043-1063.
6. Chen S., Yeh C., and Lee W. ‘Option pricing with genetic programming.’ Ge-
netic Programming 1998: Proceedings of the Third Annual Conference, vol. 1
(1998), pg. 32-37.
7. Chiraphadhanakul S., Dangprasert P., and Avatchanakorn V. ‘Genetic algo-
rithms in forecasting commercial banks deposit.’ Proceedings of the IEEE
International Conference on Intelligent Processing Systems, vol. 1 (1997),
pg. 557-565.
8. Deboeck G., editor. Trading on the Edge: Neural, Genetic, and Fuzzy Systems
for Chaotic and Financial Markets. John Wiley and Sons, Inc., 1994.
9. Diebold F. Elements of Forecasting. International Thomson Publishing, 1998.
10. Fernandez T. and Evett M. ‘Training period size and evolved trading sys-
tems.’ Genetic Programming 1997: Proceedings of the Second Annual Confer-
ence, vol. 1 (1997), pg. 95.
11. Gately E. Neural Networks for Financial Forecasting. John Wiley and Sons,
Inc., 1996.

308
Neal Wagner and Zbigniew Michalewicz
12. Gathercole C. and Ross P. ‘Small populations over many generations cand
beat large populations over few generations in genetic programming.’ Genetic
Programming 1997: Proceedings of the Second Annual Conference, vol. 1 (1997),
pg. 111-118.
13. Goto Y., Yukita K., Mizuno K., and Ichiyanagi K. ‘Daily peak load forecast-
ing by structured representation on genetic algorithms for function ﬁtting.’
Transactions of the Institute of Electrical Engineers of Japan, vol. 119 (1999),
pg. 735-736.
14. Hiden H., McKay B., Willis M., and Tham M. ‘Non-linear partial least squares
using genetic programming.’ Genetic Programming 1998: Proceedings of the
Third Annual Conference, vol. 1 (1998), pg. 128-133.
15. Iba H. and Sasaki T. ‘Using genetic programming to predict ﬁnancial data.’
Proceedings of the Congress of Evolutionary Computation, vol. 1 (1999),
pg. 244-251.
16. Iba H. and Nikolaev N. ‘Genetic programming polynomial models of ﬁnancial
data series.’ Proceedings of the 2000 Congress of Evolutionary Computation,
vol. 1 (2000), pg. 1459-1466.
17. Jeong B., Jung H., Park N. ‘A computerized causal forecasting system using
genetic algorithms in supply chain management.’ The Journal of Systems and
Software, vol. 60 (2002), pg. 223-237.
18. de Jong E., Watson R., and Pollack J. ‘Reducing bloat and promoting diversity
using multi-objective methods.’ Proceedings of the Genetic and Evolutionary
Computation Conference (GECCO 2001), vol. 1 (2001), pg. 11-18.
19. Jonsson P. and Barklund J. ‘Characterizing signal behavior using genetic
programming.’ Evolutionary Computing: Lecture Notes in Computer Science,
vol. 1143 (1996), pg. 62-72.
20. Ju Y., Kim C., and Shim J. ‘Genetic based fuzzy models: interest rate
forecasting problem.’ Computers and Industrial Engineering, vol. 33 (1997),
pg. 561-564.
21. Kaboudan M. ‘Forecasting with computer-evolved model speciﬁcations: a ge-
netic programming application.’ Computer and Operations Research, vol. 30
(2003), pg. 1661-1681.
22. Kaboudan M. ‘Genetically evolved models and normality of their residuals.’
Journal of Economics Dynamics and Control, vol. 25 (2001), pg. 1719-1749.
23. Kaboudan M. ‘Forecasting stock returns using genetic programming in C++.’
Proceedings of 11th Annual Florida Artiﬁcial Intelligence International Research
Symposium, vol. 1 (1998), pg. 502-511.
24. Kaboudan M. ‘Genetic programming prediction of stock prices.’ Computational
Economics, vol. 6 (2000), pg. 207-236.
25. Kaboudan M. ‘Genetic evolution of regression models for business and economic
forecasting.’ Proceedings of the Congress of Evolutionary Computation, vol. 2
(1999), pg. 1260-1268.
26. Kaboudan M. ‘A measure of time series’ predictability using genetic pro-
gramming applied to stock returns.’ Journal of Forecasting, vol. 18 (1999),
pg. 345-357.
27. Kim D. and Kim C. ‘Forecasting time series with genetic fuzzy predictor en-
semble.’ IEEE Transactions on Fuzzy Systems, vol. 5 (1997), pg. 523-535.
28. Kitchen J. and Monaco R. ‘Real-time forecasting in practice.’ Business Eco-
nomics: the Journal of the National Association of Business Economists, vol. 38
(2003), pg. 10-19.

Parameter Adaptation for GP Forecasting Applications
309
29. Koza J. Genetic Programming: On the Programming of Computers by Means
of Natural Selection. MIT Press, 1992.
30. Langdon W. ‘The evolution of size in variable length representations.’ 1998
IEEE International Conference of Evolutionary Computation, vol. 1 (1998),
pg. 633-638.
31. Langdon W. and Poli R. ‘Fitness causes bloat.’ Soft Computing in Engineering
Design and Manufacturing, vol. 1 (1997), pg. 13-22.
32. Lee D., Lee B., and Chang S. ‘Genetic programming model for long-term fore-
casting of electric power demand.’ Electric Power Systems Research, vol. 40
(1997), pg. 17-22.
33. Michalewicz Z. Genetic Algorithms + Data Structures = Evolution Programs.
Springer-Verlag, 1992.
34. Eiben A., Hinterding R., and Michalewicz Z. ‘Parameter control in evolutionary
algorithms.’ IEEE Transactions on Evolutionary Computation, vol. 3 (1999),
pg. 124-141.
35. Mulloy B., Riolo R., and Savit R. ‘Dynamics of genetic programming and
chaotic time series prediction.’ Genetic Programming 1996: Proceedings of the
First Annual Conference, vol. 1 (1996), pg. 166-174.
36. Neely C. and Weller P. ‘Predicting exchange rate volatility: genetic program-
ming versus GARCH and RiskMetricsTM.’ The Federal Reserve Bank of St.
Louis, (2002).
37. Smith K., Gupta J. Neural Networks in Business: Techniques and Applications.
Idea Group Pub., 2002.
38. Trippi R., Turban E., editors. Neural Networks in Finance and Investing: Using
Artiﬁcial Intelligence to Improve Real-World Performance. Irwin Professional
Pub., 1996.
39. U.S. Department of Commerce Bureau of Economic Analysis. http://www.bea.
doc.gov/bea/glossary/glossary g.htm, 2004.
40. Venkatesan R. and Kumar V. ‘A genetic algorithms approach to growth phase
forecasting of wireless subscribers.’ International Journal of Forecasting, vol. 18
(2002), pg. 625-646.
41. Wagner N. and Michalewicz Z. ‘Genetic programming with eﬃcient population
control for ﬁnancial times series prediction.’ 2001 Genetic and Evolutionary
Computation Conference Late Breaking Papers, vol. 1 (2001), pg. 458-462.
42. Wagner N., Michalewicz Z., Khouja M., and McGregor R. ‘Time series forecast-
ing for dynamic environments: the DyFor genetic program model.’ To appear
in IEEE Transactions on Evolutionary Computation, 2006.

Index
1/5th rule, 3, 49, 50
σ-stationarity assumption, 69
(µ + λ)-ES, 12
ACE, see average computational eﬀort
adaptation level, 50
component-level, 51
environment-level, 51
individual-level, 51
population-level, 50
adaptation mechanisms
historical development, 48–50
taxonomy schemes, 50–51
adaptation type, 50
absolute update rules, 50
dynamic algorithms, 51
empirical update rules, 50
static algorithms, 51
adaptive
operator allocation, 78
parameter tuning, 6
pursuit strategy, 80
Adaptive Business Intelligence
adaptation module, 291
optimization module, 291
prediction module, 291
additive decomposable problems, 201
exponentially scaled, 188
uniformly scaled, 188, 199
algorithm
design, 98
parameters, 19
alphabet cardinality, 208
annealing schedule, 10
approximation, 289
asynchronous master-slave, 262
average computational eﬀort, 149
Bayesian optimization algorithm, 189,
208
binary string representation, 4
bit-ﬂip mutation, 4
blend crossover, 58
BLX, see blend crossover
BOA, see Bayesian optimization
algorithm
Boltzmann selection, 37
boundedly-diﬃcult problem, see
problem, boundedly diﬃcult
building block, 186, 215, 263
-wise crossover, 210
competing, 208, 211
decision making, 187, 199, 206–208,
215
ﬁtness variance, 207
hypothesis, 56
length, see building block, size
mixing, 188, 190
selection error, 188, 190
size, 208
supply, 187, 206, 207, 214
uniformly-scaled, 210
canonical evolutionary algorithm, 6
Cauchy distribution, 54, 68
CEA, see cluster exact approximation
CEAIS, see computational eﬀort
average individual solution

312
Index
cGA, see compact genetic algorithm
CHC algorithm, 15
cluster exact approximation, 233
clustering, 208
CMA, see covariance matrix adaptation
comma-selection, 52
Commander, 162, 169
communication
time, 261
topology, 270
compact genetic algorithm, 235
parameter-less, 236
computation time, 261
computational eﬀort average individual
solution, 149
consumption factor, 192
control gene, 13
control parameter, 48, 51
convergence, 63
almost sure, 61, 63
order, 61
premature, 67, 205
velocity, 63
cooling schedule, 37
correlation function
Gaussian, 99
covariance matrix, 5, 13, 49, 50, 53, 57,
58
covariance matrix adaptation, 49, 50
crossover, 14, 54–55, 57–58, 60, 144
blend, 58
demands, 57
points, 4
punctuated, 48, 54
rate, 207
self-adaptation, 57–58
simulated binary, 58
UDNX, 58
uniform
parametrized, 14
CSA, see cumulative step-size
adaptation
cumulative path-length control, 49–50
cumulative step-size adaptation, 49, 50
DACE, see design and analysis of
computer experiments
data mining, 189
data reduction, 167
DBL, see dynamic lower bound
demands on the operators, 56–57
dependency structure matrix, 210
clustering, 208, 210, 211
elements, 209
depth ratios, 145
design and analysis of computer
experiments, 99, 100
Kriging, 99
design point, 98, 100, 101
deterministic schedule, 59
dimensional models, 207
distributed computing, 163
diversity, 214, 217
DSM, see dependency structure matrix
DSMGA, see dependency structure
matrix genetic algorithm
dynamic
ﬁtness landscapes, 6
parameter setting, 5
dynamic lower bound, 68
EA, see evolutionary algorithm
ECGA, see extended compact genetic
algorithm
EDA, see estimation of distribution
algorithm
elitist non-dominated sorting genetic
algorithm, 241
EMO, see evolutionary multi-objective
optimization
EP, see evolutionary programming
epistasis, 13
ES, see evolution strategy
estimation of distribution algorithm,
187, 189, 208, 210, 219, 226
evidence
absolute, 31
relative, 31
evolution equations, 64–67
evolution path, 49
evolution strategy, 3, 48–54, 58, 60–67,
135, 139
(1 + 1)-ES, 49
(1, λ)-ES, 61–62, 65–66
(µ/µD, λ)-ES, 66
(µ/µI, λ)-ES, 66
(µ/ρ, λ)-ES, 52
basic mechanism, 52

Index
313
nested, 4
self-adaptation, 60–67
research, 60–67
evolution window, 49
evolutionary algorithm, 47
canonical, 6
meta-, 121, 122, 134
parameter setting, 1
parameter-less, 15
properties, 69
simple, 3
evolutionary multi-objective optimiza-
tion, 241
evolutionary programming, 3, 48, 49,
51, 53
fast, 54
loss of step-size control, 67–68
meta-, 49
expected improvement, 100, 101
expected selection loss, 190
experimental design, 98
exploration-exploitation tradeoﬀ, 10
extended compact genetic algorithm,
207, 208, 218, 235
facetwise
decomposition, 186
models, 205–208, 212, 218
factor, 98
endogenous, 98
exogenous, 98
ﬁtness
correlation, 13
variance, 207, 208, 211, 214, 218
ﬁtness landscape
bitcounting function, 59
corridor model, 49
ﬂat ﬁtness function, 57, 58, 66–67
linear ﬁtness function, 57, 66–67,
69–70
properties, 9
pseudo boolean functions, 59
sphere model, 49, 58, 65–67
ﬁtness-proportional selection, 11
forecasting
classiﬁcation problems, 296
regression problems, 296
time series problems, 296
Forster-Lyapunov drift condition, 62
fully-connected demes, 267
fuzzy recombination, 58
GA, see genetic algorithm
gain criterion, 192
gambler’s ruin problem, 208, 263
Gaussian mutation, 3
genetic algorithm, 3, 48, 54–55, 57–60,
227
compact, 235
competent, 216
convergence quality, 187, 188
dependency structure matrix, 205,
206, 208, 210, 217
domino convergence model, 188
extinctive selection, 59
linkage learning, 206, 207, 218
messy, 14
meta-, 49
parallel, 259
parameter-less, 207, 226, 227
practitioner, 214
probabilistic model-building, 187, 227
real-coded, 56–58
selectorecombinative, 187
steady state (µ + 1)-GA, 60
genotype space, 13
geometric ergodicity, 62
Gram-Charlier series, 65
greedy algorithm, 11
Harris-recurrence, 61, 62
hBOA, see hierarchical Bayesian
optimization algorithm
hierarchical Bayesian optimization
algorithm, 207, 225
description of, 227
hybridized with CEA, 233
parameter-less, 226, 228
parameter-less, pseudocode of, 229
pseudocode of, 228
hierarchical trap, 231
hill-climbing algorithm, 11
hTrap, see hierarchical trap
hypercube, 267
IDEA, see iterated density-estimation
algorithm
independent runs, 264

314
Index
individual proportion, 145
innovation rate, 60
internal restarts, 15
Ising spin glass, see spin glass
isolated populations, 264
iterated density-estimation algorithm,
227
Kriging, see design and analysis of
computer experiments
landscape properties, 2
Latin hypercube design, 100
learning
population, 49
rate, 52, 60, 65, 66
LEGO, see linkage evolving genetic
algorithm
Levy-distribution, 54
LHD, see Latin hypercube design
lifetime strategy, 191
bi-linear, 191, 197
linear, 191
proportional, 191
linkage
identiﬁcation, 206, 210
information, 210
learning, 205–207
model, 210, 212, 214, 215, 217
linkage evolving genetic algorithm, 54
linked genes, 55
log-normal distribution, 52
Lyapunov function, 63
Markov chain, 59, 61–62, 266
homogenous, 61
Markov process, 64
martingale, 63
supermartingale, 63–64
master-slave, 259, 260
asynchronous, 262
synchronous, 262
MAX binary tree problem, 147
maximum speedup, 262
MDL, see minimal description length
mean squared error
of the predictor, 100
mechanism of adaptation, 51
adaptive algorithms, 51
deterministic algorithms, 51
self-adaptive algorithms, 51
memetic algorithm, 38
Metropolis criterion, 38
migration
parameters, 265
policy, 269
rate, 265, 267, 270
minimal description length, 209
model building, 208, 216
MOGA, see multi-objective genetic
algorithm
Monte-Carlo simulation, 62, 63
MSE, see mean squared error
multi-objective genetic algorithm, 248
multi-peaked landscapes, 7
multiple populations, 260
multiple restarts, 265
mutation, 3, 12–14, 48, 52, 56, 144
bit-ﬂip, 58
demands, 56–57
distribution, 52
Gaussian, 3
log-normal, 52, 65
meta-EP, 53
parameter, 52
rate, 58–60
real-coded, 59
real-coded requirements, 59
strength, 49, 52, 61
two-point, 54, 65, 68
neutrality, 55
NFL theorem, see no free lunch theorem
niched-Pareto genetic algorithm, 248
niching, 242
no free lunch theorem, 2
noise, 208, 289
non-dominated sorting genetic
algorithm, 248
non-overlapping generations, 12
nonlinear, 143
normal distribution, 51
Notz, W.I., 100
NPGA, see niched-Pareto genetic
algorithm
NSGA, see non-dominated sorting
genetic algorithm

Index
315
NSGA-II, see elitist non-dominated
sorting genetic algorithm
number of processors, 261
object parameter, 52
oﬀspring population size, 10
omni-optimizer, 251
onemax, 218, 219
optimal settings, 2
overlapping generations, 12
PAES, see Pareto-archived evolution
strategy
parallel
eﬃciency, 262
genetic algorithm, 259
search, 7
parameter
-less EA, 15
control, 20, 29
adaptive, 29
deterministic, 29
self-adaptive, 29
desired, 247
scope, 28, 32
setting, 1
dynamic, 5
oﬄine, 5, 206
online, 206
static, 4
taxonomy, 145
sweep
experiments, 165, 180
procedure, 4
sweet
spots, 5
tuning, 20, 121, 145
hybrid techniques, 134
in EDAs, 132
in GAs, 137
undesired, 247
parameterized
computer simulations, 161
uniform crossover, 14
Pareto-archived evolution strategy, 251
Pareto-optimal solutions, 242
phenotype space, 13
plus-selection, 52
PMBGA, see genetic algorithm,
probabilistic model-building
population diversity, 12
population homogeneity, 14
population proportion, 145
population size, 3, 262, 263, 302
adaptive schemes
APGA, 197
by estimates of schema variances,
190
GAVaPS, 190
parameter-less GA, 194
PRoFIGA, 198
SAGA, 193
Strategy adaptation by competing
subpopulations, 191
using noise and substructural
measurements, 199
adjustment, 210, 217
empirical comparative studies, 199,
201
equation, 188
ﬁxed, 213, 214
for model-building, 199
gambler’s ruin model, 188, 208
initial, 205, 207, 212, 214–216, 218,
219
model, 206, 207, 210, 216, 219
oﬀspring, 3
on-the-ﬂy, 205, 206, 216
online, 205, 206, 213, 214, 218
optimal, 207
parameter-less, 226
parent, 3, 7
recommendations, 200
scalability analysis, 201
theory, 186, 188, 189, 199, 201
population variance, 57, 58
predictor
empirical best unbiased, 100
probability matching allocation, 79
problem
-dependent constant, 208
boundedly diﬃcult, 205, 206, 216
design, 98, 100
gambler’s ruin, 208, 263
hierarchical trap, see hierarchical
trap
MAX binary tree, 147

316
Index
onemax, see onemax
trap, see trap
progress rate, 65
quality criterion, 192
racing, 121, 125
random genetic drift, 188
ratio
external, 145
internal, 145
recombination, 4, 12, 13, 52, 56
genetic repair eﬀect, 66
regulatory genes, 5
replacement tree depth, 145
representation adaptation, 14
reproductive variation, 3
restricted tournament replacement, 227
ring topology, 267
robust settings, 2
robustness, 289
RTR, see restricted tournament
replacement
SAMT-1, Shallow Variable Node Count
Mutation, 148
SAMT-2 and SAMT-2b, Mutation
Replacement Tree Depth, 149
SAMT-3, Internal/External Mutation
Selection Ratios, 149
Santner, T.J., 100
SBX, see simulated binary crossover
scalability, 218
schema, 263
schemata, 207, 210, 211
scope, 28, 32
selection, 52
bias, 145
Boltzmann, see Boltzmann selection
comma-, see comma-selection
extinctive, 59
ﬁtness-proportional, see ﬁtness-
proportional selection
intensity, 269, 270
parent, 7, 12
plus-, see plus-selection
pressure, 11, 207, 262, 269
survival, 7, 12
tournament, see tournament selection
truncation, see truncation selection
self-adaptation, 48, 49, 51–54
adaptation time, 65
convergence results, 61–64
crossover, 58
discrete, 60
premature convergence, 67
divergence, 67
eﬀect of recombination, 66
explicit, 48, 49
generalized, 55
implicit, 55
key features, 55–56
mutation rate, 58–60
parameter tuning, 6
premature convergence, 60, 67–69
bowl eﬀect, 68
suﬃcient conditions of avoidance,
68
response, 65
simple evolutionary algoritm, 3
simulated annealing, 5, 38
simulated binary crossover, 56, 58
sparse topology, 265, 267
SPEA, see strength Pareto evolutionary
algorithm
speedup, 262
spin glass, 232
energy, 233
ground state, 233
Ising, 232
two-dimensional, Ising, 233
statistical tests, 125, 127, 130
steady-state genetic algorithm, 10, 12
stochastic process model, 99
strategy parameter, 19, 52
strength Pareto evolutionary algorithm,
250
structure altering mutation techniques,
145
substructural, 205, 206
superlinear speedups, 270
survival selection, 7, 12
symbolic parameters, 122
symbolic regression, 297
synchronous master-slave, 262
time-varying environment, 289
topology degree, 267

Index
317
tournament selection, 11
training data size, 297
transition function, 50
transmission function, 55, 56
trap, 210, 212
concatenated, 230
hierarchical, see hierarchical trap
truncation selection, 11
UNDX-crossover, 58
variable shallow node, 145
variation, 52
Williams, B.J., 100
window size, 298


1 
Sparse Coding and Predictive Coding 
Can we learn a good representation of natural images? 
What does the brain do? 
2 
Eigenvectors strike again?  
1 e
2 e
Image Source: Turk & Pentland, 1991 
......... 
Input data u: 
Face images 
(N pixels total) 
Eigenvectors 
of the Input 
Covariance 
Matrix 
......... 
“Eigenfaces” 
3 e

3 
A Linear Model of Images using Eigenvectors 



N
i
i
iv
1
e
u


1v


2v



3v

Input image u 
1 e
2 e
3 e
)
(
      
1
N
M
noise
v
M
i
i
i




e
u
Suppose you use only the first M principal eigenvectors: 
4 
Eigenvector representation is good for 
compression but not so good if you want to 
extract the local components (or parts) of an 
image (e.g., parts of a face, local edges in a 
scene, etc.) 


1v


2v



3v

Input image u 
1 e
2 e
3 e
)
(
      
1
N
M
noise
v
M
i
i
i




e
u
Not so fast, Eigenvectors!  

5 
A Linear Model for Natural Images 


1v


2v



3v

Input image u 
1 g
2  g
3  g
noise
G
noise
v
M
i
i
i





v
g
u
    
1
G = matrix whose columns are gi 
v = vector whose elements are vi 
? 
? 
? 
(Note: M can be larger than N) 
6 
Defining the Generative Model: Likelihood 
]
;
|
[
G
p
v
u
]
[v
p
Causes  v 
Input Data 
u 
Generative 
model 
Likelihood 
Prior 
)
2
1
exp(
        
          
)
,
;
( 
]
;
|
[
2
v
u
v
u
v
u
G
I
G
Gaussian
G
p




noise
G 

v
u
Assume noise is Gaussian white noise: 
Linear model: 
C
G
G
p




2
2
1
]
;
|
[
log
v
u
v
u
Log likelihood: 

7 
Defining the Generative Model: Prior 
]
;
|
[
G
p
v
u
]
[v
p
Causes  v 
Input Data 
u 
Generative 
model 
Likelihood 
Prior 
For any input, we want only a few 
causes vi to be active 
• vi = 0 most of the time but 
high for some inputs 
• Suggests sparse distribution 
for p[vi]: peak at 0 but with 
heavy tail (also called super-
Gaussian distribution) 




i
i
i
i
G
v
p
p
v
p
p
]
;
[
log
]
[
log
]
[
]
[
v
v
Assuming causes vi are independent: 
8 
Examples of Sparse Prior Distributions 
c
v
g
p
v
g
c
p
i
i
i
i






)
(
]
[
log
))
(
exp(
]
[
v
v
Possible prior 
distributions 
Log prior 
|
|
)
(
v
v
g


)
1
log(
)
(
2v
v
g



Sparse 
Image Source: Dayan & Abbott textbook  

9 
Bayesian approach to finding v and learning G 
F Find v and G that maximize posterior probability of causes: 
 
F Equivalently, maximize log posterior: 
 
 
]
;
[
]
;
|
[
]
;
|
[
G
p
G
p
k
G
p
v
v
u
u
v


K
v
g
G
k
G
p
G
p
G
F
i
i 








)
(
2
1
log
]
;
[
log
]
;
|
[
log
)
,
(
2
v
u
v
v
u
v
Alternate between two steps 
(similar to EM algorithm):  
Maximize F with respect to v, 
keeping G fixed 
How? 
Maximize F with respect to G, 
given the v from above 
How? 
v
v
d
dF
dt
d

Gradient ascent 
10 
Finding v for a given input 
)
(
)
(
v
v
u
v
g
G
G
dt
d
T





Firing rate dynamics 
(Recurrent network) 
Error  
Sparseness constraint 
)
(
)
(
v
v
u
v
v
g
G
G
d
dF
dt
d
T





Gradient 
ascent 
Reconstruction 
(prediction) of u 
Derivative of g 

11 
Recurrent Network Implementation of Sparse Coding 
)
(
v
u
G

v
G
)
(
)
(
v
v
u
v
g
G
G
dt
d
T





Prediction 
Error 
Corrected 
Estimate 
Image Source: Dayan & Abbott textbook  
12 
Learning the Synaptic Weights G 
)
(
v
u
G

v
G
T
G
G
dt
dG
v
v
u
)
( 


Hebbian! 
(similar to Oja’s rule) 
Learning 
rule 
Prediction 
Error 
T
G
dG
dF
dt
dG
v
v
u
)
( 


Gradient 
ascent 

13 
Result: Learning G for Natural Images 
Each square is a column 
gi of G (obtained by 
collapsing rows of the 
square into a vector)  
The gi look like local 
edge or bar features 
similar to receptive 
fields in primary 
visual cortex (V1) 
(Olshausen & Field, 1996) 
14 
Sparse Coding Network is a special case of 
Predictive Coding Networks 
See Supplementary Materials: (Rao, Vision Research, 1999; Rao & Ballard, Nature Neurosci., 1999) 
v 

15 
See Supplementary Materials: (Rao & Ballard, Nature Neurosci., 1999) 
Predictive Coding Model of 
the Visual Cortex 
(Gilbert & Li, 2013) 
LGN 
V1 
V2 
Retina 
16  
Computational Neuroscience 
 
 
 
 
 
Next Week:  
Neurons as Classifiers and 
Reinforcement Learning 

